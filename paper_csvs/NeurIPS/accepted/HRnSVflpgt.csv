Section,Section Appearance Order,Paragraph
COMPUTATIONAL AND APPLIED MATHEMATICS,0.0,"1Computational and Applied Mathematics
Departments of 2Computer Science and 3Statistics
University of Chicago
{qingqi,richard1xur,risi}@uchicago.edu"
ABSTRACT,0.0028089887640449437,Abstract
ABSTRACT,0.0056179775280898875,"Recent works have shown that extending the message passing paradigm to sub-
graphs communicating with other subgraphs, especially via higher order messages,
can boost the expressivity of graph neural networks. In such architectures, to
faithfully account for local structure such as cycles, the local operations must be
equivariant to the automorphism group of the local environment. However, enumer-
ating the automorphism groups of all subgraphs of interest and finding appropriate
equivariant operations for each one of them separately is generally not feasible. In
this paper we propose a solution to this problem based on spectral graph theory
that bypasses having to determine the automorphism group entirely and constructs
a basis for equivariant operations directly from the graph Laplacian. We show that
this approach can boost the performance of GNNs on some standard benchmarks."
INTRODUCTION,0.008426966292134831,"1
Introduction"
INTRODUCTION,0.011235955056179775,"Message pasing neural networks (MPNNs) are the most popular paradigm for building neural
networks on graphs [18]. While MPNNs have proved to be remarkably effective in a range of
domains from program analysis [1] to drug discovery [20], a series of both empirical [3] and
theoretical [40, 10] results have shown that the fact that MPNNs are based on just vertices sending
messages to their neighbors limits their expressive power. This problem is especially acute in domains
such as chemistry, where the presence or absence of specific small structural units (functional groups)
directly influences the behavior and properties of molecules."
INTRODUCTION,0.014044943820224719,"Recent papers proposed to address this issue by extending the message paradigm to also allow for
message passing between vertices and edges [24] or between subgraphs [2, 15, 4, 46]. However, if the
actual messages remain scalars, the added expressive power of these networks is relatively limited."
INTRODUCTION,0.016853932584269662,"A newer development is the appearance of higher order MPNNs, where the messages are not just
scalars, but more complex objects indexed by the vertices of the sending and receiving subgraphs
[31, 34, 14]. For example, in an organic molecule, the internal state of a benzene ring (six carbon
atoms arranged in a cycle) can be represented as a matrix T ∈R6×c, where the rows correspond
to the individual carbons. When this benzene ring passes a message to a neighboring benzene ring
that it shares an edge with, information relating to the two shared carbons can be sent directly to
the corresponding atoms in the second ring. Information relating to the other vertices has to be
treated differently. Hypergraph neural networks [15, 45, 41] and neural networks on more abstract
mathematical structures such as simplicial complexes [7, 6] are closely related, since these also"
INTRODUCTION,0.019662921348314606,* denotes equal contribution.
INTRODUCTION,0.02247191011235955,"involve higher order generalizations of message passing between combinatorial objects interlocked in
potentially complicated ways."
INTRODUCTION,0.025280898876404494,"In prior work we introduced a formalism called P-tensors that provides a flexible framework for
implementing and reasoning about higher order MPNNs [21]. P-tensors build closely on the by now
substantial literature on permutation equivariance in neural networks from Deep Sets [43], to various
works studying higher order permutation equivariant maps [32]. It is also related to the category
theoretic approach employed in Natural Graph Neural Networks [12]."
INTRODUCTION,0.028089887640449437,"One aspect of higher order message passing that has hithero not been studied in detail is the interaction
between the local graph structure and the equivariance constraint. Intuitively, the reason that GNNs
must be equivariant to permutations is that when the same, or analogous, graphs are presented to the
network with the only difference being that the vertices have been renumbered, the final output must
remain the same. However, Thiede et al. [37] observed that enforcing full permutation equivariance,
especially at the local level, can be too restrictive. When considering specific, structurally important
subgraphs such as paths or cycles, the operations local to such a subgraph S should really only
be equivariant to the automorphism group of S, rather than all |S|! permutations. The Autobahn
architecture described in [37] explicitly accounts for the automorpism group of two specific types of
subgraphs, cycles and path. Defining bespoke convolution operations on these two types of subgraphs
was shown to improve performance on molecular datasets like ZINC [26]."
INTRODUCTION,0.03089887640449438,"The drawback of the Autobahn approach is that it requires explicitly identifying the automorphism
group of each type of subgraph, and crafting specialized equivariant operations based on its repre-
sentation theory. For more flexible architectures leveraging not just cycles and paths but many other
types of subgraphs this quickly becomes infeasible."
INTRODUCTION,0.033707865168539325,"Main contributions. The main contribution of this paper is a simple algorithm based on spectral graph
theory for constructing a basis of automorphism equivariant operations on any possible subgraph
without explicitly having to determine its automorphism group, let alone derive its irreducible
representations. The algorithm is based on imitating the structure of the group theoretical approach
to equivariance, which we summarize in Section 3, but bypassing having to use actual group theory.
Section 4 describes the algorithm itself and the resulting neural network operations, which we call
Schur layers. In Section 5 we show that the introduction of Schur layers does indeed improve the
performance of higher order graph neural networks on standard molecular benchmarks."
INTRODUCTION,0.03651685393258427,"2
Background: equivariance with side information"
INTRODUCTION,0.03932584269662921,"Let G be an undirected graph with vertex set V = {1, 2, . . . , n} and edge set E ⊆V ×V represented
by the adjacency matrix A ∈Rn×n. Graph neural networks (GNNs) address one of two, closely
related tasks: (a) given a function f in : V →R on the vertices of G, learn to transform it to another
function f out : V →R; (b) given not just one graph, but an entire collection of graphs, learn to
map each graph G, or rather its adjacency matrix, to a scalar or vector ψ(A) that characterizes G’s
structure."
INTRODUCTION,0.042134831460674156,"In both cases, the critical constraint is that the network must behave appropriately under permuting
the order in which the vertices are listed. The group of all permutations of {1, 2, . . . , n} is called the
symmetric group of degree n and denoted Sn. Applying a permutation σ ∈Sn to the vertices changes
the adjacency matrix to A′ = σ(A) with"
INTRODUCTION,0.0449438202247191,"A′
i,j = Aσ−1(i),σ−1(j).
(1)"
INTRODUCTION,0.047752808988764044,"The basic constraint on algorithms that operate on functions on graphs is that if the input is transformed
along with the numbering of the vertices, f in
i
′ = f in
σ−1(i), then the output must transform the same
way, f out
i
′ = f out
σ−1(i). This property is called equivariance. Formally, denoting the mapping from
inputs to outputs ϕ: f in 7→f out, equivariance states that the action σ: f 7→f ′ of permutations on
functions given by f ′
i = fσ−1(i) must commute with ϕ, that is,"
INTRODUCTION,0.05056179775280899,"ϕ(σ(f in)) = σ(ϕ(f in))
(2)"
INTRODUCTION,0.05337078651685393,"for any σ ∈Sn and any input function f in ∈RV . In contrast, for networks that just learn embeddings
G 7→ψ(A), the constraint is invariance to permutations, i.e., ψ(A) = ψ(σ(A)). In practice, the two
cases are closely related because most graph embedding networks work with functions defined on the"
INTRODUCTION,0.056179775280898875,"vertices, and then symmetrize over permutations in their final layer using a readout function such as
ψ(G) = P"
INTRODUCTION,0.05898876404494382,"i f out
i ."
INTRODUCTION,0.06179775280898876,"Enforcing (2) on the hypothesis space directly would be very limiting, effectively constraining GNNs
to be composed of symmetric polynomials of f in combined with pointwise nonlinearities. Message
passing graph neural networks cleverly get around this problem by using the adjacency matrix itself
as side information to guide equivariance. For example, in classical (zeroth order) MPNNs, the output
of each layer is a (vector valued) function on the graph, and the update rule from layer ℓto layer ℓ+ 1
in the simplest case is"
INTRODUCTION,0.06460674157303371,"f ℓ+1
i
= η

W P"
INTRODUCTION,0.06741573033707865,"j∈N(i)f ℓ
j + θ

,
(3)"
INTRODUCTION,0.0702247191011236,"where N(i) denotes the neighbors of node i in G, W and θ are learnable weight matrices/vectors and
η is a suitable nonlinearity. The fact that the summation only extends over the neighbors of vertex i
induces a dependence of ϕ on the adjacency matrix A. Hence it would be more accurate to use the
notation ϕA for the mapping, and write the equivariance condition as"
INTRODUCTION,0.07303370786516854,"ϕσ(A)(σ(f in)) = σ(ϕA(f in)).
(4)"
INTRODUCTION,0.07584269662921349,"In this interpretation, when the vertices are permuted, A can implicitly provide information about
this to the algorithm, allowing it to compensate. However, A cannot convey any information
about permutations that leave it invariant, so ϕA must still be equivariant to the group of all such
permutations, called the automorphism group of G, denoted Aut(G) or just Aut(A). This observation
about A acting as a source of side information that can reduce the size of the group that individual
GNN operations need to be equivariant to is the starting point for the rest of this paper."
HIGHER ORDER GNNS,0.07865168539325842,"2.1
Higher order GNNs"
HIGHER ORDER GNNS,0.08146067415730338,"Despite the great success of message passing networks, a long sequence of empirical as well as
theoretical results [40, 34, 30, 13, 35] over the last several years have made it clear that the expressive
power of algorithms based on simple update rules like (3) is severely limited. In response, the
community has extended the message passing paradigm to message passing between vertices and
edges or between carefully selected subgraphs [18, 34, 6, 8]. These networks maintain the local
and equivariant character of earlier MPNNs, but they can more faithfully reflect local topological
information and are particularly well suited to domains such as chemistry, where capturing local
structures such as functional groups is critical."
HIGHER ORDER GNNS,0.08426966292134831,"In tandem, researchers have developed higher order MPNN architectures, where the outputs of
individual edge or subgraph “neurons” are not just scalars, but vector or tensor quantities indexed by
the vertices of the graph involved in the given substructure. For example, in the chemistry setting,
if n is a titular neuron corresponding to a benzene ring in a molecule, the output of n, assuming we
have C channels, might be a matrix T ∈R6×C or a tensor T ∈R6×6×C, or T ∈R6×6×6×C, etc.. In
each of these cases the non-channel dimensions correspond to the six atoms making up the ring, and
when n communicates with other subgraph-neurons, must be treated accordingly. Such higher order
representations offer greater expressive power because they allow T to capture information about
relations between pairs of vertices, triples, and so on. The general trend towards studying higher
order message passing is also closely tied to the emergence of hypergraph neural networks [15],
“topological” neural networks and simplicial complex networks [7, 6]."
HIGHER ORDER GNNS,0.08707865168539326,"Recently, [21] proposed a general formalism for describing such higher order architectures using
so-called P-tensors. In this formalism, given a neuron n attached to a subgraph S with m vertices,
we say that the output of n is a zeroth order P-tensor T with C channels if it is simply a vector
T ∈RC. The elements of this vector are scalars in the sense that they are invariant to permutations of
the vertices of S. We say that T is a first order P-tensor if it is a matrix T ∈Rm×C whose columns
transform under permutations of S similarly to how f in and f out transform under global permutations
of the full graph:"
HIGHER ORDER GNNS,0.0898876404494382,"σ: T 7→T ′
T ′
i,c = Tσ−1(i),c
σ ∈Sm.
(5)"
HIGHER ORDER GNNS,0.09269662921348315,"We say that T is a second order P-tensor T ∈Rm×m×C, if each slice corresponding to a given
channel transforms according to the second order action of the symmetric group, similarly (1):"
HIGHER ORDER GNNS,0.09550561797752809,"σ: T 7→T ′
T ′
i,j,c = Tσ−1(i), σ−1(j),c
σ ∈Sm.
(6)"
HIGHER ORDER GNNS,0.09831460674157304,"Continuing this pattern, a k’th order P-tensor T ∈Rm×m×...×m×C transforms under local permuta-
tions as"
HIGHER ORDER GNNS,0.10112359550561797,"σ: T 7→T ′
T ′
i1,...,ik,c = Tσ−1(i1), ... , σ−1(ik),c
σ ∈Sm.
(7)"
HIGHER ORDER GNNS,0.10393258426966293,"[21] derives the general rules for equivariant message passing between such P-tensors in the cases
that the sending and receiving subgraphs S resp. S′ are (a) the same (b) partially overlap (c) are
disjoint. However, in each of these cases however it was assumed that T needs to be equivariant to
all possible permutations of the vertices of the underlying subgraphs S and S′."
HIGHER ORDER GNNS,0.10674157303370786,"As we discussed above, this is an overly restrictive condition that limits the extent to which a higher
order subgraph neural network can exploit the underlying topology. In the following sections we
focus on just the type of messages that are sent from a given subgraph S to itself (called linmaps in
the P-tensors nomenclature), and derive a way of making these messages equivariant to just Aut(S)
rather than the full symmetric group Sm."
HIGHER ORDER GNNS,0.10955056179775281,"3
Equivariance to local permutations: the group theoretic approach"
HIGHER ORDER GNNS,0.11235955056179775,"Recall that a representation of a finite group G such as Sm or Aut(S) is a (complex) matrix valued
function ρ: G →Cdρ×dρ where the ρ(σ) matrices multiply the same way as the corresponding group
elements do
ρ(σ2σ1) = ρ(σ2)ρ(σ1)
σ1, σ2 ∈G.
Two representations ρ and ρ′ are said to be equivalent if there is an invertible matrix Q such that
ρ′(σ) = Q−1ρ(σ) Q for all group elements. A representation of a finite group is said to be reducible
if there is some invertible matrix Q that reduces it to a block diagonal form"
HIGHER ORDER GNNS,0.1151685393258427,"ρ(σ) = Q−1

ρ1(σ) ρ2(σ) 
Q."
HIGHER ORDER GNNS,0.11797752808988764,"Some fundamental results in representation theory tell us that G only has a finite number of inequiva-
lent irreducible representations (irreps, for short), these irreps can be chosen to all be unitary, and
that any representation of G is reducible to some combination of them [36]. These facts give rise to a
type of generalized Fourier analysis on finite groups that can decompose vectors that G acts on into
parts transforming according to the unitary irreps of the group."
HIGHER ORDER GNNS,0.12078651685393259,"The general approach to defining Aut(S)-equivariant maps for first, second, and higher order
subgraph neurons would use this machinery. In particular, defining permutation matrices as usual as"
HIGHER ORDER GNNS,0.12359550561797752,"[Pσ]i,j =
 1
if σ(j) = i
0
otherwise,"
HIGHER ORDER GNNS,0.12640449438202248,"dropping the channel indices without loss of generality, and writing our P-tensors in vectorized form
T = vec(T) ∈Rmk, (5)–(7) can be written in a unified form"
HIGHER ORDER GNNS,0.12921348314606743,T ′ = P k(σ) T
HIGHER ORDER GNNS,0.13202247191011235,"where P k(σ) is the k-fold Kronecker product matrix P k(σ) = Pσ ⊗Pσ ⊗. . . Pσ. Crucially, as σ
ranges over the automorphisms of S, these product matrices P k(σ), form a unitary representation of
the automorphism group."
HIGHER ORDER GNNS,0.1348314606741573,"According to representation theory, P k must then be decomposable into a direct sum of irreps
ρ1, . . . , ρp of Aut(S) with corresponding multiplicities κ1, . . . , κp. The same unitary matrix Q
that accomplishes this can also be used to decompose T into a combinations of smaller vectors
(ti
j ∈Rdρi)i,j:"
HIGHER ORDER GNNS,0.13764044943820225,"QT =
M i κi
M"
HIGHER ORDER GNNS,0.1404494382022472,"j=1
ti
j,"
HIGHER ORDER GNNS,0.14325842696629212,"where each ti
j now transforms independently under the action of the group as ti
j 7→ρi(σ) ti
j. Al-
ternatively, stacking all ti
j vectors transforming according to the same irrep ρi together in a matrix
T i ∈Rdρi×κi, we arrive at a sequence of matrices T 1, T 2, . . . , T p transforming as"
HIGHER ORDER GNNS,0.14606741573033707,"T 1 7→ρ1(σ) T 1
T 2 7→ρ2(σ) T 2
. . .
T p 7→ρp(σ) T p.
(8)"
HIGHER ORDER GNNS,0.14887640449438203,"It is very easy to see how one might construct learnable linear operations that are equivariant to these
actions: simply multiply each T i from the right by a learnable weight matrix W i."
HIGHER ORDER GNNS,0.15168539325842698,"This general, group theoretic approach to constructing automorphism group equivariant linear maps
between k’th order P-tensors can be seen as a special case of [29, 38]. Operationally, it just reduces
to the following sequence of steps:"
HIGHER ORDER GNNS,0.1544943820224719,"1. Find the unitary matrix Q that decomposes P k(σ) = Pσ ⊗. . . ⊗Pσ into a direct sum of Aut(S)
irreps.
2. Use Q to decompose the input P-tensor into a sequence of matrices T 1. . . T p transforming as (8).
3. Multiply each T i by an appopriate learnable weight matrix W i ∈Rκi×κi.
4. Use the inverse map Q−1 = Q† to reassemble T 1, T 2, . . . , T p into the output P-tensor T out."
HIGHER ORDER GNNS,0.15730337078651685,"Notwithstanding its elegance, this representation approach to implementing automorphism group
equivariance also has some disadvantages. Specifically, it requires to (a) determine the automorphism
group of each subgraph, and (b) explicitly find its irreducible representations, which is also not
trivial. The underlying mathematical structure however is important because it forms the basis to
generalizing the approach to a much simpler framework in the next section:"
HIGHER ORDER GNNS,0.1601123595505618,"1. We have a collection of (orthogonal) linear maps {P k(σ): U 7→U}σ∈Aut(S) (with U = Rmk)
that the neuron’s operation needs to be equivariant to.
2. U is decomposed into an orthogonal sum of subspaces U = U1 ⊕. . . ⊕Up corresponding to
the different irreps featured in the decomposition of P k.
3. Each Ui is further decomposed into an orthogonal sum of subspaces Ui = V i
1 ⊕. . . ⊕V i
κi
corresponding to the different columns of the T i matrices.
4. The decomposition is such that the {P k(σ)} maps fix each V i
j subspace. Moreover, for a fixed
i, {P k(σ)} acts the same way on each V i
j subspace by ρi(σ).
5. This structure implies that any linear map that linearly mixes the V i
1 , . . . V i
κi subspaces but does
not mix information across subspaces with different values of i is equivariant."
HIGHER ORDER GNNS,0.16292134831460675,"4
Equivariance via spectral graph theory: Schur layers"
HIGHER ORDER GNNS,0.16573033707865167,"In place of the representation theoretical approach described in the previous section, in this paper we
advocate a simpler way of implementing automorphism group equivariance based on just spectral
graph theory. The cornerstones of this approach are the following two theorems. The proofs can be
found in the Appendix.
Theorem 1. Let G be a finite group acting on a space U by the linear action {g: U →U}g∈G.
Assume that we have a decomposition of U into a sequence of spaces of the form"
HIGHER ORDER GNNS,0.16853932584269662,U = U1 ⊕. . . ⊕Up
HIGHER ORDER GNNS,0.17134831460674158,"where each Ui is invariant under the action of the group (this means that for any g ∈G and v ∈Ui, we
have g(v) ∈Ui). Let ϕ: U →U be a linear map that is a homothety on each Ui, i.e., ϕ(w) = αiw for
some fixed scalar αi for any w ∈Ui. Then ϕ is equivariant to the action of G, i.e., ϕ(g(u)) = g(ϕ(u))
for any u ∈U and any g ∈G."
HIGHER ORDER GNNS,0.17415730337078653,"The representation theoretic result of the previous section corresponds to a refinement of this result
involving a further decomposition of each Ui space into a sequence of smaller subspaces."
HIGHER ORDER GNNS,0.17696629213483145,"Theorem 2. Let G and U be as in Theorem 1, but now assume that each Ui further decomposes into
an orhthogonal sum of subspaces in the form Ui = V i
1 ⊕. . . ⊕V i
κi such that
(a) Each V i
j subspace is individually invariant by G;
(b) For any fixed value of i, the spaces V i
1 , . . . , V i
κi are isomorphic and there is a set of canonical
isomorphisms ιi
j→j′ : V i
j →V i
j′ between them such that"
HIGHER ORDER GNNS,0.1797752808988764,"g(ιi
j→j′(v)) = ιi
j→j′(g(v))
∀v ∈V i
j ."
HIGHER ORDER GNNS,0.18258426966292135,Let ϕ: U →U be a map of the form
HIGHER ORDER GNNS,0.1853932584269663,"ϕ(v) =
X"
HIGHER ORDER GNNS,0.18820224719101122,"j′
αi
j,j′ ιi
j→j′(v)
v ∈V i
j"
HIGHER ORDER GNNS,0.19101123595505617,"for some fixed set of coefficients {αi
j,j′}. Then ϕ is equivariant to the action of G on U."
HIGHER ORDER GNNS,0.19382022471910113,"In the matrix language of the previous section, U1, . . . , Up correspond to the T 1, T 2, . . . , T p matrices,
whereas the V i
1 , . . . , V i
κi subspaces of Ui correspond to individual columns of T i. For any fixed i,
the
 
αi
j,j′
"
HIGHER ORDER GNNS,0.19662921348314608,"j,j′ scalars correspond to the individual matrix entries of the learnable weight matrix W i.
For our simplified spectral approach to automorphism group equivariance we will content ourselves
with using Theorem 1 rather than Theorem 2."
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.199438202247191,"4.1
Automorphism invariance the simple way via spectral graph theory"
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.20224719101123595,"The key insight of this paper is that we do not necessarily need to use heavy representation theoretic
machinery to find a system of subspaces to plug into Theorem 1. In particular, we have the following
simple lemma."
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.2050561797752809,"Lemma 1. Let S be an undirected graph with m vertices, AutS its automorphism group, and
L its combinatorial graph Laplacian. Assume that L has p distinct eigenvalues λ1, . . . , λp and
corresponding subspaces U1, . . . , Up. Then each Ui is invariant under the first order action (5) of
AutS on Rm."
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.20786516853932585,"Proof. Since AutS is a subgroup of the full group of vertex permutations , its action on Rm is just
v 7→σ(v) = Pσv with σ ∈AutS. L is a real symmetric matrix, so its eigenspaces U1, . . . , Up are
mutually orthogonal and U1 ⊕. . . ⊕Up = Rn. Furthermore, v ∈Ui if and only if Lv = λiv. By
definition, AutS is the set of permutations that leave the adjacency matrix, and consequently the
Laplacian invariant, so. In particular, PσLPσ−1 = L for any σ ∈AutS. Therefore, for any v ∈Ui
L (σ(v)) = LPσv = PσLPσ−1Pσv = PσLv = λiPσv = λiσ(v)
∀σ ∈AutS
showing that σ(v) ∈Ui. Hence Ui is an invariant subspace."
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.21067415730337077,"The following Corollary puts this lemma to use, providing a surprisingly easy way of creating locally
automorphism equivariant neurons. We define a Schur layer as a neural network module that applies
this operation to every instance of a given subgraph in the graph, for example, every benzene ring
in a molecule. For the sake of global permutation equivariance, the weight matrices for any given
subgraph S must be shared across all instances of S across in the graph."
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.21348314606741572,"Corollary 1. Consider a GNN on a graph that involves a neuron nS corresponding to a subgraph S
with m vertices. Assume that the input of nS is a matrix T ∈Rm×cin, the rows of which transform
covariantly with permutations of S and cin is the number of channels. Let L be the combinatorial
Laplacian of S, U1, . . . , Up be the eigenspaces of L, and Mi an orthognal basis for the i’th eigenspace
stacked into an Rn×dim(Ui) dimensional matrix. Then for any collection of learnable weight matrices
W1, . . . , Wp ∈Rcin×cout,"
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.21629213483146068,"ϕ: T 7−→ p
X"
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.21910112359550563,"i=1
MiM ⊤
i T Wi
(9)"
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.22191011235955055,is a permutation equivariant linear operation.
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.2247191011235955,"The spectral approach also generalizes to higher order permutation equivariance, in which case we
can take advantage of the more refined two-level subspace structure implied by Theorem 2."
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.22752808988764045,"Theorem 3. Let S, L and the Mi’s be as in Corollary 1. Given a multi-index i = (i1, . . . , ik) ∈
{1, . . . , p}k, we define its type as the tuple n = (n1, . . . , np), where nj is the number of occurrences
of j in i and we define In as the set of all multi-indices of type n. Assume that the input to neuron nS
is a k’th order permutation equivariant tensor T ∈Rm×...×m×cin, as defined in Section 3. For any
given i, define the k’th order eigen-projector
Πi = Pi(M ⊤
i1 ⊗M ⊤
i2 ⊗. . . ⊗M ⊤
ik ⊗I): Rm×...×m×c →Rm×...×m×c"
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.2303370786516854,"where Pi is a permutation map that canonicalizes the form of the projection, as defined in the
Appendix. Let T ◦W denote multiplying T by the matrix W only along its last dimension. Then for
any collection of weight matrices {Wi′,i′ ∈Rcout×cin} the map"
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.23314606741573032,"ϕ: T 7−→
X n X i′∈In X"
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.23595505617977527,"i∈In
Π⊤
i′ (Πi(T ⊙Wi′,i′))"
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.23876404494382023,is a permutation equivariant map.
AUTOMORPHISM INVARIANCE THE SIMPLE WAY VIA SPECTRAL GRAPH THEORY,0.24157303370786518,"Corollary 1 and Theorem 3 give sufficient but not necessary conditions for equivariance w.r.t. the
automorphism group. A discussion of the potential gap between our approach and the full group
theoretical approach described in Section 3 can be found in Appendix D."
EXPERIMENTS,0.2443820224719101,"5
Experiments"
EXPERIMENTS,0.24719101123595505,"To empirically evaluate our Schur layers, we implement the first order case described in Corollary 1
and compare it with other higher order MPNNs. Note that the theorem only gives the equivariant
maps to transform local representation on a subgraph, i.e., ϕ : T in 7→T out where T in ∈Rm×cin and
T out ∈Rm×cout are the inputs and output of a neuron corresponding to a specific subgraph. The rest
of the message passing is conducted in the usual way, in particular, everything is implemented in the
P-tensors framework [21]."
EXPERIMENTS,0.25,"There are several design details about the architecture that are worth mentioning: (1) We chose
cycles of lengths three to eight in most of the experiments and also added branched cycles to show
our algorithm’s scalability. The reason is that in the chemical dataset we used, cycles are the most
important functional group to the property to be predicted, other subgraphs such as m-stars, and
m-paths did not help the performance. (2) While having first-order representation on the cycles,
we also maintain 0’th-order representation (i.e., scalars) on node and edges, as in [6, 21]. These
node and edge representations capture more elementary information about the graph, such as k-hop
neighborhoods, and are still important in higher order MPNNs. The representations pass messages
with each other by intersection rule as defined in P-tensor framework. (3) As discussed in Appendix
G, we view Schur layer’s operation as a spectral convolution filter [5] applied to the subgraph and the
number of channels to indicate how many times it expands the input feature to the output feature. The
codes used to run our experiments can be found at https://github.com/risilab/SchurNet."
SCHUR LAYER IMPROVES OVER LINMAPS,0.25280898876404495,"5.1
Schur layer improves over Linmaps"
SCHUR LAYER IMPROVES OVER LINMAPS,0.2556179775280899,"First of all, we want to show that considering more possible equivariant maps on the subgraph can
indeed boost the performance. To this end, we performed controlled experiments to compare Schur
layer with the equivariant maps w.r.t. Sm (following [21] we called these linmaps). To make a fair
comparison, we use the same architecture including MLPs and message passing defined by P-tensor
and only replace the equivariant maps used in Linmaps by what is defined in Corollary 1. We didn’t
compare with Autobahn [37] because it didn’t use the P-tensor framework in the original paper and
it’s hard for us to implement the convolution w.r.t. automorphism group for all the subgraphs we
chose. However we note that for the case of the cycles (see Table 4), the equivariant maps given by
our approach are equal to that given by the group theoretical approach."
SCHUR LAYER IMPROVES OVER LINMAPS,0.25842696629213485,"We’ll present the results on the commonly used molecular benchmark ZINC-12K [26] dataset in the
main text. Results on TUdatasets as well as runtime comparison can be found in Appendix H. The
task on ZINC-12K is to regress the log P coefficient of each molecule and the Mean absolute error
(MAE) between the prediction and the ground truth is used for evaluation."
SCHUR LAYER IMPROVES OVER LINMAPS,0.2612359550561798,"We design experiments to compare Linmaps and Schur layers in various scenarios, to showcase the
robustness of the improvement. Table 1 shows under various message passing schemes between
edges and cycles, Schur Layer consistently outperforms Linmaps. Those are indications of the
added expressive power of extra equivariant maps in Schur layer, and they’re effective in various
architectural design settings. Another ablation study regarding different cycle sizes can be found in
Appendix H."
SCHUR LAYER IMPROVES OVER LINMAPS,0.2640449438202247,"Layer
Pass message when overlap ≥1
Pass message when overlap ≥2
Linmaps (baseline)
0.074 ± 0.008
0.074 ± 0.005
Schur layer
0.070 ± 0.006
0.071 ± 0.003"
SCHUR LAYER IMPROVES OVER LINMAPS,0.26685393258426965,"Table 1: Comparison between Schur Layer and Linmaps with different message passing schemes.
The message passing scheme is a design choice in P-tensor framework, where the user can set when
two subgraph’s representations communicate. The mostly common use case is to require at least k
vertices in the intersection of two subgraphs for them to communicate. Experiments on ZINC-12k
dataset and all scores are test MAE. Cycle sizes of {3,4,5,6,7,8,11} are used."
SCHUR LAYER IMPROVES OVER LINMAPS,0.2696629213483146,"Then we studied the possibilities of adding Schur layer in different places of higher-order message
passing scheme. In table 2, we observed that the more condensed the higher-order feature is, the
more improvement that the Schur Layer brings to us over Linmaps. We attribute the improvements of
adding/replacing Schur layer in various scenarios over Linmaps the benefit gained from utilizing the"
SCHUR LAYER IMPROVES OVER LINMAPS,0.27247191011235955,"subgraph structure and increased number of equivariant maps. We also tried other ways to use Schur
layer, the result is summarized in Appendix G."
SCHUR LAYER IMPROVES OVER LINMAPS,0.2752808988764045,"Model
Test MAE
Linmaps
0.071 ± 0.004
Simple Schur-Net
0.070 ± 0.005
Linmap Schur-Net
0.068 ± 0.002
Complete Schur-Net
0.064 ± 0.002"
SCHUR LAYER IMPROVES OVER LINMAPS,0.27808988764044945,"Table 2: An experiment demonstrating different ways of using Schur layer. ""Complete Schur Layer""
means that we apply Schur Layer on the incoming messages together with the original cycle represen-
tation. ""Linmap SchuLayer"" means that we just apply the Schur Layer on the aggregated subgraph
representation feature. ""Simple Schur Layer"" means we directly apply Schur Layer on the subgraph
features without any preprocessing. We can observe that as the subgraph information diversifies,
Schur layer tends to decouple the dense information better and results in better performance. The test
MAE of Linmaps in this table is taken from [21]."
FLEXIBILITY,0.2808988764044944,"5.2
Flexibility"
FLEXIBILITY,0.28370786516853935,"The other advantage of Schur layer is that it computes the feature transform only based on the
subgraph’s Laplacian, bypassing a difficult step of finding the automorphisms group and irreps of the
subgraph it acts on. As discussed in the theory, Schur layer constructs equivariant maps only based
on the subgraph’s Laplacian and is applicable directly to any subgraphs, making the implementation
much easier when different subgraphs are chosen than the group theoretical approach. This allows
it to easily extend to any subgraph templates that’re favorable by the user. To demonstrate this,
we augment the subgraphs in the model by all the five and six cycles with one to three branches
(including in total 16 non-isomorphic subgraph templates), comparing with baseline model where
only the cycle itself is considered. Results can be found in Appendix G."
BENCHMARK RESULTS,0.28651685393258425,"5.3
Benchmark results"
BENCHMARK RESULTS,0.2893258426966292,"Finally, and most importantly, we compare the Schur-Net to several other higher-order MPNNs 1
on ZINC-12k dataset and OGB-HIV dataset [23] in table 3. We included baselines of (1) classical
MPNNs: GCN[28], GIN [40], GINE [24], PNA[11], HIMP [16] (2) higher order MPNNs: N 2-GNN
[14] 2, CIN [6], P-tensors [21] (3) Subgraph-GNNs: DS-GNN(EGO+) and DSS-GNN(EGO+) [4],
GNN-AK+ [46], SUN(EGO+)[17] (4) Autobahn [37]."
BENCHMARK RESULTS,0.29213483146067415,"We find that Schur Net ranked second on ZINC-12K and outperformed all other baselines on OGB-
HIV dataset. This shows the expressivity of adding more equivariant maps by leveraging the subgraph
topology. Furthermore, note that while N 2-GNN outperforms Schur Net on ZINC-12K, it’s a second-
order model whereas in our experiment, we only used first-order activation. Also, the partial reason
Autobahn didn’t perform well is in the original implementation, the authors didn’t use P-tensor
framework and used only a part of all possible linear message passing schemes. This shows to get the
full power of equivariant maps w.r.t. subgraph automorphism group, we need to combine it with a
general message passing framework between subgraphs as well."
LIMITATIONS,0.2949438202247191,"6
Limitations"
LIMITATIONS,0.29775280898876405,"Unlike the representation theoretic approach, the spectral approach is not guaranteed the give the finest
possible decomposition into invariant subspaces. Hence, equations like (9) do not necessarily define
the most general possible automorphism-equivariant linear maps. In this paper we did not investigate
from a theoretical point of view the extent of this gap. In general, being able to craft automorphism-
equivariant layers of any order for any types of subgraphs opens up a host of possibilities for making
GNNs more powerful. Our experiments are limited to some of the simplest cases, such as exploiting
cycles and edges. We also only used first order activations."
LIMITATIONS,0.300561797752809,"1A discussion of related work can be found in Appendix C.
2The works [31, 34] don’t have results in those two dataset."
LIMITATIONS,0.30337078651685395,"Model
ZINC-12K MAE(↓)
OGB-HIV ROC-AUC(% ↑)"
LIMITATIONS,0.3061797752808989,"GCN
0.321 ± 0.009
76.07 ± 0.97
GIN
0.408 ± 0.008
75.58 ± 1.40
GINE
0.252 ± 0.014
75.58 ± 1.40
PNA
0.133 ± 0.011
79.05 ± 1.32
HIMP
0.151 ± 0.002
78.80 ± 0.82"
LIMITATIONS,0.3089887640449438,"N 2-GNN
0.059 ± 0.002
-
CIN
0.079 ± 0.006
80.94 ± 0.57
P-tensors
0.071 ± 0.004
80.76 ± 0.82"
LIMITATIONS,0.31179775280898875,"DS-GNN (EGO+)
0.105 ± 0.003
77.40 ± 2.19
DSS-GNN (EGO+)
0.097 ± 0.006
76.78 ± 1.66
GNN-AK+
0.091 ± 0.011
79.61 ± 1.19
SUN (EGO+)
0.084 ± 0.002
80.03 ± 0.55"
LIMITATIONS,0.3146067415730337,"Autobahn
0.106 ± 0.004
78.0 ± 0.30"
LIMITATIONS,0.31741573033707865,"Schur-Net
0.064 ± 0.002
81.6 ± 0.295"
LIMITATIONS,0.3202247191011236,Table 3: Comparison of different models on the ZINC-12K and OGBG-MOLHIV datasets.
CONCLUSIONS,0.32303370786516855,"7
Conclusions"
CONCLUSIONS,0.3258426966292135,"Enforcing equivariance to the automorphism group of subgraphs in higher order neural networks
seemingly requires the use of advanced tools from group representation theory. This is likely a large
part of the reason why automorphism-based architectures such as [37] are not used more commonly
in practical applications. In this paper we have shown that a simpler approach based on spectral graph
theory, following the same underlying logic as the group theoretic approach but bypassing having
to enumerate all irreducible representations of the automorphism group, can lead to an architecture
that is almost as expressive. Our algorithm, called Schur Nets, easily generalizes to higher order
activations, as well as incorporating other types of side information such as vertex labels. In a
practical setting, Schur Nets is easiest to deploy in conjunction with a message passing framework
like P-tensors that hides the complexities of the higher order message passing component. The
empirical performance of Schur Nets on the ZINC 12K dataset is superiror to all other comparable
(non-transformer based) architectures that we are aware of."
CONCLUSIONS,0.32865168539325845,"Given the similarity between the way we utilize the eigenspaces of the graph Laplacian and the
so-called graph Fourier transform, our approach exposes heretofore unexplored connections between
permutation equivariance and spectral GNNs such as [9, 22]. It also highlights the fact that while
permutation equivariance is a fundamental constraint on graph neural networks, the key to building
high performing, expressive GNNs is to reduce the size of the group that the network needs to be
equivariant to as much as possible, using whatever side-information we can employ, whether that be
the adjacency matrix, vertex degrees or something else."
CONCLUSIONS,0.33146067415730335,Acknowledgements
CONCLUSIONS,0.3342696629213483,"We would like to thank Andrew Hands for many valuable pieces of advice and much practical
assistance that he has given to the experimental side of this project. We also gratefully acknowledge
the Toyota Technological Institute of Chicago and the Data Science Institute at the University of
Chicago for making their computational resources available for our use, as well as NSF MRI-1828629
for additional infrastructure that was used in the course in this project."
REFERENCES,0.33707865168539325,References
REFERENCES,0.3398876404494382,"[1] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to Represent
Programs with Graphs. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.34269662921348315,"[2] Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik.
Subgraph Neural
Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.3455056179775281,"[3] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, and et al. Relational Inductive Biases,
Deep Learning, and Graph Networks. In arXiv:1806.01261, 2018."
REFERENCES,0.34831460674157305,"[4] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai,
Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant Subgraph
Aggregation Networks. In International Conference on Learning Representations (ICLR), 2022."
REFERENCES,0.351123595505618,"[5] Deyu Bo, Xiao Wang, Yang Liu, Yuan Fang, Yawen Li, and Chuan Shi. A Survey on Spectral
Graph Neural Networks. arXiv preprint arXiv:2302.05631, 2023."
REFERENCES,0.3539325842696629,"[6] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Liò, Guido Montufar, and
Michael Bronstein. Weisfeiler and Lehman Go Cellular: CW Networks. In Advances in Neural
Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.35674157303370785,"[7] Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F Montufar, Pietro Lió,
and Michael Bronstein. Weisfeiler and Lehman go topological: Message passing simplicial
networks. In International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.3595505617977528,"[8] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving
Graph Neural Network Expressivity via Subgraph Isomorphism Counting. In International
Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.36235955056179775,"[9] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral Networks and Locally
Connected Networks on Graphs. In International Conference on Learning Representations
(ICLR), 2014."
REFERENCES,0.3651685393258427,"[10] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can Graph Neural Networks Count
Substructures? In Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.36797752808988765,"[11] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Velickovic. Principal
Neighbourhood Aggregation for Graph Nets. In Advances in Neural Information Processing
Systems (NeurIPS), 2020."
REFERENCES,0.3707865168539326,"[12] Pim de Haan, Taco S. Cohen, and Max Welling. Natural Graph Networks. In Advances in
Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.37359550561797755,"[13] Nima Dehmamy, Albert-László Barabási, and Rose Yu. Understanding the Representation Power
of Graph Neural Networks in Learning Graph Topology. In Advances in Neural Information
Processing Systems (NeurIPS), 2019."
REFERENCES,0.37640449438202245,"[14] Jiarui Feng, Lecheng Kong, Hao Liu, Dacheng Tao, Fuhai Li, Muhan Zhang, and Yixin Chen.
Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-
Lehman. In Advances in Neural Information Processing Systems (NeurIPS), 2023."
REFERENCES,0.3792134831460674,"[15] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph Neural
Networks. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2019."
REFERENCES,0.38202247191011235,"[16] Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical Inter-Message Passing for
Learning on Molecular Graphs. In ICML Graph Representation Learning and Beyond (GRL+)
Workhop, 2020."
REFERENCES,0.3848314606741573,"[17] Fabrizio Frasca, Beatrice Bevilacqua, Michael M. Bronstein, and Haggai Maron. Understanding
and Extending Subgraph GNNs by Rethinking Their Symmetries. In Advances in Neural
Information Processing Systems (NeurIPS), 2022."
REFERENCES,0.38764044943820225,"[18] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
Message Passing for Quantum Chemistry. International Conference on Machine Learning
(ICML), 2017."
REFERENCES,0.3904494382022472,"[19] Martin Grohe. Descriptive Complexity, Canonisation, and Definable Graph Structure Theory.
Cambridge University Press, 2017."
REFERENCES,0.39325842696629215,"[20] Rafael Gómez-Bombarelli, Jennifer N. Wei, David Duvenaud, José Miguel Hernández-Lobato,
Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel,
Ryan P. Adams, and Alán Aspuru-Guzik. Automatic Chemical Design Using a Data-Driven
Continuous Representation of Molecules. ACS Central Science, 2018.
[21] Andrew R. Hands, Tianyi Sun, and Risi Kondor. P-Tensors: A General Framework for Higher
Order Message Passing in Subgraph Neural Networks. In Proceedings of The 27th International
Conference on Artificial Intelligence and Statistics (AISTATS), 2024.
[22] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep Convolutional Networks on Graph-
Structured Data. arXiv:1506.05163, 2015.
[23] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele
Catasta, and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs.
In Advances in Neural Information Processing Systems (NeurIPS), 2020.
[24] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure
Leskovec. Strategies for Pre-Training Graph Neural Networks. In International Conference on
Learning Representations (ICLR), 2020.
[25] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training
by Reducing Internal Covariate Shift. In International Conference on Machine Learning (ICML),
2015.
[26] John J. Irwin, Khanh G. Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R. Wong,
Munkhzul Khurelbaatar, Yurii S. Moroz, John Mayfield, and Roger A. Sayle. ZINC20—A Free
Ultrlarge-Scale Chemical Database for Ligand Discovery. Journal of Chemical Information
and Modeling, 2020.
[27] Diederik P. Kingma and Jimmy Ba.
Adam: A Method for Stochastic Optimization.
In
International Conference on Learning Representations (ICLR), 2015.
[28] T. N. Kipf and M. Welling. Semi-Supervised Classification with Graph Convolutional Networks.
In International Conference on Learning Representations (ICLR), 2017.
[29] Risi Kondor and Shubhendu Trivedi. On the Generalization of Equivariance and Convolution in
Neural Networks to the Action of Compact Groups. In International Conference on Machine
Learning (ICML), 2018.
[30] Andreas Loukas. How Hard is it to Distinguish Graphs with Graph Neural Networks? In
Advances in Neural Information Processing Systems (NeurIPS), 2020.
[31] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably Powerful
Graph Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[32] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and Equivariant
Graph Networks. In International Conference on Learning Representations (ICLR), 2019.
[33] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion
Neumann. TUDataset: A Collection of Benchmark Datasets for Learning with Graphs. ICML
Graph Representation Learning and Beyond (GRL+) Workhop, 2020.
[34] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen,
Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman Go Neural: Higher-Order Graph
Neural Networks. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
2019.
[35] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Approximation Ratios of Graph Neural
Networks for Combinatorial Problems. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
[36] Jean-Pierre Serre. Linear Representations of Finite Groups. Springer-Verlag, 1977.
[37] Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: Automorphism-Based Graph Neural
Nets. In Advances in Neural Information Processing Systems (NeurIPS), 2021.
[38] Erik Henning Thiede, Truong-Son Hy, and Risi Kondor. The General Theory of Permutation
Equivariant Neural Networks and Higher Order Graph Variational Encoders. arXiv:2004.03990,
2020.
[39] Boris Weisfeiler and Andrei Leman. The Reduction of a Graph to a Canonical Form and the
Algebra Which Appears Therein. Nauchno-Technicheskaya Informatsiya, 1968."
REFERENCES,0.3960674157303371,"[40] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural
Networks. In International Conference on Learning Representations (ICLR), 2019.
[41] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and
Partha Talukdar. HyperGCN: A New Method for Training Graph Convolutional Networks on
Hypergraphs. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
[42] Jiaxuan You, Jonathan Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-Aware Graph
Neural Networks. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
2021.
[43] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R. Salakhutdinov,
and Alexander J. Smola. Deep Sets. In Advances in Neural Information Processing Systems
(NeurIPS), 2017.
[44] Muhan Zhang and Pan Li. Nested Graph Neural Networks. In Advances in Neural Information
Processing Systems (NeurIPS), 2021.
[45] Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-SAGNN: A Self-Attention Based Graph
Neural Network for Hypergraphs. In International Conference on Learning Representations
(ICLR), 2020.
[46] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From Stars to Subgraphs: Uplifting Any
GNN with Local Structure Awareness. In International Conference on Learning Representations
(ICLR), 2022."
REFERENCES,0.398876404494382,"A
Additional details"
REFERENCES,0.40168539325842695,"The purpose of the permutation map Pi in Theorem 3 is to remap the tensor product space
Rdim(Ui1)×...×dim(Uik )×C into a canonical form so that those dimensions that correspond to ij = 1
are mapped to the first n1 slots, those dimensions with ij = 2 are mapped to the second n2 slots, and
so on. Given the permutation τ : {1, 2, . . . , k} →{1, 2, . . . , k} that canonicalizes the indices in this
way, Pi is just the corresponding permutation map, i.e.,"
REFERENCES,0.4044943820224719,Pi(x1 ⊗. . . xk ⊗xc) = xτ(1) ⊗. . . xτ(k) ⊗xc.
REFERENCES,0.40730337078651685,"B
Proofs"
REFERENCES,0.4101123595505618,"Proof of Theorem 1. Let {g↓Ui : Ui →Ui}g∈G be the restriction of the action of G to Ui. Since G
fixes each Ui, the action of G on the full space decomposes in the form"
REFERENCES,0.41292134831460675,"g(u) = p
X"
REFERENCES,0.4157303370786517,"i=1
g↓ui(u↓Ui)
u ∈U."
REFERENCES,0.41853932584269665,"Now since multiplication by scalars commutes with linear maps,"
REFERENCES,0.42134831460674155,"ϕ(g(u)) = p
X"
REFERENCES,0.4241573033707865,"i=1
αi g↓ui(u↓Ui) = p
X"
REFERENCES,0.42696629213483145,"i=1
g↓ui(αi u↓Ui) = g(ϕ(u))"
REFERENCES,0.4297752808988764,"for any u ∈U and any g ∈G.
■"
REFERENCES,0.43258426966292135,"Proof of Theorem 2. Each of the V i
j subspaces is invariant, hence a homothety on each is an
equivariant operation for the same reason as in Theorem 1. In addition, since G acts the same way
on any pair of subspaces (V i
j , V i
j′), any map of the form ξi
j,j′ : V i
j V i
j′ that is just a scaling is also
equivariant. The composition of equivariant linear maps is an equivariant linear map, hence any map
ϕ of the given form is equivariant.
■"
REFERENCES,0.4353932584269663,"Proof of Theorem 3. For any multi-index i, the subspace Im(M ⊤
i1 ⊗M ⊤
i2 ⊗. . . ⊗M ⊤
ik ⊗Ic) is
invariant to permutations. Further after applying the permutation map Pi, all such subspaces of the
same type n, the action of Sk on all such subspaces will be the same. Hence we can directly apply
Theorem 2 to prove this theorem.
■"
REFERENCES,0.43820224719101125,"B.1
Alternative Proof for Corollary 1"
REFERENCES,0.4410112359550562,Here we give a more direct proof for Corollary 1.
REFERENCES,0.4438202247191011,"Proof. Without loss of generality, we assume cin = cout = 1.
Since the Mi’s and λi’s are
eigenspaces and eigenvectors of the Laplacian L, L = MΛM T , where M = [M1, · · · , Mp] and
Λ = diag(λ1, · · · , λm)."
REFERENCES,0.44662921348314605,"The neuron nS is a linear transform ϕ : Rm →Rm, with ϕ(T) = Σp
i MiM T
i TWi = MV M T T,
where V = diag(W1In1, · · · , WpInp). Here the Wi’s are just scalars and Ini is an identity matrix of
the same size as the corresponding eigenspace."
REFERENCES,0.449438202247191,"Given a permutation σ ∈Sn, we want to show that ϕσ(σ ◦T) = σ ◦ϕe(T), which is the definition
of equivariance. Note that ϕσ actually depends on σ, since we’re doing eigenvalue decomposition on
the Laplacian of the subgraph transformed by σ. This is the key to proving equivariance. We use e to
denote the identity permutation."
REFERENCES,0.45224719101123595,"Now Lσ = PσLP T
σ = MσΛM T
σ , where Mσ = PσM , while the eigenvalues remain invariant after
permutation and eigenspaces are transformed in the same manner as L. Thus,"
REFERENCES,0.4550561797752809,"ϕσ(σ ◦T) = MσV M ⊤
σ PσT"
REFERENCES,0.45786516853932585,"= PσMV M ⊤
σ P ⊤
σ PσT
= Pσ ϕe(T)"
REFERENCES,0.4606741573033708,which is the desired result.
REFERENCES,0.46348314606741575,"Remark 1. The key to the proof is that the transform ϕ depends on an object transformed with
permutation σ, specifically. graph Laplacian L. And all we need is that the object L is transformed
equivariantly with σ. We can also add side informatino such as node labels or degrees to further
constrain the automorphism group and increase the number of distinct eigenvalues. One easy way to
do that is to set L′ = L + V , where V = diag(v1, · · · , vm) captures the node labels or degrees, and
build the Schur neuron nS from L′."
REFERENCES,0.46629213483146065,"C
Related works"
REFERENCES,0.4691011235955056,"Higher order MPNNs have become a popular research area in graph representation learning since
the seminal work by [32], which characterized the space of equivariant map w.r.t. Sn for k’th order
tensors. The following works have mainly been divided into two streams, the one is based on
k-Weisfeiler Leman test (k-WL test) [39, 19], for instances, [31] proposed k-order graph neural
networks that is as powerful as k-WL test; [34] proposed k-GNN to approximate (or simulate) k-WL
test; [14] further designed a (k, t)-FWL+ hierarchy that extends the k-WL test and implemented the
corresponding neuron version. A common feature of these approaches is they all work on all k-tuples
of vertices for k-th order neural networks and thus make their space and time complexity at least
θ(nk)."
REFERENCES,0.47191011235955055,"The other line of work is seeking ways to choose subgraphs in the graph, that are representative
or contain important information, such as functional groups in a molecule. [7] chooses simplicial
complex and [6] further extends this to any cell complex such as cycles. [21] incorporates any
subgraph template and uses the equivariant maps defined [32] for local feature transform and devised
a high order message passing scheme between those subgraphs, called P-tensor. This is arguably the
most general framework in this line of work. This line of work is kind of independent of the k-WL
test since the k-WL test is aimed to distinguish any pair of non-isomorphic graphs while chosen
subgraphs are meant to work in some specific regions with domain prior. It’s possible to still compare
this kind of network with the distinguishing power of k-WL test, for example, [6] shows if they
include cycles of size at most k (including nodes and edges), their network are as powerful as WL
test. However, such comparisons are not very meaningful to indicate the expressive power of such a
domain-specific approach. In our opinion, the ability to learn certain features (such as count cycles of
certain sizes or learn some function related to specific functional groups) might be a better criterion.
It’ll be an interesting research direction to design suitable criteria for the expressive power of such
higher order MPNNs in general."
REFERENCES,0.4747191011235955,"Our work follows this line of work since we’re choosing specific subgraphs to learn representation
on. But we utilize the subgraph’s automorphism group to devise more possible equivariant maps
(none of the aforementioned methods are built on the automorphism group). The only other work to
our knowledge that uses the automorphism group is Autobahn [37], which is the first to introduce
equivariance to the automorphism group to GNNs. The difference between our work and it lines
twofold: (1) Autobahn theoretically introduces equivariant maps w.r.t. automorphism group via gen-
eralized convolution and Cosets, which is another group theoretical approach to construct equivariant
maps, while we’re using the decomposition to irreps (2) More importantly, Autobahn didn’t mention
a practical approach to construct those equivariant maps explicitly, hindering the utilization of more
diverse subgraphs. In their experiments, they only use cycles of length five and six and paths of length
three to six and construct the equivariant maps potentially by hand. In contrast, we give a simple and
efficient way to construct equivariant maps w.r.t. automorphism group by EVD, which is very general
to be used by any subgraph. Though our approach doesn’t give the full set of equivariant maps,
experiments show improvement over the traditional approach where only equivariant maps w.r.t. Sn
are used. We believe this algorithm, together with the group theoretical idea about decomposition
Rmk into stable subspaces and irreps are beneficial to the research community."
REFERENCES,0.47752808988764045,"There is a ""third"" type of higher order GNNs that is called Subgraph GNN, which deviates from the
original definition of higher order MPNNs but can still be considered as higher order network. In
particular, node-based GNNs such as [44, 46, 4, 42, 17] associate each node with a subgraph (by
deleting the node, marking the node or extracting the EGO-network) and do MPNN on each subgraph,"
REFERENCES,0.4803370786516854,"where they get the resulting representation X ∈Rn×n that can be considered as a 2nd order tensor
representation on the original graph."
REFERENCES,0.48314606741573035,"D
Analysis of Schur layer"
REFERENCES,0.4859550561797753,"First of all, we notice that the equivariant map characterized in 1 (or 3) may not be the full set
of equivariant maps w.r.t. AutS. Because (1) In the decomposition of U = Rmk to eigenspaces
U = U1 ⊕. . . Ut, while Ui is stable subspaces, it might not be irreducible and finer decomposition
may be possible. In other words, there might be two irreps (isomorphic or not) corresponding to
the same eigenvalue. (2) The maps defined by 1 didn’t take into account the isomorphic subspaces
corresponding to the same type of irrep, thus ignored the possible equivariant maps between V i
j and
V i
j′. So, it is of interest to find out how much the gap would be between our approach and the group
theoretical approach. We first look at some examples in the first-order case (see table 4)."
REFERENCES,0.4887640449438202,"Graph
AutS
# of distinct Eigenvalues
(Schur Layer)"
REFERENCES,0.49157303370786515,"P
i(κi)2
irreps approach P i κi"
-CYCLE,0.4943820224719101,"6-cycle
D6
4
4
4
5-cycle
D5
3
3
3
4-cycle
D4
3
3
3
3-cycle
D3
2
2
2
5-star
S4
3
5
3
4-star
S3
3
5
3
3-path
S2
3
5
3
n-cliques
Sn
2
2
2
5-cycle
with one branch
S2
6
20
6"
"-CYCLE
WITH ONE BRANCH",0.49719101123595505,"6-cycle
with one branch
S2
7
29
7"
"-CYCLE
WITH ONE BRANCH",0.5,"Table 4: Examples on EVD approach vs group theoretical approach towards # of equivariant maps
w.r.t. AutS. To calculate the decomposition of Rm into irreps, one can calculate κi = (ϕ|χi) where
ϕ and χi is the character of the first-order action and ith irrep respectively, and (|) is inner product.
Another quick approach is to use ϕ(σ) = P"
"-CYCLE
WITH ONE BRANCH",0.5028089887640449,"k κkχk(σ) and look at some examples of σ to determine
what the κk should be."
"-CYCLE
WITH ONE BRANCH",0.5056179775280899,"We note that for cycles in the graph case, there’s no gap. However, when we add branches to the
cycle to make the automorphism group smaller, the multiplicities of the irreps increase, e.g., in
5-cycle with one branch case, S2 only have two 1-dimensional irreps: trivial and sign representation
of permutation, and the first-order action decompose to 4 copies of trivial and 2 copies of sign
representation, gives in total 4 ∗4 + 2 ∗2 = 20 possible equivariant maps. However, note that # of
irreps (counting multiplicities given by P"
"-CYCLE
WITH ONE BRANCH",0.5084269662921348,"i κi) is equal to # of distinct eigenvalues, meaning that
each eigenspace corresponds to an irreducible subspace and the decomposition of Rm provided by
eigenspaces is indeed a decomposition to the irreps in all of the cases listed in the table. Therefore,
the multiplicities are the key reason for the gap between our EVD approach and the group theoretical
approach, since our EVD approach can’t capture the isomorphic property between subspaces. In
principle, it is possible to find out which eigenspace is isomorphic to which, but in our current
implementation, we didn’t take this into account because we found out that only considering cycles is
enough for a very good performance in the datasets we used."
"-CYCLE
WITH ONE BRANCH",0.5112359550561798,"Generally, it’s complicated to determine the gap between our EVD approach and the group theoretic
approach, especially in higher-order cases (where merely determining the κi’s is tricky), so we leave
this into feature exploration."
"-CYCLE
WITH ONE BRANCH",0.5140449438202247,"E
A brief overview about P-tensor framework"
"-CYCLE
WITH ONE BRANCH",0.5168539325842697,"The P-tensor framework is a framework for linear equivariant maps w.r.t. Sn both between the same
subgraph and across different subgraphs. It is built on the equivariant maps characterized by [32]."
"-CYCLE
WITH ONE BRANCH",0.5196629213483146,"Definition 1 (P-tensors). Let U be a finite set of atoms and D = (x1, . . . , xd) an ordered subset of
U. We say that a k-th order tensor T ∈Rd×d×···×d is a k-th order permutation covariant tensor (or
P-tensor for short) with reference domain D if under reordering by τ ∈Sd D it transforms to"
"-CYCLE
WITH ONE BRANCH",0.5224719101123596,"[τ ◦T]i1,i2,...,ik = Tτ −1(i1),...,τ −1(ik)."
"-CYCLE
WITH ONE BRANCH",0.5252808988764045,"E.1
Message passing between P-tensors with the same reference domain"
"-CYCLE
WITH ONE BRANCH",0.5280898876404494,"Consider the equivariant maps sending T on D to T out on D. The space of equivariant maps w.r.t Sm
is characterized by [32]:
Proposition 1. The space of linear maps ϕ : Rdk1 →Rdk2 that is equivariant to permutations
τ ∈Sd is spanned by a basis indexed by the partitions of the set {1, 2, . . . , k1 + k2}."
"-CYCLE
WITH ONE BRANCH",0.5308988764044944,"Then the authors designed a straightforward way to write the maps explicitly. Specifically, for each
partition of the set {1, 2, . . . , k1 + k2}, there are three parts that determines the equivariant map:
(1) summing over specific dimensions or diagonals of T in (2) transferring T in to T out by identifying
indices of T in with indices of T out (3) broadcasting the result along certain dimensions of T out. These
three operations correspond to the three different types of sets that can occur in a given partition P:
(1) those that only involve the second k2 numbers, (2) those that involve a mixture of the first k1 and
k2 and (3) those only involve the first k1 numbers. The type (1) - (3) corresponds to type (1) - (3) of
operations with the dimensions in the sets."
"-CYCLE
WITH ONE BRANCH",0.5337078651685393,"For example, in the case k1 = k2 = 3, the P = {{1, 3}, {2, 5, 6}, {4}} partition corresponds to (a)
summing T in along its first dimension (corresponding to {4}) (b) transferring the diagonal along the
second and third dimension of T in to the second dimension of T out (corresponding to {2, 5, 6}) (c)
broadcasting the result along the diagonal of the first and third dimensions (corresponding to {1, 3}).
Explicitly, this gives the equivariant map:"
"-CYCLE
WITH ONE BRANCH",0.5365168539325843,"T out
a,b,a =
X"
"-CYCLE
WITH ONE BRANCH",0.5393258426966292,"c
T in
c,b,b"
"-CYCLE
WITH ONE BRANCH",0.5421348314606742,See table 5 for another example for k1 = k2 = 2 case.
"-CYCLE
WITH ONE BRANCH",0.5449438202247191,"P
ϕ
P
ϕ
{{1}, {2}, {3}, {4}}
T out
a,b = P"
"-CYCLE
WITH ONE BRANCH",0.547752808988764,"c,d T in
c,d
{{2}, {1,3,4}}
T out
b,a = T in
b,b
{{1}, {2}, {3,4}}
T out
a,b = P"
"-CYCLE
WITH ONE BRANCH",0.550561797752809,"c T in
c,c
{{1,2,3}, {4}}
T out
a,a = P"
"-CYCLE
WITH ONE BRANCH",0.5533707865168539,"b T in
a,b
{{1}, {2,4}, {3}}
T out
a,b = P"
"-CYCLE
WITH ONE BRANCH",0.5561797752808989,"c T in
c,b
{{1,2,4}, {3}}
T out
a,a = P"
"-CYCLE
WITH ONE BRANCH",0.5589887640449438,"b T in
b,a
{{1}, {2,3}, {4}}
T out
a,b = P"
"-CYCLE
WITH ONE BRANCH",0.5617977528089888,"c T in
b,c
{{1,2}, {3,4}}
T out
a,a = P"
"-CYCLE
WITH ONE BRANCH",0.5646067415730337,"c T in
c,c
{{2}, {1,4,3}}
T out
b,a = T in
b,b
{{1,3}, {2,4}}
T out
a,b = T in
a,b
{{1,3}, {2,4}}
T out
b,a = P"
"-CYCLE
WITH ONE BRANCH",0.5674157303370787,"c T in
c,b
{{1,4}, {2,3}}
T out
b,a = T in
b,a
{{1,2,3,4}}
T out
a,a = T in
a,a
{{1,2,3,4}}
T out
a,a = T in
a,a
Table 5: The B(4) = 15 possible partitions of the set {1, 2, 3, 4} and the corresponding permutation
equivariant linear maps ϕ : Rk×k →Rk×k."
"-CYCLE
WITH ONE BRANCH",0.5702247191011236,"E.2
Message passing between P-tensors with the different reference domains"
"-CYCLE
WITH ONE BRANCH",0.5730337078651685,"If T in has reference domain D1 and T out has D2, with D1 ̸= D2 and D1 ∩D2 ̸= ∅. We could have
more options corresponding to summing either over the intersection or over D1, and broadcasting
either over the intersection or over D2. So the number of maps corresponging to partition of type
(p1, p2, p3) is 2(p1+p3). Let D1 ∩D2 = d∩, the previous example would have the following maps in
D1 ̸= D2 case:"
"-CYCLE
WITH ONE BRANCH",0.5758426966292135,"T out
a,b,a = (Pd∩"
"-CYCLE
WITH ONE BRANCH",0.5786516853932584,"c=1 T in
c,b,b
a, b ≤d∩"
"-CYCLE
WITH ONE BRANCH",0.5814606741573034,"0
otherwise,
(a ∈{1, . . . , d∩}, c ∈{1, . . . , d∩})"
"-CYCLE
WITH ONE BRANCH",0.5842696629213483,"T out
a,b,a = (Pd∩"
"-CYCLE
WITH ONE BRANCH",0.5870786516853933,"c=1 T in
c,b,b
b ≤d∩"
"-CYCLE
WITH ONE BRANCH",0.5898876404494382,"0
otherwise,
(a ∈{1, . . . , d2}, c ∈{1, . . . , d∩})"
"-CYCLE
WITH ONE BRANCH",0.5926966292134831,"T out
a,b,a ="
"-CYCLE
WITH ONE BRANCH",0.5955056179775281,"(Pd1
c=1 T in
c,b,b
a, b ≤d∩"
"-CYCLE
WITH ONE BRANCH",0.598314606741573,"0
otherwise,
(a ∈{1, . . . , d∩}, c ∈{1, . . . , d1})"
"-CYCLE
WITH ONE BRANCH",0.601123595505618,"T out
a,b,a ="
"-CYCLE
WITH ONE BRANCH",0.6039325842696629,"(Pd1
c=1 T in
c,b,b
b ≤d∩"
"-CYCLE
WITH ONE BRANCH",0.6067415730337079,"0
otherwise,
(a ∈{1, . . . , d2}, c ∈{1, . . . , d1})"
"-CYCLE
WITH ONE BRANCH",0.6095505617977528,"F
Group Representation Theory Background"
"-CYCLE
WITH ONE BRANCH",0.6123595505617978,"Here, we explain in detail the decomposition of k-th order permutation to irreps of Sm."
"-CYCLE
WITH ONE BRANCH",0.6151685393258427,"Proposition 2. Let ρλ, λ ⊢m be the irreps of Sm, U = Rmk, σ ∈Sm act on U in the manner of
equation 7, and suppose this action contains irreps ρλ1, . . . , ρλp with multiplicities κ1, . . . , κp, we
have:"
"-CYCLE
WITH ONE BRANCH",0.6179775280898876,"U = U1 ⊕U2 ⊕· · · ⊕Up,
Ui = κi
M"
"-CYCLE
WITH ONE BRANCH",0.6207865168539326,"j
V j
i"
"-CYCLE
WITH ONE BRANCH",0.6235955056179775,"where the first part gives the canonical decomposition of U and the second part further decompose
each Ui into irreducible subspaces, where V j
i correspond to ρλi. In matrix form,"
"-CYCLE
WITH ONE BRANCH",0.6264044943820225,"P (k)
σ
= M  

"
"-CYCLE
WITH ONE BRANCH",0.6292134831460674,"ρλ1(σ)
0
· · ·
0
0
ρλ2(σ)
· · ·
0
...
...
...
...
0
0
· · ·
ρλp(σ) "
"-CYCLE
WITH ONE BRANCH",0.6320224719101124,"

M T ,
for all σ ∈Sm"
"-CYCLE
WITH ONE BRANCH",0.6348314606741573,"where M is the orthogonal transformation for basis and we abuse ρλi(σ) to denote also the matrix
of i-th irrep in the decomposition."
"-CYCLE
WITH ONE BRANCH",0.6376404494382022,"If we take a closer look at M, we can find that since P (k)
σ
is under the standard basis of Rmk,
thus M is just the orthonormal basis of each Ui (thus each V j
i ) combined together, we denote
M = (M1, . . . , Mp) with Mi a mk × (dλi ∗κi) dimensional matrix, where dλi is the degree of ρλi.
Therefore, P (k)
σ T becomes:"
"-CYCLE
WITH ONE BRANCH",0.6404494382022472,"P (k)
σ T = [M1, M2, . . . , Mp]  

"
"-CYCLE
WITH ONE BRANCH",0.6432584269662921,"ρλ1(σ)
0
· · ·
0
0
ρλ2(σ)
· · ·
0
...
...
...
...
0
0
· · ·
ρλp(σ)  

 "
"-CYCLE
WITH ONE BRANCH",0.6460674157303371,"


"
"-CYCLE
WITH ONE BRANCH",0.648876404494382,"M T
1
M T
2...
M T
p "
"-CYCLE
WITH ONE BRANCH",0.651685393258427,"


T"
"-CYCLE
WITH ONE BRANCH",0.6544943820224719,"|
{z
}
(1)
|
{z
}
2 (10)"
"-CYCLE
WITH ONE BRANCH",0.6573033707865169,The part (1) is a generalized Fourier transform of T to its Fourier components (coordinates under
"-CYCLE
WITH ONE BRANCH",0.6601123595505618,the orthonormal basis) ˆT = 
"-CYCLE
WITH ONE BRANCH",0.6629213483146067,"


"
"-CYCLE
WITH ONE BRANCH",0.6657303370786517,"M T
1 T
M T
2 T
...
M T
p T "
"-CYCLE
WITH ONE BRANCH",0.6685393258426966,"


≜  

"
"-CYCLE
WITH ONE BRANCH",0.6713483146067416,"B1
B2
...
Bp "
"-CYCLE
WITH ONE BRANCH",0.6741573033707865,"

, and the part (2) is the irreps ρλ1, . . . , ρλp act"
"-CYCLE
WITH ONE BRANCH",0.6769662921348315,"independently on ˆT with each component Bi 7→  
"
"-CYCLE
WITH ONE BRANCH",0.6797752808988764,"ρλi(σ)
· · ·
0
...
...
...
0
· · ·
ρλi(σ) "
"-CYCLE
WITH ONE BRANCH",0.6825842696629213,"
Bi. Note that there’re κi"
"-CYCLE
WITH ONE BRANCH",0.6853932584269663,"multiple of ρλi act the same on components of Bi correspond to V j
i , with a slight abuse of notation,
we can think Bi ∈Rdλi×κi (instead of R(dλi∗κi)×1) and write this map as ρλi(σ)Bi, the fact that
the map by irrep ρλi act independently on each column of Bi allow us to identify directly a set of
equivariant maps by multiplying Bi with matrix Wi ∈Rκi×κi to the right, and using associativity of
matrix multiplication: ρλi(σ)(BiWi) = (ρλi(σ)Bi)Wi. This gives in total of P"
"-CYCLE
WITH ONE BRANCH",0.6882022471910112,"i κ2
i independent
equivariant maps, as stated in the main text. The last part of the above equation maps Fourier
components to their original space. In short, we can write:"
"-CYCLE
WITH ONE BRANCH",0.6910112359550562,"P (k)
σ T =
X"
"-CYCLE
WITH ONE BRANCH",0.6938202247191011,"i
Miρλi(σ) M T
i T
| {z }
Bi (11)"
"-CYCLE
WITH ONE BRANCH",0.6966292134831461,with the abuse of notation to rearrange elements in Bi mentioned above.
"-CYCLE
WITH ONE BRANCH",0.699438202247191,Then we present a detailed version of theorem 2 in the main text.
"-CYCLE
WITH ONE BRANCH",0.702247191011236,"Theorem 4 (Necessary and sufficient condition for equivariant map). Let G be a finite group acting
on a vector space U by the linear action {g : U →U}g∈G and assume the action can be decomposed
into irreps ρ1, . . . , ρp with multiplicities κ1, . . . , κp and degree di:"
"-CYCLE
WITH ONE BRANCH",0.7050561797752809,"U = U1 ⊕U2 ⊕· · · ⊕Up,
Ui = κi
M"
"-CYCLE
WITH ONE BRANCH",0.7078651685393258,"j=1
V j
i"
"-CYCLE
WITH ONE BRANCH",0.7106741573033708,Then ϕ : U →U is an equivariant map w.r.t. this group action if and only if ϕ is of the form:
"-CYCLE
WITH ONE BRANCH",0.7134831460674157,"ϕ(v) =
X"
"-CYCLE
WITH ONE BRANCH",0.7162921348314607,"j′
αi
j,j′τ i
j→j′(v)
for v ∈V i
j
(12)"
"-CYCLE
WITH ONE BRANCH",0.7191011235955056,"for some fixed set of coefficient {αi
j,j′ : i ∈[1, . . . , p], j, j′ ∈[1, . . . , κi]}.
In matrix form, suppose the matrix of g is Rg under basis (ei) and dim(U) = n, and it decompose
into:"
"-CYCLE
WITH ONE BRANCH",0.7219101123595506,"Rg = M  

"
"-CYCLE
WITH ONE BRANCH",0.7247191011235955,"ρ1(g)
0
· · ·
0
0
ρ2(g)
· · ·
0
...
...
...
...
0
0
· · ·
ρp(g) "
"-CYCLE
WITH ONE BRANCH",0.7275280898876404,"

M T
(13) =
X"
"-CYCLE
WITH ONE BRANCH",0.7303370786516854,"i
Miρi(g)M T
i
(14)"
"-CYCLE
WITH ONE BRANCH",0.7331460674157303,Then ϕ : Rn →Rn is equivariant if and only if it is of the form:
"-CYCLE
WITH ONE BRANCH",0.7359550561797753,"ϕ(T) = MM T T  

"
"-CYCLE
WITH ONE BRANCH",0.7387640449438202,"W1
0
· · ·
0
0
W2
· · ·
0
...
...
...
...
0
0
· · ·
Wp "
"-CYCLE
WITH ONE BRANCH",0.7415730337078652,"


(15) =
X"
"-CYCLE
WITH ONE BRANCH",0.7443820224719101,"i
Mi M T
i T
| {z }
Bi"
"-CYCLE
WITH ONE BRANCH",0.7471910112359551,"Wi
(16)"
"-CYCLE
WITH ONE BRANCH",0.75,"where T ∈Rn, Wi ∈Rκi×κi is the fixed coefficients and Bi rearrange to Rdi×κi when needed. The
coefficients αi
j,j′ corresponds to (Wi)j,j′. 3"
"-CYCLE
WITH ONE BRANCH",0.7528089887640449,"Proof. Sufficiency: firstly, αi
j,j′τ i
j→j′(v) is equivariant by definition of isomorphism map. And the
equivariance of ϕ follows from the fact that sum of equivariant maps are equivariant."
"-CYCLE
WITH ONE BRANCH",0.7556179775280899,"Necessity: consider ϕ on V i
j and let W = Im(ϕ|V i
j ) the image of ϕ on V i
j . Since ϕ ◦g = g ◦ϕ and
g(v) ∈V i
j for any v ∈V i
j , we have g(ϕ(v)) = ϕ(g(v)) ∈W, so W is stable under the action. Thus
we can decompose W into irreducible spaces W = W1 ⊕. . . Wp. Apply this decomposition to ϕ(v)
for v ∈V i
j , we get:
ϕ(v) = ϕ1(v) + . . . + ϕp(v)
where ϕi(v) ∈Wi
In other words ϕk = Projk ◦ϕ where k is the projection of W onto Wk (uniquely defined by the
direct sum decomposition). Then ϕk : V i
j →Wk and ϕk ◦g = g ◦ϕk. By Schur’s lemma [36], for
ϕk ̸= 0, we must have Wk isomorphic to V i
j and ϕk(v) = θkτV i
j ,Wk(v). Since only V i
j′ is isomorphic
to V i
j , we have Wk = V i
k and ϕk(v) = αi
j,kτj,k(v), where τj,k(v) is the isomorphic map sending V i
j
to V i
k. Thus ϕ(v) = P"
"-CYCLE
WITH ONE BRANCH",0.7584269662921348,k ϕk(v) = P
"-CYCLE
WITH ONE BRANCH",0.7612359550561798,"k αi
j,kτj,k(v) for v ∈V i
j ."
"-CYCLE
WITH ONE BRANCH",0.7640449438202247,"Finally, by linearity and U = L"
"-CYCLE
WITH ONE BRANCH",0.7668539325842697,"i,j V i
j , the only possible equivariant function ϕ : U →U is given by
12."
"-CYCLE
WITH ONE BRANCH",0.7696629213483146,"3Similar discussions could be found in Section 4 of [38], but without a proof for necessity."
"-CYCLE
WITH ONE BRANCH",0.7724719101123596,"Connection to matrix form: first note that the isomorphism τ i
j→j′ : V i
j →V i
j′ is given by:"
"-CYCLE
WITH ONE BRANCH",0.7752808988764045,"τ i
j→j′(ui
j,l) = ui
j′,l"
"-CYCLE
WITH ONE BRANCH",0.7780898876404494,"where {ui
j,l, l = 1, . . . , κi} and {ui
j′,l, l = 1, . . . , κi} are orthonormal basis for V i
j and V i
j′ respec-
tively such that the action of g is associated with matrix ρi(g). Therefore,"
"-CYCLE
WITH ONE BRANCH",0.7808988764044944,"ϕ(ui
j,l) =
X"
"-CYCLE
WITH ONE BRANCH",0.7837078651685393,"j′
αi
j,j′τ i
j→j′(ui
j,l) =
X"
"-CYCLE
WITH ONE BRANCH",0.7865168539325843,"j′
αi
j,j′(ui
j′,l)"
"-CYCLE
WITH ONE BRANCH",0.7893258426966292,"Thus the matrix of ϕ under basis (ui
j,l, i
=
1, . . . , p, j
=
1, . . . , κi, l
=
1, . . . , di) is
"
"-CYCLE
WITH ONE BRANCH",0.7921348314606742,"







"
"-CYCLE
WITH ONE BRANCH",0.7949438202247191,"W1
...
W2
...
Wp
... "
"-CYCLE
WITH ONE BRANCH",0.797752808988764,"







"
"-CYCLE
WITH ONE BRANCH",0.800561797752809,"(which is the same as  

"
"-CYCLE
WITH ONE BRANCH",0.8033707865168539,"W1
0
· · ·
0
0
W2
· · ·
0
...
...
...
...
0
0
· · ·
Wp "
"-CYCLE
WITH ONE BRANCH",0.8061797752808989,"

with Bi rear-"
"-CYCLE
WITH ONE BRANCH",0.8089887640449438,"range to Rdi×κi) and M is just the matrix to transform standard basis (ei) to basis (ui
j,l)."
"-CYCLE
WITH ONE BRANCH",0.8117977528089888,"G
Ways to Use Schur Neuron"
"-CYCLE
WITH ONE BRANCH",0.8146067415730337,"While Schur neuron can be viewed as a generic transform on any subgraph’s first-order activation, we
provide some thoughts and possibilities to instantiate it."
"-CYCLE
WITH ONE BRANCH",0.8174157303370787,"In our experiment, we primarily view Schur neuron as a way to expand the feature of a subgraph. It
can be viewed as an analogue to CNN’s convolution filter and it’s indeed a constrained version of
spectral convolution filter [5] on the subgraph. In light of this, we call number of channels of Schur
neuron as how many times it expand the output feature versus the input feature."
"-CYCLE
WITH ONE BRANCH",0.8202247191011236,"Besides, we observe that the linear equivariant map described [32] is a special case of Schur neuron’s
map. The two possible linear map in [32] is (1) identity map, corresponding to take all weights
Wi = 1 (2) T →ΣiTi1, where 1 is all ones vector, corresponding to set W1 = 1, and Wi = 0 for
i ̸= 1 since 1 is eigenvector of any graph Laplacian with eigenvalue 0. Consequently, we suggest
to keep the two basic but important linear maps and augment them with Schur neuron. Empirically
we verified this by an experiment only vary the number of channels. In figure 1, we see that further
increase the number of channels beyond 4 wouldn’t give us performance gain since cycle 5 and 6
only has 3 and 4 distinct eigenvalues respectively."
"-CYCLE
WITH ONE BRANCH",0.8230337078651685,"Furthermore, we suggest that number of channels of Schur neuron be proportional or approximately
equal to number of distinct eigenvalues of the subgraph it attach to. This is because the later is the
number of independent linear maps for Schur neuron."
"-CYCLE
WITH ONE BRANCH",0.8258426966292135,Figure 1: A study on num of channels in Schur layer. Cycle 5 and 6 are included.
"-CYCLE
WITH ONE BRANCH",0.8286516853932584,"Additonally, we’re wondering if we can use Schur layer in place of MLP’s to provide a local structure
aware transform to subgraph activations. Therefore, we tried to replace the 2-layer MLP with 2-layer"
"-CYCLE
WITH ONE BRANCH",0.8314606741573034,"Schur layer in the original Linmaps architecture. But it turns out this use case wasn’t as effective
as the use as learning new feature. Possibly because Schur layer learns feature itself while MLP
transform the learned feature to some desired space."
"-CYCLE
WITH ONE BRANCH",0.8342696629213483,"Methods
Validation MAE
Linmaps
0.087
Schur layer
(in place of MLP)
0.081"
"-CYCLE
WITH ONE BRANCH",0.8370786516853933,"Schur layer
(as learning new feature)
0.076"
"-CYCLE
WITH ONE BRANCH",0.8398876404494382,Table 6: Comparison about two use cases of Schur layer. Experiment on ZINC-12K.
"-CYCLE
WITH ONE BRANCH",0.8426966292134831,"Finally, regarding flexibility as discussed 5.2, in table 7, we see that adding cycles with branches could
provide the Schur Net with more detailed topological information, thus improving the performance.
And the code for adding this actually only requires a single definition of those templates without
modification to the neural network."
"-CYCLE
WITH ONE BRANCH",0.8455056179775281,"Model
Validation MAE
Schur-Net on 5,6 cycles(baseline)
0.113 ± 0.008
Schur-Net on 5,6 cycles with up to three branches
0.105 ± 0.001
Table 7: Flexibility of Schur layer. Experiments on ZINC-12k dataset. All other settings are the same.
A smaller network than previous experiments was used."
"-CYCLE
WITH ONE BRANCH",0.848314606741573,"H
More experiment results"
"-CYCLE
WITH ONE BRANCH",0.851123595505618,"We first start with some molecular datasets in TUdataset [33], which is a small but commonly used
benchmark for GNN. In table 8, we see that Schur layer consistently improves over Linmaps in
various bioinformatics and molecule datasets."
"-CYCLE
WITH ONE BRANCH",0.8539325842696629,"Dataset
Linmaps
Schur Layer
Proteins
74.7 ± 3.8
75.4 ± 4.8
MUTAG
89.9 ± 5.5
90.94 ± 4.7
PTC_MR
61.1 ± 6.9
64.6 ± 5.9
NCI1
82.1 ± 1.8
82.7 ± 1.9
Table 8: Comparison of Linmaps and Schur Layer performance on TUdatasets. Numbers are binary
classification accuracy."
"-CYCLE
WITH ONE BRANCH",0.8567415730337079,"In table 9, we compare the runtime of our Schur layer and Linmaps, which shows the extra compu-
tational cost of Schur layer wasn’t significant while being able to use more equivariant maps and
achieving better accuracy."
"-CYCLE
WITH ONE BRANCH",0.8595505617977528,"Dataset
Linmaps
Schur Layer
Zinc-12k
25.4s
27.6s
NCI1
9.5s
11.5s
Table 9: Runtime per epoch with hyper-params num_layers = 4, rep_dim = 128, dropout = 0.0,
batch_size = 256, num of channels = 4, cycle_sizes = 3,4,5,6,7,8. This shows Schur Layer didn’t add
much computational costs to Linmaps while being more expressive."
"-CYCLE
WITH ONE BRANCH",0.8623595505617978,"Another ablation study is perform on ZINC-12K dataset. In table 10, we see that Schur layer
outperforms Linmaps when certain cycle sizes are considered, especially when only cycles 5 and 6
are chosen as subgraphs in the neural network."
"-CYCLE
WITH ONE BRANCH",0.8651685393258427,"Layer
cycle size {5,6}
cycle size {3,4,5,6}
Linmaps (baseline)
0.139 ± 0.007
0.103 ± 0.011
Schur layer
0.114 ± 0.014
0.100 ± 0.009
Table 10: Comparison between Schur Layer and Linmaps with different set of cycles chosen.
Experiments on ZINC-12k dataset and all scores are test MAE."
"-CYCLE
WITH ONE BRANCH",0.8679775280898876,"I
Architecture and experiment details"
"-CYCLE
WITH ONE BRANCH",0.8707865168539326,"In designing of our architecture, we design a message passing scheme similar to [21] and add Schur
layer to each of the convolution layers. A visualization of our architecture can be found in Figure
2. In each convolution layer, we maintain 0th-order node representation (hv ∈R|V |∗dim) and edge
representation (he ∈R|E|∗dim) where we assume the input graph G = (V, E). We further maintain
1st-order representation on cycles (hDk ∈R(k∗(ck)∗dim)) where Dk is cycle of length k and ck is
the number of such cycle in G. We did message passing between node and edge representation the
same as CIN [6]. The edge and cycle pass message with each other by tranfer maps described by [21]
and then the incoming message to cycles as well as the original cycle representation is transformed
by Schur layer. When updating the edge representation and cycle representation, we combine the
original representation with the incomming message by either concatenation or ϵ-add described in
GIN [40] and feed it into an MLP to get new representations."
"-CYCLE
WITH ONE BRANCH",0.8735955056179775,"Figure 2: An example illustration of one layer of our model architecture. For an input graph, we first
execute the standard node and edge level message passing and lift up the model to higher-order by
forming higher-order representations on specific template graphs. In this example, we learned the
representation on a Naphthalene. To further decouple its information, we may apply the SchurLayer
on the template graphs that would potentially provide us insight about the graph. In this example, the
6 cycle, star graph, and path graph are all applying SchuLayer to learn the features locally. Through
our architecture design, we can also incorporate the peripheral information. Automorphism group
then helps us discover distinct key features such as non-symmetries, and we pass these aggregated
features into the next layer of the architecture."
"-CYCLE
WITH ONE BRANCH",0.8764044943820225,"Hyperparameters
For experiments on ZINC-12K and OGB-HIV, we tune representation dimension
in {32, 64, 128}, and experiment on cycles up to length 18. In the best-performing model, we use
representation dimension 128 and cycle lengths from 3 to 8 and number-of-layers is always 4. For
Schur layer, we tuned number of channels from 2 to 8 and found 2, 4 are a suitable choice when
it is used as an augmentation to Linmaps. For MLPs, we either use 2 to 3 layers depending on
how dense the input’s information is. We always use a batchnorm [25] after the Linear layer and
then do ReLu activation. For training hyperparameters, we use an init learning rate of 0.01 and use
ReduceLROnPlateau scheduler in PyTorch with patience of 10 or 30. We use Adam optimizer [27]
for all trainings. We train for 500 or 1000 epochs depending on model size. In particular, in table 10,
the models have representation dimension 32, so it is trained to only 500 epochs. All other models
are trained 1000 epochs. A batch size of 256 is used for all models. All results are run at least three
times to calculate the standard deviation."
"-CYCLE
WITH ONE BRANCH",0.8792134831460674,"For experiments on TUdataset (table 8), we chose a set of hyper-params by intuition (we didn’t tune
them because this experiment is to demonstrate the effectiveness of Schur layer and compare Schur
layer with Linmaps under the same experiment setting. we don’t aim to compare with Sota on this
experiment). Hyper-params for both Schur Layer and Linmaps: num_layers = 4, rep_dim = 32,64,
dropout = 0.5, batch_size = 32,128, lr = 0.01, num of channels = {2,4}, cycle sizes = 3,4,5,6. We’re
training with StepLR scheduler where reduce lr to 1/2 after every 50 epochs, with a total of 300
epochs. The hyperparams weren’t tuned, we just chose a smaller value for a smaller dataset and a
bigger value for bigger datasets by rule-of-thumb. Linmaps is implemented on our own according to
the description of P-tensor, then Schur Layer replaces Linmaps operation by Schur operation. We
follow the evaluation strategy specified in [40]."
"-CYCLE
WITH ONE BRANCH",0.8820224719101124,"Used Compute Resources
Experiments are all run on one Tesla L4 from PyTorch Lightning
Workspace and one NVIDIA RTX 4090. The code is implemented in PyTorch. For the running time,
on the ZINC-12k dataset, it takes around 8.53 ± 1.2 hours to finish training for 1000 epochs with 4
layers and 128 dimension representation on an NVIDIA RTX 4090. We’re running on a desktop with
a Ryzen 7900 CPU and 64GB of RAM."
"-CYCLE
WITH ONE BRANCH",0.8848314606741573,NeurIPS Paper Checklist
CLAIMS,0.8876404494382022,1. Claims
CLAIMS,0.8904494382022472,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8932584269662921,Answer: [Yes]
CLAIMS,0.8960674157303371,"Justification: The claims made in the paper relate to specific properties of message passing
algorithms. The claims that we make about our own algorithm are accurate."
LIMITATIONS,0.898876404494382,2. Limitations
LIMITATIONS,0.901685393258427,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9044943820224719,Answer: [Yes] .
LIMITATIONS,0.9073033707865169,Justification: We have a section on limitations.
THEORY ASSUMPTIONS AND PROOFS,0.9101123595505618,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9129213483146067,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9157303370786517,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9185393258426966,Justification: For each Lemma and Theorem in the paper we provide a proof.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9213483146067416,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9241573033707865,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9269662921348315,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9297752808988764,"Justification: We document our experiments in detail, including the dataset, the architecture
and parameters."
OPEN ACCESS TO DATA AND CODE,0.9325842696629213,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9353932584269663,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9382022471910112,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9410112359550562,"Justification: We have included the link to a github repository containing all the files needed
to reproduce the experiments."
OPEN ACCESS TO DATA AND CODE,0.9438202247191011,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9466292134831461,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.949438202247191,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.952247191011236,"Justification: Yes, we give all these details in the Appendix."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9550561797752809,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9578651685393258,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9606741573033708,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9634831460674157,"Justification: For each of our results on the benchmarks datasets we provide the standard
deviation of the performance."
EXPERIMENTS COMPUTE RESOURCES,0.9662921348314607,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9691011235955056,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9719101123595506,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9747191011235955,"Justification: We provide this information in the Appendix.
9. Code Of Ethics"
EXPERIMENTS COMPUTE RESOURCES,0.9775280898876404,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We have reviewed the code of ethics and our research conforms to it.
10. Broader Impacts"
EXPERIMENTS COMPUTE RESOURCES,0.9803370786516854,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA] ."
EXPERIMENTS COMPUTE RESOURCES,0.9831460674157303,"Justification: Our work is foundational/theoretical in nature and we not foresee it having any
direct social impacts. Indirectly, it could have impact because higher order automorphism-
equivariant could be used in drug discovery, for example. Applications with negative social
impacts are also possible, but a little more difficult to pinpoint. Space limitations prevented
us from discussing this topic in more detail.
11. Safeguards"
EXPERIMENTS COMPUTE RESOURCES,0.9859550561797753,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA] .
Justification: The model itself does not pose such a risk.
12. Licenses for existing assets"
EXPERIMENTS COMPUTE RESOURCES,0.9887640449438202,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]"
EXPERIMENTS COMPUTE RESOURCES,0.9915730337078652,"Justification: We provide a citation for every baseline model and benchmark dataset that we
use. All assets were used in a way that conforms to their published licenses.
13. New Assets"
EXPERIMENTS COMPUTE RESOURCES,0.9943820224719101,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA] .
Justification: We do not provide new assets.
14. Crowdsourcing and Research with Human Subjects"
EXPERIMENTS COMPUTE RESOURCES,0.9971910112359551,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA] .
Justification: This research did not involve crowdsourcing or human subjects research.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA] .
Justification: This research did not involve crowdsourcing or human subjects research."
