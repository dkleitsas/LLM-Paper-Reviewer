Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003367003367003367,"We present implicit displacement ﬁelds, a novel representation for detailed 3D ge-
ometry. Inspired by a classic surface deformation technique, displacement map-
ping, our method represents a complex surface as a smooth base surface plus a
displacement along the base’s normal directions, resulting in a frequency-based
shape decomposition, where the high-frequency signal is constrained geometri-
cally by the low-frequency signal.
Importantly, this disentanglement is unsu-
pervised thanks to a tailored architectural design that has an innate frequency
hierarchy by construction.
We explore implicit displacement ﬁeld surface re-
construction and detail transfer and demonstrate superior representational power,
training stability, and generalizability.
Code and data available at: https:
//github.com/yifita/idf"
INTRODUCTION,0.006734006734006734,"1
INTRODUCTION"
INTRODUCTION,0.010101010101010102,"Neural implicit functions have emerged as a powerful tool for representing a variety of signals.
Compared to conventional discrete representations, neural implicits are continuous and thus not tied
to a speciﬁc resolution. Recently, neural implicits have gained signiﬁcant attraction in a variety of
applications ranging from 3D reconstruction (Sitzmann et al., 2019; Niemeyer et al., 2020; Yariv
et al., 2020; Peng et al., 2020), neural rendering (Mildenhall et al., 2020; Pumarola et al., 2020), im-
age translation (Skorokhodov et al., 2020; Chen et al., 2020a) to deformation approximation (Deng
et al., 2019). In this paper, we focus on neural implicit representations for 3D geometry."
INTRODUCTION,0.013468013468013467,"While neural implicits can theoretically model geometry with inﬁnite resolution, in prac-
tice the output resolution is dependent on the representational power of neural nets.
So
far, the research community approaches the problem from two main directions.
The ﬁrst is
to partition the implicit function using spatial structures (Chabra et al., 2020; Jiang et al.,
2020; Liu et al., 2020; Takikawa et al., 2021), thus making the memory and computation"
INTRODUCTION,0.016835016835016835,"Figure 1: Displacement mapping in 1D. The
detailed surface (upper blue) is created by
offsetting samples of the base surface (upper
black) using the height map shown below."
INTRODUCTION,0.020202020202020204,"demands dependent on the geometric complexity. The
other direction focuses on improving networks’ ability to
represent high-frequency signals, either in a preprocess-
ing step (referred to as positional encoding) (Mildenhall
et al., 2020) or by using sinusoidal representation net-
works (SIREN) (Sitzmann et al., 2020). However, training
these networks is very challenging, as they are prone to
overﬁtting and optimization local minima."
INTRODUCTION,0.02356902356902357,"Inspired by the classic computer graphics technique, dis-
placement mapping (Cook, 1984; Cook et al., 1987), we
propose a novel parameterization of neural implicit func-
tions, implicit displacement ﬁeld, abbreviated as IDF, to
circumvent the above issues. Our method automatically
disentangles a given detailed shape into a coarse base
shape represented as a continuous, low-frequency signed distance function and a continuous high-
frequency implicit displacement ﬁeld, which offsets the base iso-surface along the normal direction."
INTRODUCTION,0.026936026936026935,"The key novelty of our approach lies in extending the classic displacement mapping, which is dis-
crete and lies only on the base surface, to a continuous function in the R3 domain and incorporating
it into contemporary neural implicit representations, ergo achieving a disentanglement of geometric
details in an unsupervised manner."
INTRODUCTION,0.030303030303030304,Our main technical contribution includes
A PRINCIPLED AND THEORETICALLY GROUNDED EXTENSION OF EXPLICIT DISCRETE DISPLACEMENT MAPPING,0.03367003367003367,"1. a principled and theoretically grounded extension of explicit discrete displacement mapping
to the implicit formulation,"
A NEURAL ARCHITECTURE THAT CREATES A GEOMETRICALLY INTERPRETABLE FREQUENCY HIERARCHY IN THE,0.037037037037037035,"2. a neural architecture that creates a geometrically interpretable frequency hierarchy in the
neural implicit shape representation by exploiting the inductive bias of SIRENs, and"
A NEURAL ARCHITECTURE THAT CREATES A GEOMETRICALLY INTERPRETABLE FREQUENCY HIERARCHY IN THE,0.04040404040404041,"3. introducing transferable implicit displacement ﬁelds by replacing the common coordinates
input with carefully constructed transferrable features, thus opening up new opportunities
for implicit geometry manipulation and shape modeling."
A NEURAL ARCHITECTURE THAT CREATES A GEOMETRICALLY INTERPRETABLE FREQUENCY HIERARCHY IN THE,0.04377104377104377,"Systematic evaluations show that our approach is signiﬁcantly more powerful in representing geo-
metric details, while being lightweight and highly stable in training."
RELATED WORK,0.04713804713804714,"2
RELATED WORK"
RELATED WORK,0.050505050505050504,"Hierachical neural implicit shape representation.
Neural implicit shape representation was
initially proposed by several works concurrently (Park et al., 2019; Chen & Zhang, 2019; Mescheder
et al., 2019), and since then many works have sought to introduce hierarchical structures into the
neural representation for better expressiveness and generalizability. The majority of these methods
focus on spatial structures. Chabra et al. (2020); Saito et al. (2019; 2020) use sparse regular voxels
and dense 2D grid, respectively, to improve detail reconstruction. In the spirit of classic approaches,
e.g. Frisken et al. (2000); Ohtake et al. (2003), Liu et al. (2020); Takikawa et al. (2021); Martel
et al. (2021) store learned latent codes in shape-adaptive octrees, leading to signiﬁcantly higher
reconstruction quality and increased rendering speed. A common disadvantage of these methods
is that the memory use and model complexity are directly tied to the desired geometric resolution.
In parallel, other proposed methods learn the spatial partition. Some of these methods decompose
the given shape using parameterized templates, such as anisotropic Gaussians (Genova et al., 2019),
convex shape CVXNet (Deng et al., 2020; Chen et al., 2020b) or simple primitives (Hao et al.,
2020), while others represent local shapes with small neural networks and combine them together
either using Gaussians (Genova et al., 2020) or surface patches (Tretschk et al., 2020). Due to
limitations of template functions and delicate spatial blending issues, these methods can only handle
very coarse geometries."
RELATED WORK,0.05387205387205387,"Concurrently, Li & Zhang (2021) propose a two-level neural signed distance function for single-
view reconstruction. Exploiting the fact that most man-made shapes have ﬂat surfaces, it represents
a given shape as a coarse SDF plus a frontal and rear implicit displacement map for better detail
construction. Besides having entirely different applications – we focus on representing signiﬁcantly
higher geometry resolutions – our implicit displacement is grounded in geometry principles and
applies to general shapes."
RELATED WORK,0.05723905723905724,"High-frequency representation in neural networks
As formally explained by Xu (2018); Xu
et al. (2019); Rahaman et al. (2019); Basri et al. (2020), neural networks have a tendency to learn
low-frequency functions. To combat this issue, Mildenhall et al. (2020) incorporate “positional en-
coding” for neural rendering and demonstrate remarkable progress in terms of detail reconstruction,
which is a sinusoidal mapping for the input signal, a practice later theoretically justiﬁed by Tan-
cik et al. (2020). Alternatively, SIREN also shows impressive advances in detail representation by
replacing ReLU activation with sin functions. With these new networks gaining popularity, a few
works delve deeper and apply a coarse-to-ﬁne frequency hierarchy in the training process for de-
formable shape representation (Park et al., 2020b) and meshing (Hertz et al., 2021). In our method,
we also create a frequency hierarchy by leveraging this new form of networks – not only in the train-
ing scheme but also explicitly in the construction of the networks to reﬂect our geometry-motivated
design principles. +
="
RELATED WORK,0.06060606060606061,"base SDF
implicit displacement
detailed SDF"
RELATED WORK,0.06397306397306397,"Figure 2: Method overview. We represent detailed geometries as a sum of a coarse base shape represented
as low-frequency signed distance function and a high-frequency implicit displacement ﬁeld, which offsets the
base iso-surface along the base’s normal directions."
RELATED WORK,0.06734006734006734,"Detail transfer
Detail transfer refers to transplanting the disentangled geometric details from a
source shape onto a target object with high ﬁdelity and plausibility. Classic detail transfer methods
represent surface details as normal displacements (Botsch et al., 2010; Zhou et al., 2007; Sorkine
& Botsch, 2009). The majority of them are parametric (Ying et al., 2001; Biermann et al., 2002;
Sorkine et al., 2004; Zhou et al., 2006; Takayama et al., 2011), relying on a consistent surface
parameterization between the source and the target shape. Non-parametric approaches (Chen et al.,
2012; Berkiten et al., 2017), on the other hand, ﬁnd best-matching surface patches between the
source and target, and copy the details iteratively from coarse to ﬁne. These classic approaches
produce high quality results, but often require a pre-deﬁned base surface or abundant user inputs. In
the “deep” realm, DeepCage (Yifan et al., 2020) proposed a neural deformation method that maps
solely the coarse geometry, hence allowing detail transfer without tackling detail disentanglement.
Hertz et al. (2020) learn the coarse-to-detail correspondence iteratively from multi-scale training
data, while Chen et al. (2021) synthesizes details by upsampling a coarse voxel shape according to a
style code of another shape using GANs. All of these approaches use explicit representations, hence
they are subject to self-intersection and resolution limitations. D2IM-Net (Li & Zhang, 2021) uses
two planar displacement maps to transfer surface details by mapping the coordinates of the source
and target shapes using part segementation, thus limiting the application to man-made rigid shapes.
In comparison, our method does not require any correspondence mapping."
METHOD,0.0707070707070707,"3
METHOD"
METHOD,0.07407407407407407,implicit inverse displacement
METHOD,0.07744107744107744,implicit displacement
METHOD,0.08080808080808081,"Figure 3: An implicit displacement ﬁeld for
a 1D-curve. The displacement is deﬁned not
only on the zero-isosurface S0 but also on
arbitrary isosurfaces Sτ"
METHOD,0.08417508417508418,"We represent a shape with ﬁne geometric details using
two SIREN networks of different frequencies in the activa-
tion functions. The SIREN with lower frequency describes
a smooth base surface; the SIREN with higher frequency
adds microstructure to the base iso-surface by producing
an implicit displacement ﬁeld along the base’s normal di-
rection (see Figure 2)."
METHOD,0.08754208754208755,"In this section, we ﬁrst formally deﬁne implicit displace-
ment ﬁeld by generalizing the classic explicit and discrete
displacement mapping in Sec 3.1, then in Sec 3.2 we in-
troduce the network architectures and training strategies
that are tailored to this deﬁnition, ﬁnaly in Sec 3.3 we
extend the implicit displacement to address transferability."
IMPLICIT DISPLACEMENT FIELDS,0.09090909090909091,"3.1
IMPLICIT DISPLACEMENT FIELDS"
IMPLICIT DISPLACEMENT FIELDS,0.09427609427609428,"In classic displacement mapping as shown in Figure 1, high-frequency geometric details are ob-
tained on a smooth base surface by taking samples from the base surface and offsetting them along
their normal directions by a distance obtained (with interpolation) from a discrete height map. Two
elements in this setting impede a direct adaptation for implicit shape representation: 1. the displace-
ment mapping is deﬁned discretely and only on the base surface, whereas implicit surface functions
are typically deﬁned continuously on the R3 domain; 2. the base surface is known and ﬁxed, whereas
our goal is to learn the base surface and the displacement jointly on-the-ﬂy."
IMPLICIT DISPLACEMENT FIELDS,0.09764309764309764,"Addressing the above challenges, we ﬁrst deﬁne implicit displacement ﬁelds (IDF), which are con-
tinuous analog to height maps that extend displacement mapping to the R3 domain."
IMPLICIT DISPLACEMENT FIELDS,0.10101010101010101,"Deﬁnition 1. Given two signed distance functions f and ˆf and their respective iso-surfaces at
a given value τ ∈R, Sτ =

x ∈R3|f(x) = τ
	
and ˆSτ =
n
x ∈R3| ˆf(x) = τ
o
, an implicit"
IMPLICIT DISPLACEMENT FIELDS,0.10437710437710437,displacement ﬁeld d: R3 →R deﬁnes the deformation from Sτ to ˆSτ such that
IMPLICIT DISPLACEMENT FIELDS,0.10774410774410774,"f (x) = ˆf (x + d (x) n) , where n =
∇f (x)
∥∇f (x) ∥.
(1)"
IMPLICIT DISPLACEMENT FIELDS,0.1111111111111111,"This deﬁnition is schematically illustrated in Figure 3, where the iso-surface S0 and Sτ are mapped
to ˆS0 and ˆSτ with the same implicit displacement ﬁeld d. Notably, the height map in classic dis-
placement mapping is a discrete sampling of IDF for the limited case τ = 0."
IMPLICIT DISPLACEMENT FIELDS,0.11447811447811448,"In the context of surface decomposition, our goal is to estimate the base surface f and the displace-
ment d given an explicitly represented detailed surface ˆS0. Following equation 1, we can do so by
minimizing the difference between the base and the ground truth signed distance at query points
x ∈R3 and their displaced position ˆx = x + d (x) n, i.e., min |f (x) −ˆfGT (ˆx) |."
IMPLICIT DISPLACEMENT FIELDS,0.11784511784511785,"However, this solution requires evaluating ˆfGT (ˆx) dynamically at variable positions ˆx, which is a
costly operation as the detailed shapes are typically given in explicit form, e.g., as point clouds or
meshes. Hence, we consider the inverse implicit displacement ﬁeld ˆd, which deﬁnes a mapping from
ˆSτ to Sτ, f

ˆx + ˆd (ˆx) n

= ˆf (ˆx), as depicted in Figure 3."
IMPLICIT DISPLACEMENT FIELDS,0.12121212121212122,"Assuming the displacement distance is small, we can approximate n, the normal after inverse dis-
placement, with that before the inverse displacement, i.e."
IMPLICIT DISPLACEMENT FIELDS,0.12457912457912458,"f

ˆx + ˆd (ˆx) ˆn

= ˆf (ˆx) , where ˆn =
∇f (ˆx)
∥∇f (ˆx) ∥.
(2)"
IMPLICIT DISPLACEMENT FIELDS,0.12794612794612795,"This is justiﬁed by the following theorem and corollary, which we prove in the Appendix A."
IMPLICIT DISPLACEMENT FIELDS,0.13131313131313133,"Theorem 1. If function f : Rn →R is differentiable, Lipschitz-continuous with constant L and
Lipschitz-smooth with constant M, then ∥∇f (x + δ ∇f (x)) −∇f (x) ∥≤|δ|LM."
IMPLICIT DISPLACEMENT FIELDS,0.13468013468013468,"Corollary 1. If a signed distance function f satisfying the eikonal equation up to error ϵ > 0,
|∥∇f∥−1| < ϵ, is Lipschitz-smooth with constant M, then ∥∇f (x + δ ∇f (x)) −∇f (x) ∥<
(1 + ϵ)|δ|M."
IMPLICIT DISPLACEMENT FIELDS,0.13804713804713806,"Given n =
∇f(x)
∥∇f(x)∥, ˆn =
∇f(ˆx)
∥∇f(ˆx)∥, and ˆx = x + d (x) n, let δ =
d(x)
∥∇f(x)∥, we can show ∥ˆn −n∥≤
1 + ϵ
1 −ϵ |δ| M (c.f. Appendix A). In other words, the difference of ˆn and n is bounded by a small"
IMPLICIT DISPLACEMENT FIELDS,0.1414141414141414,"constant. Thus we obtain the approximation in equation 2, which allows us to presample training
samples {ˆx} and use precomputed ˆfGT (ˆx) or its derivatives (see Sec 3.2) for supervision."
NETWORK DESIGN AND TRAINING,0.1447811447811448,"3.2
NETWORK DESIGN AND TRAINING"
NETWORK DESIGN AND TRAINING,0.14814814814814814,"The formulation of (inverse) implicit ﬁeld in the previous section is based on three assumptions:
(i) f is smooth, (ii) d is small, (iii) f satisﬁes the eikonal constraint up to an error bound. In this
section, we describe our network architecture and training technique, with emphasis on meeting
these requirements."
NETWORK DESIGN AND TRAINING,0.15151515151515152,"Network architecture.
We propose to model f and ˆd with two SIRENs denoted as N ωB and
N ωD, where ωB and ωD refer to the frequency hyperparameter in the sine activation functions x 7→
sin (ωx). Evidently, as shown in Figure 4, ω dictates an upper bound on the frequencies the network
is capable of representing, thereby it also determines the network’s inductive bias for smoothness.
Correspondingly, we enforce the smoothness of f and detail-representing capacity of ˆd using a
smaller ωB and a larger ωD, e.g. ωB = 15 and ωD = 60. Moreover, we add a scaled tanh activation
to the last linear layer of N ωD, i.e. α tanh (·), which ensures that the displacement distance is
smaller than the constant α. More insight about the choice of ωB/D is detailed in Section B.2."
NETWORK DESIGN AND TRAINING,0.15488215488215487,"Figure 4: Smoothness control via SIREN’s frequency hyperpa-
rameter ω. Overﬁtting SIREN to the right image with ω = 30
(ﬁrst) and ω = 60 (middle) shows that smaller ω leads to a
smoother result."
NETWORK DESIGN AND TRAINING,0.15824915824915825,"0
0.05
0.1
0.15
0.2
0 0.2 0.4 0.6 0.8 1"
NETWORK DESIGN AND TRAINING,0.16161616161616163,ν = 0.02
NETWORK DESIGN AND TRAINING,0.16498316498316498,ν = 0.05
NETWORK DESIGN AND TRAINING,0.16835016835016836,ν = 0.1
NETWORK DESIGN AND TRAINING,0.1717171717171717,N ωB (x) χ
NETWORK DESIGN AND TRAINING,0.1750841750841751,"Figure 5: Attenuation as a function of base
SDF."
NETWORK DESIGN AND TRAINING,0.17845117845117844,"Networks containing high-frequency signals, e.g. N ωD, require large amounts of accurate ground
truth data for supervision to avoid running into optimization local minima (Park et al., 2020b).
Consequently, when dense and accurate ground truth SDF values are not available, high-frequency
signals often create artifacts. This is often the case in void regions when learning from point clouds,
as only implicit regularization and fuzzy supervision is applied (see the ﬁrst and last terms of equa-
tion 4). Hence, we apply an attenuation function χ (N ωB) =
1
1+(N ωB (x)/ν)
4 to subdue N ωD far"
NETWORK DESIGN AND TRAINING,0.18181818181818182,"from the base surface, where ν determines the speed of attenuation as depicted in Figure 5."
NETWORK DESIGN AND TRAINING,0.18518518518518517,"Combining the aforementioned components, we can compute the signed distance of the detailed
shape at query point x in two steps:"
NETWORK DESIGN AND TRAINING,0.18855218855218855,"f (x) = N ωB (x) ,
ˆf (x) = N ωB

x + χ (f (x)) N ωD (x)
∇f (x)
∥∇f (x) ∥"
NETWORK DESIGN AND TRAINING,0.1919191919191919,"
.
(3)"
NETWORK DESIGN AND TRAINING,0.19528619528619529,"Training.
We adopt the loss from SIREN, which is constructed to learn SDFs directly from
oriented point clouds by solving the eikonal equation with boundary constraint at the on-surface
points. Denoting the input domain as Ω(by default set to [−1, 1]3) and the ground truth point cloud
as P = {(pi, ni)}, the loss computed as in equation 4, where λ{0,1,2,3} denote loss weights:"
NETWORK DESIGN AND TRAINING,0.19865319865319866,"L ˆ
f = P"
NETWORK DESIGN AND TRAINING,0.20202020202020202,"x∈Ωλ0
∥∇ˆf (x) ∥−1
 +
X"
NETWORK DESIGN AND TRAINING,0.2053872053872054,"(p,n)∈P"
NETWORK DESIGN AND TRAINING,0.20875420875420875," 
λ1| ˆf (p) | + λ2

1 −
D
∇ˆf (p) , n
E  +
X"
NETWORK DESIGN AND TRAINING,0.21212121212121213,"x∈Ω\P
λ3 exp

−100 ˆf (x)

.
(4)"
NETWORK DESIGN AND TRAINING,0.21548821548821548,"As the displacement and the attenuation functions depend on the base network, it is beneﬁcial to
have a well-behaving base network when training the displacement (see Sec 4.2). Therefore, we
adopt a progressive learning scheme, which ﬁrst trains N ωB, and then gradually increase the impact
of N ωD. Notably, similar frequency-based coarse-to-ﬁne training techniques are shown to improve
the optimization result in recent works (Park et al., 2020b; Hertz et al., 2021)."
NETWORK DESIGN AND TRAINING,0.21885521885521886,"We implement the progressive training via symmetrically diminishing/increasing learning rates and
loss weights for the base/displacement networks. For brevity, we describe the procedure for loss
weights only, and we apply the same to the learning rates in our implementation. First, we train N ωB"
NETWORK DESIGN AND TRAINING,0.2222222222222222,"by substituting ˆf in the loss equation 4 with f, resulting a base-only loss denoted Lf. Then, starting
from a training percentile Tm ∈[0, 1], we combine Lf and L ˆ
f via κ Lf + (1 −κ) L ˆ
f
with
κ =
1
2"
NETWORK DESIGN AND TRAINING,0.2255892255892256,"
1 + cos

π (t−Tm)"
NETWORK DESIGN AND TRAINING,0.22895622895622897,"(1−Tm)

, where t ∈[Tm, 1] denotes the current training progress."
NETWORK DESIGN AND TRAINING,0.23232323232323232,"3.3
TRANSFERABLE IMPLICIT DISPLACEMENT FIELD."
NETWORK DESIGN AND TRAINING,0.2356902356902357,"In classic displacement mapping, the displacement is queried by the UV-coordinates from surface
parameterization, which makes the displacement independent of deformations of the base surface.
We can achieve similar effect without parameterization by learning query features, which emulate
the UV-coordinates to describe the location of the 3D query points w.r.t. the base surface."
NETWORK DESIGN AND TRAINING,0.23905723905723905,"We construct the query features using two pieces of information: (i) a global context descriptor,
φ (x), describing the location of the query point in relation to the base surface in a semantically"
NETWORK DESIGN AND TRAINING,0.24242424242424243,Feature Extractor
NETWORK DESIGN AND TRAINING,0.24579124579124578,Sparse Point Cloud
NETWORK DESIGN AND TRAINING,0.24915824915824916,point normals
NETWORK DESIGN AND TRAINING,0.25252525252525254,Transferable IDF T ωD
NETWORK DESIGN AND TRAINING,0.2558922558922559,Non-transferable IDF N ωD
NETWORK DESIGN AND TRAINING,0.25925925925925924,"Figure 6: Illustrations for transferable and non-transferable implicit ﬁelds. The transferable modules are in pink
and the shape-speciﬁc modules are in yellow. Instead of consuming the euclidean coordinates, the transferable
displacement network takes a scale-and-translation invariant feature as inputs, which describes the relative
position of the query point to the base shape.
meaningful way, (ii) the base signed distance value f (x), which gives more precise relative location
with respect to the base surface. Since both are differentiable w.r.t. the euclidean coordinates of the
query point, we can still train N ωD using derivatives as in equation 4."
NETWORK DESIGN AND TRAINING,0.26262626262626265,"Our global context descriptor is inspired by Convolutional Occupancy Networks (Peng et al., 2020).
Speciﬁcally, we project the sparse on-surface point features obtained using a conventional point
cloud encoder onto a regular 3D (or 2D, c.f. Sec 4.3) grid, then use a convolutional module to prop-
agate sparse on-surface point features to the off-surface area, ﬁnally obtain the query feature using
bilinear interpolation. We use normals instead of point positions as the input to the point cloud
encoder, making the features scale-invariant and translation-invariant. Note that ideally the features
should also be rotation-invariant. Nevertheless, as we empirically show later, normal features can
in fact generalize under small local rotational deformations, which is sufﬁcient for transferring dis-
placements between two roughly aligned shapes. We leave further explorations in this direction for
future work."
NETWORK DESIGN AND TRAINING,0.265993265993266,"N ωD is tasked to predict the displacement conditioning on φ (x) and f (x). However, empiri-
cal studies (Chan et al., 2020) suggest that SIREN does not handle high-dimensional inputs well.
Hence, we adopt the FiLM conditioning (Perez et al., 2018; Dumoulin et al., 2018) as suggested
by Chan et al. (2020), which feeds the conditioning latent vector as an afﬁne transformation to
the features of each layer.
Speciﬁcally, a mapping network M converts φ (x) to a set of C-
dimensional frequency modulators and phase shifters {γi, βi}, which transform the i-th linear layer
to
 
1 + 1"
NETWORK DESIGN AND TRAINING,0.26936026936026936,"2γi

◦(Wi x + bi) + βi, where Wi and bi are the parameters in the linear layer and
◦denotes element-wise multiplication. Finally, since SIREN assumes inputs in range (−1, 1), we
scale f using ¯f (x) = tanh
  1"
NETWORK DESIGN AND TRAINING,0.2727272727272727,"ν f (x)

to capture the variation close to the surface area, where ν is
the attenuation parameter described in section 3.2."
NETWORK DESIGN AND TRAINING,0.2760942760942761,"Figure 6 summarizes the difference between transferable and non-transferable displacement ﬁelds.
Formally, the signed distance function of the detailed shape in equation 3 can be rewritten as"
NETWORK DESIGN AND TRAINING,0.27946127946127947,"ˆf (x) = N ωB

x + χ (f (x)) T ωD   ¯f (x) , M (φ (x))

∇f (x)
∥∇f (x) ∥"
NETWORK DESIGN AND TRAINING,0.2828282828282828,"
.
(5)"
RESULTS,0.28619528619528617,"4
RESULTS"
RESULTS,0.2895622895622896,"We now present the results of our method. In Sec 4.1, we evaluate our networks in terms of geometric
detail representation by comparing with state-of-the-art methods on the single shape ﬁtting task. We
then evaluate various design components in an ablation study in Sec 4.2. Finally, we validate the
transferability of the displacement ﬁelds in a detail transfer task in Sec 4.3. Extended qualitative and
quantitative evaluations are included in section B."
RESULTS,0.29292929292929293,"Implementation details.
By default, both the base and the displacement nets have 4 hidden lay-
ers with 256 channels each. The maximal displacement α, attenuation factors ν, and the switching
training percentile is set to Tm are set to 0.05, 0.02 and 0.2 respectively; The loss weights λ{0,1,2,3}
in equation 4 are set to 5, 400, 40 and 50. We train our models for 120 epochs using ADAM opti-"
RESULTS,0.2962962962962963,Chamfer distance points to point distance ·10−3 / normal cosine distance ·10−2
RESULTS,0.2996632996632997,"Progressive
FFN
NGLOD
(LOD4)
NGLOD
(LOD6)
SIREN-3
ω = 60
SIREN-7
ω = 30
SIREN-7
ω = 60
Direct
Residual
D-SDF
Ours"
RESULTS,0.30303030303030304,"SketchFab-16
5.47/3.77
2.27/4.24
1.35/1.97
9.85/6.64
4.85/2.56
-
181/59.0
2.85/4.39
1.22/1.25"
RESULTS,0.3063973063973064,"Table 1: Quantitative comparison. Among the benchmarking methods, only NGLOD at LOD-6, using 256×
number of parameters compared to our model, can yield results close to ours. SIREN models with larger ω have
convergence issues: despite our best efforts, the models still diverged in most cases. Please refer to Table 4 for
a more comprehensive evaluation."
RESULTS,0.30976430976430974,"ground truth
ours
(4.8MB)
NGLOD-LOD6
(946MB)"
RESULTS,0.31313131313131315,"NGLOD-LOD4
(16MB)"
RESULTS,0.3164983164983165,"D-SDF
(7.4MB)"
RESULTS,0.31986531986531985,"SIREN-7 ω = 30
(5.6MB)"
RESULTS,0.32323232323232326,"Figure 7: Comparison of detail reconstruction (better viewed with zoom-in). We show the best 5 methods and
their model sizes according to Table 1, more results are provided in section B.1."
RESULTS,0.3265993265993266,"mizer with initial learning rate of 0.0001 and decay to 0.00001 using cosine annealing (Loshchilov
& Hutter, 2016) after ﬁnishing 80% of the training epochs. We presample 4 million surface points
per mesh for supervision. Each training iteration uses 4096 subsampled surface points and 4096 off-
surface points uniformly sampled from the [−1, 1]3 bounding box. To improve the convergence rate,
we initialize SIREN models by pre-training the base model N ωB (and baseline SIREN, c.f. Sec 4.1)
to a sphere with radius 0.5. This initialization is optional for our training but is critical for baseline
SIRENs."
RESULTS,0.32996632996632996,"Data.
We test our method using 16 high-resolution shapes, including 14 from Sketchfab (ske,
2021) and 2 from Stanford 3DScanRepo (sta, 2021). Our transferable displacement model is tested
using shapes provided by Berkiten et al. (2017), Yang et al. (2020), and Zhou & Jacobson (2016)."
RESULTS,0.3333333333333333,"4.1
DETAIL REPRESENTATION."
RESULTS,0.3367003367003367,"We compare our approaches with 5 baseline methods. 1) FFN (Tancik et al., 2020) with SOFTPLUS
activation and 8 frequency bands progressively trained from coarse-to-ﬁne, where a total of 8 hidden
layers each of size 256 are used to match our model size; additionally we apply a skip-connection
in the middle layer as proposed in DeepSDF Park et al. (2019). 2) NGLOD (Takikawa et al., 2021)
trained using 4 and 6 levels of detail (LODs) corresponding to 643 and 2563 spatial resolution re-
spectively, with LOD4 comparable with our model in terms of the number of parameters. 3) baseline
SIREN, for which we trained three different variations in hope of overcoming training divergence is-
sues;. 4) direct residual, where we compose the signed distance value simply as the sum of base and
displacement nets, i.e. ˆf (x) = N ωB (x)+N ωD (x). 5) D-SDF (inspired by Pumarola et al. (2021);
Park et al. (2020a)), which represents the displacement as a R3 vector, i.e. ˆf = f(x + ∆), where
∆∈R3 is predicted in the second network. We follow network specs of D-Nerf Pumarola et al.
(2020), which contains two 8-layer MLP networks with RELU activation and positional encodings.
Among these, NGLOD, direct residual and D-SDF requires ground truth SDF for supervision, the
rest are trained using our training loss. Two-way point-to-point distance and normal cosine distance
are computed as the evaluation metrics on 5 million points randomly sampled from meshes extracted
using marching cubes with 5123 resolution."
RESULTS,0.3400673400673401,"α test (with ν = 0.02)
ν test with (α = 0.05)
0.01
0.02
0.05
0.1
0.2
0.01
0.02
0.05
0.1
0.2"
RESULTS,0.3434343434343434,"point-to-point distance
 
·10−3
1.178 1.171 1.147 1.146 1.149 1.146 1.147 1.147 1.149 1.152
normal cosine distance
 
·10−2
1.525 1.490 1.252 1.251 1.260 1.254 1.253 1.251 1.250 1.274"
RESULTS,0.3468013468013468,"Table 3: Study of the hyperparameters α (left) and ν (left). The reconstruction accuracy remains stable and
highly competetive throughout hyperparameters variation."
RESULTS,0.3501683501683502,"As shown in Table 1 and Figure 7, our method outperforms the baseline methods with much higher
reconstruction ﬁdelity. NGLOD with 6 LODs is the only method onpar with ours in terms of de-
tail representation, however it requires storing more than 300 times as many as parameters as our
model. SIREN networks with larger ω have severe convergence issues even with sphere initialization
(c.f. Implementation Details) and gradient clipping. Direct residual doesn’t enforce displacement
directions and produces large structural artifacts. D-SDF yields qualitatively poor results, as the dis-
placement net is unable to learn meaningful information (more analysis is shown in section B.1.2)."
ABLATION STUDY,0.35353535353535354,"4.2
ABLATION STUDY"
ABLATION STUDY,0.3569023569023569,"We study the contributions of different design components, namely the displacement scaling α tanh,
the attenuation function χ and the progressive training."
ABLATION STUDY,0.3602693602693603,"α tanh
χ
prog.
training
average
CD ·10−3"
ABLATION STUDY,0.36363636363636365,"1.44
✓
1.41
✓
✓
1.38
✓
✓
✓
1.24"
ABLATION STUDY,0.367003367003367,"Table 2: Ablation study. Our model beneﬁts
from the proposed architectural and training
designs, yet it is also robust against variations."
ABLATION STUDY,0.37037037037037035,"As Table 2 shows, all the test modes converge within
comparable range, even for the model with the least
constraints.
This shows that our model is robust
against violations of theoretical assumptions speciﬁed
in Sec 3.1. At the same time, the performance rises
with increasingly constrained architecture and progres-
sive training, suggesting that the proposed mechanisms
further boost training stability."
ABLATION STUDY,0.37373737373737376,"Table 3 shows that in a reasonable range of α and ν
there is very little variance in reconstruction quality, in-
dicating the robustness w.r.t. parameter selection. If α is too small, the displacement may no longer
be sufﬁcient to correct the difference between the base and detailed surface, causing the slight in-
crease of chamfer distances in the table for α = {0.01, 0.02}. When ν is too large (0.2), i.e. the high
frequency signal is not suppressed in the void region, which leads to higher chamfer distances due
to off-surface high-frequency noise."
TRANSFERABILITY,0.3771043771043771,"4.3
TRANSFERABILITY"
TRANSFERABILITY,0.38047138047138046,"We apply our method to detail transfer in order to validate the transferability of IDF. Speciﬁcally, we
want to transfer the displacements learned for a source shape to a different aligned target shape. In
the ﬁrst test scenario, the base shape is provided and lies closely to the ground truth detailed surface.
In the second scenario, we are only provided with the detailed shapes and thus need to estimate
the base and the displacements jointly. The pipeline consists of the following steps: 1) train N ωB
by ﬁtting the source shape (or the source base shape if provided), 2) train T ωD, M and the query
feature extractor φ jointly by ﬁtting the source shape using equation 5 while keeping N ωB ﬁxed,
3) train N ωB
new by ﬁtting the target shape (or the target base shape if provided), 4) evaluate equation 5
by replacing N ωB with N ωB
new. To prevent the base network from learning high-frequency details
when the base is unknown, we use ωB = 5 and three 96-channel hidden layers for N ωB."
TRANSFERABILITY,0.3838383838383838,"Example outputs for both scenarios are shown in Figure 10, where the base shapes are provided
for the shorts model. We use a 323 and a 1282 grid (for the frontal view), for the shorts and face
model respectively in φ to extract the query features. Our displacement ﬁelds, learned solely from
the source shape, generate plausible details on the unseen target shape. The transferred details
contain high-frequency signals (e.g. the eyebrows on the face), which is challenging for explicit
representations. However, for the second scenario the performance degenerates slightly since the
displacement ﬁeld has to compensate errors stemming speciﬁcally from the base SDF."
TRANSFERABILITY,0.3872053872053872,"In additional, we evaluate the design of the transferable IDF model by removing the mapping net
and the convolutional context descriptor φ. For the former case, we drop the FiLM conditioning"
TRANSFERABILITY,0.39057239057239057,"source (top) / target
(bottom)
ours
ours w/o mapping
ours w/o φ
DGTS"
TRANSFERABILITY,0.3939393939393939,"Figure 9: Transferring spatially-variant geometric details using various methods. Small to severe distortions are
introduced when removing different components of the proposed transferable IDF. Thanks to the combination
of global/local query feature, our method transfers spatially-variant details while Hertz et al. (2020) can only
handle spatially-invariant isometric details."
TRANSFERABILITY,0.39730639730639733,"source
learned source base"
TRANSFERABILITY,0.4006734006734007,"target
learned target base"
TRANSFERABILITY,0.40404040404040403,"learned source base +
displacement"
TRANSFERABILITY,0.4074074074074074,"target base +
transferred displacement"
TRANSFERABILITY,0.4107744107744108,"source
learned source base"
TRANSFERABILITY,0.41414141414141414,"target
learned target base
target base +
transferred displacement"
TRANSFERABILITY,0.4175084175084175,"learned source base +
displacement"
TRANSFERABILITY,0.4208754208754209,"Figure 10: Transferable IDF applied to detail transfer. Left: the base shape is provided and lies closely to the
ground truth detailed surface; right: only the detailed shapes are provided, thus the base and the displacements
need to estimated jointly.
and simply use concatenation of φ (x) and ¯f (x) as the inputs to N ωD; for the latter we directly
use the normal at the sampled position as the context descriptor, i.e. φ (x) = ∇f (x). As Figure 9
shows, the removal of mapping net and φ lead to different degrees of feature distortions. We also
compare with the DGTS (Hertz et al., 2020), which fails completely at this example since it only
consumes local intrinsic features. Furthermore, the effect of scaling ¯f is shown in Figure 8, where
using unscaled f as input to T ωD leads to artifacts at the boundary."
CONCLUSION AND LIMITATIONS,0.42424242424242425,"5
CONCLUSION AND LIMITATIONS"
CONCLUSION AND LIMITATIONS,0.4276094276094276,"Figure 8: Detail trans-
fer without scaling ¯f."
CONCLUSION AND LIMITATIONS,0.43097643097643096,"In this paper, we proposed a new parameterization of neural implicit func-
tions for detailed geometry representation. Extending displacement mapping,
a classic shape modeling technique, our formulation represents a given shape
by a smooth base surface and a high-frequency displacement ﬁeld that offsets
the base surface along its normal directions. This resulting frequency parti-
tion enables the network to concentrate on regions with rich geometric details,
signiﬁcantly boosting its representational power. Thanks to the theoretically
grounded network design, the high-frequency signal is well constrained, and
as a result our model shows convergence qualities compared to other models
leveraging high-frequency signals, such as SIREN and positional encoding.
Furthermore, emulating the deformation-invariant quality of classic displace-
ment mapping, we extend our method to enable transferability of the implicit displacements, thus
making it possible to use implicit representations for new geometric modeling tasks."
CONCLUSION AND LIMITATIONS,0.43434343434343436,"A limitation of our detail transfer application is the necessity to pre-align the two shapes. In future
work, we consider exploring sparse correspondences as part of the input, which is a common practice
in computer graphics applications, to facilitate subsequent automatic shape alignment."
CONCLUSION AND LIMITATIONS,0.4377104377104377,ACKNOWLEDGMENTS
CONCLUSION AND LIMITATIONS,0.44107744107744107,This work is sponsored by Apple’s AI/ML PhD fellowship program.
REFERENCES,0.4444444444444444,REFERENCES
REFERENCES,0.4478114478114478,"Sketchfab. https://sketchfab.com, 2021."
REFERENCES,0.4511784511784512,"Stanford 3d scan repository.
http://graphics.stanford.edu/data/3Dscanrep/,
2021."
REFERENCES,0.45454545454545453,"Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman.
Frequency bias in neural networks for input of non-uniform density. In International Conference
on Machine Learning, pp. 685–694. PMLR, 2020."
REFERENCES,0.45791245791245794,"Sema Berkiten, Maciej Halber, Justin Solomon, Chongyang Ma, Hao Li, and Szymon Rusinkiewicz.
Learning detail transfer based on geometric features. Computer Graphics Forum, 36(2):361–373,
2017."
REFERENCES,0.4612794612794613,"Henning Biermann, Ioana Martin, Fausto Bernardini, and Denis Zorin. Cut-and-paste editing of
multiresolution surfaces. ACM Transactions on Graphics (TOG), 21(3):312–321, 2002."
REFERENCES,0.46464646464646464,"Mario Botsch, Leif Kobbelt, Mark Pauly, Pierre Alliez, and Bruno L´evy. Polygon mesh processing.
CRC press, 2010."
REFERENCES,0.468013468013468,"Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, and
Richard Newcombe. Deep local shapes: Learning local sdf priors for detailed 3d reconstruc-
tion. In European Conference on Computer Vision, pp. 608–625. Springer, Springer International
Publishing, 2020."
REFERENCES,0.4713804713804714,"Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Pe-
riodic implicit generative adversarial networks for 3d-aware image synthesis.
arXiv preprint
arXiv:2012.00926, 2020."
REFERENCES,0.47474747474747475,"Xiaobai Chen, Tom Funkhouser, Dan B Goldman, and Eli Shechtman.
Non-parametric texture
transfer using meshmatch. Technical Report Technical Report 2012-2, 2012."
REFERENCES,0.4781144781144781,"Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local
implicit image function. arXiv preprint arXiv:2012.09161, 2020a."
REFERENCES,0.48148148148148145,"Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for generative shape modeling. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5939–5948, 2019."
REFERENCES,0.48484848484848486,"Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. Bsp-net: Generating compact meshes via binary
space partitioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 45–54, 2020b."
REFERENCES,0.4882154882154882,"Zhiqin Chen, Vladimir Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, and Siddhartha Chaud-
huri.
DecorGAN: 3d shape detailization by conditional reﬁnement.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021."
REFERENCES,0.49158249158249157,"Robert L Cook. Shade trees. In Proceedings of the 11th annual conference on Computer graphics
and interactive techniques, pp. 223–231, 1984."
REFERENCES,0.494949494949495,"Robert L Cook, Loren Carpenter, and Edwin Catmull. The reyes image rendering architecture. ACM
SIGGRAPH Computer Graphics, 21(4):95–102, 1987."
REFERENCES,0.4983164983164983,"Boyang Deng, John P Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey Hinton, Mohammad
Norouzi, and Andrea Tagliasacchi. Nasa: neural articulated shape approximation. arXiv preprint
arXiv:1912.03207, 2019."
REFERENCES,0.5016835016835017,"Boyang Deng, Kyle Genova, Soroosh Yazdani, Soﬁen Bouaziz, Geoffrey Hinton, and Andrea
Tagliasacchi. Cvxnet: Learnable convex decomposition. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 31–44, 2020."
REFERENCES,0.5050505050505051,"Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries, Aaron Courville,
and Yoshua Bengio. Feature-wise transformations. Distill, 3(7):e11, 2018."
REFERENCES,0.5084175084175084,"Sarah F Frisken, Ronald N Perry, Alyn P Rockwood, and Thouis R Jones. Adaptively sampled
distance ﬁelds: A general representation of shape for computer graphics. In Proceedings of the
27th annual conference on Computer graphics and interactive techniques, pp. 249–254, 2000."
REFERENCES,0.5117845117845118,"Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T Freeman, and Thomas
Funkhouser. Learning shape templates with structured implicit functions. In Proceedings of
the IEEE/CVF International Conference on Computer Vision, pp. 7154–7164, 2019."
REFERENCES,0.5151515151515151,"Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep
implicit functions for 3d shape. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 4857–4866, 2020."
REFERENCES,0.5185185185185185,"Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, and Serge Belongie. Dualsdf: Semantic shape
manipulation using a two-level representation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 7631–7641, 2020."
REFERENCES,0.5218855218855218,"Amir Hertz, Rana Hanocka, Raja Giryes, and Daniel Cohen-Or. Deep geometric texture synthesis.
ACM Trans. Graph., 39(4), July 2020. ISSN 0730-0301. doi: 10.1145/3386569.3392471. URL
https://doi.org/10.1145/3386569.3392471."
REFERENCES,0.5252525252525253,"Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, and Daniel Cohen-Or.
Progressive
encoding for neural optimization. arXiv preprint arXiv:2104.09125, 2021."
REFERENCES,0.5286195286195287,"Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nießner, Thomas
Funkhouser, et al.
Local implicit grid representations for 3d scenes.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6001–6010, 2020."
REFERENCES,0.531986531986532,"Manyi Li and Hao Zhang. D2im-net: Learning detail disentangled implicit ﬁelds from single images.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021."
REFERENCES,0.5353535353535354,"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse
voxel ﬁelds. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 15651–15663. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
b4b758962f17808746e9bb832a6fa4b8-Paper.pdf."
REFERENCES,0.5387205387205387,"Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016."
REFERENCES,0.5420875420875421,"Julien NP Martel, David B Lindell, Connor Z Lin, Eric R Chan, Marco Monteiro, and Gordon
Wetzstein. Acorn: Adaptive coordinate networks for neural scene representation. arXiv preprint
arXiv:2105.02788, 2021."
REFERENCES,0.5454545454545454,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Oc-
cupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 4460–4470, 2019."
REFERENCES,0.5488215488215489,"Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In European
Conference on Computer Vision, pp. 405–421. Springer, Springer International Publishing, 2020."
REFERENCES,0.5521885521885522,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumet-
ric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3504–3515, 2020."
REFERENCES,0.5555555555555556,"Yutaka Ohtake, Alexander Belyaev, Marc Alexa, Greg Turk, and Hans-Peter Seidel. Multi-level
partition of unity implicits. ACM Trans. Graph., 22(3):463–470, 2003. ISSN 0730-0301. doi:
10.1145/882262.882293. URL https://doi.org/10.1145/882262.882293."
REFERENCES,0.5589225589225589,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
Deepsdf: Learning continuous signed distance functions for shape representation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 165–174,
2019."
REFERENCES,0.5622895622895623,"Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Soﬁen Bouaziz, Dan B Goldman, Steven M
Seitz, and Ricardo Martin-Brualla.
Deformable neural radiance ﬁelds.
arXiv preprint
arXiv:2011.12948, 2020a."
REFERENCES,0.5656565656565656,"Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Soﬁen Bouaziz, Dan B Goldman, Steven M.
Seitz, and Ricardo Martin-Brualla.
Deformable neural radiance ﬁelds.
arXiv preprint
arXiv:2011.12948, 2020b."
REFERENCES,0.569023569023569,"Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolu-
tional occupancy networks. In European Conference on Computer Vision (ECCV), Cham, August
2020. Springer International Publishing."
REFERENCES,0.5723905723905723,"Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual
reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, 2018."
REFERENCES,0.5757575757575758,"Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural
radiance ﬁelds for dynamic scenes. arXiv preprint arXiv:2011.13961, 2020."
REFERENCES,0.5791245791245792,"Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural
radiance ﬁelds for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10318–10327, 2021."
REFERENCES,0.5824915824915825,"Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Confer-
ence on Machine Learning, pp. 5301–5310. PMLR, 2019."
REFERENCES,0.5858585858585859,"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li.
Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceed-
ings of the IEEE/CVF International Conference on Computer Vision, pp. 2304–2314, 2019."
REFERENCES,0.5892255892255892,"Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned
implicit function for high-resolution 3d human digitization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 84–93, 2020."
REFERENCES,0.5925925925925926,"Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wetzstein. Scene representation networks: Con-
tinuous 3d-structure-aware neural scene representations. arXiv preprint arXiv:1906.01618, 2019."
REFERENCES,0.5959595959595959,"Vincent
Sitzmann,
Julien
Martel,
Alexander
Bergman,
David
Lindell,
and
Gordon
Wetzstein.
Implicit
neural
representations
with
periodic
activation
functions.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in
Neural Information Processing Systems, volume 33, pp. 7462–7473. Curran Associates,
Inc.,
2020.
URL
https://proceedings.neurips.cc/paper/2020/file/
53c04118df112c13a8c34b38343b9c10-Paper.pdf."
REFERENCES,0.5993265993265994,"Ivan Skorokhodov, Savva Ignatyev, and Mohamed Elhoseiny. Adversarial generation of continuous
images. arXiv preprint arXiv:2011.12026, 2020."
REFERENCES,0.6026936026936027,"Olga Sorkine and Mario Botsch. Tutorial: Interactive shape modeling and deformation. In EURO-
GRAPHICS, 2009."
REFERENCES,0.6060606060606061,"Olga Sorkine, Daniel Cohen-Or, Yaron Lipman, Marc Alexa, Christian R¨ossl, and H-P Seidel.
Laplacian surface editing. In Proceedings of the 2004 Eurographics/ACM SIGGRAPH sympo-
sium on Geometry processing, pp. 175–184, 2004."
REFERENCES,0.6094276094276094,"Kenshi Takayama, Ryan Schmidt, Karan Singh, Takeo Igarashi, Tamy Boubekeur, and Olga Sorkine.
Geobrush: Interactive mesh geometry cloning. Computer Graphics Forum, 30(2):613–622, 2011."
REFERENCES,0.6127946127946128,"Towaki
Takikawa,
Joey
Litalien,
Kangxue
Yin,
Karsten
Kreis,
Charles
Loop,
Derek
Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of
detail: Real-time rendering with implicit 3D shapes. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2021."
REFERENCES,0.6161616161616161,"Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh
Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn
high frequency functions in low dimensional domains. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 7537–7547. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/55053683268957697aa39fba6f231c68-Paper.pdf."
REFERENCES,0.6195286195286195,"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh¨ofer, Carsten Stoll, and Christian
Theobalt. Patchnets: Patch-based generalizable deep implicit 3d shape representations. In Euro-
pean Conference on Computer Vision, pp. 293–309. Springer, Springer International Publishing,
2020."
REFERENCES,0.622895622895623,"Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in
frequency domain. In International Conference on Neural Information Processing, pp. 264–274.
Springer, 2019."
REFERENCES,0.6262626262626263,"Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis.
arXiv preprint arXiv:1808.04295, 2018."
REFERENCES,0.6296296296296297,"Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, and Xun Cao.
Facescape: a large-scale high quality 3d face dataset and detailed riggable 3d face prediction. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020."
REFERENCES,0.632996632996633,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lip-
man. Multiview neural surface reconstruction by disentangling geometry and appearance. Ad-
vances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.6363636363636364,"Wang Yifan, Noam Aigerman, Vladimir G Kim, Siddhartha Chaudhuri, and Olga Sorkine-Hornung.
Neural cages for detail-preserving 3d deformations. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 75–83, 2020."
REFERENCES,0.6397306397306397,"Lexing Ying, Aaron Hertzmann, Henning Biermann, and Denis Zorin. Texture and shape synthesis
on surfaces. In Eurographics Workshop on Rendering Techniques, pp. 301–312. Springer, 2001."
REFERENCES,0.6430976430976431,"Howard Zhou, Jie Sun, Greg Turk, and James M Rehg. Terrain synthesis from digital elevation
models. IEEE transactions on visualization and computer graphics, 13(4):834–848, 2007."
REFERENCES,0.6464646464646465,"Kun Zhou, Xin Huang, Xi Wang, Yiying Tong, Mathieu Desbrun, Baining Guo, and Heung-Yeung
Shum. Mesh quilting for geometric texture synthesis. In ACM SIGGRAPH 2006 Papers, SIG-
GRAPH ’06, pp. 690–697, New York, NY, USA, 2006. Association for Computing Machinery.
ISBN 1595933646. doi: 10.1145/1179352.1141942. URL https://doi.org/10.1145/
1179352.1141942."
REFERENCES,0.6498316498316499,"Qingnan Zhou and Alec Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. arXiv
preprint arXiv:1605.04797, 2016."
REFERENCES,0.6531986531986532,"A
PROOFS"
REFERENCES,0.6565656565656566,"Theorem 1. If function f : Rn →R is differentiable, Lipschitz-continuous with constant L and
Lipschitz-smooth with constant M, then ∥∇f (x + δ ∇f (x)) −∇f (x) ∥≤|δ|LM."
REFERENCES,0.6599326599326599,"Proof. If a differentiable function f is Lipschitz-continuous with constant L and Lipschitz-smooth
with constant M, then"
REFERENCES,0.6632996632996633,"∥∇f (x) ∥≤L
(6)
∥∇f (x) −∇f (y) ∥≤M∥x −y∥.
(7)"
REFERENCES,0.6666666666666666,"∥∇f (x + δ ∇f (x)) −∇f (x) ∥≤M∥δ ∇f (x) ∥
by equation 7
≤|δ|LM
by equation 6"
REFERENCES,0.67003367003367,"Corollary 1. If a signed distance function f satisfying the eikonal equation up to error ϵ > 0,
∥∇f∥−1
 < ϵ, is Lipschitz-smooth with constant M, then ∥∇f (x + δ ∇f (x)) −∇f (x) ∥<"
REFERENCES,0.6734006734006734,(1 + ϵ)|δ|M.
REFERENCES,0.6767676767676768,Proof.
REFERENCES,0.6801346801346801,"∥∇f∥−1
 < ϵ
⇒
∥∇f∥< ϵ + 1. This means f is Lipschitz-continuous with constant"
REFERENCES,0.6835016835016835,"ϵ + 1. Then by Theorem 1, ∥∇f (x + δ∇f (x)) −∇f (x) ∥< |δ|(1 + ϵ)M."
REFERENCES,0.6868686868686869,"Finally we show the upper bound for the normalized gradient, i.e.,"
REFERENCES,0.6902356902356902,∥ˆn −n∥≤1 + ϵ
REFERENCES,0.6936026936026936,"1 −ϵ |δ| M,
(8)"
REFERENCES,0.696969696969697,"where n =
∇f(x)
∥∇f(x)∥, ˆn =
∇f(ˆx)
∥∇f(ˆx)∥and ˆx = x+d (x) n with d (x) denoting the small displacement."
REFERENCES,0.7003367003367004,Proof.
REFERENCES,0.7037037037037037,"∥ˆn −n∥=

∇f (ˆx)
∥∇f (ˆx)∥−
∇f (x)
∥∇f (x)∥ .
(9)"
REFERENCES,0.7070707070707071,"For brevity, we denote ∇f (ˆx) and ∇f (x) as u and v. Without loss of generality, we assume
∥u∥≤∥v∥. Then"
REFERENCES,0.7104377104377104,"∥ˆn −n∥=

u
∥u∥−
v
∥v∥ (10)"
REFERENCES,0.7138047138047138,"(∗)
≤
1
∥u∥∥u −v∥
(11)"
REFERENCES,0.7171717171717171,"≤
1
1 −ϵ∥u −v∥
by Eikonal constraint
(12)"
REFERENCES,0.7205387205387206,"=
1
1 −ϵ ∥∇f (ˆx) −∇f (x)∥
(13)"
REFERENCES,0.7239057239057239,"=
1
1 −ϵ"
REFERENCES,0.7272727272727273,"∇f

x + d (x) ∇f (x)"
REFERENCES,0.7306397306397306,∥∇f (x)∥
REFERENCES,0.734006734006734,"
−∇f (x)
 .
(14)"
REFERENCES,0.7373737373737373,"Since |d (x)| is a small and ∥∇f (x)∥is close to 1, we can set δ =
d (x)
∥∇f (x)∥. Thereby using"
REFERENCES,0.7407407407407407,"Corollary 1, we conclude"
REFERENCES,0.7441077441077442,"1
1 −ϵ"
REFERENCES,0.7474747474747475,"∇f

x + d (x) ∇f (x)"
REFERENCES,0.7508417508417509,∥∇f (x)∥
REFERENCES,0.7542087542087542,"
−∇f (x)
 ≤1 + ϵ"
REFERENCES,0.7575757575757576,"1 −ϵ|δ|M,
(15) thus"
REFERENCES,0.7609427609427609,∥ˆn −n∥≤1 + ϵ
REFERENCES,0.7643097643097643,"1 −ϵ|δ|M.
(16)"
REFERENCES,0.7676767676767676,Eq. equation 11 can be proved as follows
REFERENCES,0.7710437710437711,"d
z
}|
{
u
∥u∥−
v
∥v∥  = "
REFERENCES,0.7744107744107744,"d′
z
}|
{
u
∥u∥−
v
∥u∥+"
REFERENCES,0.7777777777777778,"e
z
}|
{
( v"
REFERENCES,0.7811447811447811,"∥u∥−
v
∥v∥) "
REFERENCES,0.7845117845117845,",
(17)"
REFERENCES,0.7878787878787878,"which depicts the distance of the unit sphere projections of u and v. Obviously, as shown in
Figure 11, ∥d∥≤∥d′∥if ∢(d, e) ≥90◦."
REFERENCES,0.7912457912457912,"Since e = (
1
∥u∥−
1
∥v∥)v and (
1
∥u∥−
1
∥v∥) ≥0), to show that ∢⟨d, e⟩≥90◦is the same as to show
that ∢⟨d, v⟩≥90◦. Indeed:"
REFERENCES,0.7946127946127947,Figure 11: Sketch for proof.
REFERENCES,0.797979797979798,"⟨d, v⟩=
 u"
REFERENCES,0.8013468013468014,"∥u∥−
v
∥v∥, v

(18)"
REFERENCES,0.8047138047138047,"= ⟨u, v⟩"
REFERENCES,0.8080808080808081,"∥u∥−∥v∥
(19)"
REFERENCES,0.8114478114478114,≤∥u∥∥v∥
REFERENCES,0.8148148148148148,"∥u∥
−∥v∥
by Cauchy-Schwarz inequality
(20)"
REFERENCES,0.8181818181818182,"= 0
(21)"
REFERENCES,0.8215488215488216,"B
ADDITIONAL EXPERIMENTS AND RESULTS."
REFERENCES,0.8249158249158249,"B.1
ADDITIONAL INFORMATION TO THE COMPARISON IN SEC 4.1"
REFERENCES,0.8282828282828283,"B.1.1
ADDITIONAL EVALUATION RESULTS"
REFERENCES,0.8316498316498316,"Below we show the per-model Chamfer distances in addition to the average shown in Table 1. More
qualitative results are shown in Figure 16."
REFERENCES,0.835016835016835,Chamfer distance points to point distance ·10−3 / normal cosine distance ·10−2
REFERENCES,0.8383838383838383,"model
Progressive
FFN
NGLOD
(LOD4)
NGLOD
(LOD6)
SIREN-3
ω = 60
SIREN-7
ω = 30
SIREN-7
ω = 60
Direct
Residual
D-SDF
Ours"
REFERENCES,0.8417508417508418,"angel
6.00/4.19
2.28/2.87
1.47/1.43
9.54/5.47
5.57/2.85
-/-
251/87.9
3.36/3.70
1.30/0.89
asian dragon
4.96/4.02
1.66/4.05
1.03/1.91
6.13/5.28
3.65/2.36
7.24/4.03
269/92.7
2.84/1.80
0.93/1.36
camera
4.62/1.51
1.56/1.15
1.32/0.62
6.50/2.38
4.11/1.10
-/-
281/38.5
1.99/2.01
1.25/0.34
compressor
5.55/1.35
1.64/0.88
1.52/0.44
8.83/2.82
4.63/0.82
-/-
330/56.9
2.66/3.71
1.39/0.23
dragon
5.10/4.00
1.80/3.77
1.39/2.37
7.04/4.75
3.80/2.49
-/-
263/76.4
2.68/1.21
1.24/1.50
dragon warrior
5.94/7.25
2.46/8.12
1.52/5.21
7.27/8.45
3.68/4.83
5.96/8.01
6.09/9.77
2.22/4.46
1.47/4.56
dragon wing
5.68/5.41
2.01/4.98
1.47/2.87
6.40/5.46
7.92/3.84
-/-
167/76.7
2.66/4.09
1.31/1.49
dragon china
6.20/2.31
2.32/1.75
1.39/1.01
9.15/4.35
6.20/2.29
-/-
272/69.7
3.31/7.06
1.40/0.51
dragon cup
4.39/3.13
1.83/3.57
1.24/1.17
7.67/4.69
5.50/2.15
-/-
173/86.9
3.21/4.93
1.10/0.51
helmet
4.79/1.02
1.70/0.871
1.40/0.410
8.40/2.72
5.19/0.83
-/-
263/94.6
2.59/1.99
1.29/0.13
hunter
4.17/4.66
2.03/5.08
1.18/2.08
8.96/6.66
3.40/2.58
-/-
3.39/3.64
2.61/4.89
0.91/1.14
luyu
7.22/4.29
2.19/3.30
1.53/1.81
8.98/6.83
6.16/3.16
-/-
206/94.6
5.10/9.76
1.28/1.02
pearl dragon
7.48/5.97
2.37/6.10
1.49/2.67
10.1/9.56
5.05/4.07
-/-
66.1 /49.8
3.26/6.24
1.30/1.43
ramesses
4.24/2.30
1.47/2.47
0.97/1.93
6.40/3.77
4.20/2.33
-/-
3.78/6.60
3.16/9.66
0.92/1.58
Thai Statue
5.23/7.01
7.16/16.7
1.30/4.77
6.27/7.48
3.81/4.17
-/-
117/45.2
1.76/2.46
1.07/2.92
Vase Lion
5.92/1.93
1.86/2.18
1.39/0.77
39.9/25.6
4.68/1.02
-/-
227/54.8
2.26/2.31
1.31/0.43
AVG
5.47/3.77
2.27/4.24
1.35/1.97
9.85/6.64
4.85/2.56
-/6.02
181/59.0
2.85/4.39
1.22/1.25"
REFERENCES,0.8451178451178452,Table 4: Detailed quantitative evaluation (corresponding to Table 1).
REFERENCES,0.8484848484848485,"B.1.2
ANALYSIS"
REFERENCES,0.8518518518518519,"We provide further diagnosis for the underwhelming results from the direct residual and the D-SDF
models. Direct residual composes the ﬁnal SDF as a simple sum of the base SDF and a residual
value. We train this model also with the attenuation and scaled tanh activation, and supervise
both the base SDF and the composed SDF to stablize the base prediction. However, as shown in
Figure 12a, the composed SDF often contain large structural errors. Further inspections show that
during the training, such structural errors change quickly and the reconstruction oscillates in scale."
REFERENCES,0.8552188552188552,"a. direct residual
b. D-SDF"
REFERENCES,0.8585858585858586,Figure 12: Examples of the direct residual and D-SDF models.
REFERENCES,0.8619528619528619,"Chamfer distance
points to point distance / normal cosine distance ·10−2"
REFERENCES,0.8653198653198653,"training points
noise σ
ours
poisson
reconstruction"
REFERENCES,0.8686868686868687,"40000
0.002
1.07/7.54
1.08/7.78
40000
0.005
1.05/7.57
1.08/7.82
400000
0.002
1.00/6.01
1.04/6.63
400000
0.005
1.00/5.99
1.04/6.60"
REFERENCES,0.8720538720538721,Table 5: Quantitative evaluation given sparse and noisy inputs.
REFERENCES,0.8754208754208754,"These indicate that the without the directional constraints on the displacement, the two networks are
not sufﬁciently coupled and interfere with each other during training."
REFERENCES,0.8787878787878788,"In D-SDF the displacement is not enforced to be along the normal direction. D-SDF yields compet-
itive quantitative result, but as Figure 12b shows, the predicted displacement vectors are homoge-
neous, indicating that due to the lack of constraints the displacement network is not incentivised to
return meaningful outputs."
REFERENCES,0.8821548821548821,"B.2
DISCUSSION ABOUT ωB AND ωD"
REFERENCES,0.8855218855218855,"As pointed out in Section 3.2, ω controls the upper bound of the frequency the network is capable of
representing. When using a single SIREN, a larger ω can represent higher frequencies (more details)
but at the same time tends to create high-frequency artifacts and issues with convergence. Therefore,
the choice of ωB and ωD is guided solely by two simple criteria: 1) ωB should be relatively small to
provide a smooth and stably trainable base surface. 2) ωD should be sufﬁciently large to represent
the amount of details observed in the given shape."
REFERENCES,0.8888888888888888,"Based on the empirical experience of the previous work Sitzmann et al. (2020), for a baseline SIREN
network, ω = 30 provides a good balance for stability and detail representation. Based on this
value, we choose ωB = 15, so that the base is smoother than the input surface, thereby creating a
necessity for the displacement ﬁeld; ωD = 60 is chosen empirically as a value that is capable of
representing the high-frequency signals exhibited in the high-resolutions shapes we tested. If the
input shape is very simple and smooth (e.g. the shape in the ﬁrst row of Figure 13), the base SDF
with ωB = 15 is already sufﬁcient to represent the groundtruth surface, and the displacement has
little impact. In order to enforce a frequency separation as in the detail transfer application, one can
reduce ωB (e.g. to 5, as shown in Figure 13(b)). For very detailed surfaces, ωD needs to be high
enough to enable sufﬁcient resolution of the displacement ﬁeld. We choose ωD = 60, which is
suitable for all the tested shapes. When keeping ωD ﬁxed, varying ωB determines the smoothness
of the base, therefore also decides how much correction the displacement network must deliver. If
ωB is too small, the displacement network can become overburdened with the task, leading to faulty
reconstruction and training instabilities."
REFERENCES,0.8922558922558923,"B.3
STRESS TESTS"
REFERENCES,0.8956228956228957,"While we used dense and clean sampled point clouds as inputs in the paper, as our focus is on detail
representation, we examine the behavior of our method under noisy and sparse inputs. Speciﬁcally,
we train our network with 400 thousand and 40 thousand sampled points (10% and 1% of the amount
in our main experiment, respectively), and added σ = 0.002 and σ = 0.005 Gaussian noise on both"
REFERENCES,0.898989898989899,"base
composed
base
composed"
REFERENCES,0.9023569023569024,"groundtruth
(a) ωB = 15, ωD = 60
(b) ωB = 5, ωD = 15"
REFERENCES,0.9057239057239057,"base
composed
base
composed"
REFERENCES,0.9090909090909091,"groundtruth
(c) ωB = 30, ωD = 60
(d) ωB = 5, ωD = 60"
REFERENCES,0.9124579124579124,"Figure 13: Effect of ωB and ωD demonstrated on meshes with different level of details. For smooth low-
resolution meshes such as the example shown in the ﬁrst row, a small ω sufﬁces to represent all the details in
the given mesh. In this case, the base SDF (e.g. ωB = 15) alone can accurately express the surface geometry,
rendering the displacement network unnecessary, as a result the composed surface is indistinguishable from
the base surface, as shown in (a). To enforce detail separation, one can reduce ωB, e.g. to 5, as shown in (b).
On the other hand, given detailed meshes (such as in the second row), ωD ought to be large enough to be able
to capture the high-frequency signals. When keeping ωD ﬁxed, reducing ωB increases the smoothness of the
base SDF, as shown in (c) and (d), therefore also increases how much correction the displacement network
must deliver. In the extreme case, ωB = 0, we would have a single high-frequency SIREN, which is subject to
convergence issues, as shown in Tab 4."
REFERENCES,0.9158249158249159,Figure 14: Qualitative evaluation given sparse and noisy inputs.
REFERENCES,0.9191919191919192,"the point normals and the point positions. From the qualitative and quantitative results shown in
Figure 14 and Table 5, we can see that our method recovers geometric details better than Poisson
reconstruction given sufﬁcient training data (c.f. the left half of the ﬁgure). When the training
sample is sparse, our method tends to generate more high-frequency noise as a result of overﬁtting."
REFERENCES,0.9225589225589226,source
REFERENCES,0.9259259259259259,"learned
source base"
REFERENCES,0.9292929292929293,learned source
REFERENCES,0.9326599326599326,"learned
target base
targets with
detail from the source
targets"
REFERENCES,0.936026936026936,"Figure 15: Detail transfer to multiple similarly aligned target shapes. Given a (detailed) source shape, we train a
base network N ωB to represent the smooth base surface (see learned source base), as well as a feature extractor
φ and a transferrable displacement network T ωD to represent the surface details. During detail transfer, we
only need to ﬁt the lightweight base network for each new target, while the feature extractor and displacement
net can be applied to the new shapes without adaptation."
REFERENCES,0.9393939393939394,"B.4
DETAIL TRANSFER."
REFERENCES,0.9427609427609428,"The proposed transferable IDF makes it possible to deploy φ and T ωD a new base surface without
any ﬁne-tuning or adaptation. In other words, to transfer details to multiple targets, we only need to
ﬁt a new base network N ωB
new. The composed SDF ˆf (x) can be computed simply by replacing N ωB
with N ωB
new. We show the result of a multi-target detail transfer in Figure 15. It is worth mentioning
that even though the point extractor is trained on a single source shape, it is able to generalize across
different identities thanks to the built-in scale and translation invariance."
REFERENCES,0.9461279461279462,"B.5
INFERENCE AND TRAINING TIME"
REFERENCES,0.9494949494949495,"Training the models as described in the paper takes 2412 seconds (around 40 minutes), which
amounts to 120 epochs, i.e., around 20 seconds per epoch, where each epoch comprises 4 million
surface samples and 4 million off-surface samples. In comparison, the original implementation of
NGLOD6 takes 110 minutes to train 250 epochs, where each epoch comprises 200000 surface sam-
ples and 300000 off-surface samples. As for inference, using the same evaluation setup, NGLOD6
takes 193.9s for 5123 points, while our inference takes 250.06 seconds for 5123 query points. All
benchmarking is performed on a single Nvidia 2080 RTX GPU. These timings could be improved
by optimizing the model for performance, which we did not. For example, instead of using autodiff
for computing the base surface normals, one could exploit the fact that the differentiation of SIREN is
also a SIREN and explicitly construct a computation graph for computing the base surface normals."
REFERENCES,0.9528619528619529,"FFN
SIREN-3
ω = 60
SIREN-7
ω = 30
direct reisdual
D-SDF"
REFERENCES,0.9562289562289562,"NGLOD
LOD4
NGLOD
LOD6
ours
ground truth"
REFERENCES,0.9595959595959596,"FFN
SIREN-3
ω = 60
SIREN-7
ω = 30
direct reisdual
D-SDF"
REFERENCES,0.9629629629629629,"NGLOD
LOD4
NGLOD
LOD6
ours
ground truth"
REFERENCES,0.9663299663299664,"FFN
SIREN-3
ω = 60
SIREN-7
ω = 30
direct reisdual
D-SDF"
REFERENCES,0.9696969696969697,"NGLOD
LOD4
NGLOD
LOD6
ours
ground truth"
REFERENCES,0.9730639730639731,"Figure 16: Comparison of detail reconstruction (better viewed with zoom-in). Methods that did not converge
are omitted in the visual comparison."
REFERENCES,0.9764309764309764,"FFN
SIREN-3
ω = 60
SIREN-7
ω = 30
direct reisdual
D-SDF"
REFERENCES,0.9797979797979798,"NGLOD
LOD4
NGLOD
LOD6
ours
ground truth"
REFERENCES,0.9831649831649831,"FFN
SIREN-3
ω = 60
SIREN-7
ω = 30
direct reisdual
D-SDF"
REFERENCES,0.9865319865319865,"NGLOD
LOD4
NGLOD
LOD6
ours
ground truth"
REFERENCES,0.98989898989899,"FFN
SIREN-3
ω = 60
SIREN-7
ω = 30
direct reisdual
D-SDF"
REFERENCES,0.9932659932659933,"NGLOD
LOD4
NGLOD
LOD6
ours
ground truth"
REFERENCES,0.9966329966329966,"Figure 16: (Cont.) Comparison of detail reconstruction (better viewed with zoom-in). Methods that did not
converge are omitted in the visual comparison."
