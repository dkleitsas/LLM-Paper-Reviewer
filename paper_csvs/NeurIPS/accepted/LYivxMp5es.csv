Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005042864346949068,"In this study, we investigate the under-explored intervention planning aimed at
disseminating accurate information within dynamic opinion networks by leveraging
learning strategies. Intervention planning involves identifying key nodes (search)
and exerting control (e.g., disseminating accurate/official information through the
nodes) to mitigate the influence of misinformation. However, as the network size
increases, the problem becomes computationally intractable. To address this, we
first introduce a ranking algorithm to identify key nodes for disseminating accurate
information, which facilitates the training of neural network (NN) classifiers that
provide generalized solutions for the search and planning problems. Second, we
mitigate the complexity of label generation—which becomes challenging as the
network grows—by developing a reinforcement learning (RL)-based centralized
dynamic planning framework. We analyze these NN-based planners for opinion
networks governed by two dynamic propagation models. Each model incorporates
both binary and continuous opinion and trust representations. Our experimental
results demonstrate that the ranking algorithm-based classifiers provide plans
that enhance infection rate control, especially with increased action budgets for
small networks. Further, we observe that the reward strategies focusing on key
metrics, such as the number of susceptible nodes and infection rates, outperform
those prioritizing faster blocking strategies. Additionally, our findings reveal that
graph convolutional network (GCN)-based planners facilitate scalable centralized
plans that achieve lower infection rates (higher control) across various network
configurations (e.g., Watts-Strogatz topology, varying action budgets, varying
initial infected nodes, and varying degree of infected nodes)."
INTRODUCTION,0.0010085728693898135,"1
Introduction"
INTRODUCTION,0.0015128593040847202,"The spread of information across social networks profoundly impacts public opinion, collective
behaviors, and societal outcomes [1]. Especially during crises such as disease outbreaks or disasters,
there is often too much information coming from different sources. Sometimes, the resultant flood
of information is unreliable or misleading, or spreads too quickly, which can have serious effects
on society and health [8]. Online platforms such as Facebook, X, and WeChat, while essential
for communication, significantly contribute to the rapid spread of misinformation [2]. This has
led to public confusion and panic in events ranging from the Fukushima disaster to the COVID-
19 pandemic, and even the U.S. presidential elections [11], demonstrating the need for effective
information management on these platforms [8]."
INTRODUCTION,0.002017145738779627,"The first step in combating misinformation propagation is to reliably detect them. However, detec-
tion alone is insufficient to effectively mitigate its spread. It must be complemented with strategic"
INTRODUCTION,0.0025214321734745334,"intervention planning to contain its impact. In this context, numerous studies have focused on rumor
detection [53, 47, 40, 19], while comprehensive strategies for controlling misinformation are limited
[15]. Existing research works on controlling misinformation emphasizes three primary strategies:
node removal [9, 45, 29], edge removal [21, 19, 44], and counter-rumor dissemination [5, 42, 12].
Node removal involves identifying and neutralizing key nodes using community detection methods,
with dynamic models that adapt to changes in propagation. For example, [50, 16] present ranking
algorithms that are critical for identifying influential nodes within complex networks, which can then
be targeted to block, remove, or cascade information, to reduce the overall spread of misinformation.
These algorithms rank nodes based on various metrics, determining their importance or influence
within the network. The second approach, edge removal, focuses on disrupting misinformation path-
ways by strategically severing connections between nodes. For example, authors in [38] considered
mitigating misinformation by identifying potential purveyors to block their posts. However, taking
strong measures like censoring user posts may violate user rights."
INTRODUCTION,0.0030257186081694403,"The third strategy, counter-rumor dissemination, promotes the spread of factual information, leverag-
ing user participation and ‘positive cascades’ to counteract misinformation [41]. For instance, authors
in [13] developed a method that involves learning an intervention model to enhance the spread of true
news, thereby reducing the influence of fake news. The effectiveness of such intervention planning
methods relies on the ability of intervention models to identify key nodes and disseminate accu-
rate/official information through these nodes to mitigate the influence of misinformation. Following
this strategy, in [34], a search problem was formulated and sequential plans using automated planning
techniques were generated for the targeted spread of information. Despite several model-based efforts
to strengthen this approach (see Appendix A.1 for a review of relevant literature), the existing research
often overlooks key features of opinion propagation models, such as their rich network dynamics,
asynchronous communication, and the impact of factors like the degree of infected nodes, action
budget, and various reward models on the effectiveness of planners."
INTRODUCTION,0.0035300050428643467,"We address this gap by studying the intervention planning problem using two learning methodologies.
In both these methodologies, we investigate three distinct cases of opinion network models, ranging
from discrete to continuous representations of opinion and mutual trust. Specifically, in this paper, we
propose a novel ranking algorithm integrated with a supervised learning (SL) framework to identify
influential nodes using a robust feature set of network nodes and evaluate their performance in all
three cases. Additionally, we also develop a reinforcement learning (RL)-based solution to design
centralized planners that are suitable for larger networks. Furthermore, we develop comprehensive
datasets with a wide range of Watts-Strogatz (or small-world) network topologies, varying degrees of
initial infected nodes, action budgets, and reward models, e.g., from those requiring local network
information to those utilizing global real-time network states, and analyze both the developed
methodologies. Next, we use an example to illustrate the intervention planning problem and highlight
our contributions."
INTRODUCTION,0.004034291477559254,"Example: Consider a research community discussing the NeurIPS submission deadline. In this
context, a topic is just a statement such as ‘NeurIPS submission deadline is on May 22’. An opinion
of an agent on this topic is defined as the belief of the agent in the truthfulness of the statement [3].
A positive (respectively negative) opinion value represents that the agent believes the statement is
true (respectively false). In our study, we consider the opinion value of an agent on a topic lies in
[−1, 1]. Our problem setup consists of a network of connected agents (with opinion values on a given"
INTRODUCTION,0.0045385779122541605,"Figure 1: Example of misinformation propagation and control choices at each timestep. Blue nodes:
neutral (opinion value 0), red nodes: misinformed (opinion value −1), green nodes: received accurate
information (opinion value 1)."
INTRODUCTION,0.005042864346949067,"topic) represented by a graph. We consider that a subset of agents propagate misinformation to their
neighbors. Our goal is to counteract this by disseminating accurate information to selected agents
at each time step. The agents receiving accurate information update their opinion to a level where
they no longer believe the misinformation and, consequently, cease to propagate it. The process ends
when no agents are left to receive the misinformation. Figure 1 illustrates such a scenario where the
red nodes represent the agents not believing about the NeurIPS deadline being May 22. When they
interact with other agents, they share this misinformation, leading to a spread of incorrect information
throughout the network. The blue nodes represent neutral agents who are unaware of the deadline,
and the green nodes represent agents who believe in the NeurIPS deadline being on May 22. Initially,
agents, i.e., nodes {0, 1}, propagate misinformation to their neighbors. To control the spread of this
misinformation, timely control actions are taken at each timestep to disseminate accurate information
to selected agents. In the example, with an action budget of 1, agents 7, 8, and 6 are sequentially
chosen to receive the accurate information, represented by the green nodes. Through these control
actions, the example demonstrates how timely intervention at critical points can effectively mitigate
misinformation spread, ensuring agents are accurately informed."
INTRODUCTION,0.005547150781643974,"Our paper makes several significant contributions to intervention planning, focusing on the integra-
tion of more realistic and complex modeling approaches, label generation techniques, and training
methodologies: (a) We utilize a continuous opinion scale to model the dynamics of misinformation
spread (instead of just binary scale as shown in the example), providing a more realistic representation
of opinion changes over time. (b) We develop a ranking algorithm for generating labels in networks
with discrete opinions, addressing a significant gap in efficient data preparation for SL algorithms in
this domain. (c) We develop an RL methodology and perform comprehensive analysis. This allows
for adaptive intervention strategies in response to evolving misinformation spread patterns, a critical
improvement over traditional static approaches. (d) Utilizing graph convolutional networks (GCNs)
with an enhanced feature set of opinion value, connectivity degree, and proximity to a misinformed
node, we improve the training of models for selecting effective intervention strategies. This enhance-
ment ensures scalability and generalizes well across various network structures, demonstrating the
robust capabilities of GCNs in complex scenarios. Table 3 highlights our contributions by providing a
comparative overview of the key features and innovations in our work to that of previous studies. The
code and the datasets developed as part of the analysis presented in this paper can be found in [33]."
PROBLEM FORMULATION,0.006051437216338881,"2
Problem Formulation"
PROBLEM FORMULATION,0.006555723651033787,"In this section, we discuss our approach to modeling the opinion network environment, the dynamics
of information propagation, and our strategy for containing misinformation."
ENVIRONMENT DESCRIPTION,0.0070600100857286935,"2.1
Environment Description"
ENVIRONMENT DESCRIPTION,0.007564296520423601,"An opinion network is formally represented as a directed graph G = (V, E), where V denotes the set
of nodes (agents), and E denotes the set of edges (relationships or trust) between agents [15]. The
graph structure we consider for our study is undirected, indicating that relationships between agents
are bi-directional. Each node within the graph represents an individual agent, and each agent holds a
specific opinion on a given topic. An edge between any two nodes signifies a direct connection or
relationship between those agents, facilitating the exchange of opinions."
ENVIRONMENT DESCRIPTION,0.008068582955118508,"Opinion values are quantified within the range [−1, 1], representing different levels of belief on a given
topic, and the weight assigned to each edge quantifies the mutual trust level between connected agents,
scaled within the interval [0, 1]. While existing works in the literature have explored only binary
opinion and trust models, in computational social science, researchers have developed models with
opinion and trust values as continuous variables. Investigation of planning strategies in continuous
models remains under-explored. In this paper, we explore three distinct cases of opinion and trust
values. Case-1 involves binary opinion values with binary trust, simplifying the network dynamics
into discrete states. In Case-2, we use floating-point opinion values while maintaining binary
trust, allowing for a more granular assessment of opinions while still simplifying trust dynamics.
Finally, Case-3 features both floating-point opinion values and floating-point trust, representing
more realistic opinion and trust relationships within the network capturing continuous variations."
PROPAGATION MODEL,0.008572869389813415,"2.2
Propagation Model"
PROPAGATION MODEL,0.009077155824508321,"In the analysis of opinion networks, it is essential to understand how opinions form and evolve,
guided by the dynamics of trust among agents. In our analysis, the evolution and propagation of
opinions within opinion networks are modeled using a linear adjustment mechanism (discrete linear
maps), as described by the following transition function"
PROPAGATION MODEL,0.009581442259203227,"xi(t + 1) = xi(t) + µik(xk(t) −xi(t)),
t = 0, 1, . . . .
(1)"
PROPAGATION MODEL,0.010085728693898134,"Equation 1 models the dynamics of opinion evolution, where the opinion of agent i at time t + 1
depends on its current opinion and the influence exerted by a connected agent k, who is actively
sharing some information with agent i, moderated by the trust factor µik. This asynchronous
propagation model adapts differently across various experimental setups as detailed below."
PROPAGATION MODEL,0.01059001512859304,"In Case-1 and Case-2, where mutual trust values are discrete {0, 1}, the application of Equation
1 results in immediate shifts in opinion. For example, if an agent i with a current opinion value of
0.5 on some topic is influenced by a connected neighboring agent k with an opinion value of -1 on
the same topic, agent i’s opinion immediately shifts to -1 in the next timestep, reflecting a discrete
transition. Conversely, in Case-3, which involves a continuous range of opinion and trust values,
changes are more gradual. Here, if agent i holds an opinion of 0.5 and is influenced by a neighbor k
with an opinion of -1 and a moderate trust factor, the opinion of agent i incrementally moves closer
to -1 in subsequent timesteps. This reflects a gradual shift towards a consensus opinion, depending
on the magnitude of the trust level between agents i and k."
PROPAGATION MODEL,0.011094301563287948,"To further enhance our understanding of opinion dynamics in networks with continuous trust relation-
ships, we have also used the DeGroot propagation model [6] in Case-3. The propagation of opinions
in this model is governed by the following equation:"
PROPAGATION MODEL,0.011598587997982855,"xi(t + 1) = n
X"
PROPAGATION MODEL,0.012102874432677761,"k=1
µikxk(t),
t = 0, 1, . . . .
(2)"
PROPAGATION MODEL,0.012607160867372668,"Equation 2 describes the opinion of agent i at time t + 1 as a weighted average of the opinions of
all the neighboring agents at time t, where the weights µik represent the trust agent i has in agent k.
Often, in the DeGroot model, which is a synchronous propagation model, the summation in 2 is a
convex sum, i.e., the trust values add to one so that we have Pn
k=1 µik = 1 for each i = 1, . . . , n.
This normalization allows the DeGroot model to exhibit stable asymptotic behaviour."
PROPAGATION MODEL,0.013111447302067574,"At each timestep, the following processes occur: Nodes with opinion values lower than -0.95
are identified as sources of misinformation and transmit the misinformation to their immediate
neighbors (referred to as ‘candidate nodes‘) according to one of the propagation model detailed
in Equations 1 and 2. Concurrently, an intervention strategy is applied where a subset of these
neighbors—constrained by an action budget—is selected to receive credible information from a
trusted source. This source is characterized by an opinion value of 1 and we vary trust parameter
among 1, 0.8, and 0.75. The process includes a blocking mechanism where a node that exceeds a
positive opinion threshold of 0.95 is considered ‘blocked’, ceasing to interact with the misinformation
spread or disseminate positive influence further. The simulation concludes when there are no viable
‘candidate nodes’ left to propagate misinformation. Our primary objective is to devise a
learning mechanism that efficiently identifies and selects key nodes within the network to disseminate
accurate information at each time step."
METHODS,0.01361573373676248,"3
Methods"
METHODS,0.014120020171457387,"In this section, we will explain our methodologies, presenting an overview of the network architectures
employed, including the GCN and ResNet frameworks. Additional details about the neural network
architecture utilized for our experiments can be found in Appendix A.2. We detail our proposed
ranking algorithm utilized in the SL process. Additionally, we elaborate on the implementation
of the Deep Value Network (DVN) with experience replay for the proposed RL-based planners.
Furthermore, we provide an explanation of the various reward functions analyzed in our RL setup."
RANKING ALGORITHM-BASED SUPERVISED LEARNING,0.014624306606152295,"3.1
Ranking Algorithm-based Supervised Learning"
RANKING ALGORITHM-BASED SUPERVISED LEARNING,0.015128593040847202,"In this section, we propose a ranking algorithm based SL model to classify the key nodes at each
time step to disseminate accurate information. Our SL method utilizes a GCN architecture."
RANKING ALGORITHM-BASED SUPERVISED LEARNING,0.015632879475542108,"Ranking Algorithm: We pose the ranking algorithm as a search problem where the objective is to
find the optimal set of nodes that, when blocked, minimizes the overall infection rate. The network is
represented as a graph G, where nodes can be infected, blocked, or possess opinion values within
the range [−0.95, 0.95]. Initially, a simulation network S is created by setting the opinion values of
infected nodes to −1 and removing blocked nodes. Let M denote the number of nodes in S that are
neither infected nor blocked. Given an action budget K, we select K nodes from M in
 M
K

possible
ways, forming the set C of all possible combinations. For each subset c ∈C, we temporarily block
the nodes in c by setting their opinion values to 1 and simulate the spread of misinformation within
S. The resulting infection rates for each subset c are stored in the set R. We identify the subset
c∗∈C that yields the minimal infection rate, denoted as r∗. This subset c∗is our target set. We then
construct a target matrix T ∈RN×1, where N is the total number of nodes in the original network
G. All entries of T are initialized to 0, and for each node i ∈c∗, the i-th entry of T is set to 1. This
target matrix T is subsequently used to train the GCN-based model. A pseudocode for this ranking
algorithm is presented in Algorithm 1 in Appendix A.5."
RANKING ALGORITHM-BASED SUPERVISED LEARNING,0.016137165910237016,"Overall Training Procedure: The training of our GCN-based model leverages the labels defined in
the target matrix T ∈RN×1. This matrix is compared with the model’s output matrix O ∈RN×1,
which estimates the blocking probability of each node. We evaluate training efficacy using the binary
cross-entropy loss between T and O, which quantifies prediction errors. Model weight adjustments
are implemented via standard backpropagation [23] based on this loss."
RANKING ALGORITHM-BASED SUPERVISED LEARNING,0.01664145234493192,"Each training iteration consists of several episodes, starting with the generation of a random graph
state G containing initially infected nodes. The GCN then processes this state to output matrix O
using the graph’s features and structure. Labels are generated, as detailed above using the ranking
algorithm, generating the target matrix T. The binary cross-entropy loss between O and T is
calculated for backpropagation. The environment updates by blocking predicted nodes, allowing
the infection spread, and adjusting node attributes. The process repeats until misinformation spread
is halted, with each episode refining the graph’s state for subsequent iterations. The results of the
planners for difference cases are summarized in the Appendix A.6.1."
RANKING ALGORITHM-BASED SUPERVISED LEARNING,0.01714573877962683,"However, we note that, in the case of continuous opinion and continuous trust (case-3), the process
of label generation becomes more complex. In such scenarios, agents do not change their opinion
immediately but gradually, making it difficult to predict which agents will be misinformed based on a
single propagation simulation. Therefore, simulations across multiple time steps are necessary to
identify the optimal nodes to block. As the ranking algorithm uses a brute force method to determine
optimal nodes, this approach becomes increasingly challenging with continuous opinion models."
REINFORCEMENT LEARNING-BASED CENTRALIZED DYNAMIC PLANNERS,0.017650025214321734,"3.2
Reinforcement Learning-based Centralized Dynamic Planners"
REINFORCEMENT LEARNING-BASED CENTRALIZED DYNAMIC PLANNERS,0.018154311649016642,"In SL, the process of generating labels can be costly and impractical as network size increases. This is
evident while considering mitigating misinformation propagation in large networks, where identifying
the optimal set of nodes for blocking requires a combinatorial search that is computationally infeasible.
Thus, RL emerges as a viable alternative."
REINFORCEMENT LEARNING-BASED CENTRALIZED DYNAMIC PLANNERS,0.018658598083711547,"Deep Q-networks (DQNs) [32] using random exploration combined with experience replay have been
demonstrated to effectively learn Q-values for sequential decision making with high-dimensional
data. Unlike the classical DQN, where the network outputs a Q-value corresponding to each possible
action, in our problem, which also deals with high-dimensional data, we develop a DVN, as the
number of available actions at each time step need not be fixed. Consequently, the output layer
consists of a single neuron that outputs the value for a given input state. The agent’s experiences at
each time step are stored in a replay memory buffer for the neural network parameter updates. The
loss function for training is given by"
REINFORCEMENT LEARNING-BASED CENTRALIZED DYNAMIC PLANNERS,0.019162884518406455,"L(st, st+1|θ) =

rt + ˆVθ−(st+1) −Vθ(st)

,
(3)"
REINFORCEMENT LEARNING-BASED CENTRALIZED DYNAMIC PLANNERS,0.019667170953101363,"where t represents the current time step, st is the current state, st+1 denotes the subsequent state
after action at is taken, and rt is the reward received for taking at in st. The parameters θ denote"
REINFORCEMENT LEARNING-BASED CENTRALIZED DYNAMIC PLANNERS,0.020171457387796268,"the weights of the value network used to estimate the state value Vθ(st), while θ−represents the
parameters of a target network, typically a lagged copy of θ, used to stabilize training. Here,
ˆVθ−(st+1) is the estimated value of the next state st+1 according to the target network. The specific
reward functions used in this study are discussed later in the section. Algorithm 2, in Appendix A.5,
provides a detailed implementation of our DVN with experience replay."
REWARD FUNCTIONS FOR RL SETUP,0.020675743822491176,"3.2.1
Reward Functions for RL setup"
REWARD FUNCTIONS FOR RL SETUP,0.02118003025718608,"The reward function is designed to encourage policies that effectively mitigate the spread of misinfor-
mation. Specifically, the reward functions modeled for our study are: (1) R0 = −(∆infection rate),
where ∆infection rate is defined as the change in infection rate resulting from taking action at. Specifi-
cally, ∆infection rate = infection rate at st+1−infection rate at st. This reward structure encourages
the model to reduce the rate at which misinformation spreads by penalizing increases in the infection
rate; (2) R1 = −(# candidate nodes), targets the number of immediate neighbors of infected nodes
that are susceptible to becoming infected in the next timestep, thereby promoting strategies that mini-
mize the potential for misinformation to spread; (3) R2 = −(# candidate nodes) −(∆infection rate),
takes into account the previous two rewards, balancing the need to control both the number of
susceptible nodes and the overall infection rate; (4) R3 = 1 −(
# time steps
Total time steps), rewards quicker reso-
lutions, providing higher rewards for strategies that contain misinformation rapidly and evaluating
the effectiveness only at the end of each episode; (5) R4 = −(infection rate), directly penalizes the
current infection rate, thus favoring actions that achieve lower overall infection rates; and finally,
a combined reward that incorporates elements of both R3 and R1. Throughout the simulation, the
agent continually receives rewards based on the number of candidate nodes, fostering strategies that
limit the expansion of the infection network. As the simulation concludes, the agent receives an
episodic reward calculated as (6) R5 = −(# candidate nodes) −
# time steps
Total time steps, thereby reinforcing the
importance of quick and efficient resolution of misinformation spread. Note that all these reward
structures, in addition to differing in how they represent the goal for the planners, also differ in the
network information required to compute them."
NETWORK ARCHITECTURES,0.02168431669188099,"3.3
Network Architectures"
NETWORK ARCHITECTURES,0.022188603126575897,"In our experiments, we utilized two neural network architectures. First, a GCN to model node
features within a network. Each node was characterized by three key features. These features were
represented in a matrix F ∈RN×3, where N denotes the total number of nodes. The feature matrix is
dynamic and evolves to reflect changes in the network. It includes the opinion value, the connectivity
degree, which identifies nodes potentially susceptible to misinformation while excluding those already
blocked or misinformed, and the proximity to a misinformed node, which is calculated as the shortest
path to the nearest infected node, assigning a distance of infinity to unreachable nodes."
NETWORK ARCHITECTURES,0.0226928895612708,"We have also considered using Residual Network (ResNet) Architecture. The ResNet model imple-
mented in our study is a variant of the conventional ResNet architecture [14]. The core component
of our ResNet model is the ResidualBlock, which allows for the training of deeper networks by
addressing the vanishing gradient problem through skip connections. Each ResidualBlock consists
of two sequences of convolutional layers (Conv2d), batch normalization (BatchNorm2d), and sig-
moid activations. Complete details about the model architectures used in our study are provided in
Appendix A.2."
EXPERIMENTS,0.02319717599596571,"4
Experiments"
EXPERIMENTS,0.023701462430660614,"In this section, we present the details about training data generation and configurations chosen for
our SL and RL methodologies. We also explain the test data used for evaluating the trained models."
TRAINING SETUP,0.024205748865355523,"4.1
Training Setup"
TRAINING SETUP,0.024710035300050427,"In the SL setup, we experimented with three distinct graph structures: Watts-Strogatz (with nearest
neighbors k = 3 and a rewiring probability p = 0.4), Erdos-Renyi (with branching factor of 4),
and Tree graphs (with branching factors randomly selected from the range [1, 4]). Each graph type
facilitated training models to evaluate the influence of various structural dynamics on performance."
TRAINING SETUP,0.025214321734745335,"We used the GCN model for the SL method. Due to the consistent performance of the trained models
on the different graph topologies, we chose the small-world topology to present all the subsequent
analyses and summarize the results in Appendix A.6.1. On the other hand, we trained centralized RL
planners using both ResNet and GCN network architectures. Each trained configuration is represented
as model-n-x-y, where model ∈{ResNet, GCN}, n ∈{10,25,50} represents the network length (in
terms of the number of nodes), x ∈{1,2,3} represents the number of initial infected nodes, and y ∈
{1,2,3} represents the action budget."
TEST DATA GENERATION,0.025718608169440244,"4.2
Test Data Generation"
TEST DATA GENERATION,0.026222894604135148,"The datasets used in related works, such as [17], typically consist of network structures, and no
real-time opinion propagation data could be found. Therefore, to evaluate our intervention strategies,
we generated two sets of synthetic datasets using the Watts-Strogatz model with the training dataset’s
configurations. This approach allows us to simulate complex networks and control the structure,
connectivity, and initial infected nodes to assess our models effectively."
TEST DATA GENERATION,0.026727181038830056,"Dataset v1 examines the effects of network size and the initial count of infected nodes on misinfor-
mation spread. We generated data with network sizes of 10, 25, and 50 nodes with 1, 2, and 3 initially
infected nodes, respectively, creating 9 unique datasets. Each configuration has 1000 random network
states with the opinion values of non-infected nodes uniformly distributed between −0.5 and 0.6."
TEST DATA GENERATION,0.02723146747352496,"Dataset v2 examines how the initial connections of infected nodes affect the spread of misinformation.
Like Dataset v1, it includes networks of 10, 25, and 50 nodes. However, the initial number of
connections (degrees of connectivity) for the infected nodes varies from 1 to 4. Here by degree of
connectivity, we mean the number of candidate nodes present at the start of the simulation. This
variation results in a total of 12 datasets for each configuration, with each dataset containing 1000
states. In these configurations, the number of initially infected nodes is randomly chosen from 1 to 3."
RESULTS AND DISCUSSION,0.02773575390821987,"5
Results and Discussion"
RESULTS AND DISCUSSION,0.028240040342914774,"In this section, we evaluate the models, using the infection rate metric, trained using our ranking-
based SL and RL algorithms with various reward functions. We discuss the efficiency of these models
using Dataset v2, particularly on a network of 50 nodes with a connectivity degree of 4, as it
represents the most complex test dataset we generated. Similar evaluation results for other datasets
can be found in the Appendix A.6. Additionally, we have also evaluated our planning algorithms
using directed and undirected real-world network models reported in the literature. These evaluations
are presented in Table 2. The details of the hardware used for our experiments are provided in the
Appendix A.4. Our empirical investigation yielded insightful results regarding the performance of
our trained models under various training conditions. With comprehensive experimental evaluations,
we were able to address the following research questions."
RESULTS AND DISCUSSION,0.028744326777609682,"Objective and Research Questions: O: Identify the optimal combination of initially infected nodes
and action budget parameters for training models to effectively control the spread of misinformation.
RQ1: For reward functions that focus on the blocking time, does adding any other factor lead to
better results? If yes, which factor? RQ2: Do reward functions that look at global graph information
perform better than those considering local, neighboring information? RQ3: Does GCN offer better
scalability and performance when compared with ResNet."
RESULTS AND DISCUSSION,0.02924861321230459,"O: What is the best combination of initially infected nodes and action budget parameters for training
the models to control the misinformation spread?"
RESULTS AND DISCUSSION,0.029752899646999495,"To examine this, we focused our analysis on the Mean Squared Error (MSE) loss plots obtained
during the training phase. Figure 7 in Appendix A.6 illustrates the comparison of training loss across
various network parameter settings for all considered reward types in Case-1, employing a ResNet
model trained on a network of 50 nodes. The trend in loss convergence across episodes was found
to be consistent for both the ResNet and GCN models across all cases examined. The analysis
revealed that reward functions exhibiting lower and more stable loss values correlate with improved
model learning performance. Our findings highlight that increasing the number of initially infected
nodes typically elevates the stabilization point of MSE loss, indicating a more challenging learning
environment. Additionally, a higher action budget contributes to increased MSE variability, reflecting"
RESULTS AND DISCUSSION,0.030257186081694403,"Table 1: Inference results on Dataset v2, with a degree of connectivity 4, featuring a network of 50
nodes. This table presents the average infection rates for different models, with ResNet trained on a
network of 50 nodes and GCN trained on networks of 10 nodes, under various methods (M.) tested
with varying action budgets (A.) across different cases considered."
RESULTS AND DISCUSSION,0.030761472516389308,"A.
M.
Case-1
Case-2
Case-3"
RESULTS AND DISCUSSION,0.031265758951084216,"ResNet(50)
GCN(10)
ResNet(50)
GCN(10)
ResNet(50)
GCN(10) 1"
RESULTS AND DISCUSSION,0.03177004538577912,"RL+R0
0.2334
0.2481
0.2496
0.2454
0.0449
0.0461
RL+R1
0.1917
0.1608
0.1965
0.1607
0.0449
0.0435
RL+R2
0.2427
0.1608
0.2148
0.1608
0.0451
0.0438
RL+R3
0.2331
0.2958
0.2281
0.3381
0.0444
0.046
RL+R4
0.199
0.1593
0.2513
0.1596
0.045
0.0442
RL+R5
-
0.1607
-
0.1605
-
0.0439"
RESULTS AND DISCUSSION,0.03227433182047403,"SL+GCN(25)
0.304
0.2889
0.3715 2"
RESULTS AND DISCUSSION,0.03277861825516894,"RL+R0
0.0974
0.1012
0.0992
0.1007
0.0398
0.04
RL+R1
0.0886
0.0842
0.0901
0.0843
0.0398
0.0398
RL+R2
0.097
0.0842
0.0957
0.0843
0.0399
0.0398
RL+R3
0.0959
0.0969
0.0962
0.1032
0.0398
0.04
RL+R4
0.0898
0.0842
0.1005
0.0842
0.0399
0.0398
RL+R5
-
0.0842
-
0.0842
-
0.0398"
RESULTS AND DISCUSSION,0.03328290468986384,"SL+GCN(25)
0.1464
0.1032
0.3491 3"
RESULTS AND DISCUSSION,0.033787191124558746,"RL+R0
0.0599
0.0599
0.0599
0.06
0.0397
0.0399
RL+R1
0.0599
0.0597
0.0598
0.0597
0.0398
0.0397
RL+R2
0.0598
0.0597
0.0599
0.0597
0.0398
0.0397
RL+R3
0.06
0.0598
0.0601
0.0602
0.0397
0.0399
RL+R4
0.0598
0.0597
0.06
0.0597
0.0398
0.0398
RL+R5
-
0.0597
-
0.0597
-
0.0398"
RESULTS AND DISCUSSION,0.03429147755925366,"SL+GCN(25)
0.0488
0.0559
0.2526"
RESULTS AND DISCUSSION,0.03479576399394856,"the added complexity and generally poorer performance during training. Based on this analysis, we
find ResNet-n-1-1 and GCN-n-1-1, n ∈{10,25,50}, to be the best training configurations."
RESULTS AND DISCUSSION,0.03530005042864347,"Table 1 presents the average infection rate values across different cases considered for Dataset v2
with a degree of connectivity 4, featuring a network of 50 nodes, detailing the average infection rates.
It compares the performance of the ResNet model, trained on a network of 50 nodes, with the GCN
model, trained on a network of 10 nodes, using the RL training algorithm across the different reward
types, and the GCN model trained using SL on a network of 25 nodes. Results on the additional
datasets are provided in Appendix A.6."
RESULTS AND DISCUSSION,0.03580433686333838,"RQ1: For reward functions that focus on blocking time, does adding any other factor lead to better
result? If yes, which factor? Answer: Yes. #candidate nodes."
RESULTS AND DISCUSSION,0.036308623298033284,"Reward function R3, which is formulated to minimize the number of time steps required to halt the
spread of misinformation, might inadvertently not be the most effective strategy for minimizing the
overall infection rate within the network. As the reward is solely based on the speed of response,
it does not directly account for the magnitude of the misinformation spread, that is, the number of
nodes affected. Therefore, the agent may prioritize actions that conclude the propagation swiftly but
do not necessarily result in the most substantial reduction in the spread of misinformation. However,
our results indicate that under specific training configurations with an action budget or initial infected
nodes greater than 1, the reward function R3 outperforms others in maintaining lower infection rates,
as shown in Figure 8 in Appendix A.6. This finding is significant since R3, a sparser reward type,
requires less computational effort and is independent of network observability. As the action budget
increases the propagation tends to conclude in fewer timesteps thereby resulting in the RL agent
receiving a higher reward in the case of R3. Figure 2 shows that the RL agent trained with the R3
reward function chooses actions that conclude propagation in the least time. Conversely, Figure 1
illustrates the sequence of actions chosen by an RL agent trained with the R1 reward function on the
same network. Although R1 requires more time steps than R3, it results in a lower infection rate. This
can also be observed from Table 1, where the infection rate is higher for the R3 reward function than"
RESULTS AND DISCUSSION,0.03681290973272819,"Table 2: Average infection rate values from experiments conducted on 100 random instantiations
for each real-world network, each starting with 1 random initially infected node, obtained using
GCN model trained on synthetic networks of 10 nodes, under various reward structures (M.) tested
with varying action budgets (A.). The network properties of # nodes (V), # edges (E), and Average
Degree for each network are shown in the table."
RESULTS AND DISCUSSION,0.03731719616742309,"A.
M.
Zachary’s Karate
Club [Undirected]
[51]"
RESULTS AND DISCUSSION,0.037821482602118005,"Facebook [Undi-
rected] [25]
Email [Directed]
[24]
Cora
[Undi-
rected] [31]"
RESULTS AND DISCUSSION,0.03832576903681291,"Network Properties
V: 34, E: 78
V: 250, E: 1352
V: 300, E: 2358
V: 2000, E: 2911
Avg. Deg.: 4.59
Avg. Deg.: 10.8
Avg. Deg.: 7.9
Avg. Deg.: 2.9 1"
RESULTS AND DISCUSSION,0.038830055471507814,"RL+R0
0.8579
0.4569
0.395
0.0603
RL+R1
0.5279
0.4547
0.315
0.0095
RL+R2
0.5279
0.4547
0.3169
0.0095
RL+R3
0.8468
0.511
0.3759
0.0609
RL+R4
0.5326
0.4589
0.3183
0.0096
RL+R5
0.5276
0.4547
0.316
0.0096 3"
RESULTS AND DISCUSSION,0.039334341906202726,"RL+R0
0.2762
0.3621
0.2512
0.0166
RL+R1
0.1641
0.3022
0.0988
0.002
RL+R2
0.1641
0.3024
0.1062
0.002
RL+R3
0.2562
0.3697
0.1838
0.0142
RL+R4
0.1582
0.3047
0.0966
0.002
RL+R5
0.1641
0.3022
0.0988
0.002 5"
RESULTS AND DISCUSSION,0.03983862834089763,"RL+R0
0.0926
0.233
0.103
0.0126
RL+R1
0.0697
0.1649
0.0347
0.0017
RL+R2
0.0662
0.1647
0.0347
0.0017
RL+R3
0.085
0.2039
0.0587
0.0052
RL+R4
0.0662
0.1633
0.0351
0.0017
RL+R5
0.0662
0.164
0.0347
0.0017"
RESULTS AND DISCUSSION,0.040342914775592535,Figure 2: Sequence of actions chosen by the RL agent trained using reward function R3.
RESULTS AND DISCUSSION,0.04084720121028744,"any other reward function. In order to effectively implement this we have considered combining this
episodic reward along with R1, resulting in reward type R5. This has shown a significant performance
improvement when compared to the original version."
RESULTS AND DISCUSSION,0.04135148764498235,"RQ2: Do reward functions that look at global graph information perform better than those considering
local, neighboring information ? Answer: Yes"
RESULTS AND DISCUSSION,0.041855774079677256,"Analysis of the inference outcomes using Dataset v2, as presented in Table 1, shows that single factor
reward functions, specifically R1 = −(# candidate nodes) and R4 = −(infection rate), consistently
resulted in lower infection rates across various settings compared to their more complex counterparts
with R4 providing relatively better results compared to R1. This trend was observed in both ResNet
and GCN models. From a practical standpoint, R1 can be particularly advantageous because it does
not require complete observability of the network, but just the immediate neighbors of infected
nodes. Conversely, to compute R4, which reflects the infection rate of the network, complete
understanding of the state of each node within the network is required. This requirement for total
network observability could limit the practicality of R4 in situations where such detailed information
is unavailable or difficult to gather."
RESULTS AND DISCUSSION,0.04236006051437216,"Table 3: This table outlines the unique attributes of our approach, including the use of a deep
value network, network dynamicity across multiple cases, asynchronous communication, and the
exploration of five different reward models."
RESULTS AND DISCUSSION,0.04286434694906707,"Implications
Features of Our
work
Our Work
Previous Work
Citations"
RESULTS AND DISCUSSION,0.04336863338376198,"Action-Space
In-
variant
Deep
Value
Net-
work
✓
✗
DQN [17]"
RESULTS AND DISCUSSION,0.04387291981845688,"Expressive Models
Network Dynamic-
ity
Case 1, Case 2, Case 3
Only Case 1
[28-40]"
RESULTS AND DISCUSSION,0.044377206253151794,"Realistic Communi-
cation Dynamics
Asynchronous Com-
munication
✓
✗
[37]"
RESULTS AND DISCUSSION,0.0448814926878467,"Wider Applications
Reward Models
5 variants studies
Typically 1
[10, 13, 17, 36]"
RESULTS AND DISCUSSION,0.0453857791225416,RQ3: Does GCN offer better scalability and performance when compared with ResNet? Answer: Yes
RESULTS AND DISCUSSION,0.04589006555723651,"GCNs are hypothesized to outperform traditional convolution-based architectures like ResNet in tasks
involving graph data due to their ability to naturally process the structural information of networks
and their enhanced ability to represent complex feature sets [4]. This study compares the scalability
and performance of a GCN, which excels in node classification within graphs, to a ResNet model
that, despite its success in image recognition, may not scale as effectively to larger graph structures
beyond the size it was initially trained on. Referring to Table 1, the GCN model, trained on only
10 node networks, consistently exhibits lower average infection rates across all the cases and under
varying action budgets, when compared with the ResNet model trained on 50 node networks. The
ability of GCN to maintain lower infection rates even as network complexity increases underscores its
robustness and scalability in more complex network scenarios. This performance contrast highlights
the suitability of GCN architectures for graph-based tasks."
CONCLUSIONS,0.04639435199193142,"6
Conclusions"
CONCLUSIONS,0.046898638426626324,"This paper investigates scalable and innovative intervention strategies for containing the spread of
misinformation within dynamic opinion networks. Our significant contributions include analysis
using continuous opinion models, a design of ranking algorithm for identifying key nodes to facilitate
SL-based classifiers, and the utilization of GCNs to optimize intervention strategies. Additionally, we
design and study various reward functions for reinforcement learning, enhancing our approach to
misinformation mitigation."
CONCLUSIONS,0.04740292486132123,"Despite significant progress, our work has limitations. In the field of computational social science,
often more complex agent models are being investigated. While we have made significant efforts to
extend the understanding of planning strategies, especially in continuous opinion networks, exploring
complex agent traits such as stubbornness and the representation of directed trust, and implementing
topic-dependency in a multi-topic network along with distributed planners instead of centralized
planners as in our work is a compelling future direction."
CONCLUSIONS,0.04790721129601614,"Broader Societal Impact: This work provides methods that can be used to exert control on information
spread. When used responsibly by authorized information providers, which the authors support, it
will help reduce prevalent infodemics in social media. But it may also be misused by an adversary
to wean control from an authorized party (e.g., information owner) and counter efforts to tackle
misinformation. Overall, the authors believe more research efforts are needed to understand opinion
networks and information dissemination strategies in dynamic and uncertain environments in pursuit
of long-term societal benefits."
CONCLUSIONS,0.048411497730711045,Acknowledgments and Disclosure of Funding
CONCLUSIONS,0.04891578416540595,"This material is based upon work supported in parts by the Air Force Office of Scientific Research
under award number FA9550-24-1-0228 and NSF award number 2337998. Any opinions, findings,
conclusions, or recommendations expressed here are those of the authors and do not necessarily
reflect the views of the sponsors. The authors thank the anonymous reviewers for their insightful and
constructive reviews."
REFERENCES,0.049420070600100854,References
REFERENCES,0.049924357034795766,"[1] Daron Acemoglu and Asuman Ozdaglar. Opinion dynamics and learning in social networks.
Dynamic Games and Applications, 1:3–49, 2011."
REFERENCES,0.05042864346949067,"[2] Hunt Allcott and Matthew Gentzkow. Social media and fake news in the 2016 election. Journal
of economic perspectives, 31(2):211–236, 2017."
REFERENCES,0.050932929904185575,"[3] Brian DO Anderson and Mengbin Ye. Recent advances in the modelling and analysis of
opinion dynamics on influence networks. International Journal of Automation and Computing,
16(2):129–149, 2019."
REFERENCES,0.05143721633888049,"[4] Uzair Aslam Bhatti, Hao Tang, Guilu Wu, Shah Marjan, and Aamir Hussain. Deep learning
with graph convolutional networks: An overview and latest applications in computational
intelligence. International Journal of Intelligent Systems, 2023(1):8342104, 2023."
REFERENCES,0.05194150277357539,"[5] Ceren Budak, Divyakant Agrawal, and Amr El Abbadi. Limiting the spread of misinformation
in social networks. In Proceedings of the 20th international conference on World wide web,
pages 665–674, 2011."
REFERENCES,0.052445789208270296,"[6] Morris H DeGroot. Reaching a consensus. Journal of the American Statistical association,
69(345):118–121, 1974."
REFERENCES,0.0529500756429652,"[7] Xuejun Ding, Mengyu Li, Yong Tian, and Man Jiang. Rbotue: Rumor blocking considering
outbreak threshold and user experience. IEEE Transactions on Engineering Management, 2021."
REFERENCES,0.05345436207766011,"[8] Israel Junior Borges Do Nascimento, Ana Beatriz Pizarro, Jussara M Almeida, Natasha
Azzopardi-Muscat, Marcos André Gonçalves, Maria Björklund, and David Novillo-Ortiz.
Infodemics and health misinformation: a systematic review of reviews. Bulletin of the World
Health Organization, 100(9):544, 2022."
REFERENCES,0.05395864851235502,"[9] Lidan Fan, Zaixin Lu, Weili Wu, Bhavani Thuraisingham, Huan Ma, and Yuanjun Bi. Least cost
rumor blocking in social networks. In 2013 IEEE 33rd International Conference on Distributed
Computing Systems, pages 540–549. IEEE, 2013."
REFERENCES,0.05446293494704992,"[10] Mehrdad Farajtabar, Jiachen Yang, Xiaojing Ye, Huan Xu, Rakshit Trivedi, Elias Khalil, Shuang
Li, Le Song, and Hongyuan Zha. Fake news mitigation via point process based intervention. In
International conference on machine learning, pages 1097–1106. PMLR, 2017."
REFERENCES,0.054967221381744834,"[11] Adam Fourney, Miklos Z Racz, Gireeja Ranade, Markus Mobius, and Eric Horvitz. Geographic
and temporal trends in fake news consumption during the 2016 us presidential election. In
CIKM, volume 17, pages 6–10, 2017."
REFERENCES,0.05547150781643974,"[12] Chao Gao and Jiming Liu. Modeling and restraining mobile virus propagation. IEEE transac-
tions on mobile computing, 12(3):529–541, 2012."
REFERENCES,0.05597579425113464,"[13] Mahak Goindani and Jennifer Neville. Social reinforcement learning to combat fake news
spread. In Uncertainty in Artificial Intelligence, pages 1006–1016. PMLR, 2020."
REFERENCES,0.05648008068582955,"[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016."
REFERENCES,0.05698436712052446,"[15] Qiang He, Dafeng Zhang, Xingwei Wang, Lianbo Ma, Yong Zhao, Fei Gao, and Min Huang.
Graph convolutional network-based rumor blocking on social networks. IEEE Transactions on
Computational Social Systems, 2022."
REFERENCES,0.057488653555219364,"[16] Yanqing Hu, Shenggong Ji, Yuliang Jin, Ling Feng, H Eugene Stanley, and Shlomo Havlin.
Local structure can identify and quantify influential global spreaders in large scale social
networks. Proceedings of the National Academy of Sciences, 115(29):7468–7472, 2018."
REFERENCES,0.05799293998991427,"[17] Jiajian Jiang, Xiaoliang Chen, Zexia Huang, Xianyong Li, and Yajun Du. Deep reinforce-
ment learning-based approach for rumor influence minimization in social networks. Applied
Intelligence, 53(17):20293–20310, 2023."
REFERENCES,0.05849722642460918,"[18] Zhongyuan Jiang, Xianyu Chen, Jianfeng Ma, and S Yu Philip. Rumordecay: rumor dissemina-
tion interruption for target recipients in social networks. IEEE Transactions on Systems, Man,
and Cybernetics: Systems, 52(10):6383–6395, 2022."
REFERENCES,0.059001512859304085,"[19] Elias Boutros Khalil, Bistra Dilkina, and Le Song. Scalable diffusion-aware optimization of
network topology. In Proceedings of the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1226–1235, 2014."
REFERENCES,0.05950579929399899,"[20] Ling Min Serena Khoo, Hai Leong Chieu, Zhong Qian, and Jing Jiang. Interpretable rumor
detection in microblogs by attending to user interactions. In Proceedings of the AAAI conference
on artificial intelligence, volume 34, pages 8783–8790, 2020."
REFERENCES,0.0600100857286939,"[21] Masahiro Kimura, Kazumi Saito, and Hiroshi Motoda. Minimizing the spread of contamination
by blocking links in a network. In Aaai, volume 8, pages 1175–1180, 2008."
REFERENCES,0.060514372163388806,"[22] Sumeet Kumar and Kathleen M Carley. Tree lstms with convolution units to predict stance and
rumor veracity in social media conversations. In Proceedings of the 57th annual meeting of the
association for computational linguistics, pages 5047–5058, 2019."
REFERENCES,0.06101865859808371,"[23] Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for back-
propagation. In Proceedings of the 1988 connectionist models summer school, volume 1, pages
21–28, 1988."
REFERENCES,0.061522945032778616,"[24] Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densification and
shrinking diameters. ACM transactions on Knowledge Discovery from Data (TKDD), 1(1):2–es,
2007."
REFERENCES,0.06202723146747353,"[25] Jure Leskovec and Julian Mcauley. Learning to discover social circles in ego networks. Advances
in neural information processing systems, 25, 2012."
REFERENCES,0.06253151790216843,"[26] Yaguang Lin, Zhipeng Cai, Xiaoming Wang, and Fei Hao. Incentive mechanisms for crowd-
blocking rumors in mobile social networks. IEEE Transactions on Vehicular Technology,
68(9):9220–9232, 2019."
REFERENCES,0.06303580433686334,"[27] Bo Liu, Xiangguo Sun, Qing Meng, Xinyan Yang, Yang Lee, Jiuxin Cao, Junzhou Luo, and
Roy Ka-Wei Lee. Nowhere to hide: Online rumor detection based on retweeting graph neural
networks. IEEE Transactions on Neural Networks and Learning Systems, 2022."
REFERENCES,0.06354009077155824,"[28] Jing Ma, Wei Gao, and Kam-Fai Wong. Rumor detection on Twitter with tree-structured
recursive neural networks. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the
56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 1980–1989, Melbourne, Australia, July 2018. Association for Computational Linguistics."
REFERENCES,0.06404437720625315,"[29] Ling-ling Ma, Chuang Ma, Hai-Feng Zhang, and Bing-Hong Wang. Identifying influential
spreaders in complex networks based on gravity formula. Physica A: Statistical Mechanics and
its Applications, 451:205–212, 2016."
REFERENCES,0.06454866364094806,"[30] Mohammad Ali Manouchehri, Mohammad Sadegh Helfroush, and Habibollah Danyali. Tempo-
ral rumor blocking in online social networks: A sampling-based approach. IEEE Transactions
on Systems, Man, and Cybernetics: Systems, 52(7):4578–4588, 2021."
REFERENCES,0.06505295007564296,"[31] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating
the construction of internet portals with machine learning. Information Retrieval, 3:127–163,
2000."
REFERENCES,0.06555723651033787,"[32] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.06606152294503277,"[33] B. Muppasani, P. Nag, V. Narayanan, B. Srivastava, and M. N. Huhns. Code and datasets for the
paper, 2024. Available at: https://github.com/ai4society/InfoSpread-NeurIPS-24."
REFERENCES,0.06656580937972768,"[34] B. Muppasani, V. Narayanan, B. Srivastava, and M. N. Huhns. Expressive and flexible simulation
of information spread strategies in social networks using planning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 38, pages 23820–23822, 2024."
REFERENCES,0.0670700958144226,"[35] Le Nguyen and Nidhi Rastogi. Graph-based approach for studying spread of radical online
sentiment. In Companion Proceedings of the ACM Web Conference 2023, pages 1373–1380,
2023."
REFERENCES,0.06757438224911749,"[36] Abu Quwsar Ohi, MF Mridha, Muhammad Mostafa Monowar, and Md Abdul Hamid. Exploring
optimal control of epidemic spread using reinforcement learning. Scientific reports, 10(1):22106,
2020."
REFERENCES,0.0680786686838124,"[37] Hegselmann Rainer and Ulrich Krause. Opinion dynamics and bounded confidence: models,
analysis and simulation. 2002."
REFERENCES,0.06858295511850732,"[38] Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. Fake news detection on social
media: A data mining perspective. ACM SIGKDD explorations newsletter, 19(1):22–36, 2017."
REFERENCES,0.06908724155320221,"[39] Santhoshkumar Srinivasan and Dhinesh Babu LD. A social immunity based approach to
suppress rumors in online social networks. International Journal of Machine Learning and
Cybernetics, 12:1281–1296, 2021."
REFERENCES,0.06959152798789713,"[40] Tetsuro Takahashi and Nobuyuki Igata. Rumor detection on twitter. In The 6th International
Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium
on Advanced Intelligence Systems, pages 452–457. IEEE, 2012."
REFERENCES,0.07009581442259204,"[41] Guangmo Tong, Weili Wu, and Ding-Zhu Du. Distributed rumor blocking with multiple positive
cascades. IEEE Transactions on Computational Social Systems, 5(2):468–480, 2018."
REFERENCES,0.07060010085728693,"[42] Guangmo Tong, Weili Wu, Ling Guo, Deying Li, Cong Liu, Bin Liu, and Ding-Zhu Du. An
efficient randomized algorithm for rumor blocking in online social networks. IEEE Transactions
on Network Science and Engineering, 7(2):845–854, 2017."
REFERENCES,0.07110438729198185,"[43] Guangmo Amo Tong and Ding-Zhu Du. Beyond uniform reverse sampling: A hybrid sampling
technique for misinformation prevention. In IEEE INFOCOM 2019-IEEE conference on
computer communications, pages 1711–1719. IEEE, 2019."
REFERENCES,0.07160867372667676,"[44] Hanghang Tong, B Aditya Prakash, Tina Eliassi-Rad, Michalis Faloutsos, and Christos Faloutsos.
Gelling, and melting, large graphs by edge manipulation. In Proceedings of the 21st ACM
international conference on Information and knowledge management, pages 245–254, 2012."
REFERENCES,0.07211296016137166,"[45] Senzhang Wang, Xiaojian Zhao, Yan Chen, Zhoujun Li, Kai Zhang, and Jiali Xia. Negative
influence minimizing by blocking nodes in social networks. In Workshops at the Twenty-Seventh
AAAI Conference on Artificial Intelligence, 2013."
REFERENCES,0.07261724659606657,"[46] Penghui Wei, Nan Xu, and Wenji Mao. Modeling conversation structure and temporal dynamics
for jointly predicting rumor stance and veracity. arXiv preprint arXiv:1909.08211, 2019."
REFERENCES,0.07312153303076148,"[47] Qingqing Wu, Xianguan Zhao, Lihua Zhou, Yao Wang, and Yudi Yang. Minimizing the
influence of dynamic rumors based on community structure. International Journal of Crowd
Science, 3(3):303–314, 2019."
REFERENCES,0.07362581946545638,"[48] Ruidong Yan, Deying Li, Weili Wu, Ding-Zhu Du, and Yongcai Wang. Minimizing influence of
rumors by blockers on social networks: algorithms and analysis. IEEE transactions on network
science and engineering, 7(3):1067–1078, 2019."
REFERENCES,0.07413010590015129,"[49] Ruidong Yan, Yi Li, Weili Wu, Deying Li, and Yongcai Wang. Rumor blocking through
online link deletion on social networks. ACM Transactions on Knowledge Discovery from Data
(TKDD), 13(2):1–26, 2019."
REFERENCES,0.07463439233484619,"[50] Enyu Yu, Yan Fu, Qing Tang, Jun-Yan Zhao, and Duan-Bing Chen. A re-ranking algorithm for
identifying influential nodes in complex networks. IEEE Access, 8:211281–211290, 2020."
REFERENCES,0.0751386787695411,"[51] Wayne W Zachary. An information flow model for conflict and fission in small groups. Journal
of anthropological research, 33(4):452–473, 1977."
REFERENCES,0.07564296520423601,"[52] Ahmad Zareie and Rizos Sakellariou. Rumour spread minimization in social networks: A
source-ignorant approach. Online Social Networks and Media, 29:100206, 2022."
REFERENCES,0.07614725163893091,"[53] Jianguo Zheng and Li Pan. Least cost rumor community blocking optimization in social
networks. In 2018 third international conference on security of smart cities, industrial control
system and communications (SSIC), pages 1–5. IEEE, 2018."
REFERENCES,0.07665153807362582,"A
Appendix"
REFERENCES,0.07715582450832073,"A.1
Related Works"
REFERENCES,0.07766011094301563,"This section reviews existing studies on controlling the flow of misinformation in networks. While
most previous research has focused on detecting fake news through various features such as linguistic,
demographic, or community-based indicators, there has been comparatively less work on mitigating
misinformation. Mitigation strategies are typically categorized into three main approaches: removing
critical nodes, severing essential connections, and countering rumors with factual information. In
the following sections, we will first discuss the literature on misinformation detection, followed by
a review of studies aimed at mitigating the spread of misinformation. Finally, we will provide an
overview of approaches utilizing Graph Neural Networks (GNN) and Reinforcement Learning (RL)
to mitigate misinformation spread."
REFERENCES,0.07816439737771054,"Misinformation Detection:
Initial efforts in misinformation detection aimed at curbing rumor
spread through strategic node blocking. Wu et al. [47] developed a community detection algorithm
to segment network nodes, evaluate their influence, and block key nodes. Zheng and Pan [53]
addressed the least cost rumor community blocking (LCRCBO) using a community-centric influence
model and a greedy algorithm to select optimal nodes for containment. However, both methods have
raised concerns regarding cost-effectiveness and operational efficiency. Expanding on node-centric
approaches, Ding et al. [7] developed a dynamic rumor propagation model with algorithms to
identify and remove critical nodes and connections, introducing an ’outbreak threshold’ to evaluate
interventions. In contrast, Khalil et al. [19] and Yan et al. [49] advanced edge removal strategies
under a linear threshold (LT) model, creating heuristic algorithms to manage misinformation spread
effectively [18]. Tong and Du [43] used a hybrid sampling method, which could assign high
weights to users susceptible to misinformation, to pinpoint users most vulnerable to fake news, while
Zareie and Sakellariou [52] introduced a passive edge-blocking technique that leverages entropy
to balance network diffusion efficiency. Nguyen et al. [35] applied network analysis to explore
sentiment propagation in social networks, finding that sentiments often cluster within comment
threads, suggesting that online forums may serve as echo chambers that reinforce uniform opinions
among participants."
REFERENCES,0.07866868381240545,"Misinformation Propagation Minimization:
This research category promotes disseminating
truthful information as a counter-rumor measure. Lin et al. [26] suggested a crowdsourcing framework
to enable users to select from various collaborative or independent rumor control methods. Tong et
al. [41] analyzed the effectiveness of the peer-to-peer independent cascade (PIC) model in private
social networks, where independent rumor agents create ’positive cascades’, and demonstrated that
such strategies are robust under Nash equilibria. Yan et al. [48] identified the rumor minimization
challenge as monotonically decreasing and devised a two-stage process for selecting effective
blocking candidates. Manouchehri et al. [30] addressed maximizing influence blocking (IBM) with
considerations for timing and urgency, proposing an efficient, theoretically sound sampling method.
Lastly, Srinivasan and LD [39] proposed a competitive cascade model that focuses on leveraging user
opinions and the critical nature of rumors to identify and activate influential nodes promoting positive
information."
REFERENCES,0.07917297024710035,"These studies highlight the complexity of misinformation propagation minimization and underscore
the need for deeper analysis. Our study builds on these works by exploring how misinformation
spreads under different environmental conditions and agent dynamics. We aim to propose more
effective mitigation strategies, contributing to a nuanced understanding of misinformation dynamics
and enhancing the resilience of information networks."
REFERENCES,0.07967725668179526,"GNN Based Approaches:
Graph Convolutional Networks (GCNs) are increasingly being utilized
to detect the rumors and propagation patterns within social networks. For instance, Wei et al. [46]
developed a GCN-based model to analyze user stances and conversation content for better rumor
detection. Ma et al. [28] created a tree kernel to compare similarities between subtrees in retweeting
trees. Kumar and Carley [22] employed a multitask learning framework to extract representations
from retweeting trees for rumor and stance detection. Similarly, Khoo et al. [20] explored various
influences within a retweeting tree and utilized a transformer model to enhance rumor detection by
learning the interactions among these influences. Moreover, Liu et al. [27] proposed a structure-aware"
REFERENCES,0.08018154311649017,"retweeting GNN that identifies rumor patterns based on retweeting behaviors. This method leverages
both node and structural-level data, suggesting that propagation paths offer distinct insights into the
credibility of the disseminated information. Contrastingly, while the detection of rumors has been
extensively studied, the use of GCNs for rumor minimization remains under-explored. Authors in
[15] introduce an innovative approach for blocking rumors on social networks by integrating user
opinions with confidence levels into a new model (CBOA) and employing a directed GCN (DGCN)
to identify and block critical nodes capable of mitigating rumor spread."
REFERENCES,0.08068582955118507,"However, there are opportunities to enhance their study. Our research investigates the scalability
of GCNs and their performance across three different environments with varying agent dynamics.
Additionally, we propose a novel ranking algorithm for training GNNs. Furthermore, we identify sce-
narios where supervised learning faces challenges and address these limitations using reinforcement
learning techniques."
REFERENCES,0.08119011598587998,"RL
Goindani and Neville [13] develop a social reinforcement learning approach to mitigate the
spread of fake news through social networks. Their method involves learning an intervention model
to enhance the spread of true news, thereby reducing the influence of fake news. The authors in [10]
model the news diffusion process using a Multivariate Hawkes Process (MHP) and employ policy
optimization to learn intervention strategies. Ohi et al. [36] investigate strategies to mitigate the
spread of pandemics using a reinforcement learning approach. The model is based on the SEIR
(Susceptible-Exposed-Infectious-Recovered) compartmental model. It allows for dynamic interaction
where individuals move randomly, influencing the spread of the disease. The agent is trained to
determine optimal movement restrictions (from no restrictions to full lockdowns) to minimize disease
spread while considering economic factors."
REFERENCES,0.08169440242057488,"Current research in this field largely focuses on identifying misinformation or removing nodes and
edges to limit rumor propagation. While some studies, such as those by He et al. [15], investigate
misinformation suppression methods, they often do not address complex environmental scenarios,
which our study aims to explore. Our research specifically targets the minimization of misinformation
spread after the detection of misinformed agents is done. We aim to hinder their attempts to dissemi-
nate false information by strategically blocking selective nodes with positive information. Our work
begins with the implementation of a GCN-based supervised learning model to detect misinformation.
To overcome the limitations of supervised learning, we further incorporate a reinforcement learning
paradigm. This progression enables us to develop and optimize more effective strategies within
complex network environments, ultimately enhancing the robustness of misinformation control
measures."
REFERENCES,0.08219868885526979,"A.2
Model Details"
REFERENCES,0.0827029752899647,"A.2.1
Graph Neural Networks (GNNs)"
REFERENCES,0.0832072617246596,"Graph Neural Networks (GCNs) are advanced deep learning models tailored for handling data with
a graph structure. Such models are particularly adept at processing information within complex
networks by learning to synthesize node representations. These representations are derived through
the aggregation and transformation of information from neighboring nodes. Specifically, the represen-
tation of a node vi in a graph G is iteratively updated by integrating information from its immediate
neighbors, denoted as N(vi). This process employs a propagation function f, parameterized by
neural network weights W, and an activation function σ. The updated representation of a node in a
multi-layered GCN, as proposed in the literature, can be mathematically expressed as:"
REFERENCES,0.08371154815935451,"H(l+1) = σ

˜D−1"
REFERENCES,0.08421583459404942,2 ˜A ˜D−1
REFERENCES,0.08472012102874432,"2 H(l)W (l)
(4)"
REFERENCES,0.08522440746343923,"Here, ˜A = A + IN represents the augmented adjacency matrix of the graph G, incorporating self-
connections by adding the identity matrix IN. The diagonal matrix ˜Dii = P
j ˜Aij facilitates the
normalization of ˜A. The term W (l) denotes the weight matrix at layer l, and σ(·) is the activation
function. The matrix H(l) ∈RN×D encapsulates the node features at layer l."
REFERENCES,0.08572869389813415,"We engage with a graph comprising N nodes in the application discussed. Our GCN outputs a vector
O ∈RN×1, representing the likelihood of each node vi ∈V being pivotal for propagation through"
REFERENCES,0.08623298033282904,"the network. This mechanism enables the model to figure out significance of each node in mitigating
information spread within the graph structure."
REFERENCES,0.08673726676752395,"GCN Architecture Overview
Our model is structured as follows: It comprises three graph
convolutional layers followed by a linear layer. The architecture is designed to progressively transform
the input node features into a space where the final classification (e.g., determining the likelihood of
a node being pivotal in propagation minimization)."
REFERENCES,0.08724155320221887,"• Initial Graph Convolution: The model begins with a graph convolutional layer (GCNConv),
taking input_size features and transforming them into a hidden representation of size
hidden_size. This layer is followed by a ReLU activation function."
REFERENCES,0.08774583963691376,"• Hidden Graph Convolutional Layers: After the initial layer, the architecture includes four
GCNConv layers, all utilizing hidden_size units. These layers are designed to iteratively
process and refine the features extracted from the graph’s structure. Each of these layers is
followed by a ReLU activation to introduce non-linearity."
REFERENCES,0.08825012607160868,"– The model employs repeated application of the GCNConv layer with hidden_size
units, demonstrating the capacity to deepen the network’s understanding of the graph’s
topology through successive transformations."
REFERENCES,0.08875441250630359,"• Output Graph Convolution: A final GCNConv layer reduces the hidden representation to
the desired num_classes, preparing the features for the prediction task."
REFERENCES,0.08925869894099848,"• Linear Layer and Sigmoid Activation: The architecture concludes with a linear transforma-
tion (nn.Linear) directly mapping the output of the last GCNConv layer to num_classes.
A sigmoid activation function is applied to this output, producing probabilities for each class
in a binary classification scenario."
REFERENCES,0.0897629853756934,"In this specific implementation, the input_size is set to 3, indicative of the initial feature dimen-
sionality per node. The hidden_size is configured at 128, providing a substantial feature space
for intermediate representations. Lastly, the num_classes is established at 1, signifying only one
numerical output for each nodes."
REFERENCES,0.0902672718103883,"Packages Used
The development of our supervised learning models, particularly those utilizing
graph convolutional networks, leveraged several Python packages instrumental in defining, training,
and evaluating our models. Below is a list of these packages and a brief description of their roles in
our implementation:"
REFERENCES,0.0907715582450832,"• torch: Serves as the foundational framework for constructing and training various neural
network models, including those for graph-based data."
REFERENCES,0.09127584467977812,"• torch_geometric: An extension of PyTorch tailored for graph neural networks. It provides
efficient data structures for graphs and a collection of methods for graph convolutional
operations, making it essential for implementing graph convolutional neural networks
(GCNs)."
REFERENCES,0.09178013111447302,"• networkx: Utilized for generating and manipulating complex networks. In our project,
networkx is primarily used for creating synthetic graph data and for preprocessing tasks
that require graph analysis and manipulations before feeding the data into the neural network
models."
REFERENCES,0.09228441754916793,"A.2.2
Residual Network (ResNet)"
REFERENCES,0.09278870398386284,"The Residual Network (ResNet) model implemented in our study is a variant of the conventional
ResNet architecture. The core component of our ResNet model is the ResidualBlock, which allows
for the training of deeper networks by addressing the vanishing gradient problem through skip
connections. Each ResidualBlock consists of two sequences of convolutional layers (Conv2d), batch
normalization (BatchNorm2d), and sigmoid activations. A distinctive feature is the adaptation of the
skip connection to include a convolutional layer and batch normalization if there is a discrepancy in
the input and output channels or the stride is not equal to one."
REFERENCES,0.09329299041855774,ResNet Architecture Overview
REFERENCES,0.09379727685325265,"• Initial Convolution: Begins with a convolutional layer applying 64 filters of size 3x3,
followed by batch normalization and sigmoid activation."
REFERENCES,0.09430156328794756,"• Residual Blocks: Three main layers (layer1, layer2, layer3) constructed with the
_make_layer method. Each layer contains a sequence of ResidualBlocks, with channel
sizes of 32, 64, and 128, respectively. The number of blocks per layer is determined by the
num_blocks parameter."
REFERENCES,0.09480584972264246,"– Each ResidualBlock implements two sequences of convolutional operations, batch
normalization, and sigmoid activation, with an optional convolution in the shortcut
connection for channel or stride adjustments."
REFERENCES,0.09531013615733737,"• Adaptive Input Reshaping: Inputs are dynamically reshaped to a square form based on the
square root of the second dimension, ensuring compatibility with different input sizes."
REFERENCES,0.09581442259203228,"• Pooling and Output Layer: Concludes with an average pooling layer to reduce spatial
dimensions, followed by a fully connected layer mapping 128 features to a single output,
thus producing the final prediction."
REFERENCES,0.09631870902672718,"Training Details
Our model is trained using the PyTorch library, leveraging its comprehen-
sive suite of neural network tools and functions. The optimizer of choice is the Adam optimizer
(torch.optim.Adam), selected for its adaptive learning rate properties, which helps in converging
faster. The learning rate was set to 0.0005, balancing the trade-off between training speed and the
risk of overshooting minimal loss values. The training process involved the iterative adjustment of
weights through backpropagation, minimizing the loss calculated at the output layer. This procedure
was executed repeatedly over batches of training data, with the model parameters updated in each
iteration to reduce the prediction error."
REFERENCES,0.09682299546142209,"Packages Used
The implementation of our ResNet model and the training process was facilitated
by the following Python packages:"
REFERENCES,0.09732728189611699,• torch: Provides the core framework for defining and training neural networks.
REFERENCES,0.0978315683308119,"• torch.nn: A submodule of PyTorch that contains classes and methods specifically designed
for building neural networks."
REFERENCES,0.09833585476550681,"• torch.nn.functional: Offers functional interfaces for operations used in building neural
networks, like activations and pooling functions."
REFERENCES,0.09884014120020171,"• torch.optim: Contains optimizers such as Adam, which are used for updating model
parameters during training."
REFERENCES,0.09934442763489662,"A.3
Metrics"
REFERENCES,0.09984871406959153,Training Loss
REFERENCES,0.10035300050428643,"• SL: In the Supervised Learning (SL) framework, for our study, we employed the Binary
Cross Entropy (BCE) loss to train the Graph Convolutional Network (GCN) model. Here,
we delineate the iterative process used during the training phase under this setting. The
model is fed with a new graph state at the beginning of each iteration. The GCN produces an
output vector O ∈[0, 1]N×1, where N is the number of nodes in the graph. Each component
of O, denoted as Oi, represents the probability that node i should be blocked to minimize
the propagation rate in the network. A greedy algorithm is employed to ascertain the optimal
node to block. For each node i, temporarily set the node as blocked. We compute the
propagation rate of the network with node i blocked. Then, we revert the blockage of node i
and proceed to evaluate the next node. We select the node that, when blocked, results in the
lowest propagation rate across the network. Upon determining the node j, which yields the
minimum propagation rate when blocked, a target vector T ∈0, 1N×1 is constructed such
that Tj = 1 (indicating the target node to block), and Ti = 0 for all i ̸= j.
The BCE loss between the output vector O and the target vector T is computed as follows:"
REFERENCES,0.10085728693898134,"BCE Loss = −1 N N
X"
REFERENCES,0.10136157337367625,"i=1
[Ti log(Oi) + (1 −Ti) log(1 −Oi)]"
REFERENCES,0.10186585980837115,"The Binary Cross Entropy (BCE) loss is particularly useful in this context because it
measures the performance of the model in terms of how effectively it can predict the binary
outcome (block/no block) for each node in the graph. Unlike our reinforcement learning
setup, which utilizes batch updates across multiple states or episodes, the supervised learning
approach updates the model weights based on the loss calculated from a single graph state
per iteration.
• RL: The training loss for our model is computed using the Mean Squared Error (MSE)
metric. In each episode, the loss is calculated across a batch of samples drawn from the
replay buffer. Specifically, it measures the squared difference between the predicted value
function of the current state from the policy network and the Bellman target — the observed
reward plus the value of the subsequent state as estimated by the target network. Executing
this calculation over batches of experiences allows the policy network to learn from a diverse
set of state transitions, thereby refining its predictions to better approximate the true expected
rewards through temporal difference learning."
REFERENCES,0.10237014624306606,"Evaluation Metric - Infection Rate
The infection rate measures the proportion of the network
that is infected over time and is a crucial metric for evaluating the spread of misinformation within a
simulated environment. It is calculated as the ratio of infected nodes to the total number of nodes
within the network at a given timestep:"
REFERENCES,0.10287443267776097,Infection Rate = Number of Infected Nodes
REFERENCES,0.10337871911245587,"Total Number of Nodes
This metric serves not only as a means to understand the dynamics of misinformation spread during
simulations but also as a vital testing metric for evaluating model performance on test datasets.
Models that effectively contain or reduce the infection rate are considered to have performed well, as
they demonstrate the ability to mitigate the spread of misinformation across the network."
REFERENCES,0.10388300554715078,"A.4
Hardware details"
REFERENCES,0.1043872919818457,"We have used two servers to run our experiments. One with 48-core nodes each hosting 2 V100 32G
GPUs and 128GB of RAM. Another with 256-cores, eight A100 40GB GPUs, and 1TB of RAM.
The processor speed is 2.8 GHz."
REFERENCES,0.10489157841654059,"A.5
Training Details"
REFERENCES,0.1053958648512355,"A.5.1
SL"
REFERENCES,0.1059001512859304,"Training Methodology:
Our SL setup is coupled with a ranking algorithm which is shown in
Algorithm 1. We GCN with an input size of 3 (opinion value, degree of node, proximity to source
node), a hidden size of 128, and an output size of 1. The model was trained using the Adam optimizer
with a learning rate of 0.001 and a binary cross-entropy loss function. The training process involved
1000 epochs, where in each epoch, a graph with 25 nodes was generated. During each epoch, the
model iteratively minimized the infection rate by selecting nodes to block based on GCN output until
no uninfected nodes remained. The loss was calculated and backpropagated, and the weights were
updated accordingly. The total loss for each epoch was averaged over iterations."
REFERENCES,0.10640443772062531,"A.5.2
RL"
REFERENCES,0.10690872415532023,"Training Algorithm:
Shown in Algorithm 2.
In the DQN framework with experience replay, two neural networks are utilized: the target network
ˆQθ−and the policy network Qθ. The target network is periodically updated by copying weights from
the policy network, while the policy network is optimized continuously through the replay memory
D, which stores agent’s past experiences. During training, the agent samples random mini-batches of
transitions from D to minimize the loss function via gradient descent, enabling the policy network"
REFERENCES,0.10741301059001512,Algorithm 1 Ranking Algorithm
REFERENCES,0.10791729702471003,"1: Input: Graph G, infected nodes I, blocked nodes B, action budget K
2: Output: Target matrix T ∈RN×1"
REFERENCES,0.10842158345940495,"3: S ←initialize_simulation_network(G, I, B)
4: M ←get_uninfected_and_unblocked_nodes(S)
5: C ←combinations(M, K)
6: min_rate ←1
7: target_set ←None
8:
9: for c ∈C do
10:
temp_S ←S
11:
block_nodes(temp_S, c)
▷Temporarily block nodes in c by setting their opinion values to 1
12:
rate ←simulate_propagation(temp_S)
▷Infection Rate = Number of Infected Nodes"
REFERENCES,0.10892586989409984,"Total Number of Nodes
13:
if rate < min_rate then
14:
min_rate ←rate
15:
target_set ←c
16:
end if
17: end for
18:
19: T ←[0]N×1"
REFERENCES,0.10943015632879476,"20: for i ∈target_set do
21:
T[i] ←1
22: end for
23:
24: return T"
REFERENCES,0.10993444276348967,"to estimate optimal actions. This strategy helps the agent break the temporal correlation inherent in
sequential data, enhancing the stability and convergence of the learning process. We use the Mean
Squared Error (MSE) metric to calculate the loss value between the policy network and the target
network."
REFERENCES,0.11043872919818457,"Training Methodology:
The Neural Network model is trained using a variant of Q-learning, with a
replay buffer approach to stabilize the learning process, aimed at learning the value function. Training
commences with a fully exploratory policy (epsilon = 1) and transitions to an exploitation-focused
strategy as epsilon decays over time. The learning rate is set to 5 × 10−4, and mean squared error
(MSE) loss is utilized to measure the prediction quality."
REFERENCES,0.11094301563287948,"At each episode, 200 random initial states are generated, with a selected parameter of the number of
initially infected nodes, and actions are determined through an epsilon-greedy method, balancing
between exploration and exploitation. The agent performs actions by selecting nodes in the network to
effectively contain misinformation spread. For each action, the network’s new state and corresponding
reward are observed, which are then stored in the replay buffer."
REFERENCES,0.11144730206757439,"Batch updates are carried out by sampling from this buffer, ensuring that learning occurs across a
diverse set of state-action-reward-next-state tuples. We have used a batch-size of 100 across the
experiments. The policy network parameters are optimized using the Adam optimizer, and the target
network’s parameters are periodically updated to reflect the policy network, reducing the likelihood
of divergence."
REFERENCES,0.11195158850226929,"The training process continues for 300 number of episodes, with the epsilon parameter decaying after
each timestep within an episode, encouraging the model to rely more on learned values rather than
random actions as training progresses. The duration of each episode and overall training, along with
average rewards and loss, are logged for post-training analysis. The model parameters yielding the
best performance on the validation set are preserved for subsequent evaluation phases."
REFERENCES,0.1124558749369642,"A.6
Inference Results"
REFERENCES,0.1129601613716591,"A.6.1
SL"
REFERENCES,0.11346444780635401,"We have conducted comprehensive testing of our SL model across various topographies and environ-
ments, examining the performance under different conditions. The overall performance comparison"
REFERENCES,0.11396873424104892,Algorithm 2 DVN with experience replay
REFERENCES,0.11447302067574382,"1: Input: states per episode n, batch size m, action budget k, parameter update interval T ′, max number of
episodes emax
2: Output: V network ˆVθ
3: Initialize experience replay memory D
4: Initialize policy network V with random weights θ
5: Initialize target network ˆV with weights θ−"
REFERENCES,0.11497730711043873,"6: Initialize ϵ-decay to 1 and anneal to 0.1 with training
7: for e = 1 to emax do
8:
t ←1
9:
Generate n random states st with initial infected nodes
10:
Calculate candidate node set C for st
11:
while any |C| > 0 do
12:
Initialize blocker set Bt ←∅
13:
Randomly sample a number x from uniform distribution U(0, 1)
14:
if x < ϵ then
15:
Randomly sample k candidates from C as blocker set Bt
16:
else
17:
Initialize the infection prediction set K ←∅
18:
for all node u of candidate set C do
19:
Calculate the infection number Ku using Vπ(st+1), where st+1 is the state resulting after
taking action u in state st
20:
Append Ku to K
21:
end for
22:
Select the k nodes with the least infection prediction from K as the blocker set Bt
23:
end if
24:
Block the nodes in the blocker set Bt
25:
Update the state st+1
26:
Update the candidate set C
27:
Update the reward rt
28:
t ←t + 1
29:
for i = 0 to n −1 do
30:
Store the transition (si
t−1, Bi
t, ri
t, si
t) in D
31:
end for
32:
Sample a random minibatch of m transitions (sj, aj, rj, sj+1) from D"
REFERENCES,0.11548159354513364,"33:
yj ←"
REFERENCES,0.11598587997982854,"(
rj
if episode terminates at step j + 1
rj + ˆVθ−(sj+1)
otherwise
34:
Calculate loss using mean squared error between yj and Vθ(sj), set gradients to zero, perform
backpropagation, and update weights using the Adam optimizer
35:
if t mod T ′ = 0 then
36:
Update target network θ−←θ
37:
end if
38:
end while
39: end for"
REFERENCES,0.11649016641452345,"for each model under these varied conditions is illustrated in Figure 3. This figure provides a com-
prehensive view of how the model performs across the different environments and topographical
scenarios. 1.0 2.0 3.0"
REFERENCES,0.11699445284921836,Budget 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
REFERENCES,0.11749873928391326,Infection Rate
REFERENCES,0.11800302571860817,Case 1: Tree
REFERENCES,0.11850731215330308,"Methods
gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0"
REFERENCES,0.11901159858799798,Budget
REFERENCES,0.11951588502269289,Case 1: Erdos Renyi
REFERENCES,0.1200201714573878,"Methods
gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0"
REFERENCES,0.1205244578920827,Budget
REFERENCES,0.12102874432677761,Case 1: Watts Strogatz
REFERENCES,0.12153303076147251,"Methods
gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0"
REFERENCES,0.12203731719616742,Budget 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
REFERENCES,0.12254160363086233,Infection Rate
REFERENCES,0.12304589006555723,Case 2: Tree
REFERENCES,0.12355017650025214,"Methods
gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0"
REFERENCES,0.12405446293494705,Budget
REFERENCES,0.12455874936964195,Case 2: Erdos Renyi
REFERENCES,0.12506303580433686,"Methods
gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0"
REFERENCES,0.12556732223903178,Budget
REFERENCES,0.1260716086737267,Case 2: Watts Strogatz
REFERENCES,0.12657589510842157,"Methods
gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0"
REFERENCES,0.12708018154311648,Budget 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
REFERENCES,0.1275844679778114,Infection Rate
REFERENCES,0.1280887544125063,Case 3: Tree
REFERENCES,0.12859304084720122,"Methods
gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0"
REFERENCES,0.12909732728189613,Budget
REFERENCES,0.129601613716591,Case 3: Erdos Renyi
REFERENCES,0.13010590015128592,"Methods
gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0"
REFERENCES,0.13061018658598084,Budget
REFERENCES,0.13111447302067575,Case 3: Watts Strogatz
REFERENCES,0.13161875945537066,"Methods
gnn
random
max_degree_static
max_degree_dynamic"
REFERENCES,0.13212304589006554,"Figure 3: Comparative analysis of the GCN-Based SL Model Against Baseline Models Across
Different Network Types and Budgets. Each subfigure represents one of the three cases (1, 2, and
3), organized by rows, for three different types of networks: Tree, Erd˝os-Rényi, and Watts-Strogatz,
organized by columns. Within each panel, the infection rate is plotted for four methodologies. SL
based on GCN (blue), random node selection (orange), static selection of maximum degrees (green),
and dynamic selection of maximum degrees (red) across three levels of budget (1, 2, and 3). These
results underscore the variability in performance with changes in network structure and budget
allocation, highlighting the superior effectiveness of the GCN model in simpler cases and under
increased budget conditions, with diminishing returns in more complex environments."
REFERENCES,0.13262733232476046,"Dataset v2 Testing In addition to the initial dataset, we have also tested our model using dataset v2.
For a more granular analysis, we have compiled the test results into three distinct cases:"
REFERENCES,0.13313161875945537,• Case 1: Detailed in Figure 4 1.0 2.0 3.0 4.0
REFERENCES,0.13363590519415028,Average Degree 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
REFERENCES,0.1341401916288452,Infection Rate
REFERENCES,0.1346444780635401,"Sources: 1, Budget: 1 1.0 2.0 3.0 4.0"
REFERENCES,0.13514876449823499,Average Degree
REFERENCES,0.1356530509329299,"Sources: 1, Budget: 2 1.0 2.0 3.0 4.0"
REFERENCES,0.1361573373676248,Average Degree
REFERENCES,0.13666162380231972,"Sources: 1, Budget: 3"
REFERENCES,0.13716591023701463,Methods
REFERENCES,0.13767019667170954,"gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0 4.0"
REFERENCES,0.13817448310640443,Average Degree 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
REFERENCES,0.13867876954109934,Infection Rate
REFERENCES,0.13918305597579425,"Sources: 2, Budget: 1 1.0 2.0 3.0 4.0"
REFERENCES,0.13968734241048916,Average Degree
REFERENCES,0.14019162884518407,"Sources: 2, Budget: 2 1.0 2.0 3.0 4.0"
REFERENCES,0.14069591527987896,Average Degree
REFERENCES,0.14120020171457387,"Sources: 2, Budget: 3 1.0 2.0 3.0 4.0"
REFERENCES,0.14170448814926878,Average Degree 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
REFERENCES,0.1422087745839637,Infection Rate
REFERENCES,0.1427130610186586,"Sources: 3, Budget: 1 1.0 2.0 3.0 4.0"
REFERENCES,0.14321734745335352,Average Degree
REFERENCES,0.1437216338880484,"Sources: 3, Budget: 2 1.0 2.0 3.0 4.0"
REFERENCES,0.1442259203227433,Average Degree
REFERENCES,0.14473020675743822,"Sources: 3, Budget: 3"
REFERENCES,0.14523449319213314,"Figure 4: Case-1: Comparative Mean Infection Rate across different parameter settings for a GCN-
based SL model trained on a 25-node dataset and tested on dataset v2 consisting of 50 nodes"
REFERENCES,0.14573877962682805,• Case 2: Detailed in Figure 5 1.0 2.0 3.0 4.0
REFERENCES,0.14624306606152296,Average Degree 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.14674735249621784,Infection Rate
REFERENCES,0.14725163893091275,"Sources: 1, Budget: 1 1.0 2.0 3.0 4.0"
REFERENCES,0.14775592536560767,Average Degree
REFERENCES,0.14826021180030258,"Sources: 1, Budget: 2 1.0 2.0 3.0 4.0"
REFERENCES,0.1487644982349975,Average Degree
REFERENCES,0.14926878466969237,"Sources: 1, Budget: 3"
REFERENCES,0.14977307110438728,Methods
REFERENCES,0.1502773575390822,"gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0 4.0"
REFERENCES,0.1507816439737771,Average Degree 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.15128593040847202,Infection Rate
REFERENCES,0.15179021684316693,"Sources: 2, Budget: 1 1.0 2.0 3.0 4.0"
REFERENCES,0.15229450327786181,Average Degree
REFERENCES,0.15279878971255673,"Sources: 2, Budget: 2 1.0 2.0 3.0 4.0"
REFERENCES,0.15330307614725164,Average Degree
REFERENCES,0.15380736258194655,"Sources: 2, Budget: 3 1.0 2.0 3.0 4.0"
REFERENCES,0.15431164901664146,Average Degree 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.15481593545133637,Infection Rate
REFERENCES,0.15532022188603126,"Sources: 3, Budget: 1 1.0 2.0 3.0 4.0"
REFERENCES,0.15582450832072617,Average Degree
REFERENCES,0.15632879475542108,"Sources: 3, Budget: 2 1.0 2.0 3.0 4.0"
REFERENCES,0.156833081190116,Average Degree
REFERENCES,0.1573373676248109,"Sources: 3, Budget: 3"
REFERENCES,0.1578416540595058,"Figure 5: Case-2: Comparative Mean Infection Rate across different parameter settings for a GCN-
based SL model trained on a 25-node dataset and tested on dataset v2 consisting of 50 nodes"
REFERENCES,0.1583459404942007,• Case 3: Detailed in Figure 6 1.0 2.0 3.0 4.0
REFERENCES,0.1588502269288956,Average Degree 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.15935451336359052,Infection Rate
REFERENCES,0.15985879979828543,"Sources: 1, Budget: 1 1.0 2.0 3.0 4.0"
REFERENCES,0.16036308623298035,Average Degree
REFERENCES,0.16086737266767523,"Sources: 1, Budget: 2 1.0 2.0 3.0 4.0"
REFERENCES,0.16137165910237014,Average Degree
REFERENCES,0.16187594553706505,"Sources: 1, Budget: 3"
REFERENCES,0.16238023197175996,Methods
REFERENCES,0.16288451840645488,"gnn
random
max_degree_static
max_degree_dynamic 1.0 2.0 3.0 4.0"
REFERENCES,0.16338880484114976,Average Degree 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.16389309127584467,Infection Rate
REFERENCES,0.16439737771053958,"Sources: 2, Budget: 1 1.0 2.0 3.0 4.0"
REFERENCES,0.1649016641452345,Average Degree
REFERENCES,0.1654059505799294,"Sources: 2, Budget: 2 1.0 2.0 3.0 4.0"
REFERENCES,0.16591023701462432,Average Degree
REFERENCES,0.1664145234493192,"Sources: 2, Budget: 3 1.0 2.0 3.0 4.0"
REFERENCES,0.1669188098840141,Average Degree 0.0 0.2 0.4 0.6 0.8
REFERENCES,0.16742309631870902,Infection Rate
REFERENCES,0.16792738275340394,"Sources: 3, Budget: 1 1.0 2.0 3.0 4.0"
REFERENCES,0.16843166918809885,Average Degree
REFERENCES,0.16893595562279376,"Sources: 3, Budget: 2 1.0 2.0 3.0 4.0"
REFERENCES,0.16944024205748864,Average Degree
REFERENCES,0.16994452849218356,"Sources: 3, Budget: 3"
REFERENCES,0.17044881492687847,"Figure 6: Case-3: Comparative Mean Infection Rate across different parameter settings for a GCN-
based SL model trained on a 25-node dataset and tested on dataset v2 consisting of 50 nodes"
REFERENCES,0.17095310136157338,"A.6.2
RL"
REFERENCES,0.1714573877962683,"Comparison of MSE loss across different reward functions for Case-1: Figure 7. 10
4 10
3 10
2 10
1 10
0 Loss"
REFERENCES,0.17196167423096317,"Inf.: 1, Act.: 1 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.17246596066565809,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.172970247100353,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.1734745335350479,"R0
R1
R2
R3
R4 10
4 10
3 10
2 10
1 10
0 10
1 Loss"
REFERENCES,0.17397881996974282,"Inf.: 2, Act.: 1 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.17448310640443773,"Inf.: 2, Act.: 2 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.17498739283913262,"Inf.: 2, Act.: 3"
REFERENCES,0.17549167927382753,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.17599596570852244,"Episodes 10
4 10
3 10
2 10
1 10
0 10
1 Loss"
REFERENCES,0.17650025214321735,"Inf.: 3, Act.: 1"
REFERENCES,0.17700453857791226,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.17750882501260717,"Episodes 10
4 10
3 10
2 10
1 10
0 10
1"
REFERENCES,0.17801311144730206,"Inf.: 3, Act.: 2"
REFERENCES,0.17851739788199697,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.17902168431669188,"Episodes 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.1795259707513868,"10
1
Inf.: 3, Act.: 3"
REFERENCES,0.1800302571860817,Loss Plots for Case1 - 50 Nodes
REFERENCES,0.1805345436207766,"Figure 7: Case-1: Comparative MSE loss across different reward functions for a ResNet model
trained on a 50-node dataset. Columns represent an increase in action budget during training, while
rows indicate a rise in the number of initial infected nodes."
REFERENCES,0.1810388300554715,"Comparison of Mean Infection Rate across different reward functions, showcasing that the reward R3
performs better in model configurations with higher action budget and higher initial infected nodes:
Figure 8"
REFERENCES,0.1815431164901664,"50_1_1
50_1_2
50_1_3
Model 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.18204740292486132,Mean Infection Rate
REFERENCES,0.18255168935955624,Reward
REFERENCES,0.18305597579425115,"r0
r1
r2
r3
r4"
REFERENCES,0.18356026222894603,"50_2_1
50_2_2
50_2_3
Model"
REFERENCES,0.18406454866364094,Mean Infection Rate
REFERENCES,0.18456883509833585,Reward
REFERENCES,0.18507312153303077,"r0
r1
r2
r3
r4"
REFERENCES,0.18557740796772568,"50_3_1
50_3_2
50_3_3
Model"
REFERENCES,0.18608169440242056,Mean Infection Rate
REFERENCES,0.18658598083711547,Reward
REFERENCES,0.18709026727181038,"r0
r1
r2
r3
r4"
REFERENCES,0.1875945537065053,Mean Infection Rate for 50 nodes on Infection Degree 3 dataset
REFERENCES,0.1880988401412002,"Figure 8: Case-1: Comparative Mean Infection Rate across different reward functions for a ResNet
model trained on a 50-node dataset tested on Dataset v2 of 50 nodes with degree of connectivity 3."
REFERENCES,0.18860312657589512,Case-1
REFERENCES,0.18910741301059,"• Type: Binary Opinion and Binary Trust.
• Opinion Dynamic Model: Discrete Switching."
REFERENCES,0.18961169944528491,"R0
The loss plot is presented in Figure 9. Dataset v1 Inference Result: 50 Nodes - Figure 10. 10
1 10
0 Loss"
REFERENCES,0.19011598587997983,"Inf.: 1, Act.: 1 10
1"
REFERENCES,0.19062027231467474,"Inf.: 1, Act.: 2 10
3 10
2 10
1"
REFERENCES,0.19112455874936965,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.19162884518406456,"10
25
50
10
25
50
10
25
50 10
0 10
1 Loss"
REFERENCES,0.19213313161875945,"Inf.: 2, Act.: 1 10
0"
REFERENCES,0.19263741805345436,"Inf.: 2, Act.: 2 10
0"
REFERENCES,0.19314170448814927,"Inf.: 2, Act.: 3"
REFERENCES,0.19364599092284418,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.1941502773575391,"Episodes 10
1 Loss"
REFERENCES,0.19465456379223398,"Inf.: 3, Act.: 1"
REFERENCES,0.1951588502269289,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.1956631366616238,"Episodes 10
0 10
1"
REFERENCES,0.1961674230963187,"Inf.: 3, Act.: 2"
REFERENCES,0.19667170953101362,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.19717599596570853,"Episodes 10
0"
REFERENCES,0.19768028240040342,"Inf.: 3, Act.: 3"
REFERENCES,0.19818456883509833,Loss Plots for case1 - r0
REFERENCES,0.19868885526979324,"Figure 9: Case-1 using R0: Training MSE loss evolution for RL policy using ResNet model across networks of
10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources (Inf.) and action budgets
(Act.). Plotted on a logarithmic scale, the loss decreases over episodes, indicating improved policy performance
and adaptation across network sizes."
REFERENCES,0.19919314170448815,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.19969742813918306,Mean Infection Rate
REFERENCES,0.20020171457387798,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.20070600100857286,Actions
REFERENCES,0.20121028744326777,"a1
a2
a3"
REFERENCES,0.20171457387796268,"d1
d2
d3
Dataset"
REFERENCES,0.2022188603126576,Mean Infection Rate
REFERENCES,0.2027231467473525,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.2032274331820474,Actions
REFERENCES,0.2037317196167423,"a1
a2
a3"
REFERENCES,0.2042360060514372,"d1
d2
d3
Dataset"
REFERENCES,0.20474029248613212,Mean Infection Rate
REFERENCES,0.20524457892082704,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.20574886535552195,Actions
REFERENCES,0.20625315179021683,"a1
a2
a3"
REFERENCES,0.20675743822491174,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.20726172465960666,Mean Infection Rate
REFERENCES,0.20776601109430157,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.20827029752899648,Actions
REFERENCES,0.2087745839636914,"a1
a2
a3"
REFERENCES,0.20927887039838627,"d1
d2
d3
Dataset"
REFERENCES,0.20978315683308119,Mean Infection Rate
REFERENCES,0.2102874432677761,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.210791729702471,Actions
REFERENCES,0.21129601613716592,"a1
a2
a3"
REFERENCES,0.2118003025718608,"d1
d2
d3
Dataset"
REFERENCES,0.21230458900655572,Mean Infection Rate
REFERENCES,0.21280887544125063,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.21331316187594554,Actions
REFERENCES,0.21381744831064045,"a1
a2
a3"
REFERENCES,0.21432173474533536,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.21482602118003025,Mean Infection Rate
REFERENCES,0.21533030761472516,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.21583459404942007,Actions
REFERENCES,0.21633888048411498,"a1
a2
a3"
REFERENCES,0.2168431669188099,"d1
d2
d3
Dataset"
REFERENCES,0.21734745335350478,Mean Infection Rate
REFERENCES,0.2178517397881997,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.2183560262228946,Actions
REFERENCES,0.2188603126575895,"a1
a2
a3"
REFERENCES,0.21936459909228442,"d1
d2
d3
Dataset"
REFERENCES,0.21986888552697934,Mean Infection Rate
REFERENCES,0.22037317196167422,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.22087745839636913,Actions
REFERENCES,0.22138174483106404,"a1
a2
a3"
REFERENCES,0.22188603126575895,"Figure 10: Case-1 using R0: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.22239031770045387,"R1
The loss plot is presented in Figure 11. Dataset v1 Inference Result: 50 Nodes - Figure 12. 10
3 10
2 Loss"
REFERENCES,0.22289460413514878,"Inf.: 1, Act.: 1 10
4 10
3 10
2 10
1"
REFERENCES,0.22339889056984366,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.22390317700453857,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.22440746343923348,"10
25
50
10
25
50
10
25
50 10
2 Loss"
REFERENCES,0.2249117498739284,"Inf.: 2, Act.: 1 10
3 10
2 10
1"
REFERENCES,0.2254160363086233,"Inf.: 2, Act.: 2 10
4 10
3 10
2"
REFERENCES,0.2259203227433182,"Inf.: 2, Act.: 3"
REFERENCES,0.2264246091780131,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.22692889561270801,"Episodes 10
2 10
1 Loss"
REFERENCES,0.22743318204740293,"Inf.: 3, Act.: 1"
REFERENCES,0.22793746848209784,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.22844175491679275,"Episodes 10
3 10
2 10
1"
REFERENCES,0.22894604135148763,"Inf.: 3, Act.: 2"
REFERENCES,0.22945032778618255,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.22995461422087746,"Episodes 10
3 10
2 10
1"
REFERENCES,0.23045890065557237,"Inf.: 3, Act.: 3"
REFERENCES,0.23096318709026728,Loss Plots for case1 - r1
REFERENCES,0.2314674735249622,"Figure 11: Case-1 using R1: Training MSE loss evolution for RL policy using ResNet model across
networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources
(Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over episodes,
indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.23197175995965708,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.232476046394352,Mean Infection Rate
REFERENCES,0.2329803328290469,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.2334846192637418,Actions
REFERENCES,0.23398890569843672,"a1
a2
a3"
REFERENCES,0.2344931921331316,"d1
d2
d3
Dataset"
REFERENCES,0.23499747856782652,Mean Infection Rate
REFERENCES,0.23550176500252143,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.23600605143721634,Actions
REFERENCES,0.23651033787191125,"a1
a2
a3"
REFERENCES,0.23701462430660616,"d1
d2
d3
Dataset"
REFERENCES,0.23751891074130105,Mean Infection Rate
REFERENCES,0.23802319717599596,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.23852748361069087,Actions
REFERENCES,0.23903177004538578,"a1
a2
a3"
REFERENCES,0.2395360564800807,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.2400403429147756,Mean Infection Rate
REFERENCES,0.2405446293494705,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.2410489157841654,Actions
REFERENCES,0.2415532022188603,"a1
a2
a3"
REFERENCES,0.24205748865355523,"d1
d2
d3
Dataset"
REFERENCES,0.24256177508825014,Mean Infection Rate
REFERENCES,0.24306606152294502,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.24357034795763993,Actions
REFERENCES,0.24407463439233484,"a1
a2
a3"
REFERENCES,0.24457892082702976,"d1
d2
d3
Dataset"
REFERENCES,0.24508320726172467,Mean Infection Rate
REFERENCES,0.24558749369641958,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.24609178013111446,Actions
REFERENCES,0.24659606656580937,"a1
a2
a3"
REFERENCES,0.24710035300050429,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.2476046394351992,Mean Infection Rate
REFERENCES,0.2481089258698941,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.248613212304589,Actions
REFERENCES,0.2491174987392839,"a1
a2
a3"
REFERENCES,0.24962178517397882,"d1
d2
d3
Dataset"
REFERENCES,0.25012607160867373,Mean Infection Rate
REFERENCES,0.25063035804336864,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.25113464447806355,Actions
REFERENCES,0.25163893091275846,"a1
a2
a3"
REFERENCES,0.2521432173474534,"d1
d2
d3
Dataset"
REFERENCES,0.2526475037821483,Mean Infection Rate
REFERENCES,0.25315179021684314,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.25365607665153805,Actions
REFERENCES,0.25416036308623297,"a1
a2
a3"
REFERENCES,0.2546646495209279,"Figure 12: Case-1 using R1: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.2551689359556228,"R2
The loss plot is presented in Figure 13. Dataset v1 Inference Result: 50 Nodes - Figure 14. 10
0 Loss"
REFERENCES,0.2556732223903177,"Inf.: 1, Act.: 1 10
1"
REFERENCES,0.2561775088250126,"Inf.: 1, Act.: 2 10
2 10
1"
REFERENCES,0.2566817952597075,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.25718608169440244,"10
25
50
10
25
50
10
25
50 10
0 10
1 Loss"
REFERENCES,0.25769036812909735,"Inf.: 2, Act.: 1 10
0"
REFERENCES,0.25819465456379226,"Inf.: 2, Act.: 2 10
0"
REFERENCES,0.2586989409984871,"Inf.: 2, Act.: 3"
REFERENCES,0.259203227433182,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.25970751386787694,"Episodes 10
1 Loss"
REFERENCES,0.26021180030257185,"Inf.: 3, Act.: 1"
REFERENCES,0.26071608673726676,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.2612203731719617,"Episodes 10
0 10
1"
REFERENCES,0.2617246596066566,"Inf.: 3, Act.: 2"
REFERENCES,0.2622289460413515,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.2627332324760464,"Episodes 10
0"
REFERENCES,0.2632375189107413,"Inf.: 3, Act.: 3"
REFERENCES,0.26374180534543623,Loss Plots for case1 - r2
REFERENCES,0.2642460917801311,"Figure 13: Case-1 using R2: Training MSE loss evolution for RL policy using ResNet model across
networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources
(Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over episodes,
indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.264750378214826,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.2652546646495209,Mean Infection Rate
REFERENCES,0.2657589510842158,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.26626323751891073,Actions
REFERENCES,0.26676752395360565,"a1
a2
a3"
REFERENCES,0.26727181038830056,"d1
d2
d3
Dataset"
REFERENCES,0.26777609682299547,Mean Infection Rate
REFERENCES,0.2682803832576904,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.2687846696923853,Actions
REFERENCES,0.2692889561270802,"a1
a2
a3"
REFERENCES,0.2697932425617751,"d1
d2
d3
Dataset"
REFERENCES,0.27029752899646997,Mean Infection Rate
REFERENCES,0.2708018154311649,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.2713061018658598,Actions
REFERENCES,0.2718103883005547,"a1
a2
a3"
REFERENCES,0.2723146747352496,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.27281896116994453,Mean Infection Rate
REFERENCES,0.27332324760463944,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.27382753403933435,Actions
REFERENCES,0.27433182047402926,"a1
a2
a3"
REFERENCES,0.2748361069087242,"d1
d2
d3
Dataset"
REFERENCES,0.2753403933434191,Mean Infection Rate
REFERENCES,0.27584467977811394,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.27634896621280886,Actions
REFERENCES,0.27685325264750377,"a1
a2
a3"
REFERENCES,0.2773575390821987,"d1
d2
d3
Dataset"
REFERENCES,0.2778618255168936,Mean Infection Rate
REFERENCES,0.2783661119515885,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.2788703983862834,Actions
REFERENCES,0.2793746848209783,"a1
a2
a3"
REFERENCES,0.27987897125567324,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.28038325769036815,Mean Infection Rate
REFERENCES,0.28088754412506306,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.2813918305597579,Actions
REFERENCES,0.28189611699445283,"a1
a2
a3"
REFERENCES,0.28240040342914774,"d1
d2
d3
Dataset"
REFERENCES,0.28290468986384265,Mean Infection Rate
REFERENCES,0.28340897629853756,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.2839132627332325,Actions
REFERENCES,0.2844175491679274,"a1
a2
a3"
REFERENCES,0.2849218356026223,"d1
d2
d3
Dataset"
REFERENCES,0.2854261220373172,Mean Infection Rate
REFERENCES,0.2859304084720121,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.28643469490670703,Actions
REFERENCES,0.2869389813414019,"a1
a2
a3"
REFERENCES,0.2874432677760968,"Figure 14: Case-1 using R2: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.2879475542107917,"R3
The loss plot is presented in Figure 15. Dataset v1 Inference Result: 50 Nodes - Figure 16. 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.2884518406454866,"Inf.: 1, Act.: 1 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.28895612708018154,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.28946041351487645,"10
0
Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.28996469994957136,"10
25
50
10
25
50
10
25
50 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.29046898638426627,"Inf.: 2, Act.: 1 10
4 10
3 10
2 10
1"
REFERENCES,0.2909732728189612,"10
0
Inf.: 2, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.2914775592536561,"10
0
Inf.: 2, Act.: 3"
REFERENCES,0.291981845688351,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.2924861321230459,"Episodes 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.2929904185577408,"Inf.: 3, Act.: 1"
REFERENCES,0.2934947049924357,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.2939989914271306,"Episodes 10
4 10
3 10
2 10
1"
REFERENCES,0.2945032778618255,"Inf.: 3, Act.: 2"
REFERENCES,0.2950075642965204,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.29551185073121533,"Episodes 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.29601613716591024,"Inf.: 3, Act.: 3"
REFERENCES,0.29652042360060515,Loss Plots for case1 - r3
REFERENCES,0.29702471003530007,"Figure 15: Case-1 using R3: Training MSE loss evolution for RL policy using ResNet model across
networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources
(Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over episodes,
indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.297528996469995,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.2980332829046899,Mean Infection Rate
REFERENCES,0.29853756933938475,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.29904185577407966,Actions
REFERENCES,0.29954614220877457,"a1
a2
a3"
REFERENCES,0.3000504286434695,"d1
d2
d3
Dataset"
REFERENCES,0.3005547150781644,Mean Infection Rate
REFERENCES,0.3010590015128593,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.3015632879475542,Actions
REFERENCES,0.3020675743822491,"a1
a2
a3"
REFERENCES,0.30257186081694404,"d1
d2
d3
Dataset"
REFERENCES,0.30307614725163895,Mean Infection Rate
REFERENCES,0.30358043368633386,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.3040847201210287,Actions
REFERENCES,0.30458900655572363,"a1
a2
a3"
REFERENCES,0.30509329299041854,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.30559757942511345,Mean Infection Rate
REFERENCES,0.30610186585980836,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.3066061522945033,Actions
REFERENCES,0.3071104387291982,"a1
a2
a3"
REFERENCES,0.3076147251638931,"d1
d2
d3
Dataset"
REFERENCES,0.308119011598588,Mean Infection Rate
REFERENCES,0.3086232980332829,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.30912758446797783,Actions
REFERENCES,0.30963187090267275,"a1
a2
a3"
REFERENCES,0.3101361573373676,"d1
d2
d3
Dataset"
REFERENCES,0.3106404437720625,Mean Infection Rate
REFERENCES,0.3111447302067574,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.31164901664145234,Actions
REFERENCES,0.31215330307614725,"a1
a2
a3"
REFERENCES,0.31265758951084216,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.31316187594553707,Mean Infection Rate
REFERENCES,0.313666162380232,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.3141704488149269,Actions
REFERENCES,0.3146747352496218,"a1
a2
a3"
REFERENCES,0.3151790216843167,"d1
d2
d3
Dataset"
REFERENCES,0.3156833081190116,Mean Infection Rate
REFERENCES,0.3161875945537065,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.3166918809884014,Actions
REFERENCES,0.3171961674230963,"a1
a2
a3"
REFERENCES,0.3177004538577912,"d1
d2
d3
Dataset"
REFERENCES,0.31820474029248613,Mean Infection Rate
REFERENCES,0.31870902672718104,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.31921331316187596,Actions
REFERENCES,0.31971759959657087,"a1
a2
a3"
REFERENCES,0.3202218860312658,"Figure 16: Case-1 using R3: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.3207261724659607,"R4
The loss plot is presented in Figure 17. Dataset v1 Inference Result: 50 Nodes - Figure 18. 10
3 10
2 10
1 Loss"
REFERENCES,0.32123045890065555,"Inf.: 1, Act.: 1 10
4 10
3 10
2"
REFERENCES,0.32173474533535046,"10
1
Inf.: 1, Act.: 2 10
4 10
3 10
2"
REFERENCES,0.32223903177004537,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.3227433182047403,"10
25
50
10
25
50
10
25
50 10
1 Loss"
REFERENCES,0.3232476046394352,"Inf.: 2, Act.: 1 10
2"
REFERENCES,0.3237518910741301,"10
1
Inf.: 2, Act.: 2 10
3 10
2 10
1"
REFERENCES,0.324256177508825,"Inf.: 2, Act.: 3"
REFERENCES,0.32476046394351993,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.32526475037821484,"Episodes 10
1 Loss"
REFERENCES,0.32576903681290975,"Inf.: 3, Act.: 1"
REFERENCES,0.32627332324760466,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.3267776096822995,"Episodes 10
2 10
1"
REFERENCES,0.32728189611699443,"Inf.: 3, Act.: 2"
REFERENCES,0.32778618255168934,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.32829046898638425,"Episodes 10
2 10
1"
REFERENCES,0.32879475542107917,"Inf.: 3, Act.: 3"
REFERENCES,0.3292990418557741,Loss Plots for case1 - r4
REFERENCES,0.329803328290469,"Figure 17: Case-1 using R4: Training MSE loss evolution for RL policy using ResNet model across
networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources
(Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over episodes,
indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.3303076147251639,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.3308119011598588,Mean Infection Rate
REFERENCES,0.3313161875945537,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.33182047402924864,Actions
REFERENCES,0.33232476046394355,"a1
a2
a3"
REFERENCES,0.3328290468986384,"d1
d2
d3
Dataset"
REFERENCES,0.3333333333333333,Mean Infection Rate
REFERENCES,0.3338376197680282,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.33434190620272314,Actions
REFERENCES,0.33484619263741805,"a1
a2
a3"
REFERENCES,0.33535047907211296,"d1
d2
d3
Dataset"
REFERENCES,0.3358547655068079,Mean Infection Rate
REFERENCES,0.3363590519415028,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.3368633383761977,Actions
REFERENCES,0.3373676248108926,"a1
a2
a3"
REFERENCES,0.3378719112455875,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.3383761976802824,Mean Infection Rate
REFERENCES,0.3388804841149773,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.3393847705496722,Actions
REFERENCES,0.3398890569843671,"a1
a2
a3"
REFERENCES,0.340393343419062,"d1
d2
d3
Dataset"
REFERENCES,0.34089762985375693,Mean Infection Rate
REFERENCES,0.34140191628845185,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.34190620272314676,Actions
REFERENCES,0.34241048915784167,"a1
a2
a3"
REFERENCES,0.3429147755925366,"d1
d2
d3
Dataset"
REFERENCES,0.3434190620272315,Mean Infection Rate
REFERENCES,0.34392334846192635,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.34442763489662126,Actions
REFERENCES,0.34493192133131617,"a1
a2
a3"
REFERENCES,0.3454362077660111,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.345940494200706,Mean Infection Rate
REFERENCES,0.3464447806354009,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.3469490670700958,Actions
REFERENCES,0.34745335350479073,"a1
a2
a3"
REFERENCES,0.34795763993948564,"d1
d2
d3
Dataset"
REFERENCES,0.34846192637418055,Mean Infection Rate
REFERENCES,0.34896621280887546,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.3494704992435703,Actions
REFERENCES,0.34997478567826523,"a1
a2
a3"
REFERENCES,0.35047907211296014,"d1
d2
d3
Dataset"
REFERENCES,0.35098335854765506,Mean Infection Rate
REFERENCES,0.35148764498234997,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.3519919314170449,Actions
REFERENCES,0.3524962178517398,"a1
a2
a3"
REFERENCES,0.3530005042864347,"Figure 18: Case-1 using R4: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.3535047907211296,Dataset v2 Results
REFERENCES,0.3540090771558245,"Degree of connectivity 1
50 Nodes - Figure 19"
REFERENCES,0.35451336359051944,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.35501765002521435,Mean Infection Rate
REFERENCES,0.3555219364599092,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.3560262228946041,Actions
REFERENCES,0.35653050932929903,"a1
a2
a3"
REFERENCES,0.35703479576399394,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.35753908219868885,Mean Infection Rate
REFERENCES,0.35804336863338376,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.3585476550680787,Actions
REFERENCES,0.3590519415027736,"a1
a2
a3"
REFERENCES,0.3595562279374685,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.3600605143721634,Mean Infection Rate
REFERENCES,0.3605648008068583,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.3610690872415532,Actions
REFERENCES,0.3615733736762481,"a1
a2
a3"
REFERENCES,0.362077660110943,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.3625819465456379,Mean Infection Rate
REFERENCES,0.3630862329803328,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.36359051941502774,Actions
REFERENCES,0.36409480584972265,"a1
a2
a3"
REFERENCES,0.36459909228441756,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.36510337871911247,Mean Infection Rate
REFERENCES,0.3656076651538074,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.3661119515885023,Actions
REFERENCES,0.36661623802319715,"a1
a2
a3"
REFERENCES,0.36712052445789206,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.367624810892587,Mean Infection Rate
REFERENCES,0.3681290973272819,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.3686333837619768,Actions
REFERENCES,0.3691376701966717,"a1
a2
a3"
REFERENCES,0.3696419566313666,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.37014624306606153,Mean Infection Rate
REFERENCES,0.37065052950075644,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.37115481593545135,Actions
REFERENCES,0.37165910237014627,"a1
a2
a3"
REFERENCES,0.3721633888048411,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.37266767523953603,Mean Infection Rate
REFERENCES,0.37317196167423095,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.37367624810892586,Actions
REFERENCES,0.37418053454362077,"a1
a2
a3"
REFERENCES,0.3746848209783157,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.3751891074130106,Mean Infection Rate
REFERENCES,0.3756933938477055,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.3761976802824004,Actions
REFERENCES,0.3767019667170953,"a1
a2
a3"
REFERENCES,0.37720625315179024,Case2 - Infection Rate for 50 Nodes on Dataset deg_1
REFERENCES,0.37771053958648515,"Figure 19: Case-1 inference on Dataset v2: Performance comparison of the RL policies trained using
ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of connectivity 1, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.37821482602118,Degree of connectivity 2 50 Nodes - Figure 20
REFERENCES,0.3787191124558749,"R0
R1
R2
R3
R4
Reward Type 0.000 0.025 0.050 0.075 0.100 0.125 0.150"
REFERENCES,0.37922339889056983,Mean Infection Rate
REFERENCES,0.37972768532526474,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.38023197175995965,Actions
REFERENCES,0.38073625819465456,"a1
a2
a3"
REFERENCES,0.3812405446293495,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.3817448310640444,Mean Infection Rate
REFERENCES,0.3822491174987393,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.3827534039334342,Actions
REFERENCES,0.3832576903681291,"a1
a2
a3"
REFERENCES,0.383761976802824,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.3842662632375189,Mean Infection Rate
REFERENCES,0.3847705496722138,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.3852748361069087,Actions
REFERENCES,0.3857791225416036,"a1
a2
a3"
REFERENCES,0.38628340897629854,"R0
R1
R2
R3
R4
Reward Type 0.000 0.025 0.050 0.075 0.100 0.125 0.150"
REFERENCES,0.38678769541099345,Mean Infection Rate
REFERENCES,0.38729198184568836,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.38779626828038327,Actions
REFERENCES,0.3883005547150782,"a1
a2
a3"
REFERENCES,0.3888048411497731,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.38930912758446795,Mean Infection Rate
REFERENCES,0.38981341401916286,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.3903177004538578,Actions
REFERENCES,0.3908219868885527,"a1
a2
a3"
REFERENCES,0.3913262733232476,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.3918305597579425,Mean Infection Rate
REFERENCES,0.3923348461926374,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.39283913262733233,Actions
REFERENCES,0.39334341906202724,"a1
a2
a3"
REFERENCES,0.39384770549672216,"R0
R1
R2
R3
R4
Reward Type 0.000 0.025 0.050 0.075 0.100 0.125 0.150"
REFERENCES,0.39435199193141707,Mean Infection Rate
REFERENCES,0.394856278366112,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.39536056480080684,Actions
REFERENCES,0.39586485123550175,"a1
a2
a3"
REFERENCES,0.39636913767019666,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.39687342410489157,Mean Infection Rate
REFERENCES,0.3973777105395865,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.3978819969742814,Actions
REFERENCES,0.3983862834089763,"a1
a2
a3"
REFERENCES,0.3988905698436712,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.39939485627836613,Mean Infection Rate
REFERENCES,0.39989914271306104,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.40040342914775595,Actions
REFERENCES,0.4009077155824508,"a1
a2
a3"
REFERENCES,0.4014120020171457,Case2 - Infection Rate for 50 Nodes on Dataset deg_2
REFERENCES,0.40191628845184063,"Figure 20: Case-1 inference on Dataset v2: Performance comparison of the RL policies trained using
ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of connectivity 2, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.40242057488653554,Degree of connectivity 3 50 Nodes - Figure 21
REFERENCES,0.40292486132123045,"R0
R1
R2
R3
R4
Reward Type 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.40342914775592537,Mean Infection Rate
REFERENCES,0.4039334341906203,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.4044377206253152,Actions
REFERENCES,0.4049420070600101,"a1
a2
a3"
REFERENCES,0.405446293494705,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.4059505799293999,Mean Infection Rate
REFERENCES,0.4064548663640948,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.4069591527987897,Actions
REFERENCES,0.4074634392334846,"a1
a2
a3"
REFERENCES,0.4079677256681795,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.4084720121028744,Mean Infection Rate
REFERENCES,0.40897629853756934,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.40948058497226425,Actions
REFERENCES,0.40998487140695916,"a1
a2
a3"
REFERENCES,0.4104891578416541,"R0
R1
R2
R3
R4
Reward Type 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.410993444276349,Mean Infection Rate
REFERENCES,0.4114977307110439,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.41200201714573875,Actions
REFERENCES,0.41250630358043366,"a1
a2
a3"
REFERENCES,0.4130105900151286,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.4135148764498235,Mean Infection Rate
REFERENCES,0.4140191628845184,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.4145234493192133,Actions
REFERENCES,0.4150277357539082,"a1
a2
a3"
REFERENCES,0.41553202218860313,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.41603630862329805,Mean Infection Rate
REFERENCES,0.41654059505799296,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.41704488149268787,Actions
REFERENCES,0.4175491679273828,"a1
a2
a3"
REFERENCES,0.41805345436207764,"R0
R1
R2
R3
R4
Reward Type 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.41855774079677255,Mean Infection Rate
REFERENCES,0.41906202723146746,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.41956631366616237,Actions
REFERENCES,0.4200706001008573,"a1
a2
a3"
REFERENCES,0.4205748865355522,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.4210791729702471,Mean Infection Rate
REFERENCES,0.421583459404942,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.42208774583963693,Actions
REFERENCES,0.42259203227433184,"a1
a2
a3"
REFERENCES,0.42309631870902675,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.4236006051437216,Mean Infection Rate
REFERENCES,0.4241048915784165,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.42460917801311143,Actions
REFERENCES,0.42511346444780634,"a1
a2
a3"
REFERENCES,0.42561775088250126,Case2 - Infection Rate for 50 Nodes on Dataset deg_3
REFERENCES,0.42612203731719617,"Figure 21: Case-1 inference on Dataset v2: Performance comparison of the RL policies trained using
ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of connectivity 3, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.4266263237518911,"Degree of connectivity 4
50 Nodes - Figure 22"
REFERENCES,0.427130610186586,"R0
R1
R2
R3
R4
Reward Type 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.4276348966212809,Mean Infection Rate
REFERENCES,0.4281391830559758,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.4286434694906707,Actions
REFERENCES,0.4291477559253656,"a1
a2
a3"
REFERENCES,0.4296520423600605,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.4301563287947554,Mean Infection Rate
REFERENCES,0.4306606152294503,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.43116490166414523,Actions
REFERENCES,0.43166918809884014,"a1
a2
a3"
REFERENCES,0.43217347453353505,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.43267776096822996,Mean Infection Rate
REFERENCES,0.4331820474029249,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.4336863338376198,Actions
REFERENCES,0.4341906202723147,"a1
a2
a3"
REFERENCES,0.43469490670700955,"R0
R1
R2
R3
R4
Reward Type 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.43519919314170447,Mean Infection Rate
REFERENCES,0.4357034795763994,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.4362077660110943,Actions
REFERENCES,0.4367120524457892,"a1
a2
a3"
REFERENCES,0.4372163388804841,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.437720625315179,Mean Infection Rate
REFERENCES,0.43822491174987394,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.43872919818456885,Actions
REFERENCES,0.43923348461926376,"a1
a2
a3"
REFERENCES,0.43973777105395867,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.4402420574886536,Mean Infection Rate
REFERENCES,0.44074634392334844,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.44125063035804335,Actions
REFERENCES,0.44175491679273826,"a1
a2
a3"
REFERENCES,0.4422592032274332,"R0
R1
R2
R3
R4
Reward Type 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.4427634896621281,Mean Infection Rate
REFERENCES,0.443267776096823,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.4437720625315179,Actions
REFERENCES,0.4442763489662128,"a1
a2
a3"
REFERENCES,0.44478063540090773,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.44528492183560264,Mean Infection Rate
REFERENCES,0.44578920827029755,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.4462934947049924,Actions
REFERENCES,0.4467977811396873,"a1
a2
a3"
REFERENCES,0.44730206757438223,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.44780635400907715,Mean Infection Rate
REFERENCES,0.44831064044377206,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.44881492687846697,Actions
REFERENCES,0.4493192133131619,"a1
a2
a3"
REFERENCES,0.4498234997478568,Case2 - Infection Rate for 50 Nodes on Dataset deg_4
REFERENCES,0.4503277861825517,"Figure 22: Case-1 inference on Dataset v2: Performance comparison of the RL policies trained using
ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of connectivity 4, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.4508320726172466,Case-2
REFERENCES,0.4513363590519415,"• Type: Floating Point Opinion and Binary Trust.
• Opinion Dynamic Model: Linear Adjustment."
REFERENCES,0.4518406454866364,"R0
The loss plot is presented in Figure 23. Dataset v1 Inference Result: 50 Nodes - Figure 24. 10
3 10
2 10
1 Loss"
REFERENCES,0.4523449319213313,"Inf.: 1, Act.: 1 10
4 10
3 10
2 10
1"
REFERENCES,0.4528492183560262,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.4533535047907211,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.45385779122541603,"10
25
50 10
1 10
0 Loss"
REFERENCES,0.45436207766011094,"Inf.: 2, Act.: 1 10
2 10
1"
REFERENCES,0.45486636409480585,"Inf.: 2, Act.: 2 10
3 10
2"
REFERENCES,0.45537065052950076,"Inf.: 2, Act.: 3"
REFERENCES,0.4558749369641957,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.4563792233988906,"Episodes 10
1 Loss"
REFERENCES,0.4568835098335855,"Inf.: 3, Act.: 1"
REFERENCES,0.45738779626828036,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.45789208270297527,"Episodes 10
2 10
1"
REFERENCES,0.4583963691376702,"Inf.: 3, Act.: 2"
REFERENCES,0.4589006555723651,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.45940494200706,"Episodes 10
2 10
1 10
0"
REFERENCES,0.4599092284417549,"Inf.: 3, Act.: 3"
REFERENCES,0.4604135148764498,Loss Plots for case2 - r0
REFERENCES,0.46091780131114474,"Figure 23: Case-2 using R0: Training MSE loss evolution for RL policy using ResNet model across
networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources
(Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over episodes,
indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.46142208774583965,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.46192637418053456,Mean Infection Rate
REFERENCES,0.46243066061522947,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.4629349470499244,Actions
REFERENCES,0.46343923348461924,"a1
a2
a3"
REFERENCES,0.46394351991931415,"d1
d2
d3
Dataset"
REFERENCES,0.46444780635400906,Mean Infection Rate
REFERENCES,0.464952092788704,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.4654563792233989,Actions
REFERENCES,0.4659606656580938,"a1
a2
a3"
REFERENCES,0.4664649520927887,"d1
d2
d3
Dataset"
REFERENCES,0.4669692385274836,Mean Infection Rate
REFERENCES,0.46747352496217853,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.46797781139687344,Actions
REFERENCES,0.46848209783156836,"a1
a2
a3"
REFERENCES,0.4689863842662632,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4694906707009581,Mean Infection Rate
REFERENCES,0.46999495713565304,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.47049924357034795,Actions
REFERENCES,0.47100353000504286,"a1
a2
a3"
REFERENCES,0.47150781643973777,"d1
d2
d3
Dataset"
REFERENCES,0.4720121028744327,Mean Infection Rate
REFERENCES,0.4725163893091276,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.4730206757438225,Actions
REFERENCES,0.4735249621785174,"a1
a2
a3"
REFERENCES,0.47402924861321233,"d1
d2
d3
Dataset"
REFERENCES,0.4745335350479072,Mean Infection Rate
REFERENCES,0.4750378214826021,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.475542107917297,Actions
REFERENCES,0.4760463943519919,"a1
a2
a3"
REFERENCES,0.47655068078668683,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.47705496722138174,Mean Infection Rate
REFERENCES,0.47755925365607665,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.47806354009077157,Actions
REFERENCES,0.4785678265254665,"a1
a2
a3"
REFERENCES,0.4790721129601614,"d1
d2
d3
Dataset"
REFERENCES,0.4795763993948563,Mean Infection Rate
REFERENCES,0.4800806858295512,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.48058497226424607,Actions
REFERENCES,0.481089258698941,"a1
a2
a3"
REFERENCES,0.4815935451336359,"d1
d2
d3
Dataset"
REFERENCES,0.4820978315683308,Mean Infection Rate
REFERENCES,0.4826021180030257,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.4831064044377206,Actions
REFERENCES,0.48361069087241554,"a1
a2
a3"
REFERENCES,0.48411497730711045,"Figure 24: Case-2 using R0: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.48461926374180536,"R1
The loss plot is presented in Figure 25. Dataset v1 Inference Result: 50 Nodes - Figure 26. 10
3 10
2 10
1 Loss"
REFERENCES,0.4851235501765003,"Inf.: 1, Act.: 1 10
4 10
3 10
2"
REFERENCES,0.4856278366111952,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.48613212304589004,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.48663640948058495,"10
25
50 10
2 10
1 Loss"
REFERENCES,0.48714069591527986,"Inf.: 2, Act.: 1 10
3 10
2 10
1"
REFERENCES,0.4876449823499748,"Inf.: 2, Act.: 2 10
3 10
2 10
1"
REFERENCES,0.4881492687846697,"Inf.: 2, Act.: 3"
REFERENCES,0.4886535552193646,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.4891578416540595,"Episodes 10
2 10
1 Loss"
REFERENCES,0.4896621280887544,"Inf.: 3, Act.: 1"
REFERENCES,0.49016641452344933,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.49067070095814425,"Episodes 10
3 10
2 10
1"
REFERENCES,0.49117498739283916,"Inf.: 3, Act.: 2"
REFERENCES,0.491679273827534,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.4921835602622289,"Episodes 10
3 10
2"
REFERENCES,0.49268784669692384,"Inf.: 3, Act.: 3"
REFERENCES,0.49319213313161875,Loss Plots for case2 - r1
REFERENCES,0.49369641956631366,"Figure 25: Case-2 using R1: Training MSE loss evolution for RL policy using ResNet model across
networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources
(Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over episodes,
indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.49420070600100857,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4947049924357035,Mean Infection Rate
REFERENCES,0.4952092788703984,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.4957135653050933,Actions
REFERENCES,0.4962178517397882,"a1
a2
a3"
REFERENCES,0.49672213817448313,"d1
d2
d3
Dataset"
REFERENCES,0.497226424609178,Mean Infection Rate
REFERENCES,0.4977307110438729,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.4982349974785678,Actions
REFERENCES,0.4987392839132627,"a1
a2
a3"
REFERENCES,0.49924357034795763,"d1
d2
d3
Dataset"
REFERENCES,0.49974785678265254,Mean Infection Rate
REFERENCES,0.5002521432173475,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.5007564296520424,Actions
REFERENCES,0.5012607160867373,"a1
a2
a3"
REFERENCES,0.5017650025214322,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5022692889561271,Mean Infection Rate
REFERENCES,0.502773575390822,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.5032778618255169,Actions
REFERENCES,0.5037821482602118,"a1
a2
a3"
REFERENCES,0.5042864346949067,"d1
d2
d3
Dataset"
REFERENCES,0.5047907211296017,Mean Infection Rate
REFERENCES,0.5052950075642966,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.5057992939989914,Actions
REFERENCES,0.5063035804336863,"a1
a2
a3"
REFERENCES,0.5068078668683812,"d1
d2
d3
Dataset"
REFERENCES,0.5073121533030761,Mean Infection Rate
REFERENCES,0.507816439737771,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.5083207261724659,Actions
REFERENCES,0.5088250126071608,"a1
a2
a3"
REFERENCES,0.5093292990418558,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5098335854765507,Mean Infection Rate
REFERENCES,0.5103378719112456,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.5108421583459405,Actions
REFERENCES,0.5113464447806354,"a1
a2
a3"
REFERENCES,0.5118507312153303,"d1
d2
d3
Dataset"
REFERENCES,0.5123550176500252,Mean Infection Rate
REFERENCES,0.5128593040847201,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.513363590519415,Actions
REFERENCES,0.51386787695411,"a1
a2
a3"
REFERENCES,0.5143721633888049,"d1
d2
d3
Dataset"
REFERENCES,0.5148764498234998,Mean Infection Rate
REFERENCES,0.5153807362581947,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.5158850226928896,Actions
REFERENCES,0.5163893091275845,"a1
a2
a3"
REFERENCES,0.5168935955622794,"Figure 26: Case-2 using R1: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.5173978819969742,"R2
The loss plot is presented in Figure 27. Dataset v1 Inference Result: 50 Nodes - Figure 28. 10
3 10
2 10
1 Loss"
REFERENCES,0.5179021684316691,"Inf.: 1, Act.: 1 10
4 10
3 10
2 10
1"
REFERENCES,0.518406454866364,"Inf.: 1, Act.: 2 10
4 10
3 10
2"
REFERENCES,0.518910741301059,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.5194150277357539,"10
25
50 10
2 Loss"
REFERENCES,0.5199193141704488,"Inf.: 2, Act.: 1 10
3 10
2"
REFERENCES,0.5204236006051437,"Inf.: 2, Act.: 2 10
4 10
3 10
2"
REFERENCES,0.5209278870398386,"Inf.: 2, Act.: 3"
REFERENCES,0.5214321734745335,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.5219364599092284,"Episodes 10
2 10
1 Loss"
REFERENCES,0.5224407463439233,"Inf.: 3, Act.: 1"
REFERENCES,0.5229450327786183,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.5234493192133132,"Episodes 10
3 10
2 10
1"
REFERENCES,0.5239536056480081,"Inf.: 3, Act.: 2"
REFERENCES,0.524457892082703,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.5249621785173979,"Episodes 10
3 10
2 10
1"
REFERENCES,0.5254664649520928,"Inf.: 3, Act.: 3"
REFERENCES,0.5259707513867877,Loss Plots for case2 - r2
REFERENCES,0.5264750378214826,"Figure 27: Case-2 using R2: Training MSE loss evolution for RL policy using ResNet model across
networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources
(Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over episodes,
indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.5269793242561776,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5274836106908725,Mean Infection Rate
REFERENCES,0.5279878971255674,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.5284921835602622,Actions
REFERENCES,0.5289964699949571,"a1
a2
a3"
REFERENCES,0.529500756429652,"d1
d2
d3
Dataset"
REFERENCES,0.5300050428643469,Mean Infection Rate
REFERENCES,0.5305093292990418,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.5310136157337367,Actions
REFERENCES,0.5315179021684316,"a1
a2
a3"
REFERENCES,0.5320221886031266,"d1
d2
d3
Dataset"
REFERENCES,0.5325264750378215,Mean Infection Rate
REFERENCES,0.5330307614725164,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.5335350479072113,Actions
REFERENCES,0.5340393343419062,"a1
a2
a3"
REFERENCES,0.5345436207766011,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.535047907211296,Mean Infection Rate
REFERENCES,0.5355521936459909,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.5360564800806858,Actions
REFERENCES,0.5365607665153808,"a1
a2
a3"
REFERENCES,0.5370650529500757,"d1
d2
d3
Dataset"
REFERENCES,0.5375693393847706,Mean Infection Rate
REFERENCES,0.5380736258194655,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.5385779122541604,Actions
REFERENCES,0.5390821986888553,"a1
a2
a3"
REFERENCES,0.5395864851235502,"d1
d2
d3
Dataset"
REFERENCES,0.540090771558245,Mean Infection Rate
REFERENCES,0.5405950579929399,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.5410993444276349,Actions
REFERENCES,0.5416036308623298,"a1
a2
a3"
REFERENCES,0.5421079172970247,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5426122037317196,Mean Infection Rate
REFERENCES,0.5431164901664145,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.5436207766011094,Actions
REFERENCES,0.5441250630358043,"a1
a2
a3"
REFERENCES,0.5446293494704992,"d1
d2
d3
Dataset"
REFERENCES,0.5451336359051941,Mean Infection Rate
REFERENCES,0.5456379223398891,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.546142208774584,Actions
REFERENCES,0.5466464952092789,"a1
a2
a3"
REFERENCES,0.5471507816439738,"d1
d2
d3
Dataset"
REFERENCES,0.5476550680786687,Mean Infection Rate
REFERENCES,0.5481593545133636,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.5486636409480585,Actions
REFERENCES,0.5491679273827534,"a1
a2
a3"
REFERENCES,0.5496722138174484,"Figure 28: Case-2 using R2: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.5501765002521433,"R3
The loss plot is presented in Figure 29. Dataset v1 Inference Result: 50 Nodes - Figure 30. 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.5506807866868382,"Inf.: 1, Act.: 1 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.551185073121533,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.5516893595562279,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.5521936459909228,"10
25
50 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.5526979324256177,"Inf.: 2, Act.: 1 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.5532022188603126,"Inf.: 2, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.5537065052950075,"Inf.: 2, Act.: 3"
REFERENCES,0.5542107917297024,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.5547150781643974,"Episodes 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.5552193645990923,"Inf.: 3, Act.: 1"
REFERENCES,0.5557236510337872,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.5562279374684821,"Episodes 10
4 10
3 10
2 10
1"
REFERENCES,0.556732223903177,"Inf.: 3, Act.: 2"
REFERENCES,0.5572365103378719,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.5577407967725668,"Episodes 10
4 10
3 10
2 10
1"
REFERENCES,0.5582450832072617,"Inf.: 3, Act.: 3"
REFERENCES,0.5587493696419567,Loss Plots for case2 - r3
REFERENCES,0.5592536560766516,"Figure 29: Case-2 using R3: Training MSE loss evolution for RL policy using ResNet model across
networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources
(Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over episodes,
indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.5597579425113465,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.5602622289460414,Mean Infection Rate
REFERENCES,0.5607665153807363,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.5612708018154312,Actions
REFERENCES,0.5617750882501261,"a1
a2
a3"
REFERENCES,0.562279374684821,"d1
d2
d3
Dataset"
REFERENCES,0.5627836611195158,Mean Infection Rate
REFERENCES,0.5632879475542107,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.5637922339889057,Actions
REFERENCES,0.5642965204236006,"a1
a2
a3"
REFERENCES,0.5648008068582955,"d1
d2
d3
Dataset"
REFERENCES,0.5653050932929904,Mean Infection Rate
REFERENCES,0.5658093797276853,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.5663136661623802,Actions
REFERENCES,0.5668179525970751,"a1
a2
a3"
REFERENCES,0.56732223903177,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.567826525466465,Mean Infection Rate
REFERENCES,0.5683308119011599,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.5688350983358548,Actions
REFERENCES,0.5693393847705497,"a1
a2
a3"
REFERENCES,0.5698436712052446,"d1
d2
d3
Dataset"
REFERENCES,0.5703479576399395,Mean Infection Rate
REFERENCES,0.5708522440746344,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.5713565305093293,Actions
REFERENCES,0.5718608169440242,"a1
a2
a3"
REFERENCES,0.5723651033787192,"d1
d2
d3
Dataset"
REFERENCES,0.5728693898134141,Mean Infection Rate
REFERENCES,0.573373676248109,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.5738779626828038,Actions
REFERENCES,0.5743822491174987,"a1
a2
a3"
REFERENCES,0.5748865355521936,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.5753908219868885,Mean Infection Rate
REFERENCES,0.5758951084215834,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.5763993948562783,Actions
REFERENCES,0.5769036812909732,"a1
a2
a3"
REFERENCES,0.5774079677256682,"d1
d2
d3
Dataset"
REFERENCES,0.5779122541603631,Mean Infection Rate
REFERENCES,0.578416540595058,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.5789208270297529,Actions
REFERENCES,0.5794251134644478,"a1
a2
a3"
REFERENCES,0.5799293998991427,"d1
d2
d3
Dataset"
REFERENCES,0.5804336863338376,Mean Infection Rate
REFERENCES,0.5809379727685325,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.5814422592032275,Actions
REFERENCES,0.5819465456379224,"a1
a2
a3"
REFERENCES,0.5824508320726173,"Figure 30: Case-2 using R3: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.5829551185073122,"R4
The loss plot is presented in Figure 31. Dataset v1 Inference Result: 50 Nodes - Figure 32. 10
3 10
2 10
1 Loss"
REFERENCES,0.5834594049420071,"Inf.: 1, Act.: 1 10
4 10
3 10
2"
REFERENCES,0.583963691376702,"Inf.: 1, Act.: 2 10
4 10
3 10
2"
REFERENCES,0.5844679778113969,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.5849722642460918,"10
25
50 10
1 Loss"
REFERENCES,0.5854765506807866,"Inf.: 2, Act.: 1 10
2 10
1"
REFERENCES,0.5859808371154815,"Inf.: 2, Act.: 2 10
3 10
2 10
1"
REFERENCES,0.5864851235501765,"Inf.: 2, Act.: 3"
REFERENCES,0.5869894099848714,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.5874936964195663,"Episodes 10
1 Loss"
REFERENCES,0.5879979828542612,"Inf.: 3, Act.: 1"
REFERENCES,0.5885022692889561,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.589006555723651,"Episodes 10
2 10
1"
REFERENCES,0.5895108421583459,"Inf.: 3, Act.: 2"
REFERENCES,0.5900151285930408,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.5905194150277358,"Episodes 10
2 10
1"
REFERENCES,0.5910237014624307,"Inf.: 3, Act.: 3"
REFERENCES,0.5915279878971256,Loss Plots for case2 - r4
REFERENCES,0.5920322743318205,"Figure 31: Case-2 using R4: Training MSE loss evolution for RL policy using ResNet model across
networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation sources
(Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over episodes,
indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.5925365607665154,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5930408472012103,Mean Infection Rate
REFERENCES,0.5935451336359052,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.5940494200706001,Actions
REFERENCES,0.594553706505295,"a1
a2
a3"
REFERENCES,0.59505799293999,"d1
d2
d3
Dataset"
REFERENCES,0.5955622793746849,Mean Infection Rate
REFERENCES,0.5960665658093798,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.5965708522440747,Actions
REFERENCES,0.5970751386787695,"a1
a2
a3"
REFERENCES,0.5975794251134644,"d1
d2
d3
Dataset"
REFERENCES,0.5980837115481593,Mean Infection Rate
REFERENCES,0.5985879979828542,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.5990922844175491,Actions
REFERENCES,0.599596570852244,"a1
a2
a3"
REFERENCES,0.600100857286939,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6006051437216339,Mean Infection Rate
REFERENCES,0.6011094301563288,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.6016137165910237,Actions
REFERENCES,0.6021180030257186,"a1
a2
a3"
REFERENCES,0.6026222894604135,"d1
d2
d3
Dataset"
REFERENCES,0.6031265758951084,Mean Infection Rate
REFERENCES,0.6036308623298033,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.6041351487644983,Actions
REFERENCES,0.6046394351991932,"a1
a2
a3"
REFERENCES,0.6051437216338881,"d1
d2
d3
Dataset"
REFERENCES,0.605648008068583,Mean Infection Rate
REFERENCES,0.6061522945032779,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.6066565809379728,Actions
REFERENCES,0.6071608673726677,"a1
a2
a3"
REFERENCES,0.6076651538073626,"d1
d2
d3
Dataset 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6081694402420574,Mean Infection Rate
REFERENCES,0.6086737266767523,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.6091780131114473,Actions
REFERENCES,0.6096822995461422,"a1
a2
a3"
REFERENCES,0.6101865859808371,"d1
d2
d3
Dataset"
REFERENCES,0.610690872415532,Mean Infection Rate
REFERENCES,0.6111951588502269,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.6116994452849218,Actions
REFERENCES,0.6122037317196167,"a1
a2
a3"
REFERENCES,0.6127080181543116,"d1
d2
d3
Dataset"
REFERENCES,0.6132123045890066,Mean Infection Rate
REFERENCES,0.6137165910237015,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.6142208774583964,Actions
REFERENCES,0.6147251638930913,"a1
a2
a3"
REFERENCES,0.6152294503277862,"Figure 32: Case-2 using R4: Performance comparison of the RL policies trained using ResNet model
for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3 of
Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.6157337367624811,Dataset v2 Results
REFERENCES,0.616238023197176,"Degree of connectivity 1
50 Nodes - Figure 33"
REFERENCES,0.6167423096318709,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.6172465960665658,Mean Infection Rate
REFERENCES,0.6177508825012608,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.6182551689359557,Actions
REFERENCES,0.6187594553706506,"a1
a2
a3"
REFERENCES,0.6192637418053455,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6197680282400403,Mean Infection Rate
REFERENCES,0.6202723146747352,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.6207766011094301,Actions
REFERENCES,0.621280887544125,"a1
a2
a3"
REFERENCES,0.6217851739788199,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6222894604135148,Mean Infection Rate
REFERENCES,0.6227937468482098,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.6232980332829047,Actions
REFERENCES,0.6238023197175996,"a1
a2
a3"
REFERENCES,0.6243066061522945,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.6248108925869894,Mean Infection Rate
REFERENCES,0.6253151790216843,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.6258194654563792,Actions
REFERENCES,0.6263237518910741,"a1
a2
a3"
REFERENCES,0.626828038325769,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.627332324760464,Mean Infection Rate
REFERENCES,0.6278366111951589,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.6283408976298538,Actions
REFERENCES,0.6288451840645487,"a1
a2
a3"
REFERENCES,0.6293494704992436,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6298537569339385,Mean Infection Rate
REFERENCES,0.6303580433686334,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.6308623298033282,Actions
REFERENCES,0.6313666162380231,"a1
a2
a3"
REFERENCES,0.6318709026727181,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.632375189107413,Mean Infection Rate
REFERENCES,0.6328794755421079,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.6333837619768028,Actions
REFERENCES,0.6338880484114977,"a1
a2
a3"
REFERENCES,0.6343923348461926,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6348966212808875,Mean Infection Rate
REFERENCES,0.6354009077155824,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.6359051941502774,Actions
REFERENCES,0.6364094805849723,"a1
a2
a3"
REFERENCES,0.6369137670196672,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6374180534543621,Mean Infection Rate
REFERENCES,0.637922339889057,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.6384266263237519,Actions
REFERENCES,0.6389309127584468,"a1
a2
a3"
REFERENCES,0.6394351991931417,Case2 - Infection Rate for 50 Nodes on Dataset deg_1
REFERENCES,0.6399394856278366,"Figure 33: Case-2 inference on Dataset v2: Performance comparison of the RL policies trained using
ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of Connectivity 1, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.6404437720625316,"Degree of connectivity 2
50 Nodes - Figure 34"
REFERENCES,0.6409480584972265,"R0
R1
R2
R3
R4
Reward Type 0.000 0.025 0.050 0.075 0.100 0.125 0.150"
REFERENCES,0.6414523449319214,Mean Infection Rate
REFERENCES,0.6419566313666163,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.6424609178013111,Actions
REFERENCES,0.642965204236006,"a1
a2
a3"
REFERENCES,0.6434694906707009,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6439737771053958,Mean Infection Rate
REFERENCES,0.6444780635400907,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.6449823499747857,Actions
REFERENCES,0.6454866364094806,"a1
a2
a3"
REFERENCES,0.6459909228441755,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6464952092788704,Mean Infection Rate
REFERENCES,0.6469994957135653,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.6475037821482602,Actions
REFERENCES,0.6480080685829551,"a1
a2
a3"
REFERENCES,0.64851235501765,"R0
R1
R2
R3
R4
Reward Type 0.000 0.025 0.050 0.075 0.100 0.125 0.150"
REFERENCES,0.649016641452345,Mean Infection Rate
REFERENCES,0.6495209278870399,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.6500252143217348,Actions
REFERENCES,0.6505295007564297,"a1
a2
a3"
REFERENCES,0.6510337871911246,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6515380736258195,Mean Infection Rate
REFERENCES,0.6520423600605144,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.6525466464952093,Actions
REFERENCES,0.6530509329299042,"a1
a2
a3"
REFERENCES,0.653555219364599,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.654059505799294,Mean Infection Rate
REFERENCES,0.6545637922339889,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.6550680786686838,Actions
REFERENCES,0.6555723651033787,"a1
a2
a3"
REFERENCES,0.6560766515380736,"R0
R1
R2
R3
R4
Reward Type 0.000 0.025 0.050 0.075 0.100 0.125 0.150"
REFERENCES,0.6565809379727685,Mean Infection Rate
REFERENCES,0.6570852244074634,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.6575895108421583,Actions
REFERENCES,0.6580937972768532,"a1
a2
a3"
REFERENCES,0.6585980837115482,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6591023701462431,Mean Infection Rate
REFERENCES,0.659606656580938,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.6601109430156329,Actions
REFERENCES,0.6606152294503278,"a1
a2
a3"
REFERENCES,0.6611195158850227,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6616238023197176,Mean Infection Rate
REFERENCES,0.6621280887544125,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.6626323751891074,Actions
REFERENCES,0.6631366616238024,"a1
a2
a3"
REFERENCES,0.6636409480584973,Case2 - Infection Rate for 50 Nodes on Dataset deg_2
REFERENCES,0.6641452344931922,"Figure 34: Case-2 inference on Dataset v2: Performance comparison of the RL policies trained using
ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of Connectivity 2, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.6646495209278871,"Degree of connectivity 3
50 Nodes - Figure 35"
REFERENCES,0.6651538073625819,"R0
R1
R2
R3
R4
Reward Type 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.6656580937972768,Mean Infection Rate
REFERENCES,0.6661623802319717,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.6666666666666666,Actions
REFERENCES,0.6671709531013615,"a1
a2
a3"
REFERENCES,0.6676752395360565,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6681795259707514,Mean Infection Rate
REFERENCES,0.6686838124054463,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.6691880988401412,Actions
REFERENCES,0.6696923852748361,"a1
a2
a3"
REFERENCES,0.670196671709531,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6707009581442259,Mean Infection Rate
REFERENCES,0.6712052445789208,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.6717095310136157,Actions
REFERENCES,0.6722138174483107,"a1
a2
a3"
REFERENCES,0.6727181038830056,"R0
R1
R2
R3
R4
Reward Type 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.6732223903177005,Mean Infection Rate
REFERENCES,0.6737266767523954,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.6742309631870903,Actions
REFERENCES,0.6747352496217852,"a1
a2
a3"
REFERENCES,0.6752395360564801,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.675743822491175,Mean Infection Rate
REFERENCES,0.6762481089258698,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.6767523953605648,Actions
REFERENCES,0.6772566817952597,"a1
a2
a3"
REFERENCES,0.6777609682299546,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6782652546646495,Mean Infection Rate
REFERENCES,0.6787695410993444,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.6792738275340393,Actions
REFERENCES,0.6797781139687342,"a1
a2
a3"
REFERENCES,0.6802824004034291,"R0
R1
R2
R3
R4
Reward Type 0.00 0.05 0.10 0.15 0.20 0.25 0.30"
REFERENCES,0.680786686838124,Mean Infection Rate
REFERENCES,0.681290973272819,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.6817952597075139,Actions
REFERENCES,0.6822995461422088,"a1
a2
a3"
REFERENCES,0.6828038325769037,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6833081190115986,Mean Infection Rate
REFERENCES,0.6838124054462935,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.6843166918809884,Actions
REFERENCES,0.6848209783156833,"a1
a2
a3"
REFERENCES,0.6853252647503782,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6858295511850732,Mean Infection Rate
REFERENCES,0.6863338376197681,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.686838124054463,Actions
REFERENCES,0.6873424104891579,"a1
a2
a3"
REFERENCES,0.6878466969238527,Case2 - Infection Rate for 50 Nodes on Dataset deg_3
REFERENCES,0.6883509833585476,"Figure 35: Case-2 inference on Dataset v2: Performance comparison of the RL policies trained using
ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of Connectivity 3, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.6888552697932425,"Degree of connectivity 4
50 Nodes - Figure 36"
REFERENCES,0.6893595562279374,"R0
R1
R2
R3
R4
Reward Type 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.6898638426626323,Mean Infection Rate
REFERENCES,0.6903681290973273,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.6908724155320222,Actions
REFERENCES,0.6913767019667171,"a1
a2
a3"
REFERENCES,0.691880988401412,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6923852748361069,Mean Infection Rate
REFERENCES,0.6928895612708018,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.6933938477054967,Actions
REFERENCES,0.6938981341401916,"a1
a2
a3"
REFERENCES,0.6944024205748865,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6949067070095815,Mean Infection Rate
REFERENCES,0.6954109934442764,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.6959152798789713,Actions
REFERENCES,0.6964195663136662,"a1
a2
a3"
REFERENCES,0.6969238527483611,"R0
R1
R2
R3
R4
Reward Type 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.697428139183056,Mean Infection Rate
REFERENCES,0.6979324256177509,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.6984367120524458,Actions
REFERENCES,0.6989409984871406,"a1
a2
a3"
REFERENCES,0.6994452849218356,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.6999495713565305,Mean Infection Rate
REFERENCES,0.7004538577912254,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.7009581442259203,Actions
REFERENCES,0.7014624306606152,"a1
a2
a3"
REFERENCES,0.7019667170953101,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.702471003530005,Mean Infection Rate
REFERENCES,0.7029752899646999,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.7034795763993948,Actions
REFERENCES,0.7039838628340898,"a1
a2
a3"
REFERENCES,0.7044881492687847,"R0
R1
R2
R3
R4
Reward Type 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.7049924357034796,Mean Infection Rate
REFERENCES,0.7054967221381745,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.7060010085728694,Actions
REFERENCES,0.7065052950075643,"a1
a2
a3"
REFERENCES,0.7070095814422592,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.7075138678769541,Mean Infection Rate
REFERENCES,0.708018154311649,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.708522440746344,Actions
REFERENCES,0.7090267271810389,"a1
a2
a3"
REFERENCES,0.7095310136157338,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.7100353000504287,Mean Infection Rate
REFERENCES,0.7105395864851235,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.7110438729198184,Actions
REFERENCES,0.7115481593545133,"a1
a2
a3"
REFERENCES,0.7120524457892082,Case2 - Infection Rate for 50 Nodes on Dataset deg_4
REFERENCES,0.7125567322239031,"Figure 36: Case-2 inference on Dataset v2: Performance comparison of the RL policies trained using
ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of Connectivity 4, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.7130610186585981,Case-3 v1
REFERENCES,0.713565305093293,"• Type: Floating Point Opinion and Floating Point Trust.
• Opinion Dynamic Model: Linear Adjustment."
REFERENCES,0.7140695915279879,"R0
The loss plot is presented in Figure 37. Dataset v1 Inference Result: 50 Nodes - Figure 38. 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.7145738779626828,"Inf.: 1, Act.: 1 10
4 10
3 10
2 10
1"
REFERENCES,0.7150781643973777,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.7155824508320726,"10
0
Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.7160867372667675,"10
25
50 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.7165910237014624,"Inf.: 2, Act.: 1 10
4 10
3 10
2 10
1"
REFERENCES,0.7170953101361573,"Inf.: 2, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.7175995965708523,"Inf.: 2, Act.: 3"
REFERENCES,0.7181038830055472,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.7186081694402421,"Episodes 10
4 10
3 10
2 Loss"
REFERENCES,0.719112455874937,"Inf.: 3, Act.: 1"
REFERENCES,0.7196167423096319,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.7201210287443268,"Episodes 10
4 10
3 10
2"
REFERENCES,0.7206253151790217,"Inf.: 3, Act.: 2"
REFERENCES,0.7211296016137166,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.7216338880484114,"Episodes 10
4 10
3 10
2"
REFERENCES,0.7221381744831064,"Inf.: 3, Act.: 3"
REFERENCES,0.7226424609178013,Loss Plots for case3-v1 - r0
REFERENCES,0.7231467473524962,"Figure 37: Case-3 v1 using R0: Training MSE loss evolution for RL policy using ResNet model
across networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation
sources (Inf.) and action budgets (Act.). The loss decreases over episodes, indicating improved policy
performance and adaptation across network sizes."
REFERENCES,0.7236510337871911,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.724155320221886,Mean Infection Rate
REFERENCES,0.7246596066565809,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.7251638930912758,Actions
REFERENCES,0.7256681795259707,"a1
a2
a3"
REFERENCES,0.7261724659606656,"d1
d2
d3
Dataset"
REFERENCES,0.7266767523953606,Mean Infection Rate
REFERENCES,0.7271810388300555,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.7276853252647504,Actions
REFERENCES,0.7281896116994453,"a1
a2
a3"
REFERENCES,0.7286938981341402,"d1
d2
d3
Dataset"
REFERENCES,0.7291981845688351,Mean Infection Rate
REFERENCES,0.72970247100353,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.7302067574382249,Actions
REFERENCES,0.7307110438729199,"a1
a2
a3"
REFERENCES,0.7312153303076148,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.7317196167423097,Mean Infection Rate
REFERENCES,0.7322239031770046,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.7327281896116995,Actions
REFERENCES,0.7332324760463943,"a1
a2
a3"
REFERENCES,0.7337367624810892,"d1
d2
d3
Dataset"
REFERENCES,0.7342410489157841,Mean Infection Rate
REFERENCES,0.734745335350479,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.735249621785174,Actions
REFERENCES,0.7357539082198689,"a1
a2
a3"
REFERENCES,0.7362581946545638,"d1
d2
d3
Dataset"
REFERENCES,0.7367624810892587,Mean Infection Rate
REFERENCES,0.7372667675239536,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.7377710539586485,Actions
REFERENCES,0.7382753403933434,"a1
a2
a3"
REFERENCES,0.7387796268280383,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.7392839132627332,Mean Infection Rate
REFERENCES,0.7397881996974282,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.7402924861321231,Actions
REFERENCES,0.740796772566818,"a1
a2
a3"
REFERENCES,0.7413010590015129,"d1
d2
d3
Dataset"
REFERENCES,0.7418053454362078,Mean Infection Rate
REFERENCES,0.7423096318709027,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.7428139183055976,Actions
REFERENCES,0.7433182047402925,"a1
a2
a3"
REFERENCES,0.7438224911749874,"d1
d2
d3
Dataset"
REFERENCES,0.7443267776096822,Mean Infection Rate
REFERENCES,0.7448310640443772,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.7453353504790721,Actions
REFERENCES,0.745839636913767,"a1
a2
a3"
REFERENCES,0.7463439233484619,"Figure 38: Case-3 v1 using R0: Performance comparison of the RL policies trained using ResNet
model for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3
of Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.7468482097831568,"R1
The loss plot is presented in Figure 39. Dataset v1 Inference Result: 50 Nodes - Figure 40. 10
3 10
2 10
1 Loss"
REFERENCES,0.7473524962178517,"Inf.: 1, Act.: 1 10
4 10
3 10
2 10
1"
REFERENCES,0.7478567826525466,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.7483610690872415,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.7488653555219364,"10
25
50 10
2 10
1 Loss"
REFERENCES,0.7493696419566314,"Inf.: 2, Act.: 1 10
3 10
2 10
1"
REFERENCES,0.7498739283913263,"Inf.: 2, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.7503782148260212,"Inf.: 2, Act.: 3"
REFERENCES,0.7508825012607161,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.751386787695411,"Episodes 10
2 10
1 Loss"
REFERENCES,0.7518910741301059,"Inf.: 3, Act.: 1"
REFERENCES,0.7523953605648008,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.7528996469994957,"Episodes 10
2 10
1"
REFERENCES,0.7534039334341907,"Inf.: 3, Act.: 2"
REFERENCES,0.7539082198688856,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.7544125063035805,"Episodes 10
2 10
1"
REFERENCES,0.7549167927382754,"Inf.: 3, Act.: 3"
REFERENCES,0.7554210791729703,Loss Plots for case3-v1 - r1
REFERENCES,0.7559253656076651,"Figure 39: Case-3 v1 using R1: Training MSE loss evolution for RL policy using ResNet model
across networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation
sources (Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over
episodes, indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.75642965204236,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.7569339384770549,Mean Infection Rate
REFERENCES,0.7574382249117498,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.7579425113464447,Actions
REFERENCES,0.7584467977811397,"a1
a2
a3"
REFERENCES,0.7589510842158346,"d1
d2
d3
Dataset"
REFERENCES,0.7594553706505295,Mean Infection Rate
REFERENCES,0.7599596570852244,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.7604639435199193,Actions
REFERENCES,0.7609682299546142,"a1
a2
a3"
REFERENCES,0.7614725163893091,"d1
d2
d3
Dataset"
REFERENCES,0.761976802824004,Mean Infection Rate
REFERENCES,0.762481089258699,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.7629853756933939,Actions
REFERENCES,0.7634896621280888,"a1
a2
a3"
REFERENCES,0.7639939485627837,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.7644982349974786,Mean Infection Rate
REFERENCES,0.7650025214321735,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.7655068078668684,Actions
REFERENCES,0.7660110943015633,"a1
a2
a3"
REFERENCES,0.7665153807362582,"d1
d2
d3
Dataset"
REFERENCES,0.7670196671709532,Mean Infection Rate
REFERENCES,0.767523953605648,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.7680282400403429,Actions
REFERENCES,0.7685325264750378,"a1
a2
a3"
REFERENCES,0.7690368129097327,"d1
d2
d3
Dataset"
REFERENCES,0.7695410993444276,Mean Infection Rate
REFERENCES,0.7700453857791225,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.7705496722138174,Actions
REFERENCES,0.7710539586485123,"a1
a2
a3"
REFERENCES,0.7715582450832073,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.7720625315179022,Mean Infection Rate
REFERENCES,0.7725668179525971,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.773071104387292,Actions
REFERENCES,0.7735753908219869,"a1
a2
a3"
REFERENCES,0.7740796772566818,"d1
d2
d3
Dataset"
REFERENCES,0.7745839636913767,Mean Infection Rate
REFERENCES,0.7750882501260716,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.7755925365607665,Actions
REFERENCES,0.7760968229954615,"a1
a2
a3"
REFERENCES,0.7766011094301564,"d1
d2
d3
Dataset"
REFERENCES,0.7771053958648513,Mean Infection Rate
REFERENCES,0.7776096822995462,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.7781139687342411,Actions
REFERENCES,0.7786182551689359,"a1
a2
a3"
REFERENCES,0.7791225416036308,"Figure 40: Case-3 v1 using R1: Performance comparison of the RL policies trained using ResNet
model for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3
of Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.7796268280383257,"R2
The loss plot is presented in Figure 41. Dataset v1 Inference Result: 50 Nodes - Figure 42. 10
3 10
2 Loss"
REFERENCES,0.7801311144730206,"Inf.: 1, Act.: 1 10
4 10
3 10
2"
REFERENCES,0.7806354009077155,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.7811396873424105,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.7816439737771054,"10
25
50 10
3 10
2 10
1 Loss"
REFERENCES,0.7821482602118003,"Inf.: 2, Act.: 1 10
3 10
2 10
1"
REFERENCES,0.7826525466464952,"Inf.: 2, Act.: 2 10
3 10
2 10
1"
REFERENCES,0.7831568330811901,"Inf.: 2, Act.: 3"
REFERENCES,0.783661119515885,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.7841654059505799,"Episodes 10
2 10
1 Loss"
REFERENCES,0.7846696923852748,"Inf.: 3, Act.: 1"
REFERENCES,0.7851739788199698,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.7856782652546647,"Episodes 10
2 10
1"
REFERENCES,0.7861825516893596,"Inf.: 3, Act.: 2"
REFERENCES,0.7866868381240545,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.7871911245587494,"Episodes 10
2 10
1"
REFERENCES,0.7876954109934443,"Inf.: 3, Act.: 3"
REFERENCES,0.7881996974281392,Loss Plots for case3-v1 - r2
REFERENCES,0.7887039838628341,"Figure 41: Case-3 v1 using R2: Training MSE loss evolution for RL policy using ResNet model
across networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation
sources (Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over
episodes, indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.789208270297529,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.789712556732224,Mean Infection Rate
REFERENCES,0.7902168431669188,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.7907211296016137,Actions
REFERENCES,0.7912254160363086,"a1
a2
a3"
REFERENCES,0.7917297024710035,"d1
d2
d3
Dataset"
REFERENCES,0.7922339889056984,Mean Infection Rate
REFERENCES,0.7927382753403933,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.7932425617750882,Actions
REFERENCES,0.7937468482097831,"a1
a2
a3"
REFERENCES,0.794251134644478,"d1
d2
d3
Dataset"
REFERENCES,0.794755421079173,Mean Infection Rate
REFERENCES,0.7952597075138679,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.7957639939485628,Actions
REFERENCES,0.7962682803832577,"a1
a2
a3"
REFERENCES,0.7967725668179526,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.7972768532526475,Mean Infection Rate
REFERENCES,0.7977811396873424,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.7982854261220373,Actions
REFERENCES,0.7987897125567323,"a1
a2
a3"
REFERENCES,0.7992939989914272,"d1
d2
d3
Dataset"
REFERENCES,0.7997982854261221,Mean Infection Rate
REFERENCES,0.800302571860817,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.8008068582955119,Actions
REFERENCES,0.8013111447302067,"a1
a2
a3"
REFERENCES,0.8018154311649016,"d1
d2
d3
Dataset"
REFERENCES,0.8023197175995965,Mean Infection Rate
REFERENCES,0.8028240040342914,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.8033282904689864,Actions
REFERENCES,0.8038325769036813,"a1
a2
a3"
REFERENCES,0.8043368633383762,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.8048411497730711,Mean Infection Rate
REFERENCES,0.805345436207766,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.8058497226424609,Actions
REFERENCES,0.8063540090771558,"a1
a2
a3"
REFERENCES,0.8068582955118507,"d1
d2
d3
Dataset"
REFERENCES,0.8073625819465456,Mean Infection Rate
REFERENCES,0.8078668683812406,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.8083711548159355,Actions
REFERENCES,0.8088754412506304,"a1
a2
a3"
REFERENCES,0.8093797276853253,"d1
d2
d3
Dataset"
REFERENCES,0.8098840141200202,Mean Infection Rate
REFERENCES,0.8103883005547151,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.81089258698941,Actions
REFERENCES,0.8113968734241049,"a1
a2
a3"
REFERENCES,0.8119011598587998,"Figure 42: Case-3 v1 using R2: Performance comparison of the RL policies trained using ResNet
model for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3
of Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.8124054462934948,"R3
The loss plot is presented in Figure 43. Dataset v1 Inference Result: 50 Nodes - Figure 44. 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.8129097327281896,"Inf.: 1, Act.: 1 10
4 10
3 10
2 10
1"
REFERENCES,0.8134140191628845,"10
0
Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.8139183055975794,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.8144225920322743,"10
25
50 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.8149268784669692,"Inf.: 2, Act.: 1 10
4 10
3 10
2 10
1"
REFERENCES,0.8154311649016641,"Inf.: 2, Act.: 2 10
4 10
3 10
2 10
1 10
0"
REFERENCES,0.815935451336359,"Inf.: 2, Act.: 3"
REFERENCES,0.8164397377710539,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.8169440242057489,"Episodes 10
4 10
3 10
2 10
1 Loss"
REFERENCES,0.8174483106404438,"Inf.: 3, Act.: 1"
REFERENCES,0.8179525970751387,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.8184568835098336,"Episodes 10
4 10
3 10
2 10
1"
REFERENCES,0.8189611699445285,"Inf.: 3, Act.: 2"
REFERENCES,0.8194654563792234,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.8199697428139183,"Episodes 10
4 10
3 10
2 10
1"
REFERENCES,0.8204740292486132,"10
0
Inf.: 3, Act.: 3"
REFERENCES,0.8209783156833081,Loss Plots for case3-v1 - r3
REFERENCES,0.8214826021180031,"Figure 43: Case-3 v1 using R3: Training MSE loss evolution for RL policy using ResNet model
across networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation
sources (Inf.) and action budgets (Act.). The loss decreases over episodes, indicating improved policy
performance and adaptation across network sizes."
REFERENCES,0.821986888552698,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.8224911749873929,Mean Infection Rate
REFERENCES,0.8229954614220878,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.8234997478567827,Actions
REFERENCES,0.8240040342914775,"a1
a2
a3"
REFERENCES,0.8245083207261724,"d1
d2
d3
Dataset"
REFERENCES,0.8250126071608673,Mean Infection Rate
REFERENCES,0.8255168935955622,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.8260211800302572,Actions
REFERENCES,0.8265254664649521,"a1
a2
a3"
REFERENCES,0.827029752899647,"d1
d2
d3
Dataset"
REFERENCES,0.8275340393343419,Mean Infection Rate
REFERENCES,0.8280383257690368,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.8285426122037317,Actions
REFERENCES,0.8290468986384266,"a1
a2
a3"
REFERENCES,0.8295511850731215,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.8300554715078164,Mean Infection Rate
REFERENCES,0.8305597579425114,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.8310640443772063,Actions
REFERENCES,0.8315683308119012,"a1
a2
a3"
REFERENCES,0.8320726172465961,"d1
d2
d3
Dataset"
REFERENCES,0.832576903681291,Mean Infection Rate
REFERENCES,0.8330811901159859,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.8335854765506808,Actions
REFERENCES,0.8340897629853757,"a1
a2
a3"
REFERENCES,0.8345940494200706,"d1
d2
d3
Dataset"
REFERENCES,0.8350983358547656,Mean Infection Rate
REFERENCES,0.8356026222894604,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.8361069087241553,Actions
REFERENCES,0.8366111951588502,"a1
a2
a3"
REFERENCES,0.8371154815935451,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.83761976802824,Mean Infection Rate
REFERENCES,0.8381240544629349,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.8386283408976298,Actions
REFERENCES,0.8391326273323247,"a1
a2
a3"
REFERENCES,0.8396369137670197,"d1
d2
d3
Dataset"
REFERENCES,0.8401412002017146,Mean Infection Rate
REFERENCES,0.8406454866364095,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.8411497730711044,Actions
REFERENCES,0.8416540595057993,"a1
a2
a3"
REFERENCES,0.8421583459404942,"d1
d2
d3
Dataset"
REFERENCES,0.8426626323751891,Mean Infection Rate
REFERENCES,0.843166918809884,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.843671205244579,Actions
REFERENCES,0.8441754916792739,"a1
a2
a3"
REFERENCES,0.8446797781139688,"Figure 44: Case-3 v1 using R3: Performance comparison of the RL policies trained using ResNet
model for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3
of Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.8451840645486637,"R4
The loss plot is presented in Figure 45. Dataset v1 Inference Result: 50 Nodes - Figure 46. 10
3 10
2 10
1 Loss"
REFERENCES,0.8456883509833586,"Inf.: 1, Act.: 1 10
4 10
3 10
2"
REFERENCES,0.8461926374180535,"Inf.: 1, Act.: 2 10
4 10
3 10
2 10
1"
REFERENCES,0.8466969238527483,"Inf.: 1, Act.: 3 Nodes"
REFERENCES,0.8472012102874432,"10
25
50 10
2 10
1 Loss"
REFERENCES,0.8477054967221381,"Inf.: 2, Act.: 1 10
2 10
1"
REFERENCES,0.848209783156833,"Inf.: 2, Act.: 2 10
3 10
2 10
1"
REFERENCES,0.848714069591528,"10
0
Inf.: 2, Act.: 3"
REFERENCES,0.8492183560262229,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.8497226424609178,"Episodes 10
2 10
1 Loss"
REFERENCES,0.8502269288956127,"Inf.: 3, Act.: 1"
REFERENCES,0.8507312153303076,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.8512355017650025,"Episodes 10
3 10
2 10
1"
REFERENCES,0.8517397881996974,"Inf.: 3, Act.: 2"
REFERENCES,0.8522440746343923,"1
31
61
91
121 151 181 211 241 271"
REFERENCES,0.8527483610690872,"Episodes 10
3 10
2"
REFERENCES,0.8532526475037822,"Inf.: 3, Act.: 3"
REFERENCES,0.8537569339384771,Loss Plots for case3-v1 - r4
REFERENCES,0.854261220373172,"Figure 45: Case-3 v1 using R4: Training MSE loss evolution for RL policy using ResNet model
across networks of 10 (blue), 25 (orange), and 50 (green) nodes, for varying initial misinformation
sources (Inf.) and action budgets (Act.). Plotted on a logarithmic scale, the loss decreases over
episodes, indicating improved policy performance and adaptation across network sizes."
REFERENCES,0.8547655068078669,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.8552697932425618,Mean Infection Rate
REFERENCES,0.8557740796772567,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.8562783661119516,Actions
REFERENCES,0.8567826525466465,"a1
a2
a3"
REFERENCES,0.8572869389813415,"d1
d2
d3
Dataset"
REFERENCES,0.8577912254160364,Mean Infection Rate
REFERENCES,0.8582955118507312,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.8587997982854261,Actions
REFERENCES,0.859304084720121,"a1
a2
a3"
REFERENCES,0.8598083711548159,"d1
d2
d3
Dataset"
REFERENCES,0.8603126575895108,Mean Infection Rate
REFERENCES,0.8608169440242057,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.8613212304589006,Actions
REFERENCES,0.8618255168935955,"a1
a2
a3"
REFERENCES,0.8623298033282905,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.8628340897629854,Mean Infection Rate
REFERENCES,0.8633383761976803,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.8638426626323752,Actions
REFERENCES,0.8643469490670701,"a1
a2
a3"
REFERENCES,0.864851235501765,"d1
d2
d3
Dataset"
REFERENCES,0.8653555219364599,Mean Infection Rate
REFERENCES,0.8658598083711548,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.8663640948058497,Actions
REFERENCES,0.8668683812405447,"a1
a2
a3"
REFERENCES,0.8673726676752396,"d1
d2
d3
Dataset"
REFERENCES,0.8678769541099345,Mean Infection Rate
REFERENCES,0.8683812405446294,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.8688855269793243,Actions
REFERENCES,0.8693898134140191,"a1
a2
a3"
REFERENCES,0.869894099848714,"d1
d2
d3
Dataset 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.8703983862834089,Mean Infection Rate
REFERENCES,0.8709026727181038,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.8714069591527988,Actions
REFERENCES,0.8719112455874937,"a1
a2
a3"
REFERENCES,0.8724155320221886,"d1
d2
d3
Dataset"
REFERENCES,0.8729198184568835,Mean Infection Rate
REFERENCES,0.8734241048915784,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.8739283913262733,Actions
REFERENCES,0.8744326777609682,"a1
a2
a3"
REFERENCES,0.8749369641956631,"d1
d2
d3
Dataset"
REFERENCES,0.875441250630358,Mean Infection Rate
REFERENCES,0.875945537065053,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.8764498234997479,Actions
REFERENCES,0.8769541099344428,"a1
a2
a3"
REFERENCES,0.8774583963691377,"Figure 46: Case-3 v1 using R4: Performance comparison of the RL policies trained using ResNet
model for a 50-node network. The barplot displays the mean infection rate for datasets d1, d2, and d3
of Dataset v1 type, differentiated by the number of initial misinformation sources (Inf.) and action
budgets (Act.: a1, a2, a3). Each subplot illustrates the performance of a policy trained with the
parameters indicated in its title. Lower infection rates indicate more effective policy learning and
misinformation containment."
REFERENCES,0.8779626828038326,Dataset v2 Results
REFERENCES,0.8784669692385275,"Degree of connectivity 1
50 Nodes - Figure 47"
REFERENCES,0.8789712556732224,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.8794755421079173,Mean Infection Rate
REFERENCES,0.8799798285426123,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.8804841149773072,Actions
REFERENCES,0.880988401412002,"a1
a2
a3"
REFERENCES,0.8814926878466969,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.8819969742813918,Mean Infection Rate
REFERENCES,0.8825012607160867,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.8830055471507816,Actions
REFERENCES,0.8835098335854765,"a1
a2
a3"
REFERENCES,0.8840141200201714,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.8845184064548663,Mean Infection Rate
REFERENCES,0.8850226928895613,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.8855269793242562,Actions
REFERENCES,0.8860312657589511,"a1
a2
a3"
REFERENCES,0.886535552193646,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.8870398386283409,Mean Infection Rate
REFERENCES,0.8875441250630358,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.8880484114977307,Actions
REFERENCES,0.8885526979324256,"a1
a2
a3"
REFERENCES,0.8890569843671206,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.8895612708018155,Mean Infection Rate
REFERENCES,0.8900655572365104,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.8905698436712053,Actions
REFERENCES,0.8910741301059002,"a1
a2
a3"
REFERENCES,0.8915784165405951,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.8920827029752899,Mean Infection Rate
REFERENCES,0.8925869894099848,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.8930912758446797,Actions
REFERENCES,0.8935955622793746,"a1
a2
a3"
REFERENCES,0.8940998487140696,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.8946041351487645,Mean Infection Rate
REFERENCES,0.8951084215834594,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.8956127080181543,Actions
REFERENCES,0.8961169944528492,"a1
a2
a3"
REFERENCES,0.8966212808875441,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.897125567322239,Mean Infection Rate
REFERENCES,0.8976298537569339,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.8981341401916288,Actions
REFERENCES,0.8986384266263238,"a1
a2
a3"
REFERENCES,0.8991427130610187,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.8996469994957136,Mean Infection Rate
REFERENCES,0.9001512859304085,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.9006555723651034,Actions
REFERENCES,0.9011598587997983,"a1
a2
a3"
REFERENCES,0.9016641452344932,Case3-v1 - Infection Rate for 50 Nodes on Dataset deg_1
REFERENCES,0.9021684316691881,"Figure 47: Case-3-v1 inference on Dataset v2: Performance comparison of the RL policies trained
using ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of Connectivity 1, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.902672718103883,"Degree of connectivity 2
50 Nodes - Figure 48"
REFERENCES,0.903177004538578,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.9036812909732728,Mean Infection Rate
REFERENCES,0.9041855774079677,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.9046898638426626,Actions
REFERENCES,0.9051941502773575,"a1
a2
a3"
REFERENCES,0.9056984367120524,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9062027231467473,Mean Infection Rate
REFERENCES,0.9067070095814422,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.9072112960161371,Actions
REFERENCES,0.9077155824508321,"a1
a2
a3"
REFERENCES,0.908219868885527,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9087241553202219,Mean Infection Rate
REFERENCES,0.9092284417549168,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.9097327281896117,Actions
REFERENCES,0.9102370146243066,"a1
a2
a3"
REFERENCES,0.9107413010590015,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.9112455874936964,Mean Infection Rate
REFERENCES,0.9117498739283914,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.9122541603630863,Actions
REFERENCES,0.9127584467977812,"a1
a2
a3"
REFERENCES,0.9132627332324761,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.913767019667171,Mean Infection Rate
REFERENCES,0.9142713061018659,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.9147755925365607,Actions
REFERENCES,0.9152798789712556,"a1
a2
a3"
REFERENCES,0.9157841654059505,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9162884518406454,Mean Infection Rate
REFERENCES,0.9167927382753404,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.9172970247100353,Actions
REFERENCES,0.9178013111447302,"a1
a2
a3"
REFERENCES,0.9183055975794251,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.91880988401412,Mean Infection Rate
REFERENCES,0.9193141704488149,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.9198184568835098,Actions
REFERENCES,0.9203227433182047,"a1
a2
a3"
REFERENCES,0.9208270297528997,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9213313161875946,Mean Infection Rate
REFERENCES,0.9218356026222895,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.9223398890569844,Actions
REFERENCES,0.9228441754916793,"a1
a2
a3"
REFERENCES,0.9233484619263742,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9238527483610691,Mean Infection Rate
REFERENCES,0.924357034795764,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.9248613212304589,Actions
REFERENCES,0.9253656076651539,"a1
a2
a3"
REFERENCES,0.9258698940998488,Case3-v1 - Infection Rate for 50 Nodes on Dataset deg_2
REFERENCES,0.9263741805345436,"Figure 48: Case-3-v1 inference on Dataset v2: Performance comparison of the RL policies trained
using ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of Connectivity 2, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.9268784669692385,"Degree of connectivity 3
50 Nodes - Figure 49"
REFERENCES,0.9273827534039334,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05 0.06"
REFERENCES,0.9278870398386283,Mean Infection Rate
REFERENCES,0.9283913262733232,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.9288956127080181,Actions
REFERENCES,0.929399899142713,"a1
a2
a3"
REFERENCES,0.929904185577408,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9304084720121029,Mean Infection Rate
REFERENCES,0.9309127584467978,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.9314170448814927,Actions
REFERENCES,0.9319213313161876,"a1
a2
a3"
REFERENCES,0.9324256177508825,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9329299041855774,Mean Infection Rate
REFERENCES,0.9334341906202723,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.9339384770549672,Actions
REFERENCES,0.9344427634896622,"a1
a2
a3"
REFERENCES,0.9349470499243571,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05 0.06"
REFERENCES,0.935451336359052,Mean Infection Rate
REFERENCES,0.9359556227937469,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.9364599092284418,Actions
REFERENCES,0.9369641956631367,"a1
a2
a3"
REFERENCES,0.9374684820978316,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9379727685325264,Mean Infection Rate
REFERENCES,0.9384770549672213,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.9389813414019162,Actions
REFERENCES,0.9394856278366112,"a1
a2
a3"
REFERENCES,0.9399899142713061,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.940494200706001,Mean Infection Rate
REFERENCES,0.9409984871406959,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.9415027735753908,Actions
REFERENCES,0.9420070600100857,"a1
a2
a3"
REFERENCES,0.9425113464447806,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05 0.06"
REFERENCES,0.9430156328794755,Mean Infection Rate
REFERENCES,0.9435199193141705,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.9440242057488654,Actions
REFERENCES,0.9445284921835603,"a1
a2
a3"
REFERENCES,0.9450327786182552,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9455370650529501,Mean Infection Rate
REFERENCES,0.946041351487645,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.9465456379223399,Actions
REFERENCES,0.9470499243570348,"a1
a2
a3"
REFERENCES,0.9475542107917297,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9480584972264247,Mean Infection Rate
REFERENCES,0.9485627836611196,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.9490670700958144,Actions
REFERENCES,0.9495713565305093,"a1
a2
a3"
REFERENCES,0.9500756429652042,Case3-v1 - Infection Rate for 50 Nodes on Dataset deg_3
REFERENCES,0.9505799293998991,"Figure 49: Case-3-v1 inference on Dataset v2: Performance comparison of the RL policies trained
using ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of Connectivity 3, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.951084215834594,"Degree of connectivity 4
50 Nodes - Figure 50"
REFERENCES,0.9515885022692889,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05 0.06"
REFERENCES,0.9520927887039838,Mean Infection Rate
REFERENCES,0.9525970751386788,"Nodes: 50, Inf.: 1, Act.: 1"
REFERENCES,0.9531013615733737,Actions
REFERENCES,0.9536056480080686,"a1
a2
a3"
REFERENCES,0.9541099344427635,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9546142208774584,Mean Infection Rate
REFERENCES,0.9551185073121533,"Nodes: 50, Inf.: 1, Act.: 2"
REFERENCES,0.9556227937468482,Actions
REFERENCES,0.9561270801815431,"a1
a2
a3"
REFERENCES,0.956631366616238,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.957135653050933,Mean Infection Rate
REFERENCES,0.9576399394856279,"Nodes: 50, Inf.: 1, Act.: 3"
REFERENCES,0.9581442259203228,Actions
REFERENCES,0.9586485123550177,"a1
a2
a3"
REFERENCES,0.9591527987897126,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05 0.06"
REFERENCES,0.9596570852244075,Mean Infection Rate
REFERENCES,0.9601613716591024,"Nodes: 50, Inf.: 2, Act.: 1"
REFERENCES,0.9606656580937972,Actions
REFERENCES,0.9611699445284921,"a1
a2
a3"
REFERENCES,0.961674230963187,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.962178517397882,Mean Infection Rate
REFERENCES,0.9626828038325769,"Nodes: 50, Inf.: 2, Act.: 2"
REFERENCES,0.9631870902672718,Actions
REFERENCES,0.9636913767019667,"a1
a2
a3"
REFERENCES,0.9641956631366616,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9646999495713565,Mean Infection Rate
REFERENCES,0.9652042360060514,"Nodes: 50, Inf.: 2, Act.: 3"
REFERENCES,0.9657085224407463,Actions
REFERENCES,0.9662128088754413,"a1
a2
a3"
REFERENCES,0.9667170953101362,"R0
R1
R2
R3
R4
Reward Type 0.00 0.01 0.02 0.03 0.04 0.05 0.06"
REFERENCES,0.9672213817448311,Mean Infection Rate
REFERENCES,0.967725668179526,"Nodes: 50, Inf.: 3, Act.: 1"
REFERENCES,0.9682299546142209,Actions
REFERENCES,0.9687342410489158,"a1
a2
a3"
REFERENCES,0.9692385274836107,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9697428139183056,Mean Infection Rate
REFERENCES,0.9702471003530005,"Nodes: 50, Inf.: 3, Act.: 2"
REFERENCES,0.9707513867876955,Actions
REFERENCES,0.9712556732223904,"a1
a2
a3"
REFERENCES,0.9717599596570852,"R0
R1
R2
R3
R4
Reward Type"
REFERENCES,0.9722642460917801,Mean Infection Rate
REFERENCES,0.972768532526475,"Nodes: 50, Inf.: 3, Act.: 3"
REFERENCES,0.9732728189611699,Actions
REFERENCES,0.9737771053958648,"a1
a2
a3"
REFERENCES,0.9742813918305597,Case3-v1 - Infection Rate for 50 Nodes on Dataset deg_4
REFERENCES,0.9747856782652546,"Figure 50: Case-3-v1 inference on Dataset v2: Performance comparison of the RL policies trained
using ResNet model for a 50-node network. The barplot displays the mean infection rate for different
reward functions on Dataset v2 of Degree of Connectivity 4, differentiated by the number of initial
misinformation sources (Inf.) and action budgets (Act.: a1, a2, a3). Each subplot illustrates the
performance of a policy trained with the parameters indicated in its title. Lower infection rates
indicate more effective policy learning and misinformation containment."
REFERENCES,0.9752899646999496,NeurIPS Paper Checklist
CLAIMS,0.9757942511346445,1. Claims
CLAIMS,0.9762985375693394,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The claims made in the abstract are properly explained and proved in the main
paper.
Guidelines:"
CLAIMS,0.9768028240040343,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9773071104387292,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: A discussion on limitations is provided in the Conclusion (Section 6)
Guidelines:"
CLAIMS,0.9778113968734241,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.978315683308119,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]"
CLAIMS,0.9788199697428139,Justification: [NA]
CLAIMS,0.9793242561775088,Guidelines:
CLAIMS,0.9798285426122038,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9803328290468987,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9808371154815936,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9813414019162885,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9818456883509834,"Justification: The different experiment setups along with the details of generating train-
ing and testing data are clearly provided in the main paper with additional details in the
Appendix."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9823499747856783,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9828542612203732,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.983358547655068,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9838628340897629,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We uploaded a zip file for our code and datasets used in our study as supple-
mentary material.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9843671205244579,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
OPEN ACCESS TO DATA AND CODE,0.9848714069591528,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide these expressive details in the Appendix.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9853756933938477,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
OPEN ACCESS TO DATA AND CODE,0.9858799798285426,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report the mean-variance error bars for our results and these are presented
in the Appendix.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9863842662632375,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
OPEN ACCESS TO DATA AND CODE,0.9868885526979324,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
OPEN ACCESS TO DATA AND CODE,0.9873928391326273,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Hardware details are provided in the Appendix
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9878971255673222,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
OPEN ACCESS TO DATA AND CODE,0.9884014120020171,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We followed the ethical guidelines properly.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.988905698436712,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
OPEN ACCESS TO DATA AND CODE,0.989409984871407,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: In conclusion (Section 6)
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9899142713061019,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
OPEN ACCESS TO DATA AND CODE,0.9904185577407968,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9909228441754917,11. Safeguards
SAFEGUARDS,0.9914271306101866,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9919314170448815,Answer: [NA]
SAFEGUARDS,0.9924357034795764,Justification: [NA]
SAFEGUARDS,0.9929399899142713,Guidelines:
SAFEGUARDS,0.9934442763489663,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9939485627836612,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.994452849218356,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9949571356530509,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9954614220877458,Justification: Yes MIT license.
LICENSES FOR EXISTING ASSETS,0.9959657085224407,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9964699949571356,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9969742813918305,"• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9974785678265254,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide detailed documentation of our code and datasets as a zip file.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9979828542612204,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9984871406959153,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9989914271306102,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9994957135653051,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
