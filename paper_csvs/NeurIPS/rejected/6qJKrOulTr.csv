Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013192612137203166,"This paper introduces a general method for the exploration of equivalence classes in
1"
ABSTRACT,0.002638522427440633,"the input space of Transformer models. The proposed approach is based on sound
2"
ABSTRACT,0.00395778364116095,"mathematical theory which describes the internal layers of a Transformer architec-
3"
ABSTRACT,0.005277044854881266,"ture as sequential deformations of the input manifold. Using eigendecomposition
4"
ABSTRACT,0.006596306068601583,"of the pullback of the distance metric defined on the output space through the
5"
ABSTRACT,0.0079155672823219,"Jacobian of the model, we are able to reconstruct equivalence classes in the input
6"
ABSTRACT,0.009234828496042216,"space and navigate across them. We illustrate how this method can be used as a
7"
ABSTRACT,0.010554089709762533,"powerful tool for investigating how a Transformer sees the input space, facilitating
8"
ABSTRACT,0.011873350923482849,"local and task-agnostic explainability in Computer Vision and Natural Language
9"
ABSTRACT,0.013192612137203167,"Processing tasks.
10"
INTRODUCTION,0.014511873350923483,"1
Introduction
11"
INTRODUCTION,0.0158311345646438,"In this paper, we propose a method for exploring the input space of Transformer models by identifying
12"
INTRODUCTION,0.017150395778364115,"equivalence classes with respect to their predictions. We define an equivalence class of a Transformer
13"
INTRODUCTION,0.018469656992084433,"model as the set of vectors in the embedding space whose outcomes under the Transformer process
14"
INTRODUCTION,0.01978891820580475,"are the same. The study of the input manifold on which the inverse image of models lies provides
15"
INTRODUCTION,0.021108179419525065,"insights for both explainability and sensitivity analyses. Existing methods aiming at the exploration
16"
INTRODUCTION,0.022427440633245383,"of the input space of Deep Neural Networks and Transformers either rely on perturbations of input
17"
INTRODUCTION,0.023746701846965697,"data using heuristic or gradient-based criteria [16, 22, 17, 14], or they analyze specific properties of
18"
INTRODUCTION,0.025065963060686015,"the embedding space [5].
19"
INTRODUCTION,0.026385224274406333,"Our approach is based on sound mathematical theory which describes the internal layers of a
20"
INTRODUCTION,0.027704485488126648,"Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition
21"
INTRODUCTION,0.029023746701846966,"of the pullback of the distance metric defined on the output space through the Jacobian of the model,
22"
INTRODUCTION,0.030343007915567283,"we are able to reconstruct equivalence classes in the input space and navigate across them. In the
23"
INTRODUCTION,0.0316622691292876,"XAI scenario, our framework can facilitate local and task-agnostic explainability methods applicable
24"
INTRODUCTION,0.032981530343007916,"to Computer Vision (CV) and Natural Language Processing (NLP) tasks, among others.
25"
INTRODUCTION,0.03430079155672823,"In Section 2, we summarise the preliminaries of the mathematical foundations of our approach.
26"
INTRODUCTION,0.03562005277044855,"In Section 3, we present our method for the exploration of equivalence classes in the input of the
27"
INTRODUCTION,0.036939313984168866,"Transformer models. In Section 4, we perform a preliminary investigation of some applicability
28"
INTRODUCTION,0.03825857519788918,"options of our method on textual and visual data. In Section 5, we discuss the relevant literature about
29"
INTRODUCTION,0.0395778364116095,"embedding space exploration and feature importance. Finally, in Section 6, we give our concluding
30"
INTRODUCTION,0.040897097625329816,"remarks1.
31"
INTRODUCTION,0.04221635883905013,1The code to reproduce our experiments can be found in the Supplementary Materials.
PRELIMINARIES,0.04353562005277045,"2
Preliminaries
32"
PRELIMINARIES,0.044854881266490766,"In this Section, we provide the theoretical foundation of the proposed approach, namely the Geometric
33"
PRELIMINARIES,0.04617414248021108,"Deep Learning framework based on Riemannian Geometry [2].
34"
PRELIMINARIES,0.047493403693931395,"A neural network is considered as a sequence of maps, the layers of the network, between manifolds,
35"
PRELIMINARIES,0.048812664907651716,"and the latter are the spaces where the input and the outputs of the layers belong to.
36"
PRELIMINARIES,0.05013192612137203,"Definition 1 (Neural Network). A neural network is a sequence of C1 maps Λi between manifolds of
37"
PRELIMINARIES,0.051451187335092345,"the form:
38"
PRELIMINARIES,0.052770448548812667,"M0
M1
M2
· · ·
Mn−1
Mn
Λ1
Λ2
Λ4
Λn−1
Λn
(1)"
PRELIMINARIES,0.05408970976253298,"We call M0 the input manifold and Mn the output manifold. All the other manifolds of the sequence
39"
PRELIMINARIES,0.055408970976253295,"are called representation manifolds. The maps Λi are the layers of the neural network. We denote
40"
PRELIMINARIES,0.05672823218997362,"with N(i) = Λn ◦· · · ◦Λi : Mi →Mn the mapping from the i-th representation layer to the output
41"
PRELIMINARIES,0.05804749340369393,"layer.
42"
PRELIMINARIES,0.059366754617414245,"As an example, consider a shallow network with just one layer, the composition of a linear operator
43"
PRELIMINARIES,0.06068601583113457,"A · +b with a sigmoid function σ, where A ∈Rm×n and b ∈Rm: then, the input manifold M0 and
44"
PRELIMINARIES,0.06200527704485488,"the output manifold M1 shall be Rn and Rm, respectively, and the map Λ1(·) = σ(A · +b). We
45"
PRELIMINARIES,0.0633245382585752,"generalize this observation into the following definition.
46"
PRELIMINARIES,0.06464379947229551,"Definition 2 (Smooth layer). A map Λi : Mi−1 →Mi is called a smooth layer if it is the restriction
47"
PRELIMINARIES,0.06596306068601583,"to Mi−1 of a function Λ
(i)(x) : Rdi−1 →Rdi of the form
48"
PRELIMINARIES,0.06728232189973615,"Λ
(i)
α (x) = F (i)
α  X"
PRELIMINARIES,0.06860158311345646,"β
A(i)
αβxβ + b(i)
α  
(2)"
PRELIMINARIES,0.06992084432717678,"for i = 1, · · · , n, x ∈Rdi, b(i) ∈Rdi and A(i) ∈Rdi×di−1, with F (i) : Rdi →Rdi a diffeomor-
49"
PRELIMINARIES,0.0712401055408971,"phism.
50"
PRELIMINARIES,0.07255936675461741,"Remark 1. Transformers implicitly apply for this framework, since their modules are smooth
51"
PRELIMINARIES,0.07387862796833773,"functions, such as fully connected layers, GeLU and sigmoid activations.
52"
PRELIMINARIES,0.07519788918205805,"Our aim is to transport the geometric information on the data lying in the output manifold to the
53"
PRELIMINARIES,0.07651715039577836,"input manifold: this allows us to obtain insight on how the network ""sees"" the input space, how it
54"
PRELIMINARIES,0.07783641160949868,"manipulates it for reaching its final conclusion. For fulfilling this objective, we need several tools
55"
PRELIMINARIES,0.079155672823219,"from differential geometry. The first key ingredient is the notion of singular Riemannian metric,
56"
PRELIMINARIES,0.08047493403693931,"which has the intuitive meaning of a degenerate scalar product which changes point to point.
57"
PRELIMINARIES,0.08179419525065963,"Definition 3 (Singular Riemannian metric). Let M = Rn or an open subset of Rn. A singular
58"
PRELIMINARIES,0.08311345646437995,"Riemannian metric g over M is a map g : M →Bil(Rn × Rn) that associates to each point p a
59"
PRELIMINARIES,0.08443271767810026,"positive semidefinite symmetric bilinear form gp : Rn × Rn →R in a smooth way.
60"
PRELIMINARIES,0.08575197889182058,"Without loss of generality, we can assume the following hypotheses on the sequence (1): i) The
61"
PRELIMINARIES,0.0870712401055409,"manifolds Mi are open and path-connected sets of dimension dim Mi = di. ii) The maps Λi are C1
62"
PRELIMINARIES,0.08839050131926121,"submersions. iii) Λi(Mi−1) = Mi for every i = 1, · · · , n. iv) The manifold Mn is equipped with
63"
PRELIMINARIES,0.08970976253298153,"the structure of Riemannian manifold, with metric g(n). Definition 3 naturally leads to the definition
64"
PRELIMINARIES,0.09102902374670185,"of the pseudolength and of energy of a curve.
65"
PRELIMINARIES,0.09234828496042216,"Definition 4 (Pseudolength and energy of a curve). Let γ : [a, b] →Rn a curve defined on the
66"
PRELIMINARIES,0.09366754617414248,"interval [a, b] ⊂R and ∥v∥p =
p"
PRELIMINARIES,0.09498680738786279,"gp(v, v) the pseudo–norm induced by the pseudo–metric gp at
67"
PRELIMINARIES,0.09630606860158311,"point p. Then the pseudolength of γ and its energy are defined as
68"
PRELIMINARIES,0.09762532981530343,"Pl(γ) =
Z b"
PRELIMINARIES,0.09894459102902374,"a
∥˙γ(s)∥γ(s)ds =
Z b a q"
PRELIMINARIES,0.10026385224274406,"gγ(s)(˙γ(s), ˙γ(s))ds,
E(γ) =
Z b"
PRELIMINARIES,0.10158311345646438,"a
∥˙γ(s)∥2
γ(s)ds
(3)"
PRELIMINARIES,0.10290237467018469,"The notion of pseudolength leads naturally to define the distance between two points.
69"
PRELIMINARIES,0.10422163588390501,"Definition 5 (Pseudodistance). Let x, y ∈M = Rn. The pseudodistance between x and y is then
70"
PRELIMINARIES,0.10554089709762533,"Pd(x, y) = inf{Pl(γ) | γ : [0, 1] →M, γ ∈C1([0, 1]), γ(0) = x, γ(1) = y}.
(4)"
PRELIMINARIES,0.10686015831134564,"One can observe that endowing the space Rn with a singular Riemannian metric leads to have
71"
PRELIMINARIES,0.10817941952506596,"non trivial curves whose length is zero. A straightforward consequence is that there are distinct
72"
PRELIMINARIES,0.10949868073878628,"points whose pseudodistance is therefore zero: a natural equivalence relation arises, i.e. x ∼y ⇔
73"
PRELIMINARIES,0.11081794195250659,"Pd(x, y) = 0, obtaining thus a metric space (Rn/ ∼, Pd).
74"
PRELIMINARIES,0.11213720316622691,"The second crucial tool is the notion of pullback of a function. Let f be a function from Rp to Rq,
75"
PRELIMINARIES,0.11345646437994723,"and fix the coordinate systems x = (x1, . . . , xp) and y = (y1, . . . , yq) on Rp and on Rq, respectively.
76"
PRELIMINARIES,0.11477572559366754,"Moreover, we endow Rq with the standard Euclidean metric g, whose associated matrix is the identity.
77"
PRELIMINARIES,0.11609498680738786,"The space Rp can be equipped with the pullback metric f ∗g whose representation matrix reads as
78"
PRELIMINARIES,0.11741424802110818,"(f ∗g)ij = q
X h,k=1 ∂fh ∂xi 
ghk ∂fk ∂xj"
PRELIMINARIES,0.11873350923482849,"
.
(5)"
PRELIMINARIES,0.12005277044854881,"The sequence (1) shows that a neural network can be considered simply as a function, a composition
79"
PRELIMINARIES,0.12137203166226913,"of maps: hence, taking f = Λn ◦Λn−1 ◦· · · ◦Λ1 and supposing that M0 = Rp, Mn = Rq, the
80"
PRELIMINARIES,0.12269129287598944,"generalization of (5) applied to (1) provides with the pullback of a generic neural network.
81"
PRELIMINARIES,0.12401055408970976,"Hereafter, we consider in (1) the case Mn = Rq, equipped with the trivial metric g(n) = Iq, i.e.,
82"
PRELIMINARIES,0.12532981530343007,"the identity. Each manifold Mi of the sequence (1) is equipped with a Riemannian singular metric,
83"
PRELIMINARIES,0.1266490765171504,"denoted with g(i), obtained via the pullback of N(i). The pseudolength of a curve γ on the i-th
84"
PRELIMINARIES,0.1279683377308707,"manifold, namely Pli(γ), is computed via the relative metric g(i) via (3).
85"
GENERAL RESULTS,0.12928759894459102,"2.1
General results
86"
GENERAL RESULTS,0.13060686015831136,"We depict hereafter the theoretical bases of our approach. We denote with Ni the submap Λi◦· · ·◦Λn :
87"
GENERAL RESULTS,0.13192612137203166,"Mi →Mn, and with N ≡N0 the map describing the action of the complete network. The starting
88"
GENERAL RESULTS,0.13324538258575197,"point is to consider the pair (Mi, Pdi): this is a pseudometric space, which can be turned into a
89"
GENERAL RESULTS,0.1345646437994723,"full-fledged metric space Mi/ ∼i by the metric identification x ∼i y ⇔Pdi(x, y) = 0. The first
90"
GENERAL RESULTS,0.1358839050131926,"result states that the length of a curve on the i-th manifold is preserved among the mapping on the
91"
GENERAL RESULTS,0.13720316622691292,"subsequent manifolds.
92"
GENERAL RESULTS,0.13852242744063326,"Proposition 1. Let γ : [0, 1] →Mi be a piecewise C1 curve. Let k ∈{i, i + 1, · · · , n} and consider
93"
GENERAL RESULTS,0.13984168865435356,"the curve γk = Λk ◦· · · ◦Λi ◦γ on Mk. Then Pli(γ) = Plk(γk).
94"
GENERAL RESULTS,0.14116094986807387,"In particular this is true when k = n, i.e., the length of a curve is preserved in the last manifold. This
95"
GENERAL RESULTS,0.1424802110817942,"result leads naturally to claim that if two points are in the same class of equivalence, then they are
96"
GENERAL RESULTS,0.1437994722955145,"mapped into the same point under the action of the neural network.
97"
GENERAL RESULTS,0.14511873350923482,"Proposition 2. If two points p, q ∈Mi are in the same class of equivalence, then Ni(p) = Ni(q).
98"
GENERAL RESULTS,0.14643799472295516,"The next step is to prove that the sets Mi/ ∼i are actually smooth manifolds: to this aim, we introduce
99"
GENERAL RESULTS,0.14775725593667546,"another equivalence relation: x ∼Ni y if and only if there exists a piecewise γ : [0, 1] →Mi such
100"
GENERAL RESULTS,0.14907651715039577,"that γ(0) = x, γ(1) = y and Ni ◦γ(s) = Ni(x) ∀s ∈[0, 1]. The introduction of this equivalence
101"
GENERAL RESULTS,0.1503957783641161,"relation allows us to easily state the following proposition.
102"
GENERAL RESULTS,0.1517150395778364,"Proposition 3. Let x, y ∈Mi, then x ∼i y if and only if x ∼Ni y.
103"
GENERAL RESULTS,0.15303430079155672,"The following corollary contains the natural consequences of the previous result; the second point of
104"
GENERAL RESULTS,0.15435356200527706,"the claim below is the counterpart of Proposition 2.
105"
GENERAL RESULTS,0.15567282321899736,"Corollary 1. Under the hypothesis of Proposition 3, one has that Mi/∼i = Mi/∼Ni+1. Moreover,
106"
GENERAL RESULTS,0.15699208443271767,"if two points p, q ∈Mi are connected by a C1 curve γ : [0, 1] →Mi satisfying Ni(p) = Ni ◦γ(s)
107"
GENERAL RESULTS,0.158311345646438,"for every s ∈[0, 1], then they lie in the same class of equivalence.
108"
GENERAL RESULTS,0.15963060686015831,"Making use of the Godement’s criterion, we are now able to prove that the set Mi/ ∼i is a smooth
109"
GENERAL RESULTS,0.16094986807387862,"manifold, together with its dimension.
110"
GENERAL RESULTS,0.16226912928759896,Proposition 4. Mi
GENERAL RESULTS,0.16358839050131926,"∼i
is a smooth manifold of dimension dim(N(M0)).
111"
GENERAL RESULTS,0.16490765171503957,"This last achievement provides practical insights about the projection πi on the quotient space, that
112"
GENERAL RESULTS,0.1662269129287599,"consists the building block of the algorithms used for recovering and exploring the equivalence
113"
GENERAL RESULTS,0.16754617414248021,"classes of a neural network.
114"
GENERAL RESULTS,0.16886543535620052,"Proposition 5. πi : Mi →Mi/ ∼i is a smooth fiber bundle, with Ker(dπi) = VMi, which is
115"
GENERAL RESULTS,0.17018469656992086,"therefore an integrable distribution. VMi is the vertical bundle of Mi. Every class of equivalence
116"
GENERAL RESULTS,0.17150395778364116,"[p] is a path-connected submanifold of Mi and coincide with the fiber of the bundle over the point
117"
GENERAL RESULTS,0.17282321899736147,"p ∈Mi.
118"
METHODOLOGY,0.1741424802110818,"3
Methodology
119"
METHODOLOGY,0.17546174142480211,"The results depicted in Section 2.1 provide powerful tools for investigating how a neural network
120"
METHODOLOGY,0.17678100263852242,"sees the input space starting from a point x. In particular we point out the following remarks: i) If
121"
METHODOLOGY,0.17810026385224276,"two points x, y belonging to the input manifold M0 are are such that x ∼0 y, then N(x) = N(y); ii)
122"
METHODOLOGY,0.17941952506596306,"given a point p ∈Mn, the counterimage N −1(p) is a smooth manifold, whose connected components
123"
METHODOLOGY,0.18073878627968337,"are classes of equivalences in M0 with respect to ∼0. A necessary condition for two points x, y ∈M0
124"
METHODOLOGY,0.1820580474934037,"to be in the same class of equivalence is that N(x) = N(y); iii) any class of equivalence [x], x ∈M0,
125"
METHODOLOGY,0.18337730870712401,"is a maximal integral submanifold of VM0. The above observations directly provide with a strategy
126"
METHODOLOGY,0.18469656992084432,"to build up the equivalence class of an input point x ∈M0. Proposition 5 tells us that VM0 is an
127"
METHODOLOGY,0.18601583113456466,"integrable distribution, with dimension equal to the dimension of the kernel of g(0): we can hence find
128"
METHODOLOGY,0.18733509234828497,"dim(Ker(g(0))) vector fields which are a base for the tangent space of M0. This means that we can
129"
METHODOLOGY,0.18865435356200527,"compute the eigenvalue decomposition of g(0)
x
and consider the L linearly independent eigenvectors,
130"
METHODOLOGY,0.18997361477572558,"namely {vl}l=1,...,L, associated to the null eigenvalue: these eigenvectors depend smoothly on the
131"
METHODOLOGY,0.19129287598944592,"point, a fact that is not trivial when the matrix associated to the metric depends on several parameters
132"
METHODOLOGY,0.19261213720316622,"[15]. We can build then all the null curves by randomly selecting one eigenvector ˜v ∈{vl} and then
133"
METHODOLOGY,0.19393139841688653,"reconstruct the curve along the direction ˜v from the starting point x. From a practical point of view,
134"
METHODOLOGY,0.19525065963060687,"one is led to solve the Cauchy problem, a first order differential equation, with ˙γ = ˜v and initial
135"
METHODOLOGY,0.19656992084432717,"condition γ(0) = x.
136"
INPUT SPACE EXPLORATION,0.19788918205804748,"3.1
Input Space Exploration
137"
INPUT SPACE EXPLORATION,0.19920844327176782,"This whole procedure is coded in the Singular Metric Equivalence Class (SiMEC) and the Singular
138"
INPUT SPACE EXPLORATION,0.20052770448548812,"Metric Exploration (SiMExp) algorithms, whose general schemes are depicted in Algorithms 1 and 2.
139"
INPUT SPACE EXPLORATION,0.20184696569920843,"SiMEC reconstructs the class of equivalence of the input via the exploration of the input space by
140"
INPUT SPACE EXPLORATION,0.20316622691292877,"randomly selecting one of the eigenvectors related to the zero eigenvalue. On the opposite, in SiMExp,
141"
INPUT SPACE EXPLORATION,0.20448548812664907,"in order to move from a class of equivalence to another we consider the eigenvectors relative to the
142"
INPUT SPACE EXPLORATION,0.20580474934036938,"nonzero eigenvalues. This requires the slight difference in lines 5 to 7 between Algorithm 1 and
143"
INPUT SPACE EXPLORATION,0.20712401055408972,"Algorithm 2.
144"
INPUT SPACE EXPLORATION,0.20844327176781002,"Algorithm 1 The Singular Metric Equivalence Class
(SiMEC) algorithm."
INPUT SPACE EXPLORATION,0.20976253298153033,"1: Set the network N; choose the maximum number
of iterations. Choose the input p0.
2: for k = 0, 1, . . . , K −1 do
3:
Compute gn
N (pk)
4:
Compute the pullback metric g0
pk
5:
Diagonalize g0
pk and find the eigenvectors
{vl}l associated to the zero eigenvalue
6:
Randomly select ˜v ∈{vl}l
7:
δ = 1/
p"
INPUT SPACE EXPLORATION,0.21108179419525067,"max(eigenvalues of g0pk)
8:
pk+1 ←pk + δ˜v
9: end for
10: Optionally: store {pk}k=0,...,K for optimizing
future computations
11: Project pk to the nearest feasible region"
INPUT SPACE EXPLORATION,0.21240105540897097,"Algorithm 2 The Singular Metric Exploration (SiM-
Exp) algorithm."
INPUT SPACE EXPLORATION,0.21372031662269128,"1: Set the network N; choose the maximum number
of iterations. Choose the input p0.
2: for k = 0, 1, . . . , K −1 do
3:
Compute gn
N (pk)
4:
Compute the pullback metric g0
pk
5:
Diagonalize g0
pk and find the eigenvectors
{wl}l associated to the non-zero eigenvalue
6:
Randomly select ˜w ∈{wl}l
7:
δ = 2/
p"
INPUT SPACE EXPLORATION,0.21503957783641162,"max(eigenvalues of g0pk)
8:
pk+1 ←pk + δ ˜w
9: end for
10: Optionally: store {pk}k=0,...,K for optimizing
future computations
11: Project pk to the nearest feasible region 145"
INPUT SPACE EXPLORATION,0.21635883905013192,"There are some remarks to point out. From a numerical point of view, the diagonalization of the
146"
INPUT SPACE EXPLORATION,0.21767810026385223,"pullback may lead to have even negative eigenvalues: hence one may use the notion of energy of
147"
INPUT SPACE EXPLORATION,0.21899736147757257,"a curve, related to the pseudolength. The update rule for the new point (line 8) amounts to solve
148"
INPUT SPACE EXPLORATION,0.22031662269129287,"the differential problem via the Euler method: for a reliable solution, we suggest to choose a small
149"
INPUT SPACE EXPLORATION,0.22163588390501318,"step-length δ. On the other hand, if the value of δ is too small more iterations are needed to move
150"
INPUT SPACE EXPLORATION,0.22295514511873352,"away from the starting point sensibly. Therefore there is a trade-off between the reliability of the
151"
INPUT SPACE EXPLORATION,0.22427440633245382,"solution and the exploration pace. The proof of the well-posedness theorem for Cauchy problems,
152"
INPUT SPACE EXPLORATION,0.22559366754617413,"cf. [18, Theorem 2.1], yields some insights, suggesting to set δ equal to the inverse of the Lipschitz
153"
INPUT SPACE EXPLORATION,0.22691292875989447,"constant of the map N – which in practice we can estimate with the inverse of the square root of the
154"
INPUT SPACE EXPLORATION,0.22823218997361477,"largest eigenvalue λM of the pullback metric g0
pk. This is our default choice for Algorithm 1. We also
155"
INPUT SPACE EXPLORATION,0.22955145118733508,"note that Algorithm 1 is more sensitive to the choice of the parameter δ compared to Algorithm 2.
156"
INPUT SPACE EXPLORATION,0.23087071240105542,"To build points in the same equivalence class Algorithm 1 needs to follow a null curve closely with
157"
INPUT SPACE EXPLORATION,0.23218997361477572,"as little approximations as possible, namely with a small δ. In contrast Algorithm 2, whose goal is
158"
INPUT SPACE EXPLORATION,0.23350923482849603,"to change the equivalence class from one iteration to the next, does not have the same problem and
159"
INPUT SPACE EXPLORATION,0.23482849604221637,"larger δ are allowed. Out default choice is therefore to set δ = 2λ−1/2
M
for Algorithm 2. As for the
160"
INPUT SPACE EXPLORATION,0.23614775725593667,"computational complexity of the two algorithms, the most demanding step is the computation of the
161"
INPUT SPACE EXPLORATION,0.23746701846965698,"eigenvalues and eigenvectors, which is O(n3), with n the dimension of the square matrix g0
pk [20].
162"
INPUT SPACE EXPLORATION,0.23878627968337732,"Since all the other operations are either O(n) or O(n2), we conclude that the complexity of both
163"
INPUT SPACE EXPLORATION,0.24010554089709762,"Algorithms 1 and 2 is O(n3).
164"
INTERPRETABILITY,0.24142480211081793,"3.2
Interpretability
165"
INTERPRETABILITY,0.24274406332453827,"Algorithms 1 and 2 allow for the exploration of the equivalence classes in the input space of a Trans-
166"
INTERPRETABILITY,0.24406332453825857,"former model. However, the points explored by these algorithms may not be directly interpretable
167"
INTERPRETABILITY,0.24538258575197888,"by a human perspective. For instance, an image or a piece of text may need to be decoded to be
168"
INTERPRETABILITY,0.24670184696569922,"“readable” by a human observer. Furthermore, we present an interpretation of the eigenvalues of the
169"
INTERPRETABILITY,0.24802110817941952,"pullback metric which allows us to define a feature importance metric. We present two interpretability
170"
INTERPRETABILITY,0.24934036939313983,"methods for Transformers based on input space exploration. Both methods are then demonstrated on
171"
INTERPRETABILITY,0.25065963060686014,"a Vision Transformer (ViT) trained for digit classification [8], and two BERT models, one trained for
172"
INTERPRETABILITY,0.2519788918205805,"hate speech classification and the other trained for MLM [7, 19].
173"
INTERPRETABILITY,0.2532981530343008,"Algorithm 3 Feature Importance Analysis Using Pull-
back Metric g0
xe
1: Inputs:
2:
Transformer model T with: Tokenizer tT , Em-
bedding layer eT , Intermediate layers lT
3:
Input data x
4: Tokenize input x to obtain tokens xt = tT (x)
5: Compute embeddings xe = eT (xt)
6: Compute intermediate representations gn
lT (xe)
7: Calculate the pullback metric g0
xe
8: Diagonalize g0
xe to extract eigenvalues
9: Identify the maximum eigenvalue for each embed-
ding, indicating its importance
10: Output: Heatmap of embedding importance based
on the eigenvalues"
INTERPRETABILITY,0.2546174142480211,"Algorithm 4 Exploration of Embedding Space in
Transformers"
INTERPRETABILITY,0.2559366754617414,"1: Inputs:
2:
Transformer model T with: Tokenizer tT , Em-
bedding layer eT , Intermediate layers lT
3:
Input data x (image or text)
4: Retrieve segments xt = tT (x).
5: Choose segments P = {p|p ∈xt} for updates;
keep others unchanged.
6: Compute embeddings xe = eT (xt).
7: Apply SiMEC or SiMExp on xe, updating embed-
dings for segments in P.
8: Outputs: Modified input embedding, one for each
SiMEC/SiMExp iteration. 174"
INTERPRETABILITY,0.25725593667546176,"Feature importance.
Consider a Transformer model T whose architecture includes a tokenizer tT
175"
INTERPRETABILITY,0.25857519788918204,"(or patcher for images) that segments the input so that each segment can be converted into a continuous
176"
INTERPRETABILITY,0.2598944591029024,"representation by an embedding layer eT . This results in a matrix of dimensions ns × h, where ns
177"
INTERPRETABILITY,0.2612137203166227,"represents the number of segments, and h denotes the hidden size of the model’s embeddings. The
178"
INTERPRETABILITY,0.262532981530343,"eigenvalues of the pullback metric can be used to deduce the importance of each embedding and, by
179"
INTERPRETABILITY,0.2638522427440633,"extension, the significance of the segments they represent, with respect to the final prediction. The
180"
INTERPRETABILITY,0.26517150395778366,"process for determining the importance of textual tokens or image patches is outlined in Algorithm 3.
181"
INTERPRETABILITY,0.26649076517150394,"The appearance of the resulting heatmaps varies according to the type of input used. An example
182"
INTERPRETABILITY,0.2678100263852243,"of experiments with ViT on the MNIST dataset [12] is shown in Figure 1 that depicts heatmaps for
183"
INTERPRETABILITY,0.2691292875989446,"two MNIST instances. Figure 2, on the left, illustrates two experiment using Algorithm 3 on both a
184"
INTERPRETABILITY,0.2704485488126649,"BERT model for hate speech detection and a BERT model for MLM.
185"
INTERPRETABILITY,0.2717678100263852,"Interpretation of input space exploration.
Using SiMEC and SiMExp to explore the embedding
186"
INTERPRETABILITY,0.27308707124010556,"space reveals how Transformer models perceive equivalence among different data points. Specifically,
187"
INTERPRETABILITY,0.27440633245382584,"these methodologies facilitate the sequential acquisition of embedding matrices p0 . . . pK at each
188"
INTERPRETABILITY,0.2757255936675462,"iteration, as detailed in Algorithms 1 and 2. Algorithm 4 implements a practical application of the
189"
INTERPRETABILITY,0.2770448548812665,"Figure 1: Example output from Algorithm 3 applied to digit classification. These two instances are
predicted as 3 (left) and 4 (right). The brightness of the color indicates the eigenvalue’s magnitude.
The brighter the color, the more sensitive the patch. This indicates that changes in the values of these
sensitive patches are likely to have a greater impact on the prediction probabilities. Each patch in the
heatmap corresponds to a 2 × 2 square pixel."
INTERPRETABILITY,0.2783641160949868,"Figure 2: Example outputs from Algorithm 3. The darker the color, the higher the token’s eigenvalue.
Left: The sentence analysed is classified as “offensive” by the BERT for hate speech detection, with
significant contributions from tokens [CLS], politicians, corrupt, and ##eit (part of the word
deceitful). Right: Example instance processed by a BERT model for masked language modeling.
[MASK] is predicted as “ham”, with the most influential tokens being pizza and cheese."
INTERPRETABILITY,0.2796833773087071,"SiMEC/SiMExp approach with Transformer models. A key feature of this method is its ability
190"
INTERPRETABILITY,0.28100263852242746,"to selectively update specific tokens (for text inputs) or patches (for image inputs) during each
191"
INTERPRETABILITY,0.28232189973614774,"iteration. This selective updating allows us to explore targeted modifications that prompt the model
192"
INTERPRETABILITY,0.2836411609498681,"to either categorize different inputs as the same class or recognize them as distinct. Unlike traditional
193"
INTERPRETABILITY,0.2849604221635884,"approaches where modifications are predetermined, this method lets the model itself guide us to
194"
INTERPRETABILITY,0.2862796833773087,"understand which data points belong to specific equivalence classes. To interpret embeddings resulted
195"
INTERPRETABILITY,0.287598944591029,"from the exploration process, they must be mapped back into a human-understandable form, such as
196"
INTERPRETABILITY,0.28891820580474936,"text or images. The interpretation of an embedding vector depends on the operations performed by
197"
INTERPRETABILITY,0.29023746701846964,"the Transformer’s embedding module eT . If eT consists only of invertible operations, it is feasible to
198"
INTERPRETABILITY,0.29155672823219,"construct a layer that performs the inverse operation relative to eT . The output can then be visualized
199"
INTERPRETABILITY,0.2928759894459103,"and directly interpreted by humans, allowing for a comparison with the original input to discern
200"
INTERPRETABILITY,0.2941952506596306,"how differences in embeddings reflect differences in their representations (e.g., text, images). If the
201"
INTERPRETABILITY,0.2955145118733509,"operations in eT are non-invertible, a trained decoder is required to reconstruct an interpretable output
202"
INTERPRETABILITY,0.29683377308707126,"from each embedding matrix p0 . . . pK. When using a BERT model, it is feasible to utilize layers
203"
INTERPRETABILITY,0.29815303430079154,"that are specialized for the masked language modeling (MLM) task to map input embeddings back to
204"
INTERPRETABILITY,0.2994722955145119,"tokens. This approach is effective whether the BERT model in question is specifically designed for
205"
INTERPRETABILITY,0.3007915567282322,"MLM or for sentence classification. In the case of sentence classification models, it is necessary to
206"
INTERPRETABILITY,0.3021108179419525,"select a corresponding MLM BERT model that shares the same internal architecture, including the
207"
INTERPRETABILITY,0.3034300791556728,"number of layers and embedding size.
208"
INTERPRETABILITY,0.30474934036939316,"Algorithm 5 depicts the process of interpreting Algorithm 4 outputs for both ViT and BERT experi-
209"
INTERPRETABILITY,0.30606860158311344,"ments. After initializing the decoder according to the model type, the embeddings p0 . . . pK need to
210"
INTERPRETABILITY,0.3073878627968338,"be constrained to a feasible region. This region is defined by the distribution of embeddings derived
211"
INTERPRETABILITY,0.3087071240105541,"from the original input instances. Next, the embeddings are decoded, and the selected segments
212"
INTERPRETABILITY,0.3100263852242744,"for exploration are extracted. These segments are then used to replace the corresponding parts of
213"
INTERPRETABILITY,0.3113456464379947,"the original input instance. Figure 3 depicts an example outcome of Algorithm 5 applied on a ViT
214"
INTERPRETABILITY,0.31266490765171506,"exploration experiment. Given that the interpretation process includes both a capping step and a
215"
INTERPRETABILITY,0.31398416886543534,"decoding step (lines 10 and 11 of Algorithm 5), it’s important to note that there isn’t a direct 1:1
216"
INTERPRETABILITY,0.3153034300791557,"correspondence between each iteration’s update and the interpretation outcomes. Our primary focus
217"
INTERPRETABILITY,0.316622691292876,"is on exploring the input embedding space, rather than the input image or input sentence spaces.
218"
INTERPRETABILITY,0.3179419525065963,"For further investigation, we provide a detailed discussion on considering interpretation outputs as
219"
INTERPRETABILITY,0.31926121372031663,"alternative prompts in Section 4.
220"
INTERPRETABILITY,0.32058047493403696,Algorithm 5 Interpretation for Exploration results for ViT and BERT models.
INTERPRETABILITY,0.32189973614775724,"1: Inputs:
2:
Transformer model T with: Tokenizer tT , Embedding layer eT , Intermediate layers lT
3:
Modified embeddings p0 . . . pK resulted from Algorithm 4 applied on an input x
4:
P = {p|p ∈xt} indices of updated segments
5: If T is ViT:
6:
Initialize decoder d with weights from eT .
7: If T is BERT:
8:
Initialize decoder with intermediate and final layers of a BERT for MLM task.
9: Compute embeddings distributions for original input data
10: Use the original embeddings distributions to cap p0 . . . pK
11: Decode modified embeddings p0 . . . pK using d to generate the corresponding images/sentences X′ =
x′
0 . . . x′
K.
12: For each x′ ∈X′: replace segments relative to indices P in x with those in x′.
13: Outputs:
14: Modified input images/sentences, one for each SiMEC/SiMExp iteration."
INTERPRETABILITY,0.3232189973614776,"Figure 3: Example of SiMEC and SiMExp output interpretation for ViT digit classification. Left:
Original MNIST image of an “8”. Center: Interpretation of a p1000 from a SiMEC experiment, where
p1000 is predicted as “8”. Right: Interpretation of a p1000 from a SiMExp experiment, where p1000 is
predicted as “4”. All patches are subject to SiMEC and SiMExp updates."
EXPERIMENTS,0.3245382585751979,"4
Experiments
221"
EXPERIMENTS,0.3258575197889182,"Experiments are conducted on textual and visual data. We aim to perform a preliminary investigation
222"
EXPERIMENTS,0.32717678100263853,"of 3 features of our approach: (i) how the class probability changes on the decoded output of
223"
EXPERIMENTS,0.32849604221635886,"SiMEC/SiMExp, (ii) what is the trade-off between the quantity and the quality of the output, and (iii)
224"
EXPERIMENTS,0.32981530343007914,"how our method can be used to extract feature importance-based explanations.
225"
EXPERIMENTS,0.3311345646437995,"In the textual case, we experiment with hate speech classification datasets: we use HateXplain2 [13],
226"
EXPERIMENTS,0.3324538258575198,"which provides a ground truth for feature importance, plus a sample of 100 hate speech sentences
227"
EXPERIMENTS,0.3337730870712401,"generated by prompting ChatGPT3, which serve purposes (i) and (ii). In the visual case, we perform
228"
EXPERIMENTS,0.33509234828496043,"experiments on MNIST [12] dataset.
229"
EXPERIMENTS,0.33641160949868076,"Using interpretation outputs as alternative prompts
An interesting investigation is to determine
230"
EXPERIMENTS,0.33773087071240104,"if our interpretation algorithm (Algorithm 5) can generate alternative prompts that stay in the same
231"
EXPERIMENTS,0.3390501319261214,"equivalence class as the original input data or move to a different one, based on SiMEC and SiMExp
232"
EXPERIMENTS,0.3403693931398417,"explorations. We test how the probability assigned to the original equivalence class by the Transformer
233"
EXPERIMENTS,0.341688654353562,"model changes as the SiMEC and SiMExp algorithms explore the input embedding manifold.
234"
EXPERIMENTS,0.34300791556728233,"For BERT experiments we generate prompts to inspect the probability distribution over the vocabulary
235"
EXPERIMENTS,0.34432717678100266,"for tokens updated by Algorithms 1 and 2. We decode the updated p0 . . . pK using Algorithm 5,
236"
EXPERIMENTS,0.34564643799472294,"focusing on tokens updated through the iterations. For each of these decoded tokens, we extract
237"
EXPERIMENTS,0.3469656992084433,"the top-5 scores to obtain 5 alternative tokens to replace the original ones, creating 5 alternate
238"
EXPERIMENTS,0.3482849604221636,"prompts. We then extract the prediction i∗= arg maxi yi for the original sentence, which represents
239"
EXPERIMENTS,0.3496042216358839,"the output whose equivalence class we aim to explore. Finally, we classify the new prompts,
240"
EXPERIMENTS,0.35092348284960423,"obtaining the corresponding predictions Y = y(0) . . . y(K), where each y(k) ∈RN, N being
241"
EXPERIMENTS,0.35224274406332456,"the number of prediction classes. We visualize the prediction trend for the i∗th value in every
242"
EXPERIMENTS,0.35356200527704484,"y(0) . . . y(K) categorizing the images into two subsets: those that lead to a change in prediction
243"
EXPERIMENTS,0.3548812664907652,"Yc = {y(k) ∈Y | arg maxi y(k)
i
̸= i∗} and those that don’t Ys = {yi ∈Y | arg maxi y(k)
i
= i∗}.
244"
MIT LICENSE,0.3562005277044855,"2MIT License
3Used prompts are included in the Supplementary Materials."
MIT LICENSE,0.3575197889182058,"Figure 4: Analysis involving results SiMEC and SiMExp applied to BERT for hate speech detection.
Left: Prediction values for i∗for each y ∈Yc. Right: Prediction values for y ∈Ys."
MIT LICENSE,0.35883905013192613,"Sentence classification experiments4 involved 1000 iterations from both SiMEC and SiMExp, applied
245"
MIT LICENSE,0.36015831134564646,"to a subset of 8 sentences from the ChatGPT hate speech dataset. The plot on the left side of Figure 4
246"
MIT LICENSE,0.36147757255936674,"illustrates that, as the original embeddings are increasingly modified, SiMExp tends to produce
247"
MIT LICENSE,0.3627968337730871,"alternatives with lower prediction values for i∗compared to SiMEC. Thus, even if predictions change
248"
MIT LICENSE,0.3641160949868074,"in SiMEC experiments, the equivalence class prediction value remains approximately constant and
249"
MIT LICENSE,0.3654353562005277,"higher than in SiMExp. Considering the plot on the right side of Figure 4, SiMExp identifies prompts
250"
MIT LICENSE,0.36675461741424803,"that lower the prediction value for i∗. ViT and MLM experiments are detailed in the Supplementary
251"
MIT LICENSE,0.36807387862796836,"Materials.
252"
MIT LICENSE,0.36939313984168864,"Input space exploration
We measure the time required to explore the input space of a ViT with
253"
MIT LICENSE,0.370712401055409,"the SiMEC algorithm and compare it with a perturbation-based method. The perturbation-based
254"
MIT LICENSE,0.3720316622691293,"method mimics a trial-and-error approach as it takes an input image and, at each iteration, perturbs
255"
MIT LICENSE,0.3733509234828496,"it by a semi-random vector vt+1 = atvt + ηϵ, where at = 1 if yt = yt−1, at = −1 otherwise, ϵ is
256"
MIT LICENSE,0.37467018469656993,"an orthogonal random vector from a standard normal distribution and η is the step length. With the
257"
MIT LICENSE,0.3759894459102902,"perturbation, we obtain a new image, then check whether the model yields the same label for the
258"
MIT LICENSE,0.37730870712401055,"new image. The perturbation vector is re-initialized at random from a normal distribution 20% of the
259"
MIT LICENSE,0.3786279683377309,"times to allow for exploration. We construct this method to have a direct comparison with ours in the
260"
MIT LICENSE,0.37994722955145116,"absence of a consolidated literature about the task.
261"
MIT LICENSE,0.3812664907651715,"We train a ViT model having 4 layers and 4 heads per layer on the MNIST dataset5. The SiMEC
262"
MIT LICENSE,0.38258575197889183,"algorithm is run for 1000 iterations, so that it can generate 1000 examples starting from a single
263"
MIT LICENSE,0.3839050131926121,"image. In a sample of 100 images, the average time is approximately 339 seconds.6 In the same
264"
MIT LICENSE,0.38522427440633245,"time, the perturbation-based algorithm can produce up to 36000 images. However, we notice that
265"
MIT LICENSE,0.3865435356200528,"the perturbation-based algorithm ends up producing monochrome (pixel color has zero variance) or
266"
MIT LICENSE,0.38786279683377306,"totally noisy images, which provide little information about the behavior of the model. Excluding
267"
MIT LICENSE,0.3891820580474934,"only the images with low color variance (< 0.01), we are left, on average, with 19 images (standard
268"
MIT LICENSE,0.39050131926121373,"deviation 13.9). SiMEC, in contrast, doesn’t present this behavior, as all 1000 images have high
269"
MIT LICENSE,0.391820580474934,"enough intensity variance and are thus useful for explainability purposes.
270"
MIT LICENSE,0.39313984168865435,"As BERT has many more parameters with respect to our ViT model, processing textual data takes
271"
MIT LICENSE,0.3944591029023747,"longer. Specifically, in a sample of 16 sentences, the average time needed to run 1000 iterations on a
272"
MIT LICENSE,0.39577836411609496,"sentence is 7089 seconds, taking into account both MLM and classification experiments.
273"
MIT LICENSE,0.3970976253298153,"Feature importance-based explanations
We compare our method against Attention Rollout
274"
MIT LICENSE,0.39841688654353563,"(AR) [1] and the Relevancy method proposed by Chefer et al. [6]. In the textual case, we provide a
275"
MIT LICENSE,0.3997361477572559,"quantitative evaluation using the HateXplain dataset, which contains 20147 sentences (of which 1924
276"
MIT LICENSE,0.40105540897097625,"in the test set) annotated with normal, offensive and hate speech labels as well as the positions of
277"
MIT LICENSE,0.4023746701846966,"words that support the label decision. We then measure the cosine similarity between the importance
278"
MIT LICENSE,0.40369393139841686,"assigned by each method to each word in a sentence and the ground truth. Notice that, since the
279"
MIT LICENSE,0.4050131926121372,"4Model used: huggingface.co/ctoraman/hate-speech-bert
5Using Adam optimizer, the model achieved the highest validation accuracy (96.25%) in 20 epochs.
6All experiments are based on the current PyTorch implementation of the algorithms and run on a Ubuntu
20.04 machine endowed with one NVIDIA A100 GPU and CUDA 12.4."
MIT LICENSE,0.40633245382585753,"dataset contains multiple annotations, the ground truth y for each word w is obtained as the average
280"
MIT LICENSE,0.4076517150395778,"of the binary labels assigned by each annotator, and therefore y(w) ∈[0; 1]. We also normalize all
281"
MIT LICENSE,0.40897097625329815,"scores in [0; 1] so to have them on the same scale. The average similarity achieved by our method is
282"
MIT LICENSE,0.4102902374670185,"0.707 (standard deviation σ = 0.302), against 0.7 (σ = 0.315) for Relevancy and 0.583 (σ = 0.318) for
283"
MIT LICENSE,0.41160949868073876,"AR. This proves our method to be more effective in finding the most sensitive tokens for classification.
284"
MIT LICENSE,0.4129287598944591,"We provide an example on image classification in the Supplementary Materials.
285"
RELATED WORK,0.41424802110817943,"5
Related work
286"
RELATED WORK,0.4155672823218997,"Our work relates to embedding space exploration literature, and has at least one collateral applications
287"
RELATED WORK,0.41688654353562005,"in the XAI domain, namely producing feature importance-based explanations.
288"
RELATED WORK,0.4182058047493404,"Embedding space exploration.
Works dealing with embedding space exploration mostly focus
289"
RELATED WORK,0.41952506596306066,"on the study of specific properties of the embedding space of Transformers, especially in NLP. For
290"
RELATED WORK,0.420844327176781,"instance, Cai et al. [5] challenge the idea that the embedding space is inherently anisotropic [10]
291"
RELATED WORK,0.42216358839050133,"discovering local isotropy, and find low-dimensional manifold structures in the embedding space
292"
RELATED WORK,0.4234828496042216,"of GPT and BERT. Bi´s et al. [3] argue that the anisotropy of the embedding space derives from
293"
RELATED WORK,0.42480211081794195,"embeddings shifting in common directions during training. In the field of CV, Vilas et al. [21] map
294"
RELATED WORK,0.4261213720316623,"internal representations of a ViT onto the output class manifold, enabling the early identification of
295"
RELATED WORK,0.42744063324538256,"class-related patches and the computation of saliency maps on the input image for each layer and
296"
RELATED WORK,0.4287598944591029,"head. Applying Singular Value Decomposition to the Jacobian matrix of a ViT, Salman et al. [17]
297"
RELATED WORK,0.43007915567282323,"treat the input space as the union of two subspaces: one in which image embedding doesn’t change,
298"
RELATED WORK,0.4313984168865435,"and another one for which it changes. Except for the last one, all the aforementioned approaches rely
299"
RELATED WORK,0.43271767810026385,"on data samples. By studying the inverse image of the model, instead, we can do away with data
300"
RELATED WORK,0.4340369393139842,"samples.
301"
RELATED WORK,0.43535620052770446,"Feature importance-based explanations.
Feature importance is a measure of the contribution of
302"
RELATED WORK,0.4366754617414248,"each data feature to a model prediction. In the context of Computer Vision and Natural Language
303"
RELATED WORK,0.43799472295514513,"Processing, it amounts to giving a weight to pixels (or patches of pixels) in an image and tokens
304"
RELATED WORK,0.4393139841688654,"in a piece of text, respectively. In recent years, much research has focused on Transformers in
305"
RELATED WORK,0.44063324538258575,"both CV and NLP. Most approaches are based on the attention mechanism of the Transformer
306"
RELATED WORK,0.4419525065963061,"architecture. Abnar and Zuidema [1] quantify the overall attention of the output on the input by
307"
RELATED WORK,0.44327176781002636,"computing a linear combination of layer attentions (Attention Rollout) or applying a maximum
308"
RELATED WORK,0.4445910290237467,"flow algorithm (Attention Flow). To overcome the limitations [4] of attention-based methods, Hao
309"
RELATED WORK,0.44591029023746703,"et al. [11] use the concept of attribution, which is obtained by multiplying attention matrices by
310"
RELATED WORK,0.4472295514511873,"the integrated gradient of the model with respect to them. Chefer et al. [6] propose the Relevancy
311"
RELATED WORK,0.44854881266490765,"metric to generalize attribution to bi-modal and encoder-decoder architectures. Other methods are
312"
RELATED WORK,0.449868073878628,"perturbation-based, where perturbations of input data are used to record any change in the output and
313"
RELATED WORK,0.45118733509234826,"draw a saliency map on the input. In order to overcome the main issue with such methods, i.e. the
314"
RELATED WORK,0.4525065963060686,"generation of outlier inputs, Englebert et al. [9] apply perturbations after the position encoding of the
315"
RELATED WORK,0.45382585751978893,"patches. In contrast with these methods, ours does not need arbitrary perturbations of inputs, and
316"
RELATED WORK,0.4551451187335092,"considers all parameters of the model, not only the attention query and key matrices.
317"
CONCLUSIONS,0.45646437994722955,"6
Conclusions
318"
CONCLUSIONS,0.4577836411609499,"Our exploration of the Transformer architecture through a theoretical framework grounded in Rie-
319"
CONCLUSIONS,0.45910290237467016,"mannian Geometry led to the application of our two algorithms, SiMEC and SiMExp, for examining
320"
CONCLUSIONS,0.4604221635883905,"equivalence classes in the Transformers’ input space. We demonstrated how the results of these explo-
321"
CONCLUSIONS,0.46174142480211083,"ration methods can be interpreted in a human-readable form and conducted preliminary investigations
322"
CONCLUSIONS,0.4630606860158311,"into their potential applications. Notably, our methods show promise for ranking feature importance
323"
CONCLUSIONS,0.46437994722955145,"and generating alternative prompts within the same or different equivalence classes.
324"
CONCLUSIONS,0.4656992084432718,"Future research directions include expanding our experimental results and delving deeper into the po-
325"
CONCLUSIONS,0.46701846965699206,"tential of our framework for controlled input generation within an equivalence class. This application
326"
CONCLUSIONS,0.4683377308707124,"holds significant promise for enhancing the explainability of Transformer models’ decisions and for
327"
CONCLUSIONS,0.46965699208443273,"addressing issues related to bias and hallucinations.
328"
REFERENCES,0.470976253298153,"References
329"
REFERENCES,0.47229551451187335,"[1] S. Abnar and W. Zuidema. Quantifying attention flow in transformers. 2020. arXiv:2005.00928.
330"
REFERENCES,0.4736147757255937,"[2] A. Benfenati and A. Marta. A singular Riemannian geometry approach to Deep Neural Networks
331"
REFERENCES,0.47493403693931396,"I. Theoretical foundations. Neural Networks, 158:331–343, 2023.
332"
REFERENCES,0.4762532981530343,"[3] D. Bi´s, M. Podkorytov, and X. Liu. Too much in common: Shifting of embeddings in trans-
333"
REFERENCES,0.47757255936675463,"former language models and its implications. In Proceedings of the 2021 conference of the
334"
REFERENCES,0.4788918205804749,"North American chapter of the Association for Computational Linguistics: Human Language
335"
REFERENCES,0.48021108179419525,"Technologies, pages 5117–5130, 2021.
336"
REFERENCES,0.4815303430079156,"[4] G. Brunner, Y. Liu, D. Pascual, O. Richter, M. Ciaramita, and R. Wattenhofer. On identifiability
337"
REFERENCES,0.48284960422163586,"in transformers. International Conference on Learning Representations, 2019.
338"
REFERENCES,0.4841688654353562,"[5] X. Cai, J. Huang, Y. Bian, and K. Church. Isotropy in the contextual embedding space: Clusters
339"
REFERENCES,0.48548812664907653,"and manifolds. In International conference on learning representations, 2020.
340"
REFERENCES,0.4868073878627968,"[6] H. Chefer, S. Gur, and L. Wolf. Generic attention-model explainability for interpreting bi-modal
341"
REFERENCES,0.48812664907651715,"and encoder-decoder transformers. In Proceedings of the IEEE/CVF International Conference
342"
REFERENCES,0.4894459102902375,"on Computer Vision, pages 397–406, 2021.
343"
REFERENCES,0.49076517150395776,"[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional
344"
REFERENCES,0.4920844327176781,"transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors,
345"
REFERENCES,0.49340369393139843,"Proceedings of the 2019 Conference of the North American Chapter of the Association for
346"
REFERENCES,0.4947229551451187,"Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-
347"
REFERENCES,0.49604221635883905,"pers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational
348"
REFERENCES,0.4973614775725594,"Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.
349"
REFERENCES,0.49868073878627966,"[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
350"
REFERENCES,0.5,"M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16
351"
REFERENCES,0.5013192612137203,"words: Transformers for image recognition at scale, 2021. arXiv:2010.11929.
352"
REFERENCES,0.5026385224274407,"[9] A. Englebert, S. Stassin, G. Nanfack, S. A. Mahmoudi, X. Siebert, O. Cornu, and
353"
REFERENCES,0.503957783641161,"C. De Vleeschouwer. Explaining through transformer input sampling. In Proceedings of
354"
REFERENCES,0.5052770448548812,"the IEEE/CVF International Conference on Computer Vision, pages 806–815, 2023.
355"
REFERENCES,0.5065963060686016,"[10] J. Gao, D. He, X. Tan, T. Qin, L. Wang, and T.-Y. Liu. Representation degeneration problem
356"
REFERENCES,0.5079155672823219,"in training natural language generation models. In International conference on learning rep-
357"
REFERENCES,0.5092348284960422,"resentations, volume abs/1907.12009, 2019. URL https://api.semanticscholar.org/
358"
REFERENCES,0.5105540897097626,"CorpusID:59317065.
359"
REFERENCES,0.5118733509234829,"[11] Y. Hao, L. Dong, F. Wei, and K. Xu. Self-attention attribution: Interpreting information
360"
REFERENCES,0.5131926121372031,"interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence,
361"
REFERENCES,0.5145118733509235,"volume 35, pages 12963–12971, 2021.
362"
REFERENCES,0.5158311345646438,"[12] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
363"
REFERENCES,0.5171503957783641,"recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791.
364"
REFERENCES,0.5184696569920845,"[13] B. Mathew, P. Saha, S. M. Yimam, C. Biemann, P. Goyal, and A. Mukherjee. Hatexplain: A
365"
REFERENCES,0.5197889182058048,"benchmark dataset for explainable hate speech detection. In Proceedings of the AAAI conference
366"
REFERENCES,0.521108179419525,"on artificial intelligence, volume 35, pages 14867–14875, 2021.
367"
REFERENCES,0.5224274406332454,"[14] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The Limitations
368"
REFERENCES,0.5237467018469657,"of Deep Learning in Adversarial Settings. In 2016 IEEE European Symposium on Security and
369"
REFERENCES,0.525065963060686,"Privacy (EuroS&P), pages 372–387, Mar. 2016. doi: 10.1109/EuroSP.2016.36.
370"
REFERENCES,0.5263852242744064,"[15] F. Rellich and J. Berkowitz. Perturbation Theory of Eigenvalue Problems. New York University.
371"
REFERENCES,0.5277044854881267,"Institute of Mathematical Sciences. Gordon and Breach, 1969. ISBN 9780677006802.
372"
REFERENCES,0.5290237467018469,"[16] M. T. Ribeiro, S. Singh, and C. Guestrin. "" why should i trust you?"" explaining the predictions of
373"
REFERENCES,0.5303430079155673,"any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge
374"
REFERENCES,0.5316622691292876,"discovery and data mining, pages 1135–1144, 2016.
375"
REFERENCES,0.5329815303430079,"[17] S. Salman, M. M. B. Shams, and X. Liu. Intriguing equivalence structures of the embedding
376"
REFERENCES,0.5343007915567283,"space of vision transformers, 2024. arXiv:2401.15568.
377"
REFERENCES,0.5356200527704486,"[18] M. E. Taylor. Partial differential equations. I: Basic theory, volume 115 of Appl. Math. Sci.
378"
REFERENCES,0.5369393139841688,"Cham: Springer, 3rd corrected and expanded edition edition, 2023. ISBN 978-3-031-33858-8;
379"
REFERENCES,0.5382585751978892,"978-3-031-33861-8; 978-3-031-33859-5. doi: 10.1007/978-3-031-33859-5.
380"
REFERENCES,0.5395778364116095,"[19] C. Toraman, F. ¸Sahinuç, and E. H. Yilmaz. Large-scale hate speech detection with cross-
381"
REFERENCES,0.5408970976253298,"domain transfer. In Proceedings of the Language Resources and Evaluation Conference, pages
382"
REFERENCES,0.5422163588390502,"2215–2225, Marseille, France, June 2022. European Language Resources Association. URL
383"
REFERENCES,0.5435356200527705,"https://aclanthology.org/2022.lrec-1.238.
384"
REFERENCES,0.5448548812664907,"[20] L. N. Trefethen and D. I. Bau. Numerical linear algebra. Twenty-fifth anniversary edition,
385"
REFERENCES,0.5461741424802111,"volume 181 of Other Titles Appl. Math. Philadelphia, PA: Society for Industrial and Applied
386"
REFERENCES,0.5474934036939314,"Mathematics (SIAM), 2022. ISBN 978-1-61197-715-8.
387"
REFERENCES,0.5488126649076517,"[21] M. G. Vilas, T. Schaumlöffel, and G. Roig. Analyzing vision transformers for image classi-
388"
REFERENCES,0.5501319261213721,"fication in class embedding space. Advances in Neural Information Processing Systems, 36,
389"
REFERENCES,0.5514511873350924,"2024.
390"
REFERENCES,0.5527704485488126,"[22] M. Wu, H. Wu, and C. Barrett. Verix: Towards verified explainability of deep neural networks.
391"
REFERENCES,0.554089709762533,"Advances in neural information processing systems, 36, 2024.
392"
REFERENCES,0.5554089709762533,"NeurIPS Paper Checklist
393"
CLAIMS,0.5567282321899736,"1. Claims
394"
CLAIMS,0.558047493403694,"Question: Do the main claims made in the abstract and introduction accurately reflect the
395"
CLAIMS,0.5593667546174143,"paper’s contributions and scope?
396"
CLAIMS,0.5606860158311345,"Answer: [Yes]
397"
CLAIMS,0.5620052770448549,"Justification: In the abstract and introduction we claim that we present a method for the
398"
CLAIMS,0.5633245382585752,"exploration of equivalence classes in the input space of Transformer models, which is
399"
CLAIMS,0.5646437994722955,"analyzed in depth in Section 3. The mathematical theory we refer to is deepened in Section
400"
CLAIMS,0.5659630606860159,"2.
401"
CLAIMS,0.5672823218997362,"Guidelines:
402"
CLAIMS,0.5686015831134564,"• The answer NA means that the abstract and introduction do not include the claims
403"
CLAIMS,0.5699208443271768,"made in the paper.
404"
CLAIMS,0.5712401055408971,"• The abstract and/or introduction should clearly state the claims made, including the
405"
CLAIMS,0.5725593667546174,"contributions made in the paper and important assumptions and limitations. A No or
406"
CLAIMS,0.5738786279683378,"NA answer to this question will not be perceived well by the reviewers.
407"
CLAIMS,0.575197889182058,"• The claims made should match theoretical and experimental results, and reflect how
408"
CLAIMS,0.5765171503957783,"much the results can be expected to generalize to other settings.
409"
CLAIMS,0.5778364116094987,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
410"
CLAIMS,0.579155672823219,"are not attained by the paper.
411"
LIMITATIONS,0.5804749340369393,"2. Limitations
412"
LIMITATIONS,0.5817941952506597,"Question: Does the paper discuss the limitations of the work performed by the authors?
413"
LIMITATIONS,0.58311345646438,"Answer: [Yes]
414"
LIMITATIONS,0.5844327176781002,"Justification: We discuss the limitations and tradeoff given by numeric integration in
415"
LIMITATIONS,0.5857519788918206,"Subsection 3.1 and theoretical assumptions are enumerated in Section 2. Computational
416"
LIMITATIONS,0.5870712401055409,"efficiency of our algorithms is discussed in Subsection 3.1. We conducted experiments on 3
417"
LIMITATIONS,0.5883905013192612,"datasets only, one of which of small dimensions since our main focus is on the mathematical
418"
LIMITATIONS,0.5897097625329816,"theory grounding the application of the method to Transformers, as stated at the beginning
419"
LIMITATIONS,0.5910290237467019,"of Section 4. Other limitations are mentioned throughout Section 4, including the fact that
420"
LIMITATIONS,0.5923482849604221,"our investigations in the human-readable scenario are at a preliminary stage.
421"
LIMITATIONS,0.5936675461741425,"Guidelines:
422"
LIMITATIONS,0.5949868073878628,"• The answer NA means that the paper has no limitation while the answer No means that
423"
LIMITATIONS,0.5963060686015831,"the paper has limitations, but those are not discussed in the paper.
424"
LIMITATIONS,0.5976253298153035,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
425"
LIMITATIONS,0.5989445910290238,"• The paper should point out any strong assumptions and how robust the results are to
426"
LIMITATIONS,0.600263852242744,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
427"
LIMITATIONS,0.6015831134564644,"model well-specification, asymptotic approximations only holding locally). The authors
428"
LIMITATIONS,0.6029023746701847,"should reflect on how these assumptions might be violated in practice and what the
429"
LIMITATIONS,0.604221635883905,"implications would be.
430"
LIMITATIONS,0.6055408970976254,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
431"
LIMITATIONS,0.6068601583113457,"only tested on a few datasets or with a few runs. In general, empirical results often
432"
LIMITATIONS,0.6081794195250659,"depend on implicit assumptions, which should be articulated.
433"
LIMITATIONS,0.6094986807387863,"• The authors should reflect on the factors that influence the performance of the approach.
434"
LIMITATIONS,0.6108179419525066,"For example, a facial recognition algorithm may perform poorly when image resolution
435"
LIMITATIONS,0.6121372031662269,"is low or images are taken in low lighting. Or a speech-to-text system might not be
436"
LIMITATIONS,0.6134564643799473,"used reliably to provide closed captions for online lectures because it fails to handle
437"
LIMITATIONS,0.6147757255936676,"technical jargon.
438"
LIMITATIONS,0.6160949868073878,"• The authors should discuss the computational efficiency of the proposed algorithms
439"
LIMITATIONS,0.6174142480211082,"and how they scale with dataset size.
440"
LIMITATIONS,0.6187335092348285,"• If applicable, the authors should discuss possible limitations of their approach to
441"
LIMITATIONS,0.6200527704485488,"address problems of privacy and fairness.
442"
LIMITATIONS,0.6213720316622692,"• While the authors might fear that complete honesty about limitations might be used by
443"
LIMITATIONS,0.6226912928759895,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
444"
LIMITATIONS,0.6240105540897097,"limitations that aren’t acknowledged in the paper. The authors should use their best
445"
LIMITATIONS,0.6253298153034301,"judgment and recognize that individual actions in favor of transparency play an impor-
446"
LIMITATIONS,0.6266490765171504,"tant role in developing norms that preserve the integrity of the community. Reviewers
447"
LIMITATIONS,0.6279683377308707,"will be specifically instructed to not penalize honesty concerning limitations.
448"
THEORY ASSUMPTIONS AND PROOFS,0.6292875989445911,"3. Theory Assumptions and Proofs
449"
THEORY ASSUMPTIONS AND PROOFS,0.6306068601583114,"Question: For each theoretical result, does the paper provide the full set of assumptions and
450"
THEORY ASSUMPTIONS AND PROOFS,0.6319261213720316,"a complete (and correct) proof?
451"
THEORY ASSUMPTIONS AND PROOFS,0.633245382585752,"Answer: [Yes]
452"
THEORY ASSUMPTIONS AND PROOFS,0.6345646437994723,"Justification: The full proofs are part of two previously published papers which we cannot
453"
THEORY ASSUMPTIONS AND PROOFS,0.6358839050131926,"disclose for anonymity requirements. We replicate the relevant proofs in the supplementary
454"
THEORY ASSUMPTIONS AND PROOFS,0.637203166226913,"material, part of which will be removed from the final version of the paper, referencing to
455"
THEORY ASSUMPTIONS AND PROOFS,0.6385224274406333,"the other papers.
456"
THEORY ASSUMPTIONS AND PROOFS,0.6398416886543535,"Guidelines:
457"
THEORY ASSUMPTIONS AND PROOFS,0.6411609498680739,"• The answer NA means that the paper does not include theoretical results.
458"
THEORY ASSUMPTIONS AND PROOFS,0.6424802110817942,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
459"
THEORY ASSUMPTIONS AND PROOFS,0.6437994722955145,"referenced.
460"
THEORY ASSUMPTIONS AND PROOFS,0.6451187335092349,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
461"
THEORY ASSUMPTIONS AND PROOFS,0.6464379947229552,"• The proofs can either appear in the main paper or the supplemental material, but if
462"
THEORY ASSUMPTIONS AND PROOFS,0.6477572559366754,"they appear in the supplemental material, the authors are encouraged to provide a short
463"
THEORY ASSUMPTIONS AND PROOFS,0.6490765171503958,"proof sketch to provide intuition.
464"
THEORY ASSUMPTIONS AND PROOFS,0.6503957783641161,"• Inversely, any informal proof provided in the core of the paper should be complemented
465"
THEORY ASSUMPTIONS AND PROOFS,0.6517150395778364,"by formal proofs provided in appendix or supplemental material.
466"
THEORY ASSUMPTIONS AND PROOFS,0.6530343007915568,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
467"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6543535620052771,"4. Experimental Result Reproducibility
468"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6556728232189973,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
469"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6569920844327177,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
470"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.658311345646438,"of the paper (regardless of whether the code and data are provided or not)?
471"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6596306068601583,"Answer: [Yes]
472"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6609498680738787,"Justification: Pseudo-code of the proposed algorithms is reported in Subsections 3.1 and 3.2
473"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.662269129287599,"so to make the algorithms reproducible, plus our implementation is made available in the
474"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6635883905013192,"supplementary material. Experiments, including the complete setting, and the respective
475"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6649076517150396,"baselines are described in Section 4.
476"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6662269129287599,"Guidelines:
477"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6675461741424802,"• The answer NA means that the paper does not include experiments.
478"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6688654353562006,"• If the paper includes experiments, a No answer to this question will not be perceived
479"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6701846965699209,"well by the reviewers: Making the paper reproducible is important, regardless of
480"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6715039577836411,"whether the code and data are provided or not.
481"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6728232189973615,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
482"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6741424802110818,"to make their results reproducible or verifiable.
483"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6754617414248021,"• Depending on the contribution, reproducibility can be accomplished in various ways.
484"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6767810026385225,"For example, if the contribution is a novel architecture, describing the architecture fully
485"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6781002638522428,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
486"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.679419525065963,"be necessary to either make it possible for others to replicate the model with the same
487"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6807387862796834,"dataset, or provide access to the model. In general. releasing code and data is often
488"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6820580474934037,"one good way to accomplish this, but reproducibility can also be provided via detailed
489"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.683377308707124,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
490"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6846965699208444,"of a large language model), releasing of a model checkpoint, or other means that are
491"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6860158311345647,"appropriate to the research performed.
492"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6873350923482849,"• While NeurIPS does not require releasing code, the conference does require all submis-
493"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6886543535620053,"sions to provide some reasonable avenue for reproducibility, which may depend on the
494"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6899736147757256,"nature of the contribution. For example
495"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6912928759894459,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
496"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6926121372031663,"to reproduce that algorithm.
497"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6939313984168866,"(b) If the contribution is primarily a new model architecture, the paper should describe
498"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6952506596306068,"the architecture clearly and fully.
499"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6965699208443272,"(c) If the contribution is a new model (e.g., a large language model), then there should
500"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6978891820580475,"either be a way to access this model for reproducing the results or a way to reproduce
501"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6992084432717678,"the model (e.g., with an open-source dataset or instructions for how to construct
502"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7005277044854882,"the dataset).
503"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7018469656992085,"(d) We recognize that reproducibility may be tricky in some cases, in which case
504"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7031662269129287,"authors are welcome to describe the particular way they provide for reproducibility.
505"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7044854881266491,"In the case of closed-source models, it may be that access to the model is limited in
506"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7058047493403694,"some way (e.g., to registered users), but it should be possible for other researchers
507"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7071240105540897,"to have some path to reproducing or verifying the results.
508"
OPEN ACCESS TO DATA AND CODE,0.7084432717678101,"5. Open access to data and code
509"
OPEN ACCESS TO DATA AND CODE,0.7097625329815304,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
510"
OPEN ACCESS TO DATA AND CODE,0.7110817941952506,"tions to faithfully reproduce the main experimental results, as described in supplemental
511"
OPEN ACCESS TO DATA AND CODE,0.712401055408971,"material?
512"
OPEN ACCESS TO DATA AND CODE,0.7137203166226913,"Answer: [Yes]
513"
OPEN ACCESS TO DATA AND CODE,0.7150395778364116,"Justification: All experiments are made reproducible through scripts provided as supplemen-
514"
OPEN ACCESS TO DATA AND CODE,0.716358839050132,"tary material.
515"
OPEN ACCESS TO DATA AND CODE,0.7176781002638523,"Guidelines:
516"
OPEN ACCESS TO DATA AND CODE,0.7189973614775725,"• The answer NA means that paper does not include experiments requiring code.
517"
OPEN ACCESS TO DATA AND CODE,0.7203166226912929,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
518"
OPEN ACCESS TO DATA AND CODE,0.7216358839050132,"public/guides/CodeSubmissionPolicy) for more details.
519"
OPEN ACCESS TO DATA AND CODE,0.7229551451187335,"• While we encourage the release of code and data, we understand that this might not be
520"
OPEN ACCESS TO DATA AND CODE,0.7242744063324539,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
521"
OPEN ACCESS TO DATA AND CODE,0.7255936675461742,"including code, unless this is central to the contribution (e.g., for a new open-source
522"
OPEN ACCESS TO DATA AND CODE,0.7269129287598944,"benchmark).
523"
OPEN ACCESS TO DATA AND CODE,0.7282321899736148,"• The instructions should contain the exact command and environment needed to run to
524"
OPEN ACCESS TO DATA AND CODE,0.7295514511873351,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
525"
OPEN ACCESS TO DATA AND CODE,0.7308707124010554,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
526"
OPEN ACCESS TO DATA AND CODE,0.7321899736147758,"• The authors should provide instructions on data access and preparation, including how
527"
OPEN ACCESS TO DATA AND CODE,0.7335092348284961,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
528"
OPEN ACCESS TO DATA AND CODE,0.7348284960422163,"• The authors should provide scripts to reproduce all experimental results for the new
529"
OPEN ACCESS TO DATA AND CODE,0.7361477572559367,"proposed method and baselines. If only a subset of experiments are reproducible, they
530"
OPEN ACCESS TO DATA AND CODE,0.737467018469657,"should state which ones are omitted from the script and why.
531"
OPEN ACCESS TO DATA AND CODE,0.7387862796833773,"• At submission time, to preserve anonymity, the authors should release anonymized
532"
OPEN ACCESS TO DATA AND CODE,0.7401055408970977,"versions (if applicable).
533"
OPEN ACCESS TO DATA AND CODE,0.741424802110818,"• Providing as much information as possible in supplemental material (appended to the
534"
OPEN ACCESS TO DATA AND CODE,0.7427440633245382,"paper) is recommended, but including URLs to data and code is permitted.
535"
OPEN ACCESS TO DATA AND CODE,0.7440633245382586,"6. Experimental Setting/Details
536"
OPEN ACCESS TO DATA AND CODE,0.7453825857519789,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
537"
OPEN ACCESS TO DATA AND CODE,0.7467018469656992,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
538"
OPEN ACCESS TO DATA AND CODE,0.7480211081794196,"results?
539"
OPEN ACCESS TO DATA AND CODE,0.7493403693931399,"Answer: [Yes]
540"
OPEN ACCESS TO DATA AND CODE,0.7506596306068601,"Justification: All details are provided in Section 4: details of the analyzed architectures,
541"
OPEN ACCESS TO DATA AND CODE,0.7519788918205804,"number of iterations of the SiMEC/SiMExp algorithms, technical infrastructure on which
542"
OPEN ACCESS TO DATA AND CODE,0.7532981530343008,"the experiments were performed, amount of data the experiments were performed on.
543"
OPEN ACCESS TO DATA AND CODE,0.7546174142480211,"Guidelines:
544"
OPEN ACCESS TO DATA AND CODE,0.7559366754617414,"• The answer NA means that the paper does not include experiments.
545"
OPEN ACCESS TO DATA AND CODE,0.7572559366754618,"• The experimental setting should be presented in the core of the paper to a level of detail
546"
OPEN ACCESS TO DATA AND CODE,0.758575197889182,"that is necessary to appreciate the results and make sense of them.
547"
OPEN ACCESS TO DATA AND CODE,0.7598944591029023,"• The full details can be provided either with the code, in appendix, or as supplemental
548"
OPEN ACCESS TO DATA AND CODE,0.7612137203166227,"material.
549"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.762532981530343,"7. Experiment Statistical Significance
550"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7638522427440633,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
551"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7651715039577837,"information about the statistical significance of the experiments?
552"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7664907651715039,"Answer: [Yes]
553"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7678100263852242,"Justification: The standard deviation is reported for the experiment that supports the claims
554"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7691292875989446,"of the paper, i.e. the one on feature importance-based explanations, in Section 4. Stan-
555"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7704485488126649,"dard deviation is also reported for the number of uninformative images produced by the
556"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7717678100263852,"perturbation-based baseline method in the Input space exploration experiment.
557"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7730870712401056,"Guidelines:
558"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7744063324538258,"• The answer NA means that the paper does not include experiments.
559"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7757255936675461,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
560"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7770448548812665,"dence intervals, or statistical significance tests, at least for the experiments that support
561"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7783641160949868,"the main claims of the paper.
562"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7796833773087071,"• The factors of variability that the error bars are capturing should be clearly stated (for
563"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7810026385224275,"example, train/test split, initialization, random drawing of some parameter, or overall
564"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7823218997361477,"run with given experimental conditions).
565"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.783641160949868,"• The method for calculating the error bars should be explained (closed form formula,
566"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7849604221635884,"call to a library function, bootstrap, etc.)
567"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7862796833773087,"• The assumptions made should be given (e.g., Normally distributed errors).
568"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.787598944591029,"• It should be clear whether the error bar is the standard deviation or the standard error
569"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7889182058047494,"of the mean.
570"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7902374670184696,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
571"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7915567282321899,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
572"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7928759894459103,"of Normality of errors is not verified.
573"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7941952506596306,"• For asymmetric distributions, the authors should be careful not to show in tables or
574"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7955145118733509,"figures symmetric error bars that would yield results that are out of range (e.g. negative
575"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7968337730870713,"error rates).
576"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7981530343007915,"• If error bars are reported in tables or plots, The authors should explain in the text how
577"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7994722955145118,"they were calculated and reference the corresponding figures or tables in the text.
578"
EXPERIMENTS COMPUTE RESOURCES,0.8007915567282322,"8. Experiments Compute Resources
579"
EXPERIMENTS COMPUTE RESOURCES,0.8021108179419525,"Question: For each experiment, does the paper provide sufficient information on the com-
580"
EXPERIMENTS COMPUTE RESOURCES,0.8034300791556728,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
581"
EXPERIMENTS COMPUTE RESOURCES,0.8047493403693932,"the experiments?
582"
EXPERIMENTS COMPUTE RESOURCES,0.8060686015831134,"Answer: [Yes]
583"
EXPERIMENTS COMPUTE RESOURCES,0.8073878627968337,"Justification: All the experiments were performed on the same infrastructure, which is
584"
EXPERIMENTS COMPUTE RESOURCES,0.8087071240105541,"reported in a footnote in Section 4. Time of execution is one of the key indicators reported
585"
EXPERIMENTS COMPUTE RESOURCES,0.8100263852242744,"for the Input space exploration experiments. More computing power would be required for
586"
EXPERIMENTS COMPUTE RESOURCES,0.8113456464379947,"experiments on bigger Transformer models.
587"
EXPERIMENTS COMPUTE RESOURCES,0.8126649076517151,"Guidelines:
588"
EXPERIMENTS COMPUTE RESOURCES,0.8139841688654353,"• The answer NA means that the paper does not include experiments.
589"
EXPERIMENTS COMPUTE RESOURCES,0.8153034300791556,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
590"
EXPERIMENTS COMPUTE RESOURCES,0.816622691292876,"or cloud provider, including relevant memory and storage.
591"
EXPERIMENTS COMPUTE RESOURCES,0.8179419525065963,"• The paper should provide the amount of compute required for each of the individual
592"
EXPERIMENTS COMPUTE RESOURCES,0.8192612137203166,"experimental runs as well as estimate the total compute.
593"
EXPERIMENTS COMPUTE RESOURCES,0.820580474934037,"• The paper should disclose whether the full research project required more compute
594"
EXPERIMENTS COMPUTE RESOURCES,0.8218997361477572,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
595"
EXPERIMENTS COMPUTE RESOURCES,0.8232189973614775,"didn’t make it into the paper).
596"
CODE OF ETHICS,0.8245382585751979,"9. Code Of Ethics
597"
CODE OF ETHICS,0.8258575197889182,"Question: Does the research conducted in the paper conform, in every respect, with the
598"
CODE OF ETHICS,0.8271767810026385,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
599"
CODE OF ETHICS,0.8284960422163589,"Answer: [Yes]
600"
CODE OF ETHICS,0.8298153034300791,"Justification: We comply with the terms of use of the datasets employed in the experiments,
601"
CODE OF ETHICS,0.8311345646437994,"and we deem our work has no potentially harmful effect on people safety, security, discrimi-
602"
CODE OF ETHICS,0.8324538258575198,"nation, surveillance, harassment, nor on human rights. Our proposal does not contribute to
603"
CODE OF ETHICS,0.8337730870712401,"spread bias and unfairness towards certain groups of people nor to harm the environment.
604"
CODE OF ETHICS,0.8350923482849604,"Guidelines:
605"
CODE OF ETHICS,0.8364116094986808,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
606"
CODE OF ETHICS,0.837730870712401,"• If the authors answer No, they should explain the special circumstances that require a
607"
CODE OF ETHICS,0.8390501319261213,"deviation from the Code of Ethics.
608"
CODE OF ETHICS,0.8403693931398417,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
609"
CODE OF ETHICS,0.841688654353562,"eration due to laws or regulations in their jurisdiction).
610"
BROADER IMPACTS,0.8430079155672823,"10. Broader Impacts
611"
BROADER IMPACTS,0.8443271767810027,"Question: Does the paper discuss both potential positive societal impacts and negative
612"
BROADER IMPACTS,0.8456464379947229,"societal impacts of the work performed?
613"
BROADER IMPACTS,0.8469656992084432,"Answer: [No]
614"
BROADER IMPACTS,0.8482849604221636,"Justification: Although the impacts of XAI on society is broad and deep, in this paper we
615"
BROADER IMPACTS,0.8496042216358839,"focus only on the technical problem of exploring the equivalence classes in the input space
616"
BROADER IMPACTS,0.8509234828496042,"of Transformers, which doesn’t add any specific impact to the discussion about XAI in
617"
BROADER IMPACTS,0.8522427440633246,"general.
618"
BROADER IMPACTS,0.8535620052770448,"Guidelines:
619"
BROADER IMPACTS,0.8548812664907651,"• The answer NA means that there is no societal impact of the work performed.
620"
BROADER IMPACTS,0.8562005277044855,"• If the authors answer NA or No, they should explain why their work has no societal
621"
BROADER IMPACTS,0.8575197889182058,"impact or why the paper does not address societal impact.
622"
BROADER IMPACTS,0.8588390501319261,"• Examples of negative societal impacts include potential malicious or unintended uses
623"
BROADER IMPACTS,0.8601583113456465,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
624"
BROADER IMPACTS,0.8614775725593667,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
625"
BROADER IMPACTS,0.862796833773087,"groups), privacy considerations, and security considerations.
626"
BROADER IMPACTS,0.8641160949868074,"• The conference expects that many papers will be foundational research and not tied
627"
BROADER IMPACTS,0.8654353562005277,"to particular applications, let alone deployments. However, if there is a direct path to
628"
BROADER IMPACTS,0.866754617414248,"any negative applications, the authors should point it out. For example, it is legitimate
629"
BROADER IMPACTS,0.8680738786279684,"to point out that an improvement in the quality of generative models could be used to
630"
BROADER IMPACTS,0.8693931398416886,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
631"
BROADER IMPACTS,0.8707124010554089,"that a generic algorithm for optimizing neural networks could enable people to train
632"
BROADER IMPACTS,0.8720316622691293,"models that generate Deepfakes faster.
633"
BROADER IMPACTS,0.8733509234828496,"• The authors should consider possible harms that could arise when the technology is
634"
BROADER IMPACTS,0.8746701846965699,"being used as intended and functioning correctly, harms that could arise when the
635"
BROADER IMPACTS,0.8759894459102903,"technology is being used as intended but gives incorrect results, and harms following
636"
BROADER IMPACTS,0.8773087071240105,"from (intentional or unintentional) misuse of the technology.
637"
BROADER IMPACTS,0.8786279683377308,"• If there are negative societal impacts, the authors could also discuss possible mitigation
638"
BROADER IMPACTS,0.8799472295514512,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
639"
BROADER IMPACTS,0.8812664907651715,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
640"
BROADER IMPACTS,0.8825857519788918,"feedback over time, improving the efficiency and accessibility of ML).
641"
SAFEGUARDS,0.8839050131926122,"11. Safeguards
642"
SAFEGUARDS,0.8852242744063324,"Question: Does the paper describe safeguards that have been put in place for responsible
643"
SAFEGUARDS,0.8865435356200527,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
644"
SAFEGUARDS,0.8878627968337731,"image generators, or scraped datasets)?
645"
SAFEGUARDS,0.8891820580474934,"Answer: [NA]
646"
SAFEGUARDS,0.8905013192612137,"Justification: The paper poses no such risks.
647"
SAFEGUARDS,0.8918205804749341,"Guidelines:
648"
SAFEGUARDS,0.8931398416886543,"• The answer NA means that the paper poses no such risks.
649"
SAFEGUARDS,0.8944591029023746,"• Released models that have a high risk for misuse or dual-use should be released with
650"
SAFEGUARDS,0.895778364116095,"necessary safeguards to allow for controlled use of the model, for example by requiring
651"
SAFEGUARDS,0.8970976253298153,"that users adhere to usage guidelines or restrictions to access the model or implementing
652"
SAFEGUARDS,0.8984168865435356,"safety filters.
653"
SAFEGUARDS,0.899736147757256,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
654"
SAFEGUARDS,0.9010554089709762,"should describe how they avoided releasing unsafe images.
655"
SAFEGUARDS,0.9023746701846965,"• We recognize that providing effective safeguards is challenging, and many papers do
656"
SAFEGUARDS,0.9036939313984169,"not require this, but we encourage authors to take this into account and make a best
657"
SAFEGUARDS,0.9050131926121372,"faith effort.
658"
LICENSES FOR EXISTING ASSETS,0.9063324538258575,"12. Licenses for existing assets
659"
LICENSES FOR EXISTING ASSETS,0.9076517150395779,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
660"
LICENSES FOR EXISTING ASSETS,0.9089709762532981,"the paper, properly credited and are the license and terms of use explicitly mentioned and
661"
LICENSES FOR EXISTING ASSETS,0.9102902374670184,"properly respected?
662"
LICENSES FOR EXISTING ASSETS,0.9116094986807388,"Answer: [Yes]
663"
LICENSES FOR EXISTING ASSETS,0.9129287598944591,"Justification: The datasets used in the paper are explicitely mentioned in the references, as
664"
LICENSES FOR EXISTING ASSETS,0.9142480211081794,"required by the terms of use. Where applicable, the license is also reported.
665"
LICENSES FOR EXISTING ASSETS,0.9155672823218998,"Guidelines:
666"
LICENSES FOR EXISTING ASSETS,0.91688654353562,"• The answer NA means that the paper does not use existing assets.
667"
LICENSES FOR EXISTING ASSETS,0.9182058047493403,"• The authors should cite the original paper that produced the code package or dataset.
668"
LICENSES FOR EXISTING ASSETS,0.9195250659630607,"• The authors should state which version of the asset is used and, if possible, include a
669"
LICENSES FOR EXISTING ASSETS,0.920844327176781,"URL.
670"
LICENSES FOR EXISTING ASSETS,0.9221635883905013,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
671"
LICENSES FOR EXISTING ASSETS,0.9234828496042217,"• For scraped data from a particular source (e.g., website), the copyright and terms of
672"
LICENSES FOR EXISTING ASSETS,0.924802110817942,"service of that source should be provided.
673"
LICENSES FOR EXISTING ASSETS,0.9261213720316622,"• If assets are released, the license, copyright information, and terms of use in the
674"
LICENSES FOR EXISTING ASSETS,0.9274406332453826,"package should be provided. For popular datasets, paperswithcode.com/datasets
675"
LICENSES FOR EXISTING ASSETS,0.9287598944591029,"has curated licenses for some datasets. Their licensing guide can help determine the
676"
LICENSES FOR EXISTING ASSETS,0.9300791556728232,"license of a dataset.
677"
LICENSES FOR EXISTING ASSETS,0.9313984168865436,"• For existing datasets that are re-packaged, both the original license and the license of
678"
LICENSES FOR EXISTING ASSETS,0.9327176781002638,"the derived asset (if it has changed) should be provided.
679"
LICENSES FOR EXISTING ASSETS,0.9340369393139841,"• If this information is not available online, the authors are encouraged to reach out to
680"
LICENSES FOR EXISTING ASSETS,0.9353562005277045,"the asset’s creators.
681"
NEW ASSETS,0.9366754617414248,"13. New Assets
682"
NEW ASSETS,0.9379947229551451,"Question: Are new assets introduced in the paper well documented and is the documentation
683"
NEW ASSETS,0.9393139841688655,"provided alongside the assets?
684"
NEW ASSETS,0.9406332453825857,"Answer: [NA]
685"
NEW ASSETS,0.941952506596306,"Justification: The paper does not release new assets.
686"
NEW ASSETS,0.9432717678100264,"Guidelines:
687"
NEW ASSETS,0.9445910290237467,"• The answer NA means that the paper does not release new assets.
688"
NEW ASSETS,0.945910290237467,"• Researchers should communicate the details of the dataset/code/model as part of their
689"
NEW ASSETS,0.9472295514511874,"submissions via structured templates. This includes details about training, license,
690"
NEW ASSETS,0.9485488126649076,"limitations, etc.
691"
NEW ASSETS,0.9498680738786279,"• The paper should discuss whether and how consent was obtained from people whose
692"
NEW ASSETS,0.9511873350923483,"asset is used.
693"
NEW ASSETS,0.9525065963060686,"• At submission time, remember to anonymize your assets (if applicable). You can either
694"
NEW ASSETS,0.9538258575197889,"create an anonymized URL or include an anonymized zip file.
695"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9551451187335093,"14. Crowdsourcing and Research with Human Subjects
696"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9564643799472295,"Question: For crowdsourcing experiments and research with human subjects, does the paper
697"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9577836411609498,"include the full text of instructions given to participants and screenshots, if applicable, as
698"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9591029023746702,"well as details about compensation (if any)?
699"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9604221635883905,"Answer: [NA]
700"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9617414248021108,"Justification: Our work doesn’t include crowdsourcing nor research with human subjects.
701"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9630606860158312,"Guidelines:
702"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9643799472295514,"• The answer NA means that the paper does not involve crowdsourcing nor research with
703"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9656992084432717,"human subjects.
704"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670184696569921,"• Including this information in the supplemental material is fine, but if the main contribu-
705"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9683377308707124,"tion of the paper involves human subjects, then as much detail as possible should be
706"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9696569920844327,"included in the main paper.
707"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9709762532981531,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
708"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722955145118733,"or other labor should be paid at least the minimum wage in the country of the data
709"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9736147757255936,"collector.
710"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974934036939314,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
711"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762532981530343,"Subjects
712"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775725593667546,"Question: Does the paper describe potential risks incurred by study participants, whether
713"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978891820580475,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
714"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802110817941952,"approvals (or an equivalent approval/review based on the requirements of your country or
715"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815303430079155,"institution) were obtained?
716"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828496042216359,"Answer: [NA]
717"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841688654353562,"Justification: Our work does not involve crowdsourcing nor research with human subjects.
718"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854881266490765,"Guidelines:
719"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868073878627969,"• The answer NA means that the paper does not involve crowdsourcing nor research with
720"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881266490765171,"human subjects.
721"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894459102902374,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
722"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907651715039578,"may be required for any human subjects research. If you obtained IRB approval, you
723"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920844327176781,"should clearly state this in the paper.
724"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934036939313984,"• We recognize that the procedures for this may vary significantly between institutions
725"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947229551451188,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
726"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996042216358839,"guidelines for their institution.
727"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973614775725593,"• For initial submissions, do not include any information that would break anonymity (if
728"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986807387862797,"applicable), such as the institution conducting the review.
729"
