Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016129032258064516,"The generalization capacity of various machine learning models exhibits different
phenomena in the under- and over-parameterized regimes. In this paper, we focus
on regression models such as feature regression and kernel regression and ana-
lyze a generalized weighted least-squares optimization method for computational
learning and inversion with noisy data. The highlight of the proposed framework
is that we allow weighting in both the parameter space and the data space. The
weighting scheme encodes both a priori knowledge on the object to be learned and
a strategy to weight the contribution of different data points in the loss function.
Here, we characterize the impact of the weighting scheme on the generalization
error of the learning method, where we derive explicit generalization errors for the
random Fourier feature model in both the under- and over-parameterized regimes.
For more general feature maps, error bounds are provided based on the singular
values of the feature matrix. We demonstrate that appropriate weighting from
prior knowledge can improve the generalization capability of the learned model."
INTRODUCTION,0.0032258064516129032,"1
INTRODUCTION"
INTRODUCTION,0.004838709677419355,"Given N data pairs {xj, yj}N
j=1, where xj ∈R, yj ∈C, j = 1, . . . , N, we are interested in learning
a random Fourier feature (RFF) model (Rahimi & Recht, 2008; Liao et al., 2020; Xie et al., 2020)"
INTRODUCTION,0.0064516129032258064,fθ(x) =
INTRODUCTION,0.008064516129032258,"P −1
X"
INTRODUCTION,0.00967741935483871,"k=0
θkeikx, x ∈[0, 2π],
(1)"
INTRODUCTION,0.01129032258064516,"where P ∈N is a given positive integer and we used the short-hand notation θ := (θ0, · · · , θP −1)T
with the superscript T denoting the transpose operation."
INTRODUCTION,0.012903225806451613,"This exact model as well as its generalization to more complicated setups have been extensively
studied; see for instance Liao & Couillet (2018); Shahrampour & Kolouri (2019); d’Ascoli et al.
(2020); Li et al. (2020); ¨Ozcelikkale (2020); Liu et al. (2020; 2021) and references therein. While
this model may seem to be overly simpliﬁed from a practical perspective for many real-world appli-
cations, it serves as a prototype for theoretical understandings of different phenomena in machine
learning models (Sriperumbudur & Szabo, 2015; Belkin et al., 2020; Li et al., 2021a)."
INTRODUCTION,0.014516129032258065,"A common way to computationally solve this learning problem is to reformulate it as an optimization
problem where we ﬁnd θ by minimizing the model and data mismatch for a given dataset. In
this paper, we assume that the training data are collected on a uniform grid of x over the domain
[0, 2π]. That is, {xj = 2πj"
INTRODUCTION,0.016129032258064516,"N }N−1
j=0 . Let ωN = exp( 2πi"
INTRODUCTION,0.017741935483870968,"N ) where i is the imaginary unit. We introduce
Ψ ∈CN×P to be the feature matrix with elements"
INTRODUCTION,0.01935483870967742,"(Ψ)jk = (ωN)jk,
0 ≤j ≤N −1, 0 ≤k ≤P −1 ."
INTRODUCTION,0.020967741935483872,"Based on the form of fθ(x) in (1), we can then write the 2-norm based data mismatch into the form
PN−1
j=0 |fθ(xj) −yj|2 = ∥Ψθ −y∥2
2 where the column data vector y = (y0, · · · , yN−1)T. The
learning problem is therefore recast as a least-squares optimization problem of the form"
INTRODUCTION,0.02258064516129032,"bθ = arg min
θ
∥Ψθ −y∥2
2,
(2)"
INTRODUCTION,0.024193548387096774,Published as a conference paper at ICLR 2022
INTRODUCTION,0.025806451612903226,"assuming that a minimizer does exist, especially when we restrict θ to an appropriate space."
INTRODUCTION,0.027419354838709678,"In a general feature regression problem, the Fourier feature {eikx}P −1
k=0 is then replaced with a dif-
ferent feature model {ϕk(x)}P −1
k=0 , while the least-squares form (2) remains unchanged except that
the entries of the matrix Ψ is now Ψjk = ϕk(xj). We emphasize that this type of generalization will
be discussed in Section 5. Moreover, we remark that this least-squares optimization formulation is
a classical computational inversion tool in solving the general linear inverse problems of the form
Ψθ = y; see for instance Engl et al. (1996); Tarantola (2005) and references therein."
INTRODUCTION,0.02903225806451613,"Previous work on weighted optimization for feature and kernel learning. Xie et al. (2020) stud-
ied the ﬁtting problem for this model under the assumption that the coefﬁcient vector θ is sampled
from a distribution with the property that γ is a positive constant,"
INTRODUCTION,0.03064516129032258,"Eθ[θ] = 0, Eθ[θθ∗] = cγΛ−2γ
[P ] ,
(3)"
INTRODUCTION,0.03225806451612903,"where the superscript ∗denotes the Hermitian transpose and the diagonal matrix Λ[P ] has diagonal
elements (Λ[P ])kk = tk = 1 + k, k ≥0. That is,"
INTRODUCTION,0.03387096774193549,"Λ[P ] = diag{t0, t1, t2, . . . , tk, . . . , tP −1},
tk := 1 + k .
(4)"
INTRODUCTION,0.035483870967741936,"The subscript [P] indicates that Λ[P ] is a diagonal submatrix of Λ that contains its element indexed
in the set [P] := {0, 1, · · · , P −1}. The normalization constant cγ = 1/(PP −1
k=0 (1 + k)−2γ) is
only selected so that Eθ[∥θ∥2] = 1. It does not play a signiﬁcant role in the rest of the paper."
INTRODUCTION,0.037096774193548385,"The main assumption in (3) says that statistically, the signal to be recovered has algebraically decay-
ing Fourier coefﬁcients. This is simply saying that the target function we are learning is relatively
smooth, which is certainly the case for many functions as physical models in practical applications."
INTRODUCTION,0.03870967741935484,"It was shown in Xie et al. (2020) that, to learn a model with p ≤P features, it is advantageous to
use the following weighted least-squares formulation"
INTRODUCTION,0.04032258064516129,"bθp = Λ−β
[p] bw, with bw = arg minθ ∥Ψ[N×p]Λ−β
[p] w −y∥2
2,
(5)"
INTRODUCTION,0.041935483870967745,"when the learning problem is overparameterized, i.e., p > N. Here, Ψ[N×p] ∈CN×p is the matrix
containing the ﬁrst p columns of Ψ, and β > 0 is some pre-selected exponent that can be different
from the γ in (3). To be more precise, we deﬁne the the generalization error of the learning problem"
INTRODUCTION,0.043548387096774194,"Eβ(P, p, N) := Eθ
h
∥fθ(x) −fbθp(x)∥2
L2([0,2π])
i
= Eθ
h
∥bθp −θ∥2
2
i
,
(6)"
INTRODUCTION,0.04516129032258064,"where the equality comes from the Parseval’s identity, and bθp is understood as the vector
(θT
p , 0, · · · , 0)T so that θ and bθp are of the same length P. The subscript θ in Eθ indicates that
the expectation is taken with respect to the distribution of the random variable θ. It was shown
in Xie et al. (2020) that the lowest generalization error achieved from the weighted least-squares
approach (5) in the overparameterized regime (p > N) is strictly less than the lowest possible gen-
eralization error in the underparameterized regime (p ≤N). This, together with the analysis and
numerical evidence in previous studies such as those in Belkin et al. (2019; 2020), leads to the under-
standing that smoother approximations (i.e., solutions that are dominated by lower Fourier modes)
give better generalization in learning with the RFF model (1)."
INTRODUCTION,0.0467741935483871,"Main contributions of this work. In this work, we analyze a generalized version of (5) for gen-
eral feature regression from noisy data. Following the same notations as before, we introduce the
following weighted least-squares formulation for feature regression:"
INTRODUCTION,0.04838709677419355,"bθ
δ
p = Λ−β
[p] bw, with bw = arg min
w
∥Λ−α
[N]

Ψ[N×p]Λ−β
[p] w −yδ
∥2
2,
(7)"
INTRODUCTION,0.05,"where the superscript δ on y and bθp denotes the fact that the training data contain random noise
of level δ (which will be speciﬁed later). The exponent α is pre-selected and can be different from
β. While sharing similar roles with the weight matrix Λ−β
[P ], the weight matrix Λ−α
[N] provides us
the additional ability to deal with noise in the training data. Moreover, as we will see later, the
weight matrix Λ−α
[N] does not have to be either diagonal or in the same form as the matrix Λ−β
[p] ; the"
INTRODUCTION,0.05161290322580645,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0532258064516129,"current form is to simplify the calculations for the RFF model. It can be chosen based on the a priori
information we have on the operator Ψ as well as the noise distribution of the training data."
INTRODUCTION,0.054838709677419356,"The highlight and also one of the main contributions of our work is that we introduce a new weight
matrix Λ−α
[N] that emphasizes the data mismatch in terms of its various modes, in addition to Λ−β
[p] ,
the weight matrix imposed on the unknown feature coefﬁcient vector θ. This type of generalization
has appeared in different forms in many computational approaches for solving inverse and learning
problems where the standard 2-norm (or ℓ2 in the inﬁnite-dimensional setting) is replaced with a
weighted norm that is either weaker or stronger than the unweighted 2-norm."
INTRODUCTION,0.056451612903225805,"In this paper, we characterize the impact of the new weighted optimization framework (7) on the
generalization capability of various feature regression and kernel regression models. The new con-
tributions of this work are threefold. First, we discuss in detail the generalized weighted least-
squares framework (7) in Section 2 and summarize the main results for training with noise-free data
in Section 3 for the RFF model in both the overparameterized and the underparameterized regimes.
This is the setup considered in Xie et al. (2020), but our analysis is based on the proposed weighted
model (7) instead of (5) as in their work. Second, we provide the generalization error in both two
regimes for the case of training with noisy data; see Section 4. This setup was not considered in Xie
et al. (2020), but we demonstrate here that it is a signiﬁcant advantage of the weighted optimization
when data contains noise since the weighting could effectively minimize the inﬂuence of the noise
and thus improve the stability of feature regression. Third, we extend the same type of results to
more general models in feature regression and kernel regression that are beyond the RFF model,
given that the operator Ψ satisﬁes certain properties. In the general setup presented in Section 5, we
derive error bounds in the asymptotic limit when P, N, and p all become very large. Our analy-
sis provides some guidelines on selecting weighting schemes through either the parameter domain
weighting or the data domain weighting, or both, to emphasize the features of the unknowns to be
learned based on a priori knowledge."
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.05806451612903226,"2
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.05967741935483871,"There are four essential elements in the least-squares formulation of the learning problem: (i) the
parameter to be learned (θ), (ii) the dataset used in the training process (y), (iii) the feature matrix
(Ψ), and (iv) the metric chosen to measure the data mismatch between Ψθ and y."
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.06129032258064516,"Element (i) of the problem is determined not only by the data but also by a priori information
we have. The information encoded in (3) reveals that the size (i.e., the variance) of the Fourier
modes in the RFF model decays as fast as (1 + k)−2γ. Therefore, the low-frequency modes in (1)
dominate high-frequency modes, which implies that in the learning process, we should search for
the solution vectors that have more low-frequency components than the high-frequency components.
The motivation behind introducing the weight matrix Λ−β
[p] in (5) is exactly to force the optimization
algorithm to focus on admissible solutions that are consistent with the a priori knowledge given
in (3), which is to seek θ whose components |θk|2 statistically decay like (1 + k)−2β."
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.06290322580645161,"When the problem is formally determined (i.e., p = N), the operator Ψ is invertible, and the training
data are noise-free, similar to the weight matrix Λ−β
[p] , the weight matrix Λ−α
[N] does not change the
solution of the learning problem. However, as we will see later, these two weight matrices do impact
the solutions in various ways under the practical setups that we are interested in, for instance, when
the problem is over-parameterized or when the training data contain random noise."
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.06451612903225806,"The weight matrix Λ−α
[N] is introduced to handle elements (ii)-(iv) of the learning problem. First,
since Λ−α
[N] is directly applied to the data yδ, it allows us to suppress (when α > 0) or promote
(when α < 0) high-frequency components in the data during the training process. In particular,
when transformed back to the physical space, the weight matrix Λ−α
[N] with α > 0 corresponds
to a smoothing convolutional operator whose kernel has Fourier coefﬁcients decaying at the rate
k−α. This operator suppresses high-frequency information in the data. Second, Λ−α
[N] is also directly
applied to Ψθ. This allows us to precondition the learning problem by making Λ−α
[N]Ψ a better-
conditioned operator (in an appropriate sense) than Ψ, for some applications where the feature matrix
Ψ has certain undesired properties. Finally, since Λ−α
[N] is applied to the residual Ψθ −y, we can"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.06612903225806452,Published as a conference paper at ICLR 2022
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.06774193548387097,"regard the new weighted optimization formulation (7) as the generalization of the classic least-
squares formulation with a new loss function (a weighted norm) measuring the data mismatch."
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.06935483870967742,"Weighting optimization schemes such as (7) have been studied, implicitly or explicitly, in different
settings (Needell et al., 2014; Byrd & Lipton, 2019; Engquist et al., 2020; Li, 2021; Yang et al.,
2021). For instance, if we take β = 0, then we have a case where we rescale the classical least-
squares loss function with the weight Λ−α
[N]. If we take α = 1, then this least-squares functional
is equivalent to the loss function based on the H−1 norm, instead of the usual L2 norm, of the
mismatch between the target function fθ(x) and the learned model fbθ(x). Based on the asymptotic
equivalence between the quadratic Wasserstein metric and the H−1 semi-norm (on an appropriate
functional space), this training problem is asymptotically equivalent to the same training problem
based on a quadratic Wasserstein loss function; see for instance Engquist et al. (2020) for more
detailed illustration on the connection. In the classical statistical inversion setting, Λ2α plays the role
of the covariance matrix of the additive Gaussian random noise in the data (Kaipio & Somersalo,
2005). When the noise is sampled from mean-zero Gaussian distribution with covariance matrix
Λ2α, a standard maximum likelihood estimator (MLE) is often constructed as the minimizer of
(Ψθ −y)∗Λ−2α
[N] (Ψθ −y) = ∥Λ−α
[N](Ψθ −y)∥2
2."
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.07096774193548387,"The exact solution to (7), with X+ denoting the Moore–Penrose inverse of operator X, is given by"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.07258064516129033,"bθ
δ
p = Λ−β
[p]

Λ−α
[N]Ψ[N×p]Λ−β
[p]
+
Λ−α
[N]yδ .
(8)"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.07419354838709677,"In the rest of this paper, we analyze this training result and highlight the impact of the weight
matrices Λ−α
[N] and Λ−β
[N] in different regimes of the learning problem. We reproduce the classical
bias-variance trade-off analysis in the weighted optimization framework. For that purpose, we utilize"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.07580645161290323,"the linearity of the problem to decompose bθ
δ
p as"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.07741935483870968,"bθ
δ
p = Λ−β
[p] (Λ−α
[N]Ψ[N×p]Λ−β
[p] )+Λ−α
[N]y + Λ−β
[p] (Λ−α
[N]Ψ[N×p]Λ−β
[p] )+Λ−α
[N](yδ −y),
(9)"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.07903225806451612,"where the ﬁrst part is simply bθp, the result of learning with noise-free data, while the second part is
the contribution from the additive noise. We deﬁne the generalization error in this case as"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.08064516129032258,"Eδ
α,β(P, p, N) = Eθ,δ
h
∥fθ(x) −fbθ
δ
p(x)∥2
L2([0,2π])
i
= Eθ,δ
h
∥bθ
δ
p −bθp + bθp −θ∥2
2
i
,
(10)"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.08225806451612903,"where the expectation is taken over the joint distribution of θ and the random noise δ. By the stan-
dard triangle inequality, this generalization error is bounded by sum of the generalization error from
training with noise-free data and the error caused by the noise. We will use this simple observation to
bound the generalization errors when no exact formulas can be derived. We also look at the variance
of the generalization error with respect to the random noise, which is"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.08387096774193549,"Varδ(Eθ[∥bθ
δ −θ∥2
2]) := Eδ[(Eθ[∥bθ
δ −θ∥2
2] −Eθ,δ[∥bθ
δ −θ∥2
2])2] .
(11)"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.08548387096774193,"In the rest of the work, we consider two parameter regimes of learning:
(i) In the overparameterized regime, we have the following setup of the parameters:"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.08709677419354839,"N < p ≤P, and, P = µN, p = νN for some µ, ν ∈N s.t. µ ≥ν ≫1 .
(12)"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.08870967741935484,"(ii) In the underparameterized regime, we have the following scaling relations:"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.09032258064516129,"p ≤N ≤P, and, P = µN for some µ ∈N.
(13)"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.09193548387096774,"The formally-determined case of p = N ≤P is included in both the overparameterized and the
underparameterized regimes. We make the following assumptions throughout the work:"
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.0935483870967742,"(A-I) The random noise δ in the training data is additive in the sense that yδ = y + δ.
(A-II) The random vectors δ and θ are independent.
(A-III) The random noise δ ∼N(0, σI[P ]) for some constant σ > 0."
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.09516129032258064,"While assumptions (A-I) and (A-II) are essential, assumption (A-III) is only needed to simplify the
calculations. Most of the results we obtain in this paper can be reproduced straightforwardly for the
random noise δ with any well-deﬁned covariance matrix."
GENERALIZED WEIGHTED LEAST-SQUARES FORMULATION,0.0967741935483871,Published as a conference paper at ICLR 2022
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.09838709677419355,"3
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.1,"We start with the problem of training with noise-free data. In this case, we utilize tools developed
in Belkin et al. (2020) and Xie et al. (2020) to derive exact generalization errors. Our main objective
is to compare the difference and similarity of the roles of the weight matrices Λ−α
[N] and Λ−β
[p] ."
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.10161290322580645,"We have the following results on the generalization error. The proof is in Appendix A.1 and A.2.
Theorem 3.1 (Training with noise-free data). Let δ = 0, and θ be sampled with the properties
in (3). Then the generalization error in the overparameterized regime (12) is:"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.1032258064516129,"E0
α,β(P, p, N) = 1 −2cγ N−1
X k=0"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.10483870967741936,"Pν−1
η=0 t−2β−2γ
k+Nη
Pν−1
η=0 t−2β
k+Nη
+ cγ N−1
X k=0"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.1064516129032258,"(Pν−1
η=0 t−4β
k+Nη)(Pµ−1
η=0 t−2γ
k+Nη)"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.10806451612903226,"(Pν−1
η=0 t−2β
k+Nη)2
,
(14)"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.10967741935483871,and the generalization error in the underparameterized regime (13) is:
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.11129032258064517,"E0
α,β(P, p, N) = cγ"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.11290322580645161,"P −1
X"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.11451612903225807,"j=N
t−2γ
j
+ cγ p−1
X k=0 µ−1
X"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.11612903225806452,"η=1
t−2γ
k+Nη −cγ N−1
X k=p µ−1
X"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.11774193548387096,"η=1
t−2γ
k+Nη + N"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.11935483870967742,"N−p−1
X i,j=0"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.12096774193548387,"ee(N)
ij be(N)
ji
ΣiiΣjj
, (15)"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.12258064516129032,"where {tj}P −1
j=0 and cγ are those introduced in (3) and (4), while"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.12419354838709677,"ee(N)
ij
= PN−1
k=0 t2α
k U kiUkj,
be(N)
ij
= PN−p−1
k′=0
(cγt−2γ
p+k′ +χp+k′)V ik′Vjk′,
0 ≤i, j ≤N −p−1 ,"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.12580645161290321,"with UΣV∗being the singular value decomposition of Λα
[N]Ψ[N]\[p] and {χm}N−1
m=0 deﬁned as"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.12741935483870967,"χm = PN−1
k=0
 Pµ−1
η=1 t−2γ
k+Nη
 1"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.12903225806451613,"N
PN−1
j=0 ω(m−k)j
N

, 0 ≤m ≤N −1 ."
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.13064516129032258,"We want to emphasize that the generalization errors we obtained in Theorem 3.1 are for the weighted
optimization formulation (7) where we have an additional weight matrix Λ−α
[N] compared to the for-
mulation in Xie et al. (2020), even though our results look similar to the previous results of Belkin
et al. (2020) and Xie et al. (2020). Moreover, we kept the weight matrix Λ−β
[p] in the underparame-
terized regime, which is different from the setup in Xie et al. (2020) where the same weight matrix
was removed in this regime. The reason for keeping Λ−β
[p] in the underparameterized regime will
become more obvious in the case of training with noisy data, as we will see in the next section."
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.13225806451612904,"Here are some key observations from the above results, which, we emphasize again, are obtained
in the setting where the optimization problems are solved exactly, and the data involved contain no
random noise. The conclusion will differ when data contain random noise or when optimization
problems cannot be solved exactly."
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.1338709677419355,"First, the the weight matrix Λ−β
[p] only matters in the overparameterized regime while Λ−α
[N] only"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.13548387096774195,"matters in the underparameterized regime. In the overparameterized regime, the weight Λ−β
[p] forces
the inversion procedure to focus on solutions that are biased toward the low-frequency modes. In
the underparameterized regime, the matrix Λ−α
[N] re-weights the frequency content of the “residual”"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.13709677419354838,"(data mismatch) before it is backprojected into the learned parameter bθ. Using the scaling P = µN
and p = νN in the overparameterized regime, and the deﬁnition of cγ, we can verify that when
α = β = 0, the generalization error reduces to"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.13870967741935483,"E0
0,0(P, p, N) = 1 −N"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.1403225806451613,p + 2N
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.14193548387096774,"p cγ
PP −1
j=p t−2γ
j
= 1 + N p −2N"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.1435483870967742,"p cγ
Pp−1
j=0 t−2γ
j
.
(16)"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.14516129032258066,"This is given in Xie et al. (2020, Theorem 1)."
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.14677419354838708,"Second, when the learning problem is formally determined, i.e., when p = N, neither weight matri-
ces play a role when the training data are generated from the true model with no additional random
noise and the minimizer can be found exactly. The generalization error simpliﬁes to"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.14838709677419354,"E0
α,β(P, p, N) = 2cγ
PP −1
j=p t−2γ
j
.
(17)"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.15,Published as a conference paper at ICLR 2022
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.15161290322580645,Singular value index k
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.1532258064516129,"(a) singular values of Λ−α
[N]Ψ[N]\[p]
(b) double-descent curves with different α"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.15483870967741936,"Figure 1: Left: singular values of Λ−α
[N]Ψ[N]\[p] for a system with N = 1024 and p = 512. Shown are
singular values for α = 0, α = 0.4, α = 0.6, α = 0.8, and α = 1.0; Right: double-descent curves
for the generalization error E0
α,β for the cases of N = 64, γ = 0.3, β = 0.3, and α = 0, 0.3, 0.8."
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.15645161290322582,"This is not surprising as, in this case, Ψ is invertible (because it is a unitary matrix scaled by the
constant N). The true solution to Ψθ = y is simply θ = Ψ−1y. The weight matrices in the
optimization problem are invertible and therefore do not change the true solution of the problem.
The generalization error, in this case, is therefore only due to the Fourier modes that are not learned
from the training data, i.e., modes p to P −1."
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.15806451612903225,"While it is obvious from the formulas for the generalization error that the weight matrix Λ−α
[N] indeed
plays a role in the underparameterized regime, we show in Figure 1a the numerical calculation of
the singular value decomposition of the matrix Λα
[N]Ψ[N]\[p] for the case of (N, p) = (1024, 512).
The impact of α can be seen by comparing the singular values to their correspondence in the α = 0
case (where all the singular values are the same and equal to
√"
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.1596774193548387,"N). When the system size is large
(in this case N = 1024), even a small α (e.g., α = 0.4) can signiﬁcantly impact the result. In
Figure 1b, we plot the theoretical prediction of E0
α,β in Theorem 3.1 to demonstrate the double-
descent phenomenon observed in the literature on the RFF model. We emphasize again that in this
particular noise-free setup with perfectly solved minimization problem by the pseudoinverse, Λ−α
[N]
only plays a role in the underparameterized regime as can be seen from the double-descent curves."
GENERALIZATION ERROR FOR TRAINING WITH NOISE-FREE DATA,0.16129032258064516,"Selecting p to minimize generalization error.
It is clear (and also expected) from (16) and (17)
that, in the cases of p = N or α = β = γ = 0, the generalization error decreases monotonically
with respect to the number of modes learned in the training process. One should then learn as many
Fourier coefﬁcients as possible. When p ̸= N or α, β ̸= 0, E0
α,β(P, p, N), for ﬁxed P and N, does
not change monotonically with respect to p anymore. In such a situation, we can choose the p values
that minimize the generalization error and perform learning with these p values."
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.1629032258064516,"4
ERROR BOUNDS FOR TRAINING WITH NOISY DATA"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.16451612903225807,"In this section, we study the more realistic setting of training with noisy data. The common practice
is that when we solve the minimization problem in such a case, we should avoid overﬁtting the model
to the data by stopping the optimization algorithm early at an appropriate level of values depending
on the noise level in the data, but see Bartlett et al. (2020); Li et al. (2021b) for some analysis in the
direction of “benign overﬁtting”. When training with noisy data, the impact of the weight matrices
Λ−α
[N] and Λ−β
[p] on the generalization error becomes more obvious. In fact, both weight matrices play
non-negligible roles in the overparameterized and the underparameterized regimes, respectively."
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.16612903225806452,"We start with the circumstance where we still match the data perfectly for each realization of the
noise in the data. The result is summarized as follows."
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.16774193548387098,Published as a conference paper at ICLR 2022
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.1693548387096774,"Lemma 4.1 (Training with noisy data: exact error). Under the assumptions (A-I)-(A-III), the gen-
eralization error Eδ
α,β(P, p, N) is given as"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.17096774193548386,"Eδ
α,β(P, p, N) = E0
α,β(P, p, N) + Enoise(P, p, N) ,"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.17258064516129032,"where E0
α,β(P, p, N) is the generalization error from training with noise-free data given in Theo-
rem 3.1 and Enoise(P, p, N) is the error due to noise. The error due to noise and the variance of the
generalization error with respect to noise are respectively"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.17419354838709677,"Enoise(P, p, N) = σ2
N−1
X k=0"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.17580645161290323,"Pν−1
η=0 t−4β
k+Nη
h Pν−1
η=0 t−2β
k+Nη
i2 , Varδ

Eθ[∥θδ−θ∥2
2]

= 2σ4 N 2 N−1
X k=0"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.1774193548387097,"h Pν−1
η=0 t−4β
k+Nη
i2"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.17903225806451614,"h Pν−1
η=0 t−2β
k+Nη
i4 ."
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.18064516129032257,"(18)
in the overparameterized regime (12), and"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.18225806451612903,"Enoise(P, p, N)
=
σ22p −N N
+"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.18387096774193548,"N−p−1
X j=0"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.18548387096774194,"ee(N)
jj
Σ2
jj"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.1870967741935484,"
, p > N/2,"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.18870967741935485,"Varδ

Eθ[∥θδ −θ∥2
2]

=
2σ42p −N N 2
+"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.19032258064516128,"N−p−1
X i,j=0"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.19193548387096773,"ee(N)
ij ee(N)
ji
Σ2
iiΣ2
jj 
, (19)"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.1935483870967742,in the underparameterized regime (13).
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.19516129032258064,"The proof of this lemma is documented in Appendix A.3. Note that due to the assumption that
the noise δ and the coefﬁcient θ are independent, the noise-averaged generalization errors are split
exactly into two separate parts: the part due to θ and the part due to δ. The coupling between them
disappears. This also leads to the fact that the variance of the generalization error with respect to
noise only depends on the noise distribution (instead of both the noise variance and the θ variance)."
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.1967741935483871,"For the impact of noise on the generalization error, we see again that the impact of Λ−β
[P ] is only
seen in the overparameterized regime while that of Λ−α
[N] is only seen in the underparameterized
regime. This happens because we assume that we can solve the optimization problem exactly for
each realization of the noisy data. In the overparameterized regime, when β = 0, we have that
Enoise = Nσ2/p as expected. In such a case, the variance reduces to Varδ(Eθ[∥θδ −θ∥2
2]) =
2Nσ4/p2. In the underparameterized regime, when α = 0, we have that Enoise = pσ2/N. The
corresponding variance reduces to Varδ(Eθ[∥θδ −θ∥2
2]) = 2pσ4/N 2."
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.19838709677419356,"In the limit when p →N, that is, in the formally determined regime, the mean generalization error
due to random noise (resp. the variance of the generalization error with respect to noise) converges
to the same value Enoise(P, p, N) = σ2 (resp. Varδ(Eθ[∥θδ −θ∥2
2]) = 2σ4/N) from both the
overparameterized and the underparameterized regimes. This is the classical result in statistical
learning theory (Kaipio & Somersalo, 2005)."
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.2,"The explicit error characterization above is based on the assumption that we solve the optimization
exactly by minimizing the mismatch to 0 in the learning process. In practical applications, it is often
the case that we stop the minimization process when the value of the loss function reaches a level
that is comparable to the size of the noise in the data (normalized by the size of the target function,
for instance). We now present the generalization error bounds for such a case.
Theorem 4.2 (Training with noisy data: error bounds). In the same setup as Lemma 4.1, for ﬁxed
N, we have the following general bound on the generalization error when p is sufﬁciently large:"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.20161290322580644,"Eδ
α,β(P, p, N) ≲p−2bαEδ[∥δ∥2
2,Λ−α
[N]] + p−2βEθ[∥θ∥2
2,Λ−β
[p] ] ,"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.2032258064516129,"where bα := α + 1/2 (resp bα := α) in the overparameterized (resp. underparameterized) regime.
When bα ≥0, the error decreases monotonically with respect to p. When bα < 0, the bound is
minimized by selecting"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.20483870967741935,"p ∼

Eδ[∥δ∥2
2,Λ−α
[N]]−1Eθ[∥θ∥2
2,Λ−β
[p] ]

1
2(β−b
α) ,
(20)"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.2064516129032258,in which case we have
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.20806451612903226,"Eδ
α,β(P, p, N) ≲Eθ[∥θ∥2
2,Λ−β
[p] ]"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.20967741935483872,"−2 b
α
2(β−b
α) Eδ[∥δ∥2
2,Λ−α
[N]]"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.21129032258064517,"2β
2(β−b
α) .
(21)"
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.2129032258064516,Published as a conference paper at ICLR 2022
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.21451612903225806,"Note that the calculations in (20) and (21) are only to the leading order. We neglected the contribu-
tion from the term involving γ. Also, the quantities in expectations can be simpliﬁed. We avoided
doing so to make these quantities easily recognizable as they are the main quantities of interests."
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.2161290322580645,"In the underparameterized regime, that is, the regime of reconstruction, classical statistical inversion
theory shows that it is statistically beneﬁcial to introduce the weight matrix Λ−α
[N] that is related to the
covariance matrix of the noise in the data (Kaipio & Somersalo, 2005) (see also Bal & Ren (2009)
for an adaptive way of adjusting the weight matrix for some speciﬁc applications). Our result here
is consistent with the classical result as we could see that if we take Λ−α
[N] to be the inverse of the
covariance matrix for the noise distribution, the size of the noise contribution in the generalization
error is minimized."
ERROR BOUNDS FOR TRAINING WITH NOISY DATA,0.21774193548387097,"Theorem 4.2 shows that, in the case of training with noisy data, the α parameter can be tuned to
reduce the generalization error of the learning process just as the β parameter in the weight matrix
Λ−β
[P ]. Moreover, this result can serve as a guidance on the selection of the number of features to
be pursued in the training process to minimize the generalization error, depending on the level of
random noise in the training data as well as the regime of the problem."
EXTENSION TO GENERAL FEATURE REGRESSION,0.21935483870967742,"5
EXTENSION TO GENERAL FEATURE REGRESSION"
EXTENSION TO GENERAL FEATURE REGRESSION,0.22096774193548388,"The results in the previous sections, even though are obtained for the speciﬁc form of Fourier feature
regression, also hold in more general settings. Let fθ be a general random feature model of the form"
EXTENSION TO GENERAL FEATURE REGRESSION,0.22258064516129034,"fθ(x) = PP −1
k=0 θkϕk(x), x ∈X,
(22)"
EXTENSION TO GENERAL FEATURE REGRESSION,0.22419354838709676,"constructed from a family of orthonormal features {ϕk}P
k=0 in L2(X). Let Ψ be the feature matrix
constructed from the dataset {xj, yj}N−1
j=0 :"
EXTENSION TO GENERAL FEATURE REGRESSION,0.22580645161290322,"(Ψ)jk = ϕk(xj), 0 ≤j, k ≤P −1 ."
EXTENSION TO GENERAL FEATURE REGRESSION,0.22741935483870968,"We can then apply the same weighted optimization framework (7) to this general model. As in the
case of RFF model, the data in the learning process allows decomposition"
EXTENSION TO GENERAL FEATURE REGRESSION,0.22903225806451613,yδ = Ψ[N]×[p]θ[p] + Ψ[N]×([P ]\[p])θ[P ]\[p] + δ .
EXTENSION TO GENERAL FEATURE REGRESSION,0.2306451612903226,"The learning process only aims to learn the ﬁrst p modes, i.e., θ[p], indicating that the effective noise
that we backpropagate into θ[p] in the learning process is Ψ[N]×([P ]\[p])θ[P ]\[p] + δ. The frequency
contents of this effective noise can come from the true noise δ, the part of Fourier modes that we are
not learning (i.e., θ[P ]\[p]), or even the feature matrix Ψ[N]×([P ]\[p]). For the RFF model, the feature
matrix Ψ[N]×[N] is unitary after being normalized by 1/
√"
EXTENSION TO GENERAL FEATURE REGRESSION,0.23225806451612904,"N. Therefore, all its singular values are
homogeneously
√"
EXTENSION TO GENERAL FEATURE REGRESSION,0.23387096774193547,"N. For learning with many other features in practical applications, we could
have feature matrices Ψ[N]×[N] with fast-decaying singular values. This, on one hand, means that
Ψ[N]×([P ]\[p])θ[P ]\[p] will decay even faster than θ[P ]\[p], making its impact on the learning process
smaller. On the other hand, Ψ[N]×[N] having fast-decaying singular values will make learning θ[p]
harder (since it is less stable)."
EXTENSION TO GENERAL FEATURE REGRESSION,0.23548387096774193,"The impact of the weighting scheme on the generalization error as an expectation over θ and δ
(deﬁned in Lemma 4.1) is summarized in Theorem 5.1. Its proof is in Appendix A.4. Note that here
we do not assume any speciﬁc structure on the distributions of θ and δ except there independence.
Theorem 5.1. Let Ψ = UΣV∗be the singular value decomposition of Ψ. Under the assumptions
(A-I)-(A-III), assume further that Σkk ∼t−ζ
k
for some ζ > 0. Then the generalization error of
training, using the weighted optimization scheme (7) with Λ−α
[N] replaced with Λ−α
[N]U∗and Λ−β
[P ]
replaced with VΛ−β
[P ], satisﬁes"
EXTENSION TO GENERAL FEATURE REGRESSION,0.23709677419354838,"Eδ
α,β(P, p, N) ≲p2(ζ−α)Eδ[∥δ∥2
2,Λ−α
[N]] + p−2βEθ[∥θ∥2
2,Λ−β
[p] ] ,"
EXTENSION TO GENERAL FEATURE REGRESSION,0.23870967741935484,"in the asymptotic limit p ∼N ∼P →∞. The bound is minimized, as a function of p, when"
EXTENSION TO GENERAL FEATURE REGRESSION,0.2403225806451613,"p ∼(Eδ[∥δ∥2
2,Λ−α
[N]]−1Eθ[∥θ∥2,Λ−β
[p] ]2)"
EXTENSION TO GENERAL FEATURE REGRESSION,0.24193548387096775,"1
2(ζ+β−α) ,
(23)"
EXTENSION TO GENERAL FEATURE REGRESSION,0.2435483870967742,Published as a conference paper at ICLR 2022
EXTENSION TO GENERAL FEATURE REGRESSION,0.24516129032258063,in which case we have that
EXTENSION TO GENERAL FEATURE REGRESSION,0.2467741935483871,"Eδ
α,β(P, p, N) ≲Eθ[∥θ∥2
2,Λ−β
[p] ]"
EXTENSION TO GENERAL FEATURE REGRESSION,0.24838709677419354,"ζ−α
(ζ+β−α) Eδ[∥δ∥2
2,Λ−α
[N]]"
EXTENSION TO GENERAL FEATURE REGRESSION,0.25,"β
(ζ+β−α) .
(24)"
EXTENSION TO GENERAL FEATURE REGRESSION,0.25161290322580643,"On the philosophical level, the result says that when the learning model is smoothing, that is, when
the singular values of Ψ decays fast, we can select the appropriate weight matrix to compensate the
smoothing effect so that the generalization error is optimized. The assumption that we have access
to the exact form of the singular value decomposition of the feature matrix is only made to trivialize
the calculations, and is by no means essential. When the optimization algorithm is stopped before
perfect matching can be achieve, the ﬁtting from the weighted optimization scheme generated a
smoother approximation (with a smaller p, according to (23), than what we would obtain with a
regular least-squares minimization)."
EXTENSION TO GENERAL FEATURE REGRESSION,0.2532258064516129,"Feature matrices with fast-decaying singular values are ubiquitous in applications. In Appendix B,
we provide some discussions on the applicability of this result in understanding general kernel learn-
ing (Amari & Wu, 1999; Kamnitsas et al., 2018; Jean et al., 2018; Owhadi & Yoo, 2019; Bordelon
et al., 2020; Canatar et al., 2021) and learning with simpliﬁed neural networks."
CONCLUDING REMARKS,0.25483870967741934,"6
CONCLUDING REMARKS"
CONCLUDING REMARKS,0.2564516129032258,"In this work, we analyzed the impact of weighted optimization on the generalization capability of
feature regression with noisy data for the RFF model and generalized the result to the case of fea-
ture regression and kernel regression. For the RFF model, we show that the proposed weighting
scheme (7) allows us to minimize the impact of noise in the training data while emphasizing the cor-
responding features according to the a priori knowledge we have on the distribution of the features.
In general, emphasizing low-frequency features (i.e., searching for smoother functions) provide bet-
ter generalization ability."
CONCLUDING REMARKS,0.25806451612903225,"While what we analyze in this paper is mainly motivated by the machine learning literature, the
problem of ﬁtting models such as the RFF model (1) to the observed data is a standard inverse
problem that has been extensively studied (Engl et al., 1996; Tarantola, 2005). The main focus of
classical inversion theory is on the case when Ψ[N×p] is at least rank p so that there is a unique least-
squares solution to the equation Ψθ = y for any given dataset y. This corresponds to the learning
problem we described above in the underparameterized regime."
CONCLUDING REMARKS,0.25967741935483873,"In general, weighting the least-squares allows us to have an optimization algorithm that prioritize the
modes that we are interested in during the iteration process. This can be seen as a preconditioning
strategy from the computational optimization perspective."
CONCLUDING REMARKS,0.26129032258064516,"It would be of great interest to derive a rigorous theory for the weighted optimization (7) for general
non-convex problems (note that while the RFF model itself is nonlinear from input x to output
fθ(x), the regression problem is linear). Take a general nonlinear model of the form
F(x; θ) = y,
where F is nonlinear with respect to θ. For instance, F could be a deep neural network with θ
representing the parameters of the network (for instance, the weight matrices and bias vectors at
different layers). We formulate the learning problem with a weighted least-squares as
bθ = Λ−β
[p] bw, with bw = arg min
w
∥Λ−α
[N](F(x; Λ−β
[p] w) −y)∥2
2 ,"
CONCLUDING REMARKS,0.2629032258064516,"where the weight matrices are used to control the smoothness of the gradient of the optimization
procedure. The linearized problem for the learning of w gives a problem of the form Ψeθ = ey with"
CONCLUDING REMARKS,0.2645161290322581,"Ψ = (Λs
[N]F(x; Λβ
[p]w))∗(Λβ
[p])∗(F ′)∗(x; Λβ
[p]w)(Λβ
[N])∗, ey = (Λβ
[p])∗(F ′)∗(x; Λβ
[p]w)(Λs
[N])∗y."
CONCLUDING REMARKS,0.2661290322580645,"For many practical applications in computational learning and inversion, F ′ (with respect to θ)
has the properties we need for Theorem 5.1 to hold. Therefore, a local theory could be obtained.
The question is, can we show that the accumulation of the results through an iterative optimization
procedure (e.g., a stochastic gradient descent algorithm) does not destroy the local theory so that
the conclusions we have in this work would hold globally? We believe a thorough analysis along
the lines of the recent work of Ma & Ying (2021) is possible with reasonable assumptions on the
convergence properties of the iterative process."
CONCLUDING REMARKS,0.267741935483871,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.2693548387096774,ACKNOWLEDGMENTS
CONCLUDING REMARKS,0.2709677419354839,"This work is partially supported by the National Science Foundation through grants DMS-1620396,
DMS-1620473, DMS-1913129, and DMS-1913309. Y. Yang acknowledges supports from Dr. Max
R¨ossler, the Walter Haefner Foundation and the ETH Z¨urich Foundation. This work was done in
part while Y. Yang was visiting the Simons Institute for the Theory of Computing in Fall 2021. The
authors would like to thank Yuege Xie for comments that helped us correct a mistake in an earlier
version of the paper."
CONCLUDING REMARKS,0.2725806451612903,SUPPLEMENTARY MATERIAL
CONCLUDING REMARKS,0.27419354838709675,"Supplementary material for the paper “A Generalized Weighted Optimization Method for Compu-
tational Learning and Inversion” is organized as follows."
CONCLUDING REMARKS,0.27580645161290324,"A
PROOF OF MAIN RESULTS"
CONCLUDING REMARKS,0.27741935483870966,"We provide here the proofs for all the results that we summarized in the main part of the paper. To
simplify the presentation, we introduce the following notations. For the feature matrix Ψ ∈CN×P ,
we denote by
ΨT ∈CN×p and ΨTc ∈CN×(P −p)"
CONCLUDING REMARKS,0.27903225806451615,"the submatrices of Ψ corresponding to the ﬁrst p columns and the last P −p columns respectively.
In the underparameterized regime (13), we will also use the following submatrices"
CONCLUDING REMARKS,0.2806451612903226,"Ψ[N] ∈CN×N
and Ψ[N]\T ∈CN×(N−p),"
CONCLUDING REMARKS,0.28225806451612906,"corresponding to the ﬁrst N columns of Ψ and the last N −p columns of Ψ[N], respectively. We
will use Ψ∗to denote the Hermitian transpose of Ψ. For a P × P diagonal matrix Λ, ΛT (≡Λ[p])
and ΛTc denote respectively the diagonal matrices of sizes p×p and (P −p)×(P −p) that contain
the ﬁrst p and the last P −p diagonal elements of Λ. Matrix Λ[N] is of size N × N and used to
denote the ﬁrst N diagonal elements of Λ. For any column vector θ ∈CP ×1, θT and θTc denote
respectively the column vectors that contain the ﬁrst p elements and the last P −p elements of θ."
CONCLUDING REMARKS,0.2838709677419355,"To study the impact of the weight matrices Λ−α
[N] and Λ−β
[p] , we introduce the re-scaled version of the
feature matrix Ψ, denoted by Φ, as"
CONCLUDING REMARKS,0.2854838709677419,"Φ := Λ−α
[N]ΨΛ−β
[P ] .
(25)"
CONCLUDING REMARKS,0.2870967741935484,"In terms of the notations above, we have that"
CONCLUDING REMARKS,0.2887096774193548,"ΦT := Λ−α
[N]ΨTΛ−β
T
,
and ΦTc := Λ−α
[N]ΨTcΛ−β
Tc .
(26)"
CONCLUDING REMARKS,0.2903225806451613,"For a given column vector θ ∈Cd and a real-valued square-matrix X ∈Rd×d, we denote by"
CONCLUDING REMARKS,0.29193548387096774,"∥θ∥2,X := ∥Xθ∥2 =
p"
CONCLUDING REMARKS,0.29354838709677417,(Xθ)∗Xθ
CONCLUDING REMARKS,0.29516129032258065,"the X-weighted 2-norm of θ. We will not differentiate between ﬁnite- and inﬁnite-dimensional
vectors. In the inﬁnite-dimensional case, we simply understand the 2-norm as the usual ℓ2-norm."
CONCLUDING REMARKS,0.2967741935483871,"We ﬁrst derive a general form for the generalization error for training with θ sampled from a distri-
bution with given diagonal covariance matrix. It is the starting point for most of the calculations in
this paper. The calculation procedure is similar to that of Lemma 1 of Xie et al. (2020). However,
our result is for the case with the additional weight matrix Λ−α
[N]."
CONCLUDING REMARKS,0.29838709677419356,"A.1
PROOFS OF LEMMA A.1-LEMMA A.3"
CONCLUDING REMARKS,0.3,"Lemma A.1 (General form of E0
α,β(P, p, N)). Let K ∈CP ×P be a diagonal matrix, and θ be
drawn from a distribution such that"
CONCLUDING REMARKS,0.3016129032258065,"Eθ[θ] = 0,
Eθ[θθ∗] = K .
(27)"
CONCLUDING REMARKS,0.3032258064516129,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.30483870967741933,Then the generalization error for training with weighted optimization (7) can be written as
CONCLUDING REMARKS,0.3064516129032258,"E0
α,β(P, p, N) = tr
 
K

+ Pα,β + Qα,β,
(28)"
CONCLUDING REMARKS,0.30806451612903224,"where
Pα,β = tr
 
Φ+
T ΦTΛ−2β
T
Φ+
T ΦTΛβ
TKTΛβ
T

−2tr
 
KTΦ+
T ΦT

,"
CONCLUDING REMARKS,0.3096774193548387,"and
Qα,β = tr
 
(Φ+
T )∗Λ−2β
T
Φ+
T ΦTcΛβ
TcKTcΛβ
TcΦ∗
Tc

,"
CONCLUDING REMARKS,0.31129032258064515,with ΦT and ΦTc given in (26).
CONCLUDING REMARKS,0.31290322580645163,Proof. We introduce the new variables
CONCLUDING REMARKS,0.31451612903225806,"w = Λβ
[P ]θ, and z = Λ−α
[N] y ."
CONCLUDING REMARKS,0.3161290322580645,We can then write the solution to the weighted least-square problem (7) as
CONCLUDING REMARKS,0.317741935483871,"bwT = Φ+
T z,
and
bwTc = 0 ,"
CONCLUDING REMARKS,0.3193548387096774,"where Φ+
T is the Moore–Penrose inverse of ΦT. Using the fact that the data y contain no random
noise, we write"
CONCLUDING REMARKS,0.3209677419354839,"y = ΨTΛ−β
T wT + ΨTcΛ−β
Tc wTc,
and z = Λ−α
[N]y = ΦTwT + ΦTcwTc."
CONCLUDING REMARKS,0.3225806451612903,"Therefore, we have, following simple linear algebra, that"
CONCLUDING REMARKS,0.3241935483870968,"∥bθ −θ∥2
2
=
∥Λ−β
T (bwT −wT)∥2
2 + ∥Λ−β
Tc (bwTc −wTc)∥2
2
=
∥Λ−β
T Φ+
T (ΦTwT + ΦTcwTc) −Λ−β
T wT∥2
2 + ∥Λ−β
Tc wTc∥2
2
=
∥Λ−β
T Φ+
T ΦTcwTc −Λ−β
T (IT −Φ+
T ΦT)wT∥2
2 + ∥Λ−β
Tc wTc∥2
2 .
(29)"
CONCLUDING REMARKS,0.3258064516129032,"Next, we make the following expansions:"
CONCLUDING REMARKS,0.32741935483870965,"∥Λ−β
T Φ+
T ΦTcwTc −Λ−β
T (I −Φ+
T ΦT)wT∥2
2
=
∥Λ−β
T Φ+
T ΦTcwTc∥2
2 + ∥Λ−β
T (I −Φ+
T ΦT)wT∥2
2 −T1,"
CONCLUDING REMARKS,0.32903225806451614,"∥Λ−β
T (I −Φ+
T ΦT)wT∥2
2
=
∥Λ−β
T wT∥2
2 + ∥Λ−β
T Φ+
T ΦTwT∥2
2 −T2"
CONCLUDING REMARKS,0.33064516129032256,with T1 and T2 given respectively as
CONCLUDING REMARKS,0.33225806451612905,"T1 := 2ℜ
 
Λ−β
T Φ+
T ΦTcwTc∗Λ−β
T (I −Φ+
T ΦT)wT

,"
CONCLUDING REMARKS,0.3338709677419355,"and
T2 := 2ℜ

w∗
TΛ−β
T Λ−β
T Φ+
T ΦTwT

."
CONCLUDING REMARKS,0.33548387096774196,"We therefore conclude from these expansions and (29), using the linearity property of the expectation
over θ, that"
CONCLUDING REMARKS,0.3370967741935484,"Eθ[∥bθ −θ∥2
2] = Eθ[∥Λ−β
T wT∥2
2] + Eθ[∥Λ−β
Tc wTc∥2
2]"
CONCLUDING REMARKS,0.3387096774193548,"+ Eθ[∥Λ−β
T Φ+
T ΦTwT∥2
2] + Eθ[∥Λ−β
T Φ+
T ΦTcwTc∥2
2] −Eθ[T1] −Eθ[T2] .
(30)"
CONCLUDING REMARKS,0.3403225806451613,"We ﬁrst observe that the ﬁrst two terms in the error are simply tr
 
K

; that is,"
CONCLUDING REMARKS,0.3419354838709677,"Eθ[∥Λ−β
T wT∥2
2] + Eθ[∥Λ−β
Tc wTc∥2
2] = tr
 
K

.
(31)"
CONCLUDING REMARKS,0.3435483870967742,"By the assumption (27), we also have that Eθ[θTcXθ∗
T] = 0 for any matrix X such that the product
is well-deﬁned. Using the relation between θ and w, we conclude that"
CONCLUDING REMARKS,0.34516129032258064,"Eθ[T1] = 0 .
(32)"
CONCLUDING REMARKS,0.3467741935483871,"We then verify, using simple trace tricks, that"
CONCLUDING REMARKS,0.34838709677419355,"Eθ[T2] = 2Eθ
h
ℜ

tr
 
w∗
TΛ−β
T Λ−β
T Φ+
T ΦTwT
i
= 2ℜ

tr
 
KTΦ+
T ΦT

= 2tr
 
KTΦ+
T ΦT

, (33)"
CONCLUDING REMARKS,0.35,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.35161290322580646,"where we have also used the fact that KTΛ−β
T
= Λ−β
T KT since both matrices are diagonal, and the
last step comes from the fact that Φ+
T ΦT is Hermitian."
CONCLUDING REMARKS,0.3532258064516129,"Next, we observe, using standard trace tricks and the fact that Φ+
T ΦT is Hermitian, that"
CONCLUDING REMARKS,0.3548387096774194,"Eθ[∥Λ−β
T Φ+
T ΦTwT∥2
2]
=
Eθ[tr
 
w∗
TΦ+
T ΦTΛ−2β
T
Φ+
T ΦTwT

]"
CONCLUDING REMARKS,0.3564516129032258,"=
tr
 
Φ+
T ΦTΛ−2β
T
Φ+
T ΦTEθ[wTw∗
T]
"
CONCLUDING REMARKS,0.3580645161290323,"=
tr
 
Φ+
T ΦTΛ−2β
T
Φ+
T ΦTΛβ
TKTΛβ
T

,
(34) and"
CONCLUDING REMARKS,0.3596774193548387,"Eθ[∥Λ−β
T Φ+
T ΦTcwTc∥2
2]
=
Eθ[tr
 
w∗
TcΦ∗
Tc(Φ+
T )∗Λ−2β
T
Φ+
T ΦTcwTc
]"
CONCLUDING REMARKS,0.36129032258064514,"=
tr
 
Φ∗
Tc(Φ+
T )∗Λ−2β
T
Φ+
T ΦTcEθ[wTcw∗
Tc]
"
CONCLUDING REMARKS,0.3629032258064516,"=
tr
 
Φ∗
Tc(Φ+
T )∗Λ−2β
T
Φ+
T ΦTcΛβ
TcKTcΛβ
Tc
"
CONCLUDING REMARKS,0.36451612903225805,"=
tr
 
(Φ+
T )∗Λ−2β
T
Φ+
T ΦTcΛβ
TcKTcΛβ
TcΦ∗
Tc

.
(35)"
CONCLUDING REMARKS,0.36612903225806454,"The proof is complete when we put (31), (32), (33), (34) and (35) into (30)."
CONCLUDING REMARKS,0.36774193548387096,Next we derive the general formula for the generalization error for training with noisy data.
CONCLUDING REMARKS,0.36935483870967745,"Lemma A.2 (General form of Eδ
α,β(P, p, N)). Let θ be given as in Lemma A.1. Then, under the
assumptions (A-I)-(A-II), the generalization error for training with weighted optimization (7) from
noisy data yδ can be written as"
CONCLUDING REMARKS,0.3709677419354839,"Eδ
α,β(P, p, N) = E0
α,β(P, p, N) + Enoise(P, p, N)
(36)"
CONCLUDING REMARKS,0.3725806451612903,"where E0
α,β(P, p, N) is given as in (28) and Enoise(P, p, N) is given as"
CONCLUDING REMARKS,0.3741935483870968,"Enoise(P, p, N) = tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]Eδ[δδ∗]

."
CONCLUDING REMARKS,0.3758064516129032,The variance of the generalization error with respect to the random noise is
CONCLUDING REMARKS,0.3774193548387097,"Varδ

Eθ[∥bθ
δ −θ∥2
2]

= Eδ

tr
 
δ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δδ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δ
"
CONCLUDING REMARKS,0.3790322580645161,"−
 
Enoise(P, p, N)
2 .
(37)"
CONCLUDING REMARKS,0.38064516129032255,Proof. We start with the following standard error decomposition
CONCLUDING REMARKS,0.38225806451612904,"∥bθ
δ −θ∥2
2 = ∥bθ
δ −bθ + bθ −θ∥2
2 = ∥bθ −θ∥2
2 + ∥bθ
δ −bθ∥2
2 + 2ℜ

(bθ
δ −bθ)∗(bθ −θ)

."
CONCLUDING REMARKS,0.38387096774193546,"Using the fact that
bθ
δ
Tc = bθTc = 0 ,"
CONCLUDING REMARKS,0.38548387096774195,the error can be simpliﬁed to
CONCLUDING REMARKS,0.3870967741935484,"∥bθ
δ −θ∥2
2 = ∥bθ −θ∥2
2 + ∥bθ
δ
T −bθT∥2
2 + T3,
(38)"
CONCLUDING REMARKS,0.38870967741935486,"where
T3 = 2ℜ

(bθ
δ
T −bθT)∗(bθT −θT)

."
CONCLUDING REMARKS,0.3903225806451613,"Taking expectation with respect to θ and then the noise δ, we have that"
CONCLUDING REMARKS,0.3919354838709677,"Eδ,θ[∥bθ
δ −θ∥2
2] = Eθ[∥bθ −θ∥2
2] + Eδ[∥bθ
δ
T −bθT∥2
2] + Eδ,θ[T3] .
(39)"
CONCLUDING REMARKS,0.3935483870967742,"The ﬁrst term on the right-hand side is simply E0
α,β(P, p, N) and does not depend on δ. To evaluate
the second term which does not depend on θ, we use the fact that"
CONCLUDING REMARKS,0.3951612903225806,"bθ
δ
T −bθT = Λ−β
T Φ+
T (zδ −z) = Λ−β
T Φ+
T Λ−α
[N](yδ −y) = Λ−β
T Φ+
T Λ−α
[N]δ ."
CONCLUDING REMARKS,0.3967741935483871,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.39838709677419354,This leads to
CONCLUDING REMARKS,0.4,"Eδ[∥bθ
δ
T −bθT∥2
2]
=
Eδ[∥Λ−β
T Φ+
T Λ−α
[N]δ∥2
2]"
CONCLUDING REMARKS,0.40161290322580645,"=
Eδ[tr
 
δ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δ

]"
CONCLUDING REMARKS,0.4032258064516129,"=
tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]Eδ[δδ∗]

."
CONCLUDING REMARKS,0.40483870967741936,The last step is to realize that
CONCLUDING REMARKS,0.4064516129032258,"Eδ,θ[T3] = 2ℜ

Eδ,θ
h 
Λ−β
T Φ+
T Λ−α
[N]δ
∗(bθT −θT)
i
= 0"
CONCLUDING REMARKS,0.4080645161290323,due to the assumption that δ and θ are independent.
CONCLUDING REMARKS,0.4096774193548387,"Utilizing the formulas in (38) and (39), and the fact that Eθ[T3] = 0, we can write down the variance
of the generalization error with respect to the random noise as"
CONCLUDING REMARKS,0.4112903225806452,"Varδ

Eθ[∥bθ
δ −θ∥2
2]

= Eδ[∥bθ
δ
T −bθT∥4
2] −

Eδ[∥bθ
δ
T −bθT∥2
2]
2"
CONCLUDING REMARKS,0.4129032258064516,"= Eδ[tr
 
δ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δδ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δ

] −
 
Enoise(P, p, N)
2 ."
CONCLUDING REMARKS,0.41451612903225804,The proof is complete.
CONCLUDING REMARKS,0.4161290322580645,We also need the following properties on the the feature matrix Ψ of the random Fourier model.
CONCLUDING REMARKS,0.41774193548387095,"Lemma A.3. (i) For any ζ ≥0, we deﬁne, in the overparameterized regime (12), the matrices
Π1 ∈CN×N and Π2 ∈CN×N respectively as"
CONCLUDING REMARKS,0.41935483870967744,"Π1 := ΨTΛ−ζ
T Ψ∗
T and Π2 = ΨTcΛ−ζ
Tc Ψ∗
Tc ."
CONCLUDING REMARKS,0.42096774193548386,"Then Π1 and Π2 admit the decomposition Π1 = Ψ[N]ΛΠ1,ζΨ∗
[N] and Π2 = Ψ[N]ΛΠ2,ζΨ∗
[N] re-
spectively with ΛΠ1,ζ and ΛΠ2,ζ diagonal matrices whose m-th diagonal elements are respectively"
CONCLUDING REMARKS,0.42258064516129035,"λ(m)
Π1,ζ = N−1
X k=0"
CONCLUDING REMARKS,0.4241935483870968," ν−1
X"
CONCLUDING REMARKS,0.4258064516129032,"η=0
t−ζ
k+Nη

e(N)
m,k, 0 ≤m ≤N −1 , and"
CONCLUDING REMARKS,0.4274193548387097,"λ(m)
Π2,ζ = N−1
X k=0"
CONCLUDING REMARKS,0.4290322580645161," µ−1
X"
CONCLUDING REMARKS,0.4306451612903226,"η=ν
t−ζ
k+Nη

e(N)
m,k, 0 ≤m ≤N −1 ,"
CONCLUDING REMARKS,0.432258064516129,"where e(N)
m,k is deﬁned as, denoting ωN := e
2πi N ,"
CONCLUDING REMARKS,0.4338709677419355,"e(N)
m,k := 1 N N−1
X"
CONCLUDING REMARKS,0.43548387096774194,"j=0
ω(m−k)j
N
=

1,
k = m
0,
k ̸= m ,
0 ≤m, k ≤N −1 ."
CONCLUDING REMARKS,0.43709677419354837,"(ii) For any α ≥0, we deﬁne, in the underparameterized regime (13) with p < N, the matrix
Ξ ∈Cp×(N−p) as
Ξ :=
 
Ψ∗
TΛ−2α
[N] ΨT
−1Ψ∗
TΛ−2α
[N] Ψ[N]\T ."
CONCLUDING REMARKS,0.43870967741935485,Then Ξ∗Ξ has the following representation:
CONCLUDING REMARKS,0.4403225806451613,"Ξ∗Ξ = N(Ψ∗
[N]\TΛ2α
[N]Ψ[N]\T)−∗Ψ∗
[N]\TΛ4αΨ[N]\T(Ψ∗
[N]\TΛ2α
[N]Ψ[N]\T)−1 −I[N−p] ."
CONCLUDING REMARKS,0.44193548387096776,"Proof. Part (i) is simply Lemma 2 of Xie et al. (2020). It was proved by verifying that Π1 and Π2
are both circulant matrices whose eigendecompositions are standard results in linear algebra."
CONCLUDING REMARKS,0.4435483870967742,"To prove part (ii), let us introduce the matrices P, Q, R through the relation"
CONCLUDING REMARKS,0.44516129032258067,"Ψ∗
[N]Λ−2α
[N] Ψ[N] ="
CONCLUDING REMARKS,0.4467741935483871,"Ψ∗
TΛ−2α
[N] ΨT
Ψ∗
TΛ−2α
[N] Ψ[N]\T
Ψ∗
[N]\TΛ−2α
[N] ΨT
Ψ∗
[N]\TΛ−2α
[N] Ψ[N]\T !"
CONCLUDING REMARKS,0.4483870967741935,"≡

P
R
R∗
Q 
."
CONCLUDING REMARKS,0.45,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.45161290322580644,"Then by standard linear algebra, we have that

Ψ∗
[N]Λ−2α
[N] Ψ[N]
−1
=

P−1 + P−1R(Q −R∗P−1R)−1R∗P−1
−P−1R(Q −R∗P−1R)−1"
CONCLUDING REMARKS,0.4532258064516129,"−(Q −R∗P−1R)−1R∗P−1
(Q −R∗P−1R)−1 "
CONCLUDING REMARKS,0.45483870967741935,"since P = Ψ∗
TΛ−2α
[N] ΨT is invertible."
CONCLUDING REMARKS,0.45645161290322583,"Meanwhile, since
1
√"
CONCLUDING REMARKS,0.45806451612903226,"N Ψ[N] is unitary, we have that"
CONCLUDING REMARKS,0.4596774193548387,"
Ψ∗
[N]Λ−2α
[N] Ψ[N]
−1
=
1
N 2 Ψ∗
[N]Λ2α
[N]Ψ[N]"
CONCLUDING REMARKS,0.4612903225806452,"=
1
N 2"
CONCLUDING REMARKS,0.4629032258064516,"Ψ∗
TΛ2α
[N]ΨT
Ψ∗
TΛ2α
[N]Ψ[N]\T
Ψ∗
[N]\TΛ2α
[N]ΨT
Ψ∗
[N]\TΛ2α
[N]Ψ[N]\T !"
CONCLUDING REMARKS,0.4645161290322581,"≡
 eP
eR
eR∗
eQ 
."
CONCLUDING REMARKS,0.4661290322580645,Comparing the two inverse matrices lead us to the following identity
CONCLUDING REMARKS,0.46774193548387094,P−1 −P−1R eR∗= eP .
CONCLUDING REMARKS,0.4693548387096774,"This gives us that
P−1R = ePR(I[N−p] −eR∗R)−1 .
Therefore, we have"
CONCLUDING REMARKS,0.47096774193548385,R∗P−∗P−1R = (I[N−p] −eR∗R)−∗R∗eP∗ePR(I[N−p] −eR∗R)−1 .
CONCLUDING REMARKS,0.47258064516129034,Utilizing the fact that
CONCLUDING REMARKS,0.47419354838709676,"eR∗R
=
1
N 2 Ψ∗
[N]\TΛ2α
[N]ΨTΨ∗
TΛ−2α
[N] Ψ[N]\T"
CONCLUDING REMARKS,0.47580645161290325,"=
1
N 2 Ψ∗
[N]\TΛ2α
[N]

NI[N] −Ψ[N]\TΨ∗
[N]\T

Λ−2α
[N] Ψ[N]\T = I[N−p] −eQQ ,"
CONCLUDING REMARKS,0.4774193548387097,we can now have the following identity
CONCLUDING REMARKS,0.4790322580645161,R∗P−∗P−1R = ( eQQ)−∗R∗eP∗ePR( eQQ)−1 .
CONCLUDING REMARKS,0.4806451612903226,"Using the formulas for eP and R, as well as ΨTΨ∗
T = NI[N] −Ψ[N]\TΨ∗
[N]\T, we have"
CONCLUDING REMARKS,0.482258064516129,"R∗eP∗ePR
=
1
N 4 Ψ∗
[N]\TΛ−2α
[N] ΨTΨ∗
TΛ2α
[N]ΨTΨ∗
TΛ2α
[N]ΨTΨ∗
TΛ−2α
[N] Ψ[N]\T"
CONCLUDING REMARKS,0.4838709677419355,"=
1
N 3 QΨ∗
[N]\TΛ4α
[N]Ψ[N]\TQ −Q eQ eQQ ."
CONCLUDING REMARKS,0.4854838709677419,This ﬁnally gives us
CONCLUDING REMARKS,0.4870967741935484,"R∗P−∗P−1R =
1
N 3 eQ−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\T eQ−1 −I[N−p] ."
CONCLUDING REMARKS,0.48870967741935484,The proof is complete when we insert the deﬁnion of eQ into the result.
CONCLUDING REMARKS,0.49032258064516127,"A.2
PROOF OF THEOREM 3.1"
CONCLUDING REMARKS,0.49193548387096775,"We provide the proof of Theorem 3.1 here. We split the proof into the overparameterized and the
underparameterized regimes."
CONCLUDING REMARKS,0.4935483870967742,"Proof of Theorem 3.1 (Overparameterized Regime). In the overparameterized regime (12), we have
that the Moore–Penrose inverse Φ+
T = Φ∗
T(ΦTΦ∗
T)−1. Using the deﬁnitions of ΦT in (26), we can
verify that
Φ+
T ΦTΛ−β
T
= Λ−β
T Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−2β
T
,"
CONCLUDING REMARKS,0.49516129032258066,"Λ−β
T Φ+
T ΦT = Λ−2β
T
Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−β
T
,
and"
CONCLUDING REMARKS,0.4967741935483871,"Φ+
T ΦTΛ−2β
T
Φ+
T ΦT = Λ−β
T Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−4β
T
Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−β
T
."
CONCLUDING REMARKS,0.49838709677419357,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.5,"We therefore have
tr
 
Φ+
T ΦTΛ−2β
T
Φ+
T ΦTΛβ
TKTΛβ
T
"
CONCLUDING REMARKS,0.5016129032258064,"=
tr
 
(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−4β
T
Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨTKTΨ∗
T

.
Meanwhile, a trace trick leads to
tr
 
KTΦ+
T ΦT

=
tr
 
KTΦ∗
T(ΦTΦ∗
T)−1ΦT

= tr
 
(ΦTΦ∗
T)−1ΦTKTΦ∗
T
"
CONCLUDING REMARKS,0.5032258064516129,"=
tr
 
(Λ−α
[N]ΨTΛ−2β
T
Ψ∗
TΛ−α
[N])−1Λ−α
[N]ΨTΛ−β
T KTΛ−β
T Ψ∗
TΛ−α
[N]
"
CONCLUDING REMARKS,0.5048387096774194,"=
tr
 
(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−β
T KTΛ−β
T Ψ∗
T

.
(40)
Therefore, based on Lemma A.1, we have ﬁnally that"
CONCLUDING REMARKS,0.5064516129032258,"Pα,β = tr
 
(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−4β
T
Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨTKTΨ∗
T
"
CONCLUDING REMARKS,0.5080645161290323,"−2tr
 
(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−β
T KTΛ−β
T Ψ∗
T

.
(41)
In a similar manner, we can check that
Qα,β
=
tr
 
(Φ+
T )∗Λ−2β
T
Φ+
T ΦTcΛβ
TcKTcΛβ
TcΦ∗
Tc
"
CONCLUDING REMARKS,0.5096774193548387,"=
tr
 
(ΦTΦ∗
T)−∗ΦTΛ−2β
T
Φ∗
T(ΦTΦ∗
T)−1ΦTcΛβ
TcKTcΛβ
TcΦ∗
Tc
"
CONCLUDING REMARKS,0.5112903225806451,"=
tr
 
(ΨTΛ−2β
T
Ψ∗
T)−∗ΨTΛ−4β
T
Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨTcKTcΨ∗
Tc

.
(42)"
CONCLUDING REMARKS,0.5129032258064516,The above calculations give us
CONCLUDING REMARKS,0.5145161290322581,"E0
α,β(P, p, N) = tr
 
K

−2tr
 
(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−β
T KTΛ−β
T Ψ∗
T
"
CONCLUDING REMARKS,0.5161290322580645,"+ tr
 
(ΨTΛ−2β
T
Ψ∗
T)−∗ΨTΛ−4β
T
Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨKΨ∗
.
(43)"
CONCLUDING REMARKS,0.5177419354838709,"We are now ready to evaluate the terms in the generalization error. First, we have that since K =
cγΛ−2γ
[P ] , we have, using the deﬁnition of cγ, that"
CONCLUDING REMARKS,0.5193548387096775,"tr
 
K

= cγ"
CONCLUDING REMARKS,0.5209677419354839,"P −1
X"
CONCLUDING REMARKS,0.5225806451612903,"j=0
t−2γ
j
= 1 .
(44)"
CONCLUDING REMARKS,0.5241935483870968,"Second, using the results in part (i) of Lemma A.3 and the fact that
1
√"
CONCLUDING REMARKS,0.5258064516129032,"N Ψ[N] is unitary, we have"
CONCLUDING REMARKS,0.5274193548387097,"tr
 
(ΨTΛ−2β
T
Ψ∗
T)−1ΨTΛ−β
T KTΛ−β
T Ψ∗
T
"
CONCLUDING REMARKS,0.5290322580645161,"=
cγtr
  1"
CONCLUDING REMARKS,0.5306451612903226,"N 2 Ψ[N]Λ−1
Π1,2βΨ∗
[N]Ψ[N]ΛΠ1,2β+2γΨ∗
[N]
"
CONCLUDING REMARKS,0.532258064516129,"=
cγtr
  1"
CONCLUDING REMARKS,0.5338709677419354,"N Ψ[N]Λ−1
Π1,2βΛΠ1,2β+2γΨ∗
[N]

= cγtr
 
Λ−1
Π1,2βΛΠ1,2β+2γ
 =
cγ N−1
X m=0"
CONCLUDING REMARKS,0.535483870967742,"PN−1
k=0
 Pν−1
η=0 t−2β−2γ
k+Nη

e(N)
m,k
PN−1
k=0
 Pν−1
η=0 t−2β
k+Nη

e(N)
m,k
= cγ N−1
X k=0"
CONCLUDING REMARKS,0.5370967741935484,"Pν−1
η=0 t−2β−2γ
k+Nη
Pν−1
η=0 t−2β
k+Nη
.
(45)"
CONCLUDING REMARKS,0.5387096774193548,The results in Lemma A.3 also give that
CONCLUDING REMARKS,0.5403225806451613,"tr
 
(ΨTΛ−2β
T
Ψ∗
T)−∗ΨTΛ−4β
T
Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨKΨ∗"
CONCLUDING REMARKS,0.5419354838709678,"=
tr
  1"
CONCLUDING REMARKS,0.5435483870967742,"N 2 Ψ[N]Λ−1
Π1,2βΨ∗
[N]Ψ[N]ΛΠ1,4βΨ∗
[N]
1
N 2 Ψ[N]Λ−1
Π1,2βΨ∗
[N]Ψ[N]
 
ΛΠ1,2γ + ΛΠ2,2γ

Ψ∗
[N]
"
CONCLUDING REMARKS,0.5451612903225806,"=
tr
 
Λ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2β
 
ΛΠ1,2γ + ΛΠ2,2γ

.
We can plug in the formula of A.3 to get"
CONCLUDING REMARKS,0.5467741935483871,"tr
 
(ΨTΛ−2β
T
Ψ∗
T)−∗ΨTΛ−4β
T
Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1ΨKΨ∗ = N−1
X m=0"
CONCLUDING REMARKS,0.5483870967741935,"h PN−1
k=0
 Pν−1
η=0 t−4β
k+Nη

e(N)
m,k
ih PN−1
k=0
 Pµ−1
η=0 t−2γ
k+Nη

e(N)
m,k
i"
CONCLUDING REMARKS,0.55,"h PN−1
k=0
 Pν−1
η=0 t−2β
k+Nη

e(N)
m,k
i2 = N−1
X k=0"
CONCLUDING REMARKS,0.5516129032258065," Pν−1
η=0 t−4β
k+Nη
 Pµ−1
η=0 t−2γ
k+Nη
"
CONCLUDING REMARKS,0.5532258064516129,"h Pν−1
η=0 t−2β
k+Nη
i2
.
(46)"
CONCLUDING REMARKS,0.5548387096774193,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.5564516129032258,"We can now put (44), (45) and (46) together into the general error formula (43) to get the result (14)
in Theorem 3.1. The proof is complete."
CONCLUDING REMARKS,0.5580645161290323,"Proof of Theorem 3.1 (Underparameterized Regime). In the underparameterized regime (13), we
have that the Moore–Penrose inverse Φ+
T = (Φ∗
TΦT)−1Φ∗
T. This leads to"
CONCLUDING REMARKS,0.5596774193548387,"Φ+
T ΦT = IT and (Φ+
T )∗Λ−2β
T
Φ+
T = ΦT(Φ∗
TΦT)−∗Λ−2β
T
(Φ∗
TΦT)−1Φ∗
T ."
CONCLUDING REMARKS,0.5612903225806452,"Therefore, using notations in Lemma A.1, we have that Pα,β is simpliﬁed to"
CONCLUDING REMARKS,0.5629032258064516,"Pα,β = −tr
 
KT

,"
CONCLUDING REMARKS,0.5645161290322581,"while Qα,β is simpliﬁed to"
CONCLUDING REMARKS,0.5661290322580645,"Qα,β
=
tr
 
ΦT(Φ∗
TΦT)−∗Λ−2β
T
(Φ∗
TΦT)−1Φ∗
TΛ−α
[N]ΨTcKTcΨ∗
TcΛ−α
[N]
"
CONCLUDING REMARKS,0.567741935483871,"=
tr
 
Λ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] ΨTcKTcΨ∗
Tc

.
(47)"
CONCLUDING REMARKS,0.5693548387096774,"In this regime, p ≤N ≤P. We therefore have Tc =
 
[N]\T

∪[N]c ([N]c := [P]\[N]). Using
the fact that K is diagonal, we obtain"
CONCLUDING REMARKS,0.5709677419354838,"ΨTcKTcΨ∗
Tc = Ψ[N]\TK[N]\TΨ∗
[N]\T + Ψ[N]cK[N]cΨ∗
[N]c"
CONCLUDING REMARKS,0.5725806451612904,"Following the result of Lemma A.3, the second part of decomposition is simply the matrix Π2 with
ν = 1. To avoid confusion, we denote it by Π3 and use the result of Lemma A.3 to get"
CONCLUDING REMARKS,0.5741935483870968,"Ψ[N]cK[N]cΨ∗
[N]c = Ψ[N]ΛΠ3,2γΨ∗
[N]"
CONCLUDING REMARKS,0.5758064516129032,"where the diagonal elements of ΛΠ3,2γ are"
CONCLUDING REMARKS,0.5774193548387097,"λ(m)
Π3,2γ = N−1
X k=0"
CONCLUDING REMARKS,0.5790322580645161," µ−1
X"
CONCLUDING REMARKS,0.5806451612903226,"η=1
t−2γ
k+Nη
 1 N N−1
X"
CONCLUDING REMARKS,0.582258064516129,"j=0
ω(m−k)j
N

, 0 ≤m ≤N −1 .
(48)"
CONCLUDING REMARKS,0.5838709677419355,Next we perform the decomposition
CONCLUDING REMARKS,0.5854838709677419,"Ψ[N]ΛΠ3,2γΨ∗
[N] = ΨTΛΠ3,2γTΨ∗
T + Ψ[N]\TΛΠ3,2γ[N]\TΨ∗
[N]\T ,"
CONCLUDING REMARKS,0.5870967741935483,and deﬁne the diagonal matrices
CONCLUDING REMARKS,0.5887096774193549,"bK1 := ΛΠ3,2γT and
bK2 := K[N]\T + ΛΠ3,2γ[N]\T ."
CONCLUDING REMARKS,0.5903225806451613,"We can then have
ΨTcKTcΨ∗
Tc = ΨT bK1Ψ∗
T + Ψ[N]\T bK2Ψ∗
[N]\T ."
CONCLUDING REMARKS,0.5919354838709677,"Plugging this into the expression for Qα,β, we have"
CONCLUDING REMARKS,0.5935483870967742,"Qα,β = tr
 
Λ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] ΨT bK1Ψ∗
T
"
CONCLUDING REMARKS,0.5951612903225807,"+ tr
 
Λ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] Ψ[N]\T bK2Ψ∗
[N]\T

.
(49)"
CONCLUDING REMARKS,0.5967741935483871,"The ﬁrst term simpliﬁes to tr
  bK1

. The second term vanishes in the case of α = 0 and in the case
when the problem is formally determined; that is, when p = N, but is in general nonzero when
p < N. Using the result in part (ii) of Lemma A.3, we have that"
CONCLUDING REMARKS,0.5983870967741935,"tr
 
Λ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] Ψ[N]\T bK2Ψ∗
[N]\T
"
CONCLUDING REMARKS,0.6,"=
tr
 
Ψ∗
[N]\TΛ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] Ψ[N]\T bK2
"
CONCLUDING REMARKS,0.6016129032258064,"=
Ntr
 
X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1 bK2

−tr
  bK2

, X := Ψ∗
[N]\TΛ2α
[N]Ψ[N]\T ."
CONCLUDING REMARKS,0.603225806451613,Using this result and the singular value decomposition
CONCLUDING REMARKS,0.6048387096774194,"Λα
[N]Ψ[N]\T = Udiag([Σ00, · · · , Σ(N−p−1)(N−p−1)])V∗"
CONCLUDING REMARKS,0.6064516129032258,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.6080645161290322,"introduced in Theorem 3.1, we have"
CONCLUDING REMARKS,0.6096774193548387,"Qα,β
=
tr
  bK1 −bK2

+ Ntr
 
Σ−1U∗Λ2α
[N]UΣ−1V∗bK2V
"
CONCLUDING REMARKS,0.6112903225806452,"=
tr
 
ΛΠ3,2γT −K[N]\T −ΛΠ3,2γ[N]\T

+ N"
CONCLUDING REMARKS,0.6129032258064516,"N−p−1
X i=0"
CONCLUDING REMARKS,0.614516129032258,"N−p−1
X j=0"
CONCLUDING REMARKS,0.6161290322580645,"ee(N)
ij be(N)
ji
ΣiiΣjj
. where"
CONCLUDING REMARKS,0.617741935483871,"ee(N)
ij
= N−1
X"
CONCLUDING REMARKS,0.6193548387096774,"k=0
t2α
k U kiUkj,
be(N)
ij
="
CONCLUDING REMARKS,0.6209677419354839,"N−p−1
X"
CONCLUDING REMARKS,0.6225806451612903,"k′=0
(cγt−2γ
p+k′ + λ(p+k′)
Π3,2γ )V ik′Vjk′,
0 ≤i, j ≤N −p −1 ."
CONCLUDING REMARKS,0.6241935483870967,"The proof is complete when we insert the expressions of Pα,β and Qα,β back to the general for-
mula (28) and use the form of λ(m)
Π3,2γ in (48)."
CONCLUDING REMARKS,0.6258064516129033,"When the problem is formally determined, i.e., in the case of N = p (equivalent to ν = 1),
Ψ∗
TΛ−2α
[N] ΨT = ΨTΛ−2α
[N] Ψ∗
T. This allows us to ﬁnd that"
CONCLUDING REMARKS,0.6274193548387097,"Qα,β
=
tr
  1"
CONCLUDING REMARKS,0.6290322580645161,"N 2 Ψ[N]Λ−1
Π1,2αΨ∗
[N]Ψ[N]ΛΠ1,4αΨ∗
[N]Ψ[N]Λ−1
Π1,2αΨ∗
[N]
1
N 2 Ψ[N]ΛΠ2,2γΨ∗
[N]
"
CONCLUDING REMARKS,0.6306451612903226,"=
tr
 
Λ−2
Π1,2αΛΠ1,4αΛΠ2,2γ

= N−1
X k=0"
CONCLUDING REMARKS,0.632258064516129," Pν−1
η=0 t−4α
k+Nη
 Pµ−1
η=ν t−2γ
k+Nη
"
CONCLUDING REMARKS,0.6338709677419355,"h Pν−1
η=0 t−2α
k+Nη
i2
,"
CONCLUDING REMARKS,0.635483870967742,"which degenerates to its form (46) in the overparameterized regime with ν = 1. The result is then
independent of α since the terms that involve α cancel each other. Similarly, the simpliﬁcation of
Qα,β in (46) with ν = 1 (N = p) also leads to the fact that β disappears from the formula."
CONCLUDING REMARKS,0.6370967741935484,"A.3
PROOFS OF LEMMA 4.1 AND THEOREM 4.2"
CONCLUDING REMARKS,0.6387096774193548,"Proof of Lemma 4.1. By Lemma A.2, the main task is to estimate the size of the generalization error
caused by the random noise in the two regimes. The error is, according to (36),"
CONCLUDING REMARKS,0.6403225806451613,"Enoise(P, p, N) = tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]Eθ[δδ∗]

."
CONCLUDING REMARKS,0.6419354838709678,"In the overparameterized regime, we have that"
CONCLUDING REMARKS,0.6435483870967742,"Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]
=
Λ−α
[N](ΦTΦ∗
T)−∗ΦTΛ−2β
T
Φ∗
T(ΦTΦ∗
T)−1Λ−α
[N]"
CONCLUDING REMARKS,0.6451612903225806,"=
(ΨTΛ−2β
T
Ψ∗
T)−∗ΨTΛ−4β
T
Ψ∗
T(ΨTΛ−2β
T
Ψ∗
T)−1 ."
CONCLUDING REMARKS,0.646774193548387,"Therefore we have, using the results in part (i) of Lemma A.3,"
CONCLUDING REMARKS,0.6483870967741936,"Enoise(P, p, N)
=
σ2tr
  1"
CONCLUDING REMARKS,0.65,"N 2 Ψ[N]Λ−1
Π1,2βΨ∗
[N]Ψ[N]ΛΠ1,4βΨ∗
[N]
1
N 2 Ψ[N]Λ−1
Π1,2βΨ∗
[N]
 =
σ2"
CONCLUDING REMARKS,0.6516129032258065,"N tr
 
Λ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2β

= σ2 N N−1
X k=0"
CONCLUDING REMARKS,0.6532258064516129,"Pν−1
η=0 t−4β
k+Nη
h Pν−1
η=0 t−2β
k+Nη
i2 ."
CONCLUDING REMARKS,0.6548387096774193,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.6564516129032258,"To get the variance of the generalization error with respect to noise, we ﬁrst compute"
CONCLUDING REMARKS,0.6580645161290323,"Eδ[tr
 
δ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δδ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δ

]"
CONCLUDING REMARKS,0.6596774193548387,"=
σ4tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]

tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]
"
CONCLUDING REMARKS,0.6612903225806451,"+2σ4tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]
 =
σ4"
CONCLUDING REMARKS,0.6629032258064517,"N 4 tr
 
Ψ[N]Λ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2βΨ∗
[N]

tr
 
Ψ[N]Λ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2βΨ[N]
 +2σ4"
CONCLUDING REMARKS,0.6645161290322581,"N 4 tr
 
Ψ[N]Λ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2βΨ∗
[N]Ψ[N]Λ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2βΨ∗
[N]
 =
σ4"
CONCLUDING REMARKS,0.6661290322580645,"N 2 tr
 
Λ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2β

tr
 
Λ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2β
 +2σ4"
CONCLUDING REMARKS,0.667741935483871,"N 2 tr
 
Λ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2βΛ−1
Π1,2βΛΠ1,4βΛ−1
Π1,2β
 =
σ4 N 2"
CONCLUDING REMARKS,0.6693548387096774,"h N−1
X k=0"
CONCLUDING REMARKS,0.6709677419354839,"Pν−1
η=0 t−4β
k+Nη
h Pν−1
η=0 t−2β
k+Nη
i2
i2
+ 2σ4 N 2 N−1
X k=0"
CONCLUDING REMARKS,0.6725806451612903,"h Pν−1
η=0 t−4β
k+Nη
i2"
CONCLUDING REMARKS,0.6741935483870968,"h Pν−1
η=0 t−2β
k+Nη
i4 ."
CONCLUDING REMARKS,0.6758064516129032,"This, together with the general form of the variance in (37), gives the result in (18)."
CONCLUDING REMARKS,0.6774193548387096,"In the underparameterized regime, we have that"
CONCLUDING REMARKS,0.6790322580645162,"Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N] = Λ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] ."
CONCLUDING REMARKS,0.6806451612903226,"Therefore we have, using the fact that ΨTΨ∗
T + Ψ[N]\TΨ∗
[N]\T = NI[N], that"
CONCLUDING REMARKS,0.682258064516129,"Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]
(50)"
CONCLUDING REMARKS,0.6838709677419355,"=
1
N 2"
CONCLUDING REMARKS,0.6854838709677419,"
ΨTΨ∗
T + Ψ[N]\TΨ∗
[N]\T

Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]

ΨTΨ∗
T + Ψ[N]\TΨ∗
[N]\T
"
CONCLUDING REMARKS,0.6870967741935484,"=
1
N 2 ΨTΨ∗
T + 1"
CONCLUDING REMARKS,0.6887096774193548,"N 2 ΨT(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] Ψ[N]\TΨ∗
[N]\T + 1"
CONCLUDING REMARKS,0.6903225806451613,"N 2 Ψ[N]\TΨ∗
[N]\TΛ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗Ψ∗
T + 1"
CONCLUDING REMARKS,0.6919354838709677,"N 2 Ψ[N]\TΨ∗
[N]\TΛ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] Ψ[N]\TΨ∗
[N]\T ."
CONCLUDING REMARKS,0.6935483870967742,"This leads to, using the fact that Ψ∗
TΨT = NI[p] and properties of traces,"
CONCLUDING REMARKS,0.6951612903225807,"Enoise(P, p, N) = σ2tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]
 = p"
CONCLUDING REMARKS,0.6967741935483871,N σ2 + σ2
CONCLUDING REMARKS,0.6983870967741935,"N tr
 
Ψ∗
[N]\TΛ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] Ψ[N]\T
"
CONCLUDING REMARKS,0.7,"Using the result in part (ii) of Lemma A.3, we have that the second term simpliﬁes to"
CONCLUDING REMARKS,0.7016129032258065,"tr
 
Ψ∗
[N]\TΛ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] Ψ[N]\T
"
CONCLUDING REMARKS,0.7032258064516129,"= Ntr
 
X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1
−(N −p), X := Ψ∗
[N]\TΛ2α
[N]Ψ[N]\T ."
CONCLUDING REMARKS,0.7048387096774194,Using this result and the singular value decomposition
CONCLUDING REMARKS,0.7064516129032258,"Λα
[N]Ψ[N]\T = Udiag([Σ00, · · · , Σ(N−p−1)(N−p−1)])V∗"
CONCLUDING REMARKS,0.7080645161290322,"introduced in Theorem 3.1, we have"
CONCLUDING REMARKS,0.7096774193548387,"Enoise(P, p, N) = p"
CONCLUDING REMARKS,0.7112903225806452,N σ2−N −p
CONCLUDING REMARKS,0.7129032258064516,"N
σ2+σ2tr
 
Σ−1U∗Λ2α
[N]UΣ−1
= σ22p −N N
+"
CONCLUDING REMARKS,0.714516129032258,"N−p−1
X j=0"
CONCLUDING REMARKS,0.7161290322580646,"ee(N)
jj
Σ2
jj 
."
CONCLUDING REMARKS,0.717741935483871,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.7193548387096774,"Following the same procedure as in the overparameterized regime, the variance of the generalization
error with respect to noise in this case is"
CONCLUDING REMARKS,0.7209677419354839,"Eδ[tr
 
δ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δδ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δ

]"
CONCLUDING REMARKS,0.7225806451612903,"=
σ4tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]

tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]
"
CONCLUDING REMARKS,0.7241935483870968,"+2σ4tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]
"
CONCLUDING REMARKS,0.7258064516129032,"The ﬁrst term on the right hand side is simply
 
Enoise(P, p, N)
2. To evaluate the second term, we
use the formula (50). It is straightforward to obtain, after some algebra, that"
CONCLUDING REMARKS,0.7274193548387097,"tr
 
Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]
"
CONCLUDING REMARKS,0.7290322580645161,"=
1
N 3 tr
 
ΨTΨ∗
T

+ 2"
CONCLUDING REMARKS,0.7306451612903225,"N 2 tr
 
Ψ∗
[N]\TΛ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] Ψ[N]\T
 + 1"
CONCLUDING REMARKS,0.7322580645161291,"N 2 tr
 
Ψ∗
[N]\TΛ−2α
[N] ΨT(Ψ∗
TΛ−2α
[N] ΨT)−∗(Ψ∗
TΛ−2α
[N] ΨT)−1Ψ∗
TΛ−2α
[N] Ψ[N]\T
2"
CONCLUDING REMARKS,0.7338709677419355,"=
p
N 2 + 2 N 2"
CONCLUDING REMARKS,0.7354838709677419,"
Ntr
 
X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1
−(N −p)
 + 1 N 2"
CONCLUDING REMARKS,0.7370967741935484,"
N 2tr
 
X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1"
CONCLUDING REMARKS,0.7387096774193549,"−2Ntr
 
X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1
+ tr
 
I[N−p]
"
CONCLUDING REMARKS,0.7403225806451613,"=
2p −N"
CONCLUDING REMARKS,0.7419354838709677,"N 2
+ tr
 
X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1"
CONCLUDING REMARKS,0.7435483870967742,"=
2p −N"
CONCLUDING REMARKS,0.7451612903225806,"N 2
+ tr
 
X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1X−∗Ψ∗
[N]\TΛ4α
[N]Ψ[N]\TX−1"
CONCLUDING REMARKS,0.7467741935483871,This leads to
CONCLUDING REMARKS,0.7483870967741936,"Eδ[tr
 
δ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δδ∗Λ−α
[N](Φ+
T )∗Λ−2β
T
Φ+
T Λ−α
[N]δ

]"
CONCLUDING REMARKS,0.75,"=
 
Enoise(P, p, N)
2 + 2σ42p −N"
CONCLUDING REMARKS,0.7516129032258064,"N 2
+ tr
 
Σ−2U∗
TΛ2α
[N]UTΣ−2U∗
TΛ2α
[N]UT
"
CONCLUDING REMARKS,0.7532258064516129,"=
 
Enoise(P, p, N)
2 + 2σ42p −N N 2
+"
CONCLUDING REMARKS,0.7548387096774194,"N−p−1
X i=0"
CONCLUDING REMARKS,0.7564516129032258,"N−p−1
X j=0"
CONCLUDING REMARKS,0.7580645161290323,"ee(N)
ij ee(N)
ji
Σ2
iiΣ2
jj 
."
CONCLUDING REMARKS,0.7596774193548387,"Inserting this into the general form of the variance, i.e. (37), will lead to (19) for the underparame-
terized regime. The proof is complete."
CONCLUDING REMARKS,0.7612903225806451,"Proof of Theorem 4.2. By standard decomposition, we have that"
CONCLUDING REMARKS,0.7629032258064516,"∥bθ
δ −θ∥2
2
=
∥bθ
δ −bθ + bθ −θ∥2
2 ="
CONCLUDING REMARKS,0.7645161290322581,"
Ψ+
T
O(P −p)×N"
CONCLUDING REMARKS,0.7661290322580645,"
(yδ −y) + (I[P ] −

Ψ+
T Ψ
O(P −p)×P 
)θ 2 2"
CONCLUDING REMARKS,0.7677419354838709,"≤
2∥Ψ+
T δ∥2
2 + 2
(I[P ] −

Ψ+
T Ψ
O(P −p)×P 
)θ 2"
CONCLUDING REMARKS,0.7693548387096775,"2
.
(51)"
CONCLUDING REMARKS,0.7709677419354839,"From the calculations in the previous sections and the normalization of Ψ we used, it is straightfor-
ward to verify that, for any ﬁxed N,"
CONCLUDING REMARKS,0.7725806451612903,"∥Ψ+
T ∥2,Λ−α
[N]7→2 ∼

1
√"
CONCLUDING REMARKS,0.7741935483870968,νN p−α = p−1
CONCLUDING REMARKS,0.7758064516129032,"2 −α,
p = νN
p−α,
p < N"
CONCLUDING REMARKS,0.7774193548387097,"and
I[P ] −

Ψ+
T Ψ
O(P −p)×P"
CONCLUDING REMARKS,0.7790322580645161,"
2,Λ−β
[P ]7→2
∼p−β ,"
CONCLUDING REMARKS,0.7806451612903226,"where ∥X∥2,Λ−α
[N]7→2 denotes the norm of X as an operator from the Λ−α
[N]-weighted CN to Cp."
CONCLUDING REMARKS,0.782258064516129,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.7838709677419354,"This allows us to conclude, after taking expectation with respect to θ and then δ, that"
CONCLUDING REMARKS,0.785483870967742,"Eδ,θ[∥bθ
δ −θ∥2
2] ≤2∥Ψ+∥2
2,Λ−α
[N]7→2Eδ[∥Λ−α
[N](yδ −y)∥2
2]"
CONCLUDING REMARKS,0.7870967741935484,"+ 2∥(I[P ] −Ψ+Ψ)∥2
2,Λ−β
[P ]7→2Eθ[∥θ∥2
2,Λ−β
[P ]] ≲ρp−2αEδ[∥δ∥2
2,Λ−α
[N]] + p−2βEθ[∥θ∥2
2,Λ−β
[P ]] ,"
CONCLUDING REMARKS,0.7887096774193548,"where ρ = min(1, N/p). For any ﬁxed N, when α > −1/2 in the over-parameterized regime (resp.
α > 0 in the under-parameterized regime), the error decrease monotonically with respect to p. When
α < −1/2 in the over-parameterized regime (resp. α < 0 in the under-parameterized regime), the
ﬁrst term increase with p while the second term descreases with p. To minimize the right-hand side,
we take
p ∼(Eδ[∥δ∥2
2,Λ−α
[N]]−1Eθ[∥θ∥2
2,Λ−β
[P ]])"
CONCLUDING REMARKS,0.7903225806451613,"1
2(β−b
α) ,
(52)"
CONCLUDING REMARKS,0.7919354838709678,where bα := α + 1
CONCLUDING REMARKS,0.7935483870967742,"2 in the over-parameterized regime and bα = α in the under-parameterized regime.
This leads to
Eθ,δ[∥bθ
δ −θ∥2
2] ≲Eθ[∥θ∥2
2,Λ−β
[P ]]"
CONCLUDING REMARKS,0.7951612903225806,"−2 b
α
2(β−b
α) Eδ[∥δ∥2
2,Λ−α
[N]]"
CONCLUDING REMARKS,0.7967741935483871,"2β
2(β−b
α) .
(53)"
CONCLUDING REMARKS,0.7983870967741935,The proof is now complete.
CONCLUDING REMARKS,0.8,"A.4
PROOF OF THEOREM 5.1"
CONCLUDING REMARKS,0.8016129032258065,"Proof of Theorem 5.1. Without loss of generality, we assume that Ψ is diagonal with diagonal ele-
ments Ψkk ∼k−ζ. If not, we can rescale the weight matrix Λ−β
[p] by V and weight matrix Λ−α
[N] by
U as given in the theorem."
CONCLUDING REMARKS,0.8032258064516129,Let Ψ+ be the pseudoinverse of Ψ that consists of the ﬁrst p features such that
CONCLUDING REMARKS,0.8048387096774193,"(Ψ+)qq ∼
qζ,
q ≤p
0,
q > p
where ζ is the exponent in the SVD of Ψ assumed in Theorem 5.1."
CONCLUDING REMARKS,0.8064516129032258,"We can then check that the operators I−Ψ+Ψ : ℓ2
Λ−β
[P ] 7→ℓ2 and Ψ+ : ℓ2
Λ−α
[N] 7→ℓ2 have the following"
CONCLUDING REMARKS,0.8080645161290323,norms respectively
CONCLUDING REMARKS,0.8096774193548387,"∥Ψ+∥2,Λ−α
[N]7→2 ∼pζ−α
and
∥(I −Ψ+Ψ)∥2,Λ−β
[P ]7→2 ∼p−β."
CONCLUDING REMARKS,0.8112903225806452,"By the error decomposition (51), we then conclude, after taking expectation with respect to θ and
then δ, that"
CONCLUDING REMARKS,0.8129032258064516,"Eδ,θ[∥bθ
δ
c −θ∥2
2] ≲∥Ψ+∥2
2,Λ−α
[N]7→2Eδ[∥Λ−α
[N]δ∥2
2]"
CONCLUDING REMARKS,0.8145161290322581,"+ ∥(I −Ψ+Ψ)∥2
2,Λ−β
[P ]7→2Eθ[∥θ∥2
2,Λ−β
[P ]] ≲p2(ζ−α)Eδ[∥δ∥2
2,Λ−α
[N]] + p−2βEθ[∥θ∥2
2,Λ−β
[P ]]."
CONCLUDING REMARKS,0.8161290322580645,"We can now select
p ∼(Eδ[∥δ∥2
2,Λ−α
[N]]−1Eθ[∥θ∥2,Λ−β
[P ]]2)"
CONCLUDING REMARKS,0.817741935483871,"1
2(ζ+β−α)
(54)"
CONCLUDING REMARKS,0.8193548387096774,to minimize the error. This leads to
CONCLUDING REMARKS,0.8209677419354838,"Eδ,θ[∥bθ
δ −θ∥2
2] ≲Eθ[∥θ∥2
2,Λ−β
[P ]]"
CONCLUDING REMARKS,0.8225806451612904,"ζ−α
(ζ−α+β) Eδ[∥δ∥2
2,Λ−α
[N]]"
CONCLUDING REMARKS,0.8241935483870968,"β
(ζ+β−α) .
(55)"
CONCLUDING REMARKS,0.8258064516129032,The proof is now complete.
CONCLUDING REMARKS,0.8274193548387097,"B
ON THE APPLICABILITY OF THEOREM 5.1"
CONCLUDING REMARKS,0.8290322580645161,"In order for the result of Theorem 5.1 to hold, we need the model to be learned to have the smooth-
ing property. In the case of feature regression, this requires that the feature matrices (or sampling
matrices) correspond to kernels whose singular values decay algebraically. This turns out to be true
for many kernel regression models in practical applications. In the case of solving inverse prob-
lems, this is extremely common as most inverse problems based on physical models have smoothing
operators; see, for instance, Isakov (2006); Kirsch (2011) for a more in-depth discussion on this
issue."
CONCLUDING REMARKS,0.8306451612903226,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.832258064516129,"General kernel regression.
Let H be a reproducing kernel Hilbert space (RKHS) over X, and
K : H × H →R be the corresponding (symmetric and positive semideﬁnite) reproducing kernel.
We are interested in learning a function f ∗from given data {xj, yδ
j}N
j=1. The learning process can
be formulated as"
CONCLUDING REMARKS,0.8338709677419355,"min
f∈H N
X j=1"
CONCLUDING REMARKS,0.8354838709677419,"
f(xj) −yδ
j
2
+ β∥f∥2
H .
(56)"
CONCLUDING REMARKS,0.8370967741935483,"Let {ϕk}k≥0 be the eigenfunctions of the kernel K such that
Z
K(x, ˜x)ϕk(˜x)µ(˜x)d˜x = λkϕk(x) ,
(57)"
CONCLUDING REMARKS,0.8387096774193549,"where µ is the probability measure that generates the data. Following Mercer’s Theorem (Rasmussen
& Williams, 2006), we have that K admits a representation in terms of its kernel eigenfunctions; that
is, K(x, ˜x) = P"
CONCLUDING REMARKS,0.8403225806451613,"k≥0 λkϕk(x)ϕk(˜x) . Moreover, the solution to the learning problem as well as the
target function can be approximated respectively as"
CONCLUDING REMARKS,0.8419354838709677,"f ∗
θ(x) ="
CONCLUDING REMARKS,0.8435483870967742,"P −1
X"
CONCLUDING REMARKS,0.8451612903225807,"k=0
θkϕk(x),
and fbθp(x) = p−1
X"
CONCLUDING REMARKS,0.8467741935483871,"k=0
bθkϕk(x) ,
(58)"
CONCLUDING REMARKS,0.8483870967741935,"where to be consistent with the setup in the previous sections, we have used P and p to denote the
numbers of modes in the target function and the learning solution respectively. We can now deﬁne
Ψ to be the feature matrix (with components (Ψ)kj = ϕk(xj)) so that the kernel regression problem
can be recast as the optimization problem"
CONCLUDING REMARKS,0.85,"bθ
δ
p = arg min
θ
∥Ψθp −yδ∥2
2 + β∥θp∥2
2 .
(59)"
CONCLUDING REMARKS,0.8516129032258064,"For a given training data set consisting of N data points, the generalization error for this learning
problem can then be written in the same form as (6); that is,"
CONCLUDING REMARKS,0.853225806451613,"Eδ
α,β(P, p, N) = Eθ,δ
h
∥f ∗
θ(x) −fbθp(x)∥2
L2(X)
i
= Eθ,δ
h
∥bθp −θ∥2
2
i
,
(60)"
CONCLUDING REMARKS,0.8548387096774194,"using the generalized Parseval’s identity (also Mercer’s Theorem) in the corresponding reproducing
kernel Hilbert space."
CONCLUDING REMARKS,0.8564516129032258,"Popular kernels (Rasmussen & Williams, 2006, Chapter 4) in applications include the polynomial
kernel
KP olynomial(x, ˜x) = (α⟨x, ˜x⟩+ 1)d,
(61)
where d is the degree of the polynomial, and the Gaussian RBF (Radial Basis Function) kernel"
CONCLUDING REMARKS,0.8580645161290322,"KGaussian(x, ˜x) = exp

−∥x −˜x∥2 2σ2"
CONCLUDING REMARKS,0.8596774193548387,"
.
(62)"
CONCLUDING REMARKS,0.8612903225806452,"where σ is the standard deviation. For appropriate datasets, such as those normalized ones that
live on the unit sphere Sd−1, there is theoretical as well as numerical evidence to show that the
feature matrix Ψ has eigenvalues decay fast (for instance, algebraically). This means that for such
problems, we also have that the low-frequency modes dominate the high-frequency modes in the
target function. This means that the weighted optimization framework we analyzed in the previous
sections should also apply here. We refer interested readers to Rasmussen & Williams (2006) and
references therein for more technical details and summarize the main theoretical results here."
CONCLUDING REMARKS,0.8629032258064516,"Neural tangent kernel.
It turns out that a similar technique can be used to understand some as-
pects of learning with neural networks. It is particularly related to the frequency bias of neural
networks that has been extensively studied (Ronen et al., 2019; Wang et al., 2020). It is also closely
related to the regularization properties of neural networks (Martin & Mahoney, 2018). To make
the connection, we consider the training of a simple two-layer neural network following the work
of Yang & Salman (2019) and Ronen et al. (2019). We refer interested readers to Daniely et al.
(2016) where kernel formulation of the initialization of deep neural nets was ﬁrst introduced. In
their setting, the neural network is a concentration of a computation skeleton, and the initialization
of the neural network is done by sampling a Gaussian random variable."
CONCLUDING REMARKS,0.864516129032258,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.8661290322580645,"We denote by f a two-layer neural network that takes input vector x ∈Rd to output a scalar value.
We assume that the hidden layer has J neurons. We can then write the network, with activation
function σ, as"
CONCLUDING REMARKS,0.867741935483871,"f(x; Θ, α) =
1
√ M M
X"
CONCLUDING REMARKS,0.8693548387096774,"m=1
αmσ(θT
mx),
(63)"
CONCLUDING REMARKS,0.8709677419354839,"where Θ = [θ1, · · · , θM] ∈Rd×M and α = [α1, · · · , αM]T ∈RJ are respectively the weights of
the hidden layer and the output layer of the network. We omit bias in the model only for simplicity."
CONCLUDING REMARKS,0.8725806451612903,"In the analysis of Ronen et al. (2019); Yang & Salman (2019), it is assumed that the weight of the
output layer α is known and one is therefore only interested in ﬁtting the data to get Θ. This training
process is done by minimizing the L2 loss over the data set {xj, yj}N
j=1:"
CONCLUDING REMARKS,0.8741935483870967,"Φ(Θ) = 1 2 N
X j=1"
CONCLUDING REMARKS,0.8758064516129033,"
yj −f(xj; Θ, α)
2
."
CONCLUDING REMARKS,0.8774193548387097,"When the activation function σ is taken as the ReLU function σ(x) = max(x, 0), we can deﬁne the
matrix, which depends on Θ,"
CONCLUDING REMARKS,0.8790322580645161,"Ψ(Θ) =
1
√ M  

"
CONCLUDING REMARKS,0.8806451612903226,"α1χ11x1
α2χ12x1
· · ·
αMχ1Mx1
α1χ21x2
α2χ22x2
· · ·
αMχ2Mx2
...
...
...
...
α1χN1xN
α2χN2xN
· · ·
αMχNMxN "
CONCLUDING REMARKS,0.882258064516129,"

,"
CONCLUDING REMARKS,0.8838709677419355,"where χjm = 1 if θT
mxj ≥0 and χjm = 0 if θT
mxj < 0. The least-squares training loss can then
be written as 1"
CONCLUDING REMARKS,0.885483870967742,"2∥Ψ(Θ)Θ −y∥2
2. A linearization of the least-squares problem around Θ0 can then be
formulated as
∆Θ = arg min
∆Θ
∥Ψ(Θ0)∆Θ −∆y∥2
2,
(64)"
CONCLUDING REMARKS,0.8870967741935484,where ∆y := y −Ψ(Θ0)Θ0 is the perturbed data.
CONCLUDING REMARKS,0.8887096774193548,"Under the assumption that the input data are normalized such that ∥x∥= 1 and the weight αk ∼
U(−1, 1) (1 ≤k ≤M), it was shown in Ronen et al. (2019) that the singular values of the matrix
Ψ(Θ0) decays algebraically. In fact, starting from initialization θm ∼N(0, κ2I), this result holds
during the whole training process under some mild assumptions."
CONCLUDING REMARKS,0.8903225806451613,"We summarize the main result on the Gaussian kernel and the linear kernel in the following theorem.
Theorem B.1 (Theorem 2 and Theorem 3 of Minh et al. (2006)). Let X = Sn−1, n ∈N and n ≥2.
Let µ be the uniform probability distribution on Sn−1. Then eigenvalues and eigenfunctions for the
Gaussian kernel (62) are respectively"
CONCLUDING REMARKS,0.8919354838709678,λk = e−2/σ2σn−2Ik+n/2−1  2 σ2
CONCLUDING REMARKS,0.8935483870967742,"
Γ
n 2"
CONCLUDING REMARKS,0.8951612903225806,"
(65)"
CONCLUDING REMARKS,0.896774193548387,"for all k ≥0, where I denotes the modiﬁed Bessel function of the ﬁrst kind. Each λk occurs with
multiplicity N(n, k) with the corresponding eigenfunctions being spherical harmonics of order k
on Sd−1. The λk’s are decreasing if σ ≥
p"
CONCLUDING REMARKS,0.8983870967741936,"2/n for the Gaussian kernel (Rasmussen & Williams,
2006). Furthermore, λk forms a descreasing sequence and
 2e σ2"
CONCLUDING REMARKS,0.9,"k
A1
(2k + n −2)k+ n−1"
CONCLUDING REMARKS,0.9016129032258065,"2
< λk  2e σ2"
CONCLUDING REMARKS,0.9032258064516129,"k
A2
(2k + n −2)k+ n−1 2"
CONCLUDING REMARKS,0.9048387096774193,The nonzero eigenvalues are
CONCLUDING REMARKS,0.9064516129032258,"λk = 2d+n−2
d!
(d −k)!
Γ(d + n−1"
CONCLUDING REMARKS,0.9080645161290323,2 )Γ( n
CONCLUDING REMARKS,0.9096774193548387,"2 )
√πΓ(d + k + n −1)
(66)"
CONCLUDING REMARKS,0.9112903225806451,"for the polynommial kernel (61) with 0 ≤k ≤d. Each λk occurs with multiplicity N(n, k), with
the correpsonding eigenfunctions being spherical harmonics of order k on Sn−1. Furthermore, λk
forms a descreasing sequence and
B1
(k + d + n −2)2d+n−3"
CONCLUDING REMARKS,0.9129032258064517,"2 < λk <
B2
(k + d + n −2)d+n−3 2 ."
CONCLUDING REMARKS,0.9145161290322581,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.9161290322580645,"Figure 2: Decay of singular values of sampling matrices for the polynomial and Gaussian kernels
respectively for the (normalized) MNIST data set. The x-axis represents the singular value index."
CONCLUDING REMARKS,0.917741935483871,"The results have been proved in different settings. When the samples are not on Sn−1 but in the
cube [−1, 1]n or the unit ball Bn, or the underlying distrubion of the data is not uniform, there are
similar results. We refer interested readers to Minh et al. (2006); Rasmussen & Williams (2006) and
references therein for more details. In Figure 2, we plot the singular values of the sampling matrix
for the polynomial kernel and the Gaussian RBF kernel for the MINST data set."
CONCLUDING REMARKS,0.9193548387096774,"For the theorem to work for learning with neural networks as we discussed in Section 5, it has been
shown that the neural tangent kernel also satisﬁes the required property in different settings. The
main argument is that neural tangent kernel is equivalent to kernel regression with the Laplace kernel
as proved in Chen & Xu (2020). We cite the following result for the two-layer neural network model."
CONCLUDING REMARKS,0.9209677419354839,"Theorem B.2 (Proposition 5 of Bietti & Mairal (2019)). For any x, ex ∈Sn−1, the eigenvalues of
the neural tangent kernel K are non-negative, satisfying µ0, µ1 > 0, µk = 0 if k = 2j + 1 with
j ≥1, and otherwise µk ∼C(n)k−n as k →∞, with C(n) a constant depending only on n. The
eigenfunction corresponding to µk is the spherical harmonic polynomials of degree k."
CONCLUDING REMARKS,0.9225806451612903,"The proof of this result can be found in Bietti & Mairal (2019). The similarity between neural
tangent kernel and the Laplace kernel was ﬁrst documented in Geifman et al. (2020), and a rigorous
theory was developed in Chen & Xu (2020)."
REFERENCES,0.9241935483870968,REFERENCES
REFERENCES,0.9258064516129032,"S. Amari and S. Wu. Improving support vector machine classiﬁers by modifying kernel functions.
Neural Networks, 12:783–789, 1999."
REFERENCES,0.9274193548387096,"G. Bal and K. Ren. Physics-based models for measurement correlations. application to an inverse
Sturm-Liouville problem. Inverse Problems, 25, 2009. 055006."
REFERENCES,0.9290322580645162,"Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020."
REFERENCES,0.9306451612903226,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
practice and the bias-variance trade-off. Proceedings of the National Academy of Sciences, 116:
15849–15854, 2019."
REFERENCES,0.932258064516129,"Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167–1180, 2020."
REFERENCES,0.9338709677419355,"A. Bietti and J. Mairal. On the inductive bias of neural tangent kernels. Advances in Neural Infor-
mation Processing Systems, pp. 12893–12904, 2019."
REFERENCES,0.9354838709677419,"Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024–1034. PMLR, 2020."
REFERENCES,0.9370967741935484,Published as a conference paper at ICLR 2022
REFERENCES,0.9387096774193548,"Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning.
International Conference on Machine Learning, pp. 872–881, 2019."
REFERENCES,0.9403225806451613,"Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment
explain generalization in kernel regression and inﬁnitely wide neural networks. Nature communi-
cations, 12(1):1–12, 2021."
REFERENCES,0.9419354838709677,"Lin Chen and Sheng Xu. Deep neural tangent kernel and Lsaplace kernel have the same rkhs. arXiv
preprint arXiv:2009.10683, 2020."
REFERENCES,0.9435483870967742,"A. Daniely, R. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The power
of initialization and a dual view on expressivity. Neural Information Processing Systems (NIPS),
pp. 2253–2261, 2016."
REFERENCES,0.9451612903225807,"St´ephane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overﬁtting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020."
REFERENCES,0.9467741935483871,"H. W. Engl, M. Hanke, and A. Neubauer. Regularization of Inverse Problems. Kluwer Academic
Publishers, Dordrecht, The Netherlands, 1996."
REFERENCES,0.9483870967741935,"B. Engquist, K. Ren, and Y. Yang. The quadratic Wasserstein metric for inverse data matching.
Inverse Problems, 36:055001, 2020. arXiv:1911.06911."
REFERENCES,0.95,"A. Geifman, A. Yadav, Y. Kasten, M. Galun, D. Jacobs, and R. Basri. On the similarity between the
Laplace and neural tangent kernels. arXiv:2007.01580, 2020."
REFERENCES,0.9516129032258065,"V. Isakov. Inverse Problems for Partial Differential Equations. Springer-Verlag, New York, second
edition, 2006."
REFERENCES,0.9532258064516129,"N. Jean, S. M. Xie, and S. Ermon. Semi-supervised deep kernel learning: Regression with unlabeled
data by minimizing predictive variance. NIPS, 2018."
REFERENCES,0.9548387096774194,"J. Kaipio and E. Somersalo. Statistical and Computational Inverse Problems. Applied Mathematical
Sciences. Springer, New York, 2005."
REFERENCES,0.9564516129032258,"Konstantinos Kamnitsas, Daniel Castro, Loic Le Folgoc, Ian Walker, Ryutaro Tanno, Daniel Rueck-
ert, Ben Glocker, Antonio Criminisi, and Aditya Nori. Semi-supervised learning via compact la-
tent space clustering. In International Conference on Machine Learning, pp. 2459–2468. PMLR,
2018."
REFERENCES,0.9580645161290322,"A. Kirsch. An Introduction to the Mathematical Theory of Inverse Problems. Springer-Verlag, New
York, second edition, 2011."
REFERENCES,0.9596774193548387,"Weilin Li. Generalization error of minimum weighted norm and kernel interpolation. SIAM Journal
on Mathematics of Data Science, 3(1):414–438, 2021."
REFERENCES,0.9612903225806452,"Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic. Towards a uniﬁed analysis of random
Fourier features. Journal of Machine Learning Research, 22:1–51, 2021a."
REFERENCES,0.9629032258064516,"Zhu Li, Zhi-Hua Zhou, and Arthur Gretton. Towards an understanding of benign overﬁtting in neural
networks. arXiv preprint arXiv:2106.03212, 2021b."
REFERENCES,0.964516129032258,"Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential
equations. arXiv preprint arXiv:2010.08895, 2020."
REFERENCES,0.9661290322580646,"Zhenyu Liao and Romain Couillet. On the spectrum of random features maps of high dimensional
data. Proceedings of the 35th International Conference on Machine Learning, 80:3063–3071,
2018."
REFERENCES,0.967741935483871,"Zhenyu Liao, Romain Couillet, and Michael W Mahoney. A random matrix analysis of random
fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding
double descent. arXiv preprint arXiv:2006.05013, 2020."
REFERENCES,0.9693548387096774,Published as a conference paper at ICLR 2022
REFERENCES,0.9709677419354839,"Fanghui Liu, Xiaolin Huang, Yudong Chen, and Johan A. K. Suykens. Random features for kernel
approximation: A survey on algorithms, theory, and beyond. arXiv:2004.11154, 2020."
REFERENCES,0.9725806451612903,"Fanghui Liu, Zhenyu Liao, and Johan Suykens. Kernel regression in high dimensions: Reﬁned
analysis beyond double descent. Proceedings of The 24th International Conference on Artiﬁcial
Intelligence and Statistics (PMLR), 130:649–657, 2021."
REFERENCES,0.9741935483870968,"Chao Ma and Lexing Ying.
The Sobolev regularization effect of stochastic gradient descent.
arXiv:2105.13462v1, 2021."
REFERENCES,0.9758064516129032,"Charles H. Martin and Michael W. Mahoney. Implicit self-regularization in deep neural networks:
Evidence from random matrix theory and implications for learning. arXiv:1810.01075v1, 2018."
REFERENCES,0.9774193548387097,"H. Q. Minh, P. Niyogi, and Y. Yao. Mercer’s theorem, feature maps, and smoothing. In G. Lugosi
and H. U. Simon (eds.), Learning Theory, Lecture Notes in Computer Science, Berlin, Heidelberg,
2006. Springer."
REFERENCES,0.9790322580645161,"Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling,
and the randomized Kaczmarz algorithm. Advances in neural information processing systems,
27:1017–1025, 2014."
REFERENCES,0.9806451612903225,"Houman Owhadi and Gene Ryan Yoo. Kernel ﬂows: From learning kernels from data into the abyss.
Journal of Computational Physics, 389:22–47, 2019."
REFERENCES,0.9822580645161291,"A. ¨Ozcelikkale. Sparse recovery with non-linear Fourier features. 2020 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP), 2020:5715–5719, 2020."
REFERENCES,0.9838709677419355,"Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems, pp. 1177–1184, 2008."
REFERENCES,0.9854838709677419,"C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press,
Cambridge, MA, 2006."
REFERENCES,0.9870967741935484,"Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural net-
works for learned functions of different frequencies. Advances in Neural Information Processing
Systems, 32:4761–4771, 2019."
REFERENCES,0.9887096774193549,"Shahin Shahrampour and Soheil Kolouri. On sampling random features from empirical leverage
scores: Implementation and theoretical guarantees. arXiv:1903.08329, 2019."
REFERENCES,0.9903225806451613,"Bharath Sriperumbudur and Zoltan Szabo. Optimal rates for random Fourier features. Proceedings
of the 28th International Conference on Neural Information Processing Systems, 1:1144–1152,
2015."
REFERENCES,0.9919354838709677,"A. Tarantola.
Inverse Problem Theory and Methods for Model Parameter Estimation.
SIAM,
Philadelphia, 2005."
REFERENCES,0.9935483870967742,"Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing. High-frequency component helps explain
the generalization of convolutional neural networks. CVPR, 2020."
REFERENCES,0.9951612903225806,"Yuege Xie, Hung-Hsu Chou, Holger Rauhut, and Rachel Ward. Overparameterization and general-
ization error: weighted trigonometric interpolation. arXiv preprint arXiv:2006.08495v3, 2020."
REFERENCES,0.9967741935483871,"Greg Yang and Hadi Salman.
A ﬁne-grained spectral perspective on neural networks.
arXiv:1907.10599v4, 2019."
REFERENCES,0.9983870967741936,"Yunan Yang, Jingwei Hu, and Yifei Lou. Implicit regularization effects of the Sobolev norms in
image processing. arXiv:2109.06255, 2021."
