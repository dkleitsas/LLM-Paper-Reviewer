Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0038910505836575876,"Decisions made by machine learning systems have increasing inﬂuence on the
world, yet it is common for machine learning algorithms to assume that no such
inﬂuence exists. An example is the use of the i.i.d. assumption in content recom-
mendation. In fact, the (choice of) content displayed can change users’ percep-
tions and preferences, or even drive them away, causing a shift in the distribution
of users. We introduce the term auto-induced distributional shift (ADS) to de-
scribe the phenomenon of an algorithm causing a change in the distribution of its
own inputs. Whether it’s desirable (or not) for an algorithm to cause ADS is not
captured by its performance metric – metrics are an incomplete speciﬁcation of
desired behaviour. When real-world conditions violate assumptions, this under-
speciﬁcation can result in unexpected behaviour or ‘gaming’. To diagnose when
this happens, we introduce the approach of unit tests for incentives: simple en-
vironments designed to show whether an algorithm will hide or reveal incentives
to achieve performance via certain means (in our case, via ADS). We use these
unit tests to demonstrate that changes to the learning algorithm (e.g. introducing
meta-learning) can cause previously hidden incentives to be revealed, resulting in
a complete change in behaviour despite no change in performance metric. We
further introduce a toy environment for modelling real-world issues with ADS in
content recommendation, where we demonstrate that strong meta-learners achieve
gains in performance via ADS. These experiments conﬁrm that the unit tests work
– an algorithm’s failure of the unit test correctly diagnoses its propensity to reveal
incentives for ADS."
INTRODUCTION,0.007782101167315175,"1
INTRODUCTION"
INTRODUCTION,0.011673151750972763,"Consider a content recommendation system whose performance is measured by accuracy in pre-
dicting what users will click. This system can achieve better performance by either 1 : Making
better predictions, or
2 : Changing the distribution of users such that predictions are easier to
make. We propose the term auto-induced distributional shift (ADS) to describe this latter kind
of distributional shift, caused by the algorithm’s own predictions or behaviour (Figure 1). ADS are
not inherently bad; often they are desirable. But unexpected ADS can lead to unintended behavior.
While it is common in machine learning (ML) to assume (e.g. via the i.i.d. assumption) that ADS
will not occur, ADS are inevitable in many real-world applications. Thus it is important to under-
stand how ML algorithms behave when such assumptions are violated, i.e. in the actual scenario
they will encounter during training – this is the motivation of our work."
INTRODUCTION,0.01556420233463035,"In many cases, including news recommendation, we would consider 2 a form of speciﬁcation
gaming (Krakovna et al., 2020) – the algorithm changed the task rather than solving it as intended.
We care which means the algorithm used to solve the problem – 1 vs.
2 – but we only told it
about the ends, so it didn’t know not to ‘cheat’. This is an example of a speciﬁcation problem
(Leike et al., 2017; Ortega et al., 2018): a problem which arises from a discrepancy between the
performance metric (maximize accuracy) and “what we really meant” (maximize accuracy only
via 1 ), which is difﬁcult to encode as a performance metric. Ideally, we’d like to quantify the
desirability of all possible means, e.g. assign appropriate rewards to all potential strategies and
side-effects, but this is intractable for real-world settings. Using human feedback to learn reward
functions which account for such impacts is a promising approach to specifying desired behavior
(Leike et al., 2018; Christiano et al., 2017). But the same issue can arise whenever human feedback"
INTRODUCTION,0.019455252918287938,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.023346303501945526,"Figure 1: Distributions of users over time. Left: A distribution which remains constant over time,
following the i.i.d assumption. Right: Auto-induced Distributional Shift (ADS) results in a change
in the distribution of users in our content recommendation environment. (see Section 5.2 for details)."
INTRODUCTION,0.027237354085603113,"is used in training: a means of improving performance could be to alter human preferences, making
them easier to satisfy. Thus in this work, we pursue a complementary approach: managing learners’
incentives."
INTRODUCTION,0.0311284046692607,"A learner has an incentive to behave in a certain way when doing so can increase performance
(e.g. accuracy or reward). Informally, we say an incentive is hidden when the learner behaves
as if it were not present. But we note that changes to the learning algorithm or training regime
could cause previously hidden incentives to be revealed, resulting in unexpected and potentially
undesirable behaviour. Managing incentives (e.g. controlling which incentives are hidden/ revealed)
can allow algorithm designers to disincentivize broad classes of strategies (such as any that rely on
manipulating human preferences) without knowing their exact instantiation."
INTRODUCTION,0.03501945525291829,"Our goal in this work is to provide insight and practical tools for understanding and managing
learners’ incentives, via unit tests for incentives. We present unit tests for diagnosing incentives for
ADS in both supervised learning (SL) and reinforcement learning (RL). The unit tests both have two
means by which the learner can improve performance: one which creates ADS and one which does
not. The intended method of improving performance is one that does not induce ADS; the other
is hidden and we want it to remain hidden. A learner fails the unit test if it pursues the incentive
to increase performance via ADS. In both the RL and SL unit tests, we ﬁnd that ‘vanilla’ learning
algorithms (e.g. minibatch SGD) pass the test, but introducing an outer-loop of meta-learning (e.g.
Population-Based Training (PBT) (Jaderberg et al., 2017)) can lead to high levels of failure."
INTRODUCTION,0.038910505836575876,Our contributions include:
INTRODUCTION,0.042801556420233464,"1. Deﬁning Auto-induced Distributional Shift (ADS) and identifying issues that can arise
from learners pursuing incentives for ADS in myopic reinforcement learning or online
supervised learning problems.
2. Creating unit tests that can determine which learning algorithms are liable to pursue incen-
tives for ADS in these settings.
3. Using these unit tests to experimentally conﬁrm qualitative features of learning algorithms
that affect their tendency to pursue incentives for ADS.
4. Constructing a novel synthetic content recommendation environment that illustrates social
problems associated with ADS, and experimentally validating that our unit tests are predic-
tive of learning algorithms’ behavior in this more complex environment.
5. Proposing a mitigation strategy called context swapping that can effectively hide incen-
tives for ADS."
INTRODUCTION,0.04669260700389105,"Broadly speaking, our experiments demonstrate that performance metrics are incomplete speciﬁca-
tions of which behavior is desired, and that we must consider other algorithmic choices as part of the
speciﬁcation process. In particular, considering which incentives are revealed by different learning
algorithms provides a natural way of specifying which means of achieving high performance are
acceptable."
INTRODUCTION,0.05058365758754864,Under review as a conference paper at ICLR 2022
BACKGROUND,0.054474708171206226,"2
BACKGROUND"
META-LEARNING AND POPULATION BASED TRAINING,0.058365758754863814,"2.1
META-LEARNING AND POPULATION BASED TRAINING"
META-LEARNING AND POPULATION BASED TRAINING,0.0622568093385214,"Meta-learning is the use of machine learning techniques to learn machine learning algorithms. This
involves running multiple training scenarios in an inner loop (IL), while an outer loop (OL) uses
the outcomes of the inner loop(s) as data-points from which to learn which learning algorithms are
most effective (Metz et al., 2019). The number of IL steps per OL step is called the interval."
META-LEARNING AND POPULATION BASED TRAINING,0.06614785992217899,"Population-based training (PBT) (Jaderberg et al., 2017) is a meta-learning algorithm that trains
multiple learners L1, ..., Ln in parallel, after each interval (T steps of IL) applying an evolutionary
OL step which consists of: (1) Evaluate the performance of each learner, (2) Replace both param-
eters and hyperparameters of 20% lowest-performing learners with copies of those from the 20%
high-performing learners (EXPLOIT). (3) Randomly perturb the hyperparameters (but not the pa-
rameters) of all learners (EXPLORE). Two distinctive features of PBT are notable because they give
the OL more control than many other meta-learning algorithms over the learning process. First,
PBT applies optimization to parameters, not just hyperparameters; this means the OL can directly
select for parameters which lead to ADS, instead of only being able to inﬂuence parameter values
via hyperparameters. Second, PBT performs multiple OL steps per training run."
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.07003891050583658,"2.2
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION"
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.07392996108949416,"In general, distributional shift refers to change of the data distribution over time. In supervised
learning with data x and labels y, this can be more speciﬁcally described as dataset shift: change
in the joint distribution of P(x, y) between the training and test sets (Moreno-Torres et al., 2012;
Quionero-Candela et al., 2009). As identiﬁed by Moreno-Torres et al. (2012), two common kinds
of shift are: (1) Covariate shift: changing P(x). In content recommendation, this corresponds to
changing the user base of the recommendation system. For instance, a media outlet which publishes
inﬂammatory content may appeal to users with extreme views while alienating more moderate users.
This self-selection effect (Kayhan, 2015) may appear to a recommendation system as an increase in
performance, leading to a feedback effect, as previously noted by Shah et al. (2018). This type of
feedback effect has been identiﬁed as contributing to ﬁlter bubbles and radicalization (Pariser, 2011;
Kayhan, 2015). (2) Concept shift: changing P(y|x). In content recommendation, this corresponds
to changing a given user’s interest in different kinds of content. For example, exposure to a fake news
story has been shown to increase the perceived accuracy of (and thus presumably future interest in)
the content, an example of the illusory truth effect (Pennycook et al., 2019). For further details on
such effects in content recommendation, see Appendix A."
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.07782101167315175,"3
AUTO-INDUCED DISTRIBUTION SHIFT (ADS)"
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.08171206225680934,"Auto-induced distribution shift (ADS) is distributional shift caused by an algorithm’s behaviour.
This is in contrast to distributional shift which would happen even if the learner were not present –
e.g. for a crash-prediction algorithm trained on data from the summer, encountering snowy roads is
an example of distributional shift, but not auto-induced distributional shift (ADS)."
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.08560311284046693,"We emphasize that ADS are not inherently bad or good; often ADS can even be desirable: consider
the crash-prediction algorithm. If it works well, such a system will help drivers avoid collisions,
thus making self-refuting predictions which result in ADS. What separates desirable and undesirable
ADS? The collision-alert system alters its data distribution in a way that is aligned with the goal of
fewer collisions, whereas the news manipulation results in changes that are misaligned with the goal
of better predicting existing users’ interests (Leike et al., 2018)."
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.08949416342412451,"In reinforcement learning (RL), ADS are typically encouraged as a means to increase performance.
On the other hand, in supervised learning (SL), the i.i.d. assumption precludes ADS in theory. In
practice, however, the possibility of using ADS to increase performance (and thus an incentive to
do so) often remains. For instance, this occurs in online learning. In our experiments, we explicitly
model such situations where i.i.d. assumptions are violated: We study the behavior of SL and myopic
RL algorithms, in environments designed to include incentives for ADS, in order to understand when
incentives are effectively hidden. Fig. 2 contrasts these settings with typical RL and SL."
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.0933852140077821,"Under review as a conference paper at ICLR 2022 a1
a2 s1
s2 r1
r2"
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.09727626459143969,"(a)
RL:
Incentives
for
ADS are present and pur-
suing them is desirable"
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.10116731517509728,"ˆy1
ˆy2 x1
x2 ℓ1
ℓ2"
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.10505836575875487,"(b) SL with i.i.d. data: In-
centives for ADS are ab-
sent a1
a2 s1
s2 r1
r2"
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.10894941634241245,"(c) Myopic RL: Incentives
for ADS are present and
pursuing them is undesir-
able"
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.11284046692607004,"ˆy1
ˆy2 x1
x2 ℓ1
ℓ2"
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.11673151750972763,"(d) SL with ADS: Incen-
tives for ADS are present
and pursuing them is un-
desirable"
DISTRIBUTIONAL SHIFT AND CONTENT RECOMMENDATION,0.12062256809338522,"Figure 2: In the widely studied problems of (a) reinforcement learning (RL) with state s, action
s, reward s tuples, and (b) i.i.d. supervised learning (SL) with inputs x, predictions ˆy and loss l,
there are no issues of undesirable incentives for ADS. We focus on cases where there are incentives
present which the learner is not meant to pursue (c,d). Lines show paths of inﬂuence. The learner
may have incentives to inﬂuence any nodes descending from its action, A, or prediction, ˆy. Which
incentives are undesirable (orange) or desirable (cyan) for the learner to pursue is context-dependent."
INCENTIVES,0.1245136186770428,"4
INCENTIVES"
INCENTIVES,0.12840466926070038,"For our study of incentives, we use the following terminology: an incentive for a behavior (e.g. an
action, a classiﬁcation, etc.) is present (not absent) to the extent that the behaviour will increase
performance (e.g. reward, accuracy, etc.) (Everitt & Hutter, 2019). This incentive is revealed to (not
hidden from) a learner if it would, at higher than chance levels, learn to perform the behavior given
sufﬁcient capacity and training experience. The incentive is pursued (not eschewed) by a learner if
it actually performs the incentivized behaviour. Note even when an incentive is revealed, it may not
be pursued, e.g. due to limited capacity and/or data, or simply chance."
INCENTIVES,0.13229571984435798,"For example, in content recommendation, the incentive to drive users away is present if some user
types are easier to predict than others. But this incentive may be hidden from the learner by using
a myopic algorithm, e.g. one that does not see the effects of its actions on the distribution of users.
The incentive might instead be revealed to the outer loop of a meta-learning algorithm like PBT,
which does see the effects of learner’s actions."
INCENTIVES,0.13618677042801555,"Even when this incentive is revealed, however, it might not end up being pursued. For example, this
could happen if predicting which recommendations will drive away users is too difﬁcult a learning
problem, or if the incentive to do so is dominated by other incentives (e.g. change individual users’
interests, or improve accuracy of predictions). In general, it may be difﬁcult to determine empirically
which incentives are revealed, because failure to pursue an incentive can be due to limited capacity,
insufﬁcient training, and/or random chance. To address this challenge, we devise extremely simple
environments (‘unit tests’), where we can be conﬁdent that revealed incentives will be pursued."
INCENTIVES,0.14007782101167315,"Hiding incentives can be an effective method of inﬂuencing learner behavior. For example, hiding
the incentive to manipulate users from a content recommendation algorithm could prevent it from
inﬂuencing users in a way they would not endorse. However, if machine learning practitioners are
not aware that incentives are present, or that properties of the learning algorithm are hiding them,
then seemingly innocuous changes to the learning algorithm may lead to signiﬁcant unexpected
changes in behavior."
INCENTIVES,0.14396887159533073,"Hiding incentives for ADS may seem counter-intuitive and counter-productive in the context of rein-
forcement learning (RL), where moving towards high-reward states is typically desirable. However,
for real-world applications of RL, the ultimate goal is not a system that achieves high reward, but
rather one that behaves according to the designer’s intentions. And as we discussed, it can be in-
tractable to design reward functions that perfectly specify intended behavior. Moreover, substantial
real-world issues could result from improper management of learners’ incentives. Examples include
tampering with human-generated reward signals (Everitt & Hutter, 2018) (e.g. selecting news arti-
cles to manipulate users), and making “self-fulﬁlling prophecies” (e.g. driving up an asset’s value
by publicly predicting its value will increase (Armstrong & O’Rorke, 2017))."
INCENTIVES,0.14785992217898833,Under review as a conference paper at ICLR 2022
INCENTIVES,0.1517509727626459,"Hiding incentives for ADS via Context Swapping
We propose a technique called context swap-
ping that can hide incentives for ADS that might otherwise be revealed by the use of meta-learning
or other algorithmic choices. The technique trains N learners in parallel, and (e.g. deterministically)
shufﬂes the learners through N different copies of the same (or similar) environments. When N
is larger than the interval of the OL optimizer, each learner inhabits each copy for at most a single
time-step before an OL step is applied. This can hide incentives for ADS in practice, see Sec. 5.1.1."
INCENTIVES,0.1556420233463035,"Figure 3: (a) No context swapping (b) Context swapping rotates learners through different environ-
ments. This removes the incentive for a learner to “invest” in a given environment, since it will be
swapped out of that context later and not be able to reap the beneﬁts of its investment."
EXPERIMENTS,0.15953307392996108,"5
EXPERIMENTS"
EXPERIMENTS,0.16342412451361868,"In Section 5.1 we introduce unit tests that determine whether incentives for ADS are revealed. Our
experiments show that you can have a learner which behaves as intended, and just by introduc-
ing meta-learning (e.g. PBT), without changing the performance metric (e.g. loss or rewards), the
learner’s behavior can change completely. We also show that context swapping is an effective mit-
igation technique in these environments. As we demonstrate, the unit tests can be used to compare
learning algorithms and diagnose their propensity to reveal incentives."
EXPERIMENTS,0.16731517509727625,"In Section 5.2, we model a content recommendation system. The goal of these experiments is to
demonstrate how revealed incentives for ADS could create issues for real-world content recommen-
dation systems such as news feeds.They also validate the usefulness of the unit tests: algorithms that
failed the unit tests also reveal incentives for ADS in this setting. We emphasize that ADS takes
place in this environment by construction. The point of our experiments is that meta-learning can
increase the rate and/or extent of ADS, by revealing this incentive. We ﬁnd that context swapping
is not effective in this environment, highlighting the need for alternative mitigation strategies."
EXPERIMENTS,0.17120622568093385,"5.1
ADS INCENTIVE UNIT TEST (MYOPIC RL)"
EXPERIMENTS,0.17509727626459143,"This unit test a POMDP (Kaelbling et al., 1998) inspired by the prisoner’s dilemma (Prisner, 2014),
where an agent plays each round against its past self; details in Appendix C.1.1. The reward function
is presented in Table 1. An agent in this environment has a long-term, non-myopic, incentive to
cooperate (with its future self), but a current-time-step, myopic, incentive to defect (from its
future self). The unit test evaluates whether a learning algorithm reveals the non-myopic incentive,
even when the agent is meant to optimize for the present reward only (i.e. uses discount rate γ = 0).
While this may seem like an easy ‘brute-force’ way to hide incentives for ADS, we show it is in fact
non-trivial to implement. Naively, we’d expect the non-myopic incentive to be hidden from an agent
with γ = 0, and for the agent to consistently defect; learning algorithms that do so pass the test.
But some learning algorithms fail the unit test, revealing the incentive for the agent to cooperate
with its future self. We create a similar unit test for supervised learning, and ﬁnd similar results,
detailed in Appendix B."
EXPERIMENTS,0.17898832684824903,"Table 1: Rewards for the RL unit test. Note that the myopic defect action always increases reward
at the current time-step, but decreases reward at the next time-step – the incentive is hidden from the
point of view of a myopic learner. A learner ‘fails’ the unit test if the hidden incentive to cooperate
is revealed, i.e. if we see more cooperate (C) actions than defect (D)."
EXPERIMENTS,0.1828793774319066,"at = D
at = C
st = at−1 = D
−1/2
−1
st = at−1 = C
1/2
0"
EXPERIMENTS,0.1867704280155642,Under review as a conference paper at ICLR 2022
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.19066147859922178,"5.1.1
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION"
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.19455252918287938,"We ﬁrst show that agents trained with PBT fail the unit tests more often than “vanilla” algorithms
which do not use meta-learning. Policies are represented by a single real-valued parameter θ (initial-
ized as θ ∼N(0, 1)) passed through a sigmoid whose output represents P(at = defect). We use
REINFORCE (Williams, 1992) with discount factor γ = 0 as the baseline/IL optimizer. PBT (with
default settings, see Section 2.2) is used to tune the learning rate, with reward on the ﬁnal time-step
of the interval as the performance measure for PBT. We initialize the learning rate log-uniformly
between 0.01 and 1.0 for all experiments (whether using PBT or not)."
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.19844357976653695,"We expect and conﬁrm that the following two factors lead to higher rates of unit test fail-
ure:
(1) Shorter intervals: These give the OL more opportunities to inﬂuence the popula-
tion.
(2) Larger populations: These make outliers with exceptional non-myopic performance
more likely, and OL makes them likely to survive and propagate."
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.20233463035019456,"The baseline (no meta-learning) algorithms all pass the unit tests: hidden incentives are almost
never revealed - see blue curves in Fig. 4. However, agents trained with meta-learning and large
populations often fail the unit tests: see orange curves in top rows of Fig. 4. 0.00 0.25 0.50 0.75 1.00"
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.20622568093385213,||y2||
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.21011673151750973,"#agents=10
#agents=100"
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.2140077821011673,no env swapping
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.2178988326848249,#agents=1000
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.22178988326848248,"0
200
400
600
800
1000
time-step 0.00 0.25 0.50 0.75 1.00"
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.22568093385214008,||y2||
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.22957198443579765,"0
200
400
600
800
1000
time-step"
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.23346303501945526,"0
200
400
600
800
1000
time-step"
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.23735408560311283,env swapping
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.24124513618677043,(A) Myopic RL Unit Test. OL=PBT.
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.245136186770428,(B) Myopic RL Unit Test. OL=REINFORCE
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.2490272373540856,"Figure 4: Average level of non-myopic cooperate behavior observed in the RL unit test, with two
meta-learning algorithms (A) PBT and (B) REINFORCE. Lower is better, since the goal is for (non-
myopic) incentives for ADS to remain hidden. Despite the inner loop being fully myopic (γ = 0),
outer-loop (OL) optimizers reveal incentives for ADS (top rows). Context swapping effectively
hides this incentive, reducing ADS (bottom rows)."
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.2529182879377432,"Furthermore, we verify that context swapping signiﬁcantly mitigates the effect of HI-ADS in both
unit tests, decreasing undesirable behaviour to near-baseline levels – see bottom rows of Fig. 4.
This effect can be explained as follows: Because context swapping transfers the beneﬁts of one
learner’s action to the next learner to inhabit that environment, it increases the second learner’s
ﬁtness, and thereby reduces the relative ﬁtness (as evaluated by PBT’s EXPLOIT step) of the non-
myopic cooperate behaviour. We observe some interesting exceptions with the combination of
small populations and short PBT intervals: Although context swapping still signiﬁcantly decreases
the effect of HI-ADS, non-myopic cooperate behaviour is observed as much as 20% of the time
(for #learners=10, T = 1; see bottom-left plot)."
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.25680933852140075,Under review as a conference paper at ICLR 2022
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.2607003891050584,"We also observe that PBT reveals incentives for ADS even when T = 1, where the explanation that
PBT operates on a longer time horizon than the inner loop does not apply. We provide a detailed
explanation for how this might happen in Appendix C.1.3, but in summary, we hypothesize that
there are at least 2 mechanisms by which PBT is revealing incentives for ADS: (1) optimizing over
a longer time-scale, and (2) picking up on the correlation between an agent’s current policy and the
underlying state. Mechanism (2) can be explained informally as reasoning as: “If I’m cooperating,
then I was probably cooperating on the last time-step as well, so my reward should be higher”. As
support for these hypotheses, we run control experiments identifying two algorithms (each sharing
only one of these properties) that can fail the unit test. Context swapping remains effective in both."
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.26459143968871596,"(1) Optimizing over a longer time-scale: replacing PBT with REINFORCE as an outer-loop op-
timizer. The outer-loop optimizes the parameters to maximize the summed reward of the last T
time-steps. As with PBT, we observe non-myopic behavior, but now only when T > 1. This sup-
ports our hypothesis that revealing incentives for ADS is due not to PBT in particular, but rather to
the introduction of sufﬁciently powerful meta-learning. See Fig. 4 B2."
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.26848249027237353,"(2) Exploiting correlation: Q-learning with γ = 0 an ϵ = 0.1-greedy behavior policy and no
meta-learning. If either state was equally likely, the Q-values would be the average of the values
in each column in Table 1, so the estimated Q(defect) would be larger. But the ϵ-greedy policy
correlates the previous action (i.e. the current state) and current action (so long as the policy did not
just change), so the top-left and bottom-right entries carry more weight in the estimates, sometimes
causing Q(defect) ≈Q(cooperate) and persistent nonmyopic behavior. See Fig. 5 for results,
and Appendix C.1.4 for experimental details. A similar effect is observed in ofﬂine RL, even though
the learner cannot inﬂuence its data distribution, see Appendix C.1.5 for details."
MYOPIC RL UNIT TEST EXPERIMENTAL RESULTS AND DISCUSSION,0.2723735408560311,"Figure 5: Left: Ofﬂine Q-learning can reveal incentives for ADS when pooling data from different
policies. Yellow regions represent policy pairs (θ1, θ2) for which Q(C) > Q(D) in the Myopic
RL unit test, resulting in non-myopic behavior. Right: Even online, Q-learning fails the unit test
for some random seeds; empirical p(cooperate) stays around 80-90% in 3 of 5 experiments
(bottom row). Each column represents an independent experiment. Q-values for the cooperate
and defect actions stay tightly coupled in the failure cases (col. 1,2,5), while in the cases passing
the unit test (col. 3,4) the Q-value of cooperate decreases over time."
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION,0.27626459143968873,"5.2
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION"
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION,0.2801556420233463,"We now present a toy environment for modeling content recommendation of news articles, which
includes the potential for ADS by incorporating the mechanisms mentioned in Sec. 2.2, discussed
as contributing factors to the problems of fake news and ﬁlter bubbles. Speciﬁcally, the environment
assumes that presenting an article to a user can inﬂuence (1) their interest in similar articles, and
(2) their propensity to use the recommendation service. These correspond to modeling auto-induced
concept shift of users, and auto-induced covariate shift of the user base, respectively (see Sec. 2.2)."
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION,0.2840466926070039,"This environment includes the following components, which change over (discrete) time: User type:
xt, Article type: yt, User interests: Wt (propensity for users of each type to click on articles of
each type), and User loyalty: gt (propensity for users of each type to use the platform). At each
time step t, a user xt is sampled from a categorical distribution, based on the loyalty of the different
user types. The recommendation system (a classiﬁer) selects which type of article to present in the
top position, and ﬁnally the user ‘clicks’ an article yt, according to their interests. User loyalty for
user type xt undergoes covariate shift: in accordance with the self-selection effect, gt increases or"
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION,0.28793774319066145,Under review as a conference paper at ICLR 2022
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION,0.2918287937743191,"decreases proportionally to that user type’s interest in the top article. The interests of user type xt
(represented by a column of Wt) undergoing concept shift; in accordance with the illusory truth
effect, interest in the topic of the top article chosen by the recommender system always increases."
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION,0.29571984435797666,"Formally, this environment is similar to a POMDP\R, i.e. a POMDP with no reward function, also
known as a world model (Armstrong & O’Rourke, 2017; Hadﬁeld-Menell et al., 2017); the differ-
ence is that the learner observes the input (ot
pre) before acting and only observes the target (ot
post)
after acting. The states s, observations o, and actions a are computed as follows:"
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION,0.29961089494163423,"st = (gt, Wt, xt, yt)"
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION,0.3035019455252918,"ot
pre, at, ot
post = (xt, ˆyt, yt)"
INCENTIVES FOR ADS IN CONTENT RECOMMENDATION,0.30739299610894943,"For further details on this environment, including the state transition function, see Appendix C.2.1."
CONTENT RECOMMENDATION EXPERIMENTAL RESULTS AND DISCUSSION,0.311284046692607,"5.2.1
CONTENT RECOMMENDATION EXPERIMENTAL RESULTS AND DISCUSSION"
CONTENT RECOMMENDATION EXPERIMENTAL RESULTS AND DISCUSSION,0.3151750972762646,"We ﬁnd that PBT yields signiﬁcant improvements in training time and accuracy, but also greater
distributional shift (Fig. 6). User base and user interests both change faster with PBT, and user
interests change more overall. We observe that the distributions over user types typically saturate
(to a single user type) after a few hundred time-steps (Fig 1 and Fig. 6, Right). We run long enough
to reach such states, to demonstrate that the increase in ADS from PBT is not transitory. The
environment has a number of free parameters, and our results are qualitatively consistent so long as
(1) the initial user distribution is approximately uniform, and (2) the covariate shift rate (α1) is faster
than the concept shift rate (α2). See Appendix C.1 for details."
CONTENT RECOMMENDATION EXPERIMENTAL RESULTS AND DISCUSSION,0.31906614785992216,"We measure concept shift (change in P(y|x)) as the cosine distance between each user types’ initial
and current interest vectors. And we measure covariate shift (change in P(x)) as the KL-divergence
between the current and initial user distributions, parametrized by g1 and gt, respectively. Our
recommender system is a 1-layer MLP trained with SGD-momentum. Actions are sampled from
the MLP’s predictive distribution. For PBT, we use T = 10 and 20 agents, and use accuracy to
evaluate performance. We run 20 trials, and match random seeds for trials with and without PBT.
See Appendix C.2 for full experimental details."
CONTENT RECOMMENDATION EXPERIMENTAL RESULTS AND DISCUSSION,0.3229571984435798,"Figure 6: Content recommendation experiments. Left: using Population Based Training (PBT)
increases accuracy of predications faster, leads to a faster and larger drift in users’ interests, P(y|x),
(Center); as well as the distribution of users, P(x), (Right). Shading shows std error over 20 runs."
RELATED WORK,0.32684824902723736,"6
RELATED WORK"
RELATED WORK,0.33073929961089493,"ADS in practice:
We introduce the term ADS, but we are far from the ﬁrst to study it. Caruana
et al. (2015) provide an example of asthmatic patients having lower predicted risk of pneumonia.
Treating asthmatics with pneumonia less aggressively on this basis would be an example of harmful
ADS; the reason they had lower pneumonia risk was because they had received more aggressive
care already. Schulam & Saria (2017) note that such predictive models are commonly used to
inform decision-making, and propose modeling counterfactuals (e.g. “how would this patient fare
with less aggressive treatment”) to avoid such self-refuting predictions. While their goal is to make
accurate predictions in the presence of ADS, our goal is to identify and manage incentives for ADS.
Goodfellow (2019) argues that adversarial defenses that do not account for ADS are critically ﬂawed."
RELATED WORK,0.3346303501945525,Under review as a conference paper at ICLR 2022
RELATED WORK,0.33852140077821014,"Non-i.i.d bandits: Contextual bandits (Wang et al., 2005; Langford & Zhang, 2008) are frequently
discussed as an approach to content recommendation (Li et al., 2010). While bandit algorithms
typically make the i.i.d. assumption, counter-examples exist (Gheshlaghi Azar et al., 2014; Shah
et al., 2018); most famously, adversarial bandits (Auer et al., 1995). Closest to our work is Shah et al.
(2018), who consider covariate shift caused by multi-armed bandits. Our task in Sec. 5.2 is similar
to their problem statement, but more general in that we include user features, thus disentangling
covariate shift and concept shift. Our motivation is also different: Shah et al. (2018) seek to exploit
ADS, whereas we aim to avoid hidden incentives for ADS."
RELATED WORK,0.3424124513618677,"Safety and incentives:
Emergent incentives to inﬂuence the world are at the heart of many con-
cerns about the safety of advanced AI systems (Omohundro, 2008; Bostrom, 2014). Understanding
and managing the incentives of learners is a focus of Armstrong & O’Rourke (2017); Everitt (2018);
Everitt et al. (2019); Cohen et al. (2019). While Everitt et al. (2019) focus on identifying which
incentives are present, we note that incentives may be present and yet not be revealed or pursued –
for example, in supervised learning, there is an incentive to over-ﬁt the test set, but hiding the test
set from the learner hides this incentive. While Carey et al. (2020); Everitt et al. (2019); Armstrong
& O’Rourke (2017) discuss methods of removing problematic incentives, we note in practice in-
centives are often hidden rather than removed. Our work addresses the efﬁcacy of this approach of
hiding incentives and ways in which it can fail."
RELATED WORK,0.3463035019455253,"Incentives and meta-learning:
We believe our work is the ﬁrst to consider the problem of hid-
ing/revealing incentives for ADS, and the relation to meta-learning. A few previous works have
some relevance or resemblance. Rabinowitz (2019) documents qualitative differences in learning
behavior when meta-learning is applied. MacKay et al. (2019) and Lorraine & Duvenaud (2018)
view meta-learning as a bilevel optimization problem, with the inner loop playing a best-response
to the outer loop. In our work, the inner loop is unable to achieve such best-response behavior; the
outer loop is too powerful (see Fig. 4). Finally, Sutton et al. (2007) note that meta-learning can
change learning behavior and improve performance by preventing convergence of the inner loop."
RELATED WORK,0.35019455252918286,"Underspeciﬁcation: D’Amour et al. (2020) discuss underspeciﬁcation as a source of poor behavior
in real world settings. They focus on differences in training vs. deployment performance, similarly to
(Ilyas et al., 2019; Koch et al., 2021). We go beyond this by showing how changing which incentives
are revealed can lead to fundamentally different solutions with different training performance."
DISCUSSION AND CONCLUSION,0.3540856031128405,"7
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.35797665369649806,"We have identiﬁed the phenomenon of auto-induced distributional shift (ADS), and the problems that
can arise when previously hidden incentives for learners to induce distributional shift are revealed.
Our experiments demonstrate that using meta-learning can reveal incentives for ADS, leading learn-
ers to use ADS as a means of increasing performance."
DISCUSSION AND CONCLUSION,0.36186770428015563,"Our work highlights the interdisciplinary nature of issues with real-world deployment of ML systems
– we show how revealing incentives for ADS could play a role in important technosocial issues like
ﬁlter bubbles and the propagation of fake news. There are a number of potential implications for
our work: (1) When ADS are a concern, our methodology and environments can be used to help
diagnose whether and to what extent the ﬁnal performance/behavior of a learner is due to ADS
and/or incentives for ADS, i.e. to quantify their inﬂuence on that learner.
(2) Comparing this
quantitative analysis for different algorithms could help us understand which features of algorithms
affect their propensity to reveal incentives for ADS, and aid in the development of safer and more
robust algorithms.
(3) Characterizing and identifying incentives for ADS in these tests is a ﬁrst
step to analyzing and mitigating other (problematic) incentives, as well as to developing theoretical
understanding of incentives."
DISCUSSION AND CONCLUSION,0.3657587548638132,"Our work emphasizes that the choice of machine learning algorithm plays an important role in
speciﬁcation, independently of the choice of performance metric. A learner can use ADS to increase
performance according to the intended performance metric, and yet still behave in an undesirable
way, if we did not intend the learner to improve performance by that method. In other words,
performance metrics are incomplete speciﬁcations: they only specify our goals or ends, while our
choice of learning algorithm plays a role in specifying the means by which we intend an learner to
achieve those ends. With increasing deployment of ML algorithms in daily life, we believe that (1)
understanding incentives and (2) specifying desired/allowed means of improving performance are
important avenues of future work to ensure fair, robust, and safe outcomes."
DISCUSSION AND CONCLUSION,0.36964980544747084,Under review as a conference paper at ICLR 2022
REFERENCES,0.3735408560311284,REFERENCES
REFERENCES,0.377431906614786,"Hunt Allcott and Matthew Gentzkow. Social media and fake news in the 2016 election. Journal of
Economic Perspectives, 31(2):211–36, May 2017."
REFERENCES,0.38132295719844356,"Michelle A. Amazeen and Bartosz W. Wojdynski. Reducing native advertising deception: Revis-
iting the antecedents and consequences of persuasion knowledge in digital news contexts. Mass
Communication and Society, 0(0):1–26, 2018."
REFERENCES,0.3852140077821012,"Stuart Armstrong and Xavier O’Rorke.
Good and safe uses of ai oracles.
arXiv preprint
arXiv:1711.05541, 2017."
REFERENCES,0.38910505836575876,"Stuart Armstrong and Xavier O’Rourke. Indifference methods for managing agent rewards. Tech-
nical report, Future of Humanity Institute, 2017."
REFERENCES,0.39299610894941633,"K. J. Åström. Optimal control of Markov Processes with incomplete state information. Journal of
Mathematical Analysis and Applications, 10:174–205, January 1965."
REFERENCES,0.3968871595330739,"P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. Gambling in a rigged casino: The adver-
sarial multi-armed bandit problem. In Foundations of Computer Science, 1995."
REFERENCES,0.40077821011673154,"Eytan Bakshy, Solomon Messing, and Lada A. Adamic. Exposure to ideologically diverse news
and opinion on Facebook.
Science, 348(6239):1130–1132, 2015.
ISSN 0036-8075.
doi:
10.1126/science.aaa1160.
URL http://science.sciencemag.org/content/348/
6239/1130."
REFERENCES,0.4046692607003891,"Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, Inc., New
York, NY, USA, 1st edition, 2014."
REFERENCES,0.4085603112840467,"Ryan Carey, Eric Langlois, Tom Everitt, and Shane Legg. The incentives that shape behaviour.
arXiv preprint arXiv:2001.07118, 2020."
REFERENCES,0.41245136186770426,"Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intel-
ligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In
International Conference on Knowledge Discovery and Data Mining, pp. 1721–1730, 2015."
REFERENCES,0.4163424124513619,"Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences, 2017."
REFERENCES,0.42023346303501946,"Michael K. Cohen, Elliot Catt, and Marcus Hutter. A strongly asymptotically optimal agent in
general environments. Proceedings of the Twenty-Eighth International Joint Conference on Ar-
tiﬁcial Intelligence, Aug 2019. doi: 10.24963/ijcai.2019/302. URL http://dx.doi.org/
10.24963/ijcai.2019/302."
REFERENCES,0.42412451361867703,"Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel,
Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdi-
ari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma,
Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan,
Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica
Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymy-
rov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley.
Underspeciﬁcation presents challenges for credibility in modern machine learning, 2020."
REFERENCES,0.4280155642023346,"Dominic DiFranzo and Kristine Gloria-Garcia. Filter bubbles and fake news. XRDS, 23(3):32–35,
April 2017. ISSN 1528-4972. doi: 10.1145/3055153. URL http://doi.acm.org/10.
1145/3055153."
REFERENCES,0.43190661478599224,"Mostafa M. El-Bermawy. Your echo chamber is destroying democracy, 2016. URL https://
www.wired.com/2016/11/filter-bubble-destroying-democracy/."
REFERENCES,0.4357976653696498,"Tom Everitt. Towards Safe Artiﬁcial General Intelligence. PhD thesis, Australian National Univer-
sity, 2018."
REFERENCES,0.4396887159533074,Under review as a conference paper at ICLR 2022
REFERENCES,0.44357976653696496,"Tom Everitt and Marcus Hutter. The alignment problem for bayesian history-based reinforcement
learners. 2018."
REFERENCES,0.4474708171206226,"Tom Everitt and Marcus Hutter. Reward tampering problems and solutions in reinforcement learn-
ing: A causal inﬂuence diagram perspective. arXiv preprint arXiv:1908.04734, 2019."
REFERENCES,0.45136186770428016,"Tom Everitt, Pedro A. Ortega, Elizabeth Barnes, and Shane Legg. Understanding agent incentives
using causal inﬂuence diagrams. part i: Single action settings, 2019."
REFERENCES,0.45525291828793774,"Lisa K. Fazio, Nadia M. Brashier, B. Keith Payne, and Elizabeth J. Marsh. Knowledge does not
protect against illusory truth. Journal of Experimental Psychology: General, 144(5):993–1002,
10 2015. ISSN 0096-3445. doi: 10.1037/xge0000098."
REFERENCES,0.4591439688715953,"Seth Flaxman and Sharad Goel.
Filter bubbles, echo chambers, and online news consumption.
Public Opinion Quarterly, 2015."
REFERENCES,0.46303501945525294,"Mohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Online stochastic opti-
mization under correlated bandit feedback. ArXiv preprint, 2014."
REFERENCES,0.4669260700389105,"Ian J. Goodfellow. A research agenda: Dynamic models to defend against correlated attacks. ArXiv
preprint, 2019."
REFERENCES,0.4708171206225681,"Jacob Groshek and Karolina Koc-Michalska. Helping populism win? Social media use, ﬁlter bub-
bles, and support for populist presidential candidates in the 2016 us election campaign. Informa-
tion, Communication & Society, 20(9):1389–1407, 2017. doi: 10.1080/1369118X.2017.1329334.
URL https://doi.org/10.1080/1369118X.2017.1329334."
REFERENCES,0.47470817120622566,"Dylan Hadﬁeld-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca Dragan. Inverse
reward design. In Neural Information Processing Systems, 2017."
REFERENCES,0.4785992217898833,"Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features, 2019."
REFERENCES,0.48249027237354086,"M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals,
T. Green, I. Dunning, K. Simonyan, C. Fernando, and K. Kavukcuoglu. Population Based Train-
ing of Neural Networks. ArXiv preprint, 2017."
REFERENCES,0.48638132295719844,"Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in par-
tially observable stochastic domains. Artiﬁcial Intelligence, 101(1-2):99–134, May 1998. ISSN
0004-3702."
REFERENCES,0.490272373540856,"Varol Kayhan. Conﬁrmation bias: Roles of search engines and search contexts. In International
Conference on Information Systems, 2015."
REFERENCES,0.49416342412451364,"Jack Koch, Lauro Langosco, Jacob Pfau, James Le, and Lee Sharkey. Objective robustness in deep
reinforcement learning, 2021."
REFERENCES,0.4980544747081712,"Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ra-
mana Kumar, Zac Kenton, Jan Leike, and Shane Legg.
Speciﬁcation gaming:
the
ﬂip side of ai ingenuity, 2020.
URL https://deepmind.com/blog/article/
Specification-gaming-the-flip-side-of-AI-ingenuity."
REFERENCES,0.5019455252918288,"John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Neural Information Processing Systems, 2008."
REFERENCES,0.5058365758754864,"Ed.
Lee
Howell.
Digital
wildﬁres
in
a
hyperconnected
world.
In
Lee
Howell
(ed.),
Global
Risks
2013.
World
Economic
Forum,
2013.
URL
http://reports.weforum.org/global-risks-2013/risk-case-1/
digital-wildfires-in-a-hyperconnected-world/."
REFERENCES,0.5097276264591439,"Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq,
Laurent Orseau, and Shane Legg. AI safety gridworlds. Technical report, DeepMind, 2017."
REFERENCES,0.5136186770428015,Under review as a conference paper at ICLR 2022
REFERENCES,0.5175097276264592,"Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable
agent alignment via reward modeling: a research direction. Technical report, DeepMind Safety
Research, 2018."
REFERENCES,0.5214007782101168,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems, 2020."
REFERENCES,0.5252918287937743,"Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to
personalized news article recommendation. In International Conference on World Wide Web,
2010."
REFERENCES,0.5291828793774319,"Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through hypernet-
works. ArXiv preprint, 2018."
REFERENCES,0.5330739299610895,"D. D. Luxton, J. D. June, and J. M. Fairall. Social media and suicide: A public health perspective.
American journal of public health, 102(2):195–200, 2012."
REFERENCES,0.5369649805447471,"Matthew MacKay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger Grosse.
Self-
tuning networks: Bilevel optimization of hyperparameters using structured best-response func-
tions. ArXiv preprint, 2019."
REFERENCES,0.5408560311284046,"Merriam-Webster.
The
real
story
of
fake
news,
2017.
URL
https://www.
merriam-webster.com/words-at-play/the-real-story-of-fake-news."
REFERENCES,0.5447470817120622,"Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Learning unsuper-
vised learning rules. In International Conference on Learning Representations, 2019."
REFERENCES,0.5486381322957199,"Paul Mihailidis and Samantha Viotty. Spreadable spectacle in digital culture: Civic expression, fake
news, and the role of media literacies in ""post-fact"" society. American Behavioural Scientist,
2017."
REFERENCES,0.5525291828793775,"Jose G. Moreno-Torres, Troy Raeder, RocíO Alaiz-RodríGuez, Nitesh V. Chawla, and Francisco
Herrera. A unifying view on dataset shift in classiﬁcation. Pattern Recognition, 45(1):521–530,
January 2012."
REFERENCES,0.556420233463035,"Tien T. Nguyen, Pik-Mai Hui, F. Maxwell Harper, Loren Terveen, and Joseph A. Konstan. Exploring
the ﬁlter bubble: The effect of using recommender systems on content diversity. In Proceedings
of the 23rd International Conference on World Wide Web, WWW ’14, pp. 677–686, New York,
NY, USA, 2014. ACM. ISBN 978-1-4503-2744-2. doi: 10.1145/2566486.2568012. URL http:
//doi.acm.org/10.1145/2566486.2568012."
REFERENCES,0.5603112840466926,"Saﬁya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. NYC
Press, 2018."
REFERENCES,0.5642023346303502,"Stephen M. Omohundro. The basic AI drives. In Conference on Artiﬁcial General Intelligence,
2008."
REFERENCES,0.5680933852140078,"Pedro A. Ortega, Vishal Maini, et al. Building safe artiﬁcial intelligence: speciﬁcation, robustness,
and assurance, 2018."
REFERENCES,0.5719844357976653,"Eli Pariser. The Filter Bubble: What the Internet Is Hiding from You. The Penguin Group, 2011."
REFERENCES,0.5758754863813229,"Gordon Pennycook, Tyrone D Cannon, and David G. Rand. Prior exposure increases perceived
accuracy of fake news. Journal of Experimental Psychology (forthcoming), 2019."
REFERENCES,0.5797665369649806,"Erich Prisner. Game Theory Through Examples. Mathematical Association of America, 2014."
REFERENCES,0.5836575875486382,"Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence.
Dataset Shift in Machine Learning. The MIT Press, 2009."
REFERENCES,0.5875486381322957,"Neil C. Rabinowitz. Meta-learners’ learning dynamics are unlike learners’. ArXiv preprint, 2019."
REFERENCES,0.5914396887159533,"David Robson. The myth of the online echo chamber, 2018. URL http://www.bbc.com/
future/story/20180416-the-myth-of-the-online-echo-chamber."
REFERENCES,0.5953307392996109,Under review as a conference paper at ICLR 2022
REFERENCES,0.5992217898832685,"Peter Schulam and Suchi Saria. Reliable decision support using counterfactual models. In Neural
Information Processing Systems, 2017."
REFERENCES,0.603112840466926,"Virag Shah, Jose Blanchet, and Ramesh Johari. Bandit learning with positive externalities. Neural
Information Processing Systems, 2018."
REFERENCES,0.6070038910505836,"Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng Yang, Alessandro Flammini,
and Filippo Menczer. The spread of low-credibility content by social bots. Nature Communica-
tions, 9(4787), 2018."
REFERENCES,0.6108949416342413,"Richard S Sutton and Andrew G Barto. Introduction to Reinforcement Learning. MIT Press, 1998."
REFERENCES,0.6147859922178989,"Richard S Sutton, Anna Koop, and David Silver. On the role of tracking in stationary environments.
In International conference on Machine learning, 2007."
REFERENCES,0.6186770428015564,"Techopedia. Filter bubble, 2018. URL https://www.techopedia.com/definition/
28556/filter-bubble."
REFERENCES,0.622568093385214,"Chih-Chun Wang, Sanjeev R Kulkarni, and H Vincent Poor. Bandit problems with side observations.
IEEE Transactions on Automatic Control, 50(3):338–355, 2005."
REFERENCES,0.6264591439688716,"Wikipedia contributors.
Conﬁrmation bias — Wikipedia, the free encyclopedia, 2018.
URL
https://en.wikipedia.org/w/index.php?title=Confirmation_bias&
oldid=875026726. [Online; accessed 20-January-2019]."
REFERENCES,0.6303501945525292,"Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. In Machine Learning, pp. 229–256, 1992."
REFERENCES,0.6342412451361867,Under review as a conference paper at ICLR 2022
REFERENCES,0.6381322957198443,"A
CONTENT RECOMMENDATION IN THE WILD"
REFERENCES,0.642023346303502,"Filter bubbles, the spread of fake news, and other techno-social issues are widely reported to be
responsible for the rise of populism (Groshek & Koc-Michalska, 2017), increase in racism and
prejudice against immigrants and refugees (Noble, 2018), increase in social isolation and suicide
(Luxton et al., 2012), and, particularly with reference to the 2016 US elections, are decried as
threatening the foundations of democracy (El-Bermawy, 2016). Even in 2013, well before the 2016
American elections, a World Economic Forum report identiﬁed these problems as a global crisis
(Lee Howell, 2013)."
REFERENCES,0.6459143968871596,"We focus on two related issues in which content recommendation algorithms play a role: fake news
and ﬁlter bubbles."
REFERENCES,0.6498054474708171,"A.1
FAKE NEWS"
REFERENCES,0.6536964980544747,"Fake news (also called false news or junk news) is an extreme version of yellow journalism, pro-
paganda, or clickbait, in which media that is ostensibly providing information focuses on being
eye-catching or appealing, at the expense of the quality of research and exposition of factual infor-
mation. Fake news is distinguished by being speciﬁcally and deliberately created to spread false-
hoods or misinformation (Merriam-Webster, 2017; Mihailidis & Viotty, 2017)."
REFERENCES,0.6575875486381323,"Why does fake news spread? It may at ﬁrst seem the solution is simply to educate people about the
truth, but research tells us the problem is more multifaceted and insidious, due to a combination of
related biases and cognitive effects including conﬁrmation bias (people are more likely to believe
things that ﬁt with their existing beliefs), priming (exposure to information unconsciously inﬂuences
the processing of subsequent information, i.e. seeing something in a credible context makes things
seem more credible) and the illusory truth effect (i.e. people are more likely to believe something
simply if they are told it is true)."
REFERENCES,0.6614785992217899,"Allcott & Gentzkow (2017) track about 150 fake news stories during the 2016 US election, and
ﬁnd the average American adult saw 1-2 fake news stories, just over half believed the story was
true, and likelihood of believing fake news increased with ideological segregation (polarization)
of their social media. Shao et al. (2018) examine the role of social bots in spreading fake news
by analyzing 14 million Twitter messages. They ﬁnd that bots are far more likely than humans
to spread misinformation, and that success of a fake news story (in terms of human retweets) was
heavily dependent on whether bots had shared the story."
REFERENCES,0.6653696498054474,"Pennycook et al. (2019) examine the role of the illusory truth effect in fake news. They ﬁnd that
even a single exposure to a news story makes people more likely to believe that it is true, and repeat
viewings increase this likelihood. They ﬁnd that this is not true for extremely implausible statements
(e.g. “the world is a perfect cube”), but that “only a small degree of potential plausibility is sufﬁcient
for repetition to increase perceived accuracy” of the story. The situation is further complicated by
peoples’ inability to distinguish promoted content from real news - Amazeen & Wojdynski (2018)
ﬁnd that fewer than 1/10 people were able to tell when content was an advertisement, even when it
was explicitly labelled as such. Similarly, Fazio et al. (2015) ﬁnd that repeated exposure to incorrect
trivia make people more likely to believe it, even when they are later able to identify the trivia as
incorrect."
REFERENCES,0.669260700389105,"A.2
FILTER BUBBLES"
REFERENCES,0.6731517509727627,"Filter bubbles, a term coined and popularized by Pariser (2011) are created by positive or negative
feedback loops which encourage users or groups of users towards increasing within-group similarity,
while driving up between-group dissimilarity. The curation of this echo chamber is called self-
selection (people are more likely to look for or select things that ﬁt their existing preferences), and
favours what Techopedia (2018) calls intellectual isolation. In the context of social and political
opinions, this is often called the polarization effect (Wikipedia contributors, 2018)."
REFERENCES,0.6770428015564203,"Filter bubbles can be encouraged by algorithms in two main ways. The ﬁrst is the most commonly
described: simply by showing content that is similar to what a user has already searched for, search
or recommender systems create a positive feedback loop of increasingly-similar content (Pariser,
2011; Kayhan, 2015). The second way is similar but opposite - if the predictions of an algorithm"
REFERENCES,0.6809338521400778,Under review as a conference paper at ICLR 2022
REFERENCES,0.6848249027237354,"are good for a certain group of people, but bad for others, the algorithm can do better on its metrics
by driving hard-to-predict users away. Then new users to the site will either be turned off entirely,
or see an artiﬁcially homogenous community of like-minded peers, a phenomena Shah et al. (2018)
call positive externalities."
REFERENCES,0.688715953307393,"In a study of 50,000 US-based internet users, Flaxman & Goel (2015) ﬁnd that two things increase
with social media and search engine use: (1) exposure of an individual to opposing or different
viewpoints, and (2) mean ideological distance between users. Many studies cite the ﬁrst result as
evidence of the beneﬁts of internet and social media (Robson, 2018; Bakshy et al., 2015), but the
correlation of exposure with ideological distances demonstrates that exposure is not enough, and
might even be counterproductive."
REFERENCES,0.6926070038910506,"Facebook’s own study on ﬁlter bubbles results show that the impact of the news feed algorithm on
ﬁlter bubble “size” (a measure of homogeneity of posts relative to a baseline) is almost as large as
the impact of friend group composition (Bakshy et al., 2015). Kayhan (2015) speciﬁcally study the
role of search engines in conﬁrmation bias, and ﬁnd that search context and the similarity of results
in search engine results both reinforce existing biases and increase the likelihood of future biased
searches. Nguyen et al. (2014) similarly study the effect of recommender systems on individual
users’ content diversity, and ﬁnd that the set of options recommended narrows over time."
REFERENCES,0.6964980544747081,"Filter bubbles create an ideal environment for the spread of fake news: they increase the likelihood
of repeat viewings of similar content, and because of the illusory truth effect, that content is more
likely to be believed and shared (Pennycook et al., 2019; DiFranzo & Gloria-Garcia, 2017; Pariser,
2011). We are not claiming that incentives for ADS are entirely or even mostly responsible for these
problems, but we do note that they can play a role that is worth addressing."
REFERENCES,0.7003891050583657,Under review as a conference paper at ICLR 2022
REFERENCES,0.7042801556420234,incentive-compatible (β = 0.5)
REFERENCES,0.708171206225681,incentive-orthogonal (β = 0.0)
REFERENCES,0.7120622568093385,incentive-opposed (β = −0.5)
REFERENCES,0.7159533073929961,"Figure 7: Average level of non-myopic (i.e. cooperate) behavior learned by agents in the unit
test for incentives for ADS. Despite making the inner loop fully myopic (γ = 0), population-based
training (PBT) can reveal incentives for ADS, leading agents to choose the cooperate action (top
row). context swapping successfully prevents this (bottom row). Columns (from left to right) show
results for populations of 10, 100, and 1000 learners. In the legend, “interval” refers to the interval
(T) of PBT (see Sec. 2.2). Sufﬁciently large populations and short intervals are necessary for PBT
to induce nonmyopic behavior."
REFERENCES,0.7198443579766537,Under review as a conference paper at ICLR 2022
REFERENCES,0.7237354085603113,"B
ADS INCENTIVE UNIT TEST (SUPERVISED LEARNING)"
REFERENCES,0.7276264591439688,"This unit test consists of a simple prediction problem. There are no inputs, only an underlying state
s ∈{0, 1}, and targets y ∈R2 with y1, y2 ∼N(0, s ∗σ2), N(0, 1), with corresponding predictions
ˆy1, ˆy2. Additionally, st+1 = 0 iff ˆy2 > .5. We use Mean Squared Error as the loss function, so the
optimal predictor is ˆy1, ˆy2 = (0, 0). However, predicting ˆy2 > .5 reduces the variance of ˆy1, i.e.
reduces future loss."
REFERENCES,0.7315175097276264,"The baseline/IL predictor learns ˆy1, ˆy2 as parameters using SGD with a learning rate of 0.001. For
experiments with meta-learning, PBT is the OL (with default settings, see Section 2.2), used to tune
the learning rate, with negative loss on the ﬁnal time-step of the interval as the performance measure
for PBT. 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.7354085603112841,||y2||
REFERENCES,0.7392996108949417,"#agents=10
#agents=100"
REFERENCES,0.7431906614785992,no env swapping
REFERENCES,0.7470817120622568,#agents=1000
REFERENCES,0.7509727626459144,"0
200
400
600
800
1000
time-step 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.754863813229572,||y2||
REFERENCES,0.7587548638132295,"0
200
400
600
800
1000
time-step"
REFERENCES,0.7626459143968871,"0
200
400
600
800
1000
time-step"
REFERENCES,0.7665369649805448,env swapping
REFERENCES,0.7704280155642024,"Figure 8: Results on the Supervised Learning ADS unit test mirror those on the RL unit test.
PBT produces larger values of ˆy2, sacriﬁcing present performance for future performance (i.e. non-
myopic exploitation of ADS)."
REFERENCES,0.77431906614786,"C
EXTRA EXPERIMENTS AND REPRODUCIBILITY DETAILS"
REFERENCES,0.7782101167315175,"C.1
ADS INCENTIVE UNIT TEST"
REFERENCES,0.7821011673151751,"C.1.1
FORMAL DEFINITION OF MYOPIC RL ADS UNIT TEST ENVIRONMENT"
REFERENCES,0.7859922178988327,"Formally, this environment is not a 2x2 game (as the original prisoner’s dilemma); it’s a partially
observable Markov Decision Process (Åström, 1965; Kaelbling et al., 1998):
st, ot =
at−1, {}
at ∈{defect, cooperate}
P(st, at) = δ(at)
R(st, at) = I(st = cooperate) +
β I(at = cooperate) −1/2"
REFERENCES,0.7898832684824902,"where I is an indicator function, and β = −1/2 is a parameter controlling the alignment of incen-
tives (see Appendix 3.2 for an exploration of different β values.). The initial state is sampled as
s0 ∼U(defect, cooperate)."
REFERENCES,0.7937743190661478,"C.1.2
ALIGNMENT OF INCENTIVES EXPLORATION"
REFERENCES,0.7976653696498055,"This section presents an exploration of the parameter β, which controls the alignment of incentives
in the Myopic RL unit test (see Table 2)."
REFERENCES,0.8015564202334631,"To clarify the interpretation of experiments, we distinguish between environments in which myopic
(defect) vs. nonmyopic (cooperate) incentives are opposed, orthogonal, or compatible. Note that
in this unit test myopic behaviour (defection) is what we want to see."
REFERENCES,0.8054474708171206,"1. Incentive-opposed: Optimal myopic behavior is incompatible with optimal nonmyopic be-
havior (classic prisoner’s dilemma; these experiments are in the main paper)."
REFERENCES,0.8093385214007782,Under review as a conference paper at ICLR 2022
REFERENCES,0.8132295719844358,"2. Incentive-orthogonal: Optimal myopic behavior may or may not be optimal nonmyopic be-
havior."
REFERENCES,0.8171206225680934,"3. Incentive-compatible: Optimal myopic behavior is necessarily also optimal nonmyopic be-
havior."
REFERENCES,0.8210116731517509,"We focused on incentive-opposed environment (β = −1/2) in the main paper in order to demon-
strate that incentives for ADS can be powerful enough to change the behavior of the system in an
undesirable way. Here we also explore incentive-compatible and incentive-orthogonal environments
because they provide useful baselines, helping us distinguish a systematic bias towards nonmyopic
behavior from other reasons (such as randomness or optimization issues) for behavior that does not
follow a myopically optimal policy."
REFERENCES,0.8249027237354085,"C.1.3
WORKING THROUGH A DETAILED EXAMPLE FOR PBT WITH T = 1"
REFERENCES,0.8287937743190662,"To help provide intuition on how (mechanistically) PBT could lead to persistent levels of coopera-
tion, we walk through a simple example (with no inner loop). Consider PBT with T = 1 and a pop-
ulation of 5 deterministic agents A1, ..., A5 playing cooperate and receiving reward of r(Ai) = 0.
Now suppose A1 suddenly switches to play defect. Then r(A1) = 1/2 on the next time-step (while
the other agents’ reward is still 0), and so PBT’s EXPLOIT step will copy A1 (without loss of gen-
erality to A2). On the following time-step, r(A2) = 1/2, and r(A1) = −1/2, so PBT will clone A2
to A1, and the cycle repeats. Similar reasoning applies for larger populations, and T > 1."
REFERENCES,0.8326848249027238,Table 2: β controls the extent to which myopic and nonmyopic incentives are aligned.
REFERENCES,0.8365758754863813,"β
Environment
Cooperating
< 0
incentive-opposed
yields less reward on the current time-step (myopically detrimental)
= 0
incentive-orthogonal
does not affect the current reward (myopically indifferent)
> 0
incentive-compatible
yields more reward on the current time-step (myopically beneﬁcial)"
REFERENCES,0.8404669260700389,"C.1.4
Q-LEARNING EXPERIMENT DETAILS"
REFERENCES,0.8443579766536965,"We show that, under certain conditions, Q-learning can learn to (primarily) cooperate, and thus fails
the Myopic RL unit test. We estimate Q-values using the sample-average method, which is guar-
anteed to converge in the fully observed, tabular case (Sutton & Barto, 1998). The agent follows
the ϵ-greedy policy with ϵ = 0.1. In order to achieve this result, we additionally start the agent off
with one synthetic memory where both state and action are defect and therefor R(defect) = −.5,
and we hard-code the starting state to be cooperate (which normally only happens 50% of the
time).
Without this kind of an initialization, the agent always learns to defect.
However, un-
der these conditions, we ﬁnd that 10/30 agents learned to play cooperate most of the time, with
Q(cooperate) and Q(defect) both hovering around −0.07, while others learn to always defect, with
Q(cooperate) ≈−0.92 and Q(defect) ≈−0.45. context swapping, however, prevents majority-
cooperate behavior from ever emerging, see Figure 11."
REFERENCES,0.8482490272373541,"C.1.5
OFFLINE Q-LEARNING CAN REVEAL INCENTIVES FOR ADS"
REFERENCES,0.8521400778210116,"In practice, RL agents are often trained ofﬂine (Levine et al., 2020). Incentives for ADS can still be
revealed in ofﬂine RL, even though the learner cannot inﬂuence its data distribution. In particular,
while Q(D) > Q(C) for data from a single policy, this does not always hold when pooling data
from different policies, see Figure 5. Intuitively, pooling data from 2 policies is similar to collecting
data from an ϵ-greedy policy trained online (as in Figure 5). This sort of data and approach in
very common in real world applications, including content recommendation, and more generally,
“A/B testing”, where 2 groups of users are assigned to 2 different policies, in order to compare the
policies’ performance."
REFERENCES,0.8560311284046692,Under review as a conference paper at ICLR 2022
REFERENCES,0.8599221789883269,"C.1.6
Q-LEARNING: FURTHER RESULTS"
REFERENCES,0.8638132295719845,"To give a more representative picture of how often Q-learning fails the unit test, we run a larger set
of experiments with Q-learning, results are in Figure 10. It’s possible that the failure of Q-learning
is not persistent, since we have not proved otherwise, but we did run much longer experiments and
still observe persistent failure, see Figure 9."
REFERENCES,0.867704280155642,"Figure 9: The same experiments as Figures 5, 10, run for 50,000 time-steps instead of 3000, to
illustrate the persistence of non-myopic behavior."
REFERENCES,0.8715953307392996,Under review as a conference paper at ICLR 2022
REFERENCES,0.8754863813229572,"Figure 10: More independent experiments with Q-learning, exactly following Figure 5. Q-learning
fails the unit test in a total of 10/30 experiments (including those from Figure 5)."
REFERENCES,0.8793774319066148,Under review as a conference paper at ICLR 2022
REFERENCES,0.8832684824902723,"Figure 11: More independent experiments with Q-learning, exactly following Figure 5, except also
using context swapping. This leads to a 100% success rate on the unit test."
REFERENCES,0.8871595330739299,Under review as a conference paper at ICLR 2022
REFERENCES,0.8910505836575876,"C.2
CONTENT RECOMMENDATION"
REFERENCES,0.8949416342412452,"C.2.1
ENVIRONMENT DETAILS"
REFERENCES,0.8988326848249028,The evironment has the following components:
REFERENCES,0.9027237354085603,"1. User type, xt: categorical variable representing different types of users. The content rec-
ommender conditions its predictions on the type of the current user.
2. User loyalty, gt: the propensity for users of each type to use the platform. User xt is
sampled from a categorical distribution with parameters given by softmax(gt).
3. Article type, yt: a categorical variable (one-hot encoding) representing the type of article
selected by the user.
4. User interests, Wt: a matrix whose entries W t
x,y represent the average interest user of
type x have in articles of type y."
REFERENCES,0.9066147859922179,"At each time step t, a user xt is sampled from a categorical distribution (based on the loyalty of the
different user types), then the recommendation system selects which type of article to present in the
top position, and ﬁnally, the user selects an article. The goal of the recommendation system is to
predict the likelihood that the user would click on each of the available articles, in order to select the
one which is most interesting to the user."
REFERENCES,0.9105058365758755,"User loyalty for xt then changes in accordance with the self-selection effect, increasing or decreasing
proportionally to their interest in the top article. The interests of user type xt (represented by a
column of Wt) also change; in accordance with the illusory truth effect, their interest in the topic of
the top article (as chosen by the recommender system) always increases. Overall, this environment
is an extremely crude representation of reality, but it allows us to incorporate both the effects of
self-selection (via covariate shift), and the illusory truth effect (via concept shift)."
REFERENCES,0.914396887159533,"Formally, this environment is similar to a POMDP\R, i.e. a POMDP with no reward function,
also known as a world model (Armstrong & O’Rourke, 2017; Hadﬁeld-Menell et al., 2017); the
difference is that the learner observes the input before acting and only observes the target after
acting. The states, observations, and actions given below."
REFERENCES,0.9182879377431906,"st = (gt, Wt, xt, yt)"
REFERENCES,0.9221789883268483,"ot
pre, at, ot
post = (xt, ˆyt, yt)"
REFERENCES,0.9260700389105059,The state transition function is deﬁned by:
REFERENCES,0.9299610894941635,"gt+1
xt
= gt
xt + α1W t
xt,ˆyt"
REFERENCES,0.933852140077821,"Wt+1/2
xt,ˆyt = W t
xt,ˆyt + α2;
Wt+1
xt
=
Wt+1/2
xt"
REFERENCES,0.9377431906614786,"∥Wt+1/2
xt
∥2
xt+1 ∼softmax(gt+1)"
REFERENCES,0.9416342412451362,"yt+1 ∼softmax(Wt+1
xt+1)"
REFERENCES,0.9455252918287937,"Where ˆyt is the top article as chosen by the recommender, and α1, α2 represent the rate of covariate
and concept shift (respectively). The update for Wt+1 merely increases the interest of user type xt
in article type ˆyt, then normalizes the interests for that user type."
REFERENCES,0.9494163424124513,"C.2.2
REPRODUCIBILITY DETAILS"
REFERENCES,0.953307392996109,"For these experiments, the recommendation system is a ReLU-MLP with 1 hidden layer of 100
units, trained via supervised learning with SGD (learning rate = 0.01) to predict which article a user
will select. Actions are sampled from the MLP’s predictive distribution. We apply PBT without
any hyperparameter selection (this amounts to just doing the EXPLOIT step), and an interval of 10,
selecting on accuracy. We use a population of 20 learners (whether applying PBT or not), and match
random seeds for the trials with and without PBT. We initialize g1 and W1 to be the same across the
20 copies of the environment (i.e. the learners start with the same user population), but these values
diverge throughout learning. For the environment, we set the number of user and article types both
to 10. Initial user loyalties are randomly sampled from N(0, 0.03), α1 = 0.03, and α2 = 0.003."
REFERENCES,0.9571984435797666,Under review as a conference paper at ICLR 2022
REFERENCES,0.9610894941634242,"C.2.3
CONTEXT SWAPPING IN CONTENT RECOMMENDATION"
REFERENCES,0.9649805447470817,"We believe context swapping is not appropriate for the content recommendation environment, since
when the environments diverge, optimal behavior may differ across environments. Nevertheless, we
ran experiments with it for completeness. The main effect appears to be to hamper learning when
PBT is not used, see Figure 12. Notably, it does not appear to signiﬁcantly inﬂuence the rate or
extent of ADS when combined with PBT."
REFERENCES,0.9688715953307393,"C.2.4
EXPLORATION OF ENVIRONMENT PARAMETERS"
REFERENCES,0.9727626459143969,"In Figure 13, we examine the effect of the rate-of-change parameters (α1, α2) of the content rec-
ommendation environment on the results provided in the paper. As noted there, our results are
qualitatively consistent so long as (1) the initial user distribution is approximately uniform, and (2)
the covariate shift rate (α1) is faster than the concept shift rate (α2). These distributions are updated
by different mechanisms, and are not directly comparable. Concept shift changes the task more
radically, requiring a learner to change its predictions, rather than just become accurate on a wider
range of inputs. We conjecture that changes in P(y|x) must therefore be kept smooth enough for
the outer loop to have pressure to capitalize on ADS."
REFERENCES,0.9766536964980544,"Figure 12: Context swapping doesn’t have the desired effect in the content recommendation envi-
ronment."
REFERENCES,0.980544747081712,Under review as a conference paper at ICLR 2022
REFERENCES,0.9844357976653697,"α1 = 0.01, α2 = 0.001
α1 = 0.1, α2 = 0.001"
REFERENCES,0.9883268482490273,"α1 = 0.01, α2 = 0.01
α1 = 0.1, α2 = 0.01"
REFERENCES,0.9922178988326849,"α1 = 0.01, α2 = 0.1
α1 = 0.1, α2 = 0.1"
REFERENCES,0.9961089494163424,"Figure 13: Content recommendation results for different values of α1, α2."
