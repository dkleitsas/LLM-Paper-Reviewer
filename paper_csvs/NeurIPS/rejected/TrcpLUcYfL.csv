Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002,"The ubiquitous backpropagation algorithm requires sequential updates across
1"
ABSTRACT,0.004,"blocks of a network, introducing a locking problem. Moreover, backpropaga-
2"
ABSTRACT,0.006,"tion relies on the transpose of weight matrices to calculate updates, introducing a
3"
ABSTRACT,0.008,"weight transport problem across blocks. Both these issues prevent efficient par-
4"
ABSTRACT,0.01,"allelisation and horizontal scaling of models across devices. We propose a new
5"
ABSTRACT,0.012,"method that introduces a twin network that propagates information backwards from
6"
ABSTRACT,0.014,"the targets to the input to provide auxiliary local losses. Forward and backward
7"
ABSTRACT,0.016,"propagation can work in parallel and with different sets of weights, addressing the
8"
ABSTRACT,0.018,"problems of weight transport and locking. Our approach derives from a statistical
9"
ABSTRACT,0.02,"interpretation of end-to-end training which treats activations of network layers as
10"
ABSTRACT,0.022,"parameters of probability distributions. The resulting learning framework uses
11"
ABSTRACT,0.024,"these parameters locally to assess the matching between forward and backward
12"
ABSTRACT,0.026,"information. Error backpropagation is then performed locally within each block,
13"
ABSTRACT,0.028,"leading to “block-local” learning. Several previously proposed alternatives to error
14"
ABSTRACT,0.03,"backpropagation emerge as special cases of our model. We present results on vari-
15"
ABSTRACT,0.032,"ous tasks and architectures, including transformers, demonstrating state-of-the-art
16"
ABSTRACT,0.034,"performance using block-local learning. These results provide a new principled
17"
ABSTRACT,0.036,"framework to train very large networks in a distributed setting and can also be
18"
ABSTRACT,0.038,"applied in neuromorphic systems.
19"
INTRODUCTION,0.04,"1
Introduction
20"
INTRODUCTION,0.042,"Recent developments in machine learning have seen deep neural network architectures scaling to
21"
INTRODUCTION,0.044,"billions of parameters [Touvron et al., 2023, Brown et al., 2020]. This development has boosted
22"
INTRODUCTION,0.046,"the capabilities of these models to unprecedented levels but simultaneously pushed the computing
23"
INTRODUCTION,0.048,"hardware on which large network models are running to its limits. It is therefore becoming increas-
24"
INTRODUCTION,0.05,"ingly important to distribute learning algorithms over a large number of independent compute nodes.
25"
INTRODUCTION,0.052,"However, today’s machine learning algorithms are ill-suited for distributed computing. The error
26"
INTRODUCTION,0.054,"backpropagation (backprop) algorithm requires an alternation of inter-depended forward and back-
27"
INTRODUCTION,0.056,"ward phases, introducing a locking problem (the two phases have to wait for each other) [Jaderberg
28"
INTRODUCTION,0.058,"et al., 2016a]. Furthermore, the two phases rely on the same weight matrices to calculate updates,
29"
INTRODUCTION,0.06,"introducing a weight transport problem across blocks [Grossberg, 1987, Lillicrap et al., 2014a]. These
30"
INTRODUCTION,0.062,"two issues make efficient parallelisation and horizontal scaling of large machine learning models
31"
INTRODUCTION,0.064,"across compute nodes extremely difficult.
32"
INTRODUCTION,0.066,"We propose a new method to address these problems by distributing a globally defined optimisation
33"
INTRODUCTION,0.068,"algorithm across a large network of nodes that use only local learning. Our approach uses a message-
34"
INTRODUCTION,0.07,"passing approach that uses results from probabilistic models and communicates uncertainty messages
35"
INTRODUCTION,0.072,"forward and backwards between compute nodes in parallel. To do so, we augment a network
36"
INTRODUCTION,0.074,"architecture with a twin network that propagates information backwards from the targets to the
37"
INTRODUCTION,0.076,"input to provide uncertainty measures and auxiliary targets for local losses. Forward and backward
38"
INTRODUCTION,0.078,"messages comprise information about extracted features and feature uncertainties and are matched
39"
INTRODUCTION,0.08,"against each other using local probabilistic losses. Importantly, forward and backward propagation can
40"
INTRODUCTION,0.082,"work in parallel, reducing the locking problem. Inside each block, conventional error backpropagation
41"
INTRODUCTION,0.084,"is performed locally (“block-local”). These local updates can be used in the forward network and its
42"
INTRODUCTION,0.086,"backward twin for adapting parameters during training. The developed theoretical learning provides a
43"
INTRODUCTION,0.088,"new principled method to distribute very large networks over multiple compute nodes. The solutions
44"
INTRODUCTION,0.09,"emerging from this framework show striking similarities to earlier models that used random feedback
45"
INTRODUCTION,0.092,"weights as local targets [Lillicrap et al., 2020, Frenkel et al., 2021] but also provide a principled way
46"
INTRODUCTION,0.094,"to train these feedback weights.
47"
INTRODUCTION,0.096,"In summary, the contribution of this paper is threefold:
48"
WE PROVIDE A THEORETICAL FRAMEWORK ON HOW INTERPRETING THE REPRESENTATIONS OF DEEP NEURAL,0.098,"1. We provide a theoretical framework on how interpreting the representations of deep neural
49"
WE PROVIDE A THEORETICAL FRAMEWORK ON HOW INTERPRETING THE REPRESENTATIONS OF DEEP NEURAL,0.1,"networks as probability distributions provides a principled approach for block-local training
50"
WE PROVIDE A THEORETICAL FRAMEWORK ON HOW INTERPRETING THE REPRESENTATIONS OF DEEP NEURAL,0.102,"of these networks. This can be used to distribute learning and inference over many interacting
51"
WE PROVIDE A THEORETICAL FRAMEWORK ON HOW INTERPRETING THE REPRESENTATIONS OF DEEP NEURAL,0.104,"neural network blocks for various neural network architectures.
52"
WE DEMONSTRATE AN INSTANCE OF THIS PROBABILISTIC LEARNING MODEL ON SEVERAL BENCHMARK,0.106,"2. We demonstrate an instance of this probabilistic learning model on several benchmark
53"
WE DEMONSTRATE AN INSTANCE OF THIS PROBABILISTIC LEARNING MODEL ON SEVERAL BENCHMARK,0.108,"classification tasks, where classifiers are split into multiple blocks and trained without
54"
WE DEMONSTRATE AN INSTANCE OF THIS PROBABILISTIC LEARNING MODEL ON SEVERAL BENCHMARK,0.11,"end-to-end gradient computation.
55"
WE DEMONSTRATE HOW THIS FRAMEWORK CAN BE USED TO ALLOW DEEP NETWORKS TO PRODUCE,0.112,"3. We demonstrate how this framework can be used to allow deep networks to produce
56"
WE DEMONSTRATE HOW THIS FRAMEWORK CAN BE USED TO ALLOW DEEP NETWORKS TO PRODUCE,0.114,"uncertainty estimates over their predictions. This principle is showcased on an autoencoder
57"
WE DEMONSTRATE HOW THIS FRAMEWORK CAN BE USED TO ALLOW DEEP NETWORKS TO PRODUCE,0.116,"network that automatically predicts uncertainties alongside pixel intensity values after
58"
WE DEMONSTRATE HOW THIS FRAMEWORK CAN BE USED TO ALLOW DEEP NETWORKS TO PRODUCE,0.118,"training.
59"
RELATED WORK,0.12,"2
Related work
60"
RELATED WORK,0.122,"A number of methods for using local learning in DNNs had been introduced previously. Lomnitz et al.
61"
RELATED WORK,0.124,"[2022] introduced Target Projection Stochastic Gradient Descent (tpSGD), which uses layer-wise
62"
RELATED WORK,0.126,"SGD and local targets generated via random projections of the labels, but does not adapt the backward
63"
RELATED WORK,0.128,"weights. LocoProp [Amid et al., 2022] uses a layer-wise loss that consists of a target term and a
64"
RELATED WORK,0.13,"regularizer, which is used however to enable 2nd order learning and does not focus on distributing
65"
RELATED WORK,0.132,"the gradient optimization. Jimenez Rezende et al. [2016] used a generative model and a KL-loss for
66"
RELATED WORK,0.134,"local unsupervised learning of 3D structures.
67"
RELATED WORK,0.136,"Some previous methods are based on probabilistic or energy-based cost functions and use a contrastive
68"
RELATED WORK,0.138,"approach with positive and negative data samples. Contrastive learning Chen et al. [2020], Oord
69"
RELATED WORK,0.14,"et al. [2019] can be used to construct block-local losses Xiong et al. [2020], Illing et al. [2021].
70"
RELATED WORK,0.142,"Equilibrium propagation replaces target clamping with a target nudging phase [Scellier and Bengio,
71"
RELATED WORK,0.144,"2017]. Another interesting contrastive approach was recently introduced [Hinton, 2022, Ororbia and
72"
RELATED WORK,0.146,"Mali, 2023, Zhao et al., 2023]. However, it needs task-specific negative examples. [Han et al., 2018]
73"
RELATED WORK,0.148,"uses a local predictive loss to improve recurrent networks’ performance. In contrast to these methods,
74"
RELATED WORK,0.15,"our approach does not need separate positive and negative data samples and focuses on block-local
75"
RELATED WORK,0.152,"learning.
76"
RELATED WORK,0.154,"Feedback alignment [Lillicrap et al., 2020, Sanfiz and Akrout, 2021] uses random projections to
77"
RELATED WORK,0.156,"propagate gradient information backwards. Jaderberg et al. [2016b] used pseudo-reward functions
78"
RELATED WORK,0.158,"which are optimized simultaneously by reinforcement learning to improve performance. Random
79"
RELATED WORK,0.16,"feedback alignment [Amid et al., 2022, Refinetti et al., 2021] and related approaches [Clark et al., 2021,
80"
RELATED WORK,0.162,"Nøkland, 2016, Launay et al., 2020], use fixed random feedback weights to back-propagate errors.
81"
RELATED WORK,0.164,"[Jaderberg et al., 2017] used decoupled synthetic gradients for local training. Target propagation
82"
RELATED WORK,0.166,"demonstrates non-trivial performance with random projections for target labels instead of errors
83"
RELATED WORK,0.168,"[Frenkel et al., 2021]. In contrast to these methods, we provide a principled way to adapt feedback
84"
RELATED WORK,0.17,"weights.
85"
RELATED WORK,0.172,"Other methods [Belilovsky et al., 2019, Löwe et al., 2019] used greedy local, block- or layer-wise
86"
RELATED WORK,0.174,"optimization. Notably, Nøkland and Eidnes [2019] achieved good results by combining a matching
87"
RELATED WORK,0.176,"and a local cross-entropy loss. [Siddiqui et al., 2023] recently used block-local learning based on a
88"
RELATED WORK,0.178,"cross-correlation metric over feature embeddings [Zbontar et al., 2021], demonstrating promising
89"
RELATED WORK,0.18,"Figure 1: Illustration of use of block-local representations as learning signals on intermediate network
layers. A deep neural network architecture NA is split into multiple blocks (forward blocks) and
trained on an auxiliary local loss. Targets for local losses are provided by a twin backward network
NB."
RELATED WORK,0.182,"performance. [Wu et al., 2021] used greedy layer-wise optimization of hierarchical autoencoders for
90"
RELATED WORK,0.184,"video prediction. [Wu et al., 2022] used an encoder-decoder stage for pretraining. In contrast to these
91"
RELATED WORK,0.186,"methods, we do not rely solely on local greedy optimization but provide a principled way to combine
92"
RELATED WORK,0.188,"local losses with feedback information without locking and weight transport across blocks.
93"
A PROBABILISTIC FORMULATION OF DISTRIBUTED LEARNING,0.19,"3
A probabilistic formulation of distributed learning
94"
A PROBABILISTIC FORMULATION OF DISTRIBUTED LEARNING,0.192,"At a high level, our method interprets the activations of a neural network as the parameters of
95"
A PROBABILISTIC FORMULATION OF DISTRIBUTED LEARNING,0.194,"probability distributions of latent variables. We use these intermediate representations at each block
96"
A PROBABILISTIC FORMULATION OF DISTRIBUTED LEARNING,0.196,"to derive block local losses. These latent variables over multiple blocks implicitly define a Markov
97"
A PROBABILISTIC FORMULATION OF DISTRIBUTED LEARNING,0.198,"chain, which allows us to tractably minimize the block’s local loss. We show that the derived block
98"
A PROBABILISTIC FORMULATION OF DISTRIBUTED LEARNING,0.2,"local losses and the resulting block local learning (BLL) are a general form of various existing local
99"
A PROBABILISTIC FORMULATION OF DISTRIBUTED LEARNING,0.202,"losses and provide an upper bound to a global loss.
100"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.204,"3.1
Using latent representations to construct probabilistic block-local losses
101"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.206,"Learning in deep neural networks can be formulated probabilistically [Ghahramani, 2015] in
102"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.208,"terms of maximum likelihood, i.e.
the problem is to minimize the negative log-likelihood
103"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.21,"L
=
−log p (x, y)
=
−log p (y | x) −log p (x) with respect to the network parameters θ.
104"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.212,"For many practical cases where we may not be interested in the prior distribution p (x), we would
105"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.214,"like to directly minimize L = −log p (y | x).
106"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.216,"This probabilistic interpretation of deep learning can be used to define block-local losses and distribute
107"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.218,"the learning over multiple blocks of networks by introducing intermediate latent representations. The
108"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.22,"idea is illustrated in Fig. 1. A neural network that computes the distribution log p (y | x) takes x as
109"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.222,"input and outputs the statistical parameters to the conditional distribution. The deep neural network
110"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.224,"is split at an intermediate layer k (in Fig. 1 we used k ∈(1, 2)) and end-to-end estimation of the
111"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.226,"gradient is replaced by two estimators that optimize the sub-networks x →zk and zk →y separately.
112"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.228,"To do this, consider the gradient of the log-likelihood loss function
113 −∂"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.23,"∂θL =
∂
∂θ log p (y | x) .
(1)"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.232,"For any deep network, it is possible to choose any intermediate activation at layer k as latent
114"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.234,"representations zk, such that log p (y | x) =
D
p (y | zk) p (zk | x)
E"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.236,"p(zk | x,y), where
D E"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.238,"p denotes
115"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.24,"expectation with respect to p. Therefore, the representations of y depend on x only through zk as
116"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.242,"expected for a feed-forward network. Using this conditional independence property, the log-likelihood
117"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.244,"(1) expands to
118 −∂"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.246,"∂θL =
∂
∂θ log p (y | x) =
 ∂"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.248,∂θ log p (y | zk) + ∂
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.25,∂θ log p (zk | x)
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.252,"p(zk | x,y)
.
(2)"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.254,"This well-known result is the foundation of the Expectation-Maximization (EM) algorithm [Dempster
119"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.256,"et al., 1977]. Computing the marginal with respect to p (zk | x, y) corresponds to the E-step and
120"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.258,"calculating the gradients corresponds to the M-step. The sum inside the expectation separates the
121"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.26,"gradient estimators into two parts: x →zk and zk →y.
122"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.262,"However, the E-step is impractical to compute for most interesting applications because of the
123"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.264,"combinatorial explosion in the state space of zk. To get around this, we use a variational lower bound
124"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.266,"to EM, based on the ELBO loss LV = −log p (y | x) + DKL (q | p) [Mnih and Gregor, 2014] and
125"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.268,"demonstrate that this yields a practical solution to split gradients in a similar fashion to Eq. (2). In the
126"
USING LATENT REPRESENTATIONS TO CONSTRUCT PROBABILISTIC BLOCK-LOCAL LOSSES,0.27,"next section, we describe how we construct the variational distribution q.
127"
AUXILIARY LATENT REPRESENTATIONS,0.272,"3.2
Auxiliary latent representations
128"
AUXILIARY LATENT REPRESENTATIONS,0.274,"As described earlier, the output of any layer of a DNN can be interpreted as parameters to a distribution
129"
AUXILIARY LATENT REPRESENTATIONS,0.276,"over latent random variable zk. The sequence of blocks across a network therefore implicitly defines
130"
AUXILIARY LATENT REPRESENTATIONS,0.278,"a Markov chain x →z1 →z2 →. . . (see Fig. 2A). This probabilistic interpretation of hidden layer
131"
AUXILIARY LATENT REPRESENTATIONS,0.28,"activity is valid under relatively mild assumptions, studied in more detail in the Supplement. It is
132"
AUXILIARY LATENT REPRESENTATIONS,0.282,"important to note that the network at no point produces samples from the implicit random variables
133"
AUXILIARY LATENT REPRESENTATIONS,0.284,"zk, but they are introduced here only to conceptualize the mathematical framework. Instead the
134"
AUXILIARY LATENT REPRESENTATIONS,0.286,"network outputs the parameters to αk(zk) which is the probability distribution over zk (e.g. means
135"
AUXILIARY LATENT REPRESENTATIONS,0.288,"and variances if αk is Gaussian). The network thus translates αk−1 →αk →. . . by outputting the
136"
AUXILIARY LATENT REPRESENTATIONS,0.29,"statistical parameters of the conditional distribution αk(zk) and taking αk(zk−1) parameters as input.
137"
AUXILIARY LATENT REPRESENTATIONS,0.292,"More precisely, the network implicitly computes a marginal distribution
138"
AUXILIARY LATENT REPRESENTATIONS,0.294,"αk (zk) = p (zk | x) =
D
pk (zk | zk−1)
E"
AUXILIARY LATENT REPRESENTATIONS,0.296,"p(zk−1 | x) =
D
pk (zk | zk−1)
E"
AUXILIARY LATENT REPRESENTATIONS,0.298,"αk−1(zk−1) ,
(3)"
AUXILIARY LATENT REPRESENTATIONS,0.3,"where
D E"
AUXILIARY LATENT REPRESENTATIONS,0.302,"p denotes expectation with respect to the probability distribution p. Consequently, the
139"
AUXILIARY LATENT REPRESENTATIONS,0.304,"network realizes a conditional probability distribution p (y | x) (where x and y are network inputs
140"
AUXILIARY LATENT REPRESENTATIONS,0.306,"and outputs, respectively). And by the universal approximator property of deep neural networks,
141"
AUXILIARY LATENT REPRESENTATIONS,0.308,"an accurate representation of this distribution can be learnt in the network weights through error
142"
AUXILIARY LATENT REPRESENTATIONS,0.31,"back-propagation (as demonstrated for the example in Fig. 2). Eq. (3) is an instance of the belief
143"
AUXILIARY LATENT REPRESENTATIONS,0.312,"propagation algorithm to efficiently compute conditional probability distributions.
144"
AUXILIARY LATENT REPRESENTATIONS,0.314,"To construct the variational distribution q we introduce the backward network NB that propagates
145"
AUXILIARY LATENT REPRESENTATIONS,0.316,"messages βk backwards according to Eq. 4 (see Fig. 1 for an illustration). Inference over the posterior
146"
AUXILIARY LATENT REPRESENTATIONS,0.318,"distribution p (zk | x, y) for any latent variable zk can be made using the belief propagation algorithm,
147"
AUXILIARY LATENT REPRESENTATIONS,0.32,"propagating messages αk (zk) forward through the network using Eq. (3). In addition messages
148"
AUXILIARY LATENT REPRESENTATIONS,0.322,"βk (zk) need to be propagated backward according to
149"
AUXILIARY LATENT REPRESENTATIONS,0.324,"βk (zk) = p (y | zk) =
D
p (y | zk+1)
E"
AUXILIARY LATENT REPRESENTATIONS,0.326,"pk(zk+1 | zk) =
D
βk+1 (zk+1)
E"
AUXILIARY LATENT REPRESENTATIONS,0.328,"pk(zk+1 | zk) ,
(4)"
AUXILIARY LATENT REPRESENTATIONS,0.33,"such that the posterior p (zk | x, y) can be computed up to normalization
150"
AUXILIARY LATENT REPRESENTATIONS,0.332,"ρk (zk) = p (zk | x, y)
∝
p (zk | x) p (y | zk) = αk (zk) βk (zk) .
(5)"
AUXILIARY LATENT REPRESENTATIONS,0.334,"We make use of the fact that, through Eq. (3), the parameters of a probability distribution p (zk | x)
151"
AUXILIARY LATENT REPRESENTATIONS,0.336,"are a function of the parameters to p (zi | x), for 0 < i < k, e.g. if α is assumed to be Gaussian
152"
AUXILIARY LATENT REPRESENTATIONS,0.338,"we have
 
µ (αk) , σ2 (αk)

= f
 
µ (αi) , σ2 (αi)

, where µ (.) and σ2 (.) are the mean and vari-
153"
AUXILIARY LATENT REPRESENTATIONS,0.34,"ance of the distribution respectively. Thus, if a network outputs
 
µ (αi) , σ2 (αi)

on layer i and
154
 
µ (αk) , σ2 (αk)

on layer k, a suitable probabilistic loss function will allow the network to learn
155"
AUXILIARY LATENT REPRESENTATIONS,0.342,"Figure 2: Zero shot learning of predicted uncertainties. A: Gaussian convolutional autoencoder
network. Variance inputs and outputs are set to a constant during the whole training process. The
network implements an implicit Markov chain. B: Example images showing self-prediction of
uncertainties. C: Uncertainty mismatch metrics throughout learning. D: The network in (A) can be
‘folded’ to provide targets for local losses L0, L1, . . ."
AUXILIARY LATENT REPRESENTATIONS,0.344,"f from examples. Therefore, the conditional distributions pk (zk | zk−1) and the expectation in
156"
AUXILIARY LATENT REPRESENTATIONS,0.346,"Eq. (3) are only implicitly encoded in the network weights. We will study the exponential family of
157"
AUXILIARY LATENT REPRESENTATIONS,0.348,"probability distributions for which this observation can be formalized more thoroughly.
158"
AUXILIARY LATENT REPRESENTATIONS,0.35,"Exponential family distributions:
To derive concrete losses and update rules for the forward
159"
AUXILIARY LATENT REPRESENTATIONS,0.352,"and backward networks, we assume that αk are from the exponential family (EF) of probability
160"
AUXILIARY LATENT REPRESENTATIONS,0.354,"distributions, given by
161"
AUXILIARY LATENT REPRESENTATIONS,0.356,"αk (zk) =
Y"
AUXILIARY LATENT REPRESENTATIONS,0.358,"j
αkj (zkj) =
Y"
AUXILIARY LATENT REPRESENTATIONS,0.36,"j
h(zkj) exp (T (zkj) ϕkj −A (ϕkj)) ,
(6)"
AUXILIARY LATENT REPRESENTATIONS,0.362,"with base measure h, sufficient statistics T, log-partition function A, and natural parameters ϕkj.
162"
AUXILIARY LATENT REPRESENTATIONS,0.364,"This rich class contains the most common distributions, such as Gaussian, Poisson or Bernoulli, as
163"
AUXILIARY LATENT REPRESENTATIONS,0.366,"special cases. For the example of a Bernoulli random variable we have zkj ∈{0, 1}, T (zkj) = zkj
164"
AUXILIARY LATENT REPRESENTATIONS,0.368,"and A (ϕkj) = log
 
1 + eϕkj
[Koller and Friedman, 2009]. A network directly implements an EF
165"
AUXILIARY LATENT REPRESENTATIONS,0.37,"distribution if the activations akj encode the natural parameters, akj = ϕkj. Using this result, a
166"
AUXILIARY LATENT REPRESENTATIONS,0.372,"feed-forward DNN NA : x →y, can be split into N blocks by introducing implicit latent variables
167"
AUXILIARY LATENT REPRESENTATIONS,0.374,"zk : x →zk →y, and generating the respective natural parameters. In principle, blocks can be
168"
AUXILIARY LATENT REPRESENTATIONS,0.376,"separated after any arbitrary layer, but some splits may turn out more natural for a particular network
169"
AUXILIARY LATENT REPRESENTATIONS,0.378,"architecture.
170"
AUXILIARY LATENT REPRESENTATIONS,0.38,"Conveniently, if both αkj and βkj are members of the EF with natural parameters akj and bkj, then
171"
AUXILIARY LATENT REPRESENTATIONS,0.382,"ρkj is also EF with parameters akj + bkj. We will use this property to deconstruct a single global
172"
AUXILIARY LATENT REPRESENTATIONS,0.384,"loss into multiple block-local losses.
173"
AUXILIARY LATENT REPRESENTATIONS,0.386,"3.3
Illustrative example: forward-backward networks as an autoencoder
174"
AUXILIARY LATENT REPRESENTATIONS,0.388,"Probability representations in DNNs are useful since they provide a principled way to represent
175"
AUXILIARY LATENT REPRESENTATIONS,0.39,"uncertainties in the network. Before we establish our main result to show how a DNN can be
176"
AUXILIARY LATENT REPRESENTATIONS,0.392,"deconstructed into local blocks, we first demonstrate how representations of Bayesian uncertainty
177"
AUXILIARY LATENT REPRESENTATIONS,0.394,"can emerge in DNNs by using appropriate probabilistic losses. We consider the autoencoder network
178"
AUXILIARY LATENT REPRESENTATIONS,0.396,"illustrated in Fig. 2A and use it to learn representations for the Fashion-MNIST dataset [Xiao
179"
AUXILIARY LATENT REPRESENTATIONS,0.398,"et al., 2017]. The CNN comprises a bottleneck layer y that implicitly splits the architecture into a
180"
AUXILIARY LATENT REPRESENTATIONS,0.4,"decoder and encoder part (Fig. 2A). It is well known that such a network is able to learn compact
181"
AUXILIARY LATENT REPRESENTATIONS,0.402,"representations and features that allow it to reconstruct the gray scale pixel intensities of a given
182"
AUXILIARY LATENT REPRESENTATIONS,0.404,"input [Kingma and Welling, 2013]. Here we demonstrate that autoencoders are also able to learn
183"
AUXILIARY LATENT REPRESENTATIONS,0.406,"representations of uncertainties, i.e. to automatically output high uncertainties for pixel values that
184"
AUXILIARY LATENT REPRESENTATIONS,0.408,"are poorly represented in the learnt features.
185"
AUXILIARY LATENT REPRESENTATIONS,0.41,"To show this, we augmented the pixel representations on the inputs and outputs with additional
186"
AUXILIARY LATENT REPRESENTATIONS,0.412,"channels that represented the logarithms of the variances of a Gaussian distribution (see Supplement
187"
AUXILIARY LATENT REPRESENTATIONS,0.414,"for details). The input and outputs now represent the parameters of probability distributions, where
188"
AUXILIARY LATENT REPRESENTATIONS,0.416,"the variances are proxies for the uncertainties. An appropriate loss function for this architecture is
189"
AUXILIARY LATENT REPRESENTATIONS,0.418,"one that measures the distance between probability distributions. We used the Kullback-Leibler (KL)
190"
AUXILIARY LATENT REPRESENTATIONS,0.42,"divergence between Gaussian distributions. This augmentation to conventional deep auto-encoders
191"
AUXILIARY LATENT REPRESENTATIONS,0.422,"requires us to also provide uncertainty values for training data samples. Since the Fashion-MNIST
192"
AUXILIARY LATENT REPRESENTATIONS,0.424,"dataset does not contain this information, we set the variances of pixels for all training samples to the
193"
AUXILIARY LATENT REPRESENTATIONS,0.426,"same small constant values, reflecting high confidence (low variance) in the training set. Thus, during
194"
AUXILIARY LATENT REPRESENTATIONS,0.428,"training, the network has only seen the same constant inputs (and outputs) for the variance channels.
195"
AUXILIARY LATENT REPRESENTATIONS,0.43,"Fig. 2B shows representative sample outputs for the test dataset after training. As expected, the
196"
AUXILIARY LATENT REPRESENTATIONS,0.432,"network is able to represent the means of gray scale values in the dataset well and generalize to
197"
AUXILIARY LATENT REPRESENTATIONS,0.434,"new images. Interestingly, the network also learned meaningful representations of the variances.
198"
AUXILIARY LATENT REPRESENTATIONS,0.436,"Although the network has only seen constant values for the variances during training, it is able to
199"
AUXILIARY LATENT REPRESENTATIONS,0.438,"infer information about its own uncertainty during testing. The true MSE errors between inputs and
200"
AUXILIARY LATENT REPRESENTATIONS,0.44,"predictions qualitatively match the pixel-level variance predictions across a wide variety of inputs.
201"
AUXILIARY LATENT REPRESENTATIONS,0.442,"For example, the network poorly represents the logo on the shirt (leftmost example) and predicts
202"
AUXILIARY LATENT REPRESENTATIONS,0.444,"high variance in the output for these pixels. Other samples like the trousers (second from left) that
203"
AUXILIARY LATENT REPRESENTATIONS,0.446,"are well represented correctly predict low variance. To further quantify this result, we developed
204"
AUXILIARY LATENT REPRESENTATIONS,0.448,"additional metrics that measure the mismatch between estimated and true prediction errors (Fig. 2C,
205"
AUXILIARY LATENT REPRESENTATIONS,0.45,"see Supplement for details). These metrics consistently decrease throughout training even though
206"
AUXILIARY LATENT REPRESENTATIONS,0.452,"they were not directly minimized. These results suggest that DNNs are able to represent uncertainties
207"
AUXILIARY LATENT REPRESENTATIONS,0.454,"well enough that they show zero-shot generalizations to unseen data from very limited training data.
208"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.456,"3.4
Modularized learning using local variational losses
209"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.458,"The autoencoder example described in Section 3.3 shows that DNNs can represent probability
210"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.46,"distributions well in principle, and also provides an idea of how probabilistic losses could be
211"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.462,"constructed locally at any layer. By ‘folding’ the network along the bottleneck layer y we are able
212"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.464,"to construct a sequence of pairs of auxiliary targets (z1, z′
1), (z2, z′
2), . . . (see Fig. 2D). Finally, by
213"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.466,"introducing suitable loss functions L0, L1, . . . , the mismatch between the encoder and decoder parts
214"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.468,"of the network can be minimized on a per-layer basis.
215"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.47,"The forward and backward networks NA and NB can be used to construct local loss functions L(k)
V
at
216"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.472,"blocks k. In the Supplement, we show in detail that minimizing L(k)
V
locally and in parallel optimizes
217"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.474,"a lower bound to the log-likelihood loss L (Eq. 1), without propagating gradients end-to-end. To
218"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.476,"arrive at this result, we take the forward αk and posterior messages ρk to be given by EF distributions
219"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.478,"with natural parameters ϕkj and γkj. Using this we show in the Supplement that the local loss can be
220"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.48,"optimized using the modularized gradient estimator
221 −∂"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.482,"∂θL(k)
V
=
X j"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.484," 
µ (ρkj) −µ (αkj)
"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.486,"|
{z
}
forward weight"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.488,"∂
∂θϕkj + σ2 (ρkj) (ϕkj −γkj)
|
{z
}
posterior weight"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.49,"∂
∂θγkj ,
(7)"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.492,"where µ(·) and σ2(·) are means and variances of EF distribution. Note that the gradients of the natural
222"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.494,"parameters ϕkj and γkj are computed independently and modulated by the forward and posterior
223"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.496,"weight, respectively.
224"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.498,"The result in Eq. (7) holds for general EF distributions. For the special case of Bernoulli random
225"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.5,"variables we get
226 −∂"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.502,"∂θL(k)
V
=
X"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.504,"k,j
(ρkj −αkj) ∂"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.506,∂θakj −ρkj (1 −ρkj) bkj  ∂
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.508,∂θakj + ∂ ∂θbkj
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.51,"
,
(8)"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.512,"where akj = fj(ak−1) and bkj = gj(bk+1), are the outputs of the forward and backward network at
227"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.514,"block k,
228"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.516,"ρkj = S (akj + m bkj)
and
αkj = S (akj) ,
(9)
where m is a mixing parameter described below and S(x) = 1/1 + e−x is the sigmoid/logistic
229"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.518,"function.
230"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.52,"The Bernoulli solution in Eq. (8) is convenient because it is a single parameter distribution (mean
231"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.522,"and variance share one parameter) such that all channels in z can be treated independently. Also the
232"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.524,"structure of Eq. 9 is well suited for a DNN implementation. In our experiments, we focus on this
233"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.526,"Bernoulli variant of the general result in Eq. (7). In the Supplement, we study a number of other
234"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.528,"relevant members of the EF. Furthermore, it is interesting to study the structure of Eq. (8) more
235"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.53,"carefully. The first term minimizes the mismatch between the forward and the posterior distribution
236"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.532,"with respect to the forward blocks. The second term is the uncertainty-weighted backward activation
237"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.534,"bkj which modulates local gradients (see Supplement). Therefore, the backward activations bkj
238"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.536,"act directly as learning signals for local updates. The BLL method is therefore related to feedback
239"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.538,"alignment [Lillicrap et al., 2020] and target propagation [Frenkel et al., 2021] where backward
240"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.54,"information is provided through random weights. However, since the gradients of the backward
241"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.542,"blocks appear in the second term, our model also provides a principled way to optimize the backward
242"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.544,"flow of information from the targets.
243"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.546,"Data mixing schedule:
The equation for the posterior distribution Eq. 9 contains a data mixing
244"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.548,"parameter m, with 0 ≤m ≤1, that scales the influence of the backward messages in the posterior
245"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.55,"distribution. This parameter serves two important functions, (1) It scales the balance between forward
246"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.552,"and backward messages in the posterior distribution ρ and (2) it scales the first term in the parameter
247"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.554,"updates Eq. 8. We found that a annealing schedule for this parameter that decreases m slowly
248"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.556,"during learning works well in practice. If not stated otherwise, we used m = (1 + τ M)−1 in
249"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.558,"our experiments, where M is the index of the current epoch and τ is a scaling parameter (see the
250"
MODULARIZED LEARNING USING LOCAL VARIATIONAL LOSSES,0.56,"Supplement for further details).
251"
EXPERIMENTAL RESULTS,0.562,"4
Experimental results
252"
EXPERIMENTAL RESULTS,0.564,"We evaluated the BLL model on a number of vision and sequence learning tasks. All models used the
253"
EXPERIMENTAL RESULTS,0.566,"Bernoulli BLL gradients described in Eq. (8) for local optimization. Additional details of the network
254"
EXPERIMENTAL RESULTS,0.568,"models can be found in the Supplement.
255"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.57,"4.1
Block-local learning of vision benchmark tasks
256"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.572,"We compare the performance of our block local learning (BLL) algorithm with that of end-to-end
257"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.574,"backprop (BP) and Feedback Alignment (FA) Lillicrap et al. [2014b]. Three datasets are considered:
258"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.576,"MNIST, Fashion MNIST and CIFAR10 together with two residual network architectures [He et al.,
259"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.578,"2016]: ResNet-18 and ResNet-50, each trained with one of the three methods (BP, FA, BLL).
260"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.58,"The BLL architectures were split into 4 blocks that were trained locally using the Bernoulli loss
261"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.582,"in Eq. (8). Splits were introduced after residual layers of the ResNet architecture by grouping
262"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.584,"subsequent layers into blocks. Group sizes were (4,5,4,5) for ResNet-18 and (12,13,12,13) for
263"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.586,"ResNet-50. Backward twin networks were here constructed simply by using the same network
264"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.588,"architecture (ResNet-18 or ResNet-50) in reverse order, introducing appropriate splits to provide
265"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.59,"intermediate targets. For CIFAR-10 gradients were propagated between two neighboring blocks
266"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.592,"(see Supplement for details and a comparison with purely local gradients). The kernels of ResNet-
267"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.594,"18/ResNet-50 + FA architectures used during backpropagation are fixed and uniformly initialised
268"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.596,"following the Kaiming He et al. [2015] initialisation method. The bias is set to one.
269"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.598,"The results are summarized in Table 1. Test top-1, top-3 and train top-1 accuracies are shown. Top-3
270"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.6,"accuracies count the number of test samples for which the correct class was among the network’s
271"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.602,"MNIST
Fahion-MNIST
CIFAR-10
test-1
test-3
train-1
test-1
test-3
train-1
test-1
test-3
train-1"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.604,"ResNet-18 + BP
99.5
100
99.7
92.7
99.3
96.0
95.2
99.3
100
ResNet-50 + BP
99.5
99.9
100
89.0
98.9
92.7
94.0
99.2
99.8
ResNet-18 + FA
99.0
99.9
100
87.9
98.6
92.1
70.4
92.5
80.9
ResNet-50 + FA
98.9
99.9
100
83.1
97.9
83.7
70.3
92.0
79.3
ResNet-18 + BLL
99.4
100
99.6
91.2
98.8
91.0
72.2
93.0
98.8
ResNet-50 + BLL
99.4
99.8
99.2
88.7
99.0
85.9
73.4
92.7
99.7
Table 1: Classification accuracy (% correct) on vision tasks. BP: end-to-end backprop, FA: feedback
alignment, BLL: block local learning. Test-1, test-3 and train-1 represent the top-1, top-3 test accuracy
and top-1 training accuracy respectively."
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.606,"Figure 3: Block local learning of transformer architecture. A: Illustration of the transformer twin
network. B: Learning curves of block local (BLL) and backprop (BP) training. C: Test accuracy vs.
number of blocks in the transformer model. Error bars show standard deviations over 5 independent
runs."
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.608,"3 highest output activations. See Supplement for results over multiple runs. BLL achieved good
272"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.61,"performance on MNIST and Fashion-MNIST, closely matching end-to-end training and outperforming
273"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.612,"FA networks. Note that in contrast to FA and BP, BLL does not need to compute error gradients at
274"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.614,"the output but can work directly with the target labels. Performance on CIFAR-10 was significantly
275"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.616,"lower than BP but outperformed FA. Interestingly the performance on the training set was close to
276"
BLOCK-LOCAL LEARNING OF VISION BENCHMARK TASKS,0.618,"perfect for ResNet-50 suggesting over-fitting the task.
277"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.62,"4.2
Block-local transformer architecture for sequence-to-sequence learning
278"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.622,"Transformer architectures are in principle well suited for distributed computing due to their modular
279"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.624,"network structure that comprises a repetition of homogeneous blocks. We demonstrate a proof-of-
280"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.626,"concept result on training a transformer with BLL. We used a transformer model with 20 self-attention
281"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.628,"blocks with a single attention head each. Block local losses were added after each layer and blocks
282"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.63,"were trained locally. A backward twin network was constructed by projecting targets through dense
283"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.632,"layers and used the Bernoulli loss Eq. (8) for local training (see Fig. 3 A for an illustration). The
284"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.634,"transformer was trained on a sequence-to-sequence task, where a random permutation of numbers
285"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.636,"0..9 was presented on the input and had to be re-generated at the output in reverse order. We trained
286"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.638,"the network for 5 epochs.
287"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.64,"BLL achieves convergence speed that is comparable to that of end-to-end BP on this task. Fig. 3 B
288"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.642,"shows learning curves of BLL and BP. Both algorithms converge after around 3 epochs to nearly
289"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.644,"perfect performance. BLL also achieved good performance for a wide range of network depths.
290"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.646,"Fig. 3 C shows the performance after 5 epochs for different transformer architectures. Using only 5
291"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.648,"transformer blocks yields performance of around 99.9% (average over five independent runs). The
292"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.65,"test accuracy on this task for the 20 block transformer was 99.6%. These results suggest that the BLL
293"
BLOCK-LOCAL TRANSFORMER ARCHITECTURE FOR SEQUENCE-TO-SEQUENCE LEARNING,0.652,"method is equally applicable to transformer architectures.
294"
DISCUSSION,0.654,"5
Discussion
295"
DISCUSSION,0.656,"In this work, we have demonstrated a general purpose probabilistic framework for rigorously defining
296"
DISCUSSION,0.658,"block-local losses for deep architectures. This not only provides a novel way of performing distributed
297"
DISCUSSION,0.66,"training of large models but also hints at new paradigms of self-supervised training that are biologically
298"
DISCUSSION,0.662,"plausible. We have also shown that our block-local training approach outperforms existing local
299"
DISCUSSION,0.664,"training approaches while still getting around the locking and weight transport problems. Our method
300"
DISCUSSION,0.666,"introduces a twin network that propagates information backwards from the targets to the input
301"
DISCUSSION,0.668,"and automatically estimates uncertainties on intermediate layers. This is achieved by representing
302"
DISCUSSION,0.67,"probability distributions in the network activations. The forward network and its backward twin can
303"
DISCUSSION,0.672,"work in parallel and with different sets of weights.
304"
DISCUSSION,0.674,"The proposed method may also help further blur the boundary between deep learning and probabilistic
305"
DISCUSSION,0.676,"models. A number of previous models have shown that DNNs are capable of representing probability
306"
DISCUSSION,0.678,"distribution [Abdar et al., 2021, Pawlowski et al., 2017, Tran et al., 2019, Malinin and Gales, 2019].
307"
DISCUSSION,0.68,"Unlike these previous methods, our method does not require Monte Carlo sampling or contrastive
308"
DISCUSSION,0.682,"training, but instead exploits the log-linear structure of exponential family distributions to efficiently
309"
DISCUSSION,0.684,"propagate uncertainty-aware messages through a network using a belief-propagation strategy. We
310"
DISCUSSION,0.686,"have demonstrated that implicit uncertainty messages can be learnt from sparse data and accurately
311"
DISCUSSION,0.688,"represent the network’s performance.
312"
DISCUSSION,0.69,"Greedy block-local learning has recently shown compelling performance on a number of tasks
313"
DISCUSSION,0.692,"[Nøkland and Eidnes, 2019, Siddiqui et al., 2023]. These methods use local losses with an information-
314"
DISCUSSION,0.694,"theoretic motivation but are agnostic to global back-propagating information. In future work, it may
315"
DISCUSSION,0.696,"be interesting to combine these approaches with the proposed model to get the best of both worlds.
316"
DISCUSSION,0.698,"Being able to produce block-level uncertainty predictions can also be useful for enhancing the sparsity
317"
DISCUSSION,0.7,"of the network and using optimal amount of compute for predictions. The uncertainty predictions
318"
DISCUSSION,0.702,"can also be used to handle missing labels, and for evaluating the model’s confidence about its
319"
DISCUSSION,0.704,"predictions. Since the framework is flexible enough to apply to self-supervised training, it can be
320"
DISCUSSION,0.706,"used on unlabelled and multi-modal datasets as well. Due to the local nature of the training process,
321"
DISCUSSION,0.708,"our method is particularly attractive for application on neuromorphic systems that co-locate memory
322"
DISCUSSION,0.71,"and compute and use orders of magnitude less energy if the computation is local.
323"
DISCUSSION,0.712,"This work addresses potential problems of modern ML: The estimation of uncertainties in neural
324"
DISCUSSION,0.714,"networks is an important open problem and understanding the underlying mechanisms better will
325"
DISCUSSION,0.716,"likely help to make ML models safer and more reliable. Also the main focus of this work, which is on
326"
DISCUSSION,0.718,"distributing large ML models over many compute nodes may make these model more energy efficient
327"
DISCUSSION,0.72,"in the future. The energy consumption and resulting carbon footprint of ML is a major concern and
328"
DISCUSSION,0.722,"the proposed model may provide a new direction to approach this problem. This method may enable
329"
DISCUSSION,0.724,"training of larger models which also come with associated risks in terms of biases and inappropriate
330"
DISCUSSION,0.726,"use in the real world. It is also not known what biases using this method itself and extensions with
331"
DISCUSSION,0.728,"sparsity may introduce in the models predictions.
332"
REFERENCES,0.73,"References
333"
REFERENCES,0.732,"Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad
334"
REFERENCES,0.734,"Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A
335"
REFERENCES,0.736,"review of uncertainty quantification in deep learning: Techniques, applications and challenges.
336"
REFERENCES,0.738,"Information Fusion, 76:243–297, 2021.
337"
REFERENCES,0.74,"Ehsan Amid, Rohan Anil, and Manfred Warmuth. LocoProp: Enhancing BackProp via local loss
338"
REFERENCES,0.742,"optimization. In Proceedings of The 25th International Conference on Artificial Intelligence and
339"
REFERENCES,0.744,"Statistics, pages 9626–9642. PMLR, 2022. URL https://proceedings.mlr.press/v151/
340"
REFERENCES,0.746,"amid22a.html.
341"
REFERENCES,0.748,"Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale
342"
REFERENCES,0.75,"to ImageNet. In Proceedings of the 36th International Conference on Machine Learning, pages 583–
343"
REFERENCES,0.752,"593. PMLR, 2019. URL https://proceedings.mlr.press/v97/belilovsky19a.html.
344"
REFERENCES,0.754,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
345"
REFERENCES,0.756,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
346"
REFERENCES,0.758,"Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
347"
REFERENCES,0.76,"Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
348"
REFERENCES,0.762,"Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
349"
REFERENCES,0.764,"Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs],
350"
REFERENCES,0.766,"July 2020. URL http://arxiv.org/abs/2005.14165.
351"
REFERENCES,0.768,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
352"
REFERENCES,0.77,"contrastive learning of visual representations. In International conference on machine learning,
353"
REFERENCES,0.772,"pages 1597–1607. PMLR, 2020.
354"
REFERENCES,0.774,"David Clark, L F Abbott, and Sueyeon Chung. Credit assignment through broadcasting a global error
355"
REFERENCES,0.776,"vector. In Advances in Neural Information Processing Systems, volume 34, pages 10053–10066.
356"
REFERENCES,0.778,"Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
357"
REFERENCES,0.78,"532b81fa223a1b1ec74139a5b8151d12-Abstract.html.
358"
REFERENCES,0.782,"A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM
359"
REFERENCES,0.784,"algorithm. 39(1):1–22, 1977. ISSN 00359246. doi: 10.1111/j.2517-6161.1977.tb01600.x. URL
360"
REFERENCES,0.786,"https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1977.tb01600.x.
361"
REFERENCES,0.788,"Charlotte Frenkel, Martin Lefebvre, and David Bol. Learning without feedback: Fixed random
362"
REFERENCES,0.79,"learning signals allow for feedforward training of deep neural networks. 15, 2021. ISSN 1662-
363"
REFERENCES,0.792,"453X. URL https://www.frontiersin.org/articles/10.3389/fnins.2021.629892.
364"
REFERENCES,0.794,"Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):
365"
REFERENCES,0.796,"452–459, 2015.
366"
REFERENCES,0.798,"Stephen Grossberg. Competitive learning: From interactive activation to adaptive resonance. Cogni-
367"
REFERENCES,0.8,"tive science, 11(1):23–63, 1987.
368"
REFERENCES,0.802,"Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, Eugenio Culurciello, and Zhongming Liu. Deep
369"
REFERENCES,0.804,"predictive coding network with local recurrent processing for object recognition, 2018. URL
370"
REFERENCES,0.806,"http://arxiv.org/abs/1805.07526.
371"
REFERENCES,0.808,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
372"
REFERENCES,0.81,"human-level performance on imagenet classification, 2015.
373"
REFERENCES,0.812,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
374"
REFERENCES,0.814,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
375"
REFERENCES,0.816,"pages 770–778, 2016.
376"
REFERENCES,0.818,"Geoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. arXiv preprint
377"
REFERENCES,0.82,"arXiv:2212.13345, 2022.
378"
REFERENCES,0.822,"Bernd Illing, Jean Ventura, Guillaume Bellec, and Wulfram Gerstner.
Local plasticity
379"
REFERENCES,0.824,"rules can learn deep representations using self-supervised contrastive predictions.
In Ad-
380"
REFERENCES,0.826,"vances in Neural Information Processing Systems, volume 34, pages 30365–30379. Cur-
381"
REFERENCES,0.828,"ran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
382"
REFERENCES,0.83,"feade1d2047977cd0cefdafc40175a99-Abstract.html.
383"
REFERENCES,0.832,"Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David
384"
REFERENCES,0.834,"Silver, and Koray Kavukcuoglu.
Decoupled Neural Interfaces using Synthetic Gradients.
385"
REFERENCES,0.836,"arXiv:1608.05343 [cs], August 2016a. URL http://arxiv.org/abs/1608.05343.
386"
REFERENCES,0.838,"Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David
387"
REFERENCES,0.84,"Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks, 2016b.
388"
REFERENCES,0.842,"URL http://arxiv.org/abs/1611.05397.
389"
REFERENCES,0.844,"Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David
390"
REFERENCES,0.846,"Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In
391"
REFERENCES,0.848,"Proceedings of the 34th International Conference on Machine Learning, pages 1627–1635. PMLR,
392"
REFERENCES,0.85,"2017. URL https://proceedings.mlr.press/v70/jaderberg17a.html.
393"
REFERENCES,0.852,"Danilo Jimenez Rezende, S. M. Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, and
394"
REFERENCES,0.854,"Nicolas Heess. Unsupervised learning of 3d structure from images. In Advances in Neural Informa-
395"
REFERENCES,0.856,"tion Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.
396"
REFERENCES,0.858,"neurips.cc/paper/2016/hash/1d94108e907bb8311d8802b48fd54b4a-Abstract.html.
397"
REFERENCES,0.86,"Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv:1312.6114 [cs, stat],
398"
REFERENCES,0.862,"December 2013. URL http://arxiv.org/abs/1312.6114.
399"
REFERENCES,0.864,"Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT
400"
REFERENCES,0.866,"press, 2009.
401"
REFERENCES,0.868,"Julien Launay, Iacopo Poli, François Boniface, and Florent Krzakala. Direct feedback alignment scales
402"
REFERENCES,0.87,"to modern deep learning tasks and architectures. In Advances in Neural Information Processing Sys-
403"
REFERENCES,0.872,"tems, volume 33, pages 9346–9360. Curran Associates, Inc., 2020. URL https://proceedings.
404"
REFERENCES,0.874,"neurips.cc/paper/2020/hash/69d1fc78dbda242c43ad6590368912d4-Abstract.html.
405"
REFERENCES,0.876,"Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random feedback
406"
REFERENCES,0.878,"weights support learning in deep neural networks. arXiv:1411.0247 [cs, q-bio], November 2014a.
407"
REFERENCES,0.88,"URL http://arxiv.org/abs/1411.0247.
408"
REFERENCES,0.882,"Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random feedback
409"
REFERENCES,0.884,"weights support learning in deep neural networks, 2014b.
410"
REFERENCES,0.886,"Timothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman, and Geoffrey Hinton. Backprop-
411"
REFERENCES,0.888,"agation and the brain. 21(6):335–346, 2020. ISSN 1471-0048. doi: 10.1038/s41583-020-0277-3.
412"
REFERENCES,0.89,"URL https://www.nature.com/articles/s41583-020-0277-3.
413"
REFERENCES,0.892,"Michael Lomnitz, Zachary Daniels, David Zhang, and Michael Piacentino. Learning with local
414"
REFERENCES,0.894,"gradients at the edge, 2022. URL http://arxiv.org/abs/2208.08503.
415"
REFERENCES,0.896,"Sindy Löwe, Peter O’ Connor, and Bastiaan Veeling. Putting an end to end-to-end: Gradient-isolated
416"
REFERENCES,0.898,"learning of representations. In Advances in Neural Information Processing Systems, volume 32.
417"
REFERENCES,0.9,"Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
418"
REFERENCES,0.902,"851300ee84c2b80ed40f51ed26d866fc-Abstract.html.
419"
REFERENCES,0.904,"Andrey Malinin and Mark Gales. Reverse kl-divergence training of prior networks: Improved
420"
REFERENCES,0.906,"uncertainty and adversarial robustness. Advances in Neural Information Processing Systems, 32,
421"
REFERENCES,0.908,"2019.
422"
REFERENCES,0.91,"Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
423"
REFERENCES,0.912,"International Conference on Machine Learning, pages 1791–1799. PMLR, 2014.
424"
REFERENCES,0.914,"Arild Nøkland and Lars Hiller Eidnes.
Training neural networks with local error signals.
In
425"
REFERENCES,0.916,"International conference on machine learning, pages 4839–4850. PMLR, 2019.
426"
REFERENCES,0.918,"Arild
Nøkland.
Direct
feedback
alignment
provides
learning
in
deep
neural
net-
427"
REFERENCES,0.92,"works.
In Advances in Neural Information Processing Systems, volume 29. Curran
428"
REFERENCES,0.922,"Associates, Inc., 2016.
URL https://proceedings.neurips.cc/paper/2016/hash/
429"
REFERENCES,0.924,"d490d7b4576290fa60eb31b5fc917ad1-Abstract.html.
430"
REFERENCES,0.926,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
431"
REFERENCES,0.928,"coding, 2019. URL http://arxiv.org/abs/1807.03748.
432"
REFERENCES,0.93,"Alexander Ororbia and Ankur Mali. The predictive forward-forward algorithm. arXiv preprint
433"
REFERENCES,0.932,"arXiv:2301.01452, 2023.
434"
REFERENCES,0.934,"Nick Pawlowski, Andrew Brock, Matthew CH Lee, Martin Rajchl, and Ben Glocker. Implicit weight
435"
REFERENCES,0.936,"uncertainty in neural networks. arXiv preprint arXiv:1711.01297, 2017.
436"
REFERENCES,0.938,"Maria Refinetti, Stéphane d’Ascoli, Ruben Ohana, and Sebastian Goldt. Align, then memorise:
437"
REFERENCES,0.94,"the dynamics of learning with feedback alignment, 2021. URL http://arxiv.org/abs/2011.
438"
REFERENCES,0.942,"12428.
439"
REFERENCES,0.944,"Albert Jiménez Sanfiz and Mohamed Akrout. Benchmarking the accuracy and robustness of feedback
440"
REFERENCES,0.946,"alignment algorithms, 2021. URL http://arxiv.org/abs/2108.13446.
441"
REFERENCES,0.948,"Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-
442"
REFERENCES,0.95,"based models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.
443"
REFERENCES,0.952,"Shoaib Ahmed Siddiqui, David Krueger, Yann LeCun, and Stéphane Deny. Blockwise self-supervised
444"
REFERENCES,0.954,"learning at scale, 2023. URL http://arxiv.org/abs/2302.01647.
445"
REFERENCES,0.956,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
446"
REFERENCES,0.958,"Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand
447"
REFERENCES,0.96,"Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language
448"
REFERENCES,0.962,"Models, February 2023. URL http://arxiv.org/abs/2302.13971.
449"
REFERENCES,0.964,"Dustin Tran, Mike Dusenberry, Mark Van Der Wilk, and Danijar Hafner. Bayesian layers: A module
450"
REFERENCES,0.966,"for neural network uncertainty. Advances in neural information processing systems, 32, 2019.
451"
REFERENCES,0.968,"Bohan Wu, Suraj Nair, Roberto Martin-Martin, Li Fei-Fei, and Chelsea Finn. Greedy hierarchi-
452"
REFERENCES,0.97,"cal variational autoencoders for large-scale video prediction. In Proceedings of the IEEE/CVF
453"
REFERENCES,0.972,"Conference on Computer Vision and Pattern Recognition, pages 2318–2328, 2021.
454"
REFERENCES,0.974,"Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. TinyViT:
455"
REFERENCES,0.976,"Fast pretraining distillation for small vision transformers, 2022. URL http://arxiv.org/abs/
456"
REFERENCES,0.978,"2207.10666.
457"
REFERENCES,0.98,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
458"
REFERENCES,0.982,"machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
459"
REFERENCES,0.984,"Yuwen Xiong, Mengye Ren, and Raquel Urtasun. LoCo: Local contrastive representation learning.
460"
REFERENCES,0.986,"In Advances in Neural Information Processing Systems, volume 33, pages 11142–11153. Cur-
461"
REFERENCES,0.988,"ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
462"
REFERENCES,0.99,"7fa215c9efebb3811a7ef58409907899-Abstract.html.
463"
REFERENCES,0.992,"Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised
464"
REFERENCES,0.994,"learning via redundancy reduction, 2021. URL http://arxiv.org/abs/2103.03230.
465"
REFERENCES,0.996,"Gongpei Zhao, Tao Wang, Yidong Li, Yi Jin, Congyan Lang, and Haibin Ling. The cascaded forward
466"
REFERENCES,0.998,"algorithm for neural network training. arXiv preprint arXiv:2303.09728, 2023.
467"
