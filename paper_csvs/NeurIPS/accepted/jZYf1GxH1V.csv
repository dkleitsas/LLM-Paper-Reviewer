Section,Section Appearance Order,Paragraph
WESTLAKE UNIVERSITY,0.0,"1Westlake University
2Zhejiang University
3Huawei Noah‚Äôs Ark Lab"
ABSTRACT,0.0021691973969631237,Abstract
ABSTRACT,0.004338394793926247,"In this work, we decouple the iterative bi-level offline RL (value estimation and
policy extraction) from the offline training phase, forming a non-iterative bi-level
paradigm and avoiding the iterative error propagation over two levels. Specifi-
cally, this non-iterative paradigm allows us to conduct inner-level optimization
(value estimation) in training, while performing outer-level optimization (policy
extraction) in testing. Naturally, such a paradigm raises three core questions that
are not fully answered by prior non-iterative offline RL counterparts like reward-
conditioned policy: Q1) What information should we transfer from the inner-level
to the outer-level? Q2) What should we pay attention to when exploiting the
transferred information for safe/confident outer-level optimization? Q3) What are
the benefits of concurrently conducting outer-level optimization during testing?
Motivated by model-based optimization (MBO), we propose DROP (Design fROm
Policies), which fully answers the above questions. Specifically, in the inner-level,
DROP decomposes offline data into multiple subsets and learns an MBO score
model (A1). To keep safe exploitation to the score model in the outer-level, we
explicitly learn a behavior embedding and introduce a conservative regularization
(A2). During testing, we show that DROP permits test-time adaptation, enabling an
adaptive inference across states (A3). Empirically, we find that DROP, compared to
prior non-iterative offline RL counterparts, gains an average improvement probabil-
ity of more than 80%, and achieves comparable or better performance compared to
prior iterative baselines."
INTRODUCTION,0.006507592190889371,"1
Introduction"
INTRODUCTION,0.008676789587852495,"Offline reinforcement learning (RL) [37, 40] describes a task of learning a policy from static offline
data. Due to the overestimation of values at out-of-distribution (OOD) state-actions, recent iterative
offline RL methods introduce various policy/value regularization to avoid deviating from the offline
data distribution in the training phase. Then, these methods directly deploy the learned policy in an
online environment to test its performance. To unfold our following analysis, we term this kind of
learning procedure as iterative bi-level offline RL (Figure 1 left), where the inner-level optimization
refers to trying to eliminate the OOD issue, the outer-level optimization refers to trying to infer
an optimal policy that will be employed at testing. Here, we use the ""iterative"" term to emphasize
that the inner-level and outer-level are iteratively optimized in the training phase. However, due to
the iterative error exploitation and propagation [6] over the two levels, performing such an iterative
bi-level optimization completely in training often struggles to learn a stable policy/value function."
INTRODUCTION,0.010845986984815618,‚àóCorresponding author: Donglin Wang <wangdonglin@westlake.edu.cn>
INTRODUCTION,0.013015184381778741,"Iterative bi-level offline RL optimization 
Non-iterative bi-level offline RL optimization ùúã‚àó"
INTRODUCTION,0.015184381778741865,(Online rollout       )
INTRODUCTION,0.01735357917570499,Inner-level
INTRODUCTION,0.019522776572668113,Outer-level
INTRODUCTION,0.021691973969631236,Inner-level
INTRODUCTION,0.02386117136659436,"Outer-level
ùë†ùë°
ùúã‚àó(ùëéùë°|ùë†ùë°)
ùë†ùë°
Fixed"
INTRODUCTION,0.026030368763557483,(Online rollout       )
INTRODUCTION,0.028199566160520606,"Training
Testing/Deployment"
INTRODUCTION,0.03036876355748373,ùúã‚àó(ùëéùë°|ùë†ùë°)
INTRODUCTION,0.03253796095444685,"Training
Testing/Deployment"
INTRODUCTION,0.03470715835140998,"Figure 1: Framework of (iterative and non-iterative) bi-level offline RL optimization, where the
inner-level optimization refers to regularizing the policy/value function (for OOD issues) and the
outer-level refers to updating the policy for reward maximizing. The "" "" transferred from inner-level
to outer-level depends on the specific choice of algorithm used (see different choices for
in Table 1)."
INTRODUCTION,0.0368763557483731,"In this work, we thus advocate for non-iterative bi-level optimization (Figure 1 right) that decouples
the bi-level optimization from the training phase, i.e., performing inner-level optimization (eliminating
OOD) in training and performing outer-level optimization (maximizing return) in testing. Intuitively,
incorporating the outer-level optimization into the testing phase can eliminate the iterative error
propagation over the two levels, i.e., there is no iteratively learned value function that bootstraps off
of itself and thus propagates errors further. Then, three core questions are: Q1) What information
("" "") should we transfer from the inner-level to the outer-level? Q2) What should we pay special
attention to when exploiting "" "" for safe/confident outer-level optimization? Q3) Notice that the
outer-level optimization and the testing rollout form a new loop, what new benefit does this give us?"
INTRODUCTION,0.039045553145336226,"Intriguingly, prior works under such a non-iterative framework have proposed to transfer ("" "" in
Q1) filtered trajectories [10], a reward-conditioned policy [14, 34], and the Q-value estimation of the
behavior policy [6, 24], all of which, however, partially address the above questions (we will discuss
these works in Section 3.4). In this work, we thus propose a new alternative method that transfers
an embedding-conditioned score model (Q-value) and we will show that this method sufficiently
answers the raised questions and benefits most from the non-iterative bi-level framework."
INTRODUCTION,0.04121475054229935,"Before introducing our method, we introduce a conceptually similar task (to the non-iterative bi-level
optimization) ‚Äî offline model-based optimization (MBO2 [62]), which aims to discover, from static
input-score pairs, a new design input that will lead to the highest score. Typically, offline MBO first
learns a score model that maps the input to its score via supervised regression (corresponding to
inner-level optimization), and then conducts inference with the learned score model (as "" ""), for
instance, by optimizing the input against the learned score model via gradient ascent (corresponding
to the outer-level). To enable this MBO implementation in offline RL, we are required to decompose
an offline RL task into multiple sub-tasks, each of which thus corresponds to a behavior policy-
return (parameters-return) pair. However, there are practical optimization difficulties when using
high-dimensional policy‚Äôs parameter space (as input for the score model) to learn such a score model
(inner-level) and conduct inference (outer-level). At inference, directly extrapolating the learned
score model ("" "") also tends to drive the high-dimensional candidate policy parameters towards
OOD, invalid, and low-scoring parameters [32], as these are falsely and over-optimistically scored by
the learned score model in inner-level optimization."
INTRODUCTION,0.04338394793926247,"To tackle these problems, we suggest3 A1) learning low-dimensional embeddings for those sub-tasks
decomposed in the MBO implementation, over which we estimate an embedding-conditioned Q-value
("" "" in Q1), and A2) introducing a conservative regularization, which pushes down the predicted
scores on OOD embeddings. Intuitively, this conservation aims to avoid over-optimistic exploitation
and protect against producing unconfident embeddings when conducting outer-level optimization (Q2).
Meanwhile, we suggest A3) conducting test-time adaptation, which means we can dynamically adjust
the inferred embeddings across testing states (aka deployment adaptation). We name our method
DROP (Design fROm Policies). Compared with standard offline MBO for parameter design [62],
test-time adaptation in DROP leverages the sequential structure of RL tasks, rather than simply
conducting inference at the beginning of testing rollout. Empirically, we demonstrate that DROP can
effectively extrapolate a better policy that benefits from the non-iterative framework by answering the
raised questions, and achieves better performance compared to state-of-the-art offline RL algorithms."
INTRODUCTION,0.0455531453362256,"2Please note that this MBO is different from the typical model-based RL, where the model in MBO
denotes a score model while that in the model-based RL denotes a transition dynamics model.
3We use A1, A2, and A3 to denote our answers to the raised questions (Q1, Q2, and Q3) respectively."
PRELIMINARIES,0.04772234273318872,"2
Preliminaries"
PRELIMINARIES,0.049891540130151846,"Reinforcement learning. We model the interaction between agent and environment as a Markov
Decision Process (MDP) [61], denoted by the tuple (S, A, P, R, p0), where S is the state space, A
is the action space, P : S √ó A √ó S ‚Üí[0, 1] is the transition kernel, R : S √ó A ‚ÜíR is the reward
function, and p0 : S ‚Üí[0, 1] is the initial state distribution. Let œÄ ‚ààŒ† := {œÄ : S √ó A ‚Üí[0, 1]}
denotes a policy. In RL, we aim to find a stationary policy that maximizes the expected discounted
return J(œÄ) := EœÑ‚àºœÄ [P‚àû
t=0 Œ≥tR(st, at)] in the environment, where œÑ = (s0, a0, r0, s1, a1, . . . ),
rt = R(st, at), is a sample trajectory and Œ≥ ‚àà(0, 1) is the discount factor. We also define the
state-action value function QœÄ(s, a) := EœÑ‚àºœÄ [P‚àû
t=0 Œ≥tR(st, at)|s0 = s, a0 = a], which describes
the expected discounted return starting from state s and action a and following œÄ afterwards, and
the state value function V œÄ(s) = Ea‚àºœÄ(a|s) [QœÄ(s, a)]. To maximize J(œÄ), the actor-critic algorithm
alternates between policy evaluation and improvement. Given initial Q0 and œÄ0, it iterates"
PRELIMINARIES,0.052060737527114966,"Qk+1 ‚Üêarg min
Q
E(s,a,s‚Ä≤)‚àºD+,a‚Ä≤‚àºœÄk(a‚Ä≤|s‚Ä≤)

(R(s, a) + Œ≥Qk(s‚Ä≤, a‚Ä≤) ‚àíQ(s, a))2
,
(1)"
PRELIMINARIES,0.05422993492407809,"œÄk+1 ‚Üêarg max
œÄ
Es‚àºD+,a‚àºœÄ(a|s)

Qk+1(s, a)

,
(2)"
PRELIMINARIES,0.05639913232104121,"where the value function (critic) Q(s, a) is updated by minimizing the mean squared Bellman error
with an experience replay D+ and, following the deterministic policy gradient theorem [59], the
policy (actor) œÄ(a|s) is updated to maximize the estimated Qk+1(s, œÄ(a|s))."
PRELIMINARIES,0.05856832971800434,"Offline reinforcement learning. In offline RL [40], the agent is provided with a static data D = {œÑ}
which consists of trajectories collected by running some data-generating policies. Note that here
we denote static offline data D, distinguishing from the experience replay D+ in the online setting.
Unlike the online RL problem where the experience D+ in Equation 1 can be dynamically updated,
the agent in offline RL is not allowed to interact with the environment to collect new experience data.
As a result, naively performing policy evaluation as in Equation 1 may query the estimated Qk(s‚Ä≤, a‚Ä≤)
on actions that lie far outside of the static offline data D, resulting in pathological value Qk+1(s, a)
that incurs large error. Further, iterating policy evaluation and improvement will cause the inferred
policy œÄk+1(a|s) to be biased towards OOD actions with erroneously overestimated values."
PRELIMINARIES,0.06073752711496746,"Offline model-based optimization. Model-based optimization (MBO) [63] aims to find an optimal
design input x‚àówith a given score function f ‚àó: X ‚ÜíY ‚äÇR, i.e., x‚àó= arg maxx f ‚àó(x). Typically,
we can repeatedly query the oracle score model f ‚àófor new candidate design, until it produces
the best design. However, we often do not have the oracle score function f ‚àóbut are provided
with a static offline dataset {(x, y)} of labeled input-score pairs. To track such a design from
data question, we can fit a parametric model f to the static offline data {(x, y)} via the empirical"
PRELIMINARIES,0.06290672451193059,"risk minimization (ERM): f ‚Üêarg minf E(x,y)
h
(f(x) ‚àíy)2i
. Then, starting from the best point in
the dataset, we can perform gradient ascent on the design input and set the inferred optimal design
x‚àó= x‚ó¶
K := GradAscentf(x‚ó¶
0, K), where"
PRELIMINARIES,0.0650759219088937,"x‚ó¶
k+1 ‚Üêx‚ó¶
k + Œ∑‚àáxf(x)|x=x‚ó¶
k, for k = 0, . . . , K ‚àí1.
(3)"
PRELIMINARIES,0.06724511930585683,"For simplicity, we will omit subscript f in GradAscentf. Since the aim is to find a better design
beyond all the designs in the dataset while directly optimizing score model f with ERM can not
ensure new candidates (OOD designs/inputs) receive confident scores, one crucial requirement of
offline MBO is thus to conduct confident extrapolation."
PRELIMINARIES,0.06941431670281996,"3
DROP: Design from Policies"
PRELIMINARIES,0.07158351409978309,"We present our framework in Figure 2. In Sections 3.1 and 3.2, we will answer questions Q1 and Q2,
setting a learned MBO score model as "" "" (A1) and introducing a conservative regularization over
the score model (A2). In Section 3.3, we will answer Q3, where we show that conducting outer-level
optimization during testing can produce an adaptive embedding inference across states (A3)."
TASK DECOMPOSITION,0.0737527114967462,"3.1
Task Decomposition"
TASK DECOMPOSITION,0.07592190889370933,"Our core idea is to explore MBO in the non-iterative bi-level offline RL framework (Figure 1
right), while capturing the sequential characteristics of RL tasks and answering the raised questions
(Q1, Q2, and Q3). To begin with, we first decompose the offline data D into N offline subsets"
TASK DECOMPOSITION,0.07809110629067245,"environment
ùõΩ(a|ùë†, ùëß)
ùëì(ùë†, a, ùëß) v"
TASK DECOMPOSITION,0.08026030368763558,"ùúã‚àóa ùë†= ùõΩ(a|ùë†, ùëß‚àó) ùëß1
ùëß2"
TASK DECOMPOSITION,0.0824295010845987,"ùëß3
ùëß4
ùúá(ùëß)"
TASK DECOMPOSITION,0.08459869848156182,"offline data (task)              sub-tasks                
deployment adaptation ùíü ùë† a ùíü1
ùíü2 ùíü3
ùíü4"
TASK DECOMPOSITION,0.08676789587852494,ùúô(ùëß|ùëõ) ‚áå ‚Ü¶
TASK DECOMPOSITION,0.08893709327548807,"training phase (inner-level     )
testing/deployment phase ùõΩ, ùëì"
TASK DECOMPOSITION,0.0911062906724512,outer-level       (using ùëì)
TASK DECOMPOSITION,0.09327548806941431,"task embedding & 
conservative MBO ùëß‚àó"
TASK DECOMPOSITION,0.09544468546637744,"Figure 2: Overview of DROP. Given offline dataset D, we decompose the data into N (= 4 in the
diagram) subsets D[N]. Over the decomposed sub-tasks, we learn a task embedding œï(z|n) and con-
duct conservative MBO by learning a score model f(s, a, z) and N behavior policies (modeled by a
contextual policy Œ≤(a|s, z)). During test-time, at state s, we perform test-time adaptation by adapting
the policy (contextual variable) with œÄ‚àó(a|s) = Œ≤ (a|s, z‚àó), where z‚àó= arg maxz f(s, Œ≤(a|s, z), z)."
TASK DECOMPOSITION,0.09761388286334056,"D[N] := {D1, . . . , DN}. In other words, we decompose an offline task, learning with D, into
multiple offline sub-tasks, learning with Dn ‚ààD[N] respectively. Then, for sub-task n ‚àà[1, N], we
can perform behavior cloning (BC) to fit a parametric behavior policy Œ≤n : S √ó A ‚Üí[0, 1] to model
the corresponding offline subset Dn:"
TASK DECOMPOSITION,0.09978308026030369,"Œ≤n ‚Üêarg max
Œ≤n E(s,a)‚àºDn

log Œ≤n(a|s)

.
(4)"
TASK DECOMPOSITION,0.1019522776572668,"Additionally, such a decomposition also comes with the benefit that it provides an avenue to exploit
the hybrid modes in offline data D, because that D is often collected using hybrid data-generating
behavior policies [18], which suggests that fitting a single behavior policy may not be optimal to
model the multiple modes of the offline data distribution (see Appendix A for empirical evidence). In
real-world tasks, it is also often the case that offline data exhibit hybrid behaviors. Thus, to encourage
the emergence of diverse sub-tasks that capture distinct behavior modes, we perform a simple task
decomposition according to the returns of trajectories in D, heuristically ensuring that trajectories in
the same sub-task share similar returns and trajectories from different sub-tasks have distinct returns4."
TASK EMBEDDING AND CONSERVATIVE MBO,0.10412147505422993,"3.2
Task Embedding and Conservative MBO"
TASK EMBEDDING AND CONSERVATIVE MBO,0.10629067245119306,"Naive MBO over behavior policies. Benefiting from the above offline task decomposition, we can
conduct MBO over a set of input-score (x, y) pairs, where we model the (parameters of) behavior
policies Œ≤n as the design input x and the corresponding expected returns at initial states J(Œ≤n) as the
score y. Note that, ideally, evaluating behavior policy Œ≤n, i.e., calculating J(Œ≤n) ‚â°Es0

V Œ≤n(s0)

,
with subset Dn will never trigger the overestimation of values in the inner-level optimization. By
introducing a score model f : Œ† ‚ÜíR and setting it as the transfer information "" "" in Q1, we can
then perform outer-level policy inference with"
TASK EMBEDDING AND CONSERVATIVE MBO,0.10845986984815618,"œÄ‚àó(a|s) ‚Üêarg max
œÄ
f (œÄ) , where f = arg min
f
En
h 
f(Œ≤n) ‚àíJ(Œ≤n)
2i
.
(5)"
TASK EMBEDDING AND CONSERVATIVE MBO,0.11062906724511931,"However, directly performing optimization-based inference (outer-level optimization), maxœÄ f (œÄ),
will quickly find an invalid input for which the learned score model f outputs erroneously large
values (Q2). Furthermore, it is particularly severe if we perform the inference directly over the
parameters of policy networks, accounting for the fact that the parameters of behavior policies lie on
a narrow manifold in a high-dimensional parametric space."
TASK EMBEDDING AND CONSERVATIVE MBO,0.11279826464208242,"Task embedding. To enable feasible out-level policy inference, we propose to decouple MBO
techniques from high-dimensional policy parameters. We achieve this by learning a latent embedding
space Z with an information bottleneck (dim(Z) ‚â™min(N, dim(Œ†))). As long as the high-
dimensional parameters of behavior policies can be inferred from the embedding z ‚ààZ, we can use
z to represent the corresponding sub-task (or the behavior policy). Formally, we learn a deterministic
task embedding5 œï : RN ‚ÜíZ and a contextual behavior policy Œ≤ : S √óZ √óA ‚Üí[0, 1] that replaces"
TASK EMBEDDING AND CONSERVATIVE MBO,0.11496746203904555,"4See more data decomposition rules in Appendix A.2.
5We feed the one-hot encoding of the sub-task specification (n = 1, . . . , N) into the embedding network œï.
We write the embedding function as œï(z|n) instead of œï(n) to emphasize that the output of œï is z."
TASK EMBEDDING AND CONSERVATIVE MBO,0.11713665943600868,"N separate behavior policies in Equation 4:
Œ≤(a|s, z), œï(z|n) ‚Üêarg max
Œ≤,œï EDn‚àºD[N]E(s,a)‚àºDn [log Œ≤(a|s, œï(z|n))] ,
(6)"
TASK EMBEDDING AND CONSERVATIVE MBO,0.1193058568329718,"Conservative MBO. In principle, by substituting the learned task embedding œï(z|n) and contextual
behavior policy Œ≤(a|s, z) into Equation 5, we can then conduct MBO over the embedding space: first"
TASK EMBEDDING AND CONSERVATIVE MBO,0.12147505422993492,"learning f : Z ‚ÜíR with minf En,œï(z|n)
h
(f(z) ‚àíJ(Œ≤n))2i
, and then setting the optimal embedding"
TASK EMBEDDING AND CONSERVATIVE MBO,0.12364425162689804,"with z‚àó= arg maxz f(z) and the corresponding optimal policy with œÄ‚àó(a|s) = Œ≤(a|s, z‚àó). However,
we must deliberate a new OOD issue in the Z-space, stemming from the original distribution shift in
the parametric space when directly optimizing Equation 5."
TASK EMBEDDING AND CONSERVATIVE MBO,0.12581344902386118,"Motivated by the energy model [38] and the conservative regularization [35], we introduce a conser-
vative score model objective, additionally regularizing the scores of OOD embeddings ¬µ(z):"
TASK EMBEDDING AND CONSERVATIVE MBO,0.1279826464208243,"f ‚Üêarg min
f
En,œï(z|n)
h 
f(z) ‚àíJ(Œ≤n)
2i
, s.t. E¬µ(z) [f(z)] ‚àíEn,œï(z|n) [f(z)] ‚â§Œ∑,
(7)"
TASK EMBEDDING AND CONSERVATIVE MBO,0.1301518438177874,"where we set ¬µ(z) to be the uniform distribution over Z-space. Intuitively, as long as the scores of
OOD embeddings E¬µ(z) [f(z)] is lower than that of in-distribution embeddings En,œï(z|n) [f(z)] (up
to a threshold Œ∑), conducting embedding inference with z‚àó= arg maxz f(z) would produce the best
and confident solution, avoiding towards OOD embeddings that are far away from the training set."
TASK EMBEDDING AND CONSERVATIVE MBO,0.13232104121475055,"Now we have reframed the non-iterative bi-level offline RL problem as one of offline MBO: in the
inner-level optimization (Q1), we set the practical choice for "" "" as the learned score model f (A1);
in the outer-level optimization (Q2), we introduce task embedding and conservative regularization to
avoid over-optimistic exploitation when exploiting f for policy/embedding inference (A2). In the
next section, we will show how to slightly change the form of the score model f, so as to leverage
the sequential characteristic (loop in Figure 1) of RL tasks and answer the left Q3."
TEST-TIME ADAPTATION,0.13449023861171366,"3.3
Test-time Adaptation"
TEST-TIME ADAPTATION,0.13665943600867678,"Recalling that we update f(z) to regress the value at initial states Es0

V Œ≤n(s0)

in Equation 7, we
then conduct outer-level inference with z‚àó= arg maxz f(z) and rollout the z‚àó-conditioned policy
œÄ‚àó(a|s) := Œ≤(a|s, z‚àó) until the end of testing/deployment rollout episode. In essence, such an
inference can produce a confident extrapolation over the distribution of behavior policy embeddings.
Going beyond the outer-level policy/embedding inference only at the initial states, we propose that
we can benefit from performing inference at any rollout state in deployment (A3)."
TEST-TIME ADAPTATION,0.13882863340563992,"To enable test-time adaptation, we model the score model with f : S √ó A √ó Z ‚ÜíR, taking
a state-action as an extra input. Then, we encourage the score model to regress the values of
behavior policies over all state-action pairs, minf En,œï(z|n)E(s,a)‚àºDn
h 
f(s, a, z) ‚àíQŒ≤n(s, a)
2i
."
TEST-TIME ADAPTATION,0.14099783080260303,"For simplicity, instead of learning a separate value function QŒ≤n for each behavior policy, we learn
the score model directly with the TD-error used for learning the value function QŒ≤n(s, a) as in
Equation 1, together with the conservative regularization in Equation 7:"
TEST-TIME ADAPTATION,0.14316702819956617,"f ‚Üêarg min
f
EDn‚àºD[N]E(s,a,s‚Ä≤,a‚Ä≤)‚àºDn
h 
R(s, a) + Œ≥ ¬Øf(s‚Ä≤, a‚Ä≤, œï(z|n)) ‚àíf(s, a, œï(z|n))
2i
, (8)"
TEST-TIME ADAPTATION,0.14533622559652928,"s.t. En,¬µ(z)Es‚àºDn,a‚àºŒ≤(a|s,z) [f(s, a, z)] ‚àíEn,œï(z|n)Es‚àºDn,a‚àºŒ≤(a|s,z) [f(s, a, z)] ‚â§Œ∑,"
TEST-TIME ADAPTATION,0.1475054229934924,where ¬Øf denotes a target network and we update the target ¬Øf with soft updates: ¬Øf = (1 ‚àíœÖ) ¬Øf + œÖf.
TEST-TIME ADAPTATION,0.14967462039045554,"In test-time, we thus can dynamically adapt the outer-level optimization, setting the inferred optimal
policy œÄ‚àó(a|s) = Œ≤(a|s, z‚àó(s)), where z‚àó(s) = arg maxz f
 
s, Œ≤(a|s, z), z

. Specifically, at any
state s in the test-time, we can perform gradient ascent to find the optimal behavior embedding:
z‚àó(s) = z‚ó¶
K(s) := GradAscent(s, z‚ó¶
0, K), where z‚ó¶
0 is the starting point and for k = 0, 1 . . . , K ‚àí1,
z‚ó¶
k+1(s) ‚Üêz‚ó¶
k(s) + Œ∑‚àázf(s, Œ≤(a|s, z), z))|z=z‚ó¶
k.
(9)"
CONNECTION TO PRIOR NON-ITERATIVE OFFLINE METHODS,0.15184381778741865,"3.4
Connection to Prior Non-iterative Offline Methods"
CONNECTION TO PRIOR NON-ITERATIVE OFFLINE METHODS,0.1540130151843818,"In Table 1, we summarize the comparison with prior representative non-iterative offline RL methods.
Intuitively, our DROP (using returns to decompose D) is similar in spirit to F-BC6 and RvS-R [14],"
CONNECTION TO PRIOR NON-ITERATIVE OFFLINE METHODS,0.1561822125813449,6F-BC performs BC over filtered trajectories with high returns.
CONNECTION TO PRIOR NON-ITERATIVE OFFLINE METHODS,0.15835140997830802,"Table 1: Comparison of non-iterative bi-level offline RL methods, where we use R(œÑ) to represent
the return of a sampling trajectory œÑ and use R(s, a) to represent the expected return of trajectories
starting from (s, a). The checkmark in A2 indicates whether the exploitation to "" "" is regularized for
outer-level optimization and that in A3 indicates whether test-time adaptation is supported."
CONNECTION TO PRIOR NON-ITERATIVE OFFLINE METHODS,0.16052060737527116,"Inner-level
Outer-level
"" "" in A1
A2
A3"
CONNECTION TO PRIOR NON-ITERATIVE OFFLINE METHODS,0.16268980477223427,"F-BC
BC over filtered œÑ with high R(œÑ)
‚Äî
œÄ(a|s)
%
%
RvS-R [14]
minœÄ ‚àíE [log œÄ(a|s, R(œÑ))]
handcraft Rtarget
œÄ(a|s, ¬∑)
%
%
Onestep [6]
minQ L(Q(s, a), R(s, a))
arg maxa QŒ≤(s, Œ≤(a|s))
QŒ≤(s, a)
%
""
COMs [62]
minf L(f(Œ≤œÑ), R(œÑ))
arg maxŒ≤ f(Œ≤)
f(Œ≤)
""
%
DROP (ours)
minf L(f(s, a, z), R(s, a, z))
arg maxz f(s, Œ≤(a|z), z)
f(s, a, z)
""
"""
CONNECTION TO PRIOR NON-ITERATIVE OFFLINE METHODS,0.1648590021691974,"both of which use return R(œÑ) to guide the inner-level optimization. However, both F-BC and RvS-R
leave Q2 unanswered. In outer-level, F-BC can not enable policy extrapolation, which heavily relies
on the data quality in offline tasks. RvS-R needs to handcraft a target return (as the contextual
variable for œÄ(a|s, ¬∑)), which also probably triggers the potential distribution mismatch7 between the
hand-crafted contextual variable (i.e., the desired return) and the actual rollout return induced by the
learned policy when conditioning on the given contextual variable."
CONNECTION TO PRIOR NON-ITERATIVE OFFLINE METHODS,0.16702819956616052,"Diving deeper into the bi-level optimization, we can also find DROP combines the advantages of On-
estep [6] and COMs [62], where Onestep answers Q3 and performs outer-level optimization in action
space (arg maxa), COMs answers Q2 and performs that in parameter space (arg maxŒ≤), while our
DROP answers both Q2 and Q3 and performs that in embedding space (arg maxz). Specifically, DROP
thus allows us to (A2) conduct safe extrapolation in outer-level and (A3) perform test-time adaptation
in testing rather than simply conducting outer-level optimization at initial states as in COMs."
PRACTICAL IMPLEMENTATION,0.16919739696312364,"3.5
Practical Implementation"
PRACTICAL IMPLEMENTATION,0.17136659436008678,"Algorithm 1 DROP (Training)
Require: Dataset of trajectories: D = {œÑ}."
PRACTICAL IMPLEMENTATION,0.1735357917570499,"1: Decompose D into N sub-sets D[N].
2: Initialize œï(z|n), Œ≤(a|s, z), and f(s, a, z).
3: while not converged do
4:
Sample a sub-task: Dn ‚àºD[N].
5:
Learn œï, Œ≤, and f with Equations 6 and 8.
6: end while
Return: Œ≤(a|s, z) and f(s, a, z)."
PRACTICAL IMPLEMENTATION,0.175704989154013,"Algorithm 2 DROP (Testing/Deployment)
Require: Env, Œ≤(a|s, z), and f(s, a, z)."
PRACTICAL IMPLEMENTATION,0.17787418655097614,"1: s0 = Env.Reset().
2: while not done do
3:
Test-time adaptation:
z‚àó(st) = arg maxz f(st, Œ≤(at|st, z), z).
4:
Sample action: at ‚àºŒ≤(at|st, z‚àó(st)).
5:
Step Env: st+1 ‚àºP(st+1|st, at).
6: end while"
PRACTICAL IMPLEMENTATION,0.18004338394793926,"We now summarize the DROP algorithm (see Algorithm 1 for the training phase and Algorithm 2 for
the testing phase). During training (inner-level optimization), we alternate between updating œï(z|n),
Œ≤(a|s, z), and f(s, a, z), wherein we update œï with both maximum likelihood loss and TD-error loss
in Equations 6 and 8. During testing (outer-level optimization), we use the gradient ascent as specified
in Equation 9 to infer the optimal embedding z‚àó. Instead of simply sampling a single starting point
z‚ó¶
0, we choose N starting points corresponding to all the embeddings {zn} of sub-tasks, and then
choose the optimal z‚àó(s) from those updated embeddings for which the learned f outputs the highest
score, i.e., z‚àó(s) = arg maxz f(s, Œ≤(a|s, z), z) s.t. z ‚àà{GradAscent(s, zn, K)|n = 1, . . . , N}.
Then, we sample action from œÄ‚àó(a|s) := Œ≤(a|s, z‚àó(s))."
RELATED WORK,0.1822125813449024,"4
Related Work"
RELATED WORK,0.1843817787418655,"Offline RL. In offline RL, learning with offline data is prone to exploiting OOD state-actions and
producing over-estimated values, which makes vanilla iterative policy/value optimization challenging.
To eliminate the problem, a number of methods have been explored, in essence, by either introducing
a policy/value regularization in the iterative loop or trying to eliminate the iterative loop itself."
RELATED WORK,0.18655097613882862,"Iterative methods: Sticking with the normal iterative updates in RL, offline policy regularization
methods aim to keep the learning policy to be close to the behavior policy under a probabilistic"
RELATED WORK,0.18872017353579176,"7For more illustrative examples w.r.t. this mismatch, we refer the reader to Figure 6 of the RvS-R paper [14]."
RELATED WORK,0.19088937093275488,"distance [8, 19, 30, 33, 45, 46, 52, 54, 58, 67, 73]. Some works also conduct implicit policy
regularization with variants of importance sampling [39, 47, 51]. Besides regularizing policy, it is
also feasible to constrain the substitute value function in the iterative loop. Methods constraining
the value function aim at mitigating the over-estimation, which typically introduces pessimism to
the prediction of the Q-values [9, 27, 35, 41, 48, 49] or penalizes the value with an uncertainty
quantification [3, 5, 56, 68], making the value for OOD state-actions more conservative. Similarly,
another branch of model-based methods [29, 71, 72, 57] also perform iterative bi-level updates,
alternating between regularized evaluation and improvement. Different from this iterative paradigm,
DROP only evaluates values of behavior policies in the inner-level optimization, avoiding the potential
overestimation for values of learning policies and eliminating error propagation between two levels."
RELATED WORK,0.19305856832971802,"Non-iterative methods: Another complementary line of work studies how to eliminate the iterative
updates, which simply casts RL as a weighted or conditional imitation learning problem (Q1). Derived
from the behavior-regularization RL [22, 64], the former conducts weighted behavior cloning: first
learning a value function for the behavior policy, then weighing the state-action pairs with the learned
values or advantages [1, 12, 66, 54]. Besides, some works propose an implicit behavior policy
regularization that similarly does not estimate values of new learning policies: initializing the learning
policy with a behavior policy [50] or performing only a ""one-step"" update (policy improvement)
over the behavior policy [20, 24, 75]. For the latter, this branch method typically builds upon the
hindsight information matching [4, 15, 28, 36, 44, 43, 55, 65], assuming that the future trajectory
information can be useful to infer the middle decision that leads to the future and thus relabeling
the trajectory with the reached states or returns. Due to the simplicity and stability, RvS-based
methods advocate for learning a goal-conditioned or reward-conditioned policy with supervised
regression [10, 13, 14, 21, 25, 42, 60, 69]. However, these works do not fully exploit the non-iterative
framework and fail to answer the raised questions, which either does not regularize the inner-level
optimization when exploiting "" "" in outer-level (Q2), or does not support test-time adaptation (Q3)."
RELATED WORK,0.19522776572668113,"Offline MBO. Similar to offline RL, the main challenge of MBO is to reason about uncertainty and
OOD values [7, 16], since a direct gradient-ascent against the learned score model can easily produce
invalid inputs that are falsely and highly scored. To counteract the effect of model exploitation,
prior works introduce various techniques, including normalized maximum likelihood estimation [17],
model inversion networks [32], local smoothness prior [70], and conservative objective models
(COMs) [62]. Compared to COMs, DROP shares similarities with the conservative model, but
instantiates on the embedding space instead of the parameter space. Such difference is nontrivial, not
only because DROP avoids the adversarial optimization employed in COMs, but also because DROP
allows test-time adaptation, enabling dynamical inference across states at testing."
EXPERIMENTS,0.19739696312364424,"5
Experiments"
EXPERIMENTS,0.19956616052060738,"In this section, we present our empirical results. We first give examples to illustrate the test-time
adaptation. Then we evaluate DROP against prior offline RL algorithms on the D4RL benchmark.
Finally, we provide the computation cost regarding the test-time adaptation protocol. For more offline-
to-online fine-tuning results, ablation studies w.r.t. the decomposition rules and the conservative
regularization, and training details on the hyper-parameters, we refer the readers to the appendix."
EXPERIMENTS,0.2017353579175705,"Illustration of test-time adaptation. To better understand the test-time adaptation of DROP, we
include four comparisons that exhibit different embedding inference rules at testing/deployment:"
EXPERIMENTS,0.2039045553145336,"(1) DROP-Best: At initial state s0, we choose the best embedding from those embeddings of behavior
policies, z‚àó
0(s0) = arg maxz f(s0, Œ≤(a0|s0, z), z) s.t. z ‚ààz[N] := {z1, . . . , zN}, and keep this
embedding fixed for the entire episode, i.e., setting œÄ‚àó(at|st) = Œ≤(at|st, z‚àó
0(s0))."
EXPERIMENTS,0.20607375271149675,"(2) DROP-Grad: At initial state s0, we conduct inference (gradient ascent on starting point z‚àó
0(s0))
with z‚àó(s0) = arg maxz f(s0, Œ≤(a0|s0, z), z), and keep z‚àó(s0) fixed throughout the test rollout."
EXPERIMENTS,0.20824295010845986,"(3) DROP-Best-Ada: We adapt the policy by setting œÄ‚àó(at|st) = Œ≤(at|st, z‚àó
0(st)), where we choose
the best embedding z‚àó
0(st) directly from those embeddings of behavior policies for which the score
model outputs the highest score, i.e., z‚àó
0(st) = arg maxz f(st, Œ≤(at|st, z), z) s.t. z ‚ààz[N]."
EXPERIMENTS,0.210412147505423,"(4) DROP-Grad-Ada (as described in Section 3.5): We set œÄ‚àó(at|st)= Œ≤(at|st, z‚àó(st)) and choose
the best embedding z‚àó(st) from those updated embeddings of behavior policies, i.e., z‚àó(st) =
arg maxz f(st, Œ≤(at|st, z), z) s.t. z ‚àà{GradAscent(st, zn, K)|n = 1, . . . , N}."
EXPERIMENTS,0.21258134490238612,"Grad. Ascent
z[N]
z *
0 (s0)
z * (s0)
z *
0 (st)
z * (st)"
EXPERIMENTS,0.21475054229934923,"0.0
0.8
0.00 0.25 0.50 0.75"
EXPERIMENTS,0.21691973969631237,"(a)
0.0
0.8
0.00 0.25 0.50 0.75 (b) t=0 t=50 t=100"
EXPERIMENTS,0.21908893709327548,"0.0
0.8
0.00 0.25 0.50 0.75 (c1) t=17"
EXPERIMENTS,0.22125813449023862,"0.0
0.8
0.00 0.25 0.50 0.75 (c2) t=50 0 50 100"
EXPERIMENTS,0.22342733188720174,Normalized return (d)
EXPERIMENTS,0.22559652928416485,"Figure 3: Visualization of the embedding inference (a, b, c1, c2) and performance comparison (d).
z[N] denotes embeddings of all behavior policies. z‚àó
0(s0), z‚àó(s0), z‚àó
0(st), and z‚àó(st) denote the se-
lected embeddings in DROP- Best, Grad, Best-Ada, and Grad-Ada implementations respectively."
EXPERIMENTS,0.227765726681128,"40
50
60
normalized return COMs"
EXPERIMENTS,0.2299349240780911,"DT
RvS-R
Onestep
DROP-BA
DROP-GA"
EXPERIMENTS,0.23210412147505424,Median
EXPERIMENTS,0.23427331887201736,"40
50
60
normalized return IQM"
EXPERIMENTS,0.23644251626898047,"40
50
60
normalized return Mean"
EXPERIMENTS,0.2386117136659436,"0.8
0.6
P(X > Y)"
EXPERIMENTS,0.24078091106290672,DROP-BA
EXPERIMENTS,0.24295010845986983,DROP-BA
EXPERIMENTS,0.24511930585683298,DROP-GA
EXPERIMENTS,0.2472885032537961,DROP-GA
EXPERIMENTS,0.24945770065075923,Algorithm X COMs
EXPERIMENTS,0.25162689804772237,Onestep COMs
EXPERIMENTS,0.25379609544468545,"Onestep
  Algorithm Y
Probability of improvement"
EXPERIMENTS,0.2559652928416486,"Figure 4: (Left) Aggregate metrics [2] on 12 D4RL tasks. Higher median, IQM, and mean scores
are better. (Right) Each row shows the probability of improvement that the algorithm X on the left
outperforms algorithm Y on the right. The CIs are estimated using the percentile bootstrap (95%)
with stratified sampling. For all results of our method, we average the normalized returns across 5
seeds; for each seed, we run 10 evaluation episodes. (GA: Grad-Ada. BA: Best-Ada.)"
EXPERIMENTS,0.25813449023861174,"In Figure 3, we visualize the four different inference rules and report the corresponding performance
in the halfcheetah-medium-expert task [18]. In Figure 3 (a), we set the starting point as the best
embedding z‚àó
0(s0) in z[N], and perform gradient ascent to find the optimal z‚àó
0(s0) for DROP-Grad. In
Figure 3 (b), we can find that at different time steps, DROP-Best-Ada chooses different embeddings
for Œ≤(at|st, ¬∑). Intuitively, performing such a dynamical embedding inference enables us to combine
different embeddings, thus stitching behavior policies at different states. Further, in Figure 3 (c1, c2),
we find that performing additional inference (with gradient ascent) in DROP-Grad-Ada allows to
extrapolate beyond the embeddings of behavior policies, and thus results in sequential composition
of new embeddings (policies) across different states. For practical impacts of these different inference
rules, we provide the performance comparison in Figure 3 (d), where we can find that performing
gradient-based optimization (*-Grad-*) outperforms the natural selection among those embeddings of
behavior policies/sub-tasks (*-Best-*), and rollout with adaptive embedding inference (DROP-*-Ada)
outperforms that with fixed embeddings (DROP-Best and DROP-Grad). In subsequent experiments,
if not explicitly stated, DROP takes the DROP-Grad-Ada implementation by default."
EXPERIMENTS,0.2603036876355748,"Empirical performance on benchmark tasks. We evaluate DROP on a number of tasks from the
D4RL dataset and make comparisons with prior non-iterative offline RL counterparts8."
EXPERIMENTS,0.26247288503253796,"As suggested by Agarwal et al. [2], we provide the aggregate comparison results to account for
the statistical uncertainty. In Figure 4, we report the median, IQM [2], and mean scores of DROP
and prior non-iterative offline baselines in 12 D4RL tasks (see individual comparison results in
Appendix C). We can find our DROP (Best-Ada and Grad-Ada) consistently outperforms prior non-
iterative baselines in both median, IQM, and mean metrics. Compared to the most related baseline
Onestep and COMs (see Table 1), we provide the probability of performance improvement in Figure 4
right. We can find that DROP-Grad-Ada shows a robust performance improvement over Onestep and
COMs, with an average improvement probability of more than 80%."
EXPERIMENTS,0.2646420824295011,"Comparison with latent policy methods. Note that one additional merit of DROP is that it
naturally accounts for hybrid modes in D by conducting task decomposition in inner-level, we
thus compare DROP to latent policy methods (PLAS [74] and LAPO [11]) that use conditional
variational autoencoder (CVAE) to model offline data and also account for multi-modes in offline data.
Essentially, both our DROP and baselines (PLAS and LAPO) learn a latent policy in the inner-level
optimization, except that we adopt the non-iterative bi-level learning while baselines are instantiated"
EXPERIMENTS,0.2668112798264642,"8Due to the page limit, we provide the comparison with prior iterative baselines in Appendix C."
EXPERIMENTS,0.26898047722342733,"Table 2: Comparison in AntMaze and Gym-MuJoCo domains (v2). Results are averaged over 5
seeds; for each seed, we run 10 evaluation episodes. u: umaze (antmaze). um: umaze-medium. ul:
umaze-large. p: play. d: diverse. wa: walker2d. ho: hopper. ha: halfcheetah. r: random. m: medium."
EXPERIMENTS,0.27114967462039047,"u
u-d
um-p
um-d
ul-p
ul-d
wa-r
ho-r
ha-r
wa-m
ho-m
ha-m
mean"
EXPERIMENTS,0.27331887201735355,"CQL
74.0
84.0
61.2
53.7
15.8
14.9
-0.2
8.3
22.2
82.1
71.6
49.8
44.8
IQL
87.5
62.2
71.2
70.0
39.6
47.5
5.4
7.9
13.1
77.9
65.8
47.8
49.7
PLAS
70.7
45.3
16.0
0.7
0.7
0.3
9.2
6.7
26.5
75.5
51.0
44.5
28.9
LAPO
86.5
80.6
68.5
79.2
48.8
64.8
1.3
23.5
30.6
80.8
51.6
46.0
55.2
DROP
90.5
92.2
74.1
82.9
57.2
63.3
5.2
20.8
32.0
82.1
74.9
52.4
60.6
DROP std
¬±2.4
¬±1.7
¬±3.9
¬±3.5
¬±5.5
¬±2.4
¬±1.6
¬±0.3
¬±2.5
¬±5.2
¬±2.8
¬±2.2"
EXPERIMENTS,0.2754880694143167,"under the iterative paradigm. By answering Q3, DROP permits test-time adaptation, enabling us to
dynamically stitch ""skills""(latent behaviors/embeddings as shown in Figure 3) and thus encouraging
high-level abstract exploitation in testing. However, the aim of introducing the latent policy in PLAS
and LAPO is to regularize the inner-level optimization, which fairly answers Q2 in the iterative offline
counterpart but can not provide the potential benefit (test-time adaptation) by answering Q3."
EXPERIMENTS,0.27765726681127983,"Note in our naive DROP implementation, we heuristically use the return to conduct task decomposition
(motivated by RvS-R), while PLAS and LAPO take each trajectory as a sub-task and use CVAE to
learn the latent policy. For a fair comparison, here we also adopt CVAE to model the offline data
and afterward take the learned latent embedding in CVAE as the embedding of behaviors, instead of
conducting return-guided task decomposition. We provide implementation details (DROP+CVAE)
in Appendix A.4 and the comparison results in Table 2 (taking DROP-Grad-Ada implementation).
We can observe that our DROP consistently achieves better performance than PLAS in all tasks, and
performs better or comparably to LAPO in 10 out of 12 tasks. Additionally, we also provide the results
of CQL and IQL [31]. We can find that DROP also leads to significant improvement in performance,
consistently demonstrating the competitive performance of DROP against state-of-the-art offline
iterative/non-iterative baselines."
EXPERIMENTS,0.279826464208243,Diffuser
EXPERIMENTS,0.28199566160520606,Diffuser*
EXPERIMENTS,0.2841648590021692,Onestep
EXPERIMENTS,0.28633405639913234,DROP(1)
EXPERIMENTS,0.2885032537960954,DROP(10)
EXPERIMENTS,0.29067245119305857,DROP(20)
EXPERIMENTS,0.2928416485900217,DROP(50)
EXPERIMENTS,0.2950108459869848,DROP(100)
EXPERIMENTS,0.29718004338394793,DROP(150)
EXPERIMENTS,0.2993492407809111,"DROP(200) 10
2 10
0"
EXPERIMENTS,0.30151843817787416,Average inference time (1) (10) (20) (50) (100) (150) (200) 40 45
EXPERIMENTS,0.3036876355748373,DROP's scores on AntMaze
EXPERIMENTS,0.30585683297180044,"Figure 5: Computation cost across different
models and DROP implementations with vary-
ing inference time intervals.
The number in
parenthesis denotes the inference time intervals.
In top-right, we also provide the average perfor-
mance of DROP on AntMaze tasks when using
different inference time intervals. (Diffuser* de-
notes the warm-start implementation of Diffuser.)"
EXPERIMENTS,0.3080260303687636,"Computation cost. One limitation of DROP
is that conducting test-time adaptation at each
state is usually expensive for inference time.
To reduce the computation cost, we find that
we can resample (infer) the best embedding z‚àó
after a certain time interval, i.e., z‚àódoes not
necessarily need to be updated at each testing
state. In Figure 5, we plot the average inference
time versus different models and DROP imple-
mentations with varying inference time intervals.
We see that compared to Diffuser (one offline
RL method that also supports test-time adapta-
tion, see quantitative performance comparison
in Table 4) [26], DROP can achieve significantly
lower computation costs. In Figure 5 top-right,
we also illustrate the trade-off between perfor-
mance and runtime budget as we vary the infer-
ence time intervals. We find that by choosing a
suitable time interval, we can reduce the inference cost with only a modest drop in performance:
DROP(50) brings less computational burden while providing stable performance."
EXPERIMENTS,0.31019522776572667,"Table 3: Comparison to ""distilled"" DROP (D-
DROP), using Grad-Ada and CVAE implementation."
EXPERIMENTS,0.3123644251626898,"wa-r
ho-r
ha-r
wa-m
ho-m
ha-m"
EXPERIMENTS,0.31453362255965295,"CQL
-0.2
8.3
22.2
82.1
71.6
49.8
IQL
5.4
7.9
13.1
77.9
65.8
47.8
D-DROP
4.5
18.9
27.8
84.2
67.1
50.9
DROP
5.2
20.8
32.0
82.1
74.9
52.4"
EXPERIMENTS,0.31670281995661603,"Comparison to a ""distilled"" DROP imple-
mentation. Our answer to Q3 is that we con-
duct outer-level optimization in the testing
phase. Similarly, we can design a ""distilled""
DROP implementation: we keep the same
treatment as DROP for Q1and Q2, but we per-
form outer-level optimization on the existing
offline data instead of on the testing states,
i.e., conducting optimization in z-space for
all states in the dataset. Then, we can ""distill"" the resulting contextual policy and the inferred"
EXPERIMENTS,0.3188720173535792,"z‚àóinto a fixed rollout policy. As shown in Table 3, we can see that such a ""distilled"" DROP im-
plementation achieves competitive performance, approaching DROP‚Äôs overall score and slightly
outperforming CQL and IQL, which further supports the superiority afforded by the non-iterative
offline RL paradigm versus the iterative one."
EXPERIMENTS,0.3210412147505423,"Table 4: Comparison to adaptive baselines (APE-V and
Diffuser). DROP uses Grad-Ada and CVAE implementation."
EXPERIMENTS,0.3232104121475054,"wa-mr
ho-mr
ha-mr
wa-me
ho-me
ha-me"
EXPERIMENTS,0.32537960954446854,"APE-V
82.9
98.5
64.6
110.0
105.7
101.4
Diffuser
70.6
93.6
37.7
106.9
103.3
88.9
DROP
83.5
96.3
50.9
109.3
107.2
102.2"
EXPERIMENTS,0.3275488069414317,"Comparison to adaptive baselines.
One merit of DROP resides in its test-
time adaptation ability across rollout
states. Thus, here we compare DROP
with two offline RL methods that also
support test-time adaptation: 1) APE-
V [23] learns an uncertainty-adaptive
policy and dynamically updates a con-
textual belief vector at test states, and 2) Diffuser [26] employs a score/return function to guide the
diffusion denoising process, i.e., score-guided action sampling. As shown in Table 4, we can see
that in medium-replay (mr) and medium-expert (me) domains (v2), DROP can achieve comparable
results to APE-V, with a clear improvement over Diffuser."
DISCUSSION,0.3297180043383948,"6
Discussion"
DISCUSSION,0.3318872017353579,"In this work, we introduce non-iterative bi-level offline RL, and based on this paradigm, we raise
three questions (Q1, Q2, and Q3). To answer that, we reframe the offline RL problem as one of MBO
and learn a score model (A1), introduce embedding learning and conservative regularization (A2),
and propose test-time adaptation in testing (A3). We evaluate DROP on various tasks, showing that
DROP gains comparable or better performance compared to prior methods."
DISCUSSION,0.33405639913232105,"Limitations. DROP also has several limitations. First, the offline data decomposition dominates
the following bi-level optimization, and thus choosing a suitable decomposition rule is a crucial
requirement for policy inference (see experimental analysis in Appendix A.2). An exciting direction
for future work is to study generalized task decomposition rules (e.g., our DROP+CVAE implemen-
tation). Second, we find that when the number of sub-tasks is too large, the inference is unstable,
where adjacent checkpoint models exhibit larger variance in performance (such instability also exists
in prior offline RL methods, discovered by Fujimoto and Gu [19]). One natural approach to this
instability is conducting online fine-tuning (see Appendix A.3 for our empirical studies)."
DISCUSSION,0.3362255965292842,"Going forward, we believe our work suggests a feasible alternative for generalizable offline robotic
learning: by decomposing a single robotic dataset into multiple subsets, offline policy inference can
benefit from performing MBO and the test-time adaptation protocol."
DISCUSSION,0.3383947939262473,Acknowledgments and Disclosure of Funding
DISCUSSION,0.3405639913232104,"We sincerely thank the anonymous reviewers for their insightful suggestions. This work was
supported by the National Science and Technology Innovation 2030 - Major Project (Grant No.
2022ZD0208800), and NSFC General Program (Grant No. 62176215)."
REFERENCES,0.34273318872017355,References
REFERENCES,0.34490238611713664,"[1] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and
Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920,
2018."
REFERENCES,0.3470715835140998,"[2] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Belle-
mare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural
information processing systems, 34:29304‚Äì29320, 2021."
REFERENCES,0.3492407809110629,"[3] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline
reinforcement learning with diversified q-ensemble. Advances in Neural Information Processing
Systems, 34, 2021."
REFERENCES,0.351409978308026,"[4] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,
Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience
replay. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.35357917570498915,"[5] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhihong Deng, Animesh Garg, Peng Liu, and
Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning.
arXiv preprint arXiv:2202.11566, 2022."
REFERENCES,0.3557483731019523,"[6] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without
off-policy evaluation. Advances in Neural Information Processing Systems, 34:4933‚Äì4946,
2021."
REFERENCES,0.3579175704989154,"[7] David Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling
for robust design. In International conference on machine learning, pages 773‚Äì782. PMLR,
2019."
REFERENCES,0.3600867678958785,"[8] Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral priors
and dynamics models: Improving performance and domain transfer in offline rl. arXiv preprint
arXiv:2106.09119, 2021."
REFERENCES,0.36225596529284165,"[9] Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex
Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised
offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021."
REFERENCES,0.3644251626898048,"[10] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. Advances in neural information processing systems, 34, 2021."
REFERENCES,0.3665943600867679,"[11] Xi Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin Liang, Chelsea
Finn, and Chongjie Zhang. Latent-variable advantage-weighted policy optimization for offline
rl. arXiv preprint arXiv:2203.08949, 2022."
REFERENCES,0.368763557483731,"[12] Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail:
Best-action imitation learning for batch deep reinforcement learning.
arXiv preprint
arXiv:1910.12179, 2019."
REFERENCES,0.37093275488069416,"[13] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation
learning. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.37310195227765725,"[14] Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential
for offline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021."
REFERENCES,0.3752711496746204,"[15] Ben Eysenbach, Xinyang Geng, Sergey Levine, and Russ R Salakhutdinov. Rewriting history
with inverse rl: Hindsight inference for policy improvement. Advances in neural information
processing systems, 33:14783‚Äì14795, 2020."
REFERENCES,0.3774403470715835,"[16] Clara Fannjiang and Jennifer Listgarten. Autofocused oracles for model-based design. Advances
in Neural Information Processing Systems, 33:12945‚Äì12956, 2020."
REFERENCES,0.3796095444685466,"[17] Justin Fu and Sergey Levine. Offline model-based optimization via normalized maximum
likelihood estimation. arXiv preprint arXiv:2102.07970, 2021."
REFERENCES,0.38177874186550975,"[18] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets
for deep data-driven reinforcement learning. CoRR, abs/2004.07219, 2020. URL https:
//arxiv.org/abs/2004.07219."
REFERENCES,0.3839479392624729,"[19] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.38611713665943603,"[20] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning
without exploration. In International Conference on Machine Learning, pages 2052‚Äì2062.
PMLR, 2019."
REFERENCES,0.3882863340563991,"[21] Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for
offline hindsight information matching. arXiv preprint arXiv:2111.10364, 2021."
REFERENCES,0.39045553145336226,"[22] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2160‚Äì2169.
PMLR, 2019."
REFERENCES,0.3926247288503254,"[23] Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline rl policies should be
trained to be adaptive. In International Conference on Machine Learning, pages 7513‚Äì7530.
PMLR, 2022."
REFERENCES,0.3947939262472885,"[24] Caglar Gulcehre, Sergio G√≥mez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine,
Konrad Zolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and Nando de Freitas. Regu-
larized behavior value estimation. arXiv preprint arXiv:2103.09575, 2021."
REFERENCES,0.3969631236442516,"[25] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big
sequence modeling problem. Advances in neural information processing systems, 34, 2021."
REFERENCES,0.39913232104121477,"[26] Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion
for flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022."
REFERENCES,0.40130151843817785,"[27] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning, pages 5084‚Äì5096. PMLR, 2021."
REFERENCES,0.403470715835141,"[28] Yachen Kang, Diyuan Shi, Jinxin Liu, Li He, and Donglin Wang. Beyond reward: Offline
preference-guided policy optimization. arXiv preprint arXiv:2305.16217, 2023."
REFERENCES,0.40563991323210413,"[29] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel:
Model-based offline reinforcement learning. Advances in neural information processing systems,
33:21810‚Äì21823, 2020."
REFERENCES,0.4078091106290672,"[30] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement
learning with fisher divergence critic regularization. In International Conference on Machine
Learning, pages 5774‚Äì5783. PMLR, 2021."
REFERENCES,0.40997830802603036,"[31] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit
q-learning. arXiv preprint arXiv:2110.06169, 2021."
REFERENCES,0.4121475054229935,"[32] Aviral Kumar and Sergey Levine. Model inversion networks for model-based optimization.
Advances in Neural Information Processing Systems, 33:5126‚Äì5137, 2020."
REFERENCES,0.41431670281995664,"[33] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-
policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing
Systems, 32, 2019."
REFERENCES,0.4164859002169197,"[34] Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint
arXiv:1912.13465, 2019."
REFERENCES,0.41865509761388287,"[35] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning
for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:
1179‚Äì1191, 2020."
REFERENCES,0.420824295010846,"[36] Yao Lai, Jinxin Liu, Zhentao Tang, Bin Wang, HAO Jianye, and Ping Luo. Chipformer:
Transferable chip placement via offline decision transformer. ICML, 2023. URL https:
//openreview.net/pdf?id=j0miEWtw87."
REFERENCES,0.4229934924078091,"[37] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In
Reinforcement learning, pages 45‚Äì73. Springer, 2012."
REFERENCES,0.42516268980477223,"[38] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based
learning. Predicting structured data, 1(0), 2006."
REFERENCES,0.42733188720173537,"[39] Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. Optidice:
Offline policy optimization via stationary distribution correction estimation. In International
Conference on Machine Learning, pages 6120‚Äì6130. PMLR, 2021."
REFERENCES,0.42950108459869846,"[40] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.4316702819956616,"[41] Jinning Li, Chen Tang, Masayoshi Tomizuka, and Wei Zhan. Dealing with the unknown:
Pessimistic offline reinforcement learning. In Conference on Robot Learning, pages 1455‚Äì1464.
PMLR, 2022."
REFERENCES,0.43383947939262474,"[42] Qinjie Lin, Han Liu, and Biswa Sengupta. Switch trajectory transformer with distributional
value approximation for multi-task reinforcement learning. arXiv preprint arXiv:2203.07413,
2022."
REFERENCES,0.4360086767895879,"[43] Jinxin Liu, Hao Shen, Donglin Wang, Yachen Kang, and Qiangxing Tian. Unsupervised
domain adaptation with dynamics-aware rewards in reinforcement learning. Advances in Neural
Information Processing Systems, 34:28784‚Äì28797, 2021."
REFERENCES,0.43817787418655096,"[44] Jinxin Liu, Donglin Wang, Qiangxing Tian, and Zhengyu Chen. Learn goal-conditioned
policy with intrinsic motivation for deep reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI), volume 36, pages 7558‚Äì7566, 2022."
REFERENCES,0.4403470715835141,"[45] Jinxin Liu, Hongyin Zhang, and Donglin Wang. Dara: Dynamics-aware reward augmentation
in offline reinforcement learning. arXiv preprint arXiv:2203.06662, 2022."
REFERENCES,0.44251626898047725,"[46] Jinxin Liu, Ziqi Zhang, Zhenyu Wei, Zifeng Zhuang, Yachen Kang, Sibo Gai, and Donglin
Wang. Beyond ood state actions: Supported cross-domain offline reinforcement learning. arXiv
preprint arXiv:2306.12755, 2023."
REFERENCES,0.44468546637744033,"[47] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient
with state distribution correction. arXiv preprint arXiv:1904.08473, 2019."
REFERENCES,0.44685466377440347,"[48] Xiaoteng Ma, Yiqin Yang, Hao Hu, Qihan Liu, Jun Yang, Chongjie Zhang, Qianchuan Zhao, and
Bin Liang. Offline reinforcement learning with value-based episodic memory. arXiv preprint
arXiv:2110.09796, 2021."
REFERENCES,0.4490238611713666,"[49] Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani.
Conservative offline distributional
reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.4511930585683297,"[50] Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu.
Deployment-efficient reinforcement learning via model-based offline optimization. arXiv
preprint arXiv:2006.03647, 2020."
REFERENCES,0.45336225596529284,"[51] Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Al-
gaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019."
REFERENCES,0.455531453362256,"[52] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online rein-
forcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020."
REFERENCES,0.45770065075921906,"[53] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020."
REFERENCES,0.4598698481561822,"[54] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019."
REFERENCES,0.46203904555314534,"[55] Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models:
Model-free deep rl for model-based control. arXiv preprint arXiv:1802.09081, 2018."
REFERENCES,0.4642082429501085,"[56] Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, L√©onard Hussenot, Olivier Bachem, Olivier
Pietquin, and Matthieu Geist. Offline reinforcement learning as anti-exploration. arXiv preprint
arXiv:2106.06431, 2021."
REFERENCES,0.46637744034707157,"[57] Marc Rigter, Bruno Lacerda, and Nick Hawes. Rambo-rl: Robust adversarial model-based
offline reinforcement learning. arXiv preprint arXiv:2204.12581, 2022."
REFERENCES,0.4685466377440347,"[58] Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael
Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing
what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint
arXiv:2002.08396, 2020."
REFERENCES,0.47071583514099785,"[59] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning,
pages 387‚Äì395. PMLR, 2014."
REFERENCES,0.47288503253796094,"[60] Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja¬¥skowski, and J√ºrgen
Schmidhuber. Training agents using upside-down reinforcement learning. arXiv preprint
arXiv:1912.02877, 2019."
REFERENCES,0.4750542299349241,"[61] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018."
REFERENCES,0.4772234273318872,"[62] Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative objective
models for effective offline model-based optimization. In International Conference on Machine
Learning, pages 10358‚Äì10368. PMLR, 2021."
REFERENCES,0.4793926247288503,"[63] Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-bench: Bench-
marks for data-driven offline model-based optimization. arXiv preprint arXiv:2202.08450,
2022."
REFERENCES,0.48156182212581344,"[64] Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R√©mi Munos, and Matthieu
Geist. Leverage the average: an analysis of KL regularization in reinforcement learning. In
Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,
2020."
REFERENCES,0.4837310195227766,"[65] Michael Wan, Jian Peng, and Tanmay Gangwani. Hindsight foresight relabeling for meta-
reinforcement learning. arXiv preprint arXiv:2109.09031, 2021."
REFERENCES,0.48590021691973967,"[66] Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020."
REFERENCES,0.4880694143167028,"[67] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement
learning. arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.49023861171366595,"[68] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov,
and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. arXiv
preprint arXiv:2105.08140, 2021."
REFERENCES,0.4924078091106291,"[69] Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie
Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. arXiv
preprint arXiv:2202.04478, 2022."
REFERENCES,0.4945770065075922,"[70] Sihyun Yu, Sungsoo Ahn, Le Song, and Jinwoo Shin. Roma: Robust model adaptation for
offline model-based optimization. Advances in Neural Information Processing Systems, 34,
2021."
REFERENCES,0.4967462039045553,"[71] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural
Information Processing Systems, 33:14129‚Äì14142, 2020."
REFERENCES,0.49891540130151846,"[72] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea
Finn. Combo: Conservative offline model-based policy optimization. Advances in neural
information processing systems, 34:28954‚Äì28967, 2021."
REFERENCES,0.5010845986984815,"[73] Chi Zhang, Sanmukh Kuppannagari, and Prasanna Viktor. Brac+: Improved behavior regular-
ized actor critic for offline reinforcement learning. In Asian Conference on Machine Learning,
pages 204‚Äì219. PMLR, 2021."
REFERENCES,0.5032537960954447,"[74] Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline
reinforcement learning. arXiv preprint arXiv:2011.07213, 2020."
REFERENCES,0.5054229934924078,"[75] Zifeng Zhuang, Kun Lei, Jinxin Liu, Donglin Wang, and Yilang Guo. Behavior proximal policy
optimization. arXiv preprint arXiv:2302.11312, 2023."
REFERENCES,0.5075921908893709,Appendix
REFERENCES,0.5097613882863341,"A
More Experiments"
REFERENCES,0.5119305856832972,"A.1
Single Behavior Policy v.s. Multiple Behavior Policies"
REFERENCES,0.5140997830802603,"0
50
100
Training steps (k) 0.1 0.2 0.3"
REFERENCES,0.5162689804772235,Training loss
REFERENCES,0.5184381778741866,antmaze-medium-play
REFERENCES,0.5206073752711496,"0
50
100
Training steps (k) 0.1 0.2 0.3"
REFERENCES,0.5227765726681128,Training loss
REFERENCES,0.5249457700650759,antmaze-medium-diverse
REFERENCES,0.527114967462039,"0
50
100
Training steps (k) 0.1 0.2 0.3"
REFERENCES,0.5292841648590022,Training loss
REFERENCES,0.5314533622559653,antmaze-large-play
REFERENCES,0.5336225596529284,"0
50
100
Training steps (k) 0.1 0.2 0.3"
REFERENCES,0.5357917570498916,Training loss
REFERENCES,0.5379609544468547,antmaze-large-diverse
REFERENCES,0.5401301518438177,"1
5
10
50
100
200
500
1000
Figure 6: Learning curves of behavior cloning on AntMaze suites (*-v2) in D4RL, where the x-axis
denotes the training steps, and the y-axis denotes the training loss. The number N in the legend
denotes the number of sub-tasks. If N = 1, we learn a single behavior policy for the whole of-
fline dataset."
REFERENCES,0.5422993492407809,"In Figure 6, we provide empirical evidence that learning a single behavior policy (using BC) is not
sufficient to characterize the whole offline dataset, and multiple behavior policies (conducting task
decomposition) deliver better resilience to characterize the offline data than a single behavior policy."
REFERENCES,0.544468546637744,"A.2
Decomposition Rules"
REFERENCES,0.5466377440347071,"In DROP algorithm, we explicitly decompose an offline task into multiple sub-tasks, over which we
then reframe the offline policy learning problem as one of offline model-based optimization. In this
section, we discuss three different designs for the task decomposition rule."
REFERENCES,0.5488069414316703,"Random(N, M): We decomposition offline dataset D := {œÑ} into N subsets, each of which contains
at most M trajectories that are randomly sampled from the offline dataset."
REFERENCES,0.5509761388286334,"Quantization(N, M): Leveraging the returns of trajectories in offline data, we first quantize offline
trajectories into N bins, and then randomly sample at most M trajectories (as a sub-task) from each
bin. Specifically, in the i-th bin, the quantized trajectories {œÑi} satisfy Rmin + ‚àÜ‚àói < Return(œÑi) ‚â§
Rmin + ‚àÜ‚àó(i + 1), where ‚àÜ= (Rmax‚àíRmin)"
REFERENCES,0.5531453362255966,"N
, Return(œÑi) denotes the return of trajectory œÑi, and Rmax
and Rmin denote the maximum and minimum trajectory returns in the offline dataset respectively."
REFERENCES,0.5553145336225597,"Rank(N, M): We first rank the offline trajectories descendingly based on their returns, and then
sequentially sample M trajectories for each subset. (We adopt this decomposition rule in main paper.)"
REFERENCES,0.5574837310195228,"In Figure 7, we provide the comparison of the above three decomposition rules (see the selected
number of sub-tasks and the number of trajectories in each sub-task in Table 9). We can find
that across a variety of tasks, the decomposition rule has a fundamental impact on the subsequent
model-based optimization. Across different tasks and different embedding inference rules, Random
and Quantization decomposition rules tend to exhibit large performance fluctuations, which reveals
the importance of choosing a suitable task decomposition rule. In our paper, we adopt the Rank
decomposition rule, as it demonstrates a more robust performance shown in Figure 7. In Appendix A.4,
we adopt the conditional variational auto-encoder (CVAE) to conduct automatic task decomposition
(treating each trajectory in offline dataset as an individual task) and we find such implementation
(DROP+CVAE) can further improve DROP‚Äôs performance. In future work, we also encourage
better decomposition rules to decompose offline tasks so as to enable more effective model-based
optimization for offline RL tasks."
REFERENCES,0.559652928416486,"Comparison with filtered behavior cloning.
We also note that the Rank decomposition rule
leverages more high-quality (high-return) trajectories than the other two decomposition rules (Random
and Quantization). Thus, a natural question to ask is, is the performance of Rank better than that of
Random and Quantization due to the presence of more high-quality trajectories in the decomposed
sub-tasks? That is, whether DROP (using the Rank decomposition rule) only conducts behavioral
cloning over those high-quality trajectories, thus leading to better performance."
REFERENCES,0.561822125813449,"Rand
Quan
Rank
2.2 2.3 2.4"
REFERENCES,0.5639913232104121,Normalized return
REFERENCES,0.5661605206073753,halfcheetah-random
REFERENCES,0.5683297180043384,"Rand
Quan
Rank 35 36 37"
REFERENCES,0.5704989154013015,halfcheetah-medium
REFERENCES,0.5726681127982647,"Rand
Quan
Rank 50 100"
REFERENCES,0.5748373101952278,halfcheetah-medium-expert
REFERENCES,0.5770065075921909,"Rand
Quan
Rank 35 40"
REFERENCES,0.579175704989154,halfcheetah-medium-replay
REFERENCES,0.5813449023861171,"Rand
Quan
Rank 9 10 11"
REFERENCES,0.5835140997830802,Normalized return
REFERENCES,0.5856832971800434,hopper-random
REFERENCES,0.5878524945770065,"Rand
Quan
Rank
0 50"
REFERENCES,0.5900216919739696,hopper-medium
REFERENCES,0.5921908893709328,"Rand
Quan
Rank 50 100"
REFERENCES,0.5943600867678959,hopper-medium-expert
REFERENCES,0.596529284164859,"Rand
Quan
Rank 20 40"
REFERENCES,0.5986984815618221,hopper-medium-replay
REFERENCES,0.6008676789587852,"Rand
Quan
Rank
0.0 2.5 5.0"
REFERENCES,0.6030368763557483,Normalized return
REFERENCES,0.6052060737527115,walker2d-random
REFERENCES,0.6073752711496746,"Rand
Quan
Rank
0 25 50"
REFERENCES,0.6095444685466378,walker2d-medium
REFERENCES,0.6117136659436009,"Rand
Quan
Rank
0 50 100"
REFERENCES,0.613882863340564,walker2d-medium-expert
REFERENCES,0.6160520607375272,"Rand
Quan
Rank 10 20"
REFERENCES,0.6182212581344902,walker2d-medium-replay
REFERENCES,0.6203904555314533,"Rand
Quan
Rank 25 50 75"
REFERENCES,0.6225596529284165,Normalized return
REFERENCES,0.6247288503253796,antmaze-umaze
REFERENCES,0.6268980477223427,"Rand
Quan
Rank 25 50 75"
REFERENCES,0.6290672451193059,antmaze-umaze-diverse
REFERENCES,0.631236442516269,"Rand
Quan
Rank
0 25 50"
REFERENCES,0.6334056399132321,antmaze-medium-play
REFERENCES,0.6355748373101953,"Rand
Quan
Rank
0 20 40"
REFERENCES,0.6377440347071583,antmaze-medium-diverse
REFERENCES,0.6399132321041214,"Rand
Quan
Rank
0 25 50"
REFERENCES,0.6420824295010846,Normalized return
REFERENCES,0.6442516268980477,antmaze-large-play
REFERENCES,0.6464208242950108,"Rand
Quan
Rank
0 20"
REFERENCES,0.648590021691974,antmaze-large-diverse
REFERENCES,0.6507592190889371,"Rand DROP-Grad
Rand DROP-Best-Ada
Rand DROP-Grad-Ada
Quan DROP-Grad
Quan DROP-Best-Ada
Quan DROP-Grad-Ada"
REFERENCES,0.6529284164859002,"Rank DROP-Grad
Rank DROP-Best-Ada
Rank DROP-Grad-Ada"
REFERENCES,0.6550976138828634,"Figure 7: Comparison of three different decomposition rules on D4RL MuJoCo-Gym suite (*-v0)
and AntMaze suite (*-v2), where ""Rand"", ""Quan"" and ""Rank"" denote the Random, Quantization, and
Rank decomposition rules respectively. We can find across 18 tasks (AntMaze and MuJoCo-Gym
suites) and 3 embedding inference methods (DROP-Grad, DROP-Best-Ada, and DROP-Grad-Ada),
Rank is more stable and yields better performance compared with the other two decomposition rules."
REFERENCES,0.6572668112798264,"0
50
100
Training steps 2000 3000 4000 5000"
REFERENCES,0.6594360086767896,Episode return
REFERENCES,0.6616052060737527,halfcheetah-medium-replay
REFERENCES,0.6637744034707158,"seed-1
seed-2"
REFERENCES,0.665943600867679,"0
50
100
Training steps 1000 2000 3000"
REFERENCES,0.6681127982646421,Episode return
REFERENCES,0.6702819956616052,hopper-medium-replay
REFERENCES,0.6724511930585684,"0
50
100
Training steps 0 1000 2000 3000"
REFERENCES,0.6746203904555315,Episode return
REFERENCES,0.6767895878524945,walker2d-medium-replay
REFERENCES,0.6789587852494577,"0
50
100
Training steps 0.00 0.25 0.50 0.75"
REFERENCES,0.6811279826464208,Episode return
REFERENCES,0.6832971800433839,antmaze-large-play
REFERENCES,0.6854663774403471,"Figure 8: Learning curves of DROP, where the x-axis denotes the training steps (k), the y-axis denotes
the evaluation return (using DROP-Best embedding inference rule). We only show two seeds for legi-
bility."
REFERENCES,0.6876355748373102,"To answer the above question, we compare DROP (using the Rank decomposition rule) with filtered
behavior cloning (F-BC), where the latter (F-BC) performs behavior cloning after filtering for
trajectories with high returns. We provide the comparison results in Table 5. We can find that in
AntMaze tasks, the overall performance of DROP is higher than that of F-BC. For the MuJoCo-
Gym suite, DROP-based methods outperform F-BC on these offline tasks that contain plenty of
sub-optimal trajectories, including the random, medium, and medium-replay domains. This result
indicates that DROP can leverage embedding inference (extrapolation) to find a better policy beyond
all the behavior policies in sub-tasks, which is more effective than simply performing imitation
learning on a subset of the dataset."
REFERENCES,0.6898047722342733,"A.3
Online Fine-tuning"
REFERENCES,0.6919739696312365,"Online fine-tuning (checkpoint-level).
In Figure 8, we show the learning curves of DROP-Best on
four DR4L tasks. We can find that DROP exhibits a high variance (in performance) across training"
REFERENCES,0.6941431670281996,"Table 5: Comparison between our DROP (using the Rank decomposition rule) and filtered behavior
cloning (F-BC) on D4RL AntMaze and MuJoCo suites (*-v2). We take the baseline results of BC
and F-BC from Emmons et al. [14], where F-BC is trained over the top 10% trajectories, ordered by
the returns. Our DROP results are computed over 5 seeds and 10 episodes for each seed."
REFERENCES,0.6963123644251626,"Tasks
BC
F-BC
DROP-Grad
DROP-Best-Ada
DROP-Grad-Ada"
REFERENCES,0.6984815618221258,"antmaze-umaze
54.6
60
72 ¬± 17.2
78 ¬± 11.7
80 ¬± 12.6
antmaze-umaze-diverse
45.6
46.5
48 ¬± 22.3
62 ¬± 16
66 ¬± 12
antmaze-medium-play
0
42.1
24 ¬± 10.2
34 ¬± 12
30 ¬± 21
antmaze-medium-diverse
0
37.2
20 ¬± 19
24 ¬± 12
30 ¬± 16.7
antmaze-large-play-
0
28
24 ¬± 8
36 ¬± 17.4
42 ¬± 17.2
antmaze-large-diverse
0
34.3
14 ¬± 8
20 ¬± 14.1
26 ¬± 13.6"
REFERENCES,0.7006507592190889,"halfcheetah
random
2.3
2
2.3 ¬± 0
2.3 ¬± 0
2.3 ¬± 0
hopper
random
4.8
4.1
5.1 ¬± 0.8
5.4 ¬± 0.7
5.5 ¬± 0.6
walker2d
random
1.7
1.7
2.8 ¬± 1.7
3 ¬± 1.6
3 ¬± 1.8
halfcheetah
medium
42.6
42.5
42.4 ¬± 0.7
42.9 ¬± 0.4
43.1 ¬± 0.4
hopper
medium
52.9
56.9
57.5 ¬± 6.4
60.3 ¬± 6.1
59.5 ¬± 5.1
walker2d
medium
75.3
75
76.5 ¬± 2.4
75.8 ¬± 3
79.1 ¬± 1.4
halfcheetah
medium-replay
36.6
40.6
39.5 ¬± 1
40.4 ¬± 0.8
40.3 ¬± 1.2
hopper
medium-replay
18.1
75.9
48 ¬± 17.7
83.4 ¬± 6.5
87.4 ¬± 2.1
walker2d
medium-replay
26
62.5
37.4 ¬± 13.5
60.9 ¬± 7.4
61.9 ¬± 2.3"
REFERENCES,0.702819956616052,"steps9, which means the performance of the agent may be dependent on the specific stopping point
chosen for evaluation (such instability also exists in prior offline RL methods, Fujimoto and Gu [19])."
REFERENCES,0.7049891540130152,"To choose a suitable stopping checkpoint over which we perform the DROP inference (DROP-Grad,
DROP-Best-Ada and DROP-Grad-Ada), we propose to conduct checkpoint-level online fine-tuning
(see Algorithm 3 in Section B for more details): we evaluate each of the latest T checkpoint models
and choose the best one that leads to the highest episode return."
REFERENCES,0.7071583514099783,"In Figure 9, we show the total normalized returns across all the tasks in each suite (including Maze2d,
AntMaze, and MuJoCo-Gym). We can find that in most tasks, fine-tuning (FT) can guarantee
performance improvement. However, we also find such fine-tuning causes negative impacts on
performance in AntMaze(*-v0) suite. The main reason is that, in this checkpoint-level fine-tuning,
we choose the ""suitable"" checkpoint model using the DROP-Best embedding inference rule, while
we adopt the other three embedding inference rules (DROP-Grad, DROP-Best-Ada and DROP-Grad-
Ada) at the test time. Such a finding also implies that the success of DROP‚Äôs test-time adaptation is
not entirely dependent on the best embedding across sub-tasks 10 (i.e., the best embedding z‚àó
0(s0) in
DROP-Best), but requires switching between some ""suboptimal"" embeddings (using DROP-Best-Ada)
or extrapolating new embeddings (using DROP-Grad-Ada)."
REFERENCES,0.7093275488069414,"Online fine-tuning (embedding-level).
Beyond the above checkpoint-level fine-tuning procedure,
we can also conduct embedding-level online fine-tuning: we aim to choose a suitable gradient update
step for the gradient-based embedding inference rules (including DROP-Grad and DROP-Grad-Ada).
Similar to the checkpoint-level fine-tuning, we first conduct the test-time adaptation procedure
(DROP-Grad and DROP-Grad-Ada) over a set of gradient update steps, and then choose the best step
that leads to the highest episode return (see Algorithm 4 in Section B for more details)."
REFERENCES,0.7114967462039046,"9In view of such instability, we evaluate our methods over multiple checkpoints for each seed, instead
of choosing the final checkpoint models during the training loop (see the detailed evaluation protocol in
Appendix B).
10Conversely, if the performance of DROP depends on the best embedding across sub-tasks (i.e., z‚àó
0(s0) in
DROP-Best), then the checkpoint model we choose by fine-tuning with DROP-Best should enable a consistent
performance improvement for rules that perform embedding inference with DROP-Best-Ada and DROP-Grad-
Ada. However, we find a performance drop in AntMaze(*-v0) suite, which means there is no explicit depen-
dency between the best embedding z‚àó
0(s0) and the inferred embedding using the adaptive inference rules."
REFERENCES,0.7136659436008677,DROP-Grad
REFERENCES,0.7158351409978309,DROP-Grad FT
REFERENCES,0.7180043383947939,DROP-Best-Ada
REFERENCES,0.720173535791757,DROP-Best-Ada FT
REFERENCES,0.7223427331887202,DROP-Grad-Ada
REFERENCES,0.7245119305856833,DROP-Grad-Ada FT 0 50
REFERENCES,0.7266811279826464,Total normalized return
REFERENCES,0.7288503253796096,Maze2d
REFERENCES,0.7310195227765727,DROP-Grad
REFERENCES,0.7331887201735358,DROP-Grad FT
REFERENCES,0.735357917570499,DROP-Best-Ada
REFERENCES,0.737527114967462,DROP-Best-Ada FT
REFERENCES,0.7396963123644251,DROP-Grad-Ada
REFERENCES,0.7418655097613883,DROP-Grad-Ada FT 0 100 200
REFERENCES,0.7440347071583514,AntMaze(-v0)
REFERENCES,0.7462039045553145,DROP-Grad
REFERENCES,0.7483731019522777,DROP-Grad FT
REFERENCES,0.7505422993492408,DROP-Best-Ada
REFERENCES,0.7527114967462039,DROP-Best-Ada FT
REFERENCES,0.754880694143167,DROP-Grad-Ada
REFERENCES,0.7570498915401301,DROP-Grad-Ada FT 0 250 500
REFERENCES,0.7592190889370932,MuJoCo-Gym(-v0)
REFERENCES,0.7613882863340564,DROP-Grad
REFERENCES,0.7635574837310195,DROP-Grad FT
REFERENCES,0.7657266811279827,DROP-Best-Ada
REFERENCES,0.7678958785249458,DROP-Best-Ada FT
REFERENCES,0.7700650759219089,DROP-Grad-Ada
REFERENCES,0.7722342733188721,DROP-Grad-Ada FT 0 100 200
REFERENCES,0.7744034707158352,AntMaze(-v2)
REFERENCES,0.7765726681127982,DROP-Grad
REFERENCES,0.7787418655097614,DROP-Grad FT
REFERENCES,0.7809110629067245,DROP-Best-Ada
REFERENCES,0.7830802603036876,DROP-Best-Ada FT
REFERENCES,0.7852494577006508,DROP-Grad-Ada
REFERENCES,0.7874186550976139,DROP-Grad-Ada FT 0 250 500
REFERENCES,0.789587852494577,MuJoCo-Gym(-v2)
REFERENCES,0.7917570498915402,"Figure 9: Total normalized returns across all the tasks in Maze2d, AntMaze, and MuJoCo-Gym suites."
REFERENCES,0.7939262472885033,"Table 6: Online fine-tuning results (initial performance ‚Üíperformance after online fine-tuning).
The baseline results of AWAC, CQL, and IQL are taken from Kostrikov et al. [31], where they
run 1M online steps to fine-tune the learned policy. For our DROP method (DROP-Grad and
DROP-Grad-Ada), we run 0.3M (= 6checkpoint √ó 50Kmax √ó 1000steps per episode) online steps to fine-tune
(embedding-level) the policy, i.e., aiming to find the optimal gradient ascent step that is used to
infer the contextual embedding z‚àó(s0) or z‚àó(st) for œÄ‚àó(at|st) := Œ≤(at|st, ¬∑) (see Algorithm 4 for
the details). Moreover, for medium-* and large-* tasks, we conduct additional parametric-level
fine-tuning, with 0.7M online steps to update the policy‚Äôs parameters."
REFERENCES,0.7960954446854663,"Task (*-v0)
CQL
IQL
DROP-Grad
DROP-Grad-Ada"
REFERENCES,0.7982646420824295,"umaze
70.1 ‚Üí99.4
86.7 ‚Üí96
70 ‚Üí96 ¬± 1.2
76 ‚Üí98 ¬± 0
umaze-diverse
31.1 ‚Üí99.4
75
‚Üí84
54 ‚Üí88 ¬± 8
66 ‚Üí94 ¬± 4.9
medium-play
0 23 ‚Üí0
72
‚Üí95
20 ‚Üí56 ¬± 8.9
30 ‚Üí50 ¬± 6.3 ‚Üí94 ¬± 2.9
medium-diverse
23
‚Üí32.3
68.3 ‚Üí92
12 ‚Üí44 ¬± 4.9
22 ‚Üí38 ¬± 4.9 ‚Üí96 ¬± 0.8
large-play
1
‚Üí0
25.5 ‚Üí46
16 ‚Üí38 ¬± 8.9
16 ‚Üí40 ¬± 6.3 ‚Üí53 ¬± 1.3
large-diverse
1
‚Üí0
42.6 ‚Üí60.7
20 ‚Üí40 ¬± 13.6
22 ‚Üí46 ¬± 10.2 ‚Üí58 ¬± 4.5
‚Üí
|{z}
1M
‚Üí
|{z}
1M
‚Üí
|{z}
0.3M
‚Üí
|{z}
0.3M
‚Üí
|{z}
0.7M"
REFERENCES,0.8004338394793926,"In Table 6, we compare our DROP (DROP-Grad and DROP-Grad-Ada) to three offline RL methods
(AWAC [53], CQL [35] and IQL [31]), reporting the initial performance and the performance after
online fine-tuning. We can find that the embedding-level fine-tuning (0.3M) enables a significant
improvement in performance. The fine-tuned DROP-Grad-Ada (0.3M) outperforms the AWAC and
CQL counterparts in most tasks, even though we take fewer rollout steps to conduct the online
fine-tuning (baselines take 1M online rollout steps, while DROP-based fine-tuning takes 0.3M steps).
However, there is still a big gap between the fine-tuned IQL and the embedding-level fine-tuned
DROP (0.3M). Considering that there remain 0.7M online steps in the comparison, we further conduct
""parametric-level"" fine-tuning (updating the parameters of the policy network) for our DROP-Grad-
Ada on medium-* and large-* tasks, we can find which achieves competitive fine-tuning performance
even compared with IQL."
REFERENCES,0.8026030368763557,"A.4
DROP + CVAE Implementation"
REFERENCES,0.8047722342733189,"CVAE-based embedding learning. Similar to LAPO [11] and PLAS [74], we adopt the conditional
variational auto-encoder (CVAE) to model offline data. Specifically, we learn the contextual policy
and behavior embedding:"
REFERENCES,0.806941431670282,"Œ≤(a|s, z), œï(z|s) ‚Üêarg max
Œ≤,œï E(s,a)‚àºDE(z)‚àºœï(z|s)

log Œ≤(a|s, z)

‚àíKL(œï(z|s)‚à•p(z)).
(10)"
REFERENCES,0.8091106290672451,"Then, we learn the score model f with the TD-error and the conservative regularization:"
REFERENCES,0.8112798264642083,"f ‚Üêarg min
f
E(s,a,s‚Ä≤,a‚Ä≤)‚àºD
h 
R(s, a) + Œ≥ ¬Øf(s‚Ä≤, a‚Ä≤, œï(z|s)) ‚àíf(s, a, œï(z|s))
2i
,
(11)"
REFERENCES,0.8134490238611713,"s.t. Es‚àºD,z‚àº¬µ(z),a‚àºŒ≤(a|s,z) [f(s, a, z)] ‚àíEs‚àºD,z‚àºœï(z|s),a‚àºŒ≤(a|s,z) [f(s, a, z)] ‚â§Œ∑,"
REFERENCES,0.8156182212581344,where ¬Øf denotes a target network and ¬µ(z) denotes the uniform distribution over the Z-space.
REFERENCES,0.8177874186550976,"In testing, we also dynamically adapt the outer-level optimization, setting policy inference with
œÄ‚àó(a|s) = Œ≤(a|s, z‚àó(s)), where z‚àó(s) = arg maxz f
 
s, Œ≤(a|s, z), z

."
REFERENCES,0.8199566160520607,"A.5
Ablation Study"
REFERENCES,0.8221258134490239,"0
30
60
90
120 150 25 50 75 100"
REFERENCES,0.824295010845987,"Grad
Grad w/o Reg"
REFERENCES,0.8264642082429501,"0
30
60
90
120 150 25 50 75 100"
REFERENCES,0.8286334056399133,"Grad-Ada
Grad-Ada w/o Reg"
REFERENCES,0.8308026030368764,"Figure 10: Ablation on the conservative regular-
ization. The y-axis represents the normalize re-
turn, and the x-axis represents the number of
gradient-ascent steps used for embedding infer-
ence at deployment. We plot each random seed as
a transparent line; the solid line corresponds to the
average across 5 seeds."
REFERENCES,0.8329718004338394,"Note that our embedding inference depends on
the learned score model f. Without proper regu-
larization, such inference will lead to out-of-
distribution embeddings that are erroneously
high-scored (Q2). Here we conduct an ablation
study to examine the impact of the conservative
regularization used for learning the score model."
REFERENCES,0.8351409978308026,"In Figure 10, we compare DROP-Grad and
DROP-Grad-Ada to their naive implementa-
tion (w/o Reg) that ablates the regularization on
halfcheetah-medium-expert. We can find that re-
moving the conservative regularization leads to
unstable performance when changing the update
steps of gradient-based optimization. However, we empirically find that in some tasks such a naive
implementation (w/o Reg) does not necessarily bring unstable inference (Appendix C). Although
improper gradient update step leads to faraway embeddings, to some extent, embedding-conditioned
behavior policy can correct such deviation."
REFERENCES,0.8373101952277657,"B
Implementation Details"
REFERENCES,0.8394793926247288,"For the practical implementation of DROP, we parameterize the task embedding function œï(z|n),
the contextual behavior policy Œ≤(a|s, z), and the score model f(s, a, z) with neural networks. For
Equation 8 in the main paper, we construct a Lagrangian and solve the optimization through primal-
dual gradient descent. For the choice of ¬µ(z), we simply set ¬µ(z) to be the uniform distribution
over the Z-space and empirically find that such uniform sampling can effectively avoid the out-of-
distribution extrapolation at inference."
REFERENCES,0.841648590021692,"Lagrangian relaxation.
To optimize the constrained objective in Equation 8 in the main paper, we
construct a Lagrangian and solve the optimization through primal-dual gradient descent,"
REFERENCES,0.8438177874186551,"min
f
max
Œª>0
EDn‚àºD[N]E(s,a,s‚Ä≤,a‚Ä≤)‚àºDn
h 
R(s, a) + Œ≥ ¬Øf(s‚Ä≤, a‚Ä≤, œï(z|n)) ‚àíf(s, a, œï(z|n))
2i
+"
REFERENCES,0.8459869848156182,"Œª
 
En,¬µ(z)Es‚àºDn,a‚àºŒ≤(a|s,z) [f(s, a, z)] ‚àíEn,œï(z|n)Es‚àºDn,a‚àºŒ≤(a|s,z) [f(s, a, z)] ‚àíŒ∑

."
REFERENCES,0.8481561822125814,"This unconstrained objective implies that if the expected difference in scores of out-of-distribution
embeddings and in-distribution embeddings is less than a threshold Œ∑, Œª is going to be adjusted to 0, on
the contrary, Œª is likely to take a larger value, used to punish the over-estimated value function. This
objective encourages that out-of-distribution embeddings score lower than in-distribution embeddings,
thus performing embedding inference will not lead to these out-of-distribution embeddings that are
falsely and over-optimistically scored by the learned score model."
REFERENCES,0.8503253796095445,"In our experiments, we tried five different values for the Lagrange threshold Œ∑ (1.0, 2.0, 3.0, 4.0, and
5.0). We did not observe a significant difference in performance across these values. Therefore, we
simply set Œ∑ = 2.0."
REFERENCES,0.8524945770065075,"Hyper-parameters.
In Table 7, we provide the hyper-parameters of the task embedding œï(z|s),
the contextual behavior policy Œ≤(a|s, z), and the score function f(s, a, z). For the gradient ascent
update steps (used for embedding inference), we set K = 100 for all the embedding inference rules
in experiments."
REFERENCES,0.8546637744034707,"In Table 9, we provide the number of sub-tasks, the number of trajectories in each sub-task, and the
dimension of the embedding for each sub-task (behavior policy). The selection of hyperparameter N
is based on two evaluation metrics: (1) the fitting loss of the decomposed behavioral policies to the
offline data, and (2) the testing performance of DROP. Specifically,"
REFERENCES,0.8568329718004338,"Table 7: Hyper-parameters of the task embedding function œï(z|s), the contextual behavior policy
Œ≤(a|s, z), and the score function f(s, a, z). The task embedding function œï(z|s): z ‚ÜêEnc_0(n).
The contextual behavior policy Œ≤(a|s, z): a ‚ÜêEnc_2(z, Enc_1(s)). The score function f(s, a, z):
f ‚ÜêEnc_4(z, Enc_3(s, a))."
REFERENCES,0.8590021691973969,"Enc_0
Enc_1
Enc_2
Enc_3
Enc_4"
REFERENCES,0.8611713665943601,"Optimizer
Adam
Adam
Adam
Adam
Adam
Hidden layer
2
2
3
2
3
Hidden dim
512
512
512
512
512
Activation function
ReLU
ReLU
ReLU
ReLU
ReLU
Learning rate
1.00E-03
1.00E-03
1.00E-03
1.00E-03
1.00E-03
Mini-batch size
1024
1024
1024
1024
1024"
REFERENCES,0.8633405639913232,"‚Ä¢ (Step1) Over a hyperparameter (the number of sub-tasks) set, we conduct the hyperparam-
eter search using the fitting loss of behavior policies, then we choose/filter the four best
hyperparameters;
‚Ä¢ (Step2) We follow the normal practice of hyperparameter selection and tune the four hyper-
meters selected in Step1 by interacting with the simulator to estimate the performance of
DROP under each hyperparameter setting."
REFERENCES,0.8655097613882863,Table 8: Hyperparameter (the number of sub-tasks) set.
REFERENCES,0.8676789587852495,"tasks
the number of sub-tasks"
REFERENCES,0.8698481561822126,"Antmaze
500 (v0), 150 (v2)
Gym-mujoco
10, 20, 50, 100, 200, 500, 800, 1000
Adroit
10, 20, 50, 100, 200, 500, 800, 1000"
REFERENCES,0.8720173535791758,"We provide the hyperparameter sets
in Table 8. In Step2, we tune the (fil-
tered) hyperparameters using 1 seed,
then evaluate the best hyperparameter
by training on an additional 4 seeds
and finally report the results on the 5
total seeds (see next ""evaluation pro-
tocol""). In Antmaze domain, a single
fixed N works well for many tasks;
while in Gym-mujoco and Adroit domains, we did not find a fixed N that provides good results for all
tasks in the corresponding domain in D4RL, thus we use the above hyperparameter selection rules
(Step1 and Step2) to choose the number."
REFERENCES,0.8741865509761388,"Baseline details.
For the comparison of our method to prior iterative offline RL methods, we
consider the v0 versions of the datasets in D4RL11. We take the baseline results of BEAR, BCQ,
CQL, and BRAC-p from the D4RL paper [18], and take the results of TD3+BC from their origin
paper [19]. For the comparison of our method to prior non-iterative offline RL method, we use
the v2 versions of the dataset in D4RL. All the baseline results of behavior cloning (BC), Decision
Transform (DT), RvS-R, and Onestep are taken from Emmons et al. [14]. In our implementation of
COMs, we take the parameters (neural network weights) of behavior policies as the design input for
the score model; and during testing, we conduct parameters inference (outer-level optimization) with
200 steps gradient ascent over the learned score function, then the rollout policy is initialized with
the inferred parameters. For the specific architecture, we instantiate the policy network with dim(S)
input units, two layers with 64 hidden units, and a final output layer with dim(A)."
REFERENCES,0.8763557483731019,"Evaluation protocol.
We evaluate our results over 5 seeds. For each seed, instead of taking the
final checkpoint model produced by a training loop, we take the last T (T = 6 in our experiments)
checkpoint models, and evaluate them over 10 episodes for each checkpoint. That is to say, we report
the average of the evaluation scores over 5seed √ó 6checkpoint √ó 10episode rollouts."
REFERENCES,0.8785249457700651,"Online fine-tuning (checkpoint-level): Instead of re-training the learned (final) policy with online
rollouts, we fine-tune our policy with enumerated trial-and-error over the last T checkpoint models
(Algorithm 3). Specifically, for each seed, we run the last T checkpoint models in the environment
over one episode for each checkpoint. The checkpoint model which achieves the maximum episode
return is returned. In essence, this fine-tuning procedure imitates the online RL evaluation protocol:
if the current policy is unsatisfactory, we can use checkpoints of previous iterations of the policy."
REFERENCES,0.8806941431670282,"11We noticed that Maze2D-v0 in the D4RL dataset (https://rail.eecs.berkeley.edu/datasets/) is not available, so
we used v1 version instead in our experiment. For simplicity, we still use v0 in the paper exposition."
REFERENCES,0.8828633405639913,"Algorithm 3 DROP: Online fine-tuning (checkpoint-level)
Require: Env, last T checkpoint models: Œ≤t(a|s, z) and ft(s, a, z) (t = 1, ¬∑ ¬∑ ¬∑ , T)."
REFERENCES,0.8850325379609545,"1: RMAX = ‚àí‚àû.
2: Œ≤best ‚ÜêNone.
3: fbest ‚ÜêNone.
4: while t = 1, ¬∑ ¬∑ ¬∑ , T do
5:
s0 = Env.Reset().
6:
z‚àó
0(s0) ‚ÜêConduct embedding inference with DROP-Best.
7:
Return ‚ÜêEvaluate Œ≤t and ft on Env, setting œÄ‚àó(a|s) = Œ≤(a|s, z‚àó
0(s0)).
8:
if RMAX < Return then
9:
Update the best checkpoint models: Œ≤best ‚ÜêŒ≤t, fbest ‚Üêft.
10:
Update the optimal return: RMAX ‚ÜêReturn.
11:
end if
12: end while
Return: Œ≤best and fbest."
REFERENCES,0.8872017353579176,"Algorithm 4 DROP: Online fine-tuning (embedding-level)
Require: Env, last T checkpoint models: Œ≤t(a|s, z) and ft(s, a, z) (t = 1, ¬∑ ¬∑ ¬∑ , T)."
REFERENCES,0.8893709327548807,"1: RMAX = ‚àí‚àû.
2: Œ≤best ‚ÜêNone.
3: fbest ‚ÜêNone.
4: kbest ‚Üê0.
5: while t = 1, ¬∑ ¬∑ ¬∑ , T do
6:
while k = 1, ¬∑ ¬∑ ¬∑ , Kmax do
7:
s0 = Env.Reset().
# Conduct embedding inference with DROP-Grad or DROP-Grad-Ada
8:
Return ‚ÜêEvaluate Œ≤t and ft on Env, setting œÄ‚àó(a|s) = Œ≤(a|s, z‚àó(s0)) or Œ≤(a|s, z‚àó(s)), where we
conduct k gradient ascent steps to obtain z‚àó(s0) or z‚àó(s).
9:
if RMAX < Return then
10:
Update the best checkpoint models: Œ≤best ‚ÜêŒ≤t, fbest ‚Üêft.
11:
Update the best gradient update step: kbest ‚Üêk.
12:
Update the optimal return: RMAX ‚ÜêReturn.
13:
end if
14:
end while
15: end while
Return: Œ≤best, fbest and kbest."
REFERENCES,0.8915401301518439,"Online fine-tuning (embedding-level): The embedding-level fine-tuning aims to find a suitable gradient
ascent step that is used to conduct the embedding inference in DROP-Grad or DROP-Grad-Ada.
Thus, we enumerate a list of gradient update steps and pick the best update step (according to the
episode returns)."
REFERENCES,0.8937093275488069,"Codebase.
Our code is based on d3rlpy: https://github.com/takuseno/d3rlpy. We provide
our source code in the supplementary material."
REFERENCES,0.89587852494577,"Computational resources.
The experiments were run on a computational cluster with 22x GeForce
RTX 2080 Ti, and 4x NVIDIA Tesla V100 32GB for 20 days."
REFERENCES,0.8980477223427332,"C
Additional Results"
REFERENCES,0.9002169197396963,"Comparison with iterative offline RL baselines.
Here, we compare the performance of DROP
(Grad, Best-Ada, and Grad-Ada ) to iterative offline RL baselines (BEAR [33], BCQ [20], CQL [35],
BRAC-p [67], and TD3+BC [19]) that perform iterative bi-level offline RL paradigm with (explicit or
implicit) value/policy regularization in inner-level. In Table 10, we present the results for AntMaze,
Gym-MuJoCo, and Adroit suites in standard D4RL benchmark (*-v0), where we can find that
DROP-Grad-Ada performs comparably or surpasses prior iterative bi-level works on most tasks:
outperforming (or comparing) these policy regularized methods (BRAC-p and TD3+BC) on 25 out
of 33 tasks and outperforming (or comparing) these value regularized algorithms (BEAR, BCQ, and
CQL) on 19 out of 33 tasks."
REFERENCES,0.9023861171366594,"Table 9: The number (N) of sub-tasks, the number (M) of trajectories in each sub-task, and the
dimension (dim(z)) of the embedding for each sub-task."
REFERENCES,0.9045553145336226,"Domain
Task Name
Parameters (*-v0)
Parameters (*-v2)"
REFERENCES,0.9067245119305857,"N
M
dim(z)
N
M
dim(z)"
REFERENCES,0.9088937093275488,"Maze 2D
umaze
500
5
5
medium
150
50
5
large
100
15
5"
REFERENCES,0.911062906724512,Antmaze
REFERENCES,0.913232104121475,"umaze
500
50
5
150
50
5
umaze-diverse
500
50
5
150
50
5
Medium-play
500
50
5
150
50
5
Medium-diverse
500
50
5
150
50
5
Large-play
500
50
5
150
50
5
Large-diverse
500
50
5
150
50
5"
REFERENCES,0.9154013015184381,halfcheetah
REFERENCES,0.9175704989154013,"random
1000
1
5
1000
1
5
medium
100
2
5
100
2
5
medium-expert
1000
1
5
1000
1
5
medium-replay
50
10
5
50
10
5"
REFERENCES,0.9197396963123644,hopper
REFERENCES,0.9219088937093276,"random
100
2
5
100
2
5
medium
100
5
5
100
5
5
medium-expert
100
2
5
100
2
5
medium-replay
50
5
5
10
30
5"
REFERENCES,0.9240780911062907,walker2d
REFERENCES,0.9262472885032538,"random
500
2
5
500
2
5
medium
50
5
5
50
5
5
medium-expert
50
5
5
50
5
5
medium-replay
1000
5
5
10
50
5"
REFERENCES,0.928416485900217,"door
cloned
1000
2
5
expert
500
5
5
human
50
3
5"
REFERENCES,0.93058568329718,"hammer
cloned
1000
1
5
expert
500
5
5
human
20
3
5"
REFERENCES,0.9327548806941431,"pen
cloned
500
5
5
expert
500
5
5
human
50
5
5"
REFERENCES,0.9349240780911063,"relocate
cloned
500
5
5
expert
500
5
5
human
50
4
5"
REFERENCES,0.9370932754880694,"Ablation studies.
In Figure 11, we provide more results for the ablation of the conservative
regularization term in Equation 8 in the main paper. We can find that for the halfcheetah-medium and
hopper-medium tasks, the performance of DROP-Grad-Ada w/o Reg depends on the choice of the
gradient update steps, showing that too small or too large number of gradient update step deteriorates
the performance. Such a result is also consistent with COMs [62], which also observes the sensitivity
of naive gradient update (i.e., w/o Reg) to the number of update steps used for design input inference.
By comparison, the conservative score model learned with DROP-Grad-Ada exhibits more stable and
robust performance to the gradient update steps."
REFERENCES,0.9392624728850325,"Further, we also find that in walker2d-medium and walker2d-medium-expert tasks, the naive gradient
update (w/o Reg) does not affect performance significantly across a wide range of gradient update
steps. The main reason is that although the excessive gradient updates lead to faraway embeddings,
conditioned on the inferred embeddings, the learned contextual behavior policy can safeguard against
the embeddings distribution shift. Compared to prior model-based optimization that conducts direct
gradient optimization (inference) over the design input itself, such ""self-safeguard"" is a special merit
in the offline RL domain as long as we reframe the offline RL problem as one of model-based
optimization and conduct inference over the embedding space. Thus, we encourage the research
community to pursue further into this model-based optimization view for the offline RL problem."
REFERENCES,0.9414316702819957,"0
30
60
90
120 150
20 30 40"
REFERENCES,0.9436008676789588,Normalized returns
REFERENCES,0.9457700650759219,halfcheetah-medium
REFERENCES,0.9479392624728851,"0
30
60
90
120 150 30 40 50"
REFERENCES,0.9501084598698482,hopper-medium
REFERENCES,0.9522776572668112,"0
30
60
90
120 150
20 22 24 26"
REFERENCES,0.9544468546637744,walker2d-medium
REFERENCES,0.9566160520607375,"0
30
60
90
120 150
60 80"
REFERENCES,0.9587852494577006,walker2d-medium-expert
REFERENCES,0.9609544468546638,"DROP-Grad-Ada
DROP-Grad-Ada w/o Reg
Figure 11: The performance comparison of DROP-Grad-Ada and DROP-Grad-Ada w/o Reg, where
we ablate the conservative regularization for the w/o Reg implementation. The y-axis denotes the
normalized return, the x-axis denotes the number of gradient-ascent steps used for embedding
inference at deployment."
REFERENCES,0.9631236442516269,"Table 10: Comparison of our method to prior offline methods that perform iterative (regularized) RL
paradigm on D4RL. We take the baseline results of BEAR, BCQ, CQL, and BRAC-p from Fu et al.
[18], and the results of TD3-BC from Fujimoto and Gu [19]. For all results of our method (DROP),
we average the normalized returns across 5 seeds; for each seed, we run 10 evaluation episodes.
For proper comparison, we use ‚ñ≤and ‚ñ≤to denote DROP (*-Ada) achieves comparable or better
performance compared with value and policy regularized offline RL methods respectively."
REFERENCES,0.96529284164859,"Task Name
Value Reg.
Policy Reg.
DROP-"
REFERENCES,0.9674620390455532,"BEAR
BCQ
CQL
BRAC-p
TD3+BC
Grad
Best-Ada
Grad-Ada"
REFERENCES,0.9696312364425163,antmaze
REFERENCES,0.9718004338394793,"umaze
73.0
78.9
74.0
50.0
‚Äì
72.0‚ñ≤
78.0‚ñ≤‚ñ≤
80.0‚ñ≤‚ñ≤
umaze-diverse
61.0
55.0
84.0
40.0
‚Äì
48.0‚ñ≤
62.0‚ñ≤‚ñ≤
66.0‚ñ≤‚ñ≤
medium-play
0.0
0.0
61.2
0.0
‚Äì
24.0‚ñ≤
34.0‚ñ≤‚ñ≤
30.0‚ñ≤‚ñ≤
medium-diverse
8.0
0.0
53.7
0.0
‚Äì
20.0‚ñ≤
24.0‚ñ≤‚ñ≤
30.0‚ñ≤‚ñ≤
large-play
0.0
6.7
15.8
0.0
‚Äì
24.0‚ñ≤
36.0‚ñ≤‚ñ≤
42.0‚ñ≤‚ñ≤
large-diverse
0.0
2.2
14.9
0.0
‚Äì
14.0‚ñ≤
20.0‚ñ≤‚ñ≤
26.0‚ñ≤‚ñ≤"
REFERENCES,0.9739696312364425,halfcheetah
REFERENCES,0.9761388286334056,"random
25.1
2.2
35.4
24.1
10.2
2.3‚ñ≤
2.3‚ñ≤‚ñ≤
2.3‚ñ≤‚ñ≤
medium
41.7
40.7
44.4
43.8
42.8
42.4‚ñ≤
42.9‚ñ≤‚ñ≤
43.1‚ñ≤‚ñ≤
medium-expert
53.4
64.7
62.4
44.2
97.9
86.6‚ñ≤
88.5‚ñ≤‚ñ≤
88.9‚ñ≤‚ñ≤
medium-replay
38.6
38.2
46.2
45.4
43.3
39.5‚ñ≤
40.4‚ñ≤‚ñ≤
40.3‚ñ≤‚ñ≤"
REFERENCES,0.9783080260303688,hopper
REFERENCES,0.9804772234273319,"random
11.4
10.6
10.8
11.0
11.0
5.1‚ñ≤
5.4‚ñ≤‚ñ≤
5.5‚ñ≤‚ñ≤
medium
52.1
54.5
58.0
32.7
99.5
57.5‚ñ≤
60.3‚ñ≤‚ñ≤
59.5‚ñ≤‚ñ≤
medium-expert
96.3
110.9
98.7
1.9
112.2
103.5‚ñ≤
102.5‚ñ≤‚ñ≤
105.9‚ñ≤‚ñ≤
medium-replay
33.7
33.1
48.6
0.6
31.4
48.0‚ñ≤
83.4‚ñ≤‚ñ≤
87.4‚ñ≤‚ñ≤"
REFERENCES,0.982646420824295,walker2d
REFERENCES,0.9848156182212582,"random
7.3
4.9
7.0
-0.2
1.4
2.8‚ñ≤
3.0‚ñ≤‚ñ≤
3.0‚ñ≤‚ñ≤
medium
59.1
53.1
79.2
77.5
79.7
76.5‚ñ≤
75.8‚ñ≤‚ñ≤
79.1‚ñ≤‚ñ≤
medium-expert
40.1
57.5
111.0
76.9
101.1
107.5‚ñ≤
106.8‚ñ≤‚ñ≤
106.9‚ñ≤‚ñ≤
medium-replay
19.2
15.0
26.7
-0.3
25.2
37.4‚ñ≤
60.9‚ñ≤‚ñ≤
61.9‚ñ≤‚ñ≤ door"
REFERENCES,0.9869848156182213,"cloned
-0.1
0.0
0.4
-0.1
‚Äì
0.5‚ñ≤
2.5‚ñ≤‚ñ≤
2.7‚ñ≤‚ñ≤
expert
103.4
99.0
101.5
-0.3
‚Äì
98.6‚ñ≤
102.2‚ñ≤‚ñ≤
102.6‚ñ≤‚ñ≤
human
-0.3
0.0
9.9
-0.3
‚Äì
3.3‚ñ≤
1.9‚ñ≤‚ñ≤
3.0‚ñ≤‚ñ≤"
REFERENCES,0.9891540130151844,hammer
REFERENCES,0.9913232104121475,"cloned
0.3
0.4
2.1
0.3
‚Äì
0.3‚ñ≤
0.3‚ñ≤‚ñ≤
0.3‚ñ≤‚ñ≤
expert
127.3
107.2
86.7
0.3
‚Äì
65.7‚ñ≤
73.3‚ñ≤‚ñ≤
77.7‚ñ≤‚ñ≤
human
0.3
0.5
4.4
0.3
‚Äì
1.1‚ñ≤
0.3‚ñ≤‚ñ≤
2.1‚ñ≤‚ñ≤ pen"
REFERENCES,0.9934924078091106,"cloned
26.5
44.0
39.2
1.6
‚Äì
76.7‚ñ≤
77.1‚ñ≤‚ñ≤
82.4‚ñ≤‚ñ≤
expert
105.9
114.9
107.0
-3.5
‚Äì
113.1‚ñ≤
118.6‚ñ≤‚ñ≤
116.7‚ñ≤‚ñ≤
human
-1.0
68.9
37.5
8.1
‚Äì
71.1‚ñ≤
85.2‚ñ≤‚ñ≤
81.5‚ñ≤‚ñ≤"
REFERENCES,0.9956616052060737,relocate
REFERENCES,0.9978308026030369,"cloned
-0.3
-0.3
-0.1
-0.3
‚Äì
0.1‚ñ≤
0.5‚ñ≤‚ñ≤
0.2‚ñ≤‚ñ≤
expert
98.6
41.6
95.0
-0.3
‚Äì
2.5‚ñ≤
6.2‚ñ≤‚ñ≤
5.4‚ñ≤‚ñ≤
human
-0.3
-0.1
0.2
-0.3
‚Äì
0.0‚ñ≤
0.0‚ñ≤‚ñ≤
0.0‚ñ≤‚ñ≤"
