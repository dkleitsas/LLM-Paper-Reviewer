Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009765625,"A common problem of classical neural network architectures is that additional
1"
ABSTRACT,0.001953125,"information or expert knowledge cannot be naturally integrated into the learning
2"
ABSTRACT,0.0029296875,"process. To overcome this limitation, we propose a two-step approach consisting
3"
ABSTRACT,0.00390625,"of (1) generating formal rules from knowledge and (2) using these rules to define
4"
ABSTRACT,0.0048828125,"rule based layers – a new type of dynamic neural network layer. The focus of this
5"
ABSTRACT,0.005859375,"work is on the second step, i.e., rule based layers that are designed to dynamically
6"
ABSTRACT,0.0068359375,"arrange learnable parameters in the weight matrices and bias vectors for each input
7"
ABSTRACT,0.0078125,"sample following a formal rule. Indeed, we prove that our approach generalizes
8"
ABSTRACT,0.0087890625,"classical feed-forward layers such as fully connected and convolutional layers by
9"
ABSTRACT,0.009765625,"choosing appropriate rules. As a concrete application we present rule based graph
10"
ABSTRACT,0.0107421875,"neural networks (RuleGNNs) that are by definition permutation equivariant and
11"
ABSTRACT,0.01171875,"able to handle graphs of arbitrary sizes. Our experiments show that RuleGNNs
12"
ABSTRACT,0.0126953125,"are comparable to state-of-the-art graph classifiers using simple rules based on
13"
ABSTRACT,0.013671875,"the Weisfeiler-Leman labeling and pattern counting. Moreover, we introduce new
14"
ABSTRACT,0.0146484375,"synthetic benchmark graph datasets to show how to integrate expert knowledge
15"
ABSTRACT,0.015625,"into RuleGNNs making them more powerful than ordinary graph neural networks.
16"
INTRODUCTION,0.0166015625,"1
Introduction
17"
INTRODUCTION,0.017578125,"Using expert knowledge to increase the efficiency, interpretability or predictive performance of
18"
INTRODUCTION,0.0185546875,"a neural network is an evolving research direction in machine learning [21, 23]. Many ordinary
19"
INTRODUCTION,0.01953125,"neural network architectures are not capable of using external and structural information such as
20"
INTRODUCTION,0.0205078125,"expert knowledge or meta-data, e.g., graph structures in a dynamic way. We would like to motivate
21"
INTRODUCTION,0.021484375,"the importance of “expert knowledge” by considering the following example. Maybe one of the
22"
INTRODUCTION,0.0224609375,"best studied examples based on knowledge integration are convolutional neural networks [12].
23"
INTRODUCTION,0.0234375,"Convolutional neural networks for images use at least two extra pieces of “expert knowledge” that is:
24"
INTRODUCTION,0.0244140625,"neighbored pixels correlate, and the structure of images is homogeneous. The consequence of this
25"
INTRODUCTION,0.025390625,"knowledge is the use of receptive fields and weight sharing. It is a common fact that the usage of
26"
INTRODUCTION,0.0263671875,"this information about images has highly improved the predictive performance over fully connected
27"
INTRODUCTION,0.02734375,"neural networks. But what if expert knowledge suggests that rectangular convolutional kernels are
28"
INTRODUCTION,0.0283203125,"not suitable to solve the task? In this case the ordinary convolutional neural network architecture
29"
INTRODUCTION,0.029296875,"is too static to adapt to the new information. Dynamic neural networks are not only applicable to
30"
INTRODUCTION,0.0302734375,"images but also to other data types such as video [25], text [10], or graphs [19]. The limitation
31"
INTRODUCTION,0.03125,"of such approaches is that expert knowledge is somehow implicit and not directly encoded in the
32"
INTRODUCTION,0.0322265625,"network structure, i.e., for each new information a new architecture has to be designed. Thus, our
33"
INTRODUCTION,0.033203125,"goal is to extract the essence of dynamic neural networks by defining a new type of neural network
34"
INTRODUCTION,0.0341796875,"layer that is on the one side able to use expert knowledge in a dynamic way and on the other side
35"
INTRODUCTION,0.03515625,"easily configurable. Our solution to this problem are rule based layers that are able to encode expert
36"
INTRODUCTION,0.0361328125,"(a) Learned weights and bias for the best model of
the DHFR dataset."
INTRODUCTION,0.037109375,"(b) Learned weights for the best model of the IMDB-
BINARY dataset."
INTRODUCTION,0.0380859375,"Figure 1: Visualization of the learnable parameters of our RuleGNN on DHFR (a) and IMDB-
BINARY (b) for three different graphs. Positive weights are denoted by red arrows and negative
weights by blue arrows. The arrow thicknesss and color corresponds to the absolute value of the
weight. The bias is denoted by the size of the node. The second image of (a) resp. (b) shows the
weights the 10 resp. 5 largest positive and negative weights."
INTRODUCTION,0.0390625,"knowledge directly in the network structure. As far as we know, this is the first work that defines a
37"
INTRODUCTION,0.0400390625,"dynamic neural network layer in this generality.
38"
INTRODUCTION,0.041015625,"Main Idea
We simplify and unify the integration of expert knowledge and additional informa-
39"
INTRODUCTION,0.0419921875,"tion into neural networks by proposing a two-step approach and show how to encode given extra
40"
INTRODUCTION,0.04296875,"information directly into the structure of a neural network in a dynamic way. In the first step the
41"
INTRODUCTION,0.0439453125,"extra information or expert knowledge is formalized using appropriate rules (e.g., certain pixels in
42"
INTRODUCTION,0.044921875,"images are important, only nodes in a graph of type A and B interact, some patterns, e.g., cycles
43"
INTRODUCTION,0.0458984375,"or cliques, in a graph are important, etc.). In the second step the rules are used to manipulate the
44"
INTRODUCTION,0.046875,"structure of the neural network. More precisely, the rules determine the positions of the weights in
45"
INTRODUCTION,0.0478515625,"the weight matrix and the bias terms. We note that the focus of this work is on the second step as we
46"
INTRODUCTION,0.048828125,"show how to use given rules to dynamically adapt the layers. In fact, we do not provide a general
47"
INTRODUCTION,0.0498046875,"instruction for deriving formal rules from given expert knowledge. In difference to ordinary network
48"
INTRODUCTION,0.05078125,"layers we consider a set W of learnable parameters instead of fixed weight matrices. The weight
49"
INTRODUCTION,0.0517578125,"matrices and bias terms are then constructed for each input sample independently using the learnable
50"
INTRODUCTION,0.052734375,"parameters from W. Indeed, each learnable parameter in W is associated with a specific relation
51"
INTRODUCTION,0.0537109375,"between an input and output feature of a layer. As an example consider Figure 1 where each input and
52"
INTRODUCTION,0.0546875,"output feature corresponds to a specific node in the graph. The input samples are (a) molecule graphs
53"
INTRODUCTION,0.0556640625,"respectively (b) snippets of social networks and the task is to predict the graph class. Each colored
54"
INTRODUCTION,0.056640625,"arrow in the figure corresponds to a learned parameter from W, i.e., a specific relation between two
55"
INTRODUCTION,0.0576171875,"atoms in the molecules or two nodes in the social network. Considering only the weights with the
56"
INTRODUCTION,0.05859375,"largest absolute values, see the second image of (a) respectively (b), our approach has learned how to
57"
INTRODUCTION,0.0595703125,"propagate information from outer atoms to the rings respectively from the nodes to the “important”
58"
INTRODUCTION,0.060546875,"nodes of the social network. This example shows several advantages of our approach: (1) rule based
59"
INTRODUCTION,0.0615234375,"layer type has a much more flexible structure than layers in classical architectures and allow to deal
60"
INTRODUCTION,0.0625,"with arbitrary input dimensions, (2) the layers are easily integrable into existing architectures, and
61"
INTRODUCTION,0.0634765625,"(3) the learned parameters, hence the model, is interpretable and can possibly be used to extract new
62"
INTRODUCTION,0.064453125,"knowledge from the data or to improve the existing rules.
63"
INTRODUCTION,0.0654296875,"Main Contributions
We define a new type of neural network layer called rule based layer. This
64"
INTRODUCTION,0.06640625,"new layer can be integrated into arbitrary architectures making them dynamic, i.e., the structure
65"
INTRODUCTION,0.0673828125,"of the network changes based on the input data and predefined rules. We prove that rule based
66"
INTRODUCTION,0.068359375,"layers generalize classical feed-forward layers such as fully connected and convolutional layers.
67"
INTRODUCTION,0.0693359375,"Additionally, we show that rule based layers can be applied to graph classification tasks, by introducing
68"
INTRODUCTION,0.0703125,"RuleGNNs, a new type of graph neural networks. In this way we are able to extend the concept of
69"
INTRODUCTION,0.0712890625,"dynamic neural networks to graph neural networks together with all the advantages of dynamic neural
70"
INTRODUCTION,0.072265625,"networks, e.g., that RuleGNNs are by definition permutation equivariant and able to handle graphs
71"
INTRODUCTION,0.0732421875,"of arbitrary sizes. Considering various real-world graph datasets, we demonstrate that RuleGNNs
72"
INTRODUCTION,0.07421875,"are competitive with state-of-the-art graph neural networks and other graph classification methods.
73"
INTRODUCTION,0.0751953125,"Using synthetic graph datasets we show that “expert knowledge” is easily integrable into our neural
74"
INTRODUCTION,0.076171875,"network and also necessary for classification1
75"
INTRODUCTION,0.0771484375,"The rest of the paper is organized as follows: We introduce the concept of rule based layers in Section 2
76"
INTRODUCTION,0.078125,"and prove in Section 3 that rule based layers generalize fully connected and convolutional layers.
77"
INTRODUCTION,0.0791015625,"In Section 4 we present RuleGNNs and apply them in Section 5 to different benchmark datasets
78"
INTRODUCTION,0.080078125,"and compare the results with state-of the art graph neural networks. Finally, we discuss limitations,
79"
INTRODUCTION,0.0810546875,"related work and conclude the paper in Section 6.
80"
RULE BASED LEARNING,0.08203125,"2
Rule Based Learning
81"
RULE BASED LEARNING,0.0830078125,"Introducing the concept of rule based learning we first present some basic definitions followed by the
82"
RULE BASED LEARNING,0.083984375,"formal definition of rule based layers.
83"
RULE BASED LEARNING,0.0849609375,"Preliminaries
For some n ∈N we denote by [n] the set {1, . . . , n}. A neural network is denoted
84"
RULE BASED LEARNING,0.0859375,"by a function f(−, Θ) : Rn −→Rm with the learnable parameters Θ. We extend this notation
85"
RULE BASED LEARNING,0.0869140625,"introducing an additional parameter R, that is the set of formal rules R = {R1, . . . , Rk}. The
86"
RULE BASED LEARNING,0.087890625,"exact definition of these rules is given in the next paragraph. Informally, a rule R is a function
87"
RULE BASED LEARNING,0.0888671875,"that determines the distribution of the weights in the weight matrix or the bias vector of a layer. A
88"
RULE BASED LEARNING,0.08984375,"rule R is called dynamic if it is a function in the input samples x ∈Rn otherwise it is called static.
89"
RULE BASED LEARNING,0.0908203125,"An example of a static rule is the one used to define convolutional layers, see Proposition 2. An
90"
RULE BASED LEARNING,0.091796875,"example of a dynamic rule can be found in Section 4. In our setting, a neural network is a function
91"
RULE BASED LEARNING,0.0927734375,"f(−, Θ, R) : R∗−→R∗that depends on a set of learnable parameters denoted by Θ and some
92"
RULE BASED LEARNING,0.09375,"rule set R derived from expert knowledge or additional information. The notation ∗in the domain
93"
RULE BASED LEARNING,0.0947265625,"and codomain of f indicates that the input and output can be of arbitrary or variable dimension. As
94"
RULE BASED LEARNING,0.095703125,"usual f is a concatenation of sub-functions f 1, . . . , f l called the layers of the neural network. More
95"
RULE BASED LEARNING,0.0966796875,"precisely, the i-th layer is a function f i(−, Θi, Ri) : R∗−→R∗where Θi is a subset of the learnable
96"
RULE BASED LEARNING,0.09765625,"parameters Θ and Ri is an element of the ruleset R. We call a layer f i static if Ri is a static rule and
97"
RULE BASED LEARNING,0.0986328125,"dynamic if Ri is a dynamic rule. The input data is a triple (D, L, I), where D = {x1 . . . , xk} with
98"
RULE BASED LEARNING,0.099609375,"xi ∈R∗is the set of examples drawn from some unknown distribution. The labels are denoted by
99"
RULE BASED LEARNING,0.1005859375,"L = (y1 . . . , yk) with yi ∈R∗and I is some additional information known about the input data D.
100"
RULE BASED LEARNING,0.1015625,"This can be for example knowledge about the graph structure, node or edge labels, importance of
101"
RULE BASED LEARNING,0.1025390625,"neighborhoods and many more. One main assumption of this paper is that I can be used to derive a
102"
RULE BASED LEARNING,0.103515625,"set of static or dynamic rules R. Again we would like to mention that we concentrate on the analysis
103"
RULE BASED LEARNING,0.1044921875,"of the effects of applying different rules R and not on the very interesting but also wide field of
104"
RULE BASED LEARNING,0.10546875,"deriving the best rules R from I, see some discussion in Section 6. Nonetheless, we always motivate
105"
RULE BASED LEARNING,0.1064453125,"the choice of the rules derived by I.
106"
RULE BASED LEARNING,0.107421875,"Rule Based Layers
We now give a formal definition of rule based layers. Given some dataset
107"
RULE BASED LEARNING,0.1083984375,"(D, L, I) defined as before and the rule set R derived from I, the task is to learn the weights Θ of
108"
RULE BASED LEARNING,0.109375,"the neural network f to predict the labels of unseen examples drawn from an unknown distribution.
109"
RULE BASED LEARNING,0.1103515625,"Our contribution concentrates on single layers and is fully compatible with other layers such as
110"
RULE BASED LEARNING,0.111328125,"linear layers, convolutional layers Hence, in the following we restrict to the i-th layer f i(−, Θi, Ri) :
111"
RULE BASED LEARNING,0.1123046875,"R∗−→R∗of a network f. For simplicity, we assume i = 1 and omit the indices, i.e., we write
112"
RULE BASED LEARNING,0.11328125,"f := f i, Θ := Θi and R := Ri. The forward propagation step of the rule based layer f which will be
113"
RULE BASED LEARNING,0.1142578125,"a generalization of certain known layers as shown in Section 3 is as follows. Fix some input sample
114"
RULE BASED LEARNING,0.115234375,"x ∈D with x ∈Rn. Then f(−, Θ, R) : Rn −→Rm for n, m ∈N is given by
115"
RULE BASED LEARNING,0.1162109375,"f(x, Θ, R) = σ(WRW (x) · x + bRb(x)) .
(1)"
RULE BASED LEARNING,0.1171875,"Here σ denotes an arbitrary activation function and WRW (x) ∈Rm×n rsp. bRb(x) ∈Rm is some
116"
RULE BASED LEARNING,0.1181640625,"weight matrix rsp. weight vector depending on the input vector x and the rule R. The set Θ :=
117"
RULE BASED LEARNING,0.119140625,"{w1, . . . , wN, b1, . . . , bM} consists of all possible learnable parameters of the layer. The parameters
118"
RULE BASED LEARNING,0.1201171875,"{w1, . . . , wN} are possible entries of the weight matrix while {b1, . . . , bM} are possible entries of
119"
RULE BASED LEARNING,0.12109375,"the bias vector. The key point here is that the rule R determines the choices and the positions of
120"
RULE BASED LEARNING,0.1220703125,"the weights from Θ in the weight matrix WRW (x) and the bias vector bRb(x) depending on the input
121"
RULE BASED LEARNING,0.123046875,"1Our code, results and the datasplits used can be found here."
RULE BASED LEARNING,0.1240234375,"sample x. More precisely, not all learnable parameters must be used in the weight matrix and the
122"
RULE BASED LEARNING,0.125,"bias vector for some input sample x. Note that for two samples x, y ∈D of different dimensionality,
123"
RULE BASED LEARNING,0.1259765625,"e.g., x ∈Rn and y ∈Rk with n ̸= k the weight matrices WRW (x) and WRW (y) also have different
124"
RULE BASED LEARNING,0.126953125,"dimensions and the learnable parameters can be in totally different positions in the weight matrix.
125"
RULE BASED LEARNING,0.1279296875,"This is where the rules R and their associated rule functions, see (2) below, come into play.
126"
RULE BASED LEARNING,0.12890625,"Given the set of learnable parameters Θ := {w1, . . . , wN, b1, . . . , bM}, for each input x ∈Rn the
127"
RULE BASED LEARNING,0.1298828125,"rule R induces the following two rule functions
128"
RULE BASED LEARNING,0.130859375,"RW (x) : [m] × [n] −→{0} ∪[N]
and
Rb(x) : [m] −→{0} ∪[M]
(2)"
RULE BASED LEARNING,0.1318359375,"where m ∈N is the output dimension of the layer that can also depend on x. In the following we
129"
RULE BASED LEARNING,0.1328125,"abbreviate RW (x)(i, j) by RW (x, i, j) and Rb(x)(i) by Rb(x, i). We note that for simplicity we
130"
RULE BASED LEARNING,0.1337890625,"assume that the matrix and vector indices start at 1 and not at 0. Using the associated rule functions (2)
131"
RULE BASED LEARNING,0.134765625,"we can construct the weight matrix resp. bias vector by defining the entry (i, j) ∈Rm×n in the i-th
132"
RULE BASED LEARNING,0.1357421875,"row and the j-th column of the weight matrix WR(x) ∈Rm×n via
133"
RULE BASED LEARNING,0.13671875,"WRW (x)(i, j) :=
0
if RW (x, i, j) = 0
wRW (x,i,j)
o.w.
(3)"
RULE BASED LEARNING,0.1376953125,"and the entry at position k in the bias vector bRb(x) ∈Rm by
134"
RULE BASED LEARNING,0.138671875,"bRb(x)(k) :=
0
if Rb(x, k) = 0
bRb(x,k)
o.w.
.
(4)"
RULE BASED LEARNING,0.1396484375,"Summarizing, the rule based layer defined in (1) is a standard feed-forward layer with the difference
135"
RULE BASED LEARNING,0.140625,"that the weights in the weight matrix and the bias vector are determined by a predifined rule R.
136"
RULE BASED LEARNING,0.1416015625,"In fact, weight matrix and bias vector depend on the input and can contain shared weights. More
137"
RULE BASED LEARNING,0.142578125,"precisely, the rule controls the connection between the i-th input and the j-th output feature in the
138"
RULE BASED LEARNING,0.1435546875,"weight matrix. A rule R is called static if it is independent of the input x ∈D, i.e., R(x) ≡R(y)
139"
RULE BASED LEARNING,0.14453125,"for all inputs x, y ∈R ∈D otherwise it is called dynamic. We call a rule based layer as defined in (1)
140"
RULE BASED LEARNING,0.1455078125,"static if it is based on a static rule R and dynamic otherwise. We will show in Section 3 that rule
141"
RULE BASED LEARNING,0.146484375,"based layers generalize known concepts of neural network layers for specific rules R. In fact, we
142"
RULE BASED LEARNING,0.1474609375,"show that fully connected layers and convolution layers are static rule based layers. Examples of
143"
RULE BASED LEARNING,0.1484375,"dynamic rule based layers are given later on in Section 4. The back-propagation of such a layer can
144"
RULE BASED LEARNING,0.1494140625,"be done as usual enrolling the computation graph of the forward step and applying iteratively the
145"
RULE BASED LEARNING,0.150390625,"chain rule to all the computation steps. We will not go into the details of this computation as it is
146"
RULE BASED LEARNING,0.1513671875,"similar to many other computations using backpropagation with shared weights. For the experiments
147"
RULE BASED LEARNING,0.15234375,"we use the automatic backpropagation tool of PyTorch [16] which fully meets our requirements.
148"
RULE BASED LEARNING,0.1533203125,"Assumptions and Examples
Rule based learning relies on the following two main assumptions:
149"
RULE BASED LEARNING,0.154296875,"A1) There is a connection between the additional information or expert knowledge I and the used
150"
RULE BASED LEARNING,0.1552734375,"rule R and A2) The distribution of weights given by the rule R in the weight matrix WR(x) improves
151"
RULE BASED LEARNING,0.15625,"the predictive performance or increases the interpretability of the neural network. As stated before
152"
RULE BASED LEARNING,0.1572265625,"we concentrate on the second assumption and consider different distribution of weights in the weight
153"
RULE BASED LEARNING,0.158203125,"matrix given by different rules. In fact, we assume without further consideration that it is possible to
154"
RULE BASED LEARNING,0.1591796875,"derive a meaningful ruleset R from the additional information or expert knowledge I. For example if
155"
RULE BASED LEARNING,0.16015625,"the dataset consists of images we can derive the “informal” rule that neighboured pixels are more
156"
RULE BASED LEARNING,0.1611328125,"important than pixels far away and in case of chemical data there exists, e.g., the ortho-para rule for
157"
RULE BASED LEARNING,0.162109375,"benzene rings that makes assumptions about the influence of atoms for specific positions regarding
158"
RULE BASED LEARNING,0.1630859375,"the ring. This rule was already learned by a neural network in [28]. It is another very interesting task
159"
RULE BASED LEARNING,0.1640625,"which is beyond the scope of this work how to formalize these “informal” rules or to learn the “best”
160"
RULE BASED LEARNING,0.1650390625,"formal rules from the additional information I.
161"
RULE BASED LEARNING,0.166015625,"In the following sections we focus on the concept of rule based layers and therefore for simplicity
162"
RULE BASED LEARNING,0.1669921875,"and space reasons only consider the rule function of weight matrices. The rule function associated
163"
RULE BASED LEARNING,0.16796875,"with the bias vector can be constructed similarly. For simplicity, we write R instead of RW .
164"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1689453125,"3
Theoretical Aspects of Rule Based Layers
165"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.169921875,"In this section we provide a theoretical analysis of rule based layers and show that they generalize fully
166"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1708984375,"connected and convolutional layers. More precisely, we define two static rules RFC and RCNN and
167"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.171875,"show that the rule based layer as defined in (1) based on RFC is a fully connected layer and the rule
168"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1728515625,"based layer based on RCNN is a convolutional layer. All the proofs can be found in the Appendix A.
169"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.173828125,"Proposition 1 Let f : Rn −→Rm with
170"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1748046875,"f(y, Θ, RFC) = σ(WRFC(x) · y)
be a rule based layer of a neural network as defined in (1) (without bias term) with learnable
171"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.17578125,"parameters Θ = {w1, . . . , wn·m} and y = f i(x) is the result of the first i −1 layers. Then for the
172"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1767578125,"rule function RFC(x) : [m] × [n] →[m · n] defined for all inputs x as follows
173"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.177734375,"RFC := RFC(x)(i, j) := (i −1) · n + j,
the rule based layer f is equivalent to a fully connected layer with activation function σ.
174"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1787109375,"Proposition 1 shows that rule based layers generalize fully connected layers of arbitrary size without
175"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1796875,"bias vector and can be easily adapted to include the bias vector. Hence, this shows that rule based
176"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1806640625,"layers generalize arbitrary fully connected layers. Moreover, fully connected layers are static rule
177"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.181640625,"based layers as the rule RFC is static because it does not depend on the particular input x.
178"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1826171875,"Proposition 2 Let f : Rn·m −→R(n−N+1)·(m−N+1) with
179"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.18359375,"f(y, Θ, RCNN) = σ(WRCNN(x) · y)"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1845703125,"be a rule based layer of a neural network as defined in (1) (without bias term) and W i =
180"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.185546875,"{w1, . . . , wN2} be the set of learnable parameters. Then for the rule function RCNN : [(n −
181"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1865234375,"N + 1) · (m −N + 1)] × [n · m] →[N 2] defined by
182"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1875,"RCNN := RCNN(x)(i, j) := 
 "
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1884765625,"τ(i, j)
if 0 < γ(i, j) < N · n and
0 < j (mod n) −j + γ(i, j) < N
0
o.w.
183"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.189453125,"with
τ(i, j)
=
γ(i, j) −((γ(i, j) −1)//n) · (n −N)
and
γ(i, j)
=
j −((i −1)//(n −N + 1)) · n + (i −1) (mod (n −N + 1))"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1904296875,"the rule based layer f is equivalent to a convolution layer with quadratic kernel of size N (N < n,
184"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.19140625,"N < m) and a stride of one over a two-dimensional image of size n × m (without padding and bias
185"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1923828125,"vector) with activation function σ. The notation a//b denotes the integer division of two integers a
186"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.193359375,"and b.
187"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1943359375,"Proposition 2 shows that rule based layers generalize 2D-image convolution without padding and
188"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1953125,"bias term. By adaption of the rule function it is possible to include the bias vector and padding.
189"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1962890625,"Moreover, the result can be generalized to higher dimensions kernels, non-quadratic kernels and
190"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.197265625,"arbitrary input and output channels. Hence, rule based layers also generalize arbitrary convolutional
191"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.1982421875,"layers. Convolutional layers are static rule based layers as the rule RCNN is static because it is
192"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.19921875,"independent of the input. The following result is a direct implication from Propositions 1 and 2.
193"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.2001953125,"Theorem 1 Rule based layers generalize fully connected and convolutional feed-forward layers.
194"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.201171875,"Moreover, both layers are static rule based layers.
195"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.2021484375,"We claim that also other types of feed-forward layers can be generalized by rule based layers using
196"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.203125,"appropriate rule functions. Because of space limitations we would rather present a specific application
197"
THEORETICAL ASPECTS OF RULE BASED LAYERS,0.2041015625,"of dynamic rule based layers on graphs.
198"
RULE BASED LEARNING ON GRAPHS,0.205078125,"4
Rule Based Learning on Graphs
199"
RULE BASED LEARNING ON GRAPHS,0.2060546875,"One of the main advantages of rule based layers as introduced in this work is that they give rise to
200"
RULE BASED LEARNING ON GRAPHS,0.20703125,"a dynamic neural network architecture that is freely configurable using different rules. In fact, the
201"
RULE BASED LEARNING ON GRAPHS,0.2080078125,"network is independent of the dimension and structure of the input samples. Hence, a natural applica-
202"
RULE BASED LEARNING ON GRAPHS,0.208984375,"tion of our approach is graph classification. We would like to emphasize that graph classification is
203"
RULE BASED LEARNING ON GRAPHS,0.2099609375,"only one of many possible applications of rule based layers. Other possible applications are node
204"
RULE BASED LEARNING ON GRAPHS,0.2109375,"classification, regression tasks, graph embeddings or completely different data-structures.
205"
RULE BASED LEARNING ON GRAPHS,0.2119140625,"Graph Preliminaries
By a graph we mean a pair G = (V, E) with V denoting the set of nodes of
206"
RULE BASED LEARNING ON GRAPHS,0.212890625,"G and E ⊆{{i, j} | i, j ∈V } the set of edges. We assume that the graph is undirected and does
207"
RULE BASED LEARNING ON GRAPHS,0.2138671875,"not contain self-loops or parallel edges. In case that it is clear from the context we omit G and only
208"
RULE BASED LEARNING ON GRAPHS,0.21484375,"use V and E. The distance between two nodes i, j ∈V in a graph, i.e., the length of the shortest
209"
RULE BASED LEARNING ON GRAPHS,0.2158203125,"path between i and j, is denoted by d(i, j). A labeled graph is a graph G = (V, E) equipped with
210"
RULE BASED LEARNING ON GRAPHS,0.216796875,"a function l : V →L that assigns to each node a label from the set L ⊆N. In this paper the input
211"
RULE BASED LEARNING ON GRAPHS,0.2177734375,"samples corresponding to a graph (V, E) are always vectors of length equal to |V |. In particular, the
212"
RULE BASED LEARNING ON GRAPHS,0.21875,"input vectors can be interpreted as signals over the graph and each dimension of the input vector
213"
RULE BASED LEARNING ON GRAPHS,0.2197265625,"corresponds to the one-dimensional input signal of a graph node.
214"
GRAPH RULES,0.220703125,"4.1
Graph Rules
215"
GRAPH RULES,0.2216796875,"The example on molecule graphs in Figure 2 and Appendix A.4 motivates the intuition behind
216"
GRAPH RULES,0.22265625,"different graph specific rules that can be used to define a graph neural network based on rule layers.
217"
GRAPH RULES,0.2236328125,"The underlying general scheme to define a rule based layer on graphs is as follows: Let G = (V, E)
218"
GRAPH RULES,0.224609375,"be a graph and l : V →L a permutation equivariant labeling function of the nodes, i.e., for some
219"
GRAPH RULES,0.2255859375,"permutation π of V it holds l(π(V )) = π(l(V )). Assuming that input and output dimension of the
220"
GRAPH RULES,0.2265625,"layer is equal to |V | the rule functions R as defined in (2) map each pair of nodes (i, j) ∈V × V
221"
GRAPH RULES,0.2275390625,"to an integer which is the index of the learnable parameter in the set of all learnable parameters.
222"
GRAPH RULES,0.228515625,"The mapping is injective based on the labels l(i), l(j) and an additionally defined shared property
223"
GRAPH RULES,0.2294921875,"between the nodes i and j. Examples for such shared properties can be the distance between i and
224"
GRAPH RULES,0.23046875,"j, the type of the edge connecting i and j or the information, that i and j are in one circle. As an
225"
GRAPH RULES,0.2314453125,"example RMol as defined in Appendix A.4 is induced by the permutation equivariant function l that
226"
GRAPH RULES,0.232421875,"maps each node to its atom label and the shared property between two nodes is the type of the edge
227"
GRAPH RULES,0.2333984375,"connecting the nodes or the absence of an edge. Besides RMol the simple rule that is based on the
228"
GRAPH RULES,0.234375,"given node labels in this paper we focus on three different rule based layers for graphs.
229"
GRAPH RULES,0.2353515625,"Proposition 3 Let π be some permutation of the nodes of G = (V, E) and x its corresponding input
230"
GRAPH RULES,0.236328125,"vector. If R permutation equivariant, i.e., R(π(x))(i, j) = R(x)(π(i), π(j)) then the rule based
231"
GRAPH RULES,0.2373046875,"layer is also equivariant under node permutations, i.e., f(π(x), Θ, RMol) = π(f(x, Θ, RMol)).
232"
GRAPH RULES,0.23828125,"Weisfeiler-Leman Rule
Recent research has shown that the Weisfeiler-Leman labeling is a powerful
233"
GRAPH RULES,0.2392578125,"tool for graph classification [18, 14, 2, 22]. Thus, we propose to use Weisfeiler-Leman labels as
234"
GRAPH RULES,0.240234375,"one option to define the rule based layer for graph classification. The Weisfeiler-Leman algorithm
235"
GRAPH RULES,0.2412109375,"assigns in the k-th iteration to each node of a graph a label based on the structure of its local k-hop
236"
GRAPH RULES,0.2421875,"neighborhood, see [18]. Let l(v) be the result of the k-th iteration of the Weisfeiler-Leman algorithm
237"
GRAPH RULES,0.2431640625,"for some node v ∈V . Then the Weisfeiler-Leman Rule RW Lk,d assigns to each node pair (i, j) an
238"
GRAPH RULES,0.244140625,"integer or zero based on the Weisfeiler-Leman labels l(i), l(j) and the distance between the nodes i
239"
GRAPH RULES,0.2451171875,"and j. The result is zero if the distance between i and j is not between 1 and d. Note that we are not
240"
GRAPH RULES,0.24609375,"restricted to look at consecutive distances from 1 to d. It is also possible to look at certain distances
241"
GRAPH RULES,0.2470703125,"only if the expert knowledge suggests it. In fact, (i, j) and (k, l) are mapped to the same integer if
242"
GRAPH RULES,0.248046875,"and only if l(i) = l(k), l(j) = l(l) and the distance between i and j is equal to the distance between
243"
GRAPH RULES,0.2490234375,"k and l. The layer defined by this rule is related to ordinary message passing but messages can pass
244"
GRAPH RULES,0.25,"between nodes of arbitrary distances. For computational reasons in the experiments we restrict the
245"
GRAPH RULES,0.2509765625,"maximum number of different Weisfeiler-Leman labels considered by some bound L. We relabel
246"
GRAPH RULES,0.251953125,"the most frequent l −1 labels to 1, · · · , l −1 and set all other labels to l. The corresponding layer is
247"
GRAPH RULES,0.2529296875,"denoted by fW Lk,d,L.
248"
GRAPH RULES,0.25390625,"Pattern Counting Rule
Beyond labeling nodes via the Weisfeiler-Leman algorithm, it is a common
249"
GRAPH RULES,0.2548828125,"approach to use subgraph isomorphism counting to distinguish graphs [3]. This is in fact necessary
250"
GRAPH RULES,0.255859375,"as the 1-Weisfeiler-Leman algorithm is not able to distinguish some types of graphs, for example
251"
GRAPH RULES,0.2568359375,"circular skip link graphs [4] and strongly regular graphs [2, 3]. Thus, we propose the pattern counting
252"
GRAPH RULES,0.2578125,"rule and show in Section 5 that RuleGNNs based on this rule are able to perform well on synthetic
253"
GRAPH RULES,0.2587890625,"benchmark datasets while message passing models based on the Weisfeiler-Leman algorithm fail. In
254"
GRAPH RULES,0.259765625,"general, subgraph isomorphis counting is a hard problem [5], but for the real-world and synthetic
255"
GRAPH RULES,0.2607421875,"benchmark graph datasets that are usually considered, subgraphs of size k ∈{3, 4, 5, 6} can be
256"
GRAPH RULES,0.26171875,"enumerated in a preprocessing step in a reasonable time, see Table 5. Given a set of patterns, say
257"
GRAPH RULES,0.2626953125,"P, we compute all possible embeddings of these patterns in the graph dataset in a preprocessing
258"
GRAPH RULES,0.263671875,"step. Then for each pattern P ∈P and each node i ∈V we count how often the node i is part of an
259"
GRAPH RULES,0.2646484375,"embedding of P. Using those counts we define a labeling function l : V →L. Two nodes i, j ∈V
260"
GRAPH RULES,0.265625,"are mapped to the same label if and only if their counts are equal for all patterns in P. Patterns
261"
GRAPH RULES,0.2666015625,"that are often used in practice are small cycles, cliques, stars, paths, etc. The Pattern Counting Rule
262"
GRAPH RULES,0.267578125,"RPd assigns each node pair (i, j) an integer or zero based on the values of l(i), l(j) and the distance
263"
GRAPH RULES,0.2685546875,"between i and j. As for the Weisfeiler-Leman Rule we restrict the maximum number of different
264"
GRAPH RULES,0.26953125,"labels to some number L. The corresponding layer is denoted by fPd,L.
265"
GRAPH RULES,0.2705078125,"Summary Rule
The summary rule RN
Out can be used as the output layer as its output is a fixed
266"
GRAPH RULES,0.271484375,"dimensional vector of size N ∈N independent of the size of the input data and the output is invariant
267"
GRAPH RULES,0.2724609375,"under node permutations. Again, let l : V →L be a function that maps each node of a graph to some
268"
GRAPH RULES,0.2734375,"integer. Then the summary rule RN
Out assigns each pair (n, i) with i ∈V and n ∈[N] an integer
269"
GRAPH RULES,0.2744140625,"or zero based on n and l(i). In fact, for each element of L the rule defines n different learnable
270"
GRAPH RULES,0.275390625,"parameters. The corresponding layer is denoted by fRN
Out.
271"
GRAPH RULES,0.2763671875,"All the above rules define dynamic rule based neural network layers because the weight matrix and
272"
GRAPH RULES,0.27734375,"bias terms defined by the rules depend on the input vectors x corresponding to different graphs. Note
273"
GRAPH RULES,0.2783203125,"that the layers defined by the above rules are permutation equivariant as the node labeling function l
274"
GRAPH RULES,0.279296875,"used to define the rule is equivariant under node permutations. Thus, using the layers corresponding
275"
GRAPH RULES,0.2802734375,"to the above defined rules we can build a graph classification architecture that by definition does not
276"
GRAPH RULES,0.28125,"depend on the order of the nodes in the input graphs. Moreover, a layer is able to pass information
277"
GRAPH RULES,0.2822265625,"between nodes of arbitrary distances in the graph. Thus, as shown in the experiments below, it is not
278"
GRAPH RULES,0.283203125,"necessary to use deep networks to achieve good performance on the real-world benchmark datasets.
279"
GRAPH RULES,0.2841796875,"4.2
Rule Graph Neural Networks (RuleGNNs)
280"
GRAPH RULES,0.28515625,"The layers derived from the above rules are the building blocks of the RuleGNNs. Each RuleGNN is
281"
GRAPH RULES,0.2861328125,"a concatenation of different rule based layers from Weisfeiler-Leman rules and pattern counting rules
282"
GRAPH RULES,0.287109375,"followed by a summary rule using arbitrary activation functions. To define the learnable parameters
283"
GRAPH RULES,0.2880859375,"of the bias term we also use the summary rule. The input of the network is a signal x ∈R|V |
284"
GRAPH RULES,0.2890625,"corresponding to a graph G = (V, E). We note that for simplicity we focus on one-dimensional
285"
GRAPH RULES,0.2900390625,"signals but also multidimensional signals, i.e., x ∈R|V |×d are possible. The output of the network
286"
GRAPH RULES,0.291015625,"is a vector of fixed size N ∈N determined by the summary rule where N is usually the number
287"
GRAPH RULES,0.2919921875,"of classes of the graph classification task. The output can be also used as an intermediate vectorial
288"
GRAPH RULES,0.29296875,"representation of the graph or for regression tasks.
289"
EXPERIMENTS,0.2939453125,"5
Experiments
290"
EXPERIMENTS,0.294921875,"We evaluate the performance of RuleGNNs on different real-world and synthetic benchmark graph
291"
EXPERIMENTS,0.2958984375,"dataset and compare the results to the state-of-the-art graph classification algorithms. For comparabil-
292"
EXPERIMENTS,0.296875,"ity and reproducibility of the results, also with future algorithms, we make use of the experimental
293"
EXPERIMENTS,0.2978515625,"setup from [7]. That means, for each graph dataset we perform a 10-fold cross validation, i.e., we use
294"
EXPERIMENTS,0.298828125,"fixed splits of the dataset into 10 equally sized parts (the splits can be found in our repository), and
295"
EXPERIMENTS,0.2998046875,"use 9 of them for training, parameter tuning and validation. We then use the model that performs
296"
EXPERIMENTS,0.30078125,"best on the validation set and report the performance on the previously unseen test set. We train the
297"
EXPERIMENTS,0.3017578125,"best model 3 times and average the results on each fold to decrease random effects. The standard
298"
EXPERIMENTS,0.302734375,"deviation reported in the tables is computed over the results on the 10 folds.
299"
EXPERIMENTS,0.3037109375,"Data and Algorithm Selection
A problem of several heavily used graph benchmark datasets
300"
EXPERIMENTS,0.3046875,"like MUTAG or PTC [13] is that node and edge labels seems to be more important than the graph
301"
EXPERIMENTS,0.3056640625,"structure itself, i.e., there is no significant improvement over simple baselines [17]. Moreover, in
302"
EXPERIMENTS,0.306640625,"case of MUTAG the performance of the model is highly dependent on the data split because of the
303"
EXPERIMENTS,0.3076171875,"small number of samples. Thus, in this work for benchmarking we choose DHFR, Mutagenicity,
304"
EXPERIMENTS,0.30859375,"NCI1, NCI109, IMDB-BINARY and IMDB-MULTI from the TU Dortmund Benchmark Graphs
305"
EXPERIMENTS,0.3095703125,"repository [13] because the structure of the graphs seems to play an important role, i.e., the simple
306"
EXPERIMENTS,0.310546875,"baselines presented in [17, 7] are significantly worse than the state-of-the-art graph classification
307"
EXPERIMENTS,0.3115234375,"algorithms. Additionally, we consider circular skip link graphs CSL [4] and constructed some new
308"
EXPERIMENTS,0.3125,"synthetic benchmark graph datasets called LongRings, EvenOddRings and Snowflakes [15] to show
309"
EXPERIMENTS,0.3134765625,"the advantages of RuleGNNs on more complex graph structures with given expert knowledge. For
310"
EXPERIMENTS,0.314453125,"NCI1
NCI109
Mutagenicity
DHFR
IMDB-B
IMDB-M"
EXPERIMENTS,0.3154296875,"Baseline (NoG) [17]
69.2 ± 1.9
68.4 ± 2.2
74.8 ± 1.8
71.8 ± 5.3
71.9 ± 4.8
47.7 ± 4.0
WL-Kernel[18]
85.2 ± 2.3
85.0 ± 1.7
83.8 ± 2.4
83.5 ± 5.1
71.8 ± 4.5
51.9 ± 5.6
DGCNN[27]
76.4 ± 1.7
73.0 ± 2.4
77.0 ± 2.0
72.6 ± 3.1
69.2 ± 3.0
45.6 ± 3.4
DGCNN (features)
73.6 ± 1.0
72.5 ± 1.5
76.3 ± 1.2
76.1 ± 3.4
69.1 ± 3.5
45.8 ± 2.9
GraphSage[8]
76.0 ± 1.8
77.1 ± 1.8
79.8 ± 1.1
80.7 ± 4.5
68.8 ± 4.5
47.6 ± 3.5
GraphSage (features)
79.4 ± 2.2
78.6 ± 1.6
80.1 ± 1.3
82.4 ± 3.9
69.7 ± 3.1
46.6 ± 4.8
GIN[26]
80.0 ± 1.4
79.7 ± 2.0
81.9 ± 1.4
79.1 ± 4.4
71.2 ± 3.9
48.5 ± 3.3
GIN (features)
77.3 ± 1.8
77.7 ± 2.0
80.6 ± 1.3
81.8 ± 5.1
70.9 ± 3.8
48.3 ± 2.7"
EXPERIMENTS,0.31640625,"GSN (paper) [3]
83.5 ± 2.3
-
-
-
77.8 ± 3.3
54.3 ± 3.3
CIN (paper) [1]
83.6 ± 1.4
84.0 ± 1.6
-
-
75.6 ± 3.7
52.7 ± 3.1
SIN (paper)[2]
82.7 ± 2.1
-
-
-
75.6 ± 3.2
52.4 ± 2.9
PIN (paper) [22]
85.1 ± 1.5
84.0 ± 1.5
-
-
76.6 ± 2.9
-
RuleGNN
82.8 ± 2.0
83.2 ± 2.1
81.5 ± 1.3
84.3 ± 3.2
75.4 ± 3.3
52.0 ± 4.3
Table 1: Test set performance of several state-of-the-art graph classification algorithms averaged
over three different runs and 10 folds. The ± values report the standard deviation over the 10 folds.
The overall best results are colored red and the best ones obtained for the fair comparison from [7]
are in bold. The (features) variant of the algorithms uses the same information as the RuleGNN as
input features additionally to node labels. The (paper) results are taken from the respective papers
and might be obtained with different splits of the datasets."
EXPERIMENTS,0.3173828125,"more details on the datasets see Appendix A.5. For NCI1, IMDB-BINARY and IMDB-MULTI we
311"
EXPERIMENTS,0.318359375,"use the same splits as in [7] and for CSL we use the splits as in [6] and a 5-fold cross validation. We
312"
EXPERIMENTS,0.3193359375,"evaluate the performance of the RuleGNNs on these datasets and compare the results to the baselines
313"
EXPERIMENTS,0.3203125,"from [7] and [17] and the Weisfeiler-Leman subtree kernel (WL-Kernel) [18] which is one of the best
314"
EXPERIMENTS,0.3212890625,"performing graph classification algorithm besides the graph neural networks. For comparison with
315"
EXPERIMENTS,0.322265625,"state-of-the-art graph classification algorithms we follow [7] and compare to DGCNN [27], GIN [26]
316"
EXPERIMENTS,0.3232421875,"and GraphSAGE [8]. Additionally, we compare to the results of some newer state-of-the-art graph
317"
EXPERIMENTS,0.32421875,"classification algorithms [3, 1, 2, 22]. For the latter we use the results from the respective papers that
318"
EXPERIMENTS,0.3251953125,"might be obtained with different splits of the datasets.
319"
EXPERIMENTS,0.326171875,"Experimental Settings and Resources
All experiments were conducted on a AMD Ryzen 9 7950X
320"
EXPERIMENTS,0.3271484375,"16-Core Processor with 128 GB of RAM. For the competitors we use the implementations from [7].
321"
EXPERIMENTS,0.328125,"For the real-world datasets we were not aware of expert-knowledge, hence we tested different rules
322"
EXPERIMENTS,0.3291015625,"and combinations of the layers defined in Section 4.1. More details on the tested hyperparameters
323"
EXPERIMENTS,0.330078125,"can be found in Appendix A.7. We always use tanh for activation and the Adam optimizer [11] with
324"
EXPERIMENTS,0.3310546875,"a learning rate of 0.05 (real-world datasets) resp. 0.1 (synthetic datasets). For the real-world datasets
325"
EXPERIMENTS,0.33203125,"the learning rate was decreased by a factor of 0.5 after each 10 epochs. For the loss function we use
326"
EXPERIMENTS,0.3330078125,"the cross entropy loss. All models are trained for 50 (real-world) resp. 200 (synthetic) epochs and the
327"
EXPERIMENTS,0.333984375,"batch size was set to 128. We stopped if the validation accuracy did not improve for 25 epochs.
328"
EXPERIMENTS,0.3349609375,"Real-World Datasets
The results on the real-world datasets (Table 1) show that RuleGNNs are
329"
EXPERIMENTS,0.3359375,"able to outperform the state-of-the-art graph classification algorithms in the setting of [7] even if
330"
EXPERIMENTS,0.3369140625,"we add all the additional label information that RuleGNNs use to the input features of the graph
331"
EXPERIMENTS,0.337890625,"neural networks (see the (features) results in Table 1). This shows that the structural encoding of
332"
EXPERIMENTS,0.3388671875,"the additional label information is crucial for the performance of the graph neural networks and not
333"
EXPERIMENTS,0.33984375,"replacable by using more input features. Moreover, the results show that the Weisfeiler-Leman subtree
334"
EXPERIMENTS,0.3408203125,"kernel is the best performing graph classification algorithm on NC1, NCI109 and Mutagenicity. For
335"
EXPERIMENTS,0.341796875,"IMDB-BINARY and IMDB-MULTI our approach performs worse than the state-of-the-art graph
336"
EXPERIMENTS,0.3427734375,"classification algorithms that are not evaluated within the same experimental setup.
337"
EXPERIMENTS,0.34375,"Synthetic Datasets
The results on the synthetic benchmark graph dataset show that RuleGNNs
338"
EXPERIMENTS,0.3447265625,"outperform the state-of-the-art graph classification algorithms if expert knowledge is available even
339"
EXPERIMENTS,0.345703125,"in the case that mesage passing is enough to solve the task. In fact, CLS and Snowflakes are not
340"
EXPERIMENTS,0.3466796875,"solvable by the message passing model because they are not distinguishable by the 1-WL test. The
341"
EXPERIMENTS,0.34765625,"results on LongRings show that long range dependencies can be easily captured by RuleGNNs and
342"
EXPERIMENTS,0.3486328125,"also dependencies between nodes of different distances as in case of the EvenOddRings dataset can
343"
EXPERIMENTS,0.349609375,"be encoded by appropriate rules.
344"
EXPERIMENTS,0.3505859375,"LongRings
EvenOddRings
EvenOddRingsCount
CSL
Snowflakes"
EXPERIMENTS,0.3515625,"Baseline (NoG) [17]
30.17 ± 3.2
22.25 ± 3.0
47.9 ± 3.9
10.0 ± 0.0
27.3 ± 5.3
WL-Kernel [18]
100.0 ± 0.0
26.83 ± 4.2
47.8 ± 4.3
10.0 ± 0.0
27.9 ± 4.1
DGCNN [27]
29.9 ± 2.6
28.4 ± 2.5
59.1 ± 5.2
10.0 ± 0.0
26.0 ± 3.3
GraphSAGE [8]
29.8 ± 2.8
24.9 ± 2.7
51.3 ± 1.9
10.0 ± 0.0
25.0 ± 1.8
GIN [26]
32.0 ± 3.1
26.8 ± 2.5
51.0 ± 3.7
10.0 ± 0.0
24.5 ± 2.2
RuleGNN
99.0 ± 3.3
90.2 ± 7.2
100.0 ± 0.0
100.0 ± 0.0
97.9 ± 3.2
Table 2: Test set performance of several state-of-the-art graph classification algorithms averaged
over three different runs and 10 folds. The ± values report the standard deviation over the 10 folds.
The best results and our algorithm are highlighted in bold."
EXPERIMENTS,0.3525390625,"Interpretability of the Rule Based Layers
Each learnable parameter of RuleGNNs can be inter-
345"
EXPERIMENTS,0.353515625,"preted in terms of the importance of a connection between two nodes in a graph with respect to their
346"
EXPERIMENTS,0.3544921875,"labels and their shared property (in our case the distance). In Figures 1 and6 we see how the network
347"
EXPERIMENTS,0.35546875,"has learned the importance of different connections between nodes for different distances and labels.
348"
EXPERIMENTS,0.3564453125,"6
Related Work, Limitations and Concluding Remarks
349"
EXPERIMENTS,0.357421875,"Dynamic neural networks have been proven to be more efficient, have more representation power
350"
EXPERIMENTS,0.3583984375,"and better interpretability than static neural networks [9]. Our approach can be seen as a sample
351"
EXPERIMENTS,0.359375,"dependent dynamic neural network as for each input sample the network structure is adapted. In
352"
EXPERIMENTS,0.3603515625,"contrast to other sample dependent dynamic neural networks [20, 24], our approach changes the
353"
EXPERIMENTS,0.361328125,"layer structure based on a predefined rule instead of the whole architecture. The rule based layers
354"
EXPERIMENTS,0.3623046875,"of RuleGNNs use the Weissfeiler-Leman labeling algorithm and subgraph isomorphism counting
355"
EXPERIMENTS,0.36328125,"which are both recently used concepts in graph classification algorithms [18, 3, 2, 1]. The challenge
356"
EXPERIMENTS,0.3642578125,"for graph neural networks is the heterogenicity of the input data and the lack of a fixed order of the
357"
EXPERIMENTS,0.365234375,"input data. [19] proposes a dynamic neural network for graph classification that uses node and edge
358"
EXPERIMENTS,0.3662109375,"labels and is similar to our approach. In fact, they also show that their approach generalizes CNNs.
359"
EXPERIMENTS,0.3671875,"In contrast, they do not provide a general scheme to encode expert knowledge into the network.
360"
EXPERIMENTS,0.3681640625,"Moreover, their approach is not able to encode long range dependencies in the graph using only
361"
EXPERIMENTS,0.369140625,"one layer. There exist graph neural networks that have learned the ortho-para rule for molecules
362"
EXPERIMENTS,0.3701171875,"[28]. While the additional information used in these algorithms is mostly hard-coded, we are able to
363"
EXPERIMENTS,0.37109375,"integrate arbitrary rules.
364"
EXPERIMENTS,0.3720703125,"Limitations
Input Features: So far we have only considered 1-dimensional input signals and
365"
EXPERIMENTS,0.373046875,"node labels, i.e., our experimental results are restricted to graphs that have no multi-dimensional
366"
EXPERIMENTS,0.3740234375,"node features. Additionally, we have not considered edge features in our rules. In principle, multi-
367"
EXPERIMENTS,0.375,"dimensional node features and edge labels can be handled by our approach with the cost of increased
368"
EXPERIMENTS,0.3759765625,"complexity. Space: For each graph we need to precompute the pairwise distances and store the
369"
EXPERIMENTS,0.376953125,"positions of the weights in the weight-matrix. This is a disadvantage for large and dense graphs
370"
EXPERIMENTS,0.3779296875,"as we need to store a large number of positions. For dense graphs the number of positions can be
371"
EXPERIMENTS,0.37890625,"quadratic in the number of nodes. Structure: To define a meaningful rule for a layer the input and
372"
EXPERIMENTS,0.3798828125,"output features need to be logically connected. Fortunately, this is the case for graphs but this fact can
373"
EXPERIMENTS,0.380859375,"be a limitation for other structures. Combinatorics: If it is not possible to define a formal rule given
374"
EXPERIMENTS,0.3818359375,"some informal expert knowledge the number of possible rules that have to be tested can be very large.
375"
EXPERIMENTS,0.3828125,"Thus, it is an interesting question if it is possible to automatically learn a rule that captures the expert
376"
EXPERIMENTS,0.3837890625,"knowledge in the best way. Implementation: As stated in [9] there is a “gap between theoretical &
377"
EXPERIMENTS,0.384765625,"practical efficiency” regarding dynamic neural networks, i.e., common libraries such as PyTorch or
378"
EXPERIMENTS,0.3857421875,"TensorFlow are not optimized for dynamic neural networks.
379"
EXPERIMENTS,0.38671875,"Concluding Remarks
We have introduced a new type of neural network layer that dynamically
380"
EXPERIMENTS,0.3876953125,"arranges the learnable parameters in the weight matrices and bias vectors according to a formal
381"
EXPERIMENTS,0.388671875,"rule. On the one hand our approach generalizes classical neural network components such as fully
382"
EXPERIMENTS,0.3896484375,"connected layers and convolutional layers. On the other hand we are able to apply rule based layers
383"
EXPERIMENTS,0.390625,"to the task of graph classification showing that expert knowledge can be integrated into the learning
384"
EXPERIMENTS,0.3916015625,"process. Moreover, our approach gives rise to a more interpretable neural network architecture as
385"
EXPERIMENTS,0.392578125,"every learnable parameter is related to a specific connection between input and output features.
386"
REFERENCES,0.3935546875,"References
387"
REFERENCES,0.39453125,"[1] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Li`o, Guido F. Mont´ufar,
388"
REFERENCES,0.3955078125,"and Michael M. Bronstein. Weisfeiler and lehman go cellular: CW networks. In Marc’Aurelio
389"
REFERENCES,0.396484375,"Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan,
390"
REFERENCES,0.3974609375,"editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural
391"
REFERENCES,0.3984375,"Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages
392"
REFERENCES,0.3994140625,"2625–2640, 2021.
393"
REFERENCES,0.400390625,"[2] Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F. Mont´ufar, Pietro Li´o,
394"
REFERENCES,0.4013671875,"and Michael M. Bronstein. Weisfeiler and lehman go topological: Message passing simplicial
395"
REFERENCES,0.40234375,"networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International
396"
REFERENCES,0.4033203125,"Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of
397"
REFERENCES,0.404296875,"Proceedings of Machine Learning Research, pages 1026–1037. PMLR, 2021.
398"
REFERENCES,0.4052734375,"[3] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving
399"
REFERENCES,0.40625,"graph neural network expressivity via subgraph isomorphism counting. IEEE Trans. Pattern
400"
REFERENCES,0.4072265625,"Anal. Mach. Intell., 45(1):657–668, 2023.
401"
REFERENCES,0.408203125,"[4] Jin-yi Cai, Martin F¨urer, and Neil Immerman. An optimal lower bound on the number of
402"
REFERENCES,0.4091796875,"variables for graph identification. Comb., 12(4):389–410, 1992.
403"
REFERENCES,0.41015625,"[5] Stephen A. Cook. The complexity of theorem-proving procedures. In Michael A. Harrison,
404"
REFERENCES,0.4111328125,"Ranan B. Banerji, and Jeffrey D. Ullman, editors, Proceedings of the 3rd Annual ACM Sym-
405"
REFERENCES,0.412109375,"posium on Theory of Computing, May 3-5, 1971, Shaker Heights, Ohio, USA, pages 151–158.
406"
REFERENCES,0.4130859375,"ACM, 1971.
407"
REFERENCES,0.4140625,"[6] Vijay Prakash Dwivedi, Chaitanya K. Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio,
408"
REFERENCES,0.4150390625,"and Xavier Bresson. Benchmarking graph neural networks. J. Mach. Learn. Res., 24:43:1–43:48,
409"
REFERENCES,0.416015625,"2023.
410"
REFERENCES,0.4169921875,"[7] Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph
411"
REFERENCES,0.41796875,"neural networks for graph classification. ArXiv, abs/1912.09893, 2019.
412"
REFERENCES,0.4189453125,"[8] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on
413"
REFERENCES,0.419921875,"large graphs. In Neural Information Processing Systems, 2017.
414"
REFERENCES,0.4208984375,"[9] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic
415"
REFERENCES,0.421875,"neural networks: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 44(11):7436–7456, 2022.
416"
REFERENCES,0.4228515625,"[10] Yacine Jernite, Edouard Grave, Armand Joulin, and Tom´as Mikolov. Variable computation in
417"
REFERENCES,0.423828125,"recurrent neural networks. In 5th International Conference on Learning Representations, ICLR
418"
REFERENCES,0.4248046875,"2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,
419"
REFERENCES,0.42578125,"2017.
420"
REFERENCES,0.4267578125,"[11] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
421"
REFERENCES,0.427734375,"Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
422"
REFERENCES,0.4287109375,"ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
423"
REFERENCES,0.4296875,"[12] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard,
424"
REFERENCES,0.4306640625,"Wayne E. Hubbard, and Lawrence D. Jackel. Handwritten digit recognition with a back-
425"
REFERENCES,0.431640625,"propagation network. In David S. Touretzky, editor, Advances in Neural Information Processing
426"
REFERENCES,0.4326171875,"Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pages 396–404.
427"
REFERENCES,0.43359375,"Morgan Kaufmann, 1989.
428"
REFERENCES,0.4345703125,"[13] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion
429"
REFERENCES,0.435546875,"Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML
430"
REFERENCES,0.4365234375,"2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020.
431"
REFERENCES,0.4375,"[14] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen,
432"
REFERENCES,0.4384765625,"Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural
433"
REFERENCES,0.439453125,"networks. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The
434"
REFERENCES,0.4404296875,"Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth
435"
REFERENCES,0.44140625,"AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,
436"
REFERENCES,0.4423828125,"Hawaii, USA, January 27 - February 1, 2019, pages 4602–4609. AAAI Press, 2019.
437"
REFERENCES,0.443359375,"[15] Harish G. Naik, Jan Polster, Raj Shekhar, Tam´as Horv´ath, and Gy¨orgy Tur´an. Iterative graph
438"
REFERENCES,0.4443359375,"neural network enhancement via frequent subgraph mining of explanations, 2024.
439"
REFERENCES,0.4453125,"[16] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
440"
REFERENCES,0.4462890625,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
441"
REFERENCES,0.447265625,"Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
442"
REFERENCES,0.4482421875,"Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style,
443"
REFERENCES,0.44921875,"High-Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer,
444"
REFERENCES,0.4501953125,"F. d’Alch´e Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
445"
REFERENCES,0.451171875,"Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.
446"
REFERENCES,0.4521484375,"[17] Till Hendrik Schulz and Pascal Welke. On the necessity of graph kernel baselines. 2019.
447"
REFERENCES,0.453125,"[18] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M.
448"
REFERENCES,0.4541015625,"Borgwardt. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res., 12:2539–2561, 2011.
449"
REFERENCES,0.455078125,"[19] Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional
450"
REFERENCES,0.4560546875,"neural networks on graphs. In 2017 IEEE Conference on Computer Vision and Pattern Recogni-
451"
REFERENCES,0.45703125,"tion, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 29–38. IEEE Computer Society,
452"
REFERENCES,0.4580078125,"2017.
453"
REFERENCES,0.458984375,"[20] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. Branchynet: Fast inference via early
454"
REFERENCES,0.4599609375,"exiting from deep neural networks. In 23rd International Conference on Pattern Recognition,
455"
REFERENCES,0.4609375,"ICPR 2016, Canc´un, Mexico, December 4-8, 2016, pages 2464–2469. IEEE, 2016.
456"
REFERENCES,0.4619140625,"[21] Geoffrey G. Towell and Jude W. Shavlik. Knowledge-based artificial neural networks. Artif.
457"
REFERENCES,0.462890625,"Intell., 70(1-2):119–165, 1994.
458"
REFERENCES,0.4638671875,"[22] Quang Truong and Peter Chin. Weisfeiler and lehman go paths: Learning topological features
459"
REFERENCES,0.46484375,"via path complexes. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors,
460"
REFERENCES,0.4658203125,"Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference
461"
REFERENCES,0.466796875,"on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on
462"
REFERENCES,0.4677734375,"Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver,
463"
REFERENCES,0.46875,"Canada, pages 15382–15391. AAAI Press, 2024.
464"
REFERENCES,0.4697265625,"[23] Laura von R¨uden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach,
465"
REFERENCES,0.470703125,"Raoul Heese, Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, Michal
466"
REFERENCES,0.4716796875,"Walczak, Jochen Garcke, Christian Bauckhage, and Jannis Schuecker. Informed machine
467"
REFERENCES,0.47265625,"learning - A taxonomy and survey of integrating prior knowledge into learning systems. IEEE
468"
REFERENCES,0.4736328125,"Trans. Knowl. Data Eng., 35(1):614–633, 2023.
469"
REFERENCES,0.474609375,"[24] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez. Skipnet: Learning
470"
REFERENCES,0.4755859375,"dynamic routing in convolutional networks. In Vittorio Ferrari, Martial Hebert, Cristian Smin-
471"
REFERENCES,0.4765625,"chisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference,
472"
REFERENCES,0.4775390625,"Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, volume 11217 of Lecture
473"
REFERENCES,0.478515625,"Notes in Computer Science, pages 420–436. Springer, 2018.
474"
REFERENCES,0.4794921875,"[25] Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, and Gao Huang. Adaptive
475"
REFERENCES,0.48046875,"focus for efficient video recognition. In 2021 IEEE/CVF International Conference on Computer
476"
REFERENCES,0.4814453125,"Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 16229–16238. IEEE,
477"
REFERENCES,0.482421875,"2021.
478"
REFERENCES,0.4833984375,"[26] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
479"
REFERENCES,0.484375,"networks? In 7th International Conference on Learning Representations, ICLR 2019, New
480"
REFERENCES,0.4853515625,"Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
481"
REFERENCES,0.486328125,"[27] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
482"
REFERENCES,0.4873046875,"architecture for graph classification. In Sheila A. McIlraith and Kilian Q. Weinberger, editors,
483"
REFERENCES,0.48828125,"Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the
484"
REFERENCES,0.4892578125,"30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium
485"
REFERENCES,0.490234375,"on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA,
486"
REFERENCES,0.4912109375,"February 2-7, 2018, pages 4438–4445. AAAI Press, 2018.
487"
REFERENCES,0.4921875,"[28] Zhenpeng Zhou and Xiaocheng Li. Graph convolution: A high-order and adaptive approach.
488"
REFERENCES,0.4931640625,"arXiv: Learning, 2017.
489 H H H H C C H H H H C C H H C C C H H C C C"
REFERENCES,0.494140625,"Figure 2: Information propagation in a simple two layer RuleGNN based on the molecule graphs of
ethylene (left) and cyclopropenylidene (right) and the rules RMol (5) and ROut (6). The input signal
is propagated from left to right. The graph nodes represent the neurons of the neural network. Edges
of the same color denote shared weights in a layer. For more details see Appendix A.4."
REFERENCES,0.4951171875,"A
Appendix / supplemental material
490"
REFERENCES,0.49609375,"A.1
Proof of Proposition 1
491"
REFERENCES,0.4970703125,"To show the equivalence between the two layers it suffices to show that their weight matrices coincide.
492"
REFERENCES,0.498046875,"In case of fully connected layers we have to show that the weight matrix WRFC(x) ∈Rm×n is filled
493"
REFERENCES,0.4990234375,"with n · m distinct weights. This can be easily checked by computing WRFC(x) using the definition
494"
REFERENCES,0.5,"of the weight distribution based on the rule function in (3).
495"
REFERENCES,0.5009765625,"A.2
Proof of Proposition 2
496"
REFERENCES,0.501953125,"Instead of the original two-dimensional image of size n×m we consider a reshaped vector x ∈Rn·m
497"
REFERENCES,0.5029296875,"as our definition of rule based layers is restricted to simple vector matrix multiplication. The output
498"
REFERENCES,0.50390625,"vector of dimension (n−N+1)·(m−N+1) can then again be reshaped into a two-dimensional image
499"
REFERENCES,0.5048828125,"of size (n−N +1)×(m−N +1). Unfortunately, the reshaping makes the rule function complicated
500"
REFERENCES,0.505859375,"as the indices of the reshaped vector have to be mapped to the indices of the two-dimensional image.
501"
REFERENCES,0.5068359375,"First note that convolution with a N × N kernel corresponds to matrix-vector multiplication of a
502"
REFERENCES,0.5078125,"doubly block circulant matrix that is a special case of a block Toeplitz matrix. Hence, to show the
503"
REFERENCES,0.5087890625,"equivalence between the layers we have to compare the weight matrices and show that the entries in
504"
REFERENCES,0.509765625,"WRCNN(x) ∈R(n−N+1)·(m−N+1)×n·m exactly matches the entries in the block Toeplitz matrix of
505"
REFERENCES,0.5107421875,"the same dimension that corresponds to the convolution kernel. Comparing the definition of block
506"
REFERENCES,0.51171875,"Toeplitz matrices with the above given rule shows that the rule exactly returns the entries of the block
507"
REFERENCES,0.5126953125,"Toeplitz matrix. Hence, the multiplication of x with WRCNN(x) is equivalent to multiplication of x
508"
REFERENCES,0.513671875,"with the block Toeplitz matrix that is equivalent to the convolution of x with a kernel of size N × N.
509"
REFERENCES,0.5146484375,"A.3
Proof of Proposition 3
510"
REFERENCES,0.515625,"The proof of Proposition 3 follows directly from the definitions of the rule based layers, see (1), and
511"
REFERENCES,0.5166015625,"the rule functions, see (2). If the order of the nodes in the graph is permuted and rule function is
512"
REFERENCES,0.517578125,"permutation equivariant, then the node labels are permuted accordingly. Hence, the positions of the
513"
REFERENCES,0.5185546875,"weights in the weight matrix and the bias term are permuted in the same way as the node labels. Thus,
514"
REFERENCES,0.51953125,"the result of f, i.e., the multiplication of permuted weight matrix with the permuted input signal, is
515"
REFERENCES,0.5205078125,"the same as the permutation of the result of the multiplication of the original weight matrix with the
516"
REFERENCES,0.521484375,"original input signal.
517"
REFERENCES,0.5224609375,"A.4
Example: RuleGNN for Molecule Graphs
518"
REFERENCES,0.5234375,"Assume the task is to learn a property of a molecule based on its graph structure. In this example we
519"
REFERENCES,0.5244140625,"present a RuleGNN that is a concatenation of two very simple rule based layers. The advantage of
520"
REFERENCES,0.525390625,"rule based layers and hence also RuleGNNs is that they encode the graph structure (in this example
521"
REFERENCES,0.5263671875,"the structure of two molecules) directly into the neural network. Moreover, the input samples can be
522"
REFERENCES,0.52734375,"arbitrary molecule graphs and the output is a vector of fixed size k that encodes the property of the
523"
REFERENCES,0.5283203125,"molecule or some intermediate vectorial representation. In this example we consider the molecule
524"
REFERENCES,0.529296875,"graphs of ethylene and cyclopropenylidene given in Figure 3 together with their corresponding input
525 C6
C5 H1 H2
H3 H4 C3 H1 C5 C4 H2"
REFERENCES,0.5302734375,"Figure 3: Molecule graphs of ethylene (left) and cyclopropenylidene (right). The indices denote the
order of the nodes."
REFERENCES,0.53125,"signals x ∈R6 and y ∈R5. The atoms of the molecules (hydrogen H and carbon C) correspond
526"
REFERENCES,0.5322265625,"to the nodes of a graph and the bond types (single and double) correspond to the edges. The atom
527"
REFERENCES,0.533203125,"labels and the atom bond types can be seen as additional information I that is known about the input
528"
REFERENCES,0.5341796875,"samples. The graph nodes are indexed via integers in some arbitrary but fixed order and the atom
529"
REFERENCES,0.53515625,"corresponding to a graph node are given by the labeling function l : V →{H, C}.
530"
REFERENCES,0.5361328125,"The RuleGNN consists of two rule based layers f1(, Θ1, RMol) and f(, Θ2, ROut) with learnable
531"
REFERENCES,0.537109375,"parameters Θ1 = {w1, . . . , w6} and Θ2 = {w′
1, . . . , w′
2·k} and the following rule functions RMol
532"
REFERENCES,0.5380859375,"and ROut. For some graph G = (V, E) and its corresponding input signal z we define RMol as
533"
REFERENCES,0.5390625,"follows:
534"
REFERENCES,0.5400390625,"RMol(z) :
[|V |] × [|V |]
−→
{0} ∪[6]"
REFERENCES,0.541015625,"(i, j)
7→"
REFERENCES,0.5419921875,"









"
REFERENCES,0.54296875,"








"
REFERENCES,0.5439453125,"1
if i = j and l(i) = H
2
if i = j and l(i) = C
3
if (i, j) is an edge (-), l(i) = H, l(j) = C
4
if (i, j) is an edge (-),l(i) = C, l(j) = H
5
if (i, j) is an edge (-),l(i) = l(j) = C
6
if (i, j) is an edge (=),l(i) = l(j) = C
0
o.w. (5)"
REFERENCES,0.544921875,"For some graph G = (V, E) and its corresponding input signal z we define ROut as follows:
535"
REFERENCES,0.5458984375,"ROut(z) :
[|V |] × [k]
−→
{0} ∪[2 · k]"
REFERENCES,0.546875,"(i, j)
7→ 
 "
REFERENCES,0.5478515625,"1 · j
l(i) = H
2 · j
l(i) = C
0
o.w. (6)"
REFERENCES,0.548828125,"Note that RMol and ROut are not restricted to the two molecules from above but can be applied
536"
REFERENCES,0.5498046875,"to arbitrary molecule graphs. Indeed, applying it to molecules with atom labels different from H
537"
REFERENCES,0.55078125,"or C makes the rules less powerful, i.e., it should be adapted to the type of molecules. Using the
538"
REFERENCES,0.5517578125,"definition (3) of weight distribution defined by the rule function we can construct the weight matrices
539"
REFERENCES,0.552734375,"WRMol(x), WROut(x) for the ethylene graph and WRMol(y), WROut(y) for the cyclopropenylidene
540"
REFERENCES,0.5537109375,"graph as follows:
541"
REFERENCES,0.5546875,"WRMol(x) =  
"
REFERENCES,0.5556640625,"w1
0
0
0
w3
0
0
w1
0
0
w3
0
0
0
w1
0
0
w3
0
0
0
w1
0
w3
w4 w4
0
0
w2 w5
0
0
w4 w4 w5 w2 "
REFERENCES,0.556640625,"

WROut(x) ="
REFERENCES,0.5576171875,"w′
1
w′
1
w′
1
w′
1
w′
2
w′
2
...
...
...
...
...
...
w′
2k−1 w′
2k−1 w′
2k−1 w′
2k−1 w′
2k w′
2k !"
REFERENCES,0.55859375,WRMol(y) =  
REFERENCES,0.5595703125,"w1
0
w3
0
0
0
w1
0
w3
0
w4
0
w2 w6 w5
0
w3 w6 w2 w5
0
0
w5 w5 w2 "
REFERENCES,0.560546875,"
WROut(y) ="
REFERENCES,0.5615234375,"w′
1
w′
1
w′
2
w′
2
w′
2
...
...
...
...
...
w′
2k−1 w′
2k−1 w′
2k w′
2k w′
2k !"
REFERENCES,0.5625,"Combining the two rule based layers we obtain the RuleGNN and the forward propagation is given
542"
REFERENCES,0.5634765625,"by σ(WROut(x) · σ(WRMol(x) · x)) for the ethylene graph and σ(WROut(y) · σ(WRMol(y) · y)) for the
543"
REFERENCES,0.564453125,"cyclopropenylidene graph.
544"
REFERENCES,0.5654296875,"Dataset
#Graphs
#Nodes
#Edges
Diameter
#Node Labels
#Classes
max
avg
min
max
avg
min
max
avg
min"
REFERENCES,0.56640625,"NCI1
4 110
111
29.9
3
119
32.3
2
45
11.5
0
37
2
NCI109
4 127
111
29.7
4
119
32.1
3
61
11.3
0
38
2
Mutagenicity
4 337
417
30.3
4
112
30.8
3
41
6.3
0
14
2
DHFR
756
71
42.4
20
73
44.5
21
22
14.6
8
9
2
IMDB-BINARY
1 000
136
19.8
12
1249
96.5
26
2
1.9
1
1
2
IMDB-MULTI
1 500
89
13.0
7
1467
65.9
12
2
1.5
1
1
3
Table 3: Details on the real-world datasets used in the experiments. The datasets are from the TU
Dortmund Graph Database [13]."
REFERENCES,0.5673828125,"Note that the forward propagation of the layer corresponding to the rule RMol is kind of a multiplica-
545"
REFERENCES,0.568359375,"tion with a weighted adjacency matrix of the graph where the weights of the adjacency matrix are
546"
REFERENCES,0.5693359375,"given by the learnable parameters, see also Figure 2. In contrast to adjacency matrices the weight
547"
REFERENCES,0.5703125,"matrix is not necessary symmetric. The computation graph induced by the weight matrix exactly
548"
REFERENCES,0.5712890625,"represent the graph structure while the edge weights are shared across the network using the rule, see
549"
REFERENCES,0.572265625,"Figure 2. Note that the above defined rule is very flexible as also edge labels (e.g., atomic bonds)
550"
REFERENCES,0.5732421875,"can be taken into account by increasing the size of the weight set. Moreover, it is possible to include
551"
REFERENCES,0.57421875,"bigger neighbourhoods, i.e., all nodes reachable by k-hops. Of course using other information of
552"
REFERENCES,0.5751953125,"the graph (e.g., substructures (such as circles or cliques), node degrees, connections not depicted by
553"
REFERENCES,0.576171875,"edges) more complicated rules can be defined.
554"
REFERENCES,0.5771484375,"A.5
Dataset Details
555"
REFERENCES,0.578125,"In this section we provide additional details on the datasets used in the experiments. Table 3 shows
556"
REFERENCES,0.5791015625,"an overview of the real-world datasets and Table 4 provides an overview of the synthetic datasets.
557"
REFERENCES,0.580078125,"We consider the following synthetic datasets. The CSL dataset is from []. We constructed the other
558"
REFERENCES,0.5810546875,"datasets to demonstrate the strength of our approach to encode expert knowledge into the neural
559"
REFERENCES,0.58203125,"network.
560"
REFERENCES,0.5830078125,"LongRings
LongRings consists of 1200 cycles of 100 nodes each. Four nodes are labeled by
561"
REFERENCES,0.583984375,"1, 2, 3, 4 and all other nodes are labeled by 0. The distance between each pair of the four nodes is
562"
REFERENCES,0.5849609375,"exactly 25 or 50. The label of the graph is 0 if 1 and 2 have distance 50, 1 if 1 and 3 have distance
563"
REFERENCES,0.5859375,"50 and 2 if 1 and 4 have distance 50. There are 400 graphs for each class. The difficulty of the
564"
REFERENCES,0.5869140625,"classification task is that information has to be propagated over a long distance. Regarding RuleGNNs
565"
REFERENCES,0.587890625,"this is very easy because if the expert knows that distance 50 is relevant we can define an appropriate
566"
REFERENCES,0.5888671875,"rule.
567"
REFERENCES,0.58984375,"EvenOddRings
EvenOddRings consists of 1200 cycles of 16 nodes each. The nodes in each graph
568"
REFERENCES,0.5908203125,"are labeled from 0 to 15. The graph label is based on the labels of the nodes that have distance 8
569"
REFERENCES,0.591796875,"respectively 4 to the node with label 0. We denote them by x resp. y, z. We distinct four cases: x is
570"
REFERENCES,0.5927734375,"even and y + z is even, x is even and y + z is odd, x is odd and y + z is even, x is odd and y + z is
571"
REFERENCES,0.59375,"odd. There are 300 graphs for each class, i.e., each of the four cases. The expert knowledge we use is
572"
REFERENCES,0.5947265625,"that the information has to be collected from nodes of distance 8 and 4.
573"
REFERENCES,0.595703125,"EvenOddRingsCount
EvenOddRingsCount consists of the same graphs as EvenOddRings but
574"
REFERENCES,0.5966796875,"the graph labels are different. For all nodes and their opposite node in the circle the sum of the
575"
REFERENCES,0.59765625,"labels is computed. If there are more even sums than odd sums the graph is labeled by 0 and by 1
576"
REFERENCES,0.5986328125,"otherwise. There are 600 graphs for each class. The expert knowledge we use is the information that
577"
REFERENCES,0.599609375,"only distance 8 is relevant.
578"
REFERENCES,0.6005859375,"Snowflakes
Snowflakes is a dataset consisting of graphs proposed by [15] that are not distinguish-
579"
REFERENCES,0.6015625,"able by the 1-WL test, see Figure 4 for an example. The dataset consists of circles of length 3 to 12
580"
REFERENCES,0.6025390625,"and at each circle node a graph from M0, M1, M2 or M3 is attached, see Figure 5 and [15] for the
581"
REFERENCES,0.603515625,"details. M0, M1, M2 and M3 are non-isomorphic graphs that are not distinguishable by the 1-WL
582"
REFERENCES,0.6044921875,"test. One label in the circle is labeled by 1 and all other nodes are labeled by 0. The label of the graph
583"
REFERENCES,0.60546875,"is determined by the graph M0, M1, M2 or M3 that is attached to the circle node with label 1.
584"
REFERENCES,0.6064453125,Figure 4: Example graphs from the Snowflakes dataset.
REFERENCES,0.607421875,"Figure 5: The graphs M0, M1, M2, M3 from [15] that are not distinguishable by the 1-WL test."
REFERENCES,0.6083984375,"Dataset
#Graphs
#Nodes
#Edges
Diameter
#Node Labels
#Classes
max
avg
min
max
avg
min
max
avg
min"
REFERENCES,0.609375,"LongRings
1 200
100
100.0
100
100
100.0
100
50
50.0
50
5
3
EvenOddRings
1 200
16
16.0
16
16
16.0
16
8
8.0
8
16
4
EvenOddRingsCount
1 200
16
16.0
16
16
16.0
16
8
8.0
8
16
2
CSL
150
41
41.0
41
82
82.0
82
10
6.0
4
1
10
Snowflakes
1 000
180
112.5
45
300
187.5
75
18
15.5
13
2
4
Table 4: Details of the synthetic datasets used in the experiments. The CSL dataset is from [4]."
REFERENCES,0.6103515625,"A.6
RuleGNNs: Runtimes
585"
REFERENCES,0.611328125,"Table 5 shows more details of the RuleGNN model. In particular, we see that except for the DHFR
586"
REFERENCES,0.6123046875,"dataset we need less than 12 epochs on average to reach the best result. This shows that our approach
587"
REFERENCES,0.61328125,"is very efficient and converges quickly. At the first glance the average time per epoch seems to be very
588"
REFERENCES,0.6142578125,"high. This has two reasons. One is also mentioned in [9] that there is a gap between the theoretical
589"
REFERENCES,0.615234375,"and practical runtime of dynamic neural networks because the implementation in PyTorch is not
590"
REFERENCES,0.6162109375,"optimized for dynamic neural networks. The other reason is that we parallelized the computation, i.e.,
591"
REFERENCES,0.6171875,"we are able to run all the three runs and 10 folds in parallel on the same machine. Of course, this
592"
REFERENCES,0.6181640625,"produces some overhead. As stated above the preprocessing times are not relevant for the experiments
593"
REFERENCES,0.619140625,"as they are only needed once. The third column shows the time needed to compute all the pairwise
594"
REFERENCES,0.6201171875,"distances between the nodes of the graph. The fourth column shows the time needed to compute the
595"
REFERENCES,0.62109375,"node labels used for the best model. The most preprocessing time is needed for IMDB-BINARY and
596"
REFERENCES,0.6220703125,"IMDB-MULTI because the graphs are much denser than the other datasets. For the synthetic datasets
597"
REFERENCES,0.623046875,"except for CSL we do not need any label preprocessing time as the original node labels are used.
598"
REFERENCES,0.6240234375,"A.7
RuleGNNs: Architectures and Hyperparameters
599"
REFERENCES,0.625,"Table 6 provides an overview of the different architectures used in the experiments that achieved
600"
REFERENCES,0.6259765625,"the best results. One advantage of our approach is that messages can be passed over long distances.
601"
REFERENCES,0.626953125,"Hence, except for the EvenOddRings dataset we used only one layer and the output layer. In case
602"
REFERENCES,0.6279296875,"of NCI1, NCI109, Mutagenicity it turns out that the best model uses the Weisfeiler-Leman rule
603"
REFERENCES,0.62890625,"with k = 2 iterations. We restricted the number of maximum labels considered to 500 which
604"
REFERENCES,0.6298828125,"results in 250000 learnable parameters for the weight matrix and 500 for the bias vector. For the
605"
REFERENCES,0.630859375,"output layer we used the bound of 50000 learnable parameters which was larger than the number of
606"
REFERENCES,0.6318359375,"different Weisfeiler-Leman labels in the second iteration. Interestingly, for NCI1 and NCI109 the
607"
REFERENCES,0.6328125,"best validation accuracy was achieved if considering node pairs with maximum distance 10. In case
608"
REFERENCES,0.6337890625,"Dataset
Best Epoch
Avg. Epoch (s)
Preproc. Distances (s)
Preproc. Labels (s)
Num. Graphs"
REFERENCES,0.634765625,"NCI1
7.3 ± 5.3
377.1 ± 20.7
2.0
11.9
4 110
NCI109
5.4 ± 2.9
386.7 ± 1.9
2.4
13.2
4 127
Mutagenicity
9.1 ± 4.1
575.8 ± 66.4
2.2
15.2
4 337
DHFR
23.1 ± 14.6
44.4 ± 9.0
0.7
3.1
756
IMDB-BINARY
11.3 ± 4.6
24.3 ± 0.9
0.2
206.5
1 000
IMDB-MULTI
6.7 ± 3.5
19.6 ± 1.3
0.2
195.0
1 500"
REFERENCES,0.6357421875,"LongRings
194.2 ± 15.1
0.7 ± 0.2
6.6
-
1 200
EvenOddRings
176.1 ± 15.2
1.2 ± 0.3
0.2
-
1 200
EvenOddRingsCount
199.0 ± 0.0
0.5 ± 0.1
0.1
-
1 200
CSL
49.0 ± 0.0
1.6 ± 0.0
0.1
11.8
150
Snowflakes
191.7 ± 18.9
0.5 ± 0.1
7.1
-
1 000
Table 5: Runtimes and preprocessing times of the different datasets used in the experiments. All
values are averaged over the best runs. The first column shows the best epoch (highest validation
accuracy), the second column shows the average time per epoch, the third column shows the time
needed to compute all the pairwise distances between the nodes of the graph, the fourth column
shows the time needed to compute the node labels used for the best model and the last column shows
the number of graphs in the dataset."
REFERENCES,0.63671875,"of Mutagenicity the best model uses only node pairs with distance 3 although we also considered
609"
REFERENCES,0.6376953125,"the hyperparameter d = 10. We also tested different small patterns, e.g., simple cycles, but they
610"
REFERENCES,0.638671875,"did not improve the results. For DHFR this was different as the best model uses the pattern (simple
611"
REFERENCES,0.6396484375,"cycles with length at most 10) for the output layer. We also tested the Weisfeiler-Leman rule in this
612"
REFERENCES,0.640625,"case but the validation accuracy was lower. For IMDB-BINARY and IMDB-MULTI the best model
613"
REFERENCES,0.6416015625,"uses the pattern (simple cycles with length at most 10, triangle, edge). Note that the embedding of
614"
REFERENCES,0.642578125,"one edge as a pattern is equivalent to the degree of the node. We also tested the Weisfeiler-Leman
615"
REFERENCES,0.6435546875,"rule but the validation accuracy was lower. All in all we considered many different rules from type
616"
REFERENCES,0.64453125,"Weisfeiler-Leman and patterns but of course we did not test all possible rules. A full list of tested
617"
REFERENCES,0.6455078125,"hyperparameters can be found here. As a next step it would be interesting to consider more rules,
618"
REFERENCES,0.646484375,"rules that come from expert knowledge or also deeper architectures with more rule based layers
619"
REFERENCES,0.6474609375,"concatenated. Regarding the number of learnable parameters we would like to mention that the
620"
REFERENCES,0.6484375,"number is relatively high but lots of parameters are not used in the weight matrix. Hence, it might be
621"
REFERENCES,0.6494140625,"possible to prune the set of learnable parameters by removing those that are not used or those that
622"
REFERENCES,0.650390625,"have a small absolute value.
623"
REFERENCES,0.6513671875,"For the synthetic datasets we use “expert knowledge” to define the rules. Hence we did not tested
624"
REFERENCES,0.65234375,"other rules than those in Table 6. For LongRings, EvenOddRings and EvenOddRingsCount we used
625"
REFERENCES,0.6533203125,"the original node labels for the rule based layers. Moreover, instead considering learnable parameters
626"
REFERENCES,0.654296875,"for all node pairs of certain labels with distance smaller or equal to d we considered only the node
627"
REFERENCES,0.6552734375,"pairs with distance d (denoted by “only: d”). In case of EvenOddRings we used two layers. The first
628"
REFERENCES,0.65625,"layer that considers only node pairs with distance 8 collects all the necessary information of opposite
629"
REFERENCES,0.6572265625,"nodes. The second layer that considers only node pairs with distance 4 collects the information of
630"
REFERENCES,0.658203125,"the nodes that are 4 hops away from the nodes with label 0, see also Figure 6. For CSL we used as
631"
REFERENCES,0.6591796875,"patterns all simple cycles with length at most 10. For the Snowflakes dataset we used the patterns
632"
REFERENCES,0.66015625,"cycle of length 4 and 5 and collect the information of the nodes that have pairwise distance 3. In this
633"
REFERENCES,0.6611328125,"way the RuleGNN is able to distinguish the graphs M0, M1, M2 and M3 that are not distinguishable
634"
REFERENCES,0.662109375,"by the 1-WL test. In the output layer we used the Weisfeiler-Leman rule with k = 2 iterations to
635"
REFERENCES,0.6630859375,"collect the relevant information from nodes with different Weisfeiler-Leman labels.
636"
REFERENCES,0.6640625,"A.8
RuleGNNs: Interpretability
637"
REFERENCES,0.6650390625,"One advantage of our approach is that each weight can be interpreted, i.e., we can see the relevance
638"
REFERENCES,0.666015625,"of two nodes i, j in a graph with labels l(i), l(j) and distance d(i, j). Figure 6 shows an example of
639"
REFERENCES,0.6669921875,"the learned parameters for some synthetic dataset. Figure 1 shows an example of the relevance of the
640"
REFERENCES,0.66796875,"weights for a graph from the DHFR dataset using the weights of the best model. Considering Figure 6b
641"
REFERENCES,0.6689453125,"we can see that in the first layer the RuleGNN passes the messages between opposite nodes as given
642"
REFERENCES,0.669921875,"by the rule. In the second layer it has learned to collect the information from the nodes that have
643"
REFERENCES,0.6708984375,"distance 4 to the node with label 0 (dark blue node) all other connections of distance 4 have a smaller
644"
REFERENCES,0.671875,"weight.
645"
REFERENCES,0.6728515625,"Dataset
Rules
Hyperparameter
#Learnable Parameters per Layer
k
d
L"
REFERENCES,0.673828125,"NCI1
wl
2
10
500
2 500 500
wl
2
-
50000 4 220"
REFERENCES,0.6748046875,"NCI109
wl
2
10
500
2 500 500
wl
2
-
50000
4 336"
REFERENCES,0.67578125,"Mutagenicity
wl
2
3
500
750 500
wl
2
-
50000 4 972"
REFERENCES,0.6767578125,"DHFR
wl
2
6
500
1 382 880
pattern: (simple cycles≤10)
-
-
-
112"
REFERENCES,0.677734375,"IMDB-BINARY
pattern: (triangle, edge)
-
2
-
963 966
pattern: (induced cycles≤5)
-
-
-
990"
REFERENCES,0.6787109375,"IMDB-MULTI
pattern: (triangle, edge)
-
2
-
551 775
pattern: (triangle, edge)
10
-
-
1 578"
REFERENCES,0.6796875,"LongRings
labels
-
only: 25
-
30
labels
-
-
-
18"
REFERENCES,0.6806640625,"EvenOddRings
labels
-
only: 8
-
272
labels
-
only: 4
-
272
labels
-
-
-
68"
REFERENCES,0.681640625,"EvenOddRingsCount
labels
-
only: 8
-
272
labels
-
-
-
34"
REFERENCES,0.6826171875,"CSL
pattern: (simple cycles≤10)
-
-
-
8930
pattern: (simple cycles≤10)
-
-
-
950"
REFERENCES,0.68359375,"Snowflakes
pattern: (cycle 4, cycle 5)
-
only: 3
-
90
wl
2
-
-
20"
REFERENCES,0.6845703125,Table 6: Overview over the hyperparameters of the best models.
REFERENCES,0.685546875,"(a) EvenOddCount
(b) EvenOddRings
(c) Snowflakes"
REFERENCES,0.6865234375,"Figure 6: Visualization of the learned weights and biases for the RuleGNN on the EvenOd-
dRingsCount (a), EvenOddRings (b) and Snowflakes (c) dataset. The first column shows the graphs
and the colors of the nodes represent the different node labels. The other columns show the learned
weights and biases of the RuleGNN for the respective rule based layer. The message passing weights
are visualized by arrows (thicker for higher absolute values) and the biases are visualized by the size
of the node (red for positive and blue for negative weights)."
REFERENCES,0.6875,"NeurIPS Paper Checklist
646"
CLAIMS,0.6884765625,"1. Claims
647"
CLAIMS,0.689453125,"Question: Do the main claims made in the abstract and introduction accurately reflect the
648"
CLAIMS,0.6904296875,"paper’s contributions and scope?
649"
CLAIMS,0.69140625,"Answer: [Yes]
650"
CLAIMS,0.6923828125,"Justification: The theoretical and experimentally claims made in the abstract are consistent
651"
CLAIMS,0.693359375,"with the results presented in the paper and reflect the contributions made.
652"
CLAIMS,0.6943359375,"Guidelines:
653"
CLAIMS,0.6953125,"• The answer NA means that the abstract and introduction do not include the claims
654"
CLAIMS,0.6962890625,"made in the paper.
655"
CLAIMS,0.697265625,"• The abstract and/or introduction should clearly state the claims made, including the
656"
CLAIMS,0.6982421875,"contributions made in the paper and important assumptions and limitations. A No or
657"
CLAIMS,0.69921875,"NA answer to this question will not be perceived well by the reviewers.
658"
CLAIMS,0.7001953125,"• The claims made should match theoretical and experimental results, and reflect how
659"
CLAIMS,0.701171875,"much the results can be expected to generalize to other settings.
660"
CLAIMS,0.7021484375,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
661"
CLAIMS,0.703125,"are not attained by the paper.
662"
LIMITATIONS,0.7041015625,"2. Limitations
663"
LIMITATIONS,0.705078125,"Question: Does the paper discuss the limitations of the work performed by the authors?
664"
LIMITATIONS,0.7060546875,"Answer: [Yes]
665"
LIMITATIONS,0.70703125,"Justification: In the conclusion and also in the experiments section we discuss the limitations
666"
LIMITATIONS,0.7080078125,"of the approach.
667"
LIMITATIONS,0.708984375,"Guidelines:
668"
LIMITATIONS,0.7099609375,"• The answer NA means that the paper has no limitation while the answer No means that
669"
LIMITATIONS,0.7109375,"the paper has limitations, but those are not discussed in the paper.
670"
LIMITATIONS,0.7119140625,"• The authors are encouraged to create a separate ”Limitations” section in their paper.
671"
LIMITATIONS,0.712890625,"• The paper should point out any strong assumptions and how robust the results are to
672"
LIMITATIONS,0.7138671875,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
673"
LIMITATIONS,0.71484375,"model well-specification, asymptotic approximations only holding locally). The authors
674"
LIMITATIONS,0.7158203125,"should reflect on how these assumptions might be violated in practice and what the
675"
LIMITATIONS,0.716796875,"implications would be.
676"
LIMITATIONS,0.7177734375,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
677"
LIMITATIONS,0.71875,"only tested on a few datasets or with a few runs. In general, empirical results often
678"
LIMITATIONS,0.7197265625,"depend on implicit assumptions, which should be articulated.
679"
LIMITATIONS,0.720703125,"• The authors should reflect on the factors that influence the performance of the approach.
680"
LIMITATIONS,0.7216796875,"For example, a facial recognition algorithm may perform poorly when image resolution
681"
LIMITATIONS,0.72265625,"is low or images are taken in low lighting. Or a speech-to-text system might not be
682"
LIMITATIONS,0.7236328125,"used reliably to provide closed captions for online lectures because it fails to handle
683"
LIMITATIONS,0.724609375,"technical jargon.
684"
LIMITATIONS,0.7255859375,"• The authors should discuss the computational efficiency of the proposed algorithms
685"
LIMITATIONS,0.7265625,"and how they scale with dataset size.
686"
LIMITATIONS,0.7275390625,"• If applicable, the authors should discuss possible limitations of their approach to
687"
LIMITATIONS,0.728515625,"address problems of privacy and fairness.
688"
LIMITATIONS,0.7294921875,"• While the authors might fear that complete honesty about limitations might be used by
689"
LIMITATIONS,0.73046875,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
690"
LIMITATIONS,0.7314453125,"limitations that aren’t acknowledged in the paper. The authors should use their best
691"
LIMITATIONS,0.732421875,"judgment and recognize that individual actions in favor of transparency play an impor-
692"
LIMITATIONS,0.7333984375,"tant role in developing norms that preserve the integrity of the community. Reviewers
693"
LIMITATIONS,0.734375,"will be specifically instructed to not penalize honesty concerning limitations.
694"
THEORY ASSUMPTIONS AND PROOFS,0.7353515625,"3. Theory Assumptions and Proofs
695"
THEORY ASSUMPTIONS AND PROOFS,0.736328125,"Question: For each theoretical result, does the paper provide the full set of assumptions and
696"
THEORY ASSUMPTIONS AND PROOFS,0.7373046875,"a complete (and correct) proof?
697"
THEORY ASSUMPTIONS AND PROOFS,0.73828125,"Answer: [Yes]
698"
THEORY ASSUMPTIONS AND PROOFS,0.7392578125,"Justification: For each of the theoretical results we provide a complete proof (in the appendix)
699"
THEORY ASSUMPTIONS AND PROOFS,0.740234375,"and a full set of assumptions.
700"
THEORY ASSUMPTIONS AND PROOFS,0.7412109375,"Guidelines:
701"
THEORY ASSUMPTIONS AND PROOFS,0.7421875,"• The answer NA means that the paper does not include theoretical results.
702"
THEORY ASSUMPTIONS AND PROOFS,0.7431640625,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
703"
THEORY ASSUMPTIONS AND PROOFS,0.744140625,"referenced.
704"
THEORY ASSUMPTIONS AND PROOFS,0.7451171875,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
705"
THEORY ASSUMPTIONS AND PROOFS,0.74609375,"• The proofs can either appear in the main paper or the supplemental material, but if
706"
THEORY ASSUMPTIONS AND PROOFS,0.7470703125,"they appear in the supplemental material, the authors are encouraged to provide a short
707"
THEORY ASSUMPTIONS AND PROOFS,0.748046875,"proof sketch to provide intuition.
708"
THEORY ASSUMPTIONS AND PROOFS,0.7490234375,"• Inversely, any informal proof provided in the core of the paper should be complemented
709"
THEORY ASSUMPTIONS AND PROOFS,0.75,"by formal proofs provided in appendix or supplemental material.
710"
THEORY ASSUMPTIONS AND PROOFS,0.7509765625,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
711"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.751953125,"4. Experimental Result Reproducibility
712"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7529296875,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
713"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75390625,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
714"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7548828125,"of the paper (regardless of whether the code and data are provided or not)?
715"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.755859375,"Answer: [Yes]
716"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7568359375,"Justification: We give all information that is needed to reproduce the experimental results
717"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7578125,"and also provide the code and data which is not online.
718"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7587890625,"Guidelines:
719"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.759765625,"• The answer NA means that the paper does not include experiments.
720"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607421875,"• If the paper includes experiments, a No answer to this question will not be perceived
721"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.76171875,"well by the reviewers: Making the paper reproducible is important, regardless of
722"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7626953125,"whether the code and data are provided or not.
723"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.763671875,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
724"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7646484375,"to make their results reproducible or verifiable.
725"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.765625,"• Depending on the contribution, reproducibility can be accomplished in various ways.
726"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7666015625,"For example, if the contribution is a novel architecture, describing the architecture fully
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.767578125,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7685546875,"be necessary to either make it possible for others to replicate the model with the same
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.76953125,"dataset, or provide access to the model. In general. releasing code and data is often
730"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7705078125,"one good way to accomplish this, but reproducibility can also be provided via detailed
731"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.771484375,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7724609375,"of a large language model), releasing of a model checkpoint, or other means that are
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7734375,"appropriate to the research performed.
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7744140625,"• While NeurIPS does not require releasing code, the conference does require all submis-
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.775390625,"sions to provide some reasonable avenue for reproducibility, which may depend on the
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7763671875,"nature of the contribution. For example
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.77734375,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7783203125,"to reproduce that algorithm.
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.779296875,"(b) If the contribution is primarily a new model architecture, the paper should describe
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7802734375,"the architecture clearly and fully.
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.78125,"(c) If the contribution is a new model (e.g., a large language model), then there should
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7822265625,"either be a way to access this model for reproducing the results or a way to reproduce
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.783203125,"the model (e.g., with an open-source dataset or instructions for how to construct
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7841796875,"the dataset).
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.78515625,"(d) We recognize that reproducibility may be tricky in some cases, in which case
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7861328125,"authors are welcome to describe the particular way they provide for reproducibility.
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.787109375,"In the case of closed-source models, it may be that access to the model is limited in
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7880859375,"some way (e.g., to registered users), but it should be possible for other researchers
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7890625,"to have some path to reproducing or verifying the results.
750"
OPEN ACCESS TO DATA AND CODE,0.7900390625,"5. Open access to data and code
751"
OPEN ACCESS TO DATA AND CODE,0.791015625,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
752"
OPEN ACCESS TO DATA AND CODE,0.7919921875,"tions to faithfully reproduce the main experimental results, as described in supplemental
753"
OPEN ACCESS TO DATA AND CODE,0.79296875,"material?
754"
OPEN ACCESS TO DATA AND CODE,0.7939453125,"Answer: [Yes]
755"
OPEN ACCESS TO DATA AND CODE,0.794921875,"Justification: We provide open access to the code, datasplits and synthetic datasets used in
756"
OPEN ACCESS TO DATA AND CODE,0.7958984375,"the paper.
757"
OPEN ACCESS TO DATA AND CODE,0.796875,"Guidelines:
758"
OPEN ACCESS TO DATA AND CODE,0.7978515625,"• The answer NA means that paper does not include experiments requiring code.
759"
OPEN ACCESS TO DATA AND CODE,0.798828125,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
760"
OPEN ACCESS TO DATA AND CODE,0.7998046875,"public/guides/CodeSubmissionPolicy) for more details.
761"
OPEN ACCESS TO DATA AND CODE,0.80078125,"• While we encourage the release of code and data, we understand that this might not be
762"
OPEN ACCESS TO DATA AND CODE,0.8017578125,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
763"
OPEN ACCESS TO DATA AND CODE,0.802734375,"including code, unless this is central to the contribution (e.g., for a new open-source
764"
OPEN ACCESS TO DATA AND CODE,0.8037109375,"benchmark).
765"
OPEN ACCESS TO DATA AND CODE,0.8046875,"• The instructions should contain the exact command and environment needed to run to
766"
OPEN ACCESS TO DATA AND CODE,0.8056640625,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
767"
OPEN ACCESS TO DATA AND CODE,0.806640625,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
768"
OPEN ACCESS TO DATA AND CODE,0.8076171875,"• The authors should provide instructions on data access and preparation, including how
769"
OPEN ACCESS TO DATA AND CODE,0.80859375,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
770"
OPEN ACCESS TO DATA AND CODE,0.8095703125,"• The authors should provide scripts to reproduce all experimental results for the new
771"
OPEN ACCESS TO DATA AND CODE,0.810546875,"proposed method and baselines. If only a subset of experiments are reproducible, they
772"
OPEN ACCESS TO DATA AND CODE,0.8115234375,"should state which ones are omitted from the script and why.
773"
OPEN ACCESS TO DATA AND CODE,0.8125,"• At submission time, to preserve anonymity, the authors should release anonymized
774"
OPEN ACCESS TO DATA AND CODE,0.8134765625,"versions (if applicable).
775"
OPEN ACCESS TO DATA AND CODE,0.814453125,"• Providing as much information as possible in supplemental material (appended to the
776"
OPEN ACCESS TO DATA AND CODE,0.8154296875,"paper) is recommended, but including URLs to data and code is permitted.
777"
OPEN ACCESS TO DATA AND CODE,0.81640625,"6. Experimental Setting/Details
778"
OPEN ACCESS TO DATA AND CODE,0.8173828125,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
779"
OPEN ACCESS TO DATA AND CODE,0.818359375,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
780"
OPEN ACCESS TO DATA AND CODE,0.8193359375,"results?
781"
OPEN ACCESS TO DATA AND CODE,0.8203125,"Answer: [Yes]
782"
OPEN ACCESS TO DATA AND CODE,0.8212890625,"Justification: We provide all data splits and hyperparameter choices for our algorithm in
783"
OPEN ACCESS TO DATA AND CODE,0.822265625,"the paper. Moreover, in the code we have understandable config files that contain all the
784"
OPEN ACCESS TO DATA AND CODE,0.8232421875,"hyperparameters used in the experiments.
785"
OPEN ACCESS TO DATA AND CODE,0.82421875,"Guidelines:
786"
OPEN ACCESS TO DATA AND CODE,0.8251953125,"• The answer NA means that the paper does not include experiments.
787"
OPEN ACCESS TO DATA AND CODE,0.826171875,"• The experimental setting should be presented in the core of the paper to a level of detail
788"
OPEN ACCESS TO DATA AND CODE,0.8271484375,"that is necessary to appreciate the results and make sense of them.
789"
OPEN ACCESS TO DATA AND CODE,0.828125,"• The full details can be provided either with the code, in appendix, or as supplemental
790"
OPEN ACCESS TO DATA AND CODE,0.8291015625,"material.
791"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.830078125,"7. Experiment Statistical Significance
792"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8310546875,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
793"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.83203125,"information about the statistical significance of the experiments?
794"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8330078125,"Answer: [Yes]
795"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.833984375,"Justification: We use standard deviation to show the variability of the results. We do
796"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8349609375,"not use statistical significance tests because our main claim is not that our method is
797"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8359375,"significantly better than state-of-the-art methods, but that it is an interesting new approach
798"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8369140625,"that is applicable to a wide range of tasks.
799"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.837890625,"Guidelines:
800"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8388671875,"• The answer NA means that the paper does not include experiments.
801"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.83984375,"• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
802"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8408203125,"dence intervals, or statistical significance tests, at least for the experiments that support
803"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.841796875,"the main claims of the paper.
804"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8427734375,"• The factors of variability that the error bars are capturing should be clearly stated (for
805"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.84375,"example, train/test split, initialization, random drawing of some parameter, or overall
806"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8447265625,"run with given experimental conditions).
807"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.845703125,"• The method for calculating the error bars should be explained (closed form formula,
808"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8466796875,"call to a library function, bootstrap, etc.)
809"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.84765625,"• The assumptions made should be given (e.g., Normally distributed errors).
810"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8486328125,"• It should be clear whether the error bar is the standard deviation or the standard error
811"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.849609375,"of the mean.
812"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8505859375,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
813"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8515625,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
814"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8525390625,"of Normality of errors is not verified.
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.853515625,"• For asymmetric distributions, the authors should be careful not to show in tables or
816"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8544921875,"figures symmetric error bars that would yield results that are out of range (e.g. negative
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.85546875,"error rates).
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8564453125,"• If error bars are reported in tables or plots, The authors should explain in the text how
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.857421875,"they were calculated and reference the corresponding figures or tables in the text.
820"
EXPERIMENTS COMPUTE RESOURCES,0.8583984375,"8. Experiments Compute Resources
821"
EXPERIMENTS COMPUTE RESOURCES,0.859375,"Question: For each experiment, does the paper provide sufficient information on the com-
822"
EXPERIMENTS COMPUTE RESOURCES,0.8603515625,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
823"
EXPERIMENTS COMPUTE RESOURCES,0.861328125,"the experiments?
824"
EXPERIMENTS COMPUTE RESOURCES,0.8623046875,"Answer: [Yes]
825"
EXPERIMENTS COMPUTE RESOURCES,0.86328125,"Justification: We specify the computer resources and the time needed to run the experiments
826"
EXPERIMENTS COMPUTE RESOURCES,0.8642578125,"in the paper.
827"
EXPERIMENTS COMPUTE RESOURCES,0.865234375,"Guidelines:
828"
EXPERIMENTS COMPUTE RESOURCES,0.8662109375,"• The answer NA means that the paper does not include experiments.
829"
EXPERIMENTS COMPUTE RESOURCES,0.8671875,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
830"
EXPERIMENTS COMPUTE RESOURCES,0.8681640625,"or cloud provider, including relevant memory and storage.
831"
EXPERIMENTS COMPUTE RESOURCES,0.869140625,"• The paper should provide the amount of compute required for each of the individual
832"
EXPERIMENTS COMPUTE RESOURCES,0.8701171875,"experimental runs as well as estimate the total compute.
833"
EXPERIMENTS COMPUTE RESOURCES,0.87109375,"• The paper should disclose whether the full research project required more compute
834"
EXPERIMENTS COMPUTE RESOURCES,0.8720703125,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
835"
EXPERIMENTS COMPUTE RESOURCES,0.873046875,"didn’t make it into the paper).
836"
CODE OF ETHICS,0.8740234375,"9. Code Of Ethics
837"
CODE OF ETHICS,0.875,"Question: Does the research conducted in the paper conform, in every respect, with the
838"
CODE OF ETHICS,0.8759765625,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
839"
CODE OF ETHICS,0.876953125,"Answer: [Yes]
840"
CODE OF ETHICS,0.8779296875,"Justification: The research conducted in the paper conforms with the NeurIPS Code of
841"
CODE OF ETHICS,0.87890625,"Ethics.
842"
CODE OF ETHICS,0.8798828125,"Guidelines:
843"
CODE OF ETHICS,0.880859375,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
844"
CODE OF ETHICS,0.8818359375,"• If the authors answer No, they should explain the special circumstances that require a
845"
CODE OF ETHICS,0.8828125,"deviation from the Code of Ethics.
846"
CODE OF ETHICS,0.8837890625,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
847"
CODE OF ETHICS,0.884765625,"eration due to laws or regulations in their jurisdiction).
848"
BROADER IMPACTS,0.8857421875,"10. Broader Impacts
849"
BROADER IMPACTS,0.88671875,"Question: Does the paper discuss both potential positive societal impacts and negative
850"
BROADER IMPACTS,0.8876953125,"societal impacts of the work performed?
851"
BROADER IMPACTS,0.888671875,"Answer: [NA]
852"
BROADER IMPACTS,0.8896484375,"Justification: The paper does not address societal impact because we present a very basic
853"
BROADER IMPACTS,0.890625,"approach that is not directly applicable to any specific societal problem.
854"
BROADER IMPACTS,0.8916015625,"Guidelines:
855"
BROADER IMPACTS,0.892578125,"• The answer NA means that there is no societal impact of the work performed.
856"
BROADER IMPACTS,0.8935546875,"• If the authors answer NA or No, they should explain why their work has no societal
857"
BROADER IMPACTS,0.89453125,"impact or why the paper does not address societal impact.
858"
BROADER IMPACTS,0.8955078125,"• Examples of negative societal impacts include potential malicious or unintended uses
859"
BROADER IMPACTS,0.896484375,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
860"
BROADER IMPACTS,0.8974609375,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
861"
BROADER IMPACTS,0.8984375,"groups), privacy considerations, and security considerations.
862"
BROADER IMPACTS,0.8994140625,"• The conference expects that many papers will be foundational research and not tied
863"
BROADER IMPACTS,0.900390625,"to particular applications, let alone deployments. However, if there is a direct path to
864"
BROADER IMPACTS,0.9013671875,"any negative applications, the authors should point it out. For example, it is legitimate
865"
BROADER IMPACTS,0.90234375,"to point out that an improvement in the quality of generative models could be used to
866"
BROADER IMPACTS,0.9033203125,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
867"
BROADER IMPACTS,0.904296875,"that a generic algorithm for optimizing neural networks could enable people to train
868"
BROADER IMPACTS,0.9052734375,"models that generate Deepfakes faster.
869"
BROADER IMPACTS,0.90625,"• The authors should consider possible harms that could arise when the technology is
870"
BROADER IMPACTS,0.9072265625,"being used as intended and functioning correctly, harms that could arise when the
871"
BROADER IMPACTS,0.908203125,"technology is being used as intended but gives incorrect results, and harms following
872"
BROADER IMPACTS,0.9091796875,"from (intentional or unintentional) misuse of the technology.
873"
BROADER IMPACTS,0.91015625,"• If there are negative societal impacts, the authors could also discuss possible mitigation
874"
BROADER IMPACTS,0.9111328125,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
875"
BROADER IMPACTS,0.912109375,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
876"
BROADER IMPACTS,0.9130859375,"feedback over time, improving the efficiency and accessibility of ML).
877"
SAFEGUARDS,0.9140625,"11. Safeguards
878"
SAFEGUARDS,0.9150390625,"Question: Does the paper describe safeguards that have been put in place for responsible
879"
SAFEGUARDS,0.916015625,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
880"
SAFEGUARDS,0.9169921875,"image generators, or scraped datasets)?
881"
SAFEGUARDS,0.91796875,"Answer: [NA]
882"
SAFEGUARDS,0.9189453125,"Justification: We do not use scraped datasets or models that have a high risk for misuse.
883"
SAFEGUARDS,0.919921875,"Guidelines:
884"
SAFEGUARDS,0.9208984375,"• The answer NA means that the paper poses no such risks.
885"
SAFEGUARDS,0.921875,"• Released models that have a high risk for misuse or dual-use should be released with
886"
SAFEGUARDS,0.9228515625,"necessary safeguards to allow for controlled use of the model, for example by requiring
887"
SAFEGUARDS,0.923828125,"that users adhere to usage guidelines or restrictions to access the model or implementing
888"
SAFEGUARDS,0.9248046875,"safety filters.
889"
SAFEGUARDS,0.92578125,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
890"
SAFEGUARDS,0.9267578125,"should describe how they avoided releasing unsafe images.
891"
SAFEGUARDS,0.927734375,"• We recognize that providing effective safeguards is challenging, and many papers do
892"
SAFEGUARDS,0.9287109375,"not require this, but we encourage authors to take this into account and make a best
893"
SAFEGUARDS,0.9296875,"faith effort.
894"
LICENSES FOR EXISTING ASSETS,0.9306640625,"12. Licenses for existing assets
895"
LICENSES FOR EXISTING ASSETS,0.931640625,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
896"
LICENSES FOR EXISTING ASSETS,0.9326171875,"the paper, properly credited and are the license and terms of use explicitly mentioned and
897"
LICENSES FOR EXISTING ASSETS,0.93359375,"properly respected?
898"
LICENSES FOR EXISTING ASSETS,0.9345703125,"Answer: [Yes]
899"
LICENSES FOR EXISTING ASSETS,0.935546875,"Justification: We mention all creators and original owners of code and data we use in the
900"
LICENSES FOR EXISTING ASSETS,0.9365234375,"paper.
901"
LICENSES FOR EXISTING ASSETS,0.9375,"Guidelines:
902"
LICENSES FOR EXISTING ASSETS,0.9384765625,"• The answer NA means that the paper does not use existing assets.
903"
LICENSES FOR EXISTING ASSETS,0.939453125,"• The authors should cite the original paper that produced the code package or dataset.
904"
LICENSES FOR EXISTING ASSETS,0.9404296875,"• The authors should state which version of the asset is used and, if possible, include a
905"
LICENSES FOR EXISTING ASSETS,0.94140625,"URL.
906"
LICENSES FOR EXISTING ASSETS,0.9423828125,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
907"
LICENSES FOR EXISTING ASSETS,0.943359375,"• For scraped data from a particular source (e.g., website), the copyright and terms of
908"
LICENSES FOR EXISTING ASSETS,0.9443359375,"service of that source should be provided.
909"
LICENSES FOR EXISTING ASSETS,0.9453125,"• If assets are released, the license, copyright information, and terms of use in the
910"
LICENSES FOR EXISTING ASSETS,0.9462890625,"package should be provided. For popular datasets, paperswithcode.com/datasets
911"
LICENSES FOR EXISTING ASSETS,0.947265625,"has curated licenses for some datasets. Their licensing guide can help determine the
912"
LICENSES FOR EXISTING ASSETS,0.9482421875,"license of a dataset.
913"
LICENSES FOR EXISTING ASSETS,0.94921875,"• For existing datasets that are re-packaged, both the original license and the license of
914"
LICENSES FOR EXISTING ASSETS,0.9501953125,"the derived asset (if it has changed) should be provided.
915"
LICENSES FOR EXISTING ASSETS,0.951171875,"• If this information is not available online, the authors are encouraged to reach out to
916"
LICENSES FOR EXISTING ASSETS,0.9521484375,"the asset’s creators.
917"
NEW ASSETS,0.953125,"13. New Assets
918"
NEW ASSETS,0.9541015625,"Question: Are new assets introduced in the paper well documented and is the documentation
919"
NEW ASSETS,0.955078125,"provided alongside the assets?
920"
NEW ASSETS,0.9560546875,"Answer: [NA]
921"
NEW ASSETS,0.95703125,"Justification: The paper does not introduce new assets.
922"
NEW ASSETS,0.9580078125,"Guidelines:
923"
NEW ASSETS,0.958984375,"• The answer NA means that the paper does not release new assets.
924"
NEW ASSETS,0.9599609375,"• Researchers should communicate the details of the dataset/code/model as part of their
925"
NEW ASSETS,0.9609375,"submissions via structured templates. This includes details about training, license,
926"
NEW ASSETS,0.9619140625,"limitations, etc.
927"
NEW ASSETS,0.962890625,"• The paper should discuss whether and how consent was obtained from people whose
928"
NEW ASSETS,0.9638671875,"asset is used.
929"
NEW ASSETS,0.96484375,"• At submission time, remember to anonymize your assets (if applicable). You can either
930"
NEW ASSETS,0.9658203125,"create an anonymized URL or include an anonymized zip file.
931"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.966796875,"14. Crowdsourcing and Research with Human Subjects
932"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9677734375,"Question: For crowdsourcing experiments and research with human subjects, does the paper
933"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.96875,"include the full text of instructions given to participants and screenshots, if applicable, as
934"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9697265625,"well as details about compensation (if any)?
935"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970703125,"Answer: [NA]
936"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716796875,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
937"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97265625,"Guidelines:
938"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9736328125,"• The answer NA means that the paper does not involve crowdsourcing nor research with
939"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974609375,"human subjects.
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9755859375,"• Including this information in the supplemental material is fine, but if the main contribu-
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9765625,"tion of the paper involves human subjects, then as much detail as possible should be
942"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775390625,"included in the main paper.
943"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978515625,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
944"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794921875,"or other labor should be paid at least the minimum wage in the country of the data
945"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98046875,"collector.
946"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814453125,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
947"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982421875,"Subjects
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833984375,"Question: Does the paper describe potential risks incurred by study participants, whether
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984375,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853515625,"approvals (or an equivalent approval/review based on the requirements of your country or
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986328125,"institution) were obtained?
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873046875,"Answer: [NA]
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98828125,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892578125,"Guidelines:
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990234375,"• The answer NA means that the paper does not involve crowdsourcing nor research with
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912109375,"human subjects.
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921875,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931640625,"may be required for any human subjects research. If you obtained IRB approval, you
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994140625,"should clearly state this in the paper.
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951171875,"• We recognize that the procedures for this may vary significantly between institutions
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99609375,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970703125,"guidelines for their institution.
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998046875,"• For initial submissions, do not include any information that would break anonymity (if
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990234375,"applicable), such as the institution conducting the review.
965"
