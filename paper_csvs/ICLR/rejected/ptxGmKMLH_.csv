Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0026666666666666666,"The prototypical network is a prototype classifier based on meta-learning and
is widely used for few-shot learning because it classifies unseen examples by
constructing class-specific prototypes without adjusting hyper-parameters during
meta-testing. Interestingly, recent research has attracted a lot of attention, showing
that training a new linear classifier, which does not use a meta-learning algorithm,
performs comparably with the prototypical network. However, the training of a
new linear classifier requires the retraining of the classifier every time a new class
appears. In this paper, we analyze how a prototype classifier works equally well
without training a new linear classifier and meta-learning. We experimentally find
that directly using the feature vector extracted using standard pre-trained models
to construct a prototype classifier in meta-testing does not perform as well as the
prototypical network and training a new linear classifiers and feature vectors of
pre-trained models. Thus, we derive a novel generalization bound for the proto-
typical network and show that focusing on the variance of the norm of a feature
vector can improve performance. We experimentally investigate several normal-
ization methods for minimizing the variance of the norm and find that the same
performance can be obtained by using the L2 normalization and embedding space
transformation without training a new classifier or meta-learning."
INTRODUCTION,0.005333333333333333,"1
INTRODUCTION"
INTRODUCTION,0.008,"Few-shot learning is used to adapt quickly to new classes with low annotation cost. Meta-learning
is a standard training procedure to tackle the few-shot learning problem and Prototypical Network
(Snell et al., 2017) a.k.a ProtoNet is a widely used meta-learning algorithm for few-shot learning.
In ProtoNet, we use a prototype classifier based on meta-learning to predict the classes of unob-
served objects by constructing class-specific prototypes without adjusting the hyper-parameters dur-
ing meta-testing."
INTRODUCTION,0.010666666666666666,"ProtoNet has the following advantages. (1) Since the nearest neighbor method is applied on query
data and class prototypes during the meta-test phase, no hyper-parameters are required in meta-test
phase. (2) Since the number of data for few-shot is small, the inference time is almost negligible.
(3) The classifiers can quickly adapt to new environments because they do not have to be re-trained
for the support set when new classes appear. The generalization bound of ProtoNet in relation to
the number of shots in a support set has been studied (Cao et al., 2020). The bound suggests that
the performance of ProtoNet depends on the ratio of the between-class variance to the within-class
variance of features of a support set extracted using the meta-trained model."
INTRODUCTION,0.013333333333333334,"There have been studies on training a new linear classifier on the features extracted using a pre-
trained model without meta-learning, which can perform comparably with the meta-learned models
(Chen et al., 2019; Tian et al., 2020). We call this approach as linear-evaluation-based approach. In
these studies, the models are trained with the standard classification problem, i.e., models are trained
with cross-entropy loss after linear projection from the embedding space to the class-probability
space. The linear-evaluation-based approach has the following advantages over meta-learning. (1)
Training converges faster than meta-learning. (2) Implementation is simpler. (3) Meta-learning
decreases in performance if the number of shots does not match between meta-training and meta-
testing (Cao et al., 2020); however, the linear-evaluation-based approach do not need to take this"
INTRODUCTION,0.016,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.018666666666666668,"(a) ProtoNet
(b) cross-entropy loss with linear projection layer"
INTRODUCTION,0.021333333333333333,"Figure 1: Distribution of features extracted using a neural network with two dimensional final layer trained on
CIFAR-10 with (a): ProtoNet loss (b): cross-entropy loss with linear projection layer. The ProtoNet features
distribute closer to its class center than the features extracted using the model trained on cross-entropy loss with
linear projection layer."
INTRODUCTION,0.024,"into account. However, the linear-evaluation-based approach requires retraining a linear classifier
every time a new class appears."
INTRODUCTION,0.02666666666666667,"In contrast, a prototype classifier can be applied to any trained feature extractor and does not require
model learning in the testing phase. Therefore, a prototype classifier can be a a practical and useful
first step for few-shot learning problems. In order to avoid meta-learning during the training phase
and the linear evaluation during the testing phase, we focus on using a prototype classifier on the
testing phase and training models in a standard classification manner. As we discuss in section 4,
we found that when we directly constructed prototypes from the feature vectors extracted using pre-
trained models and applied the nearest neighbor method as in the testing phase in ProtoNet , this
does not perform as well as the linear-evaluation-based approach."
INTRODUCTION,0.029333333333333333,"We hypothesize that the reason is the difference between the loss function in ProtoNet and pre-
trained models. As described in section 3, if we consider a prototype as a pseudo sample average
of the features in each class, the loss function of ProtoNet can be considered having a regularizing
effect that makes it closer to the sample average of the class. Since standard classification training
computes cross-entropy loss with additional linear projection to make the features linearly separa-
ble, the loss function does not have such an effect and can cause large within-class variance. Figure
1 shows a scatter plot of the features extracted using a neural network with two dimension output
trained on cifar-10 with ProtoNet(1a) and cross-entropy loss with a linear projection layer(1b). This
figure implies that the features extracted using a model trained in a standard classification manner
distributes away from the origin and causes large within-class variance along the direction of the
norm of class mean vectors, while that of ProtoNet is more centered to its class means. This phe-
nomenon is also observed in face recognition literatures (Wen et al., 2016; Liu et al., 2017; Wang
et al., 2018; Deng et al., 2019)."
INTRODUCTION,0.032,"We now focus on the theoretical analysis on a prototype classifier. A current study (Cao et al.,
2020) analyzed an upper bound of risk by using a prototype classifier. The bound depends on the
number of shots of a support set, between-class variance, and within-class variance. However, the
bound requires the class-conditioned distribution of features to be Gaussian and to have the same
covariance matrix among classes. In addition, since the bound does not depend on the norm of
the feature vectors, it is not clear from the bound what feature-transformation method can lead to
performance improvement. Thus, we derive a novel bound for a prototype classifier."
INTRODUCTION,0.034666666666666665,Our contributions are threefold.
INTRODUCTION,0.037333333333333336,"1. We relax the assumption; specifically, the bound does not require that the features distribute
on any specific distribution, and each covariance matrix does not have to be the same among
classes."
WE CLARIFY THE EFFECT OF THE VARIANCE OF THE NORM OF THE FEATURE VECTORS ON THE PERFORMANCE,0.04,"2. We clarify the effect of the variance of the norm of the feature vectors on the performance
of a prototype classifier."
WE CLARIFY THE EFFECT OF THE VARIANCE OF THE NORM OF THE FEATURE VECTORS ON THE PERFORMANCE,0.042666666666666665,3. We investigate the effectiveness of reducing the variance of the norm empirically.
WE CLARIFY THE EFFECT OF THE VARIANCE OF THE NORM OF THE FEATURE VECTORS ON THE PERFORMANCE,0.04533333333333334,Under review as a conference paper at ICLR 2022
RELATED WORK,0.048,"2
RELATED WORK"
RELATED WORK,0.050666666666666665,"We summarize related work by describing a prototype classifier with meta-learning, linear-
evaluation-based approach without meta-learning, and theoretical analysis related to the few-shot
learning problem."
RELATED WORK,0.05333333333333334,"A prototype classifier with meta-learning
On the basis of the hypothesis that features well distin-
guished in the training phase are also useful for classifying new classes, constructing one or multiple
prototypes for classifying unseen examples is a widely used approach (Vinyals et al., 2016; Snell
et al., 2017; Pahde et al., 2021; Ji et al., 2021; Sung et al., 2018; Allen et al., 2019; Doersch et al.,
2020; Qi et al., 2018). Certain algorithms compute similarities between multiple prototypes and
unseen examples by using their own modules, such as attention mechanism (Vinyals et al., 2016;
Doersch et al., 2020), relation network (Sung et al., 2018), reweighting mechanisms by taking into
account between-class and within-class interaction (Ji et al., 2021), and latent clusters (Allen et al.,
2019). Prototypes are also constructed in a multi-modal way (Pahde et al., 2021). Another line
of research is transforming the space of extracted features to a better distinguishable space (Simon
et al., 2020; Yoon et al., 2019; Das et al., 2020; Das & Lee, 2020) or taking variance of fetures into
account (Bateni et al., 2020) . Because of its convenience, the approach with a prototype classifier
is adapted to other domain such as semantic segmentation (Dong & Xing, 2018), text classification
(Sun et al., 2019), and speech recognition (Wang et al., 2019a)."
RELATED WORK,0.056,"Update-based meta-learning
In contrast to the approach with a prototype classifier and meta-
learning , in update-based meta-learning approaches, model parameters are adjusted in the test phase
so that a model can adapt to new classes. Model-agnostic meta-Learning (MAML) and its variants
(Finn et al., 2017; 2018; Rajeswaran et al., 2017) search for a good initialization parameters that
adapt to new classes with a few labeled data and a few update steps of the parameters. Another ap-
proach involves learning an effective update rule for the parameters of a base-learner model through
a sequence of training episodes (Bertinetto et al., 2019; Lee et al., 2019). Both approaches require
additional learning of hyper-parameters and training time; thus, they prevent quick adaptation to
new classes."
RELATED WORK,0.058666666666666666,"linear-evaluation-based approach without meta-learning
Interestingly, recent studies have
shown that training a new linear classifier with features extracted using a model trained with cross-
entropy loss on base-dataset performs comparably with meta-learning based methods (Chen et al.,
2019) (Wang et al., 2019b). More effective method for training a new classifier in few-shot settings
has been proposed (Yang et al., 2021; Phoo & Hariharan, 2021), such as calibrating distribution
generated by a support set (Yang et al., 2021) , self-supervised learning on query data (Phoo & Har-
iharan, 2021), and distilling knowledge to obtain better embeddings (Tian et al., 2020). Liu et al.
(2020) focuses on pretraining phase and found the negative margin in cross-entropy helps improving
the performance in few-shot settings. However, similar to update-based meta-learning, these studies
require additional hyper-parameters and training to apply to new classes; thus if we tackle these
points, we would have an alternative method that is easier and more convenient to use."
RELATED WORK,0.06133333333333333,"Theoretical analysis of few-shot learning
Even though much improvement has been empirically
made in few-shot learning, theoretical analysis is scarce. In the context of meta-learning, Du et al.
(2021) provided a risk bound on the meta-testing phase that is related to the number of meta-training
data and meta-testing data. Lucas et al. (2021) derived information-theoretic lower-bounds on mini-
max rates of convergence for algorithms that are trained on data from multiple sources and tested on
novel data. Cao et al. (2020) derived a new bound on a prototype classifier and theoretically demon-
strated that the mismatch regarding the number of shots in a support set between meta-training and
meta-testing degrades the performance of prototypical networks, which has been only experimen-
tally observed. However, their bound depends on several assumptions: the class-conditional distri-
butions of features are Gaussian and have the same covariance matrix among classes. In contrast,
we derive a novel bound that does not depend on any specific distribution."
RELATED WORK,0.064,Under review as a conference paper at ICLR 2022
"THEORETICAL ANALYSIS OF PROTOTYPE CLASSIFIER IN TERMS OF
VARIANCE OF NORM OF FEATURE VECTORS",0.06666666666666667,"3
THEORETICAL ANALYSIS OF PROTOTYPE CLASSIFIER IN TERMS OF
VARIANCE OF NORM OF FEATURE VECTORS"
"THEORETICAL ANALYSIS OF PROTOTYPE CLASSIFIER IN TERMS OF
VARIANCE OF NORM OF FEATURE VECTORS",0.06933333333333333,"In this section, we first formulate our problem settings and point out the drawbacks of the current
theoretical analysis on a prototype classifier. Next we provide our novel bound of a prototype classi-
fier with the bound related to the variance of the norm of the feature vectors. Finally, we list several
methods that can improve the performance of a prototype classifier based on our bound."
PROBLEM SETTING,0.072,"3.1
PROBLEM SETTING"
PROBLEM SETTING,0.07466666666666667,"Let Y be a space of a class, τ a probability distribution over Y, X a space of input data, D a
probability distribution over X, Dy a probability distribution over X given a class y. We define
D⊗nk and D⊗n
y
by D⊗n
y
= Πn
i=1Dy and D⊗nk ≡Πn
i=1D⊗k
i
, respectively. We sample N classes
from τ to form the N-way classification problem. Denote by K a number of annotated data in each
class and x ∈X, y ∈Y as input data and its class respectively. We define a set of support data
of class c sampled from τ as Sc = {xi | (xi, yi) ∈X × Y ∩yi = c}K
i=1 and a set of support
data in the N-way K-shot classification problem as S = SN
c=1 Sc. Suppose a feature extractor
computes a function ϕ : X →RD, where D is the number of the embedding dimensions. ϕ(Sc)
is defined by ϕ(Sc) =
1
K
P"
PROBLEM SETTING,0.07733333333333334,"x∈Sc ϕ(x). Let Φ be a space of the extractor function ϕ. Denote by
M : Φ × X × (X × Y)NK →RN a prototype classifier function that computes probabilities of
input x belonging to class c as follows."
PROBLEM SETTING,0.08,"M(ϕ, x, S)c = pM(y = c|x, S, ϕ) =
exp

−∥ϕ(x) −ϕ(Sc)∥2"
PROBLEM SETTING,0.08266666666666667,"PN
l=1 exp

−∥ϕ(x) −ϕ(Sl)∥2
,
(1)"
PROBLEM SETTING,0.08533333333333333,"where ∥v∥2 = PD
d=1
 
v(d)2, and v(d) is the d-th dimension of vector v. The prediction of an input
x, denoted by ˆy ∈Y , is computed by taking argmax for M(ϕ, x, S), i.e., ˆy = argmaxM(ϕ, x, S).
We denote by Ez∼q(z)[g(z)] an operation to take the expectation of g(z) over z distributed as f(z)
and we simply denote Ez∼q(z)[g(z)] as Ez[g(z)] when z is obviously distributed on q(z). We define
Varz∼q(z)[g(z)] as an operation to take variance of g(z) over z distributed as q(z). With I denoting
the indicator function, we define expected risk RM of a prototype classifier as"
PROBLEM SETTING,0.088,"RM(ϕ) = ES∼D⊗nkEc∼τEx∼Dc[I[argmaxM(ϕ, x, S) ̸= c].
(2)"
PROBLEM SETTING,0.09066666666666667,"For simplicity, we now discuss the binary classification setting. We show a case of multi-class
classification in Appendix A.5 due to lack of space."
PROBLEM SETTING,0.09333333333333334,"Let c1 and c2 denote any pair of classes sampled from τ. We consider that a query data x belongs to
class c1 and support sets S consist of class c1’s support set and c2’s support set. Then, equation 2 is
written as follows."
PROBLEM SETTING,0.096,"RM(ϕ) = ES∼D⊗2kEc1∼τEx∼Dc1[I[argmaxM(ϕ, x, Sc1 ∪Sc2) ̸= c1]].
(3)"
PROBLEM SETTING,0.09866666666666667,"3.2
WHAT FEATURE-TRANSFORMATION METHOD EXPECTED TO BE EFFECTIVE?"
PROBLEM SETTING,0.10133333333333333,"The current theoretical analysis for a prototype classifier (Cao et al., 2020) has the following two
drawbacks (see Appendix A.1 for the details). The first is that the modeling assumption requires a
class-conditioned distribution of the features to follow a Gaussian distribution with the same covari-
ance matrix among classes. For example, when we use the ReLU activation function in last layer, it
is not normally distributed and the class-conditioned distribution does not have the same covariance
matrix as shown in Figure 1. The second drawback is that it is not clear what feature-transformation
method can reduce the upper bound. A feature-transformation method to maximize the between-
class variance and minimize the within-class variance such as linear discriminant analysis (LDA)
(Fukunaga, 1990) and Embedding Space Transformation (EST) (Cao et al., 2020) can be expected
to improve performance; however, it is not clear how the second term of the denominator changes."
PROBLEM SETTING,0.104,"From Figure 1 , the distribution of each class feature stretches in the direction of the norm of its
class-mean feature vector. This property is also observed in metric learning literatures (Wen et al.,"
PROBLEM SETTING,0.10666666666666667,Under review as a conference paper at ICLR 2022
PROBLEM SETTING,0.10933333333333334,"2016; Liu et al., 2017). A model trained in the cross-entropy loss after linear projection from the
embedding space to the class-probability space computes a probability of input x belonging to class
c given by"
PROBLEM SETTING,0.112,"p(y = c|x, W, ϕ) =
exp
 
ϕ(x)⊤Wc
"
PROBLEM SETTING,0.11466666666666667,"PN
j=1 exp (ϕ(x)⊤Wj)
,
(4)"
PROBLEM SETTING,0.11733333333333333,"where W is a weight matrix that transforms features from the embedding space to the class-
probability space."
PROBLEM SETTING,0.12,"Comparing equation 4 with equation 1, we found that equation 4 does not have a regularization
term similar to the one appearing in equation 1 that forces the features to be close to its class-mean
feature vector. This implies the features extracted using a model trained with the cross-entropy loss
on equation 4 are less close to its class-mean feature vector than the ProtoNet loss on equation 1.
Through this observation and the property mentioned above, we hypothesis that normalizing the
norm of the feature vectors can push the features to each class-mean feature vector and can boost
the performance of a prototype classifier trained with cross-entropy loss on equation 4."
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.12266666666666666,"3.3
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.12533333333333332,"To understand what effect the variance of the norm of the feature vectors has on the performance of
a prototype classifier, we analyze how variance contributes to the expected risk when an embedding
function ϕ is fixed. The following theorem provides a generalization bound for the expected risk of
a prototype classifier in terms of the variance of the norm of the feature vectors computed using a
feature extractor.
Theorem 1. Let M be an operation of a prototype classifier on binary classification defined by
equation 1. For µc = Ex∼Dc[ϕ(x)], Σc = Ex∼Dc[(ϕ(x) −µc)(ϕ(x) −µc)⊤], µ = Ec∼τ[µc],Σ =
Ec∼τ[(µc −µ)(µc −µ)⊤], and Ec∼τ[Σc] = Στ, if ϕ(x) has the variance of its norm, then the
miss-classification risk of the prototype classifier on binary classification RM satisfies"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.128,"RM(ϕ) ≤1 −
4(Tr(Σ))2"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.13066666666666665,"EV[hL2(ϕ(x))] + VTr(Σc1) + Vwit(Στ, Σ, µ) + E dist2
L2(µc1, µc2),
(5) where"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.13333333333333333,EV[hL2(ϕ(x))] = 4
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.136,"K Ec∼τ
h
Varx∼Dc
h
∥ϕ(x)∥2ii
,
(6)"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.13866666666666666,"VTr(Σci) =
 4 K + 2 K2"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.14133333333333334,"
Varc∼τ [Tr (Σci)] ,
(7)"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.144,"Vwit(Στ, Σ, µ) = 8"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.14666666666666667,"K Tr(Στ)
 
Tr(Στ) + Tr(Σ) + µ⊤µ

+ 4(Tr(Σ) + µ⊤µ)2,
(8)"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.14933333333333335,"E dist2
L2(µc1, µc2) =Ec1,c2"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.152,"
(µc1 −µc2)⊤(µc1 −µc2)
2
.
(9)"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.15466666666666667,"Remark. The term EV[hL2(ϕ(Sy))] is the variance of the norm of the feature vectors. The term
VTr(Σc1) is the variance of the summation with diagonal element of the covariance matrix from
each class embedding; it can be interpreted as the difference of the distribution shape constructed
from each class embedding. The term Vwit(Στ, Σ, µ) is correlated with the within-class variance
because Vwit(Στ ,Σ,µ)"
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.15733333333333333,"Tr(Σ)
is a secondary expression for the ratio of between-class variance and within-
class variance. The term E dist2
L2(µc1, µc2) is the expectation of the Euclidean distance between
the class mean vectors ."
RELATING VARIANCE OF NORM TO UPPER BOUND OF EXPECTED RISK,0.16,This bound has the following properties.
"ITS DERIVATION DOES NOT REQUIRE THE FEATURES TO BE DISTRIBUTED ON ANY SPECIFIC DISTRIBUTIONS
AND THE CLASS-CONDITIONED COVARIANCE MATRIX DOES NOT HAVE TO BE THE SAME AMONG CLASSES",0.16266666666666665,"1. Its derivation does not require the features to be distributed on any specific distributions
and the class-conditioned covariance matrix does not have to be the same among classes
2. The bound can decrease when any of the following statistics decreases with fixed between-
class variance (Σ): (i) the variance of the norm of the feature vectors, as what we discussed
in Section 3.2, (ii) the difference in the distribution shape constructed from each class
embedding, (iii) the within-class variance (Στ), (iv) the Euclidean distance between the
class-mean vectors."
"ITS DERIVATION DOES NOT REQUIRE THE FEATURES TO BE DISTRIBUTED ON ANY SPECIFIC DISTRIBUTIONS
AND THE CLASS-CONDITIONED COVARIANCE MATRIX DOES NOT HAVE TO BE THE SAME AMONG CLASSES",0.16533333333333333,Under review as a conference paper at ICLR 2022
"ITS DERIVATION DOES NOT REQUIRE THE FEATURES TO BE DISTRIBUTED ON ANY SPECIFIC DISTRIBUTIONS
AND THE CLASS-CONDITIONED COVARIANCE MATRIX DOES NOT HAVE TO BE THE SAME AMONG CLASSES",0.168,"As a result, our bound loosens the modeling assumption of Theorem2 in appendix A.1 and has its
property in 2-(iii). We show the proof in Appendix A.2."
FEATURE-TRANSFORMATION METHODS,0.17066666666666666,"3.4
FEATURE-TRANSFORMATION METHODS"
FEATURE-TRANSFORMATION METHODS,0.17333333333333334,"We hypothesize from Theorem 1 that in addition to a feature-transformation method related to equa-
tion 6, eliminating the affect from the difference in the distribution shape among classes and low-
ering the ratio of between-class variance to within-class variance can improve the performance of
a prototype classifier. Regarding equation 9, the ratio of the Euclidean distance between the class
mean vectors and the between-class variance is supposed to be constant. Thus, we focus on the
transformation methods relating to equation 6, equation 7 and equation 8. We analyze the following
feature-transformation methods: L2-normalization (L2-norm), variance-normalization, LDA, EST,
and EST+L2-norm. We show the detail of each method in Section A.3 due to lack of space."
EXPERIMENTAL EVALUATION,0.176,"4
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.17866666666666667,"In this section, we experimentally analyzed the effectiveness of the feature-transformation methods
mentioned in Section3.4. The center loss (Wen et al., 2016) and affinity loss (Hayat et al., 2019)
have been proposed to efficiently pull the features of the same class to their centers in the training
phase; however, we focus on a widely used pre-trained model in this experiments following the line
of studies Chen et al. (2019) and Tian et al. (2020)."
DATASETS AND EVALUATION PROTOCOL,0.18133333333333335,"4.1
DATASETS AND EVALUATION PROTOCOL"
DATASETS AND EVALUATION PROTOCOL,0.184,"miniImageNet. The miniImageNet dataset (Vinyals et al., 2016) is a standard bench-mark for few-
shot learning algorithms for recent studies. It contains 100 classes randomly sampled from Im-
ageNet. Each class contains 600 images. We follow the widely-used splitting protocol (Ravi &
Larochelle, 2017) to split the dataset into 64/16/20 for training/validation/testing respectively."
DATASETS AND EVALUATION PROTOCOL,0.18666666666666668,"tieredImageNet The tieredImageNet dataset (Ren et al., 2018) is another subset of ImageNet but
has 608 classes. These classes are grouped into 34 higher level categories in accordance with the
ImageNet hierarchy and these categories are split into 20 training (351 classes), 6 validation (97
classes), 8 testing categories (160 classes). This splitting protocol ensures that the training set is
distinctive enough from the testing set and makes the problem more realistic since we generally
cannot assume that test classes will be similar to those seen in training."
DATASETS AND EVALUATION PROTOCOL,0.18933333333333333,"CIFAR-FS The CIFAR-FS dataset (Bertinetto et al., 2019) is a recently proposed fewshot image
classification benchmark, consisting of all 100 classes from CIFAR-100 (Krizhevsky et al.). The
classes are randomly split into 64, 16, and 20 for meta-training, meta-validation, and meta-testing
respectively. Each class contains 600 images of size 32 × 32."
DATASETS AND EVALUATION PROTOCOL,0.192,"FC100 The FC100 dataset (Lee et al., 2019) is another dataset derived from CIFAR-100 (Krizhevsky
et al.), containing 100 classes which are grouped into 20 categories. These categories are split into
12 categories for training (60 classes), from 4 categories for validation (20 classes), 4 categories for
testing (20 classes)."
IMPLEMENTATION DETAILS,0.19466666666666665,"4.2
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.19733333333333333,"We compared the feature-transformation methods applied on against ProtoNet (Snell et al., 2017),
Baseline and Baseline++ (Chen et al., 2019). In Baseline, the linear projection layer is trained on a
support set and in Baseline++, the norm of the linear projection layer and features are normalized
to be constant. We call Baseline and Baseline++ as “linear evlauation” methods. We also com-
pared with the feature-transformation method proposed in a previous study (Wang et al., 2019b).
In that study they transformed the features so that the mean of all features to be origin before L2-
normalization and then use a prototype classifier without training a new linear classifier. We call
this operation as centering+L2-norm. We re-implemented these methods following the training pro-
cedure in previous study (Chen et al., 2019). In the pre-training stage, where the cross-entropy
loss was used and meta-learning was not used, we trained 400 epochs with a batch size of 16. In
the meta-training stage for ProtoNet, we trained 60, 000 episodes for 1-shot and 40, 000 episodes"
IMPLEMENTATION DETAILS,0.2,Under review as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.20266666666666666,"Table 1: Classification accuracies on miniImageNet and tieredImageNet of ProtoNet ,
linear evaluation methods (Chen et al., 2019), and ours. The best performing methods and
any other runs within 95% confidence margin are in bold."
IMPLEMENTATION DETAILS,0.20533333333333334,"Method
miniImageNet
tieredImageNet
ResNet12
ResNet18
ResNet12
ResNet18
1-shot
5-shot
1-shot
5-shot
1-shot
5-shot
1-shot
5-shot
ProtoNet
53.48%
73.56%
56.26%
74.02%
55.40%
77.67%
60.50%
81.40%
B@FT
54.54%
76.50%
55.41%
76.95%
61.67%
81.62%
63.38%
83.18%
B+@FT
56.33%
74.62%
55.07%
74.15%
63.02%
81.07%
64.20%
81.62%
B@CL2
58.66%
75.98%
57.67%
70.78%
64.88%
80.42%
65.26%
81.63%
B+@CL2
57.50%
74.00%
57.00%
74.06%
63.31%
79.19%
65.67%
81.41%
B
46.36%
73.97%
43.86%
72.36%
50.60%
78.10%
56.16%
80.33%
B@L2-N
57.18%
77.12%
56.57%
76.44%
64.04%
81.73%
65.19%
82.93%
B@V-N
−
63.55%
−
62.78%
−
66.08%
−
75.56%
B@LDA
−
73.75%
−
73.54%
−
77.44%
−
80.85%
B@EST
51.28%
72.80%
44.19%
72.99%
53.90%
78.09%
57.12%
80.59%
B@EST+L2-N
58.00%
76.90%
56.39%
76.24%
64.54%
81.40%
64.71%
83.24%
B+
41.18%
73.97%
36.80%
63.76%
46.52%
73.85%
48.27%
75.87%
B+@L2-N
57.96%
75.38%
57.21%
74.89%
64.96%
81.08%
65.40%
82.49%
B+@V-N
−
55.92%
−
54.31%
−
62.19%
−
64.33%
B+@LDA
−
69.60%
−
74.38%
−
72.88%
−
75.55%
B+@EST
47.11%
69.01%
47.21%
69.11%
52.01%
74.26%
53.49%
75.38%
B+@EST+L2-N
58.32%
77.49%
57.00%
77.13%
57.63%
80.99%
60.87%
81.80%"
IMPLEMENTATION DETAILS,0.208,"for 5-shot tasks. We used the validation set to select the training episodes with the best accuracy.
In each episode, we sampled N classes to form N-way classification (In meta-training N=20 and
meta-testing N=5 following the original study of ProtoNet (Snell et al., 2017)). For each class, we
selected K labeled instances as our support set and 16 instances for the query set for a K-shot task."
IMPLEMENTATION DETAILS,0.21066666666666667,"In the linear evaluation or meta-testing stage for all methods, we averaged the results over 600 trials.
In each trial, we randomly sampled 5 classes from novel classes, and in each class, we also selected
K instances for the support set and 16 for the query set. For Baseline and Baseline++, we used the
entire support set to train a new classifier for 100 iterations with a batch size of 4. With ProtoNet,
we used the models trained in the same shots as meta-testing stage since the mismatch of number of
shots between meta-training and meta-testing degrades performance (Cao et al., 2020). All methods
were trained from scratch, and the Adam optimizer with initial learning rate 10−3 was used. We
applied standard data augmentation including random crop, horizontal-flip, and color jitter in both
the training stages. We used a ResNet12 network and ResNet18 network following the previous
study (Chen et al., 2019; Yoon et al., 2019)."
IMPLEMENTATION DETAILS,0.21333333333333335,"Since LDA constructs a within-class covariance matrix from a support set and variance-
normalization computes sample variance, we did not investigate the LDA score and variance-
normalization score of 1-shot settings. For LDA we set λ = 0.0001 for equation 18 in Appendix
A.3 and for EST we set the dimensions of the projected space to 60 following the settings of the
original study (Cao et al., 2020)."
RESULTS,0.216,"4.3
RESULTS"
RESULTS,0.21866666666666668,"We present the experimental results on Table 1 and Table 2 on the basis of backbones with ResNet12
and ResNet18 for a comprehensive comparison. Table 1 shows the results on miniImageNet and
tieredImageNet, and Table 2 shows the results on CIFARFS and FC100. We show the detailed
performance results with 95% confidence margin and other transformation methods in A.6."
RESULTS,0.22133333333333333,"Method notations in experimental results
We denote B@FT and B+@FT as the linear evalua-
tion methods: Baseline and Baseline++. We also denote B@CL2, B+@CL2 as centering+L2-norm
(Wang et al., 2019b) on features trained with Baseline and Baseilne++. B, B@L2-N, B@V-N,
B@LDA, B@EST, and B@EST+L2-N stands for a prototype classifier with applying no trans-
formation, L2-norm, variance-normalization, LDA, EST, and EST+L2-N on the features extracted
using a model pre-trained in the same manner as B@FT. B+, B+@L2-N, B+@V-N, B+@LDA,
B+@EST, and B+@EST+L2-N stands for a prototype classifier with applying no transformation,"
RESULTS,0.224,Under review as a conference paper at ICLR 2022
RESULTS,0.22666666666666666,"Table 2: Classification accuracies on CIFARFS and FC100 of ProtoNet, linear evaluation
methods (Chen et al., 2019), and ours. The best performing methods and any other runs
within 95% confidence margin are in bold."
RESULTS,0.22933333333333333,"Method
CIFARFS
FC100
ResNet12
ResNet18
ResNet12
ResNet18
1-shot
5-shot
1-shot
5-shot
1-shot
5-shot
1-shot
5-shot
ProtoNet
58.65%
75.33%
62.05%
78.25%
35.56%
51.12%
36.02%
51.02%
B@FT
55.97%
76.50%
56.46%
77.43%
39.72%
56.04%
40.06%
57.04%
B+@FT
61.08%
76.15%
61.34%
77.86%
36.01%
50.73%
36.93%
50.41%
B@CL2
58.61%
74.73%
58.09%
76.52%
41.58%
55.82%
41.51%
56.44%
B+@CL2
61.59%
76.10%
63.17%
77.49%
37.51%
50.38%
38.30%
51.06%
B
44.12%
72.47%
47.86%
75.66%
31.60%
52.50%
35.90%
55.20%
B@L2-N
59.43%
77.45%
58.51%
77.43%
40.34%
56.61%
40.49%
57.71%
B@V-N
−
44.66%
−
67.72%
−
41.99%
−
49.69%
B@LDA
−
75.73%
−
75.25%
−
52.90%
−
55.44%
B@EST
52.62%
72.80%
55.57%
75.10%
44.45%
52.97%
47.49%
55.68%
B@EST+L2-N
60.70%
77.04%
61.14%
77.48%
47.57%
56.96%
50.13%
59.94%
B+
45.73%
73.97%
36.80%
63.76%
29.72%
46.85%
30.76%
47.62%
B+@L2-N
61.07%
76.71%
63.48%
77.86%
37.16%
51.10%
38.55%
50.15%
B+@V-N
−
57.20%
−
57.16%
−
36.76%
−
40.66%
B+@LDA
−
68.58%
−
69.86%
−
47.10%
−
47.63%
B+@EST
50.18%
70.32%
49.82%
71.07%
37.60%
46.81%
39.92%
47.65%
B+@EST+L2-N
59.83%
76.32%
63.00%
77.99%
40.88%
50.45%
41.65%
50.74%"
RESULTS,0.232,"L2-norm, and variance-normalization on the features extracted using the same pre-trained model as
with B+@FT."
RESULTS,0.23466666666666666,"Comparison of the feature transformation methods with ProtoNet, linear evaluation meth-
ods, and centering+L2-norm
From Table 1 and Table 2, we can observe that the prototype
classifier with L2-norm and EST+L2-norm performs comparably with ProtoNet and the linear-
evaluation-based approach in all settings and EST+L2-norm performs the best among the data trans-
formation methods. Comparing the feature-transformation methods described in Section 3.4 with
centering+L2-norm (Wang et al., 2019b), centering+L2-norm can slightly improve the performance
of the prototype classifier in several 1-shot settings . However, in 5-shot settings, the boost decreases
and even performs worse than L2-norm, e.g. miniImageNet and tieredImageNet with ResNet12."
RESULTS,0.23733333333333334,"Comparison of the feature transformation methods with B and B+
In the 1-shot setting, al-
though EST falls short of ProtoNet and linear-evaluation-based approach, it also improves the per-
formance of a prototype classifier. The performance gain of both L2-norm, EST and EST+L2-norm
decrease when the number of shots increases. This phenomenon can be explained through Theorem
1. The term relating to the variance of the norm and the ratio of the between-class variance to the
within-class variance depends on K.Since the term diminishes with increasing K, the performance
gain of the feature-transformation methods decreases."
RESULTS,0.24,"Discussion on the feature transformation methods
Surprisingly, variance normalization per-
forms worse than applying no feature transformation. This can be explained by the following two
reasons. The first is that the term in the bound related to the difference among the class distribution
shapes is smaller than other terms such as the variance of the norm and the ratio of the between-class
variance to the within-class variance. Figure 2 shows the term associated with the bound computed
from each dataset and backbone. The larger the value is, the more it contributes to larger risk. We
found the term associated with equation 7 is small compared with other values and thus variance-
normalization did not worked well in our experiment. The second reason is that estimating variance
with a support set is unstable and make prediction unreliable in few-shot learning. LDA and EST in
5-shot setting does not improve performance so much compared with L2-norm variants while in the
1-shot settings, EST helps improving performance. This is because in the 5-shot settings, the values
computed from equation 8 gets smaller with larger K and the effect of equation 8 on the risk of a
prototype classifier in the 1-shot settings is larger. Especially, EST outperforms L2-norm in FC100
with the 1-shot setting. We found from Figure 2 the features of FC100 shows the largest ratio of the"
RESULTS,0.24266666666666667,Under review as a conference paper at ICLR 2022
RESULTS,0.24533333333333332,"Figure 2: Left: We plotted the values shown in equations 6,7,8 divided by Tr(Σ)2. The values are computed
from test-split of each dataset with ResNet12 and ResNet18. We scaled the values so that each dataset’s
Vwit(Στ ,Σ,µ)"
RESULTS,0.248,"Tr(Σ)2
to be 1 for simplicity. Right: We showed the ratio of the within-class variance to the between-class
variance computed from the test-split’s features extracted using each backbone trained on each corresponding
dataset. We scaled the values so that the maximium value to be 1 for simplicity."
RESULTS,0.25066666666666665,"Figure 3: Histogram of cosine similarities with class mean vectors and largest eigen vectors of Left:Σ, Right:Σc"
RESULTS,0.25333333333333335,"within-class to between-class variance among all dataset. Thus the method reducing the ratio works
better with FC100 features than any other dataset’s features."
EFFECT ON COVARIANCE MATRIX,0.256,"4.4
EFFECT ON COVARIANCE MATRIX"
EFFECT ON COVARIANCE MATRIX,0.25866666666666666,"We further analyzed how Tr(Σ) and the trace of class c’s covariance matrix Tr(Σc) fluctuate with
L2-norm. Since L2-norm lowers the variance in the direction of the normalized vector, if the co-
variance matrix stretches in the direction of the class mean vector, L2-norm can reduce its trace.
Thus we analyzed the cosine similarities between class-mean feature vectors and the eigenvectors
with the largest eigen value of the covariance matrices. Figure 3 illustrates the distribution of cosine
similarities of each class-mean feature vector and the eigenvectors in test class of miniImageNet.
This figure shows Σc tends to stretches to the direction of class mean vector and Σ stretches in the
orthogonal direction to each class-mean feature vector. From this observation, we found that L2-
norm can reduce Tr(Σc) and has less effect on Tr(Σ). L2-norm can reduce the variance of the norm
and the ratio of the within-class variance to the between-class variance."
CONCLUSION,0.2613333333333333,"5
CONCLUSION"
CONCLUSION,0.264,"We theoretically and experimentally analyzed how the variance of the norm of feature vectors affects
the performance of a prototype classifier and found that using EST+L2-norm makes the classifier
comparable with ProtoNet and the linear-evaluation-based approach. We also found that when the
number of shots in a support set increases, the performance gain from a feature-transformation
method decreases, which is consistent with the results of the theoretical analysis. A prototypical
classifier is expected to be a practically useful first step in tackling few-shot learning problems
because of its simplicity."
CONCLUSION,0.26666666666666666,Under review as a conference paper at ICLR 2022
REFERENCES,0.2693333333333333,REFERENCES
REFERENCES,0.272,"Kelsey R. Allen, Evan Shelhamer, Hanul Shin, and Joshua B. Tenenbaum. Infinite mixture proto-
types for few-shot learning. PMLR, pp. 232–241, 2019."
REFERENCES,0.27466666666666667,"Peyman Bateni, Raghav Goyal, Frank Wood Vaden Masrani, , and Leonid Sigal. Improved few-
shot visual classification. In Proceedings of the IEEE conference on computer vision and pattern
recognition (CVPR), pp. 3–14, 2020."
REFERENCES,0.2773333333333333,"Luca Bertinetto, Jo˜ao Henriques, Philip H.S. Torr, and Andrea Vedaldi. Meta-learning with differ-
entiable closed-form solvers. In International Conference on Learning Representations (ICLR),
2019."
REFERENCES,0.28,"Tiansi Cao, Marc T.Law, and Sanja Fidler. A theoretical analysis of the number of shots in few-shot
learning. In International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.2826666666666667,"Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Wang, and Jia-Bin Huang. A closer look at
few-shot classification. In International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.2853333333333333,"Debasmit Das and C.S. George Lee. A two-stage approach to few-shot learning for image recogni-
tion. In IEEE Transactions on Image Processing, volume 29, 2020."
REFERENCES,0.288,"Debasmit Das, J. H. Moon, and C. S. George Lee. Few-shot image recognition with manifolds. In
International Symposium on Visual Computing (ISVC), pp. 3–14, 2020."
REFERENCES,0.2906666666666667,"Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin
loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition (CVPR), 2019."
REFERENCES,0.29333333333333333,"Carl Doersch, Ankush Gupta, and Andrew Zisserman. Crosstransformers: spatially-aware few-shot
transfer. In Advances in Neural Information Processing Systems, pp. 4003–4014, 2020."
REFERENCES,0.296,"Nanqing Dong and Eric P. Xing. Few-shot semantic segmentation with prototype learning. In British
Machine Vision Conference (BMVC), 2018."
REFERENCES,0.2986666666666667,"Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learn-
ing the representation, provably. In International Conference on Learning Representations(ICLR),
2021."
REFERENCES,0.30133333333333334,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning
(ICML), pp. 1126–1135. PMLR, 2017."
REFERENCES,0.304,"Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Ad-
vances in Neural Information Processing Systems, pp. 9516–9527, 2018."
REFERENCES,0.30666666666666664,"Keinosuke Fukunaga. Introduction to Statistical Pattern Recognition (2Nd Ed.). Academic Press
Professional, Inc., 1990."
REFERENCES,0.30933333333333335,"Munawar Hayat, Salman Khan, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Max-margin
class imbalanced learning with gaussian affinity. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), pp. 6469–6479, 2019."
REFERENCES,0.312,"Zhong Ji, Xingliang Chai, Yunlong Yu, and Zhongfei Zhang. Reweighting and information-guidance
networks for few-shot learning. In Neurocomputing, volume 423, pp. 13–23, 2021."
REFERENCES,0.31466666666666665,"Alex Krizhevsky, Vinod Nair, , and Geoffrey Hinton. Cifar-100."
REFERENCES,0.31733333333333336,"Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 10657–10665, 2019."
REFERENCES,0.32,"Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Mingsheng Long, and Han Hu. Negative margin
matters: Understanding margin in few-shot classification. In European Conference on Computer
Vision (ECCV), 2020."
REFERENCES,0.32266666666666666,Under review as a conference paper at ICLR 2022
REFERENCES,0.3253333333333333,"Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition (CVPR), 2017."
REFERENCES,0.328,"James Lucas, Mengye Ren, Irene Raissa KAMENI KAMENI, Toniann Pitassi, and Richard Zemel.
Theoretical bounds on estimation error for meta-learning. In International Conference on Learn-
ing Representations(ICLR), 2021."
REFERENCES,0.33066666666666666,"Frederik Pahde, Mihai Puscas, Tassilo Klein, and Moin Nabi. Multimodal prototypical networks
for few-shot learning. In Winter Conference on Applications of Computer Vision(WACV), pp.
2644–2653, 2021."
REFERENCES,0.3333333333333333,"Michael D. Perlman.
Jensen’s inequality for a convex vector-valued function on an infininite-
dimensional space. In J Multivar Anal, pp. 52–65, 1974."
REFERENCES,0.336,"Cheng Perng Phoo and Bharath Hariharan. Self-training for few-shot transfer across extreme task
difference. In International Conference on Learning Representations(ICLR), 2021."
REFERENCES,0.33866666666666667,"Hang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In
Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pp.
5822–5830, 2018."
REFERENCES,0.3413333333333333,"Aravind Rajeswaran, Chelsea Finn, Sham M. Kakade, and Sergey Levine. Meta-learning with im-
plicit gradient. PMLR, pp. 1126–1135, 2017."
REFERENCES,0.344,"Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International
Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.3466666666666667,"Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo
Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised fewshot classification. In
International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.34933333333333333,"Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash Harandi. Adaptive subspaces for few-
shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition
(CVPR), 2020."
REFERENCES,0.352,"Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.3546666666666667,"Shengli Sun, Qingfeng Sun, Kevin Zhou, and Tengchao Lv. Hierarchical attention prototypical
networks for few-shot text classification. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pp. 476–485, 2019."
REFERENCES,0.35733333333333334,"Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales.
Learning to compare: Relation network for few-shot learning. In Computer Vision and Pattern
Recognition (CVPR), 2018."
REFERENCES,0.36,"Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, and Phillip Isola. Rethinking
few-shot image classification: a good embedding is all you need? In European Conference on
Computer Vision (ECCV), 2020."
REFERENCES,0.3626666666666667,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In Advances in Neural Information Processing Systems, 2016."
REFERENCES,0.36533333333333334,"Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei
Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition (CVPR), 2018."
REFERENCES,0.368,"Jixuan Wang, Kuan-Chieh Wang, Marc T. Law, Frank Rudzicz, and Michael Brudno. Centroid-
based deep metric learning for speaker recognition. In International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 3652–3656. IEEE, 2019a."
REFERENCES,0.37066666666666664,Under review as a conference paper at ICLR 2022
REFERENCES,0.37333333333333335,"Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Laurens van der Maaten. Simpleshot: Revis-
iting nearest-neighbor classification for few-shot learning. volume abs/:1911.04623, 2019b."
REFERENCES,0.376,"Yandong Wen, Kaipeng Zhang, Zhifeng Li, , and Yu Qiao. A discriminative feature learning ap-
proach for deep face recognition. In European Conference on Computer Vision (ECCV), 2016."
REFERENCES,0.37866666666666665,"Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In
International Conference on Learning Representations(ICLR), 2021."
REFERENCES,0.38133333333333336,"Sung Whan Yoon, Jun Seo, and Jaekyun Moon. Tapnet: Neural network augmented with task-
adaptive projection for few-shot learning. In Proceedings of the 36th International Conference on
Machine Learning (ICML), 2019."
REFERENCES,0.384,"A
APPENDIX"
REFERENCES,0.38666666666666666,"A.1
EXISTING UPPER BOUND ON EXPECTED RISK FOR PROTOTYPE CLASSIFIER"
REFERENCES,0.3893333333333333,"To analyze the behavior of a prototype classifier, we start from the current study (Cao et al., 2020).
The following theorem is the upper bound of the expected risk of prototypical networks with the
next conditions."
REFERENCES,0.392,"• The probability distribution of an extracted feature ϕ(x) given its class y = c is Gaussian i.e
Dy = N(µc, Σc), where µc = Ex∼Dc[ϕ(x)] and Σc = Ex∼Dc[(ϕ(x)−µc)(ϕ(x)−µc)⊤].
• All class-conditioned distributions have the same covariance matrix, i.e., ∀(c, c′), Σc =
Σc′.
Theorem 2. Let M be an operation of a prototype classifier on binary classification defined by
equation 1. Then for µ = Ec∼τ[µc] and Σ = Ec∼τ[(µc −µ)(µc −µ)⊤], the miss-classification risk
of the prototype classifier on binary classification RM satisfies"
REFERENCES,0.39466666666666667,"RM(ϕ) ≤1 −
4 Tr(Σ)2"
REFERENCES,0.3973333333333333,"8(1 + 1/k)2 Tr (Σ2c) + 16(1 + 1/k) Tr (ΣΣc) + E dist2
L2(µc1, µc2),
(10)"
REFERENCES,0.4,"where E dist2
L2(µc1, µc2) = Ec1,c2
h
((µc1 −µc2) ⊤(µc1 −µc2))2i
."
REFERENCES,0.4026666666666667,We show the detail of the derivation in Appendix A.2.
REFERENCES,0.4053333333333333,"A.2
DERIVATION DETAILS OF THEOREM 2"
REFERENCES,0.408,"we will explain the rough sketch of derivation of Theorem 2. In prototype classifier, from equation 1
and equation 3, RM is written with sigmoid function σ as follows:"
REFERENCES,0.4106666666666667,"RM(ϕ) =Prc1,c2∼τ,x∼Dc1,S∼D⊗2K"
REFERENCES,0.41333333333333333,"
σ(∥ϕ(x) −ϕ(Sc2)∥−∥ϕ(x) −ϕ(Sc1)∥) ≤1 2 "
REFERENCES,0.416,"=Pr(α < 0),
(11)"
REFERENCES,0.4186666666666667,"where α ≜∥ϕ(x) −ϕ(Sc2)∥−∥ϕ(x) −ϕ(Sc1)∥. we bound equation 11 with expectation and
variance of α by following proposition.
Proposition 1. From the one-sided Chebyshev’s inequality, it immediately follows that:"
REFERENCES,0.42133333333333334,"RM(ϕ) = Pr(α < 0) ≤1 −
ES∼D⊗2kEc1,c2∼τEx∼Dc1[α]2"
REFERENCES,0.424,"VarS,c1,c2,x[α] + ES∼D⊗2kEc1,c2∼τEx∼Dc1[α]2 .
(12)"
REFERENCES,0.4266666666666667,equation 11 can be further write down as follows by Law of Total Expectation
REFERENCES,0.42933333333333334,"VarS,c,x(α) = ES∼D⊗2kEc1,c2∼τEx∼Dc[α2] −
 
ES∼D⊗2kEc1,c2∼τEx∼Dc1[α]
2"
REFERENCES,0.432,"= Ec1,c2Ex,S[α2|c1, c2] −
 
ES∼D⊗2kEc1,c2∼τEx∼Dc1[α]
2"
REFERENCES,0.43466666666666665,"= Ec1,c2[Varx,S(α|c1, c2) + Ex,S[α|c1, c2]2] −Ec1,c2∼τEx∼Dc1ES∼D⊗2K[α]2."
REFERENCES,0.43733333333333335,Under review as a conference paper at ICLR 2022
REFERENCES,0.44,"Therefore,"
REFERENCES,0.44266666666666665,"Pr(α < 0) ≤1 −
ES∼D⊗2kEc1,c2∼τEx∼Dc[α]2"
REFERENCES,0.44533333333333336,"Ec1,c2[Varx∼Dc1,S[α] + Ex∼Dc1,S[α]2].
(13)"
REFERENCES,0.448,"We write down the expection and variance of α with following Lemmas 3 and 4.
Lemma 3. Under the same notation and assumptions as Theorem 2, then,"
REFERENCES,0.45066666666666666,ES∼D⊗2kEx∼Dc1[α] = (µc1 −µc2)⊤(µc1 −µc2)
REFERENCES,0.4533333333333333,"Ec1,c2∼τEx∼Dc1ES∼D⊗2K[α] = 2Tr(Σ)."
REFERENCES,0.456,"Lemma 4. Under the same notation and assumptions as Theorem 2, then,"
REFERENCES,0.45866666666666667,"Ec1,c2 [Varx,S [α|c1, c2]] ≤8

1 + 1 K"
REFERENCES,0.4613333333333333,"
Tr

Στ((1 + 1"
REFERENCES,0.464,"K )Στ + 2Σ)

.
(14)"
REFERENCES,0.4666666666666667,"The proofs of the above lemmas are in the current study (Cao et al., 2020)."
REFERENCES,0.4693333333333333,"With Proposition 1 and Lemma 3, Lemma 4, we obtain Theorem 2."
REFERENCES,0.472,"A.3
DETAIL OF FEATURE-TRASNFORMING METHODS"
REFERENCES,0.4746666666666667,"L2 normalization (L2-norm)
From equation 6, normalizing the norm of the feature vectors can
improve the performance of a prototype classifier. We denote a function normalizing the norm as
ψL2 given by"
REFERENCES,0.47733333333333333,"ψL2(ϕ(x)) =
ϕ(x)
∥ϕ(x)∥.
(15)"
REFERENCES,0.48,"variance-normalization
From equation 7, normalizing the Euclidean distance in equation 1 by
the variance of each class embedding can improve the performance of a prototype classifier. This
can be interpreted as a variation of the Mahalanobis distance with a diagonal covariance matrix.
We denote a function that measures the distance of two vectors normalized by its variance at each
dimension as follows:"
REFERENCES,0.4826666666666667,"ψmaha(ϕ(x)1), ϕ(x)2)) = D
X d=1"
REFERENCES,0.48533333333333334," 
ϕ(x)1)(d) −ϕ(x)2)(d)2"
REFERENCES,0.488,"Var[ϕ(x)1)(d)]
,
(16)"
REFERENCES,0.49066666666666664,"where Var[z] is sample variance of z. equation 1 is rewritten with ψmaha(ϕ(x1), ϕ(x2)) as the
following equation."
REFERENCES,0.49333333333333335,"M(ϕ, x, S)j =
exp

−ψmaha

ϕ(x), ϕ(Sj)
"
REFERENCES,0.496,"PN
l=1 exp

−ψmaha

ϕ(x), ϕ(Sl)
.
(17)"
REFERENCES,0.49866666666666665,"This normalization is similar to Bateni et al. (2020)’s work but our method differs in that we do not
need to do meta-learning and we do not need to add other modules to the backbone network."
REFERENCES,0.5013333333333333,"LDA
From equation 8, increasing the ratio of the between-class variance to the within-class vari-
ance can improve the performance of a prototype classifier. LDA (Fukunaga, 1990) is widely used to
search for a projection space that maximizes the between-class variance and minimizes the within-
class variance. It computes the eigenvectors of a matrix ˆ
Στ
−1 ˆΣ, where ˆΣ is the covariance matrix of
prototypes and ˆ
Στ is the class-conditioned covariance matrix. Since the number of data is small in
few-shot settings, ˆ
Στ
−1 cannot be estimated stably and we add a regularizer term to ˆ
Στ and define
it as ˆ
Στ reg."
REFERENCES,0.504,"ˆ
Στ reg = ˆ
Στ + λI,
(18)"
REFERENCES,0.5066666666666667,"where λ ∈R1, I ∈RD×D is identity matrix."
REFERENCES,0.5093333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.512,"EST
Since computation of LDA is unstable, we also analyze the effect of EST (Cao et al., 2020).
EST computes eigenvectors of a matrix ˆΣ −ρ ˆ
Στ: the difference between the covariance matrix of
the class mean vectors and the class mean covariance matrix with weight parameter ρ. Similar to
LDA, EST also searches for the projection space that maximizes the Σ and minimizes the Στ. We
compute the eigenvectors of the matrix with two small modification. The first is that we remove
hyper-parameter rho and then simply computes the eigenvectors of ˆΣ −ˆ
Στ. The second is that we
constructs the matrix from a support set in the multi-shot setting while we follows the original study
in the 1-shot setting."
REFERENCES,0.5146666666666667,"EST+L2-norm
We hypothesize that the combination of the transforming methods can improve
the performance of a prototype classifier independently from each other. Specifically, we focus on
reducing equation 6 and equation 8 by combining EST and L2-norm. We first apply EST to reduce
equation 8 and after that we apply L2-norm to reduce equation 6. At the end of the operation we
want the variance of the norm to be 0, thus we apply EST and after that we apply L2-norm."
REFERENCES,0.5173333333333333,"variance-normalization+L2-norm
We focus on reducing equation 6 and equation 7 by combin-
ing variance-normalization and L2-norm. Following the similar procedure of EST+L2-norm,
we first apply variance-normalization and after that we apply L2-norm. We show the experimental
results of this transformation methods in A.6."
REFERENCES,0.52,"LDA+L2-norm
We focus on reducing equation 8 and equation 7 by combining LDA and L2-
norm. Following the similar procedure of EST+L2-norm, we first apply LDA and after that we
apply L2-norm. We show the experimental results of this transformation methods in A.6."
REFERENCES,0.5226666666666666,"A.4
DERIVATION DETAILS OF THEOREM 1"
REFERENCES,0.5253333333333333,"We start the proof from equation 13. We first prove the following Lemma 5 related to the expectation
statistics of α in equation 13."
REFERENCES,0.528,"Lemma 5. Under the same notations and assumptions as Theorem 1, then,"
REFERENCES,0.5306666666666666,ES∼D⊗2kEx∼Dc1[α] = 1
REFERENCES,0.5333333333333333,K (Tr(Σc2) −Tr(Σc1)) + (µc1 −µc2)⊤(µc1 −µc2)
REFERENCES,0.536,"Ec1,c2∼τEx∼Dc1ES∼D⊗2K[α] = 2Tr(Σ)."
REFERENCES,0.5386666666666666,"Proof. First, from the definition of α, we split ES∼D⊗2kEx∼Dc1[α] in to two parts and examine
them seperately."
REFERENCES,0.5413333333333333,"ES∼D⊗2kEx∼Dc1[α] = E
ϕ(x) −ϕ(Sc2)

2"
REFERENCES,0.544,"|
{z
}
(i)"
REFERENCES,0.5466666666666666,"−E
ϕ(x) −ϕ(Sc1)

2"
REFERENCES,0.5493333333333333,"|
{z
}
(ii)"
REFERENCES,0.552,".
(19)"
REFERENCES,0.5546666666666666,"In regular conditions, for random vector X, the expectation of the norm is"
REFERENCES,0.5573333333333333,"E[X⊤X] = Tr(Var(X)) + E[X]⊤E[X],
(20)"
REFERENCES,0.56,and the variance of the vector is
REFERENCES,0.5626666666666666,"Var(X) = E[XX⊤] −E[X]E[X]⊤
(21)"
REFERENCES,0.5653333333333334,"Σci
∆= Varx∼Dci(ϕ(x)).
(22)"
REFERENCES,0.568,"Hence,"
REFERENCES,0.5706666666666667,(i) = Ex∼Dc1ES
REFERENCES,0.5733333333333334,"ϕ(x) −ϕ(Sc2)

2"
REFERENCES,0.576,"= Tr

Varx∼Dc1,S
h
ϕ(x) −ϕ(Sc2)
i
+ ExES
h
ϕ(x) −ϕ(Sc2)
i⊤
ExES
h
ϕ(x) −ϕ(Sc2)
i
, (23)"
REFERENCES,0.5786666666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.5813333333333334,where the first term inside the trace can be expanded as:
REFERENCES,0.584,"Var
h
ϕ(x) −ϕ(Sc2)
i
= E

ϕ(x) −ϕ(Sc2)
 
ϕ(x) −ϕ(Sc2)
⊤
−(µc1 −µc2)(µc1 −µc2)⊤"
REFERENCES,0.5866666666666667,"= Var [ϕ(x)] + E [ϕ(x)] E [ϕ(x)]⊤+ Var
h"
REFERENCES,0.5893333333333334,"ϕ(Sc2)
i
+ E
h"
REFERENCES,0.592,"ϕ(Sc2)
i
E
h"
REFERENCES,0.5946666666666667,"ϕ(Sc2)
i⊤"
REFERENCES,0.5973333333333334,"−µc2µ⊤
c1 −µc1µ⊤
c2 −(µc1 −µc2)(µc1 −µc2)⊤"
REFERENCES,0.6,= Σc1 + 1
REFERENCES,0.6026666666666667,"K Σc2
(Last terms cancel out).
(24)"
REFERENCES,0.6053333333333333,The second term in equation 23 is simply as follows.
REFERENCES,0.608,"Ex∼Dc1ES
h
ϕ(x) −ϕ(Sc2)
i
= µc1 −µc2.
(25)"
REFERENCES,0.6106666666666667,From equation 24 and equation 25 we obtain
REFERENCES,0.6133333333333333,(i) = Tr(Σc1) + 1
REFERENCES,0.616,"K Tr(Σc2) + (µc1 −µc2)⊤(µc1 −µc2).
(26)"
REFERENCES,0.6186666666666667,"Similarly for ii,"
REFERENCES,0.6213333333333333,"(ii) = Tr

Varx∼Dc1,S
h
ϕ(x) −ϕ(Sc1)
i
+ ExES
h
ϕ(x) −ϕ(Sc1)
i⊤
ExES[ϕ(x) −ϕ(Sc1)]"
REFERENCES,0.624,= Tr(Σc1) + 1
REFERENCES,0.6266666666666667,"K Tr(Σc1).
(27)"
REFERENCES,0.6293333333333333,"From (i) and (ii), and equation 19"
REFERENCES,0.632,ES∼D⊗2kEx∼Dc1[α] = 1
REFERENCES,0.6346666666666667,"K (Tr (Σc2) −Tr (Σc1)) +
 
µc1 −µc2
⊤ 
µc1 −µc2

.
(28)"
REFERENCES,0.6373333333333333,"Since Ec1,c2,x,S[α] = Ec1,c2∼τ[ES∼D⊗2kEx∼Dc1[α]],"
REFERENCES,0.64,"Ec1,c2,x,S [α] = Ec1,c2∼τ  1"
REFERENCES,0.6426666666666667,"K (Tr(Σc2) −Tr(Σc1)) +
 
µc1 −µc2
⊤ 
µc1 −µc2
"
REFERENCES,0.6453333333333333,"= Ec1,c2∼τ
h 
µc1 −µc2
⊤ 
µc1 −µc2
i"
REFERENCES,0.648,"= Ec1,c2∼τ

µ⊤
c1µc1 + µ⊤
c2µc2 −µ⊤
c1µc2 −µ⊤
c2µc1
"
REFERENCES,0.6506666666666666,"= Tr (Σ) + µ⊤µ + Tr (Σ) + µ⊤µ −2µ⊤µ
= 2Tr (Σ) .
(29)"
REFERENCES,0.6533333333333333,"Next we prove the following Lemma 6 related to the conditioned variance of α.
Lemma 6. Under the same notation and assumptions as Theorem 1,"
REFERENCES,0.656,"Ec1,c2Varx∼Dc1,S∼D⊗2N [α] ≤4"
REFERENCES,0.6586666666666666,"K Ec∼τVar
h
∥ϕ(x)∥2i
+ 4"
REFERENCES,0.6613333333333333,"K Varc∼τ [Tr(Σc)] + Vwit(Στ, Σ, µ), (30) where"
REFERENCES,0.664,"Vwit (Στ, Σ, µ) = 8"
REFERENCES,0.6666666666666666,"K (Tr(Στ))
 
Tr(Σ) + µ⊤µ

+ 4
 
Tr(Σ) + µ⊤µ
2 .
(31)"
REFERENCES,0.6693333333333333,"Proof. We start with the inequality between the variance of 2 random variables.
We define
Cov(A, B) as covariance of 2 random variables A, B."
REFERENCES,0.672,"Var[A + B] = Var[A] + Var[B] + 2Cov(A, B)"
REFERENCES,0.6746666666666666,"≤Var[A] + Var[B] + 2
p"
REFERENCES,0.6773333333333333,Var[A]Var[B]
REFERENCES,0.68,≤Var[A] + Var[B] + 2 · Var[A] + Var[B]
REFERENCES,0.6826666666666666,"2
= 2Var[A] + 2Var[B].
(32)"
REFERENCES,0.6853333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.688,"For Varx∼Dc1,S[α],"
REFERENCES,0.6906666666666667,"Varx∼Dc1,S[α] = Var
ϕ(x) −ϕ(Sc2)

2
−
ϕ(x) −ϕ(Sc1)

2"
REFERENCES,0.6933333333333334,"= Var
ϕ(Sc1)

2
−
ϕ(Sc2)

2
−2ϕ(x)⊤"
REFERENCES,0.696,"ϕ(Sc2) −ϕ(Sc1)
"
REFERENCES,0.6986666666666667,"≤2Var
ϕ(Sc1)

2
−
ϕ(Sc2)

2"
REFERENCES,0.7013333333333334,"+ 4Var
h
ϕ(x)⊤"
REFERENCES,0.704,"ϕ(Sc2) −ϕ(Sc1)
i
(∵equation 32)"
REFERENCES,0.7066666666666667,"= 2Var
ϕ(Sc1)

2
+ 2Var
ϕ(Sc2)

2
+ 4Var
h
ϕ(x)⊤(ϕ(Sc2) −ϕ(Sc1))
i
. (33)"
REFERENCES,0.7093333333333334,"From 3rd line to 4th line we deompose the variance of
ϕ(Sc1)

2
−
ϕ(Sc2)

2
use the independence"
REFERENCES,0.712,"of ϕ(Sc1) and ϕ(Sc2) with their class given. Next we focus on Var
h
ϕ(x)⊤(ϕ(Sc2) −ϕ(Sc1))
i
."
REFERENCES,0.7146666666666667,"Var
h
ϕ(x)⊤(ϕ(Sc2) −ϕ(Sc1))
i
= E

ϕ(x)⊤"
REFERENCES,0.7173333333333334,"ϕ(Sc2) −ϕ(Sc1)
2"
REFERENCES,0.72,"−

E [ϕ(x)]⊤E
h"
REFERENCES,0.7226666666666667,"ϕ(Sc2) −ϕ(Sc1)
i2"
REFERENCES,0.7253333333333334,"≤E

∥ϕ(x)∥2 ϕ(Sc2) −ϕ(Sc1)

2
−(µc1⊤(µc2 −µc1))2"
REFERENCES,0.728,"= E
h
∥ϕ(x)∥2i
E
ϕ(Sc2) −ϕ(Sc1))

2
−
 
µ⊤
c1
 
µc2 −µc1
2 . (34)"
REFERENCES,0.7306666666666667,"From the 2nd line to the 3rd line we use Cauchy–Schwarz inequality. For E
ϕ(Sc2) −ϕ(Sc1)

2
,"
REFERENCES,0.7333333333333333,with equation 20
REFERENCES,0.736,"E
ϕ(Sc2) −ϕ(Sc1)

2
= 1"
REFERENCES,0.7386666666666667,"K (Tr (Σc1) + Tr (Σc2)) +
 
µc2 −µc1
⊤ 
µc2 −µc1

.
(35)"
REFERENCES,0.7413333333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.744,"Thus Ec1,c2∼τ"
REFERENCES,0.7466666666666667,"
Ex∼Dc1,S
h
∥ϕ(x)∥2i
Ex∼Dc1,S"
REFERENCES,0.7493333333333333,"ϕ(Sc2) −ϕ(Sc1)

2
is calculated as follows."
REFERENCES,0.752,"Ec1,c2∼τ"
REFERENCES,0.7546666666666667,"
Ex∼Dc1,S
h
∥ϕ(x)∥2i
Ex∼Dc1,S"
REFERENCES,0.7573333333333333,"ϕ(Sc2) −ϕ(Sc1)

2"
REFERENCES,0.76,"= Ec1,c2∼τ"
REFERENCES,0.7626666666666667,"
(Tr(Σc1) + µ⊤
c1µc1)
 1"
REFERENCES,0.7653333333333333,"K (Tr(Σc1) + Tr(Σc2)) + (µc2 −µc1)⊤(µc2 −µc1)
"
REFERENCES,0.768,"= Ec1,c2∼τ  1 K"
REFERENCES,0.7706666666666667,"
Tr (Σc1)2 + Tr(Σc1) Tr (Σc2)
"
REFERENCES,0.7733333333333333,"+ Ec1,c2∼τ  2"
REFERENCES,0.776,"K (Tr (Στ)) µ⊤
c1µc1 + µ⊤
c1µc1
 
µc2 −µc1
⊤ 
µc2 −µc1
 = 1 K"
REFERENCES,0.7786666666666666,"
Ec1,c2∼τ

Tr(Σc1)2
+ Ec1,c2∼τ [Tr(Σc1)]2 + 2"
REFERENCES,0.7813333333333333,"K (Tr (Στ))
 
Tr (Σ) + µ⊤µ

+ E

µ⊤
c1µc1(µc2 −µc1)⊤(µc2 −µc1)
 = 1"
REFERENCES,0.784,K Varc∼τ [Tr (Σc)] + 2
REFERENCES,0.7866666666666666,K Tr (Στ)2 + 2
REFERENCES,0.7893333333333333,"K (Tr (Στ))
 
Tr (Σ) + µ⊤µ

+ Ec1,c2
h
µ⊤
c1µc1
 
µc2 −µc1
⊤ 
µc2 −µc1
i = 1"
REFERENCES,0.792,K Varc∼τ [Tr(Σc)] + 2
REFERENCES,0.7946666666666666,"K Tr(Στ)
 
Tr(Στ) + Tr(Σ) + µ⊤µ

+ Ec1,c2
h
µ⊤
c1µc1
 
µc2 −µc1
⊤ 
µc2 −µc1
i
. (36)"
REFERENCES,0.7973333333333333,"Now we take into account the term −(µc1⊤(µc2 −µc1))2 in equation 34,"
REFERENCES,0.8,"Ec1,c2

µ⊤
c1µc1(µc2 −µc1)⊤(µc2 −µc1) −(µ⊤
c1(µc2 −µc1))2"
REFERENCES,0.8026666666666666,"= Ec1,c2

µ⊤
c1µc1µ⊤
c2µc2 −(µ⊤
c1µc2)2"
REFERENCES,0.8053333333333333,"≤Ec1,c2

µ⊤
c1µc1µ⊤
c2µc2
"
REFERENCES,0.808,"=
 
Tr(Σ) + µ⊤µ
2 .
(37)"
REFERENCES,0.8106666666666666,"Thus Ec1,c2
h
Var[ϕ(x)⊤(ϕ(Sc2) −ϕ(Sc1))]
i
is calculated as follows."
REFERENCES,0.8133333333333334,"Ec1,c2
h
Var[ϕ(x)⊤(ϕ(Sc2) −ϕ(Sc1))]
i
(38) = 1"
REFERENCES,0.816,K Varc∼τ [Tr(Σc)] + 2
REFERENCES,0.8186666666666667,"K Tr(Στ)
 
Tr(Στ) + Tr(Σ) + µ⊤µ

+ (Tr(Σ) + µ⊤µ)2.
(39)"
REFERENCES,0.8213333333333334,"Regarding
ϕ(Sc)

2
, since the function computing square norm is convex, next equation holds with
D-dimensional Jensen’s inequlality (Perlman, 1974):"
REFERENCES,0.824,"ϕ(Sc)

2
= "
K,0.8266666666666667,"1
K X"
K,0.8293333333333334,"i=0
x∈Sc ϕ(x)  2 ≤1 K  X"
K,0.832,"i=0
x∈Sc ϕ(x)"
K,0.8346666666666667,".
(40)"
K,0.8373333333333334,Under review as a conference paper at ICLR 2022
K,0.84,"Combining equation 33, equation 38, and equation 40 we obtain"
K,0.8426666666666667,"Ec1,c2Varx∼Dc1,S∼D⊗2N [α] = 4"
K,0.8453333333333334,"K Ec∼τVarx∼Dc
h
∥ϕ(x)∥2i
+ 4"
K,0.848,K Varc∼τ [Tr(Σc)] + 8
K,0.8506666666666667,"K Tr(Στ)
 
Tr(Στ) + Tr(Σ) + µ⊤µ

+ 4
 
Tr(Σ) + µ⊤µ
2 .
(41)"
K,0.8533333333333334,"To complete the proof of Theorem 1, we calculate Ec1,c2Ex∼Dc1,S∼D⊗2K[α]2."
K,0.856,"Ec1,c2Ex∼Dc1,S∼D⊗2K[α]2
(42)"
K,0.8586666666666667,"= Ec1,c2 "" 1"
K,0.8613333333333333,"K (Tr(Σc1) −Tr(Σc2)) + (µc1 −µc2)⊤(µc1 −µc2)
2#"
K,0.864,"= Ec1,c2 "" 1"
K,0.8666666666666667,"K (Tr(Σc1) −Tr(Σc2))
2#"
K,0.8693333333333333,"+ Ec1,c2
h 
(µc1 −µc2)⊤(µc1 −µc2)
2i"
K,0.872,"+ 2Ec1,c2  1"
K,0.8746666666666667,"K (Tr(Σc1) −Tr(Σc2))
  
(µc1 −µc2)⊤(µc1 −µc2)
"
K,0.8773333333333333,"= Varc1,c2∼τ  1"
K,0.88,"K (Tr(Σc1) −Tr(Σc2))

+

Ec1,c2∼τ  1"
K,0.8826666666666667,"K (Tr(Σc1) −Tr(Σc2))
2"
K,0.8853333333333333,"+ Ec1,c2
h 
(µc1 −µc2)⊤(µc1 −µc2)
2i"
K,0.888,"=
2
K2 Varc∼τ [Tr(Σc)] + Ec1,c2
h 
(µc1 −µc2)⊤(µc1 −µc2)
2i
.
(43)"
K,0.8906666666666667,"From 2nd line to 3rd line, we use the symmetry of the last term with respect to c1 and c2 and erase
the term. Combining equation 13, Lemma 5, Lemma 6, and equation 42, we obtain the bound."
K,0.8933333333333333,"A.5
THEOREM 1 WITH N-WAY CLASSIFICATION"
K,0.896,The upper bound on the risk of N-way prototype classifier is as follows.
K,0.8986666666666666,"Theorem 7. Let operation of binary class prototype classifier M as defined in equation 1. Then
for ϕ(Sc) =
1
K Σx∈Scϕ(x), µc = Ex∼Dc[ϕ(x)], Σc = Ex∼Dc[(ϕ(x) −µc)(ϕ(x) −µc)⊤], µ =
Ec∼τ[µc],Σ = Ec∼τ[(µc −µ)(µc −µ)⊤], Ec∼τ[Σc] = Στ, if ϕ(x) has its fourth moment, miss
classification risk of binary class prototype classifier RM satisfy"
K,0.9013333333333333,"R(M(ϕ, x, {Si}N
i=1), y)"
K,0.904,"≤N −1 − N
X"
K,0.9066666666666666,"c=1
c̸=y"
K,0.9093333333333333,4(Tr(Σ))2
K,0.912,"EV[hL2(ϕ(x))] + VTr(Σy) + Vwit(Στ, Σ, µ) + E dist2
L2(µy, µc),
(44) where"
K,0.9146666666666666,EV[hL2(ϕ(x))] = 4
K,0.9173333333333333,"K Ey∼τ
h
Varxc∼Dc
h
∥ϕ(x)∥2ii
,"
K,0.92,"VTr(Σy) =
 4 K + 2 K2"
K,0.9226666666666666,"
Varc∼τ [Tr (Σc)] ,"
K,0.9253333333333333,"Vwit(Στ, Σ, µ) = 8"
K,0.928,"K Tr(Στ)
 
Tr(Στ) + Tr(Σ) + µ⊤µ

+ 4(Tr(Σ) + µ⊤µ)2,"
K,0.9306666666666666,"E dist2
L2(µy, µc) =Ey,c"
K,0.9333333333333333," 
µcy −µc
⊤(µy −µc)
2
."
K,0.936,Under review as a conference paper at ICLR 2022
K,0.9386666666666666,"Table 3: Classification accuracies on miniImageNet of ProtoNet , linear evaluation meth-
ods (Chen et al., 2019), and ours. The best performing methods and any other runs within
95% confidence margin are in bold."
K,0.9413333333333334,"Method
ResNet12
ResNet18
1-shot
5-shot
1-shot
5-shot
ProtoNet
53.48 ± 0.89%
73.56 ± 0.65%
56.26 ± 0.85%
74.02 ± 0.65%
B@FT
54.54 ± 0.80%
76.50 ± 0.62%
55.41 ± 0.82%
76.95 ± 0.61%
B+@FT
56.33 ± 0.81%
74.62 ± 0.65%
55.07 ± 0.81%
74.15 ± 0.66%
B@CL2
58.66 ± 0.83%
75.98 ± 0.62%
57.67 ± 0.83%
70.78 ± 0.67%
B+@CL2
57.50 ± 0.81%
74.00 ± 0.61%
57.00 ± 0.64%
74.06 ± 0.61%
B
46.36 ± 0.58%
73.97 ± 0.62%
43.86 ± 0.80%
72.36 ± 0.92%
B@L2-N
57.18 ± 0.80%
77.12 ± 0.44%
56.57 ± 0.80%
76.44 ± 0.61%
B@V-N
−
63.55 ± 0.49%
−
62.78 ± 1.02%
B@V-N+L2-N
−
65.42 ± 0.64%
−
64.09 ± 0.67%
B@LDA
−
73.75 ± 0.64%
−
73.54 ± 0.85%
B@LDA+L2-N
−
74.80 ± 0.64%
−
73.70 ± 0.62%
B@EST
51.28 ± 0.88%
72.80 ± 0.64%
44.19 ± 0.82%
72.99 ± 0.63%
B@EST+L2-N
58.00 ± 0.86%
76.90 ± 0.62%
56.39 ± 0.79%
76.24 ± 0.64%
B+
41.18 ± 0.76%
73.97 ± 0.66%
36.80 ± 0.76%
63.76 ± 0.71%
B+@L2-N
57.96 ± 0.83%
75.38 ± 0.61%
57.21 ± 0.83%
74.89 ± 0.65%
B+@V-N
−
55.96 ± 0.63%
−
54.31 ± 0.64%
B+@V-N+L2-N
−
56.20 ± 0.64%
−
55.42 ± 0.68%
B+@LDA
−
69.60 ± 0.70%
−
74.38 ± 0.65%
B+@LDA+L2-N
−
74.34 ± 0.63%
−
74.38 ± 0.63%
B+@EST
47.11 ± 0.81%
69.01 ± 0.63%
47.21 ± 0.77%
69.11 ± 0.64%
B+@EST+L2-N
58.32 ± 0.81%
77.49 ± 0.63%
57.00 ± 0.75%
77.13 ± 0.64%"
K,0.944,"Proof. Let x, y be the input and its class of a query data. We define αc by αc =
ϕ(x) −ϕ(Sc)

2
−
ϕ(x) −ϕ(Sy)

2
. Then a prototype classifier miss-classify a class of input x, ˆy, if ∃c ∈[1, N], c ̸="
K,0.9466666666666667,"y, αc < 0 . Hence: RM(ϕ) = Pr(SN
c=1
c̸=y αc < 0)"
K,0.9493333333333334,"By Frechet’s inequality, next equation holds:"
K,0.952,"RM(ϕ) ≤ N
X"
K,0.9546666666666667,"c=1
c̸=y"
K,0.9573333333333334,Pr(αi < 0).
K,0.96,"Noting that Theorem 1 can be applied to each term in the summation and then we obtain Theorem
7"
K,0.9626666666666667,"A.6
DETAILED PERFORMANCE RESULTS"
K,0.9653333333333334,"We show the detailed performance results in this section with 95% confidence margin. We add
the two future normalization methods, variance normalization before L2-normalization (V-N+L2-
N) and LDA before L2-normalization (LDA+L2-N) to the tables. Table 3 and Table 4 show the
performance on miniImagenet and tieredImagenet; table 5 and table 6 show the performance on
CIFARFS and FC100."
K,0.968,"A.7
THE COMPARISON OF THE RATIO OF THE BETWEEN-CLASS VARIANCE TO THE
WITHIN-CLASS VARIANCE BEFORE AND AFTER APPLYING L2-NORM"
K,0.9706666666666667,"We show the ratio of the between-class variance to the within-class variance before and after ap-
plying L2-norm in Figure 4. We calculated the ratio of the between-class variance to each class’s
variance and averaged over the test classes of each dataset. Although L2-normalization is expected
to reduce Tr(Σ) rather than Σc according to 4.4, the figure shows L2-norm marginaly reduces the
ratio in CIFARFS dataset and the ratio does not changed so much in the other dataset."
K,0.9733333333333334,Under review as a conference paper at ICLR 2022
K,0.976,"Table 4: Classification accuracies on tieredImageNet of ProtoNet , linear evaluation meth-
ods (Chen et al., 2019), and ours. The best performing methods and any other runs within
95% confidence margin are in bold."
K,0.9786666666666667,"Method
ResNet12
ResNet18
1-shot
5-shot
1-shot
5-shot
ProtoNet
55.40 ± 0.98%
77.67 ± 0.70%
60.50 ± 1.01%
81.40 ± 0.68%
B@FT
61.67 ± 0.92%
81.62 ± 0.64%
63.38 ± 0.91%
83.18 ± 0.64%
B+@FT
63.02 ± 0.91%
81.07 ± 0.69%
64.20 ± 0.92%
81.62 ± 0.69%
B@CL2
64.88 ± 0.86%
80.42 ± 0.64%
65.26 ± 0.88%
81.63 ± 0.64%
B+@CL2
63.31 ± 0.91%
79.19 ± 0.68%
65.67 ± 0.91%
81.41 ± 0.68%
B
50.60 ± 0.87%
78.10 ± 0.67%
56.16 ± 0.89%
80.33 ± 0.66%
B@L2-N
64.04 ± 0.89 %
81.73 ± 0.66%
65.19 ± 0.87%
82.93 ± 0.66%
B@V-N
−
66.08 ± 0.69%
−
75.56 ± 0.75%
B@V-N+L2-N
−
66.85 ± 0.70%
−
76.66 ± 0.67%
B@LDA
−
77.44 ± 0.74%
−
80.85 ± 0.67%
B@LDA+L2-N
−
81.01 ± 0.69%
−
81.48 ± 0.68%
B@EST
53.90 ± 0.94%
78.09 ± 0.67%
57.12 ± 0.93%
80.59 ± 0.64%
B@EST+L2-N
64.54 ± 0.91%
81.40 ± 0.64%
64.71 ± 0.91%
83.24 ± 0.67%
B+
46.52 ± 0.87%
73.85 ± 0.70%
48.27 ± 0.91%
75.87 ± 0.71%
B+@L2-N
64.96 ± 0.90%
81.08 ± 0.69%
65.40 ± 0.94%
82.49 ± 0.68%
B+@V-N
−
65.19 ± 0.79%
−
64.33 ± 0.77%
B+@V-N+L2-N
−
66.57 ± 0.74%
−
67.15 ± 0.74%
B+@LDA
−
72.88 ± 0.74%
−
75.55 ± 0.77%
B+@LDA+L2-N
−
79.57 ± 0.72%
−
81.98 ± 0.67%
B+@EST
52.01 ± 0.90%
74.26 ± 0.70%
53.49 ± 0.90%
75.38 ± 0.71%
B+@EST+L2-N
57.63 ± 0.88%
80.99 ± 0.72%
60.87 ± 1.01%
81.80 ± 0.67%"
K,0.9813333333333333,"Table 5: Classification accuracies on CIFARFS of ProtoNet, linear evaluation methods
(Chen et al., 2019), and ours. The best performing methods and any other runs within
95% confidence margin are in bold."
K,0.984,"Method
ResNet12
ResNet18
1-shot
5-shot
1-shot
5-shot
ProtoNet
58.65 ± 1.07%
75.33 ± 0.73%
62.05 ± 0.97%
78.25 ± 0.71 %
B@FT
55.97 ± 0.86%
76.50 ± 0.74%
56.46 ± 0.84%
77.43 ± 0.66%
B+@FT
61.08 ± 0.95%
76.15 ± 0.75%
61.34 ± 0.93%
77.86 ± 0.71%
B@CL2
58.61 ± 0.86%
74.73 ± 0.74%
58.09 ± 0.84%
76.52 ± 0.71%
B+@CL2
61.59 ± 0.91%
76.10 ± 0.73%
63.17 ± 0.94%
77.49 ± 0.72%
B
44.12 ± 0.89%
72.47 ± 0.73%
47.62 ± 0.84%
75.66 ± 0.71%
B@L2-N
59.43 ± 0.90%
77.45 ± 0.70 %
58.51 ± 0.86%
77.43 ± 0.69%
B@V-N
−
44.66 ± 1.44%
−
67.72 ± 0.72%
B@V-N+L2-N
−
55.91 ± 0.70%
−
67.70 ± 0.69%
B@LDA
−
72.61 ± 0.70%
−
75.25 ± 0.67%
B@LDA+ L2-N
−
75.73 ± 0.71%
−
77.12 ± 0.71%
B@EST
52.62 ± 0.91%
72.80 ± 0.71%
55.57 ± 0.90%
75.10 ± 0.71%
B@EST+L2-N
60.70 ± 0.95 %
77.04 ± 0.70 %
61.14 ± 0.96%
77.48 ± 0.68%
B+
45.73 ± 0.88%
73.97 ± 0.76%
44.42 ± 0.89%
70.85 ± 0.76%
B+@L2-N
61.07 ± 0.93 %
76.71 ± 0.74%
63.48 ± 0.94%
77.86 ± 0.71%
B+@V-N
−
57.20 ± 0.70%
−
57.16 ± 0.72%
B+@V-N+L2-N
−
57.59 ± 0.69%
−
57.24 ± 0.73%
B+@LDA
−
68.58 ± 0.74%
−
69.86 ± 0.74%
B+@LDA+L2-N
−
76.09 ± 0.75%
−
69.86 ± 0.74%
B+@EST
50.18 ± 0.93%
70.32 ± 0.79%
49.82 ± 0.94%
71.07 ± 0.70%
B+@EST+L2-N
59.83 ± 0.98%
76.32 ± 0.72%
63.00 ± 0.94%
77.99 ± 0.71%"
K,0.9866666666666667,Under review as a conference paper at ICLR 2022
K,0.9893333333333333,"Table 6: Classification accuracies on FC100 of ProtoNet, linear evaluation methods (Chen
et al., 2019), and ours. The best performing methods and any other runs within 95%
confidence margin are in bold."
K,0.992,"Method
ResNet12
ResNet18
1-shot
5-shot
1-shot
5-shot
ProtoNet
35.56 ± 0.77%
51.12 ± 0.71%
36.02 ± 0.70%
51.02 ± 0.72%
B@FT
39.72 ± 0.68%
56.04 ± 0.76%
40.06 ± 0.68%
57.04 ± 0.71%
B+@FT
36.01 ± 0.64%
50.73 ± 0.72%
36.93 ± 0.70%
50.41 ± 0.73%
B@CL2
41.58 ± 0.74%
55.82 ± 0.76%
41.51 ± 0.72%
56.44 ± 0.74%
B+@CL2
37.51 ± 0.71%
50.38 ± 0.72%
38.30 ± 0.74%
51.06 ± 0.70%
B
31.60 ± 0.61%
52.50 ± 0.74%
35.90 ± 0.61%
55.20 ± 0.77%
B@L2-N
40.34 ± 0.71%
56.61 ± 0.71%
40.49 ± 0.71%
57.71 ± 0.76%
B@V-N
−
41.99 ± 0.63%
−
49.69 ± 0.67%
B@V-N+L2-N
−
42.03 ± 0.65%
−
49.99 ± 0.70%
B@LDA
−
52.90 ± 0.72%
−
55.44 ± 0.72%
B@LDA+L2-N
−
55.12 ± 0.70%
−
56.32 ± 0.75%
B@EST
44.45 ± 0.88%
52.97 ± 0.81%
47.49 ± 0.91%
55.68 ± 0.74%
B@EST+L2-N
47.57 ± 0.83%
56.96 ± 0.77%
50.13 ± 0.91%
59.94 ± 0.74%
B+
29.72 ± 0.57%
46.85 ± 0.67%
30.76 ± 0.62%
47.62 ± 0.69%
B+@L2-N
37.16 ± 0.67%
51.10 ± 0.71%
38.55 ± 0.72%
50.15 ± 0.71%
B+@V-N
−
36.76 ± 0.59%
−
40.66 ± 0.64%
B+@V-N+L2-N
−
36.55 ± 0.55%
−
40.16 ± 0.61%
B+@LDA
−
47.10 ± 0.70%
−
47.63 ± 0.69%
B+@LDA+L2-N
−
49.03 ± 0.70%
−
50.60 ± 0.72%
B+@EST
37.60 ± 0.78%
46.81 ± 0.67%
39.92 ± 0.88%
47.65 ± 0.69%
B+@EST+L2-N
40.88 ± 0.83%
50.45 ± 0.72%
41.65 ± 0.85%
50.74 ± 0.70%"
K,0.9946666666666667,Figure 4: We plotted the ratio of the between-class variance to the within-class variance ( Tr(Σc)
K,0.9973333333333333,"Tr(Σ) ) before and
after applying L2-norm averaged over the test classes of each dataset."
