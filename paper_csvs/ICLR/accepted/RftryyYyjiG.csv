Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002531645569620253,"Recent work explored the potential of large-scale Transformer-based pre-trained
models, especially Pre-trained Language Models (PLMs) in natural language pro-
cessing. This raises many concerns from various perspectives, e.g., ﬁnancial costs
and carbon emissions. Compressing PLMs like BERT with negligible performance
loss for faster inference and cheaper deployment has attracted much attention. In
this work, we aim to explore larger compression ratios for PLMs, among which
tensor decomposition is a potential but under-investigated one. Two decomposition
and reconstruction protocols are further proposed to improve the effectiveness and
efﬁciency during compression. Our compressed BERT 1 with 1/7 parameters in
Transformer layers performs on-par with, sometimes slightly better than the origi-
nal BERT in GLUE benchmark. A tiny version achieves 96.7% performance of
BERT-base with 1/48 encoder parameters (i.e., less than 2M parameters exclud-
ing the embedding layer) and 2.7× faster on inference. To show that the proposed
method is orthogonal to existing compression methods like knowledge distillation,
we also explore the beneﬁt of the proposed method on a distilled BERT."
INTRODUCTION,0.005063291139240506,"1
INTRODUCTION"
INTRODUCTION,0.007594936708860759,"Pre-trained Language Models such as BERT (Devlin et al., 2018) and ALBERT (Lan et al., 2019) have
signiﬁcantly improved various NLP tasks with signiﬁcant improvement. Much recent work (Brown
et al., 2020; Narayanan et al., 2021; Fedus et al., 2021) explores the potential of super large-scale
PLMs. However, such large-scale PLMs are both economically and ecologically unfriendly (Bender
et al., 2021; Patterson et al., 2021). Furthermore, deployment of large-scale PLMs is also challenging
since (1) a model cannot be fully deployed or stored in a single GPU server, model parallelism would
consume extra time for network communication among many servers; (2) edge devices may not have
enough space for storing models; (3) the long inference time cannot support real-time feedback."
INTRODUCTION,0.010126582278481013,"Scaling down a model with negligible performance drop would facilitate the real-world applications
of PLMs in a smaller size, faster inference time, and less network communication cost. For example,
recent work explores quantization (Zhang et al., 2020; Bai et al., 2020), weights pruning (Hou et al.,
2020), and knowledge distillation (Jiao et al., 2020; Sanh et al., 2020) for BERT (one of the most
popular PLMs). We argue that existing methods cannot largely compress large-scale PLMs as stated
in Sec. 2. In this paper, we aim to explore extreme parameter compression (i.e., bigger compression
ratios) although they are by deﬁnition challenging."
INTRODUCTION,0.012658227848101266,"The parameter redundancy in PLMs was demonstrated by (Kovaleva et al., 2019; Michel et al.,
2019; Voita et al., 2019; Cordonnier et al., 2021), for which we divide into two groups: intra-
matrix redundancy and inter-matrix redundancy. The former happens in different heads that are"
INTRODUCTION,0.015189873417721518,"∗Benyou and Yuxin contributed to this work equally.
†Lifeng is the corresponding author.
1https://github.com/twinkle0331/Xcompression"
INTRODUCTION,0.017721518987341773,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020253164556962026,"calculated separately, e.g., attentions among heads act on a similar subspace and are therefore low-
rank (Cordonnier et al., 2021) – we relate this phenomenon to the so-called ‘decomposability’ deﬁned
in this paper. Like self-attention layers, decomposability also holds in FFN layers – each FFN layer
could be decomposed to many independent sub-FFNs (as explained in Appendix B). One example of
inter-matrix redundancy happens across different layers, e.g., attention maps among layers might be
similar (Clark et al., 2019; Vig, 2019; Rogers et al., 2020)."
INTRODUCTION,0.02278481012658228,"Exploration of main weight matrices in Transformer layers ﬁnds that these weight matrices are
possible to be approximated in a low-rank manner – evidencing the possible intra-matrix redundancy
and inter-matrix redundancy. We comprehensively analyze and compare different decomposition
methods for parameter compression including matrix decomposition (denoted as II), tensor train
decomposition (Oseledets, 2011) (denoted as III) and Tucker decomposition (De Lathauwer et al.,
2000) (denoted as IV). The fundamental difference between them is as below. II conducts matrix
factorization (e.g., SVD) for each weight matrix thanks to intra-matrix redundancy. Regarding inter-
matrix redundancy, III shares the head and tail matrices while keeping the core matrix individual; IV
introduces ‘matrix bank’ to make parameter scale being nearly constant w.r.t. the number of layers.
It is concluded that Tucker decomposition (IV) is more parameter-efﬁcient than others in terms of
compression ratios. ALBERT (Lan et al., 2019) and III can be considered as special cases of IV."
INTRODUCTION,0.02531645569620253,"The practical challenges of matrix/tensor decomposition for compression are twofold. First, the
decomposition may result in a discrepancy between the raw weights and approximated weights, and
exact decomposition is impossible with large compression ratios. Instead, Knowledge Distillation
(KD) is used on the compressed model to simulate the predictions of the raw model in a loss-aware
manner. Second, reconstruction may lead to additional computation costs. An efﬁcient reconstruction
protocol is implemented by reordering multiplication operations that also preserve the same results."
INTRODUCTION,0.027848101265822784,"The contributions of this work are (1) we propose a formal framework with standardized terminology
to comprehensively discuss matrix/tensor decomposition methods to compress Transformer-based
language models; (2) we adopt tensor decomposition for compressing PLMs which is also faster,
while existing work (Ma et al., 2019; Liu et al., 2021) did not show the potential for speedup in PLMs;
(3) our compressed BERT with 1/7 parameters in Transformer layers performs on-par with the
original BERT in GLUE benchmark. Also, a tiny version achieves 96.7% performance of BERT-base
with only 1/48 parameters in Transformer layers and 2.7× faster on inference. We directly use the
proposed methods on TinyBERT (Jiao et al., 2020) that is purely based on KD, since our work is
complementary to existing compression methods like KD."
RELATED WORK,0.030379746835443037,"2
RELATED WORK"
RELATED WORK,0.03291139240506329,"Compressing PLMs
Although various work was proposed to design a new efﬁcient Trans-
former (Tay et al., 2020), e.g., (Ma et al., 2019; Choromanski et al., 2020; Wang et al., 2020;
Kitaev et al., 2020; Zaheer et al., 2020; Cao et al., 2020), in this paper, we are focusing on the
compression of Transformer-based pre-trained language models. The difference is that the latter
expects to reuse well-trained models, e.g., BERT and GPT (and even GPT3 (Radford et al., 2019) and
PanGu-α (Zeng et al., 2021)) with manageable computing resources, which typically does not change
the original Transformer architecture. Taking BERT, one of the most commonly-used pre-trained
language models, as an example. Existing work explored quantization (Zhang et al., 2020; Bai et al.,
2020), weights pruning (Lagunas et al., 2021), knowledge distillation (Jiao et al., 2020; Sanh et al.,
2020), progressive module replacing (Xu et al., 2020), neural architecture search (Xu et al., 2021;
Yin et al., 2021) and matrix decomposition (Noach & Goldberg, 2020)."
RELATED WORK,0.035443037974683546,"We argue that existing compression methods (see Tab. 1) may be inadequate for extreme parameter
compression, which is under-investigated. The reasons are manifold, ﬁrst, the knowledge distillation-
based method generally learns a new student model from scratch, which cannot inherit too much
knowledge from the teacher model before distillation. Second, some methods have upper bounds of
compression ratio. For example, layer-sharing ALBERT (Lan et al., 2019) shares parameters in L
layers with maximum L times compression. Quantization replaces existing 32-bit parameters with
binary parameters with a maximum of 32 times reduction. Moreover, quantization needs further
hardware support, which is usually ad hoc to speciﬁc platforms. Weight pruning arguably cannot
achieve a big compression ratio (McCarley et al., 2019; Michel et al., 2019)."
RELATED WORK,0.0379746835443038,Published as a conference paper at ICLR 2022
RELATED WORK,0.04050632911392405,"Table 1: The comparison between compression methods for BERT. L is the number of layers and D
is the dimension of hidden states. In the ‘Compressing ratio’ column in Quantization, ‘32’ refers to
that one usually uses 32-bit ﬂoating precision for a real number."
RELATED WORK,0.043037974683544304,"Methods
Examples
Hardware
Compressing
support
ratio"
RELATED WORK,0.04556962025316456,"Knowledge distillation
(Sun et al., 2019; Jiao et al., 2020; Sanh et al., 2020)

O( LD"
RELATED WORK,0.04810126582278481,"ld )
Parameter sharing
(Lan et al., 2019)

L as upper bound
Quantization
(Zhang et al., 2020; Bai et al., 2020)

32 as upper bound
Weight pruning
(Hou et al., 2020)

-
Matrix decomposition
(Lan et al., 2019; Noach & Goldberg, 2020)

O( D d )"
RELATED WORK,0.05063291139240506,"Tensor (Tucker/TT) decomposition
-

O( LD2 ld2 )"
RELATED WORK,0.053164556962025315,"Matrix/tensor decomposition for compression
Tensor/matrix decomposition aims to approxi-
mate a given tensor using a set of smaller tensors/matrices. It has been investigated to compress and
speed up CNNs, RNNs, and Transformers for many years (Lebedev et al., 2014; Yang et al., 2017; Ye
et al., 2018; Denton et al., 2014; Winata et al., 2019). Matrix decomposition (e.g., ALBERT (Lan
et al., 2019) in the embedding layer and (Noach & Goldberg, 2020)) could decrease parameter scale
with a linear factor depending on the selected rank. More advanced tensor decomposition approaches
can be implemented by tensor network, which has recently been used to compress general neural
networks (Gao et al., 2020; Novikov et al., 2015), compress embedding layer (Khrulkov et al., 2019;
Hrinchuk et al., 2020; Panahi et al., 2019)."
RELATED WORK,0.05569620253164557,"Recently, Ma et al. (2019) redesigned a new ‘Self-Attention Network’ (SAN) in Transformer architec-
ture inspired by block-term tensor decomposition. The compression ratio is limited since the majority
of parameters in Transformer comes from another module called ‘Feed-Forward Network’ (FFN)
instead of SAN; moreover, the model does not have the potential for speedup regardless of compres-
sion ratios since the time complexity is closed to vanilla Transformer. Noach & Goldberg (2020)
reparameterized each weight matrix using matrix decomposition and further distill the compressed
models, which nearly achieves a compression ratio of 1.7. Liu et al. (2021) adopted matrix product
operators to reparameterize each group of weight matrices in embedding, SAN, and FFN, and only a
small part of tensors of MPO (called ‘auxiliary tensors’) are ﬁne-tuned in downstream tasks. The
compression ratio of the total parameter is negligible and the inference might be slow. Those works
inspire us to explore in depth extreme parameter compression for large-scale PLMs."
RELATED WORK,0.05822784810126582,"We argue that compression ratio of existing work using matrix/tensor decomposition (Ma et al., 2019;
Liu et al., 2021; Noach & Goldberg, 2020) for PLMs is relatively-small; most of them do not have
speedup effect, limiting their applications in large-scale PLMs. The potential to compress PLMs with
matrix/tensor decomposition is under-investigated. In this work, we adopt tensor decomposition, to
cubically compress the parameters of PLMs."
MOTIVATIONS FOR PARAMETER COMPRESSION,0.060759493670886074,"3
MOTIVATIONS FOR PARAMETER COMPRESSION"
MOTIVATIONS FOR PARAMETER COMPRESSION,0.06329113924050633,"Pre-trained language models are typically a stack of multiple Transformer (Vaswani et al., 2017) layers
that consist of a Self-Attention Network (SAN) module and a Feed-Forward Network (FFN) module,
see App. A for Transformer. Sec. 3.1 will introduce an important property called ‘decomposability’
for SAN and FFN, which indicates each sub-component in SAN or FFN is independently calculated
without interactions between sub-components and they may therefore be redundant."
DECOMPOSABILITY IN TRANSFORMER,0.06582278481012659,"3.1
DECOMPOSABILITY IN TRANSFORMER"
DECOMPOSABILITY IN TRANSFORMER,0.06835443037974684,"A computing module f is decomposable if its sub-components {g1, g2, ⋯gH} could be indepen-
dently calculated without interactions: f(x) = δ(g1(x), g2(x), ⋯, gH(x)). Usually, δ is a simple
operation that has negligible computing cost compared to {gh}. Especially, backpropagation between
sub-components is independent if δ is concatenation or addition. Sub-components in f could be
calculated in parallel without interactions. We will examine if SAN and FFN are decomposable."
DECOMPOSABILITY IN TRANSFORMER,0.07088607594936709,"Decomposability in SAN
Following (Hou et al., 2020), SAN could be decomposed as a sum
of the output of every head. For the query/key/value/output transformations parameterized by"
DECOMPOSABILITY IN TRANSFORMER,0.07341772151898734,Published as a conference paper at ICLR 2022
DECOMPOSABILITY IN TRANSFORMER,0.0759493670886076,"0
64
128
256
384
768
PCA components 0.0 0.5 0.9 1.0"
DECOMPOSABILITY IN TRANSFORMER,0.07848101265822785,captured variance
DECOMPOSABILITY IN TRANSFORMER,0.0810126582278481,average
DECOMPOSABILITY IN TRANSFORMER,0.08354430379746836,"(a) PCA for each single weight matrix
(b) PCA for a pair of matrices along columns"
DECOMPOSABILITY IN TRANSFORMER,0.08607594936708861,"Figure 1: PCA for existing weight block matrices in BERT-base. We got nearly similar results in
Fig. 5 for paired matrices along rows and columns, as shown in App. C."
DECOMPOSABILITY IN TRANSFORMER,0.08860759493670886,"W Q/W K/W V /W O, we divide them as Nh heads: {W
Q
h }Nh, {W K
h }Nh, {W V
h }Nh, {W O
h }Nh.
A single head is calculated as"
DECOMPOSABILITY IN TRANSFORMER,0.09113924050632911,"Atth(X) = Softmax( 1
√"
DECOMPOSABILITY IN TRANSFORMER,0.09367088607594937,"d
XW
Q
h W
K
h
T X
T )XW
V
h W
O
h
T
(1)"
DECOMPOSABILITY IN TRANSFORMER,0.09620253164556962,"Then SAN(X) = ∑
Nh
h=1 Atth(X), indicating SAN is decomposable. Cordonnier et al. (2021) argued
that the attentions among heads learn redundant key/query projections due to independent calculation."
DECOMPOSABILITY IN TRANSFORMER,0.09873417721518987,"Decomposability in FFN
The two weight matrices in FFN, denoted as W In and W Out."
DECOMPOSABILITY IN TRANSFORMER,0.10126582278481013,FFN(X) =
D,0.10379746835443038,"4D
∑"
D,0.10632911392405063,"h=1
GeLU(XW
In
⋅,h + b
In
h )W
Out
h,⋅
+ b
Out
(2)"
D,0.10886075949367088,"As Geva et al. (2020) points out, FFN also operates as a key-value mechanism similar to SAN: one
can consider the input of FFN as a query vector, and two linear layers of FFN as keys and values,
respectively. There may exist some similar key-value pairs that may introduce redundancy."
D,0.11139240506329114,"Remarks on decomposability Decomposability does not necessarily make sub-components being
complementary, especially without orthogonality-like constraints between them. Sub-components
may learn similar patterns, leading to some redundancy (Michel et al., 2019). For example, Hou et al.
(2020) demonstrated that BERT could be pruned in both width and height without performance drop."
D,0.11392405063291139,"Moreover, BERT is over-parameterized in the sense that the number of training examples in down-
stream tasks is much fewer (393k training examples for the biggest task in GLUE, i.e., MNLI)
comparing to the enormous number of parameters (110M parameters for BERT-base) – this was
thought problematic according to the traditional machine learning theory (Vapnik, 2013). We believe
that the pre-trained language models in downstream tasks could be compressed."
EXPLORATION OF AN PRE-TRAINED TRANSFORMER,0.11645569620253164,"3.2
EXPLORATION OF AN PRE-TRAINED TRANSFORMER"
EXPLORATION OF AN PRE-TRAINED TRANSFORMER,0.1189873417721519,"Beyond the decomposability of Transformer, we conduct exploration on a widely-used Transformer-
based PLM, i.e., BERT (Devlin et al., 2018). Technically, we calculate the captured variance ratio by
Principal Component Analysis (PCA), as an indicator to measure the parameter redundancy."
EXPLORATION OF AN PRE-TRAINED TRANSFORMER,0.12151898734177215,"The main weight matrices of Transformer layers are {W Q, W K, W V , W O, W In, W Out} 2. Sec.
2 and Appendix B show that FFN could be separately calculated in a multi-head fashion. We
could sequentially split both {W In} and {W Out} into four groups like {W In
h }h=4 and {W Out
h
}h=4"
EXPLORATION OF AN PRE-TRAINED TRANSFORMER,0.1240506329113924,"2We exclude embedding layer for compression, as (Ben Noach & Goldberg, 2020) did. Note that the lookup
operation in embedding layers is fast; therefore, decomposing embedding will be more time-consuming since it
involves additional computationally-expensive matrix multiplication. Moreover, this paper focuses on the core
components in Transformer, namely SAN and FFN, which are the majority parameters that also increase linearly
with network depth; while the parameter scale in embedding layer is constant w.r.t. network depth."
EXPLORATION OF AN PRE-TRAINED TRANSFORMER,0.12658227848101267,Published as a conference paper at ICLR 2022
EXPLORATION OF AN PRE-TRAINED TRANSFORMER,0.1291139240506329,"Figure 2: The three methods for parameter compression. To compress the raw weights WI, II
decomposes each matrix in WI into small matrices, i.e., two ‘narrow’ matrices and a small square
matrix. III further shares the two ‘narrow’ matrices for all weights. IV introduces a matrix bank for
these small square matrices, making parameter scale nearly constant w.r.t. the number of layers."
EXPLORATION OF AN PRE-TRAINED TRANSFORMER,0.13164556962025317,"respectively. By doing so, we could reconcile all weight matrices to be of same shape (i.e. D × D).
Here, we could get 12 D × D weight blocks for each Transformer layer, 4 for SAN, and 8 for FFN."
EXPLORATION OF AN PRE-TRAINED TRANSFORMER,0.1341772151898734,"In Fig. 1 we could ﬁnd the intra-matrix and inter-matrix redundancy: Figure 1a shows that half
dimensions could capture more than 90% variance of all weight matrices, this conﬁrms our statement
in Sec. 3.1. Furthermore, we also study the redundancy between two matrices by conducting
PCA on the concatenated matrix between two arbitrarily paired weight matrices. See Fig. 1b, half
dimensions could capture nearly 80% variance, which suggests some possibility to compress inter-
matrix redundancy. This inter-matrix redundancy may be twofold: (1) subFFNs are decomposable;
(2) calculations (e.g., attention maps) in different layers may be similar. Regarding the latter,
some existing works like RNNs (Goyal & Bengio, 2020), Neural Ordinary Differential Equations
(ODE) (Chen et al., 2018), and cross-layer sharing ALBERT (Lan et al., 2019) show that it could
work even with the parameter equivalence hypothesis among layers."
A GENERAL FRAMEWORK FOR PARAMETER COMPRESSION,0.13670886075949368,"4
A GENERAL FRAMEWORK FOR PARAMETER COMPRESSION"
A GENERAL FRAMEWORK FOR PARAMETER COMPRESSION,0.13924050632911392,"The observation that the main weight blocks in BERT could be approximated in a low-rank manner
(thanks to the intra-matrix and inter-matrix redundancy) inspires us to use decomposition. Here we
introduce and compare some standard decomposition methods (see App. D) in compressing PLMs."
EXPLORING PARAMETER COMPRESSION,0.14177215189873418,"4.1
EXPLORING PARAMETER COMPRESSION"
EXPLORING PARAMETER COMPRESSION,0.14430379746835442,"In principle, SANs and FFNs could be separately compressed; in this work, we additionally explore
stacking SAN weights and FFN weights together as a uniﬁed protocol since each weight block has an
identical shape (i.e., D × D). The main weight matrices in the j-th Transformer layer are"
EXPLORING PARAMETER COMPRESSION,0.1468354430379747,"W
(j) = [{W
Q, W
K, W
V , W
O} ⊕{W
In
h }
h=4 ⊕{W
Out
h
}
h=4]j ∈R
12×D×D.
(3)"
EXPLORING PARAMETER COMPRESSION,0.14936708860759493,Weights of a L-layers Transformer are stacked as a 3-rd order tensor in R12L×D×D. The original
EXPLORING PARAMETER COMPRESSION,0.1518987341772152,"non-decomposed weights is called I: WI = {W(j)}L
j=1 ∈R12LD2
. Each weight matrix in WI is WI
i =
Wi ∈RD×D. Here, we explore standard decomposition methods including matrix decomposition,
tensor train decomposition (Oseledets, 2011) and Tucker decomposition (De Lathauwer et al., 2000)."
EXPLORING PARAMETER COMPRESSION,0.15443037974683543,"II: matrix decomposition
Motivated by intra-matrix redundancy, one can adopt Matrix decom-
position to factorize/approximate a matrix into some smaller ones. A typical example is singular
value decomposition (SVD), called ‘II-α’, for each D × D matrix Wi ∈WI,"
EXPLORING PARAMETER COMPRESSION,0.1569620253164557,"Wi ≈WII
i = UiΣiVi ∈RD×D
(4)"
EXPLORING PARAMETER COMPRESSION,0.15949367088607594,"One can also drop the diagonal Σi by decomposing it into two parts that are multiplied to Ui and Vi
3, namely Wi ≈UiVi, denoted as ‘II-β’. Ui ∈RD×d and Vi ∈Rd×D and usually d < D. Since the
compression ratio is
D2"
EXPLORING PARAMETER COMPRESSION,0.1620253164556962,"2Dd with reducing the rank from D to d, the preserved rank of the approximated
matrix linearly decreases with the compressing rates."
EXPLORING PARAMETER COMPRESSION,0.16455696202531644,"3By decomposing Σi into two diagonal matrices, each of which has diagonal elements that are the square
root of Σi. By multiplying these two diagonal matrices to Ui and Vi. Wi ≈UiVi ∈RD×D."
EXPLORING PARAMETER COMPRESSION,0.1670886075949367,Published as a conference paper at ICLR 2022
EXPLORING PARAMETER COMPRESSION,0.16962025316455695,Table 2: Overview of methods. The most expensive term in space complexity is in bold.
EXPLORING PARAMETER COMPRESSION,0.17215189873417722,"-
Methods
Space complexity
Difference
Reference"
EXPLORING PARAMETER COMPRESSION,0.17468354430379746,"I
-
12LD2
raw
(Devlin et al., 2018)
II-α
Matrix decomposition
24LDd + 12Ld2
w/ low-rank approximation
-
II-β
Matrix decomposition
24LDd
w/t diagonal matrices
(Ben Noach & Goldberg, 2020)
III
Tensor-train decomposition
12Ld2 + 2Dd
w/ parameter sharing
-
IV
Tucker decomposition
ld2 + 12Ll + 2Dd
w/ matrix bank
-"
EXPLORING PARAMETER COMPRESSION,0.17721518987341772,"III: tensor train decomposition
Inspired by the inter-matrix redundancy, one could expect to
share weights among matrices. The biggest terms in Eq. 4 are Ui and Vi while {Σi} is relatively
small since Σi ∈Rd×d and d is relatively small compared to D. We could share {Ui} and {Vi}
among matrices to save parameters. This results in"
EXPLORING PARAMETER COMPRESSION,0.17974683544303796,"Wi ≈WIII
i = UΣiV ∈RD×D
(5)"
EXPLORING PARAMETER COMPRESSION,0.18227848101265823,"Here, {Σi} are not necessarily diagonal. This results in a tensor-train (TT) decomposition (Oseledets,
2011) 4. One can also consider higher-order TT decomposition (i.e., a longer chain for tensor
multiplications) which could be more parameter-efﬁcient; this often needs to reshape the raw tensor
into a higher-order tensor with heuristics. However, it is more time-consuming and costs more GPU
memory during training, which we leave as future work."
EXPLORING PARAMETER COMPRESSION,0.1848101265822785,"IV: Tucker decomposition
In Eq. 5, the biggest term is the {Σi} ∈R12L×d2
, especially the
number of layers may be large in practice (e.g., L = 24 for BERT-large). To this end, we propose
a ﬁxed-sized matrix bank such that a weight matrix is considered as a linear combination of these
matrices inside the bank, making the parameter scale become nearly a constant with respect to the
number of layers. Namely,
Wi ≈WIV
i = U(PiC)V ∈RD×D
(6)"
EXPLORING PARAMETER COMPRESSION,0.18734177215189873,"where C ∈Rl×d2
is a matrix bank with a size of l, each matrix is assigned with a weight vector
Pi ∈R1×l. ALBERT (Lan et al., 2019) could be considered as a special case of IV, see App. G."
EXPLORING PARAMETER COMPRESSION,0.189873417721519,"4.2
COMPARISON BETWEEN I,II, III, AND IV"
EXPLORING PARAMETER COMPRESSION,0.19240506329113924,"The comparison in parameter scale between these decomposition methods is in Tab. 2. Since D > d
and L > l, we could generally conclude that the parameter scales decrease from I, II, III to IV. We
can observe that marginal parameter cost to add a new layer in IV is nearly 12l, which is negligible
compared to the other parameters. During the inference phase, the terms that do not involve batch
size b or sequence length n could be calculated in an ofﬂine way only once before starting inference,
which costs more storage but gets slightly acceleration – since the main purpose of this work is to
compress models, we ignore it in this work but encourage doing it in speed-sensitive scenarios."
EXTREMELY COMPRESSING BERT USING TENSOR DECOMPOSITION,0.1949367088607595,"5
EXTREMELY COMPRESSING BERT USING TENSOR DECOMPOSITION"
DECOMPOSITION PROTOCOL,0.19746835443037974,"5.1
DECOMPOSITION PROTOCOL"
DECOMPOSITION PROTOCOL,0.2,"IV reduces space complexity from O(12LD2) to O(ld2 + 12Ll + 2Dd) where d < D and l < 12L.
l determines to which degree we want to share Transformer parameters among all modules, a ﬂexible
factor to smoothly transform vanilla BERT to layer-shared BERT (or called ‘ALBERT’ (Lan et al.,
2019)). d determines the expressive power (rank) of each linear transformation (originally D × D)."
DECOMPOSITION PROTOCOL,0.20253164556962025,"The decomposition protocol does not change the raw architecture of BERT, alternatively, it introduces
a new reparameterization of the existing weights. However, the approximated weights WIV usually"
A TENSOR-TRAIN DECOMPOSITION IS TO APPROXIMATE A HIGH-ORDER TENSOR WITH A PRODUCT OF MANY SMALLER THREE-,0.20506329113924052,"4A tensor-train decomposition is to approximate a high-order tensor with a product of many smaller three-
order tensors – except for the ﬁrst and last ones being matrices. Here, for a three-order tensor W ∈R12L×D×D,
it is approximated by W ≈UGV and shape transpose, where U ∈RD×r1, G ∈Rr1×12L×r2, and V ∈Rr2×D.
For a speciﬁc slice of W, Wi ≈UG⋅,i,⋅V . r1 and r2 are the ‘TT ranks’."
A TENSOR-TRAIN DECOMPOSITION IS TO APPROXIMATE A HIGH-ORDER TENSOR WITH A PRODUCT OF MANY SMALLER THREE-,0.20759493670886076,Published as a conference paper at ICLR 2022
A TENSOR-TRAIN DECOMPOSITION IS TO APPROXIMATE A HIGH-ORDER TENSOR WITH A PRODUCT OF MANY SMALLER THREE-,0.21012658227848102,Table 3: Computational complexity with different order of matrix multiplication
A TENSOR-TRAIN DECOMPOSITION IS TO APPROXIMATE A HIGH-ORDER TENSOR WITH A PRODUCT OF MANY SMALLER THREE-,0.21265822784810126,"-
Multiplication order
Computing complexity"
A TENSOR-TRAIN DECOMPOSITION IS TO APPROXIMATE A HIGH-ORDER TENSOR WITH A PRODUCT OF MANY SMALLER THREE-,0.21518987341772153,"IV-1
X(U(PiC)V )
O(bnD2 + Dd2 + D2d + ld2)
IV-2
(XU)(PiC)V
O(2bnDd + bnd2 + ld2)
IV-3
(XU)((PiC)V )
O(2bnDd + Dd2 + ld2)"
A TENSOR-TRAIN DECOMPOSITION IS TO APPROXIMATE A HIGH-ORDER TENSOR WITH A PRODUCT OF MANY SMALLER THREE-,0.21772151898734177,"are not exactly equal to the raw weights WI. Moreover, the tiny decomposition discrepancy of
weight matrices in low-layer may lead to an accumulated difference in the ﬁnal output due to the
multiple-layer neural network architecture 5. In this work, we propose to use knowledge distillation
to simulate the ﬁnal output of raw models."
A TENSOR-TRAIN DECOMPOSITION IS TO APPROXIMATE A HIGH-ORDER TENSOR WITH A PRODUCT OF MANY SMALLER THREE-,0.22025316455696203,"fWI(x) ≈fWIV(x)
(7)"
A TENSOR-TRAIN DECOMPOSITION IS TO APPROXIMATE A HIGH-ORDER TENSOR WITH A PRODUCT OF MANY SMALLER THREE-,0.22278481012658227,"fWI is the raw BERT model and fWIV is the compressed one. We argue that approximation in
prediction (like knowledge distillation in Eq. 7) is more important than approximation in weights.
Such a loss-aware strategy in compression could be found in quantization (Hou et al., 2018)."
RECONSTRUCTION PROTOCOL,0.22531645569620254,"5.2
RECONSTRUCTION PROTOCOL"
RECONSTRUCTION PROTOCOL,0.22784810126582278,"A slice of D × D parameter block is represented as matrix product, WIV
i
≈U(PiC)V ∈RD×D.
For an input X ∈Rb×n×D where b is the batch size and n is the sequence length, an output of linear
transformation between X and a D × D parameter block will be Y = XWi;; = XU(PiC)V . Since
matrix multiplication is associative 6, different multiplication order will not affect the ﬁnal result but
their computational complexity may be different 7. One can see the computational complexity for
multiplication order in Tab. 3. In practice, the batch size b will be set as big as possible to increase
data throughput and make training more stable, we could conclude that IV-3 is more efﬁcient than
IV-2. IV-3 is more efﬁcient than IV-1 when D > 2d; in practice, D is typically much bigger than d
and D > 2d. In conclusion, setting IV-3 is most efﬁcient in this scenario."
EXPERIMENTS,0.23037974683544304,"6
EXPERIMENTS"
SETTINGS,0.23291139240506328,"6.1
SETTINGS"
SETTINGS,0.23544303797468355,"Decomposition
For BERT-base (L = 12, D = 768, ), WI ∈R144D2
is decomposed into a core
tensor and three factor matrices (see Fig. 2), its reconstruction could be seen in Sec. 5.2."
SETTINGS,0.2379746835443038,"Knowledge distillation
As (Jiao et al., 2020; Zhang et al., 2020; Bai et al., 2020) did, we use
two-stage knowledge distillation for the compressed model. At General Distillation (GD) stage,
we adopt Knowledge Distillation (KD) for the compressed model to simulate the last-layer hidden
states and last-layer attention maps of the general teacher model (BERT-base). At the second stage,
we adopt Task-speciﬁc Distillation (TD) to simulate the logits of a task-speciﬁc BERT model (e.g.,
ﬁne-tuned on MNLI task). In GD, compressed models are trained with two epochs. In TD, we also
augment training data by randomly replacing a random word with a similar word according to either
word vector similarity using Glove (Pennington et al., 2014) or the predicted logistics of BERT when
masking the target word, see more details in (Jiao et al., 2020)."
SETTINGS,0.24050632911392406,"GLUE evaluation
GLUE (Wang et al., 2018) (see App. I for more details) includes datasets for
single document classiﬁcation and sentence pair classiﬁcation. Fine-tuning and evaluation on GLUE
follows the settings from Huggingface (Wolf et al., 2019). The best-performed model is selected
according to the dev set, where we select the learning rate in [1e-5, 2e-5] and batch size in [16, 32]."
SETTINGS,0.2430379746835443,"5Our experiments also show that a direct decomposition results in very low performance, see Tab. 6.
6For a sequence of matrices (e.g., [A, B, C]), matrix multiplication with different calculation orders results
in a identical result, i.e., (AB)C = A(BC)
7In this paper, we deﬁne the computational complexity of a matrix multiplication between a n × m matrix
and a m × p matrix as O(nmp), corresponding to the number of performed multiplication operations."
SETTINGS,0.24556962025316456,Published as a conference paper at ICLR 2022
SETTINGS,0.2481012658227848,"Table 4: Experimental results on test set in GLUE. ‘Para.’ counts the parameters in encoder layers,
excluding the embedding layer and prediction layer; Note the compression ratios will become smaller
when considering parameters in the embedding layer. Requests Per Second (RPS) is Throughput
calculated by a single Nvidia V100 GPU (16G) using full GPU memory, see App. J for actual
inference time. The single numeric sufﬁx in BERT-III is the dimension rank d; the two numeric
sufﬁxes in BERT-IV correspond to the layer rank l and dimension rank d in IV respectively. The
evaluation metrics follow the ofﬁcial GLUE benchmark (Wang et al., 2018). The best performance of
each task is bold. See App. L for the tailored comparison with (Ben Noach & Goldberg, 2020) since
(Ben Noach & Goldberg, 2020) used nonstandard evaluation metrics in GLUE. (Lan et al., 2019)
and (Mao et al., 2020) did not use all tasks in GLUE, we use ♠, ♥, and ♣to calculate the average
for their selected tasks. ‘†’ means that these methods have a same architecture that has identical
parameters, FLOPS, and RPS."
SETTINGS,0.25063291139240507,"Model (our models in bold)
Para.
FLOPS RPS
SST-2 MNLI
MRPC QNLI QQP RTE STS-B
Avg
acc
acc
F1
acc
F1
acc
spear.
all
♠
♥
♣
BERT-base (Devlin et al., 2018) ( BERT-I )
86.0M 22.5B
420.1
93.4
83.9/83.4
87.5
90.9
71.1
66.4 85.2
82.7 88.7 83.5 84.5
BERT-III -384
23.0M 22.5B
452.2
93.4
84.6/83.7
88.1
90.5
71.9
68.1 83.9
83.2 89.0 84.0 84.8
BERT-III -64
1.8M
4.3B
1143.6 91.9
80.1/79.6
85.5
87.7
70.7
63.3 80.7
80.0 86.0 81.0 82.0
BERT-IV -72-384
12.3M 22.5B
452.3
93.1
83.9/83.2
87.5
90.2
71.6
67.3 83.6
82.6 88.5 83.5 84.4
BERT-IV -36-256
3.9M
15.2B
596.9
92.7
82.5/81.8
87.1
88.9
71.4
65.2 81.8
81.4 87.6 82.3 83.5
BERT-IV -36-128
1.9M
8.0B
863.0
92.4
81.1/80.2
86.5
88.3
71.9
64.4 81.4
80.8 86.8 81.7 82.8
ALBERT (Lan et al., 2019) ♠
7.6M
22.5B
434.0
90.6
82.0/-
-
-
-
-
-
-
86.4
matrix decomposition (Noach & Goldberg, 2020) ( BERT-II) ♥41.8M 14.6B
656.8
92.9
-
-
90.8
-
67.8 -
-
-
83.8 -
matrix decomposition-1 (Mao et al., 2020) ♣
34.5M -
-
87.6
77.7/ 77.4
-
84.3
65.7
-
-
-
-
-
78.5
matrix decomposition-2 (Mao et al., 2020) ♣
17.3M -
-
82.8
71.8/ 71.8
-
75.4
60.3
-
-
-
-
-
72.4
TernaryBERT (Zhang et al., 2020)
86.0M -
-
93.4
83.1/82.5
86.9
90.2
71.0
68.9 83.1
82.4
BinaryBERT (Bai et al., 2020)
86.0M -
-
91.9
84.1/83.5
85.9
89.8
71.6
67.3 82.3
82.0
BERT- 6layer†
43.5M 11.3B
837.2
90.7
80.4 / 79.7 85.9
86.7
69.2
63.6 80.0
79.6
Vanilla KD (Hinton et al., 2015)†
43.5M 11.3B
837.2
91.5
80.2 / 79.8 86.2
88.3
70.1
64.7 80.3
80.1
BERT-PKD (Sun et al., 2019)†
43.5M 11.3B
837.2
92.0
81.5 / 81.0 85.0
89.0
70.7
65.5 81.6
80.8
BERT-of-Theseus (Xu et al., 2020)†
43.5M 11.3B
837.2
92.2
82.4 / 82.1 87.6
89.6
71.6
66.2 84.1
82.0"
SETTINGS,0.25316455696202533,Table 5: GLUE results on test set TinyBERT-IV and comparison with KD based methods.
SETTINGS,0.25569620253164554,"Model (our models in bold)
Para.
FLOPS
RPS
SST-2
MNLI
MRPC
QNLI
QQP
RTE
STS-B
Avg
-
-
acc
acc
F1
acc
F1
acc
spear.
TinyBERT-6layer
43.5M
11.3B
837.2
93.1
84.6/83.2
87.3
90.4
71.6
70.0
83.7
83.0
TinyBERT-IV-72-384
12.3M
11.3B
899.9
92.0
83.1/82.2
87.7
89.1
71.7
65.3
81.6
81.6
TinyBERT-IV-72-256
6.2M
7.6B
1188.4
92.0
82.7/81.9
86.7
87.9
70.9
65.5
81.0
81.1"
RESULTS,0.2582278481012658,"6.2
RESULTS"
RESULTS,0.2607594936708861,"As shown in Tab. 4, our decomposed BERT with layer rank 144 and dimension rank 384, called ‘
BERT-III-384’, outperforms the BERT-base, with only 1/7 parameters in Transformer layers and
slightly bigger throughout. BERT-IV-72-384 performs on-par with raw BERT, which is slightly
worse than BERT-III-384 due to the smaller size. Observe that a bigger rank (both for layer mode
and dimension mode) usually consistently leads to better performance. BERT-III-64 achieves 96.7%
performance (82.7 vs. 80.0) with only 1/48 parameters of Transformer layers and 2.7× speedup."
RESULTS,0.26329113924050634,"Tab. 4 shows BERT-IVs are smaller than existing parameter sharing method (Lan et al., 2019) and
decomposition method (Noach & Goldberg, 2020; Mao et al., 2020). BERT-III -384/ BERT-IV
-72-384 achieve comparable or even slightly better results than (Noach & Goldberg, 2020). BERT-IV
outperforms (Mao et al., 2020) with a large margin. BERT-IV outperforms ALBERT – the latter
needs training from scratch (1M training steps) while the former does not (less than 0.2M steps)."
RESULTS,0.26582278481012656,"Observe that BERT-III-384 outperforms BERT-IV-72-384 since the former has more parameters
and therefore is more expressive. Note that BERT-III-384 and BERT-IV-d-384 have nearly identical
RPS and inference latency. Note, we take BERT-IV-36-128 as an example to compare with matrix
decomposition (a.k.a, II, which is implemented by Noach & Goldberg (2020) with a rank of 245,
denoted as BERT-IV-245), BERT-IV-36-128 is faster (see RPS in Table 4 and inference time in 8),
smaller, and better-performed (see Table 10 for full performance comparison) than BERT-IV-245,
evidencing the advantage of BERT-IV over matrix decomposition for compression."
RESULTS,0.2683544303797468,"To contextualize BERT-III/BERT-IV with other compression methods like knowledge distillation,
Tab. 4 shows that BERT-III/BERT-IV achieves comparable performance with knowledge distillation
methods (Sun et al., 2019; Xu et al., 2020; Jiao et al., 2020) while with fewer parameters. Our results"
RESULTS,0.2708860759493671,Published as a conference paper at ICLR 2022
RESULTS,0.27341772151898736,"Table 6: Ablation experiments of knowledge distillation (KD) (including GD and TD). The test set of
GLUE with the setting 72-384 is reported. The best performance of each task is bold."
RESULTS,0.2759493670886076,"Setting
SST-2
MNLI
MRPC
QNLI
QQP
RTE
STS-B
Avg
acc
acc
F1
acc
F1
acc
spear.
GD + TD
93.1
83.9/83.2
87.5
90.2
71.6
67.3
83.6
82.6
GD + ﬁnetune
90.9
81.7/80.8
83.8
88.8
69.9
63.6
80.7
80.0
ﬁne-training w/o KD
49.9
35.6/36.5
79.9
50.5
55.0
49.7
11.3
47.3"
RESULTS,0.27848101265822783,Table 7: Experiment results on test of GLUE with SAN and FFN.
RESULTS,0.2810126582278481,"Model
Para.
FLOPS
RPS
SST-2
MNLI
MRPC
QNLI
QQP
RTE
STS-B
Avg
acc
acc
F1
acc
F1
acc
spear."
RESULTS,0.28354430379746837,"BERT-base ( BERT-I)
86.0M
22.5B
420.1
93.4
83.9/83.4
87.5
90.9
71.1
66.4
85.2
82.7
BERT-IV -72-384
12.3M
22.5B
452.3
93.1
83.9/83.2
87.5
90.2
71.6
67.3
83.6
82.6
BERT-IV-FFN-48-384
37.1 M
22.5B
463.7
93.1
84.5/84.1
88.0
90.7
71.9
69.3
83.1
83.1
BERT-IV-SAN-24-384
61.9M
22.5B
419.2
92.9
84.5/83.7
86.0
90.8
71.8
66.9
82.5
82.4"
RESULTS,0.28607594936708863,"is also comparable to quantization methods Zhang et al. (2020); Bai et al. (2020) that use 2-bit or
3-bit weights, and pruning Lagunas et al. (2021) (see Tab. 11 in App.L)."
RESULTS,0.28860759493670884,"To show the proposed method is orthogonal to existing compression methods like pure knowl-
edge distillation, we further explore the proposed method in TinyBERT (Jiao et al., 2020), called
‘TinyBERT-IV’. Tab. 5 shows performance loss to compress TinyBERT (degrading from 83.0 to 81.6
in TinyBERT-IV-72-384) is bigger than compressing raw BERT (degrading from 82.7 to 82.6 in
BERT-IV -72-384). This is probably due to smaller redundancy in TinyBERT compared to BERT."
ANALYSIS,0.2911392405063291,"6.3
ANALYSIS"
ANALYSIS,0.2936708860759494,"Ablation on the necessity of knowledge distillation
Tab. 6 shows that both GD and TD are
essential for an effective decomposed BERT. In particular, the overall performance decreases from
82.6 to 80.0 by removing TD. Note that the model will collapse if we directly take decomposed
BERT for ﬁne-tuning without knowledge distillation."
ANALYSIS,0.29620253164556964,"Decomposition on FFNs or SANs
For FFNs and SANs, we use the half size of matrix bank (i.e.,
24 for SANs and 48 for FFNs) and half dimension rank (i.e., 384) respectively. The two settings are
called ‘BERT-IV-FFN-48-384’ and ‘BERT-IV-SAN-24-384’. Tab. 7 shows that solely compressing
SANs or FFNs could nearly achieve on par with the raw model since smaller compression ratios are
achieved. In detail, FFNs are slightly easier to be compressed even with a big compression ratio
comparing to SANs. It is intriguing to notice that BERT-IV-72-384 outperforms BERT-IV-SAN-24-
384, although the former additionally compresses FFNs and has much fewer parameters. The reason
may be that the size of the matrix bank in BERT-IV-72-384 (i.e., 72), which is shared between FFNs
and SANs, is bigger than its counterpart in BERT-IV-SAN-24-384 (i.e., 24). This can shed some
light on the beneﬁt to stacking FFNs and SANs together, see more discussions in App F."
CONCLUSION AND FUTURE WORK,0.29873417721518986,"7
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.3012658227848101,"To largely compress PLMs, we also comprehensively compare many matrix/tensor decomposition
methods and conclude tensor decomposition has the potential for extreme parameter compression.
We therefore efﬁciently implemented tensor decomposition inspired compression for BERT, thanks
to an efﬁcient reconstruction protocol. To compensate for the decomposition discrepancy, knowledge
distillation is used to simulate the output of raw BERT, which is demonstrated to be essential in
ablation study. Our method with 1/7 parameters could achieve comparable with the original model,
with slight speedup. A tiny version achieves more than 96.7% performance of BERT-base with 1/48
parameters in Transformer layers and 2.7× faster on inference. In the future, we expect compression
of PLMs to shift purely encoder-based language models (e.g., BERT) to decoder language models,
since the latter has been designed as big as we could afford, e.g. GPT3 (Radford et al., 2019). The
potential of the proposed methods for compressing larger model is discussed in App. N. Furthermore,
hybrid methods by mixing knowledge distillation, quantization, parameter sharing, weight pruning,
and matrix/tensor decomposition together are potential in practice."
CONCLUSION AND FUTURE WORK,0.3037974683544304,Published as a conference paper at ICLR 2022
REFERENCES,0.30632911392405066,REFERENCES
REFERENCES,0.30886075949367087,"Abien Fred Agarap. Deep learning using rectiﬁed linear units (relu). arXiv preprint arXiv:1803.08375,
2018."
REFERENCES,0.31139240506329113,"Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin
King. Binarybert: Pushing the limit of bert quantization, 2020."
REFERENCES,0.3139240506329114,"Matan Ben Noach and Yoav Goldberg. Compressing pre-trained language models by matrix decom-
position. In Proceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the Association for
Computational Linguistics and the 10th International Joint Conference on Natural Language Pro-
cessing, pp. 884–889, Suzhou, China, December 2020. Association for Computational Linguistics.
URL https://aclanthology.org/2020.aacl-main.88."
REFERENCES,0.31645569620253167,"Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big?
In Proceedings of the 2021
ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, pp. 610–623, New
York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi:
10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922."
REFERENCES,0.3189873417721519,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165,
2020. URL https://arxiv.org/abs/2005.14165."
REFERENCES,0.32151898734177214,"Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, and Niranjan Balasubramanian. Deformer:
Decomposing pre-trained transformers for faster question answering. CoRR, abs/2005.00697,
2020. URL https://arxiv.org/abs/2005.00697."
REFERENCES,0.3240506329113924,"J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling
via an n-way generalization of “eckart-young” decomposition. Psychometrika, 35(3):283–319,
1970."
REFERENCES,0.3265822784810127,"Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differen-
tial equations. In NeurIPS, 2018."
REFERENCES,0.3291139240506329,"Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, et al. Masked language model-
ing for proteins via linearly scalable long-context transformers. arXiv preprint arXiv:2006.03555,
2020."
REFERENCES,0.33164556962025316,"Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look at?
an analysis of bert’s attention. arXiv preprint arXiv:1906.04341, 2019."
REFERENCES,0.3341772151898734,"Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi.
Multi-head attention:
Col-
laborate instead of concatenate, 2021.
URL https://openreview.net/forum?id=
bK-rJMKrOsm."
REFERENCES,0.3367088607594937,"Lieven De Lathauwer, Moor Bart De, and Vandewalle Joos. A multilinear singular value decomposi-
tion. SIAM journal on Matrix Analysis and Applications, 2000."
REFERENCES,0.3392405063291139,"Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear
structure within convolutional networks for efﬁcient evaluation. arXiv preprint arXiv:1404.0736,
2014."
REFERENCES,0.34177215189873417,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.34430379746835443,"Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure
attention loses rank doubly exponentially with depth. arXiv preprint arXiv:2103.03404, 2021."
REFERENCES,0.3468354430379747,Published as a conference paper at ICLR 2022
REFERENCES,0.3493670886075949,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021."
REFERENCES,0.3518987341772152,"Ze-Feng Gao, Song Cheng, Rong-Qiang He, Z. Y. Xie, Hui-Hai Zhao, Zhong-Yi Lu, and Tao Xiang.
Compressing deep neural networks by matrix product operators. Phys. Rev. Research, 2:023300,
Jun 2020. doi: 10.1103/PhysRevResearch.2.023300. URL https://link.aps.org/doi/
10.1103/PhysRevResearch.2.023300."
REFERENCES,0.35443037974683544,"Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are
key-value memories. arXiv preprint arXiv:2012.14913, 2020."
REFERENCES,0.3569620253164557,"Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.
arXiv preprint arXiv:2011.15091, 2020."
REFERENCES,0.3594936708860759,"Richard A Harshman. Determination and proof of minimum uniqueness conditions for parafac1.
UCLA working papers in phonetics, 22(111-117):3, 1972."
REFERENCES,0.3620253164556962,"Dan Hendrycks and Kevin Gimpel.
Gaussian error linear units (gelus).
arXiv preprint
arXiv:1606.08415, 2016."
REFERENCES,0.36455696202531646,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015."
REFERENCES,0.3670886075949367,"Lu Hou, Quanming Yao, and James T Kwok. Loss-aware weight quantization of deep networks. In
ICLR, 02 2018."
REFERENCES,0.369620253164557,"Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic bert
with adaptive width and depth, 2020."
REFERENCES,0.3721518987341772,"Oleksii Hrinchuk, Valentin Khrulkov, Leyla Mirvakhabova, Elena Orlova, and Ivan Oseledets.
Tensorized embedding layers. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: Findings, pp. 4847–4860, 2020."
REFERENCES,0.37468354430379747,"Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling bert for natural language understanding, 2020."
REFERENCES,0.37721518987341773,"Valentin Khrulkov, Oleksii Hrinchuk, Leyla Mirvakhabova, and Ivan Oseledets. Tensorized embed-
ding layers for efﬁcient model compression. arXiv preprint arXiv:1901.10787, 2019."
REFERENCES,0.379746835443038,"Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. arXiv
preprint arXiv:2001.04451, 2020."
REFERENCES,0.3822784810126582,"Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of
bert. arXiv preprint arXiv:1908.08593, 2019."
REFERENCES,0.3848101265822785,"Franc¸ois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. Block pruning for faster
transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pp. 10619–10629, 2021."
REFERENCES,0.38734177215189874,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019."
REFERENCES,0.389873417721519,"Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky.
Speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. arXiv preprint
arXiv:1412.6553, 2014."
REFERENCES,0.3924050632911392,"Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, ZY Xie, Zhong-Yi Lu, and Ji-Rong Wen. Enabling
lightweight ﬁne-tuning for pre-trained language model compression based on matrix product
operators. arXiv preprint arXiv:2106.02205, 2021."
REFERENCES,0.3949367088607595,"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song.
A tensorized transformer for language modeling. Advances in Neural Information Processing
Systems, 32:2232–2242, 2019."
REFERENCES,0.39746835443037976,Published as a conference paper at ICLR 2022
REFERENCES,0.4,"Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu Zhang,
Yunhai Tong, and Jing Bai. Ladabert: Lightweight adaptation of bert through hybrid model
compression. arXiv preprint arXiv:2004.04124, 2020."
REFERENCES,0.40253164556962023,"JS McCarley, Rishav Chakravarti, and Avirup Sil. Structured pruning of a bert-based question
answering model. arXiv preprint arXiv:1910.06360, 2019."
REFERENCES,0.4050632911392405,"Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in
Neural Information Processing Systems, 32:14014–14024, 2019."
REFERENCES,0.40759493670886077,"Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay
Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al.
Efﬁcient large-scale language model training on gpu clusters. arXiv preprint arXiv:2104.04473,
2021."
REFERENCES,0.41012658227848103,"Matan Ben Noach and Yoav Goldberg. Compressing pre-trained language models by matrix de-
composition. In Proceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the Association
for Computational Linguistics and the 10th International Joint Conference on Natural Language
Processing, pp. 884–889, 2020."
REFERENCES,0.41265822784810124,"Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. Tensorizing neural
networks. arXiv preprint arXiv:1509.06569, 2015."
REFERENCES,0.4151898734177215,"Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientiﬁc Computing, 33(5):
2295–2317, 2011."
REFERENCES,0.4177215189873418,"Aliakbar Panahi, Seyran Saeedi, and Tom Arodz. word2ket: space-efﬁcient word embeddings inspired
by quantum entanglement. arXiv preprint arXiv:1911.04975, 2019."
REFERENCES,0.42025316455696204,"David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,
David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv
preprint arXiv:2104.10350, 2021."
REFERENCES,0.42278481012658226,"Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532–1543, 2014."
REFERENCES,0.4253164556962025,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.4278481012658228,"Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about
how bert works. arXiv preprint arXiv:2002.12327, 2020."
REFERENCES,0.43037974683544306,"Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter, 2020."
REFERENCES,0.43291139240506327,"Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model
compression. arXiv preprint arXiv:1908.09355, 2019."
REFERENCES,0.43544303797468353,"Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv
preprint arXiv:2009.06732, 2020."
REFERENCES,0.4379746835443038,"Ledyard R Tucker. Implications of factor analysis of three-way matrices for measurement of change.
Problems in measuring change, 15:122–137, 1963."
REFERENCES,0.44050632911392407,"Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013."
REFERENCES,0.4430379746835443,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.44556962025316454,"Jesse Vig.
A multiscale visualization of attention in the transformer model.
arXiv preprint
arXiv:1906.05714, 2019."
REFERENCES,0.4481012658227848,Published as a conference paper at ICLR 2022
REFERENCES,0.4506329113924051,"Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head
self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint
arXiv:1905.09418, 2019."
REFERENCES,0.4531645569620253,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461, 2018."
REFERENCES,0.45569620253164556,"Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.4582278481012658,"Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J Barezi, and Pascale Fung. On the
effectiveness of low-rank matrix factorization for lstm model compression.
arXiv preprint
arXiv:1908.09982, 2019."
REFERENCES,0.4607594936708861,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von
Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-the-art
natural language processing. ArXiv, abs/1910.03771, 2019."
REFERENCES,0.46329113924050636,"Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. Bert-of-theseus: Compressing
bert by progressive module replacing. arXiv preprint arXiv:2002.02925, 2020."
REFERENCES,0.46582278481012657,"Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu. Nas-bert: Task-
agnostic and adaptive-size bert compression with neural architecture search. arXiv preprint
arXiv:2105.14444, 2021."
REFERENCES,0.46835443037974683,"Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for video
classiﬁcation. In International Conference on Machine Learning, pp. 3891–3900. PMLR, 2017."
REFERENCES,0.4708860759493671,"Jinmian Ye, Linnan Wang, Guangxi Li, Di Chen, Shandian Zhe, Xinqi Chu, and Zenglin Xu. Learning
compact recurrent neural networks with block-term tensor decomposition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 9378–9387, 2018."
REFERENCES,0.47341772151898737,"Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Autotinybert: Automatic
hyper-parameter optimization for efﬁcient pre-trained language models. In Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5146–5157, 2021."
REFERENCES,0.4759493670886076,"Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer
sequences. arXiv preprint arXiv:2007.14062, 2020."
REFERENCES,0.47848101265822784,"Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang,
Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang, Jun Wang,
Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao Tao, Dasen Yan, Zexuan Yi, Fang
Peng, Fangqing Jiang, Han Zhang, Lingfeng Deng, Yehong Zhang, Zhe Lin, Chao Zhang, Shaojie
Zhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng Jin, Qun Liu, and
Yonghong Tian. Pangu-α: Large-scale autoregressive pretrained chinese language models with
auto-parallel computation, 2021."
REFERENCES,0.4810126582278481,"Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert:
Distillation-aware ultra-low bit bert, 2020."
REFERENCES,0.4835443037974684,"A
BACKGROUND OF TRANSFORMER AND BERT"
REFERENCES,0.4860759493670886,"A.1
TRANSFORMER"
REFERENCES,0.48860759493670886,"A Transformer layer (see Fig. 3) consists of a self-attention (SAN) module and a feed-forward
network (FFN) module. An input X for SAN will be linearly transformed into query, key, value, and"
REFERENCES,0.4911392405063291,Published as a conference paper at ICLR 2022
REFERENCES,0.4936708860759494,Figure 3: Transformer architecture.
REFERENCES,0.4962025316455696,"output space {Q, K, V } as below 8:"
REFERENCES,0.49873417721518987,"⎡⎢⎢⎢⎢⎢⎢⎢⎣ Q
K
V"
REFERENCES,0.5012658227848101,"⎤⎥⎥⎥⎥⎥⎥⎥⎦
= X ×"
REFERENCES,0.5037974683544304,⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ W Q W K W V
REFERENCES,0.5063291139240507,⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ (8)
REFERENCES,0.5088607594936709,The self-attention mechanism (a.k.a Scaled Dot-Product Attention) is calculated as
REFERENCES,0.5113924050632911,"Attention(Q,K,V) = softmax(QK
√"
REFERENCES,0.5139240506329114,"dk
)V
(9)"
REFERENCES,0.5164556962025316,"For a multi-head version of the self-attention mechanism, it linearly projects Q, K, V with h times
using individual linear projections to smaller dimensions (e.g. dk = dmodel"
REFERENCES,0.5189873417721519,"h ), instead of performing
a single attention function with dmodel-dimensional keys, values and queries. Finally, the output of
SAN is
SAN(X) = [head1; ⋯; headh]W O"
REFERENCES,0.5215189873417722,"headi = Attention(Qi, Ki, Vi),
(10)"
REFERENCES,0.5240506329113924,"where
Q = [Q1; ⋯Qh], K = [K1; ⋯Kh], and V = [V1; ⋯Vh]. The individual attention heads
are independently calculated, Cordonnier et al. (2021) claims that there is some redundancy in
multi-head attention."
REFERENCES,0.5265822784810127,"Since the output of SAN is a linear transformation (using W O) of V , which is a weighted sum of
V . A stack of many purely SAN layers is not expressive (Dong et al., 2021), since it is equivalent
to a single linear transformation. To this end, a feed-forward network with non-linear activation is
alternately used with each SAN layer,"
REFERENCES,0.529113924050633,"FFN(X) = δ(XW in)W out.
(11)"
REFERENCES,0.5316455696202531,"Since some neurons after the activation function (e.g., δ is ReLU or GELU (Hendrycks & Gimpel,
2016)) become inactivated (zero), din is usually bigger than dmodel to avoid the low-rank bottleneck,
typically, din = 4 × dmodel = dout. Other tricks, such as layer normalization, residual connection,
dropout, and weight decay are also adopted to relieve the optimization and overﬁtting problems when
it goes deeper."
REFERENCES,0.5341772151898734,"A.2
BERT"
REFERENCES,0.5367088607594936,"BERT is a Transformer architecture-based pre-trained language model trained on plain corpora by
using a masked language model pre-training objective, thanks to the capacity and scalability of the"
REFERENCES,0.5392405063291139,"8For all linear transformation in this paper, the bias term is in default omitted"
REFERENCES,0.5417721518987342,Published as a conference paper at ICLR 2022
REFERENCES,0.5443037974683544,"Transformer (Devlin et al., 2018). BERT signiﬁcantly improved the SOTA performance of many
downstream tasks, including classiﬁcation benchmark GLUE (Wang et al., 2018). Note that the
parameters of SAN and FAN linearly increase with the number of layers while the embedding layers
keeps constant with respect to the layer number."
REFERENCES,0.5468354430379747,"B
‘MULT-HEAD’ FEED FORWARD NEURAL NETWORK"
REFERENCES,0.549367088607595,"(a) FFN
(b) ‘multi-head’ FFN"
REFERENCES,0.5518987341772152,"Figure 4: Fig. 4a and Fig. 4b are equivalent. Note that FFN could be a sum of 4 independent
sub-FFNs; each of them conducts a full-rank D × D transformation."
REFERENCES,0.5544303797468354,"In the multi-head attention mechanism, individual attention heads are separately calculated, however
the calculation is low-rank due to the redundancy among heads (Cordonnier et al., 2021). In
this paper, we argue that such redundancy may also appear in the feed-forward neural networks.
In the feed-forward neural networks, element-wise activation functions are usually adopted, e.g.,
GELU (Hendrycks & Gimpel, 2016) and Relu (Agarap, 2018); that is, each activation can be
independently calculated."
REFERENCES,0.5569620253164557,"By partitioning the original W in into H column groups and W out into H row groups, one can revise
a feed-forward neural layer (i.e. FFN(X) = δ(XW in)W out) as a sum of H independent ‘thin’
sub-FFNs with a dimension of DH = 4D/H as below:"
REFERENCES,0.5594936708860759,"FFN(X) = H
∑"
REFERENCES,0.5620253164556962,"h=1
δ(XW in
h )W out
h
(12)"
REFERENCES,0.5645569620253165,"Where W in
h ∈RD×Dh and W out
h
∈RDh×D, W in = [W in
1 ⊕⋯⊕W in
H] ∈RD×4D and W out =
[W out
1
⊕⋯⊕W out
H ] ∈R4D×D. In this paper, we set H = 4, since W in
h , W out
h
∈RD×D and each
transformation in sub-FFNs layer are full-rank transformations. See Fig 4 for graphical illustration.
One can refer to block partitioned matrix multiplication to understand the equivalence between Fig.
4a and 4b."
REFERENCES,0.5670886075949367,"C
CONCATENATION OVER ROWS OR COLUMNS"
REFERENCES,0.569620253164557,"PCA on columnly-stacked and rowly-stacked matrices are shown in Fig. 5a and Fig. 5b. Observe
that there is no too much difference between them."
REFERENCES,0.5721518987341773,Published as a conference paper at ICLR 2022
REFERENCES,0.5746835443037974,"(a) PCA for a pair of matrices along columns
(b) PCA for a pair of matrices along rows"
REFERENCES,0.5772151898734177,Figure 5: PCA for existing weight block matrices in BERT-base
REFERENCES,0.579746835443038,"D
MATRIX AND TENSOR DECOMPOSITION"
REFERENCES,0.5822784810126582,"Tensor is a generalized ‘matrix’ with typically a dimension of 3 or more; sometimes, one can also
call a matrix as a bi-dimensional ‘tensor’ and a vector as a one-dimensional ‘tensor’. Formally, an
n-dimensional tensor is an array with n indexes as"
REFERENCES,0.5848101265822785,"X ∈RI1,⋯,In
(13)"
REFERENCES,0.5873417721518988,"An entry in X is accessed by selecting n ordered index (i1, ⋯, in), and {0 ≤ik < In, ik ∈N}. In
this section, we mainly discuss order-3 tensors for simplicity while higher-order tensors also hold."
REFERENCES,0.589873417721519,"Typical tensor decomposition includes Canonical Polyadic (CP) decomposition (Carroll & Chang,
1970; Harshman, 1972) and Tucker decomposition (Tucker, 1963). CP decomposition approximates
a high-dimension tensor with as a sum of many rank-one tensors. In the case of three-dimensional
tensor decomposition, ˆX = R
∑"
REFERENCES,0.5924050632911393,"r=1
Ar ⊗Br ⊗Cr
(14)"
REFERENCES,0.5949367088607594,"Where Ar ∈RI1, Br ∈RI2, Cr ∈RI3 and A ∈RR×I1, B ∈RR×I2 and C ∈RR×I3. ⊗is the
tensor product 9."
REFERENCES,0.5974683544303797,"Tucker decomposition decomposes a tensor into a set of factor matrices and one small low-rank core
tensor,
ˆX = G ×1 A ×2 B ×3 C
(15)"
REFERENCES,0.6,"where A ∈RR1×I1, B ∈RR2×I2, C ∈RR3×I3 and G ∈RR1×R2×R3. ×1, ×2, and ×3 are mode-k
products 10. {R1, R2, R3} is sometimes called Tucker ranks. An entry with index (i1, i2, i3) is
calculated as"
REFERENCES,0.6025316455696202,"ˆXi1,i2,i3 = R1
∑ a=1 R2
∑ b=1 R3
∑"
REFERENCES,0.6050632911392405,"c=1
Ga,b,cAa,i1Bb,i2Cc,i3
(16)"
REFERENCES,0.6075949367088608,"9Here we give an example to calculate the tensor product of three 2-dimensional vectors, x, y, z ∈R2,
resulting in a tensor of R2×2×2. R is the CP rank shared in all modes."
REFERENCES,0.610126582278481,"x ⊗y ⊗z = [ x1
x2 ] ⊗[ y1
y2 ] ⊗[ z1
z2 ] = [ x1
x2 ] ⊗[ y1z1
y1z2
y2z1
y2z2 ] ="
REFERENCES,0.6126582278481013,⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
REFERENCES,0.6151898734177215,"[ x1y1z1
x1y1z2 ] [ x1y2z1
x1y2z2 ]"
REFERENCES,0.6177215189873417,"[ x2y1z1
x2y1z2 ] [ x2y2z1
x2y2z2 ]"
REFERENCES,0.620253164556962,⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
REFERENCES,0.6227848101265823,"10Given a tensor G ∈RR1×R2⋯Rk⋯Rn and a matrix M ∈RRk×r, a mode-k between G and M results in
G ×m M ∈RR1×R2⋯Rk−1×r×Rk+1,⋯,Rn."
REFERENCES,0.6253164556962025,Published as a conference paper at ICLR 2022
REFERENCES,0.6278481012658228,"Tucker decomposition will degrade to CP decomposition,where the core tensor G is constrained to be
super-diagonal 11 and R = R1 = R2 = R3."
REFERENCES,0.6303797468354431,"A k-slice in the ﬁrst mode, i.e., a matrix with a size of I2 × I3, would be ˆXk = R1
∑ i1=1 R2
∑ i1=1 R3
∑"
REFERENCES,0.6329113924050633,"i3=1
Gi1,i2,i3Ai1,k(Bi2 ⊗Ci3),
ˆXk ∈R
I2×I3
(17)"
REFERENCES,0.6354430379746835,"All slices in a speciﬁc mode share factor matrices (i.e., B, C) in other modes.Therefore, there exists
inter-correlation between these slices. These shared factor matrices not only make the compression
ratio of tensor decomposition being much bigger than multiple independent matrix decomposi-
tion (Noach & Goldberg, 2020), but also can be beneﬁcial especially when there is redundancy among
these parameter matrices and factor matrices can be utilized common features."
REFERENCES,0.6379746835443038,"E
DIFFERENCE WITH EXISTING WORK"
REFERENCES,0.640506329113924,"E.1
DIFFERENCE WITH TENSOR DECOMPOSITION USED FOR COMPRESSING CNNS, RNNS
AND EMBEDDINGS"
REFERENCES,0.6430379746835443,"Compressing pre-training models is a new scenario. We believe that exploring tensor decomposition
in pre-trained language models is non-trivial. In the pre-trained language models, we could test tensor
decomposition in very deep and wide networks like GPT 3 (96 layers and a hidden dimension of
12288), while this work is the ﬁrst step."
REFERENCES,0.6455696202531646,"Existing work Ma et al. (2019); Liu et al. (2021); Gao et al. (2020); Khrulkov et al. (2019); Hrinchuk
et al. (2020); Panahi et al. (2019) which use tensor network for compressing neural networks do
not have the potential for acceleration. The bottleneck in speed limits the application of tensor
decomposition in real-world applications: compressing models but consuming longer inference time
seems be useful in very rare scenarios. We argue that it is nontrivial to compress neural networks
using tensor decomposition with acceleration effects, as this work did."
REFERENCES,0.6481012658227848,"Work mechanisms for compression are totally different, previous works compress each weight matrix
(W Q, W K, W V , W O, W in, W out in each layer) individually using matrix/tensor decomposition.
They are making use of local redundancy inside each matrix. While in big models (PLMs), we
believe that making use of cross-matrix/cross-layer redundancy is also, sometimes more, beneﬁcial.
We believe that using tensor decomposition for cross-matrix/cross-layer redundancy is a signiﬁcant
difference."
REFERENCES,0.6506329113924051,"E.2
DIFFERENCE WITH CORDONNIER ET AL. (2021)"
REFERENCES,0.6531645569620254,"(Cordonnier et al., 2021) is quite impressive and inspiring. It does inspire this paper, however, we
want to highlight the difference with (Cordonnier et al., 2021)."
REFERENCES,0.6556962025316456,"Motivation
The motivation is different, (Cordonnier et al., 2021) found the redundancy in different
heads. We make use of redundancy of both intra-matrix redundancy and inter-matrix redundancy
(including cross-layer redundancy)."
REFERENCES,0.6582278481012658,"Method
The architecture in (Cordonnier et al., 2021) is slightly different from Transformer (or
BERT) since it redesigns the self-attention network with collaborated attention. While our architecture
is nearly the same as the raw model, we simulate each matrix multiplication of BERT with a product
of many smaller matrices."
REFERENCES,0.660759493670886,"Goal
Our work aims to extremely compress neural networks, which cannot be achieved by (Cordon-
nier et al., 2021). (Cordonnier et al., 2021) is to explore the possibility to share key/query projection
in SANs. Note that SAN only has 1/3 parameters in Transformer, this proportion even becomes
smaller when considering the embedding layer. By compressing only SANs with 1/3 parameters, its
overall compression ratio is limited."
REFERENCES,0.6632911392405063,"11Namely, Gi1,i2,i3 equals 1 if i1 = i2 = i3, and 0 otherwise"
REFERENCES,0.6658227848101266,Published as a conference paper at ICLR 2022
REFERENCES,0.6683544303797468,"Potential for efﬁciency
(Cordonnier et al., 2021) is faster than the raw model only if Dk is
small. The smallest setting with Dk = 64 has 20% FLOPs reduction. This shows its potential for
acceleration is limited. A similar setting with Dk = 64, we have nearly 80% reduction in terms of
FLOPs."
REFERENCES,0.6708860759493671,"Potential for compression ratios
The smallest model for BERT-base (110M) in (Cordonnier et al.,
2021) has 96.6M when Dk = 128; its best compression ratio is 1.14. While in our model, the smallest
model has a much bigger compression ratio, while being faster and performing better."
REFERENCES,0.6734177215189874,"Potential for effectiveness
The model in Cordonnier et al. (2021) (96.6M parameters) achieves
93.5% (77.6/83.0) performance with BERT base, while our smallest model with 25M parameters
(plus parameters in the embedding layer) achieves 97.7% performance (80.8/82.7) of BERT base. A
slight difference is that we test our model on the test dataset through GLUE online benchmark while
Cordonnier et al. (2021) test their model on ofﬂine dev dataset through average results for three runs,
so we use the relative performance w.r.t. the original BERT-base."
REFERENCES,0.6759493670886076,"F
MORE DISCUSSIONS TO COMPRESS WEIGHTS MATRICES IN SANS AND
FFNS TOGETHER"
REFERENCES,0.6784810126582278,"F.1
SHARING LEADS TO A BIGGER MATRIX BANK"
REFERENCES,0.6810126582278481,"If we separately compress SANs and FFNs, we would have two matrix banks: one for SANs and one
FFNs: P F F NCF F N and P SANCSAN. Each weight matrix in FFN(or SAN) is speciﬁc to a matrix
bank CF F N for FFN (or CSAN for SAN) and its weight vector over the bank P F F N
i
(or P SAN
i
).
Note that the two matrix banks have the most parameters since it is three-order tensor while others
(U, V , P ) are matrices."
REFERENCES,0.6835443037974683,"Note that matrices in two matrix banks have the same shape, one could merge (share) the two matrix
banks (a m-size d × d matrix bank and a n-size m-size d × d matrix bank) to get a single bigger
((m + n)-size) matrix bank , this could boost the expressive power for both FFNs and SANs due to
the bigger matrix bank (denoted as [CF F N; CSAN])."
REFERENCES,0.6860759493670886,"The unmerged one is a special case of the merged one. Let us deﬁne a new P ′, each element in which
is deﬁned as below:"
REFERENCES,0.6886075949367089,"P ′
i ="
REFERENCES,0.6911392405063291,⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎩
REFERENCES,0.6936708860759494,[Pi; [
REFERENCES,0.6962025316455697,"n
ÌÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÐÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÎ
0, 0, ⋯0]] for SANs [["
REFERENCES,0.6987341772151898,"m
ÌÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÐÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÒÎ
0, 0, ⋯0]; Pi] for FFNs"
REFERENCES,0.7012658227848101,"P ′
i is the new weight vector over the shared (m + n)-size matrix bank."
REFERENCES,0.7037974683544304,"Without losing any generality, we could relieve the zero-constrains in P ′ to get a general form that
each element in P ′ is not forced to be zero. This could make FFNs and SANs share more matrix
bases and therefore get better capacity."
REFERENCES,0.7063291139240506,"The beneﬁt could be empirically evidenced in Table 7: solely compressing SANs (without compress-
ing FFNs) underperforms both compressing FFNs and SANs; although the former has much more
parameters. Since in the shared setting, the shared matrix bank could compensate the counterpart of
SANs. Another na¨ıve motivation is to design a uniﬁed protocol for both SANs and FFNs."
REFERENCES,0.7088607594936709,"F.2
DISCUSSIONS ON THE SCALE DIFFERENCE BETWEEN WEIGHTS MATRICES IN SANS AND
FFNS"
REFERENCES,0.7113924050632912,"We want to clarify that we do not intend to approximate the raw weights (and their norm/scale).
Previously, we tried to add a regularizer in the training loss function to force the reconstructed weights
using decomposition to be as close as the raw weights."
REFERENCES,0.7139240506329114,Published as a conference paper at ICLR 2022
REFERENCES,0.7164556962025317,"1
2
3
4
5
6
7
8
9
10
11
12
0 5 10 15 20 25 30 35 40 WQ WK WV Wo WIn WOut"
REFERENCES,0.7189873417721518,Figure 6: L2 norms of raw weights in BERT
REFERENCES,0.7215189873417721,L = Ltraining + λ∣WIV −WI∣2
REFERENCES,0.7240506329113924,"This does not improve the performance, but worsen the performance. The reason may be as below.
Since we cannot perfectly approximate the raw weights with a decent compression ratio, there always
exists some difference between the raw weights and the reconstructed weights using decomposition.
However, even a slight difference might lead to a big difference in output. Plus, we also observe that
the ﬁnally learned WIV has a big difference with WI. So we give up our efforts to approximate the
raw weights."
REFERENCES,0.7265822784810126,"For example, we found that even randomly initializing the compressed BERT nearly achieves identical
performance, compared to decomposition from a given model. This also shows that approximating
raw weights is not beneﬁcial. Instead, we use knowledge distillation to simulate the input-output
mapping from the original model and tolerate the difference between raw weights and reconstructed
weights. We believe that the former makes more sense than the latter. Empirical results show the
former performs well."
REFERENCES,0.7291139240506329,"Norm difference in decomposition
Decomposition is to approximate a three-order tensor with
three factor tensors and a core tensor."
REFERENCES,0.7316455696202532,Wi = (CPi) ×2 U ×3 V
REFERENCES,0.7341772151898734,"Each matrix has its speciﬁc factor vector Pi, which does not have any constraints. The ﬂexibility of
norms in Pi could compensate the norm difference for the original matrices."
REFERENCES,0.7367088607594937,"In case the readers may be curious, we show the norms in Figure 6. The norms in different matrices
do have some differences. We also compare the norms between the raw weight matrices and the
reconstructed weights matrices in Figure 7. It shows that the norms of reconstructed weights also
have a big difference compared to that of corresponding original weight matrices. We argue that the
knowledge distillation does not aim to approximate the raw weight matrices but only approximate its
input-output mapping functions."
REFERENCES,0.739240506329114,Published as a conference paper at ICLR 2022
REFERENCES,0.7417721518987341,"1
2
3
4
5
6
7
8
9
10
11
12
0 5 10 15 20 25 30 35 40 WQ WK WV WO WIn WOut"
REFERENCES,0.7443037974683544,"WQ-IV
WK-IV
WV-IV"
REFERENCES,0.7468354430379747,"WO-IV
WIn-IV"
REFERENCES,0.7493670886075949,WOut-IV
REFERENCES,0.7518987341772152,Figure 7: L2 norms of raw weights in BERT and reconstructed BERT-IV.
REFERENCES,0.7544303797468355,"G
LINKING IV TO ALBERT"
REFERENCES,0.7569620253164557,"For IV, we have WIV = U(P C)V . Considering a speciﬁc case when U = V = I(D), l = 12 (i.e.,"
REFERENCES,0.759493670886076,"C ∈R12×D2
) and P ="
REFERENCES,0.7620253164556962,⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
REFERENCES,0.7645569620253164,"I(12)
I(12)
⋯
I(12)"
REFERENCES,0.7670886075949367,⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
REFERENCES,0.769620253164557,∈R12L×12. Where I(i) is an i × i identity matrix. Namely
REFERENCES,0.7721518987341772,I(i) =
REFERENCES,0.7746835443037975,⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
REFERENCES,0.7772151898734178,"1
0
⋯
0
0
1
⋯
0
⋯
0
0
⋯
1"
REFERENCES,0.779746835443038,⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
REFERENCES,0.7822784810126582,"∈Ri×i.
(18)"
REFERENCES,0.7848101265822784,"Then
W IV = U(PiC)V
= P C ="
REFERENCES,0.7873417721518987,⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
REFERENCES,0.789873417721519,"C
C
⋯
C"
REFERENCES,0.7924050632911392,⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
REFERENCES,0.7949367088607595,"∈R12LD2
(19)"
REFERENCES,0.7974683544303798,"Eq. 19 results in a layer-shared BERT like ALBERT (Lan et al., 2019). (Lan et al., 2019) additionally
compresses the embedding layer using matrix factorization."
REFERENCES,0.8,"H
HOW IV GENERALIZES III"
REFERENCES,0.8025316455696202,"III (tensor train decomposition) is a special case of IV (tucker decomposition) in this paper. Note that,
we are not stating the correspondence between tensor train decomposition and tucker decomposition
in general case, but a three-order tensor train decomposition is a special case of tucker decomposition."
REFERENCES,0.8050632911392405,Let us recall their deﬁnition:
REFERENCES,0.8075949367088607,"{Wi
III = UΣiV
Wi
IV = U(PiC)V
Or in another format:"
REFERENCES,0.810126582278481,"{WIII = Σ ×2 U ×3 V
WIV = (PC) ×2 U ×3 V"
REFERENCES,0.8126582278481013,"×2 and ×3 are the mode-2 and mode-3 multiplication. The only difference Σ ∈R12L×d×d vs.
(PC) ∈R12L×d×d. In the latter, P ∈R12L×l and C ∈Rl×d×d. In a special case of tucker"
REFERENCES,0.8151898734177215,Published as a conference paper at ICLR 2022
REFERENCES,0.8177215189873418,"Table 8: Task descriptions and statistics in GLUE (Wang et al., 2018). NLI is for ‘Natural Language
Inference’ and QA is for ‘Question Answering’. SST-2, MNLI, QNLI, QQP are considered as
relatively-big dataset according to the scale of their train set."
REFERENCES,0.8202531645569621,"CoLA
SST-2
MNLI
MRPC
QNLI
QQP
RTE
STS-B"
REFERENCES,0.8227848101265823,"task
classiﬁcation
classiﬁcation
NLI
paraphrase
QA/NLI
paraphrase
NLI
similarity
train
8.5 k
67k
393k
3.7k
105k
364k
2.5k
7k
test
1k
1.8k
20k
1.7k
5.4k
391k
3k
1.4k"
REFERENCES,0.8253164556962025,"0
64
128
256
320
Batch size 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.8278481012658228,GPU Inference Time (sec.)
REFERENCES,0.830379746835443,"BERT
ALBERT
BERT-II-245
BERT-III-384
BERT-III-64
BERT-IV-72-384
BERT-IV-36-128
BERT-IV-36-256"
REFERENCES,0.8329113924050633,"Figure 8: Inference time. Inference time in Y-axis (in a Nvidia V100 16G GPU) increases when
batchsizes (in X-axis) become bigger. BERT-III/BERT-IV with d < 384 are faster than raw BERT."
REFERENCES,0.8354430379746836,"decomposition when 1) 12L = l and 2) P = I12L, it is equivalent to the 3-order tensor train
decomposition (III). I12L is a (square) identity matrix with a size of 12L×12L, in which the diagonal
element equals one and 0 elsewhere."
REFERENCES,0.8379746835443038,"In our experiments, there existed two settings which set 12L = l = 144 that satisﬁed 1). In such
settings, the rigorous difference with BERT-III and BERT-IV is that we do not force P to be equal
to an identity matrix, instead, P is ﬂexible to be trained. We argue that adding such a ﬂexible P
could enable crossing layer sharing and enhance the capacity of III. Interestingly, once we ﬁnish
training BERT-IV-144-384 and BERT-IV-144-64, before starting inference, one could merge P and"
REFERENCES,0.8405063291139241,"and C as a single matrix: R12L×12L × R12L×d2
→R12L×d2
, which does not affect the model but
slightly improve inference speed. By doing so, it is rigorously equivalent to III. Thus, we rename
BERT-IV-144-384/BERT-IV-144-64 as BERT-III-384 and BERT-III-384."
REFERENCES,0.8430379746835444,"I
GLUE BENCHMARK"
REFERENCES,0.8455696202531645,The data statistics in GLUE is shown in Tab. 8.
REFERENCES,0.8481012658227848,"J
INFERENCE TIME"
REFERENCES,0.850632911392405,"As shown in Fig. 8, the raw BERT base is the slowest one. BERT-III/BERT-IV with dimension rank
d = 384 is slight faster than raw BERT. The inference time consistently decreases when d becomes
smaller. Especially, BERT-III-64 is 2.6 × faster than BERT, which is consistent to RPS shown in Tab.
4."
REFERENCES,0.8531645569620253,Published as a conference paper at ICLR 2022
REFERENCES,0.8556962025316456,"Note that BERT-III -384 and BERT-IV -72-384 are faster than BERT in terms of inference time and
RPS (see RPS in Tab. 4), although they have the identical FLOPS. A similar observation could be
found in ALBERT (Lan et al., 2019) – layer-shared BERT has the same FLOPS with the raw BERT
while the former is faster than the latter. The better inference time and RPS with the same FLOPS may
be due to that the relatively smaller model consumes less memory and bandwidth inside GPU, see
https://openreview.net/forum?id=H1eA7AEtvS. This speedup effect in this matter
depends on speciﬁc platforms and hardware."
REFERENCES,0.8582278481012658,"BERT-II implemented by Noach & Goldberg (2020) has a rank for hidden dimension of 245, there we
called ‘BERT-II-245’. It inference time is close to our BERT-IV-36-256, this is because the inference
time is mainly related to the dimension rank d."
REFERENCES,0.8607594936708861,"K
ON THE COMPRESSION OF THE EMBEDDING LAYER"
REFERENCES,0.8632911392405064,"In this work, we did not compress the embedding layer. The reasons to not compress the embedding
layer are manyfold. First, compressing the embedding layer will deﬁnitely increase training/inference
time, since a single embedding lookup is fast enough."
REFERENCES,0.8658227848101265,"Secondly, the embedding layer does not increase the total parameters when BERT has more trans-
former layers in the encoder. Since the parameters in the embedding layer are constant with respect to
network depth. Table 9 shows the parameter numbers when compressing main weight matrices with
a half dimension rank (when the hidden dimension is 768, its half is 384 as the rank d, meanwhile
keeping layer rank unchanged). Here, we also consider embedding layer for a fair comparison as
suggested."
REFERENCES,0.8683544303797468,"models
# Paras.
# layers (L)
D
V
BERT-IV- 12L 2
−D"
COMPRESSION RATIOS,0.8708860759493671,"2
compression ratios"
COMPRESSION RATIOS,0.8734177215189873,"BERT-base-uncased
110M
12
768
30522
35.7M
3.1
BERT-large-uncased
340M
24
1024
30522
75.8M
4.5
GPT-3 Small
125M
12
768
50257
50.7M
2.5
GPT-3 Medium
350M
24
1024
50257
85.8M
4.1
GPT-3 Large
760M
24
1536
50257
165.5M
4.6
GPT-3 XL
1.3B
24
2048
50257
243.0M
5.3
GPT-3 2.7B
2.7B
32
2560
50257
498.0M
5.4
GPT-3 6.7B
6.7B
32
4096
50257
1.1B
6.3
GPT-3 13B
13.0B
40
5140
50257
1.9B
6.8
GPT-3 175B
175.0B
96
12288
50257
22.8B
7.7"
COMPRESSION RATIOS,0.8759493670886076,"Table 9: Compression ratios with increasing models when consider parameters in the embedding
layer. The bigger the model, the closer it is to the theoretical upper bound of compression ratios (i.e.,
8)."
COMPRESSION RATIOS,0.8784810126582279,"Note that the parameters of embedding becomes more negligible when PLMs have more layers and
bigger hidden dimension, in which case the compression ratio will approximate an ideal upper bound
(8 = 2 × 22, which is linear to the deduction times of layer rank and quadratic to the deduction
times of dimension rank; in practice, we could use bigger deduction in both ranks, as we did in the
experiments)."
COMPRESSION RATIOS,0.8810126582278481,"Plus, the shape of an embedding layer (V D) is related to the size of vocabulary, which is heteroge-
neous to other weight matrices in Self-attention network and Feed-forward network (D2 or 4D2).
Therefore it is incompatible with the current compression protocol. To additionally compress the
embedding layer, we might have to design a totally different compression protocol for the embedding
layer, which makes this work more complicated."
COMPRESSION RATIOS,0.8835443037974684,"Finally, the embedding layer is relatively easy to compress, see tensorized embedding (Hrinchuk
et al., 2020) and word2ket (Panahi et al., 2019) which compress embedding with maximum 307 and
93,675 times respectively with a slight performance drop. We believe that it is trivial to additionally
compress the embedding layer. Thus, we leave compressing the embedding layer as future work."
COMPRESSION RATIOS,0.8860759493670886,Published as a conference paper at ICLR 2022
COMPRESSION RATIOS,0.8886075949367088,"Table 10: Experimental comparisons between the proposed work and (Noach & Goldberg, 2020). &
indicates the reported number is an average between these metrics. Metrics are Accuracy (MNLI
(average of MNLI match and MNLI mis-match), QNLI, RTE, SST-2), Avg of Accuracy and F1
(MRPC, QQP), Matthew’s correlation (CoLA), Avg of Pearson and Spearman correlations (STS-B)."
COMPRESSION RATIOS,0.8911392405063291,"Model
Para.
FLOPS
RPS
SST-2
MNLI
MRPC
QNLI
QQP
RTE
STS-B
Avg
acc
acc (m&mm)
acc&F1
acc
acc (m&mm)
acc
spear.&pears."
COMPRESSION RATIOS,0.8936708860759494,"BERT-base
86.0M
22.5B
420.1
93.4
83.1
85.6
90.9
80.2
66.4
86.5
83.9
BERT-III -384
23.0M
22.5B
452.2
93.4
84.2
86.2
90.5
80.5
68.1
86.1
84.0
BERT-III-64
1.8M
4.3B
1088.6
91.9
79.9
85.5
82.3
79.8
63.3
81.3
80.6
BERT-IV-72-384
12.3M
22.5B
452.3
93.1
83.6
86.0
90.2
80.4
67.3
85.6
83.7
BERT-IV-36-256
3.9M
15.2B
596.9
92.7
82.2
84.4
88.9
80.2
65.2
82.4
82.3
BERT-IV-36-128
1.9M
8.0B
863.0
92.4
80.7
86.5
83.2
80.6
64.4
82.0
81.4
matrix decomposition (Noach & Goldberg, 2020) ( BERT-II-245)
41.8M
14.6B
656.8
91.9
79.9
85.5
82.3
79.8
63.3
81.3
80.6"
COMPRESSION RATIOS,0.8962025316455696,"Table 11: Experimental comparisons between the proposed work and (Lagunas et al., 2021)."
COMPRESSION RATIOS,0.8987341772151899,"Model
Para.
FLOPS
RPS
SST-2
MNLI
QQP
Avg
acc
acc (m&mm)
f1"
COMPRESSION RATIOS,0.9012658227848102,"BERT-base
86.0M
22.5B
420.1
92.5
84.5/84.9
87.7
87.4
BERT-III -384
23.0M
22.5B
452.2
92.6
86.7/86.7
88.0
88.5
BERT-III-64
1.8M
4.3B
1088.6
91.4
80.7/80.8
87.2
85.0
BERT-IV-72-384
12.3M
22.5B
452.3
92.5
85.6/85.6
87.7
87.9
BERT-IV-36-256
3.9M
15.2B
596.9
92.2
83.4/83.8
88.0
86.9
BERT-IV-36-128
1.9M
8.0B
863.0
91.6
82.7/82.6
87.5
86.1
Sparsiﬁed BERT (Lagunas et al., 2021)
20M
-
-
91.2
83.2/83.6
87.9
86.5"
COMPRESSION RATIOS,0.9037974683544304,"L
COMPARISON TO METHODS THAT USES NON-STANDARD METRICS IN GLUE"
COMPRESSION RATIOS,0.9063291139240506,"(Noach & Goldberg, 2020) uses non-standard setting for GLUE tasks. Metrics in (Noach & Goldberg,
2020) are Accuracy (MNLI (average of MNLI match and MNLI mis-match), QNLI, RTE, SST-
2), Avg of Accuracy and F1 (MRPC, QQP), Matthew’s correlation (CoLA), Avg of Pearson and
Spearman correlations (STS-B). Tab. 10 tailored our result in their evaluation protocol. In this
work, we do not include CoLA dataset since training samples in CoLA are too few to have stable
evaluations."
COMPRESSION RATIOS,0.9088607594936708,"M
ANALYSIS OF THE LEARNED MODEL"
COMPRESSION RATIOS,0.9113924050632911,"Observing that in IV, each matrix is parameterized as WIV
i
= U(PiC)V . With shared U and V ,
each matrix has a speciﬁc parameter Pi (called ‘factor vectors’ later), we analyze {Pi} to shed some
light on understanding what BERT-IV learns."
COMPRESSION RATIOS,0.9139240506329114,"In BERT-IV-72-384, we calculate the cosine distances between any two factor vectors. d(Pi, Pj) =
1 −cos(Pi, Pj), lead to a 144 × 144 matrix. We could observe that there is a cross-layer pattern that
{W In} (the weight matrices in the layer of FFNs) among layers are relatively close. This pattern
becomes weaker in deep (top) layers, this might suggest that top layers may learn more diverse
feature projections. The reason for this pattern needs further investigation."
COMPRESSION RATIOS,0.9164556962025316,"We also visualize the in-matrix pattern in each layer (e.g. in the 1st, 4-th, 8-th, and 12-th layer) in
Figure 10. It shows that a pair between W In
h
and W Out
h
is relatively close, e.g., W In
1
and W Out
1
,
W In
2
and W Out
2
, etc."
COMPRESSION RATIOS,0.9189873417721519,"N
POTENTIAL OF COMPRESSION IN LAGER MODELS"
COMPRESSION RATIOS,0.9215189873417722,"In this section, we compare various compression ratios of II, III, and IV, see the parameters and their
compression ratios (this additionally considers parameters in the embedding layer) in Table 12. It
shows that III and IV could achieve much bigger compression ratios than II when using the same rank
for hidden dimension (i.e., 2048). We claim that the proposed model has better potential to compress
bigger models."
COMPRESSION RATIOS,0.9240506329113924,Published as a conference paper at ICLR 2022 0.0 0.2 0.4 0.6 0.8 1.0 1.2
COMPRESSION RATIOS,0.9265822784810127,"Figure 9: Distances between the factor vectors among 144 matrices in a trained BERT-IV-72-384. The
order is listed as [W Q, W K, W V , W O, W In
1 , W In
2 , W In
3 , W In
4 , W Out
1
, W Out
2
, W Out
3
, W Out
4
]
from the ﬁrst layer to last layer."
COMPRESSION RATIOS,0.9291139240506329,"Q
K
V
O
WIn
1
WIn
2
WIn
3
WIn
3
WOut
1
WOut
2
WOut
3
WOut
4 Q K V O WIn
1 WIn
2 WIn
3 WIn
3"
COMPRESSION RATIOS,0.9316455696202531,"WOut
1"
COMPRESSION RATIOS,0.9341772151898734,"WOut
2"
COMPRESSION RATIOS,0.9367088607594937,"WOut
3"
COMPRESSION RATIOS,0.9392405063291139,"WOut
4 0.0 0.2 0.4 0.6 0.8 1.0"
COMPRESSION RATIOS,0.9417721518987342,(a) The ﬁrst layer
COMPRESSION RATIOS,0.9443037974683545,"Q
K
V
O
WIn
1
WIn
2
WIn
3
WIn
3
WOut
1
WOut
2
WOut
3
WOut
4 Q K V O WIn
1 WIn
2 WIn
3 WIn
3"
COMPRESSION RATIOS,0.9468354430379747,"WOut
1"
COMPRESSION RATIOS,0.9493670886075949,"WOut
2"
COMPRESSION RATIOS,0.9518987341772152,"WOut
3"
COMPRESSION RATIOS,0.9544303797468354,"WOut
4 0.0 0.2 0.4 0.6 0.8 1.0"
COMPRESSION RATIOS,0.9569620253164557,(b) The 4-th layer later
COMPRESSION RATIOS,0.959493670886076,"Q
K
V
O
WIn
1
WIn
2
WIn
3
WIn
3
WOut
1
WOut
2
WOut
3
WOut
4 Q K V O WIn
1 WIn
2 WIn
3 WIn
3"
COMPRESSION RATIOS,0.9620253164556962,"WOut
1"
COMPRESSION RATIOS,0.9645569620253165,"WOut
2"
COMPRESSION RATIOS,0.9670886075949368,"WOut
3"
COMPRESSION RATIOS,0.9696202531645569,"WOut
4 0.0 0.2 0.4 0.6 0.8 1.0"
COMPRESSION RATIOS,0.9721518987341772,(c) The 8-th layer later.
COMPRESSION RATIOS,0.9746835443037974,"Q
K
V
O
WIn
1
WIn
2
WIn
3
WIn
3
WOut
1
WOut
2
WOut
3
WOut
4 Q K V O WIn
1 WIn
2 WIn
3 WIn
3"
COMPRESSION RATIOS,0.9772151898734177,"WOut
1"
COMPRESSION RATIOS,0.979746835443038,"WOut
2"
COMPRESSION RATIOS,0.9822784810126582,"WOut
3"
COMPRESSION RATIOS,0.9848101265822785,"WOut
4 0.0 0.2 0.4 0.6 0.8 1.0"
COMPRESSION RATIOS,0.9873417721518988,(d) The 12-th layer later.
COMPRESSION RATIOS,0.9898734177215189,"Figure
10:
Distances
between
the
factor
vectors
among
12
matrices
in
each
layer
of
a
trained
BERTIV-72-384.
The
order
is
listed
as
[W Q, W K, W V , W O, W In
1 , W In
2 , W In
3 , W In
4 , W Out
1
, W Out
2
, W Out
3
, W Out
4
]."
COMPRESSION RATIOS,0.9924050632911392,"model
Paras
L D
GPT-II-2048
GPT-III-2048
GPT-IV-12-768
GPT-IV-12-2048 GPT-IV-144-2048"
COMPRESSION RATIOS,0.9949367088607595,"GPT-3 Small
125M 12 768
493.1M (0.3× ↓) 647.2M (0.2× ↓) 48.3M (2.6× ↓)
93.5M (1.3× ↓)
647.2M (0.2× ↓)
GPT-3 Medium 350M 24 1024 1.3B (0.3× ↓)
1.3B (0.3× ↓)
56.7M (6.2× ↓)
102.5M (3.4× ↓) 656.2M (0.5× ↓)
GPT-3 Large
760M 24 1536 1.9B (0.4× ↓)
1.3B (0.6× ↓)
90.0M (8.4× ↓)
137.1M (5.5× ↓) 690.8M (1.1× ↓)
GPT-3 XL
1.3B
24 2048 2.5B (0.5× ↓)
1.3B (1.0× ↓)
102.3M (12.7× ↓) 150.8M (8.6× ↓) 704.5M (1.8× ↓)
GPT-3 2.7B
2.7B
32 2560 4.2B (0.6× ↓)
1.8B (1.5× ↓)
194.4M (13.9× ↓) 244.2M (11.1× ↓) 797.9M (3.4× ↓)
GPT-3 6.7B
6.7B
32 4096 6.7B (1.0× ↓)
1.9B (3.6× ↓)
270.9M (24.7× ↓) 324.7M (20.6× ↓) 878.4M (7.6× ↓)
GPT-3 13B
13.0B 40 5140 10.4B (1.2× ↓)
2.4B (5.5× ↓)
333.6M (39.0× ↓) 390.0M (33.3× ↓) 943.7M (13.8× ↓)
GPT-3 175B
175.0B 96 12288 59.0B (3.0× ↓)
5.9B (29.5× ↓)
1.1B (162.1× ↓)
1.2B (151.6× ↓)
1.7B (102.4× ↓)"
COMPRESSION RATIOS,0.9974683544303797,Table 12: Parameter compression ratios in various models
