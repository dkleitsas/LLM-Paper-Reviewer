Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011904761904761906,"Embedding learning has found widespread applications in recommendation sys-
tems and natural language modeling, among other domains. To learn quality em-
beddings efﬁciently, adaptive learning rate algorithms have demonstrated supe-
rior empirical performance over SGD, largely accredited to their token-dependent
learning rate. However, the underlying mechanism for the efﬁciency of token-
dependent learning rate remains underexplored. We show that incorporating fre-
quency information of tokens in the embedding learning problems leads to prov-
ably efﬁcient algorithms, and demonstrate that common adaptive algorithms im-
plicitly exploit the frequency information to a large extent. Speciﬁcally, we pro-
pose (Counter-based) Frequency-aware Stochastic Gradient Descent, which ap-
plies a frequency-dependent learning rate for each token, and exhibits provable
speed-up compared to SGD when the token distribution is imbalanced. Empir-
ically, we show the proposed algorithms are able to improve or match adaptive
algorithms on benchmark recommendation tasks and a large-scale industrial rec-
ommendation system, closing the performance gap between SGD and adaptive al-
gorithms, while using signiﬁcantly lower memory. Our results are the ﬁrst to show
token-dependent learning rate provably improves convergence for non-convex em-
bedding learning problems."
INTRODUCTION,0.002380952380952381,"1
INTRODUCTION"
INTRODUCTION,0.0035714285714285713,"Embedding learning describes a problem of learning dense real-valued vector representation for cat-
egorical data, often referred to as token (Pennington et al., 2014; Mikolov et al., 2013a;b). Good
quality embeddings can capture rich semantic information of tokens, and thus serve as the corner-
stone for downstream applications (Santos et al., 2020). Due to their signiﬁcant impact on model
performance and large memory footprint (21.8% of total parameters for BERT (Devlin et al., 2018),
95% for industrial recommenders in Section 4), how to learn quality embedding vectors efﬁciently
forms an important problem in applications, including recommendation systems and natural lan-
guage processing."
INTRODUCTION,0.004761904761904762,"Empirically, adaptive algorithms (Duchi et al., 2011; Kingma & Ba, 2014; Reddi et al., 2019) have
witnessed signiﬁcant successes, yielding state of the art performance in both industrial-scale recom-
mendation systems and natural language model (Guo et al., 2017; Zhou et al., 2018b; Devlin et al.,
2018; Liu et al., 2019). Stochastic gradient descent (SGD), on the other hand, has struggled to keep
up, often yielding much slower convergence and low quality models (Liu et al., 2020; Zhang et al.,
2019) (see also Figure 2). The sharp contrast on the efﬁciency of adaptive algorithms and SGD is
particularly distinctive, as SGD is the typical choice of optimization algorithms in the other domains
of machine learning, such as vision/image related tasks (He et al., 2016; Goyal et al., 2017)."
INTRODUCTION,0.005952380952380952,∗Work done during an internship at Meta.
INTRODUCTION,0.007142857142857143,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008333333333333333,"The common belief behind the empirical edge of adaptive learning rate algorithms over SGD is
that the former ones exploit sparsity of high dimensional feature. Speciﬁcally, a feature in a typical
embedding learning problem comes in the form of one/multi-hot encoding of tokens (e.g. wordpiece
in NLP and user/item in recommendation systems), which leads to a sparse stochastic gradient that
has only non-zero values for tokens within the mini-batch. In addition, token distributions of real
world data are often highly imbalanced and satisfy the power-law property (Piantadosi, 2014; Celma,
2010; Clauset et al., 2009), and infrequent tokens are widely believed to be more informative to
model learning. Thus adaptive algorithms can pick up information from the infrequent tokens more
efﬁciently, as they can schedule a higher learning rate for the infrequent tokens (Duchi et al., 2011)."
INTRODUCTION,0.009523809523809525,"Despite the appealing intuition, there is a signiﬁcant theory-practice gap on the empirical superiority
of adaptive learning rate algorithms over SGD, and no developed theories can explicitly justify the
previous intuition. Better dimensional dependence of adaptive algorithms has only been shown
in the convex setting (Duchi et al., 2011), which hardly generalizes to even the simplest practical
models in embedding learning problems (e.g., Factorization Machine, Rendle (2010)), whose loss
landscape is non-convex. For non-convex settings, most theoretical efforts have been devoted to
analyzing adaptive learning rate algorithms for general non-convex objectives, which yield subpar
convergence rate compared to standard SGD (Ward et al., 2018; D´efossez et al., 2020; Chen et al.,
2018; Zhou et al., 2018a). In fact, the standard SGD has been recently shown to be minimax optimal
for non-convex problems (Drori & Shamir, 2020; Arjevani et al., 2019), and thus not improvable in
general. Moreover, since adaptive algorithms are only implicitly exploiting frequency information,
and if the intuition indeed holds true, one might naturally wonder whether we can instead develop
an adaptive learning rate schedule that explicitly depends on frequency information. Motivated by
our previous discussions, we raise and aim to address the following questions"
INTRODUCTION,0.010714285714285714,Questions
INTRODUCTION,0.011904761904761904,"Can we design a frequency-dependent adaptive learning rate schedule? Can we show prov-
able beneﬁts over SGD?"
INTRODUCTION,0.013095238095238096,"Our contributions. We answer the previous question by showing that token frequency information
can be leveraged to design provably efﬁcient algorithms for embedding learning. Speciﬁcally,"
INTRODUCTION,0.014285714285714285,"• We propose Frequency-aware Stochastic Gradient Descent (FA-SGD), a simple modiﬁcation
to standard SGD, which applies a token-dependent learning rate that inversely proportional to
the frequency of the token. We also propose a variant, named Counter-based Frequency-aware
Stochastic Gradient Descent (CF-SGD), which is able to estimate frequency in an online fashion,
much similar to Adagrad (Duchi et al., 2011) and Adam (Kingma & Ba, 2014).
• Theoretically, we show that both FA-SGD and CF-SGD outperform standard SGD for embedding
learning problems. Speciﬁcally, they are able to signiﬁcantly improve convergence for learning
infrequent tokens, while maintaining convergence speed for frequent tokens. To the best of our
knowledge, our proposed algorithms are the ﬁrst to show provable speed-up over standard SGD for
non-convex embedding learning problems. This is in sharp contrast with other popular adaptive
learning rate algorithms, whose empirical performance can not be explained by existing theories.
• Empirically, we conduct extensive experiments on benchmark datasets and a large-scale industrial
recommendation system. We show that FA/CF-SGD is able to signiﬁcantly improve over SGD,
and improves/matches popular adaptive learning rate algorithms. We also observe the second-
order moment maintained by Adagrad and Adam highly correlates with the frequency information,
demonstrating intimate connections between adaptive algorithms and the proposed FA/CF-SGD."
RELATED LITERATURE,0.015476190476190477,"1.1
RELATED LITERATURE
Adaptive algorithms for non-convex problems. There has been a fruitful line of research on an-
alyzing the convergence of adaptive learning rate algorithms in non-convex setting. These results
aim to match the convergence rate of standard SGD given by O(1/
√"
RELATED LITERATURE,0.016666666666666666,"T) (Ghadimi & Lan, 2013),
however often with additional factor of log T (Ward et al., 2018; D´efossez et al., 2020; Chen et al.,
2018; Reddi et al., 2018), or with worse dimension dependence (Zhou et al., 2018a) for smooth
problem (assumed by almost all prior works). Moreover, all existing works aim to analyze the con-
vergence for general non-convex problems, ignoring unique data features in embedding learning
problems, where adaptive algorithms are most successful. We explicitly take account into the spar-"
RELATED LITERATURE,0.017857142857142856,Published as a conference paper at ICLR 2022
RELATED LITERATURE,0.01904761904761905,Algorithm 1 Frequency-aware Stochastic Gradient Descent
RELATED LITERATURE,0.02023809523809524,"Input:
Total iteration number T, token frequency {pk}k∈X, and learning rate schedule
{ηt
k}k∈X,t∈[T ] speciﬁed by (7).
Initialize: Θ0 ∈RN×d, sample τ ∼Unif([T]),
for t = 0, . . . τ do"
RELATED LITERATURE,0.02142857142857143,"(1) Sample (it, jt) ∼D, calculate gt
it = ∇θitℓ(θit, θjt; yit,jt), gt
jt = ∇θjtℓ(θit, θjt; yit,jt)
(2) Update parameters
θt+1
it
= θt
it −ηt
itgt
it, θt+1
i
= θt
i, ∀i ∈U, i ̸= it"
RELATED LITERATURE,0.02261904761904762,"θt+1
jt
= θt
jt −ηt
jtgt
jt, θt+1
j
= θt
j, ∀j ∈V, j ̸= jt
end for
Output: Θτ"
RELATED LITERATURE,0.023809523809523808,"sity of stochastic gradient, and token distribution imbalancedness into the design and analysis of our
proposed algorithms, which are the keys to better convergence properties."
RELATED LITERATURE,0.025,"Adaptive algorithms and SGD. To the best of our knowledge, the study on understanding why
adaptive learning rate algorithms outperform SGD is very limited. Zhang et al. (2019) argue that
BERT pretraining (Devlin et al., 2018) has heavy-tailed noise, implying unbounded variance and
possible non-convergence of SGD. Normalized gradient clipping method is proposed therein and
converges for a family of heavy-tailed noise distributions. Our results focus on a different direction
by showing that imbalanced token distribution is an important factor that can be leveraged to design
more efﬁcient algorithms for embedding learning problems. Our result also does not rely on the
noise to be heavy-tailed for the convergence beneﬁts of the proposed FA/CF-SGD to take effect."
RELATED LITERATURE,0.02619047619047619,"Notations: For a vector/matrix, we use ∥·∥to denotes its ℓ2-norm/Frobenius norm. We use ∥·∥2 to
denote the spectral norm of a matrix."
PROBLEM SETUP,0.02738095238095238,"2
PROBLEM SETUP"
PROBLEM SETUP,0.02857142857142857,"We consider an embedding learning problem which aims to learn user and item embeddings through
their interactions. We denote U as the set of users, and V as the set of items, and let X = U ∪V
denote the union, referred to as tokens throughout the rest of the paper. We assume |X| = N, i.e.,
the total number of user and item is N. For the ease of presentation, we always use letter i to index
user set U, letter j to index item set V , and letter k to index the union set X. The embedding learning
problem can be abstracted into the following stochastic optimization problem:"
PROBLEM SETUP,0.02976190476190476,"min
Θ∈RN×d f(Θ) = E(i,j)∼D [ℓ(θi, θj; yij)] =
X"
PROBLEM SETUP,0.030952380952380953,"i∈U,j∈V
D(i, j)ℓ(θi, θj; yij).
(1)"
PROBLEM SETUP,0.03214285714285714,"Here (i, j) denotes the user-item pair sampled from the unknown interaction distribution D, θi,
θj ∈Rd (the i, j-th row of Θ) denotes their embedding vectors respectively, and the loss ℓ(θi, θj; yij)
denotes the prediction loss for their interaction yij ∈{−1, +1} (e.g., logistic loss). We further let"
PROBLEM SETUP,0.03333333333333333,"pi =
X"
PROBLEM SETUP,0.034523809523809526,"j∈V
D(i, j), ∀i ∈U; pj =
X"
PROBLEM SETUP,0.03571428571428571,"i∈U
D(i, j), ∀j ∈V,
(2)"
PROBLEM SETUP,0.036904761904761905,"denote the marginal distribution over U and V .
Remark 2.1. Our analysis also allows treatment of additional network structure (with parame-
ters denoted by W) that takes nonlinear transformation of embedding vectors, e.g., f(Θ, W) =
E(i,j)∼Dℓ(θi, θj, W; yij). We omit their explicit treatment for presentation simplicity. In addition,
although we mainly discuss in the context of recommendation, our analysis and results only relies
on sparsity of stochastic gradient and the imbalancedness of token distributions, which allow one to
extend our results to other embedding learning problems (e.g., language model pretraining)."
PROBLEM SETUP,0.0380952380952381,"The full algorithmic descriptions of our proposed Frequency-aware Stochastic Gradient Descent
(FA-SGD) algorithm are presented in Algorithm 1. Note that randomly outputting a historical iter-
ate is commonly adopted in literature for showing convergence of stochastic gradient descent type
algorithms for non-convex problems (Ghadimi & Lan, 2013). In practice, we can simply use the last
iterate ΘT as the output solution. In addition, Section 3.3 presents CF-SGD (Algorithm 2), which
does not need the token distribution as the input and can estimate it in an online fashion."
PROBLEM SETUP,0.039285714285714285,Published as a conference paper at ICLR 2022 gt = 
PROBLEM SETUP,0.04047619047619048, 0⊤
PROBLEM SETUP,0.041666666666666664,"∇θitℓ(θit, θjt; yit,jt)⊤
...
∇θjtℓ(θit, θjt; yit,jt)⊤ 0⊤ "
PROBLEM SETUP,0.04285714285714286,"
(3)"
PROBLEM SETUP,0.04404761904761905,"At iteration t, FA-SGD samples (it, jt) ∼D, and ob-
tain the sparse stochastic gradient gt deﬁned in (3).
Note that only the it-th and jt-th row of gt are non-zero.
One can readily verify that E(it,jt)∼D [gt] = ∇Θf(Θt).
Going forward, we will denote ∇f t
k as the k-th row of
gradient ∇f(Θt), and gt
k as the k-th row of stochastic
gradient gt. Note that we have"
PROBLEM SETUP,0.04523809523809524,"Ejt

gt
it|it = i

= ∇f t
i /pi, Eit

gt
jt|jt = j

= ∇f t
j/pj.
(4)"
PROBLEM SETUP,0.04642857142857143,"We further denote δt
k =
1
pk f t
k −gt
k for all k ∈X. Then by deﬁnition E

δt
it|it = i

= 0 and
E

δt
jt|jt = j

= 0 for all i ∈U, j ∈V . We pose the following assumptions on the its variance."
PROBLEM SETUP,0.047619047619047616,"Assumption 1 (Bounded conditional variance). We assume that the variance of δt
it is bounded. That
is, there exists {σ2
k}k∈X, such that"
PROBLEM SETUP,0.04880952380952381,"E
h
∥δit∥2 |it = i
i
≤σ2
i , E
h
∥δjt∥2 |jt = j
i
≤σ2
j ,
∀i ∈U, j ∈V.
(5)"
PROBLEM SETUP,0.05,"Assumption 1 allows us to provide a ﬁner characterization on the variance of stochastic gradient
compared to typical variance assumption in literature. To illustrate, recall that standard assumption
in the stochastic optimization literature assumes Var(gt) = E ∥∇Θf t −gt∥2 ≤σ2 for some uni-
versal constant σ > 0. Consider an extreme setting, where we have exact gradient for the sampled
user-item pair, i.e., gi
it =
1
pit ∇f t
it and gi
jt =
1
pjt ∇f t
jt, then we have σk = 0 for all k ∈X. In
contrast, the variance of gt is still non-zero. In general setting, we can bound the variance as shown
in the following proposition. Note that the variance lower bound arises naturally form the extreme
sparsity of the stochastic gradient."
PROBLEM SETUP,0.05119047619047619,"Proposition 2.1. Given Assumption 1, we have
X"
PROBLEM SETUP,0.05238095238095238,"k∈X
(1/pk −1)
∇f t
k
2 ≤Var(gt) ≤
X"
PROBLEM SETUP,0.05357142857142857,"k∈X
pkσ2
k +
X"
PROBLEM SETUP,0.05476190476190476,"k∈X
(1/pk −1)
∇f t
k
2 .
(6)"
PROBLEM SETUP,0.055952380952380955,"Assumption 2 (Smoothness of prediction loss). We assume ℓ(u, v; y) is symmetric w.r.t. u and v for
any y ∈{−1, +1}, and there exists L > 0 such that
∇2
uuℓ(·, ·; ·)

2 ≤L,
∇2
uvℓ(·, ·; ·)

2 ≤L."
PROBLEM SETUP,0.05714285714285714,"The assumption on the symmetry of ℓis readily satisﬁed by almost all neural network architecture.
In essence, this assumption only requires that the parameterization of embedding vector is token
agnostic. On the other hand, the spectral upper bound on the Hessian matrix is a standard assumption
in optimization literature."
THEORETICAL RESULTS,0.058333333333333334,"3
THEORETICAL RESULTS"
THEORETICAL RESULTS,0.05952380952380952,"We ﬁrst present the convergence results of FA-SGD and standard SGD for embedding learning
problem formulated in (1), and discuss the advantage that FA-SGD offers when the token distribution
{pk}k∈X is highly imbalanced. We further propose a variant, named CF-SGD, which can estimate
frequency information in an online fashion and still provably enjoys the beneﬁts of FA-SGD."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.060714285714285714,"3.1
CONVERGENCE OF FA-SGD AND STANDARD SGD"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.06190476190476191,"Theorem 3.1 (FA-SGD). With Assumption 1 and 2, take learning rate policy to be"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.0630952380952381,"ηt
k = min
n
1/(4L), α/
p"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.06428571428571428,"Tpk
o
,
(7)"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.06547619047619048,"where T denotes the total number of iterations, and α =
q"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.06666666666666667,"(f(Θ0) −f ∗) /
 
L P"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.06785714285714285,"l∈X plσ2
l

, we
have"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.06904761904761905,"E ∥∇f τ
k ∥2 = O

L(f(Θ0)−f ∗) T
+ √pk√P"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.07023809523809524,"l∈X plσ2
l (f(Θ0)−f ∗)L
√ T"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.07142857142857142,"
,
∀k ∈X.
(8)"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.07261904761904762,Published as a conference paper at ICLR 2022
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.07380952380952381,"Remark 3.1 (Connection with Stochastic Block Coordinate Descent). Our FA-SGD shares some
similarities with Stochastic Block Coordinate Descent (SBCD) (Nesterov, 2012; Dang & Lan, 2015;
Richt´arik & Tak´aˇc, 2014) applied to problem (1), in the sense that each iteration we sample certain
blocks of variables (θit, θjt in our case), and only update the sampled blocks by following its stochas-
tic gradient. Different from SBCD, the stochastic gradient of the block variable gt
it in the FA-SGD
is biased, as shown in (4). Note that with unbiased stochastic gradient, SBCD method typically con-
verges slower than standard SGD by a factor that can be as large as number of blocks. As a concrete
example, when the token distribution is uniform, SBCD converges slower than standard SGD by a
factor of |X|, hence slower than FA-SGD by a factor of |X| from Corollary 3.1 developed later."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.075,"Recall that from Proposition 2.1, the variance of stochastic gradient is heavily inﬂuenced by the
population gradient ∇Θf(Θ), and can be huge whenever the population gradient is, presumably
in the early phase of training. This relationship is also supported by empirical ﬁndings in Zhang
et al. (2019) (Figure 2a), where the authors show that for BERT pretraining, the noise distribution
in stochastic gradient gt is highly non-stationary, which has large variance in the beginning of the
training and smaller variance at the end of training. Since existing analysis of SGD in literature
assumes a constant variance bound for the stochastic gradient, our observation in Proposition 2.1
requires an alternative analysis of SGD for problem (1)."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.0761904761904762,"To obtain the convergence rate of standard SGD in the presence of iterate-dependent variance (6),
our key insight is to tailor the convergence analysis to the sparsity of the stochastic gradient for
problem (1). We show the convergence of standard SGD as the following."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.07738095238095238,"Theorem 3.2 (Standard SGD). With Assumption 1 and 2, take learning rate policy to be ηt
k ="
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.07857142857142857,"min
n
1
4L,
α
√ T"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.07976190476190476,"o
, where T denotes the total number of iterations, and α =
r"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.08095238095238096,f(Θ0)−f ∗ L P
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.08214285714285714,"l∈X p2
l σ2
l , we have"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.08333333333333333,"E ∥∇f τ
k ∥2 = O

L(f(Θ0)−f ∗)"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.08452380952380953,"T
+
√P"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.08571428571428572,"l∈X p2
l σ2
l (f(Θ0)−f ∗)L
√ T"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.0869047619047619,"
,
∀k ∈X.
(9)"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.0880952380952381,"Note that both FA-SGD and standard SGD attain a rate of O(1/
√"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.08928571428571429,"T). Compared to existing rates of
standard SGD (Ghadimi & Lan, 2013), we do not require constant variance bound on stochastic gra-
dient, as we have discussed above. Compared to existing rates of adaptive learning rate algorithms
(Zhou et al., 2018a; Chen et al., 2018), both rates obtained here exhibits dimension-free property.
We emphasize here that due to the dimension-free nature of the bounds for both SGD and FA-SGD,
we do not claim the proposed FA-SGD has better dependence on dimension, which is the main moti-
vation of adaptive algorithms (Duchi et al., 2011; Kingma & Ba, 2014; Reddi et al., 2019). Instead,
the major difference on the convergence of FA-SGD (8) and that of standard SGD (9) is that the for-
mer one is token-dependent. Speciﬁcally, for FA-SGD, each token k ∈X has its own convergence
characterization, while all the tokens have the same convergence characterization in the standard
SGD. We ﬁrst make a simple observation stating the equivalence of FA-SGD and standard SGD,
when the token distribution {pk}k∈X is uniform."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.09047619047619047,"Corollary 3.1 (Uniform Distribution). Suppose the user distribution {pi}i∈U and item distribution
{pj}j∈V is the uniform distribution. Then FA-SGD and standard SGD is equivalent to each other,
in terms of both algorithmic execution and convergence rate."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.09166666666666666,"3.2
WHEN DOES FA-SGD OUTPERFORM STANDARD SGD?"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.09285714285714286,"We show FA-SGD shines when the token distribution {pk}k∈X, deﬁned in (2), is highly imbalanced.
Before we present detailed discussions, we make an important remark that highly imbalanced token
distributions are ubiquitous in social systems, presented in the form power-law. Examples of such
distributions include the degree of individuals in the social network (Muchnik et al., 2013); the
frequency of words in natural language (Zipf, 2016); citations for academic papers (Brzezinski,
2015); number of links on the internet (Albert et al., 1999). For more discussions on power-law
distributions in social and natural systems, we refer readers to Kumamoto & Kamihigashi (2018)."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.09404761904761905,"In Figure 1c, 1d we plot the user and item counting distribution of Movielens-1M dataset. One could
clearly see that the user and item distributions are highly imbalanced, with a small percentages of
users/items taking up the majority of rating records. We defer details on the skewness of token
distributions for Criteo dataset to Appendix C."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.09523809523809523,Published as a conference paper at ICLR 2022
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.09642857142857143,"0
3
6
9
12
15
18
Frequency Rank 0.0 0.1 0.2 0.3"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.09761904761904762,Frequency
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.0988095238095238,"Top User
Non-top User"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.1,(a) Exponential Tail
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.10119047619047619,"0
3
6
9
12
15
18
Frequency Rank 0.0 0.1 0.2 0.3"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.10238095238095238,Frequency
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.10357142857142858,"Top User
Non-top User"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.10476190476190476,(b) Polynomial Tail
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.10595238095238095,"0
2000
4000
6000
User ID Rank 0 500 1000 1500 2000"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.10714285714285714,Counts
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.10833333333333334,(c) User counts
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.10952380952380952,"0
1000
2000
3000
Item ID Rank 0 1000 2000 3000"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.11071428571428571,Counts
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.11190476190476191,(d) Item counts
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.1130952380952381,"Figure 1: Token distribution with an exponential and polynomial tail, and the user/item counting
distributions for Movielens-1M dataset.
To illustrate the comparative advantage of FA-SGD when the token distribution {pk}k∈X is highly
skewed. We consider two classes of distribution families with different tail properties, one with
exponential tail, and one with polynomial tail."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.11428571428571428,"Corollary 3.2 (Exponential Tail). Let U = {in}|U|
n=1, V = {jm}|V |
m=1, where in denote the user with
n-th largest frequency, and jm denote the item with the m-th largest frequency. Suppose"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.11547619047619048,"pin ∝exp(−τn), pjm ∝exp(−τm),
∀n ∈[|U|], m ∈[|V |]
(10)"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.11666666666666667,"for some τ > 0. Deﬁne UT as the set of users whose frequencies are within e-factor from the highest
frequency: UT = {in : n ≤1"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.11785714285714285,"τ }, and VT similarly as VT = {jm : m ≤1"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.11904761904761904,"τ }. We refer to UT as the
top users, and VT as the top items."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.12023809523809524,"Then given |U|, |V | ≥1"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.12142857142857143,"τ , the proposed FA-SGD, compared to standard SGD:"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.12261904761904761,"(1) Obtains the same rate of convergence, for the top users UT and top items VT ;
(2) E
∇f τ
in
2 can converge faster by a factor of Ω{exp (τ(n −|UT |))} for in ∈U \ UT ;"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.12380952380952381,"(3) E
∇f τ
jm
2 can converge faster by a factor of Ω{exp (τ(m −|VT |))} for jm ∈V \ VT ."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.125,"We remark that |U|, |V | ≥1"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.1261904761904762,"τ is a very mild condition, as it only requires that the most infrequent
user/item should have its frequency smaller than the most frequent user/item by at least a factor of
e. i.e., the non-top user/item set U \ UT , V \ VT is nonempty, This is readily satisﬁed by the token
distributions in recommendation systems and natural language modeling (Celma, 2010; Zipf, 2016),
where the lowest frequency is at least orders of magnitude smaller than the highest frequency. The
factor of e in deﬁning UT , VT can also be readily replaced by any constant larger than 1."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.12738095238095237,"From Corollary 3.2, we can see that FA-SGD improves signiﬁcantly over standard SGD for user/item
distribution with exponential tail. Speciﬁcally, FA-SGD achieves the same convergence rate of top
users/items compared to SGD, meanwhile it signiﬁcantly improves the convergence of the non-top
users/items. Moreover, the strength of such an improvement increases exponentially as we move
towards the tail users/items.
Corollary 3.3 (Polynomial Tail). Let U = {in}|U|
n=1, V = {jm}|V |
m=1, where in denote the user with
n-th largest frequency, and jm denote the item with the m-th largest frequency. Suppose"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.12857142857142856,"pin ∝n−ν, pjm ∝m−ν,
∀n ∈[|U|], m ∈[|V |]
(11)"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.12976190476190477,"for some ν ≥2. Deﬁne UT as the set of users whose frequencies are within 2-factor from the highest
frequency: UT = {in : n−ν ≥1/16}, and VT similarly as VT = {jm : m−ν ≥1/16}. We refer to
UT as the top users, and VT as the top items."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.13095238095238096,"Then given |U|, |V | ≥161/ν, the FA-SGD, compared to standard SGD:"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.13214285714285715,"(1) Obtains the same rate of convergence, for the top users UT and top items VT ;"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.13333333333333333,"(2) E
∇f τ
in
2 can converge faster by a factor of Ω
n
n
|UT |
νo
for each in ∈U \ UT ;"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.13452380952380952,"(3) E
∇f τ
jm
2 can converge faster by a factor of Ω
n
m
|VT |
νo
for each jm ∈V \ VT ."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.1357142857142857,"We remark that polynomial tail (37) is also the prototypical example of the power law distribution
class for modeling social behaviors (Kumamoto & Kamihigashi, 2018). The constant 2 in the condi-
tion ν ≥2 can be replaced by any constant strictly larger than 1, with slight changes to the constant
factor in the statements of the corollary."
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.13690476190476192,"From Corollary 3.3, we can see that FA-SGD improves signiﬁcantly over standard SGD for user/item
distribution with polynomial tail. Speciﬁcally, FA-SGD achieves the same convergence rate of top"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.1380952380952381,Published as a conference paper at ICLR 2022
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.1392857142857143,Algorithm 2 Counter-based Frequency-aware Stochastic Gradient Descent
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.14047619047619048,"Input: Total iteration number T.
Initialize: Θ0 ∈RN×d, counter sample τ ∼Unif({T/2, . . . , T}).
for t = 0, . . . τ do"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.14166666666666666,"(1) Sample (it, jt) ∼D, calculate gt
it = ∇θitℓ(θit, θjt; yit,jt), gt
jt = ∇θjtℓ(θit, θjt; yit,jt)
(2) Compute counter-based learning rate bηt
it
 
ct
it

, bηt
jt
 
ct
jt

speciﬁed by (12)
(3) Update parameters"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.14285714285714285,"θt+1
it
= θt
it −bηt
itgt
it, θt+1
i
= θt
i, ∀i ∈U, i ̸= it"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.14404761904761904,"θt+1
jt
= θt
jt −bηt
jtgt
jt, θt+1
j
= θt
j, ∀j ∈V, j ̸= jt
(4) Update counters
ct+1
it
= ct
it + 1, ct+1
i
= ct
i, ∀i ∈U, i ̸= it"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.14523809523809525,"ct+1
jt
= ct
jt + 1, ct+1
j
= ct
j, ∀j ∈V, j ̸= jt"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.14642857142857144,"end for
Output: Θτ"
CONVERGENCE OF FA-SGD AND STANDARD SGD,0.14761904761904762,"users/items compared to SGD, meanwhile it signiﬁcantly improves the convergence of the non-top
users/items. Moreover, the strength of such an improvement increases in polynomial order as we
move towards the tail users/items."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1488095238095238,"3.3
ONLINE ESTIMATION OF FREQUENCY INFORMATION
In certain application scenarios, the token distribution {pk}k∈X can be unknown in advance of
learning. To apply FA-SGD, one needs to employ a preprocessing step in order to estimate the token
distribution to a high accuracy, and then run the algorithm with estimated token distribution. Such a
preprocessing step often requires additional human efforts and data. To remove such an undesirable
preprocessing step, below we present an online variant of FA-SGD, which uses the counter of tokens
collected during training to estimate the token distribution dynamically. We show that the proposed
Counter-based Frequency-aware Stochastic Gradient Descent (CF-SGD) is able to retain the beneﬁts
of FA-SGD despite unknown token distribution.
Theorem 3.3 (Counter-based FA-SGD). In addition to Assumption 1 and 2, suppose ∥∇f(·)∥≤G.
Take counter-based learning rate policy in Algorithm 2 to be"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.15,"bηt
k(ct
k) = min

1/(4L), 1/
q"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.15119047619047618,"T bpt
k"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1523809523809524,"
,
bpt
k = ct
k/t,
∀k ∈X, t ∈[T],
(12)"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.15357142857142858,"where T denotes the total number of iterations, α =
q"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.15476190476190477,"Mf/
 
L P"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.15595238095238095,"l∈X plσ2
l

and Mf = f(Θ0) −"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.15714285714285714,f ∗+ P
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.15833333333333333,"k∈X pkσ2
k/L, we have"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1595238095238095,"E ∥∇f τ
k ∥2 = O

LMf T
+ √pk√P"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.16071428571428573,"l∈X plσ2
l L(f(Θ0)−f ∗)
√ T
+"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1619047619047619,"√pk(
P"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1630952380952381,"l∈X plσ2
l )
√ T"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.16428571428571428,"
, ∀k ∈X.
(13)"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.16547619047619047,"for T ≥max

minl∈X 1"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.16666666666666666,"pl ,
2 log G−log(Mf (1/2L+α/√pk)) pk 
."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.16785714285714284,"We believe the assumption on gradient bound ∥∇f(·)∥≤G is not strictly necessary and can be re-
moved with more reﬁned analysis. Nevertheless, the requirement on T only logarithmically depends
on the gradient bound G. In addition, we highlight that the convergence characterization in Theorem
3.3 is still token-dependent. Speciﬁcally, we can show that despite not knowing token distribution
beforehand, CF-SGD can gain the same advantages that FA-SGD enjoys over SGD.
Corollary 3.4 (Exponential Tail). Suppose we have the same set of conditions given in Corollary
3.2, and σ/
p"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.16904761904761906,"L (f(Θ0) −f ∗) ≤1. Deﬁne UT as the set of users whose frequencies are within e-
factor from the highest frequency: UT = {in : n ≤1"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.17023809523809524,"τ }, and VT similarly as VT = {jm : m ≤1"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.17142857142857143,"τ }.
We refer to UT as the top users, and VT as the top items."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.17261904761904762,"Then given |U|, |V | ≥1"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1738095238095238,"τ , the proposed CF-SGD, compared to standard SGD:"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.175,"(1) Obtains the same rate of convergence, for the top users UT and top items VT ;
(2) E
∇f τ
in
2 can converge faster by a factor of Ω{exp (τ(n −|UT |))} for in ∈U \ UT ;"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1761904761904762,Published as a conference paper at ICLR 2022
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1773809523809524,"(3) E
∇f τ
jm
2 can converge faster by a factor of Ω{exp (τ(m −|VT |))} for jm ∈V \ VT .
Corollary 3.5 (Polynomial Tail). Suppose we have the same set of conditions given in Corollary
3.3, and σ/
p"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.17857142857142858,"L (f(Θ0) −f ∗) ≤1. Deﬁne UT as the set of users whose frequencies are within
2-factor from the highest frequency: UT = {in : n−ν ≥1/16}, and VT similarly as VT = {jm :
m−ν ≥1/16}. We refer to UT as the top users, and VT as the top items."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.17976190476190476,"Then given |U|, |V | ≥161/ν, the FA-SGD, compared to standard SGD:"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.18095238095238095,"(1) Obtains the same rate of convergence, for the top users UT and top items VT ;"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.18214285714285713,"(2) E
∇f τ
in
2 can converge faster by a factor of Ω
n
n
|UT |
νo
for each in ∈U \ UT ;"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.18333333333333332,"(3) E
∇f τ
jm
2 can converge faster by a factor of Ω
n
m
|VT |
νo
for each jm ∈V \ VT .
The proofs of Corollary 3.4 and 3.5 follow similar lines as in the proofs of Corollary 3.2 and 3.3,
which we defer to Appendix D
4
EXPERIMENTS
We conduct extensive experiments to verify the effectiveness of our proposed algorithms and our
developed theories, on both publicly available benchmark recommendation datasets, and a large-
scale industrial recommendation system. Additional experiments on learning Word2Vec embed-
dings (Mikolov et al., 2013a) are presented in Appendix B, demonstrating the general applicability
of the FA/CF-SGD. We list key elements of our experiment setup for benchmark datasets below."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.18452380952380953,"• Datasets: MovieLens-1M (GroupLens, 2003) and Criteo 1TB Click Logs dataset (Criteo, 2014).
• Models: Factorization Machine (FM) (Rendle, 2010), and DeepFM (Guo et al., 2017).
• Metric: Training loss (cross-entropy loss), and test AUC (Area Under the ROC Curve).
• Baseline algorithms: SGD, Adam (Kingma & Ba, 2014), Adagrad (Duchi et al., 2011). Note that
Adam and Adagrad are widely popular in training recommendation systems and language models."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.18571428571428572,"We also empirically verify that the token distributions for both Movielens-1M (Figure 1) and Criteo
(Appendix C) dataset are highly imbalanced, with most of the token distributions having a clear
polynomially or exponentially decaying tail."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1869047619047619,"Since CF-SGD does not require frequency information, which is a huge practical beneﬁt compared
to FA-SGD, in our experiments we mainly evaluate our proposed CF-SGD against the baseline
algorithms. To ensure a fair comparison, for each dataset and model type, we carefully tune the
learning rate of each algorithm for best performance. We apply early stopping and stop training
whenever the validation AUC do not increase for 2 consecutive epochs, which is widely adopted in
practice (Tak´acs et al., 2009; Dacrema et al., 2021)."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1880952380952381,"0
5000
10000
15000
20000
Iteration 0.4 0.5 0.6 0.7 0.8 0.9 Loss FM"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.18928571428571428,"Adam
CF-SGD
Adagrad
SGD"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.19047619047619047,(a) Training loss
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.19166666666666668,"0
5000
10000
15000
20000
Iteration 0.5 0.6 0.7 0.8 AUC FM"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.19285714285714287,"Adam
CF-SGD
Adagrad
SGD"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.19404761904761905,(b) Validation AUC
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.19523809523809524,"0
2500
5000
7500
10000
Iteration 0.5 0.6 0.7 0.8 0.9 Loss"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.19642857142857142,DeepFM
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1976190476190476,"Adam
SGD
Adagrad
CF-SGD"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.1988095238095238,(c) Training loss
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2,"0
2500
5000
7500
10000
Iteration 0.50 0.55 0.60 0.65 0.70 0.75 0.80 AUC"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2011904761904762,DeepFM
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.20238095238095238,"Adam
SGD
Adagrad
CF-SGD"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.20357142857142857,"(d) Validation AUC
Figure 2: Movielens-1M dataset with FM and DeepFM model. CF-SGD signiﬁcantly outperforms
standard SGD, and is highly competitive against Adam, Adagrad."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.20476190476190476,"Movielens-1M: We can observe from Figure 2 that for FM and DeepFM model: (1) SGD yields the
slowest convergence in training loss and AUC. (2) The proposed CF-SGD yields signiﬁcantly faster
convergence than SGD for training loss. In addition, CF-SGD converges even faster than the adap-
tive learning algorithms in the early stage of training; (3) All the algorithms eventually reaches peak
AUC around 81.0%, while CF-SGD attains the peak AUC much faster than baseline algorithms.
These empirical observations help us conﬁrm the effectiveness of the proposed CF-SGD algorithm.
We further make an empirical observation that draws a close connection between adaptive algorithms
and CF-SGD. We plot the second-order gradient moment maintained by Adagrad and Adam against
the estimated frequency maintained by CF-SGD. Surprisingly, the second-order gradient moment
quickly develops a close-to linear relationship with the frequency information accumulated by CF-
SGD (Figure 3a,3b) . This observation suggests that Adagrad and Adam are exploiting frequency
information implicitly to a large extent."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.20595238095238094,Published as a conference paper at ICLR 2022
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.20714285714285716,"Criteo: We observe qualitative behavior of CF-SGD similar to Movielens-1M dataset, as can be
seen in Figure 3c,3d, 4a,4b."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.20833333333333334,"0
2500
5000
7500 10000 12500
Iteration 0.6 0.7 0.8 0.9"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.20952380952380953,Correlation FM
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.21071428571428572,"Adam
Adagrad"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2119047619047619,(a) Correlation
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2130952380952381,"0
2500
5000
7500
10000
Iteration 0.4 0.5 0.6 0.7 0.8 0.9"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.21428571428571427,Correlation
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2154761904761905,DeepFM
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.21666666666666667,"Adam
Adagrad"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.21785714285714286,(b) Correlation
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.21904761904761905,"0
1
2
3
4
5
Iteration
1e5 0.5 1.0 1.5 2.0 2.5 3.0 Loss FM"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.22023809523809523,"Adam
CF-SGD
SGD
Adagrad"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.22142857142857142,(c) Training loss
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2226190476190476,"0
1
2
3
4
5
Iteration
1e5 0.5 0.6 0.7 0.8 AUC FM"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.22380952380952382,"Adam
CF-SGD
SGD
Adagrad"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.225,"(d) Validation AUC
Figure 3: (a-b) Second-order gradient moment correlates linearly with frequency maintained by
CF-SGD; (c-d) Comparisons on Criteo dataset with FM model."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2261904761904762,"0
1
2
3
4
5
Iteration
1e5 0.5 1.0 1.5 2.0 Loss"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.22738095238095238,DeepFM
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.22857142857142856,"Adagrad
SGD
Adam
CF-SGD"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.22976190476190475,(a) Training loss
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.23095238095238096,"0
1
2
3
4
5
Iteration
1e5 0.5 0.6 0.7 0.8 AUC"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.23214285714285715,DeepFM
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.23333333333333334,"Adagrad
SGD
Adam
CF-SGD"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.23452380952380952,"(b) Validation AUC
(c) Train NE Curve"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2357142857142857,"0.0
0.5
1.0
1.5
2.0
2.5
Iteration
1e10 0.04 0.03 0.02 0.01 0.00"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2369047619047619,NE(CF-SGD) - NE(Adagrad)%
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.23809523809523808,Industrial Recommender Sys.
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.2392857142857143,"(d) Train NE Diff %
Figure 4: (a-b) Comparisons on Criteo dataset with DeepFM model; (c-d) Comparisons on a
industrial-scale recommendation dataset with an ultra-large recommender model."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.24047619047619048,"Alg
NE
Diff %
Adagrad
0.78643
0.0
CF-SGD
0.78628
-0.02
Table 1: Eval NE Diff %"
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.24166666666666667,"Industrial Recommendation System: We train an ultra-large
industrial recommendation model with the proposed CF-SGD.
The training data contains 10 days of user-item interaction
records, with ∼2.5 billion examples per day (25 billion exam-
ples in total). We use around 800 features, with ∼100 million
average number of tokens per feature. We compare CF-SGD with Adagrad, which has been carefully
tuned in production usage. For both algorithms, we use a batch size of 64k and do one-pass training.
Different from benchmark academic datasets, we use Normalized Entropy (NE) as the evaluating
metric (He et al., 2014) (smaller is better), which is the cross-entropy loss normalized by the entropy
of background click through rate. Note that due to numerous iterations of the production model,
any relative improvement ∼0.02% is considered to be signiﬁcant. In Figure 4c, 4d we compare
the training NE curve CF-SGD and Adagrad, we can see that CF-SGD shows faster convergence
than Adagrad (see NE difference % in Figure 4d). From Table 1 we can observe that CF-SGD also
improves over Adagrad during the serving phase."
ONLINE ESTIMATION OF FREQUENCY INFORMATION,0.24285714285714285,"Memory Efﬁciency: On top of the above empirical evidences showing that CF-SGD learns fast –
faster than standard SGD, and comparable (if not better) to adaptive algorithms, we further highlight
that CF-SGD learns cheap. Speciﬁcally, adaptive algorithms require additional memory to store
history information for each parameter. For an embedding table of size N × d (N tokens, d being
embedding dimension), the memory needed is at least 3N ×d for Adam (ﬁrst/second-order gradient
moment), and 2N × d for Adagrad (second-order gradient moment). In sharp contrast, CF-SGD
only requires N additional memory, for storing the estimated frequency. Since the choice of typical
embedding dimension d exceeds 64 (Yin & Shen, 2018), adaptive algorithms require memory at least
twice the size of the embedding table, while CF-SGD requires negligible memory overhead. Note
the industrial recommendation model in our experiments has a size over multiple terabytes, with
above 95% of consumed by embedding tables. Doubling the memory footprint by using standard
Adam/Adagrad is infeasible in terms of both engineering and environmental concern."
CONCLUSION,0.24404761904761904,"5
CONCLUSION"
CONCLUSION,0.24523809523809523,"We propose (Counter-based) Frequency-aware SGD for embedding learning problems, which adopts
frequency-dependent learning rate schedule for each token. We demonstrate provable beneﬁts that
FA/CF-SGD enjoy over standard SGD for imbalanced token distributions, with extensive exper-
iments supporting our theoretical ﬁndings. Our empirical ﬁndings also suggest that adaptive al-
gorithms can implicitly exploit frequency information and hence share close connections with the
proposed algorithms, this connection might be helpful in the direct analysis of adaptive algorithms
for embedding learning problems, which we leave as a future direction."
CONCLUSION,0.24642857142857144,Published as a conference paper at ICLR 2022
CONCLUSION,0.24761904761904763,ACKNOWLEDGEMENTS
CONCLUSION,0.2488095238095238,"We deeply appreciate Aaron Defazio and Michael Rabbat for their valuable feedbacks and insightful
discussions. We are also grateful to Yuxi Hu for his help on production model experiments."
REFERENCES,0.25,REFERENCES
REFERENCES,0.2511904761904762,"R´eka Albert, Hawoong Jeong, and Albert-L´aszl´o Barab´asi. Diameter of the world-wide web. nature,
401(6749):130–131, 1999."
REFERENCES,0.2523809523809524,"Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth.
Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019."
REFERENCES,0.25357142857142856,"Michal Brzezinski. Power laws in citation distributions: evidence from scopus. Scientometrics, 103
(1):213–228, 2015."
REFERENCES,0.25476190476190474,"`Oscar Celma. The long tail in recommender systems. In Music Recommendation and Discovery,
pp. 87–107. Springer, 2010."
REFERENCES,0.25595238095238093,"Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018."
REFERENCES,0.2571428571428571,"Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman. Power-law distributions in empirical
data. SIAM review, 51(4):661–703, 2009."
REFERENCES,0.25833333333333336,"Criteo. Criteo 1TB Click Logs dataset. https://ailab.criteo.com/ressources/, 2014."
REFERENCES,0.25952380952380955,"Maurizio Ferrari Dacrema, Simone Boglio, Paolo Cremonesi, and Dietmar Jannach. A troubling
analysis of reproducibility and progress in recommender systems research. ACM Transactions on
Information Systems (TOIS), 39(2):1–49, 2021."
REFERENCES,0.26071428571428573,"Cong D Dang and Guanghui Lan. Stochastic block mirror descent methods for nonsmooth and
stochastic optimization. SIAM Journal on Optimization, 25(2):856–881, 2015."
REFERENCES,0.2619047619047619,"Alexandre D´efossez, L´eon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence proof
of adam and adagrad. arXiv preprint arXiv:2003.02395, 2020."
REFERENCES,0.2630952380952381,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.2642857142857143,"Yoel Drori and Ohad Shamir. The complexity of ﬁnding stationary points with stochastic gradient
descent. In International Conference on Machine Learning, pp. 2658–2667. PMLR, 2020."
REFERENCES,0.2654761904761905,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011."
REFERENCES,0.26666666666666666,"Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.26785714285714285,"Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017."
REFERENCES,0.26904761904761904,"GroupLens.
Movielens-1M.
https://grouplens.org/datasets/movielens/1m/,
2003."
REFERENCES,0.2702380952380952,"Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-
machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247, 2017."
REFERENCES,0.2714285714285714,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.2726190476190476,Published as a conference paper at ICLR 2022
REFERENCES,0.27380952380952384,"Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf
Herbrich, Stuart Bowers, et al. Practical lessons from predicting clicks on ads at facebook. In
Proceedings of the Eighth International Workshop on Data Mining for Online Advertising, pp.
1–9, 2014."
REFERENCES,0.275,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.2761904761904762,"Shin-Ichiro Kumamoto and Takashi Kamihigashi. Power laws in stochastic processes for social
phenomena: An introductory review. Frontiers in Physics, 6:20, 2018."
REFERENCES,0.2773809523809524,"Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difﬁ-
culty of training transformers. arXiv preprint arXiv:2004.08249, 2020."
REFERENCES,0.2785714285714286,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.27976190476190477,"Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016."
REFERENCES,0.28095238095238095,"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013a."
REFERENCES,0.28214285714285714,"Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representa-
tions of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111–3119, 2013b."
REFERENCES,0.2833333333333333,"Lev Muchnik, Sen Pei, Lucas C Parra, Saulo DS Reis, Jos´e S Andrade Jr, Shlomo Havlin, and
Hern´an A Makse. Origins of power-law degree distribution in the heterogeneity of human activity
in social networks. Scientiﬁc reports, 3(1):1–8, 2013."
REFERENCES,0.2845238095238095,"Yu Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM
Journal on Optimization, 22(2):341–362, 2012."
REFERENCES,0.2857142857142857,"Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pp. 1532–1543, Doha, Qatar, October 2014. Association for Com-
putational Linguistics. doi: 10.3115/v1/D14-1162. URL https://aclanthology.org/
D14-1162."
REFERENCES,0.2869047619047619,"Steven T Piantadosi. Zipf?s word frequency law in natural language: A critical review and future
directions. Psychonomic bulletin & review, 21(5):1112–1130, 2014."
REFERENCES,0.28809523809523807,"S Reddi, Manzil Zaheer, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for
nonconvex optimization. In Proceeding of 32nd Conference on Neural Information Processing
Systems (NIPS 2018), 2018."
REFERENCES,0.2892857142857143,"Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019."
REFERENCES,0.2904761904761905,"Steffen Rendle. Factorization machines. In 2010 IEEE International conference on data mining, pp.
995–1000. IEEE, 2010."
REFERENCES,0.2916666666666667,"Peter Richt´arik and Martin Tak´aˇc. Iteration complexity of randomized block-coordinate descent
methods for minimizing a composite function. Mathematical Programming, 144(1):1–38, 2014."
REFERENCES,0.29285714285714287,"Joaquim Santos, Bernardo Consoli, and Renata Vieira. Word embedding evaluation in downstream
tasks and semantic analogies. In Proceedings of the 12th Language Resources and Evaluation
Conference, pp. 4828–4834, Marseille, France, May 2020. European Language Resources Asso-
ciation. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.
594."
REFERENCES,0.29404761904761906,Published as a conference paper at ICLR 2022
REFERENCES,0.29523809523809524,"G´abor Tak´acs, Istv´an Pil´aszy, Botty´an N´emeth, and Domonkos Tikk. Scalable collaborative ﬁltering
approaches for large recommender systems. The Journal of Machine Learning Research, 10:623–
656, 2009."
REFERENCES,0.29642857142857143,"Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2, 2018."
REFERENCES,0.2976190476190476,"Zi Yin and Yuanyuan Shen.
On the dimensionality of word embedding.
arXiv preprint
arXiv:1812.04224, 2018."
REFERENCES,0.2988095238095238,"Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv
Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? arXiv preprint
arXiv:1912.03194, 2019."
REFERENCES,0.3,"Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
On
the convergence of adaptive gradient methods for nonconvex optimization.
arXiv preprint
arXiv:1808.05671, 2018a."
REFERENCES,0.3011904761904762,"Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin,
Han Li, and Kun Gai. Deep interest network for click-through rate prediction. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
1059–1068, 2018b."
REFERENCES,0.30238095238095236,"George Kingsley Zipf. Human behavior and the principle of least effort: An introduction to human
ecology. Ravenio Books, 2016."
REFERENCES,0.30357142857142855,Published as a conference paper at ICLR 2022
REFERENCES,0.3047619047619048,"A
EXPERIMENT DETAILS"
REFERENCES,0.305952380952381,"A.1
DATASETS AND PREPROCESSING"
REFERENCES,0.30714285714285716,"• Movielens-1M: Movielens-1M contains 1,000,209 anonymous ratings of approximately 3,900
movies made by 6,040 MovieLens users. Each ratings is an integer ranging from 1 to 5. We use
only user id and movie id to make prediction. We treat samples with rating less or equal to 3 as
negative examples, and samples with rating greater than 3 as positive examples.
• Criteo: The dataset consists of a portion of Criteo’s trafﬁc over a period of 7 days. Each sample
corresponds to an ad served by Criteo. The label is either 0 (indicating ad being clicked) or 1
(indicating ad being ignored). The dataset consists of 13 features (integer values) and 26 categor-
ical features. There is 45840617 total examples. For the integer valued features, we apply the log
transformation (log(x) whenever x > 2), and convert into categorical features, as suggested by
the winner of Criteo Competition 1."
REFERENCES,0.30833333333333335,"For both Movielens-1M and Criteo dataset, we random split into training set, validation set and test
set, taking up 80%, 10%, and 10% of the total samples respectively."
REFERENCES,0.30952380952380953,"Implementation: We build upon torchfm2, which contains implementation of various popular rec-
ommendation models."
REFERENCES,0.3107142857142857,"A.2
MODEL ARCHITECTURE"
REFERENCES,0.3119047619047619,"For all FM models, we use 64 as the embedding size. For all DeepFM models, we use 16 as the
embedding size, and use (16, 16) as the widths of the hidden layers."
REFERENCES,0.3130952380952381,"A.3
HYPERPARAMETERS"
REFERENCES,0.3142857142857143,"For Movielens-1M dataset, the learning rate of different algorithms are list in Table 2."
REFERENCES,0.31547619047619047,"Model Type
SGD
CF-SGD
Adagrad
Adam
FM
1e1
1e0
2e-2
1e-3
DeepFM
1e-2
2e0
4e-2
2e-3"
REFERENCES,0.31666666666666665,"Table 2: Learning rates for Movielens-1M dataset.
For Criteo dataset, the learning rate of different algorithms are list in Table 3."
REFERENCES,0.31785714285714284,"Model Type
SGD
CF-SGD
Adagrad
Adam
FM
1e-2
1e-1
1e-2
1e-3
DeepFM
1e-2
1e-2
1e-2
1e-3"
REFERENCES,0.319047619047619,"Table 3: Learning rates for Criteo dataset.
All the algorithms use 1024 as the batch size during training."
REFERENCES,0.3202380952380952,"B
ADDITIONAL EXPERIMENTS ON WORD2VEC EMBEDDING LEARNING"
REFERENCES,0.32142857142857145,"We demonstrate the effectiveness of the proposed FA/CF-SGD for embedding learning problems in
natural language modeling. Speciﬁcally, we conduct experiments for learning Word2Vec embed-
dings proposed in Mikolov et al. (2013a). Two learning models are considered:"
REFERENCES,0.32261904761904764,"(1) Continuous Bag-of-Words (CBOW): CBOW aims to predict each word (which we refer to as
the center word), given its neighboring words. The training task is deﬁned by taking each word
in the corpus as the center word, and minimize the total prediction loss.
(2) Skip-Gram: Skip-Gram aims to predict each context word, given a center word. The training
task is deﬁned by taking each word in the corpus as the center word, and minimize the total
prediction loss."
REFERENCES,0.3238095238095238,"Dataset and Preprocessing. We use WikiText-2 dataset (Merity et al., 2016), which contains 36k
text lines and 2M tokens in the training dataset. We remove extremely rare tokens with less than"
REFERENCES,0.325,"1https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/10555
2https://github.com/rixwew/pytorch-fm"
REFERENCES,0.3261904761904762,Published as a conference paper at ICLR 2022
REFERENCES,0.3273809523809524,"50 occurrences in the training dataset. Note that removing extremely rare tokens was also proposed
in the original Word2Vec paper Mikolov et al. (2013a), where only the top 1 million most frequent
tokens are selected."
REFERENCES,0.32857142857142857,"Experiment details and results. We choose the embedding dimension to be 300 as suggested value
in Mikolov et al. (2013a). Note that for each word w, Word2Vec represents it by a pair of embedding
vectors (uw, vw), which we refer to as the center embedding and context embedding, respectively.
Speciﬁcally, uw is used when w serves as the center word, and vw is used when w serves as the
context word. This makes the proposed FA/CF-SGD perfectly applicable for learning Word2Vec
embeddings, by simply setting U as the set of center embedding vectors, and V as the set of context
embedding vectors."
REFERENCES,0.32976190476190476,"We compare CF-SGD with standard SGD, and Adam. Following the suggestion from Mikolov et al.
(2013a), we decrease the learning rate linearly as epoch increases. We use an initial stepsize of 1.0
for both CF-SGD and SGD, and the stepsize of 0.025 for Adam. We iterate over the training dataset
for 20 epochs, with a batch size of 96. Note the original Word2Vec was trained with only 3 epochs,
albeit on a much larger corpus. The results are reported in Figure 5."
REFERENCES,0.33095238095238094,"0
2000
4000
6000
Iteration 5.5 6.0 6.5 7.0 7.5 8.0"
REFERENCES,0.33214285714285713,Training Loss
REFERENCES,0.3333333333333333,Skip-Gram
REFERENCES,0.3345238095238095,"SGD
Adam
CF-SGD"
REFERENCES,0.3357142857142857,(a) Training loss
REFERENCES,0.33690476190476193,"0
5
10
15
Epoch 5.5 6.0 6.5 7.0 7.5 8.0"
REFERENCES,0.3380952380952381,Test Loss
REFERENCES,0.3392857142857143,Skip-Gram
REFERENCES,0.3404761904761905,"SGD
Adam
CF-SGD"
REFERENCES,0.3416666666666667,(b) Testing loss
REFERENCES,0.34285714285714286,"0
2000
4000
6000
Iteration 4 5 6 7 8"
REFERENCES,0.34404761904761905,Training Loss CBOW
REFERENCES,0.34523809523809523,"SGD
Adam
CF-SGD"
REFERENCES,0.3464285714285714,(c) Training loss
REFERENCES,0.3476190476190476,"0
5
10
15
Epoch 5 6 7 8"
REFERENCES,0.3488095238095238,Test Loss CBOW
REFERENCES,0.35,"SGD
Adam
CF-SGD"
REFERENCES,0.35119047619047616,(d) Testing loss
REFERENCES,0.3523809523809524,"Figure 5: Comparison between CF-SGD, SGD, and Adam for learning Word2Vec embeddings on
WikiText-2 dataset.
One can clearly see that for both CBOW and Skip-Gram models, CF-SGD is able to signiﬁcantly
improve over standard SGD. For Skip-Gram model, we observe that CF-SGD even yields compa-
rable performance to Adam. For CBOW model, CF-SGD is able ﬁll in the huge performance gap
between Adam and SGD, and yields similar testing performance compared to Adam. Note that we
do not extensively tune the initial stepsize of CF-SGD, and the linearly-decaying stepsize annealing
rule was proposed in Mikolov et al. (2013a) for speeding-up SGD, which we believe might not the
optimal choice for CF-SGD. We believe further improvements can be made by searching for the best
initial learning rate and proper stepsize annealing rule for CF-SGD."
REFERENCES,0.3535714285714286,Published as a conference paper at ICLR 2022
REFERENCES,0.3547619047619048,"C
REAL WORLD TOKEN DISTRIBUTIONS"
REFERENCES,0.35595238095238096,"We plot token distributions for the ﬁrst 28 features (after preprocessing) of the benchmark recom-
mendation dataset Criteo. Note that the semantic information of features for the Criteo dataset is
undisclosed due to privacy concern."
REFERENCES,0.35714285714285715,"0
10
20
30
40
50
Token Rank 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.35833333333333334,Token Frequency
REFERENCES,0.3595238095238095,(a) Feature 0
REFERENCES,0.3607142857142857,"0
20
40
60
80
100
Token Rank 0.00 0.05 0.10 0.15"
REFERENCES,0.3619047619047619,Token Frequency
REFERENCES,0.3630952380952381,(b) Feature 1
REFERENCES,0.36428571428571427,"0
25
50
75
100
125
Token Rank 0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.36547619047619045,Token Frequency
REFERENCES,0.36666666666666664,(c) Feature 2
REFERENCES,0.3678571428571429,"0
10
20
30
40
Token Rank 0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.36904761904761907,Token Frequency
REFERENCES,0.37023809523809526,(d) Feature 3
REFERENCES,0.37142857142857144,"0
50
100
150
200
Token Rank 0.000 0.005 0.010 0.015 0.020 0.025"
REFERENCES,0.3726190476190476,Token Frequency
REFERENCES,0.3738095238095238,(e) Feature 4
REFERENCES,0.375,"0
25
50
75
100
Token Rank 0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.3761904761904762,Token Frequency
REFERENCES,0.3773809523809524,(f) Feature 5
REFERENCES,0.37857142857142856,"0
20
40
60
80
Token Rank 0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.37976190476190474,Token Frequency
REFERENCES,0.38095238095238093,(g) Feature 6
REFERENCES,0.3821428571428571,"0
20
40
60
Token Rank 0.00 0.02 0.04 0.06 0.08 0.10 0.12"
REFERENCES,0.38333333333333336,Token Frequency
REFERENCES,0.38452380952380955,(h) Feature 7
REFERENCES,0.38571428571428573,"0
20
40
60
80
Token Rank 0.00 0.01 0.02 0.03 0.04"
REFERENCES,0.3869047619047619,Token Frequency
REFERENCES,0.3880952380952381,(i) Feature 8
REFERENCES,0.3892857142857143,"0
2
4
6
8
Token Rank 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.3904761904761905,Token Frequency
REFERENCES,0.39166666666666666,(j) Feature 9
REFERENCES,0.39285714285714285,"0
10
20
30
Token Rank 0.0 0.1 0.2 0.3"
REFERENCES,0.39404761904761904,Token Frequency
REFERENCES,0.3952380952380952,(k) Feature 10
REFERENCES,0.3964285714285714,"0
10
20
30
40
Token Rank 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.3976190476190476,Token Frequency
REFERENCES,0.39880952380952384,(l) Feature 11
REFERENCES,0.4,"0
20
40
60
Token Rank 0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.4011904761904762,Token Frequency
REFERENCES,0.4023809523809524,(m) Feature 12
REFERENCES,0.4035714285714286,"0
500
1000
1500
Token Rank 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.40476190476190477,Token Frequency
REFERENCES,0.40595238095238095,(n) Feature 13
REFERENCES,0.40714285714285714,"0
200
400
Token Rank 0.00 0.02 0.04 0.06 0.08 0.10 0.12"
REFERENCES,0.4083333333333333,Token Frequency
REFERENCES,0.4095238095238095,(o) Feature 14
REFERENCES,0.4107142857142857,"0
100
200
300
Token Rank 0.0 0.2 0.4 0.6"
REFERENCES,0.4119047619047619,Token Frequency
REFERENCES,0.41309523809523807,(p) Feature 17
REFERENCES,0.4142857142857143,"0
5
10
15
Token Rank 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.4154761904761905,Token Frequency
REFERENCES,0.4166666666666667,(q) Feature 18
REFERENCES,0.41785714285714287,"0
2500
5000
7500
10000 12500
Token Rank 0.000 0.005 0.010 0.015 0.020"
REFERENCES,0.41904761904761906,Token Frequency
REFERENCES,0.42023809523809524,(r) Feature 19
REFERENCES,0.42142857142857143,"0
200
400
600
Token Rank 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
REFERENCES,0.4226190476190476,Token Frequency
REFERENCES,0.4238095238095238,(s) Feature 20
REFERENCES,0.425,"0
1
2
3
Token Rank 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.4261904761904762,Token Frequency
REFERENCES,0.42738095238095236,(t) Feature 21
REFERENCES,0.42857142857142855,"0
1000
2000
3000
4000
5000
Token Rank 0.000 0.005 0.010 0.015 0.020 0.025 0.030"
REFERENCES,0.4297619047619048,Token Frequency
REFERENCES,0.430952380952381,(u) Feature 23
REFERENCES,0.43214285714285716,"0
1000
2000
3000
Token Rank 0.000 0.005 0.010 0.015 0.020 0.025 0.030"
REFERENCES,0.43333333333333335,Token Frequency
REFERENCES,0.43452380952380953,(v) Feature 25
REFERENCES,0.4357142857142857,"0
5
10
15
20
25
Token Rank 0.0 0.1 0.2 0.3"
REFERENCES,0.4369047619047619,Token Frequency
REFERENCES,0.4380952380952381,(w) Feature 26
REFERENCES,0.4392857142857143,"0
2500
5000
7500
10000
Token Rank"
REFERENCES,0.44047619047619047,0.0000
REFERENCES,0.44166666666666665,0.0025
REFERENCES,0.44285714285714284,0.0050
REFERENCES,0.444047619047619,0.0075
REFERENCES,0.4452380952380952,0.0100
REFERENCES,0.44642857142857145,0.0125
REFERENCES,0.44761904761904764,0.0150
REFERENCES,0.4488095238095238,Token Frequency
REFERENCES,0.45,(x) Feature 27
REFERENCES,0.4511904761904762,Published as a conference paper at ICLR 2022
REFERENCES,0.4523809523809524,"D
ANALYSIS"
REFERENCES,0.45357142857142857,"Throughout our analysis, we use ξt = (it, jt) to denote the random user/item ids sampled from the
unknown distribution D. We use ξ[t] = {ξs}t−1
s=0 to denote the random samples collected up to the
beginning of the t-th iteration, and use ξ and ξ[T ] interchangeably when the context is clear. Finally,
we use Ft = σ(ξ[t]) to denote the σ-algebra generated by the random variables ξ[t]."
REFERENCES,0.45476190476190476,"Proof of Proposition 2.1. Let ∇f t
U ∈R|U|×d denote the submatrix that contains gradient of users
embeddings, and ∇f t
V
∈R|V |×d for the gradient of item embeddings.
Similarly, let gt
U ∈
R|U|×d, gt
V ∈R|V |×d denote the stochastic gradient of user and item embeddings, respectively."
REFERENCES,0.45595238095238094,"Eit,jt
gt
U −∇f t
U
2 = Eit "
REFERENCES,0.45714285714285713,"Ejt|it
∇f t
it −gt
it
2 +
X"
REFERENCES,0.4583333333333333,"i̸=it,i∈U"
REFERENCES,0.4595238095238095,"∇f t
i
2
  = Eit "
REFERENCES,0.4607142857142857,(1 −1
REFERENCES,0.46190476190476193,"pit
)2 ∇f t
it
2 + Ejt|it
δt
it
2 +
X"
REFERENCES,0.4630952380952381,"i̸=it,i∈U"
REFERENCES,0.4642857142857143,"∇f t
i
2
  =
X"
REFERENCES,0.4654761904761905,"i∈U
piEjt|it=i
δt
it
2 +
X"
REFERENCES,0.4666666666666667,"i∈U
pi(1 −1"
REFERENCES,0.46785714285714286,"pi
)2 ∇f t
i
2 +
X"
REFERENCES,0.46904761904761905,"i∈U
pi
X i′̸=i"
REFERENCES,0.47023809523809523,"∇f t
i′
2 =
X"
REFERENCES,0.4714285714285714,"i∈U
piEjt|it=i
δt
it
2 +
X"
REFERENCES,0.4726190476190476,"i∈U
pi(1 −1"
REFERENCES,0.4738095238095238,"pi
)2 ∇f t
i
2 +
X"
REFERENCES,0.475,"i∈U
(1 −pi)
∇f t
i
2 ."
REFERENCES,0.47619047619047616,"Similarly, we can show"
REFERENCES,0.4773809523809524,"Eit,jt
gt
V −∇f t
V
2 =
X"
REFERENCES,0.4785714285714286,"j∈V
pjEit|jt=j
δt
jt
2 +
X"
REFERENCES,0.4797619047619048,"j∈V
pj(1 −1"
REFERENCES,0.48095238095238096,"pj
)2 ∇f t
j
2 +
X"
REFERENCES,0.48214285714285715,"j∈V
(1 −pj)
∇f t
j
2 ."
REFERENCES,0.48333333333333334,"Note that ∥gt −∇f t∥2 = ∥gt
U −∇f t
U∥2 + ∥gt
V −∇f t
V ∥2 , from which we conclude the proof."
REFERENCES,0.4845238095238095,"Proposition D.1. Given Assumption 2, let δi, δj ∈Rd, and Θ′ satisfy θ′
i = θi + δi, δ′
j = θj + δj,
we have"
REFERENCES,0.4857142857142857,"f(Θ′) ≤f(Θ) + ⟨∇θif, δi⟩+

∇θjf, δj

+ Li"
REFERENCES,0.4869047619047619,2 ∥δi∥2 + Lj
REFERENCES,0.4880952380952381,"2 ∥δj∥2 ,"
REFERENCES,0.48928571428571427,where Lk = 2Lpk for all k ∈X.
REFERENCES,0.49047619047619045,"Proof. Apply second-order Taylor expansion, we have"
REFERENCES,0.49166666666666664,"f(Θ′) = f(Θ) + ⟨∇θif, δi⟩+

∇θjf, δj

+ 1"
REFERENCES,0.4928571428571429,"2(δ⊤
i , δ⊤
j )
 eH
eE
eE⊤
eF"
REFERENCES,0.49404761904761907,"
(δi, δj) where"
REFERENCES,0.49523809523809526,"eH = ∇2
θiθif(eΘ) =
X"
REFERENCES,0.49642857142857144,"j∈V
D(i, j)∇2
uuℓ(eθi, eθj; yij),"
REFERENCES,0.4976190476190476,"eE = ∇2
θiθjf(eΘ) = D(i, j)∇uvℓ(eθi, eθj; yij),"
REFERENCES,0.4988095238095238,"eF = ∇2
θjθjf(eΘ) =
X"
REFERENCES,0.5,"i∈U
D(i, j)∇2
vvℓ(eθi, eθj; yij) =
X"
REFERENCES,0.5011904761904762,"i∈U
D(i, j)∇2
uuℓ(eθi, eθj; yij),"
REFERENCES,0.5023809523809524,"for some eΘ as convex combination of Θ and Θ′, and the last equality uses the fact that ℓ(u; v) is
symmetric w.r.t u and v. Now given the assumption that
∇2
uuℓ(·, ·; ·)

2 ≤L,
∇2
uvℓ(·, ·; ·)

2 ≤L,"
REFERENCES,0.5035714285714286,Published as a conference paper at ICLR 2022
REFERENCES,0.5047619047619047,we have
REFERENCES,0.5059523809523809,"f(Θ′) = f(Θ) + ⟨∇θif, δi⟩+

∇θjf, δj

+ δ⊤
i eHδi + δ⊤
j eFδj + 2δ⊤
i eEδj"
REFERENCES,0.5071428571428571,"≤f(Θ) + ⟨∇θif, δi⟩+

∇θjf, δj

+ L 2  X"
REFERENCES,0.5083333333333333,"j′∈V
D(i, j′) ∥δi∥2 +
X"
REFERENCES,0.5095238095238095,"u′∈U
D(i′, j) ∥δj∥2"
REFERENCES,0.5107142857142857,"+ D(i, j) ∥δi∥2 + D(i, j) ∥δj∥2
"
REFERENCES,0.5119047619047619,"= f(Θ) + ⟨∇θif, δi⟩+

∇θjf, δj

+
X"
REFERENCES,0.513095238095238,"j′∈V
LD(i, j′) ∥δi∥2 +
X"
REFERENCES,0.5142857142857142,"u′∈U
LD(i′, j) ∥δj∥2"
REFERENCES,0.5154761904761904,"= f(Θ) + ⟨∇θif, δi⟩+

∇θjf, δj

+ Li"
REFERENCES,0.5166666666666667,2 ∥δi∥2 + Lj
REFERENCES,0.5178571428571429,"2 ∥δj∥2 ,"
REFERENCES,0.5190476190476191,"where in the ﬁrst inequality we use δ⊤
i eEδj ≤L ∥δi∥∥δj∥≤
L"
REFERENCES,0.5202380952380953,"2 (∥δi∥2 + ∥δj∥2), and in the last
equality we use the deﬁnition that Lk = 2Lpk for all k ∈X."
REFERENCES,0.5214285714285715,"Before we specify the concrete learning rate, we have the following generic convergence character-
ization.
Proposition D.2. Given learning rate {ηt
k}k∈X,t∈[T ], we have the following holds for Algorithm 1. E T
X t=0 X k∈X"
REFERENCES,0.5226190476190476,"
ηt
k −Lk(ηt
k)2 pk"
REFERENCES,0.5238095238095238," ∇f t
k
2 ≤f(Θ0) −f ∗+ 2 T
X t=0 X"
REFERENCES,0.525,"k∈X
pkLk(ηt
k)2σ2
k."
REFERENCES,0.5261904761904762,"Proof. From Proposition D.1, we have"
REFERENCES,0.5273809523809524,"f(Θt+1) ≤f(Θt) +

∇f t
it, θt+1
it
−θt
it

+

∇f t
jt, θt+1
jt
−θt
jt

+ Lit 2"
REFERENCES,0.5285714285714286,"θt+1
it
−θt
it
2 + Ljt 2"
REFERENCES,0.5297619047619048,"θt+1
jt
−θt
jt
2"
REFERENCES,0.530952380952381,"= f(Θt) −ηt
it

∇f t
it, gt
it

−ηt
jt

∇f t
jt, gt
jt

+ Lit"
REFERENCES,0.5321428571428571,"2 η2
it
gt
it
2 + Ljt"
REFERENCES,0.5333333333333333,"2 η2
jt
gt
jt
2
(14)"
REFERENCES,0.5345238095238095,"Conditioned on past history Ft, we have"
REFERENCES,0.5357142857142857,"Eit,jt

ηt
it

∇f t
it, gt
it

= EitEjt|it

ηt
it

∇f t
it, gt
it

=
X"
REFERENCES,0.5369047619047619,"i∈U
piηt
i
∥∇f t
i ∥2"
REFERENCES,0.5380952380952381,"pi
=
X"
REFERENCES,0.5392857142857143,"i∈U
ηt
i
∇f t
i
2 . (15)"
REFERENCES,0.5404761904761904,"Similarly, we have"
REFERENCES,0.5416666666666666,"Eit,jt

ηt
jt

∇f t
jt, gt
jt

=
X"
REFERENCES,0.5428571428571428,"i∈V
ηt
j
∇f t
j
2 .
(16)"
REFERENCES,0.544047619047619,"On the other hand, we have"
REFERENCES,0.5452380952380952,"Eit,jt
h"
REFERENCES,0.5464285714285714,"Lit(ηt
it)2 gt
it
2i
= E """
REFERENCES,0.5476190476190477,"Lit(ηt
it)2

1
pit
∇f t
it + δt
it  2#"
REFERENCES,0.5488095238095239,"≤2EitEjt|it """
REFERENCES,0.55,"Lit(ηt
it)2
 ∇f t
it
2"
REFERENCES,0.5511904761904762,"p2
it
+
δt
it
2
!#"
REFERENCES,0.5523809523809524,"≤2EitLit(ηt
it)2
 ∇f t
it
2"
REFERENCES,0.5535714285714286,"p2
it
+ σ2
it ! = 2
X"
REFERENCES,0.5547619047619048,"i∈U
piLi(ηt
i)2
 
∥∇f t
i ∥2"
REFERENCES,0.555952380952381,"p2
i
+ σ2
i !"
REFERENCES,0.5571428571428572,",
(17)"
REFERENCES,0.5583333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.5595238095238095,"where the ﬁrst inequality uses ∥a + b∥2 ≤2 ∥a∥2 + 2 ∥b∥2, the second inequality uses (5), and the
ﬁnal equality uses the deﬁnition of Li in Proposition D.1. Following similar arguments, we also
have"
REFERENCES,0.5607142857142857,"Eit,jt
h
Lit,jt(ηt
jt)2 gt
jt
2i
≤2
X"
REFERENCES,0.5619047619047619,"j∈V
pjLj(ηt
j)2
 ∇f t
j
2"
REFERENCES,0.5630952380952381,"p2
j
+ σ2
j !"
REFERENCES,0.5642857142857143,".
(18)"
REFERENCES,0.5654761904761905,"Plug in (15) (16) (17) (18) back into (14), we obtain"
REFERENCES,0.5666666666666667,"Eit,jt

f(Θt+1)|Ft

≤f(Θt) −
X i∈U"
REFERENCES,0.5678571428571428,"
ηt
i −Li(ηt
i)2 pi"
REFERENCES,0.569047619047619," ∇f t
i
2 −
X j∈V "
REFERENCES,0.5702380952380952,"ηt
j −Lj(ηt
j)2 pj"
REFERENCES,0.5714285714285714,"!
∇f t
j
2 + 2  X"
REFERENCES,0.5726190476190476,"i∈U
piLi(ηt
i)2σ2
i +
X"
REFERENCES,0.5738095238095238,"j∈U
pjLj(ηt
j)2σ2
j  "
REFERENCES,0.575,"= f(Θt) −
X k∈X"
REFERENCES,0.5761904761904761,"
ηt
k −Lk(ηt
k)2 pk"
REFERENCES,0.5773809523809523," ∇f t
k
2 + 2
X"
REFERENCES,0.5785714285714286,"k∈X
pkLk(ηt
k)2σ2
k."
REFERENCES,0.5797619047619048,"Equivalently, we have
X k∈X"
REFERENCES,0.580952380952381,"
ηt
k −Lk(ηt
k)2 pk"
REFERENCES,0.5821428571428572," ∇f t
k
2 ≤f(Θt) −Eit,jt

f(Θt+1)|Ft

+ 2
X"
REFERENCES,0.5833333333333334,"k∈X
pkLk(ηt
k)2σ2
k.
(19)"
REFERENCES,0.5845238095238096,"Sum up (19) from t = 0 to T and take total expectation, we have E T
X t=0 X k∈X"
REFERENCES,0.5857142857142857,"
ηt
k −Lk(ηt
k)2 pk"
REFERENCES,0.5869047619047619," ∇f t
k
2 ≤f(Θ0) −f ∗+ 2 T
X t=0 X"
REFERENCES,0.5880952380952381,"k∈X
pkLk(ηt
k)2σ2
k."
REFERENCES,0.5892857142857143,"Proof of Theorem 3.1. Given Proposition D.2, suppose we use constant stepsize, i.e., ηt
k = ηk for
all t ∈[T], and sample τ ∼Unif([T]), then for any k ∈X, E T
X t=0"
REFERENCES,0.5904761904761905,"
ηk −Lk(ηk)2 pk"
REFERENCES,0.5916666666666667," ∇f t
k
2 ≤f(Θ0) −f ∗+ 2 T
X t=0 X"
REFERENCES,0.5928571428571429,"k∈X
pkLk(ηk)2σ2
k,"
REFERENCES,0.594047619047619,"which implies that for any k ∈X,"
REFERENCES,0.5952380952380952,"E ∥∇f τ
k ∥2 ≤
f(Θ0) −f ∗"
REFERENCES,0.5964285714285714,"T

ηk −Lk(ηk)2 pk"
REFERENCES,0.5976190476190476," + 2
P"
REFERENCES,0.5988095238095238,"l∈X plLl(ηl)2σ2
l

ηk −Lk(ηk)2 pk "
REFERENCES,0.6,"For a given α > 0, we choose {ηt
k} as the following"
REFERENCES,0.6011904761904762,"ηt
k = min
 1"
REFERENCES,0.6023809523809524,"4L,
α
√Tpk 
,"
REFERENCES,0.6035714285714285,"Combined with Proposition D.1, we have ηk −Lk(ηk)2"
REFERENCES,0.6047619047619047,"pk
= ηk −2L(ηk)2 ≥ηk"
REFERENCES,0.6059523809523809,"2 , and hence"
REFERENCES,0.6071428571428571,"E ∥∇f τ
k ∥2 ≤2
 
f(Θ0) −f ∗"
REFERENCES,0.6083333333333333,"Tηk
+ 4 P"
REFERENCES,0.6095238095238096,"l∈X plLl(ηl)2σ2
l
ηk
."
REFERENCES,0.6107142857142858,"We can bound the ﬁrst term by
 
f(Θ0) −f ∗ Tηk
="
REFERENCES,0.611904761904762," 
f(Θ0) −f ∗"
REFERENCES,0.6130952380952381,"T
max

4L,
√Tpk α  = O"
REFERENCES,0.6142857142857143,"(
L
 
f(Θ0) −f ∗"
REFERENCES,0.6154761904761905,"T
+
√pk
 
f(Θ0) −f ∗ α
√ T ) ."
REFERENCES,0.6166666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.6178571428571429,"In addition, since X"
REFERENCES,0.6190476190476191,"l∈X
plLlσ2
l (ηl)2 ≤
X"
REFERENCES,0.6202380952380953,"l∈X
p2
l Lσ2
l
α2"
REFERENCES,0.6214285714285714,"Tpl
= L P"
REFERENCES,0.6226190476190476,"l∈X plσ2
l α2 T
,"
REFERENCES,0.6238095238095238,"thus we can bound the second term by
P
l∈X plLl(ηl)2σ2
l
ηk
≤L P
l∈X plσ2
l α2"
REFERENCES,0.625,"T
max

4L,
√pkT α "
REFERENCES,0.6261904761904762,"= O
L2α2 P"
REFERENCES,0.6273809523809524,"l∈X plσ2
l
T
+ L√pk
P"
REFERENCES,0.6285714285714286,"l∈X plσ2
l α
√ T 
."
REFERENCES,0.6297619047619047,Thus we have
REFERENCES,0.6309523809523809,"E ∥∇f τ
k ∥2 = O"
REFERENCES,0.6321428571428571,"(
L
 
f(Θ0) −f ∗"
REFERENCES,0.6333333333333333,"T
+
√pk
 
f(Θ0) −f ∗ α
√"
REFERENCES,0.6345238095238095,"T
+ L2α2 P"
REFERENCES,0.6357142857142857,"l∈X plσ2
l
T
+ L√pk
P"
REFERENCES,0.6369047619047619,"l∈X plσ2
l α
√ T ) . (20)"
REFERENCES,0.638095238095238,"By choosing α =
q"
REFERENCES,0.6392857142857142,f(Θ0)−f ∗ L P
REFERENCES,0.6404761904761904,"l∈X plσ2
l , we have"
REFERENCES,0.6416666666666667,"√pk
 
f(Θ0) −f ∗ α
√"
REFERENCES,0.6428571428571429,"T
+ L√pk
P
l∈X plσ2
l α
√ T
= O 
 "
REFERENCES,0.6440476190476191,"√pk
qP
l∈X plσ2
l (f(Θ0) −f ∗)L
√ T 
"
REFERENCES,0.6452380952380953,",
(21)"
REFERENCES,0.6464285714285715,L2α2 P
REFERENCES,0.6476190476190476,"l∈X plσ2
l
T
= L
 
f(Θ0) −f ∗"
REFERENCES,0.6488095238095238,"T
.
(22)"
REFERENCES,0.65,"Thus combining (20) (21) (22), we have"
REFERENCES,0.6511904761904762,"E ∥∇f τ
k ∥2 = O 
"
REFERENCES,0.6523809523809524,"
L
 
f(Θ0) −f ∗ T
+"
REFERENCES,0.6535714285714286,"√pk
qP
l∈X plσ2
l (f(Θ0) −f ∗)L
√ T 
"
REFERENCES,0.6547619047619048,",
∀k ∈X."
REFERENCES,0.655952380952381,"Proof of Theorem 3.2. Given Proposition D.2, suppose we use token-agnostic constant stepsize, i.e.,
ηt
k = η for all t ∈[T], k ∈X, and sample τ ∼Unif([T]), then for any k ∈X, E T
X t=0"
REFERENCES,0.6571428571428571,"
η −Lk(η)2 pk"
REFERENCES,0.6583333333333333," ∇f t
k
2 ≤f(Θ0) −f ∗+ 2 T
X t=0 X"
REFERENCES,0.6595238095238095,"k∈X
pkLk(η)2σ2
k,"
REFERENCES,0.6607142857142857,"which implies that for any k ∈X,"
REFERENCES,0.6619047619047619,"E ∥∇f τ
k ∥2 ≤
f(Θ0) −f ∗"
REFERENCES,0.6630952380952381,"T

η −Lk(η)2 pk"
REFERENCES,0.6642857142857143," + 2
P"
REFERENCES,0.6654761904761904,"l∈X plLl(ηl)2σ2
l

η −Lk(η)2 pk "
REFERENCES,0.6666666666666666,"For a given α > 0, we choose {ηt} as the following"
REFERENCES,0.6678571428571428,"ηt = min
 1"
REFERENCES,0.669047619047619,"4L, α
√ T 
,"
REFERENCES,0.6702380952380952,"Combined with Proposition D.1, we have η −Lk(η)2"
REFERENCES,0.6714285714285714,"pk
= η −2L(η)2 ≥η"
REFERENCES,0.6726190476190477,"2, and hence"
REFERENCES,0.6738095238095239,"E ∥∇f τ
k ∥2 ≤2
 
f(Θ0) −f ∗"
REFERENCES,0.675,"Tη
+ 4 P"
REFERENCES,0.6761904761904762,"l∈X plLl(ηl)2σ2
l
η
."
REFERENCES,0.6773809523809524,Published as a conference paper at ICLR 2022
REFERENCES,0.6785714285714286,"We can bound the ﬁrst term by
 
f(Θ0) −f ∗ Tη
="
REFERENCES,0.6797619047619048," 
f(Θ0) −f ∗ T
max ( 4L, √ T
α ) = O"
REFERENCES,0.680952380952381,"(
L
 
f(Θ0) −f ∗ T
+"
REFERENCES,0.6821428571428572," 
f(Θ0) −f ∗ α
√ T ) ."
REFERENCES,0.6833333333333333,"In addition, since X"
REFERENCES,0.6845238095238095,"l∈X
plLlσ2
l (ηl)2 ≤
X"
REFERENCES,0.6857142857142857,"l∈X
p2
l Lσ2
l
α2"
REFERENCES,0.6869047619047619,"T = L P
l∈X p2
l σ2
l α2 T
,"
REFERENCES,0.6880952380952381,"thus we can bound the second term by
P"
REFERENCES,0.6892857142857143,"l∈X p2
l Ll(ηl)2σ2
l
η
≤L P"
REFERENCES,0.6904761904761905,"l∈X p2
l σ2
l α2 T
max ( 4L, √ T
α )"
REFERENCES,0.6916666666666667,"= O
L2α2 P"
REFERENCES,0.6928571428571428,"l∈X p2
l σ2
l
T
+ L P"
REFERENCES,0.694047619047619,"l∈X p2
l σ2
l α
√ T 
."
REFERENCES,0.6952380952380952,Thus we have
REFERENCES,0.6964285714285714,"E ∥∇f τ
k ∥2 = O"
REFERENCES,0.6976190476190476,"(
L
 
f(Θ0) −f ∗ T
+"
REFERENCES,0.6988095238095238," 
f(Θ0) −f ∗ α
√"
REFERENCES,0.7,"T
+ L2α2 P"
REFERENCES,0.7011904761904761,"l∈X p2
l σ2
l
T
+ L P"
REFERENCES,0.7023809523809523,"l∈X p2
l σ2
l α
√ T ) . (23)"
REFERENCES,0.7035714285714286,"By choosing α =
q"
REFERENCES,0.7047619047619048,f(Θ0)−f ∗ L P
REFERENCES,0.705952380952381,"l∈X p2
l σ2
l , we have"
REFERENCES,0.7071428571428572," 
f(Θ0) −f ∗ α
√"
REFERENCES,0.7083333333333334,"T
+ L P"
REFERENCES,0.7095238095238096,"l∈X p2
l σ2
l α
√ T
= O 
  qP"
REFERENCES,0.7107142857142857,"l∈X p2
l σ2
l (f(Θ0) −f ∗)L
√ T 
"
REFERENCES,0.7119047619047619,",
(24)"
REFERENCES,0.7130952380952381,L2α2 P
REFERENCES,0.7142857142857143,"l∈X p2
l σ2
l
T
= L
 
f(Θ0) −f ∗"
REFERENCES,0.7154761904761905,"T
.
(25)"
REFERENCES,0.7166666666666667,"Thus combining (23) (24) (25), we have"
REFERENCES,0.7178571428571429,"E ∥∇f τ
k ∥2 = O 
"
REFERENCES,0.719047619047619,"
L
 
f(Θ0) −f ∗ T
+ qP"
REFERENCES,0.7202380952380952,"l∈X p2
l σ2
l (f(Θ0) −f ∗)L
√ T 
"
REFERENCES,0.7214285714285714,",
∀k ∈X."
REFERENCES,0.7226190476190476,"Proof of Theorem 3.3. We deﬁne bpt
i = Pt
s=0 1 {it = i} /t, bpt
j = Pt
s=0 1 {jt = j} /t, and bηt
k ="
REFERENCES,0.7238095238095238,"min

1
2L,
α
√"
REFERENCES,0.725,"T bpt
k"
REFERENCES,0.7261904761904762,"
for all t ∈[T], i ∈U, j ∈V, k ∈X. In addition, we deﬁne the frequency-"
REFERENCES,0.7273809523809524,"dependent learning rates ηt
k = min
n
1
2L,
α
√T pk o
."
REFERENCES,0.7285714285714285,"Note that (19) still holds. Sum up (19) from t = 0 to T and take total expectation, we have Eξ T
X t=0 X k∈X"
REFERENCES,0.7297619047619047,"
bηt
k −Lk(bηt
k)2 pk"
REFERENCES,0.7309523809523809," ∇f t
k
2 ≤f(Θ0) −f ∗+ 2Eξ T
X t=0 X"
REFERENCES,0.7321428571428571,"k∈X
pkLk(bηt
k)2σ2
k.
(26)"
REFERENCES,0.7333333333333333,"In contrast to FA-SGD and standard SGD, here the stepsize bηt
k ∈Ft is also a random variable. We
ﬁrst proceed to upper bound the right hand side of (26)."
REFERENCES,0.7345238095238096,Published as a conference paper at ICLR 2022
REFERENCES,0.7357142857142858,"For each k ∈X, denote Tk =
c
pk , where c > 0 is any absolute constant. Note that we have for any
t ≥Tk,"
REFERENCES,0.736904761904762,"P

|bpt
k −pk| ≥pk 2"
REFERENCES,0.7380952380952381,"
≤P

|bpt
k −pk| ≥pk 2"
REFERENCES,0.7392857142857143,"
≤exp

−
tp2
k
pk(1 −pk)"
REFERENCES,0.7404761904761905,"
≤exp (−tpk) .
(27)"
REFERENCES,0.7416666666666667,"Moreover, we have"
REFERENCES,0.7428571428571429,"|bpt
k −pk| ≤pk"
REFERENCES,0.7440476190476191,"2
⇒
|bηt
k −ηt
k| ≤α0ηt
k,
(28)"
REFERENCES,0.7452380952380953,"where α0 = max
n
1 −
p"
REFERENCES,0.7464285714285714,"2/3,
√"
REFERENCES,0.7476190476190476,"2 −1
o
< 1. Note that for any T > Tk, T
X"
REFERENCES,0.7488095238095238,"t=0
pkLkσ2
kEξ

(bηt
k)2
= Tk
X"
REFERENCES,0.75,"t=0
pkLkσ2
kEξ

(bηt
k)2
+ T
X"
REFERENCES,0.7511904761904762,"t=Tk+1
pkLkσ2
kEξ

(bηt
k)2
,"
REFERENCES,0.7523809523809524,"To bound the ﬁrst term, note that from deﬁnition of bηt
k, we have bηt
k ≤1"
REFERENCES,0.7535714285714286,"L, hence Tk
X"
REFERENCES,0.7547619047619047,"t=0
pkLkσ2
kEξ

(bηt
k)2
≤ Tk
X"
REFERENCES,0.7559523809523809,"t=0
pkLkσ2
k  1 L"
REFERENCES,0.7571428571428571,"2
= TkpkLkσ2
k  1 L"
REFERENCES,0.7583333333333333,"2
= cLkσ2
k  1 L 2
."
REFERENCES,0.7595238095238095,"To bound the second term, note that from (27), for any t ≥Tk with probability at least 1 −δt
k (here
δt
k = exp(−tpk)), we have that |ηt
k −bηt
k| ≤α0ηt
k. Denote Ht
k = {ω : |ηt
k −bηt
k| ≤α0ηt
k}, we have"
REFERENCES,0.7607142857142857,"Eξ(bηt
k)2 = Eξ(bηt
k)21Ht
k + Eξ(bηt
k)21(Ht
k)c"
REFERENCES,0.7619047619047619,"≤(1 + α0)2(ηt
k)2Eξ1Ht
k +
 1 L"
REFERENCES,0.763095238095238,"2
Eξ1(Ht
k)c"
REFERENCES,0.7642857142857142,"≤(1 + α0)2(ηt
k)2 + δt
k  1 L 2
."
REFERENCES,0.7654761904761904,"Hence for any t ≥Tk, T
X"
REFERENCES,0.7666666666666667,"t=Tk+1
pkLkσ2
kEξ

(bηt
k)2
≤ T
X"
REFERENCES,0.7678571428571429,"t=0
(1 + α0)2pkLkσ2
k(ηt
k)2 + T
X"
REFERENCES,0.7690476190476191,"t=Tk+1
δt
kpkLkσ2
k  1 L 2
."
REFERENCES,0.7702380952380953,"Thus we obtain T
X"
REFERENCES,0.7714285714285715,"t=0
pkLkσ2
kEξ

(bηt
k)2
≤cLkσ2
k  1 L 2
+ T
X"
REFERENCES,0.7726190476190476,"t=0
(1 + α0)2pkLkσ2
k(ηt
k)2 + T
X"
REFERENCES,0.7738095238095238,"t=Tk+1
δt
kpkLkσ2
k  1 L 2"
REFERENCES,0.775,"≤cLkσ2
k  1 L 2
+ T
X"
REFERENCES,0.7761904761904762,"t=0
(1 + α0)2pkLkσ2
k(ηt
k)2 +
exp(−c)pk
1 −exp(−pk)Lkσ2
k  1 L 2"
REFERENCES,0.7773809523809524,"≤cLkσ2
k  1 L 2
+ T
X"
REFERENCES,0.7785714285714286,"t=0
(1 + α0)2pkLkσ2
k(ηt
k)2 + α1 exp(−c)Lkσ2
k  1 L 2
,"
REFERENCES,0.7797619047619048,"where the last inequality uses the fact that PT
t=Tk+1 δt
k ≤P∞
t=Tk δt
k = exp(−Tkpk)"
REFERENCES,0.780952380952381,"1−exp(−pk) =
exp(−c)
1−exp(−pk),
and α1 = supp∈(0,1)
p
1−exp(−p)."
REFERENCES,0.7821428571428571,"Hence by denoting T0 = maxk∈X Tk, we have that for any k ∈X, and t ≥T0, Eξ T
X t=0"
REFERENCES,0.7833333333333333,"
bηt
k −Lk(bηt
k)2 pk"
REFERENCES,0.7845238095238095," ∇f t
k
2 ≤f(Θ0) −f ∗+ 2
 X"
REFERENCES,0.7857142857142857,"k∈X
cLkσ2
k  1 L 2"
REFERENCES,0.7869047619047619,"|
{z
}
(A) + T
X t=0 X"
REFERENCES,0.7880952380952381,"k∈X
(1 + α0)2pkLkσ2
k(ηt
k)2"
REFERENCES,0.7892857142857143,"|
{z
}
(B)"
REFERENCES,0.7904761904761904,"+ α1 exp(−c)Lkσ2
k  1 L 2"
REFERENCES,0.7916666666666666,"|
{z
}
(C) 
. (29)"
REFERENCES,0.7928571428571428,Published as a conference paper at ICLR 2022
REFERENCES,0.794047619047619,"Note that term (B) in (29) can be bounded following exactly the same step as in the proof of Theorem
3.1, for which we have T
X"
REFERENCES,0.7952380952380952,"l∈X
plLlσ2
l (ηl)2 ≤T
X"
REFERENCES,0.7964285714285714,"l∈X
p2
l Lσ2
l
α2"
REFERENCES,0.7976190476190477,"Tpl
= L
X"
REFERENCES,0.7988095238095239,"l∈X
plσ2
l α2."
REFERENCES,0.8,"Hence for any constant c > 0 (we can readily choose c = 1), we have Eξ T
X t=0"
REFERENCES,0.8011904761904762,"
bηt
k −Lk(bηt
k)2 pk"
REFERENCES,0.8023809523809524," ∇f t
k
2 = f(Θ0) −f ∗+ 2

c
X k∈X"
REFERENCES,0.8035714285714286,"Lkσ2
k  1 L 2"
REFERENCES,0.8047619047619048,"|
{z
}
(A′)"
REFERENCES,0.805952380952381,"+ (1 + α0)2L
X"
REFERENCES,0.8071428571428572,"k∈X
pkσ2
kα2"
REFERENCES,0.8083333333333333,"|
{z
}
(B′) +
X"
REFERENCES,0.8095238095238095,"k∈X
α1 exp(−c)Lkσ2
k  1 L 2"
REFERENCES,0.8107142857142857,"|
{z
}
(C′) 
."
REFERENCES,0.8119047619047619,"By the deﬁnition of bηt
k, we also have bηt
k −Lk(bηt
k)2"
REFERENCES,0.8130952380952381,"pk
≥bηt
k
2 , then Eξ T
X t=0"
REFERENCES,0.8142857142857143,"
bηt
k −Lk(bηt
k)2 pk"
REFERENCES,0.8154761904761905," ∇f t
k
2 ≥Eξ T
X t=0"
REFERENCES,0.8166666666666667,"bηt
k
2"
REFERENCES,0.8178571428571428,"∇f t
k
2 ."
REFERENCES,0.819047619047619,"Hence we obtain Eξ T
X t=0"
REFERENCES,0.8202380952380952,"bηt
k
2"
REFERENCES,0.8214285714285714,"∇f t
k
2
≤f(Θ0) −f ∗+ 2

c
X k∈X"
REFERENCES,0.8226190476190476,"Lkσ2
k  1 L 2"
REFERENCES,0.8238095238095238,"|
{z
}
(A′)"
REFERENCES,0.825,"+ (1 + α0)2L
X"
REFERENCES,0.8261904761904761,"k∈X
pkσ2
kα2"
REFERENCES,0.8273809523809523,"|
{z
}
(B′) +
X"
REFERENCES,0.8285714285714286,"k∈X
α1 exp(−c)Lkσ2
k  1 L 2"
REFERENCES,0.8297619047619048,"|
{z
}
(C′)"
REFERENCES,0.830952380952381,"
,
(30)"
REFERENCES,0.8321428571428572,"or equivalently, Eξ T
X t=0"
REFERENCES,0.8333333333333334,"bηt
k
PT
t=0 ηt
k"
REFERENCES,0.8345238095238096,"∇f t
k
2 ≤2
 
f(Θ0) −f ∗"
REFERENCES,0.8357142857142857,"PT
t=0 ηt
k
+
4(A′)
PT
t=0 ηt
k
+
4(B′)
PT
t=0 ηt
k
+
4(C′)
PT
t=0 ηt
k
.
(31)"
REFERENCES,0.8369047619047619,"Let eT0 denote a positive integer to be determined later, recall that from (27), for any t ≥T0, with
probability at least 1 −eδt
k (here eδt
k = exp(−tpk)), we have |ηt
k −bηt
k| ≤α0ηt
k hold. Denote
Bt
k = {w : |ηt
k −bηt
k| ≤α0ηt
k}, then we have that for any k ∈X, Eξ T
X t=0"
REFERENCES,0.8380952380952381,"bηt
k
PT
t=0 ηt
k"
REFERENCES,0.8392857142857143,"∇f t
k
2 ≥Eξ T
X"
REFERENCES,0.8404761904761905,"t= e
T0"
REFERENCES,0.8416666666666667,"bηt
k
PT
t=0 ηt
k"
REFERENCES,0.8428571428571429,"∇f t
k
2 ≥Eξ T
X"
REFERENCES,0.844047619047619,"t= e
T0"
REFERENCES,0.8452380952380952,"bηt
k
PT
t=0 ηt
k"
REFERENCES,0.8464285714285714,"∇f t
k
2 1Bt
k"
REFERENCES,0.8476190476190476,"≥(1 −α0) T
X"
REFERENCES,0.8488095238095238,"t= e
T0"
REFERENCES,0.85,"ηt
k
PT
t=0 ηt
k
Eξ
∇f t
k
2 1Bt
k"
REFERENCES,0.8511904761904762,"≥(1 −α0) T
X"
REFERENCES,0.8523809523809524,"t= e
T0"
REFERENCES,0.8535714285714285,"ηt
k
PT
t=0 ηt
k"
REFERENCES,0.8547619047619047,"n
Eξ
∇f t
k
2 −eδt
kG2o"
REFERENCES,0.8559523809523809,"= (1 −α0)(T −eT0) T T
X"
REFERENCES,0.8571428571428571,"t= e
T0 1"
REFERENCES,0.8583333333333333,T0 −eT0
REFERENCES,0.8595238095238096,"n
Eξ
∇f t
k
2 −eδt
kG2o"
REFERENCES,0.8607142857142858,≥(1 −α0)(T −eT0) T
REFERENCES,0.861904761904762,"n
EτEξ ∥∇f τ
k ∥2 −eδT0
k G2o
,
(32)"
REFERENCES,0.8630952380952381,Published as a conference paper at ICLR 2022
REFERENCES,0.8642857142857143,"where the fourth inequality uses the assumption that ∥∇f t
k∥2 ≤G2, and the last equality follows
from the deﬁnition that τ ∼Unif{ eT0, . . . , T}. Finally, choose eT0 = T/2, combine (31) and (32),
we obtain"
REFERENCES,0.8654761904761905,"EτEξ ∥∇f τ
k ∥2 = O"
REFERENCES,0.8666666666666667,"(
2
 
f(Θ0) −f ∗"
REFERENCES,0.8678571428571429,"PT
t=0 ηt
k
+
4(A′)
PT
t=0 ηt
k
+
4(B′)
PT
t=0 ηt
k
+
4(C′)
PT
t=0 ηt
k
+ eδT0
k G2
) ."
REFERENCES,0.8690476190476191,"Combine with the deﬁnition of (A’), (B’), (C’) in (30), and the deﬁnition of eδT0
k , we obtain"
REFERENCES,0.8702380952380953,"EτEξ ∥∇f τ
k ∥2 = O
f(Θ0) −f ∗+ P"
REFERENCES,0.8714285714285714,"k∈X Lkσ2
k
  1"
REFERENCES,0.8726190476190476,"L
2
PT
t=0 ηt
k
+ (1 + α0)2L P"
REFERENCES,0.8738095238095238,"k∈X pkσ2
kα2
PT
t=0 ηt
k
+ eδT0
k G2
"
REFERENCES,0.875,"= O

Mf
PT
t=0 ηt
k
+ L P"
REFERENCES,0.8761904761904762,"k∈X pkσ2
kα2
PT
t=0 ηt
k
+ exp(−Tpk/2)G2
"
REFERENCES,0.8773809523809524,"= O

Mf
PT
t=0 ηt
k
+ L P
k∈X pkσ2
kα2
PT
t=0 ηt
k 
,"
REFERENCES,0.8785714285714286,"where the last inequality holds whenever T ≥bT0 and bT0 is large enough so that exp(−bT0pk/2)G2 ≤
Mf
PT
t=0 ηt
k ≤
Mf
P b
T0
t=0 ηt
k
, and the second equality follows from the deﬁnition of Mf = f(Θ0) −f ∗+
P"
REFERENCES,0.8797619047619047,"k∈X Lkσ2
k
  1"
REFERENCES,0.8809523809523809,"L
2 = f(Θ0) −f ∗+ P"
REFERENCES,0.8821428571428571,"k∈X pkσ2
k/L. Finally, following similar lines as in the proof"
REFERENCES,0.8833333333333333,"of Theorem 3.1, by choosing α =
q"
REFERENCES,0.8845238095238095,"Mf
L P"
REFERENCES,0.8857142857142857,"l∈X plσ2
l we obtain that for T ≥max
n
T0, bT0
o
,"
REFERENCES,0.8869047619047619,"Eξ ∥∇f τ
k ∥2 = O 
 
LMf T
+"
REFERENCES,0.888095238095238,"√pk
qP
l∈X plσ2
l LMf
√ T 
  = O 
 
LMf T
+"
REFERENCES,0.8892857142857142,"√pk
qP
l∈X plσ2
l L (f(Θ0) −f ∗) +
 P
l∈X plσ2
l
2
√ T 
  = O 
 
LMf T
+"
REFERENCES,0.8904761904761904,"√pk
qP"
REFERENCES,0.8916666666666667,"l∈X plσ2
l L (f(Θ0) −f ∗)
√"
REFERENCES,0.8928571428571429,"T
+
√pk
 P"
REFERENCES,0.8940476190476191,"l∈X plσ2
l
 √ T 
"
REFERENCES,0.8952380952380953,",
∀k ∈X,"
REFERENCES,0.8964285714285715,"where the last inequality uses simple fact
√"
REFERENCES,0.8976190476190476,"a + b ≤√a +
√"
REFERENCES,0.8988095238095238,"b whenever a, b > 0. It remains to
estimate the order of bT0, we need exp(−bT0pk/2)G2 ≤
Mf
P b
T0
t=0 ηt
k
, this can be readily satisﬁed by"
REFERENCES,0.9,"taking bT0 =
2 log G−log(Mf (1/2L+α/√pk)) pk
."
REFERENCES,0.9011904761904762,"Improvement of CF-SGD over SGD. Now under the the assumption that σ2
l = σ2 for all l ∈X,
compared with the rate of convergence of standard SGD in (9), the improvement of Counter-based
Frequency-aware SGD is governed by the ratio γc deﬁned by"
REFERENCES,0.9023809523809524,"γc :=
√pk
qP
l∈X p2
l "
REFERENCES,0.9035714285714286,"1 +
σ
p"
REFERENCES,0.9047619047619048,"L(f(Θ0) −f ∗) ! ,"
REFERENCES,0.905952380952381,"which is only a constant factor away from the the improvement ratio γf of vanilla frequency-aware
SGD over standard SGD, given by"
REFERENCES,0.9071428571428571,"γf :=
√pk
qP"
REFERENCES,0.9083333333333333,"l∈X p2
l
."
REFERENCES,0.9095238095238095,"Hence both Corollary 3.2 and 3.3 hold for Counter-based Frequency-aware SGD after properly
adjusting the constant factor in the statement."
REFERENCES,0.9107142857142857,Published as a conference paper at ICLR 2022
REFERENCES,0.9119047619047619,Proof of Corollary 3.1. By trivial veriﬁcation.
REFERENCES,0.9130952380952381,"Corollary D.1 (Corollary 3.2, restated). Let U = {in}|U|
n=1, V = {jm}|V |
m=1, where in denote the
user with n-th largest frequency, and jm denote the item with the m-th largest frequency. Suppose"
REFERENCES,0.9142857142857143,"pin ∝exp(−τn), pjm ∝exp(−τm),
∀n ∈[|U|], m ∈[|V |].
(33)"
REFERENCES,0.9154761904761904,"for some τ > 0. Then there exists βU, βV > 0 such that for any user in ∈U, we have pin
P"
REFERENCES,0.9166666666666666,"l∈X plσ2
l
P
l∈X p2
l σ2
l
= βU(τ) exp (−τn) ≤1 + exp(−τ)"
REFERENCES,0.9178571428571428,"1 −exp(−τ) exp (−τn) .
(34)"
REFERENCES,0.919047619047619,"Similarly, for any item jm ∈V , we have pjm
P"
REFERENCES,0.9202380952380952,"l∈X plσ2
l
P
l∈X p2
l σ2
l
= βV (τ) exp {−τm} ≤1 + exp(−τ)"
REFERENCES,0.9214285714285714,"1 −exp(−τ) exp {−τm} .
(35)"
REFERENCES,0.9226190476190477,"In addition, there exists an absolute constant C > 0, such that for whenver |U|, |V | ≥1"
REFERENCES,0.9238095238095239,"τ , we have
βU(τ), βV (τ) ≤C, and thus"
REFERENCES,0.925,"pin
P
l∈X plσ2
l
P"
REFERENCES,0.9261904761904762,"l∈X p2
l σ2
l
≤C exp (−τn) ; pjm
P
l∈X plσ2
l
P"
REFERENCES,0.9273809523809524,"l∈X p2
l σ2
l
≤C exp {−τm} , ∀in ∈U, ∀jm ∈V.
(36)"
REFERENCES,0.9285714285714286,"Proof. Deﬁne MU
=
1−exp(−τ)
1−exp(−τ|U|), and MV
=
1−exp(−τ)
1−exp(−τ|V |), from (33) we have pin
=
MU exp(−τn), pjm = MV exp(−τm). In addition, we denote"
REFERENCES,0.9297619047619048,"MU,V =
X"
REFERENCES,0.930952380952381,"l∈X
p2
l =
X"
REFERENCES,0.9321428571428572,"i∈U
p2
i +
X"
REFERENCES,0.9333333333333333,"j∈V
p2
j = M 2
U
1 −exp(−2τ|U|)"
REFERENCES,0.9345238095238095,"1 −exp(−2τ)
+ M 2
V
1 −exp(−2τ|U|)"
REFERENCES,0.9357142857142857,1 −exp(−2τ)
REFERENCES,0.9369047619047619,= (1 + exp(−τ|U|)) (1 −exp(−τ))
REFERENCES,0.9380952380952381,(1 −exp(−τ|U|)) (1 + exp(−τ)) + (1 + exp(−τ|V |)) (1 −exp(−τ))
REFERENCES,0.9392857142857143,(1 −exp(−τ|V |)) (1 + exp(−τ)).
REFERENCES,0.9404761904761905,"Thus we obtain, pin
P"
REFERENCES,0.9416666666666667,"l∈X plσ2
l
P"
REFERENCES,0.9428571428571428,"l∈X p2
l σ2
l
=
2pin
P"
REFERENCES,0.944047619047619,"l∈X p2
l
= 2MU"
REFERENCES,0.9452380952380952,"MU,V
exp(−τn) = βU exp(−τn), ∀in ∈U"
REFERENCES,0.9464285714285714,"pjm
P
l∈X plσ2
l
P"
REFERENCES,0.9476190476190476,"l∈X p2
l σ2
l
=
2pjm
P"
REFERENCES,0.9488095238095238,"l∈X p2
l
= 2MV"
REFERENCES,0.95,"MU,V
exp(−τm) = βV exp(−τm), ∀jm ∈V,"
REFERENCES,0.9511904761904761,"From the deﬁnition of βU =
2MU
MU,V , βV = 2MV"
REFERENCES,0.9523809523809523,"MU,V , combined with the fact that MU,V ≥2 1−exp(−τ)"
REFERENCES,0.9535714285714286,"1+exp(−τ),
and MU, MV ≤1, we obtain the inequality in (34) and (35). Finally, whenever |U|, |V | ≥1"
REFERENCES,0.9547619047619048,"τ , we
have βU ≤
1−exp(−τ)
1−exp(−τ|U|)
1+exp(−τ)
1−exp(−τ) ≤
2
1−e−1 and similarly βV ≤
2
1−e−1 . Let C =
2
1−e−1 , we
obtain (36)."
REFERENCES,0.955952380952381,"Corollary D.2 (Corollary 3.3, restated). Let U = {in}|U|
n=1, V = {jm}|V |
m=1, where in denote the
user with n-th largest frequency, and jm denote the item with the m-th largest frequency. Suppose"
REFERENCES,0.9571428571428572,"pin ∝n−ν, pjm ∝m−ν,
∀n ∈[|U|], m ∈[|V |].
(37)"
REFERENCES,0.9583333333333334,"for some ν ≥2. Deﬁne UT as the set of users whose frequencies are within 2-factor from the highest
frequency: UT = {in : n−ν ≥1/2}, and VT similarly as VT = {jm : m−ν ≥1/2}. We refer to
UT as the top users, and VT as the top items. Then there exists an absolute constant C > 0, such
that pin
P"
REFERENCES,0.9595238095238096,"l∈X plσ2
l
P
l∈X p2
l σ2
l
≤Cn−ν; pjm
P"
REFERENCES,0.9607142857142857,"l∈X plσ2
l
P
l∈X p2
l σ2
l
≤Cm−ν, ∀in ∈U, ∀jm ∈V."
REFERENCES,0.9619047619047619,Published as a conference paper at ICLR 2022
REFERENCES,0.9630952380952381,"Proof. We have pin = MUn−ν, where MU =
1
P|U|
n=1 n−ν . Similarly, we have pjm = MV m−ν,"
REFERENCES,0.9642857142857143,"where MV =
1
P|V |
m=1 mν . Hence pin
P"
REFERENCES,0.9654761904761905,"l∈X plσ2
l
P"
REFERENCES,0.9666666666666667,"l∈X p2
l σ2
l
=
2pin
P"
REFERENCES,0.9678571428571429,"l∈X p2
l
=
2MUn−ν"
REFERENCES,0.969047619047619,"M 2
U
P|U|
en=1 en−2ν + M 2
V
P|V |
e
m=1 em−2ν"
REFERENCES,0.9702380952380952,"We have |U|
X"
REFERENCES,0.9714285714285714,"n=1
n−ν = 1 + |U|
X"
REFERENCES,0.9726190476190476,"2
n−ν ≥1 +
Z |U|+1"
REFERENCES,0.9738095238095238,"2
n−ν = 1 +
1
ν −1

21−ν −(|U| + 1)1−ν
, |U|
X"
REFERENCES,0.975,"n=1
n−ν < 1 +
Z |U|+1"
REFERENCES,0.9761904761904762,"1
n−ν = 1 +
1
ν −1

1 −(|U| + 1)1−ν
, |U|
X"
REFERENCES,0.9773809523809524,"n=1
n−2ν > 1 +
1
2ν −1

21−2ν −(|U| + 1)1−2ν
,"
REFERENCES,0.9785714285714285,which implies
REFERENCES,0.9797619047619047,"MU <
ν −1
21−ν −(|U| + 1)1−ν + ν −1, MU >
ν −1
(|U| + 1)1−ν + ν ,"
REFERENCES,0.9809523809523809,"MV <
ν −1
21−ν −(|V | + 1)1−ν + ν −1, MU >
ν −1
(|V | + 1)1−ν + ν , Thus pin
P"
REFERENCES,0.9821428571428571,"l∈X plσ2
l
P"
REFERENCES,0.9833333333333333,"l∈X p2
l σ2
l"
REFERENCES,0.9845238095238096,"≤
n−ν
2(2ν−1)
(21−ν−(|U|+1)1−ν+ν−1)(ν−1)

1
(|U|+1)1−ν+ν
2
(21−2ν −(|U| + 1)1−2ν + 2ν −1) +

1
(|V |+1)1−ν+ν
2
(21−2ν −(|V | + 1)1−2ν + 2ν −1)
,"
REFERENCES,0.9857142857142858,Note that
REFERENCES,0.986904761904762,"21−2ν −(|U| + 1)1−2ν + 2ν −1 > 2ν −1, 21−2ν −(|V | + 1)1−2ν + 2ν −1 > 2ν −1,
1
(|U| + 1)1−ν + ν >
1
1 + ν ,
1
(|V | + 1)1−ν + ν >
1
1 + ν ,"
REFERENCES,0.9880952380952381,"1
21−ν −(|U| + 1)1−ν + ν −1 <
1
ν −1,
1
21−ν −(|V | + 1)1−ν + ν −1 <
1
ν −1."
REFERENCES,0.9892857142857143,"Then we conclude with pin
P"
REFERENCES,0.9904761904761905,"l∈X plσ2
l
P"
REFERENCES,0.9916666666666667,"l∈X p2
l σ2
l
≤(2ν −1)"
REFERENCES,0.9928571428571429,"(ν −1) ·
(1 + ν)2"
REFERENCES,0.9940476190476191,"(ν −1)(2ν −1)n−ν ≤16n−ν,"
REFERENCES,0.9952380952380953,"where the last inequality follows from the fact that ν ≥2. Similarly we can show that pjm
P"
REFERENCES,0.9964285714285714,"l∈X plσ2
l
P"
REFERENCES,0.9976190476190476,"l∈X p2
l σ2
l
≤16m−ν."
REFERENCES,0.9988095238095238,"Take C = 16, we obtain the desired result."
