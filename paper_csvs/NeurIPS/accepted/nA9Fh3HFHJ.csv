Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002197802197802198,"Memorization of training data is an active research area, yet our understanding of
the inner workings of neural networks is still in its infancy. Recently, Haim et al.
[2022] proposed a scheme to reconstruct training samples from multilayer percep-
tron binary classifiers, effectively demonstrating that a large portion of training
samples are encoded in the parameters of such networks. In this work, we extend
their findings in several directions, including reconstruction from multiclass and
convolutional neural networks. We derive a more general reconstruction scheme
which is applicable to a wider range of loss functions such as regression losses.
Moreover, we study the various factors that contribute to networks’ susceptibility
to such reconstruction schemes. Intriguingly, we observe that using weight decay
during training increases reconstructability both in terms of quantity and quality.
Additionally, we examine the influence of the number of neurons relative to the
number of training samples on the reconstructability.
Code: https://github.com/gonbuzaglo/decoreco"
INTRODUCTION,0.004395604395604396,"1
Introduction"
INTRODUCTION,0.006593406593406593,"Neural networks are known to memorize training data despite their ability to generalize well to unseen
test data [Zhang et al., 2021, Feldman, 2020]. This phenomenon was observed in both supervised
settings [Haim et al., 2022, Balle et al., 2022, Loo et al., 2023] and in generative models [Carlini
et al., 2019, 2021, 2023]. These works shed an interesting light on generalization, memorization and
explainability of neural networks, while also posing a potential privacy risk."
INTRODUCTION,0.008791208791208791,"Current reconstruction schemes from trained neural networks are still very limited and often rely
on unrealistic assumptions, or operate within restricted settings. For instance, Balle et al. [2022]
propose a reconstruction scheme based on the assumption of having complete knowledge of the
training set, except for a single sample. Loo et al. [2023] suggest a scheme which operates under the
NTK regime [Jacot et al., 2018], and assumes knowledge of the full set of parameters at initialization.
Reconstruction schemes for unsupervised settings are specifically tailored for generative models and
are not applicable for classifiers or other supervised tasks."
INTRODUCTION,0.01098901098901099,"Recently, Haim et al. [2022] proposed a reconstruction scheme from feed-forward neural networks
under logistic or exponential loss for binary classification tasks. Their scheme requires only knowl-
edge of the trained parameters, and relies on theoretical results about the implicit bias of neural
networks towards solutions of the maximum margin problem [Lyu and Li, 2019, Ji and Telgarsky,
2020]. Namely, neural networks are biased toward KKT points of the max-margin problem (see
Theorem 3.1). By utilizing the set of conditions that KKT points satisfy, they devise a novel loss func-
tion that allows for reconstruction of actual training samples. They demonstrate reconstruction from
models trained on common image datasets (CIFAR10 [Krizhevsky et al., 2009] and MNIST [LeCun
et al., 2010])."
INTRODUCTION,0.013186813186813187,∗Equal Contribution
INTRODUCTION,0.015384615384615385,"In this work, we expand the scope of neural networks for which we have evidence of successful
sample memorization, by demonstrating sample reconstruction. Our contributions are as follows:"
INTRODUCTION,0.017582417582417582,"• We extend the reconstruction scheme of Haim et al. [2022] to a multiclass setting (Fig. 1).
This extension utilizes the implicit bias result from Lyu and Li [2019] to multiclass training.
We analyse the effects of the number of classes on reconstructability, and show that models
become more susceptible to sample reconstruction as the number of classes increases.
• We devise a reconstruction scheme that applies for general loss functions, assuming that the
model is trained with weight decay. We demonstrate reconstruction from models trained on
regression losses.
• We investigate the effects of weight decay and show that for certain values, weight decay
increases the vulnerability of models to sample reconstruction. Specifically, it allows us to
reconstruct training samples from a convolutional network, while Haim et al. [2022] only
handled MLPs.
• We analyse the intricate relation between the number of samples and the number of pa-
rameters in the trained model, and their effect on reconstrctability. We also demonstrate
successful reconstruction from a model trained on 5,000 samples, surpassing previous results
that focused on models trained on up to 1,000 samples."
RELATED WORKS,0.01978021978021978,"2
Related Works"
RELATED WORKS,0.02197802197802198,"Memorization and Samples Reconstruction.
There is no consensus on the definition of the
term “memorization” and different works study this from different perspectives. In ML theory,
memorization usually refers to label (or, model’s output) memorization [Zhang et al., 2016, Arpit
et al., 2017, Feldman, 2020, Feldman and Zhang, 2020, Brown et al., 2021], namely, fitting the
training set. Memorization in the input domain is harder to show, because in order to demonstrate
its occurrence one has to reconstruct samples from the model. Balle et al. [2022] demonstrated
reconstruction of one training sample, assuming knowledge of all other training samples and Haim
et al. [2022] demonstrated reconstruction of a substantial portion of training samples from a neural
network classifier. Loo et al. [2023] extend their work to networks trained under the NTK regime
[Jacot et al., 2018] and explore the relationship to dataset distillation. Several works have also
studied memorization and samples reconstruction in generative models like autoencoders [Erhan
et al., 2009, Radhakrishnan et al., 2018], large language models [Carlini et al., 2021, 2019, 2022] and
diffusion-based image generators [Carlini et al., 2023, Somepalli et al., 2022, Gandikota et al., 2023,
Kumari et al., 2023]."
RELATED WORKS,0.024175824175824177,"Inverting Classifiers.
Optimizing a model’s input as to minimize a class score is the common
approach for neural network visualization [Mahendran and Vedaldi, 2015]. It usually involves using
input regularization [Mordvintsev et al., 2015, Ghiasi et al., 2022], GAN prior [Nguyen et al., 2016,
2017] or knowledge of batch statistics [Yin et al., 2020]. Fredrikson et al. [2015], Yang et al. [2019]
showed reconstruction of training samples using similar approach, however these methods are limited
to classifiers trained with only a few samples per class. Reconstruction from a federated-learning
setup [Zhu et al., 2019, He et al., 2019, Hitaj et al., 2017, Geiping et al., 2020, Yin et al., 2021, Huang
et al., 2021, Wen et al., 2022] involve attacks that assume knowledge of training samples’ gradients
(see also Wang et al. [2023] for a theoretical analysis). In this work we do not assume any knowledge
on the training data and do not use any priors other than assuming bounded inputs."
PRELIMINARIES,0.026373626373626374,"3
Preliminaries"
PRELIMINARIES,0.02857142857142857,"In this section, we provide an overview of the fundamental concepts and techniques required to
understand the remainder of the paper, focusing on the fundamentals laid by Haim et al. [2022] for
reconstructing training samples from trained neural networks."
PRELIMINARIES,0.03076923076923077,"Theoretical Framework.
Haim et al. [2022] builds on the theory of implicit bias of gradient
descent. Neural networks are commonly trained using gradient methods, and when large enough,
they are expected to fit the training data well. However, it is empirically known that these models
converge to solutions that also generalize well to unseen data, despite the risk of overfitting. Several"
PRELIMINARIES,0.03296703296703297,"works pointed to this “implicit bias"" of gradient methods as a possible explanation. Soudry et al.
[2018] showed that linear classifiers trained with gradient descent on the logistic loss converge to the
same solution as that of a hard-SVM, meaning that they maximize the margins. This result was later
extended to non-linear and homogeneous neural networks by Lyu and Li [2019], Ji and Telgarsky
[2020]:"
PRELIMINARIES,0.035164835164835165,"Theorem 3.1 (Paraphrased from Lyu and Li [2019], Ji and Telgarsky [2020]) Let Φ(θ; ·) be a
homogeneous 2 ReLU neural network. Consider minimizing the logistic loss over a binary clas-
sification dataset {(xi, yi)}n
i=1 using gradient flow. Assume that there exists time t0 where the
network classifies all the samples correctly. Then, gradient flow converges in direction to a first order
stationary point (KKT point) of the following maximum-margin problem:"
PRELIMINARIES,0.03736263736263736,"min
θ
1
2 ∥θ∥2
s.t.
∀i ∈[n] yiΦ(θ; xi) ≥1 .
(1)"
PRELIMINARIES,0.03956043956043956,A KKT point of Eq. (1) is characterized by the following set of conditions:
PRELIMINARIES,0.041758241758241756,"∀j ∈[p], θj − n
X"
PRELIMINARIES,0.04395604395604396,"i=1
λi∇θj [yiΦ(θ; xi)] = 0
(stationarity)
(2)"
PRELIMINARIES,0.046153846153846156,"∀i ∈[n],
yiΦ(θ; xi) ≥1
(primal feasibility)
(3)
∀i ∈[n],
λi ≥0
(dual feasibility)
(4)
∀i ∈[n],
λi = 0 if yiΦ(θ; xi) ̸= 1
(complementary slackness)
(5)"
PRELIMINARIES,0.04835164835164835,"Reconstruction Algorithm.
Haim et al. [2022] demonstrated reconstructing samples from the
training set of such classifiers by devising a reconstruction loss. Given a trained classifier Φ(x; θ),
they initialize a set of {(xi, yi)}m
i=1 and {λi}m
i=1, and optimize xi, λi to minimize the following loss
function:"
PRELIMINARIES,0.05054945054945055,"L = ∥θ − m
X"
PRELIMINARIES,0.05274725274725275,"i=1
λi∇θj [yiΦ(θ; xi)] ∥"
PRELIMINARIES,0.054945054945054944,"|
{z
}
Lstationary + m
X"
PRELIMINARIES,0.05714285714285714,"i=1
max {−λ, −λmin}"
PRELIMINARIES,0.05934065934065934,"|
{z
}
Lλ"
PRELIMINARIES,0.06153846153846154,"+Lprior
(6)"
PRELIMINARIES,0.06373626373626373,"Where Lprior is simply bounding each pixel value at [−1, 1] 3. The number of training samples n is
unknown. However, setting m > 2n, where {yi} are set in a balanced manner allows reconstructing
samples with any label distribution. The xi’s are initialized from the Gaussian distribution N(0, σ2
xI),
and λmin, σx are hyperparameters. We note that the homogeneity condition from Theorem 3.1 is not
necessarily a practical limitation of this reconstruction scheme, as already in Haim et al. [2022] they
show reconstructions from a non-homogeneous network."
PRELIMINARIES,0.06593406593406594,"Analysing and Summarizing Results.
The optimization in Eq. (6) is executed k times (for different
hyperparameters) and results in km outputs ({ˆxi}km
i=1) that we term candidates, as they are candidates
to be reconstructed training samples. To quantify the success of the reconstruction process, each
training sample is matched with its nearest-neighbour from the km candidates. The “quality” of
reconstruction is then measured using SSIM Wang et al. [2004] (see full details in Appendix A.1)."
PRELIMINARIES,0.06813186813186813,"An important corollary of the set of KKT conditions Eqs. (2) to (5) is that the parameters of the
trained model only depend on gradients of samples that are closest to the decision boundary, the
so-called ""margin-samples"" (see end of section 3.2 in Haim et al. [2022]). Therefore, a good visual
summary for analysing reconstruction from a trained model is by plotting the reconstruction quality
(SSIM) against the distance from the decision boundary (|Φ(xi)|). We also utilize such visualizations."
PRELIMINARIES,0.07032967032967033,"Assessing the Quality of Reconstructed Samples.
Determining whether a candidate is a correct
match for some training sample is as hard as finding a good image similarity metric. No synthetic
metric such as SSIM or L2 norm can perfectly align with human perception. Perceptual similarity"
PRELIMINARIES,0.07252747252747253,"2A classifier Φ is homogeneous w.r.t to θ if there exists L ∈R s.t. ∀c ∈R, x : ϕ(x; cθ) = cLϕ(x; θ)
3Formally: Lprior = Pm
i=1
Pd
k=1 max{max{xi,k −1, 0}, max{−xi,k −1, 0}}."
PRELIMINARIES,0.07472527472527472,"metrics (e.g., LPIPS [Zhang et al., 2018]) build on top of pre-trained classifiers trained on Ima-
genet [Deng et al., 2009], and are not effective for the image resolution in this work (up to 32x32
pixels). We have observed heuristically that candidates with SSIM score higher than about 0.4 are
indeed visually similar to their nearest neighbor training sample. Hence, in this work we say that a
certain candidate is a ""good reconstruction"" if its SSIM score with its nearest neighbor is at least 0.4.
Also see discussion in Appendix A.2."
RECONSTRUCTING DATA FROM MULTI-CLASS CLASSIFIERS,0.07692307692307693,"4
Reconstructing Data from Multi-Class Classifiers"
RECONSTRUCTING DATA FROM MULTI-CLASS CLASSIFIERS,0.07912087912087912,We demonstrate that training set reconstruction can be extended to multi-class classification tasks.
RECONSTRUCTING DATA FROM MULTI-CLASS CLASSIFIERS,0.08131868131868132,"Plane
Car
Bird
Cat
Deer
Dog
Frog
Horse
Ship
Truck"
RECONSTRUCTING DATA FROM MULTI-CLASS CLASSIFIERS,0.08351648351648351,"Figure 1: Reconstructed training samples from a multi-class MLP classifier that was trained on 500
CIFAR10 images. Each column corresponds to one class and shows the 10 training samples (red)
that were best reconstructed from this class, along with their reconstructed result (blue)."
THEORY,0.08571428571428572,"4.1
Theory"
THEORY,0.08791208791208792,"The extension of the implicit bias of homogeneous neural networks to the multi-class settings is
discussed in Lyu and Li [2019] (Appendix G): Let S = {(xi, yi)}n
i=1 ⊆Rd × [C] be a multi-class
classification training set where C ∈N is any number of classes, and [C] = {1, . . . , C}. Let
Φ(θ; ·) : Rd →RC be a homogeneous neural network parameterized by θ ∈Rp. We denote the j-th
output of Φ on an input x as Φj(θ; x) ∈R. Consider minimizing the standard cross-entropy loss and
assume that after some number of iterations the model correctly classifies all the training examples.
Then, gradient flow will converge to a KKT point of the following maximum-margin problem:"
THEORY,0.09010989010989011,"min
θ
1
2 ∥θ∥2
s.t.
Φyi(θ; xi) −Φj(θ; xi) ≥1 ∀i ∈[n], ∀j ∈[C] \ {yi}
.
(7)"
THEORY,0.09230769230769231,"A KKT point of the above optimization problem is characterized by the following set of conditions: θ − n
X i=1 c
X"
THEORY,0.0945054945054945,"j̸=yi
λi,j∇θ(Φyi(θ; xi) −Φj(θ; xi)) = 0
(8)"
THEORY,0.0967032967032967,"∀i ∈[n], ∀j ∈[C] \ {yi} :
Φyi(θ; xi) −Φj(θ; xi) ≥1
(9)
∀i ∈[n], ∀j ∈[C] \ {yi} :
λi,j ≥0
(10)
∀i ∈[n], ∀j ∈[C] \ {yi} :
λi,j = 0 if Φyi(θ; xi) −Φj(θ; xi) ̸= 1
(11)"
THEORY,0.0989010989010989,"A straightforward extension of a reconstruction loss for a multi-class model that converged to the
conditions above would be to minimize the norm of the left-hand-side (LHS) of condition Eq. (8)"
THEORY,0.1010989010989011,"(namely, optimize over {xi}m
i=1 and {λi,j}i∈[n],j∈[C]\yi where m is a hyperparameter). However,
this straightforward extension failed to successfully reconstruct samples. We therefore propose the
following equivalent formulation."
THEORY,0.10329670329670329,"Note that from Eqs. (9) and (11), most λi,j zero out: the distance of a sample xi to its nearest decision
boundary, Φyi −maxj̸=yi Φj, is usually achieved for a single class j and therefore (from Eq. (11))
in this case at most one λi,j will be non-zero. For some samples xi it is also possible that all λi,j will
vanish. Following this observation, we define the following loss that only considers the distance from
the decision boundary:"
THEORY,0.1054945054945055,"Lmulticlass(x1, ..., xm, λ1, ..., λm) = θ − m
X"
THEORY,0.1076923076923077,"i=1
λi ∇θ[Φyi(xi; θ) −max
j̸=yi Φj(xi; θ)]  2"
THEORY,0.10989010989010989,"2
(12)"
THEORY,0.11208791208791209,"Eq. (12) implicitly includes Eq. (11) into the summation in Eq. (8), thereby significantly reducing the
number of summands and simplifying the overall optimization problem."
THEORY,0.11428571428571428,"While the straightforward extension failed to successfully reconstruct samples, solving Eq. (12)
enabled reconstruction from multiclass models (see Fig. 1 and results below). We attribute this
success to the large reduction in the number of optimization variables, which simplifies the overall
optimization problem (n variables in Eq. (12) compared to C · n variables in the straightforward case,
which is significant for large number of classes C)."
THEORY,0.11648351648351649,"We also use the same Lλ and Lprior as in Eq. (6), and set {yi} in a balanced manner (uniformly on all
classes). While setting m = C · n allows reconstructing any label distribution, in our experiments we
focus on models trained on balanced training sets, and use m = 2n which works sufficiently well.
An intuitive way to understand the extension of the binary reconstruction loss Eq. (6) to multi-class
reconstruction Eq. (12) is that the only difference is the definition of the distance to nearest boundary,
which is the term inside the square brackets in both equations."
RESULTS,0.11868131868131868,"4.2
Results"
RESULTS,0.12087912087912088,"We compare between reconstruction from binary classifiers, as studied in Haim et al. [2022], and
reconstruction from multi-class classifiers by using the novel loss function Eq. (12). We conduct the
following experiment: we train an MLP classifier with architecture D-1000-1000-C on samples from
the CIFAR10 [Krizhevsky et al., 2009] dataset. The model is trained to minimize the cross-entropy
loss with full-batch gradient descent, once with two classes (250 samples per class) and once for the
full 10 classes (50 samples per class). Both models train on the same amount of samples (500). The
test set accuracy of the models is 77%/32% respectively, which is far from random (50%/10% resp.).
See implementation details in Appendix B."
RESULTS,0.12307692307692308,"To quantify the quality of our reconstructed samples, for each sample in the original training set
we search for its nearest neighbour in the reconstructed images and measure the similarity using
SSIM [Wang et al., 2004] (higher SSIM means better reconstruction). In Fig. 2 we plot the quality
of reconstruction (in terms of SSIM) against the distance of the sample from the decision boundary
Φyi(xi; θ) −maxj̸=yi Φj(xi; θ). As seen, a multi-class classifier yields much more samples that are
vulnerable to being reconstructed."
RESULTS,0.12527472527472527,"10
15
20
25
30
35
40
(x) 0.0 0.2 0.4 0.6"
RESULTS,0.12747252747252746,"SSIM(x, x)"
RESULTS,0.12967032967032968,Multiclass
RESULTS,0.13186813186813187,"10
15
20
25
30
35
40
(x)"
RESULTS,0.13406593406593406,"SSIM(x, x)"
RESULTS,0.13626373626373625,Binary
RESULTS,0.13846153846153847,"Figure 2: Multi-class classifiers are more vulnerable to training-set reconstruction. For a training set of
size 500, a multi-class model (left) yields 101 reconstructed samples with good quality (SSIM>0.4),
compared to 40 in a binary classification model (right)."
RESULTS,0.14065934065934066,"We examine the relation between the ability to reconstruct from a model and the number of classes on
which it was trained. Comparing between two models trained on different number of classes is not
immediately clear, since we want to isolate the effect of the number of classes from the size of the
dataset (it was observed by Haim et al. [2022] that the number of reconstructed samples decreases as
the total size of the training set increases). We therefore train models on training sets with varying
number of classes (C ∈{2, 3, 4, 5, 10}) and varying number of samples per class (1, 5, 10, 50). The
results are visualized in Fig. 3a. As seen, for models with same number of samples per class, the
ability to reconstruct increases with the number of classes, even though the total size of the training set
is larger. This further validates our hypothesis that the more classes, the more samples are vulnerable
to reconstruction (also see Appendix C)."
RESULTS,0.14285714285714285,"Another way to validate this hypothesis is by showing the dependency between the number of
classes and the number of “good” reconstructions (SSIM>0.4) – shown in Fig. 3b. As can be seen,
training on multiple classes yields more samples that are vulnerable to reconstruction. An intuitive
explanation, is that multi-class classifiers have more “margin-samples"". Since margin-samples are
more vulnerable to reconstruction, this results in more samples being reconstructed from the model. 0.00 0.25 0.50"
RESULTS,0.14505494505494507,"0.75
50 Samples"
RESULTS,0.14725274725274726,per Class
CLASSES,0.14945054945054945,"2 Classes
3 Classes
4 Classes
5 Classes
10 Classes 0.00 0.25 0.50"
CLASSES,0.15164835164835164,"0.75
10 Samples"
CLASSES,0.15384615384615385,per Class 0.00 0.25 0.50
CLASSES,0.15604395604395604,"0.75
5 Samples"
CLASSES,0.15824175824175823,per Class
CLASSES,0.16043956043956045,"10
15
20
25
0.00 0.25 0.50"
CLASSES,0.16263736263736264,"0.75
1 Sample
per Class"
CLASSES,0.16483516483516483,"10
15
20
25
10
15
20
25
10
15
20
25
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Distance from Decision Boundary [
yi(x)
max
j
yi
j(x)] 0.0 0.2 0.4 0.6 0.8 1.0"
CLASSES,0.16703296703296702,"Reconstruction Quality [SSIM(x, x)]"
CLASSES,0.16923076923076924,"(a) Analysing the relation between number of classes and number of samples per class
in reconstruction from multiclass Classifiers."
CLASSES,0.17142857142857143,"2
3
4
5
6
7
8
9
10
# Classes"
CLASSES,0.17362637362637362,"0
10
20
30
40
50
60
70"
CLASSES,0.17582417582417584,# Good Rec.'s
CLASSES,0.17802197802197803,(Total)
CLASSES,0.18021978021978022,"2
3
4
5
6
7
8
9
10
# Classes"
CLASSES,0.1824175824175824,"0
1
2
3
4
5
6
7
8
9
10"
CLASSES,0.18461538461538463,# Good Rec.'s
CLASSES,0.18681318681318682,per Class
CLASSES,0.189010989010989,"Samples per Class:
1
5
10
50"
CLASSES,0.1912087912087912,"(b) Number of ""good"" reconstructions increases with number of classes and the samples per class"
CLASSES,0.1934065934065934,"Figure 3: Evaluating the effect of multiple classes on the ability to reconstruct. We show reconstruc-
tions from models trained with different numbers of classes and different numbers of samples per
class. As seen, multiple classes result in more reconstructed samples."
DATA RECONSTRUCTION WITH GENERAL LOSS FUNCTIONS,0.1956043956043956,"5
Data Reconstruction with General Loss Functions"
DATA RECONSTRUCTION WITH GENERAL LOSS FUNCTIONS,0.1978021978021978,"We demonstrate that data reconstruction can be generalized to a larger family of loss functions. Haim
et al. [2022] and Section 4 only considered a reconstruction scheme based on the implicit bias of
gradient methods trained with cross-entropy loss. For other loss functions, such as the square loss, a
precise characterization of the implicit bias in nonlinear networks does not exist [Vardi and Shamir,
2021]. Hence, we establish a reconstruction scheme for networks trained with explicit regularization,
i.e., with weight decay. We show that as long as the training involves a weight-decay term, we can
derive a reconstruction objective that is very similar to the previous objectives in Eqs. (6) and (12)."
THEORY,0.2,"5.1
Theory"
THEORY,0.2021978021978022,"Let ℓ(Φ(xi; θ), yi) be a loss function that gets as input the predicted output of the model Φ
(parametrized by θ) on an input sample xi, and its corresponding label yi. The total regularized loss: L = n
X"
THEORY,0.2043956043956044,"i=1
ℓ(Φ(xi; θ), yi) + λWD
1
2∥θ∥2
.
(13)"
THEORY,0.20659340659340658,"Assuming convergence (∇θL = 0), the parameters should satisfy the following : θ = n
X"
THEORY,0.2087912087912088,"i=1
ℓ′
i ∇θΦ(xi; θ)
(14)"
THEORY,0.210989010989011,"where ℓ′
i = −
1
λW D
∂ℓ(Φ(xi;θ),yi)"
THEORY,0.21318681318681318,"∂Φ(xi;θ)
. This form (which is similar to the condition in Eq. (2)), allows us
to define a generalized reconstruction loss for models trained with a weight-decay term:"
THEORY,0.2153846153846154,"Lrec(x1, ..., xm, λ1, ..., λm) = ∥θ − n
X"
THEORY,0.2175824175824176,"i=1
λi∇θΦ(xi; θ)∥2
2
(15)"
THEORY,0.21978021978021978,"As before, we also include the same Lprior as in Section 3. It is straightforward to see that Lrec is a
generalization of the reconstruction loss in Eq. (6) (yi could be incorporated into the λi term)."
RESULTS AND ANALYSIS,0.22197802197802197,"5.2
Results and Analysis"
RESULTS AND ANALYSIS,0.22417582417582418,"0.00
0.05
0.10
0.15
0.20"
E,0.22637362637362637,"1e
5 0.00 0.25 0.50 0.75 1.00"
SAMPLES,0.22857142857142856,"50
Samples L2"
SAMPLES,0.23076923076923078,"0.0
0.5
1.0
1.5
2.0
1e
5 L2.5"
SAMPLES,0.23296703296703297,"0.0
0.2
0.4"
E,0.23516483516483516,"1e
5 Huber"
E,0.23736263736263735,"0
50
100
150"
E,0.23956043956043957,"1e
5"
E,0.24175824175824176,BCE (w/ WD)
E,0.24395604395604395,"0.0
0.2
0.4
0.6"
E,0.24615384615384617,"1e
5"
E,0.24835164835164836,BCE (No WD)
E,0.25054945054945055,"0.0
0.5
1.0
1.5"
E,0.25274725274725274,"1e
5 0.00 0.25 0.50 0.75 1.00"
SAMPLES,0.2549450549450549,"100
Samples"
SAMPLES,0.2571428571428571,"0
5
10"
E,0.25934065934065936,"1e
5"
E,0.26153846153846155,"0
1
2
3
1e
5"
E,0.26373626373626374,"0
100
200
300
400
1e
5"
E,0.26593406593406593,"0.0
0.5
1.0
1.5
1e
5"
E,0.2681318681318681,"0
2
4
6
1e
5 0.00 0.25 0.50 0.75 1.00"
SAMPLES,0.2703296703296703,"300
Samples"
SAMPLES,0.2725274725274725,"0
10
20
30"
E,0.27472527472527475,"1e
5"
E,0.27692307692307694,"0
5
10"
E,0.27912087912087913,"1e
5"
E,0.2813186813186813,"0
250
500
750"
E,0.2835164835164835,"1e
5"
E,0.2857142857142857,"0
2
4
6
1e
5"
E,0.2879120879120879,"0
5
10"
E,0.29010989010989013,"1e
5 0.00 0.25 0.50 0.75 1.00"
SAMPLES,0.2923076923076923,"500
Samples"
SAMPLES,0.2945054945054945,"0
20
40"
E,0.2967032967032967,"1e
5"
E,0.2989010989010989,"0
10
20"
E,0.3010989010989011,"1e
5"
E,0.3032967032967033,"0
500
1000"
E,0.3054945054945055,"1e
5"
E,0.3076923076923077,"0
2
4
6
1e
5
Loss value ( )"
E,0.3098901098901099,"Reconstruction Quality (SSIM(x, x))"
E,0.3120879120879121,"Figure 4: Reconstruction from general losses (column) for various training set sizes (row), us-
ing Eq. (15). “Harder” samples (with higher loss) are easier to reconstruct."
E,0.3142857142857143,"We validate the above theoretical analysis by demonstrating reconstruction from models trained on
other losses than the ones shown in Section 4 and Haim et al. [2022]. We use the same dataset as
in the classification tasks – images from CIFAR10 dataset with binary labels of {−1, 1}. The only
difference is replacing the classification binary cross-entropy loss with regression losses (e.g., MSE)."
E,0.31648351648351647,"In classification tasks, we analyzed the results by plotting the reconstruction quality (SSIM) against
the sample’s distance from the decision boundary (see Section 3). This showed that reconstruction
is only feasible for margin-samples. However, in regression tasks, margin and decision boundary
lack specific meaning. We propose an alternative analysis approach – note that smaller distance from
the margin results in higher loss for binary cross-entropy. Intuitively, margin-samples are the most
challenging to classify (as reflected by the loss function). Therefore, for regression tasks, we analyze
the results by plotting the reconstruction quality against the loss (per training sample)."
E,0.31868131868131866,"In Fig. 4 we show results for reconstructions from models trained with MSE, L2.5 loss (ℓ=
|Φ(x; θ) −y|p for p=2,2.5 respectively) and Huber loss [Huber, 1992]. The reconstruction scheme
in Eq. (15) is the same for all cases, and is invariant to the loss used during training. Fig. 4 highlights
two important observations: first, the reconstruction scheme in Eq. (15) succeeds in reconstructing
large portions of the training set from models trained with regression losses, as noted from the high"
E,0.3208791208791209,"quality (SSIM) of the samples. Second, by plotting quality against the loss, one sees that “challenging”
samples (with high loss) are easier to reconstruct. Also note that the analysis works for classification
losses, namely BCE with or without weight-decay in Fig. 4). For more results see Appendix D."
ON THE DIFFERENT FACTORS THAT AFFECT RECONSTRUCTABILITY,0.3230769230769231,"6
On the Different Factors that Affect Reconstructability"
ON THE DIFFERENT FACTORS THAT AFFECT RECONSTRUCTABILITY,0.3252747252747253,"Our goal is to gain a deeper understanding of the factors behind models’ vulnerability to reconstruction
schemes. In this section, we present several analyses that shed light on several important factors."
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.3274725274725275,"6.1
The Role of Weight Decay in Data Reconstruction"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.32967032967032966,"Haim et al. [2022] assumed MLP models whose first fully-connected layer was initialized with small
(non-standard) weights. Models with standard initialization (e.g., He et al. [2015], Glorot and Bengio
[2010]) did not yield reconstructed samples. In contrast, the MLPs reconstructed in Haim et al. [2022]
were initialized with an extremely small variance in the first layer. Set to better understand this
drawback, we observed that incorporating weight-decay during training, not only enabled samples
reconstruction in models with standard initialization, but often increase the reconstructability of
training samples."
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.33186813186813185,"10
5
10
4
10
3
10
2
10
1
Weight-Decay Value (
WD) 0 20 40 60"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.33406593406593404,# Good Reconstructions
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.3362637362637363,(out of total 100)
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.3384615384615385,"WD = 0, Small initialization
       (Haim et al. 2022)"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.34065934065934067,"WD = 0, Standard initialization"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.34285714285714286,"MLP (2 Classes, 50 Samples per Class)"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.34505494505494505,"10
7
10
6
10
5
10
4
10
3
Weight-Decay Value (
WD) 0 50 100"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.34725274725274724,# Good Reconstructions
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.34945054945054943,(out of total 500)
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.3516483516483517,"WD = 0, Small initialization
       (Haim et al. 2022)"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.35384615384615387,"WD = 0, Standard initialization"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.35604395604395606,"MLP (10 Classes, 50 Samples per Class)"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.35824175824175825,"10
4
10
3
10
2
10
1
Weight-Decay Value (
WD) 0 25 50 75 100"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.36043956043956044,# Good Reconstructions
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.3626373626373626,(out of total 500)
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.3648351648351648,"WD = 0, Small initialization"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.367032967032967,"CNN (2 Classes, 250 Samples per Class)"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.36923076923076925,"(a)
(b)
(c)"
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.37142857142857144,Figure 5: Using weight-decay during training increases vulnerability to sample reconstruction
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.37362637362637363,"In Fig. 5a-b we show the number of good reconstructions for different choices of the value of
the weight decay (λWD), for MLP classifiers trained on C=2,10 classes and 50 samples per class
(Fig. 5 a, b resp.). We add two baselines trained without weight-decay: model trained with standard
initialization (black) and model with small-initialized first-layer (red). See how for some values of
weight-decay, the reconstructability is significantly higher than what was observed for models with
non-standard initialization. By examining the training samples’ distance to the boundary, one observes
that using weight-decay results in more margin-samples which are empirically more vulnerable to
reconstruction (see full details in Appendix E)."
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.3758241758241758,"We now give an intuitive theoretical explanation to the role of weight decay in data reconstruction.
Theorem 3.1 is used to devise the reconstruction loss in Eq. (6), which is based on the directional
convergence to a KKT point of the max-margin problem. However, this directional convergence
occurs asymptotically as the time t →∞, and the rate of convergence in practice might be extremely
slow. Hence, even when training for, e.g., 106 iterations, gradient descent might reach a solution which
is still too far from the KKT point, and therefore reconstruction might fail. Thus, even when training
until the gradient of the empirical loss is extremely small, the direction of the network’s parameters
might be far from the direction of a KKT point. In Moroshko et al. [2020], the authors proved
that in diagonal linear networks (i.e., a certain simplified architecture of deep linear networks) the
initialization scale controls the rate of convergence to the KKT point, namely, when the initialization
is small gradient flow converges much faster to a KKT point. A similar phenomenon seems to occur
also in our empirical results: when training without weight decay, small initialization seems to be
required to allow reconstruction. However, when training with weight decay, our theoretical analysis
in Section 5.1 explains why small initialization is no longer required. Here, the reconstruction does
not rely on converging to a KKT point of the max-magin problem, but relies on Eq. (14) which holds
(approximately) whenever we reach a sufficiently small gradient of the training objective. Thus,
when training with weight decay and reaching a small gradient Eq. (14) holds, which allows for
reconstruction, contrary to training without weight decay where reaching a small gradient does not
imply converging close to a KKT point."
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.378021978021978,Figure 6: Reconstruction from CNN. Training samples (red) and their best reconstructions (blue)
THE ROLE OF WEIGHT DECAY IN DATA RECONSTRUCTION,0.3802197802197802,"Reconstruction from Convolutional Neural Networks (CNNs).
CNNs adhere to the assumptions
of Theorem 3.1, yet Haim et al. [2022] failed to apply their reconstruction scheme Eq. (6) to CNNs.
We observe that incorporating weight-decay during training (using standard initialization) enables
samples reconstruction. In Fig. 6 we show an example for reconstruction from a binary classifier
whose first layer is a Conv-layer with kernel size 3 and 32 output channels, followed by two fully
connected layers (CONV(k=3,Cout=32)-1000-1). The weight-decay term is λWD=0.001 (the training
setup is similar to that of MLP). In Fig. 5c we show the reconstructability for the same CNN model
trained with other values of λWD. Note how the weight-decay term plays similar role in the CNN as
in the MLP case. See full details in Appendix F."
THE EFFECT OF THE NUMBER OF PARAMETERS AND SAMPLES ON RECONSTRUCTABILITY,0.3824175824175824,"6.2
The Effect of the Number of Parameters and Samples on Reconstructability"
THE EFFECT OF THE NUMBER OF PARAMETERS AND SAMPLES ON RECONSTRUCTABILITY,0.38461538461538464,"Haim et al. [2022] observed that models trained on fewer samples are more susceptible to reconstruc-
tion in terms of both quantity and quality. In this section, we delve deeper into this phenomenon,
focusing on the intricate relationship between the number of parameters in the trained model and the
number of training samples. We conduct the following experiment:"
THE EFFECT OF THE NUMBER OF PARAMETERS AND SAMPLES ON RECONSTRUCTABILITY,0.3868131868131868,"We train 3-layer MLPs with architecture D-W-W-1 on N training samples from binary CIFAR10
(animals vs. vehicles), where W ∈{5, 10, 50, 100, 500, 1000} and N ∈{10, 50, 100, 300, 500}.
We conduct the experiment for both classification and regression losses, with BCE and MSE loss
respectively. Generalization error is 23%-31% for BCE (classification) and 0.69-0.88 for MSE
(regression), compared to 50%/0.97 for similar models with random weights."
THE EFFECT OF THE NUMBER OF PARAMETERS AND SAMPLES ON RECONSTRUCTABILITY,0.389010989010989,"We reconstruct each model using Eq. (15) and record the number of good reconstructions. The
results are shown in Fig. 7. Note that as W/N increases, our reconstruction scheme is capable of
reconstructing more samples, and vice versa. For example, consider the case when N=10. To
successfully reconstruct the entire training set, it is sufficient for W to be greater than 50/10 (for
MSE/BCE). However, when N=500, even larger models (with larger W) can only reconstruct up to
8% of the samples."
THE EFFECT OF THE NUMBER OF PARAMETERS AND SAMPLES ON RECONSTRUCTABILITY,0.3912087912087912,"Lastly, we reconstruct from a model with W=10,000, trained on N=5,000 samples (5 times larger
than any previous model). While there is some degradation in the quality of the reconstructions
compared to models trained on fewer samples, it is evident that our scheme can still reconstruct some
of the training samples. For full results see Appendix G."
CONCLUSIONS,0.3934065934065934,"7
Conclusions"
CONCLUSIONS,0.3956043956043956,"We present improved reconstruction methods and conduct a comprehensive analysis of the recon-
struction method proposed by Haim et al. [2022]. Particularly, we extend their reconstruction scheme
to a multi-class setting and devise a novel reconstruction scheme for general loss functions, allowing
reconstruction in a regression setting (e.g., MSE loss). We examine various factors influencing
reconstructability. We shed light on the role of weight decay in samples memorization, allowing
for sample reconstruction from convolutional neural networks. Lastly, we examine the intricate
relationship between the number of parameters, the number of samples, and the vulnerability of the
model to reconstruction schemes. We acknowledge that our reconstruction method raises concerns
regarding privacy. We consider it crucial to present such methodologies as they encourage researchers
to study the potential hazards associated with training neural networks. Additionally, it allows for the
development of protective measures aimed at preventing the leakage of sensitive information."
CONCLUSIONS,0.3978021978021978,"10
50
100
300
500
N - #Samples"
CONCLUSIONS,0.4,"5
10
50
100
500
1000
W - #Neurons per layer"
CONCLUSIONS,0.4021978021978022,"4
(40.0%)"
CONCLUSIONS,0.4043956043956044,"3
(6.0%)"
CONCLUSIONS,0.4065934065934066,"0
(0.0%)"
CONCLUSIONS,0.4087912087912088,"0
(0.0%)"
CONCLUSIONS,0.41098901098901097,"0
(0.0%)"
CONCLUSIONS,0.41318681318681316,"7
(70.0%)"
CONCLUSIONS,0.4153846153846154,"10
(20.0%)"
CONCLUSIONS,0.4175824175824176,"2
(2.0%)"
CONCLUSIONS,0.4197802197802198,"0
(0.0%)"
CONCLUSIONS,0.421978021978022,"0
(0.0%)"
CONCLUSIONS,0.42417582417582417,"10
(100.0%)"
CONCLUSIONS,0.42637362637362636,"25
(50.0%)"
CONCLUSIONS,0.42857142857142855,"29
(29.0%)"
CONCLUSIONS,0.4307692307692308,"5
(1.7%)"
CONCLUSIONS,0.432967032967033,"2
(0.4%)"
CONCLUSIONS,0.4351648351648352,"10
(100.0%)"
CONCLUSIONS,0.43736263736263736,"31
(62.0%)"
CONCLUSIONS,0.43956043956043955,"33
(33.0%)"
CONCLUSIONS,0.44175824175824174,"20
(6.7%)"
CONCLUSIONS,0.44395604395604393,"4
(0.8%)"
CONCLUSIONS,0.4461538461538462,"10
(100.0%)"
CONCLUSIONS,0.44835164835164837,"31
(62.0%)"
CONCLUSIONS,0.45054945054945056,"49
(49.0%)"
CONCLUSIONS,0.45274725274725275,"31
(10.3%)"
CONCLUSIONS,0.45494505494505494,"15
(3.0%)"
CONCLUSIONS,0.45714285714285713,"10
(100.0%)"
CONCLUSIONS,0.4593406593406593,"34
(68.0%)"
CONCLUSIONS,0.46153846153846156,"48
(48.0%)"
CONCLUSIONS,0.46373626373626375,"35
(11.7%)"
CONCLUSIONS,0.46593406593406594,"17
(3.4%)"
CONCLUSIONS,0.46813186813186813,Good Reconstructions (MSE Loss) 0 20 40 60 80 100
CONCLUSIONS,0.4703296703296703,"10
50
100
300
500
N - #Samples"
CONCLUSIONS,0.4725274725274725,"5
10
50
100
500
1000
W - #Neurons per layer"
CONCLUSIONS,0.4747252747252747,"4
(40.0%)"
CONCLUSIONS,0.47692307692307695,"4
(8.0%)"
CONCLUSIONS,0.47912087912087914,"3
(3.0%)"
CONCLUSIONS,0.48131868131868133,"1
(0.3%)"
CONCLUSIONS,0.4835164835164835,"0
(0.0%)"
CONCLUSIONS,0.4857142857142857,"10
(100.0%)"
CONCLUSIONS,0.4879120879120879,"9
(18.0%)"
CONCLUSIONS,0.4901098901098901,"5
(5.0%)"
CONCLUSIONS,0.49230769230769234,"1
(0.3%)"
CONCLUSIONS,0.4945054945054945,"0
(0.0%)"
CONCLUSIONS,0.4967032967032967,"10
(100.0%)"
CONCLUSIONS,0.4989010989010989,"22
(44.0%)"
CONCLUSIONS,0.5010989010989011,"23
(23.0%)"
CONCLUSIONS,0.5032967032967033,"9
(3.0%)"
CONCLUSIONS,0.5054945054945055,"12
(2.4%)"
CONCLUSIONS,0.5076923076923077,"10
(100.0%)"
CONCLUSIONS,0.5098901098901099,"23
(46.0%)"
CONCLUSIONS,0.512087912087912,"28
(28.0%)"
CONCLUSIONS,0.5142857142857142,"27
(9.0%)"
CONCLUSIONS,0.5164835164835165,"21
(4.2%)"
CONCLUSIONS,0.5186813186813187,"10
(100.0%)"
CONCLUSIONS,0.5208791208791209,"27
(54.0%)"
CONCLUSIONS,0.5230769230769231,"38
(38.0%)"
CONCLUSIONS,0.5252747252747253,"43
(14.3%)"
CONCLUSIONS,0.5274725274725275,"36
(7.2%)"
CONCLUSIONS,0.5296703296703297,"10
(100.0%)"
CONCLUSIONS,0.5318681318681319,"26
(52.0%)"
CONCLUSIONS,0.5340659340659341,"40
(40.0%)"
CONCLUSIONS,0.5362637362637362,"54
(18.0%)"
CONCLUSIONS,0.5384615384615384,"49
(9.8%)"
CONCLUSIONS,0.5406593406593406,Good Reconstructions (BCE Loss) 0 20 40 60 80 100
CONCLUSIONS,0.5428571428571428,"Figure 7: Effect of the number of neurons and number of training samples on reconstructability.
We train 3-layer MLPs with architecture D-W-W-1 on N training samples from binary CIFAR10
(animals vs. vehicles), using MSE (left) or BCE (right) loss. At each cell we report the number of
good reconstructions (SSIM>0.4), in both absolute numbers and as a percentage relative to N."
CONCLUSIONS,0.545054945054945,"Limitations.
While we have relaxed several of the previous assumptions presented in Haim et al.
[2022], our method still exhibits certain limitations. First, we only consider relatively small-scale
models: up to several layers, without residual connections, that have trained for many iterations on a
relatively small dataset without augmentations. Second, determining the optimal hyperparameters for
the reconstruction scheme poses a challenge as it requires exploring many configurations for even a
single model."
CONCLUSIONS,0.5472527472527473,"Future work.
All of the above extend our knowledge and understanding of how memorization
works in certain neural networks. This opens up several possibilities for future research including
extending our reconstruction scheme to practical models (e.g., ResNets), exploring reconstruction
from models trained on larger datasets or different data types (e.g., text, time-series, tabular data),
analyzing the impact of optimization methods and architectural choices on reconstructability, and
developing privacy schemes to protect vulnerable samples from reconstruction attacks."
CONCLUSIONS,0.5494505494505495,"Acknowledgements.
This project received funding from the European Research Council (ERC)
under the European Union’s Horizon 2020 research and innovation programme (grant agreement
No 788535), and ERC grant 754705, and from the D. Dan and Betty Kahn Foundation. GV
acknowledges the support of the NSF and the Simons Foundation for the Collaboration on the
Theoretical Foundations of Deep Learning."
REFERENCES,0.5516483516483517,References
REFERENCES,0.5538461538461539,"D. Arpit, S. Jastrz˛ebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer,
A. Courville, Y. Bengio, et al. A closer look at memorization in deep networks. In International
conference on machine learning, pages 233–242. PMLR, 2017."
REFERENCES,0.5560439560439561,"B. Balle, G. Cherubin, and J. Hayes. Reconstructing training data with informed adversaries. arXiv
preprint arXiv:2201.04845, 2022."
REFERENCES,0.5582417582417583,"L. Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/.
Software available from wandb.com."
REFERENCES,0.5604395604395604,"G. Brown, M. Bun, V. Feldman, A. Smith, and K. Talwar. When is memorization of irrelevant training
data necessary for high-accuracy learning? In Proceedings of the 53rd Annual ACM SIGACT
Symposium on Theory of Computing, pages 123–132, 2021."
REFERENCES,0.5626373626373626,"N. Carlini, C. Liu, Ú. Erlingsson, J. Kos, and D. Song. The secret sharer: Evaluating and testing
unintended memorization in neural networks. In 28th USENIX Security Symposium (USENIX
Security 19), pages 267–284, 2019."
REFERENCES,0.5648351648351648,"N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown,
D. Song, U. Erlingsson, et al. Extracting training data from large language models. In 30th
USENIX Security Symposium (USENIX Security 21), pages 2633–2650, 2021."
REFERENCES,0.567032967032967,"N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying memorization
across neural language models. arXiv preprint arXiv:2202.07646, 2022."
REFERENCES,0.5692307692307692,"N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramèr, B. Balle, D. Ippolito, and E. Wallace.
Extracting training data from diffusion models. arXiv preprint arXiv:2301.13188, 2023."
REFERENCES,0.5714285714285714,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pages
248–255. Ieee, 2009."
REFERENCES,0.5736263736263736,"D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.
University of Montreal, 1341(3):1, 2009."
REFERENCES,0.5758241758241758,"V. Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings of
the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 954–959, 2020."
REFERENCES,0.578021978021978,"V. Feldman and C. Zhang. What neural networks memorize and why: Discovering the long tail via
influence estimation. Advances in Neural Information Processing Systems, 33:2881–2891, 2020."
REFERENCES,0.5802197802197803,"M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information
and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer
and communications security, pages 1322–1333, 2015."
REFERENCES,0.5824175824175825,"R. Gandikota, J. Materzy´nska, J. Fiotto-Kaufman, and D. Bau. Erasing concepts from diffusion
models. arXiv preprint arXiv:2303.07345, 2023."
REFERENCES,0.5846153846153846,"J. Geiping, H. Bauermeister, H. Dröge, and M. Moeller. Inverting gradients-how easy is it to
break privacy in federated learning? Advances in Neural Information Processing Systems, 33:
16937–16947, 2020."
REFERENCES,0.5868131868131868,"A. Ghiasi, H. Kazemi, S. Reich, C. Zhu, M. Goldblum, and T. Goldstein. Plug-in inversion: Model-
agnostic inversion for vision with data augmentations. 2022."
REFERENCES,0.589010989010989,"X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the thirteenth international conference on artificial intelligence and statistics,
pages 249–256. JMLR Workshop and Conference Proceedings, 2010."
REFERENCES,0.5912087912087912,"N. Haim, G. Vardi, G. Yehudai, O. Shamir, and M. Irani. Reconstructing training data from trained
neural networks. NeurIPS, 2022."
REFERENCES,0.5934065934065934,"K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level per-
formance on imagenet classification. In Proceedings of the IEEE international conference on
computer vision, pages 1026–1034, 2015."
REFERENCES,0.5956043956043956,"Z. He, T. Zhang, and R. B. Lee. Model inversion attacks against collaborative inference. In
Proceedings of the 35th Annual Computer Security Applications Conference, pages 148–162, 2019."
REFERENCES,0.5978021978021978,"B. Hitaj, G. Ateniese, and F. Perez-Cruz. Deep models under the gan: information leakage from
collaborative deep learning. In Proceedings of the 2017 ACM SIGSAC conference on computer
and communications security, pages 603–618, 2017."
REFERENCES,0.6,"Y. Huang, S. Gupta, Z. Song, K. Li, and S. Arora. Evaluating gradient inversion attacks and defenses
in federated learning. Advances in Neural Information Processing Systems, 34:7232–7241, 2021."
REFERENCES,0.6021978021978022,"P. J. Huber. Robust estimation of a location parameter. Breakthroughs in statistics: Methodology and
distribution, pages 492–518, 1992."
REFERENCES,0.6043956043956044,"A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. Advances in neural information processing systems, 31, 2018."
REFERENCES,0.6065934065934065,"Z. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. Advances in Neural
Information Processing Systems, 33:17176–17186, 2020."
REFERENCES,0.6087912087912087,"A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.610989010989011,"N. Kumari, B. Zhang, S.-Y. Wang, E. Shechtman, R. Zhang, and J.-Y. Zhu. Ablating concepts in
text-to-image diffusion models. Arxiv, 2023."
REFERENCES,0.6131868131868132,"Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. ATT Labs [Online]. Available:
http://yann.lecun.com/exdb/mnist, 2, 2010."
REFERENCES,0.6153846153846154,"N. Loo, R. Hasani, M. Lechner, and D. Rus. Dataset distillation fixes dataset reconstruction attacks.
arXiv preprint arXiv:2302.01428, 2023."
REFERENCES,0.6175824175824176,"K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. arXiv
preprint arXiv:1906.05890, 2019."
REFERENCES,0.6197802197802198,"A. Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5188–5196,
2015."
REFERENCES,0.621978021978022,"A. Mordvintsev, C. Olah, and M. Tyka. Inceptionism: Going deeper into neural networks. 2015."
REFERENCES,0.6241758241758242,"E. Moroshko, B. E. Woodworth, S. Gunasekar, J. D. Lee, N. Srebro, and D. Soudry. Implicit bias in
deep linear classification: Initialization scale vs training accuracy. Advances in neural information
processing systems, 33:22182–22193, 2020."
REFERENCES,0.6263736263736264,"Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images
with unsupervised feature learning. 2011."
REFERENCES,0.6285714285714286,"A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune. Synthesizing the preferred inputs
for neurons in neural networks via deep generator networks. Advances in neural information
processing systems, 29, 2016."
REFERENCES,0.6307692307692307,"A. Nguyen, J. Clune, Y. Bengio, A. Dosovitskiy, and J. Yosinski. Plug & play generative networks:
Conditional iterative generation of images in latent space. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 4467–4477, 2017."
REFERENCES,0.6329670329670329,"A. Radhakrishnan, K. Yang, M. Belkin, and C. Uhler. Memorization in overparameterized autoen-
coders. arXiv preprint arXiv:1810.10333, 2018."
REFERENCES,0.6351648351648351,"G. Somepalli, V. Singla, M. Goldblum, J. Geiping, and T. Goldstein. Diffusion art or digital forgery?
investigating data replication in diffusion models. arXiv preprint arXiv:2212.03860, 2022."
REFERENCES,0.6373626373626373,"D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient
descent on separable data. The Journal of Machine Learning Research, 19(1):2822–2878, 2018."
REFERENCES,0.6395604395604395,"G. Vardi and O. Shamir. Implicit regularization in relu networks with the square loss. In Conference
on Learning Theory, pages 4224–4258. PMLR, 2021."
REFERENCES,0.6417582417582418,"Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error
visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004."
REFERENCES,0.643956043956044,"Z. Wang, J. Lee, and Q. Lei. Reconstructing training data from model gradient, provably. In
International Conference on Artificial Intelligence and Statistics, pages 6595–6612. PMLR, 2023."
REFERENCES,0.6461538461538462,"Y. Wen, J. Geiping, L. Fowl, M. Goldblum, and T. Goldstein. Fishing for user data in large-batch
federated learning via gradient magnification. arXiv preprint arXiv:2202.00580, 2022."
REFERENCES,0.6483516483516484,"Z. Yang, J. Zhang, E.-C. Chang, and Z. Liang. Neural network inversion in adversarial setting via
background knowledge alignment. In Proceedings of the 2019 ACM SIGSAC Conference on
Computer and Communications Security, pages 225–240, 2019."
REFERENCES,0.6505494505494506,"H. Yin, P. Molchanov, J. M. Alvarez, Z. Li, A. Mallya, D. Hoiem, N. K. Jha, and J. Kautz. Dreaming
to distill: Data-free knowledge transfer via deepinversion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 8715–8724, 2020."
REFERENCES,0.6527472527472528,"H. Yin, A. Mallya, A. Vahdat, J. M. Alvarez, J. Kautz, and P. Molchanov. See through gradients:
Image batch recovery via gradinversion. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 16337–16346, 2021."
REFERENCES,0.654945054945055,"C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. CoRR, abs/1611.03530, 2016. URL http://arxiv.org/abs/1611.
03530."
REFERENCES,0.6571428571428571,"C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires
rethinking generalization. Communications of the ACM, 64(3):107–115, 2021."
REFERENCES,0.6593406593406593,"R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR, 2018."
REFERENCES,0.6615384615384615,"L. Zhu, Z. Liu, and S. Han. Deep leakage from gradients. Advances in Neural Information Processing
Systems, 32, 2019."
REFERENCES,0.6637362637362637,"A
Analyzing and Visualizing the Results of the Reconstruction Optimization"
REFERENCES,0.6659340659340659,"The analysis of the results of the various reconstruction losses Eqs. (6), (12) and (15), involve
verifying and checking which of the training samples were reconstructed. In this section we provide
further details on our method for analyzing the reconstruction results, and how we measure the quality
of our reconstructions."
REFERENCES,0.6681318681318681,"A.1
Analyzing the Results of the Reconstruction Optimization"
REFERENCES,0.6703296703296703,"In order to match between samples from the training set and the outputs of the reconstruction
algorithm (the so-called ""candidates"") we follow the same protocol of Haim et al. [2022]. Note that
before training our models, we subtract the mean image from the given training set. Therefore the
training samples are d-dimensional objects where each entry is in [−1, 1]."
REFERENCES,0.6725274725274726,"First, for each training sample we compute the distance to all the candidates using a normalized L2
score:"
REFERENCES,0.6747252747252748,"d(x, y) =

x −µx"
REFERENCES,0.676923076923077,"σx
−y −µy σy  2"
REFERENCES,0.6791208791208792,"2
(16)"
REFERENCES,0.6813186813186813,"Where x, y ∈Rd are a training sample or an output candidate from the reconstruction algorithm,"
REFERENCES,0.6835164835164835,µx = 1
REFERENCES,0.6857142857142857,"d
Pd
i=1 x(i) is the mean of x and σx =
q"
REFERENCES,0.6879120879120879,"1
d−1
Pd
i=1(x(i) −µx)2 is the standard deviation
of x (and the same goes for y, µy, σy)."
REFERENCES,0.6901098901098901,"Second, for each training sample, we take C candidates with the smallest distance according to
Eq. (16). C is determined by finding the first candidate whose distance is larger than B times the
distance to the closest nearest neighbour (where B is a hyperparameter). Namely, for a training sample
x, the nearest neighbour is y1 with a distance d(x, y1), then C is determined by finding a candidate
yC+1 whose distance is d(x, yC+1) > B · d(x, y1), and for all j ≤C, d(x, yj) ≤B · d(x, y1). B
was chosen heuristically to be B = 1.1 for MLPs, and B = 1.5 for convolutional models. The C
candidates are then summed to create the reconstructed sample ˆx = 1"
REFERENCES,0.6923076923076923,"C
PC
j=1 yj. In general, we can
also take only C = 1 candidate, namely just one nearest neighbour per training sample, but choosing
more candidates improve the visual quality of the reconstructed samples."
REFERENCES,0.6945054945054945,"Third, the reconstructed sample ˆx is scaled to an image in [0, 1] by adding the training set mean and
linearly ""stretching"" the minimal and maximal values of the result to [0, 1]. Finally, we compute
the SSIM between the training sample x and the reconstructed sample ˆx to measure the quality of
reconstruction."
REFERENCES,0.6967032967032967,"A.2
Deciding whether a Reconstruction is “Good”"
REFERENCES,0.6989010989010989,"Here we justify our selection for SSIM=0.4 as the threshold for what we consider as a “good""
reconstruction. In general, the problem of deciding whether a reconstruction is the correct match to a
given sample, or whether a reconstruction is a “good"" reconstruction is equivalent to the problem of
comparing between images. No “synthetic"" metric (like SSIM, l2 etc.) will be aligned with human
perception. A common metric for this purpose is LPIPS [Zhang et al., 2018] that uses a classifier
trained on Imagenet [Deng et al., 2009], but since CIFAR images are much smaller than Imagenet
images (32 × 32 vs. 224 × 224) it is not clear that this metric will be better than SSIM."
REFERENCES,0.701098901098901,"As a simple rule of thumb, we use SSIM>0.4 for deciding that a given reconstruction is “good"".
To justify, we plot the best reconstructions (in terms of SSIM) in Fig. 8. Note that almost all
samples with SSIM>0.4 are also visually similar (for a human). Also note that some of the samples
with SSIM<0.4 are visually similar, so in this sense we are “missing"" some good reconstructions.
In general, determining whether a candidate output of a reconstruction algorithm is a match to a
training sample is an open question and a problem in all other works for data reconstruction, see for
example Carlini et al. [2023] that derived a heuristic for reconstructed samples from a generative
model. This cannot be dealt in the scope of this paper, and is an interesting future direction for our
work."
REFERENCES,0.7032967032967034,"0.66
0.63
0.62
0.62
0.62
0.61
0.6
0.55
0.55
0.55
0.54
0.53
0.53
0.51
0.5
0.5
0.5
0.5
0.5
0.48"
REFERENCES,0.7054945054945055,"0.48
0.48
0.47
0.47
0.47
0.47
0.46
0.46
0.46
0.46
0.45
0.45
0.45
0.45
0.45
0.44
0.44
0.44
0.44
0.44"
REFERENCES,0.7076923076923077,"0.44
0.44
0.44
0.44
0.44
0.44
0.43
0.43
0.43
0.43
0.43
0.42
0.42
0.42
0.42
0.42
0.42
0.41
0.41
0.41"
REFERENCES,0.7098901098901099,"0.41
0.41
0.4
0.4
0.4
0.4
0.4
0.4
0.39
0.39
0.39
0.38
0.38
0.38
0.38
0.38
0.38
0.38
0.38
0.38"
REFERENCES,0.7120879120879121,"0.38
0.38
0.38
0.37
0.37
0.37
0.37
0.37
0.37
0.37
0.37
0.37
0.37
0.37
0.37
0.37
0.36
0.36
0.36
0.36"
REFERENCES,0.7142857142857143,"0.36
0.36
0.36
0.36
0.36
0.36
0.35
0.35
0.35
0.35
0.35
0.35
0.35
0.35
0.35
0.34
0.34
0.34
0.34
0.34"
REFERENCES,0.7164835164835165,"Figure 8: Justifying the threshold of SSIM= 0.4 as good rule-of-thumb for a threshold for a “good""
reconstruction. The SSIM values are shown above each train-reconstruction pair. Note that samples
with SSIM> 0.4 (blue) are visually similar. Also some of the samples with SSIM< 0.4 (red) are
similar. In general deciding whether a reconstruction is “good"" is an open question beyond the scope
of this paper."
REFERENCES,0.7186813186813187,"B
Implementation Details"
REFERENCES,0.7208791208791209,"Further Training Details.
The models that were reconstructed in the main part of the paper were
trained with learning rates of 0.01 for binary classifiers (both MLP and convolutional), and 0.5 in the
case of multi-class classifier (Section 4). The models were trained with full batch gradient descent for
106 epochs, to guarantee convergence to a KKT point of Eq. (1) or a local minima of Eq. (13). When
small initialization of the first layer is used (e.g., in Figs. 2 and 3), the weights are initialized with a
scale of 10−4. We note that Haim et al. [2022] observed that models trained with SGD can also be
reconstructed. The experiment in Appendix G (large models with many samples) also uses SGD and
results with similar conclusion, that some models trained with SGD can be reconstructed. In general,
exploring reconstruction from models trained with SGD is an interesting direction for future works."
REFERENCES,0.7230769230769231,"Runtime and Hardware.
Runtime of a single reconstruction run (specific choice of hyperparame-
ters) from a model D-1000-1000-1 takes about 20 minutes on a GPU Tesla V-100 32GB or NVIDIA
Ampere Tesla A40 48GB."
REFERENCES,0.7252747252747253,"Hyperparameters of the Reconstruction Algorithm.
Note that the reconstruction loss contains the
derivative of a model with ReLU layers, which is flat and not-continuous. Thus, taking the derivative
of the reconstruction loss results in a zero function. To address this issue we follow a solution
presented in Haim et al. [2022]. Namely, given a trained model, we replace in the backward phase
of backpropogation the ReLU function with the derivative of a softplus function (or SmoothReLU)
f(x) = α log(1+e−x), where α is a hyperparameter of the reconstruction scheme. The functionality
of the model itself does not change, as in the foraward phase the function remains a ReLU. Only
the backward function is replaced with a smoother version of the derivative of ReLU which is
f ′(x) = ασ(x) =
α
1+e−x (here σ is the Sigmoid function). To find good reconstructions we run the"
REFERENCES,0.7274725274725274,"algorithm multiple times (typically 100 times) with random search over the hyperparameters (using
the Weights & Biases framework [Biewald, 2020]). The exact parameters for the hyperparameters
search are:"
REFERENCES,0.7296703296703296,"• Learning rate: log-uniform in [10−5, 1]
• σx: log-uniform in [10−6, 0.1]
• λmin: uniform in [0.01, 0.5]
• α: uniform in [10, 500]"
REFERENCES,0.7318681318681318,"C
Multiclass Reconstruction - More Results"
REFERENCES,0.734065934065934,"C.1
Experiments with Different Number of Classes and Fixed Training Set Size"
REFERENCES,0.7362637362637363,"10
15
20
25
0.0 0.2 0.4"
REFERENCES,0.7384615384615385,"0.6
Good Rec.'s: 12"
CLASSES,0.7406593406593407,2 Classes
CLASSES,0.7428571428571429,"10
15
20
25"
CLASSES,0.7450549450549451,Good Rec.'s: 19
CLASSES,0.7472527472527473,3 Classes
CLASSES,0.7494505494505495,"10
15
20
25"
CLASSES,0.7516483516483516,Good Rec.'s: 17
CLASSES,0.7538461538461538,4 Classes
CLASSES,0.756043956043956,"10
15
20
25"
CLASSES,0.7582417582417582,Good Rec.'s: 28
CLASSES,0.7604395604395604,5 Classes
CLASSES,0.7626373626373626,"10
15
20
25"
CLASSES,0.7648351648351648,Good Rec.'s: 68
CLASSES,0.7670329670329671,10 Classes
CLASSES,0.7692307692307693,"0.0
0.2
0.4
0.6
0.8
1.0
Distance from Decision Boundary [
yi(x)
max
j
yi
j(x)] 0.0 0.2 0.4 0.6 0.8 1.0"
CLASSES,0.7714285714285715,Reconstruction
CLASSES,0.7736263736263737,"Quality [SSIM(x, x)]"
CLASSES,0.7758241758241758,"Figure 9: Experiments of reconstruction from models trained on a a fixed training set size (500
samples) for different number of classes. Number of “good"" reconstruction is shown for each model."
CLASSES,0.778021978021978,"To complete the experiment shown in Fig. 3, we also perform experiments on models trained on
various number of classes (C ∈{2, 3, 4, 5, 10}) and with a fixed training set size of 500 samples
(distributed equally between classes), see Fig. 9. It can be seen that as the number of classes increases,
also does the number of good reconstructions, where for 10 classes there are more than 6 times good
reconstructions than for 2 classes. Also, the quality of the reconstructions improves as the number of
classes increase, which is depicted by an overall higher SSIM score. We also note, that the number of
good reconstructions in Fig. 9 is very similar to the number of good reconstructions from Fig. 3 for
50 samples per class. We hypothesize that although the number of training samples increases, the
number of ""support vectors"" (i.e samples on the margin which can be reconstructed) that are required
for successfully interpolating the entire dataset does not change by much."
CLASSES,0.7802197802197802,"C.2
Results on SVHN Dataset
As shown in Fig. 10, our multiclass reconstruction scheme is not limited to CIFAR10 dataset, but can
be extended to other datasets as well, specifically SVHN dataset [Netzer et al., 2011]. The model
whose reconstructions are shown in Fig. 10 was trained on 50 samples per class (total of 10 classes)
and the rest of the training hyperparameters are the same as that of its CIFAR10 equivalent (of 50
sample per class with 10 classes)."
CLASSES,0.7824175824175824,"0
1
2
3
4
5
6
7
8
9"
CLASSES,0.7846153846153846,Figure 10: Reconstruction form model trained on 50 samples per class from the SVHN dataset.
CLASSES,0.7868131868131868,"D
General Losses - More Results"
CLASSES,0.789010989010989,"Following the discussion in Section 5 and Fig. 4, Figures 11, 12, 13 present visualizations of training
samples and their reconstructions from models trained with L2, L2.5 and Huber loss, respectively."
CLASSES,0.7912087912087912,"Figure 11: Reconstruction using L2 loss. Training samples (red) and their best reconstructions
(blue) using an MLP classifier that was trained on 300 CIFAR10 images using an L2 regression loss,
as described in Section 5 and Fig. 4."
CLASSES,0.7934065934065934,"Figure 12: Reconstruction using L2.5 loss. Training samples (red) and their best reconstructions
(blue) using an MLP classifier that was trained on 300 CIFAR10 images using an L2.5 regression
loss, as described in Section 5 and Fig. 4."
CLASSES,0.7956043956043956,"Figure 13: Reconstruction using Huber loss. Training samples (red) and their best reconstructions
(blue) using an MLP classifier that was trained on 300 CIFAR10 images using Huber loss, as described
in Section 5 and Fig. 4."
CLASSES,0.7978021978021979,"E
Further Analysis of Weight Decay"
CLASSES,0.8,"By looking at the exact distribution of reconstruction quality to the distance from the margin, we
observe that weight-decay (for some values) results in more training samples being on the margin of
the trained classifier, thus being more vulnerable to our reconstruction scheme."
CLASSES,0.8021978021978022,"This observation is shown in Fig. 14 where we show the scatter plots for all the experiments from Fig. 5
(a). We also provide the train and test errors for each model. It seems that the test error does not
change significantly. However, an interesting observation is that reconstruction is possible even for
models with non-zero training errors, i.e. models that do not interpolate the data, for which the
assumptions of Lyu and Li [2019] do not hold. 0.00 0.25 0.50 0.75 1.00"
CLASSES,0.8043956043956044,"Etrn = 0.00
Etst = 0.22"
CLASSES,0.8065934065934066,WD = 1e-05
CLASSES,0.8087912087912088,"Etrn = 0.00
Etst = 0.21"
CLASSES,0.810989010989011,WD = 2.5e-05
CLASSES,0.8131868131868132,"Etrn = 0.00
Etst = 0.22"
CLASSES,0.8153846153846154,WD = 5e-05
CLASSES,0.8175824175824176,"Etrn = 0.00
Etst = 0.22"
CLASSES,0.8197802197802198,WD = 7.5e-05
CLASSES,0.8219780219780219,"Etrn = 0.00
Etst = 0.22"
CLASSES,0.8241758241758241,WD = 0.0001
CLASSES,0.8263736263736263,"Etrn = 0.00
Etst = 0.22"
CLASSES,0.8285714285714286,WD = 0.0005
CLASSES,0.8307692307692308,"5 0
5 10 15 20 25
0.00 0.25 0.50 0.75 1.00"
CLASSES,0.832967032967033,"Etrn = 0.00
Etst = 0.22"
CLASSES,0.8351648351648352,WD = 0.001
CLASSES,0.8373626373626374,"5 0
5 10 15 20 25"
CLASSES,0.8395604395604396,"Etrn = 0.00
Etst = 0.22"
CLASSES,0.8417582417582418,WD = 0.005
CLASSES,0.843956043956044,"5 0
5 10 15 20 25"
CLASSES,0.8461538461538461,"Etrn = 0.00
Etst = 0.22"
CLASSES,0.8483516483516483,WD = 0.01
CLASSES,0.8505494505494505,"5 0
5 10 15 20 25"
CLASSES,0.8527472527472527,"Etrn = 0.14
Etst = 0.24"
CLASSES,0.8549450549450549,WD = 0.1
CLASSES,0.8571428571428571,"5 0
5 10 15 20 25"
CLASSES,0.8593406593406593,"Etrn = 0.15
Etst = 0.25"
CLASSES,0.8615384615384616,WD = 0.2
CLASSES,0.8637362637362638,"5 0
5 10 15 20 25"
CLASSES,0.865934065934066,"Etrn = 0.22
Etst = 0.27"
CLASSES,0.8681318681318682,WD = 0.3
CLASSES,0.8703296703296703,"0.0
0.2
0.4
0.6
0.8
1.0"
CLASSES,0.8725274725274725,"Distance from Decision Boundary [
yi(x)
max
j
yi
j(x)] 0.0 0.2 0.4 0.6 0.8 1.0"
CLASSES,0.8747252747252747,"Reconstruction Quality [SSIM(x, x)]"
CLASSES,0.8769230769230769,"Figure 14: Scatter plots of the 12 experiments from Fig. 5 (a). Each plot is model trained with a
different value of weight decay on 2 classes with 50 samples in each class. Certain values of weight
decay make the model more susceptible to our reconstruction scheme."
CLASSES,0.8791208791208791,"F
Convolutional Neural Networks - Ablations and Observations"
CLASSES,0.8813186813186813,"In this section we provide more results and visualizations to the experiments on convolutional neural
network in Section 6.1."
CLASSES,0.8835164835164835,"In Fig. 15 we show ablations for the choice of the kernel-size (k) and number of output channels
(Cout) for models with architecture CONV(kernel-size=k,output-channels=Cout)-1000-1. All models
were trained on 500 images (250 images per class) from the CIFAR10 dataset, with weight-decay"
CLASSES,0.8857142857142857,"term λWD=0.001. As can be seen, for such convolutional models we are able to reconstruct samples
for a wide range of choices."
CLASSES,0.8879120879120879,"Note that the full summary of reconstruction quality versus the distance from the decision boundary
for the model whose reconstucted samples are shown in Fig. 6, is shown in Fig. 15 for kernel-size 3
(first row) and number of output channels 32 (third column)."
CLASSES,0.8901098901098901,"Further analysis of Fig. 15.
As expected for models with less parameters, the reconstructability
decreases as the number of output channels decrease. An interesting phenomenon is observed for
varying the kernel size: for a fixed number of output channel, as the kernel size increases, the
susceptibility of the model to our reconstruction scheme decreases. However, as the kernel size
approaches 32 (the full resolution of the input image), the reconstructability increases once again.
On the one hand it is expected, since for kernel-size=32 the model is essentially an MLP, albeit
with smaller hidden dimension than usual (at most 64 here, whereas the typical model used in the
paper had 1000). On the other hand, it is not clear why for some intermediate values of kernel size
(in between 3 and 32) the reconstructability decreases dramatically (for many models there are no
reconstructed samples at all). This observation is an interesting research direction for future works."
CLASSES,0.8923076923076924,"0.0
0.2
0.4
0.6
0.8 3"
CLASSES,0.8945054945054945,"16.80%
0.4 8"
CLASSES,0.8967032967032967,"14.20%
0.4 16"
CLASSES,0.8989010989010989,"19.00%
0.4 32"
CLASSES,0.9010989010989011,"8.40%
0.4 64"
CLASSES,0.9032967032967033,"0.0
0.2
0.4
0.6
0.8 5"
CLASSES,0.9054945054945055,"31.40%
0.4
18.80%
0.4
21.20%
0.4
21.00%
0.4"
CLASSES,0.9076923076923077,"0.0
0.2
0.4
0.6
0.8 7"
CLASSES,0.9098901098901099,"5.80%
0.4
9.00%
0.4
16.00%
0.4
20.20%
0.4"
CLASSES,0.9120879120879121,"0.0
0.2
0.4
0.6
0.8 9"
CLASSES,0.9142857142857143,"9.00%
0.4
21.80%
0.4
21.00%
0.4
12.00%
0.4"
CLASSES,0.9164835164835164,"0.0
0.2
0.4
0.6
0.8 11"
CLASSES,0.9186813186813186,"9.40%
0.4
17.20%
0.4
17.20%
0.4
14.40%
0.4"
CLASSES,0.9208791208791208,"0.0
0.2
0.4
0.6
0.8 13"
CLASSES,0.9230769230769231,"1.80%
0.4
2.60%
0.4
11.00%
0.4
0.40%
0.4"
CLASSES,0.9252747252747253,"0.0
0.2
0.4
0.6
0.8 17"
CLASSES,0.9274725274725275,"0.00%
0.4
0.80%
0.4
0.80%
0.4
0.40%
0.4"
CLASSES,0.9296703296703297,"0.0
0.2
0.4
0.6
0.8 19"
CLASSES,0.9318681318681319,"0.20%
0.4
0.00%
0.4
0.00%
0.4
0.20%
0.4"
CLASSES,0.9340659340659341,"0.0
0.2
0.4
0.6
0.8 21"
CLASSES,0.9362637362637363,"0.00%
0.4
0.00%
0.4
0.00%
0.4
0.00%
0.4"
CLASSES,0.9384615384615385,"0.0
0.2
0.4
0.6
0.8 25"
CLASSES,0.9406593406593406,"0.00%
0.4
0.00%
0.4
0.00%
0.4
1.00%
0.4"
CLASSES,0.9428571428571428,"0.0
0.2
0.4
0.6
0.8 29"
CLASSES,0.945054945054945,"0.00%
0.4
0.00%
0.4
0.20%
0.4
0.60%
0.4"
CLASSES,0.9472527472527472,"20
10
0
10
20
0.0
0.2
0.4
0.6
0.8 32"
CLASSES,0.9494505494505494,"0.00%
0.4"
CLASSES,0.9516483516483516,"20
10
0
10
20"
CLASSES,0.9538461538461539,"1.00%
0.4"
CLASSES,0.9560439560439561,"20
10
0
10
20"
CLASSES,0.9582417582417583,"0.80%
0.4"
CLASSES,0.9604395604395605,"20
10
0
10
20"
CLASSES,0.9626373626373627,"4.80%
0.4"
CLASSES,0.9648351648351648,"0.0
0.2
0.4
0.6
0.8
1.0"
CLASSES,0.967032967032967,Number of Channels (Cout) 0.0 0.2 0.4 0.6 0.8 1.0
CLASSES,0.9692307692307692,Kernel Size (k)
CLASSES,0.9714285714285714,"Figure 15: Ablating the choice of the kernel size and output-channels for reconstruction from neu-
ral binary classifiers with architecture CONV(kernel-size=k,output-channels=Cout)-1000-1. (Please
note that this figure might not be displayed well on Google Chrome. Please open in Acrobat reader.)"
CLASSES,0.9736263736263736,"Visualizing Kernels.
In Haim et al. [2022], it was shown that some of the training samples can
be found in the first layer of the trained MLPs, by reshaping and visualizing the weights of the first
fully-connected layer. As opposed to MLPs, in the case of a model whose first layer is a convolution
layer, this is not possible. For completeness, in Fig. 16 we visualize all 32 kernels of the Conv layer.
Obviously, full images of shape 3x32x32 cannot be found in kernels of shape 3x3x3, which makes
reconstruction from such models (with convolution first layer) even more interesting."
CLASSES,0.9758241758241758,"Figure 16: The kernels of the model whose reconstructions are shown in Fig. 6, displayed as RGB
images."
CLASSES,0.978021978021978,"G
Reconstruction From a Larger Number of Samples"
CLASSES,0.9802197802197802,"One of the major limitations of Haim et al. [2022] is that they reconstruct from models that trained on
a relatively small number of samples. Specifically, in their largest experiment, a model is trained with
only 1,000 samples. Here we take a step further, and apply our reconstruction scheme for a model
trained on 5,000 data samples."
CLASSES,0.9824175824175824,"To this end, we trained a 3-layer MLP, where the number of neurons in each hidden layer is 10,000.
Note that the size of the hidden layer is 10 times larger than in any other model we used. Increasing
the number of neurons seems to be one of the major reasons for which we are able to reconstruct
from such large datasets, although we believe it could be done with smaller models, which we leave
for future research. We used the CIFAR100 dataset, with 50 samples in each class, for a total of 5000
samples."
CLASSES,0.9846153846153847,"In Fig. 17a we give the best reconstructions of the model. Note that although there is a degradation in
the quality of the reconstruction w.r.t a model trained on less samples, it is still clear that our scheme
can reconstruct some of the training samples to some extent. In Fig. 17b we show a scatter plot of the
SSIM score w.r.t the distance from the boundary, similar to Fig. 3a. Although most of the samples
are on or close to the margin, only a few dozens achieve an SSIM> 0.4. This may indicate that there
is a potential for much more images to reconstruct, and possibly with better quality."
CLASSES,0.9868131868131869,(a) Full Images. Original samples from the training set (red) and reconstructed results (blue)
CLASSES,0.989010989010989,"6
8
10
12
14
16
18
Distance from Decision Boundary [
yi(x)
max
j
yi
j(x)] 0.0 0.2 0.4"
CLASSES,0.9912087912087912,Reconstruction
CLASSES,0.9934065934065934,"Quality [SSIM(x, x)]"
CLASSES,0.9956043956043956,(b) Scatter plot (similar to Fig. 3) .
CLASSES,0.9978021978021978,"Figure 17: Reconstruction from a model trained on 50 images per class from the CIFAR100 dataset
(100 classes, total of 5000 datapoints). The model is a 3-layer MLP with 10000 neurons in each layer.
(Please note that this figure might not be displayed well on Google Chrome. Please open in Acrobat
reader.)"
