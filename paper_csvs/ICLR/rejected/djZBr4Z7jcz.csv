Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016366612111292963,"Recently, a wide range of recommendation algorithms inspired by deep learning
techniques have emerged as the performance leaders on several standard recommen-
dation benchmarks. While these algorithms were built on different deep learning
techniques (e.g., dropouts, autoencoder), they have similar performance and even
similar cost functions. This paper studies whether the models’ comparable per-
formance are sheer coincidence, or they can be unified under a single framework.
We find that all competitive linear models effectively add only a nuclear-norm
regularizer, or a Frobenius-norm regularizer. The former ones possess a (surprising)
rigid structure that limits the models’ predictive power but their solutions are low
rank and have closed-form. The latter ones are more expressive so they have better
performance. However, the performance leaders EASE and EDLAE have only
full-rank closed-form solutions. Their low-rank counterparts can be solved only
by hard-to-tune numeric procedures such as Alternating Directions Method of
Multipliers (ADMM). Along this line of findings, we further propose two low-
rank, closed-form solutions, derived from carefully generalizing Frobenius-norm
regularization techniques."
INTRODUCTION,0.0032733224222585926,"1
INTRODUCTION"
INTRODUCTION,0.004909983633387889,"Research progress on algorithms for recommendation has escalated in recent years, partially fueled
by the adoption of deep learning (DL) techniques. However, recent studies have found that many
new deep learning recommendation models have shown sub-par performance against simpler linear
recommendation models (Dacrema et al., 2019; Rendle et al., 2019). Although some studies are
available to analyze linear vs non-linear models (Dacrema et al., 2019), it remains puzzling why
these seemingly different techniques all result in models with similar performance or even similar
cost functions. In the latest study, Jin et al. (2021) examine the relationship between the widely
used matrix factorization (MF), such as ALS (Hu et al., 2008), and the linear autoencoder (LAE)
which encompasses the recent performance leaders, such as EASE (Steck, 2019) and EDLAE (Steck,
2020). They considered two basic regularization forms (see Eqs. (1) and (6)) and found that the
optimal (closed-form) solutions of both models recover the directions of principal components, while
shrinking the corresponding singular values differently. They suggest that the difference may enable
LAE to utilize a larger number of latent dimensions to improve recommendation accuracy. This
finding highlights the similarity and difference between LAE and MF."
INTRODUCTION,0.006546644844517185,"In this paper, we go much beyond the two basic models studied in (Jin et al., 2021), to analyze a large
number of recent performance leaders of (linear) recommendation algorithms, and aim to provide an
in-depth understanding of various regularized objective functions and towards unifying them through
their closed-form solutions. In return, the closed-forms serve as the barebones engine to help reveal
what drives the performance improvement for the recent recommendation algorithms."
INTRODUCTION,0.008183306055646482,"We examine three open and closely related questions: (i) Characterization of models: Recent
recommendation models are built upon a diverse set of techniques, including dropouts (Srivastava
et al., 2014), variational methods (Kingma & Welling, 2014), and matrix denoising (Tipping &
Bishop, 1999b). We aim to identify a unified regularization framework to (re)-interpret different
models and understand how they are related. We are specifically interested in how models based
on variational autoencoder (VAE, a natural generalization of recent approaches) relate to other
models (Liang et al., 2018). (ii) Weighted generalization of regularizers: A key idea in a recent
performance leader EDLAE (Steck, 2020) is to utilize of weighted/non-uniform regularizers on the"
INTRODUCTION,0.009819967266775777,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.011456628477905073,"parameters’ Frobenius norm. Applying dropouts(Cavazza et al., 2018) is shown to be equivalent to
re-weighting the exponents in the regularizers, such as designing the weighted sum of regularizers
based on other norms or tuning the exponent weights. We are also interested in determining what
circumstance closed-form solutions still exist. (iii) Low-rank closed-form solutions for EDLAE.
While most linear recommendation models are shown to have closed-form solutions, the low-rank
version of recent performance leader EDLAE (Steck, 2020) remains an exception. Fitting this
model requires using ADMM (Steck, 2020). Although its performance may not be better than its
full-rank counterpart, low-rank solutions are easier to interpret, use less storage, and can be more
scalable with respect to the number of items. In addition, a closed-form solution disentangles key
performance drivers from nuances (e.g., need to tune learning rate or deal with local optimal), and
can help reveal the key driven factors comparing with other closed-form solutions. More importantly,
such solutions are significantly easier to implement and be tested using a generic matrix computation
platform (without specialized recommendation library). Can we approximate low-rank EDLAE with
closed-form solutions?"
INTRODUCTION,0.01309328968903437,Our investigation led to the following discovery and resolves the above questions:
INTRODUCTION,0.014729950900163666,"Regularizer dichotomy (Section 2): We found that all of the leading (linear) recommendation
models can be categorized into those that implement possibly weighted nuclear-norm regularizers, or
those that implement Frobenius-norm regularizers. Specifically, we characterize the Variational Linear
Autoencoders (VLAE) as a form of the weighted nuclear-norm regularization problem, in which the
weights possess a specific combinatorial structure. We observe that it is not matrix factorization or
LAE that determines the shrinkage structure (as Jin et al. (2021) suggested), but instead it is the form
of regularization. Thus, our paper provides a more complete and accurate characterization of a linear
recommendation model’s performance under different regularizations."
INTRODUCTION,0.016366612111292964,"Rigidity of nuclear-norm regularizers (Section 3): With the dichotomy result, we next aim to
understand whether the weighted sum idea for Frobenius-norm regularizers (Steck, 2020; Jin et al.,
2021) can be generalized to nuclear-norm regularizers, and whether tuning exponent weights can
be beneficial. First, while VLAE is equivalent to nuclear-norm regularization and is easier to
optimize (weighted nuclear norms are non-smooth but VLAE’s objectives are smooth), its closed-
form solution possesses a (surprisingly) rigid structure, i.e., the weighted regularization will lead to
an auto-sorting singular value reduction and the larger single values tend to receive smaller reduction
(non-ascending reduction). Second, it has been shown that the solution structure for a model with
a squared nuclear-norm regularizer ∥W∥2
∗(i.e., dropout’s equivalence) is strikingly similar to that
for using ∥W∥∗-regularizer (Gu et al., 2014). We generalize the result to show that the solution
structures for ∥W∥p
∗are highly similar for all p ≥1 (Regularization invariant/rigidity with respect to
p). But when p = 1, 2, the solution and hyperparameters possess favorable properties which make
hyperparameter searches easier. This also partially explains why only p = 1, 2 have been extensively
considered. These rigidity properties severely limit the search space and explains why models that
use only nuclear-norm regularizers share the same performance ceiling even when hyperparameters
are extensively searched."
INTRODUCTION,0.01800327332242226,"Closed-form solution for low-rank EDLAE (Section 4): The (weighted) Frobenius-norm reg-
ularizers ∥ΛW∥2
F (Λ is the hyperparameter diagonal matrix) are implemented in EASE (Steck,
2019) and EDLAE (Steck, 2020). These models produce closed-form full-rank estimators; and if
the zero-diagonal constraint on W is enforced, their singular vectors will no longer coincide with
those of the data, and will deliver (slightly) better performance. However, no closed-form solution
for the low-rank estimator is known and existing approaches rely on ADMM or Alternating Least
Square (Steck, 2020). In this paper, we propose two low-rank, closed-form estimators that deliver
comparable results to the full-rank models (EASE and full-rank EDLAE) as well as the ADMM-based
solutions (Steck, 2020), and thus resolve the aforementioned third open question."
BACKGROUND AND REGULARIZER DICHOTOMY,0.019639934533551555,"2
BACKGROUND AND REGULARIZER DICHOTOMY"
BACKGROUND AND REGULARIZER DICHOTOMY,0.02127659574468085,"This section first explains the background, and then give an overview of the dichotomy results. Some
key theorems are deferred to Section 3 and Appendices."
BACKGROUND AND REGULARIZER DICHOTOMY,0.022913256955810146,"Background. Recommendation algorithms can be categorized into explicit ones that aim to predict
unseen ratings between a user and an item and implicit ones that aim to predict actions, such as user
click or add-cart (Steck, 2019; Dacrema et al., 2019; Zhang et al., 2019). We focus on the implicit"
BACKGROUND AND REGULARIZER DICHOTOMY,0.024549918166939442,Under review as a conference paper at ICLR 2022
BACKGROUND AND REGULARIZER DICHOTOMY,0.02618657937806874,"Table 1: Investigating the closed/analytic solutions of linear models. dMat(·) denotes a diagonal
matrix, diag(X) is the vector on the diagonal of X. Λ is the (hyperparameter) diagonal matrix as a
coefficient of the regularization term. W ∗(or P ∗, Q∗, etc) is the optimal solution for corresponding
case, except for cases 9-12, where c
W is the low-rank closed-form solution.
Model
Regularization
Solution"
BACKGROUND AND REGULARIZER DICHOTOMY,0.027823240589198037,Nuclear norm
BACKGROUND AND REGULARIZER DICHOTOMY,0.029459901800327332,"1. Regularized PCA (Zheng et al., 2018)
min
P,Q ||X −PQ||2
F + λ · (||P||2
F + ||Q||2
F )
X
SVD
= UΣV T
Ω=
p"
BACKGROUND AND REGULARIZER DICHOTOMY,0.031096563011456628,"(σi −λ)+
P ∗= Uk
Q∗= ΩV T
k"
BACKGROUND AND REGULARIZER DICHOTOMY,0.03273322422258593,"2. MF dropout (Cavazza et al., 2018)"
BACKGROUND AND REGULARIZER DICHOTOMY,0.03436988543371522,"min
P,Q,d ||X −PQ||2
F + d1 −p p
· d
X"
BACKGROUND AND REGULARIZER DICHOTOMY,0.03600654664484452,"k=1
||Pk||2
2 · ||QT
k ||2
2"
BACKGROUND AND REGULARIZER DICHOTOMY,0.03764320785597381,"min
Y
||X −Y ||2
F + 1 −p"
BACKGROUND AND REGULARIZER DICHOTOMY,0.03927986906710311,"p
||Y ||2
∗"
BACKGROUND AND REGULARIZER DICHOTOMY,0.04091653027823241,"X
SVD
= UΣV T"
BACKGROUND AND REGULARIZER DICHOTOMY,0.0425531914893617,Y ∗= P ∗· Q∗
BACKGROUND AND REGULARIZER DICHOTOMY,0.044189852700491,= U · Sµ(Σ) · V T
BACKGROUND AND REGULARIZER DICHOTOMY,0.04582651391162029,"3. WLAE (Bao et al., 2020)
min
W1,W2 ∥X −XW1W2∥2
F + ∥W1Λ
1
2 ∥2
F + ∥Λ
1
2 W2∥2
F ,
W ∗
1 = V (I −ΛS−2)
1
2 P T"
BACKGROUND AND REGULARIZER DICHOTOMY,0.04746317512274959,"W ∗
2 = P(I −ΛS−2)
1
2 V T"
BACKGROUND AND REGULARIZER DICHOTOMY,0.049099836333878884,4. VLAE (this paper)
BACKGROUND AND REGULARIZER DICHOTOMY,0.05073649754500818,"min
P,Q ||X −PQ||2
F + ||Λ1/2Q||2
F + ||PΛ1/2||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.05237315875613748,"min
A,B ||X −XAB||2
F + ||ΛB||2
F + ||XA||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.054009819967266774,"min
rank(W )≤k ||X −W||2
F + 2||W||w,∗"
BACKGROUND AND REGULARIZER DICHOTOMY,0.05564648117839607,"X
SVD
= UΣV T"
BACKGROUND AND REGULARIZER DICHOTOMY,0.057283142389525366,"P ∗= Uk · diag(
q"
BACKGROUND AND REGULARIZER DICHOTOMY,0.058919803600654665,"σ1 −λ(k), . . . ,
q"
BACKGROUND AND REGULARIZER DICHOTOMY,0.060556464811783964,σk −λ(1)) · Ω
BACKGROUND AND REGULARIZER DICHOTOMY,0.062193126022913256,"Q∗= ΩT · diag(
q"
BACKGROUND AND REGULARIZER DICHOTOMY,0.06382978723404255,"σ1 −λ(k), . . . ,
q"
BACKGROUND AND REGULARIZER DICHOTOMY,0.06546644844517185,"σk −λ(1)) · V T
k"
BACKGROUND AND REGULARIZER DICHOTOMY,0.06710310965630115,"A∗= X†P ∗Λ
1
2
B∗= Λ−1 2 Q∗"
BACKGROUND AND REGULARIZER DICHOTOMY,0.06873977086743044,Frobenius norm
BACKGROUND AND REGULARIZER DICHOTOMY,0.07037643207855974,"5. EASE (full rank) (Steck, 2019)"
BACKGROUND AND REGULARIZER DICHOTOMY,0.07201309328968904,"min
W ||X −XW||2
F + λ · ||W||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.07364975450081833,"s.t.
diag(W) = 0"
BACKGROUND AND REGULARIZER DICHOTOMY,0.07528641571194762,C = (XT X + λI)−1
BACKGROUND AND REGULARIZER DICHOTOMY,0.07692307692307693,W ∗= I −C · dMat(diag(1 ⊘C))
BACKGROUND AND REGULARIZER DICHOTOMY,0.07855973813420622,"6. DLAE(full rank) (Steck, 2020)"
BACKGROUND AND REGULARIZER DICHOTOMY,0.08019639934533551,"min
W ||X −XW||2
F + ||Λ1/2 · W||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.08183306055646482,"Λ =
p
1 −pdMat(diag(XT X))
W ∗= (XT X + Λ)−1XT X"
BACKGROUND AND REGULARIZER DICHOTOMY,0.08346972176759411,"7. EDLAE(full rank) (Steck, 2020)"
BACKGROUND AND REGULARIZER DICHOTOMY,0.0851063829787234,"min
W ||X −XW||2
F + ||Λ1/2 · W||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.0867430441898527,"Λ =
p
1 −pdMat(diag(XT X))"
BACKGROUND AND REGULARIZER DICHOTOMY,0.088379705400982,"s.t.
diag(W) = 0"
BACKGROUND AND REGULARIZER DICHOTOMY,0.09001636661211129,C = (XT X + Λ)−1
BACKGROUND AND REGULARIZER DICHOTOMY,0.09165302782324058,W ∗= I −C · dMat(diag(1 ⊘C))
BACKGROUND AND REGULARIZER DICHOTOMY,0.09328968903436989,"8. LRR (Jin et al., 2021)
min
rank(W )≤k ||X −XW||2
F + ||ΓW||2
F
Y
∗= XW ∗SVD
= UΣV
c
W = (XT X + ΓT Γ)−1XT X(VkV T
k )"
BACKGROUND AND REGULARIZER DICHOTOMY,0.09492635024549918,"9. EDLAE-ADMM (Steck, 2020)"
BACKGROUND AND REGULARIZER DICHOTOMY,0.09656301145662848,"min
A,B ||X −X(AB −dMat(diag(AB)))||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.09819967266775777,"+||Λ1/2 · (AB −dMat(diag(AB))||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.09983633387888707,"ADMM update A, B"
BACKGROUND AND REGULARIZER DICHOTOMY,0.10147299509001637,10. LR-DLAE (this paper)
BACKGROUND AND REGULARIZER DICHOTOMY,0.10310965630114566,"min
rank(W )≤k ||X −XW||2
F + ||Λ1/2 · W||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.10474631751227496,"Λ =
p
1 −pdMat(diag(XT X))"
BACKGROUND AND REGULARIZER DICHOTOMY,0.10638297872340426,W ∗= (XT X + Λ)−1XT X
BACKGROUND AND REGULARIZER DICHOTOMY,0.10801963993453355,"Y
∗= XW ∗SVD
= UΣV T"
BACKGROUND AND REGULARIZER DICHOTOMY,0.10965630114566285,"c
W = W ∗(VkV T
k )"
BACKGROUND AND REGULARIZER DICHOTOMY,0.11129296235679215,11. LR-EDLAE-1 (this paper)
BACKGROUND AND REGULARIZER DICHOTOMY,0.11292962356792144,"min
rank(W ′)≤k ||X −XW||2
F + ||Λ1/2 · W||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.11456628477905073,W = W ′ −dMat(diag(W ′))
BACKGROUND AND REGULARIZER DICHOTOMY,0.11620294599018004,"Λ =
p
1 −pdMat(diag(XT X))"
BACKGROUND AND REGULARIZER DICHOTOMY,0.11783960720130933,C = (XT X + Λ)−1
BACKGROUND AND REGULARIZER DICHOTOMY,0.11947626841243862,W ∗= I −C · dMat(diag(1 ⊘C))
BACKGROUND AND REGULARIZER DICHOTOMY,0.12111292962356793,"Y
∗= XW ∗SVD
= UΣV T"
BACKGROUND AND REGULARIZER DICHOTOMY,0.12274959083469722,"c
W = W ∗(VkV T
k )"
BACKGROUND AND REGULARIZER DICHOTOMY,0.12438625204582651,12. LR-EDLAE-2 (this paper)
BACKGROUND AND REGULARIZER DICHOTOMY,0.1260229132569558,"min
rank(W ′)≤k ||X −XW||2
F + ||Λ1/2 · W||2
F"
BACKGROUND AND REGULARIZER DICHOTOMY,0.1276595744680851,W = W ′ −dMat(diag(W ′))
BACKGROUND AND REGULARIZER DICHOTOMY,0.12929623567921442,"Λ =
p
1 −pdMat(diag(XT X))"
BACKGROUND AND REGULARIZER DICHOTOMY,0.1309328968903437,C = (XT X + Λ)−1
BACKGROUND AND REGULARIZER DICHOTOMY,0.132569558101473,W ∗= I −C · dMat(diag(1 ⊘C))
BACKGROUND AND REGULARIZER DICHOTOMY,0.1342062193126023,"W ∗SVD
= UΣV T
c
W = UkΣkV T
k"
BACKGROUND AND REGULARIZER DICHOTOMY,0.13584288052373159,"problem because it is more economically relevant. Here, let n be the number of items and m be
the number of users. We are given a binary matrix X ∈{0, 1}m×n that represents the interaction
between users and items so far, i.e., Xi,j = 1 iff user i has purchased or made a rating on item j.
Our goal is to produce a real-valued matrix ˆX, which we evaluate against future interactions using
information retrieval (Top-k) metrics such as Recall or nDCG."
BACKGROUND AND REGULARIZER DICHOTOMY,0.13747954173486088,"The problem is closely related to matrix completion (MC) because Xi,j = 0 can be viewed as
“missing a data point”. But MC’s evaluation metric is mean-squared error and is different from
ours (Candès & Tao, 2010). The connection between two problems results in models with similar
objectives (Zheng et al., 2018). A technique developed for one problem often finds its counterpart
for the other. Nevertheless, because of the difference in evaluation, efficacy of an algorithm for
one problem does not imply its performance guarantee for the other. Thus, the non-overlapping
component between two problems remains substantial. We also remark that our structural results on
weighted nuclear-norm is new and applicable to MC."
BACKGROUND AND REGULARIZER DICHOTOMY,0.13911620294599017,Under review as a conference paper at ICLR 2022
DICHOTOMY OF THE MODELS,0.1407528641571195,"2.1
DICHOTOMY OF THE MODELS"
DICHOTOMY OF THE MODELS,0.14238952536824878,"This section explains that (i) All linear recommendation models can be categorizeed into those that
use nuclear-norm regularizers and those that use Frobenius norm (see also Table 1), and (ii) The
form of regularization, instead of whether the problem shall be cast as matrix factorization or LAE,
determines the shrinkage structure (Proposition 1)."
DICHOTOMY OF THE MODELS,0.14402618657937807,"Nuclear-norm regularizers. Let X ∈Rm×n be a matrix of rank at most k with k leading singular
values being σ1(X) ≥σ2(X) ≥· · · ≥σk(X). Let ω = (ω1, . . . , ωk) ∈(R+)k. The weighted
nuclear norm of X with respect to ω is defined as ∥X∥ω,∗= Pk
i=1 ωi · σi(X). This is a natural
generalization of the weighted nuclear norm for low-rank matrices (Gu et al., 2014). Despite its name,
the weighted nuclear norm is neither convex nor differentiable unless ωi’s are sorted in descending
order (Chen et al., 2013; Iglesias et al., 2020)."
DICHOTOMY OF THE MODELS,0.14566284779050737,"Nuclear-norm regularizers perform ℓ1-shrinkage over the estimator’s singular values, which resembles
performing ℓ1-shrinkage for coefficients in a linear model in LASSO (Tibshirani, 1996). Therefore,
nuclear-norm regularizers also promote sparsity over the solution’s singular values (i.e., the solution
is usually low rank). Algorithms below effectively add only a nuclear-norm regularizer to MF."
DICHOTOMY OF THE MODELS,0.14729950900163666,"A1. Regularized PCA (Udell et al., 2016; Zheng et al., 2018) aims to solve"
DICHOTOMY OF THE MODELS,0.14893617021276595,"min
P,Q ∥X −PQ∥2
F + λ∥P∥2
F + λ∥Q∥2
F
(1)"
DICHOTOMY OF THE MODELS,0.15057283142389524,"where λ is the hyperparameter. It has been known that Eq. (1) is equivalent to min ˆ
X ∥X −ˆX∥2
F +
2λ∥ˆX∥∗. To solve Eq. (1), one can use factored gradient descent (Bhojanapalli et al., 2016) or directly
derive its closed-form solution (Kunin et al., 2019), which involves computation of SVD of X (see
case 1 in Table 1)."
DICHOTOMY OF THE MODELS,0.15220949263502456,"A2. Matrix Factorization with dropouts.
This approach uses PQ (P ∈Rm×k, Q ∈Rk×n) to
approximate X. A standard dropout technique is utilized when we train P and Q. Cavazza et al.
(2018) show that optimization with dropout is equivalent to solving min ˆ
X ∥X −ˆX∥2
F + λ∥ˆX∥2
∗, and
the closed-form solution is obtained by shrinking all singular values of X by the same magnitude of
µ, which depends on the data X and the choice of hyperparameter λ. See details in Appendix E.3."
DICHOTOMY OF THE MODELS,0.15384615384615385,"The next two approaches leverage techniques from (variational) autoencoder. We show that they also
effectively add variants of nuclear-norm regularizers although this may not be clear at the first glance."
DICHOTOMY OF THE MODELS,0.15548281505728315,"A3. Weighted Linear Autoencoder (WLAE). Consider the following (non-uniform) weighted ℓ2-
regularization (Bao et al., 2020):"
DICHOTOMY OF THE MODELS,0.15711947626841244,"∥X −XW1W2∥2
F + ∥W1Λ
1
2 ∥2
F + ∥Λ
1
2 W2∥2
F ,
(2)"
DICHOTOMY OF THE MODELS,0.15875613747954173,"where Λ is a hyperparameter diagonal matrix and W1, W2 denote the encoder and the decoder
network respectively. Bao et al. (2020) have shown a closed-form solution for Eq. (2) with a specific
of diagonal matrix Λ = dMat(λ1, λ2, · · · , λk) when the weight is non-descending: λ1 ≤λ2 ≤
· · · ≤λk. It remains unclear if a closed-form solution exists for an arbitrary weight order."
DICHOTOMY OF THE MODELS,0.16039279869067102,"A4. Variational Linear Autoencoders (VLAE). While not explicitly studied before, it is also natural to
consider linear simplification of Variational Autoencoders, such as Multi-VAE (Liang et al., 2018),
which has shown to exhibit strong performance for recommendation. To optimize VLAE, we need to
find the maximum likelihood estimation (MLE) for the probabilistic model"
DICHOTOMY OF THE MODELS,0.16202945990180032,"p(x | z) = N
 
Wz + µ, σ2I

and p(z | x) = N(V (x −µ), D)
(3)"
DICHOTOMY OF THE MODELS,0.16366612111292964,"where V , W denote the encoder and the decoder networks, σ is a parameter, and D is a diagonal
covariance matrix (for the data X ∈Rm×n). We have the following observation."
DICHOTOMY OF THE MODELS,0.16530278232405893,"Lemma 1. Consider optimizing the ELBO (Evidence Lower Bound) (Kingma & Welling, 2014) for
the above LVAE model (Eq. (3)). When µ = 0, this optimization problem is equivalent to minimizing"
DICHOTOMY OF THE MODELS,0.16693944353518822,"L = ∥X −XV TW T∥2
F + m∥
√"
DICHOTOMY OF THE MODELS,0.1685761047463175,"DW T ∥2
F + σ2||XV T∥2
F + g(D, σ)
(4)"
DICHOTOMY OF THE MODELS,0.1702127659574468,"where g(D, σ) = −σ2m
 
log |D| −tr(D) + k −n log(2πσ2)

."
DICHOTOMY OF THE MODELS,0.1718494271685761,Under review as a conference paper at ICLR 2022
DICHOTOMY OF THE MODELS,0.1734860883797054,"Here, we set µ = 0 for simplicity and following standard practices in recommendations. The
proof can be found in Appendix D.1. We may further “clear up” Eq. (4) and obtain the following
optimization problem (see Appendix D.3):"
DICHOTOMY OF THE MODELS,0.1751227495908347,"min
A∈Rn×k,B∈Rk×n ∥X −XAB∥2
F + ∥XA∥2
F + ∥ΛB∥2
F ,
(5)"
DICHOTOMY OF THE MODELS,0.176759410801964,"in which decision variables are A and B, and the hyperparameter is a diagonal matrix Λ ∈Rk×k."
DICHOTOMY OF THE MODELS,0.1783960720130933,"In Section 3, we show that solving Eq. (5) (A4) is equivalent to solving ∥X −W∥2
F + λ∥W∥ω,∗
subject to rank(W) ≤k, where ω consists of Λ’s diagonal values, sorted in non-descending order.
Furthermore, we characterize the optimal solution for ∥X −W∥2
F + λ∥W∥p
∗for any p ≥1. We
shall show that regardless the choice of p, the optimal solution uniformly shrinks all X’s singular
values by a constant magnitude µ (and to 0 if a singular value is already less than µ). Finally, for all
nuclear-norm-based approaches discussed above, the estimators always keep singular vectors of X
and shrink its singular values. Therefore, the solution space offered by nuclear-norm regularizers is
quite constrained, which limits their predictive power."
DICHOTOMY OF THE MODELS,0.18003273322422259,"Frobenius-norm regularizers. Most algorithms below were originally motivated by the design of
(denoising) autoencoders. It has been shown that they effectively add a Frobenius-norm regularizer."
DICHOTOMY OF THE MODELS,0.18166939443535188,"A5. EASE (Steck, 2019) aims to optimize minW ||X −XW||2
F + λ · ||W||2
F subject to the constraint
that diag(W) = 0. A closed-form solution exists for this problem (case 5 in Table 1)."
DICHOTOMY OF THE MODELS,0.18330605564648117,"A6. DLAE (Steck, 2020) adds a weighted Frobenius-norm regularizer so the objective becomes
minW ||X −XW||2
F + ||Λ1/2 · W||2
F , where Λ =
p
1−pdMat(diag(XT X)), dMat(·) denotes a
diagonal matrix, diag(X) is the vector on the diagonal of X. The closed-form solution also exists
(case 6 in Table 1)."
DICHOTOMY OF THE MODELS,0.18494271685761046,"A7. EDLAE (Steck, 2020) integrates weighted Frobenius norm in DLAE with EASE’s diagonal
constraint so its objective is the same as DLAE but it requires diag(W) = 0. A closed-form solution
exists for this problem (case 7 in Table 1). For the low-rank EDLAE (case 9 in Table 1), an ADMM
algorithm is used."
DICHOTOMY OF THE MODELS,0.18657937806873978,"A8. Tikhonov regularization/Low Rank Regression (LRR) (Jin et al., 2021). Let Vk be the k leading
right singular vectors of X. Jin et al. (2021) find an estimator that solves"
DICHOTOMY OF THE MODELS,0.18821603927986907,"W = arg
min
rank(W )≤k ∥X −XW∥2
F + ∥ΓW∥2
F ,
(6)"
DICHOTOMY OF THE MODELS,0.18985270049099837,"where Γ = Λ
1
2 V T
k and Λ = diag(λ′
1, · · · , λ′
k) is a hyperparameter. The closed-form solution is"
DICHOTOMY OF THE MODELS,0.19148936170212766,"W ∗= Vk · dMat(
σ2
1
σ2
1 + λ′
1
, . . . ,
σ2
k
σ2
k + λ′
k
)V T
k
(7)"
DICHOTOMY OF THE MODELS,0.19312602291325695,"We first remark that A5 and A6 produce full-rank estimators. A7 can produce either full-rank or
low-rank estimator (via ADMM) and has the best performance (among all approaches we discussed).
The estimator from A8 is low-rank and has a closed-form solution but it has to keep singular vectors
of X so its predictive power is also limited. Nevertheless, A8 is conceptually interesting because it
uses Frobenius-norm regularizers but its solution space cover the solution space offered in A4 (and
thus also A1-A4).
Proposition 1. For any regularized instances in the form of Eq. (5) with regularization parameter
Λ such that σi(X) ≥λ(k−i) for all i, there is a corresponding Tikhonov regularized instance with
Γ = Λ
1
2 V T
k that provides the same regularization effect."
DICHOTOMY OF THE MODELS,0.19476268412438624,"The proof is in Appendix D.2. A major implication of Proposition 1 is that we can focus on designing
Frobenius-norm regularizers because it also gets the value from using nuclear-norm regularizers."
DICHOTOMY OF THE MODELS,0.19639934533551553,"As mentioned earlier, no closed-form solution exists for the low-rank EDLAE (case 9 in Table 1)
so existing approaches rely on ADMM or Alternating Least Square algorithms (Steck, 2020). In
Section 4, we will introduce two low-rank models based on Frobenius-norm regularizers that have
both competitive performance and closed-form solutions."
DICHOTOMY OF THE MODELS,0.19803600654664485,Under review as a conference paper at ICLR 2022
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.19967266775777415,"3
RIGIDITY OF NUCLEAR-NORM REGULARIZERS"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.20130932896890344,"This section presents two results. (i) Eq. (5) (A4) is equivalent to a model with weighted nuclear-norm,
where the weights are diagonal values of Λ arranged in non-descending order. Because weighted
nuclear norms are non-differentiable in general so Eq. (5) compiles non-differentiable objectives into
differentiable ones, which are easier to optimize. Also, the auto-sorting property restricts Eq. (5)
from expressing an arbitrary weight sequence in ∥W∥ω,∗, which limits its predictive power. (ii) We
give a closed-form solution for models with regularizer ∥W∥p
∗for all p ≥1. The solutions share the
same structure so tuning p will not improve a model’s predictive power."
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.20294599018003273,Rigidity of closed-form solutions. We first analyze Eq. (5) (A4). We start with the problem:
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.20458265139116202,"min
P,Q ∥X−PQ∥2
F +∥Λ
1
2 Q∥2
F +∥P Λ
1
2 ||2
F , or equivalently, min
P,Q ∥X−PQ∥2
F +∥ΛQ∥2
F +∥P∥2
F , (8)"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.20621931260229132,"Note that when we let P ′ = XA∗and Q′ = B∗, where A∗and B∗are an optimal solution of Eq. (5),
the syntax of Eq. (8) matches with that of Eq. (5). This implies that the solution for Eq. (8) is a lower
bound of that for Eq. (5). These two solutions coincide only when the columns in the optimal P ∗in
Eq. (8) are spanned by the columns of X."
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.2078559738134206,"We next find a closed-form solution (P ∗, Q∗) for Eq. (8), and show that indeed the column space of
P ∗is in the column space of X."
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.20949263502454993,"Proposition 2. Let f : Rm×n →R+ be any cost function. Let P ∈Rm×k and Q ∈Rk×n. Let Λ ∈
Rk×k be a diagonal matrix such that λi = Λii ≥0 (i ∈[k]). Let also ω = (λπ(1), λπ(2), . . . , λπ(k)),
where π is a permutation on [k] such that λπ(1) ≤λπ(2) ≤· · · ≤λπ(k). The following two
optimization problems have the same optimal values"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.21112929623567922,"OPT1 :
minP,Q
f(PQ) + ∥Λ
1
2 Q∥2
F + ∥PΛ
1
2 ∥2
F ."
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.2127659574468085,"OPT2 :
minW
f(W) + 2∥W∥ω,∗
subject to
rank(W) ≤k."
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.2144026186579378,"In addition, if (P ∗, Q∗) is an optimal solution for OPT1, then W ∗= P ∗Q∗is an optimal solution
for OPT2. If W ∗is an optimal solution for OPT2, then there exists an optimal solution (P ∗, Q∗)
for OPT1 such that W ∗= P ∗Q∗."
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.2160392798690671,"See Appendix D.4 and Appendix D.5 for the full analysis. We note that diagonals of Λ do not have to
be sorted in ascending order as stated in (Bao et al., 2020) because any permutation of the diagonals
will be equivalent to OPT2. In addition, Proposition 2 is applicable to A3 & A4. f(·) in A3 & A4 is
the reconstruction error, in which case closed-form solutions exist:
Corollary 1. Let X ∈Rm×n (m ≥n) be a full rank matrix. Let Λ be a diagonal matrix with
Λii ≥0 for all i. Let P ∈Rm×k and Q ∈Rk×n. Consider the optimization problems"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.2176759410801964,"min
P,Q ∥X −PQ∥2
F + ∥Λ
1
2 Q∥2
F + ∥PΛ
1
2 ∥2
F
(9)"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.2193126022913257,"min
A,B ∥X −XAB∥2
F + ∥ΛB∥2
F + ∥XA∥2
F .
(10)"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.220949263502455,"Let the SVD of X be UΣV T, and let Σk ∈Rk×k be a matrix comprising the k largest singular
values of X, and Uk and Vk be the corresponding singular vectors. Let λ(1) ≥λ(2) ≥· · · ≥λ(k) be
the sorted sequence of the diagonal values from Λ. Eq. (9) has a closed-form solution:"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.2225859247135843,"P ∗= Ukdiag(
q"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.22422258592471359,"(σ1 −λ(k))+), . . . ,
q"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.22585924713584288,"(σk −λ(1))+)Ω,"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.22749590834697217,"Q∗= ΩTdiag(
q"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.22913256955810146,"(σ1 −λ(k))+), . . . ,
q"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.23076923076923078,"(σk −λ(1))+)V T
k ,
(11)"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.23240589198036007,"where Ωis a unitary matrix that corresponds to the permutation π such that λπ(1) ≤· · · ≤λπ(k).
In addition, Eq. (10) has a closed-form solution: A∗= X†P ∗Λ
1
2 and B∗= Λ−1"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.23404255319148937,"2 Q∗, where X† is
the pseudo-inverse of X."
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.23567921440261866,"Rigidity of ∥W ∥p
∗In (Cavazza et al., 2018), it was observed that when we use a neural net X = PQ
(with learnable parameters being P and Q) to train a model and a standard dropout is used, the"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.23731587561374795,Under review as a conference paper at ICLR 2022
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.23895253682487724,"objective is equivalent to solving minW ∥X −W∥2
F + λ∥W∥2
∗. While the regularizer ∥W∥2
∗deviates
from the standard one ∥W∥∗, the optimal solution here is W = USµ(Σ)V T, where UΣV T is SVD
of X, and Sµ(Σ) is a diagonal matrix such that its (i, i)-th element is (Σi,i −µ)+, in which µ depends
on the data X and λ. In other words, the optimal solutions for regularizers ∥W∥2
∗and ∥W∥∗are
strikingly similar. Thus, we are interested in how regularizers with different exponents are connected.
Our main observation is that for any regularizer ∥W∥p
∗(p ≥1), the optimal solution has the same
structure.
Lemma 2. Let X ∈Rm×n, where m ≥n. Let the d leading SVDs of X be Ud and Vd respectively.
Let σ1, . . . , σn be the singular values of X. Consider the optimization problem:"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.24058919803600654,"min
W
1
2∥X −W∥2
F + λ∥W∥p
∗.
(12)"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.24222585924713586,"Let µk = 1 k
P"
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.24386252045826515,"i≤k σi(X), η(µk) be the positive root of the function z + λkzp−1 −kµk and d be the
largest value such that σd(X) −λ(η(µd))p−1 ≥0. Let µ = λ(η(µd))p−1. The optimal solution of
W is Ud · dMat(σ1 −µ, σ2 −µ, . . . , σd −µ) · V T
d ."
RIGIDITY OF NUCLEAR-NORM REGULARIZERS,0.24549918166939444,"Lemma 2 implies that regularizers ∥W∥p
∗with different p’s differ in how the shrinkage variable µ is
obtained. Observing that µ is a hyperparameter that requires extensive tuning, all regularizers ∥W∥p
∗
provide the same learning power. As noted earlier, µ is a function of the data X unless p = 1; λ
needs to be rescaled when X is scaled by a constant factor unless p = 2. This implies it could be
easier to tune µ when p = 1, 2, and explains why only p = 1, 2 have been extensively considered."
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.24713584288052373,"4
LOW-RANK FROBENIUS-NORM REGULARIZATIONS"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.24877250409165302,This section presents two closed-form low-rank estimators with competitive performance.
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.25040916530278234,Approximate low-rank DLAE and EDLAE. Recall that (full-rank) DLAE solves:
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2520458265139116,"min
W ||X −XW||2
F + ||Λ
1
2 W||2
F
s.t.
Λ =
p
1 −pdMat(diag(XT X))
(13)"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.25368248772504093,"and EDLAE with the additional diag(W) = 0 constraint. While both the regularizations use nuclear-
norm and the full-rank DLAE/EDLAE solutions all have closed-form solutions, such solution is
unknown for the low-rank DLAE and EDLAE, which still require ADMM (Steck, 2020). The
closed-form solutions will help both better understand and compare these models, and determine the
hyperparameters such as learning rates, which is usually difficult for the ADMM type solutions."
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2553191489361702,"Low-rank DLAE is a special case of Tikhonov regularization (Eq. (6)) so it has a closed-form solution,
namely LR-DLAE (Jin et al., 2021). See case 10 in Table 1. However, for EDLAE, it enforces
the zero-diagonal constraint, which makes the exact solution difficult to express. Recall that that
low-rank EDLAE aims to solve :"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2569558101472995,"W = arg
min
rank(W )≤k ||X −X(W −dMat(diag(W))||2
F + ||Λ
1
2 (W −dMat(diag(W))||2
F (14)"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.25859247135842883,"Here, we present an approximate low-rank closed-form solution of Eq. (14) by decomposing the
optimization problem into two subproblems, which is similar to (Jin et al., 2021). For detailed
analysis and process, please refer to Appendix B. We first consider the full-rank closed-form solution
for EDLAE (Eq. (13) with zero-diagonal constraint), which is given by:"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2602291325695581,"W ∗= I −C · dMat(1 ⊘diag(C)), where C = (XT X + Λ)−1"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2618657937806874,"Then, we consider two (closed-form) approaches to produce low-rank matrix approximation of W ∗:"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2635024549918167,"Method 1 (LR-EDLAE-1): Selecting c
W to best approximate the performance of W ∗without
the zero-diagonal constraint:"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.265139116202946,"c
W = arg
min
rank(W )≤k ||XW ∗−XW||2
F"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.26677577741407527,"= arg
min
rank(W )≤k ||XW ∗−XW||2
F + ||Λ
1
2 (W ∗−W)||2
F
(15)"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2684124386252046,"where X =
 X
Λ
1
2"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2700490998363339,"
. Noting, in the full-rank problem Eq. (13), it forces the diagonal of derived matrix"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.27168576104746317,"(W ∗) to be zero. Here, we relax the zero-diagonal constraint. This relaxation corresponds to lower"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2733224222585925,Under review as a conference paper at ICLR 2022
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.27495908346972175,"bounding an approximate low-rank EDLAE objective function when rank k is sufficiently large.
(Details in Appendix B). The closed-from solution of Eq. (15) is given by:"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.2765957446808511,"c
W = W ∗(QkQT
k )
(16)"
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.27823240589198034,"where Qk takes the first k rows of matrix Q: XW ∗SVD
= PΣQT ."
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.27986906710310966,"Method 2 (LR-EDLAE-2): SVD approximation of W ∗: The alternative solution is to simply
perform SVD: W ∗SVD
= UΣV T , and which gives a low-rank estimation of W ∗: c
W = UkΣkV T
k .
This solution corresponds to upper bounding the minimization objective in Eq. (15) (Details in
Appendix B)."
LOW-RANK FROBENIUS-NORM REGULARIZATIONS,0.281505728314239,"Both approaches relax the hard zoer-diagonal constraint on c
W. In the next section, the experimental
results show both approaches can provide comparable or better performance compared with the
ADMM solution, and also very close to the full rank EDLAE solution."
EXPERIMENTAL RESULTS,0.28314238952536824,"5
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.28477905073649756,"This section experiments different regularizations for linear recommendation models, with a goal to
validate the efficacy of various regularizations (all can be categorized under nuclear or Frobenius
norms) together with their closed-form solutions. We answer three questions: Q1. How do the
closed-form solutions of low-rank Frobenius-norm perform compared with the ADMM solutions
(Section 4) and how does the weighted nuclear-norm regularizer for matrix factorization (Proposition 2
and Corollary 1) perform? Q2. What is the tradeoff between the number of factors (rank k) and
the recommendation accuracy? Q3. How does the ordering of weights from small to large (non-
descending) for adjusting the singular values (Corollary 1) affect the recommendation performance?"
EXPERIMENTAL RESULTS,0.2864157119476268,"Experimental Setup: We use three commonly used datasets for recommendation studies: MovieLens
20 Mil. (ML-20M) (Harper & Konstan, 2015), Netflix Prize (Netflix) (Bennett et al., 2007), and the
Million Song Data (MSD)(Bertin-Mahieux et al., 2011). We obtained the datasets and all benchmarks
from authors of EASE (Steck, 2019), EDLAE (Steck, 2020), and Mult-VAE (Liang et al., 2018)."
EXPERIMENTAL RESULTS,0.28805237315875615,"Similar to the latest study in EASE (Steck, 2019), and EDLAE (Steck, 2020), we consider the follow-
ing state-of-the-art recommendation models: ALS (WMF) (Hu et al., 2008) for matrix factorization
approaches, SLIM (Ning & Karypis, 2011), EASE (Steck, 2019), and EDLAE (Steck, 2020) for
linear autoendoers, CDAE (Wu et al., 2016), Mult-DAE and Mult-VAE (Liang et al., 2018) for
deep learning models. The experiment settings for these baseline are the same as (Liang et al.,
2018; Steck, 2019; 2020). Also we follow their practice (Liang et al., 2018; Steck, 2019; 2020) for
the strong generalization by splitting the users into training, validation and tests group, and report
performance metrics Recall@20, Recall@50 and nDCG@100. Finally, we note that our code are
openly available (see Appendix E.1)."
EXPERIMENTAL RESULTS,0.2896890343698854,"Q1: Low-Rank Frobenius-Norm and (Weighted) Nuclear-Norm Regularization: We evaluate the
low-rank Frobenius-norm-based regularization and the nuclear-norm-based regularization (Eq. (1))
for the matrix factorization. Here EDLAE-ADMM, LRR, LR-DLAE, LR-EDLAE-1, LR-EDLAE-2,
MF dropout and VLAE are listed in Table 1. To determine the non-descending order of weights λi for
the closed-form solution in Eq. (11), we follow the practice in weighted nuclear-norm regularization
in (Gu et al., 2014) as well as the optimized pPCA weight (Lucas et al., 2019b). Let λi = C"
EXPERIMENTAL RESULTS,0.29132569558101473,"σi where
C is a hyperparameter, and we perform grid-search to find the optimal one."
EXPERIMENTAL RESULTS,0.29296235679214405,"Table 2 shows that the weighted nuclear-norm regularization (VLAE) based matrix factorization
performs worse than the constant weighted version (Regularized PCA). The latter demonstrates strong
performance comparing against the WFM/ALS (one of the most popular implicit matrix factorization
algorithms). The closed-form solutions (LR-DLAE, LR-EDLAE-1 and LR-EDLAE-2) all perform
comparable with the ADMM-based low-rank solution and the full-rank DLAE and EDLAE solutions."
EXPERIMENTAL RESULTS,0.2945990180032733,"Q2: nDCG vs Rank k for low-rank Frobenius norm: In this experiment, we focus on evaluating
the recommendation accuracy (using nDCG) against the rank k. Specifically, we vary k from around
1K to around 10K, and we tune and compare four different methods, including EDLAE-ADMM,
LR-DLAE, LR-EDLAE-1, and LR-EDLAE-2. We have the following observations from Fig. 1: 1) As
k increases, the recommendation accuracy also increases in general; however, most of them reach a
plateau around similar K, and for different datasets, the saturating point varies. 2) LR-DLAE performs"
EXPERIMENTAL RESULTS,0.29623567921440264,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.2978723404255319,"Table 2: The performance comparison between different regularizations. We highlight the best results
in bold and underline the 2nd best results for each metric. See Table 1 for the notation."
EXPERIMENTAL RESULTS,0.2995090016366612,"Model
ML-20M
Netflix
MSD
Recall@20
Recall@50
nDCG@100
Recall@20
Recall@50
nDCG@100
Recall@20
Recall@50
nDCG@100"
EXPERIMENTAL RESULTS,0.3011456628477905,"Frobenius
Norm"
EXPERIMENTAL RESULTS,0.3027823240589198,"full
rank"
EXPERIMENTAL RESULTS,0.3044189852700491,"EASE
0.391
0.521
0.420
0.362
0.445
0.393
0.333
0.428
0.389
DLAE
0.392
0.527
0.424
0.362
0.446
0.395
0.329
0.426
0.387
EDLAE
0.393
0.523
0.424
0.366
0.449
0.398
0.334
0.429
0.392"
EXPERIMENTAL RESULTS,0.3060556464811784,"low
rank"
EXPERIMENTAL RESULTS,0.3076923076923077,"EDLAE-ADMM
0.392
0.524
0.424
0.365
0.448
0.396
0.330
0.424
0.386
LRR
0.376
0.511
0.408
0.348
0.431
0.380
0.248
0.335
0.301
LR DLAE
0.392
0.527
0.424
0.362
0.445
0.395
0.306
0.403
0.363
LR-EDLAE-1
0.392
0.523
0.424
0.365
0.449
0.398
0.327
0.423
0.384
LR-EDLAE-2
0.392
0.523
0.424
0.365
0.449
0.398
0.325
0.421
0.382"
EXPERIMENTAL RESULTS,0.309328968903437,"Nuclear
Norm"
EXPERIMENTAL RESULTS,0.3109656301145663,"MF dropout
0.367
0.501
0.393
0.334
0.418
0.365
0.270
0.367
0.328
Regularized PCA
0.364
0.501
0.392
0.331
0.417
0.365
0.229
0.313
0.279
VLAE
0.348
0.474
0.378
0.325
0.405
0.357
0.205
0.254
0.286"
EXPERIMENTAL RESULTS,0.31260229132569556,Baseline
EXPERIMENTAL RESULTS,0.3142389525368249,"WMF/ALS
0.360
0.498
0.386
0.316
0.404
0.351
0.211
0.312
0.257
SLIM
0.370
0.495
0.401
0.347
0.428
0.379
no results in (Ning & Karypis, 2011)
CDAE
0.391
0.523
0.418
0.343
0.428
0.376
0.188
0.283
0.237
MULT-DAE
0.387
0.524
0.419
0.344
0.438
0.380
0.266
0.363
0.313
MULT-VAE
0.395
0.537
0.426
0.351
0.444
0.386
0.266
0.364
0.316
# items
20108
17769
41140
# users
136677
463435
571353
# interactions
10million
57million
34million"
EXPERIMENTAL RESULTS,0.3158756137479542,"2000
4000
6000
8000
k 0.410 0.412 0.414 0.416 0.418 0.420 0.422 0.424"
EXPERIMENTAL RESULTS,0.31751227495908346,nDCG@100
EXPERIMENTAL RESULTS,0.3191489361702128,"ML
20M"
EXPERIMENTAL RESULTS,0.32078559738134205,"EDLAE-ADMM
LR-EDLAE-2
LR-DLAE
LR-EDLAE-1 (a)"
EXPERIMENTAL RESULTS,0.32242225859247137,"2000
4000
6000
8000
k 0.375 0.380 0.385 0.390 0.395 0.400"
EXPERIMENTAL RESULTS,0.32405891980360063,nDCG@100
EXPERIMENTAL RESULTS,0.32569558101472995,Netflix
EXPERIMENTAL RESULTS,0.32733224222585927,"EDLAE-ADMM
LR-EDLAE-2
LR-DLAE
LR-EDLAE-1 (b)"
EXPERIMENTAL RESULTS,0.32896890343698854,"2000
4000
6000
8000
10000
12000
k 0.28 0.30 0.32 0.34 0.36 0.38 0.40"
EXPERIMENTAL RESULTS,0.33060556464811786,nDCG@100 MSD
EXPERIMENTAL RESULTS,0.3322422258592471,"EDLAE-ADMM
LR-EDLAE-2
LR-DLAE
LR-EDLAE-1 (c)"
EXPERIMENTAL RESULTS,0.33387888707037644,Figure 1: Low-rank models nDCG@100 on test data for 3 datasets.
EXPERIMENTAL RESULTS,0.3355155482815057,"worse than EDLAE-based approaches in two out of three datasets. This partially demonstrates the
benefits of zero-diagonal constraint. 3) The closed-form solution of EDLAE performs comparable
or even slightly better than ADMM methods as K grows; but when K is relatively small, ADMM
method performs slightly better. But none-the-less, for most of the reasonable choices of k when low
rank approximates full rank, the closed-form solution performs comparable or better."
EXPERIMENTAL RESULTS,0.337152209492635,"Q3: Impact of Weight Ordering: Finally, we study how the ordering of weights from small to
large (non-descending) for adjusting the singular values ( Corollary 1) affects the recommendation
performance using matrix factorization (closed-form solution in Eq. (11)). Our results are in Table 3.
Here, we obtain the searched optimal weight parameters from weighted Tikhonov regularization
(following the approach in (Jin et al., 2021)), and map it back to the parameters in the closed-form
solution ( Proposition 1). Then we sort the parameters in the non-descending order, and then report
their results in the second row of Table 3. We can see that the recommendation performance becomes
significant worst. This help confirm our conjecture that the strict ordering of weight on matrix
factorization and other regularizations can be an inherent limitation for those approaches."
"CONCLUSION AND DISCUSSION
THIS WORK PROVIDES A COMPLETE ANALYSIS ON THE RECENTLY PROPOSED LINEAR MODELS FOR RECOMMENDATION",0.33878887070376434,"6
CONCLUSION AND DISCUSSION
This work provides a complete analysis on the recently proposed linear models for recommendation
systems. Despite that models leverage different deep learning techniques, they achieve similar perfor-
mance. We find that this is not coincident: all the models add either a nuclear-norm-based (Lemma 1)
or a Frobenius-norm-based regularizer. The nuclear-norm-based approach results in estimators that
keep X’s singular vectors and shrink its singular values in a quite rigid way ( Proposition 2 and
Lemma 2), which limit their prediction power. The Frobenius-norm models are more expressive
( Proposition 1) and effective but their estimators are either full-rank or do not have closed-form solu-
tions. To get the best of both nuclear and Frobenius worlds, we propose two low-rank and closed-form
estimators ( Section 4) based on carefully generalizing Frobenius-norm-based regularizers. These
estimators have competitive performance against linear performance leaders, and thus concisely pack
all the benefits obtained by a recent long line of research and abstract out all the computation nuance."
"CONCLUSION AND DISCUSSION
THIS WORK PROVIDES A COMPLETE ANALYSIS ON THE RECENTLY PROPOSED LINEAR MODELS FOR RECOMMENDATION",0.3404255319148936,Table 3: Investigating the weight ordering of Matrix Factorization
"CONCLUSION AND DISCUSSION
THIS WORK PROVIDES A COMPLETE ANALYSIS ON THE RECENTLY PROPOSED LINEAR MODELS FOR RECOMMENDATION",0.34206219312602293,"Model
ML-20M
Netflix
MSD
Recall@20
Recall@50
nDCG@100
Recall@20
Recall@50
nDCG@100
Recall@20
Recall@50
nDCG@100
MF/LRR weighted
0.3806
0.5175
0.4102
0.3484
0.4320
0.3797
0.2508
0.3390
0.3037
MF sorted
0.3017
0.4507
0.3361
0.2860
0.3801
0.3265
0.2288
0.3148
0.2802"
"CONCLUSION AND DISCUSSION
THIS WORK PROVIDES A COMPLETE ANALYSIS ON THE RECENTLY PROPOSED LINEAR MODELS FOR RECOMMENDATION",0.3436988543371522,Under review as a conference paper at ICLR 2022
REFERENCES,0.3453355155482815,REFERENCES
REFERENCES,0.3469721767594108,"Charu C. Aggarwal. Recommender Systems: The Textbook. Springer, 1st edition, 2016. ISBN
3319296574."
REFERENCES,0.3486088379705401,"Josh Alman and Virginia Vassilevska Williams. A refined laser method and faster matrix multipli-
cation. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pp.
522–539. SIAM, 2021."
REFERENCES,0.3502454991816694,"Xuchan Bao, James Lucas, Sushant Sachdeva, and Roger B. Grosse. Regularized linear autoencoders
recover the principal components, eventually. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.3518821603927987,"James Bennett, Charles Elkan, Bing Liu, Padhraic Smyth, and Domonkos Tikk. Kdd cup and
workshop 2007. 2007. doi: 10.1145/1345448.1345459. URL https://doi.org/10.1145/
1345448.1345459."
REFERENCES,0.353518821603928,"Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. The million song
dataset. 2011."
REFERENCES,0.35515548281505727,"Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster
semi-definite optimization. In Conference on Learning Theory, pp. 530–582. PMLR, 2016."
REFERENCES,0.3567921440261866,"Emmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010."
REFERENCES,0.35842880523731585,"Jacopo Cavazza, Pietro Morerio, Benjamin Haeffele, Connor Lane, Vittorio Murino, and Rene Vidal.
Dropout as a low-rank regularizer for matrix factorization. In Proceedings of the Twenty-First
International Conference on Artificial Intelligence and Statistics. PMLR, 2018."
REFERENCES,0.36006546644844517,"Kun Chen, Hongbo Dong, and Kung-Sik Chan. Reduced rank regression via adaptive nuclear norm
penalization. Biometrika, 100(4):901–920, 2013."
REFERENCES,0.3617021276595745,"Evangelia Christakopoulou and George Karypis. Hoslim: Higher-order sparse linear method for
top-n recommender systems. In Advances in Knowledge Discovery and Data Mining, 2014."
REFERENCES,0.36333878887070375,"Maurizio Ferrari Dacrema, P. Cremonesi, and D. Jannach. Are we really making much progress? a
worrying analysis of recent neural recommendation approaches. In RecSys’19, 2019."
REFERENCES,0.3649754500818331,"Mukund Deshpande and George Karypis. Item-based top-n recommendation algorithms. ACM Trans.
Inf. Syst., 2004."
REFERENCES,0.36661211129296234,"C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika,
1(3):211–218, 1936."
REFERENCES,0.36824877250409166,"Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization
with application to image denoising. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 2862–2869, 2014."
REFERENCES,0.3698854337152209,"F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM Trans.
Interact. Intell. Syst., 2015. doi: 10.1145/2827872."
REFERENCES,0.37152209492635024,"Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering for implicit feedback datasets. In ICDM’08,
2008."
REFERENCES,0.37315875613747956,"José Pedro Iglesias, Carl Olsson, and Marcus Valtonen Örnhag. Accurate optimization of weighted
nuclear norm for non-rigid structure from motion. arXiv preprint arXiv:2003.10281, 2020."
REFERENCES,0.37479541734860883,"Ruoming Jin, Dong Li, Jing Gao, Zhi Liu, Li Chen, and Yang Zhou. Towards a better understanding
of linear recommendation models. In KDD’21, 2021. URL https://arxiv.org/abs/
2105.12937."
REFERENCES,0.37643207855973815,"Santosh Kabbur, Xia Ning, and George Karypis. Fism: Factored item similarity models for top-n
recommender systems. KDD ’13, 2013."
REFERENCES,0.3780687397708674,Under review as a conference paper at ICLR 2022
REFERENCES,0.37970540098199673,"Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014."
REFERENCES,0.381342062193126,"Yehuda Koren. Factorization meets the neighborhood: A multifaceted collaborative filtering model.
In KDD’08, 2008."
REFERENCES,0.3829787234042553,"Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30–37, August 2009."
REFERENCES,0.38461538461538464,"Daniel Kunin, Jonathan Bloom, Aleksandrina Goeva, and Cotton Seed. Loss landscapes of regularized
linear autoencoders. In Proceedings of the 36th International Conference on Machine Learning,
Proceedings of Machine Learning Research, pp. 3560–3569. PMLR, 09–15 Jun 2019."
REFERENCES,0.3862520458265139,"Xiaopeng Li and James She. Collaborative variational autoencoder for recommender systems. In
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD ’17, pp. 305–314, 2017."
REFERENCES,0.3878887070376432,"Dawen Liang, R. G. Krishnan, M. D. Hoffman, and T. Jebara. Variational autoencoders for collabora-
tive filtering. In WWW’18, 2018."
REFERENCES,0.3895253682487725,"James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don't blame the elbo!
a linear vae perspective on posterior collapse. In Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019a."
REFERENCES,0.3911620294599018,"James Lucas, George Tucker, Roger B. Grosse, and Mohammad Norouzi. Don’t blame the elbo!
A linear VAE perspective on posterior collapse. In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
9403–9413, 2019b. URL https://proceedings.neurips.cc/paper/2019/hash/
7e3315fe390974fcf25e44a9445bd821-Abstract.html."
REFERENCES,0.39279869067103107,"Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise and
high-dimensional scaling. The Annals of Statistics, pp. 1069–1097, 2011."
REFERENCES,0.3944353518821604,"Xia Ning and George Karypis. Slim: Sparse linear methods for top-n recommender systems. ICDM
’11, 2011."
REFERENCES,0.3960720130932897,"Steffen Rendle, Li Zhang, and Yehuda Koren. On the difficulty of evaluating baselines: A study on
recommender systems, 2019."
REFERENCES,0.397708674304419,"Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Darius Braziunas. On the effectiveness of
linear models for one-class collaborative filtering. AAAI’16, 2016."
REFERENCES,0.3993453355155483,"Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh, and Sergey I. Nikolenko. Recvae:
A new variational autoencoder for top-n recommendations with implicit feedback. In Proceedings
of the 13th International Conference on Web Search and Data Mining, WSDM ’20, pp. 528–536,
2020."
REFERENCES,0.40098199672667756,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929–1958, 2014."
REFERENCES,0.4026186579378069,"Andreas Stathopoulos and James R McCombs. Primme: Preconditioned iterative multimethod
eigensolver—methods and software description. ACM Transactions on Mathematical Software
(TOMS), 37(2):1–30, 2010."
REFERENCES,0.40425531914893614,"Harald Steck. Embarrassingly shallow autoencoders for sparse data. WWW’19, 2019."
REFERENCES,0.40589198036006546,"Harald Steck. Autoencoders that don’t overfit towards the identity. In NIPS, 2020."
REFERENCES,0.4075286415711948,"Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267–288, 1996."
REFERENCES,0.40916530278232405,Under review as a conference paper at ICLR 2022
REFERENCES,0.41080196399345337,"Michael E. Tipping and Chris M. Bishop. Probabilistic principal component analysis. JOURNAL OF
THE ROYAL STATISTICAL SOCIETY, SERIES B, 61(3):611–622, 1999a."
REFERENCES,0.41243862520458263,"Michael E. Tipping and Christopher M. Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999b. doi:
https://doi.org/10.1111/1467-9868.00196. URL https://rss.onlinelibrary.wiley.
com/doi/abs/10.1111/1467-9868.00196."
REFERENCES,0.41407528641571195,"Madeleine Udell, Corinne Horn, Reza Zadeh, and Stephen Boyd. Generalized low rank models.
Found. Trends Mach. Learn., 9(1):1–118, June 2016."
REFERENCES,0.4157119476268412,"Wessel N. van Wieringen. Lecture notes on ridge regression, 2020."
REFERENCES,0.41734860883797054,"Lingfei Wu, Eloy Romero, and Andreas Stathopoulos. Primme_svds: A high-performance precondi-
tioned svd solver for accurate large-scale computations. SIAM Journal on Scientific Computing, 39
(5):S248–S271, 2017."
REFERENCES,0.41898527004909986,"Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester. Collaborative denoising auto-
encoders for top-n recommender systems. WSDM ’16, 2016."
REFERENCES,0.4206219312602291,"Man-Chung Yue. A matrix generalization of the hardy-littlewood-p\’olya rearrangement inequality
and its applications. arXiv preprint arXiv:2006.08144, 2020."
REFERENCES,0.42225859247135844,"Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep learning based recommender system: A survey
and new perspectives. ACM Comput. Surv., 2019."
REFERENCES,0.4238952536824877,"Shuai Zheng, Chris Ding, and Feiping Nie. Regularized singular value decomposition and application
to recommender system, 2018."
REFERENCES,0.425531914893617,Under review as a conference paper at ICLR 2022
REFERENCES,0.42716857610474634,"A
RELATED WORK
There have been extensive researches on recommendation (Aggarwal, 2016). Besides the basic
user-based and item-based collaborative filtering (Deshpande & Karypis, 2004), the full-rank lin-
ear autoencoder approaches include SLIM (Ning & Karypis, 2011), HOLISM (Christakopoulou
& Karypis, 2014), EASE (Steck, 2019), DLAE (Steck, 2020), whereas low-rank approaches in-
clude (Kabbur et al., 2013; Sedhain et al., 2016; Steck, 2020). All the customized recommendation
models have been enforcing zero diagonal constraints for generalization purpose, whereas we show an
approximate closed-form solution for a two-term Tikhonov regularization without the zero diagonal
constraint can be as effective as these models."
REFERENCES,0.4288052373158756,"Matrix factorization has been been widely studied in practice, partially due to Netflix competition (Ko-
ren et al., 2009). Methods like SVD++ (Koren, 2008) and implicit Alternating Least Square (ALS)
method (Hu et al., 2008) (also weighted matrix factorization) have been very influential. Jin et al.
(2021) shows the relationship between linear autoencoders and matrix factorization, and pointed
out a potential advantage of linear autoencoders. In this work, we take a step further to reveal a
deeper relationship between Tikhonov regularized linear autoencoders and a few other regularizations
including matrix factorization, and show the potential limitation of the class of regularization. We also
utilize the variational linear autoencoders (VLAE) to study how the deep VAE based recommendation
approaches (Li & She, 2017; Liang et al., 2018; Shenbin et al., 2020) relate to linear autoencoders
and matrix factorization."
REFERENCES,0.43044189852700493,"Outside recommendation, there have been a few recent studies on regularization landscapes of linear
(variational) autoencoders (Kunin et al., 2019; Bao et al., 2020; Lucas et al., 2019a). They do not
provide the general weighted ℓ2 regularization and thus did not find the inherent limitation on the
regularization (for MF). Our VLAE inspired regularization is also never studied before."
REFERENCES,0.4320785597381342,"Nuclear-norm regularizers can recover low-rank matrices in the vector regression setting (Negahban
& Wainwright, 2011). Its weighted generalization can be applied in the area of image processing (Gu
et al., 2014). Because weighted nuclear-norm is usually not convex or differentiable, finding optimal
solutions is difficult except for a few special cases (Chen et al., 2013)."
REFERENCES,0.4337152209492635,"B
APPROXIMATE LOW RANK EDLAE"
REFERENCES,0.4353518821603928,"B.1
LOW-RANK CLOSED-FORM SOLUTION FOR DLAE"
REFERENCES,0.4369885433715221,Recall that full-rank DLAE problem:
REFERENCES,0.4386252045826514,"min
W ||X −XW||2
F + ||Λ
1
2 W||2
F
s.t.
Λ =
p
1 −pdMat(diag(XT X))
(17)"
REFERENCES,0.4402618657937807,And now consider the following basic low-rank DLAE model:
REFERENCES,0.44189852700491,"arg
min
rank(W )≤k ||X −XW||2
F + ||Λ
1
2 W||2
F
s.t.
Λ =
p
1 −pdMat(diag(XT X))
(18)"
REFERENCES,0.44353518821603927,"We leverage the strategy used in (Jin et al., 2021) to derive the low-rank closed-form solution. We
formalize the low-rank problem as a standard regression problem:"
REFERENCES,0.4451718494271686,"Y =

X
0 "
REFERENCES,0.44680851063829785,"X =
 X
Λ
1
2"
REFERENCES,0.44844517184942717,"
(19)"
REFERENCES,0.4500818330605565,"Equiped with this, the above regression problem can be formulated as:"
REFERENCES,0.45171849427168576,"min
rank(W )≤k||Y −XW||2
F"
REFERENCES,0.4533551554828151,"=
min
rank(W )≤k||Y −XW ∗||2
F + ||XW ∗−XW||2
F ,"
REFERENCES,0.45499181669394434,"where W ∗= arg min ||Y −XW ∗||2
F , is the optimal solution for the full-rank regression problem
Eq. (17). The primary loss function ||Y −XW||2
F then can be decomposed into two parts: ||Y −
XW ∗||2
F (full-rank regression problem), and ||XW ∗−XW||2
F (low-rank approximation problem)."
REFERENCES,0.45662847790507366,Under review as a conference paper at ICLR 2022
REFERENCES,0.4582651391162029,"Since W ∗is the optimum, the vector ( Y −XW ∗) is orthogonal to XW ∗−XW = X(W ∗−W)
(van Wieringen, 2020), the above equation holds."
REFERENCES,0.45990180032733224,Thus the primary low-rank regression problem Eq. (18) can be broken into two subproblems:
REFERENCES,0.46153846153846156,(Subproblem 1:) full-rank closed-from solution:
REFERENCES,0.46317512274959083,"W ∗= arg min ||Y −XW ∗||2
F
= (XT X + Λ)−1XT X"
REFERENCES,0.46481178396072015,(Subproblem 2:) low-rank approximation solution:
REFERENCES,0.4664484451718494,"c
W = arg
min
rank(W )≤k ||XW ∗−XW||2
F"
REFERENCES,0.46808510638297873,"= arg
min
rank(W )≤k ||XW ∗−XW||2
F + ||Λ
1
2 (W ∗−W)||2
F"
REFERENCES,0.469721767594108,"Denote Y
∗= XW ∗, the best rank k approximation of Y
∗in Frobenius norm can be derived from
SVD (Eckart & Young, 1936):"
REFERENCES,0.4713584288052373,"Y
∗SVD
= PΣQT"
REFERENCES,0.47299509001636664,"Y
∗(k) = PkΣkQT
k
where Qk takes the first k rows of matrix Q, as well as P and Σ. Since, Q are orthogonal matrices,
we notice that:"
REFERENCES,0.4746317512274959,"Y
∗(k) = PkΣkQT
k = PΣQT (QkQT
k )"
REFERENCES,0.4762684124386252,"= XW ∗(QkQT
k ) = XW"
REFERENCES,0.4779050736497545,"Imediately, we have
c
W = W ∗(QkQT
k )"
REFERENCES,0.4795417348608838,The complete low-rank closed-form solution for Eq. (18) is:
REFERENCES,0.48117839607201307,"c
W = (XT X + Λ)−1XT X(QkQT
k )"
REFERENCES,0.4828150572831424,"B.2
APPROXIMATE LOW-RANK CLOSED-FORM SOLUTION FOR EDLAE (LR-EDLAE-1)"
REFERENCES,0.4844517184942717,Recall that that low-rank EDLAE aims to solve
REFERENCES,0.486088379705401,"W = arg
min
rank(W )≤k ||X −X(W −dMat(diag(W))||2
F + ||Λ
1
2 (W −dMat(diag(W))||2
F (20)"
REFERENCES,0.4877250409165303,"When the rank k of the low-rank model is sufficiently large (for instance k ≈1000), its objective
function can be approximate as (Steck, 2020):"
REFERENCES,0.48936170212765956,"W = arg
min
rank(W )≤k ||X −XW||2
F + ||Λ
1
2 W||2
F"
REFERENCES,0.4909983633387889,"s.t.
diag(W) = 0
(21)"
REFERENCES,0.49263502454991814,"Note that for most of our experimental studies, the rank k in the low-rank model is typically fairly
large (over 1K), and thus the above approximation is reasonable. Compared to Eq. (18), low-rank
EDLAE imposes only an additional zero-diagonal constraint. Our goal is to mimic the strategy
developed in Appendix B.1 to build a two-staged algorithm to produce a low-rank approximation.
We have the following key Lemma.
Lemma 3. Let W ∗be a (full rank) solution to"
REFERENCES,0.49427168576104746,"min
W ∗
∥Y −XW ∗∥2
F"
REFERENCES,0.4959083469721768,diag(W ∗) = 0.
REFERENCES,0.49754500818330605,Under review as a conference paper at ICLR 2022
REFERENCES,0.49918166939443537,"where,"
REFERENCES,0.5008183306055647,"Y =

X
0 "
REFERENCES,0.502454991816694,"X =
 X
Λ
1
2 "
REFERENCES,0.5040916530278232,Let W be any matrix such that diag(W) = 0. We have
REFERENCES,0.5057283142389526,"⟨Y −XW ∗, XW ∗−XW⟩= 0."
REFERENCES,0.5073649754500819,"Proof. First, observe that W ∗can be found by solving a sequence of independent linear regressions,
each of which correpsonds to a column of Y . Let X(−1) be the matrix that removes the i-th column
of X. Then the i-th column of W ∗, namely W ∗
i can be found using two steps:"
REFERENCES,0.5090016366612111,"• Step 1. ˆWi = (X
T
(−i)X(−1))−1X
T
(−i)Y ."
REFERENCES,0.5106382978723404,"• Step 2. Construct W ∗
i by inserting a zero between the (i −1)-st and i-th entries of ˆWi."
REFERENCES,0.5122749590834698,"This is because enforcing W ∗
i,i = 0 is the same as removing the i-th column in X to fit Yi."
REFERENCES,0.513911620294599,"Next, we move to show that for any i,"
REFERENCES,0.5155482815057283,"⟨Y i −XW ∗
i , XW ∗
i −XWi⟩= 0."
REFERENCES,0.5171849427168577,"Observing that W ∗
i is the optimal solution for ∥Y i −XW ∗
i ∥2
2, Y i −XW ∗
i is orthogonal to the
column space of X(−i). On the other hand, XW ∗
i −XWi = X(W ∗
i −Wi) and (W ∗
i −Wi)i = 0.
This implies that XW ∗
i −XWi is in the column space of X(−i). Therefore, Y i −XW ∗
i is orthogonal
to XW ∗
i −XWi, which directly implies the lemma."
REFERENCES,0.5188216039279869,"Using Lemma 3, we see that"
REFERENCES,0.5204582651391162,"min
rank(W )≤k ||Y −XW||2
F"
REFERENCES,0.5220949263502455,"s.t.
diag(W) = 0"
REFERENCES,0.5237315875613748,"=
min
rank(W )≤k ||Y −XW ∗||2
F + ||XW ∗−XW||2
F ,"
REFERENCES,0.5253682487725041,"s.t.
diag(W ∗) = 0
diag(W) = 0"
REFERENCES,0.5270049099836334,"≥
min
rank(W )≤k ||Y −XW ∗||2
F + ||XW ∗−XW||2
F ,"
REFERENCES,0.5286415711947627,"s.t.
diag(W ∗) = 0 (22)"
REFERENCES,0.530278232405892,"The last inequality holds because one constraint diag(W) = 0 is removed so the objective will only
improve. With the above analysis, we are ready to describe our algorithm."
REFERENCES,0.5319148936170213,(Subproblem 1:) full-rank closed-from solution:
REFERENCES,0.5335515548281505,"W ∗= arg min ||Y −XW ∗||2
F
= arg min
W ||X −XW ∗||2
F + ||Λ
1
2 W ∗||2
F"
REFERENCES,0.5351882160392799,"s.t.
diag(W ∗) = 0 (23)"
REFERENCES,0.5368248772504092,The closed-form solution is:
REFERENCES,0.5384615384615384,C = (XT X + Λ)−1
REFERENCES,0.5400981996726678,W ∗= I −C · dMat(1 ⊘diag(C))
REFERENCES,0.5417348608837971,Under review as a conference paper at ICLR 2022
REFERENCES,0.5433715220949263,"Table 4: Time complexity analysis for the models listed in Table 1. X ∈Rm×n. Assume m > n.
And we treat XT X as precomputed."
REFERENCES,0.5450081833060556,"Models
Complexity Bounding Operation
Complexity
1. Regularized PCA
Singular Value Decomposition
O(kn2 + knm)
2. MF dropout
Eigen Decomposition
O(m2n + mn2)
4. VLAE
k leading Eigen Decomposition
O(km2 + kn2)
5. EASE
6. DLAE
7. EDLAE
Multiplication
O(n2.373)
9. EDLAE-ADMM
ADMM
O(t22.373)
3. WLAE
8. LRR
10. LR-DLAE
11. LR-EDLAE-1
12. LR-EDLAE-2
k leading Eigen Decomposition
O(kn2 + n2.373)"
REFERENCES,0.546644844517185,(Subproblem 2:) low-rank matrix approximation:
REFERENCES,0.5482815057283142,"c
W = arg
min
rank(W )≤k ||XW ∗−XW||2
F"
REFERENCES,0.5499181669394435,"= arg
min
rank(W )≤k ||XW ∗−XW||2
F + ||Λ
1
2 (W ∗−W)||2
F
(24)"
REFERENCES,0.5515548281505729,"Note, in the full-rank subproblem Eq. (23), it forces the diagonal of derived matrix (W ∗) to be zero.
Here, we relax the zero-diagonal constraint - the diagonal of low rank approximate matrix c
W doesn’t
have to be strictly zero."
REFERENCES,0.5531914893617021,"Given this, the closed-from solution (LR-EDLAE-1) of Eq. (24) is given by:"
REFERENCES,0.5548281505728314,"c
W = W ∗(QkQT
k )"
REFERENCES,0.5564648117839607,"where Qk comes from: XW ∗SVD
= PΣQT ."
REFERENCES,0.55810147299509,"Intuition on the efficacy of the proposed approximation. We believe that because the full-rank
W ∗performs well for recommendation accuracy and better than the low-rank solutions (Steck,
2020; Jin et al., 2021), when W’s estimation is closer to W ∗with respect to the final recovery of
Y , XW ≈XW ∗, it has a comparable performance against XW ∗. In other words, it may not be
essential for the diagonal of low-rank approximate matrix c
W to be zero."
REFERENCES,0.5597381342062193,"B.3
SVD OF W ∗(LR-EDLAE-2)"
REFERENCES,0.5613747954173486,"Furthermore, if we consider the inequality"
REFERENCES,0.563011456628478,"min
rank(W )≤k ||XW ∗−XW||2
F ≤
min
rank(W )≤k ||X||2
F ||W ∗−W||2
F ,"
REFERENCES,0.5646481178396072,or a tighter one
REFERENCES,0.5662847790507365,"min
rank(W )≤k ||XW ∗−XW||2
F ≤
min
rank(W )≤k ||X||2
2||W ∗−W||2
F"
REFERENCES,0.5679214402618658,"Both suggest if we simply perform SVD on W ∗, we can obtain an upper bound error between XW ∗"
REFERENCES,0.5695581014729951,and XW.
REFERENCES,0.5711947626841244,"Simply, the other closed-form solution c
W (LR-EDLAE-2) is:"
REFERENCES,0.5728314238952537,"W ∗SVD
= UΣV T"
REFERENCES,0.574468085106383,"c
W = UkΣkV T
k"
REFERENCES,0.5761047463175123,"C
TIME COMPLEXITY ANALYSIS"
REFERENCES,0.5777414075286416,"The running time of all linear models discussed in this paper is determined by one or more of the
following factors: (i) Computation of matrix multiplication and inversion. The best known theoretical
bound is O(n2.373)(Alman & Williams, 2021) but to the authors’ knowledge the algorithm is not
implemented in any mainstream libraries. (ii) Computation of k leading singular vectors. The running"
REFERENCES,0.5793780687397708,Under review as a conference paper at ICLR 2022
REFERENCES,0.5810147299509002,"time is O(kn2) for an n × n matrix. But recent trends believe that the constant terms in the big-O
notation matter so the asymptotic analysis becomes less fashionable (Stathopoulos & McCombs,
2010; Wu et al., 2017). (iii) Convergence rate of a gradient-based algorithm. Except for special cases,
when an objective is not convex (e.g., when W is decomposed into W = PQ), the convergence rate
of an algorithm is remarkably difficult to analyze and often has substantial discrepancies with its
practical performance (Bhojanapalli et al., 2016)."
REFERENCES,0.5826513911620295,"We remark that all three factors exhibit significant gaps between theory and practice, i.e., the best
theoretical algorithms are not implemented, constants in big-O matter, and accurate characterization
of convergence rate is difficult. Therefore:"
REFERENCES,0.5842880523731587,"In theory, (i) the running time for full-rank Frobenius-norm models (cases 5. EASE, 6. DLAE, and 7.
EDLAE in Table 1) is O(n2.373) because they involve merely matrix inversion and multiplication
operations; (ii) the running time for most low-rank models (cases 3. WLAE, 8. LRR, 10. LR-DLAE,
11. LR-EDLAE-1, and 12. LR-EDLAE-2 in Table 1) are O(n2.373 + kn2) because they require
computation of k leading singular vectors or eigenvectors; (iii) the nuclear-norm models require
the multiplication with X ∈Rm×n. So the running time for cases 1 (Regularized PCA), 2 (MF
dropout), and 4 (VLAE) are O(kn2 + mnk), O(m2n + mn2), O(km2 + kn2) respectively; and (iv)
The running time ADMM-based model (case 9. EDLAE-ADMM in Table 1) is O(tn2.373), where t
is the number of iteration needed to converge. We summarize the results in Table 4"
REFERENCES,0.5859247135842881,"In practice, solving low-rank SVD and computing matrix multiplication require similar time especially
after carefully using HPC techniques. So our proposed algorithms LR-EDLAE-1 and LR-EDLAE-2
(Section 4) are neither slower nor faster than existing ones that do not use gradient algorithms (e.g.
full-rank EDLAE, etc.). The clock time for gradient-based (e.g. ADMM) algorithms is usually worse
than that in theory because determination of hyper-parameters such as learning rate is usually a
guesswork. A rule of thumb is that LR-EDLAE-1 and LR-EDLAE-2 are faster for small k and slower
(than e.g., ADMM-based methods) for large k."
REFERENCES,0.5875613747954174,"Finally, we would like to address again on the advantage of having any closed-form solutions: they
are easy to implement and use, and can be quickly tested using any standard matrix computation
platform without the specialized recommendation library. They are more robust to error and bug, and
can serve as the competitive baseline for recommendation evaluation tasks."
REFERENCES,0.5891980360065466,"D
PROOFS"
REFERENCES,0.5908346972176759,"D.1
PROOF OF LEMMA 1"
REFERENCES,0.5924713584288053,"Consider X ∈Rm×n. Suppose latent variables z ∈Rk and generate data x ∈Rn. The Variational
Linear Autoencoder (VLAE) is defined in the same way as (Lucas et al., 2019b):"
REFERENCES,0.5941080196399345,"p(x | z) = N
 
Wz + µ, σ2I
"
REFERENCES,0.5957446808510638,"q(z | x) = N(V (x −µ), D)
(25)"
REFERENCES,0.5973813420621932,"For simplification, we set µ = 0 in following context. And the ELBO of VLAE is known as:"
REFERENCES,0.5990180032733224,Lx = −KL(q(z|x)||p(z)) + Eq(z|x)[log p(x|z)]
REFERENCES,0.6006546644844517,KL(q(z|x)||p(z)) = −log |D| + xT V T V x + tr(D) −k
REFERENCES,0.602291325695581,"Eq(z|x)[log p(x|z)] = −
1
2σ2"
REFERENCES,0.6039279869067103,"
tr(WDW T ) + xT V T W T WV x −2xT WV x + xT x
 −n"
REFERENCES,0.6055646481178396,2 log 2πσ2 (26)
REFERENCES,0.6072013093289689,"Again, the (maximizing) ELBO can be written as:"
REFERENCES,0.6088379705400983,Under review as a conference paper at ICLR 2022
REFERENCES,0.6104746317512275,Lx = −1 2
REFERENCES,0.6121112929623568,"
−log |D| + xT V T V x + tr(D) −k

−n"
REFERENCES,0.613747954173486,2 log 2πσ2
REFERENCES,0.6153846153846154,"−
1
2σ2"
REFERENCES,0.6170212765957447,"
tr(WDW T ) + xT V T W T WV x −2xT WV x + xT x
 = −1"
REFERENCES,0.618657937806874,"2||V x||2
2 −
1
2σ2"
REFERENCES,0.6202945990180033,"
||W
√"
REFERENCES,0.6219312602291326,"D||2
F + ||x −WV x||2
2

+ f(D, σ) (27)"
REFERENCES,0.6235679214402619,"where f(D, σ) = 1"
REFERENCES,0.6252045826513911,2 log |D| −1
REFERENCES,0.6268412438625205,2tr(D) + k 2 −n
REFERENCES,0.6284779050736498,"2 log 2πσ2, x ∈Rn and z ∈Rk."
REFERENCES,0.630114566284779,"For whole data, it is equivalent to minimize:"
REFERENCES,0.6317512274959084,"L = ||XT −WV XT ||2
F + m||W
√"
REFERENCES,0.6333878887070377,"D||2
F + σ2||V XT ||2
F + g(D, σ)"
REFERENCES,0.6350245499181669,"= ||X −XV T W T ||2
F + m||
√"
REFERENCES,0.6366612111292962,"DW T ||2
F + σ2||XV T ||2
F + g(D, σ)
(28)"
REFERENCES,0.6382978723404256,"where g(D, σ) = −σ2m
 
log |D| −tr(D) + k −n log 2πσ2)."
REFERENCES,0.6399345335515548,"D.2
PROOF OF PROPOSITION 1"
REFERENCES,0.6415711947626841,"Note that when σi ≤λ(k−i), the new singular value shrinks to zero, and can be removed. Basically,
for any λ(1) ≥· · · ≥λ(k), we can build the corresponding Tikhonov regularized instance by setting"
REFERENCES,0.6432078559738135,"σ2
i
σ2
i + λ′
i
= σi −λ(k−i)"
REFERENCES,0.6448445171849427,"σi
, i.e.,λ′
k =
σ3
i
σi −λ(k−i)
−σ2
i .
(29)"
REFERENCES,0.646481178396072,"Discussion of Proposition 1: Further, the same observation holds true for the regularization (Eq. (2)),
and the weighted-nuclear norm regularization in when the weights are in the non-ascending order.
This observation suggests a potentially limitation of the earlier regularization as they will always try to
maintain the larger singular values: when a singular value is large, the shrinkage will be small. Such
regularization has shown to work well in the areas such as image processing (Gu et al., 2014). But it
has not been studied or confirmed if it will work for the recommendation. In Section 5, we report our
experimental study which shows such regularization could be too restrictive for recommendation."
REFERENCES,0.6481178396072013,"D.3
“CLEAR UP” EQ. (4)"
REFERENCES,0.6497545008183306,"Since our objective is for recommendation (not purely on recovering the low rank factors of the data),
we treat the covariance matrix D as hyperparameters. Thus, the term g(D, σ) becomes a constant,
and let A = σV T , B = 1/σW T , Λ = σ
√ mD."
REFERENCES,0.6513911620294599,"If we consider D as an optimization parameter, then the optimal solution of VLAE is equivalent to that
of pPCA (Tipping & Bishop, 1999a). In this case, D = σ2(Σ2/m)−1, where Σ2 = diag(σ2
i ) are"
REFERENCES,0.6530278232405892,"the eigen-values of the covariance matrix of X, and σ2 =
1
n−k
Pn
j=k+1
σ2
j
m . Further, the closed-form
solutions of V (A) and W (B) are characterized. However, for recommendation, the matrix D can be
considered as a hyperparameter (to be tuned); in this case, the closed-form solution is not studied yet."
REFERENCES,0.6546644844517185,It is easy to see we can “clear up” Eq. (4) and obtain the following optimization problem :
REFERENCES,0.6563011456628478,"min
A∈Rn×k,B∈Rk×n ∥X −XAB∥2
F + ∥XA∥2
F + ∥ΛB∥2
F ,
(30)"
REFERENCES,0.6579378068739771,"in which decision variables are A and B, and the hyperparameter is a diagonal matrix Λ ∈Rk×k."
REFERENCES,0.6595744680851063,"D.4
INTUITION FOR PROVING PROPOSITION 2"
REFERENCES,0.6612111292962357,"Here, We explain the intuition for proving Appendix D.5 Proposition 2 (see Appendix D.5 for the
full analysis). Consider OPT1 and let W = PQ. Our goal is to characterize the behaviors of P
and Q with the presence of the regularizers when W = PQ is known (fixed). Let the SVD of W
be UW ΣW V T
W . Because two regularizers ∥Λ
1
2 Q∥2
F and ∥PΛ
1
2 ∥2
F are symmetric, we could “guess”"
REFERENCES,0.662847790507365,P = UW Σ
REFERENCES,0.6644844517184942,"1
2
W Ωand Q = ΩTΣ"
REFERENCES,0.6661211129296236,"1
2
W V T
W , where Ωis a unitary matrix. Now we have"
REFERENCES,0.6677577741407529,Under review as a conference paper at ICLR 2022
REFERENCES,0.6693944353518821,"∥Λ
1
2 Q∥2
F + ∥PΛ
1
2 ∥2
F = ∥Λ
1
2 ΩΣ
1
2 V T
W ∥2
F + ∥UW Σ"
REFERENCES,0.6710310965630114,"1
2
W ΩΛ
1
2 ∥2
F = 2∥Λ
1
2 ΩΣ"
REFERENCES,0.6726677577741408,"1
2
W ∥2
F ."
REFERENCES,0.67430441898527,"Now the question of finding P and Q when W is known boils down to finding a unitary matrix Ω
that minimizes ∥Λ
1
2 ΩΣ"
REFERENCES,0.6759410801963993,"1
2
W ∥2
F , where diagonal matrices Λ and ΣW are given. Recall that λi = Λii
and let σi = (ΣW )ii. Note that λi’s could be unsorted and, and that σi’s are sorted in descending
order."
REFERENCES,0.6775777414075287,"If we restrict Ωto be only a permutation matrix, then we aim to find a permutation π ∈[k]
that minimizes P"
REFERENCES,0.679214402618658,"i≤k λπ(i)σi. Using a rearrangement inequality (Yue, 2020), we can see that
the minimal is achieved when λπ(1) ≤λπ(2) ≤· · · ≤λπ(k). In this case, we indeed have"
REFERENCES,0.6808510638297872,"minΩa permutation ∥Λ
1
2 ΩΣ"
REFERENCES,0.6824877250409165,"1
2
W ∥2
F = ∥ΣW ∥ω,∗, where ω = (λπ(1), . . . , λπ(k))."
REFERENCES,0.6841243862520459,"Note that because PQ = PΩΩTQ for any unitary matrix Ω, it is always beneficial to use Ωto shuffle
the rows and columns of P and Q so that the largest σi is mapped to the smallest λi, etc. This “degree
of freedom” from Ωalso explains why ordering the values along Λ’s diagonal is irrelevant."
REFERENCES,0.6857610474631751,"Appendix D.5 shows that even when Ωis allowed to be any unitary matrix, the optimal one is still a
permutation matrix. This conclusion can be viewed as a matrix version of re-arrangement inequality."
REFERENCES,0.6873977086743044,"D.5
PROOF OF PROPOSITION 2"
REFERENCES,0.6890343698854338,"By slightly abusing the notation, we shall let OPT1 (OPT2) be the value of the optimal solution for
OPT1 (OPT2). We need to show that OPT1 = OPT2. We need two directions."
REFERENCES,0.690671031096563,"OPT2 ≥OPT1: Let W ∗be an optimal solution for OPT2. Let the SVD of W ∗be U ∗Σ∗(V ∗)T.
Recall that W ∗needs to satisfy the rank constraint rank(W ∗) ≤k so U ∗∈Rm×k, Σ∗∈Rk×k,
and V ∗∈Rn×k. Let π be a permutation on [k] such that λπ(1) ≤λπ(2) ≤· · · ≤λπ(k). Let also Ω
be the corresponding permutation matrix. Specifically, Ω∈{0, 1}k×k and there is exactly one entry
in each row of Ωis 1:"
REFERENCES,0.6923076923076923,"Ωi,j =

1
if j = π(i).
0
otherwise."
REFERENCES,0.6939443535188216,"For example, consider a case in which λ1 > λ2 > · · · > λk. Then we set π = (k, k −1, . . . , 1), and
correspondingly, Ω=  
"
REFERENCES,0.6955810147299509,"0
...
0
1
0
...
1
0
...
1
...
0
0  
."
REFERENCES,0.6972176759410802,"Next, let P = U ∗(Σ∗)
1
2 Ωand Q = ΩT(Σ∗)
1
2 (V ∗)T. We have W ∗= PQ and f(W ∗) = f(PQ).
In addition,"
REFERENCES,0.6988543371522095,"∥Λ
1
2 Q∥2
F + ∥PΛ
1
2 ∥2
F = 2∥Λ
1
2 Ω(Σ∗)
1
2 ∥2
F = 2
X"
REFERENCES,0.7004909983633388,"i≤k
λπ(i)σi = 2∥W ∗∥ω,∗,"
REFERENCES,0.7021276595744681,"where σi is the i-th largest singular value of W ∗. In other words, we have found a (P, Q) pair such
that"
REFERENCES,0.7037643207855974,"f(PQ) + ∥Λ
1
2 Q∥2
F + ∥PΛ
1
2 ∥2
F = f(W ∗) + 2∥W ∗∥ω,∗= OPT2,"
REFERENCES,0.7054009819967266,which shows that OPT1 ≤OPT2.
REFERENCES,0.707037643207856,"OPT2 ≤OPT1. Let P ∗and Q∗be an optimal solution for OPT1. Let the singular values of P ∗
be σ1(P ∗) ≥σ2(P ∗) ≥· · · ≥σk(P ∗) and those of Q∗be σ1(Q∗) ≥σ2(Q∗) ≥· · · ≥σk(Q∗). Let
also σ∗
1 ≥· · · ≥σ∗
k be the singular values of P ∗Q∗."
REFERENCES,0.7086743044189853,"We shall find a lower bound of ∥Λ
1
2 Q∥2
F + ∥PΛ
1
2 ∥2
F expressed in terms of σ∗
i ’s. In fact, we shall
show that
∥Λ
1
2 Q∥2
F + ∥PΛ
1
2 ∥2
F ≥2∥P ∗Q∗∥ω,∗.
(31)"
REFERENCES,0.7103109656301145,Under review as a conference paper at ICLR 2022
REFERENCES,0.7119476268412439,"One can see that if Eq. (31) were true, we have"
REFERENCES,0.7135842880523732,"OPT2 ≤f(P ∗Q∗) + 2∥P ∗Q∗∥ω,∗≤f(P ∗Q∗) + ∥Λ
1
2 Q∗∥2
F + ∥P ∗Λ
1
2 ∥2
F = OPT1."
REFERENCES,0.7152209492635024,"Thus, it remains to prove Eq. (31). Let λ(1) ≥λ(2) ≥· · · ≥λ(k) be a sorted sequence of λi’s i.e.,
λ(k) = λπ(1), λ(k−1) = λπ(2), . . . , λ(1) = λπ(k)."
REFERENCES,0.7168576104746317,"First, we show that ∥PΛ
1
2 ∥2
F ≥Pk
i=1 λ(k−i+1)×σ2
i (P ∗) and ∥Λ
1
2 Q∥2
F ≥Pk
i=1 λ(k−i+1)×σ2
i (Q∗).
We need the following Lemma (see e.g., Theorem 2 in (Yue, 2020)):"
REFERENCES,0.7184942716857611,"Lemma 4. Let A and B be two positive definite matrices in Rk×k. Then it holds that k
X"
REFERENCES,0.7201309328968903,"i=1
σi(A)σk−i+1(B) ≤tr(B
1
2 AB
1
2 ).
(32)"
REFERENCES,0.7217675941080196,"Let the SVD of P ∗be UP ∗ΣP ∗V T
P ∗and that of Q∗be UQ∗ΣQ∗V T
Q∗. We have"
REFERENCES,0.723404255319149,"∥P ∗Λ
1
2 ∥2
F = ∥UP ∗ΣP ∗V T
P ∗Λ
1
2 ∥2
F = ∥ΣP ∗V T
P ∗Λ
1
2 ∥2
F = tr(ΣP ∗V T
P ∗ΛVP ∗ΣP ∗).
(33)"
REFERENCES,0.7250409165302782,"We now apply Lemma 4 by setting A = V T
P ∗ΛVP ∗and B = Σ2
P ∗, and obtain that"
REFERENCES,0.7266775777414075,"∥P ∗Λ
1
2 ∥2
F = tr(ΣP ∗V T
P ∗ΛVP ∗ΣP ∗) ≥ k
X"
REFERENCES,0.7283142389525368,"i=1
λ(k+1−i) × σ2
i (P ∗).
(34)"
REFERENCES,0.7299509001636661,"We may similarly prove that ∥Λ
1
2 Q∗∥2
F ≥Pk
i=1 λ(k−i+1) × σ2
i (Q∗). Therefore,"
REFERENCES,0.7315875613747954,"∥Λ
1
2 Q∗∥2
F + ∥P ∗Λ
1
2 ∥2
F ≥ k
X"
REFERENCES,0.7332242225859247,"i=1
λ(k+1−i) × (σ2
i (P ∗) + σ2
i (Q∗))
(35)"
REFERENCES,0.734860883797054,"provides a lower bound of ∥Λ
1
2 Q∗∥2
F + ∥P ∗Λ
1
2 ∥2
F in terms of σi(P ∗) and σi(Q∗). We next aim to
express the lower bound in terms of σ∗
i ’s (singular values of P ∗Q∗) directly."
REFERENCES,0.7364975450081833,"The following program gives a lower bound for ∥Λ
1
2 Q∗∥2
F + ∥P ∗Λ
1
2 ∥2
F :"
REFERENCES,0.7381342062193126,"min :
∥Λ
1
2 Q∗∥2
F + ∥P ∗Λ
1
2 ∥2
F
(36)
subject to
W = P ∗Q∗"
REFERENCES,0.7397708674304418,"σi(W) = σ∗
i
for i ≤k."
REFERENCES,0.7414075286415712,"Write the SVD of W be UW ΣW V T
W . Also, let ˜P = U T
W P ∗and ˜Q = Q∗VW . Noting that the
columns in P ∗are in the column space of W and the rows in Q∗are in the row space of W, we
have (i) σi(P ∗) = σi( ˜P) and σi(Q∗) = σi( ˜Q) for i ≤k, and (ii) ∥Λ
1
2 Q∗∥2
F + ∥P ∗Λ
1
2 ∥2
F =
∥Λ
1
2 ˜Q∥2
F + ∥˜PΛ
1
2 ∥2
F ."
REFERENCES,0.7430441898527005,"Therefore, Eq. (36) can be equivalently written as"
REFERENCES,0.7446808510638298,"min :
∥Λ
1
2 ˜Q∥2
F + ∥˜PΛ
1
2 ∥2
F
(37)"
REFERENCES,0.7463175122749591,"subject to
ΣW = ˜P ˜Q
(ΣW )i,i = σ∗
i
for i ≤k."
REFERENCES,0.7479541734860884,"Now ˜P ˜Q is positive definite. Using a similar technique developed in (Bao et al., 2020) (Theorem 1),
one can see that ˜P = ˜QT. See also Lemma 5. This implies that ˜P = Σ"
REFERENCES,0.7495908346972177,"1
2
W Ωfor some unitary matrix
Ωand σi(P ∗) = σi( ˜P) = σi(Q∗) = σi( ˜Q) =
p"
REFERENCES,0.7512274959083469,"σ∗
i for i ≤k. Together with Eq. (35), we have"
REFERENCES,0.7528641571194763,"∥Λ
1
2 Q∗∥2
F + ∥P ∗Λ
1
2 ∥2
F ≥2
X"
REFERENCES,0.7545008183306056,"i≤k
λ(k+1−i) × σ2
i (P ∗) = 2
X"
REFERENCES,0.7561374795417348,"i≤k
λ(k+1−i) × σ∗
i = 2∥P ∗Q∗∥ω,∗."
REFERENCES,0.7577741407528642,Under review as a conference paper at ICLR 2022
REFERENCES,0.7594108019639935,"D.6
PROOF OF COROLLARY 1"
REFERENCES,0.7610474631751227,"We first find an optimal solution for Eq. (9). Let the SVD of X be X = UXΣXV T
X , where
UX ∈Rm×n, ΣX ∈Rn×n, and VX ∈Rn×n. Let ¯UX be an arbitrary basis for the subspace that is
orthogonal to X’s column space so ¯UX ∈Rm×(m−n) and [UX, ¯UX] form a basis for Rm. We have"
REFERENCES,0.762684124386252,"∥X −PQ∥2
F + ∥PΛ
1
2 ∥2
F + ∥Λ
1
2 Q∥2
F ="
REFERENCES,0.7643207855973814,"
U T
X
¯U T
X"
REFERENCES,0.7659574468085106,"
XVX −

U T
X
¯U T
X"
REFERENCES,0.7675941080196399,"
PQVX  2 F
+"
REFERENCES,0.7692307692307693,"
U T
X
¯U T
X"
REFERENCES,0.7708674304418985,"
PΛ
1
2 2"
REFERENCES,0.7725040916530278,"F
+ ∥Λ
1
2 QVX∥2
F ."
REFERENCES,0.7741407528641571,"Let ˜P =

U T
X
¯U T
X"
REFERENCES,0.7757774140752864,"
P and ˜Q = QVX. Then our objective becomes"
REFERENCES,0.7774140752864157,"min
˜
P , ˜
Q "
REFERENCES,0.779050736497545,"
ΣX
0(m−n)×n"
REFERENCES,0.7806873977086743,"
−˜P ˜Q 2"
REFERENCES,0.7823240589198036,"F
+ ∥˜PΛ
1
2 ∥2
F + ∥Λ
1
2 ˜Q∥2
F .
(38)"
REFERENCES,0.7839607201309329,"Let ˜W = ˜P ˜Q and the singular values of ˜W be σ∗
1 ≥σ∗
2 ≥· · · ≥σ∗
k. Let also ˜Σ =

ΣX
0(m−n)×n 
."
REFERENCES,0.7855973813420621,Recall also that σi is the i-th largest singular value of X. We next show that
REFERENCES,0.7872340425531915,"
ΣX
0"
REFERENCES,0.7888707037643208,"
−˜P ˜Q 2"
REFERENCES,0.79050736497545,"F
= ∥˜Σ −˜P ˜Q∥2
F ≥ k
X"
REFERENCES,0.7921440261865794,"i=1
(σi −σ∗
i )2 + n
X"
REFERENCES,0.7937806873977087,"i=k+1
σ2
i ."
REFERENCES,0.795417348608838,Note first that
REFERENCES,0.7970540098199672,"∥˜Σ −˜W∥2
F = ∥˜Σ∥2
F + ∥˜W∥2
F −2⟨˜Σ, ˜W⟩.
(39)"
REFERENCES,0.7986906710310966,"Next, we have ((Zheng et al., 2018)):"
REFERENCES,0.8003273322422259,"|⟨˜Σ, ˜W⟩| = |tr(˜Σ ˜W T)∥≤|tr(˜ΣΣ ˜
W )| = k
X"
REFERENCES,0.8019639934533551,"i=1
σiσ∗
i ."
REFERENCES,0.8036006546644845,"Therefore, ⟨˜Σ, ˜W⟩is maximized when"
REFERENCES,0.8052373158756138,"˜Wi,j =

σ∗
i
if i = j ≤k
0
Otherwise."
REFERENCES,0.806873977086743,"When we plug in this optimized ˜W to Eq. (39), we get"
REFERENCES,0.8085106382978723,"∥˜Σ −˜W∥2
F ≥ k
X"
REFERENCES,0.8101472995090017,"i=1
(σi −σ∗
i )2 + n
X"
REFERENCES,0.8117839607201309,"i=k+1
σ2
i ."
REFERENCES,0.8134206219312602,"Next, from Proposition 2, we have"
REFERENCES,0.8150572831423896,"∥˜PV
1
2 ∥2
F + ∥Λ
1
2 ˜Q∥2
F ≥ k
X"
REFERENCES,0.8166939443535188,"i=1
λ(k−i+1)σ∗
i ."
REFERENCES,0.8183306055646481,"Therefore, we can find a lower bound for Eq. (5) in terms of σ∗
i ’s:"
REFERENCES,0.8199672667757774,"L(σ∗
1, . . . , σ∗
k) = k
X"
REFERENCES,0.8216039279869067,"i=1
(σi −σ∗
i )2 + 2 k
X"
REFERENCES,0.823240589198036,"i=1
λ(k−i+1)σ∗
i + m
X"
REFERENCES,0.8248772504091653,"i=k+1
σ2
i
(σ∗
1 ≥· · · ≥σ∗
k ≥0).
(40)"
REFERENCES,0.8265139116202946,"We next find a minimal value of L (by treating σ∗
i ’s as decision variables). This will give us a lower
bound (and is independent of σ∗
i ) on our optimization problem. We then show that this lower bound
can be achieved by carefully constructing ˜W (as well as ˜P and ˜Q). This means such ˜W is optimal."
REFERENCES,0.8281505728314239,Under review as a conference paper at ICLR 2022
REFERENCES,0.8297872340425532,"Specifically, we need to find an optimal solution for the following program:"
REFERENCES,0.8314238952536824,"minimizeσ∗
1,...,σ∗
k
L(σ∗
1, . . . , σ∗
k)
(41)"
REFERENCES,0.8330605564648118,"subject to:
σ∗
i ≥0
σ∗
1 ≤σ∗
2 ≤· · · ≤σ∗
k
(Ordering constraint)"
REFERENCES,0.8346972176759411,We shall first find an optimal solution for
REFERENCES,0.8363338788870703,"minimizeσ∗
1,...,σ∗
k
L(σ∗
1, . . . , σ∗
k)
(42)"
REFERENCES,0.8379705400981997,"subject to:
σ∗
i ≥0"
REFERENCES,0.839607201309329,"Note here, the ordering constraint is removed so the optimal value for Eq. (42) should be no more
than that for Eq. (41). We shall see that the optimal solution for Eq. (41) also satisfies the ordering
constraint so indeed optimal solutions for Eq. (41) and Eq. (42) are the same."
REFERENCES,0.8412438625204582,The problem Eq. (42) boils down to finding
REFERENCES,0.8428805237315876,"min
σ∗
i ≥0(σi −σ∗
i )2 + 2 k
X"
REFERENCES,0.8445171849427169,"i=1
λ(k−i+1)σ∗
i ."
REFERENCES,0.8461538461538461,"We note that σ∗
i ’s do not interact with each other so we can optimize each σ∗
i ’s independently. We get"
REFERENCES,0.8477905073649754,"σ∗
i = (σi −λ(k−i+1))+."
REFERENCES,0.8494271685761048,"We can check that σ∗
1 ≥· · · ≥σ∗
k. Therefore, the optimal value for Eq. (41) is k
X"
REFERENCES,0.851063829787234,"i=1
(σi −(σi −λ(k−i+1))+)2 + 2 k
X"
REFERENCES,0.8527004909983633,"i=1
λ(k−i+1)(σi −λ(k−i+1))+ + n
X"
REFERENCES,0.8543371522094927,"i=k+1
σ2
i ."
REFERENCES,0.855973813420622,This is also a lower bound for Eq. (9). One can check that when we set P and Q as
REFERENCES,0.8576104746317512,"P ∗= Ukdiag(
q"
REFERENCES,0.8592471358428805,"(σ1 −λ(k))+), . . . ,
q"
REFERENCES,0.8608837970540099,"(σk −λ(1))+)Ω,
(43)"
REFERENCES,0.8625204582651391,"Q∗= ΩTdiag(
q"
REFERENCES,0.8641571194762684,"(σ1 −λ(k))+), . . . ,
q"
REFERENCES,0.8657937806873978,"(σk −λ(1))+)V T
k ,"
REFERENCES,0.867430441898527,"the lower bound is achieved so Eq. (43) gives an optimal solution. Here, Uk and Vk are leading left
and right singular vectors of X."
REFERENCES,0.8690671031096563,Now we move to analyze Eq. (10). Our goal is to reduce Eq. (10) to Eq. (9). Let
REFERENCES,0.8707037643207856,P = XAΛ−1
REFERENCES,0.8723404255319149,"2
Q = Λ
1
2 B."
REFERENCES,0.8739770867430442,Then Eq. (10) becomes
REFERENCES,0.8756137479541735,"minimizeP,Q
∥X −PQ∥2
F + ∥PΛ
1
2 ∥2
F + ∥Λ
1
2 Q∥2
F
(44)"
REFERENCES,0.8772504091653028,"subject to
P = XAΛ−1"
REFERENCES,0.8788870703764321,"2
(Constraint P)"
REFERENCES,0.8805237315875614,"Q = Λ
1
2 B
(Constraint Q)."
REFERENCES,0.8821603927986906,"Here, X and Λ are given, whereas P, Q, A, and B are decision variables. The (Constraint P) says that
each column of P needs to be in a column space of X (it is a necessary and sufficient condition for A
to exist). The (Constraint Q) simply says Q and B are linearly related and does not have tangible
impact to the optimization problem."
REFERENCES,0.88379705400982,"But we note that when we put aside the constraints, an optimal (P, Q) is specified by Eq. (43). The
columns of the optimal P indeed is in the column space of X. So (P, Q) is also an optimal solution
for Eq. (44). We may find the corresponding A and B:"
REFERENCES,0.8854337152209493,"A∗= X†P ∗Λ
1
2
and B∗= Λ−1 2 Q∗,"
REFERENCES,0.8870703764320785,Under review as a conference paper at ICLR 2022
REFERENCES,0.8887070376432079,"D.7
SYMMETRIC LEMMA"
REFERENCES,0.8903436988543372,"Lemma 5. Let ˜P, ˜Q ∈Rk×k be full rank, Λ be a diagonal matrix, and ΣW be a diagonal matrix so
that (ΣW )i,i = σ∗
i , where σ∗
i ’s are sorted in descending order. Consider the optimization problem:"
REFERENCES,0.8919803600654664,"min :
∥Λ
1
2 ˜Q∥2
F + ∥˜PΛ
1
2 ∥2
F
(45)"
REFERENCES,0.8936170212765957,"subject to
ΣW = ˜P ˜Q
(ΣW )i,i = σ∗
i
for i ≤k."
REFERENCES,0.8952536824877251,There is an optimal solution such that ˜P = ˜QT
REFERENCES,0.8968903436988543,"Proof. Let ˆP = ˜PΛ
1
2 and ˆQ = Λ−1"
REFERENCES,0.8985270049099836,2 ˜Q. The problem Eq. (45) is equivalent to
REFERENCES,0.900163666121113,"min :
∥Λ ˆQ∥2
F + ∥ˆP∥2
F
(46)"
REFERENCES,0.9018003273322422,"subject to
ΣW = ˆP ˆQ
(ΣW )i,i = σ∗
i
for i ≤k."
REFERENCES,0.9034369885433715,"Let the SVD of ˆQ be U ˆ
QΣ ˆ
QV T
ˆ
Q so ˆQ−1 = V ˆ
QΣ−1
ˆ
Q U T
ˆ
Q. We can also see that ˆP = ΣW ˆQ−1.
Therefore, the objective term becomes"
REFERENCES,0.9050736497545008,"∥ΛU ˆ
QΣ ˆ
QV T
ˆ
Q ∥2
F + ∥ΣW V ˆ
QΣ−1
ˆ
Q U T
ˆ
Q∥2
F = ∥ΛU ˆ
QΣ ˆ
Q∥2
F + ∥ΣW V ˆ
QΣ−1
ˆ
Q ∥2
F ."
REFERENCES,0.9067103109656302,"Let us consider the stationary points U ˆ
Q and V ˆ
Q when Σ ˆ
Q is fixed. We can see that they need to be
permutation matrices to minimize both terms in the objective (using the rearrangement inequality
again). Therefore, we can see ˆQ = Σ1Σ ˆ
QΣ2 for two permutation matrices Σ1 and Σ2. This implies"
REFERENCES,0.9083469721767594,"that ˜Q = Λ
1
2 Σ1Σ ˆ
QΣ2, i.e., each row (column) of ˜Q has exactly one non-zero entry. We may similarly
show that each row (column) of ˜P has exactly one non-zero entry. In addition, the locations of
non-zero entries of ˜P and ˜QT are identical because ˜P ˜Q is a diagonal matrix. We may thus write"
REFERENCES,0.9099836333878887,"˜P = Σ(1)Σ ˜
P Σ(2)
˜Q = ΣT
(2)Σ( ˜
Q)ΣT
(1),"
REFERENCES,0.911620294599018,"where (Σ ˜
P )i,i = σi( ˜P) and (Σ( ˜
Q))i,i = στ(i)( ˜Q), where τ is a permutation on [k]. The set of"
REFERENCES,0.9132569558101473,"(possibly unsorted) singular values for ˜P ˜Q thus is σi( ˜P)στ(i)( ˜Q). Thus, we can see that there exists
a permutation ¯π such that"
REFERENCES,0.9148936170212766,"∥Λ
1
2 ˜Q∥2
F + ∥˜PΛ
1
2 ∥2
F =
X i≤k"
REFERENCES,0.9165302782324058," 
σ2
i (P ∗)λ¯π(i) + σ2
τ(i)(Q∗)λ¯π(i)
 ≥
X"
REFERENCES,0.9181669394435352,"i≤k
2σi(P ∗)σ∗
τ(i)(Q)λ¯π(i)"
REFERENCES,0.9198036006546645,"≥2∥P ∗Q∗∥ω,∗."
REFERENCES,0.9214402618657938,"One can see that we can set ˜P = ˜QT to make all inequality becomes equality so there is an optimal
solution such that ˜P = ˜QT."
REFERENCES,0.9230769230769231,"E
EXPERIMENTAL DETAILS"
REFERENCES,0.9247135842880524,"E.1
ANONYMOUS CODE"
REFERENCES,0.9263502454991817,"https://anonymous.4open.science/r/ICLR-2022-Anonymous-Demo-Code-B9FF/
README.md"
REFERENCES,0.9279869067103109,Under review as a conference paper at ICLR 2022
REFERENCES,0.9296235679214403,"E.2
DLAE AND HYPERPARAMETER TUNING"
REFERENCES,0.9312602291325696,"This section presents the hyperparameter tuning process on the validation data over three (ML-20M,
Netflix, MSD) datasets for the full-rank DLAE formula (case 6 in Table 1), which was introduced by
Steck (2020) yet not investigated:"
REFERENCES,0.9328968903436988,"min
W ||X −XW||2
F + ||Λ1/2 · W||2
F"
REFERENCES,0.9345335515548282,"Λ =
p
1 −pdMat(diag(XT X))"
REFERENCES,0.9361702127659575,"c
W = (XT X + Λ)−1XT X"
REFERENCES,0.9378068739770867,"In practical, l2 regularization (hyperparameter λ) is also imposed:"
REFERENCES,0.939443535188216,"c
W = (XT X + Λ + λ)−1XT X"
REFERENCES,0.9410801963993454,"The Tables 5 to 7 show the results of nDCG@100 over three datasets respectively. And the optimal
parameters are highlighted."
REFERENCES,0.9427168576104746,"Table 5: ml-20m, DLAE full rank, parameter tuning on validation dataset by nDCG@100"
REFERENCES,0.9443535188216039,"λ
800
900
1000
1100
1200
1300 p"
REFERENCES,0.9459901800327333,"0.1
0.42024
0.42063
0.42073
0.42102
0.42131
0.4212
0.2
0.43132
0.43139
0.43154
0.4314
0.43147
0.43136
0.3
0.43203
0.43211
0.43214
0.43206
0.43203
0.43196
0.4
0.43001
0.43001
0.42995
0.42996
0.42984
0.42978
0.5
0.42754
0.42745
0.42729
0.42718
0.42715
0.42704"
REFERENCES,0.9476268412438625,"Table 6: netflix, DLAE full rank, parameter tuning on validation dataset by nDCG@100"
REFERENCES,0.9492635024549918,"λ
800
900
1000
1100
1200
1300
1400 p"
REFERENCES,0.9509001636661211,"0.2
0.3904
0.3904
0.39027
0.39024
0.3902
0.3903
0.39018
0.25
0.39247
0.39252
0.39248
0.39249
0.3925
0.3925
0.39256
0.3
0.39359
0.39359
0.39366
0.39358
0.39362
0.39368
0.39369
0.35
0.39402
0.39403
0.394
0.39405
0.39399
0.39403
0.39397
0.4
0.39399
0.39393
0.39395
0.39393
0.39389
0.39388
0.39387
0.45
0.39346
0.3935
0.39343
0.39344
0.39338
0.3933
0.39329
0.5
0.39249
0.39241
0.39247
0.39241
0.39242
0.3923
0.39224"
REFERENCES,0.9525368248772504,"E.3
MATRIX FACTORIZATION WITH DROPOUT AND HYPERPARAMETER TUNING"
REFERENCES,0.9541734860883797,"Cavazza et al. (2018) show that optimization with dropout (allowing rank d to be optimized) is
equivalent to solving a matrix approximation problem with nuclear norm:"
REFERENCES,0.955810147299509,"min
P,Q,d ||X −PQ||2
F + d1 −p p
· d
X"
REFERENCES,0.9574468085106383,"k=1
||Pk||2
2 · ||QT
k ||2
2"
REFERENCES,0.9590834697217676,"min
Y
||X −Y ||2
F + 1 −p"
REFERENCES,0.9607201309328969,"p
||Y ||2
∗ (47)"
REFERENCES,0.9623567921440261,"Table 7: msd, DLAE full rank, parameter tuning on validation dataset by nDCG@100"
REFERENCES,0.9639934533551555,"λ
10
20
30
40
50
60 p"
REFERENCES,0.9656301145662848,"0.3
0.38514
0.38515
0.38517
0.38505
0.38492
0.38474
0.4
0.38596
0.38599
0.38602
0.386
0.38597
0.38592
0.5
0.38556
0.38555
0.38553
0.38557
0.38553
0.38549
0.6
0.38382
0.3838
0.38381
0.38374
0.38373
0.38366"
REFERENCES,0.967266775777414,Under review as a conference paper at ICLR 2022
REFERENCES,0.9689034369885434,and the solution is given by:
REFERENCES,0.9705400981996727,"X
SVD
= UΣV T"
REFERENCES,0.972176759410802,Y ∗= P ∗· Q∗
REFERENCES,0.9738134206219312,= U · Sµ(Σ) · V T
REFERENCES,0.9754500818330606,"Sµ(σ) = max(σ −µ, 0)"
REFERENCES,0.9770867430441899,"µ =
1 −p
p + (1 −p) ¯d ¯d
X"
REFERENCES,0.9787234042553191,"i=1
σi(X)"
REFERENCES,0.9803600654664485,where ¯d denotes the largest integer such that:
REFERENCES,0.9819967266775778,"σ ¯d(X) >
1 −p
p + (1 −p) ¯d ¯d
X"
REFERENCES,0.983633387888707,"i=1
σi(X)"
REFERENCES,0.9852700490998363,"Hence, there is only one parameter p to tuning. We present the tuning process on the validation set
below, see Tables 8 to 10. Optimal parameter p as well as induced rank ¯d is highlighted."
REFERENCES,0.9869067103109657,"Table 8: ml-20m, matrix factorization with dropout, hyper parameter tuning by nDCG@100 on
validation dataset and its induced rank ."
REFERENCES,0.9885433715220949,"p
0.9
0.99
0.995
0.996
0.997
induced rank ¯d
10
200
385
467
602
nDCG@100
0.29723
0.39369
0.40045
0.40046
0.39925"
REFERENCES,0.9901800327332242,"Table 9: netflix, matrix factorization with dropout, hyper parameter tuning by nDCG@100 on
validation dataset and its induced rank ."
REFERENCES,0.9918166939443536,"p
0.9
0.99
0.996
0.997
0.998
induced rank ¯d
9
209
524
653
883
nDCG@100
0.26026
0.35462
0.36453
0.36495
0.36406"
REFERENCES,0.9934533551554828,"Table 10: msd, matrix factorization with dropout, hyper parameter tuning by nDCG@100 on
validation dataset and its induced rank ."
REFERENCES,0.9950900163666121,"p
0.99
0.999
0.9995
0.9999
0.99995
induced rank ¯d
249
2054
3783
11380
19308
nDCG@100
0.18986
0.28532
0.307
0.32634
0.30995"
REFERENCES,0.9967266775777414,"E.4
RESOURCES"
REFERENCES,0.9983633387888707,"Our code are mainly implemented in Numpy 1.19, Pytorch 1.7.1 on CUDA 11.0. Our experiments
are performed on nodes with two sockets, each containing a 24-core Intel(R) Xeon(R) Platinum 8268
CPU @ 2.90GHz and 4 GeForce RTX 3090 24GB memory GPU."
