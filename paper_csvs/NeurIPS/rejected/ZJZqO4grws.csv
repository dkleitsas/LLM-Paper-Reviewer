Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010131712259371835,"We propose a contrastive meta-objective to enable meta-learners to emulate human-
1"
ABSTRACT,0.002026342451874367,"like rapid learning capability through enhanced alignment and discrimination. Our
2"
ABSTRACT,0.00303951367781155,"proposed approach, dubbed ConML, exploits task identity as additional supervision
3"
ABSTRACT,0.004052684903748734,"signal for meta-training, benefiting meta-learner’s fast-adaptation and task-level
4"
ABSTRACT,0.005065856129685917,"generalization abilities. This is achieved by contrasting the outputs of meta-learner,
5"
ABSTRACT,0.0060790273556231,"i.e, performing contrastive learning in the model space. Specifically, we introduce
6"
ABSTRACT,0.0070921985815602835,"metrics to minimize the inner-task distance, i.e., the distance among models learned
7"
ABSTRACT,0.008105369807497468,"on varying data subsets of the same task, while maximizing the inter-task distance
8"
ABSTRACT,0.00911854103343465,"among models derived from distinct tasks. ConML distinguishes itself through
9"
ABSTRACT,0.010131712259371834,"versatility and efficiency, seamlessly integrating with episodic meta-training meth-
10"
ABSTRACT,0.011144883485309016,"ods and the in-context learning of large language models (LLMs). We apply
11"
ABSTRACT,0.0121580547112462,"ConML to representative meta-learning algorithms spanning optimization-, metric-,
12"
ABSTRACT,0.013171225937183385,"and amortization-based approaches, and show that ConML can universally and
13"
ABSTRACT,0.014184397163120567,"significantly improve conventional meta-learning and in-context learning.
14"
INTRODUCTION,0.015197568389057751,"1
Introduction
15"
INTRODUCTION,0.016210739614994935,"Meta-learning [37, 42], or learning to learn, is a powerful paradigm that aims to enable a learning
16"
INTRODUCTION,0.017223910840932118,"system to quickly adapt to new tasks. Meta-learning has been widely applied in different fields, like
17"
INTRODUCTION,0.0182370820668693,"few-shot learning [17, 50], reinforcement learning [56, 26] and neural architecture search [16, 38]. In
18"
INTRODUCTION,0.019250253292806486,"meta-training, a meta-leaner mimics the learning processes on many relevant tasks to gain experience
19"
INTRODUCTION,0.020263424518743668,"about how to make adaptation. In meta-testing, the meta-trained adaptation process is performed
20"
INTRODUCTION,0.02127659574468085,"on unseen tasks. The adaptation process is achieved by generating task-specific model by the meta-
21"
INTRODUCTION,0.022289766970618033,"learner, which is given a set of training examples and returns a predictive model. People prefer
22"
INTRODUCTION,0.02330293819655522,"meta-learning to equip models with human’s fast learning ability, so that a good model can be
23"
INTRODUCTION,0.0243161094224924,"achieved with a few examples [50].
24"
INTRODUCTION,0.025329280648429583,"The combination of two cognitive capabilities, namely, alignment and discrimination, is essential
25"
INTRODUCTION,0.02634245187436677,"for human’s fast learning ability [23, 12, 13]. A good learner possesses the alignment [27] ability to
26"
INTRODUCTION,0.02735562310030395,"align different partial views of a certain object, which means they can integrate various aspects or
27"
INTRODUCTION,0.028368794326241134,"perspectives of information to form a coherent understanding. On the other hand, discrimination [34]
28"
INTRODUCTION,0.029381965552178316,"refers to the learner’s capacity to distinguish between one stimulus and similar stimuli, responding
29"
INTRODUCTION,0.030395136778115502,"appropriately only to the correct stimuli. This is a fundamental ability that allows learners to
30"
INTRODUCTION,0.031408308004052685,"differentiate between what is relevant and what is not, ensuring that their responses are accurate
31"
INTRODUCTION,0.03242147922998987,"and based on the correct understanding of the stimuli presented. With alignment and discrimination,
32"
INTRODUCTION,0.03343465045592705,"learners can synthesize fragmented information to construct a complete picture of an object or
33"
INTRODUCTION,0.034447821681864235,"concept, while also being able to discern subtle differences between distinct but similar objects
34"
INTRODUCTION,0.03546099290780142,"or ideas. Such learners are not only efficient in processing information but also in applying their
35"
INTRODUCTION,0.0364741641337386,"knowledge accurately in varied contexts. This dual capability is crucial for effective learning.
36"
INTRODUCTION,0.037487335359675786,"We expect meta-learners to emulate the above combination of alignment and discrimination capa-
37"
INTRODUCTION,0.03850050658561297,"bilities to approach human’s fast learning ability. By equipping a meta-learner with the ability to
38"
INTRODUCTION,0.03951367781155015,"Figure 1: ConML is performing contrastive learning in model space, where alignment and discrimi-
nation encourage the meta-learner’s fast-adaptation and task-level generalize ability respectively."
INTRODUCTION,0.040526849037487336,"align, we enable it to capture the core essence of a task and being invariant to noises. Meanwhile,
39"
INTRODUCTION,0.04154002026342452,"discrimination ensures that a meta-learner can learn specific models for unique tasks, as it is a natural
40"
INTRODUCTION,0.0425531914893617,"supposition that different tasks enjoy distinguishable models. This reflects the natural diversity of
41"
INTRODUCTION,0.04356636271529889,"problems we encounter in the real world and the varied strategies we employ to solve them. Together,
42"
INTRODUCTION,0.044579533941236066,"alignment and discrimination empower a meta-learner to not only grasp the subtleties of individual
43"
INTRODUCTION,0.04559270516717325,"tasks but also to generalize its learning across a spectrum of challenges. This dual capability can
44"
INTRODUCTION,0.04660587639311044,"makes a meta-learner robust, versatile, and more aligned with the nuanced nature of human learning
45"
INTRODUCTION,0.047619047619047616,"and reasoning. However, existing meta-learning approaches conventionally follows the idea of ""train
46"
INTRODUCTION,0.0486322188449848,"as you test"", to minimize the validation loss [46] of meta-training tasks as meta-objective, where
47"
INTRODUCTION,0.04964539007092199,"supervision signal are directly produced by sample labels. To provide stronger supervision, there
48"
INTRODUCTION,0.05065856129685917,"are works assuming that the task-specific target models of meta-training tasks are available, then
49"
INTRODUCTION,0.05167173252279635,"the meta-training can be supervised by aligning the learned model and the corresponding target
50"
INTRODUCTION,0.05268490374873354,"model, with model weights [51, 52] or knowledge distillation [55]. However, as the target models are
51"
INTRODUCTION,0.05369807497467072,"expensive to learn, and even not available in many real world problems, meta-objectives requiring the
52"
INTRODUCTION,0.0547112462006079,"target models have very restricted applications. Moreover, the importance of discrimination ability of
53"
INTRODUCTION,0.05572441742654509,"meta-learner has not been noticed in the literature.
54"
INTRODUCTION,0.05673758865248227,"To achieve this, we propose contrastive meta-learning (ConML), by directly contrasting the outputs
55"
INTRODUCTION,0.057750759878419454,"of meta-learner in the model space, shown in Figure 1. Conventional contrastive learning (CL) [14,
56"
INTRODUCTION,0.05876393110435663,"48, 44] learns an encoder in unsupervised manner by equipping the model with alignment and
57"
INTRODUCTION,0.05977710233029382,"discrimination ability by exploiting the distinguishable identity of unlabeled samples. Considering
58"
INTRODUCTION,0.060790273556231005,"tasks in meta-learning are also unlabeled but have distinguishable identity, we are inspired to adopt
59"
INTRODUCTION,0.06180344478216818,"similar strategy in meta-learning. ConML exploits tasks as CL exploits unlabeled samples. Positive
60"
INTRODUCTION,0.06281661600810537,"pairs in ConML are different subsets of the same task, while negative pairs are datasets of different
61"
INTRODUCTION,0.06382978723404255,"tasks. In the model space output by meta-learner, inner-task distance can be measured between
62"
INTRODUCTION,0.06484295845997974,"positive pairs and inter-task distance can be measured between negative pairs. The contrastive
63"
INTRODUCTION,0.06585612968591692,"meta-objective is minimizing inner-task distance while maximizing inter-task distance, corresponding
64"
INTRODUCTION,0.0668693009118541,"to the expected alignment and discrimination ability respectively. The proposed ConML is universal
65"
INTRODUCTION,0.06788247213779129,"and cheap, as it can be plugged-in any meta-learning algorithms following the episodic training,
66"
INTRODUCTION,0.06889564336372847,"and does not require additional data nor model training. In this paper, we widely study ConML on
67"
INTRODUCTION,0.06990881458966565,"representative meta-learning algorithms from different categories: optimization-based (e.g., MAML
68"
INTRODUCTION,0.07092198581560284,"[17]), metric-based (e.g., ProtoNet [39]), amortization-based (e.g., Simple CNAPS [6]). We also
69"
INTRODUCTION,0.07193515704154002,"investigate in-context learning [8] with reformulating it into the meta-learning paradigm, and show
70"
INTRODUCTION,0.0729483282674772,"how ConML integrates and helps.
71"
INTRODUCTION,0.07396149949341439,"Our contributions are:
72"
INTRODUCTION,0.07497467071935157,"• We propose to emulate cognitive alignment and discrimination capabilities in meta-learning, to
73"
INTRODUCTION,0.07598784194528875,"narrow down the gap of fast learning ability between meta-learners and humans.
74"
INTRODUCTION,0.07700101317122594,"• We generalize contrastive learning from representation space of unsupervised learning to model
75"
INTRODUCTION,0.07801418439716312,"space of meta-learning. The exploiting task identity as additional supervision benefits meta-learner’s
76"
INTRODUCTION,0.0790273556231003,"fast-adaptation and task-level generalize abilities.
77"
INTRODUCTION,0.0800405268490375,"• ConML is algorithm-agnostic, that can be incorporated into any meta-learning algorithms with
78"
INTRODUCTION,0.08105369807497467,"episodic training. We empirically show ConML can bring universal improvement with cheap
79"
INTRODUCTION,0.08206686930091185,"implementation on a wide range of meta-learning algorithms and in-context learning.
80"
RELATED WORKS,0.08308004052684904,"2
Related Works
81"
LEARNING TO LEARN,0.08409321175278622,"2.1
Learning to Learn
82"
LEARNING TO LEARN,0.0851063829787234,"Meta-learning learns to improve the learning algorithm itself [37], i.e., learns to learn. Popular
83"
LEARNING TO LEARN,0.08611955420466058,"meta-learning approaches can be roughly divided into three categories [7]: optimization-based,
84"
LEARNING TO LEARN,0.08713272543059777,"metric-based and amortization-based. Optimization-based approaches [4, 17, 28] focus on learning
85"
LEARNING TO LEARN,0.08814589665653495,"better optimization strategies for adapting to new tasks. For example MAML [17] learns initial
86"
LEARNING TO LEARN,0.08915906788247213,"model parameters, where few steps of gradient descent can quickly make adaptaion for specific
87"
LEARNING TO LEARN,0.09017223910840932,"tasks. Metric-based approaches [46, 39, 41] leverages learned similarity metrics. For example,
88"
LEARNING TO LEARN,0.0911854103343465,"Prototypical Networks [39] and Matching Networks [46] learn global shared encoders to map training
89"
LEARNING TO LEARN,0.09219858156028368,"set to embeddings, based on which task-specific model can be built. Amortization-based approaches
90"
LEARNING TO LEARN,0.09321175278622088,"[19, 33, 6] seek to learn a shared representation across tasks. They amortize the adaptation process
91"
LEARNING TO LEARN,0.09422492401215805,"by using neural networks to directly infer task-specific parameters from training set. Examples are
92"
LEARNING TO LEARN,0.09523809523809523,"CNPs [19] and CNAPs [33].
93"
LEARNING TO LEARN,0.09625126646403243,"In-context learning (ICL) [8] is designed for large language models, which integrates examples
94"
LEARNING TO LEARN,0.0972644376899696,"(input-output pairs) in a task and a query input into the prompt, thus the language model can answer
95"
LEARNING TO LEARN,0.09827760891590678,"the query. Recently, ICL has been studied as a general approach of learning to learn [2, 18, 47, 1],
96"
LEARNING TO LEARN,0.09929078014184398,"which reduces meta-learning to conventional supervised learning via training a sequence model. It
97"
LEARNING TO LEARN,0.10030395136778116,"considers training set as context to be provided along with the input to predict, forming a sequence to
98"
LEARNING TO LEARN,0.10131712259371833,"feed the model. Training such a model can be viewed as an instance of meta-learning [18].
99"
CONTRASTIVE LEARNING,0.10233029381965553,"2.2
Contrastive Learning
100"
CONTRASTIVE LEARNING,0.1033434650455927,"Contrastive learning is a powerful technique in representation learning [29, 10, 48]. Its primary goal
101"
CONTRASTIVE LEARNING,0.10435663627152988,"is to learn useful representations, which are invariant to unnecessary details, and preserve as much
102"
CONTRASTIVE LEARNING,0.10536980749746708,"information as possible. This is achieved by maximizing alignment and discrimination (uniformity)
103"
CONTRASTIVE LEARNING,0.10638297872340426,"in representation space [48]. In conventional contrastive learning, alignment refers to bringing
104"
CONTRASTIVE LEARNING,0.10739614994934144,"positive pairs (e.g., augmentations of the same sample [54, 22, 5, 21, 10]) closer together in the
105"
CONTRASTIVE LEARNING,0.10840932117527863,"learned representation space. By maximizing alignment, the representations are encouraged to be
106"
CONTRASTIVE LEARNING,0.1094224924012158,"invariant to unneeded noise factors. Discrimination refers to separating negative pairs (e.g., different
107"
CONTRASTIVE LEARNING,0.11043566362715299,"samples) farther. Maximizing discrimination without any other knowledge results in uniformity, i.e.,
108"
CONTRASTIVE LEARNING,0.11144883485309018,"uniform distribution in the representation space. By maximizing discrimination, the representations
109"
CONTRASTIVE LEARNING,0.11246200607902736,"are encouraged to preserve as much information of the data as possible [43, 5], benefiting the
110"
CONTRASTIVE LEARNING,0.11347517730496454,"generalization ability.
111"
META-LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.11448834853090173,"3
Meta-Learning with Contrastive Meta-Objective
112"
META-LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.11550151975683891,"Meta-learning is a methodology considered with ""learning to learn"" machine learning algorithms.
113"
META-LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.11651469098277609,"Define L(D; h) as the loss obtained by evaluating model h on dataset D with function ℓ(y, ˆy) (e.g.,
114"
META-LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.11752786220871327,"cross entropy or mean squared loss), g(; θ) is a meta-learner that maps a dataset D to a model h,
115"
META-LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.11854103343465046,"i.e, h = g(D; θ). Given a distribution of tasks p(τ), where each task τ consists of a training set
116"
META-LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.11955420466058764,"Dtr
τ = {(xτ,i, yτ,i)}n
i=1, and a validation set Dval
τ
= {(xτ,i, yτ,i)}m
i=n+1, the goal of meta-learning is
117"
META-LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.12056737588652482,"to learn g(; θ) to perform well on new task τ ′ sampled from p(τ ′), evaluated by L(Dval
τ ′ ; g(Dtr
τ ′; θ)).
118"
A UNIFIED VIEW OF EPISODIC TRAINING,0.12158054711246201,"3.1
A Unified View of Episodic Training
119"
A UNIFIED VIEW OF EPISODIC TRAINING,0.12259371833839919,"We aim to introduce ""learning to align and discriminate"" to
universally improve the meta-learning process. The most
conventional way of meta-training is taking the validation
loss as meta-objective to optimize θ:"
A UNIFIED VIEW OF EPISODIC TRAINING,0.12360688956433637,"min
θ
Eτ∼p(τ)L(Dval
τ ; g(Dtr
τ; θ)).
(1)"
A UNIFIED VIEW OF EPISODIC TRAINING,0.12462006079027356,"Different meta-learning algorithms tailor the function in-
side g, while sharing the same episodic meta-training to
achieve (1). Shown as Algorithm 1, in each episode, B
tasks are sampled from p(τ) to form a batch b, and valida-
tion loss of each task is aggregated as the supervision signal
Lv =
1
B
P"
A UNIFIED VIEW OF EPISODIC TRAINING,0.12563323201621074,"τ∈b L(Dval
τ ; g(Dtr
τ; θ)) to update θ. By specify-
ing the function inside g, Algorithm 1 can generalize the
meta-training process of different meta-learning algorithms."
A UNIFIED VIEW OF EPISODIC TRAINING,0.12664640324214793,"Algorithm 1 Mini-Batch Episodic
Meta-Training (Conventional)"
A UNIFIED VIEW OF EPISODIC TRAINING,0.1276595744680851,while Not converged do
A UNIFIED VIEW OF EPISODIC TRAINING,0.1286727456940223,"Sample a batch of tasks b
∼
pB(τ).
for All τ ∈b do"
A UNIFIED VIEW OF EPISODIC TRAINING,0.12968591691995948,"Get task-specific model hτ =
g(Dtr
τ; θ);
Get validation loss L(Dval
τ ; hτ);
end for
Lv = 1 B
P"
A UNIFIED VIEW OF EPISODIC TRAINING,0.13069908814589665,"τ∈b L(Dval
τ ; g(Dtr
τ; θ))
Update θ by θ ←θ −∇θLv.
end while 120"
A UNIFIED VIEW OF EPISODIC TRAINING,0.13171225937183384,"Table 1: Specifications of ConML.
Category
Examples
g(D; θ)
ψ(g(D; θ))"
A UNIFIED VIEW OF EPISODIC TRAINING,0.13272543059777103,"Optimization
-based"
A UNIFIED VIEW OF EPISODIC TRAINING,0.1337386018237082,"MAML[17],
Reptile[28]"
A UNIFIED VIEW OF EPISODIC TRAINING,0.1347517730496454,"Update model weights
θ −∇θL(D; hθ)
θ −∇θL(D; hθ)"
A UNIFIED VIEW OF EPISODIC TRAINING,0.13576494427558258,"Metric
-based"
A UNIFIED VIEW OF EPISODIC TRAINING,0.13677811550151975,"ProtoNet[39],
MatchNet[46]"
A UNIFIED VIEW OF EPISODIC TRAINING,0.13779128672745694,"Build classifier with
{({fθ(xi)}xi∈Dj, j)}N
j=1"
A UNIFIED VIEW OF EPISODIC TRAINING,0.13880445795339413,"Concatenate
[
1
|Dj|
P"
A UNIFIED VIEW OF EPISODIC TRAINING,0.1398176291793313,"xi∈Dj fθ(xi)]N
j=1"
A UNIFIED VIEW OF EPISODIC TRAINING,0.1408308004052685,"Amortization
-based"
A UNIFIED VIEW OF EPISODIC TRAINING,0.14184397163120568,"CNPs[19],
CNAPs[33]"
A UNIFIED VIEW OF EPISODIC TRAINING,0.14285714285714285,"Map D to model weights
by Hθ(D)
Hθ(D)"
A UNIFIED VIEW OF EPISODIC TRAINING,0.14387031408308004,"Specifications of optimization-based, metric-based and amortization-based algorithms are summa-
121"
A UNIFIED VIEW OF EPISODIC TRAINING,0.14488348530901723,"rized in Table 1.
122"
A UNIFIED VIEW OF EPISODIC TRAINING,0.1458966565349544,"We design ConML to be integrated with Algorithm 1 without specifying g, thus to be universally
123"
A UNIFIED VIEW OF EPISODIC TRAINING,0.1469098277608916,"applicable for meta-learning algorithms following the episodic manner. In Section 3.2, we introduce
124"
A UNIFIED VIEW OF EPISODIC TRAINING,0.14792299898682879,"how to measure the objective. Then in Section 3.3, we introduce specifications of ConML on a wide
125"
A UNIFIED VIEW OF EPISODIC TRAINING,0.14893617021276595,"range of meta-learning algorithms.
126"
INTEGRATION WITH EPISODIC META-TRAINING,0.14994934143870314,"3.2
Integration with Episodic Meta-Training
127"
INTEGRATION WITH EPISODIC META-TRAINING,0.15096251266464034,"To equip meta-learners with the desired alignment and discrimination ability, we design contrastive
128"
INTEGRATION WITH EPISODIC META-TRAINING,0.1519756838905775,"meta-objective measured in the output space of meta-learner, i.e., the model space of h. Alignment
129"
INTEGRATION WITH EPISODIC META-TRAINING,0.1529888551165147,"is achieved by minimizing inner-task distance, which is the distance among models generated from
130"
INTEGRATION WITH EPISODIC META-TRAINING,0.1540020263424519,"different subsets of the same task. Discrimination is achieved by maximize the inter-task distance,
131"
INTEGRATION WITH EPISODIC META-TRAINING,0.15501519756838905,"which is the distance among models generated from different tasks. Here we introduce how to
132"
INTEGRATION WITH EPISODIC META-TRAINING,0.15602836879432624,"measure the contrastive objective and perform optimization.
133"
INTEGRATION WITH EPISODIC META-TRAINING,0.15704154002026344,"Obtaining Model Representation.
To train the meta-learner g, the distances Din, Dout are mea-
134"
INTEGRATION WITH EPISODIC META-TRAINING,0.1580547112462006,"sured in the output space of g, i.e., the model space H. A feasible way is to first represent model
135"
INTEGRATION WITH EPISODIC META-TRAINING,0.1590678824721378,"h = g(D; θ) ∈H as fixed length vectors e ∈Rd, then measure by explicit distance function ϕ(·, ·)
136"
INTEGRATION WITH EPISODIC META-TRAINING,0.160081053698075,"(e.g., cosine distance). Note that H is algorithm-specific. Here we only introduce a projection
137"
INTEGRATION WITH EPISODIC META-TRAINING,0.16109422492401215,"ψ : H →Rd to obtain model representations e = ψ(h). The H and ψ will be elucidated and
138"
INTEGRATION WITH EPISODIC META-TRAINING,0.16210739614994935,"specified for different meta-learning algorithms in Section 3.3.
139"
INTEGRATION WITH EPISODIC META-TRAINING,0.16312056737588654,"Obtaining Inner-Task Distance.
During meta-training, Dtr
τ ∪Dval
τ
contains all the available in-
140"
INTEGRATION WITH EPISODIC META-TRAINING,0.1641337386018237,"formation about task τ. The meta-learner is expected to learn similar model given any subset κ of
141"
INTEGRATION WITH EPISODIC META-TRAINING,0.1651469098277609,"the task. Meanwhile those models from subsets are expected to be similar to the model learned
142"
INTEGRATION WITH EPISODIC META-TRAINING,0.1661600810536981,"from the full supervision Dtr
τ ∪Dval
τ . We design the following inner-task distance to minimize that
143"
INTEGRATION WITH EPISODIC META-TRAINING,0.16717325227963525,"encourages g to learn a generalizable model even from a set containing only few or biased samples.
144"
INTEGRATION WITH EPISODIC META-TRAINING,0.16818642350557245,"For ∀κ ⊆Dtr
τ ∪Dval
τ , we expect eκ
τ = e∗
τ, where eκ
τ = ψ(g(κ; θ)), e∗
τ = ψ(g(Dtr
τ ∪Dval
τ ; θ)). The
145"
INTEGRATION WITH EPISODIC META-TRAINING,0.1691995947315096,"inner-task distance Din
τ of task τ is defined as:
146"
INTEGRATION WITH EPISODIC META-TRAINING,0.1702127659574468,"Din
τ = 1 K XK"
INTEGRATION WITH EPISODIC META-TRAINING,0.171225937183384,"k=1 ϕ(eκk
τ , e∗
τ), s.t., eκk
τ
∼πκ(Dtr
τ ∪Dval
τ ),
(2)"
INTEGRATION WITH EPISODIC META-TRAINING,0.17223910840932116,"where {κk}K
k=1 are K subsets sampled from Dtr
τ ∪Dval
τ
by certain sampling strategy πκ. In each
147"
INTEGRATION WITH EPISODIC META-TRAINING,0.17325227963525835,"episode given a batch of task b containing B tasks, inner-task distance is averaged by Din =
148"
"B
P",0.17426545086119555,"1
B
P"
"B
P",0.1752786220871327,"τ∈b Din
τ .
149"
"B
P",0.1762917933130699,"Obtaining Inter-Task Distance.
Since the goal of meta-learning is improving the performance on
150"
"B
P",0.1773049645390071,"unseen tasks, it is important that the g is generalizable for diverse tasks. With a natural supposition
151"
"B
P",0.17831813576494426,"that different tasks enjoy different task-specific models, it is necessary that g can learn different
152"
"B
P",0.17933130699088146,"models from different tasks, i.e., discrimination. We define the following inter-task distance to
153"
"B
P",0.18034447821681865,"maximize to improve the task-level generalizability of g. For two tasks τ ̸= τ ′ during meta-training,
154"
"B
P",0.18135764944275581,"we expect to maximize the distance between e∗
τ and e∗
τ ′. To be practical under the mini-batch episodic
155"
"B
P",0.182370820668693,"training paradigm, we consider to measure inter-task distance among a batch of tasks:
156"
"B
P",0.1833839918946302,"Dout =
1
B(B −1) X τ∈b X"
"B
P",0.18439716312056736,"τ ′∈b\τ ϕ(e∗
τ, e∗
τ ′).
(3)"
"B
P",0.18541033434650456,"Training Procedure.
ConML mea-
sures Din by (2) and Dout by (3) in each
episode, and minimizes a combination
of the validation loss Lv and contrastive
meta-objective Din −Dout:"
"B
P",0.18642350557244175,"L = Lv + λ(Din −Dout).
(4)"
"B
P",0.18743667679837892,"The training procedure of ConML is pro-
vided in Algorithm 2. Comparing with
Algorithm 1, ConML introduces addi-
tional computation ψ(g(D; θ)) for K +1
times in each episode. Note that we im-
plement ψ with very cheap function such
as obtaining model weights (or a sin-
gle probing, i.e., feeding-forward, for
ICL), and g(D; θ) already exists in Al-
gorithm 1 while multiple g(D; θ) can be
parallel-computed. ConML could have
very comparable time consumption."
"B
P",0.1884498480243161,"Algorithm 2 Meta-Learning with Contrastive Meta-Object
(ConML)"
"B
P",0.1894630192502533,while Not converged do
"B
P",0.19047619047619047,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
"B
P",0.19148936170212766,"for k = 1, 2, · · · , K do"
"B
P",0.19250253292806485,"Sample κk from πκ(Dtr
τ ∪Dval
τ );
Get model representation eκk
τ
= ψ(g(κk; θ));
end for
Get model representation e∗
τ = ψ(g(Dtr
τ ∪Dval
τ ; θ));
Get inner-task distance Din
τ by (2);
Get task-specific model hτ = g(Dtr
τ; θ);
Get validation loss L(Dval
τ ; hτ);
end for
Get Din = 1 B
P"
"B
P",0.19351570415400202,"τ∈b Din
τ and Dout by (3);
Get loss L by (4);
Update θ by θ ←θ −∇θL.
end while 157"
INSTANTIATIONS OF CONML,0.1945288753799392,"3.3
Instantiations of ConML
158"
INSTANTIATIONS OF CONML,0.1955420466058764,"Here we demonstrate specifications of H and ψ(g(D, θ)) to obtain model representation to implement
159"
INSTANTIATIONS OF CONML,0.19655521783181357,"ConML. We show examples on representative meta-learning algorithms from different categories:
160"
INSTANTIATIONS OF CONML,0.19756838905775076,"optimization-based, metric-based and amortization-based. They are explicitly represented by model
161"
INSTANTIATIONS OF CONML,0.19858156028368795,"weights, summarized in Table 1.
162"
INSTANTIATIONS OF CONML,0.19959473150962512,"With Optimization-Based Methods. The representative algorithm of optimization-based meta-
163"
INSTANTIATIONS OF CONML,0.2006079027355623,"learning is MAML. It meta-learns an initialization from where gradient steps are taken to learn
164"
INSTANTIATIONS OF CONML,0.2016210739614995,"task-specific models, i.e., g(D; θ) = hθ−∇θL(D;hθ). As g directly generates the model weights, we
165"
INSTANTIATIONS OF CONML,0.20263424518743667,"explicitly take the model weights as model representation. The representation of model learned
166"
INSTANTIATIONS OF CONML,0.20364741641337386,"by g given a dataset D is ψ(g(D; θ)) = θ −∇θL(D; hθ). Note that there are optimization-based
167"
INSTANTIATIONS OF CONML,0.20466058763931105,"meta-learning algorithms which are based on first-order approximation of MAML, thus they do not
168"
INSTANTIATIONS OF CONML,0.20567375886524822,"strictly follows Algorithm 1 to minimize validation loss (e.g., FOMAML [17] and Reptile [28]).
169"
INSTANTIATIONS OF CONML,0.2066869300911854,"ConML can also be incorporated as long as it follows the episodic manner.
170"
INSTANTIATIONS OF CONML,0.2077001013171226,"With Metric-Based Methods. Metric-based algorithms are feasible for classification tasks. Given
171"
INSTANTIATIONS OF CONML,0.20871327254305977,"dataset D of a N-way classification task, metric-based algorithms can be summarized as classifying
172"
INSTANTIATIONS OF CONML,0.20972644376899696,"according to distances with {{fθ(xi)}xi∈Dj}N
j=1 and corresponding labels, where fθ is a meta-
173"
INSTANTIATIONS OF CONML,0.21073961499493415,"learned encoder and Dj is the set of inputs belongs to class j. We design to represent this metric-
174"
INSTANTIATIONS OF CONML,0.21175278622087132,"based classifier with the concatenation of mean embedding of each class in label-aware order. For
175"
INSTANTIATIONS OF CONML,0.2127659574468085,"example, ProtoNet [39] computes the prototype cj , i.e., mean embedding of samples in each class.
176"
INSTANTIATIONS OF CONML,0.2137791286727457,"cj =
1
|Dj|
P"
INSTANTIATIONS OF CONML,0.21479229989868287,"(xi,yi)∈Dj fθ(xi). Then classifier hθ,D is built by giving prediction p(y = j | x) =
177"
INSTANTIATIONS OF CONML,0.21580547112462006,"exp(−d(fθ(x), cj))/ P"
INSTANTIATIONS OF CONML,0.21681864235055726,"j′ exp(−d(fθ(x), cj′)). As the outcome model hθ,D depends on D through
178"
INSTANTIATIONS OF CONML,0.21783181357649442,"{cj}N
j=1 and corresponding labels, the representation is specified as ψ(g(D; θ)) = [c1|c2| · · · |cN],
179"
INSTANTIATIONS OF CONML,0.2188449848024316,"where [·|·] means concatenation.
180"
INSTANTIATIONS OF CONML,0.2198581560283688,"With Amortization-Based Methods. Amortization-based approaches meta-learns a hypernetwork
181"
INSTANTIATIONS OF CONML,0.22087132725430597,"Hθ, which aggregates information from D to task-specific parameter α and serves as weights of
182"
INSTANTIATIONS OF CONML,0.22188449848024316,"main-network h, resulting in task-specific model hα. For example, Simple CNAPS [6] adopts the
183"
INSTANTIATIONS OF CONML,0.22289766970618036,"hypernetwork to generate only a small amount of task-specific parameter, which performs feature-wise
184"
INSTANTIATIONS OF CONML,0.22391084093211752,"linear modulation (FiLM) on convolution channels of the main-network. For contrasting we represent
185"
INSTANTIATIONS OF CONML,0.22492401215805471,"hα by α, i.e., the output of hypernetwork Hθ: ψ(g(D; θ)) = Hθ(D). The detailed procedures of
186"
INSTANTIATIONS OF CONML,0.2259371833839919,"different meta-learning algorithms with ConML are provided in Appendix A.
187"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.22695035460992907,"4
In-Context Learning with Contrastive Meta-Objective
188"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.22796352583586627,"In-context learning (ICL) is first proposed for large language models [8], where examples in a task
189"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.22897669706180346,"are integrated into the prompt (input-output pairs) and given a new query input, the language model
190"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.22998986828774062,"can generate the corresponding output. This approach allows pre-trained model to address new tasks
191"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.23100303951367782,"without fine-tuning the model. For example, given ""happy->positive; sad->negative; blue->"", the
192"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.232016210739615,"model can output ""negative"", while given ""green->cool; yellow->warm; blue->"" the model can
193"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.23302938196555217,"output ""cool"". ICL has the ability to learn from the prompt. Training ICL can be viewed as learning
194"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.23404255319148937,"to learn, like meta-learning [25, 18, 24]. More generally, the input and output are not necessarily
195"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.23505572441742653,"to be natural language. In ICL, a sequence model Tθ (typically transformer [45]) is trained to map
196"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.23606889564336372,"sequence [x1, y1, x2, y2, · · · , xm−1, ym−1, xm] (prompt prefix) to prediction ym. Given distribution
197"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.23708206686930092,"P of training prompt t, then training ICL follows an auto-regressive manner:
198"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.23809523809523808,"min
θ
Et∼P (t)
1
m Xm−1"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.23910840932117527,"i=0 ℓ(yt,i+1, Tθ([xt,1, yt,1, · · · , xt,i+1])).
(5)"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.24012158054711247,"It has been mentioned that the training of ICL can be viewed as an instance of meta-learning [18, 2]
199"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.24113475177304963,"as Tθ learns to learn from prompt. In this section we first formally reformulate Tθ to meta-learner
200"
IN-CONTEXT LEARNING WITH CONTRASTIVE META-OBJECTIVE,0.24214792299898683,"g(; θ), then introduce how ConML can be integrated with ICL.
201"
A META-LEARNING REFORMULATION,0.24316109422492402,"4.1
A Meta-learning Reformulation
202"
A META-LEARNING REFORMULATION,0.24417426545086118,"Denote a sequentialized D as ⃗D where the sequentializer is default to bridge p(τ) and P(t). Then
203"
A META-LEARNING REFORMULATION,0.24518743667679838,"the prompt [xτ,1, yτ,1, · · · , xτ,m, yτ,m] can be viewed as ⃗
Dtr
τ which is providing task-specific infor-
204"
A META-LEARNING REFORMULATION,0.24620060790273557,"mation. Note that ICL does not specify an explicit output model h(x) = g(D; θ)(x); instead, this
205"
A META-LEARNING REFORMULATION,0.24721377912867273,"procedure exists only implicitly through the feeding-forward of the sequence model, i.e., task-specific
206"
A META-LEARNING REFORMULATION,0.24822695035460993,"prediction is given by g([ ⃗D, x]; θ). Thus we can reformulate the training of ICL (5) as:
207"
A META-LEARNING REFORMULATION,0.24924012158054712,"min
θ
Eτ∼p(τ)
1
m Xm−1"
A META-LEARNING REFORMULATION,0.2502532928064843,"i=0 ℓ(yτ,i+1, g([ ⃗Dτ,0:i, xτ,i+1]; θ)).
(6)"
A META-LEARNING REFORMULATION,0.2512664640324215,"Equation (6) can be regarded as the validation loss (1) in meta-learning, where each task in each
208"
A META-LEARNING REFORMULATION,0.25227963525835867,"episode is sampled multiple times to form Dval
τ and Dtr
τ in an auto-regressive manner. The training
209"
A META-LEARNING REFORMULATION,0.25329280648429586,"of ICL thus follows the episodic meta-training (Algorithm 1), where the validation loss with deter-
210"
A META-LEARNING REFORMULATION,0.25430597771023306,"mined Dtr
τ and Dval
τ : L(Dval
τ ; g(Dtr
τ; θ)), is replaced by loss validated in the auto-regressive manner:
211"
M,0.2553191489361702,"1
m
Pm−1
i=0 ℓ(yτ,i+1, g([ ⃗Dτ,0:i, xτ,i+1]; θ)).
212"
INTEGRATION WITH ICL,0.2563323201621074,"4.2
Integration with ICL
213"
INTEGRATION WITH ICL,0.2573454913880446,"Since the training of ICL could be reformulated as episodic meta-training, the three steps to measure
214"
INTEGRATION WITH ICL,0.25835866261398177,"ConML proposed in Section 3.2 can be also adopted for ICL, but the first step to obtain model
215"
INTEGRATION WITH ICL,0.25937183383991896,"representation ψ(g(D, θ)) needs modification. Due to the absence of an inner learning procedure for
216"
INTEGRATION WITH ICL,0.26038500506585616,"a predictive model for prediction h(x) = g(D; θ)(x), representation by explicit model weights of h
217"
INTEGRATION WITH ICL,0.2613981762917933,"is not feasible for ICL.
218"
INTEGRATION WITH ICL,0.2624113475177305,"To represent what g learns from D, we design to incorporate ⃗D with a dummy input u, which
219"
INTEGRATION WITH ICL,0.2634245187436677,"functions as a probe and its corresponding output can be readout as representation:
220"
INTEGRATION WITH ICL,0.26443768996960487,"ψ(g(D; θ)) = g([ ⃗D, u]; θ),
(7)"
INTEGRATION WITH ICL,0.26545086119554206,"where u is constrained to be in the same shape as x, and has consistent value in an episode. The
221"
INTEGRATION WITH ICL,0.26646403242147926,"complete algorithm of ConML for ICL is provided in Appendix A. From the perspective of learning
222"
INTEGRATION WITH ICL,0.2674772036474164,"to learn, ConML encourages ICL to align and discriminate like it does for conventional meta-learning,
223"
INTEGRATION WITH ICL,0.2684903748733536,"while the representations to evaluate inner- and inter- task distance are obtained by probing output
224"
INTEGRATION WITH ICL,0.2695035460992908,"rather than explicit model weights. Thus, incorporating ConML into the training process of ICL
225"
INTEGRATION WITH ICL,0.270516717325228,"benefits the fast-adaptation and task-level generalization ability. From the perspective of supervised
226"
INTEGRATION WITH ICL,0.27152988855116517,"learning, ConML is performing unsupervised data augmentation that it introduces the dummy input
227"
INTEGRATION WITH ICL,0.2725430597771023,"and contrastive objective as additional supervision to train ICL.
228"
EXPERIMENTS,0.2735562310030395,"5
Experiments
229"
EXPERIMENTS,0.2745694022289767,"In this secrion, we first empirically investigate the alignment and discrimination empowered by
230"
EXPERIMENTS,0.2755825734549139,"ConML. Then we show the effect of ConML that it significantly improve meta-learning performance
231"
EXPERIMENTS,0.2765957446808511,"on a wide range of meta-learning algorithms on few-shot image classification, and the effect of
232"
EXPERIMENTS,0.27760891590678827,"ConML-ICL with in-context learning general functions. Additionally, by applying ConML we provide
233"
EXPERIMENTS,0.2786220871327254,"a SOTA approach for few-shot molecular property prediction problem, provided in Appendix B.
234"
EXPERIMENTS,0.2796352583586626,"Code is provided in supplementary materials.
235"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2806484295845998,"5.1
Impact of Alignment and Discrimination
236"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.281661600810537,"There are two important questions to understand the way ConML works: First, does ConML equip
237"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2826747720364742,"meta-learners with better alignment and discrimination as expected? Second, what is the contribution
238"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.28368794326241137,"of inner-task and inter-task distance respectively? We take ConML-MAML as example and investigate
239"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2847011144883485,"above questions with few-shot regression problem following the same settings in [17], where each
240"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2857142857142857,"task involves regressing from the input to the output of a sine wave. We use this synthetic regression
241"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2867274569402229,Table 2: Meta-testing and clustering performance of few-shot sinusoidal regression.
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2877406281661601,"Method
MSE (5-shot)
MSE (10-shot)
Silhouette
DBI
CHI"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2887537993920973,"MAML
.6771 ± .0377
.0678 ± .0022
.1068 ± .0596
.0678 ± .0021
31.55 ± 2.52"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.28976697061803447,ConML-MAML .3935 ± .0100 .0397 ± .0009 .1945 ± .0621 .0397 ± .0009 39.22 ± 2.61
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2907801418439716,"dataset to be able to sample data and vary the distribution as needed for investigation. The implement
242"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2917933130699088,"of ConML-MAML is consistent with Section 5.2. Firstly the meta-testing performance in Table 2
243"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.292806484295846,"shows that ConML is effective for the regression problem.
244"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2938196555217832,"(a) Model distribution of MAML.
(b) Inner-task distance distribution.
(c) Varying test shots."
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2948328267477204,"(d) Model distribution of ConML-
MAML."
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.29584599797365757,"(e) Inter-task distance distribution.
(f) Varying test distribution."
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2968591691995947,"Figure 2: Investigating the way ConML works.
Clustering.
If ConML enhances the alignment and discrimination abilities, ConML-MAML can
245"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2978723404255319,"generate more similar models from different subsets of the same task, while generating more separable
246"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2988855116514691,"models from different tasks. This can be verified by evaluating the clustering performance for model
247"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.2998986828774063,"representations e. During meta-testing, we randomly sample 10 different tasks, inside each we sample
248"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3009118541033435,"10 different subsets, each one contains N = 10 samples. Taking these 100 different Dtr as input,
249"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.30192502532928067,"meta-learner generates 100 models. Figure 2(a) and 2(d) show the visualization of model distribution.
250"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3029381965552178,"It can be obviously observed ConML-MAML performs better alignment and discrimination than
251"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.303951367781155,"MAML. To quantity the results, we also evaluate the supervised clustering performance, where task
252"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3049645390070922,"identity is used as label. Table 2 shows the supervised clustering performance of different metrics:
253"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3059777102330294,"Silhouette score [35], Davies-Bouldin index (DBI) [15] and Calinski-Harabasz index (CHI) [9],
254"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3069908814589666,"where ConML-MAML shows much better performance.
255"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3080040526849038,"Decoupling Inner- and Inter-Task Distance.
In conventional unsupervised contrastive learning,
256"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3090172239108409,"where objective only relies on contrasting of positive pairs and negative pairs, positive and negative
257"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3100303951367781,"pairs are both necessary to avoid learning representations without useful information. However, in
258"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3110435663627153,"ConML, there is validation loss Lv plays a necessary and fundamental role in ""learning to learn"",
259"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3120567375886525,"and the contrastive objective is introduced as additional supervision to enhance alignment and
260"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3130699088145897,"discrimination. Thus, distance of positive pairs (Din) and negative pairs (Dout) in ConML could be
261"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3140830800405269,"decoupled and incorporated with Lv respectively. We aim to understand how Din and Dout contributes
262"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.315096251266464,"respectively. This gives birth to two variants of ConML: in-MAML which optimize Lv and Din,
263"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3161094224924012,"out-MAML which optimize Lv and Dout. During meta-testing, we randomly sample 1000 different
264"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3171225937183384,"tasks, inside each we sample 10 different subsets each one contains N = 10 samples. We aggregate
265"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3181357649442756,"different subsets from the same task to form a N = 100 set to obtaining e∗
τ for each task. The
266"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3191489361702128,"distribution of Din and Dout are shown in Figure 2(b) and 2(e) respectively, where the dashed lines
267"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.32016210739615,"are mean values. We can find that: the alignment and discrimination ability corresponds to optimizing
268"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3211752786220871,"Din and Dout respectively; the alignment and discrimination capabilities are generalizable; ConML
269"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3221884498480243,"shows the couple of both capabilities. Figure 2(c) shows the testing performance given different
270"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3232016210739615,"numbers of examples per task (shot), while the meta-leaner is trained with fixed N = 10. We can find
271"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3242147922998987,"that the improvement brought by Din is much more significant than Dout under few-shot scenario,
272"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3252279635258359,"which indicates that alignment is closely related to the fast-adaptation ability of the meta-learner.
273"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3262411347517731,Table 3: Meta-testing accuracy on miniImageNet.
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3272543059777102,"Category
Algorithm
Setting (5-way)
w/o ConML
ConML-
Relative Gain Relative Time"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3282674772036474,"Optimization-
Based"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3292806484295846,"MAML
1-shot
48.75 ± 1.25 56.25 ± 0.94
9.16%
1.1×
5-shot
64.50 ± 1.02 67.37 ± 0.97"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3302938196555218,"FOMAML
1-shot
48.12 ± 1.40 57.64 ± 1.29
12.65%
1.2×
5-shot
63.86 ± 0.95 68.50 ± 0.78"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.331306990881459,"Reptile
1-shot
49.21 ± 0.60 52.82 ± 1.06
5.58%
1.5×
5-shot
64.31 ± 0.97 67.04 ± 0.81"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3323201621073962,"Metric-
Based"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3333333333333333,"MatchNet
1-shot
43.92 ± 1.03 48.75 ± 0.88
10.59%
1.2×
5-shot
56.26 ± 0.90 62.04 ± 0.89"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3343465045592705,"ProtoNet
1-shot
48.90 ± 0.84 51.03 ± 0.91
3.31%
1.2×
5-shot
65.69 ± 0.96 67.35 ± 0.72"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3353596757852077,"Amortization-
Based
SCNAPs
1-shot
53.14 ± 0.88 55.73 ± 0.86
3.12%
1.3×
5-shot
70.43 ± 0.76 71.70 ± 0.71"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3363728470111449,"Figure 2(f) shows the out-of-distribution testing performance. While meta-trained on tasks with
274"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3373860182370821,"amplitudes that uniformly distribute on [0.1, 5], meta-testing is performed on tasks with amplitudes
275"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3383991894630192,"that uniformly distribute on [0.1 + δ, 5 + δ] (the distribution shift δ is indicated as x-axis). We can
276"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3394123606889564,"find that the improvement brought by Dout is notably more significant as the distribution gap grows
277"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3404255319148936,"than Din. This indicates that discrimination is closely related to the task-level generalization ability
278"
IMPACT OF ALIGNMENT AND DISCRIMINATION,0.3414387031408308,"of meta-learner. ConML takes both advantages brought by Din and Dout.
279"
FEW-SHOT IMAGE CLASSIFICATION,0.342451874366768,"5.2
Few-Shot Image Classification
280"
FEW-SHOT IMAGE CLASSIFICATION,0.3434650455927052,"To evaluate ConML on conventional meta-learning approaches, we follow existing works [46, 17, 39,
281"
FEW-SHOT IMAGE CLASSIFICATION,0.3444782168186423,"28, 6] to evaluate the meta-learning performance with few-shot image classification problem. We
282"
FEW-SHOT IMAGE CLASSIFICATION,0.3454913880445795,"consider representative meta-learning algorithms from different categories, including optimization-
283"
FEW-SHOT IMAGE CLASSIFICATION,0.3465045592705167,"based: MAML [17], FOMAML [17], Reptile [28]; metric-based: MatchNet [46], ProtoNet [39];
284"
FEW-SHOT IMAGE CLASSIFICATION,0.3475177304964539,"and amortization-based: SCNAPs (Simple CNAPS) [6]. We evaluate their original meta-learning
285"
FEW-SHOT IMAGE CLASSIFICATION,0.3485309017223911,"performance (w/o ConML) and performance meta-trained with the proposed ConML (ConML-). The
286"
FEW-SHOT IMAGE CLASSIFICATION,0.3495440729483283,"implementation of ConML- follows the general Algorithm 2 and the specification for corresponding
287"
FEW-SHOT IMAGE CLASSIFICATION,0.3505572441742654,"category in Section 3.3.
288"
FEW-SHOT IMAGE CLASSIFICATION,0.3515704154002026,"Datasets and Settings.
We consider two few-shot image classification benchmarks: miniImageNet
289"
FEW-SHOT IMAGE CLASSIFICATION,0.3525835866261398,"[46] and tieredImageNet [32]. 5-way 1-shot and 5-way 5-shot tasks are trained and evaluated
290"
FEW-SHOT IMAGE CLASSIFICATION,0.353596757852077,"respectively. Note that we focus on the improvement comparing ConML- and the corresponding
291"
FEW-SHOT IMAGE CLASSIFICATION,0.3546099290780142,"algorithm without ConML, rather than performance comparison across different algorithms. So we
292"
FEW-SHOT IMAGE CLASSIFICATION,0.3556231003039514,"conduct the experiment on each algorithm following the originally reported settings. All baselines
293"
FEW-SHOT IMAGE CLASSIFICATION,0.3566362715298885,"share the same settings of hyperparameters related to the measurement of ConML: task batch
294"
FEW-SHOT IMAGE CLASSIFICATION,0.3576494427558257,"size B = 32, inner-task sampling K = 1 and πκ(Dtr
τ ∪Dval
τ ) = Dtr
τ, ϕ(a, b) = 1 −a·b/∥a∥∥b∥
295"
FEW-SHOT IMAGE CLASSIFICATION,0.3586626139817629,"(cosine distance) and λ = 0.1. For other settings of hyperparameters about model architecture and
296"
FEW-SHOT IMAGE CLASSIFICATION,0.3596757852077001,"training procedure, each baseline is consistent with its originally reported. Note that K = 1 and
297"
FEW-SHOT IMAGE CLASSIFICATION,0.3606889564336373,"πκ(Dtr
τ ∪Dval
τ ) = Dtr
τ is the most simple and efficient implementation, provided as Efficient-ConML
298"
FEW-SHOT IMAGE CLASSIFICATION,0.3617021276595745,"in Appendix A. In this case, considering the consumption of feeding-forward neural networks in each
299"
FEW-SHOT IMAGE CLASSIFICATION,0.36271529888551163,"task, Algorithm 1 takes h = g(Dtr
τ; θ) and L(Dval
τ ; h), while ConML only introduces an additional
300"
FEW-SHOT IMAGE CLASSIFICATION,0.3637284701114488,"g(Dtr
τ ∪Dval
τ ; θ), which results in very comparable time consumption.
301"
FEW-SHOT IMAGE CLASSIFICATION,0.364741641337386,"Results.
Table 3 and 4 show the results on miniImageNet and tieredImageNet respectively. The
302"
FEW-SHOT IMAGE CLASSIFICATION,0.3657548125633232,"relative gain is calculated in terms of the summation of 1-shot and 5-shot accuracy. The relative
303"
FEW-SHOT IMAGE CLASSIFICATION,0.3667679837892604,"time is comparing the total time consumption of meta-training. Significant relative gain and very
304"
FEW-SHOT IMAGE CLASSIFICATION,0.3677811550151976,"comparable relative time consumption show that ConML brings universal improvement on different
305"
FEW-SHOT IMAGE CLASSIFICATION,0.36879432624113473,"meta-learning algorithms with cheap implementation.
306"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3698074974670719,"5.3
In-Context Learning General Functions
307"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3708206686930091,"Following [18], we investigate ConML on ICL by learning to learn synthetic functions including
308"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3718338399189463,"linear regression (LR), sparse linear regression (SLR), decision tree (DT) and 2-layer neural network
309"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3728470111448835,"with ReLU activation (NN). We train the GPT-2 [30]-like transformer for each function with ICL and
310"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3738601823708207,"ConML-ICL respectively and compare the inference (meta-testing) performance. We follow the same
311"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.37487335359675783,"model structure, data generation and training settings [18]. We implement ConML-ICL with K = 1
312"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.375886524822695,"and πκ([x1, y1, · · · , xn, yn]) = [x1, y1, · · · , x⌊n"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3768996960486322,"2 ⌋, y⌊n"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3779128672745694,"2 ⌋]. To obtain the implicit representation (7),
313"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3789260385005066,"we sample u from a standard normal distribution (the same with x’s distribution) independently in
314"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3799392097264438,Table 4: Meta-testing accuracy on tieredImageNet.
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.38095238095238093,"Category
Algorithm
Setting (5-way)
w/o ConML
ConML-
Relative Gain Relative Time"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3819655521783181,"Optimization-
Based"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3829787234042553,"MAML
1-shot
51.39 ± 1.31 58.75 ± 1.45
10.07%
1.1×
5-shot
68.25 ± 0.98 72.94 ± 0.98"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3839918946301925,"FOMAML
1-shot
51.44 ± 1.51 58.21 ± 1.22
9.78%
1.2×
5-shot
68.32 ± 0.95 73.26 ± 0.78"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3850050658561297,"Reptile
1-shot
47.88 ± 1.62 55.01 ± 1.28
10.78%
1.5×
5-shot
65.10 ± 1.13 70.15 ± 1.00"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3860182370820669,"Metric-
Based"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.38703140830800403,"MatchNet
1-shot
48.74 ± 1.06 53.29 ± 1.05
11.00%
1.2×
5-shot
61.30 ± 0.94 67.86 ± 0.77"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3880445795339412,"ProtoNet
1-shot
52.50 ± 0.96 54.62 ± 0.79
3.94%
1.2×
5-shot
71.03 ± 0.74 73.78 ± 0.75"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3890577507598784,"Amortization-
Based
SCNAPs
1-shot
62.88 ± 1.04 65.06 ± 0.95
2.91%
1.3×
5-shot
79.82 ± 0.87 81.79 ± 0.80"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3900709219858156,Table 5: Performance comparison of ConML-ICL and ICL.
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3910840932117528,"Function (max prompt len.)
LR (10 shot)
SLR (10 shot)
DT (20 shot)
NN (40 shot)"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.39209726443769,"Rel. Min. Error
0.42 ± 0.09
0.49 ± .06
0.81 ± 0.12
0.74 ± 0.19"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.39311043566362713,"Shot Spare
−4.68 ± 0.45
−3.94 ± 0.62
−4.22 ± 1.29
−11.25 ± 2.07"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3941236068895643,"each episode. Since the output of (7) is a scalar, i.e., representation e ∈R, we adopt distance measure
315"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3951367781155015,"ϕ(a, b) = σ((a −b)2), where σ(·) is sigmoid function to bound the squared error. λ = 0.02.
316"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3961499493414387,"(a) LR.
(b) SLR.
(c) DT.
(d) NN.
Figure 3: In-context learning performance."
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3971631205673759,"Results.
Figure 3 shows that varying the number of in-context examples during inference, ConML-
317"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.3981762917933131,"ICL always makes more accurate predictions than ICL. Table 5 collects the two values to show the
318"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.39918946301925023,"effect ConML brings to ICL: Rel. Min. Error is ConML-ICL’s minimal inference error given different
319"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.4002026342451874,"number of examples, divided by ICL’s; Shot Spare is when ConML-ICL obtain an error no larger
320"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.4012158054711246,"than ICL’s minimal error, the difference between the corresponding example numbers. Note that the
321"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.4022289766970618,"learning of different functions (different meta-datasets) share the same settings about ConML, which
322"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.403242147922999,"shows ConML can bring ICL universal improvement with cheap implementation. We notice that
323"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.40425531914893614,during training of LR and SLR ⌊n
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.40526849037487334,"2 ⌋= 5, which happens to equals to the dimension of the regression
324"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.40628166160081053,"task. This means sampling by πκ would results in the minimal sufficient information to learn the
325"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.4072948328267477,"task. In this case, minimizing Din is particularly beneficial for the fast-adaptation ability, shown as
326"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.4083080040526849,"Figure 3(a) and 3(b). This indicates that introducing prior knowledge to design the hyperparameter
327"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.4093211752786221,"settings of ConML could bring more advantage. The effect of ConML for ICL is without loss of
328"
IN-CONTEXT LEARNING GENERAL FUNCTIONS,0.41033434650455924,"generalizability to real-world applications like pretraining large language models.
329"
CONCLUSION,0.41134751773049644,"6
Conclusion
330"
CONCLUSION,0.41236068895643363,"In this work, we propose ConML that introduce an additional supervision for episodic meta-training
331"
CONCLUSION,0.4133738601823708,"by exploiting task identity. The contrastive meta-objective is designed to emulate the alignment and
332"
CONCLUSION,0.414387031408308,"discrimination embodied in human’s fast learning ability, and measured by performing contrastive
333"
CONCLUSION,0.4154002026342452,"learning in the model space. Specifically, we design ConML to be integrated with the conventional
334"
CONCLUSION,0.41641337386018235,"episodic meta-training, and then give specifications on a wide range of meta-learning algorithms.
335"
CONCLUSION,0.41742654508611954,"We also reformulate training ICL into episodic meta-training to design ConML-ICL following the
336"
CONCLUSION,0.41843971631205673,"same principle. Empirical results show that ConML can universally and significantly improve meta-
337"
CONCLUSION,0.4194528875379939,"learning performance by benefiting the meta-learner’s fast-adaptation and task-level generalization
338"
CONCLUSION,0.4204660587639311,"ability. This work lays the groundwork for contrastive meta-learning, by identifying the importance
339"
CONCLUSION,0.4214792299898683,"of alignment and discrimination ability of meta-learner, and practicing contrastive learning in model
340"
CONCLUSION,0.42249240121580545,"space. There also exists certain limitations, such as lack of investigating advanced contrastive strategy,
341"
CONCLUSION,0.42350557244174264,"batch- and subset- sampling strategies. We would consider these as future directions.
342"
REFERENCES,0.42451874366767983,"References
343"
REFERENCES,0.425531914893617,"[1] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to imple-
344"
REFERENCES,0.4265450861195542,"ment preconditioned gradient descent for in-context learning. Advances in Neural Information
345"
REFERENCES,0.4275582573454914,"Processing Systems, 36, 2024.
346"
REFERENCES,0.42857142857142855,"[2] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
What
347"
REFERENCES,0.42958459979736574,"learning algorithm is in-context learning? investigations with linear models. arXiv preprint
348"
REFERENCES,0.43059777102330293,"arXiv:2211.15661, 2022.
349"
REFERENCES,0.4316109422492401,"[3] Han Altae-Tran, Bharath Ramsundar, Aneesh S Pappu, and Vijay Pande. Low data drug
350"
REFERENCES,0.4326241134751773,"discovery with one-shot learning. ACS Central Science, 3(4):283–293, 2017.
351"
REFERENCES,0.4336372847011145,"[4] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom
352"
REFERENCES,0.43465045592705165,"Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by
353"
REFERENCES,0.43566362715298884,"gradient descent. Advances in neural information processing systems, 29, 2016.
354"
REFERENCES,0.43667679837892603,"[5] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by
355"
REFERENCES,0.4376899696048632,"maximizing mutual information across views. Advances in neural information processing
356"
REFERENCES,0.4387031408308004,"systems, 32, 2019.
357"
REFERENCES,0.4397163120567376,"[6] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, and Leonid Sigal. Improved
358"
REFERENCES,0.44072948328267475,"few-shot visual classification. In Proceedings of the IEEE/CVF conference on computer vision
359"
REFERENCES,0.44174265450861194,"and pattern recognition, pages 14493–14502, 2020.
360"
REFERENCES,0.44275582573454914,"[7] John Bronskill, Daniela Massiceti, Massimiliano Patacchiola, Katja Hofmann, Sebastian
361"
REFERENCES,0.44376899696048633,"Nowozin, and Richard Turner. Memory efficient meta-learning with large images. Advances in
362"
REFERENCES,0.4447821681864235,"neural information processing systems, 34:24327–24339, 2021.
363"
REFERENCES,0.4457953394123607,"[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
364"
REFERENCES,0.44680851063829785,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
365"
REFERENCES,0.44782168186423504,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
366"
REFERENCES,0.44883485309017224,"[9] Tadeusz Cali´nski and Jerzy Harabasz. A dendrite method for cluster analysis. Communications
367"
REFERENCES,0.44984802431610943,"in Statistics-theory and Methods, 3(1):1–27, 1974.
368"
REFERENCES,0.4508611955420466,"[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
369"
REFERENCES,0.4518743667679838,"for contrastive learning of visual representations. In International conference on machine
370"
REFERENCES,0.45288753799392095,"learning, pages 1597–1607. PMLR, 2020.
371"
REFERENCES,0.45390070921985815,"[11] Wenlin Chen, Austin Tripp, and José Miguel Hernández-Lobato. Meta-learning adaptive deep
372"
REFERENCES,0.45491388044579534,"kernel gaussian processes for molecular property prediction. In International Conference on
373"
REFERENCES,0.45592705167173253,"Learning Representations, 2022.
374"
REFERENCES,0.4569402228976697,"[12] Zhe Chen. Object-based attention: A tutorial review. Attention, Perception, & Psychophysics,
375"
REFERENCES,0.4579533941236069,"74:784–802, 2012.
376"
REFERENCES,0.45896656534954405,"[13] Stella Christie. Learning sameness: object and relational similarity across species. Current
377"
REFERENCES,0.45997973657548125,"Opinion in Behavioral Sciences, 37:41–46, 2021.
378"
REFERENCES,0.46099290780141844,"[14] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka.
379"
REFERENCES,0.46200607902735563,"Debiased contrastive learning. Advances in neural information processing systems, 33:8765–
380"
REFERENCES,0.4630192502532928,"8775, 2020.
381"
REFERENCES,0.46403242147923,"[15] David L Davies and Donald W Bouldin. A cluster separation measure. IEEE transactions on
382"
REFERENCES,0.46504559270516715,"pattern analysis and machine intelligence, (2):224–227, 1979.
383"
REFERENCES,0.46605876393110435,"[16] Thomas Elsken, Benedikt Staffler, Jan Hendrik Metzen, and Frank Hutter. Meta-learning of
384"
REFERENCES,0.46707193515704154,"neural architectures for few-shot learning. In Proceedings of the IEEE/CVF conference on
385"
REFERENCES,0.46808510638297873,"computer vision and pattern recognition, pages 12365–12375, 2020.
386"
REFERENCES,0.4690982776089159,"[17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
387"
REFERENCES,0.47011144883485306,"tation of deep networks. In International conference on machine learning, pages 1126–1135.
388"
REFERENCES,0.47112462006079026,"PMLR, 2017.
389"
REFERENCES,0.47213779128672745,"[18] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers
390"
REFERENCES,0.47315096251266464,"learn in-context? a case study of simple function classes. Advances in Neural Information
391"
REFERENCES,0.47416413373860183,"Processing Systems, 35:30583–30598, 2022.
392"
REFERENCES,0.475177304964539,"[19] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
393"
REFERENCES,0.47619047619047616,"Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes.
394"
REFERENCES,0.47720364741641336,"In International conference on machine learning, pages 1704–1713. PMLR, 2018.
395"
REFERENCES,0.47821681864235055,"[20] Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V
396"
REFERENCES,0.47922998986828774,"Chawla. Few-shot graph learning for molecular property prediction. In The Web Conference,
397"
REFERENCES,0.48024316109422494,"pages 2559–2567, 2021.
398"
REFERENCES,0.48125633232016213,"[21] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
399"
REFERENCES,0.48226950354609927,"unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
400"
REFERENCES,0.48328267477203646,"computer vision and pattern recognition, pages 9729–9738, 2020.
401"
REFERENCES,0.48429584599797365,"[22] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,
402"
REFERENCES,0.48530901722391084,"Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information
403"
REFERENCES,0.48632218844984804,"estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.
404"
REFERENCES,0.48733535967578523,"[23] John E Hummel. Object recognition. Oxford handbook of cognitive psychology, 810:32–46,
405"
REFERENCES,0.48834853090172237,"2013.
406"
REFERENCES,0.48936170212765956,"[24] Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-
407"
REFERENCES,0.49037487335359675,"context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.
408"
REFERENCES,0.49138804457953394,"[25] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to
409"
REFERENCES,0.49240121580547114,"learn in context. arXiv preprint arXiv:2110.15943, 2021.
410"
REFERENCES,0.49341438703140833,"[26] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine,
411"
REFERENCES,0.49442755825734547,"and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-
412"
REFERENCES,0.49544072948328266,"reinforcement learning. arXiv preprint arXiv:1803.11347, 2018.
413"
REFERENCES,0.49645390070921985,"[27] VS Napper. Alignment of learning, teaching, and assessment. Encyclopedia of the sciences of
414"
REFERENCES,0.49746707193515705,"learning. Boston: Springer US, pages 200–2, 2012.
415"
REFERENCES,0.49848024316109424,"[28] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms.
416"
REFERENCES,0.49949341438703143,"arXiv preprint arXiv:1803.02999, 2018.
417"
REFERENCES,0.5005065856129686,"[29] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
418"
REFERENCES,0.5015197568389058,"predictive coding. arXiv preprint arXiv:1807.03748, 2018.
419"
REFERENCES,0.502532928064843,"[30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
420"
REFERENCES,0.5035460992907801,"Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
421"
REFERENCES,0.5045592705167173,"[31] KI Ramachandran, Gopakumar Deepa, and Krishnan Namboori. Computational chemistry and
422"
REFERENCES,0.5055724417426545,"molecular modeling: principles and applications. Springer Science & Business Media, 2008.
423"
REFERENCES,0.5065856129685917,"[32] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenen-
424"
REFERENCES,0.5075987841945289,"baum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot
425"
REFERENCES,0.5086119554204661,"classification. arXiv preprint arXiv:1803.00676, 2018.
426"
REFERENCES,0.5096251266464032,"[33] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner.
427"
REFERENCES,0.5106382978723404,"Fast and flexible multi-task classification using conditional neural adaptive processes. Advances
428"
REFERENCES,0.5116514690982776,"in Neural Information Processing Systems, 32, 2019.
429"
REFERENCES,0.5126646403242148,"[34] Donald Robbins. Stimulus selection in human discrimination learning and transfer. Journal of
430"
REFERENCES,0.513677811550152,"Experimental Psychology, 84(2):282, 1970.
431"
REFERENCES,0.5146909827760892,"[35] Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster
432"
REFERENCES,0.5157041540020263,"analysis. Journal of computational and applied mathematics, 20:53–65, 1987.
433"
REFERENCES,0.5167173252279635,"[36] Johannes Schimunek, Philipp Seidl, Lukas Friedrich, Daniel Kuhn, Friedrich Rippmann, Sepp
434"
REFERENCES,0.5177304964539007,"Hochreiter, and Günter Klambauer. Context-enriched molecule representations improve few-
435"
REFERENCES,0.5187436676798379,"shot drug discovery. arXiv preprint arXiv:2305.09481, 2023.
436"
REFERENCES,0.5197568389057751,"[37] Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to
437"
REFERENCES,0.5207700101317123,"learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.
438"
REFERENCES,0.5217831813576495,"[38] Albert Shaw, Wei Wei, Weiyang Liu, Le Song, and Bo Dai. Meta architecture search. Advances
439"
REFERENCES,0.5227963525835866,"in Neural Information Processing Systems, 32, 2019.
440"
REFERENCES,0.5238095238095238,"[39] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning.
441"
REFERENCES,0.524822695035461,"Advances in neural information processing systems, 30, 2017.
442"
REFERENCES,0.5258358662613982,"[40] Megan Stanley, John F Bronskill, Krzysztof Maziarz, Hubert Misztela, Jessica Lanini, Marwin
443"
REFERENCES,0.5268490374873354,"Segler, Nadine Schneider, and Marc Brockschmidt. FS-Mol: A few-shot learning dataset of
444"
REFERENCES,0.5278622087132725,"molecules. In Neural Information Processing Systems Track on Datasets and Benchmarks,
445"
REFERENCES,0.5288753799392097,"2021.
446"
REFERENCES,0.5298885511651469,"[41] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
447"
REFERENCES,0.5309017223910841,"Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE
448"
REFERENCES,0.5319148936170213,"conference on computer vision and pattern recognition, pages 1199–1208, 2018.
449"
REFERENCES,0.5329280648429585,"[42] Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to
450"
REFERENCES,0.5339412360688957,"learn, pages 3–17. Springer, 1998.
451"
REFERENCES,0.5349544072948328,"[43] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Com-
452"
REFERENCES,0.53596757852077,"puter Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
453"
REFERENCES,0.5369807497467072,"Proceedings, Part XI 16, pages 776–794. Springer, 2020.
454"
REFERENCES,0.5379939209726444,"[44] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
455"
REFERENCES,0.5390070921985816,"makes for good views for contrastive learning? Advances in neural information processing
456"
REFERENCES,0.5400202634245187,"systems, 33:6827–6839, 2020.
457"
REFERENCES,0.541033434650456,"[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
458"
REFERENCES,0.5420466058763931,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
459"
REFERENCES,0.5430597771023303,"processing systems, 30, 2017.
460"
REFERENCES,0.5440729483282675,"[46] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks
461"
REFERENCES,0.5450861195542046,"for one shot learning. Advances in neural information processing systems, 29, 2016.
462"
REFERENCES,0.5460992907801419,"[47] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander
463"
REFERENCES,0.547112462006079,"Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by
464"
REFERENCES,0.5481256332320162,"gradient descent. In International Conference on Machine Learning, pages 35151–35174.
465"
REFERENCES,0.5491388044579534,"PMLR, 2023.
466"
REFERENCES,0.5501519756838906,"[48] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through
467"
REFERENCES,0.5511651469098278,"alignment and uniformity on the hypersphere. In International conference on machine learning,
468"
REFERENCES,0.5521783181357649,"pages 9929–9939. PMLR, 2020.
469"
REFERENCES,0.5531914893617021,"[49] Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing Dou. Property-aware relation
470"
REFERENCES,0.5542046605876393,"networks for few-shot molecular property prediction. In Advances in Neural Information
471"
REFERENCES,0.5552178318135765,"Processing Systems, pages 17441–17454, 2021.
472"
REFERENCES,0.5562310030395137,"[50] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few
473"
REFERENCES,0.5572441742654508,"examples: A survey on few-shot learning. ACM computing surveys (csur), 53(3):1–34, 2020.
474"
REFERENCES,0.5582573454913881,"[51] Yu-Xiong Wang and Martial Hebert. Learning to learn: Model regression networks for easy
475"
REFERENCES,0.5592705167173252,"small sample learning. In Computer Vision–ECCV 2016: 14th European Conference, Amster-
476"
REFERENCES,0.5602836879432624,"dam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pages 616–634. Springer,
477"
REFERENCES,0.5612968591691996,"2016.
478"
REFERENCES,0.5623100303951368,"[52] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. Advances in
479"
REFERENCES,0.563323201621074,"neural information processing systems, 30, 2017.
480"
REFERENCES,0.5643363728470111,"[53] Michael J Waring, John Arrowsmith, Andrew R Leach, Paul D Leeson, Sam Mandrell, Robert M
481"
REFERENCES,0.5653495440729484,"Owen, Garry Pairaudeau, William D Pennie, Stephen D Pickett, Jibo Wang, et al. An analysis
482"
REFERENCES,0.5663627152988855,"of the attrition of drug candidates from four major pharmaceutical companies. Nature Reviews
483"
REFERENCES,0.5673758865248227,"Drug discovery, 14(7):475–486, 2015.
484"
REFERENCES,0.5683890577507599,"[54] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via
485"
REFERENCES,0.569402228976697,"non-parametric instance discrimination. In Proceedings of the IEEE conference on computer
486"
REFERENCES,0.5704154002026343,"vision and pattern recognition, pages 3733–3742, 2018.
487"
REFERENCES,0.5714285714285714,"[55] Han-Jia Ye, Lu Ming, De-Chuan Zhan, and Wei-Lun Chao. Few-shot learning with a strong
488"
REFERENCES,0.5724417426545086,"teacher. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.
489"
REFERENCES,0.5734549138804458,"[56] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and
490"
REFERENCES,0.574468085106383,"Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement
491"
REFERENCES,0.5754812563323202,"learning. In Conference on robot learning, pages 1094–1100. PMLR, 2020.
492"
REFERENCES,0.5764944275582573,"A
Specifications of ConML
493"
REFERENCES,0.5775075987841946,Algorithm 3 ConML
REFERENCES,0.5785207700101317,"Input: Task distribution p(τ), batch size B, inner-task sample times K and sampling strategy πκ.
while Not converged do"
REFERENCES,0.5795339412360689,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
REFERENCES,0.5805471124620061,"for k = 1, 2, · · · , K do"
REFERENCES,0.5815602836879432,"Sample κk from πκ(Dtr
τ ∪Dval
τ );
Get model representation eκk
τ
= ψ(g(κk; θ));
end for
Get model representation e∗
τ = ψ(g(Dtr
τ ∪Dval
τ ; θ));
Get inner-task distance Din
τ by (2);
Get task-specific model hτ = g(Dtr
τ; θ);
Get validation loss L(Dval
τ ; hτ);
end for
Get Din = 1 B
P"
REFERENCES,0.5825734549138805,"τ∈b Din
τ and Dout by (3);
Get loss L by (4);
Update θ by θ ←θ −∇θL.
end while"
REFERENCES,0.5835866261398176,Algorithm 4 Efficient ConML
REFERENCES,0.5845997973657548,"Input: Task distribution p(τ), batch size B (inner-task sample times K = 1 and sampling strategy
πκ(Dtr
τ ∪Dval
τ ) = Dtr
τ).
while Not converged do"
REFERENCES,0.585612968591692,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
REFERENCES,0.5866261398176292,"Get task-specific model hτ = g(Dtr
τ; θ), and model representation eκk
τ
= ψ(g(κk; θ));
Get model representation e∗
τ = ψ(g(Dtr
τ ∪Dval
τ ; θ));
Get inner-task distance Din
τ by (2);
Get validation loss L(Dval
τ ; hτ);
end for
Get Din = 1"
REFERENCES,0.5876393110435664,"B
P
τ∈b Din
τ and Dout by (3);
Get loss L by (4);
Update θ by θ ←θ −∇θL.
end while"
REFERENCES,0.5886524822695035,Algorithm 5 In-Context Learning with Contrastive Meta-Object (ConML-ICL)
REFERENCES,0.5896656534954408,"Input: Task distribution p(τ), batch size B, inner-task sample times K and sampling strategy πκ,
dummy input u (probe).
while Not converged do"
REFERENCES,0.5906788247213779,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
REFERENCES,0.5916919959473151,"for k = 1, 2, · · · , K do"
REFERENCES,0.5927051671732523,"Sample κk from πκ(Dτ);
Get eκk
τ
= g([ ⃗κk, u]; θ);
end for
Get e∗
τ = g([ ⃗
Dτ, u]; θ);
Get inner-task distance Din
τ by (2);
Get task loss 1"
REFERENCES,0.5937183383991894,"m
Pm−1
i=0 ℓ(yτ,i+1, g([ ⃗Dτ,0:i, xτ,i+1]; θ));
end for
Get Din = 1 B
P"
REFERENCES,0.5947315096251267,"τ∈b Din
τ and Dout by (3);
Get loss L = 1 B
P"
REFERENCES,0.5957446808510638,"τ∈b
1
m
Pm−1
i=0 ℓ(yτ,i+1, g([ ⃗Dτ,0:i, xτ,i+1]; θ)) + λ(Din −Dout);
Update θ by θ ←θ −∇θL.
end while"
REFERENCES,0.596757852077001,Algorithm 6 ConML-MAML
REFERENCES,0.5977710233029382,"Input: Task distribution p(τ), batch size B, inner-task sample times K = 1 and sampling strategy
πκ
while Not converged do"
REFERENCES,0.5987841945288754,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
REFERENCES,0.5997973657548126,"for k = 1, 2, · · · , K do"
REFERENCES,0.6008105369807497,"Sample κk from πκ(Dtr
τ ∪Dval
τ );
Get model representation eκk
τ
= θ −∇θL(κk; hθ);
end for
Get model representation e∗
τ = θ −∇θL(Dtr
τ ∪Dval
τ ; hθ).
Get inner-task distance Din
τ by (2);
Get task-specific model hθ−∇θL(Dtrτ ;θ);
Get validation loss L(Dval
τ ; hθ−∇θL(Dtrτ ;hθ));
end for
Get Din = 1 B
P"
REFERENCES,0.601823708206687,"τ∈b Din
τ and Dout by (3);
Get loss L by (4);
Update θ by θ ←θ −∇θL.
end while"
REFERENCES,0.6028368794326241,Algorithm 7 ConML-Reptile
REFERENCES,0.6038500506585613,"Input: Task distribution p(τ), batch size B. (inner-task sample times K = 1 and sampling strategy
πκ(Dtr
τ ∪Dval
τ ) = Dtr
τ)
while Not converged do"
REFERENCES,0.6048632218844985,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
REFERENCES,0.6058763931104356,"for k = 1, 2, · · · , K do"
REFERENCES,0.6068895643363729,"Sample κk from πκ(Dτ);
Get model representation eκk
τ
= θ −∇θL(κk; hθ);
end for
Get model representation e∗
τ = θ −∇θL(Dtr
τ ∪Dval
τ ; hθ).
Get inner-task distance Din
τ by (2);
end for
Get Din = 1 B
P"
REFERENCES,0.60790273556231,"τ∈b Din
τ and Dout by (3);
Get loss L by (4);
Update θ by θ ←θ + 1 B
P"
REFERENCES,0.6089159067882473,"τ∈b(e∗
τ −θ) −λ∇θ(Din −Dout).
end while"
REFERENCES,0.6099290780141844,Algorithm 8 ConML on SCNAPs
REFERENCES,0.6109422492401215,"Note: Here hw corresponds to the feature extractor fθ; Hθ corresponds to the task encoder gϕ in
[6].
Input: Task distribution p(τ), batch size B, inner-task sample times K and sampling strategy πκ.
Pretrain hw with the mixture of all meta-training data;
while Not converged do"
REFERENCES,0.6119554204660588,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
REFERENCES,0.6129685916919959,"for k = 1, 2, · · · , K do"
REFERENCES,0.6139817629179332,"Sample κk from πκ(Dtr
τ ∪Dval
τ );
Get model representation eκk
τ
= Hθ(κk);
end for
Get model representation e∗
τ = Hθ(Dtr
τ ∪Dval
τ );
Get inner-task distance Din
τ by (2);
Get task-specific model by FiLM hτ = hw,Hθ(Dtrτ );
Get validation loss L(Dval
τ ; hτ);
end for
Get Din = 1 B
P"
REFERENCES,0.6149949341438703,"τ∈b Din
τ and Dout by (3);
Get loss L by (4);
Update θ by θ ←θ −∇θL.
end while"
REFERENCES,0.6160081053698075,Algorithm 9 ConML-ProtoNet (N-way classification)
REFERENCES,0.6170212765957447,"Input: Task distribution p(τ), batch size B, inner-task sample times K = 1 and sampling strategy
πκ
while Not converged do"
REFERENCES,0.6180344478216818,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
REFERENCES,0.6190476190476191,"for k = 1, 2, · · · , K do"
REFERENCES,0.6200607902735562,"Sample κk from πκ(Dtr
τ ∪Dval
τ );
Calculate prototypes cj =
1
|κk,j|
P"
REFERENCES,0.6210739614994935,"(xi,yi)∈κk,j fθ(xi) for j = 1, · · · , N;
Get model representation eκk
τ
= [c1|c2| · · · |cN];
end for
Calculate prototypes cj =
1
|Dj|
P"
REFERENCES,0.6220871327254306,"(xi,yi)∈Dj fθ(xi) for j = 1, · · · , N;
Get model representation e∗
τ = [c1|c2| · · · |cN];
Get inner-task distance Din
τ by (2);
Get task-specific model h[c1|c2|···|cN], which gives prediction by p(y = j | x) =
exp(−d(fθ(x),cj))
P"
REFERENCES,0.6231003039513677,"j′ exp(−d(fθ(x),cj′));"
REFERENCES,0.624113475177305,"Get validation loss L(Dval
τ ; h[c1|c2|···|cN]);
end for
Get Din = 1"
REFERENCES,0.6251266464032421,"B
P
τ∈b Din
τ and Dout by (3);
Get loss L by (4);
Update θ by θ ←θ −∇θL.
end while"
REFERENCES,0.6261398176291794,"B
Few-shot Molecular Property Prediction
494"
REFERENCES,0.6271529888551165,"Few-shot molecular property prediction (FSMPP) is an important real-world application where meta-
495"
REFERENCES,0.6281661600810537,"learning has been widely applied recently [3, 20, 49, 11, 36]. Molecular property prediction, which
496"
REFERENCES,0.6291793313069909,"predicts whether desired properties will be active on given molecules, plays a crucial role in many
497"
REFERENCES,0.630192502532928,"applications like computational chemistry [31] and drug discovery [53]. As wet-lab experiments
498"
REFERENCES,0.6312056737588653,"to evaluate the actual properties of molecules are expensive and risky, usually only a few labeled
499"
REFERENCES,0.6322188449848024,"molecules are available for a specific property. Molecular property prediction can be naturally
500"
REFERENCES,0.6332320162107397,"modeled as a few-shot learning problem [3], and meta-learning approaches has been successfully
501"
REFERENCES,0.6342451874366768,"adopted for FSMPP [3, 20, 49, 11].
502"
REFERENCES,0.6352583586626139,"Dataset and Settings.
We use FS-Mol [40], a widely studied FSMPP benchmark consisting of
503"
REFERENCES,0.6362715298885512,"a large number of diverse tasks. We adopt the public data split [40]. Each training set contains 64
504"
REFERENCES,0.6372847011144883,"labeled molecules, and can be imbalanced where the number of labeled molecules from active and
505"
REFERENCES,0.6382978723404256,"inactive is not equal. All remaining molecules in the task form the validation set. The performance is
506"
REFERENCES,0.6393110435663627,"evaluated by ∆AUPRC (change in area under the precision-recall curve) w.r.t. a random classifier [40],
507"
REFERENCES,0.6403242147923,"averaged across meta-testing tasks.
508"
REFERENCES,0.6413373860182371,"Baselines.
We consider the following meta-learning-based FSMPP approaches: MAML, ProtoNet,
509"
REFERENCES,0.6423505572441742,"CNP, IterRefLSTM, PAR, ADKF-IFT. Note that MHNfs [36] is not included as it uses additional
510"
REFERENCES,0.6433637284701115,"reference molecules from external datasets, which leads to unfair comparison, and ADKF-IFT is
511"
REFERENCES,0.6443768996960486,"the SOTA approach in literature. All baselines share the same GNN-based encoder provided by the
512"
REFERENCES,0.6453900709219859,"benchmark to meta-train from scratch, which maps molecular graphs to embedding vectors.
513"
REFERENCES,0.646403242147923,Algorithm 10 Hypro
REFERENCES,0.6474164133738601,"Note: The main-network consists of two modules [40]: the molecular encoder fθ and the prototyp-
ical network classifier hθ.
Input: Task distribution p(τ), batch size B.
while Not converged do"
REFERENCES,0.6484295845997974,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
REFERENCES,0.6494427558257345,"Encode all molecules fθ(x) for x ∈Dtr
τ ∪Dval
τ
Get task-specific parameters ατ = Hθ({(fθ(xi), yi)}(xi,yi)∈Dtrτ );
Modulate all molecular embedding with ατ by FiLM, and classify with hθ; (denote the
function of this step as hθ,ατ )
Get validation loss L(Dval
τ ; hθ,ατ );
end for
Lv = 1"
REFERENCES,0.6504559270516718,"B
P
τ∈b L(Dval
τ ; hθ,ατ )
Update θ by θ ←θ −∇θLv.
end while"
REFERENCES,0.6514690982776089,"We introduce a new baseline ConML-Hypro, which achieves SOTA performance by incorporating
514"
REFERENCES,0.6524822695035462,"ConML with a simple backbone, Hypro. It is an amortization-based model built by modifying the
515"
REFERENCES,0.6534954407294833,"ProtoNet backbone, by plugging-in a hypernetwork H with a set-encoder structure, i.e., H(D) =
516"
REFERENCES,0.6545086119554204,"MLP2( 1 |D|
P"
REFERENCES,0.6555217831813577,"D MLP1([xi | yi])). We input the embedding vectors in Dtr to the hypernetwork, and take
517"
REFERENCES,0.6565349544072948,"the output to modulate embedding vectors through FiLM before classification. This hypernetwork
518"
REFERENCES,0.6575481256332321,"and modulation is typical in amortization-based models. Viewing Hypro as an amortization-based
519"
REFERENCES,0.6585612968591692,"model, we apply the specification of ConML to form ConML-Hypro. The detailed procedure to train
520"
REFERENCES,0.6595744680851063,"Hypro and ConML-Hypro are provided in Algorithm 10 and 11. The structure of H is provided
521"
REFERENCES,0.6605876393110436,"in Table 6, and two such hypernetworks are used for generate parameters for FiLM function. We
522"
REFERENCES,0.6616008105369807,"implement ConML with B = 16, ϕ(a, b) = 1 −a·b/∥a∥∥b∥(cosine distance) and λ = 0.1. As for the
523"
REFERENCES,0.662613981762918,"sampling strategy πκ and times K, for every task, we sample subset with different sizes, including
524"
REFERENCES,0.6636271529888551,"each m ∈{4, 8, 16, 32, 64}, for 128/m times respectively. A m-sized subset contains m/2 positive
525"
REFERENCES,0.6646403242147924,"and m/2 negative samples sampled randomly. The other hyperparameters of model structure and
526"
REFERENCES,0.6656534954407295,"training procedure follow the benchmark’s default setting [40].
527"
REFERENCES,0.6666666666666666,"Results.
Table 7 shows the results. ConML-Hypro shows advantage over SOTA approach under
528"
REFERENCES,0.6676798378926039,"all meta-testing scenarios with different shots. Comparing Hypro and ProtoNet, we can find the
529"
REFERENCES,0.668693009118541,Algorithm 11 ConML-Hypro
REFERENCES,0.6697061803444783,"Note: Refer to Algorithm 10 for details about Hθ(D) and hθ,α.
Input: Task distribution p(τ), batch size B, inner-task sample times K and sampling strategy πκ.
while Not converged do"
REFERENCES,0.6707193515704154,"Sample a batch of tasks b ∼pB(τ).
for All τ ∈b do"
REFERENCES,0.6717325227963525,"for k = 1, 2, · · · , K do"
REFERENCES,0.6727456940222898,"Sample κk from πκ(Dtr
τ ∪Dval
τ );
Get model representation eκk
τ
= Hθ(κk);
end for
Get model representation e∗
τ = Hθ(Dtr
τ ∪Dval
τ );
Get inner-task distance Din
τ by (2);
Get task-specific model hθ,Hθ(Dtrτ );
Get validation loss L(Dval
τ ; hθ,Hθ(Dtrτ ));
end for
Get Din = 1 B
P"
REFERENCES,0.6737588652482269,"τ∈b Din
τ and Dout by (3);
Get loss L by (4);
Update θ by θ ←θ −∇θL.
end while"
REFERENCES,0.6747720364741642,Table 6: Hypernetwork structure in Hypro and ConML-Hypro
REFERENCES,0.6757852077001013,"Layers
Output dimension"
REFERENCES,0.6767983789260384,"MLP1
input [xi | yi] (dim=2562), fully connected, LeakyReLU
2560
2× fully connected with with residual skip connection
2560
MLP2 2×fully connected with residual skip connection, LeakyReLU
2560"
REFERENCES,0.6778115501519757,"introduced hypernetwork can brings notable improvement. Comparing ConML-Hypro and Hypro,
530"
REFERENCES,0.6788247213779128,"we can find the effect of ConML is significant.
531"
REFERENCES,0.6798378926038501,"Table 7: Few-shot molecular property prediction performance (∆AUPRC) on FS-Mol. † indicates
result from [36]. ∗indicates new approach proposed in this paper."
-SHOT,0.6808510638297872,"2-shot
4-shot
8-shot
16-shot"
-SHOT,0.6818642350557245,"MAML
.009 ± .006
.125 ± .009
.146 ± .007
.159 ± .009"
-SHOT,0.6828774062816616,"PAR
.124 ± .007
.140 ± .005
.149 ± .009
.164 ± .008"
-SHOT,0.6838905775075987,"ProtoNet
.117 ± .006
.142 ± .007
.175 ± .006
.206 ± .008"
-SHOT,0.684903748733536,"CNP
.139 ± .004
.155 ± .008
.174 ± .006
.187 ± .009"
-SHOT,0.6859169199594731,"Hypro∗
.122 ± .007
.150 ± .006
.185 ± .008
.216 ± .007"
-SHOT,0.6869300911854104,"IterRefLSTM†
-
-
-
.234 ± .010"
-SHOT,0.6879432624113475,"ADKF-IFT
.131 ± .007
.166 ± .005
.202 ± .006
.234 ± .009"
-SHOT,0.6889564336372846,"ConML-Hypro∗
.175 ± .006
.196 ± .006
.218 ± .005
.239 ± .007"
-SHOT,0.6899696048632219,"NeurIPS Paper Checklist
532"
CLAIMS,0.690982776089159,"1. Claims
533"
CLAIMS,0.6919959473150963,"Question: Do the main claims made in the abstract and introduction accurately reflect the
534"
CLAIMS,0.6930091185410334,"paper’s contributions and scope?
535"
CLAIMS,0.6940222897669707,"Answer: [Yes]
536"
CLAIMS,0.6950354609929078,"Justification: [TODO]
537"
CLAIMS,0.6960486322188449,"Guidelines:
538"
CLAIMS,0.6970618034447822,"• The answer NA means that the abstract and introduction do not include the claims
539"
CLAIMS,0.6980749746707193,"made in the paper.
540"
CLAIMS,0.6990881458966566,"• The abstract and/or introduction should clearly state the claims made, including the
541"
CLAIMS,0.7001013171225937,"contributions made in the paper and important assumptions and limitations. A No or
542"
CLAIMS,0.7011144883485309,"NA answer to this question will not be perceived well by the reviewers.
543"
CLAIMS,0.7021276595744681,"• The claims made should match theoretical and experimental results, and reflect how
544"
CLAIMS,0.7031408308004052,"much the results can be expected to generalize to other settings.
545"
CLAIMS,0.7041540020263425,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
546"
CLAIMS,0.7051671732522796,"are not attained by the paper.
547"
LIMITATIONS,0.7061803444782169,"2. Limitations
548"
LIMITATIONS,0.707193515704154,"Question: Does the paper discuss the limitations of the work performed by the authors?
549"
LIMITATIONS,0.7082066869300911,"Answer: [Yes]
550"
LIMITATIONS,0.7092198581560284,"Justification: [TODO]
551"
LIMITATIONS,0.7102330293819655,"Guidelines:
552"
LIMITATIONS,0.7112462006079028,"• The answer NA means that the paper has no limitation while the answer No means that
553"
LIMITATIONS,0.7122593718338399,"the paper has limitations, but those are not discussed in the paper.
554"
LIMITATIONS,0.713272543059777,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
555"
LIMITATIONS,0.7142857142857143,"• The paper should point out any strong assumptions and how robust the results are to
556"
LIMITATIONS,0.7152988855116514,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
557"
LIMITATIONS,0.7163120567375887,"model well-specification, asymptotic approximations only holding locally). The authors
558"
LIMITATIONS,0.7173252279635258,"should reflect on how these assumptions might be violated in practice and what the
559"
LIMITATIONS,0.7183383991894631,"implications would be.
560"
LIMITATIONS,0.7193515704154002,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
561"
LIMITATIONS,0.7203647416413373,"only tested on a few datasets or with a few runs. In general, empirical results often
562"
LIMITATIONS,0.7213779128672746,"depend on implicit assumptions, which should be articulated.
563"
LIMITATIONS,0.7223910840932117,"• The authors should reflect on the factors that influence the performance of the approach.
564"
LIMITATIONS,0.723404255319149,"For example, a facial recognition algorithm may perform poorly when image resolution
565"
LIMITATIONS,0.7244174265450861,"is low or images are taken in low lighting. Or a speech-to-text system might not be
566"
LIMITATIONS,0.7254305977710233,"used reliably to provide closed captions for online lectures because it fails to handle
567"
LIMITATIONS,0.7264437689969605,"technical jargon.
568"
LIMITATIONS,0.7274569402228976,"• The authors should discuss the computational efficiency of the proposed algorithms
569"
LIMITATIONS,0.7284701114488349,"and how they scale with dataset size.
570"
LIMITATIONS,0.729483282674772,"• If applicable, the authors should discuss possible limitations of their approach to
571"
LIMITATIONS,0.7304964539007093,"address problems of privacy and fairness.
572"
LIMITATIONS,0.7315096251266464,"• While the authors might fear that complete honesty about limitations might be used by
573"
LIMITATIONS,0.7325227963525835,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
574"
LIMITATIONS,0.7335359675785208,"limitations that aren’t acknowledged in the paper. The authors should use their best
575"
LIMITATIONS,0.7345491388044579,"judgment and recognize that individual actions in favor of transparency play an impor-
576"
LIMITATIONS,0.7355623100303952,"tant role in developing norms that preserve the integrity of the community. Reviewers
577"
LIMITATIONS,0.7365754812563323,"will be specifically instructed to not penalize honesty concerning limitations.
578"
THEORY ASSUMPTIONS AND PROOFS,0.7375886524822695,"3. Theory Assumptions and Proofs
579"
THEORY ASSUMPTIONS AND PROOFS,0.7386018237082067,"Question: For each theoretical result, does the paper provide the full set of assumptions and
580"
THEORY ASSUMPTIONS AND PROOFS,0.7396149949341438,"a complete (and correct) proof?
581"
THEORY ASSUMPTIONS AND PROOFS,0.7406281661600811,"Answer: [NA]
582"
THEORY ASSUMPTIONS AND PROOFS,0.7416413373860182,"Justification: [TODO]
583"
THEORY ASSUMPTIONS AND PROOFS,0.7426545086119554,"Guidelines:
584"
THEORY ASSUMPTIONS AND PROOFS,0.7436676798378926,"• The answer NA means that the paper does not include theoretical results.
585"
THEORY ASSUMPTIONS AND PROOFS,0.7446808510638298,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
586"
THEORY ASSUMPTIONS AND PROOFS,0.745694022289767,"referenced.
587"
THEORY ASSUMPTIONS AND PROOFS,0.7467071935157041,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
588"
THEORY ASSUMPTIONS AND PROOFS,0.7477203647416414,"• The proofs can either appear in the main paper or the supplemental material, but if
589"
THEORY ASSUMPTIONS AND PROOFS,0.7487335359675785,"they appear in the supplemental material, the authors are encouraged to provide a short
590"
THEORY ASSUMPTIONS AND PROOFS,0.7497467071935157,"proof sketch to provide intuition.
591"
THEORY ASSUMPTIONS AND PROOFS,0.7507598784194529,"• Inversely, any informal proof provided in the core of the paper should be complemented
592"
THEORY ASSUMPTIONS AND PROOFS,0.75177304964539,"by formal proofs provided in appendix or supplemental material.
593"
THEORY ASSUMPTIONS AND PROOFS,0.7527862208713273,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7537993920972644,"4. Experimental Result Reproducibility
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7548125633232016,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7558257345491388,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.756838905775076,"of the paper (regardless of whether the code and data are provided or not)?
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7578520770010132,"Answer: [Yes]
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7588652482269503,"Justification: [TODO]
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7598784194528876,"Guidelines:
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7608915906788247,"• The answer NA means that the paper does not include experiments.
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7619047619047619,"• If the paper includes experiments, a No answer to this question will not be perceived
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7629179331306991,"well by the reviewers: Making the paper reproducible is important, regardless of
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7639311043566362,"whether the code and data are provided or not.
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7649442755825735,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7659574468085106,"to make their results reproducible or verifiable.
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7669706180344478,"• Depending on the contribution, reproducibility can be accomplished in various ways.
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.767983789260385,"For example, if the contribution is a novel architecture, describing the architecture fully
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7689969604863222,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7700101317122594,"be necessary to either make it possible for others to replicate the model with the same
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7710233029381965,"dataset, or provide access to the model. In general. releasing code and data is often
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7720364741641338,"one good way to accomplish this, but reproducibility can also be provided via detailed
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7730496453900709,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7740628166160081,"of a large language model), releasing of a model checkpoint, or other means that are
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7750759878419453,"appropriate to the research performed.
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7760891590678825,"• While NeurIPS does not require releasing code, the conference does require all submis-
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7771023302938197,"sions to provide some reasonable avenue for reproducibility, which may depend on the
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7781155015197568,"nature of the contribution. For example
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.779128672745694,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7801418439716312,"to reproduce that algorithm.
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7811550151975684,"(b) If the contribution is primarily a new model architecture, the paper should describe
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7821681864235056,"the architecture clearly and fully.
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7831813576494427,"(c) If the contribution is a new model (e.g., a large language model), then there should
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.78419452887538,"either be a way to access this model for reproducing the results or a way to reproduce
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852077001013171,"the model (e.g., with an open-source dataset or instructions for how to construct
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7862208713272543,"the dataset).
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7872340425531915,"(d) We recognize that reproducibility may be tricky in some cases, in which case
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7882472137791287,"authors are welcome to describe the particular way they provide for reproducibility.
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7892603850050659,"In the case of closed-source models, it may be that access to the model is limited in
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.790273556231003,"some way (e.g., to registered users), but it should be possible for other researchers
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7912867274569402,"to have some path to reproducing or verifying the results.
632"
OPEN ACCESS TO DATA AND CODE,0.7922998986828774,"5. Open access to data and code
633"
OPEN ACCESS TO DATA AND CODE,0.7933130699088146,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
634"
OPEN ACCESS TO DATA AND CODE,0.7943262411347518,"tions to faithfully reproduce the main experimental results, as described in supplemental
635"
OPEN ACCESS TO DATA AND CODE,0.795339412360689,"material?
636"
OPEN ACCESS TO DATA AND CODE,0.7963525835866262,"Answer: [Yes]
637"
OPEN ACCESS TO DATA AND CODE,0.7973657548125633,"Justification: [TODO]
638"
OPEN ACCESS TO DATA AND CODE,0.7983789260385005,"Guidelines:
639"
OPEN ACCESS TO DATA AND CODE,0.7993920972644377,"• The answer NA means that paper does not include experiments requiring code.
640"
OPEN ACCESS TO DATA AND CODE,0.8004052684903749,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
641"
OPEN ACCESS TO DATA AND CODE,0.8014184397163121,"public/guides/CodeSubmissionPolicy) for more details.
642"
OPEN ACCESS TO DATA AND CODE,0.8024316109422492,"• While we encourage the release of code and data, we understand that this might not be
643"
OPEN ACCESS TO DATA AND CODE,0.8034447821681864,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
644"
OPEN ACCESS TO DATA AND CODE,0.8044579533941236,"including code, unless this is central to the contribution (e.g., for a new open-source
645"
OPEN ACCESS TO DATA AND CODE,0.8054711246200608,"benchmark).
646"
OPEN ACCESS TO DATA AND CODE,0.806484295845998,"• The instructions should contain the exact command and environment needed to run to
647"
OPEN ACCESS TO DATA AND CODE,0.8074974670719351,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
648"
OPEN ACCESS TO DATA AND CODE,0.8085106382978723,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
649"
OPEN ACCESS TO DATA AND CODE,0.8095238095238095,"• The authors should provide instructions on data access and preparation, including how
650"
OPEN ACCESS TO DATA AND CODE,0.8105369807497467,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
651"
OPEN ACCESS TO DATA AND CODE,0.8115501519756839,"• The authors should provide scripts to reproduce all experimental results for the new
652"
OPEN ACCESS TO DATA AND CODE,0.8125633232016211,"proposed method and baselines. If only a subset of experiments are reproducible, they
653"
OPEN ACCESS TO DATA AND CODE,0.8135764944275583,"should state which ones are omitted from the script and why.
654"
OPEN ACCESS TO DATA AND CODE,0.8145896656534954,"• At submission time, to preserve anonymity, the authors should release anonymized
655"
OPEN ACCESS TO DATA AND CODE,0.8156028368794326,"versions (if applicable).
656"
OPEN ACCESS TO DATA AND CODE,0.8166160081053698,"• Providing as much information as possible in supplemental material (appended to the
657"
OPEN ACCESS TO DATA AND CODE,0.817629179331307,"paper) is recommended, but including URLs to data and code is permitted.
658"
OPEN ACCESS TO DATA AND CODE,0.8186423505572442,"6. Experimental Setting/Details
659"
OPEN ACCESS TO DATA AND CODE,0.8196555217831814,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
660"
OPEN ACCESS TO DATA AND CODE,0.8206686930091185,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
661"
OPEN ACCESS TO DATA AND CODE,0.8216818642350557,"results?
662"
OPEN ACCESS TO DATA AND CODE,0.8226950354609929,"Answer: [Yes]
663"
OPEN ACCESS TO DATA AND CODE,0.8237082066869301,"Justification: [TODO]
664"
OPEN ACCESS TO DATA AND CODE,0.8247213779128673,"Guidelines:
665"
OPEN ACCESS TO DATA AND CODE,0.8257345491388045,"• The answer NA means that the paper does not include experiments.
666"
OPEN ACCESS TO DATA AND CODE,0.8267477203647416,"• The experimental setting should be presented in the core of the paper to a level of detail
667"
OPEN ACCESS TO DATA AND CODE,0.8277608915906788,"that is necessary to appreciate the results and make sense of them.
668"
OPEN ACCESS TO DATA AND CODE,0.828774062816616,"• The full details can be provided either with the code, in appendix, or as supplemental
669"
OPEN ACCESS TO DATA AND CODE,0.8297872340425532,"material.
670"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8308004052684904,"7. Experiment Statistical Significance
671"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8318135764944276,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
672"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8328267477203647,"information about the statistical significance of the experiments?
673"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8338399189463019,"Answer:[Yes]
674"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8348530901722391,"Justification: [TODO]
675"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8358662613981763,"Guidelines:
676"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8368794326241135,"• The answer NA means that the paper does not include experiments.
677"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8378926038500507,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
678"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8389057750759878,"dence intervals, or statistical significance tests, at least for the experiments that support
679"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.839918946301925,"the main claims of the paper.
680"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8409321175278622,"• The factors of variability that the error bars are capturing should be clearly stated (for
681"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8419452887537994,"example, train/test split, initialization, random drawing of some parameter, or overall
682"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8429584599797366,"run with given experimental conditions).
683"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8439716312056738,"• The method for calculating the error bars should be explained (closed form formula,
684"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8449848024316109,"call to a library function, bootstrap, etc.)
685"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8459979736575481,"• The assumptions made should be given (e.g., Normally distributed errors).
686"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8470111448834853,"• It should be clear whether the error bar is the standard deviation or the standard error
687"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8480243161094225,"of the mean.
688"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8490374873353597,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
689"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8500506585612969,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
690"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.851063829787234,"of Normality of errors is not verified.
691"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8520770010131712,"• For asymmetric distributions, the authors should be careful not to show in tables or
692"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8530901722391084,"figures symmetric error bars that would yield results that are out of range (e.g. negative
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8541033434650456,"error rates).
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8551165146909828,"• If error bars are reported in tables or plots, The authors should explain in the text how
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.85612968591692,"they were calculated and reference the corresponding figures or tables in the text.
696"
EXPERIMENTS COMPUTE RESOURCES,0.8571428571428571,"8. Experiments Compute Resources
697"
EXPERIMENTS COMPUTE RESOURCES,0.8581560283687943,"Question: For each experiment, does the paper provide sufficient information on the com-
698"
EXPERIMENTS COMPUTE RESOURCES,0.8591691995947315,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
699"
EXPERIMENTS COMPUTE RESOURCES,0.8601823708206687,"the experiments?
700"
EXPERIMENTS COMPUTE RESOURCES,0.8611955420466059,"Answer: [Yes]
701"
EXPERIMENTS COMPUTE RESOURCES,0.8622087132725431,"Justification: [TODO]
702"
EXPERIMENTS COMPUTE RESOURCES,0.8632218844984803,"Guidelines:
703"
EXPERIMENTS COMPUTE RESOURCES,0.8642350557244174,"• The answer NA means that the paper does not include experiments.
704"
EXPERIMENTS COMPUTE RESOURCES,0.8652482269503546,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
705"
EXPERIMENTS COMPUTE RESOURCES,0.8662613981762918,"or cloud provider, including relevant memory and storage.
706"
EXPERIMENTS COMPUTE RESOURCES,0.867274569402229,"• The paper should provide the amount of compute required for each of the individual
707"
EXPERIMENTS COMPUTE RESOURCES,0.8682877406281662,"experimental runs as well as estimate the total compute.
708"
EXPERIMENTS COMPUTE RESOURCES,0.8693009118541033,"• The paper should disclose whether the full research project required more compute
709"
EXPERIMENTS COMPUTE RESOURCES,0.8703140830800405,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
710"
EXPERIMENTS COMPUTE RESOURCES,0.8713272543059777,"didn’t make it into the paper).
711"
CODE OF ETHICS,0.8723404255319149,"9. Code Of Ethics
712"
CODE OF ETHICS,0.8733535967578521,"Question: Does the research conducted in the paper conform, in every respect, with the
713"
CODE OF ETHICS,0.8743667679837892,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
714"
CODE OF ETHICS,0.8753799392097265,"Answer: [Yes]
715"
CODE OF ETHICS,0.8763931104356636,"Justification: [TODO]
716"
CODE OF ETHICS,0.8774062816616008,"Guidelines:
717"
CODE OF ETHICS,0.878419452887538,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
718"
CODE OF ETHICS,0.8794326241134752,"• If the authors answer No, they should explain the special circumstances that require a
719"
CODE OF ETHICS,0.8804457953394124,"deviation from the Code of Ethics.
720"
CODE OF ETHICS,0.8814589665653495,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
721"
CODE OF ETHICS,0.8824721377912867,"eration due to laws or regulations in their jurisdiction).
722"
BROADER IMPACTS,0.8834853090172239,"10. Broader Impacts
723"
BROADER IMPACTS,0.8844984802431611,"Question: Does the paper discuss both potential positive societal impacts and negative
724"
BROADER IMPACTS,0.8855116514690983,"societal impacts of the work performed?
725"
BROADER IMPACTS,0.8865248226950354,"Answer:[Yes]
726"
BROADER IMPACTS,0.8875379939209727,"Justification: [TODO]
727"
BROADER IMPACTS,0.8885511651469098,"Guidelines:
728"
BROADER IMPACTS,0.889564336372847,"• The answer NA means that there is no societal impact of the work performed.
729"
BROADER IMPACTS,0.8905775075987842,"• If the authors answer NA or No, they should explain why their work has no societal
730"
BROADER IMPACTS,0.8915906788247214,"impact or why the paper does not address societal impact.
731"
BROADER IMPACTS,0.8926038500506586,"• Examples of negative societal impacts include potential malicious or unintended uses
732"
BROADER IMPACTS,0.8936170212765957,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
733"
BROADER IMPACTS,0.894630192502533,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
734"
BROADER IMPACTS,0.8956433637284701,"groups), privacy considerations, and security considerations.
735"
BROADER IMPACTS,0.8966565349544073,"• The conference expects that many papers will be foundational research and not tied
736"
BROADER IMPACTS,0.8976697061803445,"to particular applications, let alone deployments. However, if there is a direct path to
737"
BROADER IMPACTS,0.8986828774062816,"any negative applications, the authors should point it out. For example, it is legitimate
738"
BROADER IMPACTS,0.8996960486322189,"to point out that an improvement in the quality of generative models could be used to
739"
BROADER IMPACTS,0.900709219858156,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
740"
BROADER IMPACTS,0.9017223910840932,"that a generic algorithm for optimizing neural networks could enable people to train
741"
BROADER IMPACTS,0.9027355623100304,"models that generate Deepfakes faster.
742"
BROADER IMPACTS,0.9037487335359676,"• The authors should consider possible harms that could arise when the technology is
743"
BROADER IMPACTS,0.9047619047619048,"being used as intended and functioning correctly, harms that could arise when the
744"
BROADER IMPACTS,0.9057750759878419,"technology is being used as intended but gives incorrect results, and harms following
745"
BROADER IMPACTS,0.9067882472137792,"from (intentional or unintentional) misuse of the technology.
746"
BROADER IMPACTS,0.9078014184397163,"• If there are negative societal impacts, the authors could also discuss possible mitigation
747"
BROADER IMPACTS,0.9088145896656535,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
748"
BROADER IMPACTS,0.9098277608915907,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
749"
BROADER IMPACTS,0.9108409321175278,"feedback over time, improving the efficiency and accessibility of ML).
750"
SAFEGUARDS,0.9118541033434651,"11. Safeguards
751"
SAFEGUARDS,0.9128672745694022,"Question: Does the paper describe safeguards that have been put in place for responsible
752"
SAFEGUARDS,0.9138804457953394,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
753"
SAFEGUARDS,0.9148936170212766,"image generators, or scraped datasets)?
754"
SAFEGUARDS,0.9159067882472138,"Answer: [NA]
755"
SAFEGUARDS,0.916919959473151,"Justification: [TODO]
756"
SAFEGUARDS,0.9179331306990881,"Guidelines:
757"
SAFEGUARDS,0.9189463019250254,"• The answer NA means that the paper poses no such risks.
758"
SAFEGUARDS,0.9199594731509625,"• Released models that have a high risk for misuse or dual-use should be released with
759"
SAFEGUARDS,0.9209726443768997,"necessary safeguards to allow for controlled use of the model, for example by requiring
760"
SAFEGUARDS,0.9219858156028369,"that users adhere to usage guidelines or restrictions to access the model or implementing
761"
SAFEGUARDS,0.922998986828774,"safety filters.
762"
SAFEGUARDS,0.9240121580547113,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
763"
SAFEGUARDS,0.9250253292806484,"should describe how they avoided releasing unsafe images.
764"
SAFEGUARDS,0.9260385005065856,"• We recognize that providing effective safeguards is challenging, and many papers do
765"
SAFEGUARDS,0.9270516717325228,"not require this, but we encourage authors to take this into account and make a best
766"
SAFEGUARDS,0.92806484295846,"faith effort.
767"
LICENSES FOR EXISTING ASSETS,0.9290780141843972,"12. Licenses for existing assets
768"
LICENSES FOR EXISTING ASSETS,0.9300911854103343,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
769"
LICENSES FOR EXISTING ASSETS,0.9311043566362716,"the paper, properly credited and are the license and terms of use explicitly mentioned and
770"
LICENSES FOR EXISTING ASSETS,0.9321175278622087,"properly respected?
771"
LICENSES FOR EXISTING ASSETS,0.9331306990881459,"Answer: [Yes]
772"
LICENSES FOR EXISTING ASSETS,0.9341438703140831,"Justification: [TODO]
773"
LICENSES FOR EXISTING ASSETS,0.9351570415400202,"Guidelines:
774"
LICENSES FOR EXISTING ASSETS,0.9361702127659575,"• The answer NA means that the paper does not use existing assets.
775"
LICENSES FOR EXISTING ASSETS,0.9371833839918946,"• The authors should cite the original paper that produced the code package or dataset.
776"
LICENSES FOR EXISTING ASSETS,0.9381965552178319,"• The authors should state which version of the asset is used and, if possible, include a
777"
LICENSES FOR EXISTING ASSETS,0.939209726443769,"URL.
778"
LICENSES FOR EXISTING ASSETS,0.9402228976697061,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
779"
LICENSES FOR EXISTING ASSETS,0.9412360688956434,"• For scraped data from a particular source (e.g., website), the copyright and terms of
780"
LICENSES FOR EXISTING ASSETS,0.9422492401215805,"service of that source should be provided.
781"
LICENSES FOR EXISTING ASSETS,0.9432624113475178,"• If assets are released, the license, copyright information, and terms of use in the
782"
LICENSES FOR EXISTING ASSETS,0.9442755825734549,"package should be provided. For popular datasets, paperswithcode.com/datasets
783"
LICENSES FOR EXISTING ASSETS,0.9452887537993921,"has curated licenses for some datasets. Their licensing guide can help determine the
784"
LICENSES FOR EXISTING ASSETS,0.9463019250253293,"license of a dataset.
785"
LICENSES FOR EXISTING ASSETS,0.9473150962512664,"• For existing datasets that are re-packaged, both the original license and the license of
786"
LICENSES FOR EXISTING ASSETS,0.9483282674772037,"the derived asset (if it has changed) should be provided.
787"
LICENSES FOR EXISTING ASSETS,0.9493414387031408,"• If this information is not available online, the authors are encouraged to reach out to
788"
LICENSES FOR EXISTING ASSETS,0.950354609929078,"the asset’s creators.
789"
NEW ASSETS,0.9513677811550152,"13. New Assets
790"
NEW ASSETS,0.9523809523809523,"Question: Are new assets introduced in the paper well documented and is the documentation
791"
NEW ASSETS,0.9533941236068896,"provided alongside the assets?
792"
NEW ASSETS,0.9544072948328267,"Answer: [Yes]
793"
NEW ASSETS,0.955420466058764,"Justification: [TODO]
794"
NEW ASSETS,0.9564336372847011,"Guidelines:
795"
NEW ASSETS,0.9574468085106383,"• The answer NA means that the paper does not release new assets.
796"
NEW ASSETS,0.9584599797365755,"• Researchers should communicate the details of the dataset/code/model as part of their
797"
NEW ASSETS,0.9594731509625126,"submissions via structured templates. This includes details about training, license,
798"
NEW ASSETS,0.9604863221884499,"limitations, etc.
799"
NEW ASSETS,0.961499493414387,"• The paper should discuss whether and how consent was obtained from people whose
800"
NEW ASSETS,0.9625126646403243,"asset is used.
801"
NEW ASSETS,0.9635258358662614,"• At submission time, remember to anonymize your assets (if applicable). You can either
802"
NEW ASSETS,0.9645390070921985,"create an anonymized URL or include an anonymized zip file.
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9655521783181358,"14. Crowdsourcing and Research with Human Subjects
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9665653495440729,"Question: For crowdsourcing experiments and research with human subjects, does the paper
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9675785207700102,"include the full text of instructions given to participants and screenshots, if applicable, as
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685916919959473,"well as details about compensation (if any)?
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9696048632218845,"Answer: [NA]
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9706180344478217,"Justification: [TODO]
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716312056737588,"Guidelines:
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9726443768996961,"• The answer NA means that the paper does not involve crowdsourcing nor research with
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9736575481256332,"human subjects.
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746707193515705,"• Including this information in the supplemental material is fine, but if the main contribu-
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9756838905775076,"tion of the paper involves human subjects, then as much detail as possible should be
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766970618034447,"included in the main paper.
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977710233029382,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787234042553191,"or other labor should be paid at least the minimum wage in the country of the data
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797365754812564,"collector.
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807497467071935,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817629179331308,"Subjects
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827760891590679,"Question: Does the paper describe potential risks incurred by study participants, whether
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983789260385005,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848024316109423,"approvals (or an equivalent approval/review based on the requirements of your country or
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858156028368794,"institution) were obtained?
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868287740628167,"Answer: [NA]
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878419452887538,"Justification: [TODO]
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888551165146909,"Guidelines:
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898682877406282,"• The answer NA means that the paper does not involve crowdsourcing nor research with
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908814589665653,"human subjects.
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918946301925026,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929078014184397,"may be required for any human subjects research. If you obtained IRB approval, you
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993920972644377,"should clearly state this in the paper.
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949341438703141,"• We recognize that the procedures for this may vary significantly between institutions
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959473150962512,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969604863221885,"guidelines for their institution.
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979736575481256,"• For initial submissions, do not include any information that would break anonymity (if
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989868287740629,"applicable), such as the institution conducting the review.
837"
