Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010235414534288639,"We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty
quantiﬁcation and mean predictions with distributed clients. In particular, we
generalize beyond normal posterior distributions and consider a general class of
models. We develop theoretical guarantees for FA-LD for strongly log-concave
distributions with non-i.i.d data and study how the injected noise and the stochastic-
gradient noise, the heterogeneity of data, and the varying learning rates affect the
convergence. Such an analysis sheds light on the optimal choice of local updates
to minimize communication cost. Important to our approach is that the commu-
nication efﬁciency does not deteriorate with the injected noise in the Langevin
algorithms. In addition, we examine in our FA-LD algorithm both independent
and correlated noise used over different clients. We observe that there is also a
trade-off between federation and communication cost there. As local devices may
become inactive in the federated network, we also show convergence results based
on different averaging schemes where only partial device updates are available."
INTRODUCTION,0.0020470829068577278,"1
INTRODUCTION"
INTRODUCTION,0.0030706243602865915,"Federated learning (FL) allows multiple parties to jointly train a consensus model without sharing user
data. Compared to the classical centralized learning regime, federated learning keeps training data
on local clients, such as mobile devices or hospitals, where data privacy, security, and access rights
are a matter of vital interest. This aggregation of various data resources heeding privacy concerns
yields promising potential in areas of internet of things Chen et al. (2020), healthcare Li et al. (2020d;
2019b), text data Huang et al. (2020), and fraud detection Zheng et al. (2020)."
INTRODUCTION,0.0040941658137154556,"A standard formulation of federated learning is a distributed optimization framework that tackles
communication costs, client robustness, and data heterogeneity across different clients Li et al.
(2020a). Central to the formulation is the efﬁciency of the communication, which directly motivates
the communication-efﬁcient federated averaging (FedAvg) McMahan et al. (2017). FedAvg introduces
a global model to synchronously aggregate multi-step local updates on the available clients and yields
distinctive properties in communication. However, FedAvg often stagnates at inferior local modes
empirically due to the data heterogeneity across the different clients Charles & Koneˇcn`y (2020);
Woodworth et al. (2020). To tackle this issue, Karimireddy et al. (2020); Pathaky & Wainwright
(2020) proposed stateful clients to avoid the unstable convergence, which are, however, not scalable
with respect to the number of clients in applications with mobile devices Al-Shedivat et al. (2021).
In addition, the optimization framework often fails to quantify the uncertainty accurately for the
parameters of interest, which are crucial for building estimators, hypothesis tests, and credible
intervals. Such a problem leads to unreliable statistical inference and casts doubts on the credibility
of the prediction tasks or diagnoses in medical applications."
INTRODUCTION,0.00511770726714432,"To unify optimization and uncertainty quantiﬁcation in federated learning, we resort to a Bayesian
treatment by sampling from a global posterior distribution, where the latter is aggregated by infrequent
communications from local posterior distributions. We adopt a popular approach for inferring
posterior distributions for large datasets, the stochastic gradient Markov chain Monte Carlo (SG-
MCMC) method Welling & Teh (2011); Vollmer et al. (2016); Teh et al. (2016); Chen et al. (2014);
Ma et al. (2015), which enjoys theoretical guarantees beyond convex scenarios Raginsky et al. (2017);
Zhang et al. (2017); Mangoubi & Vishnoi (2018); Ma et al. (2019). In particular, we examine in
the federated learning setting the efﬁcacy of the stochastic gradient Langevin dynamics (SGLD)"
INTRODUCTION,0.006141248720573183,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.007164790174002047,"algorithm, which differs from stochastic gradient descent (SGD) in an additionally injected noise for
exploring the posterior. The close resemblance naturally inspires us to adapt the optimization-based
FedAvg to a distributed sampling framework. Similar ideas have been proposed in federated posterior
averaging Al-Shedivat et al. (2021), where empirical study and analyses on Gaussian posteriors have
shown promising potential of this approach. Compared to the appealing theoretical guarantees of
optimization-based algorithms in federated learning Pathaky & Wainwright (2020); Al-Shedivat et al.
(2021), the convergence properties of approximate sampling algorithms in federated learning is far
less understood. To ﬁll this gap, we proceed by asking the following question:"
INTRODUCTION,0.008188331627430911,Can we build a uniﬁed algorithm with convergence guarantees for sampling in FL?
INTRODUCTION,0.009211873080859774,"In this paper, we make a ﬁrst step in answering this question in the afﬁrmative. We propose
the federated averaging Langevin dynamics (FA-LD) for posterior inference beyond the Gaussian
distribution. We list our contributions as follows:"
INTRODUCTION,0.01023541453428864,"• We present a novel non-asymptotic convergence analysis for FA-LD from simulating strongly
log-concave distributions on non-i.i.d data when the learning rate is ﬁxed. The frequently
used bounded gradient assumption of ℓ2 norm in FedAvg optimization is not required.
• The convergence analysis indicates that the injected noise, the data heterogeneity, and the
stochastic-gradient noise are all driving factors that affect the convergence. Such an analysis
also provides a concrete guidance on the optimal selection of the number of local updates.
• We present a convergence result for FA-LD with decaying learning rates. This strategy
accelerates the computation by a logarithmic factor to achieve the precision ϵ.
• The algorithm yields appealing extensions: (1) we can choose to inject either independent
or correlated noise across local clients, yielding a trade-off between accuracy and efﬁcacy
of federation; (2) we can choose whether to activate all the devices to avoid the straggler’s
effect in real-world applications."
INTRODUCTION,0.011258955987717503,"Roadmap.
In Section 2, we discuss the related work and literature. In Section 3, we present
the preliminary knowledge. In Section 4, we propose the federated averaging Langevin dynamics
algorithm for posterior inference. In Section 5, we lay out the required assumptions, sketch the proof,
and show the theoretical convergence results. In Section 6, we conclude our work."
RELATED WORK,0.012282497441146366,"2
RELATED WORK"
RELATED WORK,0.01330603889457523,"Federated Learning
Current federated learning follows two paradigms. The ﬁrst paradigm asks
every client to learn the model using private data and communicate in model parameters. The second
one uses encryption techniques to guarantee secure communication between clients. In this paper, we
focus on the ﬁrst paradigms Dean et al. (2012); Shokri & Shmatikov (2015); McMahan et al. (2016;
2017); Huang et al. (2021). There is a long list of works showing provable convergence algorithm for
FedAvg types of algorithms in the ﬁeld of optimization Li et al. (2020c; 2021); Huang et al. (2021);
Khaled et al. (2019); Yu et al. (2019); Wang et al. (2019); Karimireddy et al. (2020). One line of
research Li et al. (2020c); Khaled et al. (2019); Yu et al. (2019); Wang et al. (2019); Karimireddy et al.
(2020) focuses on standard assumptions in optimization (such as, convex, smooth, strongly-convex,
bounded gradient). The other line of work Li et al. (2021); Huang et al. (2021) proves the convergence
in the regime where the model of interest is an over-parameterized neural network (also called NTK
regime Jacot et al. (2018))."
RELATED WORK,0.014329580348004094,"Scalable Monte Carlo methods
SGLD Welling & Teh (2011) is the ﬁrst stochastic gradient Monte
Carlo method that tackles the scalability issue in big data problems. Ever since, variants of stochastic
gradient Monte Carlo methods were proposed to accelerate the simulations by utilizing more general
Markov dynamics Ma et al. (2015; 2018); Chen et al. (2014), Hessian approximation Ahn et al.
(2012), parallel tempering Deng et al. (2020), as well as higher-order numerical schemes Chen et al.
(2015); Li et al. (2019c); Cheng et al. (2018); Ma et al. (2021); Mou et al. (2021); Shen & Lee (2019)."
RELATED WORK,0.015353121801432957,"Distributed Monte Carlo methods
Sub-posterior aggregation was initially proposed in
Neiswanger et al. (2013); Wang & Dunson; Minsker et al. (2014) to accelerate MCMC methods to
cope with large datasets. Other parallel MCMC algorithms Nishihara et al. (2014); Ahn et al. (2014);"
RELATED WORK,0.016376663254861822,Under review as a conference paper at ICLR 2022
RELATED WORK,0.017400204708290685,"Chen et al. (2016); Chowdhury & Jermaine (2018); Li et al. (2019a) propose to improve the efﬁciency
of Monte Carlo computation in distributed or asynchronous systems. Gürbüzbalaban et al. (2021)
proposed stochastic gradient Monte Carlo methods in decentralized systems. Al-Shedivat et al. (2021)
introduced empirical studies of posterior averaging in federated learning."
RELATED WORK,0.01842374616171955,"Notation
For any positive integer n, we use [n] to denote the set {1, 2, · · · , n}. Let N denote the
number of clients. For each c ∈[N], we use f c and ∇f c as the loss function and gradient of the
function f c in client c. ∇ef c(·) is the unbiased stochastic gradient of ∇f c. In addition, we denote pc
as the weight of the c-th client such that pc =
nc
PN
i=1 ni ∈(0, 1), where nc > 0 is the number of data
points in the c-th client. Let Tϵ denote the number of global steps to achieve the precision ϵ. Let K
denote the number of local steps and hence Tϵ/K denotes the number of communications."
PRELIMINARIES,0.019447287615148412,"3
PRELIMINARIES"
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.02047082906857728,"3.1
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING"
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.021494370522006142,"Federated averaging (FedAvg) is a standard algorithm in federated learning and is typically formulated
into a distributed optimization framework as follows"
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.022517911975435005,"min
θ
ℓ(θ) :=
PN
c=1 ℓc(θ)
PN
c=1 nc
,
ℓc(θ) := nc
X"
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.02354145342886387,"i=1
l(θ; xc,i),
(1)"
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.02456499488229273,"where θ ∈Rd, l(θ; xc,j) is a certain loss function based on θ and the data point xc,j."
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.0255885363357216,One iterate of the FedAvg algorithm requires the following three steps:
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.02661207778915046,"• Broadcast: The center server broadcasts the latest model, θk, to all local clients.
• Local updates: For any c ∈[N], the c-the client ﬁrst sets θc
k = θk and then conducts K ≥1
local steps:"
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.027635619242579325,"βc
k+1 = θc
k −η∇eℓc(θc
k),"
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.028659160696008188,"where η is the learning rate and ∇eℓc is the unbiased estimate of the exact gradient ∇ℓc.
• Synchronization:
The local models are aggregated into a unique model θk+K
:=
PN
c=1 pcβc
k+K and sent to the center server."
AN OPTIMIZATION PERSPECTIVE ON FEDERATED AVERAGING,0.02968270214943705,"From the optimization perspective, Li et al. (2020c) proved the convergence of the FedAvg algorithm
on non-i.i.d data such that a larger number of local steps K and a higher order of data heterogeneity
slows down the convergence. Notably, Eq. (1) can be interpreted as maximizing the likelihood
function, which is a special case of maximum a posteriori estimation (MAP) given a uniform prior."
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.030706243602865915,"3.2
STOCHASTIC GRADIENT LANGEVIN DYNAMICS"
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.03172978505629478,"Posterior inference offers the exact uncertainty quantiﬁcation ability of the predictions. A popular
method for posterior inference with large dataset is the stochastic gradient Langevin dynamics
(SGLD) Welling & Teh (2011), which injects additional noise into the stochastic gradient and adapts
an optimization algorithm to a sampling one"
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.032753326509723645,"θk+1 = θk −η∇ef(θk) +
p"
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.033776867963152504,"2τηξk,"
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.03480040941658137,"where τ is the temperature and ξk is a standard d-dimensional Gaussian vector. f(θ) := PN
c=1 ℓc(θ)
is a energy function. ef(θ) is a unbiased estimate of f(θ). In the longtime limit, a well known result
is that θk converges weakly to the distribution π(θ) ∝exp(−f(θ)/τ) Teh et al. (2016) as η →0."
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.03582395087001024,"4
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.0368474923234391,"The increasing concern for uncertainty estimation in federated learning motivates us to consider the
simulation of the distribution π(θ) ∝exp(−f(θ)/τ) with distributed clients."
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.037871033776867964,Under review as a conference paper at ICLR 2022
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.038894575230296824,"Problem formulation
We propose the federated averaging Langevin dynamics (FA-LD) based
on the FedAvg framework in section 3.1. We follow the same broadcast step and synchronization
step but propose to inject random noises for local updates. In particular, we consider the following
scheme: for any c ∈[N], the c-the client ﬁrst sets θc
k = θk and then conducts K ≥1 local steps:"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.03991811668372569,"βc
k+1 = θc
k −η∇ef c(θc
k) +
p"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.04094165813715456,"2ητΞc
k,
(2)"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.04196519959058342,"where ∇f c(θ) =
1
pc ∇ℓc(θ). ∇ef c(θ) is the unbiased estimate of ∇f c(θ) and Ξc
k is an independent
Gaussian vector to be deﬁned later."
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.042988741044012284,"Summing Eq. (2) from clients c = 1 to N, we have the aggregated stochastic process as follows"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.044012282497441144,"βk+1 = θk −η∇ef(θk) +
p"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.04503582395087001,"2ητξk, where βk = N
X"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.04605936540429888,"c=1
pcβc
k,
θk = N
X"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.04708290685772774,"c=1
pcθc
k,
∇ef(θk) = N
X"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.048106448311156604,"c=1
pc∇ef c(θc
k),
ξk = N
X"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.04912998976458546,"c=1
pcΞc
k.
(3)"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.05015353121801433,"By the nature of the synchronization step, we always have βk = θk whether k + 1 mod K = 0 or not.
In what follows, we can write"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.0511770726714432,"θk+1 = θk −η∇ef(θk) +
p"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.052200614124872056,"2ητξk,
(4)"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.05322415557830092,"which resembles the SGLD algorithm except that the construction of stochastic gradients is dif-
ferent and θk is not accessible when k mod K ̸= 0.
Since our target is to simulate from
π(θ) ∝exp(−f(θ)/τ), we expect that ξk is a standard Gaussian vector.
By the concentra-
tion property of independent Gaussian variables, it is natural to set Ξc
k = ξc
k/√pc so that
ξk = PN
c=1 pcΞc
k = PN
c=1
√pcξc
k and ξc
k is also a standard Gaussian vector. Now we present
it in Algorithm 1."
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.05424769703172978,"Algorithm 1 Federated averaging Langevin dynamics algorithm (FA-LD), informal version of
Algorithm 4. ηk is the learning rate at iteration k. τ is the temperature. Denote by θc
k the model
parameter in the c-th client at the k-th step. Denote the immediate result of one step SGLD update
from θc
k by βc
k. ξc
k is an independent standard d-dimensional Gaussian vector at iteration k for each
client c ∈[N]. A global synchronization is conducted every K steps. 1:"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.05527123848515865,"βc
k+1 = θc
k −ηk∇ef c(θc
k) +
p"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.05629477993858751,"2ηkτ/pcξc
k,
(5) 2:"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.057318321392016376,"θc
k+1 = 
 "
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.05834186284544524,"βc
k+1
if k + 1 mod K ̸= 0"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.0593654042988741,"PN
c=1 pcβc
k+1
if k + 1 mod K = 0.
(6)"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.06038894575230297,"Algorithm 2 Hybrid federated averaging Langevin dynamics algorithm (hFA-LD), informal version
of Algorithm 5. ˙ξk is a d-dimensional Gaussian vector shared by all the clients; ξc
k is an independent
standard d-dimensional Gaussian vector at iteration k for each client c ∈[N]. ρ denotes the
correlation coefﬁcient. 1:"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.06141248720573183,"βc
k+1 = θc
k −η∇ef c(θc
k) +
p"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.062436028659160696,"2ητρ2 ˙ξk +
p"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.06345957011258956,"2η(1 −ρ2)τ/pcξc
k, 2:"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.06448311156601842,"θc
k+1 = 
 "
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.06550665301944729,"βc
k+1
if k + 1 mod K ̸= 0"
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.06653019447287616,"PN
c=1 pcβc
k+1
if k + 1 mod K = 0."
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.06755373592630501,"We observe that the local process in Eq. (5) maintains a temperature τ/pc > τ to converge to the
stationary distribution π. Such a mechanism may limit the disclosure of individual data and shows a
potential to ensure a higher level of privacy."
POSTERIOR INFERENCE VIA FEDERATED AVERAGING LANGEVIN DYNAMICS,0.06857727737973388,Under review as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS,0.06960081883316274,"5
CONVERGENCE ANALYSIS"
CONVERGENCE ANALYSIS,0.07062436028659161,"In this section, we show that FA-LD converges to the stationary distribution π(θ) in the 2-Wasserstein
(W2) distance at a rate of O(1/√Tϵ) for strongly log-concave and smooth density. The W2 distance
is deﬁned between a pair of Borel probability measures µ and ν on Rd as follows"
CONVERGENCE ANALYSIS,0.07164790174002048,"W2(µ, ν) :=
inf
γ2∈Couplings(µ,ν)"
CONVERGENCE ANALYSIS,0.07267144319344933,"Z
∥βµ −βν∥2
2dγ2(βµ, βν)
 1 2
,"
CONVERGENCE ANALYSIS,0.0736949846468782,"where ∥· ∥2 denotes the ℓ2 norm on Rd and the pair of random variables (βµ, βν) ∈Rd × Rd is
a coupling with the marginals following L(βµ) = µ and L(βν) = ν. Note that L(·) denotes a
distribution of a random variable. Such a distance is more appealing than the total variation or the
Kullback–Leibler divergence in statistical machine learning applications for providing the estimates
of the ﬁrst and second order moments."
NOTATION AND ASSUMPTIONS,0.07471852610030706,"5.1
NOTATION AND ASSUMPTIONS"
NOTATION AND ASSUMPTIONS,0.07574206755373593,"We make standard assumptions on the smoothness and convexity of the functions f 1, f 2, · · · , f N,
which naturally yields appealing tail properties of the stationary measure π. Thus, we no longer
require a restrictive assumption on the bounded gradient in ℓ2 norm as in Koloskova et al. (2019); Yu
et al. (2019); Li et al. (2020c). In addition, to control the distance between ∇f c and ∇ef c, we also
assume a bounded variance of the stochastic gradient in assumption 5.3."
NOTATION AND ASSUMPTIONS,0.0767656090071648,"Assumption 5.1 (Smoothness). For each c ∈[N], we say f c is L-smooth if for some L > 0"
NOTATION AND ASSUMPTIONS,0.07778915046059365,"f c(y) ≤f c(x) + ⟨∇f c(x), y −x⟩+ L"
NOTATION AND ASSUMPTIONS,0.07881269191402251,"2 ∥y −x∥2
2
∀x, y ∈Rd."
NOTATION AND ASSUMPTIONS,0.07983623336745138,"Assumption 5.2 (Strongly convex). For each c ∈[N], f c is m-strongly convex if for some m > 0"
NOTATION AND ASSUMPTIONS,0.08085977482088025,"f c(x) ≥f c(y) + ⟨∇f c(y), x −y⟩+ m"
NOTATION AND ASSUMPTIONS,0.08188331627430911,"2 ∥y −x∥2
2
∀x, y ∈Rd."
NOTATION AND ASSUMPTIONS,0.08290685772773797,"Assumption 5.3 (Bounded variance, informal version of Assumption A.3). For each c ∈[N], the
variance of noise in the stochastic gradient ∇ef c(x) in each client is upper bounded such that"
NOTATION AND ASSUMPTIONS,0.08393039918116683,"E[∥∇ef c(x) −∇f c(x)∥2
2] ≤σ2d,
∀x ∈Rd."
NOTATION AND ASSUMPTIONS,0.0849539406345957,"Quality of non-i.i.d data
Denote by θ∗the global minimum of f. Next, we quantify the degree
of the non-i.i.d data by γ := maxc∈[N] ∥∇f c(θ∗)∥2, which is a non-negative constant and yields a
larger scale if the data is less identically distributed."
PROOF SKETCH,0.08597748208802457,"5.2
PROOF SKETCH"
PROOF SKETCH,0.08700102354145343,"The proof hinges on showing the one-step result in the W2 distance. To facilitate the analysis, we
ﬁrst deﬁne an auxiliary continuous-time processes (¯θt)t≥0 without communication concerns"
PROOF SKETCH,0.08802456499488229,"d¯θt = −∇f(¯θt)dt +
√"
PROOF SKETCH,0.08904810644831115,"2τdW t,
(7)"
PROOF SKETCH,0.09007164790174002,"where ¯θt = PN
c=1 pc¯θc
t, ∇f(¯θt) = PN
c=1 pc∇f c(¯θc
t), ¯θc
t is the continuous-time variable at client c,
and W is a d-dimensional Brownian motion. The continuous-time algorithm is known to converge to
the stationary distribution π(θ) ∝e−f(θ)"
PROOF SKETCH,0.09109518935516889,"τ , where f(θ) = PN
c=1 pcf c(θ). Assume that ¯θ0 simulates
from the stationary distribution π, then it follows that ¯θt ∼π for any t ≥0."
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.09211873080859775,"5.2.1
DOMINATED CONTRACTION IN FEDERATED LEARNING"
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.0931422722620266,"The ﬁrst target is to show a certain contraction property of ∥β −θ −η(∇f(β) −∇f(θ))∥2
2 based on
distributed clients with infrequent communications. Consider a standard decomposition"
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.09416581371545547,"∥β −θ −η(∇f(β) −∇f(θ))∥2
2"
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.09518935516888434,Under review as a conference paper at ICLR 2022
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.09621289662231321,"= ∥β −θ∥2
2 −2η ⟨β −θ, ∇f(β) −∇f(θ)⟩
|
{z
}
I"
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.09723643807574207,"+η2 ∥∇f(β) −∇f(θ)∥2
2 ."
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.09825997952917093,"Using Eq.(3), we decompose I and apply Jensen’s inequality to obtain the lower bound of I. In what
follows, we have the following lemma.
Lemma 5.4 (Dominated contraction property, informal version of Lemma B.1). Assume assumptions
5.1 and 5.2 hold. For any learning rate η ∈(0,
1
L+m], any {θc}N
c=1, {βc}N
c=1 ∈Rd, we have"
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.0992835209825998,"∥β −θ −η(∇f(β) −∇f(θ))∥2
2 ≤(1 −ηm) · ∥β −θ∥2
2 + 4ηL N
X"
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.10030706243602866,"c=1
pc · (∥βc −β∥2
2 + ∥θc −θ∥2
2)
|
{z
}
divergence term ,"
DOMINATED CONTRACTION IN FEDERATED LEARNING,0.10133060388945753,"where β
=
PN
c=1 pcβc, θ
=
PN
c=1 pcθc, ∇f(θ)
=
PN
c=1 pc∇f c(θc), and ∇f(β)
=
PN
c=1 pc∇f c(βc). It implies that as long as the local parameters θc, βc and global θ, β don’t
differ each other too much, we can guarantee the desired convergence. In a special case when the
communication is conducted at every iteration, the divergence term disappears and recovers the
standard contraction Dalalyan & Karagulyan (2019)."
BOUNDING DIVERGENCE,0.1023541453428864,"5.2.2
BOUNDING DIVERGENCE"
BOUNDING DIVERGENCE,0.10337768679631525,"The following result shows that given a ﬁnite number of local steps K, the divergence between θc in
local client and θ in the center is bounded in ℓ2 norm. Notably, since the Brownian motion leads to a
lower order term O(η) instead of O(η2), a naïve proof framework such as Li et al. (2020c) may lead
to a crude upper bound for the ﬁnal convergence.
Lemma 5.5 (Bounded divergence, informal version of Lemma B.3). Assume assumptions 5.1, 5.2,
and 5.3 hold. For any learning rate η ∈(0, 2/m) and ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], we have
the ℓ2 upper bound of the divergence between local clients and the center as follows N
X"
BOUNDING DIVERGENCE,0.10440122824974411,"c=1
pcE∥θc
k −θk∥2
2 ≤O(K2η2d) + O(Kηd)."
BOUNDING DIVERGENCE,0.10542476970317298,"The result also relies on showing a uniform upper bound in ℓ2 norm, which avoids making extra
bounded gradient assumptions."
COUPLING TO THE STATIONARY PROCESS,0.10644831115660185,"5.2.3
COUPLING TO THE STATIONARY PROCESS"
COUPLING TO THE STATIONARY PROCESS,0.10747185261003071,"Note that ¯θt is initialized from the stationary distribution π. The solution to the continuous-time
process Eq.(7) follows:"
COUPLING TO THE STATIONARY PROCESS,0.10849539406345957,"¯θt = ¯θ0 −
Z t"
COUPLING TO THE STATIONARY PROCESS,0.10951893551688843,"0
∇f(¯θs)ds +
√"
COUPLING TO THE STATIONARY PROCESS,0.1105424769703173,"2τ · W t,
∀t ≥0.
(8)"
COUPLING TO THE STATIONARY PROCESS,0.11156601842374617,"Set t →(k + 1)η and ¯θ0 →¯θkη for Eq.(8) and consider a synchronous coupling such that W(k+1)η −
Wkη := √ηξk is used to cancel the noise terms, we have"
COUPLING TO THE STATIONARY PROCESS,0.11258955987717502,"¯θ(k+1)η = ¯θkη −
Z (k+1)η"
COUPLING TO THE STATIONARY PROCESS,0.11361310133060389,"kη
∇f(¯θs)ds +
p"
COUPLING TO THE STATIONARY PROCESS,0.11463664278403275,"2τηξk.
(9)"
COUPLING TO THE STATIONARY PROCESS,0.11566018423746162,Subtracting Eq.(4) from Eq.(9) and taking square and expectation on both sides yield that
COUPLING TO THE STATIONARY PROCESS,0.11668372569089049,"E∥¯θ(k+1)η −θk+1∥2
2 ≤(1 −ηm/2) · E∥¯θkη −θk∥2
2 + divergence term + time error."
COUPLING TO THE STATIONARY PROCESS,0.11770726714431934,"Eventually, we arrive at the one-step error bound for establishing the convergence results.
Lemma 5.6 (One step update, informal version of Lemma B.5). Assume assumptions 5.1, 5.2, and
5.3 hold. Consider Algorithm 1 with any learning rate η ∈(0,
1
2L) and ∥θc
0 −θ∗∥2
2 ≤dD2 for any
c ∈[N], where θ∗is the global minimum for the function f. Then"
COUPLING TO THE STATIONARY PROCESS,0.1187308085977482,"W 2
2 (µk+1, π) ≤(1 −ηm/2) · W 2
2 (µk, π) + O(η2d(K2 + κ)),"
COUPLING TO THE STATIONARY PROCESS,0.11975435005117707,where µk denotes the probability measure of θk and κ = L/m is the condition number.
COUPLING TO THE STATIONARY PROCESS,0.12077789150460594,Under review as a conference paper at ICLR 2022
FULL DEVICE PARTICIPATION,0.1218014329580348,"5.3
FULL DEVICE PARTICIPATION"
CONVERGENCE BASED ON INDEPENDENT NOISE,0.12282497441146366,"5.3.1
CONVERGENCE BASED ON INDEPENDENT NOISE"
CONVERGENCE BASED ON INDEPENDENT NOISE,0.12384851586489252,"When the synchronization step is conducted at every iteration k, the FA-LD algorithm is essentially
the standard SGLD algorithm Welling & Teh (2011). Theoretical analysis based on the 2-Wasserstein
distance has been established in Durmus & Moulines (2016); Dalalyan (2017a); Dalalyan & Karag-
ulyan (2019). However, in scenarios of K > 1 with distributed clients, a divergence between the
global variable θk and local variable θc
k appears and unavoidably affects the performance. The upper
bound on the sampling error is presented as follows.
Theorem 5.7 (Main result, informal version of Theorem B.6). Assume assumptions 5.1, 5.2, and 5.3
hold. Consider Algorithm 1 with a ﬁxed learning rate η ∈(0,
1
2L] and ∥θc
0 −θ∗∥2
2 ≤dD2 for any
c ∈[N], we have †"
CONVERGENCE BASED ON INDEPENDENT NOISE,0.12487205731832139,"W2(µk, π) ≤(1 −ηm/4)k ·
√"
D,0.12589559877175024,"2d
 
D +
p"
D,0.1269191402251791,"τ/m

+ 30κ
p"
D,0.12794268167860798,"ηmd ·
p"
D,0.12896622313203684,(K2 + κ)H0.
D,0.1299897645854657,"where µk denotes the probability measure of θk at iteration k, K denotes the number of local updates,
κ := L/m, γ := maxc∈[N] ∥∇f c(θ∗)∥2, and H0 := D2 + maxc∈[N]
τ
mpc +
γ2"
D,0.13101330603889458,m2d + σ2 m2 .
D,0.13203684749232344,"We observe that the initialization, the scale of the injected noise, the heterogeneity of the data, and
the noise in the stochastic gradient all affect the convergence. Similar to the result of Li et al. (2020c),
FA-LD with K-local steps resembles the behaviour of one-step SGLD with a large learning rate."
D,0.1330603889457523,"Optimal choice of K. To ensure the algorithm to achieve the precision ϵ based on the total number
of steps Tϵ and the learning rate η, we can set 30κ
p"
D,0.13408393039918118,"ηmd ·
p"
D,0.13510747185261002,"(K2 + κ)H0 ≤ϵ/2,
exp
 
−ηm"
D,0.13613101330603888,"4 Tϵ

·
√"
D,0.13715455475946775,"2d(D +
p"
D,0.13817809621289662,τ/m) ≤ϵ/2.
D,0.13920163766632548,This readily leads to
D,0.14022517911975435,"ηm ≤O

ϵ2"
D,0.14124872057318322,dκ2(K2 + κ)H0
D,0.14227226202661208,"
,
Tϵ ≥Ω
log(d/ϵ) mη 
."
D,0.14329580348004095,"Plugging into the upper bound of ηm, it implies that to reach the precision ϵ, it sufﬁces to set"
D,0.14431934493346982,"Tϵ = Ω(ϵ−2dκ2(K2 + κ)H0 · log(d/ϵ)).
(10)"
D,0.14534288638689866,"It’s obvious that H0 = Ω(D2) = Ω(1), thus we can conclude that the number of communication
rounds is around the order
Tϵ
K = Ω

K + κ K 
,"
D,0.14636642784032752,where the value of Tϵ
D,0.1473899692937564,"K ﬁrst decreases and then increases with respect to K, indicating that setting K
either too large or too small may lead to high communication costs and hurt the performance. Ideally,
K should be selected in the scale of Ω(√κ). Combining the deﬁnition of Tϵ in Eq. (10), this suggests
an interesting result that the optimal K for FA-LD should be in the order of O(√Tϵ). Similar results
have been achieved by Stich (2019); Li et al. (2020c)."
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.14841351074718526,"5.3.2
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES"
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.14943705220061412,"Theorem 5.8 (Informal version of Theorem B.7). Assume assumptions 5.1, 5.2, and 5.3 hold.
Consider Algorithm 1 with an initialization satisfying ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N] and the
varying learning rate following"
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.150460593654043,"ηk =
1
2L + (1/12)mk ,
k = 1, 2, · · · ."
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.15148413510747186,"Then for any k ≥0, we have"
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.15250767656090072,"W2(µk, π) ≤45κ
p"
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.1535312180143296,"(K2 + κ)H0 ·
 
ηkmd
1/2,
∀k ≥0."
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.15455475946775846,"†For ease of presentation, we report the result based on K2 instead of (K −1)2. The upper bound based on
(K −1)2 is detailed in the supplementary ﬁle."
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.1555783009211873,Under review as a conference paper at ICLR 2022
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.15660184237461616,"Note that the above result implies that to achieve the precision ϵ, we require"
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.15762538382804503,"W2(µk, π) ≤45κ
p"
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.1586489252814739,"(K2 + κ)H0 ·

md
2L + (1/12)mk"
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.15967246673490276,"1/2
≤ϵ."
CONVERGENCE GUARANTEES VIA VARYING LEARNING RATES,0.16069600818833163,"We therefore require Ω(ϵ−2d) iterations to achieve the precision ϵ, which improves the
Ω(ϵ−2d log(d/ϵ)) rate for FA-LD with a ﬁxed learning rate by a O(log(d/ϵ)) factor."
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.1617195496417605,"5.3.3
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.16274309109518936,"Note that Algorithm 1 requires all the local clients to generate the independent noise ξc
k. Such a
mechanism enjoys the convenience of the implementation and yields a potential to protect the privacy
of data and alleviates the security issue. However, the large scale noise inevitable slows down the
convergence. To handle this issue, the independent noise can be generalized to correlated noise based
on a correlation coefﬁcient ρ between different clients. Replacing Eq. (5) with"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.16376663254861823,"βc
k+1 = θc
k −η∇ef c(θc
k) +
p"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.1647901740020471,"2ητρ2 ˙ξk +
p"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.16581371545547594,"2η(1 −ρ2)τ/pcξc
k,
(11)"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.1668372569089048,"where ˙ξk is a d-dimensional standard Gaussian vector shared by all the clients at iteration k and ˙ξk is
dependent with ξc
k for any c ∈[N]. Following the synchronization step based on Eq. (6), we have"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.16786079836233367,"θk+1 = θk −η∇ef(θk) +
p"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.16888433981576254,"2ητξk,
(12)"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.1699078812691914,"where ξk = ρ ˙ξk +
p"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.17093142272262027,"1 −ρ2 PN
c=1
√pcξc
k. Since the variance of i.i.d variables is additive, it is clear
that ξk follows the standard d-dimensional Gaussian distribution. The inclusion of the correlated
noise implicitly reduces the temperature for each client and naturally yields a trade-off between
federation and accuracy. We refer to the algorithm with correlated noise as the hybrid federated
averaging Langevin dynamics (hFA-LD) and present it in Algorithm 2."
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.17195496417604914,"Since the inclusion of correlated noise doesn’t affect the iterate of Eq. (12), the algorithm property
maintains the same except the scale of the temperature τ and efﬁcacy of federation are changed.
Based on a target correlation coefﬁcient ρ ≥0, Eq. (11) is equivalent to applying a temperature
Tc,ρ = τ(ρ2 +(1−ρ2)/pc). In particular, setting ρ = 0 leads to Tc,0 = τ/pc, which exactly recovers
Algorithm 1; however, setting ρ = 1 leads to Tc,1 = τ, where the injected noise in local clients is
reduced by 1/pc times. Now we adjust the analysis as follows
Theorem 5.9 (Informal version of Theorem B.8). Assume assumptions 5.1, 5.2, and 5.3 hold.
Consider Algorithm 2 with a correlation coefﬁcient ρ ∈[0, 1], η ∈(0,
1
2L] and ∥θc
0 −θ∗∥2
2 ≤dD2
for any c ∈[N], we have"
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES,0.172978505629478,"W2(µk, π) ≤(1 −ηm/4)k ·
√"
D,0.17400204708290687,"2d
 
D +
p"
D,0.1750255885363357,"τ/m

+ 30κ
p"
D,0.17604912998976457,"ηmd ·
q"
D,0.17707267144319344,"(K2 + κ)Hρ,"
D,0.1780962128966223,"where µk denotes the probability measure of θk, Hρ := D2 + 1"
D,0.17911975435005117,"m maxc∈[N] Tc,ρ +
γ2"
D,0.18014329580348004,m2d + σ2 m2 .
D,0.1811668372569089,"Such a mechanism leads to a trade-off between the efﬁcacy of federation and accuracy and motivates
us to exploit the optimal ρ under the differential-privacy theories Wang et al. (2015)."
PARTIAL DEVICE PARTICIPATION,0.18219037871033777,"5.4
PARTIAL DEVICE PARTICIPATION"
PARTIAL DEVICE PARTICIPATION,0.18321392016376664,"Full device participation enjoys appealing convergence properties. However, it suffers from the
straggler’s effect in real-world applications, where the communication is limited by the slowest
device. Partial device participation handles this issue by only allowing a small portion of devices in
each communication and greatly increased the communication efﬁciency in a federated network."
PARTIAL DEVICE PARTICIPATION,0.1842374616171955,"The ﬁrst device-sampling scheme I Li et al. (2020b) selects a total of S devices, where the c-th device
is selected with a probability pc. The ﬁrst theoretical justiﬁcation for convex optimization has been
proposed by Li et al. (2020c)."
PARTIAL DEVICE PARTICIPATION,0.18526100307062435,"(Scheme I: with replacement).
Assume Sk = {n1, n2, · · · , nS}, where nj ∈[N] is a random
number that takes a value of c with a probability pc for any j ∈{1, 2, · · · , S}. The synchronization
step follows that θk = 1 S
P"
PARTIAL DEVICE PARTICIPATION,0.1862845445240532,"c∈Sk θc
k."
PARTIAL DEVICE PARTICIPATION,0.18730808597748208,"Another strategy is to uniformly select S devices without replacement. We follow Li et al. (2020c)
and assume S indices are selected uniformly without replacement. In addition, the convergence also
requires an additional assumption on balanced data Li et al. (2020c)."
PARTIAL DEVICE PARTICIPATION,0.18833162743091095,Under review as a conference paper at ICLR 2022
PARTIAL DEVICE PARTICIPATION,0.18935516888433981,"(Scheme II: without replacement).
Assume Sk = {n1, n2, · · · , nS}, where nj ∈[N] is a
random number that takes a value of c with a probability 1"
PARTIAL DEVICE PARTICIPATION,0.19037871033776868,"S for any j ∈{1, 2, · · · , S}. Assume
the data is balanced such that p1 = · · · = pN =
1
N . The synchronization step follows that
θk = N S
P"
PARTIAL DEVICE PARTICIPATION,0.19140225179119755,"c∈Sk pcθc
k = 1 S
P"
PARTIAL DEVICE PARTICIPATION,0.19242579324462641,"c∈Sk θc
k."
PARTIAL DEVICE PARTICIPATION,0.19344933469805528,"Algorithm 3 Hybrid federated Averaging Langevin dynamics Algorithm (FA-LD) with partial device
participation, informal version of Algorithm 6. Sk is sampled according to a device-sampling rule
based on scheme I or II. 1:"
PARTIAL DEVICE PARTICIPATION,0.19447287615148415,"βc
k+1 = θc
k −η∇ef c(θc
k) +
p"
PARTIAL DEVICE PARTICIPATION,0.195496417604913,"2ητρ2 ˙ξk +
p"
PARTIAL DEVICE PARTICIPATION,0.19651995905834185,"2η(1 −ρ2)τ/pcξc
k, 2:"
PARTIAL DEVICE PARTICIPATION,0.19754350051177072,"θc
k+1 = 
 "
PARTIAL DEVICE PARTICIPATION,0.1985670419651996,"βc
k+1
if k + 1 mod K ̸= 0"
PARTIAL DEVICE PARTICIPATION,0.19959058341862845,"P
c∈Sk+1
1
S βc
k+1
if k + 1 mod K = 0."
PARTIAL DEVICE PARTICIPATION,0.20061412487205732,"Theorem 5.10 (Informal version of Theorem C.3). Assume assumptions 5.1, 5.2, and 5.3 hold.
Consider Algorithm 3 with a hyperparameter ρ ∈[0, 1], a ﬁxed learning rate η ∈(0,
1
2L] and
∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], we have"
PARTIAL DEVICE PARTICIPATION,0.2016376663254862,"W2(µk, π) ≤(1 −ηm/4)k ·
√"
D,0.20266120777891505,"2d
 
D +
p"
D,0.20368474923234392,"τ/m
"
D,0.2047082906857728,"+ 30κ
p"
D,0.20573183213920163,"ηmd ·
q"
D,0.2067553735926305,"Hρ(K2 + κ) + O
r"
D,0.20777891504605936,"d
S (ρ2 + N(1 −ρ2))CS 
,"
D,0.20880245649948823,where CS = 1 for Scheme I and CS = N−S
D,0.2098259979529171,N−1 for Scheme II.
D,0.21084953940634596,"We observe that partial device participation leads to an extra bias regardless of the scale of η. To
reduce such a bias, we suggest to consider highly correlated injected noise, such as ρ = 1, to reduce
the impact of the injected noise. By setting O(
p"
D,0.21187308085977483,"d/S) ≤ϵ/3 and following a similar learning rate
as in section 5.3.1, we can achieve the precision ϵ within Ω(ϵ−2d log(d/ϵ)) iterations given a large
number of devices satisfying S = Ω(ϵ−2d)."
D,0.2128966223132037,"The device-sampling scheme I provides a viable solution to handle the straggler’s effect in full device
participation and greatly accelerates the communication efﬁciency. In addition, scheme I is rather
robust to the data heterogeneity and doesn’t require the data to be balanced. In other words, this
device-sampling scheme is more preferred if a system is free to activate any devices at any time."
D,0.21392016376663256,"In more practical cases where a system can only operate based on the ﬁrst S messages for the local
updates. The device-sampling scheme II proposes a concrete treatment to tackle this issue. Given
a balanced data across different clients and each device is uniformly sampled, we can achieve a
reasonable approximation. If S = 1, our Scheme II matches the result in the Scheme I. If S = N,
then our Scheme II recovers the result in the full device setting. If S = N −o(N), then our Scheme
II bound is better than scheme I."
CONCLUSION AND FUTURE WORK,0.21494370522006143,"6
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.21596724667349027,"We propose a novel convergence analysis for federated averaging Langevin dynamics (FA-LD) with
distributed clients. Our results no longer require the bounded gradient assumption in ℓ2 norm as in
the optimization-driven literature in federated learning. The theoretical guarantees yield a concrete
guidance on the selection of the optimal number of local updates. In addition, the convergence highly
depends on the data heterogeneity and the injected noises, where the latter also inspires us to consider
correlated injected noise to balance between the efﬁcacy of federation and accuracy."
CONCLUSION AND FUTURE WORK,0.21699078812691913,"Our work initiated the theoretical study of standard sampling algorithms in federated learning and
paved the way for future works of advanced Monte Carlo methods, such as underdamped Langevin
dynamics Cheng et al. (2018), replica exchange Monte Carlo (also known as parallel tempering)
Deng et al. (2020) in federated learning. It is also interesting to study the optimal number of local
steps under the non-strongly convex Dalalyan (2017b) or non-convex assumptions Raginsky et al.
(2017); Ma et al. (2019)."
CONCLUSION AND FUTURE WORK,0.218014329580348,Under review as a conference paper at ICLR 2022
REFERENCES,0.21903787103377687,REFERENCES
REFERENCES,0.22006141248720573,"Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian Posterior Sampling via Stochastic
Gradient Fisher Scoring. In Proc. of the International Conference on Machine Learning (ICML),
2012."
REFERENCES,0.2210849539406346,"Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed Stochastic Gradient MCMC. In
International Conference on Machine Learning (ICML), 2014."
REFERENCES,0.22210849539406347,"Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated Learning
via Posterior Averaging: A New Perspective and Practical Algorithms. In ICLR. arXiv:2010.05273,
2021."
REFERENCES,0.22313203684749233,"Zachary Charles and Jakub Koneˇcn`y. On the Outsized Importance of Learning Rates in Local Update
Methods. arXiv:2007.00878, 2020."
REFERENCES,0.2241555783009212,"Changyou Chen, Nan Ding, and Lawrence Carin. On the Convergence of Stochastic Gradient MCMC
Algorithms with High-order Integrators. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 2278–2286, 2015."
REFERENCES,0.22517911975435004,"Changyou Chen, Nan Ding, Chunyuan Li, Yizhe Zhang, and Lawrence Carin. Stochastic Gradient
MCMC with Stale Gradients. In Advances in Neural Information Processing Systems (NeurIPS),
2016."
REFERENCES,0.2262026612077789,"Mingzhe Chen, Zhaohui Yang, Walid Saad, Changchuan Yin, H Vincent Poor, and Shuguang Cui. A
Joint Learning and Communications Framework for Federated Learning over Wireless Networks.
IEEE Trans. on Wireless Communications, 2020."
REFERENCES,0.22722620266120777,"Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic Gradient Hamiltonian Monte Carlo. In
Proc. of the International Conference on Machine Learning (ICML), 2014."
REFERENCES,0.22824974411463664,"Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped Langevin
MCMC: A non-asymptotic analysis. In Conference on Learning Theory (COLT), pp. 300–323.
PMLR, 2018."
REFERENCES,0.2292732855680655,"Arkabandhu Chowdhury and Chris Jermaine. Parallel and Distributed MCMC via Shepherding
Distributions. In AISTAT, 2018."
REFERENCES,0.23029682702149437,"Arnak S. Dalalyan. Further and Stronger Analogy Between Sampling and Optimization: Langevin
Monte Carlo and Gradient Descent. In Conference on Learning Theory (COLT), June 2017a."
REFERENCES,0.23132036847492324,"Arnak S Dalalyan. Theoretical Guarantees for Approximate Sampling from Smooth and Log-concave
Densities. Journal of the Royal Statistical Society: Series B, 79(3):651–676, 2017b."
REFERENCES,0.2323439099283521,"Arnak S Dalalyan and Avetik Karagulyan. User-friendly Guarantees for the Langevin Monte Carlo
with Inaccurate Gradient. Stochastic Processes and their Applications, 129(12):5278–5311, 2019."
REFERENCES,0.23336745138178097,"Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large Scale Distributed Deep Networks. In
Advances in neural information processing systems (NeurIPS), pp. 1223–1231, 2012."
REFERENCES,0.23439099283520984,"Wei Deng, Qi Feng, Liyao Gao, Faming Liang, and Guang Lin. Non-Convex Learning via Replica
Exchange Stochastic Gradient MCMC. In Proc. of the International Conference on Machine
Learning (ICML), 2020."
REFERENCES,0.23541453428863868,"Alain Durmus and Éric Moulines. Sampling from a Strongly Log-concave Distribution with the
Unadjusted Langevin Algorithm. arXiv:1605.01559, 2016."
REFERENCES,0.23643807574206754,"Mert Gürbüzbalaban, Xuefeng Gao, Yuanhan Hu, and Lingjiong Zhu. Decentralized Stochastic
Gradient Langevin Dynamics and Hamiltonian Monte Carlo. arXiv:2007.00590v3, 2021."
REFERENCES,0.2374616171954964,"Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. FL-NTK: A Neural Tangent Kernel-based
Framework for Federated Learning Convergence Analysis. In International Conference on Machine
Learning (ICML), 2021."
REFERENCES,0.23848515864892528,Under review as a conference paper at ICLR 2022
REFERENCES,0.23950870010235414,"Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. TextHide: Tackling Data
Privacy in Language Understanding Tasks. In EMNLP, 2020."
REFERENCES,0.240532241555783,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural Tangent Kernel: Convergence and Gener-
alization in Neural Networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571–8580, 2018."
REFERENCES,0.24155578300921188,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic Controlled Averaging for Federated Learning. In
International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020."
REFERENCES,0.24257932446264074,"Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. First Analysis of Local GD on
Heterogeneous Data. arXiv:1909.04715, 2019."
REFERENCES,0.2436028659160696,"Anastasia Koloskova, Sebastian U.Stich, and Martin Jaggi. Decentralized Stochastic Optimization
and Gossip Algorithms with Compressed Communication. In Proc. of the International Conference
on Machine Learning (ICML), 2019."
REFERENCES,0.24462640736949848,"Chunyuan Li, Changyou Chen, Yunchen Pu, Ricardo Henao, and Lawrence Carin. Communication-
Efﬁcient Stochastic Gradient MCMC for Neural Networks. In Proc. of the National Conference on
Artiﬁcial Intelligence (AAAI), 2019a."
REFERENCES,0.24564994882292732,"Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated Learning: Challenges,
Methods, and Future Directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020a."
REFERENCES,0.24667349027635618,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smithy.
Federated Optimization in Heterogeneous Networks. In Proceedings of the 3rd MLSys Conference,
2020b."
REFERENCES,0.24769703172978505,"Wenqi Li, Fausto Milletarì, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian
Baust, Yan Cheng, Sébastien Ourselin, M Jorge Cardoso, et al. Privacy-preserving Federated Brain
Tumour Segmentation. In International Workshop on Machine Learning in Medical Imaging, pp.
133–141. Springer, 2019b."
REFERENCES,0.24872057318321392,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Convergence of
FedAvg on Non-IID Data. In Proc. of the International Conference on Learning Representation
(ICLR), 2020c."
REFERENCES,0.24974411463664278,"Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence Staib, Pamela Ventola, and James S Duncan.
Multi-site fMRI Analysis using Privacy-preserving Federated Learning and Domain Adaptation:
ABIDE Results. Medical Image Analysis, 2020d."
REFERENCES,0.2507676560900716,"Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learning on
Non-IID Features via Local Batch Normalization. In International Conference on Learning Repre-
sentations (ICLR), 2021. URL https://openreview.net/forum?id=6YEQUn0QICG."
REFERENCES,0.2517911975435005,"Xuechen Li, Denny Wu, Lester Mackey, and Murat A. Erdogdu. Stochastic Runge-Kutta Accelerates
Langevin Monte Carlo and Beyond. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 7746–7758, 2019c."
REFERENCES,0.25281473899692936,"Y.-A Ma, E. B. Fox, T. Chen, and L. Wu. Irreversible samplers from jump and continuous Markov
processes. Stat. Comput., pp. 1–26, 2018."
REFERENCES,0.2538382804503582,"Y.-A. Ma, N. S. Chatterji, X. Cheng, N. Flammarion, P. L. Bartlett, and M. I. Jordan. Is there an
analog of Nesterov acceleration for MCMC? Bernoulli, 27:1942–1992, 2021."
REFERENCES,0.2548618219037871,"Yi-An Ma, Tianqi Chen, and Emily B. Fox. A Complete Recipe for Stochastic Gradient MCMC. In
Neural Information Processing Systems (NeurIPS), 2015."
REFERENCES,0.25588536335721596,"Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I. Jordan. Sampling Can Be
Faster Than Optimization. PNAS, 2019."
REFERENCES,0.2569089048106448,"Oren Mangoubi and Nisheeth K. Vishnoi. Convex Optimization with Unbounded Nonconvex Oracles
using Simulated Annealing. In Proc. of Conference on Learning Theory (COLT), 2018."
REFERENCES,0.2579324462640737,Under review as a conference paper at ICLR 2022
REFERENCES,0.25895598771750256,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelli-
gence and Statistics, pp. 1273–1282. PMLR, 2017."
REFERENCES,0.2599795291709314,"H. McMahan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. Federated Learning of Deep
Networks using Model Averaging. 2016."
REFERENCES,0.2610030706243603,"S. Minsker, S. Srivastava, L. Lin, and D. B. Dunson. Scalable and Robust Bayesian Inference via the
Median Posterior. In International Conference on Machine Learning (ICML), 2014."
REFERENCES,0.26202661207778916,"Wenlong Mou, Yi-An Ma, Martin J. Wainwright, Peter L. Bartlett, and Michael I. Jordan. High-Order
Langevin Diffusion Yields an Accelerated MCMC Algorithm. Journal of Machine Learning
Research (JMLR), 22:1–41, 2021."
REFERENCES,0.263050153531218,"W. Neiswanger, C. Wang, and E. Xing. Asymptotically Exact, Embarrassingly Parallel MCMC.
arXiv:1311.4780, 2013."
REFERENCES,0.2640736949846469,"Y. Nesterov. Introductory Lectures on Convex Optimization, in: Applied Optimization. Kluwer
Academic Publishers, Boston, MA, 2004."
REFERENCES,0.26509723643807576,"R. Nishihara, I. Murray, and R. P. Adams. Parallel MCMC with Generalized Elliptical Slice Sampling.
Journal of Machine Learning Research, 15(1):2087–2112, 2014."
REFERENCES,0.2661207778915046,"Reese Pathaky and Martin J. Wainwright. Fedsplit: An Algorithmic Framework for Fast Federated
Optimization. arXiv:2005.05238, 2020."
REFERENCES,0.2671443193449335,"Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex Learning via Stochastic
Gradient Langevin Dynamics: a Nonasymptotic Analysis. In Conference on Learning Theory,
June 2017."
REFERENCES,0.26816786079836236,"R. Shen and Y. T. Lee. The randomized midpoint method for log-concave sampling. In Advances in
Neural Information Processing Systems, pp. 2098–2109, 2019."
REFERENCES,0.2691914022517912,"Reza Shokri and Vitaly Shmatikov. Privacy-preserving Deep Learning. In SIGSAC conference on
computer and communications security (CCS). ACM, 2015."
REFERENCES,0.27021494370522003,"Sebastian U. Stich. Local SGD Converges Fast and Communicates Little. arXiv:1805.09767v3, 2019."
REFERENCES,0.2712384851586489,"Yee Whye Teh, Alexandre Thiery, and Sebastian Vollmer. Consistency and Fluctuations for Stochastic
Gradient Langevin Dynamics. Journal of Machine Learning Research, 17:1–33, 2016."
REFERENCES,0.27226202661207777,"Sebastian J. Vollmer, Konstantinos C. Zygalakis, and Yee Whye Teh. Exploration of the (Non-)
Asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics. Journal of Machine
Learning Research, 17(159):1–48, 2016."
REFERENCES,0.27328556806550663,"Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive Federated Learning in Resource Constrained Edge Computing Systems.
IEEE Journal on Selected Areas in Communications, 37(6):1205–1221, 2019."
REFERENCES,0.2743091095189355,Xiangyu Wang and David B. Dunson. Parallelizing MCMC via Weierstrass Sampler.
REFERENCES,0.27533265097236437,"Yu-Xiang Wang, Stephen Fienberg, and Alex Smola. Privacy for Free: Posterior Sampling and
Stochastic Gradient Monte Carlo. In ICML, pp. 2493–2502, 2015."
REFERENCES,0.27635619242579323,"Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In
International Conference on Machine Learning, 2011."
REFERENCES,0.2773797338792221,"Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs Local SGD for Heteroge-
neous Distributed Learning. arXiv:2006.04735, 2020."
REFERENCES,0.27840327533265097,"Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel Restarted SGD with Faster Convergence and Less
Communication: Demystifying Why Model Averaging Works for Deep Learning. In In Proc. of
Conference on Artiﬁcial Intelligence (AAAI), 2019."
REFERENCES,0.27942681678607983,Under review as a conference paper at ICLR 2022
REFERENCES,0.2804503582395087,"Yuchen Zhang, Percy Liang, and Moses Charikar. A Hitting Time Analysis of Stochastic Gradient
Langevin Dynamics. In Proc. of Conference on Learning Theory (COLT), pp. 1980–2022, 2017."
REFERENCES,0.28147389969293757,"Wenbo Zheng, Lan Yan, Chao Gou, and Fei-Yue Wang. Federated meta-learning for fraudulent credit
card detection. In Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial
Intelligence (IJCAI), 2020."
REFERENCES,0.28249744114636643,Under review as a conference paper at ICLR 2022
REFERENCES,0.2835209825997953,"Roadmap.
In Section A, we layout the formulation of the algorithm, basic notations, and deﬁnitions.
In Section B, we present the main convergence analysis for full device participation. We discuss
the optimal number of local updates based on a ﬁxed learning rate, the acceleration achieved by
varying learning rates, and the privacy-accuracy trade-off through correlated noises. In Section C,
we analyze the convergence of partial device participation through two device-sampling schemes.
In Section D, we provide lemmas to upper bound the contraction, discretization and divergence for
proving the main convergence results. In Section E, we include supporting lemmas to prove results in
the previous section. In Section F, we establish the initial condition."
REFERENCES,0.28454452405322417,"A
PRELIMINARIES"
REFERENCES,0.28556806550665303,"A.1
BASIC NOTATIONS AND BACKGROUNDS"
REFERENCES,0.2865916069600819,"Let N denote the number of clients. Let Tϵ denote the number of global steps to achieve the precision
ϵ. Let K denote the number of local steps. For each c ∈[N] := {1, 2, · · · , N}, we use f c and ∇f c
denote the loss function and gradient of the function f c in client c. For the stochastic gradient oracle,
we denote by ∇ef c(·) the unbiased estimate of the exact gradient ∇f c of client c. In addition, we
denote pc as the weight of the c-th client such that pc ≥0 and PN
c=1 pc = 1. ξc
k is an independent
standard d-dimensional Gaussian vector at iteration k for each client c ∈[N] and ˙ξk is a unique
Gaussian vector shared by all the clients."
REFERENCES,0.28761514841351077,"Algorithm 4 Federated averaging Langevin dynamics algorithm (FA-LD). Denote by θc
k the model
parameter in the c-th client at the k-th step. Denote the immediate result of one step SGLD update
from θc
k by βc
k. ξc
k is an independent standard d-dimensional Gaussian vector at iteration k for each
client c ∈[N]. A global synchronization is conducted every K steps. This is a complete version of
Algorithm 1. 1:"
REFERENCES,0.28863868986693964,"βc
k+1 = θc
k −η∇ef c(θc
k) +
p"
REFERENCES,0.2896622313203685,"2ητ/pcξc
k,
(13) 2:"
REFERENCES,0.2906857727737973,"θc
k+1 = 
 "
REFERENCES,0.2917093142272262,"βc
k+1
if k + 1 mod K ̸= 0"
REFERENCES,0.29273285568065505,"PN
c=1 pcβc
k+1
if k + 1 mod K = 0.
(14)"
REFERENCES,0.2937563971340839,"Inspired by Li et al. (2020c), we deﬁne two virtual sequences βk = N
X"
REFERENCES,0.2947799385875128,"c=1
pcβc
k,
θk = N
X"
REFERENCES,0.29580348004094165,"c=1
pcθc
k,
(15)"
REFERENCES,0.2968270214943705,"which are both inaccessible when k mod K ̸= 0. For the gradients and injected noise, we also deﬁne"
REFERENCES,0.2978505629477994,"∇f(θk) = N
X"
REFERENCES,0.29887410440122825,"c=1
pc∇f c(θc
k),
∇ef(θk) = N
X"
REFERENCES,0.2998976458546571,"c=1
pc∇ef c(θc
k),
ξk = N
X c=1"
REFERENCES,0.300921187308086,"√pcξc
k.
(16)"
REFERENCES,0.30194472876151485,"In what follows, it is clear that E∇ef(θ) = PN
c=1 pcE∇ef c(θc) = ∇f(θ) for any θc ∈Rd and any
c ∈[N]. Summing Eq.(13) from clients c = 1 to N and combining Eq.(15) and Eq.(16), we have"
REFERENCES,0.3029682702149437,"βk+1 = θk −η∇ef(θk) +
p"
REFERENCES,0.3039918116683726,"2ητξk.
(17)"
REFERENCES,0.30501535312180145,"Moreover, we always have βk = θk whether k + 1 mod E = 0 or not by Eq.(14) and Eq.(15). In
what follows, we can write
θk+1 = θk −η∇ef(θk) +
p"
REFERENCES,0.3060388945752303,"2ητξk,
(18)
which resembles the SGLD algorithm Welling & Teh (2011) except that the construction of stochastic
gradients is different and θk is not accessible when k mod K ̸= 0. To facilitate the analysis, we also
deﬁne an auxiliary continuous-time processes (¯θt)t≥0"
REFERENCES,0.3070624360286592,"d¯θt = −∇f(¯θt) · dt +
√"
REFERENCES,0.30808597748208805,"2τ · dW t,
(19)"
REFERENCES,0.3091095189355169,Under review as a conference paper at ICLR 2022
REFERENCES,0.3101330603889457,"where ¯θt = PN
c=1 pc¯θc
t, ∇f(¯θt) = PN
c=1 pc∇f c(¯θc
t), ¯θc
t is the continuous-time variable at client
c, and W is a d-dimensional Brownian motion. The continuous-time algorithm is referred to as
Federated Averaging Langevin diffusion and is described as"
REFERENCES,0.3111566018423746,"d¯βc
t = −∇f c(¯θc
t) · dt +
p"
REFERENCES,0.31218014329580346,"2τ/pc · dW
c
t"
REFERENCES,0.3132036847492323,"¯θc
t = N
X"
REFERENCES,0.3142272262026612,"c=1
pc ¯βc
t ."
REFERENCES,0.31525076765609006,"Since the synchronization step is conducted at every time step t, the Federated Averaging Langevin
diffusion performs the same as the standard Langevin diffusion with the temperature τ and conver-
gences to the stationary distribution π(θ) ∝exp(−f(θ)/τ), where f(θ) = PN
c=1 pcf c(θ). Assume
that ¯θ0 simulates from the stationary distribution π, then it follows that ¯θt ∼π for any t ≥0."
REFERENCES,0.3162743091095189,"A.2
ASSUMPTIONS AND DEFINITIONS"
REFERENCES,0.3172978505629478,"Assumption A.1 (Smoothness). For each c ∈[N], we say f c is L-smooth if for some L > 0"
REFERENCES,0.31832139201637666,"f c(y) ≤f c(x) + ⟨∇f c(x), y −x⟩+ L"
REFERENCES,0.3193449334698055,"2 ∥y −x∥2
2
∀x, y ∈Rd."
REFERENCES,0.3203684749232344,Note that the above assumption is equivalent to saying that
REFERENCES,0.32139201637666326,"∥∇f c(y) −∇f c(x)∥2 ≤L∥y −x∥2,
∀x, y ∈Rd."
REFERENCES,0.3224155578300921,"Assumption A.2 (Strong convexity). For each c ∈[N], f c is m-strongly convex if for some m > 0"
REFERENCES,0.323439099283521,"f c(x) ≥f c(y) + ⟨∇f c(y), x −y⟩+ m"
REFERENCES,0.32446264073694986,"2 ∥y −x∥2
2
∀x, y ∈Rd."
REFERENCES,0.3254861821903787,An alternative formulation for strong convexity is that
REFERENCES,0.3265097236438076,"⟨∇f c(x) −∇f c(y), x −y⟩≥m ∥x −y∥2
2
∀x, y ∈Rd."
REFERENCES,0.32753326509723646,"Assumption A.3 (Bounded variance, restatement of Assumption 5.3). For each c ∈[N], the variance
of noise in the stochastic gradient ∇ef c(x) in each client is upper bounded such that"
REFERENCES,0.3285568065506653,"E[∥∇ef c(x) −∇f c(x)∥2
2] ≤σ2d,
∀x ∈Rd."
REFERENCES,0.3295803480040942,"The bounded variance in the stochastic gradient is a rather standard assumption and has been widely
used in Cheng et al. (2018); Dalalyan & Karagulyan (2019); Li et al. (2020c). Extension of bounded
variance to unbounded cases such as E[∥∇ef c(x) −∇f c(x)∥2
2] ≤δ(L2x2 + B2) for some M and
δ ∈[0, 1) is quite straightforward and has been adopted in assumption A.4 stated in Raginsky et al.
(2017). The proof framework remains the same."
REFERENCES,0.330603889457523,"Quality of non-i.i.d data
Denote by θ∗the global minimum of f. Next, we quantify the degree
of the non-i.i.d data by γ := maxc∈[N] ∥∇f c(θ∗)∥2, which is a non-negative constant and yields a
smaller scale if the data is more evenly distributed."
REFERENCES,0.33162743091095187,"Deﬁnition A.4. We deﬁne parameter Tc,ρ H2
ρ, κ and γ2"
REFERENCES,0.33265097236438074,"Tc,ρ := τ(ρ2 + (1 −ρ2)/pc),"
REFERENCES,0.3336745138178096,"Hρ :=
D2
|{z}
initialization
+ 1"
REFERENCES,0.33469805527123847,"m max
c∈[N] Tc,ρ
|
{z
}
injected noise +
γ2"
REFERENCES,0.33572159672466734,"m2d
|{z}
data heterogeneity +
σ2"
REFERENCES,0.3367451381780962,"m2
|{z}
stochastic noise ,"
REFERENCES,0.33776867963152507,"κ := L/m,"
REFERENCES,0.33879222108495394,"γ2 := max
c∈[N] ∥∇f c(θ∗)∥2
2."
REFERENCES,0.3398157625383828,Under review as a conference paper at ICLR 2022
REFERENCES,0.34083930399181167,"B
FULL DEVICE PARTICIPATION"
REFERENCES,0.34186284544524054,"B.1
ONE-STEP UPDATE"
REFERENCES,0.3428863868986694,"Wasserstein distance
We deﬁne the 2-Wasserstein distance between a pair of Borel probability
measures µ and ν on Rd as follows"
REFERENCES,0.34390992835209827,"W2(µ, ν) :=
inf
γ2∈Couplings(µ,ν)"
REFERENCES,0.34493346980552714,"Z
∥βµ −βν∥2
2dγ2(βµ, βν)
 1 2
,"
REFERENCES,0.345957011258956,"where ∥· ∥2 denotes the ℓ2 norm on Rd and the pair of random variables (βµ, βν) ∈Rd × Rd is a
coupling with the marginals following L(βµ) = µ and L(βν) = ν. L(·) denotes a distribution of a
random variable."
REFERENCES,0.34698055271238487,"The following result provides a crucial contraction property based on distributed clients with infre-
quent synchronizations.
Lemma B.1 (Dominated contraction property, restatement of Lemma 5.4). Assume assumptions A.1
and A.2 hold. For any learning rate η ∈(0,
1
L+m], any {θc}N
c=1, {βc}N
c=1 ∈Rd, we have"
REFERENCES,0.34800409416581374,"∥β −θ −η(∇f(β) −∇f(θ))∥2
2 ≤(1 −ηm) · ∥β −θ∥2
2 + 4ηL N
X"
REFERENCES,0.3490276356192426,"c=1
pc · (∥βc −β∥2
2 + ∥θc −θ∥2
2)."
REFERENCES,0.3500511770726714,"where β
=
PN
c=1 pcβc, θ
=
PN
c=1 pcθc, ∇f(θ)
=
PN
c=1 pc∇f c(θc), and ∇f(β)
=
PN
c=1 pc∇f c(βc). We postpone the proof into Section D.1. The above result implies that as
long as the local parameters θc, βc and global θ, β don’t differ each other too much, we can guarantee
the desired convergence."
REFERENCES,0.3510747185261003,"The following result ensures a bounded gap between ¯θc
s and ¯θc
η⌊s"
REFERENCES,0.35209825997952915,η ⌋in ℓ2 norm for any s ≥0 and
REFERENCES,0.353121801432958,"c ∈[N]. We postpone the proof of Lemma B.2 into Section D.2.
Lemma B.2 (Discretization error). Assume assumptions A.1, A.2, and A.3 hold. For any s ≥0, any
learning rate η ∈(0, 2/m) and ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], the iterates of (¯θs) based on the
continuous dynamics of Eq.(19) satisfy the following estimate"
REFERENCES,0.3541453428863869,"E
¯θc
s −¯θc
η⌊s"
REFERENCES,0.35516888433981575,"η ⌋
2
2 ≤8η2dκ
κγ2"
REFERENCES,0.3561924257932446,"d
+ Lτ

+ 16ηdτ."
REFERENCES,0.3572159672466735,"The following result shows that given a ﬁnite number of local steps K, the divergence between θc in
local client and θ in the center is bounded in ℓ2 norm. Notably, since the non-differentiable Brownian
motion leads to a lower order term O(η) instead of O(η2) in ℓ2 norm, a naïve proof may lead to a
crude upper bound. We delay the proof of Lemma B.3 into Section D.3.
Lemma B.3 (Bounded divergence, restatement of Lemma 5.5). Assume assumptions A.1, A.2, and
A.3 hold. For any learning rate η ∈(0, 2/m) and ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], we have the
ℓ2 upper bound of the divergence between local clients and the center as follows N
X"
REFERENCES,0.35823950870010235,"c=1
pcE∥θc
k −θk∥2
2 ≤112(K −1)2η2dL2Hρ + 8(K −1)ηdτ(ρ2 + N(1 −ρ2)),"
REFERENCES,0.3592630501535312,"where Hρ, κ and γ2 are deﬁned as Deﬁnition A.4."
REFERENCES,0.3602865916069601,"The following presents a standard result for bounding the gap between ∇f(θ) and ∇ef(θ). We delay
the proof of Lemma B.4 into Setion D.
Lemma B.4 (Bounded variance). Given assumption A.3, we have"
REFERENCES,0.36131013306038895,"E∥∇f(θ) −∇ef(θ)∥2
2 ≤d · σ2,
∀θ ∈Rd."
REFERENCES,0.3623336745138178,"Having all the preliminary results ready, now we present a crucial lemma for proving the convergence
of all the algorithms."
REFERENCES,0.3633572159672467,Under review as a conference paper at ICLR 2022
REFERENCES,0.36438075742067555,"Lemma B.5 (One step update, restatement of Lemma 5.6). Assume assumptions A.1, A.2, and A.3
hold. Consider Algorithm 4 with independently injected noise ρ = 0, any learning rate η ∈(0,
1
2L)
and ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], where θ∗is the global minimum for the function f. Then"
REFERENCES,0.3654042988741044,"W 2
2 (µk+1, π) ≤

1 −ηm 2"
REFERENCES,0.3664278403275333,"
· W 2
2 (µk, π) + 400η2dL2H0((K −1)2 + κ),"
REFERENCES,0.36745138178096215,"where µk denotes the probability measure of θk, H0, κ and γ2 are deﬁned as Deﬁnition A.4."
REFERENCES,0.368474923234391,Proof. The solution of the continuous-time process Eq.(19) follows that
REFERENCES,0.3694984646878199,"¯θt = ¯θ0 −
Z t"
REFERENCES,0.3705220061412487,"0
∇f(¯θs)ds +
√"
REFERENCES,0.37154554759467756,"2τ · W t,
∀t ≥0.
(20)"
REFERENCES,0.3725690890481064,"Set t →(k + 1)η and ¯θ0 →¯θkη for Eq.(20) and consider a synchronous coupling such that
W(k+1)η −Wkη := √ηξk"
REFERENCES,0.3735926305015353,"¯θ(k+1)η = ¯θkη −
Z (k+1)η"
REFERENCES,0.37461617195496416,"kη
∇f(¯θs)ds +
√"
REFERENCES,0.37563971340839303,2τ(W(k+1)η −Wkη)
REFERENCES,0.3766632548618219,"= ¯θkη −
Z (k+1)η"
REFERENCES,0.37768679631525076,"kη
∇f(¯θs)ds +
p"
REFERENCES,0.37871033776867963,"2τηξk.
(21)"
REFERENCES,0.3797338792221085,"We ﬁrst denote ζk := ∇ef(θk) −∇f(θk). Subtracting Eq.(18) from Eq.(21) yields that
¯θ(k+1)η −θk+1"
REFERENCES,0.38075742067553736,"= ¯θkη −θk + η∇ef(θk) −
Z (k+1)η"
REFERENCES,0.38178096212896623,"kη
∇f(¯θs)ds"
REFERENCES,0.3828045035823951,"= ¯θkη −θk −η

∇f(θk + ¯θkη −θk) −∇ef(θk)

−
Z (k+1)η kη"
REFERENCES,0.38382804503582396,"
∇f(¯θs) −∇f(¯θkη)

ds
(22)"
REFERENCES,0.38485158648925283,"= ¯θkη −θk −η

∇f(θk + ¯θkη −θk) −∇f(θk)
|
{z
}
:=Xk"
REFERENCES,0.3858751279426817,"
−
Z (k+1)η kη"
REFERENCES,0.38689866939611056,"
∇f(¯θs) −∇f(¯θkη)

ds
|
{z
}
:=Yk +ηζk."
REFERENCES,0.38792221084953943,"Taking square and expectation on both sides, we have"
REFERENCES,0.3889457523029683,"E∥¯θ(k+1)η −θk+1∥2
2
= E∥¯θkη −θk −ηXk −Yk∥2
2 + E∥ηζk∥2
2 + 2η E⟨¯θkη −θk −ηXk −Yk, ζk⟩
|
{z
}
Eζk=0 and mutual independence"
REFERENCES,0.38996929375639716,"≤(1 + q) · E∥¯θkη −θk −ηXk∥2
2 + (1 + 1/q) · E∥Yk∥2
2 + E∥ηζk∥2
2"
REFERENCES,0.390992835209826,"≤(1 + q) ·
 
(1 −ηm) · E∥¯θkη −θk∥2
2 + 4ηL N
X"
REFERENCES,0.39201637666325484,"c=1
pc ·
 
E∥¯θc
kη −¯θkη∥2
2 + E∥θc
k −θk∥2
2
 "
REFERENCES,0.3930399181166837,"+ (1 + 1/q) · E∥Yk∥2
2 + η2σ2d"
REFERENCES,0.3940634595701126,"≤(1 + q) ·

(1 −ηm)
|
{z
}
φ"
REFERENCES,0.39508700102354144,"E∥¯θkη −θk∥2
2 + 448η3d(K −1)2L3H0 + 32(K −1)η2dLτN
"
REFERENCES,0.3961105424769703,"+ (1 + 1/q) · E∥Yk∥2
2 + η2σ2d,
(23)"
REFERENCES,0.3971340839303992,"where the ﬁrst inequality follows by the AM-GM inequality for any q > 0, the second inequality
follows by Lemma B.1 and Assumption A.3. The third inequality follows by Lemma B.3 with ρ = 0;
moreover, the continuous-time process conducts synchronization at any time step, hence ¯θc
kη = ¯θkη.
Since the learning rate follows
1
2L ≤
1
m+L ≤2"
REFERENCES,0.39815762538382804,"m, the requirement of the learning rate for Lemma B.1
and Lemma B.3 is clearly satisﬁed."
REFERENCES,0.3991811668372569,Under review as a conference paper at ICLR 2022
REFERENCES,0.4002047082906858,"Recall that φ = 1 −ηm, we get 1+φ"
REFERENCES,0.40122824974411464,"2
= 1 −1"
REFERENCES,0.4022517911975435,2ηm. Choose q = 1+φ
REFERENCES,0.4032753326509724,"2φ −1 so that (1 + q)φ = (1+φ) 2
= 1 −1"
REFERENCES,0.40429887410440124,"2ηm. In addition, we have 1 + 1"
REFERENCES,0.4053224155578301,q = 1+q
REFERENCES,0.406345957011259,"q
= 1+φ"
REFERENCES,0.40736949846468784,"1−φ ≤
2
ηm. It follows that"
REFERENCES,0.4083930399181167,(1 + q) · (1 −ηm) ≤1 −1
REFERENCES,0.4094165813715456,"2ηm,
1 + q ≤1 −1"
REFERENCES,0.4104401228249744,"2ηm
1 −ηm ≤1.5,
(1 + 1/q) ≤
2
mη ,
(24)"
REFERENCES,0.41146366427840325,"where the second inequality holds because η ∈(0,
1
2L] ≤
1
2m."
REFERENCES,0.4124872057318321,"For the term E∥Yk∥2
2 in Eq.(23), we have the following estimate"
REFERENCES,0.413510747185261,"E∥Yk∥2
2 = E "
REFERENCES,0.41453428863868985,Z (k+1)η kη
REFERENCES,0.4155578300921187,"
∇f(¯θs) −∇f(¯θkη)

ds  2 2"
REFERENCES,0.4165813715455476,"≤η
Z (k+1)η"
REFERENCES,0.41760491299897645,"kη
E
∇f(¯θs) −∇f(¯θkη)
2
2ds"
REFERENCES,0.4186284544524053,"= η
Z (k+1)η kη
E  N
X"
REFERENCES,0.4196519959058342,"c=1
pc"
REFERENCES,0.42067553735926305,"
∇f c(¯θc
s) −∇f c(¯θc
kη)
 2"
DS,0.4216990788126919,"2
ds"
DS,0.4227226202661208,"≤η
Z (k+1)η kη N
X"
DS,0.42374616171954965,"c=1
pc · E
∇f c(¯θc
s) −∇f c(¯θc
kη)
2
2ds"
DS,0.4247697031729785,"≤ηL2
Z (k+1)η kη N
X"
DS,0.4257932446264074,"c=1
pc · E
¯θc
s −¯θc
kη
2
2ds"
DS,0.42681678607983625,"≤ηL2
Z (k+1)η kη"
DS,0.4278403275332651,"
8η2dκ
κγ2"
DS,0.428863868986694,"d
+ Lτ

+ 16ηdτ

ds"
DS,0.42988741044012285,"= 8η4dL4H0 + 16η3L2dτ,
(25)"
DS,0.43091095189355166,"where the ﬁrst inequality follows by Hölder’s inequality, the second inequality follows by Jensen’s
inequality, the third inequality follows by Assumption A.1, and the last inequality follows by Lemma
B.2. The last equality holds since κ"
DS,0.43193449334698053,dγ2 + Lτ ≤LmH0 and κ = L/m.
DS,0.4329580348004094,"Plugging Eq.(24) and Eq.(25) into Eq.(23), we have"
DS,0.43398157625383826,"E∥¯θ(k+1)η −θk+1∥2
2 ≤(1 −ηm"
DS,0.43500511770726713,"2 ) · E∥¯θkη −θk∥2
2"
DS,0.436028659160696,+ 672η3d(K −1)2L3H0 + 48η2d(K −1)LτN
DS,0.43705220061412486,+ 16η3dL3κH0 + 32η2dL2
DS,0.43807574206755373,m τ + η2σ2d.
DS,0.4390992835209826,"Choose the speciﬁc Langevin diffusion ¯θ in stationary regime, we have W 2
2 (µk, π) = E∥¯θkη −θk∥2
2
and W 2
2 (µk+1, π) ≤E∥¯θ(k+1)η −θk+1∥2
2. Arranging the terms, we have"
DS,0.44012282497441146,"W 2
2 (µk+1, π) ≤(1 −ηm"
DS,0.44114636642784033,"2 ) · W 2
2 (µk, π) + 400η2dL2H0((K −1)2 + κ),"
DS,0.4421699078812692,"where η ≤
1
2L, κ ≥1, mτ ≤Lτ ≤LτN ≤L maxc∈[N] Tc,0 ≤LmH0, and σ2 ≤L2H0 are
applied to the result."
DS,0.44319344933469806,"B.2
CONVERGENCE VIA INDEPENDENT NOISES"
DS,0.44421699078812693,"Theorem B.6 (Restatement of Theorem 5.7). Assume assumptions A.1, A.2, and A.3 hold. Consider
Algorithm 4 with a ﬁxed learning rate η ∈(0,
1
2L] and ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], we have"
DS,0.4452405322415558,"W2(µk, π) ≤

1 −ηm 4"
DS,0.44626407369498466,"k
·
√"
D,0.44728761514841353,"2d
 
D +
p"
D,0.4483111566018424,"τ/m

+ 30κ
p"
D,0.44933469805527126,"ηmd ·
p"
D,0.4503582395087001,((K −1)2 + κ)H0.
D,0.45138178096212894,"where µk denotes the probability measure of θk, H0, κ and γ2 are deﬁned as Deﬁnition A.4."
D,0.4524053224155578,Under review as a conference paper at ICLR 2022
D,0.4534288638689867,"Proof. Iteratively applying Theorem B.5 and arranging terms, we have that"
D,0.45445240532241554,"W 2
2 (µk, π) ≤

1 −ηm 2"
D,0.4554759467758444,"k
W 2
2 (µ0, π) + 1 −(1 −ηm 2 )k"
D,0.4564994882292733,1 −(1 −ηm 2 )
D,0.45752302968270214,"
400η2dL2H0((K −1)2 + κ)
"
D,0.458546571136131,"≤

1 −ηm 2"
D,0.4595701125895599,"k
W 2
2 (µ0, π) + 2 ηm"
D,0.46059365404298874,"
400η2dL2H0((K −1)2 + κ)
"
D,0.4616171954964176,"≤

1 −ηm 2"
D,0.4626407369498465,"k
W 2
2 (µ0, π) + 800κ2ηmd((K −1)2 + κ)H0,
(26)"
D,0.46366427840327534,where κ = L
D,0.4646878198567042,"m. By Lemma F.1 and the initialization condition ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N],
we have that"
D,0.4657113613101331,"W2(µ0, π) ≤
√"
D,0.46673490276356194,"2d(D +
p τ/m)."
D,0.4677584442169908,Applying the inequality (1 −ηm
D,0.4687819856704197,2 ) ≤(1 −ηm
D,0.46980552712384854,4 )2 completes the proof.
D,0.47082906857727735,Discussions
D,0.4718526100307062,"Optimal choice of K. To ensure the algorithm to achieve the ϵ precision based on the total number
of steps Tϵ and the learning rate η, we can set 30κ
p"
D,0.4728761514841351,"ηmd ·
p"
D,0.47389969293756395,"((K −1)2 + κ)H0 
≤ϵ 2 e−ηm"
D,0.4749232343909928,"4 Tϵ ·
√"
D,0.4759467758444217,"2d
 
D +
p"
D,0.47697031729785055,"τ/m

≤ϵ 2."
D,0.4779938587512794,This directly leads to
D,0.4790174002047083,"ηm ≤min
 m"
D,0.48004094165813715,"2L, O

ϵ2"
D,0.481064483111566,dκ2((K −1)2 + κ)H0
D,0.4820880245649949,"
,
Tϵ ≥Ω
log
  d ϵ
 mη 
."
D,0.48311156601842375,"Plugging into the upper bound of η, it implies that to reach the precision level ϵ, it sufﬁces to set"
D,0.4841351074718526,"Tϵ = Ω
dκ2((K −1)2 + κ)H0"
D,0.4851586489252815,"ϵ2
· log
d ϵ"
D,0.48618219037871035,"
.
(27)"
D,0.4872057318321392,Since H0 = Ω(D2 + τ
D,0.4882292732855681,"m), we observe that the number of communication rounds is around the order"
D,0.48925281473899696,"Tϵ
K = Ω

K + κ K 
,"
D,0.49027635619242577,where the value of Tϵ
D,0.49129989764585463,"K ﬁrst decreases and then increases with respect to K, indicating that setting K
either too large or too small may lead to high communication costs and hurt the performance. Ideally,
K should be selected in the scale of Ω(√κ). Combining the deﬁnition of Tϵ in Eq.(27), this suggests
an interesting result that the optimal K should be in the order of O(√Tϵ). Similar results have been
achieved by Stich (2019); Li et al. (2020c)."
D,0.4923234390992835,"B.3
CONVERGENCE VIA VARYING LEARNING RATES"
D,0.49334698055271237,"Theorem B.7 (Restatement of Theorem 5.8). Assume assumptions A.1, A.2, and A.3 hold. Consider
Algorithm 4 with an initialization satisfying ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N] and varying learning
rate following"
D,0.49437052200614123,"ηk =
1
2L + (1/12)mk ,
k = 1, 2, · · · ."
D,0.4953940634595701,"Then for any k ≥0, we have"
D,0.49641760491299897,"W2(µk, π) ≤45κ
p"
D,0.49744114636642783,"((K −1)2 + κ)H0 ·
 
ηkmd
1/2,
∀k ≥0,"
D,0.4984646878198567,Under review as a conference paper at ICLR 2022
D,0.49948822927328557,Proof. We ﬁrst denote
D,0.5005117707267144,"Cκ = 30κ
p"
D,0.5015353121801432,"((K −1)2 + κ)H0.
Next, we proceed to show the following inequality by the induction method"
D,0.5025588536335721,"W2(µk, π) ≤1.5Cκ"
D,0.503582395087001,"
d
2L + (1/12)mk"
D,0.5046059365404298,"1/2
= 1.5Cκ
 
ηkmd
1/2,
∀k ≥0,
(28)"
D,0.5056294779938587,where the decreasing learning rate follows that
D,0.5066530194472876,"ηk =
1
2L + (1/12)mk ."
D,0.5076765609007164,"(i) For the case of k = 0, since"
D,0.5087001023541453,"Cκ ≥4√κ
p"
D,0.5097236438075742,H0 ≥4√κ s
D,0.510747185261003,D2 + 1
D,0.5117707267144319,"m max
c∈[N] Tc,0 ≥4
p"
D,0.5127942681678608,"κ/d
√ dD2 + s"
D,0.5138178096212896,"d
m max
c∈[N] Tc,0  ≥4
p"
D,0.5148413510747185,"κ/dW2(µ0, π),
(29)"
D,0.5158648925281474,"where the last inequality follows by Lemma F.1 and ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N]."
D,0.5168884339815762,"It is clear that W2(µ0, π) ≤1 4Cκ
q md"
D,0.5179119754350051,"L ≤1.5Cκ
√η0md by Eq.(29)."
D,0.518935516888434,"(ii) If now that Eq.(28) holds for some k ≥0, it follows by Lemma B.5 that"
D,0.5199590583418628,"W 2
2 (µk+1, π) ≤
 
1 −ηkm"
D,0.5209825997952917,"2

· W 2
2 (µk, π) + 400η2
kdL2H0((K −1)2 + κ)"
D,0.5220061412487206,"≤
 
1 −ηkm"
D,0.5230296827021494,"2

· W 2
2 (µk, π) + η2
km2"
D,0.5240532241555783,"2
C2
κd"
D,0.5250767656090072,"≤
 
1 −ηkm"
D,0.526100307062436,"2

· 2.25C2
κηkmd + ηkm"
D,0.5271238485158649,"3
2.25C2
κηkmd"
D,0.5281473899692938,"≤
 
1 −ηkm"
D,0.5291709314227226,"6

· 2.25C2
κηkmd."
D,0.5301944728761515,"Since
 
1 −ηkm"
D,0.5312180143295804,"6

≤
 
1 −ηkm"
D,0.5322415557830092,"12
2, we have"
D,0.5332650972364381,"W2(µk+1, π) ≤
 
1 −ηkm"
D,0.534288638689867,"12

· 1.5Cκ
 
ηkmd
1/2."
D,0.5353121801432958,"To prove W2(µk+1, π) ≤1.5Cκ
 
ηk+1md
1/2, it sufﬁces to show
 
1 −ηkm"
D,0.5363357215967247,"12

η1/2
k
≤ηk+1, which is
detailed as follows
 
1 −ηkm"
D,0.5373592630501536,"12

η1/2
k
= √"
D,0.5383828045035824,12(24L + mk −m)
D,0.5394063459570113,(24L + mk)3/2 ≤ √
D,0.5404298874104401,12(24L + mk −m)1/2
D,0.5414534288638689,24L + mk ≤ √
D,0.5424769703172978,"12
(24L + m(k + 1))1/2 := ηk+1,"
D,0.5435005117707267,"where the last inequality follows since
(24L + mk −m)(24L + mk + m)) ≤(24L + mk)2."
D,0.5445240532241555,"The above result implies that to achieve the precision ϵ, we require"
D,0.5455475946775844,"W2(µk, π) ≤1.5Cκ"
D,0.5465711361310133,"
md
2L + (1/12)mk"
D,0.5475946775844421,"1/2
≤ϵ."
D,0.548618219037871,The means that we only require k = Ω( d
D,0.5496417604912999,"ϵ2 ) to achieve the precision ϵ. By contrast, the ﬁxed learning"
D,0.5506653019447287,"rate requires Tϵ = Ω

d
ϵ2 ·log
 
d/ϵ

, which is much slower than the algorithm with varying learning"
D,0.5516888433981576,"rate by O
 
log
 
d/ϵ

times."
D,0.5527123848515865,Under review as a conference paper at ICLR 2022
D,0.5537359263050153,"B.4
PRIVACY-ACCURACY TRADE-OFF VIA CORRELATED NOISES"
D,0.5547594677584442,"Note that Algorithm 4 requires all the local clients to generate the independent noise ξc
k. Such a
mechanism enjoys the convenience of the implementation and yields a potential to protect the privacy
of data and alleviates the security issue. However, the scale of noises is maximized and inevitable
slows down the convergence. For extensions, it can be naturally generalized to correlated noise
based on a hyperparameter, namely the correlation coefﬁcient ρ between different clients. Replacing
Eq.(13) with
βc
k+1 = θc
k −η∇ef c(θc
k) +
p"
D,0.5557830092118731,"2ητρ2 ˙ξk +
p"
D,0.5568065506653019,"2η(1 −ρ2)τ/pcξc
k,
(30)"
D,0.5578300921187308,"where ˙ξk is a d-dimensional standard Gaussian vector shared by all the clients at iteration k, ξc
k is a
unique d-dimensional Gaussian vector generated by client c ∈[N] only. Moreover, ˙ξk is dependent
with ξc
k for any c ∈[N]. Following the same synchronization step based Eq.(14), we have"
D,0.5588536335721597,"θk+1 = θk −η∇ef(θk) +
p"
D,0.5598771750255885,"2ητξk,
(31)"
D,0.5609007164790174,"where ξk = ρξk +
p"
D,0.5619242579324463,"1 −ρ2 PN
c=1
√pcξc
k. Since the variance of i.i.d variables is additive, it is clear
that ξk follows the standard d-dimensional Gaussian distribution. The inclusion of the correlated
noise implicitly reduces the temperature and naturally yields a trade-off between federation and
accuracy. We refer to the algorithm with correlated noise as the generalized Federated Averaging
Langevin dynamics (gFA-LD) and present it in Algorithm 5."
D,0.5629477993858751,"Since the inclusion of correlated noise doesn’t affect the formulation of Eq.(31), the algorithm
property maintains the same except the scale of the temperature τ and federation are changed.
Based on a target correlation coefﬁcient ρ ≥0, Eq.(30) is equivalent to applying a temperature
Tc,ρ = τ(ρ2 + (1 −ρ2)/pc). In particular, setting ρ = 0 leads to Tc,0 = (1 −ρ2)/pc, which exactly
recovers Algorithm 4; however, setting ρ = 1 leads to Tc,1 = τ, where the injected noise in local
clients is reduced by 1/pc times. Now we adjust the analysis as follows"
D,0.563971340839304,"Theorem B.8 (Restatement of Theorem 5.9). Assume assumptions A.1, A.2, and A.3 hold. Con-
sider Algorithm 5 with a correlation coefﬁcient ρ ∈[0, 1], a ﬁxed learning rate η ∈(0,
1
2L] and
∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], we have"
D,0.5649948822927329,"W2(µk, π) ≤

1 −ηm 4"
D,0.5660184237461617,"k
·
√"
D,0.5670419651995906,"2d
 
D +
p"
D,0.5680655066530195,"τ/m

+ 30κ
p"
D,0.5690890481064483,"ηmd ·
q"
D,0.5701125895598772,"((K −1)2 + κ)Hρ,"
D,0.5711361310133061,"where µk denotes the probability measure of θk, Hρ, κ and γ2 are deﬁned as Deﬁnition A.4."
D,0.5721596724667349,"Algorithm 5 Hybrid federated averaging Langevin dynamics algorithm (hFA-LD). Denote by θc
k the
model parameter in the c-th client at the k-th step. Denote the immediate result of one step SGLD
update from θc
k by βc
k. ξc
k is an independent standard d-dimensional Gaussian vector at iteration k for
each client c ∈[N] and ˙ξk is a d-dimensional standard Gaussian vector shared by all the clients. ρ
denotes the correlation coefﬁcient of the injected noises. A global synchronization is conducted every
K steps. This is a complete version of Algorithm 2. 1:"
D,0.5731832139201638,"βc
k+1 = θc
k −η∇ef c(θc
k) +
p"
D,0.5742067553735927,"2ητρ2 ˙ξk +
p"
D,0.5752302968270215,"2η(1 −ρ2)τ/pcξc
k, 2:"
D,0.5762538382804504,"θc
k+1 = 
 "
D,0.5772773797338793,"βc
k+1
if k + 1 mod K ̸= 0"
D,0.5783009211873081,"PN
c=1 pcβc
k+1
if k + 1 mod K = 0."
D,0.579324462640737,"C
PARTIAL DEVICE PARTICIPATION"
D,0.5803480040941658,"Full device participation enjoys appealing convergence properties. However, it suffers from the
straggler’s effect in real-world applications, where the communication is limited by the slowest
device. Partial device participation handles this issue by only allowing a small portion of devices in
each communication and greatly increased the communication efﬁciency in a federated network."
D,0.5813715455475946,Under review as a conference paper at ICLR 2022
D,0.5823950870010235,"C.1
UNBIASED SAMPLING SCHEMES"
D,0.5834186284544524,"The ﬁrst device-sampling scheme I Li et al. (2020b) selects a total of S devices, where the c-th device
is selected with a probability pc. The ﬁrst theoretical justiﬁcation for convex optimization has been
proposed by Li et al. (2020c)."
D,0.5844421699078812,"(Scheme I: with replacement).
Assume Sk = {n1, n2, · · · , nS}, where nj ∈[N] is a random
number that takes a value of c with a probability pc for any j ∈{1, 2, · · · , S}. The synchronization
step follows that θk = 1"
D,0.5854657113613101,"S
P
c∈Sk θc
k."
D,0.586489252814739,"Another strategy is to uniformly select S devices without replacement. We follow Li et al. (2020c)
and assume S indices are selected uniformly without replacement and the synchronization step is the
same as before. In addition, the convergence also requires an additional assumption on balanced data
Li et al. (2020c)."
D,0.5875127942681678,"(Scheme II: without replacement).
Assume Sk = {n1, n2, · · · , nS}, where nj ∈[N] is a
random number that takes a value of c with a probability 1"
D,0.5885363357215967,"S for any j ∈{1, 2, · · · , S}. Assume
the data is balanced such that p1 = · · · = pN =
1
N . The synchronization step follows that
θk = N S
P"
D,0.5895598771750256,"c∈Sk pcθc
k = 1 S
P"
D,0.5905834186284544,"c∈Sk θc
k."
D,0.5916069600818833,"Algorithm 6 Hybrid federated averaging Langevin dynamics algorithm (hFA-LD) with partial device
participation. ξc
k is the independent Gaussian vector proposed by each client c ∈[N] and ˙ξk is a
unique Gaussian vector shared by all the clients. ρ denotes the correlation coefﬁcient. A global
synchronization is conducted every K steps. Sk is a subset that contains S indices according to a
device-sampling rule based on scheme I or II. This is a complete version of Algorithm 3. 1:"
D,0.5926305015353122,"βc
k+1 = θc
k −η∇ef c(θc
k) +
p"
D,0.593654042988741,"2ητρ2 ˙ξk +
p"
D,0.5946775844421699,"2η(1 −ρ2)τ/pcξc
k, 2:"
D,0.5957011258955988,"θc
k+1 = 
 "
D,0.5967246673490276,"βc
k+1
if k + 1 mod K ̸= 0"
D,0.5977482088024565,"P
c∈Sk+1
1
S βc
k+1
if k + 1 mod K = 0."
D,0.5987717502558854,"Lemma C.1 (Unbiased sampling scheme). For any k mod K = 0 based on scheme I or II, we have"
D,0.5997952917093142,"Eθk = E
X"
D,0.6008188331627431,"c∈Sk
θc
k = βk := N
X"
D,0.601842374616172,"c=1
pcβc
k."
D,0.6028659160696008,"Proof. According to the deﬁnition of scheme I or II, we have θk = 1 S
P"
D,0.6038894575230297,"c∈Sk θc
k. In what follows,
Eθk =
1
S EP
c∈Sk θc
k =
1
S
P
c0∈Sk
PN
c=1 pcβc
k = PN
c=1 pcβc
k, where p1 = p2 = · · · = pN for
scheme II in particular."
D,0.6049129989764586,"C.2
BOUNDED DIVERGENCE BASED ON PARTIAL DEVICE"
D,0.6059365404298874,"Lemma C.2 (Bounded divergence based on partial device). Assume assumptions A.1, A.2, and A.3
hold. Consider Algorithm 6 with a correlation coefﬁcient ρ ∈[0, 1], any learning rate η ∈(0, 2/m)
and ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], we have the following results"
D,0.6069600818833163,"For Scheme I, the divergence between θk and βk is upper bounded by"
D,0.6079836233367452,"E∥βk −θk∥2
2 ≤112"
D,0.609007164790174,S K2η2dL2Hρ + 8
D,0.6100307062436029,S Kηdτ(ρ2 + N(1 −ρ2)).
D,0.6110542476970318,"For Scheme II, assuming the data is balanced such that p1 = · · · = pN = 1"
D,0.6120777891504606,"N , the divergence between
θk and βk is upper bounded by"
D,0.6131013306038895,"E∥βk −θk∥2
2 ≤
N −S
S(N −1)"
D,0.6141248720573184,"
112K2η2dL2Hρ + 8Kηdτ(ρ2 + N(1 −ρ2))

."
D,0.6151484135107472,"where Hρ, κ and γ2 are deﬁned as Deﬁnition A.4."
D,0.6161719549641761,Under review as a conference paper at ICLR 2022
D,0.617195496417605,"Proof. We prove the bounded divergence for the two schemes, respectively."
D,0.6182190378710338,"For scheme I with replacement, ¯θk = P"
D,0.6192425793244627,"c∈Sk
1
S βc
k for a subset of indices Sk. Taking expectation
with respect to Sk, we have"
D,0.6202661207778914,"E∥θk −βk∥2
2 = 1 S2 S
X"
D,0.6212896622313203,"i=1
E∥βni
k −βk∥2
2 = 1 S N
X"
D,0.6223132036847492,"c=1
pc ∥βc
k −βk∥2
2 ,
(32)"
D,0.623336745138178,"where the ﬁrst equality follows by the independence and unbiasedness of θni
k for any i ∈[S]."
D,0.6243602865916069,"To further upper bound Eq.(32), we follow the same technique as in Lemma B.3. Since k mod K = 0,
k0 = k −K is also the communication time, which yields the same θc
k0 for any c ∈[N]. in what
follows, N
X"
D,0.6253838280450358,"c=1
pc ∥βc
k −βk∥2
2 = N
X"
D,0.6264073694984647,"c=1
pc ∥βc
k −θk0 −(βk −θk0)∥2
2 ≤ N
X"
D,0.6274309109518935,"c=1
pc ∥βc
k −θk0∥2
2 ,
(33)"
D,0.6284544524053224,"where the last inequality follows by βk = PN
c=1 pcβc
k and E∥x −Ex∥2
2 ≤E∥x∥2
2. Combining
Eq.(32) and Eq.(33), we have"
D,0.6294779938587513,"E∥θk −βk∥2
2 ≤1 S N
X"
D,0.6305015353121801,"c=1
pc ∥βc
k −θk0∥2
2 ≤1 S N
X"
D,0.631525076765609,"c=1
pc
βc
k −θc
k0
2
2 ≤1 S N
X"
D,0.6325486182190379,"c=1
pcE k−1
X"
D,0.6335721596724667,"k=k0
2Kη2 ∇ef c(θc
k)

2"
D,0.6345957011258956,"2 + 4Kηdτ
 
ρ2 + (1 −ρ2)/pc
 ≤1 S N
X"
D,0.6356192425793245,"c=1
pc k−1
X"
D,0.6366427840327533,"k=k0
2Kη2E
∇ef c(θc
k)

2"
D,0.6376663254861822,"2 + 4Kηdτ
 
ρ2 + (1 −ρ2)/pc

! ≤28"
D,0.638689866939611,S K2η2dL2Hρ + 4
D,0.6397134083930399,S Kηdτ(ρ2 + N(1 −ρ2))
D,0.6407369498464688,where the last inequality follows a similar argument as in Lemma B.3.
D,0.6417604912998977,"For scheme II, given p1 = p2 = · · · = pN = 1"
D,0.6427840327533265,"N , we have θk = 1 S
P"
D,0.6438075742067554,"c∈Sk βc
k, which leads to"
D,0.6448311156601843,"E∥θk −βk∥2
2 = E"
S,0.6458546571136131,"1
S X"
S,0.646878198567042,"c∈Sk
βc
k −βk  2 2
= 1 S2 E  N
X"
S,0.6479017400204709,"c=1
Ic∈Sk(βc
k −βk)  2 2
,"
S,0.6489252814738997,where IA is an indicator function that equals to 1 if the event A happens.
S,0.6499488229273286,Plugging the facts that P(c ∈Sk) = S
S,0.6509723643807575,"N and P(c1, c2 ∈Sk) =
S(S−1)
N(N−1) for any c1 ̸= c2 ∈[N] into
the above equation, we have"
S,0.6519959058341863,"E∥θk −βk∥2
2 = 1 S2  X"
S,0.6530194472876152,"c∈[N]
P(c ∈Sk) ∥βc
k −βk∥2
2 +
X"
S,0.654042988741044,"c1̸=c2
P(c1, c2 ∈Sk)⟨βc1
k −βk, βc2
k −βk⟩
"
S,0.6550665301944729,"=
1
SN N
X"
S,0.6560900716479018,"c=1
∥βc
k −βk∥2
2 +
X"
S,0.6571136131013307,c1̸=c2
S,0.6581371545547595,"S −1
SN(N −1)⟨βc1
k −βk, βc2
k −βk⟩"
S,0.6591606960081884,"=
1 −S"
S,0.6601842374616171,"N
S(N −1) N
X"
S,0.661207778915046,"c=1
∥βc
k −βk∥2
2 ,"
S,0.6622313203684749,Under review as a conference paper at ICLR 2022
S,0.6632548618219037,where the last equality holds since P
S,0.6642784032753326,"c∈[N] ∥βc
k −βk∥2
2 + P"
S,0.6653019447287615,"c1̸=c2⟨βc1
k
−βk, βc2
k
−βk⟩="
S,0.6663254861821903,"∥βk −βk∥2
2 = 0."
S,0.6673490276356192,"Eventually, we have"
S,0.6683725690890481,"E∥θk −βk∥2
2 =
N −S
S(N −1)E 1 N N
X"
S,0.6693961105424769,"c=1
∥βc
k −βk∥2
2"
S,0.6704196519959058,"≤
N −S
S(N −1)E 1 N N
X"
S,0.6714431934493347,"c=1
∥βc
k −θk0∥2
2"
S,0.6724667349027635,"≤
N −S
S(N −1)"
S,0.6734902763561924,"
28K2η2dL2Hρ + 4Kηdτ
 
ρ2 + N(1 −ρ2)

,"
S,0.6745138178096213,"where the ﬁrst inequality follows a similar argument as in Eq.(33) and the last inequality follows by
Lemma B.3."
S,0.6755373592630501,"C.3
CONVERGENCE VIA PARTIAL DEVICE PARTICIPATION"
S,0.676560900716479,"Theorem C.3 (Restatement of Theorem 5.10). Assume assumptions A.1, A.2, and A.3 hold. Con-
sider Algorithm 6 with a correlation coefﬁcient ρ ∈[0, 1], a ﬁxed learning rate η ∈(0,
1
2L] and
∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], we have"
S,0.6775844421699079,"W2(µk, π) ≤

1 −ηm 4"
S,0.6786079836233367,"k
·
√"
D,0.6796315250767656,"2d
 
D +
p"
D,0.6806550665301945,"τ/m
"
D,0.6816786079836233,"+ 30κ
p"
D,0.6827021494370522,"ηmd ·
q"
D,0.6837256908904811,Hρ((K −1)2 + κ) + 2 r CKdτ
D,0.6847492323439099,"Sm (ρ2 + N(1 −ρ2))CS,"
D,0.6857727737973388,"where CK =
ηmK"
D,0.6867963152507677,1−e−ηmK
D,0.6878198567041965,"2
, CS = 1 for Scheme I and CS = N−S"
D,0.6888433981576254,N−1 for Scheme II.
D,0.6898669396110543,Proof. Note that
D,0.6908904810644831,"E
¯θ(k+1)η −θk+1
2
2
= E
¯θ(k+1)η −βk+1 + βk+1 −θk+1
2
2
= E
¯θ(k+1)η −βk+1
2
2 + E∥βk+1 −θk+1∥2
2 + E2⟨¯θ(k+1)η −βk+1, βk+1 −θk+1⟩"
D,0.691914022517912,"= E
¯θ(k+1)η −βk+1
2
2 + E∥βk+1 −θk+1∥2
2,"
D,0.6929375639713409,where the last equality follows by the unbiasedness of the device-sampling scheme in Lemma C.1.
D,0.6939611054247697,"If k + 1 mod K ̸= 0, we always have βk+1 = θk+1 and E∥βk+1 −θk+1∥2
2 = 0. Following the same
argument as in Lemma B.5, both schemes lead to the one-step iterate as follows"
D,0.6949846468781986,"W 2
2 (µk+1, π) ≤(1 −ηm"
D,0.6960081883316275,"2 ) · W 2
2 (µk, π) + 400η2dL2Hρ((K −1)2 + κ).
(34)"
D,0.6970317297850563,"If k + 1 mod K = 0, combining Lemma C.2 and Lemma B.5, we have"
D,0.6980552712384852,"W 2
2 (µk+1, π) ≤(1 −ηm"
D,0.6990788126919141,"2 ) · W 2
2 (µk, π) + 450η2dL2Hρ(K2 + κ) + 4Kdητ"
D,0.7001023541453428,"S
(ρ2 + N(1 −ρ2))CS, (35)"
D,0.7011258955987717,where CS = 1 for Scheme I and CS = N−S
D,0.7021494370522006,N−1 for Scheme II.
D,0.7031729785056294,"Repeatedly applying Eq.(34) and Eq.(35) and arranging terms, we have that"
D,0.7041965199590583,"W 2
2 (µk, π) ≤

1 −ηm 2"
D,0.7052200614124872,"k
W 2
2 (µ0, π) + 2 ηm"
D,0.706243602865916,"
450η2dL2Hρ(K2 + κ)
"
D,0.7072671443193449,Under review as a conference paper at ICLR 2022
D,0.7082906857727738,+ (1 −(1 −ηm
D,0.7093142272262026,2 )K)⌊k/K⌋
D,0.7103377686796315,1 −(1 −ηm 2 )K
D,0.7113613101330604,4Kdητ
D,0.7123848515864892,"S
(ρ2 + N(1 −ρ2))CS "
D,0.7134083930399181,"≤

1 −ηm 2"
D,0.714431934493347,"k
W 2
2 (µ0, π) + 900ηmdκ2H0((K −1)2 + κ) +
ηmK"
D,0.7154554759467758,1 −e−ηmK
D,0.7164790174002047,"2
|
{z
}
CK"
D,0.7175025588536336,"4Kdητ
ηmKS (ρ2 + N(1 −ρ2))CS,"
D,0.7185261003070624,"=

1 −ηm 2"
D,0.7195496417604913,"k
W 2
2 (µ0, π) + 900ηmdκ2H0((K −1)2 + κ)"
D,0.7205731832139202,+ 4CKdτ
D,0.721596724667349,"Sm
(ρ2 + N(1 −ρ2))CS,"
D,0.7226202661207779,where the second inequality follows by (1 −r)K ≤e−rK for any r ≥0.
D,0.7236438075742068,"D
BOUNDING CONTRACTION, DISCRETIZATION, AND DIVERGENCE"
D,0.7246673490276356,"D.1
DOMINATED CONTRACTION PROPERTY"
D,0.7256908904810645,"Proof of Lemma B.1 . Given a client index c ∈[N], applying Theorem 2.1.12 Nesterov (2004) leads
to"
D,0.7267144319344934,"⟨y −x, ∇f c(y) −∇f c(x)⟩≥
mL
L + m ∥y −x∥2
2 +
1
L + m ∥∇f c(y) −∇f c(x)∥2
2 ,
∀x, y ∈Rd. (36)"
D,0.7277379733879222,"In what follows, we have"
D,0.7287615148413511,"∥β −θ −η(∇f(β) −∇f(θ))∥2
2
= ∥β −θ∥2
2 −2η ⟨β −θ, ∇f(β) −∇f(θ)⟩
|
{z
}
I"
D,0.72978505629478,"+η2 ∥∇f(β) −∇f(θ)∥2
2 .
(37)"
D,0.7308085977482088,"For the second item I in the right hand side, we have I = N
X"
D,0.7318321392016377,"c=1
pc

β −θ, ∇f c(βc) −∇f c(θc) = N
X"
D,0.7328556806550666,"c=1
pc

β −βc + βc −θc + θc −θ, ∇f c(βc) −∇f c(θc) = − N
X"
D,0.7338792221084954,"c=1
pc
 
βc −β, ∇f c(βc) −∇f c(θc)

+

θ −θc, ∇f c(βc) −∇f c(θc)
 + N
X"
D,0.7349027635619243,"c=1
pc

βc −θc, ∇f c(βc) −∇f c(θc) ≥− N
X"
D,0.7359263050153532,"c=1
pc ·
 
(m + L) ∥βc −β∥2
2 + (m + L) ∥θc −θ∥2
2 +
1
2(m + L) ∥∇f c(βc) −∇f c(θc)∥2
2
 + N
X"
D,0.736949846468782,"c=1
pc ·
  mL"
D,0.7379733879222109,"L + m ∥βc −θc∥2
2 +
1
L + m ∥∇f c(βc) −∇f c(θc)∥2
2
"
D,0.7389969293756398,"≥−(m + L) N
X"
D,0.7400204708290685,"c=1
pc

∥βc −β∥2
2 + ∥θc −θ∥2
2

+
mL
L + m ∥β −θ∥2
2"
D,0.7410440122824974,"+
1
2(L + m) ∥∇f(β) −∇f(θ)∥2
2 ,
(38)"
D,0.7420675537359263,Under review as a conference paper at ICLR 2022
D,0.7430910951893551,"where the ﬁrst inequality follows by the AM-GM inequality and Eq.(36), respectively; the last
inequality follows by Jensen’s inequality such that N
X"
D,0.744114636642784,"c=1
pc∥βc −θc∥2
2 ≥  N
X"
D,0.7451381780962129,"c=1
pc(βc −θc)  2"
D,0.7461617195496417,"2
= ∥β −θ∥2
2 N
X"
D,0.7471852610030706,"c=1
pc ∥∇f c(βc) −∇f c(θc)∥2
2 ≥  N
X"
D,0.7482088024564995,"c=1
pc"
D,0.7492323439099283,"
∇f c(βc) −∇f c(θc)
 2"
D,0.7502558853633572,"2
= ∥∇f(β) −∇f(θ)∥2
2 ."
D,0.7512794268167861,"Plugging Eq.(38) into Eq.(37), we have"
D,0.7523029682702149,"∥β −θ −η · (∇f(β) −∇f(θ))∥2
2"
D,0.7533265097236438,"≤
 
1 −2ηmL"
D,0.7543500511770727,"m + L

· ∥β −θ∥2
2 + η
 
η −
1
m + L
|
{z
}
≤0 if η≤
1
m+L"
D,0.7553735926305015,"
· ∥∇f(β) −∇f(θ)∥2
2"
D,0.7563971340839304,"+ 2η(m + L) N
X"
D,0.7574206755373593,"c=1
pc · (∥βc −β∥2
2 + ∥θc −θ∥2
2)"
D,0.7584442169907881,"≤(1 −ηm) ∥β −θ∥2
2 + 4ηL N
X"
D,0.759467758444217,"c=1
pc ·
 
∥βc −β∥2
2 + ∥θc −θ∥2
2

,"
D,0.7604912998976459,"where the last inequality follows by
2L
m+L ≥1, m ≤L, 1 −2a ≤(1 −a)2 for any a, and
η ∈(0,
1
m+L]."
D,0.7615148413510747,"D.2
DISCRETIZATION ERROR"
D,0.7625383828045036,"Proof of Lemma B.2. For any s ∈[0, ∞), there exists a certain k ∈N+ such that s ∈[kη, (k + 1)η).
By the continuous dynamics of Eq. (19), we have"
D,0.7635619242579325,"¯θc
s = ¯θc
η⌊s"
D,0.7645854657113613,"η ⌋+ (s −kη)∇f c(¯θc
η⌊s"
D,0.7656090071647902,"η ⌋) +
√"
D,0.7666325486182191,"2τ
Z s"
D,0.7676560900716479,"kη
dW t,"
D,0.7686796315250768,which suggests that
D,0.7697031729785057,"sup
s∈[kη,(k+1)η)"
D,0.7707267144319345,"¯θc
s −¯θc
η⌊s"
D,0.7717502558853634,"η ⌋

2 ≤(s −kη)
∇f c(¯θc
η⌊s"
D,0.7727737973387923,"η ⌋)

2 +
sup
s∈[kη,(k+1)η)  Z s kη √"
D,0.7737973387922211,"2τdW t 2
."
D,0.77482088024565,We ﬁrst square the terms on both sides and take Young’s inequality and expectation
D,0.7758444216990789,"E
sup
s∈[kη,(k+1)η)"
D,0.7768679631525077,"¯θc
s −¯θc
η⌊s"
D,0.7778915046059366,"η ⌋
2
2 ≤2E
(s −kη)∇f c(¯θc
η⌊s"
D,0.7789150460593655,"η ⌋)
2
2"
D,0.7799385875127943,"+ 2E
sup
s∈[kη,(k+1)η)  Z s kη √"
D,0.7809621289662231,"2τdW t  2 2
."
D,0.781985670419652,"Then, by Burkholder-Davis-Gundy inequality (50) and Itô isometry, we have"
D,0.7830092118730808,"E
sup
s∈[kη,(k+1)η)"
D,0.7840327533265097,"¯θc
s −¯θc
η⌊s"
D,0.7850562947799385,"η ⌋
2
2 ≤2E
(s −kη)∇f c(¯θc
η⌊s"
D,0.7860798362333674,"η ⌋)
2
2 + 8 d
X"
D,0.7871033776867963,"i=1
E
Z s"
D,0.7881269191402251,"kη
2τdt"
D,0.789150460593654,"≤2η2E
∇f c(¯θc
η⌊s"
D,0.7901740020470829,"η ⌋)
2
2 + 16ηdτ.
(39)"
D,0.7911975435005117,"By Young’s inequality and the smoothness assumption A.1, we have"
D,0.7922210849539406,E∥∇f c(¯θη⌊s
D,0.7932446264073695,"η ⌋)∥2
2 = E∥∇f c(¯θc
η⌊s"
D,0.7942681678607983,"η ⌋) −∇f c(θ∗) + ∇f c(θ∗)∥2
2"
D,0.7952917093142272,Under review as a conference paper at ICLR 2022
D,0.7963152507676561,"≤2E∥∇f c(¯θc
η⌊s"
D,0.797338792221085,"η ⌋) −∇f c(θ∗)∥2
2 + 2∥∇f c(θ∗)∥2
2"
D,0.7983623336745138,"≤2L2E∥¯θc
η⌊s"
D,0.7993858751279427,"η ⌋−θ∗∥2
2 + 2γ2"
D,0.8004094165813715,"≤2L2
 1 m γ2"
D,0.8014329580348004,"m + 2dτ

+ 2γ2"
D,0.8024564994882293,"≤4dκ
κγ2"
D,0.8034800409416581,"d
+ 4Lτ

,
(40)"
D,0.804503582395087,"where the third inequality follows by Lemma E.2, the fourth step holds since κ ≥1. Combining
Eq. (39) and Eq. (40), we have"
D,0.8055271238485159,"E
sup
s∈[kη,(k+1)η)"
D,0.8065506653019447,"¯θc
s −¯θc
η⌊s"
D,0.8075742067553736,"η ⌋
2
2 ≤8η2dκ
κγ2"
D,0.8085977482088025,"d
+ Lτ

+ 16ηdτ."
D,0.8096212896622313,"D.3
BOUNDED DIVERGENCE"
D,0.8106448311156602,"Proof of Lemma B.3. For any k ≥0, consider k0 = K⌊k"
D,0.8116683725690891,"K ⌋such that k ≤k0 and θc
k0 = θk0 for any
k ≥0. It is clear that k −k0 ≤K −1 for all k ≥0. Consider the non-increasing learning rate such
that ηk0 ≤2ηk for all k −k0 ≤K −1."
D,0.812691914022518,"By the iterate Eq.(18), we have N
X"
D,0.8137154554759468,"c=1
pcE∥θc
k −θk∥2
2 = N
X"
D,0.8147389969293757,"c=1
pcE∥θc
k −θk0 −(θk −θk0)∥2
2 ≤ N
X"
D,0.8157625383828045,"c=1
pcE∥θc
k −θk0∥2
2 ≤ N
X"
D,0.8167860798362334,"c=1
pcE k−1
X"
D,0.8178096212896623,"k=k0
2(K −1)η2
k
∇ef c(θc
k)

2"
D,0.8188331627430911,"2 + 4(K −1)ηkdτ(ρ2 + (1 −ρ2)/pc) ≤ N
X"
D,0.81985670419652,"c=1
pc"
D,0.8208802456499488," k−1
X"
D,0.8219037871033776,"k=k0
2(K −1)η2
k0E
∇ef c(θc
k)

2"
D,0.8229273285568065,"2 + 4(K −1)ηk0dτ(ρ2 + (1 −ρ2)/pc)
"
D,0.8239508700102354,"≤112(K −1)2η2
kdL2Hρ + 8(K −1)ηkdτ(ρ2 + N(1 −ρ2)),"
D,0.8249744114636642,"where the ﬁrst inequality holds by E∥θ −Eθ∥2
2 ≤E∥θ∥2
2 for a stochastic variable θ; the second
inequality follows by (PK−1
i=1 ai)2 ≤(K −1) PK−1
i=1 a2
i ; the last inequality follows by Lemma E.3
and η2
k0 ≤4η2
k. Hρ is deﬁned in Deﬁnition A.4."
D,0.8259979529170931,"D.4
BOUNDED VARIANCE"
D,0.827021494370522,"Proof of Lemma B.4. By assumption A.3, we have"
D,0.8280450358239508,"E
∇f(θ) −∇ef(θ)

2 2 = E  N
X"
D,0.8290685772773797,"c=1
pc"
D,0.8300921187308086,"
∇f c(θc) −∇ef c(θc)
 2 2 = N
X"
D,0.8311156601842374,"c=1
p2
cE
∇f c(θc) −∇ef c(θc)

2 2"
D,0.8321392016376663,Under review as a conference paper at ICLR 2022
D,0.8331627430910952,"≤dσ2
N
X"
D,0.834186284544524,"c=1
p2
c ≤dσ2
 N
X"
D,0.8352098259979529,"c=1
pc !2"
D,0.8362333674513818,:= dσ2.
D,0.8372569089048106,"E
UNIFORM UPPER BOUND"
D,0.8382804503582395,"E.1
DISCRETE DYNAMICS"
D,0.8393039918116684,"Lemma E.1 (Discrete dynamics). Assume assumptions A.1, A.2, and A.3 hold. We consider the
generalized formulation in Algorithm 5 with the temperature"
D,0.8403275332650972,"Tc,ρ = τ(ρ2 + (1 −ρ2)/pc)"
D,0.8413510747185261,"given a correlation coefﬁcient ρ. For any learning rate η ∈(0, 2/m) and ∥θc
0 −θ∗∥2
2 ≤dD2 for any
c ∈[N], we have the ℓ2 norm upper bound as follows"
D,0.842374616171955,"sup
k
E∥θc
k −θ∗∥2
2 ≤dD2 + 6d m"
D,0.8433981576253838,"
max
c∈[N] Tc,ρ + σ2"
D,0.8444216990788127,"m + γ2 md 
,"
D,0.8454452405322416,where γ := maxc∈[N] ∥∇f c(θ∗)∥2 and θ∗denotes the global minimum for the function f.
D,0.8464687819856704,"Proof. First, we consider the k-th iteration, where k ∈{1, 2, · · · , K −2, (K −1)−} and (K −1)−
denotes the (K −1)-step before synchronization. Following the iterate of Eq.(13) in a local client of
c ∈[N], we have"
D,0.8474923234390993,"E
θc
k+1 −θ∗
2
2
= E∥θc
k −θ∗−η∇ef c(θc
k)∥2
2 +
p"
D,0.8485158648925282,"8ηTc,ρE⟨θc
k −θ∗−η∇ef c(θc
k), ξk⟩+ 2ηTc,ρE∥ξk∥2
2
= E∥θc
k −θ∗−η∇ef c(θc
k)∥2
2 + 2ηdTc,ρ,
(41)"
D,0.849539406345957,"where the last equality follows from Eξk = 0 and the conditional independence of θc
k −θ∗−ef c(θc
k)
and ξk. Note that"
D,0.8505629477993859,"E∥θc
k −θ∗−η ef c(θc
k)∥2
2
= E∥θc
k −θ∗−η∇f c(θc
k)∥2
2 + η2E∥∇f c(θc
k) −∇ef c(θc
k)∥2
2
+ 2ηE⟨θc
k −θ∗−η∇f c(θc
k), ∇f c(θc
k) −∇ef c(θc
k)⟩"
D,0.8515864892528148,"= E∥θc
k −θ∗−η∇f c(θc
k)∥2
2 + η2E∥∇f c(θc
k) −∇ef c(θc
k)∥2
2
≤E∥θc
k −θ∗−η∇f c(θc
k)∥2
2 + η2dσ2,
(42)"
D,0.8526100307062436,"where the ﬁrst step follows from simple algebra, the second step follows from the unbiasedness of
the stochastic gradient, and the last step follows from Assumption A.3. For any q > 0, we can upper
bound the ﬁrst term of Eq.(42) as follows"
D,0.8536335721596725,"E∥θc
k −θ∗−η∇f c(θc
k)∥2
2
= E∥θc
k −θ∗−η(∇f c(θc
k) −∇f c(θ∗)) −η∇f c(θ∗)∥2
2"
D,0.8546571136131014,"≤(1 + q)E∥θc
k −θ∗−η(∇f c(θc
k) −∇f c(θ∗))∥2
2 + η2

1 + 1 q"
D,0.8556806550665302,"
∥∇f c(θ∗)∥2
2"
D,0.8567041965199591,"≤(1 + q)

1 −ηm 2 2"
D,0.857727737973388,"|
{z
}
ψ2"
D,0.8587512794268168,"E∥θc
k −θ∗∥2
2 + η2

1 + 1 q"
D,0.8597748208802457,"
γ2,
(43)"
D,0.8607983623336745,"where the ﬁrst inequality follows by the AM-GM inequality; the second inequality is a special
case of Lemma B.1 based on Assumption A.2, where no local steps is involved before the syn-
chronization step. Similar results have been achieved in Theorem 3 Dalalyan (2017a). In addition,
γ := maxc∈[N] ∥∇f c(θ∗)∥2."
D,0.8618219037871033,Under review as a conference paper at ICLR 2022
D,0.8628454452405322,Choose q = ( 1+ψ
D,0.8638689866939611,"2ψ )2 −1 so that (1 + q)ψ2 =
(1+ψ)2"
D,0.8648925281473899,"4
. Moreover, since ψ = 1 −ηm"
D,0.8659160696008188,"2 , we get 1+ψ"
D,0.8669396110542477,"2
= 1 −1"
D,0.8679631525076765,"4ηm. In addition, we have 1 + 1"
D,0.8689866939611054,q = 1+q
D,0.8700102354145343,"q
=
(1+ψ)2"
D,0.8710337768679631,"(1−ψ)(1+3ψ) ≤
2
ηm. It follows that"
D,0.872057318321392,"η2

1 + 1 q 
≤2η"
D,0.8730808597748209,"m .
(44)"
D,0.8741044012282497,"Combining Eq. (41), Eq. (42), Eq. (43), and Eq. (44), we have the following iterate"
D,0.8751279426816786,"E∥θc
k+1 −θ∗∥2
2 ≤

1 −ηm 4 2"
D,0.8761514841351075,"|
{z
}
:=g(η)"
D,0.8771750255885363,"E∥θc
k −θ∗∥2
2 + 2ηdTc,ρ + η2dσ2 + 2ηγ2 m ."
D,0.8781985670419652,"Note that
1
1−g(η) =
1
ηm"
D,0.8792221084953941,2 (1−ηm
D,0.8802456499488229,"8 ) ≤
3
ηm given η ∈(0, 2"
D,0.8812691914022518,"m). Recursively applying the above equation k
times, where k ∈{1, 2, · · · , K −1, K−} and K−denotes the K-step without synchronization, it
follows that"
D,0.8822927328556807,"E∥θc
k −θ∗∥2
2 ≤g(η)k∥θc
0 −θ∗∥2
2 + 1 −g(η)k"
D,0.8833162743091095,"1 −g(η) ·

2ηdTc,ρ + η2dσ2 + 2ηγ2 m"
D,0.8843398157625384,"
(45)"
D,0.8853633572159673,"≤∥θc
0 −θ∗∥2
2 + 3"
D,0.8863868986693961,"ηm ·

2ηdTc,ρ + η2dσ2 + 2ηγ2 m "
D,0.887410440122825,≤dD2 + 6d m
D,0.8884339815762539,"
max
c∈[N] Tc,ρ + σ2"
D,0.8894575230296827,m + γ2 md 
D,0.8904810644831116,"|
{z
}
:=U ,"
D,0.8915046059365405,"where the second inequality holds by g(η) ≤1, the third inequality holds because ∥θc
0 −θ∗∥2
2 ≤dD2"
D,0.8925281473899693,for any c ∈[N] and η < 2
D,0.8935516888433982,"m. In particular, the K-th step before synchronization yields that"
D,0.8945752302968271,"E∥θc
K−−θ∗∥2
2 ≤dD2 + U.
(46)"
D,0.8955987717502559,"Having all the results ready, for the K-local step after synchronization, applying Jensen’s inequality"
D,0.8966223132036848,"E∥θc
K −θ∗∥2
2 = E N
X"
D,0.8976458546571137,"c=1
pcθc
K−−θ∗  2 2 ≤ N
X"
D,0.8986693961105425,"c=1
pcE
θc
K−−θ∗
2
2"
D,0.8996929375639714,"≤dD2 + U,
(47)
Now starting from iteration K, we adapt the recursion of Eq.(45) for the k-th step, where k ∈
{K + 1, · · · , 2K −1, (2K)−} and (2K)−denotes the 2K-step without synchronization, we have"
D,0.9007164790174002,"E∥θc
k −θ∗∥2
2"
D,0.901740020470829,"≤g(η)k−K · E∥θc
K −θ∗∥2
2 + 1 −g(η)k−K"
D,0.9027635619242579,"1 −g(η)
·

2ηd max
c∈[N] Tc,ρ + η2dσ2 + 2ηγ2 m "
D,0.9037871033776868,≤g(η)k−K(dD2 + U) + 1 −g(η)k−K
D,0.9048106448311156,"mη/3
mη"
U,0.9058341862845445,3 U
U,0.9068577277379734,≤dD2 + g(η)k−KU + (1 −g(η)k−K)U
U,0.9078812691914022,"≤dD2 + U,
(48)"
U,0.9089048106448311,"where the second inequality follows by Eq.(47), the fact that 1 −g(η) ≥ηm/3 and η ≤2"
U,0.90992835209826,"m, and the
deﬁnition of U. The third one holds since g(η) ≤1."
U,0.9109518935516888,"By repeating Eq.(47) and (48), we have that for all k ≥0, we can obtain the desired uniform upper
bound."
U,0.9119754350051177,"Discussions: Since the above result is independent of the learning rate η, it can be naturally applied
to the setting with decreasing learning rates. The details are omitted."
U,0.9129989764585466,Under review as a conference paper at ICLR 2022
U,0.9140225179119754,"E.2
CONTINUOUS DIFFUSION"
U,0.9150460593654043,"Lemma E.2 (Continuous time). Assume assumption A.2 holds. We have the ℓ2 norm upper bound as
follows"
U,0.9160696008188332,"sup
t E
¯θc
t −θ∗
2
2 ≤1 m γ2"
U,0.917093142272262,"m + 2dτ

,"
U,0.9181166837256909,where γ := maxc∈[N] ∥∇f c(θ∗)∥2 and θ∗denotes the global minimum for the function f.
U,0.9191402251791198,"Proof. Since the synchronization is conducted at every time t, the essential temperature applied to
each client is τ. Let q(¯θc
t) =
¯θc
t −θ∗
2
2. For any time t ≥0, applying Itô’s lemma leads to"
U,0.9201637666325486,"dq(¯θc
t) = −2⟨¯θc
t −θ∗, ∇f c(¯θc
t)⟩dt + 2dτdt +
√"
U,0.9211873080859775,"8τ⟨¯θc
t −θ∗, dW t⟩"
U,0.9222108495394064,"= −2⟨¯θc
t −θ∗, ∇f c(¯θc
t) −∇f c(θ∗) + ∇f c(θ∗)⟩dt + 2dτdt +
√"
U,0.9232343909928352,"8τ⟨¯θc
t −θ∗, dW t⟩"
U,0.9242579324462641,"≤−2m
¯θc
t −θ∗
2
2 dt −2⟨¯θc
t −θ∗, ∇f c(θ∗)⟩dt + 2dτdt +
√"
U,0.925281473899693,"8τ⟨¯θc
t −θ∗, dW t⟩"
U,0.9263050153531218,"≤−2m
¯θc
t −θ∗
2
2 dt + m
¯θc
t −θ∗
2
2 dt + ∥∇f c(θ∗)∥2
2
m
dt + 2dτdt +
√"
U,0.9273285568065507,"8τ⟨¯θc
t −θ∗, dW t⟩"
U,0.9283520982599796,"≤−mq(¯θc
t)dt +
γ2"
U,0.9293756397134084,"m + 2dτ

dt +
√"
U,0.9303991811668373,"8τ⟨¯θc
t −θ∗, dW t⟩,"
U,0.9314227226202662,"where the ﬁrst inequality follows by Assumption A.2; the second inequality follows by the AM-GM
inequality; the third inequality follows by the deﬁnition that γ2 = maxc∈[N] ∥∇f c(θ∗)∥2
2."
U,0.932446264073695,"In other words, we have"
U,0.9334698055271239,"d(emtq(¯θc
t)) = memtq(¯θc
t)dt + emtdq(¯θc
t)"
U,0.9344933469805528,"≤memtq(¯θc
t)dt + emt

−mq(¯θc
t)dt +
γ2"
U,0.9355168884339816,"m + 2dτ

dt +
√"
U,0.9365404298874105,"8τ⟨¯θc
t −θ∗, dW t⟩
"
U,0.9375639713408394,"≤emt
γ2"
U,0.9385875127942682,"m + 2dτ

dt +
√"
U,0.9396110542476971,"8τemt⟨¯θc
t −θ∗, dW t⟩."
U,0.9406345957011258,The solution is upper bounded by
U,0.9416581371545547,"emtq(¯θc
t) ≤em·0q(¯θc
0) +
Z t 0"
U,0.9426816786079836,"
ems
γ2"
U,0.9437052200614124,"m + 2dτ

ds +
√"
U,0.9447287615148413,"8τems⟨¯θc
s −θ∗, dW s⟩

."
U,0.9457523029682702,"By the martingale property of Itô integral, taking expectations yields"
U,0.946775844421699,"Eq(¯θc
t) ≤e−mtEq(¯θc
0) + e−mt
γ2"
U,0.9477993858751279,"m + 2dτ
 Z t"
EMSDS,0.9488229273285568,"0
emsds"
EMSDS,0.9498464687819856,"= e−mtEq(¯θc
0) + 1 −e−mt m
 γ2"
EMSDS,0.9508700102354145,"m + 2dτ
"
EMSDS,0.9518935516888434,"≤e−mtEq(¯θc
0) + 1 −e−mt"
EMSDS,0.9529170931422722,"m
  γ2"
EMSDS,0.9539406345957011,"m + 2dτ
|
{z
}
:=V"
EMSDS,0.95496417604913,"
,
(49)"
EMSDS,0.9559877175025588,"where the last inequality follows since the synchronization is conducted at any time step t. Since ¯θc
0 is
simulated from the stationary distribution π, by Lemma 12 Durmus & Moulines (2016) or Theorem
17 Cheng et al. (2018), we have"
EMSDS,0.9570112589559877,"Eq(¯θc
0) = E∥¯θc
0 −θ∗∥2
2 ≤dτ m ≤1 m(γ2"
EMSDS,0.9580348004094166,"m + 2dτ) = V m,"
EMSDS,0.9590583418628454,which completes the proof.
EMSDS,0.9600818833162743,Under review as a conference paper at ICLR 2022
EMSDS,0.9611054247697032,"E.3
BOUNDED GRADIENT"
EMSDS,0.962128966223132,"Lemma E.3 (Bounded gradient in ℓ2 norm). Given assumptions A.1, A.2, and A.3 hold, for any
client c and any learning rate η ∈(0, 2/m) and ∥θc
0 −θ∗∥2
2 ≤dD2 for any c ∈[N], we have the ℓ2
norm upper bound as follows"
EMSDS,0.9631525076765609,"E∥∇ef c(θc
k)∥2
2 ≤14dL2Hρ,"
EMSDS,0.9641760491299898,where Hρ = D2 + 1
EMSDS,0.9651995905834186,"m maxc∈[N] Tc,ρ +
γ2"
EMSDS,0.9662231320368475,m2d + σ2 m2 .
EMSDS,0.9672466734902764,Proof. Decompose the ℓ2 of the gradient as follows
EMSDS,0.9682702149437052,"E
∇ef c(θc
k)

2"
EMSDS,0.9692937563971341,"2 = E
∇ef c(θc
k) −∇f c(θc
k) + ∇f c(θc
k)

2 2"
EMSDS,0.970317297850563,"= E∥∇f c(θc
k)∥2
2 + E
∇ef c(θc
k) −∇f c(θc
k)

2 2"
EMSDS,0.9713408393039918,"+ 2E
D
∇ef c(θc
k) −∇f c(θc
k), ∇f c(θc
k)
E"
EMSDS,0.9723643807574207,"≤E∥∇f c(θc
k)∥2
2 + σ2d"
EMSDS,0.9733879222108496,"= E∥∇f c(θc
k) −∇f c(θ∗) + ∇f c(θ∗)∥2
2 + σ2d"
EMSDS,0.9744114636642784,"≤2E∥∇f c(θc
k) −∇f c(θ∗)∥2
2 + 2E
∇f c(θ∗)
2
2 + σ2d"
EMSDS,0.9754350051177073,"≤2L2E∥θc
k −θ∗∥2
2 + 2γ2 + σ2d"
EMSDS,0.9764585465711362,≤2dL2D2 + 12dL2
EMSDS,0.977482088024565,"m
·

max
c∈[N] Tc,ρ + σ2"
EMSDS,0.9785056294779939,m + γ2 md
EMSDS,0.9795291709314228,"
+ 2γ2 + σ2d"
EMSDS,0.9805527123848515,"≤14dL2 ·

D2 + 1"
EMSDS,0.9815762538382804,"m max
c∈[N] Tc,ρ + γ2"
EMSDS,0.9825997952917093,m2d + σ2 m2
EMSDS,0.9836233367451381,"
:= 14dL2Hρ,"
EMSDS,0.984646878198567,"where the ﬁrst inequality follows by Assumption A.3; the second inequality follows by Young’s
inequality; the third inequality follows by Assumption A.1 and the deﬁnition that γ
:=
maxc∈[N] ∥∇f c(θ∗)∥2; the fourth inequality follows by Lemma E.1; the last inequality follows
by κ := L m ≥1."
EMSDS,0.9856704196519959,"F
INITIAL CONDITION"
EMSDS,0.9866939611054247,"Lemma F.1 (Initial condition). Let µ0 denote the Dirac delta distribution at θ0. Then, we have"
EMSDS,0.9877175025588536,"W2(µ0, π) ≤
√"
EMSDS,0.9887410440122825,"2(∥θ0 −θ∗∥2 +
p"
EMSDS,0.9897645854657113,dτ/m).
EMSDS,0.9907881269191402,"Proof. By Cheng et al. (2018), there exists an optimal coupling between µ0 and π such that"
EMSDS,0.9918116683725691,"W 2
2 (µ0, π) ≤Eθ∼π[∥θ0 −θ∥2
2]"
EMSDS,0.9928352098259979,"≤2Eθ∼π[∥θ0 −θ∗∥2
2] + 2Eθ∼π[∥θ −θ∗∥2
2]"
EMSDS,0.9938587512794268,"= 2∥θ0 −θ∗∥2
2 + 2Eθ∼π[∥θ −θ∗∥2
2]"
EMSDS,0.9948822927328557,"≤2∥θ0 −θ∗∥2
2 + 2dτ/m,"
EMSDS,0.9959058341862845,"where the second step follows from triangle inequality, the last step follows from Lemma 12 Durmus
& Moulines (2016) and the temperature τ is included to adapt to the time scaling."
EMSDS,0.9969293756397134,"Burkholder-Davis-Gundy inequality Let φ : [0, ∞) →Rr×d for some positive integers r and
d. In addition, we assume E
R ∞
0
|ψ(s)|2ds < ∞and let Z(t) =
R t
0 ψ(s)dWs, where Ws is a
d-dimensional Brownian motion. Then for all t ≥0, we have"
EMSDS,0.9979529170931423,"E sup
0≤s≤t
|Z(s)|2 ≤4E
Z t"
EMSDS,0.9989764585465711,"0
|φ(s)|2ds.
(50)"
