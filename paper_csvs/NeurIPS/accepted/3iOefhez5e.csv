Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0004008016032064128,"We study the low-degree hardness of broadcasting on trees. Broadcasting on trees
has been extensively studied in statistical physics, in computational biology in
relation to phylogenetic reconstruction and in statistics and computer science in
the context of block model inference, and as a simple data model for algorithms
that may require depth for inference.
The inference of the root can be carried by celebrated Belief Propagation (BP)
algorithm which achieves Bayes-optimal performance. Recent works indicated that
this algorithm in fact requires high level of complexity. Moitra, Mossel and Sandon
constructed a chain for which estimating the root better than random (for a typical
input) is NC1 complete. Kohler and Mossel constructed chains such that for trees
with N leaves, recovering the root better than random requires a polynomial of
degree N Ω(1). Both works above asked if such complexity bounds hold in general
below the celebrated Kesten-Stigum bound.
In this work, we prove that this is indeed the case for low degree polynomials.
We show that for the broadcast problem using any Markov chain on trees with
N leaves, below the Kesten Stigum bound, any O(log N) degree polynomial has
vanishing correlation with the root.
Our result is one of the first low-degree lower bound that is proved in a setting that
is not based or easily reduced to a product measure."
INTRODUCTION,0.0008016032064128256,"1
Introduction"
INTRODUCTION,0.0012024048096192384,"Understanding the computational complexity inference problems of random instances has been
extensively studies in different research areas including statistics, cryptography, computational
complexity, computational learning theory and statistical physics. The emerging field of research is
mainly devoted to the study of computational-to-statistical gaps. ([3, 41, 24])"
INTRODUCTION,0.0016032064128256513,"Recently, low-degree polynomials have emerged as a popular tool for predicting computational-
to-statistical gaps, especially in the context of the Bayesian framework. Our work follows [23] in
studying the polynomial hardness of broadcasting on trees."
INTRODUCTION,0.002004008016032064,"A very exciting line of work, including [19, 18, 25, 4, 15, 26, 17, 9, 40] recently showed that the
“low-degree heuristic” can be used to predict computational-statistical gaps for a variety of problems
such as recovery in general stochastic block models, sparse PCA, tensor PCA, the planted clique
problem, certification in the zero-temperature Sherrington-Kirkpatrick model, the planted sparse
vector problem, and for finding solutions in random k-SAT problems."
INTRODUCTION,0.002404809619238477,"Interestingly, it was observed that the predictions from this method often agree with predictions from
statistical physics heuristics based on the replica and cavity methods which are closely related to the
analysis of BP/AMP fixed points, see e.g. [12, 13, 28])."
INTRODUCTION,0.0028056112224448897,"It is often argued that low-degree polynomials algorithms are relatively easy to use (e.g. compared to
proving SOS lower bounds), and that low degree polynomials capture the power of the “local algo-
rithms” framework used in e.g. [16, 10] as well as algorithms which incorporate global information,
such as spectral methods or a constant number of iterations of Approximate Message Passing [29]."
INTRODUCTION,0.0032064128256513026,"In this work, we continue to study the power of low-degree polynomials for the (average case)
broadcast on trees problem. In broadcasting on trees the goal is to estimate the value of the Markov
process at the root given its value at the leaves and the goal is to do so for arbitrarily deep trees. Two
key parameters of the model are the arity of the tree d and the magnitude of the second eigenvalue λ
of the broadcast chain."
INTRODUCTION,0.0036072144288577155,"A fundamental result in this area [22] is that when d|λ|2 > 1 nontrivial reconstruction of the root is
possible by counting the number of the leaves of different types, an algorithm that could be described
as a linear function of the leave values. In contrast, when d|λ|2 < 1, such linear estimators have no
mutual information with the root (but more complex statistics of the leaves may) [30, 36]."
INTRODUCTION,0.004008016032064128,"This threshold d|λ|2 = 1 is known as the Kesten-Stigum threshold. A series of works showed that
the KS threshold is the information theory threshold for non-trivial root inference for some specific
channels, including the binary symmetric channel [6, 14, 21, 20] and binary channels that are close
to symmetric [8], as well as 3 × 3 symmetric channels for large d [39]."
INTRODUCTION,0.004408817635270541,"While the Kesten-Stigum bound is easy to compute, it turns out that in many cases, it is not the
information-theoretic threshold for root recovery. This was first established in [30] for symmetric
channels with sufficiently many states, specifically when q ≥C for some large constant C. Later it
was shown for symmetric channels with q ≥5 states in [39]. Recently, the results [37] provide more
information about the case of q = 3 and q = 4. Many of the finer results in this area prove predictions
from statistical physics. The connection between the broadcast problems and phase transitions in
statistical physics was made in [27], and more recent predictions include [5, 2, 38]. Moreover, the
information-theoretic threshold may depend on the specific structure of the channel rather than solely
on d and λ. Notably, [30] also showed that there exists channels where non-trivial predictions of the
root are achievable even when |λ| = 0."
INTRODUCTION,0.004809619238476954,"Much of the interest in Kesten-Stigum threshold comes from the fundamental role it plays in problems,
such as algorithmic recovery in the stochastic block model [11, 33, 7, 35, 1] and phylogenetic
reconstruction [31]. Count statistics can be viewed as degree 1 polynomials of the leaves, which begs
the question of what information more general polynomials can extract from the leaves. See [32, 34]
for surveys on the topic."
INTRODUCTION,0.0052104208416833666,"In [23] it was shown that λ = 0 even polynomials of degree N c, where N = dℓis the number of
leaves of for a d-ary tree of depth ℓ, for a small c > 0 are not able to correlate with the root label (as
ℓtends to ∞), whereas computationally efficient reconstruction is generally possible as long as d is a
sufficiently large constant [30]."
INTRODUCTION,0.0056112224448897794,"The main motivation of [23] was to prove that low degree polynomials fail below the Kesten Stigum
bound: “It is natural to wonder if the Kesten-Stigum threshold d|λ|2 = 1 is sharp for low-degree
polynomial reconstruction, analogous to how it is sharp for robust reconstruction."" However the main
result of [23] only established this in the very special case of λ = 0. This problem is also stated in
the ICM 2022 paper and talk on the broadcast process [34]: “ The authors of [23] ask if a similar
phenomenon holds through the non-linear regime. For example, is it true that polynomials of bounded
degree have vanishing correlation with X0 in the regime where dλ2 < 1? "" The main results of this
paper prove that this is indeed the case. We proceed with formal definitions and statement of the main
result."
DEFINITIONS AND MAIN RESULT,0.006012024048096192,"1.1
Definitions and Main Result"
DEFINITIONS AND MAIN RESULT,0.006412825651302605,Rooted Tree
DEFINITIONS AND MAIN RESULT,0.006813627254509018,"Recall that every rooted tree T inherently defines a partial order relation among its vertices: For a
pair of distinct vertices, u is said to be an ancestor of v (and v a descendant of u), denoted as v < u
in this paper, if u is contained in the unique path from v to the root ρ. Specifically, if v and u are
directly connected by an edge, we also refer to v as a child vertex of u (and u as the parent vertex of
v). By v ≤u we mean that v is either a descendant of u or v = u. In general, if v < u and the path
distance between them is k, v is referred to as a kth-descendant of u (and u as the kth ancestor of v)."
DEFINITIONS AND MAIN RESULT,0.007214428857715431,"For a nonnegative integer k, the kth layer of the tree refers to the set of kth descendants of the root ρ.
(The root here is considered at the 0th layer of the tree.) The depth of the tree is defined as the largest
non-negative integer ℓfor which the ℓth layer is not empty, and we denote the ℓth layer by L."
DEFINITIONS AND MAIN RESULT,0.007615230460921844,The height of a vertex u is defined as
DEFINITIONS AND MAIN RESULT,0.008016032064128256,"h(u) = ℓ−the layer of u.
(1)"
DEFINITIONS AND MAIN RESULT,0.008416833667334669,"In particular, when L is the set of leaves, then h(u) is simply the distance from u to L. In this paper,
we may abuse the notation by writing x ∈T or S ⊆T to mean that x is a vertex or S is a subset of
vertices in the tree T."
DEFINITIONS AND MAIN RESULT,0.008817635270541082,"The standard rooted d-ary tree with depth ℓis a tree where each vertex u /∈L has exactly d children
vertices. Let us start by defining the type of trees we will be investigating in this paper, which is a
slight generalization of d-ary tree."
DEFINITIONS AND MAIN RESULT,0.009218436873747494,"Figure 1: An example of a binary rooted tree of depth 5 is shown. The vertex u is at the 3rd layer
and h(u) = 2. Further, the following relationships hold: v < u and v is a child of u, s ∈L is a 2nd
descendant of u, w is the parent of u, and t is the 2nd ancestor of u."
DEFINITIONS AND MAIN RESULT,0.009619238476953907,"Definition 1.1. A rooted tree T with root ρ has degree dominated by d ≥1 with parameter R ≥1 if
for every vertex u and positive integer k, the number of kth descendants of u is at most Rdk."
DEFINITIONS AND MAIN RESULT,0.01002004008016032,"With the above definition, a d-ary rooted tree has degree dominated by d ≥1 with parameter R = 1.
Further, a typical realization of a Galton-Watson tree of Poisson type with average degree d and of
depth ℓ(a random tree in which each vertex u /∈L has on average d children vertices) has a degree
dominated by d ≥1 with parameter R ≃log(ℓ)."
DEFINITIONS AND MAIN RESULT,0.010420841683366733,Broadcasting Process on Rooted Trees
DEFINITIONS AND MAIN RESULT,0.010821643286573146,"Next, we will define the broadcasting process on a rooted tree T with root ρ. Consider a q × q ergodic
transition matrix M, where q ≥2. Recall that every eigenvalue of a transition matrix M has an
absolute value at most 1. Let 0 ≤λ ≤1 represent the second largest absolute value among the
eigenvalues of M. Additionally, we define the stationary distribution of M as π."
DEFINITIONS AND MAIN RESULT,0.011222444889779559,"The broadcasting process X = (Xv)v∈T , with state space [q] := {1, 2, . . . , q} and transition matrix
M, can be formally described as follows: We initialize the value of Xρ according to some initial
distribution ν. As we reveal the values layer by layer, when the value Xu is revealed, the value Xv
for any child vertex v of u is independently distributed according to a specific row of M depending
on the value of Xu:"
DEFINITIONS AND MAIN RESULT,0.011623246492985972,P{Xv = t | Xu = s} = Mst.
DEFINITIONS AND MAIN RESULT,0.012024048096192385,"In other words, each vertex’s value depends only on its parent vertex’s value. The definition of the
process is given below:
Definition 1.2 (Broadcasting Process on Tree). Let q ≥2 be a positive integer. For any rooted tree T
with root ρ and a q × q ergodic transition matrix M, the broadcasting process X = (Xv)v∈T with
state space [q], according to transition matrix M with an initial distribution ν for Xρ, is a random
process with joint distribution given by:"
DEFINITIONS AND MAIN RESULT,0.012424849699398798,"∀x = (xv)v∈T ∈[q]T ,
P{X = x} = ν(xρ)
Y"
DEFINITIONS AND MAIN RESULT,0.01282565130260521,"(v,u)
Mxu,xv,"
DEFINITIONS AND MAIN RESULT,0.013226452905811623,"where the product is taken over all pairs (v, u) such that v is a child vertex of u."
DEFINITIONS AND MAIN RESULT,0.013627254509018036,"For a subset of vertices A ⊂T, let us denote"
DEFINITIONS AND MAIN RESULT,0.014028056112224449,XA = (Xv)v∈A .
DEFINITIONS AND MAIN RESULT,0.014428857715430862,"If the tree is just a path, then the process reduces to a Markov chain. If we assume ν = π, then
Xv ∼π for every v ∈T, as (Xv)v∈P for every downward path of T forms a Markov Chain with
transition matrix M. Further, let us make a remark about the Markov property of the process.
Remark 1.3 (Markov Property). The broadcasting process establishes a Markov Random Field
on tree T: Given any three disjoint subsets A, B, and C of T, if every path from a vertex in A to a
vertex in C passes through a vertex in B, then the random variables XA and XC are conditionally
independent given XB."
DEFINITIONS AND MAIN RESULT,0.014829659318637275,Polynomials of xL and the Main Result
DEFINITIONS AND MAIN RESULT,0.015230460921843688,"Definition 1.4. Let x ∈[q]T . For u ∈T, let x≤u = (xv)v≤u. For subset U ⊆T, let xU = (xu)u∈U."
DEFINITIONS AND MAIN RESULT,0.0156312625250501,"The next definition is about the notion of degrees for functions with variables xL = (xv)v∈L. This is
the generalization of degree of a polynomial.
Definition 1.5 (Efron-Stein Degree). A function f with variables xL has Efron-Stein degree at most
k if it can be expressed as
f =
X α
fα,"
DEFINITIONS AND MAIN RESULT,0.01603206412825651,"where the summation is over a finite set of indices α, and each fα is a function of the variable xS for
some S ⊆L with |S| ≤k."
DEFINITIONS AND MAIN RESULT,0.016432865731462926,"Now, we could properly formulate the main result of the paper:
Theorem 1.6. Let T be a rooted tree with root ρ of depth ℓand has degree dominated by d ≥1 with
parameter R ≥1. Consider the broadcasting process on T with a q × q transition matrix M and
Xρ ∼π. If M is ergodic and dλ2 < 1, then there exists a constant c > 0 which depends on M and
dλ2 such that the following holds: For any function f(xL) of Efron-Stein degree ≤c
ℓ
1+log(R), we
have"
DEFINITIONS AND MAIN RESULT,0.016833667334669337,"Var(E

f(XL)
 Xρ

) ≤(max{dλ2, λ})ℓ/4Var(f(XL))."
DEFINITIONS AND MAIN RESULT,0.017234468937875752,"Remark 1.7. Given that dλ2 < 1 implies λ2 < 1, and λ < 1 if and only if λ2 < 1, we can infer
that λ < 1 from the given conditions. Consequently, the term max{dλ2, λ} < 1 follows from the
assumptions of the theorem. Therefore, the R.H.S. of the inequality decays exponentially with the
depth of the tree."
DEFINITIONS AND MAIN RESULT,0.017635270541082163,"Follows from the theorem and propteries of conditonal expectation, we have the following corollary."
DEFINITIONS AND MAIN RESULT,0.018036072144288578,"Corollary 1.8. With the same setting as in Theorem 1.6, for any function f(xL) of Efron-Stein degree
≤c
ℓ
1+log(R), and any function g(xρ) of the root value, their correlation satisfies"
DEFINITIONS AND MAIN RESULT,0.01843687374749499,"Corr(f(XL), g(Xρ)) := E

(f(XL) −Ef(XL))(g(Xρ) −Eg(Xρ))
 p"
DEFINITIONS AND MAIN RESULT,0.018837675350701404,"Var(f(XL)
p"
DEFINITIONS AND MAIN RESULT,0.019238476953907815,"Var(g(Xρ))
≤(max{dλ2, λ})ℓ/8."
DEFINITIONS AND MAIN RESULT,0.01963927855711423,"The proof of the theorem is based on recursion on a notion of fractal capacity of functions. Indeed,
the main result is optimal in the fractal sense (Theorem 1.16), as we will later demonstrate that all
functions with fractal capacity up to a level proportional to ℓexhibit vanishing correlation with the
root, whereas all functions of the leaves have fractal capacity at most ℓ+ 1. Let us introduce the
necessary definitions and notations to introduce both the fractal capacity and the proof overview."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.02004008016032064,"1.2
Fractal Capacity and Proof Overview"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.020440881763527055,"To provide a clearer illustration, we establish a correspondence between the vertices of T and words
of varying lengths from 0 to ℓ, with vertices at the kth layer represented as words of length k. We"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.020841683366733466,"denote the root ρ as the empty word (). For each vertex u, represented by the word (b1, b2, . . . , bk),
we define du as the number of children vertices of u, and we identify these children vertices as
(b1, b2, . . . , bk, i) with i ∈[du] := {1, 2, . . . , du}. Notice that v is a descendant of u is equivalent to
u is a prefix of v. For brevity, for each u = (b1, . . . , bk) ∈T and i ∈[du], let"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.02124248496993988,"ui := (u, i) = (b1, . . . , bk, i)."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.021643286573146292,"For I ⊆[du], let"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.022044088176352707,uI = {ui}i∈I.
FRACTAL CAPACITY AND PROOF OVERVIEW,0.022444889779559118,"Furthermore, we denote the parent vertex of u as p(u) = (b1, . . . , bk−1) and the set of children
vertices of u as c(u) = u[du]."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.022845691382765532,"Figure 2: In the these figures, we present the vertices as words and adapt the notations u1, u2,etc.,
for the descendants of u. For the right figure, if S = {u21, u22, u32}, then ρ(S) = u, I(S) = {2, 3},
and S2 = {u21, u22}, S3 = {u32}. Further, S1 ∈A3, S2 ∈A2, and S3 ∈A1."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.023246492985971944,"Definition 1.9. For a non-empty subset S ⊆L, we introduce the notation ρ(S) to represent the
nearest common ancestor of the vertices in S, meaning that ρ(S) is the vertex with smallest height
that is an ancestor of all vertices in S."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.023647294589178358,"Here we consider the case when |S| > 1. Notice that ρ(S) is not at the ℓth layer L. For each child
ρ(S)i of ρ(S) for i ∈[dρ(S)], we define the set Si as"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.02404809619238477,"Si = S ∩{v ∈L : v ≤ρ(S)i},"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.024448897795591184,which is the collection of vertices in S that are descendants of ρ(S)i. Let
FRACTAL CAPACITY AND PROOF OVERVIEW,0.024849699398797595,I(S) = {i ∈[dρ(S)] : Si ̸= ∅}.
FRACTAL CAPACITY AND PROOF OVERVIEW,0.02525050100200401,"Then, S can be expressed as the disjoint union of the sets Si for i ∈I(S):"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.02565130260521042,S = ⊔i∈I(S)Si.
FRACTAL CAPACITY AND PROOF OVERVIEW,0.026052104208416832,"We call the above disjoint union the branch decomposition of S, and each Si for i ∈I(S) a branch
part of S."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.026452905811623247,"The branch decomposition is a key concept in the proof, and we define the fractal capacity according
to the number of iterations to decompose S into singletons."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.026853707414829658,"Definition 1.10. Let
A1 :=
n
{u} : u ∈L
o
⊆2L\{∅},"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.027254509018036072,"be the collection of singletons of L. We say a subcollection A ⊆2L\{∅} is closed under decompo-
sition with base A1 if for every S ∈A\A1, we have Si ∈A for i ∈I(S)."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.027655310621242483,"Definition 1.11. For any A ⊆2L\{∅}, let"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.028056112224448898,B(A) ⊆2L\{∅}
FRACTAL CAPACITY AND PROOF OVERVIEW,0.02845691382765531,be a new subcollection defined according to the following rules:
FRACTAL CAPACITY AND PROOF OVERVIEW,0.028857715430861724,"For any S ∈2L\{∅}, S ∈B if and only if one of the following two conditions holds"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.029258517034068135,1. S ∈A1.
FRACTAL CAPACITY AND PROOF OVERVIEW,0.02965931863727455,2. S /∈A1 and Si ∈A for i ∈I(S).
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03006012024048096,"For example, B(A) contains sets of size ≤2.
Lemma 1.12. If A1 ⊆A ⊆2L\{∅} is a subcollection closed under decomposition with base A1 ,
then the collection B = B(A) contains A and it is also closed under decomposition with base A1."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.030460921843687375,"Proof. To show A ⊆B, it is sufficient to show A\A1 ⊆B. For any S ∈A\A1, because A is closed
under decomposition, Si ∈A for i ∈I(S). Hence, S ∈B follows from the definition of B. Now, for
S ∈B\A1, each Si with i ∈I(S) is contained in A ⊆B, which in turn implies B is closed under
decomposition."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.030861723446893786,"Now, we define recursively that"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.0312625250501002,"Ak = B(Ak−1),
(2)"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.031663326653306616,"for positive integer k ≥2. Observe the following two facts: Consider any non-singleton set S ⊆L
and Si with i ∈I(S). First, S ∈Ak ⇒Si ∈Ak−1 by the definition of Ak. Second, ρ(S) > ρ(Si)
by the definition of branch decomposition. Given these two facts, we can prove inductively that"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03206412825651302,h(ρ(S)) = k ⇒S ∈Ak+1.
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03246492985971944,"Since that there are only ℓlayers of the tree, we conclude that every non-emptyset of S ⊆L is in
Aℓ+1. Therefore, together with Lemma 1.12, we have the following chain of subcollections:"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03286573146292585,{{u} : u ∈L} = A1 ⊆A2 ⊆· · · ⊆Aℓ+1 = 2L\{∅}.
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03326653306613227,"Definition 1.13 (Fractal Capacity). For any non-empty subset S ⊆L, we define the fractal capacity
of S as the smallest k such that S ∈Ak."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.033667334669338675,"We introduce the notion of fractal capacity, borrowing terminology from fractal geometry. The
recursive nature of our definition on subsets of trees mirrors the self-similar complexity found in
fractal structures. This recursive and inherently intricate structure motivates our choice of the term
fractal capacity, capturing the fractal-like properties that emerge in the collections (Ak)k∈[l+1]."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03406813627254509,"Definition 1.14. Given a collection A ⊆2L\{∅}. A function f : [q]T →R is called an A-
polynomial if we can express
f(x) =
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.034468937875751504,"S∈A
fS(xS)"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03486973947895792,"where each fS is a function of xS = (xv)v∈S. A function f : [q]T →R has fractal capacity ≤k if
it is a Ak-polynomial.
Remark 1.15. It is not hard to verify that Ak contains all non-empty subsets S ⊆L with |S| ≤k.
Thus, for any function f with variables xL,"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.035270541082164326,Efron-Stein degree of f ≤k ⇒f is an Ak-polynomial.
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03567134268537074,"On the other hand, it is worth to remark that for the d-ary tree of depth ℓ≥k, there exists S ∈Ak
with |S| = dk−1. (Namely, taking S = {v ∈L : v < u} for some u with h(u) = k −1.) Thus, an
Ak-polynomial could have an Efron-Stein degree exponential in k."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.036072144288577156,"The main result of the paper in terms of the fractal capacity is the following:
Theorem 1.16. With the same setting as in Theorem 1.6, there exists a constant c > 0 which
depends on M and dλ2 such that the following holds: For any function f(xL) with fractal capacity
≤c
ℓ
1+log(R), we have"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03647294589178357,"Var(E

f(XL)
 Xρ

) ≤(max{dλ2, λ})ℓ/4Var(f(XL))."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03687374749498998,"Indeed, Theorem 1.6 is a direct consequence of Theorem 1.16, as Ak-polynomials contains all
polynomials of Efron-Stein degree ≤k. Further, in terms of fractal capacity, the theorem is optimal
because the correlation decay persists up to an order proportional to ℓ, while a fractal capacity ≤ℓ
includes all functions of the leaves."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03727454909819639,"Overview of the Proof Idea: For illustration, let us consider the case where T is a binary tree of"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03767535070140281,"depth ℓwith M =
 1+λ 2
1−λ 2
1−λ 2
1+λ 2"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03807615230460922,"
, such matrix has eigenvalues λ and 1. Here we assume 2λ2 < 1."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03847695390781563,"We recall that for this binary symmetric broadcasting process, it is information-theoretically impossi-
ble to recover the root label from the leaves below the KS bound. This implies that all polynomials
of XL have vanishing correlation with Xρ. Still, we use this simple process to illustrate the proof
idea as our arguments for low-degree polynomials generalize to general broadcasting processes
below the Kesten-Stigum threshold, including cases where it is information theoretically possible
to estimate the root from the leaves non-trivially (in such cases there exist functions f so that
lim infℓ→∞Corr(f(XL), Xρ) > 0)."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.038877755511022044,"Now, let us consider degree-1 polynomials. Suppose f is a A1-polynomial (equivalently, of Efron-
Stein degree 1), we can express it in the form"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.03927855711422846,"f(xL) =
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.039679358717434866,"u∈L
fu(xu),"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04008016032064128,"where each fu is a function of xu. Given our focus on the variance, we may assume Efu(Xu) = 0
for each u ∈L. Then, our goal is to prove E
 
E[f(XL) | Xρ]
2
is negligible comparing to
E

(f(XL))2
. Following from the Cauchy-Schwarz inequality that (
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.040480961923847696,"i∈[m]
ai)2 = (
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04088176352705411,"i∈[m]
1 · ai)2 ≤m
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04128256513026052,"i∈[k]
a2
i ,"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04168336673346693,we have
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04208416833667335,"E
 
E[f(XL) | Xρ]
2
≤|L|
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04248496993987976,"u∈L
E
 
E[fu(Xu) | Xρ]
2
(3) ≲2ℓX"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04288577154308617,"u∈L
λ2ℓE

(fu(Xu))2
= (2λ2)ℓX"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.043286573146292584,"u∈L
E

(fu(Xu))2
,"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.043687374749499,"where the second inequality is derived from the variance decay property of in a Markov Chain. Thus,
if we can establish P"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04408817635270541,"u∈L E

(fu(Xu))2
is at the same order as E

(f(XL))2
, the proof is complete."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04448897795591182,Notice that
FRACTAL CAPACITY AND PROOF OVERVIEW,0.044889779559118236,"E

(f(XL))2
=
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04529058116232465,"u,v∈L
E[fu(Xu)fv(Xv)] =
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.045691382765531065,"u∈L
E

(fu(Xu))2
+
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04609218436873747,"u̸=v∈L
E[fu(Xu)fv(Xv)]."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04649298597194389,"Thus, the goal here is to show

X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.0468937875751503,"u̸=v∈L
E[fu(Xu)fv(Xv)]
 < c
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.047294589178356716,"u∈L
E

(fu(Xu))2
,
(4)"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.047695390781563124,"for some constant c ∈(0, 1), which in turn implies the desired result:"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04809619238476954,"E
 
E[f(XL) | Xρ]
2
≲(2λ2)ℓX"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04849699398797595,"u∈L
E

(fu(Xu))2
≲(2λ2)ℓ
1
1 −cE

(f(XL))2
."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04889779559118237,"Roughly speaking, (4) holds if for most pairs u and v within L, the correlation between fu(Xu) and
fv(Xv) is sufficiently small, which is the case for degree-1 polynomials."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.049298597194388775,"Now, let us take a closer look. Fix any two vertices u and v in L, with w as their nearest common
ancestor. Suppose u ≤w1 and v ≤w2. Let X̸≤w1 = (Xu′)u′̸≤w1. We have
E[fu(Xu)fv(Xv)]
 =|E[E[fu(Xu) | X̸≤w1]fv(Xv)]

(5) ≤
q"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.04969939879759519,"E[(E[fu(Xu) | X̸≤w1])2] ·
p"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.050100200400801605,E[(fv(Xv))2]
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05050100200400802,≲λh(w)p
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05090180360721443,"E[(fu(Xu))2]
p"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05130260521042084,"E[(fv(Xv))2],"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.051703406813627256,"where the last inequality follows from the variance decay of the Markov Chain for length h(w). The
above inequality implies that the correlation between fu(Xu) and fv(Xv) is at most of order λh(w)."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.052104208416833664,"The above bound can be improved to λ2k by also taking the conditional expectation E[fv(Xv) | X̸≤w2]
into account, which requires the Markov Property that Xu and Xv are independent conditioned on
Xw. From here, one can properly arrange the terms and apply Cauchy-Schwarz inequality to show
(4) holds."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05250501002004008,"Our proof of the main theorem tries to generalize the argument above to low degree polynomials. Let
us summarize it by the following five pieces of descriptions."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05290581162324649,"I. Bounding Covariance: First, we generalized the idea on how (5) works for degree-1 polynomials.
Suppose fα and fβ are two functions so that the following holds."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05330661322645291,1. fα(xS) is a function of xS for some set S ⊆L such that
FRACTAL CAPACITY AND PROOF OVERVIEW,0.053707414829659315,"E[(E[fα(XS) | X̸≤w′])2] ≪(E(fα(XS))2,"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05410821643286573,"where we use a ≪b to indicate a is much smaller than b. We keep this not precise to avoid
technical details, but expect that the ratio is exponentially small in h(w′).
2. fβ(xS′) is a function of xS′ with S′ ⊆L satisfying S′ ∩{v′ : v′ ≤w′} = ∅."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.054509018036072145,"Then, following the same derivation as shown in (5) we have
E[fα(XS)fβ(XS′)]
 =
E

E[fα(XS) | X̸≤w′]fβ(XS′)
 ≪
p"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05490981963927856,"E[(fα(XS))2]
q"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05531062124248497,E[(fβ(XS′))2].
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05571142284569138,(See Figure 3 for an illustration.)
FRACTAL CAPACITY AND PROOF OVERVIEW,0.056112224448897796,"Figure 3: In the left figure, the purple dots represent the corresponding input variables for fα, and
the yellow dots represent the corresponding input variables for fβ. In the right figure, the purple dots
represent the corresponding input variables for E[fα(X) | X̸≤w1] and the variables do not involve xv
for vertex v not illustrated due to the Markov property."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05651302605210421,"II. Choosing a good decomposition of the function: In essence, our proof strategy for any given
function f(xL) with Ef(XL) = 0 revolves around decomposing f(xL) into a sum of functions
fα(xL) for α in some index set I, such that"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05691382765531062,"1. |I| ≲2ℓ,
2. For each α, Efα(XL) = 0 and E[(E[fα(XL) | Xρ]2] ≪E[(fα(XL))2],
3. Whenever α ̸= β, we can find w ∈T so that fα and fβ satisfy the covariance bound in I.
(Possibly with a switch of the roles of α and β)."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05731462925851703,"Let us remark that while we are writing fα(xL), it does not mean that fα depends on every leave
variables."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05771543086172345,"If this is the case, then we could follow the argument in the degree 1 case to show that desired result
holds."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05811623246492986,"III. From Ak polynomials to Ak+1 polynomials: The proof of the main theorem builds on I and II
and advancing through a recursion on the fractal capacity of the function. This recursive approach
relies on the following property:"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.05851703406813627,"Suppose we have shown the second moment decay Ak-polynomials with mean 0 in the following
sense: For every Ak-polynomial f(xS) with mean 0 and variable xS where S ⊆{v ∈L : v ≤ρ′}
for some vertex ρ′,"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.058917835671342685,"E[(E[f(XL) | Xρ′])2] ≤(2λ2)h(ρ′)−hAk E[(f(XL))2],
(6)"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.0593186372745491,"where hAk is some penalty constant depending on Ak. (The bigger the value hAk, the weaker the
second moment decay.)"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.059719438877755514,"Consider a specific type of Ak+1-polynomials. Fix a pivot vertex w with h(w) large enough, let
x≤wi = (xv)v≤wi for i ∈[2]. Let g be a function of the form"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06012024048096192,"g(x) = g(x≤w1, x≤w2) =
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.060521042084168336,"α∈J
gα,1(x≤w1) · gα,2(x≤w2),"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06092184368737475,"where J is a finite index set and every gα,i is an Ak-polynomial whose variables are x≤wi (more
precisely, its variables are xS′ where S′ ⊆{v ∈L : v ≤wi}) and Egα,i(X≤wi) = 0. Then, the
function g(x) is a Ak+1-polynomial with variable xS for S ⊆{v ∈L : v ≤w}."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.061322645290581165,"Observe that, if we fix x≤w2, then x≤w1 7→g(x≤w1, x≤w2) is an Ak-polynomial with variable xS′
for S′ ⊆{v ∈L : v ≤w1} and mean 0. This allows us to apply the assumption for Ak polynomials
to show"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06172344689378757,"E[(E[g(X≤w1, x≤w2) | X̸≤w1])2] ≲(dλ2)h(w)−hAk E[(g(X≤w1, x≤w2))2]."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06212424849699399,"Clearly, the same inequality holds with the roles of x≤w1 and x≤w2 switched."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.0625250501002004,"Base on this, one key step in the paper is to show g satisfies"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06292585170340681,"E[(E[g(X≤w1, X≤w2) | X̸≤wi])2] ≲(dλ2)h(w)−hAk E[(g(X≤w1, X≤w2))2] for i ∈[2].
(7)"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06332665330661323,"This inequality is immediate if X≤w1 and X≤w2 are independent, which is not the case in the
broadcasting process. One of the main technical challenges in the proof is to show that the inequality
holds when X≤w1 and X≤w2 are conditionally independent given Xw by the Markov Property. It
turns out to impose a significant technical challenge when some entries of M can be 0."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06372745490981964,"Observe that (7) not only implies g satisfies the desired second moment decay for II(2), but also I(1)
with w′ to be either w1 or w2. Indeed, these two properties will also hold for ˜g(x) := g(x) −Eg(X),
the normalized g with mean 0, due to (Eg(X))2 is negligible comparing to Eg(X)2."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06412825651302605,IV. A closer look at the decomposition:
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06452905811623247,"Consider any Ak+1-polynomial f of variable xS for S ⊆{v ∈L : v ≤ρ′} for some vertex ρ′.
Before we proceed to the discussion of the decomposition, we remark that one cannot simply express
f in the form of ˜g described above with any pivot vertex w."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06492985971943888,"Let us give an example to illustrate why: Consider two functions f1, f2, where fi is a function of
xS′ with S′ ⊆{v ∈L : v ≤ρ′
i} and S′ ∈Ak+1\Ak. Let f = f1 + f2. Then one can justify that f
cannot be expressed in the form of ˜g with any pivot w by using the property ""if we fix x≤w2, then
xx≤w1 7→g(x≤w1, x≤w2) is a Ak-polynomial"" discucssed in III to derive a contradiction."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06533066132264528,"The way we decompose f is to express it as a sum of functions of the form ˜g(x) in III. While the
actual decomposition requires a bit more adjustment, it follows from the idea to decompose f in the
form
f =
X"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.0657314629258517,"w∈I
fw,"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06613226452905811,"where the index set I is the set of vertices of T with height slightly greater than hAk (to ensure
correlation decay). Each fw is a function with variable xS for S ⊆{u ∈L : u ≤w} satisfies the
description of ˜g(x) in III."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06653306613226453,Observe that this decomposition satisfies the description in II:
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06693386773547094,• The size of I is bounded by 2ℓ+ 2ℓ−1 + · · · 1 ≤2ℓ+1.
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06733466933867735,• The second moment decay property of fw follows from III.
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06773547094188377,"• Finally, consider u, v ∈I. If v < u, say v ≤u1, then fu and fv satisies the covariance
bound condition stated in I with fα = fu, fβ = f2, and w′ = u2. The case when
u < v is similar. When u and v are not comparable, then following the Markov Property,
E[fu(X)fv(X)] = EE[fu(X) | X̸<w]E[fv(X) | X̸<w] with w being the nearest common
ancestor of u and v and X̸<w = (Xw′)w′̸<w, which makes it easier to show the covariance
bound. (See Figure 4 for an illustration.)"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06813627254509018,"Figure 4: In both figures, the purple dots represent the corresponding input variables for fu, and the
yellow dots represent the corresponding input variables for fv. In the left figure, we have v ≤u1 < u.
In the right figure, we have u and v are incomparable and w is the nearest common ancestor."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06853707414829659,"With this desirable decomposition, one could try to apply some argument similar to the degree-1 case
to show the second moment decay (6) for mean 0 Ak+1-polynomials with a slightly bigger penalty
constant hAk+1 than hAk."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06893787575150301,"V. Overview on the induction Given the decomposition of f as described in IV, together with the
second moment assumption (6) on Ak-polynomials described in III, the proof of the main theorem
proceeds by induction. The goal is to show that the penalty constant hAk associated with Ak, which
appeared (6), satisfies the following recursive inequality:"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06933867735470942,"hAk+1 ≤hAk + C, and hA1 ≤C"
FRACTAL CAPACITY AND PROOF OVERVIEW,0.06973947895791584,"for some constant C depending on M and dλ2. If true, by taking k to be proportional to ℓ/C, then
the theorem follows. The proof of the theorem requires a careful analysis of the covariance and
variance decay to demonstrate that it resembles the behavior observed in the degree-1 case, in order
to capture the Kesten-Stigum bound."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.07014028056112225,"Comparison to other work in low-degree polynomials While some high-level ideas align with
previous work in low degree polynomials mentioned fore, our approach focuses on establishing low-
degree lower bounds in a setting where direct comparisons are challenging due to structural differences.
Specifically, our work addresses the broadcasting process on trees, where the underlying structures
are highly correlated and do not naturally lend themselves to a product measure representation,
presenting unique technical challenges not encountered in more independent setups."
FRACTAL CAPACITY AND PROOF OVERVIEW,0.07054108216432865,Acknowledgments
FRACTAL CAPACITY AND PROOF OVERVIEW,0.07094188376753507,"Han Huang was supported by Elchanan Mossel’s Vannevar Bush Faculty Fellowship ONR-N00014-
20-1-2826 and by Elchanan Mossel’s Simons Investigator award (622132). Elchanan Mossel was
partially supported by Bush Faculty Fellowship ONR-N00014-20-1-2826, Simons Investigator award
(622132), ARO MURI W911NF1910217 and NSF award CCF 1918421."
REFERENCES,0.07134268537074148,References
REFERENCES,0.07174348697394789,"[1] Emmanuel Abbe. Community detection and stochastic block models: recent developments. The
Journal of Machine Learning Research, 18(1):6446–6531, 2017."
REFERENCES,0.07214428857715431,"[2] Emmanuel Abbe and Colin Sandon. Proof of the achievability conjectures for the general
stochastic block model. Communications on Pure and Applied Mathematics, 71(7):1334–1406,
2018."
REFERENCES,0.07254509018036072,"[3] Afonso Bandeira, Amelia Perry, and Alexander S. Wein. Notes on computational-to-statistical
gaps: predictions using statistical physics. Port. Math., 75(2), 2018."
REFERENCES,0.07294589178356714,"[4] Afonso S Bandeira, Dmitriy Kunisky, and Alexander S Wein. Computational hardness of
certifying bounds on constrained pca problems. In ITCS, 2020."
REFERENCES,0.07334669338677355,"[5] Jess Banks, Cristopher Moore, Joe Neeman, and Praneeth Netrapalli. Information-theoretic
thresholds for community detection in sparse networks. In Conference on Learning Theory,
pages 383–416. PMLR, 2016."
REFERENCES,0.07374749498997996,"[6] P. M. Bleher, J. Ruiz, and V. A. Zagrebnov. On the purity of the limiting Gibbs state for the
Ising model on the Bethe lattice. J. Statist. Phys., 79(1-2):473–482, 1995."
REFERENCES,0.07414829659318638,"[7] Charles Bordenave, Marc Lelarge, and Laurent Massoulié. Non-backtracking spectrum of
random graphs: community detection and non-regular ramanujan graphs. In Foundations of
Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 1347–1357. IEEE,
2015."
REFERENCES,0.07454909819639279,"[8] C. Borgs, J. Chayes, E. Mossel, and S. Roch. The kesten-stigum reconstruction bound is tight
for roughly symmetric binary channels. In Proceedings of IEEE FOCS 2006, pages 518–530,
2006."
REFERENCES,0.07494989979959919,"[9] Guy Bresler and Brice Huang. The algorithmic phase transition of random k-sat for low degree
polynomials. In FOCS, 2021."
REFERENCES,0.07535070140280561,"[10] Wei-Kuo Chen, David Gamarnik, Dmitry Panchenko, and Mustazee Rahman. Suboptimality of
local algorithms for a class of max-cut problems. The Annals of Probability, 47(3):1587–1618,
2019."
REFERENCES,0.07575150300601202,"[11] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborová. Asymptotic analysis of the stochas-
tic block model for modular networks and its algorithmic applications. Physics Review E,
84:066106, Dec 2011."
REFERENCES,0.07615230460921844,"[12] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborová. Asymptotic
analysis of the stochastic block model for modular networks and its algorithmic applications.
Physical Review E, 84(6):066106, 2011."
REFERENCES,0.07655310621242485,"[13] Yash Deshpande and Andrea Montanari. Finding hidden cliques of size
p"
REFERENCES,0.07695390781563126,"N/e in nearly linear
time. Foundations of Computational Mathematics, 15(4):1069–1128, 2015."
REFERENCES,0.07735470941883768,"[14] W. S. Evans, C. Kenyon, Yuval Y. Peres, and L. J. Schulman. Broadcasting on trees and the
Ising model. Ann. Appl. Probab., 10(2):410–433, 2000."
REFERENCES,0.07775551102204409,"[15] David Gamarnik, Aukosh Jagannath, and Alexander S Wein. Low-degree hardness of random
optimization problems. In 2020 IEEE 61st Annual Symposium on Foundations of Computer
Science (FOCS), pages 131–140. IEEE, 2020."
REFERENCES,0.0781563126252505,"[16] David Gamarnik and Madhu Sudan. Limits of local algorithms over sparse random graphs.
In Proceedings of the 5th conference on Innovations in theoretical computer science, pages
369–376, 2014."
REFERENCES,0.07855711422845692,"[17] Justin Holmgren and Alexander S Wein. Counterexamples to the low-degree conjecture. In
ITCS, 2020."
REFERENCES,0.07895791583166333,"[18] Samuel Hopkins. Statistical inference and the sum of squares method. PhD thesis, Cornell
University, 2018."
REFERENCES,0.07935871743486973,"[19] Samuel Hopkins and David Steurer. Efficient bayesian estimation from few samples: community
detection and related problems. In 2017 IEEE 58th Annual Symposium on Foundations of
Computer Science (FOCS), pages 379–390. IEEE, 2017."
REFERENCES,0.07975951903807615,"[20] D. Ioffe. Extremality of the disordered state for the Ising model on general trees. In Trees
(Versailles, 1995), volume 40 of Progr. Probab., pages 3–14. Birkhäuser, Basel, 1996."
REFERENCES,0.08016032064128256,"[21] D. Ioffe. On the extremality of the disordered state for the Ising model on the Bethe lattice. Lett.
Math. Phys., 37(2):137–143, 1996."
REFERENCES,0.08056112224448898,"[22] Harry Kesten and Bernt P Stigum. Additional limit theorems for indecomposable multidi-
mensional galton-watson processes. The Annals of Mathematical Statistics, 37(6):1463–1481,
1966."
REFERENCES,0.08096192384769539,"[23] Frederic Koehler and Elchanan Mossel. Reconstruction on trees and low-degree polynomials.
Advances in Neural Information Processing Systems, 35:18942–18954, 2022."
REFERENCES,0.0813627254509018,"[24] D. Kunisky, A.S. Wein, and A.S. Bandeira. Notes on computational hardness of hypothesis test-
ing: Predictions using the low-degree likelihood ratio. Mathematical Analysis, its Applications
and Computation. ISAAC 2019. Springer Proceedings in Mathematics and Statistics, 385, 2022."
REFERENCES,0.08176352705410822,"[25] Dmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira. Notes on computational hard-
ness of hypothesis testing: Predictions using the low-degree likelihood ratio. arXiv preprint
arXiv:1907.11636, 2019."
REFERENCES,0.08216432865731463,"[26] Cheng Mao and Alexander S Wein. Optimal spectral recovery of a planted vector in a subspace.
arXiv preprint arXiv:2105.15081, 2021."
REFERENCES,0.08256513026052104,"[27] M. Mézard and A. Montanari. Reconstruction on trees and the spin glass transition. Journal of
Statistical Physics, 124:1317–1350, 2006."
REFERENCES,0.08296593186372746,"[28] Sidhanth Mohanty, Siqi Liu, and Prasad Raghavendra. On statistical inference when fixed
points of belief propagation are unstable. In IEEE 62st Annual Symposium on Foundations of
Computer Science (FOCS), 2021."
REFERENCES,0.08336673346693386,"[29] Andrea Montanari and Alexander S. Wein. Equivalence of approximate message passing and
low-degree polynomials in rank-one matrix estimation. Probability Theory and Related Fields,
2024."
REFERENCES,0.08376753507014029,"[30] E. Mossel. Reconstruction on trees: beating the second eigenvalue. Ann. Appl. Probab.,
11(1):285–300, 2001."
REFERENCES,0.0841683366733467,"[31] E. Mossel. Phase transitions in phylogeny. Trans. Amer. Math. Soc., 356(6):2379–2404
(electronic), 2004."
REFERENCES,0.0845691382765531,"[32] E. Mossel. Survey: Information flow on trees. In J. Nestril and P. Winkler, editors, Graphs,
Morphisms and Statistical Physics. DIMACS series in discrete mathematics and theoretical
computer science, pages 155–170. 2004."
REFERENCES,0.08496993987975952,"[33] E. Mossel, J. Neeman, and A. Sly. Reconstruction and estimation in the planted partition model.
Probability Theory and Related Fields, (3-4):431–461, 2015. The Arxiv version of this paper is
titled Stochastic Block Models and Reconstruction."
REFERENCES,0.08537074148296593,"[34] Elchanan Mossel. Combinatorial statistics and the sciences. In International Congress of
Mathematicians: 2022 July 6-14, volume 6, chapter 5553, pages 1–20. 2023."
REFERENCES,0.08577154308617234,"[35] Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.
Combinatorica, 38(3):665–708, 2018."
REFERENCES,0.08617234468937876,"[36] Elchanan Mossel and Yuval Peres. Information flow on trees. The Annals of Applied Probability,
13(3):817–844, 2003."
REFERENCES,0.08657314629258517,"[37] Elchanan Mossel, Allan Sly, and Youngtak Sohn. Exact phase transitions for stochastic block
models and reconstruction on trees. In STOC 2023: Proceedings of the 55th Annual ACM
Symposium on Theory of Computing, pages 96–102, 2023."
REFERENCES,0.08697394789579159,"[38] Federico Ricci-Tersenghi, Guilhem Semerjian, and Lenka Zdeborová. Typology of phase
transitions in bayesian inference problems. Physical Review E, 99(4):042109, 2019."
REFERENCES,0.087374749498998,"[39] A. Sly. Reconstruction of random colourings. Comm. Math. Phys., 288, 2009."
REFERENCES,0.0877755511022044,"[40] Alexander S Wein. Optimal low-degree hardness of maximum independent set. arXiv preprint
arXiv:2010.06563, 2020."
REFERENCES,0.08817635270541083,"[41] Lenka Zdeborova and Florent Krzakala. Statistical physics of inference: Thresholds and
algorithms. Advances in Physics, 65(5), 2016."
REFERENCES,0.08857715430861723,Appendix
REFERENCES,0.08897795591182364,Overview
REFERENCES,0.08937875751503006,"• In Section A, we give additional notations and basic tools.
• In Section B, we formulate the main theorem we want to prove as an induction statement.
• In Section C, we discuss the case for degree 1 polynomial, which prove the base case of
the induction in the theorem, and the results for degree 1 polynomial will be used in the
inductive step.
• In Section D, we give a procedure to decompose B-polynomial f for a given collection B.
• In Section E and F, we derive the proof of Theorem B.6, the inductive step for proving
Theorem B.1.
• In Section G and H, we derive the main result in the general case.
• In Section I, we provide a proof of Proposition C.3, which is one technical obstacle for
getting our main result from Theorem B.1 to the general setting (Theorem 1.6). It is
postponed to this section due to the proof is essentially a result about Markov Chain.
• In Section J, we provided some standard result for decay of Markov Chain."
REFERENCES,0.08977955911823647,"A
Additional Notations and Tools"
REFERENCES,0.09018036072144289,"Let us begin with the proof of Remark 1.15 which shows Ak contains all non-empty subsets of L of
size ≤k:"
REFERENCES,0.0905811623246493,"Proof of Remark 1.15. The proof will be carried out by induction on k. The base case with k = 1
follows from the definition A1 :=

{v} : v ∈L
	
. Suppose the claim holds up to some positive
integer k. Let ∅̸= S ⊆L of size |S| ≤k + 1. If |S| ≤k, then S ∈Ak ⊆B(Ak) = Ak+1. In the
case |S| = k + 1 ≥2, we first observe that ρ(S) is not a leave. Consider the branch decomposition
of S (See Definition 1.9):
S = ⊔i∈I(S)Si."
REFERENCES,0.09098196392785571,"Because |I(S)| > 1, for each i ∈I(S) we have |Si| < |S| = k + 1. Therefore, Si ∈Ak for
i ∈I(S), which in turn implies S ∈B(Ak) = Ak+1. Therefore, the claim follows."
REFERENCES,0.09138276553106213,"Now, to show the second statement. For every node w, let Sw = {v ∈L : v ≤w}. Observe that if
w is k −1 layers above L, then Sw are the (k −1)th descendants of w, which has size |Sw| = dk−1."
REFERENCES,0.09178356713426854,"We claim that for w which are k −1 layers above L, then Sw ∈Ak. Let us prove the claim by
induction. First, it is clear that for w ∈L, Sw = {w} ∈A1. Suppose the statement holds up to k.
Take any w which is k layer above L. Then, the branch decomposition of Sw = ⊔i∈[d]Swi. With
each wi is k −1 layer above L, we have Swi ∈Ak. Hence, Sw ∈Ak+1 due to Ak+1 = B(Ak)."
REFERENCES,0.09218436873747494,"A.1
Additional notations"
REFERENCES,0.09258517034068137,"For any positive integer n, let [n] denote the set of positive integers from 1 to n, inclusive: [n] =
1, 2, . . . , n. For integers a and b where a < b, let [a, b] = {a, a + 1, . . . , b}."
REFERENCES,0.09298597194388777,Additional Notation for Trees
REFERENCES,0.09338677354709418,"For u ∈T, we define
Lk(u)
to be the set of kth descendants of u. For brevity, let Lk := Lk(ρ). Further, for h ∈[0, h(u)], let"
REFERENCES,0.0937875751503006,"Dh(u) = {v ∈T : v ≤u and h(v) = h},"
REFERENCES,0.09418837675350701,namely the set of descendants of u which has height h. Observe that
REFERENCES,0.09458917835671343,Dk(u) = Lh(u)−k(u).
REFERENCES,0.09498997995991984,"Last, let
Tu
be the induced subgraph of T with vertex set {v ∈T : v ≤u} and Lu = Tu ∩L. It is worth noting
that Tu can be seen as a rooted tree with root u and h(u) layers, and the h(u)th layer is Lu."
REFERENCES,0.09539078156312625,Additional Notation for collection A ⊆2L \ {∅}
REFERENCES,0.09579158316633267,"Definition A.1. For a given collection of of subsets A ⊆2L\{∅}, we define the following subcol-
lections: For each u ∈T, let Au := {S ∈A : ρ(S) = u}, A≤u := {S ∈A : ρ(S) ≤u}, and
A<u := {S ∈A : ρ(S) < u}."
REFERENCES,0.09619238476953908,Notation for Conditional Expectation
REFERENCES,0.09659318637274548,"Definition A.2. Recall that an antichain U ⊆T is a collection of vertices such that no two vertices
in U are comparable under the ≤relation. For x ∈[q]T , we can decompose x in the form"
REFERENCES,0.0969939879759519,"x = (x<U, xU, x̸≤U), where"
REFERENCES,0.09739478957915831,"x<U = (xv : ∃u ∈U s.t. v ≤u), and x̸≤U = (xv : ∀u ∈U, v ̸≤u)."
REFERENCES,0.09779559118236474,(See Figure 5 for an illustration.)
REFERENCES,0.09819639278557114,"Figure 5: In this figure, the purple dots represent the set U. The yellow dots represent the vertices
corresponding to the variables x<U, and the green dots represent the vertices corresponding to the
variables x̸≤U."
REFERENCES,0.09859719438877755,Definition A.3. [Conditional Expectation]
REFERENCES,0.09899799599198397,For each antichain U ⊆T and f : [q]T →R let
REFERENCES,0.09939879759519038,"(EUf)(x) := E
h
f(X)
 XU = xU, X̸≤U = x̸≤U
i
."
REFERENCES,0.09979959919839679,"To rephrase it, the function (EUf)(x) represents the expected value of f(X) condition on Xv = xv
for all vertices v that are not descendents of any u ∈U. In the case when U = {u}, we will abuse
the notation and denote Eu as E{u} for u ∈T. Futher, for any h ∈[0, ℓ], we set"
REFERENCES,0.10020040080160321,"(Ehf)(x) := E
h
f(X)
 ∀v ∈T with h(v) ≥h, Xv = xv
i
."
REFERENCES,0.10060120240480962,"Remark A.4 (Conditional Expectation and the Markov Property). Suppose f is a function of xL and
U an antichain of T. Let
˜L = {v ∈L : ∃u ∈U s.t. v ≤u} and L′ = L \ ˜L."
REFERENCES,0.10100200400801604,"We remark that by the Markov Property,"
REFERENCES,0.10140280561122245,"EUf(x) = EUf(xU, xL′)"
REFERENCES,0.10180360721442885,"is a function of xU and xL′. To see that, by the Markov Property, x<U and x̸≤U are independent
conditioned on xU. Thus,"
REFERENCES,0.10220440881763528,"(EUf)(xU, x̸≤U) =E
h
f(X˜L, XL′)
 XU = xU, X̸≤U = x̸≤U
i"
REFERENCES,0.10260521042084168,"=E
h
f(X˜L, xL′)
 XU = xU
i
,"
REFERENCES,0.10300601202404809,which implies EUf is a function of xU and xL′. (See Figure 6 for an illustration.)
REFERENCES,0.10340681362725451,"Figure 6:
The purple dots represent the set U. In the left figure, the yellow dots represent the
vertices corresponding to the variables of f. In the right figure, the yellow dots represent the vertices
corresponding to the variable of EUf."
REFERENCES,0.10380761523046092,"A.2
Basis Functions on [q]L and some decay properties"
REFERENCES,0.10420841683366733,"The following lemma is a well-known statement from Markov-Chain. Let us formulate it using the
Broadcasting Process. (For completeness, we include a proof of this lemma in Section J.)"
REFERENCES,0.10460921843687375,"Lemma A.5. Suppose M is irreducible and aperiodic, then there exists C = C(M) > 1 so that the
following hold: For any u ∈T and k ∈N so that pk(u) = p ◦p ◦. . . p
|
{z
}"
REFERENCES,0.10501002004008016,k times
REFERENCES,0.10541082164328658,"(u), the kth ancester of u,"
REFERENCES,0.10581162324649299,"exists. For every function a with variable xu,"
REFERENCES,0.1062124248496994,"Var
h
(Epk(u)a)(Xpk(u))
i
≤Ck2qλ2kVar

a(Xu)

(8) and"
REFERENCES,0.10661322645290582,"C−1

max
θ∈[q]"
REFERENCES,0.10701402805611222,"a(θ) −Ea(Xu)

2
≤VarY ∼πa(Y ) ≤C

max
θ∈[q]"
REFERENCES,0.10741482965931863,"a(θ) −Ea(Xu)

2
.
(9)"
REFERENCES,0.10781563126252505,"And from the above two inequalities, adjusting the constant C if necessary, we also have"
REFERENCES,0.10821643286573146,"max
θ∈[q]"
REFERENCES,0.10861723446893788,"(Epk(u)a)(θ) −Ea(Xu)
 ≤Ckqλk max
θ∈[q]"
REFERENCES,0.10901803607214429,"a(θ) −Ea(Xu)
.
(10)"
REFERENCES,0.1094188376753507,"In this paper, we will fix a basis for the space of functions from [q] to R for a Markov chain M."
REFERENCES,0.10981963927855712,"Definition A.6. For a given q × q ergodic and irreducible transition matrix M, we fix a basis
{ϕi}i∈[0,q−1] for the space of functions from [q] to R such that ϕ0 is the constant function 1 and ϕi
for i ∈[q −1] are functions such that"
REFERENCES,0.11022044088176353,"EY ∼πϕi(Y ) = 0
and
EY ∼πϕ2
i (Y ) = 1."
REFERENCES,0.11062124248496993,"Definition A.7. Suppose a given ℓlayer rooted tree T and q × q transition matrix described in
Lemma A.5 have been given. For σ ∈[0, q −1]L, let"
REFERENCES,0.11102204408817636,"S(σ) = {v : σ(v) ̸= 0} ⊆L,"
REFERENCES,0.11142284569138276,"the set of vertices in L in which σ is non-zero. Let |σ| = |S(σ)|. When |σ| ≥1, we define"
REFERENCES,0.11182364729458918,ρ(σ) = ρ(S(σ)).
REFERENCES,0.11222444889779559,"And when |σ| ≥2, we define
I(σ) := I(S(σ))."
REFERENCES,0.112625250501002,"Further, let
ϕσ(x) :=
Y"
REFERENCES,0.11302605210420842,"v∈L
ϕσ(v)(xv) =
Y"
REFERENCES,0.11342685370741483,"v∈S(σ)
ϕσ(v)(xv)."
REFERENCES,0.11382765531062124,"and
˜ϕσ(x) := ϕσ(x) −Eϕσ(X)."
REFERENCES,0.11422845691382766,Remark A.8. We remark that ϕσ(x) is a function with variables xS(σ).
REFERENCES,0.11462925851703407,"The fact that ϕ0, ϕ1, . . . , ϕq−1 forms a basis implies that:"
REFERENCES,0.11503006012024049,Fact A.9. Every function f : [q]U →R can be expressed uniquely in the form
REFERENCES,0.1154308617234469,"f(x) =
X"
REFERENCES,0.1158316633266533,"σ : S(σ)⊆U
cσϕσ(x).
(11)"
REFERENCES,0.11623246492985972,"Remark A.10. With the above representation, the Efron-Stein degree of function f equals to the
largest magnitude of |σ| among those σ such that cσ ̸= 0."
REFERENCES,0.11663326653306613,"Definition A.11. Given a tree T and a q × q ergodic transition matrix M, let {ϕi}i∈[q−1] be the
functions described in Lemma A.5. For a collection of subsets A ⊆2L\{∅}, let"
REFERENCES,0.11703406813627254,"F(A) := {σ ∈[0, q −1]L : S(σ) ∈A}.
(12)"
REFERENCES,0.11743486973947896,"For any σ ∈F(A) with |σ| > 1, let"
REFERENCES,0.11783567134268537,"ψσ(x) :=
Y"
REFERENCES,0.11823647294589178,i∈I(σ)
REFERENCES,0.1186372745490982,"˜ϕPiσ(x),
(13)"
REFERENCES,0.1190380761523046,"where, for each i ∈I(σ), Piσ is the restriction of σ to S(σ)i:"
REFERENCES,0.11943887775551103,(Piσ)(v) = σ(v)1(v ∈S(σ)i) for v ∈L.
REFERENCES,0.11983967935871744,"B
The overall inductive argument"
REFERENCES,0.12024048096192384,"Here we present the version of the theorem with additional assumption on the transition matrix M
that"
REFERENCES,0.12064128256513026,"cM := min
i,j∈[q] Mij > 0.
(14)"
REFERENCES,0.12104208416833667,"Theorem B.1. Given the rooted tree T and the transition matrix M described in Theorem 1.6, and
under the additional assumption that cM = mini,j∈[q] Mij > 0, there exists c > 0 dependent on M
and dλ2 (and implicitly on cM as well) so that the following holds: For any function f of the leaves
with fractal capacity ≤c
ℓ
log(dR),"
REFERENCES,0.12144288577154308,"Var(E

f(XL)
 Xρ

) ≤(max{dλ2, λ})ℓ/4Var(f(XL))."
REFERENCES,0.1218436873747495,"We will first derive the version mentioned above, as it substantially reduces the technical complexity
without compromising the structural integrity of the proof in the general setting where cM might be
0."
REFERENCES,0.12224448897795591,"The proof of Theorem 1.6 will be carried out by induction on Ak-polynomials. Let us introduce the
necessary notations to outline this induction process."
REFERENCES,0.12264529058116233,Definition B.2. Let ε > 0 be the constant such that
REFERENCES,0.12304609218436874,"max{dλ2, λ} = exp(−1.1ε)."
REFERENCES,0.12344689378757515,"The constant ε is introduced to improve the readability of the paper. Intuitively, we aim to define
dλ2 = exp(−ε), but we relax this definition slightly so that inequalities like the following hold when
ℓis sufficiently large:
poly(ℓ)(dλ2)ℓ≤exp(−εℓ)."
REFERENCES,0.12384769539078157,"Assumption B.3. We say that A satisfies assumption B.3 with parameters (h∗, c∗) where h∗> 0
and 0 < c∗< 1, if
A1 ⊆A ⊆2L\{∅}"
REFERENCES,0.12424849699398798,"is closed under decomposition, and morever,"
REFERENCES,0.12464929859719438,"1. For any v ∈T with h(v) ≥h∗and a A≤v-polynomial f,"
REFERENCES,0.1250501002004008,"Var

(Evf)(X)

≤exp
 
−ε(h(v) −h∗
)Var

f(X)

.
(15)"
REFERENCES,0.1254509018036072,"2. For any v ∈T \ {ρ} with h(v) ≥h∗and a A≤v-polynomial f with Ef(X) = 0,"
REFERENCES,0.12585170340681362,"c∗E

(Evf 2)(Xv)

≤E[(Evf 2)(Xv) | Xp(v) = θ] ≤1"
REFERENCES,0.12625250501002003,"c∗E

(Evf 2)(Xv)

,
(16)"
REFERENCES,0.12665330661322646,for all θ ∈[q].
REFERENCES,0.12705410821643287,"The inequality (15) bears a resemblance to the inequality we aim to prove in Theorem B.1. The
second inequality, (16), will later be seen as a crucial step proving the inductive phase of our proof.
Indeed, in the case where cM > 0, the condition (16) can be easily satisfied by appropriately choosing
c∗.
Lemma B.4. For any given A1 ⊆A ⊆2L\{∅} which is closed under decomposition. If it satisfies
(15) with a given parameter h∗and cM > 0, then A satisfies Assumption B.3 with parameter h∗and"
REFERENCES,0.12745490981963928,"c∗:= min
n
cM,
1
minj π(j)"
REFERENCES,0.12785571142284569,"o
> 0."
REFERENCES,0.1282565130260521,"In other words, we can choose c∗with no dependence on either h∗or A."
REFERENCES,0.12865731462925853,Proof. Consider an arbitrary function f with variables (xu : u ≤v) for some v ∈T\{ρ}.
REFERENCES,0.12905811623246494,"Let g(x) := (Evf 2)(x). By the Markov Property, (Evf 2)(x) is a function of xv, which in turn
implies g(x) = g(xv). Now, for any θ ∈[q], fix an index j0 ∈[q] such that g(j0) ≥Eg(Xv).
Relying on g is a non-negative function,"
REFERENCES,0.12945891783567134,"E[g(Xv) | Xp(v) = θ] =
X"
REFERENCES,0.12985971943887775,"j∈[q]
Mθjg(j) ≥Mθj0g(j0) ≥cMEg(Xj)."
REFERENCES,0.13026052104208416,"By unraveling the definition of g, we can satisfy the first inequality of (16) as long as c∗< cM. The
proof for the second inequality follows a similar logic, using the condition c∗≤
1
minj π(j) and the
trival inequality maxi,j Mij ≤1."
REFERENCES,0.13066132264529057,"Given this notation, the proof of Theorem B.1 proceeds by induction, with the base case and inductive
articulated in the subsequent two statements.
Proposition B.5. Given the rooted tree T and the transition matrix M described in Theorem 1.6,
and under the additional assumption that cM = mini,j∈[q] Mij > 0. There exists C = C(M, ε) ≥1
so that the following holds:"
REFERENCES,0.131062124248497,"Fix ρ′ ∈T and 0 ≤m ≤h(ρ′), if f(x) is a degree 1 polynomials of variables (xv : v ∈Dm(ρ′)),
then"
REFERENCES,0.1314629258517034,"Var

(Eρ′f)(X)

≤exp

−ε
 
h(ρ′) −m −C(log(R) + 1)

Var

f(X)

.
(17)"
REFERENCES,0.13186372745490982,"Theorem B.6. Given the rooted tree T and the transition matrix M described in Theorem 1.6, and
under the additional assumption that cM = mini,j∈[q] Mij > 0. Suppose A is a collection of subsets
satisfying Assumption B.3 with parameters (h∗, c∗). Then, there exists C = C(M, d, c∗) ≥1 such
that B = B(A) satisfies Assumption B.3 with parameters
 
h∗+ C(log(R) + 1), c∗
."
REFERENCES,0.13226452905811623,Let us derive the proof of Theorem B.1 based on the above two statements.
REFERENCES,0.13266533066132263,"Proof of Theorem B.1. We apply Proposition B.5 and Lemma B.4 to get A1 satisfies Assumption B.3
with parameter h∗= CB.5(log(R) + 1), where CB.5 = C(M, d) is the constant introduced in the
Proposition and"
REFERENCES,0.13306613226452907,"c∗= min
n
cM,
1
minj π(j)"
REFERENCES,0.13346693386773548,"o
> 0."
REFERENCES,0.13386773547094188,"Then, by applying Theorem B.6 inductively on the chain Ak, we can conclude that Ak satisfies
Assumption B.3 with parameter h∗= C(log(R) + 1)k and the same c∗described above, provided
that C = C(M, d, c∗) is the maximum of the constants C described in Proposition B.5 and Theorem
B.6. In other words, for any Ak-polynomial f,
Var

(Eρf)(X)

≤exp
 
−ε(ℓ−C(log(R) + 1)k)

Var

f(X)

."
REFERENCES,0.1342685370741483,"The theorem follows by choosing k =
1
2C(log(R)+1)ℓ."
REFERENCES,0.1346693386773547,"C
Variance Decomposition and Variance Estimate for degree 1 polynomials"
REFERENCES,0.13507014028056114,"To describe the goal of this section, let us begin with the variance decomposition of degree 1
polynomials in a slight generalized form. Essentially, the following statement is a direct consequence
of the conditional variance formula. We will state the main results first and provide the proof later in
this section.
Lemma C.1. Fix ρ′ ∈T and 0 ≤k ≤h(ρ′), consider a function g : [q]T →R of the form"
REFERENCES,0.13547094188376754,"g(x) =
X"
REFERENCES,0.13587174348697395,"v∈Dk(ρ′)
gv(x)
where
gv(x) = gv(x≤v)."
REFERENCES,0.13627254509018036,"For w ∈Tρ′\{ρ′} with h(w) ≥k + 1, let"
REFERENCES,0.13667334669338677,"gw(x) :=
X"
REFERENCES,0.13707414829659317,"v∈Dk(w)
gv(x). Then,"
REFERENCES,0.1374749498997996,"Var[g(X)] =Var

(Eρ′g)(Xρ′)

+
X"
REFERENCES,0.13787575150300602,"w∈Tρ′\{ρ′}:h(w)≥k
EVar

(Ewgw)(Xw)
 Xp(w)
 +
X"
REFERENCES,0.13827655310621242,"v∈Dk(ρ′)
EVar

gv(X)
 Xv

."
REFERENCES,0.13867735470941883,"Our goal is to show that when dλ2 < 1, Var[g(X)] is of the same order as P"
REFERENCES,0.13907815631262524,"v∈Dk(ρ′) Var[gv(X)],
based on the above lemma.
Lemma C.2. Suppose the transition matrix M satisfies dλ2 < 1 and the tree T has growth factor R.
Then, there exists a constant C = C(M, ε) ≥1 so that the following holds. Let ρ′ ∈T, l′ := h(ρ′),
and k ∈[0, l′]. Consider a function of the form g(x) = P"
REFERENCES,0.13947895791583168,"v∈Dk(ρ′) gv(xv). Then,"
REFERENCES,0.13987975951903808,"Var[g(X)] ≤CR
X"
REFERENCES,0.1402805611222445,"v∈Dk(ρ′)
Var[gv(Xv)].
(18)"
REFERENCES,0.1406813627254509,"The opposite bound does not depend on dλ2 ≤1. However, the proof in the general case where
cM = 0 is not straight-forward. We state it in full generality but will defer the general proof and
prove it here in the simpler case where cM > 0.
Proposition C.3. There exists a constant C = C(M, d) ≥1 so that the following holds. Let ρ′ ∈T,
and k ∈[0, h(ρ′)]. For any degree-1 function g with variables (xv : v ∈Dk(ρ′)). There exists
functions gv(x) = gv(xv) for v ∈Dk(ρ′) so that the following holds:"
REFERENCES,0.1410821643286573,1. g(X) = P
REFERENCES,0.14148296593186374,"v∈Dk(ρ′) gv(Xu) almost surely. (They may not agree as functions from [q]T to
R.)"
REFERENCES,0.14188376753507015,"2. For any u ∈Tρ′ with h(u) ≥k,
X"
REFERENCES,0.14228456913827656,"v∈Dk(u)
Var[gv(Xv)] ≤CR3Var

X"
REFERENCES,0.14268537074148296,"v∈Dk(u)
gv(Xv)

.
(19)"
REFERENCES,0.14308617234468937,"In particular, taking u = ρ′ we have
X"
REFERENCES,0.14348697394789578,"v∈Dk(ρ′)
Var[gv(Xv)] ≤CR3Var[g(X)].
(20)"
REFERENCES,0.14388777555110221,"We postpone the proof of Proposition in full generality in Appendix I, due to the technical complexity
of the proof and the fact that the proof is about properties of a Markov Chain. Instead, a statement of
the proposition and its proof in the case where cM > 0 is provided in this section."
REFERENCES,0.14428857715430862,"Now, the purpose of this section is twofold."
REFERENCES,0.14468937875751503,"• First, it is the derivation of the variance related estimates: Lemma C.1, Lemma C.2, and
Proposition C.3 with the additional assumption that cM > 0. Additionally, we summarise
the estimates into a single statements, as stated in Lemma C.7.
• Second, it is the derivation of the base case of the induction, Proposition B.5."
REFERENCES,0.14509018036072144,"C.1
Variance Decomposition and Estimates"
REFERENCES,0.14549098196392785,"Before we proceed to the proof of Lemma C.1, let us remark on the following consequence of the
lemma.
Remark C.4. For any g described in Lemma C.1, if we define h(x)
:=
(Ekg)(x)
=
P"
REFERENCES,0.14589178356713428,"v∈Dk(ρ′)(Evgv)(xv) and hv(x) := (Evgv)(xv), then by applying the lemma to both g and h,
we conclude that"
REFERENCES,0.1462925851703407,"Var[g(X)] = Var[(Ekg)(X)] +
X"
REFERENCES,0.1466933867735471,"v∈Dk(ρ′)
EVar

gv(X)
 Xv

."
REFERENCES,0.1470941883767535,"Proof of Lemma C.1 . First, for v ∈Tρ′\{ρ′} with h(v) ≥k,"
REFERENCES,0.1474949899799599,˜gv(x) := gv(x) −Egv(X).
REFERENCES,0.14789579158316632,Notice the following holds:
REFERENCES,0.14829659318637275,"˜g(x) := g(x) −Eg(X) =
X"
REFERENCES,0.14869739478957916,"v∈Dk(ρ′)
˜gv(x)."
REFERENCES,0.14909819639278557,Let us start decomposing the variance of g.
REFERENCES,0.14949899799599198,"Var[g(X)] = Var[˜g(X)] =
X"
REFERENCES,0.14989979959919839,"v,v′∈Dk(ρ′)
E

˜gv(X)˜gv′(X)
 =
X"
REFERENCES,0.15030060120240482,w∈Tρ′:h(w)>k X
REFERENCES,0.15070140280561123,"v,v′∈Dk(ρ′) : ρ(v,v′)=w
E

˜gv(X)˜gv′(X)

+
X"
REFERENCES,0.15110220440881764,"v∈Dk(ρ′)
E

˜g2
v(X)
"
REFERENCES,0.15150300601202404,"By the Markov Property and rearrangement of the terms, for each w ∈Tρ′ with h(w) > k,
X"
REFERENCES,0.15190380761523045,"v,v′∈Dk(ρ′) : ρ(v,v′)=w
E

˜gv(X)˜gv′(X)
 =
X"
REFERENCES,0.1523046092184369,"v,v′∈Dk(ρ′) : ρ(v,v′)=w
E

(Ew˜gv)(X)(Ew˜gv′)(X)
 =
X"
REFERENCES,0.1527054108216433,"v,v′∈Dk(w)
E

(Ew˜gv)(X)(Ew˜gv′)(X)

−
X"
REFERENCES,0.1531062124248497,"v,v′∈Dk(w) : ρ(v,v′)<w
E

(Ew˜gv)(X)(Ew˜gv′)(X)
"
REFERENCES,0.1535070140280561,"=E

(Ew˜gw)2(X)

−
X"
REFERENCES,0.15390781563126252,"w′∈c(w)
E

(Ew˜gw′)2(X)
"
REFERENCES,0.15430861723446893,"Hence,"
REFERENCES,0.15470941883767536,"Var[g(X)] =
X"
REFERENCES,0.15511022044088177,w∈Tρ′:h(w)>k
REFERENCES,0.15551102204408818,"
E

(Ew˜gw)2(X)

−
X"
REFERENCES,0.15591182364729458,"w′∈c(w)
E

(Ew˜gw′)2(X)

+
X"
REFERENCES,0.156312625250501,"v∈Dk(ρ′)
E

˜g2
v(X)
 =
X"
REFERENCES,0.15671342685370743,"w∈Tρ′:h(w)>k
E

(Ew˜gw)2(X)

−
X"
REFERENCES,0.15711422845691383,"w′∈Tρ′\{ρ′}:h(w′)≥k
E

(Ep(w′)˜gw′)2(X)

+
X"
REFERENCES,0.15751503006012024,"v∈Dk(ρ′)
E

˜g2
v(X)
"
REFERENCES,0.15791583166332665,"=E

(Eρ′˜gρ′)2(X)

+
X"
REFERENCES,0.15831663326653306,"w∈Tρ′\{ρ′}:h(w)>k
E

(Ew˜gw)2(X) −(Ep(w)˜gw)2(X)
 +
X"
REFERENCES,0.15871743486973947,v∈Dk(ρ′)
REFERENCES,0.1591182364729459,"
E

˜g2
v(X)

−E

(Ev˜gv)2(X)

+ E

(Ev˜gv)2(X)
"
REFERENCES,0.1595190380761523,"|
{z
}
=0"
REFERENCES,0.15991983967935872,"−E(Ep(v)˜gv)2(X)
"
REFERENCES,0.16032064128256512,"=E

(Eρ′˜gρ′)2(X)

+
X"
REFERENCES,0.16072144288577153,"w∈Tρ′\{ρ′}:h(w)≥k
EVar

(Ewgw)(Xw)
 Xp(w)

. +
X"
REFERENCES,0.16112224448897797,"v∈Dk(ρ′)
EVar

gv(X)
 Xv
"
REFERENCES,0.16152304609218437,"Next, let us show the proof of Lemma C.2."
REFERENCES,0.16192384769539078,"Proof of Lemma C.2. Let C0 = C0(M, d) denote the constant introduced in the statement of the
Lemma. Its precise value will be determined along the proof. Without lose of generality, we may
assume both Eg(X) = 0 and Egv(Xv) = 0 for v ∈Dk(ρ′), and the variance of each function
is simply its the second moment. For brevity, let τu := (E(gu(X))2)1/2 for u ∈Dk(ρ′) and
ℓ′ := h(ρ′)."
REFERENCES,0.1623246492985972,"By (8) from Lemma A.5, for s ∈[l′ −k],"
REFERENCES,0.1627254509018036,"E

(Eps(u)gu)2(Xps(u))

≤CA.5s2qλ2sτ 2
u,
(21)"
REFERENCES,0.16312625250501003,"where CA.5 ≥1 is the constant introduced in the Lemma. In particular, if ρ(u, u′) = ps(u) for
u, u′ ∈Dk(ρ′), then"
REFERENCES,0.16352705410821644,"|Egu(X)gu′(X)| =
E

(Eps(u)gu)(Xps(u))(Eps(u)gu′)(Xps(u))
"
REFERENCES,0.16392785571142285,"≤
 
E

(Eps(u)gu)2(Xps(u))
1/2 ·
 
E

(Eps(u)gu′)2(Xps(u))
1/2"
REFERENCES,0.16432865731462926,"≤CA.5s2qλ2sτuτu′. Then,"
REFERENCES,0.16472945891783566,"E(g(X))2 ≤
X"
REFERENCES,0.16513026052104207,"u,u′∈Dk(ρ′)
|Egu(X)gu′(X)| =
X"
REFERENCES,0.1655310621242485,s∈[l′−k] X
REFERENCES,0.16593186372745491,v∈Dk+s(ρ′) X
REFERENCES,0.16633266533066132,"u,u′
CA.5s2qλ2sτuτu′,"
REFERENCES,0.16673346693386773,where the sum P
REFERENCES,0.16713426853707414,"u,u′ is taken over all ordered pairs (u, u′) with u, u′ ∈Dk(v) with ρ(u, u′) = v.
By relaxing the constraint of the summation we have"
REFERENCES,0.16753507014028057,"E(g(X))2 ≤
X"
REFERENCES,0.16793587174348698,s∈[l′−k] X
REFERENCES,0.1683366733466934,v∈Dk+s(ρ′) X
REFERENCES,0.1687374749498998,"u,u′∈Dk(v)
CA.5s2qλ2sτuτu′ =
X"
REFERENCES,0.1691382765531062,s∈[l′−k] X
REFERENCES,0.1695390781563126,"v∈Dk+s(ρ′)
CA.5s2qλ2s
X"
REFERENCES,0.16993987975951905,"u∈Dk(v)
τu
2 ≤
X"
REFERENCES,0.17034068136272545,s∈[l′−k] X
REFERENCES,0.17074148296593186,"v∈Dk+s(ρ′)
CA.5s2qλ2sRds
X"
REFERENCES,0.17114228456913827,"u∈Dk(v)
τ 2
u =

X"
REFERENCES,0.17154308617234468,"s∈[l′−k]
CA.5s2qλ2sRds
X"
REFERENCES,0.1719438877755511,"u∈Dk(ρ′)
τ 2
u. Next, ∞
X"
REFERENCES,0.17234468937875752,"s=1
CA.5s2qλ2sRds ≤ ∞
X"
REFERENCES,0.17274549098196393,"s=1
RCA.5s2q exp(−1.1εs) := C0R."
REFERENCES,0.17314629258517034,"Hence, C0 depends on ε, q, and CA.5. It is a constant which is determined by M and ε."
REFERENCES,0.17354709418837674,"Let us formulate Proposition C.3 under the additional assumption that cM = mini,j∈[q] Mij > 0.
Indeed, in this case, the bound does not depend on R."
REFERENCES,0.17394789579158318,"Proposition C.5. Suppose the transition matrix M satisfies cM > 0. There exists a constant
C = C(M, ε) ≥1 so that the following holds: Let ρ′ ∈T, l′ := h(ρ′), and k ∈[0, l′]. For any
function g = [q]T →R of the form"
REFERENCES,0.1743486973947896,"g(x) =
X"
REFERENCES,0.174749498997996,"v∈Dk(ρ′)
gv(xv)."
REFERENCES,0.1751503006012024,"The following holds: For any u ∈Tρ′ with h(u) ≥k,
X"
REFERENCES,0.1755511022044088,"v∈Dk(u)
Var[gv(Xv)] ≤CVar

X"
REFERENCES,0.17595190380761522,"v∈Dk(u)
gv(Xv)

.
(22)"
REFERENCES,0.17635270541082165,"In particular, taking u = ρ′ we have
X"
REFERENCES,0.17675350701402806,"v∈Dk(ρ′)
Var[gv(Xv)] ≤CVar[g(X)].
(23)"
REFERENCES,0.17715430861723447,"The proof of the Proposition relies on the following immediate consequence of cM > 0:
Lemma C.6. Suppose M is a q × q ergodic transition matrix with cM = mini,j∈[q] Mij > 0. There
exists C = C(M) ≥1 so that the following holds. For any u ∈T\{ρ} and a function h(x) = h(xu),"
REFERENCES,0.17755511022044088,"EVar[h(Xu) | Xp(u)] ≥
1
C(M)Var[h(Xu)]."
REFERENCES,0.17795591182364728,"Proof. Let θ1 = argminθ∈[q]h(θ) and θ2 = argmaxθ∈[q]h(θ). (In the case of a tie, we may choose
any of the minimizers or maximizers.) First, we have"
REFERENCES,0.17835671342685372,Var[h(Xu)] ≤(h(θ2) −h(θ1))2.
REFERENCES,0.17875751503006013,"Next, for any β ∈[q], we have"
REFERENCES,0.17915831663326653,"max

|E[h(Xu) | Xp(u) = β] −h(θ1)|, |E[h(Xu) | Xp(u) = β] −h(θ2)|
	
≥1"
REFERENCES,0.17955911823647294,2|h(θ2) −h(θ1)|.
REFERENCES,0.17995991983967935,"Let i ∈{1, 2} be the index such that |E[h(Xu) | Xp(u) = β] −h(θi)| ≥1"
REFERENCES,0.18036072144288579,"2|h(θ2) −h(θ1)|, and we
will use this together with cM > 0 to give a lower bound on the conditional variance:"
REFERENCES,0.1807615230460922,"Var[h(Xu) | Xp(u) = β] ≥
 
E[h(Xu) | Xp(u) = β] −h(θi)
2P{Xu = θi | Xp(u) = β} ≥1"
REFERENCES,0.1811623246492986,4(h(θ2) −h(θ1))2cM.
REFERENCES,0.181563126252505,"Since it holds for every β ∈[q], we conclude that"
REFERENCES,0.18196392785571142,EVar[h(Xu) | Xp(u)] ≥1
REFERENCES,0.18236472945891782,4(h(θ2) −h(θ1))2cM ≥cM
REFERENCES,0.18276553106212426,4 Var[h(Xu)].
REFERENCES,0.18316633266533067,"Proof of Proposition C.5. We adapt the notation from Lemma C.1. For w ≤ρ′ with h(w) > k, let"
REFERENCES,0.18356713426853707,"gw(x) :=
X"
REFERENCES,0.18396793587174348,"u∈Dk(w)
gu(x)."
REFERENCES,0.1843687374749499,"Now, we apply Lemma C.1 and Lemma C.6 to get"
REFERENCES,0.18476953907815633,"Var[g(X)] =Var

(Eρ′gρ′)(Xρ′)

+
X"
REFERENCES,0.18517034068136273,"w∈Tρ′\{ρ′}:h(w)≥k
EVar

(Ewgw)(Xw)
 Xp(w)
 +
X"
REFERENCES,0.18557114228456914,"v∈Dk(ρ′)
EVar

gv(X)
 Xv
 ≥
X"
REFERENCES,0.18597194388777555,"u∈Dk(ρ′)
EVar

(Eugu)(Xu)
 Xp(u)
"
REFERENCES,0.18637274549098196,"≥
1
CC.6 X"
REFERENCES,0.18677354709418836,"u∈Dk(ρ′)
Var[gu(Xu)],"
REFERENCES,0.1871743486973948,"where we used the fact that all the terms are non-negative, the first inequality is obtained by looking at
the second terms for the summands with h(w) = k and CC.6 is the M-dependent constant introduced
in Lemma C.6."
REFERENCES,0.1875751503006012,"Lemma C.7. Suppose dλ2 < 1 and the growth factor is at most R. There exists a constant
C = C(M, d) ≥1 so that the following holds. Fix ρ′ ∈T and 0 ≤m ≤h(ρ′), if fm(x) is a function
in the form
fm(x) =
X"
REFERENCES,0.18797595190380761,"v∈Dm(ρ′)
fv(x≤v)."
REFERENCES,0.18837675350701402,"with
Efm(X) = 0.
Then, there exists ¯fv(x≤v) for v ∈Dm(ρ′) such that their sum ¯fm(x) = P"
REFERENCES,0.18877755511022043,"v∈Dm(ρ′) ¯fv(x≤v)
satisfies
¯fm(X) = fm(X) almost surely,
and for u ≤ρ′ with h(u) ≥m,"
REFERENCES,0.18917835671342687,"1
CR3
X"
REFERENCES,0.18957915831663327,"v∈Dm(u)
E ¯f 2
v (X) ≤E

X"
REFERENCES,0.18997995991983968,v∈Dm(u)
REFERENCES,0.1903807615230461,"¯fv(X)
2
≤CR
X"
REFERENCES,0.1907815631262525,"v∈Dm(u)
E ¯f 2
v (X).
(24)"
REFERENCES,0.19118236472945893,"The statement of the lemma using ¯fm and ¯fb covers also the case cM = 0. We will prove the Lemma
by using either Proposition C.3 or Proposition C.5 with the assumption cM > 0. In the later case, it
suffice to simply take fv(x≤v) = ¯fv(x≤v).
Remark C.8. Note that the lemma implies the following: For any u ≤ρ′ with h(u) ≥m, let"
REFERENCES,0.19158316633266534,"¯fm,u(x) :=
X"
REFERENCES,0.19198396793587175,v∈Dm(u)
REFERENCES,0.19238476953907815,"¯fv(x).
(25)"
REFERENCES,0.19278557114228456,"Then, for any given m ≤k < k′ ≤h(ρ′) and u ∈Dk′(ρ′), we have"
REFERENCES,0.19318637274549097,"E ¯f 2
m,u(X) ≤CR
X"
REFERENCES,0.1935871743486974,"v∈Dm(u)
E ¯f 2
v (X) = CR
X"
REFERENCES,0.1939879759519038,w∈Dk(u) X
REFERENCES,0.19438877755511022,"v∈Dm(w)
E ¯f 2
v (X) ≤C2R4
X"
REFERENCES,0.19478957915831663,w∈Dk(u)
REFERENCES,0.19519038076152304,"¯f 2
m,w(X),"
REFERENCES,0.19559118236472947,"where the first inequality follows from the second inequality of (24) and the second inequality follows
from the first inequality of (24)."
REFERENCES,0.19599198396793588,Proof of Lemma C.7. Let
REFERENCES,0.1963927855711423,"h(x) := (Emfm)(x) =
X"
REFERENCES,0.1967935871743487,"v∈Dm(ρ′)
(Evfv)(xv)."
REFERENCES,0.1971943887775551,"Note that h is a degree one function of the variables (xv : v ∈Dm(ρ′)). Thus, we could apply
Proposition C.3 to show the existence of 1-variable functions hv(xv) for v ∈Dm(ρ′) such that"
REFERENCES,0.1975951903807615,"h(X) =
X"
REFERENCES,0.19799599198396794,"v∈Dm(ρ′)
hv(Xv)
almost surely
(26)"
REFERENCES,0.19839679358717435,"and for any u ∈T(ρ′) with h(u) ≥m,
X"
REFERENCES,0.19879759519038076,"v∈Dm(u)
Var[hv(Xv)] ≤CC.3R3Var

X"
REFERENCES,0.19919839679358717,"v∈Dm(u)
hv(Xv)

,
(27)"
REFERENCES,0.19959919839679358,where CC.3 ≥1 is the constant introduced in Proposition C.3.
REFERENCES,0.2,"Since Eh(X) = E(Emfm)(X) = 0, we may also assume that"
REFERENCES,0.20040080160320642,Ehv(X) = 0
REFERENCES,0.20080160320641283,"for v ∈Dm(ρ′), as a constant shift of the functions will not affect (26) and (27). Now, consider the
following functions: For v ∈Dm(ρ′), let
¯fv(x≤v) = fv(x≤v) −Efv(X≤v) + hv(xv)"
REFERENCES,0.20120240480961923,"and
¯fm(x) =
X"
REFERENCES,0.20160320641282564,v∈Dm(ρ′)
REFERENCES,0.20200400801603208,¯fv(x).
REFERENCES,0.20240480961923848,"First, since ¯fv(x≤v) is defined as the sum of three terms with mean 0, we have E ¯fv(X≤v) = 0."
REFERENCES,0.2028056112224449,"Second,"
REFERENCES,0.2032064128256513,"¯fm(X) =
X"
REFERENCES,0.2036072144288577,v∈Dm(ρ′)
REFERENCES,0.20400801603206412,"
fv(X≤v) −(Evfv)(Xv) + hv(Xv)
"
REFERENCES,0.20440881763527055,"=fm(X) −h(X) +
X"
REFERENCES,0.20480961923847696,"v∈Dm(ρ′)
hv(Xv)"
REFERENCES,0.20521042084168337,"=
a.s.fm(X)."
REFERENCES,0.20561122244488977,"By Remark C.4,"
REFERENCES,0.20601202404809618,"Var

X"
REFERENCES,0.20641282565130262,v∈Dm(u)
REFERENCES,0.20681362725450902,"¯fv(X)

=Var
h
Em
X"
REFERENCES,0.20721442885771543,v∈Dm(u)
REFERENCES,0.20761523046092184,"¯fv

(X)
i
+
X"
REFERENCES,0.20801603206412825,"v∈Dm(u)
EVar[ ¯fv(X) | Xv]"
REFERENCES,0.20841683366733466,"=Var
h
X"
REFERENCES,0.2088176352705411,"v∈Dm(u)
(Ev ¯fv)(X)
i
+
X"
REFERENCES,0.2092184368737475,"v∈Dm(u)
EVar[fv(X) −(Evfv)(Xv) + hv(Xv) | Xv]"
REFERENCES,0.2096192384769539,"=Var
h
X"
REFERENCES,0.21002004008016031,"v∈Dm(u)
hv(X)
i
+
X"
REFERENCES,0.21042084168336672,"v∈Dm(u)
EVar[fv(X) | Xv]
(28)"
REFERENCES,0.21082164328657316,"To estimate the lower bound, we rely on own choice of hv. By (27) we have"
REFERENCES,0.21122244488977956,"(28) ≥
1
CC.3R3
X"
REFERENCES,0.21162324649298597,"v∈Dm(u)
Var

hv(X)

+
X"
REFERENCES,0.21202404809619238,"v∈Dm(u)
EVar[fv(X) | Xv]"
REFERENCES,0.2124248496993988,"≥
1
CC.3R3
X"
REFERENCES,0.21282565130260522,v∈Dm(u)
REFERENCES,0.21322645290581163,"
Var

hv(X)

+ EVar[fv(X) | Xv]
"
REFERENCES,0.21362725450901804,"=
1
CC.3R3
X"
REFERENCES,0.21402805611222445,v∈Dm(u)
REFERENCES,0.21442885771543085,"
Var

(Ev ¯fv)(Xv)

+ EVar[ ¯fv(X) | Xv]
"
REFERENCES,0.21482965931863726,"=
1
CC.3R3
X"
REFERENCES,0.2152304609218437,"v∈Dm(u)
Var[ ¯fv(X)]."
REFERENCES,0.2156312625250501,"As for the upper bound, we can apply Lemma C.2 and repeat the same derivation as above to get"
REFERENCES,0.2160320641282565,"(28) ≤CC.2R
X"
REFERENCES,0.21643286573146292,"v∈Dm(u)
Var

hv(X)

+
X"
REFERENCES,0.21683366733466933,"v∈Dm(u)
EVar[fv(X) | Xv]"
REFERENCES,0.21723446893787576,"≤CC.2R
X"
REFERENCES,0.21763527054108217,"v∈Dm(u)
Var[ ¯fv(X)]."
REFERENCES,0.21803607214428858,"Therefore, by taking the constant C stated in the lemma to be the maximum of CC.3 and CC.2, the
proof follows."
REFERENCES,0.218436873747495,"C.2
Proof of the base Case of Proposition B.5"
REFERENCES,0.2188376753507014,"We now prove the base case of Proposition B.5:
Lemma C.9. There exists a constant C = C(M, d) ≥1 so that the following holds. For any degree
1 function f with variables (xu : u ∈Dk(ρ′)) for some ρ′ ∈T with k ≤h(ρ′),"
REFERENCES,0.21923847695390783,"Var

E

f(X)
 Xρ′
≤CR4(h′)2q(dλ2)h′Var[f(X)].
(29)"
REFERENCES,0.21963927855711424,where h′ = h(ρ′) −k.
REFERENCES,0.22004008016032064,Proof. Let fu for u ∈Dk(ρ′) be the functions from Proposition C.3 so that
REFERENCES,0.22044088176352705,"f(X) =
X"
REFERENCES,0.22084168336673346,"u∈Dk(ρ′)
fu(Xu) almost surely.
(30)"
REFERENCES,0.22124248496993987,"We can assume Ef(X) = 0 and Efu(X) = 0 for every u ∈Dk(ρ′) without affecting (30). From
Proposition C.3, we have
X"
REFERENCES,0.2216432865731463,"u∈Dk(ρ′)
E[f 2
u(X)] ≤CC.3R3E[f 2(X)],"
REFERENCES,0.2220440881763527,where CC.3 denotes the constant C introduced in the Proposition.
REFERENCES,0.22244488977955912,We could apply Lemma A.5 to get
REFERENCES,0.22284569138276553,"E

(Eρ′f)2(Xρ′)

≤|Dk(ρ′)|
X"
REFERENCES,0.22324649298597193,v∈Dk(ρ′)
REFERENCES,0.22364729458917837,"
(Eρ′fv)2(Xρ′)
"
REFERENCES,0.22404809619238478,"≤|Dk(ρ′)|CA.5h′2qλ2h′
X"
REFERENCES,0.22444889779559118,"u∈Dk(ρ′)
E[f 2
u(X)] ≤CC.3CA.5R4(h′)2q(dλ2)h′E[f 2(X)]"
REFERENCES,0.2248496993987976,where CA.5 denotes the constant C stated in the Lemma.
REFERENCES,0.225250501002004,"Proof of the Base Case Proposition B.5. Given ρ′ ∈T and 0 ≤m ≤h(ρ′) described in the Proposi-
tion. Let h′ = h(ρ′) −m. By Lemma C.9, any function f(x) = P"
REFERENCES,0.2256513026052104,v∈Dm(ρ′) fv(xv) satisfies
REFERENCES,0.22605210420841684,"Var

(Eρ′f)(X)

≤CC.9R4(h′)q(dλ2)h′Var[f(X)],"
REFERENCES,0.22645290581162325,where CC.9 denotes the M-dependent constant introduced in the Lemma. With
REFERENCES,0.22685370741482966,"CC.9R4(h′)q(dλ2)h′ ≤CC.9R4(h′)q exp(−1.1εh′) ≤exp
 
−ε(h′ −C1(log(R) + 1))

,"
REFERENCES,0.22725450901803607,"for some C1 ≥1 which depends on M, d."
REFERENCES,0.22765531062124247,"D
Decomposition of Polynomials"
REFERENCES,0.2280561122244489,"In this section we study the representation of functions in terms of ϕσ and ψσ. Roughly speaking ψσ
are “more orthogonal"" than the ϕσ. More formally we will show that under appropriate conditioning
expections of ψσ factorize. Thus some of the effort in the proof and particularly in this section is
devoted to relating the ϕ and ψ representations and bounding moments of such representations.
Lemma D.1. Assuming dλ2 < 1 and growth factor of R, there exists C = C(M, d) ≥1 so that
the following holds. Let A1 ⊆B ⊆2L\{∅} be a collection of subsets which is closed under
decomposition. Fix a positive integer k1 and ρ′ ∈T with l′ := h(ρ′) > k1. For every function f of
the form"
REFERENCES,0.22845691382765532,"f(x) =
X"
REFERENCES,0.22885771543086172,"σ:σ̸=0∈F(B≤ρ′)
cσϕσ(x)"
REFERENCES,0.22925851703406813,"with
Ef(X) = 0,
here exists a decomposition of f"
REFERENCES,0.22965931863727454,"f(X) =
X"
REFERENCES,0.23006012024048098,u≤ρ′ : h(u)≥k
REFERENCES,0.23046092184368738,"˜fu(X) almost surely,"
REFERENCES,0.2308617234468938,"where, for each u ≤ρ′ with h(u) ≥k1, we have a function fu(x) = fu(x≤u) and"
REFERENCES,0.2312625250501002,"1. For u ∈Tρ′ with h(u) > k1, fu(x) is a linear combination of ψσ(x) with σ ∈F(Bu) and
˜fu(x) = fu(x) −Efu(X)."
REFERENCES,0.2316633266533066,"2. For w ≤ρ′ with h(w) ≥k1, we have"
REFERENCES,0.232064128256513,"1
CR3 E
h
X"
REFERENCES,0.23246492985971945,"u∈Dk1(w)
f 2
u(X)
i
≤E(
X"
REFERENCES,0.23286573146292586,"u∈Dk1(w)
fu(X))2 ≤CRE
h
X"
REFERENCES,0.23326653306613226,"v∈Dk1(w)
f 2
u(X)
i
.
(31)"
REFERENCES,0.23366733466933867,"We may group the fu according to h(u) and define for k1 ≤k ≤h(ρ′),"
REFERENCES,0.23406813627254508,"fk(x) :=
X"
REFERENCES,0.23446893787575152,u∈Dk(ρ′)
REFERENCES,0.23486973947895792,˜fu(x).
REFERENCES,0.23527054108216433,"In other words,
f(X) =
X"
REFERENCES,0.23567134268537074,"k∈[k1,h(ρ′)]
fk(X) almost surely."
REFERENCES,0.23607214428857715,"To prove the main lemma, let us begin by comparing ϕσ(x) and ψσ(x) (See Definition A.11)."
REFERENCES,0.23647294589178355,"Lemma D.2. For σ ∈F(Bu), ϕσ(x) can be expressed in the form"
REFERENCES,0.23687374749499,"ϕσ(x) =
Y"
REFERENCES,0.2372745490981964,"i∈I(σ)
ψPiσ(x) −a⊂,σ(x) −a<,σ(x) −ac,σ,
(32)"
REFERENCES,0.2376753507014028,where:
REFERENCES,0.2380761523046092,"• a⊂,σ(x) is a linear combination of ϕσ′(x) for σ′ ∈F(Bu) such that I(σ′) is a proper
subset of I(σ)."
REFERENCES,0.23847695390781562,"• a<,σ(x) is a linear combination of ϕσ′(x) for σ′ ∈F(B<u) (recall that B<u = {S ∈B :
ρ(S) < u}), and"
REFERENCES,0.23887775551102206,"• ac,σ is a constant."
REFERENCES,0.23927855711422846,"Proof. Fix σ ∈F(B) and let u = ρ(S) and S = S(σ). Recall that Piσ ∈[0, q−1]T is the projection
of σ to Si. We can also decompose the function ϕσ according to {Piσ}i∈I(σ):"
REFERENCES,0.23967935871743487,"ϕσ(x) =
Y"
REFERENCES,0.24008016032064128,"i∈I(S)
ϕPiσ(x).
(33)"
REFERENCES,0.24048096192384769,"Before we proceed, let us note that by Lemma 1.12 and the definition of B, we have Piσ ∈F(B≤ui).
Now, let us expand the function ψσ according to its definition:
Y"
REFERENCES,0.24088176352705412,"i∈I(S)
ψPiσ(x) =
Y"
REFERENCES,0.24128256513026053,i∈I(S)
REFERENCES,0.24168336673346694,"
ϕPiσ(x) −EϕPiσ(X)

=
X I1,I2  Y"
REFERENCES,0.24208416833667334,"i∈I1
ϕPiσ(x)
 Y"
REFERENCES,0.24248496993987975,"i∈I2
(−EϕPiσ(X))

,"
REFERENCES,0.24288577154308616,"where the summation is taken over all possible partition I1 ⊔I2 = I(σ). Next, we can group the
summands into four types based on |I1| and |I2|:"
REFERENCES,0.2432865731462926,Type 1 |I1| = |I(σ)|. The summand is simply ϕσ(x).
REFERENCES,0.243687374749499,Type 2 2 ≤|I1| ≤|I(σ)| −1.
REFERENCES,0.2440881763527054,"Each summand is a constant multiple of ϕσ′(x) where σ′ is the projection of σ to the indices
⊔i∈I1Si. Clearly, S(σ′) = ⊔i∈I1Si. With |I1| ≥2, we have ρ(σ′) = u. Further, each
Si ∈A≤ui for i ∈I(σ), it follows that S(σ′) ∈Bu, which in turn implies σ′ ∈F(Bu)."
REFERENCES,0.24448897795591182,"We denote the sum of summands of this type by a⊂,σ(x)."
REFERENCES,0.24488977955911823,"Type 3 |I1| = 1. Each summand is a constant multiple of ϕPiσ(x), where i is the element in I1.
Notice that Piσ ∈F(A<u) ⊂F(B<u) where the inclusion follows from Lemma 1.12. We
denote the sum of summands of this type as a<,σ(x)."
REFERENCES,0.24529058116232466,"Type 4 |I1| = 0 There is only one summand, which is a constant. We denote this constant by ac,Piσ."
REFERENCES,0.24569138276553107,"With this decomposition, (32) follows."
REFERENCES,0.24609218436873748,"Given the expressions for ψσ(x) in terms ϕσ(x) and vice-versa, for any given u ∈T\L, we can
convert a linear combination of ϕσ(x) with σ ∈F(Bu) to that of ψσ(x) with σ ∈F(Bu)."
REFERENCES,0.24649298597194388,"Lemma D.3. For u ∈T\L, consider any function of the form"
REFERENCES,0.2468937875751503,"pu(x) =
X"
REFERENCES,0.2472945891783567,"σ∈F(Bu)
cσϕσ(x)."
REFERENCES,0.24769539078156314,Then there exists a decomposition
REFERENCES,0.24809619238476954,"pu(x) = ˜fu(x) + p<,u(x) + cu,"
REFERENCES,0.24849699398797595,where:
REFERENCES,0.24889779559118236,"• ˜fu(x) = fu(x) −Efu(X) and fu(x) is a linear combination of ψσ(x) for σ ∈F(Bu),"
REFERENCES,0.24929859719438877,"• p<,u(x) is a linear combination of ϕσ(x) with σ ∈F(B<u), and"
REFERENCES,0.2496993987975952,• cu is a constant.
REFERENCES,0.2501002004008016,Proof. The decomposition is constructed through recursion on the following expression:
REFERENCES,0.250501002004008,"r(pu) := argmax

|I(σ)| : σ ∈F(Bu), cσ ̸= 0
	
."
REFERENCES,0.2509018036072144,Suppose r(pu) = 2. Then the statement of simply follows from Lemma D.2.
REFERENCES,0.25130260521042086,"Suppose the statement of the lemma holds whenever r(pu) ≤r for 2 ≤r < Rd. Consider any
function pu with r(pu) = r + 1:"
REFERENCES,0.25170340681362724,"pu(x) =
X"
REFERENCES,0.2521042084168337,"σ∈F(Bu) :|I(σ)|≤r+1
cσϕσ(x) =
X"
REFERENCES,0.25250501002004005,"σ∈F(Bu) :|I(σ)|=r+1
cσϕσ(x)"
REFERENCES,0.2529058116232465,"|
{z
}
:=pu,r+1(x) +
X"
REFERENCES,0.2533066132264529,"σ∈F(Bu) :|I(σ)|≤r
cσϕσ(x)"
REFERENCES,0.2537074148296593,"|
{z
}
:=pu,≤r(x) ."
REFERENCES,0.25410821643286574,"According to the decomposition of ϕσ(x) in Lemma D.2, let"
REFERENCES,0.2545090180360721,"fu,r+1(x) :=
X"
REFERENCES,0.25490981963927856,"σ∈F(Bu) : |I(σ)|=r+1
cσψσ(x)"
REFERENCES,0.255310621242485,"p∗,u,r+1(x) :=
X"
REFERENCES,0.25571142284569137,"σ∈F(Bu) : |I(σ)|=r+1
cσa∗,σ(x)"
REFERENCES,0.2561122244488978,"where ∗can be ⊂, <, or c. Then,"
REFERENCES,0.2565130260521042,"pu,r+1(x) = fu,r+1(x) + p⊂,u,r+1(x) + p<,u,r+1(x) + pc,u,r+1(x).
(34)"
REFERENCES,0.2569138276553106,"Observe that pu,≤r(x)+p⊂,u,r+1(x) is a linear combination of ϕσ(x) with σ ∈F(Bu) and |I(σ)| ≤
r. Thus, by the inductive assumption, the summation can be expressed in the form"
REFERENCES,0.25731462925851706,"pu,≤r(x) + p⊂,u,r+1(x) = ˜f ′
u(x) + p′
<,u(x) + c′
u."
REFERENCES,0.25771543086172344,"Finally, let"
REFERENCES,0.2581162324649299,"fu(x) =f ′
u(x) + fu,r+1(x),"
REFERENCES,0.25851703406813625,"p<,u(x) =p′
<,u(x) + p<,u,r+1(x),"
REFERENCES,0.2589178356713427,"cu =c′
u + p′
c,u + E

fu,r+1(X)

,"
REFERENCES,0.2593186372745491,and we have
REFERENCES,0.2597194388777555,"pu(x) =fu,r+1(x) + p<,u,r+1(x) + pc,u,r+1(x) + ˜f ′
u(x) + p′
<,u(x) + c′
u
= ˜fu(x) + p<,u(x) + cu."
REFERENCES,0.26012024048096194,Proof of Lemma D.1. We will construct fu(x) for u starting from top layer (u = ρ′) to bottom layer.
REFERENCES,0.2605210420841683,"For k ∈[k1, l′ −1], when fu(x) is constructed for u ∈Tρ′ with h(u) > k, we define"
REFERENCES,0.26092184368737475,"f≤k(x) = f(x) −
X"
REFERENCES,0.26132264529058113,u : h(u)>k+1
REFERENCES,0.26172344689378757,"˜fu(x),
(35)"
REFERENCES,0.262124248496994,"where ˜fu(x) = fu(x) −Efu(X). Without lose of generality, let f≤l′(x) = f(x)."
REFERENCES,0.2625250501002004,"Fix k ∈[k1 + 1, l′]. For the induction step, suppose {fu(x)}u∈Tρ′ : h(u)>k {f≤s(x)}s∈[k,l′] have
been constructed such that f≤k(x) can be expressed in the form"
REFERENCES,0.2629258517034068,"f≤k(x) = c′ +
X"
REFERENCES,0.2633266533066132,"σ∈F(B) : k1<h(ρ(σ))≤k
c′
σϕσ(x) +
X"
REFERENCES,0.26372745490981964,"σ∈F(2L) : h(ρ(σ))≤k1
c′
σϕσ(x).
(36)"
REFERENCES,0.26412825651302607,"Clearly, this holds when k = l′."
REFERENCES,0.26452905811623245,"For each u with h(u) = k, let pu(x) = P"
REFERENCES,0.2649298597194389,"σ∈F(Bu) c′
σϕσ(x), and define ˜fu(x), p<,u(x), and cu
according to Lemma D.3. Then,"
REFERENCES,0.26533066132264527,"f≤k−1(x) =f≤k(x) −
X"
REFERENCES,0.2657314629258517,u : h(u)=k1
REFERENCES,0.26613226452905814,˜fu(x)
REFERENCES,0.2665330661322645,"=c′ +
X"
REFERENCES,0.26693386773547095,"σ∈F(B) : k1<h(ρ(σ))≤k−1
c′
σϕσ(x) +
X"
REFERENCES,0.26733466933867733,"σ∈F(2L) : h(ρ(σ))≤k1
c′
σϕσ(x) +
X"
REFERENCES,0.26773547094188377,"u : h(u)=k
(p<,u(x) + cu)."
REFERENCES,0.2681362725450902,"Recall from Lemma D.3 that p<,u(x) is a linear combination of ϕσ(x) with σ ∈F(B<u), the
function f≤k−1(x) satisfies (36) as well (with k been replaced by k −1)."
REFERENCES,0.2685370741482966,"Once the induction terminated at layer k1, we obtain"
REFERENCES,0.268937875751503,"fk1(x) = c +
X"
REFERENCES,0.2693386773547094,u∈Dk1(ρ′) X
REFERENCES,0.26973947895791583,"σ∈F(2Lu\{∅})
cσϕσ(x)."
REFERENCES,0.27014028056112227,"Now, observe that for k1 < k ≤h(ρ′), we have fk(x) is defined as the sum of ˜fu for u ∈Dk(ρ′),
which are functions of mean 0. Together with the assumption that Ef(X) = 0, we have"
REFERENCES,0.27054108216432865,Efk1(X) = Ef(X) −
REFERENCES,0.2709418837675351,"h(ρ′)
X"
REFERENCES,0.27134268537074147,"k=k1+1
Efk(X) = 0."
REFERENCES,0.2717434869739479,"Notice that fk1 satisfies the assumption of the function stated in Lemma C.7 with m = k1. By
replacing fk1 by ¯fk1 and fu by ¯fu for each u ∈Dk1(ρ′), the second statement follows while the
third statement of the Lemma remains true. Hence, the proof is completed."
REFERENCES,0.27214428857715434,"E
Induction Step 1: Decay of fu"
REFERENCES,0.2725450901803607,"The goal this section and next section is to prove Theorem B.6. Let us restate the theorem here.
Theorem. Given the rooted tree T and the transition matrix M described in Theorem 1.6, and
under the additional assumption that cM = mini,j∈[q] Mij > 0. Suppose A is a collection of subsets
satisfying Assumption B.3 with parameters h∗and c∗. Then, there exists C = C(M, d, c∗) ≥1 such
that B = B(A) satisfies Assumption B.3 with parameters h∗+ C(log(R) + 1) and c∗."
REFERENCES,0.27294589178356715,"In this and the following section, we will fix a collection A that meets Assumption B.3 with some
parameters l∗and c∗. Additionally, we abbreviate"
REFERENCES,0.27334669338677353,B = B(A).
REFERENCES,0.27374749498997997,"Further, we will fix ρ′ ∈T and a function f described in the Assumption B.3, and assume"
REFERENCES,0.27414829659318635,Ef(X) = 0.
REFERENCES,0.2745490981963928,"The proof is grounded in the decomposition of f as described in Lemma D.1, which splits f into
summation of fk and subsequently into summations of ˜fu. Accordingly, this section is devoted to
derive the variance decay properties of fu stated as Proposition E.1 below. The proposition will be
used to derive variance decay properties of fk, and toward the proof of Theorem B.6 in next section.
Proposition E.1. There exists C = C(M, c∗) ≥1 so that the following holds. For any u ∈T,
consider a function fu of the form"
REFERENCES,0.2749498997995992,"fu(x) =
X"
REFERENCES,0.2753507014028056,"0̸=σ∈F(Bu)
cσψσ(x)."
REFERENCES,0.27575150300601203,"Then, for θ ∈[q], we have the following bounds on (Eufu)(x) (recall that that by the Markov
Property, (Eufu)(x) is a function of xu):"
REFERENCES,0.2761523046092184,"(Eufu)2(θ) ≤exp(−2ε(h(u) −C(log(R) + 1) −h∗))(Euf 2
u)(θ)
(37) and"
REFERENCES,0.27655310621242485,"E

(Eu ˜fu)2(Xu)

≤exp(−2ε(h(u) −C(log(R) + 1) −h∗))E ˜f 2
u(X).
(38)"
REFERENCES,0.2769539078156313,"Additionally, for any function a(x) having inputs involving only (xv : v ∈Tui) for some i ∈[du]:"
REFERENCES,0.27735470941883766,"E

| ˜fu(X)a(X)|

≤exp

−ε"
REFERENCES,0.2777555110220441,"2(h(u) −C(log(R) + 1) −h∗)

(E ˜f 2
u(X))1/2(Ea2(X))1/2.
(39)"
REFERENCES,0.2781563126252505,"Remark E.2. The statement of the Proposition E.1 is exactly the statement of Theorem B.6 restricted
to functions all of whose non-zero cσ have ρ(σ) = ρ′. Thus in some sense in this section we prove
the Theorem for the most complex terms. And in the next section we will control the correlations
between different terms."
REFERENCES,0.2785571142284569,"This is an analogue in our setting to the classical fact in Fourier analysis that high amplitude functions
have sharp decay under noise.
Remark E.3. We remark that the proposition holds immediately whenever |du| ≤1, since Bu = ∅."
REFERENCES,0.27895791583166335,"Before we proceed further, we need to decompose fu(x).
Definition E.4. For u ∈T\L and fu(x) = P"
REFERENCES,0.27935871743486973,"σ∈F(Bu) cσψσ(x), let"
REFERENCES,0.27975951903807617,"fu,I(x) :=
X"
REFERENCES,0.28016032064128255,"σ∈F(Bu) : I(σ)=I
cσψσ(x),
and
(40)"
REFERENCES,0.280561122244489,"˜fu,I(x) :=fu,I(x) −Efu,I(X)
(41)"
REFERENCES,0.2809619238476954,for each I ⊆[du] with |I| ≥2.
REFERENCES,0.2813627254509018,"Given the above definition, we have"
REFERENCES,0.28176352705410823,"fu(x) =
X"
REFERENCES,0.2821643286573146,"I⊆[du] :|I|≥2
fu,I(x)."
REFERENCES,0.28256513026052105,"Proposition E.5. There exists C = C(M, c∗) ≥1 so that the following holds. For any u ∈T \ L
and I ⊆[du] with |I| ≥2. Consider a function of the form"
REFERENCES,0.2829659318637275,"a(x) =
X"
REFERENCES,0.28336673346693386,"σ∈F(Bu) :I(σ)=I
cσψσ(x)."
REFERENCES,0.2837675350701403,"Then, for I′ ⊆I, let
U = T \
  ["
REFERENCES,0.2841683366733467,"i∈I′
Tui

,"
REFERENCES,0.2845691382765531,"and we have
 
(EUa)(x)
2 ≤exp
 
−ε|I′|(h(u) −C −h∗)

(EUa2)(x).
(42)"
REFERENCES,0.2849699398797595,"Roughly speaking the proposition states that under the decay of correlation in Assumption B.3, for
functions all of whose coefficient cσ have S(σ) = I for some large set I we get a variance decay of
the form exp(−ϵ|I|h(u)). For later applications the statement is more general allowing to condition
on some of the subtrees. This is an analogue in our setting to the classical fact in Fourier analysis that
high amplitude functions have sharp decay under noise."
REFERENCES,0.28537074148296593,"Proof. We fix u ∈T\L and I ⊆[du]. Without lose of generality, we assume I′ = [s]."
REFERENCES,0.28577154308617236,"Let C0 = C0(M, c∗) denote the constant described in the statement of the Proposition. The precise
value of C0 will be determined during the proof."
REFERENCES,0.28617234468937874,"For brevity, we introduce some notations that are only used in this proof."
REFERENCES,0.2865731462925852,1. Decomposition of x ∈[q]T : Consider the representation of x as
REFERENCES,0.28697394789579156,"x = (xu, x0, x1, . . . , xs),"
REFERENCES,0.287374749498998,"where, ∀k ∈[s], xk := (xv : v ≤uk), and x0 = (xv : v ∈U \ {u})."
REFERENCES,0.28777555110220443,"Further, let
x≤k = (x0, x1, . . . , xk)."
REFERENCES,0.2881763527054108,"For k ∈[0, s],"
REFERENCES,0.28857715430861725,"a≤k(x≤k) := E
h
a(X)
 Xu = xu and X≤k = x≤k
i
."
REFERENCES,0.2889779559118236,"Before we proceed to the proof, observe that applying Jenson’s inequality on conditional expectation,
we can form a chain of inequalities"
REFERENCES,0.28937875751503006,"(EUa)2(x) = (EUa2
≤0)(x) ≤(EUa2
≤1)(x) ≤(EUa2
≤2)(x) ≤. . . ≤(EUa2
≤s)(x) = (EUa2)(x)."
REFERENCES,0.2897795591182365,"If h(u) ≤C0 + h∗, then the statement of the Proposition is weaker than the inequality (EUa)2(x) ≤
(EUa2)(x) stated above. So the lemma follows immediately in that case. From now on we assume"
REFERENCES,0.2901803607214429,"h(u) > C0 + h∗.
(43)"
REFERENCES,0.2905811623246493,We will improve each inequality in the above chain by leveraging the assumption (15).
REFERENCES,0.2909819639278557,"Given the definition of a(x),"
REFERENCES,0.2913827655310621,"a(x) =
X"
REFERENCES,0.29178356713426856,"σ
cσ
Y"
REFERENCES,0.29218436873747494,i∈I\[s]
REFERENCES,0.2925851703406814,"˜ϕPiσ(x0)
Y i∈[s]"
REFERENCES,0.29298597194388776,˜ϕPiσ(xi)
REFERENCES,0.2933867735470942,"By the Markov Property, the random variables (Xi|Xu = xu)i∈[0,s] are independent. This gives rise
to:"
REFERENCES,0.29378757515030063,"a≤k(x) =E
h X"
REFERENCES,0.294188376753507,"σ
cσ
Y"
REFERENCES,0.29458917835671344,i∈I\[s]
REFERENCES,0.2949899799599198,"˜ϕPiσ(X0)
Y i∈[s]"
REFERENCES,0.29539078156312626,"˜ϕPiσ(Xi)
 Xu = xu and X≤k = x≤k
i =
X"
REFERENCES,0.29579158316633264,"σ
cσ
Y"
REFERENCES,0.2961923847695391,i∈I\[s]
REFERENCES,0.2965931863727455,"˜ϕPiσ(x0)
Y i∈[k]"
REFERENCES,0.2969939879759519,˜ϕPiσ(xk)
REFERENCES,0.2973947895791583,"|
{z
}
This part is freezed. Y"
REFERENCES,0.2977955911823647,"i∈[k+1,s]
(Eu ˜ϕPiσ)(xu)."
REFERENCES,0.29819639278557114,"|
{z
}
This part is a function of xu"
REFERENCES,0.2985971943887776,"Now, fix k ∈[s] and express a≤k(x) = a≤k(xu, x≤k−1, xk). An essence of this proof is that the
mapping:"
REFERENCES,0.29899799599198396,"yk 7→a≤k(xu, x≤k−1, yk)"
REFERENCES,0.2993987975951904,"is a linear combination of ˜ϕσ(yk) with σ ∈F(Auk) and the coefficients are functions of (xu, x≤k−1),
which gives us room to apply the inductive assumption, or (15) from Assumption B.3."
REFERENCES,0.29979959919839677,"To aid our analysis, we introduce Yk, an independent copy of Xk. By (15), we have"
REFERENCES,0.3002004008016032,"E
h 
E[a≤k(xu, x≤k−1, Yk)|Yu]
2i
≤exp(−ε(h(u) −h∗))EYk

a2
≤k(xu, x≤k−1, Yk)

.
(44)"
REFERENCES,0.30060120240480964,"The reason we introduce Yk is that the L.H.S. and R.H.S. of the above inequality are not related
to (any moments of) conditional expectation of a(X). However, it can still be used with some
adjustment, relying on (16) from Assumption B.3."
REFERENCES,0.301002004008016,"Given the assumption on C0 being greater than or equal to 1, we have"
REFERENCES,0.30140280561122246,"h(uk) = h(u) −1
(43)
≥h∗+ C0 −1 ≥h∗."
REFERENCES,0.30180360721442884,"Applying (16) to our function yk 7→a≤k(xu, x≤k−1, yk) we get"
REFERENCES,0.3022044088176353,"EYk

a2
≤k(xu, x≤k−1, Yk)

≤1"
REFERENCES,0.3026052104208417,"c∗min
θ∈[q] EYk

a2
≤k(xu, x≤k−1, Yk)
Yu = θ
 ≤1"
REFERENCES,0.3030060120240481,"c∗EYk

a2
≤k(xu, x≤k−1, Yk)
Yu = xu

.
(45)"
REFERENCES,0.3034068136272545,"On the other hand,"
REFERENCES,0.3038076152304609,"π(xu)
 
EYk[a≤k(xu, x≤k−1, Yk)|Yu = xu]
2 ≤E
h 
EYk[a≤k(xu, x≤k−1, Yk)|Yu]
2i
."
REFERENCES,0.30420841683366734,"Combining the above expression, (44), and (45), we conclude that
 
E

a≤k(xu, x≤k−1, Yk)
 Yu = xu
2 ≤
1
c∗π(xu) exp(−ε(h(u) −h∗))EYk

a2
≤k(xu, x≤k−1, Yk)
 Yu = xu

. (46)"
REFERENCES,0.3046092184368738,Notice that the expression inside the square in L.H.S. is
REFERENCES,0.30501002004008015,"E

a≤k(xu, x≤k−1, Yk)
 Yu = xu
"
REFERENCES,0.3054108216432866,"=E

a≤k(xu, x≤k−1, Xk)
 Xu = xu
"
REFERENCES,0.30581162324649297,"=E

a≤k(Xu, X≤k−1, Xk)
 Xu = xu, X≤k−1 = x≤k−1
"
REFERENCES,0.3062124248496994,=a≤k−1(x).
REFERENCES,0.3066132264529058,"Similarly,"
REFERENCES,0.3070140280561122,"EYk

a2
≤k(xu, x≤k−1, Yk)
 Yu = xu

=E

a2
≤k(xu, x≤k−1, Xk)
 Xu = xu
"
REFERENCES,0.30741482965931866,"=E

a2
≤k(Xu, X≤k−1, Xk)
 Xu = xu, X≤k−1 = x≤k−1
"
REFERENCES,0.30781563126252504,"=a2
≤k−1(x)."
REFERENCES,0.30821643286573147,By imposing the first assumption on C0 that C0 ≥1
REFERENCES,0.30861723446893785,"ε log

1
c∗minj∈[q] π(j) 
,"
REFERENCES,0.3090180360721443,it follows from (46) that
REFERENCES,0.3094188376753507,"a2
≤k−1(x) ≤exp(−ε(h(u) −C0 −h∗))E

a2
≤k(X)
 Xu = xu and X≤k−1 = x≤k−1
"
REFERENCES,0.3098196392785571,"By taking Conditional Expectation on both sides,"
REFERENCES,0.31022044088176354,"(EUa2
≤k−1)(x) ≤exp(−ε(h(u) −C0 −h∗))
 
EUa2
≤k

(x)."
REFERENCES,0.3106212424849699,"Finally, we apply this inequality consecutively for k ∈[s] we obtain"
REFERENCES,0.31102204408817635,(EUa)2(x) ≤exp(−ε|I′|(h(u) −C0 −h∗))(EUa2)(x).
REFERENCES,0.3114228456913828,"Corollary E.6. Fix u ∈T\L and a function fu(x) following the form described in Definition E.4. If
I, J ⊆[du] are subsets of [du] of size at least 2, then for every θ ∈[q],"
REFERENCES,0.31182364729458917,"|(Eufu,I)(θ)| ≤exp

−ε|I|"
REFERENCES,0.3122244488977956,2 (h(u) −C −h∗q
REFERENCES,0.312625250501002,"(Euf 2
u,I)(θ)
(47)"
REFERENCES,0.3130260521042084,"|(Eufu,I · fu,J)(θ)| ≤exp

−ε|I∆J|"
REFERENCES,0.31342685370741485,"2
(h(u) −C −h∗)
q"
REFERENCES,0.31382765531062123,"(Euf 2
u,I)(θ) ·
q"
REFERENCES,0.31422845691382767,"(Euf 2
u,J)(θ),
(48)"
REFERENCES,0.31462925851703405,"where C = C(M, c∗) is the constant introduced in Proposition E.5, and I∆J := (I \ J) ∪(J \ I)."
REFERENCES,0.3150300601202405,"Proof. For the first statement, it follows from Proposition E.5 with a(x) = fu,I(x) and I = I′."
REFERENCES,0.3154308617234469,"To prove the second statement, we begin by noting that the inputs of fu,I(x) and fu,J(x) do not
include
 
xv : v ∈S"
REFERENCES,0.3158316633266533,"i∈J\I Tui

and
 
xv : v ∈S"
REFERENCES,0.31623246492985974,"i∈I\J Tui

, respectively. Thus, we can apply the
Markov Property and that fact that if Y, Z, W are ind pendent then:"
REFERENCES,0.3166332665330661,"E[g(Y, Z)h(Z, W)] = E[E[g(Y, Z)h(Z, W)|Z]] = E[E[g(Y, Z)|Z]h(Z, W)]"
REFERENCES,0.31703406813627255,and this in turn becomes:
REFERENCES,0.31743486973947893,"E[E[g(Y, Z)|Z]h(Z, W)|W] = E[E[g(Y, Z)|Z]E[h(Z, W)|W]],"
REFERENCES,0.31783567134268537,"to obtain
 
Eufu,Ifu,J

(x)"
REFERENCES,0.3182364729458918,"=E

E
h
fu,I(X)
 Xv : v /∈
["
REFERENCES,0.3186372745490982,"i∈I\J
Tui
i
· E
h
fu,J(X)
 Xv : v /∈
["
REFERENCES,0.3190380761523046,"i∈J\I
Tui
i  Xv = xv : v ̸< u

."
REFERENCES,0.319438877755511,"In terms of absolute value, by Proposition E.5 we have
(Eufu,Ifu,J)(x)"
REFERENCES,0.31983967935871743,"≤E
E
h
fu,I(X)fu,J(X)
 Xv : v /∈
["
REFERENCES,0.32024048096192387,"i∈I∆J
Tui
i"
REFERENCES,0.32064128256513025,"Xv = xv : v ̸< u
"
REFERENCES,0.3210420841683367,"=E
E
h
fu,I(X)
 Xv : v /∈
["
REFERENCES,0.32144288577154306,"i∈I\J
Tui
i ·
E
h
fu,J(X)
 Xv : v /∈
["
REFERENCES,0.3218436873747495,"i∈J\I
Tui
i"
REFERENCES,0.32224448897795593,"Xv = xv : v ̸< u
 ≤E
s"
REFERENCES,0.3226452905811623,"exp(−ε|I\J|(h(u) −C −h∗)) · E
h
f 2
u,I(X)
 Xv : v /∈
["
REFERENCES,0.32304609218436875,"i∈I\J
Tui
i ·
s"
REFERENCES,0.32344689378757513,"exp(−ε|J\I|(h(u) −C −h∗)) · E
h
f 2
u,J(X)
 Xv : v /∈
["
REFERENCES,0.32384769539078156,"i∈J\I
Tui
i  Xv = xv : v ̸< u
"
REFERENCES,0.324248496993988,"≤exp

−ε"
REFERENCES,0.3246492985971944,"2|I∆J|(h(u) −C −h∗)
v
u
u
tE

E
h
f 2
u,I(X)
 Xv : v /∈
["
REFERENCES,0.3250501002004008,"i∈I\J
Tui
i  Xv = xv : v ̸< u
 ·"
REFERENCES,0.3254509018036072,"v
u
u
tE

E
h
f 2
u,J(X)
 Xv : v /∈
["
REFERENCES,0.32585170340681363,"i∈J\I
Tui
i  Xv = xv : v ̸< u
"
REFERENCES,0.32625250501002007,"= exp

−ε"
REFERENCES,0.32665330661322645,"2|I∆J|(h(u) −C −h∗)
q"
REFERENCES,0.3270541082164329,"(Euf 2
u,I)(x) · (Euf 2
u,J)(x),"
REFERENCES,0.32745490981963926,where the second to last inequality follows from Hölder’s inequality.
REFERENCES,0.3278557114228457,"Corollary E.7. There exists C = C(M, d, c∗) ≥1 so that the following holds. If u ∈T\L with
h(u) ≥h∗+ C(1 + log(R)), then for any fu(x) in the form as described in Definition E.4,"
REFERENCES,0.3282565130260521,"∀θ ∈[q], 1 2 ·
X"
REFERENCES,0.3286573146292585,"I⊂[du] : |I|≥2
(Euf 2
u,I)(θ) ≤(Euf 2
u)(θ).
(49)"
REFERENCES,0.32905811623246495,"Proof. Let C0 = C0(M, d, c∗) denote the constant introduced in the statement of the Lemma. Its
value will be determined along the proof."
REFERENCES,0.3294589178356713,"The statement of the Corollary is trivial when du < 2 since in that case Bu = ∅, implying fu = 0.
From now on, we assume du ≥2."
REFERENCES,0.32985971943887776,"First,
(Euf 2
u)(x) −
X"
REFERENCES,0.33026052104208414,"I∈[du] : |I|≥2
(Euf 2
u,I)(x) =
X"
REFERENCES,0.3306613226452906,"{I,J}
2(Eufu,I · fu,J)(x)"
REFERENCES,0.331062124248497,where P
REFERENCES,0.3314629258517034,"{I,J} refers to the sum over all unordered pairs {I, J} with I and J being distinct subsets
of [du] of size at least 2."
REFERENCES,0.33186372745490983,"We can apply (48) to estimate the absolute value of the difference.

X"
REFERENCES,0.3322645290581162,"{I,J}
2(Eufu,I · fu,J)(x) ≤
X"
REFERENCES,0.33266533066132264,"{I,J}
2 exp

−ε|I∆J|"
REFERENCES,0.3330661322645291,"2
(h(u) −CE.5 −h∗)
q"
REFERENCES,0.33346693386773546,"(Euf 2
u,I)(x) ·
q"
REFERENCES,0.3338677354709419,"(Euf 2
u,J)(x),
(50)"
REFERENCES,0.3342685370741483,"where the constant CE.5 is the constant CE.5 introduced in Proposition E.5. By 2|ab| ≤a2 + b2 for
a, b ∈R,"
Q,0.3346693386773547,"2
q"
Q,0.33507014028056115,"(Euf 2
u,I)(x) ·
q"
Q,0.3354709418837675,"(Euf 2
u,J)(x) ≤(Euf 2
u,I)(x) + (Euf 2
u,J)(x)."
Q,0.33587174348697396,"Hence,"
Q,0.33627254509018034,"(50) ≤
X"
Q,0.3366733466933868,"I⊂[du] : |I|≥2
(Euf 2
u,I)(x)

X"
Q,0.3370741482965932,"J⊂[du] : I̸=J
exp

−ε|I∆J|"
Q,0.3374749498997996,"2
(h(u) −CE.5 −h∗)

."
Q,0.337875751503006,"With |{J ⊆[du] : |I∆J| = i}| =
 du
i

≤di
u, X"
Q,0.3382765531062124,"J⊂[du] : I̸=J
exp

−ε|I∆J|"
Q,0.33867735470941884,"2
(h(u) −CE.5 −h∗)

≤ ∞
X"
Q,0.3390781563126252,"i=1
di
u exp

−εi"
Q,0.33947895791583166,"2 (h(u) −CE.5 −h∗)

≤1/4, (51)"
Q,0.3398797595190381,provided that h(u) −CE.5 −log(du)
Q,0.3402805611222445,"ε
−h∗≥16 ε ."
Q,0.3406813627254509,"Now, we impose the first assumption on C0 that"
Q,0.3410821643286573,"C0 ≥CE.5 + log(d) ε
+ 1"
Q,0.3414829659318637,"ε + 16 ε , then"
Q,0.34188376753507016,h(u) ≥h∗+ C0(log(R) + 1) ≥CE.5 + log(du)
Q,0.34228456913827654,"ε
+ 16"
Q,0.342685370741483,ε + h∗.
Q,0.34308617234468936,"Hence,
(Euf 2
u)(x) −
X"
Q,0.3434869739478958,"I∈[d] : |I|≥2
(Euf 2
u,I)(x)
 ≤1 4 X"
Q,0.3438877755511022,"I∈[d] : |I|≥2
(Euf 2
u,I)(x)
(52)"
Q,0.3442885771543086,and the proof follows.
Q,0.34468937875751504,"Proof of Proposition E.1. Let C0 = C0(M, d, c∗) denote the constant introduced in the statement
of the Proposition. Its precise value will be determined along the proof. From Remark E.3, it is
sufficient to consider the case when |du| ≥2. Further, it suffices to prove in the case when"
Q,0.3450901803607214,"h(u) ≥h∗+ C0(log(R) + 1),
(53)"
Q,0.34549098196392786,since otherwise the statements follow from either Cauchy-Schwarz or Jenson’s inequality.
Q,0.3458917835671343,Part I: Derivation of (37) and (38).
Q,0.34629258517034067,"First, by (47),"
Q,0.3466933867735471,"(Eufu)2(θ)) =

X"
Q,0.3470941883767535,"I⊆[du] : |I|≥2
(Eufu,I)(θ)
2
(54) ≤

X"
Q,0.3474949899799599,"I⊆[du] : |I|≥2
exp

−ε|I|"
Q,0.34789579158316636,"2 (h(u) −CE.5 −h∗)

·
q"
Q,0.34829659318637274,"(Euf 2
u,I)(θ)
2 ≤

X"
Q,0.3486973947895792,"I⊆[du]:|I|≥2
exp

−ε|I|(h(u) −CE.5 −h∗)

·

X"
Q,0.34909819639278555,"I⊆[du]:|I|≥2
(Euf 2
u,I)(θ)

,"
Q,0.349498997995992,"where we applied Cauchy-Schwarz inequality in the last inequality; the constant CE.5 is the constant
C introduced in Proposition E.5."
Q,0.3498997995991984,"With the coarse estimate

I ⊆[du] : |I| = t
	 =
du
i"
Q,0.3503006012024048,"
≤dt
u ≤(Rd)t,"
Q,0.35070140280561124,"we have

X"
Q,0.3511022044088176,"I⊆[du]:|I|≥2
exp

−ε|I|(h(u) −CE.5 −h∗)
 ≤ ∞
X"
Q,0.35150300601202406,"t=2
exp

−εt

h(u) −CE.5 −log(R) + log(d)"
Q,0.35190380761523044,"ε
−h∗
.
(55)"
Q,0.35230460921843687,"The geometric series above is finite if h(u) is large enough, and this can be achieved by imposing
assumption of C0 and relying on (53). Now, let us impose the first assumption on C0:"
Q,0.3527054108216433,"C0 ≥CE.5 + (2 + 2 log(d) + 100)/ε.
(56)"
Q,0.3531062124248497,"Then, by (53) we have"
Q,0.3535070140280561,h(u) ≥h∗+ C0(log(R) + 1) ≥h∗+ CE.5 + 2log(R) + log(d)
Q,0.3539078156312625,"ε
+ 100 ε ,"
Q,0.35430861723446894,which in term implies the R.H.S. of (55) is
Q,0.35470941883767537,"exp

−2ε

h(u) −CE.5 −log(R)+log(d)"
Q,0.35511022044088175,"ε
−h∗"
Q,0.3555110220440882,"1 −exp

−ε

h(u) −CE.5 −log(R)+log(d)"
Q,0.35591182364729457,"ε
−h∗
"
Q,0.356312625250501,"≤
exp

−2ε

h(u) −CE.5 −log(R)+log(d)"
Q,0.35671342685370744,"ε
−h∗"
Q,0.3571142284569138,1 −e−100
Q,0.35751503006012025,"≤2 exp

−2ε

h(u) −CE.5 −log(R) + log(d)"
Q,0.35791583166332663,"ε
−h∗ =1"
EXP,0.35831663326653307,"4 exp

−2ε

h(u) −CE.5 −log(R) + log(d)"
EXP,0.3587174348697395,"ε
−h∗−log(8) 2ε  ≤1"
EXP,0.3591182364729459,"4 exp

−2ε
 
h(u) −C0(log(R) + 1) −h∗"
EXP,0.3595190380761523,"Substituting the above estimate into (54), together with (49) we have"
EXP,0.3599198396793587,(Eufu)2(θ) ≤1
EXP,0.36032064128256514,"4 exp

−2ε
 
h(u) −C0(log(R) + 1) −h∗
·

X"
EXP,0.36072144288577157,"I⊆[du]:|I|≥2
(Euf 2
u,I)(θ)
 ≤1"
EXP,0.36112224448897795,"2 exp

−2ε
 
h(u) −C0(log(R) + 1) −h∗
(Euf 2
u)(θ).
(57)"
EXP,0.3615230460921844,"Therefore, we have derived an inequality which is slightly stronger than (37)."
EXP,0.36192384769539077,"To derive (38), let us first show Efu(X) is relatively small using (57) and Jesnon’s inequality:
 
E[fu(X)]
2 ≤E

(Eufu)2(X)

≤1"
EXP,0.3623246492985972,"2 exp

−2ε
 
h(u) −C0(log(R) + 1) −h∗
E

f 2
u(X)
 ≤1"
E,0.3627254509018036,"2E

f 2
u(X)

."
E,0.36312625250501,"Thus, the variance and the second moment of fu(X) are the same up to a factor of 2:"
E,0.36352705410821645,"E
 ˜f 2
u(X)

= E

f 2
u(X)

−
 
E[fu(X)]
2 ≥1"
E,0.36392785571142283,"2E

f 2
u(X)

.
(58)"
E,0.36432865731462927,We conclude that
E,0.36472945891783565,"E

(Eu ˜fu)2(X)

≤E

(Eufu)2(X)
 ≤1"
EXP,0.3651302605210421,"2 exp

−2ε
 
h(u) −C0(log(R) + 1) −h∗
E

f 2
u(X)
"
EXP,0.3655310621242485,"≤exp(−2ε(h(u) −C0(1 + log(R)) −h∗))E
 ˜f 2
u(X)

."
EXP,0.3659318637274549,"Therefore, we complete the proof of (38)."
EXP,0.36633266533066133,Part II: Derivation of (39).
EXP,0.3667334669338677,"It remains to show (39) and the proof is similar. Fix I ⊂[du] with |I| ≥2, let I′ = I\{i} and we
represent x ∈[q]T as (x0, x1), where"
EXP,0.36713426853707415,"x0 :=
 
xv : v /∈
["
EXP,0.3675350701402806,"j∈I′
Tuj

and
x1 :=
 
xv : v ∈
["
EXP,0.36793587174348696,"j∈I′
Tuj

."
EXP,0.3683366733466934,"With this notation, we have a(x) = a(x0). Thus,"
EXP,0.3687374749498998,"E

| ˜fu,I(X)a(X)|

=E
hE
 ˜fu,I(X)
 X0

· a(X0)

i ≤ r"
EXP,0.3691382765531062,"E
h 
E
 ˜fu,I(X)
 X0
2i
·
q"
EXP,0.36953907815631265,"E

a2(X0)
 ≤ r"
EXP,0.36993987975951903,"E
h 
E

fu,I(X)
 X0
2i
·
q"
EXP,0.37034068136272547,"E

a2(X0)
"
EXP,0.37074148296593185,"≤exp

−ε"
EXP,0.3711422845691383,"2|I \ {i}|(h(u) −CE.5 −h∗)
q"
EXP,0.3715430861723447,"E

f 2
u,I(X)

·
q"
EXP,0.3719438877755511,"E

a2(X)

,"
EXP,0.37234468937875753,"where the last inequality follows from Proposition E.5. Hence,"
EXP,0.3727454909819639,"E

| ˜fu(X)a(X)|
 ≤
X"
EXP,0.37314629258517035,"I⊆[du] : |I|≥2
exp

−ε"
EXP,0.3735470941883767,"2|I \ {i}|(h(u) −CE.5 −h∗)
q"
EXP,0.37394789579158316,"E

f 2
u,I(X)

·
q"
EXP,0.3743486973947896,"E

a2(X)
 ≤

X"
EXP,0.374749498997996,"I⊆[du] : |I|≥2
exp
 
−ε|I \ {i}|(h(u) −CE.5 −h∗)
1/2
·
s
X"
EXP,0.3751503006012024,"I⊆[du] : |I|≥2
E

f 2
u,I(X)

·
q"
EXP,0.3755511022044088,"E

a2(X)

. (59)"
EXP,0.37595190380761523,"Next, we impose the second assumption on C0 that"
EXP,0.37635270541082166,"C0 ≥CE.7,"
EXP,0.37675350701402804,"where CE.7 is the constant introduced in Corollary E.7. Together our assumption h(u) ≥h∗+
C0(log(R) + 1) at the beginning of the proof, we can apply the Corollary and (58) to get
X"
EXP,0.3771543086172345,"I⊆[du] : |I|≥2
Ef 2
u,I(X) ≤2E(fu(X))2 ≤4E( ˜fu(X))2.
(60)"
EXP,0.37755511022044086,"Repeating the same argument as in the proof of (38) and relying on the assumption (56) of C0, X"
EXP,0.3779559118236473,"I⊆[du] : |I|≥2
exp(−ε|I\{i}|(h(u) −CE.5 −h∗)) ≤ ∞
X"
EXP,0.37835671342685373,"t=1
exp

−εt

h(u) −CE.5 −h∗−2log(Rd) ε  ≤1"
EXP,0.3787575150300601,"4 exp

−ε

h(u) −C0(log(R) + 1) −h∗
. (61)"
EXP,0.37915831663326655,"Therefore, combining (60), (61), and (59) we get"
EXP,0.3795591182364729,"E

| ˜fu(X)a(X)|

≤exp

−ε"
EXP,0.37995991983967936,"2
 
h(u) −C0(log(R) + 1) −h∗
·
q"
EXP,0.3803607214428858,"E
 ˜f 2u(X)

·
q"
EXP,0.3807615230460922,"E

a2(X)

."
EXP,0.3811623246492986,"F
Induction Step 2: Decay of fk and the proof of Theorem B.6"
EXP,0.381563126252505,"As a continuation of the inductive step, we adapt the notation introduced in the previous section.
Building on the properties of an single component fu from Proposition E.1, our objective is to deduce
variance and covariance decay of fk, which is stated in Proposition F.1 below. Once it is established,
we will be ready to prove Theorem B.6."
EXP,0.3819639278557114,"F.1
Properties of fk"
EXP,0.38236472945891786,"The main goal of this subsection is to derive the following Proposition.
Proposition F.1. There exists C = C(M, d, c∗) ≥1 so that the following holds. For any ρ′ ∈T
satisfying
h(ρ′) ≥h∗+ C(1 + log(R))."
EXP,0.38276553106212424,Fix a positive integer k1 such that
EXP,0.3831663326653307,h(ρ′) ≥k1 ≥h∗+ C(1 + log(R)).
EXP,0.38356713426853706,Consider a function
EXP,0.3839679358717435,"f(x) =c +
X"
EXP,0.3843687374749499,"σ∈F(B≤ρ′)
cσϕσ(x)"
EXP,0.3847695390781563,"with Ef(X) = 0. We decompose f according to Lemma D.1 with the given k1. Then, the following
holds:"
EXP,0.38517034068136274,"• for k ∈[k1 + 1, h(ρ′)],"
EXP,0.3855711422845691,"E

(Eρ′fk)2(Xρ′)

≤exp

−ε
 
h(ρ′) + k −C(log(R) + 1) −2h∗
Ef 2
k(X),
(62)"
EXP,0.38597194388777556,"• for k = k1,"
EXP,0.38637274549098194,"E

(Eρ′fk1)2(Xρ′)

≤exp

−ε
 
h(ρ′) −k −C(log(R) + 1)

Ef 2
k1(X), and
(63)"
EXP,0.3867735470941884,"• for k1 ≤m < k ≤h(ρ′),"
EXP,0.3871743486973948,"E

fk(X)fm(X)
 ≤exp

−ε"
EXP,0.3875751503006012,"2
 
k −C(log(R) + 1) −h∗q"
EXP,0.3879759519038076,"E

f 2
k(X)

E

f 2m(X)

, (64)"
EXP,0.388376753507014,"Before we prove the Proposition, let us prove the following second moment bounds for the partial
sums of ˜fu.
Lemma F.2. There exists a constant C = C(M, d, c∗) ≥1 so that the following holds. Consider the
same description as stated in Proposition F.1 and k1 ≥h∗+ C(log(R) + 1). Let (h, k) be a pair of
integers satisfying k1 < h ≤k ≤l′. For u ∈Dk(ρ′), let"
EXP,0.38877755511022044,"fh,u(x) =
X"
EXP,0.3891783567134269,v∈Dh(u)
EXP,0.38957915831663326,˜fv(x).
EXP,0.3899799599198397,"In other words,
fh(x) =
X"
EXP,0.39038076152304607,"u∈Dk(ρ′)
fh,u(x)."
EXP,0.3907815631262525,"The following holds: First, for u ∈Dk(ρ′), 1
2 X"
EXP,0.39118236472945894,"v∈Dh(u)
E ˜f 2
v (X) ≤Ef 2
h,u(X) ≤2
X"
EXP,0.3915831663326653,"v∈Dh(u)
E ˜f 2
v (X)."
EXP,0.39198396793587176,"Second,
1
4 X"
EXP,0.39238476953907814,"u∈Lk(ρ′)
Ef 2
h,u(X) ≤Ef 2
h(X) ≤4
X"
EXP,0.3927855711422846,"u∈Lk(ρ′)
Ef 2
h,u(X)."
EXP,0.393186372745491,"Proof. Let C0 = C0(M, d, ε′) denote the constant introduced in the statement of the Lemma. Its
precise value will be determined along the proof."
EXP,0.3935871743486974,"Let us fix u ∈Dk(ρ′). Consider the following conditional expectation of fh,u(x)."
EXP,0.3939879759519038,"(Ehfh,u)(x) = E

fh,u(X)
 Xv = xv : h(v) ≥h

=
X"
EXP,0.3943887775551102,"v∈Dh(ρ′)
(Ev ˜fv)(xv)."
EXP,0.39478957915831664,"Comparing the second moments of fh,u(x) = P
v∈Dh(u) ˜fv(x) and P
v∈Dh(u)(Ev ˜fv)(xv) we get"
EXP,0.395190380761523,"E
h
X"
EXP,0.39559118236472945,v∈Dh(u)
EXP,0.3959919839679359,"˜fv(X)
2i
=
X"
EXP,0.39639278557114227,"v∈Dh(u)
E
 ˜f 2
v (X)

+
X"
EXP,0.3967935871743487,"v,v′∈Dh(u) : v̸=v′
E
 ˜fv(X) ˜fv′(X)
 =
X"
EXP,0.3971943887775551,"v∈Dh(u)
E
 ˜f 2
v (X)

+
X"
EXP,0.3975951903807615,"(v,v′)∈(Dh(u))2 : v̸=v′
E

E
h
(Ev ˜fv)(X)(Ev′ ˜fv′)(X)
 Xp(v,v′)
i"
EXP,0.39799599198396796,"=E
h
X"
EXP,0.39839679358717434,"v∈Dh(u)
(Ev ˜fv)(X)
2i
+
X"
EXP,0.39879759519038077,v∈Dh(u)
EXP,0.39919839679358715,"
E
 ˜f 2
v (X)

−E

(Ev ˜fv)2(X)
 ≥
X"
EXP,0.3995991983967936,v∈Dh(u)
EXP,0.4,"
1 −exp

−ε
 
h −CE.1(1 + log(R)) −h∗
E
 ˜f 2
v (X)

, (65)"
EXP,0.4004008016032064,"where the last inequality follow from Proposition E.1 and CE.1 is the constant C introduced in
Proposition E.1."
EXP,0.40080160320641284,Here we impose the first assumption on C0:
EXP,0.4012024048096192,"C0 > 10 max{ε−1, CE.1}."
EXP,0.40160320641282565,"Then, due to k1 ≥h∗+ C0(log(R) + 1), we have"
EXP,0.4020040080160321,"exp(−ε(h−CE.1(1+log(R))−h∗))) ≤exp(−ε(k1−CE.1(1+log(R))−h∗))) ≤exp(−ε·0.9C0) ≤1/2,"
EXP,0.40240480961923847,and thus (65) can be simplified to
EXP,0.4028056112224449,"E

f 2
h,u(X)

≥1 2 X"
EXP,0.4032064128256513,"v∈Dh(u)
E
 ˜f 2
v (X)

.
(66)"
EXP,0.4036072144288577,"With the lower bound been established, the upper bound can also be derived in the same fashion. Let
us first recycle the first three lines of (65):"
EXP,0.40400801603206415,"E
h
X"
EXP,0.40440881763527053,v∈Dh(u)
EXP,0.40480961923847697,"˜fv(X)
2i
=E

X"
EXP,0.40521042084168335,"v∈Dh(u)
(Ev ˜fv)(X)
2
+
X"
EXP,0.4056112224448898,"v∈Dh(u)
E( ˜fv(X))2 −E(Ev ˜fv)2(X)"
EXP,0.40601202404809617,"≤E

X"
EXP,0.4064128256513026,"v∈Dh(u)
(Ev ˜fv)(X)
2
+
X"
EXP,0.40681362725450904,"v∈Dh(u)
E( ˜fv(X))2."
EXP,0.4072144288577154,"Notice that we can apply Lemma C.2 for the first summand in the above expression. E

X"
EXP,0.40761523046092185,"v∈Dh(u)
(Ev ˜fv)(X)
2
= E

X"
EXP,0.40801603206412823,"v∈Dh(u)
(Ev ˜fv)(Xv)
2
≤CC.2R
X"
EXP,0.40841683366733467,"v∈Dh(u)
(Ev ˜fv)2(Xv)"
EXP,0.4088176352705411,"where CC.2 is the constant introduced in Lemma C.2. Again, applying the estimate from Proposition
E.1 we have"
EXP,0.4092184368737475,"CC.2R
X"
EXP,0.4096192384769539,"v∈Dh(u)
(Ev ˜fv)2(Xv) ≤CC.2R exp

−2ε
 
h−CE.1(1+log(R))−h∗
X"
EXP,0.4100200400801603,"v∈Dh(u)
E ˜f 2
v (X)."
EXP,0.41042084168336673,Here we impose the second assumption on C0 that
EXP,0.41082164328657317,C0 ≥CE.1 + 1 + log(CC.2)
EXP,0.41122244488977955,"2ε
.
(67)"
EXP,0.411623246492986,"Then, relying on h > k1 ≥h∗+ C0(log(R) + 1),"
EXP,0.41202404809619236,"CC.2R exp

−2ε
 
h −CE.1(1 + log(R)) −h∗"
EXP,0.4124248496993988,"≤exp

−2ε

h −CE.1(1 + log(R)) −h∗−log(CC.2) + log(R) 2ε  ≤1,"
EXP,0.41282565130260523,"which in turn implies
E(fh,u(X))2 ≤2
X"
EXP,0.4132264529058116,"v∈Dh(u)
E( ˜fv(X))2."
EXP,0.41362725450901805,"Now it remains to show the second statement. Notice that fh = fh,ρ′, we immediately have 1
2 X"
EXP,0.41402805611222443,"v∈Dh(ρ′)
E ˜f 2
v (X) ≤Ef 2
h(X) ≤2
X"
EXP,0.41442885771543087,"v∈Dh(ρ′)
E ˜f 2
v (X)"
EXP,0.4148296593186373,"Together with 1
2 X"
EXP,0.4152304609218437,"u∈Dk(ρ′)
Ef 2
k,u(X) ≤
X"
EXP,0.4156312625250501,"v∈Dh(ρ′)
E ˜f 2
v (X) ≤2
X"
EXP,0.4160320641282565,"u∈Dk(ρ′)
Ef 2
k,u(X),"
EXP,0.41643286573146293,the second statement of the lemma follows.
EXP,0.4168336673346693,"Proof of Proposition F.1. Let C0 = C0(M, d, c∗) denote the constant introduced in the statement of
the Lemma. Its precise value will be determined along the proof. Let us make the first assumption
on C0 that
C0 ≥CF.2,
where CF.2 is the constant introduced in Lemma F.2. Now, we could apply the statements of the
Lemma."
EXP,0.41723446893787575,Part 1: Derivation of (62).
EXP,0.4176352705410822,"Fix k ∈[k1 + 1, l′]. Applying Lemma F.2 with the parameters h and k in the Lemma setting to be k,"
EXP,0.41803607214428856,E(fk(X))2 ≥1 2 X
EXP,0.418436873747495,"u∈Dk(ρ′)
E( ˜fu(X))2.
(68)"
EXP,0.4188376753507014,"The next step is to compare the sum of E ˜f 2
u(X) with E

(Eρ′fk)2(Xρ′)

. By Jenson’s inequality,"
EXP,0.4192384769539078,"E

(Eρ′fk)2(Xρ′)

=E
h
X"
EXP,0.41963927855711425,"u∈Dk(ρ′)
(Eρ′ ˜fu)(Xρ′)
2i"
EXP,0.42004008016032063,"≤E
h
|Dk(ρ′)|
X"
EXP,0.42044088176352706,"u∈Dk(ρ′)
(Eρ′ ˜fu)2(Xρ′)
i"
EXP,0.42084168336673344,"=|Dk(ρ′)|
X"
EXP,0.4212424849699399,"u∈Dk(ρ′)
E

(Eρ′ ˜fu)2(Xρ′)

."
EXP,0.4216432865731463,"For each summand, we can apply (8) from Lemma A.5 to get the following estimate."
EXP,0.4220440881763527,"E

(Eρ′ ˜fu)2(Xρ′)

≤CA.5(l′ −k)2qλ2(l′−k)E

(Eu ˜fu)2(Xu)
"
EXP,0.42244488977955913,"where CA.5 is the constant introduced in the Lemma. Together with |Dk(ρ′)| ≤Rdl′−k from the
assumption on T and dλ2 ≤exp(−1.1ε) from the definiton of ε,"
EXP,0.4228456913827655,"E

(Eρ′fk)2(Xρ′)

≤CA.5(l′ −k)2qR exp(−1.1ε(l′ −k))
X"
EXP,0.42324649298597194,"u∈Dk(ρ′)
E

(Eu ˜fu)2(Xu)
 ≤1"
EXP,0.4236472945891784,"2 exp

−ε

l′ −C′(1 + log(R)) −k

X"
EXP,0.42404809619238476,"u∈Dk(ρ′)
E

(Eu ˜fu)2(Xu)

(69)"
EXP,0.4244488977955912,"where we set
C′ = 1 + log
 
1 + 2CA.5 max
n∈N n2q exp(−0.1εn)

< +∞."
EXP,0.4248496993987976,By Proposition E.1 we have
EXP,0.425250501002004,"E

(Eu ˜fu)2(X)

≤exp(−2ε(k −CE.1(1 + log(R)) −h∗))E( ˜fu(X))2,
(70)"
EXP,0.42565130260521045,"where CE.1 is the constant C introduced in the Proposition. Substituting this inequality into (69),
together with (68) from first step,"
EXP,0.4260521042084168,"E

(Eρ′fk)2(Xρ′)

≤1"
EXP,0.42645290581162326,"2 exp

−ε

l′ + k −(C′ + CE.1)(log(R) + 1) −2h∗
X"
EXP,0.42685370741482964,"u∈Dk(ρ′)
E( ˜fu(X))2"
EXP,0.4272545090180361,≤exp(−ε(l′ + k −(C′ + CE.1)(log(R) + 1) −2h∗))E(fk(X))2.
EXP,0.4276553106212425,"Now, we impose the second assumption on C0 that"
EXP,0.4280561122244489,"C0 ≥(C′ + CE.1),"
EXP,0.42845691382765533,we finished the proof of (62).
EXP,0.4288577154308617,Part 2: Derivation of (63).
EXP,0.42925851703406814,Let us consider
EXP,0.4296593186372745,"hk1(x) := (Ek1fk1)(x) = E

fk1(X)
 Xu = xu : u ∈Dk1(ρ′)

."
EXP,0.43006012024048096,"In other words, we may view hk1(x) as a linear function with variables xu for u ∈Dk1(ρ′) with
Ehk1(X) = Efk1(X) = 0. Then,"
EXP,0.4304609218436874,"E

(Eρ′fk1)2(Xρ′)

= E

(Eρ′hk1)2(Xρ′)

≤exp
 
−ε(h(ρ′) −k1 −CB.5)

Eh2
k1(X)"
EXP,0.4308617234468938,"≤exp
 
−ε(h(ρ′) −k1 −CB.5)

Ef 2
k1(X)"
EXP,0.4312625250501002,"The first inequality follows from Proposition B.5. The second inequality follows from Jensen’s
inequality. Here we impose the third assumption on C0 that"
EXP,0.4316633266533066,"C0 ≥CB.5,"
EXP,0.432064128256513,the derivation of (63) follows.
EXP,0.43246492985971946,Part 3: Derivation of (64)
EXP,0.43286573146292584,"For w ≤ρ′ with m ≤h(w) ≤k, let"
EXP,0.4332665330661323,"fm,w(x) =
X"
EXP,0.43366733466933866,v∈Dk(w)
EXP,0.4340681362725451,˜fv(x).
EXP,0.4344689378757515,"Let us make a remark that either by second property of f from Lemma D.1 when m = k1 or by
Lemma F.2 in the case when m > k1, we have the following: For w ≤ρ′ and m ≤k′ ≤h(w),

X"
EXP,0.4348697394789579,"u∈Dk′(w)
Ef 2
m,u(X)
1/2
≤CD.1R2 
Ef 2
m,w(X)
1/2.
(71)"
EXP,0.43527054108216434,"With this notation,"
EXP,0.4356713426853707,"Efk(X)fm(X) =
X"
EXP,0.43607214428857716,"u∈Dk(ρ′)
E ˜fu(X)fm,u(X) +
X"
EXP,0.4364729458917836,"u,u′∈Dk(ρ′) : u̸=u′
E ˜fu(X)fm,u′(X) =
X"
EXP,0.43687374749499,"u∈Dk(ρ′)
E ˜fu(X)fm,u(X) +
X"
EXP,0.4372745490981964,"u,u′∈Dk(ρ′) : u̸=u′
E

E
h
(Eu ˜fu)(X)(Eu′fm,u′)(X)
 Xp(u,u′)
i"
EXP,0.4376753507014028,"=E
h
X"
EXP,0.4380761523046092,"u∈Dk(ρ′)
(Eu ˜fu)(X)

X"
EXP,0.43847695390781566,"u∈Dk(ρ′)
(Eufm,u)(X)
i
+
X"
EXP,0.43887775551102204,"u∈Dk(ρ′)
E ˜fu(X)fm,u(X)
(72) −
X"
EXP,0.4392785571142285,"u∈Dk(ρ′)
E
h
(Eu ˜fu)(X)(Eufm,u)(X)
i
."
EXP,0.43967935871743485,We will estimate the three summands individually.
EXP,0.4400801603206413,"Part 3.1: Estimating first summand of (72) First, we apply Cauchy-Schwarz inequality,
E
h
(
X"
EXP,0.44048096192384767,"u∈Dk(ρ′)
(Eu ˜fu)(X))(
X"
EXP,0.4408817635270541,"u∈Dk(ρ′)
(Eufm,u)(X))
i =
E
h
(Ekfk)(X)(Ekfm)(X)
i ≤
q"
EXP,0.44128256513026054,"E

(Ekfk)2(X)
q"
EXP,0.4416833667334669,"E

(Ekfm)2(X)

.
(73)"
EXP,0.44208416833667336,"Now, combining (70) and (68), we have
q"
EXP,0.44248496993987974,"E

(Ekfk)2(X)

≤
√"
EXP,0.44288577154308617,"2 exp
 
−ε
 
k −CE.1(1 + log(R)) −h∗
(Ef 2
k(X))1/2.
(74)"
EXP,0.4432865731462926,By setting
EXP,0.443687374749499,C1 = CE.1 + 1 ε 1
EXP,0.4440881763527054,"2 log(2) + log(2CD.1) + 2

,"
EXP,0.4444889779559118,"we can conclude that E
h
(
X"
EXP,0.44488977955911824,"u∈Dk(ρ′)
(Eu ˜fu)(X))(
X"
EXP,0.4452905811623247,"u∈Dk(ρ′)
(Eufm,u)(X))
i"
EXP,0.44569138276553105,"≤exp(−ε(k −C1(log(R) + 1) −h∗))
q"
EXP,0.4460921843687375,"Ef 2
k(X)Ef 2m(X),
(75)"
EXP,0.44649298597194387,"Part 3.2: Estimating second summand of (72) For the second summand of (72), we begin with the
estimate for each u ∈Dk(ρ′):"
EXP,0.4468937875751503,"E| ˜fu(X)fm,u(X)| ≤
X"
EXP,0.44729458917835674,"i∈[du]
E| ˜fu(X)fm,ui(X)|."
EXP,0.4476953907815631,"Since for each i ∈[du] we have fm,ui(x) = fm,ui(x≤ui), we apply (39) from Proposition E.1 to ˜fu
and a(x) = fm,ui(x) to get
X"
EXP,0.44809619238476955,"i∈[du]
E| ˜fu(X)fm,ui(X)| ≤
X"
EXP,0.44849699398797593,"i∈[du]
exp

−ε"
EXP,0.44889779559118237,"2
 
k −CE.1(log(R) + 1) −h∗
(E ˜f 2
u(X))1/2(Ef 2
m,ui(X))1/2,"
EXP,0.4492985971943888,"where CE.1 is the constant introduced in the Proposition. Applying Jensen’s inequality and (71) with
w = u and k′ = k −1,
X"
EXP,0.4496993987975952,"i∈[du]
(Ef 2
m,ui(X))1/2 ≤d1/2
u
 X"
EXP,0.4501002004008016,"i∈[du]
Ef 2
m,ui(X)
1/2
≤(Rd)1/2CD.1R2 
Ef 2
m,u(X)
1/2."
EXP,0.450501002004008,"Hence,"
EXP,0.45090180360721444,"E| ˜fu(X)fm,u(X)| ≤(Rd)1/2CD.1R2 exp

−ε"
EXP,0.4513026052104208,"2
 
k −CE.1(log(R) + 1) −h∗
(E ˜f 2
u(X))1/2 
Ef 2
m,u(X)
1/2"
EXP,0.45170340681362725,"≤exp

−ε"
EXP,0.4521042084168337,"2
 
k −C2(log(R) + 1) −h∗
(E ˜f 2
u(X))1/2(Ef 2
m,u(X))1/2, (76)"
EXP,0.45250501002004007,"where
C2 = 2 ε 3 2 + 1"
EXP,0.4529058116232465,"2 log(d) + log(CD.1)

+ CE.1."
EXP,0.4533066132264529,"Now, returning to the summation, we apply (76) and Cauchy-Schwarz inequality to get

X"
EXP,0.4537074148296593,"u∈Dk(ρ′)
E ˜fu(X)fm,u(X) ≤
X"
EXP,0.45410821643286575,"u∈Dk(ρ′)
exp

−ε"
EXP,0.45450901803607213,"2(k −C2(log(R) + 1) −h∗)

(E ˜f 2
u(X))1/2(Ef 2
m,u(X))1/2"
EXP,0.45490981963927857,"≤exp

−ε"
EXP,0.45531062124248495,"2(k −C2(log(R) + 1) −h∗)

X"
EXP,0.4557114228456914,"u∈Dk(ρ′)
E ˜f 2
u(X)
1/2
X"
EXP,0.4561122244488978,"u∈Dk(ρ′)
Ef 2
m,u(X)
1/2"
EXP,0.4565130260521042,"≤2CD.1R2 exp

−ε"
EXP,0.45691382765531063,"2(k −C2(log(R) + 1) −h∗)
 
Ef 2
k(X)
1/2 
Ef 2
m(X)
1/2"
EXP,0.457314629258517,"≤exp

−ε"
EXP,0.45771543086172345,"2(k −C3(log(R) + 1) −h∗)
q"
EXP,0.4581162324649299,"Ef 2
k(X)Ef 2m(X).
(77)"
EXP,0.45851703406813626,"In the derivation above, we applied Lemma F.2 for the term
 P"
EXP,0.4589178356713427,"u∈Dk(ρ′) E ˜f 2
u(X)
1/2
and (71) for"
EXP,0.4593186372745491,"the term
 P"
EXP,0.4597194388777555,"u∈Dk(ρ′) Ef 2
m,u(X)
1/2
in the secont to last inequality. The constant C2 in the last
inequality is defined as"
EXP,0.46012024048096195,C3 = 2
EXP,0.46052104208416833,ε(log(2CD.1) + 2) + C2.
EXP,0.46092184368737477,"Part 3.3: Estimating third summand of (72) It remains to bound the third summand, it can be
reduced to the upper bound for first summand. Applying the Cauchy-Schwarz inequality and Hölder’s
inequality we have

X"
EXP,0.46132264529058115,"u∈Dk(ρ′)
E
h
(Eu ˜fu)(X)(Eufm,u)(X)
i"
EXP,0.4617234468937876,"≤E
h
X"
EXP,0.46212424849699396,"u∈Dk(ρ′)
(Eu ˜fu)(X)(Eufm,u)(X)

i"
EXP,0.4625250501002004,"≤E
h
X"
EXP,0.46292585170340683,"u∈Dk(ρ′)
(Eu ˜fu)2(X)
1/2
X"
EXP,0.4633266533066132,"u∈Dk(ρ′)
(Eufm,u)2(X)
1/2i"
EXP,0.46372745490981965,"≤

E
X"
EXP,0.464128256513026,"u∈Dk(ρ′)
(Eu ˜fu)2(X)
1/2
E
X"
EXP,0.46452905811623246,"u∈Dk(ρ′)
(Eufm,u)2(X)
1/2 ≤
√"
EXP,0.4649298597194389,"2 exp
 
−ε
 
k −CE.1(1 + log(R)) −h∗
· CD.1R2q"
EXP,0.4653306613226453,"Ef 2
k(X)Ef 2m(X)."
EXP,0.4657314629258517,where in the last inequality we applied (74) and (71).
EXP,0.4661322645290581,By setting
EXP,0.46653306613226453,C4 = 1 ε 1
EXP,0.46693386773547096,"2 log(2) + log(CD.1) + 2

,"
EXP,0.46733466933867734,"we conclude that

X"
EXP,0.4677354709418838,"u∈Dk(ρ′)
E
h
(Eu ˜fu)(X)(Eufm,u)(X)
i ≤exp
 
−ε
 
k −C4(1 + log(R)) −h∗q"
EXP,0.46813627254509016,"Ef 2
k(X)Ef 2m(X). (78)"
EXP,0.4685370741482966,"Now, combining the three estimates of the summands (75), (77), and (78) for (72) we conclude that"
EXP,0.46893787575150303,"|Efk(X)fm(X)| ≤exp

−ε"
EXP,0.4693386773547094,"2
 
k −C5(log(R) + 1) −h∗q"
EXP,0.46973947895791585,"Ef 2
k(X)Ef 2m(X),"
EXP,0.4701402805611222,"where
C5 := 2"
EXP,0.47054108216432866,"ε log(3) + max{C1, C3, C4}."
EXP,0.4709418837675351,"Now we impose the forth assumption on C0 that
C0 ≥C5,
and (64) follows."
EXP,0.4713426853707415,"F.2
Proof of Theorem B.6"
EXP,0.4717434869739479,"Let C0 = C0(M, d, c∗) denote the constant introduced in the statement of the Lemma. Its precise
value will be determined along the proof."
EXP,0.4721442885771543,"Let k1 be a positive integer with the precise value to be determined later. Here we impose our first
assumption on k1 that
k1 ≥CF.1(log(R) + 1) + h∗"
EXP,0.4725450901803607,where CF.1 is a constant that appears in Proposition F.1.
EXP,0.4729458917835671,"Now, let us consider a function f described in the Theorem. Without lose of generality, we may
assume Ef(X) = 0. Then, it is equivalent to estimate the second moments."
EXP,0.47334669338677354,"Further, let us assume h(ρ′) ≥k1 and the decomposition of f according to Lemma D.1:"
EXP,0.47374749498998,"f(x) =
X"
EXP,0.47414829659318636,"k∈[k1,h(ρ′)]
fk(X).
(79)"
EXP,0.4745490981963928,"Our first goal is to show
Ef(X)2 ≃
X"
EXP,0.4749498997995992,"k∈[k1,h(ρ′)]
f 2
k(X),"
EXP,0.4753507014028056,by showing Efk(X)fm(X) is insignificant whenever k ̸= m.
EXP,0.47575150300601204,"For k1 ≤m < k, by Propostion F.1,"
EXP,0.4761523046092184,"2|Efk(X)fm(X)| ≤2 exp

−ε"
EXP,0.47655310621242486,"2
 
k −CF.1(log(R) + 1) −h∗
(Ef 2
k(X))1/2(Ef 2
m(X))1/2"
EXP,0.47695390781563124,"≤exp

−ε"
EXP,0.4773547094188377,"2
 
k −CF.1(log(R) + 1) −h∗
Ef 2
k(X)"
EXP,0.4777555110220441,"+ exp

−ε"
EXP,0.4781563126252505,"2
 
k −CF.1(log(R) + 1) −h∗
Ef 2
m(X)."
EXP,0.4785571142284569,Applying the above inequality to bound the second moment of f(X) we get
EXP,0.4789579158316633,"Ef 2(X) =E
X"
EXP,0.47935871743486974,"k,m∈[k1,h(ρ′)]
Efk(X)fm(X) ≥
X"
EXP,0.4797595190380762,"k∈[k1,h(ρ′)]
Ef 2
k(X) ·

1 −
X"
EXP,0.48016032064128256,"s∈[k1,h(ρ′)]
exp

−ε"
EXP,0.480561122244489,"2
 
s −CF.1(log(R) + 1) −h∗
."
EXP,0.48096192384769537,"Notice that there exists t0 which depends on ε so that ∞
X"
EXP,0.4813627254509018,"t=t0
exp

−ε"
T,0.48176352705410824,"2t

≤1 2."
T,0.4821643286573146,"By setting
k1 := ⌈h∗+ CF.1(log(R) + 1) + t0⌉,
we get"
T,0.48256513026052106,Ef 2(X) ≥1 2 X
T,0.48296593186372744,"k∈[k1,h(ρ′)]
Ef 2
k(X).
(80)"
T,0.4833667334669339,"Our second goal is comparing E

(Eρ′fk)2(Xρ′)

and P"
T,0.48376753507014025,"k∈[k1,∞] Ef 2
k(X). Starting with the variance
and ℓ∞norm comparison from Lemma A.5,"
T,0.4841683366733467,"E

(Eρ′f)2(Xρ′)] ≤E
h
X"
T,0.4845691382765531,"k∈[k1,h(ρ′)]
max
θk∈[q]"
T,0.4849699398797595,"(Eρ′fk)(θk)

2i =

X"
T,0.48537074148296594,"k∈[k1,h(ρ′)]
max
θk∈[q]"
T,0.4857715430861723,"(Eρ′fk)(θk)

2"
T,0.48617234468937875,"≤CA.5

X"
T,0.4865731462925852,"k∈[k1,h(ρ′)] q"
T,0.48697394789579157,"E(Eρ′fk)2(Xρ)
2
,"
T,0.487374749498998,where CA.5 is the constant introduced in the Lemma.
T,0.4877755511022044,"By (62), from Proposition F.1, for k ∈[k1 + 1, h(ρ′)],"
T,0.4881763527054108,"E

(Eρ′fk)2(Xρ′)

≤exp
 
−ε(k −h∗)

· exp

−ε(h(ρ′) −CF.1(log(R) + 1) −h∗)

Ef 2
k(X)"
T,0.48857715430861726,"And for k = k1 = ⌈h∗+ CF.1(log(R) + 1) + t0⌉, we apply (63) to get"
T,0.48897795591182364,"E

(Eρ′fk1)2(Xρ′)

≤exp
 
−ε(h(ρ′) −k1 −CF.1(log(R) + 1))

Ef 2
k1(X)"
T,0.48937875751503007,"≤exp
 
−ε(−t0 −1)
"
T,0.48977955911823645,"· exp

−ε(h(ρ′) −2CF.1(log(R) + 1) −h∗)

Ef 2
k1(X)."
T,0.4901803607214429,Substituting these estimate and by Cauchy-Schwarz inequality we have
T,0.4905811623246493,"E

(Eρ′f)2(Xρ′)] ≤CA.5 exp

−ε(h(ρ′) −2CF.1(log(R) + 1) −h∗)
"
T,0.4909819639278557,"·

exp(ε(t0 + 1)) + ∞
X"
T,0.49138276553106214,"t=0
exp(−εt)
"
T,0.4917835671342685,"|
{z
}
:=C1 ·
X"
T,0.49218436873747495,"k∈[k1,h(ρ′)]
Ef 2
k(X)"
T,0.4925851703406814,"≤CA.5C12 exp

−ε(h(ρ′) −2CF.1(log(R) + 1) −h∗)

Ef 2(X)."
T,0.49298597194388777,"Now, by taking"
T,0.4933867735470942,"C0 ≥max
n
2CF.1 + 1"
T,0.4937875751503006,"ε log(CA.5C12) , CF.1 + t0 + 1
o
,"
T,0.494188376753507,we conclude that
T,0.4945891783567134,"E

(Eρ′f)2(Xρ′)] ≤exp

−ε(h(ρ′) −C0(log(R) + 1) −h∗)

Ef 2(X)."
T,0.49498997995991983,"It remains to show the case when h(ρ′) ≤k1. From the assumption that C0 ≥CF.1 + t0 + 1 and
k1 ≤h∗+ CF.1(log(R) + 1) + t0 + 1, we have"
T,0.49539078156312627,"exp

−ε(h(ρ′) −C0(log(R) + 1) −h∗
≥1."
T,0.49579158316633265,"Hence, the statement follows directly from Jensen’s inequality."
T,0.4961923847695391,"G
General Case: Base Case"
T,0.49659318637274547,"Now, we want to establish Theorem 1.6, which does not rely on the assumption cM > 0. Let us first
establish analogues of Assumption B.3 (the inductive assumption), Proposition B.5 (the base case),
and Theorem B.1 (the inductive step) in the general case.
Assumption G.1. By stating that A satisfies this assumption with given parameter h◦, we mean
A1 ⊆A ⊆2L\{∅} is closed under decomposition, and the following holds:"
T,0.4969939879759519,"For every u ∈T and any A≤u-polynomials functions f and g, we have"
T,0.49739478957915834,"Var

(Euf)(X)

≤exp(−ε(h(u) −h◦))Var

f(X)

.
(81)"
T,0.4977955911823647,"Further, suppose Ef = Eg = 0 and h(u) ≥h◦. Notice that by the Markov Property, (Eufg)(x),
(Euf 2)(x), and (Eug2)(x) are functions of xu. Then,"
T,0.49819639278557115,"max
θ∈[q] |(Eufg)(θ) −Efg| ≤exp

−ε"
T,0.49859719438877753,"2(h(u) −h◦)
q"
T,0.49899799599198397,"min
θ (Euf 2)(θ) min
θ′ (Eug2)(θ).
(82)"
T,0.4993987975951904,"The main difference of this assumption and Assumption B.3 is the difference of (82) and (16).
Proposition G.2. Consider the rooted tree T and transition matrix M described in Theorem 1.6.
There exists C = C(M, d) ≥1 such that A1 satisfies Assumption G.1 with some parameter h◦
satisfying"
T,0.4997995991983968,"h◦≤C(log(R) + 1).
(83)"
T,0.5002004008016032,"Theorem G.3. Consider the rooted tree T and transition matrix M described in Theorem 1.6. There
exists C = C(M, d) > 1 so that the following holds. Suppose A satisfies Assumption G.1 with some
parameter h◦. Let B = B(A) (see Definition 1.11). Then, B satisfies Assumption G.1 with parameter"
T,0.5006012024048097,h◦+ C(log(R) + 1).
T,0.501002004008016,"Proof of Theorem 1.6. The proof of Theorem 1.6 is analogous to that of Theorem B.1, employing a
similar strategy by leveraging Proposition G.2 and Theorem G.3 in the former, and Proposition B.5
and Theorem B.6 in the latter."
T,0.5014028056112224,"In this section we will prove the Base Case Proposition G.2.
Lemma G.4. There exists a constant C = C(M, ε) ≥1 so such that for any ρ′ ∈T and 0 ≤m ≤
h(ρ′):"
T,0.5018036072144288,Consider two degree 1 polynomials f and g with variables (xu : u ∈Dm(ρ′)). Suppose
T,0.5022044088176353,"f(X) =
X"
T,0.5026052104208417,"u∈Dm(ρ′)
fu(X) almost surely,"
T,0.503006012024048,"where fu(x) = fu(xu) and E[fu(X)] = 0, and we assume the same conditions for the polynomial g
and gu. Then,"
T,0.5034068136272545,"max
θ∈[q]"
T,0.5038076152304609,"(Eρ′fg)(θ) −Efg
 ≤CR exp(−ε(h(ρ′) −m))
s
X"
T,0.5042084168336673,"u∈Dm(ρ′)
Ef 2u(X)
s
X"
T,0.5046092184368738,"u∈Dm(ρ′)
Eg2u(X)."
T,0.5050100200400801,"Proof. Let C0 = C0(M, d) denote the constant introduced in the statement of the Lemma. Its value
will be determined along the proof."
T,0.5054108216432865,"First of all,"
T,0.505811623246493,"max
θ∈[q]"
T,0.5062124248496994,"(Eρ′fg)(θ) −Efg
 = max
θ∈[q] X"
T,0.5066132264529059,"u,v∈Dm(ρ′)"
T,0.5070140280561122,"
(Eρ′fugv)(θ) −Efugv

 ≤
X"
T,0.5074148296593186,"u,v∈Dm(ρ′)
max
θ∈[q]"
T,0.507815631262525,"(Eρ′fugv)(θ) −Efugv
."
T,0.5082164328657315,"Our proof will be carried out by estimating each summand. Fix any pair u, v ∈Dm(ρ′) and consider"
T,0.5086172344689379,"∥(Eρ′fugv) −Efugv∥∞= max
θ"
T,0.5090180360721442,"Eρ′
fu(X)gv(X) −Efugv
 Xρ′ = θ
."
T,0.5094188376753507,"Let w = ρ(u, v). Since fu and gv are functions of x≤w, relying on the Markov Property we know
the function (Ewfu · gv)(x) is a function of xw with expected value Efu(X)gv(X). With"
T,0.5098196392785571,"(Eρ′fugv)(xρ′) = E

(Ewfugv)(Xw)
 Xρ′ = xρ′],"
T,0.5102204408817635,"applying (10) from Lemma A.5,"
T,0.51062124248497,"∥(Eρ′fugv) −Efugv∥∞≤CA.5(h(ρ′) −h(w))qλh(ρ′)−h(w)(Ewfugv)(θ) −Efugv

∞,"
T,0.5110220440881763,where CA.5 is the M-dependent constant introduced in the Lemma.
T,0.5114228456913827,"Next, we will estimate
(Ewfugv)(θ) −Efugv

∞. In the case u ̸= v, there exists i ̸= j such that
u ≤wi and v ≤wj, which in turn implies that (X≤u | Xw = xw) and (X≤v | Xw = xw) are jointly
independent by the Markov Property. Thus,"
T,0.5118236472945892,"(Ewfugv)(θ) = (Ewfu)(θ)(Ewgv)(θ),"
T,0.5122244488977956,which implies
T,0.512625250501002,"max
θ∈[q] |(Ewfugv)(θ)| ≤max
θ∈[q] |(Ewfu)(θ)| · max
θ∈[q] |(Ewgv)(θ)|"
T,0.5130260521042084,"≤C3
A.5(h(w) −m)2qλ2(h(w)−m)p"
T,0.5134268537074148,"Ef 2u(X)Eg2v(X),"
T,0.5138276553106212,"where we applied (10) and (9) from Lemma A.5 in the last inequality. If u = v, then the same
estimate follows immediately without relying on (10)."
T,0.5142284569138277,"Now, we convert the above estimate to that of ∥(Ewfugv)(θ) −Efugv∥∞, which relies on the simple
bound that |Efu(X)gv(X)| ≤maxθ∈[q] |(Ewfugv)(θ)|. Thus,
(Ewfugv)(θ) −Efugv

∞≤2 max
θ∈[q] |(Ewfugv)(θ)|"
T,0.5146292585170341,"≤2C3
A.5(h(w) −m)2qλ2(h(w)−m)p"
T,0.5150300601202404,Ef 2u(X)Eg2v(X).
T,0.5154308617234469,"Together we conclude that for a pair u, v ∈Dm(ρ′) with w = ρ(u, v),"
T,0.5158316633266533,"∥(Eρ′fugv) −Efugv∥∞≤2C4
A.5(h(w) −m)2q(h(ρ′) −h(w))qλh(ρ′)+h(w)−2mp"
T,0.5162324649298597,Ef 2u(X)g2v(X)
T,0.5166332665330662,"≤2C4
A.5(h(ρ′) −m)3qλh(ρ′)+h(w)−2mp"
T,0.5170340681362725,Ef 2u(X)g2v(X).
T,0.5174348697394789,"Relying on this estimate, we are ready to bound the l∞-norm of (Eρ′fg)(xρ′) −Efg."
T,0.5178356713426854,"max
θ∈[q]"
T,0.5182364729458918,"(Eρ′fg)(θ) −Efg ≤
X"
T,0.5186372745490982,"u,v∈Dm(ρ′)
max
θ∈[q]"
T,0.5190380761523046,"(Eρ′fugv)(θ) −Efugv =
X"
T,0.519438877755511,"k∈[m,h(ρ′)] X"
T,0.5198396793587174,w∈Dk(ρ′) X
T,0.5202404809619239,"u,v : ρ(u,v)=w
max
θ∈[q]"
T,0.5206412825651303,"(Eρfugv)(θ) −Efugv ≤
X"
T,0.5210420841683366,"k∈[m,h(ρ′)] X"
T,0.5214428857715431,w∈Dk(ρ′) X
T,0.5218436873747495,"u,v : ρ(u,v)=w
2C4
A.5(h(ρ′) −m)3qλh(ρ′)+k−2mp"
T,0.522244488977956,"Ef 2u(X)g2v(X).
(84)"
T,0.5226452905811623,"Next, relaxing the condition ρ(u, v) = w in the summation,"
T,0.5230460921843687,"(∗) ≤
X"
T,0.5234468937875751,"k∈[m,h(ρ′)] X"
T,0.5238476953907816,w∈Dk(ρ′) X
T,0.524248496993988,"u,v∈Dm(w)
2C4
A.5(h(ρ′) −m)3qλh(ρ′)+k−2mp"
T,0.5246492985971943,"Ef 2u(X)g2v(X) =
X"
T,0.5250501002004008,"k∈[m,h(ρ′)] X"
T,0.5254509018036072,"w∈Dk(ρ′)
2C4
A.5(h(ρ′) −m)3qλh(ρ′)+k−2m 
X"
T,0.5258517034068136,u∈Dm(w) p
T,0.5262525050100201,"Ef 2u(X)
 
X"
T,0.5266533066132264,u∈Dm(w) p
T,0.5270541082164328,"Eg2u(X)

."
T,0.5274549098196393,Notice the inequality P
T,0.5278557114228457,"i∈[n]
|ti|"
T,0.5282565130260521,"n
≤
qP"
T,0.5286573146292585,"i∈[n]
|ti|2"
T,0.5290581162324649,"n
follows from Jenson’s inequality applying to
the function t 7→t2 and the uniform measure on [n]. Now apply this inequality to the collection
{
p"
T,0.5294589178356713,"Ef 2u(X)} and {
p"
T,0.5298597194388778,"Eg2u(X)} respectively, together with |Dm(w)| ≤Rdh(w)−m, from our tree
asscumption, we have"
T,0.5302605210420842,"(∗) ≤
X"
T,0.5306613226452905,"k∈[m,h(ρ′)] X"
T,0.531062124248497,"w≤ρ′ : w∈Dk(ρ′)
2C4
A.5(h(ρ′) −m)3qλh(ρ′)+k−2mRdk−m ·
s
X"
T,0.5314629258517034,"u∈Dm(w)
Ef 2u(X)
s
X"
T,0.5318637274549098,"u∈Dm(w)
Eg2u(X) ≤
X"
T,0.5322645290581163,"k∈[m,h(ρ′)]
2C4
A.5(h(ρ′) −m)3qλh(ρ′)+k−2mRdk−m ·
s
X"
T,0.5326653306613226,w≤ρ′ : h(w)=k X
T,0.533066132264529,"u∈Dm(w)
Ef 2u(X) ·
s
X"
T,0.5334669338677355,w≤ρ′ : h(w)=k X
T,0.5338677354709419,"u∈Dm(w)
Eg2u(X) =
X"
T,0.5342685370741483,"k∈[m,h(ρ′)]
2C4
A.5(h(ρ′) −m)3qλh(ρ′)+k−2mRdk−m
s
X"
T,0.5346693386773547,"u∈Dm(ρ′)
Ef 2u(X)
s
X"
T,0.5350701402805611,"u∈Dm(ρ′)
Eg2u(X), (85)"
T,0.5354709418837675,"where the last inequality follows from Cauchy-Schwarz inequality. Finally,
X"
T,0.535871743486974,"k∈[m,h(ρ′)]
2C4
A.5(h(ρ′) −m)3qλh(ρ′)+k−2mRdk−m"
T,0.5362725450901804,"≤2C4
A.5R(h(ρ′) −m)3q · (h(ρ′) −m)λh(ρ′)−m
max
k∈[m,h(ρ′)] λk−mdk−m"
T,0.5366733466933867,"=2C4
A.5R(h(ρ′) −m)3q · (h(ρ′) −m)
 
max{dλ2, λ}
h(ρ′)−m"
T,0.5370741482965932,"=2C4
A.5R(h(ρ′) −m)3q+1 exp(−1.1ε(h(ρ′) −m))"
T,0.5374749498997996,"≤C0R exp(−ε(h(ρ′) −m)),"
T,0.537875751503006,"where
C0 = 2C4
A.5 max
n∈N n3q+1 exp(−0.1εn) < +∞"
T,0.5382765531062125,is a constant depending on M and ε. Combining the above estimate with (85) we conclude that
T,0.5386773547094188,"max
θ∈[q]"
T,0.5390781563126252,"(Eρ′fg)(θ) −Efg
 ≤C0R exp
 
−ε(h(ρ′) −m)
s
X"
T,0.5394789579158317,"u∈Dm(ρ′)
Ef 2u(X)
s
X"
T,0.5398797595190381,"u∈Dm(ρ′)
Eg2u(X),"
T,0.5402805611222445,and the lemma follows.
T,0.5406813627254509,"The statement of Lemma G.4 together with Proposition C.3 implies the following:
Corollary G.5. There exists a constant C = C(M, ε) ≥1 so that the following holds. For ρ′ ∈T
and 0 ≤m ≤h(ρ′), consider two degree 1 polynomials f and g with variables (xu : u ∈Dm(ρ′))
with Ef(X) = Eg(X) = 0. Notice that by the Markov Property, (Eρ′fg)(x) is a function of xρ′.
Then,
max
θ∈[q]"
T,0.5410821643286573,"(Eρ′fg)(θ) −Efg
 ≤CR4 exp(−ε(h(ρ′) −m))
p"
T,0.5414829659318637,"Ef 2(X)
p"
T,0.5418837675350702,Eg2(X).
T,0.5422845691382766,"Remark G.6. By taking the degree 1 polynomial f = g with the assumption that Ef(X) = 0, we
get"
T,0.5426853707414829,"E

(Eρ′f 2)(X) −Ef 2(X)
2 ≤

max
θ∈[q]"
T,0.5430861723446894,"(Eρ′fg)(θ) −Efg

2
≤C2R6 exp(−2εh(ρ′))(Ef 2(X))2. (86)"
T,0.5434869739478958,"In other words, if h(ρ′) is sufficiently large, (Eρ′f 2)(Xρ′) is almost the same as Ef 2(X) with a
small fluctuation. Let us state this as a seperate lemma."
T,0.5438877755511022,"Lemma G.7. There exists C = C(M, d) so that the following holds. For ρ′ ∈T with"
T,0.5442885771543087,"h(ρ′) ≥C(log(R) + 1) ε
,"
T,0.544689378757515,any degree 1 polynomial f of variables (xu : u ∈Lρ′) with Ef(X) = 0 satisfies
T,0.5450901803607214,"max
θ∈[q](Eρ′f 2)(θ) ≤2 min
θ∈[q](Eρ′f 2)(θ)."
T,0.5454909819639279,"Proof. By Corollary G.5, for every θ ∈[q],
(Eρ′f 2)(θ) −Ef 2(X)
 ≤CG.5R4 exp(−εh(ρ′))Ef 2(X)."
T,0.5458917835671343,"where CG.5 is the constant introduced in Lemma G.5. Now, we set the constant described in the
lemma as
C = 1 ε"
T,0.5462925851703406,"
log(4CG.5) + 4

,"
T,0.5466933867735471,which implies
T,0.5470941883767535,CG.4R exp(−εh(ρ′)) ≤1
EXP,0.5474949899799599,"4 exp
 
−ε(h(ρ′) −C(log(R) + 1))

."
EXP,0.5478957915831664,"Then, with h(ρ′) ≥C(log(R) + 1)"
EXP,0.5482965931863727,|(Eρ′f 2)(θ) −Ef 2(X)| ≤1
EXP,0.5486973947895791,"4Ef 2(X),"
EXP,0.5490981963927856,"which in term implies
maxθ∈[q](Eρ′f 2)(θ)"
EXP,0.549498997995992,minθ∈[q](Eρ′f 2)(θ) ≤
EXP,0.5498997995991984,"5
4Ef 2(X)"
EXP,0.5503006012024048,"3
4Ef 2(X) < 2."
EXP,0.5507014028056112,"Proof of Proposition G.2. Let C0 denote the constant introduced in the statement of the Proposition.
Its precise value will be determined along the proof."
EXP,0.5511022044088176,"Let ρ′ ∈T with h′ := h(ρ′). By Lemma C.9, any degree-1 polynomial f(x) with variables
(xu : u ∈Lρ′) satisfies"
EXP,0.5515030060120241,"Var

(Eρ′f)(X)

≤CC.9R4(h′)2q(dλ2)h′Var[f(X)],"
EXP,0.5519038076152305,"where CC.9 denotes the M-dependent constant introduced in the Lemma. For the term in front of
Var[f(X)],"
EXP,0.5523046092184368,"CC.9R4(h′)2q(dλ2)h′ ≤CC.9R4(h′)2q exp(−1.1εh′) ≤exp
 
−ε
 
h′ −C1(log(R) + 1)

,"
EXP,0.5527054108216433,"where
C1 := 1 ε"
EXP,0.5531062124248497,"
log(CC.9) + 4 + max
n∈N n2q exp(−0.1εq)

."
EXP,0.5535070140280561,"Thus, if we impose the first assumption on C0 that"
EXP,0.5539078156312626,"C0 ≥C1,"
EXP,0.5543086172344689,then the first condition (81) in Assumption G.1 holds for A1 if we take h◦≥C0(1 + log(R)).
EXP,0.5547094188376753,"It remains to establish (82). Let f, g be two degree-1 polynomials in the variables (xu : u ∈Lρ′)
satisfying Ef(X) = Eg(X) = 0. First, by Corollary G.5,"
EXP,0.5551102204408818,"max
θ∈[q] |(Eρ′fg)(θ) −Efg| ≤CG.5R4 exp(−εh(ρ′))
p"
EXP,0.5555110220440882,"Ef 2(X)
p"
EXP,0.5559118236472946,"Eg2(X),"
EXP,0.556312625250501,"where CG.5 is the constant introduced in Corollary G.5. Next, we would like to apply Lemma G.7.
Assuming"
EXP,0.5567134268537074,h(u) ≥CG.7(log(R) + 1) ε
EXP,0.5571142284569138,"where CG.7 is the constant introduced in the Lemma, we can apply the lemma to get"
EXP,0.5575150300601203,"Ef 2(X) ≤2 min
θ (Eρ′f 2)(θ)"
EXP,0.5579158316633267,and the same holds for g. Together we may conclude that
EXP,0.558316633266533,"max
θ∈[q] |(Eρ′fg)(θ) −Efg| ≤2CG.5R4 exp(−εh(ρ′))
q"
EXP,0.5587174348697395,"min
θ (Eρ′f 2)(θ) min
θ (Eρ′g2)(θ)"
EXP,0.5591182364729459,"Now, we impose the second assumption on C0 that"
EXP,0.5595190380761523,"C0 ≥max
n1 ε"
EXP,0.5599198396793588,"
log(2CG.5) + 4

, CG.7 ε o
."
EXP,0.5603206412825651,"Then, we conclude that"
EXP,0.5607214428857715,"max
θ∈[q] |(Eufg)(θ) −Efg| ≤exp

−ε
 
h(ρ′) −C0(log(R) + 1)
q"
EXP,0.561122244488978,"min
θ (Euf 2)(θ) min
θ (Eug2)(θ)"
EXP,0.5615230460921844,"provided that
h(ρ′) ≥C0(log(R) + 1)."
EXP,0.5619238476953908,"Therefore, we can conclude that A1 satisfies Assumption G.1 with"
EXP,0.5623246492985972,h◦= C0(log(R) + 1).
EXP,0.5627254509018036,"H
Inductive Step in General Case"
EXP,0.56312625250501,"The goal in this section is to prove Theorem G.3. Let us restate the theorem here:
Theorem. Consider the rooted tree T and transition matrix M described in Theorem 1.6. There
exists C = C(M, d) > 1 so that the following holds. Suppose A satisfies Assumption G.1 with some
parameter h◦. Let B = B(A) (see Definition 1.11). Then, B satisfies Assumption G.1 with parameter"
EXP,0.5635270541082165,h◦+ C(log(R) + 1).
EXP,0.5639278557114229,"In this section, we fix a subcollection A satisfying Assumption G.1 with a given parameter h◦
and let B = B(A)."
EXP,0.5643286573146292,"We begin with the following lemma, which allows us to recycle some of the results from the case
cM > 0.
Lemma H.1. Suppose A satisfies Assumption G.1 with parameter h◦. Then, then A satisfies
Assumption B.3 with h∗= h◦+ 2"
EXP,0.5647294589178357,ε log(2) and c∗= 1 2.
EXP,0.5651302605210421,"Proof. Let f be a A≤v-polynomial. If we set h∗≥h◦, then (15) follows immediately from (81)."
EXP,0.5655310621242485,"Now, we assume that Ef(X) = 0 and h(v) ≥h◦. We could apply (82) with g = f to get"
EXP,0.565931863727455,"max
θ∈[q]"
EXP,0.5663326653306613,"(Evf 2)(θ) −Ef 2(X)
 ≤exp

−ε"
EXP,0.5667334669338677,"2(h(v) −h◦)

Ef 2(X)."
EXP,0.5671342685370742,"If exp

−ε"
EXP,0.5675350701402806,"2(h(v) −h◦)

≤1"
EXP,0.5679358717434869,"2, or equivalently,"
EXP,0.5683366733466934,h(v) ≥h◦+ 2
EXP,0.5687374749498998,"ε log(2),"
EXP,0.5691382765531062,"then, for every θ ∈[q],"
EXP,0.5695390781563127,"1
2Eh2(X) ≤(Evh2)(θ) ≤3"
EXP,0.569939879759519,2Eh2(X).
EXP,0.5703406813627254,"Therefore, if we set h∗≥h◦+ 2"
EXP,0.5707414829659319,ε log(2) and c∗= 1
EXP,0.5711422845691383,"2, both (15) and (16) hold."
EXP,0.5715430861723447,"In the remainning of this section, we set"
EXP,0.571943887775551,h∗= h◦+ 2
EXP,0.5723446893787575,ε log(2) and c∗= 1
EXP,0.5727454909819639,"2,
(87)"
EXP,0.5731462925851704,"and we will rely on the fact that A satisfies Assumption B.3 with these two parameters. In particular,
we could apply Theorem B.6 to show the existence of CB.6 = C(M, ε, 1/2) such that for any
B≤v-polynomial f,"
EXP,0.5735470941883768,"Var

(Evf)(X)

≤exp

−ε
 
h(v) −h◦+ CB.6(log(R) + 1)

Var

f(X)

."
EXP,0.5739478957915831,"Therefore, to establish Theorem G.3, it remains to show the existence of C = C(M, d) so that any
B≤v-polynomials f and g with h(v) ≥h◦+ C(log(R) + 1) and Ef(X) = Eg(X) = 0 satisfy"
EXP,0.5743486973947896,"max
θ∈[q] |(Evfg)(θ) −Efg| ≤exp

−ε"
EXP,0.574749498997996,"2(h(v) −h◦−C(log(R) + 1))
q"
EXP,0.5751503006012024,"min
θ (Evf 2)(θ) min
θ′ (Evg2)(θ)."
EXP,0.5755511022044089,"To establish the above inequality, the higher level structure is essentially the same as that for deriving
Theorem B.6. We again decompose f and g according to Lemma D.1. To the proof of the theorem,
similarly it contains three steps:"
EXP,0.5759519038076152,"1. Establish properties of ˜fu and ˜gu, see Proposition H.2.
2. Estalbish properties of fk and gk, see Proposition H.6.
3. Establish Theorem G.3."
EXP,0.5763527054108216,"H.1
Properties of fu"
EXP,0.576753507014028,"The main goal we want to prove in this subsection is the following Proposition.
Proposition H.2. There exsits C = C(M, d) ≥1 so that the following holds. For a given u ∈T\L
with
h(u) ≥h◦+ C(log(R) + 1),
suppose fu and gu are two functions which are linear combination of ψσ(x) with σ ∈F(Bu). Then,
for any θ, θ′ ∈[q],
(Eufugu)(θ) −(Eufugu)(θ′)"
EXP,0.5771543086172345,"≤exp

−ε"
EXP,0.5775551102204409,"2(h(u) −C(log(R) + 1) −h◦)
q"
EXP,0.5779559118236473,"min
θ (Euf 2u)(θ) min
θ (Eug2u)(θ)."
EXP,0.5783567134268537,"With a minor modification to our approach, we are able to obtain an analogous result wherein fu and
gu are substituted by ˜fu and ˜gu, respectively:
Corollary H.3. There exsits C = C(M, d) ≥1 so that the following holds. For a given u ∈T\L
with
h(u) ≥h◦+ C(log(R) + 1),
suppose fu and gu are two functions which are linear combination of ψS(x) with S ∈F(Bu). Then,
for any θ, θ′ ∈[q],
(Eu ˜fu˜gu)(θ) −(Eu ˜fu˜gu)(θ′)"
EXP,0.5787575150300601,"≤exp

−ε"
EXP,0.5791583166332666,"2(h(u) −C(log(R) + 1) −h◦)
r"
EXP,0.579559118236473,"min
θ (Eu ˜f 2u)(θ) min
θ (Eu˜g2u)(θ)."
EXP,0.5799599198396793,Let us prove Corollary first.
EXP,0.5803607214428858,"Proof. Let C0 denote the constant introduced in the Corollary. Its value will be dervied during the
proof."
EXP,0.5807615230460922,From the identity
EXP,0.5811623246492986,"˜fu(x)˜gu(x) = fu(x)gu(x) −fu(x)Egu(X) −Efu(X)gu(x) + Efu(X)Egu(X),"
EXP,0.5815631262525051,"it follows that
(Eu ˜fu˜gu)(θ) −(Eu ˜fu˜gu)(θ′)
 ≤
(Eufugu)(θ) −(Eufugu)(θ′)
 + |Egu(X)|
(Eufu)(θ) −(Eufu)(θ′)"
EXP,0.5819639278557114,"+ |Efu(X)|
(Eugu)(θ) −(Eugu)(θ′)"
EXP,0.5823647294589178,"≤
(Eufugu)(θ) −(Eufugu)(θ′)
 + 4 max
θ′
|Eufu(θ)| max
θ′
|Eugu(θ)|."
EXP,0.5827655310621243,"First, we apply Propostion E.1 with the fact that A satisfies Assumption G.1 with parameter h∗=
h◦+ 2"
EXP,0.5831663326653307,"ε log(2) and c∗= 1 2,"
EXP,0.5835671342685371,"max
θ′ (Eufu)2(θ) ≤exp(−2ε(h(u) −CE.1(log(R) + 1) −h∗)) max
θ′ (Euf 2
u)(θ′)"
EXP,0.5839679358717434,"where CE.1 = C(M, d, 1"
EXP,0.5843687374749499,2) is the constant introduced in the Proposition.
EXP,0.5847695390781563,"Second, applying Proposition H.2 with fu = gu we have
 max
θ (Euf 2
u)(θ) −min
θ′ (Euf 2
u)(θ′)"
EXP,0.5851703406813628,"≤exp

−ε"
EXP,0.5855711422845692,"2(h(u) −CH.2(log(R) + 1) −h◦)

min
θ (Euf 2
u)(θ)"
EXP,0.5859719438877755,where CH.2 is the constant introduced in Proposition H.2.
EXP,0.586372745490982,"Let us impose the first assumption that C0 ≥CH.2. Then, with h(u) ≥h◦+ C0(log(R) + 1), we
can conclude that
max
θ′ (Euf 2
u)(θ′) ≤2 min
θ′ (Euf 2
u)(θ′)."
EXP,0.5867735470941884,"Clearly, the same derivation also holds for gu."
EXP,0.5871743486973948,"Therefore, we conclude that
(Eu ˜fu˜gu)(θ) −(Eu ˜fu˜gu)(θ′)"
EXP,0.5875751503006013,"≤
(Eufugu)(θ) −(Eufugu)(θ′)"
EXP,0.5879759519038076,"+ 8 exp(−2ε(h(u) −C1(log(R) + 1) −h∗))
q"
EXP,0.588376753507014,"min
θ (Euf 2u)(θ) min
θ (Eug2u)(θ)"
EXP,0.5887775551102205,"≤exp

−ε"
EXP,0.5891783567134269,"2(h(u) −CH.2(log(R) + 1) −h◦)
q"
EXP,0.5895791583166332,"min
θ (Euf 2u)(θ) min
θ (Eug2u)(θ)"
EXP,0.5899799599198396,"+ 8 exp

−2ε

h(u) −CE.1(log(R) + 1) −h◦−2"
EXP,0.5903807615230461,"ε log(2)
q"
EXP,0.5907815631262525,"min
θ (Euf 2u)(θ) min
θ (Eug2u)(θ)"
EXP,0.591182364729459,"≤exp

−ε"
EXP,0.5915831663326653,"2(h(u) −C0(log(R) + 1) −h◦)
q"
EXP,0.5919839679358717,"min
θ (Euf 2u)(θ) min
θ (Eug2u)(θ),"
EXP,0.5923847695390781,where the last inequality follows by imposing the second assumption on C0 that C0 ≥2
EXP,0.5927855711422846,"ε log(2) + max
n
CH.2, CE.1 + 2"
EXP,0.593186372745491,ε log(2) + 1
EXP,0.5935871743486973,"2ε log(8)
o
."
EXP,0.5939879759519038,This completes the proof of the Corollary.
EXP,0.5943887775551102,"The main technical part for proving Proposition H.2 is the following:
Lemma H.4. For any u ∈T with exp
 
−ε"
EXP,0.5947895791583167,"2(h(u) −h◦)

≤
1
4Rd, the following holds: Let I ⊂[du]
be a subset of size at least 2. For any a(x) and b(x) which are linear combinations of ψσ(x) with
σ ∈Bu satisfying I(σ) = I, we have"
EXP,0.5951903807615231,"max
θ,θ′∈[q]"
EXP,0.5955911823647294,"(Euab)(θ) −(Euab)(θ′)
 ≤4dR exp

−ε"
EXP,0.5959919839679358,"2(h(u) −h◦)
q"
EXP,0.5963927855711423,"min
θ (Eua2)(θ) · min
θ (Eub2)(θ)."
EXP,0.5967935871743487,Remark H.5. From the assumption that h(u) satisfies
DR EXP,0.5971943887775552,"4dR exp

−ε"
DR EXP,0.5975951903807615,"2(h(u) −h◦)

≤1 ⇔h(u) ≥h◦+ 2"
DR EXP,0.5979959919839679,ε log(4dR).
DR EXP,0.5983967935871743,By taking a(x) = b(x) we have
DR EXP,0.5987975951903808,"max
θ (Eua2)(θ) ≤2 min
θ (Eua2)(θ).
(88)"
DR EXP,0.5991983967935872,"Proof. Let u, a(x), and b(x) be the vertex and functions described in the Lemma. Let us introduce
some notations for the ease of expressing the calculation later. For brevity, let"
DR EXP,0.5995991983967935,"δ = exp

−ε"
DR EXP,0.6,"2(h(u) −h◦)

."
DR EXP,0.6004008016032064,"For x ∈[q]T , let
xu,I = (xui)i∈I.
For any given function h(x) with variables in (xv : v ∈S"
DR EXP,0.6008016032064128,"i∈I Tui), we define"
DR EXP,0.6012024048096193,"(Eu,Ih)(x) := E
h
h(X)
 ∀v /∈
["
DR EXP,0.6016032064128256,"i∈I
{w < ui}, Xv = xv
i
."
DR EXP,0.602004008016032,Observe that
DR EXP,0.6024048096192385,"(Eu,Ia)(x), (Eu,Ib)(x), and (Eu,Iab)(x)"
DR EXP,0.6028056112224449,"are functions with input xu,I. This is due to the fact that a and b –and consequently ab– are functions
of variables (xv : xv ∈S"
DR EXP,0.6032064128256514,i∈I Lui) and Markov Property.
DR EXP,0.6036072144288577,"Claim: The function xu,I 7→(Eu,Iab)(xu,I) is Lipschitz continuous with respect to the Hamming
Distance with Lipschitz constant 2δ
r"
DR EXP,0.6040080160320641,"max
xu,I (Eu,Ia2)(xu,I)
r"
DR EXP,0.6044088176352705,"max
xu,I (Eu,Ib2)(xu,I).
(89)"
DR EXP,0.604809619238477,"We begin with the proof of the claim. Fix an index i0 ∈I. Without lose of generality, we assume
I = [k] and i0 = 1. For x ∈[q]T , let
xi = x≤ui
for i ≤[du], and set
x0 = (x2, . . . , xk).
With this notation above, we can express"
DR EXP,0.6052104208416834,"a(x) =a(x0, x1)
and
b(x) =b(x0, x1)."
DR EXP,0.6056112224448897,"Fix any value of x0, the function"
DR EXP,0.6060120240480962,"x1 7→a(x0, x1)"
DR EXP,0.6064128256513026,"is a linear combination of ˜ϕσ1(x1) with σ1 ∈F(A≤u). Notably, this implies that Ea(x0, X1) = 0.
The same properties hold for the function x1 7→b(x0, x1)."
DR EXP,0.606813627254509,"Now, given the assumption exp
 
−ε"
DR EXP,0.6072144288577155,"2(h(u) −h◦)

≤
1
4Rd implies h(u) ≥h◦, we can apply (82)
from Assumption G.1 to get that"
DR EXP,0.6076152304609218,"max
θ,θ′∈[q]"
DR EXP,0.6080160320641282,"E

a(x0, X1)b(x0, X1)
 Xu1 = θ1

−E

a(x0, X1)b(x0, X1)
 Xu1 = θ2
"
DR EXP,0.6084168336673347,"≤2 max
θ∈[q]"
DR EXP,0.6088176352705411,"E

a(x0, X1)b(x0, X1)
 Xu1 = θ1

−Ea(x0, X1)b(x0, X1) ≤2δ
q"
DR EXP,0.6092184368737475,"min
θ
E

a2(x0, X1)
 Xu1 = θ

min
θ
E

b2(x0, X1)
 Xu1 = θ

."
DR EXP,0.6096192384769539,"For any x ∈[q]T , let xu0 = (xu2, xu3, . . . , xudu ). By the Markov Property, for any θ ∈[q],"
DR EXP,0.6100200400801603,"(X0 | Xu0 = xu0, Xu1 = θ) =(X0 | Xu0 = xu0) and
(X1 | Xu0 = xu0, Xu1 = θ) =(Xu1 | Xu1 = θ)"
DR EXP,0.6104208416833667,"are jointly independent. Hence,"
DR EXP,0.6108216432865732,"E

a(X0, X1)b(X0, X1)
 Xu0 = xu0, Xu1 = θ
"
DR EXP,0.6112224448897795,"=E

a(Y0, X1)b(Y0, X1)
 Xu1 = θ
"
DR EXP,0.6116232464929859,"where Y0 is an independent copy of (X0 | Xu0 = xu0). We have
E

a(X0, X1)b(X0, X1)
 Xu0 = xu0, Xu1 = θ

−E

a(X0, X1)b(X0, X1)
 Xu0 = xu0, Xu1 = θ′"
DR EXP,0.6120240480961924,"=
EY0
h
EX1[a(Y0, X1)b(Y0, X1) | Xu1 = θ] −EX1[a(Y0, X1)b(Y0, X1) | Xu1 = θ′]
i"
DR EXP,0.6124248496993988,"≤EY0
hEX1[a(Y0, X1)b(Y0, X1) | Xu1 = θ] −EX1[a(Y0, X1)b(Y0, X1) | Xu1 = θ′]

i"
DR EXP,0.6128256513026052,"≤2δEY0
h 
min
θ
EX1[a2(Y0, X1) | Xu1 = θ]
1/2 ·
 
min
θ′ EX1[b2(Y0, X1) | Xu1 = θ′]
1/2i"
DR EXP,0.6132264529058116,"≤2δ
 
EY0

min
θ
EX1[a2(Y0, X1) | Xu1 = θ]
1/2 ·
 
EY0

min
θ′ EX1[b2(Y0, X1) | Xu1 = θ′]
1/2,"
DR EXP,0.613627254509018,"where the last inequality follows from Hölder’s inequality. Further,"
DR EXP,0.6140280561122244,"EY0

min
θ
EX1[a2(Y0, X1) | Xu1 = θ]

≤min
θ
EY0

EX1[a2(Y0, X1) | Xu1 = θ]
"
DR EXP,0.6144288577154309,"= min
θ
E

a2(X)
 Xu0 = xu0, Xu1 = θ
"
DR EXP,0.6148296593186373,"≤max
xu,I (Eu,Ia2)(xu,I)."
DR EXP,0.6152304609218436,Applying the same derivation to b we get
DR EXP,0.6156312625250501,"EY0

min
θ
EX1[b2(Y0, X1) | Xu1 = θ]

≤max
xu,I (Eu,Ib2)(xu,I)."
DR EXP,0.6160320641282565,"Therefore, our claim (89) follows: For any θ, θ′ ∈[q],
E

a(X)b(X)
 Xu0 = xu0, xu1 = θ

−E

a(X)b(X)
 Xu0 = xu0, xu1 = θ′ ≤2δ
r"
DR EXP,0.6164328657314629,"max
xu,I (Ea2)(xu,I) max
xu,I (Eb2)(xu,I)."
DR EXP,0.6168336673346694,"With the Lipschitz continuity been established, essentially the lemma follows when δ is sufficiently
small. Let us proceed with the remaining argument. Let"
DR EXP,0.6172344689378757,"x′
u,I =argminxu,I(Eu,Ia2)(xu,I)
and
x′′
u,I =argmaxxu,I(Eu,Ia2)(xu,I)."
DR EXP,0.6176352705410821,"Applying (89) with the assumption a(x) = b(x) and the fact |I| ≤du,"
DR EXP,0.6180360721442886,"(Eu,Ia2)(x′′
u,I) −(Eu,Ia2)(x′
u,I) ≤2duδ(Eu,Ia2)(a′′
u,I),"
DR EXP,0.618436873747495,and hence
DR EXP,0.6188376753507014,"max
xu,I (Eu,Ia2)(xu,I) ≤
1
1 −2duδ min
xu,I (Eu,Ia2)(xu,I) ≤
1
1 −2duδ min
s (Eua2)(s),
(90)"
DR EXP,0.6192384769539078,provided that 2duδ < 1.
DR EXP,0.6196392785571142,"Again, the same derivation also holds for b. Combining (89) and (90) we conclude that for any
θ, θ′ ∈[q],"
DR EXP,0.6200400801603206,"|(Euab)(θ) −(Euab)(θ′)| ≤| max
xu,I (Euab)(xu,I) −min
x′
u,I
(Euab)(x′
u,I)|"
DR EXP,0.6204408817635271,"≤
2duδ
1 −2duδ q"
DR EXP,0.6208416833667335,"min
θ (Eua2)(θ) min
θ′ (Eub2)(θ′)."
DR EXP,0.6212424849699398,"With our assumption on the tree T that du ≤Rd, our assumption"
DR EXP,0.6216432865731463,"δ = exp

−ε"
DR EXP,0.6220440881763527,"2(h(u) −h◦)

≤
1
4Rd,"
DR EXP,0.6224448897795591,"implies that
2duδ
1 −2duδ ≤4Rdδ."
DR EXP,0.6228456913827656,We conclude that
DR EXP,0.6232464929859719,"|(Euab)(θ) −(Euab)(θ′)| ≤4Rd exp
ε"
DR EXP,0.6236472945891783,"2(h(u) −h◦)
q"
DR EXP,0.6240480961923848,"min
θ (Eua2)(θ) min
θ′ (Eub2)(θ′)."
DR EXP,0.6244488977955912,"Proof of Proposition H.2. Let C0 = C0(M, d) denote the constant introduced in the statement of the
Proposition. Recall the decomposition of fu into fu,I from Definition E.4, consider the decomposition"
DR EXP,0.6248496993987976,"fu(x) =
X"
DR EXP,0.625250501002004,"I⊆[du] : |I|≥2
fu,I(x) and gu(x) =
X"
DR EXP,0.6256513026052104,"I⊆[du] : |I|≥2
gu,I(x)."
DR EXP,0.6260521042084168,"The proof of the Proposition will proceed by bounding summands in the formula below:
(Eufugu)(θ) −(Eufugu)(θ′)
 ≤
X"
DR EXP,0.6264529058116233,"I,J⊆[du] : |I|,|J|≥2"
DR EXP,0.6268537074148297,"(Eufu,Igu,J)(θ) −(Eufu,Igu,J)(θ′)
. (91)"
DR EXP,0.627254509018036,"Estimate of summands in (91): For any I, J ⊆[du] with |I|, |J| ≥2, we have two cases to consider:
First, we consider the case I ̸= J. Notice that, by Lemma H.1, A satisfies Assumption B.3 with
parameters (h◦+ 2"
DR EXP,0.6276553106212425,"ε log(2), 1"
DR EXP,0.6280561122244489,"2). This allows us to invoke Corollary E.6, yielding
(Eufu,Igu,J)(θ) −(Eufu,Igu,J)(θ′)"
DR EXP,0.6284569138276553,"≤2 max
θ∈[q] |(Eufu,Igu,J)(θ)|"
DR EXP,0.6288577154308618,"≤2 exp

−ε|I∆J|"
DR EXP,0.6292585170340681,"2
(h(u) −CE.6 −h◦−2"
DR EXP,0.6296593186372745,"ε log(2))
 
max
θ∈[q](Euf 2
u,I)(θ)
1/2 ·
 
max
θ∈[q](Eug2
u,J)(θ)
1/2,"
DR EXP,0.630060120240481,"where CE.6 = CE.6(M, d, 1"
DR EXP,0.6304609218436874,2) is the constant introduced in the Corollary.
DR EXP,0.6308617234468938,Let us impose the first assumption on C0 that C0 ≥2
DR EXP,0.6312625250501002,"ε(1 + log(4d)),"
DR EXP,0.6316633266533066,which implies that h(u) ≥h◦+ C0(log(R) + 1) ≥h◦+ 2
DR EXP,0.632064128256513,"ε log(4dR). With this assumption, we
could apply the remark (88) of Lemma H.4 to get
 
max
θ∈[q](Euf 2
u,I)(θ)
1/2 ≤2
 
min
θ (Euf 2
u,I)(θ)
1/2"
DR EXP,0.6324649298597195,"and the same holds for gu,J. Hence, for I ̸= J we have
(Eufu,Igu,J)(θ) −(Eufu,Igu,J)(θ′)"
DR EXP,0.6328657314629259,"≤4 exp

−ε|I∆J|"
DR EXP,0.6332665330661322,"2
(h(u) −CE.6 −h◦+ 2"
DR EXP,0.6336673346693387,"ε log(2))
 
min
θ∈[q](Euf 2
u,I)(θ)
1/2 ·
 
min
θ∈[q](Eug2
u,J)(θ)
1/2."
DR EXP,0.6340681362725451,"Second, we consider the case I = J. Here we simply apply Lemma H.4, yielding
(Eufu,Igu,I)(θ) −(Eufu,Igu,I)(θ′)"
DR EXP,0.6344689378757515,"≤4Rd exp

−ε"
DR EXP,0.6348697394789579,"2(h(u) −h◦)
 
min
θ∈[q](Euf 2
u,I)(θ)
1/2 ·
 
min
θ∈[q](Eug2
u,J)(θ)
1/2."
DR EXP,0.6352705410821643,Let us unify the above two estimates by introducing
DR EXP,0.6356713426853707,"C1 = max
n
CE.6 + 2"
DR EXP,0.6360721442885772,ε log(2) + 2
DR EXP,0.6364729458917836,"ε log(8), 2"
DR EXP,0.6368737474949899,"ε(1 + log(24d))
o
."
DR EXP,0.6372745490981964,"Then,
(Eufu,Igu,J)(θ) −(Eufu,Igu,J)(θ′)

(92) ≤1"
EXP,0.6376753507014028,"6 exp

−ε"
EXP,0.6380761523046092,"2 max{|I∆J|, 1}(h(u) −C1(log(R) + 1) −h◦)
"
EXP,0.6384769539078157,"|
{z
}
:=aI,J"
EXP,0.638877755511022," 
min
θ∈[q](Euf 2
u,I)(θ)
1/2"
EXP,0.6392785571142284,"|
{z
}
:=αI"
EXP,0.6396793587174349,"·
 
min
θ∈[q](Eug2
u,J)(θ)
1/2"
EXP,0.6400801603206413,"|
{z
}
:=βJ , (93)"
EXP,0.6404809619238477,"for every pair I, J ⊆[du] with |I| ≥2 and |J| ≥2."
EXP,0.6408817635270541,"Using this inequality, (91) becomes
(Eufugu)(θ) −(Eufugu)(θ′)
 ≤
X"
EXP,0.6412825651302605,"I,J⊆[du] :|I|,|J|≥2
aI,JαIβJ = ⃗α⊤A⃗β ≤∥⃗α∥· ∥A∥· ∥⃗β∥
(94)"
EXP,0.6416833667334669,"where ⃗α = (αI)I⊆[du] : |I|≥2, ⃗β = (β)I⊆[du] : |I|≥2, and A = (aI,J)I,J⊆[du] :|I|,|J|≥2. Further, ∥⃗α∥
and ∥⃗β∥are the ℓ2 norms of ⃗α and ⃗β, respectively, and ∥A∥is the operator norm of A."
EXP,0.6420841683366734,"Estimate of operator norm of A: Notice that A is a symmetric matrix. Thus, we can fix a unit vector
⃗γ satisfying ∥A∥= ⃗γ⊤A⃗γ. For each pair I, J ⊆[du] with |I| ≥2 and |J| ≥2, since aI,J ≥0,"
EXP,0.6424849699398798,"γIaI,JγJ ≤aI,J"
EXP,0.6428857715430861,"2 γ2
I + aI,J"
EXP,0.6432865731462926,"2 γ2
J,"
EXP,0.643687374749499,"and thus,"
EXP,0.6440881763527054,"∥A∥=
X"
EXP,0.6444889779559119,"I,J⊆[du] :|I|,|J|≥2
aI,JγIγJ ≤
X"
EXP,0.6448897795591182,"I⊆[du] :|I|≥2
γ2
I

X"
EXP,0.6452905811623246,"J⊆[du] :|J|≥2
aI,J

.
(95)"
EXP,0.6456913827655311,"For each I ⊆[du] with |I| ≥2, the number of J ⊆[du] with |I∆J| = k is bounded above by
dk−1
u
≤(Rd)k. Then, with the given estimate of aI,J in (92),
X"
EXP,0.6460921843687375,"J⊆[du] :|J|≥2
aI,J ≤1"
EXP,0.6464929859719439,"6 exp

−ε"
EXP,0.6468937875751503,"2(h(u) −C1(log(R) + 1) −h◦)
 +
X t≥1"
EXP,0.6472945891783567,"1
6(Rd)t exp

−ε"
EXP,0.6476953907815631,"2t(h(u) −C1(log(R) + 1) −h◦)

.
(96)"
EXP,0.6480961923847696,"Now, we impose the second assumption on C0 that"
EXP,0.648496993987976,C0 ≥C1 + 2 ε
EXP,0.6488977955911823,"
1 + log(2d)

."
EXP,0.6492985971943888,"With the assumption that h(u) ≥h◦+ C0(log(R) + 1), the geometric sum in (96) has a decay rate
smaller than 1/2. Therefore,
X"
EXP,0.6496993987975952,"J⊆[du] :|J|≥2
aI,J ≤1"
EXP,0.6501002004008016,"2 exp

−ε"
EXP,0.6505010020040081,"2(h(u) −C2(log(R) + 1) −h◦)

,"
EXP,0.6509018036072144,"where
C2 = C1 + 2 ε"
EXP,0.6513026052104208,"
1 + log(d)

."
EXP,0.6517034068136273,"Now applying the above estimate, together with P"
EXP,0.6521042084168337,"I⊆[du] :|I|≥2 γ2
I = 1, to (95), we obtain the
following bound: ∥A∥≤1"
EXP,0.6525050100200401,"2 exp

−ε"
EXP,0.6529058116232465,"2(h(u) −C1(log(R) + 1) −h◦)

."
EXP,0.6533066132264529,Comparison of P
EXP,0.6537074148296593,"I minθ∈[q](Euf 2
u,I)(θ) and minθ∈[q](Euf 2
u)(θ) (and the same for g): Here is the
last step toward the proof of the Proposition. Returning to (94), we have
(Eufugu)(θ) −(Eufugu)(θ′) ≤1"
EXP,0.6541082164328658,"2 exp

−ε"
EXP,0.6545090180360722,"2(h(u) −C1(log(R) + 1) −h◦)

·
sX"
EXP,0.6549098196392785,"I
min
θ∈[q](Euf 2
u,I)(θ) ·
sX"
EXP,0.655310621242485,"I
min
θ∈[q](Eug2
u,I)(θ)."
EXP,0.6557114228456914,Let us impose the third assumption on C0 that
EXP,0.6561122244488978,C0 ≥CE.7 + 2
EXP,0.6565130260521042,ε log(2)
EXP,0.6569138276553106,where CE.7 introduced in Corollary E.7. Recall that we have h∗= h◦+ 2
EXP,0.657314629258517,"ε log(2) from (87). We can
invoke this Corollary to yield:"
EXP,0.6577154308617235,"∀θ ∈[q],
sX"
EXP,0.6581162324649299,"I
(Euf 2
u,I)(θ) ≤
p"
EXP,0.6585170340681362,2Ef 2u(θ).
EXP,0.6589178356713427,"Let θ0 ∈[q] be the value minimizing θ 7→
p"
EXP,0.6593186372745491,"Ef 2u(θ). Then,"
EXP,0.6597194388777555,"min
θ∈[q] p"
EXP,0.660120240480962,"2Ef 2u(θ) =
p"
EXP,0.6605210420841683,"2Ef 2u(θ0) ≥
sX"
EXP,0.6609218436873747,"I
(Euf 2
u,I)(θ0) ≥
sX"
EXP,0.6613226452905812,"I
min
θ∈[q](Euf 2
u,I)(θ)."
EXP,0.6617234468937876,"Clearly, the same derivation also holds for gu. Together we conclude that
(Eufugu)(θ) −(Eufugu)(θ′)"
EXP,0.662124248496994,"≤exp

−ε"
EXP,0.6625250501002004,"2(h(u) −C2(log(R) + 1) −h◦)
 
min
θ∈[q](Euf 2
u)(θ)
1/2 
min
θ∈[q](Eug2
u)(θ)
1/2."
EXP,0.6629258517034068,"Finally, if we impose the forth assumption on C0 that"
EXP,0.6633266533066132,"C0 ≥C2,"
EXP,0.6637274549098197,then the Proposition follows.
EXP,0.6641282565130261,"H.2
Properties of fk: Products"
EXP,0.6645290581162324,The goal of this subsection is to establish the following.
EXP,0.6649298597194389,"Proposition H.6. There exists C = C(M, d) ≥1 so that the following holds. For any ρ′ ∈T
satisfying
h(ρ′) ≥h◦+ C(log(R) + 1)"
EXP,0.6653306613226453,"and a positive integer h◦+ C(log(R) + 1) ≤k1 ≤h(ρ′). Consider a function f and g are B≤ρ′
polynomials with Ef(X) = Eg(X) = 0. We decompose f and g according to Lemma D.1 with the
given k1. Then, the following holds: For k1 ≤m, k ≤h(ρ′),"
EXP,0.6657314629258517,"• If max{m, k} > k1,"
EXP,0.6661322645290582,"max
θ,θ′∈[q]"
EXP,0.6665330661322645,(Eρ′fkgm)(θ) −(Eρ′fkgm)(θ′)
EXP,0.6669338677354709,"≤exp

−ε"
EXP,0.6673346693386774,"2(2h(ρ′) −max{k, m} −C(log(R) + 1) −h◦)

(Ef 2
k(X))1/2(Eg2
m(X))1/2."
EXP,0.6677354709418838,"• If k = m = k1,"
EXP,0.6681362725450902,"max
θ,θ′∈[q]"
EXP,0.6685370741482966,(Eρ′fkgm)(θ) −(Eρ′fkgm)(θ′)
EXP,0.668937875751503,"≤exp

−ε"
EXP,0.6693386773547094,"2(2h(ρ′) −2k1 −C(log(R) + 1))

(Ef 2
k(X))1/2(Eg2
m(X))1/2."
EXP,0.6697394789579159,"The proof mirrors the structure used in Proposition F.1. In this case, we rely on both Proposition E.1
and Proposition H.2. Through this subsection, let"
EXP,0.6701402805611223,"C◦= C◦(M, d)"
EXP,0.6705410821643286,"be the constant described in the Proposition. The functions f, g, and k1 are as introduced in the
Proposition."
EXP,0.670941883767535,"Assuming without lose of generality that m ≤k, we apply the reasoning from (72) in Proposition
F.1, yielding
(Eρ′fkgm)(θ)"
EXP,0.6713426853707415,"=E
h
X"
EXP,0.6717434869739479,"u∈Dk(ρ′)
(Eu ˜fu)(X)

X"
EXP,0.6721442885771544,"u∈Dk(ρ′)
(Eugm,u)(X)
  Xρ′ = θ
i
+
X"
EXP,0.6725450901803607,"u∈Dk(ρ′)
(Eρ′ ˜fugm,u)(θ) −
X"
EXP,0.6729458917835671,"u∈Dk(ρ′)
E
h
(Eu ˜fu)(Xu)(Eugm,u)(Xu)
 Xρ′ = θ
i
,"
EXP,0.6733466933867736,"and hence,
(Eρ′fkgm)(θ) −(Eρ′fkgm)(θ′)"
EXP,0.67374749498998,"=
 
Eρ′(Ekfk)(Ekgm)

(θ) −
 
Eρ′(Ekfk)(Ekgm)

(θ′) +
X"
EXP,0.6741482965931864,u∈Dk(ρ′)
EXP,0.6745490981963927," 
(Eρ′ ˜fugm,u)(θ) −(Eρ′ ˜fugm,u)(θ′)
 −

X"
EXP,0.6749498997995992,u∈Dk(ρ′)
EXP,0.6753507014028056," 
(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ) −
 
(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ′)

.
(97)"
EXP,0.675751503006012,"Similar to the derivation of (64) from Proposition F.1. The proof is dedicated into estimating the
above three summands."
EXP,0.6761523046092185,"We begin with the following estimate:
Lemma H.7. There exists a constant C = C(M, d) so that the following holds. Suppose C◦≥CH.2,
where CH.2 is the constant introduced in Proposition H.2. Then, the following holds: For u ∈Dk(ρ′),"
EXP,0.6765531062124248,"1. if k > k1, then"
EXP,0.6769539078156313,"max
θ,θ′∈[q]"
EXP,0.6773547094188377,"(Eρ′ ˜fugm,u)(θ) −Eρ′ ˜fugm,u)(θ′)"
EXP,0.6777555110220441,"≤exp

−ε"
EXP,0.6781563126252504,"2(2h(ρ′) −k −C(log(R) + 1) −h◦)

(E ˜f 2
u(X))1/2(Eg2
m,u(X))1/2;"
EXP,0.6785571142284569,"2. if k = m = k1, then"
EXP,0.6789579158316633,"max
θ,θ′∈[q]"
EXP,0.6793587174348698,"(Eρ′ ˜fugm,u)(θ) −Eρ′ ˜fugm,u)(θ′)"
EXP,0.6797595190380762,"≤exp

−ε"
EXP,0.6801603206412825,"2(2h(ρ′) −2k −C)

(E ˜f 2
u(X))1/2(Eg2
m,u(X))1/2."
EXP,0.680561122244489,"Proof. Step 1. Bound E| ˜fu(X)gm,u(X) −E ˜fugm,u| from above: By Hölder’s inequality,"
EXP,0.6809619238476954,"Var

(Eu ˜fugm,u)(Xu)
"
EXP,0.6813627254509018,"|
{z
}
ℓ2-norm"
EXP,0.6817635270541083,"≤max
θ∈[q]"
EXP,0.6821643286573146,"(Eu ˜fugm,u)(θ) −E ˜fugm,u"
EXP,0.682565130260521,"|
{z
}
ℓ∞-norm"
EXP,0.6829659318637274,"· E
(Eu ˜fugm,u)(Xu) −E ˜fugm,u

|
{z
}
ℓ1-norm ≤
q"
EXP,0.6833667334669339,"CA.5Var

(Eu ˜fugm,u)(Xu)

· E| ˜fu(X)gm,u(X) −E ˜fugm,u| ⇔
q"
EXP,0.6837675350701403,"Var

(Eu ˜fugm,u)(Xu)

≤
p"
EXP,0.6841683366733466,"CA.5E| ˜fu(X)gm,u(X) −E ˜fugm,u|,
(98)"
EXP,0.6845691382765531,"where we applied (9) from Lemma A.5 with CA.5 is the constant introduced in the Lemma. Further,
relying on (98), together with (9) and (8) from the Lemma A.5, we have"
EXP,0.6849699398797595,"max
θ,θ′∈[q]"
EXP,0.685370741482966,"(Eρ′ ˜fugm,u)(θ) −Eρ′ ˜fugm,u)(θ′)"
EXP,0.6857715430861724,"≤2 max
θ∈[q]"
EXP,0.6861723446893787,"(Eρ′ ˜fugm,u)(θ) −E ˜fugm,u"
EXP,0.6865731462925851,"≤2CA.5(h(ρ′) −k)qλh(ρ′)−k max
θ∈[q] |(Eu ˜fugm,u)(θ) −E ˜fugm,u|"
EXP,0.6869739478957916,"≤2C2
A.5(h(ρ′) −k)qλh(ρ′)−kp"
EXP,0.687374749498998,"CA.5E| ˜fu(X)gm,u(X) −E ˜fugm,u|"
EXP,0.6877755511022045,"=C1 exp(−ε(h(ρ′) −k))E| ˜fu(X)gm,u(X) −E ˜fugm,u|,
(99)"
EXP,0.6881763527054108,"where
C1 = 2C5/2
A.5 · max
n∈N nq exp(−0.1εn)."
EXP,0.6885771543086172,Case 1: m < k. Here we can simply recycle the estimate from (76):
EXP,0.6889779559118236,"E| ˜fu(X)gm,u(X) −E ˜fugm,u| ≤2E| ˜fu(X)gm,u(X)|"
EXP,0.6893787575150301,"≤exp

−ε"
EXP,0.6897795591182365,"2
 
k −C2(log(R) + 1) −h◦
(E ˜f 2
u(X))1/2(Eg2
m,u(X))1/2, (100) where"
EXP,0.6901803607214428,C2 = 2 ε 3 2 + 1
EXP,0.6905811623246493,"2 log(d) + log(CD.1)

+ CE.1 + 2 · 2"
EXP,0.6909819639278557,"ε log(2),"
EXP,0.6913827655310621,"where CD.1 is the constant introduced in Lemma D.1 and CE.1 is the constant introduced in Proposi-
tion E.1."
EXP,0.6917835671342686,Case 2: k1 < m = k.
EXP,0.6921843687374749,"This is the case where we need Proposition H.2. With the assumption that C◦≥CH.2, where
CH.2 ≥1 is the constant introduced in the Proposition, we have"
EXP,0.6925851703406813,"m = k > k1 ≥h◦+ C◦(log(R) + 1) ≥h◦+ CH.2(log(R) + 1),"
EXP,0.6929859719438878,so that we could apply the Proposition to get
EXP,0.6933867735470942,"E| ˜fu(X)gm,u(X) −E ˜fugm,u|"
EXP,0.6937875751503007,"≤max
θ,θ′
(Eufugu)(θ) −(Eufugu)(θ′)"
EXP,0.694188376753507,"≤exp

−ε"
EXP,0.6945891783567134,"2(k −CH.2(log(R) + 1) −h◦)

(E ˜f 2
u(X))1/2(Eg2
m,u(X))1/2."
EXP,0.6949899799599198,Case 3: k1 = m = k The last case is straightforward:
EXP,0.6953907815631263,"E| ˜fu(X)gm,u(X) −E ˜fugm,u| ≤2E| ˜fu(X)gm,u(X)| ≤2
q"
EXP,0.6957915831663327,"E ˜f 2u(X)
q"
EXP,0.696192384769539,"Eg2m,u(X)."
EXP,0.6965931863727455,"By taking C3 = max{C2, CH.2}+ 2"
EXP,0.6969939879759519,"ε log(C1), the statement of the Lemma follows with C = C3."
EXP,0.6973947895791583,"As an analogue of the above Lemma, we also have"
EXP,0.6977955911823648,"Lemma H.8. There exists a constant C = C(M, d) so that the following holds. Suppose C◦≥CH.2
is the constant introduced in Proposition H.2. Then, the following holds: For u ∈Dk(ρ′),"
EXP,0.6981963927855711,"1. if k > k1, then"
EXP,0.6985971943887775,"max
θ,θ′∈[q]"
EXP,0.698997995991984,"(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ) −
 
(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ′)"
EXP,0.6993987975951904,"≤exp

−ε"
EXP,0.6997995991983968,"2(2h(ρ′) −k −C(log(R) + 1) −h◦)

(E ˜f 2
u(X))1/2(Eg2
m,u(X))1/2."
EXP,0.7002004008016032,"2. if k = m = k1, then
(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ) −
 
(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ′)"
EXP,0.7006012024048096,"≤exp

−ε"
EXP,0.701002004008016,"2(2h(ρ′) −2k −C)

(E ˜f 2
u(X))1/2(Eg2
m,u(X))1/2."
EXP,0.7014028056112225,"Since the proof is simpler and the structure is the same as that for Lemma H.7, we will outline a
sketch proof in this case."
EXP,0.7018036072144288,"Proof. Let au(xu) = (Eu ˜fu)(xu) and bu = (Eugm,u)(xu). Repeating the first step of the proof of
Lemma H.7, we have"
EXP,0.7022044088176352,"max
θ,θ′∈[q]"
EXP,0.7026052104208417,"(Eρ′aubu)(θ) −Eρ′aubu)(θ′)
 ≤C1 exp(−ε(h(ρ′) −k))E|au(X)bu(X) −Eaubu|,"
EXP,0.7030060120240481,"with
C1 := 2C5/2
A.5 · max
n∈N nq exp(−0.1εn),"
EXP,0.7034068136272545,"which is exactly the same constant stated in Lemma H.7. Next,"
EXP,0.7038076152304609,"E|au(X)bu(X)−Eaubu| ≤2E|au(X)bu(X)| ≤2
p"
EXP,0.7042084168336673,"Ea2u(X)
p"
EXP,0.7046092184368737,"Eb2u(X) ≤2
p"
EXP,0.7050100200400802,"Ea2u(X)
q"
EXP,0.7054108216432866,"Eg2m,u(X)."
EXP,0.7058116232464929,"If k > k1, we could apply (38) from Proposition E.1 to ˜fu, and get
p"
EXP,0.7062124248496994,"Ea2u(X) ≤exp

−2ε(k −CE.1(log(R) + 1) −h◦−2"
EXP,0.7066132264529058,"ε log(2)
|
{z
}
−h∗ )
q"
EXP,0.7070140280561122,E ˜f 2u(X).
EXP,0.7074148296593187,"Indeed, this tail bound is stronger than what we got from Lemma H.7. The remainning part involves
combining these estimates with a suitable constant C so that the lemma holds. Given the argument
was already presented in the proof of Lemma H.7, we will omit these details."
EXP,0.707815631262525,"Before bounding the summands in (97), let us bound P"
EXP,0.7082164328657314,"u∈Dk(ρ′) E ˜f 2
u(X) and P"
EXP,0.7086172344689379,"u∈Dk(ρ′) Eg2
m,u(X)
from above by Ef 2
k(X) and Eg2
m(X), respectively.
Lemma H.9. Suppose"
EXP,0.7090180360721443,C◦≥CF.2 + 2
EXP,0.7094188376753507,"ε log(2),"
EXP,0.7098196392785571,"where CF.2 is the constant introduced in Lemma F.2. Then,
X"
EXP,0.7102204408817635,"u∈Dk(ρ′)
Eg2
m,u(X) ≤max

4, C2
D.1R4	
· Eg2
m(X), and
X"
EXP,0.7106212424849699,"u∈Dk(ρ′)
E ˜f 2
u(X) ≤max

4, C2
D.1R4	
· Ef 2
k(X)."
EXP,0.7110220440881764,Proof. Given that C◦≥CF.2 + 2
EXP,0.7114228456913828,"ε log(2), we have"
EXP,0.7118236472945891,k1 ≥h◦+ C◦(log(R) + 1) > h◦+ 2
EXP,0.7122244488977956,"ε log(2) + CF.2(log(R) + 1) = h∗+ CF.2(log(R) + 1),"
EXP,0.712625250501002,"and thus we could apply Lemma F.2. When m > k1, the lemma yields
X"
EXP,0.7130260521042084,"u∈Dk(ρ′)
Eg2
m,u(X) =
X"
EXP,0.7134268537074149,"u∈Dk(ρ′)
E
h
X"
EXP,0.7138276553106212,"v∈Dm(u)
˜gv(X)
2i
≤
X"
EXP,0.7142284569138276,u∈Dk(ρ′) X
EXP,0.7146292585170341,"v∈Dm(u)
2E˜g2
v(X) ≤4Eg2
m(X)."
EXP,0.7150300601202405,"And in the case when m = k1, we use the same derivation with Lemma F.2 been replaced by (31) in
Lemma D.1 to get
X"
EXP,0.7154308617234469,"u∈Dk(ρ′)
Eg2
m,u(X) ≤CD.1R3 · CD.1REg2
m(X)."
EXP,0.7158316633266533,"Clearly, the same derivation also holds for the comparison of P"
EXP,0.7162324649298597,"u∈Dk(ρ′) E ˜f 2
u(X) and Ef 2
k(X)."
EXP,0.7166332665330661,"Now, relying on the above two lemmas, we will estimate the second and third summand of (97):
Corollary H.10. There exists a constant C = C(M, d) ≥1 so that the following holds. Suppose"
EXP,0.7170340681362726,"C◦≥max
n
CH.2, CF.2 + 2"
EXP,0.717434869739479,"ε log(2)
o
,"
EXP,0.7178356713426853,"where the constants are introduced in Proposition H.2 and Lemma F.2, respectively. Then,"
EXP,0.7182364729458918,"1. if k > k1, then

X"
EXP,0.7186372745490982,u∈Dk(ρ′)
EXP,0.7190380761523046," 
(Eρ′ ˜fugm,u)(θ) −(Eρ′ ˜fugm,u)(θ′)
 −

X"
EXP,0.7194388777555111,u∈Dk(ρ′)
EXP,0.7198396793587174," 
(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ) −
 
(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ′)
"
EXP,0.7202404809619238,"≤exp

−ε"
EXP,0.7206412825651303,"2(2h(ρ′) −k −C(log(R) + 1) −h◦)

(Ef 2
k(X))1/2(Eg2
m(X))1/2."
EXP,0.7210420841683367,"2. if k = m = k1, then the above term above can be bounded by"
EXP,0.7214428857715431,"exp

−ε"
EXP,0.7218436873747495,"2(2h(ρ′) −2k −C)

(Ef 2
k(X))1/2(Eg2
m(X))1/2."
EXP,0.7222444889779559,"Proof. Let C1 be the maximum of the two constants introduced in Lemma H.7 and Lemma H.8. For
convenience, let"
EXP,0.7226452905811623,"U :=

X"
EXP,0.7230460921843688,u∈Dk(ρ′)
EXP,0.7234468937875751," 
(Eρ′ ˜fugm,u)(θ) −(Eρ′ ˜fugm,u)(θ′)
 −

X"
EXP,0.7238476953907815,u∈Dk(ρ′)
EXP,0.724248496993988," 
(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ) −
 
(Eρ′ 
Eu ˜fu)(Eugm,u)

(θ′)
."
EXP,0.7246492985971944,"By the two lemmas together with the triangle inequality, in the case when k > k1, we have U ≤
X"
EXP,0.7250501002004008,"u∈Dk(ρ′)
2 exp

−ε"
EXP,0.7254509018036072,"2(2h(ρ′) −k −C1(log(R) + 1) −h◦)

(E ˜f 2
u(X))1/2(Eg2
m,u(X))1/2"
EXP,0.7258517034068136,"≤2 exp

−ε"
EXP,0.72625250501002,"2(2h(ρ′) −k −C1(log(R) + 1) −h◦)
s
X"
EXP,0.7266533066132265,"u∈Dk(ρ′)
E ˜f 2u(X)
s
X"
EXP,0.7270541082164329,"u∈Dk(ρ′)
Eg2m,u(X)"
EXP,0.7274549098196392,"≤2 max
n
4, C2
D.1R4oq"
EXP,0.7278557114228457,"Ef 2
k(X)
p"
EXP,0.7282565130260521,"Eg2m(X),
(101)"
EXP,0.7286573146292585,"where the last inequality follows from Lemma H.9. Similarly, when k = m = k1, we have"
EXP,0.729058116232465,"U ≤2 exp

−ε"
EXP,0.7294589178356713,"2(2h(ρ′) −2k −C1)
s
X"
EXP,0.7298597194388777,"u∈Dk(ρ′)
E ˜f 2u(X)
s
X"
EXP,0.7302605210420842,"u∈Dk(ρ′)
Eg2m,u(X).
(102)"
EXP,0.7306613226452906,By setting C = 2
EXP,0.731062124248497,"ε
 
C1 + log(4) + log(C2
D.1)

,"
EXP,0.7314629258517034,the corollary follows.
EXP,0.7318637274549098,"It remains to estimate the first summand of (97):
Lemma H.11. There exists a constant C = C(M, d) ≥1 so that the following holds. Suppose"
EXP,0.7322645290581162,C◦≥CF.2 + 2
EXP,0.7326653306613227,ε log(2)
EXP,0.7330661322645291,"where CF.2 is the constant introduced in Lemma F.2. Then,"
EXP,0.7334669338677354,"1. if k > k1, then"
EXP,0.7338677354709419,"max
θ,θ′∈[q]"
EXP,0.7342685370741483," 
Eρ′(Ekfk)(Ekgm)

(θ) −
 
Eρ′(Ekfk)(Ekgm)

(θ′)"
EXP,0.7346693386773547,"≤exp

−ε(h(ρ′) −C(log(R) + 1) −h◦)
q"
EXP,0.7350701402805612,"Ef 2
k(X)
p"
EXP,0.7354709418837675,Eg2m(X).
EXP,0.7358717434869739,"2. if k = m = k1, then the above term is bounded by"
EXP,0.7362725450901804,"exp(−ε(h(ρ′) −k1 −C(log(R) + 1)))
q"
EXP,0.7366733466933868,"Ef 2
k(X)
p"
EXP,0.7370741482965932,Eg2m(X).
EXP,0.7374749498997996,"Proof. Observe
that
both
(Ekfk)(x)
=
P
u∈Dk(ρ′)(Eu ˜fu)(xu)
and
(Ekgm)(x)
=
P"
EXP,0.737875751503006,"u∈Dk(ρ′)(Ekgm,u)(xu) are both degree-1 polynomials with variables (xu : u ∈Dk(ρ′)) sat-
isfying
E(Ekgm)(X) = E(Ekfk) = 0.
This allows us to apply Lemma G.4, yielding"
EXP,0.7382765531062124,"max
θ,θ′∈[q]"
EXP,0.7386773547094189," 
Eρ′(Ekfk)(Ekgm)

(θ) −
 
Eρ′(Ekfk)(Ekgm)

(θ′)"
EXP,0.7390781563126253,"≤2 max
θ"
EXP,0.7394789579158316," 
Eρ′(Ekfk)(Ekgm)

(θ) −E
 
Ekfk)(Ekgm)
"
EXP,0.7398797595190381,"≤2CG.4R exp(−ε(h(ρ′) −k))
s
X"
EXP,0.7402805611222445,"u∈Dk(ρ′)
E

(Eu ˜fu)2(X)
s
X"
EXP,0.7406813627254509,"u∈Dk(ρ′)
E

(Eugm,u)2(X)

,"
EXP,0.7410821643286574,"where CG.4 ≥1 is the constant introduced in Lemma G.4. Next, we apply Lemma H.9 (which is
why we need the assumption on C◦) to get
s
X"
EXP,0.7414829659318637,"u∈Dk(ρ′)
E

(Eugm,u)2(X)

≤
s
X"
EXP,0.7418837675350701,"u∈Dk(ρ′)
Eg2m,u(X) ≤
q"
EXP,0.7422845691382766,"max

4, C2
D.1R4	
· Eg2m(X)."
EXP,0.742685370741483,"As for
qP"
EXP,0.7430861723446894,"u∈Dk(ρ′) E

(Eu ˜fu)2(X)

, if k = m = k1, then we can apply the same derivation to get
s
X"
EXP,0.7434869739478958,"u∈Dk(ρ′)
E

(Eu ˜fu)2(X)

≤
q"
EXP,0.7438877755511022,"max

4, C2
D.1R4	
· Ef 2
k(X)."
EXP,0.7442885771543086,This leads to
EXP,0.7446893787575151,"max
θ,θ′∈[q]"
EXP,0.7450901803607214," 
Eρ′(Ekfk)(Ekgm)

(θ) −
 
Eρ′(Ekfk)(Ekgm)

(θ′)"
EXP,0.7454909819639278,"≤2CG.4 max

4, C2
D.1R4	
exp(−ε(h(ρ′) −k))
q"
EXP,0.7458917835671343,"Ef 2
k(X)
p"
EXP,0.7462925851703407,Eg2m(X).
EXP,0.7466933867735471,"If k > k1, then we can apply Proposition E.1 and Lemma H.9 to get
s
X"
EXP,0.7470941883767535,"u∈Dk(ρ′)
E

(Eu ˜fu)2(X)
"
EXP,0.7474949899799599,"≤exp

−ε(k −CE.1(log(R) + 1) −h◦−2"
EXP,0.7478957915831663,"ε log(2))
s
X"
EXP,0.7482965931863728,"u∈Dk(ρ′)
E ˜f 2u(X) ≤
q"
EXP,0.7486973947895792,"max

4, C2
D.1R4	
exp

−ε(k −CE.1(log(R) + 1) −h◦−2"
EXP,0.7490981963927855,"ε log(2))
q"
EXP,0.749498997995992,"Ef 2
k(X)."
EXP,0.7498997995991984,"In this case, we have"
EXP,0.7503006012024048,"max
θ,θ′∈[q]"
EXP,0.7507014028056113," 
Eρ′(Ekfk)(Ekgm)

(θ) −
 
Eρ′(Ekfk)(Ekgm)

(θ′)"
EXP,0.7511022044088176,"≤2CG.4 max

4, C2
D.1R4	
exp

−ε(h(ρ′) −CE.1(log(R) + 1) −h◦−2"
EXP,0.751503006012024,"ε log(2))
q"
EXP,0.7519038076152305,"Ef 2
k(X)
p"
EXP,0.7523046092184369,Eg2m(X).
EXP,0.7527054108216433,By taking
EXP,0.7531062124248497,C = CE.1 + 2
EXP,0.7535070140280561,ε log(2) + 1 ε
EXP,0.7539078156312625,"
log(2CG.4) + log(C2
D.1) + 4

,"
EXP,0.754308617234469,both statements of the lemma follows.
EXP,0.7547094188376754,"Proof of Proposition H.6. Without lose of generality, it is sufficient to prove the case when m ≤k."
EXP,0.7551102204408817,"First, we impose the first assumption that"
EXP,0.7555110220440882,"C◦≥max

CH.2, CF.2 + 2"
EXP,0.7559118236472946,"ε log(2)

,"
EXP,0.756312625250501,"where the constants are introduced in Proposition H.2 and Lemma F.2, respectively. This allows us to
apply Corollary H.10 and Lemma H.11. For simplicity, let"
EXP,0.7567134268537075,"C1 := max{CH.10, CH.11}."
EXP,0.7571142284569138,"Then, combining the Corollary and the Lemma to the estimate (97) we can conclude that: For
k1 ≤m ≤k with k > k1,"
EXP,0.7575150300601202,"max
θ,θ′∈[q]"
EXP,0.7579158316633267,(Eρ′fkgm)(θ) −(Eρ′fkgm)(θ′)
EXP,0.7583166332665331,"≤2 exp

−ε"
EXP,0.7587174348697395,"2(2h(ρ′) −k −C1(log(R) + 1) −h◦)

(Ef 2
k(X))1/2(Eg2
m(X))1/2,"
EXP,0.7591182364729459,"and in the case where k = m = k1, the above term is bounded by"
EXP,0.7595190380761523,"2 exp

−ε"
EXP,0.7599198396793587,"2(2h(ρ′) −2k −C1(log(R) + 1))

(Ef 2
k(X))1/2(Eg2
m(X))1/2."
EXP,0.7603206412825652,"Then, the proof of the proposition follows by making the second assumption on C◦that"
EXP,0.7607214428857716,C◦≥C1 + 2
EXP,0.7611222444889779,ε log(2).
EXP,0.7615230460921844,"H.3
Proof of Theorem G.3"
EXP,0.7619238476953908,"Proof. Now we are ready to establish the main theorem. As usual, let C0 = C0(M, d) denote the
constant introduced in the statement of the Theorem. The value of C0 will be determined as the proof
proceeds."
EXP,0.7623246492985972,Applying Theorem B.6 with A and h∗= h◦+ 2
EXP,0.7627254509018037,"ε log(2) and c∗= 1/2, we conclude that"
EXP,0.76312625250501,"Var

(Eρ′f)(X)

≤exp
 
−ε(h(ρ′) −CB.6(log(R) + 1) −h∗
Var

f(X)

."
EXP,0.7635270541082164,"for any B≤ρ′-polynomial f, where CB.6 = C(M, d, 1/2) is the constant introduced by the theorem.
We impose the first assumption on C0 that"
EXP,0.7639278557114229,C0 ≥CB.6 + 2
EXP,0.7643286573146293,"ε log(2),"
EXP,0.7647294589178357,and conclude that
EXP,0.765130260521042,"Var

(Eρ′f)(X)

≤exp
 
−ε(h(ρ′) −C0(log(R) + 1) −h◦
Var

f(X)

."
EXP,0.7655310621242485,"Now, it remains to show that with the suitable choice of C0, for any ρ′ with h(ρ′) ≥h◦+C0(log(R)+
1) and any two B≤ρ′-polynomials f and g, we have"
EXP,0.7659318637274549,"max
θ∈[q] |(Eρ′fg)(θ)−Efg| ≤exp

−ε"
EXP,0.7663326653306614,"2(h(ρ′)−h◦−C0(log(R)+1))
q"
EXP,0.7667334669338677,"min
θ (Eρ′f 2)(θ) min
θ′ (Eρ′g2)(θ). Let"
EXP,0.7671342685370741,"C1 := max
n
CH.6, 2"
EXP,0.7675350701402806,"ε log(2) + CF.1 + t0
o
, where"
EXP,0.767935871743487,"• t0 is the constant such that P∞
t=t0 exp

−ε"
T,0.7683366733466934,"2t

≤1 2,"
T,0.7687374749498997,"• CH.6 is the constant introduced in Proposition H.6, and"
T,0.7691382765531062,"• CF.1 = C(M, d, 1/2) is the constant introduced in Proposition F.1."
T,0.7695390781563126,"Next, let
k1 = ⌈h◦+ C1(log(R) + 1)⌉.
The chocie of C1 and k1 allow us to apply Proposition H.6 and Proposition F.1 toward both f and g."
T,0.769939879759519,"Next, we impose the second assumption on C0 that"
T,0.7703406813627255,C0 ≥2C1 + 2.
T,0.7707414829659318,"This assumption implies that there is a gap between h(ρ′) and k1, which is necessary for the proof."
T,0.7711422845691382,"Now, we fix such ρ′ and consider two B≤ρ′-polynomial f and g with Ef(X) = Eg(X) = 0. Further,
consider the decomposition of f and g according to Lemma D.1 with the above chosen k1."
T,0.7715430861723447,"First, by our choice of C1, we have"
T,0.7719438877755511,k1 ≥⌈h◦+ 2
T,0.7723446893787576,"ε log(2)
|
{z
}
=h∗"
T,0.7727454909819639,+CF.1(log(R) + 1) + t0⌉.
T,0.7731462925851703,"This assumption allow us to recycle the partial step in the proof of Theorem B.6 to obtain (80):
X"
T,0.7735470941883767,"k∈[k1,h(ρ′)]
Ef 2
k(X) ≤2Ef 2(X)
and
X"
T,0.7739478957915832,"k∈[k1,h(ρ′)]
Eg2
k(X) ≤2Eg2(X).
(103)"
T,0.7743486973947896,"Second, with our assumption that k1 ≥⌈h◦+ CH.6(log(R) + 1)⌉, we can apply Proposition H.6 to
get"
T,0.774749498997996,"max
θ,θ′∈[q]"
T,0.7751503006012024,"(Eρ′fg)(θ) −(Eρ′fg)(θ′)
 ≤
X"
T,0.7755511022044088,"m,k∈[k1,h(ρ′)]
max
θ,θ′∈[q]"
T,0.7759519038076153,"(Eρ′fkgm)(θ) −(Eρ′fkgm)(θ′) ≤
X"
T,0.7763527054108217,"m,k∈[k1,h(ρ′)]
akmαkβm = ⃗α⊤A⃗β ≤∥⃗α∥∥A∥∥⃗β∥,"
T,0.776753507014028,"where ⃗α = (αk1, αk2, . . . , αh(ρ′)) with αk =
p"
T,0.7771543086172344,"Ef 2
k(X), ⃗β = (βk1, βk2, . . . , βh(ρ′)) with βm =
p"
T,0.7775551102204409,"Eg2m(X), and A = (akm)k,m∈[k1,h(ρ′)] with"
T,0.7779559118236473,"akm := 
 "
T,0.7783567134268538,"exp

−ε"
T,0.7787575150300601,"2(h(ρ′) + h(ρ′) −max{k, m} −CH.6(log(R) + 1) −h◦)

max{k, m} > k1"
T,0.7791583166332665,"exp

−ε"
T,0.779559118236473,"2(h(ρ′) + h(ρ′) −2k1 −CH.6(log(R) + 1))

k = m = k1."
T,0.7799599198396794,"Together with (103), we have"
T,0.7803607214428858,"∥⃗α∥∥A∥∥⃗β∥≤2∥A∥
p"
T,0.7807615230460921,"Ef 2(X)
p"
T,0.7811623246492986,Eg2(X).
T,0.781563126252505,"The next goal is to bound ∥A∥from above. Notice the fact that the matrix A is symmetric implies
there exists a unit vector ⃗γ such that ∥A∥= ⃗γ⊤A⃗γ. Now we fix such vector ⃗γ. Relying on the fact
that akm ≥0,"
T,0.7819639278557114,"∥A∥=
X"
T,0.7823647294589179,"k,m∈[k1,h(ρ′)]
akmγkγm ≤
X"
T,0.7827655310621242,"k,m∈[k1,h(ρ′)] akm"
T,0.7831663326653306,"2 (γ2
k + γ2
m) =
X"
T,0.7835671342685371,"m∈[k1,h(ρ′)]
γ2
m

X"
T,0.7839679358717435,"k∈[k1,h(ρ′)]
akm

."
T,0.78436873747495,"Clearly, from the definition of akm, the term P"
T,0.7847695390781563,"k∈[k1,h(ρ′)] akm is maximized when m = k1.
X"
T,0.7851703406813627,"k∈[k1,h(ρ′)]
akk1 = exp

−ε"
T,0.7855711422845691,"2(h(ρ′) + h(ρ′) −2k1 −CH.6(log(R) + 1))
 +
X"
T,0.7859719438877756,"k∈[k1,h(ρ′)]
exp

−ε"
T,0.786372745490982,"2(h(ρ′) + h(ρ′) −k −CH.6(log(R) + 1) −h◦)
"
T,0.7867735470941883,"= exp

−ε"
T,0.7871743486973948,"2(h(ρ′) −CH.6(log(R) + 1) −h◦)

·"
T,0.7875751503006012,"·

exp

−ε"
T,0.7879759519038076,"2(h(ρ′) −2k1 + h◦)

+
X"
T,0.7883767535070141,"k∈[k1,h(ρ′)]
exp

−ε"
T,0.7887775551102204,"2(h(ρ′) −k)

."
T,0.7891783567134268,"First,
X"
T,0.7895791583166333,"k∈[k1,h(ρ′)]
exp

−ε"
T,0.7899799599198397,"2(h(ρ′) −k)

≤
1
1 −exp(−ε/2) ≤4 ε."
T,0.790380761523046,"Second,"
T,0.7907815631262525,"exp

−ε"
T,0.7911823647294589,"2(h(ρ′) −2k1 + h◦)

≤exp

−ε 2"
T,0.7915831663326653,"
C0(log(R) + 1) + h◦−2(h◦+ C1(log(R) + 1) + 1) + h◦"
T,0.7919839679358718,"≤exp

−ε"
T,0.7923847695390781,"2(C0 −2C1 −2)(log(R) + 1)

≤1,"
T,0.7927855711422845,"which in turn implies that

exp

−ε"
T,0.793186372745491,"2(h(ρ′) −2k1 + h◦)

+
X"
T,0.7935871743486974,"k∈[k1,h(ρ′)]
exp

−ε"
T,0.7939879759519038,"2(h(ρ′) −k)

≤5 ε."
T,0.7943887775551102,"Hence, we conclude that ∥A∥≤5"
T,0.7947895791583166,"ε exp

−ε"
T,0.795190380761523,"2(h(ρ′) −CH.6(log(R) + 1) −h◦)

."
T,0.7955911823647295,"Together we conclude that when h(ρ) ≥h◦+ C1(log(R) + 1), any two B≤ρ′-polynomials f and g
with Ef(X) = Eg(X) = 0 satisfies"
T,0.7959919839679359,"max
θ,θ′∈[q]"
T,0.7963927855711422,"(Eρ′fg)(θ) −(Eρ′fg)(θ′)

(104) ≤10"
T,0.7967935871743487,"ε exp

−ε"
T,0.7971943887775551,"2(h(ρ′) −CH.6(log(R) + 1) −h◦)
p"
T,0.7975951903807615,"Ef 2(X)
p"
T,0.797995991983968,Eg2(X).
T,0.7983967935871743,"Now, we impose the third assumption on C0 that"
T,0.7987975951903807,C0 ≥CH.6 + 2
T,0.7991983967935872,"ε log(20/ε), then 10"
T,0.7995991983967936,"ε exp

−ε"
T,0.8,"2(h(ρ′)−CH.6(log(R)+1)−h◦)

≤10"
T,0.8004008016032064,"ε exp

−ε"
T,0.8008016032064128,"2(C0 −CH.6)(log(R)+1)

≤1/2."
T,0.8012024048096192,"Next, we apply (104) to the special case that f = g:"
T,0.8016032064128257,"Ef 2(X) −min
θ∈[q](Eρ′f 2)(θ) ≤max
θ,θ′∈[q]"
T,0.8020040080160321,"(Eρ′fg)(θ) −(Eρ′fg)(θ′)
 ≤1"
T,0.8024048096192384,2Ef 2(X)
T,0.8028056112224449,"⇒Ef 2(X) ≤2 min
θ∈[q](Eρ′f 2)(θ)."
T,0.8032064128256513,"Clearly, the same statemnet holds for g as well. Substituting these estimates back to (104), we
can conclude that when h(ρ) ≥h◦+ C0(log(R) + 1), any two B≤ρ′-polynomials f and g with
Ef(X) = Eg(X) = 0 satisfies"
T,0.8036072144288577,"max
θ,θ′∈[q]"
T,0.8040080160320642,(Eρ′fg)(θ) −(Eρ′fg)(θ′) ≤20
T,0.8044088176352705,"ε exp

−ε"
T,0.8048096192384769,"2(h(ρ′) −CH.6(log(R) + 1) −h◦)
r"
T,0.8052104208416834,"min
θ∈[q](Eρ′f 2)(θ)
r"
T,0.8056112224448898,"min
θ∈[q](Eρ′g2)(θ)"
T,0.8060120240480962,"≤exp

−ε"
T,0.8064128256513026,"2(h(ρ′) −C0(log(R) + 1) −h◦)
r"
T,0.806813627254509,"min
θ∈[q](Eρ′f 2)(θ)
r"
T,0.8072144288577154,"min
θ∈[q](Eρ′g2)(θ)."
T,0.8076152304609219,"Therefore, the theorem follows."
T,0.8080160320641283,"I
Variance Estimate for degree 1 polynomial"
T,0.8084168336673346,"This section is dedicated to prove Proposition C.3. Let us restate it here:
Proposition I.1. There exists a constant C = C(M, d) ≥1 so that the following holds: Fix ρ′ ∈T,
and 0 ≤k ≤h(ρ′), then for any degree 1 function f with variables (xu : u ∈Dk(ρ′)). There exists
functions fu(x) = fu(xu) for u ∈Dk(ρ′) so that the following holds:"
T,0.8088176352705411,"1. f(X) = P
u∈Dk(ρ′) fu(Xu) almost surely. (They may not agree as functions from [q]T to
R.)"
T,0.8092184368737475,"2. For any v ∈Tρ′ with h(u) ≥k,
X"
T,0.8096192384769539,"u∈Dk(v)
Var[fu(Xu)] ≤CR3Var

X"
T,0.8100200400801604,"u∈Dk(v)
fu(Xu)

."
T,0.8104208416833667,"Example I.2. Suppose u, v ∈c(ρ′) for u, v, ρ′ ∈T and consider M = 1 2  "
T,0.8108216432865731,"1
0
1
0
0
1
0
1
1
0
1
0
0
1
0
1  ."
T,0.8112224448897796,Let us consider the function f(x) = fu(x) + fv(x) where
T,0.811623246492986,"fu(x) = 11,3(xu) =
1
if xu ∈{1, 3},
0
otherwise."
T,0.8120240480961923,"and fv(x) = −11,3(xv)."
T,0.8124248496993988,"Condition on Xρ′ ∈{1, 3}, f(Xu) + f(Xv) = 1 −1 = 0 condition on Xρ′ ∈{1, 3} and condition
on Xρ′ ∈{2, 4}, f(Xu) + f(Xv) = 0 −0 = 0. Put it differntly, Var[f(X)] = 0 since f(X) = 0
almost surely. However, observe that π is the uniform measure on [4], which implies"
T,0.8128256513026052,Var[fu(Xu)] = Var[fv(Xv)] = 1
T,0.8132264529058116,4 > 0.
T,0.8136272545090181,"Therefore, it is not true that (23) holds for the standard (Efron-Stein) decomposition of f(x) =
P"
T,0.8140280561122244,v∈Dk(ρ′) fv(xv).
T,0.8144288577154308,"Let us make a simple observation to give the insight for the construction. If f(Xu) is a function of
Xp(u), then for each i ∈[q], the function f must take the same value for all possible outcomes of Xu
conditioned on Xp(u) = i. In other words, the values of f are constant on the set"
T,0.8148296593186373,"Si = supp(rowi(M))
(105)"
T,0.8152304609218437,"for every i ∈[q]. Now, let us consider the case where f(Xu) is a function of Xpr(u). This can be
reformulated as follows: for k ∈[0, r−1], E

f(Xu)
 Xpk(u)

is a function of Xpk+1(u). Equivalently,
the values of M kf are constant on the set Si for every i ∈[q]."
T,0.8156312625250501,"Therefore, it is evident that the construction of the basis should primarily revolve around the sets
{Si}i∈[q] and their interaction with M."
T,0.8160320641282565,"Following from this discussion, the proof of the Proposition I.1 is divided into the following steps:"
T,0.8164328657314629,"Step 1 (Section I.1): We try to give a precise description of when f(Xu) is a function of Xpk(u) for
some k ∈N. To this end, we introduce the following notation.
Definition I.3. We define the following partial order relation ≤on the collection of all partitions of
[q]: Specifically, for two partitions P and P′, we say that P ≤P′ if P′ is finer than or equal to P."
T,0.8168336673346693,"Further, there exists r ∈N such that Pt,0 for t ≥r is the trivial partition.
Lemma I.4. There exists a chain of paritions"
T,0.8172344689378758,"P0,0 ≥P1,0 ≥P2,0 · · · ≥Pr,0 ≥. . ."
T,0.8176352705410822,"A function f : [q] 7→R satisfies that f(Xu) is a function of Xpr(u) for some r ∈N if and only if f is
a linear combination of 1P for P ∈Pr,0."
T,0.8180360721442885,"(The double index for the partitions is due to a technical reason, which will be clear in the construction
of the partitions.)"
T,0.818436873747495,"Step 2 (Section I.2): Next, we try to extract a basis of functions according to the partitions fro the
previous step, along with suitable quantitative estimates:
Proposition I.5. Let M be an ergodic and irreducible transition matrix defined on the state space
[q]. We can construct"
T,0.8188376753507014,"• a basis of functions from [q] to R, denoted as"
T,0.8192384769539078,"{ξw}w∈W,"
T,0.8196392785571143,"where W is a set of size q,"
T,0.8200400801603206,"• a function
r : W →N ∪{0},"
T,0.820440881763527,• and a constant C > 1 (which depends on M)
T,0.8208416833667335,so that the following holds:
LET,0.8212424849699399,1. Let
LET,0.8216432865731463,"r0 := max
w∈W r(w)."
LET,0.8220440881763527,"There exists unique w0 ∈W such that r(w0) = r0. Moreover, ξw0 ≡1."
LET,0.8224448897795591,"2. For each w ̸= w0, ξw(Xu) is a function of Xv where v = pr(w)(u) and Eξw(Xu) = 0. 3."
LET,0.8228456913827655,"Var
 X"
LET,0.823246492985972,"w
cwξw(Xu)

≤C(
max
w : r(w)̸=r0 |cw|)2."
LET,0.8236472945891784,"4. For any 0 ≤r′ < r0 such that {w ∈W : r(w) = r′} is not empty,"
LET,0.8240480961923847,"EVar
h
E

X"
LET,0.8244488977955912,"w : r(w)=r′
cwξw(Xu) |Xv
  Xp(v)
i
≥1"
LET,0.8248496993987976,"C (
max
w : r(w)=r′ |cw|)2."
LET,0.825250501002004,"5. For any 0 ≤r′ < r0 such that {w ∈W : r(w) < r′} is not empty,"
LET,0.8256513026052105,"EVar
h
E

X"
LET,0.8260521042084168,"w : r(w)<r′
cwξw(Xu) |Xv
  Xp(v)
i
≤C(
max
w : r(w)<r′ |cw|)2."
LET,0.8264529058116232,"Remark I.6. For w ∈W and l ∈[r(w)], let"
LET,0.8268537074148297,"ξ(l)
w := M lξw,
(106)"
LET,0.8272545090180361,"where we treated ξ as an vector in R[q]. Equivalently,"
LET,0.8276553106212425,"ξ(l)(θ) = E

ξ(Xu) | Xv = θ]"
LET,0.8280561122244489,"where u, v ∈T are vertices such that v = pl(u)."
LET,0.8284569138276553,"Step 3 (Section I.3): Finally, we will use the basis from the previous step to decompose degree-1
polynomials to prove Proposition I.1."
LET,0.8288577154308617,"I.1
Partitions of [q]"
LET,0.8292585170340682,"Let us begin with the following observation.
Lemma I.7. Suppose {Oα}α∈I is a collection of non-empty subsets of [q]. Then, there exists a
unique partition P of [q] that satisfies the following 2 conditions:"
LET,0.8296593186372746,"1. For each α ∈I and P ∈P, either Oα ∈P or Oα ∩P = ∅."
LET,0.8300601202404809,"2. For any other partition P′ that also satisfies the above property, P′ ≤P."
LET,0.8304609218436874,Proof. The proof can be carried out by constructing the partition P.
LET,0.8308617234468938,"Without lose of generality, we may assume the collection {Oα}α∈I contains

{θ}"
LET,0.8312625250501002,"θ∈[q], since for a
singleton {θ} and a set P, it is always true that either {θ} ⊆P or {θ} ∩P = ∅. Consequently, we
may assume
["
LET,0.8316633266533067,"α∈I
Oα = [q].
(107)"
LET,0.832064128256513,"First, we define an equivalence relation ≃on {Oα}α∈I as follows: For any α, α′ ∈I, we denote
Oα ≃Oα′ if there exists a chain (α1, α2, . . . , αl) such that Oαi−1 ∩Oαi ̸= ∅for i ∈[l]. Let
I1, . . . , Ik0 ⊆I be the partition of I such that {Oα}α∈Ik for k ∈[k0] form the equivalence classes
of the relation. Now, let P := {P1, . . . , Pk0}, where"
LET,0.8324649298597194,Pk := ∪α∈IkOk.
LET,0.8328657314629259,"Claim 1: For every α ∈I and k ∈[k0], either Oα ⊆Pk or Oα ∩Pk = ∅."
LET,0.8332665330661323,"To prove this claim, consider any α and k described above. Suppose Oα ∩Pk ̸= ∅. Let θ ∈Oα ∩Pk
and pick an index α′ ∈Ik such that θ ∈Oα′. Such an index exists because Pk = S"
LET,0.8336673346693386,"α′′∈Ik Oα′′.
Then, we have Oα ∩Oα′ ̸= ∅, implying α ∈Ik. Consequently, Oα ⊆Pk. Therefore, the claim is
proven."
LET,0.8340681362725451,Claim 2: P is a partition of [q].
LET,0.8344689378757515,We need to verify three properties:
S,0.8348697394789579,1. S
S,0.8352705410821644,"k∈[k0] Pk = [q],"
S,0.8356713426853707,"2. ∀k ∈[k0], Pk ̸= ∅, and"
S,0.8360721442885771,3. Pk ∩Pk′ = ∅whenever k ̸= k′.
S,0.8364729458917836,"First, for each θ ∈[q], by (107), there exists α ∈I such that θ ∈Oα. Then, θ ∈Oα ∈Pk where k is
the index such that α ∈Ik. Hence, we conclude that S"
S,0.83687374749499,k∈[k0] Pk = [q].
S,0.8372745490981964,"Second, for each k ∈[k0], let α ∈Ik. We have ∅̸= Oα ⊆Pk. Thus, Pk is not an empty set."
S,0.8376753507014028,"Finally, for any distinct k, k′ ∈[k0], suppose θ ∈Pk ∩Pk′. By (107), let α ∈I be the index so that
θ ∈Oα. Hence, both Oα ∩Pk and Oα ∩Pk′. In particular, it is necessary that α ∈Ik and α ∈Ik′,
which forces k = k′, leading to a contradiction. Therefore, Pk ∩Pk′ = ∅whenever k ̸= k′. Hence,
the claim follows."
S,0.8380761523046092,Claim 3: P′ ≤P for any P′ described in the statement.
S,0.8384769539078156,"To prove the claim, it suffices to show that for any P ′ ∈P′ and Pk ∈P with k ∈[k0], if P ′ ∩Pk ̸= ∅,
then Pk ⊆P ′."
S,0.8388777555110221,"Let us consider an arbitrary pair of P ′ ∈P′ and Pk ∈P and assume that P ′ ∩Pk ̸= ∅. There exists
an index α such that Oα ∩P ′ ∩Pk ̸= ∅. Based on the assumptions regarding P and P′, we have
α ∈Ik and Oα ⊆P ′."
S,0.8392785571142285,"For every other α′ ∈Ik, there exists a chain (α = α0, α1, . . . , αl0 = α′) such that Oαl−1 ∩Oαl ̸= ∅
for l ∈[l0]. Observe that if Oαl−1 ⊆P ′, then Oαl ⊆P ′, due to Oαl ∩P ′ ⊇Oαl ∩Oαl−1 ̸= ∅. With
O0 ⊆P ′ as our starting point, we can apply this observation repeatedly to conclude that Oα′ ⊂P ′.
Since the argument works for every α′ ∈Ik, we conclude that Pk = S"
S,0.8396793587174348,α′′∈Ik Oα′′ ⊆P ′.
S,0.8400801603206413,"Definition I.8. For any given collection of subsets {Oα}α∈I of [q], let P({Oα}α∈I) denote the
partition P defined in Lemma I.7."
S,0.8404809619238477,"For any given partition Q of [q], let"
S,0.8408817635270541,"PSC(Q) := P
 
{Q}Q∈Q ∪{Si}i∈[q]

."
S,0.8412825651302606,"Remark I.9. Clearly, PSC(Q) ≤Q.
Definition I.10. Let
P0,0 =

{1}, {2}, . . . , {q}"
S,0.8416833667334669,"and
P1,0 = PSC(P0,0)."
S,0.8420841683366733,"Let us remark that P1,0 is the finest partition of [q] so that each part P ∈P1,0 either contains Si or
disjoint from Si for i ∈[q]."
S,0.8424849699398798,"We use double indices for indexing the partitions because constructing such a chain of partitions
requires the creation of multiple partitions along the way, as we will illustrate shortly."
S,0.8428857715430862,"To proceed, let us begin with a simple observation.
Lemma I.11. If P ∈P1,0, then
M1P = 1Q
where
Q = {i ∈[q] : Si ⊆P}.
Suppose P1,0 = {P1, P2, . . . , Pk0}. Then, the collection Q := {Q1, Q2, . . . , Qk0} where"
S,0.8432865731462926,"M1Pi = 1Qi
is also a partition provided that M is irreducible."
S,0.843687374749499,"Proof. For i with Si ∩P = ∅, it is immediate that (M1P )i = 0. Conversely, when Si ∩P ̸= ∅, it is
necessary that Si ⊆P. Consequently, (M1P )i = P"
S,0.8440881763527054,j∈[q] Mij = 1.
S,0.8444889779559118,"To establish that Q is a partition, we need to demonstrate the following three conditions:"
S,0.8448897795591183,"1. Qk ∩Qk′ = ∅for all distinct k, k′ ∈[k0]."
S,0.8452905811623247,2. S
S,0.845691382765531,k∈[k] Qk = [q].
S,0.8460921843687375,3. Qk ̸= ∅for all k ∈[k0].
S,0.8464929859719439,"For the first condition, suppose there exists i ∈Qk ∩Qk′ for some distinct k and k′. By definition,
Si ⊆Pk and Si ⊆Pk′, which is a contradiction. Hence, Qk ∩Qk′ = ∅."
S,0.8468937875751503,"For the second condition, for every i ∈[q], we know that Si ⊆Pk for some k. Consequently, i ∈Qk,
ensuring S"
S,0.8472945891783568,α∈[k] Qk = [q].
S,0.8476953907815631,"For the third condition, if we assume Qk = ∅, implying that no i ∈[q] satisfies Si ⊆Pk, then M is
not irreducible, since the states in Pk cannot be reached."
S,0.8480961923847695,"Definition I.12. Let P1,1 = Q where Q is the partition described in Lemma I.11.
Lemma I.13. If P is a finite union of parts in P1,0, then"
S,0.848496993987976,"M1P = 1Q
(108)"
S,0.8488977955911824,"where Q is a finite union of parts in P1,1. The above map induces a bijection between subsets of [q]
that are finite union of parts of P1,0 and subsets of [q] that are finite union of parts of P1,1, in which
preserve the inclusion relation is preserved."
S,0.8492985971943888,"Proof. Let us express P1,0 = {P1, P2, . . . , P[k0]} and P1,1 = {Q1, Q2, . . . , Qk0} where 1Qk =
M1Pk."
S,0.8496993987975952,"For each I ⊆[k0], let PI = S"
S,0.8501002004008016,k∈I Pk and QI = S
S,0.850501002004008,k∈I Qk. Since 1PI = P
S,0.8509018036072145,"k∈I 1Pk and 1QI =
P"
S,0.8513026052104209,"k∈I 1Qk, clearly we have
1QI = M1PI."
S,0.8517034068136272,"Since naturally both finite union of parts of P and of Q are identified with a subset I ⊂[k0] in the
above way, the statement of the lemma follows."
S,0.8521042084168337,An immediate consequence is the following.
S,0.8525050100200401,"Corollary I.14. The transition matrix M induces a bijection between partitions that are ≤P1,0 and
partitions that are ≤P1,1. For convenience, we adopt the following definitions:"
S,0.8529058116232465,"1. For any partition P such that P ≤P1,0, define"
S,0.853306613226453,"MP := {Q : ∃P ∈P such that 1Q = M1P } ≤P1,1."
S,0.8537074148296593,"2. Given any P ≤P1,0 and for each P ∈P, let MP represent a part in MP where"
S,0.8541082164328657,1MP = M1P .
S,0.8545090180360722,"Next, we will build a collection of partitions Pr,s for r ≥0 and 0 ≤s ≤r starting with P0,0 =

{1}, {2}, . . . , {q}
	
and establishing the relationship illustrated by the diagram below."
S,0.8549098196392786,"P0,0
≥
SC
P1,0
≥
P2,0
≥
P3,0
≥
P4,0
. . ."
S,0.855310621242485,"↓
↓
↓
↓
P1,1
≥
SC
P2,1
≥
P3,1
≥
P4,1
. . ."
S,0.8557114228456913,"↓
↓
↓
P2,2
≥
SC
P3,2
≥
P4,2
. . ."
S,0.8561122244488978,"↓
↓
P3,3
≥
SC
P4,3
. . ."
S,0.8565130260521042,"↓
P4,4
. . .
..."
S,0.8569138276553107,"( In the above diagram, P →Q indicates that Q = MP; Q ≥
SC P indicates P = PSC(Q).)"
S,0.857314629258517,"Indeed, the initial definition of P0,0 and the relation diagram determine the collection of partitions
completely. Let us summarise it as a statement:
Lemma I.15. There exists a unique collection of partitions {Pr,s}r≥s≥0 that satisfies the following
properties: For 0 ≤s < r,"
S,0.8577154308617234,"1. P0,0 =

{1}, {2}, . . . , {q}
	
."
S,0.8581162324649299,"2. Pr,s ≤P1,0."
S,0.8585170340681363,"3. Pr,s+1 = MPr,s."
S,0.8589178356713427,"4. Pr+1,s ≤Pr,s."
S,0.859318637274549,"5. Pr+1,r = PSC(Pr,r)."
S,0.8597194388777555,"Proof of Lemma I.15. The proof is proceeded by induction. We assume that Pr,s is constructed and
uniquely determined for 0 ≤r < r0 and 0 ≤s ≤r for some r0 ≥0 so that it satisfies the properties
described in the lemma."
S,0.8601202404809619,"We will define the partitions in the next column {Pr0,s}s∈[0,r0] by starting with Pr0,r0−1 =
PSC(Pr0−1,r0−1)."
S,0.8605210420841684,"Besides constructing the rest of partitions, we also need to show that these partitions satisfy the
following list of conditions ( let us denote it as List A): For s ∈[0, r0, −1],"
S,0.8609218436873748,"1. Pr0,s ≤P1,0."
S,0.8613226452905811,"2. Pr0,s ≤Pr0−1,s for s ∈[0, r0 −1]."
S,0.8617234468937875,"3. Pr0,s+1 = MPr0,s."
S,0.862124248496994,"By definition of the map PSC, the first and second condition in the list are satisfied for s = r0 −1.
Relying on Pr0,r0−1 ≤P1,0, we can define Pr0,r0 = MPr0,r0−1. Hence, the third condition in the
list is also satisfied for s = r0 −1."
S,0.8625250501002004,"It remains to construct Pr0,s for s ∈[0, r0 −2] and they satisfy those 3 conditions in the list. This
can be proceeded inductively starting from s = r0 −2."
S,0.8629258517034069,"Claim: For s ∈[0, r0 −2], if Pr0,s+1 ≤Pr0−1,s+1, then there exists a unique partition Pr0,s which
satisfies the conditions in List A for s."
S,0.8633266533066132,"Suppose the Claim holds. With Pr0,r0−1 ≤Pr0−1,r0−1, we could apply the claim repeatedly and
the lemma follows. The rest of the proof is to show the claim holds."
S,0.8637274549098196,"Let us assume Pr0,s+1 ≤Pr0−1,s+1 for some s ∈[0, r0 −2]. First, from our assumption on
{Pr,s} for 0 ≤s ≤r0 −1, Pr0−1,s+1 = MPr0−1,s. By Corollary I.14, Pr0−1,s+1 ≤P1,1. Since
Pr0,s+1 ≤Pr0−1,s+1, we conclude that Pr0,s+1 ≤P1,1."
S,0.864128256513026,"Applying Corollary I.14 again, we know there exists an unique partition P ≤P1,0 so that Pr0,s+1 =
MP. We set Pr0,s := P. In particular, the choice of Pr0,s is unique in order to satisfy the first and
third condition from the list."
S,0.8645290581162325,"It remains to show that Pr0,s also satisfies the second condition in List A. Notice that from Corollary
I.14, the induced map of M on partitions preserves ≤relation. Hence, Pr0,s+1 ≤Pr0−1,s+1 implies
Pr0−1,s ≤Pr0−1,s. Therefore, the claim holds."
S,0.8649298597194389,"Proof of Lemma I.4. We start with the proof on the ⇒implication. Suppose f is a function satisfied
the first condition described in the lemma."
S,0.8653306613226452,"Since f(Xu) is a function of Xpr(u), this is equivalent to"
S,0.8657314629258517,"0 =E
h
Var

f(Xu)
 Xpr(u)
i
."
S,0.8661322645290581,"Relying on the identity Var[Y ] = EVar[Y | Z] + Var

E[Y | Z]

and (Xpr(u), Xpr−1(u), . . . , Xu) is
a Markov Chain,"
S,0.8665330661322646,"E
h
Var

f(Xu)
 Xpr(u)
i
= r
X"
S,0.866933867735471,"s=1
E
h
Var

f(Xu)
 Xps(u)
i
."
S,0.8673346693386773,"Hence, E
h
Var

f(Xu)
 Xps(u)
i
= 0 for s ∈[r], which in turn implies E

f(Xu)
 Xps−1(u)
"
S,0.8677354709418837,"conditioned on Xps(u) is a constant function for each s ∈[r]. Equivalently, M s−1f takes the same
value for all elements in each Si for i ∈[q]."
S,0.8681362725450902,"Claim: For s ∈[r], if f can expressed in the form f = P"
S,0.8685370741482966,"P ∈Ps−1,0 cs−1,P 1P , then it can be
expressed in the form f = P"
S,0.868937875751503,"P ∈Ps,0 cs,P 1P ."
S,0.8693386773547094,"Clearly, if the claim holds, then we can apply it repeatedly to draw the conclusion that f is a linear
combination of 1P for P ∈Pr,0."
S,0.8697394789579158,"Now, we fix s ∈[r] and assume f = P"
S,0.8701402805611222,"P ∈Ps−1,0 cs−1,P 1P . Then,"
S,0.8705410821643287,"E

f(Xu)
 Xps−1(u) = a

= (M s−1f)(a) =
X"
S,0.8709418837675351,"P ∈Ps−1,0
cs−1,P M s−11P =
X"
S,0.8713426853707414,"P ∈Ps−1,0
cs−1,P 1P s−1,"
S,0.8717434869739479,"where for each P ∈Ps−1,0, P s−1 ∈Ps−1,s−1 is the corresponding part such that M s−11P =
1P s−1. In other words, M s−1f is a linear combination of 1P for P ∈Ps−1,s−1."
S,0.8721442885771543,"Because M s−1f takes the same value not only for all elements in each Si for i ∈[q], but also for all
elements in each P for P ∈Ps−1,s−1, it implies M s−1f takes the same value for all elements in
each P ′ ∈PSC(Ps−1,s−1) = Ps,s−1."
S,0.8725450901803607,"Together with the fact that the induced map of M on partitions preserves ≤relation, we conclude that
cs−1,P1 = cs−1,P2 for P1, P2 ∈P s−1,0 whenever P1 and P2 are both contained in some P ∈Ps,0."
S,0.8729458917835672,"Equivalently, within each P ∈Ps,0, f is a constant function. Hence, we can express f as a linear
combination of 1P for P ∈Ps,0."
S,0.8733466933867735,"For the ⇐implication, suppose f is a linear combination of 1P with P ∈Pr,0."
S,0.87374749498998,"What we need to show is for s ∈[0, r −1], M sf takes the same values for all elements in each Si
for i ∈[q]. From the chain Pr,0 →Pr,1 →· · · →Pr,r and by (108), for s ∈[0, r −1], M sf is a
linear combination of 1P with P ∈Pr,s."
S,0.8741482965931864,"Since Ps+1,s = PSC(bP s,s) ≤P1,0 and Ps+1,s ≥· · · ≥Pr,s, we have Pr,s ≤P1,0, which
implies M sf takes the same values for all elements in each Si for i ∈[q]. Therefore, the proof is
completed."
S,0.8745490981963928,"Now, it remains to prove the second statement of the lemma."
S,0.8749498997995993,"First, if there exists r ∈N such that Pr,0 is trivial. Then Pt,0 is also trivial for t > r since
Pt,0 ≤Pr,0. Hence, it is enough to show the existence of r such that Pr,0 is trivial.Pr,0 is trivial."
S,0.8753507014028056,"From the assumption on M, we knew that the stationary distribution π of M satisfies mini∈[q] π(i) >
0 and M r converges entry-wise to the matrix whose row is identically π. Therefore, for sufficiently
large r, mini,j∈[q](M r)ij > 0."
S,0.875751503006012,"Now, let us fix such r and assume Pr,0 is not trivial. Let us express Pr,s = {P r,s
1 , . . . , P r,s
kr } with for
s ∈[0, r] and kr ≥2 where the index is assigned so that P r,s
k
= MP r,s−1
k
for s ∈[r] and k ∈[kr].
First,
1P r,r
1
= M r1P r,0
1 ."
S,0.8761523046092184,"With 1P r,0
1
is non-negative and not zero, every component of M r1P r,0
1
is non-zero. This forces
P r,r
1
= [q], which contradicts to the assumption that Pr,r is non-trivial."
S,0.8765531062124249,"I.2
A basis of functions from [q] 7→R according to the partition"
S,0.8769539078156313,"From now on, let r0 be the smallest non-negative integer such that Pr,0 is trivial. Consider the
collection

(P, s) : s ∈[0, r0], P ∈Ps,0"
S,0.8773547094188376,"We will establish an identification between elements of the set described above and words whose
alphabet consists of non-negative integers. This identification is constructed through induction,
following these steps:"
S,0.8777555110220441,"• First, we identify ([q], r0) with the word (1)."
S,0.8781563126252505,"• Assuming that elements in {(P, s + 1) : P ∈Ps+1,0} have already been identified with
unique words, we proceed as follows: For each (P, s + 1), suppose there are k pairs of
(P ′, s) such that P ′ ⊆P. We identify these k pairs with the words (w, i) for i ∈[0, k −1],
in any order of preference. For each (P ′, s), due to Ps,0 is a finer than or equal to Ps+1,0,
there exists an unique pair (P, s + 1) so that P ′ ⊆P. This guarantees the above procedure
assigns each (P ′, s) a unique word."
S,0.878557114228457,"We denote the set of words described above as eW, and we adopt the notation w ∼(P, s) to indicate
that (P, s) is associated with the word w. For a given w ∈eW, we represent the corresponding pair as
(Pw, r(w)), where r(w) = r0 + 1 −len(w)."
S,0.8789579158316633,"Now, let us make the following observations"
S,0.8793587174348697,"1. If w ∈eW is a word with len(w) < r0 + 1, then (w, 0) ∈eW."
S,0.8797595190380761,"2. Each (P, s) corresponds to a word of length r0 + 1 −s."
S,0.8801603206412826,"3. Suppose w, w′ ∈eW such that w is a prefix of w′. Then, Pw′ ⊆Pw."
S,0.880561122244489,"Let T e
W be the tree defined on eW using the prefix relation. In this tree, edges are drawn from w′ to w
if r(w′) = r(w) + 1 and Pw ⊆Pw′. Now, we will select q parts from these elements (P, s) based on
their corresponding words."
S,0.8809619238476953,"Lemma I.16. Let W ⊆eW be the subcollection of words which end with a positive integer. Then,
|W| = q."
S,0.8813627254509018,"Proof. First of all, there are exactly q words in eW with length r + 1, since P0,0 =

{i}"
S,0.8817635270541082,"i∈[q] has q
parts. For each i ∈[q], let w′
i be the word corresponding to ({i}, 0) and let wi be the longest word
ending with a positive integer so that is either a prefix of equals to w′
i. This is well-defined since
every word in eW is a word starting with 1."
S,0.8821643286573146,"The proof of the lemma follows if we can show the following claim: w1, w2, . . . , wq are distinct and
are all words which ends with a positive integer."
S,0.8825651302605211,"To prove the claim, we begin by showing wi ̸= wj whenever i ̸= j. Suppose wi = wj for some
distinct pair of i, j ∈[q]. Let ˜w be the longest prefix of w′
1, w′
2, necessarily we have wi = wj is
either a prefix of ˜w or ˜w itself. Further, the length of ˜w is less equal than r, since otherwise it implies
w′
i = w′
j, which is a contradiction."
S,0.8829659318637274,"Now, let (˜w, ei) and (˜w, ej) be the two words which are prefix of w′
i and w′
j, respectively. From the
definition that ˜w is the longest common prefix, ei and ej are distinct non-negative integers. Since
wi is a prefix of (w, ei), it is necessary that ei = 0, otherwise it violates the definition of wi. For the
same reason, ej = 0. Therefore, we reach a contradiction."
S,0.8833667334669338,"The remaining part to prove the claim is to show that {wi}i∈[q] are all the words in eW ending with a
positive integer. Suppose w is a word in which ends with a positive integer. If len(w) < r + 1, we
can keep fill 0 until its length is r + 1 and denote the resulting word by w′. Observe that w′ ∈eW.
Together with the length of w′ is r + 1, necessarily w′ = w′
i for some i. Recall the definition of wi,
we conclude w = wi. Therefore, the claim follows."
S,0.8837675350701403,"Lemma I.17. For any given 0 ≤r′ < r, suppose Wr′ := {w ∈W : r(w) = r′} is non-empty.
Consider a linear combination P"
S,0.8841683366733467,"w∈Wr′ cw1Pw. If it can be expressed as a linear combination of 1P
for P ∈Pr′+1,0, then cw are identically 0."
S,0.8845691382765531,"Proof. Let w1, . . . , wk0 be the words with r(wk) = r′ + 1 and corresponding to each part of Pr′+1,0.
Then, the words that corresponds to pairs of the form (P, r′) with P ∈Pr′,0 are

(wk, t)"
S,0.8849699398797595,"k∈[k0],t∈[0,tk]"
S,0.8853707414829659,"where tk are non-negative integers. Now, we express
X"
S,0.8857715430861723,"w∈Wr′
cw1Pw =
X"
S,0.8861723446893788,k∈[k0] X
S,0.8865731462925852,"t∈[tk]
c(wk,t)1P(wk,t)."
S,0.8869739478957915,"For each k ∈[k0] and any θ ∈Pwk, we have
X"
S,0.887374749498998,k′∈[k0] X
S,0.8877755511022044,"t∈[tk′]
c(wk′,t)1P(wk′ ,t)(θ) =
X"
S,0.8881763527054108,"t∈[tk]
c(wk,t)1P(wk,t)(θ)."
S,0.8885771543086173,"Therefore, P
w∈Wr′ cw1Pw can be expressed as P
k∈[k0] cwk1Pwk if and only if P
t∈[tk] c(w,t)1P(w,t)
is a constant on Pwk."
S,0.8889779559118236,"For each k ∈[k0], let θ ∈P(wk,0), then we have
X"
S,0.88937875751503,k′∈[k0] X
S,0.8897795591182365,"t∈[tk′]
c(wk′,t)1P(wk′ ,t)(θ) =
X"
S,0.8901803607214429,"t∈[tk]
c(wk,t)1P(wk,t)(θ) = 0,"
S,0.8905811623246493,"which forces c(wk,t) = 0 for every t > 0 (if it exists). Therefore, the proof is complete."
S,0.8909819639278557,"Definition I.18. Let B := {ξw}w∈W be a collection of q functions from [q] to R, defined as follows:"
S,0.8913827655310621,"1. If w = (1), ξw = 1Pw = 1."
S,0.8917835671342685,"2. If w ̸= (1),"
S,0.892184368737475,ξw(θ) := 1Pw(θ) −EY ∼π1Pw(Y ).
S,0.8925851703406814,"Remark I.19. The remaining goal in this subsection is to show that B is the desired basis described
in Proposition I.5. We also remark that the first two properties stated in Proposition I.5 are already
satisfied with this construction: argmaxw∈Wr(w) = (1) with ξ(1) = 1[q] = 1; ξw(Xu) is a function
of Xv where v = pr(w)(u).
Lemma I.20. The collection B forms a linear basis for functions from [q] to R."
S,0.8929859719438877,"Proof. Since there are exactly q functions, our goal is to show"
S,0.8933867735470942,"R[q] = span({ξw}w∈W),"
S,0.8937875751503006,"and the R.H.S. is the same as span({1Pw}w∈W). It suffices to show for each i ∈[q], 1{i} can be
expressed as a linear combination of 1Pw with w ∈W."
S,0.894188376753507,"To prove this statement, we will use induction, showing that for s from r0 to 0, each 1P with P ∈Ps,0
can be expressed as a linear combination of of 1Pw with w ∈W. Since P0,0 =

{1}, . . . , {q}
	
, the
proof follows once we establish this inductive statement."
S,0.8945891783567135,"First, when s = r, since 1[q] is the only part in Pr0,0 and [q] = P(1), the statement holds for s = r0."
S,0.8949899799599198,"Now, suppose the inductive hypothesis holds for s+1 with s < r0. Pick any P ∈Ps,0, let w = (w′, t)
be the word associate with (P, s). If t = 0, then"
S,0.8953907815631262,"1P = 1Pw′ −
X"
S,0.8957915831663327,"P ′′
1P ′′"
S,0.8961923847695391,"where the sum is taken over all parts P ′′ ∈Ps,0\{P} contained in Pw′. Each P ′′ in the summation (if
it exists) must corresponds to a word of the form (w′, t′′) with t′′ > 0, or equivalently (w′, t′′) ∈W.
From the induction hypothesis, 1Pw′ is a linear combination of 1Pw with w ∈W. Therefore, we
conclude that 1P is also a linear combination of 1Pw with w ∈W. If t > 0, then w ∈W, and
the same conclusion follows immediately. With no restriction on the choice of P, the induction
hypothesis holds for s as well."
S,0.8965931863727455,"Therefore, the lemma follows from induction."
S,0.8969939879759519,"Lemma I.21. For any given 0 ≤r′ < r, suppose Wr′ := {w ∈W : r(w) = r′} is non-empty.
Then, there exists a constant C ≥1 (which could depends on M) such that the following holds: Let
u, v ∈T be two vertices such that v = pr′(u). We have"
S,0.8973947895791583,"EVar
h
E
 X"
S,0.8977955911823647,"w∈Wr′
cwξw(Xu)
 Xv
  Xp(v)
i
≥1"
S,0.8981963927855712,"C ( max
w∈Wr′ |cw|)2.
(109)"
S,0.8985971943887776,"Proof. First, both sides of (109) scale by a factor h2 if every term cw is multiplied by h ∈R. Hence,
it suffices to establish the inequality in the case"
S,0.8989979959919839,"max
w∈Wr′ |cw| = 1."
S,0.8993987975951904,"Given this, consider the set

(cw)w∈Wr′ : maxw∈Wr′ |cw| = 1
	
⊆RWr′ . It is compact set and"
S,0.8997995991983968,"EVar
h
E
 X"
S,0.9002004008016032,"w∈Wr′
cwξw(Xu)
 Xv
  Xp(v)
i
(110)"
S,0.9006012024048096,"is continuous in (cw)w∈Wr′ (it is a polynomial of cw). By a compact argument one can estalbish the
existence of C ≥1 described in the lemma if for every (cw)w∈Wr′ with maxw∈Wr′ |cw| = 1,"
S,0.901002004008016,"EVar
h
E
 X"
S,0.9014028056112224,"w∈Wr′
cwξw(Xu)
 Xv
  Xp(v)
i
> 0."
S,0.9018036072144289,"We can simplify this by observing that
X"
S,0.9022044088176353,"w∈Wr′
cwξw =
X"
S,0.9026052104208416,"w∈Wr′
cw1Pw + constant,"
S,0.9030060120240481,"and hence,"
S,0.9034068136272545,"EVar
h
E
 X"
S,0.9038076152304609,"w∈Wr′
cwξw(Xu)
 Xv
  Xp(v)
i
=EVar
h
E
 X"
S,0.9042084168336674,"w∈Wr′
cw1Pw(Xu)
 Xv
  Xp(v)
i
(111)"
S,0.9046092184368737,"=EVar
h X"
S,0.9050100200400801,"w∈Wr′
cw1Pw(Xu)
 Xp(v)
i
,"
S,0.9054108216432866,where the second equality follows from that P
S,0.905811623246493,w∈Wr′ cw1Pw(Xu) is a function of Xv by Lemma I.4.
S,0.9062124248496994,"Moreover, to show EVar
h P"
S,0.9066132264529058,"w∈Wr′ cw1Pw(Xu)
 Xp(v)
i
> 0, this is the same as showing
X"
S,0.9070140280561122,"w∈Wr′
cw1Pw(Xu)"
S,0.9074148296593186,"is not a function of Xp(v). By Lemma I.4, this is equivalent to show P
w∈Wr′ cw1Pw is not a linear"
S,0.9078156312625251,"combination of 1P for P ∈Pr′+1, which was proven in Lemma I.17. Therefore, the proof is
complete."
S,0.9082164328657315,"Lemma I.22. For any given 0 ≤r′ < r, suppose W<r′ := {w ∈W : r(w) < r′} is non-empty.
Then, there exists C ≥1 so that the following holds: Let u, v ∈T be two nodes such that v = pr′(u).
We have
EVar
h
E

X"
S,0.9086172344689378,"w∈W<r′
cwξw(Xu)
 Xv
  Xp(v)
i
≤C( max
w∈W<r′ |cw|)2.
(112)"
S,0.9090180360721443,"Proof. The proof is more straightforward compared to the arguments presented in the proof of Lemma
I.21. First, both sides of (112) scale by a factor h2 if we scaled each cw by h ∈R. Therefore, it
suffices to establish the inequality when"
S,0.9094188376753507,"EVar
h
E

X"
S,0.9098196392785571,"w∈W<r′
cwξw(Xu)
 Xv
  Xp(v)
i
= 1."
S,0.9102204408817636,"If there is no (cw)w∈W<r′ satisfying the above condition, then the proof is completed. Now we assume"
S,0.9106212424849699,"this set is not empty. Notice that EVar
h
E
 P
w∈W<r′ cwξw(Xu)
 Xv
  Xp(v)
i
is a continuous"
S,0.9110220440881763,"function of (cw)w∈W<r′ which takes value 0 when (cw)w∈W<r′ = ⃗0. Thus, there is an open ball
B ⊆RW<r′ centered at ⃗0 such that for (cw)w∈Wr′ ∈B,"
S,0.9114228456913828,"EVar
h
E

X"
S,0.9118236472945892,"w∈W<r′
cwξw(Xu)
 Xv
  Xp(v)
i
< 1."
S,0.9122244488977956,"On the other hand, by choosing C sufficiently large, the set
n
(cw)w∈Wr′ : ( max
w∈W<r′ |cw|)2 ≤1/C
o
,"
S,0.912625250501002,"which is the cube of side length 2/C1/2 centered at ⃗0, is contained in B. Therefore, the lemma
follows."
S,0.9130260521042084,"Proof of Proposition I.5. From Remark I.19 and Lemma I.20, it remains to show B satisfies the last
3 properties stated in the Proposition."
S,0.9134268537074148,"As for the third property, notice that the variance of P
w∈W cwξw(Xu) is not zero as long as cw are
not identically 0 for w ̸= w0. Following the same arguments in the proof of Lemma I.21, the property
follows if C ≥1 is sufficiently large."
S,0.9138276553106213,"The last two follows by applying Lemma I.21 and Lemma I.22 to every 0 ≤r′ < r and choosing the
constant C can be chosen to be the maximum of those constants C from the two lemmas."
S,0.9142284569138277,"I.3
Proof of Proposition I.1"
S,0.914629258517034,"In this subsection, we consider soley degree 1 polynomial of the leave values.
Definition I.23. For any given ρ′ ∈T and a degree-1 polynomial f of {xu}u∈Lρ′ , the function can
be expressed uniquely in the form"
S,0.9150300601202405,"f(x) =
X"
S,0.9154308617234469,"w∈W, u∈Lρ′
cw,uξw(xu)
(113)"
S,0.9158316633266533,where {ξw}w∈W is the basis introduced in Proposition I.5.
S,0.9162324649298598,"For u ∈Tρ′, let fu(x) := P
w∈W , v∈Lu cw,vξw(xv). Observe that from this definition, for each
0 ≤l ≤r,"
S,0.9166332665330661,"f(x) =
X"
S,0.9170340681362725,"u∈Tρ′ : h(u)=l
fu(x)."
S,0.917434869739479,"Further, for u ∈Tρ′\Lρ′, let"
S,0.9178356713426854,"cw,u :=
X"
S,0.9182364729458918,"v∈Lu
cw,v."
S,0.9186372745490982,"Remark I.24. From the definition above, for each ρ′ ∈T and degree-1 polynomial f of variables
{xu}u∈Lρ′, we have"
S,0.9190380761523046,"∀u ∈Tρ′, ∀x ∈Rq, (Eufu)(x) =
X"
S,0.919438877755511,"w
cw,uξ(l)
w (xu),"
S,0.9198396793587175,"where ξ(l)
w (θ) is introduced in Remark I.6."
S,0.9202404809619239,"Proposition I.25. There exists a constant C = C(M, d) ≥1 so that the following holds: Suppose"
S,0.9206412825651302,"f(x) =
X"
S,0.9210420841683367,"u∈Lρ′, w∈W
cw,uξ(xu)"
S,0.9214428857715431,where ρ′ ∈T is a node satisfying h(ρ′) ≤r0 and
S,0.9218436873747495,"cw,u = cw,v"
S,0.9222444889779559,"for u, v ∈Lρ′ satisfying h(ρ(u, v)) ≤r(w), where ρ(u, v) is the lowest common ancestor of u and
v. Then,
X"
S,0.9226452905811623,"u∈Lρ′
Var[fu(X)] ≤CR3EVar

f(X) | Xρ′"
S,0.9230460921843687,"If Proposition I.25 is proven, then Proposition I.1 follows as a corollary:"
S,0.9234468937875752,"Proof of Proposition I.1. Reduction to h(ρ′) ≤r0: Without loss of generality, it is sufficient to
consider degree 1 functions of L, rather than degree 1 functions of variables in Dk(u) for some u in
the tree and 0 ≤k ≤h(u)."
S,0.9238476953907816,"Recall that
Dr0(ρ) = {w ∈T : h(w) = r0}.
We know that we can express f(x) = P"
S,0.9242484969939879,"w∈Dr0(ρ) fw(x) so that each of them is a degree-1 polynomial
with variables {xu}u∈Lw."
S,0.9246492985971944,Together with the variance decomposition for degree-1 polynomials (See Lemma C.1)
S,0.9250501002004008,"Var[f(X)] ≥
X"
S,0.9254509018036072,"w∈Dr0(ρ)
EVar[fw(Xw) | Xw],"
S,0.9258517034068137,"it suffices to prove the same statement for degree-1 polynomials of xu with u ∈Lρ′ for ρ′ satisfying
h(ρ′) ≤r0."
S,0.92625250501002,"Now, we fix such ρ′ and consider"
S,0.9266533066132264,"f(x) =
X"
S,0.9270541082164329,"w,u∈Lρ′
cw,uξw(xu)."
S,0.9274549098196393,"Averaging the Coefficients: For each w ∈W and for each u ∈Dr(w)(ρ′), we know that for any
v1, v2 ∈Lu,
ξw(Xv1) = ξw(Xv2)
almost surely. As a consequcne, we have X"
S,0.9278557114228457,"v∈Lu
cw,vξw(Xv) =
X v∈Lu P"
S,0.928256513026052,"v∈Lu cw,v"
S,0.9286573146292585,"|Lu|
ξw(Xv)"
S,0.9290581162324649,"almost surely. Now, we repeat this averaging process for each w ∈W and for each u ∈Dr(w)(ρ′).
We denote the resulting function by ˜f. While ˜f and f may not be the same function, ˜f(X) = f(X)
almost surely. On the other hand, ˜f is a function which satisfies the condition in Proposition I.25.
Following from the proposition, we have
X"
S,0.9294589178356714,"u∈L
EVar[ ˜fu(X)] ≤CR3EVar[ ˜f(X) | Xρ′] = CR3Var[f(X) | Xρ′]."
S,0.9298597194388778,The proof is complete.
S,0.9302605210420841,"Let us begin with an intermediate step toward the proof of the Proposition I.25.
Lemma I.26. Suppose f is a function described in Definition I.23. For any given 1 ≤l < r such
that Wl := {w ∈W : r(w) = l} is non-empty. Let u ∈Tρ′ with h(u) = r(w), suppose"
S,0.9306613226452906,"t = max
w∈Wl |cw,u| > 0."
S,0.931062124248497,Then one of the following statement holds:
S,0.9314629258517034,"• Either EVar

(Eufu)(Xu) | Xp(u)

≥πmin"
S,0.9318637274549099,"2C0 t2, or"
S,0.9322645290581162,"• maxw∈W<l |cw,u| ≥
√πmin"
S,0.9326653306613226,2C0 t.
S,0.9330661322645291,"Here, C0 ≥1 is the constant C described in Proposition I.5 and πmin := minθ∈[q] π(θ)."
S,0.9334669338677355,"Further, in the case when l = 0, then we simply have EVar

(Eufu)(Xu) | Xp(u)

≥
1
C0 t2."
S,0.9338677354709419,Proof. We decompose (Eufu)(x) into three components:
S,0.9342685370741483,"(Eufu)(x) =
X"
S,0.9346693386773547,"w :r(w)<l
cw,uξ(l)
w (xu) +
X"
S,0.9350701402805611,"w :r(w)=l
cw,uξ(l)
w (xu) +
X"
S,0.9354709418837676,"w :r(w)>l
cw,uξ(l)
w (xu),"
S,0.935871743486974,"where ξ(l)
w is introduced in Remark I.6."
S,0.9362725450901803,"For each w with r(w) > l, ξ(l)
w (Xu) is a function of Xv with v = pr(w)−l(u). Hence, the last compo-
nent P"
S,0.9366733466933868,"w :r(w)>l cw,uξ(l)
w (xu) is a constant function whenever we condition on Xp(u). Consequently,"
S,0.9370741482965932,"EVar

(Eufu)(Xu) | Xp(u)

= EVar
h
X"
S,0.9374749498997996,"w :r(w)<l
cw,uξ(l)
w (Xu) +
X"
S,0.9378757515030061,"w :r(w)=l
cw,uξ(l)
w (Xu)
 Xp(u)
i
. (114)"
S,0.9382765531062124,"From Proposition I.5, we know that"
S,0.9386773547094188,"EVar
h
X"
S,0.9390781563126253,"w :r(w)=l
cw,uξ(l)
w (Xu)
 Xp(u)
i
≥t2"
S,0.9394789579158317,"C0
,
(115)"
S,0.9398797595190381,"where the constant C0 is the constant C stated in the Proposition. Intuitively, from (115) it should
be clear that if the R.H.S. of (114) is small, then EVar
h P"
S,0.9402805611222445,"w :r(w)<l cw,uξ(l)
w (Xu)
 Xp(u)
i
cannot be
small. Let us derive this with a coarse estimate."
S,0.9406813627254509,"By (115), we know there exists θ ∈[q] such that"
S,0.9410821643286573,"Var
h
X"
S,0.9414829659318638,"w :r(w)=l
cw,uξ(l)
w (Xu)
 Xp(u) = θ
i
≥t2 C0
."
S,0.9418837675350702,"Now, suppose Var
h P"
S,0.9422845691382765,"w :r(w)<l cw,uξ(l)
w (Xu)
 Xp(u) = θ
i
<
t2
4C0 . We could apply triangle inequal-
ity to get
s"
S,0.942685370741483,"Var
h
X"
S,0.9430861723446894,"w :r(w)<l
cw,uξ(l)
w (Xu) +
X"
S,0.9434869739478958,"w :r(w)=l
cw,uξ(l)
w (Xu)
 Xp(u) = θ
i ≥
s"
S,0.9438877755511023,"Var
h
X"
S,0.9442885771543086,"w :r(w)=l
cw,uξ(l)
w (Xu)
 Xp(u) = θ
i
−
s"
S,0.944689378757515,"Var
h
X"
S,0.9450901803607215,"w :r(w)<l
cw,uξ(l)
w (Xu)
 Xp(u) = θ
i ≥
t"
S,0.9454909819639279,"2C1/2
0
,"
S,0.9458917835671342,"and together with (114),"
S,0.9462925851703406,"EVar

(Eufu)(Xu) | Xp(u)

≥π(θ)Var
h
X"
S,0.9466933867735471,"w :r(w)<l
cw,uξ(l)
w (Xu) +
X"
S,0.9470941883767535,"w :r(w)=l
cw,uξ(l)
w (Xu)
 Xp(u)
i ≥πmin"
S,0.94749498997996,"2C0
t2."
S,0.9478957915831663,"Consider the opposite case where Var
h P"
S,0.9482965931863727,"w :r(w)<l cw,uξ(l)
w (Xu)
 Xp(u) = θ
i
≥
t2
4C0 . First,"
S,0.9486973947895792,"EVar
h
X"
S,0.9490981963927856,"w :r(w)<l
cw,uξ(l)
w (Xu)
 Xp(u) = θ
i
≥πmin"
S,0.949498997995992,"4C0
t2."
S,0.9498997995991983,"By applying the 4th property stated in Proposition I.5, we conclude that"
S,0.9503006012024048,"max
w :r(w)<l |cw,u| ≥
√πmin"
S,0.9507014028056112,"2C0
t."
S,0.9511022044088177,"In the case when l = 0. The argument is simpler, which follows directly from (114) and the
Proposition I.5."
S,0.9515030060120241,"Proof of Proposition I.25. Let t0 = maxw,u∈Lρ′ |cw,u| and let w′ ∈W and u′ ∈Lρ′ be the pair such
that t0 = |cw′,u′|. Further, let l0 = r(w′) and u0 = pl(u′)."
S,0.9519038076152304,"If l0 > 0, then we have
|cw′,u0| =
X"
S,0.9523046092184368,"v∈Lu
|cw′,v| ≥|cw′,u′| = t0,"
S,0.9527054108216433,"where the first equality follows from the assumptions of the coefficients. We will try to construct
a sequence of triples (lk, tk, uk) indexed by k such that (lk)k≥0 is strictly decreasing such that
Wlk ̸= ∅, h(uk) = lk, and tk = maxw∈Wlk |cw,uk|."
S,0.9531062124248497,"Suppose we have a triple (lk, tk, uk) such that lk ≥0, h(uk) = lk, Wlk ̸= ∅, and tk =
maxw∈Wlk |cw,uk| for some index k ≥0."
S,0.9535070140280562,We apply Lemma I.26 to get
EITHER EVAR,0.9539078156312625,"1. Either EVar

(Eukfuk)(Xuk)
 Xp(uk)

≥πmin"
EITHER EVAR,0.9543086172344689,"2C0 t2
k, or"
EITHER EVAR,0.9547094188376753,"2. maxw∈W<ℓk |cw,uk| ≥
√πmin"
EITHER EVAR,0.9551102204408818,2C0 tk. (This case cannot happen if ℓk = 0.)
EITHER EVAR,0.9555110220440882,"If the first case is true, then we terminate the process of finding next triple (ℓk+1, tk+1, uk+1)."
EITHER EVAR,0.9559118236472945,"If the second case is true, let w′′ ∈W be the vertex such that |cw′′,uk| = maxw∈W<ℓk |cw,uk| and set
ℓk+1 = r(w′′). Since
cw,uk =
X"
EITHER EVAR,0.956312625250501,"u∈Dℓk+1(uk)
cw,u,"
EITHER EVAR,0.9567134268537074,we have
EITHER EVAR,0.9571142284569139,"tk+1 :=
max
u∈Tuk : h(u)=ℓk+1 |cw,u| ≥
1
Rdℓk−ℓk+1 |cw,uk| =
1
Rdℓk−ℓk+1 tk.
(116)"
EITHER EVAR,0.9575150300601203,"Further, let uk+1 = argmaxu∈Tuk : h(u)=ℓk+1|cw,u|."
EITHER EVAR,0.9579158316633266,"In this way, we produce a new triple satisfying the same assumption as (lk, tk, uk) described above."
EITHER EVAR,0.958316633266533,"Since l0 > l1 > l2 . . . is a monotone decreasing chain of non-negative number, it means this
argument must terminated in r0 steps. Now, suppose it terminates at the k-th step, resulting a triple
(lk, tk, uk), and"
EITHER EVAR,0.9587174348697395,"EVar

(Eukfuk)(Xuk)
 Xp(uk)

≥πmin"
EITHER EVAR,0.9591182364729459,"2C0
t2
k"
EITHER EVAR,0.9595190380761524,"(116)
≥πmin 2C0"
EITHER EVAR,0.9599198396793587, √πmin
EITHER EVAR,0.9603206412825651,"2C0
Rd
−2r0t2
0."
EITHER EVAR,0.9607214428857715,"On the other hand, from Proposition I.5,
X"
EITHER EVAR,0.961122244488978,"u∈Lρ′
Var[fu(Xu)] ≤CRdr0t2
0."
EITHER EVAR,0.9615230460921844,"Therefore, we conclude that X"
EITHER EVAR,0.9619238476953907,"u∈Lρ′
Var[fu(Xu)] ≤C(M, d)R2r0+1EVar

f(X) | Xρ′
."
EITHER EVAR,0.9623246492985972,"J
Properties of Markov Chains and Galton-Watson Tree"
EITHER EVAR,0.9627254509018036,"J.0.1
Proof of Lemma A.5"
EITHER EVAR,0.96312625250501,"Recall that real Jordon Canonical form of M is a q × q diagonal block matrix J
=
diag(J0, J1, . . . , Js1) for some s1 ≤q."
EITHER EVAR,0.9635270541082165,"Since M is ergodic, the eigenspace corresponds to eigenvalue 1 is 1-dimensional. Thus, We may
assume J0 = [1] is the unique Jordan block corresponds to eigenvalue 1."
EITHER EVAR,0.9639278557114228,"For each s ∈[1, s1], Js is either a ms × ms matrix of the form Js =   λs
1"
EITHER EVAR,0.9643286573146292,"λs
...
...
1
λs  "
EITHER EVAR,0.9647294589178357,"for some λs ∈R satisfying |λs| ≤λ; or a Js is a 2ms × 2ms matrix of the form Js =
 "
EITHER EVAR,0.9651302605210421,"λsRs
I2"
EITHER EVAR,0.9655310621242486,"λsRs
...
...
I2
λsRs "
EITHER EVAR,0.9659318637274549,", where |λs| ≤λ, and Rs =

cos(θs)
sin(θs)
−cos(θs)
sin(θs)"
EITHER EVAR,0.9663326653306613,"
is a rotation ma-"
EITHER EVAR,0.9667334669338677,"trix in R2 with parameter θs ∈(0, 2π). In the later case, it corresponds to the conjugate pair of
eigenvalues λs(cos(θs) ± i sin(θs))"
EITHER EVAR,0.9671342685370742,"According to Jordon Decomposition, there exists an invertible matrix P such that M = PJP −1."
EITHER EVAR,0.9675350701402805,"For i ∈[1, q −1], let ϕi be the i + 1th column of P. Because P is invertible, {ϕi}i∈[q] form a linear
basis of functions from [q] to R."
EITHER EVAR,0.9679358717434869,"Since π is a left-eigenvector of M with eigenvalue 1, we have"
EITHER EVAR,0.9683366733466934,"EY ∼πϕi(Y ) = π⊤ϕi = 0,"
EITHER EVAR,0.9687374749498998,because ϕi is a sum of up to two generalized eigenvectors with eigenvalues not equal to 1.
EITHER EVAR,0.9691382765531062,"(A generalized eigenvector v with eigenvalue λ′ of M is a vector which satisfies (M −λ′)kv = ⃗0 for
some positive integer k. Whenever λ′ ̸= 1,"
EITHER EVAR,0.9695390781563126,"π⊤v = (
1
(1 −λ′)k π⊤(M −λ′)k)v =
1
(1 −λ′)k π⊤·⃗0 = 0."
EITHER EVAR,0.969939879759519,"If index i corresponds to Js which associated with a real eigenvalue, then ϕi is a generalized
eigenvector with eigenvalue λs; And if Js associates with a complex conjugate pair or eigenvalues,
then ϕi is a sum of two generalized eigenvectors with eigenvalues λs(cos(θs) + i sin(θs)) and
λs(cos(θs) −i sin(θs)), respectively. ) As a consequence, every function f : [q] 7→R can be
uniquely decomposed in the form"
EITHER EVAR,0.9703406813627254,"f = Ef +
X"
EITHER EVAR,0.9707414829659319,"i∈[q−1]
δiϕi.
(117)"
EITHER EVAR,0.9711422845691383,"With this unique decomposition, let us define a semi-norm"
EITHER EVAR,0.9715430861723446,"∥f∥M = max
i∈[q−1] |δi|."
EITHER EVAR,0.9719438877755511,"Lemma J.1. There exists C > 0 so that for every f : [q] →R,"
EITHER EVAR,0.9723446893787575,"C−1∥f∥2
M ≤VarY ∼π(f(Y )) ≤C∥f∥2
M.
(118)"
EITHER EVAR,0.972745490981964,"Proof. Without lose of generality, let f = P"
EITHER EVAR,0.9731462925851704,"i∈[2,q] δiϕi, since both ∥f∥M and VarY ∼π(f(Y )) are
invariant under a constant shift."
EITHER EVAR,0.9735470941883767,"Let Dπ = diag(π1, . . . , πq). Also, let ⃗δ = (0, δ2, . . . , δq). Then,"
EITHER EVAR,0.9739478957915831,"∥f∥M =∥⃗δ∥∞
and
VarY ∼π(f(Y )) =⃗δ⊤P ⊤DπP⃗δ."
EITHER EVAR,0.9743486973947896,"Let smax and smin be the maximum and minimum singular value of P ⊤DπP, respectively. Together
with q−1/2∥⃗δ∥2 ≤∥⃗δ∥∞≤∥⃗δ∥2, we have"
EITHER EVAR,0.974749498997996,"s2
minq−1∥f∥2
M ≤s2
minq−1∥⃗δ∥2
2 ≤VarY ∼π(f(Y )) ≤s2
max∥⃗δ∥2
2 ≤s2
max∥f∥2
M.
(119)"
EITHER EVAR,0.9751503006012024,"If smin > 0, then we can complete the proof by taking C = max{s2
max, q/s2
min}. It remains to
show that smin > 0, or equvialently P ⊤DπP is invertible. Because M is ergodic, each entry of π
is positive, and thus Dπ is invertible. Hence, P ⊤DπP is invertible because it is a product of three
invertible matrices."
EITHER EVAR,0.9755511022044088,"Lemma J.2. There exists C > 0 so that for every f : [q] →R,"
EITHER EVAR,0.9759519038076152,"C−1∥f∥M ≤∥f −EY ∼πf(Y )∥∞≤C∥f∥M.
(120)"
EITHER EVAR,0.9763527054108216,"Proof. This simply follows from both ∥f∥M and ∥f −Ef∥∞are both norms on the finite dimensional
space {f : [q] →R : EY ∼πf(Y ) = 0}."
EITHER EVAR,0.9767535070140281,"Lemma J.3. There exists C ≥1 depending on M such that For any function f : [q] 7→R and k ∈N,"
EITHER EVAR,0.9771543086172345,"∥M kf∥M ≤Ckqλk∥f∥M.
(121)"
EITHER EVAR,0.9775551102204408,Remark J.4. Notice that M kf can be interpreted as
EITHER EVAR,0.9779559118236473,"E

f(Xu)
 Xpk(u) = i

= (M kf)(i),"
EITHER EVAR,0.9783567134268537,for every u ∈T where pk(u) is well-defined.
EITHER EVAR,0.9787575150300601,Proof.
EITHER EVAR,0.9791583166332666,"∥M kf∥M =∥PJk(
X"
EITHER EVAR,0.9795591182364729,"i∈[q]
δiei)∥M = ∥Jk(
X"
EITHER EVAR,0.9799599198396793,"i∈[2,q]
δiei)∥∞. ≤q max
i∈[2,q] ∥δi∥max
i,j∈[2,q] |Jk
ij|.
(122)"
EITHER EVAR,0.9803607214428858,"Notice that Jk is the diagonal block matrix whose blocks Jk
s for s ∈[s1]. The block Jk
s can be
computed directly: In the case when Js corresponds to a complex conjugate pair of eigenvalues,"
EITHER EVAR,0.9807615230460922,"Jk
s =  "
EITHER EVAR,0.9811623246492986,"λk
sRk
s
 k
1

λk−1
s
Rk−1
s
. . .
 
k
ms−1

λk−ms+1
s
Rk−ms+1
s"
EITHER EVAR,0.981563126252505,"λk
sRk
s
...
...
...
 k
1

λk−1
s
Rk−1
s
λk
sRk
s, "
EITHER EVAR,0.9819639278557114,"
(123)"
EITHER EVAR,0.9823647294589178,"where we treat
 k
r

= 0 if r > k. It can be verified directly by induction, relying on the identity
  k
r−1

+
 k
r

=
 k+1
r

. Further, removing the Rs terms in the above equation we obtain the formula
for Jk
s when Js corresponds to a real eigenvalue."
EITHER EVAR,0.9827655310621243,"Therefore, with
 k
q

≤kq, |λr
s| ≤λr, and maxi,j Rr
sij < 1 for r ≥1, we obtain the bound"
EITHER EVAR,0.9831663326653307,"max
s∈[2,q] max
i,j∈[q] |(Js
k)ij| ≤C′kqλk,
(124)"
EITHER EVAR,0.983567134268537,where C′ is a constant which depends on q and λ.
EITHER EVAR,0.9839679358717435,Now we substitute the above bound into (122) to get
EITHER EVAR,0.9843687374749499,∥M kf∥M ≤qC′kqλk∥f∥M.
EITHER EVAR,0.9847695390781563,The proof is completed by taking C = qC′.
EITHER EVAR,0.9851703406813628,"Proof of Lemma A.5. The proof of Lemma A.5 follows from the ∥· ∥M decay from Lemma J.3 and
that both Var[f] and ∥f −Ef∥∞are comparable to ∥f∥M within a constant multiplicative factor
(Lemma J.1 and Lemma J.2)."
EITHER EVAR,0.9855711422845691,NeurIPS Paper Checklist
EITHER EVAR,0.9859719438877755,"The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit."
EITHER EVAR,0.986372745490982,"Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:"
EITHER EVAR,0.9867735470941884,"• You should answer [Yes] , [No] , or [NA] ."
EITHER EVAR,0.9871743486973948,"• [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available."
EITHER EVAR,0.9875751503006012,• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
EITHER EVAR,0.9879759519038076,"The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper."
EITHER EVAR,0.988376753507014,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
proper justification is given (e.g., ""error bars are not reported because it would be too computationally
expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found."
EITHER EVAR,0.9887775551102205,"IMPORTANT, please:"
EITHER EVAR,0.9891783567134268,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"","
EITHER EVAR,0.9895791583166332,"• Keep the checklist subsection headings, questions/answers and guidelines below."
EITHER EVAR,0.9899799599198397,• Do not modify the questions and only use the provided macros for your answers.
CLAIMS,0.9903807615230461,1. Claims
CLAIMS,0.9907815631262525,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9911823647294589,Answer: [Yes]
CLAIMS,0.9915831663326653,"Justification: The main claim in the abstract and introduction is Theorem 1.6. The appendix
is dedicated to the proof of the theorem."
LIMITATIONS,0.9919839679358717,2. Limitations
LIMITATIONS,0.9923847695390782,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9927855711422846,Answer: [Yes]
LIMITATIONS,0.9931863727454909,"Justification: The result is a theoretical result which justifies low degree hardness for the
tree reconstruction model. This is a first indication that Kesten-Stigum bound appears to be
the computational-statistical barrier for the problem. In terms of the limitation, the paper
does not recover the same result as shown in the paper of Kohler and Moseel [23], where
they only deal with the case λ = 0."
THEORY ASSUMPTIONS AND PROOFS,0.9935871743486974,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9939879759519038,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9943887775551102,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9947895791583167,"Justification: The paper provides a full set of assumptions and the appendix is dedicated to
the complete proof for Theorem 1.6.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.995190380761523,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The result in the paper is a theoretical result. The paper does not include any
experimental results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.9955911823647294,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The result in the paper is a theoretical result. The paper does not include any
experimental results.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.9959919839679359,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The result in the paper is a theoretical result. The paper does not include any
experimental results.
7. Experiment Statistical Significance"
THEORY ASSUMPTIONS AND PROOFS,0.9963927855711423,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The result in the paper is a theoretical result. The paper does not include any
experimental results.
8. Experiments Compute Resources"
THEORY ASSUMPTIONS AND PROOFS,0.9967935871743487,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The result in the paper is a theoretical result. The paper does not include any
experimental results.
9. Code Of Ethics"
THEORY ASSUMPTIONS AND PROOFS,0.9971943887775551,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: The research conducted in the paper conforms with the NeurIPS Code of
Ethics.
10. Broader Impacts"
THEORY ASSUMPTIONS AND PROOFS,0.9975951903807615,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The result of the paper is a theoretical result, which suggests a previous known
bound (Kesten-Stigum) is likely a statistical-computational bound for the tree reconstruction
model. It is an generic result that suggest it is hard to design an efficient algorithm for the
problem below the Kesten-Stigum bound. On the other hand, the result does not have any
direct societal impact."
SAFEGUARDS,0.9979959919839679,11. Safeguards
SAFEGUARDS,0.9983967935871744,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The result in the paper is a theoretical result. The paper does not process and
realease any data or models.
12. Licenses for existing assets"
SAFEGUARDS,0.9987975951903808,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
13. New Assets"
SAFEGUARDS,0.9991983967935871,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
14. Crowdsourcing and Research with Human Subjects"
SAFEGUARDS,0.9995991983967936,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects."
