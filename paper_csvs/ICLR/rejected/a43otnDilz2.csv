Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018867924528301887,"Estimation of (differential) entropy and the related mutual information has been
pursued with signiﬁcant efforts by the machine learning community. To address
shortcomings in previously proposed estimators for differential entropy, here we
introduce KNIFE, a fully parameterized, differentiable kernel-based estimator of
differential entropy. The ﬂexibility of our approach also allows us to construct
KNIFE-based estimators for conditional (on either discrete or continuous variables)
differential entropy, as well as mutual information. We empirically validate our
method on high-dimensional synthetic data and further apply it to guide the training
of neural networks for real-world tasks. Our experiments on a large variety of
tasks, including visual domain adaptation, textual fair classiﬁcation, and textual
ﬁne-tuning demonstrate the effectiveness of KNIFE-based estimation."
INTRODUCTION,0.0037735849056603774,"1
INTRODUCTION"
INTRODUCTION,0.005660377358490566,"Learning tasks requires information (Principe et al., 2006) in the form of training data. Thus,
information measures (Shannon, 1948) (e.g. entropy, conditional entropy and mutual information)
have been a source of inspiration for the design of learning objectives in modern machine learning
(ML) models (Linsker, 1989; Torkkola, 2006). Over the years, a plethora of estimators have been
introduced to estimate the value of the aforementioned measures of information and they have been
applied to many different problems, including information and coding theory, limiting distributions,
model selection, design of experiment and optimal prior distribution, data disclosure, and relative
importance of predictors (Ebrahimi et al., 2010). In these applications, traditional research focused
on both developing new estimators and obtaining provable guarantees on the asymptotic behavior of
these estimators (Liu et al., 2012; Verdú, 2019)."
INTRODUCTION,0.007547169811320755,"However, when used for training deep neural networks, additional requirements need to be satisﬁed.
In particular, the estimator needs to be differentiable w.r.t. the data distribution (R1), computationally
tractable (R2), and rapidly adapt to changes in the underlying distribution (R3). For instance,
Mutual Information (MI), a fundamental measure of dependence between variables, only became
a popular (standalone or regularizing) learning objective for DNNs once estimators satisfying the
above requirements were proposed (Poole et al., 2019; Barber & Agakov, 2003). Although MI
is notoriously difﬁcult to estimate in high dimensions (Kraskov et al., 2004; Pichler et al., 2020;
McAllester & Stratos, 2020), these estimators have demonstrated promising empirical results in
unsupervised representation learning (Krause et al., 2010; Bridle et al., 1992; Hjelm et al., 2019;
Tschannen et al., 2020), discrete/invariant representations (Hu et al., 2017; Ji et al., 2019), generative
modelling (Chen et al., 2016; Zhao et al., 2017), textual disentangling (Cheng et al., 2020b; Colombo
et al., 2021), and applications of the Information Bottleneck (IB) method (Mahabadi et al., 2021;
Devlin et al., 2018; Alemi et al., 2016) among others. Compared to MI, Differential Entropy (DE)
has received less attention from the ML community while also having interesting applications."
INTRODUCTION,0.009433962264150943,"In this paper, we focus on the problem of DE estimation as this quantity naturally appears in many
applications (e.g. reinforcement learning (Shyam et al., 2019; Hazan et al., 2019; Ahmed et al.,
2019; Kim et al., 2019), IB (Alemi et al., 2016), mode collapse (Belghazi et al., 2018)). Traditional
estimators of DE often violate at least one of the requirements (R1) – (R3) listed above (e.g. k-
nearest neighbor based estimators violate (R1)). As a consequence, the absence of DE estimator for
arbitrary data distributions forces deep learning researchers to either restrict themselves to special
cases where closed-form expressions for DE are available (Shyam et al., 2019) or use MI as a proxy"
INTRODUCTION,0.011320754716981131,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.013207547169811321,"(Belghazi et al., 2018). In this work, we introduce a Kernelized Neural dIFferential Entropy (KNIFE)
estimator, that satisﬁes the aforementioned requirements and addresses limitations of existing DE
estimators (Schraudolph, 2004; McAllester & Stratos, 2020). Stemming from recent theoretical
insights (McAllester & Stratos, 2020) that justify the use of DE estimators as building blocks to better
estimate MI, we further apply KNIFE to MI estimation. In the context of deep neural networks with
high dimensional data (e.g. image, text), KNIFE achieves competitive empirical results in applications
where DE or MI is required."
CONTRIBUTIONS,0.01509433962264151,"1.1
CONTRIBUTIONS"
CONTRIBUTIONS,0.016981132075471698,Our work advances methods in DE and MI estimation in several ways.
CONTRIBUTIONS,0.018867924528301886,"1. We showcase limitation of the existing DE estimators proposed in Schraudolph (2004); McAllester
& Stratos (2020) with respect to desirable properties required for training deep neural networks. To
address these shortcomings, we introduce KNIFE, a fully learnable kernel-based estimator of DE. The
ﬂexibility of KNIFE allows us to construct KNIFE-based estimators for conditional DE, conditioning
on either a discrete or continuous random variable.
2. We prove learnability under natural conditions on the underlying probability distribution. By
requiring a ﬁxed Lipschitz condition and bounded support we are not only able to provide an
asymptotic result, but also a conﬁdence bound in the case of a ﬁnite training set. This extends the
consistency result by Ahmad & Lin (1976).
3. We validate on synthetic datasets (including multi-modal, non-Gaussian distributions), that KNIFE
addresses the identiﬁed limitations and outperforms existing methods on both DE and MI estimation.
In particular, KNIFE more rapidly adapts to changes in the underlying data distribution.
4. We conduct extensive experiments on natural datasets (including text and images) to compare
KNIFE-based MI estimators to most recent MI estimators. First, we apply KNIFE in the IB principle
to ﬁne-tune a pretrained language model. Using KNIFE, we leverage a closed-form expression of a
part of the training objective and achieve the best scores among competing MI estimators. Second,
on fair textual classiﬁcation, the KNIFE-based MI estimator achieves near perfect disentanglement
(with respect to the private, discrete label) at virtually no degradation of accuracy in the main task.
Lastly, in the challenging scenario of visual domain adaptation, where both variables are continuous,
KNIFE-based MI estimation also achieves superior results."
EXISTENT METHODS AND RELATED WORKS,0.020754716981132074,"1.2
EXISTENT METHODS AND RELATED WORKS"
EXISTENT METHODS AND RELATED WORKS,0.022641509433962263,"DE estimation. Existing methods for estimating DE ﬁt into one of three categories (Beirlant
et al., 1997; Hlaváˇcková-Schindler et al., 2007; Verdú, 2019): plug-in estimates (Ahmad & Lin,
1976; Györﬁ& Van der Meulen, 1987), estimates based on sample-spacings (Tarasenko, 1968),
and estimates based on nearest neighbor distances (Kozachenko & Leonenko, 1987; Tsybakov &
Van der Meulen, 1996); (Berrett et al., 2019). Our proposed estimator falls into the ﬁrst category
and we will thus focus here on previous work using that methodology. Excellent summaries of all
the available methods can be found in the works (Beirlant et al., 1997; Hlaváˇcková-Schindler et al.,
2007; Wang et al., 2009; Verdú, 2019). In Ahmad & Lin (1976), a ﬁrst nonparametric estimator of
DE was suggested and theoretically analyzed. It builds on the idea of kernel density estimation using
Parzen-Rosenblatt windowing (Rosenblatt, 1956; Parzen, 1962). More detailed analysis followed
(Joe, 1989; Hall & Morton, 1993) but the estimator remained essentially unchanged. Unfortunately,
this classical literature is mostly concerned with appropriate regularity conditions that guarantee
asymptotic properties of estimators, such as (asymptotic) unbiasedness and consistency. Machine
learning applications, however, usually deal with a ﬁxed—often very limited—number of samples."
EXISTENT METHODS AND RELATED WORKS,0.024528301886792454,"Differentiable DE estimation. A ﬁrst estimator that employed a differential learning rule was
introduced in Viola et al. (1996). Indeed, the estimator proposed therein is optimized using stochastic
optimization, it only used a single kernel with a low number of parameters. An extension that
uses a heteroscedastic kernel density estimate, i.e., using different kernels at different positions,
has been proposed in Schraudolph (2004). Still the number of parameters was quite low and vary-
ing means in the kernels or variable weights were not considered. Although the estimation of
DE remained a topic of major interest as illustrated by recent works focusing on special classes
of distributions (Kolchinsky & Tracey, 2017; Chaubey & Vu, 2021) and nonparametric estima-
tors (Sricharan et al., 2013; Kandasamy et al., 2015; Moon et al., 2021), the estimator introduced in
Schraudolph (2004) was not further reﬁned and hardly explored in recent works."
EXISTENT METHODS AND RELATED WORKS,0.026415094339622643,Under review as a conference paper at ICLR 2022
EXISTENT METHODS AND RELATED WORKS,0.02830188679245283,"Differentiable MI estimation. In contrast, there has been a recent surge on new methods for the
estimation of the closely related MI between two random variables. The most prominent examples
include unnormalized energy-based variational lower bounds (Poole et al., 2019), the lower bounds
developed in Nguyen et al. (2010) using variational characterization of f-divergence, the MINE-
estimator developed in Belghazi et al. (2018) from the Donsker-Varadhan representation of MI
which can be also interpreted as an improvement of the plug-in estimator of Suzuki et al. (2008), the
noise-contrastive based bound developed in van den Oord et al. (2018) and ﬁnally a contrastive upper
bound (Cheng et al., 2020a). McAllester & Stratos (2020) point out shortcomings in other estimation
strategies and introduce their own Differences of Entropies (DOE) method."
KNIFE,0.03018867924528302,"2
KNIFE"
KNIFE,0.03207547169811321,"In this section we identify limitations of existing entropy estimators introduced in Schraudolph (2004);
McAllester & Stratos (2020). Subsequently, we present KNIFE, which addresses these shortcomings."
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.033962264150943396,"2.1
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.035849056603773584,"Consider a continuous random vector X ∼p in Rd. Our goal is to estimate the DE h(X) :=
−
R
p(x) log p(x) dx. Given the intractability of this integral, we will rely on a Monte-Carlo estimate
of h(X), using N i.i.d. samples Dx = {xn}N
n=1 to obtain"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.03773584905660377,"bhORACLE(Dx) := −1 N N
X"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.03962264150943396,"n=1
log p(xn).
(1)"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.04150943396226415,"Unfortunately, assuming access to the true density p is often unrealistic, and we will thus construct
an estimate ˆp that can then be plugged into (1) instead of p. If ˆp is smooth, the resulting plug-in
estimator of DE is differentiable (R1)."
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.04339622641509434,"Assuming access to an additional—ideally independent—set of M i.i.d. samples E = {x′
m}M
m=1, we
build upon the Parzen-Rosenblatt estimator (Rosenblatt, 1956; Parzen, 1962)"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.045283018867924525,"ˆp(x; w, E) =
1
wdM M
X"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.04716981132075472,"m=1
κ
x −x′
m
w"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.04905660377358491,"
,
(2)"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.0509433962264151,"where w > 0 denotes the bandwidth and κ is a kernel density. The resulting entropy estimator when
replacing p in (1) by (2) was analyzed in Ahmad & Lin (1976). In Schraudolph (2004), this approach
was extended using the kernel estimator"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.052830188679245285,"ˆpSCHRAU.(x; A, E) := 1 M M
X"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.05471698113207547,"m=1
κAm(x −x′
m),
(3)"
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.05660377358490566,"where A := (A1, . . . , AM) are (distinct, diagonal) covariance matrices and κA(x) = N(x; 0, A) is
a centered Gaussian density with covariance matrix A."
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.05849056603773585,"The DOE method of McAllester & Stratos (2020) is a MI estimator that separately estimates a DE
and a conditional DE. For DE, a simple Gaussian density estimate ˆpDOE(x; θ) = κA(x −µ) is used,
where θ = (A, µ) are the training parameters, the diagonal covariance matrix A and the mean µ."
LIMITATIONS OF EXISTING DIFFERENTIAL ENTROPY ESTIMATORS,0.06037735849056604,"While both SCHRAU. and DOE yield differentiable plug-in estimators for DE, they each have a major
disadvantage. The strategy of Schraudolph (2004) ﬁxes the kernel mean values at E, which implies
that the method cannot adapt to a shifting input distribution (R3). On the other hand, DOE allows for
rapid adaptation, but its simple structure makes it inadequate for the DE estimation of multi-modal
densities. We illustrate these limitations in Section 3.1."
KNIFE ESTIMATOR,0.062264150943396226,"2.2
KNIFE ESTIMATOR"
KNIFE ESTIMATOR,0.06415094339622641,"In KNIFE, the kernel density estimate is given by"
KNIFE ESTIMATOR,0.0660377358490566,"ˆpKNIFE(x; θ) := M
X"
KNIFE ESTIMATOR,0.06792452830188679,"m=1
umκAm(x −am),
(4)"
KNIFE ESTIMATOR,0.06981132075471698,Under review as a conference paper at ICLR 2022
KNIFE ESTIMATOR,0.07169811320754717,"where θ := (A, a, u) and the additional parameters 0 ≤u = (u1, u2, . . . , uM) with 1 · u = 1 and
a = (a1, . . . , aM) are introduced. Note that ˆpKNIFE(x; θ) is a smooth function of θ, and so is our
proposed plug-in estimator"
KNIFE ESTIMATOR,0.07358490566037736,"bhKNIFE(Dx; θ) := −1 N N
X"
KNIFE ESTIMATOR,0.07547169811320754,"n=1
log ˆpKNIFE(xn; θ).
(5)"
KNIFE ESTIMATOR,0.07735849056603773,"KNIFE combines the ideas of Schraudolph (2004); McAllester & Stratos (2020). It is differentiable
and able to adapt to shifting input distributions, while capable of matching multi-modal distributions.
Thus, as we will see in synthetic experiments, incorporating um and shifts am in the optimization
enables the use of KNIFE in non-stationary settings, where the distribution of X evolves over time."
KNIFE ESTIMATOR,0.07924528301886792,"Learning step:
Stemming from the observation that, by the Law of Large Numbers (LLN),"
KNIFE ESTIMATOR,0.08113207547169811,"bhKNIFE(Dx, θ)
LLN
≈−E

log ˆpKNIFE(X; θ)

= h(X) + DKL(p∥ˆpKNIFE( · ; θ)) ≥h(X),
(6)"
KNIFE ESTIMATOR,0.0830188679245283,"we propose to learn the parameters θ by minimizing bhKNIFE, where E may be used to initialize a.
Although not strictly equivalent due to the Monte-Carlo approximation, minimizing bhKNIFE can be
understood as minimizing the Kullback-Leibler (KL) divergence in (6), effectively minimizing the gap
between bhKNIFE and h(X). In fact, bhKNIFE can also be interpreted as the standard maximum likelihood
objective, widely used in modern machine learning. It is worth to mention that the KNIFE estimator
is fully differentiable with respect to θ and the optimization can be tackled by any gradient-based
method (e.g., Adam (Kingma & Ba, 2014) or AdamW (Loshchilov & Hutter, 2017))."
CONVERGENCE ANALYSIS,0.08490566037735849,"2.3
CONVERGENCE ANALYSIS"
CONVERGENCE ANALYSIS,0.08679245283018867,"Note that the classical Parzen-Rosenblatt estimator bh(Dx; w), where (2) is plugged into (1), is a
special case of KNIFE. Thus, the convergence analysis provided in (Ahmad & Lin, 1976, Theorem 1)
also applies and yields sufﬁcient conditions for bhKNIFE(Dx, θ) →h(X). In Appendix C, we extend
this result and, assuming that the underlying distribution p is compactly supported on X = [0, 1]d
and L-Lipschitz continuous, the following theorem is proved.
Theorem 1. For any δ > 0, there exists a function ε(N, M, w) such that, with probability at least
1−δ,
bh(Dx; w)−h(X)
 ≤ε(N, M, w). Additionally, ε(N, M, w) →0 as M,N →∞and w →0
provided that"
CONVERGENCE ANALYSIS,0.08867924528301886,"Nw →0
and
N 2 log N"
CONVERGENCE ANALYSIS,0.09056603773584905,"w2dM
→0,
(7)"
CONVERGENCE ANALYSIS,0.09245283018867924,"where M and N denote the number of samples in E and Dx, respectively."
CONVERGENCE ANALYSIS,0.09433962264150944,"The precise assumptions for Theorem 1 and an explicit formula for ε(N, M, w) are given in Theorem 2
in Appendix C. For instance, Theorem 1 provides a bound on the speed of convergence for the
consistency analysis in (Ahmad & Lin, 1976, Theorem 1)."
ESTIMATING CONDITIONAL DIFFERENTIAL ENTROPY AND MUTUAL INFORMATION,0.09622641509433963,"2.4
ESTIMATING CONDITIONAL DIFFERENTIAL ENTROPY AND MUTUAL INFORMATION"
ESTIMATING CONDITIONAL DIFFERENTIAL ENTROPY AND MUTUAL INFORMATION,0.09811320754716982,"Similar to (McAllester & Stratos, 2020), the proposed DE estimator can be used to estimate other
information measures. In particular, we can use KNIFE to construct estimators of conditional DE and
MI. When estimating the conditional DE and MI for a pair of random variables (X, Y ) ∼p, we not
only use Dx = {xn}N
n=1, but also the according i.i.d. samples Dy = {yn}N
n=1, where (xn, yn) are
drawn according to p."
ESTIMATING CONDITIONAL DIFFERENTIAL ENTROPY AND MUTUAL INFORMATION,0.1,"Conditional Differential Entropy.
We estimate conditional DE h(X|Y ) by considering θ to be a
parameterized function Θ(y) of y. Then all relations previously established naturally generalize and"
ESTIMATING CONDITIONAL DIFFERENTIAL ENTROPY AND MUTUAL INFORMATION,0.1018867924528302,"ˆpKNIFE(x|y; Θ) := ˆpKNIFE(x; Θ(y)),
bhKNIFE(Dx|Dy; Θ) := −1 N N
X"
ESTIMATING CONDITIONAL DIFFERENTIAL ENTROPY AND MUTUAL INFORMATION,0.10377358490566038,"n=1
log ˆpKNIFE(xn|yn; Θ).
(8)"
ESTIMATING CONDITIONAL DIFFERENTIAL ENTROPY AND MUTUAL INFORMATION,0.10566037735849057,"Naturally, minimization of (6) is now performed over the parameters of Θ. If Y is a continuous
random variable, we use an artiﬁcial neural network Θ(y), taking y as its input. On the other hand, if
Y ∈Y is a discrete random variable, we have one parameter θ for each y ∈Y, i.e., Θ = {θy}y∈Y
and ˆpKNIFE(x|y; Θ) = ˆpKNIFE(x; Θ(y)) = ˆpKNIFE(x; θy)."
ESTIMATING CONDITIONAL DIFFERENTIAL ENTROPY AND MUTUAL INFORMATION,0.10754716981132076,Under review as a conference paper at ICLR 2022
ESTIMATING CONDITIONAL DIFFERENTIAL ENTROPY AND MUTUAL INFORMATION,0.10943396226415095,"Mutual Information.
To estimate the MI between random variables X and Y (either discrete or
continuous), recall that MI can be written as I(X; Y ) = h(X) −h(X|Y ). Therefore, we use the
marginal and conditional DE estimators (5) and (8) to build a KNIFE-based MI estimator
bIKNIFE(Dx, Dy; θ, Θ) := bhKNIFE(Dx; θ) −bhKNIFE(Dx|Dy; Θ).
(9)"
EXPERIMENTS USING SYNTHETIC DATA,0.11132075471698114,"3
EXPERIMENTS USING SYNTHETIC DATA"
DIFFERENTIAL ENTROPY ESTIMATION,0.11320754716981132,"3.1
DIFFERENTIAL ENTROPY ESTIMATION"
DIFFERENTIAL ENTROPY ESTIMATION,0.11509433962264151,"In this section we apply KNIFE for DE estimation, comparing it to (3), the method introduced
in Schraudolph (2004), subsequently labeled “SCHRAU.”. It is worth to mention that we did not
perform the Expectation Maximization algorithm, as suggested in (Schraudolph, 2004), but instead
opted to use the same optimization technique as for KNIFE to facilitate a fair comparison."
GAUSSIAN DISTRIBUTION,0.1169811320754717,"3.1.1
GAUSSIAN DISTRIBUTION"
GAUSSIAN DISTRIBUTION,0.11886792452830189,"As a sanity check, we test KNIFE on multivariate normal data in moderately high dimensions,
comparing it to SCHRAU. and DOE, which we trained with the exact same parameters. We performed
these experiments with d = 10 and d = 64 dimensional data. KNIFE yielded the lowest bias and
variance in both cases, despite DOE being perfectly adapted to matching a multivariate Gaussian
distribution. Additional details can be found in Appendix A.1."
GAUSSIAN DISTRIBUTION,0.12075471698113208,"In order to use a DE estimation primitive in a machine learning system, it must be able to adapt to
a changing input distribution during training (R3). As already pointed out in Section 2.1, this is a
severe limitation of SCHRAU., as re-drawing the kernel support E can be either impractical or at
the very least requires a complete re-training of the entropy estimator. Whereas in (4), the kernel
support a is trainable and it can thus adapt to a change of the input distribution. In order to showcase
this ability, we utilize the approach of Cheng et al. (2020a) and successively decrease the entropy,
observing how the estimator adapts. We perform this experiment with data of dimension d = 64 and
repeatedly multiply the covariance matrix of the training vectors with a factor of a = 1"
THE RESULTING,0.12264150943396226,"2. The resulting
entropy estimation is depicted in Figure 1. It is apparent that SCHRAU. suffers from a varying bias.
The bias increases with decreasing variance, as the kernel support is ﬁxed and cannot adapt as the
variance of Dx shrinks. DOE is perfectly adapted to a single Gaussian distribution and performs
similar to KNIFE."
THE RESULTING,0.12452830188679245,"0
1250
2500
3750
5000
Iterations 0 20 40 60 80 100 120 140"
THE RESULTING,0.12641509433962264,Differential Entropy
THE RESULTING,0.12830188679245283,"KNIFE
Schrau.
DoE
True"
THE RESULTING,0.13018867924528302,"Figure 1:
Estimating DE of
Gaussian data with decreasing
variance."
THE RESULTING,0.1320754716981132,"0
4
10
x"
THE RESULTING,0.1339622641509434,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8 PDF"
THE RESULTING,0.13584905660377358,"KNIFE
Schrau.
DoE
True"
THE RESULTING,0.13773584905660377,"0
10000
20000
30000
40000
Iterations 0 5 10 15 20"
THE RESULTING,0.13962264150943396,Differential Entropy
THE RESULTING,0.14150943396226415,"KNIFE
Schrau.
DoE
True"
THE RESULTING,0.14339622641509434,"Figure 2: Left: PDF when estimating DE of a triangle mixture
in 1 dimension. Right: Training run when estimating DE of a
2-component triangle mixture in 8 dimensions."
TRIANGLE MIXTURE,0.14528301886792452,"3.1.2
TRIANGLE MIXTURE"
TRIANGLE MIXTURE,0.1471698113207547,"KNIFE is able to cope with distributions that have multiple modes. While (3) is also capable of
matching multi-modal distributions, DOE is unable to do so, as it approximates any distribution
with a multivariate Gaussian. We illustrate this by matching a mixture of randomly drawn triangle
distributions. The resulting estimated PDFs as well as the ground truth when estimating the entropy
of a 1-dimensional mixture of triangles with 10 components can be observed in Figure 2 (left). With
increasing dimension the difﬁculty of this estimation rises quickly as in d dimensions, the resulting
PDF of independent c-component triangle mixtures has cd modes. To showcase the performance of
KNIFE in this challenging task, we ran 10 training runs for DE estimation of 2-component triangle
mixtures in 8 dimensions. An example training run is depicted in Figure 2 (right)."
TRIANGLE MIXTURE,0.1490566037735849,Under review as a conference paper at ICLR 2022
TRIANGLE MIXTURE,0.1509433962264151,"0
5 k
10 k
15 k
20 k
Steps 0 5 10 15 MI CLUB"
TRIANGLE MIXTURE,0.15283018867924528,"I
True"
TRIANGLE MIXTURE,0.15471698113207547,"0
5 k
10 k
15 k
20 k
Steps DOE"
TRIANGLE MIXTURE,0.15660377358490565,"0
5 k
10 k
15 k
20 k
Steps"
TRIANGLE MIXTURE,0.15849056603773584,InfoNCE
TRIANGLE MIXTURE,0.16037735849056603,"0
5 k
10 k
15 k
20 k
Steps MINE"
TRIANGLE MIXTURE,0.16226415094339622,"0
5 k
10 k
15 k
20 k
Steps NWJ"
TRIANGLE MIXTURE,0.1641509433962264,"0
5 k
10 k
15 k
20 k
Steps KNIFE"
TRIANGLE MIXTURE,0.1660377358490566,"0
5 k
10 k
15 k
20 k
Steps 0 5 10 15 MI CLUB"
TRIANGLE MIXTURE,0.16792452830188678,"I
True"
TRIANGLE MIXTURE,0.16981132075471697,"0
5 k
10 k
15 k
20 k
Steps DOE"
TRIANGLE MIXTURE,0.17169811320754716,"0
5 k
10 k
15 k
20 k
Steps"
TRIANGLE MIXTURE,0.17358490566037735,InfoNCE
TRIANGLE MIXTURE,0.17547169811320754,"0
5 k
10 k
15 k
20 k
Steps MINE"
TRIANGLE MIXTURE,0.17735849056603772,"0
5 k
10 k
15 k
20 k
Steps NWJ"
TRIANGLE MIXTURE,0.1792452830188679,"0
5 k
10 k
15 k
20 k
Steps KNIFE"
TRIANGLE MIXTURE,0.1811320754716981,"0
5 k
10 k
15 k
20 k
Steps 0 5 10 15 MI CLUB"
TRIANGLE MIXTURE,0.1830188679245283,"I
True"
TRIANGLE MIXTURE,0.18490566037735848,"0
5 k
10 k
15 k
20 k
Steps DOE"
TRIANGLE MIXTURE,0.18679245283018867,"0
5 k
10 k
15 k
20 k
Steps"
TRIANGLE MIXTURE,0.18867924528301888,InfoNCE
TRIANGLE MIXTURE,0.19056603773584907,"0
5 k
10 k
15 k
20 k
Steps MINE"
TRIANGLE MIXTURE,0.19245283018867926,"0
5 k
10 k
15 k
20 k
Steps NWJ"
TRIANGLE MIXTURE,0.19433962264150945,"0
5 k
10 k
15 k
20 k
Steps KNIFE"
TRIANGLE MIXTURE,0.19622641509433963,"Figure 3: Top: Estimation of I(Xd; Y d), where (X, Y ) are multivariate Gaussian with correlation
coefﬁcient ρi in the i-th epoch and d = 20. Middle: Estimation of I(Xd; (Y 3)d). Bottom: Estimation
of I(Xd; Y d) for uniform (X, E) and Y = ρiX +
p"
TRIANGLE MIXTURE,0.19811320754716982,"1 −ρ2
i E in the i-th epoch."
MUTUAL INFORMATION ESTIMATION,0.2,"3.2
MUTUAL INFORMATION ESTIMATION"
MUTUAL INFORMATION ESTIMATION,0.2018867924528302,"Multivariate Gauss
We repeat the experiments in (Cheng et al., 2020a), stepping up the MI
I(Xd; Y d) between d i.i.d. copies of joint normal random variables (X, Y ) by increasing their
correlation coefﬁcient, i.e., (X, Y ) are multivariate Gaussian with correlation coefﬁcient ρi in the
i-th epoch. A training run is depicted in the top of Figure 3. As in (Cheng et al., 2020a), we also
repeat the experiment, applying a cubic transformation to Y . The estimation of MI between d i.i.d.
copies of X and Y 3 can be observed in the middle row of Figure 3. The MI is unaffected by this
bijective transformation. In Appendix A.3, the bias and variance are depicted separately."
MUTUAL INFORMATION ESTIMATION,0.2037735849056604,"Sum of Uniformly Distributed Variables
In order to test the ability of KNIFE to adapt to distribu-
tions substantially different from the Gaussian kernel shape, we apply it in MI estimation of I(Xd; Y d)
with uniformly distributed data. To this end, let X and E be centered, uniformly distributed random
variables with E[X2] = E[E2] = 1 and deﬁne Y = ρiX +
p"
MUTUAL INFORMATION ESTIMATION,0.20566037735849058,"1 −ρ2
i E in the i-th epoch. One training
run with d = 20 is shown in Figure 3 (bottom). Details about the source distribution as well as details
of the experiments can be found in Appendix A.3."
EXPERIMENTS ON NATURAL DATA,0.20754716981132076,"4
EXPERIMENTS ON NATURAL DATA"
EXPERIMENTS ON NATURAL DATA,0.20943396226415095,"In this section, we benchmark our proposed KNIFE-based MI estimator on three practical applications,
spanning textual and visual data. We reproduce and compare our method to the most recent MI
estimators including MINE (Belghazi et al., 2018), NWJ (Nguyen et al., 2010), InfoNCE (van den
Oord et al., 2018), CLUB (Cheng et al., 2020a), and DOE (McAllester & Stratos, 2020). We do not
explicitly include the SMILE estimator Song & Ermon (2019) in our comparison as it has the same
gradient as NWJ."
EXPERIMENTS ON NATURAL DATA,0.21132075471698114,"Common notation: In all following applications, we will use Φψ : X →Z to denote an encoder,
where X is the raw input space (i.e., texts or images), and Z denotes a lower dimensional continuous
feature space. Additionally, we will use Cψ : Z →Y to denote a shallow classiﬁer from the latent
space Z to a discrete or continuous target space Y for classiﬁcation or regression, respectively. We
will use ψ to denote the parameters of both models, Φψ and Cψ. CE denotes the cross entropy loss."
EXPERIMENTS ON NATURAL DATA,0.21320754716981133,Under review as a conference paper at ICLR 2022
EXPERIMENTS ON NATURAL DATA,0.21509433962264152,"Table 1: Fine-tuning on GLUE. Following (Lee et al., 2019; Dodge et al., 2020), mean and variance
are computed for 10 seeds."
EXPERIMENTS ON NATURAL DATA,0.2169811320754717,"MRPC
STS-B
RTE"
EXPERIMENTS ON NATURAL DATA,0.2188679245283019,"F1
Accuracy
Pearson
Spearman
Accuracy"
EXPERIMENTS ON NATURAL DATA,0.22075471698113208,"BERT (Devlin et al., 2018)
83.4 ±0.9
88.2 ±0.7
89.2 ±0.4
88.8 ±0.4
69.4 ±0.4
CLUB (Cheng et al., 2020a)
85.0 ±0.4
89.0 ±0.7
89.7 ±0.2
89.4 ±0.1
70.7 ±0.1
InfoNCE (van den Oord et al., 2018)
84.9 ±0.8
88.9 ±0.6
89.4 ±0.4
89.7 ±0.6
70.6 ±0.1
MINE (Belghazi et al., 2018)
80.0 ±2.5
85.0 ±0.9
88.0 ±0.7
88.0 ±0.6
69.0 ±0.9
NWJ (Nguyen et al., 2010)
84.6 ±0.8
88.1 ±0.7
89.8 ±0.1
89.6 ±0.2
69.6 ±0.7
VUB/VIBERT (Alemi et al., 2016)
85.1 ±0.5
89.1 ±0.3
90.0 ±0.2
89.5 ±0.3
70.9 ±0.1
DOE (McAllester & Stratos, 2020)
84.1 ±0.2
88.3 ±0.2
89.6 ±0.2
89.5 ±0.2
69.6 ±0.2
KNIFE
85.3 ±0.1
90.1 ±0.1
90.3 ±0.0
90.1 ±0.0
72.3 ±0.2"
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.22264150943396227,"4.1
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING"
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.22452830188679246,"IB has recently been applied to ﬁne-tune large-scale pretrained models (Mahabadi et al., 2021) such as
BERT (Devlin et al., 2018) and aims at suppressing irrelevant features in order to reduce overﬁtting."
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.22641509433962265,"Problem statement.
Given a textual input X ∈X and a target label Y ∈Y, the goal is to learn
the encoder Φψ and classiﬁer Cψ, such that Φψ(X) retains little information about X, while still
producing discriminative features, allowing the prediction of Y . Thus, the loss of interest is:"
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.22830188679245284,"L = λ · I(Φψ(X); X)
|
{z
}
compression term"
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.23018867924528302,"−I(Φψ(X); Y )
|
{z
}
downstream term"
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.2320754716981132,",
(10)"
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.2339622641509434,where λ controls the trade-off between the downstream and the compression terms.
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.2358490566037736,"Setup.
Following Mahabadi et al. (2021) (relying on VUB), we work with the VIBERT model,
which uses a Gaussian distribution as prior. Φψ is implemented as a stochastic encoder Φψ(X) =
Z ∼N(µψ(X), Σψ(X)). Details on the architecture of µψ and Σψ can be found in Appendix B.
The classiﬁer Cψ is composed of dense layers. To minimize L, the second part of the objective (10)
is bounded using the variational bound from Barber & Agakov (2003). Since we use a Gaussian prior,
h(Z|X) can be expressed in closed form.1 Thus, when using KNIFE, I(X; Z) = h(Z) −h(Z|X)
can be estimated by using bhKNIFE to estimate h(Z). We compare this KNIFE-based MI estimator with
aforementioned MI estimators and the variational upper bound (VUB). For completeness, we also
compare against a BERT model trained by direct minimization of a CE loss."
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.23773584905660378,"We closely follow the protocol of (Mahabadi et al., 2021) and work on the GLUE benchmark (Wang
et al., 2018) originally composed of 5 datasets. However, following (Mahabadi et al., 2021), we
choose to ﬁnetune neither on WNLI (Morgenstern & Ortiz, 2015) nor on CoLA (Warstadt et al.,
2019) due to reported ﬂaws in these datasets. The evaluation is carried out on the standard validation
splits as the test splits are not available. Following standard practice (Liu et al., 2019; Yang et al.,
2019), we report the accuracy and the F1 for MRPC, the accuracy for RTE and the Pearson and
Spearman correlation coefﬁcient for STS-B."
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING,0.23962264150943396,"Results.
Table 1 reports our results on the GLUE benchmark. We observe that KNIFE obtains
the best results on all three datasets and the lowest variance on MRPC and STS-B. The use of a
Gaussian prior in the stochastic encoder Φψ could explain the observed improvement of KNIFE-based
estimation over MI-estimators such as CLUB, InfoNCE, MINE, DOE, or NWJ."
FAIR TEXTUAL CLASSIFICATION,0.24150943396226415,"4.2
FAIR TEXTUAL CLASSIFICATION"
FAIR TEXTUAL CLASSIFICATION,0.24339622641509434,"In fair classiﬁcation, we would like the model to take its decision without utilizing private information
such as gender, age, or race. For this task, MI can be minimized to disentangle the output of the
encoder Z and a private label S ∈S (e.g., gender, age, or race)."
FAIR TEXTUAL CLASSIFICATION,0.24528301886792453,1h(Z|X) = 1
FAIR TEXTUAL CLASSIFICATION,0.24716981132075472,2 ln |Σψ(X)| + d
FAIR TEXTUAL CLASSIFICATION,0.2490566037735849,"2 ln(2πe), where d is the dimension of X and | · | denotes the determinant."
FAIR TEXTUAL CLASSIFICATION,0.2509433962264151,Under review as a conference paper at ICLR 2022
FAIR TEXTUAL CLASSIFICATION,0.2528301886792453,"10
2
10
1
10
0
10
1
10
2 0.50 0.55 0.60 0.65 0.70"
FAIR TEXTUAL CLASSIFICATION,0.25471698113207547,Main Task Accuracy
FAIR TEXTUAL CLASSIFICATION,0.25660377358490566,> 0.71
FAIR TEXTUAL CLASSIFICATION,0.25849056603773585,Random Guess
FAIR TEXTUAL CLASSIFICATION,0.26037735849056604,"Adv
CLUB
InfoNCE
NWJ
DoE
KNIFE"
FAIR TEXTUAL CLASSIFICATION,0.2622641509433962,(a) Y (sentiment)
FAIR TEXTUAL CLASSIFICATION,0.2641509433962264,"10
2
10
1
10
0
10
1
10
2
0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66"
FAIR TEXTUAL CLASSIFICATION,0.2660377358490566,Private Task Accuracy
FAIR TEXTUAL CLASSIFICATION,0.2679245283018868,> 0.64
FAIR TEXTUAL CLASSIFICATION,0.269811320754717,Perfect Privacy
FAIR TEXTUAL CLASSIFICATION,0.27169811320754716,(b) S (sentiment)
FAIR TEXTUAL CLASSIFICATION,0.27358490566037735,"10
3
10
2
10
1
10
0
10
1
0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85"
FAIR TEXTUAL CLASSIFICATION,0.27547169811320754,Main Task Accuracy
FAIR TEXTUAL CLASSIFICATION,0.27735849056603773,> 0.84
FAIR TEXTUAL CLASSIFICATION,0.2792452830188679,Random Guess
FAIR TEXTUAL CLASSIFICATION,0.2811320754716981,(c) Y (mention)
FAIR TEXTUAL CLASSIFICATION,0.2830188679245283,"10
3
10
2
10
1
10
0
10
1
0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675"
FAIR TEXTUAL CLASSIFICATION,0.2849056603773585,Private Task Accuracy
FAIR TEXTUAL CLASSIFICATION,0.28679245283018867,> 0.66
FAIR TEXTUAL CLASSIFICATION,0.28867924528301886,Perfect Privacy
FAIR TEXTUAL CLASSIFICATION,0.29056603773584905,(d) S (mention)
FAIR TEXTUAL CLASSIFICATION,0.29245283018867924,"Figure 4: Results on the fair classiﬁcation task for both main (Figures 4a and 4c) and private task
(Figures 4b and 4d) for both mention and sentiment labels. Results of MINE are not reported because
of instabilities that prevent the network from converging. Figures 4b and 4d are obtained by training
an ofﬂine classiﬁer to recover the protected attribute S from Φψ(X).
Problem Statement.
Given an input text X, a discrete target label Y and a private label S, the loss
is given by
L = CE(Y ; Φψ(X))
|
{z
}
downstream task"
FAIR TEXTUAL CLASSIFICATION,0.2943396226415094,"+λ · I(Φψ(X); S)
|
{z
}
disentangled"
FAIR TEXTUAL CLASSIFICATION,0.2962264150943396,",
(11)"
FAIR TEXTUAL CLASSIFICATION,0.2981132075471698,"where λ controls the trade-off between minimizing MI and CE loss. In this framework, a classiﬁer
is said to be fair or to achieve perfect privacy if no statistical information about S can be extracted
from Φψ(X) by an adversarial classiﬁer. Overall, a good model should achieve high accuracy on the
main task (i.e., prediction of Y ) while removing information about the protected attribute S. This
information is measured by training an ofﬂine classiﬁer to recover the protected attribute S from
Φψ(X)."
FAIR TEXTUAL CLASSIFICATION,0.3,"Setup.
We compute the second term of (11) with competing MI estimators, as well as the model
from Elazar & Goldberg (2018), which will be referred to as “Adv”, as it utilizes an adversary to
recover the private label from the latent representation Z. For KNIFE-based MI estimation, we use
two DE estimators (as S is a binary label), following the approach outlined in Section 2.4. All
derivations are detailed in Appendix B.
We follow the experimental setting from Elazar & Goldberg (2018); Barrett et al. (2019) and use two
datasets from the DIAL corpus (Blodgett et al., 2016) (over 50 million tweets) where the protected
attribute S is the race and the main labels are sentiment or mention labels. The mention label indicates
whether a tweet is conversational or not. We follow the ofﬁcial split using 160 000 tweets for training
and two additional sets composed of 10 000 tweets each for development and testing. In all cases, the
labels S and Y are binary and balanced, thus a random guess corresponds to 50% accuracy."
FAIR TEXTUAL CLASSIFICATION,0.3018867924528302,"Results.
Figure 4 gathers results on the fair classiﬁcation task. The upper dashed lines represent
the (private and main) task accuracies when training a model with only the CE loss (case λ = 0
in (11)). This shows that the learned encoding Φψ(X) contains information about the protected
attribute, when training is only performed for the main task. On both the sentiment and mention task,
we observe that a KNIFE-based estimator can achieve perfect privacy (see Figures 4b and 4d) with
nearly no accuracy loss in the main task (see Figures 4a and 4c). The other MI estimators exhibit
different behavior. For sentiment labels, most MI estimators fail to reach perfect privacy (CLUB,
NWJ, DOE, and Adv) while others (InfoNCE) achieve perfect privacy while degrading the main task
accuracy (10% loss on main accuracy). For mention labels, CLUB can also reach perfect privacy with
almost no degradation of the accuracy of the main task. Overall, it is worth noting that KNIFE-based
MI estimation enables better control of the degree of disentanglement than the reported baselines."
UNSUPERVISED DOMAIN ADAPTATION,0.30377358490566037,"4.3
UNSUPERVISED DOMAIN ADAPTATION"
UNSUPERVISED DOMAIN ADAPTATION,0.30566037735849055,"In unsupervised domain adaptation, the goal is to transfer knowledge from the source domain (S)
with a potentially large number of labeled examples to a target domain (T), where only unlabeled
examples are available."
UNSUPERVISED DOMAIN ADAPTATION,0.30754716981132074,"Problem Statement.
The learner is given access to labeled images from a source domain (xs, y) ∼
(XS, Y ) ∈XS × Y and unlabeled images from a target domain xt ∼XT ∈XT . The goal is to"
UNSUPERVISED DOMAIN ADAPTATION,0.30943396226415093,Under review as a conference paper at ICLR 2022
UNSUPERVISED DOMAIN ADAPTATION,0.3113207547169811,"Table 2: Domain adaptation results: M (MNIST), MM (MNIST M), U (USPS), SV (SVHN), C
(CIFAR10) and S (STL10). Results are averaged over 3 seeds."
UNSUPERVISED DOMAIN ADAPTATION,0.3132075471698113,"M →MM
S →C
U →M
M →U
C →S
SV →M
Mean"
UNSUPERVISED DOMAIN ADAPTATION,0.3150943396226415,"Source only
51.9 ±0.8
58.3 ±0.2
91.1 ±0.7
93.5 ±0.6
72.3 ±0.5
54.7 ±2.8
70.3 ±0.9"
UNSUPERVISED DOMAIN ADAPTATION,0.3169811320754717,"CLUB
79.1 ±2.2
59.9 ±1.9
96.0 ±0.2
96.8 ±0.5
71.6 ±1.3
83.8 ±3.4
81.2 ±1.7
DOE
82.2 ±2.6
58.9 ±0.8
97.2 ±0.3
94.2 ±0.9
68.8 ±1.4
86.4 ±5.4
81.3 ±1.9
INFONCE
77.3 ±0.5
61.0 ±0.1
97.4 ±0.2
97.0 ±0.3
70.6 ±0.8
89.2 ±4.1
82.1 ±1.0
MINE
76.7 ±0.4
61.2 ±0.3
97.7 ±0.1
97.3 ±0.1
70.8 ±1.0
91.8 ±0.8
82.6 ±0.4
NWJ
77.1 ±0.6
61.2 ±0.3
97.6 ±0.1
97.3 ±0.5
72.1 ±0.7
91.4 ±0.8
82.8 ±0.5
KNIFE
78.7 ±0.7
61.8 ±0.5
97.7 ±0.3
97.4 ±0.4
71.2 ±1.8
93.2 ±0.2
83.4 ±0.6"
UNSUPERVISED DOMAIN ADAPTATION,0.31886792452830187,"learn a classiﬁcation model {Φψ, Cψ} that generalizes well to the target domain. Training models
on the supervised source data only results in domain-speciﬁc latent representations Φψ(X) leading
to poor generalization (when X is chosen randomly from {XS, XT }). In order to make the latent
representations as domain-agnostic as possible, we follow the information-theoretic method proposed
by Gholami et al. (2020), and used in Cheng et al. (2020a). The idea is to learn an additional binary
model {Φd
ν, Cd
ν}, whose goal it is to guess the domain D ∈{0, 1} of X. The latent representation
learned by Φd
ν will therefore contain all the domain-speciﬁc information that we would like the
main encoder Φψ to discard. In other words, we would like Φψ(X) and Φd
ν(X) to be completely
disentangled, which naturally corresponds to the minimization of I(Φψ(X); Φd
ν(X)). Concretely,
the domain classiﬁer is trained to minimize the CE between domain labels D and its own predictions,
whereas the main classiﬁer is trained to properly classify support samples while minimizing the MI
between Φψ(X) and Φd
ν(X). Using f d
ν := Cd
ν ◦Φd
ν and fψ := Cψ ◦Φψ, the objectives are"
UNSUPERVISED DOMAIN ADAPTATION,0.32075471698113206,"min
ν
CE(D; f d
ν(X))
and
min
ψ
CE(Y ; fψ(XS)) + λ · I(Φψ(X); Φd
ν(X)).
(12)"
UNSUPERVISED DOMAIN ADAPTATION,0.32264150943396225,"Setup.
The different MI estimators are compared based on their ability to guide training by
estimating I(Φψ(X); Φd
ν(X)) in (12). We follow the setup of Cheng et al. (2020a) as closely as
possible, and consider a total of 6 source/target scenarios formed with MNIST (LeCun & Cortes,
2010), MNIST-M (Ganin et al., 2016), SVHN (Netzer et al., 2011), CIFAR-10 (Krizhevsky et al.,
2009), and STL-10 (Coates et al., 2011) datasets. We reproduce all methods and allocate the same
budget for hyper-parameter tuning to every method. The exhaustive list of hyper-parameters can be
found in Appendix B."
UNSUPERVISED DOMAIN ADAPTATION,0.32452830188679244,"Results.
Results are presented in Table 2. The KNIFE-based estimator is able to outperform MI
estimators in this challenging scenario where both Φψ(X) and Φd
ν(X) are continuous."
CONCLUDING REMARKS,0.3264150943396226,"5
CONCLUDING REMARKS"
CONCLUDING REMARKS,0.3283018867924528,"We introduced KNIFE, a fully learnable, differentiable kernel-based estimator of differential entropy,
designed for deep learning applications. We constructed a mutual information estimator based on
KNIFE and showcased several applications. KNIFE is a general purpose estimator and does not
require any special properties of the learning problem. It can thus be incorporated as part of any
training objective, where differential entropy or mutual information estimation is desired. In the case
of mutual information, one random variable may even be discrete."
CONCLUDING REMARKS,0.330188679245283,"Despite the fundamental challenges in the problem of differential entropy estimation, beyond limita-
tions arising from the use of a ﬁnite number of samples, KNIFE has demonstrated promising empirical
results in various representation learning tasks."
CONCLUDING REMARKS,0.3320754716981132,"Future work will focus on improving the conﬁdence bounds given in Theorem 1. In particular,
tailoring them towards KNIFE using tools from (Birge & Massart, 1995; Singh & Poczos, 2014).
Another potential extension is direct estimation of the gradient of entropy, when ˆpKNIFE(x; θ) has
been learned (Mohamed et al., 2020; Song et al., 2020). This could be applied after the learning
phase of KNIFE and is left for future work."
CONCLUDING REMARKS,0.3339622641509434,Under review as a conference paper at ICLR 2022
REFERENCES,0.33584905660377357,REFERENCES
REFERENCES,0.33773584905660375,"I. Ahmad and Pi-Erh Lin. A nonparametric estimation of the entropy for absolutely continuous
distributions. IEEE Trans. Inf. Theory, 22(3):372–375, May 1976."
REFERENCES,0.33962264150943394,"Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the
impact of entropy on policy optimization. 97:151–160, 09–15 Jun 2019."
REFERENCES,0.34150943396226413,"Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv, abs/1612.00410, 2016."
REFERENCES,0.3433962264150943,"David Barber and Felix Agakov. The im algorithm: A variational approach to information maxi-
mization. In Proceedings of the 16th International Conference on Neural Information Processing
Systems, NIPS’03, pp. 201–208, Cambridge, MA, USA, 2003. MIT Press."
REFERENCES,0.3452830188679245,"Maria Barrett, Yova Kementchedjhieva, Yanai Elazar, Desmond Elliott, and Anders Søgaard. Ad-
versarial removal of demographic attributes revisited. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pp. 6331–6336, 2019."
REFERENCES,0.3471698113207547,"Jan Beirlant, Edward J. Dudewicz, László Györﬁ, and Edward C. Van der Meulen. Nonparametric
entropy estimation: An overview. International Journal of Mathematical and Statistical Sciences,
6(1):17–39, 1997."
REFERENCES,0.3490566037735849,"Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R. Devon Hjelm, and Aaron C. Courville. MINE:
mutual information neural estimation. arXiv, abs/1801.04062, 2018."
REFERENCES,0.35094339622641507,"Thomas B Berrett, Richard J Samworth, and Ming Yuan. Efﬁcient multivariate entropy estimation
via k-nearest neighbour distances. The Annals of Statistics, 47(1):288–318, 2019."
REFERENCES,0.35283018867924526,"Lucien Birge and Pascal Massart. Estimation of Integral Functionals of a Density. The Annals of
Statistics, 23(1):11 – 29, 1995. doi: 10.1214/aos/1176324452. URL https://doi.org/10.
1214/aos/1176324452."
REFERENCES,0.35471698113207545,"Su Lin Blodgett, Lisa Green, and Brendan O’Connor. Demographic dialectal variation in social
media: A case study of african-american english. arXiv, abs/1608.08868, 2016."
REFERENCES,0.35660377358490564,"John Bridle, Anthony Heading, and David MacKay. Unsupervised classiﬁers, mutual information
and 'phantom targets. In J. Moody, S. Hanson, and R. P. Lippmann (eds.), Advances in Neural
Information Processing Systems, volume 4. Morgan-Kaufmann, 1992."
REFERENCES,0.3584905660377358,"Yogendra P Chaubey and Nhat Linh Vu. On the estimation of entropy for non-negative data. Journal
of Statistical Theory and Practice, 15(2), 2021."
REFERENCES,0.360377358490566,"Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. pp.
2180–2188, 2016."
REFERENCES,0.3622641509433962,"Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. CLUB: A
contrastive log-ratio upper bound of mutual information. 119, 13–18 Jul 2020a."
REFERENCES,0.3641509433962264,"Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong
Li, and Lawrence Carin. Improving disentangled text representation learning with information-
theoretic guidance. arXiv, abs/2006.00693, 2020b."
REFERENCES,0.3660377358490566,"Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence
and statistics. JMLR Workshop and Conference Proceedings, 2011."
REFERENCES,0.36792452830188677,"Pierre Colombo, Chloe Clavel, and Pablo Piantanida. A novel estimator of mutual information for
learning to disentangle textual representations. arXiv, abs/2105.02685, 2021."
REFERENCES,0.36981132075471695,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv, abs/1810.04805, 2018."
REFERENCES,0.37169811320754714,Under review as a conference paper at ICLR 2022
REFERENCES,0.37358490566037733,"Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith.
Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.
arXiv, abs/2002.06305, 2020."
REFERENCES,0.3754716981132076,"Nader Ebrahimi, Ehsan S. Sooﬁ, and Reﬁk Soyer. Information measures in perspective. International
Statistical Review, 78(3):383–412, 2010."
REFERENCES,0.37735849056603776,"Yanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data.
arXiv, abs/1808.06640, 2018."
REFERENCES,0.37924528301886795,"Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The journal of machine learning research, 17(1), 2016."
REFERENCES,0.38113207547169814,"Behnam Gholami, Pritish Sahu, Ognjen Rudovic, Konstantinos Bousmalis, and Vladimir Pavlovic.
Unsupervised multi-target domain adaptation: An information theoretic approach. IEEE Transac-
tions on Image Processing, 29, 2020."
REFERENCES,0.38301886792452833,"László Györﬁand Edward C. Van der Meulen. Density-free convergence properties of various
estimators of entropy. Computational Statistics & Data Analysis, 5(4):425–436, 1987."
REFERENCES,0.3849056603773585,"Peter Hall and Sally Morton. On the estimation of entropy. Annals of the Institute of Statistical
Mathematics, 1993."
REFERENCES,0.3867924528301887,"Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efﬁcient maximum entropy
exploration. 97:2681–2691, 09–15 Jun 2019."
REFERENCES,0.3886792452830189,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. 2019."
REFERENCES,0.3905660377358491,"Katerina Hlaváˇcková-Schindler, Milan Paluš, Martin Vejmelka, and Joydeep Bhattacharya. Causality
detection based on information-theoretic approaches in time series analysis. Physics Reports, 441
(1), 2007."
REFERENCES,0.39245283018867927,"Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning
discrete representations via information maximizing self-augmented training. In Doina Precup
and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning Research, pp. 1558–1567. PMLR, 06–11 Aug
2017."
REFERENCES,0.39433962264150946,"Xu Ji, Joao F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised
image classiﬁcation and segmentation. In Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV), October 2019."
REFERENCES,0.39622641509433965,"Harry Joe. Estimation of entropy and other functionals of a multivariate density. Annals of the
Institute of Statistical Mathematics, 41(4):683–697, 1989."
REFERENCES,0.39811320754716983,"Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry Wasserman, and
james m robins.
Nonparametric von mises estimators for entropies, divergences and mu-
tual informations.
In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-
ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
06138bc5af6023646ede0e1f7c1eac75-Paper.pdf."
REFERENCES,0.4,"Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. EMI:
exploration with mutual information. 97:3360–3369, 2019."
REFERENCES,0.4018867924528302,Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2014.
REFERENCES,0.4037735849056604,"Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances.
Entropy, 19(7), 2017."
REFERENCES,0.4056603773584906,Under review as a conference paper at ICLR 2022
REFERENCES,0.4075471698113208,"LF Kozachenko and Nikolai N Leonenko. Sample estimate of the entropy of a random vector.
Problemy Peredachi Informatsii, 23(2):9–16, 1987."
REFERENCES,0.40943396226415096,"Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Phys.
Rev. E, 69, Jun 2004."
REFERENCES,0.41132075471698115,"Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized informa-
tion maximization. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta (eds.),
Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc., 2010."
REFERENCES,0.41320754716981134,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.41509433962264153,Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
REFERENCES,0.4169811320754717,"Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to ﬁnetune
large-scale pretrained language models. arXiv, abs/1909.11299, 2019."
REFERENCES,0.4188679245283019,"Ralph Linsker. How to generate ordered maps by maximizing the mutual information between input
and output signals. In Neural Comput., 1989."
REFERENCES,0.4207547169811321,"Han Liu, Larry Wasserman, and John Lafferty. Exponential concentration for mutual information
estimation with application to forests. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger
(eds.), Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc.,
2012."
REFERENCES,0.4226415094339623,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv, abs/1907.11692, 2019."
REFERENCES,0.42452830188679247,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv, abs/1711.05101,
2017."
REFERENCES,0.42641509433962266,"Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. Variational information bottle-
neck for effective low-resource ﬁne-tuning. ICLR. OpenReview. net, 2021."
REFERENCES,0.42830188679245285,"David McAllester and Karl Stratos. Formal limitations on the measurement of mutual information.
In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International
Conference on Artiﬁcial Intelligence and Statistics, volume 108 of Proceedings of Machine
Learning Research, pp. 875–884. PMLR, 26–28 Aug 2020."
REFERENCES,0.43018867924528303,"Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient
estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020."
REFERENCES,0.4320754716981132,"Kevin R. Moon, Kumar Sricharan, and Alfred O. Hero. Ensemble estimation of generalized mutual
information with applications to genomics. IEEE Transactions on Information Theory, 67(9):
5963–5996, 2021. doi: 10.1109/TIT.2021.3100108."
REFERENCES,0.4339622641509434,"Leora Morgenstern and Charles Ortiz. The winograd schema challenge: Evaluating progress in
commonsense reasoning. In Twenty-Seventh IAAI Conference, 2015."
REFERENCES,0.4358490566037736,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011."
REFERENCES,0.4377358490566038,"XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
2010."
REFERENCES,0.439622641509434,"Liam Paninski. Estimation of entropy and mutual information. Neural computation, 15(6), 2003."
REFERENCES,0.44150943396226416,"Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathemat-
ical statistics, 33(3):1065–1076, 1962."
REFERENCES,0.44339622641509435,Under review as a conference paper at ICLR 2022
REFERENCES,0.44528301886792454,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32. Curran Associates, Inc.,
2019."
REFERENCES,0.44716981132075473,"Georg Pichler, Pablo Piantanida, and Günther Koliander. On the estimation of information measures
of continuous distributions. arXiv, abs/2002.02851, 2020."
REFERENCES,0.4490566037735849,"Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceed-
ings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of
Machine Learning Research, pp. 5171–5180. PMLR, 09–15 Jun 2019."
REFERENCES,0.4509433962264151,"Jose C Principe, Dongxin Xu, John Fisher, and Simon Haykin. Information theoretic learning.
Citeseer, 2006."
REFERENCES,0.4528301886792453,"Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out:
Guarding protected attributes by iterative nullspace projection. arXiv preprint arXiv:2004.07667,
2020."
REFERENCES,0.4547169811320755,"Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. Ann. Math.
Statist., 27(3):832–837, 1956."
REFERENCES,0.45660377358490567,"N. N. Schraudolph. Gradient-based manipulation of nonparametric entropy estimates. IEEE Transac-
tions on Neural Networks, 15(4), 2004."
REFERENCES,0.45849056603773586,"C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27(3):
379–423, July 1948."
REFERENCES,0.46037735849056605,"Pranav Shyam, Wojciech Ja´skowski, and Faustino Gomez. Model-based active exploration. In
Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
5779–5788. PMLR, 09–15 Jun 2019."
REFERENCES,0.46226415094339623,"Shashank Singh and Barnabas Poczos.
Exponential concentration of a density functional
estimator.
In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Wein-
berger (eds.), Advances in Neural Information Processing Systems, volume 27. Curran Asso-
ciates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/
af5afd7f7c807171981d443ad4f4f648-Paper.pdf."
REFERENCES,0.4641509433962264,"Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. arXiv, abs/1910.06222, 2019."
REFERENCES,0.4660377358490566,"Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach
to density and score estimation. In Uncertainty in Artiﬁcial Intelligence, pp. 574–584. PMLR,
2020."
REFERENCES,0.4679245283018868,"Kumar Sricharan, Dennis Wei, and Alfred O. Hero. Ensemble estimators for multivariate entropy
estimation. IEEE Transactions on Information Theory, 59(7):4374–4388, 2013. doi: 10.1109/TIT.
2013.2251456."
REFERENCES,0.469811320754717,"Taiji Suzuki, Masashi Sugiyama, Jun Sese, and Takafumi Kanamori. Approximating mutual informa-
tion by maximum likelihood density ratio estimation. In New challenges for feature selection in
data mining and knowledge discovery, pp. 5–20. PMLR, 2008."
REFERENCES,0.4716981132075472,"F.P. Tarasenko. On the evaluation of an unknown probability density function, the direct estimation of
the entropy from independent observations of a continuous random variable, and the distribution-
free entropy test of goodness-of-ﬁt. Proceedings of the IEEE, 56(11):2052–2053, 1968."
REFERENCES,0.47358490566037736,"Kari Torkkola. Information-Theoretic Methods in “Feature Extraction: Foundations and Applica-
tions”, pp. 167–185. Springer, Berlin, Heidelberg, 2006."
REFERENCES,0.47547169811320755,Under review as a conference paper at ICLR 2022
REFERENCES,0.47735849056603774,"Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. In International Conference on Learning
Representations, 2020."
REFERENCES,0.47924528301886793,"Alexandre B. Tsybakov and E. C. Van der Meulen. Root-n consistent estimators of entropy for
densities with unbounded support. Scandinavian Journal of Statistics, pp. 75–83, 1996."
REFERENCES,0.4811320754716981,"Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv, abs/1807.03748, 2018."
REFERENCES,0.4830188679245283,"Sergio Verdú. Empirical estimation of information measures: A literature guide. Entropy, 21(8),
2019."
REFERENCES,0.4849056603773585,"Paul Viola, Nicol N Schraudolph, and Terrence J Sejnowski. Empirical entropy manipulation for
real-world problems. Advances in neural information processing systems, 1996."
REFERENCES,0.4867924528301887,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv,
abs/1804.07461, 2018."
REFERENCES,0.48867924528301887,"Qing Wang, Sanjeev R. Kulkarni, and Sergio Verdú. Universal estimation of information measures
for analog sources. Foundations and Trends in Communications and Information Theory, 5(3):
265–353, 2009."
REFERENCES,0.49056603773584906,"Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.
Transactions of the Association for Computational Linguistics, 7:625–641, 2019."
REFERENCES,0.49245283018867925,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers:
State-of-the-art natural language processing. arXiv, abs/1910.03771, 2019."
REFERENCES,0.49433962264150944,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.
Xlnet: Generalized autoregressive pretraining for language understanding. arXiv, abs/1906.08237,
2019."
REFERENCES,0.4962264150943396,"Shengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Information maximizing variational
autoencoders. arXiv, abs/1706.02262, 2017."
REFERENCES,0.4981132075471698,Under review as a conference paper at ICLR 2022
REFERENCES,0.5,APPENDIX
REFERENCES,0.5018867924528302,"A
EXPERIMENTAL DETAILS OF EXPERIMENTS WITH SYNTHETIC DATA"
REFERENCES,0.5037735849056604,"Implementation of KNIFE in PyTorch (Paszke et al., 2019) is rather straightforward. The constraint
on the weights u can be satisﬁed by applying a softmax transformation. The covariance matrices
were parameterized by the lower-triangular factor in the Cholesky decomposition of the precision
matrices, guaranteeing the deﬁniteness constraint to be satisﬁed."
REFERENCES,0.5056603773584906,"A.1
DIFFERENTIAL ENTROPY ESTIMATION OF GAUSSIAN DATA"
REFERENCES,0.5075471698113208,"In Section 3.1.1, the estimation of the entropy h(X) = d"
REFERENCES,0.5094339622641509,"2 log 2πe for X ∼N(0, Id) was performed
with the hyperparameters given in Table 3. The mean error and its empirical standard deviation are
reported in Table 5 over 20 runs, where an independently drawn evaluation set with the same size as
the training set is used. At d = 10 we have the entropy h = d"
REFERENCES,0.5113207547169811,"2 log 2πe = 14.19, while for the higher
dimension, d = 64 we ﬁnd h = 90.81."
REFERENCES,0.5132075471698113,"In the experiment depicted in Figure 1, entropy is decreased after every epoch by letting Xi ∼
N(0, aiId), where i = 0, . . . , 4 is the epoch index. That is, Xi =
√"
REFERENCES,0.5150943396226415,"aiGd, where G is a standard
normal random variable, resulting in an decrease of the DE by ∆= −d"
REFERENCES,0.5169811320754717,2 log a ≈22.18 for a = 1
REFERENCES,0.5188679245283019,"2
with every epoch. We start at h(X0) = d"
REFERENCES,0.5207547169811321,"2 log 2πe ≈90.81 and successively decrease until h(X4) =
h(X0) + 4∆≈2.1. Additional parameters can be found in Table 4."
REFERENCES,0.5226415094339623,"Computational Resources.
Training was performed on an NVidia V100 GPU. Taken together,
training for the ﬁrst experiments of entropy estimation in dimensions d = 10, 64, as well as the
experiment depicted in Figure 1 used GPU time of less than 5 minutes."
REFERENCES,0.5245283018867924,"A.2
DIFFERENTIAL ENTROPY ESTIMATION OF TRIANGLE MIXTURES"
REFERENCES,0.5264150943396226,"In Section 3.1.2, we perform an estimation of the entropy of c-component triangle mixture distribu-
tions. The PDF of such a c-component triangle-mixture, is given by"
REFERENCES,0.5283018867924528,"p(x) = c
X"
REFERENCES,0.530188679245283,"i=1
wiΛsi"
REFERENCES,0.5320754716981132,"
x −i −1 2"
REFERENCES,0.5339622641509434,"
,
(13)"
REFERENCES,0.5358490566037736,where Λs(x) := 1
REFERENCES,0.5377358490566038,"s max{0, 2 −4s|x|} is a centered triangle PDF with width s > 0. The scales
s = (s1, . . . , sc) and weights w = (w1, . . . , wc) satisfy 0 < si, wi < 1 and Pc
i=1 wi = 1. Before
the experiment, we choose w uniformly at random from the c-probability simplex and the scales are
chosen uniformly at random in [0.1, 1.0]. An example for c = 10 is the true PDF depicted in Figure 2"
REFERENCES,0.539622641509434,"Table 3: Experimental details of ﬁrst experi-
ment in Section 3.1.1."
REFERENCES,0.5415094339622641,"Parameter
Value"
REFERENCES,0.5433962264150943,"Source Distribution X
X ∼N(0, Id)
Dimension d
10 and 64
Optimizer
Adam
Learning Rate
0.01
Batch Size N
128
Kernel Size M
128
Iterations per epoch
200
Epochs
1
Runs
20"
REFERENCES,0.5452830188679245,"Table 4: Experimental details of the experiment
depicted in Figure 1."
REFERENCES,0.5471698113207547,"Parameter
Value"
REFERENCES,0.5490566037735849,"Source Distribution X
X ∼N(0, aiId)
for i = 0, . . . , 4
Dimension d
64
Factor a
1
2
Optimizer
Adam
Learning Rate
0.01
Batch Size N
128
Kernel Size M
128
Iterations per epoch
1000
Epochs
5
Runs
1"
REFERENCES,0.5509433962264151,Under review as a conference paper at ICLR 2022
REFERENCES,0.5528301886792453,"Table 5: Results of ﬁrst experiment in Section 3.1.1 with Gaussian data. We provide the average
distance |h −bh| and the empirical standard deviation. Experimental details are given in Table 3."
REFERENCES,0.5547169811320755,"|h −bh|
d = 10
d = 64"
REFERENCES,0.5566037735849056,"DOE
0.8388 ± 1.0045
3.3170 ± 1.8281
SCHRAU.
0.7301 ± 0.0428
9.8919 ± 0.1604
KNIFE
0.0461 ± 0.0139
2.8045 ± 0.0796"
REFERENCES,0.5584905660377358,"Table 6: Experimental details of the experiment
resulting in the PDF in Figure 2 (left)."
REFERENCES,0.560377358490566,"Parameter
Value"
REFERENCES,0.5622641509433962,"Source Distribution X
c-component
triangle mixtures
Components c
10
Dimension d
1
Optimizer
Adam
Learning Rate
0.1
Batch Size N
128
Kernel Size M
128
Iterations per epoch
100
Epochs
10
Runs
1"
REFERENCES,0.5641509433962264,"Table 7: Experimental details of the experiment
resulting in the training depicted in Figure 2
(right)."
REFERENCES,0.5660377358490566,"Parameter
Value"
REFERENCES,0.5679245283018868,"Source Distribution X
c-component
triangle mixtures
Components c
2
Dimension d
8
Optimizer
Adam
Learning Rate
0.001
Batch Size N
128
Kernel Size M
128
Iterations per epoch
1000
Epochs
20
Runs
10"
REFERENCES,0.569811320754717,"(left). For d > 1, we perform the estimation on d i.i.d. copies. Note that the triangle mixture with c
components in d-dimensional space has cd modes, i.e., the support can be partitioned into cd disjoint
components."
REFERENCES,0.5716981132075472,"The parameters of the experiment yielding Figure 2 (left) are given in Table 6, while the details of the
experiment depicted in Figure 2 (right) can be found in Table 7. In the latter experiment, over ten runs,
entropy was estimated to an accuracy of 1.6563 ± 0.8528 by KNIFE, accurate to 2.4445 ± 0.5439
using (3) and with an accuracy of 7.1070 ± 2.7984 by DOE. This is the mean absolute error and its
empirical standard deviation over all 10 runs, where the evaluation set was drawn independently from
the training set and has the same size as the training set."
REFERENCES,0.5735849056603773,"Computational Resources.
Training was performed on an NVidia V100 GPU. Training in d = 1
dimension, that resulted in Figure 2 (left) can be performed in seconds, while all training required for
producing Figure 2 (right) used approximately 1.5 hours of GPU time."
REFERENCES,0.5754716981132075,"A.3
MUTUAL INFORMATION ESTIMATION"
REFERENCES,0.5773584905660377,"In Section 3.2, we estimate I(Xd; Y d) and I(Xd; (Y 3)d) where (X, Y ) are multivariate correlated
Gaussian distributions with correlation coefﬁcient ρi in the i-th epoch. Subsequently, we estimate
I(Xd; Y d) where X, E ∼U[−
√ 3,
√"
REFERENCES,0.5792452830188679,"3] are independent and Y is given by Y = ρiX +
p"
REFERENCES,0.5811320754716981,"1 −ρ2
i E.
In both cases, ρi is chosen such that I(Xd; Y d) = 2i in the i-th epoch."
REFERENCES,0.5830188679245283,"All neural networks are randomly initialized. The bias, variance, and MSE during training as a
function of the MI, can be observed in Figure 5."
REFERENCES,0.5849056603773585,"The estimation is performed in 10 runs, randomly choosing the training meta-parameters as proposed
by McAllester & Stratos (2020). In Figure 3 (bottom), we present the best run for each method,
selected by distance from the true MI at the end of training. The bias, variance, and MSE during
training, as a function of the MI, can be observed in Figure 6. Details about the source distribution as
well as details of the experiments can be found in Table 8. During experimentation it turned out to be"
REFERENCES,0.5867924528301887,Under review as a conference paper at ICLR 2022
REFERENCES,0.5886792452830188,"2
4
6
8
10 0 2 4 6 8 Bias"
REFERENCES,0.590566037735849,Gaussian
REFERENCES,0.5924528301886792,"2
4
6
8
10 10
2 10
1 100"
REFERENCES,0.5943396226415094,Variance
REFERENCES,0.5962264150943396,"2
4
6
8
10
MI Values 0 10 20 30 40 50 60 70 MSE"
REFERENCES,0.5981132075471698,"CLUB
DOE
InfoNCE
MINE
NWJ
KNIFE"
REFERENCES,0.6,"2
4
6
8
10 2 4 6 8 Bias Cubic"
REFERENCES,0.6018867924528302,"2
4
6
8
10 10
2 10
1 100 101"
REFERENCES,0.6037735849056604,Variance
REFERENCES,0.6056603773584905,"2
4
6
8
10
MI Values 0 20 40 60 80 100 MSE"
REFERENCES,0.6075471698113207,"CLUB
DOE
InfoNCE
MINE
NWJ
KNIFE"
REFERENCES,0.6094339622641509,Figure 5: Left: Estimation of I(X; Y ); Right: Estimation of I(X; Y 3) (cubic transformation).
REFERENCES,0.6113207547169811,Under review as a conference paper at ICLR 2022
REFERENCES,0.6132075471698113,Table 8: Experimental details of the training depicted in Figure 3 (bottom).
REFERENCES,0.6150943396226415,"Parameter
Value"
REFERENCES,0.6169811320754717,"Dimension d
20
Optimizer
Adam
Learning Rates
0.01, 0.003, 0.001, 0.0003
Batch Size N
128
Kernel Size M
128
Iterations per epoch
20 000
Epochs
1
Runs
10"
REFERENCES,0.6188679245283019,"beneﬁcial to train the parameters Θ and θ in (9) separately and substantially increase the learning
rate for the training of θ. Thus, we increase the learning rate for the training of θ by a factor of 103."
REFERENCES,0.620754716981132,"Model Architecture for Θ.
We utilize the feed-forward architecture, also used in McAllester
& Stratos (2020). It is a simple architecture with two linear layers, one hidden layer using tanh
activation, immediately followed by an output layer. The number of neurons in the hidden layer is a
meta-parameter selected randomly from {64, 128, 256} for each training run. Three models with this
architecture are used for the three parameters (A, a, u), as described by (4), where only the output
dimension is changed to ﬁt the parameter dimension."
REFERENCES,0.6226415094339622,"Computational Resources.
Training was performed, using about 6 hours of GPU time on an
NVidia V100 GPU to carry out the experiment depicted in Figure 3 (bottom)."
REFERENCES,0.6245283018867924,"B
EXPERIMENTAL DETAILS OF EXPERIMENTS ON NATURAL DATA"
REFERENCES,0.6264150943396226,"B.1
ON THE PARAMETER UPDATE"
REFERENCES,0.6283018867924528,"In Section 4, we rely on two different types of models: pretrained (e.g., ﬁne tuning with VIBERT)
and randomly initialized (e.g., in fair classiﬁcation and domain adaptation). When working with
randomly initialized networks the parameters are updated. However, it is worth noting that in the
literature the pretrained model parameters (i.e. ψ) are not always updated (see Ravfogel et al. (2020)).
In our experiments: (i) We always update the parameters (even for pretrained models), and (ii) we
did not change the way the parameters were updated in concurrent works (to ensure fair comparison).
Speciﬁcally,"
REFERENCES,0.630188679245283,"• for language model ﬁnetuning (Appendix B.2), we followed Mahabadi et al. (2021) and did
a joint update;"
REFERENCES,0.6320754716981132,"• for the fair classiﬁcation task (Appendix B.3), we followed common practice and used the
algorithm described in Algorithm 1 which rely on an alternated update;"
REFERENCES,0.6339622641509434,"• for the domain adaptation task (Appendix B.4), we followed common practice and used a
joint method."
REFERENCES,0.6358490566037736,"B.2
INFORMATION BOTTLENECK FOR LANGUAGE MODEL FINETUNING"
REFERENCES,0.6377358490566037,"For this experiment we follow the experimental setting introduced in Mahabadi et al. (2021) and
work with the GLUE data2."
REFERENCES,0.6396226415094339,"Model Architecture.
We report in Table 9, the multilayer perceptron (MLP) used to compute the
compressed sentence representations produced by BERT. Variance and Mean MLP networks are
composed of fully connected layers."
REFERENCES,0.6415094339622641,2see https://gluebenchmark.com/faq
REFERENCES,0.6433962264150943,Under review as a conference paper at ICLR 2022
REFERENCES,0.6452830188679245,"2
4
6
8
10 2 4 6 8 Bias"
REFERENCES,0.6471698113207547,"2
4
6
8
10 10
3 10
2 10
1 100 101"
REFERENCES,0.6490566037735849,Variance
REFERENCES,0.6509433962264151,"2
4
6
8
10
MI Values 0 20 40 60 80 MSE"
REFERENCES,0.6528301886792452,"CLUB
DOE
InfoNCE
MINE
NWJ
KNIFE"
REFERENCES,0.6547169811320754,"Figure 6: Bias, variance, and MSE for MI estimation on uniformly distributed data."
REFERENCES,0.6566037735849056,Algorithm 1 Disentanglement using a MI-based regularizer
REFERENCES,0.6584905660377358,"1: INPUT Labelled training set D = {(xj, sj, yj)∀j ∈[n + 1, N]}; independent set of samples E;
θ parameters KNIFE; ψ parameters of network.
2: INITIALIZE parameters θ, ψ
3: OPTIMIZATION
4: while (θ, ψ) not converged do
5:
for
i ∈[1, Unroll] do
▷Learning Step for KNIFE
6:
Sample a batch B from E
7:
Update θ using ((9)).
8:
end for
9:
Sample a batch B′ from D
10:
Update θ with B′ ((11)).
11: end while
12: OUTPUT Encoder and classiﬁer weights ψ"
REFERENCES,0.660377358490566,Under review as a conference paper at ICLR 2022
REFERENCES,0.6622641509433962,"Table 9: Architecture of the model used in the
IB ﬁnetuning experiment. We use ReLU as
an activation function."
REFERENCES,0.6641509433962264,"Layer type
Input shape
Output shape"
REFERENCES,0.6660377358490566,"Fully connected
768
2304+K"
FULLY CONNECTED,0.6679245283018868,"4
Fully connected
2304+K"
FULLY CONNECTED,0.6698113207547169,"4
768+K 2"
FULLY CONNECTED,0.6716981132075471,"Table 10: Experimental details on Informa-
tion Bottleneck."
FULLY CONNECTED,0.6735849056603773,"Parameter
Value"
FULLY CONNECTED,0.6754716981132075,"Learning Rate
See Appendix B.2
Optimizer
AdamW
Warmup Steps
0.0
Dropout
0.0
Batch Size
32"
FULLY CONNECTED,0.6773584905660377,Table 11: Datasets from the GLUE as used in our experiments.
FULLY CONNECTED,0.6792452830188679,"#Labels
Train
Val.
Test"
FULLY CONNECTED,0.6811320754716981,"RTE
2
2.5k
0.08k
3k
STS-B
1 (regression)
5.8k
1.5k
1.4k
MRPC
2
3.7k
0.4k
1.7k"
FULLY CONNECTED,0.6830188679245283,"Model Training.
For model training, all models are trained for 6 epochs and we use early stopping
(best model is selected on validation set error). For IB, λ is selected in {10−4, 10−5, 10−6} and K is
selected in {144, 192, 288, 384}. We follow (Alemi et al., 2016) where the posterior is averaged over
5 samples and a linear annealing schedule is used for λ. Additional hyper-parameters are reported in
Table 10."
FULLY CONNECTED,0.6849056603773584,"Dataset Statistics.
Table 11 reports the statistics of the dataset used in our ﬁnetuning experiment."
FULLY CONNECTED,0.6867924528301886,"Computational Resources.
For all these experiments we rely on NVidia-P100 with 16GB of RAM.
To complete the full grid-search on 10 seeds and on the three datasets, approximately 1.5k hours are
required."
FULLY CONNECTED,0.6886792452830188,"B.3
FAIR TEXTUAL CLASSIFICATION"
FULLY CONNECTED,0.690566037735849,"In this section, we gather the experimental details for the textual fair classiﬁcation task."
FULLY CONNECTED,0.6924528301886792,"B.3.1
DETAILS OF THE KNIFE-BASED ESTIMATOR"
FULLY CONNECTED,0.6943396226415094,"In this experiment, we estimate the MI between a continuous random variable, namely Z = Φψ(X),
and a discrete variable, denoted by S ∈S = {1, 2, . . . , |S|}. We follow the strategy outlined in
Section 2.4 for estimating the conditional DE h(Z|S). However, we will reuse the estimate of the
conditional PDF ˆp(z|s; Θ) to compute an estimate of the DE as"
FULLY CONNECTED,0.6962264150943396,"h(Z) ≈−1 N N
X"
FULLY CONNECTED,0.6981132075471698,"n=1
log X"
FULLY CONNECTED,0.7,"s∈S
ˆpKNIFE(zn|s; Θ)ˆp(s) !"
FULLY CONNECTED,0.7018867924528301,",
(14)"
FULLY CONNECTED,0.7037735849056603,where ˆp(s) = 1
FULLY CONNECTED,0.7056603773584905,"N |{n : sn = s}| is used to indicate the empirical distribution of S in the training set
Ds.3 In our experiments, with |S| = 2, we found that estimating the DE h(Z) based on the KNIFE
estimator learnt for h(Z|S) increases the stability of training. We adopted the same strategy for DOE."
FULLY CONNECTED,0.7075471698113207,"B.3.2
EXPERIMENTAL DETAILS"
FULLY CONNECTED,0.7094339622641509,"Model Architecture.
For the encoder, we use a bidirectionnal GRU with two layers with hidden
and input dimension set to 128. We use LeakyReLU as the activation function. The classiﬁcation
head is composed of fully connected layers of input dimension 256. We use a learning rate of 0.0001
for AdamW. The dropout rate is set to 0.2. The number of warmup steps is set to 1000."
FULLY CONNECTED,0.7113207547169811,"3As we work with balanced batches, we will have ˆp(s) =
1
|S|."
FULLY CONNECTED,0.7132075471698113,Under review as a conference paper at ICLR 2022
FULLY CONNECTED,0.7150943396226415,"Computational Resources.
For all these experiments, we rely on NVIDIA-P100 with 16GB of
RAM. Each model is trained for 30k steps. The model with the lowest MI is selected. The training of
a single network takes around 3 hours."
FULLY CONNECTED,0.7169811320754716,"B.4
UNSUPERVISED DOMAIN ADAPTATION"
FULLY CONNECTED,0.7188679245283018,"We follow the experimental setup given in Cheng et al. (2020a) as closely as possible, i.e., we pick
hyperparameters given in the paper, or if not provided, those set in the code:4"
FULLY CONNECTED,0.720754716981132,"Model Training.
We use Adam optimizer for all modules with a learning rate of 0.001. Batch size
is set to 128. We set the weighting parameter λ = 0.1. The original code of Cheng et al. (2020a) uses
15 000 training iterations, but we found most methods had not properly converged at this stage, and
hence use 25 000 iterations instead. Similar to other experiments, we set the kernel size M = 128."
FULLY CONNECTED,0.7226415094339622,"Model Architecture.
Table 12 summarizes the architectures used for the different modules. For
the MI network of each method, the best conﬁguration, based on the validation set of the ﬁrst task
MNIST →MNIST-M, is chosen among 4 conﬁgurations: with or without LayerNorm and with ReLU
or tanh activation."
FULLY CONNECTED,0.7245283018867924,"Computational Resources.
For these experiments, we used a cluster of NVIDIA-V100 with 16GB
of RAM. Each training (i.e., 25k iterations) on a single task requires on average 2 hours. Given that
we have 6 tasks, and repeat the training for 3 different seeds, on average 36 hours computation time
is required for each method."
FULLY CONNECTED,0.7264150943396226,"C
BOUNDING THE ERROR"
FULLY CONNECTED,0.7283018867924528,"In the following, ﬁx L > 0 and let PL be the set of L-Lipschitz PDFs supported5 on X := [0, 1]d,
i.e.,
R"
FULLY CONNECTED,0.730188679245283,"X p(x) dx = 1, and"
FULLY CONNECTED,0.7320754716981132,"∀x, y ∈Rd : |p(x) −p(y)| ≤L∥x −y∥
(15)"
FULLY CONNECTED,0.7339622641509433,"for p ∈PL, where6 ∥x∥:= P"
FULLY CONNECTED,0.7358490566037735,k |xk|.
FULLY CONNECTED,0.7377358490566037,"Assume p ∈PL and let κ be a PDF supported on X. In order to show that estimation of h(X) is
achievable, we use a standard Parzen-Rosenblatt estimator ˆp(x; w) :=
1
Mwd"
FULLY CONNECTED,0.7396226415094339,"PM
m=1 κ
  x−X′
m
w

, as
in (2). The entropy estimate is then deﬁned by the empirical average"
FULLY CONNECTED,0.7415094339622641,"bh(Dx; w) := −1 N N
X"
FULLY CONNECTED,0.7433962264150943,"n=1
log ˆp(Xn; w).
(16)"
FULLY CONNECTED,0.7452830188679245,"Further, deﬁne the following quantities, which are assumed to be ﬁnite:"
FULLY CONNECTED,0.7471698113207547,"pmax := max{p(x) : x ∈X},
(17)"
FULLY CONNECTED,0.7490566037735849,"C1 :=
Z
p(x) log2 p(x)dx,
(18)"
FULLY CONNECTED,0.7509433962264151,"C2 := L
Z
∥u∥κ(u)du,
(19)"
FULLY CONNECTED,0.7528301886792453,"Kmax := max{κ(x) : x ∈X}.
(20)"
FULLY CONNECTED,0.7547169811320755,Note that it is easily seen that pmax ≤L
FULLY CONNECTED,0.7566037735849057,"2 and C1 ≤max

pmax log2 pmax, 4e−2	
by our assumptions.
The requirement C2, Kmax < ∞represents a mild condition on the kernel function κ."
FULLY CONNECTED,0.7584905660377359,We can now show the following.
FULLY CONNECTED,0.7603773584905661,"4https://github.com/Linear95/CLUB/tree/master/MI_DA.
5Any known compact support sufﬁces. An afﬁne transformation then yields X = [0, 1]d, while possibly
resulting in a different Lipschitz constant.
6The ℓ1 norm is chosen to facilitate subsequent computations. By the equivalence of norms on Rd, any norm
sufﬁces."
FULLY CONNECTED,0.7622641509433963,Under review as a conference paper at ICLR 2022
FULLY CONNECTED,0.7641509433962265,"Table 12: Architectures used for the Unsupervised Domain Adaptation experiments. For the MI
network of each method, we chose the best performing conﬁguration between with or without
LayerNorm layer and best activation between ReLU and tanh, using the validation set of MNIST-M."
FULLY CONNECTED,0.7660377358490567,"Encoder (both Φ and Φd)
Layer type
Input shape
Output shape
Details"
FULLY CONNECTED,0.7679245283018868,"Convolution sequence
(3, H, W)
(64, H, W)
Cf. below
Noisy downsampling
(64, H, W)
(64, H // 2, W // 2)
Cf. below
Convolution sequence
(64, H // 2, W // 2)
(64, H // 2, W // 2)
Cf. below
Noisy downsampling
(64, H // 2, W // 2)
(64, H // 4, W // 4)
Cf. below
Convolution sequence
(64, H // 4, W // 4)
(64, H // 4, W // 4)
Cf. below
Global Average Pool
(64, H // 4, W // 4)
(64,)
-"
FULLY CONNECTED,0.769811320754717,"Main classiﬁer C
Layer type
Input shape
Output shape"
FULLY CONNECTED,0.7716981132075472,"Fully connected
(64,)
(10,)"
FULLY CONNECTED,0.7735849056603774,Domain classiﬁer Cd
FULLY CONNECTED,0.7754716981132076,"Layer type
Input shape
Output shape"
FULLY CONNECTED,0.7773584905660378,"Fully connected
(64,)
(2,)"
FULLY CONNECTED,0.779245283018868,"Convolution sequence
Layer type
Input shape
Output shape
Parameters"
D CONVOLUTION,0.7811320754716982,"2D convolution
(3, H, W)
(64, H, W)
3x3, 64 channels, Stride=1, Padding=1
2D BatchNorm
(3, H, W)
(64, H, W)
-
Activation
(3, H, W)
(64, H, W)
LeakyRelu 0.1
2D convolution
(64, H, W)
(64, H, W)
3x3, 64 channels, Stride=1, Padding=1
2D BatchNorm
(64, H, W)
(64, H, W)
-
Activation
(64, H, W)
(64, H, W)
LeakyRelu 0.1
2D convolution
(64, H, W)
(64, H, W)
3x3, 64 channels, Stride=1, Padding=1
2D BatchNorm
(64, H, W)
(64, H, W)
-
Activation
(64, H, W)
(64, H, W)
LeakyRelu 0.1"
D CONVOLUTION,0.7830188679245284,"Noisy downsampling
Layer type
Input shape
Output shape
Parameters"
D CONVOLUTION,0.7849056603773585,"MaxPool
(64, H, W)
(64, H // 2, H // 2)
2x2, Stride=2
Dropout
(64, H // 2, W // 2)
(64, H // 2, H // 2)
p=0.5
Noise
(64, H // 2, W // 2)
(64, H // 2, H // 2)
Gaussian with σ = 1"
D CONVOLUTION,0.7867924528301887,"MI network
Layer type
Input shape
Output shape
Details"
D CONVOLUTION,0.7886792452830189,"LayerNorm
(Cin,)
(Cin,)
Optional
Fully connected
(Cin,)
(64,)
Activation = [ReLU, tanh]
LayerNorm
(Cin,)
(64,)
Optional
Fully connected
(64,)
(Cout,)
Optional"
D CONVOLUTION,0.7905660377358491,Theorem 2. With probability greater than 1 −δ we have
D CONVOLUTION,0.7924528301886793,| h(X) −bh(Dx; w)| ≤−log 
D CONVOLUTION,0.7943396226415095,1 −3NKmax wdδ s
D CONVOLUTION,0.7962264150943397,log 6N
D CONVOLUTION,0.7981132075471699,"δ
2M
−3NC2w δ  + r 3C1"
D CONVOLUTION,0.8,"Nδ ,
(21)"
D CONVOLUTION,0.8018867924528302,if the expression in the logarithm is positive.
D CONVOLUTION,0.8037735849056604,Under review as a conference paper at ICLR 2022
D CONVOLUTION,0.8056603773584906,"In particular, the estimation error approaches zero as N →∞if w = w(N) →0, M = M(N) →∞
are chosen such that"
D CONVOLUTION,0.8075471698113208,"Nw →0,
(22)"
D CONVOLUTION,0.809433962264151,N 2 log N
D CONVOLUTION,0.8113207547169812,"w2dM
→0.
(23)"
D CONVOLUTION,0.8132075471698114,We prove Theorem 2 in several Lemmas.
D CONVOLUTION,0.8150943396226416,"Lemma 3. Fix δ > 0 and x0 ∈X. Then, with probability greater than 1 −δ,"
D CONVOLUTION,0.8169811320754717,|p(x0) −ˆp(x0)| ≤Kmax wd s log 2
D CONVOLUTION,0.8188679245283019,"δ
2M + C2w.
(24)"
D CONVOLUTION,0.8207547169811321,"Proof. First, we can show that"
D CONVOLUTION,0.8226415094339623,|E[ˆp(x0)] −p(x0)| =
MWD,0.8245283018867925,"1
Mwd M
X m=1"
MWD,0.8264150943396227,"Z
κ
x0 −x w"
MWD,0.8283018867924529,"
p(x)dx −p(x0) (25)"
MWD,0.8301886792452831,"=

1
wd"
MWD,0.8320754716981132,"Z
κ
x0 −x w"
MWD,0.8339622641509434,"
p(x)dx −p(x0)

(26) ="
MWD,0.8358490566037736,"Z
κ (u) p(x0 −wu)du −p(x0)

(27) ="
MWD,0.8377358490566038,"Z
κ (u) [p(x0 −wu) −p(x0)]du

(28)"
MWD,0.839622641509434,"≤
Z
κ (u) |p(x0 −wu) −p(x0)|du
(29)"
MWD,0.8415094339622642,"≤
Z
κ (u) Lw∥u∥du
(30)"
MWD,0.8433962264150944,"= wC2.
(31)"
MWD,0.8452830188679246,"Next, note that"
MWD,0.8471698113207548,|E[ˆp(x0)] −ˆp(x0)| ≤Kmax wd s log 2
MWD,0.8490566037735849,"δ
2M
(32)"
MWD,0.8509433962264151,"holds with probability greater than 1 −δ as the requirements of McDiarmid’s inequality (Paninski,
2003, Sec. 3) are satisﬁed with cj = Kmax"
MWD,0.8528301886792453,Mwd and thus P{|E[ˆp(x0)] −ˆp(x0)| ≥ε} ≤δ with
MWD,0.8547169811320755,ε = Kmax wd s log 2
MWD,0.8566037735849057,"δ
2M .
(33)"
MWD,0.8584905660377359,Combining (31) and (32) gives (24).
MWD,0.8603773584905661,"Lemma 4. For any continuous random variable X supported on X and a ≥0, we have"
MWD,0.8622641509433963,"P{p(X) ≤a} ≤a.
(34)"
MWD,0.8641509433962264,"Proof. We apply Markov’s inequality to the random variable Y =
1
p(X) and observe that"
MWD,0.8660377358490566,"P{p(X) ≤a} = P{Y ≥a−1} ≤vol(X)a = a.
(35)"
MWD,0.8679245283018868,"Lemma 5. If x > 0, y ≥a > 0, 0 < a < 1, and |x −y| ≤δ < a, then"
MWD,0.869811320754717,"| log x −log y| ≤log
a
a −δ = −log

1 −δ a"
MWD,0.8716981132075472,"
.
(36)"
MWD,0.8735849056603774,Under review as a conference paper at ICLR 2022
MWD,0.8754716981132076,"Proof. Case x ≥y. We can write y = a+b and x = y +c = a+b+c for b ≥0 and 0 ≤c ≤δ < a.
log x y"
MWD,0.8773584905660378,"= log

1 +
c
a + b"
MWD,0.879245283018868,"
(37)"
MWD,0.8811320754716981,"≤log

1 + c a"
MWD,0.8830188679245283,"
≤log

1 + δ a"
MWD,0.8849056603773585,"
.
(38)"
MWD,0.8867924528301887,"Furthermore,"
MWD,0.8886792452830189,"log

a
a −δ"
MWD,0.8905660377358491,"
−log

1 + δ a"
MWD,0.8924528301886793,"
= log
1
(a + δ)(a −δ)
(39)"
MWD,0.8943396226415095,"= log
1
a2 −δ2
(40)"
MWD,0.8962264150943396,≥log 1
MWD,0.8981132075471698,"a2 = −2 log a > 0.
(41)"
MWD,0.9,"Case x < y. Here, we can write y = a + b and x = y −c = a + b −c for b ≥0 and 0 ≤c ≤δ < a.
log x y"
MWD,0.9018867924528302,= log y
MWD,0.9037735849056604,"x
(42)"
MWD,0.9056603773584906,"= log

a + b
a + b −c"
MWD,0.9075471698113208,"
(43)"
MWD,0.909433962264151,"≤log

a
a −c"
MWD,0.9113207547169812,"
(44)"
MWD,0.9132075471698113,"≤log

a
a −δ"
MWD,0.9150943396226415,"
= −log

1 −δ a"
MWD,0.9169811320754717,"
.
(45)"
MWD,0.9188679245283019,"Proof of Theorem 2. We apply Lemma 3 N times and use the union bound to show that with proba-
bility greater than 1 −δ"
MWD,0.9207547169811321,3 we have for every n ∈[N]
MWD,0.9226415094339623,|p(Xn) −ˆp(Xn)| ≤Kmax wd s
MWD,0.9245283018867925,log 6N
MWD,0.9264150943396227,"δ
2M
+ C2w.
(46)"
MWD,0.9283018867924528,"Similarly, by Lemma 4, we have with probability greater than 1 −δ"
THAT,0.930188679245283,3 that
THAT,0.9320754716981132,"p(Xn) ≥
δ
3N
(47)"
THAT,0.9339622641509434,for all n ∈[N].
THAT,0.9358490566037736,"Again by the union bound, we have that with probability greater than 1 −2δ"
THAT,0.9377358490566038,"3 both (46) and (47) hold
for all n ∈[N], and thus, by Lemma 5, we obtain

bh(Dx; w) + 1 N N
X"
THAT,0.939622641509434,"n=1
log p(Xn) ="
N,0.9415094339622642,"1
N N
X"
N,0.9433962264150944,"n=1
log p(Xn)"
N,0.9452830188679245,ˆp(Xn) (48) ≤−log  1 − Kmax wd q
N,0.9471698113207547,log 6N
N,0.9490566037735849,"δ
2M
+ C2w δ
3N "
N,0.9509433962264151,"
(49)"
N,0.9528301886792453,= −log 
N,0.9547169811320755,1 −3NKmax wdδ s
N,0.9566037735849057,log 6N
N,0.9584905660377359,"δ
2M
−3NC2w δ "
N,0.960377358490566,",
(50)"
N,0.9622641509433962,Under review as a conference paper at ICLR 2022
N,0.9641509433962264,"provided the argument in the logarithm is positive. Finally, we have the upper bound on the variance E   "
N,0.9660377358490566,"h(X) + 1 N N
X"
N,0.9679245283018868,"n=1
log p(Xn) !2"
N,0.969811320754717,"=
1
N 2 N
X"
N,0.9716981132075472,"n=1
E[(h(X) + log p(X))2]
(51) = 1"
N,0.9735849056603774,"N (E[log2 p(X)] −h(X)2)
(52) ≤1"
N,0.9754716981132076,"N C1
(53)"
N,0.9773584905660377,"and apply Chebychev’s inequality, showing that with probability greater than 1 −δ"
N,0.9792452830188679,"3,
h(X) + 1 N N
X"
N,0.9811320754716981,"n=1
log p(Xn) ≤ r 3C1"
N,0.9830188679245283,"Nδ .
(54)"
N,0.9849056603773585,The union bound and the triangle inequality applied to (50) and (54) yields the desired result.
N,0.9867924528301887,"D
LIBRARIES USED"
N,0.9886792452830189,"For our experiments, we built upon code from the following sources."
N,0.9905660377358491,"• VIBERT (Mahabadi et al., 2021) at github.com/rabeehk/vibert."
N,0.9924528301886792,"• TRANSFORMERS (Wolf et al., 2019) at github.com/huggingface/transformers."
N,0.9943396226415094,"• DOE (McAllester & Stratos, 2020) at github.com/karlstratos/doe."
N,0.9962264150943396,"• SMILE (Song & Ermon, 2019) at github.com/ermongroup/smile-mi-estimator."
N,0.9981132075471698,"• InfoNCE, MINE, NWJ, CLUB (Cheng et al., 2020a) at github.com/Linear95/CLUB."
