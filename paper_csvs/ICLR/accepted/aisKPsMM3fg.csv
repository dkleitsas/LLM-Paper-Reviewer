Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00205761316872428,"Stochastic dual dynamic programming (SDDP) is a state-of-the-art method for
solving multi-stage stochastic optimization, widely used for modeling real-world
process optimization tasks. Unfortunately, SDDP has a worst-case complexity that
scales exponentially in the number of decision variables, which severely limits
applicability to only low dimensional problems. To overcome this limitation, we
extend SDDP by introducing a trainable neural model that learns to map problem
instances to a piece-wise linear value function within intrinsic low-dimension
space, which is architected speciÔ¨Åcally to interact with a base SDDP solver, so that
can accelerate optimization performance on new instances. The proposed Neural
Stochastic Dual Dynamic Programming (ŒΩ-SDDP) continually self-improves by
solving successive problems. An empirical investigation demonstrates that ŒΩ-SDDP
can signiÔ¨Åcantly reduce problem solving cost without sacriÔ¨Åcing solution quality
over competitors such as SDDP and reinforcement learning algorithms, across a
range of synthetic and real-world process optimization problems."
INTRODUCTION,0.00411522633744856,"1
INTRODUCTION"
INTRODUCTION,0.006172839506172839,"Multi-stage stochastic optimization (MSSO) considers the problem of optimizing a sequence of
decisions over a Ô¨Ånite number of stages in the presence of stochastic observations, minimizing
an expected cost while ensuring stage-wise action constraints are satisÔ¨Åed (Birge & Louveaux,
2011; Shapiro et al., 2014). Such a problem formulation captures a diversity of real-world process
optimization problems, such as asset allocation (Dantzig & Infanger, 1993), inventory control (Shapiro
et al., 2014; Nambiar et al., 2021), energy planning (Pereira & Pinto, 1991), and bio-chemical process
control (Bao et al., 2019), to name a few. Despite the importance and ubiquity of the problem, it has
proved challenging to develop algorithms that can cope with high-dimensional action spaces and
long-horizon problems (Shapiro & Nemirovski, 2005; Shapiro, 2006)."
INTRODUCTION,0.00823045267489712,"There have been a number of attempts to design scalable algorithms for MSSO, which generally
attempt to exploit scenarios-wise or stage-wise decompositions. An example of a scenario-wise
approach is Rockafellar & Wets (1991), which proposed a progressive hedging algorithm that
decomposes the sample averaged approximation of the problem into individual scenarios and applies
an augmented Lagrangian method to achieve consistency in a Ô¨Ånal solution. Unfortunately, the number
of subproblems and variables grows exponentially in the number of stages, known as the ‚Äúcurse-of-
horizon‚Äù. A similar proposal in Lan & Zhou (2020) considers a dynamic stochastic approximation,
a variant of stochastic gradient descent, but the computational cost also grows exponentially in the
number of stages. Alternatively, stochastic dual dynamic programming (SDDP) (Birge, 1985; Pereira
& Pinto, 1991), considers a stage-wise decomposition that breaks the curse of horizon (F¬®ullner &
Rebennack, 2021) and leads to an algorithm that is often considered state-of-the-art. The method
essentially applies an approximate cutting plane method that successively builds a piecewise linear
convex lower bound on the optimal cost-to-go function. Unfortunately, SDDP can require an
exponential number of iterations with respect to the number of decision variables (Lan, 2020), known
as the ‚Äúcurse-of-dimension‚Äù (Bal¬¥azs et al., 2015)."
INTRODUCTION,0.0102880658436214,"Beyond the scaling challenges, current approaches share a common shortcoming that they treat each
optimization problem independently. It is actually quite common to solve a family of problems that
share structure, which intuitively should allow the overall computational cost to be reduced (Khalil
et al., 2017; Chen et al., 2019). However, current methods, after solving each problem instance via"
INTRODUCTION,0.012345679012345678,‚àóEqual contribution
INTRODUCTION,0.01440329218106996,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01646090534979424,"intensive computation, discard all intermediate results, and tackle all new problems from scratch.
Such methods are destined to behave as a perpetual novice that never shows any improvement with
problem solving experience."
INTRODUCTION,0.018518518518518517,"In this paper, we present a meta-learning approach, Neural Stochastic Dual Dynamic Programming (ŒΩ-
SDDP), that, with problem solving experience, learns to signiÔ¨Åcantly improve the efÔ¨Åciency of SDDP
in high-dimensional and long-horizon problems. In particular, ŒΩ-SDDP exploits a specially designed
neural network architecture that produces outputs interacting directly and conveniently with a base
SDDP solver. The idea is to learn an operator that take information about the speciÔ¨Åc problem
instance, and map it to a piece-wise linear function in the intrinsic low-dimension space for accurate
value function approximation, so that can be plugged into a SDDP solver. The mapping is trainable,
leading to an overall algorithm that self-improves as it solves more problem instances. There are
three primary beneÔ¨Åts of the proposed approach with carefully designed components:"
INTRODUCTION,0.0205761316872428,"i) By adaptively generating a low-dimension projection for each problem instance, ŒΩ-SDDP reduces
the curse-of-dimension effect for SDDP.
ii) By producing a reasonable value function initialization given a description of the problem
instance, ŒΩ-SDDP is able to amortize its solution costs, and gain a signiÔ¨Åcant advantage over the
initialization in standard SDDP on the two benchmarks studied in the paper.
iii) By restricting value function approximations to a piece-wise afÔ¨Åne form, ŒΩ-SDDP can be
seamlessly incorporated into a base SDDP solver for further reÔ¨Åning solution, which allows
solution time to be reduced."
INTRODUCTION,0.02263374485596708,Figure 1 provides an illustration of the overall ŒΩ-SDDP method developed in this paper.
INTRODUCTION,0.024691358024691357,"Figure 1: Overall illustration of ŒΩ-SDDP. For training, the algorithm iterates N times to solve different
problem instances. For each instance, it repeats two passes: forward (solving LPs to estimate
an optimal action sequence) and backward (adding new afÔ¨Åne components to the value function
estimate). Once a problem instance is solved, the optimal value function and optimal actions are used
for neural network training. During inference time for a new problem, it can predict high-quality
value function with little cost, which can be embedded into SDDP for further improvements."
INTRODUCTION,0.026748971193415638,"The remainder of the paper is organized as follows. First, we provide the necessary background on
MSSO and SDDP in Section 2. Motivated by the difÔ¨Åculty of SDDP and shortcomings of existing
learning-based approaches, we then propose ŒΩ-SDDP in Section 3, with the design of the neural
component and the learning algorithm described in Section 3.1 and Section 3.2 respectively. We
compare the proposed approach with existing algorithms that also exploit supervised learning (SL)
and reinforcement learning (RL) for MSSO problems in Section 4. Finally, in Section 5 we conduct
an empirical comparison on synthetic and real-world problems and Ô¨Ånd that ŒΩ-SDDP is able to
effectively exploit successful problem solving experiences to greatly accelerate the planning process
while maintaining the quality of the solutions found."
PRELIMINARIES,0.02880658436213992,"2
PRELIMINARIES"
PRELIMINARIES,0.030864197530864196,"We begin by formalizing the multi-stage stochastic optimization problem (MSSO), and introducing
the stochastic dual dynamic programming (SDDP) strategy we exploit in the subsequent algorithmic
development. We emphasize the connection and differences between MSSO and a Markov decision
process (MDP), which shows the difÔ¨Åculties in applying the advanced RL methods for MSSO."
PRELIMINARIES,0.03292181069958848,"Multi-Stage Stochastic Optimization (MSSO).
Consider a multi-stage decision making problem
with stages t = 1, . . . , T, where an observation Œæt ‚àºPt(¬∑) is drawn at each stage from a known"
PRELIMINARIES,0.03497942386831276,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.037037037037037035,"observation distribution Pt. A full observation history {Œæt}T
t=1 forms a scenario, where the obser-
vations are assumed independent between stages. At stage t, an action is speciÔ¨Åed by a vector xt.
The goal is to choose a sequence of actions {xt}T
t=1 to minimize the overall expected sum of linear
costs PT
t=1 ct(Œæt)‚ä§xt under a known cost function ct. This is particularly challenging with feasible
constraints on the action set. Particularly, the feasible action set œát at stage t is given by
œát(xt‚àí1, Œæt)
:=
{xt|At (Œæt) xt = bt (Œæt) ‚àíBt‚àí1 (Œæt) xt‚àí1, xt ‚©æ0} ,
‚àÄt = 2, . . . , T, (1)
œá1 (Œæ1)
:=
{x1|A1(Œæ1)x1 = b1 (Œæ1) , x1 ‚©æ0} ,
(2)
where At, Bt and bt are known functions. Notably, the feasible set œát(xt‚àí1, Œæt) at stage t depends
on the previous action xt‚àí1 and the current stochastic observation Œæt. The MSSO problem can then
be expressed as v :="
PRELIMINARIES,0.03909465020576132,"(
minx1 c1(Œæ1)‚ä§x1 + EŒæ2
h
minx2 c2 (Œæ2)‚ä§x2¬∑ ¬∑ ¬∑ + EŒæT
h
minxT cT (ŒæT )‚ä§xT
ii
,"
PRELIMINARIES,0.0411522633744856,"s.t.
xt ‚ààœát(xt‚àí1, Œæt)
‚àÄt = {1, . . . , T} , (3)"
PRELIMINARIES,0.043209876543209874,"where Œæ1 and the problem context U = {ut}T
t=1 for ut := (Pt, ct, At, Bt, bt) are provided. Given the
context U given, the MSSO is speciÔ¨Åed. Since the elements in U are probability or functions, the
deÔ¨Ånition of U is only conceptual. In practice, we implement U with its sufÔ¨Åcient representations. We
will demonstrate the instantiation of U in our experiment section. MSSO is often used to formulate
real-world inventory control and portfolio management problems; we provide formulations for these
speciÔ¨Åc problems in in Appendix B.1 and Appendix B.2."
PRELIMINARIES,0.04526748971193416,"Similar to MDPs, value functions provide a useful concept for capturing the structure of the optimal
solution in terms of a temporal recurrence. Following the convention in the MSSO literature (F¬®ullner
& Rebennack, 2021), let
Qt (xt‚àí1, Œæt) :=
min
xt‚ààœát(xt‚àí1,Œæt) ct (Œæt)‚ä§xt + EŒæt+1 [Qt+1 (xt, Œæt+1)]
|
{z
}
Vt+1(xt)"
PRELIMINARIES,0.047325102880658436,",
‚àÄt = 2, . . . , T,
(4)"
PRELIMINARIES,0.04938271604938271,"which expresses a Bellman optimality condition over the feasible action set. Using this deÔ¨Ånition, the
MSSO problem (3) can then be rewritten as
v :=

minx1 c1(Œæ1)‚ä§x1 + V2 (x1) ,
s.t. x1 ‚ààœá1(Œæ1)
	
.
(5)"
PRELIMINARIES,0.051440329218107,"Theorem 1 (informal, Theorem 1.1 and Corollary 1.2 in F¬®ullner & Rebennack (2021)) When
the optimization (5) almost surely has a feasible solution for every realized scenario, the value
functions Qt (¬∑, Œæ) and Vt (¬∑) are piecewise linear and convex in xt for all t = 1, . . . , T."
PRELIMINARIES,0.053497942386831275,"Algorithm 1 SDDP(

V 0
t
	T
t=1 , Œæ1, n)"
PRELIMINARIES,0.05555555555555555,"1: Sample {Œæj
t }m
j=1 ‚àºPt (¬∑) for t = 2, . . . , T
2: for i = 1, . . . , n do
3:
Select J samples from uniform{1, ..., m}
‚ñ∑minibatch
4:
for t = 1, . . . , T and j = 1, . . . , J do
‚ñ∑forward pass
5:"
PRELIMINARIES,0.05761316872427984,"xi
tj ‚àà
arg min ct(Œæj
t )‚ä§xt + V i
t+1 (xt),
s.t. xtj ‚ààœát(xi
t‚àí1,j, Œæj
t )  (6)"
PRELIMINARIES,0.059670781893004114,"6:
end for
7:
for t = T, . . . , 1 do
‚ñ∑backward pass
8:
Calculate the dual variables of (6) for each Œæj
t in (29);
9:
Update V i
t with dual variables via (32) (Appendix C)
10:
end for
11: end for"
PRELIMINARIES,0.06172839506172839,"Stochastic Dual Dynamic Pro-
gramming (SDDP).
Given the
problem speciÔ¨Åcation (3) we
now consider solution strate-
gies. Stochastic dual dynamic
programming (SDDP) (Shapiro
et al., 2014; F¬®ullner & Reben-
nack, 2021) is a state-of-the-art
approach that exploits the key ob-
servation in Theorem 1 that the
optimal V function can be ex-
pressed as a maximum over a
Ô¨Ånite number of linear compo-
nents. Given this insight, SDDP
applies Bender‚Äôs decomposition
to the sample averaged approximation of (3). In particular, it performs two steps in each iteration: (i)
in a forward pass from t = 0, trial solutions for each stage are generated by solving subproblems (4)
using the current estimate of the future expected-cost-to-go function Vt+1; (ii) in a backward pass
from t = T, each of the V -functions are then updated by adding cutting planes derived from the
optimal actions xt‚àí1 obtained in the forward pass. (Details for the cutting plan derivation and the
connection to TD-learning are given in Appendix C due to lack of space.) After each iteration, the
current Vt+1 provides a lower bound on the true optimal expected-cost-to-go function, which is being
successively tightened; see Algorithm 1."
PRELIMINARIES,0.06378600823045268,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.06584362139917696,"MSSO vs. MDPs. At the Ô¨Årst glance, the dynamics in MSSO (3) is describing markovian relationship
on actions, i.e., the current action xt is determined by current state Œæt and previous action xt‚àí1, which
is different from the MDPs on markovian states. However, we can equivalently reformulate MSSO as"
PRELIMINARIES,0.06790123456790123,"a MDP with state-dependent feasible action set, by deÔ¨Åning the t-th step state as st := (xt‚àí1, Œæt),
and action as at
:=
xt
‚àà
œát (xt‚àí1, Œæt).
This indeed leads to the markovian transition
pset (st+1|st, xt) = 1 (xt ‚ààœát (xt‚àí1, Œæt)) pt+1 (Œæt+1), where 1 (xt ‚ààœát (xt‚àí1, Œæt)) := {1 if xt ‚àà
œát (xt‚àí1, Œæt) , 0 otherwise}."
PRELIMINARIES,0.06995884773662552,"Although we can represent MSSO equivalently in MDP, the MDP formulation introduces extra
difÔ¨Åculty in maintaining state-dependent feasible action and ignores the linear structure in feasibility,
which may lead to the inefÔ¨Åciency and infeasibility when applying RL algorithms (see Section 5).
Instead MSSO take these into consideration, especially the feasibility."
PRELIMINARIES,0.0720164609053498,"Unfortunately, MSSO and MDP comes from different communities, the notational conventions of the
MSSO versus MDP literature are directly contradictory: the Q-function in (4) corresponds to the
state-value V -function in the MDP literature, whereas the V -function in (4) is particular to the MSSO
setting, integrating out of randomness in state-value function, which has no standard correspondent
in the MDP literature. In this paper, we will adopt the notational convention of MSSO."
NEURAL STOCHASTIC DUAL DYNAMIC PROGRAMMING,0.07407407407407407,"3
NEURAL STOCHASTIC DUAL DYNAMIC PROGRAMMING
Although SDDP is a state-of-the-art approach that is widely deployed in practice, it does not scale
well in the dimensionality of the action space (Lan, 2020). That is, as the number of decision variables
in xt increases, the number of generated cutting planes in the Vt+1 approximations tends to grow
exponentially, which severely limits the size of problem instance that can be practically solved.
To overcome this limitation, we develop a new approach to scaling up SDDP by leveraging the
generalization ability of deep neural networks across different MSSO instances in this section."
NEURAL STOCHASTIC DUAL DYNAMIC PROGRAMMING,0.07613168724279835,"We Ô¨Årst formalize the learning task by introducing the contextual MSSO. SpeciÔ¨Åcally, as discussed
in Section 2, the problem context U = {ut}T
t=1 with ut := (Pt, ct, At, Bt, bt) soly deÔ¨Ånes the
MSSO problem, therefore, we denote W (U) as an instance of MSSO (3) with explicit dependence
on U. We assume the MSSO samples can be instantiated from contextual MSSO following some
distribution, i.e., W(U) ‚àºP (W), or equivalently, U ‚àºP (U). Then, instead of treating each MSSO
independently from scratch, we can learn to amortize and generalize the optimization across different
MSSOs in P (W). We develop a meta-learning strategy where a model is trained to map the ut
and t to a piecewise linear convex Vt-approximator that can be directly used to initialize the SDDP
solver in Algorithm 1. In principle, if optimal value information can be successfully transferred
between similar problem contexts, then the immense computation expended to recover the optimal Vt
functions for previous problem contexts can be leveraged to shortcut the nearly identical computation
of the optimal Vt functions for a novel but similar problem context. In fact, as we will demonstrate
below, such a transfer strategy proves to be remarkably effective."
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.07818930041152264,"3.1
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.08024691358024691,"To begin the speciÔ¨Åc development, we consider the structure of the value functions, which are desired
in the deep neural approximator."
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.0823045267489712,"‚Ä¢ Small approximation error and easy optimization over action: recall from Theorem 1 that the
optimal Vt-function must be a convex, piecewise linear function. Therefore, it is sufÔ¨Åcient for the
output representation from the deep neural model to express max-afÔ¨Åne function approximations
for Vt, which conveniently are also directly usable in the minimization (6) of the SDDP solver.
‚Ä¢ Encode the instance-dependent information: to ensure the learned neural mapping can account
for instance speciÔ¨Åc structure when transferring between tasks, the output representation needs to
encode the problem context information, {(Pt, ct, At, Bt, bt)}T
t=1.
‚Ä¢ Low-dimension representation of state and action: the complexity of subproblems in SDDP
depends on the dimension of the state and action exponentially, therefore, the output Vt-function
approximations should only depend on a low-dimension representation of x."
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.08436213991769548,"For the Ô¨Årst two requirements, we consider a deep neural representation for functions f (¬∑, ut) ‚ààMK
for t = 1, . . . , T, where MK is the piece-wise function class with K components, i.e.,"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.08641975308641975,"MK :=

œÜ (¬∑) : X ‚ÜíR
œÜ (x) =
max
k=1,...,K Œ≤‚ä§
k x + Œ±k, Œ≤k ‚ààRd, Œ±k ‚ààR

.
(7)"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.08847736625514403,Published as a conference paper at ICLR 2022
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.09053497942386832,"That is, such a function f takes the problem context information u as input, and outputs a set of
parameters ({Œ±k}, {Œ≤k}) that deÔ¨Åne a max-afÔ¨Åne function œÜ. We emphasize that although we
consider MK with Ô¨Åxed number of linear components, it is straightforward to generalize to the
function class with context-dependent number of components via introducing learnable K(ut) ‚ààN.
A key property of this output representation is that it always remains within the set of valid V -
functions, therefore, it can be naturally incorporated into SDDP as a warm start to reÔ¨Åne the solution.
This approach leaves design Ô¨Çexibility around the featurization of the problem context U and actions
x, while enabling the use of neural networks for f (¬∑, u), which can be trained end-to-end."
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.09259259259259259,"To further achieve a low-dimensional dependence on the action space while retaining convexity of the
value function representations for the third desideratum, we incorporate a linear projection x = Gy
with y ‚ààRp and p < d, such that G = œà (u) satisÔ¨Åes G‚ä§G = I. With this constraint, the mapping
f (¬∑, u) will be in:"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.09465020576131687,"MK
G :=

œÜ(¬∑) : Y ‚ÜíR
œÜG (y) =
max
k=1,...,K Œ≤‚ä§
k Gy + Œ±k, Œ≤k ‚ààRd, G ‚ààRd√óp, Œ±k ‚ààR

.
(8)"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.09670781893004116,"We postpone the learning of f and œà to Section 3.2 and Ô¨Årst illustrate the accelerated SDDP solver
in the learned effective dimension of the action space in Algorithm 2. Note that after we obtain the
solution y in line 3 in Algorithm 2 of the projected problem, we can recover x = Gy as a coarse
solution for fast inference. If one wanted a more reÔ¨Åned solution, the full SDDP could be run on the
un-projected instance starting from the updated algorithm state after the fast call."
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.09876543209876543,"Algorithm 2 Fast-Inference({ut}T
t=1 , f, œà, Œæ1)"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.10082304526748971,"1: Set G = œà (U)
‚ñ∑fast inference
2: Projected problem instance {qt}T
t=1 = {Gut}T
t=1, 3:"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.102880658436214,"n
Àúyt

Œæj
t
o"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.10493827160493827,"t,j =SDDP

{f (¬∑, qt)}T
t=1 , Œæ1, 1"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.10699588477366255,"
,
‚ñ∑we only"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.10905349794238683,need one forward pass in low-dimension space. 4:
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.1111111111111111,"n
Àúxt

Œæj
t
o"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.11316872427983539,"t,j =
n
GÀúyt

Œæj
t
o t,j,"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.11522633744855967,/* Optional reÔ¨Ånement */ 5:
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.11728395061728394,"n
x‚àó
t (Œæj
t )
o"
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.11934156378600823,"t,j = SDDP

{f (¬∑, ut)}T
t=1 , Œæ1, n

‚ñ∑reÔ¨Åne solution."
NEURAL ARCHITECTURE FOR MAPPING TO VALUE FUNCTION REPRESENTATIONS,0.12139917695473251,"Practical representation de-
tails:
In our implementation,
we Ô¨Årst encode the index of time
step t by a positional encoding
(Vaswani et al., 2017) and exploit
sufÔ¨Åcient statistics to encode the
distribution P(Œæ) (assuming in
addition that the Pt are station-
ary).
As the functions ct, At,
Bt and bt are typically static and
problem speciÔ¨Åc, there structures
will remain the same for differ-
ent Pt. In our paper we focus on
the generalization within a problem type (e.g., the inventory management) and do not expect the
generalization across problems (e.g., train on portfolio management and deploy on inventory manage-
ment). Hence we can safely ignore these LP speciÔ¨Åcations which are typically of high dimensions.
The full set of features are vectorized, concatenated and input to a 2-layer MLP with 512-hidden
relu neurons. The MLP outputs k linear components, {Œ±k} and {Œ≤k}, that form the piece-wise
linear convex approximation for Vt. For the projection G, we simply share one G across all tasks,
although it is not hard to incorporate a neural network parametrized G. The overall deep neural
architecture is illustrated in Figure 5 in Appendix D."
META SELF-IMPROVED LEARNING,0.12345679012345678,"3.2
META SELF-IMPROVED LEARNING
The above architecture will be trained using a meta-learning strategy, where we collect successful"
META SELF-IMPROVED LEARNING,0.12551440329218108,"prior experience Dn :=
n
zi :=

U, {V ‚àó
t }T
t=1 , {x‚àó
t (Œæj)}T,m
t=1,j
 i on"
META SELF-IMPROVED LEARNING,0.12757201646090535,i=1 by using SDDP to solve a set
META SELF-IMPROVED LEARNING,0.12962962962962962,"of training problem instances. Here, U = {ut}T
t=1 denotes a problem context, and {V ‚àó
t }T
t=1 and

x‚àó
tj
	T,m
t,j
are the optimal value functions and actions obtained by SDDP for each stage."
META SELF-IMPROVED LEARNING,0.13168724279835392,"Given the dataset Dn, the parameters of f and œà, W := {Wf, Wœà}, can be learned by optimizing
the following objective via stochastic gradient descent (SGD):"
META SELF-IMPROVED LEARNING,0.1337448559670782,"minW
X"
META SELF-IMPROVED LEARNING,0.13580246913580246,"z‚ààDn
‚Ñì(W; z) := n
X i=1 T
X t=1  ‚àí m
X"
META SELF-IMPROVED LEARNING,0.13786008230452676,"j
(xi‚àó
tj)‚ä§Gi
œà

Gi
œà
‚ä§
xi‚àó
tj + EMD

f(¬∑; ui
t), V i‚àó
t (¬∑)
!"
META SELF-IMPROVED LEARNING,0.13991769547325103,"+ ŒªœÉ (W) ,"
META SELF-IMPROVED LEARNING,0.1419753086419753,"s.t.

Gi
œà
‚ä§
Gi
œà = Ip,
‚àÄi = 1, . . . , n
(9)"
META SELF-IMPROVED LEARNING,0.1440329218106996,"where Gœà := œà(u), EMD (f, V ) denotes the Earth Mover‚Äôs Distance between f and V , and
œÉ (W) denotes a convex regularizer on W. Note that the loss function (9) is actually seeking"
META SELF-IMPROVED LEARNING,0.14609053497942387,Published as a conference paper at ICLR 2022
META SELF-IMPROVED LEARNING,0.14814814814814814,Algorithm 3 ŒΩ-SDDP
META SELF-IMPROVED LEARNING,0.15020576131687244,"1: Initialize dataset D0;
2: for epoch i = 1, . . . , n do
3:
Sample a multi-stage stochastic decision problems U = {ut}T
t=1 ‚àºP (U);"
META SELF-IMPROVED LEARNING,0.1522633744855967,"4:
Initial

V 0
t
	T
t=1 = (1 ‚àíŒ≥)0 + Œ≥
n
fW

¬∑, {ut}T
t=1
oT"
META SELF-IMPROVED LEARNING,0.15432098765432098,t=0 with Œ≥ ‚àºB (pi); 5:
META SELF-IMPROVED LEARNING,0.15637860082304528,"n
x‚àó(Œæj
t )
oT,m t,j=1"
META SELF-IMPROVED LEARNING,0.15843621399176955,"
= SDDP(

V 0
t
	T
t=1 , Œæ1, n);"
META SELF-IMPROVED LEARNING,0.16049382716049382,"6:
Collect solved optimization instance Di = Di‚àí1 ‚à™

U, {V ‚àó
t }T
t=1 , {x‚àó
t (Œæj)}T,m
t=1,j

;"
META SELF-IMPROVED LEARNING,0.16255144032921812,"7:
for iter = 1, . . . , b do
8:
Sample zl ‚àºDi;
9:
Update parameters W with stochastic gradients: W = W ‚àíŒ∑‚àáW ‚Ñì(W; zl) ;
10:
end for
11: end for"
META SELF-IMPROVED LEARNING,0.1646090534979424,"to maximize Pn
i=1
Pt
t=1
Pm
j (xi‚àó
tj)‚ä§GœàG‚ä§
œàxi‚àó
tj under orthonormality constraints, hence it seeks
principle components of the action spaces to achieve dimensionality reduction."
META SELF-IMPROVED LEARNING,0.16666666666666666,"To explain the role of EMD, recall that f(x, u) outputs a convex piecewise linear function represented
by

(Œ≤f
k)‚ä§x + Œ±f
k
	K
k=1, while the optimal value function V ‚àó
t (x) :=

(Œ≤‚àó
l )‚ä§x + Œ±‚àó
l (Œæ)
	t
l=1 in
SDDP is also a convex piecewise linear function, hence expressible by a maximum over afÔ¨Åne
functions. Therefore, EMD (f, V ‚àó
t ) is used to calculate the distance between the sets

Œ≤f
k, Œ±f
k
	K
k=1
and {Œ≤‚àó
l , Œ±‚àó
l }t
l=1, which can be recast as"
META SELF-IMPROVED LEARNING,0.16872427983539096,"min
M‚àà‚Ñ¶(K,t) ‚ü®M, D‚ü©,
‚Ñ¶(K, t) =

M ‚ààRK√ót
+
|M1 ‚©Ω1, M ‚ä§1 ‚©Ω1, 1‚ä§M1 = min(K, t)
	
, (10)"
META SELF-IMPROVED LEARNING,0.17078189300411523,"where D ‚ààRK√ót denotes the pairwise distances between elements of the two sets. Due to space
limits, please refer to Figure 6 in Appendix D for an illustration of the overall training setup. The
main reason we use EMD is due to the fact that f and V ‚àóare order invariant, and EMD provides an
optimal transport comparison (Peyr¬¥e et al., 2019) in terms of the minimal cost over all pairings.
Remark (Alternative losses): One could argue that it sufÔ¨Åces to use the vanilla regression losses,
such as the L2-square loss ‚à•f (¬∑, u, Œæ) ‚àíV ‚àó(¬∑, Œæ)‚à•2
2, to Ô¨Åt f to V ‚àó. However, there are several
drawbacks with such a direct approach. First, such a loss ignores the inherent structure of the
functions. Second, to calculate the loss, the observations x are required, and the optimal actions
from SDDP are not sufÔ¨Åcient to achieve a robust solution. This approach would require an additional
sampling strategy that is not clear how to design (Defourny et al., 2012).
Training algorithm: The loss (9) pushes f to approximate the optimal value functions for the
training contexts, while also pushing the subspace Gœà to acquire principle components in the action
space. The intent is to achieve an effective approximator for the value function in a low-dimensional
space that can be used to warm-start SDDP inference Algorithm 2. Ideally, this should result in an
efÔ¨Åcient optimization procedure with fewer optimization variables that can solve a problem instance
with fewer forward-backward passes. In an on-line deployment, the learned components, f and œà,
can be continually improved from the results of previous solves. One can also optinally exploit the
learned component for the initialization of value function in SDDP by annealing with a mixture of
zero function, where the weight is sampled from a Bernolli distribution. Overall, this leads to the
Meta Self-Improved SDDP algorithm, ŒΩ-SDDP, shown in Algorithm 3.
4
RELATED WORK
The importance of MSSO and the inherent difÔ¨Åculty of solving MSSO problems at a practical scale
has motivated research on hand-designed approximation algorithms, as discussed in Appendix A.
Learning-based MSSO approximations have attracted more attentions. Rachev & R¬®omisch (2002);
H√∏yland et al. (2003); Hochreiter & PÔ¨Çug (2007) learn a sampler for generating a small scenario tree
while preserving statistical properties. Recent advances in RL is also exploited. Defourny et al. (2012)
imitate a parametrized policy that maps from scenarios to actions from some SDDP solvers. Direct
policy improvement from RL have also been considered. Ban & Rudin (2019) parametrize a policy
as a linear model in (25), but introducing large approximation errors. As an extension, Bertsimas
& Kallus (2020); Oroojlooyjadid et al. (2020) consider more complex function approximators for
the policy parameterization in (25). Oroojlooyjadid et al. (2021); Hubbs et al. (2020); Balaji et al."
META SELF-IMPROVED LEARNING,0.1728395061728395,Published as a conference paper at ICLR 2022
META SELF-IMPROVED LEARNING,0.1748971193415638,"(2019); Barat et al. (2019) directly apply deep RL methods. Avila et al. (2021) exploit off-policy
RL tricks for accelerating the SDDP Q-update. More detailed discussion about Avila et al. (2021)
can be found in Appendix A. Overall, the majority of methods are not able to easily balance MSSO
problem structures and Ô¨Çexibility while maintaining strict feasibility with efÔ¨Åcient computation. They
also tend to focus on learning a policy for a single problem, which does not necessarily guarantee
effective generalization to new cases, as we Ô¨Ånd in the empirical evaluation.
Context-based meta-RL is also relevant, where the context-dependent policy (Hausman et al., 2018;
Rakelly et al., 2019; Lan et al., 2019) or context-dependent value function (Fakoor et al., 2019;
Arnekvist et al., 2019; Raileanu et al., 2020) is introduced. Besides the difference in MSSO vs.
MDP in Section 2, the most signiÔ¨Åcant difference is the parameterization and inference usage of
context-dependent component. In ŒΩ-SDDP, we design the speciÔ¨Åc neural architecture with the output
as a piece-wise linear function, which takes the structure of MSSO into account and can be seamlessly
integrated with SDDP solvers for further solution reÔ¨Ånement with the feasibility guaranteed; while in
the vanilla context-based meta-RL methods, the context-dependent component with arbitrary neural
architectures, which will induce extra approximation error, and is unable to handle the constraints.
Meanwhile, the design of the neural component in ŒΩ-SDDP also leads to our particular learning
objective and stochastic algorithm, which exploits the inherent piece-wise linear structure of the
functions, meanwhile bypasses the additional sampling strategy required for alternatives."
EXPERIMENTS,0.17695473251028807,"5
EXPERIMENTS"
EXPERIMENTS,0.17901234567901234,"Problem Setting
ConÔ¨Åguration (S-I-C, T )"
EXPERIMENTS,0.18106995884773663,"Small-size topology, Short horizon (Sml-Sht)
2-2-4, 5
Mid-size topology, Long horizon (Mid-Lng)
10-10-20, 10
Portfolio Optimization
T = 5"
EXPERIMENTS,0.1831275720164609,Table 1: Problem ConÔ¨Åguration in Inventory Optimization.
EXPERIMENTS,0.18518518518518517,"Problem DeÔ¨Ånition.
We Ô¨Årst tested on
inventory optimization with the prob-
lem conÔ¨Åguration in Table. 1. We break
the problem contexts into two sets: 1)
topology, parameterized via the number
of suppliers S, inventories I, and cus-
tomers C; 2) decision horizon T. Note
that in the Mid-Lng setting there are 310 continuous action variables, which is of magnitudes larger
than the ones used in inventory control literature (Graves & Willems, 2008) and benchmarks, e.g.,
ORL (Balaji et al., 2019) or meta-RL (Rakelly et al., 2019) on MuJoCo (Todorov et al., 2012)."
EXPERIMENTS,0.18724279835390947,"Within each problem setting, a problem instance is further captured by the problem context. In
inventory optimization, a forecast model is usually used to produce continuous demand forecasts
and requires re-optimization of the inventory decisions based on the new distribution of the demand
forecast, forming a group of closely related problem instances. We treat the parameters of the demand
forecast as the primary problem context. In the experiment, demand forecasts are synthetically
generated from a normal distribution: dt ‚àºN(¬µd, œÉd). For both problem settings, the mean and
the standard deviation of the demand distribution are sampled from the meta uniform distributions:
¬µd ‚àºU(11, 20), œÉd ‚àºU(0, 5). Transport costs from inventories to customers are also subject to
frequent changes. We model it via a normal distribution: ct ‚àºN(¬µc, œÉc) and use the distribution
mean ¬µc ‚àºU(0.3, 0.7) as the secondary problem context parameter with Ô¨Åxed œÉc = 0.2. Thus in
this case, the context for each problem instance that f(¬∑, ¬µt) needs to care about is ut = (¬µd, œÉd, ¬µc)."
EXPERIMENTS,0.18930041152263374,"The second environment is portfolio optimization. A forecast model is ususally used to produce
updated stock price forcasts and requires re-optimization of asset allocation decisions based on the
new distribution of the price forecast, forming a group of closely related problem instances. We use
an autoregressive process of order 2 to learn the price forecast model based on the real daily stock
prices in the past 5 years. The last two-day historical prices are used as problem context parameters
in our experiments. In this case the stock prices of Ô¨Årst two days are served as the context U for f."
EXPERIMENTS,0.19135802469135801,"Due to the space limitation, we postpone the detailed description of problems and additional perfor-
mances comparison in Appendix E."
EXPERIMENTS,0.1934156378600823,"Baselines.
In the following experiments, we compare ŒΩ-SDDP with mainstream methodologies:"
EXPERIMENTS,0.19547325102880658,"‚Ä¢ SDDP-optimal: This is the SDDP solver that runs on each test problem instance until convergence,
and is expected to produce the best solution and serve as the ground-truth for comparison.
‚Ä¢ SDDP-mean: It is trained once based on the mean values of the problem parameter distribution,
and the resulting V -function will be applied in all different test problem instances as a surrogate of
the true V -function. This approach enjoys the fast runtime time during inference, but would yield
suboptimal results as it cannot adapt to the change of the problem contexts."
EXPERIMENTS,0.19753086419753085,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.19958847736625515,"Task
Parameter Domain
SDDP-mean
ŒΩ-SDDP-fast
ŒΩ-SDDP-accurate
Best RL"
EXPERIMENTS,0.20164609053497942,Sml-Sht
EXPERIMENTS,0.2037037037037037,"demand mean (¬µd)
16.15 ¬± 18.61%
2.42 ¬± 1.84%
1.32 ¬± 1.13%
38.42 ¬± 17.78%"
EXPERIMENTS,0.205761316872428,"joint (¬µd & œÉd)
20.93 ¬± 22.31%
4.77 ¬± 3.80%
1.81 ¬± 2.19%
33.08 ¬± 8.05%"
EXPERIMENTS,0.20781893004115226,Mid-Long
EXPERIMENTS,0.20987654320987653,"demand mean (¬µd)
24.77 ¬± 27.04%
2.90 ¬± 1.11%
1.51 ¬± 1.08%
17.81 ¬± 10.26%"
EXPERIMENTS,0.21193415637860083,"joint (¬µd & œÉd)
27.02 ¬± 29.04%
5.16 ¬± 3.22%
3.32 ¬± 3.06%
50.19 ¬± 5.57%"
EXPERIMENTS,0.2139917695473251,"joint (¬µd & œÉd & ¬µc)
29.99 ¬± 32.33%
7.05 ¬± 3.60%
3.29 ¬± 3.23%
135.78 ¬± 17.12%"
EXPERIMENTS,0.21604938271604937,Table 2: Average Error Ratio of Objective Value.
EXPERIMENTS,0.21810699588477367,"Task
Parameter Domain
SDDP-optimal
SDDP-mean
ŒΩ-SDDP-fast
ŒΩ-SDDP-accurate
Best RL"
EXPERIMENTS,0.22016460905349794,Sml-Sht
EXPERIMENTS,0.2222222222222222,"demand mean (¬µd)
6.80 ¬± 7.45
14.83 ¬± 17.90
9.60 ¬± 3.35
10.12 ¬± 4.03
3.90¬± 8.39"
EXPERIMENTS,0.2242798353909465,"joint (¬µd & œÉd)
10.79 ¬± 19.75
19.83 ¬± 22.02
11.04 ¬± 10.83
13.73 ¬± 16.64
1.183¬± 4.251"
EXPERIMENTS,0.22633744855967078,Mid-Long
EXPERIMENTS,0.22839506172839505,"demand mean (¬µd)
51.96 ¬± 14.90
73.39 ¬± 59.90
44.27 ¬± 9.00
33.42 ¬± 18.01
1.98¬± 2.65"
EXPERIMENTS,0.23045267489711935,"joint (¬µd & œÉd)
54.89 ¬± 32.35
85.76 ¬± 77.62
45.53 ¬± 24.14
36.31 ¬± 20.49
205.51 ¬± 150.90"
EXPERIMENTS,0.23251028806584362,"joint (¬µd & œÉd & ¬µc)
55.14 ¬± 38.93
86.26 ¬± 81.14
44.80 ¬± 28.57
36.19 ¬± 20.08
563.19 ¬± 114.03"
EXPERIMENTS,0.2345679012345679,"Table 3: Objective Value Variance.
‚Ä¢ Model-free RL algorithms: Four RL algorithms, including DQN, DDPG, SAC, PPO, are directly
trained online on the test instances without the budget limit of number of samples. So this setup
has more privileges compared to typical meta-RL settings. We only report the best RL result
in Table 2 and Table 3 due to the space limit. Detailed hyperparameter tuning along with the other
performance results are reported in Appendix E.
‚Ä¢ ŒΩ-SDDP-fast: This is our algorithm where the the meta-trained neural-based V -function is directly
evaluated on each problem instance, which corresponds to Algorithm 2 without the last reÔ¨Ånement
step. In this case, only one forward pass of SDDP using the neural network predicted V -function is
needed and the V -function will not be updated. The only overhead compared to SDDP-mean is the
feed-forward time of neural network, which can be ignored compared to the expensive LP solving.
‚Ä¢ ŒΩ-SDDP-accurate: It is our full algorithm presented in Algorithm 2 where the meta-trained
neural-based V -function is further reÔ¨Åned with 10 more iterations of vanilla SDDP algorithm."
SOLUTION QUALITY COMPARISON,0.2366255144032922,"5.1
SOLUTION QUALITY COMPARISON"
SOLUTION QUALITY COMPARISON,0.23868312757201646,"For each new problem instance, we evaluate the algorithm performance by solving and evaluating
the optimization objective value using the trained V -function model over 50 randomly sampled
trajectories. We record the mean(candidate) and the standard derivation of these objective values
produced by each candidate method outlined above. As SDDP-optimal is expected to produce the
best solution, we use its mean on each problem instance to normalize the difference in solution quality.
SpeciÔ¨Åcally, error ratio of method candidate with respect to SDDP-optimal is:"
SOLUTION QUALITY COMPARISON,0.24074074074074073,œÜ = mean(candidate)‚àímean(SDDP-optimal)
SOLUTION QUALITY COMPARISON,0.24279835390946503,"abs{mean(SDDP-optimal)}
(11)"
SOLUTION QUALITY COMPARISON,0.2448559670781893,"Inventory optimization: We report the average optimalty ratio of each method on the held-out
test problem set with 100 instances in Table 2. By comparison, ŒΩ-SDDP learns to adaptive to each
problem instance, and thus is able to outperform these baselines by a signiÔ¨Åcantly large margin. Also
we show that by tuning the SDDP with the V -function initialized with the neural network generated
cutting planes for just 10 more steps, we can further boost the performance (ŒΩ-SDDP-accurate). In
addition, despite the recent reported promising results in applying deep RL algorithms in small-scale
inventory optimization problems (Bertsimas & Kallus, 2020; Oroojlooyjadid et al., 2020; 2021;
Hubbs et al., 2020; Balaji et al., 2019; Barat et al., 2019), it seems that these algorithms get worse
results than SDDP and ŒΩ-SDDP variants when the problem size increases."
SOLUTION QUALITY COMPARISON,0.24691358024691357,"We further report the average variance along with its standard deviation of different methods in Table 3.
We Ô¨Ånd that generally our proposed ŒΩ-SDDP (both fast and accurate variants) can yield solutions
with comparable variance compared to SDDP-optimal. SDDP-mean gets higher variance, as its
performance purely depends on how close the sampled problem parameters are to their means."
SOLUTION QUALITY COMPARISON,0.24897119341563786,"Portfolio optimization: We evaluated the same metrics as above. We train a multi-dimensional
second-order autoregressive model for the selected US stocks over last 5 years as the price forecast
model, and use either synthetic (low) or estimated (high) variance of the price to test different models.
When the variance is high, the best policy found by SDDP-optimal is to buy (with appropriate but
different asset allocations at different days) and hold for each problem instance. We found our"
SOLUTION QUALITY COMPARISON,0.25102880658436216,Published as a conference paper at ICLR 2022
SOLUTION QUALITY COMPARISON,0.25308641975308643,"10
1
100
Time (s) 0.0 0.2 0.4 0.6 0.8"
SOLUTION QUALITY COMPARISON,0.2551440329218107,Relative Error
SOLUTION QUALITY COMPARISON,0.257201646090535,"SDDP-2
SDDP-4
SDDP-8
SDDP-16
SDDP-32
SDDP-mean"
SOLUTION QUALITY COMPARISON,0.25925925925925924,-SDDP-fast
SOLUTION QUALITY COMPARISON,0.2613168724279835,"100
101
Time (s) 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
SOLUTION QUALITY COMPARISON,0.26337448559670784,Relative Error
SOLUTION QUALITY COMPARISON,0.2654320987654321,"SDDP-2
SDDP-4
SDDP-8
SDDP-16
SDDP-32
SDDP-mean"
SOLUTION QUALITY COMPARISON,0.2674897119341564,-SDDP-fast
SOLUTION QUALITY COMPARISON,0.26954732510288065,"0.0
2.5
5.0
7.5 10.0 12.5 15.0 17.5 20.0"
SOLUTION QUALITY COMPARISON,0.2716049382716049,Time (s) 101 102
SOLUTION QUALITY COMPARISON,0.2736625514403292,Error ratio/%
SOLUTION QUALITY COMPARISON,0.2757201646090535,"-SDDP-accurate
-SDDP-fast SDDP"
SOLUTION QUALITY COMPARISON,0.2777777777777778,"Sml-Sht-joint (¬µd & œÉd)
Mid-Lng-joint (¬µd & œÉd & ¬µc) Mid-Lng-joint (¬µd & œÉd & ¬µc)"
SOLUTION QUALITY COMPARISON,0.27983539094650206,"Figure 2: Time-solution trade-off. In the left two plots, each dot represents a problem instance with
the runtime and the solution quality obtained by corresponding algorithm. The right most plot shows
how ŒΩ-SDDP-accurate improves further when integrated into SDDP solver."
SOLUTION QUALITY COMPARISON,0.28189300411522633,"0
10
20
30
40
50
# generated cutting planes 0 200 400 600 800 1000"
SOLUTION QUALITY COMPARISON,0.2839506172839506,Error ratio/%
SOLUTION QUALITY COMPARISON,0.28600823045267487,"0
20
40
60
80
100
# generated cutting planes 0 25 50 75 100 125 150 175 200"
SOLUTION QUALITY COMPARISON,0.2880658436213992,Error ratio/%
SOLUTION QUALITY COMPARISON,0.29012345679012347,"Sml-Sht-joint (¬µd & œÉd)
Mid-Lng-joint (¬µd & œÉd & ¬µc)"
SOLUTION QUALITY COMPARISON,0.29218106995884774,Figure 3: ŒΩ-SDDP-fast with different # generated cutting planes.
SOLUTION QUALITY COMPARISON,0.294238683127572,"50
100
150
200
250
300
Dimension of generated cutting planes 0 10 20 30 40 50 60"
SOLUTION QUALITY COMPARISON,0.2962962962962963,Error ratio/%
SOLUTION QUALITY COMPARISON,0.29835390946502055,-SDDP-fast
SOLUTION QUALITY COMPARISON,0.3004115226337449,SDDP-mean
SOLUTION QUALITY COMPARISON,0.30246913580246915,"Figure 4:
Performance of ŒΩ-
SDDP with low-rank projection.
ŒΩ-SDDP is able to rediscover this policy; when the variance is low, our model is also able to achieve
much lower error ratio than SDDP-mean. We provide study details in Appendix E.2."
TRADE-OFF BETWEEN RUNNING TIME AND ALGORITHM PERFORMANCE,0.3045267489711934,"5.2
TRADE-OFF BETWEEN RUNNING TIME AND ALGORITHM PERFORMANCE
We study the trade-off between the runtime and the obtained solution quality in Figure 2 based on the
problem instances in the test problem set. In addition to ŒΩ-SDDP-fast and SDDP-mean, we plot the
solution quality and its runtime obtained after different number of iterations of SDDP (denoted as
SDDP-n with n iterations). We observe that for the small-scale problem domain, SDDP-mean runs
the fastest but with very high variance over the performance. For the large-scale problem domain,
ŒΩ-SDDP-fast achieves almost the same runtime as SDDP-mean (which roughly equals to the time
for one round of SDDP forward pass). Also for large instances, SDDP would need to spend 1 or 2
magnitudes of runtime to match the performance of ŒΩ-SDDP-fast. If we leverage ŒΩ-SDDP-accurate to
further update the solution in each test problem instance for just 10 iterations, we can further improve
the solution quality. This suggests that our proposed ŒΩ-SDDP achieves better time-solution trade-offs."
STUDY OF NUMBER OF GENERATED CUTTING PLANES,0.3065843621399177,"5.3
STUDY OF NUMBER OF GENERATED CUTTING PLANES
In Figure 3 we show the performance of ŒΩ-SDDP-fast with respect to different model capacities,
captured by the number of cutting planes the neural network can generate. A general trend indicates
that more generated cutting planes would yield better solution quality. One exception lies in the
Mid-Lng setting, where increasing the number of cutting planes beyond 64 would yield worse results.
As we use the cutting planes generated by last n iterations of SDDP solving in training ŒΩ-SDDP-fast,
our hypothesis is that the cutting planes generated by SDDP during the early stages in large problem
settings would be of high variance and low-quality, which in turn provides noisy supervision. A more
careful cutting plane pruning during the supervised learning stage would help resolve the problem."
LOW-DIMENSION PROJECT PERFORMANCE,0.30864197530864196,"5.4
LOW-DIMENSION PROJECT PERFORMANCE
Finally in Figure 4 we show the performance using low-rank projection. We believe that in reality
customers from the same cluster (e.g., region/job based) would express similar behaviors, thus we
created another synthetic environment where the customers form 4 clusters with equal size and
thus have the same demand/transportation cost within each cluster. We can see that as long as the
dimension goes above 80, our approach can automatically learn the low-dimension structure, and
achieve much better performance than the baseline SDDP-mean. Given that the original decision
problem is in 310-dimensional space, we expect having 310/4 dimensions would be enough, where
the experimental results veriÔ¨Åed our hypothesis. We also show the low-dimension projection results
for the problems with full-rank structure in Appendix E.1."
LOW-DIMENSION PROJECT PERFORMANCE,0.31069958847736623,Published as a conference paper at ICLR 2022
LOW-DIMENSION PROJECT PERFORMANCE,0.31275720164609055,ACKNOWLEDGMENTS
LOW-DIMENSION PROJECT PERFORMANCE,0.3148148148148148,"The authors would like to thank Sherry Yang, Bethany Wang, Ben Sprecher and others from Cloud
AI optimization, and the anonymous reviewers for their valuable feedbacks."
REFERENCES,0.3168724279835391,REFERENCES
REFERENCES,0.31893004115226337,"Isac Arnekvist, Danica Kragic, and Johannes A Stork. Vpe: Variational policy embedding for transfer
reinforcement learning. In 2019 International Conference on Robotics and Automation (ICRA), pp.
36‚Äì42. IEEE, 2019."
REFERENCES,0.32098765432098764,"Daniel Avila, Anthony Papavasiliou, and Nils L¬®ohndorf. Batch learning in stochastic dual dynamic
programming. submitted, 2021."
REFERENCES,0.3230452674897119,"Bharathan Balaji, Jordan Bell-Masterson, Enes Bilgin, Andreas Damianou, Pablo Moreno Garcia,
Arpit Jain, Runfei Luo, Alvaro Maggiar, Balakrishnan Narayanaswamy, and Chun Ye. Orl:
Reinforcement learning benchmarks for online stochastic optimization problems. arXiv preprint
arXiv:1911.10641, 2019."
REFERENCES,0.32510288065843623,"G. Bal¬¥azs, A. Gy¬®orgy, and Cs. Szepesv¬¥ari. Near-optimal max-afÔ¨Åne estimators for convex regression.
In AISTATS, pp. 56‚Äì64, 2015."
REFERENCES,0.3271604938271605,"Gah-Yi Ban and Cynthia Rudin. The big data newsvendor: Practical insights from machine learning.
Operations Research, 67(1):90‚Äì108, 2019."
REFERENCES,0.3292181069958848,"Hanxi Bao, Zhiqiang Zhou, Georgios Kotsalis, Guanghui Lan, and Zhaohui Tong. Lignin valorization
process control under feedstock uncertainty through a dynamic stochastic programming approach.
Reaction Chemistry & Engineering, 4(10):1740‚Äì1747, 2019."
REFERENCES,0.33127572016460904,"Souvik Barat, Harshad Khadilkar, Hardik Meisheri, Vinay Kulkarni, Vinita Baniwal, Prashant Kumar,
and Monika Gajrani. Actor based simulation for closed loop control of supply chain using
reinforcement learning. In Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems, pp. 1802‚Äì1804, 2019."
REFERENCES,0.3333333333333333,"Gilles Bareilles, Yassine Laguel, Dmitry Grishchenko, Franck Iutzeler, and J¬¥erÀÜome Malick. Random-
ized progressive hedging methods for multi-stage stochastic programming. Annals of Operations
Research, 295(2):535‚Äì560, 2020."
REFERENCES,0.33539094650205764,"Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1 edition,
1957."
REFERENCES,0.3374485596707819,"Dimitri P Bertsekas. Dynamic Programming and Optimal Control, Two Volume Set. Athena ScientiÔ¨Åc,
2001."
REFERENCES,0.3395061728395062,"Dimitris Bertsimas and Nathan Kallus. From predictive to prescriptive analytics. Management
Science, 66(3):1025‚Äì1044, 2020."
REFERENCES,0.34156378600823045,"J. Birge. The value of the stochastic solution in stochastic linear programs with Ô¨Åxed recourse.
Mathematical Programming, 24:314‚Äì325, 1982."
REFERENCES,0.3436213991769547,"John R Birge. Decomposition and partitioning methods for multistage stochastic linear programs.
Operations research, 33(5):989‚Äì1007, 1985."
REFERENCES,0.345679012345679,"John R Birge and Francois Louveaux. Introduction to stochastic programming. Springer Science &
Business Media, 2011."
REFERENCES,0.3477366255144033,"Binghong Chen, Bo Dai, Qinjie Lin, Guo Ye, Han Liu, and Le Song. Learning to plan in high
dimensions via neural exploration-exploitation trees. In International Conference on Learning
Representations, 2019."
REFERENCES,0.3497942386831276,"George B Dantzig and Gerd Infanger. Multi-stage stochastic linear programs for portfolio optimization.
Annals of Operations Research, 45(1):59‚Äì76, 1993."
REFERENCES,0.35185185185185186,Published as a conference paper at ICLR 2022
REFERENCES,0.35390946502057613,"Boris Defourny, Damien Ernst, and Louis Wehenkel. Multistage stochastic programming: A scenario
tree based approach to planning under uncertainty. In Decision theory models for applications in
artiÔ¨Åcial intelligence: concepts and solutions, pp. 97‚Äì143. IGI Global, 2012."
REFERENCES,0.3559670781893004,"Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J Smola. Meta-q-learning. arXiv
preprint arXiv:1910.00125, 2019."
REFERENCES,0.35802469135802467,Christian F¬®ullner and Steffen Rebennack. Stochastic dual dynamic programming - a review. 2021.
REFERENCES,0.360082304526749,"Stephen C Graves and Sean P Willems. Strategic inventory placement in supply chains: Nonstationary
demand. Manufacturing & service operations management, 10(2):278‚Äì287, 2008."
REFERENCES,0.36213991769547327,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning, pp. 1861‚Äì1870. PMLR, 2018."
REFERENCES,0.36419753086419754,"Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller.
Learning an embedding space for transferable robot skills.
In International Conference on
Learning Representations, 2018."
REFERENCES,0.3662551440329218,"Pascal Van Hentenryck and Russell Bent. Online stochastic combinatorial optimization. The MIT
Press, 2006."
REFERENCES,0.3683127572016461,"Ronald Hochreiter and Georg Ch PÔ¨Çug. Financial scenario generation for stochastic multi-stage
decision processes as facility location problems. Annals of Operations Research, 152(1):257‚Äì272,
2007."
REFERENCES,0.37037037037037035,"Kjetil H√∏yland, Michal Kaut, and Stein W Wallace. A heuristic for moment-matching scenario
generation. Computational optimization and applications, 24(2):169‚Äì185, 2003."
REFERENCES,0.3724279835390947,"Kai Huang and Shabbir Ahmed. The value of multistage stochastic programming in capacity planning
under uncertainty. Operations Research, 57(4):893‚Äì904, 2009."
REFERENCES,0.37448559670781895,"Christian D Hubbs, Hector D Perez, Owais Sarwar, Nikolaos V Sahinidis, Ignacio E Grossmann,
and John M Wassick. Or-gym: A reinforcement learning library for operations research problem.
arXiv preprint arXiv:2008.06319, 2020."
REFERENCES,0.3765432098765432,"Elias B Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial
optimization algorithms over graphs. In NIPS, 2017."
REFERENCES,0.3786008230452675,"K. Kim, M. O. Franz, and B. Sch¬®olkopf. Iterative kernel principal component analysis for image
modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(9):1351‚Äì1366,
2005."
REFERENCES,0.38065843621399176,"Guanghui Lan. Complexity of stochastic dual dynamic programming. Mathematical Programming,
pp. 1‚Äì38, 2020."
REFERENCES,0.38271604938271603,"Guanghui Lan and Zhiqiang Zhou. Dynamic stochastic approximation for multi-stage stochastic
optimization. Mathematical Programming, pp. 1‚Äì46, 2020."
REFERENCES,0.38477366255144035,"Lin Lan, Zhenguo Li, Xiaohong Guan, and Pinghui Wang. Meta reinforcement learning with task
embedding and shared policy. arXiv preprint arXiv:1905.06527, 2019."
REFERENCES,0.3868312757201646,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR,
2016."
REFERENCES,0.3888888888888889,"RE Mahony, U Helmke, and JB Moore. Gradient algorithms for principal component analysis. The
ANZIAM Journal, 37(4):430‚Äì450, 1996."
REFERENCES,0.39094650205761317,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.39300411522633744,Published as a conference paper at ICLR 2022
REFERENCES,0.3950617283950617,"Mila Nambiar, David Simchi-Levi, and He Wang. Dynamic inventory allocation with demand
learning for seasonal goods. Production and Operations Management, 30(3):750‚Äì765, 2021."
REFERENCES,0.39711934156378603,"Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006."
REFERENCES,0.3991769547325103,"Afshin Oroojlooyjadid, Lawrence V Snyder, and Martin Tak¬¥aÀác. Applying deep learning to the
newsvendor problem. IISE Transactions, 52(4):444‚Äì463, 2020."
REFERENCES,0.4012345679012346,"Afshin Oroojlooyjadid, MohammadReza Nazari, Lawrence V Snyder, and Martin Tak¬¥aÀác. A deep q-
network for the beer game: Deep reinforcement learning for inventory optimization. Manufacturing
& Service Operations Management, 2021."
REFERENCES,0.40329218106995884,"Mario V. F. Pereira and Leontina M. V. G. Pinto. Multi-stage stochastic optimization applied to
energy planning. Mathematical Programming, 52(2):359‚Äì375, 1991."
REFERENCES,0.4053497942386831,"Gabriel Peyr¬¥e, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends¬Æ in Machine Learning, 11(5-6):355‚Äì607, 2019."
REFERENCES,0.4074074074074074,"Svetlozar T Rachev and Werner R¬®omisch. Quantitative stability in stochastic programming: The
method of probability metrics. Mathematics of Operations Research, 27(4):792‚Äì818, 2002."
REFERENCES,0.4094650205761317,"Roberta Raileanu, Max Goldstein, Arthur Szlam, and Rob Fergus. Fast adaptation to new environ-
ments via policy-dynamics value functions. In International Conference on Machine Learning, pp.
7920‚Äì7931. PMLR, 2020."
REFERENCES,0.411522633744856,"Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. EfÔ¨Åcient off-policy
meta-reinforcement learning via probabilistic context variables. In International conference on
machine learning, pp. 5331‚Äì5340. PMLR, 2019."
REFERENCES,0.41358024691358025,"R Tyrrell Rockafellar and Roger J-B Wets. Scenarios and policy aggregation in optimization under
uncertainty. Mathematics of operations research, 16(1):119‚Äì147, 1991."
REFERENCES,0.4156378600823045,"T. D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward network. Neural
Networks, 2:459‚Äì473, 1989."
REFERENCES,0.4176954732510288,"John Schulman. Optimizing expectations: From deep reinforcement learning to stochastic computa-
tion graphs. PhD thesis, UC Berkeley, 2016."
REFERENCES,0.41975308641975306,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.4218106995884774,"Alexander Shapiro. On complexity of multistage stochastic programs. Operations Research Letters,
34(1):1‚Äì8, 2006."
REFERENCES,0.42386831275720166,"Alexander Shapiro and Arkadi Nemirovski. On complexity of stochastic programming problems. In
Continuous optimization, pp. 111‚Äì146. Springer, 2005."
REFERENCES,0.42592592592592593,"Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy¬¥nski. Lectures on stochastic program-
ming: modeling and theory. SIAM, 2014."
REFERENCES,0.4279835390946502,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.43004115226337447,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026‚Äì5033.
IEEE, 2012."
REFERENCES,0.43209876543209874,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017."
REFERENCES,0.43415637860082307,"Bo Xie, Yingyu Liang, and Le Song. Scale up nonlinear component analysis with doubly stochastic
gradients. CoRR, abs/1504.03655, 2015."
REFERENCES,0.43621399176954734,Appendix
REFERENCES,0.4382716049382716,"A
MORE RELATED WORK"
REFERENCES,0.4403292181069959,"To scale up the MSSO solvers, a variety of hand-designed approximation schemes have been
investigated. One natural approach is restricting the size of the scenario tree using either a scenario-
wise or state-wise simpliÔ¨Åcation. For example, as a scenario-wise approach, the expected value
of perfect information (EVPI, (Birge, 1982; Hentenryck & Bent, 2006)) has been investigated for
optimizing decision sequences within a scenario, which are then heuristically combined to form a
full solution. Bareilles et al. (2020) instantiates EVPI by considering randomly selected scenarios
in a progressive hedging algorithm (Rockafellar & Wets, 1991) with consensus combination. For
a stage-wise approach, a two-stage model can be used as a surrogate, leveraging a bound on the
approximation gap (Huang & Ahmed, 2009). All of these approximations rely on a Ô¨Åxed prior design
for the reduction mechanism, and cannot adapt to a particular distribution of problem instances.
Consequently, we do not expect such methods to be competitive with learning approaches that can
adapt the approximation strategy to a given problem distribution."
REFERENCES,0.44238683127572015,"Difference to Learning from Cuts in Avila et al. (2021):
the Batch Learning-SDDP (BL-SDDP)
is released recently, where machine learning technique is also used for accelerating MSSO solver.
However, this work is signiÔ¨Åcant different from the proposed ŒΩ-SDDP:"
REFERENCES,0.4444444444444444,"‚Ä¢ Firstly and most importantly, the setting and target of these works are orthogonal: the BL-SDDP
speeds up the SDDP for a particular given MSSO problem via parallel computation; while ŒΩ-SDDP
works for the meta-learning setting that learns from a dataset composed by plenty of MSSO
problems sampled from a distribution, and the learning target is to generalize to new MSSO
instances from the same distribution well;"
REFERENCES,0.44650205761316875,"‚Ä¢ The technique contribution in BL-SDDP and ŒΩ-SDDP are different. SpeciÔ¨Åcally, BL-SDDP exploits
existing off-policy RL tricks for accelerating the SDDP Q-update; while we proposed two key
techniques for quick initialization i), with predicted convex functions; ii), dimension reduction
techniques, to generalize different MSSOs and alleviate curse-of-dimension issues in SDDP, which
has not been explored in BL-SDDP."
REFERENCES,0.448559670781893,"Despite being orthogonal, we think that the BL-SDDP can be used in our framework to provide
better supervision for our cut function prediction, and serve as an alternative for Ô¨Åne-tuning after
ŒΩ-SDDP-fast."
REFERENCES,0.4506172839506173,"One potential drawback of our ŒΩ-SDDP is that, when the test instance distributions deviate a lot from
what has been trained on, the neural initialization may predict cutting planes that are far away from the
good ones, which may slow down the convergence of SDDP learning. Characterizing in-distribution
v.s. out-of-distribution generalization and building the conÔ¨Ådence measure is an important future
work of current approach."
REFERENCES,0.45267489711934156,"B
PRACTICAL PROBLEM INSTANTIATION"
REFERENCES,0.4547325102880658,"In this section, we reformulate the inventory control and portfolio management as multi-stage
stochastic decision problems."
REFERENCES,0.4567901234567901,"B.1
INVENTORY CONTROL"
REFERENCES,0.4588477366255144,"Let S, V, C be the number of suppliers, inventories, and customers, respectively. We denote the
parameters of the inventory control optimization as:"
REFERENCES,0.4609053497942387,‚Ä¢ procurement price matrix: pt ‚ààRSV √ó1;
REFERENCES,0.46296296296296297,‚Ä¢ sales price matrix: qt ‚ààRV C√ó1;
REFERENCES,0.46502057613168724,‚Ä¢ unit holding cost vector: ht ‚ààRV √ó1;
REFERENCES,0.4670781893004115,‚Ä¢ demand vector: dt ‚ààRC√ó1;
REFERENCES,0.4691358024691358,Published as a conference paper at ICLR 2022
REFERENCES,0.4711934156378601,‚Ä¢ supplier capacity vector: ut ‚ààRS√ó1;
REFERENCES,0.4732510288065844,‚Ä¢ inventory capacity vector: vt ‚ààRV √ó1;
REFERENCES,0.47530864197530864,‚Ä¢ initial inventory vector w0 ‚ààRV √ó1.
REFERENCES,0.4773662551440329,The decision variables of the inventory control optimization are denoted as:
REFERENCES,0.4794238683127572,"‚Ä¢ sales variable: yt ‚ààRV C√ó1, indicating the amount of sales from inventories to customers
at the beginning of stage t;"
REFERENCES,0.48148148148148145,"‚Ä¢ procurement variable: zt ‚ààRSV √ó1, indicating the amount of procurement at the beginning
of stage t after sales;"
REFERENCES,0.4835390946502058,"‚Ä¢ inventory variable: wt ‚ààRV √ó1, indicating the inventory level at the beginning of stage t
after procurement."
REFERENCES,0.48559670781893005,"We denote the decision variables as
xt = [yt, zt, wt],
the state as
Œæt = [pt, qt, ht, dt, ut, vt, w0]."
REFERENCES,0.4876543209876543,"The goal of inventory management is to maximize the net proÔ¨Åt for each stage, i.e.,"
REFERENCES,0.4897119341563786,"c‚ä§
t xt := p‚ä§
t zt + h‚ä§
t wt ‚àíq‚ä§
t yt,
and subject to the constraints of 1) supplier capacity; 2) inventory capacity; 3) customer demand, i.e.,"
REFERENCES,0.49176954732510286,"œát (xt‚àí1, Œæt) =

yt, zt, wt  V
X"
REFERENCES,0.49382716049382713,"v=1
yv
t ‚©Ωdt
(demand bound constraints),
(12) V
X"
REFERENCES,0.49588477366255146,"v=1
zv
t ‚©Ωut
(supplier capacity constraints)
(13)"
REFERENCES,0.49794238683127573,"wt ‚©Ωvt
(inventory capacity constraints)
(14) C
X"
REFERENCES,0.5,"c=1
yc
t ‚àíwt‚àí1 ‚©Ω0
(sales bounded by inventory)
(15) S
X"
REFERENCES,0.5020576131687243,"s=1
zs
t ‚àí C
X"
REFERENCES,0.5041152263374485,"c=1
yc
t + wt‚àí1 = wt
(inventory transition)
(16)"
REFERENCES,0.5061728395061729,"zt, yt, wt, ‚©æ0
(non-negativity constraints)

.
(17)"
REFERENCES,0.5082304526748971,"To sum up, the optimization problem can be deÔ¨Åned recursively as follows:"
REFERENCES,0.5102880658436214,"min
x1
c‚ä§
1 x1 + EŒæ2"
REFERENCES,0.5123456790123457,"
min
x2 c2 (Œæ2)‚ä§x2 (Œæ2) + EŒæ3"
REFERENCES,0.51440329218107,"
¬∑ ¬∑ ¬∑ + EŒæT"
REFERENCES,0.5164609053497943,"
min
xT cT (ŒæT )‚ä§xT (ŒæT )

¬∑ ¬∑ ¬∑

, (18)"
REFERENCES,0.5185185185185185,"s.t.
xt ‚ààœát(xt‚àí1, Œæt), ‚àÄt = {1, . . . , T} .
(19)
In fact, the inventory control problem (18) is simpliÔ¨Åed the multi-stage stochastic decision problem (3)
by considering state independent transition."
REFERENCES,0.5205761316872428,"B.2
PORTFOLIO MANAGEMENT"
REFERENCES,0.522633744855967,"Let I be the number of assets, e.g., stocks, being managed. We denote the parameters of the portfolio
optimization are:"
REFERENCES,0.5246913580246914,‚Ä¢ ask (price to pay for buying) open price vector pt ‚ààRI√ó1;
REFERENCES,0.5267489711934157,‚Ä¢ bid (price to pay for sales) open price vector qt ‚ààRI√ó1;
REFERENCES,0.5288065843621399,‚Ä¢ initial amount of investment vector w0 ‚ààRI√ó1;
REFERENCES,0.5308641975308642,‚Ä¢ initial amount of cash r0 ‚ààR+.
REFERENCES,0.5329218106995884,Published as a conference paper at ICLR 2022
REFERENCES,0.5349794238683128,The decision variables of the portfolio optimization are:
REFERENCES,0.5370370370370371,"‚Ä¢ sales vector yt ‚ààRI√ó1, indicating the amount of sales of asset i at the beginning of stage t."
REFERENCES,0.5390946502057613,"‚Ä¢ purchase vector zt ‚ààRI√ó1, indicating the amount of procurement at the beginning of stage
t;"
REFERENCES,0.5411522633744856,"‚Ä¢ holding vector wt ‚ààRI√ó1, indicating the amount of assets at the beginning of stage t after
purchase and sales;"
REFERENCES,0.5432098765432098,"‚Ä¢ cash scalar rt, indicating the amount of cash at the beginning of stage t."
REFERENCES,0.5452674897119342,"We denote the decision variables as
xt = [yt, zt, wt, rt] ,
and the state as
Œæt = [pt, qt] .
The goal of portfolio optimization is to maximize the net proÔ¨Åt i.e.,"
REFERENCES,0.5473251028806584,"c‚ä§
t xt := p‚ä§
t zt ‚àíq‚ä§
t yt,
subject to the constraints of initial investment and the market prices, i.e.,"
REFERENCES,0.5493827160493827,"œát (xt‚àí1, Œæt) :=

yt, zt, wt, rt"
REFERENCES,0.551440329218107,"yt ‚àíwt‚àí1 ‚©Ω0
(individual stock sales constraints)
(20)"
REFERENCES,0.5534979423868313,"p‚ä§
t zt ‚àírt‚àí1 ‚©Ω0
(stock purchase constraints)
(21)
yt ‚àízt + wt ‚àírt‚àí1 = 0
(22)
(individual stock position transition)"
REFERENCES,0.5555555555555556,"q‚ä§
t yt ‚àíp‚ä§
t zt ‚àírt + rt‚àí1 = 0
(23)"
REFERENCES,0.5576131687242798,"(cash position transition)

."
REFERENCES,0.5596707818930041,"With the ct and œát (xt‚àí1, Œæt) deÔ¨Åned above, we initiate the multi-stage stochastic decision problem (3)
for portfolio management."
REFERENCES,0.5617283950617284,"C
DETAILS ON STOCHASTIC DUAL DYNAMIC PROGRAMMING"
REFERENCES,0.5637860082304527,"We have introduced the SDDP in Section 2. In this section, we provide the derivation of the updates
in forward and backward pass,"
REFERENCES,0.565843621399177,"‚Ä¢ Forward pass, updating the action according to (5) based on the current estimation of the value
function at each stage via (4). SpeciÔ¨Åcally, for i-th iteration of t-stage with sample Œæj
t , we solve the
optimization"
REFERENCES,0.5679012345679012,"xt ‚àà
argmin
xt‚ààœát(xt‚àí1,Œæj
t)
ct

Œæj
t
‚ä§
xt + V i
t+1 (xt).
(24)"
REFERENCES,0.5699588477366255,"In fact, the V -function is a convex piece-wise function. SpeciÔ¨Åcally, for i-th iteration of t-stage, we
have"
REFERENCES,0.5720164609053497,"V i
t+1 (xt) = max
k‚©Ωi"
REFERENCES,0.5740740740740741,"n 
Œ≤k
t+1
‚ä§xt + Œ±k
t+1
o
,"
REFERENCES,0.5761316872427984,"Then, we can rewrite the optimization (24) into standard linear programming, i.e.,"
REFERENCES,0.5781893004115226,"min
xt,Œ∏t+1
ct

Œæj
t
‚ä§
xt + Œ∏t+1
(25)"
REFERENCES,0.5802469135802469,"s.t.
At

Œæj
t

xt = bt

Œæj
t

‚àíBt‚àí1

Œæj
t

xt‚àí1,
(26)"
REFERENCES,0.5823045267489712,"‚àí
 
Œ≤k
t+1
‚ä§xt + Œ∏t+1 ‚©æŒ±k
t+1, ‚àÄk = 1, . . . , i,
(27)
xt ‚©æ0,
(28)"
REFERENCES,0.5843621399176955,Published as a conference paper at ICLR 2022
REFERENCES,0.5864197530864198,"‚Ä¢ Backward pass, updating the estimation of the value function via the dual of (25), i.e., for i-th
iteration of t-stage with sample Œæj
t , we calculate"
REFERENCES,0.588477366255144,"max
œât,œÅt"
REFERENCES,0.5905349794238683,"
bt

Œæj
t

‚àíBt‚àí1

Œæj
t
‚ä§
œât + i
X"
REFERENCES,0.5925925925925926,"k=1
œÅk
t Œ±k
t+1,
(29)"
REFERENCES,0.5946502057613169,"s.t.
At

Œæj
t
‚ä§
œât ‚àí i
X"
REFERENCES,0.5967078189300411,"k=1
œÅk
t
 
Œ≤k
t+1
‚ä§‚©Ωct

Œæj
t

,
(30)"
REFERENCES,0.5987654320987654,"‚àí1 ‚©ΩœÅ‚ä§
t 1 ‚©Ω1.
(31)
Then, we have the
V i+1
t
(xt‚àí1) = max

V i
t (xt‚àí1) , vi+1
t
(xt‚àí1)
	
,
(32)
which is still convex piece-wise linear function, with"
REFERENCES,0.6008230452674898,"vi+1
t
(xt‚àí1) :=
 
Œ≤i+1
t
‚ä§xt‚àí1 + Œ±i+1
t
,
(33)
where
 
Œ≤i+1
t
‚ä§:= 1 m m
X j=1"
REFERENCES,0.602880658436214,"
‚àíBt‚àí1

Œæj
t
‚ä§
œât

Œæj
t

,"
REFERENCES,0.6049382716049383,"Œ±i+1
t
:= 1 m m
X j=1 """
REFERENCES,0.6069958847736625,"bt

Œæj
t
‚ä§
œât

Œæj
t

+ i
X"
REFERENCES,0.6090534979423868,"k=1
Œ±k
t+1

Œæj
t

œÅk
t

Œæj
t
# ,"
REFERENCES,0.6111111111111112,"with (œât (Œæt) , œÅt (Œæt)) as the optimal dual solution with realization Œæt."
REFERENCES,0.6131687242798354,"In fact, although we split the forward and backward pass, in our implementation, we exploit the
primal-dual method for LP, which provides both optimal primal and dual variables, saving the
computation cost."
REFERENCES,0.6152263374485597,"Note that SDDP can be interpreted as a form of TD-learning using a non-parametric piecewise linear
model for the V -function. It exploits the property induced by the parametrization of value functions,
leading to the update w.r.t. V -function via adding dual component by exploiting the piecewise linear
structure in a closed-form functional update. That is, TD-learning (Sutton & Barto, 2018; Bertsekas,
2001) essentially conducts stochastic approximate dynamic programming based on the Bellman
recursion (Bellman, 1957)."
REFERENCES,0.6172839506172839,"D
NEURAL NETWORK AND LEARNING SYSTEM DESIGN"
REFERENCES,0.6193415637860082,"Neural network design:
In Figure 5 we present the design of the neural network that tries to
approximate the V -function. The neural network takes two components as input, namely the feature
vector that represents the problem conÔ¨Åguration, and the integer that represents the current stage
of the multi-stage solving process. The stage index is converted into ‚Äòtime-encoding‚Äò, which is a
128-dimensional learnable vector. We use the parameters of distributions as the feature of œÜ (P(Œæ))
for simplicity. The characteristic function or kernel embedding of distribution can be also used here.
The feature vector will also be projected to the same dimension and added together with the time
encoding to form as the input to the MLP. The output of MLP is a matrix of size k √ó (N + 1),
where k is the number of linear pieces, N is the number of variable (and we also need one additional
dimension for intercept). The result is a piecewise linear function that speciÔ¨Åes a convex lowerbound.
We show an illustration of 2D case on the right part of the Figure 5."
REFERENCES,0.6213991769547325,"This neural network architecture is expected to adapt to different problem conÔ¨Ågurations and time
steps, so as to have the generalization and transferability ability across different problem conÔ¨Ågura-
tions."
REFERENCES,0.6234567901234568,"Remark (Stochastic gradient computation):
Aside from Gœà, unbiased gradient estimates for
all other variables in the loss (9) can be recovered straightforwardly. However, Gœà requires special
treatment since we would like it to satisfy the constraints G‚ä§
œàGœà = Ip. Penalty method is one of the"
REFERENCES,0.6255144032921811,"choices, which switches the constraints to a penalty in objective, i.e., Œ∑
G‚ä§
œàGœà ‚àíIp

2"
REFERENCES,0.6275720164609053,"2. However,
the solution satisÔ¨Åes the constraints, only if Œ∑ ‚Üí‚àû(Nocedal & Wright, 2006, Chapter 17). We"
REFERENCES,0.6296296296296297,Published as a conference paper at ICLR 2022
REFERENCES,0.6316872427983539,{                  }
REFERENCES,0.6337448559670782,Linear Projection
REFERENCES,0.6358024691358025,"Time Position 
Encoding MLP x V (x)"
REFERENCES,0.6378600823045267,Problem Context
REFERENCES,0.6399176954732511,Step Index
REFERENCES,0.6419753086419753,Figure 5: Hypernet style parameterization of neural V -function.
REFERENCES,0.6440329218106996,Task Sampling
REFERENCES,0.6460905349794238,Task instance
REFERENCES,0.6481481481481481,Training Tasks
REFERENCES,0.6502057613168725,Task instance
REFERENCES,0.6522633744855967,Task instance
REFERENCES,0.654320987654321,Validation Tasks
REFERENCES,0.6563786008230452,Task instance
REFERENCES,0.6584362139917695,Test Tasks SDDP
REFERENCES,0.6604938271604939,"Forward 
pass"
REFERENCES,0.6625514403292181,"Backward 
pass"
REFERENCES,0.6646090534979424,V-functions
REFERENCES,0.6666666666666666,Trajectory
REFERENCES,0.668724279835391,Sampling SDDP SDDP
REFERENCES,0.6707818930041153,Training Examples
REFERENCES,0.6728395061728395,Validation Examples
REFERENCES,0.6748971193415638,"Task 
params"
REFERENCES,0.676954732510288,"V-func 
params"
REFERENCES,0.6790123456790124,"Task 
params"
REFERENCES,0.6810699588477366,"V-func 
params
...
... ... ..."
REFERENCES,0.6831275720164609,"Task 
params"
REFERENCES,0.6851851851851852,"V-func 
params
..."
REFERENCES,0.6872427983539094,ùû∂-SDDP Inference
REFERENCES,0.6893004115226338,"Forward 
pass"
REFERENCES,0.691358024691358,"Neural 
V-Functions"
REFERENCES,0.6934156378600823,ùû∂-SDDP Training
REFERENCES,0.6954732510288066,"Neural 
V-Functions"
REFERENCES,0.6975308641975309,Figure 6: Illustration of the overall system design.
REFERENCES,0.6995884773662552,"derive the gradient over the Stiefel manifold (Mahony et al., 1996), which ensures the orthonormal
constraints,
gradGœà‚Ñì=
 
I ‚àíG‚ä§
œàGœà

ŒûG‚ä§
œà,
(34)"
REFERENCES,0.7016460905349794,with Œû := P
REFERENCES,0.7037037037037037,"t
Pn
i=1
Pm
j xi‚àó
tj
 
xi‚àó
tj
‚ä§. Note that this gradient can be estimated stochastically since Œû
can be recognized as an expectation over samples."
REFERENCES,0.7057613168724279,"The gradients on Stiefel manifold

G|G‚ä§G = I
	
can be found in Mahony et al. (1996). We derive
the gradient (34) via Lagrangian for self-completeness, following Xie et al. (2015)."
REFERENCES,0.7078189300411523,Consider the Lagrangian as
REFERENCES,0.7098765432098766,"L (Gœà, Œõ) =
X"
REFERENCES,0.7119341563786008,"z‚ààDn
‚Ñì(W; z) + tr
  
G‚ä§
œàGœà ‚àíI

Œõ

,"
REFERENCES,0.7139917695473251,"where the Œõ is the Lagrangian multiplier. Then, the gradient of the Lagrangian w.r.t. Gœà is"
REFERENCES,0.7160493827160493,"‚àáGœàL = 2ŒûG‚ä§
œà + G‚ä§
œà
 
Œõ + Œõ‚ä§
.
(35)
With the optimality condition"
REFERENCES,0.7181069958847737,"‚àáGœà,ŒõL = 0 ‚áí"
REFERENCES,0.720164609053498,"(
G‚ä§
œàGœà ‚àíI = 0
2ŒûG‚ä§
œà + G‚ä§
œà
 
Œõ + Œõ‚ä§
= 0
‚áí‚àí2GœàŒûG‚ä§
œà =
 
Œõ + Œõ‚ä§
.
(36)"
REFERENCES,0.7222222222222222,"Plug (36) into the gradient (35), we have the optimality condition,
 
I ‚àíG‚ä§
œàGœà

ŒûG‚ä§
œà
|
{z
}
gradGœà"
REFERENCES,0.7242798353909465,"= 0.
(37)"
REFERENCES,0.7263374485596708,"To better numerical isolation of the individual eigenvectors, we can exploit Gram-Schmidt process
into the gradient estimator (34), which leads to the generalized Hebbian rule (Sanger, 1989; Kim
et al., 2005; Xie et al., 2015),
^
gradGœà‚Ñì=
 
I ‚àíLT
 
G‚ä§
œàGœà

ŒûG‚ä§
œà = ŒûG‚ä§
œà ‚àíLT
 
G‚ä§
œàGœà

ŒûG‚ä§
œà.
(38)"
REFERENCES,0.7283950617283951,"The LT (¬∑) extracts the lower triangular part of a matrix, setting the upper triangular part and diagonal
to zero, therefore, is mimicking the Gram-Schmidt process to subtracts the contributions from each"
REFERENCES,0.7304526748971193,Published as a conference paper at ICLR 2022
REFERENCES,0.7325102880658436,"10
1
100
Time (s) 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.7345679012345679,Relative Error
REFERENCES,0.7366255144032922,"SDDP-2
SDDP-4
SDDP-8
SDDP-16
SDDP-32
SDDP-mean"
REFERENCES,0.7386831275720165,-SDDP-fast
REFERENCES,0.7407407407407407,"10
1
100
Time (s) 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.742798353909465,Relative Error
REFERENCES,0.7448559670781894,"SDDP-2
SDDP-4
SDDP-8
SDDP-16
SDDP-32
SDDP-mean"
REFERENCES,0.7469135802469136,-SDDP-fast
REFERENCES,0.7489711934156379,"Sml-Sht-mean (¬µd)
Sml-Sht-joint (¬µd & œÉd)"
REFERENCES,0.7510288065843621,"100
101
Time (s) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
REFERENCES,0.7530864197530864,Relative Error
REFERENCES,0.7551440329218106,"SDDP-2
SDDP-4
SDDP-8
SDDP-16
SDDP-32
SDDP-mean"
REFERENCES,0.757201646090535,-SDDP-fast
REFERENCES,0.7592592592592593,"100
101
Time (s) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
REFERENCES,0.7613168724279835,Relative Error
REFERENCES,0.7633744855967078,"SDDP-2
SDDP-4
SDDP-8
SDDP-16
SDDP-32
SDDP-mean"
REFERENCES,0.7654320987654321,-SDDP-fast
REFERENCES,0.7674897119341564,"100
101
Time (s) 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
REFERENCES,0.7695473251028807,Relative Error
REFERENCES,0.7716049382716049,"SDDP-2
SDDP-4
SDDP-8
SDDP-16
SDDP-32
SDDP-mean"
REFERENCES,0.7736625514403292,-SDDP-fast
REFERENCES,0.7757201646090535,"Mid-Lng-mean (¬µd)
Mid-Lng-joint (¬µd & œÉd)
Mid-Lng-joint (¬µd & œÉd & ¬µc)"
REFERENCES,0.7777777777777778,Figure 7: Time-solution trade-off.
REFERENCES,0.779835390946502,"other eigenvectors to achieve orthonormality. Sanger (1989) shows that the updates with (38) will
converges to the Ô¨Årst p eigenvectors of Œû."
REFERENCES,0.7818930041152263,"System design:
Next we present the entire system end-to-end in Figure 6."
REFERENCES,0.7839506172839507,"‚Ä¢ Task sampling: the task sampling component draws the tasks from the same meta distribution. Note
that each task is a speciÔ¨Åcation of the distribution (e.g., the center of Gaussian distribution), where
the speciÔ¨Åcation follows the same meta distribution.
‚Ä¢ We split the task instances into train, validation and test splits:"
REFERENCES,0.7860082304526749,"‚Äì Train: We solve each task instance using SDDP. During the solving of SDDP we need to
perform multiple rounds of forward pass and backward pass to update the cutting planes
(V -functions), as well as sampling trajectories for monte-carlo approximation. The learned
neural V -function will be used as initialization. After SDDP solving converges, we collect the
corresponding task instance speciÔ¨Åcation (parameters) and the resulting cutting planes at each
stage to serve as the training supervision for our neural network module.
‚Äì Validation: We do the same thing for validation tasks, and during training of neural network
we will dump the models that have the best validation loss.
‚Äì Test: In the test stage, we also solve the SDDP until convergence as groundtruth, which is only
used for evaluating the quality of different algorithms. For our neural network approach, we
can generate the convex lower-bound using the trained neural nework, conditioning on each
pair of (test task instance speciÔ¨Åcation, stage index). With the predicted V -functions, we can
run the forward pass only once to retrieve the solution at each stage. Finally we can evaluate
the quality of the obtained solution with respect to the optimal ones obtained by SDDP."
REFERENCES,0.7880658436213992,"E
MORE EXPERIMENTS"
REFERENCES,0.7901234567901234,"E.1
INVENTORY OPTIMIZATION"
REFERENCES,0.7921810699588477,"E.1.1
ADDITIONAL RESULTS ON ŒΩ-SDDP"
REFERENCES,0.7942386831275721,"We Ô¨Årst show the full results of time-solution quality trade-off in Figure 7, and how ŒΩ-SDDP-accurate
improves from ŒΩ-SDDP-fast with better trade-off than SDDP solver iterations in Figure 8. We can
see the conclution holds for all the settings, where our proposed ŒΩ-SDDP achieves better trade-off."
REFERENCES,0.7962962962962963,"Then we also show the ablation results of using different number of predicted cutting planes in
Figure 9. We can see in all settings, generally the more the cutting planes the better the results. This
suggests that in higher dimensional case it might be harder to obtain high quality cutting planes, and"
REFERENCES,0.7983539094650206,Published as a conference paper at ICLR 2022
REFERENCES,0.8004115226337448,"0
2
4
6
8
10
12
14
Time (s) 101 102"
REFERENCES,0.8024691358024691,Error ratio/%
REFERENCES,0.8045267489711934,"-SDDP-accurate
-SDDP-fast SDDP"
REFERENCES,0.8065843621399177,"0.0
2.5
5.0
7.5 10.0 12.5 15.0 17.5"
REFERENCES,0.808641975308642,Time (s) 101 102
REFERENCES,0.8106995884773662,Error ratio/%
REFERENCES,0.8127572016460906,"-SDDP-accurate
-SDDP-fast SDDP"
REFERENCES,0.8148148148148148,"0.0
2.5
5.0
7.5 10.0 12.5 15.0 17.5 20.0"
REFERENCES,0.8168724279835391,Time (s) 101 102
REFERENCES,0.8189300411522634,Error ratio/%
REFERENCES,0.8209876543209876,"-SDDP-accurate
-SDDP-fast SDDP"
REFERENCES,0.823045267489712,"Mid-Lng-mean (¬µd)
Mid-Lng-joint (¬µd & œÉd)
Mid-Lng-joint (¬µd & œÉd & ¬µc)"
REFERENCES,0.8251028806584362,"Figure 8: Time-solution trade-off when ŒΩ-SDDP-accurate improves the solution from ŒΩ-SDDP-fast
further."
REFERENCES,0.8271604938271605,"0
10
20
30
40
50
# generated cutting planes 0 200 400 600 800 1000"
REFERENCES,0.8292181069958847,Error ratio/%
REFERENCES,0.831275720164609,"0
10
20
30
40
50
# generated cutting planes 0 200 400 600 800 1000"
REFERENCES,0.8333333333333334,Error ratio/%
REFERENCES,0.8353909465020576,"Sml-Sht-mean (¬µd)
Sml-Sht-joint (¬µd & œÉd)"
REFERENCES,0.8374485596707819,"0
20
40
60
80
100
# generated cutting planes 0 20 40 60 80 100 120 140"
REFERENCES,0.8395061728395061,Error ratio/%
REFERENCES,0.8415637860082305,"0
20
40
60
80
100
# generated cutting planes 0 20 40 60 80 100 120 140 160"
REFERENCES,0.8436213991769548,Error ratio/%
REFERENCES,0.845679012345679,"0
20
40
60
80
100
# generated cutting planes 0 25 50 75 100 125 150 175 200"
REFERENCES,0.8477366255144033,Error ratio/%
REFERENCES,0.8497942386831275,"Mid-Lng-mean (¬µd)
Mid-Lng-joint (¬µd & œÉd)
Mid-Lng-joint (¬µd & œÉd & ¬µc)"
REFERENCES,0.8518518518518519,Figure 9: Ablation: number of generated cutting planes.
REFERENCES,0.8539094650205762,"due to the convex-lowerbound nature of the V -function, having a bad cutting plane could possibly
hurt the overall quality. How to prune and prioritize the cutting planes will be an important direction
for future works."
REFERENCES,0.8559670781893004,"We provide the full ablation results of doing low-dimensional projection for solving SDDP in
Figure 10. The trend generally agrees with our expectation that, there is a trade-off of the low-
dimensionality that would balance the quality of LP solving and the difÔ¨Åculty of neural network
learning."
REFERENCES,0.8580246913580247,"Longer horizon: we further experiment with the Mid-Lng-joint (¬µd&œÉd&¬µc) by varying T in
{10, 20, 30, 40, 50}. See Table 4 for more information."
REFERENCES,0.8600823045267489,Table 4: Average error ratio of ŒΩ-SDDP-fast on Mid-Lng-joint (¬µd&œÉd&¬µc) setting with varying T.
REFERENCES,0.8621399176954733,"Horizon length
10
20
30
40
50
Average error ratio
3.29%
3.47%
3.53%
2.65%
0.82%"
REFERENCES,0.8641975308641975,"E.1.2
ADDITIONAL RESULTS ON MODEL-FREE RL ALGORITHMS"
REFERENCES,0.8662551440329218,"We implemented the inventory control problem as an environment in the TensorÔ¨Çow TF-Agents
library and used the implementation of DQN (Mnih et al., 2013)1, DDPG (Lillicrap et al., 2016),
PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018) from the TF-Agent to evaluate the
performance of these four model-free RL algorithms. Note that the TF-Agent environment follows a"
REFERENCES,0.8683127572016461,1We provided a simple extension of DQN to support multi-dimensional actions.
REFERENCES,0.8703703703703703,Published as a conference paper at ICLR 2022
REFERENCES,0.8724279835390947,"280
285
290
295
300
305
310
Dimension of generated cutting planes 2.0 2.2 2.4 2.6 2.8 3.0 3.2"
REFERENCES,0.8744855967078189,Error ratio/%
REFERENCES,0.8765432098765432,"280
285
290
295
300
305
310
Dimension of generated cutting planes 5.4 5.6 5.8 6.0 6.2 6.4"
REFERENCES,0.8786008230452675,Error ratio/%
REFERENCES,0.8806584362139918,"280
285
290
295
300
305
310
Dimension of generated cutting planes 9 10 11 12 13 14 15 16 17"
REFERENCES,0.8827160493827161,Error ratio/%
REFERENCES,0.8847736625514403,"Mid-Lng-mean (¬µd)
Mid-Lng-joint (¬µd & œÉd)
Mid-Lng-joint (¬µd & œÉd & ¬µc)"
REFERENCES,0.8868312757201646,"Figure 10: Low-dim projection results when the underlying problem does not have a low-rank
structure."
REFERENCES,0.8888888888888888,"Task
Parameter Domain
DQN
DDPG
PPO
SAC"
REFERENCES,0.8909465020576132,Sml-Sht
REFERENCES,0.8930041152263375,"demand mean (¬µd)
1157.86 ¬± 452.50%
28.62 ¬± 8.69 %
2849.931 ¬± 829.91%
38.42 ¬± 17.78%"
REFERENCES,0.8950617283950617,"joint (¬µd & œÉd)
3609.62 ¬± 912.54%
100.00 ¬± 0.00 %
3273.71 ¬± 953.13%
33.08 ¬± 8.05%"
REFERENCES,0.897119341563786,Mid-Long
REFERENCES,0.8991769547325102,"demand mean (¬µd)
5414.15 ¬± 1476.21%
100.00 ¬± 0.00%
5411.16 ¬± 1474.19%
17.81 ¬± 10.26%"
REFERENCES,0.9012345679012346,"joint (¬µd & œÉd)
5739.68 ¬± 1584.63%
100.00 ¬± 0.00%
5734.75¬± 1582.68 %
50.19 ¬± 5.57%"
REFERENCES,0.9032921810699589,"joint (¬µd & œÉd & ¬µc)
6382.87 ¬± 2553.527%
100.00 ¬± 0.00%
6377.93 ¬±2550.03 %
135.78 ¬± 17.12%"
REFERENCES,0.9053497942386831,Table 6: Average Error Ratio of Objective Value.
REFERENCES,0.9074074074074074,"MDP formulation. When adapting to the inventory control problem, the states of the environment
are the levels of the inventories and the actions are the amount of procurement and sales. As a
comparison, the states and actions in the MDP formulation collectively form the decision varialbles
in the MSSO formulation and their relationship ‚Äì inventory level transition, is captured as a linear
constraint. Following Schulman (2016), we included the timestep into state in these RL algorithms to
have non-stationary policies for Ô¨Ånite-horizon problems. In the experiments, input parameters for
each problem domain and instances are normalized for model training for policy gradient algorithms
and the results are scaled back for reporting."
REFERENCES,0.9094650205761317,"We report the average error ratio of these four RL algorithms in Table. 6 along with the average
variance in Table. 7. Note that the SAC performance is also reported in the main text in Table. 2 and
Table. 3. The model used in the evaluation is selected based on the best mean return over the 50
trajectories from the validation environment, based on which the hyperparameters are also tuned. We
report the selected hyperparameters for each algorithm in Table. 5. We use MLP with 3 layers as the
Q-network for DQN, as the actor network and the critic/value network for SAC, PPO and DDPG. All
networks have the same learning rate and with a dropout parameter as 0.001."
REFERENCES,0.911522633744856,Table 5: Hyperparameter Selections.
REFERENCES,0.9135802469135802,"Algorithm
Hyperparameters
SAC
learning rate(0.01), num MLP units (50), target update period (5), target update tau (0.5)
PPO
learning rate(0.001), num MLP units (50), target update period (5), target update tau (0.5)
DQN
learning rate(0.01), num MLP units (100), target update period (5), target update tau (0.5)
DDPG
learning rate(0.001), num MLP units (50), ou stddev (0.2), ou damping (0.15)"
REFERENCES,0.9156378600823045,"We see that SAC performs the best among the four algorithms in terms of solution quality. All the
algorithms can not scale to Mid-Long setting. DDPG, for example, produces a trivial policy of no
action in most of setups (thus has an error ratio of 100). The policies learned by DQN and PPO are
even worse, producing negative returns2."
REFERENCES,0.9176954732510288,"To understand the behavior of each RL algorithm, we plotted the convergence of the average mean
returns in Figure. 11 for the Sml-Sht task. In each plot, we show four runs of the respective algorithm
under the selected hparameter. We could see that though SAC converges the slowest, it is able to"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9197530864197531,"2Negative returns are caused by over-procurement in the early stages and the leftover inventory at the last
stage."
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9218106995884774,Published as a conference paper at ICLR 2022
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9238683127572016,"Task
Parameter Domain
DQN
DDPG
PPO
SAC"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9259259259259259,Sml-Sht
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9279835390946503,"demand mean (¬µd)
46.32 ¬± 85.90
0.34 ¬± 0.22
119.08 ¬±112.00
3.90¬± 8.39"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9300411522633745,"joint (¬µd & œÉd)
86.097 ¬± 100.81
0.00 ¬± 0.00
169.08 ¬± 147.24
1.183¬± 4.251"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9320987654320988,Mid-Long
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.934156378600823,"demand mean (¬µd)
1334.30 ¬± 270.00
0.00 ¬± 0.00
339.97 ¬± 620.01
1.98¬± 2.65"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9362139917695473,"joint (¬µd & œÉd)
1983.71 ¬± 1874.61
0.00 ¬± 0.00
461.27 ¬± 1323.24
205.51 ¬± 150.90"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9382716049382716,"joint (¬µd & œÉd & ¬µc)
1983.74 ¬± 1874.65
0.00 ¬± 0.00
462.74 ¬± 1332.30
563.19 ¬± 114.03"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9403292181069959,Table 7: Objective Value Variance.
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9423868312757202,"SAC
PPO
DQN
DDPG"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9444444444444444,Figure 11: Average mean return (values are normalized with optimal mean value as 1.736 ).
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9465020576131687,"achieve the best return. For all algorithms, their performance is very sensitive to initialization. DDPG,
for example, has three runs with 0 return, while one run with a return of 1.2. For PPO and DQN, the
average mean returns are both negative ."
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.948559670781893,"We further check the performance of these algorithms in the validation environment based on the
same problem instance (i.e., the same problem parameters) as in SDDP-mean, where the model is
trained and selected. We expect this would give the performance upper bound for these algorithms.
Again similar results are observed. The best return mean over the validation environment is ‚àí38.51
for PPO, 0.95 for DQN, 1.31 for SAC and 1.41 for DDPG, while the SDDP optimal return value
is 1.736. It is also worth noting that DDPG shows the strongest sensitivity to initialization and its
performance drops quickly when the problem domain scales up."
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9506172839506173,"E.2
PORTFOLIO OPTIMIZATION"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9526748971193416,"We use the daily opening prices from selected Nasdaq stocks in our case study. This implies that
the asset allocation and rebalancing in our portfolio optimization is performed once at each stock
market opening day. We Ô¨Årst learn a probabilistic forecasting model from the historical prices ranging
from 2015-1-1 to 2020-01-01. Then the forecasted trajectories are sampled from the model for the
stochastic optimization. Since the ask price is always slightly higher than the bid price, at most one
of the buying or selling operation will be performed for each stock, but not both, on a given day."
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9547325102880658,"E.2.1
STOCK PRICE FORECAST"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9567901234567902,"The original model for portfolio management in Appendix B.2 is too restrict. We generalize the
model with autoregressive process (AR) of order o with independent noise is used to model and
predict the stock price: pt = o
X"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9588477366255144,"i=1
(œÜipt‚àíi + œµr
i ) + œµo"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9609053497942387,"where œÜi is the autoregressive coefÔ¨Åcient and œµri ‚àºN(0, œÉ2
i ) is a white noise of order i. œµo ‚àº
N(0, œÉ2
o) is a white noise of the observation. Each noise term is œµ assumed to be independent. It is
easy to check that the MSSO formulation for portfolio management is still valid by replacing the
expectation for œµ, and setting the concatenate state [pt‚àío
t
, qt], where pt‚àío
t
:= [pt‚àíi]o
i=0."
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9629629629629629,"We use variational inference to Ô¨Åt the model. A variational loss function (i.e., the negative evidence
lower bound (ELBO)) is minimized to Ô¨Åt the approximate posterior distributions for the above
parameters. Then we use the posterior samples as inputs for forecasting. In our study, we have studied
the forecasting performance with different length of history, different orders of the AR process and
different groups of stocks. Figure. 12 shows the ELBO loss convergence behavior under different
setups. As we can see, AR models with lower orders converge faster and smoother."
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9650205761316872,Published as a conference paper at ICLR 2022
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9670781893004116,"(a) 5 Stocks with AR order = 2
(b) 5 Stocks with AR order = 5
(c) 8 Stock Clusters with AR order = 5"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9691358024691358,Figure 12: Evidence lower bound (ELBO) loss curve.
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9711934156378601,"AR order = 2
AR order = 5"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9732510288065843,Figure 13: Probablistic Forecast of 5 Stocks with Different AR Orders.
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9753086419753086,"We further compare the forecasting performance with different AR orders. Figure. 13 plots a side-
by-side comparison of the forecasted mean trajectory with a conÔ¨Ådence interval of two standard
deviations (95%) for 5 randomly selected stocks (with tickers GOOG, COKE, LOW, JPM, BBBY)
with AR order of 2 and 5 from 2016-12-27 for 5 trading days. As we could see, a higher AR order (5)
provides more time-variation in forecast and closer match to the ground truth."
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.977366255144033,"In addition, we cluster the stocks based on their price time series (i.e., each stock is represented by
a T-dimensional vector in the clustering algorithm, where T is the number of days in the study).
We randomly selected 1000 stocks from Nasdaq which are active from 2019-1-1 to 2020-1-1 and
performed k-means clustering to form 8 clusters of stocks. We take the cluster center time series as
the training input. Figure. 12(c) shows the ELBO loss convergence of an AR process of order 5 based
on these 8 cluster center time series. As we see, the stock cluster time series converge smoother and
faster compared with the individual stocks as the aggregated stock series are less Ô¨Çuctuated. The
forecasting trajectories of these 8 stock clusters starting from 2019-03-14 are plotted in Figure. 14."
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9794238683127572,"E.2.2
SOLUTION QUALITY COMPARISON"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9814814814814815,Table 8: Portfolio optimization with synthetic standard deviation of stock price.
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9835390946502057,"SDDP-mean
ŒΩ-SDDP-zeroshot
5 stocks (STD scaled by 0.01)
290.05 ¬± 221.92 %
1.72 ¬± 4.39 %
5 stocks (STD scaled by 0.001)
271.65 ¬± 221.13 %
1.84 ¬± 3.67 %
8 clusters (STD scaled by 0.1)
69.18 ¬± 77.47 %
1.43e‚àí6 ¬± 4.30e‚àí5%
8 clusters (STD scaled by 0.01)
65.81 ¬± 77.33 %
3.25e‚àí6 ¬± 3.44e‚àí5%"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.98559670781893,Published as a conference paper at ICLR 2022
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9876543209876543,Figure 14: Probablistic Forecast of 8 Stock Clusters.
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9897119341563786,"With an AR forecast model of order o, the problem context of a protfolio optimizaton instance is then
captured by the joint distribution of the historical stock prices over a window of o days. We learn
this distribution using kernel density estimation. To sample the scenarios for each stage using the
AR model during training for both SDDP and ŒΩ-SDDP, we randomly select the observations from
the previous stage to seed the observation sampling in the next stage. Also we approximate the state
representation by dropping the term of pt‚àío
t
. With such treatments, we can obtain SDDP results with
manageable computation cost. We compare the performance of SDDP-optimal, SDDP-mean and
ŒΩ-SDDP-zeroshot under different forecasting models for a horizon of 5 trading days. First we observe
that using the AR model with order 2 as the forecasting model, as suggested by the work (Dantzig
& Infanger, 1993), produce a very simple forecasting distribution where the mean is monotonic
over the forecasting horizon. As a result, all the algorithms will lead to a simple ‚Äúbuy and hold‚Äù
policy which make no difference in solution quality. We further increase the AR order gradually"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9917695473251029,Published as a conference paper at ICLR 2022
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9938271604938271,"from 2 to 5 and Ô¨Ånd AR order at 5 produces sufÔ¨Åcient time variation that better Ô¨Åts the ground truth.
Second we observe that the variance learned from the real stock data based on variational inference is
signiÔ¨Åcant. With high variance, both SDDP-optimal and ŒΩ-SDDP-zeroshot would achieve the similar
result, which is obtained by a similar ‚Äúbuy and hold‚Äù policy. To make the task more challenging, we
rescale the standard deviation (STD) of stock price by a factor in Table 8 for both the 5-stock case
and 1000-stock case with 8 clusters situations. For the cluster case, the ŒΩ-SDDP-zeroshot can achieve
almost the same performance as the SDDP-optimal."
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9958847736625515,"E.3
COMPUTATION RESOURCE"
NEGATIVE RETURNS ARE CAUSED BY OVER-PROCUREMENT IN THE EARLY STAGES AND THE LEFTOVER INVENTORY AT THE LAST,0.9979423868312757,"For the SDDP algorithms we run using multi-core CPUs, where the LP solving can be parallelized at
each stage. For RL based approaches and our ŒΩ-SDDP, we train using a single V100 GPU for each
hparameter conÔ¨Åguration for at most 1 day or till convergence."
