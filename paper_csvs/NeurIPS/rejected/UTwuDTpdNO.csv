Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005757052389176742,"Recent research has uncovered that federated learning (FL) systems are vulnera-
1"
ABSTRACT,0.0011514104778353484,"ble to various security threats. Although various defense mechanisms have been
2"
ABSTRACT,0.0017271157167530224,"proposed, they are typically non-adaptive and tailored to specific types of attacks,
3"
ABSTRACT,0.002302820955670697,"leaving them insufficient in the face of adaptive or mixed attacks. In this work,
4"
ABSTRACT,0.0028785261945883708,"we formulate adversarial federated learning as a Bayesian Stackelberg Markov
5"
ABSTRACT,0.0034542314335060447,"game (BSMG) to tackle poisoning attacks of unknown/uncertain types. We further
6"
ABSTRACT,0.004029936672423719,"develop an efficient meta-learning approach to solve the game, which provides a
7"
ABSTRACT,0.004605641911341394,"robust and adaptive FL defense. Theoretically, we show that our algorithm provably
8"
ABSTRACT,0.0051813471502590676,"converges to the first-order ε-equilibrium point in O(ε−2) gradient iterations with
9"
ABSTRACT,0.0057570523891767415,"O(ε−4) samples per iteration. Empirical results show that our meta-Stackelberg
10"
ABSTRACT,0.0063327576280944155,"framework obtains superb performance against strong model poisoning and back-
11"
ABSTRACT,0.0069084628670120895,"door attacks with unknown/uncertain types.
12"
INTRODUCTION,0.007484168105929764,"1
Introduction
13"
INTRODUCTION,0.008059873344847437,"Federated learning (FL) allows multiple devices with private data to jointly train a model without
14"
INTRODUCTION,0.008635578583765112,"sharing their local data [39]. However, FL systems are vulnerable to various adversarial attacks
15"
INTRODUCTION,0.009211283822682787,"such as untargeted model poisoning attacks (e.g., IPM [68], LMP [15]) and backdoor attacks (e.g.,
16"
INTRODUCTION,0.00978698906160046,"BFL [2], DBA [71]). To address these vulnerabilities, various robust aggregation rules such as
17"
INTRODUCTION,0.010362694300518135,"Krum [7], coordinate-wise trimmed mean [69], and FLTrust [10] have been proposed to defend against
18"
INTRODUCTION,0.010938399539435808,"untargeted attacks, and both training-stage and post-training defenses such as Norm bounding [57],
19"
INTRODUCTION,0.011514104778353483,"NeuroClip [62], and Prun [64] have been proposed to mitigate backdoor attacks. Further, dynamic
20"
INTRODUCTION,0.012089810017271158,"defenses that myopically adapt parameters such as learning rate [45], norm clipping threshold [21],
21"
INTRODUCTION,0.012665515256188831,"and regularizer [1] have been proposed. However, state-of-the-art defenses remain inadequate in
22"
INTRODUCTION,0.013241220495106506,"countering advanced adaptive attacks (e.g., the reinforcement learning (RL)-based attacks [31, 32])
23"
INTRODUCTION,0.013816925734024179,"that dynamically adjust the attack strategy to obtain long-term advantages. Further, current defenses
24"
INTRODUCTION,0.014392630972941854,"are typically designed to counter specific types of attacks, rendering them ineffective in the presence
25"
INTRODUCTION,0.014968336211859529,"of mixed attacks. As shown in Table 1 (Section 4), simply combining existing defenses with manual
26"
INTRODUCTION,0.015544041450777202,"tuning proves ineffective due to the interference between defense methods, the defender’s lack of
27"
INTRODUCTION,0.016119746689694875,"information about adversaries, and the dynamic nature of FL.
28"
INTRODUCTION,0.01669545192861255,"In this work, we propose a meta-Stackelberg game (meta-SG) framework that obtains superb defense
29"
INTRODUCTION,0.017271157167530225,"performance even in the presence of strong adaptive attacks and a mix of attacks of the same or
30"
INTRODUCTION,0.017846862406447898,"different types (e.g., the coexistence of model poisoning and backdoor attacks). Our meta-SG defense
31"
INTRODUCTION,0.018422567645365574,"framework is built upon the following key observations. First, when the attack type (to be defined in
32"
INTRODUCTION,0.018998272884283247,"Section 2.1) is known as priori, the defender can utilize the limited amount of local data at the server
33"
INTRODUCTION,0.01957397812320092,"and publicly available information to build an approximate world model of the FL system. This
34"
INTRODUCTION,0.020149683362118594,"allows the defender to identify a robust defense policy offline by solving either a Markov decision
35"
INTRODUCTION,0.02072538860103627,"process (MDP) when the attack is non-adaptive or a Markov game when the attack is adaptive. This
36"
INTRODUCTION,0.021301093839953943,"approach naturally applies to both a single attack and the coexistence of multiple attacks and can
37"
INTRODUCTION,0.021876799078871616,"potentially produce a (nearly) optimal defense. Second, when the attacks are unknown or uncertain,
38"
INTRODUCTION,0.022452504317789293,"as in more realistic settings, the problem can be formulated as a Bayesian Stackelberg Markov game
39"
INTRODUCTION,0.023028209556706966,"(BSMG) [52], which provides a general model for adversarial FL. However, the standard solution
40"
INTRODUCTION,0.02360391479562464,"concept for BSMG, namely, the Bayesian Stackelberg equilibrium, targets the expected case and does
41"
INTRODUCTION,0.024179620034542316,"not adapt to the actual attack with an unknown/uncertain type.
42"
INTRODUCTION,0.02475532527345999,"Motivated by this limitation, we propose a novel solution concept called meta-Stackelberg equilibrium
43"
INTRODUCTION,0.025331030512377662,"(meta-SE) for BSMG as a principled way of developing robust and adaptive defenses for FL. By
44"
INTRODUCTION,0.025906735751295335,"integrating meta-learning and Stackelberg reasoning, meta-SE offers a computationally efficient
45"
INTRODUCTION,0.02648244099021301,"approach to address information asymmetry in adversarial FL and enables strategic adaptation in
46"
INTRODUCTION,0.027058146229130685,"online execution in the presence of multiple (adaptive) attackers. Before training an FL model,
47"
INTRODUCTION,0.027633851468048358,"a meta policy is learned by solving the BSMG using experiences sampled from a set of possible
48"
INTRODUCTION,0.028209556706966035,"attacks. When facing an actual attacker during FL training, the meta-policy is quickly adapted
49"
INTRODUCTION,0.028785261945883708,"using a relatively small number of samples collected on the fly. The proposed meta-SG framework
50"
INTRODUCTION,0.02936096718480138,"only requires a rough estimate of possible worst-case attacks during meta-training, thanks to the
51"
INTRODUCTION,0.029936672423719057,"generalization ability brought by meta-learning.
52"
INTRODUCTION,0.03051237766263673,"To solve the BSMG in the pre-training phase, we propose a meta-Stackelberg learning (meta-SL)
53"
INTRODUCTION,0.031088082901554404,"algorithm based on the debiased meta-reinforcement learning approach in [14]. The meta-SL
54"
INTRODUCTION,0.03166378814047208,"provably converges to the first-order ε-approximate meta-SE in O(ε−2) iterations, and the associated
55"
INTRODUCTION,0.03223949337938975,"sample complexity per iteration is of O(ε−4). Even though meta-SL achieves state-of-the-art sample
56"
INTRODUCTION,0.03281519861830743,"efficiency presented in [24], its operation involves the Hessian of the defender’s value function. To
57"
INTRODUCTION,0.0333909038572251,"obtain a more practical solution (to bypass the Hessian computation), we further propose a fully
58"
INTRODUCTION,0.033966609096142776,"first-order pre-training algorithm, called Reptile meta-SL, inspired by Reptile [43]. Reptile meta-SL
59"
INTRODUCTION,0.03454231433506045,"only utilizes the first-order stochastic gradients from the attacker’s and the defender’s problem to
60"
INTRODUCTION,0.03511801957397812,"solve for the approximate equilibrium. The numerical results in Table 1 demonstrate its effectiveness
61"
INTRODUCTION,0.035693724812895795,"in handling various types of non-adaptive attacks, including mixed attacks , while Figure 2 and
62"
INTRODUCTION,0.03626943005181347,"Figure 9 highlight its efficiency in coping with uncertain or unknown attacks, including adaptive
63"
INTRODUCTION,0.03684513529073115,"attacks. Due to the space limit, we move related work section to Appendix A. Our contributions are
64"
INTRODUCTION,0.03742084052964882,"summarized as follows:
65"
INTRODUCTION,0.037996545768566495,"• We address critical security problems in FL in the face of attacks that may be adaptive or
66"
INTRODUCTION,0.03857225100748417,"mixed with multiple types.
67"
INTRODUCTION,0.03914795624640184,"• We develop a Bayesian Stackelberg game model (Section 2.2) to capture the information
68"
INTRODUCTION,0.039723661485319514,"asymmetry in the adversarial FL under multiple uncertain/unknown attacks.
69"
INTRODUCTION,0.04029936672423719,"• To create a strategically adaptable defense, we propose a new equilibrium concept: meta-
70"
INTRODUCTION,0.04087507196315487,"Stackelberg equilibrium (meta-SE), where the defender (the leader) commits to a meta
71"
INTRODUCTION,0.04145077720207254,"policy and an adaptation strategy, leading to a data-driven approach to tackle information
72"
INTRODUCTION,0.042026482440990214,"asymmetry.
73"
INTRODUCTION,0.04260218767990789,"• To learn the meta equilibrium defense in the pre-training phase, we develop meta-Stackelberg
74"
INTRODUCTION,0.04317789291882556,"learning (Algorithm 1), an efficient first-order meta RL algorithm, which provably converges
75"
INTRODUCTION,0.04375359815774323,"to ε-approximate equilibrium in O(ε−2) gradient steps with O(ε−4) samples per iteration,
76"
INTRODUCTION,0.04432930339666091,"matching the state-of-the-art efficiency in stochastic bilevel optimization.
77"
INTRODUCTION,0.044905008635578586,"• We conduct extensive experiments in real-world settings to demonstrate the superb perfor-
78"
INTRODUCTION,0.04548071387449626,"mance of our proposed method.
79"
META STACKELBERG DEFENSE FRAMEWORK,0.04605641911341393,"2
Meta Stackelberg Defense Framework
80"
FRAMEWORK OVERVIEW,0.046632124352331605,"2.1
Framework Overview
81"
FRAMEWORK OVERVIEW,0.04720782959124928,"As shown in Figure 1, the meta-learning framework includes two stages: pre-training, online
82"
FRAMEWORK OVERVIEW,0.04778353483016695,"adaptation. The pre-training stage is implemented in a simulated environment, which allows
83"
FRAMEWORK OVERVIEW,0.04835924006908463,"sufficient training using trajectories generated from the interactions between the defender and the
84"
FRAMEWORK OVERVIEW,0.048934945308002305,"attacker with its type randomly sampled from a set of potential attacks. Both adaptive and non-
85"
FRAMEWORK OVERVIEW,0.04951065054691998,"adaptive attacks could be considered for pre-training. After obtaining a meta-policy, the defender will
86"
FRAMEWORK OVERVIEW,0.05008635578583765,"interact with the real FL environment in the online adaptation stage to tune its defense policy using
87"
FRAMEWORK OVERVIEW,0.050662061024755324,"feedback (i.e., model updates and environment parameters) received in the face of real attacks that
88"
FRAMEWORK OVERVIEW,0.051237766263673,"Figure 1: A graphical abstract of meta-Stackelberg defense. In the pertaining stage, a simulated
environment is constructed using generated data and the attack domain. The defender utilizes meta-
Stackelberg learning (Algorithm 1) to obtain the meta policy to be online adapted in the real FL."
FRAMEWORK OVERVIEW,0.05181347150259067,"are not necessarily in the pre-training attack set. Finally, at the last round of FL training, the defender
89"
FRAMEWORK OVERVIEW,0.05238917674150835,"will perform a post-training defense on the global model, which may or may not be considered in the
90"
FRAMEWORK OVERVIEW,0.05296488198042602,"design of intelligent attacks. Pre-training and online adaptation are indispensable in the proposed
91"
FRAMEWORK OVERVIEW,0.0535405872193437,"framework. Table 5 in Appendix D indicate that directly applying defense learned from pre-training
92"
FRAMEWORK OVERVIEW,0.05411629245826137,"without online adaptation, as well as adaptation from a randomly initialized defense policy without
93"
FRAMEWORK OVERVIEW,0.05469199769717904,"pre-training, both fail to address malicious attacks.
94"
FRAMEWORK OVERVIEW,0.055267702936096716,"FL objective. Consider a learning system that includes one server and n clients, each client possesses
95"
FRAMEWORK OVERVIEW,0.055843408175014396,"its own private dataset Di = (xj
i, yj
i )|Di|
j=1 where |Di| is the size of the dataset for the i-th client.
96"
FRAMEWORK OVERVIEW,0.05641911341393207,"Let U = {D1, D2, . . . , Dn} denote the collection of all client datasets. The objective of federated
97"
FRAMEWORK OVERVIEW,0.05699481865284974,"learning is to obtain a model w that minimizes the average loss across all the devices: minw F(w) :=
98"
"N
PN",0.057570523891767415,"1
n
Pn
i=1 f(w, Di), where f(w, Di) :=
1
|Di|
P|Di|
j=1 ℓ(w, (xj
i, yj
i )) is the local empirical loss with
99"
"N
PN",0.05814622913068509,"ℓ(·, ·) being the loss function.
100"
"N
PN",0.05872193436960276,"Attack objective. We consider two major categories of attacks: untargeted model poisoning attacks
101"
"N
PN",0.059297639608520435,"and backdoor attacks. An untargeted model poisoning attack aims to maximize the average model loss,
102"
"N
PN",0.059873344847438115,"i.e., minw −F(w), while a targeted one strives to cause misclassification of poisoned test inputs to
103"
"N
PN",0.06044905008635579,"one or more target labels (e.g., backdoor attacks). A malicious client i employing targeted attack first
104"
"N
PN",0.06102475532527346,"produces a poisoned dataset D′
i by altering a subset of data samples (xj
i, yj
i ) ∈Di to (ˆxj
i, c∗). Here ˆxj
i
105"
"N
PN",0.061600460564191134,"is the tainted sample with a backdoor trigger inserted, and c∗̸= yj
i , c∗∈C is the targeted label. Let
106"
"N
PN",0.06217616580310881,"ρi = |D′
i|/|Di| denote the poisoning ratio, which is typically unknown to the defender. To simplify
107"
"N
PN",0.06275187104202648,"the notation, we assume that among the M = M1 + M2 malicious clients, the first M1 malicious
108"
"N
PN",0.06332757628094415,"clients carry out a targeted attack, and the following M2 malicious clients undertake an untargeted
109"
"N
PN",0.06390328151986183,"attack. Note that clients in the same category may use different attack methods. Then, the joint
110"
"N
PN",0.0644789867587795,"objective of these malicious clients is minw F ′(w) :=
1
M1
PM1
i=1 f(w, D′
i)−1"
"N
PN",0.06505469199769717,"M2
PM
i=M1+1 f(w, Di).
111"
"N
PN",0.06563039723661486,"FL process. At each round t out of H rounds of FL training, the server randomly selects a subset of
112"
"N
PN",0.06620610247553253,"clients St and sends them the most recent global model wt
g. Every benign client k ∈St updates the
113"
"N
PN",0.0667818077144502,"model using their local data via one or more iterations of stochastic gradient descent and returns the
114"
"N
PN",0.06735751295336788,"model update gt
k to the server. In contrast, an adversary j ∈St creates a malicious model update
115"
"N
PN",0.06793321819228555,"egt
j and sends it back. The server then collects the set of model updates {egt
i ∪egt
j ∪gt
k}i,j,k∈St, for
116"
"N
PN",0.06850892343120323,"i ∈{1, . . . , M1}, j ∈{M1 +1, . . . , M}, k ∈St \[M], utilizes an aggregation rule Aggr to combine
117"
"N
PN",0.0690846286701209,"them, and updates the global model: wt+1
g
= wt
g −ηtAggr(egt
i ∪egt
j ∪gt
k), which is then sent to
118"
"N
PN",0.06966033390903857,"clients in round t + 1. At the end of each round, the defender will perform a post-training defense
119"
"N
PN",0.07023603914795624,"h(·) on the global model bwt
g = h(wt
g) to evaluate the current defense performance. Only at the final
120"
"N
PN",0.07081174438687392,"round H or whenever a client is leaving the FL systems, the global model with post-training defense
121"
"N
PN",0.07138744962579159,"bwt
g will be sent to all (leaving) clients.
122"
"N
PN",0.07196315486470926,"Attack types. To simplify the exposition, we assume that a single mastermind attacker controls all
123"
"N
PN",0.07253886010362694,"malicious clients within the FL system and employs diverse attack strategies on each controlled client.
124"
"N
PN",0.07311456534254462,"We introduce the concept of attack type to differentiate various attack scenarios, which typically
125"
"N
PN",0.0736902705814623,"include the following three aspects. The first aspect is the attack objective chosen by a malicious
126"
"N
PN",0.07426597582037997,"client. Let Ω1 be the set of all possible attack objectives from the defender’s knowledge base. We set
127"
"N
PN",0.07484168105929764,"Ω1 = {untargeted, targeted} in this work. The second aspect specifies the attack method (i.e., the
128"
"N
PN",0.07541738629821532,"algorithm used to generate the actual attack policy) adopted by a malicious client. Let Ω2 be the set
129"
"N
PN",0.07599309153713299,"of all possible attack methods from the defender’s knowledge base. The third aspect captures the
130"
"N
PN",0.07656879677605066,"configuration associated with an attack method, including its hyperparameters and other attributes
131"
"N
PN",0.07714450201496834,"(e.g., triggers implanted in backdoor attacks, labels used in targeted attacks, and attacker’s knowledge
132"
"N
PN",0.07772020725388601,"about the FL system). Let Ω3 denote the set of all possible configurations. For each malicious client
133"
"N
PN",0.07829591249280368,"i, the tuple (ω1, ω2, ω3)i where ωk ∈Ωk for each k fully specifies its particular attack type. Let
134"
"N
PN",0.07887161773172136,"ξ = {(ω1, ω2, ω3)i}M
i=1 be the joint attack type. Further, let Ξ = (Ω1 × Ω2 × Ω3)M denote the
135"
"N
PN",0.07944732297063903,"domain of attacks that the defender is aware of. Table 2 in Appendix C gives the types of all the
136"
"N
PN",0.0800230282095567,"attacks considered in this work. However, the actual attack type encountered during FL training is
137"
"N
PN",0.08059873344847437,"not necessary in Ξ, although it is presumably similar to a known type in Ξ.
138"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08117443868739206,"2.2
Pre-training as a Bayesian Stackelberg Markov game
139"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08175014392630973,"From the discussion above, the global model updates and the final output are jointly influenced by the
140"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08232584916522741,"defender (through aggregation) and the malicious clients (through corrupted gradients). Hence, the
141"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08290155440414508,"FL process in an adversarial environment can be formulated as a two-player discrete time Bayesian
142"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08347725964306275,"Stackelberg Markov game (BSMG) defined by a tuple ⟨S, AD, Aξ, T , r, γ, H⟩. Using discrete time
143"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08405296488198043,"index t (one step corresponds to one FL round), we have the following.
144"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.0846286701208981,"• S is the state space, and its elements represent the global model at each round st = wt
g.
145"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08520437535981577,"• AD is the defender’s action set. Each action at
D represents a combination of the robust
146"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08578008059873345,"aggregation and post-training defenses: at
D = {Aggr(·), h(·)}.
147"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08635578583765112,"• Aξ is the type-ξ attacker’s action set. Each action includes the joint model updates of all
148"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08693149107656879,"malicious clients: at
A = {egt
i}M1
i=1 ∪{egt
i}M
i=M1+1.
149"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08750719631548647,"• T (st+1|st, Aggr(·), at
A) specifies the distribution of the next state given the current state
150"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08808290155440414,"and joint actions at t, which is determined by the global model update: wt+1
g
= wt
g −
151"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08865860679332183,"ηtAggr(egt
i ∪egt
j ∪gt
k).
152"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.0892343120322395,"• rD, rξ are the defender’s and the attacker’s reward functions (to be maximized), respectively.
153"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.08981001727115717,"The defender aims to minimize the loss after the post-training: rt
D := −F( bwt
g) where
154"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09038572251007485,"bwt
g = h(wt
g). The attacker’s rt
ξ is given by the joint attack objective: −F ′( bwt
g).
155"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09096142774899252,"Remark 2.1. The post-training defense is only applied in the final round or to a client leaving the
156"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09153713298791019,"FL system and does not interfere with the model updates on wt
g. The defender’s reward function is
157"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09211283822682786,"crafted to encompass post-training, as we prioritize a practical, long-term average reward within an
158"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09268854346574554,"online process, which enables clients to seamlessly join and depart from the FL system. This design
159"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09326424870466321,"enables us to incorporate a post-training defense along with techniques for modifying the model
160"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09383995394358088,"structure, such as drop-off and pruning.
161"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09441565918249856,"Simulated environment in the white-box setting. With the game model defined above, the defender
162"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09499136442141623,"(i.e., the server) can, in principle, identify a strong defense by solving the game (we discuss different
163"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.0955670696603339,"solution concepts in Section 3). Due to efficiency and privacy concerns in FL, however, it is often
164"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09614277489925158,"infeasible to solve the game in real time when facing the actual attacker. Instead, the defender can
165"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09671848013816926,"create a simulated environment to approximate the actual FL system during the pre-training stage.
166"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09729418537708694,"The main challenge, however, is that the defender often lacks information about the individual devices
167"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09786989061600461,"in FL. We first consider the white-box setting where the defender is aware of the number of malicious
168"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09844559585492228,"devices in each category (i.e., M1 and M2) and their actual attack types, as well as the non-i.i.d. level
169"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09902130109383996,"(to be defined in Section 4.1) of local data distributions across devices. However, it does not have
170"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.09959700633275763,"access to individual devices’ local data and random seeds, making it difficult to simulate clients’ local
171"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.1001727115716753,"training and evaluate rewards. To this end, we assume that the server has a small amount of root data
172"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10074841681059298,"randomly sampled from the the collection of all client dataset U as in previous work [10, 40]. We
173"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10132412204951065,"then use generative model (e.g., conditional GAN model [41] for MNIST and diffusion model [55]
174"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10189982728842832,"for CIFAR-10 in our experiments) to generate as much data as necessary to mimic the local training
175"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.102475532527346,"(see details in Appendix C). We give an ablation study (Table 6) in Appendix D to evaluate the
176"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10305123776626367,"influence of limited/biased root data. We remark that the purpose of pre-training is to derive a defense
177"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10362694300518134,"policy rather than the model itself. Directly using the shifted data (root or generated) to train the FL
178"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10420264824409903,"model will result in low model accuracy (see Table 5 in Appendix D).
179"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.1047783534830167,"Handling the black-box setting. We then consider the more realistic black-box setting, where
180"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10535405872193437,"the defender has no access to the number of malicious devices and their actual attack types,
181"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10592976396085205,"nor the non-i.i.d. level of local data distributions. To obtain a robust defense, we assume the
182"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10650546919976972,"server considers the worst-case scenario based on a rough estimate of the missing information
183"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.1070811744386874,"(see our ablation study in the experiment section) and adopts the RL-based attacks to simulate
184"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10765687967760507,"the worst-case attacks (see Section 3.1) when the attack is unknown or adaptive. In the face of
185"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10823258491652274,"an unknown backdoor attack, the defender does not know the backdoor triggers and targeted la-
186"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10880829015544041,"bels. To simulate a backdoor attacker’s behavior, we first implement multiple GAN-based attack
187"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10938399539435809,"models as in [12] to generate worst-case triggers (which maximizes attack performance given the
188"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.10995970063327576,"backdoor objective) in the simulated environment. Since the defender does not know the poi-
189"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.11053540587219343,"soning ratio ρi and the target label of the attacker’s poisoned dataset (needed to determine the
190"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.1111111111111111,"attack objective F ′), we approximate the attacker’s reward function by rt
A = −F ′′( bwt+1
g
), where
191"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.11168681635002879,F ′′(w) := minc∈C[ 1
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.11226252158894647,"M1
PM1
i=1
1
|D′
i|
P|D′
i|
j=1 ℓ(w, (ˆxj
i, c))] −
1
M2
PM
i=M1+1 f(ω, Di). F ′′ differs F ′
192"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.11283822682786414,"only in the first M1 clients, where we use a strong target label (that minimizes the expected loss) as a
193"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.11341393206678181,"surrogate to the true label c∗. We compare the defense performance against white-box and black-box
194"
PRE-TRAINING AS A BAYESIAN STACKELBERG MARKOV GAME,0.11398963730569948,"backdoor attacks ( see Figure 10 in Appendix D).
195"
META STACKELBERG LEARNING,0.11456534254461716,"3
Meta Stackelberg Learning
196"
META STACKELBERG LEARNING,0.11514104778353483,"Since the pre-training is modeled by a Bayesian Markov Stackelberg game, solving the game
197"
META STACKELBERG LEARNING,0.1157167530224525,"efficiently is crucial to a successful defense. This work’s main contribution includes the formulation
198"
META STACKELBERG LEARNING,0.11629245826137018,"of a new solution concept to the game, meta-Stackelberg equilibrium (meta-SE), and a learning
199"
META STACKELBERG LEARNING,0.11686816350028785,"algorithm to approximate such equilibrium in finite time. To motivate the proposed concept, we begin
200"
META STACKELBERG LEARNING,0.11744386873920552,"by addressing the defense against non-adaptive attacks.
201"
META STACKELBERG LEARNING,0.1180195739781232,"Consider the attacker employing a non-adaptive attack of type ξ; in other words, the attack action at
202"
META STACKELBERG LEARNING,0.11859527921704087,"each iteration is determined by a fixed attack strategy πξ, where πξ(a) gives the probability of taken
203"
META STACKELBERG LEARNING,0.11917098445595854,"action a ∈Aξ, independent of the FL training and the defense strategy. In this case, BSMG reduces
204"
META STACKELBERG LEARNING,0.11974668969487623,"to an MDP, where the transition kernel is Tξ(·|s, aD) ≜
R"
META STACKELBERG LEARNING,0.1203223949337939,"Aξ T (·|s, aA, aD)dπξ(aA). Parameterizing
205"
META STACKELBERG LEARNING,0.12089810017271158,"the defender’s policy πD(at
D|st; θ) by a neural network with model weights θ ∈Θ, the solution
206"
META STACKELBERG LEARNING,0.12147380541162925,"to the following optimization problem maxθ∈Θ Eat
D∼πD,st∼Tξ[PH
t=1 γtrt
D] ≜JD(θ, ξ) gives the
207"
META STACKELBERG LEARNING,0.12204951065054692,"optimal defense against the non-adaptive attack. When the actual attack in the online stage falls
208"
META STACKELBERG LEARNING,0.1226252158894646,"within Ξ, which the defender is uncertain of, one can consider the defense against the expected attack:
209"
META STACKELBERG LEARNING,0.12320092112838227,"maxθ Eξ∼QJD(θ, ξ), where Q is a distribution over the attack domain to be designed by the defender.
210"
META STACKELBERG LEARNING,0.12377662636729994,"One intuitive design is to include all reported attack methods in history as the attack domain and their
211"
META STACKELBERG LEARNING,0.12435233160621761,"empirical frequency as the Q distribution.
212"
META STACKELBERG LEARNING,0.12492803684513529,"In stark contrast to non-adaptive attacks, an adaptive attack can adjust attack actions to the FL
213"
META STACKELBERG LEARNING,0.12550374208405296,"environment and the defense mechanism [31, 32]. Most existing attacks are history-independent [50,
214"
META STACKELBERG LEARNING,0.12607944732297063,"65]. Hence, we assume that an adaptive attack takes the current state (global model) as input, i.e., the
215"
META STACKELBERG LEARNING,0.1266551525618883,"attack policy is a Markov policy denoted by πA(at
A|st). Denoted by ξ the attack type; then, an optimal
216"
META STACKELBERG LEARNING,0.12723085780080598,"adaptive attack policy, parameterized by ϕ, is the best response to the existing defense πD(·|st; θ):
217"
META STACKELBERG LEARNING,0.12780656303972365,"ϕ ∈arg max Eat
A∼πξ,at
D∼πD[PH
t=1 γtrt
ξ] ≜JA(θ, ϕ, ξ). Denote by ϕ∗
ξ the maximizer, and then, the
218"
META STACKELBERG LEARNING,0.12838226827864133,"defender’s cumulative rewards under such attack is JD(θ, ϕ∗
ξ, ξ) ≜Eat
A∼πξ,at
D∼πD[PH
t=1 γtrt
D].
219"
RL-BASED ATTACKS AND DEFENSES,0.128957973517559,"3.1
RL-based attacks and defenses
220"
RL-BASED ATTACKS AND DEFENSES,0.12953367875647667,"The actual attack type (which could be either adaptive or non-adaptive) encountered in the online
221"
RL-BASED ATTACKS AND DEFENSES,0.13010938399539435,"phase may be not in Ξ and thus unknown to the defender. To prepare for these unknown attacks,
222"
RL-BASED ATTACKS AND DEFENSES,0.13068508923431202,"we propose to use multiple RL-based attacks with different objectives, adapted from RL-based
223"
RL-BASED ATTACKS AND DEFENSES,0.13126079447322972,"untargeted model poising attack [31] and RL-based backdoor attack [32], as surrogates for unknown
224"
RL-BASED ATTACKS AND DEFENSES,0.1318364997121474,"attacks, which are added to the attack domain for pre-training. The rationale behind the RL surrogates
225"
RL-BASED ATTACKS AND DEFENSES,0.13241220495106507,"includes: (1) they achieve strong attack performance by optimizing long-term objectives; (2) they
226"
RL-BASED ATTACKS AND DEFENSES,0.13298791018998274,"adopt the most general action space (i.e., model updates), which allows them to mimic any adaptive
227"
RL-BASED ATTACKS AND DEFENSES,0.1335636154289004,"or non-adaptive attacks given the corresponding objectives; (3) they are flexible enough to incorporate
228"
RL-BASED ATTACKS AND DEFENSES,0.13413932066781808,"multiple attack methods by using RL to tune the hyper-parameters of a mixture of attacks. A similar
229"
RL-BASED ATTACKS AND DEFENSES,0.13471502590673576,"argument applies to RL-based defenses. We remark that in this paper, an RL-based attack (defense)
230"
RL-BASED ATTACKS AND DEFENSES,0.13529073114565343,"is not a single attack (defense) as in [31, 32] but a systematically synthesized combination of existing
231"
RL-BASED ATTACKS AND DEFENSES,0.1358664363845711,"attacks (defenses). In the simulated environment, we train our defense against the strongest white-box
232"
RL-BASED ATTACKS AND DEFENSES,0.13644214162348878,"RL attacks in [31, 32] with different objectives (e.g., untargeted or targeted), which is considered the
233"
RL-BASED ATTACKS AND DEFENSES,0.13701784686240645,"optimal attack strategy. The “worst-case” scenario is commonly used in security scenarios to ensure
234"
RL-BASED ATTACKS AND DEFENSES,0.13759355210132412,"the associated defense has performance guarantees under “weaker” attacks with similar objectives.
235"
RL-BASED ATTACKS AND DEFENSES,0.1381692573402418,"Such a robust defense policy gives us a good starting point to further adapt to uncertain or unknown
236"
RL-BASED ATTACKS AND DEFENSES,0.13874496257915947,"attacks. Our defense is generalizable to other adaptive attacks (see Table 8 in Appendix D). The key
237"
RL-BASED ATTACKS AND DEFENSES,0.13932066781807714,"novelty of our RL-based defense is that instead of using a fixed and hand-crafted algorithm as in
238"
RL-BASED ATTACKS AND DEFENSES,0.13989637305699482,"existing approaches, we use RL to optimize the policy network πD(at
D|st; θ). Similar to RL-based
239"
RL-BASED ATTACKS AND DEFENSES,0.1404720782959125,"attacks, the most general action space could be the set of global model parameters. However, the
240"
RL-BASED ATTACKS AND DEFENSES,0.14104778353483016,"high dimensional action space will lead to an extremely large search space that is prohibitive in terms
241"
RL-BASED ATTACKS AND DEFENSES,0.14162348877374784,"of training time and memory space. Thus, we apply compression techniques (see Appendix C) to
242"
RL-BASED ATTACKS AND DEFENSES,0.1421991940126655,"reduce the action from high-dimensional space to a 3-dimensional space. Note that the execution
243"
RL-BASED ATTACKS AND DEFENSES,0.14277489925158318,"of our defense policy is lightweight, without using any extra data for evaluation/validation. See the
244"
RL-BASED ATTACKS AND DEFENSES,0.14335060449050085,"discussion in Appendix C on how we apply our RL-based defense during online adaptation.
245"
META-STACKELBERG EQUILIBRIUM,0.14392630972941853,"3.2
Meta-Stackelberg equilibrium
246"
META-STACKELBERG EQUILIBRIUM,0.1445020149683362,"As discussed in Section 2.2, one of the key challenges to solving the BSMG is the defender’s
247"
META-STACKELBERG EQUILIBRIUM,0.14507772020725387,"incomplete information on attack types. Prior works have explored a Bayesian equilibrium approach
248"
META-STACKELBERG EQUILIBRIUM,0.14565342544617155,"to address this issue [52]. Given the set of possible attacks Ξ that the defender is aware of and a
249"
META-STACKELBERG EQUILIBRIUM,0.14622913068508925,"prior distribution Q over the domain, the Bayesian Stackelberg equilibrium (BSE) is given by the
250"
META-STACKELBERG EQUILIBRIUM,0.14680483592400692,"following bi-level optimization:
251"
META-STACKELBERG EQUILIBRIUM,0.1473805411629246,"max
θ∈Θ Eξ∼Q[JD(θ, ϕ∗
ξ, ξ)]
s.t. ϕ∗
ξ ∈arg max JA(θ, ϕ, ξ).
(BSE)"
META-STACKELBERG EQUILIBRIUM,0.14795624640184227,"In (BSE), unaware of the exact attacker type, the defender (the leader) aims to maximize the defense
252"
META-STACKELBERG EQUILIBRIUM,0.14853195164075994,"performance against an average of all attack types, anticipating their best responses.
253"
META-STACKELBERG EQUILIBRIUM,0.1491076568796776,"From a game-theoretic viewpoint, the Bayesian equilibrium in (BSE) is of ex-ante. The defender
254"
META-STACKELBERG EQUILIBRIUM,0.1496833621185953,"determines its equilibrium strategy only knowing the type distribution Q. However, as the Markov
255"
META-STACKELBERG EQUILIBRIUM,0.15025906735751296,"game proceeds, the attacker’s moves (e.g., malicious global model updates) during the interim stage
256"
META-STACKELBERG EQUILIBRIUM,0.15083477259643063,"(online stage) reveal additional information on the attacker’s private type. This Bayesian equilibrium
257"
META-STACKELBERG EQUILIBRIUM,0.1514104778353483,"defense strategy fails to handle the emerging information on the attacker’s hidden type in the interim
258"
META-STACKELBERG EQUILIBRIUM,0.15198618307426598,"stage, as the policy obtained from (BSE) remains fixed throughout the online stage without adaptation.
259"
META-STACKELBERG EQUILIBRIUM,0.15256188831318365,"To address the limitation of Bayesian equilibrium, we introduce the novel solution concept, meta-
260"
META-STACKELBERG EQUILIBRIUM,0.15313759355210133,"Stackelberg equilibrium (meta-SE), to equip the defender with online responsive intelligence under
261"
META-STACKELBERG EQUILIBRIUM,0.153713298791019,"incomplete information. As a synthesis of meta-learning and Stackelberg equilibrium, the meta-SE
262"
META-STACKELBERG EQUILIBRIUM,0.15428900402993667,"aims to pre-train a meta policy on a variety of attack types sampled from the attack domain Ξ such
263"
META-STACKELBERG EQUILIBRIUM,0.15486470926885434,"that online gradient adaption applied to the base produces a decent defense against the actual attack
264"
META-STACKELBERG EQUILIBRIUM,0.15544041450777202,"in the online environment. Using mathematical terms, we denote by τξ := (sk, ak
D, ak
ξ)H
k=1 the
265"
META-STACKELBERG EQUILIBRIUM,0.1560161197466897,"trajectory of the FL system under type-ξ attacker up to round H, which is subject to the distribution
266"
META-STACKELBERG EQUILIBRIUM,0.15659182498560736,"q(θ, ξ) := QH
t=1 πD(at
D|st; θ)πξ(at
A|st)T (st+1|st, at
D, at
A). Let ˆ∇θJD(τ) be the unbiased estimate
267"
META-STACKELBERG EQUILIBRIUM,0.15716753022452504,"of the policy gradient ∇θJD using the sample trajectory τξ (see Appendix E). Then, a one-step
268"
META-STACKELBERG EQUILIBRIUM,0.1577432354634427,"gradient adaptation using the sample trajectory is given by θ + η∇θJD. Incorporating this gradient
269"
META-STACKELBERG EQUILIBRIUM,0.15831894070236038,"adaptation into (BSE) leads to the proposed meta-SE.
270"
META-STACKELBERG EQUILIBRIUM,0.15889464594127806,"max
θ∈Θ Eξ∼QEτ∼q[JD(θ + η ˆ∇θJD(τ), ϕ∗
ξ, ξ)],"
META-STACKELBERG EQUILIBRIUM,0.15947035118019573,"s.t. ϕ∗
ξ ∈arg max Eτ∼qJA(θ + η ˆ∇θJD(τ), ϕ, ξ).
(meta-SE)"
META-STACKELBERG EQUILIBRIUM,0.1600460564191134,"The idea of adding the gradient adaptation to the equilibrium is inspired by the recent developments
271"
META-STACKELBERG EQUILIBRIUM,0.16062176165803108,"in gradient-based meta-learning [16, 43]. When the attack is non-adaptive, the BSMG reduces to
272"
META-STACKELBERG EQUILIBRIUM,0.16119746689694875,"MDP problem, as delineated at the beginning of this section. Consequently, (meta-SE) turns into
273"
META-STACKELBERG EQUILIBRIUM,0.16177317213586645,"the standard form of meta-learning [16]. Unlike the conventional (BSE), the solution to (meta-SE
274"
META-STACKELBERG EQUILIBRIUM,0.16234887737478412,"gives the defender a decent defense initialization after pre-training whose gradient adaptation in the
275"
META-STACKELBERG EQUILIBRIUM,0.1629245826137018,"online stage is tailored to type ξ, since the online trajectory follows the distribution q(θ, ξ). The
276"
META-STACKELBERG EQUILIBRIUM,0.16350028785261947,"novelty of (meta-SE) lies in that the leader (defender) determines an optimal adaptation scheme
277"
META-STACKELBERG EQUILIBRIUM,0.16407599309153714,"rather than a policy, which is computed using an online trajectory without knowing the actual type,
278"
META-STACKELBERG EQUILIBRIUM,0.16465169833045482,"creating a data-driven strategic adaptation after the pre-training. Besides equation BSE, Appendix G
279"
META-STACKELBERG EQUILIBRIUM,0.1652274035693725,"also compares the perfect Bayesian equilibrium with the proposed meta-SE, highlighting the latter’s
280"
META-STACKELBERG EQUILIBRIUM,0.16580310880829016,"scalability to complex FL systems.
281"
META-STACKELBERG LEARNING,0.16637881404720783,"3.3
Meta-Stackelberg learning
282"
META-STACKELBERG LEARNING,0.1669545192861255,Algorithm 1 Meta-Stackelberg Learning
META-STACKELBERG LEARNING,0.16753022452504318,"1: Input: the distribution Q(Ξ), initial defense meta
policy θ0, pre-defined attack methods {πξ}ξ∈Ξ, pre-
trained RL attack policies {ϕ0
ξ}ξ∈Ξ, step size param-
eters κD, κA, η, and iterations numbers NA, ND;"
META-STACKELBERG LEARNING,0.16810592976396085,"2: Output: θND;
3: for iteration t = 0 to ND −1 do
4:
if meta-RL (for non-adaptive) then
5:
Sample a batch of K attack types ξ from Ξ;
6:
Estimate ˆ∇JD(ξ) := ˆ∇θJD(θ, πξ, ξ)|θ=θt
ξ;
7:
end if
8:
if meta-SG then
9:
Sample a batch of K attack types ξ ∈Ξ;
10:
for each sampled attack ξ do
11:
Apply one-step adaptation
θt
ξ ←θt + η ˆ∇θJD(θt, ϕt
ξ, ξ);
12:
ϕt
ξ(0) ←ϕt
ξ;
13:
for iteration k = 0, . . . , NA −1 do
14:
ϕt
ξ(k + 1) ←ϕt
ξ(k)+
15:
κA ˆ∇ϕJA(θt
ξ, ϕt
ξ(k), ξ);
16:
end for
17:
ˆ∇JD(ξ) ←ˆ∇θJD(θ, ϕt
ξ(NA), ξ)|θ=θt
ξ;
18:
end for
19:
end if
20:
θt+1 ←θtκD/K P"
META-STACKELBERG LEARNING,0.16868163500287853,"ξ ˆ∇JD(ξ)
21: end for"
META-STACKELBERG LEARNING,0.1692573402417962,"Unlike finite Stackelberg Markov games that
283"
META-STACKELBERG LEARNING,0.16983304548071387,"can be solved (approximately) using mixed-
284"
META-STACKELBERG LEARNING,0.17040875071963155,"integer programming [59] or Q-learning [52],
285"
META-STACKELBERG LEARNING,0.17098445595854922,"our BSMG admits high-dimensional continu-
286"
META-STACKELBERG LEARNING,0.1715601611974669,"ous state and action spaces, posing a more chal-
287"
META-STACKELBERG LEARNING,0.17213586643638457,"lenging computation issue. Hence, we resort
288"
META-STACKELBERG LEARNING,0.17271157167530224,"to a two-timescale policy gradient (PG) algo-
289"
META-STACKELBERG LEARNING,0.1732872769142199,"rithm, referred to as meta-Stackelberg learning
290"
META-STACKELBERG LEARNING,0.17386298215313759,"(meta-SL) presented in Algorithm 1, to solve
291"
META-STACKELBERG LEARNING,0.17443868739205526,"for the meta-SE in a similar vein to [33]. In
292"
META-STACKELBERG LEARNING,0.17501439263097293,"plain words, meta-SL first learns the attacker’s
293"
META-STACKELBERG LEARNING,0.1755900978698906,"best response at a fast scale (lines 13-15), based
294"
META-STACKELBERG LEARNING,0.17616580310880828,"on which it updates the defender’s meta pol-
295"
META-STACKELBERG LEARNING,0.17674150834772595,"icy at a slow scale at each iteration using ei-
296"
META-STACKELBERG LEARNING,0.17731721358664365,"ther debiased meta-learning [14] or reptile [43].
297"
META-STACKELBERG LEARNING,0.17789291882556132,"The two-timescale meta-SL alleviates the non-
298"
META-STACKELBERG LEARNING,0.178468624064479,"stationarity caused by concurrent policy updates
299"
META-STACKELBERG LEARNING,0.17904432930339667,"from both players [70]. Of particular note is
300"
META-STACKELBERG LEARNING,0.17962003454231434,"that the debiased meta-learning involves Hes-
301"
META-STACKELBERG LEARNING,0.18019573978123202,"sian computation when evaluating the gradient
302"
META-STACKELBERG LEARNING,0.1807714450201497,"of the defender’s objective function since the
303"
META-STACKELBERG LEARNING,0.18134715025906736,"attacker’s best response ϕ∗
ξ(θ) also depends on
304"
META-STACKELBERG LEARNING,0.18192285549798504,"θ. In contrast, reptile uses a first-order approx-
305"
META-STACKELBERG LEARNING,0.1824985607369027,"imation to avoid Hessian. The mathematical
306"
META-STACKELBERG LEARNING,0.18307426597582038,"subties between two policy gradient estimations
307"
META-STACKELBERG LEARNING,0.18364997121473806,"are deferred to the Appendix E.
308"
META-STACKELBERG LEARNING,0.18422567645365573,"The rest of this subsection addresses the com-
309"
META-STACKELBERG LEARNING,0.1848013816925734,"putation expense of the proposed meta-SL. We begin with an alternative solution concept for
310"
META-STACKELBERG LEARNING,0.18537708693149108,"our first-order gradient algorithm, which is slightly weaker than (meta-SE). Let LD(θ, ϕ, ξ) ≜
311"
META-STACKELBERG LEARNING,0.18595279217040875,"Eτ∼qJD(θ + η ˆ∇θJD(τ), ϕ, ξ), LA(θ, ϕ, ξ) ≜Eτ∼qJA(θ + ˆ∇θJD(τ), ϕ, ξ), for a fixed type ξ ∈Ξ.
312"
META-STACKELBERG LEARNING,0.18652849740932642,"In the sequel, we will assume LD and LA to be continuously twice differentiable and Lipschitz-
313"
META-STACKELBERG LEARNING,0.1871042026482441,"smooth with respect to both θ and ϕ as in [33], see Appendix F.
314"
META-STACKELBERG LEARNING,0.18767990788716177,"Definition 3.1. For ε ∈(0, 1), a pair (θ∗, {ϕ∗
ξ}ξ∈Ξ) ∈Θ × Φ|Ξ| is a ε-meta First-Order Stackelbeg
315"
META-STACKELBERG LEARNING,0.18825561312607944,"Equilibrium (ε-meta-FOSE) of the meta-SG if it satisfies the following conditions: for ξ ∈Ξ,
316"
META-STACKELBERG LEARNING,0.1888313183649971,"maxθ∈B(θ∗)⟨∇θLD(θ∗, ϕ∗
ξ, ξ), θ −θ∗⟩≤ε, maxϕ∈B(ϕ∗
ξ)⟨∇ϕLA(θ∗, ϕ∗
ξ, ξ), ϕ −ϕ∗
ξ⟩≤ε, where
317"
META-STACKELBERG LEARNING,0.1894070236039148,"B(θ∗) := {θ ∈Θ : ∥θ −θ∗∥≤1}, and B(ϕ∗
ξ) := {ϕ ∈Φ : ∥ϕ −ϕ∗
ξ∥≤1}.
318"
META-STACKELBERG LEARNING,0.18998272884283246,"Definition 3.1 contains the necessary equilibrium condition for meta-SE in (meta-SE), which can be
319"
META-STACKELBERG LEARNING,0.19055843408175013,"reduced to ∥∇θLD(θ∗, ϕξ, ξ)∥≤ε and ∥∇ϕLA(θ∗, ϕξ, ξ)∥≤ε in the unconstraint settings. Since
320"
META-STACKELBERG LEARNING,0.1911341393206678,"we utilize stochastic gradient in practice, all inequalities mentioned above shall be considered in
321"
META-STACKELBERG LEARNING,0.19170984455958548,"expectation. The existence of meta-FOSE is guaranteed Theorem F.1 in Appendix F.
322"
META-STACKELBERG LEARNING,0.19228554979850315,"Since the value functions JA, JD are nonconvex, we impose a regularity assumption adapted from
323"
META-STACKELBERG LEARNING,0.19286125503742085,"the Polyak-Łojasiewicz (PL) condition [26], which is customary in nonconvex analysis. Despite the
324"
META-STACKELBERG LEARNING,0.19343696027633853,"lack of theoretical justifications for the PL condition in the literature, [33] empirically demonstrates
325"
META-STACKELBERG LEARNING,0.1940126655152562,"that the cumulative rewards in meta-reinforcement learning satisfy the PL condition, see Figure 4
326"
META-STACKELBERG LEARNING,0.19458837075417387,"Appendix D therein. Assumption 3.2 subsequently leads to the main result in Theorem 3.3
327"
META-STACKELBERG LEARNING,0.19516407599309155,"Assumption 3.2 (Stackelberg Polyak-Łojasiewicz condition). There exists a positive constant µ such
328"
META-STACKELBERG LEARNING,0.19573978123200922,"that for any (θ, ϕ) ∈Θ × Φ and ξ ∈Ξ, the following inequalities hold:
1
2µ∥∇ϕLD(θ, ϕ, ξ)∥2 ≥
329"
META-STACKELBERG LEARNING,0.1963154864709269,"maxϕ LD(θ, ϕ, ξ) −LD(θ, ϕ, ξ),
1
2µ∥∇ϕLA(θ, ϕ, ξ)∥2 ≥maxϕ LA(θ, ϕ, ξ) −LA(θ, ϕ, ξ).
330"
META-STACKELBERG LEARNING,0.19689119170984457,"Theorem 3.3. Under assumption 3.2 and other regularity assumptions in Appendix F, for any
331"
META-STACKELBERG LEARNING,0.19746689694876224,"given ε ∈(0, 1), let the learning rates κA and κD be properly chosen; let NA ∼O(log ϵ−1) and
332"
META-STACKELBERG LEARNING,0.1980426021876799,"Nb ∼O(ϵ−4) be properly chosen (Appendix F), then, Algorithm 1 finds a ε-meta-FOSE within
333"
META-STACKELBERG LEARNING,0.19861830742659758,"ND ∼O(ε−2) iterations.
334"
META-STACKELBERG LEARNING,0.19919401266551526,"Finally, we conclude this section by analyzing the meta-SG defense’s generalization ability when
335"
META-STACKELBERG LEARNING,0.19976971790443293,"the learned meta policy is exposed to attacks unseen in the pre-training. Proposition 3.4 asserts that
336"
META-STACKELBERG LEARNING,0.2003454231433506,"meta-SG is generalizable to the unseen attacks, given that the unseen is not distant from those seen.
337"
META-STACKELBERG LEARNING,0.20092112838226828,"The formal statement is deferred to Appendix F.
338"
META-STACKELBERG LEARNING,0.20149683362118595,"Proposition 3.4. Consider sampled attack types ξ1, . . . , ξm during the pre-training and the unseen
339"
META-STACKELBERG LEARNING,0.20207253886010362,"attack type ξm+1 in the online stage. The generalization error is upper-bounded by the “discrepancy”
340"
META-STACKELBERG LEARNING,0.2026482440990213,"between the unseen and the seen attacks C(ξm+1, {ξi}m
i=1).
341"
EXPERIMENTS,0.20322394933793897,"4
Experiments
342"
EXPERIMENT SETTINGS,0.20379965457685664,"4.1
Experiment Settings
343"
EXPERIMENT SETTINGS,0.20437535981577432,"Dataset. Our experiments are conducted on MNIST [30] and CIFAR-10 [28] datasets with a CNN
344"
EXPERIMENT SETTINGS,0.204951065054692,"classifier and ResNet-18 model respectively (see Appendix C for details). We consider horizontal FL
345"
EXPERIMENT SETTINGS,0.20552677029360966,"and adopt the approach introduced in [15] to measure the diversity of local data distributions among
346"
EXPERIMENT SETTINGS,0.20610247553252733,"clients. Let the dataset encompass C classes, such as C = 10 for datasets like MNIST and CIFAR-10.
347"
EXPERIMENT SETTINGS,0.206678180771445,"Client devices are divided into C groups (with M attackers evenly distributed among these groups).
348"
EXPERIMENT SETTINGS,0.20725388601036268,"Each group is allocated 1/C of the training samples in the following manner: a training instance
349"
EXPERIMENT SETTINGS,0.20782959124928038,"labeled as c is assigned to the c-th group with a probability of q ≥1/C, while being assigned to
350"
EXPERIMENT SETTINGS,0.20840529648819806,"every other group with a probability of (1 −q)/(C −1). Within each group, instances are evenly
351"
EXPERIMENT SETTINGS,0.20898100172711573,"distributed among clients. A higher value of q signifies a greater non-i.i.d. level. By default, we
352"
EXPERIMENT SETTINGS,0.2095567069660334,"set q = 0.5 as the standard non-i.i.d. level. We assume the server holds a small amount of root
353"
EXPERIMENT SETTINGS,0.21013241220495107,"data randomly sampled from the the collection of all client dataset U. (100 for MNIST and 200 for
354"
EXPERIMENT SETTINGS,0.21070811744386875,"CIFAR-10).
355"
EXPERIMENT SETTINGS,0.21128382268278642,"Baseline. We evaluate our meta-RL and meta-SG defenses under the following untargeted model
356"
EXPERIMENT SETTINGS,0.2118595279217041,"poisoning attacks including IPM [68] (with scaling factor 2), LMP [15], RL [31], and backdoor
357"
EXPERIMENT SETTINGS,0.21243523316062177,"attacks including BFL [2] (with poisoning ratio 1), DBA [67] (with 4 sub-triggers evenly distributed
358"
EXPERIMENT SETTINGS,0.21301093839953944,"to malicious clients and poisoning ratio 0.5), BRL [32], and a mix of attacks from the two categories
359"
EXPERIMENT SETTINGS,0.2135866436384571,"(see Table 2 for all attacks’ categories in Appendix C). We consider various strong defenses as
360"
EXPERIMENT SETTINGS,0.2141623488773748,"baselines, including training-stage defenses such as Coordinate-wise trimmed mean/median [69],
361"
EXPERIMENT SETTINGS,0.21473805411629246,"Norm bounding [57], FLTrust [10], Krum [7], and post-training stage defenses such as NeuroClip [62]
362"
EXPERIMENT SETTINGS,0.21531375935521013,"and Prun [64] and the selected combination of them. We utilize the Twin Delayed DDPG (TD3) [18]
363"
EXPERIMENT SETTINGS,0.2158894645941278,"algorithm to train both attacker’s and defender’s policies. We use the following default parameters:
364"
EXPERIMENT SETTINGS,0.21646516983304548,"number of devices = 100, number of malicious clients for untargeted model poisoning attack = 10,
365"
EXPERIMENT SETTINGS,0.21704087507196315,"number of malicious clients for backdoor attack = 5 (20 for DBA), client subsampling rate = 10%,
366"
EXPERIMENT SETTINGS,0.21761658031088082,"number of FL epochs = 500 (1000) for MNIST (CIFAR-10). We fix the initial model and the
367"
EXPERIMENT SETTINGS,0.2181922855497985,"random seeds for client subsampling and local data sampling for fair comparisons. The details of the
368"
EXPERIMENT SETTINGS,0.21876799078871617,"experiment setup and additional results are provided in Appendices C and D.
369"
EXPERIMENT RESULTS,0.21934369602763384,"4.2
Experiment Results
370"
EXPERIMENT RESULTS,0.21991940126655152,"Acc/Bac
FedAvg
Trimed Mean
FLTrust
ClipMed
FLTrust+NC
Meta-RL (ours)"
EXPERIMENT RESULTS,0.2204951065054692,"NA
0.7082/0.1
0.7093/0.1078
0.7139/0.1066
0.5280/0.1212
0.7100/0.1061
0.7053/0.0999
IPM
0.1369/0.0312
0.6542/0.1174
0.6828/0.1054
0.5172/0.1220
0.6656/0.0971
0.6862/0.0637
LMP
0.1115/0.1174
0.6224/0.1033
0.7071/0.099
0.5144/0.121
0.7075/0.104
0.7109/0.037
BFL
0.7137/1.0
0.7034/1.0
0.7145/1.0
0.5198/0.5337
0.7100/0.1061
0.7106/0.0143
DBA
0.7007/0.7815
0.6904/0.7737
0.7010/0.8048
0.4935/0.6261
0.6618/0.9946
0.6699/0.2838
IPM+BFL
0.3104/0.8222
0.6415/1.0
0.6911/1.0
0.5097/0.5776
0.6817/0.0267
0.6949/0.0025
LMP+DBA
0.1124/0.1817
0.6444/0.7311
0.7007/0.7620
0.4841/0.6342
0.6032/0.8422
0.6934/0.2136
Table 1: Comparisons of average global model accuracy (acc: higher the better) and backdoor
accuracy (bac: lower the better) after 500 rounds under single/multiple type attacks on CIFAR-10.
All parameters are set as default and random seeds are fixed."
EXPERIMENT RESULTS,0.22107081174438686,"Effectiveness against single/multiple type of attacks. We examine the defense performance of
371"
EXPERIMENT RESULTS,0.22164651698330454,"our meta-RL compared with other defense combinations in Table 1 based on average global model
372"
EXPERIMENT RESULTS,0.2222222222222222,"accuracy after 500 FL rounds on CIFAR-10, which measures the success of defense and learning
373"
EXPERIMENT RESULTS,0.22279792746113988,"speed ignoring the randomness influence (corner-case updates, bias data, etc.) at the bargaining stage
374"
EXPERIMENT RESULTS,0.22337363270005758,"of FL. The meta-RL first learns a meta-defense policy from the attack domain involving {NA, IPM,
375"
EXPERIMENT RESULTS,0.22394933793897526,"Figure 2: Comparisons of defenses against untargeted model poisoning attacks (i.e., LMP and RL) on MNIST
and CIFAR-10. All parameters are set as default and random seeds are fixed."
EXPERIMENT RESULTS,0.22452504317789293,"LMP, BFL, DBA}, then adapts it to the real single/mixed attack. We observe that multiple types
376"
EXPERIMENT RESULTS,0.2251007484168106,"of attacks may intervene with each other (e.g., IPM+BFL, LMP+DBA), which makes it impossible
377"
EXPERIMENT RESULTS,0.22567645365572828,"to manually address the entangled attacks. It is not surprising to see FedAvg [39] and defenses
378"
EXPERIMENT RESULTS,0.22625215889464595,"specifically designed for untargeted attacks (i.e., Trimmed mean, FLTrust) fail to defend backdoor
379"
EXPERIMENT RESULTS,0.22682786413356362,"attacks (i.e., BFL, DBA) due to the huge deviation of defense objective from the optimum. For
380"
EXPERIMENT RESULTS,0.2274035693724813,"a fair comparison, we further manually tune the norm threshold (more results in Appendix D)
381"
EXPERIMENT RESULTS,0.22797927461139897,"from [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1] for ClipMed (i.e., Norm bounding + Coordinate-wise Median)
382"
EXPERIMENT RESULTS,0.22855497985031664,"and clipping range from [2 : 2 : 10] for FLTrust + NeuroClip to achieve the best performance to
383"
EXPERIMENT RESULTS,0.22913068508923431,"balance the global model and backdoor accuracy in linear form (i.e., Acc - Bac). Intuitively, a tight
384"
EXPERIMENT RESULTS,0.229706390328152,"threshold/range has better performance in defending against backdoor attacks, yet will hinder or even
385"
EXPERIMENT RESULTS,0.23028209556706966,"damage the FL progress. On the other hand, a loose threshold/range fails to defend backdoor injection.
386"
EXPERIMENT RESULTS,0.23085780080598733,"Nevertheless, manually tuning in real-world FL scenarios is nearly impossible due to the limited
387"
EXPERIMENT RESULTS,0.231433506044905,"knowledge of the ongoing environment and the presence of asymmetric adversarial information.
388"
EXPERIMENT RESULTS,0.23200921128382268,"Instead of suffering from the above concerns and exponential growth of parameter combination
389"
EXPERIMENT RESULTS,0.23258491652274035,"possibilities, our data-driven meta-RL approach can automatically tune multiple parameters at each
390"
EXPERIMENT RESULTS,0.23316062176165803,"round. Targeting the cumulative defense rewards, the RL approach naturally holds more flexibility
391"
EXPERIMENT RESULTS,0.2337363270005757,"than myopic optimization.
392"
EXPERIMENT RESULTS,0.23431203223949337,"Adaptation to uncertain/unknown attacks. To evaluate the necessity and efficiency of adaptation
393"
EXPERIMENT RESULTS,0.23488773747841105,"from the meta-SG policy in the face of unknown attacks, we plot the global model accuracy graph
394"
EXPERIMENT RESULTS,0.23546344271732872,"over FL epochs. The meta-RL pre-trained from non-adaptive attack domain {NA, IPM, LMP, BFL,
395"
EXPERIMENT RESULTS,0.2360391479562464,"DBA} (RL attack is unknown), while meta-SG pre-train from interacting with a group of RL attacks
396"
EXPERIMENT RESULTS,0.23661485319516407,"initially target on {FedAvg, Coordinate-wise Median, Norm bounding, Krum, FLTrust } (LMP is
397"
EXPERIMENT RESULTS,0.23719055843408174,"unknown). The meta-SG plus (i.e., meta-SG+) is a pre-trained model from the combined attack
398"
EXPERIMENT RESULTS,0.2377662636729994,"domain of the above two. All three defenses then adapt to the real FL environments under LMP or RL
399"
EXPERIMENT RESULTS,0.23834196891191708,"attacks. As shown in Figure 2, the meta-SG can quickly adapt to both uncertain RL-based adaptive
400"
EXPERIMENT RESULTS,0.23891767415083479,"attack (attack action is time-varying during FL) and unknown LMP attack, while meta-RL can only
401"
EXPERIMENT RESULTS,0.23949337938975246,"slowly adapt to or fail to adapt to the unseen RL-based adaptive attacks on MNIST and CIFAT-10
402"
EXPERIMENT RESULTS,0.24006908462867013,"respectively. In addition, the first and the third Figures in Figure 2 demonstrate the power of meta-SG
403"
EXPERIMENT RESULTS,0.2406447898675878,"against unknown LMP attacks, even if LMP is not directly used during its pre-training stage. The
404"
EXPERIMENT RESULTS,0.24122049510650548,"results are only slightly worse than meta-SG plus, where LMP is seen during pre-training. Similar
405"
EXPERIMENT RESULTS,0.24179620034542315,"observations are given under IPM in Appendix D.
406"
CONCLUSION,0.24237190558434082,"5
Conclusion
407"
CONCLUSION,0.2429476108232585,"We have proposed a meta-Stackelberg framework to tackle attacks of uncertain or unknown types in
408"
CONCLUSION,0.24352331606217617,"federated learning through data-driven adaptation. The proposed meta-Stackelberg learning approach
409"
CONCLUSION,0.24409902130109384,"is computationally tractable and strategically adaptable, targeting mixed and adaptive attacks under
410"
CONCLUSION,0.24467472654001152,"incomplete information. The major limitation of our current approach pertains to privacy concerns.
411"
CONCLUSION,0.2452504317789292,"Our current simulation necessitates that the defender either accesses a small portion of root data or
412"
CONCLUSION,0.24582613701784686,"learns clients’ data through inversion, which slightly violates the privacy principles of FL. To minimize
413"
CONCLUSION,0.24640184225676454,"privacy risks, we train our meta-policy in a simulated environment and apply data augmentation to
414"
CONCLUSION,0.2469775474956822,"blur the learned data. In our experiments, the current “black-box” setting operates under certain
415"
CONCLUSION,0.24755325273459988,"conditions: we test only one or a few agnostic variables at a time while leaving other information
416"
CONCLUSION,0.24812895797351756,"known to the defender (see Appendix D). In our future work, we plan to incorporate additional
417"
CONCLUSION,0.24870466321243523,"state-of-the-art defense algorithms to counter more potent attacks, such as edge-case attacks [63], as
418"
CONCLUSION,0.2492803684513529,"well as other attack types, such as privacy-leakage attacks [37]. We will also explore new application
419"
CONCLUSION,0.24985607369027057,"scenarios, including NLP and large generative models. Our framework could be further improved by
420"
CONCLUSION,0.2504317789291883,"including a client-side defense mechanism that closely mirrors real-world scenarios, replacing the
421"
CONCLUSION,0.2510074841681059,"current processes of self-data generation.
422"
REFERENCES,0.2515831894070236,"References
423"
REFERENCES,0.25215889464594127,"[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and
424"
REFERENCES,0.25273459988485897,"Venkatesh Saligrama. Federated learning based on dynamic regularization. In International
425"
REFERENCES,0.2533103051237766,"Conference on Learning Representations, 2020.
426"
REFERENCES,0.2538860103626943,"[2] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How
427"
REFERENCES,0.25446171560161196,"to backdoor federated learning. In International Conference on Artificial Intelligence and
428"
REFERENCES,0.25503742084052966,"Statistics, pages 2938–2948. PMLR, 2020.
429"
REFERENCES,0.2556131260794473,"[3] Pierre Bernhard and Alain Rapaport. On a theorem of Danskin with an application to a theorem
430"
REFERENCES,0.256188831318365,"of Von Neumann-Sion. Nonlinear Analysis: Theory, Methods & Applications, 24(8):1163–1181,
431"
REFERENCES,0.25676453655728265,"1995.
432"
REFERENCES,0.25734024179620035,"[4] Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd
433"
REFERENCES,0.257915947035118,"with majority vote is communication efficient and fault tolerant. In International Conference on
434"
REFERENCES,0.2584916522740357,"Learning Representations(ICLR), 2018.
435"
REFERENCES,0.25906735751295334,"[5] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing
436"
REFERENCES,0.25964306275187105,"federated learning through an adversarial lens. In International Conference on Machine Learn-
437"
REFERENCES,0.2602187679907887,"ing(ICML), 2019.
438"
REFERENCES,0.2607944732297064,"[6] Umang Bhaskar, Yu Cheng, Young Kun Ko, and Chaitanya Swamy. Hardness results for
439"
REFERENCES,0.26137017846862404,"signaling in bayesian zero-sum and network routing games. In Proceedings of the 2016 ACM
440"
REFERENCES,0.26194588370754174,"Conference on Economics and Computation, pages 479–496, 2016.
441"
REFERENCES,0.26252158894645944,"[7] Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine learning with adversaries:
442"
REFERENCES,0.2630972941853771,"Byzantine tolerant gradient descent.
In Advances in Neural Information Processing Sys-
443"
REFERENCES,0.2636729994242948,"tems(NeurIPS), 2017.
444"
REFERENCES,0.26424870466321243,"[8] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir
445"
REFERENCES,0.26482440990213013,"Ivanov, Chloé Kiddon, Jakub Koneˇcný, Stefano Mazzocchi, Brendan McMahan, Timon
446"
REFERENCES,0.2654001151410478,"Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated
447"
REFERENCES,0.2659758203799655,"learning at scale: System design. In Proceedings of Machine Learning and Systems, 2019.
448"
REFERENCES,0.2665515256188831,"[9] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
449"
REFERENCES,0.2671272308578008,"and Wojciech Zaremba. Openai gym, 2016.
450"
REFERENCES,0.26770293609671847,"[10] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust
451"
REFERENCES,0.26827864133563617,"federated learning via trust bootstrapping. In Network and Distributed System Security (NDSS)
452"
REFERENCES,0.2688543465745538,"Symposium, 2021.
453"
REFERENCES,0.2694300518134715,"[11] Katherine
Crowson.
Trains
a
diffusion
model
on
cifar-10
(version
2).
454"
REFERENCES,0.27000575705238916,"https://colab.research.google.com/drive/1IJkrrV-D7boSCLVKhi7t5docRYqORtm3, 2018.
455"
REFERENCES,0.27058146229130686,"[12] Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust
456"
REFERENCES,0.2711571675302245,"backdoor attacks. In Proceedings of the IEEE/CVF International Conference on Computer
457"
REFERENCES,0.2717328727691422,"Vision, pages 11966–11976, 2021.
458"
REFERENCES,0.27230857800805985,"[13] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl 2:
459"
REFERENCES,0.27288428324697755,"Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,
460"
REFERENCES,0.2734599884858952,"2016.
461"
REFERENCES,0.2740356937248129,"[14] Alireza Fallah, Kristian Georgiev, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence
462"
REFERENCES,0.27461139896373055,"theory of debiased model-agnostic meta-reinforcement learning, 2021.
463"
REFERENCES,0.27518710420264825,"[15] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to
464"
REFERENCES,0.2757628094415659,"byzantine-robust federated learning. In 29th USENIX Security Symposium, 2020.
465"
REFERENCES,0.2763385146804836,"[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
466"
REFERENCES,0.27691421991940124,"tation of deep networks. In International conference on machine learning, pages 1126–1135.
467"
REFERENCES,0.27748992515831894,"PMLR, 2017.
468"
REFERENCES,0.27806563039723664,"[17] Drew Fudenberg and Jean Tirole. Game Theory. MIT Press, Cambridge, MA, 1991.
469"
REFERENCES,0.2786413356361543,"[18] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error
470"
REFERENCES,0.279217040875072,"in actor-critic methods. In International conference on machine learning, pages 1587–1596.
471"
REFERENCES,0.27979274611398963,"PMLR, 2018.
472"
REFERENCES,0.28036845135290733,"[19] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. Inverting gradients-
473"
REFERENCES,0.280944156591825,"how easy is it to break privacy in federated learning? Advances in Neural Information Processing
474"
REFERENCES,0.2815198618307427,"Systems, 33:16937–16947, 2020.
475"
REFERENCES,0.2820955670696603,"[20] Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A
476"
REFERENCES,0.282671272308578,"client level perspective. arXiv preprint arXiv:1712.07557, 2017.
477"
REFERENCES,0.28324697754749567,"[21] Yifan Guo, Qianlong Wang, Tianxi Ji, Xufei Wang, and Pan Li. Resisting distributed backdoor
478"
REFERENCES,0.28382268278641337,"attacks in federated learning: A dynamic norm clipping approach. In 2021 IEEE International
479"
REFERENCES,0.284398388025331,"Conference on Big Data (Big Data), pages 1172–1182. IEEE, 2021.
480"
REFERENCES,0.2849740932642487,"[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-
481"
REFERENCES,0.28554979850316636,"age recognition. In Proceedings of the IEEE conference on computer vision and pattern
482"
REFERENCES,0.28612550374208406,"recognition(CVPR), 2016.
483"
REFERENCES,0.2867012089810017,"[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
484"
REFERENCES,0.2872769142199194,"in Neural Information Processing Systems, 33:6840–6851, 2020.
485"
REFERENCES,0.28785261945883706,"[24] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and
486"
REFERENCES,0.28842832469775476,"enhanced design. In International conference on machine learning, pages 4882–4892. PMLR,
487"
REFERENCES,0.2890040299366724,"2021.
488"
REFERENCES,0.2895797351755901,"[25] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Ar-
489"
REFERENCES,0.29015544041450775,"jun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings,
490"
REFERENCES,0.29073114565342545,"et al. Advances and open problems in federated learning. Foundations and Trends® in Machine
491"
REFERENCES,0.2913068508923431,"Learning, 14(1–2):1–210, 2021.
492"
REFERENCES,0.2918825561312608,"[26] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
493"
REFERENCES,0.2924582613701785,"gradient methods under the polyak-łojasiewicz condition. In Machine Learning and Knowledge
494"
REFERENCES,0.29303396660909614,"Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy,
495"
REFERENCES,0.29360967184801384,"September 19-23, 2016, Proceedings, Part I 16, pages 795–811. Springer, 2016.
496"
REFERENCES,0.2941853770869315,"[27] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.
497"
REFERENCES,0.2947610823258492,"Advances in neural information processing systems, 34:21696–21707, 2021.
498"
REFERENCES,0.29533678756476683,"[28] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
499"
REFERENCES,0.29591249280368453,"2009.
500"
REFERENCES,0.2964881980426022,"[29] Artur Lacerda. Pytorch conditional gan. https://github.com/arturml/mnist-cgan, 2018.
501"
REFERENCES,0.2970639032815199,"[30] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
502"
REFERENCES,0.2976396085204375,"applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
503"
REFERENCES,0.2982153137593552,"[31] Henger Li, Xiaolin Sun, and Zizhan Zheng. Learning to attack federated learning: A model-
504"
REFERENCES,0.2987910189982729,"based reinforcement learning attack framework. In Advances in Neural Information Processing
505"
REFERENCES,0.2993667242371906,"Systems, 2022.
506"
REFERENCES,0.2999424294761082,"[32] Henger Li, Chen Wu, Senchun Zhu, and Zizhan Zheng. Learning to backdoor federated learning.
507"
REFERENCES,0.3005181347150259,"arXiv preprint arXiv:2303.03320, 2023.
508"
REFERENCES,0.30109383995394357,"[33] Tao Li, Haozhe Lei, and Quanyan Zhu. Sampling attacks on meta reinforcement learning: A
509"
REFERENCES,0.30166954519286127,"minimax formulation and complexity analysis. arXiv preprint arXiv:2208.00081, 2022.
510"
REFERENCES,0.3022452504317789,"[34] Tao Li, Guanze Peng, Quanyan Zhu, and Tamer Baar. The Confluence of Networks, Games,
511"
REFERENCES,0.3028209556706966,"and Learning a Game-Theoretic Framework for Multiagent Decision Making Over Networks.
512"
REFERENCES,0.30339666090961426,"IEEE Control Systems, 42(4):35–67, 2022.
513"
REFERENCES,0.30397236614853196,"[35] Tao Li, Yuhan Zhao, and Quanyan Zhu. The role of information structures in game-theoretic
514"
REFERENCES,0.3045480713874496,"multi-agent learning. Annual Reviews in Control, 53:296–314, 2022.
515"
REFERENCES,0.3051237766263673,"[36] Tao Li and Quanyan Zhu. On the price of transparency: A comparison between overt persuasion
516"
REFERENCES,0.30569948186528495,"and covert signaling. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages
517"
REFERENCES,0.30627518710420265,"4267–4272, 2023.
518"
REFERENCES,0.3068508923431203,"[37] Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, and S Yu
519"
REFERENCES,0.307426597582038,"Philip. Privacy and robustness in federated learning: Attacks and defenses. IEEE transactions
520"
REFERENCES,0.3080023028209557,"on neural networks and learning systems, 2022.
521"
REFERENCES,0.30857800805987334,"[38] Mohammad Hossein Manshaei, Quanyan Zhu, Tansu Alpcan, Tamer Bac¸sar, and Jean-Pierre
522"
REFERENCES,0.30915371329879104,"Hubaux. Game theory meets network security and privacy. ACM Comput. Surv., 45(3), jul 2013.
523"
REFERENCES,0.3097294185377087,"[39] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
524"
REFERENCES,0.3103051237766264,"Communication-efficient learning of deep networks from decentralized data. In Artificial
525"
REFERENCES,0.31088082901554404,"intelligence and statistics (AISTATS), pages 1273–1282. PMLR, 2017.
526"
REFERENCES,0.31145653425446174,"[40] Yinbin Miao, Ziteng Liu, Hongwei Li, Kim-Kwang Raymond Choo, and Robert H Deng.
527"
REFERENCES,0.3120322394933794,"Privacy-preserving byzantine-robust federated learning via blockchain systems. IEEE Transac-
528"
REFERENCES,0.3126079447322971,"tions on Information Forensics and Security, 17:2848–2861, 2022.
529"
REFERENCES,0.31318364997121473,"[41] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
530"
REFERENCES,0.31375935521013243,"arXiv:1411.1784, 2014.
531"
REFERENCES,0.3143350604490501,"[42] Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen Möllering, Hossein
532"
REFERENCES,0.3149107656879678,"Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza Zeitouni, et al.
533"
REFERENCES,0.3154864709268854,"Flame: Taming backdoors in federated learning. Cryptology ePrint Archive, 2021.
534"
REFERENCES,0.3160621761658031,"[43] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms.
535"
REFERENCES,0.31663788140472077,"arXiv preprint arXiv:1803.02999, 2018.
536"
REFERENCES,0.31721358664363847,"[44] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with
537"
REFERENCES,0.3177892918825561,"auxiliary classifier gans. In International conference on machine learning, pages 2642–2651.
538"
REFERENCES,0.3183649971214738,"PMLR, 2017.
539"
REFERENCES,0.31894070236039146,"[45] Mustafa Safa Ozdayi, Murat Kantarcioglu, and Yulia R Gel. Defending against backdoors in
540"
REFERENCES,0.31951640759930916,"federated learning with robust learning rate. In Proceedings of the AAAI Conference on Artificial
541"
REFERENCES,0.3200921128382268,"Intelligence, volume 35, pages 9268–9276, 2021.
542"
REFERENCES,0.3206678180771445,"[46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
543"
REFERENCES,0.32124352331606215,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
544"
REFERENCES,0.32181922855497985,"style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
545"
REFERENCES,0.3223949337938975,"[47] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated
546"
REFERENCES,0.3229706390328152,"learning. IEEE Transactions on Signal Processing, 70:1142–1154, 2022.
547"
REFERENCES,0.3235463442717329,"[48] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah
548"
REFERENCES,0.32412204951065055,"Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of
549"
REFERENCES,0.32469775474956825,"Machine Learning Research, 22(268):1–8, 2021.
550"
REFERENCES,0.3252734599884859,"[49] Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, and Ahmad-Reza Sadeghi. Deepsight:
551"
REFERENCES,0.3258491652274036,"Mitigating backdoor attacks in federated learning through deep model inspection. arXiv preprint
552"
REFERENCES,0.32642487046632124,"arXiv:2201.00763, 2022.
553"
REFERENCES,0.32700057570523894,"[50] Nuria Rodríguez-Barroso, Daniel Jiménez-López, M Victoria Luzón, Francisco Herrera, and
554"
REFERENCES,0.3275762809441566,"Eugenio Martínez-Cámara. Survey on federated learning threats: Concepts, taxonomy on
555"
REFERENCES,0.3281519861830743,"attacks and defences, experimental study and challenges. Information Fusion, 90:148–173,
556"
REFERENCES,0.32872769142199193,"2023.
557"
REFERENCES,0.32930339666090963,"[51] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.
558"
REFERENCES,0.3298791018998273,"arXiv preprint arXiv:2202.00512, 2022.
559"
REFERENCES,0.330454807138745,"[52] Sailik Sengupta and Subbarao Kambhampati. Multi-agent Reinforcement Learning in Bayesian
560"
REFERENCES,0.3310305123776626,"Stackelberg Markov Games for Adaptive Moving Target Defense. arXiv, 2020.
561"
REFERENCES,0.3316062176165803,"[53] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep
562"
REFERENCES,0.33218192285549797,"learning. Journal of big data, 6(1):1–48, 2019.
563"
REFERENCES,0.33275762809441567,"[54] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness
564"
REFERENCES,0.3333333333333333,"with principled adversarial training. In International Conference on Learning Representations,
565"
REFERENCES,0.333909038572251,"2018.
566"
REFERENCES,0.33448474381116866,"[55] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
567"
REFERENCES,0.33506044905008636,"vised learning using nonequilibrium thermodynamics. In International Conference on Machine
568"
REFERENCES,0.335636154289004,"Learning, pages 2256–2265. PMLR, 2015.
569"
REFERENCES,0.3362118595279217,"[56] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
570"
REFERENCES,0.33678756476683935,"preprint arXiv:2010.02502, 2020.
571"
REFERENCES,0.33736327000575705,"[57] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really
572"
REFERENCES,0.3379389752446747,"backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.
573"
REFERENCES,0.3385146804835924,"[58] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
574"
REFERENCES,0.3390903857225101,"methods for reinforcement learning with function approximation. In Advances in Neural
575"
REFERENCES,0.33966609096142775,"Information Processing Systems 12, pages 1057—1063. MIT press, 2000.
576"
REFERENCES,0.34024179620034545,"[59] Yevgeniy Vorobeychik and Satinder Singh. Computing stackelberg equilibria in discounted
577"
REFERENCES,0.3408175014392631,"stochastic games. Proceedings of the AAAI Conference on Artificial Intelligence, 26(1):1478–
578"
REFERENCES,0.3413932066781808,"1484, Sep. 2021.
579"
REFERENCES,0.34196891191709844,"[60] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and
580"
REFERENCES,0.34254461715601614,"Ben Y. Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.
581"
REFERENCES,0.3431203223949338,"In 2019 IEEE Symposium on Security and Privacy (SP), pages 707–723, 2019.
582"
REFERENCES,0.3436960276338515,"[61] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and
583"
REFERENCES,0.34427173287276913,"Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.
584"
REFERENCES,0.34484743811168683,"In 2019 IEEE Symposium on Security and Privacy (SP), pages 707–723. IEEE, 2019.
585"
REFERENCES,0.3454231433506045,"[62] Hang Wang, Zhen Xiang, David J Miller, and George Kesidis. Mm-bd: Post-training detection
586"
REFERENCES,0.3459988485895222,"of backdoor attacks with arbitrary backdoor pattern types using a maximum margin statistic. In
587"
REFERENCES,0.3465745538284398,"2024 IEEE Symposium on Security and Privacy (SP), pages 15–15. IEEE Computer Society,
588"
REFERENCES,0.3471502590673575,"2023.
589"
REFERENCES,0.34772596430627517,"[63] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal,
590"
REFERENCES,0.34830166954519287,"Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you
591"
REFERENCES,0.3488773747841105,"really can backdoor federated learning. Advances in Neural Information Processing Systems,
592"
REFERENCES,0.3494530800230282,"33:16070–16084, 2020.
593"
REFERENCES,0.35002878526194586,"[64] Chen Wu, Xian Yang, Sencun Zhu, and Prasenjit Mitra. Mitigating backdoor attacks in federated
594"
REFERENCES,0.35060449050086356,"learning. arXiv preprint arXiv:2011.01767, 2020.
595"
REFERENCES,0.3511801957397812,"[65] Geming Xia, Jian Chen, Chaodong Yu, and Jun Ma. Poisoning attacks in federated learning: A
596"
REFERENCES,0.3517559009786989,"survey. IEEE Access, 11:10708–10722, 2023.
597"
REFERENCES,0.35233160621761656,"[66] Chulin Xie, Minghao Chen, Pin-Yu Chen, and Bo Li. Crfl: Certifiably robust federated
598"
REFERENCES,0.35290731145653426,"learning against backdoor attacks. In International Conference on Machine Learning, pages
599"
REFERENCES,0.3534830166954519,"11372–11382. PMLR, 2021.
600"
REFERENCES,0.3540587219343696,"[67] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against
601"
REFERENCES,0.3546344271732873,"federated learning. In International conference on learning representations, 2019.
602"
REFERENCES,0.35521013241220495,"[68] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-
603"
REFERENCES,0.35578583765112265,"tolerant sgd by inner product manipulation. In Uncertainty in Artificial Intelligence (UAI),
604"
REFERENCES,0.3563615428900403,"pages 261–270. PMLR, 2020.
605"
REFERENCES,0.356937248128958,"[69] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
606"
REFERENCES,0.35751295336787564,"learning: Towards optimal statistical rates. In International Conference on Machine Learning,
607"
REFERENCES,0.35808865860679334,"pages 5650–5659. PMLR, 2018.
608"
REFERENCES,0.358664363845711,"[70] Bora Yongacoglu, Gürdal Arslan, and Serdar Yüksel. Asynchronous Decentralized Q-Learning:
609"
REFERENCES,0.3592400690846287,"Two Timescale Analysis By Persistence. arXiv, 2023.
610"
REFERENCES,0.35981577432354633,"[71] Xianyang Zhang, Chen Hu, Bing He, and Zhiguo Han. Distributed reptile algorithm for meta-
611"
REFERENCES,0.36039147956246403,"learning over multi-agent systems. IEEE Transactions on Signal Processing, 70:5443–5456,
612"
REFERENCES,0.3609671848013817,"2022.
613"
REFERENCES,0.3615428900402994,"[72] Chen Zhao, Yu Wen, Shuailou Li, Fucheng Liu, and Dan Meng. Federatedreverse: A detection
614"
REFERENCES,0.362118595279217,"and defense method against backdoor attacks in federated learning. In Proceedings of the 2021
615"
REFERENCES,0.3626943005181347,"ACM Workshop on Information Hiding and Multimedia Security, IH&MMSec ’21, page 51–62,
616"
REFERENCES,0.36327000575705237,"New York, NY, USA, 2021. Association for Computing Machinery.
617"
REFERENCES,0.3638457109959701,"A
Related Works
618"
REFERENCES,0.3644214162348877,"Poisoning/backdoor attacks and defenses in FL
Several defensive strategies against model
619"
REFERENCES,0.3649971214738054,"poisoning attacks broadly fall into two categories. The first category includes robust-aggregation-
620"
REFERENCES,0.36557282671272306,"based defenses encompassing techniques such as dimension-wise filtering. These methods treat
621"
REFERENCES,0.36614853195164077,"each dimension of local updates individually, as explored in studies by [4, 69]. Another strategy is
622"
REFERENCES,0.3667242371905584,"client-wise filtering, aiming to limit or entirely eliminate the influence of clients who might harbor
623"
REFERENCES,0.3672999424294761,"malicious intent. This approach has been examined in the works of [7, 47, 57]. Some defensive
624"
REFERENCES,0.36787564766839376,"methods necessitate the server having access to a minimal amount of root data, as detailed in the
625"
REFERENCES,0.36845135290731146,"study by [10]. Naive backdoor attacks are limited by even simple defenses like norm-bounding
626"
REFERENCES,0.3690270581462291,"[57] and weak differential private [20] defenses. Despite the sophisticated design of state-of-the-art
627"
REFERENCES,0.3696027633851468,"non-addaptive backdoor attacks against federated learning, post-training stage defenses [64, 42, 49]
628"
REFERENCES,0.3701784686240645,"can still effectively erase suspicious neurons/parameters in the backdoored model.
629"
REFERENCES,0.37075417386298215,"Incomplete Information in Adversarial Machine Learning
Prior works have attempted to tackle
630"
REFERENCES,0.37132987910189985,"the challenge of incomplete information through two distinct approaches. The first approach is the
631"
REFERENCES,0.3719055843408175,"“infer-then-counter” approach, where the hidden information regarding the attacks is first inferred
632"
REFERENCES,0.3724812895797352,"through observations. For example, one can infer the backdoor triggers through reverse engineering
633"
REFERENCES,0.37305699481865284,"using model weights [60], based on which the backdoor attacks can be mitigated [72]. The inference
634"
REFERENCES,0.37363270005757054,"helps adapt the defense to the present malicious attacks. However, this inference-based adaptation
635"
REFERENCES,0.3742084052964882,"requires prior knowledge of the potential attacks (i.e., backdoor attacks) and does not directly lend
636"
REFERENCES,0.3747841105354059,"itself to mixed/adaptive attacks. Moreover, the inference and adaptation are offline, unable to counter
637"
REFERENCES,0.37535981577432354,"online adaptive backdoor attack [31]. The other approach explored the notion of robustness that
638"
REFERENCES,0.37593552101324124,"prepares the defender for the worst case [54, 52], which often leads to a Stackelberg game (SG)
639"
REFERENCES,0.3765112262521589,"between the defender and the attacker. Yet, such a Stackelberg approach often leads to conservative
640"
REFERENCES,0.3770869314910766,"defense, lacking adaptability.
641"
REFERENCES,0.3776626367299942,"B
Broader Impact
642"
REFERENCES,0.37823834196891193,"Towards Universal Robust Federated Learning.
Our goal is to establish a comprehensive frame-
643"
REFERENCES,0.3788140472078296,"work for universal federated learning defense against all kinds of attacks. This framework ensures
644"
REFERENCES,0.3793897524467473,"that the server remains oblivious to any details pertaining to the environment or potential attackers.
645"
REFERENCES,0.3799654576856649,"Still, it possesses the ability to swiftly adapt and respond to uncertain or unknown attackers during
646"
REFERENCES,0.3805411629245826,"the actual federated learning process. Nevertheless, achieving this universal defense necessitates an
647"
REFERENCES,0.38111686816350027,"extensive attack set through pre-training, which often results in a protracted convergence time toward
648"
REFERENCES,0.38169257340241797,"a meta-policy. Moreover, the effectiveness and efficiency of generalizing from a wide range of diverse
649"
REFERENCES,0.3822682786413356,"distributions pose additional challenges. Considering these, we confine our experiments in this paper
650"
REFERENCES,0.3828439838802533,"to specifically address a subset of uncertainties and unknowns. This includes variables such as the
651"
REFERENCES,0.38341968911917096,"method of attacker, the number of attackers, the level of independence and identically distributed data,
652"
REFERENCES,0.38399539435808866,"backdoor triggers, backdoor targets, and other relevant aspects. However, we acknowledge that our
653"
REFERENCES,0.3845710995970063,"focus is not all-encompassing, and there may be other factors that remain unexplored in our research.
654"
REFERENCES,0.385146804835924,"Meta Equilibrium and Information Asymmetry.
Information asymmetry is a prevailing phe-
655"
REFERENCES,0.3857225100748417,"nomenon arising in a variety of contexts, including adversarial machine learning (e.g. FL discussed in
656"
REFERENCES,0.38629821531375935,"this work), cyber security [38], and large-scale network systems [34]. Our proposed meta-equilibrium
657"
REFERENCES,0.38687392055267705,"offers a data-driven approach tackling asymmetric information structure in dynamic games without
658"
REFERENCES,0.3874496257915947,"Bayesian-posterior beliefs. Achieving the strategic adaptation through stochastic gradient descent,
659"
REFERENCES,0.3880253310305124,"the meta-equilibrium is computationally superior to perfect Bayesian equilibrium and better suited
660"
REFERENCES,0.38860103626943004,"for real-world engineering systems involving high-dimensional continuous parameter spaces. It is
661"
REFERENCES,0.38917674150834775,"expected that the meta-equilibrium can also be relevant to other adversarial learning contexts, cyber
662"
REFERENCES,0.3897524467472654,"defense, and decentralized network systems.
663"
REFERENCES,0.3903281519861831,"C
Experiment Setup
664"
REFERENCES,0.39090385722510074,"Datasets.
We consider two datasets: MNIST [30] and CIFAR-10 [28], and default i.i.d. local data
665"
REFERENCES,0.39147956246401844,"distributions, where we randomly split each dataset into n groups, each with the same number of
666"
REFERENCES,0.3920552677029361,"training samples. MNIST includes 60,000 training examples and 10, 000 testing examples, where
667"
REFERENCES,0.3926309729418538,"each example is a 28×28 grayscale image, associated with a label from 10 classes. CIFAR-10 consists
668"
REFERENCES,0.39320667818077143,"of 60,000 color images in 10 classes of which there are 50, 000 training examples and 10,000 testing
669"
REFERENCES,0.39378238341968913,"examples. For the non-i.i.d. setting (see Figure 11(d)), we follow the method of [15] to quantify the
670"
REFERENCES,0.3943580886586068,"heterogeneity of the data. We split the workers into C = 10 (for both MNIST and CIFAR-10) groups
671"
REFERENCES,0.3949337938975245,"and model the non-i.i.d. federated learning by assigning a training instance with label c to the c-th
672"
REFERENCES,0.3955094991364421,"group with probability q and to all the groups with probability 1 −q. A higher q indicates a higher
673"
REFERENCES,0.3960852043753598,"level of heterogeneity.
674"
REFERENCES,0.39666090961427747,"Federated Learning Setting.
We use the following default parameters for the FL environment:
675"
REFERENCES,0.39723661485319517,"local minibatch size = 128, local iteration number = 1, learning rate = 0.05, number of workers
676"
REFERENCES,0.3978123200921128,"= 100, number of backdoor attackers = 5, number of untargeted model poisoning attackers = 20,
677"
REFERENCES,0.3983880253310305,"subsampling rate = 10%, and the number of FL training rounds = 500 (resp. 1000) for MNIST (resp.
678"
REFERENCES,0.39896373056994816,"CIFAR-10). For MNIST, we train a neural network classifier of 8×8, 6×6, and 5×5 convolutional
679"
REFERENCES,0.39953943580886586,"filter layers with ReLU activations followed by a fully connected layer and softmax output. For
680"
REFERENCES,0.40011514104778356,"CIFAR-10, we use the ResNet-18 model [22]. We implement the FL model with PyTorch [46] and
681"
REFERENCES,0.4006908462867012,"run all the experiments on the same 2.30GHz Linux machine with 16GB NVIDIA Tesla P100 GPU.
682"
REFERENCES,0.4012665515256189,"We use the cross-entropy loss as the default loss function and stochastic gradient descent (SGD) as
683"
REFERENCES,0.40184225676453655,"the default optimizer. For all the experiments except Figures 11(c) and 11(d), we fix the initial model
684"
REFERENCES,0.40241796200345425,"and random seeds of subsampling for fair comparisons.
685"
REFERENCES,0.4029936672423719,"Baselines.
We evaluate our defense method against various state-of-the-art attacks, including non-
686"
REFERENCES,0.4035693724812896,"adaptive and adaptive untargeted model poison attacks (i.e., IPM [68], LMP [15], RL [31]), as well as
687"
REFERENCES,0.40414507772020725,"backdoor attacks (BFL [2] without model replacement, BRL [32], with tradeoff parameter λ = 0.5,
688"
REFERENCES,0.40472078295912495,"DBA [67] where each selected attacker randomly chooses a sub-trigger as shown in Figures 6, PGD
689"
REFERENCES,0.4052964881980426,"attack [63] with a projection norm of 0.05), and a combination of both types. To establish the
690"
REFERENCES,0.4058721934369603,"effectiveness of our defense, we compare it with several strong defense techniques. These baselines
691"
REFERENCES,0.40644789867587794,"include defenses implemented during the training stage, such as Krum [7], ClipMed [69, 57, 31] (with
692"
REFERENCES,0.40702360391479564,"norm bound 1), FLTrust [10] with 100 root data samples and bias q = 0.5, training stage CRFL [66]
693"
REFERENCES,0.4075993091537133,"with norm bound of 0.02 and noise level 1e −3 as well as post-training defenses like NeuroClip [62]
694"
REFERENCES,0.408175014392631,"and Prun [64]. We use the original clipping thresholds 7 in [62] and set the default Prun number to
695"
REFERENCES,0.40875071963154863,"256.
696"
REFERENCES,0.40932642487046633,"Attack type
Category
Adaptivity"
REFERENCES,0.409902130109384,"IPM [68]
untargeted model poisoning
non-adaptive
LMP [15]
untargeted model poisoning
non-adaptive
BFL [2]
backdoor
non-adaptive
DBA [67]
backdoor
non-adaptive
RL [31]
untargeted model poisoning
adaptive
BRL [32]
backdoor
adaptive"
REFERENCES,0.4104778353483017,"Table 2: A table showcasing all attacks in the experiments, with their corresponding categories and
adaptivities."
REFERENCES,0.4110535405872193,"Reinforcement Learning Setting.
In our RL-based defense, since both the action space and state
697"
REFERENCES,0.411629245826137,"space are continuous, we choose the state-of-the-art Twin Delayed DDPG (TD3) [18] algorithm to
698"
REFERENCES,0.41220495106505467,"individually train the untargeted defense policy and the backdoor defense policy. We implement our
699"
REFERENCES,0.41278065630397237,"simulated environment with OpenAI Gym [9] and adopt OpenAI Stable Baseline3 [48] to implement
700"
REFERENCES,0.41335636154289,"TD3. The RL training parameters are described as follows: the number of FL rounds = 300 rounds,
701"
REFERENCES,0.4139320667818077,"policy learning rate = 0.001, the policy model is MultiInput Policy, batch size = 256, and γ = 0.99 for
702"
REFERENCES,0.41450777202072536,"updating the target networks. The default λ = 0.5 when calculating the backdoor rewards.
703"
REFERENCES,0.41508347725964306,"Settings
Pre-training
Online-adaptation
Related figures/tables"
REFERENCES,0.41565918249856076,"meta-RL
{NA, IPM, LMP, BFL, DBA}
{IPM, LMP, BFL, DBA, IPM+BFL, LMP+DBA}
Table 1,Figures 2, 9 and 11
meta-SG
{RL, BRL}
{IPM, LMP, RL, BRL}
Tables 4 and 8,Figures 2 and 9 to 11
meta-SG+
{NA, IPM, LMP, BFL, DBA, RL, BRL}
{IPM, LMP, RL, BRL}
Figures 2 and 9"
REFERENCES,0.4162348877374784,"Table 3: A table showcasing the attacks and defenses employed during pre-training and online-
adaptation, with links to the relevant figures or tables. RL and BRL are initially target on {FedAvg,
ClipMed, Krum, FLTrust+NC} during pre-training."
REFERENCES,0.4168105929763961,"Meta-learning Setting.
The attack domains (i.e., potential attack sets) are built as following: For
704"
REFERENCES,0.41738629821531376,"meta-RL, we consider IPM [68], LMP [15], EB [5] as three possible attack types. For meta-SG against
705"
REFERENCES,0.41796200345423146,"untargeted model poisoning attack, we consider RL-based attacks [31] trained against Krum [7] and
706"
REFERENCES,0.4185377086931491,"ClipMed [31, 69, 57] as initial attacks. For meta-SG against backdoor attack, we consider RL-based
707"
REFERENCES,0.4191134139320668,"backdoor attacks [32] trained against Norm-bounding [57] and NeuroClip [62] (Prun [64]) as initial
708"
REFERENCES,0.41968911917098445,"attacks. For meta-SG against mix type of attacks, we consider both RL-based attacks [31] and
709"
REFERENCES,0.42026482440990215,"RL-based backdoor attacks [32] described above as initial attacks.
710"
REFERENCES,0.4208405296488198,"At the pre-training stage, we set the number of iterations T = 100. In each iteration, we uniformly
711"
REFERENCES,0.4214162348877375,"sample K = 10 attacks from the attack type domain (see Algorithm 2 and Algorithm 1). For each
712"
REFERENCES,0.42199194012665514,"attack, we generate a trajectory of length H = 200 for MNIST (H = 500 for CIFAR-10), and update
713"
REFERENCES,0.42256764536557284,"both attacker’s and defender’s policies for 10 steps using TD3 (i.e., l = NA = ND = 10). At the
714"
REFERENCES,0.4231433506044905,"online adaptation stage, the meta-policy is adapted for 100 steps using TD3 with T = 10, H = 100
715"
REFERENCES,0.4237190558434082,"for MNIST (H = 200 for CIFAR-10), and l = 10. Other parameters are described as follows: single
716"
REFERENCES,0.42429476108232583,"task step size κ = κA = κD = 0.001, meta-optimization step size = 1, adaptation step size = 0.01.
717"
REFERENCES,0.42487046632124353,"Space Compression.
Following the BSMG model, it is most generally to use wt
g or (wt
g, It) as
718"
REFERENCES,0.4254461715601612,"the state, and {egt
k}M1+M2
k=1
or wt+1
g
as the action for the attacker and the defender, respectively, if
719"
REFERENCES,0.4260218767990789,"the federated learning model is small. However, when we use federated learning to train a high-
720"
REFERENCES,0.4265975820379965,"dimensional model (i.e., a large neural network), the original state/action space will lead to an
721"
REFERENCES,0.4271732872769142,"extremely large search space that is prohibitive in terms of training time and memory space. We
722"
REFERENCES,0.42774899251583187,"adopt the RL-based attack in [31] to simulate an adaptive model poisoning attack and the RL-based
723"
REFERENCES,0.4283246977547496,"local search in [32] to simulate an adaptive backdoor attack, both having a 3-dimensioanl real action
724"
REFERENCES,0.4289004029936672,"spaces after space comparison (see ). We further restrict all malicious devices controlled by the same
725"
REFERENCES,0.4294761082325849,"attacker to take the same action. To compress the state space, we reduce wt
g to only include its last
726"
REFERENCES,0.43005181347150256,"two hidden layers for both attacker and defender and reduce It to the number of malicious clients
727"
REFERENCES,0.43062751871042027,"sampled at round t.
728"
REFERENCES,0.43120322394933797,"Our approach rests on an RL-based synthesis of existing specialized defense methods against mixed
729"
REFERENCES,0.4317789291882556,"attacks, where multiple defenses can be selected at the same time and combined with dynamically
730"
REFERENCES,0.4323546344271733,"tuned hyperparameters. The following specialized defenses are selected in our implementation. For
731"
REFERENCES,0.43293033966609096,"training stage aggregation-based defenses, we first normalize the magnitude of all gradients to a
732"
REFERENCES,0.43350604490500866,"threshold α ∈(0, maxi∈St{∥gt
i∥}], then apply coordinate-wise trimmed mean [69] with trimmed
733"
REFERENCES,0.4340817501439263,"rate β ∈[0, 1). For post-training defense, NeuroClip [62] with clip range ε or Prun [64] with mask
734"
REFERENCES,0.434657455382844,"rate σ is applied. The concrete approach used in each of the above defenses can be replaced by other
735"
REFERENCES,0.43523316062176165,"defense methods. The key novelty of our approach is that instead of using a fixed and hand-crafted
736"
REFERENCES,0.43580886586067935,"algorithm as in existing approaches, we use RL to optimize the policy network πD(at
D|st; θ). Similar
737"
REFERENCES,0.436384571099597,"to RL-based attacks, the most general action space could be the set of global model parameters.
738"
REFERENCES,0.4369602763385147,"However, the high dimensional action space will lead to an extremely large search space that is
739"
REFERENCES,0.43753598157743234,"prohibitive in terms of training time and memory space. Thus, we apply reduce the action space to
740"
REFERENCES,0.43811168681635004,"at
D := (αt, βt, εt/σt). Note that the execution of our defense policy is lightweight, without using
741"
REFERENCES,0.4386873920552677,"any extra data for evaluation/validation.
742"
REFERENCES,0.4392630972941854,"Self-generated Data.
We begin by acknowledging that the server only holds a small amount of
743"
REFERENCES,0.43983880253310303,"initial data (200 samples with q = 0.1 in this work) learned from first 20 FL rounds using inverting
744"
REFERENCES,0.44041450777202074,"gradient [19], to simulate training set with 60,000 images (for both MNIST and CIFAR-10) for FL.
745"
REFERENCES,0.4409902130109384,"This limited data is augmented using several techniques such as normalization, random rotation, and
746"
REFERENCES,0.4415659182498561,"color jittering to create a larger and more varied dataset, which will be used as an input for generative
747"
REFERENCES,0.4421416234887737,"models.
748"
REFERENCES,0.44271732872769143,"Figure 3: Self-generated MNIST images using conditional GAN [41] (second row) and CIFAR-10
images using a diffusion model [55] (fourth row)."
REFERENCES,0.4432930339666091,"Figure 4: Generated backdoor triggers using GAN-based models [12]. Original image (first row).
Backdoor image (second row). Residual (third row)."
REFERENCES,0.4438687392055268,"0
10
20 0 10 20"
REFERENCES,0.4444444444444444,Original
REFERENCES,0.4450201496833621,"0
10
20 0 10 20"
ST TRIGGER,0.44559585492227977,1st Trigger
ST TRIGGER,0.44617156016119747,"0
10
20 0 10 20"
ND TRIGGER,0.44674726540011517,2nd Trigger
ND TRIGGER,0.4473229706390328,"0
10
20 0 10 20"
RD TRIGGER,0.4478986758779505,3rd Trigger
RD TRIGGER,0.44847438111686816,"0
10
20 0 10 20"
RD TRIGGER,0.44905008635578586,Global Trigger
RD TRIGGER,0.4496257915947035,"Figure 5: MNIST backdoor trigger patterns. The global trigger is considered the default poison pattern and is
used for backdoor accuracy evaluation. The sub-triggers are used by pre-training and DBA only. 0
20 0 10 20 30"
RD TRIGGER,0.4502014968336212,"Original 0
20 0 10 20 30"
ST TRIGGER,0.45077720207253885,"1st Trigger 0
20 0 10 20 30"
ND TRIGGER,0.45135290731145655,"2nd Trigger 0
20 0 10 20 30"
RD TRIGGER,0.4519286125503742,"3rd Trigger 0
20 0 10 20 30"
TH TRIGGER,0.4525043177892919,"4th Trigger 0
20 0 10 20 30"
TH TRIGGER,0.45308002302820954,Global Trigger
TH TRIGGER,0.45365572826712725,"Figure 6: CIFAR-10 fixed backdoor trigger patterns. The global trigger is considered the default poison pattern
and is used for online adaptation stage backdoor accuracy evaluation. The sub-triggers are used by pre-training
and DBA only."
TH TRIGGER,0.4542314335060449,Figure 7: Examples of reconstructed images using inverting gradient (before and after denoising)
TH TRIGGER,0.4548071387449626,"For MNIST, we use the augmented dataset to train a Conditional Generative Adversarial Network
749"
TH TRIGGER,0.45538284398388024,"(cGAN) model [41, 44] built upon the codebase in [29]. The cGAN model for the MNIST dataset
750"
TH TRIGGER,0.45595854922279794,"comprises two main components - a generator and a discriminator, both of which are neural networks.
751"
TH TRIGGER,0.4565342544617156,"Specifically, we use a dataset with 5,000 augmented data as the input to train cGAN, keep the network
752"
TH TRIGGER,0.4571099597006333,"parameters as default, and set the training epoch as 100.
753"
TH TRIGGER,0.45768566493955093,"For CIFAR-10, we leverage a diffusion model implemented in [11] that integrates several recent
754"
TH TRIGGER,0.45826137017846863,"techniques, including a Denoising Diffusion Probabilistic Model (DDPM) [23], DDIM-style deter-
755"
TH TRIGGER,0.4588370754173863,"ministic sampling [56], continuous timesteps parameterized by the log SNR at each timestep [27] to
756"
TH TRIGGER,0.459412780656304,"enable different noise schedules during sampling. The model also employs the ‘v’ objective, derived
757"
TH TRIGGER,0.4599884858952216,"from Progressive Distillation for Fast Sampling of Diffusion Models [51], enhancing the conditioning
758"
TH TRIGGER,0.4605641911341393,"of denoised images at high noise levels. During the training process, we use a dataset with 50,000
759"
TH TRIGGER,0.46113989637305697,"augmented data samples as the input to train this model, keep the parameters as default, and set the
760"
TH TRIGGER,0.46171560161197467,"training epoch as 30.
761"
TH TRIGGER,0.46229130685089237,"Simulated Environment.
To further improve efficiency and privacy, the defender simulate a smaller
762"
TH TRIGGER,0.46286701208981,"FL system when solving the game. In our experiments, we include 10 clients in pre-training while
763"
TH TRIGGER,0.4634427173287277,"using 100 clients in the online FL system. The simulation relies on a smaller dataset (generated from
764"
TH TRIGGER,0.46401842256764536,"root data) and endures a shorter training time (100 (500) FL rounds for MINST (CIFAR-10) v.s. 1000
765"
TH TRIGGER,0.46459412780656306,"rounds in online FL experiments). Although the offline simulated Markov game deviates from the
766"
TH TRIGGER,0.4651698330454807,"ground truth, the learned meta-defense policy can quickly adapt to the real FL during the online
767"
TH TRIGGER,0.4657455382843984,"adaptation, as shown in our experiment section.
768"
TH TRIGGER,0.46632124352331605,"Backdoor Attacks.
We consider the trigger patterns shown in Figure 4 and Figure 6 for backdoor
769"
TH TRIGGER,0.46689694876223375,"attacks. For triggers generated using GAN (see Figure 4), the goal is to classify all images of different
770"
TH TRIGGER,0.4674726540011514,"classes to the same target class (all-to-one). For fixed patterns (see Figure 6), the goal is to classify
771"
TH TRIGGER,0.4680483592400691,"images of the airplane class to the truck class (one-to-one). The default poisoning ratio is 0.5 in
772"
TH TRIGGER,0.46862406447898675,"both cases. The global trigger in Figure 6 is considered the default poison pattern and is used for the
773"
TH TRIGGER,0.46919976971790445,"online adaptation stage for backdoor accuracy evaluation. In practice, the defender (i.e., the server)
774"
TH TRIGGER,0.4697754749568221,"does not know the backdoor triggers and targeted labels. To simulate a backdoor attacker’s behavior,
775"
TH TRIGGER,0.4703511801957398,"we first implement multiple GAN-based attack models as in [12] to generate worst-case triggers
776"
TH TRIGGER,0.47092688543465744,"(which maximizes attack performance given backdoor objective) in the simulated environment.
777"
TH TRIGGER,0.47150259067357514,"Since the defender does not know the poisoning ratio ρi and target label of the attacker’s poisoned
778"
TH TRIGGER,0.4720782959124928,"dataset (involved in the attack objective F ′), we approximate the attacker’s reward function as rt
A =
779"
TH TRIGGER,0.4726540011514105,"−F ′′( bwt+1
g
), F ′′(w) := minc∈C[ 1"
TH TRIGGER,0.47322970639032813,"M1
PM1
i=1
1
|D′
i|
P|D′
i|
j=1 ℓ(w, (ˆxj
i, c))]−
1
M2
PM
i=M1+1 f(ω, Di). F ′′
780"
TH TRIGGER,0.47380541162924583,"differs F ′ only in the first M1 clients, where we use a strong target label (the minimizer) as a surrogate
781"
TH TRIGGER,0.4743811168681635,"to the true label c∗.
782"
TH TRIGGER,0.4749568221070812,"Inverting Gradient/Reverse Engineering.
In invert gradient, we set the step size for inverting
783"
TH TRIGGER,0.4755325273459988,"gradients η′ = 0.05, the total variation parameter β = 0.02, optimizer as Adam, the number of
784"
TH TRIGGER,0.4761082325849165,"iterations for inverting gradients max_iter = 10, 000, and learn the data distribution from scratch.
785"
TH TRIGGER,0.47668393782383417,"The number of steps for distribution learning is set to τE = 100. 32 images are reconstructed (i.e.,
786"
TH TRIGGER,0.47725964306275187,"B′ = 32) and denoised in each FL epoch. If no attacker is selected in the current epoch, the aggregate
787"
TH TRIGGER,0.47783534830166957,"gradient estimated from previous model updates is reused for reconstructing data. To build the
788"
TH TRIGGER,0.4784110535405872,"denoising autoencoder, a Gaussian noise sampled from 0.3N(0, 1) is added to each dimension of
789"
TH TRIGGER,0.4789867587795049,"images in Dreconstructed, which are then clipped to the range of [0,1] in each dimension. The result
790"
TH TRIGGER,0.47956246401842256,"is shown in Figure 7.
791"
TH TRIGGER,0.48013816925734026,"In the process of reverse engineering, we use Neural Cleanse [61] to find hidden triggers (See
792"
TH TRIGGER,0.4807138744962579,"Figure 8) connected to backdoor attacks. This method is essential for uncovering hidden triggers
793"
TH TRIGGER,0.4812895797351756,"Figure 8: Reversed MNIST backdoor trigger patterns. Original triggers (first row). Reversed triggers (second
row)"
TH TRIGGER,0.48186528497409326,"Figure 9: Comparisons of defenses against untargeted model poisoning attacks (i.e., IPM and RL) on MNIST
and CIFAR-10. RL-based attacks are trained before FL round 0 against the associate defenses (i.e., Krum and
meta-policy of meta-RL/meta-SG). All parameters are set as default and all random seeds are fixed."
TH TRIGGER,0.48244099021301096,"and for preventing such attacks. In particular, we use the global model, root generated data and
794"
TH TRIGGER,0.4830166954519286,"inverted data as inputs to reverse backdoor triggers. The Neural Cleanse class from ART is used for
795"
TH TRIGGER,0.4835924006908463,"this purpose. The reverse engineering process in this context involves using the generated backdoor
796"
TH TRIGGER,0.48416810592976395,"method from the Neural Cleanse defense to find the trigger pattern that the model is sensitive to. The
797"
TH TRIGGER,0.48474381116868165,"returned pattern and mask can be visualized to understand the nature of the backdoor.
798"
TH TRIGGER,0.4853195164075993,"Online Adaptation and Execution.
During the online adaptation stage, the defender starts by
799"
TH TRIGGER,0.485895221646517,"using the meta-policy learned from the pre-training stage to interact with the true FL environment,
800"
TH TRIGGER,0.48647092688543464,"while collecting new samples {s, a, er, s′}. Here, the estimated reward er is calculated using the
801"
TH TRIGGER,0.48704663212435234,"self-generated data and simulated triggers from the pertaining stage, as well as new data inferred
802"
TH TRIGGER,0.48762233736327,"online through methods such as inverting gradient [19] and reverse engineering [61]. Inferred data
803"
TH TRIGGER,0.4881980426021877,"samples are blurred using data augmentation [53] while protecting clients’ privacy. For a fixed
804"
TH TRIGGER,0.48877374784110533,"number of FL rounds (e.g., 50 for MNIST and 100 for CIFAR-10 in our experiments), the defense
805"
TH TRIGGER,0.48934945308002303,"policy will be updated using gradient ascents from the collected trajectories. Ideally, the defender’s
806"
TH TRIGGER,0.4899251583189407,"adaptation time (including the time for collecting new samples and that for updating the policy)
807"
TH TRIGGER,0.4905008635578584,"should be significantly less than the whole FL training period so that the defense execution will not
808"
TH TRIGGER,0.491076568796776,"be delayed. In real-world FL training, the server typically waits for up to 10 minutes before receiving
809"
TH TRIGGER,0.4916522740356937,"responses from the clients [8, 25], enabling defense policy’s online update with enough episodes.
810"
TH TRIGGER,0.49222797927461137,"D
Additional Experiment Results
811"
TH TRIGGER,0.49280368451352907,"More untargetd model poisoning/backdoor results.
As shown in Figure 9, similar to results
812"
TH TRIGGER,0.4933793897524468,"in Figure 2 as described in Section 4, meta-SG plus achieves the best performance (slightly better
813"
TH TRIGGER,0.4939550949913644,"than meta-SG) under IPM attacks for both MNIST and CIFAR-10. On the other hand, meta-SG
814"
TH TRIGGER,0.4945308002302821,"performs the best (significantly better than meta-RL) against RL-based attacks for both MNIST
815"
TH TRIGGER,0.49510650546919976,"and CIFAR-10. Notably, Krum can be easily compromised by RL-based attacks by a large margin.
816"
TH TRIGGER,0.49568221070811747,"In contrast, meta-RL gradually adapts to adaptive attacks, while meta-SG displays near-immunity
817"
TH TRIGGER,0.4962579159470351,"against RL-based attacks. In addition, we illustrate results under backdoor attacks and defenses on
818"
TH TRIGGER,0.4968336211859528,"MNIST in Table 4.
819"
TH TRIGGER,0.49740932642487046,"Defender’s knowledge of backdoor attacks.
We consider two settings: 1) the server knows the
820"
TH TRIGGER,0.49798503166378816,"backdoor trigger but is uncertain about the target label, and 2) the server knows the target label but
821"
TH TRIGGER,0.4985607369027058,"not the backdoor trigger. In the former case, the meta-SG first pre-trains the defense policy with RL
822"
TH TRIGGER,0.4991364421416235,"attacks using a known fixed global pattern (see Figure 6) targeting all 10 classes in CIFAR-10, then
823"
TH TRIGGER,0.49971214738054115,"adapts with an RL-based backdoor attack using the same trigger targeting class 0 (airplane), with
824"
TH TRIGGER,0.5002878526194589,"Bac
Krum
CRFL
Meta-SG (ours)"
TH TRIGGER,0.5008635578583766,"BFL
0.8257
0.4253
0.0086
DBA
0.4392
0.215
0.2256
BRL
0.9901
0.8994
0.2102"
TH TRIGGER,0.5014392630972941,"Table 4: Comparisons of average backdoor accuracy (lower the better) after 250 FL rounds under
backdoor attacks and defenses on MNIST. All parameters are set as default and all random seeds are
fixed."
TH TRIGGER,0.5020149683362118,"Figure 10: Comparisons of baseline defenses, i.e., NeuroClip, Prun, ClipMed, FLTrust+NeuroClip (from left
to right) and whitebox/blackbox meta-SG under RL-based backdoor attack (BRL) on CIFAR-10. The BRLs
are trained before FL round 0 against the associate defenses (i.e., NeuroClip, Prun, ClipMed, FLTrust+NC and
meta-policy of meta-SG). Other parameters are set as default and all random seeds are fixed."
TH TRIGGER,0.5025906735751295,"results shown in the third figure of Figure 10. In the latter case where the defender does not know the
825"
TH TRIGGER,0.5031663788140472,"true backdoor trigger used by the attacker, we implement the GAN-based model [12] to generate the
826"
TH TRIGGER,0.5037420840529648,"worst-case triggers (see Figure 4) targeting one known label (truck). The meta-SG will train a defense
827"
TH TRIGGER,0.5043177892918825,"policy with the RL-based backdoor attacks using the worst-case triggers targeting the known label,
828"
TH TRIGGER,0.5048934945308002,"then adapt with a RL-based backdoor attack using a fixed global pattern (see Figure 6) targeting the
829"
TH TRIGGER,0.5054691997697179,"known label in the real FL environment (results shown in the fourth graph in Figure 10. We call the
830"
TH TRIGGER,0.5060449050086355,"two above cases blackbox settings since the defender misses key backdoor information and solely
831"
TH TRIGGER,0.5066206102475532,"depends on their own generated data/triggers w/o inverting/reversing during online adaptation. In
832"
TH TRIGGER,0.5071963154864709,"the whitebox setting, the server knows the backdoor trigger pattern (global) and the targeted label
833"
TH TRIGGER,0.5077720207253886,"(truck), and is trained by true clients’ data. The corresponding results are in the first two graphs of
834"
TH TRIGGER,0.5083477259643063,"Figures 10, which show the upper bound performance of meta-SG and may not be practical in a real
835"
TH TRIGGER,0.5089234312032239,"FL environment. Post-training defenses alone (i.e., NeuroClip and Prun) and combined defenses
836"
TH TRIGGER,0.5094991364421416,"(i.e., ClipMed and FLTrust+NC) are susceptible to RL-based attacks once the defense mechanism
837"
TH TRIGGER,0.5100748416810593,"is known. On the other hand, as depicted in Figure 10, we demonstrate that our whitebox meta-SG
838"
TH TRIGGER,0.510650546919977,"approach is capable of effectively eliminating the backdoor influence while preserving high main
839"
TH TRIGGER,0.5112262521588946,"task accuracy simultaneously, while blackbox meta-SG against uncertain labels is unstable since
840"
TH TRIGGER,0.5118019573978123,"the meta-policy will occasionally target a wrong label, even with adaptation and blackbox meta-SG
841"
TH TRIGGER,0.51237766263673,"against unknown trigger is not robust enough as its backdoor accuracy still reaches nearly 50% at the
842"
TH TRIGGER,0.5129533678756477,"end of FL training.
843"
TH TRIGGER,0.5135290731145653,"Acc
NA/FedAvg
Root data
Generated data
Pre-train only
Online-adapt only"
TH TRIGGER,0.514104778353483,"MNIST
0.9016
0.4125
0.5676
0.6125
0.4134
CIFAR-10
0.7082
0.2595
0.3833
0.1280
0.3755"
TH TRIGGER,0.5146804835924007,"Table 5: Ablation studies of only using root data/generated dataset in simulated environment to learn
the FL model and the defense performance under IPM of directly applying meta-policy learned from
pre-training without adaptation/starting online adaptation from a randomly initialized defense policy.
Results are average globel model accuracy after 250 (500) FL rounds on MNIST (CIFAR-10). All
parameters are set as default and all random seeds are fixed.."
TH TRIGGER,0.5152561888313184,"Importance of inverting/reversing methods. In the ablation study, we examine a practical and
844"
TH TRIGGER,0.515831894070236,"relatively well-performed graybox meta-SG. The graybox meta-SG has the same setting as blackbox
845"
TH TRIGGER,0.5164075993091537,"meta-SG during pre-training as describe in Section 2.2, but utilizes inverting gradient [19] and reverse
846"
TH TRIGGER,0.5169833045480714,"engineering [61] during online adaptation to learn clients’ data and backdoor trigger in a way without
847"
TH TRIGGER,0.5175590097869891,"breaking the privacy condition in FL. The graybox approach only learns ambiguous data from clients,
848"
TH TRIGGER,0.5181347150259067,"then applies data augmentation (e.g., noise, distortion) and combines them with previously generated
849"
TH TRIGGER,0.5187104202648244,"data before using. Figure 11(a) illustrates that graybox meta-SG exhibits a more stable and robust
850"
TH TRIGGER,0.5192861255037421,"mitigation of the backdoor attack compared to blackbox meta-SG. Furthermore, in Figure 11(b),
851"
TH TRIGGER,0.5198618307426598,"(a)
(b)
(c)
(d)"
TH TRIGGER,0.5204375359815774,"Figure 11: Ablation studies. (a)-(b): uncertain backdoor target and unknown backdoor triggers, where the
meta-policies are trained by worst-case triggers generated from GAN-based models [12] or targeting multiple
labels on CIFAR-10 during pre-training and utilizing inverting gradient [19] and reverse engineering [61] during
online adaptation. (c)-(d): meta-RL tested by the number of malicious clients in [20%, 30%, 40%] and non-i.i.d.
level in q = [0.5, 0.6, 0.7] on MNIST compared with Krum and ClipMed under LMP attack. Other parameters
are set as default."
TH TRIGGER,0.5210132412204951,"graybox meta-SG demonstrates a significant reduction in the impact of the backdoor attack, achieving
852"
TH TRIGGER,0.5215889464594128,"nearly a 70% mitigation, outperforming blackbox meta-SG.
853"
TH TRIGGER,0.5221646516983305,"Number of malicious clients/Non-i.i.d. level. Here we apply our meta-RL to study the impact of
854"
TH TRIGGER,0.5227403569372481,"inaccurate knowledge of the number of malicious clients and the non-i.i.d. level of clients’ local data
855"
TH TRIGGER,0.5233160621761658,"distribution. With rough knowledge that the number of malicious clients is in the range of 5%-50%,
856"
TH TRIGGER,0.5238917674150835,"the meta-SG will pre-train on LMP attacks with malicious clients [5 : 5 : 50], and adapt to three cases
857"
TH TRIGGER,0.5244674726540012,"with 20%, 30%, and 40% malicious clients in online adaptation, respectively. Similarly, when the
858"
TH TRIGGER,0.5250431778929189,"non-i.i.d. level is between 0.1-1, the meta-SG will pre-train on LMP attacks with non-i.i.d. level
859"
TH TRIGGER,0.5256188831318365,"[0.1 : 0.1 : 1] and adapt to q= 0.5, 0.6, 0.7 in online adaptation. As illustrated in Figures 11(c)
860"
TH TRIGGER,0.5261945883707542,"and 11(d), meta-SG reaches the highest model accuracy for all numbers of malicious clients and
861"
TH TRIGGER,0.5267702936096719,"non-i.i.d. levels under LMP.
862"
TH TRIGGER,0.5273459988485896,"Importance of pre-training and online adaptation
As shown in Table 5, the pre-training is to
863"
TH TRIGGER,0.5279217040875072,"derive defense policy rather than the model itself. Directly using those shifted data (root or generated)
864"
TH TRIGGER,0.5284974093264249,"to train the FL model will result in model accuracy as low as 0.2-0.3 (0.4-0.5) for CIFAR-10 (MNIST)
865"
TH TRIGGER,0.5290731145653426,"in our setting. Pre-training and online adaptation are indispensable in the proposed framework. Our
866"
TH TRIGGER,0.5296488198042603,"experiments in Table 5 indicate that directly applying defense learned from pre-training w/o online
867"
TH TRIGGER,0.5302245250431779,"adaptation and adaptation from randomly initialized defense policy w/o pre-training both fail to
868"
TH TRIGGER,0.5308002302820956,"address malicious attacks, resulting in global model accuracy as low as 0.3-0.6 (0.1-0.4) on MNIST
869"
TH TRIGGER,0.5313759355210133,"(CIFAR-10). In the absence of adaptation, meta policy itself falls short of the distribution shift
870"
TH TRIGGER,0.531951640759931,"between the simulated and the real environment. Likewise, the online adaptation fails to attain the
871"
TH TRIGGER,0.5325273459988485,"desired defense policy without the pre-trained policy serving as a decent initialization.
872"
TH TRIGGER,0.5331030512377662,"Biased/Limited root data
We evaluate the average model accuracy after 250 FL epochs under the
873"
TH TRIGGER,0.533678756476684,"meta-SG framework against the IPM attack, using root data with varying i.i.d. levels (as defined in
874"
TH TRIGGER,0.5342544617156016,"the experiment setting section). Here, q = 0.1 (indicating the root data is i.i.d.) serves as our baseline
875"
TH TRIGGER,0.5348301669545192,"meta-SG, as presented in the paper. We designate class 0 as the reference class. For instance, when q
876"
TH TRIGGER,0.5354058721934369,"= 0.4, it indicates a 40% probability for each data labeled as class 0 within the root data, while the
877"
TH TRIGGER,0.5359815774323546,"remaining 60% are distributed equally among the other classes. We observe that when q is as high
878"
TH TRIGGER,0.5365572826712723,"as 0.7, there is one class (i.e., 3) missing in the root data. Although, through inverting methods in
879"
TH TRIGGER,0.5371329879101899,"online adaptation, the defender can learn the missing data in the end, it suffered the slower adaptation
880"
TH TRIGGER,0.5377086931491076,"compared with a good initial defense policy. In addition, we test the average model accuracy after
881"
TH TRIGGER,0.5382843983880253,"250 FL epochs under meta-SG against IPM attack using different numbers of root data (i.e., 100, 60,
882"
TH TRIGGER,0.538860103626943,"20), where 100 root data is our original meta-SG setting in the rest of paper. We overserve that when
883"
TH TRIGGER,0.5394358088658607,"number of root data is 20, two classes of data are missing (i.e., 1 and 5).
884"
TH TRIGGER,0.5400115141047783,"Generalization to unseen adaptive attacks
We thoroughly search related works considering
885"
TH TRIGGER,0.540587219343696,"adaptive attacks in FL and find very limited works (with solid and lightweight open-source implemen-
886"
TH TRIGGER,0.5411629245826137,"tation) that can be used as our benchmark. As a result, we introduce two new benchmark adaptive
887"
TH TRIGGER,0.5417386298215314,"attack methods in the testing stage as unseen adaptive attacks: (1) adaptive LMP![15], which requires
888"
TH TRIGGER,0.542314335060449,"access to normal clients’ updates in each FL round, and (2) RL attack [31] restricted 1-dimensional
889"
TH TRIGGER,0.5428900402993667,"action space (i.e., adaptive scalar factor) compared with the baseline 3-dimensional RL attack [31]
890"
TH TRIGGER,0.5434657455382844,"showing in our paper. The defender in pre-training only interacts with the 3-dimensional RL attack.
891"
TH TRIGGER,0.5440414507772021,"We test the average model accuracy after 250 FL epochs under meta-SG against different (unseen)
892"
TH TRIGGER,0.5446171560161197,"Biased Level
q = 0.1
q = 0.4
q = 0.7"
TH TRIGGER,0.5451928612550374,"Acc
0.8951
0.8612
0.7572"
TH TRIGGER,0.5457685664939551,(a) Ablation study of biased root data.
TH TRIGGER,0.5463442717328728,"Number of Root Data
100
60
20"
TH TRIGGER,0.5469199769717904,"Acc
0.8951
0.8547
0.6902"
TH TRIGGER,0.5474956822107081,(b) Ablation study of limited root data.
TH TRIGGER,0.5480713874496258,"Table 6: Results of the average model accuracy on MNIST after 250 FL epochs under meta-SG
against IPM attack using root data with (a) different i.i.d levels and (b) different numbers of root data.
All random seeds are fixed and all other parameters are set as default."
TH TRIGGER,0.5486470926885435,"Acc/Bac
NormBound 0.2
NormBound 0.1
NormBound 0.05"
TH TRIGGER,0.5492227979274611,"DBA
0.6313/0.9987
0.5192/0.6994
0.3610/0.4392
IPM+BFL
0.6060/0.5123
0.4917/0.2104
0.3614/0.2253
Acc/Bac
NeuroClip 10
NeuroClip 6
NeuroClip 1"
TH TRIGGER,0.5497985031663788,"DBA
0.6221/0.9974
0.6141/0.9984
0.2515/0.0002
IPM+BFL
0.1/0.0020
0.1/0
0.1/0"
TH TRIGGER,0.5503742084052965,"Table 7: Results of manually tuning norm threshold [57] and clipping range [62]. All other parameters
are set as default and all random seeds are fixed."
TH TRIGGER,0.5509499136442142,"adaptive attacks. What is interesting here is that meta-SG can achieve even better performance against
893"
TH TRIGGER,0.5515256188831318,"unseen attacks.
894"
TH TRIGGER,0.5521013241220495,"Attack Methods
Model Acc"
-DIMENSIONAL RL,0.5526770293609672,"3-dimensional RL
0.8652
Adaptive LMP
0.8692
1-dimensional RL
0.8721"
-DIMENSIONAL RL,0.5532527345998849,"Table 8: Comparisons of average model accuracy after 250 FL rounds under different adaptive attacks
on MNIST. All parameters are set as default and all random seeds are fixed."
-DIMENSIONAL RL,0.5538284398388025,"E
Algorithms
895"
-DIMENSIONAL RL,0.5544041450777202,"This section elaborates on meta-learning defense and meta-Stackelberg defense in equation meta-SE.
896"
-DIMENSIONAL RL,0.5549798503166379,"To begin with, we first review the policy gradient method [58] in RL and its Monte-Carlo estimation.
897"
-DIMENSIONAL RL,0.5555555555555556,"To simplify our exposition, we fix the attacker’s policy ϕ, and then the Markov game reduces to a
898"
-DIMENSIONAL RL,0.5561312607944733,"single-agent MDP, where the optimal policy to be learned is the defender’s θ.
899"
-DIMENSIONAL RL,0.5567069660333909,"Policy Gradient
The idea of the policy gradient method is to apply gradient ascent to the
900"
-DIMENSIONAL RL,0.5572826712723086,"value function JD.
Following [58], we obtain ∇θJD := Eτ∼q(θ)[g(τ; θ)], where g(τ; θ) =
901
PH
t=1 ∇θ log π(at
D|st; θ)R(τ) and R(τ) = PH
t=1 γtr(st, at
D). Note that for simplicity, we sup-
902"
-DIMENSIONAL RL,0.5578583765112263,"press the parameter ϕ, ξ in the trajectory distribution q, and instead view it as a function of θ. In
903"
-DIMENSIONAL RL,0.558434081750144,"numerical implementations, the policy gradient ∇θJD is replaced by its Monte-Carlo (MC) estima-
904"
-DIMENSIONAL RL,0.5590097869890616,"tion using sample trajectory. Suppose a batch of trajectories {τi}Nb
i=1, and Nb denotes the batch size,
905"
-DIMENSIONAL RL,0.5595854922279793,"then the MC estimation is
906"
-DIMENSIONAL RL,0.560161197466897,"ˆ∇θJD(θ, τ) := 1/Nb
X"
-DIMENSIONAL RL,0.5607369027058147,"τi
g(τi; θ).
(E1)"
-DIMENSIONAL RL,0.5613126079447323,"The same deduction also holds for the attacker’s problem when fixing the defense θ.
907"
-DIMENSIONAL RL,0.56188831318365,"Meta-Learning FL Defense
As discussed in Section 3, meta-learning-based defense (meta defense)
908"
-DIMENSIONAL RL,0.5624640184225677,"mainly targets non-adaptive attack methods, where πA(·; ϕ, ξ) is a pre-fixed attack strategy following
909"
-DIMENSIONAL RL,0.5630397236614854,"some rulebook, such as IPM [68] and LMP [15]. In this case, the BSMG reduces to single-agent MDP
910"
-DIMENSIONAL RL,0.563615428900403,"for the defender, where the transition kernel is determined by the attack method. Mathematically, the
911"
-DIMENSIONAL RL,0.5641911341393206,"meta-defense problem is given by
912"
-DIMENSIONAL RL,0.5647668393782384,"max
θ,Ψ Eξ∼Q(·)[JD(Ψ(θ, τ), ϕ, ξ)].
(E2)"
-DIMENSIONAL RL,0.565342544617156,"Since the attack type is hidden from the defender, the adaptation mapping Ψ is usually defined in a
913"
-DIMENSIONAL RL,0.5659182498560736,"data-driven manner. For example, Ψ(θ, τ) can be defined as a one-step stochastic gradient update
914"
-DIMENSIONAL RL,0.5664939550949913,"with learning rate η: Ψ(θ, τ) = θ + η ˆ∇JD(τξ) [16] or a recurrent neural network in [13]. This
915"
-DIMENSIONAL RL,0.567069660333909,"work mainly focuses on gradient adaptation for the purpose of deriving theoretical guarantees in
916"
-DIMENSIONAL RL,0.5676453655728267,"Appendix F.
917"
-DIMENSIONAL RL,0.5682210708117443,"With the one-step gradient adaptation, the meta-defense problem in equation E2 can be simplified as
918"
-DIMENSIONAL RL,0.568796776050662,"max
θ
Eξ∼Q(·)Eτ∼q(θ)[JD(θ + η ˆ∇θJD(τ), ϕ, ξ)].
(E3)"
-DIMENSIONAL RL,0.5693724812895797,"Recall that the attacker’s strategy is pre-determined, ϕ, ξ can be viewed as fixed parameters, and
919"
-DIMENSIONAL RL,0.5699481865284974,"hence, the distribution q is a function of θ. To apply the policy gradient method to equation E3, one
920"
-DIMENSIONAL RL,0.5705238917674151,"needs an unbiased estimation of the gradient of the objective function in equation E3. Consider the
921"
-DIMENSIONAL RL,0.5710995970063327,"gradient computation using the chain rule:
922"
-DIMENSIONAL RL,0.5716753022452504,"∇θEτ∼q(θ)[JD(θ + η ˆ∇θJD(τ), ϕ, ξ)]"
-DIMENSIONAL RL,0.5722510074841681,"= Eτ∼q(θ){∇θJD(θ + η ˆ∇θJD(τ), ϕ, ξ)(I + η ˆ∇2
θJD(τ))
|
{z
}
①"
-DIMENSIONAL RL,0.5728267127230858,"+ JD(θ + η ˆ∇θJD(τ))∇θ H
X"
-DIMENSIONAL RL,0.5734024179620034,"t=1
π(at|st; θ)"
-DIMENSIONAL RL,0.5739781232009211,"|
{z
}
② }. (E4)"
-DIMENSIONAL RL,0.5745538284398388,"The first term results from differentiating the integrand JD(θ + η ˆ∇θJD(τ), ϕ, ξ) (the expectation is
923"
-DIMENSIONAL RL,0.5751295336787565,"taken as integration), while the second term is due to the differentiation of q(θ). One can see from
924"
-DIMENSIONAL RL,0.5757052389176741,"the first term that the above gradient involves a Hessian ˆ∇2JD, and its sample estimate is given by
925"
-DIMENSIONAL RL,0.5762809441565918,"the following. For more details on this Hessian estimation, we refer the reader to [14].
926"
-DIMENSIONAL RL,0.5768566493955095,"ˆ∇2JD(τ) = 1 Nb Nb
X"
-DIMENSIONAL RL,0.5774323546344272,"i=1
[g(τi; θ)∇θ log q(τi; θ)T + ∇θg(τi; θ)]
(E5)"
-DIMENSIONAL RL,0.5780080598733448,"Finally, to complete the sample estimate of ∇θEτ∼q(θ)[JD(θ + η ˆ∇θJD(τ), ϕ, ξ)], one still needs to
927"
-DIMENSIONAL RL,0.5785837651122625,"estimate ∇θJD(θ + η ˆ∇θJD(τ), ϕ, ξ) in the first term. To this end, we need to first collect a batch
928"
-DIMENSIONAL RL,0.5791594703511802,"of sample trajectories τ ′ using the adapted policy θ′ = θ + η ˆ∇θJD(τ). Then, the policy gradient
929"
-DIMENSIONAL RL,0.5797351755900979,"estimate of ˆ∇θJD(θ′) proceeds as in equation E1. To sum up, constructing an unbiased estimate of
930"
-DIMENSIONAL RL,0.5803108808290155,"equation E4 takes two rounds of sampling. The first round is under the meta policy θ, which is used
931"
-DIMENSIONAL RL,0.5808865860679332,"to estimate the Hessian equation E5 and to adapt the policy to θ′. The second round aims to estimate
932"
-DIMENSIONAL RL,0.5814622913068509,"the policy gradient ∇θJD(θ + η ˆ∇θJD(τ), ϕ, ξ) in the first term in equation E4.
933"
-DIMENSIONAL RL,0.5820379965457686,"In the experiment, we employ a first-order meta-learning algorithm called Reptile [43] to avoid the
934"
-DIMENSIONAL RL,0.5826137017846862,"Hessian computation. The gist is to simply ignore the chain rule and update the policy using the
935"
-DIMENSIONAL RL,0.5831894070236039,"gradient ∇θJD(θ′, ϕ, ξ)|θ′=θ+η ˆ∇θJD(τ). Naturally, without the Hessian term, the gradient in this
936"
-DIMENSIONAL RL,0.5837651122625216,"update is biased, yet it still points to the ascent direction as argued in [43], leading to effective meta
937"
-DIMENSIONAL RL,0.5843408175014393,"policy. The advantage of Reptile is more evident in multi-step gradient adaptation. Consider a l-step
938"
-DIMENSIONAL RL,0.584916522740357,"gradient adaptation, the chain rule computation inevitably involves multiple Hessian terms (each
939"
-DIMENSIONAL RL,0.5854922279792746,"gradient step brings a Hessian term) as shown in [14]. In contrast, Reptile only requires first-order
940"
-DIMENSIONAL RL,0.5860679332181923,"information, and the meta-learning algorithm (l-step adaptation) is given by Algorithm 2.
941"
-DIMENSIONAL RL,0.58664363845711,"Meta-Stackelberg Learning
Recall that in meta-SE, the attacker’s policy ϕ∗
ξ is not pre-fixed.
942"
-DIMENSIONAL RL,0.5872193436960277,"Instead, it is the best response to the defender’s adapted policy as shown in equation meta-SE. To
943"
-DIMENSIONAL RL,0.5877950489349453,Algorithm 2 Reptile Meta-Reinforcement Learning with l-step adaptation
-DIMENSIONAL RL,0.588370754173863,"1: Input: the type distribution Q(ξ), step size parameters κ, η
2: Output: θT"
-DIMENSIONAL RL,0.5889464594127807,3: randomly initialize θ0
-DIMENSIONAL RL,0.5895221646516984,"4: for iteration t = 1 to T do
5:
Sample a batch ˆΞ of K attack types from Q(ξ);
6:
for each ξ ∈ˆΞ do
7:
θt
ξ(0) ←θt"
-DIMENSIONAL RL,0.590097869890616,"8:
for k = 0 to l −1 do
9:
Sample a batch trajectories τ of the horizon length H under θt
ξ(k);"
-DIMENSIONAL RL,0.5906735751295337,"10:
Evaluate ˆ∇θJD(θt
ξ(k), τ) using MC in equation E1;"
-DIMENSIONAL RL,0.5912492803684514,"11:
θt
ξ(k + 1) ←θt
ξ(k) + κ ˆ∇θJD(θt, τ)
12:
end for
13:
end for
14:
Update θt+1 ←θt + 1/K P"
-DIMENSIONAL RL,0.5918249856073691,"ξ∈ˆΞ(θt
ξ(l) −θt);
15: end for"
-DIMENSIONAL RL,0.5924006908462867,"obtain this best response, one needs alternative training: fixing the defense policy, and applying
944"
-DIMENSIONAL RL,0.5929763960852044,"gradient ascent to the attacker’s problem until convergence. It should be noted that the proposed
945"
-DIMENSIONAL RL,0.5935521013241221,"meta-SL utilizes the unbiased gradient estimation in equation E5, which paves the way for theoretical
946"
-DIMENSIONAL RL,0.5941278065630398,"analysis in Appendix F. Yet, we turn to the Reptile to speed up pre-straining in the experiments. We
947"
-DIMENSIONAL RL,0.5947035118019574,"present both algorithms in Algorithm 3, and only consider one-step adaptation for simplicity. The
948"
-DIMENSIONAL RL,0.595279217040875,multi-step version is a straightforward extension of Algorithm 3.
-DIMENSIONAL RL,0.5958549222797928,Algorithm 3 (Reptile) Meta-Stackelberg Learning with one-step adaptation
-DIMENSIONAL RL,0.5964306275187105,"1: Input: the type distribution Q(ξ), initial defense meta policy θ0, pre-trained attack policies
{ϕ0
ξ}ξ∈Ξ, step size parameters κD, κA, η, and iterations numbers NA, ND;"
-DIMENSIONAL RL,0.597006332757628,2: Output: θND
-DIMENSIONAL RL,0.5975820379965457,"3: for iteration t = 0 to ND −1 do
4:
Sample a batch ˆΞ of K attack types from Q(ξ);
5:
for each ξ ∈ˆΞ do
6:
Sample a batch of trajectories using ϕt and ϕt
ξ;"
-DIMENSIONAL RL,0.5981577432354634,"7:
Evaluate ˆ∇θJD(θt, ϕt
ξ, ξ) using equation E1;"
-DIMENSIONAL RL,0.5987334484743811,"8:
Perform one-step adaptation θt
ξ ←θt + η ˆ∇θJD(θt
ξ(k), ϕt
ξ, ξ);
9:
ϕt
ξ(0) ←ϕt
ξ;
10:
for k = 0, . . . , NA −1 do
11:
Sample a batch of trajectories using θt
ξ and ϕt
ξ(k);"
-DIMENSIONAL RL,0.5993091537132987,"12:
ϕt
ξ(k + 1) ←ϕt
ξ(k) + κA ˆ∇ϕJA(θt
ξ, ϕt
ξ(k), ξ);
13:
end for
14:
if Reptile then
15:
Sample a batch of trajectories using θt
ξ and ϕt
ξ(NA);"
-DIMENSIONAL RL,0.5998848589522164,"16:
Evaluate ˆ∇JD(ξ) := ˆ∇θJD(θ, ϕt
ξ(NA), ξ)|θ=θt
ξ using equation E1;
17:
else
18:
Sample a batch of trajectories using θt and ϕt
ξ(NA);
19:
Evaluate the Hessian using equation E5;
20:
Sample a batch of trajectories using θt
ξ and ϕt
ξ(NA);"
-DIMENSIONAL RL,0.6004605641911341,"21:
Evaluate ˆ∇JD(ξ) := ˆ∇θJD(θt
ξ, ϕt
ξ(NA), ξ) using equation E4;
22:
end if
23:
¯θt
ξ ←θt + κD ˆ∇JD(ξ);
24:
end for
25:
θt+1 ←θt + 1/K P"
-DIMENSIONAL RL,0.6010362694300518,"ξ∼ˆΞ(¯θt
ξ −θt), ϕt+1
ξ
←ϕt
ξ(NA);
26: end for"
-DIMENSIONAL RL,0.6016119746689695,"F
Theoretical Results
949"
-DIMENSIONAL RL,0.6021876799078871,"F.1
Existence of Meta-SG
950"
-DIMENSIONAL RL,0.6027633851468048,"Theorem F.1. Under the conditions that Θ and Φ are compact and convex, the meta-SG admits at
951"
-DIMENSIONAL RL,0.6033390903857225,"least one meta-FOSE.
952"
-DIMENSIONAL RL,0.6039147956246402,"Proof. Clearly, Θ × Φ|Ξ| is compact and convex, let ϕ ∈Φ|Ξ|, ϕξ ∈Φ be the (type-aggregated)
953"
-DIMENSIONAL RL,0.6044905008635578,"attacker’s strategy, since the consider twice continuously differentiable utility functions ℓD(θ, ϕ) :=
954"
-DIMENSIONAL RL,0.6050662061024755,"Eξ∼QLD(θ, ϕξ, ξ) and ℓξ(θ, ϕ) := LA(θ, ϕξ, ξ) for all ξ ∈Ξ. Then, there exists a constant γc > 0,
955"
-DIMENSIONAL RL,0.6056419113413932,"such that the auxiliary utility functions:
956"
-DIMENSIONAL RL,0.6062176165803109,"˜ℓD(θ; (θ′, ϕ′)) ≡ℓD(θ, ϕ) −γc"
-DIMENSIONAL RL,0.6067933218192285,2 ∥θ −θ′∥2
-DIMENSIONAL RL,0.6073690270581462,"˜ℓξ(ϕξ; (θ′, ϕ′) ≡ℓξ(θ′, (ϕξ, ϕ′
−ξ)) −γc"
-DIMENSIONAL RL,0.6079447322970639,"2 ∥ϕξ −ϕ′
ξ∥2
∀ξ ∈Ξ
(F6)"
-DIMENSIONAL RL,0.6085204375359816,"are γc-strongly concave in spaces θ ∈Θ, ϕξ ∈Φ for all ξ ∈Ξ, respectively for any fixed (θ′, ϕ′) ∈
957"
-DIMENSIONAL RL,0.6090961427748992,"Θ × Φ|Ξ|.
958"
-DIMENSIONAL RL,0.6096718480138169,"Define the self-map h : Θ × Φ|Ξ| →Θ × Φ|Ξ| with h(θ′, ϕ′) ≡(¯θ(θ′, ϕ′), ¯ϕ(θ′, ϕ′)), where
959"
-DIMENSIONAL RL,0.6102475532527346,"¯θ(θ′, ϕ′) = arg max
θ∈Θ
˜ℓD(θ, ϕ′),
¯ϕξ(θ′, ϕ′) = arg max
ϕξ∈Φ
˜ℓξ(θ′, (ϕξ, ϕ′
−ξ))."
-DIMENSIONAL RL,0.6108232584916523,"Due to compactness, h is well-defined. By strong concavity of ˜ℓD(·; (θ′, ϕ′)) and ˜ℓξ(·; (θ′, ϕ′)), it
960"
-DIMENSIONAL RL,0.6113989637305699,"follows that ¯θ, ¯ϕ are continuous self-mapping from Θ × Φ|Ξ| to itself. By Brouwer’s fixed point
961"
-DIMENSIONAL RL,0.6119746689694876,"theorem, there exists at least one (θ∗, ϕ∗) ∈Θ × Φ|Ξ| such that h(θ∗, ϕ∗) = (θ∗, ϕ∗). Then, one can
962"
-DIMENSIONAL RL,0.6125503742084053,"verify that (θ∗, ϕ∗) is a meta-FOSE of the meta-SG with utility function ℓD and ℓξ, ξ ∈Ξ, in view of
963"
-DIMENSIONAL RL,0.613126079447323,"the following inequality
964"
-DIMENSIONAL RL,0.6137017846862406,"⟨∇θ˜ℓD(θ∗; (θ∗, ϕ∗)), θ −θ∗⟩= ⟨∇θℓD(θ∗, ϕ∗), θ −θ∗⟩"
-DIMENSIONAL RL,0.6142774899251583,"⟨∇ϕξ ˜ℓξ(θ∗; (θ∗, ϕ∗)), ϕξ −ϕ∗
ξ⟩= ⟨∇ϕξℓξ(θ∗, ϕ∗), ϕξ −ϕ∗
ξ⟩,"
-DIMENSIONAL RL,0.614853195164076,"therefore, the equilibrium conditions for meta-SG with utility functions ˜ℓD and {˜ℓξ}ξ∈Ξ are the same
965"
-DIMENSIONAL RL,0.6154289004029937,"as with utility functions ℓD and {ℓξ}ξ∈Ξ, hence the claim follows.
966"
-DIMENSIONAL RL,0.6160046056419114,"F.2
Proofs: Non-Asymptotic Analysis
967"
-DIMENSIONAL RL,0.616580310880829,"In the sequel, we make the following smoothness assumptions for every attack type ξ ∈Ξ. In
968"
-DIMENSIONAL RL,0.6171560161197467,"addition, we assume, for analytical simplicity, that all types of attackers are unconstrained, i.e., Φ is
969"
-DIMENSIONAL RL,0.6177317213586644,"the Euclidean space with proper finite dimension.
970"
-DIMENSIONAL RL,0.6183074265975821,"Assumption F.2 ((ξ-wise) Lipschitz smoothness). The functions LD and LA are continuously
971"
-DIMENSIONAL RL,0.6188831318364997,"diffrentiable in both θ and ϕ. Furthermore, there exists constants L11, L12, L21, and L22 such that
972"
-DIMENSIONAL RL,0.6194588370754174,"for all θ, θ1, θ2 ∈Θ and ϕ, ϕ1, ϕ2 ∈Φ, we have, for any ξ ∈Ξ,
973"
-DIMENSIONAL RL,0.6200345423143351,"∥∇θLD (θ1, ϕ, ξ) −∇θLD (θ2, ϕ, ξ)∥≤L11 ∥θ1 −θ2∥
(F7)
∥∇ϕLD (θ, ϕ1, ξ) −∇ϕLD (θ, ϕ2, ξ)∥≤L22 ∥ϕ1 −ϕ2∥
(F8)
∥∇θLD (θ, ϕ1, ξ) −∇θLD (θ, ϕ2, ξ)∥≤L12 ∥ϕ1 −ϕ2∥
(F9)
∥∇ϕLD (θ1, ϕ, ξ) −∇ϕLD (θ2, ϕ, ξ)∥≤L12 ∥θ1 −θ2∥
(F10)
∥∇ϕLA(θ, ϕ1, ξ) −∇ϕLA(θ, ϕ2, ξ)∥≤L21∥ϕ1 −ϕ2∥.
(F11)"
-DIMENSIONAL RL,0.6206102475532528,"We also make the following strict-competitiveness assumption. This notion can be treated as a
974"
-DIMENSIONAL RL,0.6211859527921704,"generalization of zero-sum games: if one joint action (aD, aA) leads to payoff increases for one
975"
-DIMENSIONAL RL,0.6217616580310881,"player, it must decrease the other’s payoff.
976"
-DIMENSIONAL RL,0.6223373632700058,"Assumption F.3 (Strict-Competitiveness). The BSMG is strictly competitive, i.e., there exist con-
977"
-DIMENSIONAL RL,0.6229130685089235,"stants c < 0, d such that ∀ξ ∈Ξ, s ∈S, aD, aA ∈AD × Aξ, rD(s, aD, aA) = crA(s, aD, aA) + d.
978"
-DIMENSIONAL RL,0.6234887737478411,"In adversarial FL, the untargeted attack naturally makes the game zero-sum (hence, SC). The purpose
979"
-DIMENSIONAL RL,0.6240644789867588,"of introducing Assumption F.3 is to establish the Danskin-type result [3] for the Stackelberg game
980"
-DIMENSIONAL RL,0.6246401842256765,"with nonconvex value functions (see Lemma F.5), which spares us from the Hessian inversion.
981"
-DIMENSIONAL RL,0.6252158894645942,"Lemma F.4 (Implicit Function Theorem (IFT) for Meta-SG). Suppose for (¯θ, ¯ϕ) ∈Θ × Φ|Ξ|,
982"
-DIMENSIONAL RL,0.6257915947035118,"ξ ∈Ξ we have ∇ϕLA(¯θ, ¯ϕ, ξ) = 0 the Hessian ∇2
ϕLA(¯θ, ¯ϕ, ξ) is non-singular. Then, there exists
983"
-DIMENSIONAL RL,0.6263672999424295,"a neighborhood Bε(¯θ), ε > 0 centered around ¯θ and a C1-function ϕ(·) : Bε(¯θ) →Φ|Ξ| such that
984"
-DIMENSIONAL RL,0.6269430051813472,"near (¯θ, ¯ϕ) the solution set {(θ, ϕ) ∈Θ × Φ|Ξ| : ∇ϕLA(θ, ϕ, ξ) = 0} is a C1-manifold locally near
985"
-DIMENSIONAL RL,0.6275187104202649,"(¯θ, ¯ϕ). The gradient ∇θϕ(θ) is given by −(∇2
ϕLA(θ, ϕ, ξ))−1∇2
ϕθLA(θ, ϕ, ξ).
986"
-DIMENSIONAL RL,0.6280944156591824,"Lemma F.5. Under assumptions F.2, 3.2, there exists {ϕξ : ϕξ ∈arg maxϕ LA(θ, ϕ, ξ)}ξ∈Ξ, such
that
∇θV (θ) = ∇θEξ∼Q,τ∼qJD(θ + η ˆ∇θJD(τ), ϕξ, ξ)."
-DIMENSIONAL RL,0.6286701208981001,"Moreover, the function V (θ) is L-Lipschitz-smooth, where L = L11 + L12L21 µ
987"
-DIMENSIONAL RL,0.6292458261370178,∥∇θV (θ1) −∇θV (θ2)∥≤L∥θ1 −θ2∥.
-DIMENSIONAL RL,0.6298215313759356,"Proof of Lemma F.5. First,
we show that for any θ1, θ2
∈
Θ, ξ
∈
Ξ,
and ϕ1
∈
988"
-DIMENSIONAL RL,0.6303972366148531,"arg maxϕ LA(θ1, ϕ, ξ), there exists ϕ2 ∈arg maxϕ LA(θ2, ϕ, ξ) such that ∥ϕ1 −ϕ2∥≤L12"
-DIMENSIONAL RL,0.6309729418537708,"µ ∥θ1 −
989"
-DIMENSIONAL RL,0.6315486470926885,"θ2∥. Indeed, based on smoothness assumption equation F11 and equation F10,
990"
-DIMENSIONAL RL,0.6321243523316062,"∥∇ϕLA(θ1, ϕ1, ξ) −∇ϕLA(θ2, ϕ1, ξ)∥≤L21∥θ1 −θ2∥,
∥∇ϕLD(θ1, ϕ1, ξ) −∇ϕLD(θ2, ϕ1, ξ)∥≤L12∥θ1 −θ2∥."
-DIMENSIONAL RL,0.6327000575705239,"Since ϕ2 ∈arg maxϕ LA(θ2, ϕ, ξ), ∇ϕLA(θ2, ϕ2, ξ) = 0. Apply PL condition to ∇ϕLA(θ, ϕ2, ξ),
991"
-DIMENSIONAL RL,0.6332757628094415,"max
ϕ
LA(θ1, ϕ, ξ) −LA(θ1, ϕ2, ξ) ≤1"
-DIMENSIONAL RL,0.6338514680483592,"2µ∥∇ϕLA(θ1, ϕ2, ξ)∥2 = 1"
-DIMENSIONAL RL,0.6344271732872769,"2µ∥∇ϕLA(θ1, ϕ2, ξ) −∇ϕLA(θ2, ϕ2, ξ)∥2"
-DIMENSIONAL RL,0.6350028785261946,"≤L2
21
2µ ∥θ1 −θ2∥2
by equation F11."
-DIMENSIONAL RL,0.6355785837651122,"Since PL condition implies quadratic growth, we also have
992"
-DIMENSIONAL RL,0.6361542890040299,"LA(θ1, ϕ1, ξ) −LA(θ1, ϕ2, ξ) ≥µ"
-DIMENSIONAL RL,0.6367299942429476,2 ∥ϕ1 −ϕ2∥2.
-DIMENSIONAL RL,0.6373056994818653,"Combining the two inequalities above we obtain the Lipschitz stability for ϕ∗
ξ(·), i.e.,"
-DIMENSIONAL RL,0.6378814047207829,∥ϕ1 −ϕ2∥≤L21
-DIMENSIONAL RL,0.6384571099597006,µ ∥θ1 −θ2∥.
-DIMENSIONAL RL,0.6390328151986183,"Second, show that ∇θV (θ) can be directly evaluated at {ϕ∗
ξ}ξ∈Ξ. Inspired by Danskin’s theorem, we
993"
-DIMENSIONAL RL,0.639608520437536,"first made the following argument, consider the definition of directional derivative. Let ℓ(θ, ϕ) :=
994"
-DIMENSIONAL RL,0.6401842256764536,"∇θEξ,τJD(θ + η ˆ∇JD(τ), ξ). For a constant τ and an arbitrary direction d,
995"
-DIMENSIONAL RL,0.6407599309153713,"ℓ(θ + τd, ϕ∗(θ + τd)) −ℓ(θ, ϕ∗(θ)))
= ℓ(θ + τd, ϕ∗(θ + τd)) −ℓ(θ + τd, ϕ∗(θ)) + ℓ(θ + τd, ϕ∗(θ)) −ℓ(θ, ϕ∗(θ))"
-DIMENSIONAL RL,0.641335636154289,"= ∇ϕℓ(θ + τd, ϕ∗(θ))⊤[ϕ∗(θ + τd) −ϕ∗(θ))]
|
{z
}
∆ϕ"
-DIMENSIONAL RL,0.6419113413932067,+o(∆ϕ2)
-DIMENSIONAL RL,0.6424870466321243,"+ τ∇θℓ(θ, ϕ∗(θ))T d + o(d2)."
-DIMENSIONAL RL,0.643062751871042,"Hence, a sufficient condition for the first equation is ∇ϕℓ(θ + τd, ϕ∗(θ)) = 0, meaning that ℓD(θ, ϕ)
996"
-DIMENSIONAL RL,0.6436384571099597,"and LA(θ, ϕ, ξ) share the first-order stationarity at every ϕ when fixing θ. Indeed, by Lemma F.4, we
997"
-DIMENSIONAL RL,0.6442141623488774,"have, the gradient is locally determined by
998"
-DIMENSIONAL RL,0.644789867587795,"∇θV = Eξ∼Q[∇θLD(θ, ϕξ, ξ) + (∇θϕξ(θ))⊤∇ϕLD(θ, ϕξ, ξ)]"
-DIMENSIONAL RL,0.6453655728267127,"= Eξ∼Q

∇θLD(θ, ϕξ, ξ) −[(∇2
ϕLA(θ, ϕ, ξ))−1∇2
ϕθLA(θ, ϕ, ξ)]⊤∇ϕLD(θ, ϕξ, ξ)

."
-DIMENSIONAL RL,0.6459412780656304,"Given a trajectory τ := (s1, at
D, at
A, . . . , aH
D, aH
A, sH+1), let RD(τ, ξ) := PH
t=1 γt−1rD(st, at, ξ)
999"
-DIMENSIONAL RL,0.6465169833045481,"and RD(τ, ξ) := PH
t=1 γt−1rD(st, at, ξ). Denote by µ(τ; θ, ϕ) the trajectory distribution, that the
1000"
-DIMENSIONAL RL,0.6470926885434658,"log probability of µ is given by
1001"
-DIMENSIONAL RL,0.6476683937823834,"log µ(τ; θ, ϕ) = H
X"
-DIMENSIONAL RL,0.6482440990213011,"t=1
(log πD(at
D|st; θ + η ˆ∇θJD(τ)) + log πA(at
A|st; ϕ) + log P(st+1|at
D, at
A, st)"
-DIMENSIONAL RL,0.6488198042602188,"According to the policy gradient theorem, we have
1002"
-DIMENSIONAL RL,0.6493955094991365,"∇ϕLD(θ, ϕ, ξ) = Eµ[RD(τ, ξ) H
X"
-DIMENSIONAL RL,0.6499712147380541,"t=1
∇ϕ log(πA(at
A|st; ϕ))],"
-DIMENSIONAL RL,0.6505469199769718,"∇ϕLA(θ, ϕ, ξ) = Eµ[RA(τ, ξ) H
X"
-DIMENSIONAL RL,0.6511226252158895,"t=1
∇ϕ log(πA(at
A|st; ϕ))]."
-DIMENSIONAL RL,0.6516983304548072,"By SC Assumption F.3, when ∇ϕLA(θ, ϕ, ξ) = 0, there exists c < 0, d, such that ∇ϕLD(θ, ϕ, ξ) =
1003"
-DIMENSIONAL RL,0.6522740356937248,"Eµ[cRA(τ, ξ) PH
t=1 ∇ϕ log(πA(at
A|st; ϕ))] + Eµ[PH
t=1 γt−1d PH
t=1 ∇ϕ log(πA(at
A|st; ϕ))] = 0.
1004"
-DIMENSIONAL RL,0.6528497409326425,"Hence ∇θV = Eξ∼Q[∇θLD(θ, ϕξ, ξ)].
1005"
-DIMENSIONAL RL,0.6534254461715602,"Third, V (θ) is also Lipschitz smooth. As we notice that, ℓD is Lipschitz smooth since Eξ∼Q is a
1006"
-DIMENSIONAL RL,0.6540011514104779,"linear operator, we have,
1007"
-DIMENSIONAL RL,0.6545768566493955,"∥∇θV (θ1) −∇θV (θ2)∥
≤∥∇θEξ∼QLD(θ1, ϕ1, ξ) −∇θEξ∼QLD(θ2, ϕ2, ξ)∥
= ∥∇θℓD(θ1, ϕ1) −∇θℓD(θ2, ϕ1) + ∇θℓD(θ2, ϕ1) −∇θℓD(θ2, ϕ2)∥
≤∥∇θℓD(θ1, ϕ1) −∇θℓD(θ2, ϕ1)∥+ ∥∇θℓD(θ2, ϕ1) −∇θℓD(θ2, ϕ2)∥
≤L11∥θ1 −θ2∥+ L12∥ϕ1 −ϕ2∥"
-DIMENSIONAL RL,0.6551525618883132,≤(L11 + L12L21
-DIMENSIONAL RL,0.6557282671272309,"µ
)∥θ1 −θ2∥,"
-DIMENSIONAL RL,0.6563039723661486,which implies the Lipschitz constant L = L11 + L12L21
-DIMENSIONAL RL,0.6568796776050662,"µ
.
1008"
-DIMENSIONAL RL,0.6574553828439839,"It is impossible to present the convergence theory without the assistance of some standard assumptions
1009"
-DIMENSIONAL RL,0.6580310880829016,"in batch reinforcement learning, of which the justification can be found in [14]. We also require some
1010"
-DIMENSIONAL RL,0.6586067933218193,"additional information about the parameter space and function structure. These assumptions are all
1011"
-DIMENSIONAL RL,0.6591824985607369,"stated in Assumption F.6.
1012"
-DIMENSIONAL RL,0.6597582037996546,"Assumption F.6.
1013"
-DIMENSIONAL RL,0.6603339090385723,"(a) The policy gradients are bounded, ∥∇θLD(θ, ϕ, ξ)∥≤G2, ∥∇ϕLA(θ, ϕ, ξ)∥≤G2 for all
1014"
-DIMENSIONAL RL,0.66090961427749,"θ, ϕ ∈Θ × Φ and ξ ∈Ξ.
1015"
-DIMENSIONAL RL,0.6614853195164075,"(b) The policy gradient estimations are unbiased, i.e.,
1016"
-DIMENSIONAL RL,0.6620610247553252,"E[ ˆ∇ϕJA(θt, ϕt
ξ, ξ) −∇ϕJA(θt, ϕt
ξ, ξ)] = 0"
-DIMENSIONAL RL,0.662636729994243,"(c) The variances for the stochastic gradients are bounded, i.e., for all θt, ϕt
ξ, ξ,
1017"
-DIMENSIONAL RL,0.6632124352331606,"E[∥ˆ∇ϕJA(θt, ϕt
ξ, ξ) −∇ϕJA(θt, ϕt
ξ, ξ)∥2] ≤σ2 Nb
."
-DIMENSIONAL RL,0.6637881404720783,"E[∥ˆ∇ϕJD(θt, ϕt
ξ, ξ) −∇θJD(θt, ϕt
ξ, ξ)∥2] ≤σ2 Nb
."
-DIMENSIONAL RL,0.6643638457109959,"(d) The parameter space Θ has diameter DΘ := supθ1,θ2∈Θ ∥θ1 −θ2∥; the initialization θ0
1018"
-DIMENSIONAL RL,0.6649395509499136,"admits at most DV function gap, i.e., DV := maxθ∈Θ V (θ) −V (θ0).
1019"
-DIMENSIONAL RL,0.6655152561888313,"(e) It holds that the parameters satisfy 0 < µ < −cL22.
1020"
-DIMENSIONAL RL,0.666090961427749,"Equipped with Assumption F.6 we are able to unfold our main result Theorem 3.3, before which
1021"
-DIMENSIONAL RL,0.6666666666666666,"we show in Lemma F.7 that ϕ∗
ξ can be efficiently approximated by the inner loop in the sense that
1022"
-DIMENSIONAL RL,0.6672423719055843,"∇θEξ∼QLD(θt, ϕt
ξ(NA), ξ) ≈∇θV (θt), where ϕt
ξ(NA) is the last iterate output of the attacker
1023"
-DIMENSIONAL RL,0.667818077144502,"policy.
1024"
-DIMENSIONAL RL,0.6683937823834197,"Lemma F.7. Under Assumption F.6, 3.2, F.3, and F.2, let ρ := 1 +
µ
cL22
∈(0, 1), ¯L =
1025"
-DIMENSIONAL RL,0.6689694876223373,"max{L11, L12, L22, L21, V∞} where V∞:= max{max ∥∇V (θ)∥, 1}. For all ε > 0, if the attacker
1026"
-DIMENSIONAL RL,0.669545192861255,"learning iteration NA and batch size Nb are large enough such that
1027"
-DIMENSIONAL RL,0.6701208981001727,"NA ≥
1
log ρ−1 log 32D2
V (2V∞+ LDΘ)4 ¯L|c|G2"
-DIMENSIONAL RL,0.6706966033390904,L2µ2ε4
-DIMENSIONAL RL,0.671272308578008,"Nb ≥32µL2
21D2
V (2V∞+ LDΘ)4"
-DIMENSIONAL RL,0.6718480138169257,"|c|L2
22σ2 ¯LLε4
,"
-DIMENSIONAL RL,0.6724237190558434,"then, for zt := ∇θEξ∼QLD(θt, ϕt
ξ(NA), ξ) −∇θV (θt),
1028"
-DIMENSIONAL RL,0.6729994242947611,"E[∥zt∥] ≤
Lε2"
-DIMENSIONAL RL,0.6735751295336787,"4DV (2V∞+ LDΘ)2 ,"
-DIMENSIONAL RL,0.6741508347725964,"and
1029"
-DIMENSIONAL RL,0.6747265400115141,"E[∥∇ϕLA(θt, ϕt
ξ(N), ξ)∥] ≤ε."
-DIMENSIONAL RL,0.6753022452504318,"Proof of Lemma F.7. Fixing a ξ ∈Ξ, due to Lipschitz smoothness,
1030"
-DIMENSIONAL RL,0.6758779504893494,"LD(θt, ϕt
ξ(N), ξ) −LD(θt, ϕt
ξ(N −1), ξ)"
-DIMENSIONAL RL,0.6764536557282671,"≤⟨∇ϕLD(θt, ϕt
ξ(N −1), ξ), ϕt
ξ(N) −ϕt
ξ(N −1)⟩+ L22"
-DIMENSIONAL RL,0.6770293609671848,"2 ∥ϕt
ξ(N) −ϕt
ξ(N −1)∥2."
-DIMENSIONAL RL,0.6776050662061025,"The inner loop updating rule ensures that when κA
=
1
L21 , ϕt
ξ(N) −ϕt
ξ(N −1)
=
1031"
-DIMENSIONAL RL,0.6781807714450202,"1
L21 ˆ∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ). Plugging it into the inequality, we arrive at
1032"
-DIMENSIONAL RL,0.6787564766839378,"LD(θt, ϕt
ξ(N), ξ) −LD(θt, ϕt
ξ(N −1), ξ)"
-DIMENSIONAL RL,0.6793321819228555,"≤
1
L21
⟨∇ϕLD(θt, ϕt
ξ(N −1), ξ), ˆ∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ)⟩+ L22"
-DIMENSIONAL RL,0.6799078871617732,"2L2
21
∥ˆ∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ)∥2."
-DIMENSIONAL RL,0.6804835924006909,"Therefore, we let (Ft
n)0≤n≤N be the filtration generated by σ({ϕt
ξ(τ)}ξ∈Ξ|τ ≤n) and take condi-
1033"
-DIMENSIONAL RL,0.6810592976396085,"tional expectations on Ft
n:
1034"
-DIMENSIONAL RL,0.6816350028785262,"E[V (θt) −ℓD(θt, ϕt(N))|Ft
N−1] ≤V (θt) −ℓD(θt, ϕt(N −1)) Eξ  1"
-DIMENSIONAL RL,0.6822107081174439,"L21
⟨∇ϕLD, ∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ)⟩+ L22"
-DIMENSIONAL RL,0.6827864133563616,"2L2
21
∥ˆ∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ)∥2

."
-DIMENSIONAL RL,0.6833621185952792,"By variance-bias decomposition, and Assumption F.6 (b) and (c),
1035"
-DIMENSIONAL RL,0.6839378238341969,"E[∥ˆ∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ)∥2|Ft
N−1]"
-DIMENSIONAL RL,0.6845135290731146,"= E[∥ˆ∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ) −∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ) + ∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ)∥2|Ft
N−1]"
-DIMENSIONAL RL,0.6850892343120323,"= E[∥( ˆ∇ϕ −∇ϕ)JA(θt
ξ, ϕt
ξ(N −1), ξ)∥2|Ft
N−1] + E[∥∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ)∥2|Ft
N−1]"
-DIMENSIONAL RL,0.6856649395509499,"+ E[2⟨( ˆ∇ϕ −∇ϕ)JA(θt
ξ, ϕt
ξ(N −1), ξ), ∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ)⟩|Ft
N−1] ≤σ2"
-DIMENSIONAL RL,0.6862406447898676,"Nb
+ ∥∇ϕJA(θt
ξ, ϕt
ξ(N −1), ξ)∥2."
-DIMENSIONAL RL,0.6868163500287853,"Applying the PL condition (Assumption 3.2), and Assumption F.6 (a) we obtain
1036"
-DIMENSIONAL RL,0.687392055267703,"E[V (θt) −ℓD(θ, ϕt(N))|ϕN−1] −V (θt) −ℓD(θ, ϕt(N −1)) ≤Eξ  1"
-DIMENSIONAL RL,0.6879677605066206,"L21
⟨∇ϕLD, ∇ϕLA(θt, ϕt
ξ(N −1), ξ)⟩+ L22"
-DIMENSIONAL RL,0.6885434657455383,"2L2
21
( σ2"
-DIMENSIONAL RL,0.689119170984456,"Nb
+ ∥∇ϕLA(θt, ϕt
ξ(N −1), ξ)∥2)
 = Eξ"
-DIMENSIONAL RL,0.6896948762233737,"
−
1
2L22
∥∇ϕLD∥2 +
1
2L22
∥∇ϕ(LD + L22"
-DIMENSIONAL RL,0.6902705814622913,"L21
LA)(θt, ϕt
ξ(N −1), ξ)∥2 + L22σ2"
-DIMENSIONAL RL,0.690846286701209,"2L2
21Nb "
-DIMENSIONAL RL,0.6914219919401267,"≤
µ
cL21
(max
ϕ
ℓD(θt, ϕ) −ℓD(θt, ϕt(N −1))) + L22σ2"
-DIMENSIONAL RL,0.6919976971790444,"2L2
21Nb
,"
-DIMENSIONAL RL,0.6925734024179621,"rearranging the terms yields
1037"
-DIMENSIONAL RL,0.6931491076568796,"E[V (θt) −ℓD(θt, ϕt(N))|Ft
n] ≤ρ(V (θt) −ℓD(θt, ϕt(N −1))) + L22σ2"
-DIMENSIONAL RL,0.6937248128957973,"2L2
21Nb
,"
-DIMENSIONAL RL,0.694300518134715,"where we use the fact that −maxϕ ℓD(θt, ϕ) ≤−V (θt). Telescoping the inequalities from τ = 0 to
1038"
-DIMENSIONAL RL,0.6948762233736328,"τ = N, we arrive at
1039"
-DIMENSIONAL RL,0.6954519286125503,"E[V (θt) −ℓD(θt, ϕt(N))] ≤ρN(V (θt) −ℓD(θt, ϕt(0))) + 1 −ρN 1 −ρ"
-DIMENSIONAL RL,0.696027633851468, L22σ2
-DIMENSIONAL RL,0.6966033390903857,"2L2
21Nb 
."
-DIMENSIONAL RL,0.6971790443293034,"PL-condition implies quadratic growth,
we also know that V (θt) −ℓD(θt, ϕt(N))
≤
1040 Eξ 1"
-DIMENSIONAL RL,0.697754749568221,"2µ∥∇ϕLD(θt, ϕt
ξ(N), ξ)∥2 ≤
1
2µG2, by Assumption F.3,
1041"
-DIMENSIONAL RL,0.6983304548071387,"∥ϕ∗
ξ(θt) −ϕt
ξ(N)∥2 ≤2"
-DIMENSIONAL RL,0.6989061600460564,"µ(LA(θt, ϕ∗
ξ, ξ) −LA(θt, ϕt
ξ(N), ξ)) ≤2|c| µ"
-DIMENSIONAL RL,0.6994818652849741,"LD(θt, ϕ∗
ξ, ξ) −LD(θt, ϕt
ξ(N), ξ)"
-DIMENSIONAL RL,0.7000575705238917,"Hence, with Jensen inequality and choice of NA and Nb,
1042"
-DIMENSIONAL RL,0.7006332757628094,"E[∥zt∥] = E[∥∇θV (θt) −Eξ∇θLD(θt, ϕt
ξ(NA), ξ)∥]"
-DIMENSIONAL RL,0.7012089810017271,"≤L12E[∥ϕt
ξ(NA) −ϕ∗
ξ∥] ≤L12 s 2|c|"
-DIMENSIONAL RL,0.7017846862406448,"µ E[V (θt) −ℓD(θt, ϕt(NA))] ≤L12 s"
-DIMENSIONAL RL,0.7023603914795624,"|c|
µ2 ρNAG2 + (1 −ρNA)|c|L2
22σ2"
-DIMENSIONAL RL,0.7029360967184801,"µL2
21Nb
."
-DIMENSIONAL RL,0.7035118019573978,"Now we adjust the size of NA and Nb to make E[∥zt∥] small enough, to this end, we set
1043"
-DIMENSIONAL RL,0.7040875071963155,ρNA |c|G2
-DIMENSIONAL RL,0.7046632124352331,"µ2
≤
ε4L2"
-DIMENSIONAL RL,0.7052389176741508,"32D2
V (2V∞+ LDΘ)4 ¯L"
-DIMENSIONAL RL,0.7058146229130685,"|c|L2
22σ2"
-DIMENSIONAL RL,0.7063903281519862,"L2
21Nb
≤
ε4L2µ2"
-DIMENSIONAL RL,0.7069660333909038,"32D2
V (2V∞+ LDΘ)4 ¯L,"
-DIMENSIONAL RL,0.7075417386298215,"which further indicates that
1044"
-DIMENSIONAL RL,0.7081174438687392,"NA ≥
1
log ρ−1 log 32D2
V (2V∞+ LDΘ)4 ¯L|c|G2"
-DIMENSIONAL RL,0.7086931491076569,L2µ2ε4
-DIMENSIONAL RL,0.7092688543465746,"Nb ≥32µL2
21D2
V (2V∞+ LDΘ)4"
-DIMENSIONAL RL,0.7098445595854922,"|c|L2
22σ2 ¯LLε4
."
-DIMENSIONAL RL,0.7104202648244099,"In the setting above, it is not hard to verify that
1045"
-DIMENSIONAL RL,0.7109959700633276,"E[∥zt∥] ≤
Lε2"
-DIMENSIONAL RL,0.7115716753022453,4DV (2V∞+ LDΘ)2 ≤ε.
-DIMENSIONAL RL,0.7121473805411629,"Also note that ∥∇ϕLA(θt, ϕt
ξ(NA), ξ)∥= ∥∇ϕLA(θt, ϕt
ξ(NA), ξ) −∇ϕLA(θt, ϕ∗
ξ, ξ)∥, given the
1046"
-DIMENSIONAL RL,0.7127230857800806,"proper choice of NA and Nb, one has
1047"
-DIMENSIONAL RL,0.7132987910189983,"E∥∇ϕLA(θt, ϕt
ξ(NA), ξ) −∇ϕLA(θt, ϕ∗
ξ, ξ)∥"
-DIMENSIONAL RL,0.713874496257916,"≤L21E[∥ϕt
ξ(NA) −ϕ∗
ξ∥] ≤
Lε2"
-DIMENSIONAL RL,0.7144502014968336,"4DV (2V∞+ LDΘ)2 ≤ε,"
-DIMENSIONAL RL,0.7150259067357513,"which indicates the ξ-wise inner loop stability.
1048"
-DIMENSIONAL RL,0.715601611974669,"Now we are ready to provide the convergence guarantee of the first-order outer loop.
1049"
-DIMENSIONAL RL,0.7161773172135867,"Theorem F.8. Under Assumption F.6, Assumption F.3, and Assumption F.2, let the stepsizes be,
1050"
-DIMENSIONAL RL,0.7167530224525043,"κA =
1
L22 , κD = 1"
-DIMENSIONAL RL,0.717328727691422,"L, if ND, NA, and Nb are large enough,
1051"
-DIMENSIONAL RL,0.7179044329303397,"ND ≥ND(ε) ∼O(ε−2)
NA ≥NA(ε) ∼O(log ε−1),
Nb ≥Nb(ε) ∼O(ε−4)"
-DIMENSIONAL RL,0.7184801381692574,"then there exists t ∈N such that (θt, {ϕt
ξ(NA)}ξ∈Ξ) is ε-meta-FOSE.
1052"
-DIMENSIONAL RL,0.719055843408175,"Proof. According to the update rule of the outer loop, (here we omit the projection analysis for
1053"
-DIMENSIONAL RL,0.7196315486470927,"simplicity)
1054"
-DIMENSIONAL RL,0.7202072538860104,θt+1 −θt = 1
-DIMENSIONAL RL,0.7207829591249281,"L
ˆ∇θℓD(θt, ϕt(NA)),"
-DIMENSIONAL RL,0.7213586643638457,"one has, due to unbiasedness assumption, let (Ft)0≤t≤ND be the filtration generated by σ(θt|k ≤t)
1055"
-DIMENSIONAL RL,0.7219343696027634,"E[⟨∇θℓD(θt, ϕt(NA)), θt+1 −θt⟩|Ft] = 1"
-DIMENSIONAL RL,0.7225100748416811,"LE[∥∇θℓD(θt, ϕt(NA))∥2|Ft]"
-DIMENSIONAL RL,0.7230857800805988,"= LE∥θt+1 −θt∥2|Ft],"
-DIMENSIONAL RL,0.7236614853195165,"which leads to
1056"
-DIMENSIONAL RL,0.724237190558434,"E[⟨∇θℓD(θt, ϕ∗), θt+1 −θt⟩|Ft] = E[⟨zt, θt −θt+1⟩|Ft] + LE[∥θt+1 −θt∥2∥]."
-DIMENSIONAL RL,0.7248128957973518,"Since V (·) is L-Lipschitz smooth,
1057"
-DIMENSIONAL RL,0.7253886010362695,"E[V (θt) −V (θt+1)] ≤E[⟨∇θV (θt), θt −θt+1⟩] + L"
-DIMENSIONAL RL,0.7259643062751872,2 E[∥θt+1 −θt∥2]
-DIMENSIONAL RL,0.7265400115141047,"≤E[⟨zt, θt+1 −θt⟩] −E[⟨∇θℓD(θt, ϕt(NA)), θt+1 −θt⟩] + L"
-DIMENSIONAL RL,0.7271157167530224,2 E[∥θt+1 −θt∥2]
-DIMENSIONAL RL,0.7276914219919401,"≤E[⟨zt, θt+1 −θt⟩] −L"
-DIMENSIONAL RL,0.7282671272308578,2 E[∥θt+1 −θt∥2]. (F12)
-DIMENSIONAL RL,0.7288428324697754,"Fixing a θ ∈Θ, let et := ⟨∇θℓD(θt, ϕt(NA)), θ −θt⟩, we have
1058"
-DIMENSIONAL RL,0.7294185377086931,"E[et|Ft] = LE[⟨θt+1 −θt, θ −θt⟩|Ft]"
-DIMENSIONAL RL,0.7299942429476108,"= E[⟨∇θℓD(θt, ϕt(NA)) −∇θV (θt), θt+1 −θt⟩+ ⟨∇θV (θt), θt+1 −θt⟩]"
-DIMENSIONAL RL,0.7305699481865285,"+ LE[⟨θt+1 −θt, θ −θt+1⟩]"
-DIMENSIONAL RL,0.7311456534254461,≤E[(∥zt∥+ V∞+ LDΘ)∥θt+1 −θt∥] (F13)
-DIMENSIONAL RL,0.7317213586643638,"By the choice of Nb, we have, since V∞= max{maxθ ∥∇V (θ)∥, 1},
1059"
-DIMENSIONAL RL,0.7322970639032815,"E[∥zt∥] ≤L12E[∥ϕN −ϕ∗∥] ≤
Lε2"
-DIMENSIONAL RL,0.7328727691421992,4DV (2V∞+ LDΘ) ≤V∞.
-DIMENSIONAL RL,0.7334484743811168,"Thus, the relation equation F13 can be reduced to
1060"
-DIMENSIONAL RL,0.7340241796200345,E[et] ≤(2V∞+ LDΘ)E[∥θt+1 −θt∥].
-DIMENSIONAL RL,0.7345998848589522,"Telescoping equation F12 yields
1061"
-DIMENSIONAL RL,0.7351755900978699,−DV ≤E[V (θ0) −V (θND)] ≤DΘ
-DIMENSIONAL RL,0.7357512953367875,"T −1
X"
-DIMENSIONAL RL,0.7363270005757052,"t=0
E[∥zt∥] −
L
2(2V∞+ LDΘ)2 E["
-DIMENSIONAL RL,0.7369027058146229,"T −1
X"
-DIMENSIONAL RL,0.7374784110535406,"t=0
E[e2
t|Ft]."
-DIMENSIONAL RL,0.7380541162924582,"Thus, setting ND ≥4DV (2V∞+LDΘ)2"
-DIMENSIONAL RL,0.7386298215313759,"Lε2
, and then by Lemma F.7, we obtain that,
1062"
ND,0.7392055267702936,"1
ND"
ND,0.7397812320092113,"ND−1
X"
ND,0.740356937248129,"t=0
E[e2
t] ≤ε2"
ND,0.7409326424870466,2 + 2DV (2V∞+ LDΘ)2
ND,0.7415083477259643,"LND
≤ε2"
ND,0.742084052964882,"which implies there exists t ∈{0, . . . , ND −1} such that E[e2
t] ≤ε2.
1063 1064"
ND,0.7426597582037997,"F.3
Generalization to Unseen Attacks
1065"
ND,0.7432354634427173,"In the online adaptation phase, the pre-trained meta-defense may be exposed to attacks unseen in the
1066"
ND,0.743811168681635,"pre-training phase, which poses an out-of-distribution (OOD) generalization issue to the proposed
1067"
ND,0.7443868739205527,"meta-SG framework. Yet, Proposition F.9 and Proposition F.13 assert that meta-SG is generalizable
1068"
ND,0.7449625791594704,"to the unseen attacks, given that the unseen is not distant from those seen. The formal statement is
1069"
ND,0.745538284398388,"deferred to Appendix F, and the proof mainly targets those unseen non-adaptive attacks for simplicity.
1070"
ND,0.7461139896373057,"Proposition F.9 (OOD Generalization Informal Statement). Consider sampled attack types
1071"
ND,0.7466896948762234,"ξ1, . . . , ξm during the pre-training and the unseen attack type ξm+1 in the online stage. The gen-
1072"
ND,0.7472654001151411,"eralization error is upper-bounded by the “discrepancy” between the unseen and the seen attacks
1073"
ND,0.7478411053540587,"C(ξm+1, {ξi}m
i=1).
1074"
ND,0.7484168105929764,"Our main goal is to quantify the value discrepancy under an attack type that is out of empirical
1075"
ND,0.7489925158318941,"distribution. We consider attack types ξ1, . . . , ξm to be empirically sampled from distribution Q(·)
1076"
ND,0.7495682210708118,"during the pre-training stage, and an unseen attack type ξm+1 in the online stage. The quantification
1077"
ND,0.7501439263097294,"of distance C(ξm+1, {ξi}m
i=1) relies on the total variation,
1078"
ND,0.7507196315486471,"Definition F.10 (total variation). For two distributions P and Q, defined over the sample space Ω
1079"
ND,0.7512953367875648,"and σ-field F, the total variation between P and Q is ∥P −Q∥T V := supU∈F |P(U) −Q(U)|.
1080"
ND,0.7518710420264825,"The celebrated result shows the following characterization of total variation,
1081"
ND,0.7524467472654001,"∥P −Q∥T V =
sup
f:0≤f≤1
Ex∼P [f(x)] −Ex∼Q[f(x)]."
ND,0.7530224525043178,"Let the fixed attack policies ϕi, i = 1, . . . , m + 1 corresponding to each attack type. To formalize
1082"
ND,0.7535981577432355,"the generalization error, for each θ ∈Θ, we define populational values
1083"
ND,0.7541738629821532,"ˆV (θ) := 1 m m
X"
ND,0.7547495682210709,"i=1
Eτ∼qθ
i JD(θ −η ˆ∇θJD(τ), ϕi, ξi)"
ND,0.7553252734599885,"ˆVm+1(θ) := Eτ∼qθ
m+1JD(θ −η ˆ∇θJD(τ), ϕm+1, ξm+1)"
ND,0.7559009786989062,"where qθ
i (·) : (S × A × S)H−1 × S →[0, 1] is the trajectory distribution determined by state
1084"
ND,0.7564766839378239,"dependent policies πD(·|s; θ), πA(·|s; ϕi, ξi) and transition kernel T . Since qθ
i is factorizable, we
1085"
ND,0.7570523891767416,"have Lemma F.11 to eliminate ∥qθ
i −qθ
m+1∥T V dependence on θ by upper bounding it using another
1086"
ND,0.7576280944156591,"pair of mariginal distributions.
1087"
ND,0.7582037996545768,"Lemma F.11. For any θ ∈Θ, there exist marginals di, dm+1 : (S × AA × S)H−1 × S →[0, 1]
1088"
ND,0.7587795048934945,"total variation ∥qθ
i −qθ
m+1∥T V can be bounded by ∥di −dm+1∥T V .
1089"
ND,0.7593552101324123,"Proof. By factorization, for a trajectory τ, any θ ∈Θ, and any type index i = 1, . . . , m + 1:
1090"
ND,0.7599309153713298,"qθ
i (τ) = H−1
Y"
ND,0.7605066206102475,"t=1
πD(at
D|st; θ) H−1
Y"
ND,0.7610823258491652,"t=1
πA(at
A|st, ϕi, ξi) H−1
Y"
ND,0.7616580310880829,"t=1
T (st+1|st, at),"
ND,0.7622337363270005,"thus, by the inequality of product measure,
1091"
ND,0.7628094415659182,"∥qθ
i −qθ
m+1∥T V ≤ H−1
X"
ND,0.7633851468048359,"t=1
∥πD(·|st; θ) −πD(·|st; θ)∥T V
|
{z
}
0"
ND,0.7639608520437536,"+∥di −dm+1∥T V ,"
ND,0.7645365572826712,"where di and dm+1 are the residue factors after removing πA(·|st; θ).
1092"
ND,0.7651122625215889,"Assumption F.12. For any ξ ∈Ξ and ϕξ, the function JD(θ, ϕξ, ξ) is G-Lipschitz continuous w.r.t.
1093"
ND,0.7656879677605066,"θ ∈Θ;
1094"
ND,0.7662636729994243,"Proposition F.13. Under assumption 3.2 and certain regularity conditions, fixing a policy θ ∈Θ, we
1095"
ND,0.7668393782383419,"have, there exist some marginal distribution of
1096"
ND,0.7674150834772596,"| ˆVm+1(θ) −ˆV (θ)| ≤C(dm+1, {di}m
i=1),
where the constant C depending on the total variation between dm+1 and {di}m
i=1:
1097"
ND,0.7679907887161773,"C(dm+1, {di}m
i=1) := 2ηG2 m m
X"
ND,0.768566493955095,"i=1
∥dm+1 −di∥T V + 1 −γH"
ND,0.7691421991940126,"1 −γ ∥dm+1 −1 m m
X"
ND,0.7697179044329303,"i=1
di∥T V ,"
ND,0.770293609671848,"here, G is the Lipschitz parameter of JD w.r.t. both θ.
1098"
ND,0.7708693149107657,"Proof. We start with the decomposition of the generalization error, for an arbitrary attack type ξi,
1099"
ND,0.7714450201496834,"i = 1, . . . , m, fixing a policy θ ∈Θ determines jointly with each ϕi the trajectory distribution qθ
i .
1100"
ND,0.772020725388601,"Denoting the one-step adaptation policy θ′(τ) = θ −η∇JD(τ) as a function of trajectory τ, we have
1101"
ND,0.7725964306275187,"the following decomposition,
1102"
ND,0.7731721358664364,"ˆVm+1(θ) −ˆV (θ) = Eτm+1∼qθ
m+1JD(θ′(τm+1), ϕm+1, ξm+1) −1 m m
X"
ND,0.7737478411053541,"i=1
Eτi∼qθ
i JD(θ′(τi), ϕi, ξi)"
ND,0.7743235463442717,"= Eτm+1∼qθ
m+1JD(θ′(τm+1), ϕm+1, ξm+1) −1 m m
X"
ND,0.7748992515831894,"i=1
Eτm+1∼qθ
m+1JD(θ′(τm+1), ϕi, ξi)"
ND,0.7754749568221071,"|
{z
}
(i) + 1 m m
X"
ND,0.7760506620610248,"i=1
Eτm+1∼qθ
m+1JD(θ′(τm+1), ϕi, ξi) −1 m m
X"
ND,0.7766263672999424,"i=1
Eτi∼qθ
i JD(θ′(τi), ϕi, ξi)"
ND,0.7772020725388601,"|
{z
}
(ii) ."
ND,0.7777777777777778,"We assume (τm+1, τi) is drawn from a joint distribution which has marginals qθ
m+1 and qθ
i and is
1103"
ND,0.7783534830166955,"corresponding to the maximal coupling of these two. Then,
1104"
ND,0.7789291882556131,"τm+1 ∼qθ
m+1,
τi ∼qθ
i ,
P(τm+1 ̸= τi) = ∥qθ
i −qθ
m+1∥T V ,"
ND,0.7795048934945308,"if τm+1 disagrees with τi, for (ii), we have, since Jθ
D is Lipschitz with respect to θ,
1105"
ND,0.7800805987334485,"∥JD(θ′(τm+1), ϕi, ξi) −JD(θ′(τi), ϕi, ξi)∥"
ND,0.7806563039723662,≤ηG∥ˆ∇θJD(τm+1) −ˆ∇θJD(τi)∥
ND,0.7812320092112838,"≤2ηG2,"
ND,0.7818077144502015,"as a result, denoting the maximal coupling of qθ
m+1 and qθ
i as gives,
1106"
ND,0.7823834196891192,"[Eτm+1∼qθ
m+1JD(θ′(τm+1), ϕi, ξi) −Eτi∼qθ
i JD(θ′(τi), ϕ, ξi)]"
ND,0.7829591249280369,"= E(τm+1,τi)∼Q(qθ
m+1,qθ
i )[JD(θ′(τm+1), ϕi, ξi) −JD(θ′(τi), ϕ, ξi)]"
ND,0.7835348301669545,"≤2ηG2∥qθ
m+1 −qθ
i ∥T V ≤2ηG2∥di −dm+1∥T V ,"
ND,0.7841105354058722,"where the last inequality is due to Lemma F.11. Averaging the m empirical ξi’s yeilds the result:
1107"
ND,0.7846862406447899,"(ii) ≤2ηG2 m m
X"
ND,0.7852619458837076,"i=1
∥di −dm+1∥T V ."
ND,0.7858376511226253,"Since the trajectory distribution is a product measure, the difference between qθ
i and qθ
m+1 only lies
1108"
ND,0.7864133563615429,"by attacker’s type, ∥qθ′(τm+1)
m+1
−qθ′(τm+1)
i
∥T V = ∥qθ
m+1 −qθ
i ∥T V ≤∥dm+1 −di∥T V .
1109"
ND,0.7869890616004606,"Now we bound (i), for ease of exposition we let q′′ = qθ′(τm+1)
m+1
and q′
i := qθ′(τm+1)
i
. By the finiteness
1110"
ND,0.7875647668393783,"of total trajectory reward R(τ) for any trajectory τ, R(τ) ≤1−γH"
ND,0.788140472078296,"1−γ , hence,
1111"
ND,0.7887161773172136,"(i) = Eτm+1∼qθ
m+1JD(θ′(τm+1), ϕm+1, ξm+1) −1 m m
X"
ND,0.7892918825561313,"i=1
Eτm+1∼qθ
m+1JD(θ′(τm+1), ϕi, ξi)"
ND,0.789867587795049,"= Eτm+1∼qθ
m+1 """
ND,0.7904432930339667,"Eτ ′′∼q′′RD(τ ′′) −1 m m
X"
ND,0.7910189982728842,"i=1
Eτ ′
i∼q′
iRD(τ ′
i) #"
ND,0.791594703511802,"≤Eτm+1∼qθ
m+1
1 −γH"
ND,0.7921704087507196,"1 −γ ∥q′′
m+1 −1 m m
X"
ND,0.7927461139896373,"i=1
q′
i∥T V"
ND,0.7933218192285549,≤1 −γH
ND,0.7938975244674726,"1 −γ ∥dm+1 −1 m m
X"
ND,0.7944732297063903,"i=1
di∥T V . 1112"
ND,0.795048934945308,"G
A Game-theoretic Perspective on Meta Equilibrium
1113"
ND,0.7956246401842256,"This section offers further justification for the meta-equilibrium in (meta-SE), and we argue that meta-
1114"
ND,0.7962003454231433,"equilibrium provides a data-driven approach to address incomplete information in dynamic games.
1115"
ND,0.796776050662061,"Note that information asymmetry is prevalent in the adversarial machine learning context, where the
1116"
ND,0.7973517559009787,"attacker enjoys an information advantage (e.g., the attacker’s type). The proposed meta-equilibrium
1117"
ND,0.7979274611398963,"notion can shed light on these related problems beyond the adversarial FL context.
1118"
ND,0.798503166378814,"We begin with the insufficiency of Bayesian Stackelberg equilibrium defined as the solution to the
1119"
ND,0.7990788716177317,"bilevel optimization in equation BSE in handling information asymmetry, a customary solution
1120"
ND,0.7996545768566494,"concept in security studies [35].
1121"
ND,0.8002302820955671,"max
θ∈Θ Eξ∼Q(·)[JD(θ, ϕ∗
ξ, ξ)]
s.t. ϕ∗
ξ ∈arg max JA(θ, ϕ, ξ), ∀ξ ∈Ξ.
(BSE)"
ND,0.8008059873344847,"One can see from equation BSE that such an equilibrium is of ex-ante type: the defender’s strategy is
1122"
ND,0.8013816925734024,"determined before the game starts. It targets a “representative” attacker (an average of all types). As
1123"
ND,0.8019573978123201,"the game unfolds, new information regarding the attacker’s private type is revealed (e.g., through
1124"
ND,0.8025331030512378,"the global model updates). However, this ex-ante strategy does not enable the defender to adjust its
1125"
ND,0.8031088082901554,"strategy as the game proceeds. Using game theory language, the defender fails to handle the emerging
1126"
ND,0.8036845135290731,"information in the interim stage.
1127"
ND,0.8042602187679908,"To create interim adaptability in this dynamic game of incomplete information, one can consider
1128"
ND,0.8048359240069085,"introducing the belief system to capture the defender’s learning process on the hidden type. Let It
1129"
ND,0.8054116292458261,"be the defender’s observations up to time t, i.e., It := (sk, ak
D)t
k=1st+1. Denote by B the belief
1130"
ND,0.8059873344847438,"generation operator bt+1(ξ) = B[It]. With the Bayesian equilibrium framework, the belief generation
1131"
ND,0.8065630397236615,"can be defined recursively as below
1132"
ND,0.8071387449625792,"bt+1(ξ) = B[st, at
D, bt] :=
bt(ξ)πA(at
A|st; ξ)T (st+1|st, at
A, at
D)
P"
ND,0.8077144502014968,"ξ′ bt(ξ′)πA(at
A|st; ξ′)T (st+1|st, at
A, at
D).
(G1)"
ND,0.8082901554404145,"Since bt is the defender’s belief on the hidden type at time t, its belief-dependent Markovian strategy
1133"
ND,0.8088658606793322,"is defined as πD(st, bt). Therefore, the interim equilibrium, also called Perfect Bayesian Equilibrium
1134"
ND,0.8094415659182499,"(PBE) [17] is given by a tuple (π∗
D, π∗
A, {bt}H
t=1) satisfying
1135"
ND,0.8100172711571675,"π∗
D = arg max Eξ∼QEπD,π∗
A[ H
X"
ND,0.8105929763960852,"t=1
rD(st, at
D, at
A)bt(ξ)]"
ND,0.8111686816350029,"π∗
A = arg max EπD,πA[ H
X"
ND,0.8117443868739206,"t=1
rA(st, at
D, at
A)], ∀ξ,"
ND,0.8123200921128382,"{bk}H
k=1 satisfies (G1) for realized actions and states. (PBE)"
ND,0.8128957973517559,"In contrast with (BSE), this perfect Bayesian equilibrium notion (PBE) enables the defender to make
1136"
ND,0.8134715025906736,"good use of the information revealed by the attacker, and subsequently adjust its actions according to
1137"
ND,0.8140472078295913,"the revealed information through the belief generation. From a game-theoretic viewpoint, both (PBE)
1138"
ND,0.8146229130685089,"and (meta-SE) create strategic online adaptation: the defender can infer and adapt to the attacker’s
1139"
ND,0.8151986183074266,"private type through the revealed information since different types aim at different objectives, hence,
1140"
ND,0.8157743235463443,"leading to different actions. Compared with PBE, the proposed meta-equilibrium notion is better
1141"
ND,0.816350028785262,"suited for large-scale complex systems where players’ decision variables can be high-dimensional
1142"
ND,0.8169257340241797,"and continuous, as argued in the ensuing paragraph.
1143"
ND,0.8175014392630973,"To achieve the strategic adaptation, PBE relies on the Bayesian-posterior belief updates, which soon
1144"
ND,0.818077144502015,"become intractable as the denominator in equation G1 involves integration over high-dimensional
1145"
ND,0.8186528497409327,"space and discretization inevitably leads to the curse of dimensionality. Despite the limited practicality,
1146"
ND,0.8192285549798504,"PBE is inherently difficult to solve, even in finite-dimensional cases. It is shown in [6] that the
1147"
ND,0.819804260218768,"equilibrium computation in games with incomplete information is NP-hard, and how to solve for
1148"
ND,0.8203799654576857,"PBE in dynamic games remains an open problem. Even though there have been encouraging attempts
1149"
ND,0.8209556706966034,"at solving PBE in two-stage games [36], it is still challenging to address PBE computation in generic
1150"
ND,0.8215313759355211,"Markov games.
1151"
ND,0.8221070811744386,"NeurIPS Paper Checklist
1152"
CLAIMS,0.8226827864133563,"1. Claims
1153"
CLAIMS,0.823258491652274,"Question: Do the main claims made in the abstract and introduction accurately reflect the
1154"
CLAIMS,0.8238341968911918,"paper’s contributions and scope?
1155"
CLAIMS,0.8244099021301093,"Answer: [Yes]
1156"
CLAIMS,0.824985607369027,"Justification: We propose a novel meta Stackelberg game to address adaptive and mixed
1157"
CLAIMS,0.8255613126079447,"poisoning attacks in federated learning.
1158"
CLAIMS,0.8261370178468624,"Guidelines:
1159"
CLAIMS,0.82671272308578,"• The answer NA means that the abstract and introduction do not include the claims
1160"
CLAIMS,0.8272884283246977,"made in the paper.
1161"
CLAIMS,0.8278641335636154,"• The abstract and/or introduction should clearly state the claims made, including the
1162"
CLAIMS,0.8284398388025331,"contributions made in the paper and important assumptions and limitations. A No or
1163"
CLAIMS,0.8290155440414507,"NA answer to this question will not be perceived well by the reviewers.
1164"
CLAIMS,0.8295912492803684,"• The claims made should match theoretical and experimental results, and reflect how
1165"
CLAIMS,0.8301669545192861,"much the results can be expected to generalize to other settings.
1166"
CLAIMS,0.8307426597582038,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
1167"
CLAIMS,0.8313183649971215,"are not attained by the paper.
1168"
LIMITATIONS,0.8318940702360391,"2. Limitations
1169"
LIMITATIONS,0.8324697754749568,"Question: Does the paper discuss the limitations of the work performed by the authors?
1170"
LIMITATIONS,0.8330454807138745,"Answer: [Yes]
1171"
LIMITATIONS,0.8336211859527922,"Justification: Please refer to conclusion section and Appendix B.
1172"
LIMITATIONS,0.8341968911917098,"Guidelines:
1173"
LIMITATIONS,0.8347725964306275,"• The answer NA means that the paper has no limitation while the answer No means that
1174"
LIMITATIONS,0.8353483016695452,"the paper has limitations, but those are not discussed in the paper.
1175"
LIMITATIONS,0.8359240069084629,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
1176"
LIMITATIONS,0.8364997121473805,"• The paper should point out any strong assumptions and how robust the results are to
1177"
LIMITATIONS,0.8370754173862982,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
1178"
LIMITATIONS,0.8376511226252159,"model well-specification, asymptotic approximations only holding locally). The authors
1179"
LIMITATIONS,0.8382268278641336,"should reflect on how these assumptions might be violated in practice and what the
1180"
LIMITATIONS,0.8388025331030512,"implications would be.
1181"
LIMITATIONS,0.8393782383419689,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
1182"
LIMITATIONS,0.8399539435808866,"only tested on a few datasets or with a few runs. In general, empirical results often
1183"
LIMITATIONS,0.8405296488198043,"depend on implicit assumptions, which should be articulated.
1184"
LIMITATIONS,0.8411053540587219,"• The authors should reflect on the factors that influence the performance of the approach.
1185"
LIMITATIONS,0.8416810592976396,"For example, a facial recognition algorithm may perform poorly when image resolution
1186"
LIMITATIONS,0.8422567645365573,"is low or images are taken in low lighting. Or a speech-to-text system might not be
1187"
LIMITATIONS,0.842832469775475,"used reliably to provide closed captions for online lectures because it fails to handle
1188"
LIMITATIONS,0.8434081750143926,"technical jargon.
1189"
LIMITATIONS,0.8439838802533103,"• The authors should discuss the computational efficiency of the proposed algorithms
1190"
LIMITATIONS,0.844559585492228,"and how they scale with dataset size.
1191"
LIMITATIONS,0.8451352907311457,"• If applicable, the authors should discuss possible limitations of their approach to
1192"
LIMITATIONS,0.8457109959700633,"address problems of privacy and fairness.
1193"
LIMITATIONS,0.846286701208981,"• While the authors might fear that complete honesty about limitations might be used by
1194"
LIMITATIONS,0.8468624064478987,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
1195"
LIMITATIONS,0.8474381116868164,"limitations that aren’t acknowledged in the paper. The authors should use their best
1196"
LIMITATIONS,0.8480138169257341,"judgment and recognize that individual actions in favor of transparency play an impor-
1197"
LIMITATIONS,0.8485895221646517,"tant role in developing norms that preserve the integrity of the community. Reviewers
1198"
LIMITATIONS,0.8491652274035694,"will be specifically instructed to not penalize honesty concerning limitations.
1199"
THEORY ASSUMPTIONS AND PROOFS,0.8497409326424871,"3. Theory Assumptions and Proofs
1200"
THEORY ASSUMPTIONS AND PROOFS,0.8503166378814048,"Question: For each theoretical result, does the paper provide the full set of assumptions and
1201"
THEORY ASSUMPTIONS AND PROOFS,0.8508923431203224,"a complete (and correct) proof?
1202"
THEORY ASSUMPTIONS AND PROOFS,0.8514680483592401,"Answer: [Yes]
1203"
THEORY ASSUMPTIONS AND PROOFS,0.8520437535981578,"Justification: Please refer to Appendix F.
1204"
THEORY ASSUMPTIONS AND PROOFS,0.8526194588370755,"Guidelines:
1205"
THEORY ASSUMPTIONS AND PROOFS,0.853195164075993,"• The answer NA means that the paper does not include theoretical results.
1206"
THEORY ASSUMPTIONS AND PROOFS,0.8537708693149108,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
1207"
THEORY ASSUMPTIONS AND PROOFS,0.8543465745538285,"referenced.
1208"
THEORY ASSUMPTIONS AND PROOFS,0.8549222797927462,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
1209"
THEORY ASSUMPTIONS AND PROOFS,0.8554979850316637,"• The proofs can either appear in the main paper or the supplemental material, but if
1210"
THEORY ASSUMPTIONS AND PROOFS,0.8560736902705814,"they appear in the supplemental material, the authors are encouraged to provide a short
1211"
THEORY ASSUMPTIONS AND PROOFS,0.8566493955094991,"proof sketch to provide intuition.
1212"
THEORY ASSUMPTIONS AND PROOFS,0.8572251007484168,"• Inversely, any informal proof provided in the core of the paper should be complemented
1213"
THEORY ASSUMPTIONS AND PROOFS,0.8578008059873344,"by formal proofs provided in appendix or supplemental material.
1214"
THEORY ASSUMPTIONS AND PROOFS,0.8583765112262521,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
1215"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8589522164651698,"4. Experimental Result Reproducibility
1216"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8595279217040875,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
1217"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8601036269430051,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
1218"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8606793321819228,"of the paper (regardless of whether the code and data are provided or not)?
1219"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8612550374208405,"Answer: [Yes]
1220"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8618307426597582,"Justification: Please refer to Appendix C.
1221"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8624064478986759,"Guidelines:
1222"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8629821531375935,"• The answer NA means that the paper does not include experiments.
1223"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8635578583765112,"• If the paper includes experiments, a No answer to this question will not be perceived
1224"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8641335636154289,"well by the reviewers: Making the paper reproducible is important, regardless of
1225"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8647092688543466,"whether the code and data are provided or not.
1226"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8652849740932642,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
1227"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8658606793321819,"to make their results reproducible or verifiable.
1228"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8664363845710996,"• Depending on the contribution, reproducibility can be accomplished in various ways.
1229"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8670120898100173,"For example, if the contribution is a novel architecture, describing the architecture fully
1230"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8675877950489349,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
1231"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8681635002878526,"be necessary to either make it possible for others to replicate the model with the same
1232"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8687392055267703,"dataset, or provide access to the model. In general. releasing code and data is often
1233"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.869314910765688,"one good way to accomplish this, but reproducibility can also be provided via detailed
1234"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8698906160046056,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
1235"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8704663212435233,"of a large language model), releasing of a model checkpoint, or other means that are
1236"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.871042026482441,"appropriate to the research performed.
1237"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8716177317213587,"• While NeurIPS does not require releasing code, the conference does require all submis-
1238"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8721934369602763,"sions to provide some reasonable avenue for reproducibility, which may depend on the
1239"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.872769142199194,"nature of the contribution. For example
1240"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8733448474381117,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
1241"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8739205526770294,"to reproduce that algorithm.
1242"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.874496257915947,"(b) If the contribution is primarily a new model architecture, the paper should describe
1243"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8750719631548647,"the architecture clearly and fully.
1244"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8756476683937824,"(c) If the contribution is a new model (e.g., a large language model), then there should
1245"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8762233736327001,"either be a way to access this model for reproducing the results or a way to reproduce
1246"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8767990788716177,"the model (e.g., with an open-source dataset or instructions for how to construct
1247"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8773747841105354,"the dataset).
1248"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8779504893494531,"(d) We recognize that reproducibility may be tricky in some cases, in which case
1249"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8785261945883708,"authors are welcome to describe the particular way they provide for reproducibility.
1250"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8791018998272885,"In the case of closed-source models, it may be that access to the model is limited in
1251"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8796776050662061,"some way (e.g., to registered users), but it should be possible for other researchers
1252"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8802533103051238,"to have some path to reproducing or verifying the results.
1253"
OPEN ACCESS TO DATA AND CODE,0.8808290155440415,"5. Open access to data and code
1254"
OPEN ACCESS TO DATA AND CODE,0.8814047207829592,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
1255"
OPEN ACCESS TO DATA AND CODE,0.8819804260218768,"tions to faithfully reproduce the main experimental results, as described in supplemental
1256"
OPEN ACCESS TO DATA AND CODE,0.8825561312607945,"material?
1257"
OPEN ACCESS TO DATA AND CODE,0.8831318364997122,"Answer: [Yes]
1258"
OPEN ACCESS TO DATA AND CODE,0.8837075417386299,"Justification: The datasets (MNIST and CIFAR-10) are open source, and we will publish the
1259"
OPEN ACCESS TO DATA AND CODE,0.8842832469775475,"codes during the final revision stage.
1260"
OPEN ACCESS TO DATA AND CODE,0.8848589522164652,"Guidelines:
1261"
OPEN ACCESS TO DATA AND CODE,0.8854346574553829,"• The answer NA means that paper does not include experiments requiring code.
1262"
OPEN ACCESS TO DATA AND CODE,0.8860103626943006,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
1263"
OPEN ACCESS TO DATA AND CODE,0.8865860679332181,"public/guides/CodeSubmissionPolicy) for more details.
1264"
OPEN ACCESS TO DATA AND CODE,0.8871617731721358,"• While we encourage the release of code and data, we understand that this might not be
1265"
OPEN ACCESS TO DATA AND CODE,0.8877374784110535,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
1266"
OPEN ACCESS TO DATA AND CODE,0.8883131836499712,"including code, unless this is central to the contribution (e.g., for a new open-source
1267"
OPEN ACCESS TO DATA AND CODE,0.8888888888888888,"benchmark).
1268"
OPEN ACCESS TO DATA AND CODE,0.8894645941278065,"• The instructions should contain the exact command and environment needed to run to
1269"
OPEN ACCESS TO DATA AND CODE,0.8900402993667242,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1270"
OPEN ACCESS TO DATA AND CODE,0.8906160046056419,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1271"
OPEN ACCESS TO DATA AND CODE,0.8911917098445595,"• The authors should provide instructions on data access and preparation, including how
1272"
OPEN ACCESS TO DATA AND CODE,0.8917674150834772,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1273"
OPEN ACCESS TO DATA AND CODE,0.8923431203223949,"• The authors should provide scripts to reproduce all experimental results for the new
1274"
OPEN ACCESS TO DATA AND CODE,0.8929188255613126,"proposed method and baselines. If only a subset of experiments are reproducible, they
1275"
OPEN ACCESS TO DATA AND CODE,0.8934945308002303,"should state which ones are omitted from the script and why.
1276"
OPEN ACCESS TO DATA AND CODE,0.8940702360391479,"• At submission time, to preserve anonymity, the authors should release anonymized
1277"
OPEN ACCESS TO DATA AND CODE,0.8946459412780656,"versions (if applicable).
1278"
OPEN ACCESS TO DATA AND CODE,0.8952216465169833,"• Providing as much information as possible in supplemental material (appended to the
1279"
OPEN ACCESS TO DATA AND CODE,0.895797351755901,"paper) is recommended, but including URLs to data and code is permitted.
1280"
OPEN ACCESS TO DATA AND CODE,0.8963730569948186,"6. Experimental Setting/Details
1281"
OPEN ACCESS TO DATA AND CODE,0.8969487622337363,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1282"
OPEN ACCESS TO DATA AND CODE,0.897524467472654,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1283"
OPEN ACCESS TO DATA AND CODE,0.8981001727115717,"results?
1284"
OPEN ACCESS TO DATA AND CODE,0.8986758779504893,"Answer: [Yes]
1285"
OPEN ACCESS TO DATA AND CODE,0.899251583189407,"Justification: Please check Appendix C.
1286"
OPEN ACCESS TO DATA AND CODE,0.8998272884283247,"Guidelines:
1287"
OPEN ACCESS TO DATA AND CODE,0.9004029936672424,"• The answer NA means that the paper does not include experiments.
1288"
OPEN ACCESS TO DATA AND CODE,0.90097869890616,"• The experimental setting should be presented in the core of the paper to a level of detail
1289"
OPEN ACCESS TO DATA AND CODE,0.9015544041450777,"that is necessary to appreciate the results and make sense of them.
1290"
OPEN ACCESS TO DATA AND CODE,0.9021301093839954,"• The full details can be provided either with the code, in appendix, or as supplemental
1291"
OPEN ACCESS TO DATA AND CODE,0.9027058146229131,"material.
1292"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9032815198618307,"7. Experiment Statistical Significance
1293"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9038572251007484,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1294"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9044329303396661,"information about the statistical significance of the experiments?
1295"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9050086355785838,"Answer: [Yes]
1296"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9055843408175014,"Justification: The error bars are added to Figure 11 (c) and (d), while random seeds are fixed
1297"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9061600460564191,"for other figures/tables.
1298"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9067357512953368,"Guidelines:
1299"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9073114565342545,"• The answer NA means that the paper does not include experiments.
1300"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9078871617731722,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1301"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9084628670120898,"dence intervals, or statistical significance tests, at least for the experiments that support
1302"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9090385722510075,"the main claims of the paper.
1303"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9096142774899252,"• The factors of variability that the error bars are capturing should be clearly stated (for
1304"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9101899827288429,"example, train/test split, initialization, random drawing of some parameter, or overall
1305"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9107656879677605,"run with given experimental conditions).
1306"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9113413932066782,"• The method for calculating the error bars should be explained (closed form formula,
1307"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9119170984455959,"call to a library function, bootstrap, etc.)
1308"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9124928036845136,"• The assumptions made should be given (e.g., Normally distributed errors).
1309"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9130685089234312,"• It should be clear whether the error bar is the standard deviation or the standard error
1310"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9136442141623489,"of the mean.
1311"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9142199194012666,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1312"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9147956246401843,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1313"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9153713298791019,"of Normality of errors is not verified.
1314"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9159470351180196,"• For asymmetric distributions, the authors should be careful not to show in tables or
1315"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9165227403569373,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1316"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.917098445595855,"error rates).
1317"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9176741508347726,"• If error bars are reported in tables or plots, The authors should explain in the text how
1318"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9182498560736903,"they were calculated and reference the corresponding figures or tables in the text.
1319"
EXPERIMENTS COMPUTE RESOURCES,0.918825561312608,"8. Experiments Compute Resources
1320"
EXPERIMENTS COMPUTE RESOURCES,0.9194012665515257,"Question: For each experiment, does the paper provide sufficient information on the com-
1321"
EXPERIMENTS COMPUTE RESOURCES,0.9199769717904432,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1322"
EXPERIMENTS COMPUTE RESOURCES,0.9205526770293609,"the experiments?
1323"
EXPERIMENTS COMPUTE RESOURCES,0.9211283822682786,"Answer: [Yes]
1324"
EXPERIMENTS COMPUTE RESOURCES,0.9217040875071963,"Justification: Please see Appendix C.
1325"
EXPERIMENTS COMPUTE RESOURCES,0.9222797927461139,"Guidelines:
1326"
EXPERIMENTS COMPUTE RESOURCES,0.9228554979850316,"• The answer NA means that the paper does not include experiments.
1327"
EXPERIMENTS COMPUTE RESOURCES,0.9234312032239493,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1328"
EXPERIMENTS COMPUTE RESOURCES,0.924006908462867,"or cloud provider, including relevant memory and storage.
1329"
EXPERIMENTS COMPUTE RESOURCES,0.9245826137017847,"• The paper should provide the amount of compute required for each of the individual
1330"
EXPERIMENTS COMPUTE RESOURCES,0.9251583189407023,"experimental runs as well as estimate the total compute.
1331"
EXPERIMENTS COMPUTE RESOURCES,0.92573402417962,"• The paper should disclose whether the full research project required more compute
1332"
EXPERIMENTS COMPUTE RESOURCES,0.9263097294185377,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1333"
EXPERIMENTS COMPUTE RESOURCES,0.9268854346574554,"didn’t make it into the paper).
1334"
CODE OF ETHICS,0.927461139896373,"9. Code Of Ethics
1335"
CODE OF ETHICS,0.9280368451352907,"Question: Does the research conducted in the paper conform, in every respect, with the
1336"
CODE OF ETHICS,0.9286125503742084,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1337"
CODE OF ETHICS,0.9291882556131261,"Answer: [Yes]
1338"
CODE OF ETHICS,0.9297639608520437,"Justification: We stick to the NeurIPS Code of Ethics.
1339"
CODE OF ETHICS,0.9303396660909614,"Guidelines:
1340"
CODE OF ETHICS,0.9309153713298791,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1341"
CODE OF ETHICS,0.9314910765687968,"• If the authors answer No, they should explain the special circumstances that require a
1342"
CODE OF ETHICS,0.9320667818077144,"deviation from the Code of Ethics.
1343"
CODE OF ETHICS,0.9326424870466321,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1344"
CODE OF ETHICS,0.9332181922855498,"eration due to laws or regulations in their jurisdiction).
1345"
BROADER IMPACTS,0.9337938975244675,"10. Broader Impacts
1346"
BROADER IMPACTS,0.9343696027633851,"Question: Does the paper discuss both potential positive societal impacts and negative
1347"
BROADER IMPACTS,0.9349453080023028,"societal impacts of the work performed?
1348"
BROADER IMPACTS,0.9355210132412205,"Answer: [Yes]
1349"
BROADER IMPACTS,0.9360967184801382,"Justification: Please see Appendix B.
1350"
BROADER IMPACTS,0.9366724237190558,"Guidelines:
1351"
BROADER IMPACTS,0.9372481289579735,"• The answer NA means that there is no societal impact of the work performed.
1352"
BROADER IMPACTS,0.9378238341968912,"• If the authors answer NA or No, they should explain why their work has no societal
1353"
BROADER IMPACTS,0.9383995394358089,"impact or why the paper does not address societal impact.
1354"
BROADER IMPACTS,0.9389752446747266,"• Examples of negative societal impacts include potential malicious or unintended uses
1355"
BROADER IMPACTS,0.9395509499136442,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1356"
BROADER IMPACTS,0.9401266551525619,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1357"
BROADER IMPACTS,0.9407023603914796,"groups), privacy considerations, and security considerations.
1358"
BROADER IMPACTS,0.9412780656303973,"• The conference expects that many papers will be foundational research and not tied
1359"
BROADER IMPACTS,0.9418537708693149,"to particular applications, let alone deployments. However, if there is a direct path to
1360"
BROADER IMPACTS,0.9424294761082326,"any negative applications, the authors should point it out. For example, it is legitimate
1361"
BROADER IMPACTS,0.9430051813471503,"to point out that an improvement in the quality of generative models could be used to
1362"
BROADER IMPACTS,0.943580886586068,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1363"
BROADER IMPACTS,0.9441565918249856,"that a generic algorithm for optimizing neural networks could enable people to train
1364"
BROADER IMPACTS,0.9447322970639033,"models that generate Deepfakes faster.
1365"
BROADER IMPACTS,0.945308002302821,"• The authors should consider possible harms that could arise when the technology is
1366"
BROADER IMPACTS,0.9458837075417387,"being used as intended and functioning correctly, harms that could arise when the
1367"
BROADER IMPACTS,0.9464594127806563,"technology is being used as intended but gives incorrect results, and harms following
1368"
BROADER IMPACTS,0.947035118019574,"from (intentional or unintentional) misuse of the technology.
1369"
BROADER IMPACTS,0.9476108232584917,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1370"
BROADER IMPACTS,0.9481865284974094,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1371"
BROADER IMPACTS,0.948762233736327,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1372"
BROADER IMPACTS,0.9493379389752447,"feedback over time, improving the efficiency and accessibility of ML).
1373"
SAFEGUARDS,0.9499136442141624,"11. Safeguards
1374"
SAFEGUARDS,0.9504893494530801,"Question: Does the paper describe safeguards that have been put in place for responsible
1375"
SAFEGUARDS,0.9510650546919976,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1376"
SAFEGUARDS,0.9516407599309153,"image generators, or scraped datasets)?
1377"
SAFEGUARDS,0.952216465169833,"Answer: [NA]
1378"
SAFEGUARDS,0.9527921704087507,"Justification: The paper poses no safeguards risks.
1379"
SAFEGUARDS,0.9533678756476683,"Guidelines:
1380"
SAFEGUARDS,0.953943580886586,"• The answer NA means that the paper poses no such risks.
1381"
SAFEGUARDS,0.9545192861255037,"• Released models that have a high risk for misuse or dual-use should be released with
1382"
SAFEGUARDS,0.9550949913644214,"necessary safeguards to allow for controlled use of the model, for example by requiring
1383"
SAFEGUARDS,0.9556706966033391,"that users adhere to usage guidelines or restrictions to access the model or implementing
1384"
SAFEGUARDS,0.9562464018422567,"safety filters.
1385"
SAFEGUARDS,0.9568221070811744,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1386"
SAFEGUARDS,0.9573978123200921,"should describe how they avoided releasing unsafe images.
1387"
SAFEGUARDS,0.9579735175590098,"• We recognize that providing effective safeguards is challenging, and many papers do
1388"
SAFEGUARDS,0.9585492227979274,"not require this, but we encourage authors to take this into account and make a best
1389"
SAFEGUARDS,0.9591249280368451,"faith effort.
1390"
LICENSES FOR EXISTING ASSETS,0.9597006332757628,"12. Licenses for existing assets
1391"
LICENSES FOR EXISTING ASSETS,0.9602763385146805,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1392"
LICENSES FOR EXISTING ASSETS,0.9608520437535981,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1393"
LICENSES FOR EXISTING ASSETS,0.9614277489925158,"properly respected?
1394"
LICENSES FOR EXISTING ASSETS,0.9620034542314335,"Answer: [Yes]
1395"
LICENSES FOR EXISTING ASSETS,0.9625791594703512,"Justification: We credited all assets (e.g., code, data, models) used in the paper.
1396"
LICENSES FOR EXISTING ASSETS,0.9631548647092688,"Guidelines:
1397"
LICENSES FOR EXISTING ASSETS,0.9637305699481865,"• The answer NA means that the paper does not use existing assets.
1398"
LICENSES FOR EXISTING ASSETS,0.9643062751871042,"• The authors should cite the original paper that produced the code package or dataset.
1399"
LICENSES FOR EXISTING ASSETS,0.9648819804260219,"• The authors should state which version of the asset is used and, if possible, include a
1400"
LICENSES FOR EXISTING ASSETS,0.9654576856649395,"URL.
1401"
LICENSES FOR EXISTING ASSETS,0.9660333909038572,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1402"
LICENSES FOR EXISTING ASSETS,0.9666090961427749,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1403"
LICENSES FOR EXISTING ASSETS,0.9671848013816926,"service of that source should be provided.
1404"
LICENSES FOR EXISTING ASSETS,0.9677605066206102,"• If assets are released, the license, copyright information, and terms of use in the
1405"
LICENSES FOR EXISTING ASSETS,0.9683362118595279,"package should be provided. For popular datasets, paperswithcode.com/datasets
1406"
LICENSES FOR EXISTING ASSETS,0.9689119170984456,"has curated licenses for some datasets. Their licensing guide can help determine the
1407"
LICENSES FOR EXISTING ASSETS,0.9694876223373633,"license of a dataset.
1408"
LICENSES FOR EXISTING ASSETS,0.970063327576281,"• For existing datasets that are re-packaged, both the original license and the license of
1409"
LICENSES FOR EXISTING ASSETS,0.9706390328151986,"the derived asset (if it has changed) should be provided.
1410"
LICENSES FOR EXISTING ASSETS,0.9712147380541163,"• If this information is not available online, the authors are encouraged to reach out to
1411"
LICENSES FOR EXISTING ASSETS,0.971790443293034,"the asset’s creators.
1412"
NEW ASSETS,0.9723661485319517,"13. New Assets
1413"
NEW ASSETS,0.9729418537708693,"Question: Are new assets introduced in the paper well documented and is the documentation
1414"
NEW ASSETS,0.973517559009787,"provided alongside the assets?
1415"
NEW ASSETS,0.9740932642487047,"Answer: [NA]
1416"
NEW ASSETS,0.9746689694876224,"Justification: We didn’t release new assets at this stage.
1417"
NEW ASSETS,0.97524467472654,"Guidelines:
1418"
NEW ASSETS,0.9758203799654577,"• The answer NA means that the paper does not release new assets.
1419"
NEW ASSETS,0.9763960852043754,"• Researchers should communicate the details of the dataset/code/model as part of their
1420"
NEW ASSETS,0.9769717904432931,"submissions via structured templates. This includes details about training, license,
1421"
NEW ASSETS,0.9775474956822107,"limitations, etc.
1422"
NEW ASSETS,0.9781232009211284,"• The paper should discuss whether and how consent was obtained from people whose
1423"
NEW ASSETS,0.9786989061600461,"asset is used.
1424"
NEW ASSETS,0.9792746113989638,"• At submission time, remember to anonymize your assets (if applicable). You can either
1425"
NEW ASSETS,0.9798503166378814,"create an anonymized URL or include an anonymized zip file.
1426"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804260218767991,"14. Crowdsourcing and Research with Human Subjects
1427"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810017271157168,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1428"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815774323546345,"include the full text of instructions given to participants and screenshots, if applicable, as
1429"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982153137593552,"well as details about compensation (if any)?
1430"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827288428324698,"Answer: [NA]
1431"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833045480713875,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1432"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838802533103052,"Guidelines:
1433"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844559585492227,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1434"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850316637881404,"human subjects.
1435"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856073690270581,"• Including this information in the supplemental material is fine, but if the main contribu-
1436"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861830742659758,"tion of the paper involves human subjects, then as much detail as possible should be
1437"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867587795048935,"included in the main paper.
1438"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873344847438111,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1439"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879101899827288,"or other labor should be paid at least the minimum wage in the country of the data
1440"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884858952216465,"collector.
1441"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890616004605642,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1442"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896373056994818,"Subjects
1443"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902130109383995,"Question: Does the paper describe potential risks incurred by study participants, whether
1444"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907887161773172,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1445"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913644214162349,"approvals (or an equivalent approval/review based on the requirements of your country or
1446"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9919401266551525,"institution) were obtained?
1447"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925158318940702,"Answer: [NA]
1448"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930915371329879,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1449"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936672423719056,"Guidelines:
1450"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942429476108232,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1451"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948186528497409,"human subjects.
1452"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953943580886586,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1453"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959700633275763,"may be required for any human subjects research. If you obtained IRB approval, you
1454"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965457685664939,"should clearly state this in the paper.
1455"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9971214738054116,"• We recognize that the procedures for this may vary significantly between institutions
1456"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976971790443293,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1457"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998272884283247,"guidelines for their institution.
1458"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988485895221646,"• For initial submissions, do not include any information that would break anonymity (if
1459"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9994242947610823,"applicable), such as the institution conducting the review.
1460"
