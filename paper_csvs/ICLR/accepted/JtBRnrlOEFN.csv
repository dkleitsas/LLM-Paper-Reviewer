Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033112582781456954,"State-of-the-art models in natural language processing rely on separate rigid sub-
word tokenization algorithms, which limit their generalization ability and adap-
tation to new settings. In this paper, we propose a new model inductive bias that
learns a subword tokenization end-to-end as part of the model. To this end, we
introduce a soft gradient-based subword tokenization module (GBST) that automat-
ically learns latent subword representations from characters in a data-driven fashion.
Concretely, GBST enumerates candidate subword blocks and learns to score them
in a position-wise fashion using a block scoring network. We additionally introduce
CHARFORMER, a deep Transformer model that integrates GBST and operates on
the byte level. Via extensive experiments on English GLUE, multilingual, and noisy
text datasets, we show that CHARFORMER outperforms a series of competitive
byte-level baselines while generally performing on par and sometimes outperform-
ing subword-based models. Additionally, CHARFORMER is fast, improving the
speed of both vanilla byte-level and subword-level Transformers by 28-100% while
maintaining competitive quality. We believe this work paves the way for highly
performant token-free models that are trained completely end-to-end."
INTRODUCTION,0.006622516556291391,"1
INTRODUCTION"
INTRODUCTION,0.009933774834437087,"Neural networks have achieved tremendous success in natural language processing (NLP) by replacing
feature-engineered models with stacks of functions that are learned end-to-end from vast amounts
of data (Mikolov et al., 2013; Peters et al., 2018; Howard and Ruder, 2018). The single component
of the traditional NLP pipeline (Manning and Schütze, 1999) that has so far resisted gradient-based
learning is tokenization, which is commonly applied as a pre-processing step. State-of-the-art
pre-trained language models (Devlin et al., 2019) generally rely on data-driven subword-based
tokenization algorithms (Schuster and Nakajima, 2012; Sennrich et al., 2016; Wu et al., 2016; Kudo
and Richardson, 2018) while expert-crafted segmentation algorithms are still common for languages
without whitespace separation such as Chinese, Thai, and Korean (cf. Lample and Conneau, 2019)."
INTRODUCTION,0.013245033112582781,"This reliance on rigid tokenization methods introduces a bottleneck into current NLP systems that
limits their capabilities. Subword segmentation algorithms split tokens into subwords solely based on
frequency, without taking into account lexical or semantic similarity. As a result, models are brittle to
rare words (Gong et al., 2018) and perturbations, both natural and adversarial (Belinkov and Bisk,
2018; Pruthi et al., 2019; Sun et al., 2020). In multilingual models, tokens in low-resource languages
are split into many subwords, which impacts performance on those languages and deteriorates cross-
lingual transfer (Hu et al., 2020; Wang et al., 2021). Finally, a separate tokenization algorithm leads to
a mismatch between the pre-training and downstream distribution of words when adapting pre-trained
language models to new settings, which requires signiﬁcant engineering effort to overcome."
INTRODUCTION,0.016556291390728478,"The direct application of character-level modelling into pre-trained language models in turn results in
severely increased computational and memory complexity due to an increased sequence length and
generally lower performance. To address this problem, we propose gradient-based subword tokeniza-
tion (GBST), a new method that combines the compositionality of character-level representations"
INTRODUCTION,0.019867549668874173,∗Equal Contribution
INTRODUCTION,0.023178807947019868,Published as a conference paper at ICLR 2022
INTRODUCTION,0.026490066225165563,"with the efﬁciency of subword tokenization while enabling end-to-end learning. Our method learns
latent subword representations from characters using large amounts of unlabeled data. Speciﬁcally,
GBST learns a position-wise soft selection over candidate subword blocks by scoring them with a
scoring network. In contrast to prior tokenization-free methods (Clark et al., 2021), GBST learns
interpretable latent subwords, which enables easy inspection of lexical representations and is more
efﬁcient than other byte-based models (Xue et al., 2021). Given that simply applying a standard
Transformer on a sequence of characters and bytes is computationally prohibitive, GBST paves the
way for usable, practical and highly performant character-level models. A high level overview of how
the GBST module is applied can be found at Figure 3 (Appendix)."
INTRODUCTION,0.029801324503311258,"We furthermore introduce CHARFORMER, a Transformer encoder-decoder model that uses GBST
to operate directly on the byte level. In addition, we experiment with a re-scaled variant of CHAR-
FORMER, which allocates additional capacity to the encoder to make up for the lack of discrete
subword embeddings."
INTRODUCTION,0.033112582781456956,"We evaluate our model on a range of standard and non-standard English, and multilingual downstream
tasks. On English GLUE and long document classiﬁcation tasks, CHARFORMER outperforms strong
byte-level baselines and overall achieves performance on par with subword-based models such as
BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020). On toxicity detection in social media datasets
(Borkan et al., 2019; Wulczyn et al., 2017), CHARFORMER outperforms byte-level baselines as
well as subword-based models, demonstrating robustness to spelling variation and non-standard
language. Finally, a multilingually pre-trained CHARFORMER performs on par or outperforms strong
subword-based multilingual baselines on standard cross-lingual datasets."
INTRODUCTION,0.03642384105960265,"We additionally demonstrate CHARFORMER is more efﬁcient compared to byte-level and subword-
based models with similar numbers of parameters. On a comparable setup, CHARFORMER out-
performs a baseline similar to the recent state-of-the-art byte-level model ByT5 (Xue et al., 2021)
while being 2× more memory efﬁcient and 10–93% faster. CHARFORMER also trains 28% faster than
the subword-level mT5 model (Xue et al., 2020), has 3× fewer parameters and achieves comparable
quality on well-established benchmarks. Finally, we demonstrate via visualization that the latent
subwords learned by CHARFORMER are interpretable to some extent."
CHARFORMER,0.039735099337748346,"2
CHARFORMER"
CHARFORMER,0.04304635761589404,"This section introduces our efﬁcient character-level architecture, CHARFORMER. CHARFORMER
is comprised of a Gradient-Based Subword Tokenization (GBST) module, followed by deep Trans-
former layers. The input to the GBST module is a sequence of characters or bytes1, which is then
downsampled to construct latent subwords."
CHARFORMER,0.046357615894039736,"2.1
GRADIENT-BASED SUBWORD TOKENIZATION (GBST)"
CHARFORMER,0.04966887417218543,"The input to GBST is a tensor of shape X ∈RL×d where L is the number of input characters and d is
the character embedding dimension. The key idea behind GBST is for the model to learn to perform
a latent subword segmentation of the input by selecting the most suitable subword block at every
character position. A block is a contiguous span of characters Xi:i+b of length b for 1 ≤i ≤L −b."
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.052980132450331126,"2.1.1
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS"
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.056291390728476824,"We ﬁrst enumerate all possible subword blocks of size b up to a maximum block size M. In
order to learn subword block embeddings, we use a non-parameterized strided pooling function
F : Rb×d →Rd that projects a subword block consisting of a sequence of character embeddings
Xi:i+b ∈Rb×d to a single subword block representation Xb,i ∈Rd for block size b at position i. We
compute subword blocks Xb,i with a stride s:"
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.059602649006622516,"Xb = [F(Xi:i+b); F(X(i+s):(i+s)+b); . . .]
(1)"
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.06291390728476821,"1We choose bytes rather than characters (Unicode code points) as this allows us to use a vocabulary of
256 possible byte values for all settings. We note that for languages with a Latin alphabet, many characters
correspond to a single byte. For other languages, each character corresponds to 2–3 bytes in general. For
simplicity and to align with prior work, we will generally talk about characters unless stated otherwise."
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.06622516556291391,Published as a conference paper at ICLR 2022
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.0695364238410596,"(a) Formation of subword blocks to be scored by FR.
Offsets and/or pre-GBST convolutions not shown."
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.0728476821192053,"(b) Block scores that have been expanded
back to length L. Softmax is taken over block
scores at each position i to form block weights for
constructing latent subword representations."
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.076158940397351,Figure 1: Illustration of subword block formation and scoring.
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.07947019867549669,"In practice we set s = b, thus Xb ∈R
L"
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.08278145695364239,"b ×d. The construction of latent subword blocks creates a
shorter overall sequence length by downsampling. We construct Xb for b ∈1, . . . , M, which can be
seen in Figure 1 for M = 4."
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.08609271523178808,"Considering Offsets
A limitation of a strided implementation is that it is unable to model all
possible subword windows. For instance, for the character sequence [a, b, c, d] we would only be able
to allocate [a, b] and [c, d] as subword blocks of length b = 2 and would ignore the subword block
[b, c]. Offsets can be used to model sliding windows of all possible subword blocks. We consider
enumerating all possible strided blocks by additionally shifting sequences up until the offset s. As this
increases computation, we instead propose to ﬁrst apply a 1D convolution to X, prior to enumerating
subword blocks. This effectively “smoothes” over the subword blocks. We use the variant with 1D
convolutions in our main experiments and provide additional ablations in §8.3 of the Appendix."
CONSTRUCTING CANDIDATE LATENT SUBWORD BLOCKS,0.08940397350993377,"Considering Intra-block Positions It is important to preserve the ordering of the characters within
the block Xi, Xi+1, . . . , Xi+b. E.g., the output of F should differ for the blocks abc and bca. For
certain choices of F it may be valuable to add a positional embedding (Vaswani et al., 2017) to
Xi:i+b before applying F. Note that this positional embedding would only be for individual blocks,
and is not global to the entire input sequence. That is, only positional embedding values for positions
1, . . . , b would be used. However, in practice we apply a 1D convolution before the GBST layer and
use the mean-pooling function for F. We ﬁnd this to be sufﬁcient to distinguish between same sized
blocks with different character orders."
BLOCK SCORING NETWORK,0.09271523178807947,"2.1.2
BLOCK SCORING NETWORK"
BLOCK SCORING NETWORK,0.09602649006622517,"In order to allow the model to learn which block to select for every character position, we introduce
a block scoring network. The block scoring network is simply a parameterized function FR(.) that
produces a score for each candidate block. Given a subword candidate block Xb,i ∈Rd, we compute
a score pb,i associated with the block using a simple linear transformation FR : Rd →R:"
BLOCK SCORING NETWORK,0.09933774834437085,"pb,i = FR(Xb,i)
(2)"
BLOCK SCORING NETWORK,0.10264900662251655,"We perform ranking of subword blocks with regard to each character position in the original sequence.
At every position i, the model learns to select the most suitable subword block Xb,i among all
block sizes 1 ≤b ≤M. As each sequence of subword blocks Xb is downsampled, we realign the
representations of the subword blocks by upsampling each Xb to its original sequence length L.
Speciﬁcally, for a block size of b, we replicate each block representation Xb,i b times. We then score
each candidate block at each position i using the softmax function:"
BLOCK SCORING NETWORK,0.10596026490066225,"Pi = softmax([p1,i, p1,i, · · · , pM,i]),
(3)"
BLOCK SCORING NETWORK,0.10927152317880795,"which computes a relative score of each candidate block at each position and Pi ∈RM. We show the
scoring of realigned blocks in Figure 1."
BLOCK SCORING NETWORK,0.11258278145695365,Published as a conference paper at ICLR 2022
FORMING LATENT SUBWORDS,0.11589403973509933,"2.1.3
FORMING LATENT SUBWORDS"
FORMING LATENT SUBWORDS,0.11920529801324503,"We then sum the representations of all subword blocks Xb,i at each position i multiplied by their
learned probability Pb,i to form a latent subword representation ˆ
Xi ∈Rd: ˆXi = M
X"
FORMING LATENT SUBWORDS,0.12251655629139073,"b
Pb,iXb,i
(4)"
FORMING LATENT SUBWORDS,0.12582781456953643,"Intuitively, the model learns an ideal subword block for each position. In contrast to standard
deterministic subword tokenization algorithms, this selection is soft and can thus consider different
possible segmentations at every position i. In general, however, this formulation still assumes that
subwords are contiguous sequences of characters. While additional context can be considered via the
convolutions in §2.1.1, non-concatenative morphology where morphemes are discontinuous may be
harder for the method to model.2"
POSITION-WISE SCORE CALIBRATION,0.1291390728476821,"2.1.4
POSITION-WISE SCORE CALIBRATION"
POSITION-WISE SCORE CALIBRATION,0.13245033112582782,"In the above approach, the scoring of each position is independent of other positions. We hypothesize
that it may be beneﬁcial for block scores at each position to be aware of each other. To this end, we
introduce an optional module that enables learning a consensus among block scores by calculating
dot products across the scores Pi across all positions i ∈[1, L]. This can be viewed as a form of
self-attention across block scores, albeit without any projections for computational efﬁciency. To
learn the new scores ˆP ∈RL×M, we compute ˆP = softmax(PP ⊤)P."
DOWNSAMPLING,0.1357615894039735,"2.1.5
DOWNSAMPLING"
DOWNSAMPLING,0.1390728476821192,"After learning a candidate block or mixture of blocks for each position, we use a downsampling func-
tion FD : RL×d →R
L
ds ×d that downsamples the sequence of latent subwords ˆX = [ ˆX1, . . . , ˆXL] to
˜X, reducing its sequence length by a factor of ds. We choose FD to be a non-parameterized mean
pooling operation. Notably, such simple stride-based pooling removes potential redundancies caused
by adjacent positions selecting similar blocks as the mean pool of two identical block embeddings
produces the same outcome. Intuitively, as the downsampling operation is ﬁxed, the parameterized
components preceding it should learn an optimal subword tokenization given the downsampling."
TRANSFORMER STACK,0.1423841059602649,"2.2
TRANSFORMER STACK"
TRANSFORMER STACK,0.1456953642384106,"The remainder of the CHARFORMER model remains identical to a regular Transformer encoder-
decoder model. The Transformer stack operates on the downsampled latent subwords ˜X instead of
subword embeddings."
TRANSFORMER STACK,0.1490066225165563,"Re-scaling of the Transformer Stack While subword-based models allocate much of their capacity
to subword embeddings—up to 71% of all parameters for contemporary multilingual models (Chung
et al., 2021)—, the character vocabulary of character-level models is much smaller and thus less
expressive. Similar to Xue et al. (2021), we hypothesize that character-level models require deeper
encoder stacks than subword-based models to make up for their smaller embedding capacity. Conse-
quently, we explore a scaling variant of CHARFORMER that puts more parameters at the encoder at the
expense of the decoder while preferring a deep narrow model over a larger wide model. Speciﬁcally,
we re-conﬁgure the Base model size to be similar to the T5 Small model size, with an expanded
24 layers in the encoder. The resulting CHARFORMERSBase (Scaled Base) has 134M parameters,
which is about 67% the parameter footprint of the standard base T5 model (200M parameters; Raffel
et al., 2020). Moreover, this particular CHARFORMER model is approximately 50-100% faster than
the T5 base model (see §4).3 For the re-scaled variant, we also used the GLU variant described in
(Shazeer, 2020) which is commonly referred to as the V1.1 variant in the T5 library."
FUTURE WORK COULD EXPLICITLY SEEK TO MODEL DISCONTINUOUS MORPHOLOGICAL PROCESSES BY CONSIDERING SKIP-,0.152317880794702,"2Future work could explicitly seek to model discontinuous morphological processes by considering skip-
grams in addition to character n-grams, although this would increase computational costs.
3The beneﬁts of such re-scaling have also been observed for subword-based encoder-decoder neural machine
translation models (Devlin, 2017; Kasai et al., 2021)."
FUTURE WORK COULD EXPLICITLY SEEK TO MODEL DISCONTINUOUS MORPHOLOGICAL PROCESSES BY CONSIDERING SKIP-,0.15562913907284767,Published as a conference paper at ICLR 2022
FUTURE WORK COULD EXPLICITLY SEEK TO MODEL DISCONTINUOUS MORPHOLOGICAL PROCESSES BY CONSIDERING SKIP-,0.15894039735099338,"A Note on Comparing Character-level and Subword-based Methods
Prior work on efﬁcient
methods generally compares models with the same number of parameters (Chung et al., 2021).
However, whereas embedding look-up even with large vocabularies in subword-based methods is
O(1), re-distributing the subword embedding parameters in character-level models such as ByT5
(Xue et al., 2021) to dense layers incurs much higher computational costs—a 25% penalty in training
speed. We believe that a fair re-scaling of character-level models should not only aim to match the
number of parameters but also the compute and inference costs of subword-based models under the
assumption that char/byte-level models will require longer sequences (see §4 for a comparison)."
FUTURE WORK COULD EXPLICITLY SEEK TO MODEL DISCONTINUOUS MORPHOLOGICAL PROCESSES BY CONSIDERING SKIP-,0.16225165562913907,"Span-based Pre-training Our pre-training scheme follows T5 quite closely. We mask N contiguous
characters and train to predict them in a sequence-to-sequence architecture following Xue et al. (2021).
The model optimizes the cross-entropy loss and is trained with teacher forcing."
EXPERIMENTS,0.16556291390728478,"3
EXPERIMENTS"
EXPERIMENTS,0.16887417218543047,"We evaluate our method both in English as well as in a multilingual setting on relevant benchmarks
and compare against state-of-the-art character-level and subword-based methods."
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.17218543046357615,"3.1
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.17549668874172186,"Data To showcase the effectiveness of the proposed method, we evaluate on a diverse set of standard
English tasks from GLUE covering sentiment classiﬁcation (SST-2; Socher et al., 2013), natural
language inference (MNLI, QNLI; Williams et al., 2018; Rajpurkar et al., 2016), paraphrase detection
(Dolan and Brockett, 2005, MRPC, QQP) and sentence similarity (Cer et al., 2017). In addition, we
evaluate on tasks that require dealing with long documents, both for sentiment analysis (IMDb; Maas
et al., 2011) and news classiﬁcation (AGNews; Zhang et al., 2015)."
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.17880794701986755,"Baselines We compare CHARFORMER against the following state-of-the-art subword-based models:
BERT (Devlin et al., 2019), an encoder-only pre-trained masked language model; and T5 (Raffel
et al., 2020), an encoder-decoder model. We also compare against Byte-level T5 (Xue et al., 2021), a
T5 model that is directly applied to bytes. We additionally evaluate the impact of the downsampling
in CHARFORMER by comparing it to the downsampling used by the character-level CANINE (Clark
et al., 2021) model in our framework. CANINE downsamples a character sequence using local
attention and pooling via strided convolutions. As the original CANINE uses an encoder-only model
and was only trained on multilingual data, we integrate CANINE-style downsampling into Byte-level
T5, which we refer to as Byte-level T5+LASC (local attention–strided convolution).4 As an ablation
for the GBST inductive bias, we compare against Byte-level T5+ConvBase a convolutional baseline
of Byte-level T5 with a 1D convolution of ﬁlter size 5 placed before the encoder. Note that in all the
baselines and for CHARFORMER base models, in the spirit of fair comparison, we compare them at
an equal parameterization (size). Our scaling experiments are reserved for our SBase models, which
is intended to only be compared with subword T5 models, and not to unscaled byte-level baselines.
Finally, we include an SBase scaled version of Byte-level T5 for comparison."
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.18211920529801323,"Setup
We evaluate Base and SBase conﬁgurations of CHARFORMER with 203M and 134M
parameters respectively. We compare to Base conﬁgurations of BERT and T5 that have a similar
number of parameters. We pre-train all models on the C4 corpus for 1M steps using a batch size
of 64 and sequence length of 1024. All non-subword models use a vocabulary of 256 bytes.5 Our
pre-training scheme corrupts spans with a mean length of 20 bytes. Each model is pre-trained on 16
TPU V3 chips. We pre-train our models with the Adafactor optimizer with an inverse square root
learning rate. We then ﬁne-tune on each individual task separately using a constant learning rate of
10−3. More details can be found in the Appendix."
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.18543046357615894,"4Compared to CANINE, Byte-level T5+LASC does not operate on Unicode codepoints and has a decoder. It
thus forgoes character hash embeddings and upsampling procedures respectively.
5Following Xue et al. (2021) we discard illegal UTF-8 sequences and reuse the ﬁnal 100 byte IDs as sentinel
tokens."
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.18874172185430463,Published as a conference paper at ICLR 2022
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.19205298013245034,"Table 1: Comparison of CHARFORMER against other subword and character-level models with
different parameter sizes on diverse standard English datasets."
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.19536423841059603,"Model
|θ|
SST-2
MNLI
QNLI
MRPC
QQP
STSB
COLA
AVG"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.1986754966887417,"BERTBase,Subword
110M
92.7
84.4/-
88.4
86.7/-
-
-
-
-
T5Base,Subword
220M
92.7
84.2/84.6
90.5
88.9/92.1
91.6/88.7
88.0
53.8
84.3"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.20198675496688742,"Byte-level T5Base
200M
91.6
82.5/82.7
88.7
87.3/91.0
90.9/87.7
84.3
45.1
81.5
Byte-level T5+ConvBase
205M
89.8
81.1/82.5
89.2
83.6/89.2
90.7/87.7
85.0
47.1
81.2
Byte-level T5+LASCBase
205M
90.0
80.0/80.8
87.1
82.8/88.1
89.0/85.4
83.7
25.3
77.0
CHARFORMERBase
203M
91.6
82.6/82.7
89.0
87.3/91.1
91.2/88.1
85.3
42.6
81.4"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.2052980132450331,"Byte-level T5SBase
133M
91.2
83.9/83.7
90.9
85.5/89.2
91.1/88.1
85.7
49.3
82.6
CHARFORMERSBase
134M
91.5
83.7/84.4
91.0
87.5/91.4
91.4/88.5
87.3
51.8
83.6"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.20860927152317882,"Table 2: Results on comment classiﬁcation on Civil Com-
ments and Wiki Comments. Metrics are accuracy and
AUC-PR. T5 baseline results are from (Tay et al., 2021)."
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.2119205298013245,"Model
Civil Comments
Wiki Comments"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.2152317880794702,"T5Base,Subword
81.2 / -
91.5 / -"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.2185430463576159,"Byte-level T5Base
82.8 / 78.7
93.2 / 75.4
Byte-level T5+LASCBase
82.9 / 78.2
93.0 / 75.0
CHARFORMERBase
83.0 / 78.8
92.7 / 79.7"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.22185430463576158,"CHARFORMERSBase
83.0 / 78.9
93.5 / 75.5"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.2251655629139073,"Table 3: Results on text classiﬁcation on
long documents."
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.22847682119205298,"Model
IMDb
News"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.23178807947019867,"T5Base,Subword
94.2
93.5"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.23509933774834438,"Byte-level T5Base
91.5
93.6
Byte-level T5+LASCBase
91.1
93.5
CHARFORMERBase
91.5
94.0"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.23841059602649006,"CHARFORMERSBase
94.4
94.1"
EXPERIMENTS ON MONOLINGUAL ENGLISH DATASETS,0.24172185430463577,"Results For all result tables, we divide the table into three sections: subword baseline(s), un-scaled
byte-level baselines, and scaled CHARFORMER results. If a section and task combination has more
than one model result, we underline the best result. We show result for GLUE in Table 1. CHAR-
FORMER outperforms other character-level baselines trained under the same conditions with the same
number of parameters across all tasks, while being considerably faster and requiring less compute
than T5-style models that are directly applied to bytes or characters (see §4). CHARFORMERSBase
performs even better despite having a smaller number of parameters compared to the Base conﬁgu-
ration, demonstrating the usefulness of rescaling the transformer stack for character-level models.
CHARFORMERSBase furthermore is the only model that performs on par or even outperforms the
standard subword-based models on some tasks in standard English. In Table 3 we provide results
for text classiﬁcation of long documents. Here, CHARFORMERSBase is the only byte-level model
to outperform T5Base,Subword on the IMDb classiﬁcation task, and both CHARFORMER models
outperform byte and subword level baselines on AGNews."
EXPERIMENTS ON NON-STANDARD ENGLISH DATASETS,0.24503311258278146,"3.2
EXPERIMENTS ON NON-STANDARD ENGLISH DATASETS"
EXPERIMENTS ON NON-STANDARD ENGLISH DATASETS,0.24834437086092714,"The previous set of experiments demonstrated the ability of CHARFORMER to perform well on clean
datasets consisting of standard English. However, character-level models are particularly suited to
data that is noisy, containing spelling variations, typos, and other non-standard language."
EXPERIMENTS ON NON-STANDARD ENGLISH DATASETS,0.25165562913907286,"Data To demonstrate CHARFORMER’s ability to perform well on such data, we evaluate on toxicity
detection using the Civil Comments (Borkan et al., 2019) and the Wikipedia Comments (Wulczyn
et al., 2017) datasets. Both are standard benchmarks that require estimating the toxicity of user-
generated content. We use the same setup as for the standard English datasets."
EXPERIMENTS ON NON-STANDARD ENGLISH DATASETS,0.25496688741721857,"Results
We show results in Table 2. Character-level models outperform the subword-based T5
model on both datasets, demonstrating their suitability to deal with such noisy, user-generated data.
CHARFORMER achieves performs on par or outperforms other character-level methods on both
datasets across the different model sizes."
MULTILINGUAL EXPERIMENTS,0.2582781456953642,"3.3
MULTILINGUAL EXPERIMENTS"
MULTILINGUAL EXPERIMENTS,0.26158940397350994,"Data To evaluate the effectiveness of character-level models on multilingual data, we evaluate on
standard cross-lingual question answering and classiﬁcation tasks. In particular, we evaluate on the
question answering tasks TyDiQA-GoldP (Clark et al., 2020), XQuAD (Artetxe et al., 2020), and
MLQA (Lewis et al., 2020) as well as the natural language inference task XNLI (Conneau et al., 2018)
and the paraphrase detection task PAWS-X (Yang et al., 2019) from XTREME (Hu et al., 2020). We
evaluate on the in-language multi-task setting for TyDiQA-GoldP (Clark et al., 2020) where models"
MULTILINGUAL EXPERIMENTS,0.26490066225165565,Published as a conference paper at ICLR 2022
MULTILINGUAL EXPERIMENTS,0.2682119205298013,"Table 4: Multilingual comparison of CHARFORMER against subword and byte-level models on
in-language multi-task, translate-train multi-task, and cross-lingual zero-shot (training on English)
settings. Model sizes are the same as those in Table 1. mBERT and mT5 baseline results are from
(Xue et al., 2020)."
MULTILINGUAL EXPERIMENTS,0.271523178807947,"In-Language
Translate-Train-All
Zero-Shot"
MULTILINGUAL EXPERIMENTS,0.27483443708609273,"Model
|θ|
TyDiQA-GoldP
XQuAD
MLQA
XNLI
PAWS-X
XNLI
PAWS-X"
MULTILINGUAL EXPERIMENTS,0.2781456953642384,"mBERTBase (Subword)
179M
77.6/68.0
-/-
-/-
-
-
65.4
81.9
mT5Base (Subword)
582M
80.8/70.0
75.3/59.7
67.6/48.5
75.9
89.3
75.4
86.4"
MULTILINGUAL EXPERIMENTS,0.2814569536423841,"Byte-level T5Base
200M
75.6/65.4
68.6/54.3
61.8/44.4
69.4
87.1
57.4
80.9
Byte-level T5+LASCBase
205M
70.6/59.7
66.8/52.1
58.8/41.1
67.9
84.8
55.2
79.0
CHARFORMERBase
203M
75.9/65.6
70.2/55.9
62.6/44.9
71.1
87.2
57.6
81.6"
MULTILINGUAL EXPERIMENTS,0.2847682119205298,"CHARFORMERSBase
134M
79.1/68.8
73.6/59.0
66.3/48.5
72.2
88.2
66.6
85.2
CHARFORMERSBase,LongP T
134M
81.2/71.3
74.2/59.8
67.2/49.4
72.8
88.6
67.8
83.7"
MULTILINGUAL EXPERIMENTS,0.28807947019867547,"Table 5: Comparison of pre-training compute metrics for mT5 (Subword) versus comparable qual-
ity CHARFORMER models on the mC4 dataset. 64 TPUv3 chips were used for this experiment.
CHARFORMERSBase sees the same number of tokens after downsampling as mT5Base, while
CHARFORMERSBase,LongP T roughly sees the same amount of raw text as mT5Base, given that a
SentencePiece subword token is about 4.1 bytes on average (Xue et al., 2021). CHARFORMERSBase
is 28% faster than mT5Base, while using 33% of the FLOPS."
MULTILINGUAL EXPERIMENTS,0.2913907284768212,"Model
Batch Size
L
ds
|θ|
Speed (steps/s)
FLOPS"
MULTILINGUAL EXPERIMENTS,0.2947019867549669,"mT5Base (Subword)
1024
1024
-
582M
1.54
1.3 × 1015"
MULTILINGUAL EXPERIMENTS,0.2980132450331126,"CHARFORMERSBase
1024
2048
2
134M
1.98
4.3 × 1014"
MULTILINGUAL EXPERIMENTS,0.30132450331125826,"CHARFORMERSBase,LongP T
2048
2048
2
134M
1.01
4.3 × 1014"
MULTILINGUAL EXPERIMENTS,0.304635761589404,"are ﬁne-tuned on the combined gold data in all target languages and the translate-train-all setting
where models are ﬁne-tuned on English training data plus translations in all target languages for the
other datasets. Both are the best-performing settings for the respective tasks in (Hu et al., 2020). In
addition, we evaluate on zero-shot cross-lingual transfer from English on XNLI and PAWS-X."
MULTILINGUAL EXPERIMENTS,0.3079470198675497,"Baselines We compare to strong multilingual subword-based baselines including multilingual BERT
(Devlin et al., 2019) and multilingual T5 (Xue et al., 2020). In addition, we compare to the byte-level
models from §3.1, which we pre-train on multilingual data."
MULTILINGUAL EXPERIMENTS,0.31125827814569534,"Setup We pre-train CHARFORMER as well as the Byte-level T5 and Byte-level T5+LASC baselines
on multilingual mC4 Common Crawl (Xue et al., 2020) in 101 languages. Base size models were
trained for 1M steps using a batch size of 64 and sequence length of 2048, with the exception
of Byte-level T5Base, which was trained with a sequence length of 1024, as training speed was
prohibitively slow (see Table 11). CHARFORMERSBase and CHARFORMERSBase,LongP T (longer
pre-training) are trained with larger batch sizes for fair comparison with mT5. In particular, CHAR-
FORMERSBase pre-trains on the same amount of tokens after downsampling as mT5Base, while
CHARFORMERSBase,LongP T pre-trains on roughly the same amount of raw text as mT5Base, given
that a SentencePiece subword token is about 4.1 bytes on average (Xue et al., 2021); see Table 5
for further details. All models were ﬁne-tuned with an input sequence length of 4096 for question-
answering tasks and 2048 for inference tasks. Score calibration was not used for these experiments,
as it did not beneﬁt the model in the multilingual setting. For XNLI and PAWS-X (both translate-train
and zero-shot settings), we also observed that performance improved if the GBST layer was not
updated during ﬁne-tuning; the reported CHARFORMER numbers reﬂect this conﬁguration. Otherwise,
all other hyper-parameters and model sizes are unchanged from the English experimental setup."
MULTILINGUAL EXPERIMENTS,0.31456953642384106,"Results
We show in-language multi-task, translate-train, and cross-lingual zero-shot results in
Table 4. CHARFORMERSBase is competitive with standard subword-based models and CHAR-
FORMERSBase,LongP T outperforms subword-based models on TyDiQA-GoldP (in-language multi-
task). Additionally, in the translate-train setting CHARFORMERSBase,LongP T is on par with subword
models on XQuAD and MLQA, and close to parity on PAWS-X. Furthermore, CHARFORMER
outperforms other character-level models in the zero-shot setting. However, we observe that this
setting still remains a challenge for token-free models in general. We hypothesize that model size
may be a major factor here. Finally, we provide additional comparison between GBST and LASC at
a ﬁxed down-sampling rate in Section 8.4 (Appendix), showing that GBST signiﬁcantly outperforms
LASC on TyDiQA."
MULTILINGUAL EXPERIMENTS,0.31788079470198677,Published as a conference paper at ICLR 2022
MULTILINGUAL EXPERIMENTS,0.3211920529801324,"Table 6: Pre-training compute metrics of models at different input lengths, downsampling rates,
and model sizes on the English C4 dataset. 16 TPUv3 chips were used for this experiment. These
numbers reﬂect a batch size of 64. Memory refers to per-device peak memory usage on TPUv3 chips."
MULTILINGUAL EXPERIMENTS,0.32450331125827814,"Model
L
ds
|θ|
Speed (steps/s)
FLOPS
Peak Mem."
MULTILINGUAL EXPERIMENTS,0.32781456953642385,"T5Base (Subword)
512
-
220M
9.3
1.1 × 1013
-"
MULTILINGUAL EXPERIMENTS,0.33112582781456956,"Byte-level T5Base
1024
1
200M
8.2
2.9 × 1013
3.09GB
Byte-level T5+LASCBase
1024
4
205M
15
9.9 × 1012
1.62GB
CHARFORMERBase
1024
2
206M
11
1.6 × 1013
1.95GB
CHARFORMERBase
1024
3
203M
15
1.1 × 1013
1.63GB
CHARFORMERSBase
1024
2
134M
14
1.3 × 1013
1.73GB
CHARFORMERSBase
1024
3
134M
20
8.7 × 1012
1.34GB"
MULTILINGUAL EXPERIMENTS,0.3344370860927152,"Figure 2: Visualization of block scores (softmax weights) for every byte position from multilingual
CHARFORMERSBase on an example English input."
MULTILINGUAL EXPERIMENTS,0.33774834437086093,"4
SPEED, MEMORY AND PARAMETERS"
MULTILINGUAL EXPERIMENTS,0.34105960264900664,"Table 6 reports the speed (global training steps per second), parameter sizes and number of ﬂoat-
ing point operations (FLOPS) for each forward pass of the models used in our experiments. All
experiments were run on 16 TPU-v3 chips and speed is benchmarked on English C4 pre-training
at the 1K input length (L). CHARFORMER models are generally more efﬁcient both in terms of
speed and FLOPS compared to other character-level models at different parameter sizes. With a low
down-sampling rate ds for CHARFORMER, Byte-level T5+LASC is more efﬁcient due to using a
higher down-sampling rate. Directly consuming the character sequence with a Transformer model
is slow and requires a large number of FLOPS, which is exacerbated with longer sequence lengths
where Byte-level T5 is more than 2× slower than the fastest CHARFORMER. This difference is even
larger at longer input sequence lengths, which we report in the Appendix. CHARFORMERSBase
achieves better performance (see §3) with fewer parameters but more FLOPS by using a deep thin
encoder and is twice as fast as the subword-based model with similar performance, T5Base."
VISUALIZING LATENT SUBWORDS,0.3443708609271523,"5
VISUALIZING LATENT SUBWORDS"
VISUALIZING LATENT SUBWORDS,0.347682119205298,"One beneﬁt of CHARFORMER compared to other character-level methods is that the subwords it
learns are directly interpretable and may give some indications to the behaviour of the underlying
model. We visualize the scores the multilingual CHARFORMER has learned to assign to subword
blocks of different sizes for the string ‘on subword tokenization’ in Figure 2. We observe that the
model learns to allocate single-character subword blocks predominantly to vowels and whitespace in
English. Moreover, in English the model allocates larger subword blocks to the beginning and end
consonants of a subword. Together, we believe this suggests that the model has learned a meaningful
segmentation of the input, and that it is able to dynamically mix between byte-level and subword-level
features. Such behaviour could also parallel the relative importance attributed to consonants for word
identiﬁcation observed during reading in humans (Lee et al., 2001; Carreiras et al., 2008)."
RELATED WORK,0.3509933774834437,"6
RELATED WORK"
RELATED WORK,0.3543046357615894,"Subword tokenization Standard algorithms for deterministic subword tokenization are Byte Pair
Encoding (BPE; Sennrich et al., 2016), Wordpiece (Wu et al., 2016), and SentencePiece (Kudo
and Richardson, 2018). Prior work has highlighted issues with some of these algorithms (Bostrom
and Durrett, 2020) and has generally observed that models learned with such rigid tokenization
do not cope well with variation in language (Sun et al., 2020). To make a model more robust to
morphological and compositional generalization, probabilistic segmentation algorithms such as"
RELATED WORK,0.3576158940397351,Published as a conference paper at ICLR 2022
RELATED WORK,0.3609271523178808,"subword regularization (Kudo, 2018) and BPE-dropout (Provilkov et al., 2020) have been proposed,
which sample different segmentations during training. Recent methods propose to make models
more robust for downstream tasks by enforcing prediction consistency between deterministic and
probabilistic segmentations (Wang et al., 2021) and propose to update the tokenizer based on the
downstream loss under different segmentations (Hiraoka et al., 2020; 2021). He et al. (2020)
proposed DPE (dynamic programming encoding), a segmentation-based tokenization algorithm based
on dynamic programming. Such methods, however, incur large computation costs due multiple
forward passes needing to be performed for each segmentation of an example or due to the expensive
DP computation, which make them unsuitable for pre-training."
RELATED WORK,0.36423841059602646,"Character-level models
For recurrent neural networks, pure character-level models that take a
sequence of characters as input (Graves, 2013; Zhang et al., 2015; Hwang and Sung, 2017) have
mostly been superseded by character-aware methods that compute a token-level representation using
a CNN over characters (Kim et al., 2016; Jozefowicz et al., 2016; Peters et al., 2018) due to poor
performance when learning directly from characters. Such character-aware representations have
lately been applied to deep Transformer models (El Boukkouri et al., 2020; Ma et al., 2020). These
methods, however, still require tokenization for pre-processing and cannot be directly applied to
languages without whitespace separation. Prior work also learned segmentation as part of the model
but did not scale very well (Wang et al., 2017; Kreutzer and Sokolov, 2018; Kawakami et al., 2019).
One notable exception is (Lee et al., 2017), which enabled fully character-level neural machine
translation, using stacked convolutions, max pooling, and highway networks. Building on this, recent
tokenization-free approaches such as CANINE (Clark et al., 2021) revisit the original character-level
setting in the context of large pre-trained language models with a focus on multilingual models.
Our method outperforms CANINE-style downsampling (local attention, strided convolutions) and
also leads to improvements in the monolingual setting, while using less compute and parameters to
down-sample than both Lee et al. (2017) and Clark et al. (2021). Recently, ByT5 (Xue et al., 2021)
set new start-of-the-art results for tokenization-free models, by operating on the byte-level. This work
performs on par with or outperforms ByT5, with signiﬁcant gains in speed and compute efﬁciency."
RELATED WORK,0.3675496688741722,"Multilingual models Current multilingual models are generally analogues to successful monolingual
Transformer models (Ruder et al., 2021). Consequently, models such as multilingual BERT (Devlin
et al., 2019) and XLM-R (Conneau et al., 2020) employ the same subword tokenization algorithms as
monolingual models, now applied to a massively multilingual corpus. In the multilingual setting, the
problems of subword-based tokenization are exacerbated as tokens in languages with few data are
over-segmented while high-frequency tokens are under-segmented, which limits cross-lingual transfer
(Wang et al., 2021). This motivates our work as well as recent work on character-level models."
RELATED WORK,0.3708609271523179,"Efﬁcient Transformers Moving from subwords to characters signiﬁcantly increases the sequence
length, which is an issue for Transformers due to the quadratic complexity of self-attention. Many ef-
ﬁcient self-attention models have been proposed (Choromanski et al., 2020; Wang et al., 2020; Zaheer
et al., 2020) to tackle this problem; see (Tay et al., 2020b;a) for a comprehensive overview. Notably,
the CANINE model uses local attention (Parmar et al., 2018), which could also be swapped with
another efﬁcient Transformer variant. We note that the problem of efﬁciency is important but not the
only challenge towards developing performant tokenization-free models. While applying an efﬁcient
attention mechanism might solve the fundamental computational costs of employing character-level
models, there is no guarantee that these models will learn locally meaningful compositions."
CONCLUSION,0.3741721854304636,"7
CONCLUSION"
CONCLUSION,0.37748344370860926,"We have proposed CHARFORMER, a re-scaled Transformer architecture that integrates gradient-based
subword tokenization, a novel lightweight tokenization method that enables efﬁcient end-to-end
learning of latent subwords directly from characters. We have demonstrated that English and
multilingual variants of CHARFORMER outperform strong character-level baselines across various
datasets while being more efﬁcient. CHARFORMER achieves performance on par with subword-based
models on standard English tasks and outperforms subword-based models on noisy social media
data. On multilingual data, CHARFORMER generally performs on par with subword-based models,
while being faster than both byte-level and subword-level baselines. Finally, we provide a method to
inspect the inner workings of the GBST module. Overall, we believe that the strong results presented
in this paper pave the way for highly effective and powerful token-free models."
CONCLUSION,0.38079470198675497,Published as a conference paper at ICLR 2022
CONCLUSION,0.3841059602649007,ACKNOWLEDGEMENTS
CONCLUSION,0.38741721854304634,"We would like to thank Jon Clark, Noah Constant, and Kris Cao for valuable feedback on drafts of
this manuscript."
ETHICS STATEMENT,0.39072847682119205,ETHICS STATEMENT
ETHICS STATEMENT,0.39403973509933776,"Standard subword tokenization algorithms produce segmentations that do not equally represents
words and phrases in different languages. Instead, they are biased towards languages that already
have many resources available, which leads to multilingual models performing worse on under-
represented languages (Wang et al., 2021). Tokenization-free approaches such as the one proposed in
this paper may help to ameliorate this to some extent. Another challenge to using large multilingual
models in practice is their relative computational inefﬁciency, which makes them unsuitable in
resource-constrained settings common in scenarios where under-represented languages are spoken.
CHARFORMER trains 28% faster than mT5 and has 3× fewer parameters, so may be a more suitable
choice in such settings compared to state-of-the-art multilingual models."
REPRODUCIBILITY STATEMENT,0.3973509933774834,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.40066225165562913,"All code to train the core byte-level Transformer encoder-decoder for CHARFORMER its variants
is already open-sourced as a part of the Mesh Tensorﬂow6 (Shazeer et al., 2018), T57 (Raffel et al.,
2020), and ByT58 (Xue et al., 2021) libraries. Additionally, an implementation of Charformer GBST
compatible with existing open-source models has been open-sourced9. We also include a simpliﬁed
Tensorﬂow implementation of GBST in Section 8.7 of the Appendix. All detailed experiment and
hyperparameter settings required to reproduce our experiments can be found in Section 8.2 of the
Appendix."
REFERENCES,0.40397350993377484,REFERENCES
REFERENCES,0.40728476821192056,"Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the Cross-lingual Transferability of
Monolingual Representations. In Proceedings of ACL 2020, 2020. URL http://arxiv.org/
abs/1910.11856."
REFERENCES,0.4105960264900662,"Yonatan Belinkov and Yonatan Bisk. Synthetic and Natural Noise Both Break Neural Machine
Translation. In Proceedings of ICLR 2018, 2018. URL http://arxiv.org/abs/1711.
02173."
REFERENCES,0.4139072847682119,"Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced
metrics for measuring unintended bias with real data for text classiﬁcation. CoRR, abs/1903.04561,
2019. URL http://arxiv.org/abs/1903.04561."
REFERENCES,0.41721854304635764,"Kaj Bostrom and Greg Durrett. Byte Pair Encoding is Suboptimal for Language Model Pretraining.
In Findings of EMNLP 2020, pages 4617–4624, 2020. doi: 10.18653/v1/2020.ﬁndings-emnlp.414."
REFERENCES,0.4205298013245033,"Manuel Carreiras, Margaret Gillon-Dowens, Marta Vergara, and Manuel Perea. Are vowels and
consonants processed differently? event-related potential evidence with a delayed letter paradigm.
Journal of Cognitive Neuroscience, 21(2):275–288, 2008."
REFERENCES,0.423841059602649,"Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task
1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint
arXiv:1708.00055, 2017."
REFERENCES,0.4271523178807947,"6https://github.com/tensorflow/mesh
7https://github.com/google-research/text-to-text-transfer-transformer
8https://github.com/google-research/byt5
9https://github.com/google-research/google-research/tree/master/
charformer"
REFERENCES,0.4304635761589404,Published as a conference paper at ICLR 2022
REFERENCES,0.4337748344370861,"Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794, 2020."
REFERENCES,0.4370860927152318,"Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, and Sebastian Ruder. Rethinking
Embedding Coupling in Pre-trained Language Models. In Proceedings of ICLR 2021, 2021."
REFERENCES,0.44039735099337746,"Jon Clark, Tom Kwiatkowski, Jennimaria Palomaki, Michael Collins, and Dan Garrette. TyDi QA: A
Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages. In
Transactions of the ACL, 2020."
REFERENCES,0.44370860927152317,"Jonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efﬁcient
tokenization-free encoder for language representation. arXiv preprint arXiv:2103.06874, 2021."
REFERENCES,0.4470198675496689,"Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger
Schwenk, and Veselin Stoyanov. XNLI: Evaluating Cross-lingual Sentence Representations. In
Proceedings of EMNLP 2018, 2018. URL http://arxiv.org/abs/1809.05053."
REFERENCES,0.4503311258278146,"Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-
supervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pages 8440–8451, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL
https://www.aclweb.org/anthology/2020.acl-main.747."
REFERENCES,0.45364238410596025,"Jacob Devlin. Sharp models on dull hardware: Fast and accurate neural machine translation decoding
on the cpu. arXiv preprint arXiv:1705.01991, 2017."
REFERENCES,0.45695364238410596,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding. In Proceedings of NAACL 2019, 2019.
URL http://arxiv.org/abs/1810.04805."
REFERENCES,0.4602649006622517,"William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005."
REFERENCES,0.46357615894039733,"Hicham El Boukkouri, Olivier Ferret, Thomas Lavergne, Hiroshi Noji, Pierre Zweigenbaum, and
Jun’ichi Tsujii. CharacterBERT: Reconciling ELMo and BERT for word-level open-vocabulary
representations from characters. In Proceedings of the 28th International Conference on Com-
putational Linguistics, pages 6903–6915, Barcelona, Spain (Online), December 2020. Interna-
tional Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.609. URL
https://www.aclweb.org/anthology/2020.coling-main.609."
REFERENCES,0.46688741721854304,"Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. FRAGE: Frequency-
Agnostic Word Representation. In Proceedings of NIPS 2018, 2018."
REFERENCES,0.47019867549668876,"Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,
2013."
REFERENCES,0.4735099337748344,"Xuanli He, Gholamreza Haffari, and Mohammad Norouzi. Dynamic programming encoding for
subword segmentation in neural machine translation. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, pages 3042–3051, Online, July 2020. Association
for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.275. URL https://www.
aclweb.org/anthology/2020.acl-main.275."
REFERENCES,0.4768211920529801,"Tatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki, and Naoaki Okazaki.
Optimiz-
ing word segmentation for downstream task.
In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020, pages 1341–1351, Online, November 2020. Association
for Computational Linguistics.
doi: 10.18653/v1/2020.ﬁndings-emnlp.120.
URL https:
//www.aclweb.org/anthology/2020.findings-emnlp.120."
REFERENCES,0.48013245033112584,"Tatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki, and Naoaki Okazaki. Joint Optimization
of Tokenization and Downstream Model. In Findings of ACL-IJCNLP 2021, 2021. URL http:
//arxiv.org/abs/2105.12410."
REFERENCES,0.48344370860927155,Published as a conference paper at ICLR 2022
REFERENCES,0.4867549668874172,"Jeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text Classiﬁcation.
In Proceedings of ACL 2018, 2018. URL http://arxiv.org/abs/1801.06146."
REFERENCES,0.4900662251655629,"Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson.
XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual General-
ization. In Proceedings of ICML 2020, 2020."
REFERENCES,0.49337748344370863,"Kyuyeon Hwang and Wonyong Sung. Character-level language modeling with hierarchical recurrent
neural networks.
In 2017 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 5720–5724. IEEE, 2017."
REFERENCES,0.4966887417218543,"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the
limits of language modeling. arXiv preprint arXiv:1602.02410, 2016."
REFERENCES,0.5,"Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah A. Smith. Deep Encoder, Shallow
Decoder: Reevaluating Non-autoregressive Machine Translation. In Proceedings of ICLR 2021,
2021. ISBN 0080437516."
REFERENCES,0.5033112582781457,"Kazuya Kawakami, Chris Dyer, and Phil Blunsom. Learning to discover, ground and use words with
segmental neural language models. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 6429–6441, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1645. URL https://www.aclweb.org/
anthology/P19-1645."
REFERENCES,0.5066225165562914,"Yoon Kim, Yacine Jernite, David Sontag, and Alexander Rush. Character-aware neural language
models. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 30, 2016."
REFERENCES,0.5099337748344371,"Julia Kreutzer and Artem Sokolov. Learning to segment inputs for nmt favors character-level
processing, 2018."
REFERENCES,0.5132450331125827,"Taku Kudo.
Subword regularization: Improving neural network translation models with mul-
tiple subword candidates.
In Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 66–75, Melbourne, Australia,
July 2018. Association for Computational Linguistics.
doi: 10.18653/v1/P18-1007.
URL
https://www.aclweb.org/anthology/P18-1007."
REFERENCES,0.5165562913907285,"Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71,
Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/
D18-2012. URL https://www.aclweb.org/anthology/D18-2012."
REFERENCES,0.5198675496688742,"Guillaume Lample and Alexis Conneau. Cross-lingual Language Model Pretraining. In Proceedings
of NeurIPS 2019, 2019. URL https://github.com/google-research/bert."
REFERENCES,0.5231788079470199,"Hye-Won Lee, Keith Rayner, and Alexander Pollatsek. The relative contribution of consonants and
vowels to word identiﬁcation during reading. Journal of Memory and Language, 44(2):189–205,
2001."
REFERENCES,0.5264900662251656,"Jason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine transla-
tion without explicit segmentation. Transactions of the Association for Computational Linguis-
tics, 5:365–378, 2017. doi: 10.1162/tacl_a_00067. URL https://aclanthology.org/
Q17-1026."
REFERENCES,0.5298013245033113,"Patrick Lewis, Barlas O˘guz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. MLQA: Evaluating
Cross-lingual Extractive Question Answering. In Proceedings of ACL 2020, 2020. URL http:
//arxiv.org/abs/1910.07475."
REFERENCES,0.5331125827814569,"Wentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin Wang, and Guoping Hu. CharBERT: Character-
aware pre-trained language model. In Proceedings of the 28th International Conference on
Computational Linguistics, pages 39–50, Barcelona, Spain (Online), December 2020. International
Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.4. URL https:
//www.aclweb.org/anthology/2020.coling-main.4."
REFERENCES,0.5364238410596026,Published as a conference paper at ICLR 2022
REFERENCES,0.5397350993377483,"Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human language technologies, pages 142–150, 2011."
REFERENCES,0.543046357615894,"Christopher Manning and Hinrich Schütze. Foundations of statistical natural language processing.
MIT press, 1999."
REFERENCES,0.5463576158940397,"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words
and Phrases and their Compositionality. In Advances in Neural Information Processing Systems,
2013."
REFERENCES,0.5496688741721855,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In International Conference on Machine Learning, pages
4055–4064. PMLR, 2018."
REFERENCES,0.5529801324503312,"Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-HLT
2018, 2018."
REFERENCES,0.5562913907284768,"Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. BPE-dropout: Simple and effective subword
regularization. In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, pages 1882–1892, Online, July 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.acl-main.170. URL https://www.aclweb.org/anthology/2020.
acl-main.170."
REFERENCES,0.5596026490066225,"Danish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. Combating adversarial misspellings
with robust word recognition. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 5582–5591, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1561. URL https://www.aclweb.org/
anthology/P19-1561."
REFERENCES,0.5629139072847682,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uniﬁed
Text-to-Text Transformer.
Journal of Machine Learning Research, 21, 2020.
URL http:
//arxiv.org/abs/1910.10683."
REFERENCES,0.5662251655629139,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016. Association
for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.
org/anthology/D16-1264."
REFERENCES,0.5695364238410596,"Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu,
Junjie Hu, Graham Neubig, and Melvin Johnson. Xtreme-r: Towards more challenging and
nuanced multilingual evaluation. arXiv preprint arXiv:2104.07412, 2021."
REFERENCES,0.5728476821192053,"Mike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5149–5152. IEEE, 2012."
REFERENCES,0.5761589403973509,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. In Proceedings of the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany, Au-
gust 2016. Association for Computational Linguistics.
doi: 10.18653/v1/P16-1162.
URL
https://www.aclweb.org/anthology/P16-1162."
REFERENCES,0.5794701986754967,"Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020."
REFERENCES,0.5827814569536424,"Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,
Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep
learning for supercomputers. arXiv preprint arXiv:1811.02084, 2018."
REFERENCES,0.5860927152317881,Published as a conference paper at ICLR 2022
REFERENCES,0.5894039735099338,"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,
pages 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/D13-1170."
REFERENCES,0.5927152317880795,"Lichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip Yu, and Caiming Xiong.
Adv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert. arXiv
preprint arXiv:2003.04985, 2020."
REFERENCES,0.5960264900662252,"Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient
transformers. arXiv preprint arXiv:2011.04006, 2020a."
REFERENCES,0.5993377483443708,"Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv
preprint arXiv:2009.06732, 2020b."
REFERENCES,0.6026490066225165,"Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler. Are
pre-trained convolutions better than pre-trained transformers? arXiv preprint arXiv:2105.03322,
2021."
REFERENCES,0.6059602649006622,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf."
REFERENCES,0.609271523178808,"Chong Wang, Yining Wang, Po-Sen Huang, Abdelrahman Mohamed, Dengyong Zhou, and Li Deng.
Sequence modeling via segmentations. In International Conference on Machine Learning, pages
3674–3683. PMLR, 2017."
REFERENCES,0.6125827814569537,"Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.6158940397350994,"Xinyi Wang, Sebastian Ruder, and Graham Neubig.
Multi-view Subword Regularization.
In
Proceedings of NAACL 2021, 2021. URL http://arxiv.org/abs/2103.08490."
REFERENCES,0.6192052980132451,"Adina Williams, Nikita Nangia, and Samuel Bowman.
A broad-coverage challenge corpus
for sentence understanding through inference.
In Proceedings of the 2018 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana,
June 2018. Association for Computational Linguistics.
doi: 10.18653/v1/N18-1101.
URL
https://www.aclweb.org/anthology/N18-1101."
REFERENCES,0.6225165562913907,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson,
Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith
Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex
Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s Neural
Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv
preprint arXiv:1609.08144, 2016."
REFERENCES,0.6258278145695364,"Ellery Wulczyn, Nithum Thain, and Lucas Dixon. Ex machina: Personal attacks seen at scale. In
Proceedings of the 26th International Conference on World Wide Web, WWW ’17, pages 1391–
1399, Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences
Steering Committee. ISBN 9781450349130. doi: 10.1145/3038912.3052591. URL https:
//doi.org/10.1145/3038912.3052591."
REFERENCES,0.6291390728476821,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer, 2020."
REFERENCES,0.6324503311258278,Published as a conference paper at ICLR 2022
REFERENCES,0.6357615894039735,"Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam
Roberts, and Colin Raffel. ByT5: Towards a token-free future with pre-trained byte-to-byte models.
arXiv preprint arXiv:2105.13626, 2021. URL http://arxiv.org/abs/2105.13626."
REFERENCES,0.6390728476821192,"Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A Cross-lingual Adversarial
Dataset for Paraphrase Identiﬁcation. In Proceedings of EMNLP 2019, 2019. URL http:
//arxiv.org/abs/1908.11828."
REFERENCES,0.6423841059602649,"Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer
sequences. arXiv preprint arXiv:2007.14062, 2020."
REFERENCES,0.6456953642384106,"Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level Convolutional Networks for Text
Classiﬁcation. Advances in Neural Information Processing Systems, pages 649–657, 2015. URL
http://arxiv.org/abs/1509.01626#."
REFERENCES,0.6490066225165563,Published as a conference paper at ICLR 2022
APPENDIX,0.652317880794702,"8
APPENDIX"
OVERVIEW,0.6556291390728477,"8.1
OVERVIEW"
OVERVIEW,0.6589403973509934,"Figure 3: High-level differences between traditional subword Transformer models and Charformer
which uses gradient-based subword tokenization."
HYPERPARAMETERS,0.6622516556291391,"8.2
HYPERPARAMETERS"
HYPERPARAMETERS,0.6655629139072847,This section describes the hyperparameters that we use in our experiments.
HYPERPARAMETERS,0.6688741721854304,"Monolingual English Datasets
Our small model follows the T5 small model size with 6 encoder
layers and 6 decoder layers, hidden size dmodel of 512, 8 heads, dkv of 32 and dff of 2048. This
corresponds to bi_v1_small.gin in the T5 codebase. The base model (corresponding to bi_v1.gin) has
12 encoder layers, 12 decoder layers, dmodel of 768, dff of 3072 and 12 heads. The SBase model has
24 encoder layers and 6 decoder layers, while the remainder of its hyperparameters remain identical
to the small model. All Transformer stacks use relative attention over positional encodings as per
(Raffel et al., 2020). For pre-training, we run our models for 1M steps on C4 with a batch size of
64. The maximum sequence length for all tasks is set to 1024. TPU packing is not activated for
Charformer. For Charformer, the ﬁlter size of the pre-GBST convolution is set to 5 by default. For
CHARFORMER, the downsampling rate is tuned in the range of {2, 3, 4}. For smaller models, the rate
of 2 seems to work consistently the best. For base models, the best models used a downsampling rate
of either 2 or 3. For the SBase models, the optimal downsampling rate was often 3."
HYPERPARAMETERS,0.6721854304635762,"Multilingual Datasets
Hyperparameters are kept constant between English and multilingual tasks
except for the following differences. For pre-training, we run our models for 1M steps with a
batch size of 64, except for CHARFORMERSBase which uses a batch size of 1024 and CHAR-
FORMERSBase,LongP T which usees a batch size of 2048. Models were pre-trained with a maximum
sequence length of 2048 and ﬁne-tuned with a maximum sequence length of 4096 for TyDiQA,
XQuAD, and MLQA, and 2048 for XNLI and PAWS-X. Byte-level T5Base was the only model to
be pre-trained with a maximum sequence length of 1024, as it was prohibitively slow, see Table 11.
Fine-tuning and inference for this model, however still used 4096 and 2048 input lengths identical to
other models. For all tasks, CHARFORMER models used a downsampling rate of 2, while Byte-level
T5+LASC models used a downsampling rate of 4 (Clark et al., 2021). The downsampling rate of 2
was picked by ablating the downsampling rate on the TyDiQA-GoldP validation set. CHARFORMER
models for XNLI and PAWS-X additionally did not back-propagate into the GBST layer during
ﬁne-tuning. Checkpoints were picked based on the dev set metrics, and then evaluated on test set.
Reported metrics represent the macro-average of all languages in the task."
HYPERPARAMETERS,0.6754966887417219,Published as a conference paper at ICLR 2022
ABLATION STUDY,0.6788079470198676,"8.3
ABLATION STUDY"
ABLATION STUDY,0.6821192052980133,"This section presents our ablation experiments for both English and multilingual tasks. We analyze the
impact of various hyper-parameters and modeling choices such as using offsets vs 1D convolutions.
Across experiments, we ﬁnd that pre-GBST convolutions are preferred to enumerating offset blocks,
as it results in similar (or better) quality but a more efﬁcient implementation. For English tasks, block
score calibration (BC) improves performance. We note that in the multilingual setting, block score
calibration has little effect. The impact of different downsampling rates varies across tasks and model
sizes. We also experimented with different convolution ﬁlter sizes in English and found that they
did not signiﬁcantly impact performance. Likewise, using a different character span corruption rate
during pre-training did not signiﬁcantly impact performance. Adding feed-forward layers to the
CHARFORMER module in similar fashion to a Transformer block was also not obviously helpful."
ABLATION STUDY,0.6854304635761589,Table 7: Ablation studies with CHARFORMERSmall on English tasks.
ABLATION STUDY,0.6887417218543046,"Ablation
ds
Size
SST-2
MNLImm
IMDb"
ABLATION STUDY,0.6920529801324503,"Offsets
2
S
89.11
79.50
90.49
Conv
2
S
89.11
79.65
90.63
Conv + BC
2
S
89.56
80.15
90.60
Conv + Offsets + BC
2
S
89.11
79.68
90.48"
ABLATION STUDY,0.695364238410596,"Conv
3
S
89.45
80.07
90.15
Conv
4
S
89.11
79.82
90.21"
ABLATION STUDY,0.6986754966887417,"Conv
2
B
90.60
82.92
91.46
Conv
3
B
91.40
82.74
91.46
Conv
4
B
91.40
82.67
92.33"
ABLATION STUDY,0.7019867549668874,Table 8: Effect of freezing the GBST layer for XNLI and PAWS-X.
ABLATION STUDY,0.7052980132450332,"Model
ds
Freeze GBST
XNLI (Zero)
XNLI (Translate)
PAWS-X (Zero)
PAWS-X (Translate)"
ABLATION STUDY,0.7086092715231788,"CHARFORMERSmall
2
No
44.5
62.7
27.9
37.5
CHARFORMERSmall
2
Yes
50.9
68.7
77.1
84.8
CHARFORMERSmall
3
No
47.9
67.9
29.5
36.8
CHARFORMERSmall
3
Yes
43.2
68.6
77.8
83.7
CHARFORMERSmall
4
No
47.5
47.5
30.9
36.9
CHARFORMERSmall
4
Yes
43.6
43.6
77.9
83.5"
COMPARING DOWNSAMPLING APPROACHES,0.7119205298013245,"8.4
COMPARING DOWNSAMPLING APPROACHES"
COMPARING DOWNSAMPLING APPROACHES,0.7152317880794702,"In Table 10, we compare GBST downsampling with LASC downsampling (Clark et al., 2021) on
TyDiQA-GoldP. For this experiment we use the same hyperparameters as in Section 3.3, except the
pre-training input length is 1024 instead of 2048. Note that this difference is negligible (0.1 F1) for
CHARFORMERBase, ds = 2 which also appears in Table 4. All hyperparameters are ﬁxed between
CHARFORMER and Byte-level T5+LASC. Following (Clark et al., 2021) we set ds = 4 for LASC,
and we compare CHARFORMER at the same downsampling rate. We additionally include ds = 2 and
ds = 3 for CHARFORMER for comparison. With the same hyperparameters and downsampling rate,
CHARFORMER outperforms Byte-level T5+LASC on TyDiQA-GoldP."
COMPARING DOWNSAMPLING APPROACHES,0.7185430463576159,Table 9: Effect of ds on TyDiQA-GoldP (in-language multi-task).
COMPARING DOWNSAMPLING APPROACHES,0.7218543046357616,"Model
ds
TyDiQA-GoldP F1"
COMPARING DOWNSAMPLING APPROACHES,0.7251655629139073,"CHARFORMERSmall
2
69.6
CHARFORMERSmall
3
68.1
CHARFORMERSmall
4
66.6
Byte-level T5+LASCSmall
4
64.9"
COMPARING DOWNSAMPLING APPROACHES,0.7284768211920529,"CHARFORMERBase
2
75.8
CHARFORMERBase
3
74.3
CHARFORMERBase
4
73.2
Byte-level T5+LASCBase
4
70.6"
COMPARING DOWNSAMPLING APPROACHES,0.7317880794701986,Published as a conference paper at ICLR 2022
LARGE-SCALE EXPERIMENTS,0.7350993377483444,"8.5
LARGE-SCALE EXPERIMENTS"
LARGE-SCALE EXPERIMENTS,0.7384105960264901,"In this section we report preliminary results for scaling Charformer using the same number of
parameters as mT5Large and ByT5Large (1.23B). We follow a model scaling conﬁguration identical
to ByT5 in these experiments, and use the same hyperparameter settings as our main multilingual
results."
LARGE-SCALE EXPERIMENTS,0.7417218543046358,"Table 10: Comparison on TyDiQA at 1.23B parameters. *Due to resource constraints, the Charformer
result below uses ∼100K less pretraining steps than ByT5 and mT5."
LARGE-SCALE EXPERIMENTS,0.7450331125827815,"Model
TyDiQA-GoldP F1 / EM"
LARGE-SCALE EXPERIMENTS,0.7483443708609272,"mT5Large
85.3 / 75.3
ByT5Large
87.7 / 79.2
CHARFORMER*
86.3 / 77.3"
LARGE-SCALE EXPERIMENTS,0.7516556291390728,"Results The CHARFORMER model under the same scaling as ByT5Large was able to outperform
mT5Large, a very strong baseline. Our preliminary results at this scale shows that CHARFORMER is
competitive with, but is 1.4 F1 behind ByT5Large. However, we point out two important notes. First,
the CHARFORMER result is undertrained compared to ByT5Large since 10% of the pretraining has
not ﬁnished. Second, the CHARFORMER model is also twice as fast as ByT5, as seen from Table 11."
MULTILINGUAL EXPERIMENTS,0.7549668874172185,"8.6
MULTILINGUAL EXPERIMENTS"
MULTILINGUAL EXPERIMENTS,0.7582781456953642,This section contains detailed results for our multilingual experiments.
MULTILINGUAL EXPERIMENTS,0.7615894039735099,"Table 11: Compute metrics of base models at longer (2K) input length on the mC4 pre-training
corpus, using a batch size of 64 on 16 TPU-v3 chips."
MULTILINGUAL EXPERIMENTS,0.7649006622516556,"Model
L
ds
|θ|
Speed (steps/s)
FLOPS"
MULTILINGUAL EXPERIMENTS,0.7682119205298014,"Byte-level T5Base
2048
1
200M
2.7
2.0 × 1013"
MULTILINGUAL EXPERIMENTS,0.7715231788079471,"Byte-level T5+LASCBase
2048
4
205M
11
5.5 × 1012"
MULTILINGUAL EXPERIMENTS,0.7748344370860927,"CHARFORMERBase
2048
2
203M
6.1
9.5 × 1012"
MULTILINGUAL EXPERIMENTS,0.7781456953642384,"CHARFORMERBase
2048
3
203M
10
6.5 × 1012"
MULTILINGUAL EXPERIMENTS,0.7814569536423841,"CHARFORMERSBase
2048
2
134M
6.1
9.2 × 1012"
MULTILINGUAL EXPERIMENTS,0.7847682119205298,Table 12: Per-language breakdown of in-language multi-task TyDiQA-GoldP results.
MULTILINGUAL EXPERIMENTS,0.7880794701986755,"Model
|θ|
ar
bn
en
ﬁ
id
ko
ru
sw
te
Avg."
MULTILINGUAL EXPERIMENTS,0.7913907284768212,"mBERTBase (Subword)
179M
-/-
-/-
-/-
-/-
-/-
-/-
-/-
-/-
-/-
77.6/68.0
mT5Base (Subword)
582M
84.2/71.8
80.0/69.0
76.6/65.2
80.1/69.3
85.5/75.0
70.3/61.6
77.5/64.4
83.6/74.9
88.2/78.0
80.8 / 70.0"
MULTILINGUAL EXPERIMENTS,0.7947019867549668,"Byte-level T5Base
200M
81.4/67.0
66.8/56.6
69.8/59.5
75.6/63.0
81.6/72.4
64.6/58.7
74.1/60.8
81.8/74.3
85.0/76.1
75.6/65.4
Byte-level T5+LASCBase
205M
78.1/62.3
61.1/50.4
66.7/55.2
72.5/60.4
79.9/68.3
51.5/43.5
70.4/58.7
74.7/67.5
80.2/71.2
70.6/59.7
CHARFORMERBase
203M
81.8/67.9
69.1/60.2
71.4/60.5
76.3/64.2
83.0/73.1
62.7/54.3
74.7/61.7
80.2/73.3
83.6/75.0
75.9/65.6"
MULTILINGUAL EXPERIMENTS,0.7980132450331126,"CHARFORMERSBase
134M
82.4/68.1
78.1/67.3
75.4/64.3
79.5/68.2
85.0/75.9
66.6/58.0
77.0/64.3
81.5/74.1
86.5/78.6
79.1/68.8
CHARFORMERSBase,LongP T
134M
85.7/74.5
78.7/67.3
76.8/65.9
81.9/70.6
86.7/79.1
69.4/61.6
79.2/67.1
83.7/75.2
88.8/80.6
81.2/71.3"
MULTILINGUAL EXPERIMENTS,0.8013245033112583,Table 13: Per-language breakdown of translate-train-all XQuAD results.
MULTILINGUAL EXPERIMENTS,0.804635761589404,"Model
|θ|
ar
de
el
en
es
hi
ru
th
tr
vi
zh
Avg."
MULTILINGUAL EXPERIMENTS,0.8079470198675497,"mT5Base (Subword)
582M
72.4/55.2
76.9/59.7
76.8/58.8
83.1/70.3
79.0/61.2
71.4/53.4
76.1/58.5
67.9/62.0
72.5/51.4
75.9/56.3
76.9/69.7
75.3/59.7"
MULTILINGUAL EXPERIMENTS,0.8112582781456954,"Byte-level T5Base
200M
64.8/47.9
74.3/58.3
69.2/51.8
81.5/70.4
77.2/60.4
67.0/51.5
72.3/55.5
48.3/41.9
69.6/51.7
73.3/54.4
57.3/53.3
68.6/54.3
Byte-level T5+LASCBase
205M
62.9/45.5
70.6/54.2
68.3/52.3
80.1/68.4
74.8/57.9
63.1/46.2
68.2/52.2
50.0/43.4
67.1/48.2
71.7/51.8
57.7/52.7
66.8/52.1
CHARFORMERBase
203M
65.7/49.8
74.2/58.0
71.1/53.1
82.2/70.5
77.8/61.0
67.0/51.3
73.4/57.6
54.3/48.0
70.3/53.0
74.6/55.6
62.0/56.6
70.2/55.9"
MULTILINGUAL EXPERIMENTS,0.8145695364238411,"CHARFORMERSBase
134M
70.3/53.7
78.6/61.4
74.4/55.1
85.1/73.7
79.8/63.6
69.1/52.7
76.7/61.3
57.6/51.2
73.9/55.8
76.8/57.6
67.4/62.4
73.6/59.0
CHARFORMERSBase,LongP T
134M
72.6/55.0
79.0/62.3
74.9/56.1
85.4/74.5
80.4/63.4
70.6/56.1
77.8/62.2
56.1/49.2
76.1/58.2
77.7/59.4
66.0/61.8
74.2/59.8"
MULTILINGUAL EXPERIMENTS,0.8178807947019867,Published as a conference paper at ICLR 2022
MULTILINGUAL EXPERIMENTS,0.8211920529801324,Table 14: Per-language breakdown of translate-train-all MLQA results.
MULTILINGUAL EXPERIMENTS,0.8245033112582781,"Model
|θ|
ar
de
en
es
hi
vi
zh
Avg."
MULTILINGUAL EXPERIMENTS,0.8278145695364238,"mT5Base (Subword)
582M
61.1/40.7
65.5/49.2
80.7/66.3
70.7/52.1
63.6/44.3
68.0/47.6
63.5/39.4
67.6/48.5"
MULTILINGUAL EXPERIMENTS,0.8311258278145696,"Byte-level T5Base
200M
52.6/34.2
60.5/46.1
77.7/64.8
67.1/49.2
52.9/36.5
63.6/43.8
58.3/36.4
61.8/44.4
Byte-level T5+LASCBase
205M
50.8/32.0
58.1/43.5
75.8/62.2
64.7/46.7
49.2/32.6
60.4/40.4
52.6/30.6
58.8/41.1
CHARFORMERBase
203M
53.5/34.5
61.3/46.8
78.5/65.4
67.2/49.3
54.5/37.6
64.3/43.9
58.8/36.6
62.6/44.9"
MULTILINGUAL EXPERIMENTS,0.8344370860927153,"CHARFORMERSBase
134M
58.3/39.1
65.7/50.5
81.8/68.7
71.0/53.1
57.7/40.8
67.3/46.8
62.7/40.8
66.3/48.5
CHARFORMERSBase,LongP T
134M
59.6/40.0
66.6/51.3
82.2/69.0
72.1/54.5
59.7/42.9
68.2/47.4
62.4/40.7
67.2/49.4"
MULTILINGUAL EXPERIMENTS,0.8377483443708609,Table 15: Per-language breakdown of translate-train-all and cross-lingual zero-shot XNLI results.
MULTILINGUAL EXPERIMENTS,0.8410596026490066,"Model
|θ|
ar
bg
de
el
en
es
fr
hi
ru
sw
th
tr
ur
vi
zh
Avg."
MULTILINGUAL EXPERIMENTS,0.8443708609271523,Translate-Train-All
MULTILINGUAL EXPERIMENTS,0.847682119205298,"mT5Base (Subword)
582M
74.4
78.5
77.7
78.1
82.0
79.1
77.9
72.2
76.5
71.5
75.0
74.8
70.4
74.5
76.0
75.9"
MULTILINGUAL EXPERIMENTS,0.8509933774834437,"Byte-level T5Base
200M
67.1
72.0
71.0
70.6
76.9
74.0
73.4
63.7
69.2
66.2
65.7
69.4
62.8
69.6
69.0
69.4
Byte-level T5+LASCBase
205M
65.6
72.1
70.5
67.9
75.6
73.4
72.2
63.5
68.6
65.4
64.5
67.4
62.4
68.3
61.0
67.9
CHARFORMERBase
203M
69.5
72.9
72.7
72.6
78.2
74.5
73.6
67.0
71.7
67.9
68.1
70.8
65.0
70.7
71.5
71.1"
MULTILINGUAL EXPERIMENTS,0.8543046357615894,"CHARFORMERSBase
134M
70.8
75.7
75.9
73.1
80.9
76.9
76.8
65.6
74.7
65.7
67.7
72.0
63.1
72.9
71.5
72.2
CHARFORMERSBase,LongP T
134M
71.1
75.9
73.6
74.2
80.8
76.6
76.8
69.2
72.2
68.2
71.0
71.2
65.7
72.9
73.0
72.8"
MULTILINGUAL EXPERIMENTS,0.8576158940397351,Cross-Lingual Zero-Shot
MULTILINGUAL EXPERIMENTS,0.8609271523178808,"mBERTBase (Subword)
179M
64.3
68.0
70.0
65.3
80.8
73.5
73.4
58.9
67.8
49.7
54.1
60.9
57.2
69.3
67.8
65.4
mT5Base (Subword)
582M
73.3
78.6
77.4
77.1
84.7
80.3
79.1
70.8
77.1
69.4
73.2
72.8
68.3
74.2
74.1
75.4"
MULTILINGUAL EXPERIMENTS,0.8642384105960265,"Byte-level T5Base
200M
56.7
61.2
63.0
60.9
79.2
70.1
65.3
43.9
61.0
45.5
43.5
52.0
44.3
58.3
55.6
57.4
Byte-level T5+LASCBase
205M
53.3
58.8
62.2
54.9
77.1
68.6
65.4
44.7
58.4
46.1
43.6
50.4
42.8
55.9
46.1
55.2
CHARFORMERBase
203M
55.7
61.1
64.8
60.1
77.3
69.9
67.9
44.4
60.2
45.3
47.9
54.0
43.5
59.1
53.4
57.6"
MULTILINGUAL EXPERIMENTS,0.8675496688741722,"CHARFORMERSBase
134M
66.4
71.0
72.7
68.6
82.4
77.1
75.4
57.6
70.6
48.7
61.4
61.8
54.1
68.9
62.8
66.6
CHARFORMERSBase,LongP T
134M
68.4
70.9
74.3
70.2
82.4
77.0
76.6
59.9
71.0
42.6
64.0
65.5
56.5
71.2
66.0
67.8"
MULTILINGUAL EXPERIMENTS,0.8708609271523179,Table 16: Per-language breakdown of translate-train-all and cross-lingual zero-shot PAWS-X results.
MULTILINGUAL EXPERIMENTS,0.8741721854304636,"Model
|θ|
de
en
es
fr
ja
ko
zh
Avg."
MULTILINGUAL EXPERIMENTS,0.8774834437086093,Translate-Train-All
MULTILINGUAL EXPERIMENTS,0.8807947019867549,"mT5Base (Subword)
582M
90.9
95.5
91.4
92.5
83.6
84.8
86.4
89.3"
MULTILINGUAL EXPERIMENTS,0.8841059602649006,"Byte-level T5Base
200M
89.3
94.6
90.1
90.3
81.4
81.1
82.3
87.0
Byte-level T5+LASCBase
205M
87.3
93.1
89.2
89.2
81.0
72.9
80.8
84.8
CHARFORMERBase
203M
89.9
94.6
89.8
91.4
82.7
78.4
83.3
87.2"
MULTILINGUAL EXPERIMENTS,0.8874172185430463,"CHARFORMERSBase
134M
89.9
95.9
91.8
92.2
83.9
78.9
84.4
88.2
CHARFORMERSBase,LongP T
134M
90.7
95.1
92.2
92.2
84.1
81.6
84.6
88.6"
MULTILINGUAL EXPERIMENTS,0.890728476821192,Cross-Lingual Zero-Shot
MULTILINGUAL EXPERIMENTS,0.8940397350993378,"mBERTBase (Subword)
179M
85.7
94.0
87.4
87.0
73.0
69.6
77.0
81.9
mT5Base (Subword)
582M
89.4
95.4
89.6
91.2
79.8
78.5
81.1
86.4"
MULTILINGUAL EXPERIMENTS,0.8973509933774835,"Byte-level T5Base
200M
84.7
93.8
85.8
86.4
72.2
67.9
75.2
80.9
Byte-level T5+LASCBase
205M
83.2
93.2
84.1
85.0
67.9
66.4
73.4
79.0
CHARFORMERBase
203M
86.1
94.8
87.2
88.0
70.1
69.7
75.5
81.6"
MULTILINGUAL EXPERIMENTS,0.9006622516556292,"CHARFORMERSBase
134M
89.6
95.2
90.7
90.7
77.1
74.4
78.9
85.2
CHARFORMERSBase,LongP T
134M
89.8
95.3
88.7
89.7
74.5
68.9
78.9
83.7"
EXAMPLE IMPLEMENTATION,0.9039735099337748,"8.7
EXAMPLE IMPLEMENTATION"
EXAMPLE IMPLEMENTATION,0.9072847682119205,"For additional clarity, we include a simpliﬁed implementation of the GBST module in Tensorﬂow
below. Default hyper-parameters here match those used in the paper."
EXAMPLE IMPLEMENTATION,0.9105960264900662,from typing import Optional
EXAMPLE IMPLEMENTATION,0.9139072847682119,import tensorflow as tf
EXAMPLE IMPLEMENTATION,0.9172185430463576,keras_layers = tf.keras.layers
EXAMPLE IMPLEMENTATION,0.9205298013245033,class GBSTLayer(keras_layers.Layer):
EXAMPLE IMPLEMENTATION,0.9238410596026491,"""""""Performs Charformer GBST on a sequence."
EXAMPLE IMPLEMENTATION,0.9271523178807947,Attributes:
EXAMPLE IMPLEMENTATION,0.9304635761589404,"input_shape: Shape [len, embedding_size] of input tensor in future calls,"
EXAMPLE IMPLEMENTATION,0.9337748344370861,"without batch dimension.
downsample_rate: Integer of how much to downsample by."
EXAMPLE IMPLEMENTATION,0.9370860927152318,Published as a conference paper at ICLR 2022
EXAMPLE IMPLEMENTATION,0.9403973509933775,"max_subword_block_width: Integer of max block size to use for enumeration.
block_attention: Hhether to use block score calibration.
block_scoring_network: module for parameterized block scoring.
conv_kernel_size: Integer of the size of the pre-GBST convolution kernel.
"""""""
EXAMPLE IMPLEMENTATION,0.9437086092715232,"def __init__(self,
input_shape: tf.Tensor,
downsample_rate: int = 2,
max_subword_block_width: int = 4,
block_attention: bool = False,
conv_kernel_size: Optional[int] = 5):
super(GBSTLayer, self).__init__()
self.downsample_rate = downsample_rate
self.max_subword_block_width = max_subword_block_width
self.conv_kernel_size = conv_kernel_size
self.conv_layer = keras_layers.Conv1D(
input_shape[-1], self.conv_kernel_size, input_shape=input_shape)
self.block_attention = block_attention
self.block_scoring_network = keras_layers.Dense(1, use_bias=False)"
EXAMPLE IMPLEMENTATION,0.9470198675496688,"def call(self, inputs):"
EXAMPLE IMPLEMENTATION,0.9503311258278145,"""""""Performs downsampling on the character-scale input representation. Args:"
EXAMPLE IMPLEMENTATION,0.9536423841059603,"inputs: float Tensor of shape [batch_size, seq_length,"
EXAMPLE IMPLEMENTATION,0.956953642384106,embedding_size].
EXAMPLE IMPLEMENTATION,0.9602649006622517,Returns:
EXAMPLE IMPLEMENTATION,0.9635761589403974,"<float>[batch_size, seq_length / downsample_rate , embedding_size]."
EXAMPLE IMPLEMENTATION,0.9668874172185431,"Downsampled sequences.
""""""
length = inputs.shape[1]"
EXAMPLE IMPLEMENTATION,0.9701986754966887,"if self.conv_kernel_size:
inputs = self.conv_layer(inputs)"
EXAMPLE IMPLEMENTATION,0.9735099337748344,"all_block_scores = []
all_sequences = []
for subword_len in range(1, self.max_subword_block_width):
padded_input = inputs
# Pad the sequence length if needed.
if length % subword_len != 0:
pad_amt = subword_len - int(length % subword_len)
padding = tf.constant([[0, 0], [0, pad_amt], [0, 0]])
padded_input = tf.pad(inputs, padding)"
EXAMPLE IMPLEMENTATION,0.9768211920529801,"# For this block size, form candidate block embeddings and scores.
# candidates shape: [batch, seq_len/subword_len, dim]
# block_scores shape: [batch, seq_len/subword_len, 1]
candidates = tf.nn.avg_pool(
padded_input, [subword_len], strides=[subword_len], padding=""VALID"")
block_scores = self.block_scoring_network(candidates)"
EXAMPLE IMPLEMENTATION,0.9801324503311258,"# Upsample it back to the original sequence length.
retiled_seq = tf.repeat(candidates, subword_len, axis=1)
retiled_block_scores = tf.repeat(block_scores, subword_len, axis=1)"
EXAMPLE IMPLEMENTATION,0.9834437086092715,"# Repad the upsampled sequence if needed.
if retiled_block_scores.shape[1] < length:
repad_amt = length - retiled_block_scores.shape[1]
repadding = tf.constant([[0, 0], [0, repad_amt], [0, 0]])
retiled_seq = tf.pad(retiled_seq, repadding)
retiled_block_scores = tf.pad(retiled_block_scores, repadding)"
EXAMPLE IMPLEMENTATION,0.9867549668874173,"# Make sure everything is the right length and add new dimension to concat
# candidate blocks on.
retiled_block_scores = retiled_block_scores[:, :length, :, None]
retiled_seq = retiled_seq[:, :length, :, None]
all_block_scores.append(retiled_block_scores)
all_sequences.append(retiled_seq)"
EXAMPLE IMPLEMENTATION,0.9900662251655629,"block_scores = tf.concat(all_block_scores, axis=-1)
block_scores = tf.nn.softmax(block_scores, axis=-1)
candidates = tf.concat(all_sequences, axis=-1)"
EXAMPLE IMPLEMENTATION,0.9933774834437086,"# TODO: Block score calibration / block-by-block attention is omitted in this implementation.
# batch_size x num_candidates x length x dim
candidates = candidates * block_scores
output = tf.reduce_sum(candidates, axis=-1) # bsz x length x dim"
EXAMPLE IMPLEMENTATION,0.9966887417218543,"# Downsample by mean pooling.
if self.downsample_rate > 1:
output = tf.nn.avg_pool(
output, (self.downsample_rate,),
strides=(self.downsample_rate,),
padding=""VALID"")
return output"
