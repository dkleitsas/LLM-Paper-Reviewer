Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018587360594795538,"Large language models (LMs) such as GPT-3 have the surprising ability to do
in-context learning, where the model learns to do a downstream task simply by
conditioning on a prompt consisting of input-output examples. The LM learns
from these examples without being explicitly pretrained to learn. Thus, it is unclear
what enables in-context learning. In this paper, we study how in-context learning
can emerge when pretraining documents have long-range coherence. Here, the
LM must infer a latent document-level concept to generate coherent next tokens
during pretraining. At test time, in-context learning occurs when the LM also
infers a shared latent concept between examples in a prompt. We prove when this
occurs despite a distribution mismatch between prompts and pretraining data in
a setting where the pretraining distribution is a mixture of HMMs. In contrast to
messy large-scale datasets used to train LMs capable of in-context learning, we
generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs
both exhibit in-context learning1. Beyond the theory, experiments on GINC exhibit
large-scale real-world phenomena including improved in-context performance with
model scaling (despite the same pretraining loss), sensitivity to example order, and
instances where zero-shot is better than few-shot in-context learning."
INTRODUCTION,0.0037174721189591076,"1
INTRODUCTION"
INTRODUCTION,0.0055762081784386614,"Large language models (LMs) such as GPT-3 (Brown et al., 2020; Lieber et al., 2021; Wang &
Komatsuzaki, 2021; Radford et al., 2019) are pretrained on massive text corpora to predict the next
word given previous words. They demonstrate the surprising ability to do in-context learning, where an
LM “learns” to do a task simply by conditioning on a prompt containing input-output pairs, achieving
SOTA results on LAMBADA (Paperno et al., 2016) and TriviaQA (Joshi et al., 2017) tasks (18%
and 3% over previous SOTA (Brown et al., 2020)). For example, consider the task of predicting
nationalities from names. A prompt (Figure 1) is constructed by concatenating independent “training”
examples (e.g., “Albert Einstein was German”) followed by a “test example” (“Marie Curie was”).
Conditioning on this prompt, GPT-3 places the largest probability on the correct output"
INTRODUCTION,0.007434944237918215,p(“Polish”|“Albert Einstein was German \n Mahatma Gandhi was Indian \n Marie Curie was”)
INTRODUCTION,0.00929368029739777,"by inferring the task from examples. Intruigingly, GPT-3 was not explicitly pretrained to learn from
examples, and the distribution of prompts (which concatenate independent examples) is quite different
from natural language. Our understanding of in-context learning is limited since (i) real pretraining
data is messy and (ii) in-context learning has so far required large-scale datasets and models."
INTRODUCTION,0.011152416356877323,"In this paper, we introduce a simple pretraining distribution where in-context learning emerges.
To generate a document, we ﬁrst draw a latent concept θ, which parameterizes the transitions of a
Hidden Markov Model (HMM) (Baum & Petrie, 1966), then sample a sequence of tokens from the
HMM (Figure 9). This latent variable structure is common in topic models such as LDA (Blei et al.,
2003; Gruber et al., 2007). During pretraining, the LM must infer the latent concept across multiple
sentences to generate coherent continuations. When conditioning on a prompt, in-context learning
occurs when the LM also infers a shared prompt concept across examples to make a prediction. We
assume the LM ﬁts the pretraining distribution p exactly with enough data and expressivity, so that
the question of in-context learning becomes characterizing the conditional distribution of completions"
INTRODUCTION,0.013011152416356878,"1The code, data, and experiments are located on GitHub and CodaLab."
INTRODUCTION,0.01486988847583643,Published as a conference paper at ICLR 2022
INTRODUCTION,0.016728624535315983,"Figure 1: In-context learning can emerge from modeling long-range coherence in the pretraining data.
During pretraining, the language model (LM) implicitly learns to infer a latent concept (e.g., wiki
bios, which typically transition between name (Albert Einstein) →nationality (German) →occupation
(physicist) →...) shared across sentences in a document. Although prompts are unnatural sequences
that concatenate independent examples, in-context learning occurs if the LM can still infer the shared
concept across examples to do the task (name →nationality, which is part of wiki bios)."
INTRODUCTION,0.01858736059479554,"given prompts p(output|prompt) under the pretraining distribution, where the prompt is generated
from a different distribution pprompt. This conditional distribution, which is the posterior predictive
distribution, marginalizes out the latent concepts:"
INTRODUCTION,0.020446096654275093,"p(output|prompt)=
Z"
INTRODUCTION,0.022304832713754646,"concept
p(output|concept,prompt)p(concept|prompt)d(concept).
(1)"
INTRODUCTION,0.024163568773234202,"If p(concept|prompt) concentrates on the prompt concept with more examples, then the LM learns
via marginalization by “selecting” the prompt concept. Thus, in-context learning can be viewed as
the LM implicitly performing Bayesian inference."
INTRODUCTION,0.026022304832713755,"The main challenge is that prompts are sampled from a different distribution than the pretraining
distribution. The canonical Bayesian asymptotic tool is the Bernstein-von Mises theorem (van der
Vaart,1998;Kleijn&vanderVaart,2012;Gunst&Shcherbakova,2008), whichasserts(underregularity
conditions) that the posterior distribution of a latent variable concentrates on the maximum likelihood
estimate. However, Bernstein-von Mises typically assumes observations are independent and/or drawn
from the same distribution as the model, both of which are not satisﬁed. We prove that despite the
distribution mismatch, the asymptotic prediction error of in-context learning is optimal when the signal
about the latent concept in each prompt example is larger than the error due to the distribution mismatch.
Additionally, we prove that the in-context learning error decreases with the length of each example—
thus, information in the inputs, not just the input-output mapping, can be useful for in-context learning."
INTRODUCTION,0.027881040892193308,"As a companion to this theory, we created the Generative IN-Context learning dataset (GINC), which
is a small-scale synthetic dataset for studying in-context learning. We ﬁnd that both Transform-
ers (Vaswani et al., 2017) and LSTMs (Hochreiter & Schmidhuber, 1997) trained on GINC exhibit in-
context learning. We verify intuitions from the theory, showing that the accuracy of in-context learning
improves with the number of examples and example length. Ablations of the GINC dataset show that the
latent concept structure in the pretraining distribution is crucial to the emergence of in-context learning."
INTRODUCTION,0.02973977695167286,"The experiments also bring up open questions which go beyond our theory, which only studies the
pretraining distribution. We ﬁnd that scaling up the number of model parameters steadily improves
the in-context accuracy despite achieving the same pretraining loss, showing that larger models may
improve in-context learning beyond increasing the capacity for memorizing the training data better.
Previously observed in-context learning phenomena such as sensitivity to example ordering (Zhao
et al., 2021) and the existence of settings where zero-shot is better than one/few-shot learning (Brown
et al., 2020) are also mirrored in GINC."
IN-CONTEXT LEARNING SETTING,0.031598513011152414,"2
IN-CONTEXT LEARNING SETTING"
IN-CONTEXT LEARNING SETTING,0.03345724907063197,"Pretraining distribution.
In our framework, a latent concept θ from a family of concepts Θ deﬁnes
a distribution over observed tokens o from a vocabulary O. To generate a document, we ﬁrst sample a"
IN-CONTEXT LEARNING SETTING,0.03531598513011153,Published as a conference paper at ICLR 2022
IN-CONTEXT LEARNING SETTING,0.03717472118959108,"concept from a prior p(θ) and then sample the document given the concept. Each pretraining document
is a length T sequence:"
IN-CONTEXT LEARNING SETTING,0.03903345724907063,"p(o1,...,oT )=
Z"
IN-CONTEXT LEARNING SETTING,0.040892193308550186,"θ∈Θ
p(o1,...,oT |θ)p(θ)dθ.
(2)"
IN-CONTEXT LEARNING SETTING,0.04275092936802974,"We assume p(o1,...,oT |θ) is deﬁned by a Hidden Markov Model (HMM). The concept θ determines
the transition probability matrix of the HMM hidden states h1,...,hT from a hidden state set H."
IN-CONTEXT LEARNING SETTING,0.04460966542750929,"Prompt distribution.
The prompt distribution pprompt generates prompts for in-context learning.
A prompt is a concatenation of n independent training examples and 1 test input xtest, which are all
conditioned on a shared prompt concept θ∗. The goal is to predict the test output ytest by predicting
the next token conditioned on the prompt."
IN-CONTEXT LEARNING SETTING,0.046468401486988845,"A prompt example is composed of an input token sequence x (e.g., Albert Einstein was) followed
by an output token y (e.g., German). In particular, the i-th training example Oi consists of an input
xi =Oi[1: k−1] (the ﬁrst k−1 tokens) followed by an output token yi =Oi[k] at the end2. The i-th
training example is independently generated as follows:"
"GENERATE A START HIDDEN STATE HSTART
I",0.048327137546468404,"1. Generate a start hidden state hstart
i
from a prompt start distribution pprompt.
2. Given hstart
i
, generate the example sequence Oi = [xi,yi] from p(Oi|hstart
i
,θ∗), the pretraining
distribution conditioned on a prompt concept θ∗."
"GENERATE A START HIDDEN STATE HSTART
I",0.05018587360594796,"Thetestinputxtest =xn+1 issampledsimilarly. Betweeneachexample, thereisaspecialdelimitertoken
odelim. The prompt consists of a sequence of training examples (Sn) followed by the test example xtest:"
"GENERATE A START HIDDEN STATE HSTART
I",0.05204460966542751,"[Sn,xtest]=[x1,y1,odelim,x2,y2,odelim,...,xn,yn,odelim,xtest]∼pprompt.
(3)"
"GENERATE A START HIDDEN STATE HSTART
I",0.05390334572490706,"Mismatch between prompt and pretraining distributions.
Since transitions between independent
examples can be unnatural, the prompts are low probability sequences under the pretraining distribution.
We provide a simple illustration using the names to nationalities example. Suppose that wiki bio
documents in the pretraining data typically transition between name →nationality →occupation
→.... In the prompt, the examples transition between name →nationality →name →nationality →...,
which contains low-probability transitions such as “German” →“Mahatma Gandhi”. The prompt
formatting (e.g., choice of delimiter) can also be a source of mismatch. We aim to show that despite
this mismatch, large LMs can infer the prompt concept from examples."
"GENERATE A START HIDDEN STATE HSTART
I",0.055762081784386616,"In-context predictor and task.
For in-context learning, the output target y for each example x is
sampled according to pprompt(y|x):"
"GENERATE A START HIDDEN STATE HSTART
I",0.05762081784386617,"ytest ∼pprompt(y|xtest)=Ehstart
test ∼pprompt(hstart
test |xtest)

p(y|xtest,hstart
test ,θ∗)

.
(4)"
"GENERATE A START HIDDEN STATE HSTART
I",0.05947955390334572,"where hstart
test denotes the hidden state corresponding to the ﬁrst token of xtest. We analyze the in-context
predictor fn(xtest) = argmaxyp(y|Sn,xtest), which outputs the most likely prediction over the pre-
training distribution conditioned on the prompt from the prompt distribution3. We study the in-context
predictor and its expected 0-1 error with n examples L0-1(fn)=Extest,ytest∼pprompt[1[fn(xtest)̸=ytest]]."
ASSUMPTIONS,0.06133828996282528,"2.1
ASSUMPTIONS
We detail the assumptions in our framework, including the structure of delimiters and regularity
assumptions. We ﬁrst assume that there exists a subset of delimiter hidden states D which generates
the special delimiter token odelim deterministically."
ASSUMPTIONS,0.06319702602230483,"Assumption 1 (Delimiter hidden states). Let the delimiter hidden states D be a subset of H. For any
hdelim ∈D and θ∈Θ, p(odelim|hdelim,θ)=1 and for any h /∈D, p(odelim|h,θ)=0."
ASSUMPTIONS,0.06505576208178439,"Thus, observing the delimiter odelim reveals that the corresponding hidden state is in D, but does not
reveal which element of D it is. The delimiter is usually a token that can appear in a broad range of
contexts (e.g., newline). The delimiter ideally does not distract from the examples — for example,
an adversarial delimiter could look like part of the input x. To mitigate these scenarios, we assume
that no delimiter (e.g., newline) is signiﬁcantly more likely under one concept rather than another."
ASSUMPTIONS,0.06691449814126393,"2The example length k is ﬁxed for simplicity — we leave extending our analysis to variable k as future work.
3In practice, greedy decoding or nucleus sampling (Holtzman et al., 2020) are used for likely completions."
ASSUMPTIONS,0.0687732342007435,Published as a conference paper at ICLR 2022
ASSUMPTIONS,0.07063197026022305,"Assumption 2 (Bound on delimiter transitions). For any delimiter state hdelim ∈D and any hidden
state h ∈H, the probability of transitioning to a delimiter hidden state under θ is upper bounded
p(hdelim|h,θ)<c2 for any θ∈Θ\{θ∗}, and is lower bounded p(hdelim|h,θ∗)>c1 >0 for θ∗. Addition-
ally, the start hidden state distribution for delimiter hidden states is bounded as p(hdelim|θ)∈[c3,c4]."
ASSUMPTIONS,0.0724907063197026,"The prompt start distribution is a source of distribution shift that is separate from the shift from
concatenating independent examples. We make an assumption that limits how much distribution shift
is introduced by the prompt start distribution."
ASSUMPTIONS,0.07434944237918216,"Assumption 3 (Distribution shift from prompt start distribution). We assume that the prompt
start distribution pprompt is close in TV distance to all hidden transition distributions (under θ∗)
starting from a delimiter hidden state: maxhdelim∈D TV (pprompt(h)∥p(h|hdelim,θ∗)) < ∆/4. Here,
∆= pprompt(ymax|xtest) −maxy̸=ymax pprompt(y|xtest) is the margin between the most likely label
ymax =argmaxypprompt(y|xtest) and the second most likely label."
ASSUMPTIONS,0.0762081784386617,"Even if the maximum TV distance is 0, there is still distribution shift from concatenating independent
examples. We also assume the prompt concept θ∗is in the family Θ, a broad set of concepts."
ASSUMPTIONS,0.07806691449814127,Assumption 4 (Well-speciﬁcation). The prompt concept θ∗is in Θ.
ASSUMPTIONS,0.07992565055762081,"Even though the pretraining distribution is broad, the prompt is still low probability under the
pretraining distribution since it concatenates independent examples. Finally, if the prompt has zero
probability under the prompt concept θ∗, then Bayesian inference will not be able to infer the prompt
concept as in Section 3.1. The following are regularity assumptions which mainly ensure that the
prompt is not zero probability under θ∗."
ASSUMPTIONS,0.08178438661710037,"Assumption 5 (Regularity). The pretraining distribution p satisﬁes: 1) Lower bound on transition
probability for the prompt concept θ∗: for any pair of hidden states h,h′ ∈H, p(h|h′,θ∗) > c5 > 0.
2) Start hidden state is lower bounded: for any h∈H, p(h|θ∗)≥c8 >0. 3) All tokens can be emitted:
for every symbol o, there is some hidden state h ∈H such that p(o|h,θ∗) >c6 > 0, 4) The prior p(θ)
has support over the entire concept family Θ and is bounded above everywhere."
THEORETICAL ANALYSIS,0.08364312267657993,"3
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.08550185873605948,"We prove that in the limit of inﬁnite examples, the error of the in-context predictor is optimal if a
distinguishability condition holds — the prompt concept θ∗is distinct enough from the other concepts in
Θ (e.g., when Θ is a discrete set). When distinguishability does not hold (e.g, Θ is continuous-valued),
we show that the expected error still decreases with the length of each example, showing that
information in both the inputs and the input-output mapping contribute to in-context learning."
HIGH-LEVEL APPROACH,0.08736059479553904,"3.1
HIGH-LEVEL APPROACH
Our goal is to show that argmaxyp(y|Sn,xtest)→argmaxypprompt(y|xtest) as the number of examples
n grows. In the following, assume that the prompt has non-zero probability under the pretraining
distribution p given θ∗, meaning that p(Sn,xtest|θ∗)>0. We expand p(y|Sn,xtest) to analyze its limit:"
HIGH-LEVEL APPROACH,0.08921933085501858,"p(y|Sn,xtest)=
Z"
HIGH-LEVEL APPROACH,0.09107806691449814,"θ
p(y|Sn,xtest,θ)p(θ|Sn,xtest)dθ ∝
Z"
HIGH-LEVEL APPROACH,0.09293680297397769,"θ
p(y|Sn,xtest,θ)p(Sn,xtest|θ)p(θ)dθ
(Bayes’ rule, drop the constant
1
p(Sn,xtest)) =
Z θ X"
HIGH-LEVEL APPROACH,0.09479553903345725,"hstart
test ∈H
p(y|xtest,hstart
test ,θ)p(hstart
test |Sn,xtest,θ) p(Sn,xtest|θ)"
HIGH-LEVEL APPROACH,0.09665427509293681,"p(Sn,xtest|θ∗)p(θ)dθ
(5)"
HIGH-LEVEL APPROACH,0.09851301115241635,"(Law of total prob, Markov property, divide by p(Sn,xtest|θ∗) (a constant)) =
Z θ X"
HIGH-LEVEL APPROACH,0.10037174721189591,"hstart
test ∈H
p(y|xtest,hstart
test ,θ)p(hstart
test |Sn,xtest,θ)exp(n·rn(θ))p(θ)dθ
(6)"
HIGH-LEVEL APPROACH,0.10223048327137546,where rn(θ) = 1
HIGH-LEVEL APPROACH,0.10408921933085502,"n log p(Sn,xtest|θ)"
HIGH-LEVEL APPROACH,0.10594795539033457,"p(Sn,xtest|θ∗). In Theorem 1, we prove that under a distinguishability condition,
exp(n·rn(θ)) →0 for all concepts θ except the prompt concept θ∗, where exp(n·rn(θ∗)) = 1. The
only nonzero term in the integral is when θ = θ∗, and thus the prompt concept is “selected” as a
consequence of Bayesian inference4. Lemma 1 shows that the argmax after restricting to θ∗is the"
HIGH-LEVEL APPROACH,0.10780669144981413,4We can exchange limits and integrals since the probabilities are bounded (dominated convergence).
HIGH-LEVEL APPROACH,0.10966542750929369,Published as a conference paper at ICLR 2022
HIGH-LEVEL APPROACH,0.11152416356877323,"same as the most likely label under pprompt(y|xtest) (using Assumption 3). Putting these together with
Equation 6, the in-context predictor infers the prompt concept θ∗:"
HIGH-LEVEL APPROACH,0.11338289962825279,"argmax
y
p(y|Sn,xtest)→argmax
y
pprompt(y|xtest)
(7)"
HIGH-LEVEL APPROACH,0.11524163568773234,"Thus, the in-context predictor is optimal as the number of in-context examples increases."
HEURISTIC DERIVATION,0.1171003717472119,"3.2
HEURISTIC DERIVATION
Recall from Section 3.1 that if exp(n·rn(θ))→0 for all θ̸=θ∗, then Bayesian inference “selects” the
prompt concept through marginalization. To do this, we focus on showing that rn(θ), the average
log-likelihood ratio between θ and θ∗, converges to a negative constant, and thus nrn goes to −∞."
HEURISTIC DERIVATION,0.11895910780669144,"The main technical challenge is to handle the sequence-of-examples structure of the prompt, which
makes all the examples dependent with respect to the pretraining distribution. Our approach uses
properties of delimiter tokens to approximately factorize the examples, with constant error per example.
We let Oex
i =[odelim
i−1 ,Oi] be the i-th input-output pair and the previous delimiter together for i>1 and
deﬁne Oex
1 =O1. Expanding the likelihood term inside rn(θ), our goal is to show"
HEURISTIC DERIVATION,0.120817843866171,"p(Sn,xtest|θ)=p(xtest|Sn,θ)p(Sn|θ)≈ n
Y"
HEURISTIC DERIVATION,0.12267657992565056,"i=1
O(1)p(Oi|θ)
(8)"
HEURISTIC DERIVATION,0.12453531598513011,"To show this, we expand p(Sn|θ) with the chain rule, and with Assumption 5 (to bound p(xtest|Sn,θ)
by O(1)) it can be shown that"
HEURISTIC DERIVATION,0.12639405204460966,"p(xtest|Sn,θ)p(Sn|θ)≈ n
Y"
HEURISTIC DERIVATION,0.12825278810408922,"i=1
O(1)p(Oex
i |Oex
1:i−1,θ).
(9)"
HEURISTIC DERIVATION,0.13011152416356878,"We then marginalize p(Oex
i |Oex
1:i−1,θ) over the hidden state hdelim
i−1 corresponding to the delimiter in
Oex
i =[odelim
i−1 ,Oi]: n
Y"
HEURISTIC DERIVATION,0.13197026022304834,"i=1
O(1)p(Oex
i |Oex
1:i−1,θ)= n
Y"
HEURISTIC DERIVATION,0.13382899628252787,"i=1
O(1)
X"
HEURISTIC DERIVATION,0.13568773234200743,"hdelim
i−1∈D
p(Oi|hdelim
i−1 ,θ)p(hdelim
i−1 |Oex
1:i−1,θ)≈ n
Y"
HEURISTIC DERIVATION,0.137546468401487,"i=1
O(1)p(Oi|θ) (10)"
HEURISTIC DERIVATION,0.13940520446096655,"While summing over H above would be a trivial equality, we can replace H with the set of delimiter
hidden states D since p(h|Oex
1:i−1,θ)=0 for non-delimiter hidden states h /∈D (Assumption 1). We
used in the ﬁrst equality that Oex
1:i−1 →hdelim
i−1 →Oex
i forms a Markov chain and p(odelim
i−1 |hdelim
i−1 ) = 1
(Assumption 1) to change Oex
i to Oi. Finally, we can show using properties of delimiter hidden states
(Assumption 2) that p(hdelim
i−1 |Oex
1:i−1,θ) = O(1) and P"
HEURISTIC DERIVATION,0.1412639405204461,"hdelim
i−1∈Dp(Oi|hdelim
i−1 ,θ) ≈O(1)p(Oi|θ) in the
second step. Therefore, we can upper bound rn(θ) as"
HEURISTIC DERIVATION,0.14312267657992564,"rn(θ)≤1 n  O(n)+ n
X"
HEURISTIC DERIVATION,0.1449814126394052,"i=1
log p(Oi|θ)"
HEURISTIC DERIVATION,0.14684014869888476,p(Oi|θ∗) !
HEURISTIC DERIVATION,0.14869888475836432,→O(1)+EO∼pprompt
HEURISTIC DERIVATION,0.15055762081784388,"
log p(O|θ)"
HEURISTIC DERIVATION,0.1524163568773234,p(O|θ∗)
HEURISTIC DERIVATION,0.15427509293680297,"
.
(11)"
HEURISTIC DERIVATION,0.15613382899628253,"The
expectation
term
can
be
written
as
the
difference
of
two
KL
divergences,
KL(pprompt(O)∥p(O|θ∗)) −KL(pprompt(O)∥p(O|θ)).
We bound the ﬁrst KL term by a con-
stant using Assumption 5 — intuitively for one example, pprompt and p(·|θ∗) are close. We break the
second term into a sum of negative KL divergences over k tokens. There are O(k) KL terms and
only O(1) other error terms, which come from the distribution mismatch between the prompt and
pretraining distributions. If the KL terms are larger than the error terms, then rn(θ) has a negative limit.
If this holds for all θ̸=θ∗, then we have exp(n·rn(θ))→0 for all θ̸=θ∗, enabling in-context learning."
FORMAL RESULTS,0.1579925650557621,"3.3
FORMAL RESULTS
3.3.1
IN-CONTEXT LEARNING UNDER DISTINGUISHABILITY
We deﬁne a distinguishability condition which formalizes when in-context learning occurs. Letting
pj
θ(o) := p(O[j] = o|O[1 : j −1],θ) be the output distribution of the j-th token given the previous
tokens and pj
prompt(o):=pprompt(O[j]=o|O[1:j−1]) be the analogous distribution under the prompt"
FORMAL RESULTS,0.15985130111524162,Published as a conference paper at ICLR 2022
FORMAL RESULTS,0.16171003717472118,"Figure 2: When the signal about the prompt concept within each example (green) is greater than the
error from low-probability transitions between examples, in-context learning succeeds in our latent
concept setting (Theorem 1). Increasing the example length k increases the signal. The signal for
in-context learning comes from tokens in both the inputs and the input-output mapping."
FORMAL RESULTS,0.16356877323420074,"distribution, the distinguishability condition depends on the KL divergence between pj
prompt (which
represents θ∗) and pj
θ as well as error terms ϵθ
start and ϵθ
delim coming from the distribution mismatch
between the prompt and pretraining distributions at the start and delimiter token for each example:"
FORMAL RESULTS,0.1654275092936803,"KLj(θ∗∥θ):=EO[1: j−1]∼pprompt[KL(pj
prompt∥pj
θ)]
(12)"
FORMAL RESULTS,0.16728624535315986,"ϵθ
delim :=2(log(c2)−log(c1))+log(c4)−log(c3),
ϵθ
start :=log(1/c8).
(13)"
FORMAL RESULTS,0.1691449814126394,"Condition 1 (Distinguishability). We deﬁne θ∗to be distinguishable if for all θ∈Θ,θ̸=θ∗, k
X"
FORMAL RESULTS,0.17100371747211895,"j=2
KLj(θ∗∥θ)>ϵθ
start+ϵθ
delim.
(14)"
FORMAL RESULTS,0.17286245353159851,"When the signal from KL divergence (LHS) is larger than the error terms, Equation 14 is satisﬁed
(Figure 2). For larger example lengths k, the LHS increases, improving distinguishability. Intuitively,
larger example lengths increase the proportion of the prompt sampled from the pretraining distribution
by providing more evidence for Bayesian inference. Under Condition 1, the in-context predictor
asymptotically achieves the optimal expected error."
FORMAL RESULTS,0.17472118959107807,"Theorem 1. Assume the assumptions in Section 2.1 hold. If Condition 1 holds, then as n →∞the
prediction according to the pretraining distribution is
argmax
y
p(y|Sn,xtest)→argmax
y
pprompt(y|xtest).
(15)"
FORMAL RESULTS,0.17657992565055763,"Thus, the in-context predictor fn achieves the optimal 0-1 risk: limn→∞L0-1(fn)=inff L0-1(f)."
NON-DISTINGUISHABLE CASE,0.17843866171003717,"3.3.2
NON-DISTINGUISHABLE CASE
The distinguishability condition (Condition 1) fails when there is some θ ̸= θ∗for which the KL
divergence between θ and θ∗is less than the error terms. However, this also means that the output
distributions of θ and θ∗are close in KL. We leverage this to prove that the expected 0-1 error decreases
with the example length k under two different settings where distinguishability does not hold."
NON-DISTINGUISHABLE CASE,0.18029739776951673,"Continuity.
Our ﬁrst result relies on a continuity assumption between the concept parameter and
its corresponding output distribution. Our assumption is based on prior works (Kleijn & van der Vaart,
2012), where the KL divergence is assumed to have a 2nd-order Taylor expansion."
NON-DISTINGUISHABLE CASE,0.1821561338289963,"Theorem 2. Let the set of θ which does not satisfy Equation 14 in Condition 1 to be B. Assume that
KL divergences have a 2nd-order Taylor expansion around θ∗:"
NON-DISTINGUISHABLE CASE,0.18401486988847585,"∀j >1, KLj(θ∗∥θ)= 1"
NON-DISTINGUISHABLE CASE,0.18587360594795538,"2(θ−θ∗)⊤Ij,θ∗(θ−θ∗)+O(∥θ−θ∗∥3)
(16)"
NON-DISTINGUISHABLE CASE,0.18773234200743494,"where Ij,θ∗is the Fisher information matrix of the j-th token distribution with respect to θ∗. Let
γθ∗= maxjλmax(Ij,θ∗)"
NON-DISTINGUISHABLE CASE,0.1895910780669145,"minjλmin(Ij,θ∗) where λmax,λmin return the largest and smallest eigenvalues. Then for k>1 and
as n→∞, the 0-1 risk of the in-context learning predictor fn is bounded as"
NON-DISTINGUISHABLE CASE,0.19144981412639406,"lim
n→∞L0-1(fn)≤inf
f L0-1(f)+g−1

O
γθ∗supθ∈B(ϵθ
start+ϵθ
delim)
k−1"
NON-DISTINGUISHABLE CASE,0.19330855018587362,"
(17)"
NON-DISTINGUISHABLE CASE,0.19516728624535315,where g(δ)= 1
NON-DISTINGUISHABLE CASE,0.1970260223048327,"2((1−δ)log(1−δ)+(1+δ)log(1+δ)) is a calibration function (Steinwart, 2007; ´Avila
Pires & Szepesv´ari, 2016) for the multiclass logistic loss for δ∈[0,1)."
NON-DISTINGUISHABLE CASE,0.19888475836431227,Published as a conference paper at ICLR 2022
NON-DISTINGUISHABLE CASE,0.20074349442379183,"0
20
40
60
Num examples 40 50 60 70 80 90 Acc"
NON-DISTINGUISHABLE CASE,0.20260223048327136,"k=3
k=5
k=8
k=10"
NON-DISTINGUISHABLE CASE,0.20446096654275092,"0
20
40
60
Num examples 40 60 80 100 Acc"
NON-DISTINGUISHABLE CASE,0.20631970260223048,"k=3
k=5
k=8
k=10"
NON-DISTINGUISHABLE CASE,0.20817843866171004,"Figure 3: In-context accuracy (95% intervals) of Transformers (left) and LSTMs (right) on the GINC
dataset. Accuracy increases with number of examples n and length of each example k."
NON-DISTINGUISHABLE CASE,0.2100371747211896,"0
20
40
60
Num examples 20 22 24 26 Acc"
NON-DISTINGUISHABLE CASE,0.21189591078066913,"k=3
k=5
k=8
k=10"
NON-DISTINGUISHABLE CASE,0.2137546468401487,"0
20
40
60
Num examples 1 2 3 Acc"
NON-DISTINGUISHABLE CASE,0.21561338289962825,"k=3
k=5
k=8
k=10"
NON-DISTINGUISHABLE CASE,0.2174721189591078,"0
20
40
60
Num examples 9 10 11 12 13 Acc"
NON-DISTINGUISHABLE CASE,0.21933085501858737,"k=3
k=5
k=8
k=10"
NON-DISTINGUISHABLE CASE,0.2211895910780669,"Figure 4: Ablation studies for 4 layer Transformers on the GINC dataset with vocab size 50. (Left)
When pretrained with only one concept, in-context learning fails. (Middle) When the pretraining data
has random transitions, the model sees all token transitions but in-context learning fails. (Right) When
prompts are from random unseen concepts, in-context learning fails to extrapolate."
NON-DISTINGUISHABLE CASE,0.22304832713754646,"Since the inverse calibration function g−1 is roughly linear in ϵ for ϵ ≤0.7, the excess risk roughly
decreases as O(1/k). When the “worst-case condition number” γθ∗of the Fisher information matrices
is smaller (well-conditioned), the error decreases. Intuitively, this means that there is no direction
to vary θ∗in which the output distribution will sharply change. As a consequence, the concepts θ that
are not distinguishable from the prompt concept θ∗parameterize distributions that produce similar
outputs to the prompt concept and thus achieve a small error."
NON-DISTINGUISHABLE CASE,0.22490706319702602,"Varying-length test examples.
In the setting where the length of xtest is random (uniformly from
2 to k), we can give a similar error guarantee without continuity."
NON-DISTINGUISHABLE CASE,0.22676579925650558,"Theorem 3. Let the set of θ which does not satisfy Equation 14 in Condition 1 to be B. Let the length
of the test example xtest be uniformly distributed between 2 and k, for k ≥2. Then for k ≥2 and as
n→∞, the 0-1 risk of the in-context learning predictor fn is bounded as"
NON-DISTINGUISHABLE CASE,0.22862453531598512,"lim
n→∞L0-1(fn)≤inf
f L0-1(f)+g−1

O
supθ∈B(ϵθ
start+ϵθ
delim)
k−1"
NON-DISTINGUISHABLE CASE,0.23048327137546468,"
.
(18)"
NON-DISTINGUISHABLE CASE,0.23234200743494424,"Instead of measuring only the error at the k-th token, we average the prediction error on the 2nd to k-th
tokens. However, we leave bridging the mismatch between training examples, which are consistently
length k, and test examples, which have random length, to future work."
SIMULATIONS,0.2342007434944238,"4
SIMULATIONS"
SIMULATIONS,0.23605947955390336,"We generate the GINC dataset and show that Transformers (Vaswani et al., 2017) and LSTMs (Hochre-
iter & Schmidhuber, 1997) trained on GINC exhibit in-context learning. In the theory, we assumed
that the pretrained LM ﬁts the pretraining distribution exactly. Here, we pretrain LMs to approximate
the pretraining distribution and ﬁnd that the in-context learning properties transfer to the LM."
SIMULATIONS,0.2379182156133829,"GINC dataset.
We construct the GINC dataset according to our theory (see Appendix F.1). For
pretraining, we deﬁne a uniform mixture of HMMs over a family Θ of 5 concepts to generate 1000
pretraining documents with ∼10 million tokens total. For prompting, we generate prompts with 0
to 64 training examples and example lengths k ∈{3,5,8,10} (2500 prompts for each setting). The
target token ytest is taken to be the most likely output instead of sampling so that the intrinsic error is 0."
SIMULATIONS,0.23977695167286245,"Main result.
We train GPT-2-based Transformers (Radford et al., 2019) and LSTMs on three
versions of the GINC dataset with vocabulary sizes 50, 100, and 150, then evaluate the in-context"
SIMULATIONS,0.241635687732342,Published as a conference paper at ICLR 2022
SIMULATIONS,0.24349442379182157,"0.4
0.6
0.8
1.0
Num Parameters
1e8 60 70 80 90 100 Acc"
SIMULATIONS,0.24535315985130113,"Vocab size=50
Vocab size=100
Vocab size=150"
SIMULATIONS,0.24721189591078066,"Figure 5: In-context accuracy (95%
intervals) of Transformers improves
as model size increases on the
GINC dataset for vocabulary sizes
50, 100, and 150."
SIMULATIONS,0.24907063197026022,"Model
# Params
Train loss
(pretraining)
Val loss
(pretraining)
In-context Acc"
SIMULATIONS,0.25092936802973975,"Vocab size 50, k=10,n=64
Transformer (4 layer)
29M
1.49
1.50
60.2 ± 5.7
Transformer (12 layer)
85M
1.31
1.33
81.2 ± 7.1
Transformer (16 layer)
115M
1.31
1.33
84.7 ± 3.4
LSTM
28M
1.31
1.35
95.8 ± 1.11
Vocab size 100, k=10,n=64
Transformer (4 layer)
29M
1.58
1.59
67.4 ± 4.7
Transformer (12 layer)
85M
1.40
1.42
84.6 ± 3.0
Transformer (16 layer)
115M
1.41
1.43
88.7 ± 1.6
LSTM
28M
1.43
1.44
95.8 ± 1.54
Vocab size 150, k=10,n=64
Transformer (4 layer)
29M
1.44
1.45
92.8 ± 1.9
Transformer (12 layer)
85M
1.27
1.28
98.4 ± 0.4
Transformer (16 layer)
115M
1.27
1.28
98.1 ± 0.5
LSTM
28M
1.26
1.31
99.2 ± 1.06"
SIMULATIONS,0.2527881040892193,"Figure 6: In-context accuracies (95% intervals)
on GINC with vocab sizes (50, 100, 150) for
Transformers and LSTMs. Accuracy improves
with scale even though the pretraining loss may
be the same."
SIMULATIONS,0.25464684014869887,"accuracy (see Appendix F.2, F.3). We average all results over 5 pretraining runs. Figure 3 shows that
for both Transformer and LSTMs, in-context accuracy improves as the number of prompt examples
n and the example length k increase, verifying our theory."
SIMULATIONS,0.25650557620817843,"Ablations on the latent concept structure.
We ablate the role of the mixture-of-concepts structure
in GINC. In Figure 4 (left), we pretrain a 4 layer Transformer on data with only one concept (removing
the prior) from Θ, resulting in ﬂat in-context learning curves. Figure 4 (middle) shows that pretraining
on random pretraining data, which contains all possible token transitions, in-context learning also fails.
Therefore, the mixture-of-concepts structure is important and simply seeing diverse token transitions
does not enable in-context learning."
SIMULATIONS,0.258364312267658,"Extrapolation to unseen concepts.
Full generative control of GINC allows for experimentation with
latent variables in the pretraining distribution. For example, in large-scale datasets, it is difﬁcult to test
whether a concept or task is in the pretraining data. We test this in GINC by testing the in-context accu-
racy of a 4 layer Transformer on prompts generated from 5 random concepts that are not in the pretrain-
ing family of concepts. Figure 4 (right) shows that in-context learning also fails for these novel concepts."
SIMULATIONS,0.26022304832713755,"Effect of model size and architecture.
Figure 5 shows that increasing the size of the Transformer
(4, 12, 16 layers) steadily increases the in-context accuracy, corroborating the results of Brown et al.
(2020). Table 6 shows that even though larger Transformers may have the same pretraining loss
(e.g., 12 and 16 layer Transformers both get 1.33 validation loss for vocab size 50), the in-context
accuracy still improves (81% to 85% from 12 to 16 layers), suggesting that larger models can improve
in-context learning beyond improving pretraining perplexity. This may be related to phenomena from
overparameterization and overtraining (Zhang et al., 2017; Power et al., 2021). Finally, the model
architecture also plays a role — LSTMs consistently outperform Transformers on GINC despite having
fewer parameters, perhaps due to the similarity between HMMs and LSTMs. We leave analysis of
the effect of model scaling and model architecture as open questions."
SIMULATIONS,0.2620817843866171,"Sensitivity to example ordering.
In Figure 7 (left), we test the sensitivity of in-context accuracy
on GINC to the ordering of the prompt examples, following Zhao et al. (2021). For this experiment,
we consider prompts generated from a single concept and prompt start distribution. We sample 10
different sets (leading to 10 training set IDs) of 4 examples and generate all 24 possible permutations
for each example set. We consider the in-context accuracy of the 4 layer Transformer trained on GINC
with vocabulary size 50. Similarly to the behavior of GPT-3 (Zhao et al., 2021), there is a signiﬁcant
variation (10–40% difference) between permutations of the same set of examples."
SIMULATIONS,0.26394052044609667,"Zero-shot is sometimes better than few-shot.
In some settings in GINC, we ﬁnd that zero-shot
performance can be better than few-shot performance. This mirrors GPT-3 on some datasets (e.g., LAM-
BADA, HellaSwag, PhysicalQA, RACE-m, CoQA/SAT analogies for smaller models (Brown et al.,
2020)). This occurs especially when the transition probabilities in GINC are lower entropy (controlled
via a temperature parameter). For this experiment, we consider GINC with transition matrix temper-
ature parameter 0.01 (instead of 0.1), 12 concepts, and vocabulary size 100. Figure 7 (right) shows that
here, few-shot accuracy is initially worse than zero-shot accuracy, but can recover with more examples.
We hypothesize that the distracting prompt structure initially decreases the accuracy in this setting."
SIMULATIONS,0.26579925650557623,Published as a conference paper at ICLR 2022
SIMULATIONS,0.26765799256505574,"0
1
2
3
4
5
6
7
8
9
Training set ID 30 40 50 60 70 80"
SIMULATIONS,0.2695167286245353,Accuracy
SIMULATIONS,0.27137546468401486,"0
20
40
60
Num examples 20 30 40 50 Acc"
SIMULATIONS,0.2732342007434944,"k=3
k=5
k=8
k=10"
SIMULATIONS,0.275092936802974,"Figure 7: (Left) In-context accuracy varies widely with example ordering. Each training ID refers
to a set of training examples. Each dot refers to the in-context learning accuracy of one permutation
of the training examples for that particular training ID. (Right) Zero-shot performance can be higher
than one/few-shot performance in some settings in GINC, mirroring the behavior of GPT-3 on some
datasets such as LAMBADA (Brown et al., 2020). The few-shot setting introduces the distracting
prompt structure, which can initially lower accuracy."
DISCUSSION AND RELATED WORK,0.27695167286245354,"5
DISCUSSION AND RELATED WORK"
DISCUSSION AND RELATED WORK,0.2788104089219331,"Learning via Bayesian inference and extrapolation.
The canonical Bernstein-von Mises theo-
rem (van der Vaart, 1998) does not apply for in-context learning since the prompt examples are not
independent under the pretraining distribution. Gunst & Shcherbakova (2008) show a Bernstein-von
Mises-type result for observations from an HMM, but do not handle observations from a different dis-
tribution. Future directions include more precise asymptotic results about the posterior distribution and
results under misspeciﬁcation/extrapolation (Kleijn & van der Vaart, 2012). A possible avenue for ex-
trapolation to some types of unseen concepts is to factorize the latent concept into semantics and syntax.
While the pretraining data may contain only some semantics-syntax pairs, the language model could
generalize to unseen pairs if it learns generalizable syntactical operations such as copying or reordering."
DISCUSSION AND RELATED WORK,0.28066914498141265,"Topic models and HMMs.
Topic models such as LDA (Blei et al., 2003) also have document-level
latent variables, but learning is typically relies on algorithms such as EM (Dempster et al., 1977),
variational inference (Jordan et al., 1999), or MCMC (Metropolis et al., 1953; Hastings, 1970). We
focus on learning as a natural result of Bayesian inference without an explicit inference algorithm.
Wei et al. (2021a) also use an HMM model in their pretraining analysis. However, they analyze how
pre-trained representations learned with masked LMs (Devlin et al., 2019; Liu et al., 2019; Lewis
et al., 2020; Clark et al., 2020) can improve optimization-based downstream learning (Li & Liang,
2021; Lester et al., 2021) rather than in-context learning."
DISCUSSION AND RELATED WORK,0.2825278810408922,"Bridging the mismatch between pretraining and prompting.
Prior works support our theoretical
intuitions that reducing the prompt distribution mismatch would improve in-context learning. Finetun-
ing LMs on text with a prompting format improves its zero-shot performance (Wei et al., 2021b; Sanh
et al., 2021) and optimizing prompt templates improves few-shot ﬁnetuning (Jiang et al., 2020; Schick
& Sch¨utze, 2021; Shin et al., 2020; Gao et al., 2021). Zhao et al. (2021); Holtzman et al. (2021) improve
in-context accuracy via calibration or renormalization, a form of adaptation to the prompt distribution."
DISCUSSION AND RELATED WORK,0.2843866171003718,"Meta-learning.
Meta-learning methods can also train a sequence model to learn from examples (Ravi
& Larochelle, 2017). However, meta-learning models are trained to learn, while in-context learning
emerges from LM pretraining."
DISCUSSION AND RELATED WORK,0.2862453531598513,"Studying large-scale phenomena at a small scale.
We can study in-context learning, a large scale
phenomenon, at a small scale in GINC because the complexity of the pretraining distribution (HMM
hidden state size, number of latent concepts) is small, such that the data and models are relatively larger.
Since GINC is synthetic, we can also control the latent data properties (e.g., unseen concepts) to make
predictions about large LMs while working at a small scale."
CONCLUSION,0.28810408921933084,"6
CONCLUSION"
CONCLUSION,0.2899628252788104,"We cast in-context learning as implicit Bayesian inference, where the pretrained LM implicitly infers
a concept when making a prediction. We show that in-context learning occurs when the pre-training
distribution is a mixture of HMMs. Our work provides a ﬁrst step towards understanding in-context
learning, which we hope will provide insight for improving pretraining and prompting."
CONCLUSION,0.29182156133828996,Published as a conference paper at ICLR 2022
CONCLUSION,0.2936802973977695,ACKNOWLEDGEMENTS
CONCLUSION,0.2955390334572491,"We thank Tianyi Zhang, Frieda Rong, Lisa Li, Colin Wei, Shibani Santurkar, Tri Dao, Ananya
Kumar, and Shivam Garg for helpful discussions and feedback. SMX is supported by an NDSEG
Fellowship. The work is partially supported by an Open Philanthropy Project Award, SDSI, and SAIL
at Stanford University. TM acknowledges support of Google Faculty Award, NSF IIS 2045685, the
Sloan Fellowship, and JD.com. Toyota Research Institute provided funds to support this work."
REFERENCES,0.29739776951672864,REFERENCES
REFERENCES,0.2992565055762082,"Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of ﬁnite state markov
chains. The annals of mathematical statistics, 37(6):1554–1563, 1966."
REFERENCES,0.30111524163568776,"D. Blei, Andrew Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning
Research (JMLR), 3:993–1022, 2003."
REFERENCES,0.30297397769516726,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.3048327137546468,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training
text encoders as discriminators rather than generators. In International Conference on Learning
Representations (ICLR), 2020."
REFERENCES,0.3066914498141264,"A. P. Dempster, Laird N. M., and Rubin D. B. Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society: Series B, 39(1):1–38, 1977."
REFERENCES,0.30855018587360594,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Association for Computational Linguistics
(ACL), pp. 4171–4186, 2019."
REFERENCES,0.3104089219330855,"Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. arXiv, 2021."
REFERENCES,0.31226765799256506,"Zoubin Ghahramani and Michael Jordan. Factorial hidden Markov models. Machine Learning, 29:
245–273, 1997."
REFERENCES,0.3141263940520446,"Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
Hidden topic Markov models.
In Artiﬁcial
Intelligence and Statistics (AISTATS), 2007."
REFERENCES,0.3159851301115242,"M. Gunst and O. Shcherbakova. Asymptotic behavior of Bayes estimators for hidden Markov models
with application to ion channels. Mathematical Methods of Statistics, 17, 2008."
REFERENCES,0.31784386617100374,"Keith W. Hastings. Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57(1):97–109, 1970."
REFERENCES,0.31970260223048325,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735–1780, 1997."
REFERENCES,0.3215613382899628,"Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.32342007434944237,"Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer.
Surface form
competition: Why the highest probability answer isn’t always right, 2021."
REFERENCES,0.3252788104089219,"Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language
models know? In Association for Computational Linguistics (ACL), 2020."
REFERENCES,0.3271375464684015,"Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction
to variational methods for graphical models. Machine Learning, 37:183–233, 1999."
REFERENCES,0.32899628252788105,Published as a conference paper at ICLR 2022
REFERENCES,0.3308550185873606,"Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehension.
In Association for Computational
Linguistics (ACL), 2017."
REFERENCES,0.33271375464684017,"Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.3345724907063197,"B.J.K. Kleijn and A.W. van der Vaart. The Bernstein-von mises theorem under misspeciﬁcation.
Electronic Journal of Statistics, 6, 2012."
REFERENCES,0.33643122676579923,"Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021."
REFERENCES,0.3382899628252788,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension. In Association for Computational Linguistics
(ACL), 2020."
REFERENCES,0.34014869888475835,"Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In
Association for Computational Linguistics (ACL), 2021."
REFERENCES,0.3420074349442379,"Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation.
Technical report, AI21 Labs, August 2021."
REFERENCES,0.34386617100371747,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.34572490706319703,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference
on Learning Representations (ICLR), 2019."
REFERENCES,0.3475836431226766,"Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward
Teller. Equation of state calculations by fast computing machines. The journal of chemical physics,
21(6):1087–1092, 1953."
REFERENCES,0.34944237918215615,"Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset:
Word prediction requiring a broad discourse context. In Association for Computational Linguistics
(ACL), 2016."
REFERENCES,0.3513011152416357,"Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
Grokking:
Generalization beyond overﬁtting on small algorithmic datasets. In ICLR MATH AI Workshop, 2021."
REFERENCES,0.35315985130111527,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019."
REFERENCES,0.3550185873605948,"Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International
Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.35687732342007433,"Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu,
Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal
Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng
Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman,
Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables
zero-shot task generalization, 2021."
REFERENCES,0.3587360594795539,"Timo Schick and Hinrich Sch¨utze. Exploiting cloze questions for few shot text classiﬁcation and
natural language inference. In European Association for Computational Linguistics (EACL), 2021."
REFERENCES,0.36059479553903345,"Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Eliciting
knowledge from language models using automatically generated prompts. In Empirical Methods
in Natural Language Processing (EMNLP), 2020."
REFERENCES,0.362453531598513,Published as a conference paper at ICLR 2022
REFERENCES,0.3643122676579926,"Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approximation,
26, 2007."
REFERENCES,0.36617100371747213,"A. W. van der Vaart. Asymptotic statistics. Cambridge University Press, 1998."
REFERENCES,0.3680297397769517,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017."
REFERENCES,0.36988847583643125,"Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.
https://github.com/kingoflolz/mesh-transformer-jax, May 2021."
REFERENCES,0.37174721189591076,"Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream
tasks? an analysis of head and prompt tuning. arXiv, 2021a."
REFERENCES,0.3736059479553903,"Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv, 2021b."
REFERENCES,0.3754646840148699,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFace’s
transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019."
REFERENCES,0.37732342007434944,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization.
In International Conference on Learning
Representations (ICLR), 2017."
REFERENCES,0.379182156133829,"Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving
few-shot performance of language models. In International Conference on Machine Learning
(ICML), 2021."
REFERENCES,0.38104089219330856,"Bernardo ´Avila Pires and Csaba Szepesv´ari. Multiclass classiﬁcation calibration functions. arXiv, 2016."
REFERENCES,0.3828996282527881,Published as a conference paper at ICLR 2022
REFERENCES,0.3847583643122677,"A
FRAMEWORK DETAILS"
REFERENCES,0.38661710037174724,"Prompt distribution details.
For in-context learning, we sample a prompt from a new distribution
pprompt, which consists of n independent training examples and 1 test example. We ﬁrst sample
n hidden segments H of length k by sampling the ﬁrst element hstart = H[1] from a prompt start
distribution pprompt. Then, we sample the rest of the segment Hseg =H[2:k] from the hidden transition
distribution of the pretraining distribution p corresponding to a particular concept θ∗:
H1,...,Hn,
Hi =[hi,1,...,hi,k]
(19)"
REFERENCES,0.38847583643122674,"hstart
i
=Hi[1]∼pprompt,
Hseg
i
=Hi[2:k]∼p(Hseg
i |hstart,θ∗).
(20)"
REFERENCES,0.3903345724907063,"To end each example (except the test example), we sample n delimiters hdelim ∈D from pdelim
prompt:"
REFERENCES,0.39219330855018586,"hdelim
1
,...,hdelim
n
,
hdelim
i
∼pdelim
prompt.
(21)"
REFERENCES,0.3940520446096654,"Conditioned on hidden variables Hi and hdelim
i
, we sample the observed tokens Oi =[oi,1,...,oi,k] and
odelim
i
respectively from the pre-training distribution:
O1,...,On,
Oi ∼p(Oi|Hi)
(22)"
REFERENCES,0.395910780669145,"odelim
1
,...,odelim
n
,
odelim
i
∼p(odelim
i
|hdelim
i
,θ∗)
(23)
The “input” for each example is xi =Oi[1:k−1] and the “output” is yi =Oi[k]. Taking S to be the
sequence of training examples (without the test example), the resulting prompt sequence is
[Sn,xtest]=[O1,odelim
1
,...,On,odelim
n
,xtest]=[x1,y1,odelim
1
,x2,y2,odelim
2
,...,xn,yn,odelim
n
,xtest]∼pprompt
(24)
where xtest =xn+1 =On+1[1:k−1] is sampled via the same process but with k−1 elements."
REFERENCES,0.39776951672862454,"B
PROPOSITIONS FOR THEOREM 1"
REFERENCES,0.3996282527881041,"The following propositions, which lower bound the probability of a delimiter token and probability
of an example under θ∗, are direct corollaries of the assumptions."
REFERENCES,0.40148698884758366,"Proposition 1. For all i, we have p(hdelim
i
|O1,odelim
1
,...,Oi,θ∗)>c1 and p(hdelim
i
|O1,odelim
1
,...,Oi,θ)<
c2."
REFERENCES,0.4033457249070632,"Proof. By Assumption 2,"
REFERENCES,0.4052044609665427,"p(hdelim
i
|O1,odelim
1
,...,Oi,θ)=
X"
REFERENCES,0.4070631970260223,"hi,k
p(hdelim
i
|hi,k)p(hi,k|O1,odelim
1
,...,Oi,θ)
(25) <
X"
REFERENCES,0.40892193308550184,"hi,k
c2p(hi,k|O1,odelim
1
,...,Oi,θ)=c2.
(26)"
REFERENCES,0.4107806691449814,"Similarly,"
REFERENCES,0.41263940520446096,"p(hdelim
i
|O1,odelim
1
,...,Oi,θ∗)=
X"
REFERENCES,0.4144981412639405,"hi,k
p(hdelim
i
|hi,k)p(hi,k|O1,odelim
1
,...,Oi,θ∗)
(27) >
X"
REFERENCES,0.4163568773234201,"hi,k
c1p(hi,k|O1,odelim
1
,...,Oi,θ∗)=c1.
(28)"
REFERENCES,0.41821561338289964,"Proposition 2. The probability of an example is lower bounded for θ∗: there is some c7 >0 such that
p(Oi|hstart
i
,hj,l,θ∗)>c7 for all i and future hidden states hj,l, for any l and j >i."
REFERENCES,0.4200743494423792,"Proof. By Assumption 5, we have"
REFERENCES,0.42193308550185876,"p(Oi|hstart
i
,hj,l,θ∗)=
X"
REFERENCES,0.42379182156133827,"Hi
p(Oi|Hi)p(Hi|hstart
i
,hj,l,θ∗)>(c6)k
(29)"
REFERENCES,0.4256505576208178,for some Hi. We have
REFERENCES,0.4275092936802974,"p(Hi|hstart
i
,hj,l,θ∗)= p(hj,l|H,hstart
i
,θ∗)p(H|hstart
i
,θ∗)
p(hj,l|hstart
i
,θ∗)
>c2
5
(30)"
REFERENCES,0.42936802973977695,"which lower bounds the terms in the numerator by c5 (marginalizing over previous hidden states),
and upper bounding the denominator by 1. Setting c7 =(c6)kc2
5 ﬁnishes the proof."
REFERENCES,0.4312267657992565,Published as a conference paper at ICLR 2022
REFERENCES,0.43308550185873607,"C
CONVERGENCE OF THE IN-CONTEXT PREDICTOR"
REFERENCES,0.4349442379182156,"Under Assumption 3, we show that the in-context predictor fn(xtest) = argmaxy p(y|Sn, xtest)
converges when abstracting away the Bayesian inference component (the selection of θ∗from Θ) of
the in-context predictor. We will complete the argument for the convergence of the in-context predictor
in the proof of Theorem 1."
REFERENCES,0.4368029739776952,"Lemma 1. Suppose the prompt Sn and the test input xtest are given. Under Assumption 3, we show
that the argmax of the averaged predictive distribution conditioned on θ∗and a prompt Sn is the same
as the argmax of the prompt predictive distribution:"
REFERENCES,0.43866171003717475,"argmax
y X"
REFERENCES,0.44052044609665425,"hstart
test ∈H
p(y|xtest,hstart
test ,θ∗)p(hstart
test |Sn,xtest,θ∗)=argmax
y
pprompt(y|xtest).
(31)"
REFERENCES,0.4423791821561338,"Proof. First, we note by deﬁnition that"
REFERENCES,0.44423791821561337,"pprompt(y|xtest)=
X"
REFERENCES,0.44609665427509293,"hstart
test ∈H
p(y|xtest,hstart
test ,θ∗)pprompt(hstart
test |xtest).
(32)"
REFERENCES,0.4479553903345725,"Expanding the last term, we have
pprompt(hstart
test |xtest)∝p(xtest|hstart
test ,θ∗)pprompt(hstart
test ).
(33)
which is proportional to a constant in xtest."
REFERENCES,0.44981412639405205,"On the other hand, analyzing one term inside the LHS of the lemma statement, we have
p(hstart|Sn,xtest,θ∗)∝p(xtest|hstart
test ,θ∗)p(hstart
test |Sn,θ∗)
(34)
which is proportional to a constant in xtest and Sn. The quantities differ in the last term, which we
expand below and put in matrix form. Let T ∈R|H|×|D| be the matrix that represents the transition
probabilities starting from a delimiter state: p(hstart
test |hdelim) for hstart
test ∈H and hdelim ∈D. As a result,"
REFERENCES,0.4516728624535316,"p(hstart
test |Sn,θ∗)=
X"
REFERENCES,0.45353159851301117,"hdelim
n
p(hstart
test |hdelim
n
,θ∗)p(hdelim
n
|Sn,θ∗)
(35)"
REFERENCES,0.45539033457249073,"=Tv
(36)"
REFERENCES,0.45724907063197023,"where hdelim
n
is the delimiter hidden state before hstart
test ."
REFERENCES,0.4591078066914498,"Let W ∈R|Y|×|H| be the matrix that represents the probabilities p(y|xtest,hstart
test ,θ∗)p(xtest|hstart
test ,θ∗) for
all the possible y∈Y and hstart
test ∈H. Overall, we can write
X"
REFERENCES,0.46096654275092935,"hstart
test ∈H
p(·|xtest,hstart
test ,θ∗)p(hstart
test |Sn,xtest,θ∗)=WTv
(37)"
REFERENCES,0.4628252788104089,"pprompt(·|xtest)=Wu
(38)"
REFERENCES,0.4646840148698885,where u∈R|H| is the vector of probabilities that corresponds to the prompt start distribution pprompt.
REFERENCES,0.46654275092936803,"Bounding the difference between the two predictive distributions,
∥WTv−Wu∥∞≤∥WTv−Wu∥1
(39) = |Y|
X"
REFERENCES,0.4684014869888476,"i=1
|W ⊤
i (Tv−u)|i
(40) = |Y|
X i=1  |H|
X"
REFERENCES,0.47026022304832715,"j=1
Wij(Tv−u)j (41) ≤ |Y|
X i=1 |H|
X"
REFERENCES,0.4721189591078067,"j=1
Wij|(Tv−u)j|
(Wij ≥0)
(42) = |H|
X j=1
( |Y|
X"
REFERENCES,0.4739776951672863,"i=1
Wij)|(Tv−u)j|
(43)"
REFERENCES,0.4758364312267658,"=∥Tv−u∥1.
(44)"
REFERENCES,0.47769516728624534,Published as a conference paper at ICLR 2022
REFERENCES,0.4795539033457249,"Using Assumption 3, we can further bound this by ∆/2:"
REFERENCES,0.48141263940520446,"∥Tv−u∥1 =2TV (pprompt(·)∥ |D|
X"
REFERENCES,0.483271375464684,"i=1
vip(·|hdelim =i,θ∗))
(45) ≤2 |D|
X"
REFERENCES,0.4851301115241636,"i=1
viTV (pprompt(·)∥p(·|hdelim =i,θ∗)) (convexity of TV distance)
(46)"
REFERENCES,0.48698884758364314,"≤2 max
hdelim∈DTV (pprompt(·)∥p(·|hdelim,θ∗))<∆/2.
(47)"
REFERENCES,0.4888475836431227,"Since the probability of any output does not change by more than ∆/2 and the margin between the most
likely label and the second most likely label is ∆, the argmax’s are the same, showing the result."
REFERENCES,0.49070631970260226,"D
PROOF OF THEOREM 1"
REFERENCES,0.49256505576208176,"Proof. We analyze the most likely prediction over the pretraining distribution conditioned on the
prompt argmaxyp(y|Sn,xtest)."
REFERENCES,0.4944237918215613,"p(y|Sn,xtest)=
Z"
REFERENCES,0.4962825278810409,"θ
p(y|Sn,xtest,θ)p(θ|Sn,xtest)dθ
(48) ∝
Z"
REFERENCES,0.49814126394052044,"θ
p(y|Sn,xtest,θ)p(Sn,xtest|θ)p(θ)dθ
(49) ∝
Z"
REFERENCES,0.5,"θ
p(y|Sn,xtest,θ) p(Sn,xtest|θ)"
REFERENCES,0.5018587360594795,"p(Sn,xtest|θ∗)p(θ)dθ
(50) =
Z θ X"
REFERENCES,0.5037174721189591,"hstart
test ∈H
p(y|xtest,hstart
test ,θ)p(hstart
test |Sn,xtest,θ) p(Sn,xtest|θ)"
REFERENCES,0.5055762081784386,"p(Sn,xtest|θ∗)p(θ)dθ
(51)"
REFERENCES,0.5074349442379182,"Deﬁning the following quantity,"
REFERENCES,0.5092936802973977,rn(θ)= 1
REFERENCES,0.5111524163568774,"nlog p(Sn,xtest|θ)"
REFERENCES,0.5130111524163569,"p(Sn,xtest|θ∗).
(52)"
REFERENCES,0.5148698884758365,"we will show that under distinguishability for all θ̸=θ∗, rn(θ) converges to a negative constant such that"
REFERENCES,0.516728624535316,"p(Sn,xtest|θ)
p(Sn,xtest|θ∗) =exp(n·rn(θ))→0
(53)"
REFERENCES,0.5185873605947955,"for θ̸=θ∗, whereas this ratio is always 1 for θ=θ∗. This will then “select” the desired prompt concept
through marginalization."
REFERENCES,0.5204460966542751,"Supposing that Equation 53 holds, we show that the theorem statement holds. Let"
REFERENCES,0.5223048327137546,"∆′ = max
hdelim∈DTV (pprompt(·)∥p(·|hdelim,θ∗))<∆/2,
(54)"
REFERENCES,0.5241635687732342,"and let ϵ<(∆/2−∆′)p(θ∗). Then for n large enough (due to Equation 53),
Z θ X"
REFERENCES,0.5260223048327137,"hstart
test ∈H
p(y|xtest,hstart
test ,θ)p(hstart
test |Sn,xtest,θ) p(Sn,xtest|θ)"
REFERENCES,0.5278810408921933,"p(Sn,xtest|θ∗)p(θ)dθ
(55) =
X"
REFERENCES,0.5297397769516728,"hstart
test ∈H
p(y|xtest,hstart
test ,θ∗)p(hstart
test |Sn,xtest,θ∗)p(θ∗)+
Z"
REFERENCES,0.5315985130111525,"θ̸=θ∗ϵθ(y)p(θ)dθ
(56) ∝
X"
REFERENCES,0.533457249070632,"hstart
test ∈H
p(y|xtest,hstart
test ,θ∗)p(hstart
test |Sn,xtest,θ∗)+
1
p(θ∗) Z"
REFERENCES,0.5353159851301115,"θ̸=θ∗ϵθ(y)p(θ)dθ
(57)"
REFERENCES,0.5371747211895911,where ϵθ(y)≤ϵ/2 for all y∈Y.
REFERENCES,0.5390334572490706,"By Lemma 1, the argmax of the ﬁrst term of Equation 57 is the same as argmaxypprompt(y|xtest), where
the margin between the most likely label and the second most likely is at least ∆/2−∆′. Since
1
p(θ∗) Z"
REFERENCES,0.5408921933085502,"θ̸=θ∗ϵθ(y)p(θ)≤
ϵ
2p(θ∗) <(∆/2−∆′)/2
(58)"
REFERENCES,0.5427509293680297,Published as a conference paper at ICLR 2022
REFERENCES,0.5446096654275093,"for all y∈Y, the argmax of Equation 57 is also the same as argmaxpprompt(y|xtest)."
REFERENCES,0.5464684014869888,"Now it remains to show that rn(θ) converges to a negative constant for θ ̸=θ∗. Let Oex
i =[odelim
i−1 ,Oi]
be the i-th observation segment and the previous delimiter together for i > 1 and deﬁne Oex
1 = O1.
Expanding the numerator of the ratio in rn(θ), we have"
REFERENCES,0.5483271375464684,"p(Sn,xtest|θ)=p(xtest|Sn,θ)p(Sn|θ)
(59) =
X"
REFERENCES,0.550185873605948,"hstart
test
p(xtest|hstart
test ,θ)p(hstart
test |Sn,θ)p(odelim
n
|Oex
1:n,θ) n
Y"
REFERENCES,0.5520446096654275,"i=1
p(Oex
i |Oex
1:i−1,θ)
(60) =
X"
REFERENCES,0.5539033457249071,"hstart
test
p(xtest|hstart
test ,θ)p(hstart
test |Sn,θ)
(61) X"
REFERENCES,0.5557620817843866,"hdelim
n
∈D
p(odelim
n
|hdelim
n
)p(hdelim
n
|Oex
1:n,θ) n
Y i=1 X"
REFERENCES,0.5576208178438662,"hdelim
i−1∈D
p(Oi|hdelim
i−1 ,θ)p(hdelim
i−1 |Oex
1:i−1,θ) (62) =
X"
REFERENCES,0.5594795539033457,"hstart
test
p(xtest|hstart
test ,θ)p(hstart
test |Sn,θ)
(63) X"
REFERENCES,0.5613382899628253,"hdelim
n
∈D
p(hdelim
n
|Oex
1:n,θ) n
Y i=1 X"
REFERENCES,0.5631970260223048,"hdelim
i−1∈D
p(Oi|hdelim
i−1 ,θ)p(hdelim
i−1 |Oex
1:i−1,θ)
(64) =
X"
REFERENCES,0.5650557620817844,"hstart
test
p(xtest|hstart
test ,θ)p(hstart
test |Sn,θ) n
Y i=1 X"
REFERENCES,0.5669144981412639,"hdelim
i−1∈D
p(Oi|hdelim
i−1 ,θ)p(hdelim
i−1 |Oex
1:i−1,θ)
(65)"
REFERENCES,0.5687732342007435,"Note that in the last line, the inner sum is over the set of delimiter states D by using the assumption
that observing a delimiter odelim implies that the corresponding hidden state hdelim must be in D. We
also see that P"
REFERENCES,0.570631970260223,"hdelim
n p(hdelim
n
|Oex
1:n,θ)=1."
REFERENCES,0.5724907063197026,"We restrict our attention to θ where p(Sn,xtest|θ)>0, since otherwise θ does not affect the prediction.
Expanding rn(θ), we have the following upper bound:"
REFERENCES,0.5743494423791822,rn(θ)= 1 n
REFERENCES,0.5762081784386617,"
log p(Sn,xtest|θ)"
REFERENCES,0.5780669144981413,"p(Sn,xtest|θ∗)"
REFERENCES,0.5799256505576208,"
(66) = 1 n 
log P"
REFERENCES,0.5817843866171004,"hstart
test p(xtest|hstart
test ,θ)p(hstart
test |Sn,θ)
P"
REFERENCES,0.5836431226765799,"hstart
test p(xtest|hstart
test ,θ∗)p(hstart
test |Sn,θ∗) + n
X"
REFERENCES,0.5855018587360595,"i=1
log P"
REFERENCES,0.587360594795539,"hdelim
i−1∈Dp(Oi|hdelim
i−1 ,θ)p(hdelim
i−1 |Oex
1:i−1,θ)
P"
REFERENCES,0.5892193308550185,"hdelim
i−1∈Dp(Oi|hdelim
i−1 ,θ∗)p(hdelim
i−1 |Oex
1:i−1,θ∗)  (67) ≤1 n 
log P"
REFERENCES,0.5910780669144982,"hstart
test 1·p(hstart
test |Sn,θ)
P"
REFERENCES,0.5929368029739777,"hstart
test c7·p(hstart
test |Sn,θ∗) +n(log(c2)−log(c1))+ n
X"
REFERENCES,0.5947955390334573,"i=1
log P"
REFERENCES,0.5966542750929368,"hdelim
i−1∈Dp(Oi|hdelim
i−1 ,θ)
P"
REFERENCES,0.5985130111524164,"hdelim
i−1∈Dp(Oi|hdelim
i−1 ,θ∗)  (68) = 1 n"
REFERENCES,0.6003717472118959,"
−log(c7)+n(log(c2)−log(c1))+ n
X"
REFERENCES,0.6022304832713755,"i=1
log P"
REFERENCES,0.604089219330855,"hdelim
i−1∈Dp(Oi|hdelim
i−1 ,θ)
P"
REFERENCES,0.6059479553903345,"hdelim
i−1∈Dp(Oi|hdelim
i−1 ,θ∗)"
REFERENCES,0.6078066914498141,"
(69)"
REFERENCES,0.6096654275092936,"In the above steps, we used both Propositions 1 and 2 in the terms involving c2,c1 (bounding the
probability of hdelim hidden states) and c7 (bounding the probability of xtest). Note that in the second
line, the sum can must be over the set of delimiter states D by using the assumption that observing
a delimiter odelim implies that the corresponding hidden state hdelim must be in D."
REFERENCES,0.6115241635687733,Published as a conference paper at ICLR 2022
REFERENCES,0.6133828996282528,"Focusing on the numerator of the ratio term and summing over the start hidden state for the i-th example,
X"
REFERENCES,0.6152416356877324,"hdelim
i−1∈D
p(Oi|hdelim
i−1 ,θ)=
X"
REFERENCES,0.6171003717472119,"hdelim
i−1∈D X"
REFERENCES,0.6189591078066915,"hstart
i
p(Oi|hstart
i
,θ)p(hstart
i
|hdelim
i−1 ,θ))
(70) =
X"
REFERENCES,0.620817843866171,"hstart
i
p(Oi|hstart
i
,θ)p(hstart
i
|θ)
X"
REFERENCES,0.6226765799256505,"hdelim
i−1∈D"
REFERENCES,0.6245353159851301,"p(hstart
i
|hdelim
i−1 ,θ)
p(hstart
i
|θ)
(71) =
X"
REFERENCES,0.6263940520446096,"hstart
i
p(Oi|hstart
i
,θ)p(hstart
i
|θ)
X"
REFERENCES,0.6282527881040892,"hdelim
i−1∈D"
REFERENCES,0.6301115241635687,"p(hdelim
i−1 |hstart
i
,θ)
p(hdelim
i−1 |θ)
(72)"
REFERENCES,0.6319702602230484,"where the last step applies Bayes’ rule. We can lower and upper bound the following quantity for any
θ using Assumption 2:"
REFERENCES,0.6338289962825279,"p(hdelim
i−1 |hstart
i
,θ)
p(hdelim
i−1 |θ)
≤p(hdelim
i−1 |hstart
i
,θ)
c3
(73)"
REFERENCES,0.6356877323420075,"p(hdelim
i−1 |hstart
i
,θ)
p(hdelim
i−1 |θ)
≥p(hdelim
i−1 |hstart
i
,θ)
c4
.
(74)"
REFERENCES,0.637546468401487,"This implies that
X"
REFERENCES,0.6394052044609665,"hdelim
i−1∈D"
REFERENCES,0.6412639405204461,"p(hdelim
i−1 |hstart
i
,θ)
p(hdelim
i−1 |θ)
≤1"
REFERENCES,0.6431226765799256,"c3
(75) X"
REFERENCES,0.6449814126394052,"hdelim
i−1∈D"
REFERENCES,0.6468401486988847,"p(hdelim
i−1 |hstart
i
,θ)
p(hdelim
i−1 |θ)
≥1"
REFERENCES,0.6486988847583643,"c4
.
(76)"
REFERENCES,0.6505576208178439,"Plugging in these bounds, we have"
REFERENCES,0.6524163568773235,rn(θ)≤1 n
REFERENCES,0.654275092936803,"
−log(c7)+2n(log(c2)−log(c1))+n(log(c4)−log(c3))+ n
X"
REFERENCES,0.6561338289962825,"i=1
log P"
REFERENCES,0.6579925650557621,"hstart
i p(Oi|hstart
i
,θ)p(hstart
i
|θ)
P
hstart
i p(Oi|hstart
i
,θ)p(hstart
i
|θ∗)  (77) = 1 n"
REFERENCES,0.6598513011152416,"
−log(c7)+2n(log(c2)−log(c1))+n(log(c4)−log(c3))+ n
X"
REFERENCES,0.6617100371747212,"i=1
log p(Oi|θ)"
REFERENCES,0.6635687732342007,p(Oi|θ∗)
REFERENCES,0.6654275092936803,"
(78)"
REFERENCES,0.6672862453531598,→n→∞EO∼pprompt
REFERENCES,0.6691449814126395,"
log p(O|θ)"
REFERENCES,0.671003717472119,p(O|θ∗)
REFERENCES,0.6728624535315985,"
+ϵθ
delim
(79)"
REFERENCES,0.6747211895910781,where we set
REFERENCES,0.6765799256505576,"ϵθ
delim =2(log(c2)−log(c1))+log(c4)−log(c3).
(80)"
REFERENCES,0.6784386617100372,"Next, we convert the expectation in the bound into a KL divergence. We have"
REFERENCES,0.6802973977695167,EO∼pprompt
REFERENCES,0.6821561338289963,"
log p(O|θ)"
REFERENCES,0.6840148698884758,p(O|θ∗)
REFERENCES,0.6858736059479554,"
=EO∼pprompt"
REFERENCES,0.6877323420074349,"
log p(O|θ)"
REFERENCES,0.6895910780669146,pprompt(O) +logpprompt(O)
REFERENCES,0.6914498141263941,p(O|θ∗)
REFERENCES,0.6933085501858736,"
(81)"
REFERENCES,0.6951672862453532,"=KL(pprompt∥p(·|θ∗))−KL(pprompt∥p(·|θ)).
(82)"
REFERENCES,0.6970260223048327,We will upper bound the ﬁrst KL term:
REFERENCES,0.6988847583643123,KL(pprompt∥p(·|θ∗))=EO∼pprompt
REFERENCES,0.7007434944237918,"
logpprompt(O)"
REFERENCES,0.7026022304832714,p(O|θ∗)
REFERENCES,0.7044609665427509,"
.
(83)"
REFERENCES,0.7063197026022305,"Expanding the numerator and denominator of the ratio inside, we have"
REFERENCES,0.70817843866171,"pprompt(O)=
X"
REFERENCES,0.7100371747211895,"H
pprompt(H[1])p(O[1]|H[1],θ∗) k
Y"
REFERENCES,0.7118959107806692,"j=2
p(O[j]|H[j],θ∗)p(H[j]|H[j−1],θ∗)
(84)"
REFERENCES,0.7137546468401487,"p(O|θ∗)=
X"
REFERENCES,0.7156133828996283,"H
p(H[1]|θ∗)p(O[1]|H[1],θ∗) k
Y"
REFERENCES,0.7174721189591078,"j=2
p(O[j]|H[j],θ∗)p(H[j]|H[j−1],θ∗)
(85)"
REFERENCES,0.7193308550185874,Published as a conference paper at ICLR 2022
REFERENCES,0.7211895910780669,"which differ in only the hidden start distribution. Using Assumption 5, we have that p(h|θ∗)≥c8 for
any h∈H, which implies that"
REFERENCES,0.7230483271375465,pprompt(h)
REFERENCES,0.724907063197026,p(h|θ∗) ≤1
REFERENCES,0.7267657992565055,"c8
(86)"
REFERENCES,0.7286245353159851,=⇒pprompt(O)≤1
REFERENCES,0.7304832713754646,"c8
p(O|θ∗).
(87)"
REFERENCES,0.7323420074349443,"Finally, this implies that the KL term is bounded as"
REFERENCES,0.7342007434944238,"KL(pprompt∥p(·|θ∗))≤−log(c8).
(88)"
REFERENCES,0.7360594795539034,This term is non-negative since c8 ≤1.
REFERENCES,0.7379182156133829,"Aiming to decompose the second KL term into a sum over the k tokens,
we write
pj
θ(o)=p(O[j]=o|O[1:j−1],θ) and pj
prompt(o)=pprompt(O[j]=o|O[1:j−1]). We have"
REFERENCES,0.7397769516728625,"−KL(pprompt∥p(·|θ))=−
X"
REFERENCES,0.741635687732342,"O
pprompt(O)logpprompt(O)"
REFERENCES,0.7434944237918215,"p(O|θ)
(89) =−
X"
REFERENCES,0.7453531598513011,"O
pprompt(O) k
X"
REFERENCES,0.7472118959107806,"j=1
logpprompt(O[j]|O[1:j−1]))"
REFERENCES,0.7490706319702602,"p(O[j]|O[1:j−1],θ)
(90) =− k
X j=1 X"
REFERENCES,0.7509293680297398,"O
pprompt(O)logpprompt(O[j]|O[1:j−1]))"
REFERENCES,0.7527881040892194,"p(O[j]|O[1:j−1],θ)
(91) =− k
X"
REFERENCES,0.7546468401486989,"j=1
EO[1:j−1]∼pprompt
h
KL(pj
prompt∥pj
θ)
i
(92)"
REFERENCES,0.7565055762081785,Then we have that
REFERENCES,0.758364312267658,"lim
n→∞rn(θ)<− k
X"
REFERENCES,0.7602230483271375,"j=1
EO[1:j−1]∼pprompt[KL(pj
prompt∥pj
θ)]+ϵθ
start+ϵθ
delim
(93)"
REFERENCES,0.7620817843866171,"The second term (set ϵθ
start =log( 1"
REFERENCES,0.7639405204460966,"c8 )) is an error term that depends on how different the starting prompt
distribution pprompt (which is part of pprompt) is to the pretraining distribution. The third term is an error
term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is
larger in magnitude than the error terms. Note that as k becomes larger, the number of observations
of θ∗“overpowers” the distracting transitions in the prompt distribution. This condition is equivalent
to the disinguishability condition (Condition 1)."
REFERENCES,0.7657992565055762,"By assumption, for θ̸=θ∗the Condition 1 holds, and thus"
REFERENCES,0.7676579925650557,"lim
n→∞
p(Sn,xtest|θ)
p(Sn,xtest|θ∗) = lim
n→∞exp(n·rn(θ))=0
(94)"
REFERENCES,0.7695167286245354,"since rn(θ) has a negative, constant limit. Note that exp(n·rn(θ∗))=1 for θ∗."
REFERENCES,0.7713754646840149,"E
NON-DISTINGUISHABLE CASE"
REFERENCES,0.7732342007434945,"When Condition 1 is unsatisﬁed, Equation 14), gives an upper bound on the sum of KL divergences
for the next token distributions given different-length histories. In contrast, the in-context task only
measures the accuracy of the last (k-th) token. The main challenge is to relate the different-length
histories to each other to give a more precise bound for the error on the in-context task (last token)."
REFERENCES,0.775092936802974,"Before addressing this challenge, we give the following lemma, which leverages the result of ´Avila
Pires & Szepesv´ari (2016); Steinwart (2007) to relate a bound on the KL divergence to 0-1 loss."
REFERENCES,0.7769516728624535,Published as a conference paper at ICLR 2022
REFERENCES,0.7788104089219331,"Lemma 2. Let the set of θ which does not satisfy Condition 1 to be B.
Assume that
KL(pprompt(ytest|xtest)∥p(ytest|xtest, θ) is bounded above for all θ and that θ∗minimizes the
multiclass logistic risk LCE(θ)=−Extest∼pprompt[pprompt(ytest|xtest)logp(ytest|xtest,θ)]. If"
REFERENCES,0.7806691449814126,"Extest∼pprompt[KL(pprompt(ytest|xtest)∥p(ytest|xtest,θ))]≤ϵθ for all θ∈B,
(95) then"
REFERENCES,0.7825278810408922,"lim
n→∞L0-1(fn)≤inf
f L0-1(f)+g−1

sup
θ∈B
ϵθ"
REFERENCES,0.7843866171003717,"
(96) where"
REFERENCES,0.7862453531598513,g(δ)= 1
REFERENCES,0.7881040892193308,"2((1−δ)log(1−δ)+(1+δ)log(1+δ))
(97)"
REFERENCES,0.7899628252788105,"is a calibration function for the multiclass logistic loss for δ∈[0,1]."
REFERENCES,0.79182156133829,"Proof. First, we note that we can study the 0-1 risk of the limiting predictor:"
REFERENCES,0.7936802973977695,"lim
n→∞L0-1(fn)= lim
n→∞Extest,ytest∼pprompt[1[fn(xtest)̸=ytest]]
(98)"
REFERENCES,0.7955390334572491,"=Extest,ytest∼pprompt[ lim
n→∞1[fn(xtest)̸=ytest]] (dominated convergence, boundedness of indicator) (99)"
REFERENCES,0.7973977695167286,"=Extest,ytest∼pprompt[1[ lim
n→∞fn(xtest)̸=ytest]]
(100)"
REFERENCES,0.7992565055762082,"where in the last step we use that since the output space of fn is discrete and the probabili-
ties that the in-context predictor takes an argmax over converges, then for N large enough,
fN(xtest)=limn→∞fn(xtest)."
REFERENCES,0.8011152416356877,"Note that for every input xtest, the limiting in-context learning predictor outputs the argmax of a
predictive distribution which can be a mixture of predictive distributions over B:"
REFERENCES,0.8029739776951673,"lim
n→∞fn(xtest)=argmax
y
Eθ∼q[p(y|xtest,θ)]
(101)"
REFERENCES,0.8048327137546468,"for some distribution q over B. The KL divergence between this mixture and the prompt concept is
bounded by the KL divergence of any one θ∈B, due to the convexity of KL:"
REFERENCES,0.8066914498141264,"Extest∼pprompt[KL(pprompt(y|xtest)∥Eθ∼q[p(y|xtest,θ)]]
(102)"
REFERENCES,0.8085501858736059,"≤Extest∼pprompt[Eθ∼q[KL(pprompt(y|xtest)∥p(y|xtest,θ))]]
(103)"
REFERENCES,0.8104089219330854,"=Eθ∼q[Extest∼pprompt[KL(pprompt(y|xtest)∥p(y|xtest,θ))]]
(104)"
REFERENCES,0.8122676579925651,"≤sup
θ∈B
Extest∼pprompt[KL(pprompt(y|xtest)∥p(y|xtest,θ))]
(105)"
REFERENCES,0.8141263940520446,where we can exchange the order of expectations since the KL is bounded (dominated convergence).
REFERENCES,0.8159851301115242,"From the KL bound KL(pprompt(ytest|xtest)∥p(ytest|xtest,θ), we thus have"
REFERENCES,0.8178438661710037,"Extest∼pprompt[KL(pprompt(ytest|xtest)∥p(ytest|xtest,θ))]=LCE(θ)−LCE(θ∗)≤sup
θ∈B
ϵθ
(106)"
REFERENCES,0.8197026022304833,"where LCE(θ) = −Extest∼pprompt[pprompt(ytest|xtest)logp(ytest|xtest,θ)] is the multiclass logistic risk, and
LCE(θ∗) is the optimal risk over θ ∈Θ by assumption. Applying Theorem 2.2 and 5.11 of ´Avila
Pires & Szepesv´ari (2016), g is a calibration function for the multiclass logistic loss, and allows us
to convert the surrogate risk bound to a bound on the 0-1 loss, giving the result. Note that we have
zero approximation error here, since θ∗∈Θ."
REFERENCES,0.8215613382899628,"Note that g−1 is roughly linear in ϵ for ϵ smaller than 0.7, where the bound is non-vacuous."
REFERENCES,0.8234200743494424,Published as a conference paper at ICLR 2022
REFERENCES,0.8252788104089219,"E.1
PROOF OF THEOREM 2"
REFERENCES,0.8271375464684015,"Proof. By the continuity assumption, we have for any θ in B that k
X"
REFERENCES,0.828996282527881,"j=2
KLj(θ∗∥θ)≥1 2 k
X"
REFERENCES,0.8308550185873605,"j=2
(θ−θ∗)⊤Ij,θ∗(θ−θ∗)+(k−1)O(∥θ−θ∗∥3)
(107) ≥1"
REFERENCES,0.8327137546468402,"2(k−1)λmin(Ij,θ∗)∥θ−θ∗∥2
(108)"
REFERENCES,0.8345724907063197,"=⇒∥θ−θ∗∥2 ≤
ϵθ
start+ϵθ
delim
1
2(k−1)(minj λmin(Ij,θ∗)).
(109)"
REFERENCES,0.8364312267657993,We use this to bound the last KL term by plugging it in below:
REFERENCES,0.8382899628252788,KLk(θ∗∥θ)= 1
REFERENCES,0.8401486988847584,"2(θ−θ∗)⊤Ij,θ∗(θ−θ∗)+O(∥θ−θ∗∥3)
(110) ≤1"
REFERENCES,0.8420074349442379,"2(max
j
λmax(Ij,θ∗))∥θ−θ∗∥2+O(∥θ−θ∗∥2)
(111)"
REFERENCES,0.8438661710037175,"≤(ϵθ
start+ϵθ
delim)(maxj λmax(Ij,θ∗)+O(1))
(k−1)minj λmin(Ij,θ∗)
.
(112)"
REFERENCES,0.845724907063197,"Rearranging and noting that KLk(θ∗∥θ) = Extest∼pprompt[KL(pprompt(ytest|xtest)∥p(ytest|xtest,θ))], we
have"
REFERENCES,0.8475836431226765,"Extest∼pprompt[KL(pprompt(ytest|xtest)∥p(ytest|xtest,θ))]≤(ϵθ
start+ϵθ
delim)(maxj λmax(Ij,θ∗)+O(1))
(k−1)minj λmin(Ij,θ∗)
(113)"
REFERENCES,0.8494423791821561,Plugging into Lemma 2 gives the result.
REFERENCES,0.8513011152416357,"E.2
PROOF OF THEOREM 3
Note that Condition 1 ensures that the sum of KL divergences between positions within a k-length
input is bounded. This means that we have a bound over not only the last-position KL divergence, but
also for all the intermediate tokens. Intuitively, the random length test example allows the in-context
predictor to “take credit” for ﬁtting the intermediate tokens. The proof is immediate given the KL
bound and Lemma 2, given that the length of xtest is uniformly random between 2 to k."
REFERENCES,0.8531598513011153,Proof. Let the set of θ that does not satisfy Condition 1 to be B. We have for any θ in B that
REFERENCES,0.8550185873605948,"Extest∼pprompt[KL(pprompt(ytest|xtest)∥p(ytest|xtest,θ))]
(114)"
REFERENCES,0.8568773234200744,"≤
1
k−1 k
X"
REFERENCES,0.8587360594795539,"j=2
EO[1:j−1]∼ppromptKL(pprompt(O[j]|O[1:j−1])∥p(O[j]|O[1:j−1],θ)) (115)"
REFERENCES,0.8605947955390335,"≤supθ(ϵθ
start+ϵθ
delim)
k−1
(116)"
REFERENCES,0.862453531598513,by Theorem 1 and Condition 1. Plugging this into Lemma 2 gives the result.
REFERENCES,0.8643122676579925,"F
EXPERIMENTAL DETAILS"
REFERENCES,0.8661710037174721,"F.1
GINC DATASET
Pretraining distribution.
We consider a pretraining distribution from a mixture of HMMs with an
interpretablehiddenstatestructureandemissiondistribution. TheHMMhiddenstateht =[st,vt]attime
t is composed of an entity vt ∈{1,...,|V|} (e.g., Einstein) and a property st ∈{1,...,|S|} (e.g., nationality,
ﬁrst name, last name, other grammatical tokens). We model the entities and properties as independent
Markov chains (i.e., a factorial HMM (Ghahramani & Jordan, 1997)), while the emissions depend on
both. In pretraining documents, we expect that the entities (e.g., Einstein) change slowly over time while
and the properties of the entity (e.g., their nationality) change quickly with some pattern to generate"
REFERENCES,0.8680297397769516,Published as a conference paper at ICLR 2022
REFERENCES,0.8698884758364313,"Figure 8: Example pretraining document snippet (Left) and example prompt with 3 training examples,
1 test example, and example length 3 (Right). The delimiter token is the backslash."
REFERENCES,0.8717472118959108,"Figure 9: The GINC dataset generates sequences from a mixture of HMMs. The HMM hidden states
consist of entities (v) and properties (s), which index into a memory matrix to produce the observed
token. The entity and property sequences are sampled from independent Markov chains. The concept
parameter θ is the transition matrix for properties, which deﬁnes relations between properties. In this
example, the sequence of properties [2,3,5,4] relates names to nationalities, deﬁning the in-context
task. The blue color represents hidden states/observations sampled from the prompt distribution, and
the purple color represents hidden states/observations sampled from the pretraining distribution."
REFERENCES,0.8736059479553904,"natural sentences. We implement this by ensuring that the probability of transitioning to the same entity
index in the next step is at least 0.9. The emission distribution depends on a memory matrix M with |V|
rows and |S| columns (Figure 9). At step t, we use the entity vt and property st to index into the memory
matrix. In particular, the observed tokens are deterministic with p(ot|ht)=1 if ot =M[vt,st]. This
construction satisﬁes the structure on delimiter states (Assumption 1). We ensure that all the transitions
have nonzero probability and use a uniform prior over concepts, satisfying Assumptions 2 and 5."
REFERENCES,0.8754646840148699,"Concept parameter.
The concept parameter is the property transition matrix, while the entity
transition matrix is ﬁxed for all concepts. The prompt start distribution and the concept together
determine the in-context task. We deﬁne a uniform mixture of HMMs over a family Θ of 5 concepts
to generate 1000 documents with ∼10 million tokens total."
REFERENCES,0.8773234200743495,"Vocabulary.
The GINC dataset is generated from a mixture of HMMs. These HMMs output tokens
fromavocabularyofsizein{50,100,150}. Thevocabularycontainsaspecialdelimitertoken(backslash"
REFERENCES,0.879182156133829,Published as a conference paper at ICLR 2022
REFERENCES,0.8810408921933085,"0
20
40
60
Num examples 20 25 30 35 Acc"
REFERENCES,0.8828996282527881,"k=3
k=5
k=8
k=10"
REFERENCES,0.8847583643122676,"0
20
40
60
Num examples 30 40 50 60 Acc"
REFERENCES,0.8866171003717472,"k=3
k=5
k=8
k=10"
REFERENCES,0.8884758364312267,"0
20
40
60
Num examples 40 50 60 70 80 90 Acc"
REFERENCES,0.8903345724907064,"k=3
k=5
k=8
k=10"
REFERENCES,0.8921933085501859,"Figure 10: In-context accuracy curve of the 4 layer Transformer on the GINC dataset when the entity
transition matrix does not have an additional identity component, for vocabulary sizes 50 (left), 100
(middle), and 150 (right). In-context learning is still generally successful."
REFERENCES,0.8940520446096655,"– see Figure 8, designated to be index 1. The vocabulary is generated as combinations of letters starting
from a to z, then aa to az, and so on. All sequences are tokenized by splitting on whitespaces."
REFERENCES,0.895910780669145,"Memory matrix.
The shared memory matrix has 10 entities and 10 properties, totaling 100 entries
(corresponding to 100 hidden states). The ﬁrst column of the memory matrix is ﬁxed to be the delimiter
token, while each remaining entry of the shared memory matrix is populated with a token sampled
uniformly from the vocabulary."
REFERENCES,0.8977695167286245,"Transition matrix for properties.
We generate 5 property transition matrices, one for each
component of the HMM mixture. We generate each transition matrix via a convex combination of
100 random permutation matrices. The weights of the convex combination are randomly generated as"
REFERENCES,0.8996282527881041,"softmax((u−0.5)/t)
(117)"
REFERENCES,0.9014869888475836,"where u∈R100 has uniform random entries in [0,1] and t is a temperature parameter, set to 0.1."
REFERENCES,0.9033457249070632,"Transition matrix for entities.
The entity transition matrix is shared between all the HMMs that
consistute the mixture. The entity transition matrix is generated in the same way as the property
transition matrices, except with one additional step. Letting T be a transition matrix sampled in the
same way as a property transition matrix,"
REFERENCES,0.9052044609665427,"In pretraining documents, we expect that the entities (e.g., Einstein) change slowly over time while
and the properties of the entity (e.g., their occupation) change quickly with some pattern to generate
natural sentences. We implement this by ensuring that the probability of transitioning to the same entity
index in the next step is at least 0.9. The ﬁnal entity transition matrix is then 0.1T +0.9I where I is
the identity matrix. Although we add the diagonal component for added realism, we also consider not
adding this component. Figure 10 shows in-context learning curves for a small (4 layer) Transformer
trained on data that does not add the diagonal component (we check this for vocabulary sizes 50, 100,
and 150). In-context learning still works in this case, although not as well for the 50 vocab size case."
REFERENCES,0.9070631970260223,"Start distribution.
The starting distribution for the hidden states in all HMMs in the mixture are
close to uniform. We generate the start distribution as softmax((u−0.5)/t) for random vector u with
entries uniformly from [0,1] and temperature t=10. In the pretraining documents, we only sample
from the start distribution in the beginning of the document."
REFERENCES,0.9089219330855018,"Prompt distribution.
To generate the prompts, we ﬁrst sample a concept θ uniformly at random
from Θ (well-speciﬁcation, Assumption 4), then use it to generate all the prompt examples. The prompt
start distribution is chosen to be uniform over entities but with a ﬁxed starting property that is chosen
randomly for each prompt, for consistency in the task. This may not satisfy Assumption 3, but we
found this to still work empirically and is simpler. Given the starting property, we sample k tokens from
the HMM deﬁned by the concept θ. Finally, we append the delimiter token for the example. We repeat
this process for each example in the prompt, concatenating all examples. The label is generated as"
REFERENCES,0.9107806691449815,"argmax
y
pprompt(y|xtest)
(118)"
REFERENCES,0.912639405204461,"under the prompt concept θ∗. This differs from the theory, which samples ytest instead of taking it to be
the most likely token. However, there can be a large amount of intrinsic error that sampling introduces.
We deﬁne the label this way in the simulations to remove the intrinsic error from sampling."
REFERENCES,0.9144981412639405,Published as a conference paper at ICLR 2022
REFERENCES,0.9163568773234201,"Example of prompt generation.
In the example in Figure 8 (right), the starting property is ﬁxed to be
5 (for example). The ﬁrst token (l) is generated by sampling a random entity index (3), and indexing into
the memory matrix returns l. Running the hidden state chain of the HMM forward gives the next pair
of property and entity. Since the entity Markov chain changes slowly, the entity is still 3 in the next step
– however, the property has changed to 4, and indexing into the memory matrix outputs the next token
(aw). Following this same process to generate the third token (the output for the ﬁrst example), we ﬁnish
generatingone example. Toendthe example, we appenda delimiter(backslash). Werepeat thisexample
generation process for all the examples, except for the test example at the end, where we do not generate
the last token. We condition the HMM on the generated prompt to compute the posterior distribution
over the next token pprompt(y|xtest). We take the argmax of this distribution to be the ground truth label."
REFERENCES,0.9182156133828996,"Dataset details.
The dataset contains 1000 training documents and 100 validation documents, where
training documents have 10240 tokens and validation documents have 1024 tokens. Each document
is generated by ﬁrst selecting one of the HMMs from the mixture uniformly at random, then generating
10240 tokens from the HMM."
REFERENCES,0.9200743494423792,"We also generate 2500 in-context prompts for each (example length,number of examples) pair, for
example lengths k = [3,5,8,10] and number of examples n = [0,1,2,4,8,16,32,64]. Each prompt is
generated using a random HMM in the mixture."
REFERENCES,0.9219330855018587,"F.2
TRANSFORMER DETAILS
Our Transformer models are based on the GPT-2 architectures with 4, 12, and 16 layers respectively,
with 12 attention heads, 768 dimensional embeddings, residual/embedding/attention dropout set to 0.1,
and a context window of 1024. Other than the number of layers, the other parameters are the default
settings from the HuggingFace library (Wolf et al., 2019). We train for 5 epochs using the AdamW
optimizer (Loshchilov & Hutter, 2019; Kingma & Ba, 2015) with a batch size of 8 and a linear learning
rate schedule (with 1000 step warmup) up to a learning rate of 8e-4 for the 4 layer and 12 layer model,
while for the 16 layer model we start with a constant learning rate of 8e-4 and reduce by a factor of
0.25 whenever the best validation loss does not improve. We tried both learning rate strategies for
all models and take the most consistent. We tuned these models so that the training loss curves between
seeds have smaller variability between the runs in terms of the curve shape and when the loss decreases
– we found that this is an important indication of stable results. The models took 50 minutes, 2 hours,
3 hours to train respectively. The hardware was mainly Titan Xp GPUs, trained and evaluated using
16-bit precision. All the results are reported with 5 pretraining runs (5 different seeds)."
REFERENCES,0.9237918215613383,"F.3
LSTM DETAILS
We train an LSTM language model with embedding size 768, hidden layer size 768, and 6 layers. We
use dropout 0.2 and weight decay 1e-5. The optimizer is AdamW starting with a learning rate of 1e-3,
then reducing by a factor of 0.25 whenever the best validation loss does not go down. We train for
a total of 10 epochs, with gradient clipping at norm 1.0. We use a batch size of 8 and backpropagate
through time for 1024 steps (each pretraining data segment is also 1024 tokens). Each model takes
roughly 2 hours to train on Titan Xp GPUs."
REFERENCES,0.9256505576208178,"F.4
VARYING THE VOCABULARY SIZE
To do well on the in-context learning task, the model must both infer the prompt concept and the last
HMM hidden state. In general, increasing the number of observable symbols makes the in-context task
easier by making the inference of the HMM hidden state easier. With more symbols, each hidden state
is more likely to output a different symbol, making the inference problem easier. This improvement
comes despite the number of output classes in the problem (same as the vocabulary size) increasing.
Figures 11, 12, 13, 14 show in-context learning curves for vocabulary sizes 50, 100, and 150, keeping
other hyperparmeters of the dataset the same."
REFERENCES,0.9275092936802974,"F.5
EXPERIMENT ON GPT-3
We conduct an additional experiment which shows that longer examples improve in-context learning
in GPT-3 on the LAMBADA (Paperno et al., 2016) completion task."
REFERENCES,0.929368029739777,"Data.
In this experiment, we deﬁne a short version of the LAMBADA test dataset (LAMBADA
test-short) which contains only test examples with up to 200–300 characters in length. We also
deﬁne two “training” datasets from which to sample examples for the in-context prompts from.
The short training dataset (LAMBADA train-short) contains examples from the training set that are"
REFERENCES,0.9312267657992565,Published as a conference paper at ICLR 2022
REFERENCES,0.9330855018587361,"0
20
40
60
Num examples 30 40 50 60 Acc"
REFERENCES,0.9349442379182156,"k=3
k=5
k=8
k=10"
REFERENCES,0.9368029739776952,"0
20
40
60
Num examples 40 50 60 70 Acc"
REFERENCES,0.9386617100371747,"k=3
k=5
k=8
k=10"
REFERENCES,0.9405204460966543,"0
20
40
60
Num examples 60 70 80 90 Acc"
REFERENCES,0.9423791821561338,"k=3
k=5
k=8
k=10"
REFERENCES,0.9442379182156134,"Figure 11: In-context accuracy of the 4 layer Transformer on the GINC dataset for vocabulary sizes
50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases."
REFERENCES,0.9460966542750929,"0
20
40
60
Num examples 40 60 80 Acc"
REFERENCES,0.9479553903345725,"k=3
k=5
k=8
k=10"
REFERENCES,0.949814126394052,"0
20
40
60
Num examples 40 50 60 70 80 90 Acc"
REFERENCES,0.9516728624535316,"k=3
k=5
k=8
k=10"
REFERENCES,0.9535315985130112,"0
20
40
60
Num examples 60 70 80 90 100 Acc"
REFERENCES,0.9553903345724907,"k=3
k=5
k=8
k=10"
REFERENCES,0.9572490706319703,"Figure 12: In-context accuracy of the 12 layer Transformer on the GINC dataset for vocabulary sizes
50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases."
REFERENCES,0.9591078066914498,"0
20
40
60
Num examples 40 50 60 70 80 90 Acc"
REFERENCES,0.9609665427509294,"k=3
k=5
k=8
k=10"
REFERENCES,0.9628252788104089,"0
20
40
60
Num examples 40 50 60 70 80 90 Acc"
REFERENCES,0.9646840148698885,"k=3
k=5
k=8
k=10"
REFERENCES,0.966542750929368,"0
20
40
60
Num examples 60 70 80 90 100 Acc"
REFERENCES,0.9684014869888475,"k=3
k=5
k=8
k=10"
REFERENCES,0.9702602230483272,"Figure 13: In-context accuracy of the 16 layer Transformer on the GINC dataset for vocabulary sizes
50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases."
REFERENCES,0.9721189591078067,"0
20
40
60
Num examples 40 60 80 100 Acc"
REFERENCES,0.9739776951672863,"k=3
k=5
k=8
k=10"
REFERENCES,0.9758364312267658,"0
20
40
60
Num examples 40 60 80 100 Acc"
REFERENCES,0.9776951672862454,"k=3
k=5
k=8
k=10"
REFERENCES,0.9795539033457249,"0
20
40
60
Num examples 40 60 80 100 Acc"
REFERENCES,0.9814126394052045,"k=3
k=5
k=8
k=10"
REFERENCES,0.983271375464684,"Figure 14: In-context accuracy of the LSTM on the GINC dataset for vocabulary sizes 50 (left), 100
(middle) and 150 (right). Accuracies generally improve as the vocabulary size increases."
REFERENCES,0.9851301115241635,"200–300 characters in length, which matches the distribution of test-short. The long training dataset
(LAMBADA train-long) contains training examples that are 500–600 characters long. We cut the
number of examples in the larger of the two training datasets so that the two training datasets are equally
sized (47 examples). For each test example, we sample 5 random training examples (5-shot learning)."
REFERENCES,0.9869888475836431,"We also consider equalizing the total length of the prompts in two ways. First, we consider duplicating
the 5 short examples (if the examples are [1,2,3,4,5], duplicating refers to [1,2,3,4,5,1,2,3,4,5]).
This allows for equalizing the total length without increasing the number of examples. As a skyline
comparison, we also consider sampling 10 independent short examples, which contains more
input-output pairs for the task."
REFERENCES,0.9888475836431226,"Result.
Table 1 shows that when evaluating only on LAMBADA test-short, 5-shot in-context
learning using LAMBADA train-long improves the test accuracy by almost 1% compared to
LAMBADA train-short, despite the long/short distribution mismatch between train and test. This
supports intuitions from our theory."
REFERENCES,0.9907063197026023,Published as a conference paper at ICLR 2022
REFERENCES,0.9925650557620818,"Prompt example length
Test Acc (200–300 chars)"
EXAMPLES,0.9944237918215614,"5 examples
Short (200–300 chars)
69.8
Long (500–600 chars)
70.7
10 examples
Short, duplicated examples
69.6
Short, independent examples
71.4"
EXAMPLES,0.9962825278810409,"Table 1: Accuracies for 5-shot in-context learning of GPT-3 on a ﬁltered LAMBADA test set with short
examples (200–300 characters). Even though there is distribution mismatch with the test set, having
longer examples improves the accuracy, supporting theoretical intuitions. The ﬁrst two rows use 5 train-
ing examples in the prompt, while the last two rows use 10 training examples to equalize the total length."
EXAMPLES,0.9981412639405205,"In comparison, simply increasing the total prompt length by duplicating the short examples does not
improve the accuracy. Intuitively, the longer examples have additional information that is not directly
related to mapping between the input and output, but can be leveraged to improve in-context learning
by helping the model infer the latent concept. Using 5 long examples (as opposed to 5 short examples)
closes about 56% of the gap between using 5 short examples and 10 independent short examples
despite not adding additional examples or task-related information."
