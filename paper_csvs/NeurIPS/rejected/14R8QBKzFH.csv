Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002183406113537118,"We consider the formulation of ""machine unlearning"" of Sekhari, Acharya, Kamath,
1"
ABSTRACT,0.004366812227074236,"and Suresh (NeurIPS 2021), which formalizes the so-called ""right to be forgotten""
2"
ABSTRACT,0.006550218340611353,"by requiring that a trained model, upon request, should be able to ’unlearn’ a
3"
ABSTRACT,0.008733624454148471,"number of points from the training data, as if they had never been included in
4"
ABSTRACT,0.010917030567685589,"the first place. Sekhari et al. established some positive and negative results
5"
ABSTRACT,0.013100436681222707,"about the number of data points that can be successfully unlearnt by a trained
6"
ABSTRACT,0.015283842794759825,"model without impacting the model’s accuracy (the ""deletion capacity""), showing
7"
ABSTRACT,0.017467248908296942,"that machine unlearning could be achieved by using differentially private (DP)
8"
ABSTRACT,0.019650655021834062,"algorithms. However, their results left open a gap between upper and lower
9"
ABSTRACT,0.021834061135371178,"bounds on the deletion capacity of these algorithms: our work fully closes this gap,
10"
ABSTRACT,0.024017467248908297,"obtaining tight bounds on the deletion capacity achievable by DP-based machine
11"
ABSTRACT,0.026200873362445413,"unlearning algorithms.
12"
INTRODUCTION,0.028384279475982533,"1
Introduction
13"
INTRODUCTION,0.03056768558951965,"Machine learning models trained on user data are now routinely used virtually everywhere, from
14"
INTRODUCTION,0.03275109170305677,"recommendation systems to predictive models. In many cases, this user data itself includes some
15"
INTRODUCTION,0.034934497816593885,"sensitive information (e.g., healthcare or race) or private aspects (customer habits, geographic data),
16"
INTRODUCTION,0.03711790393013101,"sometimes even protected by law. To address this issue – that the models trained on sensitive datasets
17"
INTRODUCTION,0.039301310043668124,"must not leak personal or private information – in a principled fashion, one of the leading frameworks
18"
INTRODUCTION,0.04148471615720524,"is that of differential privacy (DP) [Dwork et al., 2006], which has de facto become the standard for
19"
INTRODUCTION,0.043668122270742356,"privacy-preserving machine learning over the past decade.
20"
INTRODUCTION,0.04585152838427948,"At its core, DP requires that the output of a randomized algorithm M not change drastically if one to
21"
INTRODUCTION,0.048034934497816595,"modify one of the datapoints: that is, if X, X′ are two datasets only differing in one user’s data, then
22"
INTRODUCTION,0.05021834061135371,"for all possible outputs S of the algorithm one should have roughly the same probability of observing
23"
INTRODUCTION,0.05240174672489083,"S under both inputs:
24"
INTRODUCTION,0.05458515283842795,Pr[ M(X) ∈S ] ≤eε Pr[ M(X′) ∈S ] + δ
INTRODUCTION,0.056768558951965066,"where ε > 0 and δ ∈(0, 1] quantify the privacy guarantee (the smaller values, the better the privacy;
25"
INTRODUCTION,0.05895196506550218,"see Section 2 for formal definitions). Intuitively, an algorithm M being (ε, δ)-DP means that its output
26"
INTRODUCTION,0.0611353711790393,"does not reveal much about any particular user’s data, since the output would be nearly identical had
27"
INTRODUCTION,0.06331877729257641,"this user’s data been completely different.
28"
INTRODUCTION,0.06550218340611354,"While the use of differential privacy can mitigate many privacy concerns, it does come with some
29"
INTRODUCTION,0.06768558951965066,"limitations. The first is the overhead in brings: that is, ensuring differential privacy for a learning
30"
INTRODUCTION,0.06986899563318777,"task typically incurs an overhead in the number of data points needed to achieve the same accuracy
31"
INTRODUCTION,0.07205240174672489,"guarantee. Perhaps more importantly, DP does not solve all possible privacy concerns: even if a ML
32"
INTRODUCTION,0.07423580786026202,"model is trained on a sensitive dataset in a differentially private way, the dataset may still be subject
33"
INTRODUCTION,0.07641921397379912,"to some attacks – e.g., if the server where the training data is stored is itself compromised. Somewhat
34"
INTRODUCTION,0.07860262008733625,"tautologically: DP is not a silver bullet, and only provides meaningful guarantees against the threat
35"
INTRODUCTION,0.08078602620087336,"models it was meant to address.
36"
INTRODUCTION,0.08296943231441048,"Another type of concerns focuses on the individual right to maintain control on one’s own data:
37"
INTRODUCTION,0.0851528384279476,"broadly speaking, this is asking that each user can (under some reasonable circumstances) require that
38"
INTRODUCTION,0.08733624454148471,"their personal data and information be removed from a company’s collected data and trained models.
39"
INTRODUCTION,0.08951965065502183,"This so-called “right to be forgotten,” which allow people to request that their data be deleted entirely
40"
INTRODUCTION,0.09170305676855896,"from an ML system, has been passed into legislation or is considered in some form or another by
41"
INTRODUCTION,0.09388646288209607,"various countries or entities, prominently the European Union’s General Data Protection Regulation
42"
INTRODUCTION,0.09606986899563319,"(GDPR), the California Privacy Rights Act (CCRA), Canada’s proposed Consumer Privacy Protection
43"
INTRODUCTION,0.0982532751091703,"Act (CPPA), and most recently in Australia [Karp, 2023].
44"
INTRODUCTION,0.10043668122270742,"However, translating this “right to be forgotten” into practice comes with a host of challenges, starting
45"
INTRODUCTION,0.10262008733624454,"with how to formalize it [Cohen et al., 2022] and technically implement it – which recently led to a
46"
INTRODUCTION,0.10480349344978165,"new area of research in ML and computer science, that of machine unlearning. A naive technical
47"
INTRODUCTION,0.10698689956331878,"solution would be for a given company to keep the original training set at all times, and, upon a
48"
INTRODUCTION,0.1091703056768559,"deletion request by a user, remove this user’s data from the set before retraining the whole model on
49"
INTRODUCTION,0.11135371179039301,"the result. This, of course, comes up with two major drawbacks: first, the cost to the company, in
50"
INTRODUCTION,0.11353711790393013,"terms of time and computational resources, of retraining a large model on a regular basis. Second, the
51"
INTRODUCTION,0.11572052401746726,"privacy cost, as keeping the training set for an indefinite time in order to be able to handle the deletion
52"
INTRODUCTION,0.11790393013100436,"requests leaves the door open to potential attacks and data breaches. Fortunately, there have been,
53"
INTRODUCTION,0.12008733624454149,"over the past few years, a flurry of better (and more involved) approaches to machine unlearning, to
54"
INTRODUCTION,0.1222707423580786,"handle deletion requests much more efficiently, and requiring to maintain much less of the training
55"
INTRODUCTION,0.12445414847161572,"set (see, e.g., [Bourtoule et al., 2021, Nguyen et al., 2022], and related work below).
56"
INTRODUCTION,0.12663755458515283,"The above discussion, still, brings to light an important question: is machine unlearning, paradoxically,
57"
INTRODUCTION,0.12882096069868995,"at odds with (differential) privacy? What is the connection between the two notions: are they
58"
INTRODUCTION,0.13100436681222707,"complementary, or is there a trade-off between them?
59"
INTRODUCTION,0.1331877729257642,"This is the main question this work sets out to address. Our starting point is the formalization
60"
INTRODUCTION,0.13537117903930132,"of machine unlearning set forth by Sekhari, Acharya, Kamath, and Suresh [Sekhari et al., 2021],
61"
INTRODUCTION,0.13755458515283842,"itself reminiscent of the definition of DP (see Definition 2.5 for the formal statement): a pair
62"
INTRODUCTION,0.13973799126637554,"of algorithms (A, ¯A) is an (ε, δ)-unlearning algorithm if (1) A: X ∗→W is a (randomized)
63"
INTRODUCTION,0.14192139737991266,"learning algorithm which, given a dataset X ⊆X ∗, outputs model parameters A(X) ∈W; and
64"
INTRODUCTION,0.14410480349344978,"(2) ¯A: X ∗× W × T →W which, on input a set of deletion requests U ⊆X, previous model
65"
INTRODUCTION,0.1462882096069869,"parameters w, and some succinct additional “side information” T(X) ∈T about the original dataset,
66"
INTRODUCTION,0.14847161572052403,"output updated model parameters w′ ∈W from which the data from U has been unlearned, that is,
67"
INTRODUCTION,0.15065502183406113,"such that
68"
INTRODUCTION,0.15283842794759825,"Pr
 ¯A(U, A(X), T(X)) ∈W

≤eε Pr
 ¯A(∅, A(X \ U), T(X \ U)) ∈W

+ δ"
INTRODUCTION,0.15502183406113537,"and
69"
INTRODUCTION,0.1572052401746725,"Pr
 ¯A(∅, A(X \ U), T(X \ U)) ∈W

≤eε Pr
 ¯A(U, A(X), T(X)) ∈W

+ δ
for every possible set W ⊆W of model parameters. Loosely speaking, this requires that the outcomes
70"
INTRODUCTION,0.15938864628820962,"of (a) training a model M via A on the dataset X then unlearning some of the original training data
71"
INTRODUCTION,0.1615720524017467,"U ⊆X from M using ¯A, and (b) training a model M ′ via A directly on the dataset X \ U then
72"
INTRODUCTION,0.16375545851528384,"unlearning nothing via ¯A, be nearly indistinguishable.
73"
INTRODUCTION,0.16593886462882096,"In their paper, Sekhari et al. [Sekhari et al., 2021] focus on genralization guarantees of unlearning
74"
INTRODUCTION,0.16812227074235808,"algorithm, i.e., what can be achieved by unlearning algorithms when focusing on population loss,
75"
INTRODUCTION,0.1703056768558952,"namely, when aiming to minimize
76"
INTRODUCTION,0.17248908296943233,"F(w) := Ex∼D[f(w, x)]"
INTRODUCTION,0.17467248908296942,"given a prespecified loss function f : W × X →R, where the expectation is over the draw of a new
77"
INTRODUCTION,0.17685589519650655,"datapoint from the underlying distribution p on the sample space. The quality of a learning algorithm
78"
INTRODUCTION,0.17903930131004367,"A is then measured by the expected excess risk
79"
INTRODUCTION,0.1812227074235808,"R(f, A) := E

F(A(X)) −
inf
w∗∈W F(w∗)
"
INTRODUCTION,0.18340611353711792,"where the expectation is taking over the random choice of a dataset X ∼Dn of size n, and the
80"
INTRODUCTION,0.185589519650655,"randomness of A itself. The focus of [Sekhari et al., 2021], as is ours, is then to quantify the deletion
81"
INTRODUCTION,0.18777292576419213,"capacity achievable for (ε, δ)-unlearning given a prespecified loss function, that is, the maximum
82"
INTRODUCTION,0.18995633187772926,"number of data points one can ask to be forgotten (maximum size of the subset U) before the excess
83"
INTRODUCTION,0.19213973799126638,"risk increases by more than some threshold (see Definition 2.6).
84"
INTRODUCTION,0.1943231441048035,"In their paper, [Sekhari et al., 2021] draw a connection between DP learning algorithms and unlearning
85"
INTRODUCTION,0.1965065502183406,"ones, showing that DP learning algorithms do provide some unlearning guarantees out-of-the-box,
86"
INTRODUCTION,0.19868995633187772,"and that one can achieve non-trivial unlearning guarantees for convex loss functions by leveraging
87"
INTRODUCTION,0.20087336244541484,"the literature on differentially private optimization and learning. One of their main results is showing
88"
INTRODUCTION,0.20305676855895197,"that these DP-based unlearning algorithms, which crucially do not rely on any side information
89"
INTRODUCTION,0.2052401746724891,"(the additional input T(X) ∈T provided to the unlearning algorithm ¯A) can handle strictly fewer
90"
INTRODUCTION,0.2074235807860262,"deletion requests than general unlearning algorithms which do rely on such side information.
91"
INTRODUCTION,0.2096069868995633,"Their results, however, do not fully characterize the deletion capacity of these “DP-based” machine
92"
INTRODUCTION,0.21179039301310043,"unlearning algorithms, leaving a significant gap between their upper and lower bounds. We argue
93"
INTRODUCTION,0.21397379912663755,"that fully understanding this quantity is crucial, as DP-based unlearning algorithms are exactly those
94"
INTRODUCTION,0.21615720524017468,"for which there is no conflict between the two notions of DP and unlearning – instead, this class
95"
INTRODUCTION,0.2183406113537118,"of algorithms is the one for which they work hand in hand. This is in contrast to the more general
96"
INTRODUCTION,0.2205240174672489,"unlearning algorithms relying on maintaining and storing side information about the training set, as
97"
INTRODUCTION,0.22270742358078602,"this side information can make their deployment susceptible to privacy breaches.
98"
OUR CONTRIBUTIONS,0.22489082969432314,"1.1
Our contributions
99"
OUR CONTRIBUTIONS,0.22707423580786026,"The main contribution of our paper is a tight bound on the “amount of unlearning” achievable by any
100"
OUR CONTRIBUTIONS,0.2292576419213974,"machine unlearning algorithm which does not rely on side information. For the sake of exposition,
101"
OUR CONTRIBUTIONS,0.2314410480349345,"we state in this section informal versions of our results.
102"
OUR CONTRIBUTIONS,0.2336244541484716,"Theorem 1.1 (Unlearning For Convex Loss Functions (Informal; see Theorems 3.1 and 3.3)). Let
103"
OUR CONTRIBUTIONS,0.23580786026200873,"f : W × X →R be a 1-Lipschitz convex loss function, where W ⊆Rd is the parameter space. There
104"
OUR CONTRIBUTIONS,0.23799126637554585,"exists an (ε, δ)-machine unlearning algorithm which, trained on a dataset S ⊆X n, does not store
105"
OUR CONTRIBUTIONS,0.24017467248908297,"any side information about the training set besides the learned model, and can unlearn up to
106 m = O nεα
p"
OUR CONTRIBUTIONS,0.2423580786026201,d log(1/δ) !
OUR CONTRIBUTIONS,0.2445414847161572,"datapoints without incurring excess population risk greater than α. Moreover, this is tight: there
107"
OUR CONTRIBUTIONS,0.24672489082969432,"exists a 1-Lipschitz linear loss function such that no machine unlearning algorithm can unlearn
108"
OUR CONTRIBUTIONS,0.24890829694323144,"Ω(
nεα
√"
OUR CONTRIBUTIONS,0.25109170305676853,"d log(1/δ)) data points without excess population risk α, unless it stores side information.
109"
OUR CONTRIBUTIONS,0.25327510917030566,"Our techniques also allow us to easily establish the analogue for strongly convex optimization:
110"
OUR CONTRIBUTIONS,0.2554585152838428,"Theorem 1.2 (Unlearning For Strongly Convex Loss Functions (Informal)). Let f : W × X →R
111"
OUR CONTRIBUTIONS,0.2576419213973799,"be a 1-Lipschitz strongly convex loss function. There exists an (ε, δ)-machine unlearning algorithm
112"
OUR CONTRIBUTIONS,0.259825327510917,"which, trained on a dataset S ⊆X n, does not store any side information about the training set
113"
OUR CONTRIBUTIONS,0.26200873362445415,"besides the learned model, and can unlearn up to
114"
OUR CONTRIBUTIONS,0.26419213973799127,"m = O

n2εα
d log(1/δ) "
OUR CONTRIBUTIONS,0.2663755458515284,"datapoints without incurring excess population risk greater than α. Moreover, this is tight.
115"
OUR CONTRIBUTIONS,0.2685589519650655,"We note that, prior to our work, only bounds for the convex loss function case were known, with
116"
OUR CONTRIBUTIONS,0.27074235807860264,"an upper bound of m = ˜O(nεα/
p"
OUR CONTRIBUTIONS,0.27292576419213976,"d log(eε/δ)) (loose by polylogarithmic factors for ε = O(1), as
117"
OUR CONTRIBUTIONS,0.27510917030567683,"well as an 1/√ε factor for ε ≫1) and a limited lower bound stating that m ≥1 is only possible if
118 nε/
√"
OUR CONTRIBUTIONS,0.27729257641921395,"d = Ω(1).
119"
OUR CONTRIBUTIONS,0.2794759825327511,"Our next contribution, motivated by the similarity of the formalisations of machine unlearning
120"
OUR CONTRIBUTIONS,0.2816593886462882,"(without side information) and that of differential privacy, is to establish the analogue of key properties
121"
OUR CONTRIBUTIONS,0.2838427947598253,"of DP for machine unlearning, namely, post-processing and composition of machine unlearning
122"
OUR CONTRIBUTIONS,0.28602620087336245,"algorithms. To do so, we first identify a natural property of machine unlearning algorithms, which,
123"
OUR CONTRIBUTIONS,0.28820960698689957,"when satisfied, will allow for the composition properties:
124"
OUR CONTRIBUTIONS,0.2903930131004367,"Assumption 1.3 (Unlearning Laziness). An unlearning algorithm ( ¯A, A) is said to be lazy if, when
125"
OUR CONTRIBUTIONS,0.2925764192139738,"provided with an empty set of deletion requests, the unlearning algorithm ¯A does not update the
126"
OUR CONTRIBUTIONS,0.29475982532751094,"model. That is, ¯A(∅, A(X), T(X)) = A(X) for all datasets X.
127"
OUR CONTRIBUTIONS,0.29694323144104806,"We again emphasize that this laziness property is not only intuitive, it is also satisfied by several
128"
OUR CONTRIBUTIONS,0.29912663755458513,"existing unlearning algorithms, and in particular those proposed in Sekhari et al. [2021].
129"
OUR CONTRIBUTIONS,0.30131004366812225,"Theorem 1.4 (Post-processing of unlearning). Let ( ¯A, A) be an (ε, δ)-unlearning algorithm taking
130"
OUR CONTRIBUTIONS,0.3034934497816594,"no side information. Let f : W →W be an arbitrary (possibly randomized) function. Then (f ◦¯A, A)
131"
OUR CONTRIBUTIONS,0.3056768558951965,"is also an (ε, δ)-unlearning algorithm.
132"
OUR CONTRIBUTIONS,0.3078602620087336,"Under our laziness assumption, we also establish the following:
133"
OUR CONTRIBUTIONS,0.31004366812227074,"Theorem 1.5 (Chaining of unlearning). Let ( ¯A, A) be a lazy (ε, δ)-unlearning algorithm taking
134"
OUR CONTRIBUTIONS,0.31222707423580787,"no side information, and able to handle up to m deletion requests. Then, the algorithm which uses
135"
OUR CONTRIBUTIONS,0.314410480349345,"( ¯A, A) to sequentially unlearn k disjoint deletion requests U1, . . . , Uk ⊆X such that | ∪i Ui| ≤m,
136"
OUR CONTRIBUTIONS,0.3165938864628821,"outputting
137"
OUR CONTRIBUTIONS,0.31877729257641924,"¯A(Uk, . . . , ¯A(U1, A(X)) . . .)"
OUR CONTRIBUTIONS,0.32096069868995636,"is an (ε′, δ′)-unlearning algorithm, with ε′ = kε and δ′ = δ · ekε−1"
OUR CONTRIBUTIONS,0.3231441048034934,"eε−1 = O(kδ) (for k = O(1/ε)).
138"
OUR CONTRIBUTIONS,0.32532751091703055,"and, finally,
139"
OUR CONTRIBUTIONS,0.32751091703056767,"Theorem 1.6 (Advanced composition of unlearning). Let ( ¯A1, A), . . . , ( ¯Ak, A) be lazy (ε, δ)-
140"
OUR CONTRIBUTIONS,0.3296943231441048,"unlearning (with common learning algorithm A) taking no side information, and define the composi-
141"
OUR CONTRIBUTIONS,0.3318777292576419,"tion of those unlearning algorithms, ˜A as
142"
OUR CONTRIBUTIONS,0.33406113537117904,"˜A(U, A(X)) = f
  ¯A1(U, A(X)), . . . , ¯Ak(U, A(X))

."
OUR CONTRIBUTIONS,0.33624454148471616,"where f : Wk →W is any (possibly randomized) function. Then, for every δ′ > 0, ( ˜A, A) is an
143"
OUR CONTRIBUTIONS,0.3384279475982533,"(ε′, δ′)-unlearning taking no side information, where ε′ = k"
OUR CONTRIBUTIONS,0.3406113537117904,"2ε2 + ε
p"
OUR CONTRIBUTIONS,0.34279475982532753,"2k ln (1/δ′).
144"
RELATED WORK,0.34497816593886466,"1.2
Related work
145"
RELATED WORK,0.3471615720524017,"Albeit recent, the field of machine unlearning has already received considerable attention from the ML
146"
RELATED WORK,0.34934497816593885,"community, with an array of studies and papers focusing on practical solutions and their empirical
147"
RELATED WORK,0.35152838427947597,"performance. We focus in this section on the works most closely related to ours, mostly theoretical.
148"
RELATED WORK,0.3537117903930131,"As discussed earlier, the goal of machine unlearning (Bourtoule et al. [2021]) is to delete what models
149"
RELATED WORK,0.3558951965065502,"have learned from data. This problem coincides tangentially with the idea of differential privacy as
150"
RELATED WORK,0.35807860262008734,"they both requires to minimize the effect of a (or a group of) sample. The original, stringent definition
151"
RELATED WORK,0.36026200873362446,"of unlearning requires ε = 0 (full deletion of the user’s data, as if it had never been included in the
152"
RELATED WORK,0.3624454148471616,"training set in the first place) in contrast to differential privacy that allows ε > 0, leaving a possibility
153"
RELATED WORK,0.3646288209606987,"for “memorization.” To relax this definition, Ginart et al. [2019] proposed the probabilistic version of
154"
RELATED WORK,0.36681222707423583,"unlearning.
155"
RELATED WORK,0.36899563318777295,"Prior theoretical work of unlearning are mostly disjoint from the differential privacy literature,
156"
RELATED WORK,0.37117903930131,"in spite of a general recognition that the two notions aim to address related issues. Most works
157"
RELATED WORK,0.37336244541484714,"on machine unlearning mainly focus on empirical risk minimization of approximate unlearning
158"
RELATED WORK,0.37554585152838427,"algorithms (Guo et al. [2020], Izzo et al. [2020]), which seeks to find an approximate minimizer on
159"
RELATED WORK,0.3777292576419214,"the remaining dataset after deletion. Closest to our work is the recent paper of Sekhari et al. [2021],
160"
RELATED WORK,0.3799126637554585,"which formulated the notion of machine unlearning used in our paper and focused on population
161"
RELATED WORK,0.38209606986899564,"loss minimization of approximating unlearning algorithm (i.e., allowing ε > 0). Their objectives,
162"
RELATED WORK,0.38427947598253276,"however, were somewhat orthogonal to ours, as they focused for a large part on minimizing the space
163"
RELATED WORK,0.3864628820960699,"requirements for the side information T(X) provided to the unlearning algorithm (while our paper
164"
RELATED WORK,0.388646288209607,"focuses on algorithms which do not rely on any such side information, prone to potential privacy
165"
RELATED WORK,0.39082969432314413,"leaks). While their work, to motivate this focus, established partial bounds on the deletion capacity
166"
RELATED WORK,0.3930131004366812,"of unlearning algorithm that do not take in extra statistics, these bounds were not tight, and one
167"
RELATED WORK,0.3951965065502183,"of our main contributions is closing this gap. Following Sekhari et al. [2021], the notion of online
168"
RELATED WORK,0.39737991266375544,"unlearning algorithm – which receive the deletion requests sequentially – was put forward and studied
169"
RELATED WORK,0.39956331877729256,"in Suriyakumar and Wilson [2022], again with memory efficiency with respect to the side information
170"
RELATED WORK,0.4017467248908297,"in mind; however, their primary focus is on the empirical performance of unlearning algorithm.
171"
RELATED WORK,0.4039301310043668,"Another work closely to ours is the notion of certified data removal proposed by Guo et al. [2020].
172"
RELATED WORK,0.40611353711790393,"The main difference between (ε, δ)-certified removal and the definition from Sekhari et al. [2021] is
173"
RELATED WORK,0.40829694323144106,"that, in the former, the unlearning mechanism requires access not only to the samples to be deleted
174"
RELATED WORK,0.4104803493449782,"(the set U ⊆X), but also to the full original training set X: this is exactly the type of constraints our
175"
RELATED WORK,0.4126637554585153,"work seeks to avoid, due to the risk of data breach this entails.
176"
ORGANIZATION OF THE PAPER,0.4148471615720524,"1.3
Organization of the paper
177"
ORGANIZATION OF THE PAPER,0.4170305676855895,"We first provide the necessary background and notion on differential privacy, learning, and the
178"
ORGANIZATION OF THE PAPER,0.4192139737991266,"formulation of machine unlearning used throughout the paper in Section 2. We then provide a detailed
179"
ORGANIZATION OF THE PAPER,0.42139737991266374,"outline of the proof of our main result, Theorem 1.1, in Section 3, before concluding with a discussion
180"
ORGANIZATION OF THE PAPER,0.42358078602620086,"of results and future work in Section 4.
181"
ORGANIZATION OF THE PAPER,0.425764192139738,"Due to space constraints, the details of all other results, as well as omitted proofs, are deferred to the
182"
ORGANIZATION OF THE PAPER,0.4279475982532751,"Supplemental.
183"
PRELIMINARIES,0.43013100436681223,"2
Preliminaries
184"
PRELIMINARIES,0.43231441048034935,"In this section, we recall some notions and results we will extensively rely on in our proofs and
185"
PRELIMINARIES,0.4344978165938865,"theorems, starting with differential privacy.
186"
DIFFERENTIAL PRIVACY,0.4366812227074236,"2.1
Differential Privacy
187"
DIFFERENTIAL PRIVACY,0.4388646288209607,"Definition 2.1 ((Central) Differential Privacy (DP)). Fix ε > 0 and δ ∈[0, 1]. An algorithm
188"
DIFFERENTIAL PRIVACY,0.4410480349344978,"M : X n →Y satisfies (ε, δ)-differential privacy (DP) if for every pair of neighboring datasets X, X′,
189"
DIFFERENTIAL PRIVACY,0.4432314410480349,"and every (measurable) subset S ⊆Y:
190"
DIFFERENTIAL PRIVACY,0.44541484716157204,Pr[ M(X) ∈S ] ≤eε Pr[ M(X′) ∈S ] + δ.
DIFFERENTIAL PRIVACY,0.44759825327510916,"We further say that M satisfies pure differential privacy (ε-DP) if δ = 0, otherwise it is approximate
191"
DIFFERENTIAL PRIVACY,0.4497816593886463,"differential privacy.
192"
DIFFERENTIAL PRIVACY,0.4519650655021834,"We now recall another notion of differential privacy in terms of Renyi Divergence, from Bun and
193"
DIFFERENTIAL PRIVACY,0.45414847161572053,"Steinke [2016].
194"
DIFFERENTIAL PRIVACY,0.45633187772925765,"Definition 2.2 (Zero-Concentrated Differential Privacy (zCDP)). A randomized algorithm M : X n →
195"
DIFFERENTIAL PRIVACY,0.4585152838427948,"Y satisfies (ξ, ρ)-zCDP if for every neighboring datasets (differing on a single entry) X, X′ ∈X n,
196"
DIFFERENTIAL PRIVACY,0.4606986899563319,"and ∀α ∈(1, ∞):
197"
DIFFERENTIAL PRIVACY,0.462882096069869,"Dα(M(X)∥M(X′)) ≤ξ + ρα
where D is the α-Renyi divergence between distributions of M(X) and M(X′). We say that M is
198"
DIFFERENTIAL PRIVACY,0.4650655021834061,"ρ-zCDP when ξ = 0.
199"
DIFFERENTIAL PRIVACY,0.4672489082969432,"We use the following group privacy property of zCDP in the proof later.
200"
DIFFERENTIAL PRIVACY,0.46943231441048033,"Proposition 2.3 (k-distance group privacy of ρ-zCDP [Bun and Steinke, 2016, Proposition 1.9]). Let
201"
DIFFERENTIAL PRIVACY,0.47161572052401746,"M : X n →Y satisfy ρ-zCDP. Then, M is (k2ρ)-zCDP for every X, X′ ∈X n that differs in at most
202"
DIFFERENTIAL PRIVACY,0.4737991266375546,"k entries.
203"
LEARNING,0.4759825327510917,"2.2
Learning
204"
LEARNING,0.4781659388646288,"We also will require some definitions on learning, specifically with respect to minimizing population
205"
LEARNING,0.48034934497816595,"loss. Fix any loss function f : W × X, where W is the (model) parameter space and X is the sample
206"
LEARNING,0.48253275109170307,"space. Then, the generalization loss is defined as
207"
LEARNING,0.4847161572052402,"F(w) := Ex∼p[f(w, x)]"
LEARNING,0.4868995633187773,"in which the expectation is over the distribution of x (one sample) and w is the learning output. Let
208"
LEARNING,0.4890829694323144,"F ∗= minw∈W F(w) be the minimizer of population risk and w∗is the corresponding minimizer.
209"
LEARNING,0.4912663755458515,"Define learning algorithm A : X n →W that takes in dataset S ∈X n and returns hypothesis
210"
LEARNING,0.49344978165938863,"w := A(S) ∈W. The excess risk is given by:
211"
LEARNING,0.49563318777292575,E[F(A(S))] −F ∗
LEARNING,0.4978165938864629,"where the expectation is over the randomness of A and S.
212"
LEARNING,0.5,"Hence, we could define the sample complexity as following ([Sekhari et al., 2021, Definition 1]),
213"
LEARNING,0.5021834061135371,"which is analogous to deletion capacity, in which will be stated later.
214"
LEARNING,0.5043668122270742,"Definition 2.4 (Sample complexity of learning). The α-sample complexity of a problem is defined as:
215"
LEARNING,0.5065502183406113,"n(α) := min{n | ∃A s.t. E[F(A(S))] −F ∗≤α, ∀D}"
UNLEARNING,0.5087336244541485,"2.3
Unlearning
216"
UNLEARNING,0.5109170305676856,"As previously discussed, we rely on the definition of unlearning proposed in by Sekhari et al. [2021],
217"
UNLEARNING,0.5131004366812227,"and maintain same notation. Note that T(S) denotes the data statistics (which could be the entire
218"
UNLEARNING,0.5152838427947598,"dataset S or any form of statistic) available to ¯A.
219"
UNLEARNING,0.517467248908297,"Definition 2.5 ((ε, δ)-unlearning). For all S of size n and delete requests U ⊆S such that |U| ≤m,
220"
UNLEARNING,0.519650655021834,"and W ⊆W, a learning algorithm A and an unlearning algorithm ¯A is (ε, δ)-unlearning if:
221"
UNLEARNING,0.5218340611353712,"Pr
 ¯A(U, A(S), T(S)) ∈W

≤eε Pr
 ¯A(∅, A(S \ U), T(S \ U)) ∈W

+ δ
and
222"
UNLEARNING,0.5240174672489083,"Pr
 ¯A(∅, A(S \ U), T(S \ U)) ∈W

≤eε Pr
 ¯A(U, A(S), T(S)) ∈W

+ δ,"
UNLEARNING,0.5262008733624454,"Our results will be phrased in terms of the deletion capacity, which captures the number of deletion
223"
UNLEARNING,0.5283842794759825,"requests an unlearning algorithm can handle before seeing a noticeable drop in its output’s accuracy:
224"
UNLEARNING,0.5305676855895196,"Definition 2.6 (Deletion Capacity). Let ε, δ > 0, S be a dataset of size n drawn i.i.d. from D
225"
UNLEARNING,0.5327510917030568,"and let ℓ(w, z) be a loss function. For a pair of learning and unlearning algorithm A, ¯A that are
226"
UNLEARNING,0.5349344978165939,"(ε, δ)-unlearning, the deletion capacity mA, ¯
A
ε,δ is defined as the maximum size of deletions requests
227"
UNLEARNING,0.537117903930131,"set |U| that we can unlearn without doing worse in excess population risk than α:
228"
UNLEARNING,0.5393013100436681,"mA, ¯
A
ε,δ (α) := max{m | E

max
U⊆S:|U|≤m F( ¯A(U, A(S), T(S))) −F ∗

≤α}"
UNLEARNING,0.5414847161572053,"where F ∗:= minA(S)∈B F( ¯A(U, A(S), T(S))).
229"
MAIN RESULT,0.5436681222707423,"3
Main result
230"
MAIN RESULT,0.5458515283842795,"In this section, we provide a detailed outline of our main result on unlearning for convex loss functions,
231"
MAIN RESULT,0.5480349344978166,"Theorem 1.1, for which we prove the upper and lower bounds separately.
232"
MAIN RESULT,0.5502183406113537,"Theorem 3.1 (Deletion capacity from unlearning via DP, Lower Bound). Suppose W ⊆Rd, and fix
233"
MAIN RESULT,0.5524017467248908,"any Lipschitz convex loss function. Then there exists a lazy (ε, δ)-unlearning algorithm ( ¯A, A), where
234"
MAIN RESULT,0.5545851528384279,"¯A has the form ¯A(U, A(S), T(S)) := A(S) (and thus, in particular, takes no side information) with
235"
MAIN RESULT,0.5567685589519651,"deletion capacity
236"
MAIN RESULT,0.5589519650655022,"mA, ¯
A
ε,δ (α) ≥Ω εnα
p"
MAIN RESULT,0.5611353711790393,d log (1/δ) !
MAIN RESULT,0.5633187772925764,"where the constant in the Ω(·) only depends on the properties of the loss function.
237"
MAIN RESULT,0.5655021834061136,"Proof. Our proof follows the same general outline as that of Sekhari et al. [2021], with a key difference
238"
MAIN RESULT,0.5676855895196506,"which allows us to derive the tight bound. Namely, we start, similarly, from the observation that any
239"
MAIN RESULT,0.5698689956331878,"(ε, δ)-DP learning algorithm A whose privacy guarantee is with respect to edit distance m between
240"
MAIN RESULT,0.5720524017467249,"datasets (instead of the usual neighboring relation) readily implies an (ε, δ)-machine unlearning
241"
MAIN RESULT,0.574235807860262,"algorithm ( ¯A, A), where ¯A(w) = w (i.e., the “unlearning part” returns its input unchanged).
242"
MAIN RESULT,0.5764192139737991,"However, we depart from previous work by how we obtain this (ε, δ)-DP algorithm A with respect to
243"
MAIN RESULT,0.5786026200873362,"edit distance m. The key insight is that instead of starting with any good approximate DP learning
244"
MAIN RESULT,0.5807860262008734,"algorithm and using the grouposition property of DP to “upgrade” it to m-edit distance, we instead
245"
MAIN RESULT,0.5829694323144105,"start with a good zCDP learning algorithm. Indeed, zCDP has much tigher grouposition properties
246"
MAIN RESULT,0.5851528384279476,"than approximate DP (cf. Proposition 2.3), which in turn leads to better parameters when applying
247"
MAIN RESULT,0.5873362445414847,"grouposition to achieve DP to groups up to size m: specifically, starting with a ρ2/2-zCDP standard
248"
MAIN RESULT,0.5895196506550219,"privacy guarantee (for groups of size 1) we would by Proposition 2.3 obtain (m2ρ2/2)-zCDP for
249"
MAIN RESULT,0.5917030567685589,"neighboring datasets differin in up to m entries. Leveraging then the standard conversion from
250"
MAIN RESULT,0.5938864628820961,"concentrated to approximate DP [Bun and Steinke, 2016], this implies, for every δ > 0, an (ε, δ)-DP
251"
MAIN RESULT,0.5960698689956332,"guarantee for groups of size m, where ε = O(mρ
p"
MAIN RESULT,0.5982532751091703,"log (1/δ)). Thus, choosing ρ = Θ

ε
m√"
MAIN RESULT,0.6004366812227074,ln (1/δ)  252
MAIN RESULT,0.6026200873362445,"would suffice to achieve the desired end privacy guarantee on A (with respect to edit distance up to
253"
MAIN RESULT,0.6048034934497817,"m), and thus the (ε, δ)-unlearning one for ( ¯A, A).
254"
MAIN RESULT,0.6069868995633187,"To do so, however, we crucially need to start with a sufficiently good private learning algorithm A
255"
MAIN RESULT,0.6091703056768559,"with zCDP guarantees, instead of approximate DP. Fortunately for us, such an algorithm is provided
256"
MAIN RESULT,0.611353711790393,"by [Feldman et al., 2020, Theorem 1]:
257"
MAIN RESULT,0.6135371179039302,"Lemma 3.2 (zCDP mini-batch noisy SGD Feldman et al. [2020]). Fix any L-Lipschitz convex loss
258"
MAIN RESULT,0.6157205240174672,"function over a convex subset B of Rd of diameter D. Then there exists an algorithm A which satisfies
259"
MAIN RESULT,0.6179039301310044,"(ρ2/2)-zCDP with excess population loss:
260"
MAIN RESULT,0.6200873362445415,"E

F(θ) −min
θ∈B F(θ)

≤O  DL ·"
MAIN RESULT,0.6222707423580786,"1
√n + √ d
ρn !! (1)"
MAIN RESULT,0.6244541484716157,"where the expectation is taken over the randomness of A.
261"
MAIN RESULT,0.6266375545851528,"By the above discussion, using this zCDP-private learning algorithm with ρ = Θ

ε
m√"
MAIN RESULT,0.62882096069869,ln (1/δ)
MAIN RESULT,0.631004366812227,"
, we
262"
MAIN RESULT,0.6331877729257642,"get an excess population loss bounded by
263 O  DL"
MAIN RESULT,0.6353711790393013,"1
√n + m
p"
MAIN RESULT,0.6375545851528385,d ln (1/δ) εn !! (2)
MAIN RESULT,0.6397379912663755,"It only remains to show how the claimed deletion capacity bound frollows from this excess population
264"
MAIN RESULT,0.6419213973799127,"risk guarantee. Construct, as discussed earlier, an unlearning algorithm ¯A that returns the input
265"
MAIN RESULT,0.6441048034934498,"without making any changes (and in particular does not require any additional statistics T(S), and
266"
MAIN RESULT,0.6462882096069869,"satisfies the laziness assumption). Since A is (ε, δ)-DP, for any set U ⊆S, |U| = m, and W ⊆W,
267"
MAIN RESULT,0.648471615720524,Pr[ A(S) ∈W ] ≤eε Pr[ A(S′) ∈W ] + δ
MAIN RESULT,0.6506550218340611,Pr[ A(S′) ∈W ] ≤eε Pr[ A(S) ∈W ] + δ
MAIN RESULT,0.6528384279475983,". But since ¯A(U, A(S)) = A(S), this readily yields, letting S′ := S \ U:
268"
MAIN RESULT,0.6550218340611353,"Pr
 ¯A(U, A(S)) ∈W

≤eε Pr
 ¯A(∅, A(S′)) ∈W

+ δ"
MAIN RESULT,0.6572052401746725,"Pr
 ¯A(∅, A(S′)) ∈W

≤eε Pr
 ¯A(U, A(S)) ∈W

+ δ"
MAIN RESULT,0.6593886462882096,"which implies that (A, ¯A) is indeed (ε, δ)-unlearning for U of size (up to) m.
269"
MAIN RESULT,0.6615720524017468,"Recalling the definition of deletion capacity (Definition 2.6), we finally deduce from (2) the deletion
270"
MAIN RESULT,0.6637554585152838,"capacity with excess population risk less than α:
271"
MAIN RESULT,0.665938864628821,"mA, ¯
A
ε,δ (α) ≥m = Ω εnα
p"
MAIN RESULT,0.6681222707423581,d ln (1/δ) !
MAIN RESULT,0.6703056768558951,"where the O(·) hides constant factors depending only on the loss function (namely, the Lipschitz
272"
MAIN RESULT,0.6724890829694323,"function L, and the diameter D).
273"
MAIN RESULT,0.6746724890829694,"Theorem 3.3 (Deletion capacity from unlearning via DP, Upper Bound). There exists a Lipschitz
274"
MAIN RESULT,0.6768558951965066,"convex loss function (indeed, linear) for which any ε, δ)-unlearning algorithm ( ¯A, A) which takes no
275"
MAIN RESULT,0.6790393013100436,"side information must have deletion capacity
276"
MAIN RESULT,0.6812227074235808,"mA, ¯
A
ε,δ (α) ≤O εnα
p"
MAIN RESULT,0.6834061135371179,d log (1/δ) ! .
MAIN RESULT,0.6855895196506551,"Detailed Proof Sketch. We will consider the following linear (and therefore convex and Lipschitz)
277"
MAIN RESULT,0.6877729257641921,"loss function L(θ, S):
278"
MAIN RESULT,0.6899563318777293,"L(θ, S) := −⟨θ, n
X"
MAIN RESULT,0.6921397379912664,"i=1
xi⟩
(3)"
MAIN RESULT,0.6943231441048034,"for dataset S of n points x1, . . . , xn ∈{−1
√"
MAIN RESULT,0.6965065502183406,"d,
1
√"
MAIN RESULT,0.6986899563318777,"d}d. We also define the 1-way marginal query, i.e.
279"
MAIN RESULT,0.7008733624454149,"average, as:
280"
MAIN RESULT,0.7030567685589519,"q(S) := 1 n n
X"
MAIN RESULT,0.7052401746724891,"i=1
xi .
(4)"
MAIN RESULT,0.7074235807860262,"To establish our deletion capacity lower bound with respect to this loss function, we will proceed
281"
MAIN RESULT,0.7096069868995634,"in three stages: the first, relatively standard, is to relate population loss (what we are interested in)
282"
MAIN RESULT,0.7117903930131004,"to empirical loss – which allows us to focus on the existence of a “hard dataset.” The second step
283"
MAIN RESULT,0.7139737991266376,"is then to establish a sample complexity lower bound on the empirical risk (for this loss function)
284"
MAIN RESULT,0.7161572052401747,"of any (ε, δ)-DP algorithm, via a reduction to differentially private computing of 1-marginals. This
285"
MAIN RESULT,0.7183406113537117,"step is similar to the one underlying the (weaker) lower bound of Sekhari et al. [2021] (itself relying
286"
MAIN RESULT,0.7205240174672489,"on an argument of [Bassily et al., 2019]), although a more careful choice of building blocks for the
287"
MAIN RESULT,0.722707423580786,"reduction already allows us to obtain an improvement by logarithmic factors.
288"
MAIN RESULT,0.7248908296943232,"Third, lift this DP lower bound to a stronger lower bound for DP with respect to edit distance m.
289"
MAIN RESULT,0.7270742358078602,"This step is quite novel, as it morally corresponds to establishing the converse of the grouposition
290"
MAIN RESULT,0.7292576419213974,"property of differential privacy (for our specific setting), a converse which does not hold in general.
291"
MAIN RESULT,0.7314410480349345,"Our argument, relatively simple, will crucially rely on the linearity of our loss function.
292"
MAIN RESULT,0.7336244541484717,"We omit the details of the first step (reduction from population to empirical loss) in this detailed
293"
MAIN RESULT,0.7358078602620087,"outline, as it is quite standard. For the second step, our starting point is the following lower bound of
294"
MAIN RESULT,0.7379912663755459,"Steinke and Ullman:
295"
MAIN RESULT,0.740174672489083,"Theorem 3.4 (Lower bound for one-way marginals [Steinke and Ullman, 2016, Main Theorem]). For
296"
MAIN RESULT,0.74235807860262,"every ε ∈(0, 1), every function δ = δ(n) such that δ ≥2−o(n) and δ ≤1/n1+Ω(1), and for every
297"
MAIN RESULT,0.7445414847161572,"α ≤1/10, if A : {±1}n×d →[±1]d is (ε, δ)-differentially private and E[∥A(S) −q(S)∥1] ≤αd
298"
MAIN RESULT,0.7467248908296943,"(i.e., with average-case accuracy α) for all S ∈{±1}n×d, then we must have
299 n ≥Ω p"
MAIN RESULT,0.7489082969432315,d ln (1/δ) εα ! .
MAIN RESULT,0.7510917030567685,"Using this lower bound as a blackbox, we then can adapt the argument of [Bassily et al., 2014, Lemma
300"
MAIN RESULT,0.7532751091703057,"5.1, Part 2] to obtain the following stronger result:
301"
MAIN RESULT,0.7554585152838428,"Lemma 3.5 (Lower bound for Privately Computing 1-way Marginals). Let n, d ∈N, ε > 0, 2−on ≤
302"
MAIN RESULT,0.75764192139738,"δ(n) ≤1/n1+Ω(1). For all α ≤1/10, if A is (ε, δ)-differentially private and S ⊆{± 1
√"
MAIN RESULT,0.759825327510917,"d}n×d:
303"
MAIN RESULT,0.7620087336244541,"E[∥A(S) −q(S)∥2] = min  α, Ω p"
MAIN RESULT,0.7641921397379913,"d ln (1/δ) nε !! ,"
MAIN RESULT,0.7663755458515283,where q(S) = 1
MAIN RESULT,0.7685589519650655,"n
Pn
i=1 xi as before.
304"
MAIN RESULT,0.7707423580786026,"Combining the above with the argument strategy of [Bassily et al., 2014, Theorem 5.3] finally yields
305"
MAIN RESULT,0.7729257641921398,"the main lemma for the second step of our proof:
306"
MAIN RESULT,0.7751091703056768,"Lemma 3.6 (Lower bound on empirical loss of (ε, δ)-DP algorithms). Let n, d ∈N, ε > 0, and
307"
MAIN RESULT,0.777292576419214,"δ = o(1/n). For every (ε, δ)-differentially private algorithm with output θpriv, there is a dataset
308"
MAIN RESULT,0.7794759825327511,"S = {x1, . . . , xn} ⊆{−1
√"
MAIN RESULT,0.7816593886462883,"d,
1
√"
MAIN RESULT,0.7838427947598253,"d}d such that
309"
MAIN RESULT,0.7860262008733624,"E

L(θpriv, S) −L(θ∗, S)

= min

α2, Ω
d log(1/δ) n2ε2 "
MAIN RESULT,0.7882096069868996,where θ∗:=
MAIN RESULT,0.7903930131004366,"Pn
i=1 xi
∥Pn
i=1 xi∥2 is the minimizer of L(θ, S) := −⟨θ, Pn
i=1 xi⟩.
310"
MAIN RESULT,0.7925764192139738,"The above lemma establishes a lower bound on the empirical loss of any (ε, δ)-differentially private
311"
MAIN RESULT,0.7947598253275109,"algorithm. To derive from this our claimed lower bound on unlearning algorithms, we need to
312"
MAIN RESULT,0.7969432314410481,"introduce a dependence on m, the deletion capacity (i.e., number of points to unlearn). This is done
313"
MAIN RESULT,0.7991266375545851,"in the last (third) step of our argument, via a reduction which establishes a (restricted) converse to the
314"
MAIN RESULT,0.8013100436681223,"grouposition property of DP.
315"
MAIN RESULT,0.8034934497816594,"Recall that an algorithm M : X n →Y satisfies (ε, δ)-DP for edit distance m if for every pair of
316"
MAIN RESULT,0.8056768558951966,"neighboring datasets X, X′ that differ in up to m entries, and every S ⊆Y:
317"
MAIN RESULT,0.8078602620087336,Pr[ M(X) ∈S ] ≤eε Pr[ M(X′) ∈S ] + δ.
MAIN RESULT,0.8100436681222707,"We apply this m-edit distance (ε, δ)-DP on Lemma 3.6 by a reduction that shows: for any differentially
318"
MAIN RESULT,0.8122270742358079,"private algorithm with respect to edit distance at most m must incur an empirical loss given by
319"
MAIN RESULT,0.8144104803493449,"Lemma 3.6.
320"
MAIN RESULT,0.8165938864628821,"Lemma 3.7. Suppose there exists an m-edit distance (ε, δ)-DP algorithm M that takes in a dataset
321"
MAIN RESULT,0.8187772925764192,"S of size n to approximate q(S) (as defined in (4)), with empirical loss γ. Then, we can construct a
322"
MAIN RESULT,0.8209606986899564,"1-edit distance (i.e., standard) (ε, δ)-DP algorithm M′ that, on input a dataset S′ of N = n/m data
323"
MAIN RESULT,0.8231441048034934,"points, approximates q(S′) to error γ.
324"
MAIN RESULT,0.8253275109170306,"Proof. The reduction is quite simple: given M, construct M′ as follows for N = n"
MAIN RESULT,0.8275109170305677,"m inputs:
325"
MAIN RESULT,0.8296943231441049,"M′(x1, . . . , xN) = M(x1, . . . , x1,
|
{z
}
m
x2, . . . , x2
|
{z
}
m
, . . . , xN, . . . , xN
|
{z
}
m
) ."
MAIN RESULT,0.8318777292576419,"We immediately have that M′ is (ε, δ)-DP for the usual 1-edit distance between datasets, since
326"
MAIN RESULT,0.834061135371179,"M is DP with respect to edit distance m. The sample complexity and error bound then follows
327"
MAIN RESULT,0.8362445414847162,"direction from n = N × m, where n ≥N, N ∈N, m ≥1, and the fact that q(x1, . . . , xN) =
328"
MAIN RESULT,0.8384279475982532,"q(x1, . . . , x1, x2, . . . , x2, . . . , xN, . . . , xN) due to the definition of q.
329"
MAIN RESULT,0.8406113537117904,"Combining Lemma 3.7 with Lemma 3.6, we get that any m-edit distance (ε, δ)-DP algorithm M
330"
MAIN RESULT,0.8427947598253275,"approximating q on datasets of size n = N × m must have error γ at least
331"
MAIN RESULT,0.8449781659388647,"min

α2, Ω
d log(1/δ) N 2ε2"
MAIN RESULT,0.8471615720524017,"
= min

α2, Ω
m2d log(1/δ) n2ε2 "
MAIN RESULT,0.8493449781659389,"which, reorganising the terms and recalling the definition of deletion capacity, yields the claimed
332"
MAIN RESULT,0.851528384279476,"bound on mA, ¯
A
ε,δ .
333"
MAIN RESULT,0.8537117903930131,"We note that the proof of Theorem 1.2 follows from a very similar argument; we refer the reader to
334"
MAIN RESULT,0.8558951965065502,"the Supplemental for details.
335"
DISCUSSION AND FUTURE WORK,0.8580786026200873,"4
Discussion and future work
336"
DISCUSSION AND FUTURE WORK,0.8602620087336245,"Our work fully characterized deletion capacity of any unlearning algorithm ( ¯A, A) minimizing
337"
DISCUSSION AND FUTURE WORK,0.8624454148471615,"population risk under both convex and strongly convex loss functions, when only given the model
338"
DISCUSSION AND FUTURE WORK,0.8646288209606987,"parameters (output of the learning algorithm) and the set of deletion requests. This restriction, namely
339"
DISCUSSION AND FUTURE WORK,0.8668122270742358,"that the unlearning algorithm does not rely on any additional side information, is motivated by the
340"
DISCUSSION AND FUTURE WORK,0.868995633187773,"potential privacy risks storing (non-private) side information can entail.
341"
DISCUSSION AND FUTURE WORK,0.87117903930131,"We hope our work will lead to further study of the interplay between differential privacy and machine
342"
DISCUSSION AND FUTURE WORK,0.8733624454148472,"unlearning, and to additional study of “DP-like” properties of machine unlearning, such as the
343"
DISCUSSION AND FUTURE WORK,0.8755458515283843,"postprocessing and composition properties our present work identified. In view of the myriad
344"
DISCUSSION AND FUTURE WORK,0.8777292576419214,"applications these properties have had in privacy-preserving algorithm design, we believe that their
345"
DISCUSSION AND FUTURE WORK,0.8799126637554585,"analogue for machine unlearning will prove very useful.
346"
DISCUSSION AND FUTURE WORK,0.8820960698689956,"We leave for future work the question of which unlearning guarantees can be obtained from pure
347"
DISCUSSION AND FUTURE WORK,0.8842794759825328,"differentially private algorithms, and of whether variants of the standard threat model for differential
348"
DISCUSSION AND FUTURE WORK,0.8864628820960698,"privacy (specifically, pan-privacy, or privacy under continual observation) could have implications for
349"
DISCUSSION AND FUTURE WORK,0.888646288209607,"machine unlearning in an online setting where deletion requests come sequentially.
350"
REFERENCES,0.8908296943231441,"References
351"
REFERENCES,0.8930131004366813,"Raef Bassily, Adam D. Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
352"
REFERENCES,0.8951965065502183,"algorithms and tight error bounds. In 55th IEEE Annual Symposium on Foundations of Computer
353"
REFERENCES,0.8973799126637555,"Science, FOCS, pages 464–473. IEEE Computer Society, 2014. doi: 10.1109/FOCS.2014.56.
354"
REFERENCES,0.8995633187772926,"Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
355"
REFERENCES,0.9017467248908297,"convex optimization with optimal rates. In Advances in Neural Information Processing Systems
356"
REFERENCES,0.9039301310043668,"32: Annual Conference on Neural Information Processing Systems, pages 11279–11288, 2019.
357"
REFERENCES,0.9061135371179039,"Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin
358"
REFERENCES,0.9082969432314411,"Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 42nd IEEE
359"
REFERENCES,0.9104803493449781,"Symposium on Security and Privacy, SP 2021, San Francisco, CA, USA, 24-27 May 2021, pages
360"
REFERENCES,0.9126637554585153,"141–159. IEEE, 2021. doi: 10.1109/SP40001.2021.00019. URL https://doi.org/10.1109/
361"
REFERENCES,0.9148471615720524,"SP40001.2021.00019.
362"
REFERENCES,0.9170305676855895,"Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions,
363"
REFERENCES,0.9192139737991266,"and lower bounds. In Martin Hirt and Adam D. Smith, editors, Theory of Cryptography - 14th
364"
REFERENCES,0.9213973799126638,"International Conference, TCC 2016-B, Beijing, China, October 31 - November 3, 2016, Pro-
365"
REFERENCES,0.9235807860262009,"ceedings, Part I, volume 9985 of Lecture Notes in Computer Science, pages 635–658, 2016. doi:
366"
REFERENCES,0.925764192139738,"10.1007/978-3-662-53641-4\_24.
367"
REFERENCES,0.9279475982532751,"Aloni Cohen, Adam D. Smith, Marika Swanberg, and Prashant Nalini Vasudevan. Control, confiden-
368"
REFERENCES,0.9301310043668122,"tiality, and the right to be forgotten. CoRR, abs/2210.07876, 2022.
369"
REFERENCES,0.9323144104803494,"Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
370"
REFERENCES,0.9344978165938864,"private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography, TCC ’06,
371"
REFERENCES,0.9366812227074236,"pages 265–284, Berlin, Heidelberg, 2006. Springer.
372"
REFERENCES,0.9388646288209607,"Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: Optimal
373"
REFERENCES,0.9410480349344978,"rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
374"
REFERENCES,0.9432314410480349,"Computing, STOC 2020, page 439–449, New York, NY, USA, 2020. Association for Computing
375"
REFERENCES,0.9454148471615721,"Machinery. ISBN 9781450369794. doi: 10.1145/3357713.3384335.
376"
REFERENCES,0.9475982532751092,"Antonio A. Ginart, Melody Y. Guan, Gregory Valiant, and James Y. Zou. Making ai forget you: Data
377"
REFERENCES,0.9497816593886463,"deletion in machine learning. In Neural Information Processing Systems, 2019.
378"
REFERENCES,0.9519650655021834,"Chuan Guo, Tom Goldstein, Awni Y. Hannun, and Laurens van der Maaten. Certified data removal
379"
REFERENCES,0.9541484716157205,"from machine learning models. In Proceedings of the 37th International Conference on Machine
380"
REFERENCES,0.9563318777292577,"Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine
381"
REFERENCES,0.9585152838427947,"Learning Research, pages 3832–3842. PMLR, 2020. URL http://proceedings.mlr.press/
382"
REFERENCES,0.9606986899563319,"v119/guo20c.html.
383"
REFERENCES,0.962882096069869,"Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Y. Zou. Approximate data deletion
384"
REFERENCES,0.9650655021834061,"from machine learning models: Algorithms and evaluations. CoRR, abs/2002.10077, 2020. URL
385"
REFERENCES,0.9672489082969432,"https://arxiv.org/abs/2002.10077.
386"
REFERENCES,0.9694323144104804,"Paul Karp.
Australia to consider european-style right to be forgotten privacy laws.
The
387"
REFERENCES,0.9716157205240175,"Guardian, 2023. URL https://www.theguardian.com/australia-news/2023/jan/19/
388"
REFERENCES,0.9737991266375546,"right-to-be-forgotten-australia-europe-gdpr-privacy-laws.
389"
REFERENCES,0.9759825327510917,"Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and
390"
REFERENCES,0.9781659388646288,"Quoc Viet Hung Nguyen. A survey of machine unlearning. CoRR, abs/2209.02299, 2022. doi:
391"
REFERENCES,0.980349344978166,"10.48550/arXiv.2209.02299. URL https://doi.org/10.48550/arXiv.2209.02299.
392"
REFERENCES,0.982532751091703,"Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what
393"
REFERENCES,0.9847161572052402,"you want to forget: Algorithms for machine unlearning. In Advances in Neural Information
394"
REFERENCES,0.9868995633187773,"Processing Systems, volume 34, pages 18075–18086. Curran Associates, Inc., 2021.
395"
REFERENCES,0.9890829694323144,"Thomas Steinke and Jonathan R. Ullman. Between pure and approximate differential privacy. J. Priv.
396"
REFERENCES,0.9912663755458515,"Confidentiality, 7(2), 2016. doi: 10.29012/jpc.v7i2.648.
397"
REFERENCES,0.9934497816593887,"Vinith M. Suriyakumar and Ashia C. Wilson. Algorithms that approximate data removal: New results
398"
REFERENCES,0.9956331877729258,"and limitations. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/
399"
REFERENCES,0.9978165938864629,"2022/hash/77c7faab15002432ba1151e8d5cc389a-Abstract-Conference.html.
400"
