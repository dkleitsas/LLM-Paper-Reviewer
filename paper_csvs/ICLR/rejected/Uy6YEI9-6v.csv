Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0045045045045045045,"We present a method for composing photorealistic scenes from captured images
of objects. Our work builds upon neural radiance ﬁelds (NeRFs), which implicitly
model the volumetric density and directionally-emitted radiance of a scene from a
collection of images. While NeRFs synthesize realistic pictures, they only model
static scenes and are closely tied to speciﬁc imaging conditions. This property
makes NeRFs hard to generalize to new scenarios, including new lighting or new
arrangements of objects. Instead of learning a scene radiance ﬁeld as a NeRF
does, we propose to learn object-centric neural scattering functions (OSFs), a rep-
resentation that models per-object light transport implicitly using a lighting- and
view-dependent neural network. This enables rendering scenes even when ob-
jects or lights move, without retraining. Combined with a volumetric path tracing
procedure, our framework is capable of rendering light transport effects including
occlusions, specularities, shadows, and indirect illumination, both within individ-
ual objects and between different objects. We evaluate OSFs on synthetic and real
world datasets, and on generalizing to new scene conﬁgurations. Learning OSFs
leads to photorealistic, physically-accurate renderings of multi-object scenes."
INTRODUCTION,0.009009009009009009,"1
INTRODUCTION"
INTRODUCTION,0.013513513513513514,"Synthesizing images of dynamic scenes is an important problem in computer vision and graphics,
with applications in AR/VR and robotics (Savva et al., 2019; Xia et al., 2020). For synthetic scenes,
a user typically designs a set of 3D objects separately, then composes them into scenes to be rendered
with speciﬁed camera, material, and lighting parameters. While this traditional graphics approach
allows for ﬂexible scene compositions, it requires detailed models of geometry, lighting, materials,
and cameras, which can be difﬁcult to obtain for real-world scenes."
INTRODUCTION,0.018018018018018018,"To render real-world scenes without computer graphics models, recent works have explored using
neural implicit methods (Lombardi et al., 2019; Sitzmann et al., 2019a;b). Most notably, Milden-
hall et al. (2020) proposed neural radiance ﬁelds (NeRF), which achieve photorealistic quality by
implicitly modeling the volumetric density and directional emitted radiance of a scene."
INTRODUCTION,0.02252252252252252,"However, as shown in Figure 1, NeRF cannot gen-
eralize beyond the scene it was trained on, because
it assumes static scenes and ﬁxed illumination and
learns a radiance ﬁeld, which estimates only the
resulting radiance along a ray after all light trans-
port has occurred in a scene. Thus, for dynamic
scenes where lights and objects can move, a sep-
arate NeRF-based model is needed for each new
scene conﬁguration."
INTRODUCTION,0.02702702702702703,"(a) NeRF (Baseline)
(b) OSF (Our Method)"
INTRODUCTION,0.03153153153153153,Figure 1: (a) NeRF. (b) Our method.
INTRODUCTION,0.036036036036036036,"To address this issue, we propose Object-Centric Neural Scattering Functions (OSFs) to synthesize
dynamic scenes of objects learned from 2D images (Figure 2). We represent each object as a learned
7D scattering function with inputs (x, y, z, φi, θi, φo, θo), where (x, y, z) is the spatial location,
(φi, θi) is the incoming light direction, and (φo, θo) is the outgoing light direction. The function
outputs the volumetric density as well as the fraction of light arriving from direction (φi, θi) that
scatters in outgoing direction (φo, θo)."
INTRODUCTION,0.04054054054054054,"Each OSF models all light bounces (reﬂections) and occlusions (shadows) within an object. Since
each object’s scattering function is a radiance transfer function rather than a radiance ﬁeld, it is"
INTRODUCTION,0.04504504504504504,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04954954954954955,"(c) Compose Scene
(d) Move Light
(e) Move Camera
(f) Move Objects
(b) Object Library
(a) Params"
INTRODUCTION,0.05405405405405406,"Figure 2: We propose an object-centric neural scene representation for image synthesis. Given a
scene description (a), and a repository of neural object-centric scattering functions (OSF) trained
independently from images and frozen for each object (b), we can compose the objects into scenes
(c), and render photorealistic images as we move lights (d), cameras (e), and/or objects (f). Our
framework is capable of rendering occlusions, specularities, shadows, and indirect illumination."
INTRODUCTION,0.05855855855855856,"intrinsic to the object (independent of the scene it is in) and can be reused across different object
placements and lighting conditions without retraining. We emphasize that because NeRFs are radi-
ance ﬁelds, they cannot be composed, and cannot generalize beyond one scene. In contrast, we can
render inﬁnitely many scenes. We can build a library of OSFs trained independently for different
objects to be composed into scenes with different object placements, camera, and lighting."
INTRODUCTION,0.06306306306306306,"To model light transport between objects, we integrate our implicit object functions with volumetric
path tracing. Like NeRF, we evaluate the radiance and volumetric density at 5D samples along
every primary ray to the camera and composite them with an over operator. However, unlike NeRF,
we estimate the radiance for each 5D sample by integrating our 7D OSF across the 2D sphere of
incoming light directions. We estimate the integral with Monte Carlo path tracing (Kajiya, 1986) to
reproduce shadows and indirect illumination effects."
INTRODUCTION,0.06756756756756757,"Our key idea is to decompose the rendering problem into (i) a learned component (per-object asset
creation), and (ii) a non-learned component (per-scene path tracing). The learned component mod-
els intra-object light transport (e.g., bounces from the seat of a chair to the back of the chair). The
non-learned component handles inter-object light transport (e.g., bounces from a wall to a chair). To-
gether, they model the full rendering equation (Kajiya, 1986) (except for occluders or light sources
that intrude the object’s convex hull (Sloan et al., 2002)). Since only the inter-object light trans-
port changes as objects and lights move, no re-training is required for different scene arrangements.
Experimental results indicate that our method is capable of rendering images with novel scene com-
positions and lighting conditions better than alternative learned approaches."
INTRODUCTION,0.07207207207207207,"In summary, our contributions are:"
INTRODUCTION,0.07657657657657657,"1. Learning Object-Centric Neural Scattering Functions (OSFs) that model intra-object light trans-
port implicitly using a lighting- and view-dependent neural network.
2. Integrating implicitly learned object scattering functions with volumetric path tracing to model
inter-object light transport.
3. A rendering algorithm that enables rendering scenes with moving objects, lights and cameras,
using implicit functions."
RELATED WORK,0.08108108108108109,"2
RELATED WORK"
RELATED WORK,0.08558558558558559,"Classical object-centric representations.
Factoring light transport into intra- and inter-object il-
lumination has a long history in traditional computer graphics (Dutre et al., 2018). In most cases,
the motivation is to improve rendering efﬁciency by approximating intra-object lighting factors with
simple transfer functions (e.g., linear) for simple radiance ﬁelds (e.g., spherical harmonics) de-
rived from from computer graphics models, as in precomputed radiance transfer (PRT) (Sloan et al.,
2002), ambient occlusion (Miller, 1994), or virtual walls (Arnaldi et al., 1994). In other cases, the
motivation is to insert captured, real-world radiance ﬁelds into synthetic scenes, as in Light Field
Transfer (Cossairt et al., 2008). These methods generally store the radiance ﬁeld for objects in a
discrete representation (e.g., a sampled 2D or 4D grid). As a result, they cannot reproduce accurate
inter-object light transport, especially for objects with intersecting bounding volumes. In contrast,
we focus on learning radiance transfer from images in order to model complex real-world scattering
accurately, and utilize volumetric rendering techniques to account for inter-object illumination."
RELATED WORK,0.09009009009009009,Under review as a conference paper at ICLR 2022
RELATED WORK,0.0945945945945946,"Novel view synthesis.
Traditional methods for synthesizing novel views of a scene from cap-
tured images include using Structure-From-Motion (Hartley & Zisserman, 2003) and bundle adjust-
ment (Triggs et al., 1999) to predict a sparse point cloud and camera parameters of the scene. More
recently, a number of learning-based novel view synthesis methods have been presented but require
3D geometry as inputs (Hedman et al., 2018; Thies et al., 2019; Meshry et al., 2019; Aliev et al.,
2020; Martin-Brualla et al., 2018). Others use multiplane images as proxies for novel view synthe-
sis, but their viewing ranges are limited to interpolated input views (Flynn et al., 2016; Zhou et al.,
2018; Srinivasan et al., 2019; Mildenhall et al., 2019). Some works represent scenes as coarse voxel
grids and use a CNN-based decoder for differentiable rendering, but lack view consistency due to
the use of 2D convolutional kernels (Nguyen-Phuoc et al., 2018; 2019; 2020)."
RELATED WORK,0.0990990990990991,"Recently, volume rendering approaches have been used to render scenes represented as voxel grids
that are more view-consistent (Lombardi et al., 2019; Sitzmann et al., 2019a). However, the render-
ing resolution of these methods are limited by the time and computational complexity of discretely
sampled volumes. To address this issue, Neural Radiance Fields (NeRF) (Mildenhall et al., 2020)
directly optimizes a continuous radiance ﬁeld representation using a multi-layer perceptron. This
allows synthesizing novel views of realistic images at an unprecedented level of ﬁdelity. To make
NeRF more efﬁcient, Neural Sparse Voxel Fields (Liu et al., 2020) have been proposed as a sparse
voxel octree variant of NeRF and demonstrate the ease of composing learned NeRFs with their
voxel representation. See (Dellaert & Yen-Chen, 2020) for survey. While these implicit methods
produce high-quality novel views of a scene, their models assume a static scene with ﬁxed illumi-
nation. Our method enables synthesizing dynamic scenes with novel viewpoint, lighting, and object
conﬁgurations.
Relighting.
Learning-based methods that relight images without explicit geometric reasoning have
been proposed, but lack the ability to recover hard shadows (Sun et al., 2019; Xu et al., 2018; Zhou
et al., 2019). Other works use geometric representations that facilitate shadowing computation,
but require 3D geometry as input (Philip et al., 2019; Zhang et al., 2021; Oechsle et al., 2020;
Rematas & Ferrari, 2020). Deep Reﬂectance Volumes (Bi et al., 2020b) reconstructs a voxelized
representation of a scene and predict per-voxel BRDFs, but the ﬁxed resolution of voxel grids limits
the quality in the rendered images. Similarly, Neural Reﬂectance Fields (Bi et al., 2020a) predicts
the parameters of a BRDF model, but demonstrate higher ﬁdelity rendering by learning a continuous
scene representation. However, Neural Reﬂectance Fields focuses on relighting single objects, and
requires manual speciﬁcation of the BRDF model. Parametric BRDF models are unable to handle
complex scattering functions, including real-world scattering phenomena that are difﬁcult to model.
In contrast, our method is capable of learning all scattering functions, and can render multiple objects
in dynamic scenes."
PRELIMINARIES,0.1036036036036036,"3
PRELIMINARIES"
VOLUME RENDERING,0.10810810810810811,"3.1
VOLUME RENDERING"
VOLUME RENDERING,0.11261261261261261,"To render an image of a scene with arbitrary camera parameters, camera rays are sent into the scene,
through each pixel on the image plane. The expected color of each pixel is computed as the radiance
along each camera ray."
VOLUME RENDERING,0.11711711711711711,"Volume rendering is an approach for computing the radiance traveling along rays traced in a volume.
Let r(t) = x0 + ωot be a point along a ray r with origin x0 and direction ωo, where t ∈R is a
1D location along the ray, and the o in ωo denotes “outgoing” direction. For our purposes, we
assume non-emissive and non-absorptive volumes. From Nov´ak et al. (2018), the volume rendering
equation to compute the radiance L(x0, ωo) of the ray is deﬁned as:"
VOLUME RENDERING,0.12162162162162163,"L(x0, ωo) =
Z tf"
VOLUME RENDERING,0.12612612612612611,"tn
τ(t)σ(r(t))Ls(r(t), ωo) dt,
where
τ(t) = exp

−
Z t"
VOLUME RENDERING,0.13063063063063063,"tn
σ(r(u)) du

, (1)"
VOLUME RENDERING,0.13513513513513514,"where tn and tf are near and far integration bounds, σ(r(t)) denotes the volume density of point
r(t), and τ(t) denotes the accumulated transmittance from tn to t. The term Ls(r(t), ωo) is the light
scattered at point r(t) along direction ωo, deﬁned as the integral over all incoming light directions:"
VOLUME RENDERING,0.13963963963963963,"Ls(x, ωo) =
Z"
VOLUME RENDERING,0.14414414414414414,"S
L(x, ωl)fp(x, ωl, ωo) dωl,
(2)"
VOLUME RENDERING,0.14864864864864866,"where S is a unit sphere and fp is a phase function that evaluates the fraction of light incoming from
direction ωl at a point x that scatters out in direction ωo. In NeRF, Mildenhall et al. (2020) assume"
VOLUME RENDERING,0.15315315315315314,Under review as a conference paper at ICLR 2022
VOLUME RENDERING,0.15765765765765766,"ﬁxed illumination and do not consider any form of Equation 2. We consider a more general form of
the volume rendering equation that explicitly models light paths within and between objects. This is
important for dynamic scenes, where lighting and objects can move with respect to one another."
RAY MARCHING,0.16216216216216217,"3.2
RAY MARCHING"
RAY MARCHING,0.16666666666666666,"The continuous integrals in Equation 1 can be estimated with quadrature (Kniss et al., 2003; Max,
1995), as done in NeRF (Mildenhall et al., 2020). For each ray, stratiﬁed sampling is used to obtain
N samples {ti}N
i=1 along the ray, where ti ∈[tn, tf]. The rendering equation is approximated by:"
RAY MARCHING,0.17117117117117117,"L(x0, ωo) = N
X"
RAY MARCHING,0.17567567567567569,"i=1
τiαiLs(xi, ωo)
where
Ls(xi, ωo) = 1 |L| X"
RAY MARCHING,0.18018018018018017,"l∈L
L(xi, ωl)ρl
i,
(3)"
RAY MARCHING,0.18468468468468469,"where τi = Qi−1
j=1(1 −αj) and αi = 1 −e−σi(ti+1−ti). To compute the average over incoming light
paths Ls, we discretize over the domain S in Equation 2 by sampling a set of incoming light paths
L = {l1, . . . , lK}, where ρl
i = fp(xi, ωl, ωo) ∈[0, 1], the fraction of light incoming from light
path l that is scattered in direction ωo."
NEURAL RADIANCE FIELDS,0.1891891891891892,"3.3
NEURAL RADIANCE FIELDS"
NEURAL RADIANCE FIELDS,0.19369369369369369,"NeRF represents a continuous scene as a volumetric radiance ﬁeld, approximated with a multilayer
perceptron FΘ. The model FΘ takes spatial location x = (x, y, z) and viewing direction d = (φ, θ)
as input, and outputs the density σ and color c = (r, g, b), where r, g, b ∈[0, 1]. Frequency-based
positional encoding (Rahaman et al., 2019; Vaswani et al., 2017) is applied to the inputs to better
capture high-frequency variation in appearance and geometry."
NEURAL RADIANCE FIELDS,0.1981981981981982,"A hierarchical volume sampling procedure (Mildenhall et al., 2020; Levoy, 1990) is then employed
to more efﬁciently allocate samples along each ray. This technique biases sample allocation to
favor the visible parts of the scene that contribute the most to the ﬁnal render, avoiding occluded
or free space in the scene. NeRF simultaneously optimizes two radiance ﬁelds, where the sample
weights τi · αi from a coarse model are used to bias samples for a ﬁne model. The L2 loss is used
to optimize both models: P
r∈R∥bCc(r) −C(r)∥2
2 + ∥bCf(r) −C(r)∥2
2, where R is the set of all
camera rays, bCc(r) and bCf(r) denote the radiance along ray r predicted by the coarse and ﬁne
models respectively, and C(r) is the ground truth pixel color for r."
METHOD,0.20270270270270271,"4
METHOD"
OBJECT-CENTRIC NEURAL SCATTERING FUNCTION,0.2072072072072072,"4.1
OBJECT-CENTRIC NEURAL SCATTERING FUNCTION"
OBJECT-CENTRIC NEURAL SCATTERING FUNCTION,0.21171171171171171,"We represent each object as a 7D object-centric neural scattering function (OSF), depicted in Fig-
ure 3a. For each object, we learn an implicit function FΘ : (x, ωl, ωo) →(σ, ρ) that receives a 3D
point in the object coordinate frame, the incoming light direction, and the outgoing light direction,
and predicts the volumetric density as well as fraction of incoming light that is scattered in the out-
going direction. Θ are learned weights that parameterize the neural network, x = (x, y, z) denotes
the spatial location, ωl = (φl, θl) denotes the incoming light direction, ωo = (φo, θo) denotes the
outgoing light direction, σ denotes the volumetric density, and ρ = (ρr, ρg, ρb) denotes the frac-
tion of light arriving at x from direction ωl that is scattered and leaving in direction ωo. The ﬁnal
color of a point x is the integral of ρ multiplied by the incoming radiance over all incoming light
directions in unit sphere S (Equation 2). Following NeRF, we similarly apply positional encoding
to our inputs (x, ωl, ωo) and employ a hierarchical sampling procedure to recover higher quality
appearance and geometry of learned objects."
OBJECT-CENTRIC NEURAL SCATTERING FUNCTION,0.21621621621621623,"During training, we assume a single point light source with radiance of (1, 1, 1). This simpliﬁes
Ls from Equation 2 to Ls(x, ωo) = L(x, ωl)fp(x, ωl, ωo) = fp(x, ωl, ωo). To learn per-object
NeRFs independent of object rotation and translation, the inputs to FΘ must be in the object’s canon-
ical coordinate frame. Given a object transformation Ti for object oi, we apply T −1
i
to (r, ωl, ωo)
before feeding the inputs to the network."
RENDERING MULTIPLE OSFS,0.22072072072072071,"4.2
RENDERING MULTIPLE OSFS"
RENDERING MULTIPLE OSFS,0.22522522522522523,"Once we have learned an OSF for each object, we aim at composing the learned objects into scenes.
An overview of our procedure is visually depicted in Figure 3b."
RENDERING MULTIPLE OSFS,0.22972972972972974,Under review as a conference paper at ICLR 2022
RENDERING MULTIPLE OSFS,0.23423423423423423,"Light 
Source"
RENDERING MULTIPLE OSFS,0.23873873873873874,Camera
RENDERING MULTIPLE OSFS,0.24324324324324326,"(a) We represent each object as an object-centric
neural scattering function (OSF), which models how
light entering at a point x on the object, from direc-
tion ωl where l corresponds to a light path, undergoes
multiple bounces within the object and exits along di-
rection ωo with some fractional amount of light ρ.
We approximate the scattering function with a mul-
tilayer perceptron FΘ where Θ are learned weights
that parameterize the neural network. Given a single
point x, an incoming light direction ωl, and an out-
going direction ωo, FΘ outputs the volume density σ
of that point, as well as the fraction of light arriving at
x from direction ωl that is scattered in direction ωo."
RENDERING MULTIPLE OSFS,0.24774774774774774,Primary rays
RENDERING MULTIPLE OSFS,0.25225225225225223,Indirect Illumination
RENDERING MULTIPLE OSFS,0.25675675675675674,Shadow Ray
RENDERING MULTIPLE OSFS,0.26126126126126126,Direct illumination
RENDERING MULTIPLE OSFS,0.26576576576576577,"Light 
Source"
RENDERING MULTIPLE OSFS,0.2702702702702703,Camera
RENDERING MULTIPLE OSFS,0.2747747747747748,"(b) Our procedure for rendering an arbitrary scene
consisting of multiple objects, light sources, and cam-
eras. Given a set of objects, we compute direct illu-
mination by shooting rays from each light source to
each object (brown arrows). Shadows are computed
by sending shadow rays back to each light source
(purple arrow). The shadow ray from the desk is oc-
cluded by the mug, so the mug casts a shadow on the
desk. We send secondary rays between objects to ren-
der indirect illumination effects, such as between the
desk and the kettle (green and blue dashed arrows).
Finally, rays are sent back to the camera to render the
ﬁnal image (dark blue arrows)."
RENDERING MULTIPLE OSFS,0.27927927927927926,Figure 3: Using our method (OSFs) to render: (a) single and (b) multiple objects.
RENDERING MULTIPLE OSFS,0.28378378378378377,"(a)
Primary ray sampling"
RENDERING MULTIPLE OSFS,0.2882882882882883,"(b)
Shadow ray sampling"
RENDERING MULTIPLE OSFS,0.2927927927927928,"Figure 4: Sampling procedure. (a) Scene with a camera, light source, and object bounding boxes.
Primary rays are sent from the camera into the scene. Rays that do not intersect with objects are
pruned. Of the intersecting rays, we sample points within intersecting regions. (b) Shadow rays
from each sample are sent to the light source, and samples within intersecting regions are evaluated."
RENDERING MULTIPLE OSFS,0.2972972972972973,"Let O = {oi}N
i=1 be a set of N objects we wish to render. For simplicity, we ﬁrst describe the
rendering process for each object oi, then explain the process to combine results across all objects
to render the ﬁnal scene. Let oi ∈O denote object i with transformation Ti ∈R4×4 and bounding
box dimensions Di ∈R3. Further let r be a camera ray with origin c ∈R3 and direction ωo ∈R3,
which we deﬁne with parameters γ = [c, ωo] ∈R6. Our goal is to compute L(c, ωo) as described
in Equation 3. We compute the ray-box intersection between the ray and the object to obtain near
bound ti
n and far bound ti
f such that r(ti
n) and r(ti
f) each intersect a box plane, as shown in Figure 4.
Note that rays that do not intersect with oi are excluded from our computation. We sample M points
between ti
n and ti
f along ray r to obtain a sample Xi = {xi
m}M
m=1, where Xi ∈RM×3. Given a
light source l, we evaluate the object’s model FΘi(Xi, ωl, ωo) to obtain alpha values αi ∈RM and
phase function values ρi ∈RM×3."
RENDERING MULTIPLE OSFS,0.30180180180180183,"It is not always possible for a light ray from light source l to reach the object oi. Any of the other
objects in O′ = {oj ∈O | j ̸= i} in the scene may occlude the incoming light, casting a shadow
on object oi. We compute shadows by sending a shadow ray rm from each of the M samples in Xi"
RENDERING MULTIPLE OSFS,0.3063063063063063,Under review as a conference paper at ICLR 2022
RENDERING MULTIPLE OSFS,0.3108108108108108,"to the light source l. Evaluating the shadow ray enables us to determine the amount of light blocked
along the ray by other objects. We deﬁne the parameters of the M shadow rays as Γ ∈RM×6."
RENDERING MULTIPLE OSFS,0.3153153153153153,"For each object oj ∈O′, we compute ray-box intersections between shadow rays Γ and oj’s bound-
ing box. This allows us to compute the amount of light traveling towards oi that is blocked by
oj. Similar to primary rays, we sample M points along each shadow ray to obtain a set of points
Xj ∈RM×M. We then evaluate the object model FΘj(Xj) to obtain alpha values Aj ∈RM×M.
For each shadow ray rm, we combine samples Aj
m across the N −1 objects in O′ by sorting ac-
cording to sample distance to obtain alpha values Am ∈RM(N−1). The fraction of unobstructed
light traveling along the shadow ray rm is computed as the transmittance:"
RENDERING MULTIPLE OSFS,0.31981981981981983,"τ l
m ="
RENDERING MULTIPLE OSFS,0.32432432432432434,"M(N−1)
Y"
RENDERING MULTIPLE OSFS,0.32882882882882886,"n=1
(1 −Amn).
(4)"
RENDERING MULTIPLE OSFS,0.3333333333333333,"Thus, the adjusted incoming radiance from light source l when accounting for occlusions is com-
puted as Ll(xm, ωl) = τ l
mLl(xm, ωl)."
RENDERING MULTIPLE OSFS,0.33783783783783783,"We follow the scattering equation in Equation 2 and now consider all incoming light directions over
the unit sphere S. This accounts for secondary light rays traveling to an object oi indirectly from
another object oj (indirect illumination). We approximate the integral over the unit sphere S by
sampling K directions on the unit sphere uniformly at random. For each direction ωk randomly
sampled for a point x, we send a secondary ray rk from x in direction ωk and evaluate the radiance
L(x, ωk) traveling along the ray. To compute the radiance of the secondary ray L(x, ωk), we
employ the same technique used to compute the radiance of a primary ray L(c, ωo) (described at
the beginning of Section 4.2). The incoming radiance L(x, ωk) is multiplied with the phase function
value ρ = fp(x, ωk, ωo) to determine the outgoing radiance L(x, ωo), where ρ is evaluated using
FΘi. Note that this is possible due to the recursive nature of our formulation. Only secondary rays
are described here (two bounces), but our method supports an arbitrary number of bounces.
Rendering.
We sample and evaluate all objects in O to obtain alpha values {αi}N
i=1 and phase
function values {ρi}N
i=1 for a set of sampled points {Xi}N
i=1 along ray r. We sort the samples across
all objects to produce a ﬁnal set of P = M · N samples {xm}P
m=1, {αm}P
m=1, and {ρm}P
m=1."
RENDERING MULTIPLE OSFS,0.34234234234234234,"Given light paths L containing both direct and indirect illumination, we render the ﬁnal radiance of
a ray with origin x0 and direction ωo with the following equation:"
RENDERING MULTIPLE OSFS,0.34684684684684686,"L(x0, ωo) = 1 |L| X l∈L P
X"
RENDERING MULTIPLE OSFS,0.35135135135135137,"m=1
αmρl
mτmLl(xm, ωl),
where
τm = m−1
Y"
RENDERING MULTIPLE OSFS,0.35585585585585583,"n=1
(1 −αn),
(5)"
RENDERING MULTIPLE OSFS,0.36036036036036034,"and Ll(xm, ωl) is the radiance from light path l arriving at point xm.
Runtime.
In total, the cost of rendering a single image with Npixel pixels and Nobject objects is
O(P 2KNpixel). Note that P is an upper bound on number of samples that need to be evaluated. In
practice, a single ray often only intersects with at most one object in the scene, which means that
the proposed rendering procedure is not signiﬁcantly more expensive than the single object setting.
We also note that compared to NRF Bi et al. (2020a) or traditional volumetric path tracing methods,
OSF crucially does not require running path tracing within each object to simulate intra-object light
bounces. This is because OSF learns the object-level scattering function that directly predicts the
effects after all light bounces (reﬂections) and occlusions (shadows) within an object have occurred.
Thus OSF is signiﬁcantly faster than NRF which relies on simulating intra-object light bounces
while querying its learned BRDF model."
RENDERING MULTIPLE OSFS,0.36486486486486486,"In our experiments, rendering a single image with a single OSF at a resolution of 256 × 256 takes
roughly 3.7 seconds. While the computation cost is high, there are efforts to reduce the rendering
speed of NeRF that are orthogonal to this work. For instance, KiloNeRF (Reiser et al., 2021) can
easily adapted to this work by utilizing thousands of tiny MLPs instead of one single large MLP to
represent each OSF to obtain 1-2 orders of magnitude speed up."
EXPERIMENTS,0.36936936936936937,"5
EXPERIMENTS"
EXPERIMENTS,0.3738738738738739,"Datasets and evaluation metrics.
We evaluate our approach on several image datasets:"
EXPERIMENTS,0.3783783783783784,"• FURNITURE-SINGLE: 15 objects rendered with random object pose, point light, and viewpoint."
EXPERIMENTS,0.38288288288288286,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.38738738738738737,"• FURNITURE-RANDOM: 25 dynamic scenes, each containing a random layout of multiple objects,
point light, and viewpoint.
• FURNITURE-REALISTIC: Scenes containing realistic arrangements of objects in rooms.
• REAL-NRF: Real-world objects from Bi et al. (2020a), captured in a dark room under varying
viewing and lighting directions.
• REAL-OUTDOOR: Real-world outdoor scenes from Mildenhall et al. (2020)."
EXPERIMENTS,0.3918918918918919,"For FURNITURE datasets, we use Blender’s Cycles path tracer (Blender Foundation, 1994) to ren-
der images at 256 × 256 resolution for different object arrangements, camera views, and lighting
conﬁgurations. We report PSNR, SSIM (Wang et al., 2003), and LPIPS (Zhang et al., 2018) metrics.
Baselines and ablations.
We compare our method to the following baselines:"
EXPERIMENTS,0.3963963963963964,"1. o-NeRF: A variant of the NeRF model, but with one NeRF trained per object. When o-NeRFs
are composed into scenes, they are rendered separately.
2. o-NeRF + S: An extension of o-NeRF with inter-object shadows; reduces the light arriving at
each o-NeRF by the cumulative opacity of shadowing objects along the ray from the light (§4.2)."
EXPERIMENTS,0.4009009009009009,"These baselines represent what could be achieved by combining separately trained NeRFs into a
scene. Of course, since o-NeRFs produce radiance ﬁelds (not scattering ﬁelds), we do not expect
them to perform well in novel lighting environments or object placements."
NOVEL LIGHTING,0.40540540540540543,"5.1
NOVEL LIGHTING"
NOVEL LIGHTING,0.4099099099099099,"In the ﬁrst experiment, we investigate how OSF method handles novel lighting conditions."
NOVEL LIGHTING,0.4144144144144144,"We train one model per object in FURNITURE-
SINGLE. For each object model, we train on
400 images with randomized viewpoint and
lighting, and test on 20 images of novel view-
point and lighting.
As can be seen in Fig-
ure 5, our method produces more accurate ap-
pearance of the objects in comparison to o-
NeRF when tested on novel illumination con-
ditions.
In particular, o-NeRF fails to pre-
dict self-shadows for the couch and chair cor-
rectly.
Additionally, o-NeRF fails to disen-
tangle viewpoint versus lighting-dependent ap-
pearance, producing incorrect shadows for the
couch and chair, and fails to capture the specu-
lar details of the ottoman. Quantitative results
can be found in Table 1."
NOVEL LIGHTING,0.4189189189189189,"Couch
Chair
Table
Ottoman"
NOVEL LIGHTING,0.42342342342342343,"Ground Truth
OSF (Ours)
o-NeRF"
NOVEL LIGHTING,0.42792792792792794,Figure 5: Novel lighting results.
SCENE COMPOSITION,0.43243243243243246,"5.2
SCENE COMPOSITION"
SCENE COMPOSITION,0.4369369369369369,"In a second experiment, we conduct a scene composition task on FURNITURE-RANDOM, where
multiple object models are combined into scenes in random pose, lighting, and viewpoint conﬁgu-
rations. For this task, we use the same object models trained in Section 5.1. Results are shown in
Table 1 and Figure 6. While not shown in the main text, results for FURNITURE-REALISTIC can be
found in Appendix B."
SCENE COMPOSITION,0.44144144144144143,"Table 1: Quantitative results for novel lighting (FURNITURE-SINGLE) and scene composition
(FURNITURE-RANDOM). Rows denote different methods: our full model (OSF), a variant of NeRF
where one NeRF is trained per object (o-NeRF), and o-NeRF with shadows (o-NeRF + S)."
SCENE COMPOSITION,0.44594594594594594,"Dataset
FURNITURE-SINGLE
FURNITURE-RANDOM"
SCENE COMPOSITION,0.45045045045045046,"Method
PSNR↑
SSIM↑
LPIPS↓
PSNR↑
SSIM↑
LPIPS↓"
SCENE COMPOSITION,0.45495495495495497,"o-NeRF
33.22
0.980
0.021
12.17
0.690
0.280
o-NERF + S
—
—
—
14.70
0.697
0.267
OSF (Our Method)
44.07
0.998
0.002
19.02
0.793
0.135"
SCENE COMPOSITION,0.4594594594594595,Under review as a conference paper at ICLR 2022
SCENE COMPOSITION,0.46396396396396394,"Ground Truth
o-NeRF + S
o-NeRF
OSF (ours)"
SCENE COMPOSITION,0.46846846846846846,"Figure 6: Scene composition results on FURNITURE-RANDOM. The models OSF, o-NeRF, and
o-NeRF + S are explained in §5. Compared to o-NeRF, our model (OSF) is able to disentangle
lighting-dependent and view-dependent appearance and can render shadows."
SCENE COMPOSITION,0.47297297297297297,"These results suggest that OSF outperforms all baselines and ablations, both quantitatively and qual-
itatively. As in the previous experiment (Section 5.1), we ﬁnd that OSF reproduces object appear-
ances and self-shadows more accurately than the baselines. The difference is especially apparent
in the couches in scenes (a) and (b), where the couches predicted by o-NeRF are extremely dark.
This is due to the fact that o-NeRF is unable to disentangle view-dependence appearance from light-
dependent appearance, and simply interpolates the radiance ﬁeld learned another different lighting
conﬁguration. Please note that OSF is able to model inter-object light transport effects by rendering
shadows cast by one object onto another and on the ground plane. Plus, it is able to render indirect
illumination of one object reﬂecting light onto another. For example, light reﬂected from the left
wall causes the left of the couch and table in scenes (a) and (b) to be brighter. Neither of these
lighting effects are present in the o-NeRF results."
REAL-WORLD SCENES,0.4774774774774775,"5.3
REAL-WORLD SCENES"
REAL-WORLD SCENES,0.481981981981982,"In this section we evaluate our method on real
world objects and scenes from the REAL-NRF
and REAL-OUTDOOR datasets.
For these
experiments, we train one OSF for each object
in REAL-NRF and each scene in REAL-
OUTDOOR."
REAL-WORLD SCENES,0.4864864864864865,"Figure 7 shows a comparison between ground
truth, our method (OSF), and Neural Re-
ﬂectance Fields (NRF) (Bi et al., 2020a). We
show that OSF recovers stronger, more accurate
specular highlights compared to NRF. OSF also
produces more detailed appearances (see pony
logo). This comparison demonstrates the main
advantage of OSF: the ability to handle complex
scattering functions."
REAL-WORLD SCENES,0.49099099099099097,"Ground Truth
OSF (Ours)
NRF (Bi et al.) [3] Pony"
REAL-WORLD SCENES,0.4954954954954955,"Figure 7: Comparison of OSF (ours) to Neu-
ral Reﬂectance Fields (NRF) (Bi et al., 2020a).
OSF produces stronger, more accurate specular
highlights on the legs (see zoomed view) and
recovers more detailed appearances (see pony
logo)."
REAL-WORLD SCENES,0.5,"For scene composition, the OSFs trained on each object are composed with a synthetic ﬂoor OSF
in Figure 8 row (a). Our method is able to compute accurate shadows, such as the shadow cast by
the pony onto the two other objects in the scene. The indirect reﬂections from the ﬂoor allow the
shadowed objects to be slightly visible as shown in the “OSF” panel."
REAL-WORLD SCENES,0.5045045045045045,"Figure 8 rows (b) and (c) show results on inserting REAL-NRF objects into real outdoor scenes
(REAL-OUTDOOR). Shadows and reﬂections are rendered with randomized lighting directions to
approximate the environment lighting. Our method accurately renders occlusions between the in-
serted objects and the vase in Figure 8 row (c). Due to the compositional nature of OSFs, we are
able to insert the learned pinecone from Figure 8 (b) into (c)."
REAL-WORLD SCENES,0.509009009009009,"In Figure 8, each column shows ablated versions of OSF to study the impact of computing shadows
and indirect illumination with our path tracing algorithm. “No Shadows, No Indirect” represents
a version of our model containing only direct illumination (without modeling inter-object lighting
effects). We additionally show “No Indirect” and “Indirect Only” variants of our model which"
REAL-WORLD SCENES,0.5135135135135135,Under review as a conference paper at ICLR 2022
REAL-WORLD SCENES,0.5180180180180181,"No Indirect
Indirect Only
Full Model
No Shadows, No Indirect (a) (b) (c)"
REAL-WORLD SCENES,0.5225225225225225,NeRF [21]
REAL-WORLD SCENES,0.527027027027027,"Our Method: Object-Centric Neural Scattering Functions (OSF)
NRF [3]"
REAL-WORLD SCENES,0.5315315315315315,"Figure 8: Real-world results. NRF (Bi et al., 2020a) and NeRF (Mildenhall et al., 2020) learn
on individual static scenes or objects. In contrast, we compose real-world objects and scenes using
OSFs. The objects are composed with a (a) synthetic ﬂoor and (b, c) real outdoor scenes from REAL-
OUTDOOR. Columns show different ablated versions of our model: “No Shadows, No Indirect”
which considers only direct illumination; “No Indirect” which includes both direct illumination and
shadows; “Indirect Only” which considers only indirect illumination. Our OSFs show the most
realistic renderings, with accurate shadows (e.g., pony shadowing the two other objects (row a) and
indirect illumination (i.e., the ground and environment illuminating the objects)."
REAL-WORLD SCENES,0.536036036036036,"represent computing shadows and indirect illumination, respectively. As illustrated by Figure 8, our
full model containing both shadows and indirect illumination effects is the most realistic. Additional
results on real-world scenes, including complex shadows, can be found in Appendix A."
DISCUSSION,0.5405405405405406,"6
DISCUSSION"
DISCUSSION,0.545045045045045,"We have proposed Object-Centric Neural Scattering Functions (OSFs), a method that enables com-
posing objects captured only from photographs into photorealistic renderings of dynamic scenes.
We demonstrated that decomposing a scene into implicit object functions that are view- and light-
dependent enables reusabiliy of objects across scenes where objects, camera, and lighting can
change. We presented a method for integrating our learned implicit functions with volumetric path
tracing, and showed inter-object light transport effects such as shadow and indirect illumination for
real-world objects where no computer graphics model is available. We believe our work is a step
towards a graphics pipeline where real-world scenes are modeled by a composition of implicit func-
tions to combine the ﬂexibility of object-centric neural modeling with the photorealism of graphics
rendering algorithms."
DISCUSSION,0.5495495495495496,"There are a few main limitations to OSF. First, the computational complexity of our method is high,
but there are several works tackling the orthogonal issue of improving NeRF efﬁciency (as discussed
in Section 4.2) that can easily be applied to OSFs. Second, while learning intra-object light transport
means that intra-object path tracing is not needed, this formulation assumes that at test time, there
are no occluders or light sources that intrude the object’s convex hull (Sloan et al., 2002) (e.g., a
person sitting in a chair). However, OSFs can still be rendered even if their bounding boxes are
intersecting, as long as this assumption is not violated. Finally, acquiring datasets of real world
objects with varying point light sources and viewpoints is challenging, but we hope that in the future
such acquisition of real world datasets will become easier to capture and more widely available."
REPRODUCIBILITY STATEMENT,0.5540540540540541,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.5585585585585585,"We describe our method (Section 4) and experimental setup (Section 5) in detail to maximize repro-
ducibility. We will release our code upon publication to facilitate future research."
REPRODUCIBILITY STATEMENT,0.5630630630630631,Under review as a conference paper at ICLR 2022
REFERENCES,0.5675675675675675,REFERENCES
REFERENCES,0.5720720720720721,"Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. Neural
point-based graphics. European Conference on Computer Vision, 2020."
REFERENCES,0.5765765765765766,"Bruno Arnaldi, Xavier Pueyo, and Josep Vilaplana. On the division of environments by virtual walls
for radiosity computation.
In Photorealistic Rendering in Computer Graphics, pp. 198–205.
Springer, 1994."
REFERENCES,0.581081081081081,"Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Miloˇs Haˇsan, Yannick
Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Neural reﬂectance ﬁelds for appearance
acquisition. arXiv preprint arXiv:2008.03824, 2020a."
REFERENCES,0.5855855855855856,"Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Miloˇs Haˇsan, Yannick Hold-Geoffroy, David Kriegman,
and Ravi Ramamoorthi. Deep reﬂectance volumes: Relightable reconstructions from multi-view
photometric images. European Conference on Computer Vision, 2020b."
REFERENCES,0.5900900900900901,"Blender Foundation. Blender - a 3d modelling and rendering package. http://www.blender.
org, 1994."
REFERENCES,0.5945945945945946,"Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Generative and discriminative
voxel modeling with convolutional neural networks. arXiv preprint arXiv:1608.04236, 2016."
REFERENCES,0.5990990990990991,"Oliver Cossairt, Shree Nayar, and Ravi Ramamoorthi.
Light ﬁeld transfer: global illumination
between real and synthetic objects. ACM Transactions on Graphics, 27(3):1–6, 2008."
REFERENCES,0.6036036036036037,"Frank Dellaert and Lin Yen-Chen. Neural volume rendering: Nerf and beyond. arXiv preprint
arXiv:2101.05204, 2020."
REFERENCES,0.6081081081081081,"Philip Dutre, Philippe Bekaert, and Kavita Bala. Advanced global illumination. CRC Press, 2018."
REFERENCES,0.6126126126126126,"John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. Deepstereo: Learning to predict new
views from the world’s imagery. In Conference on Computer Vision and Pattern Recognition, pp.
5515–5524, 2016."
REFERENCES,0.6171171171171171,"Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge
university press, 2003."
REFERENCES,0.6216216216216216,"Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Bros-
tow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics,
37(6):1–15, 2018."
REFERENCES,0.6261261261261262,"James T Kajiya. The rendering equation. In Conference on Computer Graphics and Interactive
Techniques, pp. 143–150, 1986."
REFERENCES,0.6306306306306306,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.6351351351351351,"Joe Kniss, Simon Premoze, Charles Hansen, Peter Shirley, and Allen McPherson. A model for
volume lighting and modeling. IEEE Transactions on Visualization and Computer Graphics, 9
(2):150–162, 2003."
REFERENCES,0.6396396396396397,"Marc Levoy. Efﬁcient ray tracing of volume data. ACM Transactions on Graphics, 9(3):245–261,
1990."
REFERENCES,0.6441441441441441,"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel
ﬁelds. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.6486486486486487,"Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser
Sheikh. Neural volumes: learning dynamic renderable volumes from images. ACM Transactions
on Graphics, 38(4):1–14, 2019."
REFERENCES,0.6531531531531531,"Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Julien
Valentin, Sameh Khamis, Philip Davidson, Anastasia Tkach, Peter Lincoln, et al. Lookingood:
Enhancing performance capture with real-time neural re-rendering. ACM Transactions on Graph-
ics, 2018."
REFERENCES,0.6576576576576577,Under review as a conference paper at ICLR 2022
REFERENCES,0.6621621621621622,"Nelson Max. Optical models for direct volume rendering. IEEE Transactions on Visualization and
Computer Graphics, 1(2):99–108, 1995."
REFERENCES,0.6666666666666666,"Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely,
and Ricardo Martin-Brualla. Neural rerendering in the wild. In Conference on Computer Vision
and Pattern Recognition, pp. 6878–6887, 2019."
REFERENCES,0.6711711711711712,"Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ra-
mamoorthi, Ren Ng, and Abhishek Kar. Local light ﬁeld fusion: Practical view synthesis with
prescriptive sampling guidelines. ACM Transactions on Graphics, 38(4):1–14, 2019."
REFERENCES,0.6756756756756757,"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In European
Conference on Computer Vision, 2020."
REFERENCES,0.6801801801801802,"Gavin Miller. Efﬁcient algorithms for local and global accessibility shading. In Conference on
Computer Graphics and Interactive Techniques, pp. 319–326, 1994."
REFERENCES,0.6846846846846847,"Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan:
Unsupervised learning of 3d representations from natural images. In International Conference on
Computer Vision, pp. 7588–7597, 2019."
REFERENCES,0.6891891891891891,"Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-Liang Yang, and Niloy Mitra. Blockgan:
Learning 3d object-aware scene representations from unlabelled images. Advances in Neural
Information Processing Systems, 2020."
REFERENCES,0.6936936936936937,"Thu H Nguyen-Phuoc, Chuan Li, Stephen Balaban, and Yongliang Yang. Rendernet: A deep convo-
lutional network for differentiable rendering from 3d shapes. In Advances in Neural Information
Processing Systems, pp. 7891–7901, 2018."
REFERENCES,0.6981981981981982,"Jan Nov´ak, Iliyan Georgiev, Johannes Hanika, and Wojciech Jarosz. Monte carlo methods for volu-
metric light transport simulation. In Computer Graphics Forum, volume 37, pp. 551–576. Wiley
Online Library, 2018."
REFERENCES,0.7027027027027027,"Michael Oechsle, Michael Niemeyer, Lars Mescheder, Thilo Strauss, and Andreas Geiger. Learning
implicit surface light ﬁelds. International Conference on 3D Vision, 2020."
REFERENCES,0.7072072072072072,"Julien Philip, Micha¨el Gharbi, Tinghui Zhou, Alexei A Efros, and George Drettakis. Multi-view
relighting using a geometry-aware network. ACM Transactions on Graphics, 38(4):1–14, 2019."
REFERENCES,0.7117117117117117,"Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Confer-
ence on Machine Learning, pp. 5301–5310. PMLR, 2019."
REFERENCES,0.7162162162162162,"Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural
radiance ﬁelds with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV), pp. 14335–14345, October 2021."
REFERENCES,0.7207207207207207,"Konstantinos Rematas and Vittorio Ferrari. Neural voxel renderer: Learning an accurate and control-
lable rendering tool. In Conference on Computer Vision and Pattern Recognition, pp. 5417–5427,
2020."
REFERENCES,0.7252252252252253,"Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A
Platform for Embodied AI Research. In International Conference on Computer Vision, 2019."
REFERENCES,0.7297297297297297,"Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael
Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In Conference on Computer
Vision and Pattern Recognition, pp. 2437–2446, 2019a."
REFERENCES,0.7342342342342343,"Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wetzstein. Scene representation networks: Con-
tinuous 3d-structure-aware neural scene representations. In Advances in Neural Information Pro-
cessing Systems, pp. 1121–1132, 2019b."
REFERENCES,0.7387387387387387,Under review as a conference paper at ICLR 2022
REFERENCES,0.7432432432432432,"Peter-Pike Sloan, Jan Kautz, and John Snyder. Precomputed radiance transfer for real-time rendering
in dynamic, low-frequency lighting environments. In Conference on Computer Graphics and
Interactive Techniques, pp. 527–536, 2002."
REFERENCES,0.7477477477477478,"Pratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng, and Noah
Snavely. Pushing the boundaries of view extrapolation with multiplane images. In Conference on
Computer Vision and Pattern Recognition, pp. 175–184, 2019."
REFERENCES,0.7522522522522522,"Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe,
Christoph Rhemann, Jay Busch, Paul E Debevec, and Ravi Ramamoorthi. Single image portrait
relighting. ACM Transactions on Graphics, 38(4):79–1, 2019."
REFERENCES,0.7567567567567568,"Justus Thies, Michael Zollh¨ofer, and Matthias Nießner. Deferred neural rendering: Image synthesis
using neural textures. ACM Transactions on Graphics, 38(4):1–12, 2019."
REFERENCES,0.7612612612612613,"Bill Triggs, Philip F McLauchlan, Richard I Hartley, and Andrew W Fitzgibbon.
Bundle ad-
justment—a modern synthesis. In International Workshop on Vision Algorithms, pp. 298–372.
Springer, 1999."
REFERENCES,0.7657657657657657,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998–6008, 2017."
REFERENCES,0.7702702702702703,"Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In Asilomar Conference on Signals, Systems & Computers, 2003, volume 2, pp.
1398–1402. IEEE, 2003."
REFERENCES,0.7747747747747747,"Fei Xia, William B Shen, Chengshu Li, Priya Kasimbeg, Micael Edmond Tchapmi, Alexander
Toshev, Roberto Mart´ın-Mart´ın, and Silvio Savarese. Interactive gibson benchmark: A benchmark
for interactive navigation in cluttered environments. IEEE Robotics and Automation Letters, 5(2):
713–720, 2020."
REFERENCES,0.7792792792792793,"Zexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi Ramamoorthi. Deep image-based relighting
from optimal sparse samples. ACM Transactions on Graphics, 37(4):1–13, 2018."
REFERENCES,0.7837837837837838,"Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Conference on Computer Vision and
Pattern recognition, pp. 586–595, 2018."
REFERENCES,0.7882882882882883,"Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue, Rohit Pandey, Sergio
Orts-Escolano, Philip Davidson, Christoph Rhemann, Paul Debevec, et al. Neural light transport
for relighting and view synthesis. ACM Transactions on Graphics, 2021."
REFERENCES,0.7927927927927928,"Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David W Jacobs. Deep single-image portrait re-
lighting. In International Conference on Computer Vision, pp. 7194–7202, 2019."
REFERENCES,0.7972972972972973,"Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magniﬁcation:
learning view synthesis using multiplane images. ACM Transactions on Graphics, 37(4):1–12,
2018."
REFERENCES,0.8018018018018018,Under review as a conference paper at ICLR 2022
REFERENCES,0.8063063063063063,"A
REAL-WORLD SCENE COMPOSITION"
REFERENCES,0.8108108108108109,"Different scene conﬁgurations of composed objects from REAL-NRF are shown in Figure 9. We
show the effect of moving the light, camera, or objects. Notice how the the appearance and shadows
of the objects are updated across different scene conﬁgurations. Also notice that even when parts
of the palm tree object and the cartoon object are cast under the pony’s shadow, they do not appear
completely dark due to the indirect illumination from the ﬂoor."
REFERENCES,0.8153153153153153,"Analyzing the effect of different numbers of indirect (secondary) rays per primary sample, Figure 10
shows the result. As can be seen from the ﬁgure, the noisiness of the indirect illumination render
decreases as the number of samples increase. Results in this paper contain between one and ﬁve
randomly sampled secondary ray for each primary ray sample."
REFERENCES,0.8198198198198198,"Move Camera
Move Light
Move Objects"
REFERENCES,0.8243243243243243,"Figure 9: Composing real-world objects from REAL-NRF using our OSF method. We demonstrate
the effect of moving the light, camera, or objects. Note how the appearance and shadows of the ob-
jects are updated across different scene conﬁgurations. Also notice that even when parts of the palm
tree object and the cartoon object are cast under the pony’s shadow, they do not appear completely
dark due to the indirect illumination from the ﬂoor."
REFERENCES,0.8288288288288288,"Figure 10: Visualizing the effect of different numbers of indirect (secondary) rays (N) per primary
sample for our OSF model (the brightness of these images has been increased only for visualization
purposes). Note that the noisiness of the render decreases as N increases. We ﬁnd that we are able
to achieve relatively non-noisy results with approximately ﬁve samples."
REFERENCES,0.8333333333333334,Under review as a conference paper at ICLR 2022
REFERENCES,0.8378378378378378,"Single-object renderings from REAL-NRF are shown in Figure 11. The objects were captured in a
dark room with a one-light-at-a-time setup. After training OSF on each object in this dataset, we are
able to render the objects from novel viewpoints and lighting directions."
REFERENCES,0.8423423423423423,"Move Light
Move Camera
Original"
REFERENCES,0.8468468468468469,"Figure 11: Learned OSFs on objects from REAL-NRF. The objects were captured in a dark room
with a one-light-at-a-time setup. After training OSF on each object in this dataset, we are able to
render the objects from novel viewpoints and lighting directions."
REFERENCES,0.8513513513513513,Under review as a conference paper at ICLR 2022
REFERENCES,0.8558558558558559,"B
ABLATION EXPERIMENTS"
REFERENCES,0.8603603603603603,"Direct + Shadows
Direct Only
Indirect Only
OSF (Ours)"
REFERENCES,0.8648648648648649,Figure 12: Ablation results on our OSF model.
REFERENCES,0.8693693693693694,"Figure 12 shows ablation results on FURNITURE-REALISTIC. We evaluate different variants of our
model: “Direct Only” which considers only direct illumination; “Indirect Only” which considers
only indirect illumination; “Direct + Shadows” which includes both direct illumination and shad-
ows. Our full model (OSF) shows the most realistic rendering, with accurate shadows and indirect
illumination effects such as the left side of the couches and tables appearing brighter due to indirect
lighting from the left wall. Note that the white area on the right of the images represent rays with
zero density that are composited onto a white background (and therefore do not contribute indirect
illumination to the scene). (a) (b)"
REFERENCES,0.8738738738738738,"o-NeRF + S
o-NeRF
OSF (ours)
Ground Truth"
REFERENCES,0.8783783783783784,"Figure 13: Comparisons on scene composition on FURNITURE-REALISTIC. The models OSF, o-
NeRF, and o-NeRF + S are explained in §5. Compared to o-NeRF, our model (OSF) is able to
disentangle lighting-dependent appearance from view-dependent appearance for individual objects,
and is able to render shadows cast by objects onto the ground correctly."
REFERENCES,0.8828828828828829,Under review as a conference paper at ICLR 2022
REFERENCES,0.8873873873873874,"C
COMPLEX ILLUMINATION"
REFERENCES,0.8918918918918919,"In this experiment, we investigate how scenes composed of OSF objects can be rendered with com-
plex illumination from an environment map."
REFERENCES,0.8963963963963963,"Speciﬁcally, we apply the combination of a point
light source and the environment map shown in
the top-left corner of Figure 14 to light one of
our scenes in FURNITURE-REALISTIC.
This
simulates the appearance of the scene as if the
scene were inserted into a complex lighting
environment, which stresses the beneﬁts of the
OSF path tracing framework."
REFERENCES,0.9009009009009009,"For each OSF sample point, we project the
equirectangular coordinates of the environment
map into spherical coordinates, sample 20 direc-
tions on the unit sphere uniformly at random,
evaluate the OSF function for each incoming di-
rection, and integrate them outgoing radiance us-
ing Equation 5. Please note that a green-blue tint
is slightly apparent in the scene rendering, due to
the contribution of green and blue lighting from
the environment map.
Figure 14: Complex illumination results."
REFERENCES,0.9054054054054054,Under review as a conference paper at ICLR 2022
REFERENCES,0.9099099099099099,"D
IMPLEMENTATION DETAILS"
REFERENCES,0.9144144144144144,A ﬂowchart of our method is shown in Figure 15.
REFERENCES,0.918918918918919,"We approximate our model FΘ with a multilayer perception (MLP) with rectiﬁed linear activations.
The predicted density σ is view-invariant, while the scattering function value ρ is dependent on the
incoming and outgoing light directions. We use an eight-layer MLP with 256 channels to predict
σ, and a four-layer MLP with 128 channels to predict ρ. For positional encoding, we use W = 10
to encode the position x and W = 4 to encode the incoming and outgoing directions (ωl, ωo),
where W is the highest frequency level. To avoid ρ from saturating in training, we adopt a scaled
sigmoid (Brock et al., 2016) deﬁned as S′(ρ) = δ(S(ρ) −0.5) + 0.5 with δ = 1.2. We use a batch
size of 4,096 rays. rays o1 oN ..."
REFERENCES,0.9234234234234234,eval_object
REFERENCES,0.9279279279279279,eval_object
REFERENCES,0.9324324324324325,"t, rgb, α"
REFERENCES,0.9369369369369369,"t, rgb, α"
REFERENCES,0.9414414414414415,"...
sort_by_t
integrate
RGB, trans"
REFERENCES,0.9459459459459459,eval_object
REFERENCES,0.9504504504504504,"rays
T-1
intersect oi D"
REFERENCES,0.954954954954955,"sample
t, x
OSF( x, wi, wo )
scatter
t, rgb, α"
REFERENCES,0.9594594594594594,intersect_idxs
REFERENCES,0.963963963963964,secondary
REFERENCES,0.9684684684684685,"rays
light 
direction"
REFERENCES,0.972972972972973,render
REFERENCES,0.9774774774774775,"render
RGB, trans"
REFERENCES,0.9819819819819819,"rgb, α"
REFERENCES,0.9864864864864865,eval_secondary
REFERENCES,0.990990990990991,Figure 15: Flowchart of our method. See §4 for more details.
REFERENCES,0.9954954954954955,"For synthetic datasets, we sample Nc = 64 coarse samples and Nf = 128 ﬁne samples per ray. For
real world datasets, we sample Nc = 64 coarse samples and Nf = 64 ﬁne samples per ray. We use
the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 0.001, β1 = 0.9, β2 = 0.999, and
ϵ = 10−7."
