Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0056179775280898875,"For embodied reinforcement learning (RL) agents interacting with the environment,
it is desirable to have rapid policy adaptation to unseen visual observations, but
achieving zero-shot adaptation capability is considered as a challenging problem
in the RL context. To address the problem, we present a novel contrastive prompt
ensemble (CONPE) framework which utilizes a pretrained vision-language model
and a set of visual prompts, thus enabling efficient policy learning and adaptation
upon a wide range of environmental and physical changes encountered by embod-
ied agents. Specifically, we devise a guided-attention-based ensemble approach
with multiple visual prompts on the vision-language model to construct robust state
representations. Each prompt is contrastively learned in terms of an individual
domain factor that significantly affects the agent’s egocentric perception and obser-
vation. For a given task, the attention-based ensemble and policy are jointly learned
so that the resulting state representations not only generalize to various domains
but are also optimized for learning the task. Through experiments, we show that
CONPE outperforms other state-of-the-art algorithms for several embodied agent
tasks including navigation in AI2THOR, manipulation in egocentric-Metaworld,
and autonomous driving in CARLA, while also improving the sample efficiency of
policy learning and adaptation."
INTRODUCTION,0.011235955056179775,"1
Introduction"
INTRODUCTION,0.016853932584269662,"In the literature of vision-based reinforcement learning (RL), with the advance of unsupervised
techniques and large-scale pretrained models for computer vision, the decoupled structure, in which
visual encoders are separately trained and used later for policy learning, has gained popularity [1,
2, 3]. This decoupling demonstrates high efficiency in low data regimes with sparse reward signals,
compared to end-to-end RL. In this regard, several works on adopting the decoupled structure to
embodied agents interacting with the environment were introduced [4, 5], and specifically, pretrained
vision models (e.g., ResNet in [6]) or vision-language models (e.g., CLIP in [7, 8]) were exploited
for visual state representation encoders. Yet, it is non-trivial to achieve zero-shot adaptation to visual
domain changes in the environment with high diversity and non-stationarity, which are inherent for
embodied agents. It was rarely investigated how to optimize those popular large-scale pretrained
models to ensure the zero-shot capability of embodied agents."
INTRODUCTION,0.02247191011235955,"Embodied agents have several environmental and physical properties, such as egocentric camera
position, stride length, and illumination, which are domain factors making significant changes in
agents’ perception and observation. In the target (deployment) environment with uncalibrated settings
on those domain factors, RL policies relying on pretrained visual encoders remain vulnerable to
domain changes."
INTRODUCTION,0.028089887640449437,∗Honguk Woo is the corresponding author.
INTRODUCTION,0.033707865168539325,"Source Environment
Target Environments"
INTRODUCTION,0.03932584269662921,Figure 1: Visual Domain Changes of Embodied Agents
INTRODUCTION,0.0449438202247191,"Figure 1 provides an example of egocen-
tric visual domain changes experienced by
embodied agents due to different camera po-
sitions. When policies learned in the source
environment are applied to the target en-
vironment, zero-shot performance can be
significantly degraded, unless the visual en-
coder could adapt not only to environmental
differences but also to the physical diversity
of agents. In this paper, we investigate RL policy adaptation techniques for embodied agents to
enable zero-shot adaptation to domain changes, by leveraging prompt-based learning for pretrained
models in the decoupled RL structure. To this end, we present CONPE, a novel contrastive prompt
ensemble framework that uses the CLIP vision-language model as the visual encoder, and facilitates
dynamic adjustments of visual state representations against domain changes through an ensemble
of contrastively learned visual prompts. In CONPE, the ensemble employs attention-based state
composition on multiple visual embeddings from the same input observation, where each embedding
corresponds to a state representation individually prompted for a specific domain factor. Specifically,
the cosine similarity between an input observation and its respective prompted embeddings is used to
calculate attention weights effectively."
INTRODUCTION,0.05056179775280899,"Through experiments, we demonstrate the benefits of our approach. First, RL policies learned via
CONPE achieve competitive zero-shot performance upon a wide variety of egocentric visual domain
variations for several embodied agent tasks, such as navigation tasks in AI2THOR [9], vision-based
robot manipulation tasks in egocentric-Metaworld, and autonomous driving tasks in CARLA [10].
For instance, the policy via CONPE outperforms EmbCLIP [7] in zero-shot performance by 20.7%
for unseen target domains in the AI2THOR object navigation. Second, our approach achieves high
sample-efficiency in the decoupled RL structure. For instance, CONPE requires less than 50.0%
and 16.7% of the samples compared to ATC [1] and 60% and 50% of the samples compared to
EmbCLIP to achieve comparable performance in seen and unseen target domains in the AI2THOR
object navigation."
INTRODUCTION,0.056179775280898875,"In the context of RL, our work is the first to explore policy adaptation using visual prompts for
embodied agents, achieving superior zero-shot performance and high sample-efficiency. The main
contributions of our work are as follows."
INTRODUCTION,0.06179775280898876,"• We present a novel CONPE framework with an ensemble of visual contrastive prompts,
which enables zero-shot adaptation for vision-based embodied RL agents."
INTRODUCTION,0.06741573033707865,"• We devise visual prompt-based contrastive learning and guided-attention-based prompt
ensemble algorithms to represent task-specific information in the CLIP embedding space."
INTRODUCTION,0.07303370786516854,"• We experimentally show that policies via CONPE achieve comparable or superior zero-
shot performance, compared to other state-of-the-art baselines, for several tasks. We also
demonstrate high sample-efficiency in policy learning and adaptation."
INTRODUCTION,0.07865168539325842,"• We create the datasets with various visual domains in AI2THOR, egocentric-Metaworld and
CARLA, and make them publicly accessible for further research on RL policy adaptation."
PROBLEM FORMULATION,0.08426966292134831,"2
Problem Formulation"
PROBLEM FORMULATION,0.0898876404494382,"In RL formulation, a learning environment is defined as a Markov decision process (MDP) of
(S, A, P, R) with state space s ∈S, action space a ∈A, transition probability P : S × A →S
and reward function R : S × A →R. The objective of RL is to find an optimal policy π∗: S →
A maximizing the sum of discounted rewards. For embodied agents, states might not be fully
observable, and the environment is represented by a partially observable MDP (POMDP) of a tuple
(S, A, P, R, Ω, O) with an observation space o ∈Ωand a conditional observation probability [11]
O : S × A →Ω."
PROBLEM FORMULATION,0.09550561797752809,"Given visual domains in the dynamic environment, we consider policy adaptation to find the optimal
policy that remains invariant across the domains or is transferable to some target domain, where each
domain is represented by a POMDP and domain changes are formulated by different O. We denote
domains as D = (Ω, O). Aiming to enable zero-shot adaptation to various domains, we formulate"
PROBLEM FORMULATION,0.10112359550561797,(i) Prompt-based Contrastive Learning
PROBLEM FORMULATION,0.10674157303370786,"Visual Prompt Pool, 𝐩𝑣 ⋮"
PROBLEM FORMULATION,0.11235955056179775,"Brightness
𝑝1"
PROBLEM FORMULATION,0.11797752808988764,"𝑣
CAM
𝑝2"
PROBLEM FORMULATION,0.12359550561797752,𝑣Stride Length 𝑝3 𝑣
PROBLEM FORMULATION,0.12921348314606743,"CLIP 𝒯𝜙(𝑜, 𝑝𝑖 𝑣) 𝓏𝑘 𝓏𝑘"
PROBLEM FORMULATION,0.1348314606741573,"′
𝓏𝑖≠𝑘 ′ 𝑝𝑖"
PROBLEM FORMULATION,0.1404494382022472,𝑣domain factor
PROBLEM FORMULATION,0.14606741573033707,Contrast Function 𝓏𝑘 ′
PROBLEM FORMULATION,0.15168539325842698,"Pos.
Neg.
𝓏𝑘
𝓏𝑖≠𝑘 ′ ⋮ +"
PROBLEM FORMULATION,0.15730337078651685,(ii) Guided-Attention-based Prompt Ensemble
PROBLEM FORMULATION,0.16292134831460675,Source env. ⋮
PROBLEM FORMULATION,0.16853932584269662,Prompt Ensemble 𝒢
PROBLEM FORMULATION,0.17415730337078653,"Embeding Space 𝒵 𝓏0 𝓏1 𝓏2 𝓏3 𝓏4
𝓏5"
PROBLEM FORMULATION,0.1797752808988764,Guided attention weights
PROBLEM FORMULATION,0.1853932584269663,Policy 𝜋
PROBLEM FORMULATION,0.19101123595505617,(iii) Zero-shot Policy Deployment
PROBLEM FORMULATION,0.19662921348314608,Target env.
PROBLEM FORMULATION,0.20224719101123595,Unseen
PROBLEM FORMULATION,0.20786516853932585,"visual 
variation"
PROBLEM FORMULATION,0.21348314606741572,"Unseen
physical
property"
PROBLEM FORMULATION,0.21910112359550563,Domain generalization
PROBLEM FORMULATION,0.2247191011235955,Policy 𝜋
PROBLEM FORMULATION,0.2303370786516854,Contrastive Learning
PROBLEM FORMULATION,0.23595505617977527,action
PROBLEM FORMULATION,0.24157303370786518,"Figure 2: CONPE Framework. The CLIP visual encoder is enhanced offline via (i) prompt-based
contrastive learning that generates the visual prompt pool, and a policy is learned online by (ii)
guided-attention-based prompt ensemble that uses the prompt pool. In (iii) zero-shot deployment, the
policy is immediately evaluated upon domain changes."
PROBLEM FORMULATION,0.24719101123595505,the policy adaptation problem as finding the optimal policy π∗such that
PROBLEM FORMULATION,0.25280898876404495,"π∗= argmax
π """
PROBLEM FORMULATION,0.25842696629213485,"E
D∼p(D) "" ∞
X"
PROBLEM FORMULATION,0.2640449438202247,"t=1
γtR(st, π(ot)) ## (1)"
PROBLEM FORMULATION,0.2696629213483146,where p(D) is a given domain distribution and γ is a discount factor of the environment.
PROBLEM FORMULATION,0.2752808988764045,"For embodied agents, the same state can be differently observed depending on the configuration of
properties such as egocentric camera position, stride length, illumination, and object style. We refer
to such a property causing domain changes in the environment as a domain factor. Practical scenarios
often involve the interplay of multiple domain factors in the environment."
OUR APPROACH,0.2808988764044944,"3
Our Approach"
FRAMEWORK STRUCTURE,0.28651685393258425,"3.1
Framework Structure"
FRAMEWORK STRUCTURE,0.29213483146067415,"To enable zero-shot policy adaptation to unseen domains, we develop the CONPE framework consist-
ing of (i) prompt-based contrastive learning with the CLIP visual encoder, (ii) guided-attention-based
prompt ensemble, and (iii) zero-shot policy deployment, as illustrated in Figure 2. The capability
of the CLIP visual encoder is enhanced using multiple visual prompts that are contrastively learned
on expert demonstrations for several domain factors. This establishes the visual prompt pool in
(i). Then, the prompts are used to train the guided-attention-based ensemble with the environment
in (ii). To enhance learning efficiency and interpretability of attention weights, we use the cosine
similarity of embeddings. The attention module and policy are jointly learned for a specific task so
that resulting state representations tend to generalize across various domains and be optimized for task
learning. In deployment, a non-stationary environment where its visual domain varies according to the
environment conditions and agent physical properties is considered, and the zero-shot performance is
evaluated in (iii)."
PROMPT-BASED CONTRASTIVE LEARNING,0.29775280898876405,"3.2
Prompt-based Contrastive Learning"
PROMPT-BASED CONTRASTIVE LEARNING,0.30337078651685395,"To construct domain-invariant representations with respect to a specific domain factor for egocentric
perception data, we adopt several contrastive tasks for visual prompt learning, which can be learned
on a few expert demonstrations. For this, we use a visual prompt"
PROMPT-BASED CONTRASTIVE LEARNING,0.3089887640449438,"pv = [ev
1, ev
2, ..., ev
u], ev
i ∈Rd
(2)"
PROMPT-BASED CONTRASTIVE LEARNING,0.3146067415730337,"where ev
i is a continuous learnable vector with the image patch embedding dimension d (e.g.,
768 for CLIP visual encoder) and u is the length of a visual prompt. Let a pretrained model Tϕ
parameterized by ϕ maps observations o ∈Ωto the embedding space Z. With a contrast function
P : Ω× Ω→{0, 1} [1, 2, 12] to discriminate whether an observation pair is positive or not, 𝓏0 ⋯"
PROMPT-BASED CONTRASTIVE LEARNING,0.3202247191011236,Embeddings 𝒛
PROMPT-BASED CONTRASTIVE LEARNING,0.3258426966292135,"Attention Module 𝓖𝓏0, 𝒛"
PROMPT-BASED CONTRASTIVE LEARNING,0.33146067415730335,"Visual encoder 𝒯𝜙(𝑜, 𝑝𝑖 𝑣) ⋯ 〮"
PROMPT-BASED CONTRASTIVE LEARNING,0.33707865168539325,"⋯
𝑂
𝑝1"
PROMPT-BASED CONTRASTIVE LEARNING,0.34269662921348315,"𝑣
𝑂
𝑝2"
PROMPT-BASED CONTRASTIVE LEARNING,0.34831460674157305,"𝑣
𝑂
𝑝𝑛𝑣
𝑂"
PROMPT-BASED CONTRASTIVE LEARNING,0.3539325842696629,"𝑔1
𝑔2
𝑔𝑛
⋯"
PROMPT-BASED CONTRASTIVE LEARNING,0.3595505617977528,Task-specific state representation Z
PROMPT-BASED CONTRASTIVE LEARNING,0.3651685393258427,"Policy 𝜋(𝑍) ⋯ 𝑝3 𝑣
𝑂 𝜔𝑛 𝑢𝑛〮"
PROMPT-BASED CONTRASTIVE LEARNING,0.3707865168539326,"𝑚𝑙𝑝
𝑔3 𝜔5 𝑢5 𝑚𝑙𝑝 〮
⋯ 𝜔1 𝑚𝑙𝑝 〮
𝑢1 𝑚𝑙𝑝 𝜔2 〮
𝑢2"
PROMPT-BASED CONTRASTIVE LEARNING,0.37640449438202245,"𝓏1
𝓏𝑛
𝓏2
⋯
𝓏3
𝓏4
𝓏5"
PROMPT-BASED CONTRASTIVE LEARNING,0.38202247191011235,"𝓏1
𝓏𝑛
𝓏2
⋯
𝓏3
𝓏4
𝓏5"
PROMPT-BASED CONTRASTIVE LEARNING,0.38764044943820225,"𝓏1
𝓏𝑛
𝓏2
⋯
𝓏3
𝓏4
𝓏5 𝑝4"
PROMPT-BASED CONTRASTIVE LEARNING,0.39325842696629215,"𝑣
𝑂
𝑝5 𝑣
𝑂 𝑔5
𝑔4 𝜔3 𝑢3 𝑚𝑙𝑝 〮 𝜔4 𝑢4 𝑚𝑙𝑝 〮"
PROMPT-BASED CONTRASTIVE LEARNING,0.398876404494382,softmax
PROMPT-BASED CONTRASTIVE LEARNING,0.4044943820224719,"Figure 3: Guided-Attention-based Prompt Ensemble. The cosine similarity-guided attention module
G yields task-specific state representations from multiple prompted embeddings and is learned with a
policy network π."
PROMPT-BASED CONTRASTIVE LEARNING,0.4101123595505618,"consider an m-sized batch of observation pairs BP = {(oi, o′
i)}i≤m containing one positive pair
{(ok, o′
k)|P(ok, o′
k) = 1} for some k ≤m. Then, we enhance the capability of Tϕ by learning a
visual prompt pv through contrastive learning, where the contrastive loss function [13] is defined as"
PROMPT-BASED CONTRASTIVE LEARNING,0.4157303370786517,"LCON(pv, BP ) = −log"
PROMPT-BASED CONTRASTIVE LEARNING,0.42134831460674155,"S(Tϕ(ok, pv), Tϕ(o′
k, pv))
P"
PROMPT-BASED CONTRASTIVE LEARNING,0.42696629213483145,"i̸=k S(Tϕ(oi, pv), Tϕ(o′
i, pv)) !"
PROMPT-BASED CONTRASTIVE LEARNING,0.43258426966292135,", S(x, y) = 1"
PROMPT-BASED CONTRASTIVE LEARNING,0.43820224719101125,"λ exp
 ⟨x, y⟩"
PROMPT-BASED CONTRASTIVE LEARNING,0.4438202247191011,∥x∥∥y∥
PROMPT-BASED CONTRASTIVE LEARNING,0.449438202247191,"
. (3)"
PROMPT-BASED CONTRASTIVE LEARNING,0.4550561797752809,"As in [14], for latent vectors x, y ∈Z, their similarity in the embedding space Z is calculated by
S(x, y), where λ is a hyperparameter. By conducting the prompt-based contrastive learning on n
different domain factors, we obtain a visual prompt pool"
PROMPT-BASED CONTRASTIVE LEARNING,0.4606741573033708,"pv = [pv
1, pv
2, ..., pv
n].
(4)"
PROMPT-BASED CONTRASTIVE LEARNING,0.46629213483146065,"Through this process, each visual prompt in pv encapsulates domain-invariant knowledge pertinent
to its respective domain factor."
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.47191011235955055,"3.3
Guided-Attention-based Prompt Ensemble"
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.47752808988764045,"To effectively integrate individual prompted embeddings from multiple visual prompts into a task-
specific state representation, we devise a guided-attention-based prompt ensemble structure, as shown
in Figure 3 where the attention weights on the embeddings are dynamically computed via the attention
module G for each observation."
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.48314606741573035,"Given observation o and the learned visual prompt pool pv, an image embedding z0 = Tϕ(o) and
prompted embeddings z = [z1 = Tϕ(o, pv
1), ..., zn] are calculated. Then, zo and z are fed to the
attention module G, where attention weights ωi for each prompted embedding zi are optimized. Since
directly computing the attention weights using z0 and z is prone to have an uninterpretable local
optima, we introduce a guidance score gi based on the cosine similarity between the input image and
visual prompted image embeddings in Z, i.e., gi =
⟨z0,zi⟩
∥z0∥∥zi∥. Given that larger gi signifies a stronger
conformity of an observation to the domain factor relevant to the prompted embedding zi, we use
gi to steer the attention weights, aiming to not only improve learning efficiency but also provide
interpretability. With guidance gi, we compute the attention weights ωi by"
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.4887640449438202,"ωi =
exp(ui/τ)
P"
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.4943820224719101,"k exp(uk/τ),
ui = ⟨z0, ki⟩
√"
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.5,"d
gi
(5)"
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.5056179775280899,"Algorithm 1 Procedure of CONPE Framework
Dataset D = {(o1, o′
1), ...}, replay buffer ZD ←∅, pretrained vision-language model Tϕ
Visual prompt pool pv = [pv
1, ..., pv
n], attention module G, policy π
1: /* Prompt-based Contrastive Learning */
2: for i = 1, ..., n do
3:
while not converge do
4:
Sample a batch BPi = {(oj, o′
j)}j≤m ∼D
5:
Update prompt pv
i ←pv
i −∇LCON(pv
i , BPi) using (3)
6:
end while
7: end for
8: /* Prompt Ensemble-based Policy Learning */
9: for each environment step do
10:
Sample action a = π(G(Tϕ(o), z)) using (5), (6)
11:
ZD ←ZD ∪{(z, a, r)}
12:
Jointly optimize policy π and module G on {(zj, aj, rj)}j≤m ∼ZD
13: end for"
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.5112359550561798,"where ki is the projection of zi, d is dimension of z, and τ is a softmax temperature. Then, state
embedding Z is obtained by"
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.5168539325842697,"Z = G(z0, z) = z0 + n
X"
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.5224719101123596,"i=1
ωizi.
(6)"
GUIDED-ATTENTION-BASED PROMPT ENSEMBLE,0.5280898876404494,"Algorithm 1 shows the procedures in CONPE, where the first half corresponds to prompt-based
contrastive learning (in Section 3.2) and the other half corresponds to joint learning of a policy π(Z)
and the attention module G. As G is optimized by a given RL task objective in the source domains (in
line 12), the resulting Z tends to be task-specific, while Z is also domain-invariant by the ensemble of
contrastively learned visual prompts based on G with respect to the combinations of multiple domain
factors. The entire algorithm can be found in Appendix."
EVALUATION,0.5337078651685393,"4
Evaluation"
EVALUATION,0.5393258426966292,"Experiments. We use AI2THOR [9], Metaworld [15], and CARLA [10] environments, specifically
configured for embodied agent tasks with dynamic domain changes. These environments allow us
to explore various domain factors such as camera settings, stride length, rotation degree, gravity,
illuminations, wind speeds, and others. For prompt-based contrastive learning (in Section 3.2), we use
a small dataset of expert demonstrations for each domain factor (i.e., 10 episodes per domain factor).
For prompt ensemble-based policy learning (in Section 3.3), we use a few source domains randomly
generated through combinatorial variations of the seen domain factors (i.e., 4 source domains). In our
zero-shot evaluations, we use target domains that can be categorized as either seen or unseen. The
seen target domains are those encountered during the phase of prompt-based contrastive learning,
while these domains are not present during the phase of prompt ensemble-based policy learning. On
the other hand, the unseen target domains refer to those that are entirely new, implying that they are
not encountered during either learning phases."
EVALUATION,0.5449438202247191,"Baselines. We implement several baselines for comparison. LUSR [16] is a reconstruction-based
domain adaptation method in RL, which uses the variational autoencoder structure for robust rep-
resentations. CURL [2] and ACT [1] employ contrastive learning in RL frameworks for high
sample-efficiency and generalization to visual domains. ACO [12] utilizes augmentation-driven and
behavior-driven contrastive tasks in the context of RL. EmbCLIP [7] is a state-of-the-art embodied
AI model, which exploits the pretrained CLIP visual encoder for visual state representations."
EVALUATION,0.550561797752809,"Implementation. We implement CONPE using the CLIP model with ViT-B/32, similar to VPT [17]
and CoOp [18]. In prompt-based contrastive learning, we adopt various contrastive learning schemes
including augmentation-driven [2, 1, 19] and behavior-driven [12, 20, 21, 22] contrastive learning,
where the prompt length sets to be 8. In policy learning, we exploit online learning (i.e., PPO [23])
for AI2THOR and imitation learning (i.e., DAGGER [24]) for egocentric-Metaworld and CARLA."
EVALUATION,0.5561797752808989,"Table 1: Zero-shot Performance. The policies of each method (CONPE and the baselines) are learned
on 4 source domains. The Source column presents the performance for those source domains. In all
evaluations, we use 30 seen target domains and 10 unseen target domains. The Seen Target column
presents the performance for the seen target domains, and the Unseen Target column presents the
performance for the unseen target domains. The unseen target domains are not used for representation
learning."
EVALUATION,0.5617977528089888,(a) Zero-shot Performance in AI2THOR with Object and Point Goal Navigation Tasks
EVALUATION,0.5674157303370787,"Method
ObjectNav.
PointNav."
EVALUATION,0.5730337078651685,"Source
Seen Target
Unseen Target
Source
Seen Target
Unseen Target"
EVALUATION,0.5786516853932584,"LUSR
53.3±1.1
21.3±1.9
15.1±1.8
85.6±4.6
71.8±3.8
62.4±5.8
CURL
51.3±1.0
8.0±0.1
6.9±1.3
70.8±7.4
55.2±2.7
54.8±3.0
ATC
82.2±9.7
72.3±3.3
51.3±8.6
95.0±3.3
89.1±1.9
81.9±3.6
ACO
55.0±23.8
39.6±21.5
35.8±5.8
91.1±6.3
73.4±2.0
67.5±2.8
EmbCLIP
89.3±3.0
77.6±1.3
59.0±6.4
95.3±4.6
84.5±1.9
77.4±1.4
CONPE
96.3±1.0
83.3±0.3
79.7±6.4
97.8±1.0
89.7±1.6
84.3±2.0"
EVALUATION,0.5842696629213483,(b) Zero-shot Performance in egocentric-Metaworld with Reach and Reach-wall Tasks
EVALUATION,0.5898876404494382,"Method
Reach
Reach-Wall"
EVALUATION,0.5955056179775281,"Source
Seen Target
Unseen Target
Source
Seen Target
Unseen Target"
EVALUATION,0.601123595505618,"LUSR
100.0±0.0
46.0±15.1
44.7±2.3
50.0±10.0
33.3±6.1
30.7±6.4
CURL
100.0±0.0
53.3±5.0
46.7±3.1
43.3±15.3
2.0±0.0
0.7±1.2
ATC
100.0±0.0
71.3±8.1
72.0±2.0
66.7±5.8
5.3±1.2
4.0±0.0
ACO
100.0±0.0
52.0±2.0
44.0±3.5
63.3±15.3
8.7±2.3
4.7±1.2
EmbCLIP
100.0±0.0
64.7±6.1
66.7±4.2
100.0±0.0
58.0±7.2
49.3±5.0
CONPE
100.0±0.0
88.7±3.1
86.7±3.1
100.0±0.0
75.3±3.1
67.3±2.3"
EVALUATION,0.6067415730337079,(c) Zero-shot Performance in CARLA with Different Maps
EVALUATION,0.6123595505617978,"Method
Map 1
Map 2"
EVALUATION,0.6179775280898876,"Source
Seen Target
Unseen Target
Source
Seen Target
Unseen Target"
EVALUATION,0.6235955056179775,"LUSR
2141.9
635.1±606.2
1073.9±212.6
2279.6
1173.7±914.3
2159.4±146.5
CURL
945.4
864.2±638.0
1256.0±61.6
1050.1
1089.9±824.0
2190.3±10.2
ATC
2280.5
1684.4±368.2
1073.7±618.8
2272.2
2253.9±218.7
2200.1±307.8
ACO
2265.8
1545.6±596.1
1330.0±144.5
2270.6
2360.9±88.0
2415.5±53.0
EmbCLIP
2235.7
1732.2±588.6
1415.1±669.9
2262.7
2139.1±655.9
2401.3±12.3
CONPE
2237.5
1738.0±163.5
1933.4±29.7
2277.2
2422.5±79.6
2512.9±15.7"
ZERO-SHOT PERFORMANCE,0.6292134831460674,"4.1
Zero-shot Performance"
ZERO-SHOT PERFORMANCE,0.6348314606741573,"Table 1 shows zero-shot performance of CONPE and the baselines across source, seen and unseen
target domains. We evaluate with 3 different seeds and report the average performance (i.e., task
success rate in AI2THOR and egocentric-Metaworld, the sum of rewards in CARLA). As shown
in Table 1(a), CONPE outperforms the baselines in the AI2THOR tasks. It particularly surpasses
the most competitive baseline, EmbCLIP, by achieving 5.2 ∼5.7% higher success rate for seen
target domains, and 6.9∼20.7% for unseen target domains. For egocentric-Metaworld, as shown
in Table 1(b), CONPE demonstrates superior performance with a significant success rate for both
seen and unseen target domains, which is 17.3 ∼24.0% and 18.0 ∼20.0% higher than EmbCLIP,
respectively. For autonomous driving in CARLA, we take into account external environment factors,
such as weather conditions and times of day, as domain factors that can influence the driving task. In
Table 1(c), CONPE consistently maintains competitive zero-shot performance across all conditions,
outperforming the baselines."
ZERO-SHOT PERFORMANCE,0.6404494382022472,"In these experiments, LUSR shows relatively low success rates, as the reconstruction-based represen-
tation model can abate some task-specific information from observations, which is critical to conduct
vision-based complex RL tasks. EmbCLIP shows the most comparative performance among the
baselines, but its zero-shot performance for target domains is not comparable to CONPE. In contrast,"
ZERO-SHOT PERFORMANCE,0.6460674157303371,"CONPE effectively estimates the domain shifts pertaining to each domain factor through the use of
guided attention weights, leading to robust performance in both seen and unseen target domains."
M,0.651685393258427,"0.0M
0.5M
1.0M
1.5M
2.0M
2.5M
3.0M
3.5M
4.0M
Timesteps 0 20 40 60 80 100"
M,0.6573033707865169,Success Rate (%)
M,0.6629213483146067,"EmbCLIP
ATC
ConPE"
M,0.6685393258426966,(a) Success Rate in Seen Target Domain
M,0.6741573033707865,"0.0M
0.5M
1.0M
1.5M
2.0M
2.5M
3.0M
3.5M
4.0M
Timesteps 0 20 40 60 80 100"
M,0.6797752808988764,Success Rate (%)
M,0.6853932584269663,(b) Success Rate in Unseen Target Domain
M,0.6910112359550562,"Figure 4: Sample-efficiency of Prompt Ensemble-based Policy Learning for Object Navigation in
AI2THOR. The x-axis represents the number of samples (timesteps) used for policy learning, while
the y-axis represents the task success rate for zero-shot evaluation."
M,0.6966292134831461,"Sample Efficiency. Figure 4 presents performance with respect to samples (timesteps) that are used
by CONPE and baselines for policy learning. Compared to the most competitive baseline EmbCLIP,
CONPE requires less than 60.0% timesteps (online samples) for seen target domains and 50.0% for
unseen target domains to have comparable success rates."
M,0.702247191011236,1) Intra Prompted Embedding
M,0.7078651685393258,2) Inter Prompted Embedding 1) 2)
M,0.7134831460674157,(a) Prompted Embeddings
M,0.7191011235955056,Prompt
M,0.7247191011235955,Timestep
M,0.7303370786516854,(b) Attention Weights
M,0.7359550561797753,"Figure 5: Prompt Ensemble Interpretability. In (a), the embeddings in the big circle are intra prompted
embeddings obtained by varying domains within a domain factor, and the embeddings in the rectangle
are inter prompted embeddings obtained by changing the visual prompts with aligned observation.
The closely located intra prompted embeddings indicate the domain-invariant knowledge, while the
inter prompted embeddings clustered by different visual prompts indicate the alignment between the
visual prompts and the domain factors. In (b), each cell represents attention weight ωi applied for
prompted embedding zi."
M,0.7415730337078652,"Prompt Ensemble Interpretability. Figure 5(a) visualizes the prompted embeddings using the
prompt pool obtained through CONPE. For intra prompted embeddings, we use observation pairs,
where each pair is generated by varying domains within a domain factor. We observe that the
embeddings are paired to form the domain-invariant knowledge because the visual prompt is learned
through prompt-based contrastive learning. The inter prompted embeddings specify that each prompt
distinctly clusters prompted embeddings that correspond to the domains associated to its domain
factor. Figure 5(b) shows examples of the attention weight matrix of CONPE for four different
domains. The x-axis denotes the visual prompts and the y-axis denotes timesteps. This shows the
consistency of the attention weights on the prompts across the timesteps in the same domain."
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.7471910112359551,"4.2
Prompt Ensemble with a Pretrained Policy"
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.7528089887640449,"While we previously presented joint learning of a policy and the attention module G, here we also
present how to update G for a pretrained policy π to make the policy adaptable to domain changes. In
this case, we add a policy prompt pv
pol to concentrate on task-relevant features from observations for"
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.7584269662921348,"the pretrained policy π so that prompted embedding ˜z0 with the task-relevant features is incorporated
into the guided-attention-based ensemble, i.e., π(G(˜z0, z)), where ˜z0 = Tϕ(o, pv
pol)."
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.7640449438202247,"Table 2: Prompt Ensemble with a Pretrained Policy. The pretrained policies of each task are learned
on 4 source domains. The Source column presents the performance for those source domains. In all
evaluations, we use 40 unseen target domains. The Target column presents the performance for the
unseen target domains not used for policy training."
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.7696629213483146,(a) Zero-shot Performance in AI2THOR with Visual Navigation and Room Rearrangement Tasks
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.7752808988764045,"Method
ObjectNav. (Aln.)
PointNav. (Not Aln.)
ImageNav. (Not Aln.)
RoomR. (Not Aln.)"
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.7808988764044944,"Source
Target
Source
Target
Souce
Target
Scoure
Target"
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.7865168539325843,"Pretrained
87.5±17.2
65.8±19.1
95.3±4.6
80.9±1.6
77.2±3.3
56.2±2.2
87.3±3.1
75.2±13.2
CONPE
88.4±1.7
72.8±3.1
98.9±1.0
84.4±1.0
79.2±1.4
61.6±1.1
93.3±1.2
82.2±14.4"
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.7921348314606742,(b) Zero-shot Performance in egocentric-Metaworld with 4 Different Robot Manipulation Tasks
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.797752808988764,"Method
Reach (Aln.)
Reach-Wall (Not Aln.)
Button-Press (Not Aln.)
Door-Open (Not Aln.)"
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.8033707865168539,"Source
Target
Source
Target
Source
Target
Source
Target"
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.8089887640449438,"Pretrained
100.0±0.0
65.7±6.4
100.0±0.0
58.0±5.8
100.0±0.0
16.8±2.3
100.0±0.0
35.6±6.2
CONPE
100.0±0.0
74.7±5.0
100.0±0.0
75.7±9.0
100.0±0.0
73.7±8.3
100.0±0.0
93.2±1.1"
PROMPT ENSEMBLE WITH A PRETRAINED POLICY,0.8146067415730337,"Table 2 reports zero-shot performance for the scenarios when a pretrained policy is given. We evaluate
two different cases: aligned (Aln.) when prompt-based contrastive learning is conducted on data
from the same task of a pretrained policy; otherwise, not aligned (Not Aln.). In AI2THOR, we
use data from the object goal navigation task for prompt-based contrastive learning, while each
pretrained policy is learned individually through one of tasks including object goal navigation, point
goal navigation, image goal navigation, and room rearrangement. Similarly, in egocentric-Metaworld,
we use data from the reach task for prompt-based contrastive learning, while each pretrained policy is
learned individually through one of tasks including reach, reach-wall, button-press, and door-open.
In Table 2(a), CONPE enhances zero-shot performance of the pretrained policies by 3.5∼7.0% for
unseen target domains in AI2THOR. This prompt ensemble adaptation requires only 400K samples,
equivalent to 10% of the total samples used for policy learning. In Table 2(b), CONPE significantly
boosts zero-shot performance of the pretrained policies by 9.0∼57.6% in egocentric-Metaworld."
ABLATION STUDY,0.8202247191011236,"4.3
Ablation Study"
ABLATION STUDY,0.8258426966292135,Here we conduct ablation studies with AI2THOR. All the performances are reported in success rates.
ABLATION STUDY,0.8314606741573034,Table 3: Prompt Ensemble Scalability
ABLATION STUDY,0.8370786516853933,"n
Source
Seen Target
Unseen Target"
ABLATION STUDY,0.8426966292134831,"2
98.7±0.4
40.5±2.2
43.0±2.9
5
96.1±0.6
59.2±9.6
45.0±10.1
10
96.3±1.0
83.3±0.3
79.7±6.4
16
91.8±2.0
83.8±1.3
77.1±6.2
18
98.5±1.8
83.3±2.3
79.0±4.5"
ABLATION STUDY,0.848314606741573,Table 4: Prompt Ensemble Methods
ABLATION STUDY,0.8539325842696629,"Ensemble Method
Source
Seen Target
Unseen Target"
ABLATION STUDY,0.8595505617977528,"COM-UNI-AVG
52.9±12.2
51.4±7.6
43.1±12.7
COM-WEI-AVG
63.6±11.2
42.3±5.2
50.3±6.1
ENS-UNI-AVG
88.6±1.4
79.8±3.4
65.5±5.0
ENS-WEI-AVG
94.1±6.3
75.5±3.5
61.2±12.7
CONPE
96.3±1.0
83.3±0.3
79.7±6.4"
ABLATION STUDY,0.8651685393258427,"Prompt Ensemble Scalability. Table 3 evaluates CONPE with respect to the number of prompts
(n). CONPE effectively enhances zero-shot performance for both seen and unseen target domains
through prompt ensemble that captures various domain factors. Compared to the case of n = 2, for
n = 10, there was a significant improvement in zero-shot performance for both seen and unseen
target domains, with increases of 42.8% and 36.7%, respectively. For n ≥10, we observe stable
performance that specifies that CONPE can scale for combining multiple prompts to some extent."
ABLATION STUDY,0.8707865168539326,"Prompt Ensemble Methods. Tabel 4 compares the performance of various prompt integration
methods [25, 26, 27] including our guided attention-based prompt ensemble. We denote prompt-level
integration as COM, and prompted embeddings-level integration as ENS. UNI-AVG and WEI-AVG
refer to uniform average and weighted average mechanisms, respectively. CONPE achieves superior
success rates over the most competitive ensemble method ENS-UNI-AVG, showing 3.5% and 14.2%
performance gain for seen and unseen target domains."
ABLATION STUDY,0.8764044943820225,Table 5: Prompt Ensemble Adaptation
ABLATION STUDY,0.8820224719101124,"Optimization
Source
Target"
ABLATION STUDY,0.8876404494382022,"Pretrained
95.3±4.6
80.9±1.6
w/o pv
pol
59.5±2.9
54.2±0.6
w pv
pol
98.9±1.0
84.4±1.0
E2E
96.4±0.5
83.3±3.3"
ABLATION STUDY,0.8932584269662921,Table 6: Semantic Regularized Data Augmentation
ABLATION STUDY,0.898876404494382,"δ
w/o Semantic
w Semantic"
ABLATION STUDY,0.9044943820224719,"Source
Target
Source
Target"
ABLATION STUDY,0.9101123595505618,"0.1
97.4±3.8
83.6±8.9
100.0±0.0
84.1±10.2
0.2
94.7±0.0
77.6±12.8
94.8±7.4
79.7±9.1
0.3
84.2±3.7
75.3±8.8
96.1±1.9
83.1±10.0
0.4
80.3±16.2
74.0±14.9
86.9±3.8
81.3±12.3"
ABLATION STUDY,0.9157303370786517,"Prompt Ensemble Adaptation Method. Table 5 shows the effect of our ensemble adaptation method
for the situation when a pretrained policy is given. As explained in Section 4.2, in this situation,
CONPE can update the attention module with an additional prompt pv
pol. Note that pv
pol corresponds
to this case, while w/o pv
pol corresponds to the other case of using the attention module without pv
pol.
In addition, E2E denotes the fine-tuning of both the policy and the attention module along with pv
pol.
The results demonstrate that our method enhances the zero-shot performance of the pretrained policy,
showing that pv
pol facilitates the extraction of task-specific features."
ABLATION STUDY,0.9213483146067416,"Semantic Regularized Data Augmentation. So far, we have only utilized vision data, but here, we
discuss one extension of CONPE using semantic information. Specifically, we use a few samples of
object-level text descriptions to regularize the data augmentation process in policy learning. This
aims to mitigate overfitting issues [28, 29]. The detailed explanations can be found in Appendix. As
shown in Table 6, CONPE with semantic data (w Semantic) consistently yields better performance
than CONPE without semantic data (w/o Semantic) for all noise scale settings (δ). Note that the
noise scale manages the variance of augmented prompted embeddings. This experiment indicates
that CONPE can be improved by incorporating semantic information."
RELATED WORK,0.9269662921348315,"5
Related Work"
RELATED WORK,0.9325842696629213,"Adaptation in Embodied AI. In the literature of robotics, numerous studies focused on developing
generalized visual encoders for robotic agents across various domains [30, 31], exploiting pretrained
visual encoders [32, 33], and establishing robust control policies with domain randomized tech-
niques [34, 35]. Furthermore, in the field of learning embodied agents, a few works addressed
adaptation issues of agents to unseen scenarios in complex environments, using data augmentation
techniques [36, 37, 38, 39, 40] or adopting self-supervised learning schemes [41, 42, 43]. Recently,
several works showed the feasibility and benefits of adopting large-scale pretrained vision-language
models for embodied agents [7, 44, 45, 46]. Our work is in the same vein of these prior works of
embodied agents, but unlike them, we explore visual prompt learning and ensembles, aiming to
enhance both zero-shot performance and sample-efficiency."
RELATED WORK,0.9382022471910112,"Decoupled RL Structure. The decoupled structure, where a state representation model is separated
from RL, has been investigated in vision-based RL [47, 2, 16]. Recently, contrastive representation
learning on expert trajectories gains much interest, as it allows expert behavior patterns to be
incorporated into the state encoder even when a policy is not jointly learned [1, 20]. They established
generalized state representations, yet in that direction, sample-efficiency issues in both representation
learning and policy learning remain unexplored."
RELATED WORK,0.9438202247191011,"Prompt-based Learning. Prompt-based learning or prompt tuning is a parameter-efficient opti-
mization method for large pretrained models. Prompt tuning was used for computer vision tasks,
optimizing a few learnable vectors in the text encoder [18], and it was also adopted for vision
transformer models to handle a wide range of downstream tasks [17]. Recently, visual prompting [48]
was introduced, and both visual and text prompt tuning were explored together in the multi-modal
embedding space [49, 50]. We also use visual prompt tuning, but we concentrate on the ensemble
of multiple prompts to tackle complex embodied RL tasks. We take advantage of the fact that the
generalized representation capability of different prompts can vary depending on a given task and
domain, and thus we strategically utilize them to enable zero-shot adaptation of RL policies."
CONCLUSION,0.949438202247191,"6
Conclusion"
CONCLUSION,0.9550561797752809,"Limitation. Our CONPE framework exploits visual inputs and their relevant domain factors for
policy adaptation. For environments where domain changes extend beyond those domain factors, the
adaptability of the framework might be constrained. In our future work, we will adapt the framework
with semantic knowledge based on pretrained language models to improve the policy generalization
capability for embodied agents in dynamic complex environments and to cover various scenarios
associated with multi-modal agent interfaces."
CONCLUSION,0.9606741573033708,"Conclusion. In this work, we presented the CONPE framework, a novel approach that allows
embodied RL agents to adapt in a zero-shot manner across diverse visual domains, exploring the
ensemble structure that incorporates multiple contrastive visual prompts. The ensemble facilitates
domain-invariant and task-specific state representations, thus enabling the agents to generalize to vi-
sual variations influenced by specific domain factors. Through various experiments, we demonstrated
that the framework can enhance policy adaptation across various domains for vision-based object
navigation, rearrangement, manipulation tasks as well as autonomous driving tasks."
ACKNOWLEDGEMENT,0.9662921348314607,"7
Acknowledgement"
ACKNOWLEDGEMENT,0.9719101123595506,"We would like to thank anonymous reviewers for their valuable comments and suggestions. This work
was supported by Institute of Information & communications Technology Planning & Evaluation
(IITP) grant funded by the Korea government (MSIT) (No. 2022-0-01045, 2022-0-00043, 2020-0-
01821, 2019-0-00421) and by the National Research Foundation of Korea (NRF) grant funded by the
MSIT (No. NRF-2020M3C1C2A01080819, RS-2023-00213118)."
REFERENCES,0.9775280898876404,References
REFERENCES,0.9831460674157303,"[1]
Adam Stooke et al. “Decoupling representation learning from reinforcement learning”. In:
Proceedings of the 38th International Conference on Machine Learning. 2021, pp. 9870–9879.
[2]
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. “CURL: Contrastive unsupervised repre-
sentations for reinforcement learning”. In: Proceedings of the 37th International Conference
on Machine Learning. 2020, pp. 5639–5650.
[3]
Max Schwarzer et al. “Data-efficient reinforcement learning with self-predictive representa-
tions”. In: arXiv preprint arXiv:2007.05929 (2020).
[4]
Shangda Li et al. “Unsupervised domain adaptation for visual navigation”. In: arXiv preprint
arXiv:2010.14543 (2020).
[5]
Ziad Al-Halah, Santhosh K. Ramakrishnan, and Kristen Grauman. “Zero experience required:
Plug & play modular transfer learning for semantic visual navigation”. In: Proceedings of the
9th International Conference on Vision and Pattern Recognition. 2021, pp. 17010–17020.
[6]
Qianfan Zhao et al. “Zero-shot object goal visual navigation”. In: arXiv preprint
arXiv:2206.07423 (2022).
[7]
Apoorv Khandelwal et al. “Simple but Effective: CLIP embeddings for embodied AI”. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022,
pp. 14809–14818.
[8]
Arjun Majumdar et al. “Zson: Zero-shot object-goal navigation using multimodal goal embed-
dings”. In: arXiv preprint arXiv:2206.12403 (2022).
[9]
Eric Kolve et al. “Ai2-thor: An interactive 3d environment for visual ai”. In: arXiv preprint
arXiv:1712.05474 (2017).
[10]
Alexey Dosovitskiy et al. “CARLA: An open urban driving simulator”. In: Proceedings of the
1st Conference on Robot Learning. 2017, pp. 1–16.
[11]
Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT press,
2018.
[12]
Qihang zhang, Zheghao Peng, and Bolei Zhou. “Learning to drive by watching youtube videos:
Action-conditioned contrastive policy pretraining”. In: Proceedings of the 17th European
Conference on Computer Vision. 2022, pp. 111–128.
[13]
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. “Representation learning with contrastive
predictive coding”. In: arXiv preprint arXiv:1807.03748 (2018)."
REFERENCES,0.9887640449438202,"[14]
Ting Chen et al. “A simple framework for contrastive learning of visual representations”. In:
Proceedings of the 37th International Conference on Machine Learning. 2020, pp. 1597–1607.
[15]
Tianhe Yu et al. “Meta-World: A benchmark and evaluation for multi-task and meta reinforce-
ment learning”. In: Proceedings of the 3rd Conference on Robot Learning. 2019, pp. 1094–
1100.
[16]
Jinwei Xing et al. “Domain adaptation in reinforcement learning via latent unified state
representation”. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence. 2021,
pp. 10452–10459.
[17]
Menglin Jia et al. “Visual prompt tuning”. In: Proceedings of the 17th European Conference
on Computer Vision. 2022, pp. 709–727.
[18]
Kaiyang Zhou et al. “Learning to prompt for vision-language models”. In: International
Journal of Computer Vision (2022).
[19]
Bang You et al. “Integrating contrastive learning with dynamic models for reinforcement
learning from images”. In: Neurocomputing (2022).
[20]
Minbeom Kim et al. “Action-driven contrastive representation for reinforcement learning”. In:
PLOS ONE (2022).
[21]
Rishabh Agarwal et al. “Contrastive behavioral similarity embeddings for generalization in
reinforcement learning”. In: arXiv preprint arXiv:2101.05265 (2021).
[22]
Young Jae Lee et al. “STACoRe: Spatio-temporal and action-based contrastive representations
for reinforcement learning in Atari”. In: Neural Networks (2023).
[23]
John Schulman et al. “Proximal policy optimization algorithms”. In: arXiv preprint
arXiv:1707.06347 (2017).
[24]
Stéphane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. “A reduction of imitation learning
and structured prediction to no-regret online learning”. In: Proceedings of the 14th Interna-
tional Conference on Artificial Intelligence and Statistics. 2011, pp. 627–635.
[25]
Pengfei Liu et al. “Pre-train, prompt, and predict: A systematic survey of prompting methods
in natural language processing”. In: ACM Computing Surveys (2023).
[26]
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. “Effective approaches to
attention-based neural machine translation”. In: arXiv preprint arXiv:1508.04025 (2015).
[27]
Ashish Vaswani et al. “Attention is all you need”. In: Advances in neural information process-
ing systems (2017).
[28]
Denis Yarats, Ilya Kostrikov, and Rob Fergus. “Image Augmentation Is All You Need: Regu-
larizing deep reinforcement learning from pixels”. In: Proceedings of the 9th International
Conference on Learning Representations. 2021.
[29]
Samarth Sinha, Ajay Mandlekar, and Animesh Garg. “S4RL: Surprisingly Simple Self-
Supervision for Offline Reinforcement Learning in Robotics”. In: Proceedings of the 5th
Conference on Robotics Learning. 2021, pp. 907–917.
[30]
Arjun Majumdar et al. “Where are we in the search for an Artificial Visual Cortex for Embodied
Intelligence?” In: arXiv preprint arXiv:2303.18240 (2023).
[31]
Suraj Nair et al. “R3m: A universal visual representation for robot manipulation”. In: arXiv
preprint arXiv:2203.12601 (2022).
[32]
Austin Stone et al. “Open-world object manipulation using pre-trained vision-language mod-
els”. In: arXiv preprint arXiv:2303.00905 (2023).
[33]
Jiange Yang et al. “Pave the Way to Grasp Anything: Transferring Foundation Models for
Universal Pick-Place Robots”. In: arXiv preprint arXiv:2306.05716 (2023).
[34]
Dhruv Shah et al. “Gnm: A general navigation model to drive any robot”. In: Proceedings of
the IEEE International Conference on Robotics and Automation. IEEE. 2023, pp. 7226–7233.
[35]
Noriaki Hirose et al. “ExAug: Robot-conditioned navigation policies via geometric experi-
ence augmentation”. In: Proceedings of the IEEE International Conference on Robotics and
Automation. IEEE. 2023, pp. 4077–4084.
[36]
Daniel Fried et al. “Speaker-follower models for vision-and-language navigation”. In: Pro-
ceedings of the 31th Conference on Neural Information Processing Systems. 2018, pp. 3318–
3329.
[37]
Felix Yu et al. “Take the scenic route: Improving generalization in vision-and-language
navigation”. In: arXiv preprint arXiv:2003.14269 (2020)."
REFERENCES,0.9943820224719101,"[38]
Xiaofeng Gao et al. “Dialfred: Dialogue-enabled agents for embodied instruction following”.
In: arXiv preprint arXiv:2202.13330 (2022).
[39]
Chong Liu et al. “Vision-language navigation with random environmental mixup”. In: Pro-
ceedings of the International Conference on Computer Vision. 2021, pp. 1624–1634.
[40]
Jialu Li, Hao Tan, and Mohit Bansal. “EnvEdit: Environment Editing for Vision-and-Language
Navigation”. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2022, pp. 15386–15396.
[41]
Xin Wang et al. “Reinforced cross-modal matching and self-supervised imitation learning
for vision-language navigation”. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 2019, pp. 6629–6638.
[42]
Roberto Bigazzi et al. “Explore and explain: self-supervised navigation and recounting”. In:
Proceeding of the 25th International Conference on Pattern Recognition. 2020, pp. 1152–1159.
[43]
Eun Sun Lee et al. “MoDA: Map style transfer for self-supervised Domain Adaptation of
embodied agents”. In: Proceeding of the 17th European Conference on Computer Vision. 2022,
pp. 338–354.
[44]
Vishnu Sashank Dorbala et al. “CLIP-Nav: Using CLIP for zero-shot vision-and-language
navigation”. In: arXiv preprint arXiv:2211.16649 (2022).
[45]
Samir Yitzhak Gadre et al. “CLIP on Wheels: zero-shot object navigation as object localization
and exploration”. In: arXiv preprint arXiv:2203.10421 (2022).
[46]
Dhruv Shah et al. “Lm-nav: Robotic navigation with large pre-trained models of language,
vision, and action”. In: Proceedings of the 6th Conference on Robot Learning. 2022, pp. 492–
504.
[47]
Irina Higgins et al. “DARLA: Improving zero-shot transfer in reinforcement learning”. In:
Proceedings of the 34th International Conference on Machine Learning. 2018, pp. 1480–1490.
[48]
Hyojin Bahng et al. “Exploring visual prompts for adapting large-scale models”. In: arXiv
preprint arXiv:2203.17274 (2022).
[49]
Muhammad Uzair Khattak et al. “MaPLe: Multi-modal prompt learning”. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
[50]
Yuhang Zang et al. “Unified vision and language prompt learning.” In: arXiv preprint
arXiv:2210.07225 (2022)."
