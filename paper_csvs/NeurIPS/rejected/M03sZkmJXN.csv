Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002044989775051125,"Learning effective representations is crucial for understanding proteins and their
1"
ABSTRACT,0.00408997955010225,"biological functions. Recent advancements in language models and graph neural
2"
ABSTRACT,0.006134969325153374,"networks have enabled protein models to leverage primary or tertiary structure
3"
ABSTRACT,0.0081799591002045,"information to learn representations. However, the lack of practical methods to
4"
ABSTRACT,0.010224948875255624,"deeply co-model the relationships between protein sequences and structures has
5"
ABSTRACT,0.012269938650306749,"led to suboptimal embeddings. In this work, we propose CoupleNet, a network that
6"
ABSTRACT,0.014314928425357873,"couples protein sequence and structure to obtain informative protein representations.
7"
ABSTRACT,0.016359918200409,"CoupleNet incorporates multiple levels of features in proteins, including the residue
8"
ABSTRACT,0.018404907975460124,"identities and positions for sequences, as well as geometric representations for
9"
ABSTRACT,0.02044989775051125,"tertiary structures. We construct two types of graphs to model the extracted
10"
ABSTRACT,0.022494887525562373,"sequential features and structural geometries, achieving completeness on these
11"
ABSTRACT,0.024539877300613498,"graphs, respectively, and perform convolution on nodes and edges simultaneously
12"
ABSTRACT,0.026584867075664622,"to obtain superior embeddings. Experimental results on a range of tasks, such as
13"
ABSTRACT,0.028629856850715747,"protein fold classification and function prediction, demonstrate that our proposed
14"
ABSTRACT,0.03067484662576687,"model outperforms the state-of-the-art methods by large margins.
15"
INTRODUCTION,0.032719836400818,"1
Introduction
16"
INTRODUCTION,0.034764826175869123,"Proteins are the fundamental building blocks of life and play essential roles in a diversity of ap-
17"
INTRODUCTION,0.03680981595092025,"plications, from therapeutics to materials. They are composed of 20 different basic amino acids,
18"
INTRODUCTION,0.03885480572597137,"which are lined by peptide bonds and form a sequence. The one-dimensional (1D) sequence of a
19"
INTRODUCTION,0.0408997955010225,"protein determines its structure, which in turn determines its biochemical function [40]. Due to recent
20"
INTRODUCTION,0.04294478527607362,"progress in protein sequencing [34], massive numbers of protein sequences are now available. For
21"
INTRODUCTION,0.044989775051124746,"example, the UniProt [3] database contains over 200 million protein sequences with annotations,
22"
INTRODUCTION,0.04703476482617587,"e.g., gene ontology (GO) terms, similar proteins, family and domains. Notably, the development of
23"
INTRODUCTION,0.049079754601226995,"large-scale language models (LMs) in natural language processing has substantially benefited protein
24"
INTRODUCTION,0.05112474437627812,"research owing to similarities between human language and protein sequences [16, 27]. For instance,
25"
INTRODUCTION,0.053169734151329244,"models like ProtTrans [14] and ESM-series [39, 33] in learning protein representations have proven
26"
INTRODUCTION,0.05521472392638037,"successful utility of pre-training protein LMs with self-supervision to process protein sequences.
27"
INTRODUCTION,0.05725971370143149,"Thanks to the recent significant progress made by AlphaFold2 [30] in three-dimensional (3D) structure
28"
INTRODUCTION,0.05930470347648262,"prediction, a large number of protein structures from their sequence data are now made available. The
29"
INTRODUCTION,0.06134969325153374,"latest release of AlphaFold protein structure database [43] provides broad coverage of UniProt [3].
30"
INTRODUCTION,0.06339468302658487,"Recently proposed structure-based protein encoders become to utilize geometric features [25, 24,
31"
INTRODUCTION,0.065439672801636,"53], e.g., ProNet [47] learns representations of proteins with 3D structures at different levels, like the
32"
INTRODUCTION,0.06748466257668712,"amino acid, backbone or all-atom levels. There also exists a group of methods that build graph neural
33"
INTRODUCTION,0.06952965235173825,"networks and LMs (LSTMs or attention models) to process sequence and structure [53, 50, 19], for
34"
INTRODUCTION,0.07157464212678936,"example, GearNet [53] encodes sequential and spatial features by alternating node and edge message
35"
INTRODUCTION,0.0736196319018405,"passing on protein residue graphs.
36"
INTRODUCTION,0.07566462167689161,Amino acids: HSHGLFKKL...KVESRDGT
INTRODUCTION,0.07770961145194274,"Primary structure
Tertiary structure GLU"
INTRODUCTION,0.07975460122699386,"Positions: 1,2,...,n H S L
F H G K K
L"
INTRODUCTION,0.081799591002045,"Figure 1: Illustration of the protein sequence and structure. 1) The primary structure comprises n
amino acids. 2) The tertiary structure with atom arrangement in Euclidean space is presented, where
each atom has a specific 3D coordinate. Amino acids have fixed backbone atoms (Cα, C, N, O)
and side-chain atoms that vary depending on the residue types. GLU: Glutamic acid. Complete
geometries can be obtained based on these coordinates. The sequence and structure provide different
information types and data categories."
INTRODUCTION,0.08384458077709611,"The 1D sequence and 3D structure of a protein provide different types of information, in detail, as
37"
INTRODUCTION,0.08588957055214724,"shown in Figure 1, compared with the 1D sequential order and amino acids in peptide chains, the
38"
INTRODUCTION,0.08793456032719836,"tertiary structure provides 3D coordinates of each atom in protein residues, which allow them to
39"
INTRODUCTION,0.08997955010224949,"perform precise functions. Although a protein’s sequence determines its structure, various works
40"
INTRODUCTION,0.09202453987730061,"have demonstrated the effectiveness of learning from either sequence or structure [33, 25]. However,
41"
INTRODUCTION,0.09406952965235174,"rich constraints between the sequence and structure of a protein, which may be critical for protein
42"
INTRODUCTION,0.09611451942740286,"tasks [4], have yet to be fully explored. Most protein sequence-structure modeling methods cannot
43"
INTRODUCTION,0.09815950920245399,"deeply integrate the information behind sequence and structure for the reason that they tend to fuse
44"
INTRODUCTION,0.10020449897750511,"representations together, extracted from sequence and structure encoders, respectively, by message
45"
INTRODUCTION,0.10224948875255624,"passing mechanism [8] or by simple concatenation operations.
46"
INTRODUCTION,0.10429447852760736,"In this work, we aim to learn protein representations by deeply coupling the protein sequences and
47"
INTRODUCTION,0.10633946830265849,"structures. Considering the relative positions of residues in the sequence and the spatial arrangement
48"
INTRODUCTION,0.1083844580777096,"of atoms in the Euclidean space, the proposed CoupleNet constructs two categories of graphs for
49"
INTRODUCTION,0.11042944785276074,"them, respectively. The complete representations are obtained at the amino acid and backbone
50"
INTRODUCTION,0.11247443762781185,"levels on the two graphs, which are used as node and edge features to learn the final graph-level
51"
INTRODUCTION,0.11451942740286299,"representations. Rather than concatenating sequence and structure representations, we take advantage
52"
INTRODUCTION,0.1165644171779141,"of graph convolutions, performing node and edge convolutions simultaneously. The contributions of
53"
INTRODUCTION,0.11860940695296524,"this paper are threefold:
54"
INTRODUCTION,0.12065439672801637,"• We propose a novel two-graph-based approach for representing the sequence and the 3D
55"
INTRODUCTION,0.12269938650306748,"geometric structure of a protein, which is an effective way to guarantee completeness.
56"
INTRODUCTION,0.12474437627811862,"• We propose CoupleNet, a model that performs convolutions on nodes and edges of graphs
57"
INTRODUCTION,0.12678936605316973,"to effectively integrate protein sequence and structure. This can better model the node-edge
58"
INTRODUCTION,0.12883435582822086,"relationships and utilize the intrinsic associations between sequences and structures.
59"
INTRODUCTION,0.130879345603272,"• Practically, the proposed model is verified by obtaining new state-of-the-art experimental
60"
INTRODUCTION,0.1329243353783231,"results compared with current mainstream protein representation learning methods on a
61"
INTRODUCTION,0.13496932515337423,"range of tasks, including protein fold classification, enzyme reaction classification, GO term
62"
INTRODUCTION,0.13701431492842536,"prediction, domain prediction, and enzyme commission number prediction.
63"
RELATED WORK,0.1390593047034765,"2
Related Work
64"
RELATED WORK,0.1411042944785276,"Protein Representation Learning
Protein representation learning has become an active and promis-
65"
RELATED WORK,0.14314928425357873,"ing direction in biology, which is essential to various downstream tasks in protein science. Because
66"
RELATED WORK,0.14519427402862986,"of the different levels of protein structures, existing methods mainly fall into three categories: protein
67"
RELATED WORK,0.147239263803681,"LMs for sequences, structure models for geometry, and hybrid methods for both of them. As proteins
68"
RELATED WORK,0.1492842535787321,"are sequences of amino acids, considering their similarities with human languages, UniRep [1],
69"
RELATED WORK,0.15132924335378323,"UDSMProt [42] and SeqVec [23] use LSTM or its variants to learn sequence representations and
70"
RELATED WORK,0.15337423312883436,"long-range dependencies. TAPE [37] benchmarks a group of protein models, e.g., 1D CNN, LSTM,
71"
RELATED WORK,0.1554192229038855,"and Transformer by various tasks. Elnaggar et al. [14] have trained six successful transformer variants
72"
RELATED WORK,0.1574642126789366,"on billions of amino acid sequences, like ProtBert, and ProtT5. Similarly, ESM-series [39, 38, 33]
73"
RELATED WORK,0.15950920245398773,"employs a transformer architecture and a masked language modeling strategy to train robust represen-
74"
RELATED WORK,0.16155419222903886,"tations based on large-scale databases. Besides the protein sequence, as we have stated before, the
75"
RELATED WORK,0.16359918200409,"3D geometric structure is vital to enhance protein representations. Most methods commonly seek to
76"
RELATED WORK,0.1656441717791411,"encode the spatial information of protein structures by convolutional neural networks (CNNs) [11],
77"
RELATED WORK,0.16768916155419222,"or graph neural networks [19, 2, 29]. For instance, SPROF [7] employs distance maps to predict
78"
RELATED WORK,0.16973415132924335,"protein sequence profiles, and IEConv [25] introduces a convolution operator to capture all relevant
79"
RELATED WORK,0.17177914110429449,"structural levels of a protein. GVP-GNN [29] designs the geometric vector perceptrons (GVP) for
80"
RELATED WORK,0.1738241308793456,"learning both scalar and vector features in an equivariant and invariant manner, Guo et al. [21]
81"
RELATED WORK,0.17586912065439672,"adopt SE(3)-invariant features as the model inputs and reconstruct gradients over 3D coordinates to
82"
RELATED WORK,0.17791411042944785,"avoid the usage of complicated SE(3)-equivariant models. ProNet [47] learns hierarchical protein
83"
RELATED WORK,0.17995910020449898,"representations at multiple tertiary structure levels of granularity. Moreover, CDConv [15] proposes
84"
RELATED WORK,0.18200408997955012,"continuous-discrete convolution using irregular and regular approaches to model the geometry and
85"
RELATED WORK,0.18404907975460122,"sequence structures. Some protein learning methods model the multi-level of structures at the same
86"
RELATED WORK,0.18609406952965235,"time [53, 6, 15], except for the primary structure and the tertiary structure, the second refers to the
87"
RELATED WORK,0.18813905930470348,"3D form of local segments of proteins (e.g., α-helix, β-strand), the quaternary is a protein multimer
88"
RELATED WORK,0.1901840490797546,"comprising multiple polypeptides, for example, PromtProtein [48] adopts a prompt-guided multi-task
89"
RELATED WORK,0.19222903885480572,"learning strategy for different protein structures with specific pre-training tasks. While previous
90"
RELATED WORK,0.19427402862985685,"works have attempted to combine protein sequence and structure, we focus on profoundly integrat-
91"
RELATED WORK,0.19631901840490798,"ing them by specifically designing two types of graphs respectively and conducting convolutions
92"
RELATED WORK,0.1983640081799591,"simultaneously to learn protein representations.
93"
RELATED WORK,0.20040899795501022,"Complete Message Passing Mechanism
ComENet [46] proposes rotation angles and spherical
94"
RELATED WORK,0.20245398773006135,"coordinates to fulfil the global completeness of 3D information on molecular graphs. By incorporating
95"
RELATED WORK,0.20449897750511248,"these designed geometric representations into the message passing scheme [18], the complete
96"
RELATED WORK,0.2065439672801636,"representation for a whole 3D graph is eventually yielded [47]. Unlike these methods, we couple
97"
RELATED WORK,0.2085889570552147,"sequence and structure via corresponding graphs and different geometric representations to obtain
98"
RELATED WORK,0.21063394683026584,"completeness representations.
99"
METHOD,0.21267893660531698,"3
Method
100"
PRELIMINARIES,0.2147239263803681,"3.1
Preliminaries
101"
PRELIMINARIES,0.2167689161554192,"Notations
We represent a 3D graph as G = (V, E, P), where V = {vi}i=1,...,n and E =
102"
PRELIMINARIES,0.21881390593047034,"{εij}i,j=1,...,n denote the vertex and edge sets with n nodes in total, respectively, and P =
103"
PRELIMINARIES,0.22085889570552147,"{Pi}i=1,...,n is the set of position matrices, where Pi ∈Rki×3 represents the position matrix
104"
PRELIMINARIES,0.2229038854805726,"for node vi. We treat each amino acid as a graph node for a protein, then ki depends on the number
105"
PRELIMINARIES,0.2249488752556237,"of atoms in the i-th amino acid. The node feature matrix is X = [xi]i=1,...,n, where xi ∈Rdv is
106"
PRELIMINARIES,0.22699386503067484,"the feature vector of node vi. The edge feature matrix is E = [eij]i,j=1,...,n, where eij ∈Rdε is the
107"
PRELIMINARIES,0.22903885480572597,"feature vector of edge εij. dv and dε denote the dimensions of feature vectors xi and eij.
108"
PRELIMINARIES,0.2310838445807771,"Invariance and Equivariance
We consider affine transformations that preserve the distance
109"
PRELIMINARIES,0.2331288343558282,"between any two points, i.e., the isometric group SE(3) in the Euclidean space. This is called
110"
PRELIMINARIES,0.23517382413087934,"the symmetry group, and it turns out that SE(3) is the special Euclidean group that includes 3D
111"
PRELIMINARIES,0.23721881390593047,"translations and the 3D rotation group SO(3) [17, 12]. The matrix form of SE(3) is provided in
112"
PRELIMINARIES,0.2392638036809816,"Appendix A.1.
113"
PRELIMINARIES,0.24130879345603273,"Given the function f : Rm →Rm′, assuming the given symmetry group G acts on Rm and Rm′,
114"
PRELIMINARIES,0.24335378323108384,"then f is G-equivariant if,
115"
PRELIMINARIES,0.24539877300613497,"f(Tgx) = Sgf(x), ∀x ∈Rm, g ∈G
(1)"
PRELIMINARIES,0.2474437627811861,"where Tg and Sg are the transformations. For the SE(3) group, when m
′ = 1, the output of f is a
116"
PRELIMINARIES,0.24948875255623723,"scalar, we have
117"
PRELIMINARIES,0.25153374233128833,"f(Tgx) = f(x), ∀x ∈Rm, g ∈G
(2)"
PRELIMINARIES,0.25357873210633947,"thus f is SE(3)-invariant.
118"
PRELIMINARIES,0.2556237218813906,"Complete Geometric Representations
A geometric transformation F(·) is complete if two 3D
119"
PRELIMINARIES,0.25766871165644173,"graphs G1 = (V, E, P1) and G2 = (V, E, P2), there exists Tg ∈SE(3) such that the representations
120"
PRELIMINARIES,0.25971370143149286,"F(G1) = F(G2) ⇐⇒P 1
i = Tg(P 2
i ), for i = 1, . . . n
(3)"
PRELIMINARIES,0.261758691206544,"The operation Tg would not change the 3D conformation of a 3D graph [46]. Positions can generate
121"
PRELIMINARIES,0.26380368098159507,"geometric representations, which can also be recovered from them.
122"
PRELIMINARIES,0.2658486707566462,"Message Passing Paradigm
Message passing mechanism is mainly applied in graph convolutional
123"
PRELIMINARIES,0.26789366053169733,"networks (GCNs) [32], which follows an iterative scheme of updating node representations based on
124"
PRELIMINARIES,0.26993865030674846,"the feature aggregation from nearby nodes.
125"
PRELIMINARIES,0.2719836400817996,"h(0)
i
= BN (FC (xi)) ,"
PRELIMINARIES,0.2740286298568507,"u(l)
i
= f (l)
Agg(h(l−1)
j
|vj ∈N(vi)),"
PRELIMINARIES,0.27607361963190186,"h(l)
i
= f (l)
Update(h(l−1)
j
, u(l)
i ) (4)"
PRELIMINARIES,0.278118609406953,"where FC(·) and BN(·) mean the linear transformation and batch normalization respectively. N(vi)
126"
PRELIMINARIES,0.28016359918200406,"denotes the neighbours of node vi. f (l)
Agg and f (l)
Update are aggregation and transformation functions at
127"
PRELIMINARIES,0.2822085889570552,"the l-th layer, which are permutation invariant and equivariant of node representations.
128"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.2842535787321063,"3.2
Sequence-Structure Graph Construction
129"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.28629856850715746,Figure 2: The local coordinate system.
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.2883435582822086,"Specifically, we represent each amino acid as a node,
130"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.2903885480572597,"considering the residue types and their positions i =
131"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.29243353783231085,"1, 2, · · · , n (See Figure 1) in the sequence, we de-
132"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.294478527607362,"fine the sequential graph primarily on the sequence,
133"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.2965235173824131,"if ∥i −j∥< l, the edge εij exists, where l is a hyper-
134"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.2985685071574642,"parameter. Besides the sequential graph, we predefine a
135"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3006134969325153,"radius r, and build the radius graph, and there exists an
136"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.30265848670756645,"edge between node vi and vj if ∥Pi,Cα −Pj,Cα∥< r,
137"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3047034764826176,"where Pi,Cα denotes the 3D position of Cα in the i-th
138"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3067484662576687,"residue.
139"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.30879345603271985,"Firstly, we design a base approach called CoupleNetaa
140"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.310838445807771,"that only uses the Cα positions of the structures. In-
141"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3128834355828221,"spired by Ingraham et al. [28], we construct a local
142"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3149284253578732,"coordinate system (LCS) for each residue, as shown in
143"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3169734151329243,"Figure 2.
144"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.31901840490797545,"Qi = [bi
ni
bi × ni]
(5)"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3210633946830266,"where ui =
Pi,Cα−Pi−1,Cα
∥Pi,Cα−Pi−1,Cα∥, bi =
ui−ui+1
∥ui−ui+1∥, ni =
ui×ui+1
∥ui×ui+1∥. Then we can get the geometric
145"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3231083844580777,"representations at the amino acid level of a protein 3D graph,
146"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.32515337423312884,"F(G)ij,aa = (∥Pi,Cα −Pj,Cα∥, QT
i ·
Pi,Cα −Pj,Cα
∥Pi,Cα −Pj,Cα∥, QT
i · Qj)
(6)"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.32719836400818,"where · is the matrix multiplication, this implementation is SE(3)-equivariant and obtains complete
147"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3292433537832311,"representations at the amino acid level; as if we have Qi, the LCS Qj can be easily obtained by
148"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3312883435582822,"F(G)ij,aa.
149"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3333333333333333,"For a node vi, the node features xi,aa in the baseline approach is the concatenation of the one-hot
150"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.33537832310838445,"embeddings of the amino acid types and the physicochemical properties of each residue, namely, a
151"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3374233128834356,"steric parameter, hydrophobicity, volume, polarizability, isoelectric point, helix probability and sheet
152"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3394683026584867,"probability [51, 22], which provide quantitative insights into the biochemical nature of each amino
153"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.34151329243353784,"acid. And F(G)ij,aa is set as edge features for CoupleNetaa.
154"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.34355828220858897,"Secondly, we consider all backbone atoms Cα, C, N, O in CoupleNet. In detail, the peptide bond
155"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3456032719836401,"exhibits partial double-bond character due to resonance [20], indicating that the three non-hydrogen
156"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3476482617586912,"atoms comprising the bond (the carbonyl oxygen, carbonyl carbon, and amide nitrogen) are coplanar,
157"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3496932515337423,"peptide plane
1.33Å 1.45Å 1.52Å 1.23Å"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.35173824130879344,backbone
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3537832310838446,"Figure 3: The polypeptide chain depicting the characteristic backbone bond lengths, angles, and
torsion angles (Ψi, Φi, Ωi). The planar peptide groups are denoted as shaded gray regions, indicating
that the peptide plane differs from the geometric plane calculated based on the 3D positions."
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3558282208588957,"as shown in Figure 3. There is some rotation about the connection. The Ni −Cαi and Cαi −Ci
158"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.35787321063394684,"bonds, are the two bonds in the basic repeating unit of the polypeptide backbone. These single bonds
159"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.35991820040899797,"allow unrestricted rotation until sterically restricted by side chains [35, 45]. Since the coordinates of
160"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3619631901840491,"Cα can be obtained as we have the complete representations at the amino acid level, the coordinates
161"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.36400817995910023,"of other backbone atoms based on these rigid bond lengths and angles are able to be determined with
162"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3660531697341513,"the remaining degree of the backbone torsion angles Φi, Ψi, Ωi. The omega torsion angle around
163"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.36809815950920244,"the C −N peptide bond is typically restricted to nearly 180◦(trans) but can approach 0◦(cis) in
164"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.37014314928425357,"rare instances. Other than the bond lengths and angles presented in Figure 3, all the H bond lengths
165"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3721881390593047,"measure approximately 1 Å.
166"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.37423312883435583,"For the sequential graph, we compute the sine and cosine values of Φi, Ψi, Ωi for each amino acid i,
167"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.37627811860940696,"and use them as another part of nodes features for node vi.
168"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3783231083844581,"xi = xi,aa∥((sin ∧cos)(Φi, Ψi, Ωi))
(7)"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3803680981595092,"where ∥denotes concatenation. There is no isolated node for the designed graph, which means
169"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3824130879345603,"the backbone atoms can be determined one by one along the polypeptide chain based on the po-
170"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.38445807770961143,"sitions of Cα and these three backbone dihedral angles. Therefore, the existing presentations
171"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.38650306748466257,"[F(G)ij,aa]i,j=1,...,n and [xi]i=1,...,n are complete at the backbone level for the sequential graph.
172"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3885480572597137,residue
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.39059304703476483,residue
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.39263803680981596,"Figure 4: Interresidue geometries in-
cluding angles and distances. 173"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3946830265848671,"For the radius graph, we want to get the positions of back-
174"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3967280163599182,"bone atoms in any two amino acids i and j. Inspired by
175"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.3987730061349693,"trRosetta [52], the relative rotation and distance are com-
176"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.40081799591002043,"puted including the distance (dij,Cβ), three dihedral angles
177"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.40286298568507156,"(ωij, θij, θji) and two planar angles (φij, φji), as shown in
178"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4049079754601227,"Figure 4, where dij,Cβ = dji,Cβ, ωij = ωji, but θ and φ
179"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4069529652351738,"values depend on the order of residues. These interresidue
180"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.40899795501022496,"geometries define the relative locations of the backbone
181"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4110429447852761,"atoms of two residues in all their details [52], because the
182"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4130879345603272,"torsion angles of Ni −Cαi and Cαi −Ci do not influ-
183"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.41513292433537835,"ence their positions. Therefore, these six geometries are
184"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4171779141104294,"complete for amino acids at the backbone level for the
185"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.41922290388548056,"radius graph. The graph edges contain the relative spa-
186"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4212678936605317,"tial information between any two neighboring amino acids
187"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4233128834355828,"eij = F(G)ij,aa∥F(G)ij,bb, where
188"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.42535787321063395,"F(G)ij,bb = (dij,Cβ, (sin ∧cos)(ωij, θij, φij))
(8)"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4274028629856851,"The designed node and edge features, xi and eij, for the sequential and radius graphs, provide a
189"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4294478527607362,"new perspective to represent protein sequences and structures. Such integration can bring better
190"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.43149284253578735,"performance for the following graph-based learning tasks.
191"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4335378323108384,"Residue
Geometric 
representatons Edge Node Node Pool- ing Edge Node"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.43558282208588955,"Node
Pool-"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4376278118609407,"ing
Node ... ... ..."
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4396728016359918,"Message passing
Message passing"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.44171779141104295,"FC
Soft- max"
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4437627811860941,Output
SEQUENCE-STRUCTURE GRAPH CONSTRUCTION,0.4458077709611452,Figure 5: An illustration of CoupleNet.
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.44785276073619634,"3.3
Secqunce-Structure Graph Convolution
192"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4498977505112474,"Inspired by the message passing paradigm and continuous-discrete convolution [15], sequences
193"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.45194274028629855,"and structures are encoded successfully together by convolutions. To deeply couple sequences and
194"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4539877300613497,"structures of proteins and encode them jointly, we employ convolution to embed them simultaneously,
195"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4560327198364008,"exploring their relationships to generate comprehensive and effective embeddings. Different from
196"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.45807770961145194,"previous works, we innovatively construct two categories of graphs for sequence and structure and
197"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4601226993865031,"design various sequential and structural representations to achieve completeness on them at the amino
198"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4621676891615542,"acid and backbone levels. We then convolve node and edge features with the help of the message
199"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.46421267893660534,"passing mechanism.
200"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4662576687116564,"In order to implement convolution on nodes and edges simultaneously between sequence and structure,
201"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.46830265848670755,"we set εij to exist if the following conditions are satisfied
202"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4703476482617587,"∥i −j∥< l
and
∥Pi,Cα −Pj,Cα∥< r
(9)"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4723926380368098,"The existing node and edge feature matrices (X, E) are complete representations of a protein 3D
203"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.47443762781186094,"graph to reconstruct its backbone atom positions. Compared with the equation Eq. 4, the proposed
204"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.47648261758691207,"CoupleNet first apply a FC(·) layer and a BN(·) layer to the node features to obtain the initial
205"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4785276073619632,"encoded representation. Then the f (l)
Agg is applied to gather neighboring features of nodes and
206"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.48057259713701433,"edges by convolution, where σ(·) is the activation function. We use the dropout and add a residual
207"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.48261758691206547,"connection from the previous layer as f (l)
Update. For the consideration that the spatial arrangement and
208"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.48466257668711654,"tight positioning of specific amino acids, which may be spaced widely apart on the linear polypeptide,
209"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4867075664621677,"are necessary for proteins to operate as intended [10], l is set to be a relatively large number, see
210"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4887525562372188,"Appendix A.2 for details.
211"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.49079754601226994,"h(0)
i
= BN (FC (xi)) ,"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.49284253578732107,"u(l)
i
= σ(BN(
X"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.4948875255623722,"vj∈N(vi)
Weijh(l−1)
j
),"
SECQUNCE-STRUCTURE GRAPH CONVOLUTION,0.49693251533742333,"h(l)
i
= h(l)
i
+ Dropout(u(l)
i ) (10)"
MODEL ARCHITECTURE,0.49897750511247446,"3.4
Model Architecture
212"
MODEL ARCHITECTURE,0.5010224948875256,"Building upon the sequence-structure graph convolution, we build the CoupleNet, as shown in
213"
MODEL ARCHITECTURE,0.5030674846625767,"Figure 5. The inputs to the graph are the calculated sequential and structural representations (X, E).
214"
MODEL ARCHITECTURE,0.5051124744376279,"Following the existing protein graph models [15, 25, 47], our CoupleNet employs graph pooling
215"
MODEL ARCHITECTURE,0.5071574642126789,"layers to obtain deeply encoded, graph-level representations. After pooling, due to the decrease
216"
MODEL ARCHITECTURE,0.50920245398773,"in nodes, we increase the predefined radius r to include more neighbors. The message passing
217"
MODEL ARCHITECTURE,0.5112474437627812,"mechanism only executes on nodes for the consideration of reducing model complexity. Another
218"
MODEL ARCHITECTURE,0.5132924335378323,"reason is that representations of sequences and structures have already been coupled by equation
219"
MODEL ARCHITECTURE,0.5153374233128835,"Eq. 4. A detailed description of the model architecture is provided in Appendix A.2.
220"
MODEL ARCHITECTURE,0.5173824130879345,"Table 1: Accuracy (%) on fold classification and enzyme reaction classification. [∗] means the results
are taken from [15]. The best and suboptimal results are shown in bold and underline."
MODEL ARCHITECTURE,0.5194274028629857,"Input
Method
Fold Classification
Enzyme"
MODEL ARCHITECTURE,0.5214723926380368,"Fold
SuperFamily
Family
Reaction"
MODEL ARCHITECTURE,0.523517382413088,Sequence
MODEL ARCHITECTURE,0.5255623721881391,"CNN [41]∗
11.3
13.4
53.4
51.7
ResNet [37]∗
10.1
7.21
23.5
24.1
LSTM [37]∗
6.41
4.33
18.1
11.0
Transformer [37]∗
9.22
8.81
40.4
26.6"
MODEL ARCHITECTURE,0.5276073619631901,Structure
MODEL ARCHITECTURE,0.5296523517382413,"GCN [32]∗
16.8
21.3
82.8
67.3
GAT [44]∗
12.4
16.5
72.7
55.6
3DCNN_MQA [11]∗
31.6
45.4
92.5
72.2
IEConv (atom level) [25]∗
45.0
69.7
98.9
87.2"
MODEL ARCHITECTURE,0.5316973415132924,Sequence-Structure
MODEL ARCHITECTURE,0.5337423312883436,"GraphQA [2]∗
23.7
32.5
84.4
60.8
GVP [29]∗
16.0
22.5
83.8
65.5
ProNet-Amino Acid [47]
51.5
69.9
99.0
86.0
ProNet-Backbone [47]
52.7
70.3
99.3
86.4
ProNet-All-Atom [47]
52.1
69.0
99.0
85.6
IEConv (residue level) [25]∗
47.6
70.2
99.2
87.2
GearNet [53]
28.4
42.6
95.3
79.4
GearNet-IEConv [53]
42.3
64.1
99.1
83.7
GearNet-Edge [53]
44.0
66.7
99.1
86.6
GearNet-Edge-IEConv [53]
48.3
70.3
99.5
85.3
CDConv [15]
56.7
77.7
99.6
88.5"
MODEL ARCHITECTURE,0.5357873210633947,"CoupleNet (Proposed)
60.6
82.1
99.7
89.0"
EXPERIMENTS,0.5378323108384458,"4
Experiments
221"
DATASETS AND SETTINGS,0.5398773006134969,"4.1
Datasets and Settings
222"
DATASETS AND SETTINGS,0.5419222903885481,"The models are trained with the Adam optimizer [31] using the PyTorch and PyTorch Geometric
223"
DATASETS AND SETTINGS,0.5439672801635992,"libraries. Detailed descriptions of the datasets and experimental settings are provided in Appendix A.3.
224"
DATASETS AND SETTINGS,0.5460122699386503,"Following the tasks in IEconv [25], GearNet [53] and CDConv [15], here, we evaluate the CoupleNet
225"
DATASETS AND SETTINGS,0.5480572597137015,"on four protein tasks: protein fold classification, enzyme reaction classification, GO term prediction
226"
DATASETS AND SETTINGS,0.5501022494887525,"and enzyme commission (EC) number prediction.
227"
DATASETS AND SETTINGS,0.5521472392638037,"Fold Classification
Protein fold is to predict the fold class label given a protein, which is crucial for
228"
DATASETS AND SETTINGS,0.5541922290388548,"understanding how protein structure and protein evolution interact [26]. In total, this dataset contains
229"
DATASETS AND SETTINGS,0.556237218813906,"16, 712 proteins with 1, 195 fold classes. There are three test sets available, Fold: Training excludes
230"
DATASETS AND SETTINGS,0.558282208588957,"proteins from the same superfamily. Superfamily: Training does not include proteins from the same
231"
DATASETS AND SETTINGS,0.5603271983640081,"family. Family: Proteins from the same family are included in the training.
232"
DATASETS AND SETTINGS,0.5623721881390593,"Enzyme Reaction Classification
Reaction categorization aims to predict a protein’s class of
233"
DATASETS AND SETTINGS,0.5644171779141104,"enzyme-catalyzed reactions, according to all four levels of the EC number [49, 36]. Following the
234"
DATASETS AND SETTINGS,0.5664621676891616,"setting in [25], this dataset has 37, 248 proteins from 384 four-level EC numbers [5].
235"
DATASETS AND SETTINGS,0.5685071574642127,"GO Term Prediction
The goal of GO term prediction is to foretell whether a protein is related
236"
DATASETS AND SETTINGS,0.5705521472392638,"to a certain GO term. Following [19], these proteins are organized into three ontologies: molecular
237"
DATASETS AND SETTINGS,0.5725971370143149,"function (MF), biological process (BP), and cellular component (CC), which are hierarchically
238"
DATASETS AND SETTINGS,0.5746421267893661,"connected, functional classes. MF describes activities that occur at the molecular level, BP represents
239"
DATASETS AND SETTINGS,0.5766871165644172,"the larger processes, and CC describes the parts of a cell or its extracellular environment [3].
240"
DATASETS AND SETTINGS,0.5787321063394683,"EC Number Prediction
This task seeks to predict the 538 EC numbers from the third level and
241"
DATASETS AND SETTINGS,0.5807770961145194,"fourth levels of different proteins [19], which describe their catalysis of biochemical reactions.
242"
DATASETS AND SETTINGS,0.5828220858895705,"Table 2: Fmax on GO term and EC number prediction. [∗] means the results are taken from [15]. The
best and suboptimal results are shown in bold and underline."
DATASETS AND SETTINGS,0.5848670756646217,"Category
Method
GO-BP
GO-MF
GO-CC
EC"
DATASETS AND SETTINGS,0.5869120654396728,Sequence
DATASETS AND SETTINGS,0.588957055214724,"CNN [41]∗
0.244
0.354
0.287
0.545
ResNet [37]∗
0.280
0.405
0.304
0.605
LSTM [37]∗
0.225
0.321
0.283
0.425
Transformer [37]∗
0.264
0.211
0.405
0.238"
DATASETS AND SETTINGS,0.591002044989775,"Structure
GCN [32]∗
0.252
0.195
0.329
0.320
GAT [44]∗
0.284
0.317
0.385
0.368
3DCNN_MQA [11]∗
0.240
0.147
0.305
0.077"
DATASETS AND SETTINGS,0.5930470347648262,Sequence-Structure
DATASETS AND SETTINGS,0.5950920245398773,"GraphQA [2]∗
0.308
0.329
0.413
0.509
GVP [29]∗
0.326
0.426
0.420
0.489
IEConv (residue level) [25]∗
0.421
0.624
0.431
-
GearNet [53]
0.356
0.503
0.414
0.730
GearNet-IEConv [53]
0.381
0.563
0.422
0.800
GearNet-Edge [53]
0.403
0.580
0.450
0.810
GearNet-Edge-IEConv [53]
0.400
0.581
0.430
0.810
CDConv [15]
0.453
0.654
0.479
0.820"
DATASETS AND SETTINGS,0.5971370143149284,"CoupleNet (Proposed)
0.467
0.669
0.494
0.866"
BASELINES,0.5991820040899796,"4.2
Baselines
243"
BASELINES,0.6012269938650306,"We compare our proposed method with existing protein representation learning methods, which are
244"
BASELINES,0.6032719836400818,"classified into three categories based on their inputs, which could be a sequence (amino acid sequence),
245"
BASELINES,0.6053169734151329,"3D structure or both sequence and structure. 1) Sequence-based encoders, including CNN [41],
246"
BASELINES,0.6073619631901841,"ResNet [37], LSTM [37] and Transformer [37]. 2) Structure-based methods (GCN [32], GAT [44],
247"
BASELINES,0.6094069529652352,"3DCNN_MQA [11], IEConv (atom level) [25]). 3) Sequence-structure based models, e.g., GVP [29],
248"
BASELINES,0.6114519427402862,"ProNet [47], GearNet [53], CDConv [15], etc. GearNet-IEConv and GearNetEdge-IEConv [53] add
249"
BASELINES,0.6134969325153374,"the IEConv layer based on GearNet, which is found important in fold classification.
250"
BASELINES,0.6155419222903885,"4.3
Resluts of Fold and Reaction Classification.
251"
BASELINES,0.6175869120654397,"Table 1 provides the comparisons on the fold and enzyme reaction classification. The results are
252"
BASELINES,0.6196319018404908,"reported in terms of accuracy (%) for these two tasks. From this table, we can see that the proposed
253"
BASELINES,0.621676891615542,"model CoupleNet achieves the best performance across all four test sets on the fold and enzyme
254"
BASELINES,0.623721881390593,"reaction classification compared with recent state-of-the-art methods. Especially on the Fold and
255"
BASELINES,0.6257668711656442,"SuperFamily test sets, CoupleNet improves the results by about 4%, showing that CoupleNet is
256"
BASELINES,0.6278118609406953,"proficient at learning the mapping between protein sequences, structures and functions. Moreover,
257"
BASELINES,0.6298568507157464,"CDConv [15] ranks second among these methods, both CDConv and our method are implemented
258"
BASELINES,0.6319018404907976,"by sequence-structure convolution. This phenomenon illustrates that deeply coupling sequences
259"
BASELINES,0.6339468302658486,"and structures of proteins is conducive to learning better protein embeddings. And our proposed
260"
BASELINES,0.6359918200408998,"CoupleNet model utilizes complete geometric representations and the specially designed message
261"
BASELINES,0.6380368098159509,"passing mechanism, achieving new state-of-the-art results.
262"
RESULTS OF GO TERM AND EC PREDICTION,0.6400817995910021,"4.4
Results of GO Term and EC Prediction
263"
RESULTS OF GO TERM AND EC PREDICTION,0.6421267893660532,"We follow the split method in [19, 53] to guarantee that the test set only comprises PDB chains with
264"
RESULTS OF GO TERM AND EC PREDICTION,0.6441717791411042,"sequence identity no higher than 95% to the training set for GO term and EC number prediction.
265"
RESULTS OF GO TERM AND EC PREDICTION,0.6462167689161554,"Table 2 compares different protein modeling methods on GO term prediction and EC number
266"
RESULTS OF GO TERM AND EC PREDICTION,0.6482617586912065,"prediction. The results are reported in terms of Fmax, which considers both precision and recall for
267"
RESULTS OF GO TERM AND EC PREDICTION,0.6503067484662577,"evaluation, the equation of Fmax is provided in Appendix A.4. The proposed model, CoupleNet
268"
RESULTS OF GO TERM AND EC PREDICTION,0.6523517382413088,"yields the highest Fmax across these four test sets of two tasks, outperforming other state-of-the-art
269"
RESULTS OF GO TERM AND EC PREDICTION,0.65439672801636,"models. This indicates CoupleNet can effectively predict the functions, locations, and enzymatic
270"
RESULTS OF GO TERM AND EC PREDICTION,0.656441717791411,"activities of proteins. These results once again illustrate the importance of deeply coupled sequences
271"
RESULTS OF GO TERM AND EC PREDICTION,0.6584867075664622,Table 3: Ablation of our proposed method
RESULTS OF GO TERM AND EC PREDICTION,0.6605316973415133,"Method
Fold Classification
Enzyme
GO
EC
Fold
Superfamily
Family
Reaction
BP
MF
CC"
RESULTS OF GO TERM AND EC PREDICTION,0.6625766871165644,"CoupleNet
60.6
82.1
99.7
89.0
0.467
0.669
0.494
0.866
CoupleNetaa
57.8
78.7
99.6
88.6
0.458
0.660
0.484
0.851
w/o Φ, Ψ, Ω
60.3
81.3
99.6
88.7
0.463
0.666
0.490
0.862
w/o d, ω, θ, φ
60.4
81.5
99.7
88.9
0.461
0.666
0.488
0.864"
RESULTS OF GO TERM AND EC PREDICTION,0.6646216768916156,"and structures. The improvements of CoupleNet over the suboptimal CDConv [15] model indicate
272"
RESULTS OF GO TERM AND EC PREDICTION,0.6666666666666666,"the advanced modeling power of CoupleNet.
273"
RESULTS OF GO TERM AND EC PREDICTION,0.6687116564417178,"Figure 6: Fmax on EC number prediction under
different cutoffs."
RESULTS OF GO TERM AND EC PREDICTION,0.6707566462167689,"We employ different cutoff splits following [19,
274"
RESULTS OF GO TERM AND EC PREDICTION,0.6728016359918201,"15], which means that the proteins in the test set
275"
RESULTS OF GO TERM AND EC PREDICTION,0.6748466257668712,"are divided into groups that have, respectively,
276"
RESULTS OF GO TERM AND EC PREDICTION,0.6768916155419223,"30%, 40%, 50%, 70%, and 95% similarity to
277"
RESULTS OF GO TERM AND EC PREDICTION,0.6789366053169734,"the training set for GO term and EC number pre-
278"
RESULTS OF GO TERM AND EC PREDICTION,0.6809815950920245,"diction, as shown in Figure 6 and Appendix A.5.
279"
RESULTS OF GO TERM AND EC PREDICTION,0.6830265848670757,"From Figure 6, we can see that our proposed
280"
RESULTS OF GO TERM AND EC PREDICTION,0.6850715746421268,"model CoupleNet achieves the highest Fmax
281"
RESULTS OF GO TERM AND EC PREDICTION,0.6871165644171779,"scores across all cutoffs, especially when the
282"
RESULTS OF GO TERM AND EC PREDICTION,0.689161554192229,"cutoffs are at 30% to 50%. There is a larger mar-
283"
RESULTS OF GO TERM AND EC PREDICTION,0.6912065439672802,"gin compared with GearNet, GearNet-Edge [53]
284"
RESULTS OF GO TERM AND EC PREDICTION,0.6932515337423313,"and CDConv [15]. This demonstrates that Cou-
285"
RESULTS OF GO TERM AND EC PREDICTION,0.6952965235173824,"pleNet, which utilizes complete geometric repre-
286"
RESULTS OF GO TERM AND EC PREDICTION,0.6973415132924335,"sentations, is more robust, especially when there
287"
RESULTS OF GO TERM AND EC PREDICTION,0.6993865030674846,"is a low similarity between the training and test
288"
RESULTS OF GO TERM AND EC PREDICTION,0.7014314928425358,"sets.
289"
ABLATION STUDY,0.7034764826175869,"4.5
Ablation Study
290"
ABLATION STUDY,0.7055214723926381,"Table 3 presents an ablation study of the proposed CoupleNet model on the four protein tasks. We
291"
ABLATION STUDY,0.7075664621676891,"examined the impact of removing the backbone torsion angles (w/o Φ, Ψ, Ω) and removing the
292"
ABLATION STUDY,0.7096114519427403,"interresidue geometric structure representations (w/o d, ω, θ, φ). The former is designed for the
293"
ABLATION STUDY,0.7116564417177914,"sequential graph, and the latter is for the radius graph to achieve completeness at the protein backbone
294"
ABLATION STUDY,0.7137014314928425,"level. However, we combine the two types of graphs together to enhance the relationships between
295"
ABLATION STUDY,0.7157464212678937,"sequence and structure. From Table 3, we can also find that these complete geometries provide
296"
ABLATION STUDY,0.7177914110429447,"complementary information to amino acid position features, with one of their removals leading to
297"
ABLATION STUDY,0.7198364008179959,"minor performance drops for the reason that they both provide complete geometries from different
298"
ABLATION STUDY,0.721881390593047,"perspectives. Removing Φ, Ψ, Ωcauses larger performance degradation compared with removing
299"
ABLATION STUDY,0.7239263803680982,"d, ω, θ, φ. Such comparisons indicate that the backbone dihedral angles may have more effects
300"
ABLATION STUDY,0.7259713701431493,"on learning protein representations in these experimental settings. Compared with CoupleNetaa,
301"
ABLATION STUDY,0.7280163599182005,"CoupleNet achieves significant improvements on the four tasks, demonstrating the importance of
302"
ABLATION STUDY,0.7300613496932515,"complete structural representations at the backbone level in learning protein embeddings.
303"
CONCLUSIONS AND LIMITATIONS,0.7321063394683026,"5
Conclusions and Limitations
304"
CONCLUSIONS AND LIMITATIONS,0.7341513292433538,"In this work, we propose CoupleNet, a novel protein representation learning method that deeply fuses
305"
CONCLUSIONS AND LIMITATIONS,0.7361963190184049,"protein sequences and multi-level structures by conducting convolution on graph nodes and edges
306"
CONCLUSIONS AND LIMITATIONS,0.7382413087934561,"simultaneously. We design the sequential graph and the radius graph, achieving completeness on
307"
CONCLUSIONS AND LIMITATIONS,0.7402862985685071,"them at different protein structure levels. Our approach achieves new state-of-the-art results on the
308"
CONCLUSIONS AND LIMITATIONS,0.7423312883435583,"protein tasks, which demonstrates the superiority our the proposed method. A limitation is that the
309"
CONCLUSIONS AND LIMITATIONS,0.7443762781186094,"detailed inter-relationships between sequence and structures remain to be explored and uncovered.
310"
CONCLUSIONS AND LIMITATIONS,0.7464212678936605,"We leave such research for future work.
311"
CONCLUSIONS AND LIMITATIONS,0.7484662576687117,"While our model can enable advanced protein analyses and provide effective representations, there
312"
CONCLUSIONS AND LIMITATIONS,0.7505112474437627,"may exist broader impacts and harmful activities. The representations could potentially be misused,
313"
CONCLUSIONS AND LIMITATIONS,0.7525562372188139,"e.g., for designing harmful molecules or proteins.
314"
REFERENCES,0.754601226993865,"References
315"
REFERENCES,0.7566462167689162,"[1]
Ethan C. Alley et al. “Unified rational protein engineering with sequence-based deep represen-
316"
REFERENCES,0.7586912065439673,"tation learning”. In: Nature Methods (2019).
317"
REFERENCES,0.7607361963190185,"[2]
Federico Baldassarre et al. “GraphQA: protein model quality assessment using graph convolu-
318"
REFERENCES,0.7627811860940695,"tional networks.” In: Bioinformatics (2020).
319"
REFERENCES,0.7648261758691206,"[3]
Alex Bateman. “UniProt: A worldwide hub of protein knowledge”. In: Nucleic Acids Research
320"
REFERENCES,0.7668711656441718,"(2019).
321"
REFERENCES,0.7689161554192229,"[4]
Tristan Bepler and Bonnie Berger. “Learning the protein language: Evolution, structure, and
322"
REFERENCES,0.7709611451942741,"function”. In: Cell systems 12.6 (2021), pp. 654–669.
323"
REFERENCES,0.7730061349693251,"[5]
Helen M Berman et al. “The protein data bank”. In: Nucleic acids research 28.1 (2000),
324"
REFERENCES,0.7750511247443763,"pp. 235–242.
325"
REFERENCES,0.7770961145194274,"[6]
Can Chen et al. “Structure-aware protein self-supervised learning”. In: Bioinformatics 39.4
326"
REFERENCES,0.7791411042944786,"(2023), btad189.
327"
REFERENCES,0.7811860940695297,"[7]
Sheng Chen et al. “To Improve Protein Sequence Profile Prediction through Image Captioning
328"
REFERENCES,0.7832310838445807,"on Pairwise Residue Distance Map”. In: Journal of Chemical Information and Modeling
329"
REFERENCES,0.7852760736196319,"(2020).
330"
REFERENCES,0.787321063394683,"[8]
Yihong Chen et al. “Refactor gnns: Revisiting factorisation-based models from a message-
331"
REFERENCES,0.7893660531697342,"passing perspective”. In: Advances in Neural Information Processing Systems 35 (2022),
332"
REFERENCES,0.7914110429447853,"pp. 16138–16150.
333"
REFERENCES,0.7934560327198364,"[9]
G Marius Clore and Angela M Gronenborn. “NMR structure determination of proteins and
334"
REFERENCES,0.7955010224948875,"protein complexes larger than 20 kDa”. In: Current opinion in chemical biology 2.5 (1998),
335"
REFERENCES,0.7975460122699386,"pp. 564–570.
336"
REFERENCES,0.7995910020449898,"[10]
Srinivasan Damodaran. “Amino acids, peptides and proteins”. In: Fennema’s food chemistry 4
337"
REFERENCES,0.8016359918200409,"(2008), pp. 425–439.
338"
REFERENCES,0.803680981595092,"[11]
Georgy Derevyanko et al. “Deep convolutional networks for quality assessment of protein
339"
REFERENCES,0.8057259713701431,"folds”. In: Bioinformatics 34.23 (2018), pp. 4046–4053.
340"
REFERENCES,0.8077709611451943,"[12]
Weitao Du et al. “SE (3) Equivariant Graph Neural Networks with Complete Local Frames”.
341"
REFERENCES,0.8098159509202454,"In: International Conference on Machine Learning. PMLR. 2022, pp. 5583–5608.
342"
REFERENCES,0.8118609406952966,"[13]
Arun Kumar Dubey and Vanita Jain. “Comparative study of convolution neural network’s relu
343"
REFERENCES,0.8139059304703476,"and leaky-relu activation functions”. In: Applications of Computing, Automation and Wireless
344"
REFERENCES,0.8159509202453987,"Systems in Electrical Engineering: Proceedings of MARC 2018. Springer. 2019, pp. 873–880.
345"
REFERENCES,0.8179959100204499,"[14]
Ahmed Elnaggar et al. “ProtTrans: Towards Cracking the Language of Lifes Code Through
346"
REFERENCES,0.820040899795501,"Self-Supervised Deep Learning and High Performance Computing”. In: IEEE Transactions on
347"
REFERENCES,0.8220858895705522,"Pattern Analysis and Machine Intelligence (2021).
348"
REFERENCES,0.8241308793456033,"[15]
Hehe Fan et al. “Continuous-Discrete Convolution for Geometry-Sequence Modeling in
349"
REFERENCES,0.8261758691206544,"Proteins”. In: The Eleventh International Conference on Learning Representations. 2023.
350"
REFERENCES,0.8282208588957055,"[16]
Noelia Ferruz and Birte Höcker. “Controllable protein design with language models”. In:
351"
REFERENCES,0.8302658486707567,"Nature Machine Intelligence (2022), pp. 1–12.
352"
REFERENCES,0.8323108384458078,"[17]
Fabian Fuchs et al. “Se (3)-transformers: 3d roto-translation equivariant attention networks”.
353"
REFERENCES,0.8343558282208589,"In: Advances in Neural Information Processing Systems 33 (2020), pp. 1970–1981.
354"
REFERENCES,0.83640081799591,"[18]
Justin Gilmer et al. “Neural message passing for quantum chemistry”. In: International
355"
REFERENCES,0.8384458077709611,"conference on machine learning. PMLR. 2017, pp. 1263–1272.
356"
REFERENCES,0.8404907975460123,"[19]
Vladimir Gligorijevi´c et al. “Structure-based protein function prediction using graph convolu-
357"
REFERENCES,0.8425357873210634,"tional networks”. In: Nature communications 12.1 (2021), p. 3168.
358"
REFERENCES,0.8445807770961146,"[20]
ER HARD GROSS and JOHANNES MEIENHOFER. “The Peptide Bond”. In: Major Methods
359"
REFERENCES,0.8466257668711656,"of Peptide Bond Formation: The Peptides Analysis, Synthesis, Biology, Vol. 1 1 (2014), p. 1.
360"
REFERENCES,0.8486707566462167,"[21]
Yuzhi Guo et al. “Self-supervised pre-training for protein embeddings using tertiary structures”.
361"
REFERENCES,0.8507157464212679,"In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 6. 2022, pp. 6801–
362"
REFERENCES,0.852760736196319,"6809.
363"
REFERENCES,0.8548057259713702,"[22]
Jack Hanson et al. “Improving prediction of protein secondary structure, backbone angles,
364"
REFERENCES,0.8568507157464212,"solvent accessibility and contact numbers by using predicted contact maps and an ensemble
365"
REFERENCES,0.8588957055214724,"of recurrent and residual convolutional neural networks”. In: Bioinformatics 35.14 (2019),
366"
REFERENCES,0.8609406952965235,"pp. 2403–2410.
367"
REFERENCES,0.8629856850715747,"[23]
Michael Heinzinger et al. “Modeling the language of life – Deep Learning Protein Sequences”.
368"
REFERENCES,0.8650306748466258,"In: bioRxiv (2019).
369"
REFERENCES,0.8670756646216768,"[24]
Pedro Hermosilla and Timo Ropinski. “Contrastive representation learning for 3d protein
370"
REFERENCES,0.869120654396728,"structures”. In: arXiv preprint arXiv:2205.15675 (2022).
371"
REFERENCES,0.8711656441717791,"[25]
Pedro Hermosilla et al. “Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D
372"
REFERENCES,0.8732106339468303,"Protein Structures”. In: International Conference on Learning Representations (2021).
373"
REFERENCES,0.8752556237218814,"[26]
Jie Hou, Badri Adhikari, and Jianlin Cheng. “DeepSF: deep convolutional neural network for
374"
REFERENCES,0.8773006134969326,"mapping protein sequences to folds”. In: Bioinformatics 34.8 (2018), pp. 1295–1303.
375"
REFERENCES,0.8793456032719836,"[27]
Bozhen Hu et al. “Protein Language Models and Structure Prediction: Connection and Pro-
376"
REFERENCES,0.8813905930470347,"gression”. In: arXiv preprint arXiv:2211.16742 (2022).
377"
REFERENCES,0.8834355828220859,"[28]
John Ingraham et al. “Generative models for graph-based protein design”. In: Advances in
378"
REFERENCES,0.885480572597137,"neural information processing systems 32 (2019).
379"
REFERENCES,0.8875255623721882,"[29]
Bowen Jing et al. “Learning from Protein Structure with Geometric Vector Perceptrons”. In:
380"
REFERENCES,0.8895705521472392,"Learning (2020).
381"
REFERENCES,0.8916155419222904,"[30]
John Jumper et al. “Highly accurate protein structure prediction with AlphaFold”. In: Nature
382"
REFERENCES,0.8936605316973415,"596.7873 (2021), pp. 583–589.
383"
REFERENCES,0.8957055214723927,"[31]
Diederik P Kingma and Jimmy Ba. “Adam: A method for stochastic optimization”. In: arXiv
384"
REFERENCES,0.8977505112474438,"preprint arXiv:1412.6980 (2014).
385"
REFERENCES,0.8997955010224948,"[32]
Thomas N Kipf and Max Welling. “Semi-supervised classification with graph convolutional
386"
REFERENCES,0.901840490797546,"networks”. In: arXiv preprint arXiv:1609.02907 (2016).
387"
REFERENCES,0.9038854805725971,"[33]
Zeming Lin et al. “Language models of protein sequences at the scale of evolution enable
388"
REFERENCES,0.9059304703476483,"accurate structure prediction”. In: BioRxiv (2022).
389"
REFERENCES,0.9079754601226994,"[34]
Bin Ma. “Novor: real-time peptide de novo sequencing software.” In: Journal of the American
390"
REFERENCES,0.9100204498977505,"Society for Mass Spectrometry (2015).
391"
REFERENCES,0.9120654396728016,"[35]
David L Nelson, Albert L Lehninger, and Michael M Cox. Lehninger principles of biochemistry.
392"
REFERENCES,0.9141104294478528,"Macmillan, 2008.
393"
REFERENCES,0.9161554192229039,"[36]
Marina V Omelchenko et al. “Non-homologous isofunctional enzymes: a systematic analysis
394"
REFERENCES,0.918200408997955,"of alternative solutions in enzyme evolution”. In: Biology direct 5 (2010), pp. 1–20.
395"
REFERENCES,0.9202453987730062,"[37]
Roshan Rao et al. “Evaluating protein transfer learning with TAPE”. In: Advances in neural
396"
REFERENCES,0.9222903885480572,"information processing systems 32 (2019).
397"
REFERENCES,0.9243353783231084,"[38]
Roshan M Rao et al. “MSA transformer”. In: International Conference on Machine Learning.
398"
REFERENCES,0.9263803680981595,"PMLR. 2021, pp. 8844–8856.
399"
REFERENCES,0.9284253578732107,"[39]
Alexander Rives et al. “Biological Structure and Function Emerge from Scaling Unsupervised
400"
REFERENCES,0.9304703476482618,"Learning to 250 Million Protein Sequences”. In: Proceedings of the National Academy of
401"
REFERENCES,0.9325153374233128,"Sciences of the United States of America (2019).
402"
REFERENCES,0.934560327198364,"[40]
Andrew W Senior et al. “Improved protein structure prediction using potentials from deep
403"
REFERENCES,0.9366053169734151,"learning”. In: Nature 577.7792 (2020), pp. 706–710.
404"
REFERENCES,0.9386503067484663,"[41]
Amir Shanehsazzadeh, David Belanger, and David Dohan. “Is transfer learning necessary for
405"
REFERENCES,0.9406952965235174,"protein landscape prediction?” In: arXiv preprint arXiv:2011.03443 (2020).
406"
REFERENCES,0.9427402862985685,"[42]
Nils Strodthoff et al. “UDSMProt: universal deep sequence models for protein classification”.
407"
REFERENCES,0.9447852760736196,"In: Bioinformatics 36.8 (Jan. 2020), pp. 2401–2409. ISSN: 1367-4803. DOI: 10 . 1093 /
408"
REFERENCES,0.9468302658486708,"bioinformatics/btaa003.
409"
REFERENCES,0.9488752556237219,"[43]
Mihaly Varadi et al. “AlphaFold Protein Structure Database: massively expanding the structural
410"
REFERENCES,0.950920245398773,"coverage of protein-sequence space with high-accuracy models”. In: Nucleic acids research
411"
REFERENCES,0.9529652351738241,"50.D1 (2022), pp. D439–D444.
412"
REFERENCES,0.9550102249488752,"[44]
Petar Velickovic et al. “Graph attention networks”. In: stat 1050.20 (2017), pp. 10–48550.
413"
REFERENCES,0.9570552147239264,"[45]
K Peter C Vollhardt and Neil E Schore. Organic chemistry: structure and function. Macmillan,
414"
REFERENCES,0.9591002044989775,"2003.
415"
REFERENCES,0.9611451942740287,"[46]
Limei Wang et al. “ComENet: Towards Complete and Efficient Message Passing for 3D
416"
REFERENCES,0.9631901840490797,"Molecular Graphs”. In: arXiv preprint arXiv:2206.08515 (2022).
417"
REFERENCES,0.9652351738241309,"[47]
Limei Wang et al. “Learning Hierarchical Protein Representations via Complete 3D Graph
418"
REFERENCES,0.967280163599182,"Networks”. In: The Eleventh International Conference on Learning Representations. 2023.
419"
REFERENCES,0.9693251533742331,"[48]
Zeyuan Wang et al. “Multi-level Protein Structure Pre-training via Prompt Learning”. In: The
420"
REFERENCES,0.9713701431492843,"Eleventh International Conference on Learning Representations.
421"
REFERENCES,0.9734151329243353,"[49]
Edwin C Webb et al. Enzyme nomenclature 1992. Recommendations of the Nomenclature Com-
422"
REFERENCES,0.9754601226993865,"mittee of the International Union of Biochemistry and Molecular Biology on the Nomenclature
423"
REFERENCES,0.9775051124744376,"and Classification of Enzymes. Ed. 6. Academic Press, 1992.
424"
REFERENCES,0.9795501022494888,"[50]
Fang Wu, Dragomir Radev, and Jinbo Xu. “When Geometric Deep Learning Meets Pretrained
425"
REFERENCES,0.9815950920245399,"Protein Language Models”. In: bioRxiv (2023), pp. 2023–01.
426"
REFERENCES,0.983640081799591,"[51]
Gang Xu, Qinghua Wang, and Jianpeng Ma. “OPUS-Rota4: a gradient-based protein side-chain
427"
REFERENCES,0.9856850715746421,"modeling framework assisted by deep learning-based predictors”. In: Briefings in Bioinformat-
428"
REFERENCES,0.9877300613496932,"ics 23.1 (2022), bbab529.
429"
REFERENCES,0.9897750511247444,"[52]
Jianyi Yang et al. “Improved protein structure prediction using predicted interresidue ori-
430"
REFERENCES,0.9918200408997955,"entations”. In: Proceedings of the National Academy of Sciences 117.3 (2020), pp. 1496–
431"
REFERENCES,0.9938650306748467,"1503.
432"
REFERENCES,0.9959100204498977,"[53]
Zuobai Zhang et al. “Protein representation learning by geometric structure pretraining”. In:
433"
REFERENCES,0.9979550102249489,"International Conference on Learning Representations. 2023.
434"
