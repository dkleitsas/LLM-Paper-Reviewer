Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009823182711198428,"Existing large language models (LLMs) evaluation methods typically focus on test-
1"
ABSTRACT,0.0019646365422396855,"ing the performance on some closed-environment and domain-specific benchmarks
2"
ABSTRACT,0.0029469548133595285,"with human annotations. In this paper, we explore a novel unsupervised evalua-
3"
ABSTRACT,0.003929273084479371,"tion direction, utilizing peer-review mechanisms to measure LLMs automatically
4"
ABSTRACT,0.004911591355599214,"without any human feedback. In this setting, both open-source and closed-source
5"
ABSTRACT,0.005893909626719057,"LLMs lie in the same environment, capable of answering unlabeled questions and
6"
ABSTRACT,0.0068762278978389,"evaluating each other, where each LLM’s response score is jointly determined
7"
ABSTRACT,0.007858546168958742,"by other anonymous ones. To obtain the ability hierarchy among these models,
8"
ABSTRACT,0.008840864440078585,"we assign each LLM a learnable capability parameter to adjust the final ranking.
9"
ABSTRACT,0.009823182711198428,"We formalize it as a constrained optimization problem, intending to maximize the
10"
ABSTRACT,0.010805500982318271,"consistency of each LLM’s capabilities and scores. The key assumption behind is
11"
ABSTRACT,0.011787819253438114,"that high-level LLM can evaluate others’ answers more accurately than low-level
12"
ABSTRACT,0.012770137524557957,"ones, while higher-level LLM can also achieve higher response scores. Moreover,
13"
ABSTRACT,0.0137524557956778,"we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning
14"
ABSTRACT,0.014734774066797643,"human rankings. We perform experiments on multiple datasets with these metrics,
15"
ABSTRACT,0.015717092337917484,"validating the effectiveness of the proposed approach.
16"
INTRODUCTION,0.01669941060903733,"1
Introduction
17"
INTRODUCTION,0.01768172888015717,"Goodhart’s Law: “When a measure becomes a target, it ceases to be a good
18"
INTRODUCTION,0.018664047151277015,"measure.”
19"
INTRODUCTION,0.019646365422396856,"Large language models (LLMs)[11, 2, 12, 43] have achieved remarkable success across a variety
20"
INTRODUCTION,0.0206286836935167,"of real-world applications [54, 32, 36, 52]. With the increasingly widespread application of these
21"
INTRODUCTION,0.021611001964636542,"models, there is an urgent need for an effective evaluation method to ensure that their performance
22"
INTRODUCTION,0.022593320235756387,"and usability meet the growing demands. To assess the ability level of LLMs, a large number of
23"
INTRODUCTION,0.023575638506876228,"evaluation benchmarks have been proposed by using some small and domain-specific datasets with
24"
INTRODUCTION,0.02455795677799607,"human-curated labels, such as MMLU [26], HELM [30], Big-Bench[39], GLUE[45]. However, these
25"
INTRODUCTION,0.025540275049115914,"benchmarks can only measure LLMs’ core capability on a confined set of tasks (e.g. multi-choice
26"
INTRODUCTION,0.026522593320235755,"knowledge or retrieval questions), which fails to assess their alignment with human preference in
27"
INTRODUCTION,0.0275049115913556,"open-ended tasks adequately [16, 28, 34]. On the other hand, these evaluations may suffer from
28"
INTRODUCTION,0.02848722986247544,"benchmark leakage issue, referring that the evaluation data is unknowingly used for model training,
29"
INTRODUCTION,0.029469548133595286,"which can also lead to misleading evaluations [49, 56]. Therefore, blindly improving scores on
30"
INTRODUCTION,0.030451866404715127,"these public benchmarks cannot always yield a large language model that truly satisfies human
31"
INTRODUCTION,0.03143418467583497,"requirements.
32"
INTRODUCTION,0.03241650294695481,"For assessing human preferences, recent studies have focused on building crowdsourced battle
33"
INTRODUCTION,0.03339882121807466,"platforms with human ratings as the primary evaluation metric. Typical platforms include Chatbot
34"
INTRODUCTION,0.0343811394891945,"Arena [55], MT-Bench [55], and AlpacaEval [29]. It constructs anonymous battles between chatbots
35"
INTRODUCTION,0.03536345776031434,"in real-world scenarios, where users engage in conversations with two chatbots at the same time and
36"
INTRODUCTION,0.036345776031434185,"rate their responses based on personal preferences. While human evaluation is the gold standard for
37"
INTRODUCTION,0.03732809430255403,"Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same
environment, capable of answering unlabeled questions and evaluating each other, where each LLM’s response
score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to
optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review
evaluation system. The consistency optimization aims to find a final score ranking that all LLMs “agree” it."
INTRODUCTION,0.03831041257367387,"measuring human preferences, it is exceptionally slow and costly[55]. In addition, adding a new
38"
INTRODUCTION,0.03929273084479371,"LLM to the crowdsourced battle platforms also poses a cold-start issue [15]. Thus, a fundamental
39"
INTRODUCTION,0.04027504911591356,"question arises: can we construct an unsupervised LLMs evaluation system without relying on any
40"
INTRODUCTION,0.0412573673870334,"human feedback?
41"
INTRODUCTION,0.04223968565815324,"Actually, in real human evaluation systems, people build their ability hierarchy based on different
42"
INTRODUCTION,0.043222003929273084,"empirical assumptions. For example, majority voting [22, 10, 40] and rating voting [5] methods
43"
INTRODUCTION,0.04420432220039293,"are widely used during the decision-making process, which are based on the wisdom of the crowds
44"
INTRODUCTION,0.04518664047151277,"[40, 13, 50] and have been proven to lead to better results than that of an individual. Moreover, in
45"
INTRODUCTION,0.04616895874263261,"the established practice of peer-review in academic research, scholars evaluate their academic level
46"
INTRODUCTION,0.047151277013752456,"rankings based on the consistency assumption, i.e., scholars with stronger abilities have stronger
47"
INTRODUCTION,0.0481335952848723,"persuasiveness for evaluating others, and can also obtain higher achievements. This paper attempts to
48"
INTRODUCTION,0.04911591355599214,"explore whether similar phenomena exist in the LLMs evaluation systems.
49"
INTRODUCTION,0.05009823182711198,"In this work, we propose PiCO, a Peer review approach in LLMs based on Consistency Optimization.
50"
INTRODUCTION,0.05108055009823183,"In this setting, LLMs themselves act as “reviewers”, engaging in mutual assessments to achieve
51"
INTRODUCTION,0.05206286836935167,"comprehensive, efficient, and performance evaluations without relying on manually annotated data.
52"
INTRODUCTION,0.05304518664047151,"This method aims to address the limitations of existing evaluation approaches and provide insights
53"
INTRODUCTION,0.054027504911591355,"into LLMs’ real-world capabilities. As shown in Figure 1, both open-source and closed-source
54"
INTRODUCTION,0.0550098231827112,"LLMs lie in the same environment and answer the open-ended questions from an unlabeled dataset.
55"
INTRODUCTION,0.055992141453831044,"Then, we construct anonymous answer pairs, while randomly selecting other LLMs as “reviewers” to
56"
INTRODUCTION,0.05697445972495088,"evaluate both responses with a learnable confidence weight w. Finally, we employ this weight and
57"
INTRODUCTION,0.05795677799607073,"calculate the response scores G for each LLM based on the weighted joint evaluation. It is worth
58"
INTRODUCTION,0.05893909626719057,"noting that the whole peer-review process works in an unsupervised way, and our goal is to optimize
59"
INTRODUCTION,0.05992141453831041,"the confidence weights that re-rank the LLMs to be closer to human rankings.
60"
INTRODUCTION,0.060903732809430254,"To achieve this, we formalize it as a constrained optimization based on the consistency assumption. We
61"
INTRODUCTION,0.0618860510805501,"maximize the consistency of each LLM’s capability w and score G while adjusting the final ranking
62"
INTRODUCTION,0.06286836935166994,"to align with human preference more closely. The key assumption behind this is that high-level LLM
63"
INTRODUCTION,0.06385068762278978,"can evaluate others’ answers more accurately (confidence) than low-level ones, while higher-level
64"
INTRODUCTION,0.06483300589390963,"LLM can also achieve higher answer-ranking scores. As a result, the entropy (controversy) of the
65"
INTRODUCTION,0.06581532416502947,"whole peer-review evaluation system can be minimized. In other words, the consistency optimization
66"
INTRODUCTION,0.06679764243614932,"aims to find a final score ranking that all LLMs have no “disputes” regarding.
67"
INTRODUCTION,0.06777996070726916,"To evaluate the gap in aligning human rankings, we propose three metrics called PEN (Permutation
68"
INTRODUCTION,0.068762278978389,"Entropy), CIN (Count Inversions), LIS (Longest Increasing Subsequence). The experiments are
69"
INTRODUCTION,0.06974459724950884,"conducted on multiple crowdsourcing datasets and validated on these three metrics. The experimental
70"
INTRODUCTION,0.07072691552062868,"results demonstrate that the proposed PiCO framework can effectively obtain a large language models’
71"
INTRODUCTION,0.07170923379174853,"leaderboard closer to human preferences.
72"
INTRODUCTION,0.07269155206286837,"Figure 2: Preference alignment metric. Three metrics for evaluating the gap with human preferences called PEN,
CIN, and LIS, respectively"
INTRODUCTION,0.07367387033398821,"The contributions of this paper can be summarized as follows.
73"
INTRODUCTION,0.07465618860510806,"• We explore a novel unsupervised LLM evaluation direction without human feedback, uti-
74"
INTRODUCTION,0.0756385068762279,"lizing peer-review mechanisms to measure LLMs automatically. All LLMs can answer
75"
INTRODUCTION,0.07662082514734773,"unlabeled questions and evaluate each other.
76"
INTRODUCTION,0.07760314341846758,"• A constrained optimization based on the consistency assumption is proposed to re-rank the
77"
INTRODUCTION,0.07858546168958742,"LLMs to be closer to human rankings.
78"
INTRODUCTION,0.07956777996070727,"• We propose three metrics called PEN, CIN, and LIS on the PiCO framework for evaluating
79"
INTRODUCTION,0.08055009823182711,"the gap with human preferences.
80"
INTRODUCTION,0.08153241650294696,"• The experiments with these metrics on three crowdsourcing datasets validate the effective-
81"
INTRODUCTION,0.0825147347740668,"ness of the proposed approach.
82"
THE PROPOSED APPROACH,0.08349705304518663,"2
The Proposed Approach
83"
THE PROPOSED APPROACH,0.08447937131630648,"In this section, we first describe the problem definition and preference alignment evaluation, and then
84"
THE PROPOSED APPROACH,0.08546168958742632,"introduce the proposed PiCO framework in detail.
85"
DEFINITION AND METRICS,0.08644400785854617,"2.1
Definition and Metrics
86"
DEFINITION AND METRICS,0.08742632612966601,"Problem Definition. In this subsection, we aim to measure the ability level of LLMs automatically
87"
DEFINITION AND METRICS,0.08840864440078586,"without relying on human annotations. Thus we consider an unsupervised LLM evaluation scenario
88"
DEFINITION AND METRICS,0.0893909626719057,"with an unlabeled dataset Q consisting of n open-ended questions, where Q = {Qi}n
i=1. In addition,
89"
DEFINITION AND METRICS,0.09037328094302555,"we have a large language model pool M = {Mj}m
j=1, which includes both open-source and closed-
90"
DEFINITION AND METRICS,0.09135559921414538,"source models. Write M1 ≻M2 to indicate that the LLM M1 has stronger capabilities than the LLM
91"
DEFINITION AND METRICS,0.09233791748526522,"M2. Thus, we can assume that the ground-truth ranking R∗alignment with human preferences,
92"
DEFINITION AND METRICS,0.09332023575638507,"R∗:= [M1 ≻M2 ≻M3 ≻... ≻Mm],
(1)"
DEFINITION AND METRICS,0.09430255402750491,"and assume that the learned ranking ˆR by different evaluation methods is as follows,
93"
DEFINITION AND METRICS,0.09528487229862476,"ˆR := [M3 ≻M1 ≻M2 ≻... ≻Mm].
(2)"
DEFINITION AND METRICS,0.0962671905697446,"The goal is to build an LLM ranking ˆR that aligns with human ranking R∗, making the loss L of the
94"
DEFINITION AND METRICS,0.09724950884086445,"both rankings tend towards 0, i.e., L( ˆR, R∗) →0
95"
DEFINITION AND METRICS,0.09823182711198428,"Preference Alignment Metrics. Before building LLM rankings, we first need to discuss how to
96"
DEFINITION AND METRICS,0.09921414538310412,"evaluate aligned human rankings. Intuitively, the metrics we want mainly describe the differences
97"
DEFINITION AND METRICS,0.10019646365422397,"between two arrays composed of ranking indices. Assuming that human ranking R∗is defined as
98"
DEFINITION AND METRICS,0.10117878192534381,"being well-ranked in ascending order ([1, 2, 3, ..., m]) as shown in Eq 1. Thus the metric is to quantify
99"
DEFINITION AND METRICS,0.10216110019646366,"the randomness of the learned ranking array ([3, 1, 2, ..., m]) as shown in Eq 2. Based on this, we
100"
DEFINITION AND METRICS,0.1031434184675835,"propose three metrics called PEN, CIN, and LIS, respectively.
101"
DEFINITION AND METRICS,0.10412573673870335,"PEN (Permutation Entropy). Permutation entropy [8] is a concept used to quantify the complexity or
102"
DEFINITION AND METRICS,0.10510805500982318,"randomness of time series data. It provides a measure of the irregularity or unpredictability of the
103"
DEFINITION AND METRICS,0.10609037328094302,"order of values in a sequence. We thus utilize it to measure the gap with human rankings as follows,
104"
DEFINITION AND METRICS,0.10707269155206287,"LP EN( ˆR, R∗) := −
X
p(π) log p(π),
(3)"
DEFINITION AND METRICS,0.10805500982318271,"where
105"
DEFINITION AND METRICS,0.10903732809430255,"p(π) = #{t|0 ≤t ≤m −k, (Mt+1, ..., Mt+k) ∈π}"
DEFINITION AND METRICS,0.1100196463654224,"m −k + 1
."
DEFINITION AND METRICS,0.11100196463654224,"Figure 3: The pipeline of the PiCO. It is mainly composed of two components: the peer-review and consistency
optimization stages. Specifically, in the peer-review stage, the unlabeled dataset Q and the LLMs pool M are
given. Then, we let all LLMs answer each unlabeled question to obtain the response set A. We shuffle the set
and construct anonymous answer pairs, while randomly selecting other LLMs to evaluate both responses with a
learnable confidence w. As a result, we can obtain the answer-ranking data D which is a quadruple that records
the partial order between two answers and the evaluator’s confidence weight. In the consistency optimization
stage, we update the parameter w by maximizing the consistency of each LLM’s capability and score, while
re-ranking the LLMs to be closer to human rankings."
DEFINITION AND METRICS,0.11198428290766209,"π denotes different permutations, k is a hyper-parameter recommended to be set to 3 to 7, and we
106"
DEFINITION AND METRICS,0.11296660117878192,"set k = 3 in this paper. Intuitively, it samples some subsequences and calculates the entropy for all
107"
DEFINITION AND METRICS,0.11394891944990176,"permutation types. And the lower the permutation entropy in the learned LLM rankings, the closer it
108"
DEFINITION AND METRICS,0.11493123772102161,"is to the ground-truth human rankings.
109"
DEFINITION AND METRICS,0.11591355599214145,"CIN (Count Inversions). Counting inversions [27] aims to measure the degree of disorder or
110"
DEFINITION AND METRICS,0.1168958742632613,"""invertedness"" in an array or sequence of elements. We thus define it as follows,
111"
DEFINITION AND METRICS,0.11787819253438114,"LCIN( ˆR, R∗) :=
X"
DEFINITION AND METRICS,0.11886051080550099,"Mi,Mj∼M
1{Mi ≻Mj ∧i < j}.
(4)"
DEFINITION AND METRICS,0.11984282907662082,"Where 1{·} is the indicator function that the value is 1 when the condition is met, otherwise it is 0.
112"
DEFINITION AND METRICS,0.12082514734774066,"Intuitively, the fewer inverse pairs in the learned LLM rankings, the closer it is to the ground-truth
113"
DEFINITION AND METRICS,0.12180746561886051,"human rankings.
114"
DEFINITION AND METRICS,0.12278978388998035,"LIS (Longest Increasing Subsequence). The longest increasing subsequence aims to find the length
115"
DEFINITION AND METRICS,0.1237721021611002,"of the longest subsequence in a given sequence of elements, where the subsequence is in increasing
116"
DEFINITION AND METRICS,0.12475442043222004,"order. We utilize it to measure the degree of match with human rankings as follows,
117"
DEFINITION AND METRICS,0.12573673870333987,"LLIS( ˆR, R∗) := max {dp[i] | 1 ≤i ≤m} ,
(5)"
DEFINITION AND METRICS,0.12671905697445973,"where
118"
DEFINITION AND METRICS,0.12770137524557956,"dp[i] = 1 + max {dp[j] | 1 ≤j < i ∧Mj ≺Mi} .
dp[i] represents the length of the longest increasing subsequence that ends with Mi. LIS allows for
119"
DEFINITION AND METRICS,0.12868369351669942,"a nuanced understanding of the degree to which the learned ranking aligns with the ideal human
120"
DEFINITION AND METRICS,0.12966601178781925,"ranking, with a higher LIS length indicating greater alignment.
121"
ALGORITHM DETAILS,0.13064833005893908,"2.2
Algorithm Details
122"
ALGORITHM DETAILS,0.13163064833005894,"The PiCO framework, depicted in Figure 3, involves peer-review and consistency optimization stages.
123"
ALGORITHM DETAILS,0.13261296660117877,"In the peer-review stage, we first collect an unlabeled dataset Q consisting of open-ended questions,
124"
ALGORITHM DETAILS,0.13359528487229863,"and construct a large language model pool M that includes both open-source and closed-source
125"
ALGORITHM DETAILS,0.13457760314341846,"LLMs. Then, we let all LLMs answer each unlabeled question to obtain the response set A. We
126"
ALGORITHM DETAILS,0.13555992141453832,"shuffle the set and construct anonymous answer pairs, while randomly selecting other LLMs as
127"
ALGORITHM DETAILS,0.13654223968565815,"“reviewers” to evaluate both responses with a learnable confidence w. Finally, we can obtain the
128"
ALGORITHM DETAILS,0.137524557956778,"answer-ranking data D and calculate the response score G for each large language model. In the
129"
ALGORITHM DETAILS,0.13850687622789784,"consistency optimization phase, we maximize the consistency of each LLM’s capability w and score
130"
ALGORITHM DETAILS,0.13948919449901767,"G with constrained optimization, while re-ranking the LLMs to be closer to human rankings.
131"
PEER REVIEW STAGE,0.14047151277013753,"2.2.1
Peer Review Stage
132"
PEER REVIEW STAGE,0.14145383104125736,"Data Collection and LLMs Pool Construction. Benefiting from the creation of crowdsourced
133"
PEER REVIEW STAGE,0.14243614931237722,"battle platforms, we accessed open assessment datasets from Chatbot Arena[55], MT-Bench[55],
134"
PEER REVIEW STAGE,0.14341846758349705,"and AlpacaEval[29].
These open datasets include critical fields such as ""question_id"" and
135"
PEER REVIEW STAGE,0.1444007858546169,"""question_content."" Utilizing the Chatbot Arena dataset, which features pairwise data from twenty
136"
PEER REVIEW STAGE,0.14538310412573674,"LLMs with human preference annotations, we assembled an LLM pool M = {Mj}m
j=1. Leveraging
137"
PEER REVIEW STAGE,0.14636542239685657,"33K human-annotated interactions from this dataset, we established a ground-truth ranking R∗and
138"
PEER REVIEW STAGE,0.14734774066797643,"gathered responses A = {{Aj
i}n
i=1}m
j=1 for our dataset Q = {Qi}n
i=1.
139"
PEER REVIEW STAGE,0.14833005893909626,"Answer-Ranking Data Construction Based on Peer Review. After obtaining the responses set A,
140"
PEER REVIEW STAGE,0.14931237721021612,"we aim to generate answer-ranking data D through the peer-review mechanism. Specifically, for the
141"
PEER REVIEW STAGE,0.15029469548133595,"same question Qi ∈Q, we randomly construct a battle pair < Aj
i, Ak
i > for review. Each battle pair
142"
PEER REVIEW STAGE,0.1512770137524558,"will be randomly assigned five models (“reviewers”) to determine the winners or declare ties. Note
143"
PEER REVIEW STAGE,0.15225933202357564,"that the model may evaluate its own answers, but the entire process is anonymous. As a result, we
144"
PEER REVIEW STAGE,0.15324165029469547,"can obtain the quadruples (Aj
i, Ak
i , > ws), indicating the “reviewer” Ms believes that the answer Aj
i
145"
PEER REVIEW STAGE,0.15422396856581533,"is better than answer Ak
i with a confidence ws. Therefore, the answer-ranking data D can be defined
146"
PEER REVIEW STAGE,0.15520628683693516,"as follows,
147"
PEER REVIEW STAGE,0.15618860510805502,"D =
n
(Aj
i, Ak
i , >, ws)
o"
PEER REVIEW STAGE,0.15717092337917485,"i∼Q,j,k,s∼M ,
(6)"
PEER REVIEW STAGE,0.1581532416502947,"where i denotes the question index, and j, k, s indicate the model indices. ws is a learnable confidence
148"
PEER REVIEW STAGE,0.15913555992141454,"of model Ms, and > is a partial order relationship from {>, <, =}.
149"
CONSISTENCY OPTIMIZATION STAGE,0.16011787819253437,"2.2.2
Consistency Optimization Stage
150"
CONSISTENCY OPTIMIZATION STAGE,0.16110019646365423,"As shown in Eq 6, following the peer-review mechanism, we construct anonymous answer pairs and
151"
CONSISTENCY OPTIMIZATION STAGE,0.16208251473477406,"randomly select other LLMs as “reviewers” to evaluate both responses with a learnable confidence w.
152"
CONSISTENCY OPTIMIZATION STAGE,0.16306483300589392,"Next, we expect to optimize the confidence w and re-rank the LLMs to be closer to human rankings.
153"
CONSISTENCY OPTIMIZATION STAGE,0.16404715127701375,"We thus propose the consistency assumption, i.e., high-level LLM can evaluate others’ answers
154"
CONSISTENCY OPTIMIZATION STAGE,0.1650294695481336,"more accurately (confidence) than low-level ones, while higher-level LLM can also achieve higher
155"
CONSISTENCY OPTIMIZATION STAGE,0.16601178781925344,"answer-ranking scores. Formally, we maximize the consistency of each LLM’s capability w and
156"
CONSISTENCY OPTIMIZATION STAGE,0.16699410609037327,"score G with constrained optimization as follows,
157"
CONSISTENCY OPTIMIZATION STAGE,0.16797642436149313,"argmax
w
Consistency(G, w)
(7)"
CONSISTENCY OPTIMIZATION STAGE,0.16895874263261296,"s.t. Gj =
X"
CONSISTENCY OPTIMIZATION STAGE,0.16994106090373282,"(Aj
i ,Ak
i ,>,ws)∼D
1{Aj
i > Ak
i } ∗ws,"
CONSISTENCY OPTIMIZATION STAGE,0.17092337917485265,"where 1{·} is the indicator function that the value is 1 when the condition is met, otherwise, it is 0.
158"
CONSISTENCY OPTIMIZATION STAGE,0.1719056974459725,"Gj denotes the response score of model Mj, which is calculated by joint evaluation of other models.
159"
CONSISTENCY OPTIMIZATION STAGE,0.17288801571709234,"Moreover, we employ Pearson correlation [38] to measure the consistency between w and G. Note
160"
CONSISTENCY OPTIMIZATION STAGE,0.1738703339882122,"that we only introduce this straightforward implementation to validate our idea of PiCO. Other more
161"
CONSISTENCY OPTIMIZATION STAGE,0.17485265225933203,"advanced strategies may be employed to further improve the performance.
162"
CONSISTENCY OPTIMIZATION STAGE,0.17583497053045186,"Discussion: It is worth noting that the whole process (Eq. 6 and 7) works in an unsupervised way.
163"
CONSISTENCY OPTIMIZATION STAGE,0.17681728880157171,"The only thing we do is to adaptively assign each LLM a score that matches its abilities. An intuitive
164"
CONSISTENCY OPTIMIZATION STAGE,0.17779960707269155,"example is as follows: in a real peer-review system, if the academic level of three scholars a, b, and c
165"
CONSISTENCY OPTIMIZATION STAGE,0.1787819253438114,"satisfies the following relationship, wa > wb > wc. So, in the ultimate ideal scenario, the ranking
166"
CONSISTENCY OPTIMIZATION STAGE,0.17976424361493124,"of the scores submitted by these three scholars should also be, Ga > Gb > Gc. In other words, the
167"
CONSISTENCY OPTIMIZATION STAGE,0.1807465618860511,"sorting of G and w satisfies high consistency. On the other hand, scholars with stronger abilities (i.e.,
168"
CONSISTENCY OPTIMIZATION STAGE,0.18172888015717092,"scholar a) evaluate Ab > Ac have stronger persuasiveness, so scholar b should also receive higher
169"
CONSISTENCY OPTIMIZATION STAGE,0.18271119842829076,"weighted scores 1 ∗wa.
170"
CONSISTENCY OPTIMIZATION STAGE,0.18369351669941061,"Reviewer Elimination Mechanism. Realizing that not all LLMs have sufficient ability to evaluate
171"
CONSISTENCY OPTIMIZATION STAGE,0.18467583497053044,"the responses of other models. We thus introduce an unsupervised elimination mechanism to remove
172"
CONSISTENCY OPTIMIZATION STAGE,0.1856581532416503,"those LLMs that have low scores. It iteratively removes the lowest-scoring LLM from the “reviewer
173"
CONSISTENCY OPTIMIZATION STAGE,0.18664047151277013,"queue” for the next consistency optimization stage, until 60% of models are eliminated. The whole
174"
CONSISTENCY OPTIMIZATION STAGE,0.18762278978389,"process of the approach is summarized in Algorithm 1, and the details can be found in Appendix D.
175"
EXPERIMENTS,0.18860510805500982,"3
Experiments
176"
EXPERIMENTS,0.18958742632612965,"Datasets. To validate the effectiveness of the proposed approach, we perform experiments on Chatbot
177"
EXPERIMENTS,0.1905697445972495,"Arena[55], MT-Bench[55], and AlpacaEval[29]. The MT-Bench dataset assesses six LLMs’ responses
178"
EXPERIMENTS,0.19155206286836934,"to 80 multi-category questions. The Chatbot Arena Conversations Dataset, with 33K conversations
179"
EXPERIMENTS,0.1925343811394892,"from 13K IPs during April-June 2023, evaluates real dialogue performance. AlpacaEval dataset
180"
EXPERIMENTS,0.19351669941060903,"Table 1: Comparison of all methods on three datasets under data volumes of 1, 0.7 and 0.4, where the top value
is highlighted by blod font. Lower PEN and CIN scores indicate better performance, while a higher LIS score
signifies improved performance."
EXPERIMENTS,0.1944990176817289,"Datasets
Chatbot Arena
MT-Bench
AlpacaEval
Methods
1
0.7
0.4
1
0.7
0.4
1
0.7
0.4
PEN (↓)"
EXPERIMENTS,0.19548133595284872,"Majority Voting [40]
1.27±0.05
1.30±0.03
1.36±0.06
1.37±0.03
1.30±0.06
1.27±0.04
1.26±0.02
1.28±0.03
1.29±0.03"
EXPERIMENTS,0.19646365422396855,"Rating Voting [5]
1.39±0.02
1.43±0.03
1.42±0.07
1.32±0.03
1.35±0.04
1.38±0.04
1.34±0.03
1.37±0.03
1.34±0.08"
EXPERIMENTS,0.1974459724950884,"GPTScore(flan-t5-xxl)[23]
1.68±0.01
1.68±0.02
1.65±0.02
1.72±0.02
1.70±0.02
1.68±0.03
1.55±0.02
1.57±0.03
1.60±0.01"
EXPERIMENTS,0.19842829076620824,"GPTScore(davinci-002)[23] 1.54±0.02
1.64±0.02
1.68±0.05
1.51±0.02
1.61±0.01
1.61±0.04
1.25±0.02
1.23±0.08
1.26±0.14"
EXPERIMENTS,0.1994106090373281,"PandaLM[46]
1.65±0.01
1.64±0.02
1.63±0.05
1.55±0.03
1.59±0.05
1.52±0.08
1.56±0.01
1.58±0.01
1.64±0.05"
EXPERIMENTS,0.20039292730844793,"PRD[28]
1.15±0.04
1.12±0.05
1.13±0.06
1.15±0.05
1.17±0.06
1.23±0.04
1.21±0.04
1.22±0.06
1.23±0.07"
EXPERIMENTS,0.2013752455795678,"PRE[17]
1.07±0.01
1.03±0.03
1.06±0.04
1.17±0.04
1.13±0.05
1.19±0.05
1.18±0.03
1.21±0.04
1.15±0.05"
EXPERIMENTS,0.20235756385068762,"PiCO (Ours)
0.94±0.02
0.96±0.04
0.95±0.08
1.01±0.07
1.02±0.11
1.06±0.24
1.17±0.02
1.17±0.08
1.13±0.05"
EXPERIMENTS,0.20333988212180745,CIN (↓)
EXPERIMENTS,0.2043222003929273,"Majority Voting [40]
22.00±0.00
23.25±1.09
25.00±2.55
23.00±0.00
20.50±0.87
21.00±1.00
20.00±0.00
21.25±1.30
22.25±1.30"
EXPERIMENTS,0.20530451866404714,"Rating Voting [5]
24.00±0.00
24.50±1.29
25.00±1.15
22.00±0.00
22.50±1.00
24.25±0.50
22.00±0.00
22.50±0.58
22.50±1.00"
EXPERIMENTS,0.206286836935167,"GPTScore(flan-t5-xxl)[23]
67.00±0.00
66.50±0.50
68.25±1.09
53.00±0.00
55.75±2.77
54.50±2.29
35.00±0.00
36.00±0.71
37.75±1.60"
EXPERIMENTS,0.20726915520628683,"GPTScore(davinci-002)[23] 42.00±0.00
45.50±1.12
51.00±5.61
33.00±0.00
35.00±0.71
36.25±1.64
21.00±0.00
20.25±2.86
21.50±4.39"
EXPERIMENTS,0.2082514734774067,"PandaLM[46]
37.00±0.00
36.25±1.79
36.00±3.74
32.00±0.00
33.00±3.32
31.50±6.34
31.00±0.00
32.25±1.30
35.50±2.60"
EXPERIMENTS,0.20923379174852652,"PRD[28]
17.00±0.00
16.25±0.43
17.50±1.50
17.00±0.00
17.75±1.09
19.50±1.50
19.00±0.00
19.25±1.48
19.50±0.87"
EXPERIMENTS,0.21021611001964635,"PRE[17]
15.00±0.00
14.25±0.83
14.75±1.09
17.00±0.00
17.00±1.00
18.25±1.30
19.00±0.00
19.25±1.09
17.75±1.30"
EXPERIMENTS,0.2111984282907662,"PiCO (Ours)
12.00±0.00 12.50±0.50 12.25±1.09 14.50±0.50 14.75±1.64 16.00±6.36 17.00±0.00 18.00±1.87 17.25±1.09"
EXPERIMENTS,0.21218074656188604,LIS (↑)
EXPERIMENTS,0.2131630648330059,"Majority Voting [40]
7.00±0.00
6.75±0.43
6.75±0.43
7.00±0.00
8.25±0.43
8.50±1.12
8.00±0.00
7.50±0.50
7.50±0.50"
EXPERIMENTS,0.21414538310412573,"Rating Voting [5]
7.00±0.00
7.50±0.58
7.75±0.50
7.00±0.00
7.25±0.50
7.25±0.50
8.00±0.00
8.00±0.00
8.00±0.00"
EXPERIMENTS,0.2151277013752456,"GPTScore(flan-t5-xxl)[23]
5.00±0.00
5.00±0.00
4.00±0.71
4.00±0.00
4.50±0.50
4.75±0.43
6.00±0.00
6.00±0.00
6.00±0.00"
EXPERIMENTS,0.21611001964636542,"GPTScore(davinci-002)[23] 8.00±0.00
6.25±0.43
6.00±0.71
6.00±0.00
6.50±0.50
6.25±0.43
8.00±0.00
8.25±0.83
8.25±1.48"
EXPERIMENTS,0.21709233791748528,"PandaLM[46]
5.00±0.00
5.50±0.50
6.00±0.00
7.00±0.00
7.00±0.71
7.25±0.43
6.00±0.00
5.75±0.43
5.50±0.50"
EXPERIMENTS,0.2180746561886051,"PRD[28]
8.00±0.00
8.75±0.43
9.25±0.83
8.00±0.00
8.25±0.43
7.75±0.83
8.50±0.00
8.25±0.83
8.25±0.43"
EXPERIMENTS,0.21905697445972494,"PRE[17]
9.00±0.00
10.25±0.43
10.00±0.87
8.00±0.00
8.50±0.50
8.25±0.83
8.00±0.00
8.00±0.00
8.25±0.43"
EXPERIMENTS,0.2200392927308448,"PiCO (Ours)
10.00±0.00 10.25±0.71 10.50±0.43 8.75±0.43
8.75±0.87
9.00±1.22
9.00±0.00
8.75±0.43
8.50±0.50"
EXPERIMENTS,0.22102161100196463,"integrates 805 evaluations from diverse tests (e.g., Self-Instruct[48], OASST, Anthropic’s helpful[7],
181"
EXPERIMENTS,0.2220039292730845,"Vicuna[16] and Koala[25] test sets) to align evaluations real-world interactions[21]. These datasets
182"
EXPERIMENTS,0.22298624754420432,"are collected by crowdsourcing platforms from human feedback, so they have a ground-truth ranking
183"
EXPERIMENTS,0.22396856581532418,"LLMs R∗aligned with human preferences.
184"
EXPERIMENTS,0.224950884086444,"LLMs Pool. In our experiments, we employ 15 LLMs with diverse architectures to construct the
185"
EXPERIMENTS,0.22593320235756384,"LLMs pool, including GPT-3.5-Turbo[35], WizardLM-13B[51], Guanaco-33B[1], Vicuna-7B[16],
186"
EXPERIMENTS,0.2269155206286837,"Vicuna-13B[16], Koala-13B[24], Mpt-7B[42], gpt4all-13B[6], ChatGLM-6B[53], Oasst-sft-4-pythia-
187"
EXPERIMENTS,0.22789783889980353,"12B[19], FastChat-T5-3B[55], StableLM-7B[3], Dolly-12B[18], LLaMA-13B[43], Alpaca-13B[41].
188"
EXPERIMENTS,0.2288801571709234,"All models use the same evaluation template, they can be found in Appendix B
189"
EXPERIMENTS,0.22986247544204322,"Baselines. To validate the effectiveness of the proposed PiCO approach, we compare the following
190"
EXPERIMENTS,0.23084479371316308,"methods in the experiments.
191"
EXPERIMENTS,0.2318271119842829,"• The wisdom of the crowds: The two methods that perform LLMs evaluation based on the
192"
EXPERIMENTS,0.23280943025540274,"wisdom of the crowds [40, 13, 50] are compared in this experiment. 1) Majority Voting
193"
EXPERIMENTS,0.2337917485265226,"[40]: Multiple review models vote for the better answer for the same response pair, and the
194"
EXPERIMENTS,0.23477406679764243,"model with the most votes gets 1 score; 2) Rating Voting [5]: Multiple review models also
195"
EXPERIMENTS,0.2357563850687623,"vote on the same response pair, and the number of votes obtained is the score.
196"
EXPERIMENTS,0.23673870333988212,"• State-of-the-art methods: The four recent SOTA methods of using either single or multiple
197"
EXPERIMENTS,0.23772102161100198,"models for self-evaluation are compared in this experiment. PandaLM[46]: It is a fine-tuned
198"
EXPERIMENTS,0.2387033398821218,"language model based on Llama-7b designed for the preference judgment tasks to evaluate
199"
EXPERIMENTS,0.23968565815324164,"and optimize LLMs. GPTScore[23]: It employs generative pre-trained models to assess the
200"
EXPERIMENTS,0.2406679764243615,"quality of generated text. It calculates the likelihood that the text was generated in response
201"
EXPERIMENTS,0.24165029469548133,"to specific instructions and context, indicative of high quality. In our implementation, GPT-3
202"
EXPERIMENTS,0.24263261296660119,"(davinci-002) and flan-t5-xxl serve as the base models. PRD[28]: It transforms the LLMs
203"
EXPERIMENTS,0.24361493123772102,"win rates into weights for competitive ranking, while evaluating each LLM based on its
204"
EXPERIMENTS,0.24459724950884087,"preference for all possible pairs of answers, enabling a tournament-style ranking system.
205"
EXPERIMENTS,0.2455795677799607,"PRE[17]: It employs a supervised process to evaluate LLMs using a qualification exam,
206"
EXPERIMENTS,0.24656188605108054,"aggregates their scores based on accuracy, and assigns weights accordingly. PiCO (Ours):
207"
EXPERIMENTS,0.2475442043222004,"the proposed approach in this paper.
208"
EXPERIMENTS,0.24852652259332023,"Metrics. For all experiments, we employ three metrics to evaluate the aforementioned experimental
209"
EXPERIMENTS,0.24950884086444008,"setups and our Peer Review method: PEN, CIN, and LIS. Moreover, we perform the experiments for
210"
EXPERIMENTS,0.2504911591355599,"4 runs and record the average results over 4 seeds (seed = 1, 2, 3, 4).
211"
EXPERIMENTS,0.25147347740667975,"(a) ChatBot Arena (PG)
(b) MT-Bench (PG)
(c) AlpacaEval (PG)"
EXPERIMENTS,0.25245579567779963,"(d) ChatBot Arena (weighted PG)
(e) MT-Bench (weighted PG)
(f) AlpacaEval (weighted PG)"
EXPERIMENTS,0.25343811394891946,"Figure 4: Heatmap distribution of preference gap (PG) metric among seven LLMs across three datasets. Higher
values (above 0) indicate greater evaluation bias[17]. The first row shows original PG values in three datasets,
while the second row displays PG values re-weighted using our learned confidence weights."
PERFORMANCE COMPARISON,0.2544204322200393,"3.1
Performance Comparison
212"
PERFORMANCE COMPARISON,0.2554027504911591,"We validate the effectiveness of the proposed PiCO method on three datasets by comparing the
213"
PERFORMANCE COMPARISON,0.25638506876227896,"following two types of methods, i.e., the wisdom of the crowds and recent SOTA LLMs evaluation
214"
PERFORMANCE COMPARISON,0.25736738703339884,"methods. The average results of PEN, CIN and LIS are demonstrated in Table 1. The ratios of
215"
PERFORMANCE COMPARISON,0.2583497053045187,"response sets D are 1, 0.7, and 0.4, respectively.
216"
PERFORMANCE COMPARISON,0.2593320235756385,"The results presented in Table 1 illustrate the proposed PiCO method consistently surpasses com-
217"
PERFORMANCE COMPARISON,0.26031434184675833,"peting approaches across the majority of evaluated metrics Notably, PiCO achieves performance
218"
PERFORMANCE COMPARISON,0.26129666011787817,"improvements of 0.1, 2.5, and 0.92 on the PEN, CIN, and LIS metrics, respectively, compared to the
219"
PERFORMANCE COMPARISON,0.26227897838899805,"Runner-up. These results underscore the superiority of aggregating evaluations from multiple models,
220"
PERFORMANCE COMPARISON,0.2632612966601179,"such as Majority Voting, Rating Voting, PRD, and PRE, as opposed to relying solely on single-model
221"
PERFORMANCE COMPARISON,0.2642436149312377,"methods like GPTScore and PandaLM. This collective model approach, leveraging ’the wisdom of
222"
PERFORMANCE COMPARISON,0.26522593320235754,"the crowds’, more accurately aligns with human rankings in our open-question evaluation framework.
223"
PERFORMANCE COMPARISON,0.26620825147347743,"In comparison with existing peer review evaluation methods(i.e., PRD and PRE), it is evident that
224"
PERFORMANCE COMPARISON,0.26719056974459726,"PiCO exhibits improvements across various evaluation metrics. Despite PRD’s adjustment of model
225"
PERFORMANCE COMPARISON,0.2681728880157171,"weights based on their win rates and PRE’s reliance on supervised human feedback data to assign
226"
PERFORMANCE COMPARISON,0.2691552062868369,"weights through a qualification exam, neither method achieves performance superior to the fully
227"
PERFORMANCE COMPARISON,0.27013752455795675,"unsupervised PiCO approach. These methods rely on predefined criteria and human feedback,
228"
PERFORMANCE COMPARISON,0.27111984282907664,"potentially leading to biases or suboptimal performance. In contrast, PiCO leverages unsupervised
229"
PERFORMANCE COMPARISON,0.27210216110019647,"learning techniques, allowing it to autonomously adapt and discover patterns in the data without
230"
PERFORMANCE COMPARISON,0.2730844793713163,"explicit human intervention.
231"
PERFORMANCE COMPARISON,0.27406679764243613,"It is important to highlight that PandaLM, a language model equipped with 7 billion parameters, was
232"
PERFORMANCE COMPARISON,0.275049115913556,"fine-tuned using labels generated by GPT-3.5-turbo as the ground truth, achieving stable performance
233"
PERFORMANCE COMPARISON,0.27603143418467585,"across various datasets. However, in our unsupervised, open-ended experimental setup, which focuses
234"
PERFORMANCE COMPARISON,0.2770137524557957,"on ranking-based metrics, GPTScore exhibits less robustness regardless of whether the base model is
235"
PERFORMANCE COMPARISON,0.2779960707269155,"GPT-3 (davinci-002) or flan-t5-xx.
236"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.27897838899803534,"3.2
Exploring the Role of Confidence Weight
237"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.27996070726915523,"In this subsection, we will show that the confidence weight w learned by our consistency optimization
238"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.28094302554027506,"can reduce the system evaluation bias. Specifically, we first study whether the “review” model would
239"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.2819253438113949,"Figure 5: Performance comparison of the PiCO (Ours) and PRE[17] methods on the Chatbot Arena, MT-Bench,
and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is CIN, where lower
values indicate better performance."
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.2829076620825147,"prefer a particular model’s response. Following [17], we employ the preference gap (PG) to evaluate
240"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.28388998035363455,"the bias as follows,
241
PG(i, j) = Pi(i > j) −Pj(i > j),
(8)"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.28487229862475444,"where Pi(i > j) represents the winning rate of model i as the “reviewer” believes that i defeated
242"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.28585461689587427,"j. The heatmap distribution of the PG value PG(i, j) among seven LLMs across three datasets is
243"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.2868369351669941,"demonstrated in the first row of Figure 4. It can be observed that the evaluation system exhibits severe
244"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.28781925343811393,"bias. Especially on ChatGLM-6B and Mpt-7B models, they often believe that their results are better
245"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.2888015717092338,"than other ones, as their PG values are greater than 0 across three datasets.
246"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.28978388998035365,"After the consistency optimization, we assign the learned confidence weight w to the corresponding
247"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.2907662082514735,"model and ultimately obtain the re-weighting PG value ˆ
PG(i, j) as follows,
248"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.2917485265225933,"ˆ
PG(i, j) = wi × Pi(i > j) −wj × Pj(i > j).
(9)"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.29273084479371314,"The results of the re-weighting PG value ˆ
PG(i, j) are displayed on the second row of Figure 4. It can
249"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.293713163064833,"be observed that the learned confidence weight w can significantly mitigate the preference gaps of the
250"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.29469548133595286,"whole evaluation system. In our consistency optimization, LLMs such as ChatGLM-6B and Mpt-7B
251"
EXPLORING THE ROLE OF CONFIDENCE WEIGHT,0.2956777996070727,"have lower weights, and reducing their confidence can effectively alleviate the system evaluation bias.
252"
STUDY OF ELIMINATION MECHANISM,0.2966601178781925,"3.3
Study of Elimination Mechanism
253"
STUDY OF ELIMINATION MECHANISM,0.29764243614931235,"The PiCO and PRE[17] methods both employ elimination mechanisms to remove those weakest
254"
STUDY OF ELIMINATION MECHANISM,0.29862475442043224,"LLMs from the “reviewer queue” during the evaluation process. As shown in Figure 5, the x-axis
255"
STUDY OF ELIMINATION MECHANISM,0.29960707269155207,"quantifies the number of reviewers eliminated, and the y-axis measures the CIN, where lower scores
256"
STUDY OF ELIMINATION MECHANISM,0.3005893909626719,"denote higher performance. Due to space limitations, more results on PEN and LIS metrics can be
257"
STUDY OF ELIMINATION MECHANISM,0.30157170923379173,"found in Appendix E. It can be observed that both PiCO and PRE exhibit better performance with
258"
STUDY OF ELIMINATION MECHANISM,0.3025540275049116,"an increasing number of eliminated “reviewers”. The proposed PiCO approach can achieve better
259"
STUDY OF ELIMINATION MECHANISM,0.30353634577603145,"performance than PRE in most cases. It is worth noting that the PRE method employs the accuracy
260"
STUDY OF ELIMINATION MECHANISM,0.3045186640471513,"of “qualification exams” to eliminate weak LLMs, and this process requires human annotation [17].
261"
STUDY OF ELIMINATION MECHANISM,0.3055009823182711,"On the contrary, the elimination process of our PiCO method is unsupervised and can still achieve
262"
STUDY OF ELIMINATION MECHANISM,0.30648330058939094,"better evaluation results than PRE.
263"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3074656188605108,"3.4
Validation of Consistency Assumption
264"
VALIDATION OF CONSISTENCY ASSUMPTION,0.30844793713163066,"In this subsection, we conduct the ablation study to validate the effectiveness of the consistency
265"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3094302554027505,"assumption. Specifically, we first manually construct three methods: Forward Weight Voting,
266"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3104125736738703,"Uniform Weight Voting, and Reverse Weight Voting. That is, the ability weights of the model are
267"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3113948919449902,"respectively weighted forward (w = [1, 0.9, ..., 0]), uniformly (w = [1, 1, ..., 1]), and backward
268"
VALIDATION OF CONSISTENCY ASSUMPTION,0.31237721021611004,"(w = [0, 0.1, ..., 1]) according to the ground-truth human ranking. Then, we randomly initialize the
269"
VALIDATION OF CONSISTENCY ASSUMPTION,0.31335952848722987,"ability weights and employ our consistency optimization to adjust the weight. In addition, we also
270"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3143418467583497,"collect the average performance of “reviewer queue”, i.e., employing a single LLM as the “reviewer”
271"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3153241650294695,"to evaluate all response pairs and then calculate the average results of all LLMs.
272"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3163064833005894,"As shown in Table 2, it can be observed that the Forward Weight Voting achieves better results than
273"
VALIDATION OF CONSISTENCY ASSUMPTION,0.31728880157170924,"the Uniform and Backward ones in all cases, while the Backward one achieves worse results. It
274"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3182711198428291,"validates that assigning larger weights to those models with stronger capabilities can obtain better
275"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3192534381139489,"Table 2: Ablation study comparing Backward, Uniform, Forward weight voting, and Consistency Optimization
methods with the Average Performance of Reviewer Queue across three datasets."
VALIDATION OF CONSISTENCY ASSUMPTION,0.32023575638506874,"Methods
MT-Bench
Chatbot Arena
AlpacaEval
PEN (↓)
CIN(↓)
PEN (↓)
CIN(↓)
PEN (↓)
CIN(↓)
Average Performance of Reviewer Queue
1.49±0.28 34.87±14.68 1.49±0.26 38.80±19.28 1.50±0.23 33.13±13.97"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3212180746561886,"Backward Weight Voting
1.43±0.04
25.00±0.00
1.43±0.05
26.00±0.00
1.36±0.03
24.00±0.00"
VALIDATION OF CONSISTENCY ASSUMPTION,0.32220039292730845,"Uniform Weight Voting
1.34±0.23
22.00±0.00
1.39±0.02
24.00±0.00
1.34±0.03
22.00±0.00"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3231827111984283,"Forward Weight Voting
1.32±0.03
21.00±0.00
1.33±0.03
23.00±0.00
1.30±0.05
21.00±0.00"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3241650294695481,Random Weight + Consistency Optimization 1.17±0.06 17.50±0.50 1.20±0.08 18.00±1.22 1.21±0.04 19.00±0.00
VALIDATION OF CONSISTENCY ASSUMPTION,0.325147347740668,"results. Most importantly, employing our consistency optimization algorithm to assign weights to
276"
VALIDATION OF CONSISTENCY ASSUMPTION,0.32612966601178783,"different review models can further improve the performance of the evaluation system, i.e., lower PEN
277"
VALIDATION OF CONSISTENCY ASSUMPTION,0.32711198428290766,"and CIN, as well as higher LIS in all cases. Moreover, it is worth noting that the average performance
278"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3280943025540275,"of the “reviewer queue” is very poor, even worse than the Backward Weight Voting. This means
279"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3290766208251473,"that the answer-ranking data D contains a lot of evaluation noise, while the proposed approach can
280"
VALIDATION OF CONSISTENCY ASSUMPTION,0.3300589390962672,"still optimize weights and obtain better ranking results. In summary, the above experimental results
281"
VALIDATION OF CONSISTENCY ASSUMPTION,0.33104125736738704,"validate the effectiveness of the consistency assumption from various perspectives.
282"
RELATED WORK,0.3320235756385069,"4
Related Work
283"
RELATED WORK,0.3330058939096267,"Evaluation Benchmarks for Diversity. LLMs are designed to handle a variety of tasks, necessitat-
284"
RELATED WORK,0.33398821218074654,"ing comprehensive benchmarks[15]. Notable benchmarks include GLUE[45] and SuperGLUE[44],
285"
RELATED WORK,0.3349705304518664,"which simulate real-world scenarios across tasks such as text classification, translation, reading
286"
RELATED WORK,0.33595284872298625,"comprehension, and dialogue generation. HELM[30] provides a holistic evaluation of LLMs, as-
287"
RELATED WORK,0.3369351669941061,"sessing language understanding, generation, coherence, and reasoning. BIG-bench[39] pushes LLM
288"
RELATED WORK,0.3379174852652259,"capabilities with 204 diverse tasks. MMLU[26] measures multitask accuracy across domains like
289"
RELATED WORK,0.3388998035363458,"mathematics and law. However, these evaluations can be compromised by benchmark leakage, where
290"
RELATED WORK,0.33988212180746563,"evaluation data inadvertently used for training leads to inflated performance metrics[4, 56].
291"
RELATED WORK,0.34086444007858546,"Human Evaluation. Human evaluation provides reliable feedback that closely aligns with real-
292"
RELATED WORK,0.3418467583497053,"world applications[15]. Liang et al.[30] evaluated summary and misinformation scenarios across
293"
RELATED WORK,0.3428290766208251,"multiple models. Ziems et al.[57] involved experts to assess model outputs in various domain-specific
294"
RELATED WORK,0.343811394891945,"tasks. Bang et al.[9] examined ChatGPT’s performance in summarization, translation, and reasoning
295"
RELATED WORK,0.34479371316306484,"using human-annotated datasets. The LMSYS initiative introduced platforms like Chatbot Arena[55],
296"
RELATED WORK,0.34577603143418467,"relying on human ratings as the primary evaluation metric. Despite its effectiveness, human evaluation
297"
RELATED WORK,0.3467583497053045,"is costly and subject to bias and cultural differences[37].
298"
RELATED WORK,0.3477406679764244,"Large Language Models for Evaluation. The development of open-source LLMs has led to the
299"
RELATED WORK,0.3487229862475442,"use of LLMs as evaluators. GPTScore[23] uses models like GPT-3 to assign probabilities to high-
300"
RELATED WORK,0.34970530451866405,"quality content through multidimensional evaluation. Bubeck et al.[12] tested GPT-4, finding it
301"
RELATED WORK,0.3506876227897839,"rivaling human capabilities. Lin and Chen introduced LLM-EVAL[31] for evaluating dialogue quality
302"
RELATED WORK,0.3516699410609037,"with single prompts. PandaLM[46] employs LLMs as ""judges"" for evaluating instruction tuning.
303"
RELATED WORK,0.3526522593320236,"However, reliance on a single model can introduce biases such as positional[20], verbosity[47], and
304"
RELATED WORK,0.35363457760314343,"self-favoring biases[33, 55]. ChatEval[14] proposes a multi-agent framework to simulate human
305"
RELATED WORK,0.35461689587426326,"evaluation processes. Similarly, PRE[17] and PRD[28] use LLMs as evaluators, combining multiple
306"
RELATED WORK,0.3555992141453831,"evaluation outcomes for automated assessment. However, the PRE method, which relies on human
307"
RELATED WORK,0.3565815324165029,"feedback for supervised evaluation throughout the process, still incurs relatively high costs.
308"
CONCLUSION,0.3575638506876228,"5
Conclusion
309"
CONCLUSION,0.35854616895874264,"In this paper, we propose the novel Peer Review method based on the Consistency Optimization
310"
CONCLUSION,0.35952848722986247,"(PiCO) to automatically evaluate Large Language Models (LLMs) without relying on human feedback.
311"
CONCLUSION,0.3605108055009823,"PiCO utilizes peer-review mechanisms to autonomously assess LLMs in a shared environment, where
312"
CONCLUSION,0.3614931237721022,"both open-source and closed-source models can respond to unlabeled questions and evaluate each
313"
CONCLUSION,0.362475442043222,"other. In this setup, each LLM’s response score is determined collectively by other anonymous
314"
CONCLUSION,0.36345776031434185,"models, aiming to maximize consistency across capabilities and scores. We propose three metrics,
315"
CONCLUSION,0.3644400785854617,"i.e., PEN, CIN, and LIS, to quantify the disparity from human preferences. The extensive experiment
316"
CONCLUSION,0.3654223968565815,"results across multiple datasets and metrics demonstrate that PiCO effectively generates an LLM
317"
CONCLUSION,0.3664047151277014,"leaderboard that aligns closely with human preferences. In the future, we plan to extend the peer-
318"
CONCLUSION,0.36738703339882123,"review mechanism to evaluate the capabilities of multi-modality large models.
319"
REFERENCES,0.36836935166994106,"References
320"
REFERENCES,0.3693516699410609,"[1] Guanaco - generative universal assistant for natural-language adaptive context-aware omnilin-
321"
REFERENCES,0.3703339882121807,"gual outputs. https://guanaco-model.github.io/, 2023. Accessed: 15 April 2024.
322"
REFERENCES,0.3713163064833006,"[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
323"
REFERENCES,0.37229862475442044,"Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
324"
REFERENCES,0.37328094302554027,"technical report. arXiv preprint arXiv:2303.08774, 2023.
325"
REFERENCES,0.3742632612966601,"[3] Stability AI. Stablelm-tuned-alpha-7b: A fine-tuned language model for diverse applications.
326"
REFERENCES,0.37524557956778,"https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b, 2023. Accessed:
327"
REFERENCES,0.3762278978388998,"15 April 2024.
328"
REFERENCES,0.37721021611001965,"[4] Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. Can we trust the evaluation
329"
REFERENCES,0.3781925343811395,"on chatgpt?, 2023.
330"
REFERENCES,0.3791748526522593,"[5] Mohammad Allahbakhsh and Aleksandar Ignjatovic. Rating through voting: An iterative
331"
REFERENCES,0.3801571709233792,"method for robust rating. arXiv preprint arXiv:1211.0390, 2012.
332"
REFERENCES,0.381139489194499,"[6] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.
333"
REFERENCES,0.38212180746561886,"Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo.
334"
REFERENCES,0.3831041257367387,"https://github.com/nomic-ai/gpt4all, 2023.
335"
REFERENCES,0.3840864440078585,"[7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
336"
REFERENCES,0.3850687622789784,"Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
337"
REFERENCES,0.38605108055009824,"assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,
338"
REFERENCES,0.38703339882121807,"2022.
339"
REFERENCES,0.3880157170923379,"[8] Christoph Bandt and Bernd Pompe. Permutation entropy: a natural complexity measure for
340"
REFERENCES,0.3889980353634578,"time series. Physical review letters, 88(17):174102, 2002.
341"
REFERENCES,0.3899803536345776,"[9] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Love-
342"
REFERENCES,0.39096267190569745,"nia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation
343"
REFERENCES,0.3919449901768173,"of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023,
344"
REFERENCES,0.3929273084479371,"2023.
345"
REFERENCES,0.393909626719057,"[10] Robert S Boyer and J Strother Moore. Mjrty—a fast majority vote algorithm. In Automated
346"
REFERENCES,0.3948919449901768,"reasoning: essays in honor of Woody Bledsoe, pages 105–117. Springer, 1991.
347"
REFERENCES,0.39587426326129665,"[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
348"
REFERENCES,0.3968565815324165,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
349"
REFERENCES,0.39783889980353637,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
350"
REFERENCES,0.3988212180746562,"[12] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece
351"
REFERENCES,0.39980353634577603,"Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
352"
REFERENCES,0.40078585461689586,"intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
353"
REFERENCES,0.4017681728880157,"[13] David V Budescu and Eva Chen. Identifying expertise to extract the wisdom of crowds.
354"
REFERENCES,0.4027504911591356,"Management science, 61(2):267–280, 2015.
355"
REFERENCES,0.4037328094302554,"[14] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu,
356"
REFERENCES,0.40471512770137524,"and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate.
357"
REFERENCES,0.4056974459724951,"arXiv preprint arXiv:2308.07201, 2023.
358"
REFERENCES,0.4066797642436149,"[15] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen,
359"
REFERENCES,0.4076620825147348,"Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language
360"
REFERENCES,0.4086444007858546,"models. ACM Transactions on Intelligent Systems and Technology, 2023.
361"
REFERENCES,0.40962671905697445,"[16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
362"
REFERENCES,0.4106090373280943,"Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot
363"
REFERENCES,0.41159135559921417,"impressing gpt-4 with 90% chatgpt quality. https://vicuna.lmsys.org, 2023. Accessed:
364"
REFERENCES,0.412573673870334,"15 April 2024.
365"
REFERENCES,0.41355599214145383,"[17] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu. Pre: A peer review based large
366"
REFERENCES,0.41453831041257366,"language model evaluator. arXiv preprint arXiv:2401.15641, 2024.
367"
REFERENCES,0.4155206286836935,"[18] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick
368"
REFERENCES,0.4165029469548134,"Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first truly open
369"
REFERENCES,0.4174852652259332,"instruction-tuned llm, 2023.
370"
REFERENCES,0.41846758349705304,"[19] Open-Assistant
Contributors.
Oasst-sft-4-pythia-12b:
A
supervised
fine-tuning
371"
REFERENCES,0.4194499017681729,"model
for
language
understanding.
https://huggingface.co/OpenAssistant/
372"
REFERENCES,0.4204322200392927,"oasst-sft-4-pythia-12b-epoch-3.5, 2023. Accessed: 15 April 2024.
373"
REFERENCES,0.4214145383104126,"[20] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient
374"
REFERENCES,0.4223968565815324,"finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.
375"
REFERENCES,0.42337917485265225,"[21] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
376"
REFERENCES,0.4243614931237721,"Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
377"
REFERENCES,0.42534381139489197,"methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.
378"
REFERENCES,0.4263261296660118,"[22] Allan M. Feldman. Majority voting. SpringerLink, 2006.
379"
REFERENCES,0.42730844793713163,"[23] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire.
380"
REFERENCES,0.42829076620825146,"arXiv preprint arXiv:2302.04166, 2023.
381"
REFERENCES,0.4292730844793713,"[24] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine,
382"
REFERENCES,0.4302554027504912,"and Dawn Song. Koala-13b: Dialogue model for effective human-ai interaction. https:
383"
REFERENCES,0.431237721021611,"//bair.berkeley.edu/blog/2023/04/03/koala/, 2023. Accessed: 15 April 2024.
384"
REFERENCES,0.43222003929273084,"[25] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and
385"
REFERENCES,0.43320235756385067,"Dawn Song. Koala: A dialogue model for academic research. Blog post, April, 1, 2023.
386"
REFERENCES,0.43418467583497056,"[26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
387"
REFERENCES,0.4351669941060904,"Jacob Steinhardt.
Measuring massive multitask language understanding.
arXiv preprint
388"
REFERENCES,0.4361493123772102,"arXiv:2009.03300, 2020.
389"
REFERENCES,0.43713163064833005,"[27] Charles Eric Leiserson, Ronald L Rivest, Thomas H Cormen, and Clifford Stein. Introduction
390"
REFERENCES,0.4381139489194499,"to algorithms, volume 3. MIT press Cambridge, MA, USA, 1994.
391"
REFERENCES,0.43909626719056977,"[28] Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language
392"
REFERENCES,0.4400785854616896,"model based evaluations. arXiv preprint arXiv:2307.02762, 2023.
393"
REFERENCES,0.44106090373280943,"[29] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
394"
REFERENCES,0.44204322200392926,"Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following
395"
REFERENCES,0.4430255402750491,"models, 2023.
396"
REFERENCES,0.444007858546169,"[30] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,
397"
REFERENCES,0.4449901768172888,"Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of
398"
REFERENCES,0.44597249508840864,"language models. arXiv preprint arXiv:2211.09110, 2022.
399"
REFERENCES,0.44695481335952847,"[31] Yen-Ting Lin and Yun-Nung Chen. Llm-eval: Unified multi-dimensional automatic evaluation
400"
REFERENCES,0.44793713163064836,"for open-domain conversations with large language models. arXiv preprint arXiv:2305.13711,
401"
REFERENCES,0.4489194499017682,"2023.
402"
REFERENCES,0.449901768172888,"[32] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
403"
REFERENCES,0.45088408644400785,"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
404"
REFERENCES,0.4518664047151277,"processing. ACM Computing Surveys, 55(9):1–35, 2023.
405"
REFERENCES,0.45284872298624756,"[33] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval:
406"
REFERENCES,0.4538310412573674,"Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634,
407"
REFERENCES,0.4548133595284872,"2023.
408"
REFERENCES,0.45579567779960706,"[34] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-
409"
REFERENCES,0.4567779960707269,"pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
410"
REFERENCES,0.4577603143418468,"question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
411"
REFERENCES,0.4587426326129666,"[35] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022. Accessed:
412"
REFERENCES,0.45972495088408644,"[insert date here].
413"
REFERENCES,0.46070726915520627,"[36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
414"
REFERENCES,0.46168958742632615,"Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
415"
REFERENCES,0.462671905697446,"follow instructions with human feedback. Advances in Neural Information Processing Systems,
416"
REFERENCES,0.4636542239685658,"35:27730–27744, 2022.
417"
REFERENCES,0.46463654223968565,"[37] Kaiping Peng, Richard E Nisbett, and Nancy YC Wong. Validity problems comparing values
418"
REFERENCES,0.4656188605108055,"across cultures and possible solutions. Psychological methods, 2(4):329, 1997.
419"
REFERENCES,0.46660117878192536,"[38] Philip Sedgwick. Pearson’s correlation coefficient. Bmj, 345, 2012.
420"
REFERENCES,0.4675834970530452,"[39] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,
421"
REFERENCES,0.468565815324165,"Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.
422"
REFERENCES,0.46954813359528486,"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
423"
REFERENCES,0.47053045186640474,"arXiv preprint arXiv:2206.04615, 2022.
424"
REFERENCES,0.4715127701375246,"[40] James Surowiecki. The wisdom of crowds. Anchor, 2005.
425"
REFERENCES,0.4724950884086444,"[41] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
426"
REFERENCES,0.47347740667976423,"Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
427"
REFERENCES,0.47445972495088407,"https://github.com/tatsu-lab/stanford_alpaca, 2023.
428"
REFERENCES,0.47544204322200395,"[42] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially
429"
REFERENCES,0.4764243614931238,"usable llms, 2023. Accessed: 2023-05-05.
430"
REFERENCES,0.4774066797642436,"[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
431"
REFERENCES,0.47838899803536344,"thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
432"
REFERENCES,0.4793713163064833,"and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
433"
REFERENCES,0.48035363457760316,"[44] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
434"
REFERENCES,0.481335952848723,"Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose
435"
REFERENCES,0.4823182711198428,"language understanding systems. Advances in neural information processing systems, 32, 2019.
436"
REFERENCES,0.48330058939096265,"[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
437"
REFERENCES,0.48428290766208254,"Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
438"
REFERENCES,0.48526522593320237,"preprint arXiv:1804.07461, 2018.
439"
REFERENCES,0.4862475442043222,"[46] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya
440"
REFERENCES,0.48722986247544203,"Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark
441"
REFERENCES,0.48821218074656186,"for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023.
442"
REFERENCES,0.48919449901768175,"[47] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu,
443"
REFERENCES,0.4901768172888016,"David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?
444"
REFERENCES,0.4911591355599214,"exploring the state of instruction tuning on open resources. Advances in Neural Information
445"
REFERENCES,0.49214145383104124,"Processing Systems, 36, 2024.
446"
REFERENCES,0.4931237721021611,"[48] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,
447"
REFERENCES,0.49410609037328096,"and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-
448"
REFERENCES,0.4950884086444008,"tions. arXiv preprint arXiv:2212.10560, 2022.
449"
REFERENCES,0.4960707269155206,"[49] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng
450"
REFERENCES,0.49705304518664045,"Cheng, Weiwei Lü, Rui Hu, et al. Skywork: A more open bilingual foundation model. arXiv
451"
REFERENCES,0.49803536345776034,"preprint arXiv:2310.19341, 2023.
452"
REFERENCES,0.49901768172888017,"[50] Susan C Weller. Cultural consensus theory: Applications and frequently asked questions. Field
453"
REFERENCES,0.5,"methods, 19(4):339–368, 2007.
454"
REFERENCES,0.5009823182711198,"[51] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and
455"
REFERENCES,0.5019646365422397,"Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions,
456"
REFERENCES,0.5029469548133595,"2023.
457"
REFERENCES,0.5039292730844793,"[52] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm lies: Hallucinations
458"
REFERENCES,0.5049115913555993,"are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469, 2023.
459"
REFERENCES,0.5058939096267191,"[53] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,
460"
REFERENCES,0.5068762278978389,"Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv
461"
REFERENCES,0.5078585461689588,"preprint arXiv:2210.02414, 2022.
462"
REFERENCES,0.5088408644400786,"[54] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
463"
REFERENCES,0.5098231827111984,"Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv
464"
REFERENCES,0.5108055009823183,"preprint arXiv:2303.18223, 2023.
465"
REFERENCES,0.5117878192534381,"[55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
466"
REFERENCES,0.5127701375245579,"Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.
467"
REFERENCES,0.5137524557956779,"Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
468"
REFERENCES,0.5147347740667977,"[56] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin,
469"
REFERENCES,0.5157170923379175,"Ji-Rong Wen, and Jiawei Han. Don’t make your llm an evaluation benchmark cheater. arXiv
470"
REFERENCES,0.5166994106090373,"preprint arXiv:2311.01964, 2023.
471"
REFERENCES,0.5176817288801572,"[57] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large
472"
REFERENCES,0.518664047151277,"language models transform computational social science? arXiv preprint arXiv:2305.03514,
473"
REFERENCES,0.5196463654223968,"2023.
474"
REFERENCES,0.5206286836935167,"A
Dataset Format
475"
REFERENCES,0.5216110019646365,"Focusing on the MT-Bench dataset, we demonstrate the ensuing data format utilizing dataset Q.
476"
REFERENCES,0.5225933202357563,"As Figure 6 illustrates, the Question dataset Q contains ""Question id,"" ""Category,"" ""Question,""
477"
REFERENCES,0.5235756385068763,"and ""Reference."" In categories with definitive answers like ""reasoning"" or ""math,"" the ""Reference""
478"
REFERENCES,0.5245579567779961,"field is populated with standard answers; otherwise, it remains blank. Each model M in our pool
479"
REFERENCES,0.5255402750491159,"processes the Question dataset Q to generate the LLMs answer data A, consisting of ""Question
480"
REFERENCES,0.5265225933202358,"id,"" ""Answer id,"" ""Model id,"" and ""Answer."" Finally, we combine pairs in A and appoint judges to
481"
REFERENCES,0.5275049115913556,"evaluate, creating the Answer-Ranking data D, featuring ""Question id,"" ""Model 1,"" ""Model 2,"" ""G1
482"
REFERENCES,0.5284872298624754,"winner,"" ""G2 winner,"" and ""Judge."" Here, ""G1 winner"" and ""G2 winner"" indicate the outcomes of
483"
REFERENCES,0.5294695481335953,"inputting reversed order responses of Model 1 and Model 2 into the judge model, a method employed
484"
REFERENCES,0.5304518664047151,"to mitigate biases stemming from models’ preferences for input order.
485"
REFERENCES,0.5314341846758349,"Figure 6: Format of the Question dataset Q, LLMs responses data A, and the Answer-Ranking data D for Peer
Review"
REFERENCES,0.5324165029469549,"B
Detailed Prompt for Reviewers
486"
REFERENCES,0.5333988212180747,"The evaluation prompts, as detailed in Section 2.2.1, are employed during the Peer Review Stage.
487"
REFERENCES,0.5343811394891945,"These prompts are provided to the Reviewer Language Model Systems (LLMs), enabling them to
488"
REFERENCES,0.5353634577603144,"generate evaluative preferences. In our experimental framework, we devised four distinct prompt
489"
REFERENCES,0.5363457760314342,"settings. For each setting, a tailored prompt template was meticulously crafted as illustrated below:
490"
REFERENCES,0.537328094302554,"Template for Single-Turn Interaction: This template is designed for single-turn interactions
491"
REFERENCES,0.5383104125736738,"between users and LLMs, where there is no predetermined correct answer. It facilitates open-ended
492"
REFERENCES,0.5392927308447937,"dialogue, allowing for a wide range of user inquiries without the expectation of specific responses.
493"
REFERENCES,0.5402750491159135,"Referenced Template for Single-Turn Interaction: Tailored for single-turn dialogues between
494"
REFERENCES,0.5412573673870335,"users and LLMs, this template incorporates predefined correct answers. It is particularly suited for
495"
REFERENCES,0.5422396856581533,"interactions involving factual inquiries, such as mathematics or logic problems, where accuracy and
496"
REFERENCES,0.5432220039292731,"reference to correct information are paramount.
497"
REFERENCES,0.5442043222003929,"Template for Multi-Turn Interaction: This template caters to multi-turn conversations between
498"
REFERENCES,0.5451866404715128,"users and LLMs, without predefined answers. It supports extended interactions, enabling users to
499"
REFERENCES,0.5461689587426326,"explore topics in depth through a series of interconnected questions and responses.
500"
REFERENCES,0.5471512770137524,"Referenced Template for Multi-Turn Interaction: Designed for multi-turn dialogues with prede-
501"
REFERENCES,0.5481335952848723,"fined correct answers, this template is ideal for complex inquiries requiring sequential reasoning or
502"
REFERENCES,0.5491159135559921,"problem-solving, such as mathematical computations or logical deductions.
503"
REFERENCES,0.550098231827112,"Each template is carefully constructed to match its intended use-case, providing a structured frame-
504"
REFERENCES,0.5510805500982319,"work that guides the interaction between users and LLMs towards achieving desired outcomes,
505"
REFERENCES,0.5520628683693517,"whether for open-ended exploration or precise problem-solving.
506"
REFERENCES,0.5530451866404715,Template for Single-Turn Answer
REFERENCES,0.5540275049115914,"System prompt: Please act as a judge and evaluate the quality of the responses provided by
two AI assistants to the user question displayed below. You do not need to explain, just give
your judgment. Output your final verdict by strictly following this format: ""[[A]]"" if assistant
A is better, ""[[B]]"" if assistant B is better, and ""[[C]]"" for a tie.
User Question: {question}
Assistant A’s Answer: {answer a}
Assistant B’s Answer: {answer b} 507"
REFERENCES,0.5550098231827112,Referenced Template for Single-Turn Answer
REFERENCES,0.555992141453831,"System prompt: Please act as a judge and evaluate the quality of the responses provided
by two AI assistants to the user question displayed below, with reference to the provided
reference answers. You do not need to explain, just give your judgment. Output your final
verdict by strictly following this format: ""[[A]]""if assistant A is better, ""[[B]]"" if assistant B is
better, and ""[[C]]"" for a tie.
User Question: {question}
Reference Answer: {reference answer}
Assistant A’s Answer: {answer a}
Assistant B’s Answer: {answer b} 508"
REFERENCES,0.5569744597249509,Template for Multi-Turn Answer
REFERENCES,0.5579567779960707,"System prompt: Please act as a judge and evaluate the quality of the responses provided by
two AI assistants to the user question displayed below. You do not need to explain, just give
your judgment. Output your final verdict by strictly following this format: ""[[A]]"" if assistant
A is better, ""[[B]]"" if assistant B is better, and ""[[C]]"" for a tie
Assistant A’s Conversation with User:
User: {question 1}
Assistant A: {answer a1}
User: {question 2}
Assistant A: {answer a2}
Assistant B’s Conversation with User:
User: {question 1}
Assistant B: {answer b1}
User: {question 2}
Assistant B: {answer b2} 509"
REFERENCES,0.5589390962671905,Referenced Template for Multi-Turn Answer
REFERENCES,0.5599214145383105,"System prompt: Please act as a judge and evaluate the quality of the responses provided
by two AI assistants to the user question displayed below, in comparison to the reference
answers. You do not need to explain, just give your judgment. Output your final verdict by
strictly following this format: ""[[A]]""if assistant A is better, ""[[B]]"" if assistant B is better,
and ""[[C]]"" for a tie.
Reference Answer
User: {question 1}
Reference answer: {ref answer 1}
User: {question 2}
Reference answer: {ref answer 2}
Assistant A’s Conversation with User:
User: {question 1}
Assistant A: {answer a1}
User: {question 2}
Assistant A: {answer a2}
Assistant B’s Conversation with User:
User: {question 1}
Assistant B: {answer b1}
User: {question 2}
Assistant B: {answer b2} 510"
REFERENCES,0.5609037328094303,"C
Scoring Methodology
511"
REFERENCES,0.5618860510805501,"In Section 2.2.2, Equation 7 delineates the methodology for optimizing scores. Within this frame-
512"
REFERENCES,0.56286836935167,"work, the function 1{Aj
i > Ak
i } is more precisely defined as f(Aj
i, Ak
i ). Additionally, the function
513"
REFERENCES,0.5638506876227898,"f(Aj
i, Ak
i ) is not fixed and can be implemented using various computational strategies. We introduce
514"
REFERENCES,0.5648330058939096,"two distinct methodologies in this context: the Elo mechanism and the Rank mechanism.
515"
REFERENCES,0.5658153241650294,"Within the framework of the Elo mechanism, as specified by Equation 10, the BASE value is set to
516"
REFERENCES,0.5667976424361493,"10, and the SCALE factor is determined to be 400. This approach facilitates a dynamic adjustment
517"
REFERENCES,0.5677799607072691,"of scores based on the outcomes of pairwise comparisons, allowing for a nuanced reflection of
518"
REFERENCES,0.568762278978389,"performance variations among models.
519"
REFERENCES,0.5697445972495089,"Conversely, in the context of the Rank mechanism, as outlined by Equation 11, rank(j) signifies the
520"
REFERENCES,0.5707269155206287,"current ranking of model j, with the constant K assigned a value of 200. This mechanism employs
521"
REFERENCES,0.5717092337917485,"a model’s ranking within a predefined hierarchy as a pivotal factor in score calculation, thereby
522"
REFERENCES,0.5726915520628684,"providing a straightforward, yet effective, method for evaluating comparative model performance.
523"
REFERENCES,0.5736738703339882,"f(Aj
i, Ak
i ) = 

 
"
REFERENCES,0.574656188605108,"1 −
1
1+BASE((G(k)−G(j))/SCALE)
if Aj
i > Ak
i
0.5 −
1
1+BASE((G(k)−G(j))/SCALE)
if Aj
i = Ak
i
0 −
1
1+BASE((G(k)−G(j))/SCALE)
if Aj
i < Ak
i (10)"
REFERENCES,0.5756385068762279,"f(Aj
i, Ak
i ) = 

 
"
REFERENCES,0.5766208251473477,"1 + (rank(j) −rank(k))/K
if Aj
i > Ak
i
0.5
if Aj
i = Ak
i
0
if Aj
i < Ak
i (11)"
REFERENCES,0.5776031434184676,"D
Overall Algorithm of Peer Review
524"
REFERENCES,0.5785854616895875,"The overall algorithm, as delineated in Algorithm 1, encapsulates the comprehensive process outlined
525"
REFERENCES,0.5795677799607073,"in Section 2.2. This sequence commences with ""Data Collection and LLMs Pool Construction,""
526"
REFERENCES,0.5805500982318271,"progresses through ""Answer-Ranking Data Construction Based on Peer Review,"" advances to ""Con-
527"
REFERENCES,0.581532416502947,"sistency Optimization,"" and culminates with the ""Unsupervised Elimination Mechanism.""
528"
REFERENCES,0.5825147347740668,Algorithm 1 Overall Framework Algorithm of Peer Review
REFERENCES,0.5834970530451866,"Require: Unlabeled dataset Q, Pool of LLMs M, Active LLM pool M∗= M
Ensure: Consistency-optimized ranking of LLMs R∗"
REFERENCES,0.5844793713163065,"1: Initialize response matrix A ←∅
2: for each question qi ∈Q do
3:
Initialize response vector for question qi, Ai ←∅
4:
for each model mj ∈M do
5:
Ai
j ←response of model mj to question qi
6:
Ai ←Ai ∪{Ai
j}
7:
end for
8:
Shuffle Ai to obtain permuted response vector Ai"
REFERENCES,0.5854616895874263,"9:
A ←A ∪{Ai}
10: end for
11: Initialize answer-ranking data D ←∅
12: Initialize model weights vector w with Gaussian distribution
13: for each permuted response vector Ai do
14:
for each pair of responses (Aj
i, Ak
i ) in Ai do
15:
for s ←1 to 5 do
▷Randomly select 5 models for evaluation
16:
Evaluate the pair (Aj
i, Ak
i ) with model ms
17:
D ←D ∪{(Aj
i, Ak
i , > ws)}
18:
end for
19:
end for
20: end for
21: Initialize scores Gj for each model mj ∈M to the Elo initial score
22: repeat
23:
while not converged do
24:
for each model mj ∈M do
25:
Compute Gj using updated formula:
26:
Gj = P
i
P
k̸=j
P
s̸=k,s̸=j 1{Aj
i, Ak
i } × ws
(Aj
i, Ak
i , > ws, s ∈M∗) ∈D
27:
end for
28:
Update weight vector w to maximize the consistency of w and G
29:
end while
30:
Sort M∗by Gj to identify Mmin, the lowest-scoring model
31:
if size of M∗> threshold then
32:
Remove Mmin from M∗"
REFERENCES,0.5864440078585462,"33:
end if
34: until size of M∗< threshold
35: Compute the final ranking R∗based on the optimized scores Gj
36: return R∗"
REFERENCES,0.587426326129666,"E
Complete Experimental Results
529"
REFERENCES,0.5884086444007859,"In Section 3.4, we both employ elimination mechanisms to cull the weakest LLMs from the ’reviewer
530"
REFERENCES,0.5893909626719057,"queue’ during the evaluation process. In Figures 7 and 8, we present the results for the PEN and
531"
REFERENCES,0.5903732809430255,"LIS metrics, where lower PEN scores indicate better performance, and higher LIS scores denote
532"
REFERENCES,0.5913555992141454,"superior performance. It is evident that both the ’PiCO’ and PRE approaches demonstrate enhanced
533"
REFERENCES,0.5923379174852652,"performance as the number of eliminated ’reviewers’ increases. In most cases, the proposed ’PiCO’
534"
REFERENCES,0.593320235756385,"method outperforms PRE.
535"
REFERENCES,0.5943025540275049,"In Section 3.5, we validate the effectiveness of the consistency assumption and compare it with the
536"
REFERENCES,0.5952848722986247,"Average Performance of the Reviewer Queue, i.e., employing a single LLM as the ’reviewer’ to
537"
REFERENCES,0.5962671905697446,"evaluate all response pairs and then calculating the average results of all LLMs. The comprehensive
538"
REFERENCES,0.5972495088408645,"results compared with the Reviewer Queue are illustrated in Table3, Figure 9, 10 and 11, revealing
539"
REFERENCES,0.5982318271119843,"that in the full Reviewer Queue, the performance of the vast majority of LLMs is very poor, indicating
540"
REFERENCES,0.5992141453831041,"that the evaluations from most LLMs are noise. However, our ’PiCO’ approach nearly matches the
541"
REFERENCES,0.600196463654224,"evaluative prowess of the pool’s most capable LLM, GPT-3.5. Remarkably, given its unsupervised
542"
REFERENCES,0.6011787819253438,"nature, the ’PiCO’ method demonstrates the capability to mitigate the influence of noise, reaching the
543"
REFERENCES,0.6021611001964636,"Figure 7: Performance comparison of the PiCO (Ours) and PRE[17] methods on the MT-Bench, Chatbot Arena,
and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is PEN, where lower
values indicate better performance."
REFERENCES,0.6031434184675835,"Figure 8: Performance comparison of the PiCO (Ours) and PRE[17] methods on the MT-Bench, Chatbot Arena,
and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is LIS, where upper
values indicate better performance."
REFERENCES,0.6041257367387033,"Table 3: Comparison of performance across three datasets using Unsupervised methods versus using single
models in reviewer queue."
REFERENCES,0.6051080550098232,"Methods
MT-Bench
Chatbot Arena
AlpacaEval
PEN (↓) CIN(↓) LIS(↑) PEN (↓) CIN(↓) LIS(↑) PEN (↓) CIN(↓) LIS(↑)
Gpt-3.5
0.97
12.00 10.00
0.85
11.00 11.00
1.15
16.00 9.00
Guanaco-33B
1.25
21.00
8.00
1.50
28.00
7.00
1.26
20.00
9.00
Vicuna-13B
1.31
20.00
7.00
1.27
23.00
8.00
1.20
17.00
8.00
WizardLM-13B
1.15
17.00
9.00
1.27
19.00
8.00
1.17
17.00
9.00
Vicuna-7B
1.27
21.00
8.00
1.30
20.00
7.00
1.34
23.00
8.00
Koala-13B
1.67
43.00
6.00
1.34
23.00
8.00
1.54
31.00
7.00
gpt4all-13B
1.74
45.00
6.00
1.60
35.00
6.00
1.73
42.00
6.00
Mpt-7B
1.67
39.00
6.00
1.72
52.00
6.00
1.63
34.00
7.00
Oass-pythia-12B
1.77
50.00
5.00
1.74
42.00
5.00
1.70
47.00
6.00
Alpaca-13B
1.77
49.00
7.00
1.60
73.00
4.00
1.63
34.00
7.00
FastChat-T5-3B
1.45
29.00
7.00
1.53
30.00
7.00
1.30
22.00
7.00
ChatGLM-6B
1.59
33.00
7.00
1.71
55.00
5.00
1.63
34.00
6.00
StableLM-7B
1.68
63.00
5.00
1.75
44.00
5.00
1.72
56.00
4.00
Dolly-12B
1.76
46.00
6.00
1.57
71.00
6.00
1.75
54.00
6.00
LLaMA-13B
1.60
35.00
7.00
1.76
56.00
6.00
1.70
50.00
5.00
Average Performance of All Review LLMs
1.51
34.87
6.93
1.50
38.80
6.60
1.50
33.13
6.93
PRD[28]
1.15
17.00
8.00
1.15
17.00
8.00
1.21
19.00
9.00
PRE[17]
1.17
17.00
8.00
1.07
15.00
9.00
1.18
19.00
8.00
PiCO (Ours)
1.01
14.50
8.75
0.94
12.00
10.00
1.17
17.00
9.00"
REFERENCES,0.6060903732809431,"evaluation upper bound (the strongest LLM) within any given unknown LLM pool M, even in the
544"
REFERENCES,0.6070726915520629,"absence of prior ranking information.
545"
REFERENCES,0.6080550098231827,"Figure 9: Comparison of performance on the CIN metric across three datasets using Unsupervised methods
versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The
dotted line represents the average value using single models."
REFERENCES,0.6090373280943026,"Figure 10: Comparison of performance on the PEN metric across three datasets using Unsupervised methods
versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The
dotted line represents the average value using single models."
REFERENCES,0.6100196463654224,"F
Selected Models and Optimized Ranking
546"
REFERENCES,0.6110019646365422,"For our analysis, we meticulously selected 15 LLMs spanning a variety of architectures, encompassing
547"
REFERENCES,0.611984282907662,"both open-source and closed-source models, as detailed in the subsequent table. Our curated selection
548"
REFERENCES,0.6129666011787819,"features prominent LLMs including the closed-source ""gpt-3.5-turbo,"" ""chatglm"" which is predicated
549"
REFERENCES,0.6139489194499018,"on the encoder-decoder framework, ""fastchat-t5-3b"" that leverages Google’s T5 (Text-to-Text Transfer
550"
REFERENCES,0.6149312377210217,"Transformer) architecture, and ""llama-13b"" founded on the GPT architectural principles.
551"
REFERENCES,0.6159135559921415,"We have comprehensively detailed the ranking outcomes across three distinct datasets for our
552"
REFERENCES,0.6168958742632613,"comparative analysis, incorporating the optimized model rankings, names, and their respective scores.
553"
REFERENCES,0.6178781925343811,"Figure 11: Comparison of performance on the LIS metric across three datasets using Unsupervised methods
versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The
dotted line represents the average value using single models."
REFERENCES,0.618860510805501,"As delineated in Appendix C, the PiCO (Ours) is capable of employing various scoring mechanisms,
554"
REFERENCES,0.6198428290766208,"thereby facilitating the presentation of ranking outcomes on three datasets utilizing both the Elo and
555"
REFERENCES,0.6208251473477406,"Rank mechanisms. Furthermore, we have also enumerated the ranking results for PRD and PRE
556"
REFERENCES,0.6218074656188605,"methodologies across the three datasets, offering a holistic view of the competitive landscape.
557"
REFERENCES,0.6227897838899804,"F.1
PiCO
558"
REFERENCES,0.6237721021611002,Grade-Elo-Chatbot
REFERENCES,0.6247544204322201,"#1 Gpt-3.5 | Grade: 9205.162109375
#2 WizardLM-13B | Grade: 9143.46875
#3 Guanaco-33B | Grade: 5886.92626953125
#4 Vicuna-7B | Grade: 5368.9462890625
#5 Vicuna-13B | Grade: 5216.79541015625
#6 Koala-13B | Grade: 3545.1171875 | Eliminated
#7 Mpt-7B | Grade: 962.99462890625 | Eliminated
#8 Gpt4all-13B | Grade: 652.4602661132812 | Eliminated
#9 Chatglm-6B | Grade: 417.1375427246094 | Eliminated
#10 Oasst-pythia-12B | Grade: -898.2676391601562 | Eliminated
#11 Fastchat-t5-3B | Grade: -1251.7183837890625 | Eliminated
#12 StableLM-7B | Grade: -2232.66943359375 | Eliminated
#13 Dolly-12B | Grade: -3163.540283203125 | Eliminated
#14 Llama-13B | Grade: -3648.37841796875 | Eliminated
#15 Alpaca-13B | Grade: -14204.3984375 | Eliminated
559"
REFERENCES,0.6257367387033399,Grade-Elo-AlpacaEval
REFERENCES,0.6267190569744597,"#1 WizardLM-13B | Grade: 8662.7158203125
#2 Vicuna-13B | Grade: 5586.46630859375
#3 Guanaco-33B | Grade: 5445.341796875
#4 Vicuna-7B | Grade: 5374.2314453125
#5 Gpt-3.5 | Grade: 4845.91552734375
#6 Koala-13B | Grade: 4338.77783203125 | Eliminated
#7 Chatglm-6B | Grade: 2293.4208984375 | Eliminated
#8 Gpt4all-13B | Grade: 2080.511962890625 | Eliminated
#9 Mpt-7B | Grade: 1694.4945068359375 | Eliminated
#10 Fastchat-t5-3B | Grade: 1371.94287109375 | Eliminated
#11 Oasst-pythia-12B | Grade: -665.8685302734375 | Eliminated
#12 StableLM-7B | Grade: -1343.5838623046875 | Eliminated
#13 Dolly-12B | Grade: -5377.13427734375 | Eliminated
#14 Llama-13B | Grade: -5847.59130859375 | Eliminated
#15 Alpaca-13B | Grade: -13459.6162109375 | Eliminated 560"
REFERENCES,0.6277013752455796,Grade-Elo-MT_Bench
REFERENCES,0.6286836935166994,"#1 WizardLM-13B | Grade: 2178.10302734375
#2 Vicuna-13B | Grade: 1720.1114501953125
#3 Guanaco-33B | Grade: 1704.1832275390625
#4 Vicuna-7B | Grade: 1659.2799072265625
#5 Gpt-3.5 | Grade: 1535.8819580078125
#6 Mpt-7B | Grade: 1338.5235595703125 | Eliminated
#7 Koala-13B | Grade: 1267.9747314453125 | Eliminated
#8 Chatglm-6B | Grade: 1011.7701416015625 | Eliminated
#9 Gpt4all-13B | Grade: 976.5963745117188 | Eliminated
#10 Oasst-pythia-12B | Grade: 779.3573608398438 | Eliminated
#11 StableLM-7B | Grade: 512.1678466796875 | Eliminated
#12 Alpaca-13B | Grade: 334.9879455566406 | Eliminated
#13 Fastchat-t5-3B | Grade: 303.5980529785156 | Eliminated
#14 Dolly-12B | Grade: 72.63818359375 | Eliminated
#15 Llama-13B | Grade: -395.19921875 | Eliminated 561"
REFERENCES,0.6296660117878192,Grade-Rank-Chatbot
REFERENCES,0.630648330058939,"#1 WizardLM-13B | Grade: 0.30809280276298523
#2 Gpt-3.5 | Grade: 0.293962299823761
#3 Guanaco-33B | Grade: 0.28587597608566284
#4 Vicuna-7B | Grade: 0.28212910890579224
#5 Vicuna-13B | Grade: 0.27900218963623047
#6 Koala-13B | Grade: 0.2672431766986847 | Eliminated
#7 Mpt-7B | Grade: 0.2500302195549011 | Eliminated
#8 Gpt4all-13B | Grade: 0.24746862053871155 | Eliminated
#9 Chatglm-6B | Grade: 0.2466953843832016 | Eliminated
#10 Oasst-pythia-12B | Grade: 0.23637069761753082 | Eliminated
#11 Fastchat-t5-3B | Grade: 0.2350562959909439 | Eliminated
#12 StableLM-7B | Grade: 0.22843806445598602 | Eliminated
#13 Dolly-12B | Grade: 0.22219440340995789 | Eliminated
#14 Llama-13B | Grade: 0.2165679931640625 | Eliminated
#15 Alpaca-13B | Grade: 0.13975904881954193 | Eliminated 562"
REFERENCES,0.6316306483300589,Grade-Rank-AlpacaEval
REFERENCES,0.6326129666011788,"#1 WizardLM-13B | Grade: 0.4019235074520111
#2 Vicuna-13B | Grade: 0.36745429039001465
#3 Guanaco-33B | Grade: 0.3664878010749817
#4 Vicuna-7B | Grade: 0.36541733145713806
#5 Gpt-3.5 | Grade: 0.36000365018844604
#6 Koala-13B | Grade: 0.3544933795928955 | Eliminated
#7 Chatglm-6B | Grade: 0.3319571018218994 | Eliminated
#8 Gpt4all-13B | Grade: 0.3306528627872467 | Eliminated
#9 Mpt-7B | Grade: 0.32641729712486267 | Eliminated
#10 Fastchat-t5-3B | Grade: 0.32173293828964233 | Eliminated
#11 Oasst-pythia-12B | Grade: 0.2999681532382965 | Eliminated
#12 StableLM-7B | Grade: 0.2932431995868683 | Eliminated
#13 Dolly-12B | Grade: 0.24777530133724213 | Eliminated
#14 Llama-13B | Grade: 0.24381506443023682 | Eliminated
#15 Alpaca-13B | Grade: 0.16114839911460876 563"
REFERENCES,0.6335952848722987,Grade-Rank-MT_Bench
REFERENCES,0.6345776031434185,"#1 WizardLM-13B | Grade: 0.2994651198387146
#2 Vicuna-13B | Grade: 0.2809261679649353
#3 Guanaco-33B | Grade: 0.2767307460308075
#4 Vicuna-7B | Grade: 0.2758147716522217
#5 Gpt-3.5 | Grade: 0.27261608839035034
#6 Mpt-7B | Grade: 0.26338690519332886 | Eliminated
#7 Koala-13B | Grade: 0.2613368630409241 | Eliminated
#8 Gpt4all-13B | Grade: 0.24908888339996338 | Eliminated
#9 Chatglm-6B | Grade: 0.24898234009742737 | Eliminated
#10 Oasst-pythia-12B | Grade: 0.2415400892496109 | Eliminated
#11 StableLM-7B | Grade: 0.2299075722694397 | Eliminated
#12 Alpaca-13B | Grade: 0.22171474993228912 | Eliminated
#13 Fastchat-t5-3B | Grade: 0.221677765250206 | Eliminated
#14 Dolly-12B | Grade: 0.21185410022735596 | Eliminated
#15 Llama-13B | Grade: 0.192665234208107 | Eliminated 564"
REFERENCES,0.6355599214145383,"F.2
PRD
565"
REFERENCES,0.6365422396856582,PRD-Chatbot
REFERENCES,0.637524557956778,"#1 WizardLM-13B | Grade: 5565.28271484375
#2 Gpt-3.5 | Grade: 4613.22900390625
#3 Guanaco-33B | Grade: 3423.588134765625
#4 Vicuna-7B | Grade: 2985.4892578125
#5 Vicuna-13B | Grade: 2972.15673828125
#6 Koala-13B | Grade: 2237.70751953125
#7 Chatglm-6B | Grade: 875.373779296875
#8 Mpt-7B | Grade: 602.46923828125
#9 Gpt4all-13B | Grade: 356.06243896484375
#10 Fastchat-t5-3B | Grade: 184.89663696289062
#11 Dolly-12B | Grade: 52.10746765136719
#12 Oasst-pythia-12B | Grade: -307.49908447265625
#13 StableLM-7B | Grade: -691.4453735351562
#14 Llama-13B | Grade: -848.1654052734375
#15 Alpaca-13B | Grade: -7020.923828125 566"
REFERENCES,0.6385068762278978,PRD-AlpacaEval
REFERENCES,0.6394891944990176,"#1 WizardLM-13B | Grade: 5469.75634765625
#2 Guanaco-33B | Grade: 3707.014892578125
#3 Vicuna-13B | Grade: 3618.63427734375
#4 Vicuna-7B | Grade: 3569.389892578125
#5 Gpt-3.5 | Grade: 3197.755615234375
#6 Koala-13B | Grade: 2893.642578125
#7 Chatglm-6B | Grade: 1847.1300048828125
#8 Fastchat-t5-3B | Grade: 1585.66943359375
#9 Gpt4all-13B | Grade: 1561.145751953125
#10 Mpt-7B | Grade: 1332.3753662109375
#11 StableLM-7B | Grade: -33.00855255126953
#12 Oasst-pythia-12B | Grade: -92.68387603759766
#13 Dolly-12B | Grade: -3013.588623046875
#14 Llama-13B | Grade: -3211.0302734375
#15 Alpaca-13B | Grade: -7432.3701171875 567"
REFERENCES,0.6404715127701375,PRD-MT_Bench
REFERENCES,0.6414538310412574,"#1 WizardLM-13B | Grade: 1811.64697265625
#2 Vicuna-13B | Grade: 1537.8084716796875
#3 Guanaco-33B | Grade: 1481.1739501953125
#4 Vicuna-7B | Grade: 1401.5194091796875
#5 Gpt-3.5 | Grade: 1272.8072509765625
#6 Mpt-7B | Grade: 1186.5518798828125
#7 Chatglm-6B | Grade: 1166.6246337890625
#8 Koala-13B | Grade: 1124.2513427734375
#9 Gpt4all-13B | Grade: 871.2874755859375
#10 Oasst-pythia-12B | Grade: 855.3653564453125
#11 StableLM-7B | Grade: 782.702880859375
#12 Fastchat-t5-3B | Grade: 636.966064453125
#13 Alpaca-13B | Grade: 414.9374694824219
#14 Dolly-12B | Grade: 377.5018005371094
#15 Llama-13B | Grade: 78.90127563476562 568"
REFERENCES,0.6424361493123772,"F.3
PRE
569"
REFERENCES,0.6434184675834971,PRE-Chatbot
REFERENCES,0.6444007858546169,"#1 WizardLM-13B | Grade: 1113.7034715479742
#2 Gpt-3.5 | Grade: 1076.1116664199608
#3 Guanaco-33B | Grade: 1067.441581415147
#4 Vicuna-13B | Grade: 1057.702184441485
#5 Vicuna-7B | Grade: 1043.4840340151043
#6 Koala-13B | Grade: 1030.4455842017508 | Eliminated
#7 Chatglm-6B | Grade: 1012.4487557424748 | Eliminated
#8 Mpt-7B | Grade: 1000.487230109001 | Eliminated
#9 Gpt4all-13B | Grade: 1000.4111397038492 | Eliminated
#10 Fastchat-t5-3B | Grade: 992.3732179832363 | Eliminated
#11 Oasst-pythia-12B | Grade: 977.5217305871272 | Eliminated
#12 StableLM-7B | Grade: 970.3665926795535 | Eliminated
#13 Llama-13B | Grade: 929.6268868888149 | Eliminated
#14 Dolly-12B | Grade: 929.1943463130976 | Eliminated
#15 Alpaca-13B | Grade: 798.6815779514078 | Eliminated 570"
REFERENCES,0.6453831041257367,PRE-AlpacaEval
REFERENCES,0.6463654223968566,"#1 WizardLM-13B | Grade: 1127.822808841937
#2 Vicuna-7B | Grade: 1077.1823389450524
#3 Vicuna-13B | Grade: 1075.4338443616266
#4 Guanaco-33B | Grade: 1074.8043135229418
#5 Gpt-3.5 | Grade: 1065.305736105376
#6 Gpt4all-13B | Grade: 1039.4091630861865 | Eliminated
#7 Koala-13B | Grade: 1038.205749976473 | Eliminated
#8 Mpt-7B | Grade: 1032.2893401162178 | Eliminated
#9 Chatglm-6B | Grade: 1027.1937496918501 | Eliminated
#10 Fastchat-t5-3B | Grade: 992.3481168791307 | Eliminated
#11 StableLM-7B | Grade: 979.3894141445692 | Eliminated
#12 Oasst-pythia-12B | Grade: 940.6438439723215 | Eliminated
#13 Dolly-12B | Grade: 886.1412110662756 | Eliminated
#14 Llama-13B | Grade: 880.0797724297793 | Eliminated
#15 Alpaca-13B | Grade: 763.7505968602533 | Eliminated 571"
REFERENCES,0.6473477406679764,PRE-MT_Bench
REFERENCES,0.6483300589390962,"#1 WizardLM-13B | Grade: 1065.5843776639435
#2 Vicuna-13B | Grade: 1062.3934138040302
#3 Guanaco-33B | Grade: 1052.2206466556906
#4 Vicuna-7B | Grade: 1035.1112817247572
#5 Gpt-3.5 | Grade: 1029.8316754711038
#6 Koala-13B | Grade: 1024.9307662983267 | Eliminated
#7 Chatglm-6B | Grade: 1020.5238960907612 | Eliminated
#8 Mpt-7B | Grade: 1014.0683255081057 | Eliminated
#9 Gpt4all-13B | Grade: 991.7142639623017 | Eliminated
#10 StableLM-7B | Grade: 979.8443261256327 | Eliminated
#11 Oasst-pythia-12B | Grade: 977.9930430111322 | Eliminated
#12 Fastchat-t5-3B | Grade: 953.0776159143571 | Eliminated
#13 Alpaca-13B | Grade: 949.129770731626 | Eliminated
#14 Dolly-12B | Grade: 928.511065779112 | Eliminated
#15 Llama-13B | Grade: 915.0655312591185 | Eliminated 572"
REFERENCES,0.6493123772102161,"NeurIPS Paper Checklist
573"
CLAIMS,0.650294695481336,"1. Claims
574"
CLAIMS,0.6512770137524558,"Question: Do the main claims made in the abstract and introduction accurately reflect the
575"
CLAIMS,0.6522593320235757,"paper’s contributions and scope?
576"
CLAIMS,0.6532416502946955,"Answer: [Yes]
577"
CLAIMS,0.6542239685658153,"Justification: We clearly state our claims in the abstract and introduction, such as a novel
578"
CLAIMS,0.6552062868369352,"unsupervised LLM evaluation method and a consistency-based constrained optimization
579"
CLAIMS,0.656188605108055,"approach. These are substantiated in Section 3, demonstrating the alignment between our
580"
CLAIMS,0.6571709233791748,"theoretical contributions and empirical results.
581"
CLAIMS,0.6581532416502947,"Guidelines:
582"
CLAIMS,0.6591355599214146,"• The answer NA means that the abstract and introduction do not include the claims
583"
CLAIMS,0.6601178781925344,"made in the paper.
584"
CLAIMS,0.6611001964636543,"• The abstract and/or introduction should clearly state the claims made, including the
585"
CLAIMS,0.6620825147347741,"contributions made in the paper and important assumptions and limitations. A No or
586"
CLAIMS,0.6630648330058939,"NA answer to this question will not be perceived well by the reviewers.
587"
CLAIMS,0.6640471512770137,"• The claims made should match theoretical and experimental results, and reflect how
588"
CLAIMS,0.6650294695481336,"much the results can be expected to generalize to other settings.
589"
CLAIMS,0.6660117878192534,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
590"
CLAIMS,0.6669941060903732,"are not attained by the paper.
591"
LIMITATIONS,0.6679764243614931,"2. Limitations
592"
LIMITATIONS,0.668958742632613,"Question: Does the paper discuss the limitations of the work performed by the authors?
593"
LIMITATIONS,0.6699410609037328,"Answer: [No]
594"
LIMITATIONS,0.6709233791748527,"Justification: Although this paper does not have a separate ’Limitations’ section, the con-
595"
LIMITATIONS,0.6719056974459725,"sistency assumptions on which the work is based are clearly stated in the introduction, and
596"
LIMITATIONS,0.6728880157170923,"their validity is experimentally verified in Section 3.5. Moreover, the limitations of our work
597"
LIMITATIONS,0.6738703339882122,"are discussed in the conclusion, noting that the current study is conducted solely within a
598"
LIMITATIONS,0.674852652259332,"text-based llm evaluation environment, and exploring the potential for future expansion into
599"
LIMITATIONS,0.6758349705304518,"multimodal large model assessments.
600"
LIMITATIONS,0.6768172888015717,"Guidelines:
601"
LIMITATIONS,0.6777996070726916,"• The answer NA means that the paper has no limitation while the answer No means that
602"
LIMITATIONS,0.6787819253438114,"the paper has limitations, but those are not discussed in the paper.
603"
LIMITATIONS,0.6797642436149313,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
604"
LIMITATIONS,0.6807465618860511,"• The paper should point out any strong assumptions and how robust the results are to
605"
LIMITATIONS,0.6817288801571709,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
606"
LIMITATIONS,0.6827111984282908,"model well-specification, asymptotic approximations only holding locally). The authors
607"
LIMITATIONS,0.6836935166994106,"should reflect on how these assumptions might be violated in practice and what the
608"
LIMITATIONS,0.6846758349705304,"implications would be.
609"
LIMITATIONS,0.6856581532416502,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
610"
LIMITATIONS,0.6866404715127702,"only tested on a few datasets or with a few runs. In general, empirical results often
611"
LIMITATIONS,0.68762278978389,"depend on implicit assumptions, which should be articulated.
612"
LIMITATIONS,0.6886051080550099,"• The authors should reflect on the factors that influence the performance of the approach.
613"
LIMITATIONS,0.6895874263261297,"For example, a facial recognition algorithm may perform poorly when image resolution
614"
LIMITATIONS,0.6905697445972495,"is low or images are taken in low lighting. Or a speech-to-text system might not be
615"
LIMITATIONS,0.6915520628683693,"used reliably to provide closed captions for online lectures because it fails to handle
616"
LIMITATIONS,0.6925343811394892,"technical jargon.
617"
LIMITATIONS,0.693516699410609,"• The authors should discuss the computational efficiency of the proposed algorithms
618"
LIMITATIONS,0.6944990176817288,"and how they scale with dataset size.
619"
LIMITATIONS,0.6954813359528488,"• If applicable, the authors should discuss possible limitations of their approach to
620"
LIMITATIONS,0.6964636542239686,"address problems of privacy and fairness.
621"
LIMITATIONS,0.6974459724950884,"• While the authors might fear that complete honesty about limitations might be used by
622"
LIMITATIONS,0.6984282907662083,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
623"
LIMITATIONS,0.6994106090373281,"limitations that aren’t acknowledged in the paper. The authors should use their best
624"
LIMITATIONS,0.7003929273084479,"judgment and recognize that individual actions in favor of transparency play an impor-
625"
LIMITATIONS,0.7013752455795678,"tant role in developing norms that preserve the integrity of the community. Reviewers
626"
LIMITATIONS,0.7023575638506876,"will be specifically instructed to not penalize honesty concerning limitations.
627"
THEORY ASSUMPTIONS AND PROOFS,0.7033398821218074,"3. Theory Assumptions and Proofs
628"
THEORY ASSUMPTIONS AND PROOFS,0.7043222003929273,"Question: For each theoretical result, does the paper provide the full set of assumptions and
629"
THEORY ASSUMPTIONS AND PROOFS,0.7053045186640472,"a complete (and correct) proof?
630"
THEORY ASSUMPTIONS AND PROOFS,0.706286836935167,"Answer: [Yes]
631"
THEORY ASSUMPTIONS AND PROOFS,0.7072691552062869,"Justification: We thoroughly detail the Consistency Assumption which underpins our the-
632"
THEORY ASSUMPTIONS AND PROOFS,0.7082514734774067,"oretical results and provide a complete proof in Section 3.5. Furthermore, we ensure that
633"
THEORY ASSUMPTIONS AND PROOFS,0.7092337917485265,"all necessary assumptions are explicitly stated and each theorem and proof is carefully
634"
THEORY ASSUMPTIONS AND PROOFS,0.7102161100196464,"numbered and cross-referenced for clarity and accessibility.
635"
THEORY ASSUMPTIONS AND PROOFS,0.7111984282907662,"Guidelines:
636"
THEORY ASSUMPTIONS AND PROOFS,0.712180746561886,"• The answer NA means that the paper does not include theoretical results.
637"
THEORY ASSUMPTIONS AND PROOFS,0.7131630648330058,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
638"
THEORY ASSUMPTIONS AND PROOFS,0.7141453831041258,"referenced.
639"
THEORY ASSUMPTIONS AND PROOFS,0.7151277013752456,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
640"
THEORY ASSUMPTIONS AND PROOFS,0.7161100196463654,"• The proofs can either appear in the main paper or the supplemental material, but if
641"
THEORY ASSUMPTIONS AND PROOFS,0.7170923379174853,"they appear in the supplemental material, the authors are encouraged to provide a short
642"
THEORY ASSUMPTIONS AND PROOFS,0.7180746561886051,"proof sketch to provide intuition.
643"
THEORY ASSUMPTIONS AND PROOFS,0.7190569744597249,"• Inversely, any informal proof provided in the core of the paper should be complemented
644"
THEORY ASSUMPTIONS AND PROOFS,0.7200392927308448,"by formal proofs provided in appendix or supplemental material.
645"
THEORY ASSUMPTIONS AND PROOFS,0.7210216110019646,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7220039292730844,"4. Experimental Result Reproducibility
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7229862475442044,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7239685658153242,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.724950884086444,"of the paper (regardless of whether the code and data are provided or not)?
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7259332023575639,"Answer: [Yes]
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7269155206286837,"Justification: We provide detailed pseudocode of our new LLM evaluation algorithm in
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7278978388998035,"Appendix D and have made all relevant data and code publicly accessible on GitHub,
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7288801571709234,"ensuring anonymity during the review process. This comprehensive disclosure allows other
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7298624754420432,"researchers to reproduce our experimental results, fully aligning with our paper’s claims and
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.730844793713163,"enhancing the credibility of our findings.
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.731827111984283,"Guidelines:
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7328094302554028,"• The answer NA means that the paper does not include experiments.
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7337917485265226,"• If the paper includes experiments, a No answer to this question will not be perceived
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7347740667976425,"well by the reviewers: Making the paper reproducible is important, regardless of
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7357563850687623,"whether the code and data are provided or not.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7367387033398821,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.737721021611002,"to make their results reproducible or verifiable.
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7387033398821218,"• Depending on the contribution, reproducibility can be accomplished in various ways.
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7396856581532416,"For example, if the contribution is a novel architecture, describing the architecture fully
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7406679764243614,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7416502946954814,"be necessary to either make it possible for others to replicate the model with the same
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7426326129666012,"dataset, or provide access to the model. In general. releasing code and data is often
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.743614931237721,"one good way to accomplish this, but reproducibility can also be provided via detailed
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7445972495088409,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7455795677799607,"of a large language model), releasing of a model checkpoint, or other means that are
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7465618860510805,"appropriate to the research performed.
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7475442043222004,"• While NeurIPS does not require releasing code, the conference does require all submis-
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7485265225933202,"sions to provide some reasonable avenue for reproducibility, which may depend on the
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.74950884086444,"nature of the contribution. For example
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75049115913556,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7514734774066798,"to reproduce that algorithm.
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7524557956777996,"(b) If the contribution is primarily a new model architecture, the paper should describe
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7534381139489195,"the architecture clearly and fully.
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7544204322200393,"(c) If the contribution is a new model (e.g., a large language model), then there should
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7554027504911591,"either be a way to access this model for reproducing the results or a way to reproduce
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.756385068762279,"the model (e.g., with an open-source dataset or instructions for how to construct
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7573673870333988,"the dataset).
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7583497053045186,"(d) We recognize that reproducibility may be tricky in some cases, in which case
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7593320235756386,"authors are welcome to describe the particular way they provide for reproducibility.
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7603143418467584,"In the case of closed-source models, it may be that access to the model is limited in
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7612966601178782,"some way (e.g., to registered users), but it should be possible for other researchers
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.762278978388998,"to have some path to reproducing or verifying the results.
688"
OPEN ACCESS TO DATA AND CODE,0.7632612966601179,"5. Open access to data and code
689"
OPEN ACCESS TO DATA AND CODE,0.7642436149312377,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
690"
OPEN ACCESS TO DATA AND CODE,0.7652259332023575,"tions to faithfully reproduce the main experimental results, as described in supplemental
691"
OPEN ACCESS TO DATA AND CODE,0.7662082514734774,"material?
692"
OPEN ACCESS TO DATA AND CODE,0.7671905697445972,"Answer: [Yes]
693"
OPEN ACCESS TO DATA AND CODE,0.768172888015717,"Justification: All necessary data and code have been made publicly available on GitHub,
694"
OPEN ACCESS TO DATA AND CODE,0.769155206286837,"with detailed instructions for installation, environment setup, and execution commands. This
695"
OPEN ACCESS TO DATA AND CODE,0.7701375245579568,"includes all raw, pre-processed, intermediate, and generated data needed to reproduce our
696"
OPEN ACCESS TO DATA AND CODE,0.7711198428290766,"experimental results. The repository is anonymous during the review process to ensure
697"
OPEN ACCESS TO DATA AND CODE,0.7721021611001965,"compliance with double-blind requirements. This thorough documentation ensures that
698"
OPEN ACCESS TO DATA AND CODE,0.7730844793713163,"other researchers can faithfully replicate our study.
699"
OPEN ACCESS TO DATA AND CODE,0.7740667976424361,"Guidelines:
700"
OPEN ACCESS TO DATA AND CODE,0.775049115913556,"• The answer NA means that paper does not include experiments requiring code.
701"
OPEN ACCESS TO DATA AND CODE,0.7760314341846758,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
702"
OPEN ACCESS TO DATA AND CODE,0.7770137524557956,"public/guides/CodeSubmissionPolicy) for more details.
703"
OPEN ACCESS TO DATA AND CODE,0.7779960707269156,"• While we encourage the release of code and data, we understand that this might not be
704"
OPEN ACCESS TO DATA AND CODE,0.7789783889980354,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
705"
OPEN ACCESS TO DATA AND CODE,0.7799607072691552,"including code, unless this is central to the contribution (e.g., for a new open-source
706"
OPEN ACCESS TO DATA AND CODE,0.7809430255402751,"benchmark).
707"
OPEN ACCESS TO DATA AND CODE,0.7819253438113949,"• The instructions should contain the exact command and environment needed to run to
708"
OPEN ACCESS TO DATA AND CODE,0.7829076620825147,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
709"
OPEN ACCESS TO DATA AND CODE,0.7838899803536346,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
710"
OPEN ACCESS TO DATA AND CODE,0.7848722986247544,"• The authors should provide instructions on data access and preparation, including how
711"
OPEN ACCESS TO DATA AND CODE,0.7858546168958742,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
712"
OPEN ACCESS TO DATA AND CODE,0.7868369351669942,"• The authors should provide scripts to reproduce all experimental results for the new
713"
OPEN ACCESS TO DATA AND CODE,0.787819253438114,"proposed method and baselines. If only a subset of experiments are reproducible, they
714"
OPEN ACCESS TO DATA AND CODE,0.7888015717092338,"should state which ones are omitted from the script and why.
715"
OPEN ACCESS TO DATA AND CODE,0.7897838899803536,"• At submission time, to preserve anonymity, the authors should release anonymized
716"
OPEN ACCESS TO DATA AND CODE,0.7907662082514735,"versions (if applicable).
717"
OPEN ACCESS TO DATA AND CODE,0.7917485265225933,"• Providing as much information as possible in supplemental material (appended to the
718"
OPEN ACCESS TO DATA AND CODE,0.7927308447937131,"paper) is recommended, but including URLs to data and code is permitted.
719"
OPEN ACCESS TO DATA AND CODE,0.793713163064833,"6. Experimental Setting/Details
720"
OPEN ACCESS TO DATA AND CODE,0.7946954813359528,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
721"
OPEN ACCESS TO DATA AND CODE,0.7956777996070727,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
722"
OPEN ACCESS TO DATA AND CODE,0.7966601178781926,"results?
723"
OPEN ACCESS TO DATA AND CODE,0.7976424361493124,"Answer: [Yes]
724"
OPEN ACCESS TO DATA AND CODE,0.7986247544204322,"Justification: We have detailed the data processing and training procedures in Sections 2.2
725"
OPEN ACCESS TO DATA AND CODE,0.7996070726915521,"and Appendices A, B, and D. For comprehensive understanding, additional information
726"
OPEN ACCESS TO DATA AND CODE,0.8005893909626719,"such as hyperparameters, optimizer types, and detailed data splits are provided alongside
727"
OPEN ACCESS TO DATA AND CODE,0.8015717092337917,"the code due to space constraints in the paper.
728"
OPEN ACCESS TO DATA AND CODE,0.8025540275049116,"Guidelines:
729"
OPEN ACCESS TO DATA AND CODE,0.8035363457760314,"• The answer NA means that the paper does not include experiments.
730"
OPEN ACCESS TO DATA AND CODE,0.8045186640471512,"• The experimental setting should be presented in the core of the paper to a level of detail
731"
OPEN ACCESS TO DATA AND CODE,0.8055009823182712,"that is necessary to appreciate the results and make sense of them.
732"
OPEN ACCESS TO DATA AND CODE,0.806483300589391,"• The full details can be provided either with the code, in appendix, or as supplemental
733"
OPEN ACCESS TO DATA AND CODE,0.8074656188605108,"material.
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8084479371316307,"7. Experiment Statistical Significance
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8094302554027505,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8104125736738703,"information about the statistical significance of the experiments?
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8113948919449901,"Answer: [Yes]
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.81237721021611,"Justification: We conducted each experiment four times using different seeds (seed =
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8133595284872298,"1, 2, 3, 4) to ensure robustness. The results, presented as averages, are accompanied by
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8143418467583498,"standard deviations as error bars in Tables 1 and 2. This approach captures the variability
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8153241650294696,"due to different initializations and confirms the reproducibility of our results. The standard
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8163064833005894,"deviations used help clarify the extent of variability in the experiments, ensuring that our
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8172888015717092,"statistical analysis aligns with best practices for empirical research.
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8182711198428291,"Guidelines:
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8192534381139489,"• The answer NA means that the paper does not include experiments.
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8202357563850687,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8212180746561886,"dence intervals, or statistical significance tests, at least for the experiments that support
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8222003929273084,"the main claims of the paper.
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8231827111984283,"• The factors of variability that the error bars are capturing should be clearly stated (for
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8241650294695482,"example, train/test split, initialization, random drawing of some parameter, or overall
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.825147347740668,"run with given experimental conditions).
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8261296660117878,"• The method for calculating the error bars should be explained (closed form formula,
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8271119842829077,"call to a library function, bootstrap, etc.)
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8280943025540275,"• The assumptions made should be given (e.g., Normally distributed errors).
755"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8290766208251473,"• It should be clear whether the error bar is the standard deviation or the standard error
756"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8300589390962672,"of the mean.
757"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.831041257367387,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
758"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8320235756385069,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
759"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8330058939096268,"of Normality of errors is not verified.
760"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8339882121807466,"• For asymmetric distributions, the authors should be careful not to show in tables or
761"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8349705304518664,"figures symmetric error bars that would yield results that are out of range (e.g. negative
762"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8359528487229863,"error rates).
763"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8369351669941061,"• If error bars are reported in tables or plots, The authors should explain in the text how
764"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8379174852652259,"they were calculated and reference the corresponding figures or tables in the text.
765"
EXPERIMENTS COMPUTE RESOURCES,0.8388998035363457,"8. Experiments Compute Resources
766"
EXPERIMENTS COMPUTE RESOURCES,0.8398821218074656,"Question: For each experiment, does the paper provide sufficient information on the com-
767"
EXPERIMENTS COMPUTE RESOURCES,0.8408644400785854,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
768"
EXPERIMENTS COMPUTE RESOURCES,0.8418467583497053,"the experiments?
769"
EXPERIMENTS COMPUTE RESOURCES,0.8428290766208252,"Answer: [No]
770"
EXPERIMENTS COMPUTE RESOURCES,0.843811394891945,"Justification: Although we did not detail the exact compute resources for each experimental
771"
EXPERIMENTS COMPUTE RESOURCES,0.8447937131630648,"setup in the paper, we used NVIDIA A6000 graphics cards for open-source models and API
772"
EXPERIMENTS COMPUTE RESOURCES,0.8457760314341847,"calls for proprietary models. To facilitate reproducibility, we have provided all necessary
773"
EXPERIMENTS COMPUTE RESOURCES,0.8467583497053045,"data, ensuring that the experiments can be replicated on consumer-grade computers. This
774"
EXPERIMENTS COMPUTE RESOURCES,0.8477406679764243,"approach allows readers to reproduce the results without requiring high-end computational
775"
EXPERIMENTS COMPUTE RESOURCES,0.8487229862475442,"resources.
776"
EXPERIMENTS COMPUTE RESOURCES,0.849705304518664,"Guidelines:
777"
EXPERIMENTS COMPUTE RESOURCES,0.8506876227897839,"• The answer NA means that the paper does not include experiments.
778"
EXPERIMENTS COMPUTE RESOURCES,0.8516699410609038,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
779"
EXPERIMENTS COMPUTE RESOURCES,0.8526522593320236,"or cloud provider, including relevant memory and storage.
780"
EXPERIMENTS COMPUTE RESOURCES,0.8536345776031434,"• The paper should provide the amount of compute required for each of the individual
781"
EXPERIMENTS COMPUTE RESOURCES,0.8546168958742633,"experimental runs as well as estimate the total compute.
782"
EXPERIMENTS COMPUTE RESOURCES,0.8555992141453831,"• The paper should disclose whether the full research project required more compute
783"
EXPERIMENTS COMPUTE RESOURCES,0.8565815324165029,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
784"
EXPERIMENTS COMPUTE RESOURCES,0.8575638506876228,"didn’t make it into the paper).
785"
CODE OF ETHICS,0.8585461689587426,"9. Code Of Ethics
786"
CODE OF ETHICS,0.8595284872298625,"Question: Does the research conducted in the paper conform, in every respect, with the
787"
CODE OF ETHICS,0.8605108055009824,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
788"
CODE OF ETHICS,0.8614931237721022,"Answer: [Yes]
789"
CODE OF ETHICS,0.862475442043222,"Justification: The research conducted in this paper complies with the NeurIPS ethics
790"
CODE OF ETHICS,0.8634577603143418,"guidelines in all respects.
791"
CODE OF ETHICS,0.8644400785854617,"Guidelines:
792"
CODE OF ETHICS,0.8654223968565815,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
793"
CODE OF ETHICS,0.8664047151277013,"• If the authors answer No, they should explain the special circumstances that require a
794"
CODE OF ETHICS,0.8673870333988212,"deviation from the Code of Ethics.
795"
CODE OF ETHICS,0.8683693516699411,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
796"
CODE OF ETHICS,0.869351669941061,"eration due to laws or regulations in their jurisdiction).
797"
BROADER IMPACTS,0.8703339882121808,"10. Broader Impacts
798"
BROADER IMPACTS,0.8713163064833006,"Question: Does the paper discuss both potential positive societal impacts and negative
799"
BROADER IMPACTS,0.8722986247544204,"societal impacts of the work performed?
800"
BROADER IMPACTS,0.8732809430255403,"Answer: [Yes]
801"
BROADER IMPACTS,0.8742632612966601,"Justification: In the introduction, we discuss the potential positive impact of our novel
802"
BROADER IMPACTS,0.8752455795677799,"unsupervised LLM evaluation approach, which could significantly advance the field of LLM
803"
BROADER IMPACTS,0.8762278978388998,"evaluation. However, we also recognize potential negative societal impacts, such as the
804"
BROADER IMPACTS,0.8772102161100196,"misuse of this technology to unfairly or inaccurately assess LLM systems, which might
805"
BROADER IMPACTS,0.8781925343811395,"lead to biased or misleading outcomes. We suggest potential mitigation strategies, such as
806"
BROADER IMPACTS,0.8791748526522594,"implementing robust validation protocols and ethical guidelines to govern the application of
807"
BROADER IMPACTS,0.8801571709233792,"this evaluation methodology.
808"
BROADER IMPACTS,0.881139489194499,"Guidelines:
809"
BROADER IMPACTS,0.8821218074656189,"• The answer NA means that there is no societal impact of the work performed.
810"
BROADER IMPACTS,0.8831041257367387,"• If the authors answer NA or No, they should explain why their work has no societal
811"
BROADER IMPACTS,0.8840864440078585,"impact or why the paper does not address societal impact.
812"
BROADER IMPACTS,0.8850687622789783,"• Examples of negative societal impacts include potential malicious or unintended uses
813"
BROADER IMPACTS,0.8860510805500982,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
814"
BROADER IMPACTS,0.8870333988212181,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
815"
BROADER IMPACTS,0.888015717092338,"groups), privacy considerations, and security considerations.
816"
BROADER IMPACTS,0.8889980353634578,"• The conference expects that many papers will be foundational research and not tied
817"
BROADER IMPACTS,0.8899803536345776,"to particular applications, let alone deployments. However, if there is a direct path to
818"
BROADER IMPACTS,0.8909626719056974,"any negative applications, the authors should point it out. For example, it is legitimate
819"
BROADER IMPACTS,0.8919449901768173,"to point out that an improvement in the quality of generative models could be used to
820"
BROADER IMPACTS,0.8929273084479371,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
821"
BROADER IMPACTS,0.8939096267190569,"that a generic algorithm for optimizing neural networks could enable people to train
822"
BROADER IMPACTS,0.8948919449901768,"models that generate Deepfakes faster.
823"
BROADER IMPACTS,0.8958742632612967,"• The authors should consider possible harms that could arise when the technology is
824"
BROADER IMPACTS,0.8968565815324165,"being used as intended and functioning correctly, harms that could arise when the
825"
BROADER IMPACTS,0.8978388998035364,"technology is being used as intended but gives incorrect results, and harms following
826"
BROADER IMPACTS,0.8988212180746562,"from (intentional or unintentional) misuse of the technology.
827"
BROADER IMPACTS,0.899803536345776,"• If there are negative societal impacts, the authors could also discuss possible mitigation
828"
BROADER IMPACTS,0.9007858546168959,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
829"
BROADER IMPACTS,0.9017681728880157,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
830"
BROADER IMPACTS,0.9027504911591355,"feedback over time, improving the efficiency and accessibility of ML).
831"
SAFEGUARDS,0.9037328094302554,"11. Safeguards
832"
SAFEGUARDS,0.9047151277013753,"Question: Does the paper describe safeguards that have been put in place for responsible
833"
SAFEGUARDS,0.9056974459724951,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
834"
SAFEGUARDS,0.906679764243615,"image generators, or scraped datasets)?
835"
SAFEGUARDS,0.9076620825147348,"Answer: [NA]
836"
SAFEGUARDS,0.9086444007858546,"Justification: This paper introduces a new approach for unsupervised LLM evaluation and
837"
SAFEGUARDS,0.9096267190569745,"does not involve the release of pre-trained models, image generators, or newly collected
838"
SAFEGUARDS,0.9106090373280943,"datasets. Therefore, there are no direct risks associated with misuse or dual-use of such
839"
SAFEGUARDS,0.9115913555992141,"resources, making safeguards for controlled release irrelevant to this study.
840"
SAFEGUARDS,0.912573673870334,"Guidelines:
841"
SAFEGUARDS,0.9135559921414538,"• The answer NA means that the paper poses no such risks.
842"
SAFEGUARDS,0.9145383104125737,"• Released models that have a high risk for misuse or dual-use should be released with
843"
SAFEGUARDS,0.9155206286836935,"necessary safeguards to allow for controlled use of the model, for example by requiring
844"
SAFEGUARDS,0.9165029469548134,"that users adhere to usage guidelines or restrictions to access the model or implementing
845"
SAFEGUARDS,0.9174852652259332,"safety filters.
846"
SAFEGUARDS,0.918467583497053,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
847"
SAFEGUARDS,0.9194499017681729,"should describe how they avoided releasing unsafe images.
848"
SAFEGUARDS,0.9204322200392927,"• We recognize that providing effective safeguards is challenging, and many papers do
849"
SAFEGUARDS,0.9214145383104125,"not require this, but we encourage authors to take this into account and make a best
850"
SAFEGUARDS,0.9223968565815324,"faith effort.
851"
LICENSES FOR EXISTING ASSETS,0.9233791748526523,"12. Licenses for existing assets
852"
LICENSES FOR EXISTING ASSETS,0.9243614931237721,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
853"
LICENSES FOR EXISTING ASSETS,0.925343811394892,"the paper, properly credited and are the license and terms of use explicitly mentioned and
854"
LICENSES FOR EXISTING ASSETS,0.9263261296660118,"properly respected?
855"
LICENSES FOR EXISTING ASSETS,0.9273084479371316,"Answer: [Yes]
856"
LICENSES FOR EXISTING ASSETS,0.9282907662082515,"Justification: This paper utilizes the FastChat project’s code, along with several other pre-
857"
LICENSES FOR EXISTING ASSETS,0.9292730844793713,"trained models and datasets. The FastChat project adheres to the Apache License 2.0. In
858"
LICENSES FOR EXISTING ASSETS,0.9302554027504911,"compliance with the licensing requirements, we have included the original project’s licensing
859"
LICENSES FOR EXISTING ASSETS,0.931237721021611,"information in all derivative works and have clearly marked any modifications made to the
860"
LICENSES FOR EXISTING ASSETS,0.9322200392927309,"code. Additionally, we have ensured that all utilized pre-trained models and datasets are
861"
LICENSES FOR EXISTING ASSETS,0.9332023575638507,"appropriately cited.
862"
LICENSES FOR EXISTING ASSETS,0.9341846758349706,"Guidelines:
863"
LICENSES FOR EXISTING ASSETS,0.9351669941060904,"• The answer NA means that the paper does not use existing assets.
864"
LICENSES FOR EXISTING ASSETS,0.9361493123772102,"• The authors should cite the original paper that produced the code package or dataset.
865"
LICENSES FOR EXISTING ASSETS,0.93713163064833,"• The authors should state which version of the asset is used and, if possible, include a
866"
LICENSES FOR EXISTING ASSETS,0.9381139489194499,"URL.
867"
LICENSES FOR EXISTING ASSETS,0.9390962671905697,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
868"
LICENSES FOR EXISTING ASSETS,0.9400785854616895,"• For scraped data from a particular source (e.g., website), the copyright and terms of
869"
LICENSES FOR EXISTING ASSETS,0.9410609037328095,"service of that source should be provided.
870"
LICENSES FOR EXISTING ASSETS,0.9420432220039293,"• If assets are released, the license, copyright information, and terms of use in the
871"
LICENSES FOR EXISTING ASSETS,0.9430255402750491,"package should be provided. For popular datasets, paperswithcode.com/datasets
872"
LICENSES FOR EXISTING ASSETS,0.944007858546169,"has curated licenses for some datasets. Their licensing guide can help determine the
873"
LICENSES FOR EXISTING ASSETS,0.9449901768172888,"license of a dataset.
874"
LICENSES FOR EXISTING ASSETS,0.9459724950884086,"• For existing datasets that are re-packaged, both the original license and the license of
875"
LICENSES FOR EXISTING ASSETS,0.9469548133595285,"the derived asset (if it has changed) should be provided.
876"
LICENSES FOR EXISTING ASSETS,0.9479371316306483,"• If this information is not available online, the authors are encouraged to reach out to
877"
LICENSES FOR EXISTING ASSETS,0.9489194499017681,"the asset’s creators.
878"
NEW ASSETS,0.949901768172888,"13. New Assets
879"
NEW ASSETS,0.9508840864440079,"Question: Are new assets introduced in the paper well documented and is the documentation
880"
NEW ASSETS,0.9518664047151277,"provided alongside the assets?
881"
NEW ASSETS,0.9528487229862476,"Answer: [NA]
882"
NEW ASSETS,0.9538310412573674,"Justification: The paper does not release new assets.
883"
NEW ASSETS,0.9548133595284872,"Guidelines:
884"
NEW ASSETS,0.9557956777996071,"• The answer NA means that the paper does not release new assets.
885"
NEW ASSETS,0.9567779960707269,"• Researchers should communicate the details of the dataset/code/model as part of their
886"
NEW ASSETS,0.9577603143418467,"submissions via structured templates. This includes details about training, license,
887"
NEW ASSETS,0.9587426326129665,"limitations, etc.
888"
NEW ASSETS,0.9597249508840865,"• The paper should discuss whether and how consent was obtained from people whose
889"
NEW ASSETS,0.9607072691552063,"asset is used.
890"
NEW ASSETS,0.9616895874263262,"• At submission time, remember to anonymize your assets (if applicable). You can either
891"
NEW ASSETS,0.962671905697446,"create an anonymized URL or include an anonymized zip file.
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9636542239685658,"14. Crowdsourcing and Research with Human Subjects
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9646365422396856,"Question: For crowdsourcing experiments and research with human subjects, does the paper
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9656188605108055,"include the full text of instructions given to participants and screenshots, if applicable, as
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9666011787819253,"well as details about compensation (if any)?
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9675834970530451,"Answer: [NA]
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685658153241651,"Justification: This paper focuses on an unsupervised evaluation method for LLMs that
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695481335952849,"does not require human feedback or interaction. Consequently, there is no involvement of
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9705304518664047,"crowdsourcing or research with human subjects, making details about participant instructions
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9715127701375246,"and compensation irrelevant.
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724950884086444,"Guidelines:
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9734774066797642,"• The answer NA means that the paper does not involve crowdsourcing nor research with
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744597249508841,"human subjects.
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754420432220039,"• Including this information in the supplemental material is fine, but if the main contribu-
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9764243614931237,"tion of the paper involves human subjects, then as much detail as possible should be
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774066797642437,"included in the main paper.
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783889980353635,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793713163064833,"or other labor should be paid at least the minimum wage in the country of the data
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9803536345776032,"collector.
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981335952848723,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823182711198428,"Subjects
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833005893909627,"Question: Does the paper describe potential risks incurred by study participants, whether
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842829076620825,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9852652259332023,"approvals (or an equivalent approval/review based on the requirements of your country or
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862475442043221,"institution) were obtained?
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872298624754421,"Answer: [NA]
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882121807465619,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891944990176817,"Guidelines:
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901768172888016,"• The answer NA means that the paper does not involve crowdsourcing nor research with
920"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911591355599214,"human subjects.
921"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921414538310412,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
922"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931237721021611,"may be required for any human subjects research. If you obtained IRB approval, you
923"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941060903732809,"should clearly state this in the paper.
924"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950884086444007,"• We recognize that the procedures for this may vary significantly between institutions
925"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960707269155207,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
926"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970530451866405,"guidelines for their institution.
927"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980353634577603,"• For initial submissions, do not include any information that would break anonymity (if
928"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990176817288802,"applicable), such as the institution conducting the review.
929"
