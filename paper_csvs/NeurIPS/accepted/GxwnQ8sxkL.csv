Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002793296089385475,"Imagine a smart camera trap selectively clicking pictures to understand animal
movement patterns within a particular habitat. These ""snapshots"", or pieces of
data captured from a data stream at adaptively chosen times, provide a glimpse
of different animal movements unfolding through time. Learning a continuous-
time process through snapshots, such as smart camera traps, is a central theme
governing a wide array of online learning situations. In this paper, we adopt a
learning-theoretic perspective in understanding the fundamental nature of learning
different classes of functions from both discrete data streams and continuous data
streams. In our first framework, the update-and-deploy setting, a learning algorithm
discretely queries from a process to update a predictor designed to make predictions
given as input the data stream. We construct a uniform sampling algorithm that can
learn with bounded error any concept class with finite Littlestone dimension. Our
second framework, known as the blind-prediction setting, consists of a learning
algorithm generating predictions independently of observing the process, only
engaging with the process when it chooses to make queries. Interestingly, we show
a stark contrast in learnability where non-trivial concept classes are unlearnable.
However, we show that adaptive learning algorithms are necessary to learn sets
of time-dependent and data-dependent functions, called pattern classes, in either
framework. Finally, we develop a theory of pattern classes under discrete data
streams for the blind-prediction setting."
INTRODUCTION,0.00558659217877095,"1
Introduction"
TWO MOTIVATING EXAMPLES,0.008379888268156424,"1.1
Two Motivating Examples"
TWO MOTIVATING EXAMPLES,0.0111731843575419,"Pretend you’re a farmer by day and businessperson by night. As a farmer, you oversee a 10,000 acre
plot of land equipped with a smart irrigation system. To feed data to your irrigation system, you rely
on hyperspectral imaging taken from a satellite to gauge soil moisture conditions. Ideally, you would
like to constantly feed your irrigation system with hyperspectral data; however, the steep financial
cost of processing hyperspectral data prevents you from doing so. As a result, you need to devise a
strategy to sparingly use satellite data; at all other times, you rely on the smart irrigation system to
accurately extrapolate the soil moisture conditions as time passes by."
TWO MOTIVATING EXAMPLES,0.013966480446927373,"At night, you become a businessperson. You employ a translator on your work laptop during your
virtual meetings to automatically convert your voice into the preferred language of your client. This
translator is fine-tuned by a speech-to-text translation system that takes in voice data and updates
the translator’s model on the correct language translation. But, there’s a caveat. Each request costs
money. And each transmission dominates a sizable portion of the available Internet bandwidth. Your
task is to come up with the optimal strategy of balancing requests to the cloud versus trusting the
fidelity of the translator."
A NEW LEARNING PARADIGM,0.01675977653631285,"1.2
A New Learning Paradigm"
A NEW LEARNING PARADIGM,0.019553072625698324,"While these settings may seem rather creative in nature, both of these scenarios represent plausible
real-world instances of learning from continuous data streams with temporal dependencies. What type
of learning-theoretic framework should one construct when framing the question of online learning
under continuous data streams? How can we best capture the notion of temporal dependencies and
patterns that naturally arise when analyzing such data sources? While these questions are highly
pertinent, the answers aren’t clear due to a vast majority of the learning theory literature focusing
on online learnability from discrete data streams modeled as round-by-round processes. In the two
examples showcased at the beginning, it’s clear that establishing a theoretical understanding of these
settings can be an important step in tackling online learnability under continuous data streams."
A NEW LEARNING PARADIGM,0.0223463687150838,"In our paper, we present a streamlined approach in tackling these rather fundamental challenges by
first establishing two closely related, but separate, frameworks."
A NEW LEARNING PARADIGM,0.025139664804469275,"Blind-Prediction Setting
The first framework is called blind-prediction which is highlighted by
the smart irrigation system using satellite imagery data. The irrigation system receives feedback only
when hyperspectral data is requested; at all other times, the system must predict on its own with no
input from the environment. This framework is designed such that a learning algorithm must make a
prediction based only on the current timestamp and previous queries. The learner’s predictions are
independent of the current values generated by the data stream hence the name blind-prediction."
A NEW LEARNING PARADIGM,0.027932960893854747,"Update-and-Deploy Setting
The second framework, called update-and-deploy, is highlighted by
the speech-to-text translation system. The speech-to-text translation system, a learning algorithm,
and the translator, called the predictor, are considered as two separate entities where the algorithm
retrieves snapshots of the data stream to update the predictor. We describe this behavior as a learning
algorithm activating different modes at different times. A learning algorithm performs updates to a
predictor when it queries and deploys the predictor to make predictions as the process rolls by."
A NEW LEARNING PARADIGM,0.030726256983240222,"Pattern Classes
A significant portion of this work is dedicated to studying these frameworks under
pattern classes, sets of sequences encoding data-dependent and time-dependent characteristics. First
introduced by Moran et al. [1], these classes consist of a set of patterns; each pattern is a sequence
of instance-label pairs marked with the appropriate timestamp. For example, if we let X and Y
represent the instance space and label space respectively, then Z∞= (X × Y)∞represents the set of
all countably infinite patterns. A discrete pattern class P is defined as P ⊆Z∞where any P ∈P is
understood as P = (Zt)∞
t=1 = (Xt, Yt)∞
t=1."
A NEW LEARNING PARADIGM,0.0335195530726257,"Pattern classes can also be viewed as natural generalizations of concept classes. Given a concept class
H consisting of classifiers mapping instances from X to labels in Y, we can derive a pattern class
that encapsulates all sequences that could be realized by any single h ∈H. Formally speaking, the
induced pattern class P(H) is defined as P(H) = {(Zt)∞
t=1 ∈Z∞: ∃h ∈H, ∀t ∈N, h(Xt) = Yt}"
A NEW LEARNING PARADIGM,0.036312849162011177,"Now that the stage has been developed for pattern classes, we turn to a set of questions that naturally
arise under continuous data streams. What pattern classes are online learnable? Is there a natural
dimension that characterizes online learnability of pattern classes under the different querying-based
models? How does learning pattern classes and concept classes differ under continuous data streams?
We tackle these important questions in our paper using our learning frameworks."
OUR CONTRIBUTIONS,0.03910614525139665,"1.3
Our Contributions"
OUR CONTRIBUTIONS,0.04189944134078212,We detail the primary contributions of this work below.
OUR CONTRIBUTIONS,0.0446927374301676,"1. Non-Adaptive Learners in the Update-and-Deploy Setting. First, we extend the current
theory on concept classes to include online learning under continuous data streams for
the update-and-deploy setting. A non-adaptive learner is a learning algorithm that queries
independent of the process itself. For the update-and-deploy setting, we show that the non-
adaptive learner, Aunif, that uniformly samples its queries from a fixed uniform distribution,
achieves a bounded expected error with a linear querying strategy."
OUR CONTRIBUTIONS,0.04748603351955307,"Theorem 1.1 (Informal Version). Given an instance space X and a label space Y, let
H ⊆YX be a concept class where LD(H) represents the Littlestone dimension of H. For"
OUR CONTRIBUTIONS,0.05027932960893855,"any H that has LD(H) < ∞, Aunif achieves an expected error bound MBP(H)(Aunif) ≤
∆LD(H) with a linear querying strategy QAunif(t) = O(t) where ∆is an input parameter."
OUR CONTRIBUTIONS,0.05307262569832402,"2. Concept Class Learnability in the Blind-Prediction Setting. Second, we show that non-
trivial concept classes aren’t learnable within the blind-prediction setting. Letting H be any
concept class that contains a classifier that labels two points differently, then any learning
algorithm, adaptive or non-adaptive, is not learnable in the blind-prediction setting."
OUR CONTRIBUTIONS,0.055865921787709494,"Theorem 1.2 (Informal Version). For any H and two points x1, x2 ∈X such that ∃h ∈H
where h(x1) ̸= h(x2), then for any learning algorithm A, the expected mistake-bound
MBP(H)(A) = ∞."
OUR CONTRIBUTIONS,0.05865921787709497,"3. Adaptive Learners for Pattern Classes. As our third result, we investigate what types
of learning algorithms are required to learn pattern classes under continuous data streams.
In Section 4.3, we design a continuous pattern class P, where each pattern P ∈P is a
continuous sequence of point-label pairs (Xt, Yt)t≥0, that is not learnable by any random
sampling algorithm such as Aunif. Additionally, we construct an adaptive learning algorithm
that successfully learns P with zero expected error. This important example signifies a
learnability gap between concept classes and pattern classes."
OUR CONTRIBUTIONS,0.061452513966480445,"4. Discrete Data Streams. Fourth, we develop a theory for realizable learning of pattern
classes under discrete data streams in the blind-prediction setting for deterministic learning
algorithms. We characterize a combinatorial quantity called the query-learning distance
or QLD for discrete pattern classes P with a query budget Q ∈N ∪{0}. We show that
the optimal mistake-bound given Q queries, MQ(P), is lower bounded by QLD(P, Q).
Then, we construct a deterministic learning algorithm whose optimal mistake-bound is upper
bounded by QLD(P, Q)."
OUR CONTRIBUTIONS,0.06424581005586592,"Theorem 1.3 (Informal Version). For a discrete pattern class P and number of queries Q,
the optimal mistake-bound MQ(P) = QLD(P, Q)."
RELATED WORK,0.0670391061452514,"1.4
Related Work"
RELATED WORK,0.06983240223463687,"An extensively studied area in online learning theory closely related to our work is the round-by-
round learning of concept classes from discrete data streams in the realizable setting. Littlestone
[2] successfully characterized the types of concept classes H that are learnable under an adversarial
online setting which is now famously known as the Littlestone dimension or LD(H). Later, Daniely
et al. [3] extended this result to the multi-class setting, showing that LD(H) also characterizes
multi-class learnability. A recently explored setting called self-directed online learning shares an
important trait with our learning frameworks which is adaptivity in selecting points where Devulapalli
and Hanneke [4] constructed a dimension, SDdim(H), characterizing learnable concept classes."
RELATED WORK,0.07262569832402235,"While traditional approaches assume that the learner receives the true label after each round, our study
diverges by focusing on frameworks where feedback is only provided when actively queried by the
learner. Our work is conceptually aligned with the area of partial monitoring, which investigates how
various feedback constraints influence a learner’s ability to minimize regret. A series of studies have
established optimal regret bounds across different online learning scenarios, structured as discrete
data streams with diverse feedback mechanisms [5–11]."
RELATED WORK,0.07541899441340782,"A core principle within our learning frameworks is the ability of a learning algorithm to selectively
query at different time-steps within a data-stream which is shared by stream-based active learning
approaches. Several works within the field have explored theoretical guarantees of active learning
in different variations of the stream-based setting [12–15]. However, a crucial difference between
stream-based active learning and learning models in this work is the decision to query at a particular
time is carried out before the current instance is observed."
LEARNING FRAMEWORKS,0.0782122905027933,"2
Learning Frameworks"
BASIC DEFINITIONS,0.08100558659217877,"2.1
Basic Definitions"
BASIC DEFINITIONS,0.08379888268156424,"Let X and Y be arbitrary, non-empty sets where X is referred to as the instance space and Y is the
label space. A concept class H ⊆YX consists of functions f : X →Y. Depending on the context,"
BASIC DEFINITIONS,0.08659217877094973,"we will specify if we are considering a multi-class setting where |Y| ≥2 or a binary classification
setting where Y = {0, 1}."
BASIC DEFINITIONS,0.0893854748603352,"To define a continuous data stream, we use the notation (Zt)t≥0 = (Xt, Yt)t≥0 to define a point
and label pair Zt = (Xt, Yt) for each t ∈R≥0. A continuous pattern class P is defined as
P ⊆C((Xt, Yt)t≥0) where C((Xt, Yt)t≥0) represents the collection of all measurable continuous-
time processes on the space X × Y. Each pattern P ∈P is then a continuous-time process (Zt)t≥0."
BASIC DEFINITIONS,0.09217877094972067,"We now proceed to define discrete pattern classes and subsequently, discrete data streams. Let
Z = X × Y where z ∈Z and z = (x, y). Define Z∞= (X × Y)∞which is the set of all countably
infinite patterns. Then the discrete pattern class P ⊆Z∞. Both continuous and discrete pattern
classes are referred to as P so it will be clear from context which type of pattern class we are referring
to. It then follows that a discrete data stream (Zt)∞
t=1 = (Xt, Yt)∞
t=1 lives in the space Z∞."
UPDATE-AND-DEPLOY SETTING,0.09497206703910614,"2.2
Update-and-Deploy Setting"
UPDATE-AND-DEPLOY SETTING,0.09776536312849161,"In this learning framework, we aim to describe the online learning game that occurs between a learner
and an oblivious adversary. An oblivious adversary is an adversary impervious to any of the learner’s
actions; in other words, the adversary does not adapt its strategy based on the learner’s actions. As a
result, the oblivious adversary fixes the entire data stream in advance of the learning process."
UPDATE-AND-DEPLOY SETTING,0.1005586592178771,"Denote by F a class of predictor functions ˆf. With D representing the timestamps of the data stream,
either discrete or continuous, then ˆf : X ×D →Y is designed to make a prediction at every timestamp
t ∈D. In the update-and-deploy setting, we consider the learning algorithm A and the predictor ˆf
to be separate entities. Denote by QA(t) = {(Xt1, Yt1), (Xt2, Yt2), ...} the set of queries made by
learning algorithm A before time t. Intuitively, a learning algorithm is a mapping A : (X ×Y)∗→F
where (X ×Y)∗corresponds to the set QA(t). Formally, A((Xt1, Yt1), ..., (XQA(t), YQA(t))) outputs
a predictor ˆf ∈F given the history of previous queries QA(t). It’s important to note that we only
consider learning algorithms A that have a linear querying strategy or QA(t) = O(t)."
UPDATE-AND-DEPLOY SETTING,0.10335195530726257,"Assume the adversary has selected a data stream (Zt)t∈D. For each t ∈D, the predictor ˆf produces
predictions ˆYt = ˆf(Xt, t) given Xt and t. On timestamps t ∈D that the learning algorithm A
decides to query, the following procedure occurs:"
UPDATE-AND-DEPLOY SETTING,0.10614525139664804,"1. The learner A makes a decision to query and receives the true point-label pair (Xt, Yt)."
UPDATE-AND-DEPLOY SETTING,0.10893854748603352,"2. A updates the predictor ˆf with (Xt, Yt)."
UPDATE-AND-DEPLOY SETTING,0.11173184357541899,3. ˆf is redeployed as the new predictor.
UPDATE-AND-DEPLOY SETTING,0.11452513966480447,"It’s important to note that the data stream selected by the adversary is constrained to be realizable.
If the realizability is with respect to a concept class H, then ∃h ∈H, ∀t ∈D, h(Xt) = Yt. If
the setting is studied under a discrete pattern class P, then the pattern is considered realizable if
(Xt, Yt)∞
t=1 ∈P. If D represents a continuous data stream and P a continuous pattern class, then the
pattern (Xt, Yt)t≥0 ∈P implies realizability."
BLIND-PREDICTION SETTING,0.11731843575418995,"2.3
Blind-Prediction Setting"
BLIND-PREDICTION SETTING,0.12011173184357542,"For our second learning framework, we describe the online learning game between the learner and an
oblivious adversary. As similarly described in Section 2.2. an oblivious adversary acts independently
of the learner’s actions and fixes the entire data stream beforehand."
BLIND-PREDICTION SETTING,0.12290502793296089,"Let A be any learning algorithm and let QA(t) be the set of queries made by a learning algorithm A
before time t. As mentioned in Section 2.2, we consider algorithms with a linear querying strategy
where QA(t) = O(t). Letting D be the timestamps of the data stream, A is described as a mapping
A : (X × Y)∗× D →Y where (X × Y)∗corresponds to the set QA(t). At any time t, A only
observes the current timestamp t and the history of queries QA(t) when making a prediction ˆYt. If it
decides to query, then A witnesses the true instance-label pair (Xt, Yt)."
BLIND-PREDICTION SETTING,0.12569832402234637,Assume that the adversary has selected a data stream (Zt)t∈D. For each t ∈D:
BLIND-PREDICTION SETTING,0.12849162011173185,1. The learner A selects a prediction ˆYt ∈Y.
BLIND-PREDICTION SETTING,0.13128491620111732,"2. If the learner decided to query, then the pair (Xt, Yt) is revealed to the learner."
BLIND-PREDICTION SETTING,0.1340782122905028,"It’s important to note that the data stream selected by the adversary is constrained to be realizable.
Refer to Section 2.2 for realizability regarding concept classes and pattern classes."
INTEGRAL MISTAKE-BOUNDS,0.13687150837988826,"2.4
Integral Mistake-Bounds"
INTEGRAL MISTAKE-BOUNDS,0.13966480446927373,"To capture the optimal behavior of learning algorithms under continuous data streams, we formalize
the notion of integral mistake-bounds. Since we consider two separate settings, we construct a general
mistake-bound and then differentiate from context which setting the mistake-bound operates under."
INTEGRAL MISTAKE-BOUNDS,0.1424581005586592,"Due to their nature, pattern classes subsume concept classes so we define all the mistake-bounds
with respect to pattern classes. Let D = R≥0 which denotes the timestamps of a continuous stream.
The pattern class representation of a concept class H, or P(H), is defined in the following way:
P(H) = {(Xt, Yt)t≥0 : ∃h ∈H, ∀t ∈D, h(Xt) = Yt}. It is important to note that we assume that
each pattern P ∈P for any continuous pattern class P is measurable."
INTEGRAL MISTAKE-BOUNDS,0.1452513966480447,"Given a continuous pattern class P, a learning algorithm A, and some realizable continuous data
stream (Zt)t≥0, the quantity MBP(A, (Zt)t≥0) represents the expected error A makes on the data
stream (Zt)t≥0 given P. Formally,"
INTEGRAL MISTAKE-BOUNDS,0.14804469273743018,"MBP(A, (Zt)t≥0) = lim
T →∞E ""Z T"
INTEGRAL MISTAKE-BOUNDS,0.15083798882681565,"0
1[A(Xt) ̸= Yt] dt # ."
INTEGRAL MISTAKE-BOUNDS,0.15363128491620112,"To define the optimal mistake-bound for P, we take the supremum over all patterns in the class:"
INTEGRAL MISTAKE-BOUNDS,0.1564245810055866,"MBP(A) =
sup
(Zt)t≥0=P ∈P
MBP(A, (Zt)t≥0)."
INTEGRAL MISTAKE-BOUNDS,0.15921787709497207,"Finally, we obtain the optimal mistake-bound for the pattern class P by taking the infimum over all
learning algorithms corresponding to the learning setting (blind-prediction or update-and-deploy):"
INTEGRAL MISTAKE-BOUNDS,0.16201117318435754,"MBP = inf
A MBP(A)."
INTEGRAL MISTAKE-BOUNDS,0.164804469273743,"3
Update-and-Deploy Setting: Learning Concept Classes from Continuous
Data Streams"
LITTLESTONE CLASSES ARE LEARNABLE,0.16759776536312848,"3.1
Littlestone Classes are Learnable"
LITTLESTONE CLASSES ARE LEARNABLE,0.17039106145251395,"In this section, we are interested in multi-class concept classes H that are learnable in the update-
and-deploy setting with learning algorithms deploying a linear querying strategy. Below, we give
a definition of the learnability of a concept class H which allows us to frame our first important
question.
Definition 3.1. A concept class H is learnable if the following condition is satisfied: there exists an
algorithm A such that MBP(H)(A) < ∞and QA(t) = O(t)."
LITTLESTONE CLASSES ARE LEARNABLE,0.17318435754189945,"Question: What is the dimension that characterizes the learnability of a concept class H where
finiteness implies learnability and an infinite value implies non-learnability?"
LITTLESTONE CLASSES ARE LEARNABLE,0.17597765363128492,"Once we have defined learnability of a concept class H, our interest immediately swings towards the
performance of different learning algorithms with linear querying strategies. Naturally, we want to
understand if there exists optimal learning algorithms whose expected error is finite if the concept
class H is learnable. This then leads us to our second important question."
LITTLESTONE CLASSES ARE LEARNABLE,0.1787709497206704,"Question: Does there exist a learning algorithm A employing a linear querying strategy such that for
every H that is learnable, does MBP(H)(A) < ∞? If so, does the learning algorithm employ an
adaptive strategy?"
LITTLESTONE CLASSES ARE LEARNABLE,0.18156424581005587,"The Littlestone dimension [2] is a key measure that defines the learnability across various online
learning frameworks. Extending this concept, we investigate whether the Littlestone dimension can
similarly influence learnability in the context of continuous data streams. We propose that LD(H)"
LITTLESTONE CLASSES ARE LEARNABLE,0.18435754189944134,"could be a valuable combinatorial tool for designing learning algorithms in the continuous setting. To
explore this, we introduce Algorithm 1, or Aunif, which is designed to learn any concept class with a
finite Littlestone dimension, LD(H) < ∞, by using a linear querying approach."
LITTLESTONE CLASSES ARE LEARNABLE,0.1871508379888268,"The idea behind Aunif is to randomize the timestamp of the query so that the adversary has to ""guess""
which point in the data stream the learner will decide to target. If the timestamp of the query is not
randomized, then the adversary can select a data stream designed with this knowledge. A potential
strategy an adversary could employ against a deterministic learning algorithm would be to present
the same point again and again to the learner for every query. Since the learner has only received
information about one point, the adversary can present other points in the data stream at times the
learner doesn’t query forcing errors to occur. As a result, the adversary has a strategy to force
an infinite mistake-bound to a learning algorithm that employs a deterministic querying strategy
regardless if it’s adaptive or non-adaptive."
LITTLESTONE CLASSES ARE LEARNABLE,0.18994413407821228,"To avoid this issue, we fitted Aunif with a randomized querying strategy. As shown in Algorithm 1,
Aunif samples the next timestamp of the query, tq, from a uniform distribution over an interval of
fixed width ∆."
LITTLESTONE CLASSES ARE LEARNABLE,0.19273743016759776,"Algorithm 1 Uniform Sampler(H, ∆)
Require: H ̸= ∅
Require: ∆> 0"
LITTLESTONE CLASSES ARE LEARNABLE,0.19553072625698323,"1: V = H, t = time, starts at t = 0, tq ∼Unif[t, t + ∆]
2: Deploy ˆf(xt, t) = arg maxr∈{0,1} LD(V(xt,r))
3: while true do
4:
if t = tq then
5:
Query at time tq and receive point-label pair (xtq, ytq)
6:
Update V = V(xtq ,ytq )"
LITTLESTONE CLASSES ARE LEARNABLE,0.19832402234636873,"7:
Redeploy ˆf(xt, t) = arg maxr∈{0,1} LD(V(xt,r))
8:
tq ∼Unif[t, t + ∆]
9:
end if
10: end while"
LITTLESTONE CLASSES ARE LEARNABLE,0.2011173184357542,"In Algorithm 1, notice that the predictor function ˆf follows that of the Standard Optimal Algorithm,
or SOA, defined by Littlestone [2]. Since the LD(H) < ∞, and if the prediction differs from the
true label on a query point, then the Littlestone dimension of the subsequent version space is reduced
by at least 1. This property follows immediately from the analysis of the SOA, so the learner knows
that it needs only LD(H) successful queries to fully learn H from the continuous data stream."
LITTLESTONE CLASSES ARE LEARNABLE,0.20391061452513967,"Additionally, note that while Algorithm 1 decides the next tq after the previous query finishes, this is
done non-adaptively. The timestamp tq is not dependent on the true label witnessed by the previous
queries; it’s simply sampled from a uniform distribution. As a result, the set of query timestamps are
produced in a non-adaptive fashion by sampling the next query from an interval of width ∆.
Theorem 3.2. Let Aunif be Algorithm 1. For any H that has LD(H) < ∞, MBP(H)(Aunif) ≤
∆LD(H) where ∆is an input parameter from Algorithm 1. Since QAunif(t) = O(t), then H is
learnable."
LITTLESTONE CLASSES ARE LEARNABLE,0.20670391061452514,"Proof. For a given H with LD(H) < ∞, we show that the expected mistake-bound of algorithm
Aunif is bounded proportionally to the size of LD(H) using a linear querying strategy. Since Aunif
deploys the SOA as its predictor, then the mistake-bound of Aunif is inherently tied to LD(H).
In other words, if Aunif makes LD(H) successful queries, where success implies that the SOA’s
prediction is incorrect on the query point, then the version space has Littlestone dimension of 0
implying that any consistent classifier subsequently makes zero error onwards. Our analysis first
focuses on bounding the maximum expected error Aunif makes until its first successful query. We
repeat this analysis LD(H) −1 times to show that MBP(H)(Aunif) ≤∆LD(H) with a linear
querying strategy."
LITTLESTONE CLASSES ARE LEARNABLE,0.20949720670391062,"As a starting point, we define all the necessary quantities in order to begin the analysis. Since our
learning model assumes an oblivious adversary, it selects a continuous data stream (Zt)t≥0 realizable
with respect to some target concept f ∗∈H before the learning process begins. Let the random"
LITTLESTONE CLASSES ARE LEARNABLE,0.2122905027932961,"variable Bk be an indicator random variable representing the success of the kth query on the process
(Zt)t≥0. More specifically,"
LITTLESTONE CLASSES ARE LEARNABLE,0.21508379888268156,"Bk =
1
if the kth query is successful
0
else"
LITTLESTONE CLASSES ARE LEARNABLE,0.21787709497206703,"takes a value of 1 if the kth query succeeds. Then, we define P(Bk = 1|Bk−1 = 0, Bk−2 =
0, ..., B1 = 0) = ϵk which is the probability that the learner has a successful query on the kth try
given that the previous k −1 attempts failed. ϵk can be equivalently viewed as the probability of
the learner making an error on the kth interval because a successful query results in receiving a
mistake-point, or a point the predictor incorrectly predicts. Since Aunif selects its kth query tk
q from
a ∆-sized interval, then ∆ϵk represents the total potential error the learner makes on the kth interval."
LITTLESTONE CLASSES ARE LEARNABLE,0.2206703910614525,"Our primary interest is calculating the expected error Aunif makes until it reaches LD(H) successful
queries. Since the learner Aunif deploys an SOA predictor, LD(H) successful queries where the
predictor is incorrect guarantees the learner to narrow down on the right set of consistent classifiers."
LITTLESTONE CLASSES ARE LEARNABLE,0.22346368715083798,"We approach this by first computing the expected error that the learning algorithm makes until its
first successful query. It’s important to note that Aunif does not alter its querying strategy regardless
of the number of successful queries it has received; it constantly chooses its queries from intervals of
size ∆. As a result, after the learner receives its first successful query, the same process repeats again
until Aunif finds it second successful query. So, we focus on bounding the maximum expected error
Aunif will encounter until its next successful query for the data stream (Zt)t≥0."
LITTLESTONE CLASSES ARE LEARNABLE,0.22625698324022347,"Let A be a function that represents the maximum error the learner receives until its first suc-
cessful query given the values of the random variables B1, B2, ... Formally speaking, let A =
A(B1, B2, ...) = ∆(ϵ1 + ϵ2(1 −B1) + ϵ3(1 −B1)(1 −B2) + · · · ) = ∆P∞
k=1 ϵkΠk−1
i=1 (1 −Bi).
Each ∆ϵk represents the error region in the kth interval given that the previous k −1 queries failed
or each Bi = 0 for all i ≤k −1. It’s important to observe that A is the maximum error the learner
receives until the first successful query. As an example, let Bn = 1 for some n ∈N and Bj = 0 for
all j < n. Then A includes the cumulative error from the first n −1 intervals and the entire potential
error on the nth interval (represented as ∆ϵn) even though the nth query, which is successful, can
lie anywhere within the ∆ϵn error region located inside the nth interval. Now, we compute the
expectation of A."
LITTLESTONE CLASSES ARE LEARNABLE,0.22905027932960895,"E[A] = E "" ∆ ∞
X"
LITTLESTONE CLASSES ARE LEARNABLE,0.23184357541899442,"i=1
ϵkΠk−1
i=1 (1 −Bi) # = ∆ ∞
X"
LITTLESTONE CLASSES ARE LEARNABLE,0.2346368715083799,"k=1
ϵkE

Πk−1
i=1 (1 −Bi)
 = ∆ ∞
X"
LITTLESTONE CLASSES ARE LEARNABLE,0.23743016759776536,"k=1
ϵkP(B1 = 0, ..., Bk−1 = 0) = ∆ ∞
X"
LITTLESTONE CLASSES ARE LEARNABLE,0.24022346368715083,"k=1
ϵkΠk−1
i=1 (1 −ϵi)"
LITTLESTONE CLASSES ARE LEARNABLE,0.2430167597765363,"Since we are interested in the maximum expected error the learner Aunif encounters until its first
successful query, we want to bound the term ∆P∞
k=1 ϵ(k)Πk−1
i=1 (1 −ϵi) by selecting the optimal
values for ϵ1, ϵ2, ... Notice that the expression is recursive in the sense that if we pulled out the first k
terms, the structure of the sum doesn’t change. We then exploit this fact to bound the total value of
the sum. Let U ∗= sup⃗ϵ∈[0,1]∞∆P∞
k=1⃗ϵ(k)Πk−1
i=1 (1 −⃗ϵ(i)) where ⃗ϵ(1) = ϵ1,⃗ϵ(2) = ϵ2, and so on
and so forth. Then,"
LITTLESTONE CLASSES ARE LEARNABLE,0.24581005586592178,"U ∗=
sup
⃗ϵ∈[0,1]∞∆ ∞
X"
LITTLESTONE CLASSES ARE LEARNABLE,0.24860335195530725,"k=1
⃗ϵ(k)Πk−1
i=1 (1 −⃗ϵ(i)) =
sup
⃗ϵ∈[0,1]∞∆⃗ϵ(1) + ∆(1 −⃗ϵ(1)) ∞
X"
LITTLESTONE CLASSES ARE LEARNABLE,0.25139664804469275,"k=2
⃗ϵ(k)Πk−1
i=2 (1 −⃗ϵ(i))"
LITTLESTONE CLASSES ARE LEARNABLE,0.2541899441340782,"≤sup
p∈[0,1]
∆p + (1 −p) "
LITTLESTONE CLASSES ARE LEARNABLE,0.2569832402234637,"sup
⃗ϵ∈[0,1]∞∆ ∞
X"
LITTLESTONE CLASSES ARE LEARNABLE,0.25977653631284914,"k=1
⃗ϵ(k)Πk−1
i=1 (1 −⃗ϵ(i)) !"
LITTLESTONE CLASSES ARE LEARNABLE,0.26256983240223464,"≤sup
p∈[0,1]
∆p + (1 −p)U ∗"
LITTLESTONE CLASSES ARE LEARNABLE,0.26536312849162014,"≤sup
p∈[0,1]"
LITTLESTONE CLASSES ARE LEARNABLE,0.2681564245810056,"∆p
1 −(1 −p) = ∆."
LITTLESTONE CLASSES ARE LEARNABLE,0.2709497206703911,"Therefore, we show that E[A] ≤∆."
LITTLESTONE CLASSES ARE LEARNABLE,0.2737430167597765,"At the beginning of this analysis, we assumed some adversarially chosen data stream and target
concept, so the result E[A] ≤∆holds for any choice of (Zt)t≥0 realizable with respect to H."
LITTLESTONE CLASSES ARE LEARNABLE,0.276536312849162,"Now, we repeat this analysis LD(H) −1 times. Therefore, MBP(H)(Aunif) ≤∆LD(H) where
QAunif(t) = O(t)."
LITTLESTONE CLASSES ARE LEARNABLE,0.27932960893854747,"In Theorem 3.2, we establish that if LD(H) is finite, then H is learnable in the update-and-deploy
setting. This leads to our second result, which demonstrates that LD(H) serves as the defining
dimension for the learnability of a concept class H in this context.
Theorem 3.3. If LD(H) = ∞, then for any learning algorithm A with a linear querying strategy
QA(t), MBP(H)(A) = ∞implying that H is not learnable."
LITTLESTONE CLASSES ARE LEARNABLE,0.28212290502793297,"For the formal proof of Theorem 3.3, refer to Appendix A.1. While the results hold for O(t) querying
strategies, an open direction is to investigate algorithms with a broader range of querying strategies."
LITTLESTONE CLASSES ARE LEARNABLE,0.2849162011173184,"4
Blind-Prediction Setting: Learning from Discrete and Continuous Data
Streams"
LITTLESTONE CLASSES ARE LEARNABLE,0.2877094972067039,"4.1
It’s Impossible to Learn Non-Trivial Concept Classes from Continuous Data Streams"
LITTLESTONE CLASSES ARE LEARNABLE,0.2905027932960894,"In this section, we discover what constitutes learnability of multi-class concept classes in the blind-
prediction setting. We borrow Definition 3.1 to describe the learnability of a concept class H."
LITTLESTONE CLASSES ARE LEARNABLE,0.29329608938547486,"Since the blind-prediction setting is a harder variant of the update-and-deploy setting, we frame a
similar question asking if the Littlestone dimension is the right characterization of learnability."
LITTLESTONE CLASSES ARE LEARNABLE,0.29608938547486036,"Question: What characterizes the learnability of concept classes H in the blind-prediction setting?
Does LD(H) play a pertinent role?"
LITTLESTONE CLASSES ARE LEARNABLE,0.2988826815642458,"To answer this question, we come up with a simple concept class H that proves to be unlearnable in
the blind-prediction setting. This result comes in stark contrast to the results found in Section 3.1.
Below, we detail Theorem 4.1 and Corollary 4.2.
Theorem 4.1. Let H = {h} and X = {x1, x2} with h(x1) = 0 and h(x2) = 1. Then, for any
learning algorithm A with a linear querying strategy, MBP(H)(A) = ∞so H is not learnable
under the blind-prediction setting."
LITTLESTONE CLASSES ARE LEARNABLE,0.3016759776536313,"For the formal proof of Theorem 4.1, refer to Appendix A.2."
LITTLESTONE CLASSES ARE LEARNABLE,0.30446927374301674,"Corollary 4.2. If H is a concept class such that ∃h ∈H and ∃x1, x2 ∈X such that h(x1) ̸= h(x2),
then H is unlearnable in the blind-prediction setting."
LITTLESTONE CLASSES ARE LEARNABLE,0.30726256983240224,"Proof. Let H′ = h and X ′ = {x1, x2}. From Theorem 4.1, it was shown that MBP(H′)(A) = ∞
for any learning algorithm A with a linear querying strategy so H′ is unlearnable. Since H′ ⊆H
and x1, x2 ∈X, it follows that MBP(H) = ∞so H is unlearnable."
LITTLESTONE CLASSES ARE LEARNABLE,0.3100558659217877,"4.2
Are Adaptive Learners Required for Pattern Classes?"
LITTLESTONE CLASSES ARE LEARNABLE,0.3128491620111732,"In this section, we demonstrate the necessity of adaptive learning algorithms for effectively learning
pattern classes. Since concept classes represent a set of functions, and functions can be thought as
established input-output pairs, different permutations of these pairs don’t result in different functions
being realizable on the sequence. As a result, non-adaptive learning algorithms are sufficient in
learning concept classes but adaptive learning strategies may be required for pattern classes. Below,
we construct an example of a continuous pattern class P that is only learnable by any adaptive
sampling algorithm."
LITTLESTONE CLASSES ARE LEARNABLE,0.31564245810055863,"Pattern Class Example
Let H be a multi-class concept class with LD(H) = ∞. For t1, t2 ∈N
with t2 > t1, define ¯P(H, t1, t2) = {(Xt, Yt)t∈(t1,t2) : ∃h ∈H, ∀t ∈(t1, t2), h(Xt) =
Yt}.
Then, P(H, t1, t2) = {(Xt, Yt)t∈[t1,t2) : ∃P
∈
¯P(H, t1, t2) such that (Xt1, Yt1) =
(P, t2) and (Xt, Yt)t∈(t1,t2) = P}. ¯P(H, t1, t2) corresponds to the set of realizable data streams
between t1 and t2 and P(H, t1, t2) ensures that at time t1 the data stream encodes the entire
pattern from t1 to t2 in Xt1. Let N = {n ∈{0} × N∞: ∀i ∈N, n(i + 1) > n(i)} and"
LITTLESTONE CLASSES ARE LEARNABLE,0.31843575418994413,"Q = {q ∈{0} × Q∞
>0 : ∃n ∈N, ∀i ∈N, n(i) < q(i + 1) < n(i + 1)}. Then, we define the
continuous pattern class P in the following way: P = S"
LITTLESTONE CLASSES ARE LEARNABLE,0.32122905027932963,"q∈Q (Q∞
i=1 P(H, q(i), q(i + 1))) where
Q∞
i=1 P(H, q(i), q(i + 1)) represents an infinite Cartesian product among the valid patterns in each
interval dictated by q.
Lemma 4.3. For the update-and-deploy setting, any random sampling algorithm A with a linear
querying strategy QA(t) has MBP(A) = ∞."
LITTLESTONE CLASSES ARE LEARNABLE,0.3240223463687151,"Proof. Let A be a random sampling algorithm with a linear querying strategy QA(t). We will now
construct a continuous data stream (Zt)t≥0 that is realizable with respect to P in a randomized
fashion and prove a bound on the minimum expected error. Randomly select a vector q ∈Q."
LITTLESTONE CLASSES ARE LEARNABLE,0.3268156424581006,"To construct such a continuous process (Zt)t≥0, we first decompose R≥0 = ∪∞
n=1[2q(n), 2q(n+1)).
The idea behind this decomposition is to construct a pattern on each interval that corresponds
to a randomly chosen h ∈H. To do this, we take each interval [2q(n), 2q(n + 1)), letting
QA(2q(n + 1)) = k for some k ∈N, and break it further down such that [2q(n), 2q(n + 1)) ="
LITTLESTONE CLASSES ARE LEARNABLE,0.329608938547486,"∪2k
j=1
h
(q(n+1)−q(n))"
LITTLESTONE CLASSES ARE LEARNABLE,0.3324022346368715,"k
(j −1) + 2q(n), (q(n+1)−q(n))"
LITTLESTONE CLASSES ARE LEARNABLE,0.33519553072625696,"k
j + 2q(n)

. By doing this, we can take an ar-
bitrary root-to-leaf path from a Littlestone tree of depth 2k, and then paint each sub-interval with
an instance-label pair on this path. Since the number of sub-intervals is greater than the number of
queries made by the algorithm A, on some set of sub-intervals the algorithm A is forced to guess the
true label."
LITTLESTONE CLASSES ARE LEARNABLE,0.33798882681564246,"As described above, assume the interval [2q(n), 2q(n+1)) for some n ∈N, letting QA(2q(n+1)) =
k for some k ∈N.
Since LD(H) = ∞, there must exist a Littlestone tree T where the
minimum root-to-leaf depth is at least 2k.
Then, let σ = {(X1, Y1), ..., (X2k, Y2k)} corre-
spond to a randomly chosen root-to-leaf path. For each j ∈{1, ..., 2k}, populate the interval
h
(q(n+1)−q(n))"
LITTLESTONE CLASSES ARE LEARNABLE,0.3407821229050279,"k
(j −1) + 2q(n), (q(n+1)−q(n))"
LITTLESTONE CLASSES ARE LEARNABLE,0.3435754189944134,"k
j + 2q(n)

with the pair (Xj, Yj).
For simplic-"
LITTLESTONE CLASSES ARE LEARNABLE,0.3463687150837989,"ity, let Ij =
h
(q(n+1)−q(n))"
LITTLESTONE CLASSES ARE LEARNABLE,0.34916201117318435,"k
(j −1) + 2q(n), (q(n+1)−q(n))"
LITTLESTONE CLASSES ARE LEARNABLE,0.35195530726256985,"k
j + 2q(n)

. For the process at time"
LITTLESTONE CLASSES ARE LEARNABLE,0.3547486033519553,"t = 2q(n), let Zt = (P, 2q(n + 1)) where P = (X1, Y1)t∈I1\2q(n) ∪S2k
j=2[Xj, Yj)t∈Ij."
LITTLESTONE CLASSES ARE LEARNABLE,0.3575418994413408,"It’s important to note that the constructed continuous process on the interval [2q(n), 2q(n + 1)) lies
in P(H, 2q(n), 2q(n + 1)). The first point in the interval corresponds to the point (P, 2q(n + 1))
and P ∈¯P(H, 2q(n), 2q(n + 1)) since it was generated from a root-to-leaf path in T which is
realizable by some h ∈H."
LITTLESTONE CLASSES ARE LEARNABLE,0.36033519553072624,"Now, we show that on the sub-intervals A does not query in the interval [2q(n), 2q(n + 1)), A, the
minimum expected error is equal to (q(n+1)−q(n))"
LITTLESTONE CLASSES ARE LEARNABLE,0.36312849162011174,"k
. The analysis closely mirrors that of in Theorem
3.3. Let the jth sub-interval be a sub-interval, where 1 ≤j ≤2k, where A does not query. Let
E1 = {t ∈Ij : A(Xt) ̸= Yj} and E0 = {t ∈Ij : A(Xt) ̸= Y ′
j } where Y ′
j is the other label in tree T"
LITTLESTONE CLASSES ARE LEARNABLE,0.3659217877094972,"for the point Xj. Then, E
hR"
LITTLESTONE CLASSES ARE LEARNABLE,0.3687150837988827,"Ij 1[A(Xt) ̸= Yt] dt
i
= E

µ(E1)1[Yt = Yj] + µ(E0)1[Yt = Y ′
j ]

="
LITTLESTONE CLASSES ARE LEARNABLE,0.3715083798882682,"E[µ(E1)]E[1[Yt = Yj]] + E[µ(E0)]E[1[Yt = Y ′
j ]] where µ is the Lebesgue measure. Since a
random branch was chosen within the tree T, there was an equal chance of selecting Yt = Yj or
Yt = Y ′
j , then E[µ(E1)]E[1[Yt = Yj]] + E[µ(E0)]E[1[Yt = Y ′
j ]] = E[µ(E0)]/2 + E[µ(E1)]/2 ="
LITTLESTONE CLASSES ARE LEARNABLE,0.3743016759776536,E[µ(E0) + µ(E1)]/2 ≥(q(n+1)−q(n))
LITTLESTONE CLASSES ARE LEARNABLE,0.3770949720670391,"k
. Therefore, the learner A accumulates an expected error of
(q(n+1)−q(n))"
LITTLESTONE CLASSES ARE LEARNABLE,0.37988826815642457,"k
on each interval it doesn’t query. Since the learner has only k queries, it can only query
in at most k of the 2k intervals. At minimum there will exist k intervals that haven’t been queried by
the learner. Let I1, ..., Ik represent k of these intervals algorithm A does not query. It follows that
E
hR 2q(n+1)
2q(n)
1[A(Xt) ̸= Yt] dt
i
≥Pk
i=1 E
hR"
LITTLESTONE CLASSES ARE LEARNABLE,0.38268156424581007,"Ii 1[A(Xt) ̸= Yt] dt
i
= q(n + 1) −q(n)."
LITTLESTONE CLASSES ARE LEARNABLE,0.3854748603351955,"Since n ∈N was chosen arbitrarily, then it holds for all intervals [2q(n), 2q(n + 1)). As a result,"
LITTLESTONE CLASSES ARE LEARNABLE,0.388268156424581,"lim
T →∞E ""Z T"
LITTLESTONE CLASSES ARE LEARNABLE,0.39106145251396646,"0
1[A(Xt) ̸= Yt] dt #"
LITTLESTONE CLASSES ARE LEARNABLE,0.39385474860335196,"≥lim
T →∞"
LITTLESTONE CLASSES ARE LEARNABLE,0.39664804469273746,"max{n∈N:T ≥2q(n+1)}
X i=1
E"
LITTLESTONE CLASSES ARE LEARNABLE,0.3994413407821229,"""Z 2q(i+1)"
LITTLESTONE CLASSES ARE LEARNABLE,0.4022346368715084,"2q(i)
1[A(Xt) ̸= Yt] dt #"
LITTLESTONE CLASSES ARE LEARNABLE,0.40502793296089384,"= lim
T →∞"
LITTLESTONE CLASSES ARE LEARNABLE,0.40782122905027934,"max{n∈N:T ≥2q(n+1)}
X"
LITTLESTONE CLASSES ARE LEARNABLE,0.4106145251396648,"i=1
q(i + 1) −q(i) = ∞."
LITTLESTONE CLASSES ARE LEARNABLE,0.4134078212290503,"Since we constructed the process (Zt)t≥0 by randomly selecting branches from Littlestone trees
for each interval, we appeal to the probabilistic method to show that there exists a fixed choice of a
continuous-process (Zt)t≥0 such that MBP(A, (Zt)t≥0) = ∞. Therefore, MBP(A) = ∞."
LITTLESTONE CLASSES ARE LEARNABLE,0.41620111731843573,"Remark 4.4. It can be shown that the results of Lemma 4.3 directly extend for the blind-prediction
setting."
LITTLESTONE CLASSES ARE LEARNABLE,0.41899441340782123,"Now, we turn to an adaptive sampling learning algorithm, specifically Algorithm 2, that achieves
MBP(Algorithm 2) = 0. Specifically, we show this result in the blind-prediction setting. As will
be proven in the analysis of Lemma 4.5, Algorithm 2 specifically queries at the timestamps where a
portion of the future continuous data stream is revealed. As a result, Algorithm 2 makes at most a
countable number of mistakes because only a countable number of such points exist in any continuous
data stream realizable by P so it has an expected error of 0 with a linear querying strategy."
LITTLESTONE CLASSES ARE LEARNABLE,0.42178770949720673,Algorithm 2 Adaptive Sampler(P)
LITTLESTONE CLASSES ARE LEARNABLE,0.4245810055865922,"1: t = time {starts at t = 0}, tq = 0, initialize ˆf to be some function ˆf : R≥0 →Y
2: while true do
3:
Predict ˆf(t)
4:
if t = tq then
5:
Query and receive point-label pair (Xt, Yt) = (P, n)
6:
Update ˆf(t) = Yt for all (Xt, Yt) ∈P
7:
tq ←n
8:
end if
9: end while"
LITTLESTONE CLASSES ARE LEARNABLE,0.4273743016759777,"Lemma 4.5. Let A be the adaptive sampler in Algorithm 2. Then, the querying strategy QA(t) ≤t
and MBP(A) = 0 in the blind-prediction setting."
LITTLESTONE CLASSES ARE LEARNABLE,0.4301675977653631,"Proof (Sketch). Let A represent Algorithm 2 and (Zt)t≥0 = P ∈P be any adversarially chosen data
stream. Since each q ∈Q has q(1) = 0, then for any P ∈P, it must be the case that Z0 = (X0, Y0)
where X0 reveals the full sequence until time Y0. Algorithm 2 has its first query at time t = 0
and fits the predictor ˆf to output the labels of sequence X0 for all time t ∈(0, Y0). Since (Zt)t>0
follows the exact sequence described by X0 until time t = Y0, then
R Y0
0
1[A(Xt) ̸= Yt] dt = 0
implying that E[
R Y0
0
1[A(Xt) ̸= Yt] dt] = 0. At time t = Y0, A queries at exactly the right time to
gain information about a future portion of the data stream (Zt)t≥0 with the same analysis repeating
continuously. As a result, MBP(A, (Zt)t≥0) = 0 and since (Zt)t≥0 was arbitrarily chosen, then
MBP(A) = 0 with QA(t) ≤t."
LITTLESTONE CLASSES ARE LEARNABLE,0.4329608938547486,Remark 4.6. The results of Lemma 4.5 can also be extended to the update-and-deploy setting.
LEARNING PATTERN CLASSES FROM DISCRETE DATA STREAMS,0.43575418994413406,"4.3
Learning Pattern Classes from Discrete Data Streams"
LEARNING PATTERN CLASSES FROM DISCRETE DATA STREAMS,0.43854748603351956,"As witnessed in the example from Section 4.2, one can construct a rather complex pattern class to
model almost any sort of structure. While the power of pattern classes is inherent in their ability to
express complicated relationships, directly analyzing their behavior under continuous data streams
without a foundational understanding can prove to be an intractable problem."
LEARNING PATTERN CLASSES FROM DISCRETE DATA STREAMS,0.441340782122905,"As a result, we initiate a study of pattern classes under discrete data streams to provides a foundational
understanding of how learning algorithms handle data arriving in distinct, separate chunks. This
framework simplifies the complexity by allowing us to focus on key principles of sequential decision-
making such as incremental learning. By developing a theory in a discrete context, we can potentially
employ these insights that can prove to be crucial for tackling the more complex scenarios of learning
under continuous data streams. In Appendix B, we develop a complete theory on realizable learning
of pattern classes in the blind-prediction setting under discrete streams."
REFERENCES,0.4441340782122905,References
REFERENCES,0.44692737430167595,"[1] Shay Moran, Ohad Sharon, Iska Tsubari, and Sivan Yosebashvili. List online classification. In
The Thirty Sixth Annual Conference on Learning Theory, pages 1885–1913. PMLR, 2023."
REFERENCES,0.44972067039106145,"[2] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold
algorithm. Machine Learning, 2:285–318, 1988."
REFERENCES,0.45251396648044695,"[3] A. Daniely, S. Sabato, S. Ben-David, and S. Shalev-Shwartz. Multiclass learnability and the
ERM principle. Journal of Machine Learning Research, 16(12):2377–2404, 2015."
REFERENCES,0.4553072625698324,"[4] Pramith Devulapalli and Steve Hanneke. The dimension of self-directed learning. In Interna-
tional Conference on Algorithmic Learning Theory, pages 544–573. PMLR, 2024."
REFERENCES,0.4581005586592179,"[5] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Machine learning, 47:235–256, 2002."
REFERENCES,0.46089385474860334,"[6] Gergely Neu and Gábor Bartók. An efficient algorithm for learning with semi-bandit feedback.
In International Conference on Algorithmic Learning Theory, pages 234–248. Springer, 2013."
REFERENCES,0.46368715083798884,"[7] Gábor Bartók and Csaba Szepesvári. Partial monitoring with side information. In International
Conference on Algorithmic Learning Theory, pages 305–319. Springer, 2012."
REFERENCES,0.4664804469273743,"[8] Gábor Bartók, Dean P Foster, Dávid Pál, Alexander Rakhlin, and Csaba Szepesvári. Partial
monitoring—classification, regret bounds, and algorithms. Mathematics of Operations Research,
39(4):967–997, 2014."
REFERENCES,0.4692737430167598,"[9] Noga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback
graphs: Beyond bandits. In Conference on Learning Theory, pages 23–35. PMLR, 2015."
REFERENCES,0.4720670391061452,"[10] Tor Lattimore and Csaba Szepesvári. An information-theoretic approach to minimax regret in
partial monitoring. In Conference on Learning Theory, pages 2111–2139. PMLR, 2019."
REFERENCES,0.4748603351955307,"[11] Tor Lattimore. Minimax regret for partial monitoring: Infinite outcomes and rustichini’s regret.
In Conference on Learning Theory, pages 1547–1575. PMLR, 2022."
REFERENCES,0.4776536312849162,"[12] Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. Selective sampling using the
query by committee algorithm. Machine Learning, 28:133–168, 1997."
REFERENCES,0.48044692737430167,"[13] S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In
Advances in Neural Information Processing Systems 20, 2007."
REFERENCES,0.48324022346368717,"[14] Liu Yang. Active learning with a drifting distribution. Advances in Neural Information
Processing Systems, 24, 2011."
REFERENCES,0.4860335195530726,"[15] T.-K. Huang, A. Agarwal, D. J. Hsu, J. Langford, and R. E. Schapire. Efficient and parsimonious
agnostic active learning. In Advances in Neural Information Processing Systems 28, 2015."
REFERENCES,0.4888268156424581,"A
Proofs for Learning from Continuous Data Streams"
REFERENCES,0.49162011173184356,"A.1
Proof of Theorem 3.3"
REFERENCES,0.49441340782122906,"Proof. Let A be a learning algorithm with a linear querying strategy QA(t). Assume some concept
class H with LD(H) = ∞. For every n ∈N, we show that there exists an adversarially con-
structed data stream, (Zt)t≥0, such that MBP(H)(A, (Zt)t≥0) ≥n. Since this holds ∀n ∈N, then
MBP(H)(A) = sup(Zt)t≥0 MBP(H)(A, (Zt)t≥0) = ∞."
REFERENCES,0.4972067039106145,"Our learning model assumes an oblivious adversary, so we will construct a continuous data stream
(Zt)t≥0 beforehand that is realizable with respect to H. However, we construct (Zt)t≥0 in a
randomized fashion and bound the minimum expected error of this randomly constructed process.
Then, at the end of the proof, we will call upon the probabilistic method to show that there exists a
continuous process achieving at least that expected error."
REFERENCES,0.5,"Take some n ∈N and let QA(4n) = k. Since LD(H) = ∞, then there must exist a Littlestone tree
T where the minimum root-to-leaf path of T is at least 2k. Consider a random walk in T starting at the
root node that picks with probability 1/2 the left child or the right child and descends level-by-level
until it reaches a leaf node. Let σ = {(x1, y1), ..., (x2k, y2k)} correspond to the root-to-leaf path
produced by the random walk on T. Note that Hσ = {h ∈H : ∀(Xi, Yi) ∈σ, h(Xi) = Yi}, the
subset of the concept class consistent with the sequence σ, will have at least one classifier due to the
guarantee provided by the Littlestone tree that every branch in T is realizable by some h ∈H."
REFERENCES,0.5027932960893855,"Now, we describe the construction of the continuous process (Zt)t≥0 using the sequence σ. De-
compose [0, 4n) in the following way: [0, 4n) = S2k
j=1
 2n"
REFERENCES,0.505586592178771,"k (j −1), 2n"
REFERENCES,0.5083798882681564,"k j

. For the jth interval
where 1 ≤j ≤2k, ∀t ∈[ 2n"
REFERENCES,0.5111731843575419,"k (j −1), 2n"
REFERENCES,0.5139664804469274,"k j), define Zt = (Xj, Yj) = σ(j). On the time interval
[0, 4n), if one segments the process (Zt)t≥0 into intervals of size 2n"
REFERENCES,0.5167597765363129,"k , then for the jth interval, where
1 ≤j ≤2k, the process contains the point σ(j) for the entirety of the interval. For t ≥4n, then
define Zt = (Xt, Yt) to be a point-label pair (X′, Y ′) such that for each h ∈Hσ, h(X′) = Y ′."
REFERENCES,0.5195530726256983,"Now, we show that on the intervals A does not query in the time period [0, 4n), the minimum expected
error is equal to n"
REFERENCES,0.5223463687150838,"k . Let the jth interval be an interval, where 1 ≤j ≤2k, where A does not query.
Let E1 = {t ∈[ 2n"
REFERENCES,0.5251396648044693,"k (j −1), 2n"
REFERENCES,0.5279329608938548,k j] : A(Xt) ̸= Yj} and E0 = {t ∈[ 2n
REFERENCES,0.5307262569832403,"k (j −1), 2n"
REFERENCES,0.5335195530726257,"k j] : A(Xt) ̸= Y ′
j }"
REFERENCES,0.5363128491620112,"where Y ′
j is the other label in tree T for the point Xj. Then, E
hR 2n"
REFERENCES,0.5391061452513967,"k i
2n"
REFERENCES,0.5418994413407822,"k (i−1) 1[A(Xt) ̸= Yt] dt
i
="
REFERENCES,0.5446927374301676,"E

µ(E1)1[Yt = Yj] + µ(E0)1[Yt = Y ′
j ]

= E[µ(E1)]E[1[Yt = Yj]] + E[µ(E0)]E[1[Yt = Y ′
j ]]
where µ is the Lebesgue measure. Since a random branch was chosen within the tree T, there was an
equal chance of selecting Yt = Yj or Yt = Y ′
j , then E[µ(E1)]E[1[Yt = Yj]] + E[µ(E0)]E[1[Yt =
Y ′
j ]] = E[µ(E0)]/2 + E[µ(E1)]/2 = E[µ(E0) + µ(E1)]/2 ≥n/k. Therefore, the learner A
accumulates an expected error of n/k on each interval it doesn’t query. Since the learner has only k
queries, it can only query in at most k of the 2k intervals. At minimum there will exist k intervals
that haven’t been queried by the learner. Let I1, ..., Ik represent k of these intervals algorithm A does
not query. It follows that E
hR 4n
0
1[A(Xt) ̸= Yt] dt
i
≥Pk
i=1 E
hR"
REFERENCES,0.547486033519553,"Ii 1[A(Xt) ̸= Yt] dt
i
= n."
REFERENCES,0.5502793296089385,"The strategy chosen to prove a lower bound on the expected error to be n relied on generating
a continuous time process by randomly selecting a branch within the Littlestone tree T which
corresponds to a random selection of a target concept. However, we appeal to the probabilistic
method to show that if the expected error for algorithm A is at least n, then there exists a fixed
choice of a continuous-process (Zt)t≥0 such that MBP(H)(A, (Zt)t≥0) ≥n. Since we show that
for every n ∈N and any learning algorithm A there exists an adversarial strategy (Zt)t≥0 such that
MBP(H)(A, (Zt)t≥0) ≥n, then the adversary can force the learner to make an arbitrarily large error
implying that MBP(H)(A) = sup(Zt)t≥0 MBP(H)(A, (Zt)t≥0) = ∞so H is not learnable."
REFERENCES,0.553072625698324,"A.2
Proof of Theorem 4.1"
REFERENCES,0.5558659217877095,"Proof. The essence of this proof lies in the simple yet effective scheme the adversary can employ to
force any learning algorithm A with a linear querying strategy QA(t) to have MBP(H)(A) = ∞.
The idea behind this adversarial approach is to divide the timeline, R≥0, into small enough intervals"
REFERENCES,0.5586592178770949,"where each interval is populated randomly with point-label pair (x1, 0) or (x2, 1) such that A is
forced to guess the right label."
REFERENCES,0.5614525139664804,"We first describe the construction of the continuous data stream (Zt)t≥0 realizable with respect to
h. As previously done in Theorem 3.3, we use a randomized approach in constructing (Zt)t≥0 to
prove a bound on the expected error. This randomized approach draws upon a family of continuous
processes to show that the expected error is some minimum value. However, the mistake-bounds
require a singular continuous process to yield that error. So, we apply the probabilistic method to
prove the existence of a continuous data stream that can be fixed beforehand that achieves at least
that expected error."
REFERENCES,0.5642458100558659,"We first describe the construction of (Zt)t≥0 in a randomized fashion. Decompose R≥0 = ∪∞
n=1[n −
1, n). For every n ∈N, let kn = QA(n). Then, split [n−1, n) into 2kn intervals each of size
1
2kn such"
REFERENCES,0.5670391061452514,"that [n −1, n) = S2kn
j=1
h
n −1 +
1
2kn (j −1), n −1 +
1
2kn j

. Let σ ∼Unif({(x1, 0), (x2, 1)}2kn)"
REFERENCES,0.5698324022346368,"be a sequence sampled uniformly from the space {(x1, 0), (x2, 1)}2kn. Then, construct the process
(Zt)t≥0 such that ∀n ∈N, σ ∼Unif({(x1, 0), (x2, 1)}2kn), ∀j ∈{1, ..., 2kn}, ∀t ∈[n −1 +
1
2kn (j −1), n −1 +
1
2kn j) then Zt = (Xj, Yj) = σ(j). Essentially, we assign the point-label pairs
(x1, 0) and (x2, 1) randomly to each interval to construct the process."
REFERENCES,0.5726256983240223,"Letting n ∈N, then algorithm A makes at most kn queries in the interval [n −1, n) implying at least
kn of the intervals within [n −1, n) pass by the learner with no query. On the intervals the learner
does not query, we show that the learner’s expected error is equal to
1
4kn . For some 1 ≤j ≤2kn,
let the jth interval within [n −1, n) contain no queries from A. Then, let E0 and E1 represent the
portion of the jth interval that A predicts as a 0 or 1 respectively. It follows that the expected error
of algorithm A on this interval is equivalent to E[µ(E1)1[Yj = 0] + µ(E0)1[Yj = 1]] where Yj is
the true label for the jth interval and µ is the Lebesgue measure. Since there’s an equal probability
of Yj = 0 or Yj = 1, the expected error E[µ(E1)1[Yj = 0] + µ(E0)1[Yj = 1]] =
1
4kn . There are
at least kn such intervals where A does not query on [n −1, n) which implies that the minimum
expected error is equivalent to 1 4."
REFERENCES,0.5754189944134078,"Since we decomposed R≥0 = S∞
n=1[n −1, n), then limT →∞E
hR T
0 1[A(Xt) ̸= Yt] dt
i
≥"
REFERENCES,0.5782122905027933,"limT →∞
P⌊T ⌋
n=1 E
hR n
n−1 1[A(Xt) ̸= Yt] dt
i
≥limT →∞
P⌊T ⌋
n=1
1
4 = ∞. We used a randomized
method to construct a family of continuous processes such that the expected error of a randomly cho-
sen process reaches ∞. Applying the probabilistic method, there exists a continuous process whose
expected error also reaches ∞. Therefore, MBP(H)(A) = sup(Zt)t≥0 MBP(H)(A, (Zt)t≥0) = ∞
for any learning algorithm A with a linear querying strategy QA(t) so H is unlearnable."
REFERENCES,0.5810055865921788,"B
Learnability of Pattern Classes from Discrete Data Streams"
REFERENCES,0.5837988826815642,"B.1
Query-based Feedback Online Learning"
REFERENCES,0.5865921787709497,"In this section, we are interested in developing a theory of realizable learning of pattern classes under
discrete data streams in the blind-prediction setting. While a general theory of pattern classes under
discrete data streams would involve considering the agnostic case as well, we focus on the simplest
scenario which is realizable learning under a binary label space Y = {0, 1} assuming deterministic
learning algorithms. While this learning setting might seem quite restrictive, no such theory exists for
the learnability of general pattern classes so we provide the first results in this space. We also develop
this theory in the blind-prediction setting and a future direction of this work would be to characterize
it in the update-and-deploy setting."
REFERENCES,0.5893854748603352,"In the blind-prediction setting, assume a non-empty discrete pattern class P and some budget of
queries Q. Assume a deterministic learning algorithm. One full round in this setting occurs in the
following fashion at every t ∈N:"
REFERENCES,0.5921787709497207,1. The learner makes a prediction ˆYt ∈Y and decides to query or not.
REFERENCES,0.5949720670391061,"2. The learner reveals ˆYt.
3. The adversary selects the true label Yt."
REFERENCES,0.5977653631284916,"4. If the learner does query, then the adversary reveals (Xt, Yt) to the learner."
REFERENCES,0.6005586592178771,"The primary constraints governing this setting are realizability with respect to P. Letting (Zt)∞
t=1 =
(Xt, Yt)∞
t=1 be the sequence of data points and true labels, then (Zt)∞
t=1 ∈P to be considered
realizable. It’s important to note that the learner at any given time t makes a prediction ˆYt based
solely on the current timestamp and history of previous queries."
REFERENCES,0.6033519553072626,"B.2
Query-based Mistake-Bounds"
REFERENCES,0.6061452513966481,"We turn to mistake-bounds to effectively capture the minimum number of mistakes an optimal
deterministic learning algorithm will make."
REFERENCES,0.6089385474860335,"The number of mistakes a deterministic learning algorithm A makes given a target pattern/discrete
data stream P ∗= (Zt)∞
t=1 ∈P and a budget of Q queries is denoted as MQ(A, P ∗). Formally,
MQ(A, P ∗) can be understood as"
REFERENCES,0.611731843575419,"MQ(A, P ∗) = ∞
X"
REFERENCES,0.6145251396648045,"i=1
1[A(Xt) ̸= Yt]."
REFERENCES,0.61731843575419,"To consider the mistake-bound of the learning algorithm A on P given a budget of Q queries, we get"
REFERENCES,0.6201117318435754,"MQ(A, P) = sup
P ∗∈P
MQ(A, P ∗)."
REFERENCES,0.6229050279329609,"Finally, to obtain the optimal mistake-bound on P given Q queries, we take the infimum over all
deterministic learning algorithms."
REFERENCES,0.6256983240223464,"MQ(P) = inf
A MQ(A, P)"
REFERENCES,0.6284916201117319,"B.2.1
Query Trees"
REFERENCES,0.6312849162011173,"In this section, we describe a certain type of tree, the query tree, which we will be used to capture
the learning framework explained in Section B.1. Given the budget of queries Q as an input, the
query tree can be used to depict the evolution of the game in the blind-prediction setting. The name
query tree comes from the fact that these trees describe the evolution of the game from the learner’s
perspective through queries. As a result, these trees are used in conjunction to provide upper and
lower bounds on the optimal mistake-bounds for deterministic learning algorithms.
Definition B.1 (Query Tree). A query tree T is defined as a tuple (V, E, Q) where V is a collection
of nodes, E is a collection of edges, and Q is the query budget."
REFERENCES,0.6340782122905028,"• T is a rooted binary tree where each node has at most two children which are referred to as
the left child and the right child."
REFERENCES,0.6368715083798883,"• Each node Vj ∈V corresponds to some timestamp ti where V t
j = ti."
REFERENCES,0.6396648044692738,"• The root node is represented by V1 ∈V and corresponds to V t
1 = t1 where t1 is the
timestamp of the first query."
REFERENCES,0.6424581005586593,"• ∀V ∈V, if V ′ = Parent(V ), then V
′t < V t."
REFERENCES,0.6452513966480447,"• ∀e ∈E where e = (V ′, V ) with V ′ = Parent(V ), then the edge weight is defined as
ω(e) = y with y = 0 if V = LeftChild(V ′) or y = 1 if V = RightChild(V ′)."
REFERENCES,0.6480446927374302,"• Every root-to-leaf path (V1, ..., Vn) has n = Q + 1 nodes."
REFERENCES,0.6508379888268156,"B.2.2
Query Learning Distance - Blind-Prediction Setting"
REFERENCES,0.6536312849162011,"In this section, we describe a dimension based on a family of query trees that correctly characterizes
the complexity of learning pattern classes in the blind-prediction setting. In the following subsections,
we first tackle the case when Q = 0 and then characterize the general setting for Q > 0."
REFERENCES,0.6564245810055865,"Special Variant: Blind Learning
What if the learner wasn’t allowed to even query once? How
would this affect the number of mistakes the learner would make? We call this special setting
the blind learning scenario since the learner receives absolutely no feedback on any round of the
game; only the current time-step is given as input. While Section B.1 describes the exact procedure
round-by-round, a closer look at the intricacies of this scenario can reduce the game into a simple
two-step procedure. The learner is not allowed to query a single time; so this implies that the learner
cannot use information about the sequence itself to update its algorithm. Additionally, since the
learner is a deterministic learning algorithm, this implies that an all-knowing adversary has complete
knowledge about the learner’s prediction at every timestamp. Combining these two facts together, it
follows that the learner’s predictions are independent of the true labels and the adversary can simply
select the sequence of true labels all at once. Formally speaking, the entire game can be described in
the following two steps:"
REFERENCES,0.659217877094972,"1. The learner selects a prediction vector ˆy ∈{0, 1}∞."
REFERENCES,0.6620111731843575,"2. The adversary selects the true outcome vector y ∈{0, 1}∞."
REFERENCES,0.664804469273743,"Then, the number of mistakes is equivalent to P∞
i=1 1[ˆy(i) ̸= y(i)] = |ˆy −y| where | · | stands for
the L1-norm. As is consistent with the framework described in Section B.1, the vector y must be
realizable with respect to the pattern class P such that ∃P ∈P where ∀(Xt, Yt) ∈P, Yt = y(t).
Below, we present an important lemma that characterizes the optimal mistake-bound M0(P)."
REFERENCES,0.6675977653631285,"Lemma B.2. If the number of queries Q = 0, then"
REFERENCES,0.6703910614525139,"M0(P) = BlindLearningDimension(P) =
inf
ˆy∈{0,1}∞sup
y∈Py |ˆy −y|"
REFERENCES,0.6731843575418994,"where | · | represents the L1-distance and Py = {y ∈{0, 1}∞: ∃P ∈P s.t. ∀(Xt, Yt) ∈P, Yt =
y(t)} which represents the set of infinite binary vectors that are realizable with respect to P."
REFERENCES,0.6759776536312849,"Proof. We divide this proof into two parts by devoting the first half to a lower bound proof showing
that BlindLearningDimension(P) ≤M0(P). The second half of the proof is devoted to showing
that there exists an algorithm A such that M0(A, P) = BlindLearningDimension(P). Finally, we
combine these two statements to ultimately show that BlindLearningDimension(P) = M0(P)."
REFERENCES,0.6787709497206704,"We first show the lower-bound proof by letting A be any deterministic learning algorithm. Then,
M0(A, P) is the mistake-bound of the learner A given the pattern P ∈P. Let y′ be the output
of A given no queries. This is equivalent to M0(A, P) = |y′ −y| where (x, y) = P. M0(A, P)
is the maximum mistake-bound of the learning algorithm A over the entire pattern class P. More
technically, we represent M0(A, P) = supy∈Py |y′ −y|. Since y′ ∈{0, 1}∞, it then holds that
inf ˆy∈{0,1}∞supy∈Py |ˆy −y| ≤supy∈Py |y′ −y|. It follows that BlindLearningDimension(P) ≤
M0(A, P).
Since A was an arbitrary deterministic learning algorithm, it then follows that
BlindLearningDimension(P) ≤M0(P)."
REFERENCES,0.6815642458100558,"For the upper bound proof we focus on the case when BlindLearningDimension(P) < ∞since
M0(P) = ∞when BlindLearningDimension(P) = ∞. Let A be a deterministic learning al-
gorithm that predicts the vector ˆy such that supy∈Py |ˆy −y| = inf ˆy∈{0,1}∞supy∈Py |ˆy −y|.
Since BlindLearningDimension(P) < ∞and y is a binary vector, then there exists a vector ˆy
achieving the minimum. It directly follows that M0(A, P) = BlindLearningDimension(P). Since
M0(P) ≤M0(A, P), then M0(P) ≤BlindLearningDimension(P). By combining the lower
bound and upper bound statements, we get the following inequality BlindLearningDimension(P) ≤
M0(P) ≤BlindLearningDimension(P) so M0(P) = BlindLearningDimension(P)."
REFERENCES,0.6843575418994413,"General Setting
We now define the dimension QLD or query learning distance on these family
of query trees T realizable with respect to the discrete pattern class P given Q queries. The QLD
quantity can be thought as analogous to the notion of rank of a binary tree but setup in a slightly
different fashion. Below, for each T ∈T , we describe the query learning distance."
REFERENCES,0.6871508379888268,"QLDT (P, Q, i) = 1[i = Q] · BlindLearningDimension(P) + 1[i < Q] "
REFERENCES,0.6899441340782123,"inf
ˆy∈{0,1}ti−(ti−1+1)"
REFERENCES,0.6927374301675978,"sup
xti∈X"
REFERENCES,0.6955307262569832,"y∈{0,1}ti−(ti−1+1)"
REFERENCES,0.6983240223463687,"|ˆy −y| · 1[P(⋆,y) ̸= ∅] +
max{T0, T1}
if T0 ̸= T1
T0 + 1
else ! (1)"
REFERENCES,0.7011173184357542,"where
T0 = QLDTL(P(⋆,y)(xti,0), Q, i + 1)
(2)"
REFERENCES,0.7039106145251397,"T1 = QLDTR(P(⋆,y)(xti,1), Q, i + 1).
(3)"
REFERENCES,0.7067039106145251,"In Eqs. 1, 2, and 3, TL is the left subtree of T, TR is the right subtree of T, ˆy is the sequence of
predictions, y is the sequence of true labels, xti is the instance at time ti, and P(⋆,y) = {P ∈P :
∀yt ∈y, P y(t) = yt} with P y(t) referring to the tth label of pattern P. Additionally, ti and ti−1
refer to the timestamps of the ith and i −1th queries respectively that correspond to the root-to-leaf
path dictated by the recursion."
REFERENCES,0.7094972067039106,"Defining QLD(P, Q)
Let T (P, Q) be the collection of all query trees for the predict-then-query
setting that are realizable with respect to P and contains Q query nodes on each branch. Then,
T k(P, Q) = {T ∈T (P, Q) : QLDT (P, Q, 0) = k} is the collection of trees whose query learning
distance is exactly k. Finally, we define QLD(P, Q) = inf{k ∈N ∪{0} : T k(P, Q) ̸= ∅}.
Lemma B.3. For any discrete pattern class P and query budget Q ∈N ∪{0}, QLD(P, Q) ≤
MQ(P)."
REFERENCES,0.7122905027932961,"Proof. Let A be any deterministic learning algorithm. A proof by induction will be established
on the pair (P, Q) taking Q = 0 to be the base. We refer to Lemma B.2 to show that ∀P′ ⊆
P, QLD(P′, 0) = M0(P′) ≤M0(A, P′). Now, we apply the inductive step on (P′, Q′) where
P′ ⊆P and Q′ < Q, then QLD(P′, Q′) ≤MQ′(A, P′). The rest of the proof is devoted to showing
that QLD(P, Q) ≤MQ(A, P) by describing an adversarial strategy that guarantees this bound."
REFERENCES,0.7150837988826816,"Let t1 ∈N be the first query timestamp made by the learning algorithm A. Since A is a deterministic
learner, the adversary has knowledge of t1. To narrow down its selection of the true labels for the
first t1 rounds, the adversary can select an optimal query tree T based on the value QLDT (P, Q, 0)
given that the root node has V t
1 = t1. Given that QLDT (P, Q, 0) follows the piece-wise function
described in Eq. 1, the adversary can select the larger of T0 or T1 (if equal, T0 is chosen). Without
loss of generality, let T0 be the subtree chosen by the adversary. For the first t1 −1 rounds, let the
adversary selects the optimal vector of true labels y given knowledge of the procedure of algorithm A.
Let ˆy represent the vector of predicted labels by the learner for the first t1 −1 rounds. At time t1, the
learner will present its prediction ˆyt1. The adversary can select xt1 that corresponds to the supremum
in Eq. 1 and set the true label yt1 = 0. In the special case that T0 = T1, then yt1 = 1 −ˆyt1."
REFERENCES,0.7178770949720671,"Then the number of mistakes made by the learner in the first t1 rounds is equivalent to |ˆy −y| +
1[yt1 ̸= ˆyt1]. On the remaining number of rounds, MQ−1(A, P(⋆,y)(xt1,yt1)) represents the optimal
mistake-bound of the learner A. Using the inductive step, we can show that |ˆy −y| + 1[yt1 ̸=
ˆyt1] + QLD(P(⋆,y)(xt1,yt1), Q −1) ≤|ˆy −y| + 1[yt1 ̸= ˆyt1] + MQ−1(A, P(⋆,y)(xt1,yt1)). Since
MQ(A, P) is calculated as the supremum over all adversarial approaches given the algorithm A, then
MQ(A, P) ≥|ˆy−y|+1[yt1 ̸= ˆyt1]+MQ−1(A, P(⋆,y)(yt1,yt1)). Now, we show that QLD(P, Q) ≤
|ˆy−y|+1[yt1 ̸= ˆyt1]+QLD(P(⋆,y)(xt1,yt1), Q−1). Assume that the predictions made by algorithm
A induce |ˆy−y|+1[yt1 ̸= ˆyt1]+QLD(P(⋆,y)(xt1,yt1), Q−1) < QLD(P, Q). Since the adversary’s
selection of true labels is an optimal label vector given the workings of learning algorithm A, then
the adversary’s decision aligns with the supremum in Eq. 1. Then, there must exist a tree T ′
whose largest distance is equal to that value with t1 being the timestamp of the root node. Formally
speaking, this implies the existence of T ′ such that QLDT ′(P, Q, 0) < QLD(P, Q). If such a tree
existed, then the adversary would have selected T ′ which violates the minimality of QLD(P, Q)
and the assumption that the adversary chose the most minimal tree satisfying V t
1 = t1. As a result,
QLD(P, Q) ≤|ˆy −y| + 1[yt1 ̸= ˆyt1] + QLD(P(⋆,y)(xt1,yt1), Q −1). Placing all the inequalities"
REFERENCES,0.7206703910614525,"together, we get QLD(P, Q) ≤|ˆy −y| + 1[yt1 ̸= ˆyt1] + QLD(P(⋆,y)(xt1,yt1), Q −1) ≤|ˆy −y| +
1[yt1 ̸= ˆyt1]+MQ−1(A, P(⋆,y)(xt1,yt1)) ≤MQ(A, P) which results in QLD(P, Q) ≤MQ(A, P).
Since A was an arbitrary learning algorithm, then it holds that QLD(P, Q) ≤MQ(P)."
REFERENCES,0.723463687150838,"In Algorithm 3 detailed below, we denote by y1 ◦y2 the vector obtained by concatenating vector y2
after vector y1. Additionally, the usage of T0 and T1 refer to Eqs. 2 and 3 respectively. If Tyt is used,
this implies a query subtree that was either the left child if yt = 0 or the right child if yt = 1."
REFERENCES,0.7262569832402235,"Algorithm 3 BP-SOA(P, Q)
Require: P ̸= ∅
Require: Q ≥0"
REFERENCES,0.729050279329609,"1: ˆxh = history of previously observed instances
2: ˆyl = list of current predictions
3: O = history of previous queries {Elements of O are (t, y) where t is time, y is label}
4: i = 0, ti = 1 {Initial query number and timestamp}
5: Select tree T such that QLD(P, Q, 0) = QLD(P, Q), ti = V t
1
6: Tend = ∞
7: for t = 1 to Tend do
8:
if t < ti then
9:
if t = 1 then
10:"
REFERENCES,0.7318435754189944,"ˆy
=
arg min
ˆy∈{0,1}ti−1
sup
y∈{0,1}ti−1 |ˆy −y| · 1[P(⋆,y)
̸=
∅] + QLD(P(⋆,y), Q)"
REFERENCES,0.7346368715083799,"11:
Append ˆy to ˆyl
12:
end if
13:
Predict ˆyl(t), add ⋆to ˆxh
14:
else if i < Q then
15:
ˆyt
=
arg maxr∈{0,1} supxti∈X sup
y∈{0,1}ti−1"
REFERENCES,0.7374301675977654,"∀(t,y)∈O,y(t)=y
|ˆyl −y|1[P(ˆxh,y)
̸=
∅] +"
REFERENCES,0.7402234636871509,"QLDTr(P(ˆxh,y)(xti,r), Q, i + 1)
16:
Predict ˆyt and add ˆyt to ˆyh
17:
Receive (xt, yt), add (t, yt) to O, and add xt to ˆxh
18:
Set i = i + 1, ti = V t
1 {V1 is the root node of Tyt}
19:"
REFERENCES,0.7430167597765364,"ˆy =
arg min
ˆy∈{0,1}ti−(ti−1+1) sup
xti∈X
sup
y∈{0,1}ti−1"
REFERENCES,0.7458100558659218,"∀(t,y)∈O,y(t)=y"
REFERENCES,0.7486033519553073,"|ˆyl◦ˆy−y|·1[P(ˆxh◦{⋆},y) ̸= ∅]+
max{T0, T1}
if T0 ̸= T1
T0 + 1
else"
REFERENCES,0.7513966480446927,"20:
Append ˆy to ˆyl
21:
else
22:
y′ = arg max
y∈{0,1}ti
∀(t,y)∈O,y(t)=y
|ˆy−y|·1[P(ˆxh,y) ̸= ∅]+BlindLearningDimension(P(ˆxh,y))"
REFERENCES,0.7541899441340782,"23:
Let ˆy be such that supy∈Py
(ˆxh,y′) |ˆy −y| = inf ˆy∈{0,1}∞supy∈Py
(ˆxh,y′) |ˆy −y|"
REFERENCES,0.7569832402234636,"24:
Append ˆy to ˆyl
25:
Set Tend = ti
26:
end if
27: end for
28: for t = ti to ∞do
29:
Predict ˆyl(t)
30: end for"
REFERENCES,0.7597765363128491,"Lemma B.4. For any discrete pattern class P and query budget Q ∈N ∪{0}, MQ(P) ≤
QLD(P, Q)."
REFERENCES,0.7625698324022346,"Proof. Let A be the BP-SOA which is detailed in Algorithm 3. A proof by induction will be
established on the pair (P, Q) taking Q = 0 to be the base case. Let P′ ⊆P. In the base
case, we execute Algorithm 3 with the inputs P′ and Q = 0. Since Q = 0, Algorithm 3 se-
lects the vector ˆy corresponding to the BlindLearningDimension(P′) and appends it to ˆyl (lines
22-24). Then, A skips to lines 28-30 making predictions according to ˆyl. Then, we refer to
Lemma B.2 to show that BlindLearningDimension(P′) ≤M0(P′) ≤M0(Algorithm 3, P′) ≤
BlindLearningDimension(P′) implying that M0(P′) = BlindLearningDimension(P′)."
REFERENCES,0.7653631284916201,"Now, we apply the inductive step on (P′, Q′) where P′ ⊆P and Q′ < Q, then MQ′(A, P′) ≤
QLD(P′, Q′). The rest of the proof is devoted to showing that MQ(A, P) ≤QLD(P, Q)."
REFERENCES,0.7681564245810056,"From Algorithm 3, we know that A selects the query tree T such that QLDT (P, Q, 0) = QLD(P, Q)
on line 5 with the first query timestamp t1 = V t
1 . For rounds t < t1 rounds, Algorithm 3 will select
its predictions ˆyt that minimizes the optimization expression in lines 10 and 19 based on the history
of previous queries. On round t1, Algorithm 3 selects ˆyt1 in line 15 based on the larger subtree, T0
or T1. Without loss of generality, assume that T0 is the larger subtree and in the case of a tie, T0 is
selected. Then, ˆyt1 = 0 and the learner receives (xt1, yt1) after querying."
REFERENCES,0.770949720670391,"It follows that the mistakes made by the learner on the first t1 rounds correspond to |y −ˆy| + 1[ˆyt1 ̸=
yt1] where y and yt1 represent the true labels selected by the adversary. Since the adversary is
operating under the constraint of realizability, then it must hold that P(⋆,y)(xt1,yt1) ̸= ∅where
⋆is a placeholder for any sequence of instances satisfying the constraint. From the inductive
step, it follows that |ˆy −y| + 1[ˆyt1 ̸= yt1] + MQ−1(A, P(⋆,y)(xt1,yt1)) ≤|ˆy −y| + 1[ˆyt1 ̸=
yt1] + QLD(P(⋆,y)(xt1,yt1), Q −1). Assume that the adversary’s choices of instances and true labels
on the first t1 rounds yield |ˆy −y| + 1[ˆyt1 ̸= yt1] + QLD(P(⋆,y)(xt1,yt1), Q −1) > QLD(P, Q).
Since A selected tree T, this implies that QLDT (P, Q, 0) = QLD(P, Q). Additionally, A always
selects the predictions that minimizes over the worst possible game outcomes (line 19 of Algorithm 3)
with the query prediction aligning with that of the larger subtree. As a result |ˆy −y| + 1[ˆyt1 ̸= yt1] +
QLDTL(P(⋆,y)(xt1,yt1), Q, 1) ≤QLD(P, Q). And by definition QLD(P(⋆,y)(xt1,yt1), Q −1) ≤
QLDTL(P(⋆,y)(xt1,yt1), Q, 1), so it must hold that |ˆy−y|+1[ˆyt1 ̸= yt1]+QLD(P(⋆,y)(xt1,yt1), Q−
1) ≤QLD(P, Q). As a result, |ˆy −y| + 1[ˆyt1 ̸= yt1] + MQ−1(A, P(⋆,y)(xt1,yt1)) ≤QLD(P, Q).
Since this inequality holds for any choice of y, xt1, and yt1, it follows that MQ(A, P) ≤QLD(P, Q).
Since MQ(P) ≤MQ(A, P), we show that MQ(P) ≤QLD(P, Q)."
REFERENCES,0.7737430167597765,NeurIPS Paper Checklist
CLAIMS,0.776536312849162,1. Claims
CLAIMS,0.7793296089385475,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In our abstract and in Section 1.3 of the introduction, we give a detailed
description of the claims and results we prove in the paper as a technical overview.
Guidelines:"
CLAIMS,0.7821229050279329,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.7849162011173184,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Throughout the paper, we explicitly state the assumptions and conditions our
results hold under. For example, in Sections 3 and 4, we specifically work with learning
algorithms that have a linear querying strategy and an open direction would be to understand
a broader family of strategies. In Section 4.3, we mention that developing a theory for pattern
classes in the continuous case is quite challenging so we simplify the problem in the discrete
setting. In Appendix B.1 we mention that the theory only holds for the blind-prediction
setting and a future direction would extend to the update-and-deploy setting.
Guidelines:"
CLAIMS,0.7877094972067039,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.7905027932960894,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.7932960893854749,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: In every theoretical statement, we characterize the assumptions and conditions
of the statement and then we give a proof.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.7960893854748603,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.7988826815642458,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: This is a completely theoretical paper so there are no experiments.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8016759776536313,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8044692737430168,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8072625698324022,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8100558659217877,Answer: [NA]
OPEN ACCESS TO DATA AND CODE,0.8128491620111732,Justification: This is a completely theoretical paper so there are no experiments.
OPEN ACCESS TO DATA AND CODE,0.8156424581005587,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8184357541899442,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8212290502793296,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8240223463687151,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8268156424581006,Answer: [NA]
OPEN ACCESS TO DATA AND CODE,0.8296089385474861,Justification: This is a completely theoretical paper so there are no experiments.
OPEN ACCESS TO DATA AND CODE,0.8324022346368715,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.835195530726257,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8379888268156425,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.840782122905028,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8435754189944135,Answer: [NA]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8463687150837989,Justification: This is a completely theoretical paper so there are no experiments.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8491620111731844,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8519553072625698,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8547486033519553,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.8575418994413407,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.8603351955307262,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.8631284916201117,Answer: [NA]
EXPERIMENTS COMPUTE RESOURCES,0.8659217877094972,Justification: This is a completely theoretical paper so there are no experiments.
EXPERIMENTS COMPUTE RESOURCES,0.8687150837988827,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.8715083798882681,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.8743016759776536,9. Code Of Ethics
CODE OF ETHICS,0.8770949720670391,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.8798882681564246,Answer: [Yes]
CODE OF ETHICS,0.88268156424581,Justification: The research in this paper conforms to the NeurIPS Code of Ethics.
CODE OF ETHICS,0.8854748603351955,Guidelines:
CODE OF ETHICS,0.888268156424581,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.8910614525139665,10. Broader Impacts
BROADER IMPACTS,0.8938547486033519,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.8966480446927374,Answer: [NA]
BROADER IMPACTS,0.8994413407821229,"Justification: Since this is a learning theory paper focused on characterizing learnability and
complexity of learning problems, we do not see any immediate negative societal impact."
BROADER IMPACTS,0.9022346368715084,Guidelines:
BROADER IMPACTS,0.9050279329608939,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact."
BROADER IMPACTS,0.9078212290502793,"• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9106145251396648,11. Safeguards
SAFEGUARDS,0.9134078212290503,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9162011173184358,Answer: [NA]
SAFEGUARDS,0.9189944134078212,"Justification: This is a completely theoretical paper so there are no datasets or experimental
models."
SAFEGUARDS,0.9217877094972067,Guidelines:
SAFEGUARDS,0.9245810055865922,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9273743016759777,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9301675977653632,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9329608938547486,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.9357541899441341,"Justification: This is a completely theoretical paper so we don’t have any code, data, or
models."
LICENSES FOR EXISTING ASSETS,0.9385474860335196,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9413407821229051,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.9441340782122905,"• If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.946927374301676,13. New Assets
NEW ASSETS,0.9497206703910615,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.952513966480447,Answer: [NA]
NEW ASSETS,0.9553072625698324,"Justification: This is a completely theoretical paper so we don’t have any code, data, or
models."
NEW ASSETS,0.9581005586592178,Guidelines:
NEW ASSETS,0.9608938547486033,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9636871508379888,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9664804469273743,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9692737430167597,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9720670391061452,"Justification: We do not conduct any crowdsourcing experiments or research with human
subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9748603351955307,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776536312849162,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804469273743017,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832402234636871,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860335195530726,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888268156424581,"Justification: We do not conduct any crowdsourcing experiments or research with human
subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916201117318436,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994413407821229,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972067039106145,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
