Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002932551319648094,"In ofﬂine/batch reinforcement learning (RL), the predominant class of approaches
with most success have been “support constraint” methods, where trained poli-
cies are encouraged to remain within the support of the provided ofﬂine dataset.
However, support constraints correspond to an overly pessimistic assumption that
actions outside the provided data may lead to worst-case outcomes. In this work,
we aim to relax this assumption by obtaining uncertainty estimates for predicted
action values, and acting conservatively with respect to a lower-conﬁdence bound
(LCB) on these estimates. Motivated by the success of ensembles for uncertainty
estimation in supervised learning, we propose MSG, an ofﬂine RL method that
employs an ensemble of independently updated Q-functions. First, theoretically,
by referring to the literature on inﬁnite-width neural networks, we demonstrate
the crucial dependence of the quality of derived uncertainties on the manner in
which ensembling is performed, a phenomenon that arises due to the dynamic
programming nature of RL and overlooked by existing ofﬂine RL methods. Our
theoretical predictions are corroborated by pedagogical examples on toy MDPs, as
well as empirical comparisons in benchmark continuous control domains. In the
signiﬁcantly more challenging antmaze domains of the D4RL benchmark, MSG
with deep ensembles by a wide margin surpasses highly well-tuned state-of-the-
art methods. Consequently, we investigate whether efﬁcient approximations can
be similarly effective. We demonstrate that while some very efﬁcient variants
also outperform current state-of-the-art, they do not match the performance and
robustness of MSG with deep ensembles. We hope that the signiﬁcant impact of
our less pessimistic approach engenders increased focus into uncertainty estima-
tion techniques directed at RL, and engenders new efforts from the community
of deep network uncertainty estimation researchers whom thus far have not em-
ployed ofﬂine reinforcement learning domains as a testbed for validating modern
uncertainty estimation techniques."
INTRODUCTION,0.005865102639296188,"1
INTRODUCTION"
INTRODUCTION,0.008797653958944282,"Ofﬂine reinforcement learning (RL), also referred to as batch RL, is the setting where we are pro-
vided with a dataset of interactions with a Markov Decision Process (MDP), and the goal is to learn
an effective policy without further interactions with the MDP. Ofﬂine RL holds the promise of data-
efﬁciency through data reuse and improved safety due to minimizing the need for policy rollouts.
As a result, ofﬂine RL has been a subject of signiﬁcant renewed interest in the machine learning
literature (Levine et al., 2020)."
INTRODUCTION,0.011730205278592375,"One common approach to ofﬂine RL, known as model-free, uses value estimation through approxi-
mate dynamic programming (ADP). The predominant algorithmic philosophy with most success in
ADP-based ofﬂine RL is to limit obtained policies to the support set of the available ofﬂine data,
with the intuition being that such constraints would reduce inaccurate value estimates since the ac-
tions chosen by the policy are close to the observed data. A large variety of methods have been
developed for enforcing such constraints, examples of which include regularizing policies with be-
havior cloning objectives (Kumar et al., 2019; Fujimoto & Gu, 2021), only performing updates on
actions observed in (Peng et al., 2019; Nair et al., 2020; Wang et al., 2020; Ghasemipour et al., 2021)"
INTRODUCTION,0.01466275659824047,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.017595307917888565,"or close to (Fujimoto et al., 2019) the ofﬂine dataset, and regularizing to lower the estimated value
of actions not seen in the dataset (Wu et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021)."
INTRODUCTION,0.020527859237536656,"Support constraint forms of regularization correspond to an overly pessimistic assumption that any
action outside the provided data leads to highly negative outcomes (Buckman et al., 2020). Instead,
it would be preferable if we could place more trust into the predictions of value networks beyond
the training dataset. Indeed in adjacent ﬁelds of A.I., for different tasks of interest, researchers
have spent signiﬁcant effort developing architectures with built-in inductive biases to help neural
networks not only obtain strong accuracy on the training set, but perform well beyond the data they
were trained on: convolutional and residual networks for computer vision (He et al., 2016), recurrent
networks and transformers for NLP (Vaswani et al., 2017), graph neural networks for graph-based
data (Li et al., 2015), and Nerf and Siren for effective signal modelling (Sitzmann et al., 2020;
Mildenhall et al., 2020)."
INTRODUCTION,0.02346041055718475,"However, trusting the predictions of neural networks beyond the training data is a challenging ordeal.
Many works have demonstrated that neural networks are prone to making high-conﬁdence incorrect
predictions outside the training set, even on i.i.d. test data (Guo et al., 2017). This is a particularly
major issue for value estimation, since errors rapidly accumulate through dynamic programming and
lead to catastrophic results. Thus, in order to trust our models beyond the training set, a potentially
promising approach would be to obtain high quality uncertainty estimates on predicted action values."
INTRODUCTION,0.026392961876832845,"In current supervised learning literature, “Deep Ensembles” and their more efﬁcient variants have
been shown to be the most effective method for uncertainty estimation (Ovadia et al., 2019). In this
work, we aim to transfer the success of ensembles for uncertainty estimation to the setting of ofﬂine
RL. We begin by presenting MSG, an actor-critic ofﬂine RL algorithm leveraging an ensemble
of Q-value functions. MSG trains N Q-value functions completely independent of one-another,
and updates an actor network with respect to a lower conﬁdence bound (LCB) on action values
obtained from the ensemble. By referring to the literature on inﬁnite-width neural networks, we
theoretically demonstrate the critical importance of independence in Q-functions, a deviation from
standard practice in RL which more often uses the full ensemble to compute identical target values
for training each ensemble member. Our theoretical predictions are corroborated by experiments in
a toy MDP, and their relevance to practical settings is also veriﬁed through benchmark experiments."
INTRODUCTION,0.02932551319648094,"In established benchmarks for ofﬂine RL, we demonstrate that MSG matches, and in the more
challenging domains, signiﬁcantly exceeds the prior state-of-the-art. Inspired by this success, we
investigate whether the performance of MSG can be recovered through modern efﬁcient approaches
to ensembling. While we demonstrate that efﬁcient ensembles continue to outperform current state-
of-the-art batch RL algorithms, they cannot recover the performance of MSG using full deep ensem-
bles."
INTRODUCTION,0.03225806451612903,"We hope that our work highlights some of the unique challenges of uncertainty estimation
in reinforcement learning, and the signiﬁcant impact it can have on the performance of algo-
rithms. We also hope that our work encourages stronger engagement from researchers specializing
in uncertainty estimation techniques – who typically use computer vision and NLP tasks as bench-
marks – to use ofﬂine RL as an additional testbed. Our results reveal that ofﬂine RL presents unique
challenges not seen in standard supervised learning, due to the accumulation of errors over multiple
back ups, and can be a valuable domain for testing novel uncertainty estimation techniques."
RELATED WORK,0.03519061583577713,"2
RELATED WORK"
RELATED WORK,0.03812316715542522,"Uncertainty estimation is a core component of RL, since an agent only has a limited view into
the mechanics of the environment through its available experience data. Traditionally, uncertainty
estimation has been key to developing proper exploration strategies such as upper conﬁdence bound
(UCB) and Thompson sampling (Lattimore & Szepesv´ari, 2020), in which an agent is encouraged
to seek out paths where its uncertainty is high. Ofﬂine RL presents an alternative paradigm, where
the agent must act conservatively and is thus encouraged to seek out paths where its uncertainty
is low (Buckman et al., 2020). In either case, proper and accurate estimation of uncertainties is
paramount. To this end, much research has been produced towards the end of devising provably
correct uncertainty estimates (Thomas et al., 2015; Feng et al., 2020; Dai et al., 2020), or, at least,
bounds on uncertainty that are good enough for acting either exploratory (Strehl et al., 2009) or"
RELATED WORK,0.04105571847507331,Under review as a conference paper at ICLR 2022
RELATED WORK,0.04398826979472141,"conservatively (Kuzborskij et al., 2021). However, these approaches require exceedingly simple
environment structure, typically either a ﬁnite discrete state and action space or linear spaces with
linear dynamics and rewards."
RELATED WORK,0.0469208211143695,"While theoretical guarantees for uncertainty estimation are more limited in practical situations with
deep neural network function approximators, a number of works have been able to achieve practical
success, for example using deep network analogues for count-based uncertainty (Ostrovski et al.,
2017), Bayesian uncertainty (Ghavamzadeh et al., 2016; Yang et al., 2020), and bootstrapping (Os-
band et al., 2019; Kostrikov & Nachum, 2020). Many of these methods employ ensembles. In fact,
in continuous control RL, it is common to use an ensemble of two value functions and use their
minimum for computing a target value during Bellman error minimization (Fujimoto et al., 2018).
A number of works in ofﬂine RL have extended this to propose backing up minimums or lower
conﬁdence bound estimates over larger ensembles (Kumar et al., 2019; Wu et al., 2019; Agarwal
et al., 2020). In our work, we continue to ﬁnd that ensembles are extremely useful for acting con-
servatively, but the manner in which ensembles are used is critical. Speciﬁcally our proposed MSG
algorithm advocates for using independently learned ensembles, without sharing of target values,
and this import design decision is supported by empirical evidence."
RELATED WORK,0.04985337243401759,"The widespread success of ensembles for uncertainty estimation in RL echoes similar ﬁndings in
supervised deep learning. While there exist proposals for more technical approaches to uncertainty
estimation (Li et al., 2007; Neal, 2012; Pawlowski et al., 2017), ensembles have repeatedly been
found to perform best empirically (Lee et al., 2015; Lakshminarayanan et al., 2016). Much of the
active literature on ensembles in supervised learning is concerned with computational efﬁciency,
with various proposals for reducing the compute or memory footprint of training and inference on
large ensembles (Wen et al., 2020; Zhu et al., 2019; Havasi et al., 2020). While these approaches
have been able to achieve impressive results in supervised learning, our empirical results suggest
that their performance suffers signiﬁcantly in an ofﬂine RL setting compared to deep ensembles,
and even naive Multi-Head ensembles (Lee et al., 2015; Osband et al., 2016; Tran et al., 2020)
which are not considered to be as effective in the supervised learning setting(Havasi et al., 2020)."
METHODOLOGY,0.05278592375366569,"3
METHODOLOGY"
METHODOLOGY,0.05571847507331378,"Notation
Throughout this work, we represent Markov Decision Process (MDP) as M
=
⟨S, A, r, P, γ⟩, with state space S, action space A, reward function r : S ×A →R, transition
dynamics P, and discount γ. In ofﬂine RL, we assume access to a dataset of interactions with the
MDP, which we will represent as collection of tuples D = {(s, a, s′, r, term)}N, where t is an
indicator variable that is set to True when s′ is a terminal state."
METHODOLOGY,0.05865102639296188,"Two neural networks with an identical architecture, trained in an identical manner, will make differ-
ent predictions outside the training set when their weights are initialized with different random draws
from the initial weight distribution. So a natural question is, “Which network’s predictions can we
trust?” An answer to this question can be motivated by referring to the literature on inﬁnite-width
networks. When performing mean-squared error regression in the inﬁnite-width regime, informally
speaking, an intriguing property is that the distribution of predictions on unseen data is given by a
Gaussian Process whose kernel function is solely deﬁned by, 1) the architecture, and 2) the choice
of initial weight distribution (Lee et al., 2019). Hence, to leverage the built-in architectural in-
ductive biases of value networks for ofﬂine RL, we can train policies to act with respect to the
lower-conﬁdence bound (LCB) of the derived Gaussian Process."
METHODOLOGY,0.06158357771260997,"In what follows, we present our proposed algorithm, MSG, which leverages ensembles to approx-
imate the LCB. Additionally, using the literature on inﬁnite-width networks, we demonstrate the
theoretical advantage of forming ensembles in the manner we propose, which deviates from the
current use of ensembles in the reinforcement learning literature."
METHODOLOGY,0.06451612903225806,"3.1
MODEL STANDARD-DEVIATION GRADIENTS (MSG)"
METHODOLOGY,0.06744868035190615,"MSG follows an actor critic setup where in each iteration, we ﬁrst estimate the Q-values of the
current policy, and subsequently optimize the policy through gradient ascent on the lower conﬁdence
bound of action value estimates."
METHODOLOGY,0.07038123167155426,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.07331378299120235,"Policy Evaluation
At the beginning of training, we create an ensemble of N Q-functions by taking
N samples from the initial weight distribution. Throughout training, the loss for the Q-functions is
the standard least-square Q-evaluation loss,"
METHODOLOGY,0.07624633431085044,"L(θi) = E(s,a,r,s′,t)∼D
h  
Qθi(s, a) −yi(r, s′, term, π)
2 i
(1)"
METHODOLOGY,0.07917888563049853,"yi := r + (1 −term) · γ · Ea′∼π(s′)
h
Q¯θi(s′, a′)
i
(2)"
METHODOLOGY,0.08211143695014662,"where θi, ¯θi denote the parameters and target network parameters for the ith Q-function, and term =
1[s′ is terminal]. In practice, the expectation in equation 1 is estimated by a minibatch, and the
expectation in equation 2 is estimated with a single action sample from the policy. After every
update to the Q-function parameters, their corresponding target parameters are updated to be an
exponential moving average of the parameters in the standard fashion. A key factor to note is
that, in contrast to the typical usage of ensembles in actor critic algorithms, each ensemble
member’s update (and yi) is completely independent from other ensemble members. As we
will present below, this an important algorithmic choice, both theoretically (Theorem 4.1) and
empirically (section 5.3.2)."
METHODOLOGY,0.08504398826979472,"Policy Optimization
As described above, the choice of architecture and weight initialization in-
duces a distribution on predictions. Having approximated this distribution using an ensemble of
value networks, we can optimize the policy with respect to a lower conﬁdence bound on action
values. Speciﬁcally, our proposed policy optimization objective in MSG is,"
METHODOLOGY,0.08797653958944282,"max
π
Es∼D,a∼π(s)
h
µ(s, a) + β · σ(s, a)
i
where
µ(s, a) = mean
i"
METHODOLOGY,0.09090909090909091,"h
Qθi(s, a)
i
,
σ(s, a) = std
i"
METHODOLOGY,0.093841642228739,"h
Qθi(s, a)
i"
METHODOLOGY,0.0967741935483871,"where β ∈R is a hyperparameter that trades off conservatism and optimism. As our problem setting
is that of ofﬂine RL requiring conservatism, we use β ≤0."
THE TRADE-OFF BETWEEN TRUST AND PESSIMISM,0.09970674486803519,"3.2
THE TRADE-OFF BETWEEN TRUST AND PESSIMISM"
THE TRADE-OFF BETWEEN TRUST AND PESSIMISM,0.10263929618768329,"While our hope is to leverage the distributions induced by the architecture choice, it is not always
feasible to do so; designed architectures can still be fundamentally biased in some manner, or we
can simply be in a setting with insufﬁcient data coverage. Thus we need to trade-off trusting the gen-
eralization ability of our models with the pessimistic approach of support constraints. In this work,
inspired by CQL (Kumar et al., 2020), we add the following regularizer to our policy evaluation
objective (equation 1),"
THE TRADE-OFF BETWEEN TRUST AND PESSIMISM,0.10557184750733138,"R(θi) = Es∼D
h
Ea∼π(a|s)[Qθi(s, a)] −Qθi(s, aD)
i
(3)"
THE TRADE-OFF BETWEEN TRUST AND PESSIMISM,0.10850439882697947,"where aD denotes the action taken in state s in the dataset. We control the contribution of R(θi) by
weighting this term with weight parameter α. Practically, we estimate the outer expectation using
the states in the mini-batch, and we approximate the inner expectation using a single sample from
the policy. An example scenario where we have observed such a regularizer is helpful is when the
ofﬂine dataset only contains a narrow (e.g., expert) data distribution. We believe this is because
the power of ensembles comes from predicting a value distribution for unseen (s, a) based on the
available training data. Thus, if no data for sub-optimal actions is present, ensembles cannot make
accurate predictions and increased pessimism for unseen actions becomes necessary."
INDEPENDENCE IN ENSEMBLES MATTERS,0.11143695014662756,"4
INDEPENDENCE IN ENSEMBLES MATTERS"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.11436950146627566,"4.1
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.11730205278592376,"In the reinforcement learning literature, ensembles are widely used for combatting over-estimation
bias (Haarnoja et al., 2018; Fujimoto et al., 2018; 2019; Kumar et al., 2020; Agarwal et al., 2020;
Ghasemipour et al., 2021). However, the typical usage of ensembles is to compute a target value y
using an ensemble of target networks, and subsequently update all the Q-functions with respect to
the same target, typically the minimum over targets. Hence, the objectives for all the Q-functions
share the same target value. In this section, drawing upon the literature of inﬁnitely wide networks,
we demonstrate that having the updates for ensemble members be independent of one another – as"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.12023460410557185,Under review as a conference paper at ICLR 2022
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.12316715542521994,"done in MSG – results in uncertainty estimates that align more closely with intuitive expectations,
compared to when their targets are shared."
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.12609970674486803,"We study this difference in the setting of policy evaluation using an inﬁnite ensemble of inﬁnite-
width Q-function networks (i.e. an ensemble of inﬁnitely many Q-functions, each of which is an
inﬁnite-width neural network, with the only difference being that their weights are initialized through
independent random draws from the initial weight distribution). For sake of simplicity of derivations
we assume that the policy we are trying to evaluate is deterministic and that we do not have terminal
states in the MDP. The policy evaluation routine we consider is Fitted Q-Evaluation (Fonteneau et al.,
2013), which can be described as repeatedly performing the following steps,"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.12903225806451613,"• Compute TD Targets For each (s, a, r, s′) ∈D compute the TD targets yi(r, s′, π)
• Fit the Q-functions For each ensemble member, optimize the following objective until conver-
gence using full batch gradient descent.
1
|D| X"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.13196480938416422,"(s,a,r,s′)∈D"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.1348973607038123,"h  
Qθi(s, a) −yi(r, s′, π)
2 i
.
(4)"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.1378299120234604,"First we establish some notation. Let X denote a matrix where the rows are the state-action pairs
(s, a) in the ofﬂine dataset. Let R be the |D| × 1 matrix containing the rewards observed after each
(s, a) in X. Let X ′ denote a matrix where the rows are the next state and policy action (s′, π(s′))
Additionally let,"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.14076246334310852,"ˆΘ0(A, B) := ∇θQθ(A) · ∇θQθ(B)T |t=0
ˆΘ−1
0
:= ˆΘ0(X, X)−1
C := ˆΘ0(X ′, X) · ˆΘ−1
0 ."
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.1436950146627566,"ˆΘ0(A, B) above is referred to as the tangent kernel (Jacot et al., 2018), which is deﬁned as the
outerproduct of gradients of the Q-function, at initialization (iteration t = 0). The deﬁnition of ˆΘ0
does not contain the variable i indexing the ensemble members because, at inﬁnite width, ˆΘ0(A, B)
converges to a deterministic kernel and hence is the same for all ensemble members."
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.1466275659824047,"Intuitively, C is a |D| × |D| matrix where cp,q (the element at row p, column q) captures a notion
of similarity between (s′, π(s′)) in the pth row of X ′, and (s, a) in the qth row of X. Let Yi
t (with
shape |D| × 1) denote the targets used for ﬁtting Qθi at iteration t."
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.1495601173020528,"Here, we will study the difference between the following two methods for computing targets: one
where each ensemble member uses its own predictions to compute the TD targets (analogous to
MSG), and another where all ensemble members share same target (analogous to typical use of
ensembles in ofﬂine RL):"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.15249266862170088,"• Method 1 (MSG, Independent Targets): yi(s, a) = r + γ · Qθi(s′, π(s))"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.15542521994134897,"• Method 2 (Shared LCB): ∀i, yi(s, a)
=
r + γ · """
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.15835777126099707,"E
ensemble"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.16129032258064516,"h
Qθi(s′, π(s))
i
−"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.16422287390029325,"Std
ensemble"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.16715542521994134,"h
Qθi(s′, π(s))
i#"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.17008797653958943,"As can be seen in Appendix F, for the two methods considered above, under the described policy
evaluation procedure, the values Qθi(s, a)|t (at iteration t) can be computed in closed form for all
(s, a) ∈S × A. This enables us to compare the ﬁnal distribution of Q(s, a) after policy evaluation
under two methods.
Theorem 4.1. After t iterations, for both Method 1 and Method 2 we have,"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.17302052785923755,"Independent: LCB

Qt+1(X ′)

≈(1 + . . . + γtCt)CR − r"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.17595307917888564,"E
h
(1 + . . . + γtCt)(Q0(X ′) −CQ0(X))
2i"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.17888563049853373,"Shared LCB: LCB

Qt+1(X ′)

≈(1 + . . . + γtCt)CR −(1 + . . . + γtCt) r"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.18181818181818182,"E
h
Q0(X ′) −CQ0(X)
2i"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.18475073313782991,"where LCB refers to mean - std, and the square and square-root operations are applied element-wise."
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.187683284457478,Proof. Please refer to Appendix F.
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.1906158357771261,Under review as a conference paper at ICLR 2022
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.1935483870967742,"As can be seen, the equations for the lower-conﬁdence bound (LCB) in both settings are very similar,
with the main difference being in the second terms which correspond to the “pessimism” terms. In
ensembles, the only source of randomness is in the initialization of the networks. In the inﬁnite-
width setting this presents itself in the two equations above, where the random variables Q0(X ′) −
CQ0(X) produce the uncertainty in the ensemble of networks: For both Independent and Shared-
LCB, at iteration t + 1 we have,"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.19648093841642228,"Qt+1(X ′) = Q0(X ′) + C(Yt −Q0(X))
(5)"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.19941348973607037,"= CYt + (Q0(X ′) −CQ0(X))
(6)"
THE STRUCTURE OF UNCERTAINTIES DEPENDS ON HOW ENSEMBLING IS DONE,0.20234604105571846,"Thus, Q0(X ′)−CQ0(X) represents the random value accumulated in each iteration. The accumula-
tion of the uncertainty is captured by the geometric term (1+. . .+γtCt). Here is where we observe
the key difference between Independent and Shared-LCB: whether the term (1 + . . . + γtCt) is
applied inside or outside the expectation. In Independent ensembles, the randomness/uncertainty is
ﬁrst backed-up by the geometric term and afterward the standard-deviation is computed. In Shared-
LCB however, ﬁrst the standard-deviation of the randomness/uncertainty is computed, and after-
wards this value is backed up. Not only do we believe that the former (Independent) makes more
sense intuitively, but in the case of Shared-LCB, the pessimism term may contain negative values
which would actually result in an optimism bonus! As will be discussed below, we also empirically
investigate this question in section 5.3.2 (results in Figure 2) and ﬁnd that while Shared-LCB can
perform decently in D4RL gym locmotion (Fu et al., 2020), Shared-LCB (and Shared-Min) com-
pletely fail on the more challenging domains. This is in line with the observation that no prior ofﬂine
RL methods rely solely on Shared-LCB or Shared-Min as the source of pessimism/conservatism
(Fujimoto et al., 2019; Kumar et al., 2019; Ghasemipour et al., 2021; An et al., 2021)."
VALIDATING THEORETICAL PREDICTIONS,0.20527859237536658,"4.2
VALIDATING THEORETICAL PREDICTIONS"
VALIDATING THEORETICAL PREDICTIONS,0.20821114369501467,"In this section we aim to evaluate the validity of our theoretical discussion above in a simple toy
MDP that allows us to follow the idealized setting of the presented theorems more closely, and
allows for visualization of uncertainties obtained through different ensembling approaches."
VALIDATING THEORETICAL PREDICTIONS,0.21114369501466276,"Continuous Chain MDP
The MDP we consider has state-space S = [−1, 1], action space A ∈
R, deterministic transition dynamics s′ = s + a clipped to remain inside S, and reward function
r(s, a) = 1[s′ ∈[0.75, 1]]."
VALIDATING THEORETICAL PREDICTIONS,0.21407624633431085,"Data Collection & Evaluation Policy
The ofﬂine dataset we generate consists of 40 episodes,
each of length 30. At the beginning of each episode we initialize at a random state s ∈S. In each
step take a random action sampled from Unif(−0.3, 0.3), and record all transitions (s, a, r, s′). For
evaluating the uncertainties obtained from different approaches, we create regions of missing data
by removing all transitions such that s or s′ are in the range [−0.33, 0.33]. The policy we choose to
evaluate with the different approaches is ∀s, π(s) = 0.1."
VALIDATING THEORETICAL PREDICTIONS,0.21700879765395895,"Optimal Desired Form of Uncertainty
Note that the evaluation policy π(s) = 0.1 is always
moving towards the positive direction, and there is lack of data for states in the interval [−0.33, 0.33].
Hence, what we would expect is that in the region [0.33, 1] there should not be a signiﬁcant amount
of uncertainty, while in the region [−1, −0.33] there should be signiﬁcantly more uncertainty about
the Q-values of π because the policy will be passing through [−0.33, 0.33] where there is no data."
VALIDATING THEORETICAL PREDICTIONS,0.21994134897360704,"Results
We visualize and compare the uncertainties obtained when the targets in the policy evalu-
ation procedure are computed as:"
VALIDATING THEORETICAL PREDICTIONS,0.22287390029325513,"• Independent (MSG): yi = r + γ · Qθi(s′, π(s′))"
VALIDATING THEORETICAL PREDICTIONS,0.22580645161290322,"• Independent Double-Q: yi = r + γ · min
h
Q1
θi(s′, π(s′)), Q2
θi(s′, π(s′))
i"
VALIDATING THEORETICAL PREDICTIONS,0.2287390029325513,"• Shared Mean: y = r + γ · mean
h
Qθi(s′, π(s′))
i"
VALIDATING THEORETICAL PREDICTIONS,0.2316715542521994,"• Shared LCB: y = r + γ · """
VALIDATING THEORETICAL PREDICTIONS,0.23460410557184752,"mean
h
Qθi(s′, π(s′))
i
−2 · std
h
Qθi(s′, π(s′))
i#"
VALIDATING THEORETICAL PREDICTIONS,0.2375366568914956,"• Shared Min: y = r + γ · min
h
Qθi(s′, π(s′))
i"
VALIDATING THEORETICAL PREDICTIONS,0.2404692082111437,Under review as a conference paper at ICLR 2022
VALIDATING THEORETICAL PREDICTIONS,0.2434017595307918,"Figure 1:
Verifying theoretical predictions on the toy Continuous Chain MDP. The marked interval
[−0.33, 0.33] denotes the region of state-space with no data. As anticipated by Theorem 4.1, when the value
functions are trained independently, the derived uncertainties capture the interaction between available data,
the structure of the MDP, and the policy being evaluated. When the targets are shared, the networks behave
similarly to performing regression for oracle-given target values, i.e. there is randomness amongst ensemble
members only between [−0.33, 0.33] because there is no data in that region."
VALIDATING THEORETICAL PREDICTIONS,0.24633431085043989,"We include Independent Double-Q, as using Q-functions of the form Q(s, a)
="
VALIDATING THEORETICAL PREDICTIONS,0.24926686217008798,"min
h
Q1(s, a), Q2(s, a)
i
has become common practice in recent deep RL literature (Fujimoto et al.,"
VALIDATING THEORETICAL PREDICTIONS,0.25219941348973607,"2018; Haarnoja et al., 2018)1."
VALIDATING THEORETICAL PREDICTIONS,0.25513196480938416,"In Figure 1 we plot the mean and two standard deviations of the predicted values for the policy we
evaluated, π(s) = 0.1 (additional experimental details presented in Appendix G.1). The ﬁrst thing
to note is that, “Independent” ensembles effectively match our desired form of uncertainty: states
that under the evaluation policy lead to regions with little data have wider uncertainties than states
that do not. A second observation is that Shared LCB and Shared Min provide a seemingly
good approximation to the lower-bound of Independent predictions. Nonetheless, our theoretical
considerations suggest that these lower bounds may have important failure cases. Furthermore,
empirically – as we discuss below (Figure 2) – we were unable to train effective policies on ofﬂine
RL benchmarks using Shared LCB and Shared Min, despite the implementation differing in
only 2 lines of code."
VALIDATING THEORETICAL PREDICTIONS,0.25806451612903225,"Appendix G.2 presents additional very interesting empirical observations in this toy setting
which due space limitations we were unable to include in the main manuscript. We highly
encourage readers interested in the intersection of inﬁnite-width networks and RL to take a look at
our observations as they may be hinting at intriguing avenues for future theoretical and practically
important work."
EXPERIMENTS,0.26099706744868034,"5
EXPERIMENTS"
EXPERIMENTS,0.26392961876832843,"In this section we seek to empirically answer the following questions: 1) How well does MSG
perform compared to current state-of-the-art in ofﬂine RL? 2) Can we match the performance of
MSG through efﬁcient ensemble approaches popular in supervised learning literature? 3) Are the
theoretical differences in ensembling approaches elaborated on in the previous section practically
relevant?"
EXPERIMENTS,0.2668621700879765,"5.1
D4RL BENCHMARK"
EXPERIMENTS,0.2697947214076246,"We begin by evaluating MSG on the Gym (Brockman et al., 2016) subset of the D4RL ofﬂine RL
benchmark (Fu et al., 2020). Amongst the different data settings we focus our experiments on
the medium and medium-replay (sometimes referred to as mixed) settings as the other data
setting could not adequately differentiate between competitive methods. Experimental details such
as hyperparameter search procedure are described in Appendix C."
EXPERIMENTS,0.2727272727272727,"In addition to validating MSG, a secondary objective of ours is to gain a sense for the upper bound of
performance for various algorithms on D4RL Gym. For this reason – with an equal hyperparameter
tuning budget – we tune the main hyperparameter for each algorithm. As can be seen in Table 3
(Appendix A), across the board in D4RL Gym, MSG is competitive with very well-tuned current
state-of-the-art algorithms, CQL (Kumar et al., 2020) and F-BRC (Kostrikov et al., 2021). We"
EXPERIMENTS,0.2756598240469208,"1Note that Independent Double-Q is still an independent ensemble, where each ensemble member
has an architecture containing a min-pooling on top of two subnetworks."
EXPERIMENTS,0.2785923753665689,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.28152492668621704,"Domain
CQL (Reported) MSG (N = 64)
β
α"
EXPERIMENTS,0.2844574780058651,"maze2d-umaze
5.7
100.2 ± 33.0
-8
0
maze2d-medium
5.0
87.4 ± 10.6
0
0
maze2d-large
12.5
147.8 ± 55.8
-4
0.1"
EXPERIMENTS,0.2873900293255132,"antmaze-umaze
74.0
96.8 ± 2.0
−8 0.1
antmaze-umaze-diverse
84.0
60.2 ± 7.1
−8 0.5
antmaze-medium-play
61.2
80.0 ± 9.4
−4 0.1
antmaze-medium-diverse
53.7
78.8 ± 5.5
−4 0.1
antmaze-large-play
15.8
64.8 ± 10.7
−8
0
antmaze-large-diverse
14.9
68.8 ± 11.7
−8
0"
EXPERIMENTS,0.2903225806451613,"Table 1:
Result on D4RL maze2d and antmaze domains. As
we were unable to reproduce CQL antmaze results, we present the
numbers reported by the original paper which uses the same net-
work architectures."
EXPERIMENTS,0.2932551319648094,"Table 2: D4RL antmaze tasks. Figure
taken from Fu et al. (2020)."
EXPERIMENTS,0.2961876832844575,"also note that our results for baseline algorithms exceed prior reported results (often signiﬁcantly),
providing us conﬁdence in their implementation."
EXPERIMENTS,0.2991202346041056,"Thus far we established the validity of MSG as an ofﬂine RL algorithm. However, on the gym
domains considered above, MSG’s performance is on par with CQL and F-BRC, and does not mo-
tivate the use of ensembles in lieu of support constraints. To enable a better comparison amongst
competing algorithms, we experiment with the signiﬁcantly more challenging antmaze tasks."
EXPERIMENTS,0.3020527859237537,"The antmaze tasks in D4RL, and in particular the two antmaze-large settings, are considered to
be extremely challenging. The data for antmaze tasks consists of many episodes of an Ant agent
(Brockman et al., 2016) running along arbitrary paths in a maze. The data from these trajectories is
relabeled with a reward of 1 when near a particular location in the maze (at which point the episode
is terminated), and 0 otherwise. The undirected, extremely sparse reward nature of antmaze tasks
make them very challenging, especially for the large maze sizes."
EXPERIMENTS,0.30498533724340177,"To the best of our knowledge, the antmaze-large domains are considered unsolved, unless specialized
technique such as hierarchical policies – which signiﬁcantly simplify the problem – are used (e.g.
Ajay et al. (2020)). The current state-of-the-art is CQL (Kumar et al., 2019), and is the method we
compare to. Table 1 presents our empirical results. As we were unable to reproduce the reported
results for CQL, for fairness we include the numbers reported by the original work which uses the
same network architectures. As can be seen, MSG achieves unprecedented results on the antmaze
domains, clearly demonstrating the signiﬁcant advantage of employing ensembles for batch RL."
RL UNPLUGGED,0.30791788856304986,"5.2
RL UNPLUGGED"
RL UNPLUGGED,0.31085043988269795,"Figure 4 presents additional results using the RL Unplugged benchmark (Gulcehre et al., 2020). We
compare to results for Behavioral Cloning (BC) and two state-of-the-art methods, Critic-Regularized
Regression (CRR) (Wang et al., 2020) and MuZero Unplugged (Schrittwieser et al., 2021). Despite
the relatively very small architectures we used ( 1"
RL UNPLUGGED,0.31378299120234604,"60 number of parameters), we observe that MSG
is on par with the current state-of-the-art these tasks with the exception of humanoid.run which
appears to require the larger architectures used by (Gulcehre et al., 2020). Additional experimental
details as well as numerical presentation of results can be found in appendix E."
EFFICIENT ENSEMBLES & ENSEMBLE ABLATIONS,0.31671554252199413,"5.3
EFFICIENT ENSEMBLES & ENSEMBLE ABLATIONS"
EFFICIENT ENSEMBLES & ENSEMBLE ABLATIONS,0.3196480938416422,"In this section we dissect what aspects of MSG contribute to its superior results, and investigate
whether the advantages of MSG can be realized through efﬁcient ensemble approximations popular
in supervised learning."
EFFICIENT ENSMEBLES,0.3225806451612903,"5.3.1
EFFICIENT ENSMEBLES"
EFFICIENT ENSMEBLES,0.3255131964809384,"Thus far we have demonstrated the signiﬁcant performance gains attainable through MSG. An im-
portant concern however, is that of parameter and computational efﬁciency: “Deep Ensembles” re-
sult in an N-fold increase in memory and compute usage for the Q-networks. While this might not
be a signiﬁcant problem for D4RL benchmark domains due to small model footprints , it becomes
a major bottleneck with larger architectures such as those used in language and vision domains. To"
EFFICIENT ENSMEBLES,0.3284457478005865,Under review as a conference paper at ICLR 2022 0 25 50 75 100
EFFICIENT ENSMEBLES,0.3313782991202346,"halfcheetah-medium
hopper-medium
walker2d-medium
halfcheetah-mixed
hopper-mixed
walker2d-mixed
antmaze-large-diverse antmaze-large-play"
EFFICIENT ENSMEBLES,0.3343108504398827,"CQL
Deep Ensembles (default)
Multi-Head
MIMO-0.5
MIMO-1
Batch Ensembles
N=1
Shared LCB
Shared Min"
EFFICIENT ENSMEBLES,0.33724340175953077,Figure 2: Results for efﬁcient ensembles and ensemble ablations. Numerical values can be found in Table 4.
EFFICIENT ENSMEBLES,0.34017595307917886,"this end, we evaluate whether recent advances in efﬁcient ensemble approaches also transfer well to
the problem of batch RL. Speciﬁcally, the efﬁcient ensemble approaches we consider are:"
EFFICIENT ENSMEBLES,0.34310850439882695,"Multi-Head (Lee et al., 2015; Osband et al., 2016; Tran et al., 2020)
Multi-Head refers to en-
sembles that share a “trunk” network and have separate “head” networks for each ensemble member.
In this work, we modify the last layer of a value network to output N predictions instead of a single
Q-value, making the computational cost of this ensemble on par with a single network."
EFFICIENT ENSMEBLES,0.3460410557184751,"Multi-Input Multi-Output (MIMO) (Havasi et al., 2020)
MIMO is an ensembling approach that
approximately has the same parameter and computational footprint as a single network. The MIMO
approach only modiﬁes the input and output layers of a given network. In MIMO, to compute
predictions for data-points x1, ..., xN under an ensemble of size N, the data-points are concatenated
and passes to the network. The output of the network is then split into N predictions y1, ..., yN. For
added clariﬁcation, we include Figure 3a depicting how a MIMO ensemble network functions."
EFFICIENT ENSMEBLES,0.3489736070381232,"Batch Ensembles (Wen et al., 2020)
Batch Ensembles incorporate rank-1 modulations to the
weights of fully-connected layers. More speciﬁcally, let W be the weight matrix of a given fully-
connected layer and let x be the input to the layer. The output of the layer for ensemble member i
is computed as σ(((W T (x ◦ri)) ◦si) + bi), where ◦is the element-wise product, parameters with
superscript i are separate for each ensemble member, and σ is the activation function. While Batch
Ensemble is efﬁcient in terms of number of parameters, in our actor-critic setup its computational
cost is on the same order as deep ensembles since for policy updates we need to evaluate each
ensemble member separately."
EFFICIENT ENSMEBLES,0.3519061583577713,"Results
As can be seen in Table 4, the performance gains of MSG can to some extent also be
realized by efﬁcient ensemble variants. Interestingly, in our experiments the most effective efﬁcient
ensemble approach is Multi-Head ensembles, which is not considered to be the most competitive
uncertainty estimation technique in the supervised learning literature (Havasi et al., 2020). Com-
pared to CQL, Multi-Head ensembles continue to be competitive on D4RL gym, and noticeably
outperform CQL on antmaze-large tasks. Additionally, training Multi-Head ensembles is the most
efﬁcient method, on par with training without ensembling (N = 1)."
EFFICIENT ENSMEBLES,0.3548387096774194,"Nonetheless, compared to MSG using deep ensembles, there is a signiﬁcant performance gap. We
believe this observation very clearly motivates future work in developing efﬁcient uncertainty esti-
mation approaches that are better suited to the domain of reinforcement learning. To facilitate this
direction of research, in our codebase which will be open-sourced, we also include a complete boil-
erplate example amenable to drop-in implementation of novel uncertainty-estimation techniques."
ENSEMBLE ABLATIONS,0.35777126099706746,"5.3.2
ENSEMBLE ABLATIONS"
ENSEMBLE ABLATIONS,0.36070381231671556,"Finally, through the ablations in Table 4 we seek to create a better sense for the various components
of MSG.
• Comparing MSG with deep ensembles (N = 64), and Multi-Head ensembles (N = 64), to no
ensembling (N = 1) we see very clearly the massive advantage of ensembling."
ENSEMBLE ABLATIONS,0.36363636363636365,"• Comparing MSG, which uses Independent targets, to Shared LCB and Shared Min we
see that the latter non-independent approaches signiﬁcantly underperform."
ENSEMBLE ABLATIONS,0.36656891495601174,"• Comparing CQL to N = 1 (no ensembling, only CQL-inspired regularizer), we observe that the
CQL regularizer tends to be better on Gym domains, and our regularizer may be better on antmaze
domains but our results are inconclusive. The advantage of the regularizer we used is signiﬁcant
computationl efﬁcieny due to not using importance sampling."
ENSEMBLE ABLATIONS,0.36950146627565983,Under review as a conference paper at ICLR 2022
DISCUSSION,0.3724340175953079,"6
DISCUSSION
Our work has highlighted the signiﬁcant power of ensembling as a mechanism for uncertainty es-
timation for ofﬂine RL. Theoretically, and practically through benchmark experiments, we have
studied the critical importance of the manner in which ensembling is done. An important out-
standing direction is how can we design improved efﬁcient ensemble approximations, as we have
demonstrated that current approaches used in supervised learning – some of which do lead to state-
of-the-art results ofﬂine RL results – are not nearly as effective as deep ensembles. We hope that this
work engenders new efforts from the community of deep network uncertainty estimation researchers
whom thus far have not employed ofﬂine reinforcement learning domains as a testbed for validating
modern uncertainty estimation techniques."
DISCUSSION,0.375366568914956,Under review as a conference paper at ICLR 2022
REFERENCES,0.3782991202346041,REFERENCES
REFERENCES,0.3812316715542522,"Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on ofﬂine
reinforcement learning. In International Conference on Machine Learning, pp. 104–114. PMLR,
2020."
REFERENCES,0.3841642228739003,"Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Oﬁr Nachum. Opal: Ofﬂine prim-
itive discovery for accelerating ofﬂine reinforcement learning. arXiv preprint arXiv:2010.13611,
2020."
REFERENCES,0.3870967741935484,"Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based ofﬂine re-
inforcement learning with diversiﬁed q-ensemble. Advances in Neural Information Processing
Systems, 34, 2021."
REFERENCES,0.39002932551319647,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.39296187683284456,"Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in ﬁxed-
dataset policy optimization. arXiv preprint arXiv:2009.06799, 2020."
REFERENCES,0.39589442815249265,"Bo Dai, Oﬁr Nachum, Yinlam Chow, Lihong Li, Csaba Szepesv´ari, and Dale Schuurmans. Coindice:
Off-policy conﬁdence interval estimation. arXiv preprint arXiv:2010.11652, 2020."
REFERENCES,0.39882697947214074,"Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu. Accountable off-policy evaluation with
kernel bellman statistics. In International Conference on Machine Learning, pp. 3102–3111.
PMLR, 2020."
REFERENCES,0.40175953079178883,"Raphael Fonteneau, Susan A Murphy, Louis Wehenkel, and Damien Ernst. Batch mode reinforce-
ment learning based on the synthesis of artiﬁcial trajectories. Annals of operations research, 208
(1):383–416, 2013."
REFERENCES,0.4046920821114369,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.40762463343108507,"Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to ofﬂine reinforcement learning.
arXiv preprint arXiv:2106.06860, 2021."
REFERENCES,0.41055718475073316,"Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587–1596. PMLR, 2018."
REFERENCES,0.41348973607038125,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019."
REFERENCES,0.41642228739002934,"Seyed Kamyar Seyed Ghasemipour, Shixiang Gu, and Richard Zemel. Smile: Scalable meta inverse
reinforcement learning through context-conditional policies. 2019."
REFERENCES,0.41935483870967744,"Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective ofﬂine and online rl. In International Conference
on Machine Learning, pp. 3682–3691. PMLR, 2021."
REFERENCES,0.4222873900293255,"Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement
learning: A survey. arXiv preprint arXiv:1609.04436, 2016."
REFERENCES,0.4252199413489736,"Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio G´omez, Konrad Zolna,
Rishabh Agarwal, Josh S Merel, Daniel J Mankowitz, Cosmin Paduraru, et al. Rl unplugged:
A collection of benchmarks for ofﬂine reinforcement learning. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.4281524926686217,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017."
REFERENCES,0.4310850439882698,"Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018."
REFERENCES,0.4340175953079179,Under review as a conference paper at ICLR 2022
REFERENCES,0.436950146627566,"Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Laksh-
minarayanan, Andrew M Dai, and Dustin Tran. Training independent subnetworks for robust
prediction. arXiv preprint arXiv:2010.06610, 2020."
REFERENCES,0.4398826979472141,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630–645. Springer, 2016."
REFERENCES,0.44281524926686217,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018."
REFERENCES,0.44574780058651026,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.44868035190615835,"Ilya Kostrikov and Oﬁr Nachum. Statistical bootstrapping for uncertainty estimation in off-policy
evaluation. arXiv preprint arXiv:2007.13609, 2020."
REFERENCES,0.45161290322580644,"Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Oﬁr Nachum. Ofﬂine reinforcement learning
with ﬁsher divergence critic regularization. In International Conference on Machine Learning,
pp. 5774–5783. PMLR, 2021."
REFERENCES,0.45454545454545453,"Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019."
REFERENCES,0.4574780058651026,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.4604105571847507,"Ilja Kuzborskij, Claire Vernade, Andras Gyorgy, and Csaba Szepesv´ari. Conﬁdent off-policy eval-
uation and selection through self-normalized importance weighting. In International Conference
on Artiﬁcial Intelligence and Statistics, pp. 640–648. PMLR, 2021."
REFERENCES,0.4633431085043988,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016."
REFERENCES,0.4662756598240469,"Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.46920821114369504,"Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017."
REFERENCES,0.47214076246334313,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 32:8572–8583, 2019."
REFERENCES,0.4750733137829912,"Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why
m heads are better than one: Training a diverse ensemble of deep networks.
arXiv preprint
arXiv:1511.06314, 2015."
REFERENCES,0.4780058651026393,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.4809384164222874,"Ping Li, Jinde Cao, and Zidong Wang. Robust impulsive synchronization of coupled delayed neural
networks with uncertainties. Physica A: Statistical Mechanics and its Applications, 373:261–272,
2007."
REFERENCES,0.4838709677419355,"Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015."
REFERENCES,0.4868035190615836,"Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271,
2018."
REFERENCES,0.4897360703812317,"Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In European
conference on computer vision, pp. 405–421. Springer, 2020."
REFERENCES,0.49266862170087977,Under review as a conference paper at ICLR 2022
REFERENCES,0.49560117302052786,"Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Awac: Accelerating online rein-
forcement learning with ofﬂine datasets. 2020."
REFERENCES,0.49853372434017595,"Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012."
REFERENCES,0.501466275659824,"Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A Alemi, Jascha Sohl-Dickstein,
and Samuel S Schoenholz. Neural tangents: Fast and easy inﬁnite neural networks in python.
arXiv preprint arXiv:1912.02803, 2019."
REFERENCES,0.5043988269794721,"Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. Advances in neural information processing systems, 29:4026–4034, 2016."
REFERENCES,0.5073313782991202,"Ian Osband, Benjamin Van Roy, Daniel J Russo, Zheng Wen, et al. Deep exploration via randomized
value functions. J. Mach. Learn. Res., 20(124):1–62, 2019."
REFERENCES,0.5102639296187683,"Georg Ostrovski, Marc G Bellemare, A¨aron Oord, and R´emi Munos. Count-based exploration with
neural density models. In International conference on machine learning, pp. 2721–2730. PMLR,
2017."
REFERENCES,0.5131964809384164,"Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019."
REFERENCES,0.5161290322580645,"Nick Pawlowski, Andrew Brock, Matthew CH Lee, Martin Rajchl, and Ben Glocker. Implicit weight
uncertainty in neural networks. arXiv preprint arXiv:1711.01297, 2017."
REFERENCES,0.5190615835777126,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019."
REFERENCES,0.5219941348973607,"Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis
Antonoglou, and David Silver. Online and ofﬂine reinforcement learning by planning with a
learned model. arXiv preprint arXiv:2104.06294, 2021."
REFERENCES,0.5249266862170088,"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-
plicit neural representations with periodic activation functions. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.5278592375366569,"Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in ﬁnite mdps: Pac
analysis. Journal of Machine Learning Research, 10(11), 2009."
REFERENCES,0.530791788856305,"P. Thomas, G. Theocharous, and M. Ghavamzadeh.
High conﬁdence off-policy evaluation.
In
Proceedings of the 29th Conference on Artiﬁcial Intelligence, 2015."
REFERENCES,0.533724340175953,"Linh Tran, Bastiaan S Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V Dillon, Jasper Snoek,
Stephan Mandt, Tim Salimans, Sebastian Nowozin, and Rodolphe Jenatton. Hydra: Preserving
ensemble diversity for model distillation. arXiv preprint arXiv:2001.04694, 2020."
REFERENCES,0.5366568914956011,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.5395894428152492,"Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020."
REFERENCES,0.5425219941348973,"Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efﬁcient
ensemble and lifelong learning. arXiv preprint arXiv:2002.06715, 2020."
REFERENCES,0.5454545454545454,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.5483870967741935,"Greg Yang and Edward J Hu. Feature learning in inﬁnite-width neural networks. arXiv preprint
arXiv:2011.14522, 2020."
REFERENCES,0.5513196480938416,Under review as a conference paper at ICLR 2022
REFERENCES,0.5542521994134897,"Mengjiao Yang, Bo Dai, Oﬁr Nachum, George Tucker, and Dale Schuurmans. Ofﬂine policy selec-
tion under uncertainty. arXiv preprint arXiv:2012.06919, 2020."
REFERENCES,0.5571847507331378,"Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. Meta-inverse reinforcement learning with
probabilistic context variables. arXiv preprint arXiv:1909.09314, 2019."
REFERENCES,0.5601173020527859,"Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or
more networks per bit? In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4923–4932, 2019."
REFERENCES,0.5630498533724341,Under review as a conference paper at ICLR 2022
REFERENCES,0.5659824046920822,"A
D4RL GYM LOCOMOTION BENCHMARK TABLE"
REFERENCES,0.5689149560117303,"Domain
BC
CQL
α
F-BRC (no bonus)
α
MSG (N = 64) β
α
Data Top 25"
REFERENCES,0.5718475073313783,"halfcheetah-medium 36.1 50.1 ± 1.2
0
45.7 ± 0.5
0.01
50.6 ± 1.5
0
0
37.0 ± 2.0
hopper-medium
29.0 90.2 ± 10.5 2.58
99.0 ± 4.1
0.012
88.2 ± 11.6
-1 0.5
101.1 ± 1.1
walker2d-medium
6.6
82.8 ± 0.6 3.38
81.6 ± 0.2
0.016
83.6 ± 2.0
-1 0.1
77.6 ± 1.4
halfcheetah-mixed
38.4
47.6 ± 2.5
0.67
45.3 ± 0.1
0.01
52.1 ± 1.7
0
0
38.1 ± 4.1
hopper-mixed
11.8 61.1 ± 36.8 0.67
29.3 ± 0.8
0.019
82.1 ± 24.2
-2
0
101.8 ± 1.5
walker2d-mixed
11.3 20.5 ± 3.8 3.38
28.4 ± 5.7
0.038
24.0 ± 9.4
-2 0.1
50.6 ± 3.9"
REFERENCES,0.5747800586510264,"Table 3: Results on Gym subset of the D4RL benchmark. For MSG we use an ensemble size of N = 64. For
fairness of comparison, F-BRC is ran without adding a survival reward bonus. For each method we also report
the hyperparameter value used. Full experimental details are presented in Appendix C."
REFERENCES,0.5777126099706745,"B
EFFICIENT ENSEMBLES & ENSEMBLE ABLATIONS TABLE"
REFERENCES,0.5806451612903226,"Domain
CQL
Deep Ens.
MIMO-0.5
MIMO-1
Multi-Head
Batch Ens
N = 1
Shared LCB
Shared Min"
REFERENCES,0.5835777126099707,"halfcheetah-medium
50.1
50.6
40.0
44.4
49.8
40.2
48.6
47.9
36.0
hopper-medium
90.2
88.2
60.2
65.1
94.8
41.6
50.2
30.0
43.6
walker2d-medium
82.8
83.6
80.7
83.6
84.5
13.2
84.9
85.5
0.0
halfcheetah-mixed
47.6
52.1
0.0
6.7
49.4
43.0
49.6
51.0
2.3
hopper-mixed
61.1
82.1
22.7
32.5
45.5
35.0
30.5
32.5
6.7
walker2d-mixed
20.5
24.0
14.5
16.8
15.9
12.0
11.5
26.3
0.0"
REFERENCES,0.5865102639296188,"antmaze-large-diverse
15.8
64.8
11.0
26.0
33.5
0.0
noisy 30.5
0.0
0.0
antmaze-large-play
14.9
68.8
7.5
20.5
27.5
0.0
noisy 11.0
0.0
0.0"
REFERENCES,0.5894428152492669,Table 4: Results for efﬁcient ensembles and ensemble ablations.
REFERENCES,0.592375366568915,"C
D4RL GYM DETAILS"
REFERENCES,0.5953079178885631,"All policies and Q-functions are a 3 layer neural network with relu activations and hidden layer size
256. The policy output is a normal distribution that is squashed to [−1, 1] using the tanh function.
All methods were trained for 1M steps. CQL and MSG are trained with behavioral cloning (BC) for
the ﬁrst 50K steps. F-BRC pretrains with 1M steps of BC."
REFERENCES,0.5982404692082112,"CQL, F-BRC, and MSG are tuned with an equal hyperparameter search budget. Running the best
found hyperparameter using 5 new random seeds. Each run is evaluated for 10 episodes, and the
mean and standard deviation across the 5 runs is reported in the table. For fairness of comparison,
F-BRC is ran without adding a survival reward bonus. MSG and CQL are implemented in our code,
and for F-BRC we use the opensourced codebase."
REFERENCES,0.6011730205278593,"C.1
HYPERPARAMETER SEARCH"
REFERENCES,0.6041055718475073,"For all methods we performed hyperparameter search using 2 seeds. Based on the results of the 2
seeds, we chose the hyperparameter to use, and using this choice of hyperparameter we ran experi-
ments with 5 new random seed."
REFERENCES,0.6070381231671554,"MSG
β ∈{0., −1., −2., −4. −8.}, α ∈{0., 0.1, 0.5, 1., 2.}"
REFERENCES,0.6099706744868035,"CQL
α ∈{0., 0.1} + np.exp(np.linspace(np.log(0.1), np.log(10.), steps = 23))"
REFERENCES,0.6129032258064516,"F-BRC
α ∈{0.} + np.exp(np.linspace(np.log(0.01), np.log(10.0), steps = 24))"
REFERENCES,0.6158357771260997,Under review as a conference paper at ICLR 2022
REFERENCES,0.6187683284457478,(a) Visual depiction of MIMO Ensemble
REFERENCES,0.6217008797653959,"(b) D4RL antmaze tasks. Figure taken from
Fu et al. (2020). 0 250 500 750 1000"
REFERENCES,0.624633431085044,"cartpole.swingup
finger.turn_hard
fish.swim
manipulator.insert_ball manipulator.insert_peg
walker.stand
walker.walk
cheetah.run
humanoid.run
mean"
REFERENCES,0.6275659824046921,"BC
MSG (N = 64)
CRR
MuZero Unplugged"
REFERENCES,0.6304985337243402,"Figure 4: Results for DM Control Suite subset of the RL Unplugged benchmark (Gulcehre et al., 2020). We
note that: 1) the architecture we used are smaller by a factor of approximately 60x, 2) CRR results are reported
by their best checkpoint throughout training which differs from ours, BC, and MuZero Unplugged which report
performance at the end of training. Baseline results taken from Schrittwieser et al. (2021)."
REFERENCES,0.6334310850439883,"D
ANTMAZE DETAILS"
REFERENCES,0.6363636363636364,"We use the same hyperparameter search procedure as Gym results, with same architectures. The
only difference is that models are trained for 2M steps and at evaluation time they are rolled out for
100 episodes instead of 10."
REFERENCES,0.6392961876832844,"In prior work, the rewards in the ofﬂine dataset are converted using the formula 4(r −0.5). We also
use the same reward transformation."
REFERENCES,0.6422287390029325,"E
RL UNPLUGGED"
REFERENCES,0.6451612903225806,"E.1
DM CONTROL SUITE TASKS"
REFERENCES,0.6480938416422287,"The networks used in Gulcehre et al. (2020) for DM Control Suite Tasks are very large relative to
the networks we used in the D4RL benchmark; roughly the networks contain 60x more parame-
ters. Using a large ensemble size with such architectures requires training using a large number of
devices. Furthermore, since in our experiments with efﬁcient ensemble approximations we did not
ﬁnd a suitable alternative to deep ensembles (section 5.3.1), we decided to use the same network
architectures and N = 64 as in the D4RL setting (enabling single-GPU training as before)."
REFERENCES,0.6510263929618768,"Our hyperparameter search procedure was similar to before, where we ﬁrst performed a coarse
search using 2 random seeds and hyperparameters β ∈{−1., −2., −4. −8.}, α ∈{0., 0.5}, and for
the best found hyperparameter, ran ﬁnal experiments with 5 new random seeds."
REFERENCES,0.6539589442815249,"F
THEORY"
REFERENCES,0.656891495601173,"For our notation to match Lee et al. (2019), throughout this section we use the f to denote the
network Q, and we use x instead of (s, a)."
REFERENCES,0.6598240469208211,"In Lee et al. (2019) (section 2.4) it is shown that when training an inﬁnitely wide neural network
to perform regression using mean squared error, subject to technical conditions on the learning rate
used, the predictions of the trained network are equivalent to if we had linearized (Taylor expanded)"
REFERENCES,0.6627565982404692,Under review as a conference paper at ICLR 2022
REFERENCES,0.6656891495601173,"the network at its initialization, and trained the linearized network instead. This means that after t
iterations of our policy evaluation procedure, ∀x, t, f lin
t (x) = ft(x), where"
REFERENCES,0.6686217008797654,"f lin
t (x) := f0(x) + ∇θf0(x)|θ=θ0 ωt
ωt := θt −θ0"
REFERENCES,0.6715542521994134,"Hence we only need to study the evolution of the linearized network f lin
t
across iterations. The
theorems in the main manuscript are direct corollaries of the following two derivations."
REFERENCES,0.6744868035190615,"Below, we will overload some notation. When f lin is applied to a matrix we mean that we apply f lin"
REFERENCES,0.6774193548387096,"to each row of the matrix and stack the results into a vector. By ˆΘ0(x, X) we mean to treat x as a
row matrix."
REFERENCES,0.6803519061583577,"F.1
CLOSED FORM WHEN ENSEMBLE MEMBERS USE THEIR OWN TARGETS"
REFERENCES,0.6832844574780058,"For a single – inﬁnitely wide – ensemble member, using the equations in Lee et al. (2019) (sec-
tion 2.2, equations 9-10-11) we can write the following recursive updates for our policy evaluation
procedure"
REFERENCES,0.6862170087976539,"ˆΘ−1
0
:= ˆΘ0(X, X)−1
(7)"
REFERENCES,0.6891495601173021,"C := ˆΘ0(X ′, X)ˆΘ−1
0
(8)"
REFERENCES,0.6920821114369502,"Yt = R + γf lin
t (X ′)
(9)"
REFERENCES,0.6950146627565983,"f lin
t+1(X) = Yt
(10)"
REFERENCES,0.6979472140762464,"∀x, f lin
t+1(x) = f0(x) + ˆΘ0(x, X)ˆΘ−1
0 (Yt −f0(X))
(11)"
REFERENCES,0.7008797653958945,"f lin
t+1(X ′) = f0(X ′) + ˆΘ0(X ′, X)ˆΘ−1
0 (Yt −f0(X))
(12)"
REFERENCES,0.7038123167155426,"= f0(X ′) + ˆΘ0(X ′, X)ˆΘ−1
0 (R + γf lin
t (X ′) −f0(X))
(13)"
REFERENCES,0.7067448680351907,"= f0(X ′) + CR + γCf lin
t (X ′) −Cf0(X)
(14)"
REFERENCES,0.7096774193548387,"= f0(X ′) + CR −Cf0(X) + γCf lin
t (X ′)
(15)
= . . .
(16)"
REFERENCES,0.7126099706744868,"= (1 + . . . + γtCt)

f0(X ′) + CR −Cf0(X)

+ (γC)t+1f0(X ′)
(17)"
REFERENCES,0.7155425219941349,"E[f lin
t+1(X ′)] = (1 + . . . + γtCt)CR
(18)"
REFERENCES,0.718475073313783,"Var[f lin
t+1(X ′)] = E
h
(1 + . . . + γtCt)(f0(X ′) −Cf0(X)) + (γC)t+1f0(X ′)
2i
(19)"
REFERENCES,0.7214076246334311,"≈E
h
(1 + . . . + γtCt)(f0(X ′) −Cf0(X))
2i
as t →∞
(20)"
REFERENCES,0.7243401759530792,"F.2
CLOSED FORM WHEN ENSEMBLE MEMBERS USE SHARED MEAN TARGETS"
REFERENCES,0.7272727272727273,Let us consider the setting where all ensemble memebers use their mean as the target.
REFERENCES,0.7302052785923754,Under review as a conference paper at ICLR 2022
REFERENCES,0.7331378299120235,"ˆΘ−1
0
:= ˆΘ0(X, X)−1
(21)"
REFERENCES,0.7360703812316716,"C := ˆΘ0(X ′, X)ˆΘ−1
0
(22)"
REFERENCES,0.7390029325513197,"Yt = R + γf lin
t (X ′)
(23)"
REFERENCES,0.7419354838709677,"f lin
t+1(X) = Yt
(24)"
REFERENCES,0.7448680351906158,"∀x, f lin
t+1(x) = f0(x) + ˆΘ0(x, X)ˆΘ−1
0 (Yt −f0(X))
(25)"
REFERENCES,0.7478005865102639,"Yt = R + γ
E
ensemble[f lin
t (X ′)]
(26)"
REFERENCES,0.750733137829912,"= R + γ
E
ensemble[f0(X ′)] + γCYt−1 −γC
E
ensemble[f0(X)]
(27)"
REFERENCES,0.7536656891495601,"= R + γCYt
(28)
= . . .
(29)"
REFERENCES,0.7565982404692082,"= (1 + . . . + γtCt)R
(30)"
REFERENCES,0.7595307917888563,"F.3
CLOSED FORM WHEN ENSEMBLE MEMBERS USE SHARED LCB TARGETS"
REFERENCES,0.7624633431085044,Let us consider the setting where all ensemble memebers use shared LCB as the target.
REFERENCES,0.7653958944281525,"ˆΘ−1
0
:= ˆΘ0(X, X)−1
(31)"
REFERENCES,0.7683284457478006,"C := ˆΘ0(X ′, X)ˆΘ−1
0
(32)"
REFERENCES,0.7712609970674487,"Yt = LCB

R + γf lin
t (X ′)

(33)"
REFERENCES,0.7741935483870968,"= R + γLCB

f lin
t (X ′)

(34)"
REFERENCES,0.7771260997067448,"f lin
t+1(X) = Yt
(35)"
REFERENCES,0.7800586510263929,"∀x, f lin
t+1(x) = f0(x) + ˆΘ0(x, X)ˆΘ−1
0 (Yt −f0(X))
(36)"
REFERENCES,0.782991202346041,"E[ft+1(X ′)] = E[f0(X ′) + C · (Yt −f0(X))]
(37)
= C · Yt
(38)"
REFERENCES,0.7859237536656891,"Var[ft+1(X ′)] = E
h
f0(X ′) −C · f0(X)
2i
= constant
(39)"
REFERENCES,0.7888563049853372,"A :=
√"
REFERENCES,0.7917888563049853,"constant
(40)"
REFERENCES,0.7947214076246334,"LCB

f lin
t+1(X ′)

= C · Yt −A
(41)"
REFERENCES,0.7976539589442815,"Yt = R + γLCB

f lin
t (X ′)

(42)"
REFERENCES,0.8005865102639296,"= R + γCYt −γA
(43)"
REFERENCES,0.8035190615835777,"= (1 + . . . + γtCt)R −(1 + . . . + γt−1Ct−1)γA + γt+1CtLCB

f0(X ′)
 (44)"
REFERENCES,0.8064516129032258,"≈(1 + . . . + γtCt)R −(1 + . . . + γt−1Ct−1)γA
as t →∞
(45)"
REFERENCES,0.8093841642228738,"LCB

f lin
t+1(X ′)

≈(1 + . . . + γtCt)CR −(1 + . . . + γtCt)A
(46)"
REFERENCES,0.8123167155425219,"F.4
WHY IS INDEPENDENT PREFERABLE TO SHARED-LCB"
REFERENCES,0.8152492668621701,"An important question to consider is why Independent ensembles should be preferred over Shared-
LCB ensembles? Here we present our reasoning for why we Independent ensembles would be
preferable to Shared-LCB ensembles."
REFERENCES,0.8181818181818182,"With the derivations in the above sections, we can compare the difference amongst uncertainty
estimation techniques.
The key comparison needed is to understand the difference between"
REFERENCES,0.8211143695014663,Under review as a conference paper at ICLR 2022
REFERENCES,0.8240469208211144,"LCB

f lin
t+1(X ′)

under Independent vs. Shared-LCB settings. As a reminder, X ′ is a matrix where"
REFERENCES,0.8269794721407625,"each row contains (s, π(s)), and ft+1 = f lin
t+1 under the inﬁnite-width regime. From the above
equations we have:"
REFERENCES,0.8299120234604106,"Independent: LCB

f lin
t+1(X ′)

≈(1 + . . . + γtCt)CR − r"
REFERENCES,0.8328445747800587,"E
h
(1 + . . . + γtCt)(f0(X ′) −Cf0(X))
2i"
REFERENCES,0.8357771260997068,"Shared-LCB: LCB

f lin
t+1(X ′)

≈(1 + . . . + γtCt)CR −(1 + . . . + γtCt) r"
REFERENCES,0.8387096774193549,"E
h
f0(X ′) −Cf0(X)
2i"
REFERENCES,0.841642228739003,where the square and square-root operations are applied element-wise to the vector values.
REFERENCES,0.844574780058651,"As can be seen, the equations for the lower-conﬁdence bound (LCB) in both settings are very similar,
with the main difference being in the second terms which correspond to the “pessimism” terms. In
the inﬁnite-width setting, the only source of randomness is in the initialization of the networks. This
fact presents itself in the two equations above, where the random variables f0(X ′)−Cf0(X) produce
the uncertainty in the ensemble of networks; regardless of using Independent or Shared-LCB, after
any iteration t we have,"
REFERENCES,0.8475073313782991,"Yt = R + γft(X ′)
(47)"
REFERENCES,0.8504398826979472,"ft+1(X ′) = f0(X ′) + C(Yt −f0(X))
(48)"
REFERENCES,0.8533724340175953,"= CYt + (f0(X ′) −Cf0(X))
(49)"
REFERENCES,0.8563049853372434,"E[ft+1(X ′)] = CE[Yt]
(50)"
REFERENCES,0.8592375366568915,"Thus, f0(X ′) −Cf0(X) represents the random value accumulated in each iteration, and they are
accumulated through backups by the geometric term (1 + . . . + γtCt)."
REFERENCES,0.8621700879765396,"Here is where we observe the key difference between Independent and Shared-LCB: whether the
term (1+. . .+γtCt) is applied inside or outside the expectation. In Independent ensembles, the ran-
domness/uncertainties is ﬁrst backed-up by the geometric term and afterward the standard-deviation
is computed. In Shared-LCB however, ﬁrst the standard-deviation of the randomness/uncertainties
is computed, and afterwards this value is backed up. Not only do we believe that the former (Inde-
pendent) makes more sense intuitively, but in the case of Shared-LCB, the second term may contain
negative values which would actually result in an optimism bonus!"
REFERENCES,0.8651026392961877,"F.5
COMPARING THE STRUCTURE OF UNCERTAINTIES UNDER INDEPENDENT AND
SHARED-MEAN"
REFERENCES,0.8680351906158358,"The above results enable us to compute in closed form the predictions of the inﬁnite-width networks
under different training regimes ∀x."
REFERENCES,0.8709677419354839,"The equations above present the closed form expressions for the predictions of each ensemble mem-
ber after t+1 iterations of the policy evaluation procedure. Since the ensemble members only differ
in their weight initialization (random draws from the initial weight distribution), the random vari-
ables are f0(x), f0(X), f0(X ′). As mentioned in the main text, the neural tangent ˆΘ0 is identical
across ensemble members due to being in the inﬁnite-width regime (Jacot et al., 2018)."
REFERENCES,0.873900293255132,"Since ∀x,
E
ensemble[f0(x)] = 0 (Lee et al., 2019; 2017; Matthews et al., 2018), the expected values"
REFERENCES,0.8768328445747801,"of ft+1(x) is identical in for both methods of computing TD targets,"
REFERENCES,0.8797653958944281,"E
ensemble[Yt+1] = (1 + . . . + γtCt)R
(51)"
REFERENCES,0.8826979472140762,"∀x,
E
ensemble[ft+1(x)] = ˆΘ0(x, X)ˆΘ−1
0 (1 + . . . + γtCt)R
(52)"
REFERENCES,0.8856304985337243,"However, the expression for variances is very different. When the targets used are independent
we have,"
REFERENCES,0.8885630498533724,"∀x,
Var
ensemble[ft+1(x)]"
REFERENCES,0.8914956011730205,"=
E
ensemble"
REFERENCES,0.8944281524926686,"
f0(x) + ˆΘ0(x, X)ˆΘ−1
0
  
1 + . . . + γtCt  
γf0(X ′) −f0(X)
2
(53)"
REFERENCES,0.8973607038123167,Under review as a conference paper at ICLR 2022
REFERENCES,0.9002932551319648,"In contrast, when the targets are shared mean of targets, we have,"
REFERENCES,0.9032258064516129,"∀x,
Var
ensemble[ft+1(x)] =
E
ensemble"
REFERENCES,0.906158357771261,"
f0(x) + ˆΘ0(x, X)ˆΘ−1
0

−f0(X)
2
(54)"
REFERENCES,0.9090909090909091,"The matrix C captures a notion of similarity between the (s, a) in X, and the (s′, π(s′)) in X ′. Thus,
the term Ct has the interpretation of where the policy π(s) would ﬁnd itself t steps into the future,
and (1 + . . . + γtCt) can be interpreted as the policy’s discounted state-action visitation, but in the
feature-space given by the neural network architecture. Since in ensembles the standard deviation of
predictions quantiﬁes the amount of uncertainty, the expression in equation 53 tells us that when
the targets are independent, the ensemble of Q-functions “backs up the uncertainties through
dynamic programming with respect to the policy being evaluated”."
REFERENCES,0.9120234604105572,"In contrast, when the targets are shared, the closed form expression for ft+1(x) is equivalent
to an oracle presenting us with targets (1 + . . . + γtCt)R for training examples X, and
training the ensemble members using mean squared error regression to regress these values."
REFERENCES,0.9149560117302052,"G
ADDITIONAL TOY EXPERIMENTS & DETAILS"
REFERENCES,0.9178885630498533,"G.1
ADDITIONAL IMPLEMENTATION DETAILS FOR FIGURE 1"
REFERENCES,0.9208211143695014,"To evaluate the quality of uncertainties obtained from different Q-function ensembling approaches,
we create N = 64 Q-function networks, each being a one hidden layer neural network with hidden
dimension 512 and tanh activation. The initial weight distribution is a fan-in truncated normal dis-
tribution with scale 10.0, and the initial bias distribution is fan-in truncated normal distribution with
scale 0.05. We did not ﬁnd results with other activation functions and choices of initial weight and
bias distribution to be qualitatively different. We use discount γ = 0.99 and the networks are opti-
mized using the Adam (Kingma & Ba, 2014) optimizer with learning rate 1e-4. In each iteration,
we ﬁrst compute the TD targets using the desired approach (e.g. independent vs. shared targets)
and then ﬁt the Q-functions to their respective targets with 2000 steps of full batch gradient descent.
We train the networks for 1000 such iterations (for a total of 2000 × 1000 gradient steps). Note that
we do not use target networks. Given the small size of networks and data, these experiments can be
done within a few minutes using a single GPU in Google Colaboratory which we will also
opensource."
REFERENCES,0.9237536656891495,"G.2
ADDITIONAL TOY EXPERIMENTS"
REFERENCES,0.9266862170087976,"The toy experiment presented in section 4.2 uses a single-hidden layer ﬁnite-width neural network
architecture with tanh activations, uses the “standard weight parameterization” (i.e. the weight
parameterization used in practice) as opposed to the NTK parameterization (Novak et al., 2019), and
optimizes the networks using the Adam optimizer (Kingma & Ba, 2014). While this setup is close
to the practical setting and demonstrates the relevance of our proposal for independent ensembles
for the practical setting, an important question posed by our reviewers is how close these results are
too the theoretical predictions present in 4.1. To answer this question, we present the following set
of results."
REFERENCES,0.9296187683284457,"Using the identical MDP and ofﬂine data as before, we implement 1 hidden layer neural networks
with erf non-lineartiy. The networks are implemented using the Neural Tangents library (Novak
et al., 2019), and use the NTK parameterization. The networks in the ensemble are optimized using
full-batch gradient descent with learning rate 1 for 500 steps of FQE Fonteneau et al. (2013), where
each in each step the networks are updated for 1000 gradient steps. We vary the width of the
networks from 32 to 32768 in increments of a factor of 4, plotting the mean and standard deviation
of the network predictions. The ensemble size is set to N = 16, except for width 32768 where
N = 4."
REFERENCES,0.9325513196480938,"We compare the results from ﬁnite-width networks to computing the mean and standard deviation in
closed for using 4.1. Using the Neural Tangents library (Novak et al., 2019) we obtained the NTK
for the architecture described in the previous paragraph (1 hidden layer with erf non-linearity).
We found that the matrix inversion required in our equations results in numerical errors. Hence, we
make the modiﬁcation Θ(X, X) ←Θ(X, X) + 1e-3 · I."
REFERENCES,0.9354838709677419,Under review as a conference paper at ICLR 2022
REFERENCES,0.9384164222873901,"Figure 5: Comparing results of ﬁnite-width networks to closed form equations derived in Theorem
4.1. In the NTK parameterization, as width →∞, the structure of the variances collapse and resem-
ble the inﬁnite-width closed-form results. We believe this is due to inﬁnite-width networks under
the NTK regime not being able to learn features (Yang & Hu, 2020). Supporting this hypothesis,
we observe that networks parameterized by the Maximal Parameterization of Yang & Hu (2020)
maintain the desired uncertainty structure as the width of the networks grows larger."
REFERENCES,0.9413489736070382,"Figure 5 presents our results. As the width of the networks grow larger, the shape of the uncertainties
becomes more similar to our closed-form equations (i.e. the variances become very small). While
we do not have a rigorous explanation for why ﬁnite-width networks exhibit intuitively more de-
sirable behaviors, we present below a strong hypothesis backed by empirical evidence. We believe
rigorously answering this question is an incredibly interesting avenue for future work."
REFERENCES,0.9442815249266863,"Hypothesis:
Inﬁnite-width networks in the NTK parameterization/regime do not learn data-
dependent features (Yang & Hu, 2020). Furthermore, as can be seen in the equations of Theorem
4.1, the variances depend the function values and features (kernel, C matrix, etc.) at initialization.
Yang & Hu (2020) present a different approach for parameterizing inﬁnite-width networks called
the “Maximal Parameterization”, which enables iniﬁnite-width networks to learn data-dependent
features. We perform the same experiment as above, by replacing the NTK-parameterized networks
with Maximal Parameterizations. Figure 5 presents our empirical results for network widths from
32 to 32768. Excitingly, we observe that with Maximial Parameterization, even our widest
networks recover the intuitively desired form of uncertainty described in section 4.2! The so-
lutions of these networks also appear much more accurate, particularly on the right hand side of the
plot where with the stepped structure; each step appears to be approximately 0.1 in width, which is
the action of the policy being evaluated."
REFERENCES,0.9472140762463344,Under review as a conference paper at ICLR 2022
REFERENCES,0.9501466275659824,"Q
arch π
D alg"
REFERENCES,0.9530791788856305,"(a) Q-function generative process: the graphical model
above represents the induced distribution over Q-
functions when conditioning on a particular policy
evaluation algorithm, policy, ofﬂine dataset, and Q-
function network architecture."
REFERENCES,0.9560117302052786,"Q
arch
prior π
D alg N"
REFERENCES,0.9589442815249267,(b) An example of an interesting extension
REFERENCES,0.9618768328445748,"H
STATISTICAL MODEL"
REFERENCES,0.9648093841642229,"An interesting question posed by reviewers of our work was “[W]hatever formal reasoning system
we’d like to use, what is the ideal answer, given access to arbitrary computational resources, so that
approximations are unnecessary? I.e., how do we quantify our uncertainty about the MDP and value
function before seeing data, and how do we quantify it after seeing data?”"
REFERENCES,0.967741935483871,"It is important to begin by clarifying what is the mathematical object we are trying to obtain un-
certainties over. In this work, we do not quantify uncertainties about any aspects of the MDP itself
(although this is an interesting question which comes up in model-based methods as well as other
settings such as Meta-Learning (Yu et al., 2019; Ghasemipour et al., 2019)). Our goal in this work
is to directly estimate Qπ(s, a) = r(s, a) + γ · Es′∼MDP,a′∼π[Q(s′, a′)], for a ∼π(s), and obtain
uncertainties about Qπ(s, a)."
REFERENCES,0.9706744868035191,"Let Q(s, a) be a predictor S ×A →R that needs to be evaluated on – and hopefully generalize well
to – (s, a) /∈D (D being the ofﬂine dataset). When we choose to represent Q(s, a) using neural
networks, Gaussian Processes, or K-nearest-neighbours, we are not just making approximations for
computational reasons, but are actually choosing a function class which we believe will generalize
well to unseen (s, a)."
REFERENCES,0.9736070381231672,"One practical example of learning Q-functions is to use Fitted Q iteration (Fonteneau et al., 2013)
on the provided data using gradient descent with a particular neural network architecutre. Due to
the random weight initialization, this procedure induces a distribution on the Q-functions which is
captured by the probabilistic graphical model (PGM) in Figure 6a. In other words, by conditionining
on the policy, data, architecture, and policy evaluation algorithm, we are imposing a belief over Q-
functions. Note that this is essentially the same justiﬁcation as using ensembles in supervised deep
learning, where ensembles are state-of-the-art for accuracy and calibration (Ovadia et al., 2019). For
the sake of theoretical analysis (Section 4), we studied this belief distribution under the inﬁnite-width
NTK network setting, in which case the distribution over Q-functions is a Gaussian Process."
REFERENCES,0.9765395894428153,"The focus of this work is to ask the question: “Under this imposed belief, what should the policy
update be?”. Our proposed answer is to optimize the policy with respect to the lower-conﬁdence
bound of our beliefs: In an actor-critic setup, the policy optimization objective takes a form
like maxπ Ed(s)[Q(s, π(s))], where d(s) is some distribution over states (e.g. initial state dis-
tribution, or the states in the ofﬂine dataset D, etc.). Thus, our proposed policy objective takes
the form maxπ LCB

Ed(s)[Q(s, π(s))]

, and for practical reasons, in MSG we convert this to"
REFERENCES,0.9794721407624634,"maxπ Ed(s)
h
LCB

Q(s, π(s))
i
(which is a lower-bound of the ﬁrst)."
REFERENCES,0.9824046920821115,"The graphical model in Figure 6a also highlights an example of interesting future directions: Con-
sider an ofﬂine RL setup where we keep track of the various policies generating the data, and their
Q-functions. Then, by imposing a prior on the architecture we can ﬁrst infer a posterior distribu-
tion over architectures, then learn the Q-function of a new policy under the posterior architecture
distribution."
REFERENCES,0.9853372434017595,Under review as a conference paper at ICLR 2022
REFERENCES,0.9882697947214076,"Figure 7: Results on the six antmaze domains. Each color represents the mean and standard devia-
tion of results across 5 seeds for a particular hyperparameter setting (β ∈−4, −8 and α ∈0, 0.1).
As can be seen, our results are quite robust across a wide range of hyperparameter values."
REFERENCES,0.9912023460410557,"I
PRACTICAL HYPERPARAMETER TUNING ADVICE"
REFERENCES,0.9941348973607038,"In reporting our results, we preferred to also report the hyperparameter values that we used, which
may given an impression of signiﬁcant hyperparameter tuning. We emphasize that our results are
generally robust across a range of hyperparameter values. As an example, in Figure 7 we present
results on the six antmaze domains. Each color represents the mean and standard deviation of results
across 5 seeds for a particular hyperparameter setting (β ∈−4, −8 and α ∈0, 0.1). As can be seen,
our results are quite robust across a wide range of hyperparameter values."
REFERENCES,0.9970674486803519,"Here, we include practical advice on hyperparameter tuning when faced with a new domain. For a
given new domain, we would ﬁrst use low β values: β ∈{−4, −8}. If β = −4 is clearly better
than β = −8, then we would guess that in this new domain high pessimism may not be necessary
and explore the use of β ∈{0, −1, −2}. For the α hyperparameter, we found that {0, 0.1, 0.5, 1.0}
is a wide enough range to explore. If α = 1.0 was clearly better than the other values, then this
would indicate to us that maybe the ofﬂine dataset is narrow (lacks diversity, e.g. imitation learning
datasets) and then we could increase the value of α. Generally, we would prefer to lower β before
attempting to increase α, but we never tried β < −8."
