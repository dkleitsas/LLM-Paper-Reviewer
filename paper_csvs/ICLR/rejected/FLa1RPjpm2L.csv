Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0025974025974025974,"Model-based reinforcement learning methods achieve signiÔ¨Åcant sample efÔ¨Åciency
in many tasks, but their performance is often limited by the existence of the model
error. To reduce the model error, previous works use a single well-designed network
to Ô¨Åt the entire environment dynamics, which treats the environment dynamics as a
black box. However, these methods lack to consider the environmental decomposed
property that the dynamics may contain multiple sub-dynamics, which can be
modeled separately, allowing us to construct the world model more accurately. In
this paper, we propose the Environment Dynamics Decomposition (ED2), a novel
world model construction framework that models the environment in a decomposing
manner. ED2 contains two key components: sub-dynamics discovery (SD2) and
dynamics decomposition prediction (D2P). SD2 discovers the sub-dynamics in an
environment and then D2P constructs the decomposed world model following the
sub-dynamics. ED2 can be easily combined with existing MBRL algorithms and
empirical results show that ED2 signiÔ¨Åcantly reduces the model error and boosts
the performance of the state-of-the-art MBRL algorithms on various continuous
control tasks. 1"
INTRODUCTION,0.005194805194805195,"1
INTRODUCTION"
INTRODUCTION,0.007792207792207792,"Reinforcement Learning (RL) is a general learning framework for solving sequential decision-making
problems and has made signiÔ¨Åcant progress in many Ô¨Åelds (Mnih et al., 2015; Silver et al., 2016;
Vinyals et al., 2019; Schrittwieser et al., 2019). In general, RL methods can be divided into two
categories regarding whether a world model is constructed for the policy deriving: model-free RL
(MFRL) and model-based RL (MBRL). MFRL methods train the policy by directly interacting
with the environment, which results in good asymptotic performance but low sample efÔ¨Åciency. By
contrast, MBRL methods improve the sample efÔ¨Åciency by modeling the environment, but often with
limited asymptotic performance and suffer from the model error (Lai et al., 2020; Kaiser et al., 2020)."
INTRODUCTION,0.01038961038961039,"Existing MBRL algorithms can be divided into four categories according to the paradigm they follow:
the Ô¨Årst category focuses on generating imaginary data by the world model and training the policy
with these data via MFRL algorithms (Kidambi et al., 2020; Yu et al., 2020); the second category
leverages the differentiability of the world model, and generates differentiable trajectories for policy
optimization (Deisenroth & Rasmussen, 2011; Levine & Koltun, 2013; Zhu et al., 2020); the third
category aims to obtain an accurate value function by generating imaginations for temporal difference
(TD) target calculation (Buckman et al., 2018; Feinberg et al., 2018); the last category of works
focuses on reducing the computational cost of the policy deriving by combining the optimal control
algorithm (e.g. model predictive control) with the learned world models (Chua et al., 2018; Okada &
Taniguchi, 2019; Argenson & Dulac-Arnold, 2020). Regardless of paradigms, the performance of
all existing MBRL algorithms depends on the accuracy of the world model. The more accurate the
world model is, the more reliable data can be generated, and Ô¨Ånally, the better policy performance
can be achieved. Therefore, improving the world model accuracy is critical in MBRL."
INTRODUCTION,0.012987012987012988,"To this end, various techniques have been proposed to improve the model accuracy. For example,
rather than directly predict the next state, some works construct a world model for the state change"
INTRODUCTION,0.015584415584415584,1Our code is open source and available at https://github.com/ED2-source-code/ED2
INTRODUCTION,0.01818181818181818,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02077922077922078,"According 
to role"
INTRODUCTION,0.023376623376623377,"Traced to 
action"
INTRODUCTION,0.025974025974025976,"(a) Cheetah environment
(b) Decomposability
(c) Traceability
(d) Model error"
INTRODUCTION,0.02857142857142857,"According 
to position"
INTRODUCTION,0.03116883116883117,"Traced to 
action ‚Ä¶‚Ä¶
‚Ä¶‚Ä¶"
INTRODUCTION,0.033766233766233764,"Figure 1: (a) The Cheetah task with six action dimensions. (b) The dynamics can be decomposed into
multiple sub-dynamics in various ways, each sub-dynamics is described with different background
colors. (c) Dynamics can be traced to the impact caused by the action, and for each sub-dynamics,
we show the meanings of the action dimensions it traced to. (d) The model error comparison on the
Cheetah task of Dreamer and D2P-Dreamer methods (D2P-Dreamer-Role/Position correspond to
decompose the dynamics according to role/position and model each sub-dynamics separately)."
INTRODUCTION,0.03636363636363636,"prediction (Luo et al., 2019; Kurutach et al., 2018). Model ensemble is also widely used in model
construction for uncertainty estimation, which provides a more reliable prediction (Janner et al., 2019;
Pan et al., 2020). To reduce the model error in long trajectory generation, optimizing the multi-step
prediction errors is also an effective technique (Hafner et al., 2019). However, these techniques
improve the environment modeling in a black-box way, which ignores the inner decomposed structure
of environment dynamics. For example, Figure 1 (a) shows the Cheetah task from DeepMindControl
(DMC) Suite tasks, where the dynamics can be decomposed in various ways. Figure 1 (b) shows
various decomposition on the dynamics: according to the role of sub-dynamics, we can decompose it
into: {thigh, shin, foot}; alternatively, according to the position of sub-dynamics, we can decompose
it into: {back, front}. Figure 1 (d) shows that no matter whether we decompose the Cheetah task
according to role or position, modeling each decomposed sub-dynamics separately can signiÔ¨Åcantly
reduce the model error of the existing MBRL algorithm (e.g. Dreamer (Hafner et al., 2020))."
INTRODUCTION,0.03896103896103896,"Inspired by the above example, we propose environment dynamics decomposition (ED2), a novel
world model construction framework that models the dynamics in a decomposing fashion. ED2
contains two main components: sub-dynamics discovery (SD2) and dynamics decomposition pre-
diction (D2P). SD2 is proposed to decompose the dynamics into multiple sub-dynamics, which can
be Ô¨Çexibly designed and we also provide three alternative approaches: complete decomposition,
human prior, and the clustering-based method. D2P is proposed to construct the world model from
the decomposed dynamics in SD2, which models each sub-dynamics separately in an end-to-end
training manner. ED2 is orthogonal to existing MBRL algorithms and can be used as a backbone to
easily combine with any MBRL algorithm. Experiment shows ED2 improves the model accuracy
and boosts the performance signiÔ¨Åcantly when combined with existing MBRL algorithms."
BACKGROUND,0.04155844155844156,"2
BACKGROUND"
REINFORCEMENT LEARNING,0.04415584415584416,"2.1
REINFORCEMENT LEARNING"
REINFORCEMENT LEARNING,0.046753246753246755,"Given an environment, we can deÔ¨Åne a Ô¨Ånite-horizon partially observable Markov decision process
(POMDP) as (S, A, R, P, Œ≥, O, ‚Ñ¶, T), where S ‚ààRn is the state space, and A ‚ààRm is the action
space, R : S √ó A ‚ÜíR denotes the reward function, P : S √ó A ‚ÜíS denotes the environment
dynamics, Œ≥ is the discount factor. The agent receives an observation o ‚àà‚Ñ¶, which contain partial
information about the state s ‚ààS. O is the observation function, which mapping states to probability
distributions over observations. The decision process length is denoted as T."
REINFORCEMENT LEARNING,0.04935064935064935,"Let Œ∑ denote the expected return of a policy œÄ over the initial state distribution œÅ0. The goal of an RL
agent is to Ô¨Ånd the optimal policy œÄ‚àówhich maximizes the expected return:"
REINFORCEMENT LEARNING,0.05194805194805195,"œÄ‚àó= arg max
œÄ
Œ∑[œÄ] = arg max
œÄ
EœÄ[ T
X"
REINFORCEMENT LEARNING,0.05454545454545454,"t=0
Œ≥tR(st, at)],"
REINFORCEMENT LEARNING,0.05714285714285714,Under review as a conference paper at ICLR 2022
REINFORCEMENT LEARNING,0.05974025974025974,"where s0 ‚àºœÅ0, ot ‚àºO(¬∑|st), at ‚àºœÄ(¬∑|ot), st+1 ‚àºP(¬∑|st, at). If the environment is fully observable,
i.e., ‚Ñ¶= S and O is an identity function, POMDP is equivalent to the MDP: (S, A, R, P, Œ≥, T)."
REPRESENTATIVE WORLD MODELS IN MBRL,0.06233766233766234,"2.2
REPRESENTATIVE WORLD MODELS IN MBRL"
REPRESENTATIVE WORLD MODELS IN MBRL,0.06493506493506493,"The world model is a key component of MBRL that directly impacts policy training. World models
are usually formulated with latent dynamics (Janner et al., 2019; Hafner et al., 2020), and the general
form of the latent dynamics model can be summarized as follows:"
REPRESENTATIVE WORLD MODELS IN MBRL,0.06753246753246753,"Latent transition kernel:
ht = f(s‚â§t‚àí1, a‚â§t‚àí1)
Stochastic state function:
p(st|ht)
Reward function:
p(rt|ht)"
REPRESENTATIVE WORLD MODELS IN MBRL,0.07012987012987013,"The latent transition kernel (shorthand as kernel) predicts the latent state ht with input s‚â§t‚àí1 and
a‚â§t‚àí1. Based on latent state ht, the stochastic state function and reward function decode the state st
and reward rt. For the partially observable environment, two additional functions are required:"
REPRESENTATIVE WORLD MODELS IN MBRL,0.07272727272727272,"Observation function:
p(ot|st)
Representation function:
p(st|ht, ot)"
REPRESENTATIVE WORLD MODELS IN MBRL,0.07532467532467532,"In general, world models mainly differ at the implementation of kernel, which can be roughly divided
into two categories: with non-recurrent kernel and with recurrent kernel. The formal deÔ¨Ånition
of both kernels are as follows:"
REPRESENTATIVE WORLD MODELS IN MBRL,0.07792207792207792,"ht =

f(st‚àí1, at‚àí1)
With non-recurrent kernel
f(ht‚àí1, st‚àí1, at‚àí1)
With recurrent kernel"
REPRESENTATIVE WORLD MODELS IN MBRL,0.08051948051948052,"Non-recurrent kernel are relatively basic kernel for modeling, which are often implemented as
Fully-Connected Networks. Non-recurrent kernel takes the current state st‚àí1 and action at‚àí1 as
input, outputs the latent state prediction ht. Compare to non-recurrent kernel, recurrent kernel is
implemented as RNN and takes the additional input ht‚àí1, which performs better under POMDP
setting. For both kernels, the st and rt can be generated from the latent prediction ht."
ENVIRONMENT DYNAMICS DECOMPOSITION,0.08311688311688312,"3
ENVIRONMENT DYNAMICS DECOMPOSITION"
MOTIVATION,0.08571428571428572,"3.1
MOTIVATION"
MOTIVATION,0.08831168831168831,"An accurate world model is critical in MBRL policy deriving. To decrease the model error, existing
works propose various techniques as introduced in Section 1. However, these techniques improve the
environment modeling in a black-box manner, which ignores the inner properties of environment
dynamics, resulting in inaccurate world model construction and poor policy performance. To address
this problem, we propose two important environment properties when modeling an environment:"
MOTIVATION,0.09090909090909091,"1) Decomposability: The environment dynamics can be decomposed into multiple sub-
dynamics in various ways and the decomposed sub-dynamics can be combined to reconstruct
the entire dynamics."
MOTIVATION,0.09350649350649351,"2) Traceability: The environment dynamics can be traced to the action‚Äôs impact on the en-
vironment, and each sub-dynamics can be traced to the impact caused by a part of the
action."
MOTIVATION,0.09610389610389611,"For example in the Cheetah task, Figure 1 (b) demonstrates the decomposability: we can decompose
the dynamics into {thigh, shin, foot} sub-dynamics or {back, front} sub-dynamics, which depends
on the different decomposition perspectives and the combination of decomposed sub-dynamics can
constitute the entire dynamics. Figure 1 (c) explains the traceability: each sub-dynamics can be traced
to the corresponding subset of action dimensions: for the thigh dynamics, it can be regarded as the
impact caused by the front-thigh and back-thigh action dimensions. The above two properties are
closely related to environment modeling: the decomposability reveals the existence of sub-dynamics,
which allows us to model the dynamics separately, while the traceability investigates the causes of
the dynamics and guides us to decompose the dynamics at its root (i.e. the action)."
MOTIVATION,0.0987012987012987,Under review as a conference paper at ICLR 2022
MOTIVATION,0.1012987012987013,ED2 world model
MOTIVATION,0.1038961038961039,"ùëÆùëÆùüèùüè
ùüèùüè
ùüêùüê
ùüëùüë ùíéùíé ‚Ä¶ ùëÆùëÆùüêùüê ùëÆùëÆùíåùíå ‚Ä¶"
MOTIVATION,0.10649350649350649,SD2 Method
MOTIVATION,0.10909090909090909,"SD2
Method ùíîùíî"
MOTIVATION,0.11168831168831168,"Action 
decomposing"
MOTIVATION,0.11428571428571428,(Partition ùí¢ùí¢)
MOTIVATION,0.11688311688311688,"D2P 
Framework"
MOTIVATION,0.11948051948051948,"Action 
dimensions
Partition ùí¢ùí¢"
MOTIVATION,0.12207792207792208,"ùë¥ùë¥ùùìùùìùüèùüè
ùüèùüè"
MOTIVATION,0.12467532467532468,"ùë¥ùë¥ùùìùùìùüêùüê
ùüêùüê"
MOTIVATION,0.12727272727272726,"ùë¥ùë¥ùùìùùìùíåùíå
ùíåùíå"
MOTIVATION,0.12987012987012986,D2P Framework
MOTIVATION,0.13246753246753246,ùíÇùíÇùëÆùëÆùüèùüè
MOTIVATION,0.13506493506493505,ùíÇùíÇùëÆùëÆùüêùüê
MOTIVATION,0.13766233766233765,ùíÇùíÇùëÆùëÆùíåùíå ‚Ä¶ ùíîùíî ‚Ä¶
MOTIVATION,0.14025974025974025,Sub-actions models ùíâùíâùüèùüè ùíâùíâùüêùüê
MOTIVATION,0.14285714285714285,"ùíâùíâùíåùíå
‚Ä¶ ùíâùíâ ùíîùíî‚Ä≤ ùíìùíì"
MOTIVATION,0.14545454545454545,predictions
MOTIVATION,0.14805194805194805,"ùíÇùíÇ
ùíîùíî‚Ä≤, ùíìùíì"
MOTIVATION,0.15064935064935064,"Figure 2: Overview of the world model under ED2 Framework. ED2 contains two components:
SD2 and D2P. SD2 decomposes the dynamics by generating partition G on action dimensions. D2P
decomposes action a into multiple sub-actions according to G and makes decomposing predictions
based on s and each sub-action. The prediction h is the combined output of all sub-dynamics models,
from which the next state s‚Ä≤ and reward r are generated."
MOTIVATION,0.15324675324675324,"To take the above properties into account, we propose the environment dynamics decomposition
(ED2) framework (as shown in Figure 2), which contains two key components: sub-dynamics
discovery (SD2) and dynamics decomposition prediction (D2P). More speciÔ¨Åcally, by considering the
traceability, we propose to discover the latent sub-dynamics by analyzing the action (SD2, the blue
part in Figure 2); by considering the decomposability, we propose to construct the world model in a
decomposing manner (D2P, the green part in Figure 2). Our framework can be used as a backbone in
MBRL and the combination can lead to performance improvements over existing MBRL algorithms."
DYNAMICS DECOMPOSITION PREDICTION,0.15584415584415584,"3.2
DYNAMICS DECOMPOSITION PREDICTION"
DYNAMICS DECOMPOSITION PREDICTION,0.15844155844155844,"Given an environment with m-dimensional action space A ‚äÇRm, the index of each action dimension
constitutes a set Œõ = {1, 2, ¬∑ ¬∑ ¬∑ , m}, any disjoint partition G = {G1, . . . , Gk} over Œõ corresponds to
a particular way of decomposing action space. For each action dimension i in Œõ, we deÔ¨Åne the action
space as Ai, which satisÔ¨Åed A = A1 √ó ¬∑ ¬∑ ¬∑ √ó Am. The action space decomposition under partition G
is deÔ¨Åned as AG = {AG1, ¬∑ ¬∑ ¬∑ , AGk}, where sub-action space AGj = Q"
DYNAMICS DECOMPOSITION PREDICTION,0.16103896103896104,"x‚ààGj Ax. Based on above
deÔ¨Ånitions, we deÔ¨Åne the dynamics decomposition for P under partition G as follows:"
DYNAMICS DECOMPOSITION PREDICTION,0.16363636363636364,"DeÔ¨Ånition 1 Given a partition G, the decomposition for P : S √ó A ‚ÜíS can be deÔ¨Åned as:"
DYNAMICS DECOMPOSITION PREDICTION,0.16623376623376623,"P(s, a) = fc"
K,0.16883116883116883,"1
k k
X"
K,0.17142857142857143,"i=1
Pi(s, aGi) !"
K,0.17402597402597403,", ‚àÄs, a ‚ààS √ó A,
(1)"
K,0.17662337662337663,"with a set of sub-dynamics functions {P1, ..., Pk} that Pi : S √ó AGi ‚ÜíH, and a decoding function
fc : H ‚ÜíS. Note H is a latent space and aGi ‚ààAGi is a sub-action (projection) of action a."
K,0.17922077922077922,"Intuitively, the choice of partition G is signiÔ¨Åcant to the rationality of dynamics decomposition, which
should be reasonably derived from the environments. In this section, we mainly focus on dynamics
modeling, and we will introduce how to derive the partition G by using SD2 in section 3.3."
K,0.18181818181818182,"To implement D2P, we use model M i
œÜi parameterized by œÜi (i.e., neural network parameters) to
approximate each sub-dynamics Pi. As illustrated in Figure 2, given a partition G, an action a is
divided into multiple sub-actions {aG1, ¬∑ ¬∑ ¬∑ , aGk}, each model M i
œÜi takes state s and the sub-action
aGi as input and output a latent prediction hi ‚ààH. The separate latent predictions {h1, ¬∑ ¬∑ ¬∑ , hk} are
aggregated and then decoded for the generation of state s‚Ä≤ and reward r. For each kernel described in
Section 2.2, we provide the formal description here when combine with D2P: ht ="
K,0.18441558441558442,"(
1
k
Pk
i=1 f(st‚àí1, aGi
t‚àí1)
For non-recurrent kernel
1
k
Pk
i=1 f(ht‚àí1, st‚àí1, aGi
t‚àí1)
For recurrent kernel"
K,0.18701298701298702,Under review as a conference paper at ICLR 2022
K,0.18961038961038962,Figure 3: Extension of RSSM with D2P.
K,0.19220779220779222,"We propose a set of kernels, where each kernel mod-
els a speciÔ¨Åc sub-dynamics with the input of current
state s, corresponding sub-action aGi and hidden
state ht‚àí1 (ignored when applying on non-recurrent
kernel). The output of all kernels is averaged to get
the Ô¨Ånal output ht. The prediction of reward rt and
state st is generated from the output ht. SpeciÔ¨Å-
cally, we provide an example when combining with
the kernel of Recurrent State-Space Model (RSSM)
(Hafner et al., 2019) in Figure 3, which is a repre-
sentative recurrent kernel-based world model. The
original single kernel implemented as GRU are re-
place by multiple kernels with different action input."
SUB-DYNAMICS DISCOVERY,0.19480519480519481,"3.3
SUB-DYNAMICS DISCOVERY"
SUB-DYNAMICS DISCOVERY,0.1974025974025974,"The traceability of the environment introduced in Section 3.1 provides us with a basis for dynamics
decomposition: the decomposition on dynamics can be converted to the decomposition on the action
space. Therefore, we present the SD2 module for the action space decomposition and discuss three im-
plementations in this section. With the Cheetah task in Section 3.1 as the example: the straightforward
SD2 implementation is the complete decomposition, which regards each action dimension as a sub-
dynamics and decomposes the dynamics completely. SpeciÔ¨Åcally, complete decomposition decom-
poses the dynamics into six sub-dynamics: {Front, Back}√ó{Thigh, Shin, Foot}. However, com-
plete decomposition ignores the inner action dimensions correlations, which limits its performance in
many tasks. For example, the three action dimensions {Front_Thigh, Front_Shin, Front_Foot}
affect the dynamics of the front part together, thus simply separate these action dimensions would
affect the prediction accuracy. To include the action dimension correlations, incorporating human
prior for the action space decomposition is an improved implementation. Based on different human
prior, we can decompose the dynamics in different ways as introduced in Figure 1. Nevertheless,
although human prior considers the action dimension correlations, it is highly subjective and might
lead to sub-optimal results due to the limited understanding of tasks (we also provide the correspond-
ing experiment in Section 4.2.3). Therefore, human prior is not applicable in complex systems which
is beyond human understanding."
SUB-DYNAMICS DISCOVERY,0.2,"To better discover the sub-dynamics and eliminate the dependence on human prior, we propose to
automatically decompose the action space using the clustering-based method. The clustering-based
method contains two components: feature extraction and clustering criterion. Feature extraction
extracts the properties of action dimension ai into feature vector F i. Then we regard each action
dimension as a cluster and aggregate related action dimensions together with the clustering criterion.
The effectiveness of the clustering-based method depends on the quality of feature extraction and the
validity of clustering criteria, which may be different in different environments. Therefore, although
we provide a general implementation later, we still suggest readers design suitable clustering-based
methods according to task-speciÔ¨Åc information."
SUB-DYNAMICS DISCOVERY,0.2025974025974026,"Feature Extraction: We extract the properties of each action dimension by computing the Pearson
correlation coefÔ¨Åcient between action dimensions and state dimensions. SpeciÔ¨Åcally, we deÔ¨Åne
the feature vector as F i = ‚ü®|f i,1|, ¬∑ ¬∑ ¬∑ , |f i,n|‚ü©, where each f i,j denotes the Pearson correlation
coefÔ¨Åcient between action dimension i and state dimension j. F i describes the impact caused by
action dimension i and f i,j is calculated by the corresponding action value ai and state value changes
‚àÜsj (which is the difference between the next state and the current state):"
SUB-DYNAMICS DISCOVERY,0.2051948051948052,"f i,j = cov(ai, ‚àÜsj)"
SUB-DYNAMICS DISCOVERY,0.2077922077922078,"œÉaiœÉ‚àÜsj
(2)"
SUB-DYNAMICS DISCOVERY,0.21038961038961038,where cov denotes the covariance and œÉ denotes the standard deviation.
SUB-DYNAMICS DISCOVERY,0.21298701298701297,"Clustering Criterion: We deÔ¨Åne the clustering criterion as the relationship between clusters, which
can be formalized as follow:"
SUB-DYNAMICS DISCOVERY,0.21558441558441557,"Rela(Gi, Gj) =R(Gi, Gj) ‚àíR(Gj, G‚àíi) √ó œâj,‚àíi + R(Gi, G‚àíj) √ó œâi,‚àíj"
SUB-DYNAMICS DISCOVERY,0.21818181818181817,"œâi,‚àíj + œâj,‚àíi
(3)"
SUB-DYNAMICS DISCOVERY,0.22077922077922077,Under review as a conference paper at ICLR 2022
SUB-DYNAMICS DISCOVERY,0.22337662337662337,"where G‚àíi = Œõ \ Gi, œâi,j = |Gi| √ó |Gj| and R(Gi, Gj) = ‚àí
1
œâi,j
P"
SUB-DYNAMICS DISCOVERY,0.22597402597402597,"Ai‚ààGi
P"
SUB-DYNAMICS DISCOVERY,0.22857142857142856,"Aj‚ààGj ||F i, F j||D.
|| ¬∑ ||D measures the distance between vectors under distance function D (we choose the negative
cosine similarity as D)."
SUB-DYNAMICS DISCOVERY,0.23116883116883116,"Algorithm 1 Selectable clustering-based method.
Input: Task E, clustering threshold Œ∑"
SUB-DYNAMICS DISCOVERY,0.23376623376623376,"Initialize cluster set G = {{1}, ¬∑ ¬∑ ¬∑ , {m}} according to E, a
random policy œÄrand, dataset Dc ‚Üí‚àÖ
for i = 1, 2, ¬∑ ¬∑ ¬∑ , T do"
SUB-DYNAMICS DISCOVERY,0.23636363636363636,"Collect and store samples in Dc with œÄrand
Calculate F i for each action dimension i with Dc
while |G| > 1 do"
SUB-DYNAMICS DISCOVERY,0.23896103896103896,"Gmax1, Gmax2 = arg maxGi,Gj‚ààG Rela(Gi, Gj)
if Rela(Gmax1, Gmax2) > Œ∑ then"
SUB-DYNAMICS DISCOVERY,0.24155844155844156,"Remove Gmax1 and Gmax2 from G
Add Gmax1 ‚à™Gmax2 to G
else"
SUB-DYNAMICS DISCOVERY,0.24415584415584415,"Stop clustering
return G"
SUB-DYNAMICS DISCOVERY,0.24675324675324675,"Algorithm 1 presents the overall im-
plementation of the clustering-based
method. As Algorithm 1 describes, with
input task E and clustering threshold Œ∑,
we Ô¨Årst initialize the cluster set G contain-
ing m clusters (each for a single action di-
mension), a random policy œÄrand, and an
empty dataset Dc. Then for T episodes,
œÄrand collects samples from the environ-
ment and we calculate F i for each action
dimension i. After that, for each cluster-
ing step, we select the two most relevant
clusters from G and cluster them together.
The process ends when there is only one
cluster, or when the correlation of the two
most correlated clusters is less than the
threshold Œ∑. Œ∑ is a hyperparameter which assigned with a value around 0 and empirically adjusted."
SUB-DYNAMICS DISCOVERY,0.24935064935064935,"3.4
ED2 FOR MBRL ALGORITHMS"
SUB-DYNAMICS DISCOVERY,0.2519480519480519,"Algorithm 2 ED2-Dreamer
Input: Task E, clustering threshold Œ∑"
SUB-DYNAMICS DISCOVERY,0.2545454545454545,"// Sub-dynamics Discovery (SD2) Phase:
G ‚ÜêSD2 methods (E, Œ∑)
// Dynamics Decomposition Prediction (D2P) Phase:
for i = 1, 2, ¬∑ ¬∑ ¬∑ , |G| do"
SUB-DYNAMICS DISCOVERY,0.2571428571428571,"Build sub-dynamics model: M i
œÜi = f(ht‚àí1, st‚àí1, aGi
t‚àí1)
Combining all sub-dynamics models: Mc =
1
|G|
P|G|
i=1 M i
œÜi
Combining Mc with a decoding network f d
œÜd and construct
the ED2-combined world model: pœÜ = f d
œÜd(Mc)
// Training Phase:
Initialize policy œÄŒ∏, model: pœÜ
Optimize policy with Dreamer: œÄÀÜŒ∏ = Dreamer(E, œÄŒ∏, pœÜ)"
SUB-DYNAMICS DISCOVERY,0.2597402597402597,"ED2 is a general framework and can be
combined with any existing MBRL al-
gorithms. Here we provide the practi-
cal combination implementation of ED2
with Dreamer(Hafner et al., 2020) (Al-
gorithm 2) and we also combine ED2
with MBPO (Janner et al., 2019) in the
appendix. The whole process of ED2-
Dreamer contains three phases: 1) SD2
decomposes the environment dynamics
of task E, which and can be implemented
by three decomposing methods intro-
duced in Section 3.3; 2) D2P models each
sub-dynamics separately and constructs
the ED2-combined world model pœÜ by
combining all sub-models with a decoding network f d
œÜd; 3) The Ô¨Ånal training phase initializes the
policy œÄŒ∏ and the world model pœÜ, then derive the policy from Dreamer with input œÄŒ∏, pœÜ and task E."
EXPERIMENTS,0.2623376623376623,"4
EXPERIMENTS"
EXPERIMENTS,0.2649350649350649,"Baselines & Benchmarks: For a comparative study, we take two state-of-the-art MBRL methods:
MBPO (Janner et al., 2019) and Dreamer (Hafner et al., 2020) as the baselines, and extend them
with ED2 as ED2-MBPO, ED2-Dreamer. To reduce implementation bias, we reuse the code and
benchmarks from the prior works: the DMC Suite (Tassa et al., 2018) for Dreamer and Gym-
Mujoco (Brockman et al., 2016) for MBPO. Besides, we also provide ablation studies to validate the
effectiveness of each component of our ED2."
EXPERIMENTS,0.2675324675324675,"ClariÔ¨Åcation: (1) Although the data required by the clustering-based method is tiny (less than 1%
of the policy training), we include it in the Ô¨Ågures for a fair comparison. (2) We take the clustering-
based method as the main SD2 implementation (denoted as ED2-Methods) and discuss other SD2
methods in Section 4.2.3 and appendix (complete decomposition, human prior are denoted as CD, HP
respectively). (3) Due to the space limit, we leave the result of MBPO/ED2-MBPO in the appendix.
(4) All results are averaged over 5 seeds. The hyperparameters setting is left in the appendix."
EXPERIMENTS,0.2701298701298701,Under review as a conference paper at ICLR 2022
PERFORMANCE,0.2727272727272727,"4.1
PERFORMANCE"
PERFORMANCE,0.2753246753246753,"0.0
0.5
1.0
Step
√ó106 0 200 400 600 800 Value"
PERFORMANCE,0.2779220779220779,dmc_cheetah_run
PERFORMANCE,0.2805194805194805,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.2831168831168831,"0.0
0.5
1.0
Step
√ó106 0 200 400 600 800 Value"
PERFORMANCE,0.2857142857142857,dmc_finger_spin
PERFORMANCE,0.2883116883116883,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.2909090909090909,"0
1
2
3
Step
√ó106 0 200 400 Value"
PERFORMANCE,0.2935064935064935,dmc_hopper_hop
PERFORMANCE,0.2961038961038961,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.2987012987012987,"0
1
2
3
Step
√ó106 0 50 Value"
PERFORMANCE,0.3012987012987013,dmc_humanoid_stand
PERFORMANCE,0.3038961038961039,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.3064935064935065,"0
1
2
3
Step
√ó106 0 50 100 Value"
PERFORMANCE,0.3090909090909091,dmc_humanoid_walk
PERFORMANCE,0.3116883116883117,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.3142857142857143,"0
2
4
6
8
Step
√ó105 0 200 400 600 800 Value"
PERFORMANCE,0.3168831168831169,dmc_reacher_easy
PERFORMANCE,0.3194805194805195,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.3220779220779221,"0.0
0.5
1.0
Step
√ó106 0 200 400 600 800 Value"
PERFORMANCE,0.3246753246753247,dmc_walker_run
PERFORMANCE,0.32727272727272727,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.32987012987012987,"0
2
4
6
8
Step
√ó105 0 200 400 600 800 Value"
PERFORMANCE,0.33246753246753247,dmc_hopper_stand
PERFORMANCE,0.33506493506493507,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.33766233766233766,"Figure 4: Comparisons of ED2-Dreamer vs. Dreamer. The x- and y-axis represent the training steps
and performance. The line and shaded area denotes the mean value and standard deviation."
PERFORMANCE,0.34025974025974026,"We evaluate Dreamer and ED2-Dreamer on eight DMC tasks with image inputs. As shown in Figure
4, ED2-Dreamer outperforms Dreamer on all tasks. This is because ED2 establishes a more rational
and accurate world model, which leads to more efÔ¨Åcient policy training. Another Ô¨Ånding is that,
in tasks like cheetah_run, and walker_run, ED2-Dreamer achieves lower variance, demonstrating
that ED2 can also lead to a more stable training process. Furthermore, Dreamer fails to achieve
good performance in difÔ¨Åcult tasks such as humanoid_stand and humanoid_walk. In contrast, ED2-
Dreamer improves the performance signiÔ¨Åcantly, which indicating the superiority of ED2 in complex
tasks. In humanoid tasks, the dynamics are too complex for the clustering-based method with image-
based input. Therefore, we use the vector-based state for clustering and keep the policy training on
the image-based state (we will further discuss this in Section 5). The performance of MBPO and
ED2-MBPO are left in the appendix, which proves that ED2 boost MBPO‚Äôs performance signiÔ¨Åcantly."
ABLATION STUDIES,0.34285714285714286,"4.2
ABLATION STUDIES"
ABLATION STUDIES,0.34545454545454546,"4.2.1
THE EFFECTIVENESS OF D2P AND SD2"
ABLATION STUDIES,0.34805194805194806,"In this section, we investigate the contribution of each component to performance improvement.
We can summarize the improvements into three parts: multiple kernels, decomposing prediction,
reasonable partition. There is a progressive dependence between these three parts: the decomposing
prediction depends on the existence of the multiple kernels and the reasonable partition depends on
the decomposing prediction. Therefore, we design an incremental experiment for the validation."
ABLATION STUDIES,0.35064935064935066,"0.0
0.5
1.0
Step
√ó106 0 200 400 600 800 Value"
ABLATION STUDIES,0.35324675324675325,dmc_cheetah_run
ABLATION STUDIES,0.35584415584415585,"Dreamer
ED2-Dreamer
ED2-Dreamer-random
ED2-Dreamer-ensemble"
ABLATION STUDIES,0.35844155844155845,"0
1
2
3
Step
√ó106 0 50 100 Value"
ABLATION STUDIES,0.36103896103896105,dmc_humanoid_walk
ABLATION STUDIES,0.36363636363636365,"Dreamer
ED2-Dreamer
ED2-Dreamer-random
ED2-Dreamer-ensemble"
ABLATION STUDIES,0.36623376623376624,"0.0
0.5
1.0
Step
√ó106 0 200 400 600 800 Value"
ABLATION STUDIES,0.36883116883116884,dmc_walker_run
ABLATION STUDIES,0.37142857142857144,"Dreamer
ED2-Dreamer
ED2-Dreamer-random
ED2-Dreamer-ensemble"
ABLATION STUDIES,0.37402597402597404,"0
1
2
3
Step
√ó106 0 200 400 Value"
ABLATION STUDIES,0.37662337662337664,dmc_hopper_hop
ABLATION STUDIES,0.37922077922077924,"Dreamer
ED2-Dreamer
ED2-Dreamer-random
ED2-Dreamer-ensemble"
ABLATION STUDIES,0.38181818181818183,Figure 5: Performance comparisons of components ablation experiments.
ABLATION STUDIES,0.38441558441558443,"First, we employ the ED2-Dreamer-ensemble 2, which maintains the multiple kernel structure but
without dynamics decomposing (i.e. all kernels input with action a rather than sub-action aGi). We
investigate the contribution of multiple kernels by comparing ED2-Dreamer-ensemble with baselines.
Second, we employ the ED2-Dreamer-random, which maintains the D2P structure and obtains
partition randomly. We investigate the contribution of decomposing prediction by comparing ED2-
Dreamer-random with ED2-Dreamer-ensemble. Last, we investigate the contribution of reasonable
partition by comparing ED2-Dreamer with ED2-Dreamer-random."
ABLATION STUDIES,0.38701298701298703,"2The ensemble refers to kernel ensemble, which is described in detail in the appendix."
ABLATION STUDIES,0.38961038961038963,Under review as a conference paper at ICLR 2022
ABLATION STUDIES,0.3922077922077922,"Figure 5 shows that ED2-Dreamer-ensemble outperform Dreamer on humanoid_walk and cheetah_run
tasks, indicating that multiple kernels help the policy training on some tasks. ED2-Dreamer-random
outperforms ED2-Dreamer-ensemble on humanoid_walk task, but not in other tasks (even perform
worse in cheetah_run and hopper_hop). This is due to the different modeling difÔ¨Åculty of tasks: the
tasks except humanoid_walk are relatively simple and can be modeled without D2P directly (but in a
sub-optimal way). The modeling process of these tasks can be aided by a reasonable partition but
damaged by a random partition. The humanoid_walk is challenging and cannot be modeled directly,
therefore decomposing prediction (D2P) is most critical and performance can be boosted even with a
random decomposing prediction. Finally, ED2-Dreamer outperforms ED2-Dreamer-random on all
tasks, which indicates that a reasonable partition (SD2) is critical in dynamics modeling and D2P can
not contribute signiÔ¨Åcantly to the modeling process without a reasonable partition."
MODEL ERROR,0.3948051948051948,"4.2.2
MODEL ERROR"
MODEL ERROR,0.3974025974025974,"0.0
0.5
1.0
Step
√ó106 3.000 3.005 3.010 3.015 3.020 Value"
MODEL ERROR,0.4,dmc_cheetah_run
MODEL ERROR,0.4025974025974026,"Dreamer
ED2-Dreamer"
MODEL ERROR,0.4051948051948052,"0
1
2
3
Step
√ó106 3.00 3.05 3.10 3.15 3.20 3.25 3.30 3.35 Value"
MODEL ERROR,0.4077922077922078,dmc_humanoid_walk
MODEL ERROR,0.4103896103896104,"Dreamer
ED2-Dreamer"
MODEL ERROR,0.412987012987013,"0.0
0.5
1.0
Step
√ó106 3.0 3.5 4.0 4.5 Value"
MODEL ERROR,0.4155844155844156,dmc_walker_run
MODEL ERROR,0.41818181818181815,"Dreamer
ED2-Dreamer"
MODEL ERROR,0.42077922077922075,"0
1
2
3
Step
√ó106 3.00 3.01 3.02 Value"
MODEL ERROR,0.42337662337662335,dmc_hopper_hop
MODEL ERROR,0.42597402597402595,"Dreamer
ED2-Dreamer"
MODEL ERROR,0.42857142857142855,Figure 6: The model error (KL-Divergence) comparison of ED2-Dreamer and Dreamer.
MODEL ERROR,0.43116883116883115,"In this section, we further investigate whether the model error is reduced when combined with ED2.
We conduct an environment modeling experiment on the same dataset (which is collected in the
MBRL training process) and record the model error. Since the policy keeps update in the MBRL
training process, the dataset of MBRL also changes with the updated policy. For example, in the
MBRL training on the Cheetah task, the model is Ô¨Årst trained with the data like Rolling on the ground
and Ô¨Ånally trained with the data like running with high speed. To simulate the MBRL training process,
we implement our dataset by slowly expanding it from 0 to all according to the data generation time.
This setting can also help to investigate the generalization ability of the model on unfamiliar data
(i.e. the data generated by the updated policy). We list parts of the result in Figure 6 and the result of
other tasks are shown in the appendix."
MODEL ERROR,0.43376623376623374,"Figure 6 shows that ED2-Dreamer has a signiÔ¨Åcantly lower model error in all tasks compared with
Dreamer. ED2-Dreamer can also achieve a more stable world model training (i.e. with low variance)
on humanoid_walk and walker_run tasks. We also Ô¨Ånd that the baseline methods have signiÔ¨Åcantly
increasing model error on humanoid_walk and walker_run tasks, but for ED2-methods, the increase
is much smaller. We hypothesize that ED2 produces a reasonable network structure; as the dataset
grows, ED2-methods can generalize to the new data better. This property is signiÔ¨Åcant in MBRL
since the stable and accurate model prediction is critical for policy training. We also provide the
model error comparison of MBPO and ED2-MBPO in the appendix, which also proves that ED2 can
reduce the model errors when combine with MBRL methods."
MODEL ERROR,0.43636363636363634,"4.2.3
SD2 COMPARISON"
MODEL ERROR,0.43896103896103894,"In this section, we compare the performance of three proposed SD2 methods. We list the decom-
position obtained by the clustering-based method and human prior on humanoid_walk task for the
illustrating purpose and provide the corresponding performance comparison results in Figure 7. More
experimental results on other tasks are provided in the appendix."
MODEL ERROR,0.44155844155844154,"As shown in Figure 7, human prior can generate different partitions from different task understandings
and we average their performance as the Ô¨Ånal result. Experiment shows that all SD2 methods help
the policy learning. The clustering-based method performs best and baseline Dreamer performs
worst in the comparison. For the complete decomposition, it performs poorly under humanoid_walk,
which implies that humanoid_walk contains many inner action dimension correlations, and simply
complete decomposition heavily breaks this correlation thus hinders the Ô¨Ånal performance. Compared
to complete decomposition, human prior maintains more action dimension correlations by leveraging
the human prior knowledge, which leads to better performance. However, the correlations maintained"
MODEL ERROR,0.44415584415584414,Under review as a conference paper at ICLR 2022
MODEL ERROR,0.44675324675324674,"Figure 7: The performance comparison of Dreamer, ED2-Dreamer, ED2-CD-Dreamer and ED2-HP-
Dreamer. We provide the sub-dynamics visualization in the left four Ô¨Ågures. Each circle correspond
to a joint. A joint contains multiple action dimensions when the corresponding circle is separated
into multiple parts. We mark the action dimensions in the same sub-dynamics with the same color."
MODEL ERROR,0.44935064935064933,"by human prior might be false or incomplete due to human limited understanding of tasks. Compare to
human prior, the clustering-based method automatically decomposes the action space according to the
clustering criterion, which decomposes the action space better in a mathematical way. For example,
human prior aggregates {right_hip_x, right_hip_y, right_hip_z} (x, y, z denote the rotation direc-
tion) together and the clustering-based method aggregates {abdomen_x, right_hip_x, left_hip_x}
together. Although the action dimensions from human prior sub-dynamics affect the same joint
left_hip, they rotate in different directions and play a different role in the dynamics. In contrast, the
sub-dynamics discovered by the clustering-based method aggregate the action dimensions that affect
the x-direction rotation together. It maintains stronger correlations and helps the world model Ô¨Åtting
the movement on x-direction better. Therefore, it performs better than human prior on this task."
DISCUSSION,0.45194805194805193,"5
DISCUSSION"
DISCUSSION,0.45454545454545453,"In this paper, we regard SD2 as a Ô¨Çexible module that can adopt any suitable partition methods
considering the task-speciÔ¨Åc information. Currently, we discuss three kinds of SD2 methods, i.e.,
human prior, complete decomposition, and the clustering-based method, and the clustering-based
method is chosen as our main implementation since it outperforms the other two methods on these
testbeds empirically. We also analyze the reasonable decomposition provided by the clustering-based
method, which contributes a lot to the dynamics modeling process. Nevertheless, the clustering-based
method is still faced with extra challenges when solving complex tasks with image-based inputs,
such as humanoid tasks. In this paper, we use the vector-based state in the clustering stage for
humanoid tasks by considering the one-to-one correspondence between vector-based and image-
based state representations. How to leverage self-supervised learning or contrastive learning to learn
low-dimension, high-quality state features from raw images to improve the partition discovery effect
of SD2 and further apply our ED2 to more complex scenarios is worthwhile to further investigate."
DISCUSSION,0.45714285714285713,"Previous work (Doya et al., 2002) also takes the dynamics decomposed prediction into consideration,
which achieves better dynamics modeling. It decomposes the dynamics from the perspective of state
and time. Different from this work, we analyze the cause of dynamics and decompose it from its
root: the action space, which makes the modeling of environmental dynamics more reasonable and
scalable."
CONCLUSION,0.4597402597402597,"6
CONCLUSION"
CONCLUSION,0.4623376623376623,"In this paper, we propose a novel world model construction framework: Environment Dynamics
Decomposition (ED2), which explicitly considers the properties of environment dynamics and
models the dynamics in a decomposing manner. ED2 contains two components: SD2 and D2P. SD2
decomposes the environment dynamics into several sub-dynamics according to the dynamics-action
relation. D2P constructs a decomposing prediction model according to the result of SD2. With
combining ED2, the performance of existing MBRL algorithms is signiÔ¨Åcantly boosted. Currently,
this work only considers the decomposition on the dimension level, and for future work, it is
worthwhile investigating how to decompose environment dynamics at the object level, which can
further improve the interpretability and generalizability of ED2."
CONCLUSION,0.4649350649350649,Under review as a conference paper at ICLR 2022
REFERENCES,0.4675324675324675,REFERENCES
REFERENCES,0.4701298701298701,"Arthur Argenson and Gabriel Dulac-Arnold. Model-based ofÔ¨Çine planning. CoRR, abs/2008.05556,
2020."
REFERENCES,0.4727272727272727,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016."
REFERENCES,0.4753246753246753,"Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efÔ¨Åcient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural
Information Processing Systems 31, pp. 8234‚Äì8244, 2018."
REFERENCES,0.4779220779220779,"Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances in Neural
Information Processing Systems 31, pp. 4759‚Äì4770, 2018."
REFERENCES,0.4805194805194805,"Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: A model-based and data-efÔ¨Åcient
approach to policy search. In Proceedings of the 28th International Conference on Machine
Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pp. 465‚Äì472. Omnipress,
2011."
REFERENCES,0.4831168831168831,"Kenji Doya, Kazuyuki Samejima, Ken-ichi Katagiri, and Mitsuo Kawato. Multiple model-based
reinforcement learning. Neural Comput., 14(6):1347‚Äì1369, 2002."
REFERENCES,0.4857142857142857,"Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey
Levine. Model-based value estimation for efÔ¨Åcient model-free reinforcement learning. CoRR,
abs/1803.00101, 2018."
REFERENCES,0.4883116883116883,"Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and
James Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th
International Conference on Machine Learning, pp. 2555‚Äì2565, 2019."
REFERENCES,0.4909090909090909,"Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:
Learning behaviors by latent imagination. In Proceedings of the 8th International Conference on
Learning Representations, 2020."
REFERENCES,0.4935064935064935,"Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. In Advances in Neural Information Processing Systems 32, pp. 12498‚Äì12509,
2019."
REFERENCES,0.4961038961038961,"Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,
Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for
atari. In Proceedings of the 8th International Conference on Learning Representations, 2020."
REFERENCES,0.4987012987012987,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofÔ¨Çine reinforcement learning. In Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual, 2020."
REFERENCES,0.5012987012987012,"Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. In Proceedings of the 6th International Conference on Learning
Representations, 2018."
REFERENCES,0.5038961038961038,"Hang Lai, Jian Shen, Weinan Zhang, and Yong Yu. Bidirectional model-based policy optimization.
In Proceedings of the 37th International Conference on Machine Learning, pp. 5618‚Äì5627, 2020."
REFERENCES,0.5064935064935064,"Sergey Levine and Vladlen Koltun. Guided policy search. In Proceedings of the 30th International
Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of
JMLR Workshop and Conference Proceedings, pp. 1‚Äì9. JMLR.org, 2013."
REFERENCES,0.509090909090909,"Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorith-
mic framework for model-based deep reinforcement learning with theoretical guarantees. In
Proceedings of the 7th International Conference on Learning Representations, 2019."
REFERENCES,0.5116883116883116,Under review as a conference paper at ICLR 2022
REFERENCES,0.5142857142857142,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nat.,
518(7540):529‚Äì533, 2015."
REFERENCES,0.5168831168831168,"Masashi Okada and Tadahiro Taniguchi. Variational inference MPC for bayesian model-based
reinforcement learning. In Proceedings of the 3rd Annual Conference on Robot Learning, pp.
258‚Äì272, 2019."
REFERENCES,0.5194805194805194,"Shayegan OmidshaÔ¨Åei, Dong-Ki Kim, Jason Pazis, and Jonathan P. How. Crossmodal attentive skill
learner. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent
Systems, pp. 139‚Äì146. International Foundation for Autonomous Agents and Multiagent Systems
Richland, SC, USA / ACM, 2018."
REFERENCES,0.522077922077922,"Feiyang Pan, Jia He, Dandan Tu, and Qing He. Trust the model when it is conÔ¨Ådent: Masked
model-based actor-critic. In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.5246753246753246,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap,
and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. CoRR,
abs/1911.08265, 2019."
REFERENCES,0.5272727272727272,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of go with deep neural networks and tree search. Nat., 529(7587):484‚Äì489, 2016."
REFERENCES,0.5298701298701298,"Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.
Deepmind control suite. CoRR, abs/1801.00690, 2018."
REFERENCES,0.5324675324675324,"Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha√´l Mathieu, Andrew Dudzik, Junyoung
Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max
Jaderberg, Alexander Sasha Vezhnevets, R√©mi Leblond, Tobias Pohlen, Valentin Dalibard, David
Budden, Yury Sulsky, James Molloy, Tom L. Paine, √áaglar G√ºl√ßehre, Ziyu Wang, Tobias Pfaff,
Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W√ºnsch, Katrina McKinney, Oliver Smith,
Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David
Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nat., 575(7782):
350‚Äì354, 2019."
REFERENCES,0.535064935064935,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. MOPO: model-based ofÔ¨Çine policy optimization. In Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.5376623376623376,"Guangxiang Zhu, Minghao Zhang, Honglak Lee, and Chongjie Zhang. Bridging imagination and
reality for model-based deep reinforcement learning. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020."
REFERENCES,0.5402597402597402,Under review as a conference paper at ICLR 2022
REFERENCES,0.5428571428571428,"A
EXPERIMENT SETTING"
REFERENCES,0.5454545454545454,"We keep the experiment setting the same with Dreamer and MBPO. For Dreamer, we do the
experiment on Deep Mind Control environments, with images as input, and each episode length is
set to 1000. For MBPO, the experiment is processed on the Gym-Mujoco environment, takes the
structured data as input, and sets the episode length to 1000. Each experiment is averaged by running
Ô¨Åve seeds. Our speciÔ¨Åc computing infrastructure is as shown in Table 1:"
REFERENCES,0.548051948051948,Table 1: The computing infrastructure of our experiment.
REFERENCES,0.5506493506493506,"CPU
GPU
MEMORY"
REFERENCES,0.5532467532467532,"XEON(R) SILVER 4214
RTX2080TI
256G"
REFERENCES,0.5558441558441558,"B
EXPERIMENT HYPERPARAMETERS"
REFERENCES,0.5584415584415584,"For ED2-Dreamer and ED2-MBPO, we followed the ofÔ¨Åcial implementation of Dreamer and MBPO
except for the dynamics models. SpeciÔ¨Åcally, for the dynamics models, we select the hyperparameters
in each environment, as shown in Table 2."
REFERENCES,0.561038961038961,"Table 2: The hidden size and Œ∑ value for each environment. DeepMind denotes the environment that
belongs to DeepMind Control Suite, and Gym-Mujoco denotes the environment is from Gym-Mujoco."
REFERENCES,0.5636363636363636,"ENVIRONMENT
HIDDEN SIZE
Œ∑"
REFERENCES,0.5662337662337662,"HOPPER(DEEPMIND)
200
0
WALKER(DEEPMIND)
200
-0.06
CHEETAH(DEEPMIND)
200
-0.1
HUMANOID(DEEPMIND)
200
0
REACHER(DEEPMIND)
200
0
FINGER(DEEPMIND)
200
0
HALFCHEETAH(GYM-MUJOCO)
150
0
HOPPER(GYM-MUJOCO)
200
-0.3
WALKER(GYM-MUJOCO)
200
-0.2
ANT(GYM-MUJOCO)
150
-0.12"
REFERENCES,0.5688311688311688,"For the clustering process, hyperparameter Œ∑ describes the tightness of constraints on inter-group
distance and intra-group distance. When Œ∑ = 0, the clustering process stop condition is that the
distance between the two most relative clusters is equal to the distance between these two clusters
and others (this is a general condition in the most environment). In some environments, although
Œ∑ = 0 is a good choice, but the value can be further Ô¨Ånetuned to obtain more reasonable clustering
results. The Œ∑ value we use is as in Table 2."
REFERENCES,0.5714285714285714,"C
ED2-MBPO IMPLEMENTATION"
REFERENCES,0.574025974025974,"The combination of MBPO and ED2 can be described as Algorithm 3. We can also separate it into
three parts: the Ô¨Årst phase is SD2, which discover the sub-dynamics (partition G) in the environment
by using appropriate SD2 method. Then the D2P phase models each sub-dynamics separately and
construct the ED2-combined world model pœÜ. Finally, we derive the trained policy œÄÀÜŒ∏ by using
MBPO method with input task E, initialized policy œÄŒ∏ and world model ensemble P ÀÜœÜ."
REFERENCES,0.5766233766233766,Under review as a conference paper at ICLR 2022
REFERENCES,0.5792207792207792,"Algorithm 3 ED2-MBPO
Input: Task E, clustering threshold Œ∑"
REFERENCES,0.5818181818181818,"// Sub-dynamics Discovery (SD2) Phase:
G ‚ÜêSD2 methods (E, Œ∑)
// Dynamics Decomposition Prediction (D2P) Phase:
for i = 1, 2, ¬∑ ¬∑ ¬∑ , |G| do"
REFERENCES,0.5844155844155844,"Construct sub-dynamics model: M i
œÜi = f(st‚àí1, aGi
t‚àí1)
Combining all sub-dynamics models: Mc =
1
|G|
P|G|
i=1 M i
œÜi
Combining Mc with a decoding network f d
œÜd and construct the ED2-combined world model: pœÜ = f d
œÜd(Mc)
// Training Phase:
Initialize policy œÄŒ∏, model ensemble: P ÀÜœÜ = {p1
œÜ1, ¬∑ ¬∑ ¬∑ , pe
œÜe}
Optimize policy with MBPO: œÄÀÜŒ∏ = MBPO(E, œÄŒ∏, P ÀÜœÜ)"
REFERENCES,0.587012987012987,"D
KERNEL ENSEMBLE AND MODEL ENSEMBLE"
REFERENCES,0.5896103896103896,"Kernel ensemble is deployed in section 4.2. We retain the multi kernel network structure and all
kernel input with the same information: hidden state h (if combined with recurrent kernel), current
state s and current action a. Kernel ensemble can be regarded as a single model, which average the
outputs from all kernels and training in an end-to-end manner for all kernels."
REFERENCES,0.5922077922077922,"Model ensemble is widely used in MBRL for uncertainty estimation. In MBRL, the model ensemble
is generally implemented by setting different initial parameters and sampling different training data
from the same dataset. Models in model ensemble are trained separately and no connection between
them except training from the same dataset."
REFERENCES,0.5948051948051948,"Therefore, model ensemble is totally different from kernel ensemble. Model ensemble propose to
train multiple unrelated models for the same task, which can estimate the uncertainty of prediction.
But kernel ensemble propose to use one model (but construct with multiple kernels) for the prediction
task. We can also combine kernel ensemble with model ensemble and improve the accuracy of all
models (e.g. MBPO)."
REFERENCES,0.5974025974025974,"E
MODEL ERROR"
REFERENCES,0.6,"In order to verify whether we get a more accurate world model, we measure the model error. Figure 8
is the model error curve of Dreamer / ED2-Dreamer. We can see that our framework can signiÔ¨Åcantly
reduce the model error."
REFERENCES,0.6025974025974026,"0.0
0.5
1.0
Step
√ó106 3.000 3.005 3.010 3.015 3.020 Value"
REFERENCES,0.6051948051948052,dmc_cheetah_run
REFERENCES,0.6077922077922078,"Dreamer
ED2-Dreamer"
REFERENCES,0.6103896103896104,"0.0
0.5
1.0
Step
√ó106 3.00 3.02 3.04 3.06 Value"
REFERENCES,0.612987012987013,dmc_finger_spin
REFERENCES,0.6155844155844156,"Dreamer
ED2-Dreamer"
REFERENCES,0.6181818181818182,"0
1
2
3
Step
√ó106 3.00 3.01 3.02 Value"
REFERENCES,0.6207792207792208,dmc_hopper_hop
REFERENCES,0.6233766233766234,"Dreamer
ED2-Dreamer"
REFERENCES,0.625974025974026,"0
1
2
3
Step
√ó106 3.00 3.05 3.10 3.15 3.20 3.25 3.30 3.35 Value"
REFERENCES,0.6285714285714286,dmc_humanoid_stand
REFERENCES,0.6311688311688312,"Dreamer
ED2-Dreamer"
REFERENCES,0.6337662337662338,"0
1
2
3
Step
√ó106 3.00 3.05 3.10 3.15 3.20 3.25 3.30 3.35 Value"
REFERENCES,0.6363636363636364,dmc_humanoid_walk
REFERENCES,0.638961038961039,"Dreamer
ED2-Dreamer"
REFERENCES,0.6415584415584416,"0
2
4
6
8
Step
√ó105 3.00 3.01 3.02 3.03 3.04 Value"
REFERENCES,0.6441558441558441,dmc_reacher_easy
REFERENCES,0.6467532467532467,"Dreamer
ED2-Dreamer"
REFERENCES,0.6493506493506493,"0.0
0.5
1.0
Step
√ó106 3.0 3.5 4.0 4.5 Value"
REFERENCES,0.6519480519480519,dmc_walker_run
REFERENCES,0.6545454545454545,"Dreamer
ED2-Dreamer"
REFERENCES,0.6571428571428571,"0
2
4
6
8
Step
√ó105 3.000 3.005 3.010 3.015 3.020 Value"
REFERENCES,0.6597402597402597,dmc_hopper_stand
REFERENCES,0.6623376623376623,"Dreamer
ED2-Dreamer"
REFERENCES,0.6649350649350649,Figure 8: The model error reduced when combine ED2 with Dreamer.
REFERENCES,0.6675324675324675,Under review as a conference paper at ICLR 2022
REFERENCES,0.6701298701298701,"F
ED2-CD-DREAMER"
REFERENCES,0.6727272727272727,"Here we provide the experiment result of complete decomposition in Figure 9. complete decompo-
sition can boost the performance in most environments. But in some complex environments like
humanoid and walker, it fails to improve the performance. We analysis that in humanoid and walker,
the correlation between action dimensions can‚Äôt be ignored. Complete decomposition break the
correlations between action dimensions and lead to poor performance in these tasks."
REFERENCES,0.6753246753246753,"0.0
0.5
1.0
Step
√ó106 0 200 400 600 800 Value"
REFERENCES,0.6779220779220779,dmc_cheetah_run
REFERENCES,0.6805194805194805,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.6831168831168831,"0.0
0.5
1.0
Step
√ó106 0 200 400 600 800 Value"
REFERENCES,0.6857142857142857,dmc_finger_spin
REFERENCES,0.6883116883116883,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.6909090909090909,"0
1
2
3
Step
√ó106 0 200 400 Value"
REFERENCES,0.6935064935064935,dmc_hopper_hop
REFERENCES,0.6961038961038961,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.6987012987012987,"0
1
2
3
Step
√ó106 0 50 Value"
REFERENCES,0.7012987012987013,dmc_humanoid_stand
REFERENCES,0.7038961038961039,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7064935064935065,"0
1
2
3
Step
√ó106 0 50 100 Value"
REFERENCES,0.7090909090909091,dmc_humanoid_walk
REFERENCES,0.7116883116883117,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7142857142857143,"0
2
4
6
8
Step
√ó105 0 200 400 600 800 Value"
REFERENCES,0.7168831168831169,dmc_reacher_easy
REFERENCES,0.7194805194805195,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7220779220779221,"0.0
0.5
1.0
Step
√ó106 0 200 400 600 800 Value"
REFERENCES,0.7246753246753247,dmc_walker_run
REFERENCES,0.7272727272727273,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7298701298701299,"0
2
4
6
8
Step
√ó105 0 200 400 600 800 Value"
REFERENCES,0.7324675324675325,dmc_hopper_stand
REFERENCES,0.7350649350649351,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7376623376623377,Figure 9: Comparisons between ED2-CD-Dreamer and Dreamer.
REFERENCES,0.7402597402597403,"G
COMBINE WITH MBPO"
REFERENCES,0.7428571428571429,"Here we provide the experiment results of ED2-MBPO method, which include the performance
(Figure 10), model error evaluation (Figure 11) and performance under complete decomposition
(Figure 12)."
REFERENCES,0.7454545454545455,"0
2
4
6
Step
√ó104 0 1000 2000 3000 Value"
REFERENCES,0.7480519480519481,Hopper
REFERENCES,0.7506493506493507,"MBPO
ED2-MBPO"
REFERENCES,0.7532467532467533,"0
1
2
3
4
Step
√ó104 0 2000 4000 6000 Value"
REFERENCES,0.7558441558441559,HalfCheetah
REFERENCES,0.7584415584415585,"MBPO
ED2-MBPO"
REFERENCES,0.7610389610389611,"0.0
0.5
1.0
Step
√ó105 0 2000 4000 Value Ant"
REFERENCES,0.7636363636363637,"MBPO
ED2-MBPO"
REFERENCES,0.7662337662337663,"0.0
0.4
0.8
1.2
Step
√ó105 0 1000 2000 3000 4000 Value"
REFERENCES,0.7688311688311689,Walker2d
REFERENCES,0.7714285714285715,"MBPO
ED2-MBPO"
REFERENCES,0.7740259740259741,Figure 10: Performance comparisons between ED2-MBPO and MBPO.
REFERENCES,0.7766233766233767,"0
2
4
6
Step
√ó104 0.000 0.003 0.006 0.009 Value"
REFERENCES,0.7792207792207793,Hopper
REFERENCES,0.7818181818181819,"MBPO
ED2-MBPO"
REFERENCES,0.7844155844155845,"0
1
2
3
4
Step
√ó104 0.0 0.2 0.4 0.6 Value"
REFERENCES,0.787012987012987,HalfCheetah
REFERENCES,0.7896103896103897,"MBPO
ED2-MBPO"
REFERENCES,0.7922077922077922,"0.0
0.5
1.0
Step
√ó105 0.0 0.3 0.6 0.9 1.2 Value Ant"
REFERENCES,0.7948051948051948,"MBPO
ED2-MBPO"
REFERENCES,0.7974025974025974,"0.0
0.4
0.8
1.2
Step
√ó105 0.0 0.5 1.0 1.5 Value"
REFERENCES,0.8,Walker2d
REFERENCES,0.8025974025974026,"MBPO
ED2-MBPO"
REFERENCES,0.8051948051948052,Figure 11: Model error comparisons between ED2-MBPO and MBPO.
REFERENCES,0.8077922077922078,"H
SD2 CLUSTERING RESULTS"
REFERENCES,0.8103896103896104,"We visualize the Ô¨Ånal partition result obtained by clustering here with both Ô¨Ågure and table form, the
Ô¨Ågure result of DeepMind Control Suite is shown in Figure 13, and the Ô¨Ågure result of Gym-Mujoco
is shown in Figure 14. The result shows that the clustering-based method tends to group the relative
action dimensions together, and the clustering results are also reasonable from the human point of
view."
REFERENCES,0.812987012987013,Under review as a conference paper at ICLR 2022
REFERENCES,0.8155844155844156,"0
2
4
6
Step
√ó104 0 1000 2000 3000 Value"
REFERENCES,0.8181818181818182,Hopper
REFERENCES,0.8207792207792208,"MBPO
ED2-CD-MBPO"
REFERENCES,0.8233766233766234,"0
1
2
3
4
Step
√ó104 0 2000 4000 6000 Value"
REFERENCES,0.825974025974026,HalfCheetah
REFERENCES,0.8285714285714286,"MBPO
ED2-CD-MBPO"
REFERENCES,0.8311688311688312,"0.0
0.5
1.0
Step
√ó105 0 2000 4000 Value Ant"
REFERENCES,0.8337662337662337,"MBPO
ED2-CD-MBPO"
REFERENCES,0.8363636363636363,"0.0
0.4
0.8
1.2
Step
√ó105 0 1000 2000 3000 4000 Value"
REFERENCES,0.8389610389610389,Walker2d
REFERENCES,0.8415584415584415,"MBPO
ED2-CD-MBPO"
REFERENCES,0.8441558441558441,Figure 12: Performance comparisons between ED2-CD-MBPO and MBPO.
REFERENCES,0.8467532467532467,"Humanoid
Walker"
REFERENCES,0.8493506493506493,Reacher
REFERENCES,0.8519480519480519,"Finger
Hopper
Cheetah"
REFERENCES,0.8545454545454545,"Figure 13: The visualization of Ô¨Ånal partition of environments in DeepMind Control Suite. Some
joints in humanoid is divided into two or three parts (e.g. abdomen joint). It indicates that there
are multiple action dimensions contained by this joint (e.g. abdomen joint contains abdomen_x,
abdomen_y and abdomen_z action dimensions)."
REFERENCES,0.8571428571428571,"As shown in Table 3, each row denotes a sub-dynamics discovered by the clustering-based method
in this environment (expect the Ô¨Ånal two-row in humanoid environment, they belong to the same
sub-dynamics. Because of the length of the table, we write it as two lines). The sub-dynamics we
discovered is very reasonable, and there is an obvious connection between the action dimensions in
the same sub-dynamics."
REFERENCES,0.8597402597402597,"I
ATARI EXPERIMENTS"
REFERENCES,0.8623376623376623,"We also conducted model error experiments on Atari environment and Atari-like Maze environment
(called Minecraft) (OmidshaÔ¨Åei et al., 2018). In this experiment, random policy is used to generate
data for dynamics model training. As shown in Figure 15, ED2 could bring a more accurate dynamics
modeling process."
REFERENCES,0.8649350649350649,"J
DREAMER WITH BIGGER HIDDEN SIZE"
REFERENCES,0.8675324675324675,"In this section, we provide the result of Dreamer method under bigger hidden size (which keeps the
similar parameter size as ED2-Dreamer). As shown in Figure 16, increasing the size of parameters
can not improve the performance."
REFERENCES,0.8701298701298701,Under review as a conference paper at ICLR 2022
REFERENCES,0.8727272727272727,"Walker2d
Hopper
HalfCheetah
Ant"
REFERENCES,0.8753246753246753,Figure 14: The visualization of Ô¨Ånal partition of environments in Gym-Mujoco.
REFERENCES,0.8779220779220779,"0
2
4
6
Step
√ó103 0.00 0.02 0.04 Value"
REFERENCES,0.8805194805194805,Minecraft
REFERENCES,0.8831168831168831,"Baseline
ED2"
REFERENCES,0.8857142857142857,"0
2
4
6
8
Step
√ó103"
REFERENCES,0.8883116883116883,0.0005
REFERENCES,0.8909090909090909,0.0010
REFERENCES,0.8935064935064935,0.0015 Value
REFERENCES,0.8961038961038961,Enduro-v0
REFERENCES,0.8987012987012987,"Baseline
ED2"
REFERENCES,0.9012987012987013,Figure 15: Model error experiments on Atari and Atari like maze environment.
REFERENCES,0.9038961038961039,"0
1
2
3
Step
√ó106 0 200 400 Value"
REFERENCES,0.9064935064935065,dmc_hopper_hop
REFERENCES,0.9090909090909091,"Dreamer
ED2-Dreamer
Dreamer_large_hidden"
REFERENCES,0.9116883116883117,"0.0
0.5
1.0
Step
√ó106 0 200 400 600 800 Value"
REFERENCES,0.9142857142857143,dmc_walker_run
REFERENCES,0.9168831168831169,"Dreamer
ED2-Dreamer
Dreamer_large_hidden"
REFERENCES,0.9194805194805195,Figure 16: The performance of Dreamer under bigger parameter size.
REFERENCES,0.922077922077922,Under review as a conference paper at ICLR 2022
REFERENCES,0.9246753246753247,"Table 3: The meaning of action dimensions in each environment (listed according the clustering
result)"
REFERENCES,0.9272727272727272,"ENVIRONMENT
ACTION DIMENSION MEANNING"
REFERENCES,0.9298701298701298,HUAMNOID(DEEPMIND)
REFERENCES,0.9324675324675324,RIGHT_ANKLE_X
REFERENCES,0.935064935064935,"LEFT_ANKLE_X
ABDOMEN_X, RIGHT_HIP_X, LEFT_HIP_X"
REFERENCES,0.9376623376623376,"ABDOMEN_Y, RIGHT_HIP_Y, LEFT_HIP_Y"
REFERENCES,0.9402597402597402,"RIGHT_KNEE,RIGHT_ANKLE_Y"
REFERENCES,0.9428571428571428,"LEFT_KNEE,LEFT_ANKLE_Y"
REFERENCES,0.9454545454545454,"RIGHT_HIP_Z, LEFT_HIP_Z
LEFT_SHOULDER1, LEFT_ELBOW, RIGHT_SHOULDER1, RIGHT_ELBOW,
RIGHT_SHOULDER2, LEFT_SHOULDER2, ABDOMEN_Z"
REFERENCES,0.948051948051948,WALKER(DEEPMIND)
REFERENCES,0.9506493506493506,"LEFT_HIP, RIGHT_HIP
LEFT_KNEE, RIGHT_KNEE
LEFT_ANKLE, RIGHT_ANKLE"
REFERENCES,0.9532467532467532,"CHEETAH(DEEPMIND)
BACK_THIGH, BACK_SHIN, BACK_FOOT
FRONT_THIGH, FRONT_SHIN, FRONT_FOOT"
REFERENCES,0.9558441558441558,HOPPER(DEEPMIND)
REFERENCES,0.9584415584415584,"WAIST, HIP"
REFERENCES,0.961038961038961,"KNEE
ANKLE"
REFERENCES,0.9636363636363636,"REACHER(DEEPMIND)
SHOULDER WRIST"
REFERENCES,0.9662337662337662,"FINGER(DEEPMIND)
PROXIMAL"
REFERENCES,0.9688311688311688,DISTAL
REFERENCES,0.9714285714285714,HALFCHEETAH(GYM-MUJOCO)
REFERENCES,0.974025974025974,"BACK_THIGH, BACK_SHIN
FRONT_THIGH, FRONT_FSHIN"
REFERENCES,0.9766233766233766,"BACK_FOOT
FRONT_FOOT"
REFERENCES,0.9792207792207792,WALKER2D(GYM-MUJOCO)
REFERENCES,0.9818181818181818,"RIGHT_THIGH, RIGHT_LEG, LEFT_THIGH, LEFT_LEG"
REFERENCES,0.9844155844155844,RIGHT_FOOT
REFERENCES,0.987012987012987,LEFT_FOOT
REFERENCES,0.9896103896103896,"HOPPER(GYM-MUJOCO)
THIGH, LEG FOOT"
REFERENCES,0.9922077922077922,ANT(GYM-MUJOCO)
REFERENCES,0.9948051948051948,"LEFT_FRONT_HIP, RIGHT_FRONT_HIP, LEFT_BACK_HIP, RIGHT_BACK_HIP"
REFERENCES,0.9974025974025974,"LEFT_FRONT_ANKLE, LEFT_BACK_ANKLE
RIGHT_FRONT_ANKLE, RIGHT_BACK_ANKLE"
