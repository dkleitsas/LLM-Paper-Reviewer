Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0058823529411764705,"Counterfactual examples are an appealing class of post-hoc explanations for ma-
chine learning models. Given input x of class y1, its counterfactual is a contrastive
example x′ of another class y0. Current approaches primarily solve this task by a
complex optimization: deﬁne an objective function based on the loss of the coun-
terfactual outcome y0 with hard or soft constraints, then optimize this function as
a black-box. This “deep learning” approach, however, is rather slow, sometimes
tricky, and may result in unrealistic counterfactual examples. In this work, we
propose a novel approach to deal with these problems using only two gradient
computations based on tractable probabilistic models. First, we compute an un-
constrained counterfactual u of x to induce the counterfactual outcome y0. Then,
we adapt u to higher density regions, resulting in x′. Empirical evidence demon-
strates the dominant advantages of our approach."
INTRODUCTION,0.011764705882352941,"1
INTRODUCTION"
INTRODUCTION,0.01764705882352941,"Explaining decisions made by intelligent systems, especially black-box models, is important. First
of all, a model that can explain their decisions is more likely to gain human trust, especially if there
are signiﬁcant consequences for incorrect results, or the problem has not been sufﬁciently studied
and validated in real-world applications (Simpson, 2007; Hoffman et al., 2013; Doshi-Velez & Kim,
2017). Secondly, explanations can promote fairness by exposing bias towards protected attributes in
the system (Prates et al., 2019; Buolamwini & Gebru, 2018; Obermeyer et al., 2019). In addition,
explanations are a helpful tool for debugging purpose (Holte, 1993; Freitas, 2014; Rudin, 2019)."
INTRODUCTION,0.023529411764705882,"Counterfactuals are closely related to classes of explanation, which much of the early work in AI
on explaining the decisions made by an expert or rule-based systems focused on (Wachter et al.,
2017). As the name implies, counterfactual is an alternative example counter to the fact which takes
the form “If X had not occurred, Y would not have occurred”. Explicitly establishing the connec-
tion between counterfactuals and explanations, Wachter et al. (2017) concluded that counterfactual
explanations can bridge the gap between the interests of data subjects and data controllers that oth-
erwise acts as a barrier to a legally binding right to explanation. They further formulate the problem
of ﬁnding counterfactual explanations as an optimization task that minimizes the loss of a desired
outcome w.r.t. an alternative input combined with regularization for distance to the query example.
In a similar vein, the dominant majority of recent counterfactual explanation methods are proposed
based on their adapted objective functions for optimization."
INTRODUCTION,0.029411764705882353,"Unfortunately, these deep learning approaches have some downsides: First of all, they are very
time-consuming for generating explanations due to the iterative optimization process, because for
every optimization iteration the model needs to be evaluated at least once and it may take a lot of
iterations until a candidate is found. Secondly, the objective function is usually highly non-convex,
which makes it more tricky to ﬁnd a satisfying solution in practice. Thirdly, the optimizer involves
additional hyperparameters that need to be carefully selected, such as learning rate. Furthermore,
although these methods optimize for the ”closest possible world” by constraining the distance of the
counterfactual to the query instance, they are still not aware of the underlying density and the data
manifold. As a direct consequence, the counterfactuals often appear rather unnatural and unrealistic
although they are as close as possible to the query instance. See ﬁgure 3 for example. This imposes
difﬁculties for humans to understand the explanations. As Nickerson (1998) argues, humans exhibit
conﬁrmation bias, meaning that we tend to ignore information that is inconsistent with our prior"
INTRODUCTION,0.03529411764705882,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.041176470588235294,"beliefs. In analogy, unrealistic counterfactuals also have very limited effect for communicating
explanations to humans."
INTRODUCTION,0.047058823529411764,"To improve upon these limitations, we propose a novel approach to generate counterfactual expla-
nations. Unlike the overwhelmingly dominant deep learning approach, we decouple the goal of
perturbing the prediction and maintaining an additional constraint on the perturbation. Speciﬁcally,
we view the task as a two-step process, whereby the ﬁrst step is to perturb a sample towards a desired
class and the second step is to constrain the perturbed sample to be close to the data manifold so it is
realistic and natural. The ﬁrst goal aligns with adversarial attacks (Goodfellow et al., 2015; Brown
et al., 2017; Yuan et al., 2019), where a small perturbation is computed to perturb the prediction. In
the literature there are gradient-based methods (Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016)
and optimization-based methods (Szegedy et al., 2013; Carlini & Wagner, 2017). Exploiting the fact
that gradient corresponds to the direction of steepest ascent, gradient-based perturbation is not only
very easy to implement with current deep learning frameworks, but also much faster to compute
compared to optimization-based perturbation. Therefore we adopt a gradient-based approach for
the ﬁrst step. To ensure the quality and effectiveness of the second step, we learn a sum-product-
network (SPN) (Poon & Domingos, 2011), a tractable density model, on the input, and we perturb
the example further based on the gradient that directs to steepest ascent of likelihood, evaluated on
the SPN."
INTRODUCTION,0.052941176470588235,"In summary, we make the following contributions:"
INTRODUCTION,0.058823529411764705,"1. We propose the ﬁrst approach for generating counterfactual examples using probabilistic
circuits to ensure density-awareness in a tractable way."
INTRODUCTION,0.06470588235294118,"2. We experiment with complex and high-dimensional real-world dataset, which was not dealt
with using domain-agnostic methods in the literature."
WE GIVE EMPIRICAL EVIDENCE TO DEMONSTRATE THE ADVANTAGES AND EFFECTIVENESS OF OUR AP-,0.07058823529411765,"3. We give empirical evidence to demonstrate the advantages and effectiveness of our ap-
proach: visually appealing examples with high density, fast computation, and high success
rate."
WE GIVE EMPIRICAL EVIDENCE TO DEMONSTRATE THE ADVANTAGES AND EFFECTIVENESS OF OUR AP-,0.07647058823529412,"We proceed as follows: First, we give an overview of the literature and summarize the shortcomings
of the existing approaches. Then we introduce RAT-SPNs, preparing for presenting our approach.
Finally, we evaluate our approach via empirical evidence and conclude our work."
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS,0.08235294117647059,"2
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS"
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS,0.08823529411764706,"Research attention on counterfactual explanations has been increasingly raised since Wachter et al.
(2017) presented the concept of unconditional counterfactual explanations as a novel type of ex-
planation of automated decisions. Identifying the resemblance between counterfactual explanations
and adversarial perturbations, Wachter et al. (2017) proposed to generate counterfactual explanations
based on the optimisation techniques used in the adversarial perturbation literature. Speciﬁcally, a
loss function is minimized w.r.t. its input, using standard gradient-based techniques, for a desired
output and a regularizer penalizing the distance between the counterfactual and the query."
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS,0.09411764705882353,"Following Wachter et al. (2017), Mothilal et al. (2020) augmented the loss with diversity constraint
to encourage diverse solutions. However, these approaches do not have explicit knowledge of the
underlying density or data manifold, which may lead to unrealistic counterfactual explanations. A
bunch of methods counteract this issue by learning an auxiliary generative model to impose addi-
tional density constraints on the optimization process. As common choice for density approxima-
tor, Variational Autoencoder (VAE) (Kingma & Welling, 2014) and its variants (Klys et al., 2018;
Ivanov et al., 2019) are used. For instance, Dhurandhar et al. 2018 proposed contrastive explana-
tions method (CEM) for neural networks based on optimization. The objective function consists
of a hinge-like loss function and the elastic net regularizer as well as an auxiliary VAE to evalu-
ate the proximity to the data manifold. Ustun et al. 2019 deﬁned the term recourse as the ability
of a person to change the decision of a model by altering actionable input variables. Recourse is
evaluated by solving an optimization problem. Joshi et al. 2019 provided an algorithm, called RE-
VISE, to suggest a recourse, based on samples from the latent space of a VAE characterizing the
data distribution. Pawelczyk et al. 2020 developed a framework, called C-CHVAE, to generate
faithful counterfactuals. C-CHVAE trains a VAE and returns the closest counterfactual due to a"
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS,0.1,Under review as a conference paper at ICLR 2022
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS,0.10588235294117647,"Figure 1: The SPN structure
used to estimate a distribu-
tion as a mixture of class-
conditional densities.
Each
class-conditional density is in
turn represented by a sub-
SPN. Classiﬁcation is done
via Bayes’ rule."
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS,0.11176470588235295,"nearest neighbour style search in the latent space. Downs et al. 2020 proposed another algorith-
mic recourse generation method, CRUDS, that generates multiple recourses satisfying underlying
structure of the data as well as end-user speciﬁed constraints. Based on a VAE-variant, CRUDS
uses a Conditional Subspace Variational Autoencoder (CSVAE) model (Klys et al., 2018) that is
capable of extracting latent features that are relevant for prediction. Another method called CLUE
Antor´an et al. (2020) is proposed for interpreting uncertainty estimates from differentiable proba-
bilistic models using counterfactual explanations, by searching in the latent space of a VAE with
arbitrary conditioning (VAEAC) (Ivanov et al., 2019)."
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS,0.11764705882352941,"Poyiadzi et al. 2020 proposed FACE, a graph-based algorithm to generate counterfactuals that are
coherent with the underlying data distribution by constructing a graph over all the candidate targets.
Besides, several domain-speciﬁc approaches are also emerging Olson et al. (2021); Goyal et al.
(2019); Chang et al. (2019)."
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS,0.12352941176470589,"The aforementioned techniques have the following shortcomings: 1. The deep learning-like ap-
proach (Wachter et al., 2017; Mothilal et al., 2020; Dhurandhar et al., 2018; Ustun et al., 2019;
Joshi et al., 2019) is too slow to generate explanations on the ﬂy due to the iterative optimization
process, which in turn comes with additional tuning parameters. 2. Although highly expressive,
neural density estimators such as VAEs are highly intractable, which makes explicit density con-
straint on counterfactual explanations infeasible (Dhurandhar et al., 2018; Ivanov et al., 2019; Klys
et al., 2018; Joshi et al., 2019; Pawelczyk et al., 2020). In addition, they come with the overhead of
latent representation, which is not necessarily in good quality when maximum likelihood training is
used to learn them (Alemi et al., 2018; Dai & Wipf, 2019)."
RELATED WORK ON COUNTERFACTUAL EXPLANATIONS,0.12941176470588237,"We improve upon the aforementioned shortcomings by proposing the ﬁrst counterfactual method
using tractable probabilistic circuits, speciﬁcally sum-product networks (SPNs) (Darwiche, 2003;
Poon & Domingos, 2011)."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.13529411764705881,"3
GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.1411764705882353,"Before explaining our idea, we ﬁrst introduce the tractable probabilistic models we use."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.14705882352941177,"An sum-product network (SPN) S over X is a tractable probabilistic model for P(X) based on a
directed acyclic graph (DAG). This graph indicates computation for probabilistic inference and con-
sists of three types of nodes: univariate leaf nodes, sum nodes, and product nodes. Let ch(·) denote
the children of a node. A sum node S is weighted sum of its children, i.e. S = P"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.15294117647058825,"N∈ch(S) wS,NN
where the weights wS,N are non-negative and sum to 1, i.e. wS,N ≥0, P"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.1588235294117647,"N wS,N = 1. Sum nodes
can be viewed as mixtures of their child distributions. A product node P is product of its children,
i.e. P = Q"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.16470588235294117,"N∈ch(P ) N. The root node represents P(X). Products are factorized distributions,"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.17058823529411765,Under review as a conference paper at ICLR 2022
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.17647058823529413,"Figure 2: Left: Illustration of our gradient-based approach. Arrow indicates perturbation based
on a gradient step. Right: An example on MNIST. The ﬁrst row corresponds to the ﬁrst gradient
step for perturbing the prediction irregardless of the manifold. This perturbation removes some
characteristics of the current class and adds some characteristics of the counterfactual class. The
resulting sample u yields the desired class but obviously deviates from the training sets (It looks
neither like a 1 nor 7). The second row corresponds to the second gradient step for generating
in-distribution counterfactual by pushing the intermediate sample u to a region with higher density."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.18235294117647058,"implying independence assumption among their children. SPNs allow for fast, exact inference on
high-treewidth models."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.18823529411764706,"Unlike most of the probabilistic deep learning approaches, SPNs permit exact and efﬁcient inference.
Speciﬁcally, they are able to compute any marginalization and conditioning query in time linear of
the model’s representation size. By employing SPNs for deep learning, random and tensorized SPNs
(RAT-SPNs) (Peharz et al., 2020) are proposed using a simple approach to construct random SPN
structure and combine it with GPU-based optimization. It is worth noting that RAT-SPNs are not
fooled by certain out-of-domain image detection tests on which VAEs, normalizing ﬂows (NFs), and
auto-regressive density estimators (ARDEs) consistently fail (Choi & Jang, 2018; Nalisnick et al.,
2018)."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.19411764705882353,"To learn the parameters ω of a given RAT-SPN structure S in generative setting to approximate a
distribution P ∗(X), we assume i.i.d. samples X = {x1, . . . , xN} are given. Then maximum like-
lihood estimation is employed, i.e. ω = arg max 1"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.2,"N
PN
n=1 log S(xn), where S(x) is a distribution
over X represented by the RAT-SPN S."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.20588235294117646,"Apart from the standard use for density estimation, RAT-SPNs can be used as a generative classiﬁer
as well. Consider a classiﬁcation problem f : Rd →{1, . . . , C} with C labels, C roots are used
to represent class-conditional densities Sc(X) =: S(X|Y = y). The overall density distribution
is then given by S(X) = P"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.21176470588235294,y S(X|y)P(y). See ﬁgure 1 for illustration. Bayes’ rule is used to
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.21764705882352942,classify a sample x: S(Y |x) = S(x|Y )P (Y )
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.2235294117647059,"S(x)
. In other words, a RAT-SPN of this special structure
has dual use: It is both a density estimator S(X) and a classiﬁer S(Y |X). We will use this model
to demonstrate our approach because it can be used for classiﬁcation and yield tractable density
evaluation for free at the same time. However, our approach can be easily extended to deep neural
classiﬁers by training an auxiliary RAT-SPN for density estimation. For more details on RAT-SPN,
check out appendix."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.22941176470588234,"Our approach is deﬁned as two serial perturbations: In the ﬁrst step, we maximize log S(y′|x)"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.23529411764705882,S(y|x) to
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.2411764705882353,"induce desired outcome y′, which is equivalent to maximizing log S(x|y′)"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.24705882352941178,S(x|y) since
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.2529411764705882,log S(y′|x)
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.25882352941176473,"S(y|x) = log
S(x|y′)P(y′)"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.2647058823529412,"S(x)
S(x)
S(x|y)P(y)"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.27058823529411763,"
= log S(x|y′)"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.27647058823529413,"S(x|y) ,"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.2823529411764706,"when assuming a uniform class prior. Towards this end, we take a gradient step towards the steepest
ascent of the counterfactual outcome. This results in the perturbed example u where"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.28823529411764703,"u = x + ∂
 
log S(x|y′) −log S(x|y)
"
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.29411764705882354,"∂x
∗ϵ1."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.3,Under review as a conference paper at ICLR 2022
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.3058823529411765,"This step is similar to generating an adversarial perturbation in that the sample u is expected to
change the prediction to y′ from y with a very slight change."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.31176470588235294,"However, without additional constraints, the perturbed sample u is very likely to deviate from the
underlying data manifold. In order to generate in-distribution counterfactuals, we maximize the
density P ∗(u) of the current sample u, which is approximated via the SPN S(u). In contrast to the
density estimators used primarily in the literature, SPNs have the merit that they allow for tractable
density evaluation. To maximize the density S(u), we take another gradient step towards its steepest
ascent, i.e."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.3176470588235294,x′ = u + ∂S(u)
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.3235294117647059,"∂u
∗ϵ2."
"GRADIENT-BASED COUNTERFACTUAL EXPLANATIONS
USING TRACTABLE PROBABILISTIC MODELS",0.32941176470588235,x′ is the ﬁnal counterfactual explanation for x that yields y′ instead of y. See ﬁgure 2 for illustration.
EMPIRICAL EVALUATION,0.3352941176470588,"4
EMPIRICAL EVALUATION"
EMPIRICAL EVALUATION,0.3411764705882353,"To illustrate the advantages of our approach, we designed experiments to evaluate it both qualita-
tively and quantitatively across several benchmark datasets. All the experiments are implemented
in Python and Tensorﬂow, running on a Linux machine with two Intel Xeon processors with 56
hyper-threaded cores, 4 NVDIA GeForce GTX 1080 under Ubuntu Linux 14.04."
EMPIRICAL EVALUATION,0.34705882352941175,"Datasets: We experiment with three widely cited datasets commonly used in the counterfactual ex-
planation literature and one real-world dataset. MNIST (LeCun, 1998). In particular, we evaluate
across several contrastive pairs of classes where counterfactual perturbations are intuitive to com-
prehend: digit 1 and 4, digit 1 and 7, digit 3 and 8, and digit 7 and 4. German credit dataset (ger)
classiﬁes people described by a set of attributes as good or bad credit risks. Adult-Income (Ronny &
Barry, 1996) records whether a person makes over 50K a year based on census data. Caltech-UCSD
Birds (CUB) (Wah et al., 2011). This is a real-world dataset for ﬁne-grained bird classiﬁcation. This
dataset contains 200 bird species and we evaluate the counterfactuals across three contrastive pairs of
bird species: Red Faced Cormorant and Crested Auklet, Myrtle Warbler and Olive sided Flycatcher,
Horned Grebe and Eared Grebe. Bird species classiﬁcation is a difﬁcult problem that pushes the
limits of the visual abilities for both humans and computers. Some pairs of bird species are nearly
visually indistinguishable and intraclass variance is very high. This is arguably the most complex
and high dimensional problem studied so far in the counterfactual explanation literature. To make
this problem slightly more approachable, we work with feature representations extracted from the
ﬁnal convolutional layers of VGG-16 (Simonyan & Zisserman, 2015) pretrained on ImageNet. That
is, we train a RAT-SPN as a generative classiﬁer using the class-conditional feature maps."
EMPIRICAL EVALUATION,0.35294117647058826,"Baseline: We consider three widely cited model-agnostic approaches in the literature that can be
directly applied to our chosen datasets: Wachter et al., CEM (Dhurandhar et al., 2018) and FACE
(Poyiadzi et al., 2020) 1."
EMPIRICAL EVALUATION,0.3588235294117647,"Implementation details: To evaluate our approach with empirical evidence, we trained a RAT-SPN
for each dataset. Each RAT-SPN is, as in ﬁgure 1, a mixture of class-conditional densities. This
RAT-SPN is used for both classiﬁcation and density estimation. For each dataset, the counterfactual
approaches are evaluated on the same RAT-SPN classiﬁer. We used cross-validation to select hyper-
parameters for RAT-SPNs and for all the counterfactual methods. See appendix for more details."
EMPIRICAL EVALUATION,0.36470588235294116,"4.1
OUR COUNTERFACTUAL EXAMPLES ARE VISUALLY APPEALING."
EMPIRICAL EVALUATION,0.37058823529411766,"Counterfactual examples can be presented to users as contrastive explanations. The examples that
correspond to prior beliefs of the users can be better received by them due to conﬁrmation bias
(Nickerson, 1998). That means, the counterfactual examples should appear plausible and not deviate
too far from the training samples. Figure 3 demonstrates some examples on MNIST for a variety of
test cases across four methods. It is obvious to see that our approach consistently yields the most
visually appealing examples as they are smooth, clean and look very plausible. Although being"
EMPIRICAL EVALUATION,0.3764705882352941,"1Since the real-world dataset is quite high-dimensional compared to those commonly used in the literature,
we discarded some baselines with scalability issue after experimenting with it, e.g. DiCE (Mothilal et al.,
2020)."
EMPIRICAL EVALUATION,0.38235294117647056,Under review as a conference paper at ICLR 2022 Query
EMPIRICAL EVALUATION,0.38823529411764707,1 →4 1 →4 1 →4 7 →4 7 →4 7 →4 3 →8 3 →8 3 →8 8 →3 8 →3 8 →3
EMPIRICAL EVALUATION,0.3941176470588235,Wachter CEM FACE Ours
EMPIRICAL EVALUATION,0.4,"Figure 3: Counterfactual examples on MNIST across several classes and methods. The top row
indicates the original class and the target class. Query"
EMPIRICAL EVALUATION,0.40588235294117647,"Red Faced Cormorant →
Crested Auklet"
EMPIRICAL EVALUATION,0.4117647058823529,"Myrtle Warbler →
Olive sided Flycatcher
Horned Grebe →Eared Grebe"
EMPIRICAL EVALUATION,0.4176470588235294,Target
EMPIRICAL EVALUATION,0.4235294117647059,Wachter CEM FACE Ours
EMPIRICAL EVALUATION,0.4294117647058823,"Figure 4: Counterfactual examples on Caltech-UCSD Birds (CUB) across several classes and meth-
ods. The top row indicates the original class and the target class."
EMPIRICAL EVALUATION,0.43529411764705883,"smooth, our counterfactual examples still show a nice variation: For example, in the 7-th and 8-
th column, the counterfactual 8 tilts to the left when the query image tilts slightly to the left, and
likewise to the right. In comparison, Wachter et al. and CEM (Dhurandhar et al., 2018) both yield
quite wiggly and noisy results. Although CEM includes a VAE reconstruction loss in its objective
function as a proxy for constraining the examples to be in-distribution, explicit evaluation of density
is intractable. FACE (Poyiadzi et al., 2020) also yields very plausible examples because it simply
returns a training instance of the counterfactual class."
EMPIRICAL EVALUATION,0.4411764705882353,"Another visual example can be seen on CUB. As previously said, we trained a RAT-SPN as a gen-
erative classiﬁer using the class-conditional feature maps generated by VGG-16. In this case, the"
EMPIRICAL EVALUATION,0.4470588235294118,Under review as a conference paper at ICLR 2022
EMPIRICAL EVALUATION,0.45294117647058824,"RAT-SPN gives a density model on the feature maps and the counterfactual explanations are also
computed in this extracted feature space. Since feature maps can not be trivially transformed back to
raw features, we strive only for highlighting the salient counterfactual features instead of imputing
them. Figure 4 demonstrates some test examples on this dataset. The heatmap overlaid on the exam-
ple indicates the salient counterfactual features that should be perturbed to become the counterfactual
class. An example from each counterfactual class is shown in the ﬁrst row. Note that this is not the
particular target we strive for but only an example for the readers to have an intuitive idea about the
contrastive features. It can be seen that our approach consistently yields plausible heatmaps: For
the Red Faced Cormoran the beak is highlighted when the image focuses on the head, and the tail is
highlighted when the image zooms out. For the Myrtle Warbler the salient features are mostly on the
yellow spot of the feather. For the Horned Grebe the salient features are mostly highlighted around
the head. Among the baselines, CEM and FACE are implicitly density-aware and often show con-
sistent behavior with our approach. However, FACE sometimes yields saliency on the background
instead of on the bird. Wachter et al. performs the worst and yields quite random and unintuitive
heatmaps. In conclusion, our approach showed very competitive and intuitively plausible heatmaps
that highlight the contrastive features on this complex dataset."
EMPIRICAL EVALUATION,0.4588235294117647,"From visual examples on both datasets we can see our approach yields appealing and intuitive results
that are easy to comprehend."
EMPIRICAL EVALUATION,0.4647058823529412,"4.2
OUR COUNTERFACTUAL EXAMPLES HAVE HIGH LIKELIHOOD."
EMPIRICAL EVALUATION,0.47058823529411764,"In contrast to wiggly examples, smooth examples often appear more plausible and realistic, which
are in turn often tied with high likelihood. This experiment is to offer a quantitative evaluation on
likelihood."
EMPIRICAL EVALUATION,0.4764705882352941,"It is widely agreed that out-of-distribution (OOD) counterfactual examples have very little use in
communicating explanations with humans. Wachter et al. (2017) penalizes distance to the query
instance, which indirectly also prevents the counterfactual examples to deviate too far from the dis-
tribution. More recent approaches use VAEs as a neural density estimator to penalize OOD examples
(Dhurandhar et al., 2018; Ivanov et al., 2019; Klys et al., 2018; Joshi et al., 2019; Pawelczyk et al.,
2020). FACE (Poyiadzi et al., 2020) takes a different approach and searches through the training
samples directly for a counterfactual example instead of constructing it via an optimization process."
EMPIRICAL EVALUATION,0.4823529411764706,"However, these approaches are not able to constrain density explicitly and directly because den-
sity evaluation is simply intractable. The direct consequence is that the proxy constraint does not
consistently yield counterfactual examples in high-density region. To conﬁrm this with empirical
evidence, we take the advantage of RAT-SPNs on tractable inference to efﬁciently estimate and
evaluate density."
EMPIRICAL EVALUATION,0.48823529411764705,"Table 1 summarizes the average density evaluation on all the counterfactual examples across four
datasets using four candidate approaches. On three of these datasets, our approach yields the best
density among the baselines. FACE is a strong competitor in terms of likelihood because it always
returns a training instance. But its limitation is obvious: It assumes we have access to the training
samples, which is oftentimes not the case, e.g. due to privacy reasons."
EMPIRICAL EVALUATION,0.49411764705882355,"To have some intuition on tabular data, see table 2 for some randomly selected examples on the
German credit dataset. The columns from A10-1 to A15-3 are all one-hot encoded categorical
attributes. For example, A10-1, A10-2 and A10-3 encode feature 10. Due to space constraint,
only mutable features are shown. Each feature is perfectly negatively correlated with other features
from the same categorical attribute due to the constraint imposed by one-hot encoding. Therefore
we expect counterfactual perturbation to obey this feature correlation in order to stay close to the
underlying distribution. That means, if one feature has positive perturbation, the rest features from
the same attribute should have negative perturbation. That means, their perturbations should sum up
to almost zero. Ideally each perturbation should be -1 or +1, but we work with continuous values in
practice so this is often not the case. From table 2 one can see that our approach and FACE always
respect this relation — the perturbations within each attribute always sum up to zero or nearly zero,
showing an negative correlation. It is no surprise that FACE always perfectly reﬂects this constraint
because its counterfactual examples directly come from the training set, which perfectly satisfy
this constraint by construction. In contrast, Wachter et al. and CEM often violate this constraint:
Take Wachter et al. as example, its perturbation on the ﬁrst query yields 1, 1, -1 (sum up to 1) on"
EMPIRICAL EVALUATION,0.5,Under review as a conference paper at ICLR 2022
EMPIRICAL EVALUATION,0.5058823529411764,"Table 1: Density evaluation in log scale averaged on the test set (the higher, the better). Best result
indicated using •, runner-ups ◦.
Wachter
CEM
FACE
Ours"
EMPIRICAL EVALUATION,0.5117647058823529,"MNIST
-738.51
-735.13
-733.73◦
-725.13•
Credit
-50.61
-43.55◦
-43.83
-41.67•
Adult
-114.20
-96.99
-96.26•
-96.42◦
CUB
-4593.85
-4505.00
-4501.99◦
-4501.23•"
EMPIRICAL EVALUATION,0.5176470588235295,"Table 2: Counterfactual perturbations for German credit dataset with attributes encoded in on a one-
hot fashion. Attribute 10: Other debtors / guarantors — A10-1 : none, A10-2 : co-applicant, A103
: guarantor, Attribute 14: Other installment plans — A14-1 : bank, A142 : stores, A14-3 : none,
Attribute 15: Housing — A151 : rent, A15-2 : own, A15-3 : for free. A cell color “green” denotes a
negative correlation that we expect from the counterfactual perturbation. A cell color “red” denotes
a wrong correlation."
EMPIRICAL EVALUATION,0.5235294117647059,"Attribute 10
Attribute 14
Attribute 15
A10-1
A10-2
A10-3
A14-1
A14-2
A14-3
A15-1
A15-2
A15-3
ˆy
query
0.00
0.00
1.00
0.00
0.00
1.00
0.00
1.00
0.00
0
Wachter
0.00
1.00
-1.00
1.00
1.00
-1.00
1.00
-1.00
1.00
1
CEM
0.03
0.05
-0.09
0.08
0.08
-0.08
0.07
-0.29
0.08
1
FACE
1.00
0.00
-1.00
0.00
0.00
0.00
0.00
0.00
0.00
1
Ours
0.90
0.05
-0.95
0.18
0.05
-0.23
0.22
-0.39
0.16
1
query
1.00
0.00
0.00
0.00
0.00
1.00
1.00
0.00
0.00
0
Wachter
0.00
1.00
0.00
1.00
1.00
-1.00
0.00
0.00
1.00
1
CEM
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1
FACE
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1
Ours
-0.10
0.05
0.05
0.18
0.05
-0.23
-0.78
0.61
0.17
1
query
1.00
0.00
0.00
0.00
0.00
1.00
0.00
1.00
0.00
0
Wachter
0.00
1.00
0.00
1.00
1.00
-1.00
1.00
-1.00
1.00
1
CEM
0.00
0.00
0.00
0.00
0.00
0.00
0.00
-0.21
0.00
0
FACE
0.00
0.00
0.00
0.00
0.00
0.00
1.00
-1.00
0.00
1
Ours
-0.10
0.05
0.05
0.18
0.048
-0.23
0.22
-0.39
0.17
1"
EMPIRICAL EVALUATION,0.5294117647058824,"attribute 14. These perturbations are obviously not balanced. This intuitive example is related to
their likelihood evaluation (see table 1): Those who respect the constraint better tend to yield higher
likelihood because they are more realistic."
EMPIRICAL EVALUATION,0.5352941176470588,"In conclusion, our counterfactual examples have dominant advantages on staying close to high-
density region."
EMPIRICAL EVALUATION,0.5411764705882353,"4.3
OUR APPROACH IS MUCH FASTER TO COMPUTE."
EMPIRICAL EVALUATION,0.5470588235294118,"The baseline methods employ a methodology that’s widely represented in the literature: Deﬁning a
complex objective function with additional constraints and use optimization techniques to iteratively
ﬁnd a solution. This is usually too slow to yield counterfactual examples on the ﬂy. Especially when
users want to interact with machine explanations, fast computation becomes more essential. As
empirical evidence, table 3 shows that our method is an order-of-magnitude faster than the baseline
approaches. This is no surprise due to the fact that our approach takes only two gradient steps while
the baselines can easily take up to thousands of iterations."
EMPIRICAL EVALUATION,0.5529411764705883,"4.4
OUR APPROACH IS EFFECTIVE."
EMPIRICAL EVALUATION,0.5588235294117647,"The goal of counterfactual explanation is to ﬁnd a contrastive example that changes the class predic-
tion. An approach is effective if it has a higher success rate in perturbing the class prediction to the
counterfactual class. This measurement is widely reported in the literature."
EMPIRICAL EVALUATION,0.5647058823529412,Under review as a conference paper at ICLR 2022
EMPIRICAL EVALUATION,0.5705882352941176,"Table 3: Average computation time on the test set in seconds (the lower, the better). Best result
shown in bold.
Wachter
CEM
FACE
Ours"
EMPIRICAL EVALUATION,0.5764705882352941,"MNIST
181
216
281
27
Credit
59
97
757
21
Adult
52
57
1722
10
CUB
65
468
63
16"
EMPIRICAL EVALUATION,0.5823529411764706,"Table 4: Success rate is measured by the ratio of counterfactual examples that yields the target
prediction class (the higher, the better). Note that success for CEM is measured by any prediction
perturbation for its own fairness, i.e. the perturbed prediction does not need to be the speciﬁed target
class.
Wachter
CEM
FACE
Ours"
EMPIRICAL EVALUATION,0.5882352941176471,"MNIST
1.00•
0.90◦
0.54
0.71
Credit
1.00•
0.87
0.92
1.00•
Adult
1.00•
0.93
0.50
0.99◦
CUB
1.00•
0.73
0.90
0.98◦"
EMPIRICAL EVALUATION,0.5941176470588235,"Except for CEM, all the success rate are measured by the ratio between examples with the counter-
factual prediction and all the test examples. Since CEM encourages a contrastive example belonging
to any other class than the original class, measuring its success only by a speciﬁc class prediction
is not fair for CEM. Therefore we measure its success rate by the ratio between examples with per-
turbed class prediction and all the test examples. This metric is ranged between 0 and 1 where 1 is
the best and 0 is the worst."
EMPIRICAL EVALUATION,0.6,"Table 4 gives a summary of success rate. Wachter et al. is very effective at perturbing the class
prediction, with a success rate of 1.0 across various datasets. CEM and FACE are less effective. Our
approach has slightly less success rate than Wachter et al. but still very effective in general with a
very reasonable success rate. This result is not surprising: Wachter et al. has the fewest constraint
in its objective function, while CEM and our approach face a trade-off between prediction success
and density constraint."
EMPIRICAL EVALUATION,0.6058823529411764,"In conclusion, although being so fast to compute, our approach does not sacriﬁce the effectiveness
of perturbing the class prediction."
CONCLUSIONS AND FUTURE WORK,0.611764705882353,"5
CONCLUSIONS AND FUTURE WORK"
CONCLUSIONS AND FUTURE WORK,0.6176470588235294,"In this work we presented a novel way of generating counterfactual examples using tractable prob-
abilistic inference. The core idea is to view counterfactual example generation as a two-step pro-
cess: First perturb the class prediction irregardless of the underlying distribution by maximizing
the conditional likelihood of the counterfactual class. As this would probably result in an unrealis-
tic counterfactual example that is far away from the underlying distribution, we then maximize its
likelihood in the second step by taking a step towards the direction of the gradient of the likelihood
function. This is possible because density evaluation is tractable for the probabilistic models we use.
Our approach is not only very effective for generating counterfactual examples, but also very fast to
compute. In addition, the counterfactual examples have high likelihood."
CONCLUSIONS AND FUTURE WORK,0.6235294117647059,"One interesting direction to investigate in future work is generating counterfactual examples inter-
actively based on human feedback. Another direction is to enforce more complicated or speciﬁc
constraints on the counterfactual examples. As a highly tractable generative model, SPNs allow for
a wide variety of tractable probabilistic inferences, which could be used to formulate more complex
constraints. It is also interesting to incorporate human supervision on counterfactual explanations
to improve the underlying classiﬁcation model. All of these research directions would not be ap-
proachable with the existing approaches."
CONCLUSIONS AND FUTURE WORK,0.6294117647058823,Under review as a conference paper at ICLR 2022
REFERENCES,0.6352941176470588,REFERENCES
REFERENCES,0.6411764705882353,"German credit dataset.
https://archive.ics.uci.edu/ml/support/statlog+
(german+credit+data). Accessed: 2021."
REFERENCES,0.6470588235294118,"Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif A Saurous, and Kevin Murphy.
Fixing a broken elbo, 2018. In International Conference on Machine Learning, volume 1, 2018."
REFERENCES,0.6529411764705882,"Javier Antor´an, Umang Bhatt, Tameem Adel, Adrian Weller, and Jos´e Miguel Hern´andez-Lobato.
Getting a clue: A method for explaining uncertainty estimates. arXiv preprint arXiv:2006.06848,
2020."
REFERENCES,0.6588235294117647,"Tom B Brown, Dandelion Man´e, Aurko Roy, Mart´ın Abadi, and Justin Gilmer. Adversarial patch.
arXiv preprint arXiv:1712.09665, 2017."
REFERENCES,0.6647058823529411,"Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commer-
cial gender classiﬁcation. In Conference on fairness, accountability and transparency, pp. 77–91.
PMLR, 2018."
REFERENCES,0.6705882352941176,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39–57. IEEE, 2017."
REFERENCES,0.6764705882352942,"Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image clas-
siﬁers by counterfactual generation. In International Conference on Learning Representations,
2019."
REFERENCES,0.6823529411764706,Hyunsun Choi and Eric Jang. Generative ensembles for robust anomaly detection. 2018.
REFERENCES,0.6882352941176471,"Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789,
2019."
REFERENCES,0.6941176470588235,"Adnan Darwiche. A differential approach to inference in bayesian networks. Journal of the ACM
(JACM), 50(3):280–305, 2003."
REFERENCES,0.7,"Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shan-
mugam, and Payel Das. Explanations based on the missing: Towards contrastive explanations
with pertinent negatives. arXiv preprint arXiv:1802.07623, 2018."
REFERENCES,0.7058823529411765,"Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.
arXiv preprint arXiv:1702.08608, 2017."
REFERENCES,0.711764705882353,"Michael Downs, Jonathan L Chu, Yaniv Yacoby, Finale Doshi-Velez, and Weiwei Pan. Cruds: Coun-
terfactual recourse using disentangled subspaces. In ICML Workshop on Human Interpretability
in Machine Learning, 2020."
REFERENCES,0.7176470588235294,"Alex A Freitas. Comprehensible classiﬁcation models: a position paper. ACM SIGKDD explorations
newsletter, 15(1):1–10, 2014."
REFERENCES,0.7235294117647059,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, ICLR, 2015."
REFERENCES,0.7294117647058823,"Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual
explanations. In International Conference on Machine Learning, pp. 2376–2384. PMLR, 2019."
REFERENCES,0.7352941176470589,"Robert R Hoffman, Matthew Johnson, Jeffrey M Bradshaw, and Al Underbrink. Trust in automation.
IEEE Intelligent Systems, 28(1):84–88, 2013."
REFERENCES,0.7411764705882353,"Robert C Holte. Very simple classiﬁcation rules perform well on most commonly used datasets.
Machine learning, 11(1):63–90, 1993."
REFERENCES,0.7470588235294118,"Oleg Ivanov, Michael Figurnov, and Dmitry Vetrov. Variational autoencoder with arbitrary condi-
tioning. In Proceedings of the International Conference on Learning Representations, 2019."
REFERENCES,0.7529411764705882,"Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. Towards
realistic individual recourse and actionable explanations in black-box decision making systems.
arXiv preprint arXiv:1907.09615, 2019."
REFERENCES,0.7588235294117647,Under review as a conference paper at ICLR 2022
REFERENCES,0.7647058823529411,"D. P. Kingma and M. Welling. Auto-encoding variational bayes. In Proceedings of the International
Conference on Learning Representations, 2014."
REFERENCES,0.7705882352941177,"Jack Klys, Jake Snell, and Richard Zemel. Learning latent subspaces in variational autoencoders.
arXiv preprint arXiv:1812.06190, 2018."
REFERENCES,0.7764705882352941,"Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples in the physical world,
2016."
REFERENCES,0.7823529411764706,"Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998."
REFERENCES,0.788235294117647,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pp. 2574–2582, 2016."
REFERENCES,0.7941176470588235,"Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classiﬁers
through diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency, pp. 607–617, 2020."
REFERENCES,0.8,"Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do
deep generative models know what they don’t know? arXiv preprint arXiv:1810.09136, 2018."
REFERENCES,0.8058823529411765,"Raymond S Nickerson. Conﬁrmation bias: A ubiquitous phenomenon in many guises. Review of
general psychology, 2(2):175–220, 1998."
REFERENCES,0.8117647058823529,"Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias
in an algorithm used to manage the health of populations. Science, 366(6464):447–453, 2019."
REFERENCES,0.8176470588235294,"Matthew L Olson, Roli Khanna, Lawrence Neal, Fuxin Li, and Weng-Keen Wong. Counterfac-
tual state explanations for reinforcement learning agents via generative deep learning. Artiﬁcial
Intelligence, 295:103455, 2021."
REFERENCES,0.8235294117647058,"Martin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-agnostic counterfactual
explanations for tabular data. In Proceedings of The Web Conference 2020, pp. 3126–3132, 2020."
REFERENCES,0.8294117647058824,"Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao, Martin Trapp,
Kristian Kersting, and Zoubin Ghahramani. Random sum-product networks: A simple and effec-
tive approach to probabilistic deep learning. In Uncertainty in Artiﬁcial Intelligence, pp. 334–344.
PMLR, 2020."
REFERENCES,0.8352941176470589,"Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In 2011
IEEE International Conference on Computer Vision Workshops (ICCV Workshops), pp. 689–690.
IEEE, 2011."
REFERENCES,0.8411764705882353,"Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. Face: Feasible
and actionable counterfactual explanations. In Proceedings of the AAAI/ACM Conference on AI,
Ethics, and Society, pp. 344–350, 2020."
REFERENCES,0.8470588235294118,"Marcelo OR Prates, Pedro H Avelar, and Luis C Lamb. Assessing gender bias in machine translation:
a case study with google translate. Neural Computing and Applications, pp. 1–19, 2019."
REFERENCES,0.8529411764705882,"Kohavi Ronny and Becker Barry.
UCI machine learning repository, 1996.
URL https:
//archive.ics.uci.edu/ml/datasets/adult."
REFERENCES,0.8588235294117647,"Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and
use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019."
REFERENCES,0.8647058823529412,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations, ICLR, 2015."
REFERENCES,0.8705882352941177,"Jeffry A Simpson. Psychological foundations of trust. Current directions in psychological science,
16(5):264–268, 2007."
REFERENCES,0.8764705882352941,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013."
REFERENCES,0.8823529411764706,Under review as a conference paper at ICLR 2022
REFERENCES,0.888235294117647,"Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classiﬁcation. In
Proceedings of the Conference on Fairness, Accountability, and Transparency, pp. 10–19, 2019."
REFERENCES,0.8941176470588236,"Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening
the black box: Automated decisions and the gdpr. Harv. JL & Tech., 31:841, 2017."
REFERENCES,0.9,"Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. 2011."
REFERENCES,0.9058823529411765,"Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples: Attacks and defenses for
deep learning. IEEE transactions on neural networks and learning systems, 30(9):2805–2824,
2019."
APPENDIX,0.9117647058823529,"6
APPENDIX"
IMPLEMENTATION DETAILS,0.9176470588235294,"6.1
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.9235294117647059,"RAT-SPNs (Peharz et al., 2020) is a simple approach to learn an SPN in a deep learning fashion:
constructing a random SPN structure and learn the parameters via gradient-based techniques. To
construct the random SPN structure, Peharz et al. (2020) use the notion of a region graph as an
abstract representation of the network structure. Given a set of random variables (RVs) X, a region R
is deﬁned as any non-empty subset of X. Given any region R, a K-partition P of R is a collection of
K non-overlapping sub-regions R1, . . . , RK, whose union is again R, i.e. P = {R1, ..., RK}, ∀k :
Rk ̸= ∅, ∀k ̸= l : Rk ∩Rl = ∅, ∪kRk = R. In practice, only 2-partitions are considered. To
construct random regions graphs, we randomly divide the root region into two sub-regions of equal
size (possibly breaking ties) and proceed recursively until depth D, resulting in an SPN of depth
2D. This recursive splitting mechanism is repeated R times. Therefore, the size of RAT-SPNs
are controlled by the following structural parameters: split-depth D, number of split repetitions R,
number of sum nodes in regions S, number of input distributions per leaf region I."
IMPLEMENTATION DETAILS,0.9294117647058824,"In our experiments, we trained a RAT-SPN for each dataset and we set D to 1. This RAT-SPN
has dual use: It is both a density estimator S(X) and a classiﬁer S(Y |X).
The rest tuning-
parameters are determined via cross-validation. We cross-validated R ∈{19, 29, 40}, S ∈{2, 10},
I ∈{20, 25, 33}."
IMPLEMENTATION DETAILS,0.9352941176470588,"We divided each dataset into 70%-30% train and test sets where train set was used to train the
SPNs and optimize tuning-parameters via cross-validation. Test set was used to test counterfactual
examples. The baseline Wachter et al. (2017) and CEM both used gradient-based optimization
combined with Adam optimizer. Early stopping is implemented to stop early when all the query
examples are perturbed to the counterfactual class. For FACE, we used only the ﬁrst 1k training
examples for searching in order to maintain a reasonable computation time."
IMPLEMENTATION DETAILS,0.9411764705882353,"MNIST Dataset: We scaled all features to the range between 0 and 1. Since only digit 1, 3, 4, 7 and
8 are used in the experiment, we used only those images to train an SPN, i.e. the SPN has 5 classes.
Test accuracy of this SPN on the class prediction task is 98%. The reported results are based on the
following hyperparameters: R = 19, S = 10, I = 20. We also cross-validated hyperparameters for
Adam optimizer used in Wachter et al. (2017) and CEM: learning rate ∈{0.5, 0.05}. Final results
use learning rate=0.5, epochs=5000. For CEM, we set β = 1 and cross-validated c ∈{10, 100} and
γ ∈{0.1, 1}. The reported results use c = 100 and γ = 0.1. For FACE, we used mode = ’KNN’
where k = 5. For our approach, we set ϵ2 = 1 and cross-validated ϵ1 ∈{1, 10}. The reported
results use ϵ1 = 10."
IMPLEMENTATION DETAILS,0.9470588235294117,"CUB Dataset: We scaled all images to 224 × 224 × 3 so it can be used for VGG-16. We report
results with the following hyperparameters. The last convolutional layer of VGG-16 has dimension
7 × 7 × 512 and we use the ﬁrst 100 feature maps for a speedup. That is, the SPN takes input of
7 × 7 × 100 features. We did the same cross-validation as for MNIST and report the results for the
following hyperparameters: For RAT-SPN, R = 29, S = 10, I = 25. For Wachter et al., learning
rate = 0.05, epochs = 1000. For CEM, learning rate = 0.5, epochs = 7000. For FACE, we used mode
= ’KNN’ where k = 5. For our approach, ϵ1 = 10 and ϵ2 = 1."
IMPLEMENTATION DETAILS,0.9529411764705882,"Adult Dataset: We standardized features by removing the mean and scaling to unit variance and
transform categorical features by using one-hot-encoding. We report results with the following"
IMPLEMENTATION DETAILS,0.9588235294117647,Under review as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.9647058823529412,"X, y
X, y
log (X|y0)
log (X|y1)
log (X)"
IMPLEMENTATION DETAILS,0.9705882352941176,"Figure 5: Individual illustration of the two gradients used in our approach on two commonly used 2D
datasets. From left to right, the ﬁgures are: the training data for classiﬁcation task, SPN’s prediction
on this set, the contour lines of SPN’s decision boundary, i.e. log S(X|y0) −log S(X|y1), and its
gradient, the contour lines of SPN’s density on X. The orange data points are class y0 and the blue
data points are y1. Note that each ﬁgure on a row are plotted under the same scale, and the gradient
in separate ﬁgures do not have one-to-one correspondence."
IMPLEMENTATION DETAILS,0.9764705882352941,"hyperparameters: For RAT-SPN, R = 19, S = 10, I = 20. This RAT-SPN gives a test accuracy of
74%. For Wachter et al., learning rate = 0.05, epochs = 1000. For CEM, learning rate = 0.5, epochs
= 7000. For FACE, we used mode = ’KNN’ where k = 5. For our approach, ϵ1 = 10 and ϵ2 = 1."
IMPLEMENTATION DETAILS,0.9823529411764705,"German Credit Dataset: We transformed features by scaling each feature to the range between 0
and 1 and transform categorical features by using one-hot-encoding. We report results with the fol-
lowing hyperparameters. We selected the following features to use: ’checking status’, ’history’,
’purpose’, ’savings’, ’employ’, ’status’, ’others’, ’property’, ’other plans’, ’housing’, ’foreign’,
’age’, ’amount’, ’duration’. For RAT-SPN, R = 40, S = 10, I = 33. This RAT-SPN gives a
test accuracy of 69%. For Wachter et al., learning rate = 0.05, epochs = 1000. For CEM, learning
rate = 0.5, epochs = 1000. For FACE, we used mode = ’KNN’ where k = 5. For our approach,
ϵ1 = 10 and ϵ2 = 1."
IMPLEMENTATION DETAILS,0.9882352941176471,"6.2
2D INTUITION"
IMPLEMENTATION DETAILS,0.9941176470588236,"In ﬁgure 5, we plot the two gradients used in our approach on two commonly used 2D datasets to
given an intuition. We trained a RAT-SPN on each dataset, one can see in the third column that the
decision boundary of RAT-SPN is given by log S(X|y0)−log S(X|y1). Therefore, taking a gradient
of log S(X|y0) −log S(X|y1) w.r.t the input induces a direction for crossing the decision boundary
under ﬁrst-order approximation around the query example. The gradient vectors are visualized by
arrows, and the gradient in the third column are computed on randomly chosen test examples of
class y1 (from the blue cluster in the training set). In case the dataset is not dense in the input space,
like the second dataset, the ﬁrst gradient step may very likely extrapolate to a low-density region.
Fortunately, the second gradient step of S(X) serves as a ﬁrst-order approximation of the local
density and takes an example in low-density region to higher-density region. See the last column,
the gradient vectors are computed on randomly chosen out-of-distribution examples."
