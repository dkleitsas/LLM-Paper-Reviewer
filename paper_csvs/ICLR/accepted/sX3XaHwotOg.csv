Section,Section Appearance Order,Paragraph
UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,0.0,"1University of Illinois at Urbana-Champaign
2Microsoft
1{yumeng5,hanj}@illinois.edu
2{chenyan.xiong,payal.bajaj,
satiwary,paul.n.bennett,xiaso}@microsoft.com"
ABSTRACT,0.003105590062111801,ABSTRACT
ABSTRACT,0.006211180124223602,"We present a new framework AMOS that pretrains text encoders with an Adversar-
ial learning curriculum via a Mixture Of Signals from multiple auxiliary generators.
Following ELECTRA-style pretraining, the main encoder is trained as a discrim-
inator to detect replaced tokens generated by auxiliary masked language models
(MLMs). Different from ELECTRA which trains one MLM as the generator, we
jointly train multiple MLMs of different sizes to provide training signals at various
levels of difﬁculty. To push the discriminator to learn better with challenging
replaced tokens, we learn mixture weights over the auxiliary MLMs’ outputs to
maximize the discriminator loss by backpropagating the gradient from the discrim-
inator via Gumbel-Softmax. For better pretraining efﬁciency, we propose a way to
assemble multiple MLMs into one uniﬁed auxiliary model. AMOS outperforms
ELECTRA and recent state-of-the-art pretrained models by about 1 point on the
GLUE benchmark for BERT base-sized models."
INTRODUCTION,0.009316770186335404,"1
INTRODUCTION"
INTRODUCTION,0.012422360248447204,"Instead of pretraining encoders on texts with random noise (e.g., randomly masked tokens in
BERT (Devlin et al., 2019)), ELECTRA-style frameworks employ an auxiliary model to corrupt
the original text sequence using its language modeling outputs, and pretrain the main Transformer
to detect (Clark et al., 2020) or correct (Meng et al., 2021) the replaced tokens. These new self-
supervised frameworks signiﬁcantly improve the efﬁciency and effectiveness of pretraining and lead
to strong results in various downstream tasks with ﬁne-tuning (Clark et al., 2020), prompt-based
learning (Meng et al., 2021), and zero-shot cross-lingual representations (Chi et al., 2021)."
INTRODUCTION,0.015527950310559006,"Recent studies revealed that the key to the ELECTRA’s success is its new learning dynamics (Xu
et al., 2020; Meng et al., 2021). By pretraining the auxiliary model jointly with the main Transformer,
an implicit learning curriculum is formed: The noise produced by the auxiliary generator becomes
more and more plausible during pretraining, posing greater challenges for the discriminator, which
has to overcome the difﬁculty by reasoning more deeply using the contexts. This leads to signiﬁcantly
improved sample efﬁciency and effectiveness of ELECTRA-style pretrained models (Clark et al.,
2020; Chi et al., 2021; Meng et al., 2021)."
INTRODUCTION,0.018633540372670808,"On the other hand, such a training dynamic also introduced new challenges in search of the optimal
pretraining setting. First, the conﬁgurations of the auxiliary generator—its depth, width, and masking
fraction—require costly trial-and-error pretraining runs. At the same time, they also signiﬁcantly
impact the discriminator’s downstream task performance: A weak auxiliary model does not generate
hard enough pretraining signal to push the discriminator, but a too strong one can confuse the
discriminator and worsen its downstream task performance (Clark et al., 2020; Meng et al., 2021).
Second, the side-by-side training of the two models forms a pseudo “GAN-style” (Goodfellow et al.,
2014) curriculum which causes difﬁculty to improve or scale: Previous attempts to make the generator
and discriminator learning more interactive (e.g., training the generator to maximize the discriminator
loss as in actual GAN frameworks) resulted in downgraded performance (Clark et al., 2020)."
INTRODUCTION,0.021739130434782608,∗Part of this work was done while Yu was interning at Microsoft.
INTRODUCTION,0.024844720496894408,Published as a conference paper at ICLR 2022
INTRODUCTION,0.027950310559006212,"In this paper, we present a new approach that learns to automatically select pretraining signals
and constructs the learning curriculum of ELECTRA-style frameworks. Our approach, AMOS,
“Adversarial Mixture Of Signals"" for text encoder pretraining, samples a diverse set of pretraining
signals from different layers of an auxiliary generator. The sampling is conducted with Gumbel-
Softmax gradient estimation of the (reversed) gradient backpropagated from the main discriminator,
which allows adversarial learning of mixture weights over the auxiliary generator’s signals. In this
way, AMOS constructs a more diverse set of pretraining signals for the main discriminator training
and enables an automatic selection of the signals to form a more effective learning curriculum.
In addition, by sampling from different layers of one generator model, AMOS ensures minimum
computation overhead."
INTRODUCTION,0.031055900621118012,"Our experiments on the GLUE and SQuAD benchmark demonstrate the effectiveness of AMOS.
In the standard BERTBase and RoBERTaBase++ pretraining settings (Devlin et al., 2019), AMOS
shows robust advantage across all included language representation tasks. The improvements are
signiﬁcant as it advances the state-of-the-arts by about 1 absolute point on average GLUE score and
has competitive SQuAD accuracy. Our thorough ablation studies conﬁrm that such effectiveness
stems from the diverse training signals from the mixture of generators and the adversarial learning
approach to combine them.1"
INTRODUCTION,0.034161490683229816,"In the rest of this paper, we ﬁrst recap related work (Section 2) and then propose our methods
(Section 3). After that we describe our experimental settings (Section 4) and evaluation results
(Section 5). The last section concludes and discusses potential future research directions."
RELATED WORK,0.037267080745341616,"2
RELATED WORK"
RELATED WORK,0.040372670807453416,"Besides the standard auto-regressive language modeling (Radford et al., 2019) and masked language
modeling (MLM) (Devlin et al., 2019), many have explored different designs of the pretraining task,
for example, preﬁx language modeling (Raffel et al., 2019), permutational language modeling (Yang
et al., 2019), and uniﬁed language modeling (Dong et al., 2019). The advantage of manually
constructed pretraining tasks is more often observed in an application-speciﬁc manner, where prior
knowledge about the target scenario is introduced (Guu et al., 2020; Roberts et al., 2020; Jia et al.,
2021; Lu et al., 2021). To pretrain general purpose language representations, the standard MLM task
often offers the best combination of simplicity, robustness, and generalization ability among those
manually constructed pretraining signals (Raffel et al., 2019; Lewis et al., 2019)."
RELATED WORK,0.043478260869565216,"Clark et al. (2020) developed a novel framework ELECTRA to pretrain Transformers with model-
generated signals. They employed an auxiliary MLM model, the “generator”, to replace the masked
positions in the input sequence with tokens sampled from its language model head. The main
Transformer, the “discriminator”, is pretrained to detect the replaced tokens. In this way, the main
model is trained to the denoise the more challenging noises from the auxiliary language model. As a
result, ELECTRA requires much fewer pretraining data points to reach the performance of MLM
pretrained models, and offers better downstream performance when converged."
RELATED WORK,0.046583850931677016,"The unique effectiveness of ELECTRA intrigued many studies to understand its real source of
advantage. Clark et al. (2020) originally noted the sample efﬁciency comes from ELECTRA’s
replaced token detection task being able to leverage all token positions for training. Later, Xu
et al. (2020) pointed to the more inﬂuential change of the pretraining task complexity for the main
Transformer. Recently, Meng et al. (2021) conducted a thorough ablation study on many variants
of ELECTRA-style models and revealed the beneﬁts of pretraining with more challenge signals
generated by the auxiliary model, as well as the implicit learning curriculum by pretraining two
models side-by-side."
RELATED WORK,0.049689440993788817,"One popular line of research to improve ELECTRA-style models is to upgrade the replaced token
detection task to more semantically informative ones. Xu et al. (2020) proposed the multi-word
choice task which pretrains the main model to pick the original token from a small candidate set.
COCO-LM developed a corrective language modeling task which pretrains the main model to recover
the original tokens (Meng et al., 2021)."
RELATED WORK,0.052795031055900624,1Code and pretrained models can be found at https://github.com/microsoft/AMOS.
RELATED WORK,0.055900621118012424,Published as a conference paper at ICLR 2022
RELATED WORK,0.059006211180124224,"Another line of research is to improve the training signal construction on the auxiliary side. In
ELECTRA, Clark et al. (2020) experimented with adversarially training the auxiliary model using
feedback from the discriminator model via reinforcement learning, but observed worse downstream
performance. Hao et al. (2021) kept the auxiliary model unchanged and augmented the replaced
token sampling with predicted signal difﬁculty and smoothed probability via Focal Loss (Lin et al.,
2017). In this work, we continue this line of research and focus on how to more automatically and
effectively construct the learning signals for ELECTRA-style models."
METHOD,0.062111801242236024,"3
METHOD"
METHOD,0.06521739130434782,"In this section, we ﬁrst recap the preliminary of ELECTRA-style pretraining, the explorations of its
learning curriculum, and then present our AMOS framework."
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.06832298136645963,"3.1
PRELIMINARY ON ELECTRA-STYLE PRETRAINING"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.07142857142857142,"In ELECTRA-style pretraining (Clark et al., 2020), two Transformer models are trained side-by-side:
One is trained via standard masked language modeling (MLM) and is used to generate replaced
tokens to corrupt the original sequences; the other is trained to denoise the corruptions from the ﬁrst
model (e.g., via replaced token detection (RTD) (Clark et al., 2020), multi-word choice (Xu et al.,
2020), or corrective language modeling (Meng et al., 2021)). The second model is the main model to
use in downstream tasks (the “discriminator”). The ﬁrst one is the auxiliary, often referred to as the
training data “generator”. In this work we built upon the original ELECTRA setup where the main
model is trained with RTD."
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.07453416149068323,"Generator Training.
Given an original sequence Xorig = [xorig
1 , . . . , xorig
i
, . . . , xorig
n ], 15% of its
tokens are randomly replaced by [MASK] symbols, and the resulting masked sequence Xmask =
[xorig
1 , . . . , [MASK]i, . . . , xorig
n ] is fed to the generator which is trained via the following loss to
predict the original tokens from the vocabulary V at the set of masked positions M:"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.07763975155279502,"pMLM(xt|hi) =
exp(x⊤
t hi)
P|V |
t′=1 exp(x⊤
t′ hi)
;
LGEN = E  −
X"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.08074534161490683,"i∈M
log pMLM

xorig
i
hi
! ,"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.08385093167701864,"where {hi}n
i=1 are the contextualized representations generated by the Transformer (after the projec-
tion head) and {xt}|V |
t=1 are the token embeddings."
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.08695652173913043,"Discriminator Training.
A replaced sequence Xrep is constructed by sampling from the generator’s
MLM probability distribution:"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.09006211180124224,"xrep
i
∼pMLM (x|hi) , if i ∈M ;
xrep
i
= xorig
i
, else."
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.09316770186335403,"The discriminator takes Xrep as input and is trained to distinguish replaced tokens produced by the
generator against the original tokens via the binary classiﬁcation loss:"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.09627329192546584,"LDISC = E  −
X"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.09937888198757763,"xrep
i =xorig
i"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.10248447204968944,"log pRTD

xrep
i
= xorig
i
hi

−
X"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.10559006211180125,"xrep
i ̸=xorig
i"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.10869565217391304,"log

1 −pRTD

xrep
i
= xorig
i
hi

 , (1)"
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.11180124223602485,"where pRTD(xrep
i
= xorig
i
hi) = exp(w⊤hi)/(1 + exp(w⊤hi)) and w is a learnable weight vector."
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.11490683229813664,"By training the generator side-by-side with the discriminator, this ELECTRA-Style framework
constructs a natural learning curriculum for the main model. At the beginning, the generator is weak
and the replaced tokens are nearly random, making the pretraining task of the main model easy
for a head start. As the generator learns better, the corruptions from it become more challenging,
which increase the difﬁculty of the learning task for the main model. After a series of studies,
recent research conﬁrmed that this curriculum learning is the key to ELECTRA-style framework’s
effectiveness (Clark et al., 2020; Xu et al., 2020; Meng et al., 2021). Pretraining with randomly
replaced tokens or starting with a converged generator’s harder signals both signiﬁcantly worsen the
discriminator effectiveness (Meng et al., 2021)."
PRELIMINARY ON ELECTRA-STYLE PRETRAINING,0.11801242236024845,Published as a conference paper at ICLR 2022
IN SEARCH OF THE OPTIMAL CURRICULUM IN ELECTRA,0.12111801242236025,"3.2
IN SEARCH OF THE OPTIMAL CURRICULUM IN ELECTRA"
IN SEARCH OF THE OPTIMAL CURRICULUM IN ELECTRA,0.12422360248447205,"In general, conﬁguring a learning curriculum for neural model training is tricky (Bengio et al., 2009;
Hacohen & Weinshall, 2019). Many moving pieces are involved—how to measure the difﬁculty
of training signals; when and how to change the training signal difﬁculty—all require manual trial-
and-error and careful designs (Graves et al., 2017). The same challenge persists in ELECTRA-style
pretraining models. More speciﬁcally, it is unclear how to choose the size of the generator network
and how to control the training dynamics of the generator and discriminator."
IN SEARCH OF THE OPTIMAL CURRICULUM IN ELECTRA,0.12732919254658384,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Discriminator Loss 0.0 0.5 1.0 1.5"
IN SEARCH OF THE OPTIMAL CURRICULUM IN ELECTRA,0.13043478260869565,Estimated Density
-LAYER GEN,0.13354037267080746,"4-Layer Gen
6-Layer Gen
8-Layer Gen"
-LAYER GEN,0.13664596273291926,(a) Disc. Loss Distribution.
-LAYER GEN,0.13975155279503104,"0
2
4
6
8
10
12
Training Steps (×1e4) 2 4 6 8"
-LAYER GEN,0.14285714285714285,Generator Loss
-LAYER GEN,0.14596273291925466,"4-Layer Gen
6-Layer Gen
8-Layer Gen"
-LAYER GEN,0.14906832298136646,(b) Gen. Loss.
-LAYER GEN,0.15217391304347827,"Figure 1: (a) After pretraining: Distribution of discrimina-
tor loss on replaced tokens by 4/6/8-layer generators. His-
tograms/Curves are distribution bins/kernel density estimates.
(b) During pretraining: 4/6/8-layer generator loss."
-LAYER GEN,0.15527950310559005,"Generator Size. There are many fac-
tors on the auxiliary side that affect
the difﬁculty of the generated signals:
Network depth, width, MLM mask
fraction, and whether to use dropout
when sampling the replaced tokens.
All of these have notable impact on
the performance of the pretrained dis-
criminator model. The general con-
sensus is that the generator should be
smaller than the discriminator but all
the rest vary setting by setting."
-LAYER GEN,0.15838509316770186,"The original ELECTRA framework
kept the generator’s Transformer layer
depth the same with the discrimina-
tor model, enabled dropout in both its
training and sampling, and mainly explored using a smaller width (e.g., with 1/3 hidden dimensions
of the main model). They also empirically found that large-sized discriminators prefer different mask
fractions on the generator side."
-LAYER GEN,0.16149068322981366,"Later, COCO-LM (Meng et al., 2021) empirically found that it is better to use a shallower generator
but of the same width as the main model, while also disabling dropout when sampling replaced
tokens. The depth of the generator is still chosen empirically and the optimal settings vary with the
conﬁgurations of the discriminator model."
-LAYER GEN,0.16459627329192547,"To illustrate the difﬁculty of training signals by generators with different depths, we train BERTBase-
sized discriminators with 4/6/8-layer generators, and show their RTD loss on replaced tokens by
these different-sized generators in Figure 1a. The average discriminator loss grows with generator
depth, indicating the increased signal difﬁculty, and the three distributions overlap with each other.
The current practice is to enumerate different generators and choose the one leading to the best
downstream task performance of the discriminator, which is tedious and unsustainable."
-LAYER GEN,0.16770186335403728,"Generator Training Dynamics. As discussed earlier, ELECTRA constructs an implicit learning cur-
riculum with the training process of the generator. To illustrate this implicit curriculum, in Figure 1b
we plot the MLM loss of the generators during pretraining. The convergence of the MLM models
follows a logarithmic schedule: The loss drops sharply in the ﬁrst 10% of pretraining and decreases
slowly afterwards. It is unclear whether such a training process of the generator automatically leads to
the optimal curriculum for discriminator training. Unfortunately, it is challenging to manually explore
different generator training dynamics in ELECTRA-style frameworks as it is trained side-by-side
with the discriminator."
-LAYER GEN,0.17080745341614906,"One idea to improve ELECTRA-style frameworks is to connect the training of the generator with
feedbacks from the discriminator (e.g., as in GAN-style models (Goodfellow et al., 2014)) so that
the generator can be adapted based on the latest discriminator’s state throughout pretraining. Clark
et al. (2020) experimented with training the generators to produce more difﬁcult signals for the
discriminator, using policy gradients from the latter. However, this adversarial setup makes the
training unstable and yields worse results, similar to the instability of GAN-style training in other
language modeling tasks (Caccia et al., 2020). Hao et al. (2021) introduced a training signal difﬁculty
estimator to adjust the sampling of replaced tokens in MLM for more challenging training signals. It
achieved better results than the original ELECTRA but still kept the generator training intact."
-LAYER GEN,0.17391304347826086,Published as a conference paper at ICLR 2022
-LAYER GEN,0.17701863354037267,Generator
-LAYER GEN,0.18012422360248448,"· · ·
· · ·"
-LAYER GEN,0.18322981366459629,"[MASK]i
[MASK]i"
-LAYER GEN,0.18633540372670807,SgNBEJz1GeMrKp68DIrgQZbdGB/HgBePEYwRkhBmJ71xcHZ2mekVwxIQL36HFw9K8Op3ePNrdPIQVLSgoajqnp6uIJHCoOe9OxOTU9Mzs7m5/PzC4tJyYWX13MSp5lDlsYz1RcAMSKGgigIlXCQaWBRIqAVXxwO/dg3aiFidYTeBZsQ6SoSCM7RSq7DeQLjB4TtZR0O3l920VK9V2PLc/eKBX/ToiHilMfH3qO96Q2yV5+7b/Y+73Uqr8NZoxzyNQCGXzJi67yXYzJhGwSX08o3UQML4FetA3VLFIjDNbLi2R7et0qZhrG0pEP1+0TGImO6UWA7I4aX5rc3EP/y6imGR81MqCRFUHy0KEwlxZgOsqBtoYGj7FrCuBb2r5RfMs042sTyNoSvS+n/5Lzo+gdu6dSm4ZERcmSDbJId4pNDUiYnpEKqhJOMPJAn8uzcOo9O3kZtU45k18gPO6yfckpe</latexit>xn
-LAYER GEN,0.18944099378881987,SgNBEJz1GeMrKp68DIrgQZbdGB/HgBePEYwRkhBmJ71xcHZ2mekVwxIQL36HFw9K8Op3ePNrdPIQVLSgoajqnp6uIJHCoOe9OxOTU9Mzs7m5/PzC4tJyYWX13MSp5lDlsYz1RcAMSKGgigIlXCQaWBRIqAVXxwO/dg3aiFidYTeBZsQ6SoSCM7RSq7DeQLjB4TtZR0O3l920VK9V2PLc/eKBX/ToiHilMfH3qO96Q2yV5+7b/Y+73Uqr8NZoxzyNQCGXzJi67yXYzJhGwSX08o3UQML4FetA3VLFIjDNbLi2R7et0qZhrG0pEP1+0TGImO6UWA7I4aX5rc3EP/y6imGR81MqCRFUHy0KEwlxZgOsqBtoYGj7FrCuBb2r5RfMs042sTyNoSvS+n/5Lzo+gdu6dSm4ZERcmSDbJId4pNDUiYnpEKqhJOMPJAn8uzcOo9O3kZtU45k18gPO6yfckpe</latexit>xn
-LAYER GEN,0.19254658385093168,SgNBEO1xTeIWFU9eGoPgQcJMjNFjwIvHCGaBJISeTiVp0rPQXSMJQ0C8+B1ePCjBq9/hza/RziKo6IOCx3tVXV3PDaXQaNv1sLi0vLKaiKZWlvf2NxKb+9UdBApDmUeyEDVXKZBCh/KFBCLVTAPFdC1e1fTPzqDSgtAv8ahyE0Pdb1RUdwhkZqpfcaCAOcvhN3FQxH8aDljFrpjJ09zRWcnE1nxM7PiXNCnaw9RaYvG+P+6OS630W6Md8MgDH7lkWtcdO8RmzBQKLmGUakQaQsb7rAt1Q3mgW7G07UjemiUNu0EypSPdKp+n4iZp/XQc02nx7Cnf3sT8S+vHmHnvBkLP4wQfD5b1IkxYBOsqBtoYCjHBrCuBLmr5T3mGIcTWIpE8LXpfR/UslnUI2f2XSsMkMCbJPDsgRcgZKZJLUiJlwklMHsgTebZurUdrbL3MWhes+cwu+QHr9RN/4Zoh</latexit>x1
-LAYER GEN,0.1956521739130435,SgNBEO1xTeIWFU9eGoPgQcJMjNFjwIvHCGaBJISeTiVp0rPQXSMJQ0C8+B1ePCjBq9/hza/RziKo6IOCx3tVXV3PDaXQaNv1sLi0vLKaiKZWlvf2NxKb+9UdBApDmUeyEDVXKZBCh/KFBCLVTAPFdC1e1fTPzqDSgtAv8ahyE0Pdb1RUdwhkZqpfcaCAOcvhN3FQxH8aDljFrpjJ09zRWcnE1nxM7PiXNCnaw9RaYvG+P+6OS630W6Md8MgDH7lkWtcdO8RmzBQKLmGUakQaQsb7rAt1Q3mgW7G07UjemiUNu0EypSPdKp+n4iZp/XQc02nx7Cnf3sT8S+vHmHnvBkLP4wQfD5b1IkxYBOsqBtoYCjHBrCuBLmr5T3mGIcTWIpE8LXpfR/UslnUI2f2XSsMkMCbJPDsgRcgZKZJLUiJlwklMHsgTebZurUdrbL3MWhes+cwu+QHr9RN/4Zoh</latexit>x1 · · ·
-LAYER GEN,0.19875776397515527,"8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NKPzM8pWxMh7xnaUJjbvx8fu2UnFklJHSthIkc/X3RE5jYyZxYDtjiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WvcFHGU4QRO4Rw8uIG3ETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NKPzM8pWxMh7xnaUJjbvx8fu2UnFklJHSthIkc/X3RE5jYyZxYDtjiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WvcFHGU4QRO4Rw8uIG3ETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NKPzM8pWxMh7xnaUJjbvx8fu2UnFklJHSthIkc/X3RE5jYyZxYDtjiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WvcFHGU4QRO4Rw8uIG3ETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NKPzM8pWxMh7xnaUJjbvx8fu2UnFklJHSthIkc/X3RE5jYyZxYDtjiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WvcFHGU4QRO4Rw8uIG3ETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>· · · x0
x0"
-LAYER GEN,0.20186335403726707,SgNBEJz1GeMrKp68DIrgQZbdGB/HgBePEYwRkhBmJ71xcHZ2mekVwxIQL36HFw9K8Op3ePNrdPIQVLSgoajqnp6uIJHCoOe9OxOTU9Mzs7m5/PzC4tJyYWX13MSp5lDlsYz1RcAMSKGgigIlXCQaWBRIqAVXxwO/dg3aiFidYTeBZsQ6SoSCM7RSq7DeQLjB4TtZR0O3l920VK9V2PLc/eKBX/ToiHilMfH3qO96Q2yV5+7b/Y+73Uqr8NZoxzyNQCGXzJi67yXYzJhGwSX08o3UQML4FetA3VLFIjDNbLi2R7et0qZhrG0pEP1+0TGImO6UWA7I4aX5rc3EP/y6imGR81MqCRFUHy0KEwlxZgOsqBtoYGj7FrCuBb2r5RfMs042sTyNoSvS+n/5Lzo+gdu6dSm4ZERcmSDbJId4pNDUiYnpEKqhJOMPJAn8uzcOo9O3kZtU45k18gPO6yfckpe</latexit>xn
-LAYER GEN,0.20496894409937888,SgNBEJz1GeMrKp68DIrgQZbdGB/HgBePEYwRkhBmJ71xcHZ2mekVwxIQL36HFw9K8Op3ePNrdPIQVLSgoajqnp6uIJHCoOe9OxOTU9Mzs7m5/PzC4tJyYWX13MSp5lDlsYz1RcAMSKGgigIlXCQaWBRIqAVXxwO/dg3aiFidYTeBZsQ6SoSCM7RSq7DeQLjB4TtZR0O3l920VK9V2PLc/eKBX/ToiHilMfH3qO96Q2yV5+7b/Y+73Uqr8NZoxzyNQCGXzJi67yXYzJhGwSX08o3UQML4FetA3VLFIjDNbLi2R7et0qZhrG0pEP1+0TGImO6UWA7I4aX5rc3EP/y6imGR81MqCRFUHy0KEwlxZgOsqBtoYGj7FrCuBb2r5RfMs042sTyNoSvS+n/5Lzo+gdu6dSm4ZERcmSDbJId4pNDUiYnpEKqhJOMPJAn8uzcOo9O3kZtU45k18gPO6yfckpe</latexit>xn
-LAYER GEN,0.2080745341614907,SgNBEO1xTeIWFU9eGoPgQcJMjNFjwIvHCGaBJISeTiVp0rPQXSMJQ0C8+B1ePCjBq9/hza/RziKo6IOCx3tVXV3PDaXQaNv1sLi0vLKaiKZWlvf2NxKb+9UdBApDmUeyEDVXKZBCh/KFBCLVTAPFdC1e1fTPzqDSgtAv8ahyE0Pdb1RUdwhkZqpfcaCAOcvhN3FQxH8aDljFrpjJ09zRWcnE1nxM7PiXNCnaw9RaYvG+P+6OS630W6Md8MgDH7lkWtcdO8RmzBQKLmGUakQaQsb7rAt1Q3mgW7G07UjemiUNu0EypSPdKp+n4iZp/XQc02nx7Cnf3sT8S+vHmHnvBkLP4wQfD5b1IkxYBOsqBtoYCjHBrCuBLmr5T3mGIcTWIpE8LXpfR/UslnUI2f2XSsMkMCbJPDsgRcgZKZJLUiJlwklMHsgTebZurUdrbL3MWhes+cwu+QHr9RN/4Zoh</latexit>x1
-LAYER GEN,0.2111801242236025,SgNBEO1xTeIWFU9eGoPgQcJMjNFjwIvHCGaBJISeTiVp0rPQXSMJQ0C8+B1ePCjBq9/hza/RziKo6IOCx3tVXV3PDaXQaNv1sLi0vLKaiKZWlvf2NxKb+9UdBApDmUeyEDVXKZBCh/KFBCLVTAPFdC1e1fTPzqDSgtAv8ahyE0Pdb1RUdwhkZqpfcaCAOcvhN3FQxH8aDljFrpjJ09zRWcnE1nxM7PiXNCnaw9RaYvG+P+6OS630W6Md8MgDH7lkWtcdO8RmzBQKLmGUakQaQsb7rAt1Q3mgW7G07UjemiUNu0EypSPdKp+n4iZp/XQc02nx7Cnf3sT8S+vHmHnvBkLP4wQfD5b1IkxYBOsqBtoYCjHBrCuBLmr5T3mGIcTWIpE8LXpfR/UslnUI2f2XSsMkMCbJPDsgRcgZKZJLUiJlwklMHsgTebZurUdrbL3MWhes+cwu+QHr9RN/4Zoh</latexit>x1
-LAYER GEN,0.21428571428571427,Discriminator
-LAYER GEN,0.21739130434782608,rLDISC
-LAYER GEN,0.2204968944099379,SgMxFIbP1Fut6pLN8EiuLHMSKluxIiLqvYC7RDyaSZNkwmMyQZoQx9AzcuFHrG/go7nwTFy5MLwt/SHw8f/nkHOF3OmtG1/WpmFxaXlexqbm19Y3Mrv71TV1EiCa2RiEey6WFORO0pnmtBlLikOP04YXIzyxj2VikXiTg9i6oa4J5jPCNbGuj1yOvmCXbTHQvPgTKFw/n15dvUefFU7+Y92NyJSIUmHCvVcuxYuymWmhFOh7l2omiMSYB7tGVQ4JAqNx1POkQHxukiP5LmCY3G7u+OFIdKDULPVIZY9VsNjL/y1qJ9k/dlIk40VSQyUd+wpGO0Ght1GWSEs0HBjCRzMyKSB9LTLQ5Ts4cwZldeR7qx0WnXCzd2IVKCSbKwh7swyE4cAIVuIYq1ICADw/wBM9WYD1aL9brpDRjTXt24Y+stx9g4ZD6</latexit>−1
-LAYER GEN,0.2236024844720497,SgMxFIbP1Fut6pLN8EiuLHMSKluxIiLqvYC7RDyaSZNkwmMyQZoQx9AzcuFHrG/go7nwTFy5MLwt/SHw8f/nkHOF3OmtG1/WpmFxaXlexqbm19Y3Mrv71TV1EiCa2RiEey6WFORO0pnmtBlLikOP04YXIzyxj2VikXiTg9i6oa4J5jPCNbGuj1yOvmCXbTHQvPgTKFw/n15dvUefFU7+Y92NyJSIUmHCvVcuxYuymWmhFOh7l2omiMSYB7tGVQ4JAqNx1POkQHxukiP5LmCY3G7u+OFIdKDULPVIZY9VsNjL/y1qJ9k/dlIk40VSQyUd+wpGO0Ght1GWSEs0HBjCRzMyKSB9LTLQ5Ts4cwZldeR7qx0WnXCzd2IVKCSbKwh7swyE4cAIVuIYq1ICADw/wBM9WYD1aL9brpDRjTXt24Y+stx9g4ZD6</latexit>−1
-LAYER GEN,0.2267080745341615,Gradient Reversal
-LAYER GEN,0.22981366459627328,−rLDISC
-LAYER GEN,0.2329192546583851,"Gradient Backpropagation
⇥
No Gradient Backpropagation Layer d1
d1 MLM"
-LAYER GEN,0.2360248447204969,f (d1) i · · · Layer 
-LAYER GEN,0.2391304347826087,"dbwRNrwt4foICdlKI="">AB6nicbVDJSgNBEK1JXGLcouLJS2MQPIWZIOox4MVjRLNAHEJPT03SpGehu0cIQz7BiwdFvPoj/oIHwZOfop3loIkPCh7vVFVz0sEV9q2P61cfml5ZbWwVlzf2NzaLu3sNlWcSoYNFotYtj2qUPAIG5prge1EIg09gS1vcDH2W3coFY+jGz1M0A1pL+IBZ1Qb6drvVrulsl2xJyCLxJmRci3/8f2/4X1bun91o9ZGmKkmaBKdRw70W5GpeZM4Kh4mypMKBvQHnYMjWiIys0mp47IkVF8EsTSVKTJRP09kdFQqWHomc6Q6r6a98bif14n1cG5m/EoSTVGbLoSAXRMRn/TXwukWkxNIQyc2thPWppEybdIomBGf+5UXSrFac08rJlUnDhikKcACHcAwOnENLqEODWDQg3t4hCdLWA/Ws/Uybc1Zs5k9+APr9QePLZGy</latexit>d2"
-LAYER GEN,0.2422360248447205,"dbwRNrwt4foICdlKI="">AB6nicbVDJSgNBEK1JXGLcouLJS2MQPIWZIOox4MVjRLNAHEJPT03SpGehu0cIQz7BiwdFvPoj/oIHwZOfop3loIkPCh7vVFVz0sEV9q2P61cfml5ZbWwVlzf2NzaLu3sNlWcSoYNFotYtj2qUPAIG5prge1EIg09gS1vcDH2W3coFY+jGz1M0A1pL+IBZ1Qb6drvVrulsl2xJyCLxJmRci3/8f2/4X1bun91o9ZGmKkmaBKdRw70W5GpeZM4Kh4mypMKBvQHnYMjWiIys0mp47IkVF8EsTSVKTJRP09kdFQqWHomc6Q6r6a98bif14n1cG5m/EoSTVGbLoSAXRMRn/TXwukWkxNIQyc2thPWppEybdIomBGf+5UXSrFac08rJlUnDhikKcACHcAwOnENLqEODWDQg3t4hCdLWA/Ws/Uybc1Zs5k9+APr9QePLZGy</latexit>d2 MLM · · · Layer "
-LAYER GEN,0.2453416149068323,"nHgsVHP8Ogz3aGw+s="">AB6nicbVDLSgNBEOw1PmJ8RcWTl8UgeAq7Iuox4EXwEtE8IFnC7GxvMmR2dpmZFcKST/DiQRGv/oi/4EHw5Kfo5HQxIKGoqb7i4/4Uxpx/m0FnKLS8sr+dXC2vrG5lZxe6eu4lRSrNGYx7LpE4WcCaxpjk2E4k8jk2/P7FyG/coVQsFrd6kKAXka5gIaNEG+km6Fx1iWn7IxhzxN3SkqV3Mf3294XVjvF93YQ0zRCoSknSrVcJ9FeRqRmlOw0E4VJoT2SRdbhgoSofKy8alD+9AogR3G0pTQ9lj9PZGRSKlB5JvOiOiemvVG4n9eK9XhuZcxkaQaBZ0sClNu69ge/W0HTCLVfGAIoZKZW23aI5JQbdIpmBDc2ZfnSf247J6WT65NGg5MkId9OIAjcOEMKnAJVagBhS7cwyM8Wdx6sJ6tl0nrgjWd2YU/sF5/ALURkcs=</latexit>dK"
-LAYER GEN,0.2484472049689441,"nHgsVHP8Ogz3aGw+s="">AB6nicbVDLSgNBEOw1PmJ8RcWTl8UgeAq7Iuox4EXwEtE8IFnC7GxvMmR2dpmZFcKST/DiQRGv/oi/4EHw5Kfo5HQxIKGoqb7i4/4Uxpx/m0FnKLS8sr+dXC2vrG5lZxe6eu4lRSrNGYx7LpE4WcCaxpjk2E4k8jk2/P7FyG/coVQsFrd6kKAXka5gIaNEG+km6Fx1iWn7IxhzxN3SkqV3Mf3294XVjvF93YQ0zRCoSknSrVcJ9FeRqRmlOw0E4VJoT2SRdbhgoSofKy8alD+9AogR3G0pTQ9lj9PZGRSKlB5JvOiOiemvVG4n9eK9XhuZcxkaQaBZ0sClNu69ge/W0HTCLVfGAIoZKZW23aI5JQbdIpmBDc2ZfnSf247J6WT65NGg5MkId9OIAjcOEMKnAJVagBhS7cwyM8Wdx6sJ6tl0nrgjWd2YU/sF5/ALURkcs=</latexit>dK
· · · MLM γ(d1)"
-LAYER GEN,0.2515527950310559,"i
γ(d2) i γ(dK) i"
-LAYER GEN,0.2546583850931677,"pMLM
x0
Sample"
-LAYER GEN,0.2577639751552795,"Gumbel-Softmax
Gradient Estimate
−rLDISC"
-LAYER GEN,0.2608695652173913,"Generator
(Multi-Layer MLM) ⇥
⇥ ¯hi h(d1)"
-LAYER GEN,0.2639751552795031,"i
h(d2)"
-LAYER GEN,0.2670807453416149,"i
h(dK) i"
-LAYER GEN,0.2701863354037267,f (dK)
-LAYER GEN,0.2732919254658385,"i
f (d2) i"
-LAYER GEN,0.27639751552795033,"Figure 2: Overview of AMOS. The generator has multiple layers trained with MLM to provide
training signals of various levels of difﬁculty. The mixture weights over MLM outputs are learned
to maximize the discriminator loss, by backpropagating the estimated reversed gradient from the
discriminator via Gumbel-Softmax. The discriminator is trained by the RTD task."
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.2795031055900621,"3.3
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.2826086956521739,"Intuitively, different-sized generators provide training signals at different levels of difﬁculty and also
may advocate the discriminator to capture different linguistic information. For example, as shown in
various probing studies (Tenney et al., 2019a;b), a very shallow MLM model may still make trivial
syntactic mistakes, thus the discriminator can focus on learning simple language syntax to identify
these errors, while the errors made by a deep MLM model may require the discriminator to capture
more sophisticated language semantics to detect."
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.2857142857142857,"In this work, instead of empirically searching for the best generator setup, we propose the AMOS
framework that (1) employs multiple MLM generators to construct a diverse set of pretraining signals,
and (2) adversarially learns the mixture of these generator outputs from discriminator feedback to
construct more challenging signals. These two designs enable AMOS to automatically compose an
effective curriculum. Figure 2 shows an overview of our AMOS framework."
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.2888198757763975,"Multi-Layer MLM Generator. A straightforward way to obtain a diverse set of signals for replaced
token generation is to utilize multiple generator networks of different depths. However, jointly
training K different MLM generators is expensive. Therefore, we propose to train a single generator
with multiple MLM heads at different layers to mimic the effect of using multiple MLM generators.
Speciﬁcally, suppose we aim to use K generators of depths D = {d1, d2, . . . , dK} (d1 < d2 < · · · <
dK), then we will instead train one generator of depth dK, and insert an MLM head to each layer
d ∈D. The K MLM heads will be jointly trained and the MLM head parameters are shared across
the K heads:"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.2919254658385093,"pMLM

xt
h(d)
i

=
exp

x⊤
t h(d)
i
"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.2950310559006211,"P|V |
t′=1 exp

x⊤
t′ h(d)
i
; LGEN = E  −
X i∈M X"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.2981366459627329,"d∈D
log pMLM

xorig
i
h(d)
i
! , (2)"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.30124223602484473,"where {h(d)
i
}n
i=1 are the contextualized representations generated by the Transformer at the dth layer
(after the projection head). During training, gradient backpropagation from upper layers is disabled
at each layer d ∈D so that the generator is partitioned into K blocks where each block is trained via
its own MLM objective without being disturbed by other blocks. This allows each block to act as
individual MLM generators, while also leveraging the representations generated by previous blocks."
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.30434782608695654,"Learning Adversarial Mixture Weights. It would be desirable to progressively adapt the output
signals of the multi-layer MLM generator based on the discriminator’s state throughout pretraining.
The generated replaced tokens are thus more informative for discriminator training (i.e., avoid
generating easy signals which can already be well distinguished by the discriminator). Given the
instability of GAN-style training to language modeling (Caccia et al., 2020; Clark et al., 2020), we
maintain the MLM training objective of the generator, and adjust the outputs of the generator by
learning mixture weights over the multiple MLM heads’ outputs."
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.30745341614906835,"Speciﬁcally, for each masked position i ∈M, we learn mixture weights γi over the embeddings
h(d)
i
, which is generated by each MLM head, to obtain the combined MLM embedding ¯hi. Then the"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.3105590062111801,Published as a conference paper at ICLR 2022
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.3136645962732919,ﬁnal token sampling probability distribution pMLM is computed:
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.3167701863354037,"γ(d)
i
=
exp

v⊤f (d)
i
 P"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.3198757763975155,"d′∈D exp

v⊤f (d′)
i
; ¯hi =
X"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.32298136645962733,"d∈D
γ(d)
i
h(d)
i
; pMLM
 
xt
¯hi

=
exp
 
x⊤
t ¯hi
"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.32608695652173914,"P|V |
t′=1 exp
 
x⊤
t′ ¯hi
,
(3)"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.32919254658385094,"where v is a learnable weight vector; {f (d)
i
}n
i=1 are the contextualized representations generated by
the Transformer at the dth layer (before the projection head in MLM)."
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.33229813664596275,"However, directly sampling from the above MLM distribution fails to enable the mixture weight
learning to be guided by the discriminator: The sampling operation is non-differentiable and prevents
gradient backpropagation from the discriminator to the generator. We leverage Gumbel-Softmax (Jang
et al., 2017) for a continuous approximation to the sampling operation for gradient estimation:"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.33540372670807456,"ˆpMLM
 
xt
¯hi

=
exp ((log πt + gt) /τ)
P|V |
t′=1 exp ((log πt′ + gt′) /τ)
;
∀t, πt = pMLM
 
xt
¯hi

, gt ∼Gumbel(0, 1),"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.3385093167701863,where τ is a temperature hyperparameter (a smaller τ results in a more accurate approximation).
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.3416149068322981,"Once we have the gradient estimation backpropagated from the discriminator side, we will update the
mixture weights γ to maximize the discriminator loss. This forms an adversarial learning curriculum
as the multiple MLMs’ outputs are always combined to construct challenging replaced tokens given
the latest discriminator state. Such an adversarial learning setup can be easily implemented via a
gradient reversal layer (Ganin et al., 2016) that multiplies the gradient backpropagated from the
discriminator side by −1 when updating the mixture weights."
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.3447204968944099,"Overall Training. AMOS jointly trains the multi-layer MLM generator θGEN, the mixture weight
parameter v, and the discriminator θDISC via the following losses:"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.34782608695652173,"θ∗
GEN, v∗←arg min
θGEN,v (LGEN −λLDISC) ,
θ∗
DISC ←arg min
θDISC LDISC,
(4)"
ADVERSARIAL LEARNING WITH MIXTURE OF GENERATORS,0.35093167701863354,"where λ is a hyperparameter balancing the weight of the two losses. Similar to ELECTRA, the
generator minimizes LGEN in Eqn. (2) and the discriminator minimizes LDISC in Eqn. (1). The notable
difference from ELECTRA is that AMOS additionally trains the generator’s mixture weights γ (thus
v and θGEN) to maximize the discriminator loss. Note that the gradient backpropagated from the
discriminator through the Gumbel-Softmax estimation is used to update the mixture weights, but not
the MLM embeddings (i.e., in Eqn. (3), γ(d)
i
is updated by the gradient from the discriminator side,
while h(d)
i
is not). This guarantees that the generator’s language modeling ability is still acquired
through the MLM task without being disturbed by discriminator signals."
EXPERIMENTAL SETUP,0.35403726708074534,"4
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.35714285714285715,"Pretraining Settings.
We experiment with two standard pretraining settings, base and base++:
Base is the BERTBase training conﬁguration (Devlin et al., 2019): Pretraining on Wikipedia and
BookCorpus (Zhu et al., 2015) (16 GB of texts) for 256 million samples on 512 token sequences (125K
batches with 2048 batch size). We use the same corpus and 32, 768 uncased BPE vocabulary (Sennrich
et al., 2015) as with TUPE (Ke et al., 2020) and COCO-LM (Meng et al., 2021)."
EXPERIMENTAL SETUP,0.36024844720496896,"Base++ trains the base size model with larger corpora and/or more training steps. We add in
OpenWebText (Gokaslan & Cohen, 2019), CC-News (Liu et al., 2019) and STORIES (Trinh & Le,
2018), to a total of 160 GB texts, and train for 4 billion (with 2048 batch size) samples, following
recent research Bao et al. (2020); Liu et al. (2019); Yang et al. (2019). We follow the prepossessing
of UniLMV2 (Bao et al., 2020) and use 64, 000 cased BPE vocabulary."
EXPERIMENTAL SETUP,0.36335403726708076,"Model Architecture. Our base/base++ discriminator model uses the BERTBase architecture (Devlin
et al., 2019): 12 layer Transformer, 768 hidden size, plus T5 relative position encoding (Raffel et al.,
2019). Our generator is an 8-layer Transformer with the same hidden size, and the MLM heads are
inserted at the 4th, 6th and 8th layers (to mimic having three generators). We disable dropout in the
generator following Meng et al. (2021)."
EXPERIMENTAL SETUP,0.36645962732919257,"Downstream Tasks. We use the tasks included in GLUE (Wang et al., 2018) and SQuAD 2.0 reading
comprehension (Rajpurkar et al., 2016). All models are evaluated with the same standard ﬁne-tuning"
EXPERIMENTAL SETUP,0.3695652173913043,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.37267080745341613,"Table 1: Results on GLUE and SQuAD 2.0 development set (GLUE test set results can be found in
Appendix D). All results are single-task, single-model ﬁne-tuning. We use Spearman correlation for
STS, Matthews correlation for CoLA, and accuracy for the rest on GLUE. AVG is the average of the
eight tasks on GLUE. All baseline results are reported by previous research. Results not available in
public reports are marked as “–”."
EXPERIMENTAL SETUP,0.37577639751552794,"Model
Params
GLUE DEV Single Task
SQuAD 2.0"
EXPERIMENTAL SETUP,0.37888198757763975,"MNLI
QQP
QNLI
SST-2
CoLA
RTE
MRPC
STS-B
AVG
EM
F1"
EXPERIMENTAL SETUP,0.38198757763975155,"Base Setting: BERT Base Size, Wikipedia + Book Corpus (16GB)"
EXPERIMENTAL SETUP,0.38509316770186336,"BERT (Devlin et al., 2019)
110M
84.5/-
91.3
91.7
93.2
58.9
68.6
87.3
89.5
83.1
73.7
76.3
RoBERTa (Liu et al., 2019)
110M
85.8/85.5
91.3
92.0
93.7
60.1
68.2
87.3
88.5
83.3
77.7
80.5
XLNet (Yang et al., 2019)
110M
85.8/85.4
–
–
92.7
–
–
–
–
–
78.5
81.3
DeBERTa (He et al., 2021)
134M
86.3/86.2
–
–
–
–
–
–
–
–
79.3
82.5
TUPE (Ke et al., 2020)
110M
86.2/86.2
91.3
92.2
93.3
63.6
73.6
89.9
89.2
84.9
–
–
ELECTRA (Clark et al., 2020)
110M
86.9/86.7
91.9
92.6
93.6
66.2
75.1
88.2
89.7
85.5
79.7
82.6
+HPLoss+Focal (Hao et al., 2021)
110M
87.0/86.9
92.7
91.7
92.6
66.7
90.7
81.3
91.0
86.7
83.0
85.6
MC-BERT (Xu et al., 2020)
110M
85.7/85.2
89.7
91.3
92.3
62.1
75.0
86.0
88.0
83.7
–
–
COCO-LM (Meng et al., 2021)
110M
88.5/88.3
92.0
93.1
93.2
63.9
84.8
91.4
90.3
87.2
82.4
85.2"
EXPERIMENTAL SETUP,0.38819875776397517,"AMOS
110M
88.9/88.7
92.3
93.6
94.2
70.7
86.6
90.9
91.6
88.6
84.2
87.2"
EXPERIMENTAL SETUP,0.391304347826087,"Base++ Setting: BERT Base Size, Bigger Training Data, and/or More Training Steps"
EXPERIMENTAL SETUP,0.3944099378881988,"XLNet (Yang et al., 2019)
110M
86.8/-
91.4
91.7
94.7
60.2
74.0
88.2
89.5
84.6
80.2
–
RoBERTa (Liu et al., 2019)
125M
87.6/-
91.9
92.8
94.8
63.6
78.7
90.2
91.2
86.4
80.5
83.7
UniLM V2 (Bao et al., 2020)
110M
88.5/-
91.7
93.5
95.1
65.2
81.3
91.8
91.0
87.1
83.3
86.1
DeBERTa (He et al., 2021)
134M
88.8/88.5
–
–
–
–
–
–
–
–
83.1
86.2
CLEAR (Wu et al., 2020)
110M
86.7/-
90.0
92.9
94.5
64.3
78.3
89.2
89.8
85.7
–
–
COCO-LM (Meng et al., 2021)
134M
90.2/90.0
92.2
94.2
94.6
67.3
87.4
91.2
91.8
88.6
85.4
88.1"
EXPERIMENTAL SETUP,0.39751552795031053,"AMOS
134M
90.5/90.4
92.4
94.4
95.5
71.8
86.6
91.7
92.0
89.4
85.0
87.9"
EXPERIMENTAL SETUP,0.40062111801242234,"protocols: Single task learning with vanilla ﬁne-tuning and reporting the median of ﬁve random seeds
in GLUE and SQuAD. Please refer to Appendix A for more details."
EXPERIMENTAL SETUP,0.40372670807453415,"Baselines. We compare with various pretrained models in both settings. All numbers are from
reported results in recent research (more details in Appendix J)."
EXPERIMENTAL SETUP,0.40683229813664595,"Implementation Details. Our implementation builds upon the open-source implementation of
fairseq Ott et al. (2019). Standard hyperparameters in pretraining and ﬁne-tuning are used. More
details are listed in Appendix B."
EVALUATION RESULTS,0.40993788819875776,"5
EVALUATION RESULTS"
EVALUATION RESULTS,0.41304347826086957,"We conduct three groups of experiments to evaluate the performance of AMOS, the effect of leveraging
signals from multiple generators, and the inﬂuence of its adversarial training. We also provide some
case studies on the constructed pretraining sequences in AMOS."
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4161490683229814,"5.1
RESULTS AND ABLATIONS ON GLUE AND SQUAD"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4192546583850932,"GLUE and SQuAD Results. Table 1 lists the single-task ﬁne-tuning performance of AMOS and
notable baselines that are pretrained under the standard base and base++ setting. AMOS outperforms
all previous state-of-the-art pretraining methods on the overall GLUE score and SQuAD. AMOS’s
improvements are robust, often of large margins, and achieves the new state-of-the-art on multiple
tasks under this standard pretraining/ﬁne-tuning setup."
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.422360248447205,"Table 2 presents the ablation studies of AMOS variants on MNLI and SQuAD, the two most stable
and representative downstream evaluations (Refer to Appendix E for the full results). We organize
the ablations into four subgroups to study the effectiveness of Curriculum Learning, Adversarial
Training, Multi-Layer MLM, and Training Signal Diversity in AMOS."
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4254658385093168,"Effect of Curriculum Setup. Disabling the automatic mixture weights learning in AMOS leads
to downgraded downstream performance: Randomly picking the MLM head from the same set of
generator layers (w. random layer) reduces accuracy on all metrics. It increases the diversity of
pretraining signals but does not conﬁgure then as effectively. Manually conﬁguring the order of
different sets of pretraining signals by manually switching from shallower to deeper layers (i.e., 4 to
6 to 8) during pretraining (w. layer switch) does not lead to better results than random layer selection.
Note that in these manual conﬁgurations we manually switch the signals at 1/3 and 2/3 of the total"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.42857142857142855,Published as a conference paper at ICLR 2022
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.43167701863354035,"Table 2: Ablations on MNLI/SQuAD 2.0 dev sets that
remove (-), add (+) or switch (w.) one component. Val-
ues are differences (in absolute points) from AMOSBase."
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.43478260869565216,"Group
Method
MNLI
SQuAD 2.0
(m/mm)
EM/F1"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.43788819875776397,"AMOSBase
88.9/88.7
84.2/87.2"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4409937888198758,"Curriculum
w. random layer
-0.3/-0.4
-0.6/-0.6
Setup
w. layer switch
-0.3/-0.3
-0.9/-1.0"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4440993788819876,"Adversarial
- adv. train
-0.2/-0.2
-0.3/-0.4
Setup
+ adv. MLM
-0.3/0.0
0.0/0.0"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4472049689440994,"Multi-MLM
- stop grad.
-0.5/-0.1
-0.6/-0.6
Setup
w. separate MLM gen.
-0.1/0.0
-0.8/-0.7"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4503105590062112,"Backbone
4-layer gen.
-0.5/-0.5
-1.1/-1.2
(No Multi-MLM
6-layer gen.
-0.3/-0.4
-1.1/-1.1
No Adv. Train)
8-layer gen.
-0.6/-0.6
-0.9/-0.9
12-layer gen.
-1.1/-1.2
-1.8/-1.7"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.453416149068323,"Table 3: Edge probing results using differ-
ent MLM layers from the AMOS generator.
Tasks are ordered based on suggested seman-
tic depths in Tenney et al. (2019a)."
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.45652173913043476,"Tasks
layer 4
layer 6
layer 8"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.45962732919254656,"POS
92.6
93.4
91.2
Consts.
69.7
73.2
73.0
Deps.
86.4
85.1
88.3
Entities
91.7
93.9
93.9
SRL
76.9
74.2
79.2
Coref.
77.6
75.5
77.9
SPR2
79.4
79.0
79.9
Relations
72.2
76.3
73.9"
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.46273291925465837,"pretraining steps. We have experimented switching at different steps of pretraining but do not observe
signiﬁcantly better results. Manual trials are tedious and expensive in pretraining and underperform
automatically learned mixture over multiple training signals."
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4658385093167702,"Effect of Adversarial Setup. Not performing adversarial learning (- adv. train) to learn mixture
weights (i.e., always use unweighted average signals) hurts the model performance. However,
note that this ablation still beneﬁts from the curriculum learning effect as the generator gradually
learns better. In addition, we also try to backpropagate the adversarial gradient to update the MLM
embeddings (+ adv. MLM). Speciﬁcally, in Eqn. (3), both γ(d)
i
and h(d)
i
are updated by the reversed
gradient from the discriminator. Our observation is that this makes the training unstable, perhaps
because it hinders the MLM task of the auxiliary model. We have to use a very small gradient
multiplier (e.g., 0.1) when updating the MLM embeddings with the discriminator’s backpropagated
gradient, which has minimal effects on the model."
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.468944099378882,"Effect of Multi-Layer Generator Setup. We also experimented with several conﬁgurations of the
multi-layer generators: -stop grad. does not stop the gradient at the 4th and 6th layers in the generator
of AMOS; w. separate MLM gen. trains three separate generators of 4, 6 and 8 layers jointly with the
discriminator. Both conﬁgurations result in reduced performance of the discriminator and we keep
the simpler setup as in AMOS. Additional experiments on using different numbers of MLM heads
can be found in Appendix F."
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4720496894409938,"Effect of Diverse Training Signals. The last ablation compares with using a single generator in
AMOS and without any learned mixtures. Using a 4/6/8-layer generator yields worse results than
AMOS and previous ablations with the multi-MLM generator, especially on SQuAD. The 12-layer
generator is too strong and makes the pretrained discriminator signiﬁcantly worse. It is simpler and
more effective to grant the model access to a broader set of training signals and automatically learn to
leverage them."
RESULTS AND ABLATIONS ON GLUE AND SQUAD,0.4751552795031056,"Pretraining Efﬁciency. Pretraining efﬁciency study of AMOS and comparison with previous models
can be found in Appendix C."
DIVERSE PRETRAINING SIGNALS FROM DIFFERENT GENERATORS,0.4782608695652174,"5.2
DIVERSE PRETRAINING SIGNALS FROM DIFFERENT GENERATORS"
DIVERSE PRETRAINING SIGNALS FROM DIFFERENT GENERATORS,0.4813664596273292,"To study how generators of different depths provide diverse pretraining signals, we conduct probing ex-
periments on eight NLP tasks covering different linguistic aspects, following (Tenney et al., 2019a;b):
part-of-speech (POS) (Weischedel et al., 2013), constituents (Consts.), dependencies (Deps.), entities,
semantic role labeling (SRL), coreference (Coref.), semantic proto-roles (SPR2 (Rudinger et al.,
2018)), and relation classiﬁcation (SemEval (Hendrickx et al., 2010)). The major difference between
our setting and Tenney et al. (2019a;b) is that we do not combine embeddings from multiple layers
but directly use the embedding from each MLM layer of AMOS generator as the (frozen) feature to a
trainable linear classiﬁer, as we are interested in what information is captured by each MLM layer."
DIVERSE PRETRAINING SIGNALS FROM DIFFERENT GENERATORS,0.484472049689441,"As shown in Table 3, different MLM layers in AMOS generator indeed are good at different tasks–The
6th layer in the generator has the best performance on POS, constituent labeling, entity recognition"
DIVERSE PRETRAINING SIGNALS FROM DIFFERENT GENERATORS,0.48757763975155277,Published as a conference paper at ICLR 2022
DIVERSE PRETRAINING SIGNALS FROM DIFFERENT GENERATORS,0.4906832298136646,"and relation classiﬁcation, while the 8th layer performs the best on the other tasks. This demonstrates
that using different layers in the multi-layer MLM generator is helpful for creating training signals of
different levels of difﬁculty and also emphasizing different linguistic aspects. Note that although the
4th layer has worse performance than deeper layers across all tasks, it is still useful for providing
discriminator pretraining signals, because the discriminator needs to learn from the “mistakes” made
by the generator not capturing certain language semantics. Some concrete case studies of replaced
tokens generated by different generator layers can be found in Appendix I."
EFFECTS OF ADVERSARIAL TRAINING,0.4937888198757764,"5.3
EFFECTS OF ADVERSARIAL TRAINING"
EFFECTS OF ADVERSARIAL TRAINING,0.4968944099378882,"0
2
4
6
8
10
12
Training Steps (×1e4) 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTS OF ADVERSARIAL TRAINING,0.5,Average Mixture Weight
EFFECTS OF ADVERSARIAL TRAINING,0.5031055900621118,"Layer 4
Layer 6
Layer 8"
EFFECTS OF ADVERSARIAL TRAINING,0.5062111801242236,(a) Mixture Weights.
EFFECTS OF ADVERSARIAL TRAINING,0.5093167701863354,"0
2
4
6
8
10
12
Training Steps (×1e4) 0.05 0.10 0.15 0.20 0.25 0.30"
EFFECTS OF ADVERSARIAL TRAINING,0.5124223602484472,Disc. Acc. on Replaced Tokens
-LAYER GEN,0.515527950310559,"4-Layer Gen
8-Layer Gen
Random Layer
Layer Switch
AMOS"
-LAYER GEN,0.5186335403726708,(b) Disc. Accuracy.
-LAYER GEN,0.5217391304347826,"Figure 3: (a) The average mixture weights of the 4th/6th/8th
generator layers on all masked positions during pretraining.
(b) The discriminator accuracy on replaced tokens during
pretraining under different curriculum learning setups."
-LAYER GEN,0.5248447204968945,"To better understand the adversar-
ial training dynamics, we plot the
mixture weights (averaged over all
masked positions) assigned to the
4th/6th/8th layer MLM during pre-
training in Figure 3a.
In the later
pretraining stage, the 8th layer grad-
ually gains more weights with the
weights of the other two layers de-
creasing, showing that the generator
indeed tries to create more challeng-
ing training signals for the discrimi-
nator. We also compare the training
process of AMOS with several genera-
tor ablations: (1) A 4-layer generator;
(2) An 8-layer generator; (3) A 4/6/8-multi-layer MLM generator with three layers randomly picked
for generating MLM replacements (Random Layer); (4) Switching from shallow layers to deeper lay-
ers (i.e., 4 to 6 to 8) at 1/3 and 2/3 of the pretraining steps (Layer Switch). We plot the discriminator
accuracy (averaged on replaced tokens) in Figure 3b. AMOS creates an intuitive learning curriculum
that starts simpler than random layer mixing and becomes more challenging as pretraining goes on.
Making “hard” switches suddenly changes the task difﬁculty and may disrupt training."
CONCLUSIONS AND FUTURE WORK,0.5279503105590062,"6
CONCLUSIONS AND FUTURE WORK"
CONCLUSIONS AND FUTURE WORK,0.531055900621118,"In this paper we present AMOS, a new strategy for pretraining ELECTRA-style text encoders using
an adversarial mixture of multiple training signal generators. AMOS constructs the corrupted text
sequences by attaching multiple MLM heads to a deeper generator and sampling replaced tokens
from their mixed outputs. The weights of the mixtures are learned to maximize the training signals
difﬁculty for the discriminator, by backpropagating the reversed gradient from the discriminator
through Gumbel-Softmax. This upgrades the ELECTRA-style pretraining framework with an
automatically learned curriculum that composes more diverse pretraining signals."
CONCLUSIONS AND FUTURE WORK,0.5341614906832298,"Our experiments on the GLUE and SQuAD benchmarks demonstrate the empirical advantages of
AMOS. Under the standard BERTBase and RoBERTaBase++ pretraining settings, the same Transformer
network pretrained with AMOS achieves the new state-of-the-art in nearly all evaluation metrics, with
around 1 point gains on GLUE score and SQuAD accuracy. Our studies and analyses further conﬁrm
the source of AMOS’s effectiveness: The more diverse training signals from multiple generators and
our adversarial learning design to effectively utilize them throughout pretraining."
CONCLUSIONS AND FUTURE WORK,0.5372670807453416,"Our observations can be viewed as another progress of “data-centric” AI—Effectively constructed
training signals from data can lead to signiﬁcant empirical improvements without changing the model
itself. Future work along this direction includes but not limited to: More studies to understand the
role of training signals in language model pretraining, explorations of a broader set of training signal
sources, and better strategies to leverage different information sources in pretraining."
CONCLUSIONS AND FUTURE WORK,0.5403726708074534,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5434782608695652,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.546583850931677,"We strive to facilitate the reproducibility of the reported results in this paper, by (1) using the same
pretraining and ﬁne-tuning setups with previous research and reporting the median results of multiple
ﬁne-tuning runs, (2) providing details about the datasets used in Appendix A, sources of baselines
compared in Appendix J, and hyperparameters used in Appendix B, (3) reporting the exact pretraining
hours of our base models in Appendix C, and (4) releasing our pretrained models. All experiments in
this paper are conducted on 64 A100 GPUs each with 40GB memory size."
REFERENCES,0.5496894409937888,REFERENCES
REFERENCES,0.5527950310559007,"Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao,
Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo-masked language models for
uniﬁed language model pre-training. In ICML, 2020."
REFERENCES,0.5559006211180124,"Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
ICML, 2009."
REFERENCES,0.5590062111801242,"Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth PASCAL recognizing
textual entailment challenge. In TAC, 2009."
REFERENCES,0.562111801242236,"Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin.
Language GANs falling short. In ICLR, 2020."
REFERENCES,0.5652173913043478,"Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task
1: Semantic textual similarity multilingual and crosslingual focused evaluation. In International
Workshop on Semantic Evaluation, 2017."
REFERENCES,0.5683229813664596,"Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Saksham Singhal, Payal Bajaj, Xia Song, and
Furu Wei. XLM-E: Cross-lingual language model pre-training via ELECTRA. arXiv preprint
arXiv:2106.16138, 2021."
REFERENCES,0.5714285714285714,"Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training
text encoders as discriminators rather than generators. In ICLR, 2020."
REFERENCES,0.5745341614906833,"Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges Workshop, 2005."
REFERENCES,0.577639751552795,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT, 2019."
REFERENCES,0.5807453416149069,"William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
In International Workshop on Paraphrasing (IWP), 2005."
REFERENCES,0.5838509316770186,"Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. Uniﬁed language model pre-training for natural language understanding
and generation. In NeurIPS, 2019."
REFERENCES,0.5869565217391305,"Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
JMLR, 2016."
REFERENCES,0.5900621118012422,"Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing
textual entailment challenge. In ACL-PASCAL workshop on textual entailment and paraphrasing,
2007."
REFERENCES,0.593167701863354,"Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus, 2019."
REFERENCES,0.5962732919254659,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014."
REFERENCES,0.5993788819875776,Published as a conference paper at ICLR 2022
REFERENCES,0.6024844720496895,"Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated
curriculum learning for neural networks. In ICML, 2017."
REFERENCES,0.6055900621118012,"Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-
augmented language model pre-training. In ICML, 2020."
REFERENCES,0.6086956521739131,"Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep networks.
In ICML, 2019."
REFERENCES,0.6118012422360248,"R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. The second PASCAL recognising textual entailment challenge. In PASCAL Challenges
Workshop on Recognising Textual Entailment, 2006."
REFERENCES,0.6149068322981367,"Yaru Hao, Li Dong, Hangbo Bao, Ke Xu, and Furu Wei. Learning to sample replacements for
ELECTRA pre-training. In Findings of ACL, 2021."
REFERENCES,0.6180124223602484,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. DeBERTa: Decoding-enhanced bert
with disentangled attention. In ICLR, 2021."
REFERENCES,0.6211180124223602,"Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. Semeval-2010 task 8: Multi-
way classiﬁcation of semantic relations between pairs of nominals. In International Workshop on
Semantic Evaluation, 2010."
REFERENCES,0.6242236024844721,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-Softmax. In
ICLR, 2017."
REFERENCES,0.6273291925465838,"Robin Jia, Mike Lewis, and Luke Zettlemoyer. Question answering infused pre-training of general-
purpose contextualized representations. arXiv preprint arXiv:2106.08190, 2021."
REFERENCES,0.6304347826086957,"Guolin Ke, Di He, and Tie-Yan Liu. Rethinking the positional encoding in language pre-training.
arXiv preprint arXiv:2006.15595, 2020."
REFERENCES,0.6335403726708074,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension. In ACL, 2019."
REFERENCES,0.6366459627329193,"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object
detection. In ICCV, 2017."
REFERENCES,0.639751552795031,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining
Approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.6428571428571429,"Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan
Liu, and Arnold Overwijk. Less is more: Pretrain a strong siamese encoder for dense text retrieval
using a weak decoder. In EMNLP, 2021."
REFERENCES,0.6459627329192547,"Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, and Xia Song.
COCO-LM: Correcting and contrasting text sequences for language model pretraining. In NeurIPS,
2021."
REFERENCES,0.6490683229813664,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. FAIRSEQ: A fast, extensible toolkit for sequence modeling. In NAACL-HLT
Demonstrations, 2019."
REFERENCES,0.6521739130434783,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.65527950310559,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. Journal of Machine Learning Research, 2019."
REFERENCES,0.6583850931677019,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In EMNLP, 2016."
REFERENCES,0.6614906832298136,Published as a conference paper at ICLR 2022
REFERENCES,0.6645962732919255,"Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the
parameters of a language model? In EMNLP, 2020."
REFERENCES,0.6677018633540373,"Rachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, and Benjamin Van Durme. Neural-
davidsonian semantic proto-role labeling. In EMNLP, 2018."
REFERENCES,0.6708074534161491,"Thibault Sellam, Steve Yadlowsky, Jason Wei, Naomi Saphra, Alexander D’Amour, Tal Linzen,
Jasmijn Bastings, Iulia Turc, Jacob Eisenstein, Dipanjan Das, et al. The MultiBERTs: BERT
reproductions for robustness analysis. In ICLR, 2022."
REFERENCES,0.6739130434782609,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In ACL, 2015."
REFERENCES,0.6770186335403726,"Iyer
Shankar,
Dandekar
Nikhil,
and
Csernai
Kornél.
First
Quora
dataset
release:
Question
pairs,
2017.
URL
https://www.quora.com/q/quoradata/
First-Quora-Dataset-Release-Question-Pairs."
REFERENCES,0.6801242236024845,"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In EMNLP, 2013."
REFERENCES,0.6832298136645962,"Sandeep Subramanian, Sai Rajeswar Mudumba, Alessandro Sordoni, Adam Trischler, Aaron C
Courville, and Chris Pal. Towards text generation with adversarially learned neural outlines. In
NeurIPS, 2018."
REFERENCES,0.6863354037267081,"Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In ACL,
2019a."
REFERENCES,0.6894409937888198,"Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim,
Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context?
probing for sentence structure in contextualized word representations. In ICLR, 2019b."
REFERENCES,0.6925465838509317,"Trieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. arXiv preprint
arXiv:1806.02847, 2018."
REFERENCES,0.6956521739130435,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
EMNLP Workshop BlackboxNLP, 2018."
REFERENCES,0.6987577639751553,"Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.
In TACL, 2019."
REFERENCES,0.7018633540372671,"Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. Ontonotes release 5.0
ldc2013t19. Linguistic Data Consortium, 2013."
REFERENCES,0.7049689440993789,"Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In NAACL-HLT, 2018."
REFERENCES,0.7080745341614907,"Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. CLEAR: Contrastive
learning for sentence representation. arXiv preprint arXiv:2012.15466, 2020."
REFERENCES,0.7111801242236024,"Zhenhui Xu, Linyuan Gong, Guolin Ke, Di He, Shuxin Zheng, Liwei Wang, Jiang Bian, and
Tie-Yan Liu. MC-BERT: Efﬁcient language pre-training via a meta controller. arXiv preprint
arXiv:2006.05744, 2020."
REFERENCES,0.7142857142857143,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
XLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019."
REFERENCES,0.717391304347826,"Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. SeqGAN: Sequence generative adversarial nets
with policy gradient. In AAAI, 2017."
REFERENCES,0.7204968944099379,"Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin.
Adversarial feature matching for text generation. In ICML, 2017."
REFERENCES,0.7236024844720497,Published as a conference paper at ICLR 2022
REFERENCES,0.7267080745341615,"Junbo Zhao, Yoon Kim, Kelly Zhang, Alexander Rush, and Yann LeCun. Adversarially regularized
autoencoders. In ICML, 2018."
REFERENCES,0.7298136645962733,"Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and
Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching
movies and reading books. In ICCV, 2015."
REFERENCES,0.7329192546583851,"A
DETAILS OF GLUE TASKS"
REFERENCES,0.7360248447204969,Table 4: GLUE task statistics and information.
REFERENCES,0.7391304347826086,"Size
Domain
Task
Metric(s)"
REFERENCES,0.7422360248447205,"MNLI
393K
Misc.
Inference
Accuracy
QQP
364K
Social QA
Similarity
Accuracy/F1
QNLI
108K
Wikipedia
QA/Inference
Accuracy
SST-2
67K
Movie Reviews
Sentiment
Accuracy
CoLA
8.5K
Misc.
Acceptability
Matthews corr.
RTE
2.5K
Misc.
Inference
Accuracy
MRPC
3.7K
News
Paraphrase
Accuracy/F1
STS-B
5.7K
Misc.
Similarity
Pearson/Spearman."
REFERENCES,0.7453416149068323,"Below are detailed descriptions of the tasks included in the GLUE benchmark. The statistics can be
found in Table 4."
REFERENCES,0.7484472049689441,"MNLI: Multi-genre Natural Language Inference (Williams et al., 2018) has 393K training examples
via crowdsourcing. The task is to predict whether a premise sentence entails, contradicts or is neutral
to a given hypothesis sentence."
REFERENCES,0.7515527950310559,"QQP: Question Pairs (Shankar et al., 2017) has 364K training examples from the Quora question-
answering website. The task is to determine whether a given pair of questions asked are equivalent
semantically."
REFERENCES,0.7546583850931677,"QNLI: Question Natural Language Inference has 108K training examples collected from the Stanford
Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). The task is to predict whether a
given sentence includes the answer to a given question sentence."
REFERENCES,0.7577639751552795,"SST-2: Stanford Sentiment Treebank (Socher et al., 2013) has 67K training examples obtained from
movie reviews with manually-annotated sentiment scores. The tasks is to determine if the sentence
contains positive or negative sentiment."
REFERENCES,0.7608695652173914,"CoLA: Corpus of Linguistic Acceptability (Warstadt et al., 2019) has 8.5K training examples
collected from books and journal articles on linguistic theory. The task is to determine whether a
sentence is linguistically acceptable or not."
REFERENCES,0.7639751552795031,"RTE: Recognizing Textual Entailment (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al.,
2007; Bentivogli et al., 2009) has 2.5K training examples from textual entailment challenges. The
task is to predict whether a premise sentence entails a hypothesis sentence or not."
REFERENCES,0.7670807453416149,"MRPC: Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) contains 3.7K training
examples from online news sources. The task is to predict whether two sentences are equivalent
semantically."
REFERENCES,0.7701863354037267,"STS-B: Semantic Textual Similarity (Cer et al., 2017) contains 5.8K training examples collected
from multiple sources with human annotations of sentence pair semantic similarity. The task is to
predict how semantically similar two sentences are (with a 1 to 5 scoring scale)."
REFERENCES,0.7732919254658385,"B
HYPERPARAMETER SETTINGS"
REFERENCES,0.7763975155279503,"We use the default/standard values for most hyperparameters for pretraining: The generator pretraining
uses the standard 15% masking ratio. The temperature for Gumbel-Softmax is τ = 0.3. The"
REFERENCES,0.7795031055900621,Published as a conference paper at ICLR 2022
REFERENCES,0.782608695652174,Table 5: Hyperparameters used in pretraining.
REFERENCES,0.7857142857142857,"Parameters
base
base++"
REFERENCES,0.7888198757763976,"Max Steps
125K
1.95M
Peak Learning Rate
5e-4
2e-4
Batch Size
2048
2048
Warm-up Steps
10K
10K
Sequence Length
512
512
Adam ϵ
1e-6
1e-6
Adam (β1, β2)
(0.9, 0.98)
(0.9, 0.98)
Clip Norm
2.0
2.0
Dropout
0.1
0.1"
REFERENCES,0.7919254658385093,Table 6: Hyperparameter ranges searched for ﬁne-tuning.
REFERENCES,0.7950310559006211,"Parameters
GLUE Fine-tuning
SQuAD Fine-tuning"
REFERENCES,0.7981366459627329,"Max Epochs
{2, 3, 5, 10}
{2, 3}
Peak Learning Rate
{1e-5, 2e-5, 3e-5, 4e-5, 5e-5}
{2e-5, 3e-5, 4e-5, 5e-5}
Batch Size
{16, 32}
{16, 32}
Warm-up Proportion
{6%, 10%}
{6%, 10%}
Sequence Length
512
512
Adam ϵ
1e-6
1e-6
Adam (β1, β2)
(0.9, 0.98)
(0.9, 0.98)
Clip Norm
-
-
Dropout
0.1
0.1"
REFERENCES,0.8012422360248447,"discriminator loss weight λ = 50 since the loss of the binary classiﬁcation task is much lower than the
MLM task, which is a 30, 000-way classiﬁcation task. The token embeddings are shared between the
generator Transformer and the discriminator Transformer. Other hyperparameters used in pretraining
and ﬁne-tuning are reported in Tables 5 and 6, respectively."
REFERENCES,0.8043478260869565,"The same (or equivalent) set of hyperparameters for pretraining and ﬁne-tuning are used for all
compared methods. The reported downstream task results on GLUE/SQuAD are the median of ﬁve
runs with the same set of random seeds."
REFERENCES,0.8074534161490683,"C
PRETRAINING EFFICIENCY"
REFERENCES,0.8105590062111802,"0.0
2.5
5.0
7.5
10.0
12.5
Training Time (Hours) 82 84 86 88 90"
REFERENCES,0.8136645962732919,Dev. Set Acc.
REFERENCES,0.8167701863354038,"AMOS
COCO-LM
ELECTRA
RoBERTa"
REFERENCES,0.8198757763975155,(a) MNLI-m
REFERENCES,0.8229813664596274,"0.0
2.5
5.0
7.5
10.0
12.5
Training Time (Hours) 82 84 86 88 90"
REFERENCES,0.8260869565217391,Dev. Set Acc.
REFERENCES,0.8291925465838509,"AMOS
COCO-LM
ELECTRA
RoBERTa"
REFERENCES,0.8322981366459627,(b) MNLI-mm
REFERENCES,0.8354037267080745,"Figure 4: AMOSBase accuracy on MNLI Dev.
sets (y-axes) at different pretraining hours on 64
A100 (40 GB Memory) GPUs. The ﬁnal training
hours and accuracy of COCO-LM, ELECTRA and
RoBERTa (trained under the exact same settings
and computing environments) are also shown."
REFERENCES,0.8385093167701864,"We compare the pretraining efﬁciency of AMOS
with COCO-LM (Meng et al., 2021), ELEC-
TRA (Clark et al., 2020) and RoBERTa (Liu
et al., 2019) under exactly the same computation
environment for base model training. We show
the MNLI-(m/mm) development set accuracy
(via standard ﬁne-tuning) of AMOS checkpoints
trained for different GPU hours in Figure 4. For
fair comparisons, we train all compared mod-
els using the same codebase, pretraining con-
ﬁguration, and computing environments. While
AMOS takes longer to train than RoBERTa and
ELECTRA, it matches their ﬁnal MNLI perfor-
mance with signiﬁcantly fewer pretraining hours
and achieves much better performance upon con-
vergence. For example, it achieves RoBERTa’s
MNLI accuracy with two hours of pretraining
and outperforms ELECTRA in three hours, more than a 50% reduction in pretraining time. It also
reaches the accuracy of COCO-LM, the recent state-of-the-art in both pretraining accuracy and
efﬁciency, only using 60% of COCO-LM’s pretraining hours. This demonstrates the advantage of the
adversarial curriculum learning of AMOS."
REFERENCES,0.8416149068322981,Published as a conference paper at ICLR 2022
REFERENCES,0.84472049689441,"Table 7: GLUE test set scores obtained from the GLUE leaderboard. We follow the standard in recent
research to construct the test predictions: Searching for best hyperparameters with ten random seeds
on each task individually and using the best development set model on testing data. All results are
from vanilla single-task ﬁne-tuning (no ensemble, task-speciﬁc tricks, etc.)."
REFERENCES,0.8478260869565217,"Model
Params
MNLI-(m/mm)
QQP
QNLI
SST-2
CoLA
RTE
MRPC
STS-B
AVG"
REFERENCES,0.8509316770186336,Base Setting:
REFERENCES,0.8540372670807453,"BERT
110M
84.6/83.4
89.2
90.5
93.5
52.1
66.4
84.8
85.8
80.8
ELECTRA
110M
85.8/–
89.1
92.7
93.4
59.7
73.1
86.7
87.7
83.5
COCO-LM
110M
88.3/88.1
89.9
93.3
94.9
61.9
81.5
87.8
88.6
85.8
AMOS
110M
88.9/88.1
90.0
93.6
95.3
68.7
81.1
88.5
90.2
87.0"
REFERENCES,0.8571428571428571,Base++ Setting:
REFERENCES,0.860248447204969,"ELECTRA
110M
88.5/88.0
89.5
93.1
96.0
64.6
75.2
88.1
90.2
85.6
COCO-LM
134M
89.8/89.3
89.8
94.2
95.6
68.6
82.3
88.5
90.3
87.4
AMOS
134M
90.4/89.9
90.2
94.6
96.8
69.2
83.6
88.9
91.3
88.1"
REFERENCES,0.8633540372670807,"Table 8: Ablations on the development sets of all GLUE tasks and SQuAD 2.0 that eliminate (-), add
(+) or switch (w.) one component. We show the median and standard deviation (as subscripts) of ﬁve
random seeds on each task. The results are extensions of Table 2."
REFERENCES,0.8664596273291926,"Group
Method
GLUE Single Task
SQuAD 2.0"
REFERENCES,0.8695652173913043,"MNLI-(m/mm)
QQP
QNLI
SST-2
CoLA
RTE
MRPC
STS-B
EM
F1"
REFERENCES,0.8726708074534162,"AMOSBase
88.90.1/88.70.1
92.30.0
93.60.1
94.20.2
70.71.5
86.61.4
90.90.4
91.60.1
84.20.2
87.20.3"
REFERENCES,0.8757763975155279,"Curriculum
w. random layer
88.60.1/88.30.1
92.20.1
93.20.2
93.90.2
70.21.6
84.81.0
91.40.7
91.80.2
83.60.2
86.60.2
Setup
w. layer switch
88.60.1/88.40.1
92.20.1
93.00.1
94.00.3
70.21.0
85.60.9
90.91.0
91.60.1
83.30.3
86.20.1"
REFERENCES,0.8788819875776398,"Adversarial
- adv. train
88.70.1/88.50.1
92.30.1
93.20.1
94.20.2
71.30.9
87.01.1
90.90.9
91.50.1
83.90.1
86.80.2
Setup
+ adv. MLM
88.60.1/88.70.2
92.20.1
93.50.2
93.80.3
71.31.0
85.91.2
91.41.0
91.80.1
84.20.2
87.20.1
w. ﬁnal mix weights
88.40.1/88.00.1
92.20.1
93.00.2
93.60.3
70.90.8
83.01.0
90.90.5
91.30.2
83.50.1
86.50.1"
REFERENCES,0.8819875776397516,"Multi-MLM
- stop grad.
88.40.1/88.60.1
92.30.1
93.90.2
93.90.4
71.11.1
87.02.0
91.20.7
91.60.1
83.60.2
86.60.2
Setup
w. separate MLM gen.
88.80.1/88.70.2
92.30.1
93.20.1
94.00.2
72.10.8
85.21.2
90.70.6
91.60.1
83.40.3
86.50.3
w. three 4-layer gen.
88.60.1/88.40.3
91.90.0
92.80.2
93.91.0
69.71.3
83.00.9
90.91.0
91.20.2
83.00.3
85.90.2"
REFERENCES,0.8850931677018633,"Backbone
4-layer gen.
88.40.1/88.20.0
92.20.0
93.10.2
94.20.5
69.21.1
84.81.2
91.20.4
91.30.1
83.10.3
86.00.2
(No Multi-MLM
6-layer gen.
88.60.1/88.30.1
92.20.1
93.20.1
93.20.5
69.21.7
83.41.1
90.20.7
91.40.2
83.20.2
86.10.2
No Adv. Train)
8-layer gen.
88.30.2/88.10.1
92.10.1
93.00.2
93.50.5
70.01.2
85.21.0
90.71.0
91.20.1
83.30.3
86.30.3
12-layer gen.
87.80.2/87.50.2
92.10.1
92.70.1
92.70.2
69.30.4
81.60.5
90.00.7
91.40.2
82.40.2
85.50.2"
REFERENCES,0.8881987577639752,"D
GLUE TEST SET RESULTS"
REFERENCES,0.8913043478260869,"We show the GLUE test set scores obtained via private submissions to the GLUE leaderboard in
Table 7. The baseline results are directly retrieved from the leaderboard or from their original papers.
We use standard single-task ﬁne-tuning to more directly reﬂect the improvements from pretrained
models. The advantage of AMOS over strong baselines holds on the test set: Under both base and
base++ settings, AMOS outperforms ELECTRA (Clark et al., 2020) and COCO-LM (Meng et al.,
2021) on almost every task. Still, we would like to note that smaller tasks such as CoLA, RTE, and
MRPC are not stable and leaderboard runs often use more sophisticated ﬁne-tuning method to achieve
better performance, for example, continuing training from the checkpoints ﬁne-tuned on MNLI."
REFERENCES,0.8944099378881988,"E
MORE DETAILED ABLATION RESULTS"
REFERENCES,0.8975155279503105,"We show more detailed ablation results in Table 8 which extends Table 2 by including results from all
GLUE tasks and showing the standard deviations. We also include two new ablations: w. ﬁnal mix
weights pretrains the discriminator all the way with the ﬁxed mixture weights learned by AMOS at
convergence; w. three 4-layer gen. trains the discriminator with mixtures over three 4-layer generators
instead of the multi-layer MLM generator in AMOS."
REFERENCES,0.9006211180124224,"AMOS has better performance than all other ablation versions on most large tasks (MNLI, QQP,
SST-2 and SQuAD) which are considered more stable and reliable indicators of model effectiveness.
Small tasks (CoLA, RTE, MRPC) have much higher variance than larger tasks, and usually require
intermediate task training to yield stable results (i.e., starting from checkpoints that are ﬁne-tuned on
MNLI (Clark et al., 2020; He et al., 2021; Liu et al., 2019))."
REFERENCES,0.9037267080745341,Published as a conference paper at ICLR 2022
REFERENCES,0.906832298136646,"Table 9: Performance study with different numbers of MLM heads used in the generator. We show
the median and standard deviation (as subscripts) of ﬁve random seeds on each task."
REFERENCES,0.9099378881987578,"MLM Layers
GLUE Single Task
SQuAD 2.0"
REFERENCES,0.9130434782608695,"MNLI-(m/mm)
QQP
QNLI
SST-2
CoLA
RTE
MRPC
STS-B
EM
F1"
REFERENCES,0.9161490683229814,"On (4,6,8) (AMOS)
88.90.1/88.70.1
92.30.0
93.60.1
94.20.2
70.71.5
86.61.4
90.90.4
91.60.1
84.20.2
87.20.3"
REFERENCES,0.9192546583850931,"On (4,8)
88.60.2/88.40.2
92.30.1
93.40.1
94.40.2
68.91.2
85.90.7
91.40.9
91.50.1
83.80.2
86.70.2
On (2,4,6,8)
88.40.1/88.30.2
92.20.1
93.50.2
93.80.3
69.10.7
84.51.5
90.90.4
91.30.2
83.80.2
86.80.2
On all 8
88.20.1/88.00.2
92.20.1
93.10.1
94.00.4
71.51.4
84.81.0
90.71.0
91.60.2
83.10.2
86.00.1"
REFERENCES,0.922360248447205,"The instability of the GLUE small tasks is a widely-observed artifact in pretraining research. It is
standard (and recommended) practice to not rely on these small unstable tasks for ablation studies.
For example, to ensure a correct understanding of research progress, previous studies including
BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ELECTRA (Clark et al., 2020), and
DeBERTa (He et al., 2021) all perform ablation studies on large GLUE tasks like MNLI. We include
the ablation results on small GLUE tasks in Table 8 only for reference."
REFERENCES,0.9254658385093167,"Furthermore, we show the standard derivations of the same pretrained checkpoint when ﬁne-tuned
on the corresponding tasks with different random seeds. They further conﬁrm that these smaller
tasks have large variance in the ﬁne-tuning stage. Observing the variance of the pretraining stage
(i.e., changes in model performance when pretrained with different random seeds) requires running
the costly pretraining multiple times which is often infeasible. We would like to refer to a recent
study (Sellam et al., 2022) which reveals that these small GLUE tasks are very unstable, and the same
model pretrained with different random initialization can have 2 −5 points difference in performance
upon ﬁne-tuning on these small tasks, while in comparison, observations on MNLI are more reliable.
Since in this paper we only perform single-task ﬁne-tuning without any intermediate task training,
we mainly rely on large task results for evaluating the effectiveness of different model components.
The GLUE average score is more of a convenient reference point as used in the pretraining research
community."
REFERENCES,0.9285714285714286,"F
PERFORMANCE STUDY WITH DIFFERENT NUMBERS OF MLM HEADS"
REFERENCES,0.9316770186335404,"In this experiment, we study the performance of AMOS with different numbers of generator MLM
heads K. In addition to the original AMOS which uses three MLM heads, we also show in Table 9
the downstream task performance when two MLM heads (at the 4th and 8th layers), four MLM heads
(at the 2nd, 4th, 6th and 8th layers) and eight MLM heads (at each layer) are used. We note that
increasing MLM heads does not necessarily lead to better performance, since having more MLM
heads means that each MLM block (partitioned by the stop gradient operators at each MLM layer)
will become shallower with weaker MLM learning capacity. We have also tried inserting the same
amount of MLM heads at different layers of the generator, but it does not improve the results."
REFERENCES,0.9347826086956522,"G
DISCUSSIONS ON USING GUMBEL-SOFTMAX FOR GRADIENT ESTIMATION"
REFERENCES,0.937888198757764,"In this work, we use Gumbel-Softmax to enable gradient approximation of the non-differentiable
sampling operation so that we are able to use the gradient backpropagated from the discriminator
to train the mixture weights. There are other possible approaches for discriminator gradient estima-
tion, including REINFORCE (Yu et al., 2017) which has been explored in the ablation studies of
ELECTRA (Clark et al., 2020), or directly operating in the hidden states of the generator instead
of in its discrete output space which has been studied in adversarial learning based text generation
research (Subramanian et al., 2018; Zhang et al., 2017; Zhao et al., 2018). We would like to note
that as our ﬁrst study on adversarial curriculum for language model pretraining, we prefer a simple
framework and standard techniques to demonstrate that such a new direction is promising. We believe
that exploring more sophisticated and advanced realizations for speciﬁc model components in our
AMOS framework (e.g., using better gradient estimators than Gumbel-Softmax) will be an interesting
future work direction."
REFERENCES,0.9409937888198758,Published as a conference paper at ICLR 2022
REFERENCES,0.9440993788819876,"Table 11: Examples of replaced tokens by different-sized MLM generators. Underlined words are
masked out; the replaced tokens by 4/6/8-layer generators are marked in different colors."
REFERENCES,0.9472049689440993,"Example 1. ""Here, let me show you that animal."" He pointed up to (at/at/into) the canopy; hanging from
a white branch was (of/stood/stood) a pale, hairless creature (shape/plant/animal). It was bilaterally
symmetrical, with two pairs of tightly (long/ﬁrmly/strongly) folded limbs, but did not appear to have any
discernible head."
REFERENCES,0.9503105590062112,"Example 2. In 2012, about 3, 000 (6/1,800/1,800) villagers remained in Rammun, while there are about
7, 000 in the Palestinian diaspora, chieﬂy in the United States. Many in the diaspora have second homes
in the (this/the/their) village. These homes have been troubled by burglaries, therefore some owners have
organised night-watches (things/boxes/searches)."
REFERENCES,0.953416149068323,"H
PROBING EXPERIMENTS WITH DIFFERENT SETUPS"
REFERENCES,0.9565217391304348,"Table 10: Edge probing results of (sepa-
rate) generators with different numbers
of Transformer layers."
REFERENCES,0.9596273291925466,"Tasks
4 layer
6 layer
8 layer"
REFERENCES,0.9627329192546584,"POS
93.7
94.7
94.3
Consts.
75.0
76.2
76.3
Deps.
88.8
89.4
89.5
Entities
93.6
94.8
94.9
SRL
80.9
81.6
82.3
Coref.
80.0
81.7
81.7
SPR2
79.8
81.3
80.2
Relations
74.5
75.8
75.0"
REFERENCES,0.9658385093167702,"In addition to the probing experiments conducted in Sec-
tion 5.2, we also show the results under a different setup:
Instead of testing the linguistic information captured by
different MLM layers in AMOS generator, we conduct the
same tests on separate generators with different depths (4
layers, 6 layers and 8 layers). The probing test results on
eight tasks are shown in Table 10. Similar to the results in
Table 3, generators of different depths are good at captur-
ing different types of linguistic information. Therefore, it
is also possible to compose diverse training signals with
multiple separate generators with different numbers of lay-
ers. Such ﬁndings may be useful for future studies that
explore using multiple different-sized auxiliary models to
provide training signals emphasizing different linguistic
aspects."
REFERENCES,0.968944099378882,"I
CASE STUDY"
REFERENCES,0.9720496894409938,"Table 11 provides concrete cases of replaced tokens generated by different generator layers. The
“mistakes” made by different generator layers have different levels of difﬁculty to be detected–The
4th layer MLM sometimes makes simple syntactic mistakes while the replaced tokens given by the
6th/8th layer are mostly plausible and need to be distinguished based on a deep understanding of the
full contexts. This intuitively conﬁrms our motivation that using a generator of multiple MLM heads
can provide diverse pretraining signals to compose a more effective learning curriculum."
REFERENCES,0.9751552795031055,"J
THE ORIGINS OF REPORTED BASELINE SCORES"
REFERENCES,0.9782608695652174,"The baseline results reported in Table 1 are obtained from the corresponding papers except the
following: BERT/RoBERTa from Bao et al. (2020), ELECTRA from Meng et al. (2021), XLNet
base++ from Bao et al. (2020). When there are different reported scores for the same method, we
use the highest of them in our comparisons."
REFERENCES,0.9813664596273292,"K
SOCIETAL IMPACT"
REFERENCES,0.984472049689441,"There have been concerns about the extensive costs of computing resource required by pretraining
language models. The concerns include whether it is worthwhile to spend such enormous amount
of resources in pretraining, and also whether the demanding resource requirements have posed a
barrier for most institutions to conduct pretraining research, thus slowing down the overall scientiﬁc
development."
REFERENCES,0.9875776397515528,"One major motivation of this work is to more efﬁciently pretrain language models, including (1)
to achieve better model effectiveness with ﬁxed computing resource, and (2) to achieve the same"
REFERENCES,0.9906832298136646,Published as a conference paper at ICLR 2022
REFERENCES,0.9937888198757764,"downstream task accuracy with fewer pretraining computes. As shown in our experimental results,
we are able to obtain certain successes in both fronts. We demonstrate that one can achieve the same
pretraining effectiveness of RoBERTaBase in 128 GPU hours on A100 using AMOS (two hours on 64
A100 GPUs), which is quite affordable for many research institutions. We hope our observation will
inspire more future studies in efﬁcient pretraining and also enable conducting pretraining research in
more accessible computing environments."
REFERENCES,0.9968944099378882,"In addition, we also mainly focus on the base-sized models in our exploratory research which are less
costly than large or extra large models, while also being the most commonly used model conﬁguration
in the community. We release our pretrained model checkpoints to the community as well."
