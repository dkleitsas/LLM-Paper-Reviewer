Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00546448087431694,"Ultra-low power local signal processing is a crucial aspect for edge applications on
always-on devices. Neuromorphic processors emulating spiking neural networks
show great computational power while fulﬁlling the limited power budget as needed
in this domain. In this work we propose spiking neural dynamics as a natural al-
ternative to dilated temporal convolutions. We extend this idea to WaveSense, a
spiking neural network inspired by the WaveNet architecture. WaveSense uses
simple neural dynamics, ﬁxed time-constants and a simple feed-forward architec-
ture and hence is particularly well suited for a neuromorphic implementation. We
test the capabilities of this model on several datasets for keyword-spotting. The
results show that the proposed network beats the state of the art of other spiking
neural networks and reaches near state-of-the-art performance of artiﬁcial neural
networks such as CNNs and LSTMs."
INTRODUCTION,0.01092896174863388,"1
INTRODUCTION"
INTRODUCTION,0.01639344262295082,"Local signal processing is an important component of the computational pipeline for Internet-of-
Things (IoT) devices equipped with a range of sensors like audio, video, and motion sensing.
A signiﬁcant range of these sensors capture signals comprising temporal features. Ideally these
features need to be extracted by an on-board processor before decision making or relaying the pre-
processed information for further computation. Processing temporal signals is often computationally
challenging and requires large amounts of memory and power, especially in always-on scenarios.
Neuromorphic (Mead, 1990) processors with spiking-neural networks have shown promise in this
domain as ultra-low power compact solutions Indiveri et al. (2011); Benjamin et al. (2014); Merolla
et al. (2014); Furber et al. (2014); Davies et al. (2018); Liu et al. (2019)."
INTRODUCTION,0.02185792349726776,"In this work we propose an elegant way of implementing temporal convolutions in spiking neural
networks by leveraging their inherent synaptic and membrane dynamics. Based on this idea we
propose WaveSense, a Spiking Neural Network (SNN) model suitable for efﬁcient neuromorphic
implementations while retaining high accuracy on temporal data streams. This work bridges the
performance gap between Artiﬁcial Neural Networks (ANNs) and SNNs for temporal tasks. Crucially,
the proposed model"
INTRODUCTION,0.0273224043715847,"• accepts spike streams and not ‘buffered frames’ as input,
• requires no delays in its connectivity,
• utilizes a very simple spiking neuron model - Leaky Integrate and Fire (LIF) neuron - without
the need of any additional adaptive mechanisms,
• does not require recurrent connectivity (which can often be difﬁcult to tune or train),
• achieves a high classiﬁcation performance."
INTRODUCTION,0.03278688524590164,"Recently several works have shown how to build efﬁcient SNNs with accuracies equivalent to ANNs
Diehl et al. (2015); Rueckauer et al. (2017). In these studies, spiking neurons are used in rate mode
with equivalent response curves to ReLU activations, to transfer weights from pre-trained ANNs to
SNNs. This approach therefore completely neglects the temporal capabilities of spiking neurons. On
the other hand, surrogate gradient methods enable directly training SNN using Back Propagation"
INTRODUCTION,0.03825136612021858,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04371584699453552,"Through Time (BPTT) Neftci et al. (2019). (Shrestha & Orchard, 2018) for instance show temporal
processing on temporal gesture recognition task and show a good performance on a visual task.
Similar approaches have already been investigated on audio tasks Bellec et al. (2018); Wu et al.
(2019); Cramer et al. (2020). (Bellec et al., 2018) demonstrate classiﬁcation results on the TIMIT
dataset using long time constants, a complex learning strategy (Deep-R) and require signiﬁcant
computational resources to train. (Wu et al., 2019) train SNNs for automatic speech recognition
tasks in a tandem approach with an ANNs. This training pipeline integrated a language model and
pronunciation model which goes beyond the capabilities of the neuromorphic system. The same
authors showed in an earlier study the capabilities of an SNN in combination with a self-organized
map to learn to recognize digits using the TIDIGIT dataset Wu et al. (2018)."
INTRODUCTION,0.04918032786885246,"(Blouw et al., 2018) demonstrate high accuracy in a audio classiﬁcation task using dense networks
with spectrograms as inputs. This approach requires passing the frequency data of previous time steps
(deﬁned by the spectrogram time window) in every sample presentation to the network. (Kugele
et al., 2020) show that by matching ANN roll-out delays to the propagation delays in SNN, the
resultant networks can demonstrate a high accuracy on vision-based spatio-temporal classiﬁcation
tasks. Implementation of delays in neuromorphic hardware requires additional memory resources
to store and deliver spikes in a delayed fashion and could be potentially quite expensive. (Yin
et al., 2020) use a Spiking Recurrent Neural Network (SRNN) architecture and demonstrate that by
utilizing adaptive LIF neurons and learning the time constants, these networks can perform temporal
classiﬁcation tasks in a sequential manner fairly well. The authors demonstrate the effective use of
spiking neural networks and show a signiﬁcant increase in power efﬁciency. Unfortunately, having
ﬁne tuned time constants in low-power neuromorphic hardware can often be challenging especially
while using ﬁxed precision numerical representations and computations."
INTRODUCTION,0.0546448087431694,"We propose a novel network architecture for SNN that does not require buffering or delays and can
directly process temporally varying streams of spiking data from event-based sensors using simple
LIF neurons. Our architecture is derived from ﬁrst principles and inspired by the WaveNet van den
Oord et al. (2016) architecture that does not necessitate learning the time constants of the system but
could be deﬁned as the task demands. In addition we also propose an efﬁcient training strategy and a
corresponding loss-function that is suitable for streaming based models, in particular models that
could be run in real-time with neuromorphic hardware."
INTRODUCTION,0.060109289617486336,"The ﬁrst key aspect of the WaveNet architecture is the use of multi-layer causal dilated convolutions.
The causal refers to the use of data from the past, dilated refers to a sparse kernel and the convolution
is along the time axis. Stacking such convolutional operations along multiple layers enables the
network to have a long temporal memory. A second aspect is that it eliminates the need for sliding
window based inference/prediction and minimizes the number of computations within the network
when operating on a continuous stream of data."
INTRODUCTION,0.06557377049180328,"The WaveNet architecture is very amenable to general purpose micro-processors and micro-controllers
but it still requires a reasonable amount of memory and non-linear computations such as tanh and
sigmoid which are fairly complex (within the context of ultra-low power devices) and this in turn
requires higher energy and power requirements. Neuromorphic technology promises to bring the
energy required for these tasks down by utilizing SNNs to perform ultra-low power computation.
So far, while neuromorphic devices have been demonstrated to operate at extremely low power,
they have fallen short at demonstrating computational performance that is on-par or comparable to
state-of-the-art ANNs for temporal tasks, most prominently in the audio domain."
INTRODUCTION,0.07103825136612021,"The WaveSense model proposed here aims to bridge this gap. For the results in this work, we focus
on audio tasks as spatio-temporal tasks without loss of generality. Sec. 2 details all the methods
used for audio data pre-processing, conversion to spikes and the details of the network architecture.
In Sec. 3 we demonstrate the computational capability of this network over several audio datasets
for key-word spotting tasks. We compare our results to the state-of-the-art SNNs and ANNs. We
conclude in Sec. 4 where we discuss the implications and impact of this work and potential areas
where this work could be utilized."
INTRODUCTION,0.07650273224043716,"Most importantly all the code used to generate the reported results have been made open-source and
can be found at http://XXXXXXXXXXXXXX. We believe this will enable the research community
to explore other avenues that could take advantage of our work."
INTRODUCTION,0.08196721311475409,Under review as a conference paper at ICLR 2022
MATERIALS AND METHODS,0.08743169398907104,"2
MATERIALS AND METHODS"
DILATED TEMPORAL CONVOLUTIONS,0.09289617486338798,"2.1
DILATED TEMPORAL CONVOLUTIONS d=1 d=3 dt tau=1 tau=3 dt"
DILATED TEMPORAL CONVOLUTIONS,0.09836065573770492,output
DILATED TEMPORAL CONVOLUTIONS,0.10382513661202186,hidden 2
DILATED TEMPORAL CONVOLUTIONS,0.1092896174863388,hidden 1
DILATED TEMPORAL CONVOLUTIONS,0.11475409836065574,hidden 0
DILATED TEMPORAL CONVOLUTIONS,0.12021857923497267,"input
d=1 d=2 d=4 d=8 time A
B time"
DILATED TEMPORAL CONVOLUTIONS,0.12568306010928962,"time
C"
DILATED TEMPORAL CONVOLUTIONS,0.13114754098360656,Figure 1: Relegating the job of delays in dilated convolutions to synaptic dynamics.
DILATED TEMPORAL CONVOLUTIONS,0.1366120218579235,"Temporal convolutions enable a layer of neurons to integrate information from the past. The dilation
enables convolution over a large time window (or sample length) while still only using a small
number of parameters. Temporal dilated convolutions therefore, perform a weighted-accumulation
of information from different points in time, separated by the dilation parameter. This is done by
storing previous activations in ANNs. Naively, the equivalent could be achieved in SNNs by utilizing
synaptic transmission delays. In this work, we observe that neuron and synaptic dynamics could be
seen as proxies for temporal convolutional processing as shown in Figure 1. Figure 1B shows the
contributions of each projection with a kernel size 2 when implemented with delays. Implementing
synaptic transmission delays in real-time neuromorphic hardware incurs a steep overhead in terms of
memory and computational resources and only a limited number of neuromorphic devices support
them."
DILATED TEMPORAL CONVOLUTIONS,0.14207650273224043,"We instead propose to use an appropriate set of synaptic time constants τs as shown in Figure 1C.
While quantitatively, these are different, qualitatively both these approaches provide the ability to
transform and project information in the temporal domain Sheik et al. (2012). This is the key insight
we leverage to design our ﬁnal model."
NETWORK DESCRIPTION,0.14754098360655737,"2.2
NETWORK DESCRIPTION"
NETWORK DESCRIPTION,0.15300546448087432,"We take inspiration from the work of (Coucke et al., 2018) who uses the WaveNet architecture van den
Oord et al. (2016) for classiﬁcation of continuous audio streams. The WaveNet architecture provides
a prescription for distributing temporal memory and computation across layers without repeated
presentation of previous input data."
NETWORK DESCRIPTION,0.15846994535519127,"The original ANN WaveNet model comprises of a few different computational building blocks. We
translate each of these building blocks to SNNs as follows."
NETWORK DESCRIPTION,0.16393442622950818,"Dilated Temporal Convolutions as mentioned above are implemented by synaptic projections with
multiple time constants."
NETWORK DESCRIPTION,0.16939890710382513,"Rectiﬁed Linear Unit (ReLU) activations can be approximated by spiking neurons because a
spiking neuron can only produce spikes if the membrane potential crosses a threshold and is silent
otherwise Diehl et al. (2015)."
NETWORK DESCRIPTION,0.17486338797814208,"Non-linear activations like tanh and sigmoid cannot be efﬁciently translated to SNNs. So we
choose to replace these activations with SNNs activations (potentially ignoring the beneﬁts of ﬁltering
and gating). In the original model, these two activations are preceded by two sets of weights, where
as in our SNN we only use one weighted projection. (See Figure 2)"
NETWORK DESCRIPTION,0.18032786885245902,Residual connections and summation (+) are realized by a synaptic connection.
NETWORK DESCRIPTION,0.18579234972677597,"The WaveSense model is built upon these building blocks as shown in Figure 2. It comprises of
several ‘blocks’, each of which comprises of three spiking neuron layers. The ﬁrst spiking layer
receives inputs ﬁltered by two separate synapses with different time constants τs and weights. The"
NETWORK DESCRIPTION,0.1912568306010929,Under review as a conference paper at ICLR 2022
NETWORK DESCRIPTION,0.19672131147540983,"time constants τs of the slow projections in each of these blocks are chosen such that they span a
range of values relevant to the task. The number of blocks is chosen such that the sum of all these
time constants is proportional to the temporal memory demanded by the task. This layer projects
to the second spiking layer. Additionally a third spiking layer in each of these blocks projects to a
‘hidden’ layer followed by a non-spiking ‘low pass’ readout layer. The output of this block is the
summation of its input (residual connection) and the output of the second layer. These ‘blocks’ are
connected in a feed forward manner. The non-spiking ‘low-pass’(LP) layer simply acts as a weighted
low pass ﬁlter on the spikes of the ‘hidden’ layer. This is equivalent to the synapse of a spiking
neuron (without the neuron’s membrane potential or the spiking dynamics) and does not require any
extra components unavailable to spiking neurons on a typical neuromorphic platform. The choice of
leaving the output layer to be non-spiking is to enable a smooth, continuous valued readout useful for
faster learning using BPTT."
NETWORK DESCRIPTION,0.20218579234972678,"Figure 2: The WaveSense model prescribed in this work is a theoretical adaptation of the WaveNet
architecture van den Oord et al. (2016) based on ﬁrst principles."
DATASETS,0.20765027322404372,"2.3
DATASETS"
DATASETS,0.21311475409836064,"In order to evaluate the efﬁcacy of the proposed model, we train and test it against several open-source
publicly available audio datasets."
ALOHA DATASET,0.2185792349726776,"2.3.1
ALOHA DATASET"
ALOHA DATASET,0.22404371584699453,"The Aloha dataset Blouw et al. (2018) is a small collection of audio samples containing the keyword
’Aloha’ and several distractors such as ’take a load off’. As the dataset is very small, only ∼2000
samples, we augmented the samples using the MUSAN noise dataset Snyder et al. (2015). For that
end we standardized the sample length of each utterance in the training and validation set to ﬁve
seconds and added randomly selected background noise data with a signal-to-noise ratio (SNR) of 5
dB to the training data."
HEY SNIPS DATASET,0.22950819672131148,"2.3.2
HEY SNIPS DATASET"
HEY SNIPS DATASET,0.23497267759562843,"The ’Hey Snips’ dataset Coucke et al. (2018) for wake phrase spotting distinguishes between two
classes. The positive class contains 11‘000 utterances from over 2‘000 speakers of the wake phrase
’Hey Snips’ while the negative (or distractor) class contains over 86‘000 negative examples from
more than 6‘000 speakers. We split the data into a training-, validation- and test-set as provided by"
HEY SNIPS DATASET,0.24043715846994534,Under review as a conference paper at ICLR 2022
HEY SNIPS DATASET,0.2459016393442623,"the authors of the dataset. We standardized the sample length of each utterance in the training and
validation set to ﬁve seconds. As the dataset is already very large, no noise augmentation was needed."
SPEECH COMMANDS DATASET,0.25136612021857924,"2.3.3
SPEECH COMMANDS DATASET"
SPEECH COMMANDS DATASET,0.2568306010928962,"The Speech Commands Warden (2018) describe a dataset containing 35 keywords uttered in total
105‘000 times from over 2‘600 speakers. The keywords contain the numbers 0 - 10, commands such
as ""stop"", ""go"", ""left"" and ""right"" as well as other words like ""Marvin"", ""Sheila"", etc. This dataset
was initially designed for keyword spotting in a limited vocabulary and the intended experiment is to
detect 10 commands (plus silence) out of all 35 keywords (12 classes in total). Nevertheless, there
are studies training models and showing results on all 35 classes Cramer et al. (2020). We augment
the training set with noise data from the MUSAN dataset using an SNR of 5 dB just as we do for the
Aloha dataset."
PRE-PROCESSING,0.26229508196721313,"2.4
PRE-PROCESSING"
PRE-PROCESSING,0.2677595628415301,The raw audio data is pre-processed in several stages:
PRE-PROCESSING,0.273224043715847,"• Noise augmentation The training data is augmented with noise from the MUSAN noise
dataset using a SNR of 5dB (except for the HeySnips data)."
PRE-PROCESSING,0.2786885245901639,"• Length standardization and pre-ampliﬁcation The noise augmented waveform is cut into
a standard length (dependent on the dataset) and the amplitude is normalized."
PRE-PROCESSING,0.28415300546448086,"• Band-pass ﬁlters The audio is then passed through 64 Butterworth bandpass ﬁlters of 2nd
order. The bandpass ﬁlters are distributed in Mel-scale between 100Hz and 8kHz."
PRE-PROCESSING,0.2896174863387978,• Rectiﬁcation The response of the 64 bandpass ﬁlters is rectiﬁed using a full-wave rectiﬁer.
PRE-PROCESSING,0.29508196721311475,"• Spike conversion and binning The output of the rectiﬁer is applied as direct input to the
membrane of 64 simpliﬁed LIF neurons resulting in a rate code. The spike trains are binned
into 10ms timesteps allowing multiple spikes per timestep."
PRE-PROCESSING,0.3005464480874317,Figure 3: Data pre-processing pipeline ﬁgure.
TRAINING METHOD,0.30601092896174864,"2.5
TRAINING METHOD"
TRAINING METHOD,0.3114754098360656,"In order to train the parameters of the SNN (See Sec. 2.2) we use BPTT. In particular we aim to
be able to deploy the network in streaming mode i.e. the model receives the data stream directly
generated from a sensor without any frame-based (sliding window) buffering. This requires us to
employ an appropriate loss function."
TRAINING METHOD,0.31693989071038253,"Often in a classiﬁcation task, the output class can be determined by computing cross-entropy loss on
the sum of the outputs over the sample length for each output neuron. While this would yield a good
classiﬁcation accuracy, the magnitude of the output trace at ‘a given point in time’ is not indicative of
the network prediction. This is not ideal for models being run in streaming mode."
PEAK LOSS,0.3224043715846995,"2.5.1
PEAK LOSS"
PEAK LOSS,0.32786885245901637,"Typically for streaming models, a signal is predicted as belonging to a certain class when the
corresponding output trace exceeds a ‘detection threshold’. This approach is also ideal for always-on"
PEAK LOSS,0.3333333333333333,Under review as a conference paper at ICLR 2022
PEAK LOSS,0.33879781420765026,"Figure 4: An example visualization of the peak loss with peak times t∗
c for channels 0 and 1."
PEAK LOSS,0.3442622950819672,"neuromorphic systems. We therefore design our loss function to reﬂect this detection mechanism and
train our neural networks. We determine the peaks of the output traces and use only the activation
values at the peaks to compute the cross entropy loss (see Figure 4) similar to max-over-time
loss Cramer et al. (2020)."
PEAK LOSS,0.34972677595628415,Consequently the loss is computed as follows:
PEAK LOSS,0.3551912568306011,"LCE = −
X"
PEAK LOSS,0.36065573770491804,"c
λclog(pc)
(1)"
PEAK LOSS,0.366120218579235,"where λc yields 1 if class label c corresponds to the current input and 0 otherwise. pc is the prediction
probability by the neural network that the current input belongs to class c. It is calculated by a softmax
operation as shown below."
PEAK LOSS,0.37158469945355194,"pc =
eˆyc
P"
PEAK LOSS,0.3770491803278688,"i eˆyi
(2)"
PEAK LOSS,0.3825136612021858,where ˆy are the ’logits’ produced by the neural network.
PEAK LOSS,0.3879781420765027,"For temporal tasks, the input x = xT = x1...T and the output (logits) ˆy of the neural network are
time-series over time T."
PEAK LOSS,0.39344262295081966,"ˆyt = f(xt|Θ, st)
(3)"
PEAK LOSS,0.3989071038251366,"where f is the transformation of the neural network, Θ are the network parameters and st is the
internal state of the network at time t. In peak-loss we pass the peak of each output trace to the
softmax function. The peaks are calculated as follows:"
PEAK LOSS,0.40437158469945356,"ˆyc = max(yT
c ) = yt∗
c
c
(4)"
PEAK LOSS,0.4098360655737705,"where t∗
c = argmax(yT
c ) is the ’peak time’, the time of maximal activation of output trace c (see
Figure 4)."
SPIKING ACTIVITY REGULARIZATION,0.41530054644808745,"2.5.2
SPIKING ACTIVITY REGULARIZATION"
SPIKING ACTIVITY REGULARIZATION,0.4207650273224044,"The activity of LIF neurons can change dramatically during the learning process. It can either lead to
the absence of spikes which stalls learning or in exploding activation which results in high energy
utilization of the network in a neuromorphic implementation."
SPIKING ACTIVITY REGULARIZATION,0.4262295081967213,"In order to limit the activity of these neurons and maintain sparse activity, we include an activity
regularizer term in our loss function Sorbaro et al. (2020)."
SPIKING ACTIVITY REGULARIZATION,0.43169398907103823,"Lact = (N †
spk/(T · Nneurons))2
(5)"
SPIKING ACTIVITY REGULARIZATION,0.4371584699453552,"where the activation loss Lact is dependent on the total excess number of spikes N †
spk produced by
the network with a population size Nneurons in response to a input of length T time steps. N †
spk is
given as:"
SPIKING ACTIVITY REGULARIZATION,0.4426229508196721,Under review as a conference paper at ICLR 2022
SPIKING ACTIVITY REGULARIZATION,0.44808743169398907,Table 1: Aloha result model size and resource comparison.
SPIKING ACTIVITY REGULARIZATION,0.453551912568306,"Publication
#Neurons
#Parameters
Accuracy
(Blouw et al., 2018)
541
172800
95.8
This work
864
18482
98.0 ± 1.1"
SPIKING ACTIVITY REGULARIZATION,0.45901639344262296,"N †
spk =
X X"
SPIKING ACTIVITY REGULARIZATION,0.4644808743169399,"i
N t
i Θ(N t
i −1)
(6)"
SPIKING ACTIVITY REGULARIZATION,0.46994535519125685,is the sum of spikes from all neurons Ni exceeding 1 in each time bin t (Θ is a heaviside function).
SPIKING ACTIVITY REGULARIZATION,0.47540983606557374,Finally the loss function is given as:
SPIKING ACTIVITY REGULARIZATION,0.4808743169398907,"L = LCE + αLact
(7)"
SPIKING ACTIVITY REGULARIZATION,0.48633879781420764,where α was chosen to be 0.01.
RESULTS,0.4918032786885246,"3
RESULTS"
RESULTS,0.4972677595628415,"In order to validate and verify that sufﬁcient information from the input is retained after pre-processing
and conversion to spikes, we train a state-of-the-art WaveNet classiﬁer on the datasets considered
in this work and check that we can obtain a high accuracy. We implement a non-spiking dilated
Convolutional Neural Network (CNN) to replicate the WaveNet architecture very similar to that
described in (Coucke et al., 2018; van den Oord et al., 2016) (see Section 2.2 for details)."
RESULTS,0.5027322404371585,"We train this ANN on the HeySnips, Aloha and SpeechCommands datasets and compare our results to
those reported in literature Coucke et al. (2018); Blouw et al. (2018); Cramer et al. (2020). The results
obtained from this network are then used as baseline to evaluate the performance of the proposed
SNN."
ALOHA DATASET,0.5081967213114754,"3.1
ALOHA DATASET"
ALOHA DATASET,0.5136612021857924,"In order to compare our model to other SNN implementations in the keyword spotting domain,
we trained our WavseSense on the Aloha dataset Blouw et al. (2018). Table 1 shows the memory
resources of the proposed model in comparison to the work demonstrated in (Blouw et al., 2018).
With an average accuracy of 98.0% with a standard deviation of 1.1%, the model presented in this
work performs signiﬁcantly better while at the same time requiring a signiﬁcantly fewer parameters.
The best runs of the WaveSense model yielded 99.5% accuracy which is equal to the performance of
the ANN model. It is important to note that the key focus of the work by (Blouw et al., 2018) is to
benchmark energy and power consumption and not model performance."
HEYSNIPS DATASET,0.5191256830601093,"3.2
HEYSNIPS DATASET"
HEYSNIPS DATASET,0.5245901639344263,"On the HeySnips dataset, our implementation of the WaveNet reaches an accuracy of 99.8% on the
clean dataset. In (Coucke et al., 2018), the authors do not report any accuracy number but rather report
the false rejection rate (FRR) of 0.12% for a ﬁxed false alarm per hours (FAPH) of 0.5. In order to
compare our results more accurately, we implement the same metrics; our WaveNet implementation
reaches 0.95 FAPH and a 0.8% FRR on the test set. These results are slightly worse than the results
reported by (Coucke et al., 2018) but that is expected as we do not apply the same speciﬁc methods
to improve performance such as ""End-Of-Keyword labeling"" and ""masking"". Without those methods
and without gating, the FRR reported by (Coucke et al., 2018) drops to 0.98%. On the other hand,
our WaveNet implementation reaches similar or even better results than the CNN and LSTM reported
by (Coucke et al., 2018). This fact shows that our pre-processing method indeed extracts sufﬁcient
information from the input such that a neural network can reach very high accuracy. Hence, we train
a spiking version of the WaveNet architecture (WaveSense), as described in 2.2, on the same data."
HEYSNIPS DATASET,0.5300546448087432,"In the WaveSense model we do not use any gating mechanism, a kernel size of 2 and only 8 layers;
much less compared to the 24 layers and kernel size of 3 as used in the WaveNet implementation by"
HEYSNIPS DATASET,0.5355191256830601,Under review as a conference paper at ICLR 2022
HEYSNIPS DATASET,0.5409836065573771,Table 2: A comparison of model performance for various datasets and network architectures.
HEYSNIPS DATASET,0.546448087431694,"Publication
Dataset
Accuracy (%)
Architecture
(Coucke et al., 2018)
HeySnips
FRR 0.12 FAPH 0.5
WaveNet
(Coucke et al., 2018)
HeySnips
FRR 2.09 FAPH 0.5
LSTM
(Coucke et al., 2018)
HeySnips
FRR 2.51 FAPH 0.5
CNN
This work
HeySnips
99.8 (FRR 0.8 FAPH 0.95)
WaveNet
This work
HeySnips
99.6 ± 0.1 (FRR 1.0 FAPH 1.34)
SNN
(Cramer et al., 2020)
SpeechCommands(35)
50.9 ± 1.1
SNN
(Cramer et al., 2020)
SpeechCommands(35)
73 ± 0.1
LSTM
(Cramer et al., 2020)
SpeechCommands(35)
77.7 ± 0.2
CNN
(Perez-Nieves et al., 2021)
SpeechCommands(35)
57.3 ± 0.4
SNN
This work
SpeechCommands(35)
87.6
WaveNet
This work
SpeechCommands(35)
79.6 ± 0.1
SNN
(Blouw et al., 2018)
Aloha
93.8
SNN
This work
Aloha
99.5
WaveNet
This work
Aloha
98.0 ± 1.1
SNN"
HEYSNIPS DATASET,0.5519125683060109,"(Coucke et al., 2018). The memory in our model is still long enough as WaveSense implements the
dilations using synaptic dynamics with long time constants but the number of parameters drops from
47′090 to 13′042. Despite the low number of parameters and quantization from spiking activations,
the WaveSense model achieves an average accuracy of 99.6% over 11 runs (only drops by 0.2%)
. Our best run of the WaveSense model yielded the same accuracy (of 99.8%) as our WaveNet
implementation. With an FRR = 1.0% and FAPH = 1.34 the performance is indeed lower than the
WaveNet, but it is comparable to that of LSTM and CNN as reported by (Coucke et al., 2018)."
SPEECHCOMMANDS DATASET,0.5573770491803278,"3.3
SPEECHCOMMANDS DATASET"
SPEECHCOMMANDS DATASET,0.5628415300546448,"We also trained WaveSense on the SpeechCommands dataset. We evaluated our model by training
it to classify all 35 classes in the dataset. In a study by (Perez-Nieves et al., 2021), in which the
authors investigate the impact of heterogenity of time constants on the performance, the best model
reached ∼57.3% accuracy on the same dataset. In (Cramer et al., 2020) the best performing SNN
is a recurrent network which yields ∼50.9% accuracy of all 35 classes. In the same study, also an
LSTM and CNN are trained on the same data resulting in an accuracy of ∼73% resp. ∼77.7%.
The WaveSense model reaches an average accuracy of 79.6% over 11 runs (best 80.0%) which is
signiﬁcantly higher than the best SNN described in previous studies. Notably, WaveSense performs
better than the reported LSTM and CNN Cramer et al. (2020)."
DISCUSSION AND CONCLUSION,0.5683060109289617,"4
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.5737704918032787,"While the results demonstrated here are obtained using a ﬁxed set of time constants, it is conceivable
that according to the constraints of the neuromorphic hardware, an appropriate network could be
trained to obtain qualitatively similar results. This holds true even for mixed-signal neuromorphic
devices Indiveri et al. (2011) with programmable weights and tune-able time constants. Because
the algorithm provides a recipe for how to choose the time constants in the network, even if a
neuromorphic substrate has a limited range of time constants, a number of layers with an appropriate
combination (sum) of time constants can always be chosen to ﬁt the temporal task. This is in
stark contrast to recurrent neural networks that often require a tight balance between excitation and
inhibition and long time constants Bellec et al. (2018); Yin et al. (2020)."
DISCUSSION AND CONCLUSION,0.5792349726775956,"The choice of time constants and number of layers is informed by the total temporal memory required
by the task. We choose them in a similar fashion to that of WaveNet with time constants increasing
with factors of 2 and such that the sum of all the time constants is proportional to τtask. Typically we
observe that a proportionality of 2.5 is suitable with a kernel size of 2. The proportionality factor is
the length of time after which the effect of a Post Synaptic Potential (PSP) is negligible. This also
translates to compact networks with fewer parameters for the same amount of temporal memory (at
the same time resolution). In other words, given a network, the temporal memory of a given task can
be computed as follows:"
DISCUSSION AND CONCLUSION,0.5846994535519126,Under review as a conference paper at ICLR 2022
DISCUSSION AND CONCLUSION,0.5901639344262295,"τtask ≈2.5
X"
DISCUSSION AND CONCLUSION,0.5956284153005464,"i
τ i
s
(8)"
DISCUSSION AND CONCLUSION,0.6010928961748634,where i is the list of all the layers in the WaveSense network.
DISCUSSION AND CONCLUSION,0.6065573770491803,"While the results reported here are signiﬁcantly high, we believe this can be further improved
by modifying the loss function. For instance the peak loss computed only during the presence
of a keyword as opposed to the entire sample Coucke et al. (2018) has been shown to improve
performance of such models. Furthermore, a thorough architecture search could potentially result in
a better combination of time constants τm and τs, number of channels, kernel sizes etc."
DISCUSSION AND CONCLUSION,0.6120218579234973,"A crucial factor in adopting a model is ease of training, deployment and power efﬁciency. By utilizing
simple LIF neurons, we take full advantage of their computational efﬁciency Yin et al. (2020) in
additional to sparse computations afforded by SNNs. While training SNNs is relatively slow on
CPUs and GPUs, utilizing the Spike Response model (SRM) in combination with the SLAYER
algorithm Shrestha & Orchard (2018), we are able to train at a relatively high speed. All experimental
results reported in this manuscript were performed on a single NVIDIA 1080 Ti with a few hours of
run time per experiment. We further improve upon this efﬁciency with a custom fork of the SLAYER
implementation 1. The resulting models, while accurate within the SRM framework, are not identical
to simulations based on LIF neurons, supported by most digital neuromorphic devices. But we ﬁnd
that they are a close approximation and a quick retraining can recover the model’s performance using
LIF neurons."
DISCUSSION AND CONCLUSION,0.6174863387978142,"The WaveNet architecture requires storing activations of each of its layers depending on their kernel
size and dilation value: Nbuf ∝(k −1) · d + 1. In contrast, WaveSense does not buffer any
spikes(activations) from the past explicitly. Instead, the information is retained in the neuron and
synaptic states: Nbuf ∝k + 1. This makes WaveSense extremely efﬁcient in terms of memory
utilization in contrast to WaveNet."
DISCUSSION AND CONCLUSION,0.6229508196721312,"The results demonstrated here show that the WaveSense architecture is suitable for audio classiﬁcation
tasks and show a promising performance improvement in comparison to prior state-of-the-art. Audio
signals, after they are pre-processed are equivalent to a population of neurons producing spike patterns
with complex spatio-temporal correlations. We argue therefore, that the results presented here can be
extended to other modalities of sensory data such as ECG, PPG, machine vibrations or DVS data."
DISCUSSION AND CONCLUSION,0.6284153005464481,"This work we believe could contribute towards a future with a ubiquitous abundance of always-on
audio and other sensory devices responding to user commands. This could lead to potential misuse
of the technology for surveillance. Thankfully, neuromorphic algorithms such as the one proposed
here require specialized neuromorphic hardware to take full advantage. If the availability of such
hardware could be regulated, we hope that society can beneﬁt from this technology while protecting
itself from misuse."
DISCUSSION AND CONCLUSION,0.6338797814207651,1https://XXXXXXXXXXXXX
DISCUSSION AND CONCLUSION,0.639344262295082,Under review as a conference paper at ICLR 2022
REFERENCES,0.644808743169399,REFERENCES
REFERENCES,0.6502732240437158,"Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass.
Long short-term memory and learning-to-learn in networks of spiking neurons. arXiv preprint
arXiv:1803.09574, 2018."
REFERENCES,0.6557377049180327,"Ben Varkey Benjamin, Peiran Gao, Emmett McQuinn, Swadesh Choudhary, Anand R Chandrasekaran,
Jean-Marie Bussat, Rodrigo Alvarez-Icaza, John V Arthur, Paul A Merolla, and Kwabena Boa-
hen. Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations.
Proceedings of the IEEE, 102(5):699–716, 2014."
REFERENCES,0.6612021857923497,"Peter Blouw, Xuan Choo, Eric Hunsberger, and Chris Eliasmith. Benchmarking keyword spotting
efﬁciency on neuromorphic hardware, 2018."
REFERENCES,0.6666666666666666,"Alice Coucke, Mohammed Chlieh, Thibault Gisselbrecht, David Leroy, Mathieu Poumeyrol, and
Thibaut Lavril. Efﬁcient keyword spotting using dilated convolutions and gating, 2018."
REFERENCES,0.6721311475409836,"Benjamin Cramer, Yannik Stradmann, Johannes Schemmel, and Friedemann Zenke. The heidelberg
spiking data sets for the systematic evaluation of spiking neural networks. IEEE Transactions on
Neural Networks and Learning Systems, 2020."
REFERENCES,0.6775956284153005,"Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha
Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic
manycore processor with on-chip learning. Ieee Micro, 38(1):82–99, 2018."
REFERENCES,0.6830601092896175,"Peter U Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and Michael Pfeiffer.
Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing. In
2015 International joint conference on neural networks (IJCNN), pp. 1–8. ieee, 2015."
REFERENCES,0.6885245901639344,"Steve B Furber, Francesco Galluppi, Steve Temple, and Luis A Plana. The spinnaker project.
Proceedings of the IEEE, 102(5):652–665, 2014."
REFERENCES,0.6939890710382514,"Wulfram Gerstner. A framework for spiking neuron models: The spike response model. In Handbook
of Biological Physics, volume 4, pp. 469–516. Elsevier, 2001."
REFERENCES,0.6994535519125683,"Giacomo Indiveri, Bernabé Linares-Barranco, Tara Julia Hamilton, André Van Schaik, Ralph Etienne-
Cummings, Tobi Delbruck, Shih-Chii Liu, Piotr Dudek, Philipp Häﬂiger, Sylvie Renaud, et al.
Neuromorphic silicon neuron circuits. Frontiers in neuroscience, 5:73, 2011."
REFERENCES,0.7049180327868853,"Alexander Kugele, Thomas Pfeil, Michael Pfeiffer, and Elisabetta Chicca. Efﬁcient processing of
spatio-temporal data streams with spiking neural networks. Frontiers in Neuroscience, 14:439,
2020. ISSN 1662-453X. doi: 10.3389/fnins.2020.00439. URL https://www.frontiersin.
org/article/10.3389/fnins.2020.00439."
REFERENCES,0.7103825136612022,"Qian Liu, Ole Richter, Carsten Nielsen, Sadique Sheik, Giacomo Indiveri, and Ning Qiao. Live
demonstration: face recognition on an ultra-low power event-driven convolutional neural network
asic. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops, 2019."
REFERENCES,0.7158469945355191,"Carver Mead. Neuromorphic electronic systems. Proceedings of the IEEE, 78(10):1629–1636, 1990."
REFERENCES,0.7213114754098361,"Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp
Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spiking-
neuron integrated circuit with a scalable communication network and interface. Science, 345
(6197):668–673, 2014."
REFERENCES,0.726775956284153,"Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking
neural networks, 2019."
REFERENCES,0.73224043715847,"Nicolas Perez-Nieves, Vincent CH Leung, Pier Luigi Dragotti, and Dan FM Goodman. Neural
heterogeneity promotes robust learning. bioRxiv, pp. 2020–12, 2021."
REFERENCES,0.7377049180327869,"Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conver-
sion of continuous-valued deep networks to efﬁcient event-driven networks for image classiﬁcation.
Frontiers in neuroscience, 11:682, 2017."
REFERENCES,0.7431693989071039,Under review as a conference paper at ICLR 2022
REFERENCES,0.7486338797814208,"Sadique Sheik and Martino Sorbaro. Project title. https://sinabs.ai/, 2013."
REFERENCES,0.7540983606557377,"Sadique Sheik, Martin Coath, Giacomo Indiveri, Susan Denham, Thomas Wennekers, and Elisabetta
Chicca. Emergent auditory feature tuning in a real-time neuromorphic vlsi system. Frontiers
in Neuroscience, 6:17, 2012. ISSN 1662-453X. doi: 10.3389/fnins.2012.00017. URL https:
//www.frontiersin.org/article/10.3389/fnins.2012.00017."
REFERENCES,0.7595628415300546,"Sumit Bam Shrestha and Garrick Orchard.
SLAYER: Spike layer error reassignment
in time.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.),
Advances in Neural Information Processing Systems 31,
pp.
1419–1428. Curran Associates, Inc., 2018.
URL http://papers.nips.cc/paper/
7415-slayer-spike-layer-error-reassignment-in-time.pdf."
REFERENCES,0.7650273224043715,"David Snyder, Guoguo Chen, and Daniel Povey. MUSAN: A Music, Speech, and Noise Corpus,
2015. arXiv:1510.08484v1."
REFERENCES,0.7704918032786885,"Martino Sorbaro, Qian Liu, Massimo Bortone, and Sadique Sheik. Optimizing the energy consump-
tion of spiking neural networks for neuromorphic applications. Frontiers in neuroscience, 14:662,
2020."
REFERENCES,0.7759562841530054,"Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio, 2016."
REFERENCES,0.7814207650273224,"Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint
arXiv:1804.03209, 2018."
REFERENCES,0.7868852459016393,"J Wu, Y Chua, M Zhang, H Li, and KC Tan. A spiking neural network framework for robust sound
classiﬁcation. front. neurosci. 12 (2018), 2018."
REFERENCES,0.7923497267759563,"Jibin Wu, Yansong Chua, Malu Zhang, Guoqi Li, Haizhou Li, and Kay Chen Tan. A tandem learning
rule for effective training and rapid inference of deep spiking neural networks. arXiv e-prints, pp.
arXiv–1907, 2019."
REFERENCES,0.7978142076502732,"Bojian Yin, Federico Corradi, and Sander M Bohté. Effective and efﬁcient computation with multiple-
timescale spiking recurrent neural networks. In International Conference on Neuromorphic Systems
2020, pp. 1–8, 2020."
REFERENCES,0.8032786885245902,Under review as a conference paper at ICLR 2022
REFERENCES,0.8087431693989071,"A
APPENDIX"
REFERENCES,0.8142076502732241,"A.1
NEURON MODEL"
REFERENCES,0.819672131147541,"In this work we used the Leaky Integrate and Fire (LIF) neuron model with synaptic time constant τs
and membrane time constant τv. The sub-threshold dynamics of this neuron are described below."
REFERENCES,0.825136612021858,"˙v(t) = −v(t)/τv + is
(9)
˙is(t) = −is(t)/τs +
X
wjsj(t)
(10)"
REFERENCES,0.8306010928961749,"where v is the membrane potential, is is the synaptic current, w the synaptic weight and s is the input
spike train."
REFERENCES,0.8360655737704918,"In-order to optimize simulation time and computational efﬁciency, we make some alterations. Unlike
a traditional LIF which resets to a resting potential upon reaching threshold θ, we subtract a ﬁxed
value θ from the membrane potential. Furthermore, if the membrane potential increases beyond Nθ,
where N is an arbitrary positive integer, then this neuron produces N spikes, and proportionally Nθ
is subtracted from the membrane potential."
REFERENCES,0.8415300546448088,"s(t) =
[v(t)/θ],
if v(t) ≥θ
0,
otherwise
(11)"
REFERENCES,0.8469945355191257,"The use of a mechanism for generating multiple spikes enables the computation to be more robust
to the choice of simulation time steps. This enables us to choose a relatively large time step for
our simulations. In practice we observe that some neurons occasionally do produce multiple spikes.
(Producing multiple spikes in a single simulation time-step is not necessary if one chooses a small
enough time step.)"
REFERENCES,0.8524590163934426,For further simulation efﬁciency the LIF neurons were simulated using the SRM Gerstner (2001).
REFERENCES,0.8579234972677595,"v(t) =
X"
REFERENCES,0.8633879781420765,"j
wj(ϵ ∗sj)(t) + (ν ∗s)(t)
(12) where"
REFERENCES,0.8688524590163934,"ϵs(t) = e(−t/τs)
(13)"
REFERENCES,0.8743169398907104,"ϵv(t) = e(−t/τv)
(14)"
REFERENCES,0.8797814207650273,"ν(t) = −θ e(−t/τv)
(15)
ϵ(t) = (ϵs ∗ϵv)
(16)"
REFERENCES,0.8852459016393442,"We use exponential kernels for synaptic ϵs(t) and membrane dynamics ϵv(t) and derive the PSP
kernel ϵ(t). The refractory kernel ν(t) is also a negative exponential kernel with the same time
constant τv as the membrane potential. The symbol ∗denotes a convolution operation."
REFERENCES,0.8907103825136612,"As spike generation is non-differentiable, we use a surrogate gradient Neftci et al. (2019). Several
proﬁles for the surrogate gradients have been proposed in literature. We use a modiﬁed exponential
kernel for the surrogate gradient function. In order to accommodate the multi-spike behavior of the
neuron, we choose a periodic exponential function (Figure 5) as the surrogate gradient. This function
peaks as the membrane potential approaches multiples of neuron spiking threshold θ. Intuitively, this
gradient function maximizes the impact of a parameter when the neuron is close to spiking or has just
spiked and is a variant of the exponential gradient function. An extreme simpliﬁcation of the periodic
exponential would be a Heaviside function 2."
REFERENCES,0.8961748633879781,"2The heaviside function like a ReLU has a range of membrane potentials where the gradient is 0 and could
potentially prevent the network from learning at low activity levels."
REFERENCES,0.9016393442622951,Under review as a conference paper at ICLR 2022
REFERENCES,0.907103825136612,"Figure 5: The surrogate gradient of the spiking neuron as a function of the (pre-spike) membrane
potential."
REFERENCES,0.912568306010929,"The simulations were run with the library ‘sinabs’ Sheik & Sorbaro (2013)3 using an adaptation of
SLAYER Shrestha & Orchard (2018)."
REFERENCES,0.9180327868852459,"A.2
SIMULATION PARAMETERS"
REFERENCES,0.9234972677595629,The various parameters used in the experiments described in this article are listed below.
REFERENCES,0.9289617486338798,Table 3: Parameters used for the Aloha simulations (ANN).
REFERENCES,0.9344262295081968,"Parameter name
Value
n_classes
2
n_channels_in
64
n_channels_res
16
n_channels_skip
32
n_hidden
32
dilations
[2, 4, 8, 2, 4, 8, 2, 4, 8, 2, 4, 8]
kernel_size
3
bias
true"
REFERENCES,0.9398907103825137,3GNU AGPL v3 License
REFERENCES,0.9453551912568307,Under review as a conference paper at ICLR 2022
REFERENCES,0.9508196721311475,Table 4: Parameters used for the Aloha simulations (SNN).
REFERENCES,0.9562841530054644,"Parameter name
Value
n_classes
2
n_channels_in
64
n_channels_res
16
n_channels_skip
32
n_hidden
32
dilations
[2, 4, 8, 2, 4, 8, 2, 4, 8, 2, 4, 8]
threshold
1.0
learning_window
0.3
kernel_size
2
bias
true
τv
2
τs
2
weight_scaling (init)
0.5"
REFERENCES,0.9617486338797814,"Table 5: Parameters used for the HeySnips simulations (ANN).
Parameter name
Value
n_classes
2
n_channels_in
64
n_channels_res
16
n_channels_skip
32
n_hidden
32
dilations
[1, 2, 4, 8, 1, 2, 4, 8, 1, 2, 4, 8, 1, 2, 4, 8, 1, 2, 4, 8, 1, 2, 4, 8]
kernel_size
3
bias
true"
REFERENCES,0.9672131147540983,Table 6: Parameters used for the HeySnips simulations (SNN).
REFERENCES,0.9726775956284153,"Parameter name
Value
n_classes
2
n_channels_in
64
n_channels_res
16
n_channels_skip
32
n_hidden
32
dilations
[2, 4, 8, 16, 2, 4, 8, 16]
threshold
1.0
learning_window
0.3
kernel_size
2
bias
true
τv
2
τs
2
weight_scaling (init)
0.5"
REFERENCES,0.9781420765027322,"Table 7: Parameters used for the SpeechCommands simulations (ANN).
Parameter name
Value
n_classes
35
n_channels_in
64
n_channels_res
16
n_channels_skip
32
n_hidden
32
dilations
[1, 2, 4, 8, 1, 2, 4, 8, 1, 2, 4, 8, 1, 2, 4, 8, 1, 2, 4, 8, 1, 2, 4, 8]
kernel_size
3
bias
true"
REFERENCES,0.9836065573770492,Under review as a conference paper at ICLR 2022
REFERENCES,0.9890710382513661,Table 8: Parameters used for the SpeechCommands simulations (SNN).
REFERENCES,0.994535519125683,"Parameter name
Value
n_classes
35
n_channels_in
64
n_channels_res
32
n_channels_skip
64
n_hidden
128
dilations
[2, 4, 8, 16, 2, 4, 8, 16, 2, 4, 8, 16]
threshold
1.0
learning_window
0.3
kernel_size
2
bias
true
τv
2
τs
2
weight_scaling (init)
0.5"
