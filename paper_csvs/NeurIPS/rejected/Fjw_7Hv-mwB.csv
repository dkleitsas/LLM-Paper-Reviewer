Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017482517482517483,"Federated learning (FL) is a distributed learning framework that claims to protect
1"
ABSTRACT,0.0034965034965034965,"user privacy. However, gradient inversion attacks (GIAs) reveal severe privacy
2"
ABSTRACT,0.005244755244755245,"threats to FL, which can recover the users’ training data from outsourced gradients.
3"
ABSTRACT,0.006993006993006993,"Existing defense methods adopt different techniques, e.g., differential privacy,
4"
ABSTRACT,0.008741258741258742,"cryptography, and gradient perturbation, to against the GIAs. Nevertheless, all
5"
ABSTRACT,0.01048951048951049,"current state-of-the-art defense methods suffer from a trade-off between privacy,
6"
ABSTRACT,0.012237762237762238,"utility, and efficiency in FL. To address the weaknesses of existing solutions, we
7"
ABSTRACT,0.013986013986013986,"propose a novel defense method, Aligned Dual Gradient Pruning (ADGP), based
8"
ABSTRACT,0.015734265734265736,"on gradient sparsification, which can improve communication efficiency while
9"
ABSTRACT,0.017482517482517484,"preserving the utility and privacy of the federated training. Specifically, ADGP
10"
ABSTRACT,0.019230769230769232,"slightly changes gradient sparsification with a stronger privacy guarantee. Through
11"
ABSTRACT,0.02097902097902098,"primary gradient parameter selection strategies during training, ADGP can also
12"
ABSTRACT,0.022727272727272728,"significantly improve communication efficiency with a theoretical analysis of its
13"
ABSTRACT,0.024475524475524476,"convergence and generalization. Our extensive experiments show that ADGP can
14"
ABSTRACT,0.026223776223776224,"effectively defend against the most powerful GIAs and significantly reduce the
15"
ABSTRACT,0.027972027972027972,"communication overhead without sacrificing the model’s utility.
16"
INTRODUCTION,0.02972027972027972,"1
Introduction
17"
INTRODUCTION,0.03146853146853147,"Federated learning (FL) [1] is a distributed learning framework, where multiple users train and send
18"
INTRODUCTION,0.033216783216783216,"their gradients of the local models to the server without sharing their local data [1, 2, 3]. FL claims
19"
INTRODUCTION,0.03496503496503497,"to protect the users’ training data since the users do not need to share local data with the server
20"
INTRODUCTION,0.03671328671328671,"directly. However, recent studies reveal that gradients can be used to recover the original training
21"
INTRODUCTION,0.038461538461538464,"data information via gradient inversion attacks (GIAs) [4, 5]. To against GIAs, a large number of
22"
INTRODUCTION,0.04020979020979021,"studies have been proposed, where they leverage the advanced privacy protection techniques, such as
23"
INTRODUCTION,0.04195804195804196,"differential privacy (DP) [6], cryptography [7, 8, 9] and gradient perturbation [10, 11, 12]. However,
24"
INTRODUCTION,0.043706293706293704,"none of the existing defense methods could take care of all privacy, utility, and efficiency difficulties
25"
INTRODUCTION,0.045454545454545456,"in FL. For example, DP and cryptography-based methods could effectively defend GIAs, but sacrifice
26"
INTRODUCTION,0.0472027972027972,"either the utility or efficiency respectively [6, 7, 8, 9]. In order to achieve better utility and efficiency
27"
INTRODUCTION,0.04895104895104895,"in FL, perturbation-based methods design various gradient perturbations [10, 11, 12], but all existing
28"
INTRODUCTION,0.050699300699300696,"perturbation-based methods could only defend one or two kinds of GIAs in practice.
29"
INTRODUCTION,0.05244755244755245,"Fox example, recent perturbation-based defense methods (i.e., Precode [12], Soteria [10], and
30"
INTRODUCTION,0.05419580419580419,"ATS [11]) can effectively defend against optimization-based GIAs [5, 13, 14, 15], but fail to work
31"
INTRODUCTION,0.055944055944055944,"against the active GIAs [16, 17, 18]. On the contrary, the classic Top-k based gradient sparisification
32"
INTRODUCTION,0.057692307692307696,"method [19, 20] is generally considered as a bad privacy protection solution on optimization-based
33"
INTRODUCTION,0.05944055944055944,"GIAs, but in fact performs much better than recent defense methods under the active attack from
34"
INTRODUCTION,0.06118881118881119,"our experiments as shown in Table 2. The new findings inspire us to seek for a more practical
35"
INTRODUCTION,0.06293706293706294,"perturbation-based defense against both optimization-based and active GIAs.
36"
INTRODUCTION,0.06468531468531469,"Table 1: Comparison of our method with existing privacy-preserving FL methods. Note: !represents
the scheme has a high guarantee for the property, while %represents otherwise."
INTRODUCTION,0.06643356643356643,Defense
INTRODUCTION,0.06818181818181818,Privacy
INTRODUCTION,0.06993006993006994,"Utility
Efficiency
Analytical attack
Optimization attack
Active server attack
R-GAP [22]
DLG [4]
IVG [5]
Curious [16]
Rob [21]
Precode [12]
!
!
!
%
%
%
%
ATS [11]
%
!
%
%
%
!
%
Soteria [10]
!
!
!
%
%
!
%
DP [23]
!
!
!
!
!
%
%
Top-k [19]
!
!
%
!
!
!
!
ADGP (Ours)
!
!
!
!
!
!
!"
INTRODUCTION,0.07167832167832168,"In this paper, we propose a new gradient pruning based method, Aligned Dual Gradient Pruning
37"
INTRODUCTION,0.07342657342657342,"(ADGP). Specifically, ADGP consists of two components: dual gradient pruning (DGP) and gradient
38"
INTRODUCTION,0.07517482517482517,"location bounding. Dual gradient pruning is a novel gradient sparsification technique, which removes
39"
INTRODUCTION,0.07692307692307693,"top-k1 largest gradient parameters and the bottom-k2 smallest gradient parameters from the local
40"
INTRODUCTION,0.07867132867132867,"model. DGP leads to a strong privacy protection against both optimization-based and active GIAs.
41"
INTRODUCTION,0.08041958041958042,"To further reduce the expensive download costs caused by the asymmetric gradient selection among
42"
INTRODUCTION,0.08216783216783216,"different users, we propose gradient location bounding strategy to make the aggregated global gradient
43"
INTRODUCTION,0.08391608391608392,"stay in the same sparsified region. In summary, ADGP achieves a better utility and privacy trade-off,
44"
INTRODUCTION,0.08566433566433566,"increases FL system efficiency, and is robust against active attacks.
45"
INTRODUCTION,0.08741258741258741,"Furthermore, we give the theoretical analysis of ADGP, which proves the reconstruction error is
46"
INTRODUCTION,0.08916083916083917,"proportional to gradient distance. So removing larger gradient parameters could enlarge the gradient
47"
INTRODUCTION,0.09090909090909091,"distance resulting in a low reconstruction error. However, removing larger gradients will significantly
48"
INTRODUCTION,0.09265734265734266,"impact the model’s utility. Thus, to improve the sparsification ratio, which is essential to robustness
49"
INTRODUCTION,0.0944055944055944,"against active attack [21, 16], we also remove the model parameters with smaller gradients. In such a
50"
INTRODUCTION,0.09615384615384616,"way, our method could significantly mitigate GIAs without affecting the model’s utility.
51"
INTRODUCTION,0.0979020979020979,"We conduct extensive experiments over multiple datasets and models to evaluate our method. The
52"
INTRODUCTION,0.09965034965034965,"quantitative and visualized results show that our design can effectively make recovered images
53"
INTRODUCTION,0.10139860139860139,"recognizable under different attacks, and reduce nearly half of the communication costs. Our
54"
INTRODUCTION,0.10314685314685315,"contributions are as follows: 1) We revisit gradient sparsification to show its potential on mitigating
55"
INTRODUCTION,0.1048951048951049,"GIA; 2) We propose an improved gradient pruning strategy to provide sufficient privacy guarantee
56"
INTRODUCTION,0.10664335664335664,"while balancing the model accuracy and the system efficiency; 3) We conduct extensive experiments
57"
INTRODUCTION,0.10839160839160839,"to show that our design outperforms perturbation-based defense methods w.r.t privacy protection,
58"
INTRODUCTION,0.11013986013986014,"model accuracy, and system efficiency.
59"
RELATED WORK,0.11188811188811189,"2
Related work
60"
RELATED WORK,0.11363636363636363,"Federated learning [1, 3] is considered to be a privacy-preserving framework for distributed machine
61"
RELATED WORK,0.11538461538461539,"learning as the training data is not directly outsourced. However, the emerging of GIAs [4, 5, 21, 16,
62"
RELATED WORK,0.11713286713286714,"22, 24, 25, 26, 27] shatters this conception. It has been proved that the attacker (e.g., a curious server)
63"
RELATED WORK,0.11888111888111888,"can easily recover the private training data from gradient information to a great extent. The privacy
64"
RELATED WORK,0.12062937062937062,"guarantee of federated learning urgently needs to be strengthened.
65"
RELATED WORK,0.12237762237762238,"Cryptographic Defense.
Traditionally, there are two approaches to construct privacy-preserving
66"
RELATED WORK,0.12412587412587413,"federated learning: using DP to disturb gradients [6, 23, 28, 29, 30] or using cryptographic tools to
67"
RELATED WORK,0.1258741258741259,"perform secure aggregation [7, 8, 9, 31, 32]. DP [6] is a popular and effective privacy protection
68"
RELATED WORK,0.12762237762237763,"mechanism by adding random noise to the raw data, but it is well known that the noises intro-
69"
RELATED WORK,0.12937062937062938,"duced by DP can greatly degrade the model accuracy when meaningful privacy is enforced [33].
70"
RELATED WORK,0.13111888111888112,"Cryptographic-based secure aggregation can guarantee both privacy and accuracy simultaneously, but
71"
RELATED WORK,0.13286713286713286,"incurs expensive computation and communication costs [34]. Using the shuffle model [35, 36] can
72"
RELATED WORK,0.1346153846153846,"only provide anonymity. Moreover, it totally changes the system model of FL since an additional
73"
RELATED WORK,0.13636363636363635,"semi-trusted third party is introduced to work cooperatively with the server.
74"
RELATED WORK,0.1381118881118881,"Gradient Perturbation Defense.
Recently, researchers have begun to explore the possibility of
75"
RELATED WORK,0.13986013986013987,"constructing new gradient perturbation mechanisms to better balancing privacy and accuracy. Sun et
76"
RELATED WORK,0.14160839160839161,"al. [10] proposed Soteria, a scheme that perturbs the representation of training data by pruning the
77"
RELATED WORK,0.14335664335664336,"gradients of a single fully connected layer. Gao et al. [11] proposed ATS, a training data augmentation
78"
RELATED WORK,0.1451048951048951,"policy by transforming original sensitive images into alternative inputs, to reduce the visibility of
79"
RELATED WORK,0.14685314685314685,"reconstructed images. Scheliga et al. [12] presented Precode to extend the model architecture by using
80"
RELATED WORK,0.1486013986013986,"variational bottleneck (VB) [37] to prevent attackers from obtaining optimal solutions to reconstructed
81"
RELATED WORK,0.15034965034965034,"data. These defenses work well against GIAs in the semi-honest setting [4, 5, 38, 13], but fail to
82"
RELATED WORK,0.1520979020979021,"protect privacy when an active server modifies the model to launch GIAs [21, 16]. Moreover, these
83"
RELATED WORK,0.15384615384615385,"works suffer from high computation costs or huge communication burden.
84"
RELATED WORK,0.1555944055944056,"Gradient Sparsification Defense.
From an independent research domain, gradient sparsification
85"
RELATED WORK,0.15734265734265734,"has been commonly used for saving communication bandwidth. The most common sparsification
86"
RELATED WORK,0.1590909090909091,"strategy is Top-k selection, which selects top k gradient parameters with the largest absolute values
87"
RELATED WORK,0.16083916083916083,"[19, 20]. It has been widely proved that gradient sparsification provides very limited privacy protection
88"
RELATED WORK,0.16258741258741258,"ability [4, 10, 11, 12, 39] unless a high pruning ratio (e.g., removing 99% of the gradients) is used
89"
RELATED WORK,0.16433566433566432,"at the cost of 10% accuracy drop [39]. However, we emphasize that this is misunderstood as they
90"
RELATED WORK,0.1660839160839161,"only consider the Top-k sparsification strategy that has never received an in-depth investigation in the
91"
RELATED WORK,0.16783216783216784,"field of security. It is originally designed for improving system efficiency, thus a direct application
92"
RELATED WORK,0.16958041958041958,"inherently suffers from many weaknesses. As shown in Section 4, a slight modification can unleash
93"
RELATED WORK,0.17132867132867133,"the potential of gradient sparsification to provide a strong privacy guarantee.
94"
THREAT MODEL AND ATTACKS,0.17307692307692307,"3
Threat Model and Attacks
95"
THREAT MODEL AND ATTACKS,0.17482517482517482,"In this work, we consider a strong threat scenario, where an active server, after receiving gradients
96"
THREAT MODEL AND ATTACKS,0.17657342657342656,"from users, tries to reconstruct the local training data and is motivated to modify model parameters
97"
THREAT MODEL AND ATTACKS,0.17832167832167833,"in each iteration to strengthen the attack performance. Note that the server also wants to obtain a
98"
THREAT MODEL AND ATTACKS,0.18006993006993008,"high-quality global model with high accuracy. More specifically, we consider the following three
99"
THREAT MODEL AND ATTACKS,0.18181818181818182,"kinds of GIAs:
100"
THREAT MODEL AND ATTACKS,0.18356643356643357,"Analytical attack. Analytical attack exploits the structure of the gradients to recover the input,
101"
THREAT MODEL AND ATTACKS,0.1853146853146853,"such as using gradient bias terms [40]. Recently proposed R-GAP attack [22] exploits the recursive
102"
THREAT MODEL AND ATTACKS,0.18706293706293706,"relationship between gradient layers to solve the input. An effective analytical attack depends on the
103"
THREAT MODEL AND ATTACKS,0.1888111888111888,"specific structure and parameters of gradients.
104"
THREAT MODEL AND ATTACKS,0.19055944055944055,"Optimization attack. Optimization attack is first proposed using L-BFGS optimizer to solve
105"
THREAT MODEL AND ATTACKS,0.19230769230769232,"min || ∂l(x,y)"
THREAT MODEL AND ATTACKS,0.19405594405594406,"∂W
−∂l(x∗,y∗)"
THREAT MODEL AND ATTACKS,0.1958041958041958,"∂W
||2
2 and gets dummy data x∗and dummy label y∗, where y is the label of
106"
THREAT MODEL AND ATTACKS,0.19755244755244755,"x [4]. The state-of-the-art optimization attack method IVG [5] uses Adam to optimize the cosine
107"
THREAT MODEL AND ATTACKS,0.1993006993006993,"distance and has been widely used to evaluate defense works [10, 11, 12].
108"
THREAT MODEL AND ATTACKS,0.20104895104895104,"Despite different optimizers can be used to achieve better attack quality [5, 13, 14, 15], the existing
109"
THREAT MODEL AND ATTACKS,0.20279720279720279,"attacks are all measured by the distance between the generated gradients ∇W∗and the original
110"
THREAT MODEL AND ATTACKS,0.20454545454545456,"gradients ∇W. We therefore propose a general definition for optimization attack to better evaluate
111"
THREAT MODEL AND ATTACKS,0.2062937062937063,"its performance. As shown in Definition 1, a smaller ε indicates a stronger optimization attack.
112"
THREAT MODEL AND ATTACKS,0.20804195804195805,"Definition 1. An optimization attack is a (ε, δ)-attack, if it satisfies:
113"
THREAT MODEL AND ATTACKS,0.2097902097902098,"P(E(D(∇W,∇W∗)) ≤ε) ≥1 −δ.
(1)"
THREAT MODEL AND ATTACKS,0.21153846153846154,"where P represents the probability, E represents the expectation, D is the distance function commonly
114"
THREAT MODEL AND ATTACKS,0.21328671328671328,"instantiated with Euclidean or cosine distance.
115"
THREAT MODEL AND ATTACKS,0.21503496503496503,"Active server attack.
In this kind of attack, the server can actively modify the global model to
116"
THREAT MODEL AND ATTACKS,0.21678321678321677,"realize a better attack result rather than honestly executing the protocols [16, 17, 18]. Recently
117"
THREAT MODEL AND ATTACKS,0.21853146853146854,"proposed Rob attack [21] adds imprint modules to the model and uses the difference between the
118"
THREAT MODEL AND ATTACKS,0.2202797202797203,"gradient parameters in adjacent rows of the imprint module to recover the data, achieving the best
119"
THREAT MODEL AND ATTACKS,0.22202797202797203,"attack effect in the literature.
120"
ALIGNED DUAL GRADIENT PRUNING,0.22377622377622378,"4
Aligned Dual Gradient Pruning
121"
ANALYSIS OF GRADIENT SPARSIFICATION,0.22552447552447552,"4.1
Analysis of Gradient Sparsification
122"
ANALYSIS OF GRADIENT SPARSIFICATION,0.22727272727272727,"We owe the failure of common Top-k gradient sparsification methods to two reasons: 1) the distance
123"
ANALYSIS OF GRADIENT SPARSIFICATION,0.229020979020979,"between the Top-k sparsified (i.e., perturbed) gradient g and the real gradient ∇W is small; and 2)
124"
ANALYSIS OF GRADIENT SPARSIFICATION,0.23076923076923078,"large gradient parameters in ∇W also reveal label information about user data.
125"
ANALYSIS OF GRADIENT SPARSIFICATION,0.23251748251748253,"To explain the first reason, we investigate the relationship between the reconstruction error of user
126"
ANALYSIS OF GRADIENT SPARSIFICATION,0.23426573426573427,"data and distance of perturbed gradient v.s. real gradient, as shown in Proposition 1.
127"
ANALYSIS OF GRADIENT SPARSIFICATION,0.23601398601398602,"Proposition 1. For any given input x and shared model W, the distance between the recovered data
128"
ANALYSIS OF GRADIENT SPARSIFICATION,0.23776223776223776,"x′ and the real data x is bounded by:
129"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2395104895104895,"||x −x′||2 ≥
||∇W −g||2
||∂φ(x, W)/∂x||2
,
(2)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.24125874125874125,"where φ is the mapping from x to ∇W, i.e., the reconstruction quality is limited by ||∇W −g||2.
130"
ANALYSIS OF GRADIENT SPARSIFICATION,0.243006993006993,"The proof of the above proposition is moved to the supplementary due to space limit (the same
131"
ANALYSIS OF GRADIENT SPARSIFICATION,0.24475524475524477,"hereinafter). From this Proposition, it is clear that the reconstruction error is proportional to the
132"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2465034965034965,"gradient distance ||∇W −g||2, i.e., effective defense methods should enlarge the gradient distance
133"
ANALYSIS OF GRADIENT SPARSIFICATION,0.24825174825174826,"as much as possible. However, for the Top-k based gradient sparsification [19, 20], the k largest
134"
ANALYSIS OF GRADIENT SPARSIFICATION,0.25,"parameters are retained, making the gradient distance small by nature.
135"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2517482517482518,"To explain the second reason, we consider a L-layer perceptron model trained with cross-entropy
136"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2534965034965035,"loss for classification. Let a column vector r = [r1, r2, ..., rn] be the logits (the output of the
137"
ANALYSIS OF GRADIENT SPARSIFICATION,0.25524475524475526,"L-th linear layer) that input to the softmax layer, the confidence score probability vector is thus
138
h
er1
P"
ANALYSIS OF GRADIENT SPARSIFICATION,0.256993006993007,"j erj ,
er2
P"
ANALYSIS OF GRADIENT SPARSIFICATION,0.25874125874125875,"j erj , · · · ,
ern
P"
ANALYSIS OF GRADIENT SPARSIFICATION,0.26048951048951047,"j erj
i
and the succinct form of the cross-entropy loss becomes ℓ(x, y) =
139"
ANALYSIS OF GRADIENT SPARSIFICATION,0.26223776223776224,"−log(
ery
P"
ANALYSIS OF GRADIENT SPARSIFICATION,0.263986013986014,"j erj ). Focus on the L-th layer WLx + bL = r, it is easy to find
140"
ANALYSIS OF GRADIENT SPARSIFICATION,0.26573426573426573,"∂ℓ(x, y)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2674825174825175,"∂bi
= ∂ℓ(x, y)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2692307692307692,"∂ri
· ∂ri"
ANALYSIS OF GRADIENT SPARSIFICATION,0.270979020979021,"∂bi
= ∂ℓ(x, y)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2727272727272727,"∂ri
=
eri
P"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2744755244755245,"j erj −Ii=y,
(3)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2762237762237762,"and
141"
ANALYSIS OF GRADIENT SPARSIFICATION,0.27797202797202797,"∇WL = ∂ℓ(x, y)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.27972027972027974,"∂r
· xT = [∂ℓ(x, y)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.28146853146853146,"∂r1
, · · · , ∂ℓ(x, y)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.28321678321678323,"∂rn
] · xT .
(4)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.28496503496503495,"For a given x (and so xT is fixed), the magnitude of certain elements of the gradient matrix ∇WL
142"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2867132867132867,"(i.e., the i-th row) is particularly large if i is the true label of the training data x due to reason that
143"
ANALYSIS OF GRADIENT SPARSIFICATION,0.28846153846153844,"| ∂ℓ(x,y)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2902097902097902,"∂ri
| = P"
ANALYSIS OF GRADIENT SPARSIFICATION,0.291958041958042,"j̸=i | ∂ℓ(x,y)"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2937062937062937,"∂rj
|.
144"
ANALYSIS OF GRADIENT SPARSIFICATION,0.29545454545454547,"To summarize, due to the above two reasons, we conclude that common Top-k gradient sparsification
145"
ANALYSIS OF GRADIENT SPARSIFICATION,0.2972027972027972,"cannot provide sufficient protection for user data against passive optimization attacks. From another
146"
ANALYSIS OF GRADIENT SPARSIFICATION,0.29895104895104896,"point of view, a sufficient gradient sparsification ratio also plays an important role in defending against
147"
ANALYSIS OF GRADIENT SPARSIFICATION,0.3006993006993007,"active server attacks. As mentioned in Section 3, active attackers can exploit the correspondence of
148"
ANALYSIS OF GRADIENT SPARSIFICATION,0.30244755244755245,"partial gradient parameters to recover the real data. So, the gradient sparsity rate will directly destroy
149"
ANALYSIS OF GRADIENT SPARSIFICATION,0.3041958041958042,"the relationship among gradient parameters constructed by the active attacker. Intuitively, the higher
150"
ANALYSIS OF GRADIENT SPARSIFICATION,0.30594405594405594,"the sparsity rate, the more severe the impact. As will be validated in Section 6, a higher sparsity rate
151"
ANALYSIS OF GRADIENT SPARSIFICATION,0.3076923076923077,"can prevent the attacker from obtaining useful gradient information.
152"
DUAL GRADIENT PRUNING,0.3094405594405594,"4.2
Dual Gradient Pruning
153"
DUAL GRADIENT PRUNING,0.3111888111888112,"Generally speaking, large gradient parameters of local model need to be removed to make the
154"
DUAL GRADIENT PRUNING,0.3129370629370629,"gradient difference larger, but the difference should also be appropriately bounded to maintain high
155"
DUAL GRADIENT PRUNING,0.3146853146853147,"model accuracy. Moreover, it is also necessary to delete small gradient parameters to achieve a
156"
DUAL GRADIENT PRUNING,0.31643356643356646,"high sparsification ratio. With these observations, we propose dual gradients pruning (DGP), a new
157"
DUAL GRADIENT PRUNING,0.3181818181818182,"parameter selection strategy for gradient sparsification.
158"
DUAL GRADIENT PRUNING,0.31993006993006995,"The users first sort the absolute values of all Size(∇W) local gradient parameters in the descending
159"
DUAL GRADIENT PRUNING,0.32167832167832167,"order. Let Tk1(∇W) represent the set of top-k1 elements of ∇W, Bk2(∇W) represent the set of
160"
DUAL GRADIENT PRUNING,0.32342657342657344,"its bottom-k2 elements. Then the users remove Tk1(∇W) and Bk2(∇W) from ∇W for gradient
161"
DUAL GRADIENT PRUNING,0.32517482517482516,"sparsification. Note that we set p = k2/k1 as a hyperparameter to regulate the trade-off between
162"
DUAL GRADIENT PRUNING,0.3269230769230769,"privacy and accuracy. Clearly, even with a fixed value p, different user will have different sets of
163"
DUAL GRADIENT PRUNING,0.32867132867132864,"Tk1(·) and Bk2(·) because their respective local models could be different from each other.
164"
DUAL GRADIENT PRUNING,0.3304195804195804,"We emphasize that although such dual gradients pruning strategy is very simple, it can significantly
165"
DUAL GRADIENT PRUNING,0.3321678321678322,"mitigate GIAs without affecting the model accuracy. A rigorous security proof is shown in Section 5,
166"
DUAL GRADIENT PRUNING,0.3339160839160839,"and experimental results can be found in Section 6.
167"
DUAL GRADIENT PRUNING,0.3356643356643357,"Algorithm 1: Aligned Dual Gradient Pruning (ADGP)
Input
:Original gradient matrix ∇W, location binary matrix I, values of k1 and k
Output :Sparsified gradient matrix g =

g1; · · · ; gL"
DUAL GRADIENT PRUNING,0.3374125874125874,for l ←1 to L do
DUAL GRADIENT PRUNING,0.33916083916083917,Remove parameters in Tk1(∇Wl) from ∇Wl
DUAL GRADIENT PRUNING,0.3409090909090909,"Keep parameters in ∇Wl when location is in I, and discard all other parameters
Upload gl = Tk(∇Wl) to the server"
DUAL GRADIENT PRUNING,0.34265734265734266,Algorithm 2: A Complete Illustration of our Defense
DUAL GRADIENT PRUNING,0.34440559440559443,"Input
:Initial global model W0, value k and k1, total rounds T, total users N
Output :Shared global model WT
Set e0 = 0
for t ←0 to T −1 do"
DUAL GRADIENT PRUNING,0.34615384615384615,"Randomly select a user to broadcast the location matrix It of its parameter set T2k
for i ←1 to N do"
DUAL GRADIENT PRUNING,0.3479020979020979,"Pt
i = ∇Wt
i + et
i
gt
i = ADGP(k1, k, Pt
i, It)
et+1
i
= Pt
i −gt
i
Sever side aggregation:
Wt+1 = Wt −η"
DUAL GRADIENT PRUNING,0.34965034965034963,"PN
i=1 gt
i
N"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3513986013986014,"4.3
A Complete Illustration of Our Method
168"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3531468531468531,"Although dual gradient pruning provides a sufficient privacy guarantee as well as reduces upload cost
169"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3548951048951049,"of users, users’ download costs could still be expensive. This is because different users have different
170"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.35664335664335667,"sets of Tk1(·) and Bk2(·) when sparsifying their own local gradients, which will ultimately make the
171"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3583916083916084,"global gradient become dense after aggregation.
172"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.36013986013986016,"We thus propose aligned dual gradient pruning (ADGP), an improved scheme to align the selected
173"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3618881118881119,"gradients across different users. Similar to DGP, for best privacy, each user will still firstly identify
174"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.36363636363636365,"his top-k1 gradients location set Tk1. Different from DGP, ADGP also wants to save users’ download
175"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.36538461538461536,"cost by ensuring that all users’ uploaded sparsified gradients reside in the same location set. This is
176"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.36713286713286714,"achieved by randomly selecting a user, who identifies a top-2k (k1 < k) location set T2k (represented
177"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3688811188811189,"with a binary matrix I) and broadcasts I to all other users. Note that Tk1 ⊂T2k is not necessary
178"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3706293706293706,"true. Upon receiving I, each user first discards gradient parameters in Tk1 and then only transmits
179"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3723776223776224,"the k largest gradient parameters whose locations belong to I. After aggregation at the server side,
180"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3741258741258741,"users only need to download global gradients’ parameters associated with I. A detailed illustration
181"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3758741258741259,"of ADGP is shown in Algorithm 1.
182"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3776223776223776,"For ADGP pruning, in each FL iteration round, all gradient parameters whose locations are outside of
183"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3793706293706294,"I will not participate the current round global model aggregation. In the extreme case, I can remain
184"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3811188811188811,"static for all iteration rounds and the local accumulated error (accumulated unused local gradient
185"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.38286713286713286,"parameters) becomes large, thus hindering global model convergence. To reduce this negative impact
186"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.38461538461538464,"and increase convergence speed, we design an error feedback mechanism. In particular, at the iteration
187"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.38636363636363635,"round t, after user i obtaining his local gradient ∇Wt
i, he will combine ∇Wt
i with an error term
188"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.3881118881118881,"accumulated in the previous (t −1) rounds before performing the ADGP sparsification pruning. A
189"
A COMPLETE ILLUSTRATION OF OUR METHOD,0.38986013986013984,"complete illustration of our method is shown in Algorithm 2.
190"
THEORETICAL ANALYSIS,0.3916083916083916,"5
Theoretical Analysis
191"
THEORETICAL ANALYSIS,0.39335664335664333,"This section presents the security analysis with regard to passive attacks (i.e., analytical and opti-
192"
THEORETICAL ANALYSIS,0.3951048951048951,"mization attacks presented in Section 3), as well as the generalization and convergence analyses
193"
THEORETICAL ANALYSIS,0.3968531468531469,"of the proposed ADGP algorithm. Following the literature studies in [41, 42], for a given L-layer
194"
THEORETICAL ANALYSIS,0.3986013986013986,"centralized model, we model the first (L −1) layers as a robust feature extractor of any input sample.
195"
THEORETICAL ANALYSIS,0.40034965034965037,"Thus, the function of this model is characterized by f(x|W) = Wx + b, and the optimization
196"
THEORETICAL ANALYSIS,0.4020979020979021,"objective is the loss ℓ(x, y) (such as cross-entropy or L2 loss). To facilitate analyses and following
197"
THEORETICAL ANALYSIS,0.40384615384615385,"literature studies [19, 41, 43, 44], the assumptions about the smoothness of DGP, ADGP and l, as
198"
THEORETICAL ANALYSIS,0.40559440559440557,"well as the variance of the stochastic gradient are employed.
199"
THEORETICAL ANALYSIS,0.40734265734265734,"Assumption 1. The pruning mechanisms DGP(k1, k2, ∇Wt) and ADGP(k1, k, ∇Wt, It) are both
bi-Lipschitz, so the following conditions hold:"
THEORETICAL ANALYSIS,0.4090909090909091,"||∇W −DGP(k1, k2, ∇W)||2
2 = ||DGP(0, 0, ∇W) −DGP(k1, k2, ∇W)||2
2 ≥γ1||∇W||2
2,
||∇W −ADGP(k1, k, ∇Wt, It)||2
2 ≤γ2||∇W||2
2,"
THEORETICAL ANALYSIS,0.41083916083916083,"where γ1 is a constant determined by k1 and k2, and γ2 is a constant determined by k1 and k.
200"
THEORETICAL ANALYSIS,0.4125874125874126,"Assumption 2. The objective function l : Rd →R has a low bound l∗and it is Lipschitz-smooth, i.e.,
201"
THEORETICAL ANALYSIS,0.4143356643356643,"for any x1, x2, ||∇l(x1) −∇l(x2)||2 ≤K||x1 −x2||2 and l(x1) ≤l(x2) + ⟨∇l(x2), x1 −x2⟩+
202 K"
THEORETICAL ANALYSIS,0.4160839160839161,"2 ||x1 −x2||2
2.
203"
THEORETICAL ANALYSIS,0.4178321678321678,"Assumption 3. The full gradient ∇l(Wt) is bounded, i.e., ||∇l(Wt)||2
2 ≤G2, and the federated
204"
THEORETICAL ANALYSIS,0.4195804195804196,"stochastic gradient ∇Wt
i (i = [1, N]) is the unbiased estimation of the full gradient ∇l(Wt), i.e.,
205"
THEORETICAL ANALYSIS,0.42132867132867136,"E(∇Wt
i) = ∇l(Wt). Moreover, the variance between ∇Wt
i and ∇l(Wt) is bounded: E||∇Wt
i −
206"
THEORETICAL ANALYSIS,0.4230769230769231,"∇l(Wt)||2
2 ≤σ2.
207"
THEORETICAL ANALYSIS,0.42482517482517484,"Security Analysis. It is noted that, for the same sparsification ratio, user’s uploaded gradient
208"
THEORETICAL ANALYSIS,0.42657342657342656,"parameters from ADGP is generally smaller than that from DGP. Indeed, the uploaded gradient
209"
THEORETICAL ANALYSIS,0.42832167832167833,"parameters from both methods are the same only when Tk1 ⊂T2k holds. From this observation and
210"
THEORETICAL ANALYSIS,0.43006993006993005,"referring to Proposition 1, DGP is the security lower bound of our design for privacy protection.
211"
THEORETICAL ANALYSIS,0.4318181818181818,"So, our focus is the security analysis of DGP. As shown in the theorem below, we prove that DGP
212"
THEORETICAL ANALYSIS,0.43356643356643354,"achieves a stronger privacy protection in the sense of Definition 1.
213"
THEORETICAL ANALYSIS,0.4353146853146853,"Theorem 1. For any (ε, δ) optimization attack, under the presence of DGP, it will be degenerated
214"
THEORETICAL ANALYSIS,0.4370629370629371,"to (ε + √γ2||∇W||2, δ)-attack if D is measured by Euclidean distance, and degenerated to (1 −
215
√γ1(1 −ε), δ)-attack if D is measured by cosine distance.
216"
THEORETICAL ANALYSIS,0.4388111888111888,"The Theorem is based on Assumption 1 about DGP. It reveals that, with the same successful chance
217"
THEORETICAL ANALYSIS,0.4405594405594406,"1 −δ, DGP weakens the attacker’s capability to optimize a better estimation of the true ∇W.
218"
THEORETICAL ANALYSIS,0.4423076923076923,"Generalization and Convergence Analyses. The generalization analysis aims to quantify how the
219"
THEORETICAL ANALYSIS,0.44405594405594406,"trained model performs on the test data, and it is achieved by analyzing the how ADGP affects the
220"
THEORETICAL ANALYSIS,0.4458041958041958,"properties of the optima reached (without gradient pruning) [41, 42]. Assisted with Assumption 3
221"
THEORETICAL ANALYSIS,0.44755244755244755,"and Assumption 1 about ADGP gradient pruning, the following Lemma can be obatined.
222"
THEORETICAL ANALYSIS,0.4493006993006993,"Lemma 1. Let et = PN
i=1 et
i/N be the averaged accumulated error among all users at iteration t,
223"
THEORETICAL ANALYSIS,0.45104895104895104,"the expectation of the norm of et is bounded, i.e.,
224"
THEORETICAL ANALYSIS,0.4527972027972028,"E||et||2
2 ≤γ2"
THEORETICAL ANALYSIS,0.45454545454545453,2 (2 + γ2
THEORETICAL ANALYSIS,0.4562937062937063,"1 −γ2
)2(G2 + σ2).
(5)"
THEORETICAL ANALYSIS,0.458041958041958,"Note that the difference between the averaged pruned gradient gt = PN
i=1 gt
i/N and the averaged
225"
THEORETICAL ANALYSIS,0.4597902097902098,"Fed-SGD gradient ∇Wt = PN
i=1 ∇Wt
i/N is simply || PT −1
i=0 (∇Wt −gt)||2
2 = ||eT ||2
2. So
226"
THEORETICAL ANALYSIS,0.46153846153846156,"the lemma above indicates that the accumulated gradient difference between our algorithm and
227"
THEORETICAL ANALYSIS,0.4632867132867133,"Fed-SGD is bounded. That said, the optima reached by ADGP and the optima reached by Fed-
228"
THEORETICAL ANALYSIS,0.46503496503496505,"SGD will eventually be the same if the algorithm converge. Armed with Lemma 1 and based on
229"
THEORETICAL ANALYSIS,0.46678321678321677,"Assumptions 1, 2 and 3, we demonstrate the convergence of the our algorithm.
230"
THEORETICAL ANALYSIS,0.46853146853146854,"Theorem 2. The averaged norm of the full gradient ∇l(Wt) derived from centralized training is
231"
THEORETICAL ANALYSIS,0.47027972027972026,"correlated with the our algorithm as follows:
232"
THEORETICAL ANALYSIS,0.47202797202797203,"PT −1
t=0 E||∇l(Wt)||2
2
T
≤4l0 −l∗"
THEORETICAL ANALYSIS,0.4737762237762238,"ηT
+ 4η2K2 γ2"
THEORETICAL ANALYSIS,0.4755244755244755,2 (2 + γ2
THEORETICAL ANALYSIS,0.4772727272727273,"1 −γ2
)2(G2 + σ2) + 2Kη(G2 + σ2"
THEORETICAL ANALYSIS,0.479020979020979,"N ),
(6)"
THEORETICAL ANALYSIS,0.4807692307692308,"where l0 is the initialization of the objective l, and η is the learning rate.
233"
THEORETICAL ANALYSIS,0.4825174825174825,"The immediate implication of Theorem 2 is that, with an appropriate learning rate η, ADGP converges
234"
THEORETICAL ANALYSIS,0.48426573426573427,similar to Fed-SGD (slower by a negligible term O( 1
THEORETICAL ANALYSIS,0.486013986013986,"T ), as shown in Corollary 1.
235"
THEORETICAL ANALYSIS,0.48776223776223776,Table 2: Evaluation of defense performance under three attacks.
THEORETICAL ANALYSIS,0.48951048951048953,"Attack
Metric
Baseline
Precode
DP
Soteria
ATS-I
ATS-II
Top-k
Ours IVG"
THEORETICAL ANALYSIS,0.49125874125874125,"PSNR (↓)
34.8805
9.6441
6.9554
9.2447
16.6894
31.3200
14.1338
7.6192
LPIPS(↑)
0.0016
0.4473
0.5504
0.3774
0.1621
0.0015
0.2754
0.4829
SSIM (↓)
0.9273
0.4793
0.2451
0.4173
0.6851
0.9189
0.5336
0.2923 R-GAP"
THEORETICAL ANALYSIS,0.493006993006993,"PSNR (↓)
36.7656
-
5.0691
5.1817
10.8442
42.0900
5.1017
5.1196
LPIPS(↑)
0.0007
-
0.3621
0.3532
0.2094
1.8e-05
0.4817
0.4863
SSIM (↓)
0.9307
-
0.2483
0.2124
0.3962
0.9121
0.2027
0.1928 Rob"
THEORETICAL ANALYSIS,0.49475524475524474,"PSNR (↓)
102.8838
109.6553
8.7491
102.8838
9.6166
115.9886
13.0685
13.0804
LPIPS(↑)
0.0960
0.1488
1.3434
0.0960
0.6410
0.0486
0.8920
0.9184
SSIM (↓)
0.8969
0.8440
0.2064
0.8969
0.2545
0.9490
0.0428
0.0229
Final model accuracy
93.4400
93.1699
86.8900
93.2300
93.3900
93.3900
93.2099
93.1700"
THEORETICAL ANALYSIS,0.4965034965034965,"Original
Baseline
ATS
Soteria
Top‐k
Ours
DP"
THEORETICAL ANALYSIS,0.4982517482517482,"(a) R-GAP, CNN6"
THEORETICAL ANALYSIS,0.5,"Original
Baseline
Soteria
Precode
ATS
Top‐k
Ours
DP"
THEORETICAL ANALYSIS,0.5017482517482518,"(b) IVG, LeNet (Zhu)"
THEORETICAL ANALYSIS,0.5034965034965035,Figure 1: Visualization of the reconstructed data under R-GAP and IVG attacks.
THEORETICAL ANALYSIS,0.5052447552447552,"Corollary 1. Let η =
q"
THEORETICAL ANALYSIS,0.506993006993007,"l0−l∗
KT (G2+σ2/N), we have
236"
THEORETICAL ANALYSIS,0.5087412587412588,"PT −1
t=0 E||∇l(Wt)||2
2
T
≤6 s"
THEORETICAL ANALYSIS,0.5104895104895105,l0 −l∗
THEORETICAL ANALYSIS,0.5122377622377622,KT(G2 + σ2/N) + O( 1
THEORETICAL ANALYSIS,0.513986013986014,"T ).
(7)"
THEORETICAL ANALYSIS,0.5157342657342657,"6
Experiments: Privacy-Accuracy-Efficiency Tradeoff
237"
EXPERIMENTAL SETUP,0.5174825174825175,"6.1
Experimental Setup
238"
EXPERIMENTAL SETUP,0.5192307692307693,"Datasets and models.
We conduct experiments on CIFAR10 with LeNet (Zhu) [5], CIFAR10 [45]
239"
EXPERIMENTAL SETUP,0.5209790209790209,"with CNN6 and CIFAR100 [45] with LeNet (Zhu) and ResNet18 respectively. We run these experi-
240"
EXPERIMENTAL SETUP,0.5227272727272727,"ments in a pytorch environment by using a single RTX 2080 Ti GPU and 2.10GHz CPU.
241"
EXPERIMENTAL SETUP,0.5244755244755245,"Evaluation Metrics.
We quantify the privacy effect of defenses, follow [39, 46], we visualize the
242"
EXPERIMENTAL SETUP,0.5262237762237763,"reconstructed data and use learned perceptual image patch similarity (LPIPS), peak signal-to-noise
243"
EXPERIMENTAL SETUP,0.527972027972028,"ratio (PSNR), structural similarity (SSIM) to measure the quality of the recovered data. A better
244"
EXPERIMENTAL SETUP,0.5297202797202797,"defense scheme should has a larger LPIPS (↑), smaller peak signal-to-noise ratio (PSNR) (↓) and
245"
EXPERIMENTAL SETUP,0.5314685314685315,"structural similarity (SSIM) (↓).
246"
EXPERIMENTAL SETUP,0.5332167832167832,"Attack methods.
We evaluate our defense against IVG attack [5], R-GAP attack [22], and Rob
247"
EXPERIMENTAL SETUP,0.534965034965035,"attack [21], which represent three kinds of state-of-the-art GIAs, as illustrated in Section 3. All
248"
EXPERIMENTAL SETUP,0.5367132867132867,"these attacks are implemented strictly following the original settings, i.e., IVG is evaluated on
249"
EXPERIMENTAL SETUP,0.5384615384615384,"CIFAR10 with LeNet (Zhu), R-GAP is evaluated on CIFAR10 with CNN6, Rob attack is evaluated
250"
EXPERIMENTAL SETUP,0.5402097902097902,"on CIFAR100 with LeNet (Zhu). More settings for attacks are shown in the supplementary.
251"
EXPERIMENTAL SETUP,0.541958041958042,"Defense methods.
We compare our method with five state-of-the-art defenses: Soteria [10],
252"
EXPERIMENTAL SETUP,0.5437062937062938,"ATS [11], Precode [12], Differential Privacy (DP) [2], and Top-k based gradient sparsification1 [19].
253"
EXPERIMENTAL SETUP,0.5454545454545454,"Besides, we set Fed-SGD [3] as the baseline that adopts no defenses. Following the DP design
254"
EXPERIMENTAL SETUP,0.5472027972027972,"in [2], we use the Gaussian differential privacy mechanism with ε = 10.7, δ = 10−5, which is the
255"
EXPERIMENTAL SETUP,0.548951048951049,"suggested best setting for the privacy-accuracy trade-off and can make most models converge. When
256"
EXPERIMENTAL SETUP,0.5506993006993007,"quantifying the defense performance of ATS, we not only evaluate the similarity between the raw
257"
EXPERIMENTAL SETUP,0.5524475524475524,"images and the recovered data (ATS-I), but also evaluate the similarity between the disturbed training
258"
EXPERIMENTAL SETUP,0.5541958041958042,"1Hereinafter, we abuse the notion of k to denote the send rate (k/Size(∇W)) × 100% since it will not cause
ambiguity. And the sparse ratio is 1-k. The smaller the ratio k is, the better communication efficiency."
EXPERIMENTAL SETUP,0.5559440559440559,Original Top‐k DGP
EXPERIMENTAL SETUP,0.5576923076923077,(a) Recovered images under IVG attack with ResNet18 on CIFAR100
EXPERIMENTAL SETUP,0.5594405594405595,"0
20
40
60
80
100
Epoch 0 20 40 60 80 100"
EXPERIMENTAL SETUP,0.5611888111888111,Acc (%)
EXPERIMENTAL SETUP,0.5629370629370629,"Baseline: CIFAR10
Top-k: CIFAR10
DGP: CIFAR10
Baseline: CIFAR100
Top-k: CIFAR100
DGP: CIFAR100"
EXPERIMENTAL SETUP,0.5646853146853147,(b) ResNet18 on CIFAR dataset
EXPERIMENTAL SETUP,0.5664335664335665,Figure 2: A detailed comparison between Top-k and our DGP on privacy and model accuracy.
EXPERIMENTAL SETUP,0.5681818181818182,"Original
Baseline
Soteria
Precode
Top-k
ATS
Ours
DP"
EXPERIMENTAL SETUP,0.5699300699300699,Figure 3: Visualization of reconstructed images under Rob attack with batchsize=16.
EXPERIMENTAL SETUP,0.5716783216783217,"images (i.e., the real inputs) and the recovered data (ATS-II). We set the send rate k = 0.2 and the
259"
EXPERIMENTAL SETUP,0.5734265734265734,"regulation hyperparameter p = 15. The supplementary gives more experiments under different p and
260"
EXPERIMENTAL SETUP,0.5751748251748252,"k. The rest defense schemes remain the original settings.
261"
DEFENSE PERFORMANCE EVALUATION,0.5769230769230769,"6.2
Defense Performance Evaluation
262"
DEFENSE PERFORMANCE EVALUATION,0.5786713286713286,"Table 2 shows the defense performance with PSNR, SSIM, and LPIPS under three attacks. The
263"
DEFENSE PERFORMANCE EVALUATION,0.5804195804195804,"results show that ATS, Soteria, Precode perform poorly under Rob attack, while Top-k is vulnerable
264"
DEFENSE PERFORMANCE EVALUATION,0.5821678321678322,"to IVG attack although it performs better under Rob attack. In most cases, our scheme performs
265"
DEFENSE PERFORMANCE EVALUATION,0.583916083916084,"comparably with DP and outperforms all the other defenses. More evaluation results under Rob
266"
DEFENSE PERFORMANCE EVALUATION,0.5856643356643356,"attack are presented in our supplementary.
267"
DEFENSE PERFORMANCE EVALUATION,0.5874125874125874,"We also visualize the reconstructed images in order to perceptually demonstrate the defense per-
268"
DEFENSE PERFORMANCE EVALUATION,0.5891608391608392,"formance. Figure 1(a) shows the the recovered images against R-GAP and IVG attacks. We can
269"
DEFENSE PERFORMANCE EVALUATION,0.5909090909090909,"see that all the existing defenses can well defend against R-GAP attack except ATS because it does
270"
DEFENSE PERFORMANCE EVALUATION,0.5926573426573427,"not damage the gradient structure, proving that a slight perturbation on gradients can mitigate the
271"
DEFENSE PERFORMANCE EVALUATION,0.5944055944055944,"analytical attacks easily. We are not able to provide the result of Precode because its VB operation
272"
DEFENSE PERFORMANCE EVALUATION,0.5961538461538461,"destroys the model structure thus analytical attack R-GAP cannot be implemented. In Figure 1(b),
273"
DEFENSE PERFORMANCE EVALUATION,0.5979020979020979,"recovered images under IVG attack are presented. We can find that the attacker can still recover the
274"
DEFENSE PERFORMANCE EVALUATION,0.5996503496503497,"outline of inputs with ATS and Top-k defenses. DP, Soteria, Precode, and our scheme can still make
275"
DEFENSE PERFORMANCE EVALUATION,0.6013986013986014,"the recovered images unrecognizable. Figure 3 evaluates the defenses against Rob attack. It shows
276"
DEFENSE PERFORMANCE EVALUATION,0.6031468531468531,"that ATS, Soteria, and Precode fail to work and most inputs can be reconstructed.
277"
DEFENSE PERFORMANCE EVALUATION,0.6048951048951049,"In Rob attack, the attacker uses the gradient of the imprint module to reconstruct the training data.
278"
DEFENSE PERFORMANCE EVALUATION,0.6066433566433567,"Our method, Top-k, and DP can effectively defend against Rob attack because the gradients of
279"
DEFENSE PERFORMANCE EVALUATION,0.6083916083916084,"all layers are sparsed or perturbed, including those of the malicious imprint modules. However,
280"
DEFENSE PERFORMANCE EVALUATION,0.6101398601398601,"we emphasize that the main weakness of Top-k is its vulnerability to optimization attacks (e.g.,
281"
DEFENSE PERFORMANCE EVALUATION,0.6118881118881119,"IVG), as widely demonstrated in the literature [4, 10, 11, 39, 12]. We thus further evaluate Top-k
282"
DEFENSE PERFORMANCE EVALUATION,0.6136363636363636,"and our scheme under IVG attack with ResNet18 on CIFAR datasets. We set k1/Size(∇W) =
283"
DEFENSE PERFORMANCE EVALUATION,0.6153846153846154,"0.05, k2/Size(∇W) = 0.75.
284"
MODEL ACCURACY EVALUATION,0.6171328671328671,"6.3
Model Accuracy Evaluation
285"
MODEL ACCURACY EVALUATION,0.6188811188811189,"To evaluate model performance, we train ResNet18, LeNet (Zhu), VGG13_bn [47] on CIFAR10,
286"
MODEL ACCURACY EVALUATION,0.6206293706293706,"CIFAR100 with ten users, respectively. We set epoch=100, the learning rate η=0.1 if epoch ≤50,
287"
MODEL ACCURACY EVALUATION,0.6223776223776224,"0
20
40
60
80
100
Epoch 0 20 40 60 80 100"
MODEL ACCURACY EVALUATION,0.6241258741258742,Acc (%)
MODEL ACCURACY EVALUATION,0.6258741258741258,"Baseline
Top-k
DP
Ours"
MODEL ACCURACY EVALUATION,0.6276223776223776,"(a) CIFAR10, ResNet18"
MODEL ACCURACY EVALUATION,0.6293706293706294,"0
20
40
60
80
100
Epoch 0 20 40 60"
MODEL ACCURACY EVALUATION,0.6311188811188811,Acc (%)
MODEL ACCURACY EVALUATION,0.6328671328671329,"Baseline
Top-k
DP
Ours"
MODEL ACCURACY EVALUATION,0.6346153846153846,"(b) CIFAR10, LeNet (Zhu)"
MODEL ACCURACY EVALUATION,0.6363636363636364,"0
20
40
60
80
100
Epoch 0 20 40 60 80"
MODEL ACCURACY EVALUATION,0.6381118881118881,Acc (%)
MODEL ACCURACY EVALUATION,0.6398601398601399,"Baseline
Top-k
DP
Ours"
MODEL ACCURACY EVALUATION,0.6416083916083916,"(c) CIFAR10, VGG13_bn"
MODEL ACCURACY EVALUATION,0.6433566433566433,"0
20
40
60
80
100
Epoch 0 20 40 60 80"
MODEL ACCURACY EVALUATION,0.6451048951048951,Acc (%)
MODEL ACCURACY EVALUATION,0.6468531468531469,"Baseline
Top-k
DP
Ours"
MODEL ACCURACY EVALUATION,0.6486013986013986,"(d) CIFAR100, ResNet18"
MODEL ACCURACY EVALUATION,0.6503496503496503,"0
20
40
60
80
100
Epoch 0 10 20 30"
MODEL ACCURACY EVALUATION,0.6520979020979021,Acc (%)
MODEL ACCURACY EVALUATION,0.6538461538461539,"Baseline
Top-k
DP
Ours"
MODEL ACCURACY EVALUATION,0.6555944055944056,"(e) CIFAR100, LeNet (Zhu)"
MODEL ACCURACY EVALUATION,0.6573426573426573,"0
20
40
60
80
100
Epoch 0 20 40 60 80"
MODEL ACCURACY EVALUATION,0.6590909090909091,Acc (%)
MODEL ACCURACY EVALUATION,0.6608391608391608,"Baseline
Top-k
DP
Ours"
MODEL ACCURACY EVALUATION,0.6625874125874126,"(f) CIFAR100, VGG13_bn"
MODEL ACCURACY EVALUATION,0.6643356643356644,Figure 4: Evaluation of model accuracy with different datasets and model architectures.
MODEL ACCURACY EVALUATION,0.666083916083916,Table 3: Commu. cost in one iteration (MB)
MODEL ACCURACY EVALUATION,0.6678321678321678,"Method
Baseline
Soteria
Precode
ATS
DP
Top-k
Ours
Resnet18
85.2506
85.2268
88.1644
85.2506
85.2506
43.7979
27.3067
VGG13
71.8385
71.7318
74.8433
71.8385
71.8385
34.9625
22.9697
LeNet
0.1207
0.0764
1.8624
0.1207
0.1207
0.0493
0.0424"
MODEL ACCURACY EVALUATION,0.6695804195804196,"η=0.01 if epoch >50, and η=0.05 if epoch >70. We show in Table 2 the accuracy of ResNet18 over
288"
MODEL ACCURACY EVALUATION,0.6713286713286714,"CIFAR10 under different defenses, and here we only compare our scheme with the baseline Fed-SGD,
289"
MODEL ACCURACY EVALUATION,0.6730769230769231,"DP, and Top-k since they perform best for privacy protection. Because [41] showed that the error
290"
MODEL ACCURACY EVALUATION,0.6748251748251748,"feedback is beneficial to improve the model accuracy, even without using gradient sparsification. To
291"
MODEL ACCURACY EVALUATION,0.6765734265734266,"give a fair comparison, we set the error feedback mechanism as the basic setting for all the defenses.
292"
MODEL ACCURACY EVALUATION,0.6783216783216783,"The experimental results in Figure 4 show that we achieve similar model performance with the
293"
MODEL ACCURACY EVALUATION,0.6800699300699301,"baseline, while DP, as expected, significantly damage the model accuracy.
294"
EFFICIENCY EVALUATION,0.6818181818181818,"6.4
Efficiency Evaluation
295"
EFFICIENCY EVALUATION,0.6835664335664335,"To clearly demonstrate the system efficiency, we evaluate the communication cost, which is obtained
296"
EFFICIENCY EVALUATION,0.6853146853146853,"by computing the total overheads of sending updated gradients and receiving aggregated gradients.
297"
EFFICIENCY EVALUATION,0.6870629370629371,"For ease of presentation, we only show the results for one iteration. As shown in Table 3, our scheme
298"
EFFICIENCY EVALUATION,0.6888111888111889,"reduces more than half of the communication costs compared with existing defenses, and our gradient
299"
EFFICIENCY EVALUATION,0.6905594405594405,"sparsification incurs negligible computation burden. The specific computation cost evaluation is
300"
EFFICIENCY EVALUATION,0.6923076923076923,"presented in the supplementary.
301"
EFFICIENCY EVALUATION,0.6940559440559441,"7
Conclusions, Limitations, and Broader Impact
302"
EFFICIENCY EVALUATION,0.6958041958041958,"Our work firstly reveals the risks of privacy-preserving methods that only perturb the gradients of
303"
EFFICIENCY EVALUATION,0.6975524475524476,"some layers. Through a comprehensive analysis of gradient inversion attacks, we show that it is
304"
EFFICIENCY EVALUATION,0.6993006993006993,"necessary to perturb or sparse the gradients of each layer for privacy preservation. And considering
305"
EFFICIENCY EVALUATION,0.701048951048951,"the challenge of high communication cost in federated learning, we propose aligned dual gradient
306"
EFFICIENCY EVALUATION,0.7027972027972028,"sparsification method to achieve the trade-off between privacy protection, model performance, and
307"
EFFICIENCY EVALUATION,0.7045454545454546,"efficient communication, and give sufficient theoretical support. We hope that our newly proposed
308"
EFFICIENCY EVALUATION,0.7062937062937062,"gradient sparisification method can shed new light on addressing privacy leakage concern as well as
309"
EFFICIENCY EVALUATION,0.708041958041958,"saving communication bandwidth.
310"
EFFICIENCY EVALUATION,0.7097902097902098,"In terms of limitations, the success of our scheme relies on selecting a reliable user to broadcast its
311"
EFFICIENCY EVALUATION,0.7115384615384616,"gradient locations. Randomly selecting users may encounter malicious users that destroy the entire
312"
EFFICIENCY EVALUATION,0.7132867132867133,"system. Our design is delegated to protecting privacy and has no negative societal impacts in practice.
313"
REFERENCES,0.715034965034965,"References
314"
REFERENCES,0.7167832167832168,"[1] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient
315"
REFERENCES,0.7185314685314685,"learning of deep networks from decentralized data,” in Proceedings of the 20th International
316"
REFERENCES,0.7202797202797203,"Conference on Artificial Intelligence and Statistics (AISTATS’17), 2017, pp. 1273–1282.
317"
REFERENCES,0.722027972027972,"[2] M. Naseri, J. Hayes, and E. De Cristofaro, “Local and central differential privacy for robustness
318"
REFERENCES,0.7237762237762237,"and privacy in federated learning,” in Proceedings of the 29th Network and Distributed System
319"
REFERENCES,0.7255244755244755,"Security Symposium (NDSS’22), 2022.
320"
REFERENCES,0.7272727272727273,"[3] Q. Li, Z. Wen, Z. Wu, S. Hu, N. Wang, Y. Li, X. Liu, and B. He, “A survey on federated
321"
REFERENCES,0.7290209790209791,"learning systems: vision, hype and reality for data privacy and protection,” IEEE Transactions
322"
REFERENCES,0.7307692307692307,"on Knowledge and Data Engineering, 2021.
323"
REFERENCES,0.7325174825174825,"[4] L. Zhu, Z. Liu, and S. Han, “Deep leakage from gradients,” in Advances in Neural Information
324"
REFERENCES,0.7342657342657343,"Processing Systems (NeurIPS’19), 2019, pp. 14 747–14 756.
325"
REFERENCES,0.736013986013986,"[5] J. Geiping, H. Bauermeister, H. Dröge, and M. Moeller, “Inverting gradients-how easy is it to
326"
REFERENCES,0.7377622377622378,"break privacy in federated learning?” in Advances in Neural Information Processing Systems
327"
REFERENCES,0.7395104895104895,"(NeurIPS’20), 2020, pp. 16 937–16 947.
328"
REFERENCES,0.7412587412587412,"[6] C. Dwork, A. Roth et al., “The algorithmic foundations of differential privacy,” Found. Trends
329"
REFERENCES,0.743006993006993,"Theor. Comput. Sci., vol. 9, no. 3-4, pp. 211–407, 2014.
330"
REFERENCES,0.7447552447552448,"[7] R. Gilad-Bachrach, K. Laine, K. Lauter, P. Rindal, and M. Rosulek, “Secure data exchange:
331"
REFERENCES,0.7465034965034965,"A marketplace in the cloud,” in Proceedings of the 2019 ACM SIGSAC Conference on Cloud
332"
REFERENCES,0.7482517482517482,"Computing Security Workshop (CCSW’19), 2019, pp. 117–128.
333"
REFERENCES,0.75,"[8] S. Hardy, W. Henecka, H. Ivey-Law, R. Nock, G. Patrini, G. Smith, and B. Thorne, “Private fed-
334"
REFERENCES,0.7517482517482518,"erated learning on vertically partitioned data via entity resolution and additively homomorphic
335"
REFERENCES,0.7534965034965035,"encryption,” arXiv preprint arXiv:1711.10677, 2017.
336"
REFERENCES,0.7552447552447552,"[9] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage,
337"
REFERENCES,0.756993006993007,"A. Segal, and K. Seth, “Practical secure aggregation for privacy-preserving machine learning,” in
338"
REFERENCES,0.7587412587412588,"Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security
339"
REFERENCES,0.7604895104895105,"(CCS’17), 2017, pp. 1175–1191.
340"
REFERENCES,0.7622377622377622,"[10] J. Sun, A. Li, B. Wang, H. Yang, H. Li, and Y. Chen, “Soteria: Provable defense against privacy
341"
REFERENCES,0.763986013986014,"leakage in federated learning from representation perspective,” in Proceedings of the 2021
342"
REFERENCES,0.7657342657342657,"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR’21), 2021, pp.
343"
REFERENCES,0.7674825174825175,"9311–9319.
344"
REFERENCES,0.7692307692307693,"[11] W. Gao, S. Guo, T. Zhang, H. Qiu, Y. Wen, and Y. Liu, “Privacy-preserving collaborative learn-
345"
REFERENCES,0.7709790209790209,"ing with automatic transformation search,” in Proceedings of the 2021 IEEE/CVF Conference
346"
REFERENCES,0.7727272727272727,"on Computer Vision and Pattern Recognition (CVPR’21), 2021, pp. 114–123.
347"
REFERENCES,0.7744755244755245,"[12] D. Scheliga, P. Mäder, and M. Seeland, “Precode-a generic model extension to prevent deep
348"
REFERENCES,0.7762237762237763,"gradient leakage,” in Proceedings of the 2022 IEEE/CVF Winter Conference on Applications of
349"
REFERENCES,0.777972027972028,"Computer Vision (WACV’22), 2022, pp. 1849–1858.
350"
REFERENCES,0.7797202797202797,"[13] W. Wei, L. Liu, M. Loper, K.-H. Chow, M. E. Gursoy, S. Truex, and Y. Wu, “A framework for
351"
REFERENCES,0.7814685314685315,"evaluating gradient leakage attacks in federated learning,” arXiv preprint arXiv:2004.10397,
352"
REFERENCES,0.7832167832167832,"2020.
353"
REFERENCES,0.784965034965035,"[14] Y. Wang, J. Deng, D. Guo, C. Wang, X. Meng, H. Liu, C. Ding, and S. Rajasekaran, “Sapag: A
354"
REFERENCES,0.7867132867132867,"self-adaptive privacy attack from gradients,” arXiv preprint arXiv:2009.06228, 2020.
355"
REFERENCES,0.7884615384615384,"[15] M. Balunovi´c, D. I. Dimitrov, R. Staab, and M. Vechev, “Bayesian framework for gradient
356"
REFERENCES,0.7902097902097902,"leakage,” arXiv preprint arXiv:2111.04706, 2021.
357"
REFERENCES,0.791958041958042,"[16] F. Boenisch, A. Dziedzic, R. Schuster, A. S. Shamsabadi, I. Shumailov, and N. Paper-
358"
REFERENCES,0.7937062937062938,"not, “When the curious abandon honesty: Federated learning is not private,” arXiv preprint
359"
REFERENCES,0.7954545454545454,"arXiv:2112.02918, 2021.
360"
REFERENCES,0.7972027972027972,"[17] X. Pan, M. Zhang, Y. Yan, J. Zhu, and M. Yang, “Exploring the security boundary of data
361"
REFERENCES,0.798951048951049,"reconstruction via neuron exclusivity analysis.”
362"
REFERENCES,0.8006993006993007,"[18] Y. Wen, J. Geiping, L. Fowl, M. Goldblum, and T. Goldstein, “Fishing for user data in large-
363"
REFERENCES,0.8024475524475524,"batch federated learning via gradient magnification,” arXiv preprint arXiv:2202.00580, 2022.
364"
REFERENCES,0.8041958041958042,"[19] D. Alistarh, T. Hoefler, M. Johansson, N. Konstantinov, S. Khirirat, and C. Renggli, “The
365"
REFERENCES,0.8059440559440559,"convergence of sparsified gradient methods,” in Advances in Neural Information Processing
366"
REFERENCES,0.8076923076923077,"Systems (NeurIPS’18), 2018, pp. 5977–5987.
367"
REFERENCES,0.8094405594405595,"[20] Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally, “Deep gradient compression: Reducing the
368"
REFERENCES,0.8111888111888111,"communication bandwidth for distributed training,” arXiv preprint arXiv:1712.01887, 2017.
369"
REFERENCES,0.8129370629370629,"[21] L. Fowl, J. Geiping, W. Czaja, M. Goldblum, and T. Goldstein, “Robbing the fed: Directly obtain-
370"
REFERENCES,0.8146853146853147,"ing private data in federated learning with modified models,” arXiv preprint arXiv:2110.13057,
371"
REFERENCES,0.8164335664335665,"2021.
372"
REFERENCES,0.8181818181818182,"[22] J. Zhu and M. Blaschko, “R-gap: Recursive gradient attack on privacy,” in Proceedings of the
373"
REFERENCES,0.8199300699300699,"9th International Conference on Learning Representations (ICLR’21), 2021.
374"
REFERENCES,0.8216783216783217,"[23] R. C. Geyer, T. Klein, and M. Nabi, “Differentially private federated learning: A client level
375"
REFERENCES,0.8234265734265734,"perspective,” arXiv preprint arXiv:1712.07557, 2017.
376"
REFERENCES,0.8251748251748252,"[24] B. Zhao, K. R. Mopuri, and H. Bilen, “idlg: Improved deep leakage from gradients,” arXiv
377"
REFERENCES,0.8269230769230769,"preprint arXiv:2001.02610, 2020.
378"
REFERENCES,0.8286713286713286,"[25] J. Qian and L. K. Hansen, “What can we learn from gradients?”
arXiv preprint
379"
REFERENCES,0.8304195804195804,"arXiv:2010.15718, 2020.
380"
REFERENCES,0.8321678321678322,"[26] H. Yin, A. Mallya, A. Vahdat, J. M. Alvarez, J. Kautz, and P. Molchanov, “See through gradients:
381"
REFERENCES,0.833916083916084,"Image batch recovery via gradinversion,” in Proceedings of the 2021 IEEE/CVF Conference on
382"
REFERENCES,0.8356643356643356,"Computer Vision and Pattern Recognition (CVPR’21), 2021, pp. 16 337–16 346.
383"
REFERENCES,0.8374125874125874,"[27] L. Fan, K. W. Ng, C. Ju, T. Zhang, C. Liu, C. S. Chan, and Q. Yang, “Rethinking privacy
384"
REFERENCES,0.8391608391608392,"preserving deep learning: How to evaluate and thwart privacy attacks,” in Federated Learning.
385"
REFERENCES,0.8409090909090909,"Springer, 2020, vol. 12500, pp. 32–50.
386"
REFERENCES,0.8426573426573427,"[28] X. Chen, Z. S. Wu, and M. Hong, “Understanding gradient clipping in private sgd: A geometric
387"
REFERENCES,0.8444055944055944,"perspective,” in Advances in Neural Information Processing Systems (NeurIPS’20), 2020, pp.
388"
REFERENCES,0.8461538461538461,"13 773–13 782.
389"
REFERENCES,0.8479020979020979,"[29] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, “Deep
390"
REFERENCES,0.8496503496503497,"learning with differential privacy,” in Proceedings of the 2016 ACM SIGSAC conference on
391"
REFERENCES,0.8513986013986014,"computer and communications security (CCS’16), 2016, pp. 308–318.
392"
REFERENCES,0.8531468531468531,"[30] L. Yu, L. Liu, C. Pu, M. E. Gursoy, and S. Truex, “Differentially private model publishing for
393"
REFERENCES,0.8548951048951049,"deep learning,” in Proceedings of the 2019 IEEE Symposium on Security and Privacy (SP’19),
394"
REFERENCES,0.8566433566433567,"2019, pp. 332–349.
395"
REFERENCES,0.8583916083916084,"[31] P. Mohassel and Y. Zhang, “Secureml: A system for scalable privacy-preserving machine
396"
REFERENCES,0.8601398601398601,"learning,” in Proceedings of the 2017 IEEE Symposium on Security and Privacy (SP’17), 2017,
397"
REFERENCES,0.8618881118881119,"pp. 19–38.
398"
REFERENCES,0.8636363636363636,"[32] G. Danner and M. Jelasity, “Fully distributed privacy preserving mini-batch gradient descent
399"
REFERENCES,0.8653846153846154,"learning,” in Proceedings of the 15th International conference on distributed applications and
400"
REFERENCES,0.8671328671328671,"interoperable systems (IFIP’15), 2015, pp. 30–44.
401"
REFERENCES,0.8688811188811189,"[33] K. Wei, J. Li, M. Ding, C. Ma, H. H. Yang, F. Farokhi, S. Jin, T. Q. Quek, and H. V. Poor,
402"
REFERENCES,0.8706293706293706,"“Federated learning with differential privacy: Algorithms and performance analysis,” IEEE
403"
REFERENCES,0.8723776223776224,"Transactions on Information Forensics and Security, vol. 15, pp. 3454–3469, 2020.
404"
REFERENCES,0.8741258741258742,"[34] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz,
405"
REFERENCES,0.8758741258741258,"Z. Charles, G. Cormode, R. Cummings et al., “Advances and open problems in federated
406"
REFERENCES,0.8776223776223776,"learning,” Foundations and Trends® in Machine Learning, vol. 14, no. 1–2, pp. 1–210, 2021.
407"
REFERENCES,0.8793706293706294,"[35] R. Liu, Y. Cao, H. Chen, R. Guo, and M. Yoshikawa, “Flame: Differentially private federated
408"
REFERENCES,0.8811188811188811,"learning in the shuffle model,” arXiv preprint arXiv:2009.08063, 2020.
409"
REFERENCES,0.8828671328671329,"[36] L. Sun, J. Qian, and X. Chen, “Ldp-fl: Practical private aggregation in federated learning with
410"
REFERENCES,0.8846153846153846,"local differential privacy,” in Proceedings of the Thirtieth International Joint Conference on
411"
REFERENCES,0.8863636363636364,"Artificial Intelligence (IJCAI’21), 2021, pp. 1571–1578.
412"
REFERENCES,0.8881118881118881,"[37] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, “Deep variational information bottleneck,”
413"
REFERENCES,0.8898601398601399,"arXiv preprint arXiv:1612.00410, 2016.
414"
REFERENCES,0.8916083916083916,"[38] Z. Wang, M. Song, Z. Zhang, Y. Song, Q. Wang, and H. Qi, “Beyond inferring class repre-
415"
REFERENCES,0.8933566433566433,"sentatives: User-level privacy leakage from federated learning,” in Proceedings of 2019 IEEE
416"
REFERENCES,0.8951048951048951,"Conference on Computer Communications (INFOCOM’19), 2019, pp. 2512–2520.
417"
REFERENCES,0.8968531468531469,"[39] Y. Huang, S. Gupta, Z. Song, K. Li, and S. Arora, “Evaluating gradient inversion attacks
418"
REFERENCES,0.8986013986013986,"and defenses in federated learning,” in Advances in Neural Information Processing Systems
419"
REFERENCES,0.9003496503496503,"(NeurIPS’21), 2021, pp. 7232–7241.
420"
REFERENCES,0.9020979020979021,"[40] L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai, “Privacy-preserving deep learning:
421"
REFERENCES,0.9038461538461539,"Revisited and enhanced,” in Proceedings of the 2017 International Conference on Applications
422"
REFERENCES,0.9055944055944056,"and Techniques in Information Security (ATIS’17), 2017, pp. 100–110.
423"
REFERENCES,0.9073426573426573,"[41] S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi, “Error feedback fixes signsgd and
424"
REFERENCES,0.9090909090909091,"other gradient compression schemes,” in Proceedings of the 36th International Conference on
425"
REFERENCES,0.9108391608391608,"Machine Learning (ICML’19), 2019, pp. 3252–3261.
426"
REFERENCES,0.9125874125874126,"[42] A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht, “The marginal value of adaptive
427"
REFERENCES,0.9143356643356644,"gradient methods in machine learning,” in Advances in Neural Information Processing Systems
428"
REFERENCES,0.916083916083916,"(NeurIPS’17), 2017, pp. 4148–4158.
429"
REFERENCES,0.9178321678321678,"[43] C.-Y. Chen, J. Ni, S. Lu, X. Cui, P.-Y. Chen, X. Sun, N. Wang, S. Venkataramani, V. V.
430"
REFERENCES,0.9195804195804196,"Srinivasan, W. Zhang et al., “Scalecom:
Scalable sparsified gradient compression for
431"
REFERENCES,0.9213286713286714,"communication-efficient distributed training,” in Advances in Neural Information Process-
432"
REFERENCES,0.9230769230769231,"ing Systems (NeurIPS’20), 2020, pp. 13 551–13 563.
433"
REFERENCES,0.9248251748251748,"[44] X. Dai, X. Yan, K. Zhou, H. Yang, K. K. Ng, J. Cheng, and Y. Fan, “Hyper-sphere quantization:
434"
REFERENCES,0.9265734265734266,"Communication-efficient sgd for federated learning,” arXiv preprint arXiv:1911.04655, 2019.
435"
REFERENCES,0.9283216783216783,"[45] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features from tiny images,” 2009.
436"
REFERENCES,0.9300699300699301,"[46] J. Jeon, K. Lee, S. Oh, J. Ok et al., “Gradient inversion with generative image prior,” in Advances
437"
REFERENCES,0.9318181818181818,"in Neural Information Processing Systems (NeurIPS’21), 2021, pp. 29 898–29 908.
438"
REFERENCES,0.9335664335664335,"[47] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image
439"
REFERENCES,0.9353146853146853,"recognition,” arXiv preprint arXiv:1409.1556, 2014.
440"
REFERENCES,0.9370629370629371,"Checklist
441"
REFERENCES,0.9388111888111889,"1. For all authors...
442"
REFERENCES,0.9405594405594405,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
443"
REFERENCES,0.9423076923076923,"contributions and scope? [Yes]
444"
REFERENCES,0.9440559440559441,"(b) Did you describe the limitations of your work? [Yes]
445"
REFERENCES,0.9458041958041958,"(c) Did you discuss any potential negative societal impacts of your work? [Yes]
446"
REFERENCES,0.9475524475524476,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
447"
REFERENCES,0.9493006993006993,"them? [Yes]
448"
REFERENCES,0.951048951048951,"2. If you are including theoretical results...
449"
REFERENCES,0.9527972027972028,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
450"
REFERENCES,0.9545454545454546,"(b) Did you include complete proofs of all theoretical results? [Yes]
451"
REFERENCES,0.9562937062937062,"3. If you ran experiments...
452"
REFERENCES,0.958041958041958,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
453"
REFERENCES,0.9597902097902098,"mental results (either in the supplemental material or as a URL)? [Yes]
454"
REFERENCES,0.9615384615384616,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
455"
REFERENCES,0.9632867132867133,"were chosen)? [Yes] See Sec. 6
456"
REFERENCES,0.965034965034965,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
457"
REFERENCES,0.9667832167832168,"ments multiple times)? [Yes] see supplementary
458"
REFERENCES,0.9685314685314685,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
459"
REFERENCES,0.9702797202797203,"of GPUs, internal cluster, or cloud provider)? [Yes]
460"
REFERENCES,0.972027972027972,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
461"
REFERENCES,0.9737762237762237,"(a) If your work uses existing assets, did you cite the creators? [Yes]
462"
REFERENCES,0.9755244755244755,"(b) Did you mention the license of the assets? [No]
463"
REFERENCES,0.9772727272727273,"(c) Did you include any new assets either in the supplemental material or as a URL? [No]
464"
REFERENCES,0.9790209790209791,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
465"
REFERENCES,0.9807692307692307,"using/curating? [Yes] all the license of the data, basically the ones used for research
466"
REFERENCES,0.9825174825174825,"are openly accessible
467"
REFERENCES,0.9842657342657343,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
468"
REFERENCES,0.986013986013986,"information or offensive content? [No]
469"
REFERENCES,0.9877622377622378,"5. If you used crowdsourcing or conducted research with human subjects...
470"
REFERENCES,0.9895104895104895,"(a) Did you include the full text of instructions given to participants and screenshots, if
471"
REFERENCES,0.9912587412587412,"applicable? [N/A]
472"
REFERENCES,0.993006993006993,"(b) Did you describe any potential participant risks, with links to Institutional Review
473"
REFERENCES,0.9947552447552448,"Board (IRB) approvals, if applicable? [N/A]
474"
REFERENCES,0.9965034965034965,"(c) Did you include the estimated hourly wage paid to participants and the total amount
475"
REFERENCES,0.9982517482517482,"spent on participant compensation? [N/A]
476"
