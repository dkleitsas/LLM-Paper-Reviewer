Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024154589371980675,"Denoising diffusion probabilistic models have recently received much research at-
tention since they outperform alternative approaches, such as GANs, and currently
provide state-of-the-art generative performance. The superior performance of dif-
fusion models has made them an appealing tool in several applications, including
inpainting, super-resolution, and semantic editing. In this paper, we demonstrate
that diffusion models can also serve as an instrument for semantic segmentation,
especially in the setup when labeled data is scarce. In particular, for several pre-
trained diffusion models, we investigate the intermediate activations from the net-
works that perform the Markov step of the reverse diffusion process. We show that
these activations effectively capture the semantic information from an input image
and appear to be excellent pixel-level representations for the segmentation prob-
lem. Based on these observations, we describe a simple segmentation method,
which can work even if only a few training images are provided. Our approach
significantly outperforms the existing alternatives on several datasets for the same
amount of human supervision. The source code of the project is publicly available."
INTRODUCTION,0.004830917874396135,"1
INTRODUCTION"
INTRODUCTION,0.007246376811594203,"Denoising diffusion probabilistic models (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020) have
recently outperformed alternative approaches to model the distribution of natural images both in the
realism of individual samples and their diversity (Dhariwal & Nichol, 2021). These advantages of
DDPM are successfully exploited in applications, such as colorization (Song et al., 2021), inpainting
(Song et al., 2021), super-resolution (Saharia et al., 2021; Li et al., 2021b), and semantic editing
(Meng et al., 2021), where DDPM often achieve more impressive results compared to GANs."
INTRODUCTION,0.00966183574879227,"So far, however, DDPM were not exploited as a source of effective image representations for dis-
criminative computer vision problems. While the prior literature has demonstrated that various gen-
erative paradigms, such as GANs (Donahue & Simonyan, 2019) or autoregressive models (Chen
et al., 2020a), can be used to extract the representations for common vision tasks, it is not clear if
DDPM can also serve as representation learners. In this paper, we provide an affirmative answer to
this question in the context of semantic segmentation."
INTRODUCTION,0.012077294685990338,"In particular, we investigate the intermediate activations from the U-Net network that approximates
the Markov step of the reverse diffusion process in DDPM. Intuitively, this network learns to denoise
its input, and it is not clear why the intermediate activations should capture semantic information
needed for high-level vision problems. Nevertheless, we show that on certain diffusion steps, these
activations do capture such information, and therefore, can potentially be used as image representa-
tions for downstream tasks. Given these observations, we propose a simple semantic segmentation
method, which exploits these representations and works successfully even if only a few labeled
images are provided. On several datasets, we show that our DDPM-based segmentation method
outperforms the existing baselines for the same amount of supervision."
INTRODUCTION,0.014492753623188406,"To sum up, the contributions of our paper are:"
WE INVESTIGATE THE REPRESENTATIONS LEARNED BY THE STATE-OF-THE-ART DDPM AND SHOW THAT THEY,0.016908212560386472,"1. We investigate the representations learned by the state-of-the-art DDPM and show that they
capture high-level semantic information valuable for downstream vision tasks."
WE INVESTIGATE THE REPRESENTATIONS LEARNED BY THE STATE-OF-THE-ART DDPM AND SHOW THAT THEY,0.01932367149758454,Published as a conference paper at ICLR 2022
WE DESIGN A SIMPLE SEMANTIC SEGMENTATION APPROACH THAT EXPLOITS THESE REPRESENTATIONS AND,0.021739130434782608,"2. We design a simple semantic segmentation approach that exploits these representations and
outperforms the alternatives in the few-shot operating point."
WE COMPARE THE DDPM-BASED REPRESENTATIONS WITH THEIR GAN-BASED COUNTERPARTS ON THE SAME,0.024154589371980676,"3. We compare the DDPM-based representations with their GAN-based counterparts on the same
datasets and demonstrate the advantages of the former in the context of semantic segmentation."
RELATED WORK,0.026570048309178744,"2
RELATED WORK"
RELATED WORK,0.028985507246376812,"In this section, we briefly describe the existing lines of research relevant to our work."
RELATED WORK,0.03140096618357488,"Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are a class of generative models that
approximate the distribution of real images by the endpoint of the Markov chain which originates
from a simple parametric distribution, typically a standard Gaussian. Each Markov step is modeled
by a deep neural network that effectively learns to invert the diffusion process with a known Gaus-
sian kernel. Ho et al. highlighted the equivalence of diffusion models and score matching (Song &
Ermon, 2019; 2020), showing them to be two different perspectives on the gradual conversion of a
simple known distribution into a target distribution via the iterative denoising process. Very recent
works (Nichol, 2021; Dhariwal & Nichol, 2021) have developed more powerful model architectures
as well as different advanced objectives, which led to the “victory” of DDPM over GANs in terms
of generative quality and diversity. DDPM have been widely used in several applications, including
image colorization (Song et al., 2021), super-resolution (Saharia et al., 2021; Li et al., 2021b), in-
painting (Song et al., 2021), and semantic editing (Meng et al., 2021). In our work, we demonstrate
that one can also successfully use them for semantic segmentation."
RELATED WORK,0.033816425120772944,"Image segmentation with generative models is an active research direction at the moment, how-
ever, existing methods are primarily based on GANs. The first line of works (Voynov & Babenko,
2020; Voynov et al., 2021; Melas-Kyriazi et al., 2021) is based on the evidence that the latent
spaces of the state-of-the-art GANs have directions corresponding to effects that influence the fore-
ground/background pixels differently, which allows producing synthetic data to train segmentation
models. However, these approaches are currently able to perform binary segmentation only, and it is
not clear if they can be used in the general setup of semantic segmentation. The second line of works
(Zhang et al., 2021; Tritrong et al., 2021; Xu, 2021; Galeev et al., 2020) is more relevant to our study
since they are based on the intermediate representations obtained in GANs. In particular, the method
proposed in (Zhang et al., 2021) trains a pixel class prediction model on these representations and
confirms their label efficiency. In the experimental section, we compare the method from (Zhang
et al., 2021) to our DDPM-based one and demonstrate several distinctive advantages of our solution."
RELATED WORK,0.036231884057971016,"Representations from generative models for discriminative tasks. The usage of generative mod-
els, as representation learners, has been widely investigated for global prediction (Donahue & Si-
monyan, 2019; Chen et al., 2020a), and dense prediction problems (Zhang et al., 2021; Tritrong
et al., 2021; Xu, 2021; Xu et al., 2021). While previous works highlighted the practical advantages
of these representations, such as out-of-distribution robustness (Li et al., 2021a), generative models
as representation learners receive less attention compared to alternative unsupervised methods, e.g.,
based on contrastive learning (Chen et al., 2020b). The main reason is probably the difficulty of
training a high-quality generative model on a complex, diverse dataset. However, given the recent
success of DDPM on Imagenet (Deng et al., 2009), one can expect that this direction will attract
more attention in the future."
REPRESENTATIONS FROM DIFFUSION MODELS,0.03864734299516908,"3
REPRESENTATIONS FROM DIFFUSION MODELS"
REPRESENTATIONS FROM DIFFUSION MODELS,0.04106280193236715,"In the following section, we investigate the image representations learned by diffusion models. First,
we provide a brief overview of the DDPM framework. Then, we describe how to extract features
with DDPM and investigate what kind of semantic information these features might capture."
REPRESENTATIONS FROM DIFFUSION MODELS,0.043478260869565216,"Background. Diffusion models transform noise xT ∼N(0, I) to the sample x0 by gradually denois-
ing xT to less noisy samples xt. Formally, we are given a forward diffusion process:"
REPRESENTATIONS FROM DIFFUSION MODELS,0.04589371980676329,"q(xt|xt−1) := N(xt;
p"
REPRESENTATIONS FROM DIFFUSION MODELS,0.04830917874396135,"1 −βtxt−1, βtI),
(1)"
REPRESENTATIONS FROM DIFFUSION MODELS,0.050724637681159424,"for some fixed variance schedule β1, . . . , βt."
REPRESENTATIONS FROM DIFFUSION MODELS,0.05314009661835749,Published as a conference paper at ICLR 2022
REPRESENTATIONS FROM DIFFUSION MODELS,0.05555555555555555,Upsample 1 1 1 1 1 1 1 1
REPRESENTATIONS FROM DIFFUSION MODELS,0.057971014492753624,Pixel representation 1 1 Vote 
REPRESENTATIONS FROM DIFFUSION MODELS,0.06038647342995169,Feature maps
REPRESENTATIONS FROM DIFFUSION MODELS,0.06280193236714976,Pixel classifiers
REPRESENTATIONS FROM DIFFUSION MODELS,0.06521739130434782,Upsample
REPRESENTATIONS FROM DIFFUSION MODELS,0.06763285024154589,Upsample
REPRESENTATIONS FROM DIFFUSION MODELS,0.07004830917874397,Linear ReLU
REPRESENTATIONS FROM DIFFUSION MODELS,0.07246376811594203,BatchNorm
REPRESENTATIONS FROM DIFFUSION MODELS,0.0748792270531401,Linear ReLU
REPRESENTATIONS FROM DIFFUSION MODELS,0.07729468599033816,BatchNorm
REPRESENTATIONS FROM DIFFUSION MODELS,0.07971014492753623,Linear
REPRESENTATIONS FROM DIFFUSION MODELS,0.0821256038647343,"Figure 1: Overview of the proposed method. (1) x0 −→xt by adding noise according to q(xt|x0).
(2) Extracting feature maps from a noise predictor ϵθ(xt, t). (3) Collecting pixel-level representa-
tions by upsampling the feature maps to the image resolution and concatenating them. (4) Using the
pixel-wise feature vectors to train an ensemble of MLPs to predict a class label for each pixel."
REPRESENTATIONS FROM DIFFUSION MODELS,0.08454106280193237,"Importantly, a noisy sample xt can be obtained directly from the data x0:"
REPRESENTATIONS FROM DIFFUSION MODELS,0.08695652173913043,"q(xt|x0) := N(xt; √¯αtx0, (1 −¯αt)I),"
REPRESENTATIONS FROM DIFFUSION MODELS,0.0893719806763285,"xt = √¯αtx0 +
√"
REPRESENTATIONS FROM DIFFUSION MODELS,0.09178743961352658,"1 −¯αtϵ, ϵ ∼N(0, 1),
(2)"
REPRESENTATIONS FROM DIFFUSION MODELS,0.09420289855072464,"where αt := 1 −βt, ¯αt := Qt
s=1 αs."
REPRESENTATIONS FROM DIFFUSION MODELS,0.0966183574879227,Pretrained DDPM approximates a reverse process:
REPRESENTATIONS FROM DIFFUSION MODELS,0.09903381642512077,"pθ(xt−1|xt) := N(xt−1; µθ(xt, t), Σθ(xt, t)).
(3)"
REPRESENTATIONS FROM DIFFUSION MODELS,0.10144927536231885,"In practice, rather than predicting the mean of the distribution in Equation (3), the noise predictor
network ϵθ(xt, t) predicts the noise component at the step t; the mean is then a linear combination
of this noise component and xt. The covariance predictor Σθ(xt, t) can be either a fixed set of scalar
covariances or learned as well (the latter was shown to improve the model quality (Nichol, 2021))."
REPRESENTATIONS FROM DIFFUSION MODELS,0.10386473429951691,"The denoising model ϵθ(xt, t) is typically parameterized by different variants of the UNet archi-
tecture (Ronneberger et al., 2015), and in our experiments we investigate the state-of-the-art one
proposed in (Dhariwal & Nichol, 2021)."
REPRESENTATIONS FROM DIFFUSION MODELS,0.10628019323671498,"Extracting representations. For a given real image x0 ∈RH×W ×3, one can compute T sets of
activation tensors from the noise predictor network ϵθ(xt, t). The overall scheme for a timestep t is
presented in Figure 1. First, we corrupt x0 by adding Gaussian noise according to Equation (2). The
noisy xt is used as an input of ϵθ(xt, t) parameterized by the UNet model. The UNet’s intermediate
activations are then upsampled to H × W with bilinear interpolation. This allows treating them as
pixel-level representations of x0."
REPRESENTATION ANALYSIS,0.10869565217391304,"3.1
REPRESENTATION ANALYSIS"
REPRESENTATION ANALYSIS,0.1111111111111111,"We analyze the representations produced by the noise predictor ϵθ(xt, t) for different t. We consider
the state-of-the-art DDPM checkpoints trained on the LSUN-Horse and FFHQ-256 datasets1."
REPRESENTATION ANALYSIS,0.11352657004830918,"The intermediate activations from the noise predictor capture semantic information. For this
experiment, we take a few images from the LSUN-Horse and FFHQ datasets and manually assign
each pixel to one of the 21 and 34 semantic classes, respectively. Our goal is to understand whether
the pixel-level representations produced by DDPM effectively capture the information about seman-
tics. To this end, we train a multi-layer perceptron (MLP) to predict the pixel semantic label from
its features produced by one of the 18 UNet decoder blocks on a specific diffusion step t. Note
that we consider only the decoder activations because they also aggregate the encoder activations
through the skip connections. MLPs are trained on 20 images and evaluated on 20 hold-out ones.
The predictive performance is measured in terms of mean IoU."
REPRESENTATION ANALYSIS,0.11594202898550725,1https://github.com/openai/guided-diffusion
REPRESENTATION ANALYSIS,0.11835748792270531,Published as a conference paper at ICLR 2022
REPRESENTATION ANALYSIS,0.12077294685990338,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.1 0.2 0.3 0.4 0.5 mIoU"
REPRESENTATION ANALYSIS,0.12318840579710146,LSUN-Horse
REPRESENTATION ANALYSIS,0.12560386473429952,Block 2
REPRESENTATION ANALYSIS,0.1280193236714976,Block 4
REPRESENTATION ANALYSIS,0.13043478260869565,Block 6
REPRESENTATION ANALYSIS,0.13285024154589373,Block 8
REPRESENTATION ANALYSIS,0.13526570048309178,Block 10
REPRESENTATION ANALYSIS,0.13768115942028986,Block 12
REPRESENTATION ANALYSIS,0.14009661835748793,Block 14
REPRESENTATION ANALYSIS,0.14251207729468598,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 mIoU"
REPRESENTATION ANALYSIS,0.14492753623188406,FFHQ-256
REPRESENTATION ANALYSIS,0.1473429951690821,Block 2
REPRESENTATION ANALYSIS,0.1497584541062802,Block 4
REPRESENTATION ANALYSIS,0.15217391304347827,Block 6
REPRESENTATION ANALYSIS,0.15458937198067632,Block 8
REPRESENTATION ANALYSIS,0.1570048309178744,Block 10
REPRESENTATION ANALYSIS,0.15942028985507245,Block 12
REPRESENTATION ANALYSIS,0.16183574879227053,Block 14
REPRESENTATION ANALYSIS,0.1642512077294686,"Figure 2: The evolution of predictive performance of DDPM-based pixel-wise representations for
different UNet decoder blocks and diffusion steps. The blocks are numbered from the deep to
shallow ones. The most informative features typically correspond to the later steps of the reverse
diffusion process and middle layers of the UNet decoder. The earlier steps correspond to uninfor-
mative representations. The plots for other datasets are provided in Appendix A"
REPRESENTATION ANALYSIS,0.16666666666666666,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 mIoU"
REPRESENTATION ANALYSIS,0.16908212560386474,LSUN-Horse | Small-sized Objects
REPRESENTATION ANALYSIS,0.17149758454106281,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REPRESENTATION ANALYSIS,0.17391304347826086,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.0 0.1 0.2 0.3 0.4 0.5 0.6 mIoU"
REPRESENTATION ANALYSIS,0.17632850241545894,LSUN-Horse | Large-sized Objects
REPRESENTATION ANALYSIS,0.178743961352657,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REPRESENTATION ANALYSIS,0.18115942028985507,"Figure 3: The evolution of predictive performance of DDPM-based pixel-wise representations on
the LSUN-Horse dataset for classes with the smallest (Left) and largest (Right) average areas. The
predictive performance for small-sized objects starts growing later in the reverse process. The deeper
blocks are more informative for larger objects and the shallower blocks are more informative for
smaller objects. A similar evaluation for other datasets is provided in Appendix A."
REPRESENTATION ANALYSIS,0.18357487922705315,"The evolution of predictive performance across the different blocks and diffusion steps t is presented
in Figure 2. The blocks are numbered from the deep to shallow ones. Figure 2 shows that the
discriminability of the features produced by the noise predictor ϵθ(xt, t) varies for different blocks
and diffusion steps. In particular, the features corresponding to the later steps of the reverse diffusion
process typically capture semantic information more effectively. In contrast, the ones corresponding
to the early steps are generally uninformative. Across different blocks, the features produced by the
layers in the middle of the UNet decoder appear to be the most informative on all diffusion steps."
REPRESENTATION ANALYSIS,0.1859903381642512,"Also, we separately consider small-sized and large-sized semantic classes based on the average area
in the annotated dataset. Then, we evaluate mean IoU for these classes independently across the
different UNet blocks and diffusion steps. The results on LSUN-Horse are in Figure 3. As expected,
the predictive performance for large-sized objects starts growing earlier in the reverse process. The
shallower blocks are more informative for smaller objects, while the deeper blocks are more so for
the larger ones. In both cases, the most discriminative features still correspond to the middle blocks."
REPRESENTATION ANALYSIS,0.18840579710144928,"Figure 2 implies that for certain UNet blocks and diffusion steps, similar DDPM-based representa-
tions correspond to the pixels of the same semantics. Figure 4 shows the k-means clusters (k=5)
formed by the features extracted by the FFHQ checkpoint from the blocks {6, 8, 10, 12} on the dif-
fusion steps {50, 200, 400, 600, 800}, and confirms that clusters can span coherent semantic objects
and object-parts. In the block B=6, the features correspond to coarse semantic masks. At the other
extreme, the features from B=12 can discriminate between fine-grained face parts but exhibit less
semantic meaningness for coarse fragmentation. Across different diffusion steps, the most meaning-
ful features correspond to the later ones. We attribute this behavior to the fact that on the earlier steps
of the reverse process, the global structure of a DDPM sample has not yet emerged, therefore, it is
hardly possible to predict segmentation masks at this stage. This intuition is qualitatively confirmed
by the masks in Figure 4. For t=800, the masks poorly reflect the content of actual images, while
for smaller values of t, the masks and images are semantically coherent."
REPRESENTATION ANALYSIS,0.19082125603864733,Published as a conference paper at ICLR 2022
REPRESENTATION ANALYSIS,0.1932367149758454,"Image
Timestep 50
Timestep 200
Timestep 400
Timestep 600
Timestep 800
Timestep 50
Timestep 200
Timestep 400
Timestep 600
Timestep 800"
REPRESENTATION ANALYSIS,0.1956521739130435,"Block 6 [512x32x32]
Block 8 [512x64x64]"
REPRESENTATION ANALYSIS,0.19806763285024154,"Image
Timestep 50
Timestep 200
Timestep 400
Timestep 600
Timestep 800
Timestep 50
Timestep 200
Timestep 400
Timestep 600
Timestep 800"
REPRESENTATION ANALYSIS,0.20048309178743962,"Block 10 [512x64x64]
Block 12 [256x128x128]"
REPRESENTATION ANALYSIS,0.2028985507246377,"Figure 4: Examples of k-means clusters (k=5) formed by the features extracted from the UNet
decoder blocks {6, 8, 10, 12} on the diffusion steps {50, 200, 400, 600, 800}. The clusters from the
middle blocks spatially span coherent semantic objects and parts."
DDPM-BASED REPRESENTATIONS FOR FEW-SHOT SEMANTIC SEGMENTATION,0.20531400966183574,"3.2
DDPM-BASED REPRESENTATIONS FOR FEW-SHOT SEMANTIC SEGMENTATION"
DDPM-BASED REPRESENTATIONS FOR FEW-SHOT SEMANTIC SEGMENTATION,0.20772946859903382,"The potential effectiveness of the intermediate DDPM activations observed above implies their
usage as image representations for dense prediction tasks. Figure 1 schematically presents our
overall approach for image segmentation, which exploits the discriminability of these represen-
tations.
In more detail, we consider a few-shot semi-supervised setup, when a large number
of unlabeled images {X1, . . . , XN} ⊂RH×W ×3 from the particular domain are available, and
only for n training images {X1, . . . , Xn} ⊂RH×W ×3 the groundtruth K-class semantic masks
{Y1, . . . , Yn} ⊂RH×W ×{1,...,K} are provided."
DDPM-BASED REPRESENTATIONS FOR FEW-SHOT SEMANTIC SEGMENTATION,0.21014492753623187,"As a first step, we train a diffusion model on the whole {X1, . . . , XN} in an unsupervised manner.
Then, this diffusion model is used to extract the pixel-level representations of the labeled images
using the subset of the UNet blocks and diffusion steps t. In this work, we use the representations
from the middle blocks B={5, 6, 7, 8, 12} of the UNet decoder and later steps t={50, 150, 250}
of the reverse diffusion process. These blocks and time steps are motivated by the insights from
Section 3.1 but intentionally not tuned for each dataset."
DDPM-BASED REPRESENTATIONS FOR FEW-SHOT SEMANTIC SEGMENTATION,0.21256038647342995,"While the feature extraction at the particular time step is stochastic, we fix the noise for all timesteps
t and ablate this in Section 4.1. The extracted representations from all blocks B and steps t are
upsampled to the image size and concatenated, forming the feature vectors for all pixels of the
training images. The overall dimension of the pixel-level representations is 8448."
DDPM-BASED REPRESENTATIONS FOR FEW-SHOT SEMANTIC SEGMENTATION,0.21497584541062803,"Then, following (Zhang et al., 2021), we train an ensemble of independent multi-layer perceptrons
(MLPs) on these feature vectors, which aim to predict a semantic label of each pixel available for
training images. We adopt the ensemble configuration and training settings from (Zhang et al.,
2021) and exploit them across all other methods in our experiments, see Appendix C for details."
DDPM-BASED REPRESENTATIONS FOR FEW-SHOT SEMANTIC SEGMENTATION,0.21739130434782608,"To segment a test image, we extract its DDPM-based pixel-wise representations and use them to
predict the pixel labels by the ensemble. The final prediction is obtained by majority voting."
EXPERIMENTS,0.21980676328502416,"4
EXPERIMENTS"
EXPERIMENTS,0.2222222222222222,"This section experimentally confirms the advantage of the DDPM-based representations for the se-
mantic segmentation problem. We start from a thorough comparison to the existing alternatives and
then dissect the reasons for the DDPM success by additional analysis."
EXPERIMENTS,0.2246376811594203,"Datasets. In our evaluation, we mainly work with the “bedroom”, “cat” and “horse” categories
from LSUN (Yu et al., 2015) and FFHQ-256 (Karras et al., 2019). As a training set for each dataset,
we consider several images for which the fine-grained semantic masks are collected following the
protocol from (Zhang et al., 2021). For each dataset, a professional assessor was hired to annotate
train and test samples. We denote the collected datasets as Bedroom-28, FFHQ-34, Cat-15, Horse-
21, where the number corresponds to the number of semantic classes."
EXPERIMENTS,0.22705314009661837,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.22946859903381642,"Dataset
RealTrain
RealTest
GAN DDPM Total"
EXPERIMENTS,0.2318840579710145,"Bedroom-28
40
20
40
40
140"
EXPERIMENTS,0.23429951690821257,"FFHQ-34
20
20
20
20
80"
EXPERIMENTS,0.23671497584541062,"Cat-15
30
20
30
30
110"
EXPERIMENTS,0.2391304347826087,"Horse-21
30
30
30
30
120"
EXPERIMENTS,0.24154589371980675,"CelebA-19
20
500
—
—
520"
EXPERIMENTS,0.24396135265700483,"ADE-Bedroom-30
50
650
—
—
700"
EXPERIMENTS,0.2463768115942029,Table 1: Number of annotated images for each dataset used in our evaluation.
EXPERIMENTS,0.24879227053140096,"Additionally, we consider two datasets, which, in contrast to others, have publicly available annota-
tions and sizable evaluation sets:"
EXPERIMENTS,0.25120772946859904,"• ADE-Bedroom-30 is a subset of the ADE20K dataset (Zhou et al., 2018), where we extract
only images of bedroom scenes with 30 most frequent classes. We resize each image to 256 for
the smaller side and then crop them to obtain the 256×256 samples.
• CelebA-19 is a subset of the CelebAMask-HQ dataset (Lee et al., 2020), which provides the
annotation for 19 facial attributes. All images are resized to 256 resolution."
EXPERIMENTS,0.2536231884057971,The number of annotated images for each dataset are in Table 1. Other details are in Appendix E.
EXPERIMENTS,0.2560386473429952,"Methods. In the evaluation, we compare our method (denoted as DDPM) to several prior ap-
proaches which tackle the few-shot semantic segmentation setup. First, we describe the baselines
that produce a large set of annotated synthetic images to train a segmentation model:"
EXPERIMENTS,0.2584541062801932,"• DatasetGAN (Zhang et al., 2021) — this method exploits the discriminability of pixel-level
features produced by GANs. In more detail, assessors annotate a few GAN-produced images.
Then, the latent codes of these images are used to obtain the intermediate generator activations,
which are considered as pixel-level representations. Given these representations, a classifier
is trained to predict a semantic label for each pixel. This classifier is then used to label new
synthetic GAN images, which, for their part, serve as a training set for the DeepLabV3 seg-
mentation model (Chen et al., 2017). For each dataset, we increase the number of synthetic
images until the performance on the validation set is not saturated. According to (Zhang et al.,
2021), we also remove 10% of synthetic samples with the most uncertain predictions.
• DatasetDDPM mirrors the DatasetGAN baseline with the only difference being that GANs are
replaced with DDPMs. We include this baseline to compare the GAN-based and DDPM-based
representations in the same scenario."
EXPERIMENTS,0.2608695652173913,"Note that our segmentation method described in Section 3.2 is more straightforward compared to
DatasetGAN and DatasetDDPM since it does not require auxiliary steps of the synthetic dataset
generation and training the segmentation model on it."
EXPERIMENTS,0.2632850241545894,"Then, we consider a set of baselines that allow extracting intermediate activations from the real
images directly and use them as pixel-level representations similarly to our method. In contrast to
DatasetGAN and DatasetDDPM, these methods can potentially be beneficial due to the absence of
the domain gap between real and synthetic images."
EXPERIMENTS,0.26570048309178745,"• MAE (He et al., 2021) — one of the state-of-the-art self-supervised methods, which learns a
denoising autoencoder to reconstruct missing patches. We use ViT-Large (Dosovitskiy et al.,
2021) as a backbone model and reduce the patch size to 8×8 to increase the spatial dimensions
of the feature maps. We pretrain all models on the same datasets as DDPM using the official
code2. The feature extraction for this method is described in Appendix F.
• SwAV (Caron et al., 2020) — one more recent self-supervised approach. We consider a twice
wider ResNet-50 model for evaluation. All models are pretrained on the same datasets as
DDPM also using the official source code3. The input image resolution is 256.
• GAN Inversion employs the state-of-the-art method (Tov et al., 2021) to obtain the latent codes
for real images. We map the annotated real images to the GAN latent space, which allows
computing the intermediate generator activations and using them as pixel-level representations."
EXPERIMENTS,0.26811594202898553,"2https://github.com/facebookresearch/mae
3https://github.com/facebookresearch/swav"
EXPERIMENTS,0.27053140096618356,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.27294685990338163,"Method
Bedroom-28
FFHQ-34
Cat-15
Horse-21
CelebA-19∗
ADE Bedroom-30∗"
EXPERIMENTS,0.2753623188405797,"ALAE
20.0 ± 1.0
48.1 ± 1.3
—
—
49.7 ± 0.7
15.0 ± 0.5"
EXPERIMENTS,0.2777777777777778,"VDVAE
—
57.3 ± 1.1
—
—
54.1 ± 1.0
—"
EXPERIMENTS,0.28019323671497587,"GAN Inversion
13.9 ± 0.6
51.7 ± 0.8 21.4 ± 1.7
17.7 ± 0.4
51.5 ± 2.3
11.1 ± 0.2"
EXPERIMENTS,0.2826086956521739,"GAN Encoder
22.4 ± 1.6
53.9 ± 1.3 32.0 ± 1.8 26.7 ± 0.7
53.9 ± 0.8
15.7 ± 0.3"
EXPERIMENTS,0.28502415458937197,"SwAV
42.4 ± 1.7
56.9 ± 1.3
45.1 ± 2.1 54.0 ± 0.9
52.4 ± 1.3
30.6 ± 1.6"
EXPERIMENTS,0.28743961352657005,"MAE
45.0 ± 2.0
58.8 ± 1.1 52.4 ± 2.3
63.4 ± 1.4
57.8 ± 0.4
31.7 ± 1.8"
EXPERIMENTS,0.2898550724637681,"DatasetGAN
31.3 ± 2.3
57.0 ± 1.1 36.5 ± 2.3
45.4 ± 1.4
—
—"
EXPERIMENTS,0.2922705314009662,"DatasetDDPM (Ours)
47.9 ± 2.9
56.0 ± 0.9 47.6 ± 1.5 60.8 ± 1.0
—
—"
EXPERIMENTS,0.2946859903381642,"DDPM (Ours)
49.4 ± 1.9
59.1 ± 1.4
53.7 ± 3.3 65.0 ± 0.8
59.9 ± 1.0
34.6 ± 1.7"
EXPERIMENTS,0.2971014492753623,"Table 2: The comparison of the segmentation methods in terms of mean IoU. (*) On CelebA-19 and
ADE Bedroom-30, we evaluate models trained on FFHQ-256 and LSUN Bedroom, respectively."
EXPERIMENTS,0.2995169082125604,"• GAN Encoder — while GAN Inversion struggles to reconstruct images from LSUN domains,
we also consider the activations of the pretrained GAN encoder used for GAN Inversion.
• VDVAE (Child, 2021) — state-of-the-art autoencoder model. The intermediate activations are
extracted from both encoder and decoder and concatenated. While there are no pretrained mod-
els on the LSUN datasets, we evaluate this model only on the publicly available checkpoint4
on FFHQ-256. Note that VAEs are still significantly inferior to GANs and DDPMs on LSUN.
• ALAE (Pidhorskyi et al., 2020) adopts StyleGANv1 generator and adds an encoder network to
the adversarial training. We extract features from the encoder model. In our evaluation, we use
publicly available models on LSUN-Bedroom and FFHQ-10245."
EXPERIMENTS,0.30193236714975846,"Generative pretrained models. In our experiments, we use the state-of-the-art StyleGAN2 (Karras
et al., 2020) models for the GAN-based baselines and the state-of-the-art pretrained ADMs (Dhari-
wal & Nichol, 2021) for our DDPM-based method.
Since there is not a pretrained model for
FFHQ-256, we train it ourselves using the official implementation6. For evaluation on the ADE-
Bedroom-30 dataset, we use the models (including the baselines) pretrained on LSUN-Bedroom.
For Celeba-19, we evaluate the models trained on FFHQ-256."
EXPERIMENTS,0.30434782608695654,"Main results. The comparison of the methods in terms of the mean IoU measure is presented in
Table 2. The results are averaged over 5 independent runs for different data splits. We also report per
class IoUs in Appendix D. Additionally, we provide several qualitative examples of segmentation
with our method in Figure 5. Below we highlight several key observations:"
EXPERIMENTS,0.30676328502415456,"• The proposed method based on the DDPM representations significantly outperforms the alter-
natives on most datasets.
• The MAE baseline is the strongest competitor to the DDPM-based segmentation and demon-
strates comparable results on the FFHQ-34 and Cat-15 datasets.
• The SwAV baseline underperforms compared to the DDPM-based segmentation. We attribute
this behavior to the fact that this baseline is trained in the discriminative fashion and can sup-
press the details, which are needed for fine-grained semantic segmentation. This result is con-
sistent with the recent findings in (Cole et al., 2021), which shows that the state-of-the-art
contrastive methods produce representations, which are suboptimal for fine-grained problems.
• DatasetDDPM outperforms its counterpart DatasetGAN against most benchmarks. Note that
both these methods use the DeepLabV3 network. We attribute this superiority to the higher
quality of DDPM synthetics, therefore, a smaller domain gap between synthetic and real data.
• On most datasets, DDPM outperforms the DatasetDDPM competitor. We provide an addi-
tional experiment to investigate this in the discussion section below."
EXPERIMENTS,0.30917874396135264,"Overall, the proposed DDPM-based segmentation outperforms the baselines that exploit alternative
generative models and also the baselines trained in the self-supervised fashion. This result highlights
the potential of using the state-of-the-art DDPMs as strong unsupervised representation learners."
EXPERIMENTS,0.3115942028985507,"4https://github.com/openai/vdvae
5https://github.com/podgorskiy/ALAE
6https://github.com/openai/guided-diffusion"
EXPERIMENTS,0.3140096618357488,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.3164251207729469,"FFHQ
34 classes"
EXPERIMENTS,0.3188405797101449,CelebAMask
CLASSES,0.321256038647343,19 classes
CLASSES,0.32367149758454106,LSUN-Bedroom
CLASSES,0.32608695652173914,28 classes
CLASSES,0.3285024154589372,ADE-Bedroom
CLASSES,0.3309178743961353,30 classes
CLASSES,0.3333333333333333,LSUN-Cat
CLASSES,0.3357487922705314,15 classes Image
CLASSES,0.33816425120772947,LSUN-Horse
CLASSES,0.34057971014492755,21 classes
CLASSES,0.34299516908212563,"Groundtruth
DDPM
Image
Groundtruth
DDPM
Image
Groundtruth
DDPM
Image
Groundtruth
DDPM
Image
Groundtruth
DDPM"
CLASSES,0.34541062801932365,"Figure 5: The examples of segmentation masks predicted by our method on the test images along
with the groundtruth annotated masks."
CLASSES,0.34782608695652173,"Bedroom-28
Cat-15
Horse-21"
CLASSES,0.3502415458937198,"Train data
Real
DDPM
GAN
Real
DDPM
GAN
Real
DDPM
GAN"
CLASSES,0.3526570048309179,"DatasetGAN
—
—
31.3 ± 2.3
—
—
36.5 ± 2.3
—
—
45.4 ± 1.4"
CLASSES,0.35507246376811596,"DatasetDDPM
—
47.9 ± 2.9
—
—
47.6 ± 1.5
—
—
60.8 ± 1.0
—"
CLASSES,0.357487922705314,"DDPM
49.4 ± 1.9 48.7 ± 2.6
43.3 ± 2.9
53.7 ± 3.3
47.9 ± 2.7
41.1 ± 2.2
65.0 ± 0.8
62.4 ± 1.0
60.0 ± 1.0"
CLASSES,0.35990338164251207,"Table 3: Performance of DDPM-based segmentation when trained on real and synthetic images.
When trained on DDPM-produced data, DDPM demonstrates comparable performance to Dataset-
DDPM. When trained on GAN-produced data, DDPM still significantly outperforms DatasetGAN,
but the gap between them reduces."
DISCUSSION,0.36231884057971014,"4.1
DISCUSSION"
DISCUSSION,0.3647342995169082,"The effect of training on real data. The proposed DDPM method is trained on annotated real
images, while DatasetDDPM and DatasetGAN are trained on synthetic ones, which are typically less
natural, diverse, and can lack objects of particular classes. Moreover, synthetic images are harder
for human annotation since they might have some distorted objects that are difficult to assign to a
particular class. In the following experiment, we quantify the performance drop caused by training
on real or synthetic data. Specifically, Table 3 reports the performance of the DDPM approach
trained on real, DDPM-produced and GAN-produced annotated images. As can be seen, training
on real images is very beneficial on the domains where the fidelity of generative models is still
relatively low, e.g., LSUN-Cat, which indicates that annotated real images are a more reliable source
of supervision. Moreover, if the DDPM method is trained on synthetic images, its performance
becomes on par with DatasetDDPM. On the other hand, when trained on GAN-produced samples,
DDPM significantly outperforms DatasetGAN. We attribute this to the fact that DDPMs provide
more semantically-valuable pixel-wise representations compared to GANs."
DISCUSSION,0.3671497584541063,"Sample-efficiency. In this experiment, we evaluate the performance of our method when it utilizes
less annotated data. We provide mIoU for four datasets in Table 4. Importantly, DDPM is still able
to outperform most baselines in Table 2, using significantly less supervision."
DISCUSSION,0.3695652173913043,"The effect of stochastic feature extraction. Here, we investigate whether our method can benefit
from the stochastic feature extraction described in Section 3.2. We consider the deterministic case,
when the noise ϵ∼N(0, I) is sampled once and used in (2) to obtain xt for all timesteps t during
both training and evaluation. Then, we compare it to the following stochastic options:"
DISCUSSION,0.3719806763285024,"First, different ϵt are sampled for different timesteps t and shared during the training and evaluation.
Second, one samples different noise for all timesteps at each training iteration; during the evaluation
the method also uses unseen noise samples."
DISCUSSION,0.3743961352657005,Published as a conference paper at ICLR 2022
DISCUSSION,0.37681159420289856,"No corruption
Weak
Medium
Strong"
DISCUSSION,0.37922705314009664,Corruption level 10 15 20 25 30 35 40 45 50 mIoU
DISCUSSION,0.38164251207729466,"SwAV
MAE
DDPM"
DISCUSSION,0.38405797101449274,(a) Bedroom-28
DISCUSSION,0.3864734299516908,"No corruption
Weak
Medium
Strong"
DISCUSSION,0.3888888888888889,Corruption level 10 20 30 40 50 60 70 mIoU
DISCUSSION,0.391304347826087,"SwAV
MAE
DDPM"
DISCUSSION,0.39371980676328505,(b) Horse-21
DISCUSSION,0.3961352657004831,"Figure 6: mIoU degradation for different image corruption levels on the Bedroom-28 and Horse-21
datasets. DDPM demonstrates higher robustness and preserves its advantage for all distortion levels."
DISCUSSION,0.39855072463768115,"Bedroom-28
Cat-15
Horse-21"
DISCUSSION,0.40096618357487923,"Method
40
20
10
30
20
10
30
20
10"
DISCUSSION,0.4033816425120773,"DDPM
49.4 ± 1.9 46.2 ± 3.6
38.2 ± 2.9
53.7 ± 3.3
49.2 ± 4.2
42.0 ± 4.8
65.0 ± 0.8
63.8 ± 0.7
56.9 ± 2.4"
DISCUSSION,0.4057971014492754,"Table 4: Evaluation of the proposed method with a different number of labeled training data. Even
using less annotated data, DDPM still outperforms most baselines in Table 2."
DISCUSSION,0.4082125603864734,"Share Train/Test Share for t
Bedroom-28
FFHQ-34"
DISCUSSION,0.4106280193236715,"+
+
49.3 ± 1.9
59.1 ± 1.4"
DISCUSSION,0.41304347826086957,"+
-
49.1 ± 2.2
59.3 ± 1.5"
DISCUSSION,0.41545893719806765,"-
-
48.9 ± 1.6
59.3 ± 1.4"
DISCUSSION,0.4178743961352657,"Table 5: Performance of the DDPM-based method for different feature extraction variations. All
considered stochastic options provide a similar mIoU to the determinstic one."
DISCUSSION,0.42028985507246375,"The results are provided in Table 5. As one can see, the difference in the performance is marginal.
We attribute this behavior to the following reasons:"
DISCUSSION,0.4227053140096618,• Our method uses later t of the reverse diffusion process where the noise magnitude is low.
DISCUSSION,0.4251207729468599,"• Since we exploit the deep layers of the UNet model, the noise might not affect the activations
from these layers significantly."
DISCUSSION,0.427536231884058,"Robustness to input corruptions. In this experiment, we investigate the robustness of DDPM-
based representations. First, we learn pixel classifiers on the clean images using the DDPM, SwAV
and MAE representations on the Bedroom-28 and Horse-21 datasets. Then, 18 diverse corruption
types, adopted from (Hendrycks & Dietterich, 2019), are applied to test images. Each corruption
has five levels of severity. In Figure 6, we provide mean IoUs computed over all corruption types
for 1, 3, 5 levels of severity, denoted as “weak”, “medium” and “strong”, respectively."
DISCUSSION,0.42995169082125606,"One can observe that the proposed DDPM-based method demonstrates higher robustness and pre-
serves its advantage over the SwAV and MAE models even for severe image distortions."
CONCLUSION,0.4323671497584541,"5
CONCLUSION"
CONCLUSION,0.43478260869565216,"This paper demonstrates that DDPMs can serve as representation learners for discriminative com-
puter vision problems. Compared to GANs, diffusion models allow for a straightforward computa-
tion of these representations for real images, and one does not need to learn an additional encoder,
which maps images to the latent space. This DDPM’s advantage and superior generative quality pro-
vide state-of-the-art performance in the few-shot semantic segmentation task. The notable restraint
of the DDPM-based segmentation is a requirement of high-quality diffusion models trained on the
dataset at hand, which can be challenging for complex domains, like ImageNet or MSCOCO. How-
ever, given the rapid research progress on DDPM, we expect they will reach these milestones in the
nearest future, thereby extending the range of applicability for the corresponding representations."
CONCLUSION,0.43719806763285024,Published as a conference paper at ICLR 2022
REFERENCES,0.4396135265700483,REFERENCES
REFERENCES,0.4420289855072464,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments.
arXiv preprint
arXiv:2006.09882, 2020."
REFERENCES,0.4444444444444444,"Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous
convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017."
REFERENCES,0.4468599033816425,"Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
Generative pretraining from pixels. In ICML, 2020a."
REFERENCES,0.4492753623188406,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In ICML, 2020b."
REFERENCES,0.45169082125603865,"Rewon Child. Very deep {vae}s generalize autoregressive models and can outperform them on
images. In International Conference on Learning Representations, 2021."
REFERENCES,0.45410628019323673,"Elijah Cole, Xuan Yang, Kimberly Wilber, Oisin Mac Aodha, and Serge Belongie. When does
contrastive visual representation learning work? arXiv preprint arXiv:2105.05837, 2021."
REFERENCES,0.45652173913043476,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.45893719806763283,Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. 2021.
REFERENCES,0.4613526570048309,"Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. NeurIPS, 2019."
REFERENCES,0.463768115942029,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. ICLR, 2021."
REFERENCES,0.46618357487922707,"Danil Galeev, Konstantin Sofiiuk, Danila Rukhovich, Mikhail Romanov, Olga Barinova, and Anton
Konushin. Learning high-resolution domain-specific representations with a gan generator. In
S+SSPR, 2020."
REFERENCES,0.46859903381642515,"Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick.
Masked
autoencoders are scalable vision learners. arXiv:2111.06377, 2021."
REFERENCES,0.47101449275362317,"Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. In International Conference on Learning Representations, 2019."
REFERENCES,0.47342995169082125,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. 2020."
REFERENCES,0.4758454106280193,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4401–4410, 2019."
REFERENCES,0.4782608695652174,"Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Ana-
lyzing and improving the image quality of stylegan. 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 8107–8116, 2020."
REFERENCES,0.4806763285024155,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
REFERENCES,0.4830917874396135,"Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive
facial image manipulation. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2020."
REFERENCES,0.4855072463768116,"Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, and Sanja Fidler. Semantic segmentation
with generative models: Semi-supervised learning and strong out-of-domain generalization. In
CVPR, 2021a."
REFERENCES,0.48792270531400966,Published as a conference paper at ICLR 2022
REFERENCES,0.49033816425120774,"Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff:
Single image super-resolution with diffusion probabilistic models. 2021b."
REFERENCES,0.4927536231884058,"Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Finding an unsupervised
image segmenter in each of your deep generative models. arXiv preprint arXiv:2105.08127, 2021."
REFERENCES,0.49516908212560384,"Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:
Image synthesis and editing with stochastic differential equations. 2021."
REFERENCES,0.4975845410628019,"Prafulla Nichol, Alex & Dhariwal. Improved denoising diffusion probabilistic models. ICML, 2021."
REFERENCES,0.5,"Stanislav Pidhorskyi, Donald A Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders.
In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2020."
REFERENCES,0.5024154589371981,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.5048309178743962,"Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad
Norouzi. Image super-resolution via iterative refinement. 2021."
REFERENCES,0.5072463768115942,"Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In ICML, 2015."
REFERENCES,0.5096618357487923,"Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In NeurIPS, 2019."
REFERENCES,0.5120772946859904,"Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
NeurIPS, 2020."
REFERENCES,0.5144927536231884,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. 2021."
REFERENCES,0.5169082125603864,"Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder
for stylegan image manipulation. arXiv preprint arXiv:2102.02766, 2021."
REFERENCES,0.5193236714975845,"Nontawat Tritrong, Pitchaporn Rewatbowornwong, and Supasorn Suwajanakorn. Repurposing gans
for one-shot semantic part segmentation. In CVPR, 2021."
REFERENCES,0.5217391304347826,"Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan
latent space. In ICML, 2020."
REFERENCES,0.5241545893719807,"Andrey Voynov, Stanislav Morozov, and Artem Babenko. Object segmentation without labels with
large-scale generative models. ICML, 2021."
REFERENCES,0.5265700483091788,"Changxi Xu, Jianjin & Zheng. Linear semantics in generative adversarial networks. In CVPR, 2021."
REFERENCES,0.5289855072463768,"Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, and Bolei Zhou. Generative hierarchical
features from synthesizing images. In CVPR, 2021."
REFERENCES,0.5314009661835749,"Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.
Lsun: Construction of
a large-scale image dataset using deep learning with humans in the loop.
arXiv preprint
arXiv:1506.03365, 2015."
REFERENCES,0.533816425120773,"Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio
Torralba, and Sanja Fidler. Datasetgan: Efficient labeled data factory with minimal human effort.
In CVPR, 2021."
REFERENCES,0.5362318840579711,"Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic
understanding of scenes through the ade20k dataset. International Journal of Computer Vision,
127:302–321, 2018."
REFERENCES,0.538647342995169,Published as a conference paper at ICLR 2022
REFERENCES,0.5410628019323671,APPENDIX
REFERENCES,0.5434782608695652,"A
EVOLUTION OF PREDICTIVE PERFORMANCE"
REFERENCES,0.5458937198067633,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.1 0.2 0.3 0.4 0.5 mIoU"
REFERENCES,0.5483091787439613,LSUN-Cat
REFERENCES,0.5507246376811594,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REFERENCES,0.5531400966183575,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.05 0.10 0.15 0.20 0.25 0.30 0.35 mIoU"
REFERENCES,0.5555555555555556,LSUN-Bedroom
REFERENCES,0.5579710144927537,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REFERENCES,0.5603864734299517,"Figure 7: The evolution of predictive performance of DDPM-based pixel-wise representations for
different UNet blocks and diffusion steps on LSUN-Cat and LSUN-Bedroom. The blocks are num-
bered from the deep to shallow ones."
REFERENCES,0.5628019323671497,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.1 0.2 0.3 0.4 mIoU"
REFERENCES,0.5652173913043478,FFHQ-256 | Small-sized Objects
REFERENCES,0.5676328502415459,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REFERENCES,0.5700483091787439,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 mIoU"
REFERENCES,0.572463768115942,FFHQ-256 | Large-sized Objects
REFERENCES,0.5748792270531401,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REFERENCES,0.5772946859903382,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.0 0.1 0.2 0.3 0.4 mIoU"
REFERENCES,0.5797101449275363,LSUN-Cat | Small-sized Objects
REFERENCES,0.5821256038647343,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REFERENCES,0.5845410628019324,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.1 0.2 0.3 0.4 0.5 mIoU"
REFERENCES,0.5869565217391305,LSUN-Cat | Large-sized Objects
REFERENCES,0.5893719806763285,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REFERENCES,0.5917874396135265,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.00 0.05 0.10 0.15 0.20 0.25 0.30 mIoU"
REFERENCES,0.5942028985507246,LSUN-Bedroom | Small-sized Objects
REFERENCES,0.5966183574879227,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REFERENCES,0.5990338164251208,"0
100
200
300
400
500
600
700
800
900
1000
Timesteps 0.2 0.3 0.4 0.5 0.6 0.7 mIoU"
REFERENCES,0.6014492753623188,LSUN-Bedroom | Large-sized Objects
REFERENCES,0.6038647342995169,"Block 2
Block 4
Block 6
Block 8
Block 10
Block 12
Block 14"
REFERENCES,0.606280193236715,"Figure 8: The evolution of predictive performance of DDPM-based pixel-wise representations on
the FFHQ-256, LSUN-Cat and LSUN-Bedroom datasets for classes with the smallest (Left) and
largest (Right) average areas."
REFERENCES,0.6086956521739131,Published as a conference paper at ICLR 2022
REFERENCES,0.6111111111111112,"B
DATASETDDPM & DATASETGAN SATURATION"
REFERENCES,0.6135265700483091,"DatasetDDPM
DatasetGAN"
REFERENCES,0.6159420289855072,"Dataset
10k
20K
30K
40K
50K
10K
20K
30K
40K
50K"
REFERENCES,0.6183574879227053,Bedroom-28 45.1 ± 2.3 46.2 ± 2.3 46.1 ± 2.8 47.8 ± 2.3 47.9 ± 2.9 30.6 ± 2.3 30.4 ± 3.1 30.9 ± 2.4 30.9 ± 2.4 31.3 ± 2.7
REFERENCES,0.6207729468599034,"FFHQ-34
55.9 ± 0.8 55.8 ± 0.7 55.9 ± 0.7 56.0 ± 0.8 55.9 ± 0.7 56.4 ± 1.0 56.9 ± 1.0 57.0 ± 1.1 57.0 ± 1.2 57.0 ± 1.2"
REFERENCES,0.6231884057971014,"Cat-15
43.6 ± 3.0 46.4 ± 1.7 46.2 ± 1.9 47.4 ± 1.7 47.6 ± 1.5 34.7 ± 2.8 34.8 ± 2.9 36.3 ± 2.3 35.8 ± 2.5 36.5 ± 2.3"
REFERENCES,0.6256038647342995,"Horse-21
57.0 ± 1.2 59.5 ± 0.5 59.0 ± 2.0 60.4 ± 1.1 60.8 ± 0.9 41.6 ± 2.0 43.1 ± 1.8 45.4 ± 1.4 44.5 ± 1.2 44.6 ± 1.4"
REFERENCES,0.6280193236714976,"Table 6: Performance of DatasetDDPM and DatasetGAN for 10K−50K synthetic images in the
training dataset. Mean IoU of both methods saturates at 30K−50K of synthetic data."
REFERENCES,0.6304347826086957,"C
TRAINING SETUP"
REFERENCES,0.6328502415458938,"The ensemble of MLPs consists of 10 independent models. Each MLP is trained for ∼4 epochs
using the Adam optimizer (Kingma & Ba, 2015) with 0.001 learning rate. The batch size is 64. This
setting is used for all methods and datasets."
REFERENCES,0.6352657004830918,"MLP architecture. We adopt the MLP architecture from (Zhang et al., 2021). Specifically, we use
MLPs with two hidden layers with ReLU nonlinearity and batch normalization. The sizes of hidden
layers are 128 and 32 for datasets with a number of classes less than 30, and 256 and 128 for others."
REFERENCES,0.6376811594202898,"Also, we evaluate the performance of the proposed method for twice wider / deeper MLPs on the
Bedroom-28 and FFHQ-34 datasets and do not observe any noticeable difference, see Table 7."
REFERENCES,0.6400966183574879,"Method
Bedroom-28 FFHQ-34"
REFERENCES,0.642512077294686,"Original MLP
49.4
59.1"
REFERENCES,0.644927536231884,"Wider MLP
49.5
59.1"
REFERENCES,0.6473429951690821,"Deeper MLP
49.3
58.9"
REFERENCES,0.6497584541062802,"Table 7: Performance of the proposed method for twice wider / deeper MLP architecture within the
ensemble. More expressive MLPs do not improve the performance."
REFERENCES,0.6521739130434783,"D
PER CLASS IOUS"
REFERENCES,0.6545893719806763,background
REFERENCES,0.6570048309178744,person thigh leg
REFERENCES,0.6594202898550725,muzzle head
REFERENCES,0.6618357487922706,barrel tail ear neck
REFERENCES,0.6642512077294686,saddle
REFERENCES,0.6666666666666666,shoulder back mane chest
REFERENCES,0.6690821256038647,forelock hoof
REFERENCES,0.6714975845410628,leg protection
REFERENCES,0.6739130434782609,bridle
REFERENCES,0.6763285024154589,nostril eye 0 20 40 60 80 100 IoU
REFERENCES,0.678743961352657,(a) Horse-21
REFERENCES,0.6811594202898551,background head ear eye nose leg back paw tail chest
REFERENCES,0.6835748792270532,whiskers belly
REFERENCES,0.6859903381642513,tongue mouth neck 0 20 40 60 80 IoU
REFERENCES,0.6884057971014492,"DatasetDDPM
DDPM
DatasetGAN"
REFERENCES,0.6908212560386473,(b) Cat-15
REFERENCES,0.6932367149758454,"background
neck hair teeth"
REFERENCES,0.6956521739130435,forehead cheek
REFERENCES,0.6980676328502415,nose tip chin
REFERENCES,0.7004830917874396,inferior lip
REFERENCES,0.7028985507246377,eyebrow
REFERENCES,0.7053140096618358,bottom lid
REFERENCES,0.7077294685990339,ala of nose
REFERENCES,0.7101449275362319,philtrum nose ear
REFERENCES,0.7125603864734299,bridge
REFERENCES,0.714975845410628,moustache head jaw
REFERENCES,0.717391304347826,"superior lip
iris"
REFERENCES,0.7198067632850241,lobule
REFERENCES,0.7222222222222222,sclera frown helix
REFERENCES,0.7246376811594203,nostril pupil
REFERENCES,0.7270531400966184,top lid
REFERENCES,0.7294685990338164,temple
REFERENCES,0.7318840579710145,tear duct
REFERENCES,0.7342995169082126,sideburns
REFERENCES,0.7367149758454107,eyelashes
REFERENCES,0.7391304347826086,"oral commissure
wrinkles 0 20 40 60 80 IoU"
REFERENCES,0.7415458937198067,(c) FFHQ-34 bed ﬂoor wall
REFERENCES,0.7439613526570048,ceiling
REFERENCES,0.7463768115942029,window
REFERENCES,0.748792270531401,lamp shade
REFERENCES,0.751207729468599,curtain
REFERENCES,0.7536231884057971,picture
REFERENCES,0.7560386473429952,cushion
REFERENCES,0.7584541062801933,pillow
REFERENCES,0.7608695652173914,headboard table
REFERENCES,0.7632850241545893,carpet
REFERENCES,0.7657004830917874,chandelier
REFERENCES,0.7681159420289855,picture frame
REFERENCES,0.7705314009661836,table top
REFERENCES,0.7729468599033816,footboard plant
REFERENCES,0.7753623188405797,lamp column
REFERENCES,0.7777777777777778,table staff door chair
REFERENCES,0.7801932367149759,window frame
REFERENCES,0.782608695652174,plinth
REFERENCES,0.785024154589372,curtain rod pouf
REFERENCES,0.7874396135265701,side rail
REFERENCES,0.7898550724637681,wardrobe 0 20 40 60 80 IoU
REFERENCES,0.7922705314009661,"DatasetDDPM
DDPM
DatasetGAN"
REFERENCES,0.7946859903381642,(d) Bedroom-28
REFERENCES,0.7971014492753623,"Figure 9: Per class IoUs for DatasetGAN, DatasetDDPM and DDPM."
REFERENCES,0.7995169082125604,Published as a conference paper at ICLR 2022
REFERENCES,0.8019323671497585,(a) Bedroom-28 bed wall table
REFERENCES,0.8043478260869565,pillow
REFERENCES,0.8067632850241546,table top
REFERENCES,0.8091787439613527,headboard ﬂoor
REFERENCES,0.8115942028985508,picture
REFERENCES,0.8140096618357487,lamp shade
REFERENCES,0.8164251207729468,lamp column
REFERENCES,0.8188405797101449,ceiling
REFERENCES,0.821256038647343,picture frame
REFERENCES,0.8236714975845411,cushion
REFERENCES,0.8260869565217391,table staff
REFERENCES,0.8285024154589372,footboard
REFERENCES,0.8309178743961353,curtain
REFERENCES,0.8333333333333334,plinth
REFERENCES,0.8357487922705314,side rail
REFERENCES,0.8381642512077294,window
REFERENCES,0.8405797101449275,curtain rod
REFERENCES,0.8429951690821256,window frame
REFERENCES,0.8454106280193237,chandelier chair
REFERENCES,0.8478260869565217,carpet door plant pouf
REFERENCES,0.8502415458937198,wardrobe 0 10 20 30 40
REFERENCES,0.8526570048309179,# masks
REFERENCES,0.855072463768116,"Real
DDPM
GAN"
REFERENCES,0.857487922705314,(b) FFHQ-34
REFERENCES,0.8599033816425121,background
REFERENCES,0.8623188405797102,nose tip
REFERENCES,0.8647342995169082,bridge
REFERENCES,0.8671497584541062,ala of nose nose neck
REFERENCES,0.8695652173913043,oral commissure
REFERENCES,0.8719806763285024,inferior lip jaw
REFERENCES,0.8743961352657005,forehead frown head cheek chin
REFERENCES,0.8768115942028986,tear duct
REFERENCES,0.8792270531400966,eyebrow
REFERENCES,0.8816425120772947,bottom lid
REFERENCES,0.8840579710144928,philtrum
REFERENCES,0.8864734299516909,superior lip
REFERENCES,0.8888888888888888,temple
REFERENCES,0.8913043478260869,sclera iris pupil hair
REFERENCES,0.893719806763285,top lid ear
REFERENCES,0.8961352657004831,lobule
REFERENCES,0.8985507246376812,eyelashes helix
REFERENCES,0.9009661835748792,nostril
REFERENCES,0.9033816425120773,sideburns teeth
REFERENCES,0.9057971014492754,moustache
REFERENCES,0.9082125603864735,wrinkles 0 5 10 15 20
REFERENCES,0.9106280193236715,# masks
REFERENCES,0.9130434782608695,background neck head chest
REFERENCES,0.9154589371980676,muzzle ear eye
REFERENCES,0.9178743961352657,barrel mane back
REFERENCES,0.9202898550724637,nostril
REFERENCES,0.9227053140096618,shoulder leg thigh hoof
REFERENCES,0.9251207729468599,bridle tail
REFERENCES,0.927536231884058,person
REFERENCES,0.9299516908212561,saddle
REFERENCES,0.9323671497584541,forelock
REFERENCES,0.9347826086956522,leg protection 0 5 10 15 20 25 30
REFERENCES,0.9371980676328503,# masks
REFERENCES,0.9396135265700483,"Real
DDPM
GAN"
REFERENCES,0.9420289855072463,(c) Horse-21
REFERENCES,0.9444444444444444,background head eye nose ear chest leg belly back paw
REFERENCES,0.9468599033816425,whiskers neck mouth tail
REFERENCES,0.9492753623188406,tongue 0 5 10 15 20 25 30
REFERENCES,0.9516908212560387,# masks
REFERENCES,0.9541062801932367,"Real
DDPM
GAN"
REFERENCES,0.9565217391304348,(d) Cat-15
REFERENCES,0.9589371980676329,Figure 10: Number of instances of each semantic class in the annotated real and synthetic train sets.
REFERENCES,0.961352657004831,"E
DATASET DETAILS"
REFERENCES,0.9637681159420289,"E.1
CLASS NAMES"
REFERENCES,0.966183574879227,"Bedroom-28: [bed, footboard, headboard, side rail, carpet, ceiling, chandelier, curtain, cushion,
floor, table, table top, picture, pillow, lamp column, lamp shade, wall, window, curtain rod, window
frame, chair, picture frame, plinth, door, pouf, wardrobe, plant, table staff]"
REFERENCES,0.9685990338164251,"FFHQ-34: [background, head, cheek, chin, ear, helix, lobule, bottom lid, eyelashes, iris, pupil,
sclera, tear duct, top lid, eyebrow, forehead, frown, hair, sideburns, jaw, moustache, inferior lip, oral
commissure, superior lip, teeth, neck, nose, ala of nose, bridge, nose tip, nostril, philtrum, temple,
wrinkles]"
REFERENCES,0.9710144927536232,"Cat-15: [background, back, belly, chest, leg, paw, head, ear, eye, mouth, tongue, tail, nose, whiskers,
neck]"
REFERENCES,0.9734299516908212,"Horse-21: [background, person, back, barrel, bridle, chest, ear, eye, forelock, head, hoof, leg, mane,
muzzle, neck, nostril, tail, thigh, saddle, shoulder, leg protection]"
REFERENCES,0.9758454106280193,"CelebA-19: [background, cloth, ear r, eye g, hair, hat, l brow, l ear, l eye, l lip, mouth, neck, neck l,
nose, r brow, r ear, r eye, skin, u lip]"
REFERENCES,0.9782608695652174,"ADE-Bedroom-30: [wall, bed, floor, table, lamp, ceiling, painting, windowpane, pillow, curtain,
cushion, door, chair, cabinet, chest, mirror, rug, armchair, book, sconce, plant, wardrobe, clock,
light, flower, vase, fan, box, shelf, television]"
REFERENCES,0.9806763285024155,"E.2
CLASS STATISTICS"
REFERENCES,0.9830917874396136,"In Figure 10, we report the statistics of classes computed over annotated real images as well as
annotated synthetic images produced by GAN and DDPM."
REFERENCES,0.9855072463768116,Published as a conference paper at ICLR 2022
REFERENCES,0.9879227053140096,"F
EXTRACTING REPRESENTATIONS FROM MAE"
REFERENCES,0.9903381642512077,"To obtain pixelwise representations, we apply the model to a fully observed image (mask ratio=0)
of resolution 256 and extract feature maps from the deepest 12 ViT-L blocks . The feature maps from
each block have 1024×32×32 dimensions. Similarly to other methods, we upsample the extracted
feature maps to 256×256 and concatenate them. The overall dimension of the pixel representation
is 12288."
REFERENCES,0.9927536231884058,"In addition, we investigated other feature extraction strategies and got the following observations:"
REFERENCES,0.9951690821256038,1. Including activations from the decoder did not provide any noticeable gains;
REFERENCES,0.9975845410628019,"2. Extracting activations right after self-attention layers caused slightly inferior performance;
3. Extracting activations from every second encoder block also provided a bit worse results."
