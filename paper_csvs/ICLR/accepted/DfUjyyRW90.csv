Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0030211480362537764,"Model-based reinforcement learning (RL) algorithms designed for handling com-
plex visual observations typically learn some sort of latent state representation,
either explicitly or implicitly. Standard methods of this sort do not distinguish be-
tween functionally relevant aspects of the state and irrelevant distractors, instead
aiming to represent all available information equally. We propose a modiﬁed ob-
jective for model-based RL that, in combination with mutual information maxi-
mization, allows us to learn representations and dynamics for visual model-based
RL without reconstruction in a way that explicitly prioritizes functionally relevant
factors. The key principle behind our design is to integrate a term inspired by
variational empowerment into a state-space model based on mutual information.
This term prioritizes information that is correlated with action, thus ensuring that
functionally relevant factors are captured ﬁrst. Furthermore, the same empower-
ment term also promotes faster exploration during the RL process, especially for
sparse-reward tasks where the reward signal is insufﬁcient to drive exploration in
the early stages of learning. We evaluate the approach on a suite of vision-based
robot control tasks with natural video backgrounds, and show that the proposed
prioritized information objective outperforms state-of-the-art model based RL ap-
proaches with higher sample efﬁciency and episodic returns."
INTRODUCTION,0.006042296072507553,"1
INTRODUCTION"
INTRODUCTION,0.00906344410876133,"Model-based reinforcement learning (RL) provides a promising approach to accelerating skill learn-
ing: by acquiring a predictive model that represents how the world works, an agent can quickly
derive effective strategies, either by planning or by simulating synthetic experience under the model.
However, in complex environments with high-dimensional observations (e.g., images), modeling the
full observation space can present major challenges. While large neural network models have made
progress on this problem (Finn & Levine, 2017; Ha & Schmidhuber, 2018; Hafner et al., 2019a;
Watter et al., 2015; Babaeizadeh et al., 2017), effective learning in visually complex environments
necessitates some mechanism for learning representations that prioritize functionally relevant fac-
tors for the current task. This needs to be done without wasting effort and capacity on irrelevant
distractors, and without detailed reconstruction. Several recent works have proposed contrastive
objectives that maximize mutual information between observations and latent states (Hjelm et al.,
2018; Ma et al., 2020; Oord et al., 2018; Srinivas et al., 2020). While such objectives avoid recon-
struction, they still do not distinguish between relevant and irrelevant factors of variation. We thus
pose the question: can we devise non-reconstructive representation learning methods that explicitly
prioritize information that is most likely to be functionally relevant to the agent?"
INTRODUCTION,0.012084592145015106,"In this work, we derive a model-based RL algorithm from a combination of representation learning
via mutual information maximization (Poole et al., 2019) and empowerment (Mohamed & Rezende,
2015). The latter serves to drive both the representation and the policy toward exploring and repre-
senting functionally relevant factors of variation. By integrating an empowerment-based term into a"
INTRODUCTION,0.015105740181268883,∗Work done during Homanga’s research internship at Google. hbharadh@cs.cmu.edu
INTRODUCTION,0.01812688821752266,Published as a conference paper at ICLR 2022
INTRODUCTION,0.021148036253776436,"Figure 1: Overview of InfoPower. I(Ot; Zt) is the contrastive learning objective for learning an encoder
to map from image O to latent Z. I(At−1; Zt|Zt−1) is the empowerment objective that prioritizes encoding
controllable representations in Z. −I(it+1; Zt+1|Zt, At) helps learn a latent forward dynamics model so that
future Zt+k can be predicted from current Zt. I(Rt; Zt) helps learn a reward prediction model, such that the
agent can learn a plan At, ..At+k, .. through latent rollouts. Together, this combination of terms produces a
latent state space model for MBRL that captures all necessary information at convergence, while prioritizing
the most functionally relevant factors via the empowerment term."
INTRODUCTION,0.02416918429003021,"mutual information framework for learning state representations, we effectively prioritize informa-
tion that is most likely to have functional relevance, which mitigates distractions due to irrelevant
factors of variation in the observations. By integrating this same term into policy learning, we fur-
ther improve exploration, particularly in the early stages of learning in sparse-reward environments,
where the reward signal provides comparatively little guidance."
INTRODUCTION,0.027190332326283987,"Our main contribution is InfoPower, a model-based RL algorithm for control from image ob-
servations that integrates empowerment into a mutual information based, non-reconstructive frame-
work for learning state space models. Our approach explicitly prioritizes information that is most
likely to be functionally relevant, which signiﬁcantly improves performance in the presence of time-
correlated distractors (e.g., background videos), and also accelerates exploration in environments
when the reward signal is weak. We evaluate the proposed objectives on a suite of simulated robotic
control tasks with explicit video distractors, and demonstrate up to 20% better performance in terms
of cumulative rewards at 1M environment interactions with 30% higher sample efﬁciency at 100k
interactions."
PROBLEM STATEMENT AND NOTATION,0.030211480362537766,"2
PROBLEM STATEMENT AND NOTATION"
PROBLEM STATEMENT AND NOTATION,0.03323262839879154,"A partially observed Markov decision process (POMDP) is a tuple (S, A, T, R, O) that consists of
states s ∈S, actions a ∈A, rewards r ∈R, observations o ∈O, and a state-transition distribu-
tion T(s′|s, a). In most practical settings, the agent interacting with the environment doesn’t have
access to the actual states in S, but to some partial information in the form of observations O. The
underlying state-transition distribution T and reward distribution R are also unknown to the agent."
PROBLEM STATEMENT AND NOTATION,0.03625377643504532,"In this paper, we consider the observations o ∈O to be high-dimensional images, and so the agent
should learn a compact representation space Z for the latent state-space model. The problem state-
ment is to learn effective representations from observations O when there are visual distractors
present in the scene, and plan using the learned representations to maximize the cumulative sum
of discounted rewards, J = E[P"
PROBLEM STATEMENT AND NOTATION,0.03927492447129909,"t γt−1rt]. The value of a state V (Zt) is deﬁned as the expected
cumulative sum of discounted rewards starting at state Zt."
PROBLEM STATEMENT AND NOTATION,0.04229607250755287,"We use q(·) to denote parameterized variational approximations to learned distributions.
We
denote random variables with capital letters and use lowercase letters to denote particular re-
alizations (e.g., zt denotes the value of Zt).
Since the underlying distributions are unknown,
we evaluate all expectations through Monte-Carlo sampling with observed state-transition tuples
(ot, at−1, ot−1, zt, zt−1, rt)."
PROBLEM STATEMENT AND NOTATION,0.045317220543806644,Published as a conference paper at ICLR 2022
INFORMATION PRIORITIZATION FOR THE LATENT STATE-SPACE MODEL,0.04833836858006042,"3
INFORMATION PRIORITIZATION FOR THE LATENT STATE-SPACE MODEL"
INFORMATION PRIORITIZATION FOR THE LATENT STATE-SPACE MODEL,0.0513595166163142,"Our goal is to learn a latent state-space model with a representation Z that prioritizes capturing
functionally relevant parts of observations O, and devise a planning objective that explores with
the learned representation. To achieve this, our key insight is integration of empowerment in the
visual model-based RL pipeline. For representation learning we maximize MI maxZ I(O, Z) sub-
ject to a prioritization of the empowerment objective maxZ I(At−1; Zt|Zt−1). For planning, we
maximize the empowerment objective along with reward-based value with respect to the policy
maxA I(At−1; Zt|Zt−1) + I(Rt; Zt)."
INFORMATION PRIORITIZATION FOR THE LATENT STATE-SPACE MODEL,0.054380664652567974,"In the subsequent sections, we elaborate on our approach, InfoPower, and describe lower bounds
to MI that yield a tractable algorithm."
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.05740181268882175,"3.1
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT"
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.06042296072507553,"Figure 2: PGM showing decomposition of state S into con-
trollable parts S+ (directly inﬂuenced by actions A), parts
not inﬂuenced by actions that still inﬂuence the reward, ˜S−,
and distractors DS−. ˜S−
t+1 may be inﬂuenced by ˜S−
t (arrow
not shown to reduce clutter) but not by ˜S+
t ."
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.0634441087613293,"Controllable representations are features
of the observation that correspond to enti-
ties which the agent can inﬂuence through
its actions. For example, in quadrupedal
locomotion, this could include the joint
positions, velocities, motor torques, and
the conﬁgurations of any object in the en-
vironment that the robot can interact with.
For robotic manipulation, it could include
the joint actuators of the robot arm, and the
conﬁgurations of objects in the scene that
it can interact with. Such representations
are denoted by S+ in Fig. 2, which we can
formally deﬁne through conditional inde-
pendence as the smallest subspace of S,
S+ ≤S, such that I(At−1; St|S+
t ) =
0. This conditional independence relation
can be seen in Fig. 2. We explicitly priori-
tize the learning of such representations in the latent space by drawing inspiration from variational
empowerment (Mohamed & Rezende, 2015)."
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.06646525679758308,"The empowerment objective can be cast as maximizing a conditional information term
I(At−1; Zt|Zt−1) = H(At−1|Zt−1) −H(At−1|Zt, Zt−1). The ﬁrst term H(At−1|Zt−1) encour-
ages the chosen actions to be as diverse as possible, while the second term −H(At−1|Zt, Zt−1) en-
courages the representations Zt and Zt+1 to be such that the action At for transition is predictable.
While prior approaches have used empowerment in the model-free setting to learn policies by explo-
ration through intrinsic motivation (Mohamed & Rezende, 2015), we speciﬁcally use this objective
in combination with MI maximization for prioritizing the learning of controllable representations
from distracting images in the latent state-space model."
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.06948640483383686,"We include the same empowerment objective in both representation learning and policy learning.
For this, we augment the maximization of the latent value function that is standard for policy learn-
ing in visual model-based RL (Sutton, 1991), with maxA I(At−1; Zt|Zt−1). This objectives com-
plements value based-learning and further improves exploration by seeking controllable states. We
empirically analyze the beneﬁts of this in sections 4.3 and 4.5."
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.07250755287009064,"In Appendix A.1 we next describe two theorems regarding learning controllable representations. We
observe that the max P"
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.0755287009063444,"t I(At−1; Zt|Zt−1) objective for learning latent representations Z, when
used along with the planning objective, provably recovers controllable parts of the observation O,
namely S+. This result in Theorem 1 is important because in practice, we may not be able to
represent every possible factor of variation in a complex environment. In this situation, we would
expect that when |Z| ≪|O|, learning Z under the objective max P
t I(At−1; Zt|Zt−1) would
encode S+."
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.07854984894259819,"We further show through Theorem 2 that the inverse information objective alone can be used to train
a latent-state space model and a policy through an alternating optimization algorithm that converges
to a local minimum of the objective max P"
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.08157099697885196,t I(At−1; Zt|Zt−1) at a rate inversely proportional to
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.08459214501510574,Published as a conference paper at ICLR 2022
LEARNING CONTROLLABLE FACTORS AND PLANNING THROUGH EMPOWERMENT,0.08761329305135952,"the number of iterations. In Section 4.3 we empirically show how this objective helps achieve higher
sample efﬁciency compared to pure value-based policy learning."
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.09063444108761329,"3.2
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.09365558912386707,"Algorithm 1: Information Prioritization in Visual
Model-based RL (InfoPower)"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.09667673716012085,"Initialize dataset D with random episodes.
Initialize model parameters φ, χ, ψ, η.
Initialize dual variable λ.
while not converged do"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.09969788519637462,for update step c = 1..C do
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.1027190332326284,"// Model learning
Sample data {(at, ot, rt)}k+L
t=k ∼D.
Compute latents zt ∼pφ(zt|zt−1, at−1, ot).
Calculate L based on section 3.4.
(φ, χ, ψ, η) ←(φ, χ, ψ, η) + ∇φ,χ,ψ,ηL
λ ←λ −∇λL"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.10574018126888217,"// Behavior learning
Rollout latent plan, S ←S ∪{zt, at, rt}
V (zt) ≈Eπ[ln qη(rt|zt)+ln qψ(at−1|zt, zt−1)]
Update policy π and value model
end"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.10876132930513595,"// Environment interaction
for time step t = 0..T −1 do"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.11178247734138973,"zt ∼pφ(zt|zt−1, at−1, ot); at ∼π(at|zt)
rt, ot+1 ←env.step(at).
end
Add data D ←D ∪{(ot, at, rt)T
t=1}. end"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.1148036253776435,"For
visual
model-based
RL,
we
need to learn a representation space
Z, such that a forward dynamics
model deﬁning the probability of the
next state in terms of the current
state and the current action can be
learned.
The objective for this is
P"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.11782477341389729,"t −I(it; Zt|Zt−1, At−1). Here, it
denotes the dataset indices that de-
termine the observations p(ot|it) =
δ(ot −ot′). In addition to the for-
ward dynamics model, we need to
learn a reward predictor by maximiz-
ing P"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.12084592145015106,"t I(Rt; Zt), such that the agent
can plan ahead in the future by rolling
forward latent states, without having
to execute actions and observe re-
wards in the real environment."
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.12386706948640483,"Finally, we need to learn an en-
coder for encoding observations O
to latents Z.
Most successful
prior works have used reconstruction-
loss
as
a
natural
objective
for
learning this encoder (Babaeizadeh
et al., 2017; Hafner et al., 2019b;a).
A reconstruction-loss can be moti-
vated by considering the objective
I(O, Z) and computing its BA lower
bound (Agakov, 2004). I(ot; zt) ≥Ep(ot,zt)[log qφ′(ot|zt)] + H(p(ot)). The ﬁrst term here is the
reconstruction objective, with qφ′(ot|zt) being the decoder, and the second term can be ignored as
it doesn’t depend on Z. However, this reconstruction objective explicitly encourages encoding the
information from every pixel in the latent space (such that reconstructing the image is possible) and
hence is prone to not ignoring distractors."
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.1268882175226586,"In contrast, if we consider other lower bounds to I(O, Z), we can obtain tractable objectives
that do not involve reconstructing high-dimensional images. We can obtain an NCE-based lower
bound (Hjelm et al., 2018): I(ot; zt) ≥Eqφ(zt|ot)p(ot)[log fθ(zt, ot) −log P"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.1299093655589124,"t′̸=t fθ(zt, ot′)], where
qφ(zt|ot) is the learned encoder, ot is the observation at timestep t (positive sample), and all observa-
tions in the replay buffer ot′ are negative samples. fθ(zt, ot′) = exp (zT
t Wθzt′) The lower-bound is
a form of contrastive learning as it maximizes compatibility of zt with the corresponding observation
ot while minimizing compatibility with all other observations across time and batch."
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.13293051359516617,"Although prior work has explored NCE-based bounds for contrastive learning in RL (Srinivas et al.,
2020), to the best of our knowledge, prior work has not used this in conjunction with empowerment
for prioritizing information in visual model-based RL. Similarly, the Nguyen-Wainwright-Jordan
(NWJ) bound (Nguyen et al., 2010), which to the best our knowledge has not been used by prior
works in visual model-based RL, can be obtained as,
I(ot; zt) ≥Eqφ(zt|ot)p(ot)[fθ(zt, ot)] −e−1Eqφ(zt|ot)p(ot)efθ(zt,ot),
where fθ is a critic. There exists an optimal critic function for which the bound is tightest and
equality holds."
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.13595166163141995,"We refer to the InfoNCE and NWJ lower bound based objectives as contrastive learning, in order
to distinguish them from a reconstruction-loss based objective, though both are bounds on mutual"
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.13897280966767372,Published as a conference paper at ICLR 2022
MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING,0.1419939577039275,"information. We denote a lower bound to MI by I(ot, zt). We empirically ﬁnd the NWJ-bound to
perform slightly better than the NCE-bound for our approach, explained in section 4.5."
OVERALL OBJECTIVE,0.14501510574018128,"3.3
OVERALL OBJECTIVE"
OVERALL OBJECTIVE,0.14803625377643503,"We now motivate the overall objective, which consists of maximizing mutual information while pri-
oritizing the learning of controllable representations through empowerment in a latent state-space
model. Based on the discussions in Sections 3.1 and 3.2, we deﬁne the overall objective for repre-
sentation learning as"
OVERALL OBJECTIVE,0.1510574018126888,"max
Z0:H−1 H−1
X"
OVERALL OBJECTIVE,0.1540785498489426,"t=0
I(Ot; Zt) s.t. H−1
X t=0"
OVERALL OBJECTIVE,0.15709969788519637,"Ct
z
}|
{
(−I(it; Zt|Zt−1, At−1) + I(At−1; Zt|Zt−1) + I(Rt; Zt)) ≥c0."
OVERALL OBJECTIVE,0.16012084592145015,"The objective is to maximize a MI term I(Ot; Zt) through contrastive learning such that a constraint
on Ct holds for prioritizing the encoding of forward-predictive, reward-predictive and controllable
representations. We deﬁne the overall planning objective as"
OVERALL OBJECTIVE,0.16314199395770393,"max
A0:H−1 H−1
X"
OVERALL OBJECTIVE,0.1661631419939577,"t=0
I(At−1; Zt|Zt−1) + V (Zt)
; At = π(Zt)
; V (Zt) ≈
X t
Rt."
OVERALL OBJECTIVE,0.1691842900302115,"The planning objective is to learn a policy as a function of the latent state Z such that the empower-
ment term and a reward-based value term are maximized over the horizon H."
OVERALL OBJECTIVE,0.17220543806646527,"We can perform the constrained optimization for representation learning through the method of
Lagrange Multipliers, by the primal and dual updates shown in Section A.2. In order to analyze this
objective, let |O| = n and |Z| = d. Since, O corresponds to images and Z is a bottlenecked latent
representation, d ≪n."
OVERALL OBJECTIVE,0.17522658610271905,"I(O, Z) is maximized when Z contains all the information present in O such that Z is a sufﬁ-
cient statistic of O. However, in practice, this is not possible because |Z| ≪|O|. When c0 is
sufﬁciently large, and the constraint P"
OVERALL OBJECTIVE,0.1782477341389728,"t Ct ≥c0 is satisﬁed, Z = [S+, ˜S−]. Hence, the objective
max P"
OVERALL OBJECTIVE,0.18126888217522658,"t I(Ot, Zt)
s.t.
P"
OVERALL OBJECTIVE,0.18429003021148035,"t Ct ≥c0 cannot encode anything else in Z, in particular it cannot
encode distractors DS−."
OVERALL OBJECTIVE,0.18731117824773413,"To understand the importance of prioritization through Ct ≥c0, consider max P
t I(Ot, Zt) without
the constraint. This objective would try to make Z a sufﬁcient statistic of O, but since |Z| ≪|O|,
there are no guarantees about which parts of O are getting encoded in Z. This is because both
distractors S−and non-distractors S+, D ˜S−are equally important with respect to I(O, Z). Hence,
the constraint helps in prioritizing the type of information to be encoded in Z."
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.1903323262839879,"3.4
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS"
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.1933534743202417,"To arrive at a practical algorithm, we optimize the overall objective in section 3.3 through lower
bounds on each of the MI terms. For I(O, Z) we consider two variants, corresponding to the NCE
and NWJ lower bounds described in Section 3.2. We can obtain a variational lower bound on each
of the terms in Ct as follows, with detailed derivations in Appendix A.3:"
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.19637462235649547,"−I(it; zt|at−1) ≥−
X"
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.19939577039274925,"t
E[DKL(p(zt|zt−1, at−1, ot)||qχ(zt|zt−1, at−1))]"
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.20241691842900303,"I(rt; zt) ≥Ep(rt|ot)[log qη(rt|zt)] + H(p(rt))
I(at−1; zt|zt−1) ≥Ep(ot|zt−1,at−1)qφ(zt|ot)[log qψ(at−1|zt, zt−1)] + E[H(π(at−1|zt−1))]"
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.2054380664652568,"Here, qφ(zt|ot) is the observation encoder, qψ(at−1|zt, zt−1) is the inverse dynamics model,
qη(rt|zt) is the reward decoder, and qχ(zt|zt−1, at−1)) is the forward dynamics model. The inverse
model helps in learning representations such that the chosen action is predictable from successive
latent states. The forward dynamics model helps in predicting the next latent state given the current
latent state and the current action, without having the next observation. The reward decoder predicts
the reward (a scalar) at a time-step given the corresponding latent state. We use dense networks for
all the models, with details in Appendix A.7. We denote by Ct, the lower bound to Ct based on the
sum of the terms above. We construct a Lagrangian L = P"
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.2084592145015106,"t I(ot; zt) + λ (Ct - c0) and optimize it
by primal-dual gradient descent. An outline of this is shown in Algorithm 1."
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.21148036253776434,Published as a conference paper at ICLR 2022
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.21450151057401812,"Planning Objective. For planning to choose actions at every time-step, we learn a policy π(a|z)
through value estimates of task reward and the empowerment objective. We learn value estimates
with V (zt) ≈Eπ[ln qη(rt|zt) + ln qψ(at−1|zt, zt−1)]. We estimate V (zt) similar to Equation 6
of (Hafner et al., 2019a). The empowerment term qψ(at−1|zt, zt−1) in policy learning incentivizes
choosing actions at−1 such that they can be predicted from consecutive latent states zt−1, zt. This
biases the policy to explore controllable regions of the state-space. The policy is trained to maximize
the estimate of the value, while the value model is trained to ﬁt the estimate of the value that changes
as the policy is updated."
PRACTICAL ALGORITHM AND IMPLEMENTATION DETAILS,0.2175226586102719,"Finally, we note that the difference in value function of the underlying MDP Qπ(o, a) and the latent
MDP ˆQπ(z, a), where z ∼qφ(z|o) is bounded, under some regularity assumptions. We provide
this result in Theorem 3 of the Appendix Section A.4. The overall procedure for model learning,
planning, and interacting with the environment is outlined in Algorithm 1."
EXPERIMENTS,0.22054380664652568,"4
EXPERIMENTS"
EXPERIMENTS,0.22356495468277945,"Through our experiments, we aim to understand the following research questions:"
HOW DOES INFOPOWER COMPARE WITH THE BASELINES IN TERMS OF EPISODIC RETURNS IN ENVIRONMENTS,0.22658610271903323,"1. How does InfoPower compare with the baselines in terms of episodic returns in environments
with explicit background video distractors?
2. How sample efﬁcient is InfoPower when the reward signal is weak (< 100k env steps when
the learned policy typically doesn’t achieve very high rewards)?
3. How does InfoPower compare with baselines in terms of behavioral similarity of latent states?"
HOW DOES INFOPOWER COMPARE WITH THE BASELINES IN TERMS OF EPISODIC RETURNS IN ENVIRONMENTS,0.229607250755287,"Please
refer
to
the
website
for
a
summary
and
qualitative
visualization
results
https://sites.google.com/view/information-empowerment"
SETUP,0.2326283987915408,"4.1
SETUP"
SETUP,0.23564954682779457,"We perform experiments with modiﬁed DeepMind Control Suite environments (Tassa et al., 2018),
with natural video distractors from ILSVRC dataset (Russakovsky et al., 2015) in the background.
The agent receives only image observations at each time-step and does not receive the ground-
truth simulator state. This is a very challenging setting, because the agents must learn to ignore
the distractors and abstract out representations necessary for control. While natural videos that are
unrelated to the task might be easy to ignore, realistic scenes might have other elements that resemble
the controllable elements, but are not actually controllable (e.g., other cars in a driving scenario). To
emulate this, we also add distractors that resemble other potentially controllable robots, as shown
for example in Fig. 3 (1st and 6th), but are not actually inﬂuenced by actions."
SETUP,0.23867069486404835,"Figure 3: Some of the natural video background
distractors in our experiments. The videos change
every 50 time-steps. Some backgrounds (for e.g.
the top left and the bottom right) have more com-
plex distractors in the form of agent-behind-agent
i.e. the background has pre-recorded motion of a
similar agent that is being controlled."
SETUP,0.24169184290030213,"In addition to this setting, we also perform exper-
iments with gray-scale distractors based on videos
from the Kinetics dataset (Kay et al., 2017). This
setting is adapted exactly based on prior works (Fu
et al., 2021; Zhang et al., 2020) and we compare di-
rectly to the published results."
SETUP,0.24471299093655588,"We compare InfoPower with state-of-the art base-
lines that also learn world models for control:
Dreamer (Hafner et al., 2019a), C-Dreamer that is a
contrastive version of Dreamer similar to Ma et al.
(2020), TIA (Fu et al., 2021) that learns explicit
reconstructions for both the distracting background
and agent separately, DBC (Zhang et al., 2020), and
DeepMDP (Gelada et al., 2019). We also compare
variants of InfoPower with different MI bounds
(NCE and NWJ), and ablations of it that remove the
empowerment objective from policy learning."
A BEHAVIORAL SIMILARITY METRIC BASED ON GRAPH KERNELS,0.24773413897280966,"4.2
A BEHAVIORAL SIMILARITY METRIC BASED ON GRAPH KERNELS"
A BEHAVIORAL SIMILARITY METRIC BASED ON GRAPH KERNELS,0.25075528700906347,"In order to measure how good the learned latent representations are at capturing the functionally
relevant elements in the scene, we introduce a behavioral similarity metric with details in A.5. Given"
A BEHAVIORAL SIMILARITY METRIC BASED ON GRAPH KERNELS,0.2537764350453172,Published as a conference paper at ICLR 2022
A BEHAVIORAL SIMILARITY METRIC BASED ON GRAPH KERNELS,0.256797583081571,"Figure 4: Evaluation of InfoPower and baselines in a suite of DeepMind Control tasks with natural video
distractors in the background. The x-axis denotes the number of environment interactions and the y-axis shows
the episodic returns. The S.D is over 4 random seeds. Higher is better."
A BEHAVIORAL SIMILARITY METRIC BASED ON GRAPH KERNELS,0.2598187311178248,"the sets Sz = {zi}n
i=1 and Szgt = {zi
gt}n
i=1, we construct complete graphs by connecting every vertex
with every other vertex through an edge. The weight of an edge is the Euclidean distance between
the respective pairs of vertices. The label of each vertex is an unique integer and corresponding
vertices in the two graphs are labelled similarly."
A BEHAVIORAL SIMILARITY METRIC BASED ON GRAPH KERNELS,0.2628398791540785,"Let the resulting graphs be denoted as Gz = (Vz, Ez) and Gzgt = (Vzgt, Ezgt) respectively. Note that
both these graphs are shortest path graphs, by construction. Let, ei = {ui, vi} and ej = {uj, vj}.
We deﬁne a kernel ˆk that measures similarity between the edges ei, ej and the labels on respective
vertices. ˆk(ei, ej) =
kv(l(ui), l(uj))ke(l(ei), l(ej))kv(l(vi), l(vj)) + kv(l(ui), l(vj))ke(l(ei), l(ej))kv(l(vi), l(uj))
Here, the function l(·) denotes the labels of vertices and the weights of edges. kv is a Dirac delta
kernel i.e. ke(x, y) = 1 −δ(x, y) (Note that δ(x, y) = 1 iff x = y, else δ(x, y) = 0) and ke
is a Brownian ridge kernel, ke(x, y) =
1
c max(0, c −|x −y|), where c is a large number. We
deﬁne the shortest path kernel to measure similarity between the two graphs, as, k(Gz, Gzgt) =
1
|Ez|
P"
A BEHAVIORAL SIMILARITY METRIC BASED ON GRAPH KERNELS,0.26586102719033233,"ei∈Ez
P"
A BEHAVIORAL SIMILARITY METRIC BASED ON GRAPH KERNELS,0.2688821752265861,"ej∈Ezgt ˆk(ei, ej). The value of k(Gz, Gzgt) is low when a large number of pairs
of corresponding vertices in both the graphs have the same edge length. We expect methods that
recover latent state representations which better reﬂect the true underlying simulator state (i.e., the
positions of the joints) to have higher values according to this metric."
SAMPLE EFFICIENCY ANALYSIS,0.2719033232628399,"4.3
SAMPLE EFFICIENCY ANALYSIS"
SAMPLE EFFICIENCY ANALYSIS,0.27492447129909364,"In Fig. 4, we compare InfoPower and the baselines in terms of episodic returns. This version
of InfoPower corresponds to an NWJ bound on MI which we ﬁnd works slightly better than the
NCE bound variant analyzed in section 4.5. It is evident that InfoPower achieves higher returns
before 1M steps of training quickly compared to the baselines, indicating higher sample efﬁciency.
This suggests the effectiveness of the empowerment model in helping capture controllable represen-
tations early on during training, when the agent doesn’t take on actions that yield very high rewards.
4.4
BEHAVIORAL SIMILARITY OF LATENT STATES"
SAMPLE EFFICIENCY ANALYSIS,0.27794561933534745,"In this section, we analyze how similar are the learned latent representations with respect to the
underlying simulator states. The intuition for this comparison is that the proprioceptive features
in the simulator state abstract out distractors in the image, and so we want the latent states to be
behaviorally similar to the simulator states."
SAMPLE EFFICIENCY ANALYSIS,0.2809667673716012,"Quantitative results with the deﬁned metric. In Table 1, we show results for behavioral simi-
larity of latent states (Sim), based on the metric in section 4.2. We see that the value of Sim for
InfoPower is around 20% higher than the most competitive baseline, indicating high behavioral
similarity of the latent states with respect to the corresponding ground-truth simulator states."
SAMPLE EFFICIENCY ANALYSIS,0.283987915407855,"Qualitative visualizations with t-sne. Fig. 5 shows a t-SNE plot of latent states z ∼qφ(z|o) for
InfoPower and the baselines with visualizations of 3 nearest neighbors for two randomly chosen
latent states. We see that the state of the agent is similar is each group for InfoPower, although the"
SAMPLE EFFICIENCY ANALYSIS,0.28700906344410876,Published as a conference paper at ICLR 2022
SAMPLE EFFICIENCY ANALYSIS,0.29003021148036257,"Figure 5: t-SNE plot of latent states z ∼qφ(z|o) with visualizations of three nearest neighbors for two ran-
domly sampled points (in red frame). We see that the state of the agent is similar is each set for InfoPower,
whereas for Dreamer, and the most competitive baseline C-Dreamer, the nearest neighbor frames have signiﬁ-
cantly different agent conﬁgurations."
SAMPLE EFFICIENCY ANALYSIS,0.2930513595166163,"Figure 6: Evaluation of InfoPower and ablated variants in a suite of DeepMind Control tasks with natural
video distractors in the background. The x-axis denotes the number of environment interactions and the y-
axis shows the episodic returns. InfoPower-NWJ and InfoPower-NCE are full versions of our method
differing only in the MI lower bound. The versions with - Policy do not include the empowerment objective in
policy learning, but only use it from representation learning. The S.D is over 4 random seeds. Higher is better."
SAMPLE EFFICIENCY ANALYSIS,0.29607250755287007,"background scenes are signiﬁcantly different. However, for the baselines, the nearest neighbor states
are signiﬁcantly different in terms of the pose of the agent, indicating that the latent representations
encode signiﬁcant background information."
ABLATION STUDIES,0.2990936555891239,"4.5
ABLATION STUDIES"
ABLATION STUDIES,0.3021148036253776,"In Fig. 6, we compare different ablations of InfoPower. Keeping everything else the same, and
changing only the MI lower bound to NCE, we see that the performance is almost similar or slightly
worse. However, when we remove the empowerment objective from policy optimization (the ver-
sions with ‘-Policy’ in the plot), we see that performance drops. The drop is signiﬁcant in the regions
< 200k environment interactions, particularly in the sparse reward environments - cartpole balance
and ball in a cup, indicating the necessity of the empowerment objective in exploration for learning
controllable representations, when the reward signal is weak."
ABLATION STUDIES,0.30513595166163143,Published as a conference paper at ICLR 2022
RELATED WORKS,0.3081570996978852,"5
RELATED WORKS"
RELATED WORKS,0.311178247734139,"Visual model-based RL. Recent developments in video prediction and contrastive learning have
enabled learning of world-models from images (Watter et al., 2015; Babaeizadeh et al., 2017; Finn
& Levine, 2017; Hafner et al., 2019a; Ha & Schmidhuber, 2018; Hafner et al., 2019b; Xie et al.,
2020). All of these approaches learn latent representations through reconstruction objectives that
are amenable for planning. Other approaches have used similar reconstruction based objectives for
control, but not for MBRL (Lee et al., 2019; Gregor et al., 2019)."
RELATED WORKS,0.31419939577039274,"MI for representation learning. Mutual Information measures the dependence between two ran-
dom variables. The task of learning latent representations Z from images O for downstream appli-
cations, has been very successful with MI objectives of the form maxf1,f2 I(f1(O), f2(Z)) (Hjelm
& Bachman, 2020; Tian et al., 2020; Oord et al., 2018; Tschannen et al., 2019; Nachum & Yang,
2021). Since calculating MI exactly is intractable optimizing MI based objectives, it is important
to construct appropriate MI estimators that lower-bound the true MI objective (Hjelm et al., 2018;
Nguyen et al., 2010; Belghazi et al., 2018; Agakov, 2004). The choice of the estimator is crucial,
as shown by recent works (Poole et al., 2019), and different estimators yield very different behav-
iors of the algorithm. We incorporate MI maximization through the NCE (Hjelm et al., 2018) and
NWJ (Nguyen et al., 2010) lower bounds, such that typical reconstruction objectives for representa-
tion learning which do not perform well with visual distractors, can be avoided."
RELATED WORKS,0.31722054380664655,"Inverse models and empowerment. Prior approaches have used inverse dynamics models for reg-
ularization in representation learning (Agrawal et al., 2016; Zhang et al., 2018) and as bonuses
for improving policy gradient updates in RL (Shelhamer et al., 2016; Pathak et al., 2017). The
importance of information theoretic approaches for learning representations that maximize predic-
tive power has been discussed in prior work (Still, 2009) and more recently in Lee et al. (2020).
Empowerment (Mohamed & Rezende, 2015) has been used as exploration bonuses for policy learn-
ing (Leibfried et al., 2019; Klyubin et al., 2008), and for learning skills in RL (Gregor et al., 2016;
Sharma et al., 2019; Eysenbach et al., 2018). In contrast to prior work, we incorporate empower-
ment both for state space representation learning and policy learning, in a visual model-based RL
framework, with the aim of prioritizing the most functionally relevant information in the scene."
RELATED WORKS,0.3202416918429003,"RL with environment distractors. Some recent RL frameworks have studied the problem of ab-
stracting out only the task relevant information from the environment when there are explicit dis-
tractors (Hansen & Wang, 2020; Zhang et al., 2020; Fu et al., 2021; Ma et al., 2020). Zhang et al.
(2020) constrain the latent states by enforcing a bisimulation metric, without a reconstruction ob-
jective. Our approach is different from this primarily because of the empowerment objective that
provides useful signal for dealiasing controllable vs. uncontrollable representations independent of
how strong is the observed reward signal. Fu et al. (2021) model both the relevant and irrelevant
aspects of the environment separately, and differ from our approach that prioritizes learning only the
relevant aspects. (Shu et al., 2020) was and earlier approach that used contrastive representations
for control. More recently, (Nguyen et al., 2021) used temporal predictive coding and (Ma et al.,
2020) used InfoNCE based contrastive loss for learning representations while ignoring distractors in
visual MBRL. Incorporating data augmentations for improving robustness with respect to environ-
ment variations is an orthogonal line of work (Laskin et al., 2020; Hansen & Wang, 2020; Raileanu
et al., 2020; Srinivas et al., 2020; Kostrikov et al., 2020), complementary to our approach."
CONCLUSION,0.32326283987915405,"6
CONCLUSION"
CONCLUSION,0.32628398791540786,"In this paper we derived an approach for visual model-based RL such that an agent can learn a latent
state-space model and a policy by explicitly prioritizing the encoding of functionally relevant factors.
Our prioritized information objective integrates a term inspired by variational empowerment into a
non-reconstructive objective for learning state space models. We evaluate our approach on a suite
of vision-based robot control tasks with two sets of challenging video distractor backgrounds. In
comparison to state-of-the-art visual model-based RL methods, we observe higher sample efﬁciency
and episodic returns across different environments and distractor settings."
CONCLUSION,0.3293051359516616,Published as a conference paper at ICLR 2022
REFERENCES,0.3323262839879154,REFERENCES
REFERENCES,0.33534743202416917,"David Barber Felix Agakov. The im algorithm: a variational approach to information maximization.
Advances in neural information processing systems, 16(320):201, 2004."
REFERENCES,0.338368580060423,"Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke
by poking: Experiential learning of intuitive physics. arXiv preprint arXiv:1606.07419, 2016."
REFERENCES,0.3413897280966767,"Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine.
Stochastic variational video prediction. arXiv preprint arXiv:1710.11252, 2017."
REFERENCES,0.34441087613293053,"Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference
on Machine Learning, pp. 531–540. PMLR, 2018."
REFERENCES,0.3474320241691843,"Richard Blahut. Computation of channel capacity and rate-distortion functions. IEEE transactions
on Information Theory, 18(4):460–473, 1972."
REFERENCES,0.3504531722054381,"Thomas M. Cover and Joy A. Thomas. Elements of Information Theory 2nd Edition (Wiley Series in
Telecommunications and Signal Processing). Wiley-Interscience, July 2006. ISBN 0471241954."
REFERENCES,0.35347432024169184,"Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018."
REFERENCES,0.3564954682779456,"Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE
International Conference on Robotics and Automation (ICRA), pp. 2786–2793. IEEE, 2017."
REFERENCES,0.3595166163141994,"Xiang Fu, Ge Yang, Pulkit Agrawal, and Tommi Jaakkola. Learning task informed abstractions. In
International Conference on Machine Learning, pp. 3480–3491. PMLR, 2021."
REFERENCES,0.36253776435045315,"Carles Gelada, Saurabh Kumar, Jacob Buckman, Oﬁr Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, pp. 2170–2179. PMLR, 2019."
REFERENCES,0.36555891238670696,"Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016."
REFERENCES,0.3685800604229607,"Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and Aaron
van den Oord. Shaping belief states with generative environment models for rl. arXiv preprint
arXiv:1906.09237, 2019."
REFERENCES,0.3716012084592145,"David Ha and J¨urgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018."
REFERENCES,0.37462235649546827,"Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a."
REFERENCES,0.3776435045317221,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555–2565. PMLR, 2019b."
REFERENCES,0.3806646525679758,"Nicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmen-
tation. arXiv preprint arXiv:2011.13389, 2020."
REFERENCES,0.38368580060422963,"R Devon Hjelm and Philip Bachman. Representation learning with video deep infomax. arXiv
preprint arXiv:2007.13278, 2020."
REFERENCES,0.3867069486404834,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018."
REFERENCES,0.38972809667673713,"Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-
narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action
video dataset. arXiv preprint arXiv:1705.06950, 2017."
REFERENCES,0.39274924471299094,Published as a conference paper at ICLR 2022
REFERENCES,0.3957703927492447,"Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Keep your options open: an
information-based driving principle for sensorimotor systems. PloS one, 3(12):e4018, 2008."
REFERENCES,0.3987915407854985,"Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing
deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020."
REFERENCES,0.40181268882175225,"Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020."
REFERENCES,0.40483383685800606,"Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model.
arXiv preprint arXiv:1907.00953,
2019."
REFERENCES,0.4078549848942598,"Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, and Sergio
Guadarrama. Predictive information accelerates learning in rl. arXiv preprint arXiv:2007.12401,
2020."
REFERENCES,0.4108761329305136,"Felix Leibfried, Sergio Pascual-Diaz, and Jordi Grau-Moya. A uniﬁed bellman optimality principle
combining reward maximization and empowerment. arXiv preprint arXiv:1907.12392, 2019."
REFERENCES,0.41389728096676737,"Xiao Ma, Siwei Chen, David Hsu, and Wee Sun Lee. Contrastive variational model-based reinforce-
ment learning for complex observations. arXiv e-prints, pp. arXiv–2008, 2020."
REFERENCES,0.4169184290030212,"Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsi-
cally motivated reinforcement learning. arXiv preprint arXiv:1509.08731, 2015."
REFERENCES,0.4199395770392749,"Oﬁr Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive
fourier features. arXiv preprint arXiv:2105.12272, 2021."
REFERENCES,0.4229607250755287,"Kenji Nakagawa, Yoshinori Takei, Shin-ichiro Hara, and Kohei Watabe. Analysis of the convergence
speed of the arimoto-blahut algorithm by the second-order recurrence formula. IEEE Transactions
on Information Theory, 2021."
REFERENCES,0.4259818731117825,"Tung Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal predictive coding for
model-based planning in latent space. arXiv preprint arXiv:2106.07156, 2021."
REFERENCES,0.42900302114803623,"XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847–5861, 2010."
REFERENCES,0.43202416918429004,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.4350453172205438,"Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International conference on machine learning, pp. 2778–2787.
PMLR, 2017."
REFERENCES,0.4380664652567976,"Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In International Conference on Machine Learning, pp. 5171–
5180. PMLR, 2019."
REFERENCES,0.44108761329305135,"Roberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic
data augmentation for generalization in reinforcement learning. 2020."
REFERENCES,0.44410876132930516,"Kate Rakelly, Abhishek Gupta, Carlos Florensa, and Sergey Levine. Which mutual-information
representation learning objectives are sufﬁcient for control?
arXiv preprint arXiv:2106.07278,
2021."
REFERENCES,0.4471299093655589,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211–252, 2015."
REFERENCES,0.4501510574018127,"Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019."
REFERENCES,0.45317220543806647,Published as a conference paper at ICLR 2022
REFERENCES,0.4561933534743202,"Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward: Self-
supervision for reinforcement learning. arXiv preprint arXiv:1612.07307, 2016."
REFERENCES,0.459214501510574,"Rui Shu, Tung Nguyen, Yinlam Chow, Tuan Pham, Khoat Than, Mohammad Ghavamzadeh, Stefano
Ermon, and Hung Bui. Predictive coding for locally-linear control. In International Conference
on Machine Learning, pp. 8862–8871. PMLR, 2020."
REFERENCES,0.4622356495468278,"Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020."
REFERENCES,0.4652567975830816,"Susanne Still. Information-theoretic approach to interactive learning. EPL (Europhysics Letters),
85(2):28005, 2009."
REFERENCES,0.46827794561933533,"Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart
Bulletin, 2(4):160–163, 1991."
REFERENCES,0.47129909365558914,"Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018."
REFERENCES,0.4743202416918429,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
Part XI 16, pp. 776–794. Springer, 2020."
REFERENCES,0.4773413897280967,"Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019."
REFERENCES,0.48036253776435045,"Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to
control: A locally linear latent dynamics model for control from raw images. arXiv preprint
arXiv:1506.07365, 2015."
REFERENCES,0.48338368580060426,"Kevin Xie, Homanga Bharadhwaj, Danijar Hafner, Animesh Garg, and Florian Shkurti. Latent skill
planning for exploration and transfer. In International Conference on Learning Representations,
2020."
REFERENCES,0.486404833836858,"Amy Zhang, Harsh Satija, and Joelle Pineau. Decoupling dynamics and reward for transfer learning.
arXiv preprint arXiv:1804.10689, 2018."
REFERENCES,0.48942598187311176,"Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine.
Learning
invariant representations for reinforcement learning without reconstruction.
arXiv preprint
arXiv:2006.10742, 2020."
REFERENCES,0.49244712990936557,Published as a conference paper at ICLR 2022
REFERENCES,0.4954682779456193,"A
APPENDIX"
REFERENCES,0.4984894259818731,"A.1
THEORETICAL RESULTS ON CONTROLLABLE REPRESENTATIONS"
REFERENCES,0.5015105740181269,"Let Φ(·) denote the encoder such that Z = Φ(O). O is the observation seen by the agent and S is
the underlying state. S+ is deﬁned as that part of underlying state S which is directly inﬂuenced by
actions A i.e. S+ ≤S s.t. I(At−1; St|S+
t ) = 0."
REFERENCES,0.5045317220543807,"We next describe two theorems regarding learning controllable representations, with proofs in the
Appendix. We observe that the max P"
REFERENCES,0.5075528700906344,"t I(At−1; Zt|Zt−1) objective alone for learning latent repre-
sentations Z, along with the planning objective provably recovers controllable parts of the observa-
tion O, namely S+.
Theorem 1. The objective max P
t I(At−1; Zt|Zt−1) provably recovers controllable parts S+ of
the observation O. S+ is deﬁned as that part of underlying state S which is directly inﬂuenced by
actions A i.e. S+ ⊂S s.t. I(St; At−1|S+
t ) = 0."
REFERENCES,0.5105740181268882,"Proof. Based on Fig. 2 and Fig. 7, we have the following
max I(At−1; Zt) ≤max I(At−1; Ot)
Data-Processing Inequality
≤I(At−1; St)
Data-Processing Inequality"
REFERENCES,0.513595166163142,"≤I(At−1; St, S+
t )"
REFERENCES,0.5166163141993958,"≤I(At−1; St|S+
t ) + I(At−1; S+
t )
Chain Rule of MI"
REFERENCES,0.5196374622356495,"≤I(At−1; S+
t )
Conditional independence
So, the maximum above can be obtained when the encoder Φ is an identity function over S+ and
a zero function over [ ˜S−, S−]. So, Zt = S+
t . Hence, given Zt−1, I(At−1; Zt|Zt−1) is maximized
when Φ is an identity function over S+. So, max P"
REFERENCES,0.5226586102719033,"t I(At−1; Zt|Zt−1) learns the encoder Φ such
that controllable representations are recovered."
REFERENCES,0.525679758308157,"This result is important because in practice, we may not be able to represent every possible factor
of variation in a complex environment. In this situation, we would expect that when |Z| ≪|O|,
learning Z under the objective max P"
REFERENCES,0.5287009063444109,"t I(At−1; Zt|Zt−1) would encode S+. We next show that
the inverse information objective alone can be used to train a latent-state space model and a policy
through an alternating optimization algorithm that converges to a local minimum of the objective
max P"
REFERENCES,0.5317220543806647,t I(At−1; Zt|Zt−1) at a rate inversely proportional to the number of iterations.
REFERENCES,0.5347432024169184,"Theorem 2. maxπ,ψ
P"
REFERENCES,0.5377643504531722,t I(At−1; Zt|Zt−1) = P
REFERENCES,0.540785498489426,"t Eπ(at−1|zt−1)p(zt|zt−1,at−1,ot) log qψ(at−1|zt,zt−1)"
REFERENCES,0.5438066465256798,"π(at−1|zt−1)
can be optimized through an alternating optimization scheme that has a convergence rate of O(1/N)
to a local minima of the objective, where N is the number of iterations."
REFERENCES,0.5468277945619335,"Proof. Let z ∼qφ(z|o) denote a latent state sampled from the encoder distribution. Learning a
world model such that reward prediction and inverse information are maximized can be summarized
as:"
REFERENCES,0.5498489425981873,"max
φ,ψ,η X"
REFERENCES,0.552870090634441,"t
Eπ(at|zt),qφ(zt|ot)"
REFERENCES,0.5558912386706949,"
Ep(rt|zt,at) log qη(rt|zt, at) + Ep(zt+1|zt,at) log qψ(at|zt+1, zt)"
REFERENCES,0.5589123867069486,π(at|zt) 
REFERENCES,0.5619335347432024,"The policy π(at|zt) is learned by max
π X"
REFERENCES,0.5649546827794562,"t
Eπ(at|zt),qφ(zt|ot)"
REFERENCES,0.56797583081571,"
Ep(rt|zt,at) log qη(rt|zt, at) + Ep(zt+1|zt,at) log qψ(at|zt+1, zt)"
REFERENCES,0.5709969788519638,π(at|zt) 
REFERENCES,0.5740181268882175,"Model-based RL in this case involves alternating optimization between policy learning and learning
the world model. In the case where there is no reward signal from the environment, i.e. p(rt|zt), we
have the following objective"
REFERENCES,0.5770392749244713,"max
π
max
φ,ψ X"
REFERENCES,0.5800604229607251,"t
Eπ(at|zt),qφ(zt|st)p(zt+1|zt,at) log qψ(at|zt+1, zt)"
REFERENCES,0.5830815709969789,π(at|zt)
REFERENCES,0.5861027190332326,Published as a conference paper at ICLR 2022
REFERENCES,0.5891238670694864,"Figure 7: PGM of the MDP with distractor states. The state observed by the agent O consists of three parts
S+, ˜S−and DS−. S+ is the controllable part of the state i.e. it is affected by the actions of the agent, and in
turn affects the reward R; ˜S−is not controllable by the agent but affects the reward R and future S+; DS−is
not controllable by the agent and doesn’t affect the reward R and future S+."
REFERENCES,0.5921450151057401,"The optimal q∗
ψ give a ﬁxed π and φ can be derived as in page 335 of Cover & Thomas (2006)."
REFERENCES,0.595166163141994,"q∗
ψ(at|zt+1, zt) =
qφ(zt|st)p(zt+1|zt, at)π(at|zt)
R"
REFERENCES,0.5981873111782477,"a qφ(zt|st)p(zt+1|zt, at)π(at|zt)"
REFERENCES,0.6012084592145015,The optimal π∗given a ﬁxed ψ and φ can be derived as
REFERENCES,0.6042296072507553,"π∗(at|zt) =
exp (Eqφ(zt|st)p(zt+1|zt,at)[log qψ(at|zt+1, zt)])
R"
REFERENCES,0.6072507552870091,"a exp (Eqφ(zt|st)p(zt+1|zt,at)q[log qψ(at|zt+1, zt)])
We note that these alternating iterations correspond to an instantiation of the Blahut-Arimoto al-
gorithm (Blahut, 1972) for determining the information theoretic capacity of a channel. Based
on (Nakagawa et al., 2021) we see that this iterative optimization procedure between q∗
ψ and π∗"
REFERENCES,0.6102719033232629,"converges, and the worst-case rate of convergence is O(1/N) where N is the number of iterations."
REFERENCES,0.6132930513595166,"This result is useful because it shows that even in the absence of rewards from the environment
(rt = 0 ∀t), when planning to minimize regret is not possible, the inverse information objective can
be used to train a policy that explores to seek out controllable parts of the state-space. When rewards
from the environment are present, we can train the policy with this objective and the value estimates
obtained from the cumulative rewards during planning. In Section 4.3 we empirically show how this
objective helps achieve higher sample efﬁciency compared to pure value-based policy learning."
REFERENCES,0.6163141993957704,"A.2
PRIMAL DUAL UPDATES"
REFERENCES,0.6193353474320241,"Z0:H−1 = Z0:H−1+∂PH−1
t=0 I(Ot; Zt) + λ (−I(it; Zt|Zt−1, At−1) + I(At−1; Zt|Zt−1) + I(Rt; Zt) −c0)"
REFERENCES,0.622356495468278,∂Z0:H−1
REFERENCES,0.6253776435045317,Published as a conference paper at ICLR 2022
REFERENCES,0.6283987915407855,λ = λ − 
REFERENCES,0.6314199395770392,"
−I(it; Zt|Zt−1, At−1)
|
{z
}
fwd dynamics model"
REFERENCES,0.6344410876132931,"+ I(At−1; Zt|Zt−1)
|
{z
}
inv dynamics model"
REFERENCES,0.6374622356495468,"+ I(Rt; Zt)
|
{z
}
rew model −c0  
"
REFERENCES,0.6404833836858006,"A.3
FORWARD, INVERSE, AND REWARD MODELS"
REFERENCES,0.6435045317220544,"−I(it; Zt|Zt−1, At−1) = −
Z
p(zt, zt−1, at−1, it, ot) log p(zt|zt−1, at−1, ot)"
REFERENCES,0.6465256797583081,"p(zt|zt−1, at−1)"
REFERENCES,0.649546827794562,"= −
Z
p(zt, zt−1, at−1, ot)

log p(zt|zt−1, at−1, ot)"
REFERENCES,0.6525679758308157,"qχ(zt|zt−1, at−1) + log qχ(zt|zt−1, at−1)"
REFERENCES,0.6555891238670695,"p(zt|zt−1, at−1) "
REFERENCES,0.6586102719033232,"≥−
Z
p(zt, zt−1, at−1, ot) log p(zt|zt−1, at−1, ot)"
REFERENCES,0.6616314199395771,"qχ(zt|zt−1, at−1)
= −DKL(p(zt|zt−1, at−1, ot)||qχ(zt|zt−1, at−1))"
REFERENCES,0.6646525679758308,We can obtain a variational lower bound on the empowerment
REFERENCES,0.6676737160120846,"I(at; zt+k|zt) =
Z
p(at, zt+k, zt) log p(at|zt+k, zt)"
REFERENCES,0.6706948640483383,p(at|zt)
REFERENCES,0.6737160120845922,"=
Z
p(at, zt+k, zt)

log q(at|zt+k, zt)"
REFERENCES,0.676737160120846,"p(at|zt)
+ log p(at|zt+k, zt)"
REFERENCES,0.6797583081570997,"q(at|zt+k, zt) "
REFERENCES,0.6827794561933535,"≥
Z
p(at, zt+k, zt) log q(at|zt+k, zt)"
REFERENCES,0.6858006042296072,p(at|zt)
REFERENCES,0.6888217522658611,"=
Z
p(at, zt+k, zt) log q(at|zt+k, zt) −
Z
p(zt)p(zt+k|at, zt)p(at|zt) log p(at|zt)"
REFERENCES,0.6918429003021148,"= Ep(at,zt+k,zt)[log q(at|zt+k, zt)] + Ep(zt)p(zt+k|at,zt)[H(p(at|zt))]
Here, q(at|zt+k, zt) is the inverse dynamics model, and H(p(at|zt)) is the entropy of the policy."
REFERENCES,0.6948640483383686,We can obtain a variational lower bound on the reward model as
REFERENCES,0.6978851963746223,"I(rt; zt) =
Z
p(rt, zt) log p(rt|zt) p(rt)"
REFERENCES,0.7009063444108762,"=
Z
p(rt, zt)

log q(rt|zt)"
REFERENCES,0.7039274924471299,"p(rt)
+ log p(rt|zt)"
REFERENCES,0.7069486404833837,q(rt|zt) 
REFERENCES,0.7099697885196374,"=
Z
p(rt, zt) log q(rt|zt)"
REFERENCES,0.7129909365558912,"p(rt)
+ KL[p(rt, zt)||q(rt, zt)]"
REFERENCES,0.716012084592145,"≥
Z
p(rt, zt) log q(rt|zt) p(rt)"
REFERENCES,0.7190332326283988,"=
Z
p(rt, zt) log q(rt|zt) −
Z
p(rt, zt) log p(rt)"
REFERENCES,0.7220543806646526,"= Ep(rt,zt)[log q(rt|zt)] + Constant"
REFERENCES,0.7250755287009063,"Here, q(rt|zt) is the reward decoder. Since the reward is a scalar, reconstructing it is computation-
ally simpler compared to reconstructing high dimensional observations as in the BA bound for the
observation model."
REFERENCES,0.7280966767371602,"A.4
VALUE DIFFERENCE RESULT"
REFERENCES,0.7311178247734139,"We show that the difference in value function of the underlying MDP Qπ(o, a) and the latent MDP
ˆQπ(z, a), where z ∼qφ(z|o) is bounded, under some regularity assumptions. Similar to the assump-
tion in (Gelada et al., 2019), let the policy π have bounded semi-norm value functions under the total
variation distance DTV i.e. | ˆV π(z)|DTV ≤K and |Ez1∼P V (z1) −Ez2∼QV (z2)| ≤KDTV(P, Q)."
REFERENCES,0.7341389728096677,"Deﬁne the forward dynamics learning objective as minχ LT = minχ DKL(p(o′|o, a)||qχ(z′|z, a))
and the reward model objective as minη LR = minη DKL(p(r|o)||qη(r|z))"
REFERENCES,0.7371601208459214,Published as a conference paper at ICLR 2022
REFERENCES,0.7401812688821753,"Theorem 3. The difference between the Q-function in the latent MDP and that in the original MDP
is bounded by the loss in reward predictor and forward dynamics model."
REFERENCES,0.743202416918429,"E(o,a)∼dπ(o,a),z∼qφ(z|o)[Qπ(o, a) −ˆQπ(z, a)] ≤
√LR + γK√LT 1 −γ"
REFERENCES,0.7462235649546828,Proof.
REFERENCES,0.7492447129909365,"E(o,a)∼dπ(o,a),z∼qφ(z|o)[Qπ(o, a) −ˆQπ(z, a)]"
REFERENCES,0.7522658610271903,"≤E(o,a)∼dπ(o,a),z∼qφ(z|o)(|r(o, a) −ˆr(z, a)| + γ|Eo′∼p(o′|o,a)V (o′) −Ez′∼qχ(z′|z,a) ˆV (z′)|) ≤
q"
REFERENCES,0.7552870090634441,"DKL(p(r|o)||qη(r|z)) + E(o,a)∼dπ(o,a),z∼qφ(z|o)(γ|Eo′∼p(o′|o,a),z′∼qφ(z′|o′)[V (o′) −ˆV (z′)]|"
REFERENCES,0.7583081570996979,"+ γ|Eo′∼p(o′|o,a),z′∼qχ(z′|z,a)[V (o′) −ˆV (z′)]|) ≤
p"
REFERENCES,0.7613293051359517,"LR + E(o,a)∼dπ(o,a),z∼qφ(z|o)(γ|Eo′∼p(o′|o,a),z′∼qφ(z′|o′)[V (o′) −ˆV (z′)]|)"
REFERENCES,0.7643504531722054,"+ E(o,a)∼dπ(o,a),z∼qφ(z|o)(γKDTV(p(o′|o, a)||qχ(z′|z, a))) ≤
p"
REFERENCES,0.7673716012084593,"LR + γE(o,a)∼dπ(o,a),z∼qφ(z|o)Eo′∼p(o′|o,a),z′∼qφ(z′|o′)[V (o′) −ˆV (z′)])"
REFERENCES,0.770392749244713,"+ γK
q"
REFERENCES,0.7734138972809668,"DKL(p(o′|o, a)||qχ(z′|z, a)) ≤
p"
REFERENCES,0.7764350453172205,"LR + γE(o,a)∼dπ(o,a),z∼qφ(z|o)[V (o) −ˆV (z)]) + γK
p LT ≤
p"
REFERENCES,0.7794561933534743,"LR + γE(o,a)∼dπ(o,a),z∼qφ(z|o)[Q(o, a) −ˆQ(z, a)]) + γK
p"
REFERENCES,0.7824773413897281,"LT
So, we have shown that"
REFERENCES,0.7854984894259819,"E(o,a)∼dπ(o,a),z∼qφ(z|o)[Qπ(o, a) −ˆQπ(z, a)] ≤
√LR + γK√LT 1 −γ"
REFERENCES,0.7885196374622356,"We can see that, as LR, LT →0, the Q-function in the representation space of the MDP, ˆQ, becomes
increasingly closer to that of the original MDP, Q. Since this result holds for all Q-functions, it also
holds for the optimal Q∗and ˆQ∗. This result extends sufﬁciency results in prior works Rakelly et al.
(2021) to a setting with stochastic encoders and KL-divergence losses on forward dynamics and
reward, as opposed to Wasserstein metrics (Gelada et al., 2019), and bisimulation metrics (Zhang
et al., 2020)."
REFERENCES,0.7915407854984894,"A.5
BEHAVIORAL SIMILARITY METRIC DETAILS"
REFERENCES,0.7945619335347432,"Given the sets Sz = {zi}n
i=1 and Szgt = {zi
gt}n
i=1, we construct complete graphs by connecting
every vertex with every other vertex through an edge. The weight of an edge is the Euclidean
distance between the respective pairs of vertices. The label of each vertex is an unique integer and
corresponding vertices in the two graphs are labelled similarly. Let the resulting graphs be denoted
as Gz = (Vz, Ez) and Gzgt = (Vzgt, Ezgt) respectively. Note that both these graphs are shortest path
graphs, by construction."
REFERENCES,0.797583081570997,"Since the Euclidean distances in both the graphs cannot be directly compared, we scale the distances
such that the shortest edge weight among all edge weights are equal in both the graphs. Scaling
every edge weight by the same number doesn’t change the structure of the graph. Now, we deﬁne
the shortest path kernel to measure similarity between the two graphs, as follows"
REFERENCES,0.8006042296072508,"k(Gz, Gzgt) =
1
|Ez| X ei∈Ez X"
REFERENCES,0.8036253776435045,ej∈Ezgt
REFERENCES,0.8066465256797583,"ˆk(ei, ej)"
REFERENCES,0.8096676737160121,"Let, ei = {ui, vi} and ej = {uj, vj}. Here, the kernel ˆk measures similarity between the edges
ei, ej and the labels on respective vertices. Let the function l(·) denote the labels of vertices and the
weights of edges."
REFERENCES,0.8126888217522659,"ˆk(ei, ej) = kv(l(ui), l(uj))ke(l(ei), l(ej))kv(l(vi), l(vj))+kv(l(ui), l(vj))ke(l(ei), l(ej))kv(l(vi), l(uj))"
REFERENCES,0.8157099697885196,Published as a conference paper at ICLR 2022
REFERENCES,0.8187311178247734,"Here, kv is a Dirac delta kernel i.e. ke(x, y) = 1 −δ(x, y) (Note that δ(x, y) = 1 iff x = y, else
δ(x, y) = 0) and ke is a Brownian ridge kernel, ke(x, y) = 1"
REFERENCES,0.8217522658610272,"c max(0, c −|x −y|), where c is a large
number."
REFERENCES,0.824773413897281,"So, in summary, the value of ˆk(ei, ej) is low when the edge lengths between corresponding pairs of
vertices in both the graphs are close. Hence, the value of k(Gz, Gzgt) is low when a large number of
pairs of corresponding vertices in both the graphs have the same edge length."
REFERENCES,0.8277945619335347,"We expect methods that recover latent state representations which better reﬂect the true underlying
simulator state (i.e., the positions of the joints) to have higher values according to this metric."
REFERENCES,0.8308157099697885,"A.6
DESCRIPTION OF THE DISTRACTORS"
REFERENCES,0.8338368580060423,"ILSVRC dataset (Russakovsky et al., 2015) The distractors used for the main experiments are
natural video backgrounds with RGB images from the ILSVRC dataset. We use 200 videos during
training, and reserve 50 videos for testing. An illustration of this for different environments are
shown in Fig. 8. These images are snapshots of what is received by the algorithm (64x64x3 images)
and no information about proprioceptive features is provided. While natural videos that are unrelated
to the task might be easy to ignore, realistic scenes might have other elements that resemble the
controllable elements, but are not actually controllable (e.g., other cars in a driving scenario)."
REFERENCES,0.8368580060422961,"To emulate this, we also add distractors that resemble other potentially controllable robots, as shown
for example in Fig. 8 (top left and bottom right frames), but are not actually inﬂuenced by the
actions of the algorithm. These are particularly challenging because the background video has a pre-
recorded motion of the same (or similar) agent that is being controlled. We include such challenging
agent-behind-agent background videos with a frequency of 1 in very 3 videos."
REFERENCES,0.8398791540785498,"For the experiments in Table 1, examples of different levels of distractors are shown in Fig. 9. The
three different levels have distractor windows of size 32x32, 40x40, and 64x64 respectively."
REFERENCES,0.8429003021148036,"Kinetics dataset (Kay et al., 2017) For comparing directly with results in prior works DBC (Zhang
et al., 2020), and TIA (Fu et al., 2021), we evaluate on the setting where the random videos are
grayscale images from the Kinetics dataset driving car class. The settings are same as in the prior
works and we compare directly to the results in the respective papers."
REFERENCES,0.8459214501510574,"A.7
TRAINING AND NETWORK DETAILS"
REFERENCES,0.8489425981873112,"The agent observations have shape 64 x 64 x 3, action dimensions range from 1 to 12, rewards per
time-step are scalars between 0 and 1. All episodes have randomized initial states, and the initial
distractor video background is chosen randomly."
REFERENCES,0.851963746223565,"We implement our approach with TensorFlow 2 and use a single Nvidia V100 GPU and 10 CPU
cores for each training run. The training time for 1e6 environment steps is 4 hours on average. In
comparison, the average training time for 1e6 steps for Dreamer is 3 hours, for CDreamer is 4 hours,
for TIA is 4 hours, for DBC is 4.5 hours, and for DeepMDP is 5 hours."
REFERENCES,0.8549848942598187,"The encoder consists of 4 convolutional layers with kernel size 4 and channel numbers 32, 65, 128,
256. The forward model consists of deterministic and stochastic parts, as in standard in visual model-
based RL (Hafner et al., 2019b;a). The stochastic states have size 30 and deterministic state has size
200. The compatibility function fθ(zt, ot′) = exp (zT
t Wθzt′) for contrastive learning is learned
with a 200 x 200 Wθ matrix. Here, zt′ is the encoding of ot′. All other models are implemented as
three dense layers of size 300 with ELU activations."
REFERENCES,0.8580060422960725,"We use ADAM optimizer, with learning rate of 6e-4 for the latent-state space model, and 8e-5 for
the value function and policy optimization. The hyper-parameter c0 for the prioritized information
constraint is set to 1000, after doing a grid-search over the range [100, 10000] and observing similar
performance for 1000 and 10000. 100 training steps are followed by 1 episode of interacting with the
environment with the currently trained policy, for observing real transitions. The dataset is initialized
with 7 episodes collected with random actions in the environment. We kept the hyper-parameters of
InfoPower same across all environments and distractor types."
REFERENCES,0.8610271903323263,"For fair comparisons, we kept all the common hyper-parameter values same as Dreamer (Hafner
et al., 2019a). This was the protocol followed in prior works TIA (Fu et al., 2021) and Ma et al."
REFERENCES,0.8640483383685801,Published as a conference paper at ICLR 2022
REFERENCES,0.8670694864048338,"Figure 8: Illustration of the natural video background distractors used in our experiments. The videos change
after every 50 time-steps. Some video backgrounds (for example the top left and the bottom right) have more
complex distractors in the form of agent-behind-agent i.e. an agent of similar morphology moves in the back-
ground of the agent being controlled."
REFERENCES,0.8700906344410876,"Figure 9: Illustration of the different levels of distractors. L1, L2, and L3 respectively have distractor windows
of size 32x32, 40x40, and 64x64."
REFERENCES,0.8731117824773413,"(2020). For the baseline TIA, we tuned the environment-speciﬁc parameters λRadv and λOs as
mentioned in Table 2 of (Fu et al., 2021). For λRadv, we performed a gridsearch over the range
10k −50k and for λOs we performed a gridsearch over the range 1 −3 to obtain the parame-
ters for best performance, which we used for the plots in Fig. 4 and Fig. 11. Respectively for
walker-walk, cheetah-run, ﬁnger-spin, quadruped-walk, hopper-stand, ball-in-a-cup-catch, cartpole-
balance-sparse, quadruped-run, the values of λRadv are 30k,30k,20k,40k,30k,40k,20k,30k. The val-
ues of λOs are 2,2,1.5,2.5,2.5,2,2,2."
REFERENCES,0.8761329305135952,"For baseline DBC (Zhang et al., 2020), we kept all the parameters same as in Table 2 of the paper,
because DBC does not have any environment-speciﬁc parameters and the same values were used for
all environments in the DBC paper. The DeepMDP agent and its hyperparameters are adapted from
the implementation provided in the github repo of DBC (Zhang et al., 2020)."
REFERENCES,0.879154078549849,"A.8
CONTRASTIVE LEARNING"
REFERENCES,0.8821752265861027,"Since the distracting backgrounds are changing videos of a certain duration, for contrastive learning,
we contrast both across time and across batches. Contrasting across time helps model invariance
against temporally contiguous distractors (for example in the same video) while contrasting across
batches helps in learning invariance across different videos. Concretely, we sample batches from the
dataset (i){(at, ot, rt)}H
t=1, where i denotes a batch index, and for each observation ot in batch i, all
observations ot′ both in batch i and in other batches j ̸= i are considered negative samples."
REFERENCES,0.8851963746223565,"A.9
RESULT ON VARYING DISTRACTOR LEVELS"
REFERENCES,0.8882175226586103,"We consider different levels of distractors by varying the size of the window where distractors in
the background are active. Fig. 9 in the Appendix illustrates this visually. Table 1 shows results for"
REFERENCES,0.8912386706948641,Published as a conference paper at ICLR 2022
REFERENCES,0.8942598187311178,"Table 1: DM Control Walker Stand with natural video distractors at different levels. We tabulate the rewards
(Rew) and behavioral similarity (Sim) at 500k and 1M steps of training. Higher is better for both Rew and Sim."
REFERENCES,0.8972809667673716,"Name
Levels
Rew@500k
Rew@1M
Sim@500k
Sim@1M"
REFERENCES,0.9003021148036254,"Dreamer
L1
197 ± 31
240 ± 27
0.73±0.02
0.74±0.01
DBC
L1
261 ± 25
390 ± 34
0.75±0.03
0.74±0.02
C-Dreamer
L1
291 ± 34
590 ± 26
0.73±0.02
0.79±0.01
DeepMDP
L1
263 ± 31
340 ± 22
0.74±0.02
0.75±0.03
InfoPower
L1
397 ± 22
650 ± 100
0.82 ± 0.01
0.84 ± 0.03"
REFERENCES,0.9033232628398792,"Dreamer
L2
180 ± 36
197 ± 20
0.56±0.03
0.59±0.02
DBC
L2
221 ± 28
320 ± 33
0.65±0.02
0.66±0.02
C-Dreamer
L2
282 ± 30
550 ± 66
0.63±0.01
0.71±0.02
DeepMDP
L2
213 ± 34
300 ± 25
0.59±0.02
0.61±0.04
InfoPower
L2
394 ± 30
644 ± 101
0.77 ± 0.03
0.77 ± 0.03"
REFERENCES,0.9063444108761329,"Dreamer
L3
140 ± 52
157 ± 10
0.32±0.02
0.33±0.03
DBC
L3
165 ± 20
221 ± 24
0.45±0.01
0.48±0.01
C-Dreamer
L3
231 ± 39
485 ± 86
0.58±0.02
0.64±0.01
DeepMDP
L3
153 ± 23
212 ± 28
0.44±0.02
0.49±0.01
InfoPower
L3
389 ± 20
624 ± 80
0.71 ± 0.01
0.74 ± 0.03"
REFERENCES,0.9093655589123867,"Table 2: Comparison of baselines TIA and DBC with InfoPower on environments with video distractors
from the Kinetics dataset Driving Car class. The results for TIA and DBC are from the respective papers."
REFERENCES,0.9123867069486404,"TIA
DBC
InfoPower"
K,0.9154078549848943,"500k
800k
500k
800k
500k
800k"
K,0.918429003021148,"Walker-Run
389±45
602±40
165±92
210±32
417±22
630±33
Cheetah-Run
384±149
579±172
261±64
277±81
425±97
572±158
Hopper-Stand
338±221
581±214
110±92
207±120
395±140
615±176
Ball-in-a-Cup
201±197
115±110
–
–
255±104
263±116
Walker-Walk
878±37
948±28
545±57
630±68
925±18
972±26
Finger-Spin
371±96
487±107
801±10
832±4
795±34
787±58
Hopper-Hop
47±44
72±68
5±12
0±0
85±26
77±74"
K,0.9214501510574018,"the different approaches on varying distractor levels. The size of the frame for distractors at levels
L1, L2, and L3 respectively are 32x32, 40x40, and 64x64. We observe that the baselines perform
worse as the distractor window size increases. While, for InfoPower, the performance decrease
with increasing level is minimal. This indicates the effectiveness of InfoPower in ﬁltering out
background distractors from the observations while learning latent representations. Note that the
distractors in Figs. 4 and 6 are of level 3 i.e. same size as the observations 64x64."
K,0.9244712990936556,"A.10
RESULTS ON DISTRACTORS FROM THE KINETICS DATASET"
K,0.9274924471299094,"In this section, we evaluate InfoPower on the same distractors used in prior works, DBC and
TIA. The distractor videos are from the Kinetics dataset (Kay et al., 2017) Driving Car class, and
converted to grayscale. From Table 2 we see that InfoPower slightly outperforms the baselines.
We believe the performance gap is not as signiﬁcant as Fig. 4 because the distractors being grayscale
images, and not RGB might be easier to ignore by the baselines as well as InfoPower."
K,0.9305135951661632,"A.11
SETTING WITH ONLY SHIFTED AGENT BEHIND AGENT DISTRACTORS"
K,0.9335347432024169,"Figure 10: Ilustration of the distractor set-
ting for results in Table 3."
K,0.9365558912386707,"In this section we evaluate InfoPower with the base-
lines only for the challenging setting of agent-behind-
agent distrators where the background contains the same
agent being controlled (a walker or a cheetah). An il-
lustration of this is shown in Fig. 10. From the results
in Table 3, we see that InfoPower signiﬁcantly out-"
K,0.9395770392749244,Published as a conference paper at ICLR 2022
K,0.9425981873111783,"performs the competitive baselines both at 500k and 1M
environment interactions.
This suggests the utility of
InfoPower in separating out informative elements of
the observation from uninformative ones even in chal-
lenging settings."
K,0.945619335347432,"Table 3: Evaluation of InfoPower and the most competitive baselines in a suite of challenging distractors
where the background contains a shifted version of the agent being controlled. Results are averaged over 4
random seeds. Fig. 10 shows a visualization of the distractors for this setting."
K,0.9486404833836858,"Walker-Walk-shifted
Cheetah-Run-shifted"
K,0.9516616314199395,"500k
1M
500k
1M"
K,0.9546827794561934,"DBC
35±15
104±28
41±10
78±38
CDreamer
108±36
164±45
73±44
110±52
TIA
79±42
152±33
65±24
115±28
InfoPower
168±48
278±42
130±46
207±35"
K,0.9577039274924471,Published as a conference paper at ICLR 2022
K,0.9607250755287009,"Figure 11: Evaluation of InfoPower and baselines with empowerment in policy learning. The setting cor-
responds to DeepMind Control tasks with RGB natural video distractors in the background (same as that in
Figure 5 of the main paper). The x-axis denotes the number of environment interactions and the y-axis shows
the episodic returns.
A.12
COMPARISON OF BASELINES WITH EMPOWERMENT IN POLICY LEARNING"
K,0.9637462235649547,"In this section, we compare against modiﬁed versions of the baselines, such that we include empow-
erment in the policy learning of the baselines. This is the only modiﬁcation we make to the baselines,
and keep everything else unchanged. We described in section 3.3 that inclusion of empowerment
in the objective for visual model-based RL (by modifying both the representation learning objective
and the policy learning objective) is an important contribution of our paper. The aim of this exper-
iment is to disentangle the beneﬁts of empowerment in representation learning, for InfoPower
with that in policy learning."
K,0.9667673716012085,"We have plotted the results in Fig. 11. We can see that the performance of the baselines is slightly
better than Fig. 4 because of the inclusion of empowerment in the policy learning objective which as
we motivated previously in section 3.4 helps with exploring controllable regions of the state-space.
Note that by adding empowerment, we have effectively modiﬁed the baselines."
K,0.9697885196374623,"A.13
COMPARISON ON ENVIRONMENTS WITHOUT DISTRACTORS"
K,0.972809667673716,"We compare the performance of InfoPower with a state-of-the-art visual model-based RL method,
Dreamer in the default DM Control environments without distractors. We see from Table 4 that
InfoPower performs slightly better or similar to Dreamer in all the environments consistently.
This result complements our main paper result on environments with distractors and conﬁrms that
the beneﬁt of InfoPower in challenging distracting environments does not come at a cost of per-
formance in simpler non-distracting environments."
K,0.9758308157099698,"Table 4: Comparison of InfoPower with state-of-the-art visual model-based RL method, Dreamer in the
default DM Control environments without distractors. We tabulate reward at 500k and 1M environment inter-
actions. Results are averaged over 4 random seeds. Higher is better."
K,0.9788519637462235,"Dreamer
InfoPower
Rew @ 500k
Rew @ 1M
Rew @ 500k
1M
Walker Walk
890±52
985± 10
910±41
980± 17
Cheetah Run
580±190
811±95
586±142
810±102
Finger Spin
570±187
573±96
590±172
598±85
Quadruped Walk
592±45
646±50
623±65
666±47
Hopper Stand
701±68
906±33
715±60
930±39
Ball in a Cup
871±102
910±48
913±67
939±52
Cart-Pole Sparse
742±58
849±88
776±40
866±80
Quadruped Run
450±47
515±65
468±42
510±61"
K,0.9818731117824774,Published as a conference paper at ICLR 2022
K,0.9848942598187311,"A.14
ADDITIONAL ABLATIONS"
K,0.9879154078549849,"In this section, we consider additional ablations, namely a version of of InfoPower that does not
have the empowerment objective, a version of InfoPower that does not have the MI maximization
objective. We see from Table 5 that removing either of these terms drops performance."
K,0.9909365558912386,Table 5: DM Walker Stand with natural video distractors
K,0.9939577039274925,"Name
Obs bound
Rew@500k
Rew@1M
Sim@500k
Sim@1M"
K,0.9969788519637462,"Dreamer
BA
140 ± 52
157 ± 10
0.32±0.02
0.33±0.03
Contrastive-Dreamer
NCE
231 ± 39
485 ± 86
0.58±0.02
0.64±0.01
NonGenerative-Dreamer
NWJ
280 ± 25
480 ± 93
0.59±0.01
0.62±0.05
InfoPower
NCE
254 ± 27
481 ± 30
0.69±0.03
0.72±0.02
InfoPower
NWJ
389 ± 20
624 ± 80
0.71±0.01
0.74±0.03
Inverse only
None
282 ± 70
367 ± 24
0.63±0.04
0.66±0.03"
