Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008880994671403197,"The performance of machine learning models on new data is critical for their
1"
ABSTRACT,0.0017761989342806395,"success in real-world applications. However, the model’s performance may deterio-
2"
ABSTRACT,0.0026642984014209592,"rate if the new data is sampled from a different distribution than the training data.
3"
ABSTRACT,0.003552397868561279,"Current methods to detect shifts in the input or output data distributions have limi-
4"
ABSTRACT,0.004440497335701598,"tations in identifying model behavior changes. In this paper, we define explanation
5"
ABSTRACT,0.0053285968028419185,"shift as the statistical comparison between how predictions from training data are
6"
ABSTRACT,0.006216696269982238,"explained and how predictions on new data are explained. We propose explanation
7"
ABSTRACT,0.007104795737122558,"shift as a key indicator to investigate the interaction between distribution shifts and
8"
ABSTRACT,0.007992895204262877,"learned models. We introduce an Explanation Shift Detector that operates on the
9"
ABSTRACT,0.008880994671403197,"explanation distributions, providing more sensitive and explainable changes in in-
10"
ABSTRACT,0.009769094138543518,"teractions between distribution shifts and learned models. We compare explanation
11"
ABSTRACT,0.010657193605683837,"shifts with other methods based on distribution shifts, showing that monitoring
12"
ABSTRACT,0.011545293072824156,"for explanation shifts results in more sensitive indicators for varying model be-
13"
ABSTRACT,0.012433392539964476,"havior. We provide theoretical and experimental evidence and demonstrate the
14"
ABSTRACT,0.013321492007104795,"effectiveness of our approach on synthetic and real data. Additionally, we release
15"
ABSTRACT,0.014209591474245116,"an open-source Python package, skshift, which implements our method and
16"
ABSTRACT,0.015097690941385435,"provides usage tutorials for further reproducibility.
17"
INTRODUCTION,0.015985790408525755,"1
Introduction
18"
INTRODUCTION,0.016873889875666074,"ML theory provides means to forecast the quality of ML models on unseen data, provided that this
19"
INTRODUCTION,0.017761989342806393,"data is sampled from the same distribution as the data used to train and evaluate the model. If unseen
20"
INTRODUCTION,0.018650088809946713,"data is sampled from a different distribution than the training data, model quality may deteriorate,
21"
INTRODUCTION,0.019538188277087035,"making monitoring how the model’s behavior changes crucial.
22"
INTRODUCTION,0.020426287744227355,"Recent research has highlighted the impossibility of reliably estimating the performance of machine
23"
INTRODUCTION,0.021314387211367674,"learning models on unseen data sampled from a different distribution in the absence of further
24"
INTRODUCTION,0.022202486678507993,"assumptions about the nature of the shift [1, 2, 3]. State-of-the-art techniques attempt to model
25"
INTRODUCTION,0.023090586145648313,"statistical distances between the distributions of the training and unseen data [4, 5] or the distributions
26"
INTRODUCTION,0.023978685612788632,"of the model predictions [3, 6, 7]. However, these measures of distribution shifts only partially relate
27"
INTRODUCTION,0.02486678507992895,"to changes of interaction between new data and trained models or they rely on the availability of a
28"
INTRODUCTION,0.02575488454706927,"causal graph or types of shift assumptions, which limits their applicability. Thus, it is often necessary
29"
INTRODUCTION,0.02664298401420959,"to go beyond detecting such changes and understand how the feature attribution changes [8, 9, 10, 4].
30"
INTRODUCTION,0.027531083481349913,"The field of explainable AI has emerged as a way to understand model decisions [11, 12] and
31"
INTRODUCTION,0.028419182948490232,"interpret the inner workings of ML models [13]. The core idea of this paper is to go beyond the
32"
INTRODUCTION,0.02930728241563055,"modeling of distribution shifts and monitor for explanation shifts to signal a change of interactions
33"
INTRODUCTION,0.03019538188277087,"between learned models and dataset features in tabular data. We newly define explanation shift as the
34"
INTRODUCTION,0.03108348134991119,"statistical comparison between how predictions from training data are explained and how predictions
35"
INTRODUCTION,0.03197158081705151,"on new data are explained. In summary, our contributions are:
36"
INTRODUCTION,0.03285968028419183,"• We propose measures of explanation shifts as a key indicator for investigating the interaction
37"
INTRODUCTION,0.03374777975133215,"between distribution shifts and learned models.
38"
INTRODUCTION,0.03463587921847247,"• We define an Explanation Shift Detector that operates on the explanation distributions
39"
INTRODUCTION,0.035523978685612786,"allowing for more sensitive and explainable changes of interactions between distribution
40"
INTRODUCTION,0.03641207815275311,"shifts and learned models.
41"
INTRODUCTION,0.037300177619893425,"• We compare our monitoring method that is based on explanation shifts with methods that
42"
INTRODUCTION,0.03818827708703375,"are based on other kinds of distribution shifts. We find that monitoring for explanation shifts
43"
INTRODUCTION,0.03907637655417407,"results in more sensitive indicators for varying model behavior.
44"
INTRODUCTION,0.03996447602131439,"• We release an open-source Python package skshift, which implements our “Explanation
45"
INTRODUCTION,0.04085257548845471,"Shift Detector”, along usage tutorials for reproducibility.
46"
FOUNDATIONS AND RELATED WORK,0.041740674955595025,"2
Foundations and Related Work
47"
BASIC NOTIONS,0.04262877442273535,"2.1
Basic Notions
48"
BASIC NOTIONS,0.043516873889875664,"Supervised machine learning induces a function fθ : dom(X) →dom(Y ), from training data
49"
BASIC NOTIONS,0.04440497335701599,"Dtr = {(xtr
0 , ytr
0 ) . . . , (xtr
n , ytr
n )}. Thereby, fθ is from a family of functions fθ ∈F and Dtr is
50"
BASIC NOTIONS,0.0452930728241563,"sampled from the joint distribution P(X, Y ) with predictor variables X and target variable Y . fθ is
51"
BASIC NOTIONS,0.046181172291296625,"expected to generalize well on new, previously unseen data Dnew
X
= {xnew
0
, . . . , xnew
k
} ⊆dom(X).
52"
BASIC NOTIONS,0.04706927175843695,"We write Dtr
X to refer to {xtr
0 , . . . , xtr
n } and Dtr
Y to refer to Dtr
Y = {ytr
0 . . . , ytr
n }. For the purpose
53"
BASIC NOTIONS,0.047957371225577264,"of formalizations and to define evaluation metrics, it is often convenient to assume that an oracle
54"
BASIC NOTIONS,0.04884547069271759,"provides values Dnew
Y
= {ynew
0
, . . . , ynew
k
} such that Dnew = {(xnew
0
, ynew
0
), . . . , (xnew
k
, ynew
k
)} ⊆
55"
BASIC NOTIONS,0.0497335701598579,"dom(X) × dom(Y ).
56"
BASIC NOTIONS,0.050621669626998225,"The core machine learning assumption is that training data Dtr and novel data Dnew are sampled from
57"
BASIC NOTIONS,0.05150976909413854,"the same underlying distribution P(X, Y ). The twin problems of model monitoring and recognizing
58"
BASIC NOTIONS,0.052397868561278864,"that new data is out-of-distribution can now be described as predicting an absolute or relative
59"
BASIC NOTIONS,0.05328596802841918,"performance drop between perf(Dtr) and perf(Dnew), where perf(D) = P"
BASIC NOTIONS,0.0541740674955595,"(x,y)∈D ℓeval(fθ(x), y),
60"
BASIC NOTIONS,0.055062166962699825,"ℓeval is a metric like 0-1-loss (accuracy), but Dnew
Y
is unknown and cannot be used for such judgment.
61"
BASIC NOTIONS,0.05595026642984014,"Therefore related work analyses distribution shifts between training and newly occurring data. Let
62"
BASIC NOTIONS,0.056838365896980464,"two datasets D, D′ define two empirical distributions P(D), P(D′), then we write P(D) ̸∼P(D′)
63"
BASIC NOTIONS,0.05772646536412078,"to express that P(D) is sampled from a different underlying distribution than P(D′) with high
64"
BASIC NOTIONS,0.0586145648312611,"probability p > 1 −ϵ allowing us to formalize various types of distribution shifts.
65"
BASIC NOTIONS,0.05950266429840142,"Definition 2.1 (Data Shift). We say that data shift occurs from Dtr to Dnew
X
, if P(Dtr
X) ̸∼P(Dnew
X
).
66"
BASIC NOTIONS,0.06039076376554174,"Specific kinds of data shift are:
67"
BASIC NOTIONS,0.06127886323268206,"Definition 2.2 (Univariate data shift). There is a univariate data shift between P(Dtr
X) =
68"
BASIC NOTIONS,0.06216696269982238,"P(Dtr
X1, . . . , Dtr
Xp) and P(Dnew
X
) = P(Dnew
X1 , . . . , Dnew
Xp ), if ∃i ∈{1 . . . p} : P(Dtr
Xi) ̸∼P(Dnew
Xi ).
69"
BASIC NOTIONS,0.0630550621669627,"Definition 2.3 (Covariate data shift). There is a covariate data shift between P(Dtr
X)
=
70"
BASIC NOTIONS,0.06394316163410302,"P(Dtr
X1, . . . , Dtr
Xp) and P(Dnew
X
) = P(Dnew
X1 , . . . , Dnew
Xp ) if P(Dtr
X) ̸∼P(Dnew
X
), which cannot only
71"
BASIC NOTIONS,0.06483126110124333,"be caused by univariate shift.
72"
BASIC NOTIONS,0.06571936056838366,"The next two types of shift involve the interaction of data with the model fθ, which approximates the
73"
BASIC NOTIONS,0.06660746003552398,conditional P (Dtr)
BASIC NOTIONS,0.0674955595026643,"P (Dtr
X ). Abusing notation, we write fθ(D) to refer to the multiset {fθ(x)|x ∈D}.
74"
BASIC NOTIONS,0.06838365896980461,"Definition 2.4 (Predictions Shift). There is a predictions shift between distributions P(Dtr
X) and
75"
BASIC NOTIONS,0.06927175843694494,"P(Dnew
X
) related to model fθ if P(fθ(Dtr
X)) ̸∼P(fθ(Dnew
X
)).
76"
BASIC NOTIONS,0.07015985790408526,"Definition 2.5 (Concept Shift). There is a concept shift between P(Dtr) = P(Dtr
X, Dtr
Y ) and
77"
BASIC NOTIONS,0.07104795737122557,"P(Dnew) = P(Dnew
X
, Dnew
Y
) if conditional distributions change, i.e. P(Dtr)"
BASIC NOTIONS,0.0719360568383659,"P(Dtr
X ) ̸∼P(Dnew)"
BASIC NOTIONS,0.07282415630550622,"P(Dnew
X
).
78"
BASIC NOTIONS,0.07371225577264653,"In practice, multiple types of shifts co-occur together and their disentangling may constitute a
79"
BASIC NOTIONS,0.07460035523978685,"significant challenge that we do not address here [14, 15].
80"
RELATED WORK ON TABULAR DATA,0.07548845470692718,"2.2
Related Work on Tabular Data
81"
RELATED WORK ON TABULAR DATA,0.0763765541740675,"We briefly review the related works below. See Appendix A for a more detailed related work.
82"
RELATED WORK ON TABULAR DATA,0.07726465364120781,"Classifier two-sample test: Evaluating how two distributions differ has been a widely studied
83"
RELATED WORK ON TABULAR DATA,0.07815275310834814,"topic in the statistics and statistical learning literature [16, 15, 17] and has advanced in recent years
84"
RELATED WORK ON TABULAR DATA,0.07904085257548846,"[18, 19, 20]. The use of supervised learning classifiers to measure statistical tests has been explored
85"
RELATED WORK ON TABULAR DATA,0.07992895204262877,"by Lopez-Paz et al. [21] proposing a classifier-based approach that returns test statistics to interpret
86"
RELATED WORK ON TABULAR DATA,0.08081705150976909,"differences between two distributions. We adopt their power test analysis and interpretability approach
87"
RELATED WORK ON TABULAR DATA,0.08170515097690942,"but apply it to the explanation distributions.
88"
RELATED WORK ON TABULAR DATA,0.08259325044404973,"Detecting distribution shift and its impact on model behaviour: A lot of related work has aimed
89"
RELATED WORK ON TABULAR DATA,0.08348134991119005,"at detecting that data is from out-of-distribution. To this end, they have created several benchmarks
90"
RELATED WORK ON TABULAR DATA,0.08436944937833037,"that measure whether data comes from in-distribution or not [22, 23, 24, 25, 26]. In contrast, our
91"
RELATED WORK ON TABULAR DATA,0.0852575488454707,"main aim is to evaluate the impact of the distribution shift on the model.
92"
RELATED WORK ON TABULAR DATA,0.08614564831261101,"A typical example is two-sample testing on the latent space such as described by Rabanser et al. [27].
93"
RELATED WORK ON TABULAR DATA,0.08703374777975133,"However, many of the methods developed for detecting out-of-distribution data are specific to neural
94"
RELATED WORK ON TABULAR DATA,0.08792184724689166,"networks processing image and text data and can not be applied to traditional machine learning
95"
RELATED WORK ON TABULAR DATA,0.08880994671403197,"techniques. These methods often assume that the relationships between predictor and response
96"
RELATED WORK ON TABULAR DATA,0.08969804618117229,"variables remain unchanged, i.e., no concept shift occurs. Our work is applied to tabular data where
97"
RELATED WORK ON TABULAR DATA,0.0905861456483126,"techniques such as gradient boosting decision trees achieve state-of-the-art model performance [28,
98"
RELATED WORK ON TABULAR DATA,0.09147424511545293,"29, 30].
99"
RELATED WORK ON TABULAR DATA,0.09236234458259325,"Impossibility of model monitoring: Recent research findings have formalized the limitations of
100"
RELATED WORK ON TABULAR DATA,0.09325044404973357,"monitoring machine learning models in the absence of labelled data. Specifically [3, 31] prove the
101"
RELATED WORK ON TABULAR DATA,0.0941385435168739,"impossibility of predicting model degradation or detecting out-of-distribution data with certainty [32,
102"
RELATED WORK ON TABULAR DATA,0.09502664298401421,"33, 34]. Although our approach does not overcome these limitations, it provides valuable insights for
103"
RELATED WORK ON TABULAR DATA,0.09591474245115453,"machine learning engineers to understand better changes in interactions resulting from shifting data
104"
RELATED WORK ON TABULAR DATA,0.09680284191829484,"distributions and learned models.
105"
RELATED WORK ON TABULAR DATA,0.09769094138543517,"Model monitoring and distribution shift under specific assumptions: Under specific types of
106"
RELATED WORK ON TABULAR DATA,0.09857904085257549,"assumptions, model monitoring and distribution shift become feasible tasks. One type of assumption
107"
RELATED WORK ON TABULAR DATA,0.0994671403197158,"often found in the literature is to leverage causal knowledge to identify the drivers of distribution
108"
RELATED WORK ON TABULAR DATA,0.10035523978685613,"changes [35, 36, 37]. For example, Budhathoki et al. [35] use graphical causal models and feature
109"
RELATED WORK ON TABULAR DATA,0.10124333925399645,"attributions based on Shapley values to detect changes in the distribution. Similarly, other works aim
110"
RELATED WORK ON TABULAR DATA,0.10213143872113677,"to detect specific distribution shifts, such as covariate or concept shifts. Our approach does not rely
111"
RELATED WORK ON TABULAR DATA,0.10301953818827708,"on additional information, such as a causal graph, labelled test data, or specific types of distribution
112"
RELATED WORK ON TABULAR DATA,0.10390763765541741,"shift. Still, by the nature of pure concept shifts, the model behaviour remains unaffected and new
113"
RELATED WORK ON TABULAR DATA,0.10479573712255773,"data need to come with labelled responses to be detected.
114"
RELATED WORK ON TABULAR DATA,0.10568383658969804,"Explainability and distribution shift: Lundberg et al. [38] applied Shapley values to identify
115"
RELATED WORK ON TABULAR DATA,0.10657193605683836,"possible bugs in the pipeline by visualizing univariate SHAP contributions. In our work we go
116"
RELATED WORK ON TABULAR DATA,0.10746003552397869,"beyond debugging and formalize the multivariate explanation distributions where we perform a
117"
RELATED WORK ON TABULAR DATA,0.108348134991119,"two-sample classifier test to detect distribution shift impacts on the model. Furthermore, we provide
118"
RELATED WORK ON TABULAR DATA,0.10923623445825932,"a mathematical analysis of how the SHAP values contribute to detecting distribution shift.
119"
RELATED WORK ON TABULAR DATA,0.11012433392539965,"2.3
Explainable AI: Local Feature Attributions
120"
RELATED WORK ON TABULAR DATA,0.11101243339253997,"Attribution by Shapley values explains machine learning models by determining the relevance of
121"
RELATED WORK ON TABULAR DATA,0.11190053285968028,"features used by the model [38, 39]. The Shapley value is a concept from coalition game theory that
122"
RELATED WORK ON TABULAR DATA,0.1127886323268206,"aims to allocate the surplus generated by the grand coalition in a game to each of its players [40]. The
123"
RELATED WORK ON TABULAR DATA,0.11367673179396093,"Shapley value Sj for the j’th player is defined via a value function val : 2N →R of players in T:
124"
RELATED WORK ON TABULAR DATA,0.11456483126110124,"Sj(val) =
X"
RELATED WORK ON TABULAR DATA,0.11545293072824156,T ⊆N\{j}
RELATED WORK ON TABULAR DATA,0.11634103019538189,|T|!(p −|T| −1)!
RELATED WORK ON TABULAR DATA,0.1172291296625222,"p!
(val(T ∪{j}) −val(T))
(1)"
RELATED WORK ON TABULAR DATA,0.11811722912966252,"125
In machine learning, N = {1, . . . , p} is the set of features occurring in the training data. Given that x
126"
RELATED WORK ON TABULAR DATA,0.11900532859680284,"is the feature vector of the instance to be explained, and the term valf,x(T) represents the prediction
127"
RELATED WORK ON TABULAR DATA,0.11989342806394317,"for the feature values in T that are marginalized over features that are not included in T:
128"
RELATED WORK ON TABULAR DATA,0.12078152753108348,"valf,x(T) = EX|XT =xT [f(X)] −EX[f(X)]
(2)"
RELATED WORK ON TABULAR DATA,0.1216696269982238,"129
The Shapley value framework satisfies several theoretical properties [12, 40, 41, 42]. Our approach is
130"
RELATED WORK ON TABULAR DATA,0.12255772646536411,"based on the efficiency and uninformative properties:
131"
RELATED WORK ON TABULAR DATA,0.12344582593250444,"Efficiency Property. Feature contributions add up to the difference of prediction from x⋆and the
132"
RELATED WORK ON TABULAR DATA,0.12433392539964476,"expected value:
133
X"
RELATED WORK ON TABULAR DATA,0.12522202486678508,"j∈N
Sj(f, x⋆) = f(x⋆) −E[f(X)])
(3)"
RELATED WORK ON TABULAR DATA,0.1261101243339254,"134
Uninformativeness Property. A feature j that does not change the predicted value has a Shapley
135"
RELATED WORK ON TABULAR DATA,0.1269982238010657,"value of zero.
136"
RELATED WORK ON TABULAR DATA,0.12788632326820604,"∀x, xj, x′
j : f({xN\{j}, xj}) = f({xN\{j}, x′
j}) ⇒∀x : Sj(f, x) = 0.
(4)"
RELATED WORK ON TABULAR DATA,0.12877442273534637,"Our approach works with explanation techniques that fulfill efficiency and uninformative properties,
137"
RELATED WORK ON TABULAR DATA,0.12966252220248667,"and we use Shapley values as an example. It is essential to distinguish between the theoretical Shapley
138"
RELATED WORK ON TABULAR DATA,0.130550621669627,"values and the different implementations that approximate them. We use TreeSHAP as an efficient
139"
RELATED WORK ON TABULAR DATA,0.13143872113676733,"implementation for tree-based models of Shapley values [38, 12, 43], mainly we use the observational
140"
RELATED WORK ON TABULAR DATA,0.13232682060390763,"(or path-dependent) estimation [44, 45, 46], and for linear models, we use the correlation dependent
141"
RELATED WORK ON TABULAR DATA,0.13321492007104796,"implementation that takes into account feature dependencies [47].
142"
RELATED WORK ON TABULAR DATA,0.1341030195381883,"LIME is another explanation method candidate for out approach [48, 49]. LIME computes local
143"
RELATED WORK ON TABULAR DATA,0.1349911190053286,"feature attributions and also satisfies efficiency and uninformative properties, at least in theoretical
144"
RELATED WORK ON TABULAR DATA,0.13587921847246892,"aspects. However, the definition of neighborhoods in LIME and corresponding computational
145"
RELATED WORK ON TABULAR DATA,0.13676731793960922,"expenses impact its applicability. In Appendix F, we analyze LIME’s relationship with Shapley
146"
RELATED WORK ON TABULAR DATA,0.13765541740674955,"values for the purpose of describing explanation shifts.
147"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.13854351687388988,"3
A Model for Explanation Shift Detection
148"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.13943161634103018,"Our model for explanation shift detection is sketched in Fig. 1. We define it step-by-step as follows:
149"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.14031971580817051,"Definition 3.1 (Explanation distribution). An explanation function S : F × dom(X) →Rp maps a
150"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.14120781527531084,"model fθ and data x ∈Rp to a vector of attributions S(fθ, x) ∈Rp. We call S(fθ, x) an explanation.
151"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.14209591474245115,"We write S(fθ, D) to refer to the empirical explanation distribution generated by {S(fθ, x)|x ∈D}.
152"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.14298401420959148,"We use local feature attribution methods SHAP and LIME as explanation functions S.
153"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.1438721136767318,"Definition 3.2 (Explanation shift). Given a model fθ learned from Dtr, explanation shift with respect
154"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.1447602131438721,"to the model fθ occurs if S(fθ, Dnew
X
) ̸∼S(fθ, Dtr
X).
155"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.14564831261101244,"Definition 3.3 (Explanation shift metrics). Given a measure of statistical distances d, explanation shift
156"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.14653641207815277,"is measured as the distance between two explanations of the model fθ by d(S(fθ, Dtr
X), S(fθ, Dnew
X
)).
157"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.14742451154529307,"We follow Lopez et al. [21] to define an explanation shift metrics based on a two-sample test
158"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.1483126110124334,"classifier.
We proceed as depicted in Figure 1.
To counter overfitting, given the model fθ
159"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.1492007104795737,"trained on Dtr, we compute explanations {S(fθ, x)|x ∈Dval
X } on an in-distribution validation
160"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15008880994671403,"data set Dval
X . Given a dataset Dnew
X
, for which the status of in- or out-of-distribution is unknown,
161"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15097690941385436,"we compute its explanations {S(fθ, x)|x ∈Dnew
X }. Then, we construct a two-samples dataset
162"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15186500888099466,"E = {(S(fθ, x), ax)|x ∈Dval
X , ax = 0} ∪{(S(fθ, x), ax)|x ∈Dnew
X , ax = 1} and we train a
163"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.152753108348135,"discrimination model gψ : Rp →{0, 1} on E, to predict if an explanation should be classified as
164"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15364120781527532,"in-distribution (ID) or out-of-distribution (OOD):
165"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15452930728241562,"ψ = arg min
˜
ψ X"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15541740674955595,"x∈Dval
X ∪Dnew
X"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15630550621669628,"ℓ(g ˜
ψ(S(fθ, x)), ax),
(5)"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15719360568383658,"where ℓis a classification loss function (e.g. cross-entropy). gψ is our two-sample test classifier,
166"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15808170515097691,"based on which AUC yields a test statistic that measures the distance between the Dtr
X explanations
167"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15896980461811722,"and the explanations of new data Dnew
X
.
168"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.15985790408525755,"Explanation shift detection allows us to detect that a novel dataset Dnew changes the model’s behavior.
169"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.16074600355239788,"Beyond recognizing explanation shift, using feature attributions for the model gψ, we can interpret
170"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.16163410301953818,"how the features of the novel dataset Dnew
X
interact differently with model fθ than the features of the
171"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.1625222024866785,"validation dataset Dval
X . These features are to be considered for model monitoring and for classifying
172"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.16341030195381884,"new data as out-of-distribution.
173"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.16429840142095914,Explanations
A MODEL FOR EXPLANATION SHIFT DETECTION,0.16518650088809947,Train Classifier for
A MODEL FOR EXPLANATION SHIFT DETECTION,0.1660746003552398,Two-Sample Test
A MODEL FOR EXPLANATION SHIFT DETECTION,0.1669626998223801,"No Explanation Shift
Explanation Shift"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.16785079928952043,"Explain
Explanation Shift Detector
Not
Reject
Reject"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.16873889875666073,Explanations
A MODEL FOR EXPLANATION SHIFT DETECTION,0.16962699822380106,Train Classifier
A MODEL FOR EXPLANATION SHIFT DETECTION,0.1705150976909414,"Figure 1: Our model for explanation shift detection. The model fθ is trained on Dtr implying explanations for
distributions Dval
X , Dnew
X
. The AUC of the two-sample test classifier gψ decides for or against explanation shift.
If an explanation shift occurred, it could be explained which features of the Dnew
X
deviated in fθ compared to
Dval
X ."
RELATIONSHIPS BETWEEN COMMON DISTRIBUTION SHIFTS AND EXPLANATION SHIFTS,0.1714031971580817,"4
Relationships between Common Distribution Shifts and Explanation Shifts
174"
RELATIONSHIPS BETWEEN COMMON DISTRIBUTION SHIFTS AND EXPLANATION SHIFTS,0.17229129662522202,"This section analyses and compares data shifts, prediction shifts, with explanation shifts. Appendix B
175"
RELATIONSHIPS BETWEEN COMMON DISTRIBUTION SHIFTS AND EXPLANATION SHIFTS,0.17317939609236235,"extends this analysis, and Appendix C draws from these analyses to derive experiments with synthetic
176"
RELATIONSHIPS BETWEEN COMMON DISTRIBUTION SHIFTS AND EXPLANATION SHIFTS,0.17406749555950266,"data.
177"
EXPLANATION SHIFT VS DATA SHIFT,0.17495559502664298,"4.1
Explanation Shift vs Data Shift
178"
EXPLANATION SHIFT VS DATA SHIFT,0.17584369449378331,"One type of distribution shift that is challenging to detect comprises cases where the univariate
179"
EXPLANATION SHIFT VS DATA SHIFT,0.17673179396092362,"distributions for each feature j are equal between the source Dtr
X and the unseen dataset Dnew
X
, but
180"
EXPLANATION SHIFT VS DATA SHIFT,0.17761989342806395,"where interdependencies among different features change. Multi-covariance statistical testing is a
181"
EXPLANATION SHIFT VS DATA SHIFT,0.17850799289520428,"hard taks with high sensitivity that can lead to false positives. The following example demonstrates
182"
EXPLANATION SHIFT VS DATA SHIFT,0.17939609236234458,"that Shapley values account for co-variate interaction changes while a univariate statistical test will
183"
EXPLANATION SHIFT VS DATA SHIFT,0.1802841918294849,"provide false negatives.
184 185"
EXPLANATION SHIFT VS DATA SHIFT,0.1811722912966252,"Example 4.1. (Covariate Shift) Let Dtr ∼N
h
µ1
µ2
i
,
h
σ2
X1
0"
EXPLANATION SHIFT VS DATA SHIFT,0.18206039076376554,"0
σ2
X2"
EXPLANATION SHIFT VS DATA SHIFT,0.18294849023090587,"i
× Y . We fit a linear model
186"
EXPLANATION SHIFT VS DATA SHIFT,0.18383658969804617,"fθ(x1, x2) = γ + a · x1 + b · x2. If Dnew
X
∼N
h
µ1
µ2
i
,
h
σ2
X1
ρσX1 σX2
ρσX1 σX2
σ2
X2"
EXPLANATION SHIFT VS DATA SHIFT,0.1847246891651865,"i
, then P(Dtr
X1) and
187"
EXPLANATION SHIFT VS DATA SHIFT,0.18561278863232683,"P(Dtr
X2) are identically distributed with P(Dnew
X1 ) and P(Dnew
X2 ), respectively, while this does not
188"
EXPLANATION SHIFT VS DATA SHIFT,0.18650088809946713,"hold for the corresponding Sj(fθ, Dtr
X) and Sj(fθ, Dnew
X
).
189"
EXPLANATION SHIFT VS DATA SHIFT,0.18738898756660746,"The detailed analysis of example 4.1 is given in Appendix B.2.
190"
EXPLANATION SHIFT VS DATA SHIFT,0.1882770870337478,"False positives frequently occur in out-of-distribution data detection when a statistical test recognizes
191"
EXPLANATION SHIFT VS DATA SHIFT,0.1891651865008881,"differences between a source distribution and a new distribution, thought the differences do not affect
192"
EXPLANATION SHIFT VS DATA SHIFT,0.19005328596802842,"the model behavior [28, 14]. Shapley values satisfy the Uninformativeness property, where a feature
193"
EXPLANATION SHIFT VS DATA SHIFT,0.19094138543516873,"j that does not change the predicted value has a Shapley value of 0 (equation 4).
194"
EXPLANATION SHIFT VS DATA SHIFT,0.19182948490230906,"Example 4.2. Shifts on Uninformative Features. Let the random variables X1, X2 be normally
195"
EXPLANATION SHIFT VS DATA SHIFT,0.19271758436944939,"distributed with N(0; 1). Let dataset Dtr ∼X1 × X2 × Y tr, with Y tr = X1. Thus Y tr⊥X2.
196"
EXPLANATION SHIFT VS DATA SHIFT,0.1936056838365897,"Let Dnew
X
∼X1 × Xnew
2
and Xnew
2
be normally distributed with N(µ; σ2) and µ, σ ∈R. When
197"
EXPLANATION SHIFT VS DATA SHIFT,0.19449378330373002,"fθ is trained optimally on Dtr then fθ(x) = x1. P(DX2) can be different from P(Dnew
X2 ) but
198"
EXPLANATION SHIFT VS DATA SHIFT,0.19538188277087035,"S2(fθ, Dtr
X) = 0 = S2(fθ, Dnew
X
).
199"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.19626998223801065,"4.2
Explanation Shift vs Prediction Shift
200"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.19715808170515098,"Analyses of the explanations detect distribution shifts that interact with the model. In particular, if a
201"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.1980461811722913,"prediction shift occurs, the explanations produced are also shifted.
202"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.1989342806394316,"Proposition 1. Given a model fθ : DX →DY . If fθ(x′) ̸= fθ(x), then S(fθ, x′) ̸= S(fθ, x).
203"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.19982238010657194,"By efficiency property of the Shapley values [47] (equation ((3))), if the prediction between two
204"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.20071047957371227,"instances is different, then they differ in at least one component of their explanation vectors.
205"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.20159857904085257,"The opposite direction does not always hold:
206"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.2024866785079929,"Example 4.3. (Explanation shift not affecting prediction distribution) Given Dtr is generated
207"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.2033747779751332,"from (X1 × X2 × Y ), X1 ∼U(0, 1), X2 ∼U(1, 2), Y = X1 + X2 + ϵ and thus the optimal model
208"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.20426287744227353,"is f(x) = x1 + x2. If Dnew is generated from Xnew
1
∼U(1, 2), Xnew
2
∼U(0, 1),
Y new =
209"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.20515097690941386,"Xnew
1
+ Xnew
2
+ ϵ, the prediction distributions are identical fθ(Dtr
X), fθ(Dnew
X
) ∼U(1, 3), but
210"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.20603907637655416,"explanation distributions are different S(fθ, Dtr
X) ̸∼S(fθ, Dnew
X
), because Si(fθ, x) = αi · xi.
211"
EXPLANATION SHIFT VS PREDICTION SHIFT,0.2069271758436945,"Thus, an explanation shift does not always imply a prediction shift.
212"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.20781527531083482,"4.3
Explanation Shift vs Concept Shift
213"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.20870337477797513,"Concept shift comprises cases where the covariates retain a given distribution, but their relationship
214"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.20959147424511546,"with the target variable changes (cf. Section 2.1). This example shows the negative result that concept
215"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.21047957371225579,"shift cannot be indicated by the detection of explanation shift.
216"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.2113676731793961,"Example 4.4. Concept Shift Let Dtr ∼X1 × X2 × Y , and create a synthetic target ytr
i
=
217"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.21225577264653642,"a0 + a1 · xi,1 + a2 · xi,2 + ϵ. As new data we have Dnew
X
∼Xnew
1
× Xnew
2
× Y , with ynew
i
=
218"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.21314387211367672,"b0 + b1 · xi,1 + b2 · xi,2 + ϵ whose coefficients are unknown at prediction stage. With coefficients
219"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.21403197158081705,"a0 ̸= b0, a1 ̸= b1, a2 ̸= b2. We train a linear regression fθ : Dtr
X →Dtr
Y . Then explanations have the
220"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.21492007104795738,"same distribution, P(S(fθ, Dtr
X)) = P(S(fθ, Dnew
X
)), input data distribution P(Dtr
X) = P(Dnew
X
)
221"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.21580817051509768,"and predictions P(fθ(Dtr
X)) = P(fθ(Dnew
X
)). But there is no guarantee on the performance of fθ
222"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.216696269982238,"on Dnew
X
[3]
223"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.21758436944937834,"In general, concept shift cannot be detected because Dnew
Y
is unknown [3]. Some research studies
224"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.21847246891651864,have made specific assumptions about the conditional P (Dnew)
EXPLANATION SHIFT VS CONCEPT SHIFT,0.21936056838365897,"P (Dnew
X
) in order to monitor models and detect
225"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.2202486678507993,"distribution shift [7, 50].
226"
EXPLANATION SHIFT VS CONCEPT SHIFT,0.2211367673179396,"In Appendix B.2.2, we analyze a situation in which an oracle — hypothetically — provides Dnew
Y
.
227"
EMPIRICAL EVALUATION,0.22202486678507993,"5
Empirical Evaluation
228"
EMPIRICAL EVALUATION,0.22291296625222023,"We perform core evaluations of explanation shift detection methods by systematically varying models
229"
EMPIRICAL EVALUATION,0.22380106571936056,"f, model parametrizations θ, and input data distributions DX. We complement core experiments
230"
EMPIRICAL EVALUATION,0.2246891651865009,"described in this section by adding further experimental results in the appendix that (i) add details
231"
EMPIRICAL EVALUATION,0.2255772646536412,"on experiments with synthetic data (Appendix C), (ii) add experiments on further natural datasets
232"
EMPIRICAL EVALUATION,0.22646536412078153,"(Appendix D), (iii) exhibit a larger range of modeling choices (Appendix E), and (iv) include LIME as
233"
EMPIRICAL EVALUATION,0.22735346358792186,"an explanation method (Appendix F). Core observations made in this section will only be confirmed
234"
EMPIRICAL EVALUATION,0.22824156305506216,"and refined, but not countered in the appendix.
235"
BASELINE METHODS AND DATASETS,0.2291296625222025,"5.1
Baseline Methods and Datasets
236"
BASELINE METHODS AND DATASETS,0.23001776198934282,"Baseline Methods. We compare our method of explanation shift detection (Section 3) with several
237"
BASELINE METHODS AND DATASETS,0.23090586145648312,"methods that aim to detect that input data is out-of-distribution: (i) statistical Kolmogorov Smirnov test
238"
BASELINE METHODS AND DATASETS,0.23179396092362345,"on input data [27], (ii) classifier drift [51], (iii) prediction shift detection by Wasserstein distance [7],
239"
BASELINE METHODS AND DATASETS,0.23268206039076378,"(iv) prediction shift detection by Kolmogorov-Smirnov test[4], and (v) model agnostic uncertainty
240"
BASELINE METHODS AND DATASETS,0.23357015985790408,"estimation [10, 52]. Distribution Shift Metrics are scaled between 0 and 1. We also compare against
241"
BASELINE METHODS AND DATASETS,0.2344582593250444,"Classifier Two-Sample Test [21] on different distributions as discussed in Section 4, viz. (vi) classifier
242"
BASELINE METHODS AND DATASETS,0.2353463587921847,"two-sample test on input distributions (gϕ) and (vii) classifier two-sample test on the predictions
243"
BASELINE METHODS AND DATASETS,0.23623445825932504,"distributions (gΥ):
244"
BASELINE METHODS AND DATASETS,0.23712255772646537,"ϕ = arg min
˜ϕ X"
BASELINE METHODS AND DATASETS,0.23801065719360567,"x∈Dval
X ∪Dnew
X"
BASELINE METHODS AND DATASETS,0.238898756660746,"ℓ(g ˜ϕ(x)), ax)
Υ = arg min
˜Υ X"
BASELINE METHODS AND DATASETS,0.23978685612788633,"x∈Dval
X ∪Dnew
X"
BASELINE METHODS AND DATASETS,0.24067495559502664,"ℓ(g ˜Υ(fθ(x)), ax)
(6) 245"
BASELINE METHODS AND DATASETS,0.24156305506216696,"Datasets. In the main body of the paper we base our comparisons on the UCI Adult Income
246"
BASELINE METHODS AND DATASETS,0.2424511545293073,"dataset [53] and on synthetic data. In the Appendix, we extend experiments to several other
247"
BASELINE METHODS AND DATASETS,0.2433392539964476,"datasets, which confirm our findings: ACS Travel Time [54], ACS Employment [54], Stackoverflow
248"
BASELINE METHODS AND DATASETS,0.24422735346358793,"dataset [55].
249"
EXPERIMENTS ON SYNTHETIC DATA,0.24511545293072823,"5.2
Experiments on Synthetic Data
250"
EXPERIMENTS ON SYNTHETIC DATA,0.24600355239786856,"Our first experiment on synthetic data showcases the two main contributions of our method: (i)
251"
EXPERIMENTS ON SYNTHETIC DATA,0.2468916518650089,"being more sensitive than prediction shift and input shift to changes in the model and (ii) accounting
252"
EXPERIMENTS ON SYNTHETIC DATA,0.2477797513321492,"for its drivers. We first generate a synthetic dataset with a shift similar to the multivariate shift
253"
EXPERIMENTS ON SYNTHETIC DATA,0.24866785079928952,"one (cf. Section 4.2). However, we add an extra variable X3 = N(0, 1) and generate our target
254"
EXPERIMENTS ON SYNTHETIC DATA,0.24955595026642985,"Y = X1 · X2 + X3, and parametrize the multivariate shift between ρ = r(X1, X2). We train the
255"
EXPERIMENTS ON SYNTHETIC DATA,0.25044404973357015,"fθ on Dtr using a gradient boosting decision tree, while for gψ : S(fθ, Dval
X ) →{0, 1}, we use a
256"
EXPERIMENTS ON SYNTHETIC DATA,0.25133214920071045,"logistic regression for both experiments. In Appendix E we benchmark other estimators and detectors.
257"
EXPERIMENTS ON SYNTHETIC DATA,0.2522202486678508,"0.0
0.2
0.4
0.6
0.8
1.0
Correlation coefficient 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 AUC"
EXPERIMENTS ON SYNTHETIC DATA,0.2531083481349911,Sensitivity to Covariate Shift of Classifier Two Sample Test
EXPERIMENTS ON SYNTHETIC DATA,0.2539964476021314,Explanation Distribution g
EXPERIMENTS ON SYNTHETIC DATA,0.25488454706927177,Input Distribution g
EXPERIMENTS ON SYNTHETIC DATA,0.2557726465364121,Prediction Distribution g
EXPERIMENTS ON SYNTHETIC DATA,0.2566607460035524,"0.0
0.2
0.4
0.6
0.8
1.0
Correlation coefficient 0.0 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS ON SYNTHETIC DATA,0.25754884547069273,Distribution Shift Metrics
EXPERIMENTS ON SYNTHETIC DATA,0.25843694493783304,Sensitivy to Covariate Shift for Distribution Shift Methods
EXPERIMENTS ON SYNTHETIC DATA,0.25932504440497334,"Explanation Shift
Input KS
Classifier Drift
Pred. KS
Uncertainty by CP
Preds. Wasserstein"
EXPERIMENTS ON SYNTHETIC DATA,0.2602131438721137,"Figure 2: In the left figure, we apply the Classifier Two-Sample Test on (i) explanation distribution, (ii) input
distribution, (iii) prediction distribution. Explanation distribution shows highest sensitivity. Comparison of the
sensitivity of the Explanation Shift Detector. The right figure, related work comparison of distribution shift
methods, good indicators should follow a progressive steady positive slope, following the correlation coefficient
ρ."
EXPERIMENTS ON SYNTHETIC DATA,0.261101243339254,"Table 1 and Figure 2 show the results of our approach when learning on different distributions. In
258"
EXPERIMENTS ON SYNTHETIC DATA,0.2619893428063943,"our sensitivity experiment, we observed that using the explanation shift led to higher sensitivity
259"
EXPERIMENTS ON SYNTHETIC DATA,0.26287744227353466,"towards detecting distribution shift. This is due to the efficiency property of the Shapley values,
260"
EXPERIMENTS ON SYNTHETIC DATA,0.26376554174067496,"which decompose fθ(DX) into S(fθ, DX). Moreover, we can identify the features that are causing
261"
EXPERIMENTS ON SYNTHETIC DATA,0.26465364120781526,"the drift by extracting the coefficients of gψ, providing global and local explainability.
262"
EXPERIMENTS ON SYNTHETIC DATA,0.2655417406749556,"The right image in Figure 2 compares our approach against Classifier Two Sample Testing for detect-
263"
EXPERIMENTS ON SYNTHETIC DATA,0.2664298401420959,"ing multi-covariate shifts on different distributions. We can see how the explanations distributions
264"
EXPERIMENTS ON SYNTHETIC DATA,0.2673179396092362,"have more sensitivity to the others. On the left image, the same experiment against other out-of-
265"
EXPERIMENTS ON SYNTHETIC DATA,0.2682060390763766,"distribution detection methods such statistical differences on the input data (Input KS, Classifier
266"
EXPERIMENTS ON SYNTHETIC DATA,0.2690941385435169,"Drift)[51, 4], which are model-independent; uncertainty estimation methods[52, 10, 56], whose effec-
267"
EXPERIMENTS ON SYNTHETIC DATA,0.2699822380106572,"tiveness under specific types of shift is unclear; and statistical changes on the prediction distribution
268"
EXPERIMENTS ON SYNTHETIC DATA,0.27087033747779754,"(K-S and Wasserstein Distance) [57, 58, 7], which can detect changes in model but lack sensitivity
269"
EXPERIMENTS ON SYNTHETIC DATA,0.27175843694493784,"and accountability of the explanation shift. All metrics produce output scaled between 0 and 1.
270"
EXPERIMENTS ON SYNTHETIC DATA,0.27264653641207814,"Table 1: Conceptual comparison table over different detection methods over the examples discussed above.
Learning a Classifier Two-Sample test g over the explanation distributions is the only method that achieves
the desired results and is accountable. We evaluate accountability by checking if the feature attributions of the
detection method correspond with the synthetic shift generated in both scenarios"
EXPERIMENTS ON SYNTHETIC DATA,0.27353463587921845,"Detection Method
Covariate
Uninformative
Accountability
Explanation distribution (gψ)
✓
✓
✓
Input distribution(gϕ)
✓
✗
✗
Prediction distribution(gΥ)
✓
✓
✗
Input KS
✗
✗
✗
Classifier Drift
✓
✗
✗
Output KS
✓
✓
✗
Output Wasserstein
✓
✓
✗
Uncertainty
∼
✓
✓"
EXPERIMENTS ON SYNTHETIC DATA,0.2744227353463588,"5.3
Experiments on Natural Data: Inspecting Explanation Shifts
271"
EXPERIMENTS ON SYNTHETIC DATA,0.2753108348134991,"In the following experiments, we will provide use cases of our approach in two scenarios with natural
272"
EXPERIMENTS ON SYNTHETIC DATA,0.2761989342806394,"data: (i) novel group distribution shift and (ii) geopolitical and temporal shift.
273"
NOVEL COVARIATE GROUP,0.27708703374777977,"5.3.1
Novel Covariate Group
274"
NOVEL COVARIATE GROUP,0.27797513321492007,"The distribution shift in this experimental set-up relies on the appearance of a new unseen group at
275"
NOVEL COVARIATE GROUP,0.27886323268206037,"the prediction stage (the group feature is not present in the covariates). We vary the ratio of presence
276"
NOVEL COVARIATE GROUP,0.2797513321492007,"of this unseen group in Dnew
X
data. As estimators, we use a gradient-boosting decision tree and a
277"
NOVEL COVARIATE GROUP,0.28063943161634103,"logistic regression(just when indicated); we use a logistic regression for the detector. We compare
278"
NOVEL COVARIATE GROUP,0.28152753108348133,"different estimators and detectors’ performance in AppendixE.1 for a benchmark and Appendix E.2
279"
NOVEL COVARIATE GROUP,0.2824156305506217,"for experiments varying hyperparameters.
280"
NOVEL COVARIATE GROUP,0.283303730017762,"0.2
0.4
0.6
0.8
1.0
Fraction of data from previously unseen group 0.50 0.55 0.60 0.65 0.70 0.75 0.80"
NOVEL COVARIATE GROUP,0.2841918294849023,AUC of Explanation Shift Detector
NOVEL COVARIATE GROUP,0.28507992895204265,"Black
Am-Indian
Asian
Other
Mixed"
NOVEL COVARIATE GROUP,0.28596802841918295,"0.2
0.4
0.6
0.8
1.0
Fraction of data from previously unseen group 0.50 0.55 0.60 0.65 0.70 0.75 0.80 AUC"
NOVEL COVARIATE GROUP,0.28685612788632325,"g  = Explanations, f  = XGB"
NOVEL COVARIATE GROUP,0.2877442273534636,"g  = Explanations, f  = Log"
NOVEL COVARIATE GROUP,0.2886323268206039,"g  = Predictions, f  = XGB"
NOVEL COVARIATE GROUP,0.2895204262877442,"g  = Predictions, f  = Log"
NOVEL COVARIATE GROUP,0.29040852575488457,"g  = Input, f  = XGB"
NOVEL COVARIATE GROUP,0.2912966252220249,"g  = Input, f  = Log"
NOVEL COVARIATE GROUP,0.2921847246891652,"Figure 3: Novel group shift experiment on the UCI Adult Income dataset. Sensitivity (AUC) increases with the
growing fraction of previously unseen social groups. Left figure: The explanation shift indicates that different
social groups exhibit varying deviations from the distribution on which the model was trained. Right figure: We
vary the model fθ to be trained by XGBoost (solid lines) and Logistic Regression (dots), and the model g to be
trained on different distributions."
GEOPOLITICAL AND TEMPORAL SHIFT,0.29307282415630553,"5.3.2
Geopolitical and Temporal Shift
281"
GEOPOLITICAL AND TEMPORAL SHIFT,0.29396092362344584,"0.50
0.55
0.60
0.65
0.70
0.75
AUC 0 50 100 150 200"
GEOPOLITICAL AND TEMPORAL SHIFT,0.29484902309058614,AUC Kernel Density Distribution
GEOPOLITICAL AND TEMPORAL SHIFT,0.29573712255772644,AUC performance of the Explanation Shift Detector
GEOPOLITICAL AND TEMPORAL SHIFT,0.2966252220248668,"In-Distribution (CA14)
NY18
TX18
HI18
KS18
MN18
PR18
CA18"
GEOPOLITICAL AND TEMPORAL SHIFT,0.2975133214920071,"PR18
KS18
HI18
MN18
TX18
NY18
CA18"
GEOPOLITICAL AND TEMPORAL SHIFT,0.2984014209591474,PlaceOfBirth Race
GEOPOLITICAL AND TEMPORAL SHIFT,0.29928952042628776,Occupation WKHP
GEOPOLITICAL AND TEMPORAL SHIFT,0.30017761989342806,Relationship Age
GEOPOLITICAL AND TEMPORAL SHIFT,0.30106571936056836,Education COW
GEOPOLITICAL AND TEMPORAL SHIFT,0.3019538188277087,Marital Sex
GEOPOLITICAL AND TEMPORAL SHIFT,0.302841918294849,"0.81
1.1
0.72
0.68
0.11
0.08
0.014"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3037300177619893,"0.072
0.26
0.29
0.33
0.18
0.16
0.015"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3046181172291297,"0.31
0.15
0.27
0.11
0.026
0.092
0.015"
GEOPOLITICAL AND TEMPORAL SHIFT,0.30550621669627,"0.18
0.2
0.15
0.12
0.19
0.017
0.058"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3063943161634103,"0.11
0.16
0.28
0.14
0.091
0.016
0.099"
GEOPOLITICAL AND TEMPORAL SHIFT,0.30728241563055064,"0.3
0.16
0.19
0.028
0.11
0.057
0.01"
GEOPOLITICAL AND TEMPORAL SHIFT,0.30817051509769094,"0.24
0.15
0.085
0.038
0.087
0.17
0.051"
GEOPOLITICAL AND TEMPORAL SHIFT,0.30905861456483125,"0.096
0.056
0.22
0.068
0.018
0.051
0.018"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3099467140319716,"0.18
0.049
0.019
0.045
0.022
0.13
0.056"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3108348134991119,"0.086
0.03
0.0039
0.018
0.035
0.012
0.015"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3117229129662522,"Feature importance of the Explanation Shift detector (Wasserstein) 10
2 10
1 10
0"
GEOPOLITICAL AND TEMPORAL SHIFT,0.31261101243339257,"Figure 4: In the left figure, comparison of the performance of Explanation Shift Detector, in different states.
In the right figure, strength analysis of features driving the change in the model, in the y-axis the features and
on the x-axis the different states. Explanation shifts allow us to identify how the distribition shift of different
features impacted the model."
GEOPOLITICAL AND TEMPORAL SHIFT,0.31349911190053287,"In this section, we tackle a geopolitical and temporal distribution shift, for this, we train the model fθ
282"
GEOPOLITICAL AND TEMPORAL SHIFT,0.31438721136767317,"in California in 2014 and evaluate it in the rest of the states in 2018. The model gθ is trained each
283"
GEOPOLITICAL AND TEMPORAL SHIFT,0.31527531083481347,"time on each state using only the Dnew
X
in the absence of the label, and a 50/50 random train-test split
284"
GEOPOLITICAL AND TEMPORAL SHIFT,0.31616341030195383,"evaluates its performance. As models, we use a gradient boosting decision tree[59, 60] as estimator
285"
GEOPOLITICAL AND TEMPORAL SHIFT,0.31705150976909413,"fθ, and using logistic regression for the Explanation Shift Detector.
286"
GEOPOLITICAL AND TEMPORAL SHIFT,0.31793960923623443,"We hypothesize that the AUC of the “Explanation Shift Detector” on new data will be distinct from
287"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3188277087033748,"on ID data due to the OOD model explanations. Figure 4 illustrates the performance of our method
288"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3197158081705151,"on different data distributions, where the baseline is a hold-out set of ID −CA14. The AUC for
289"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3206039076376554,"CA18, where there is only a temporal shift, is the closest to the baseline, and the OOD detection
290"
GEOPOLITICAL AND TEMPORAL SHIFT,0.32149200710479575,"performance is better in the rest of the states. The most disparate state is Puerto Rico (PR18).
291"
GEOPOLITICAL AND TEMPORAL SHIFT,0.32238010657193605,"Our next objective is to identify the features where the explanations differ between Dtr
X and Dnew
X
292"
GEOPOLITICAL AND TEMPORAL SHIFT,0.32326820603907636,"data. To achieve this, we compare the distribution of linear coefficients of the detector between ID
293"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3241563055062167,"and New data. We use the Wasserstein distance as a distance measure, where we generate 1000
294"
GEOPOLITICAL AND TEMPORAL SHIFT,0.325044404973357,"in-distribution bootstraps using a 63.2% sampling fraction from California-14 and 1000 bootstraps
295"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3259325044404973,"from other states in 2018. In the right image of Figure 4, we observe that for PR18, the most crucial
296"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3268206039076377,"feature is the citizenship status1.
297"
GEOPOLITICAL AND TEMPORAL SHIFT,0.327708703374778,"Furthermore, we conduct an across-task evaluation by comparing the performance of the “Explanation
298"
GEOPOLITICAL AND TEMPORAL SHIFT,0.3285968028419183,"Shift Detector” on another prediction task in the Appendix D. Although some features are present in
299"
GEOPOLITICAL AND TEMPORAL SHIFT,0.32948490230905864,"both prediction tasks, the weights and importance order assigned by the ""Explanation Shift Detector""
300"
GEOPOLITICAL AND TEMPORAL SHIFT,0.33037300177619894,"differ. One of this method’s advantages is that it identifies differences in distributions and how they
301"
GEOPOLITICAL AND TEMPORAL SHIFT,0.33126110124333924,"relate to the model.
302"
DISCUSSION,0.3321492007104796,"6
Discussion
303"
DISCUSSION,0.3330373001776199,"In this study, we conducted a comprehensive evaluation of explanation shift by systematically
304"
DISCUSSION,0.3339253996447602,"varying models (f), model parametrizations (θ), feature attribution explanations (S), and input data
305"
DISCUSSION,0.33481349911190056,"distributions (DX). Our objective was to investigate the impact of distribution shift on the model by
306"
DISCUSSION,0.33570159857904086,"explanation shift and gain insights into its characteristics and implications.
307"
DISCUSSION,0.33658969804618116,"Our approach cannot detect concept shifts, as concept shift requires understanding the interaction
308"
DISCUSSION,0.33747779751332146,"between prediction and response variables. By the nature of pure concept shifts, such changes
309"
DISCUSSION,0.3383658969804618,"do not affect the model. To be understood, new data need to come with labelled responses. We
310"
DISCUSSION,0.3392539964476021,"work under the assumption that such labels are not available for new data, nor do we make other
311"
DISCUSSION,0.3401420959147424,"assumptions; therefore, our method is not able to predict the degradation of prediction performance
312"
DISCUSSION,0.3410301953818828,"under distribution shifts. All papers such as [3, 10, 61, 31, 32, 62, 7] that address the monitoring
313"
DISCUSSION,0.3419182948490231,"of prediction performance have the same limitation. Only under specific assumptions, e.g., no
314"
DISCUSSION,0.3428063943161634,"occurrence of concept shift or causal graph availability, can performance degradation be predicted
315"
DISCUSSION,0.34369449378330375,"with reasonable reliability.
316"
DISCUSSION,0.34458259325044405,"The potential utility of explanation shifts as distribution shift indicators that affect the model in
317"
DISCUSSION,0.34547069271758435,"computer vision or natural language processing tasks remains an open question. We have used
318"
DISCUSSION,0.3463587921847247,"Shapley values to derive indications of explanation shifts, but other AI explanation techniques may
319"
DISCUSSION,0.347246891651865,"be applicable and come with their advantages.
320"
CONCLUSIONS,0.3481349911190053,"7
Conclusions
321"
CONCLUSIONS,0.34902309058614567,"Commonly, the problem of detecting the impact of the distribution shift on the model has relied on
322"
CONCLUSIONS,0.34991119005328597,"measurements for detecting shifts in the input or output data distributions or relied on assumptions
323"
CONCLUSIONS,0.35079928952042627,"either on the type of distribution shift or causal graphs availability. In this paper, we have provided evi-
324"
CONCLUSIONS,0.35168738898756663,"dence that explanation shifts can be a more suitable indicator for detecting and identifying distribution
325"
CONCLUSIONS,0.35257548845470693,"shifts’ impact in machine learning models. We provide software, mathematical analysis examples,
326"
CONCLUSIONS,0.35346358792184723,"synthetic data, and real-data experimental evaluation. We found that measures of explanation shift
327"
CONCLUSIONS,0.3543516873889876,"can provide more insights than input distribution and prediction shift measures when monitoring
328"
CONCLUSIONS,0.3552397868561279,"machine learning models.
329"
REPRODUCIBILITY STATEMENT,0.3561278863232682,"Reproducibility Statement
330"
REPRODUCIBILITY STATEMENT,0.35701598579040855,"To ensure reproducibility, we make the data, code repositories, and experiments publicly available
331"
REPRODUCIBILITY STATEMENT,0.35790408525754885,"2. Also, an open-source Python package skshift3 is attached with methods routines and tutorials.
332"
REPRODUCIBILITY STATEMENT,0.35879218472468916,"For our experiments, we used default scikit-learn parameters [63]. We describe the system
333"
REPRODUCIBILITY STATEMENT,0.35968028419182946,"requirements and software dependencies of our experiments. Experiments were run on a 4 vCPU
334"
REPRODUCIBILITY STATEMENT,0.3605683836589698,"server with 32 GB RAM.
335"
REPRODUCIBILITY STATEMENT,0.3614564831261101,"1The ACS PUMS data dictionary contains a comprehensive list of available variables https://www.census.
gov/programs-surveys/acs/microdata/documentation.html"
REPRODUCIBILITY STATEMENT,0.3623445825932504,"2https://anonymous.4open.science/r/ExplanationShift-C0C0/README.md
3https://anonymous.4open.science/r/skshift-65A5/README.md"
REFERENCES,0.3632326820603908,"References
336"
REFERENCES,0.3641207815275311,"[1] Shai Ben-David, Tyler Lu, Teresa Luu, and Dávid Pál. Impossibility theorems for domain
337"
REFERENCES,0.3650088809946714,"adaptation. In Yee Whye Teh and D. Mike Titterington, editors, Proceedings of the Thirteenth
338"
REFERENCES,0.36589698046181174,"International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna
339"
REFERENCES,0.36678507992895204,"Resort, Sardinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceedings, pages 129–136.
340"
REFERENCES,0.36767317939609234,"JMLR.org, 2010.
341"
REFERENCES,0.3685612788632327,"[2] Zachary C. Lipton, Yu-Xiang Wang, and Alexander J. Smola. Detecting and correcting for label
342"
REFERENCES,0.369449378330373,"shift with black box predictors. In Jennifer G. Dy and Andreas Krause, editors, Proceedings
343"
REFERENCES,0.3703374777975133,"of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
344"
REFERENCES,0.37122557726465366,"Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research,
345"
REFERENCES,0.37211367673179396,"pages 3128–3136. PMLR, 2018.
346"
REFERENCES,0.37300177619893427,"[3] Saurabh Garg, Sivaraman Balakrishnan, Zachary Chase Lipton, Behnam Neyshabur, and Hanie
347"
REFERENCES,0.3738898756660746,"Sedghi. Leveraging unlabeled data to predict out-of-distribution performance. In NeurIPS 2021
348"
REFERENCES,0.3747779751332149,"Workshop on Distribution Shifts: Connecting Methods and Applications, 2021.
349"
REFERENCES,0.3756660746003552,"[4] Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, and Neil Lawrence. Continual learning
350"
REFERENCES,0.3765541740674956,"in practice. ArXiv preprint, https://arxiv.org/abs/1903.05202, 2019.
351"
REFERENCES,0.3774422735346359,"[5] Cloudera Fastforward Labs.
Inferring concept drift without labeled data.
https://
352"
REFERENCES,0.3783303730017762,"concept-drift.fastforwardlabs.com/, 2021.
353"
REFERENCES,0.37921847246891655,"[6] Saurabh Garg, Sivaraman Balakrishnan, Zico Kolter, and Zachary Lipton. Ratt: Leveraging
354"
REFERENCES,0.38010657193605685,"unlabeled data to guarantee generalization. In International Conference on Machine Learning,
355"
REFERENCES,0.38099467140319715,"pages 3598–3609. PMLR, 2021.
356"
REFERENCES,0.38188277087033745,"[7] Yuzhe Lu, Zhenlin Wang, Runtian Zhai, Soheil Kolouri, Joseph Campbell, and Katia P. Sycara.
357"
REFERENCES,0.3827708703374778,"Predicting out-of-distribution error with confidence optimal transport. In ICLR 2023 Workshop
358"
REFERENCES,0.3836589698046181,"on Pitfalls of limited data and computation for Trustworthy ML, 2023.
359"
REFERENCES,0.3845470692717584,"[8] Krishnaram Kenthapadi, Himabindu Lakkaraju, Pradeep Natarajan, and Mehrnoosh Sameki.
360"
REFERENCES,0.38543516873889877,"Model monitoring in practice: Lessons learned and open challenges. In Proceedings of the
361"
REFERENCES,0.38632326820603907,"28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD ’22, page
362"
REFERENCES,0.3872113676731794,"4800–4801, New York, NY, USA, 2022. Association for Computing Machinery.
363"
REFERENCES,0.38809946714031973,"[9] Johannes Haug, Alexander Braun, Stefan Zürn, and Gjergji Kasneci. Change detection for
364"
REFERENCES,0.38898756660746003,"local explainability in evolving data streams. In Proceedings of the 31st ACM International
365"
REFERENCES,0.38987566607460034,"Conference on Information & Knowledge Management, pages 706–716, 2022.
366"
REFERENCES,0.3907637655417407,"[10] Carlos Mougan and Dan Saattrup Nielsen. Monitoring model deterioration with explainable un-
367"
REFERENCES,0.391651865008881,"certainty estimation via non-parametric bootstrap. In AAAI Conference on Artificial Intelligence,
368"
REFERENCES,0.3925399644760213,"2023.
369"
REFERENCES,0.39342806394316165,"[11] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham
370"
REFERENCES,0.39431616341030196,"Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins,
371"
REFERENCES,0.39520426287744226,"Raja Chatila, and Francisco Herrera. Explainable artificial intelligence (xai): Concepts, tax-
372"
REFERENCES,0.3960923623445826,"onomies, opportunities and challenges toward responsible ai. Information Fusion, 58:82–115,
373"
REFERENCES,0.3969804618117229,"2020.
374"
REFERENCES,0.3978685612788632,"[12] Christoph Molnar. Interpretable Machine Learning. ., 2019. https://christophm.github.
375"
REFERENCES,0.3987566607460036,"io/interpretable-ml-book/.
376"
REFERENCES,0.3996447602131439,"[13] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and
377"
REFERENCES,0.4005328596802842,"Dino Pedreschi. A survey of methods for explaining black box models. ACM Comput. Surv.,
378"
REFERENCES,0.40142095914742454,"51(5), August 2018.
379"
REFERENCES,0.40230905861456484,"[14] Chip Huyen. Designing Machine Learning Systems: An Iterative Process for Production-Ready
380"
REFERENCES,0.40319715808170514,"Applications. O’Reilly, 2022.
381"
REFERENCES,0.40408525754884544,"[15] Joaquin Quiñonero-Candela, Masashi Sugiyama, Neil D Lawrence, and Anton Schwaighofer.
382"
REFERENCES,0.4049733570159858,"Dataset shift in machine learning. Mit Press, 2009.
383"
REFERENCES,0.4058614564831261,"[16] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning.
384"
REFERENCES,0.4067495559502664,"Springer Series in Statistics. Springer New York Inc., New York, NY, USA, 2001.
385"
REFERENCES,0.40763765541740676,"[17] Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J. Sutherland.
386"
REFERENCES,0.40852575488454707,"Learning deep kernels for non-parametric two-sample tests. In Proceedings of the 37th Interna-
387"
REFERENCES,0.40941385435168737,"tional Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume
388"
REFERENCES,0.4103019538188277,"119 of Proceedings of Machine Learning Research, pages 6316–6326. PMLR, 2020.
389"
REFERENCES,0.411190053285968,"[18] Chunjong Park, Anas Awadalla, Tadayoshi Kohno, and Shwetak N. Patel. Reliable and trustwor-
390"
REFERENCES,0.41207815275310833,"thy machine learning for health using dataset shift detection. In Marc’Aurelio Ranzato, Alina
391"
REFERENCES,0.4129662522202487,"Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances
392"
REFERENCES,0.413854351687389,"in Neural Information Processing Systems 34: Annual Conference on Neural Information
393"
REFERENCES,0.4147424511545293,"Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 3043–3056,
394"
REFERENCES,0.41563055062166965,"2021.
395"
REFERENCES,0.41651865008880995,"[19] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
396"
REFERENCES,0.41740674955595025,"out-of-distribution samples and adversarial attacks. In Samy Bengio, Hanna M. Wallach, Hugo
397"
REFERENCES,0.4182948490230906,"Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances
398"
REFERENCES,0.4191829484902309,"in Neural Information Processing Systems 31: Annual Conference on Neural Information
399"
REFERENCES,0.4200710479573712,"Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages
400"
REFERENCES,0.42095914742451157,"7167–7177, 2018.
401"
REFERENCES,0.4218472468916519,"[20] Kun Zhang, Bernhard Schölkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation
402"
REFERENCES,0.4227353463587922,"under target and conditional shift. In Proceedings of the 30th International Conference on
403"
REFERENCES,0.42362344582593253,"Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR
404"
REFERENCES,0.42451154529307283,"Workshop and Conference Proceedings, pages 819–827. JMLR.org, 2013.
405"
REFERENCES,0.42539964476021314,"[21] David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. In 5th International
406"
REFERENCES,0.42628774422735344,"Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
407"
REFERENCES,0.4271758436944938,"Conference Track Proceedings. OpenReview.net, 2017.
408"
REFERENCES,0.4280639431616341,"[22] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay
409"
REFERENCES,0.4289520426287744,"Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,
410"
REFERENCES,0.42984014209591476,"Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M. Beery, Jure
411"
REFERENCES,0.43072824156305506,"Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.
412"
REFERENCES,0.43161634103019536,"WILDS: A benchmark of in-the-wild distribution shifts. In Marina Meila and Tong Zhang,
413"
REFERENCES,0.4325044404973357,"editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021,
414"
REFERENCES,0.433392539964476,"18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research,
415"
REFERENCES,0.4342806394316163,"pages 5637–5664. PMLR, 2021.
416"
REFERENCES,0.4351687388987567,"[23] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya
417"
REFERENCES,0.436056838365897,"Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian
418"
REFERENCES,0.4369449378330373,"Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea
419"
REFERENCES,0.43783303730017764,"Finn, and Percy Liang. Extending the WILDS benchmark for unsupervised adaptation. CoRR,
420"
REFERENCES,0.43872113676731794,"abs/2112.05090, 2021.
421"
REFERENCES,0.43960923623445824,"[24] Andrey Malinin, Neil Band, German Chesnokov, Yarin Gal, Mark JF Gales, Alexey Noskov, An-
422"
REFERENCES,0.4404973357015986,"drey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, et al. Shifts: A dataset
423"
REFERENCES,0.4413854351687389,"of real distributional shift across multiple large-scale tasks. arXiv preprint arXiv:2107.07455,
424"
REFERENCES,0.4422735346358792,"2021.
425"
REFERENCES,0.44316163410301956,"[25] Andrey Malinin, Andreas Athanasopoulos, Muhamed Barakovic, Meritxell Bach Cuadra,
426"
REFERENCES,0.44404973357015987,"Mark JF Gales, Cristina Granziera, Mara Graziani, Nikolay Kartashev, Konstantinos Kyri-
427"
REFERENCES,0.44493783303730017,"akopoulos, Po-Jui Lu, et al. Shifts 2.0: Extending the dataset of real distributional shifts. arXiv
428"
REFERENCES,0.44582593250444047,"preprint arXiv:2206.15407, 2022.
429"
REFERENCES,0.4467140319715808,"[26] Andrey Malinin, Neil Band, Yarin Gal, Mark J. F. Gales, Alexander Ganshin, German Ches-
430"
REFERENCES,0.44760213143872113,"nokov, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal
431"
REFERENCES,0.44849023090586143,"Raina, Vyas Raina, Denis Roginskiy, Mariya Shmatova, Panagiotis Tigas, and Boris Yangel.
432"
REFERENCES,0.4493783303730018,"Shifts: A dataset of real distributional shift across multiple large-scale tasks. In Joaquin Van-
433"
REFERENCES,0.4502664298401421,"schoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems
434"
REFERENCES,0.4511545293072824,"Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
435"
REFERENCES,0.45204262877442275,"2021, virtual, 2021.
436"
REFERENCES,0.45293072824156305,"[27] Stephan Rabanser, Stephan Günnemann, and Zachary C. Lipton. Failing loudly: An empirical
437"
REFERENCES,0.45381882770870335,"study of methods for detecting dataset shift. In Hanna M. Wallach, Hugo Larochelle, Alina
438"
REFERENCES,0.4547069271758437,"Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances
439"
REFERENCES,0.455595026642984,"in Neural Information Processing Systems 32: Annual Conference on Neural Information
440"
REFERENCES,0.4564831261101243,"Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages
441"
REFERENCES,0.4573712255772647,"1394–1406, 2019.
442"
REFERENCES,0.458259325044405,"[28] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still
443"
REFERENCES,0.4591474245115453,"outperform deep learning on typical tabular data?
In Thirty-sixth Conference on Neural
444"
REFERENCES,0.46003552397868563,"Information Processing Systems Datasets and Benchmarks Track, 2022.
445"
REFERENCES,0.46092362344582594,"[29] Shereen Elsayed, Daniela Thyssens, Ahmed Rashed, Lars Schmidt-Thieme, and Hadi Samer
446"
REFERENCES,0.46181172291296624,"Jomaa.
Do we really need deep learning models for time series forecasting?
CoRR,
447"
REFERENCES,0.4626998223801066,"abs/2101.02118, 2021.
448"
REFERENCES,0.4635879218472469,"[30] Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and
449"
REFERENCES,0.4644760213143872,"Gjergji Kasneci. Deep neural networks and tabular data: A survey, 2021.
450"
REFERENCES,0.46536412078152756,"[31] Lingjiao Chen, Matei Zaharia, and James Y. Zou. Estimating and explaining model performance
451"
REFERENCES,0.46625222024866786,"when both covariates and labels shift. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
452"
REFERENCES,0.46714031971580816,"Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
453"
REFERENCES,0.46802841918294846,"[32] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, and Feng Liu. Is out-of-distribution
454"
REFERENCES,0.4689165186500888,"detection learnable? In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
455"
REFERENCES,0.4698046181172291,"editors, Advances in Neural Information Processing Systems, 2022.
456"
REFERENCES,0.4706927175843694,"[33] Lily H. Zhang, Mark Goldstein, and Rajesh Ranganath. Understanding failures in out-of-
457"
REFERENCES,0.4715808170515098,"distribution detection with deep generative models. In Marina Meila and Tong Zhang, editors,
458"
REFERENCES,0.4724689165186501,"Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24
459"
REFERENCES,0.4733570159857904,"July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages
460"
REFERENCES,0.47424511545293074,"12427–12436. PMLR, 2021.
461"
REFERENCES,0.47513321492007105,"[34] Joris Guerin, Kevin Delmas, Raul Sena Ferreira, and Jérémie Guiochet. Out-of-distribution
462"
REFERENCES,0.47602131438721135,"detection is not all you need. In NeurIPS ML Safety Workshop, 2022.
463"
REFERENCES,0.4769094138543517,"[35] Kailash Budhathoki, Dominik Janzing, Patrick Blöbaum, and Hoiyi Ng. Why did the distribution
464"
REFERENCES,0.477797513321492,"change? In Arindam Banerjee and Kenji Fukumizu, editors, The 24th International Conference
465"
REFERENCES,0.4786856127886323,"on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume
466"
REFERENCES,0.47957371225577267,"130 of Proceedings of Machine Learning Research, pages 1666–1674. PMLR, 2021.
467"
REFERENCES,0.48046181172291297,"[36] Haoran Zhang, Harvineet Singh, and Shalmali Joshi. ”why did the model fail?”: Attributing
468"
REFERENCES,0.48134991119005327,"model performance changes to distribution shifts. In ICML 2022: Workshop on Spurious
469"
REFERENCES,0.4822380106571936,"Correlations, Invariance and Stability, 2022.
470"
REFERENCES,0.48312611012433393,"[37] Jessica Schrouff, Natalie Harris, Oluwasanmi O Koyejo, Ibrahim Alabdulmohsin, Eva Schnider,
471"
REFERENCES,0.48401420959147423,"Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Chrsitina Chen, Awa
472"
REFERENCES,0.4849023090586146,"Dieng, Yuan Liu, Vivek Natarajan, Alan Karthikesalingam, Katherine A Heller, Silvia Chiappa,
473"
REFERENCES,0.4857904085257549,"and Alexander D’Amour. Diagnosing failures of fairness transfer across distribution shift in
474"
REFERENCES,0.4866785079928952,"real-world medical settings. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
475"
REFERENCES,0.48756660746003555,"Cho, editors, Advances in Neural Information Processing Systems, 2022.
476"
REFERENCES,0.48845470692717585,"[38] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin, Bala Nair,
477"
REFERENCES,0.48934280639431615,"Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to
478"
REFERENCES,0.49023090586145646,"global understanding with explainable ai for trees. Nature Machine Intelligence, 2(1):2522–
479"
REFERENCES,0.4911190053285968,"5839, 2020.
480"
REFERENCES,0.4920071047957371,"[39] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In
481"
REFERENCES,0.4928952042628774,"Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.
482"
REFERENCES,0.4937833037300178,"Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems
483"
REFERENCES,0.4946714031971581,"30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
484"
REFERENCES,0.4955595026642984,"Long Beach, CA, USA, pages 4765–4774, 2017.
485"
REFERENCES,0.49644760213143874,"[40] L. S. Shapley. A Value for n-Person Games, pages 307–318. Princeton University Press, 1953.
486"
REFERENCES,0.49733570159857904,"[41] Eyal Winter. Chapter 53 the shapley value. In ., volume 3 of Handbook of Game Theory with
487"
REFERENCES,0.49822380106571934,"Economic Applications, pages 2025–2054. Elsevier, 2002.
488"
REFERENCES,0.4991119005328597,"[42] Robert J Aumann and Jacques H Dreze. Cooperative games with coalition structures. Interna-
489"
REFERENCES,0.5,"tional Journal of game theory, 3(4):217–237, 1974.
490"
REFERENCES,0.5008880994671403,"[43] Artjom Zern, Klaus Broelemann, and Gjergji Kasneci. Interventional shap values and interaction
491"
REFERENCES,0.5017761989342806,"values for piecewise linear regression trees. In Proceedings of the AAAI Conference on Artificial
492"
REFERENCES,0.5026642984014209,"Intelligence, 2023.
493"
REFERENCES,0.5035523978685613,"[44] Hugh Chen, Ian C. Covert, Scott M. Lundberg, and Su-In Lee. Algorithms to estimate shapley
494"
REFERENCES,0.5044404973357016,"value feature attributions. CoRR, abs/2207.07605, 2022.
495"
REFERENCES,0.5053285968028419,"[45] Christopher Frye, Colin Rowat, and Ilya Feige. Asymmetric shapley values: incorporating causal
496"
REFERENCES,0.5062166962699822,"knowledge into model-agnostic explainability. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
497"
REFERENCES,0.5071047957371225,"Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information
498"
REFERENCES,0.5079928952042628,"Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
499"
REFERENCES,0.5088809946714032,"NeurIPS 2020, December 6-12, 2020, virtual, 2020.
500"
REFERENCES,0.5097690941385435,"[46] Hugh Chen, Joseph D. Janizek, Scott M. Lundberg, and Su-In Lee. True to the model or true to
501"
REFERENCES,0.5106571936056838,"the data? CoRR, abs/2006.16234, 2020.
502"
REFERENCES,0.5115452930728241,"[47] Kjersti Aas, Martin Jullum, and Anders Løland. Explaining individual predictions when features
503"
REFERENCES,0.5124333925399644,"are dependent: More accurate approximations to shapley values. Artif. Intell., 298:103502,
504"
REFERENCES,0.5133214920071048,"2021.
505"
REFERENCES,0.5142095914742452,"[48] Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. ""why should I trust you?"": Explaining
506"
REFERENCES,0.5150976909413855,"the predictions of any classifier. In Balaji Krishnapuram, Mohak Shah, Alexander J. Smola,
507"
REFERENCES,0.5159857904085258,"Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi, editors, Proceedings of the 22nd ACM
508"
REFERENCES,0.5168738898756661,"SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco,
509"
REFERENCES,0.5177619893428064,"CA, USA, August 13-17, 2016, pages 1135–1144. ACM, 2016.
510"
REFERENCES,0.5186500888099467,"[49] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Model-agnostic interpretability of
511"
REFERENCES,0.5195381882770871,"machine learning, 2016.
512"
REFERENCES,0.5204262877442274,"[50] Jose M. Alvarez, Kristen M. Scott, Salvatore Ruggieri, and Bettina Berendt. Domain adaptive
513"
REFERENCES,0.5213143872113677,"decision trees: Implications for accuracy and fairness. In Proceedings of the 2023 ACM Confer-
514"
REFERENCES,0.522202486678508,"ence on Fairness, Accountability, and Transparency. Association for Computing Machinery,
515"
REFERENCES,0.5230905861456483,"2023.
516"
REFERENCES,0.5239786856127886,"[51] Arnaud Van Looveren, Janis Klaise, Giovanni Vacanti, Oliver Cobb, Ashley Scillitoe, and
517"
REFERENCES,0.5248667850799289,"Robert Samoilescu. Alibi detect: Algorithms for outlier, adversarial and drift detection, 2019.
518"
REFERENCES,0.5257548845470693,"[52] Byol Kim, Chen Xu, and Rina Foygel Barber. Predictive inference is free with the jackknife+-
519"
REFERENCES,0.5266429840142096,"after-bootstrap. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
520"
REFERENCES,0.5275310834813499,"and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
521"
REFERENCES,0.5284191829484902,"Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
522"
REFERENCES,0.5293072824156305,"2020, virtual, 2020.
523"
REFERENCES,0.5301953818827708,"[53] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
524"
REFERENCES,0.5310834813499112,"[54] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for
525"
REFERENCES,0.5319715808170515,"fair machine learning. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy
526"
REFERENCES,0.5328596802841918,"Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing
527"
REFERENCES,0.5337477797513321,"Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS
528"
REFERENCES,0.5346358792184724,"2021, December 6-14, 2021, virtual, pages 6478–6490, 2021.
529"
REFERENCES,0.5355239786856127,"[55] Stackoverflow. Developer survey results 2019, 2019.
530"
REFERENCES,0.5364120781527532,"[56] Joseph D Romano, Trang T Le, William La Cava, John T Gregg, Daniel J Goldberg, Praneel
531"
REFERENCES,0.5373001776198935,"Chakraborty, Natasha L Ray, Daniel Himmelstein, Weixuan Fu, and Jason H Moore. Pmlb v1.0:
532"
REFERENCES,0.5381882770870338,"an open source dataset collection for benchmarking machine learning methods. arXiv preprint
533"
REFERENCES,0.5390763765541741,"arXiv:2012.00058v2, 2021.
534"
REFERENCES,0.5399644760213144,"[57] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution
535"
REFERENCES,0.5408525754884547,"detection. Advances in Neural Information Processing Systems, 34, 2021.
536"
REFERENCES,0.5417406749555951,"[58] Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. A unified view of label
537"
REFERENCES,0.5426287744227354,"shift estimation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,
538"
REFERENCES,0.5435168738898757,"Advances in Neural Information Processing Systems, volume 33, pages 3290–3300. Curran
539"
REFERENCES,0.544404973357016,"Associates, Inc., 2020.
540"
REFERENCES,0.5452930728241563,"[59] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of
541"
REFERENCES,0.5461811722912966,"the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
542"
REFERENCES,0.5470692717584369,"KDD ’16, pages 785–794, New York, NY, USA, 2016. ACM.
543"
REFERENCES,0.5479573712255773,"[60] Liudmila Ostroumova Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Doro-
544"
REFERENCES,0.5488454706927176,"gush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. In Samy Bengio,
545"
REFERENCES,0.5497335701598579,"Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman
546"
REFERENCES,0.5506216696269982,"Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on
547"
REFERENCES,0.5515097690941385,"Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,
548"
REFERENCES,0.5523978685612788,"Canada, pages 6639–6649, 2018.
549"
REFERENCES,0.5532859680284192,"[61] Christina Baek, Yiding Jiang, Aditi Raghunathan, and J Zico Kolter. Agreement-on-the-line:
550"
REFERENCES,0.5541740674955595,"Predicting the performance of neural networks under distribution shift. In Alice H. Oh, Alekh
551"
REFERENCES,0.5550621669626998,"Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information
552"
REFERENCES,0.5559502664298401,"Processing Systems, 2022.
553"
REFERENCES,0.5568383658969804,"[62] John Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar,
554"
REFERENCES,0.5577264653641207,"Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation
555"
REFERENCES,0.5586145648312612,"between out-of-distribution and in-distribution generalization. In Marina Meila and Tong Zhang,
556"
REFERENCES,0.5595026642984015,"editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021,
557"
REFERENCES,0.5603907637655418,"18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research,
558"
REFERENCES,0.5612788632326821,"pages 7721–7735. PMLR, 2021.
559"
REFERENCES,0.5621669626998224,"[63] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,
560"
REFERENCES,0.5630550621669627,"Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-
561"
REFERENCES,0.5639431616341031,"learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830,
562"
REFERENCES,0.5648312611012434,"2011.
563"
REFERENCES,0.5657193605683837,"[64] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
564"
REFERENCES,0.566607460035524,"examples in neural networks. In 5th International Conference on Learning Representations,
565"
REFERENCES,0.5674955595026643,"ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,
566"
REFERENCES,0.5683836589698046,"2017.
567"
REFERENCES,0.5692717584369449,"[65] Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A. DePristo, Joshua V.
568"
REFERENCES,0.5701598579040853,"Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In
569"
REFERENCES,0.5710479573712256,"Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox,
570"
REFERENCES,0.5719360568383659,"and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual
571"
REFERENCES,0.5728241563055062,"Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,
572"
REFERENCES,0.5737122557726465,"2019, Vancouver, BC, Canada, pages 14680–14691, 2019.
573"
REFERENCES,0.5746003552397868,"[66] Weitang Liu, Xiaoyun Wang, John D. Owens, and Yixuan Li. Energy-based out-of-distribution
574"
REFERENCES,0.5754884547069272,"detection. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
575"
REFERENCES,0.5763765541740675,"and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
576"
REFERENCES,0.5772646536412078,"Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
577"
REFERENCES,0.5781527531083481,"2020, virtual, 2020.
578"
REFERENCES,0.5790408525754884,"[67] Haoran Wang, Weitang Liu, Alex Bocchieri, and Yixuan Li. Can multi-label classification
579"
REFERENCES,0.5799289520426287,"networks know what they don’t know? In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.
580"
REFERENCES,0.5808170515097691,"Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information
581"
REFERENCES,0.5817051509769094,"Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021,
582"
REFERENCES,0.5825932504440497,"NeurIPS 2021, December 6-14, 2021, virtual, pages 29074–29087, 2021.
583"
REFERENCES,0.58348134991119,"[68] Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting
584"
REFERENCES,0.5843694493783304,"distributional shifts in the wild. Advances in Neural Information Processing Systems 34: Annual
585"
REFERENCES,0.5852575488454707,"Conference on Neural Information Processing Systems 2021, NeurIPS 2021, abs/2110.00218,
586"
REFERENCES,0.5861456483126111,"2021.
587"
REFERENCES,0.5870337477797514,"[69] Chunjong Park, Anas Awadalla, Tadayoshi Kohno, and Shwetak N. Patel. Reliable and trustwor-
588"
REFERENCES,0.5879218472468917,"thy machine learning for health using dataset shift detection. Advances in Neural Information
589"
REFERENCES,0.588809946714032,"Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021,
590"
REFERENCES,0.5896980461811723,"NeurIPS 2021, abs/2110.14019, 2021.
591"
REFERENCES,0.5905861456483126,"[70] Chiara Balestra, Bin Li, and Emmanuel Müller. Enabling the visualization of distributional shift
592"
REFERENCES,0.5914742451154529,"using shapley values. In NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods
593"
REFERENCES,0.5923623445825933,"and Applications, 2022.
594"
REFERENCES,0.5932504440497336,"[71] Johannes Haug and Gjergji Kasneci. Learning parameter distributions to detect concept drift
595"
REFERENCES,0.5941385435168739,"in data streams. In 2020 25th International Conference on Pattern Recognition (ICPR), pages
596"
REFERENCES,0.5950266429840142,"9452–9459. IEEE, 2021.
597"
REFERENCES,0.5959147424511545,"[72] Yongchan Kwon, Manuel A. Rivas, and James Zou. Efficient computation and analysis of
598"
REFERENCES,0.5968028419182948,"distributional shapley values. In Arindam Banerjee and Kenji Fukumizu, editors, The 24th
599"
REFERENCES,0.5976909413854352,"International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15,
600"
REFERENCES,0.5985790408525755,"2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pages 793–801.
601"
REFERENCES,0.5994671403197158,"PMLR, 2021.
602"
REFERENCES,0.6003552397868561,"[73] Amirata Ghorbani and James Y. Zou. Data shapley: Equitable valuation of data for machine
603"
REFERENCES,0.6012433392539964,"learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th
604"
REFERENCES,0.6021314387211367,"International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
605"
REFERENCES,0.6030195381882771,"California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2242–2251.
606"
REFERENCES,0.6039076376554174,"PMLR, 2019.
607"
REFERENCES,0.6047957371225577,"[74] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. L-shapley and c-shapley:
608"
REFERENCES,0.605683836589698,"Efficient model interpretation for structured data. In 7th International Conference on Learning
609"
REFERENCES,0.6065719360568383,"Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
610"
REFERENCES,0.6074600355239786,"[75] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. Fooling
611"
REFERENCES,0.6083481349911191,"LIME and SHAP: adversarial attacks on post hoc explanation methods. In Annette N. Markham,
612"
REFERENCES,0.6092362344582594,"Julia Powles, Toby Walsh, and Anne L. Washington, editors, AIES ’20: AAAI/ACM Conference
613"
REFERENCES,0.6101243339253997,"on AI, Ethics, and Society, New York, NY, USA, February 7-8, 2020, pages 180–186. ACM,
614"
REFERENCES,0.61101243339254,"2020.
615"
REFERENCES,0.6119005328596803,"Contents
616"
INTRODUCTION,0.6127886323268206,"1
Introduction
1
617"
FOUNDATIONS AND RELATED WORK,0.6136767317939609,"2
Foundations and Related Work
2
618"
FOUNDATIONS AND RELATED WORK,0.6145648312611013,"2.1
Basic Notions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
619"
RELATED WORK ON TABULAR DATA,0.6154529307282416,"2.2
Related Work on Tabular Data
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
620"
RELATED WORK ON TABULAR DATA,0.6163410301953819,"2.3
Explainable AI: Local Feature Attributions
. . . . . . . . . . . . . . . . . . . . .
3
621"
A MODEL FOR EXPLANATION SHIFT DETECTION,0.6172291296625222,"3
A Model for Explanation Shift Detection
4
622"
RELATIONSHIPS BETWEEN COMMON DISTRIBUTION SHIFTS AND EXPLANATION SHIFTS,0.6181172291296625,"4
Relationships between Common Distribution Shifts and Explanation Shifts
5
623"
RELATIONSHIPS BETWEEN COMMON DISTRIBUTION SHIFTS AND EXPLANATION SHIFTS,0.6190053285968028,"4.1
Explanation Shift vs Data Shift . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
624"
RELATIONSHIPS BETWEEN COMMON DISTRIBUTION SHIFTS AND EXPLANATION SHIFTS,0.6198934280639432,"4.2
Explanation Shift vs Prediction Shift . . . . . . . . . . . . . . . . . . . . . . . . .
5
625"
RELATIONSHIPS BETWEEN COMMON DISTRIBUTION SHIFTS AND EXPLANATION SHIFTS,0.6207815275310835,"4.3
Explanation Shift vs Concept Shift . . . . . . . . . . . . . . . . . . . . . . . . . .
6
626"
EMPIRICAL EVALUATION,0.6216696269982238,"5
Empirical Evaluation
6
627"
EMPIRICAL EVALUATION,0.6225577264653641,"5.1
Baseline Methods and Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
628"
EMPIRICAL EVALUATION,0.6234458259325044,"5.2
Experiments on Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
629"
EMPIRICAL EVALUATION,0.6243339253996447,"5.3
Experiments on Natural Data: Inspecting Explanation Shifts
. . . . . . . . . . . .
8
630"
EMPIRICAL EVALUATION,0.6252220248667851,"5.3.1
Novel Covariate Group . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
631"
EMPIRICAL EVALUATION,0.6261101243339254,"5.3.2
Geopolitical and Temporal Shift . . . . . . . . . . . . . . . . . . . . . . .
8
632"
DISCUSSION,0.6269982238010657,"6
Discussion
9
633"
CONCLUSIONS,0.627886323268206,"7
Conclusions
9
634"
CONCLUSIONS,0.6287744227353463,"A Extended Related Work
17
635"
CONCLUSIONS,0.6296625222024866,"A.1
Out-Of-Distribution Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
636"
CONCLUSIONS,0.6305506216696269,"A.2
Explainability and Distribution Shift . . . . . . . . . . . . . . . . . . . . . . . . .
17
637"
CONCLUSIONS,0.6314387211367674,"B
Extended Analytical Examples
18
638"
CONCLUSIONS,0.6323268206039077,"B.1
Explanation Shift vs Prediction Shift . . . . . . . . . . . . . . . . . . . . . . . . .
18
639"
CONCLUSIONS,0.633214920071048,"B.2
Explanation Shifts vs Input Data Distribution Shifts . . . . . . . . . . . . . . . . .
18
640"
CONCLUSIONS,0.6341030195381883,"B.2.1
Multivariate Shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
641"
CONCLUSIONS,0.6349911190053286,"B.2.2
Concept Shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
642"
CONCLUSIONS,0.6358792184724689,"C Further Experiments on Synthetic Data
19
643"
CONCLUSIONS,0.6367673179396093,"C.1
Detecting multivariate shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
644"
CONCLUSIONS,0.6376554174067496,"C.2
Detecting concept shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
645"
CONCLUSIONS,0.6385435168738899,"C.3
Uninformative features on synthetic data . . . . . . . . . . . . . . . . . . . . . . .
20
646"
CONCLUSIONS,0.6394316163410302,"C.4
Explanation shift that does not affect the prediction . . . . . . . . . . . . . . . . .
20
647"
CONCLUSIONS,0.6403197158081705,"D Further Experiments on Real Data
21
648"
CONCLUSIONS,0.6412078152753108,"D.1 ACS Employment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
649"
CONCLUSIONS,0.6420959147424512,"D.2 ACS Travel Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
650"
CONCLUSIONS,0.6429840142095915,"D.3 ACS Mobility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
651"
CONCLUSIONS,0.6438721136767318,"D.4
StackOverflow Survey Data: Novel Covariate Group
. . . . . . . . . . . . . . . .
22
652"
CONCLUSIONS,0.6447602131438721,"E
Experiments with Modeling Methods and Hyperparameters
23
653"
CONCLUSIONS,0.6456483126110124,"E.1
Varying Estimator and Explanation Shift Detector . . . . . . . . . . . . . . . . . .
23
654"
CONCLUSIONS,0.6465364120781527,"E.2
Hyperparameters Sensitivity Evaluation . . . . . . . . . . . . . . . . . . . . . . .
23
655"
CONCLUSIONS,0.6474245115452931,"F
LIME as an Alternative Explanation Method
24
656"
CONCLUSIONS,0.6483126110124334,"F.1
Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
657"
CONCLUSIONS,0.6492007104795737,"A
Extended Related Work
658"
CONCLUSIONS,0.650088809946714,"This section provides an in-depth review of the related theoretical works that inform our research.
659"
CONCLUSIONS,0.6509769094138543,"A.1
Out-Of-Distribution Detection
660"
CONCLUSIONS,0.6518650088809946,"Evaluating how two distributions differ has been a widely studied topic in the statistics and statistical
661"
CONCLUSIONS,0.6527531083481349,"learning literature [16, 15, 17], that have advanced recently in last years [18, 19, 20]. [27] provides a
662"
CONCLUSIONS,0.6536412078152753,"comprehensive empirical investigation, examining how dimensionality reduction and two-sample
663"
CONCLUSIONS,0.6545293072824157,"testing might be combined to produce a practical pipeline for detecting distribution shifts in real-life
664"
CONCLUSIONS,0.655417406749556,"machine learning systems. Other methods to detect if new data is OOD have relied on neural networks
665"
CONCLUSIONS,0.6563055062166963,"based on the prediction distributions [57, 58]. They use the maximum softmax probabilities/likelihood
666"
CONCLUSIONS,0.6571936056838366,"as a confidence score [64], temperature or energy-based scores [65, 66, 67], they extract information
667"
CONCLUSIONS,0.6580817051509769,"from the gradient space [68], they fit a Gaussian distribution to the embedding, or they use the
668"
CONCLUSIONS,0.6589698046181173,"Mahalanobis distance for out-of-distribution detection [19, 69].
669"
CONCLUSIONS,0.6598579040852576,"Many of these methods are explicitly developed for neural networks that operate on image and text
670"
CONCLUSIONS,0.6607460035523979,"data, and often they can not be directly applied to traditional ML techniques. For image and text
671"
CONCLUSIONS,0.6616341030195382,"data, one may build on the assumption that the relationships between relevant predictor variables (X)
672"
CONCLUSIONS,0.6625222024866785,"and response variables (Y ) remain unchanged, i.e., that no concept shift occurs. For instance, the
673"
CONCLUSIONS,0.6634103019538188,"essence of how a dog looks remains unchanged over different data sets, even if contexts may change.
674"
CONCLUSIONS,0.6642984014209592,"Thus, one can define invariances on the latent spaces of deep neural models, which are not applicable
675"
CONCLUSIONS,0.6651865008880995,"to tabular data in a likewise manner. For example, predicting buying behavior before, during, and
676"
CONCLUSIONS,0.6660746003552398,"after the COVID-19 pandemic constitutes a conceptual shift that is not amenable to such methods.
677"
CONCLUSIONS,0.6669626998223801,"We focus on such tabular data where techniques such as gradient boosting decision trees achieve
678"
CONCLUSIONS,0.6678507992895204,"state-of-the-art model performance [28, 29, 30].
679"
CONCLUSIONS,0.6687388987566607,"A.2
Explainability and Distribution Shift
680"
CONCLUSIONS,0.6696269982238011,"Another approach using Shapley values by Balestra et al. [70] allows for tracking distributional shifts
681"
CONCLUSIONS,0.6705150976909414,"and their impact among for categorical time series using slidSHAP, a novel method for unlabelled
682"
CONCLUSIONS,0.6714031971580817,"data streams. In our work, we define the explanation distributions and exploit its theoretical properties
683"
CONCLUSIONS,0.672291296625222,"under distribution shift where we perform a two-sample classifier test to detect
684"
CONCLUSIONS,0.6731793960923623,"Haut et al. [71] track changes in the distribution of model parameter values that are directly related
685"
CONCLUSIONS,0.6740674955595026,"to the input features to identify concept drift early on in data streams. In a more recent paper,Haug
686"
CONCLUSIONS,0.6749555950266429,"et al. [9] also exploits the idea that local changes to feature attributions and distribution shifts are
687"
CONCLUSIONS,0.6758436944937833,"strongly intertwined and uses this idea to update the local feature attributions efficiently. Their work
688"
CONCLUSIONS,0.6767317939609236,"focuses on model retraining and concept shift, in our work the original estimator fθ remains unaltered,
689"
CONCLUSIONS,0.677619893428064,"and since we are in an unsupervised monitoring scenario we can’t detect concept shift see discussion
690"
CONCLUSIONS,0.6785079928952042,"in Section 6
691"
CONCLUSIONS,0.6793960923623446,"B
Extended Analytical Examples
692"
CONCLUSIONS,0.6802841918294849,"This appendix provides more details about the analytical examples presented in Section 4.1.
693"
CONCLUSIONS,0.6811722912966253,"B.1
Explanation Shift vs Prediction Shift
694"
CONCLUSIONS,0.6820603907637656,"Proposition 2. Given a model fθ : DX →DY . If fθ(x′) ̸= fθ(x), then S(fθ, x′) ̸= S(fθ, x).
695"
CONCLUSIONS,0.6829484902309059,"Given
fθ(x) ̸= fθ(x′)
(7) p
X"
CONCLUSIONS,0.6838365896980462,"j=1
Sj(fθ, x) = fθ(x) −EX[fθ(DX)]
(8)"
CONCLUSIONS,0.6847246891651865,"then
S(f, x) ̸= S(f, x′)
(9)"
CONCLUSIONS,0.6856127886323268,"Example B.1. Explanation shift that does not affect the prediction distribution Given Dtr is
696"
CONCLUSIONS,0.6865008880994672,"generated from (X1, X2, Y ), X1 ∼U(0, 1), X2 ∼U(1, 2), Y = X1 + X2 + ϵ and thus the model
697"
CONCLUSIONS,0.6873889875666075,"is f(x) = x1 + x2. If Dnew is generated from Xnew
1
∼U(1, 2), Xnew
2
∼U(0, 1), the pre-
698"
CONCLUSIONS,0.6882770870337478,"diction distributions are identical fθ(Dtr
X), fθ(Dnew
X
), but explanation distributions are different
699"
CONCLUSIONS,0.6891651865008881,"S(fθ, Dtr
X) ̸= S(fθ, Dnew
X
)
700"
CONCLUSIONS,0.6900532859680284,"∀i ∈{1, 2}
Si(fθ, x) = αi · xi
(10)
∀i ∈{1, 2} ⇒Si(fθ, DX)) ̸= Si(fθ, Dnew
X
)
(11)
⇒fθ(DX) = fθ(Dnew
X
)
(12)"
CONCLUSIONS,0.6909413854351687,"B.2
Explanation Shifts vs Input Data Distribution Shifts
701"
CONCLUSIONS,0.6918294849023091,"B.2.1
Multivariate Shift
702"
CONCLUSIONS,0.6927175843694494,"Example B.2. Multivariate Shift Let Dtr
X = (Dnew
X1 , Dnew
X2 ) ∼N
h
µ1
µ2
i
,
h
σ2
x1
0"
CONCLUSIONS,0.6936056838365897,"0
σ2
x2"
CONCLUSIONS,0.69449378330373,"i
,Dnew
X
=
703"
CONCLUSIONS,0.6953818827708703,"(Dnew
X1 , Dnew
X2 ) ∼N
h
µ1
µ2
i
,
h
σ2
x1
ρσx1 σx2
ρσx1 σx2
σ2
x2"
CONCLUSIONS,0.6962699822380106,"i
. We fit a linear model fθ(X1, X2) = γ + a · X1 +
704"
CONCLUSIONS,0.6971580817051509,"b · X2.
DX1 and DX2 are identically distributed with Dnew
X1
and Dnew
X2 , respectively, while this
705"
CONCLUSIONS,0.6980461811722913,"does not hold for the corresponding SHAP values Sj(fθ, Dtr
X) and Sj(fθ, Dval
X ).
706"
CONCLUSIONS,0.6989342806394316,"S1(fθ, x) = a(x1 −µ1)
(13)
S1(fθ, xnew) =
(14) = 1"
CONCLUSIONS,0.6998223801065719,"2[val({1, 2}) −val({2})] + 1"
CONCLUSIONS,0.7007104795737122,"2[val({1}) −val(∅)]
(15)"
CONCLUSIONS,0.7015985790408525,"val({1, 2}) = E[fθ|X1 = x1, X2 = x2] = ax1 + bx2
(16)
val(∅) = E[fθ] = aµ1 + bµ2
(17)
val({1}) = E[fθ(x)|X1 = x1] + bµ2
(18)"
CONCLUSIONS,0.7024866785079928,val({1}) = µ1 + ρρx1
CONCLUSIONS,0.7033747779751333,"σx2
(x1 −σ1) + bµ2
(19)"
CONCLUSIONS,0.7042628774422736,val({2}) = µ2 + ρσx2
CONCLUSIONS,0.7051509769094139,"σx1
(x2 −µ2) + aµ1
(20)"
CONCLUSIONS,0.7060390763765542,"⇒S1(fθ, xnew) ̸= a(x1 −µ1)
(21)"
CONCLUSIONS,0.7069271758436945,"B.2.2
Concept Shift
707"
CONCLUSIONS,0.7078152753108348,"One of the most challenging types of distribution shift to detect are cases where distributions are
708"
CONCLUSIONS,0.7087033747779752,"equal between source and unseen data-set P(Dtr
X) = P(Dnew
X
) and the target variable P(Dtr
Y ) =
709"
CONCLUSIONS,0.7095914742451155,"P(Dnew
Y
) and what changes are the relationships that features have with the target P(Dtr
Y |Dtr
X) ̸=
710"
CONCLUSIONS,0.7104795737122558,"P(Dnew
Y
|Dnew
X
), this kind of distribution shift is also known as concept drift or posterior shift [14]
711"
CONCLUSIONS,0.7113676731793961,"and is especially difficult to notice, as it requires labeled data to detect. The following example
712"
CONCLUSIONS,0.7122557726465364,"compares how the explanations change for two models fed with the same input data and different
713"
CONCLUSIONS,0.7131438721136767,"target relations.
714"
CONCLUSIONS,0.7140319715808171,"Example B.3.
Concept shift Let DX = (X1, X2) ∼N(µ, I), and Dnew
X
= (Xnew
1
, Xnew
2
) ∼
715"
CONCLUSIONS,0.7149200710479574,"N(µ, I), where I is an identity matrix of order two and µ = (µ1, µ2). We now create two synthetic
716"
CONCLUSIONS,0.7158081705150977,"targets Y = a+α·X1 +β ·X2 +ϵ and Y new = a+β ·X1 +α·X2 +ϵ. Let fθ be a linear regression
717"
CONCLUSIONS,0.716696269982238,"model trained on fθ : DX →DY ) and hϕ another linear model trained on hϕ : Dnew
X
→Dnew
Y
).
718"
CONCLUSIONS,0.7175843694493783,"Then P(fθ(X)) = P(hϕ(Xnew)), P(X) = P(Xnew) but S(fθ, X) ̸= S(hϕ, X).
719"
CONCLUSIONS,0.7184724689165186,"X ∼N(µ, σ2 · I), Xnew ∼N(µ, σ2 · I)
(22)
→P(DX) = P(Dnew
X
)
(23)"
CONCLUSIONS,0.7193605683836589,"Y ∼a + αN(µ, σ2) + βN(µ, σ2) + N(0, σ
′2)
(24)"
CONCLUSIONS,0.7202486678507993,"Y new ∼a + βN(µ, σ2) + αN(µ, σ2) + N(0, σ
′2)
(25)
→P(DY ) = P(Dnew
Y
)
(26)"
CONCLUSIONS,0.7211367673179396,"S(fθ, DX) =

α(X1 −µ1)
β(X2 −µ2)"
CONCLUSIONS,0.7220248667850799,"
∼

N(µ1, α2σ2)
N(µ2, β2σ2)"
CONCLUSIONS,0.7229129662522202,"
(27)"
CONCLUSIONS,0.7238010657193605,"S(hϕ, DX) =

β(X1 −µ1)
α(X2 −µ2)"
CONCLUSIONS,0.7246891651865008,"
∼

N(µ1, β2σ2)
N(µ2, α2σ2)"
CONCLUSIONS,0.7255772646536413,"
(28)"
CONCLUSIONS,0.7264653641207816,"If
α ̸= β →S(fθ, DX) ̸= S(hϕ, DX)
(29)"
CONCLUSIONS,0.7273534635879219,"C
Further Experiments on Synthetic Data
720"
CONCLUSIONS,0.7282415630550622,"This experimental section explores the detection of distribution shift on the previous synthetic
721"
CONCLUSIONS,0.7291296625222025,"examples.
722"
CONCLUSIONS,0.7300177619893428,"C.1
Detecting multivariate shift
723"
CONCLUSIONS,0.7309058614564832,"Given two bivariate normal distributions DX = (X1, X2) ∼N

0,

1
0
0
1"
CONCLUSIONS,0.7317939609236235,"
and Dnew
X
=
724"
CONCLUSIONS,0.7326820603907638,"(Xnew
1
, Xnew
2
) ∼N

0,

1
0.2
0.2
1"
CONCLUSIONS,0.7335701598579041,"
, then, for each feature j the underlying distribution is equally
725"
CONCLUSIONS,0.7344582593250444,"distributed between DX and Dnew
X
, ∀j ∈{1, 2} : P(DXj) = P(Dnew
Xj ), and what is different are the
726"
CONCLUSIONS,0.7353463587921847,"interaction terms between them. We now create a synthetic target Y = X1·X2+ϵ with ϵ ∼N(0, 0.1)
727"
CONCLUSIONS,0.7362344582593251,"and fit a gradient boosting decision tree fθ(DX). Then we compute the SHAP explanation values for
728"
CONCLUSIONS,0.7371225577264654,"S(fθ, DX) and S(fθ, Dnew
X
)
729"
CONCLUSIONS,0.7380106571936057,"Table 2: Displayed results are the one-tailed p-values of the Kolmogorov-Smirnov test comparison between
two underlying distributions. Small p-values indicate that compared distributions would be very unlikely to
be equally distributed. SHAP values correctly indicate the interaction changes that individual distribution
comparisons cannot detect"
CONCLUSIONS,0.738898756660746,"Comparison
p-value
Conclusions
P(DX1), P(Dnew
X1 )
0.33
Not Distinct
P(DX2), P(Dnew
X2 )
0.60
Not Distinct
S1(fθ, DX), S1(fθ, Dnew
X
)
3.9e−153
Distinct
S2(fθ, DX), S2(fθ, Dnew
X
)
2.9e−148
Distinct"
CONCLUSIONS,0.7397868561278863,"Having drawn 50, 000 samples from both DX and Dnew
X
, in Table 2, we evaluate whether changes in
730"
CONCLUSIONS,0.7406749555950266,"the input data distribution or on the explanations are able to detect changes in covariate distribution.
731"
CONCLUSIONS,0.7415630550621669,"For this, we compare the one-tailed p-values of the Kolmogorov-Smirnov test between the input data
732"
CONCLUSIONS,0.7424511545293073,"distribution and the explanations distribution. Explanation shift correctly detects the multivariate
733"
CONCLUSIONS,0.7433392539964476,"distribution change that univariate statistical testing can not detect.
734"
CONCLUSIONS,0.7442273534635879,"C.2
Detecting concept shift
735"
CONCLUSIONS,0.7451154529307282,"As mentioned before, concept shift cannot be detected if new data comes without target labels. If new
736"
CONCLUSIONS,0.7460035523978685,"data is labelled, the explanation shift can still be a useful technique for detecting concept shifts.
737"
CONCLUSIONS,0.7468916518650088,"Given a bivariate normal distribution DX = (X1, X2) ∼N(1, I) where I is an identity matrix of
738"
CONCLUSIONS,0.7477797513321492,"order two. We now create two synthetic targets Y = X2
1 · X2 + ϵ and Y new = X1 · X2
2 + ϵ and
739"
CONCLUSIONS,0.7486678507992895,"fit two machine learning models fθ : DX →DY ) and hΥ : DX →Dnew
Y
). Now we compute the
740"
CONCLUSIONS,0.7495559502664298,"SHAP values for S(fθ, DX) and S(hΥ, DX)"
CONCLUSIONS,0.7504440497335702,"Table 3: Distribution comparison for synthetic concept shift. Displayed results are the one-tailed p-values of the
Kolmogorov-Smirnov test comparison between two underlying distributions"
CONCLUSIONS,0.7513321492007105,"Comparison
Conclusions
P(DX), P(Dnew
X
)
Not Distinct
P(DY ), P(Dnew
Y
)
Not Distinct
P(fθ(DX)), P(hΥ(Dnew
X
))
Not Distinct
P(S(fθ, DX)), P(S(hΥ, DX))
Distinct 741"
CONCLUSIONS,0.7522202486678508,"In Table 3, we see how the distribution shifts are not able to capture the change in the model behavior
742"
CONCLUSIONS,0.7531083481349912,"while the SHAP values are different. The “Distinct/Not distinct” conclusion is based on the one-tailed
743"
CONCLUSIONS,0.7539964476021315,"p-value of the Kolmogorov-Smirnov test with a 0.05 threshold drawn out of 50, 000 samples for both
744"
CONCLUSIONS,0.7548845470692718,"distributions. As in the synthetic example, in table 3 SHAP values can detect a relational change
745"
CONCLUSIONS,0.7557726465364121,"between DX and DY , even if both distributions remain equivalent.
746"
CONCLUSIONS,0.7566607460035524,"C.3
Uninformative features on synthetic data
747"
CONCLUSIONS,0.7575488454706927,"To have an applied use case of the synthetic example from the methodology section, we create a
748"
CONCLUSIONS,0.7584369449378331,"three-variate normal distribution DX = (X1, X2, X3) ∼N(0, I3), where I3 is an identity matrix of
749"
CONCLUSIONS,0.7593250444049734,"order three. The target variable is generated Y = X1 · X2 + ϵ being independent of X3. For both,
750"
CONCLUSIONS,0.7602131438721137,"training and test data, 50, 000 samples are drawn. Then out-of-distribution data is created by shifting
751"
CONCLUSIONS,0.761101243339254,"X3, which is independent of the target, on test data Dnew
X3
= Dte
X3 + 1.
752"
CONCLUSIONS,0.7619893428063943,"Table 4: Distribution comparison when modifying a random noise variable on test data. The input data shifts
while explanations and predictions do not."
CONCLUSIONS,0.7628774422735346,"Comparison
Conclusions
P(Dte
X3), P(Dnew
X3 )
Distinct
fθ(Dte
X), fθ(Dnew
X
)
Not Distinct
S(fθ, Dte
X), S(fθ, Dnew
X
)
Not Distinct"
CONCLUSIONS,0.7637655417406749,"In Table 4, we see how an unused feature has changed the input distribution, but the explanation
753"
CONCLUSIONS,0.7646536412078153,"distributions and performance evaluation metrics remain the same. The “Distinct/Not Distinct”
754"
CONCLUSIONS,0.7655417406749556,"conclusion is based on the one-tailed p-value of the Kolmogorov-Smirnov test drawn out of 50, 000
755"
CONCLUSIONS,0.7664298401420959,"samples for both distributions.
756"
CONCLUSIONS,0.7673179396092362,"C.4
Explanation shift that does not affect the prediction
757"
CONCLUSIONS,0.7682060390763765,"In this case we provide a situation when we have changes in the input data distributions that affect the
758"
CONCLUSIONS,0.7690941385435168,"model explanations but do not affect the model predictions due to positive and negative associations
759"
CONCLUSIONS,0.7699822380106572,"between the model predictions and the distributions cancel out producing a vanishing correlation in
760"
CONCLUSIONS,0.7708703374777975,"the mixture of the distribution (Yule’s effect 4.2).
761"
CONCLUSIONS,0.7717584369449378,"We create a train and test data by drawing 50, 000 samples from a bi-uniform distribution X1 ∼
762"
CONCLUSIONS,0.7726465364120781,"U(0, 1),
X2 ∼U(1, 2) the target variable is generated by Y = X1 + X2 where we train our model
763"
CONCLUSIONS,0.7735346358792184,"fθ. Then if out-of-distribution data is sampled from Xnew
1
∼U(1, 2), Xnew
2
∼U(0, 1)
764"
CONCLUSIONS,0.7744227353463587,"In Table 5, we see how an unused feature has changed the input distribution, but the explanation
765"
CONCLUSIONS,0.7753108348134992,"distributions and performance evaluation metrics remain the same. The “Distinct/Not Distinct”
766"
CONCLUSIONS,0.7761989342806395,"conclusion is based on the one-tailed p-value of the Kolmogorov-Smirnov test drawn out of 50, 000
767"
CONCLUSIONS,0.7770870337477798,"samples for both distributions.
768"
CONCLUSIONS,0.7779751332149201,"Table 5: Distribution comparison over how the change on the contributions of each feature can cancel out to
produce an equal prediction (cf. Section 4.2), while explanation shift will detect this behaviour changes on the
predictions will not."
CONCLUSIONS,0.7788632326820604,"Comparison
Conclusions
f(Dte
X), f(Dnew
X
)
Not Distinct
S(fθ, Dte
X2), S(fθ, Dnew
X2 )
Distinct
S(fθ, Dte
X1), S(fθ, Dnew
X1 )
Distinct"
CONCLUSIONS,0.7797513321492007,"D
Further Experiments on Real Data
769"
CONCLUSIONS,0.7806394316163411,"In this section, we extend the prediction task of the main body of the paper. The methodology
770"
CONCLUSIONS,0.7815275310834814,"used follows the same structure. We start by creating a distribution shift by training the model fθ
771"
CONCLUSIONS,0.7824156305506217,"in California in 2014 and evaluating it in the rest of the states in 2018, creating a geopolitical and
772"
CONCLUSIONS,0.783303730017762,"temporal shift. The model gθ is trained each time on each state using only the XNew in the absence
773"
CONCLUSIONS,0.7841918294849023,"of the label, and its performance is evaluated by a 50/50 random train-test split. As models, we
774"
CONCLUSIONS,0.7850799289520426,"use a gradient boosting decision tree[59, 60] as estimator fθ, approximating the Shapley values by
775"
CONCLUSIONS,0.7859680284191829,"TreeExplainer [38], and using logistic regression for the Explanation Shift Detector.
776"
CONCLUSIONS,0.7868561278863233,"D.1
ACS Employment
777"
CONCLUSIONS,0.7877442273534636,"The objective of this task is to determine whether an individual aged between 16 and 90 years is
778"
CONCLUSIONS,0.7886323268206039,"employed or not. The model’s performance was evaluated using the AUC metric in different states,
779"
CONCLUSIONS,0.7895204262877442,"except PR18, where the model showed an explanation shift. The explanation shift was observed to be
780"
CONCLUSIONS,0.7904085257548845,"influenced by features such as Citizenship and Military Service. The performance of the model was
781"
CONCLUSIONS,0.7912966252220248,"found to be consistent across most of the states, with an AUC below 0.60. The impact of features
782"
CONCLUSIONS,0.7921847246891652,"such as difficulties in hearing or seeing was negligible in the distribution shift impact on the model.
783"
CONCLUSIONS,0.7930728241563055,"The left figure in Figure 5 compares the performance of the Explanation Shift Detector in different
784"
CONCLUSIONS,0.7939609236234458,"states for the ACS Employment dataset.
785"
CONCLUSIONS,0.7948490230905861,"0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
AUC 0 50 100 150 200 250"
CONCLUSIONS,0.7957371225577264,AUC Kernel Density Distribution
CONCLUSIONS,0.7966252220248667,AUC performance of the Explanation Shift Detector
CONCLUSIONS,0.7975133214920072,"In-Distribution (CA14)
NY18
TX18
HI18
KS18
MN18
PR18
CA18"
CONCLUSIONS,0.7984014209591475,"PR18
NY18
HI18
MN18
CA18
KS18
TX18"
CONCLUSIONS,0.7992895204262878,Citizenship
CONCLUSIONS,0.8001776198934281,Relationship
CONCLUSIONS,0.8010657193605684,Military Age Race
CONCLUSIONS,0.8019538188277087,MobilityStat
CONCLUSIONS,0.8028419182948491,Education
CONCLUSIONS,0.8037300177619894,Disability
CONCLUSIONS,0.8046181172291297,Ancestry
CONCLUSIONS,0.80550621669627,Employment
CONCLUSIONS,0.8063943161634103,Marital
CONCLUSIONS,0.8072824156305506,EaringDiff
CONCLUSIONS,0.8081705150976909,VisionDiff DREM Sex
CONCLUSIONS,0.8090586145648313,NATIVITY
CONCLUSIONS,0.8099467140319716,"2
0.032
0.041
0.024
0.0093
0.013
0.041
0.23
0.14
0.065
0.043
0.45
0.011
0.28
0.42
0.17
0.11
0.21
0.068
0.11
0.014
0.053
0.19
0.0077
0.25
0.086
0.2
0.064
0.19
0.08
0.24
0.011
0.014
0.068
0.0047
0.42
0.041
0.023
0.058
0.0069
0.022
0.0077
0.067
0.1
0.13
0.024
0.064
0.023
0.018
0.2
0.02
0.012
0.037
0.02
0.06
0.027
0.14
0.023
0.064
0.087
0.0092
0.018
0.012
0.24
0.015
0.016
0.015
0.0098
0.012
0.03
0.21
0.032
0.037
0.0097
0.022
0.0078
0.013
0.054
0.038
0.025
0.03
0.026
0.042
0.02
0.1
0.015
0.022
0.013
0.012
0.007
0.0095
0.02
0.016
0.037
0.013
0.016
0.023
0.037
0.041
0.0094
0.029
0.019
0.012
0.0063
0.011"
CONCLUSIONS,0.8108348134991119,"Feature importance of the Explanation Shift detector (Wasserstein) 10
2 10
1 10
0"
CONCLUSIONS,0.8117229129662522,"Figure 5: The left figure shows a comparison of the performance of the Explanation Shift Detector in different
states for the ACS Employment dataset. The right figure shows the feature importance analysis for the same
dataset."
CONCLUSIONS,0.8126110124333925,"Additionally, the feature importance analysis for the same dataset is presented in the right figure in
786"
CONCLUSIONS,0.8134991119005328,"Figure 5.
787"
CONCLUSIONS,0.8143872113676732,"D.2
ACS Travel Time
788"
CONCLUSIONS,0.8152753108348135,"The goal of this task is to predict whether an individual has a commute to work that is longer than
789"
CONCLUSIONS,0.8161634103019538,"+20 minutes. For this prediction task, the results are different from the previous two cases; the state
790"
CONCLUSIONS,0.8170515097690941,"with the highest OOD score is KS18, with the “Explanation Shift Detector” highlighting features as
791"
CONCLUSIONS,0.8179396092362344,"Place of Birth, Race or Working Hours Per Week. The closest state to ID is CA18, where there is
792"
CONCLUSIONS,0.8188277087033747,"only a temporal shift without any geospatial distribution shift.
793"
CONCLUSIONS,0.8197158081705151,"0.50
0.55
0.60
0.65
0.70
0.75
0.80
AUC 0 25 50 75 100 125 150 175 200"
CONCLUSIONS,0.8206039076376554,AUC Kernel Density Distribution
CONCLUSIONS,0.8214920071047958,AUC performance of the Explanation Shift Detector
CONCLUSIONS,0.822380106571936,"In-Distribution (CA14)
NY18
TX18
HI18
KS18
MN18
PR18
CA18"
CONCLUSIONS,0.8232682060390764,"PR18
HI18
MN18
KS18
NY18
TX18
CA18 Race"
CONCLUSIONS,0.8241563055062167,Citizenship
CONCLUSIONS,0.8250444049733571,PovertyIncome
CONCLUSIONS,0.8259325044404974,WorkTravel Age
CONCLUSIONS,0.8268206039076377,Occupation
CONCLUSIONS,0.827708703374778,Relationship
CONCLUSIONS,0.8285968028419183,Employment
CONCLUSIONS,0.8294849023090586,MobilityStat Sex
CONCLUSIONS,0.8303730017761989,Education
CONCLUSIONS,0.8312611012433393,Disability
CONCLUSIONS,0.8321492007104796,Marital
CONCLUSIONS,0.8330373001776199,"0.14
0.48
0.45
0.38
0.11
0.15
0.031"
CONCLUSIONS,0.8339253996447602,"0.89
0.15
0.26
0.23
0.039
0.13
0.0086"
CONCLUSIONS,0.8348134991119005,"0.42
0.13
0.069
0.023
0.074
0.027
0.04"
CONCLUSIONS,0.8357015985790408,"0.11
0.011
0.039
0.016
0.28
0.038
0.034"
CONCLUSIONS,0.8365896980461812,"0.098
0.05
0.027
0.073
0.035
0.033
0.045"
CONCLUSIONS,0.8374777975133215,"0.067
0.11
0.0039
0.02
0.054
0.034
0.0097"
CONCLUSIONS,0.8383658969804618,"0.067
0.084
0.052
0.025
0.023
0.012
0.011"
CONCLUSIONS,0.8392539964476021,"0.031
0.016
0.094
0.044
0.031
0.037
0.015"
CONCLUSIONS,0.8401420959147424,"0.072
0.024
0.031
0.012
0.021
0.0093
0.011"
CONCLUSIONS,0.8410301953818827,"0.022
0.023
0.036
0.033
0.02
0.006
0.0061"
CONCLUSIONS,0.8419182948490231,"0.033
0.056
0.017
0.0033
0.0089
0.0068
0.016"
CONCLUSIONS,0.8428063943161634,"0.04
0.0092
0.015
0.032
0.018
0.014
0.0086"
CONCLUSIONS,0.8436944937833037,"0.033
0.0041
0.026
0.0061
0.012
0.011
0.0098"
CONCLUSIONS,0.844582593250444,"Feature importance of the Explanation Shift detector (Wasserstein) 10
2 10
1"
CONCLUSIONS,0.8454706927175843,"Figure 6: In the left figure, comparison of the performance of Explanation Shift Detector, in different states for
the ACS TravelTime prediction task. In the left figure, we can see how the state with the highest OOD AUC
detection is KS18 and not PR18 as in other prediction tasks; this difference with respect to the other prediction
task can be attributed to “Place of Birth”, whose feature attributions the model finds to be more different than in
CA14."
CONCLUSIONS,0.8463587921847247,"D.3
ACS Mobility
794"
CONCLUSIONS,0.8472468916518651,"The objective of this task is to predict whether an individual between the ages of 18 and 35 had the
795"
CONCLUSIONS,0.8481349911190054,"same residential address as a year ago. This filtering is intended to increase the difficulty of the
796"
CONCLUSIONS,0.8490230905861457,"prediction task, as the base rate for staying at the same address is above 90% for the population [54].
797"
CONCLUSIONS,0.849911190053286,"The experiment shows a similar pattern to the ACS Income prediction task (cf. Section 4), where the
798"
CONCLUSIONS,0.8507992895204263,"inland US states have an AUC range of 0.55 −0.70, while the state of PR18 achieves a higher AUC.
799"
CONCLUSIONS,0.8516873889875666,"For PR18, the model has shifted due to features such as Citizenship, while for the other states, it is
800"
CONCLUSIONS,0.8525754884547069,"Ancestry (Census record of your ancestors’ lives with details like where they lived, who they lived
801"
CONCLUSIONS,0.8534635879218473,"with, and what they did for a living) that drives the change in the model.
802"
CONCLUSIONS,0.8543516873889876,"As depicted in Figure 7, all states, except for PR18, fall below an AUC of explanation shift detection
803"
CONCLUSIONS,0.8552397868561279,"of 0.70. Protected social attributes, such as Race or Marital status, play an essential role for these
804"
CONCLUSIONS,0.8561278863232682,"states, whereas for PR18, Citizenship is a key feature driving the impact of distribution shift in model.
805"
CONCLUSIONS,0.8570159857904085,"0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
AUC 0 20 40 60 80 100 120 140 160"
CONCLUSIONS,0.8579040852575488,AUC Kernel Density Distribution
CONCLUSIONS,0.8587921847246892,AUC performance of the Explanation Shift Detector
CONCLUSIONS,0.8596802841918295,"In-Distribution (CA14)
NY18
TX18
HI18
KS18
MN18
PR18
CA18"
CONCLUSIONS,0.8605683836589698,"PR18
MN18
HI18
KS18
NY18
TX18
CA18"
CONCLUSIONS,0.8614564831261101,Citizenship
CONCLUSIONS,0.8623445825932504,Ancestry
CONCLUSIONS,0.8632326820603907,"Race
Relationship
EmploymentStatus"
CONCLUSIONS,0.8641207815275311,Education
CONCLUSIONS,0.8650088809946714,"WKHP
JWMNP"
CONCLUSIONS,0.8658969804618117,"COW
PINCP
Marital
DREM"
CONCLUSIONS,0.866785079928952,"GCL
VisionDiff"
CONCLUSIONS,0.8676731793960923,Disability
CONCLUSIONS,0.8685612788632326,"Age
Military
EaringDiff"
CONCLUSIONS,0.8694493783303731,"Sex
NATIVITY
Employment"
CONCLUSIONS,0.8703374777975134,"1.2
0.15
0.11
0.12
0.034
0.0072
0.019
0.5
0.35
0.21
0.2
0.17
0.033
0.069
0.18
0.24
0.39
0.24
0.17
0.19
0.011
0.32
0.28
0.083
0.31
0.046
0.11
0.02
0.067
0.26
0.079
0.13
0.059
0.046
0.062
0.16
0.029
0.083
0.048
0.17
0.035
0.099
0.36
0.053
0.02
0.037
0.031
0.023
0.018
0.14
0.054
0.062
0.039
0.11
0.02
0.031
0.12
0.048
0.03
0.079
0.039
0.044
0.015
0.008
0.044
0.17
0.0066
0.043
0.053
0.053
0.059
0.09
0.067
0.0072
0.035
0.021
0.029
0.082
0.044
0.024
0.044
0.036
0.039
0.031
0.038
0.011
0.1
0.054
0.01
0.044
0.013
0.083
0.016
0.05
0.047
0.011
0.041
0.0085
0.028
0.068
0.024
0.0071
0.057
0.043
0.025
0.098
0.0076
0.077
0.027
0.018
0.011
0.0055
0.0057
0.046
0.042
0.023
0.01
0.069
0.01
0.02
0.031
0.021
0.025
0.01
0.015
0.0082
0.0095
0.023
0.032
0.0045
0.036
0.011
0.0098"
CONCLUSIONS,0.8712255772646537,"Feature importance of the Explanation Shift detector (Wasserstein) 10
2 10
1 10
0"
CONCLUSIONS,0.872113676731794,"Figure 7: Left figure shows a comparison of the Explanation Shift Detector’s performance in different states for
the ACS Mobility dataset. Except for PR18, all other states fall below an AUC of explanation shift detection
of 0.70. The features driving this difference are Citizenship and Ancestry relationships. For the other states,
protected social attributes, such as Race or Marital status, play an important role."
CONCLUSIONS,0.8730017761989343,"D.4
StackOverflow Survey Data: Novel Covariate Group
806"
CONCLUSIONS,0.8738898756660746,"This experimental section evaluates the proposed Explanation Shift Detector approach on real-world
807"
CONCLUSIONS,0.8747779751332149,"data under novel group distribution shifts. In this scenario, a new unseen group appears at the
808"
CONCLUSIONS,0.8756660746003553,"prediction stage, and the ratio of the presence of this unseen group in the new data is varied. The
809"
CONCLUSIONS,0.8765541740674956,"estimator used is a gradient-boosting decision tree or logistic regression, and a logistic regression
810"
CONCLUSIONS,0.8774422735346359,"is used for the detector. The results show that the AUC of the Explanation Shift Detector varies
811"
CONCLUSIONS,0.8783303730017762,"depending on the quantification of OOD explanations, and it show more sensitivity w.r.t. to model
812"
CONCLUSIONS,0.8792184724689165,"variations than other state-of-the-art techniques.
813"
CONCLUSIONS,0.8801065719360568,"The dataset used is the StackOverflow annual developer survey has over 70,000 responses from over
814"
CONCLUSIONS,0.8809946714031972,"180 countries examining aspects of the developer experience [55]. The data has high dimensionality,
815"
CONCLUSIONS,0.8818827708703375,"leaving it with +100 features after data cleansing and feature engineering. The goal of this task is to
816"
CONCLUSIONS,0.8827708703374778,"predict the total annual compensation.
817"
CONCLUSIONS,0.8836589698046181,"2
4
6
8
10
Max Depth/Hyperparameter 0.475 0.500 0.525 0.550 0.575 0.600 0.625"
CONCLUSIONS,0.8845470692717584,Explanation Shift AUC
CONCLUSIONS,0.8854351687388987,Log. Reg. as Explanation Shift Detector
CONCLUSIONS,0.8863232682060391,"XGB
Decision Tree
Random Forest"
CONCLUSIONS,0.8872113676731794,"2
4
6
8
10
Max Depth/Hyperparameter 0.45 0.50 0.55 0.60 0.65 0.70 0.75"
CONCLUSIONS,0.8880994671403197,Explanation Shift AUC
CONCLUSIONS,0.88898756660746,XGB as Explanation Shift Detector
CONCLUSIONS,0.8898756660746003,"XGB
Decision Tree
Random Forest"
CONCLUSIONS,0.8907637655417406,"Figure 8: Both images represent the AUC of the Explanation Shift Detector for different countries on the
StackOverflow survey dataset under novel group shift. In the left image, the detector is a logistic regression,
and in the right image, a gradient-boosting decision tree classifier. By changing the model, we can see that
low-complexity models are unaffected by the distribution shift, while when increasing the model complexity, the
out-of-distribution model behaviour starts to be tangible"
CONCLUSIONS,0.8916518650088809,"E
Experiments with Modeling Methods and Hyperparameters
818"
CONCLUSIONS,0.8925399644760214,"In the next sections, we are going to show the sensitivity or our method to variations of the estimator
819"
CONCLUSIONS,0.8934280639431617,"f, the detector g, and the parameters of the estimator f θ.
820"
CONCLUSIONS,0.894316163410302,"As an experimental setup, In the main body of the paper, we have focused on the UCI Adult Income
821"
CONCLUSIONS,0.8952042628774423,"dataset. The experimental setup has been using Gradient Boosting Decision Tree as the original
822"
CONCLUSIONS,0.8960923623445826,"estimator fθ and then as “Explanation Shift Detector” gψ a logistic regression. In this section, we
823"
CONCLUSIONS,0.8969804618117229,"extend the experimental setup by providing experiments by varying the types of algorithms for a
824"
CONCLUSIONS,0.8978685612788633,"given experimental set-up: the UCI Adult Income dataset using the Novel Covariate Group Shift for
825"
CONCLUSIONS,0.8987566607460036,"the “Asian” group with a fraction ratio of 0.5 (cf. Section 5).
826"
CONCLUSIONS,0.8996447602131439,"E.1
Varying Estimator and Explanation Shift Detector
827"
CONCLUSIONS,0.9005328596802842,"OOD data detection methods based on input data distributions only depend on the type of detector
828"
CONCLUSIONS,0.9014209591474245,"used, being independent of the estimator. OOD Explanation methods rely on both the model and the
829"
CONCLUSIONS,0.9023090586145648,"data. Using explanations shifts as indicators for measuring distribution shifts impact on the model
830"
CONCLUSIONS,0.9031971580817052,"enables us to account for the influencing factors of the explanation shift. Therefore, in this section,
831"
CONCLUSIONS,0.9040852575488455,"we compare the performance of different types of algorithms for explanation shift detection using the
832"
CONCLUSIONS,0.9049733570159858,"same experimental setup. The results of our experiments show that using Explanation Shift enables
833"
CONCLUSIONS,0.9058614564831261,"us to see differences in the choice of the original estimator fθ and the Explanation Shift Detector gϕ
834"
CONCLUSIONS,0.9067495559502664,"E.2
Hyperparameters Sensitivity Evaluation
835"
CONCLUSIONS,0.9076376554174067,"This section presents an extension to our experimental setup where we vary the model complexity by
836"
CONCLUSIONS,0.9085257548845471,"varying the model hyperparameters S(f θ, X). Specifically, we use the UCI Adult Income dataset
837"
CONCLUSIONS,0.9094138543516874,"with the Novel Covariate Group Shift for the “Asian” group with a fraction ratio of 0.5 as described
838"
CONCLUSIONS,0.9103019538188277,"in Section 5.
839"
CONCLUSIONS,0.911190053285968,"In this experiment, we changed the hyperparameters of the original model: for the decision tree, we
840"
CONCLUSIONS,0.9120781527531083,"varied the depth of the tree, while for the gradient-boosting decision, we changed the number of
841"
CONCLUSIONS,0.9129662522202486,"estimators, and for the random forest, both hyperparameters. We calculated the Shapley values using
842"
CONCLUSIONS,0.9138543516873889,"Estimator fθ
Detector gϕ
XGB
Log.Reg
Lasso
Ridge
Rand.Forest
Dec.Tree
MLP
XGB
0.583
0.619
0.596
0.586
0.558
0.522
0.597
LogisticReg.
0.605
0.609
0.583
0.625
0.578
0.551
0.605
Lasso
0.599
0.572
0.551
0.595
0.557
0.541
0.596
Ridge
0.606
0.61
0.588
0.624
0.564
0.549
0.616
RandomForest
0.586
0.607
0.574
0.612
0.566
0.537
0.611
DecisionTree
0.546
0.56
0.559
0.569
0.543
0.52
0.569"
CONCLUSIONS,0.9147424511545293,"Table 6: Comparison of explanation shift detection performance, measured by AUC, for different combinations
of explanation shift detectors and estimators on the UCI Adult Income dataset using the Novel Covariate Group
Shift for the “Asian” group with a fraction ratio of 0.5 (cf. Section 5). The table shows that the choice of detector
and estimator can impact the OOD explanation performance. We can see how, for the same detector, different
estimators flag different OOD explanations performance. On the other side, for the same estimators, different
detectors achieve different results."
CONCLUSIONS,0.9156305506216696,"TreeExplainer [38]. For the Detector choice of model, we compare Logistic Regression and XGBoost
843"
CONCLUSIONS,0.91651865008881,"models.
844"
CONCLUSIONS,0.9174067495559503,"2
4
6
8
10
Max Depth/Hyperparameter 0.45 0.50 0.55 0.60 0.65 0.70"
CONCLUSIONS,0.9182948490230906,Explanation Shift AUC
CONCLUSIONS,0.9191829484902309,Log. Reg. as Explanation Shift Detector
CONCLUSIONS,0.9200710479573713,"XGB
Decision Tree
Random Forest"
CONCLUSIONS,0.9209591474245116,"2
4
6
8
10
Max Depth/Hyperparameter 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60"
CONCLUSIONS,0.9218472468916519,Explanation Shift AUC
CONCLUSIONS,0.9227353463587922,XGB as Explanation Shift Detector
CONCLUSIONS,0.9236234458259325,"XGB
Decision Tree
Random Forest"
CONCLUSIONS,0.9245115452930728,"Figure 9: Both images represent the AUC of the Explanation Shift Detector, in different states for the ACS
Income dataset under novel group shift. In the left image, the detector is a logistic regression, and in the right
image, a gradient-boosting decision tree classifier. By changing the model, we can see that vanilla models
(decision tree with depth 1 or 2) are unaffected by the distribution shift, while when increasing the model
complexity, the out-of-distribution impact of the data in the model starts to be tangible"
CONCLUSIONS,0.9253996447602132,"The results presented in Figure 9 show the AUC of the Explanation Shift Detector for the ACS Income
845"
CONCLUSIONS,0.9262877442273535,"dataset under novel group shift. We observe that the distribution shift does not affect very simplistic
846"
CONCLUSIONS,0.9271758436944938,"models, such as decision trees with depths 1 or 2. However, as we increase the model complexity,
847"
CONCLUSIONS,0.9280639431616341,"the out-of-distribution data impact on the model becomes more pronounced. Furthermore, when we
848"
CONCLUSIONS,0.9289520426287744,"compare the performance of the Explanation Shift Detector across different models, such as Logistic
849"
CONCLUSIONS,0.9298401420959147,"Regression and Gradient Boosting Decision Tree, we observe distinct differences(note that the y-axis
850"
CONCLUSIONS,0.9307282415630551,"takes different values).
851"
CONCLUSIONS,0.9316163410301954,"In conclusion, the explanation distributions serve as a projection of the data and model sensitive to
852"
CONCLUSIONS,0.9325044404973357,"what the model has learned. The results demonstrate the importance of considering model complexity
853"
CONCLUSIONS,0.933392539964476,"under distribution shifts.
854"
CONCLUSIONS,0.9342806394316163,"F
LIME as an Alternative Explanation Method
855"
CONCLUSIONS,0.9351687388987566,"Another feature attribution technique that satisfies the aforementioned properties (efficiency and
856"
CONCLUSIONS,0.9360568383658969,"uninformative features Section 2) and can be used to create the explanation distributions is LIME
857"
CONCLUSIONS,0.9369449378330373,"(Local Interpretable Model-Agnostic Explanations). The intuition behind LIME is to create a local
858"
CONCLUSIONS,0.9378330373001776,"interpretable model that approximates the behavior of the original model in a small neighbourhood of
859"
CONCLUSIONS,0.9387211367673179,"the desired data to explain [48, 49] whose mathematical intuition is very similar to the Taylor series.
860"
CONCLUSIONS,0.9396092362344582,"In this work, we have proposed explanation shifts as a key indicator for investigating the impact of
861"
CONCLUSIONS,0.9404973357015985,"distribution shifts on ML models. In this section, we compare the explanation distributions composed
862"
CONCLUSIONS,0.9413854351687388,"by SHAP and LIME methods. LIME can potentially suffers several drawbacks:
863"
CONCLUSIONS,0.9422735346358793,"• Computationally Expensive: Its currently implementation is more computationally expen-
864"
CONCLUSIONS,0.9431616341030196,"sive than current SHAP implementations such as TreeSHAP [38], Data SHAP [72, 73] or
865"
CONCLUSIONS,0.9440497335701599,"Local and Connected SHAP [74], the problem increases when we produce explanations of
866"
CONCLUSIONS,0.9449378330373002,"distributions. Even though implementations might be improved, LIME requires sampling
867"
CONCLUSIONS,0.9458259325044405,"data and fitting a linear model which is a computationally more expensive approach than the
868"
CONCLUSIONS,0.9467140319715808,"aforementioned model-specific approaches to SHAP.
869"
CONCLUSIONS,0.9476021314387212,"• Local Neighborhood: The definition of a local “neighborhood”, which can lead to instability
870"
CONCLUSIONS,0.9484902309058615,"of the explanations. Slight variations of this explanation hyperparameter lead to different
871"
CONCLUSIONS,0.9493783303730018,"local explanations. In [75] the authors showed that the explanations of two very close points
872"
CONCLUSIONS,0.9502664298401421,"can vary greatly.
873"
CONCLUSIONS,0.9511545293072824,"• Dimensionality: LIME requires as a hyperparameter the number of features to use for the
874"
CONCLUSIONS,0.9520426287744227,"local linear approximation. This creates a dimensionality problem as for our method to
875"
CONCLUSIONS,0.9529307282415631,"work, the explanation distributions have to be from the exact same dimensions as the input
876"
CONCLUSIONS,0.9538188277087034,"data. Reducing the number of features to be explained might improve the computational
877"
CONCLUSIONS,0.9547069271758437,"burden.
878"
CONCLUSIONS,0.955595026642984,"0.0
0.2
0.4
0.6
0.8
1.0
Correlation coefficient 0.5 0.6 0.7 0.8"
CONCLUSIONS,0.9564831261101243,AUC Explanation Shift Detector
CONCLUSIONS,0.9573712255772646,Sensitivity to Multicovariate Shift
CONCLUSIONS,0.9582593250444049,"Explanation Shift - SHAP
Explanation Shift - Lime"
CONCLUSIONS,0.9591474245115453,"0.2
0.4
0.6
0.8
1.0
Fraction of OOD data 0.475 0.500 0.525 0.550 0.575 0.600 0.625 0.650"
CONCLUSIONS,0.9600355239786856,AUC of Explanation Shift Detector
CONCLUSIONS,0.9609236234458259,"ShapAsian
LimeAsian"
CONCLUSIONS,0.9618117229129662,"Figure 10: Comparison of the explanation distribution generated by LIME and SHAP. The left plot shows the
sensitivity of the predicted probabilities to multicovariate changes using the synthetic data experimental setup of
2 on the main body of the paper. The right plot shows the distribution of explanation shifts for a New Covariate
Category shift (Asian) in the ASC Income dataset."
CONCLUSIONS,0.9626998223801065,"Figure 10 compares the explanation distributions generated by LIME and SHAP. The left plot
879"
CONCLUSIONS,0.9635879218472468,"shows the sensitivity of the predicted probabilities to multicovariate changes using the synthetic data
880"
CONCLUSIONS,0.9644760213143873,"experimental setup from Figure 2 in the main body of the paper. The right plot shows the distribution
881"
CONCLUSIONS,0.9653641207815276,"of explanation shifts for a New Covariate Category shift (Asian) in the ASC Income dataset. The
882"
CONCLUSIONS,0.9662522202486679,"performance of OOD explanations detection is similar between the two methods, but LIME suffers
883"
CONCLUSIONS,0.9671403197158082,"from two drawbacks: its theoretical properties rely on the definition of a local neighborhood, which
884"
CONCLUSIONS,0.9680284191829485,"can lead to unstable explanations (false positives or false negatives on explanation shift detection),
885"
CONCLUSIONS,0.9689165186500888,"and its computational runtime required is much higher than that of SHAP (see experiments below).
886"
CONCLUSIONS,0.9698046181172292,"F.1
Runtime
887"
CONCLUSIONS,0.9706927175843695,"We conducted an analysis of the runtimes of generating the explanation distributions using the two
888"
CONCLUSIONS,0.9715808170515098,"proposed methods. The experiments were run on a server with 4 vCPUs and 32 GB of RAM. We
889"
CONCLUSIONS,0.9724689165186501,"used shap version 0.41.0 and lime version 0.2.0.1 as software packages. In order to define the local
890"
CONCLUSIONS,0.9733570159857904,"neighborhood for both methods in this example we use all the data provided as background data. As
891"
CONCLUSIONS,0.9742451154529307,"an estimator, we use an xgboost and compare the results of TreeShap against LIME. When varying
892"
CONCLUSIONS,0.9751332149200711,"the number of samples we use 5 features and while varying the number of features we use 1000
893"
CONCLUSIONS,0.9760213143872114,"samples.
894"
CONCLUSIONS,0.9769094138543517,"Figure 11, shows the wall time required for generating explanation distributions using SHAP and
895"
CONCLUSIONS,0.977797513321492,"LIME with varying numbers of samples and columns. The runtime required of generating an
896"
CONCLUSIONS,0.9786856127886323,"explanation distributions using LIME is much higher than using SHAP, especially when producing
897"
CONCLUSIONS,0.9795737122557726,"100
200
500
1000
5000
Number of samples 10
1 10
0 10
1"
CONCLUSIONS,0.9804618117229129,Time (s)
CONCLUSIONS,0.9813499111900533,method
CONCLUSIONS,0.9822380106571936,"Tree Shap
Lime"
CONCLUSIONS,0.9831261101243339,"5
10
15
20
25
Number of features 10
1 10
0 10
1"
CONCLUSIONS,0.9840142095914742,Time (s)
CONCLUSIONS,0.9849023090586145,method
CONCLUSIONS,0.9857904085257548,"Tree Shap
Lime"
CONCLUSIONS,0.9866785079928952,"Figure 11: Wall time for generating explanation distributions using SHAP and LIME with different numbers of
samples (left) and different numbers of columns (right). Note that the y-scale is logarithmic. The experiments
were run on a server with 4 vCPUs and 32 GB of RAM. The runtime required to create an explanation
distributions with LIME is far greater than SHAP for a gradient-boosting decision tree"
CONCLUSIONS,0.9875666074600356,"explanations for distributions. This is due to the fact that LIME requires training a local model for
898"
CONCLUSIONS,0.9884547069271759,"each instance of the input data to be explained, which can be computationally expensive. In contrast,
899"
CONCLUSIONS,0.9893428063943162,"SHAP relies on heuristic approximations to estimate the feature attribution with no need to train a
900"
CONCLUSIONS,0.9902309058614565,"model for each instance. The results illustrate that this difference in computational runtime becomes
901"
CONCLUSIONS,0.9911190053285968,"more pronounced as the number of samples and columns increases.
902"
CONCLUSIONS,0.9920071047957372,"We note that the computational burden of generating the explanation distributions can be further
903"
CONCLUSIONS,0.9928952042628775,"reduced by limiting the number of features to be explained, as this reduces the dimensionality of the
904"
CONCLUSIONS,0.9937833037300178,"explanation distributions, but this will inhibit the quality of the explanation shift detection as it won’t
905"
CONCLUSIONS,0.9946714031971581,"be able to detect changes on the distribution shift that impact model on those features.
906"
CONCLUSIONS,0.9955595026642984,"Given the current state-of-the-art of software packages we have used SHAP values due to lower
907"
CONCLUSIONS,0.9964476021314387,"runtime required and that theoretical guarantees hold with the implementations. In the experiments
908"
CONCLUSIONS,0.9973357015985791,"performed in this paper, we are dealing with a medium-scaled dataset with around ∼1, 000, 000
909"
CONCLUSIONS,0.9982238010657194,"samples and 20 −25 features. Further work can be envisioned on developing novel mathematical
910"
CONCLUSIONS,0.9991119005328597,"analysis and software that study under which conditions which method is more suitable.
911"
