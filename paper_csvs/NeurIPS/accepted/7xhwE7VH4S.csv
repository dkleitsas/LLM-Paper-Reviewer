Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0021321961620469083,"Online free-viewpoint video (FVV) streaming is a challenging problem, which is
relatively under-explored. It requires incremental on-the-fly updates to a volumetric
representation, fast training and rendering to satisfy real-time constraints and a
small memory footprint for efficient transmission. If achieved, it can enhance user
experience by enabling novel applications, e.g., 3D video conferencing and live
volumetric video broadcast, among others. In this work, we propose a novel frame-
work for QUantized and Efficient ENcoding (QUEEN) for streaming FVV using
3D Gaussian Splatting (3D-GS). QUEEN directly learns Gaussian attribute residu-
als between consecutive frames at each time-step without imposing any structural
constraints on them, allowing for high quality reconstruction and generalizability.
To efficiently store the residuals, we further propose a quantization-sparsity frame-
work, which contains a learned latent-decoder for effectively quantizing attribute
residuals other than Gaussian positions and a learned gating module to sparsify
position residuals. We propose to use the Gaussian viewspace gradient difference
vector as a signal to separate the static and dynamic content of the scene. It acts
as a guide for effective sparsity learning and speeds up training. On diverse FVV
benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all
metrics. Notably, for several highly dynamic scenes, it reduces the model size to
just 0.7 MB per frame while training in under 5 sec and rendering at ∼350 FPS."
INTRODUCTION,0.0042643923240938165,"1
Introduction"
INTRODUCTION,0.006396588486140725,"The dynamic world that we perceive around us is not 2D, but rather 3D. Unlike 2D videos, which
are ubiquitous, the question of how to effectively capture, encode and disseminate free-viewpoint
videos (FVV) of dynamic 3D scenes, which can be viewed at any instance of time and from any
viewpoint, has intrigued computer vision and graphics researchers for much time. Free-viewpoint
video transmission, if achieved, has the potential to transform and enrich user experience in profound
ways by offering novel immersive experiences, e.g., FVV video playback and live streaming, 3D
video conferencing and telepresence, gaming, virtual spatial tutoring and teleoperation, among others."
INTRODUCTION,0.008528784648187633,"The underlying problem of reconstructing FVV involves learning a 6D plenoptic function of a dynamic
scene P(x, d, t) from sparse multiple views acquired over a window of time, with x ∈R3 being a
position in 3D space, d = (θ, ϕ) a viewing direction and t an instance of time. Neural volumetric
representations, which learn a 5D plenoptic function of a scene P(x, d) at a fixed time instance, e.g.,"
INTRODUCTION,0.010660980810234541,"∗SG’s work was done during an internship at NVIDIA. †TL and AM contributed equally to this project.
Project website: https://research.nvidia.com/labs/amri/projects/queen."
INTRODUCTION,0.01279317697228145,38th Conference on Neural Information Processing Systems (NeurIPS 2024).
INTRODUCTION,0.014925373134328358,"neural radiance fields (NeRFs) [54] and its variants [2, 7, 18, 55] present a compact and high-fidelity
representation for 3D scenes. NeRFs have also been extended to dynamic 4D scenes [1, 5, 19]
providing a powerful tool for reconstructing FVV. However, NeRFs require compositing dense
information across a 3D volume and hence are slow to train and render. Recently, 3D Gaussian
Splatting (3D-GS) [29] has emerged as a promising technique with significantly faster training and
rendering speeds in comparison with NeRFs, and they have also been extended to dynamic 4D
scenes [42, 83, 88]. While these representations accurately model 4D scenes, they are trained in an
offline fashion requiring full multi-view video sequences to learn temporal relationships between
frames. They also require long training times to achieve high reconstruction quality and are mostly
not streamable."
INTRODUCTION,0.017057569296375266,"Online FVV, e.g., for broadcast and teleconferencing applications, presents additional challenges
versus offline. It requires incremental on-the-fly updates to volumetric representation at each time-step
of the dynamic scene, fast training and rendering times to maintain real-time operation, and small
packet sizes per frame to enable effective transmission on bandwidth-limited channels. Consequently,
the more challenging problem of online FVV reconstruction remains relatively under-explored.
Notable prior solutions are those based on NeRFs using voxel grids [37, 78] or triplanes [84] to learn
3D representations that are updated on-the-fly. Unsurprisingly, they suffer from slow rendering speeds.
Recently, Sun et al. proposed 3DGStream [69], which uses 3D-GS to model a 3D scene along with
InstantNGP [55] to model its geometric transformation over time. It achieves high rendering speeds
but imposes heuristic structural constraints on the volumetric representation to achieve efficiency,
which compromises model expressiveness and quality."
INTRODUCTION,0.019189765458422176,"In this work, we propose a novel QUantized and Efficient ENcoding (QUEEN) framework, which
uses 3D-GS for online FVV. Similarly to prior approaches [69], we also learn Gaussian attribute
residuals between consecutive time-steps. To reduce memory requirements, however, [69] learns
only a subset of the Gaussian attributes at each time-step, limiting model expressiveness. Our first
insight, therefore, is to model residuals for all attributes instead, which does not compromise quality.
However, encoding all Guassian attributes increases the per frame memory requirement and hence
necessitates a means to compress them more effectively. Our second insight, then, is to learn to
directly compress the Gaussian residuals in proportion to the real-time scene dynamics, e.g., motion
and illumination changes. This contrasts with existing methods [37, 69, 78, 84] that employ a single
fixed-sized structure, e.g., a voxel-grid, a triplane, or hash encoding at all time-steps, and the result
is higher efficiency in terms of model size, training speeds, and rendering speeds. Lastly, we also
exploit temporal redundancies across time-steps to limit computations to the highly dynamic parts of
the scene only and achieve further efficiencies."
INTRODUCTION,0.021321961620469083,"Specifically, to achieve this, we propose a learned quantization-sparsity framework to simultaneously
learn and compress Gaussian attribute residuals for each time-step. We quantize all attribute residuals,
except Gaussian positions, via an end-to-end trainable integer-based latent-decoder. Once learned,
we efficiently encode the integer latents via entropy coding to achieve high compression factors.
For position residuals that exhibit greater sensitivity to quantization, we propose a learned gating
mechanism to sparsify them, which identifies the static (corresponding to 0 value) and dynamic
Gaussians and retains the sparse dynamic ones only at full precision. Finally, to achieve further
efficiencies in terms of training time and storage, we utilize the differences between the 2D viewspace
Gaussian gradients of consecutive frames to initialize our learnable gates, and to selectively render
local image regions corresponding to highly dynamic scene content."
INTRODUCTION,0.023454157782515993,"We evaluate our approach, QUEEN, on two benchmark datasets, containing diverse scenes with
large geometric motion and illumination changes. QUEEN outperforms all prior state-of-the-art
approaches for online FVV and significantly reduces the per-frame memory cost (∼10×), all while
achieving higher reconstruction quality, as well as faster training and rendering speeds. Extensive
ablations show the efficacy of the various components of our approach."
INTRODUCTION,0.0255863539445629,"To summarize, our key contributions are:"
INTRODUCTION,0.02771855010660981,"• We propose a Gaussian residual-based framework to model 3D dynamic scenes for online FVV
without any structural constraints, which allows free learning of all 3D-GS attribute residuals,
resulting in higher model expressiveness."
INTRODUCTION,0.029850746268656716,"• We introduce a learned quantization-sparsity framework for compressing per-frame residuals, and
we initialize and train it efficiently using viewspace gradient differences that separate the dynamic
and static scene content."
INTRODUCTION,0.031982942430703626,"• On various challenging real-world dynamic scenes, we surpass existing state-of-the-art approaches
on all metrics: reconstruction quality, memory utilization, as well as training and rendering speed."
RELATED WORK,0.03411513859275053,"2
Related Work"
TRADITIONAL FREE-VIEWPOINT VIDEO,0.03624733475479744,"2.1
Traditional Free-viewpoint Video"
TRADITIONAL FREE-VIEWPOINT VIDEO,0.03837953091684435,"Ever since early FVV work such as [27], a series of geometry-based FVV methods [12, 57] has been
pushing for high reconstruction quality and streamable performance. However, their rendering and
compression quality rely on the accuracy of a sophisticated pipeline of geometry reconstruction [20,
28], tracking [36], and texturing [61]. They also require high-end hardware for capturing complex
and dynamic appearance [6, 14, 26]. Purely image-based rendering [10, 13, 35, 53] relaxes the
requirement for geometric accuracy. Although methods such as [4, 95] support view interpolation
with layered representations in the dynamic setting, they require a high count of views as input to
ensure interpolation quality."
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.04051172707889126,"2.2
Neural and Gaussian-based Free-viewpoint Video"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.042643923240938165,"Offline Methods.
Compared to the traditional representations, the emergence of neural rep-
resentations [47, 54, 86, 67, 72] opened a new door for capturing FVV for dynamic hu-
mans [21, 31, 32, 39, 63, 82, 94] and monocular videos [22, 43, 44, 73, 75, 85]. In this work,
we focus on general dynamic scenes [92] from multiple views to push the quality of streamable FVV
without requiring a strong human prior [40, 48] or a very constrained input. [16, 62, 74, 80] model the
scene dynamics via explicit deformation. Although suitable for motion analysis, they inevitably face
a trade-off between motion accuracy and visual quality [74]. To tackle this, [41, 52, 59] use a spatial-
temporal formulation via time-conditioned latent codes to implicitly encode the 4D scene, enabling
reconstruction of topological changes and volumetric effects. [5, 19, 66] factorize the 4D scene into
multiple space-time feature planes and achieves higher model compactness and training efficiency.
[68, 76] decompose the 4D scene into static and dynamic volumes. [1, 46, 77, 87, 93] incorporate
efficient NeRF representations [9, 19, 91] for higher fidelity. Although, these NeRF-based method
achieve high compactness, they suffer from low rendering efficiency, even when converted [68]
to a more efficient NeRF formulation [9, 55]. Seeing their great potential for efficiency, recent
works extend 3D Gaussian representations [29] to dynamic scenes, with temporal attributes [42, 89],
generalized 4D Gaussians [88] and a hybrid representation [83]. While these methods achieve high
quality in modeling 3D dynamic scenes, they, together with the aforementioned NeRF-based methods,
are mostly offline, i.e., they require all the input video and a long time for training, which is inherently
difficult for streaming applications."
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.04477611940298507,"Online Methods. Online reconstruction for FVVs is relatively under-explored, as it imposes addi-
tional challenges of on-the-fly reconstruction using only local temporal information instead of the
full recordings. Furthermore, toward the goal of streamable FVVs, the encoding system is evaluated
by multiple metrics including compression rate, encoding and rendering speed and visual quality.
[50] tracks dense 3D Gaussians by solving their motion over time. Visual quality and dynamic
appearance is not their focus. [23] models motion by rendering scene dynamics, however their
method is not optimized for efficiency. [45] focuses on generalizable NeRF reconstruction and shows
good promise to adapt to a new frame but has a high memory footprint due to an MVSNet-style
neural network [8, 90]. [37] accelerates training and rendering speed with a special tuning strategy
and sparse voxels, however, their representation still has high temporal redundancy. [81] proposes an
incremental training scheme with natural information partitioning and achieves high compression,
but its encoding is slow. Several works [78, 79, 84] use video codec-inspired encoding paradigms for
data efficiency. [78] achieves a decent compression rate and near interactive rendering with compact
motion and residual grids. However, their training requires 10 minutes per frame. [79] focuses on
real-time decoding, streaming and rendering instead of on-the-fly encoding. [84] performs grouped
training on a hybrid representation of triplanes and volume grid. While achieving high compression
rate, their fixed encoding paradigm and aggressive quantization limits their reconstruction quality
along-with low rendering speeds. [69] is the closest work to ours for streaming FVV via 3D-GS.
They encode the position and rotation residuals via an Instant-NGP [55] based transformation cache.
While achieving faster training and rendering speeds than prior work, they have high data redundancy
due to a fixed structured modeling. Additionally, they focus on geometric transformations only and"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.046908315565031986,Attributes
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.04904051172707889,"Scale, Rotation,"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.0511727078891258,"Opacity, Color"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.053304904051172705,"Transformed 3D Gaussians
Position"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.05543710021321962,Attributes
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.057569296375266525,Position
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.05970149253731343,"Scale, Rotation,"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.06183368869936034,"Opacity, Color"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.06396588486140725,Sec. 3.2
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.06609808102345416,Adaptive Masked Training
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.06823027718550106,Sec. 3.3
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.07036247334754797,"Position 
Residuals"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.07249466950959488,Position
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.07462686567164178,Latents
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.0767590618336887,Sec. 3.2.2
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.07889125799573561,"Gates
Quantization-"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.08102345415778252,"Sparsity
Framework"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.08315565031982942,"Initialize
Quantized"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.08528784648187633,Latents
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.08742004264392324,"Decoder 
Attribute
Residuals"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.08955223880597014,"Sec. 3.2.1 +
+"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.09168443496801706,"Time
3D Gaussians Time"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.09381663113006397,"Figure 1: Overview of QUEEN for online FVV. We incrementally update Gaussian attributes at each
time-step (gray block) by simultaneously learning and compressing residuals between consecutive
time-steps via a quantization (orange block) and sparsity (yellow block) framework. We additionally
render only the dynamic Gaussians for masked regions to achieve faster convergence (green block)."
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.09594882729211088,"can approximate only small changes in new scene content or lighting variations. Our work updates
and compresses all 3D-GS attributes freely without any structural constraints while still obtaining
much better memory costs via our quantization-sparsity framework along with better training times."
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.09808102345415778,"2.3
3D Scene Representation Compression"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.10021321961620469,"Several works propose a variety of compression methodologies for reducing the memory, training time,
or rendering speed of standard static scene 3D representations. [9, 71] decompose NeRFs via low-
rank approximations. [15, 38] prune voxels along with vector quantization by [38]. [25, 70] compress
multi-resolution feature-grids via codebook/vector quantization. A large number of approaches target
3D-GS compression [17, 24, 34, 58] and acceleration via pruning. While these approaches can be
applied to static representations on a per-frame basis, their trivial frame-wise application would result
in extremely high training costs as well as large memory per frame. To enable streaming, our work,
instead, explicitly focuses on effectively leveraging the temporal redundancies across frames by
compressing the residual information between them to achieve greater efficiency."
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.1023454157782516,"3
QUEEN: Quantized Efficient Encoding for Streaming FVV"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.1044776119402985,"A solution for streamable FVV must have low-latency encoding (training) and decoding (rendering),
and low data bandwidth (memory) for transmission on a common network infrastructure. Motivated
by these constraints, we aim to generate streamable FVVs with compact representations that are fast
to train and render incrementally. In this section, we first provide an overview of 3D-GS (Sec. 3.1).
In Sec. 3.2, we propose a compression framework to efficiently represent and train Gaussian attribute
residuals at each time step. Sec. 3.3 discusses utilizing an approach based on viewspace gradient
differences to achieve greater efficiencies. An overview of our method is shown in Fig. 1."
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.10660980810234541,"3.1
Preliminary: 3D Gaussian Splatting"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.10874200426439233,"Our efficient representation for dynamic scenes is based on 3D Gaussian Splatting (3DGS) [29].
Given multi-view images I, a 3D scene is modeled by a set of Gaussians with attributes A."
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.11087420042643924,"Representation. The shape of each Gaussian i is defined by its mean pi ∈R3 and covariance
matrix Σi. The covariance matrix is represented by Σi = RiSiST
i RT
i , where Ri is a rotation
matrix parameterized by a quaternion vector qi ∈R4, and the scale matrix Si is a diagonal matrix
with elements si ∈R3. Each Gaussian also contains opacity oi ∈[0, 1] and spherical harmonic
coefficients hi for view-dependent appearance with dimensions based on the number of degrees."
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.11300639658848614,"Rendering. For rasterization, 3D Gaussians are projected into 2D Gaussians for any given view.
Given a camera with intrinsic matrix K and viewing transform W, the 2D mean and covariance are:"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.11513859275053305,"p′
i = Π(pi; K, W),
Σ′
i = JWΣiWT JT ,
(1)"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.11727078891257996,"where Π(· ) denotes the perspective projection and J is the Jacobian of the affine approximation
of the projective transform [96]. The image color ˆc at pixel location x is obtained by blending N
depth-sorted Gaussians with their view-dependent RGB color value ci computed from hi:"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.11940298507462686,"ˆc(x) = N
X"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.12153518123667377,"i=1
ciαi i−1
Y"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.12366737739872068,"j=1
(1 −αj),
αi = oi · exp

−1"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.1257995735607676,"2(x −p′
i)T Σ′−1
i
(x −p′
i)

,
(2)"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.1279317697228145,where αi is the conic opacity of Gaussian i at pixel location x multiplied by the Gaussian opacity oi.
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.1300639658848614,"Training. With a differentiable rasterizer [29], the attributes A = {pi, qi, si, oi, hi}N
i=1 are optimized
to produce renderings ˆI = R(A) that fit to input images I by optimizing a reconstruction loss which
combines the D-SSIM loss [65] and L1 loss with a hyperparameter λ:"
NEURAL AND GAUSSIAN-BASED FREE-VIEWPOINT VIDEO,0.13219616204690832,"L = λLD-SSIM + (1 −λ)L1.
(3)"
ATTRIBUTE RESIDUAL COMPRESSION,0.13432835820895522,"3.2
Attribute Residual Compression"
ATTRIBUTE RESIDUAL COMPRESSION,0.13646055437100213,"Given a multi-view image sequence {It}T −1
t=0 , our goal is to reconstruct the dynamic scene via
Gaussian attributes At for each time-step t. We model the attributes based on the trained attributes
from the previous time-step t −1 as"
ATTRIBUTE RESIDUAL COMPRESSION,0.13859275053304904,"At = At−1 + Rt,
(4)"
ATTRIBUTE RESIDUAL COMPRESSION,0.14072494669509594,"where Rt consists of learnable residuals for each attribute (in Fig. 1 (gray block)). For time-step t = 0,
we perform vanilla Gaussian splatting training to obtain attributes A0. This sequential formulation
allows us to freely and adaptively update the residuals Rt on-the-fly with incoming streaming training
views, without any structural constraints as in prior works [69]. However, representing the 4D
scene with uncompressed residuals is still highly inefficient. As residuals have low magnitudes in
comparison with the attributes themselves, they can be efficiently compressed, for which we propose
a novel quantization-sparsity framework."
ATTRIBUTE RESIDUAL QUANTIZATION,0.14285714285714285,"3.2.1
Attribute Residual Quantization"
ATTRIBUTE RESIDUAL QUANTIZATION,0.14498933901918976,"There exists spatial redundancy within the Gaussian attributes of the same time-step. Nearby
Gaussians have highly correlated residuals for shape, orientation and appearance. To reduce the
storage cost of the residuals, we propose to utilize a quantization framework during training [24]."
ATTRIBUTE RESIDUAL QUANTIZATION,0.14712153518123666,"At each time-step t, we represent the residuals via quantized latents and a shared compact decoder.
Specifically, to obtain the residuals for each category2 ri ∈RM, we maintain corresponding quantized
integer latents li ∈ZL for each Gaussian i. These latents are passed through a shared linear decoder
D with learnable parameters D ∈RM×L to obtain the decoded attribute residual ri. Such a compact
decoder has small time and memory costs due to few parameters and arithmetic operations. To
allow differentiable training of the integer latents via gradient optimization, we use a continuous
approximation ˆli ∈RL instead. ˆli are rounded to the nearest integer values for the forward pass but
can still receive backpropagated gradients via the Straight-Through Estimator (STE) [3]:"
ATTRIBUTE RESIDUAL QUANTIZATION,0.14925373134328357,"li = STE(ˆli),
ri = D(li; D) = D · float(li).
(5)"
ATTRIBUTE RESIDUAL QUANTIZATION,0.1513859275053305,"The continuous latents ˆL = {ˆli}N
i=1, and the shared decoder’s parameters D are learnable during
training. After adding the decoded residuals to the previous time-step’s attributes (Eq. 4), the standard
rasterization process (Eq. 2) is used to obtain the rendered image. This differentiable quantization
module is trained end-to-end with the main training process by optimizing the reconstruction loss.
Post-training, we entropy code the quantized latents L and directly store the decoder D. Entropy
coding results in as much as 10× reduction in model size from 44 to 4 MB without quality degradation."
POSITION RESIDUAL GATING,0.1535181236673774,"3.2.2
Position Residual Gating"
POSITION RESIDUAL GATING,0.15565031982942432,"Sparse Representation. While most of the attribute residuals can be quantized effectively with our
proposed method in Sec 3.2.1, we observe that the position residuals are sensitive to quantization and
require high precision during rendering3. Storing all the full-precision position residuals, however,
still results in high per-frame memory costs. To tackle this, we propose a learned gating methodology,
which enforces sparsity in the residuals instead of quantization. This mechanism allows us to set
a vast majority of the position residuals to zeros, while maintaining full-precision non-zero values.
Specifically, we represent the positional residual for each Gaussian i as ∆pi = gi · lpi, where the
scalar gi is the learnable gate variable and lpi ∈R3 is the learnable pre-gated residual in full precision
during training. After training, the sparse ∆pi can be efficiently stored via sparse matrix formats [60]
to reduce memory costs. Thus, our goal is to encourage the sparsity for the variable gi across all"
POSITION RESIDUAL GATING,0.15778251599147122,"2r ∈R, belong to one of five categories of Gaussian attributes: position, rotation, scale, opacity and color.
3Concurrent work [58] also discovered that positional attributes are sensitive to compression."
POSITION RESIDUAL GATING,0.15991471215351813,"Gaussians. This goal also aligns with the observation that a large portion of a dynamic scene is static
or nearly static, which can be leveraged to attain high compression performance."
POSITION RESIDUAL GATING,0.16204690831556504,"Hard Concrete Gate. Sparsity can be induced via L0 or L1 norm regularization penalties. However,
L1 norm induces shrinkage, i.e., lowers the magnitude of even non-zero values. L0 norm is the
ideal sparsity loss without shrinkage, but is computationally intractable with non-differentiability and
combinatorial complexity. To enforce sparsity, we instead propose to use the hard concrete gate [49].
For each Gaussian i, the concrete gate [51] is a continuous relaxation of the Bernoulli distribution:"
POSITION RESIDUAL GATING,0.16417910447761194,"ˆgi = Sigmoid( log αi/τ),
(6)"
POSITION RESIDUAL GATING,0.16631130063965885,"where αi is a learnable parameter and τ is the temperature parameter. Although the concrete gate
approximates the discrete Bernoulli gate, it does not include the end points {0, 1}, which does not
directly result in sparsity. The hard concrete gate “stretches” the range of the concrete gate to the
interval (γ0, γ1) and then applies a hard-sigmoid:"
POSITION RESIDUAL GATING,0.16844349680170576,"egi = ˆgi · (γ1 −γ0) + γ0,
gi = min(1, max(0, egi)).
(7)"
POSITION RESIDUAL GATING,0.17057569296375266,"This includes the end points {0, 1} for gi, required for achieving sparse residuals."
POSITION RESIDUAL GATING,0.17270788912579957,"Sparsity Loss. The hard concrete gate formulation leads to a convenient regularization loss for
encouraging L0 sparsity [49] in the gates gi:"
POSITION RESIDUAL GATING,0.17484008528784648,"Lreg = N
X"
POSITION RESIDUAL GATING,0.17697228144989338,"i=1
pi = N
X"
POSITION RESIDUAL GATING,0.1791044776119403,"i=1
Sigmoid

log αi −τ log −γ0 γ1"
POSITION RESIDUAL GATING,0.1812366737739872,"
.
(8)"
POSITION RESIDUAL GATING,0.18336886993603413,"Here, we treat α = {αi}N
i=1 to be learnable parameters for all N Gaussians at a given time-step and
{τ, γ0, γ1} as hyperparameters that are shared for all Gaussians and all time-steps."
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.18550106609808104,"3.3
Viewspace Gradient Difference for Adaptive Training Scene"
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.18763326226012794,Gaussian Scene
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.18976545842217485,Gaussian
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.19189765458422176,Scene shift
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.19402985074626866,"Threshold
Partial
Render"
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.19616204690831557,Rendering Pixel Mask
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.19829424307036247,Gradient
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.20042643923240938,vector
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.2025586353944563,Gate parameter initialization
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.2046908315565032,"Figure 2: Viewspace Gradient Difference. We use the difference
of viewspace gradients between consecutive frames to identify
dynamic scene content."
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.2068230277185501,"Real-world dynamic scenes con-
tain high amounts of temporal
redundancy with only a fraction
of the content changing between
consecutive time-steps.
The
proposed quantization-sparsity
framework can learn to iden-
tify Gaussians corresponding to
static scene content and set
their residuals to 0.
How-
ever, they still forward/backward
pass through static regions result-
ing in wasted training computa-
tion.
Additionally, initializing
the gates with 1s requires more
iterations for convergence. We
thus propose a proxy metric to identify Gaussians, which are static or dynamic at the start of training.
We use this metric to initialize our gates while also identifying dynamic image regions to perform
local rendering in, during training."
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.208955223880597,"Viewspace Gradient Difference.
The ground-truth (GT) training images contain information of
the dynamic scene content, which we leverage to separate static and dynamic Gaussians. A simple
pixel difference between consecutive frames does not account for illumination changes and is a noisy
signal for the geometric position residuals. 3D-GS utilizes 2D viewspace gradients ∂L"
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.21108742004264391,∂p′ to identify
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.21321961620469082,"poorly fitted Gaussians based on the reconstruction loss L between the rendered (ˆI), GT image (I)."
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.21535181236673773,"More concretely, after training Gaussians at time-step t −1 with an MSE loss, Lt−1 and 2D Gaussian
means p′
t−1, we compute the MSE loss for the next time-step Lt and then compute the gradient
difference. The score vector dt is the average of gradient differences across all training views v:"
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.21748400852878466,dt = 1 V
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.21961620469083157,"V −1
X v=0"
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.22174840085287847,"""
∂L(v)
t
∂p′(v)
t−1
−∂L(v)
t−1
∂p′(v)
t−1 #"
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.22388059701492538,",
L(v)
t−1 = L

I(v)
t−1, ˆI(v)
t−1

,
L(v)
t
= L

I(v)
t
, ˆI(v)
t−1

.
(9)"
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.2260127931769723,"As shown in Fig. 2, dt identifies the dynamic scene regions while factoring out the noise from
imperfect reconstructions at time-step t −1. We use the norm of the score vector |dti| to initialize
the gate parameters. We define the probability of a gate being active for Gaussian i at time-step t as"
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.2281449893390192,"dti =
|dti|
|dti| + median
i=1,2,...,N(|dti|).
(10)"
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.2302771855010661,"We set pi in Eq. 8 to be dti to solve for the initial αi. This initialization leads to better convergence
by identifying Gaussians corresponding to 0 position residuals at the start of training itself."
VIEWSPACE GRADIENT DIFFERENCE FOR ADAPTIVE TRAINING,0.232409381663113,"Adaptive Masked Training. In addition to gate initialization, we propose to utilize the score vector
dt for an adaptive masked training scheme. We split the Gaussians into static or dynamic parts by
applying a threshold td on the norm of dt. We render dynamic Gaussians for each training view
to identify corresponding dynamic image regions. We then only render and backpropagate through
these pixel locations. We perform this masked training for a fraction of the full training iterations,
and find it to improve training speeds with little to no loss of reconstruction quality."
EFFICIENT END-TO-END LEARNABLE RESIDUALS,0.2345415778251599,"3.4
Efficient End-to-end Learnable Residuals"
EFFICIENT END-TO-END LEARNABLE RESIDUALS,0.23667377398720682,"Initial Frame Reconstruction. For an incrementally updating online approach, it is important
to make sure the initial frame is well reconstructed. COLMAP used to initialize the positions of
Gaussians can result in sparse 3D points for regions with sparse camera views. Hence, we use an
off-the-shelf monocular depth estimation network to estimate point locations in these empty regions
and predict a more complete initial point cloud. Further details and results are in the supplementary."
EFFICIENT END-TO-END LEARNABLE RESIDUALS,0.23880597014925373,"End-to-end Training. We train separate decoders and quantized latents {Lc, Dc|c ∈{q, s, o, h}}
for all attributes except position. For position, we learn the gate parameters α and positional residuals
Lp. All variables are end-to-end differentiable. The total loss function that we minimize is the
reconstruction loss (Eq. 3) and the sparsity gate regularization loss (Eq. 8):"
EFFICIENT END-TO-END LEARNABLE RESIDUALS,0.24093816631130063,"Ltotal = L + λregLreg,
(11)"
EFFICIENT END-TO-END LEARNABLE RESIDUALS,0.24307036247334754,"where λreg controls tradeoffs between memory and reconstruction quality. By simultaneously quan-
tizing while training we achieve high compression while maintaining quality, unlike [84, 78] with
post-training compression that lead to quality degradations. We also apply the 3D-GS densification
stage at each time-step and is sufficient in modeling new or finer scene content. 3DGStream [69]
adds Gaussians relative to the first time-step only, which limits their approach to small scene changes."
EXPERIMENTS,0.24520255863539445,"4
Experiments"
DATASETS AND IMPLEMENTATION,0.24733475479744135,"4.1
Datasets and Implementation"
DATASETS AND IMPLEMENTATION,0.24946695095948826,"We evaluate our method on two challenging FVV video datasets. (1) Neural 3D Videos (N3DV) [41]
consists of six indoor scenes with forward-facing 20-view videos. (2) Immersive Videos [4] consists
of seven indoor and outdoor scenes captures with 46 cameras. In both datasets, the central view
is held out for testing. We implement QUEEN on [29]. We train for 500 and 350 epochs for the
first time-step, and for 10 and 15 epochs for the subsequent time-steps, for N3DV and Immersive,
respectively, on an NVIDIA A100 GPU. One epoch contains all training views. We evaluate visual
quality in terms of average frame-wise PSNR, SSIM, and LPIPS (VGG) across all videos. We also
compute the average storage size and training time for each time-step, and the rendering speed.
Additional details are provided in the supplementary materials."
QUANTITATIVE COMPARISONS,0.2515991471215352,"4.2
Quantitative Comparisons"
QUANTITATIVE COMPARISONS,0.2537313432835821,"We compare QUEEN against state-of-the-art existing online FVV methods (3DGStream [69],
StreamRF [37] and TeTriRF [84]) on N3DV and Immersive (Tab. 1). 3DGStream [69] is the overall
best-performing prior method. Since 3DGStream [69] was originally run on an older NVIDIA V100
GPU on N3DV, we re-run 3DGStream on an NVIDIA A100 GPU on both N3DV and Immersive
and denote it as 3DGStream* in Tab. 1 for consistency with QUEEN. For brevity, in Tab. 1 we
additionally compare against only selected top-performing offline FVV methods. We include a more
extensive comparison to all existing offline FVV methods in the supplementary (Tab. 8). Lastly, we
evaluate three variants of QUEEN: QUEEN-s (small), QUEEN-m (medium) and QUEEN-l (large),
with residuals trained for 6, 8 and 10 epochs, respectively."
QUANTITATIVE COMPARISONS,0.255863539445629,"Table 1: Quantitative Results. We compare QUEEN against state-of-the-art online and (a few for
brevity) offline FVV methods on N3DV [41] and Immersive [4]. We include many more offline
methods in the supplementary (Tab. 8). 3DGStream* refers to our re-implementation on the same
NVIDIA A100 GPU used by QUEEN for fairness. Bold and underlined numbers indicate the best
and the second best results, respectively, within each category."
QUANTITATIVE COMPARISONS,0.2579957356076759,Neural 3D Video (N3DV) dataset
QUANTITATIVE COMPARISONS,0.2601279317697228,"Category
Method
PSNR
(dB) ↑
SSIM ↑
LPIPS ↓
Storage
(MB) ↓
Training
(sec) ↓
Rendering
(FPS) ↑"
QUANTITATIVE COMPARISONS,0.2622601279317697,"Offline
NeRFPlayer [68]
30.69
0.932
0.209
17.10
72
0.05
HyperReel [1]
31.10
0.928
-
1.20
104
2.00
SpaceTime [42]
32.05
0.948
-
0.67
20
140"
QUANTITATIVE COMPARISONS,0.26439232409381663,Online
QUANTITATIVE COMPARISONS,0.26652452025586354,"StreamRF [37]
30.68
-
-
31.4
15
8.3
TeTriRF [84]
30.43
0.906
0.248
0.06
39
4
3DGStream [69]
31.67
-
-
7.83
12
215
3DGStream* [69]
31.58
0.941
0.140
7.80
8.5
261"
QUANTITATIVE COMPARISONS,0.26865671641791045,"QUEEN-s (ours)
31.89
0.945
0.139
0.68
4.65
345
QUEEN-m (ours)
32.03
0.946
0.137
0.69
5.96
321
QUEEN-l (ours)
32.19
0.946
0.136
0.75
7.9
248"
QUANTITATIVE COMPARISONS,0.27078891257995735,Immersive dataset
QUANTITATIVE COMPARISONS,0.27292110874200426,"Category
Method
PSNR
(dB) ↑
SSIM ↑
LPIPS ↓
Storage
(MB) ↓
Training
(sec) ↓
Rendering
(FPS) ↑"
QUANTITATIVE COMPARISONS,0.27505330490405117,"Offline
NeRFPlayer [68]
25.8
0.848
0.329
17.1
∼72
0.12
HyperReel [1]
28.8
0.874
-
1.2
∼108
4
SpaceTime [42]
29.2
0.916
-
1.2
∼72
99"
QUANTITATIVE COMPARISONS,0.2771855010660981,"Online
3DGStream* [69]
25.18
0.876
0.255
8.83
32.4
221
QUEEN-l (ours)
29.22
0.915
0.208
1.79
19.7
183"
QUANTITATIVE COMPARISONS,0.279317697228145,Table 2: Effect of Various Components Ablated on N3DV and Immersive Datasets.
QUANTITATIVE COMPARISONS,0.2814498933901919,"Components
N3DV
Immersive"
QUANTITATIVE COMPARISONS,0.2835820895522388,"PSNR
(dB) ↑
Storage
(MB) ↓
Training
(sec) ↓
Rendering
(FPS) ↑
PSNR
(dB) ↑
Storage
(MB) ↓
Training
(sec) ↓
Rendering
(FPS) ↑"
QUANTITATIVE COMPARISONS,0.2857142857142857,"Baseline
31.66
44.36
7.29
214
28.54
78.4
20.85
276
+ Attribute Quantization
32.04
4.18
7.28
285
29.01
4.57
25.17
199
+ Position Gating
32.05
0.72
6.95
274
28.99
2.01
26.99
190
+ Gate Initialization
32.14
0.60
7.92
271
29.08
1.33
27.81
177
+ Masked Training
32.19
0.74
7.88
248
29.22
1.79
19.70
183"
QUANTITATIVE COMPARISONS,0.2878464818763326,"From Tab. 1, on N3DV, QUEEN-l results in the best quality among all online FVV methods and
achieves a 10× reduction in storage size compared to 3DGStream. Although TeTriRF requires less
memory than QUEEN, it has much worse quality (−1.5dB) and rendering speed (4FPS), and higher
training time (39 sec). On Immersive, which contains more pronounced scene changes than N3DV, we
limit our comparisons to 3DGStream with longer iterations. TeTriRF requires long convergence times
to achieve reasonable reconstruction quality, limiting their training feasibility. QUEEN-l significantly
outperforms 3DGStream, obtaining +4dB PSNR, 5× smaller size, and lower training times. These
results on the more challenging scenes from the Immersive datasets reveal the structural constraints
brought by the heuristic compression design of 3DGStream. In contrast, our quantization-sparsity
framework shows higher flexibility and quality in capturing changing appearances and scene density
as well as learning compact and effective representations."
QUALITATIVE COMPARISONS,0.2899786780383795,"4.3
Qualitative Comparisons"
QUALITATIVE COMPARISONS,0.2921108742004264,"In Fig. 3 we compare the reconstruction results of the various methods. On N3DV, we reconstruct
finer details than 3DGStream, e.g., the hand and the dog, and minimize artifacts such as the tongs in
the top scene or the coffee and metal tumbler in the bottom scene. TeTriRF produces blurry outputs,
e.g., the cap or metal tumbler in the bottom scene. On Immersive, we better model illumination
changes and new scene content such as the person (first patch) and the flame (third patch) in the top
scene or the face in the bottom scene (second patch) versus 3DGStream."
QUALITATIVE COMPARISONS,0.2942430703624733,"Figure 3: Qualitative Results. A visualization of various scenes in the N3DV and Immersive datasets.
PSNR (↑) values are shown. We include additional video results in the supplement."
QUALITATIVE COMPARISONS,0.29637526652452023,"Figure 4: Effect of Updating Appearance Attributes. QUEEN updates all Gaussian attributes,
resulting in improved quality versus keeping appearance attributes fixed across a video."
ABLATIONS,0.29850746268656714,"4.4
Ablations"
ABLATIONS,0.3006396588486141,"Effect of Updating Appearance Attributes.
To ablate the importance of updating the Gaussian
appearance attributes (color and opacity) per frame in QUEEN, we run experiments on N3DV for
2 settings: (1) learning only geometric attribute residuals (position, scale and covariance) with
appearance residuals set to zero and (2) learning all residuals per frame (Tab. 3). Updating only
geometric attributes results in a drop of 0.4 dB PSNR versus updating all attributes.Visually, for the
Flame Steak scene in N3DV (Fig. 4), updating all attributes results in the highest quality, while fixing
opacity introduces artifacts at the edge of the flamethrower. Fixing color, additionally, results in a
significant drop in PSNR (-0.6 dB) producing a discolored flame (rightmost column)."
ABLATIONS,0.302771855010661,"Effect of Attribute Compression and Masked Training.
We show results for five variants of
QUEEN with incrementally added sub-components: (1) a baseline with uncompressed residual
training (Sec. 3.2), (2) adding quantization to all attributes except position (Sec. 3.2.1), (3) adding
gating of position residuals (Sec. 3.2.2), (4) gate initialization with viewspace gradient differences
and (5) masked image training (Sec. 3.3). Results are summarized in Tab. 2 for both N3DV and
Immersive datasets. Compressing attributes and gating position residuals results in significant model
size reduction on both datasets (60×, 40×). This is further reduced by gate initialization with"
ABLATIONS,0.3049040511727079,"Table 3:
Updating Appearance Attributes on
N3DV. PSNR significantly improves by updating all
attributes but with a small storage overhead."
ABLATIONS,0.3070362473347548,"Update Attributes
PSNR
(dB)
Storage
(MB)
Train.
(sec)"
ABLATIONS,0.3091684434968017,"Geometric
31.61
0.61
7.87
+ Appearance
32.03
0.74
7.57"
ABLATIONS,0.31130063965884863,"Table 4: Effect of Quantizing Scaling Attribute
on N3DV. PSNR improves while also reducing model
size and training time due to faster rendering."
ABLATIONS,0.31343283582089554,"Configuration
PSNR
(dB)
Storage
(MB)
Train.
(sec)"
ABLATIONS,0.31556503198294245,"w/o Scaling Quant.
31.69
4.39
11.01
w/ Scaling Quant.
32.08
0.69
7.07"
ABLATIONS,0.31769722814498935,"Figure 5: Effect of Gating. While a large number of gates (47%) are active at start of training (a, c),
they are pruned and only gates corresponding to changing scene content (2%) remain active (b, d)."
ABLATIONS,0.31982942430703626,"viewspace gradient differences due to faster convergence of the gates, without loss of quality. By
masked training via localized image rendering, we reduce training time by 8 seconds for Immersive
and marginally for N3DV. Overall, from the baseline, we obtain significant model size reduction with
equivalent or lower training and rendering speed. Attribute quantization framework even improves
PSNR compared to the baseline for both datasets. This largely stems from quantizing the scaling
attribute leading to a more stable optimization with better reconstruction quality while reducing
storage size (Tab. 4)."
ABLATIONS,0.32196162046908317,"Effect of Gating. As shown in Fig. 5, more than half of the gates are set to be inactive at the start of
training with viewspace gradient initialization (Sec. 3.3) and a large portion of the image is active.
However, post-training, most gates become inactive while the remaining active gates successfully
focus on the dynamic scene content, e.g., the person’s hands or the dog’s face. This validates that our
gating mechanism effectively separates static and dynamic scene content."
ABLATIONS,0.32409381663113007,"Figure 6: Adaptive Image Mask Visualization. We separate out the dynamic scene content at
different time-steps of the video through our viewspace gradient difference approach in Sec. 3.3."
ABLATIONS,0.326226012793177,"Effect of Adaptive Image Mask.
We visualize the masks obtained by our viewspace gradient
difference module in Sec. 3.3. Results on 2 scenes in the Immersive dataset are shown in Fig. 6. For
various time instance of the video (columns), we adaptively identify image regions corresponding to
the dynamic scene content. We can therefore perform local image rendering and backpropagation for
faster training skipping computation for the static parts of the scene such as the background."
CONCLUSION,0.3283582089552239,"5
Conclusion"
CONCLUSION,0.3304904051172708,"We proposed QUEEN, a framework to model 3D dynamic scenes for online FVV using 3D-GS.
We utilized an attribute residual framework, which freely updates all parameters leading to better
modeling of complex scenes. We show that the residuals can be successfully compressed via our
learned quantization-sparsity mechanism, which adapts to the dynamic scene content to achieve very
small model sizes, improved training and rendering speeds, and improved visual quality. In future
work, we aim to extend QUEEN for sparse view reconstruction or sequences with long duration."
REFERENCES,0.3326226012793177,References
REFERENCES,0.3347547974413646,"[1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf,
Matthew O’Toole, and Changil Kim. Hyperreel: High-fidelity 6-dof video with ray-conditioned
sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 16610–16620, 2023."
REFERENCES,0.3368869936034115,"[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla,
and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance
fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
5855–5864, 2021."
REFERENCES,0.3390191897654584,"[3] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013."
REFERENCES,0.3411513859275053,"[4] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew
Duvall, Jason Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec. Immersive light field
video with a layered mesh representation. ACM Transactions on Graphics (TOG), 39(4):86–1,
2020."
REFERENCES,0.34328358208955223,"[5] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130–141,
2023."
REFERENCES,0.34541577825159914,"[6] Charles-Félix Chabert, Per Einarsson, Andrew Jones, Bruce Lamond, Wan-Chun Ma, Sebastian
Sylwan, Tim Hawkins, and Paul Debevec. Relighting human locomotion with flowed reflectance
fields. In ACM SIGGRAPH 2006 Sketches, pages 76–es. Association for Computing Machinery,
2006."
REFERENCES,0.34754797441364604,"[7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello,
Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-
aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 16123–16133, 2022."
REFERENCES,0.34968017057569295,"[8] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and
Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In
Proceedings of the IEEE/CVF international conference on computer vision, pages 14124–14133,
2021."
REFERENCES,0.35181236673773986,"[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance
fields. In European Conference on Computer Vision, pages 333–350. Springer, 2022."
REFERENCES,0.35394456289978676,"[10] Shenchang Eric Chen and Lance Williams. View interpolation for image synthesis. In Proceed-
ings of the 20th annual conference on Computer graphics and interactive techniques, pages
279–288, 1993."
REFERENCES,0.35607675906183367,"[11] Hye Won Chung, Brian M Sadler, and Alfred O Hero. Bounds on variance for unimodal
distributions. IEEE Transactions on Information Theory, 63(11):6936–6949, 2017."
REFERENCES,0.3582089552238806,"[12] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese,
Hugues Hoppe, Adam Kirk, and Steve Sullivan. High-quality streamable free-viewpoint video.
ACM Transactions on Graphics (ToG), 34(4):1–13, 2015."
REFERENCES,0.3603411513859275,"[13] Abe Davis, Marc Levoy, and Fredo Durand. Unstructured light fields. Comput. Graph. Forum,
31(2pt1):305–314, may 2012. ISSN 0167-7055. doi: 10.1111/j.1467-8659.2012.03009.x. URL
https://doi.org/10.1111/j.1467-8659.2012.03009.x."
REFERENCES,0.3624733475479744,"[14] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark
Sagar. Acquiring the reflectance field of a human face. In Proceedings of the 27th annual
conference on Computer graphics and interactive techniques, pages 145–156, 2000."
REFERENCES,0.3646055437100213,"[15] Chenxi Lola Deng and Enzo Tartaglione. Compressing explicit voxel grid representations: fast
nerfs become also small. In Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision, pages 1236–1245, 2023."
REFERENCES,0.36673773987206826,"[16] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenenbaum, and Jiajun Wu. Neural radiance
flow for 4d view synthesis and video processing. In 2021 IEEE/CVF International Conference
on Computer Vision (ICCV), pages 14304–14314. IEEE Computer Society, 2021."
REFERENCES,0.36886993603411516,"[17] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Light-
gaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. arXiv preprint
arXiv:2311.17245, 2023."
REFERENCES,0.37100213219616207,"[18] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo
Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 5501–5510, 2022."
REFERENCES,0.373134328358209,"[19] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo
Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12479–12488,
2023."
REFERENCES,0.3752665245202559,"[20] Jean Furukawa, Yasutaka; Ponce. Accurate, dense, and robust multiview stereopsis. IEEE
TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 32(8), 2010."
REFERENCES,0.3773987206823028,"[21] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias Nießner. Dynamic neural radiance
fields for monocular 4d facial avatar reconstruction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 8649–8658, 2021."
REFERENCES,0.3795309168443497,"[22] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from
dynamic monocular video. In Proceedings of the IEEE International Conference on Computer
Vision, 2021."
REFERENCES,0.3816631130063966,"[23] Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang,
and Ulrich Neumann. Gaussianflow: Splatting gaussian dynamics for 4d content creation. arXiv
preprint arXiv:2403.12365, 2024."
REFERENCES,0.3837953091684435,"[24] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d
gaussians with lightweight encodings. arXiv preprint arXiv:2312.04564, 2023."
REFERENCES,0.3859275053304904,"[25] Sharath Girish, Abhinav Shrivastava, and Kamal Gupta. Shacira: Scalable hash-grid com-
pression for implicit neural representations. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), pages 17513–17524, October 2023."
REFERENCES,0.3880597014925373,"[26] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, Geoff
Harvey, Sergio Orts-Escolano, Rohit Pandey, Jason Dourgarian, et al. The relightables: Volu-
metric performance capture of humans with realistic relighting. ACM Transactions on Graphics
(ToG), 38(6):1–19, 2019."
REFERENCES,0.39019189765458423,"[27] Takeo Kanade, Peter Rander, and PJ Narayanan. Virtualized reality: Constructing virtual worlds
from real scenes. IEEE Multimedia, 4(1):34–47, 1997."
REFERENCES,0.39232409381663114,"[28] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM Transac-
tions on Graphics (ToG), 32(3):1–13, 2013."
REFERENCES,0.39445628997867804,"[29] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian
splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1–14,
2023."
REFERENCES,0.39658848614072495,"[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.39872068230277186,"[31] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, and Matthias Nießner.
Nersemble: Multi-view radiance field reconstruction of human heads. ACM Trans. Graph.,
42(4), jul 2023. ISSN 0730-0301. doi: 10.1145/3592455. URL https://doi.org/10.
1145/3592455."
REFERENCES,0.40085287846481876,"[32] Muhammed Kocabas, Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. Hugs:
Human gaussian splats, 2023. URL https://arxiv.org/abs/2311.17910."
REFERENCES,0.40298507462686567,"[33] Glen G Langdon. An introduction to arithmetic coding. IBM Journal of Research and Develop-
ment, 28(2):135–149, 1984."
REFERENCES,0.4051172707889126,"[34] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d
gaussian representation for radiance field. arXiv preprint arXiv:2311.13681, 2023."
REFERENCES,0.4072494669509595,"[35] Marc Levoy and Pat Hanrahan. Light field rendering. In Proceedings of the 23rd annual
conference on Computer graphics and interactive techniques, pages 31–42, 1996."
REFERENCES,0.4093816631130064,"[36] Hao Li, Bart Adams, Leonidas J. Guibas, and Mark Pauly. Robust single-view geometry and
motion reconstruction. ACM Transactions on Graphics (Proceedings SIGGRAPH Asia 2009),
28(5), December 2009."
REFERENCES,0.4115138592750533,"[37] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Ping Tan. Streaming radiance fields
for 3d video synthesis. Advances in Neural Information Processing Systems, 35:13485–13498,
2022."
REFERENCES,0.4136460554371002,"[38] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. Compressing volumetric
radiance fields to 1 mb. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 4222–4231, 2023."
REFERENCES,0.4157782515991471,"[39] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhöfer, Jürgen Gall, Angjoo Kanazawa, and
Christoph Lassner. Tava: Template-free animatable volumetric actors. In European Conference
on Computer Vision, pages 419–436. Springer, 2022."
REFERENCES,0.417910447761194,"[40] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning a model
of facial shape and expression from 4D scans. ACM Transactions on Graphics, (Proc. SIG-
GRAPH Asia), 36(6):194:1–194:17, 2017. URL https://doi.org/10.1145/3130800.
3130813."
REFERENCES,0.4200426439232409,"[41] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil
Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural
3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 5521–5531, 2022."
REFERENCES,0.42217484008528783,"[42] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time
dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2024."
REFERENCES,0.42430703624733473,"[43] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for
space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 6498–6508, 2021."
REFERENCES,0.42643923240938164,"[44] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar:
Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2023."
REFERENCES,0.42857142857142855,"[45] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou.
Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022
Conference Papers, pages 1–9, 2022."
REFERENCES,0.43070362473347545,"[46] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, and Xiaowei Zhou. High-
fidelity and real-time novel view synthesis for dynamic scenes. In SIGGRAPH Asia 2023
Conference Papers, pages 1–9, 2023."
REFERENCES,0.43283582089552236,"[47] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and
Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. ACM
Transactions on Graphics, 38(4):65:1–65:14, July 2019."
REFERENCES,0.4349680170575693,"[48] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black.
SMPL: A skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia),
34(6):248:1–248:16, October 2015."
REFERENCES,0.43710021321961623,"[49] Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks
through L0 regularization. arXiv preprint arXiv:1712.01312, 2017."
REFERENCES,0.43923240938166314,"[50] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians:
Tracking by persistent dynamic view synthesis. In 3DV, 2024."
REFERENCES,0.44136460554371004,"[51] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016."
REFERENCES,0.44349680170575695,"[52] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovit-
skiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo
collections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7210–7219, 2021."
REFERENCES,0.44562899786780386,"[53] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi
Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis
with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 38(4):1–14,
2019."
REFERENCES,0.44776119402985076,"[54] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi,
and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In
European Conference on Computer Vision, 2020."
REFERENCES,0.44989339019189767,"[55] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics
primitives with a multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):
1–15, 2022."
REFERENCES,0.4520255863539446,"[56] Richard A Newcombe, Steven J Lovegrove, and Andrew J Davison. Dtam: Dense tracking and
mapping in real-time. In 2011 international conference on computer vision, pages 2320–2327.
IEEE, 2011."
REFERENCES,0.4541577825159915,"[57] Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, Yury
Degtyarev, David Kim, Philip L Davidson, Sameh Khamis, Mingsong Dou, et al. Holoportation:
Virtual 3d teleportation in real-time. In Proceedings of the 29th annual symposium on user
interface software and technology, pages 741–754, 2016."
REFERENCES,0.4562899786780384,"[58] Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, and George
Drettakis. Reducing the memory footprint of 3d gaussian splatting. In Proceedings of the ACM
on Computer Graphics and Interactive Techniques, volume 7, 2024."
REFERENCES,0.4584221748400853,"[59] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B
Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: A higher-dimensional
representation for topologically varying neural radiance fields. ACM Trans. Graph., 40(6), dec
2021."
REFERENCES,0.4605543710021322,"[60] Sergio Pissanetzky. Sparse matrix technology-electronic edition. Academic Press, 1984."
REFERENCES,0.4626865671641791,"[61] Fabián Prada, Misha Kazhdan, Ming Chuang, Alvaro Collet, and Hugues Hoppe. Spatiotemporal
atlas parameterization for evolving meshes. ACM Transactions on Graphics (TOG), 36(4):1–12,
2017."
REFERENCES,0.464818763326226,"[62] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf:
Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 10318–10327, 2021."
REFERENCES,0.4669509594882729,"[63] Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, and
Matthias Nießner. Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians. arXiv
preprint arXiv:2312.02069, 2023."
REFERENCES,0.4690831556503198,"[64] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards
robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 44(3), 2022."
REFERENCES,0.47121535181236673,"[65] Umme Sara, Morium Akter, and Mohammad Shorif Uddin. Image quality assessment through
fsim, ssim, mse and psnr—a comparative study. Journal of Computer and Communications, 7
(3):8–18, 2019."
REFERENCES,0.47334754797441364,"[66] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu.
Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and
rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 16632–16642, 2023."
REFERENCES,0.47547974413646055,"[67] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael
Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 2437–2446, 2019."
REFERENCES,0.47761194029850745,"[68] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, and
Andreas Geiger. Nerfplayer: A streamable dynamic scene representation with decomposed
neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 29(5):
2732–2742, 2023. doi: 10.1109/TVCG.2023.3247082."
REFERENCES,0.47974413646055436,"[69] Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, and Wei Xing. 3dgstream:
On-the-fly training of 3d gaussians for efficient streaming of photo-realistic free-viewpoint
videos. arXiv preprint arXiv:2403.01444, 2024."
REFERENCES,0.48187633262260127,"[70] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas Müller, Morgan McGuire, Alec
Jacobson, and Sanja Fidler. Variable bitrate neural fields. In ACM SIGGRAPH 2022 Conference
Proceedings, pages 1–9, 2022."
REFERENCES,0.4840085287846482,"[71] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. Compressible-composable nerf
via rank-residual decomposition. Advances in Neural Information Processing Systems, 35:
14798–14809, 2022."
REFERENCES,0.4861407249466951,"[72] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Wang Yifan,
Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Ad-
vances in neural rendering. In Computer Graphics Forum, volume 41, pages 703–735. Wiley
Online Library, 2022."
REFERENCES,0.488272921108742,"[73] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, and
Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis
of a dynamic scene from monocular video. In IEEE International Conference on Computer
Vision (ICCV). IEEE, 2021."
REFERENCES,0.4904051172707889,"[74] Edith Tretschk, Vladislav Golyanik, Michael Zollhöfer, Aljaz Bozic, Christoph Lassner, and
Christian Theobalt. Scenerflow: Time-consistent reconstruction of general dynamic scenes. In
International Conference on 3D Vision (3DV), 2024."
REFERENCES,0.4925373134328358,"[75] Chaoyang Wang, Ben Eckart, Simon Lucey, and Orazio Gallo. Neural trajectory fields for
dynamic novel view synthesis. arXiv preprint arXiv:2105.05994, 2021."
REFERENCES,0.4946695095948827,"[76] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei Song, and Huaping Liu. Mixed neural
voxels for fast multi-view video synthesis. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 19706–19716, 2023."
REFERENCES,0.4968017057569296,"[77] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang,
Minye Wu, Jingyi Yu, and Lan Xu. Fourier plenoctrees for dynamic radiance field rendering
in real-time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 13524–13534, 2022."
REFERENCES,0.4989339019189765,"[78] Liao Wang, Qiang Hu, Qihan He, Ziyu Wang, Jingyi Yu, Tinne Tuytelaars, Lan Xu, and Minye
Wu. Neural residual radiance fields for streamably free-viewpoint videos. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 76–87,
June 2023."
REFERENCES,0.5010660980810234,"[79] Liao Wang, Kaixin Yao, Chengcheng Guo, Zhirui Zhang, Qiang Hu, Jingyi Yu, Lan Xu, and
Minye Wu. Videorf: Rendering dynamic radiance fields as 2d feature video streams, 2023."
REFERENCES,0.5031982942430704,"[80] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander
Holynski, and Noah Snavely. Tracking everything everywhere all at once. In International
Conference on Computer Vision, 2023."
REFERENCES,0.5053304904051172,"[81] Shengze Wang, Alexey Supikov, Joshua Ratcliff, Henry Fuchs, and Ronald Azuma. Inv:
Towards streaming incremental neural videos. arXiv preprint arXiv:2302.01532, 2023."
REFERENCES,0.5074626865671642,"[82] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan T Barron, and Ira Kemelmacher-
Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In
Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition, pages
16210–16220, 2022."
REFERENCES,0.509594882729211,"[83] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu,
Qi Tian, and Wang Xinggang. 4d gaussian splatting for real-time dynamic scene rendering. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024."
REFERENCES,0.511727078891258,"[84] Minye Wu, Zehao Wang, Georgios Kouros, and Tinne Tuytelaars. Tetrirf: Temporal tri-plane
radiance fields for efficient free-viewpoint video. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, 2024."
REFERENCES,0.5138592750533049,"[85] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance
fields for free-viewpoint video. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9421–9431, 2021."
REFERENCES,0.5159914712153518,"[86] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico
Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual
computing and beyond. In Computer Graphics Forum, volume 41, pages 641–676. Wiley
Online Library, 2022."
REFERENCES,0.5181236673773987,"[87] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao,
and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution. arXiv preprint
arXiv:2310.11448, 2023."
REFERENCES,0.5202558635394456,"[88] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene
representation and rendering with 4d gaussian splatting.
In International Conference on
Learning Representations (ICLR), 2024."
REFERENCES,0.5223880597014925,"[89] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. De-
formable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint
arXiv:2309.13101, 2023."
REFERENCES,0.5245202558635395,"[90] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for
unstructured multi-view stereo. In Proceedings of the European conference on computer vision
(ECCV), pages 767–783, 2018."
REFERENCES,0.5266524520255863,"[91] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for
real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 5752–5761, 2021."
REFERENCES,0.5287846481876333,"[92] Raza Yunus, Jan Eric Lenssen, Michael Niemeyer, Yiyi Liao, Christian Rupprecht, Christian
Theobalt, Gerard Pons-Moll, Jia-Bin Huang, Vladislav Golyanik, and Eddy Ilg. Recent trends
in 3d reconstruction of general non-rigid scenes. In Computer Graphics Forum, page e15062.
Wiley Online Library, 2024."
REFERENCES,0.5309168443496801,"[93] Jiakai Zhang, Liao Wang, Xinhang Liu, Fuqiang Zhao, Minzhang Li, Haizhao Dai, Boyuan
Zhang, Wei Yang, Lan Xu, and Jingyi Yu. Neuvv: Neural volumetric videos with immersive
rendering and editing. arXiv preprint arXiv:2202.06088, 2022."
REFERENCES,0.5330490405117271,"[94] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu.
Humannerf: Efficiently generated human radiance field from sparse inputs. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
7743–7753, June 2022."
REFERENCES,0.535181236673774,"[95] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, and Richard
Szeliski. High-quality video view interpolation using a layered representation. ACM transactions
on graphics (TOG), 23(3):600–608, 2004."
REFERENCES,0.5373134328358209,"[96] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Surface splatting. In
Proceedings of the 28th annual conference on Computer graphics and interactive techniques,
pages 371–378, 2001."
REFERENCES,0.5394456289978679,Appendix
REFERENCES,0.5415778251599147,"We provide supplementary results (Appendix A), additional implementation details (Appendix B),
discussion on limitations and future work (Appendix C) and the broader impact (Appendix D) of
our approach. We recommend the reader to watch the supplementary video hosted on our project
website: https://research.nvidia.com/labs/amri/projects/queen for a visual
comparison of the results of the various methods as well as more details of this project."
REFERENCES,0.5437100213219617,"A
Supplementary Results"
REFERENCES,0.5458422174840085,"A.1
Quantization vs. Gating"
REFERENCES,0.5479744136460555,"We evaluate the effect of the gating framework in comparison to the quantization framework for the
position residuals. We perform gating or quantization on the position residuals while quantizing all
other attributes. Results, averaged on the N3DV dataset, are shown in Figure 7. We vary training
epochs per frame from 6 to 15 for gating and 6 to 25 for quantization to obtain trade-off curves.
Increased training epochs result in higher reconstruction quality (PSNR) but longer training time
or a larger model size. The left figure shows the PSNR versus size tradeoff while the right figure
shows PSNR versus training time tradeoff. We see that, in both cases, the gating framework produces
much better tradeoff curves than quantization, with PSNR values more than 0.2dB higher at similar
sizes. When increasing the number of training iterations, quantization still improves in quality albeit
at a slower rate requiring more training iterations for convergence. This demonstrates that position
attributes are more sensitive to quantization errors and require full precision. It justifies our choice
of learning to sparsify them as opposed to quantizing them. However, this does not translate to the
other geometric attributes, scaling and rotation, where quantization is sufficient in compressing the
attributes. This is seen in the results in Table 5 on the Exhibit scene from the Immersive dataset.
Quantizing both rotation and scaling results in the lowest storage memory per frame at a similar
PSNR and slightly higher training time."
REFERENCES,0.5501066098081023,Table 5: Gating versus Quantization of Rotation and Scale Attributes
REFERENCES,0.5522388059701493,"Rotation
Scaling
PSNR
Storage
Mem (MB)
Training
Time (s)"
REFERENCES,0.5543710021321961,"Quantization Quantization
29.15
2.47
21.61
Gating
Quantization
29.26
3.62
20.44
Quantization
Gating
28.92
3.64
19.89"
REFERENCES,0.5565031982942431,"0.2
0.4
0.6
0.8
1.0
1.2
Size (MB) 31.0 31.2 31.4 31.6 31.8 32.0 32.2"
REFERENCES,0.55863539445629,PSNR (dB)
REFERENCES,0.5607675906183369,PSNR vs Size Type
REFERENCES,0.5628997867803838,"Quant
Gating"
REFERENCES,0.5650319829424307,"2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Frame Time (s) 31.0 31.2 31.4 31.6 31.8 32.0 32.2"
REFERENCES,0.5671641791044776,PSNR (dB)
REFERENCES,0.5692963752665245,PSNR vs Time Type
REFERENCES,0.5714285714285714,"Quant
Gating"
REFERENCES,0.5735607675906184,"Figure 7: Effect of Gating vs Quantization of Position Residuals. Gating leads to much better
PSNR versus size or PSNR versus training time tradeoff curves due to higher precision residuals."
REFERENCES,0.5756929637526652,"140
160
180
200
220
Size 31.6 31.7 31.8 31.9 32.0 32.1 PSNR"
REFERENCES,0.5778251599147122,PSNR vs Size
REFERENCES,0.579957356076759,"std
1e-08
1e-06
1e-05
3e-05
7e-05
0.0001
0.001"
REFERENCES,0.582089552238806,"165
170
175
180
185
Size 31.85 31.90 31.95 32.00 32.05 32.10 PSNR"
REFERENCES,0.5842217484008528,PSNR vs Size reg
REFERENCES,0.5863539445628998,"0.03
0.05
0.1
0.2
0.3
0.5
0.7"
REFERENCES,0.5884861407249466,"(a)
(b)"
REFERENCES,0.5906183368869936,"Figure 8: PSNR-Size Tradeoffs. We evaluate the effect of varying λstd and λreg for (a) the quantized
latents and (b) the sparse position residuals. The storage size is measured in KB for the full video
duration of 300 frames."
REFERENCES,0.5927505330490405,"Table 6: Effect of Latent Dimensions. Dim is latent dimension and Size is total size of the 300-frame
video in MB. *Metrics for Color SH are evaluated on three N3DV scenes."
REFERENCES,0.5948827292110874,"Color SH*
Color Base
Rotation
Scaling"
REFERENCES,0.5970149253731343,"Dim
PSNR
Size
Dim
PSNR
Size
Dim
PSNR
Size
Dim
PSNR
Size"
REFERENCES,0.5991471215351812,"1
33.69
155
2
32.02
211
2
32.00
182
2
32.13
215
2
33.73
157
4
32.04
211
4
31.98
183
4
32.06
219
4
33.78
156
8
32.04
213
6
32.01
182
8
32.11
224
8
33.71
156
12
32.06
215
8
32.00
182
12
32.07
226"
REFERENCES,0.6012793176972282,"A.2
Accuracy-memory Tradeoff"
REFERENCES,0.603411513859275,"We consider the effect of varying different loss coefficients to trade off between accuracy and memory.
In Figure 7 we explore the tradeoff between PSNR and size by varying the number of training
iterations. We can also control the amount of sparsity in the scene by varying the λreg loss coefficient.
As visualized in Fig. 8 (b), we find that increasing λreg leads to higher sparsity or lower memory but
also lower reconstruction quality."
REFERENCES,0.605543710021322,"We further experiment with an additional regularization loss to reduce the entropy of the latents. We
observe that lower entropy corresponds to lower memory, but also lower reconstruction quality. While
learnable probability models can successfully reduce entropy, as shown by [25], these models have
higher time and memory costs during training. We instead observe that the probability distribution of
the various attribute residuals at each time-step is unimodal and is close to a Laplacian or Gaussian
distribution. As a unimodal distribution has entropy proportional to the variance [11], we enforce a
loss on the standard deviation of the latents with a tradeoff parameter λstd controlling the effect of
this regularization loss. Fig. 8(a) shows results on the N3DV dataset by varying λstd. We observe that
increasing λstd reduces the entropy costs, leading to lower memory costs, but lower reconstruction
quality, and vice versa."
REFERENCES,0.6076759061833689,"A.3
Effect of Quantization Latent Dimension"
REFERENCES,0.6098081023454158,"We provide additional analysis on the effect of latent dimension for the various attributes in Table 6.
In general, latent dimension does not have a significant effect on reconstruction quality or model
size. Increasing the latent dimension can lead to lower per-dimension entropy due to our learnable
quantization framework and hence still maintains the overall total size for the latent. We find
that varying the total number of iterations (Appendix A.1) or the entropy loss/variance coefficient
(Appendix A.2) are more effective knobs for trading off between quality-memory or quality-time."
REFERENCES,0.6119402985074627,"0
50
100
150
200
250
300 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85"
REFERENCES,0.6140724946695096,Model Size (MB)
REFERENCES,0.6162046908315565,Model Size 5.0 5.5 6.0 6.5 7.0
REFERENCES,0.6183368869936035,Consecutive Frame Difference
E,0.6204690831556503,"1e
3
Cook Spinach"
E,0.6226012793176973,Frame Difference
E,0.6247334754797441,"0
50
100
150
200
250
300
0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80"
E,0.6268656716417911,Model Size (MB)
E,0.6289978678038379,Model Size 5.0 5.5 6.0 6.5 7.0 7.5
E,0.6311300639658849,Consecutive Frame Difference
E,0.6332622601279317,"1e
3
Cut Roasted"
E,0.6353944562899787,Frame Difference
E,0.6375266524520256,"0
25
50
75
100
125
150 1.8 1.9 2.0 2.1 2.2"
E,0.6396588486140725,Model Size (MB)
E,0.6417910447761194,Model Size 1 2 3 4 5
E,0.6439232409381663,Consecutive Frame Difference
E,0.6460554371002132,"1e
3
Truck"
E,0.6481876332622601,Frame Difference
E,0.650319829424307,"0
50
100
150
200
250
300 1.0 1.1 1.2 1.3 1.4 1.5 1.6"
E,0.652452025586354,Model Size (MB)
E,0.6545842217484008,Model Size 0 1 2 3 4 5
E,0.6567164179104478,Consecutive Frame Difference
E,0.6588486140724946,"1e
2
Face1"
E,0.6609808102345416,Frame Difference
E,0.6631130063965884,"Figure 9: Adaptive Sizes. Our quantization-sparsity framework automatically allocates more memory
for frames with larger scene changes (higher frame difference). Notice that the frame difference
spikes in the Immersive scenes (bottom row) correlate with frame-wise model size, which increases
to allow for modeling of more temporal variations."
E,0.6652452025586354,"A.4
Framewise PSNR and Size"
E,0.6673773987206824,"A key advantage of our quantization-sparsity framework is its adaptability to scene content. We
visualize the per frame sizes for 2 scenes each from N3DV and Immersive along with the visual
frame difference between consecutive frames in Figure 9. We see that our approach allocates variable
model sizes for each frame unlike 3DGStream [69], which uses a fixed-sized InstantNGP structure.
Additionally, higher frame differences result in larger model sizes and vice versa, as seen in the top
row. This shows that our method is capable of allocating more bits to frames with large scene changes.
This is especially evident from the spikes in Immersive’s scenes in the bottom row, which correlate
with the model size."
E,0.6695095948827292,"Next, we show the stability of our approach at recovering from large scene variations corresponding
to the frame difference spikes as mentioned above. We visualize the reconstructed test-view PSNR for
each frame, for 2 scenes each from N3DV and Immersive, along with the frame difference between
consecutive frames in Figure 10. A large L1 error such as around frames 175 (top left), frames 225
(top right), frames 75 (bottom left) or frames 90 and 290 (bottom right) does lead to drops in PSNR.
However, our PSNR recovers in subsequent frames showing the stability of our framework with large
scene variations present."
E,0.6716417910447762,"A.5
Effect of Improved Point Cloud Initialization."
E,0.673773987206823,"Consistent geometry for the 3D scene in the first frame is important to learn accurate residuals for
the attributes of the subsequent frames. The COLMAP-generated point cloud initialization can be
incomplete for regions that are textureless or are not sufficiently captured in multiple cameras. This
is visualized in the top row of Figure 11. The boundaries of the scene consist of limited training
view cameras as shown by the white box leading to sparse or no points in these regions by COLMAP
initialization. The densification stage in 3DGS is unable to recover from this producing erroneous
rendered depth or geometry and also leads to low quality image reconstruction."
E,0.67590618336887,"0
50
100
150
200
250
300 32.5 33.0 33.5 34.0 34.5"
E,0.6780383795309168,PSNR (dB) PSNR 5.0 5.5 6.0 6.5 7.0 7.5
E,0.6801705756929638,Consecutive Frame Difference
E,0.6823027718550106,"1e
3
Flame Steak"
E,0.6844349680170576,Frame Difference
E,0.6865671641791045,"0
50
100
150
200
250
300 33.4 33.6 33.8 34.0 34.2"
E,0.6886993603411514,PSNR (dB) PSNR 5.2 5.4 5.6 5.8 6.0 6.2 6.4
E,0.6908315565031983,Consecutive Frame Difference
E,0.6929637526652452,"1e
3
Sear Steak"
E,0.6950959488272921,Frame Difference
E,0.697228144989339,"0
25
50
75
100
125
150 26.0 26.5 27.0 27.5 28.0 28.5"
E,0.6993603411513859,PSNR (dB) PSNR 1 2 3 4 5
E,0.7014925373134329,Consecutive Frame Difference
E,0.7036247334754797,"1e
3
Truck"
E,0.7057569296375267,Frame Difference
E,0.7078891257995735,"0
50
100
150
200
250
300 26 27 28 29 30 31 32"
E,0.7100213219616205,PSNR (dB) PSNR 0 1 2 3 4 5
E,0.7121535181236673,Consecutive Frame Difference
E,0.7142857142857143,"1e
2
Face2"
E,0.7164179104477612,Frame Difference
E,0.7185501066098081,"Figure 10: Per-frame Quality Evaluation. Our approach results in higher PSNR for large scene
changes corresponding to higher consecutive frame difference such as around frame 175 (top right)
or the spikes in the bottom right scene."
E,0.720682302771855,"Figure 11: Effect of Depth Initialization. Top row: (a) COLMAP produces sparse or no points for
regions of the scene with limited texture, producing (b) erroneous image rendering and (c) incorrect
geometry or depth. Bottom row: initializing with depth maps predicted by an off-the-shelf monocular
depth network produces better reconstruction and consistent scene geometry."
E,0.7228144989339019,"Therefore, we propose to use an off-the-shelf monocular depth estimation network [64] to predict
a more complete initial point cloud. However, due to the scale-shift ambiguity of monocular depth
estimation, we align the predicted monocular depth with the true scene depth from existing COLMAP
points. To do so, we estimate the 2D pixel locations p′
i of each COLMAP point i by projecting points
from 3D world space to 2D screenspace."
E,0.7249466950959488,"p′
i = Π(pi; K, W),
(12)"
E,0.7270788912579957,"where Π(.) denotes the perspective transform described in Eq. 1 of the paper. We then query the
pixel locations p′
i in the monocular depth image to obtain the corresponding depth values ˆzi. This is"
E,0.7292110874200426,Table 7: Effect of Depth Initialization on N3DV and Immersive datasets.
E,0.7313432835820896,"Dataset
Initialization
PSNR (Test)
(dB)↑
PSNR (Train)
(dB)↑
Num. Points
(in Millions) ↓
Training
Time (s) ↓"
E,0.7334754797441365,"N3DV
COLMAP
32.02
30.06
3.24
13.24
+Depth
32.03
31.42
3.14
13.40"
E,0.7356076759061834,"Immersive
COLMAP
28.52
32.38
3.10
18.62
+Depth
29.10
32.65
3.21
18.91"
E,0.7377398720682303,"aligned with the GT depth value from the COLMAP 3D points zi by a least-squares optimization to
obtain the scale and shift parameters α, β. We then obtain the aligned dense depth map αˆzi + β."
E,0.7398720682302772,"To identify regions with empty COLMAP initializations, we iterate over training views and render
the corresponding image along with an alpha mask calculating the accumulated transmittance at
each pixel location. Mask values below a threshold tz (0.10 for N3DV and 0.03 for Immersive) are
identified to obtain the corresponding pixel locations containing few COLMAP points. We then use
the aligned depth values corresponding to these pixel locations to re-project back into the world
space."
E,0.7420042643923241,"As seen in the bottom row of Figure 11, the depth map initialization produces more dense points
in empty regions. These points maintain consistent depth with existing COLMAP points. Such an
initialization results in improved image reconstruction quality with ∼3dB improvement in PSNR for
the corresponding view while also producing better consistent depth or geometry. As we utilize the
network depth at initialization only, we do not require high-quality depth networks and a coarse depth
is sufficient for sampling new points. The training can then learn to move the Gaussians to produce
finer scene depth. This also results in minimal increases in training time as it’s a one-time operation
at the initial time-step and produces a small number of additional points in empty regions."
E,0.744136460554371,"We show quantitative results for 2 configurations with and without depth map initialization for the
datasets of N3DV and Immersive in Tab. 7. We show PSNR for the central test view as well as for the
first train view for all scenes. We also show the number of points at the end of training the first frame
with and without initialization along with the average training time per frame for the full scene. The
depth initialization does not significantly affect the test view PSNR on N3DV as it corresponds to the
central view with a large overlap with many training views. In contrast, the PSNR for the training
view (also in Fig. 11) improves considerably (+1.3dB) with the depth initialization highlighting the
efficacy of the approach in improving geometry for regions with sparse camera views. Additionally,
there is almost no overhead cost as we obtain similar number of Gaussian points at the end of training
the first frame. Training time for the full video is also only marginally higher. On Immersive, we
observe a 0.5 dB improvement in PSNR for the test view while the train view PSNR shows minor
improvements. Again, the number of Gaussians and training time is not significantly affected by the
initialization with additional depth based points."
E,0.746268656716418,"A.6
Additional Baseline Comparisons"
E,0.7484008528784648,"We show comparisons to additional offline FVV baselines on the N3DV dataset in Table 8 for the sake
of completeness. This is a superset of the Tab. 2 in the main paper. A majority of the works compute
SSIM using the scikit-image implementation, which tends to produce higher values different from
our SSIM implementation similar to MipNeRF [2]. Since the two are not comparable, we exclude
implementation numbers computed with scikit-image. We also only show results for LPIPS on VGG
for the methods that use it. This is consistent with how we compute LPIPS."
E,0.7505330490405118,"Table 8: Quantitative Comparisons on the N3DV [41] and Immersive [4] Datasets. We com-
pare QUEEN against state-of-the-art online and include offline FVV methods for completeness.
3DGStream* refers to our re-implementation on the same NVIDIA A100 GPU as used by QUEEN
for fairness. † is evaluated on the flame salmon scene only. Bold and underlined numbers indicate the
best and the second best results, respectively, within each category."
E,0.7526652452025586,"Type
Method
PSNR
(dB) ↑
SSIM ↑
LPIPS ↓
Storage
(MB) ↓
Training
(sec) ↓
Rendering
(FPS) ↑"
E,0.7547974413646056,Offline
E,0.7569296375266524,"DyNeRF [41]
29.58†
0.961†
0.083†
0.1
15600
0.02
NeRFPlayer [68]
30.69
0.932
0.209
17.1
72
0.05
HyperReel [1]
31.10
0.928
-
1.2
104
2.00
HexPlanes [5]
31.70
-
-
0.8
144
0.21
K-Planes [19]
31.63
-
-
1.0
48
0.15
MixVoxels [76]
31.34
-
-
1.7
16
38
4DGS [88]
32.01
-
-
-
-
114
HexPlanes-4DGS [83]
31.15
-
-
0.3
6
30
SpaceTime [42]
32.05
0.948
-
0.67
20
140"
E,0.7590618336886994,Online
E,0.7611940298507462,"StreamRF [37]
30.68
-
-
31.4
15
8.3
TeTriRF [84]
30.43
0.906
0.248
0.06
39
4
3DGStream [69]
31.67
-
-
7.83
12
215
3DGStream* [69]
31.58
0.941
0.140
7.80
8.5
261"
E,0.7633262260127932,"QUEEN-s
31.89
0.945
0.139
0.68
4.65
345
QUEEN-m
32.03
0.946
0.137
0.69
5.96
321
QUEEN-l
32.19
0.946
0.136
0.75
7.9
248"
E,0.7654584221748401,"A.7
Per-scene Results"
E,0.767590618336887,"In addition to the average quantitative results over the full datasets of N3DV and Immersive in Tab. 8,
we show results for each scene in both the N3DV and Immersive datasets of various frame-wise
metrics, including, PSNR, SSIM, LPIPS, size, training time and rendering time (FPS) in Tables 9
and 10."
E,0.7697228144989339,"A.8
Perceptual quality: User study"
E,0.7718550106609808,"In addition to the extensive quantitative analysis in the paper as well as supplementary, we conduct
an A/B user study to measure the perceptual quality of our video reconstructions. For each vote, we
show a pair of randomly chosen rendering results (from a test view that is not used in training) by
our method and one of the baseline methods (3DGStream [69] and TeTriRF [84]). We also show the
ground truth video as a reference for the participants to make the decision. We ask the participant
to choose the method that more faithfully matches the reference video. In total, we collected 285
responses from 15 participants within the timeline of the rebuttal. For the N3DV dataset, 76.67% of
users preferred our method over 3DGStream and 96.67% preferred our method over TeTriRF. On the
Google Immersive dataset, 97.14% of users preferred the reconstructions from our approach over
that of 3DGStream. This showed that the participants strongly prefer our results in comparison to the
baseline methods for both datasets."
E,0.7739872068230277,"B
Implementation Details"
E,0.7761194029850746,"Training. Our implementation of QUEEN builds on that of [29]. We train the Gaussians for 500 and
350 epochs for first time-step, and for 10 and 15 epochs for the subsequent time-steps, on N3DV and
Immersive, respectively, with each epoch consisting of all training views. We set the SH degree to 2
for N3DV and 3 for Immersive. We set the score vector threshold td = 0.001 for all experiments. We
additionally dilate the image mask by a 48 × 48 kernel to include neighboring image regions while
rendering as larger Gaussians can depend on multiple pixel locations. We perform masked training
for 30% of the iterations for N3DV and 65% for Immersive. We perform masked training for only a
fraction of iterations as updating Gaussians rendered only at masked locations can alter the rendered
pixels at unmasked location as well. We thus allow fine-tuning on the full image to account for any
changes in the unmasked image regions."
E,0.7782515991471215,Table 9: Per-scene Metrics for the N3DV Datasets
E,0.7803837953091685,"Scene
PSNR
(dB) ↑
SSIM ↑
LPIPS ↓
Storage
(MB) ↓
Training
(sec) ↓
Rendering
(FPS) ↑"
E,0.7825159914712153,"Coffee Martini
28.38
0.915
0.155
1.17
7.48
213
Cook Spinach
33.4
0.956
0.134
0.59
8.03
254
Cut Beef
34.01
0.959
0.132
0.57
7.59
291
Sear Steak
33.93
0.962
0.125
0.56
9.31
257
Flame Steak
34.17
0.962
0.126
0.59
7.97
266
Flame Salmon
29.25
0.923
0.145
1.00
7.00
207"
E,0.7846481876332623,"Average
32.19
0.946
0.136
0.75
7.90
248"
E,0.7867803837953091,Table 10: Per-scene Metrics for the Immersive Datasets
E,0.7889125799573561,"Scene
PSNR
(dB) ↑
SSIM ↑
LPIPS ↓
Storage
(MB) ↓
Training
(sec) ↓
Rendering
(FPS) ↑"
E,0.7910447761194029,"Welder
27.03
0.884
0.215
2.18
18.23
146
Flames
30.52
0.925
0.157
1.26
29.54
161
Truck
27.03
0.905
0.195
2.03
17.12
177
Exhibit
28.03
0.903
0.193
2.11
17.81
170
Face Paint 1
31.90
0.950
0.240
1.19
17.48
245
Face Paint 2
30.55
0.937
0.207
2.39
17.49
217
Cave
29.49
0.900
0.250
1.50
20.46
162"
E,0.7931769722814499,"Average
29.22
0.915
0.208
1.79
19.73
183"
E,0.7953091684434968,"Sparse Gating. We learn the parameters α with the Adam optimizer [30]. The position residual
learning rate is set to 0.00016 for N3DV and 0.0005 for Immersive. Other hyperparameters are
provided in Table 11."
E,0.7974413646055437,"Quantization. For both datasets, we set the learning rate of the decoder parameters to be 0.001
for the color and rotation attributes and 0.0001 for opacity and scaling, optimized with the Adam
optimizer. Other hyperparameters are provided in Table 12."
E,0.7995735607675906,"Densification. For N3DV, we perform densification for subsequent frames from epoch 6 until 80%
of the epochs with an interval of 2 epochs and a gradient threshold of 0.00125. For Immersive, we
perform densification on the 8th epoch with a gradient threshold of 0.00125."
E,0.8017057569296375,"Table 11: Gating Hyperparameters
Dataset
LR
λreg
γ0
γ1
τ"
E,0.8038379530916845,"N3DV
0.1
0.01
-0.5
1.01
0.3
Immersive
0.1
0.01
-0.1
1.1
0.5"
E,0.8059701492537313,"First-frame Quantization. Uncompressed Gaussians have large memory costs even for the first
frame. However, quantizing all attributes results in quality degradations [24]. We therefore apply
learnable quantization only on the first frame’s high-frequency spherical harmonic coefficients
(excluding the DC component) using the hyperparameters mentioned above. For example, on N3DV,
we reduce the first frame’s size from 47 to 17 MB with no quality degradation."
E,0.8081023454157783,"Storage. Post-training, we convert the learned parameters to the final compressed form. For the
quantized residuals of rotation, scale, opacity and appearance, we convert the continuous latents Lc
to the integer form and then apply entropy encoding and store this further-compressed representation
Ec as well as the decoder Dc for all four categories c ∈{r, s, o, h}. Our entropy coding approach
flattens our integer latent matrix for each attribute before encoding. For example, for L-dimensional
latent attributes for N gaussians, we flatten the matrix to obtain a vector with L*N elements. This
integer vector is then encoded using standard entropy coding approaches such as arithmetic coding."
E,0.8102345415778252,Table 12: Quantization Hyperparameters
E,0.8123667377398721,"Dataset
Rotation
Scaling
Opacity
Color Base
Color Freq"
E,0.814498933901919,"Latent
Dim
Latent
LR
Latent
Dim
Latent
LR
Latent
Dim
Latent
LR
Latent
Dim
Latent
LR
Latent
Dim
Latent
LR"
E,0.8166311300639659,"N3DV
6
0.025
8
0.01
3
0.05
8
0.0125
4
0.000625
Immersive
6
0.015
8
0.007
3
0.05
8
0.0125
12
0.000375"
E,0.8187633262260128,"The number of bits for storing each attribute residual matrix is therefore dependent on the scene
content as it relies on the amount of motion. This number can be fractional, on average, which is the
standard for the entropy coding algorithm of arithmetic coding or Huffman coding [33]. For example,
for the Sear Steak scene in the N3DV dataset, on average, we require 0.68 bits for all the quantized
attributes (corresponding to 0.5MB/frame). This depends on the entropy of the latents itself, which
varies with changing scene motion (Figure 9)."
E,0.8208955223880597,"For the sparse gates, we store the positional residuals as a sparse matrix with the indices from binarized
gate variables I = {i|gi ̸= 0}, and the full-precision residual vectors only if its corresponding gate
is on, Ep = {lpi|i ∈I}. Both operations add a negligible computation overhead. This corresponds
to the coordinate format (COO) for storing sparse matrices where the non-zero values are stored in
FP-32 precision along with their integer index locations."
E,0.8230277185501066,"B.1
Sensitivity Analysis on Hyperparameter"
E,0.8251599147121536,Table 13: Sensitivity to different hyperparameter configurations on the N3DV dataset.
E,0.8272921108742004,"Method
PSNR
Size (MB)
Ours (N3DV hyperparam.)
32.14
0.60
Ours (Immersive hyperparam.)
32.06
1.49
3DGStream [69]
31.58
7.80"
E,0.8294243070362474,"We set different hyperparameters for the two datasets in in Tables 11 and 12 to account for the widely
varying amount of scene motion between N3DV and Immersive datasets. The Immersive dataset
contains larger and more complex scene motions (e.g., a person entering and leaving the scene)
while N3DV contains relatively minor motions. We found that a higher learning rate for the position
residuals allows Gaussians to adapt to the highly dynamic scenes. The gating hyperparameters
in Table 11 for N3DV are set to utilize this prior information about the dataset where the stretch
hyperparameters γ0 and γ1 are set closer to 0 to enforce more sparsity in the position residuals.
Additionally, the Immersive dataset itself consists of a wide variety of indoor/outdoor scenes at
varying scales/scene motion/illumination. We use the same set of hyperparameters for each scene
achieving good reconstruction quality for every scene (Table 10) showing its generalization capability."
E,0.8315565031982942,"Tables 11 and 12 list out the different hyperparameters for quantizing and sparsifying residuals for both
N3DV and Immersive datasets. To test the sensitivity of reconstruction quality to hyperparameters,
we train our method on the N3DV dataset with two sets of hyperparameters. The first configuration
uses the stated hyperparameters for N3DV from Tables 11 and 12, while the second utilizes the
hyperparameters corresponding to Immersive while also matching the learning rate for the position
residuals (0.0005). We show results on N3DV datasets in Table 13. We see that the Immersive
dataset’s hyperparameter configuration still achieves similar PSNR as the original hyperparameters
for N3DV. While the model size is higher (1.49 MB) with the Immersive configuration compared to
the original configuration (0.60 MB), it is still much lower than the prior state-of-the-art 3DGStream
(7.8 MB) while maintaining higher reconstruction quality in terms of PSNR."
E,0.8336886993603412,"B.2
Evaluation"
E,0.835820895522388,"Datasets. (1) Neural 3D Video (N3DV) Datasets [41] consist of six indoor scenes with forward-
facing multiview videos with up to 20 cameras at 2704 × 2028 resolution. Similar to prior work, we
downsample videos by a factor of 2 for training and testing, holding out the central view for testing.
Each video consists of 300 frames at 30 FPS. (2) Immersive Video Datasets [4] consist of light field
videos of indoor and outdoor scenes captured using a 46-camera rig with fisheye lenses. Following"
E,0.837953091684435,"prior work, we downsample videos by a factor of 2 to obtain a resolution of 1280 × 960. We evaluate
on 7 scenes (Welder, Flames, Truck, Exhibit, Face Paint 1, Face Paint 2 and Cave) with the central
view held out for testing. We extract the first 300 frames for all scenes except for Truck, which
consists of 150 frames. We undistort the fisheye views into perspective views using the distortion
parameters. We train and evaluate on the perspective views with pinhole camera parameters."
E,0.8400852878464818,"Baselines. 3DGStream [69]: We use the official codebase4 from 3DGStream [69]. We use the
same default configuration for N3DV as provided by the authors. For Immersive, we reduce the
gradient threshold for densification to 0.0075 to allow for more Gaussians while increasing the
training iterations for Stage 1 and 2 to be 450 and 250 iterations, respectively. TeTriRF [84]: We use
the official codebase5 from TeTriRF [84] for all experiments on N3DV."
E,0.8422174840085288,"Measuring FPS. We compute FPS for all frames of the video and report the median value in our
experiments including the time taken to decode the residuals. Note that the decoding is a one-time
operation per step and rendering for a preceding frame can be performed while learning the residuals
of a subsequent ones to achieve even higher speeds."
E,0.8443496801705757,"C
Limitations and Future Work"
E,0.8464818763326226,"For efficiency and on-the-fly training, we encode sequences by learning inter-frame residuals. How-
ever, for FVVs of long duration or drastic scene update, per-frame training will face challenges in
reconstruction capability. Unlike offline reconstruction, per-frame training does not have access to
future-frame information. This setup limits the capability to effectively reason about large scene
changes (e.g., topological changes and highly varying appearance) [56]. If an object suddenly ap-
pears or disappears, it is more difficult to (de-)allocate and update scene parameters to capture such
changes. In the context of Gaussian splatting, it is challenging to schedule densification and pruning
of the Gaussians. Future work could address this by designing an efficient keyframing technique for
identifying large changes in the scene and allocate longer training times accordingly."
E,0.8486140724946695,"Furthermore, most current FVV encoding paradigms rely on the input multi-view videos for re-
construction. Exploring a general prior of the dynamic scenes, e.g., generative video models, is a
promising direction for reducing the dependence on coherent multi-view input, as the video prior
could regularize the reconstructed FVV to capture reasonable scene dynamics even if some input
views or frames are missing. Moreover, extending our approach to a single or sparse view scenario is
a challenging yet important problem for further democratizing streamable FVV. We will leave these
directions for future work."
E,0.8507462686567164,"D
Broader Impacts"
E,0.8528784648187633,"We consider our work as a neutral technology. This proposed method reconstructs free-viewpoint
videos from user-provided video inputs. As we highlighted in the introduction, this technology can
improve many aspect of people’s lives, such as through healthcare (tele-operation) and communica-
tions (3D video conferencing). There is indeed a possibility that this work can be misused. Since our
reconstruction completely relies on the video inputs, the most likely cases of misuse are those where
the input video (provided by the users) have negative impacts."
E,0.8550106609808102,"4https://github.com/SJoJoK/3DGStream
5https://github.com/wuminye/TeTriRF"
E,0.8571428571428571,NeurIPS Paper Checklist
CLAIMS,0.8592750533049041,1. Claims
CLAIMS,0.8614072494669509,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We show extensive experiments and ablations on multiple datasets which are
widely used in the area. Our claims accurately reflect the contribution and scope of our
work.
Guidelines:"
CLAIMS,0.8635394456289979,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8656716417910447,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss limitations of our work in our conclusions section. We add further
detail on several limitations of our work in Appendix C.
Guidelines:"
CLAIMS,0.8678038379530917,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.8699360341151386,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
CLAIMS,0.8720682302771855,"Answer: [NA]
Justification: Our work is not a theory work. All equations for the various components
of our work are explained in detail in Sec. 3, where we cited appropriate sources for their
theoretical work in the related work and the method sections.
Guidelines:"
CLAIMS,0.8742004264392325,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
CLAIMS,0.8763326226012793,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide extensive explanations of each component of our work in detail
in Sec. 3. Additional implementation details with corresponding hyperparameters are also
provided in Appendix B.
Guidelines:"
CLAIMS,0.8784648187633263,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8805970149253731,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8827292110874201,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: We aim to release the code in the future.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.8848614072494669,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
OPEN ACCESS TO DATA AND CODE,0.8869936034115139,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We follow benchmark and evaluation protocols that are widely used by existing
work in the area. Additional hyperparameters and experiment details are provided in the
supplementary materials B.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.8891257995735607,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
OPEN ACCESS TO DATA AND CODE,0.8912579957356077,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: We follow benchmark and evaluation protocols that are widely used by existing
work in the area. To our knowledge, most of the existing work in this area do not provide
statistical significance.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.8933901918976546,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
OPEN ACCESS TO DATA AND CODE,0.8955223880597015,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.8976545842217484,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.8997867803837953,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9019189765458422,"Answer: [Yes]
Justification: We provide details including computational resources (hardware), training
times and dataset specifications in the implementation details in the main paper and the
appendix."
EXPERIMENTS COMPUTE RESOURCES,0.9040511727078892,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.906183368869936,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.908315565031983,9. Code Of Ethics
CODE OF ETHICS,0.9104477611940298,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9125799573560768,"Answer: [Yes]
Justification: Yes, the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics."
CODE OF ETHICS,0.9147121535181236,Guidelines:
CODE OF ETHICS,0.9168443496801706,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9189765458422174,10. Broader Impacts
BROADER IMPACTS,0.9211087420042644,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9232409381663113,Answer: [Yes]
BROADER IMPACTS,0.9253731343283582,Justification: Please see Appendix D where we discuss the potential societal impacts.
BROADER IMPACTS,0.9275053304904051,Guidelines:
BROADER IMPACTS,0.929637526652452,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9317697228144989,11. Safeguards
SAFEGUARDS,0.9339019189765458,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9360341151385928,Answer: [NA]
SAFEGUARDS,0.9381663113006397,Justification: The paper poses no such risks.
SAFEGUARDS,0.9402985074626866,Guidelines:
SAFEGUARDS,0.9424307036247335,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9445628997867804,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9466950959488273,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9488272921108742,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9509594882729211,"Justification: Our method is evaluated on several datasets. We followed their license and we
have credited and cited their work and datasets."
LICENSES FOR EXISTING ASSETS,0.9530916844349681,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9552238805970149,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset."
LICENSES FOR EXISTING ASSETS,0.9573560767590619,"• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9594882729211087,13. New Assets
NEW ASSETS,0.9616204690831557,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9637526652452025,Answer: [NA] .
NEW ASSETS,0.9658848614072495,Justification: N/A.
NEW ASSETS,0.9680170575692963,Guidelines:
NEW ASSETS,0.9701492537313433,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722814498933902,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744136460554371,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976545842217484,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786780383795309,Justification: N/A.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808102345415778,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9829424307036247,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850746268656716,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872068230277186,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893390191897654,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914712153518124,Justification: N/A.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936034115138592,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957356076759062,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997867803837953,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
