Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0037593984962406013,"Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when
synthesizing novel views of time-evolving 3D scenes. However, the common
reliance on backward deformation fields makes reanimation of the captured object
poses challenging. Moreover, the state of the art dynamic models are often limited
by low visual fidelity, long reconstruction time or specificity to narrow application
domains. In this paper, we present a novel method utilizing a point-based represen-
tation and Linear Blend Skinning (LBS) to jointly learn a Dynamic NeRF and an
associated skeletal model from even sparse multi-view video. Our forward-warping
approach achieves state-of-the-art visual fidelity when synthesizing novel views and
poses while significantly reducing the necessary learning time when compared to
existing work. We demonstrate the versatility of our representation on a variety of
articulated objects from common datasets and obtain reposable 3D reconstructions
without the need of object-specific skeletal templates. The project website can be
found at https://lukas.uzolas.com/Articulated-Point-NeRF/."
INTRODUCTION,0.007518796992481203,"1
Introduction"
INTRODUCTION,0.011278195488721804,"Synthesizing novel photo-realistic views of captured 3D scenes is important for many domains includ-
ing virtual/augmented reality, video games or movie productions. In recent years, Neural Radiance
Fields (NeRFs) [1] have proved their remarkable capacity to represent complex view-dependent
effects, captured in photographs and videos, sparsely sampled from natural light fields [2]. Follow-up
works have extended the scope to dynamic scenes [3‚Äì10], facilitating rendering of unseen views at
different timestamps. Despite progress in reconstruction quality and speed [11], manipulating learned
scenes remains a challenge, but would be a highly desirable feature, as it can enable downstream
applications, such as the creation of avatars for virtual presence or 3D assets for games and movies."
INTRODUCTION,0.015037593984962405,"The key challenge of reposing a NeRF is inverting the backward-warping function that maps individual
observations to a shared canonical representation [3]. It is an inversion of traditional kinematic
animation, such as Linear Blend Skinning (LBS) [12], where a canonical shape is forward-warped to
a desired pose. Such inverse mapping often requires resolving ambiguous situations as it is difficult
to guarantee bijectivity (see Fig. 2). A common remedy are parametric templates, typically built
for narrow application domains, such as human heads and bodies [13‚Äì16], but they are difficult to
generalize. Alternatively, object shapes can be retrieved from videos as ensembles of geometric parts,
yet existing techniques provide limited image-synthesis fidelity [17‚Äì19]. Our work aims to combine
these different lines of work and enable joint learning of the NeRF representation and its pose
parameterization from sparse or dense multi-view videos. We aim for time-efficient class-agnostic
view synthesis of reposable models with high image-synthesis quality without access to a template or
pose annotation, which is a combination not currently covered by existing work (see Table 1)."
INTRODUCTION,0.018796992481203006,"NeRF
Dense Sampling"
INTRODUCTION,0.022556390977443608,Pose Regressor
INTRODUCTION,0.02631578947368421,Kinematic Warp
INTRODUCTION,0.03007518796992481,"Extracted 
Skeleton
Learned Skinning 
Weights
Feature 
Point Cloud"
INTRODUCTION,0.03383458646616541,"

"
INTRODUCTION,0.03759398496240601,"Rendered Image and Weights Rt
-1 c, œÉ"
INTRODUCTION,0.041353383458646614,"Color and Density 
Regressor"
INTRODUCTION,0.045112781954887216,"Œ≥(x)
t
{Rt , tt }"
INTRODUCTION,0.04887218045112782,"Pc
wi
J p1 p2
p3 p4"
INTRODUCTION,0.05263157894736842,"Figure 1: Overview of our method: a) First, we pre-train a NeRF backbone to initialize a feature
point cloud P c for a selected canonical timestamp and to extract an initial skeleton. b) During the
main training stage, P c is forward-warped using LBS consisting of learned time-invariant skinning
weights ÀÜwi and time-dependent pose transformations from an MLP regressor Œ¶r. The image is
obtained by integration and decoding of features aggregated from points along each camera ray. In
summary, we fine-tune the initial neural point features fi, skinning weights ÀÜwi, joints J, density and
color regressor Œ¶d and Œ¶c of the backbone. We fully train the pose regressor Œ¶r and feature point
decoder Œ¶p from scratch. In test time, we modify the pose transformations to obtain novel poses."
INTRODUCTION,0.05639097744360902,"Observation 
space
Canonical
space
Observation 
space
Canonical
space A"
INTRODUCTION,0.06015037593984962,"(a)
(b)"
INTRODUCTION,0.06390977443609022,"D
B C
A
D ? B
C"
INTRODUCTION,0.06766917293233082,"Figure 2: Examples of poses difficult for back-
ward warping. (a) Ill-defined projection from
an observation to the canonical space. Both am-
biguous solutions (green and magenta) correctly
pass through the semantically corresponding
surface points B and C. (b) Projection ambigu-
ity for points of contact between two surfaces.
Note that in contrast, it is trivial to forward warp
the object points from a well-chosen canonical
space to any observation space."
INTRODUCTION,0.07142857142857142,"Method
Pose Generic Fildelity Training"
INTRODUCTION,0.07518796992481203,"LASR [17]
No
Yes
Shape
‚â§2 h
ViSER [18]
No
Yes
Shape
2‚Äì12 h
BANMO [22]
No
Yes
Low
‚â•12 h
D-NeRF [3]
No
Yes
High
‚â•12 h
TiNeuVox [11] No
Yes
High
‚â§2 h"
INTRODUCTION,0.07894736842105263,"CASA [23]
Yes
No
Shape
‚â§2 h
LASSIE [19]
Yes
Yes
Low
‚â•12 h
WIM [20]
Yes
Yes
High
‚â•12 h
Ours
Yes
Yes
High
‚â§2 h
Table 1: A comparison of representative dy-
namic 3D representations learned from 2D
videos. We analyze reposibility, agnosticism to
object class, image synthesis fidelity and train-
ing time. Papers that demonstrate reposing are
shown below the bar. ‚ÄúShape‚Äù denotes shape-
only reconstruction without texture details."
INTRODUCTION,0.08270676691729323,"Recent work by Noguchi et al. [20] addresses a similar problem. However, we exploit the structure-
free nature of point-based NeRF representations [21], enabling direct forward warping of the canonical
object to any desired pose, while maintaining the capacity and flexibility of NeRF-like rendering
to capture highly detailed image features. Together with our automatically extracted and jointly-
optimized LBS skeletal pose parameterization, our work allows for faster training and achieves
state-of-the-art visual quality in a much shorter time. Finally, we demonstrate how to easily repose
learned representations of varied objects by directly editing joint angles of the forward LBS model."
INTRODUCTION,0.08646616541353383,"To summarize, we make the following contributions: 1) We propose a novel method for learning
articulated neural representations of dynamic objects using forward projection of neural point clouds.
2) We demonstrate state-of-the-art novel view synthesis for a variety of multi-view videos with
training times lower than comparable methods. 3) We jointly learn a skeletal model without domain-
specific priors or user annotations and demonstrate reposing of the reconstructed 3D objects."
RELATED WORK,0.09022556390977443,"2
Related Work"
RELATED WORK,0.09398496240601503,"Neural Radiance Fields and Parameterizations
Implicit neural networks have recently emerged
as a powerful tool for building differentiable representations of 3D scenes [24‚Äì47] and for learning
view-dependent Neural Radiance Fields (NeRFs) [1, 2, 48‚Äì57] enabling photo-realistic novel view
synthesis. Their high computational cost has been dramatically reduced by spatial decomposition [58‚Äì
60] and hybrid representations [61‚Äì65].Recently, neural point clouds have combined expressive
neural functions with the flexibility of explicit geometry [21, 66‚Äì69]. We also use neural point clouds,
but unlike previous work, we jointly learn a forward kinematic model along with the associated soft
part segmentation and, thus, convert fast voxel-based representations to reposable NeRFs."
RELATED WORK,0.09774436090225563,"Dynamic Neural Radiance Fields
Equivalent to 2D video, dynamic NeRFs capture temporal
changes in scenes by encoding each frame separately [70], expanding the radiance field into the
temporal domain [7, 8, 71, 72], or time-dependent interpolation in a compact latent representation [6,
9, 10]. Alternatively, a single canonical NeRF can be animated by backward warping from the
canonical space to individual time-varying observation spaces [3‚Äì6, 9, 11]. However, such warps rely
on the bijectivity of the mapping which is difficult to guarantee for all observed poses (see Fig. 2).
Forward mapping only requires a single well-posed canonical representation, which we exploit."
RELATED WORK,0.10150375939849623,"Object Reposing
Directly reposing high-dimensional deformation fields of dynamic NeRFs is
impractical. Instead, parametric templates can sparsely represent prominent deformation modes for
faces [13, 14], bodies [15, 16], hands [73], and also non-human objects, such as animals [74]. Together
with skeleton-based LBS, they enable articulated neural representations of human heads [69, 75‚Äì81]
or bodies [82‚Äì97], as well as modeling distributions in generative models [98‚Äì101]. However, the
assumption of piece-wise rigid motion and the availability of a skeletal template restrict the applicable
object classes. The former issue has been addressed by physically inspired but computationally
expensive deformation models [102]. To remedy the latter issue, the skeletal model can be retrieved
from a database [23], adapted from a generic graph [19, 103], fitted as a template to 2D observa-
tions [104‚Äì108] or fully learned during a 3D shape recovery [20, 109]. An external cage can also
be used for re-animation of static NeRFs [110]. Alternatively, a surface embedding can be learned
without a skeleton if reposing is not required [17, 18, 22]."
RELATED WORK,0.10526315789473684,"Similarly to Noguchi et al. [20], we jointly learn a 3D object representation, a skeletal model,
and observed poses. However, we utilize a point-based NeRF representation to benefit from its
computational efficiency, ease of geometric transformation, and capacity to learn complex light fields."
RELATED WORK,0.10902255639097744,"We share our point-based approach with a contemporaneous work MovingParts [111] which, however,
relies on inverting the backward flow. We demonstrate the reconstruction of large transformations
in the Robots dataset [20], not demonstrated in MovingParts. Furthermore, our approach does not
enforce binary segmentation and, hence, can represent non-rigid part transitions (see Fig. 6 right)."
PRELIMINARIES,0.11278195488721804,"3
Preliminaries"
PRELIMINARIES,0.11654135338345864,"Our method builds upon available NeRF reconstruction methods as a backbone for learning a
reposable dynamic representation. In principle, any NeRF-like representation is a suitable initial point
for our training. In practice, we use TiNeuVox [11] in all experiments, as it enables fast reconstruction
with both sparse and dense multi-view supervision in dynamic scenes. Next, we describe the core
concepts of NeRF [1] and TiNeuVox [11] to provide context for our method in Sec. 4."
PRELIMINARIES,0.12030075187969924,"Time-Aware Neural Voxels
NeRF [1] maps a point p = (x, y, z) and view direction d = (Œ∏, œï) to
color c = (r, g, b) and density œÉ. The mapping function often takes form of a Multi-Layer-Perceptron
(MLP) Œ¶, such that (c, œÉ) = Œ¶(p, d). The color of an image pixel C(r) can be obtained via volume
rendering along the ray r(t) = o + td."
PRELIMINARIES,0.12406015037593984,"TiNeuVox [11] expands NeRF to dynamic scenes and significantly increases training speed by
utilizing an explicit volume representation. To this extent, each point p at time t in observation space
is backward-warped to canonical space as"
PRELIMINARIES,0.12781954887218044,"pc = Œ¶b(ÀÜp,ÀÜt), where ÀÜp = Œ≥(p), ÀÜt = Œ¶t(Œ≥(t)),
(1)"
PRELIMINARIES,0.13157894736842105,"Here, Œ≥ is the positional encoding [1], Œ¶t a small color-decoding MLP, Œ¶b a backward-warping
MLP, and ÀÜp and ÀÜt the encoded point and time respectively. The canonical point pc is used to
sample a feature vector vm ‚ààRV from the explicit feature volume V ‚ààRX√óY √óZ√óV by means
of trilinear interpolation interp with varying stride sm at each scale: vm = Œ≥(interp(pc, V, sm)).
The feature vectors across all scales are concatenated as v = v1 ‚äï... ‚äïvm ‚äï... ‚äïvM, where ‚äïis
the concatenation operator. Finally, the view-dependent color and density are regressed by additional
MLPs Œ¶f, Œ¶d and Œ¶c:"
PRELIMINARIES,0.13533834586466165,"f = Œ¶f(v, ÀÜp,ÀÜt), where œÉ = Œ¶d(f), c = Œ¶c(f, d).
(2)"
PRELIMINARIES,0.13909774436090225,The volume rendering equation [112] integrates density/color of points pi sampled along the ray r:
PRELIMINARIES,0.14285714285714285,"ÀÜC(r) = N
X"
PRELIMINARIES,0.14661654135338345,"i=1
Ti(1 ‚àíexp(‚àíœÉiŒ¥i))ci, where Ti = exp(‚àí i‚àí1
X"
PRELIMINARIES,0.15037593984962405,"j=1
œÉjŒ¥j)),
(3)"
PRELIMINARIES,0.15413533834586465,"Here, Œ¥i is the distance between pi and pi+1. The feature volume V and all MLPs are optimized
end-to-end and supervised by the Mean Squared Error (MSE) and the corresponding ground-truth
pixel color C(r): Lphoto = || ÀÜC(r) ‚àíC(r)||2
2."
PRELIMINARIES,0.15789473684210525,"Note that because Œ¶f (Eq. 2) is conditioned by the time ÀÜt, space and time are entangled and the
feature volume V does not represent a single static canonical shape. Therefore, we cannot directly
learn a forward warping function to invert Œ¶b as proposed by Chen et al. [102]. In the next section,
we show that this is not necessary and we can learn a forward kinematic model directly without
relying on specific properties of the backbone, which will allow us to replace it in the future."
METHOD,0.16165413533834586,"4
Method"
METHOD,0.16541353383458646,"While dynamic NeRF models such as TiNeuVox [11] can reproduce motion in a dynamic scene,
they do not enable reposing. Furthermore, it is impractical to post hoc reparametrize their motion
representation due to the ambiguities of backward flow inversion (see Fig. 2)."
METHOD,0.16917293233082706,"We instead propose a completely new representation based on a neural point cloud and an explicit
kinematic model for forward warping, and we use the backbone only as an initialization for our
training procedure (see Fig. 1). First, we extract a feature point cloud from a selected canonical
frame of the backbone Sec. 4.1. Second, we describe the underlying kinematic model of our 3D
representation and its initialization Sec. 4.2. Consecutively, we introduce how the 3D point cloud is
warped from a canonical space to an observation space and rendered Sec. 4.3. Lastly, we describe our
losses Sec. 4.4."
CANONICAL FEATURE POINT CLOUD,0.17293233082706766,"4.1
Canonical Feature Point Cloud"
CANONICAL FEATURE POINT CLOUD,0.17669172932330826,"We first pre-train a backbone NeRF model (TiNeuVox [11]) using its default parameters. To initialize
the feature point cloud, we sample the canonical density function of TiNeuVox œÉ = Œ¶d(f) (Eq. 2)
on a uniform coordinate grid and discard empty samples through thresholding. The grid resolution
is adaptively chosen, such that |P| ‚âà10000, similarly to other explicit NeRFs [11, 64, 113]. After
thresholding, we obtain points P = {pi|i = 1, ..., N}. Furthermore, we extract a feature vector fi
for each point pi (see Eq. 2)."
KINEMATIC MODEL,0.18045112781954886,"4.2
Kinematic Model"
KINEMATIC MODEL,0.18421052631578946,"We forward-warp our feature point cloud from canonical space to observation space using an LBS
kinematic model to match the training images. Here, we describe how we initialize, use, and simplify
our kinematic model."
KINEMATIC MODEL,0.18796992481203006,"Skeleton Initialization
We do not rely on a class-specific template to initialize our kinematic
skeleton. Instead, we extract an approximate initial skeleton tree by Medial Axis Transform (MAT)
and refine it during training."
KINEMATIC MODEL,0.19172932330827067,"Let M = {pm|m = 1, ..., M} be the set of medial axis points extracted by applying a MAT on P c.
We choose the root of our kinematic model proot as the medial axis point that is closest to all other
medial-axis points, i.e., proot := arg min
i P"
KINEMATIC MODEL,0.19548872180451127,"j ||pi ‚àípj||2, ‚àÄpi ‚ààM, i Ã∏= j."
KINEMATIC MODEL,0.19924812030075187,"Next, we leverage dense sampling-grid neighborhoods and define a graph GM connecting neighboring
points in M. Points disconnected from proot are removed. We then use a heuristic to select sparse
joints (nodes) as a subset of GM and define the bones (edges) based on their connectivity. To this
extent, we traverse GM starting from proot in a breadth-first manner and mark pm as a joint jb if its
geodesic distance from the preceding joint exceeds a threshold Blength = 10. As a result, we obtain
a set of time-invariant canonical joint positions J = {jb} which we further optimize during training."
KINEMATIC MODEL,0.20300751879699247,"While this skeleton is usually over-segmented, we show in our experiments that this does not hamper
the training. We propose a pruning strategy to enable easier pose manipulation afterwards (see Fig. 7)."
KINEMATIC MODEL,0.20676691729323307,"Blend Skinning Weights
For each point pi we initialize its raw skinning weight vector ÀÜwi by an
exponential decay function of the distance dist to each bone line bj such that ÀÜwi,j = 1/edist(pc
i ,bj).
Before skinning, we obtain the final blend skinning weight vector wi through scaling by a global
learnable parameter Œ± and applying a softmax: wi,j = e ÀÜ
wi,j /Œ±/P"
KINEMATIC MODEL,0.21052631578947367,"k e ÀÜ
wi,k/Œ±. During the training, we
optimize the initial ÀÜwi as well as Œ±. Accounting for the per-point weights, we define our full canonical
feature point cloud as P c = {(pc
i, fi, ÀÜwi)|i = 1, ..., N}."
KINEMATIC MODEL,0.21428571428571427,"Point Warping
We forward-warp the canonical point cloud P c to an observation space of timestamp
t via LBS [12]. The local transformation matrix ÀÜT t
b of each bone b is defined by a rotation Rt
b around
its parent joint jb. Consequently, each point pc
i is transformed by a linear combination of bone
transformations as:"
KINEMATIC MODEL,0.21804511278195488,"pt
i = ¬ØT t
i pc
i = |B|
X"
KINEMATIC MODEL,0.22180451127819548,"b=1
wi,bT t
bpc
i, with T t
b = T t
pb ÀÜT t
b and ÀÜT t
b =

Rt
b
jb + Rt
bj‚àí1
b
0
1"
KINEMATIC MODEL,0.22556390977443608,"
,
(4)"
KINEMATIC MODEL,0.22932330827067668,"where T t
b is defined recursively by its parent bone pb. T t
pb of the skeleton root is identity."
KINEMATIC MODEL,0.23308270676691728,"We express rotations Rt
b ‚ààSO(3) using the Rodrigues‚Äô formula, where ÀÜr = r/||r|| is the axis of
rotation. However, we learn the rotation angle Œ∏ directly as an additional parameters because we
find it leads to a better pose regression than using Œ∏ = ||r||. The time-dependent rotations rb, Œ∏b for
each bone b are regressed by an MLP: Œ¶r(Œ≥(t)) = rt
1, Œ∏t
1, ..., rt
b, Œ∏t
b, ..., rt
B, Œ∏B, rt
r, Œ∏t
r, tt
r, where rt
r,
Œ∏t
r and tt
r are the time-dependent root rotation and translation."
KINEMATIC MODEL,0.23684210526315788,"Skeleton simplification
The initial skeleton‚Äôs over-segmentation does not hamper pose recon-
struction during training, but we optionally simplify the skeleton after training to ease pose editing
(see Fig. 7). We prune or merge bones based on the size of rotation angles Œ∏t
b produced by the
transformation regressor Œ¶r. Joints, which do not exhibit a rotational change from the rest pose above
the threshold of tr deg in more than 5% of the observed timestamps, are marked static. We then
merge the bones of such joints and their corresponding weights ÀÜw. See the supplement for details."
DYNAMIC POINT RENDERING,0.24060150375939848,"4.3
Dynamic Point Rendering"
DYNAMIC POINT RENDERING,0.24436090225563908,"We adopt the point cloud rendering from [21] but extend it to explicitly model rotational invariance
of the radiance field. For each sampling point x, we find up to N = 8 nearest observation feature
points pt
i within a radius of 0.01 and roto-translate them into their canonical frames. This enables the
feature embedding MLP Œ¶p to learn spatial relations in a pose-invariant coordinate frame as:"
DYNAMIC POINT RENDERING,0.24812030075187969,"f t
i,x = Œ¶p(fi, xt
pi), with xt
pi = Œ≥(Rt‚àí1
i
(x ‚àípt
i)),
(5)"
DYNAMIC POINT RENDERING,0.2518796992481203,"where, Rt
i is the rotation component ¬ØT t
i (Eq. 4) for point pt
i and Œ≥(.) is the positional encoding [1]."
DYNAMIC POINT RENDERING,0.2556390977443609,"The neighboring embeddings f t
i,x are aggregated by inverse distance weighting, which produces the
final feature input for Œ¶d and Œ¶c (see Eq. 2) and the consequent volume rendering (Eq. 3):"
DYNAMIC POINT RENDERING,0.2593984962406015,"f t
x = N
X i"
DYNAMIC POINT RENDERING,0.2631578947368421,"di
P|B|
j
dj
f t
i,x, with di = ||pt
i ‚àíx||‚àí1; œÉ = Œ¶d(f t
x); c = Œ¶c(f t
x, d).
(6)"
LOSSES,0.2669172932330827,"4.4
Losses"
LOSSES,0.2706766917293233,"Next to the photometric loss Lphoto (Sec. 3), we utilize a 2D chamfer loss to penalize the difference
between the point cloud projected into a training view I(P t) and the corresponding 2D ground truth"
DB,0.2744360902255639,"23.3 dB
29.4 dB
26.1 dB
32.4 dB"
DB,0.2781954887218045,"21.8 dB
26.8 dB
24.0 dB
28.7 dB"
DB,0.2819548872180451,"21.2 dB
27.5 dB
22.5 dB
28.6 dB"
DB,0.2857142857142857,"Ground Truth
2 hour training
Watch-It-Move
Ours"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.2894736842105263,"10 hour training
Watch-It-Move
Ours"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.2932330827067669,"Figure 3: Qualitative comparison displaying two held-out views-frames of scenes from the Robots
rendered by WIM [20] and our method after 2 and 10 hours of training, and the PSNR scores."
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.29699248120300753,"object mask M t: Lmask := Lchamf(I(P t), M t). The chamfer loss is defined as:"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.3007518796992481,"Lchamf(P 1, P 2) =
1
|P 1|"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.30451127819548873,"|P 1|
X"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.3082706766917293,"i
min
j
||p1
i ‚àíp2
j||2
2 +
1
|P 2|"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.31203007518796994,"|P 2|
X"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.3157894736842105,"j
min
i
||p1
i ‚àíp2
j||2
2.
(7)"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.31954887218045114,"To prevent the skeleton from diverging too much from the medial axis M, we further minimize the
chamfer loss between M and the joints J (see Sec. 4.2): Lskel := Lchamf(M, J). In addition,
we minimize the transformation angles and the root translation: Ltranf = (P"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.3233082706766917,"i |Œ∏t
i|) + |tt
r|. Local
rigidity is further enforced upon points after deformation via as-rigid-as-possible regularization:"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.32706766917293234,Larap =
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.3308270676691729,"|P t|
X i"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.33458646616541354,"N(pi)
X"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.3383458646616541,"j
|||pc
i ‚àípc
j||2
2 ‚àí||pt
i ‚àípt
j||2
2|.
(8)"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.34210526315789475,"Lastly, we apply two regularizers on the blend skinning weights.
To encourage smoothness,
we penalize divergence of skinning weights in the rendering neighborhood N: Lsmooth =
P|P t|
i
P"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.3458646616541353,"j‚ààN(pi) |wi ‚àíwj|, and, to encourage sparsity, we apply:"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.34962406015037595,Lsparse = ‚àí
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.3533834586466165,"|P c|
X i B
X"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.35714285714285715,"j
wi,j log(wi,j) + (1 ‚àíwi,j) log(1 ‚àíwi,j).
(9)"
"HOUR TRAINING
WATCH-IT-MOVE
OURS",0.3609022556390977,"In total, our training loss is L = œâ0Lphoto + œâ1Lmask + œâ2Lskel + œâ3Ltranf + œâ4Lsmooth +
œâ5Lsparse + œâ6LARAP where œâ = {200, 0.02, 1, 0.1, 10, 0.2, 0.005} in our experiments."
EXPERIMENTS,0.36466165413533835,"5
Experiments"
EXPERIMENTS,0.3684210526315789,"Here, we evaluate our work, which obtains state-of-the-art view-synthesis quality with a lower
training cost than other articulated methods. We also demonstrate class-agnostic reposing capability
and we evaluate contribution of method components in an ablation study. Video examples can be
seen on the project website."
EXPERIMENTS,0.37218045112781956,"Datasets
We chose three multi-view video datasets that are commonly used for the evaluation of
dynamic multi-view synthesis methods and pose articulation. First, the Robots dataset [20] features
multi-view synthetic videos of 8 topologically varied robots, making it well suited for testing pose
articulation performance (see Fig. 4). We use 18 views for training and 2 for evaluation. Second, the
Blender dataset [3] is a sparse multi-view synthetic dataset with 5 humanoid and 2 other articulated
objects1. Its visual quality benchmarks are used to test the fidelity of image detail reconstruction (see"
EXPERIMENTS,0.37593984962406013,1We do not use the multi-component Bouncing balls scene.
EXPERIMENTS,0.37969924812030076,"WIM
Ours"
EXPERIMENTS,0.38345864661654133,PSNR [dB] LPIPS
EXPERIMENTS,0.38721804511278196,"Training time [hours]
Training time [hours]"
EXPERIMENTS,0.39097744360902253,"Time Method
PSNR‚ÜëSSIM‚ÜëLPIPS‚Üì"
H,0.39473684210526316,"2 h
WIM [20]
25.05
0.952
0.059
Ours
28.81
0.968
0.047"
H,0.39849624060150374,"10 h
WIM [20]
27.61
0.964
0.046
Ours
30.19
0.973
0.041"
H,0.40225563909774437,"Figure 4: Quality of unseen view synthesis during training with 95% confidence intervals in the
Robots dataset [20]. The initial plateau of WIM [20] matches the 10k warm-up steps used by the
authors before training with all data. Our onset time corresponds to the 70 minutes required for
pre-training of the backbone. Training of our method was terminated after 2.5 hours."
H,0.40601503759398494,"Method
Reposable
PSNR‚Üë
SSIM‚Üë
LPIPS‚Üì
Training Time"
H,0.40977443609022557,"D-NeRF [3]
No
30.50
0.95
0.07
20 hours
TiNeuVox-B [11]
No
32.67
0.97
0.04
28 mins
Hexplane [72]
No
31.04
0.97
0.04
11.5 mins
K-Plane hybrid [71]
No
31.61
0.97
-
52min"
H,0.41353383458646614,"WIM‚àó[20]
Yes
23.81
0.91
0.10
11 hours
Ours‚àó
Yes
29.10
0.94
0.06
2.5 hours
Table 2: Quality of unseen view synthesis for the Blender dataset [3]. Results of D-NeRF and
TiNeuVox-B reprinted from Fang et al. [11]. ‚àóWithout the ‚ÄúBouncing balls‚Äù scene."
H,0.41729323308270677,Time Backbone [Iterations]
H,0.42105263157894735,PSNR [dB] SSIM
H,0.424812030075188,"Time Backbone [Iterations]
Time Backbone [Iterations] LPIPS"
H,0.42857142857142855,"Figure 5: Effect of the backbone initialization pre-training steps on the final result of our method
when trained on the Jumping Jacks scene from the Blender dataset. Our final choice of 20k iterations
corresponds to approximately 70 minutes of real time."
H,0.4323308270676692,"Fig. 6 right). We use the original training-test split. Third, ZJU-MoCap dataset [82] is a common
test suite for dynamic human reconstruction and we evaluate 5 of its multi-view sequences with the
same 6 training views as used in Watch-It-Move [20]. Finally, we evaluate image quality using peak
signal-to-noise ratio (PSNR), structural similarity (SSIM) [114], and learned perceptual image patch
similarity (LPIPS) [115] image metrics."
H,0.43609022556390975,"Implementation details
We pre-train the TiNeuVox [11] backbone using the authors‚Äô implementa-
tion and an additional distortion loss [116], as implemented in [117]. Our method is implemented in
Pytorch and we train each scene using the Adam optimizer for 160k (Blender and Robots) or 320k
(ZJU-MoCap) iterations with a batch size of 8192 rays, sampled randomly from multiple views and a
single timestamp. We choose the canonical timestamp by visual inspection, and gradually increase
the number of observed timestamps during training. We adjust ray sampling and scheduling for the
ZJU-MoCap dataset (see the Supplement). All experiments were done on a single Nvidia GPU RTX
3090Ti. See the supplementary materials for details. For more details, see the Appendix."
H,0.4398496240601504,"Baselines
We compare our method to state-of-the-art non-articulated and articulated methods for
high-fidelity multi-view video synthesis (see Table 1 for an overview). D-NeRF [3] extends NeRF
to the temporal domain by backward warping a static canonical NeRF. TiNeuVox [11] improves
performance of D-NeRF using voxel grids. Hexplane [72] and K-Planes [71] decompose to the
space-time volume to several hyper-planes. Finally, Watch-It-Move [20] (WIM) jointly learns a
surface representation and LBS model for articulation. Note, that because we aim at time-efficient"
H,0.44360902255639095,"WIM
Ours"
H,0.4473684210526316,"Ground Truth
Ours
WIM"
H,0.45112781954887216,Ground Truth
H,0.4548872180451128,"Figure 6: While our model well reproduces details and non-rigidity of the Dinosaur (right), it can
fail for complex motions combining long chains of rotations with texture cue ambiguity (left)."
H,0.45864661654135336,b) SimpliÔ¨Åed
H,0.462406015037594,"Kinematic Model
a) Learned Kinematic Model"
H,0.46616541353383456,"Figure 7: Learned LBS weights and skeleton: a) After training. b) After additional post-processing
(weight merging and skeleton pruning, see Sec. 4.2)."
H,0.4699248120300752,"learning, training of WIM was stopped after 11 hours (i.e., 80k of the original 400k optimization
steps)."
H,0.47368421052631576,"Novel view synthesis
We provide results for the Robots view synthesis without the skeleton
simplification; quantitatively in Fig. 4 and qualitatively in Fig. 3. More visual examples are available
in the supplement. After the initial pre-training, our method quickly surpasses the image quality
of WIM [20]. The mean image scores obtained for our method after 2 hours of training (incl. pre-
training) are higher than those that WIM achieves after 10 hours. We attribute this to NeRF‚Äôs capability
to approximate even complex shapes, which are difficult to fit using the signed distance function
utilized by WIM. Nevertheless, for visually simple objects with long nested pose transformation
chains, this capacity might encourage false explanations of the articulation and cause artifacts, as
visible in Fig. 6 left. However, this is not a common issue. We illustrate it in the Blender, where
WIM struggles to represent fine details (see Fig. 6 right), while our method achieves image scores
close to those of non-reposable baselines (see Table 2 and the supplement)."
H,0.4774436090225564,"Furthermore, Fig. 5 shows that our image quality is not highly dependent on the pre-training phase.
We observe that mere 100 iterations of TiNeuVox pre-training provide an initialization sufficient
for achieving PSNR scores over 30 dB by our method. However, while fine geometric details are
still recovered, the coarse initial shape limits the skeleton complexity and, therefore, we opt for 20k
pre-training steps in all other experiments."
H,0.48120300751879697,"ZJU-MoCap
In Fig. 10, we compare our method to WIM in the ZJU-MoCap dataset. We observe
that both methods are able to recover the 3D shape and the skeletal motion despite the difficulty of an
accurate fine texture detail reconstruction. This can partly be attributed to imperfections in camera
calibrations (see Supplement F of [96]) and partly by soft deformations of clothes which are modeled
neither method. Notably, with a small modification our method learns to partially compensate for this.
In Ourspose, we condition the feature embedding network Œ¶p by the skeletal poses jointly learned
from scratch by our method. This improves the image quality by modeling the residual deformations.
See the Supplement for details. Finally, OursSMP L shows that replacing our skeleton in Ourspose
with a ground truth from an annotated SMPL template [15] does not affect the performance. This
validates our skeleton initialization based on Medial Axis Transform."
H,0.4849624060150376,Figure 8: Reposing using the simplified skeleton. Interpolation from canonical to novel pose.
H,0.48872180451127817,"Full Model
No LARAP
No Lsmooth
No LSkel
Full Model
Full Model"
H,0.4924812030075188,"a)
b)
c)"
H,0.49624060150375937,"Figure 9: Ablation examples. a) LARAP enforces rigidity after pruning (see upper pipes), b) Lsmooth
results in better part-segmentation, c) Lskel enforces the joints to stay inside the shape."
H,0.5,"GT
WIM
Ours
GT
WIM
Ours"
H,0.5037593984962406,"Method
PSNR‚Üë
SSIM‚Üë
LPIPS‚Üì"
H,0.5075187969924813,"WIM [20]
31.08
0.963
0.053
Ours
29.60
0.958
0.063
Ourspose
32.05
0.967
0.056
OursSMP L
32.01
0.967
0.056"
H,0.5112781954887218,"Figure 10: Comparison in the ZJU-MoCap dataset. Ours: Our full method. Ourspose: Ours with an
additional pose-conditioned feature embedding network Œ¶p. OursSMP L: Ourspose with ground truth
SMPL [15] skeleton."
H,0.5150375939849624,"Reposing
In Fig. 7, we visualize the learned blend-skinning weights ÀÜwi and skeletons with and
without the skeleton simplification. The algorithm is able to substantially reduce the skeleton
complexity, while largely preserving semantic articulations observed in the training data. However,
it is not able to remove all unnecessary skeletal branches when complex geometry is present (see
Fig. 7 right). In Fig. 8, we show that such post-processed skeletons allow for animating of novel
poses by smoothly interpolating between user-defined poses. More animations can be found in the
Supplement."
H,0.518796992481203,"Loss ablation
Our experiments suggest the regularization does not improve the image quality, but
it improves the quality of the kinematic model, which is important for our main goal of reposing (see
Fig. 9). Specifically, LARAP avoids non-rigid distortions (a), Lsmooth leads to a more semantically
meaningful part segmentation (b), Lskel encourages functional placement of the skeleton joints inside
the object volume even for thin parts (c), and Ltranf leads to a reduction of necessary object parts
after simplification (62 components are removed instead of 49 for Atlas). More details are provided
in the Supplement."
H,0.5225563909774437,"Backbone
The TiNeuVox backbone allows us to transfer parameters learned during the pre-training
and initialize the features fi and the regressors Œ¶d and Œ¶c. Interestingly, our experiment shows that
such off-the-shelf features fi often do not need any further fine-tuning (see Table 4). Despite this, our
method is agnostic to the backbone choice by design and the end-to-end training procedure of the
entire model is essential for this. We demonstrate it by training with a random initialization. This
way, only the point positions pc
i obtainable by any 3D reconstruction method are needed. Table 3
shows that this leads to only a negligible drop in reconstruction quality in the Robots dataset. Here,
the weight of Lskel (w2 = 1) was adjusted to prevent drift of the skeleton."
H,0.5263157894736842,"PSNR‚Üë
SSIM‚Üë
LPIPS‚Üì
Full Method
30.19
0.973
0.041
Random Init.
29.97
0.971
0.045
Table 3: Results with a TiNeuVox initialization
and with a random initialization of the features
fi and the regressors Œ¶d and Œ¶c in the Robots
dataset."
H,0.5300751879699248,"PSNR‚Üë
SSIM‚Üë
LPIPS‚Üì
Full Method
29.10
0.94
0.06
No Fine-tuning
29.16
0.94
0.05
Table 4: Results with and without fine-tuning of
the feature points fi in the Blender dataset."
DISCUSSION,0.5338345864661654,"6
Discussion"
DISCUSSION,0.5375939849624061,"Limitations and Future Work
We demonstrate fast learning of articulated NeRFs for state-of-
the-art view synthesis and straightforward skeletal reposing for objects with piece-wise rigid pose
transformations. While the LBS allows for fitting partially non-rigid deformations (see Fig. 6 right),
representing fully non-rigid, topologically varying, and multi-component objects would benefit from
a higher-dimensional parameterization. Chen et al. [102] aim in that direction, but an intuitive and
cost-effective reposing still favors skeleton-based techniques such as ours."
DISCUSSION,0.5413533834586466,"The performance of our method depends on the quality of the backbone reconstruction. While
TiNeuVox [11] supports reconstruction from even sparse data, a different backbone could offer a
more robust starting geometry and improve the skeleton initialization. Although our approach works
well even for highly structured shapes and thin structures (see Fig. 6 right), we observe incorrect joint
placement for objects with long chains of highly nonlinear geometrical transformations (see Fig. 6
left). Moreover, our approach successfully reduces the number of extraneous bones but it is not able
to completely eliminate all superfluous skeletal elements (see Fig. 7)."
DISCUSSION,0.5451127819548872,"Our method is restricted to the kinematic motion space exhibited in the training sequences. While
extrapolation is possible to some extent, the proposed method is not able to generalize to arbitrary
unseen poses. Finally, we focus on image synthesis for user-defined poses rather than a direct fitting
of unseen skeletal poses from images or motion capture. An inverse fitting of skeletal poses to novel
2D observations or transfer of poses from one object to another remain opportunities for future
research."
DISCUSSION,0.5488721804511278,"Conclusion
We presented a method for fast learning of articulated models for high-quality novel
view synthesis of captured 3D objects. Our forward-warped neural point clouds avoid the pitfalls
of backward warping and elegantly integrate a skeleton-based LBS. As a result, our method merges
straightforward reposing even of strongly animated inputs, as present in the Robots dataset, with
high visual fidelity of NeRF rendering. Our work is a significant step towards low-cost acquisition of
animatable 3D objects for games, movies, and education."
DISCUSSION,0.5526315789473685,"Ethical Considerations
Our method renders novel views and poses of 3D objects including human
bodies. However, we do not focus on this class and we note that many human-specific methods exist
(see Sec. 2). Nevertheless, we acknowledge that our method could potentially be used to produce
fake images of people and that research is needed to understand the risks and their mitigation."
REFERENCES,0.556390977443609,References
REFERENCES,0.5601503759398496,"[1] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren
Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In European Conference on
Computer Vision (ECCV), 2020."
REFERENCES,0.5639097744360902,"[2] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Wang Yifan, Christoph
Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Advances in neural
rendering. In Computer Graphics Forum, volume 41, pages 703‚Äì735. Wiley Online Library, 2022."
REFERENCES,0.5676691729323309,"[3] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural
radiance fields for dynamic scenes. arXiv preprint arXiv:2011.13961, 2020."
REFERENCES,0.5714285714285714,"[4] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time
view synthesis of dynamic scenes. In CVPR, 2021."
REFERENCES,0.575187969924812,"[5] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh√∂fer, Christoph Lassner, and Christian
Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene
from monocular video. In ICCV, 2021."
REFERENCES,0.5789473684210527,"[6] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz,
and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. ICCV, 2021."
REFERENCES,0.5827067669172933,"[7] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for
free-viewpoint video. In CVPR, pages 9421‚Äì9431, 2021."
REFERENCES,0.5864661654135338,"[8] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic
monocular video. In CVPR, 2021."
REFERENCES,0.5902255639097744,"[9] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman,
Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: A higher-dimensional representation for
topologically varying neural radiance fields. ACM Trans. Graph. (SIGGRAPH Asia), 40(6), 2021."
REFERENCES,0.5939849624060151,"[10] Tianye Li, Mira Slavcheva, Michael Zollh√∂fer, Simon Green, Christoph Lassner, Changil Kim, Tanner
Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, and Zhaoyang Lv. Neural 3d video
synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 5521‚Äì5531, June 2022."
REFERENCES,0.5977443609022557,"[11] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nie√üner,
and Qi Tian. Fast dynamic radiance fields with time-aware neural voxels. In SIGGRAPH Asia 2022
Conference Papers, pages 1‚Äì9, 2022."
REFERENCES,0.6015037593984962,"[12] John P Lewis, Matt Cordner, and Nickson Fong. Pose space deformation: a unified approach to shape
interpolation and skeleton-driven deformation. In Proceedings of the 27th annual conference on Computer
graphics and interactive techniques, pages 165‚Äì172, 2000."
REFERENCES,0.6052631578947368,"[13] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In ACM SIGGRAPH,
page 187‚Äì194, 1999."
REFERENCES,0.6090225563909775,"[14] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning a model of facial shape
and expression from 4D scans. ACM Trans. Graph. (SIGGRAPH Asia), 36(6):194:1‚Äì194:17, 2017."
REFERENCES,0.6127819548872181,"[15] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A
skinned multi-person linear model. ACM Trans. Graphics (SIGGRAPH Asia), 34(6):248:1‚Äì248:16, 2015."
REFERENCES,0.6165413533834586,"[16] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios
Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from a single image.
In CVPR, pages 10975‚Äì10985, 2019."
REFERENCES,0.6203007518796992,"[17] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva
Ramanan, William T Freeman, and Ce Liu. Lasr: Learning articulated shape reconstruction from
a monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 15980‚Äì15989, 2021."
REFERENCES,0.6240601503759399,"[18] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Ce Liu, and Deva Ramanan.
Viser: Video-specific surface embeddings for articulated 3d shape reconstruction. Advances in Neural
Information Processing Systems, 34:19326‚Äì19338, 2021."
REFERENCES,0.6278195488721805,"[19] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, and Varun
Jampani. Lassie: Learning articulated shapes from sparse image ensemble via 3d part discovery. arXiv
preprint arXiv:2207.03434, 2022."
REFERENCES,0.631578947368421,"[20] Atsuhiro Noguchi, Umar Iqbal, Jonathan Tremblay, Tatsuya Harada, and Orazio Gallo. Watch it move:
Unsupervised discovery of 3d joints for re-posing of articulated objects. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 3677‚Äì3687, 2022."
REFERENCES,0.6353383458646616,"[21] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann.
Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 5438‚Äì5448, 2022."
REFERENCES,0.6390977443609023,"[22] Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ramanan, Vedaldi Andrea, and Joo Hanbyul. Banmo:
Building animatable 3d neural models from many casual videos. In CVPR, 2022."
REFERENCES,0.6428571428571429,"[23] Yuefan Wu*, Zeyuan Chen*, Shaowei Liu, Zhongzheng Ren, and Shenlong Wang. CASA: Category-
agnostic skeletal animal reconstruction. In Neural Information Processing Systems (NeurIPS), 2022."
REFERENCES,0.6466165413533834,"[24] SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Garnelo,
Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene representation and
rendering. Science, 2018."
REFERENCES,0.650375939849624,"[25] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF:
Learning continuous signed distance functions for shape representation. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2019."
REFERENCES,0.6541353383458647,"[26] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occu-
pancy networks: Learning 3D reconstruction in function space. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2019."
REFERENCES,0.6578947368421053,"[27] Mateusz Michalkiewicz, Jhony K Pontes, Dominic Jack, Mahsa Baktashmotlagh, and Anders Eriksson.
Implicit surface representations as layers in neural networks. In IEEE International Conference on
Computer Vision (ICCV), 2019."
REFERENCES,0.6616541353383458,"[28] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2019."
REFERENCES,0.6654135338345865,"[29] Matan Atzmon and Yaron Lipman. SAL: Sign agnostic learning of shapes from raw data. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2020."
REFERENCES,0.6691729323308271,"[30] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization
for learning shapes. In International Conference on Machine Learning (ICML), 2020."
REFERENCES,0.6729323308270677,"[31] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie√üner, and Thomas Funkhouser.
Local implicit grid representations for 3D scenes. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2020."
REFERENCES,0.6766917293233082,"[32] Thomas Davies, Derek Nowrouzezahrai, and Alec Jacobson. Overfit neural networks as a compact shape
representation. arXiv preprint arXiv:2009.09808, 2020."
REFERENCES,0.6804511278195489,"[33] Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, and Richard
Newcombe. Deep local shapes: Learning local SDF priors for detailed 3D reconstruction. In European
Conference on Computer Vision (ECCV), 2020."
REFERENCES,0.6842105263157895,"[34] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional
occupancy networks. In European Conference on Computer Vision (ECCV), 2020."
REFERENCES,0.6879699248120301,"[35] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai,
Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering
with implicit 3D shapes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021."
REFERENCES,0.6917293233082706,"[36] Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein.
Implicit neural representations with periodic activation functions. In Advances in Neural Information
Processing Systems (NeurIPS), 2020."
REFERENCES,0.6954887218045113,"[37] Julien N.P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, and Gordon
Wetzstein. ACORN: Adaptive coordinate networks for neural representation. ACM Transactions on
Graphics (SIGGRAPH), 2021."
REFERENCES,0.6992481203007519,"[38] Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli, and Gordon Wetzstein. Neural
lumigraph rendering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021."
REFERENCES,0.7030075187969925,"[39] Yue Jiang, Dantong Ji, Zhizhong Han, and Matthias Zwicker. SDFDiff: Differentiable rendering of
signed distance fields for 3D shape optimization. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2020."
REFERENCES,0.706766917293233,"[40] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, and Yaron Lipman.
Multiview neural surface reconstruction by disentangling geometry and appearance. In Advances in
Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.7105263157894737,"[41] Vincent Sitzmann, Michael Zollh√∂fer, and Gordon Wetzstein. Scene representation networks: Continuous
3D-structure-aware neural scene representations. In Advances in Neural Information Processing Systems
(NeurIPS), 2019."
REFERENCES,0.7142857142857143,"[42] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie√üner, Gordon Wetzstein, and Michael Zollh√∂fer.
DeepVoxels: Learning persistent 3D feature embeddings. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019."
REFERENCES,0.7180451127819549,"[43] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. PIFu:
Pixel-aligned implicit function for high-resolution clothed human digitization. In IEEE International
Conference on Computer Vision (ICCV), 2019."
REFERENCES,0.7218045112781954,"[44] Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li. Learning to infer implicit surfaces without 3D
supervision. arXiv preprint arXiv:1911.00767, 2019."
REFERENCES,0.7255639097744361,"[45] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh.
Neural volumes: Learning dynamic renderable volumes from images. ACM Transactions on Graphics
(SIGGRAPH), 2019."
REFERENCES,0.7293233082706767,"[46] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric
rendering: Learning implicit 3d representations without 3d supervision. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2020."
REFERENCES,0.7330827067669173,"[47] Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, and Zhaopeng Cui. DIST:
Rendering deep implicit signed distance function with differentiable sphere tracing. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2020."
REFERENCES,0.7368421052631579,"[48] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy,
and Daniel Duckworth. NeRF in the wild: Neural radiance fields for unconstrained photo collections. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021."
REFERENCES,0.7406015037593985,"[49] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T.
Barron. NeRV: Neural reflectance and visibility fields for relighting and view synthesis. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2021."
REFERENCES,0.7443609022556391,"[50] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural
radiance fields. arXiv preprint arXiv:2010.07492, 2020."
REFERENCES,0.7481203007518797,"[51] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla
Chaitanya, Anton S. Kaplanyan, and Markus Steinberger. DONeRF: Towards Real-Time Rendering of
Compact Neural Radiance Fields using Depth Oracle Networks. Computer Graphics Forum, 40(4), 2021."
REFERENCES,0.7518796992481203,"[52] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh
Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high
frequency functions in low dimensional domains. In Advances in Neural Information Processing Systems
(NeurIPS), 2020."
REFERENCES,0.7556390977443609,"[53] David B Lindell, Julien NP Martel, and Gordon Wetzstein. AutoInt: Automatic integration for fast neural
volume rendering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021."
REFERENCES,0.7593984962406015,"[54] Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, and Joshua M. Susskind.
Unconstrained scene generation with locally conditioned radiance fields. arXiv preprint arXiv:2104.00670,
2021."
REFERENCES,0.7631578947368421,"[55] Michael Oechsle, Songyou Peng, and Andreas Geiger. UNISURF: Unifying neural implicit surfaces and
radiance fields for multi-view reconstruction. In IEEE International Conference on Computer Vision
(ICCV), 2021."
REFERENCES,0.7669172932330827,"[56] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. FastNeRF:
High-fidelity neural rendering at 200fps. arXiv preprint arXiv:2103.10380, 2021."
REFERENCES,0.7706766917293233,"[57] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli,
Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nie√üner, et al. State of the art on neural
rendering. Eurographics Association, 2020."
REFERENCES,0.7744360902255639,"[58] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. KiloNeRF: Speeding up neural radiance
fields with thousands of tiny MLPs. In IEEE International Conference on Computer Vision (ICCV), 2021."
REFERENCES,0.7781954887218046,"[59] Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasacchi. Derf:
Decomposed radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 14153‚Äì14161, 2021."
REFERENCES,0.7819548872180451,"[60] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul Srinivasan,
Jonathan T. Barron, and Henrik Kretzschmar. Block-NeRF: Scalable large scene neural view synthesis.
arXiv, 2022."
REFERENCES,0.7857142857142857,"[61] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul Debevec. Baking
neural radiance fields for real-time view synthesis. In IEEE International Conference on Computer Vision
(ICCV), 2021."
REFERENCES,0.7894736842105263,"[62] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for real-time
rendering of neural radiance fields. In IEEE International Conference on Computer Vision (ICCV), 2021."
REFERENCES,0.793233082706767,"[63] Thomas M√ºller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives
with a multiresolution hash encoding. arXiv:2201.05989, January 2022."
REFERENCES,0.7969924812030075,"[64] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence
for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 5459‚Äì5469, 2022."
REFERENCES,0.8007518796992481,"[65] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields.
In Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.8045112781954887,"[66] Julian Ost, Issam Laradji, Alejandro Newell, Yuval Bahat, and Felix Heide. Neural point light fields. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18419‚Äì
18429, 2022."
REFERENCES,0.8082706766917294,"[67] Jad Abou-Chakra, Feras Dayoub, and Niko S√ºnderhauf. Particlenerf: Particle based encoding for online
neural radiance fields in dynamic scenes. arXiv preprint arXiv:2211.04041, 2022."
REFERENCES,0.8120300751879699,"[68] Ruofan Liang, Jiahao Zhang, Haoda Li, Chen Yang, and Nandita Vijaykumar. Spidr: Sdf-based neural
point fields for illumination and deformation. arXiv preprint arXiv:2210.08398, 2022."
REFERENCES,0.8157894736842105,"[69] Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J Black, and Otmar Hilliges. Pointavatar:
Deformable point-based head avatars from videos. arXiv preprint arXiv:2212.08377, 2022."
REFERENCES,0.8195488721804511,"[70] Shengze Wang, Alexey Supikov, Joshua Ratcliff, Henry Fuchs, and Ronald Azuma. Inv: Towards
streaming incremental neural videos. arXiv preprint arXiv:2302.01532, 2023."
REFERENCES,0.8233082706766918,"[71] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb√¶k Warburg, Benjamin Recht, and Angjoo
Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12479‚Äì12488, 2023."
REFERENCES,0.8270676691729323,"[72] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130‚Äì141, 2023."
REFERENCES,0.8308270676691729,"[73] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing
hands and bodies together. ACM Trans. Graph. (SIGGRAPH Asia), 36(6), November 2017."
REFERENCES,0.8345864661654135,"[74] Silvia Zuffi, Angjoo Kanazawa, David W Jacobs, and Michael J Black. 3d menagerie: Modeling the 3d
shape and pose of animals. In CVPR, pages 6365‚Äì6373, 2017."
REFERENCES,0.8383458646616542,"[75] Guy Gafni, Justus Thies, Michael Zollh√∂fer, and Matthias Nie√üner. Dynamic neural radiance fields for
monocular 4d facial avatar reconstruction. In CVPR, 2021."
REFERENCES,0.8421052631578947,"[76] Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, and
Michael Zollhofer. Learning compositional radiance fields of dynamic human heads. In CVPR, pages
5704‚Äì5713, 2021."
REFERENCES,0.8458646616541353,"[77] Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun Bao, and Juyong Zhang. Ad-nerf: Audio driven
neural radiance fields for talking head synthesis. In ICCV, 2021."
REFERENCES,0.849624060150376,"[78] Yufeng Zheng, Victoria Fern√°ndez Abrevaya, Marcel C. B√ºhler, Xu Chen, Michael J. Black, and Otmar
Hilliges. I M Avatar: Implicit morphable head avatars from videos. In Computer Vision and Pattern
Recognition (CVPR), 2022."
REFERENCES,0.8533834586466166,"[79] Kacper Kania, Kwang Moo Yi, Marek Kowalski, Tomasz Trzci¬¥nski, and Andrea Tagliasacchi. CoNeRF:
Controllable Neural Radiance Fields. In CVPR, 2022."
REFERENCES,0.8571428571428571,"[80] Yiyu Zhuang, Hao Zhu, Xusen Sun, and Xun Cao. Mofanerf: Morphable facial neural radiance field.
In Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022,
Proceedings, Part III, pages 268‚Äì285. Springer, 2022."
REFERENCES,0.8609022556390977,"[81] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli Shechtman, and Zhixin Shu. Rignerf: Fully
controllable neural 3d portraits. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 20364‚Äì20373, 2022."
REFERENCES,0.8646616541353384,"[82] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou.
Neural body: Implicit neural representations with structured latent codes for novel view synthesis of
dynamic humans. In CVPR, 2021."
REFERENCES,0.868421052631579,"[83] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao.
Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021."
REFERENCES,0.8721804511278195,"[84] Shih-Yang Su, Frank Yu, Michael Zollh√∂fer, and Helge Rhodin. A-nerf: Articulated neural radiance fields
for learning human shape, appearance, and pose. In NeurIPS, 2021."
REFERENCES,0.8759398496240601,"[85] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. H-nerf: Neural radiance fields for rendering
and temporal reconstruction of humans in motion. In NeurIPS, 2021."
REFERENCES,0.8796992481203008,"[86] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Neural articulated radiance field. In
ICCV, 2021."
REFERENCES,0.8834586466165414,"[87] Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry Fuchs. Neural human performer: Learning
generalizable radiance fields for human performance rendering. In NeurIPS, 2021."
REFERENCES,0.8872180451127819,"[88] Haotong Lin, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields with
learned depth-guided sampling. In arXiv, 2021."
REFERENCES,0.8909774436090225,"[89] Tao Hu, Tao Yu, Zerong Zheng, He Zhang, Yebin Liu, and Matthias Zwicker. Hvtr: Hybrid volumetric-
textural rendering for human avatars. In arXiv, 2021."
REFERENCES,0.8947368421052632,"[90] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt.
Neural actor: Neural free-view synthesis of human actors with pose control. ACM Trans. Graph.(ACM
SIGGRAPH Asia), 2021."
REFERENCES,0.8984962406015038,"[91] Tianhan Xu, Yasuhiro Fujita, and Eiichi Matsumoto. Surface-aligned neural radiance fields for controllable
3d human synthesis. In CVPR, 2022."
REFERENCES,0.9022556390977443,"[92] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. Human-
nerf: Efficiently generated human radiance field from sparse inputs. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 7743‚Äì7753, 2022."
REFERENCES,0.9060150375939849,"[93] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-
Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In CVPR,
2022."
REFERENCES,0.9097744360902256,"[94] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Selfrecon: Self reconstruction your digital avatar
from monocular video. In CVPR, 2022."
REFERENCES,0.9135338345864662,"[95] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges, and Andreas Geiger. Snarf: Differentiable
forward skinning for animating non-rigid neural implicit shapes. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 11594‚Äì11604, 2021."
REFERENCES,0.9172932330827067,"[96] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollh√∂fer, J√ºrgen Gall, Angjoo Kanazawa, and Christoph
Lassner. Tava: Template-free animatable volumetric actors. In European Conference on Computer Vision,
pages 419‚Äì436. Springer, 2022."
REFERENCES,0.9210526315789473,"[97] Yihao Zhi, Shenhan Qian, Xinhao Yan, and Shenghua Gao. Dual-space nerf: Learning animatable avatars
and scene lighting in separate spaces. In 2022 International Conference on 3D Vision (3DV), pages 1‚Äì10.
IEEE, 2022."
REFERENCES,0.924812030075188,"[98] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong Zhang. Headnerf: A real-time nerf-based
parametric head model. In CVPR, 2022."
REFERENCES,0.9285714285714286,"[99] Alexander W Bergman, P Kellnhofer, Wang Yifan, Eric R Chan, David B Lindell, and Gordon Wetzstein.
Generative neural articulated radiance fields. In Advances in Neural Information Processing Systems 35
(NeurIPS 2022): NeurIPS Proceedings. 2022."
REFERENCES,0.9323308270676691,"[100] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Unsupervised learning of efficient
geometry-aware neural articulated representations. In Computer Vision‚ÄìECCV 2022: 17th European
Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XVII, pages 597‚Äì614. Springer,
2022."
REFERENCES,0.9360902255639098,"[101] Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wenzheng Chen, and Lan Xu. Humangen: Generating
human radiance fields with explicit priors. arXiv preprint arXiv:2212.05321, 2022."
REFERENCES,0.9398496240601504,"[102] Hsiao-yu Chen, Edith Tretschk, Tuur Stuyck, Petr Kadlecek, Ladislav Kavan, Etienne Vouga, and
Christoph Lassner. Virtual elastic objects. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 15827‚Äì15837, 2022."
REFERENCES,0.943609022556391,"[103] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Magicpony: Learning
articulated 3d animals in the wild. arXiv preprint arXiv:2211.12497, 2022."
REFERENCES,0.9473684210526315,"[104] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. End-to-end recovery of
human shape and pose. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
7122‚Äì7131, 2018."
REFERENCES,0.9511278195488722,"[105] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. Learning to reconstruct
3d human pose and shape via model-fitting in the loop. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 2252‚Äì2261, 2019."
REFERENCES,0.9548872180451128,"[106] Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J Black. Learning to regress 3d face shape
and expression from an image without 3d supervision. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 7763‚Äì7772, 2019."
REFERENCES,0.9586466165413534,"[107] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned
implicit function for high-resolution 3d human digitization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 84‚Äì93, 2020."
REFERENCES,0.9624060150375939,"[108] Muhammed Kocabas, Nikos Athanasiou, and Michael J Black. Vibe: Video inference for human body
pose and shape estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5253‚Äì5263, 2020."
REFERENCES,0.9661654135338346,"[109] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, and Varun
Jampani. Hi-lassie: High-fidelity articulated shape and skeleton discovery from sparse image ensemble.
arXiv preprint arXiv:2212.11042, 2022."
REFERENCES,0.9699248120300752,"[110] Tianhan Xu and Tatsuya Harada. Deforming radiance fields with cages. In Computer Vision‚ÄìECCV
2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XXXIII, pages
159‚Äì175. Springer, 2022."
REFERENCES,0.9736842105263158,"[111] Kaizhi Yang, Xiaoshuai Zhang, Zhiao Huang, Xuejin Chen, Zexiang Xu, and Hao Su. Movingparts:
Motion-based 3d part discovery in dynamic radiance field. arXiv preprint arXiv:2303.05703, 2023."
REFERENCES,0.9774436090225563,"[112] James T Kajiya and Brian P Von Herzen. Ray tracing volume densities. ACM SIGGRAPH computer
graphics, 18(3):165‚Äì174, 1984."
REFERENCES,0.981203007518797,"[113] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields.
In Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022,
Proceedings, Part XXXII, pages 333‚Äì350. Springer, 2022."
REFERENCES,0.9849624060150376,"[114] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from
error visibility to structural similarity. IEEE transactions on image processing, 13(4):600‚Äì612, 2004."
REFERENCES,0.9887218045112782,"[115] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 586‚Äì595, 2018."
REFERENCES,0.9924812030075187,"[116] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360:
Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 5470‚Äì5479, 2022."
REFERENCES,0.9962406015037594,"[117] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Improved direct voxel grid optimization for radiance
fields reconstruction. arXiv preprint arXiv:2206.05085, 2022."
