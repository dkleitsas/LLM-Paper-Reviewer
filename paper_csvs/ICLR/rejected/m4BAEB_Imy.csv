Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.007142857142857143,"Modern image recognition models span millions of parameters occupying several
megabytes and sometimes gigabytes of space, making it difﬁcult to run on re-
source constrained edge hardware. Binary Neural Networks address this problem
by reducing the memory requirements (one single bit per weight and/or activa-
tion). The computation requirement and power consumption are also reduced
accordingly. Nevertheless, in such networks, each neuron has a large number of
inputs, making it difﬁcult to implement them efﬁciently in binary hardware accel-
erators, especially LUT-based approaches.
In this work, we present a pruning algorithm and associated results on convo-
lutional and dense layers from aforementioned binary networks. We reduce the
computation by 4-70x and the memory by 190-2200x with less than 2% loss of
accuracy on MNIST and less than 3% loss of accuracy on CIFAR-10 compared
to full precision, fully connected equivalents. Compared to very recent work on
pruning for binary networks, we still have a gain of 1% on the precision and up to
30% reduction in memory (526KiB vs 750KiB)."
INTRODUCTION,0.014285714285714285,"1
INTRODUCTION"
INTRODUCTION,0.02142857142857143,"AI has become ubiquitous in our daily lives, be it the use of search engines to ﬁnd information
or automatically tagging photos on social media. While the predictive accuracy of these models
have grown, the size and computation requirements have grown by several orders of magnitude or
more, making AI a niche ﬁeld where only few can participate. This practice is also proving to be
environmentally unfriendly with Strubell et al. (2019) estimating the footprint of training an NLP
model to be 626,000 pounds of carbon dioxide equivalent."
INTRODUCTION,0.02857142857142857,"To counter this growing computation need, it is important to develop hardware accelerators which
are easily re-conﬁgurable. A Field Programmable Gate Array (FPGA) is an integrated circuit de-
signed to be reconﬁgured by the user and acts as a blank canvas for implementing custom circuits (in
our case custom models). FPGAs embed millions of lookup tables (LUTs) that ”compute” (actually
fetch) the output given the values of their binary inputs. The problem is that these LUTs are lim-
ited in size (typically 6-12 inputs) and using multiple LUTs per neuron is less efﬁcient. This raises
the need for both binarizing our weights (and/or activation) and reducing the number of inputs per
neuron."
INTRODUCTION,0.03571428571428571,"In Sections 2 and 3 we go through existing works on binarization and pruning of fully connected,
full precision networks. In Section 4, we introduce iPrune, an effective way to prune the weights of
binary networks. In Section 5, we compare the accuracy, memory and computation requirements of
these network implementations against their full precision, fully connected equivalents. In Section
6, we report some intriguing results about a-priori pruning versus post-training pruning. Section 7
concludes this work."
BINARIZATION TECHNIQUES,0.04285714285714286,"2
BINARIZATION TECHNIQUES"
BINARIZATION TECHNIQUES,0.05,"Binarization is the process of converting the weights and/or activations to a 1-bit representation (±1)
instead of the classic N-bit representation (typically 8-bit, 16-bit or 32-bit). This gives us a direct"
BINARIZATION TECHNIQUES,0.05714285714285714,Under review as a conference paper at ICLR 2022
BINARIZATION TECHNIQUES,0.06428571428571428,"memory saving of up to 32x. There are several approaches in literature on binarization. Throughout
our experiments, we worked with BinaryConnect whose details are given in the next subsection."
BINARYCONNECT,0.07142857142857142,"2.1
BINARYCONNECT"
BINARYCONNECT,0.07857142857142857,"Courbariaux et al. (2015) introduced BinaryConnect, the ﬁrst paper of its kind, with details on
binarization of both convolutional and dense weights. There are two sub-types based on how the
binarization is performed."
BINARYCONNECT,0.08571428571428572,"• Deterministic: The weights are binarized deterministically; speciﬁcally, using the sign
function on the hard-sigmoid of the weight."
BINARYCONNECT,0.09285714285714286,"• Stochastic: The weights are set to -1 with probability (1-σ(Wi)) and 1 with probability
σ(Wi) where σ is the hard-sigmoid function."
BINARYCONNECT,0.1,The hard-sigmoid function mentioned above is given by:
BINARYCONNECT,0.10714285714285714,"σ(x) = clip
x + 1"
BINARYCONNECT,0.11428571428571428,"2
, 0, 1

(1)"
BINARYCONNECT,0.12142857142857143,"During the back-propagation, the full precision weights are updated based on the gradient with
respect to the binary weights. The pseudocode for BinaryConnect is given in Algorithm 1"
BINARYCONNECT,0.12857142857142856,"Algorithm 1 Algorithm for BinaryConnect
1. Forward propagation:"
BINARYCONNECT,0.1357142857142857,"wb ←binarize(wt−1)
for k = 1 : L do"
BINARYCONNECT,0.14285714285714285,"compute ak knowing ak−1, wb and bt−1
end for
2. Backward propagation"
BINARYCONNECT,0.15,for k = L : 2 : −1 do
BINARYCONNECT,0.15714285714285714,"Compute
∂C
∂ak−1 knowing ∂C"
BINARYCONNECT,0.16428571428571428,"∂ak and wb
end for
3. Parameter update"
BINARYCONNECT,0.17142857142857143,Compute ∂C
BINARYCONNECT,0.17857142857142858,"∂wb and
∂C
∂bt−1 knowing ∂C"
BINARYCONNECT,0.18571428571428572,"∂ak and ak−1
wt ←clip(wt−1 −η ∂C"
BINARYCONNECT,0.19285714285714287,"∂wb )
bt ←bt−1 −η
∂C
∂bt−1"
PRUNING TECHNIQUES,0.2,"3
PRUNING TECHNIQUES"
PRUNING TECHNIQUES,0.20714285714285716,"Hoeﬂer et al. (2021) discusses in detail different approaches to pruning and was used as a reference
for our methods. The different types of pruning techniques are given in Figure 1. A few approaches
are described in detail in Sections 3.1 and 3.2"
STRUCTURE VS UNSTRUCTURED PRUNING,0.21428571428571427,"3.1
STRUCTURE VS UNSTRUCTURED PRUNING"
STRUCTURE VS UNSTRUCTURED PRUNING,0.22142857142857142,"Structured pruning is the removal of neurons or weights in a structured fashion i.e. based on a ﬁxed
pattern. Common methods for structured pruning are neuron pruning (since this is just a row/column
removed from the weight matrix) and ﬁlter/transformer head removal. Similarly, strided removal of
weights after an offset is also an example of structured pruning."
STRUCTURE VS UNSTRUCTURED PRUNING,0.22857142857142856,"On the other hand, unstructured pruning refers to the pruning method where there are no patterns in
how the weights are removed. While this does not help with easier matrix calculation, it tends to
provide higher baselines than structured pruning and can often be converted to structured pruning
through a minor set of modiﬁcations."
STRUCTURE VS UNSTRUCTURED PRUNING,0.2357142857142857,Under review as a conference paper at ICLR 2022
STRUCTURE VS UNSTRUCTURED PRUNING,0.24285714285714285,Figure 1: Schemes of pruning described in Hoeﬂer et al. (2021)
MAGNITUDE PRUNING,0.25,"3.2
MAGNITUDE PRUNING"
MAGNITUDE PRUNING,0.2571428571428571,"It is a simple and effective selection scheme where the magnitude of the weights is used as a metric
to determine which weights to drop and which weights to keep. After sparsifying the network this
way, retraining is done to get high accuracies again."
MAGNITUDE PRUNING,0.2642857142857143,"There are three ways to perform magnitude pruning; globally, layer-wise or neuron-wise. Global
magnitude pruning takes all weights from all layers and keeps the top-k weights among them. This
often has the problem of vanishing gradients or even vanishing layers (because most weights have
been removed from the layer). Figure 2 gives the distribution of weights before and after applying
global magnitude pruning."
MAGNITUDE PRUNING,0.2714285714285714,"Layer-wise magnitude pruning (used in Guerra et al. (2020)) keeps the top-k weights across all
neurons per layer and overcomes the issue of vanishing layers. This is still difﬁcult to implement in
LUTs, because some neurons will have a large number of connections while others will have very
few after pruning."
MAGNITUDE PRUNING,0.2785714285714286,"Lastly, neuron-wise magnitude pruning keeps the top-k weights for each neuron. This overcomes
both the issues of vanishing layers/gradients and LUT implementation, but often comes at the cost
of low initial accuracy which can be solved by retraining the network. In all our experiments, we
use neuron-wise magnitude pruning."
MAGNITUDE PRUNING,0.2857142857142857,"Figure 2: Weight distribution before and after applying global magnitude pruning on custom VGG
model for CIFAR-10"
MAGNITUDE PRUNING,0.29285714285714287,Under review as a conference paper at ICLR 2022
LOTTERY TICKET HYPOTHESIS,0.3,"3.3
LOTTERY TICKET HYPOTHESIS"
LOTTERY TICKET HYPOTHESIS,0.30714285714285716,"The Lottery Ticket Hypothesis initially introduced in Frankle & Carbin (2018), and extended to
quantized networks in Diffenderfer & Kailkhura (2021) theorizes that there exists optimal sub-
networks in the initialization of an overparameterized network. The authors prove theoretically
and validate practically this claim in their papers. They introduce a learnable parameter ”score”
which decide the weights to retain and the ones to drop. Diffenderfer & Kailkhura (2021) claims
to reduce network sizes by 50% with negligible reduction or sometimes even increase in accuracy
compared to the fully connected counterparts."
IPRUNE,0.3142857142857143,"4
IPRUNE"
IPRUNE,0.32142857142857145,"iPrune is a magnitude based unstructured pruning technique to reduce the number of inputs to each
neuron of a particular layer in a neural network."
IPRUNE,0.32857142857142857,"For dense layers, the magnitude is computed and the top-k weights are chosen for each neuron of the
current layers (top-k columns for each row in the weight matrix of shape (out features, in features)).
For convolutions, the L1 norm across the kernel dimensions (kx, ky) is computed to reduce it to an
(out channels, in channels) representation. This is then followed by pruning using top-k weights
similar to the dense case."
IPRUNE,0.3357142857142857,"Disconnecting neurons from previous layers can be easily accomplished in most deep-learning
frameworks with the use of a mask. Neurons which remain connected have a mask value 1 and
the others have 0. In Figure 3, for a given example mask and weight, the computation of the masked
weight is shown. The ﬁgure also highlights the structure of the mask."
IPRUNE,0.34285714285714286,"Algorithm 2 provides pseudocode for iPrune. Note that the update step, performed on the full
precision weights uses the gradients with respect to the binary masked weights

∂C
∂wm 
."
IPRUNE,0.35,Figure 3: iPrune Masking procedure
EXPERIMENTAL RESULTS,0.35714285714285715,"5
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.36428571428571427,"We performed tests on MNIST (LeCun et al. (1998)) and CIFAR-10 (Krizhevsky & Hin-
ton (2009)) datasets.
For all the results given below, the network architecture used for
MNIST was (1024D - 1024D - 10D) and for CIFAR-10 a modiﬁed version of VGG-
Small, speciﬁcally: (128C3 - 128C3 - MP2 - 256C3 - 256C3 - MP2 - 512C3 -
512C3 - MP2 - 1024D - 1024D - 10D)"
EXPERIMENTAL RESULTS,0.37142857142857144,"For MNIST, no preprocessing was performed. We skipped the last layer while pruning. For CIFAR-
10 we applied ZCA preprocessing as was suggested in Courbariaux et al. (2015) for gains of around
2-3% in accuracy. We applied weight-decay (5e-2) on the last layer (dense) of the network and used
the square hinge loss as the loss function. This together behaves like an L2-SVM block. We use the
Adam optimizer with initial learning rate of 3e-3 with no learning rate scheduler. While pruning,
we skipped the ﬁrst two convolution layers and the last dense layer."
EXPERIMENTAL RESULTS,0.37857142857142856,"Figure 4 shows the effect of iPrune on the weight values for the models we trained. Notice how the
number of neurons with weight magnitude close to zero reduces in both cases. At the same time the"
EXPERIMENTAL RESULTS,0.38571428571428573,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.39285714285714285,"Algorithm 2 Algorithm for iPrune
0. Train a fully connected model
1. Compute the mask - Done only once"
EXPERIMENTAL RESULTS,0.4,"w.mask = ﬁnd mask(w)
2. Forward propagation:"
EXPERIMENTAL RESULTS,0.40714285714285714,"wb ←binarize(w)
wm = wb ∗w.mask
for k = 1 : L do"
EXPERIMENTAL RESULTS,0.4142857142857143,"compute ak knowing ak−1, wm and b
end for
3. Backward propagation"
EXPERIMENTAL RESULTS,0.42142857142857143,for k = L : 2 : −1 do
EXPERIMENTAL RESULTS,0.42857142857142855,"Compute
∂C
∂ak−1 knowing ∂C"
EXPERIMENTAL RESULTS,0.4357142857142857,"∂ak and wm
end for
4. Parameter update"
EXPERIMENTAL RESULTS,0.44285714285714284,"Compute
∂C
∂wm and ∂C"
EXPERIMENTAL RESULTS,0.45,∂b knowing ∂C
EXPERIMENTAL RESULTS,0.45714285714285713,"∂ak , ak−1 and w.mask
w ←clip(w −η ∂C"
EXPERIMENTAL RESULTS,0.4642857142857143,"∂wm )
b ←b −η ∂C ∂b"
EXPERIMENTAL RESULTS,0.4714285714285714,"number of these weights is not zero as we are applying neuron wise pruning, not global pruning.
Also note that in the BinaryConnect case, the original distribution is a mixture of three Gaussians
centered at -1, +1 and 0 while in the FP case, there is only one Gaussian centered at 0."
EXPERIMENTAL RESULTS,0.4785714285714286,(a) BinaryConnect model with 30% weights remaining in all layers except ﬁrst two convolutional and last dense
EXPERIMENTAL RESULTS,0.4857142857142857,(b) Full Precision model with 30% weights remaining in all layers except ﬁrst two convolutional and last dense
EXPERIMENTAL RESULTS,0.4928571428571429,"Figure 4: Weight distribution before, after pruning and after retraining a custom VGG model"
EXPERIMENTAL RESULTS,0.5,"Table 1 gives the results after applying iPrune on MNIST. There is less than 2.3% difference between
the fully connected, full precision network and pruned BinaryConnect deterministic model with 7/8
inputs. The memory gain is around 2200x and computation gain is around 70x."
EXPERIMENTAL RESULTS,0.5071428571428571,"Table 2 gives the accuracy memory and computation requirement for different percentages of
weights remaining per neuron. It is clear that we can reduce the memory by 1.9x-4.7x (remaining
weights 50%-20%) and computation by 1.9x-4.4x, compared to the fully connected BinaryConnect
deterministic network, with negligible loss in accuracy (less than 1%)."
EXPERIMENTAL RESULTS,0.5142857142857142,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.5214285714285715,"Table 1: Accuracy on pruning models for MNIST with ”Number of inputs” connections to each
neuron of current layer to neurons from previous layer."
EXPERIMENTAL RESULTS,0.5285714285714286,"Method
Baseline (1024 inputs)
Number of inputs
Accuracy"
EXPERIMENTAL RESULTS,0.5357142857142857,"BinaryConnect
Deterministic
98.08%"
EXPERIMENTAL RESULTS,0.5428571428571428,"5
94.47%
6
95.19%
7
95.98%
8
96.01%"
EXPERIMENTAL RESULTS,0.55,"Floating Point
98.27%"
EXPERIMENTAL RESULTS,0.5571428571428572,"5
97.51%
6
97.80%
7
97.71%
8
98.06%"
EXPERIMENTAL RESULTS,0.5642857142857143,"Table 2: Accuracy, memory and computation requirements with different percentages of weight
remaining for each neuron while pruning models for CIFAR-10."
EXPERIMENTAL RESULTS,0.5714285714285714,"Percentage
weights
remaining"
EXPERIMENTAL RESULTS,0.5785714285714286,"Accuracy
Memory
Computation
(Ops)"
EXPERIMENTAL RESULTS,0.5857142857142857,"1.0 %
72.89 %
63.787KiB
4.30361e+08
5.0 %
77.09 %
103.047KiB
3.72070e+08
10.0 %
83.41 %
187.953KiB
6.01394e+08
20.0 %
86.20 %
357.359KiB
1.05768e+09
30.0 %
87.54 %
526.047KiB
1.50689e+09
50.0 %
87.71 %
865.672KiB
2.42419e+09
80.0 %
87.86 %
1.340MiB
3.77653e+09
100.0 %
87.64 %
1.672MiB
4.69383e+09"
EXPERIMENTAL RESULTS,0.5928571428571429,"Since we observed in our early experiments with MNIST that dense layers can be pruned further, we
performed a gridsearch with different percentages/number of weights remaining in convolution and
dense layers. The results are given in Table 3. The baseline accuracy for these models is 87.64%.
The results mostly show a trend of increasing accuracy with increasing number of dense/percentage
of convolution weights per neuron apart from a few anomalies. We can also observe that the com-
putation and memory requirements are highly skewed towards reduction in convolution parameters
than dense parameters."
EXPERIMENTAL RESULTS,0.6,"Also note, based on Table 2, Table 3, and our baseline accuracy for fully connected, full precision
model (88.21%), that with less than 3% reduction in accuracy, we were able to save up to 180x in
memory and up to 4.4x in computation."
EXPERIMENTAL RESULTS,0.6071428571428571,"Previous work on this topic (Guerra et al. (2020)), which used layer wise unstructured pruning, was
able to prune BinaryConnect models to 750 KB size with 1.4x reduction in computation while our
models with 1% higher accuracy can be pruned to as low as 526 KB with upto 3.1x reduction in
computation. Compared to pruning full precision models from Li et al. (2016) we reduce memory
by upto 41x (526KB vs 21.6 MB) with 6% reduction in accuracy (Note that the comparisons have
been done with results of closest equivalent VGG models reported by the authors)."
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6142857142857143,"6
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6214285714285714,"As an ablation study, we compared iPrune on a fully trained network for MNIST against iPrune on a
randomly initialized network. Table 5 shows the results of iPrune on a randomly initialized network
vs a fully trained network."
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6285714285714286,"The idea behind this is that the initialization of networks greatly affects the magnitude of the gradient
and subsequently the ﬁnal value after training. Existing work on such ”Lottery Tickets” have been
discussed in Section 3. Those methods use a learnable metric ”score” to determine the mask. This
score is updated while training. Conversely, in this ablation, we compute the mask once, just after
initialization, and assume that this is the ”Lottery Ticket” network. We allow the update of weights
while training to allow the model to ﬁt better to the data instead of updating the mask itself."
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6357142857142857,Under review as a conference paper at ICLR 2022
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6428571428571429,"Table 3: Accuracy, Memory and computation requirements of different combinations of percentage
convolution weights remaining per neuron and number of dense weights remaining per neuron for
iPrune for CIFAR-10 BinaryConnect Deterministic."
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.65,"Dense \ Conv
8%
20%
30%
50 %
7
78.11%
63.569KiB
5.08453e+08"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6571428571428571,"80.93%
128.819KiB
1.05581e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6642857142857143,"82.43%
182.257KiB
1.50408e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6714285714285714,"83.65%
291.382KiB
2.41948e+09
8
78.33%
63.826KiB
5.08455e+08"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6785714285714286,"81.21%
129.076KiB
1.05581e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6857142857142857,"82.92%
182.514KiB
1.50408e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.6928571428571428,"84.30%
291.639KiB
2.41948e+09
10
78.40%
64.344KiB
5.08459e+08"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7,"81.68%
129.594KiB
1.05582e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7071428571428572,"83.01%
183.031KiB
1.50408e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7142857142857143,"85.10%
292.156KiB
2.41949e+09
20
78.84%
66.852KiB
5.08479e+08"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7214285714285714,"82.07%
132.102KiB
1.05584e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7285714285714285,"83.90%
185.540KiB
1.50410e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7357142857142858,"85.49%
294.665KiB
2.41951e+09
50
78.60%
74.359KiB
5.08541e+08"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7428571428571429,"82.33%
139.609KiB
1.05590e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.75,"83.49%
193.047KiB
1.50416e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7571428571428571,"84.79%
302.172KiB
2.41957e+09
100
79.05%
86.859KiB
5.08643e+08"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7642857142857142,"83.29%
152.109KiB
1.05600e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7714285714285715,"84.83%
205.547KiB
1.50427e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7785714285714286,"85.32%
314.672KiB
2.41967e+09"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7857142857142857,"Table 4: Comparison of our results with previous works, Guerra et al. (2020) on binary pruning and
Li et al. (2016) on full precision networks. The source for our results is in Table 2"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.7928571428571428,"Paper
Model
Baseline
Pruned
Memory
Memory
Reduction"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8,"Computation
Reduction
Li
et
al.
(2016)"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8071428571428572,"VGG-16 (Full
Precision)"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8142857142857143,"93.25%
93.40%
21.6MiB
1
28.8x
1.5x"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8214285714285714,"Guerra et al.
(2020)"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8285714285714286,"VGG-11 (Bi-
nary Connect)"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8357142857142857,"87.60%
86.53%
750KiB
1x
1.4x"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8428571428571429,"Ours
Custom VGG
(30% pruned)"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.85,"87.64%
87.54%
526KiB
1.4x
3.1x"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8571428571428571,"Custom VGG
(20% pruned)"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8642857142857143,"87.64%
86.20%
357KiB
2.0x
4.4x"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8714285714285714,"Table 5: Comparison between trained, pruned and retrained network and a randomly initialized,
pruned and trained BinaryConnect Deterministic network. The baseline accuracy for the Train-
Prune-Train case is 87.64%."
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8785714285714286,"Number of inputs
Train-Prune-Train
Prune-Train
20%
86.20%
85.83%
30%
87.54%
86.13%
50%
87.71%
86.52%
80%
87.86%
86.60%"
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8857142857142857,"The results show that a randomly initialized network can be pruned to less than 1.5% reduction in
accuracy for the same training parameters. This encourages further investigation in future works on
larger models to validate our hypothesis. If this phenomenon is always observed, it can greatly help
reduce the computation cost by removing the need to train larger models initially."
IPRUNE FOR RANDOMLY INITIALIZED NETWORKS,0.8928571428571429,Under review as a conference paper at ICLR 2022
CONCLUSION AND FUTURE WORK,0.9,"7
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.9071428571428571,"We were able to demonstrate pruning for binary neural networks and reduce the memory and com-
putation requirement by more than 2200x and 70x on MNIST and 190x and 4.4x on CIFAR-10,
respectively, compared to full precision, fully connected equivalents. Our CIFAR-10 results had 1%
increase in accuracy but was only 70% of the size of the model from Guerra et al. (2020) with a
reduction in computation time by 3.1x vs 1.4x in that paper."
CONCLUSION AND FUTURE WORK,0.9142857142857143,"The ability to prune randomly initialized neural networks and train to reasonably high accuracies is
an interesting outcome and is worth investigating. Finally, extension of these algorithms for other
forms of quantization (logarithmic, k-bit, etc.) would also be an interesting avenue for research."
CONCLUSION AND FUTURE WORK,0.9214285714285714,"We are also working on porting the trained weights to an FPGA to observe the gain in energy
requirements and inference time. With such low number of inputs (fan in), we expect to be able to
ﬁt our neurons in lookup tables to further boost the performance."
REFERENCES,0.9285714285714286,REFERENCES
REFERENCES,0.9357142857142857,"Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. CoRR, abs/1511.00363, 2015. URL http:
//arxiv.org/abs/1511.00363."
REFERENCES,0.9428571428571428,"James Diffenderfer and Bhavya Kailkhura. Multi-prize lottery ticket hypothesis: Finding accu-
rate binary neural networks by pruning a randomly weighted network. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
U_mat0b9iv."
REFERENCES,0.95,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Training pruned neural net-
works. CoRR, abs/1803.03635, 2018. URL http://arxiv.org/abs/1803.03635."
REFERENCES,0.9571428571428572,"Luis Guerra, Bohan Zhuang, Ian D. Reid, and Tom Drummond. Automatic pruning for quantized
neural networks. CoRR, abs/2002.00523, 2020. URL https://arxiv.org/abs/2002.
00523."
REFERENCES,0.9642857142857143,"Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep
learning: Pruning and growth for efﬁcient inference and training in neural networks. CoRR,
abs/2102.00554, 2021. URL https://arxiv.org/abs/2102.00554."
REFERENCES,0.9714285714285714,"Alex Krizhevsky and Geoffrey E. Hinton. Learning multiple layers of features from tiny images.
Master’s thesis, Department of Computer Science, University of Toronto, 2009."
REFERENCES,0.9785714285714285,"Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition”. Proceedings of the IEEE, 86(11):2278–2324, November 1998."
REFERENCES,0.9857142857142858,"Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for
efﬁcient convnets. CoRR, abs/1608.08710, 2016. URL http://arxiv.org/abs/1608.
08710."
REFERENCES,0.9928571428571429,"Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep
learning in NLP. CoRR, abs/1906.02243, 2019. URL http://arxiv.org/abs/1906.
02243."
