Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,"Abstract—Since the introduction of PointNet [1], many current
research methodologies have shifted their focus towards directly
processing point cloud data using deep learning techniques.
However, these studies predominantly concentrate on the analysis
of complete point clouds for 3D affordance analysis, and most
existing models experience a decline in performance when dealing
with incomplete point cloud inputs. The research data from 3D
AffordanceNet [2] indicates that due to the loss of geometric
information in partial point clouds relative to complete ones,
the performance of classic networks such as PointNet++ [3],
DGCNN [4], and U-Net [5] decreases by 2.3% , 4.2%, and 4.4%,
respectively, compared to their performance with complete point
clouds.
Inspired by point cloud completion networks [6] [7] [8] [9],
we initially designed a self-view fusion network that utilizes
multi-view depth image information to observe the incomplete
self-shape and generate a compact global shape. By acquiring
complete view features through the completion network, these are
then inputted into the PointNet++ network to further perform
downstream semantic segmentation tasks."
ABSTRACT,0.015384615384615385,"Index Terms—DEP-Net, 3D Affordance analysis, incomplete
point cloud, AffordanceNet, PointNet++, multi-view depth image,
semantic segmentation, Depth Enhanced"
INTRODUCTION,0.03076923076923077,"I. INTRODUCTION
T
HE concept of affordance focuses on the interaction
between humans and their environment. The capacity to
understand how humans interact with objects through visual
cues, known as visual affordance, is essential for research in
vision-guided robotics. Studies in this area cover tasks such
as the classification, segmentation, and inference of visual
affordance.
Given a 3D point cloud object without known affordance
estimations, the task of point cloud affordance estimation
aims to predict the types of affordances the object supports
and to estimate the per-point probability distribution of its
affordances.
Although a complete point cloud input can provide more
detailed geometric information for affordance estimation, in
real-world scenarios, point clouds obtained from the real world
are often imperfect and noisy due to occlusions among objects,
limited precision of scanning devices, etc. We usually only
have access to partial views of 3D shapes, represented as
partial point clouds. Hence, an important task of interest is
the affordance estimation of partial
Many past studies on affordance analysis have focused on
the 2D and 2.5D domains [10] [11] [12] [13], achieving com-
mendable performance in visual tasks such as classification"
INTRODUCTION,0.046153846153846156,"and semantic segmentation. Since the introduction of Point-
Net, many contemporary research methods have increasingly
focused on directly processing point cloud data using deep
learning techniques. However, these studies on 3D model
affordance analysis are mostly concerned with complete point
clouds. Data from the 3D AffordanceNet study [2] indicates
that due to the loss of geometric information in partial point
clouds relative to complete ones, the performance of classic
networks such as PointNet++ [3], DGCNN [4], and U-Net [5]
each experience a decline in performance compared to their
use with complete point clouds.
Inspired by the point cloud completion network [6] [7] [8]
[9], we can optimize the understanding of the global shape
of point clouds and the recovery of details by supplementing
information for feature fusion and point cloud repair.Generally,
when humans observe objects that are difficult to distinguish,
they adopt multi-view observations. Thus, it can be considered
to use multi-view depth maps and original point cloud data for
feature fusion to complete the missing geometric features.
In summary, this paper, inspired by related work in the point
cloud completion field, proposes the use of depth maps to
augment the original point cloud data. The depth maps are
directly generated by projecting the point cloud itself from
controllable viewpoints during the data preprocessing stage.
This module aims to observe the partial input from different
angles and learn effective descriptors to produce a globally
coherent and complete feature representation.
Our contributions are as follows:"
INTRODUCTION,0.06153846153846154,"• We propose a novel approach that utilizes depth maps
generated from multiple viewpoints to enhance the un-
derstanding of incomplete point clouds. This method
improves the global shape representation, which is critical
for downstream tasks such as semantic segmentation."
INTRODUCTION,0.07692307692307693,"• We introduce the View Enhanced Depth Map (VEDM)
module, which effectively fuses features from 2D depth
maps with 3D point cloud features, leading to a more
robust and complete representation of the point cloud
data."
INTRODUCTION,0.09230769230769231,"• We demonstrate through our experiments on the 3D Af-
fordanceNet dataset that our DEP-Net model outperforms
the classic PointNet++ in terms of mean Average Preci-
sion (mAP), Average Intersection Over Union (aIOU),
and Mean Squared Error (MSE) on various affordance
categories."
INTRODUCTION,0.1076923076923077,"COMPUTER SCIENCE UNDERGRADAUTE CONFERENCE 2024 @ XJTU
2"
INTRODUCTION,0.12307692307692308,research in the field of 3D computer vision and robotics.
INTRODUCTION,0.13846153846153847,II. RELATED WORK
INTRODUCTION,0.15384615384615385,"In 2017, the pioneering PointNet [1], a deep learning-based
technolmkmmlogy, was introduced, and since then, many
current research methods have increasingly focused on directly
processing point cloud data using deep learning techniques [5]
[14] [15] [16]. Researchers start from raw point cloud data,
directly processing it, which not only makes better use of
the information from each point but also reduces noise and
computational errors. However, the performance loss caused
by the loss of geometric information due to missing parts of
the point cloud for incomplete point cloud inputs is almost
inevitable."
INTRODUCTION,0.16923076923076924,A. PointNet & PointNet++
INTRODUCTION,0.18461538461538463,"PointNet: It directly processes point cloud data, using shared
multilayer perceptrons (MLPs) to extract features from each
point in the input point cloud data. Global features are obtained
through max pooling.PointNet was designed following two
main principles:
Permutation Invariance: Since point cloud data is un-
ordered, when an NxD point cloud data is arbitrarily shuffled
in the N dimension, it still represents the same object and
does not affect the overall representation of the object. This is
known as the permutation invariance of point clouds. For the
characteristic that point clouds have permutation invariance,
the designed network must be a symmetric function, that is:"
INTRODUCTION,0.2,"f(x1, x2, ..., xn) = f(xπ1, xπ2, ..., xπn), Xi ∈RD
(1a)"
INTRODUCTION,0.2153846153846154,"Common symmetric functions include SUM and MAX:
f(x1, x2, ..., xn) =
max{x1, x2, ..., xn}
f(x1, x2, ..., xn) =
x1 + x2 + ... + xn
(2a)"
INTRODUCTION,0.23076923076923078,"PointNet uses the max function as the max pooling layer
to aggregate the local features extracted from all points.
Rotation Invariance:
The rotation invariance of point
clouds means that when the point cloud data is subjected to
certain rigid changes (rotation or translation), the coordinates
(x, y, z) of all points change, but it still represents the same
object. PointNet introduces a T-Net network to learn the
rotation of point clouds, and then inputs the calibrated point
clouds into PointNet to complete classification or segmentation
tasks.
The structure of PointNet is very concise. It achieved state-
of-the-art performance using simple MLP + max pooling.
PointNet only considers global features, directly and bru-
tally max pooling all the points into a global feature, thus
losing the local information of each point, and the connection
between local points is not learned by the network. To solve
this problem, PointNet++ was further proposed. PointNet++
introduces a hierarchical neural network that recursively ap-
plies PointNet to the nested partitions of the input point set.
By utilizing metric space distances, PointNet++ can learn local
features with continually increasing context scales."
INTRODUCTION,0.24615384615384617,B. 3DCTN
INTRODUCTION,0.26153846153846155,"The 3D Convolution-Transformer Network (3DCTN) [17]
integrates convolution into the Transformer to effectively learn
local and global features of point cloud classification, and
achieves competitive results using state-of-the-art classification
methods. The network consists of two main modules: multi-
scale local feature aggregation and global feature learning,
both operating on the downsampled point set and implemented
through graph convolution and Transformer, respectively.
Local Feature Aggregating Block: The local feature ex-
traction module of 3DCTN inherits the concept of PointNet++,
using Farthest Point Sampling (FPS) to obtain a subset of
point clouds, referred to as the sampled point set. At the
same time, to ensure the diversity of the receptive field
of the sampling points, multi-scale neighborhoods for each
sampling point are constructed through ball query grouping.
For each neighborhood of the sampling points, the innovation
of 3DCTN lies in proposing a context fusion method to encode
and combine the coordinate and feature information of the
neighborhood, followed by edge convolution to aggregate local
features.
Global Feature Learning Block (GFL): 3DCTN uses the
aggregated features Y = {yi}i=S
i=1 as input, where S is the
number of points, and the Global Feature Learning (GFL)
module has two main components: self-attention mechanism
and position encoding. There are no embedding inputs (words)
in the GFL block, as Y in the LFA block can be considered
as the embedded input for the GFL block."
IMPLEMENTATION/METHODS,0.27692307692307694,III. DEP-NET
IMPLEMENTATION/METHODS,0.2923076923076923,"The input to DEP-Net is composed of three parts: an
incomplete low-resolution point cloud Pin ⊆RN×3, NV
camera positions V P ⊆RNV ×3 (three orthogonal views),
and NV depth maps D ⊆RNV ×1×H×W . Given these inputs,
our objective is to use the depth maps to ameliorate the
loss of geometric information in the incomplete point cloud
and obtain an enhanced point cloud feature representation
P2 ⊆RN2×3. The overall architecture, depicted in Figure 1,
includes two main components: the VEDM module and the
traditional PointNet++ network."
IMPLEMENTATION/METHODS,0.3076923076923077,A. View Enhance by Depth Maps (VEDM)
IMPLEMENTATION/METHODS,0.3230769230769231,"COMPUTER SCIENCE UNDERGRADAUTE CONFERENCE 2024 @ XJTU
3"
IMPLEMENTATION/METHODS,0.3384615384615385,Fig. 1. structure of the DEP-NET
IMPLEMENTATION/METHODS,0.35384615384615387,Linear
IMPLEMENTATION/METHODS,0.36923076923076925,"Duplicate
FV VP c"
IMPLEMENTATION/METHODS,0.38461538461538464,"Linear ×
×"
IMPLEMENTATION/METHODS,0.4,"Softmax
Max
Pc
Fg
Decoder
c
FP"
IMPLEMENTATION/METHODS,0.4153846153846154,"c
Concatenation"
IMPLEMENTATION/METHODS,0.4307692307692308,"Element-wise Summation
× Matrix Multiplication Q K"
IMPLEMENTATION/METHODS,0.4461538461538462,"V
F'V
FP"
IMPLEMENTATION/METHODS,0.46153846153846156,Fig. 2. Illustration of the feature fusion module in the SVDFormer.
IMPLEMENTATION/METHODS,0.47692307692307695,"In our approach to effectively integrating the cross-modal
features described earlier, we took inspiration from the SVD-
Former’s research insights. The domain difference between
2D and 3D data representations leads to suboptimal results
when simply concatenating these features. To overcome this,
we applied the feature fusion strategy from SVDFormer to
combine the global features Fp from the point cloud and
the view features Fd from the depth maps, resulting in a
comprehensive global shape descriptor Fg. This descriptor is
then processed by a decoder to produce the global features Pc.
The decoder employs 1D Conv-Transpose layers to convert
Fg into individual point features and uses self-attention layers
to estimate the 3D coordinates of each point. Building on
techniques from prior works, we merge the generated Pc with
the input Pin and resample this combined data to produce an
initial coarse shape P0.
As depicted in the figure, the feature fusion process begins
with the transformation of FV into query, key, and value
components through linear layers, under the influence of the
global shape features FP . The next step is to enhance the
distinctiveness of the view features by calculating attention
weights. These weights are based on the queries and keys,
and are adjusted according to the projected viewpoints V P.
We linearly transform V P to a latent space and utilize them as"
IMPLEMENTATION/METHODS,0.49230769230769234,"positional cues to facilitate the feature fusion. By conducting
element-wise multiplication, each element in F ′
d amalgamates
relational data from different views, steered by FP . The
culmination of this process is the derivation of the output shape
descriptor Fg from F ′
d, which is obtained through max pooling."
IMPLEMENTATION/METHODS,0.5076923076923077,"B.
PointNet++"
IMPLEMENTATION/METHODS,0.5230769230769231,"For the affordance analysis network, we employed the
classic PointNet++ network for downstream semantic segmen-
tation tasks. PointNet++ introduced a hierarchical neural net-
work that recursively applies PointNet to nested subdivisions
of the input point set. By leveraging metric space distances,
PointNet++ is able to learn local features with incrementally
increasing contextual scales. The network structure is illus-
trated in 3
Each set abstraction layer group in the network primarily
consists of three parts: Sampling layer, Grouping layer, and
PointNet layer."
IMPLEMENTATION/METHODS,0.5384615384615384,"• Sample layer: Farthest Point Sampling (FPS) is used to
sample the input points, which, compared to random
sampling, can cover the entire sampling space more
effectively."
IMPLEMENTATION/METHODS,0.5538461538461539,"• Grouping layer: Utilizes the sampled centroid points to
divide the point set into N local regions using the Ball
query method;"
IMPLEMENTATION/METHODS,0.5692307692307692,"COMPUTER SCIENCE UNDERGRADAUTE CONFERENCE 2024 @ XJTU
4"
IMPLEMENTATION/METHODS,0.5846153846153846,Fig. 3. structure of the pointnet++
IMPLEMENTATION/METHODS,0.6,"We use the Cross-Entropy Loss function LCE to train the
network."
RESULTS/EXPERIMENTS,0.6153846153846154,IV. EXPERIMENT
RESULTS/EXPERIMENTS,0.6307692307692307,A. Dataset
RESULTS/EXPERIMENTS,0.6461538461538462,"In this paper, we utilize the 3D AffordanceNet dataset [2],
which is an extension based on PartNet [18] and incorporates
fine-grained part hierarchy information of 3D shapes from
large 3D CAD model datasets such as ShapeNet [19] and 3D
Warehouse. 3D AffordanceNet, as introduced by Deng, et al.
[2], is a functional affordance dataset based on 3D point cloud
data, consisting of 56,307 well-defined affordance annotations
for 22,949 point cloud shapes, covering 18 affordance cate-
gories and 23 semantic object categories. It is also the first
large-scale dataset with well-defined probabilistic distribution
annotations for affordances. Based on this dataset, we evaluate
the affordance understanding capabilities of the PointNet++
network enhanced with depth maps and the classic PointNet++
network."
RESULTS/EXPERIMENTS,0.6615384615384615,B. Training Details
RESULTS/EXPERIMENTS,0.676923076923077,"We train our network on a point cloud dataset with shape
sizes of 1024 points. For the original dataset, we obtain a
subset of size N ∗1024 ∗3 using Farthest Point Sampling
(FPS) during the data preprocessing process. We set the initial
learning rate to 0.001 and use the Adam optimizer to optimize
parameters, reducing the learning rate by half every 20 epochs.
We train the network for 100 epochs with a batch size of 128.
The weight decay for the Adam optimizer is set to 1e-8. We
use the cosine annealing algorithm to adjust the learning rate,
which can be described by the following equation:"
RESULTS/EXPERIMENTS,0.6923076923076923,ηt = ηmin + 1
RESULTS/EXPERIMENTS,0.7076923076923077,2(ηmax −ηmin)(1 + cos( Tcur
RESULTS/EXPERIMENTS,0.7230769230769231,"Tmax
π))
(3)"
RESULTS/EXPERIMENTS,0.7384615384615385,"where ηt is the adjusted learning rate, ηmin is the minimum
learning rate, ηmax is set as the initial learning rate, and Tcur
is the number of the current epoch. We set the batch size to
128 and train the network for 100 epochs."
RESULTS/EXPERIMENTS,0.7538461538461538,C. Results
RESULTS/EXPERIMENTS,0.7692307692307693,"TABLE I
AFFORDANCE ESTIMATION RESULTS"
RESULTS/EXPERIMENTS,0.7846153846153846,"Metric
P mAP
P AUC
P aIOU
P MSE
D mAP
D AUC
D aIOU
D MSE
Avg
45.7
85.2
16.9
0.062
46.4
85.1
17.5
0.056
Grasp
43.2
81.2
14.4
0.003
39.0
79.5
13.0
0.003
Lift
80.6
96.2
45.6
0.0001
79.4
90.1
42.2
0.00005
Contain
41.9
83.3
13.2
0.005
44.2
83.6
13.1
0.005
Open
48.5
87.9
21.6
0.003
51.3
89.8
23.2
0.003
Lay
52.6
86.7
25.2
0.0006
43.3
83.8
19.9
0.0004
Sit
69.8
95.0
31.0
0.004
71.9
95.7
31.4
0.004
Support
45.5
86.5
11.2
0.013
46.9
88.3
14.9
0.011
Wrap.
20.0
71.3
3.6
0.002
18.3
66.6
4.4
0.003
Pour
47.0
88.4
17.8
0.002
48.6
87.8
18.7
0.002
Display
52.5
85.2
19.3
0.002
56.2
86.8
28.1
0.002
Push
24.1
84.9
5.7
0.0002
23.2
83.2
4.4
0.0002
Pull
36.5
86.1
11.5
0.0001
51.1
84.7
16.2
0.00006
Listen
42.1
84.2
13.4
0.0007
49.4
86.9
13.9
0.0003
Wear
15.3
64.1
2.4
0.0004
16.9
68.3
2.6
0.0003
Press
30.7
84.7
12.4
0.0006
34.8
86.6
10.6
0.0004
Move
37.8
79.6
6.2
0.025
37.0
79.3
10.2
0.022
Cut
43.3
90.2
13.5
0.0003
35.8
89.9
10.5
0.0003
Stab
92.6
98.8
37.8
0.0001
88.1
99.3
37.0
0.00009"
RESULTS/EXPERIMENTS,0.8,"COMPUTER SCIENCE UNDERGRADAUTE CONFERENCE 2024 @ XJTU
5"
RESULTS/EXPERIMENTS,0.8153846153846154,"with improvements of 0.7%, 0.6%, and a 9.7% decrease in
MSE, respectively. Except for the MSE score, all others are
displayed as percentages, with higher scores indicating better
performance. The algorithms P and D represent PointNet++
[3] and DEP-Net (Ours), respectively. The terms Contain,
Open, Sit, Support, Pour, Display, Pull, Listen, Wear, Press,
Grasp, Lift, Lay, Wrap, Push, Move, Cut, and Stab represent
different affordance categories. In the analysis of specific
affordance categories, DEP-Net outperforms PointNet++ in
predicting accuracies for affordances such as Contain, Open,
Sit, Support, Pour, Display, Pull, Listen, Wear, and Press.
However, for affordance categories like Grasp, Lift, Lay,
Wrap, Push, Move, Cut, and Stab, our DEP-Net experienced
some performance decline. Interestingly, the shapes that saw
performance improvements tend to have flatter and generally
larger surface areas compared to those where performance
decreased, suggesting that DEP-Net may be less sensitive to
minute edge details and features in convex regions."
CONCLUSION/DISCUSSION,0.8307692307692308,V. PROBLEMS & PLANS
CONCLUSION/DISCUSSION,0.8461538461538461,A. Problems
CONCLUSION/DISCUSSION,0.8615384615384616,"For the overall network, the structure is not sufficiently
streamlined. We believe there is redundancy between the
feature extraction of 3D point clouds in the depth map feature
extraction and the feature extraction of point clouds in the
downstream semantic segmentation task. It might be possible
to merge the two PointNet set abstraction feature extraction
processes to optimize network efficiency."
CONCLUSION/DISCUSSION,0.8769230769230769,B. Plans
CONCLUSION/DISCUSSION,0.8923076923076924,"With the significant advancements in the fields of natural
language processing (NLP) and computer vision, Transformer
models have demonstrated superior capabilities in learning
global features. Consequently, they have been deployed in a
variety of point cloud processing applications [20] [21] [22],
including object classification, semantic scene segmentation,
and object part segmentation. At the heart of the Transformer
model lies the self-attention mechanism, which initially com-
putes the similarity between pairs of embedded tokens, and
then employs these similarities to create a weighted sum of
all tokens, yielding a new set of outputs. This process enables
each output token to be connected with every input token,
which underpins the Transformer’s adeptness at capturing
global features. Therefore, replacing conventional convolu-
tional operations in the network with Transformer models may
enhance feature representation. This is the objective we aim to
pursue in our future work. We anticipate that by leveraging the
Transformer architecture, we can refine the network’s ability
to perceive detailed features within point clouds, thereby
improving performance in predicting affordances."
CONCLUSION/DISCUSSION,0.9076923076923077,VI. CONCLUSION
CONCLUSION/DISCUSSION,0.9230769230769231,"In this work, we introduced DEP-Net, an optimized version
of PointNet++ designed to address the challenges of semantic
segmentation in incomplete point clouds by leveraging pro-
jected depth maps. Our approach demonstrates that the inte-
gration of depth maps as additional features can enhance the"
CONCLUSION/DISCUSSION,0.9384615384615385,"performance of point cloud processing networks, particularly
in the context of affordance analysis. The VEDM module
within DEP-Net effectively captures the global shape and
recovers missing geometric information, resulting in improved
segmentation accuracy.
The experimental results on the 3D AffordanceNet dataset
confirmed the efficacy of our method, showing that DEP-
Net outperforms the classic PointNet++ in various affordance
categories. This advancement suggests that the consideration
of different views and the fusion of 2D and 3D data can lead
to a more robust understanding of 3D shapes, which is crucial
for applications in robotics and computer vision.
However, our approach also revealed certain limitations,
such as potential redundancies in feature extraction and a
lack of sensitivity to minute details in some affordance
categories. Our future work will focus on streamlining the
network structure to eliminate redundancies and exploring the
integration of Transformer models to improve the network’s
capability to capture detailed features. By incorporating these
advancements, we aim to push the boundaries of point cloud
processing and affordance prediction, paving the way for more
intelligent and capable vision-guided robotic systems."
CONCLUSION/DISCUSSION,0.9538461538461539,VII. REFERENCES SECTION
REFERENCES,0.9692307692307692,REFERENCES
REFERENCES,0.9846153846153847,"COMPUTER SCIENCE UNDERGRADAUTE CONFERENCE 2024 @ XJTU
6"
