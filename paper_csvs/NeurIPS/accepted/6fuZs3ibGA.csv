Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016207455429497568,"In consequential domains, it is often impossible to compel individuals to take treat-
ment, so that optimal treatment assignments are merely suggestions when humans
make the final treatment decisions. On the other hand, there can be different hetero-
geneity in both the actual response to treatment and final treatment decisions given
recommendations. For example, in social services, a persistent puzzle is the gap in
take-up of beneficial services among those who may benefit from them the most.
When decision-makers have equity- for fairness-minded preferences over both ac-
cess and average outcomes, the optimal decision rule changes due to these differing
heterogeneity patterns. We study identification and improved/robust estimation
under potential violations of positivity. We consider fairness constraints such as
demographic parity in treatment take-up, and other constraints, via constrained opti-
mization. We develop a two-stage, online learning-based algorithm for solving over
parametrized policy classes under general constraints to obtain variance-sensitive
regret bounds. Our framework can be extended to handle algorithmic recommenda-
tions under an often-reasonable covariate-conditional exclusion restriction, using
our robustness checks for lack of positivity in the recommendation."
INTRODUCTION,0.0032414910858995136,"1
Introduction"
INTRODUCTION,0.004862236628849271,"The intersection of causal inference and machine learning for heterogeneous treatment effect estima-
tion can improve public health, increase revenue, and improve outcomes by personalizing treatment
decisions, such as medications, e-commerce platform interactions, and social interventions, to those
who benefit from it the most [4, 30, 35, 49]. But, in many important settings, we do not have direct
control over treatment, and can only optimize over encouragements, or recommendations for treat-
ment. For example, in e-commerce, companies can rarely compel users to sign up for certain services,
rather nudge or encourage users to sign up via promotions and offers. When we are interested
in optimizing the effects of signing up – or other voluntary actions beyond a platform’s control –
on important final outcomes such as revenue, we therefore need to consider fairness-constrained
optimal encouragement designs. Often human expert oversight is required in the loop in important
settings where ensuring fairness in machine learning [6] is also of interest: doctors prescribe treat-
ment from recommendations [31], managers and workers combine their expertise to act based on
decision support [8], and in the social sector, caseworkers assign to beneficial programs based on
recommendations from risk scores that support triage [14, 19, 48]."
INTRODUCTION,0.006482982171799027,The human in the loop requires new methodology for optimal encouragement designs because
INTRODUCTION,0.008103727714748784,"when the human in the loop makes the final prescription, algorithmic recommenda-
tions do not have direct causal effects on outcomes; they change the probability of
treatment assignment."
INTRODUCTION,0.009724473257698542,"On the other hand, this is analogous to the well-understood notion of non-compliance/non-adherence
in randomized controlled trials in the real world [22, 21]. For example, patients who are prescribed
treatment may not actually take medication. A common strategy is to conduct an intention-to-treat
analysis: under assumptions of no unobserved confounders affecting treatment take-up and outcome,
we may simply view encouragement as treatment. But, in the case of prediction-informed decisions in
social settings, if we are concerned about access to the intervention in addition to utility of the policy
over the population, finer-grained analysis is warranted. If an outcome-optimal policy results in wide
disparities in access, for example in marginalized populations not taking up incentives for healthy food
due to lack of access in food deserts, or administrative burden that screens out individuals applying for
social services that could benefit the most, this could be a serious concern for decision-makers. We
ultimately may seek optimal decision rules that improve disparities in treatment access. In contrast,
previous work in algorithmic accountability primarily focuses on auditing recommendations, but not
both the access and efficacy achieved under the final decision rule. Therefore, previous methods can
fall short in mitigating potential disparities."
INTRODUCTION,0.011345218800648298,"Our contributions are as follows: we characterize optimal and resource fairness-constrained optimal
decision rules, develop statistically improved estimators and robustness checks for the setting of
algorithmic recommendations with sufficiently randomized decisions. We also develop methodology
for optimizing over a constrained policy class with less conservative out-of-sample fairness constraint
satisfaction by a two-stage procedure, and we provide sample complexity bounds. We assess improved
recommendation rules in a stylized case study of optimizing recommendation of supervised release
in the PSA-DMF pretrial risk-assessment tool while reducing surveillance disparities."
RELATED WORK,0.012965964343598054,"2
Related Work"
RELATED WORK,0.014586709886547812,"In the main text, we briefly highlight the most relevant methodological and substantive work and
defer additional discussion to the appendix."
RELATED WORK,0.01620745542949757,"Optimal encouragement designs/policy learning with constraints. There is extensive literature on
off-policy evaluation and learning, empirical welfare maximization, and optimal treatment regimes
[5, 49, 35, 30]. [39] studies an optimal individualized encouragement design, though their focus is on
optimal individualized treatment regimes with instrumental variables. [27] study fairness in pricing,
and some of the desiderata in that setting on revenue (here, marginal welfare) and demand (take-up)
are again relevant here, but in a more general setting beyond pricing. The most closely related work
in terms of problem setup is the formulation of “optimal encouragement designs"" in [39]. However,
they focus on knapsack resource constraints, which have a different solution structure than fairness
constraints. Their outcome models in regression adjustment are conditional on the recommended/not
recommended partitions which would not allow our fairness constraints that introduce treatment-
and group-dependent costs. [44] has studied uniform feasibility in constrained resource allocation,
but without encouragement or fairness. [9] studies robust extrapolation in policy learning from
algorithmic recommendation, but not fairness. Our later case study is on supervised release, where
there is a lot of randomness in final treatment decisions, rather than pretrial detention."
RELATED WORK,0.017828200972447326,"Fair off-policy learning We highlight some most closely related works in off-policy learning
(omitting works in the sequential setting). [37] studies high-probability fairness constraint satisfaction.
[29] studies doubly-robust causal fair classification, while others have imposed deterministic resource
constraints on the optimal policy formulation [13]. [26] studies (robust) bounds for treatment
responders in binary outcome settings; this desiderata is coupled to classification notions of direct
treatment. Again, our focus is on modeling the fairness implications of non-adherence. Indeed, in
order to provide general algorithms and methods, we do build on prior fair classification literature. A
different line of work studies “counterfactual"" risk assessments which models a different concern."
RELATED WORK,0.019448946515397084,"Other causal methodology for intention-to-treat We focus on deriving estimators for intention-to-
treat analyses in view of fairness constraints (which result in group-specific welfare weights). Our
interest is in imposing separate desiderata on treatment realizations under non-compliance; but we
don’t conduct instrumental variable inference and we assume unconfoundedness holds. We argue ITT
is policy-relevant whereas complier-strata specific analysis is less policy-relevant since the compliers
are unknown. Since our primary interest is in characterizing fair optimal decision rules, we don’t
model this as a mediation analysis problem (isolating the impact of recommendation even under
the same ultimate treatment in a nested counterfactual), which may be more relevant for descriptive"
RELATED WORK,0.02106969205834684,"characterization. [27] studies multi-objective desiderata for pricing and notes intention-to-treat
structure in pricing, but not fairness considerations in more general problems. A related literature
studies principal stratification [24], which has similar policy-relevance disadvantages regarding
interpretability as complier analysis does."
PROBLEM SETUP,0.022690437601296597,"3
Problem Setup"
PROBLEM SETUP,0.024311183144246355,"We briefly describe the problem setup. We work in the Neyman-Rubin potential outcomes framework
for causal inference [40]. We define the following:"
PROBLEM SETUP,0.02593192868719611,"• recommendation flag R ∈{0, 1}, where R = 1 means encouraged/recommended. (We will
use the terms encouragement/recommendation interchangeably)."
PROBLEM SETUP,0.027552674230145867,"• treatment T(R) ∈T , where T(r) = 1 indicates the treatment decision was 1 when the
recommendation reported r."
PROBLEM SETUP,0.029173419773095625,• outcome Y (t(r)) is the potential outcome under encouragement r and treatment t.
PROBLEM SETUP,0.03079416531604538,"Regarding fairness, we will be concerned about disparities in utility and treatment benefits (resources
or burdens) across different groups, denoted A ∈{a, b}. (For notational brevity, we may generically
discuss identification/estimation without additionally conditioning on the protected attribute). For
example, recommendations arise from binary high-risk/low-risk labels of classifiers. In practice, in
consequential domains, classifier decisions are rarely automated, rather used to inform humans in the
loop. The human expert in the loop decides whether or not to assign treatment. For binary outcomes,
we will interpret Y (t(r)) = 1 as the positive outcome, and when treatments are also binary, we may
further develop analogues of fair classification criteria. We let c(r, t, y): {0, 1}3 7→R denote the
cost function for r ∈{0, 1}, t ∈T , y ∈{0, 1}, which may sometimes be abbreviated crt(y). We
discuss identification and estimation based on the following recommendation, treatment propensity,
and outcome models:"
PROBLEM SETUP,0.03241491085899514,"er(X, A) := P(R = r | X, A), pt|r(X, A) := P(T = t | R = r, X, A),"
PROBLEM SETUP,0.03403565640194489,"µr,t(X, A) := E[crt(Y ) | R = r, T = t, X, A] = E[crt(Y ) | T = t, X, A] := µt(X, A) (asn.2)"
PROBLEM SETUP,0.03565640194489465,"We are generally instead interested in personalized recommendation rules π(r | X) = πr(X)
which describes the probability of assigning the recommendation r to covariates X. The average
encouragement effect is the difference in average outcomes if we refer everyone vs. no one, while the
encouragement policy value V (π) is the population expectation induced by the potential outcomes
and treatment assignments realized under a recommendation policy π."
PROBLEM SETUP,0.03727714748784441,"AEE = E[Y (T(1)) −Y (T(0))],
V (π) = E[c(π, T(π), Y (π))]."
PROBLEM SETUP,0.03889789303079417,"Because algorithmic decision makers may be differentially responsive to recommendation, and
treatment effects may be heterogeneous, the optimal recommendation rule may differ from the
(infeasible) optimal treatment rule when taking constraints into account or for simpler policy classes."
PROBLEM SETUP,0.04051863857374392,Assumption 1 (Consistency and SUTVA ). Yi = Yi(Ti(Ri)).
PROBLEM SETUP,0.04213938411669368,"Assumption 2 (Conditional exclusion restriction). Y (T(R)) ⊥⊥R | T, X, A."
PROBLEM SETUP,0.04376012965964344,"Assumption 3 (Unconfoundedness). Y (T(r)) ⊥⊥T(r) | X, A."
PROBLEM SETUP,0.04538087520259319,"Assumption 4 (Stable responsivities under new recommendations). P(T = t | R = r, X) remains
fixed from the observational to the future dataset."
PROBLEM SETUP,0.04700162074554295,"Assumption 5 (Decomposable costs). c(r, t, y) = cr(r) + ct(t) + cy(y)"
PROBLEM SETUP,0.04862236628849271,"Assumption 6 (Overlap). νr ≤er(X, A) ≤1 −νr; νt ≤pt|r(X, A) ≤1 −νt; νr, νt ≤0"
PROBLEM SETUP,0.050243111831442464,"Our key assumption beyond standard causal inference assumptions is the conditional exclusion
restriction assumption 2, i.e. that conditional on observable information X, the recommendation
has no causal effect on the outcome beyond its effect on increasing treatment probability. This
assumes that all of the covariate information that is informative of downstream outcomes is measured.
Although this may not exactly hold in all applications, stating this assumption is also a starting
point for sensitivity analysis under violations of it [25]. Assuming assumption 6 is like assuming"
PROBLEM SETUP,0.05186385737439222,"we consider a randomized controlled trial with nonadherence. But later we give arguments using
robustness to go beyond this, leveraging our finer-grained characterization."
PROBLEM SETUP,0.05348460291734198,"Assumption 4 is a structural assumption that limits our method to most appropriately re-optimize
over small changes to existing algorithmic recommendations. This is also required for the validity of
intention-to-treat analyses. For example, p0|1(x) (disagreement with algorithmic recommendation)
could be a baseline algorithmic aversion. Not all settings are appropriate for this assumption. We don’t
assume micro-foundations on how or why human decision-makers were deviating from algorithmic
recommendations, but take these patterns as given. One possibility for relaxing this assumption is via
conducting sensitivity analysis, i.e. optimizing over unknown responsivity probabilities near known
ones."
PROBLEM SETUP,0.055105348460291734,"Later on, we will be particularly interested in constrained formulations on the intention-to-treat effect
that impose separate desiderata on outcomes under treatment, as well as treatment."
METHOD,0.05672609400324149,"4
Method"
METHOD,0.05834683954619125,"We consider two settings: in the first, R is (as-if) randomized and satisfies overlap. Then R can
be interpreted as intention to treat or prescription, whereas T is the actual realization thereof. We
study identification of optimal encouragement designs with potential constraints on treatment or
outcome utility patterns by group membership. We characterize optimal unconstrained/constrained
decisions under resource parity. In the second, R is an algorithmic recommendation that does not
satisfy overlap in recommendation (but there is sufficient randomness in human decisions to satisfy
overlap in treatment): we derive robustness checks in this setting by being robust. First we discuss
causal identification in optimal encouragement designs.
Proposition 1 (Regression adjustment identification)."
METHOD,0.059967585089141004,"E[c(π, T(π), Y (π))] = P"
METHOD,0.06158833063209076,"t∈T ,r∈{0,1} E[πr(X)µt(X)pt|r(X)]"
METHOD,0.06320907617504051,Proof of Proposition 1.
METHOD,0.06482982171799027,"E[c(π, T(π), Y (π))] = P"
METHOD,0.06645056726094004,"t∈T ,r∈{0,1} E[πr(X)E[I [T(r) = t] crt(Y (r, t)) | R = r, X]] = P"
METHOD,0.06807131280388978,"t∈T ,r∈{0,1} E[πr(X)P(T = t | R = r, X)E[crt(Y (r, t)) | R = r, X]] = P"
METHOD,0.06969205834683954,"t∈T ,r∈{0,1} E[πr(X)P(T = t | R = r, X)E[crt(Y ) | T = t, X]]"
METHOD,0.0713128038897893,"where the last line follows by the conditional exclusion restriction (Assumption 2) and consistency
(Assumption 1)."
METHOD,0.07293354943273905,"Resource-parity constrained optimal decision rules
We consider a resource/burden parity fairness
constraint:"
METHOD,0.07455429497568881,"V ∗
ϵ = max
π
{E[c(π, T(π), Y (π))]: E[T(π) | A = a] −E[T(π) | A = b] ≤ϵ}
(1)"
METHOD,0.07617504051863858,"Enforcing absolute values, etc. follows in the standard way. Not all values of ϵ may be feasible; in
the appendix we give an auxiliary program to compute feasible ranges of ϵ. We first characterize a
threshold solution when the policy class is unconstrained.
Proposition 2 (Threshold solutions). Define"
METHOD,0.07779578606158834,"L(λ, X, A) = (p1|1(X, A)−p1|0(X, A))

τ(X, A) +
λ
p(A)(I [A = a] −I [A = b])

+λ(p1|0(X, a)−p1|0(X, b))"
METHOD,0.07941653160453808,"λ∗∈arg min
λ
E[L(λ, X, A)+], π∗(x, u) = I{L(λ∗, X, u) > 0}"
METHOD,0.08103727714748785,"If instead d(x) is a function of covariates x only,"
METHOD,0.08265802269043761,"λ∗∈arg min
λ
E[E[L(λ, X, A) | X]+], π∗(x) = I{E[L(λ∗, X, A) | X] > 0}"
METHOD,0.08427876823338736,"Establishing this threshold structure (follows by duality of infinite-dimensional linear programming)
allows us to provide a generalization bound argument."
GENERALIZATION,0.08589951377633712,"4.1
Generalization"
GENERALIZATION,0.08752025931928688,"Proposition
3
(Policy
value
generalization).
Assume
the
nuisance
models
η
=
[p1|0, p1|1, µ1, µ0]⊤, η
∈H are consistent and well-specified with finite VC-dimension Vη
over the product function class H. Let Π = {I{E[L(λ, X, A; η) | X] > 0: λ ∈R; η ∈F}."
GENERALIZATION,0.08914100486223663,"supπ∈Π,λ∈R |(En[πL(λ, X, A)] −E[πL(λ, X, A)])| = Op(n−1 2 )"
GENERALIZATION,0.09076175040518639,"This bound is stated for known nuisance functions: verifying stability under estimated nuisance
functions further requires rate conditions."
GENERALIZATION,0.09238249594813615,"Doubly-robust estimation
We may improve statistical properties of estimation by developing
doubly robust estimators which can achieve faster statistical convergence when both the probability
of recommendation assignment (when it is random), and the probability of outcome are consistently
estimated; or otherwise protect against misspecification of either model. We first consider the ideal
setting when algorithmic recommendations are randomized so that er(X) = P(R = r | X)."
GENERALIZATION,0.0940032414910859,Proposition 4 (Variance-reduced estimation).
GENERALIZATION,0.09562398703403566,"V (π) =
X"
GENERALIZATION,0.09724473257698542,"t∈T ,r∈{0,1}
E

πr(X)
I [R = r]"
GENERALIZATION,0.09886547811993517,"er(X) (I[T = t]cr1(Y ) −µ1(X)pt|r(X)) + µ1(X)pt|r(X)
"
GENERALIZATION,0.10048622366288493,E[T(π)] = P
GENERALIZATION,0.10210696920583469,"r∈{0,1} E
h
πr(X)
n
I[R=r]"
GENERALIZATION,0.10372771474878444,"er(X) (T(r) −p1|r(x)) + p1|r(x)
oi"
GENERALIZATION,0.1053484602917342,"Although similar characterization appears in [39] for the doubly-robust policy value alone, note that
doubly-robust versions of the constraints we study would result in differences in the Lagrangian
so we retain the full expression rather than simplifying. For example, for regression adjustment,
Proposition 9 provides interpretability on how constraints affect the optimal decision rule. In
the appendix we provide additional results describing extensions of Proposition 8 with improved
estimation."
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.10696920583468396,"4.2
Robust estimation with treatment overlap but not recommendation overlap"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.1085899513776337,"When recommendations are e.g. the high-risk/low-risk labels from binary classifiers, we may not
satisfy the overlap assumption, since algorithmic recommendations are deterministic functions of
covariates. However, note that identification in Proposition 1 requires only SUTVA and consistency,
and the exclusion restriction assumption. Additional assumptions may be required to extrapolate
pt|r(X) beyond regions of common support. On the other hand, supposing that positivity held
with respect to T given covariates X, given unconfoundedness, our finer-grained approach can be
beneficial because we only require robust extrapolation of pt|r(X), response to recommendations,
rather than the outcome models µt(X)."
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.11021069692058347,"We first describe what can be done if we allow ourselves parametric extrapolation on p1|1(X),
treatment responsivity. In the case study later on, the support of X | R = 1 is a superset of the
support of X | R = 0 in the observational data. Given this, we derive the following alternative
identification based on marginal control variates (where pt = P(T = t | X) marginalizes over the
distribution of R in the observational data):"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.11183144246353323,"Proposition 5 (Control variate for alternative identification ). Assume that Y (T(r)) ⊥T(r) | R =
r, X."
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.11345218800648298,"V (π) =
X"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.11507293354943274,"t∈T ,r∈{0,1}
E

crt(Y (t))I [T = t]"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.1166936790923825,"pt(X)
+

1 −I [T = t] pt(X)"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.11831442463533225,"
µt(X)

pt|r(X)
"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.11993517017828201,"Robust extrapolation of pt|r(X)
Let X no = {x : P(R = 1 | X) = 0} denote the no overlap
region; on this region there are no joint observations of (t, r, x). We consider uncertainty sets for
ambiguous treatment recommendation probabilities. For example, one plausible structural assumption
is monotonicity, that is, making an algorithmic recommendation can only increase the probability of
being treated."
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.12155591572123177,We define the following uncertainty set:
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.12317666126418152,"Uqt|r :=

q1|r(x′): q1|r(x) ≥p1|r(x), ∀x ∈X no, r P"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.12479740680713128,"t∈T qt|r(x) = 1, ∀x, r"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.12641815235008103,"We could assume uniform bounds on unknown probabilities, or more refined bounds, such as
Lipschitz-smoothness with respect to some distance metric d, or boundedness."
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.1280388978930308,"Ulip :=

q1|r(x′): d(q1|r(x′), p1|r(x)) ≤Ld(x′, x), (x′, x) ∈(X no × X no)
	
, Ubnd :=

q1|r(x′): b(x) ≤q1|r(x′) ≤b(x)"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.12965964343598055,Define Vov(π) := P
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.1312803889789303,"t∈T ,r∈{0,1} E[π(r | X)pt|r(X)µt(X)Iov]. Let U denote the uncertainty set
including any custom constraints, e.g. U = Uqt|r ∩Ulip. For brevity we use Ino to denote I [X ∈X no],
Iov to denote I [X ∈X ov]. Then we may obtain robust bounds by optimizing over regions of no
overlap:"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.13290113452188007,"V (π) := Vov(π) + V no(π), V no(π) :=
max
qtr(X)∈U nP"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.13452188006482982,"t∈T ,r∈{0,1} E[π(r | X)µt(X)qtr(X)Ino]]
o"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.13614262560777957,"In the specialized, but practically relevant case of binary outcomes/treatments/recommendations, we
obtain the following simplifications for bounds on the policy value, and the minimax robust policy
that optimizes the worst-case overlap extrapolation function. In the special case of constant uniform
bounds, it is equivalent (in the case of binary outcomes) to consider marginalizations:"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.13776337115072934,"Lemma 1 (Binary outcomes, constant bound). Let Ucbnd :=

qt|r(x′): B ≤q1|r(x′) ≤B
	
and
U = Uqt|r ∩Ucbnd. Define βt|r(a) := E[qt|r(X, A) | T = t, A = a]. If T ∈{0, 1},"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.1393841166936791,V no(π) = P
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.14100486223662884,"t∈T ,r∈{0,1} E[c∗
rtβt|rE[Y π(r | X) | T = t]Ino]],"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.1426256077795786,"where c∗
rt =
BI [t = 1] + BI [t = 0]
if E[Y π(r | X) | T = t] ≥0
BI [t = 0] + BI [t = 1]
if E[Y π(r | X) | T = t] < 0"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.14424635332252836,"We state the next result for simple uncertainty sets, like intervals, to deduce insights about the robust
policy. In the appendix we include a more general reformulation for polytopic uncertainty sets.
Proposition 6 (Robust linear program ). Suppose R, T ∈{0, 1}, and qr1(·, u) ∈Ubnd, ∀r, u. Define"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.1458670988654781,"τ(x, a) = µ1(x, a) −µ0(x, a), ∆Br(x, u) = (Br(x, u) −Br(x, u)), Bmid
r
(x, u) = Br(x, u) + 1"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.14748784440842788,"2∆Br(x, u),"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.14910858995137763,"E[∆ovT(π)] = E[T(π)Iov | A = a] −E[T(π)Iov | A = b], c1(π) = P"
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.1507293354943274,r E[τrπrBmid]
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.15235008103727715,Then the robust linear program is:
ROBUST ESTIMATION WITH TREATMENT OVERLAP BUT NOT RECOMMENDATION OVERLAP,0.1539708265802269,max Vov(π) + E[µ0] + c1(π) −1
P,0.15559157212317667,"2
P"
P,0.15721231766612642,"r E[|τ| π(r | X)∆Br(X, A)Ino]"
P,0.15883306320907617,s.t. P
P,0.16045380875202594,"r{E[π(r | X)Br(X, A)Ino | A = a] −E[πrBr(X, A)Ino | A = b]} + ∆T
ov(π) ≤ϵ"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.1620745542949757,"5
Additional fairness constraints and policy optimization"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.16369529983792544,"We previously discussed policy optimization, over unrestricted decision rules, given estimates. We
now introduce general methodology to handle 1) optimization over a policy class of restricted
functional form and 2) more general fairness constraints. We first introduce the fair-classification
algorithm of [1], describe our extensions to obtain variance-sensitive regret bounds and less conserva-
tive policy optimization (inspired by a regularized ERM argument given in [12]), and then provide
sample complexity analysis."
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.16531604538087522,"Algorithm and setup
In the following, to be consistent with standard form for linear programs,
note that we consider outcomes Y to be costs so that we can phrase the saddle-point as minimization-
maximization. Consider |K| linear constraints and J groups (values of protected attribute A), with
coefficient matrix M ∈RK×J, the constraint moment function hj(π), j ∈[J] (with J the number of
groups), O = (X, A, R, T, Y ) denoting our data observations, and d the constraint constant vector:"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.16693679092382496,"hj(π) = E [gj(O, π(X)) | Ej]
for j ∈J, Mh(π) ≤d"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.1685575364667747,"Importantly, gj depends on π while the conditioning event Ej cannot depend on π. Many important
fairness constraints can nonetheless be written in this framework, such as burden/resource parity,"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.17017828200972449,"Algorithm 1 MW2REDFAIR(D, g, E, M, d)"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.17179902755267423,"1: Input: D = {(Xi, Ri, Ti, Yi, Ai)}n
i=1, g, E, M, bd, B, ν, α, θ1 = 0 ∈R|K|"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.17341977309562398,"2: for t = 1, 2, . . . do
3:
Set λt,k = B
exp{θk}
1+P"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.17504051863857376,"k′∈X exp{θk′} for all k ∈K, βt ←BESTβ (λt) , bQt ←1"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.1766612641815235,"t
Pt
t′=1 βt′"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.17828200972447325,"¯L ←L( ˆQt, BESTλ( ˆQt)), ˆλt ←1"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.17990275526742303,"t
Pt
t′=1 λt′, L ←(BESTβ(ˆλt), ˆλt),"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.18152350081037277,"5:
νt ←max{L( ˆQt, ˆλt) −L,
¯L −L( ˆQt, ˆλt)}, If νt ≤ν then return ( ˆQt, ˆλt)
6:
θt+1,i = θt + log(1 −η(M ˆµ (ht) −ˆc)j), ∀i
7: end for"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.18314424635332252,"parity in true positive rates, but not measures such as calibration whose conditioning event does depend
on π. (See Appendix B.2 for examples omitted for brevity). We further consider a convexification
of Π via randomized policies Q ∈∆(Π), where ∆(Π) is the set of distributions over Π, i.e. a
randomized classifier that samples a policy π ∼Q. Therefore we solve"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.1847649918962723,"min
Q∈∆(Π){V (π):
Mh(π) ≤d}"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.18638573743922204,"On the other hand, the optimization is solved using sampled moments, so that we ought to interpret
the constraint values ˆdk = dk + ϵk, for all k. The overall algorithmic scheme is similar: we seek an
approximate saddle point so that the constrained solution is equivalent to the Lagrangian,"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.1880064829821718,"L(Q, λ) = ˆV (Q) + λ⊤(Mˆh(Q) −ˆd),
min
Q∈∆(Π){V (π): Mh(π) ≤d} =
min
Q∈∆(Π) max
λ∈RK
+
L(Q, λ)."
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.18962722852512157,We simultaneously solve for an approximate saddle point and bound the domain of Λ by B:
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.1912479740680713,"minQ∈∆maxλ∈R|X|
+
,∥λ∥1≤B L(Q, λ),
maxλ∈R|X|
+
,∥λ∥1≤B minQ∈∆L(Q, λ)"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.19286871961102106,"We play a no-regret (second-order multiplicative weights [11, 43],
a slight variant of
Hedge/exponentiated gradient [18]) algorithm for the λ−player, while using best-response ora-
cles for the Q−player. Full details are in Algorithm 1. Given λt, BESTβ (λt) computes a best
response over Q; since the worst-case distribution will place all its weight on one classifier, this
step can be implemented by a reduction to cost-sensitive/weighted classification [10, 49], which we
describe in further detail below. Computing the best response over BESTλ( ˆQt)) selects the most
violated constraint. We include further details in Appendix B.2."
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.19448946515397084,"Weighted classification reduction. There is a well-known reduction of optimizing the zero-one
loss for policy learning to weighted classification. Taking the Lagrangian will introduce datapoint-
dependent additional weights. This reduction requires π ∈{−1, +1}, T ∈{−1, +1} (for notational
convenience alone). We consider parameterized policy classes so that π(x) = sign(gθ(x)) for some
index function g depending on a parameter β ∈Rd. Consider the centered regret J(π) = E[Y (π)] −
1
2E[E[Y | R = 1, X] + E[Y | R = 0, X]]. Then J(β) = J(sgn(gβ(·))) = E[sgn(gβ(X)) {ψ}]
where ψ can be one of, where µR
r (X) = E[Y | R = r, X],"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.19611021069692058,"ψDM = (p1|1(X)−p1|0(X))(µ1(X)−µ0(X)), ψIP W =
RY
eR(X), ψDR = ψDM +ψIP W + RµR(X) eR(X)"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.19773095623987033,"We can apply the standard reduction to cost-sensitive classification since ψi sgn(gβ(Xi)) = |ψi| (1 −
2I [sgn(gβ(Xi)) ̸= sgn(ψi)]). Then we can use surrogate losses for the zero-one loss. Although many
functional forms for ℓ(·) are Fisher-consistent, one such choice of ℓis the logistic (cross-entropy)
loss given below:"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.1993517017828201,"L(β) = E[|ψ| ℓ(gβ(X), sgn(ψ))],
l(g, s) = 2 log(1 + exp(g)) −(s + 1)
(2)"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.20097244732576985,"Two-stage variance-constrained algorithm. We seek to improve upon this procedure so that we
may obtain regret bounds on policy value and fairness constraint violation that exhibit more favorable
dependence on the maximal variance over small-variance slices near the optimal policy, rather than
worst-case constants over all policies, [12, 5]. Further, note that the algorithm sets the constraint
feasibility slacks via generalization bounds that previously depended on these worst-case constants."
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.2025931928687196,Algorithm 2 Two-stage localized fair classification via reductions
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.20421393841166938,"1: Randomly split the data into D1, D2
2: Obtain ˆπ∗
1 and the index set of binding constraints ˆI1 by learning nuisances η1 and running
Algorithm 1 on D1 with MW2REDFAIR(D1, h, E, M, d; η1)
3: ˆσ2
j ←
ˆ
Var(gj(O, ˆπ∗
1)I [Ej] /pj), ∀j;
ˆd ←d + 2ˆσ2n−α"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.20583468395461912,"4: Augment additional constraints with ϵn-policy-value and constraint slices relative to ˆπ1: define
an augmented system (where subscripting by ˆI1 indexes the corresponding matrix or vector):"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.20745542949756887,"˜hj′ = En1[{g
ˆI1
j′ (O; ˆπ∗
1) −g
ˆI1
j′ (O; π)} | Ej], ∀j′ ∈ˆI1, ˜hv = En1[vDR(O; π, η1) −vDR(O; π, η1)]"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.20907617504051865,"˜
M = [M; MˆI1,⃗1], ˜h = [h(·; π), ˜h, ˜hv]⊤, ˜d = [ ˆd, ϵn, ϵn]⊤, ˜E = [E, EˆI1, ∅]⊤"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.2106969205834684,"5: Obtain ˆπ∗
2 by running Algorithm 1 on D2 with MW2REDFAIR(D, ˜g, ˜E, ˜
M, ˜d, η2)"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.21231766612641814,"These challenges motivate the two-stage procedure, described formally in Algorithm 2 and ver-
bally here. We adapt an out-of-sample regularization scheme developed in [12], which recovers
variance-sensitive regret bounds via a small modification to ERM. We split the data, learn nui-
sance estimators η1 for use in our policy value and constraint estimates, and run Algorithm 1
(MW2REDFAIR(D1, h, E, M, d; η1)) to obtain an estimate of the optimal policy ˆπ1, and the con-
straint variances at ˆπ1. Next, we augment the constraint matrix with additional constraints that
require the second-stage π to achieve ϵn close policy value and constraint moment values relative
to ˆπ1. Since errors concentrate fast, this can be viewed as variance regularization. And, we set the
constraint slacks ˜d in the second stage using estimated variance constants from ˆπ1, which will be less
conservative than using worst-case bounds. Define"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.21393841166936792,"v(·)(Q) = Eπ∼Q[πψ(·) | O], for (·) ∈{∅, DR}, gj(O; Q) = Eπ∼Q[gj(O; π) | O, Ej],"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.21555915721231766,so that V (·)(Q) = E[v(·)(Q)] and hj(Q) = E[gj(O; Q) | Ej]. Define the function classes
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.2171799027552674,"FΠ = {vDR(·, π; η): π ∈Π, η ∈H}, Fj = {g(·, π; η): π ∈Π, η ∈H}"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.2188006482982172,"and the empirical entropy integral κ(r, F) = infα≥0{4α + 10
R r
α q"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.22042139384116693,"H2(ϵ,F,n)"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.22204213938411668,"n
dϵ} where H2(ϵ, F, n)
is the L2 empirical entropy, i.e. log of the ∥·∥2 ϵ-covering number. We make a mild assumption of a
learnable function class (bounded entropy integral) [46].
Assumption 7. The function classes FΠ, {Fj}j∈J satisfy that for any constant r, κ(r, F) →0 as
n →∞. The function classes {Fj}j comprise of Lj-Lipschitz contractions of π."
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.22366288492706646,"We will assume that we are using doubly-robust/orthogonalized estimation as in proposition 4, hence
state results depending on estimation error of nuisance vector η.
Theorem 1 (Variance-Based Oracle Policy Regret). Suppose that the mean-squared-error of the
nuisance estimates is upper bounded w.p. 1−δ/2 by χ2
n,δ, over the randomness of the nuisance sample:
maxl{E[(ˆηl −ηl)2]}l∈[L] := χ2
n. Let r = supQ∈Q
p"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.2252836304700162,"E [vDR(z; π)2] and ϵn = Θ(κ (r, FΠ) + r
q"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.22690437601296595,log(1/δ)
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.22852512155591573,"n
). Moreover, let"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.23014586709886548,"Q∗(ϵ) =

Q ∈∆[Π] : V (Q0
∗) −V (Q) ≤ϵ, h(Q0
∗) −h(Q) ≤d + ϵ"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.23176661264181522,"denote an ϵ-regret slice of the policy space. Let ˜ϵn = O(ϵn + χ2
n,δ) and"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.233387358184765,"V 0
2 = sup {Var
 
v0
DR(O; Q) −v0
DR (O; Q′)

: Q, Q′ ∈Q∗(˜ϵn)}"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.23500810372771475,"denote the variance of the difference between any two policies in an ϵn-regret slice, evaluated at the
true nuisance quantities. (Define V j
2 analogously for the variance of constraint moments). Then,
letting γ(Q) := Mh(Q) denote the constraint values, the policy distribution Q2 returned by the
out-of-sample regularized ERM, satisfies w.p. 1 −δ over the randomness of S :"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.2366288492706645,"V ( ˆQ) −V (Q∗) = O(κ(
q"
ADDITIONAL FAIRNESS CONSTRAINTS AND POLICY OPTIMIZATION,0.23824959481361427,"V obj
2
, conv(FΠ)) + n−1"
Q,0.23987034035656402,"2
q"
Q,0.24149108589951376,"V obj
2
log(3/δ) + χ2
n,δ)"
Q,0.24311183144246354,"(γj( ˆQ) −cj) −(γj(Q∗) −cj) = O(κ(
q"
Q,0.24473257698541329,"V j
2 , conv(Fj)) + n−1"
Q,0.24635332252836303,"2
q"
Q,0.2479740680713128,"V j
2 log(3/δ) + χ2
n,δ)"
Q,0.24959481361426256,"The benefits are that 1) the constants are improved from an absolute, structure-agnostic bound
to depending on the variance of low-regret policies, which also reflects improved variance from
using doubly-robust estimation as in proposition 4, and 2) less-conservative out-of-sample fairness
constraint satisfaction."
EXPERIMENTS,0.25121555915721233,"6
Experiments"
EXPERIMENTS,0.25283630470016205,"Due to space constraints, in the main text we only present a case study based on the PSA-DMF for
supervised release [38]. In the appendix we include additional experiments and robustness checks,
including a case study of fully-randomized recommendations and non-adherence."
EXPERIMENTS,0.2544570502431118,"PSA-DMF case study. Our case study is on a dataset of judicial decisions on supervised release
based on risk-score-informed recommendations [38]. The PSA-DMF (Public Safety Assessment
Decision Making Framework) uses a prediction of failure to appear for a future court data to inform
pretrial decisions, including our focus on supervised release (i.e. electronic monitoring in addition to
release) where judges make the final decision. Despite a large literature on algorithmic fairness of
pretrial risk assessment, to the best of our knowledge, recommendations about supervised release are
not informed by empirical evidence. There are current policy concerns about disparities in increasing
use of supervised release given mixed evidence on outcomes [38]; e.g. Safety and Justice Challenge
[41] concludes ""targeted efforts to reduce racial disparities are necessary"". First, we acknowledge
data issues. We work with publicly available data that was discretized for privacy [38]. The final
supervision decision does not include intensity, but different intensities are recommended in the data,
which we collapse into a single level. The PSA-DMF is an algorithmic recommendation so here
we are appealing to overlap in treatment recommendations, but using parametric extrapolation in
responsivity. Finally, unconfoundedness is likely untrue, but sensitivity analysis could address this.
So, this analysis should be considered exploratory, to illustrate the relevance of the methods. Future
work will seek proprietary data for a higher-fidelity empirical study."
EXPERIMENTS,0.2560777957860616,"Next in Figure 1 we provide descriptive information illustrating heterogeneity (including by protected
attribute) in adherence and effectiveness. We observe wide variation in judges assigning supervised
release beyond the recommendation. We use logistic regression to estimate outcome models and
treatment response models. The first figure shows estimates of the causal effect for different groups,
by gender (similar heterogeneity for race). The outcome is failure to appear, so negative scores are
beneficial. The second figure illustrates the difference in responsiveness: how much more likely
decision-makers are to assign treatment when there is vs. isn’t an algorithmic recommendation to
do so. The last figure plots a logistic regression of the lift in responsiveness on the causal effect
µ1(x, a) −µ0(x, a). We observe disparities in how responsive decision-makers are conditional on
the same treatment effect efficacy. This is importantly not a claim of animus because decision-makers
didn’t have access to causal effect estimates. Nonetheless, disparities persist."
EXPERIMENTS,0.2576985413290113,"In Figure 2 we highlight results from constrained policy optimization. The first two plots in each
set illustrate the objective function value and A = a average treatment cost, respectively; for A
being race (nonwhite/white) or gender (female/male), respectively. We use costs of 100 for Y = 1
(failure to appear, 0 for Y = 0, and 20 when T = 1 (set arbitrarily). On the x-axis we plot the
penalty λ that we use to assess the solutions of Proposition 9. The vertical dashed line indicates the
solution achieving ϵ = 0, i.e. parity in treatment take-up. Near-optimal policies that reduce treatment
disparity can be of interest due to advocacy concerns about how the expansion of supervised release
could increase the surveillance of already surveillance-burdened marginalized populations. We see
that indeed, for race, surveillance-parity constrained policies can substantially reduce disparities for
nonwhites while not increasing surveillance on whites that much: the red line decreases significantly
at low increase to the blue line (and low increases to the objective value). On the other hand, for
gender, the opportunity for improvement in surveillance disparity is much smaller. See the appendix
for further experiments and computational details."
REFERENCES,0.2593192868719611,References
REFERENCES,0.26094003241491087,"[1] A. Agarwal, A. Beygelzimer, M. Dudík, J. Langford, and H. Wallach. A reductions approach
to fair classification. In International Conference on Machine Learning, pages 60–69. PMLR,
2018."
REFERENCES,0.2625607779578606,"Figure 1: Distribution of treatment effect by gender, lift in treatment probabilities p11a −p01a =
P(T = 1 | R = 1, A = a, X) −P(T = 1 | R = 0, A = a, X), and plot of p11a −p01a vs. τ."
REFERENCES,0.26418152350081037,"0.2
0.0
0.2 17.10 17.15 17.20 17.25 17.30"
REFERENCES,0.26580226904376014,Obj. reduction
REFERENCES,0.26742301458670986,"0.2
0.0
0.2 1.0 1.5 2.0 2.5 3.0 3.5"
REFERENCES,0.26904376012965964,"E[T(
)|A = a]"
REFERENCES,0.2706645056726094,"A=nonwhite
A=white"
REFERENCES,0.27228525121555913,"0.2
0.0
0.2"
REFERENCES,0.2739059967585089,17.075
REFERENCES,0.2755267423014587,17.100
REFERENCES,0.2771474878444084,17.125
REFERENCES,0.2787682333873582,17.150
REFERENCES,0.28038897893030795,17.175
REFERENCES,0.2820097244732577,17.200
REFERENCES,0.28363047001620745,17.225
REFERENCES,0.2852512155591572,Obj. reduction (A=gender)
REFERENCES,0.28687196110210694,"0.2
0.0
0.2 1.5 2.0 2.5 3.0 3.5"
REFERENCES,0.2884927066450567,"E[T(
)|A = a]"
REFERENCES,0.2901134521880065,"A=female
A=male"
REFERENCES,0.2917341977309562,"Figure 2: Policy value V (πλ), treatment value E[T(πλ) | A = a], for A = race, gender."
REFERENCES,0.293354943273906,"[2] D. Arnold, W. Dobbie, and C. S. Yang. Racial bias in bail decisions. The Quarterly Journal of
Economics, 133(4):1885–1932, 2018."
REFERENCES,0.29497568881685576,"[3] D. Arnold, W. Dobbie, and P. Hull. Measuring racial discrimination in bail decisions. American
Economic Review, 112(9):2992–3038, 2022."
REFERENCES,0.2965964343598055,"[4] S. Athey. Beyond prediction: Using big data for policy problems. Science, 2017."
REFERENCES,0.29821717990275526,"[5] S. Athey and S. Wager. Policy learning with observational data. Econometrica, 89(1):133–161,
2021."
REFERENCES,0.29983792544570503,"[6] S. Barocas, M. Hardt, and A. Narayanan. Fairness and Machine Learning. fairmlbook.org,
2018. http://www.fairmlbook.org."
REFERENCES,0.3014586709886548,"[7] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002."
REFERENCES,0.30307941653160453,"[8] H. Bastani, O. Bastani, and W. P. Sinchaisri. Improving human decision-making with machine
learning. arXiv preprint arXiv:2108.08454, 2021."
REFERENCES,0.3047001620745543,"[9] E. Ben-Michael, D. J. Greiner, K. Imai, and Z. Jiang. Safe policy learning through extrapolation:
Application to pre-trial risk assessment. arXiv preprint arXiv:2109.11679, 2021."
REFERENCES,0.3063209076175041,"[10] A. Beygelzimer and J. Langford. The offset tree for learning with partial labels. In Proceedings
of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 129–138, 2009."
REFERENCES,0.3079416531604538,"[11] N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with
expert advice. Machine Learning, 66:321–352, 2007."
REFERENCES,0.3095623987034036,"[12] V. Chernozhukov, M. Demirer, G. Lewis, and V. Syrgkanis. Semi-parametric efficient policy
learning with continuous actions. Advances in Neural Information Processing Systems, 32,
2019."
REFERENCES,0.31118314424635335,"[13] A. Chohlas-Wood, M. Coots, H. Zhu, E. Brunskill, and S. Goel. Learning to be fair: A
consequentialist approach to equitable decision-making. arXiv preprint arXiv:2109.08792,
2021."
REFERENCES,0.31280388978930307,"[14] M. De-Arteaga, R. Fogliato, and A. Chouldechova. A case for humans-in-the-loop: Decisions
in the presence of erroneous algorithmic scores. In Proceedings of the 2020 CHI Conference on
Human Factors in Computing Systems, pages 1–12, 2020."
REFERENCES,0.31442463533225284,"[15] J. L. Doleac and M. T. Stevenson. Algorithmic risk assessments in the hands of humans. Salem
Center, 2020."
REFERENCES,0.3160453808752026,"[16] A. Finkelstein, S. Taubman, B. Wright, M. Bernstein, J. Gruber, J. P. Newhouse, H. Allen,
K. Baicker, and O. H. S. Group. The oregon health insurance experiment: evidence from the
first year. The Quarterly journal of economics, 127(3):1057–1106, 2012."
REFERENCES,0.31766612641815234,"[17] D. J. Foster and V. Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036,
2019."
REFERENCES,0.3192868719611021,"[18] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of computer and system sciences, 55(1):119–139, 1997."
REFERENCES,0.3209076175040519,"[19] B. Green and Y. Chen. Disparate interactions: An algorithm-in-the-loop analysis of fairness in
risk assessments. In Proceedings of the conference on fairness, accountability, and transparency,
pages 90–99, 2019."
REFERENCES,0.3225283630470016,"[20] B. Green and Y. Chen. Algorithmic risk assessments can alter human decision-making processes
in high-stakes government contexts. Proceedings of the ACM on Human-Computer Interaction,
5(CSCW2):1–33, 2021."
REFERENCES,0.3241491085899514,"[21] K. Heard, E. O’Toole, R. Naimpally, and L. Bressler. Real world challenges to randomization
and their solutions. Boston, MA: Abdul Latif Jameel Poverty Action Lab, 2017."
REFERENCES,0.32576985413290116,[22] M. A. Hernán and J. M. Robins. Causal inference.
REFERENCES,0.3273905996758509,"[23] K. Imai, Z. Jiang, J. Greiner, R. Halen, and S. Shin. Experimental evaluation of algorithm-
assisted human decision-making: Application to pretrial public safety assessment. arXiv
preprint arXiv:2012.02845, 2020."
REFERENCES,0.32901134521880065,"[24] Z. Jiang, S. Yang, and P. Ding. Multiply robust estimation of causal effects under principal
ignorability. arXiv preprint arXiv:2012.01615, 2020."
REFERENCES,0.33063209076175043,"[25] N. Kallus and A. Zhou. Confounding-robust policy improvement. In Advances in Neural
Information Processing Systems, pages 9269–9279, 2018."
REFERENCES,0.33225283630470015,"[26] N. Kallus and A. Zhou. Assessing disparate impact of personalized interventions: identifiability
and bounds. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.3338735818476499,"[27] N. Kallus and A. Zhou. Fairness, welfare, and equity in personalized pricing. In Proceedings
of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 296–314,
2021."
REFERENCES,0.3354943273905997,"[28] N. Kallus, X. Mao, and A. Zhou. Assessing algorithmic fairness with unobserved protected
class using data combination. arXiv preprint arXiv:1906.00285, 2019."
REFERENCES,0.3371150729335494,"[29] K. Kim, E. Kennedy, and J. Zubizarreta. Doubly robust counterfactual classification. Advances
in Neural Information Processing Systems, 35:34831–34845, 2022."
REFERENCES,0.3387358184764992,[30] T. Kitagawa and A. Tetenov. Empirical welfare maximization. 2015.
REFERENCES,0.34035656401944897,"[31] W. Lin, S.-H. Kim, and J. Tong. Does algorithm aversion exist in the field? an empirical
analysis of algorithm use determinants in diabetes self-management. An Empirical Analysis
of Algorithm Use Determinants in Diabetes Self-Management (July 23, 2021). USC Marshall
School of Business Research Paper Sponsored by iORB, No. Forthcoming, 2021."
REFERENCES,0.3419773095623987,"[32] L. Liu, Z. Shahn, J. M. Robins, and A. Rotnitzky. Efficient estimation of optimal regimes
under a no direct effect assumption. Journal of the American Statistical Association, 116(533):
224–239, 2021."
REFERENCES,0.34359805510534847,"[33] J. Ludwig and S. Mullainathan. Fragile algorithms and fallible decision-makers: lessons from
the justice system. Journal of Economic Perspectives, 35(4):71–96, 2021."
REFERENCES,0.34521880064829824,"[34] K. Lum, E. Ma, and M. Baiocchi. The causal impact of bail on case outcomes for indigent
defendants in new york city. Observational Studies, 3(1):38–64, 2017."
REFERENCES,0.34683954619124796,"[35] C. Manski. Social Choice with Partial Knoweldge of Treatment Response. The Econometric
Institute Lectures, 2005."
REFERENCES,0.34846029173419774,"[36] A. Maurer. A vector-contraction inequality for rademacher complexities. In Algorithmic
Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016,
Proceedings 27, pages 3–17. Springer, 2016."
REFERENCES,0.3500810372771475,"[37] B. Metevier, S. Giguere, S. Brockman, A. Kobren, Y. Brun, E. Brunskill, and P. S. Thomas.
Offline contextual bandits with high probability fairness guarantees. Advances in neural
information processing systems, 32, 2019."
REFERENCES,0.35170178282009723,"[38] Office of the Chief Judge. Bail reform in cook county: An examination of general order 18.8a
and bail in felony cases. 2019."
REFERENCES,0.353322528363047,"[39] H. Qiu, M. Carone, E. Sadikova, M. Petukhova, R. C. Kessler, and A. Luedtke. Optimal
individualized decision rules using instrumental variable methods. Journal of the American
Statistical Association, 116(533):174–191, 2021."
REFERENCES,0.3549432739059968,"[40] D. B. Rubin. Comments on “randomization analysis of experimental data: The fisher ran-
domization test comment”. Journal of the American Statistical Association, 75(371):591–593,
1980."
REFERENCES,0.3565640194489465,"[41] Safety and C. f. C. I. Justice Challenge.
Expanding supervised release in new york
city.
2022.
URL https://safetyandjusticechallenge.org/resources/
expanding-supervised-release-in-new-york-city/."
REFERENCES,0.3581847649918963,"[42] A. Shapiro. On duality theory of conic linear problems. Semi-Infinite Programming: Recent
Advances, pages 135–165, 2001."
REFERENCES,0.35980551053484605,"[43] J. Steinhardt and P. Liang. Adaptivity and optimism: An improved exponentiated gradient
algorithm. In International conference on machine learning, pages 1593–1601. PMLR, 2014."
REFERENCES,0.36142625607779577,"[44] H. Sun, E. Munro, G. Kalashnov, S. Du, and S. Wager. Treatment allocation under uncertain
costs. arXiv preprint arXiv:2103.11066, 2021."
REFERENCES,0.36304700162074555,"[45] A. Swaminathan and T. Joachims. Counterfactual risk minimization. Journal of Machine
Learning Research, 2015."
REFERENCES,0.3646677471636953,"[46] A. W. Van Der Vaart, J. A. Wellner, A. W. van der Vaart, and J. A. Wellner. Weak convergence.
Springer, 1996."
REFERENCES,0.36628849270664504,"[47] B. Woodworth, S. Gunasekar, M. I. Ohannessian, and N. Srebro. Learning non-discriminatory
predictors. In Conference on Learning Theory, pages 1920–1953. PMLR, 2017."
REFERENCES,0.3679092382495948,"[48] Y. Yacoby, B. Green, C. L. Griffin Jr, and F. Doshi-Velez. “if it didn’t happen, why would
i change my decision?”: How judges respond to counterfactual explanations for the public
safety assessment. In Proceedings of the AAAI Conference on Human Computation and
Crowdsourcing, volume 10, pages 219–230, 2022."
REFERENCES,0.3695299837925446,"[49] Y. Zhao, D. Zeng, A. J. Rush, and M. R. Kosorok. Estimating individualized treatment rules
using outcome weighted learning. Journal of the American Statistical Association, 107(499):
1106–1118, 2012."
REFERENCES,0.3711507293354943,"A
Additional discussion"
REFERENCES,0.3727714748784441,"A.1
Additional related work"
REFERENCES,0.37439222042139386,"Principal stratification and mediation analysis in causal inference
[32] studies an optimal
test-and-treat regime under a no-direct-effect assumption, that assigning a diagnostic test has no effect
on outcomes except via propensity to treat, and studies semiparametric efficiency using Structural
Nested-Mean Models. Though our exclusion restriction is also a no-direct-effect assumption, our
optimal treatment regime is in the space of recommendations only as we do not have control over the
final decision-maker, and we consider generally nonparametric models."
REFERENCES,0.3760129659643436,"We briefly go into more detail about formal differences, due to our specific assumptions, that delineate
the differences to mediation analysis. Namely, our conditional exclusion restriction implies that
Y1T0 = YT0 and that Y0T1 = Y1T1 (in mediation notation with Tr = T(r) in our notation), so that
so-called net direct effects are identically zero and the net indirect effect is the treatment effect (also
called average encouragement effect here)."
REFERENCES,0.37763371150729336,"Human-in-the-loop in consequential domains. There is a great deal of interest in designing
algorithms for the “human in the loop"" and studying expertise and discretion in human oversight in
consequential domains [14]. On the algorithmic side, recent work focuses on frameworks for learning
to defer or human-algorithm collaboration. Our focus is prior to the design of these procedures for
improved human-algorithm collaboration: we primarily hold fixed current human responsiveness to
algorithmic recommendations. Therefore, our method can be helpful for optimizing local nudges.
Incorporating these algorithmic design ideas would be interesting directions for future work."
REFERENCES,0.37925445705024313,"Empirical literature on judicial discretion in the pretrial setting.
Studying a slightly different
substantive question, namely causal effects of pretrial decisions on later outcomes, a line of work
uses individual judge decision-makers as a leniency instrumental variable for the treatment effect of
(for example, EM) on pretrial outcomes [3, 2, 34]. And, judge IVs rely on quasi-random assignment
of individual judges. We focus on the prescriptive question of optimal recommendation rules in view
of patterns of judicial discretion, rather than the descriptive question of causal impacts of detention
on downstream outcomes."
REFERENCES,0.38087520259319285,"A number of works have emphasized the role of judicial discretion in pretrial risk assessments in
particular [20, 15, 33]. In contrast to these works, we focus on studying decisions about electronic
monitoring, which is an intermediate degree of decision lever to prevent FTA that nonetheless
imposes costs. [23] study a randomized experiment of provision of the PSA and estimate (the sign
of) principal causal effects, including potential group-conditional disparities. They are interested in a
causal effect on the principal stratum of those marginal defendants who would not commit a new
crime if recommended for detention. [9] study policy learning in the absence of positivity (since
the PSA is a deterministic function of covariates) and consider a case study on determining optimal
recommendation/detention decisions; however their observed outcomes are downstream of judicial
decision-making. Relative to their approach, we handle lack of overlap via an exclusion restriction so
that we only require ambiguity on treatment responsivity models rather than causal outcome models."
REFERENCES,0.3824959481361426,"B
Additional discussion on method"
REFERENCES,0.3841166936790924,"B.1
Additional discussion on constrained optimization"
REFERENCES,0.3857374392220421,"Feasibility program
We can obtain upper/lower bounds on ϵ in order to obtain a feasible region
for ϵ by solving the below optimization over maximal/minimal values of the constraint:"
REFERENCES,0.3873581847649919,"ϵ, ϵ ∈max
π
/ min
π E[T(π) | A = a] −E[T(π) | A = b]
(3)"
REFERENCES,0.3889789303079417,"V ∗
ϵ = max
π
{E[c(π, T(π), Y (π))]: E[T(π) | A = a] −E[T(π) | A = b] ≤ϵ}
(4)"
REFERENCES,0.3905996758508914,"B.2
Additional discussion on Algorithm 2 (general algorithm)"
REFERENCES,0.39222042139384117,"B.2.1
Additional fairness constraints and examples in this framework"
REFERENCES,0.39384116693679094,"In this section we discuss additional fairness constraints and how to formulate them in the generic
framework. Much of this discussion is quite similar to [1] (including in notation) and is included in
this appendix for completeness only. We only additionally provide novel identification results for
another fairness measure on causal policies in Appendix B.2.2, concrete discussion of the reduction
to weighted classification, and provide concrete descriptions of the causal fairness constraints in the
more general framework."
REFERENCES,0.39546191247974066,"We first discuss how to impose the treatment parity constraint. This is similar to the demographic parity
example in [1], with different coefficients, but included for completeness. (Instead, recommendation
parity in E[π | A = a] is indeed nearly identical to demographic parity.)"
REFERENCES,0.39708265802269044,"Example 1 (Writing treatment parity in the general constrained classification framework.). We write
the constraint
E[T(π) | A = a] −E[T(π) | A = b]
(5)"
REFERENCES,0.3987034035656402,in this framework as follows:
REFERENCES,0.40032414910858993,"E[T(π) | A = a] = E[π1(X)(p1|1(X, A) −p1|0(X, A)) + p1|0(X, A) | A = a]"
REFERENCES,0.4019448946515397,"For each u ∈A we enforce that
P"
REFERENCES,0.4035656401944895,"r∈{0,1} E

πr(X)p1|r(X, A) | A = u

= P"
REFERENCES,0.4051863857374392,"r∈{0,1} E

πr(X, A)p1|r(X, A)
"
REFERENCES,0.406807131280389,"We can write this in the generic notation given previously by letting J = A ∪{◦},"
REFERENCES,0.40842787682333875,"gj(O, π(X); η) = π1(X)(p1|1(X, A) −p1|0(X, A)) + p1|0(X, A), ∀j."
REFERENCES,0.4100486223662885,"We let the conditioning events Ea = {A = a}, E◦= {True}, i.e. conditioning on the latter is
equivalent to evaluating the marginal expectation. Then we express Equation (5) as a set of equality
constraints ha(π) = h◦(π), leading to pairs of inequality constraints,

hu(π) −h◦(π) ≤0
h◦(π) −hu(π) ≤0  u∈A"
REFERENCES,0.41166936790923825,"The corresponding coefficients of M over this enumeration over groups (A) and epigraphical
enforcement of equality ({+, −}) equation (1), gives K = A × {+, −} so that M(a,+),a′ =
1 {a′ = a} , M(a,+),⋆= −1, M(a,−),a′ = −1 {a′ = a} , M(a,−),⋆= 1, and d = 0. Further we
can relax equality to small amounts of constraint relaxation by instead setting dk > 0 for some (or
all) k."
REFERENCES,0.413290113452188,"Next, we discuss a more complicated fairness measure. We first discuss identification and estimation
before we also describe how to incorporate it in the generic framework."
REFERENCES,0.41491085899513774,"B.2.2
Responder-dependent fairness measures"
REFERENCES,0.4165316045380875,"We consider a responder framework on outcomes (under our conditional exclusion restriction).
Because the contribution to the AEE is indeed from the responder strata, this corresponds to additional
estimation of the responder stratum."
REFERENCES,0.4181523500810373,"We enumerate the four possible realizations of potential outcomes (given any fixed recommenda-
tion) as (Y (0(r)), Y (1(r)) ∈{0, 1}2. We call units with (Y (0(r)), Y (1(r))) = (0, 1) responders,
(Y (0(r)), Y (1(r))) = (1, 0) anti-responders, and Y (0(r)) = Y (1(r)) non-responders. Such a
decomposition is general for the binary setting.
Assumption 8 (Binary outcomes, treatment)."
REFERENCES,0.419773095623987,"T, Y ∈{0, 1}"
REFERENCES,0.4213938411669368,Assumption 9 (Monotonicity).
REFERENCES,0.42301458670988656,Y (T(1)) ≥Y (T(0))
REFERENCES,0.4246353322528363,"Importantly, the conditional exclusion restriction of Assumption 2 implies that responder status is
independent of recommendation. Conditional on observables, whether a particular individual is a
responder is independent of whether someone decides to treat them when recommended. In this
way, we study responder status analogous to its use elsewhere in disparity assessment in algorithmic
fairness [23, 28]. Importantly, this assumption implies that the conditioning event (of being a
responder) is therefore independent of the policy π, so it can be handled in the same framework. s"
REFERENCES,0.42625607779578606,We may consider reducing disparities in resource expenditure given responder status.
REFERENCES,0.42787682333873583,"We may be interested in the probability of receiving treatment assignment given responder status.
Example 2 (Fair treatment expenditure given responder status)."
REFERENCES,0.42949756888168555,"E[T(π) | Y (1(R)) > Y (0(R)), A = a] −E[T(π) | Y (1(R)) > Y (0(R)), A = b] ≤ϵ"
REFERENCES,0.43111831442463533,"We can obtain identification via regression adjustment:
Proposition 7 (Identification of treatment expenditure given responder status). Assume Assump-
tions 8 and 9."
REFERENCES,0.4327390599675851,"P(T(π) = 1 | A = a, Y (1(π)) > Y (0(π))) =
P"
REFERENCES,0.4343598055105348,"r E[πr(X)p1|r(X, A)(µ1(X, A) −µ0(X, A)) | A = a])"
REFERENCES,0.4359805510534846,"E[(µ1(X, A) −µ0(X, A)) | A = a]"
REFERENCES,0.4376012965964344,"Therefore this can be expressed in the general framework.
Example 3 (Writing treatment responder-conditional parity in the general constrained classification
framework.). For each u ∈A we enforce that
P"
REFERENCES,0.4392220421393841,"r E[πr(X)p1|r(X,A)(µ1(X,A)−µ0(X,A))|A=u])"
REFERENCES,0.44084278768233387,"E[(µ1(X,A)−µ0(X,A))|A=u]
= P"
REFERENCES,0.44246353322528365,"r E[πr(X)p1|r(X,A)(µ1(X,A)−µ0(X,A))])"
REFERENCES,0.44408427876823336,"E[(µ1(X,A)−µ0(X,A))]
We can write this in the generic notation given previously by letting J = A ∪{◦},"
REFERENCES,0.44570502431118314,"gj(O, π(X); η) = {π1(X)(p1|1(X, A) −p1|0(X, A)) + p1|0(X, A)}(µ1(X, A) −µ0(X, A))"
REFERENCES,0.4473257698541329,"E[(µ1(X, A) −µ0(X, A)) | A = a]
, ∀j."
REFERENCES,0.44894651539708263,"Let Ej
a = {A = aj}, E◦= {True}, and we express Equation (5) as a set of equality constraints of the
above moment ha(π) = h◦(π), leading to pairs of inequality constraints,

hu(π) −h◦(π) ≤0
h◦(π) −hu(π) ≤0 "
REFERENCES,0.4505672609400324,"u∈A
The corresponding coefficients of M proceed analogously as for treatment parity."
REFERENCES,0.4521880064829822,"B.2.3
Best-response oracles"
REFERENCES,0.4538087520259319,"Best-responding classifier π, given λ: BESTπ(λ)
The best-response oracle, given a particular λ
value, optimizes the Lagrangian given π:"
REFERENCES,0.4554294975688817,"L(π, λ) = ˆV (π) + λ⊤(Mˆh(π) −ˆd)"
REFERENCES,0.45705024311183146,"= ˆV (π) −λ⊤ˆd +
X k,j"
REFERENCES,0.4586709886547812,"Mk,jλk"
REFERENCES,0.46029173419773095,"pj
En [gj(O, π)1 {O ∈Ej}] ."
REFERENCES,0.4619124797406807,"Best-responding Lagrange multiplier λ, given π:
BESTλ(Q) is the best response of
the Λ player.
It can be chosen to be either 0 or put all the mass on the most violated
constraint.
Let γ(Q)
:=
Mh(Q) denote the constraint values, then BESTλ(Q) returns
0
if bγ(Q) ≤bc
Bek∗
otherwise, where k∗= arg maxk [bγk(Q) −bck]"
REFERENCES,0.46353322528363045,"B.2.4
Weighted classification reduction"
REFERENCES,0.4651539708265802,"There is a well-known reduction of optimizing the zero-one loss for policy learning to weighted
classification. A cost-sensitive classification problem is"
REFERENCES,0.46677471636953,"arg min
π1 n
X"
REFERENCES,0.4683954619124797,"i=1
π1 (Xi) C1
i + (1 −π1 (Xi)) C0
i"
REFERENCES,0.4700162074554295,"The weighted classification error is Pn
i=1 Wi1 {h (Xi) ̸= Yi} which is an equivalent formulation if
Wi =
C0
i −C1
i
 and Yi = 1

C0
i ≥C1
i
	
."
REFERENCES,0.47163695299837927,"The reduction to weighted classification is particularly helpful since taking the Lagrangian will
introduce datapoint-dependent penalties that can be interpreted as additional weights. We can
consider the centered regret J(π) = E[Y (π)] −1"
REFERENCES,0.473257698541329,"2E[E[Y | R = 1, X] + E[Y | R = 0, X]]. Then"
REFERENCES,0.47487844408427876,J(θ) = J(sgn(gθ(·))) = E[sgn(gθ(X)) {ψ}]
REFERENCES,0.47649918962722854,"where ψ can be one of, where µR
r (X) = E[Y | R = r, X],"
REFERENCES,0.47811993517017826,"ψDM = (p1|1(X)−p1|0(X))(µ1(X)−µ0(X)), ψIP W =
RY
eR(X), ψDR = ψDM+ψIP W +RµR(X) eR(X)"
REFERENCES,0.47974068071312803,"We can apply the standard reduction to cost-sensitive classification since ψi sgn(gθ(Xi)) = |ψi| (1 −
2I [sgn(gθ(Xi)) ̸= sgn(ψi)]). Then we can use surrogate losses for the zero-one loss,"
REFERENCES,0.4813614262560778,"L(θ) = E[|ψ| ℓ(gθ(X), sgn(ψ))]"
REFERENCES,0.4829821717990275,"Although many functional forms for ℓ(·) are Fisher-consistent, the logistic (cross-entropy) loss will
be particularly relevant: l(g, s) = 2 log(1 + exp(g)) −(s + 1)g."
REFERENCES,0.4846029173419773,"Example 4 (Treatment parity, continued (weighted classification reduction)). The cost-sensitive
reduction for a vector of Lagrange multipliers can be deduced by applying the weighted classification
reduction to the Lagrangian:"
REFERENCES,0.4862236628849271,"L(β) = E
h
| ˜ψλ|ℓ

gβ(X), sgn( ˜ψλ)
i
,
where ˜ψλ = ψ + λA"
REFERENCES,0.4878444084278768,"pA
(p1|1 −p1|0) −
X"
REFERENCES,0.48946515397082657,"a∈A
λa."
REFERENCES,0.49108589951377635,"where pa := ˆP(A = a) and λa := λ(a,+) −λ(a,−), effectively replacing two non-negative Lagrange
multipliers by a single multiplier, which can be either positive or negative."
REFERENCES,0.49270664505672607,"Example 5 (Responder-conditional treatment parity, continued). The Lagrangian is L(β) ="
REFERENCES,0.49432739059967584,"E
h
| ˜ψλ|ℓ

gβ(X), sgn( ˜ψλ)
i
with weights:"
REFERENCES,0.4959481361426256,˜ψλ = ψ + λA pA
REFERENCES,0.49756888168557534,"(p1|1 −p1|0)(µ1 −µ0)
En[(µ1(X, A) −µ0(X, A)) | A = a] −
X"
REFERENCES,0.4991896272285251,"a∈A
λa."
REFERENCES,0.5008103727714749,"where pa := ˆP(A = a) and λa := λ(a,+) −λ(a,−)."
REFERENCES,0.5024311183144247,"B.3
Proofs"
REFERENCES,0.5040518638573744,Proof of Proposition 7.
REFERENCES,0.5056726094003241,"P(T(π) = 1 | A = a, Y (1(π)) > Y (0(π)))"
REFERENCES,0.5072933549432739,"= P(T(π) = 1, Y (1(r)) > Y (0(r)) | A = a)"
REFERENCES,0.5089141004862237,"P(Y (1(π)) > Y (0(π)) | A = a)
by Bayes’ rule"
REFERENCES,0.5105348460291734,"= P(T(π) = 1, Y (1) > Y (0) | A = a)"
REFERENCES,0.5121555915721232,"P(Y (1) > Y (0) | A = a)
by Assumption 2 =
P"
REFERENCES,0.513776337115073,"r E[E[πr(X)I [T(r) = 1] I [Y (1) > Y (0)] | A = a, X]])"
REFERENCES,0.5153970826580226,"P(Y (1) > Y (0) | A = a)
by iter. exp
P"
REFERENCES,0.5170178282009724,"r E[πr(X)p1|r(X, A)(µ1(X, A) −µ0(X, A)) | A = a])"
REFERENCES,0.5186385737439222,"E[(µ1(X, A) −µ0(X, A)) | A = a]
by Proposition 1"
REFERENCES,0.520259319286872,"C
Proofs"
REFERENCES,0.5218800648298217,"C.1
Proofs for generalization under unconstrained policies"
REFERENCES,0.5235008103727715,"Proposition
8
(Policy
value
generalization).
Assume
the
nuisance
models
η
=
[p1|0, p1|1, µ1, µ0, er(X)]⊤, η ∈H are consistent and well-specified with finite VC-dimension Vη
over the product function class H. We provide a proof for the general case, including doubly-robust
estimators, which applies to the statement of Proposition 8 by taking η = [p1|0, p1|1, µ1, µ0]."
REFERENCES,0.5251215559157212,"Let Π = {I{E[L(λ, X, A; η) | X] > 0: λ ∈R; η ∈F}."
REFERENCES,0.526742301458671,"supπ∈Π,λ∈R |(En[πL(λ, X, A)] −E[πL(λ, X, A)])| = Op(n−1 2 )"
REFERENCES,0.5283630470016207,"The generalization bound allows deducing risk bounds on the out-of-sample value:
Corollary 2."
REFERENCES,0.5299837925445705,"E[L(ˆλ, X, A)+] ≤E[L(λ∗, X, A)+] + Op(n−1 2 )"
REFERENCES,0.5316045380875203,"Proof of Proposition 8. We study a general Lagrangian, which takes as input pseudo-outcomes
ψt|r(O; η), ψy|t(O; η), ψ1|0,∆A where each satisfies that"
REFERENCES,0.5332252836304701,"E[ψt|r(O; η) | X, A] = p1|1(X, A) −p1|0(X, A)"
REFERENCES,0.5348460291734197,"E[ψy|t(O; η) | X, A] = τ(X, A)"
REFERENCES,0.5364667747163695,"E[ψ1|0,∆A | X] = p1|0(X, a) −p1|0(X, b)"
REFERENCES,0.5380875202593193,"We make high-level stability assumptions on pseudooutcomes ψ relative to the nuisance functions η
(these are satisfied by standard estimators that we will consider):"
REFERENCES,0.539708265802269,"Assumption 10. ψt|r, ψy|t, ψ1|0,∆A respectively are Lipschitz contractions with respect to η and
bounded"
REFERENCES,0.5413290113452188,"We study a generalized Lagrangian of an optimization problem that took these pseudooutcome
estimates as inputs:"
REFERENCES,0.5429497568881686,"L(λ, X, A; η) = ψt|r(O; η)

ψy|t(O; η) +
λ
p(A)(I [A = a] −I [A = b])

+ λ(ψ1|0,∆A(O; η))"
REFERENCES,0.5445705024311183,We will show that
REFERENCES,0.546191247974068,"sup
π∈Π,λ∈R
|(En[πL(λ, X, A)] −E[πL(λ, X, A)])| = Op(n−1 2 )"
REFERENCES,0.5478119935170178,"which, by applying the generalization bound twice gives that"
REFERENCES,0.5494327390599676,"En[πL(λ, X, A)] = E[πL(λ, X, A)]) + Op(n−1 2 )"
REFERENCES,0.5510534846029174,Write Lagrangian as
REFERENCES,0.5526742301458671,"max
π
min
λ
= min
λ max
π
= min
λ E[L(O, λ; η)+]"
REFERENCES,0.5542949756888168,"Suppose the Rademacher complexity of ηk is given by R(Hk), so that [7, Thm. 12] gives that the
Rademacher complexity of the product nuisance class H is therefore P"
REFERENCES,0.5559157212317666,"k R(Hk). The main result
follows by applying vector-valued extensions of Lipschitz contraction of Rademacher complexity
given in [36]. Suppose that ψt|r, ψy|t, ψ1|0,∆A are Lipschitz with constants CL
t|r, CL
y|t, CL
1|0,∆A."
REFERENCES,0.5575364667747164,We establish VC-properties of
REFERENCES,0.5591572123176661,"FL1(O1:n) = {(gη(O1), gη(Oi), . . . gη(On)): η ∈H} , where gη(O) = ψt|r(O; η)ψy|t(O; η)"
REFERENCES,0.5607779578606159,"FL2(O1:n) = {(hη(O1), hη(Oi), . . . hη(On)): η ∈H} , where hη(O) = ψt|r(O; η)
λ
p(A)(I [A = a] −I [A = b])"
REFERENCES,0.5623987034035657,"FL3(O1:n) = {(mη(O1), mη(Oi), . . . mη(On)): η ∈H} , where mη(O) = λ(ψ1|0,∆A(O; η))"
REFERENCES,0.5640194489465153,"and the function class for the truncated Lagrangian,"
REFERENCES,0.5656401944894651,"FL+ = {{(gη(Oi) + hη(Oi) + mη(Oi))+}1:n : g ∈FL1(O1:n), h ∈FL2(O1:n), m ∈FL3(O1:n), η ∈H}"
REFERENCES,0.5672609400324149,"[36, Corollary 4] (and discussion of product function classes) gives the following: Let X be any set,
(x1, . . . , xn) ∈X n, let F be a class of functions f : X →ℓ2 and let hi : ℓ2 →R have Lipschitz
norm L. Then"
REFERENCES,0.5688816855753647,"E sup
η∈H X"
REFERENCES,0.5705024311183144,"i
ϵiψi (η (Oi)) ≤
√"
LE SUP,0.5721231766612642,"2LE sup
η∈H X"
LE SUP,0.5737439222042139,"i,k
ϵikη (Oi) ≤
√"
"L
X",0.5753646677471637,"2L
X"
"L
X",0.5769854132901134,"k
E sup
ηk∈Hk X"
"L
X",0.5786061588330632,"i
ϵiηk (Oi)
(6)"
"L
X",0.580226904376013,"where ϵik is an independent doubly indexed Rademacher sequence and fk (xi) is the k-th component
of f (xi)."
"L
X",0.5818476499189628,"Applying Equation (6) to each of the component classes FL1(O1:n), FL2(O1:n), FL3(O1:n), and
Lipschitz contraction [7, Thm. 12.4] of the positive part function FL+, we obtain the bound"
"L
X",0.5834683954619124,"sup
λ,η
|En[L(O, λ; η)+] −E[L(O, λ; η)+]| ≤
√"
"L
X",0.5850891410048622,"2(CL
t|rCL
y|t + CL
t|rBpaB + BCL
1|0,∆A)
X"
"L
X",0.586709886547812,"k
R(Hk)"
"L
X",0.5883306320907618,Proposition 9 (Threshold solutions). Define
"L
X",0.5899513776337115,"L(λ, X, A) = (p1|1(X, A)−p1|0(X, A))

τ(X, A) +
λ
p(A)(I [A = a] −I [A = b])

+λ(p1|0(X, a)−p1|0(X, b))"
"L
X",0.5915721231766613,"λ∗∈arg min
λ
E[L(λ, X, A)+], π∗(x, u) = I{L(λ∗, X, u) > 0}"
"L
X",0.593192868719611,"If instead d(x) is a function of covariates x only,"
"L
X",0.5948136142625607,"λ∗∈arg min
λ
E[E[L(λ, X, A) | X]+], π∗(x) = I{E[L(λ∗, X, A) | X] > 0}"
"L
X",0.5964343598055105,"Proof of Proposition 9. The characterization follows by strong duality in infinite-dimensional linear
programming [42]. Strict feasibility can be satisfied by, e.g. solving eq. (3) to set ranges for ϵ."
"L
X",0.5980551053484603,"C.2
Proofs for robust characterization"
"L
X",0.5996758508914101,Proof of Proposition 5.
"L
X",0.6012965964343598,"V (π) =
X"
"L
X",0.6029173419773096,"t∈T ,r∈{0,1}
E[πr(X)E[crt(Y (t))I [T(r) = t] | R = r, X]] =
X"
"L
X",0.6045380875202593,"t∈T ,r∈{0,1}
E[πr(X)E[crt(Y (t)) | R = r, X]P(T(r) = t | R = r, X)]
unconf. =
X"
"L
X",0.6061588330632091,"t∈T ,r∈{0,1}
E[πr(X)E[crt(Y (t)) | X]P(T(r) = t | R = r, X)]
Assumption 2 (ER) (7) =
X"
"L
X",0.6077795786061588,"t∈T ,r∈{0,1}
E

πr(X)E

crt(Y (t))I [T(r) = t]"
"L
X",0.6094003241491086,"pt(X)
| X

P(T(r) = t | R = r, X)

unconf. =
X"
"L
X",0.6110210696920584,"t∈T ,r∈{0,1}
E

πr(X)

E

crt(Y (t))I [T(r) = t]"
"L
X",0.6126418152350082,"pt(X)
+

1 −
T
pt(X)"
"L
X",0.6142625607779578,"
µt(X) | X

pt|r(X)

control variate =
X"
"L
X",0.6158833063209076,"t∈T ,r∈{0,1}
E

πr(X)

crt(Y (t))I [T(r) = t]"
"L
X",0.6175040518638574,"pt(X)
+

1 −
T
pt(X)"
"L
X",0.6191247974068071,"
µt(X)

pt|r(X)

(LOTE)"
"L
X",0.6207455429497569,"where pt(X) = P(T = t | X) (marginally over R in the observational data) and (LOTE) is an
abbreviation for the law of total expectation."
"L
X",0.6223662884927067,Proof of Lemma 1.
"L
X",0.6239870340356564,"V no(π) :=
max
qtr(X)∈U 
  X"
"L
X",0.6256077795786061,"t∈T ,r∈{0,1}
E[πr(X)µt(X)qtr(X)I [X ∈X no]]] 
 "
"L
X",0.6272285251215559,"=
max
qtr(X)∈U 
  X"
"L
X",0.6288492706645057,"t∈T ,r∈{0,1}
E[πr(X)E[Y | T = t, X]qtr(X)I [X ∈X no]]] 
 "
"L
X",0.6304700162074555,"Note the objective function can be reparametrized under a surjection of qt|r(X) to its marginalization,
i.e. marginal expectation over a {T = t} partition (equivalently {T = t, A = a} partition for a
fairness-constrained setting)."
"L
X",0.6320907617504052,"Define
βt|r(a) := E[qt|r(X, A) | T = t, A = a], βt|r := E[qt|r(X, A) | T = t]"
"L
X",0.6337115072933549,"Therefore we may reparametrize V no(π) as an optimization over constant coefficients (bounded by
B): = max 
  X"
"L
X",0.6353322528363047,"t∈T ,r∈{0,1}
E[{ctβt|r}πr(X)E[Y | T = t, X]I [X ∈X no]]]: B ≤c1 ≤B, c0 = 1 −c1 
  = max 
  X"
"L
X",0.6369529983792545,"t∈T ,r∈{0,1}
E[{ctβt|r}E[Y πr(X) | T = t]I [X ∈X no]]]: B ≤c1 ≤B, c0 = 1 −c1 
"
"L
X",0.6385737439222042,"
LOTE =
X"
"L
X",0.640194489465154,"t∈T ,r∈{0,1}
E[c∗
rtβt|rE[Y πr(X) | T = t]I [X ∈X no]]]"
"L
X",0.6418152350081038,"where c∗
rt =
BI [t = 1] + BI [t = 0]
if E[Y πr(X) | T = t] ≥0
BI [t = 0] + BI [t = 1]
if E[Y πr(X) | T = t] < 0"
"L
X",0.6434359805510534,Proof of proposition 6.
"L
X",0.6450567260940032,"max
π
E[c(π, T(π), Y (π))I [X ̸∈X no]] + E[c(π, T(π), Y (π))I [X ∈X no]]
(8)"
"L
X",0.646677471636953,"E[T(π)I [X ̸∈X no] | A = a] −E[T(π)I [X ̸∈X no] | A = b]
(9)
+ E[T(π)I [X ∈X no] | A = a] −E[T(π)I [X ∈X no] | A = b] ≤ϵ, ∀qr1 ∈U
(10)"
"L
X",0.6482982171799028,"Define
gr(x, u) = (µr1(x, u) −µr0(x, u))"
"L
X",0.6499189627228525,"then we can rewrite this further and apply the standard epigraph transformation: max t t −
Z"
"L
X",0.6515397082658023,x∈X no X
"L
X",0.653160453808752,"u∈{a,b} X"
"L
X",0.6547811993517018,"r∈{0,1}
{gr(x, u)πr(x, u)f(x, u)}qr1(x, u)}dx ≤Vov(π) + E[µ0], ∀qr1 ∈U Z"
"L
X",0.6564019448946515,"x∈X no{f(x | a)(
X"
"L
X",0.6580226904376013,"r
πr(x, a)qr1(x, a)) −f(x | b)(
X"
"L
X",0.6596434359805511,"r
πr(x, b)qr1(x, b))} + E[∆ovT(π)] ≤ϵ, ∀qr1 ∈U"
"L
X",0.6612641815235009,"Project the uncertainty set onto the direct product of uncertainty sets: max t t −
Z"
"L
X",0.6628849270664505,x∈X no X
"L
X",0.6645056726094003,"u∈{a,b} X"
"L
X",0.6661264181523501,"r∈{0,1}
{gr(x, u)πr(x, u)f(x, u)}qr1(x, u)}dx ≤Vov(π) + E[µ0], ∀qr1 ∈U∞ Z"
"L
X",0.6677471636952999,"x∈X no{f(x | a)(
X"
"L
X",0.6693679092382496,"r
πr(x, a)qr1(x, a)) −f(x | b)(
X"
"L
X",0.6709886547811994,"r
πr(x, b)qr1(x, b))} + E[∆ovT(π)] ≤ϵ, ∀qr1 ∈U∈"
"L
X",0.6726094003241491,"Clearly robust feasibility of the resource parity constraint over the interval is obtained by the
highest/lowest bounds for groups a, b, respectively: max t t −
Z"
"L
X",0.6742301458670988,x∈X no X
"L
X",0.6758508914100486,"u∈{a,b} X"
"L
X",0.6774716369529984,"r∈{0,1}
{gr(x, u)πr(x, u)f(x, u)}qr1(x, u)}dx ≤Vov(π) + E[µ0], ∀qr1 ∈U∞ Z"
"L
X",0.6790923824959482,"x∈X no{f(x | a)(
X"
"L
X",0.6807131280388979,"r
πr(x, a)Br(x, a)) −f(x | b)(
X"
"L
X",0.6823338735818476,"r
πr(x, b)Br(x, u))} + E[∆ovT(π)] ≤ϵ"
"L
X",0.6839546191247974,We define
"L
X",0.6855753646677472,"δr1(x, u) = 2(qr1(x, u) −Br(x, u))"
"L
X",0.6871961102106969,"Br(x, u) −Br(x, u)
−(Br(x, u) −Br(x, u)),"
"L
X",0.6888168557536467,"then
{Br(x, u) ≤qr1(x, u) ≤Br(x, u)} =⇒{∥δr1(x, u)∥∞≤1}
and
qr1(x, u) = Br(x, u) + 1"
"L
X",0.6904376012965965,"2(Br(x, u) −Br(x, u))(δr1(x, u) + 1)."
"L
X",0.6920583468395461,"For brevity we denote ∆B = (Br(x, u) −Br(x, u)), so max t"
"L
X",0.6936790923824959,"t +
max
∥δr1(x,u)∥∞≤1
r∈{0,1},u∈{a,b} 
 −
Z"
"L
X",0.6952998379254457,x∈X no X
"L
X",0.6969205834683955,"u∈{a,b} X"
"L
X",0.6985413290113452,"r∈{0,1}
{gr(x, u)πr(x, u)f(x, u)}1"
"L
X",0.700162074554295,"2∆B(x, u)δr1(x, u)dx 
"
"L
X",0.7017828200972447,−c1(π) ≤Vov(π) + E Z
"L
X",0.7034035656401945,"x∈X no{f(x | a)(
X"
"L
X",0.7050243111831442,"r
πr(x, a)Br(x, a)) −f(x | b)(
X"
"L
X",0.706645056726094,"r
πr(x, b)Br(x, u))} + E[∆ovT(π)] ≤ϵ, where"
"L
X",0.7082658022690438,"c1(π) =
Z"
"L
X",0.7098865478119936,x∈X no X
"L
X",0.7115072933549432,"u∈{a,b} X"
"L
X",0.713128038897893,"r∈{0,1}
{gr(x, u)πr(x, u)f(x, u)}(Br(x, u) + 1"
"L
X",0.7147487844408428,"2(Br(x, u) −Br(x, u)))dx"
"L
X",0.7163695299837926,"This is equivalent to: max t t +
Z"
"L
X",0.7179902755267423,x∈X no X
"L
X",0.7196110210696921,"u∈{a,b} X"
"L
X",0.7212317666126418,"r∈{0,1}
|−gr(x, u)πr(x, u)f(x, u)| 1"
"L
X",0.7228525121555915,"2∆B(x, u)dx −c1(π) ≤Vov(π) + E[µ0] Z"
"L
X",0.7244732576985413,"x∈X no{f(x | a)(
X"
"L
X",0.7260940032414911,"r
πr(x, a)Br(x, a)) −f(x | b)(
X"
"L
X",0.7277147487844409,"r
πr(x, b)Br(x, u))} + E[∆ovT(π)] ≤ϵ"
"L
X",0.7293354943273906,"Undoing the epigraph transformation, we obtain:"
"L
X",0.7309562398703403,"max Vov(π) + E[µ0] + c1(π) −
Z"
"L
X",0.7325769854132901,x∈X no X
"L
X",0.7341977309562399,"u∈{a,b} X"
"L
X",0.7358184764991896,"r∈{0,1}
|−gr(x, u)πr(x, u)f(x, u)| 1"
"L
X",0.7374392220421394,"2∆B(x, u)dx Z"
"L
X",0.7390599675850892,"x∈X no{f(x | a)(
X"
"L
X",0.7406807131280388,"r
πr(x, a)Br(x, a)) −f(x | b)(
X"
"L
X",0.7423014586709886,"r
πr(x, b)Br(x, u))} + E[∆ovT(π)] ≤ϵ"
"L
X",0.7439222042139384,and simplifying the absolute value:
"L
X",0.7455429497568882,"max Vov(π) + E[µ0] + c1(π) −
Z"
"L
X",0.747163695299838,x∈X no X
"L
X",0.7487844408427877,"u∈{a,b} X"
"L
X",0.7504051863857374,"r∈{0,1}
|gr(x, u)πr(x, u)f(x, u)| 1"
"L
X",0.7520259319286872,"2∆B(x, u)dx Z"
"L
X",0.7536466774716369,"x∈X no{f(x | a)(
X"
"L
X",0.7552674230145867,"r
πr(x, a)Br(x, a)) −f(x | b)(
X"
"L
X",0.7568881685575365,"r
πr(x, b)Br(x, u))} + E[∆ovT(π)] ≤ϵ"
"L
X",0.7585089141004863,"C.3
Proofs for general fairness-constrained policy optimization algorithm and analysis"
"L
X",0.7601296596434359,"We begin with some notation that will simplify some statemetns. Define, for observation tuples
O ∼(X, A, R, T, Y ), the value estimate v(Q; η) given some pseudo-outcome ψ(O; η) dependent on
observation information and nuisance functions η. (We often suppress notation of η for brevity). We
let estimators sub/super-scripted by 1 denote estimators from the first dataset."
"L
X",0.7617504051863857,"v(·)(Q) = Eπ∼Q[πψ(·) | O], for (·) ∈{∅, DR}"
"L
X",0.7633711507293355,V (·)(Q) = E[v(·)(Q)]
"L
X",0.7649918962722853,"ˆV (·)
1 (Q) = En1[v(·)(Q)]
gj(O; Q) = Eπ∼Q[gj(O; π) | O, Ej]
hj(Q) = E[gj(O; Q) | Ej]
ˆh1
j(Q) = En1[gj(O; Q) | Ej]"
"L
X",0.766612641815235,"C.3.1
Preliminaries: results from other works used without proof"
"L
X",0.7682333873581848,"Theorem 3 (Thm.
3, [1] (saddle point generalization bound (non-localized)) ). Let ρ :=
maxh ∥M ˆµ(h) −ˆc∥∞. Let Assumption 1 hold for C′ ≥2C + 2 +
p"
"L
X",0.7698541329011345,"ln(4/δ)/2, where δ > 0.
Let Q⋆minimize V (Q) subject to Mµ(Q) ≤c. Then Algorithm 1 with ν ∝n−α, B ∝nα and
η ∝ρ−2n−2α terminates in O
 
ρ2n4α ln |K|

iterations and returns ˆQ. If np⋆
j ≥8 log(2/δ) for all
j, then with probability at least 1 −(|J | + 1)δ then for all k ˆQ satisfies:"
"L
X",0.7714748784440842,"V ( ˆQ) ≤V (Q⋆) + eO
 
n−α"
"L
X",0.773095623987034,"γk( bQ) ≤ck + 1 + 2ν B
+
X"
"L
X",0.7747163695299838,"j∈J
|Mk,j| eO
 
np⋆
j
−α"
"L
X",0.7763371150729336,"The proof of [1, Thm. 3] is modular in invoking Rademacher complexity bounds on the objective
function and constraint moments, so that invoking standard Rademacher complexity bounds for off-
policy evaluation/learning [5, 45] yields the above statement for V (π) (and analogously, randomized
policies by [7, Thm. 12.2] giving stability for convex hulls of policy classes).
Lemma 2 (Lemma 4, [17]). Consider a function class F : X →Rd, with supf∈F ∥f∥∞,2 ≤1 and
pick any f ∗∈F. Assume that v(π) is L-Lipschitz in its first argument with respect to the ℓ2 norm
and let:
Zn(r) = sup
Q∈Q
{|En[ˆv(Q) −ˆv(Q∗)] −E[v(Q) −v(Q∗)]| : E[(Eπ∼Q[v(π)]−Eπ∼Q∗[v(π)])2]
1
2 ≤r}"
"L
X",0.7779578606158833,"Then for some universal constants c1, c2 : Pr """
"L
X",0.779578606158833,"Zn(r) ≥16L d
X"
"L
X",0.7811993517017828,"t=1
R (r, conv(Πt) −Q∗
t ) + u #"
"L
X",0.7828200972447326,"≤c1 exp

−
c2nu2"
"L
X",0.7844408427876823,L2r2 + 2Lu 
"L
X",0.7860615883306321,"Moreover, if δn is any solution to the inequalities:
∀t ∈{1, . . . , d} : R (δ; star (conv(Πt) −Q∗
t )) ≤δ2"
"L
X",0.7876823338735819,then for each r ≥δn :
"L
X",0.7893030794165316,"P (Zn(r) ≥16Ldrδn + u) ≤c1 exp

−
c2nu2"
"L
X",0.7909238249594813,L2r2 + 2Lu 
"L
X",0.7925445705024311,"Lemma 3 (Concentration of conditional moments ([1, 47]). For any j ∈J , with probability at least
1 −δ, for all Q,
bhj(Q) −hj(Q)
 ≤2Rnj(H) +
2
√nj
+ s"
"L
X",0.7941653160453809,ln(2/δ)
NJ,0.7957860615883307,"2nj
If np⋆
j ≥8 log(2/δ), then with probability at least 1 −δ, for all Q,"
NJ,0.7974068071312804,"bhj(Q) −hj(Q)
 ≤2Rnp⋆
j /2(H) + 2 s"
NJ,0.7990275526742301,"2
np⋆
j
+ s"
NJ,0.8006482982171799,"ln(4/δ) np⋆
j"
NJ,0.8022690437601296,"Lemma 4 (Orthogonality (analogous to [12] (Lemma 8), others)). Suppose the nuisance estimates
satisfy a mean-squared-error bound"
NJ,0.8038897893030794,"max
l
{E[(ˆηl −ηl)2]}l∈[L] := χ2
n"
NJ,0.8055105348460292,"Then w.p. 1 −δ over the randomness of the policy sample,"
NJ,0.807131280388979,"V (Q0) −V ( ˆQ) ≤O(Rn,δ + χ2
n)"
NJ,0.8087520259319287,"C.4
Adapted lemmas"
NJ,0.8103727714748784,"In this subsection we collect results similar to those that have appeared previously, but that require
substantial additional argumentation in our specific saddle point setting.
Lemma 5 (Feasible vs. oracle nuisances in low-variance regret slices ([12], Lemma 9) ). Consider
the setting of Corollary 7. Suppose that the mean squared error of the nuisance estimates is upper
bounded w.p. 1 −δ by h2
n,δ and suppose h2
n,δ ≤ϵn. Then:"
NJ,0.8119935170178282,"V 0
2 =
sup
π,π′∈Π∗(ϵn+2h2
n,δ)
Var
 
v0
DR(x; π) −v0
DR (x; π′)
"
NJ,0.813614262560778,"Then V2 ≤V 0
2 + O (hn,δ)."
NJ,0.8152350081037277,"C.5
Proof of Theorem 1"
NJ,0.8168557536466775,"Proof of Theorem 1. We first study the meta-algorithm with “oracle"" nuisance functions η = η0."
NJ,0.8184764991896273,Define
NJ,0.820097244732577,"Π2 (ϵn) =
n
π ∈Π : En1[v(Q; η0) −v( ˆQ1; η0)] ≤ϵn, En1 [gj(O; π, η0) −gj (O; ˆπ1, η0) | Ej] ≤ϵn, j ∈ˆI1
o"
NJ,0.8217179902755267,"Q2 (ϵn) = {Q ∈∆(Π2(ϵn))}
Q∗(ϵn) = {Q ∈∆(Π) : E[(v(Q; η0) −v(Q∗; η0)] ≤ϵn, E[gj(O; Q, η0) | Ej] −E[gj(O; Q∗, η0) | Ej] ≤ϵn}"
NJ,0.8233387358184765,"In the following, we suppress notational dependence on η0."
NJ,0.8249594813614263,Note that ˆQ1 ∈Q2 (ϵn) .
NJ,0.826580226904376,"Step 1: First we argue that w.p. 1 −δ/6, Q∗∈Q2."
NJ,0.8282009724473258,"Invoking Theorem 3 on the output of the first stage of the algorithm, yields that with probability
1 −δ"
NJ,0.8298217179902755,"6 over the randomness in D1, by choice of ϵn = ¯O(n−α)),"
NJ,0.8314424635332253,V ( ˆQ1) ≤V (Q∗) + ϵn/2
NJ,0.833063209076175,"γk( ˆQ1) ≤dk +
X"
NJ,0.8346839546191248,"j∈J
|Mk,j| eO
 
(np∗
j)−α
≤dk + ϵn/2
for all k"
NJ,0.8363047001620746,"Further, by Lemma 2,"
NJ,0.8379254457050244,"sup
Q∈Q
|En1[(v(Q) −v(Q∗))] −E[(v(Q) −v(Q∗))]| ≤ϵn/2"
NJ,0.839546191247974,"sup
Q∈Q
|En1[(g(O; Q) −g(O; Q∗))] −E[(g(O; Q) −g(O; Q∗))]| ≤ϵn/2"
NJ,0.8411669367909238,"Therefore, with high probability on the good event, Q∗∈Q2."
NJ,0.8427876823338736,"Step 2: Again invoking Theorem 3, this time on the output of the second stage of the algorithm with
function space Π2 (hence implicitly Q2), and conditioning on the “good event"" that Q∗∈Q2, we
obtain the bound that with probability ≥1 −δ/3 over the randomness of the second sample D2,"
NJ,0.8444084278768234,V ( ˆQ2) ≤V (Q∗) + ϵn/2
NJ,0.8460291734197731,γk( ˆQ2) ≤γk(Q∗) + ϵn/2
NJ,0.8476499189627229,"Step 3: empirical small-regret slices relate to population small-regret slices, and variance bounds"
NJ,0.8492706645056726,"We show that if Q ∈Q2, then with high probability Q ∈Q0
2 (defined on small population value- and
constraint-regret slices relative to ˆQ1 rather than small empirical regret slices)"
NJ,0.8508914100486223,"Q0
2 = {Q ∈conv(Π):
V (Q) −V ( ˆQ1)
 ≤ϵn/2, E[gj(O; Q) −gj(O; ˆQ1)) | Ej] ≤ϵn, ∀j}"
NJ,0.8525121555915721,"so that w.h.p. Q2 ⊆Q0
2."
NJ,0.8541329011345219,"Note that for Q ∈Q, w.h.p. 1 −δ/6 over the first sample, we have that"
NJ,0.8557536466774717,"sup
Q∈Q"
NJ,0.8573743922204214,"En[v(Q) −v( ˆQ1)] −E[v(Q) −v( ˆQ1)]
 ≤2 sup
Q∈Q
|En[v(Q)] −E[v(Q)]| ≤ϵ,"
NJ,0.8589951377633711,"sup
Q∈Q"
NJ,0.8606158833063209,En1[gj(O; Q) −gj(O; ˆQ1) | Ej] −E[gj(O; Q) −gj(O; ˆQ1) | Ej]
NJ,0.8622366288492707,"≤2 sup
Q∈Q
|En1[gj(O; Q) | Ej] −E[gj(O; Q) | Ej]| ≤ϵ, ∀j"
NJ,0.8638573743922204,"The second bound follows from [7, Theorem 12.2] (equivalence of Rademacher complexity over
convex hull of the policy class) and linearity of the policy value and constraint estimators in π, and
hence Q."
NJ,0.8654781199351702,"On the other hand since Q1 achieves low policy regret, the triangle inequality implies that we can
contain the true policy by increasing the error radius. That is, for all Q ∈Q2, with high probability
≥1 −δ/3:"
NJ,0.86709886547812,"|E[(v(Q) −v(Q∗))]| ≤
E[(v(Q) −v( ˆQ1))]
 +
E[(v( ˆQ1) −v(Q∗))]
 ≤ϵn"
NJ,0.8687196110210696,"|E[gj(O; Q) −gj(O; Q∗) | Ej]| ≤
E[gj(O; Q) −gj(O; ˆQ1) | Ej]
 +
E[gj(O; ˆQ1) −gj(O; Q∗) | Ej]
 ≤ϵn"
NJ,0.8703403565640194,"Define the space of distributions over policies that achieve value and constraint regret in the population
of at most ϵn :"
NJ,0.8719611021069692,"Q∗(ϵn) = {Q ∈Q: V (Q) −V (Q∗) ≤ϵn, E[gj(O; Q) −gj(O; Q∗) | Ej] ≤ϵn, ∀j},"
NJ,0.873581847649919,"so that on that high-probability event,"
NJ,0.8752025931928687,"Q0
2(ϵn) ⊆Q∗(ϵn).
(11)"
NJ,0.8768233387358185,"Then on that event with probability ≥1 −δ/3,"
NJ,0.8784440842787682,"r2
2 = sup
Q∈Q2
E[(v(Q) −v(Q∗))2] ≤
sup
Q∈Q∗(ϵn)
E[(v(Q) −v(Q∗))2]"
NJ,0.880064829821718,"=
sup
Q∈Q∗(ϵn)
Var(v(Q) −v(Q∗)) + E[(v(Q) −v(Q∗))]2"
NJ,0.8816855753646677,"≤
sup
Q∈Q∗(ϵn)
Var(v(Q) −v(Q∗)) + ϵ2
n"
NJ,0.8833063209076175,Therefore:
NJ,0.8849270664505673,"r2 ≤
r"
NJ,0.8865478119935171,"sup
Q∈Q∗(ϵn)
Var (v(Q) −v(Q∗)) + 2ϵn =
p"
NJ,0.8881685575364667,V2 + 2ϵn
NJ,0.8897893030794165,"Combining this with the local Rademacher complexity bound, we obtain that:"
NJ,0.8914100486223663,"E[v( ˆQ2) −v(Q∗)] = O  κ
p"
NJ,0.893030794165316,"V2 + 2ϵn, Q∗(ϵn)

+ r"
NJ,0.8946515397082658,V2 log(3/δ) n !
NJ,0.8962722852512156,These same arguments apply for the variance of the constraints
NJ,0.8978930307941653,"V j
2 = sup {Var (gj(O; Q) −gj (O; Q′)) : Q, Q′ ∈Q∗(˜ϵn)}"
NJ,0.899513776337115,"C.6
Proofs of auxiliary/adapted lemmas"
NJ,0.9011345218800648,"Proof of Lemma 5. The proof is analogous to that of [12, Lemma 9] except for the step of establishing
that π∗∈Q0
ϵn+O(χ2
n,δ): in our case we must establish relationships between saddlepoints under
estimated vs. true nuisances. We show an analogous version below."
NJ,0.9027552674230146,Define the saddle points to the following problems (with estimated vs. true nuisances):
NJ,0.9043760129659644,"(Q∗
0,0, λ∗
0,0) ∈arg min
Q max
λ
E[vDR(Q; η0)] + λ⊤(γDR(Q; η0) −d) := L(Q, λ; η0, η0) := L(Q, λ),"
NJ,0.9059967585089141,"(Q∗
η,0, λ∗
η,0) ∈arg min
Q max
λ
E[vDR(Q; η)] + λ⊤(γDR(Q; η0) −d),"
NJ,0.9076175040518638,"(Q∗, λ∗) ∈arg min
Q max
λ
E[vDR(Q; η)] + λ⊤(γDR(Q; η) −d)."
NJ,0.9092382495948136,We have that:
NJ,0.9108589951377634,"E[vDR(Q∗)] ≤L(Q∗, λ∗; η, η) + ν"
NJ,0.9124797406807131,"≤L(Q∗, λ∗; η, η0) + ν + χ2
n,δ
≤L(Q∗, λ∗; η, η0) + ν + χ2
n,δ
by Lemma 4"
NJ,0.9141004862236629,"≤L(Q∗, λ∗
η,0; η, η0) + ν + χ2
n,δ
by saddlepoint prop."
NJ,0.9157212317666127,"≤L(Q∗
η,0, λ∗
η,0; η, η0) +
L(Q∗
η,0, λ∗
η,0; η, η0) −L(Q∗, λ∗
η,0; η, η0)
 + ν + χ2
n,δ
triangle ineq."
NJ,0.9173419773095624,"≤L(Q∗
η,0, λ∗
η,0; η, η0) + ϵn + ν + χ2
n,δ
assuming ϵn ≥χ2
n,δ
≤E[vDR(Q∗
η,0; η)] + ϵn + 2ν + χ2
n,δ
apx. complementary slackness"
NJ,0.9189627228525121,"≤E[vDR(Q∗
0,0; η)] + ϵn + 2ν + χ2
n,δ
suboptimality"
NJ,0.9205834683954619,"Hence
E[vDR(Q∗; η)] −E[vDR(Q∗
0,0; η)] ≤ϵn + 2ν + χ2
n,δ."
NJ,0.9222042139384117,"We generally assume that the saddlepoint suboptimality ν is of lower order than ϵn (since it is under
our computational control)."
NJ,0.9238249594813615,Applying Lemma 4 gives;
NJ,0.9254457050243112,"V (Q∗) −V (Q∗
0,0) ≤ϵn + 2ν + 2χ2
n,δ."
NJ,0.9270664505672609,"Define policy classes with respect to small-population regret slices (with a nuisance-estimation
enlarged radius):"
NJ,0.9286871961102107,"Q0(ϵ) = {Q ∈∆(Π): V (Q∗
0) −V (Q) ≤ϵ, γ(Q∗
0) −γ(Q) ≤ϵ}"
NJ,0.9303079416531604,Then we have that
NJ,0.9319286871961102,"V obj
2
≤
sup
Q∈Q0(ϵn)
Var(vDR(O; π) −vDR(O; π∗)),"
NJ,0.93354943273906,"where we have shown that π∗∈Q0(ϵ + 2ν + 2χ2
n,δ)."
NJ,0.9351701782820098,"Following the result of the argumentation in [12, Lemma 9] from here on out gives the result."
NJ,0.9367909238249594,"D
Case Studies"
NJ,0.9384116693679092,"D.1
Oregon Health Insurance Study"
NJ,0.940032414910859,"The Oregon Health Insurance Study [16] is an important study on the causal effect of expanding
public health insurance on healthcare utilization, outcomes, and other outcomes. It is based on
a randomized controlled trial made possible by resource limitations, which enabled the use of a
randomized lottery to expand Medicaid eligibility for low-income uninsured adults. Outcomes of
interest included health care utilization, financial hardship, health, and labor market outcomes and
political participation."
NJ,0.9416531604538088,"Because the Oregon Health Insurance Study expanded access to enroll in Medicaid, a social safety
net program, the effective treatment policy is in the space of encouragement to enroll in insurance
(via access to Medicaid) rather than direct enrollment. This encouragement structure is shared by
many other interventions in social services that may invest in nudges to individuals to enroll, tailored
assistance, outreach, etc., but typically do not automatically enroll or automatically initiate transfers.
Indeed this so-called administrative burden of requiring eligible individuals to undergo a costly
enrollment process, rather than automatically enrolling all eligible individuals, is a common policy
design lever in social safety net programs. Therefore we expect many beneficial interventions in this
consequential domain to have this encouragement structure."
NJ,0.9432739059967585,"We preprocess the data by partially running the Stata replication file, obtaining a processed data file
as input, and then selecting a subset of covariates. These covariates include household information
that affected stratified lottery probabilities, socioeconomic demographics, medical status and other
health information."
NJ,0.9448946515397083,"In the notation of our framework, the setup of the optimal/fair encouragement policy design question
is as follows:"
NJ,0.946515397082658,"• X covariates (baseline household information, socioeconomic demographics, health infor-
mation)"
NJ,0.9481361426256077,"• A race (non-white/white), or gender (female/male)
These protected attributes were binarized."
NJ,0.9497568881685575,"• R encouragement: lottery status of expanded eligibility (i.e. invitation to enroll when
individual was previously ineligible to enroll)"
NJ,0.9513776337115073,"• T: whether the individual is enrolled in insurance ever
Note that for R = 1 this can be either Medicaid or private insurance while for R = 0 this is
still well-defined as this can be private insurance."
NJ,0.9529983792544571,"• Y : number of doctor visits
This outcome was used as a measure of healthcare utilization. Overall, the study found
statistically significant effects on healthcare utilization. An implicit assumption is that
increased healthcare utilization leads to better health outcomes."
NJ,0.9546191247974068,"We subsetted the data to include complete cases only (i.e. without missing covariates). We learned
propensity and treatment propensity models via logistic regression for each group, and used gradient-
boosted regression for the outcome model. We first include results for regression adjustment identifi-
cation."
NJ,0.9562398703403565,"In Figure 3 we plot descriptive statistics. We include histograms of the treatment responsivity lifts
p1|1a(x, a) −p1|0a(x, a). We see some differences in distributions of responsivity by gender and race.
We then regress treatment responsivity on the outcome-model estimate of τ. We find substantially
more heterogeneity in treatment responsivity by race than by gender: whites are substantially more
likely to take up insurance when made eligible, conditional on the same expected treatment effect
heterogeneity in increase in healthcare utilization. (This is broadly consistent with health policy
discussions regarding mistrust of the healthcare system)."
NJ,0.9578606158833063,"Next we consider imposing treatment parity constraints on an unconstrained optimal policy (defined
on these estimates). In Figure 4 we display the objective value, and E[T(π) | A = a], for gender and
race, respectively, enumerated over values of the constraint. We use costs of 2 for the number of
doctors visits and 1 for enrollment in Medicaid (so E[T(π) | A = a] is on the scale of probability of"
NJ,0.9594813614262561,"0.0
0.2
0.4
0.6
0.0 0.5 1.0 1.5 2.0 2.5"
NJ,0.9611021069692058,"p1
1, a
p1
0, a, a= gender"
NJ,0.9627228525121556,"male
female"
NJ,0.9643435980551054,"0.0
0.2
0.4
0.6
0.0 0.5 1.0 1.5 2.0 2.5"
NJ,0.965964343598055,"p1
1, a
p1
0, a, a= race"
NJ,0.9675850891410048,"nonwhite
white"
NJ,0.9692058346839546,"10
0
10
s 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
NJ,0.9708265802269044,"p1a
p0a vs s, a= gender"
NJ,0.9724473257698542,"male
female"
NJ,0.9740680713128039,"10
0
10
s 0.0 0.1 0.2 0.3 0.4 0.5"
NJ,0.9756888168557536,"p1a
p0a vs s, a= race"
NJ,0.9773095623987034,"nonwhite
white"
NJ,0.9789303079416531,"Figure 3: Distribution of lift in treatment probabilities p1|1,a −p1|0,a = P(T = 1 | R = 1, A =
a, X) −P(T = 1 | R = 0, A = a, X), and plot of p1|1,a −p1|0,a vs. τ."
NJ,0.9805510534846029,"1.0
0.5
0.0
0.5 4.23 4.24 4.25 4.26 4.27 4.28"
NJ,0.9821717990275527,Obj.  (A=gender)
NJ,0.9837925445705025,"1.0
0.5
0.0
0.5 0.30 0.32 0.34 0.36 0.38 0.40 0.42"
NJ,0.9854132901134521,"E[T(
)|A = a]"
NJ,0.9870340356564019,"A=male
A=female"
NJ,0.9886547811993517,"1.0
0.5
0.0
0.5 4.24 4.25 4.26 4.27 4.28"
NJ,0.9902755267423015,Obj.  (A=race)
NJ,0.9918962722852512,"1.0
0.5
0.0
0.5
0.15 0.20 0.25 0.30 0.35 0.40"
NJ,0.993517017828201,"E[T(
)|A = a]"
NJ,0.9951377633711507,"A=nonwhite
A=white"
NJ,0.9967585089141004,"Figure 4: Policy value V (πλ), treatment value E[T(πλ) | A = a], for A = race, gender."
NJ,0.9983792544570502,"enrollment). These costs were chosen arbitrarily. Finding optimal policies that improve disparities
in group-conditional access can be done with relatively little impact to the overall objective value.
These group-conditional access disparities can be reduced from 4 percentage points (0.04) for gender
and about 6 percentage points (0.06) for race at a cost of 0.01 or 0.02 in objective value (twice the
number of doctors’ visits). On the other hand, relative improvements/compromises in access value
for the “advantaged group"" show different tradeoffs. Plotting the tradeoff curve for race shows that,
consistent with the large differences in treatment responsivity we see for whites, improving access
for blacks. Looking at this disparity curve given λ however, we can also see that small values of λ
can have relatively large improvements in access for blacks before these improvements saturate, and
larger λ values lead to smaller increases in access for blacks vs. larger decreases in access for whites."
