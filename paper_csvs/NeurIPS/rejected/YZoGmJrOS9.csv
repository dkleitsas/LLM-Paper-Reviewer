Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001277139208173691,"In-Context Learning (ICL) is a phenomenon where task learning occurs through
1"
ABSTRACT,0.002554278416347382,"a prompt sequence without the necessity of parameter updates. ICL in Multi-
2"
ABSTRACT,0.0038314176245210726,"Headed Attention (MHA) with absolute positional embedding has been the focus
3"
ABSTRACT,0.005108556832694764,"of more study than other sequence model varieties. We examine implications
4"
ABSTRACT,0.006385696040868455,"of architectural differences between GPT-2 and LLaMa as well as Llama and
5"
ABSTRACT,0.007662835249042145,"Mamba. We extend work done by Garg et al. (2022) and Park et al. (2024)
6"
ABSTRACT,0.008939974457215836,"to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models – examining the
7"
ABSTRACT,0.010217113665389528,"interplay between sequence transformation blocks and regressive performance
8"
ABSTRACT,0.011494252873563218,"in-context. We note that certain architectural changes cause degraded training
9"
ABSTRACT,0.01277139208173691,"efficiency/ICL accuracy by converging to suboptimal predictors or converging
10"
ABSTRACT,0.0140485312899106,"slower. We also find certain hybrids showing optimistic performance improve-
11"
ABSTRACT,0.01532567049808429,"ments, informing potential future ICL-focused architecture modifications. Ad-
12"
ABSTRACT,0.016602809706257982,"ditionally, we propose the ""ICL regression score"", a scalar metric describing a
13"
ABSTRACT,0.017879948914431672,"model’s whole performance on a specific task. Compute limitations impose re-
14"
ABSTRACT,0.019157088122605363,"strictions on our architecture-space, training duration, number of training runs,
15"
ABSTRACT,0.020434227330779056,"function class complexity, and benchmark complexity. To foster reproducible and
16"
ABSTRACT,0.021711366538952746,"extensible research, we provide a typed, modular, and extensible Python package
17"
ABSTRACT,0.022988505747126436,"on which we run all experiments. This code is available at https://github.
18"
ABSTRACT,0.024265644955300127,"com/anonymousforneurips64/neurips2024-submission21757.
19"
INTRODUCTION,0.02554278416347382,"1
Introduction
20"
INTRODUCTION,0.02681992337164751,"Popularized by Large Language Models such as GPT-2 [1] and GPT-3 [2], In-Context Learning (ICL)
21"
INTRODUCTION,0.0280970625798212,"is the ability for highly expressive generative sequence models to predict phenomena by processing
22"
INTRODUCTION,0.02937420178799489,"demonstrations without performing traditional gradient steps. Such phenomena vary from effective
23"
INTRODUCTION,0.03065134099616858,"control systems [3] to answering questions in natural language [4, 5]. A large body of recent work
24"
INTRODUCTION,0.031928480204342274,"has studied this phenomenon in transformer models [6, 7, 2, 1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
25"
INTRODUCTION,0.033205619412515965,"19, 20, 21, 22, 23, 24, 25] , which derive in structure from Vaswani et al. [26].
26"
INTRODUCTION,0.034482758620689655,"Some recent examples of this research on ICL include Garg et al [6], which studies ICL by providing
27"
INTRODUCTION,0.035759897828863345,"a variety of function classes for models to learn, additionally benchmarking robustness by testing per-
28"
INTRODUCTION,0.037037037037037035,"formance on out-of-distribution data. Guo et al[11] shows the validity of composing simple function
29"
INTRODUCTION,0.038314176245210725,"classes to produce complex ones, while Liu et al [20] produced a metric for model information recall.
30"
INTRODUCTION,0.03959131545338442,"These works give us a set of metrics with which we can use to compare model performance on ICL.
31"
INTRODUCTION,0.04086845466155811,"ICL was initially primarily studied in attention-based models but has recently been explored in
32"
INTRODUCTION,0.0421455938697318,"other sequence models, creating discussion on its differences across those models and why these
33"
INTRODUCTION,0.04342273307790549,"Task
dim (d)
points (N)
x distribution
y calculation / parameter distribution
Task-specific"
INTRODUCTION,0.04469987228607918,"Linear Regression
20
41
N(0, Id)
w ∼N(0, Id)
–
Sparse Linear
20
41
N(0, Id)
w ∼N(0, Id), sparsity(w) ←k
k = 3
2-Layer MLP
20
101
N(0, Id)
W (1)
ij , W (2)
ij
∼N(0, 1)
width = 100
Decision Tree
20
101
N(0, Id)
leaf ∼N(0, 1), non_leaf ∼{1, ..., d}
depth = 4
Sparse Parity
10
140
{−1, 1}d
y = Q"
INTRODUCTION,0.04597701149425287,"j∈I x[j]
k = 2
Vector MQAR
20
128
Unif(Sd−1)
y ∼Unif(Sd−1)
–"
INTRODUCTION,0.04725415070242656,"Table 1: Summary of tasks. Each regression target fθ(xi) is either parametrized by a randomly
sampled θ or directly computed/sampled as detailed above."
INTRODUCTION,0.04853128991060025,"occur architecturally. In our paper, we study this by substituting key modern transformer (Llama)
34"
INTRODUCTION,0.04980842911877394,"components with Mamba blocks and GPT-2 components and richly benchmarking.
35"
INTRODUCTION,0.05108556832694764,"Since ICL for complete natural language understanding often requires training models with over a
36"
INTRODUCTION,0.05236270753512133,"billion parameters, the effects of architectural changes on fine-grained ICL abilities are often left
37"
INTRODUCTION,0.05363984674329502,"unexplored. As a consequence, although language models have progressed quickly and entertained
38"
INTRODUCTION,0.05491698595146871,"radically new architectures, there is limited extensible research that explores the effects of fine-grained
39"
INTRODUCTION,0.0561941251596424,"architecture choices on ICL ability [8, 14]. Garg et al. established using simple function classes to
40"
INTRODUCTION,0.05747126436781609,"evaluate ICL ability and examined solely GPT-2 as a sequence model. Lee et al. [8] expanded this
41"
INTRODUCTION,0.05874840357598978,"analysis on a slightly different set of function classes for a variety of base models. Park et al. [14]
42"
INTRODUCTION,0.06002554278416347,"evaluated ICL performance of 2 hybrid architectures between Mamba and GPT-2. Using unmodified
43"
INTRODUCTION,0.06130268199233716,"Llama/Mamba/GPT-2 as a control, we analyze GPT2-Llama and Llama-Mamba hybrid architectures
44"
INTRODUCTION,0.06257982120051085,"derived from replacing portions of GPT2 components with analogous Llama sections and LLama
45"
INTRODUCTION,0.06385696040868455,"with Mamba blocks, respectively, in 12 total architectures (3 unmodified + 9 hybrid).
46"
INTRODUCTION,0.06513409961685823,"We observe that the code written to analyze ICL with simple function classes – although almost
47"
INTRODUCTION,0.06641123882503193,"unanimously extensions of Garg et al.’s – often requires substantial, structural changes to the parent
48"
INTRODUCTION,0.06768837803320563,"codebase1, greatly heightening the barrier to extending each project in turn. Inspired by Donoho’s
49"
INTRODUCTION,0.06896551724137931,"ideal of Frictionless Reproducibility [27], we provide a set of simple abstractions and interfaces to
50"
INTRODUCTION,0.070242656449553,"facilitate extensions and modifications to our code while promoting interoperability between forks.
51"
RELATED WORK,0.07151979565772669,"2
Related Work
52"
RELATED WORK,0.07279693486590039,"There are many ways to capture qualitative aspects of ICL with quantitative measures. Weber et al.
53"
RELATED WORK,0.07407407407407407,"[17] compare the agreement between generations of a language model under varying prompts of
54"
RELATED WORK,0.07535121328224777,"equal meaning to test robustness to variations. Olsson et al. [22] compute a heuristic ""ICL score"" to
55"
RELATED WORK,0.07662835249042145,"measure an accuracy increase in predictions of a model given more context. We adapt this metric to
56"
RELATED WORK,0.07790549169859515,"fit our experimental setup more aptly, regularizing along both the number of in-context examples and
57"
RELATED WORK,0.07918263090676884,"against a baseline predictor.
58"
RELATED WORK,0.08045977011494253,"In general, evaluating ICL ability has been approached from two primary avenues: both when the
59"
RELATED WORK,0.08173690932311622,"only solution at train time is to meta-learn an algorithm [6, 8, 28, 11, 19] and when optimal loss
60"
RELATED WORK,0.08301404853128991,"at train time can also be satisfied by memorization or otherwise leveraging previously trained-on
61"
RELATED WORK,0.0842911877394636,"data [10, 23]. In this work, we take the former approach through learning a regression algorithm to
62"
RELATED WORK,0.08556832694763729,"randomized simple function classes [6, 11, 15].
63"
RELATED WORK,0.08684546615581099,"Further still, non-transformer architectures are capable of ICL [8]. Lee et al. [8] observed ICL
64"
RELATED WORK,0.08812260536398467,"in numerous sequence model architectures (e.g. RNNs, Mamba, S4, CNNs, GPT-2, and Llama)
65"
RELATED WORK,0.08939974457215837,"and found qualitative differences in each architecture’s performance. Chan et al. [25] found that
66"
RELATED WORK,0.09067688378033206,"Transformers depend on ""burstiness"" and long-tail distributions of natural data to outperform RNNs
67"
RELATED WORK,0.09195402298850575,"and LSTMs in ICL tasks. Park et al. [14] uses simple function classes similar to Garg et al. [6]
68"
RELATED WORK,0.09323116219667944,"in evaluating the ICL ability of Mamba, S4, S4-Mamba, and GPT-2. They find an overlapping but
69"
RELATED WORK,0.09450830140485313,"inequivalent set of function classes for which each model succeeds and construct a hybrid architecture
70"
RELATED WORK,0.09578544061302682,"1As mentioned, our code takes notable inspiration from the code distributed by Garg et al. [6], Park et
al. [14], and Lee et al. [8], which can be found at https://github.com/dtsip/in-context-learning,
https://github.com/krafton-ai/mambaformer-icl, and https://github.com/ivnle/synth-icl
respectively. The first two repositories are licensed under the MIT License and we could not identify the license
for the third."
RELATED WORK,0.0970625798212005,"to achieve the union of these abilities. We further this work by closely examining the contributions of
71"
RELATED WORK,0.0983397190293742,"individual architectural changes for GPT-2 and Llama-style transformers towards ICL ability.
72"
METHODS,0.09961685823754789,"3
Methods
73"
METHODS,0.10089399744572158,"As established by Garg et al. and extended by recent work, our ICL tasks take the following form
74"
METHODS,0.10217113665389528,"[6, 8, 14]:
75"
METHODS,0.10344827586206896,"x0, fθ(x0), x1, fθ(x1), ...,"
METHODS,0.10472541507024266,"query
z}|{
xN
|
{z
}
prompt P"
METHODS,0.10600255427841634,", fθ(xN)
| {z }
completion"
METHODS,0.10727969348659004,"where P is a series of input-output pairs followed by a lone query. The model predicts a completion
76"
METHODS,0.10855683269476372,"based on the prompt it received. The function parameters θ and the inputs xi are randomly sampled
77"
METHODS,0.10983397190293742,"from a function class domain and an input domain, respectively. The tasks we regress to are
78"
METHODS,0.1111111111111111,"summarized in Table 1 and detailed in Section 3.1
79"
METHODS,0.1123882503192848,"We train models for ICL by minimizing the expected loss over a distribution of prompts and cor-
80"
METHODS,0.1136653895274585,"responding function outputs. This approach allows us to observe qualitative differences in model
81"
METHODS,0.11494252873563218,"architectures by their ability to behave similarly to optimal or baseline estimators. To further simplify
82"
METHODS,0.11621966794380588,"ICL aptitude evaluation, we introduce a proxy value summarizing a given model’s ICL ability for
83"
METHODS,0.11749680715197956,"a specific task. This metric averages the error of a model normalized by the baseline error at each
84"
METHODS,0.11877394636015326,"context length. We detail this further in Section 3.3.
85"
TRAINING,0.12005108556832694,"3.1
Training
86"
TRAINING,0.12132822477650064,"To determine task-specific ICL ability, our sequence models regress onto the functions shown
87"
TRAINING,0.12260536398467432,"above [14]. We replicate the function classes Linear Regression, Sparse Linear Regression,
88"
TRAINING,0.12388250319284802,"2-Layer MLP Regression, and Decision Tree Regression from Garg et al.
[6] as they
89"
TRAINING,0.1251596424010217,"present a wide range of ""difficulty"" for sequence models. In addition, to capture the existence
90"
TRAINING,0.12643678160919541,"of some ICL ability, we also regress onto the two function classes examined in Park et al. [14]: parity
91"
TRAINING,0.1277139208173691,"function with induced sparsity (Sparse Parity) and parallel associative recall (Vector MQAR).
92"
TRAINING,0.12899106002554278,"Unless otherwise specified, we train all models with 12 layers, 8 attention heads, an expansion factor
93"
TRAINING,0.13026819923371646,"of 4 (in the case of models with Mamba Mixer layers), and linear layers to transform the input
94"
TRAINING,0.13154533844189017,"sequences into and from the embedding dimension of 256. We use the ADAM optimizer with a
95"
TRAINING,0.13282247765006386,"learning rate of 0.0001 for 500k steps. Our expansion factor was selected to ensure similar parameter
96"
TRAINING,0.13409961685823754,"counts across baselines and all other hyperparameters were chosen for consistency with Garg et al.
97"
TRAINING,0.13537675606641125,"[6]. Note for the four function classes from Garg et al., the same curriculum was used during training.
98"
TRAINING,0.13665389527458494,"No curriculum is used for the two new function classes from Park et al. [14]. For our compute2, we
99"
TRAINING,0.13793103448275862,"utilized 898.90 hours on an A10, 55.74 hours on an RTX 3090, 151.90 hours on an RTX 4090, 75.48
100"
TRAINING,0.1392081736909323,"hours on an RTX 4070 Ti, and 9.83 hours on an RTX 6000.
101"
TRAINING,0.140485312899106,"Linear Regression and Sparse Linear Regression Each function in these tasks is parametrized as a
102"
TRAINING,0.1417624521072797,"single weight vector (w) of dimension equal to that of the x-values (i.e. 20) so that y = wT x. We
103"
TRAINING,0.14303959131545338,"sample the coordinate values from a normal distribution and (in the Sparse Linear case) zero out all
104"
TRAINING,0.14431673052362706,"values except a uniformly at random selected k coordinates. In essence, one can consider Linear
105"
TRAINING,0.14559386973180077,"Regression to be the degenerate case where the k = 20. We preserve these tasks from Garg et al. [6]
106"
TRAINING,0.14687100893997446,"to verify that none of our hybrid modifications lose the near-optimal performance that was already
107"
TRAINING,0.14814814814814814,"found with GPT-2.
108"
TRAINING,0.14942528735632185,"2-Layer MLP Regression We fill two weight matrices W (1) ∈R100×20 and W (2) ∈R1×100 with
109"
TRAINING,0.15070242656449553,"scalar samples from a normal distribution. y values are computed as the result of a forward pass
110"
TRAINING,0.15197956577266922,"through a 2-layer multi layer perceptron with a ReLU activation. That is: y = W (2)ReLU(W (1)x).
111"
TRAINING,0.1532567049808429,"This is a more complex function class that Garg et al. [6] found that GPT-2 can perform very well at,
112"
TRAINING,0.1545338441890166,"suggesting that this task can capture some ICL ability of an architecture.
113"
TRAINING,0.1558109833971903,"2On an A10, the approximate training time for Linear Regression and Sparse Linear Regression
was 12 hours, for 2-Layer MLP Regression and Decision Tree Regression was 2 days, and for Vector
MQAR was 5 hours."
TRAINING,0.15708812260536398,"Decision Tree Regression We construct full decision trees of depth 4 with leaf values sampled from a
114"
TRAINING,0.1583652618135377,"normal distribution and branching conditions to be selected uniformly at random over the coordinates
115"
TRAINING,0.15964240102171137,"of the input dimension. The left branch is taken if the selected input coordinate is less than 0 and the
116"
TRAINING,0.16091954022988506,"right branch is taken otherwise. Garg et al. [6] found that GPT-2 was able to achieve much lower
117"
TRAINING,0.16219667943805874,"error for lower context lengths than XGBoost or Greedy Tree Learning, suggesting that this task can
118"
TRAINING,0.16347381864623245,"capture some ICL ability of an architecture.
119"
TRAINING,0.16475095785440613,"Sparse Parity We select k = 2 values to consider and compute their parity, expressed as either −1 or
120"
TRAINING,0.16602809706257982,"1. That is, we uniformly sample without replacement θ ∼{1, ..., 10}k and compute y = Q"
TRAINING,0.1673052362707535,"i∈θ x[i].
121"
TRAINING,0.1685823754789272,"Along with a higher learning rate of 0.0004, this is identical to the scheme implemented in Park et al.
122"
TRAINING,0.1698595146871009,"[14]. They [14] found that GPT-2 style transformers do not perform well on this task, suggesting that
123"
TRAINING,0.17113665389527458,"this is a discerning proxy for measuring ICL ability. Finally, as convergence was quick for this task,
124"
TRAINING,0.1724137931034483,"we only trained models up to 200k steps.
125"
TRAINING,0.17369093231162197,"Vector MQAR We sample 2N points from the d-sphere of radius
√"
TRAINING,0.17496807151979565,"d and group them randomly into
126"
TRAINING,0.17624521072796934,"pairs to forming N key-value pairs. For consistency with the experiments of Park et al. [14] and to
127"
TRAINING,0.17752234993614305,"reliably allow for the formation of transformer circuits highly relevant to this task [22, 14], we reduce
128"
TRAINING,0.17879948914431673,"model complexity by using an embedding dimension of 128, 2 layers, and a higher learning rate of
129"
TRAINING,0.18007662835249041,"0.0002. Park et al. [14] found that Mamba, our representative of SSM-type models, performed poorly,
130"
TRAINING,0.18135376756066413,"suggesting that this task can serve to ensure we don’t lose capabilities provided by transformers.
131"
TRAINING,0.1826309067688378,"Model Variation
Pos. Emb.
FFN
Normalization
(1)
GPT-2
Absolute
GELU MLP
Layer Norm
(1.1)
GPT-2 RMS
Absolute
GELU MLP
RMS Norm
(1.2)
GPT-2 RoPE
RoPE
GELU MLP
Layer Norm
(1.3)
GPT-2 SwiGLU
Absolute
SwiGLU
Layer Norm
(1.4)
GPT-2 RMS SwiGLU
Absolute
SwiGLU
RMS Norm
(1.5)
GPT-2 RMS RoPE
RoPE
GELU MLP
RMS Norm
(1.6)
GPT-2 RoPE SwiGLU
RoPE
SwiGLU
Layer Norm
(2)
Llama
RoPE
SwiGLU
RMS Norm
(2.1)
Llama RoPE-less
Mamba Mixer
SwiGLU
RMS Norm
(2.2)
Llama SwiGLU-less
RoPE
Mamba Mixer
RMS Norm
(2.3)
Llama RoPE,SwiGLU-less
Mamba Mixer
Mamba Mixer
RMS Norm
(3)
Mamba
–
Mamba Mixer
RMS Norm"
TRAINING,0.1839080459770115,"(a) For our hybrid architectures, we modify 3 types of architectural
sub-blocks: positional embeddings, feed-forward network, and normal-
izations. We specify the sub-block alternatives used for each architecture."
TRAINING,0.18518518518518517,"(b) A block diagram illustrating how
each variation affects the overall archi-
tecture. Note that vertical arrows in a
given block indicate that some varia-
tions skip that block entirely."
TRAINING,0.18646232439335889,Figure 1: Visual aid for our explored hybrid models in tabular and graphical format.
ARCHITECTURES,0.18773946360153257,"3.2
Architectures
132"
ARCHITECTURES,0.18901660280970625,"As detailed by Radford et al. [1], GPT-2 is almost identical to the original decoder-only transformer,
133"
ARCHITECTURES,0.19029374201787994,"with absolute positional embedding, pre-norm layer normalization, and a GELU activation function
134"
ARCHITECTURES,0.19157088122605365,"in the feed-forward network (FFN) (which is otherwise a multi-layer perceptron). In contrast, Llama
135"
ARCHITECTURES,0.19284802043422733,"[29, 30] combines a number of modern transformer modifications, including swapping layer norm
136"
ARCHITECTURES,0.194125159642401,"with RMS norm [31], changing the architecture and activation function of the FFN, and using rotary
137"
ARCHITECTURES,0.19540229885057472,"GPT-2
Llama
Mamba"
ARCHITECTURES,0.1966794380587484,"Positional Embedding
Absolute
RoPE
None
Feed Forward Network
2 layer MLP
Convolutional MLP
None
Attention Mechanism
Multi-Query Multi-Head
Multi-Query Multi-Head
Mamba Mixer
Normalization
Layer Norm
RMS Norm
RMS Norm"
ARCHITECTURES,0.1979565772669221,"Table 2: A summary of the primary architectural differences between GPT-2, Llama, and Mamba.
We examine all variations between GPT-2 and Llama and all variations between Llama and Mamba."
ARCHITECTURES,0.19923371647509577,"positional embeddings instead of absolute positional embeddings [32]. We acknowledge that the
138"
ARCHITECTURES,0.20051085568326948,"larger variations of Llama2 [30] and both variations of Llama3 [33] used Grouped-Query Attention
139"
ARCHITECTURES,0.20178799489144317,"(GQA), however we surmise that at our model scales of ∼10 million parameters, GQA will not
140"
ARCHITECTURES,0.20306513409961685,"significantly affect the performance of our models. From an entirely different method of sequence
141"
ARCHITECTURES,0.20434227330779056,"modeling, Mamba forgoes positional embedding entirely, combining features of the Gated Linear
142"
ARCHITECTURES,0.20561941251596424,"Unit and state space expansion to remove the need for distinct attention and feed-forward blocks.
143"
ARCHITECTURES,0.20689655172413793,"We summarize these architectural differences in Table 2. We examine all combinations of these
144"
ARCHITECTURES,0.2081736909323116,"different components, training 12 total architectures (listed in Figure 1a) on our 6 tasks for a total of
145"
ARCHITECTURES,0.20945083014048532,"72 model-task pairs. Figure 1b illustrates how each of these variations compose into a model. We
146"
ARCHITECTURES,0.210727969348659,"provide individual diagrams of each architecture in Appendix A.
147"
EVALUATION,0.2120051085568327,"3.3
Evaluation
148"
EVALUATION,0.21328224776500637,"In addition to the baseline metric (squared error as a function of context length) from Garg et. al.
149"
EVALUATION,0.21455938697318008,"[6], we’ve established another metric: ICL regression score. This is a scalar expressing overall
150"
EVALUATION,0.21583652618135377,"performance of a model on a task. Abstractly, the metric aims to capture the proportion of the baseline
151"
EVALUATION,0.21711366538952745,"error saved by a model. The regression score is calculated by (1) computing the difference in error
152"
EVALUATION,0.21839080459770116,"achieved by the model and the zero estimator at each context length, (2) computing the average of
153"
EVALUATION,0.21966794380587484,"this value over the length of the sequence, (3) computing the same value for the baseline estimator,
154"
EVALUATION,0.22094508301404853,"and (4) taking the ratio of these.
155"
EVALUATION,0.2222222222222222,"In summary, ICL regression score can be calculated as follows:
156"
EVALUATION,0.22349936143039592,Smodel = P
EVALUATION,0.2247765006385696,"i

ξ(i)
model −ξ(i)
0
 P"
EVALUATION,0.2260536398467433,"i

ξ(i)
base −ξ(i)
0

(1)"
EVALUATION,0.227330779054917,"where ξ(i)
model is the squared error of the model of interest at context length i. Sim. ξ(i)
base for baseline
157"
EVALUATION,0.22860791826309068,"and ξ(i)
0
for the zero estimator
158"
EVALUATION,0.22988505747126436,"Summation over context length allows our ICL regression score to be used for the comparison of
159"
EVALUATION,0.23116219667943805,"tasks with significantly differing context lengths. An interpretation for each of different possible
160"
EVALUATION,0.23243933588761176,"values of our ICL regression score is given in 2a. This approach builds off of Olsson et al.’s ""ICL
161"
EVALUATION,0.23371647509578544,"Score"" [22] by generalizing their selection of 500 and 50 in-context examples and reducing along the
162"
EVALUATION,0.23499361430395913,"context length, allowing for tasks with widely different context lengths to be directly compared. We
163"
EVALUATION,0.23627075351213284,"list our baselines in Table 2b.
164"
EVALUATION,0.23754789272030652,"We replicate the baseline predictors for linear regression, sparse linear regression, and MLP regression
165"
EVALUATION,0.2388250319284802,"from Garg et al. [6] due to the lack of a higher-performing baseline. However, we opted to use
166"
EVALUATION,0.24010217113665389,"a pretrained GPT-2 model with identical structure to that used in Garg et al. to serve as a more
167"
EVALUATION,0.2413793103448276,"calibrated baseline than Greedy Tree Learning or XGBoost. They showed superior decision tree ICL
168"
EVALUATION,0.24265644955300128,"performance for a trained GPT-2 transformer compared to Greedy Tree Learning or XGBoost. For
169"
EVALUATION,0.24393358876117496,"consistency with Park et al. [14] and due to the algorithmic hardness of Sparse Parity, we used
170"
EVALUATION,0.24521072796934865,"our Mamba model trained on this task. Park et al. showed that Mamba can effectively learn this task,
171"
EVALUATION,0.24648786717752236,"so we repeat our strategy as in Decision Tree Regression with our Mamba model (instead of
172"
EVALUATION,0.24776500638569604,"GPT-2) as a baseline.
173"
REPRODUCIBILITY STATEMENT,0.24904214559386972,"3.4
Reproducibility Statement
174"
REPRODUCIBILITY STATEMENT,0.2503192848020434,"For ease of experimentation and reproducibility, we have built a typed, extensible, and modular
175"
REPRODUCIBILITY STATEMENT,0.2515964240102171,"Python codebase. We achieved this by identifying isolated processes in the training regime and
176"
REPRODUCIBILITY STATEMENT,0.25287356321839083,"Condition
Interpretation"
REPRODUCIBILITY STATEMENT,0.2541507024265645,"Smodel > 1
model outperforms baseline
Smodel = 1
model matches baseline
Smodel < 1
model underperforms baseline
Smodel < 0
model underperforms zero estimator"
REPRODUCIBILITY STATEMENT,0.2554278416347382,"(a) Interpretation of possible Smodel values com-
puted over context length."
REPRODUCIBILITY STATEMENT,0.2567049808429119,"Task
Baseline Predictor"
REPRODUCIBILITY STATEMENT,0.25798212005108556,"Linear
Least Squares
Sparse Linear
LASSO
MLP
2-layer NN
Decision Tree
GPT-2
Sparse Parity
Mamba"
REPRODUCIBILITY STATEMENT,0.25925925925925924,"(b) The baselines for each task. The 2-layer NN is trained
for 1000 gradient steps, with a batch consisting of a ran-
domly selected point in the context. GPT-2 and Mamba
are trained for 500k steps on the specified task in the
same format as all other models."
REPRODUCIBILITY STATEMENT,0.26053639846743293,Figure 2: Predictors and conditions for computation and interpretation of ICL regression score.
REPRODUCIBILITY STATEMENT,0.26181353767560667,"structuring our code to reflect them. In particular, the specification of (1) a function class, (2) a
177"
REPRODUCIBILITY STATEMENT,0.26309067688378035,"model type, (3) an evaluation scheme, and (4) a stage of training under a curriculum are all inherent
178"
REPRODUCIBILITY STATEMENT,0.26436781609195403,"to the experiment archetype as proposed by Garg et al. [6] and repeated by others [8, 15, 14]. We
179"
REPRODUCIBILITY STATEMENT,0.2656449553001277,"integrate standard reporting software Weights and Biases [34] and leverage fast implementations
180"
REPRODUCIBILITY STATEMENT,0.2669220945083014,"of attention [35] and 1-D convolutions [36]. We also implement a configuration-based system for
181"
REPRODUCIBILITY STATEMENT,0.2681992337164751,"training, loading, and evaluating models to facilitate frictionless repeatability of all experiments.
182"
RESULTS,0.26947637292464877,"4
Results
183"
RESULTS,0.2707535121328225,"We confirm the results from Garg et al. [6] and Park et al. [14] that GPT-2 and Mamba can
184"
RESULTS,0.2720306513409962,"learn our first four regression tasks in context. Park et al. [14] that Mamba struggles to perform
185"
RESULTS,0.27330779054916987,"Vector MQAR while transformers and hybrid architectures excel. We note that Llama and GPT-2
186"
RESULTS,0.27458492975734355,"have very comparable performance in Sparse Parity and Vector MQAR. We plot all qualitatively
187"
RESULTS,0.27586206896551724,"non-optimal squared error profiles in Figure 3 and all squared error profiles in Appendix B.
188"
RESULTS,0.2771392081736909,"(a) Notable phenomena for Sparse Linear. We
observe that while GPT-2 (orange) performs very
similarly to our baseline, adding RMS norm with-
out RoPE (red and green) leads to models perform-
ing notably worse than optimal."
RESULTS,0.2784163473818646,"(b) Notable phenomena for Decision Tree. We
note that Mamba (green) performs somewhat sub-
optimally while GPT-2 RMS (orange) fails to learn
the task entirely."
RESULTS,0.2796934865900383,"Figure 3: Squared error profiles that do not exhibit near-optimal behavior. Shaded regions are 99%
confidence intervals."
RESULTS,0.280970625798212,"Models can converge to suboptimal regression schemes. We find that some model-task pairs
189"
RESULTS,0.2822477650063857,"produce suboptimal predictions, not as a result of insufficient training. A clear example is GPT-2
190"
RESULTS,0.2835249042145594,"RMS SwiGLU (model 1.4) on Sparse Linear. This model appears to not achieve optimal error
191"
RESULTS,0.2848020434227331,"– achieving an ICL Regression Score of only 0.754, opposed to ∼0.93 by other models – and yet
192"
RESULTS,0.28607918263090676,"its performance does not significantly improve with more gradient steps. We plot the squared error
193"
RESULTS,0.28735632183908044,"achieved by various checkpoints for model 1.4 in Figure 4a. We observe that this error profile appears
194"
RESULTS,0.2886334610472541,"similar to that of models trained on the Linear task and so also examine the prediction quality of the
195"
RESULTS,0.28991060025542786,"(a) GPT-2 RMS SwiGLU Checkpoints on Sparse
Linear. We see that GPT-2 RMS SwiGLU converges
to the least squares solution, despite Lasso being the op-
timal solution. This suggests that GPT-2 RMS SwiGLU
fails to learn to utilize its context to its fullest extent."
RESULTS,0.29118773946360155,"(b) GPT-2 RMS SwiGLU trained on
Sparse Linear and evaluated on Linear.
When evaluated on a similar task to which
it was trained on, GPT-2 RMS SwiGLU ap-
pears to perform better than its siblings, de-
spite the fact that it performed worse than its
siblings on its original task! This suggests
that it learned a different regression scheme
than GPT-2 on the same training data."
RESULTS,0.29246487867177523,"Figure 4: Detailing plots to showcase GPT-2 RMS SwiGLU (model 1.4) learning a more general
but sub-optimal regression scheme when trained on Sparse Linear. Shaded regions are 99%
confidence intervals."
RESULTS,0.2937420178799489,"same model (GPT-2 RMS SwiGLU trained on Sparse Linear) on Linear in Figure 4b. We find
196"
RESULTS,0.2950191570881226,"that it indeed mimics the error profile of least squares. This result builds on Akyürek et al.’s findings
197"
RESULTS,0.2962962962962963,"[19] in what functions transformer models develop representations of. Akyürek et al. analyzed
198"
RESULTS,0.29757343550446996,"algorithms representable by GPT-2 like architectures. We note that they did not examine other layer
199"
RESULTS,0.2988505747126437,"types such as Mamba Mixer or SwiGLU.
200"
RESULTS,0.3001277139208174,"Models can escape suboptimal regression schemes. We see that GPT-2 SwiGLU (model 1.3)
201"
RESULTS,0.30140485312899107,"Sparse Linear on adopts a suboptimal regression scheme (least squares) partway in training,
202"
RESULTS,0.30268199233716475,"eventually unlearning its scheme in favor of the optimal regression scheme (lasso). We plot the
203"
RESULTS,0.30395913154533843,"squared error on Sparse Linear achieved by various checkpoints for Model 1.3 in Figure 5a, noting
204"
RESULTS,0.3052362707535121,"that the error of the checkpoint at 100k steps closely matches the error of least squares. Further, we
205"
RESULTS,0.3065134099616858,"examine the squared errors on Linear Regression for the various checkpoints for Model 1.3 in 5b and
206"
RESULTS,0.30779054916985954,"see that the checkpoint at 100k most closely matches least squares. This suggests that model 1.3
207"
RESULTS,0.3090676883780332,"learned the linear regression scheme in the beginning of training, but was eventually able to learn to
208"
RESULTS,0.3103448275862069,"utilize the sparse nature of its training data.
209"
RESULTS,0.3116219667943806,"Models can fail to converge within our training horizon. We find that a number of models
210"
RESULTS,0.3128991060025543,"performed strikingly poorly in their trained task. In particular, GPT-2 with Layer norm replaced by
211"
RESULTS,0.31417624521072796,"RMS norm (model 1.1) performed very poorly on Sparse Linear Regression and Decision
212"
RESULTS,0.31545338441890164,"Tree, as indicated by the lowest ICL Regression Score achieved in those tasks (0.535 and 0.114,
213"
RESULTS,0.3167305236270754,"respectively) and in Figures 3a and 3b. We also observe that GPT-2 with RMS and SwiGLU (model
214"
RESULTS,0.31800766283524906,"1.4) also did not converge to a regression scheme, despite apparently modelling a different regression
215"
RESULTS,0.31928480204342274,"scheme entirely. Similarly, Mamba (model 3) did not converge to a training scheme on Decision
216"
RESULTS,0.3205619412515964,"Tree as illustrated in Figure 6a. We believe this suggests a lower training efficiency for certain
217"
RESULTS,0.3218390804597701,"architectures on these tasks.
218"
RESULTS,0.3231162196679438,"Models can fail to learn the task entirely. In the case of Decision Tree, GPT-2 with RMS (model
219"
RESULTS,0.3243933588761175,"1.1) failed to learn the task entirely as not only indicated by its final ICL Regression Score but also
220"
RESULTS,0.32567049808429116,"its consistency in achieving very high error throughout training. We plot squared error for various
221"
RESULTS,0.3269476372924649,"checkpoints in Figure 6b.
222"
RESULTS,0.3282247765006386,"ICL Regression Scores reflect qualitative information contained in squared-error plots. Com-
223"
RESULTS,0.32950191570881227,"puted ICL Regression Scores are summarized in Table 3. Overall, most models are able to perform
224"
RESULTS,0.33077905491698595,"comparably to our baseline estimators, with nearly all examined models achieving a regression score
225"
RESULTS,0.33205619412515963,"of approximately 1 on all four function classes from Garg et al. (Linear Regression, Sparse
226"
RESULTS,0.3333333333333333,"Linear Regression, 2-Layer MLP, Decision Tree). The ICL Regression Scores for Linear
227"
RESULTS,0.334610472541507,"(a)
GPT-2
SwiGLU
Checkpoints
on
Sparse Linear. In the beginning of train-
ing, GPT-2 SwiGLU quickly converges to
least squares, but it is able to escape this re-
gression scheme and eventually has its error
profile approach that of Lasso."
RESULTS,0.33588761174968074,"(b) GPT-2 SwiGLU Checkpoints trained
on Sparse Parity and evaluated on
Linear Regression. We see that an ear-
lier checkpoint (100k) of GPT-2 SwiGLU
outperforms later checkpoints on a similar
task different from the task it was trained on."
RESULTS,0.3371647509578544,"Figure 5: Detailing plots to showcase GPT-2 SwiGLU (model 1.3) starting by learning a more general
but sub-optimal regression scheme but eventually converging to the optimal regression scheme when
trained on Sparse Linear. Shaded regions are 99% confidence intervals."
RESULTS,0.3384418901660281,"(a) Mamba Checkpoints on Decision Tree.
We see that Mamba does keep improving its er-
ror profile throughout training. This suggests that
Mamba did not reach convergence, and thus has
lower training efficiency on this task."
RESULTS,0.3397190293742018,"(b) GPT-2 RMS Checkpoints on Decision
Tree. We see that all checkpoints of GPT-2 per-
form very similarly, with little to no change in error
profile throughout training."
RESULTS,0.34099616858237547,"Figure 6: Squared error as a function of context length computed for various checkpoints for both
Mamba (model 3) and GPT-2 RMS (model 1.1) on Decision Tree. Shaded regions are 99%
confidence intervals."
RESULTS,0.34227330779054915,"Regression and 2-Layer MLP, along with their corresponding graphs of squared error as a function
228"
RESULTS,0.34355044699872284,"of context length, corroborate the claims from Garg et al. [6] that transformers can ""learn"" these tasks.
229"
RESULTS,0.3448275862068966,"Further, the ICL Regression Scores for Sparse Parity are consistent with Park et al. [14], with all
230"
RESULTS,0.34610472541507026,"hybrids between GPT-2, and Llama failing to ""learn"" the task and all hybrids between Llama and
231"
RESULTS,0.34738186462324394,"Mamba succeeding in ""learning"" the task. Indeed, the ICL Regression Score achieved by Mamba
232"
RESULTS,0.3486590038314176,"captures the qualitatively sub-optimal performance detailed above on Decision Tree.
233"
DISCUSSION,0.3499361430395913,"5
Discussion
234"
DISCUSSION,0.351213282247765,"Even simple function classes leave room for local minima. We find that despite distilling down the
235"
DISCUSSION,0.3524904214559387,"phenomenon of In Context Learning to regression against simple function classes, there still exists
236"
DISCUSSION,0.3537675606641124,"room for models to adopt various regression schemes. This is supported by the apparent convergence
237 Model"
DISCUSSION,0.3550446998722861,"Linear
±0.001"
DISCUSSION,0.3563218390804598,"Sparse Linear
±0.001"
-LAYER MLP,0.35759897828863346,"2-Layer MLP
±0.06"
-LAYER MLP,0.35887611749680715,"Decision Tree
±0.001"
-LAYER MLP,0.36015325670498083,"Sparse Parity
±0.001"
-LAYER MLP,0.3614303959131545,"(1)
GPT-2
0.996
0.932
1.130
1.000*
0.023
(1.1)
GPT-2 RMS
0.997
0.535
1.130
0.114
–
(1.2)
GPT-2 RoPE
0.995
0.927
1.130
1.004
–
(1.3)
GPT-2 SwiGLU
0.997
0.913
1.128
0.994
–
(1.4)
GPT-2 RMS SwiGLU
0.997
0.754
1.129
0.971
–
(1.5)
GPT-2 RMS RoPE
0.996
0.927
1.128
1.005
–
(1.6)
GPT-2 RoPE SwiGLU
0.996
0.929
1.129
1.011
–
(2)
Llama
0.997
0.933
1.129
1.007
0.023
(2.1)
Llama RoPE-less
0.996
0.928
1.130
1.018
1.000
(2.2)
Llama SwiGLU-less
0.996
0.927
1.129
0.980
1.000
(2.3)
Llama RoPE,SwiGLU-less
0.996
0.938
1.130
1.012
1.000
(3)
Mamba
0.995
0.925
1.123
0.832
1.000*"
-LAYER MLP,0.36270753512132825,"Table 3: ICL Regression Scores for each architecture on each task, averaged over many sampled
functions, with 95% confidence intervals in the headers for each row. Best-in-task values are in
boldface except when not statistically significant from another architecture. GPT-2/Llama hybrids
were not evaluated on Sparse Parity due to compute constraints and lack of supporting evidence that
they should succeed. *These models were used as the baseline for this task."
-LAYER MLP,0.36398467432950193,"of the error profiles of GPT-2 RMS (model 1.1) and GPT-2 RMS SwiGLU (model 1.4) to least
238"
-LAYER MLP,0.3652618135376756,"squares regression for shorter context lengths.
239"
-LAYER MLP,0.3665389527458493,"Hybrid architectures and function classes have varying levels of compatibility. Specific hybrid
240"
-LAYER MLP,0.367816091954023,"architectures can hesitate to learn/converge for certain function classes. This behavior is especially
241"
-LAYER MLP,0.36909323116219667,"apparent in GPT-2 RMS’s (model 1.1) Decision Tree error graph and GPT-2 RMS SwiGLU’s (model
242"
-LAYER MLP,0.37037037037037035,"1.4) Sparse Linear performance. It seems that GPT-2 RMS SwiGLU shows greater affinity towards
243"
-LAYER MLP,0.3716475095785441,"learning least squares instead of LASSO. Certain hybrid architecture variations may place inductive
244"
-LAYER MLP,0.37292464878671777,"biases on certain solution forms, resulting in extreme convergence times when these solution forms
245"
-LAYER MLP,0.37420178799489145,"greatly vary from the optimal predictor’s form.
246"
-LAYER MLP,0.37547892720306514,"Extensible Research as Reproducible Research. In the development of this work, continuously
247"
-LAYER MLP,0.3767560664112388,"iterating to minimize the friction of reproduction has enabled rapid extension of our Python artifacts
248"
-LAYER MLP,0.3780332056194125,"to support even abstractly defined hybrid architectures, which are often considered inextricable from
249"
-LAYER MLP,0.3793103448275862,"highly bespoke code or dedicated packages such as xFormers [37]. We implore the reader to seriously
250"
-LAYER MLP,0.38058748403575987,"consider the value of making their research extensible with a minimum of friction. We hope that our
251"
-LAYER MLP,0.3818646232439336,"attempts to maximize extensibility and reproducibility contribute to the longevity of this work as a
252"
-LAYER MLP,0.3831417624521073,"reliable, tested, and simple framework to use for studying simple function classes in context.
253"
LIMITATIONS AND FUTURE WORK,0.384418901660281,"5.1
Limitations and Future Work
254"
LIMITATIONS AND FUTURE WORK,0.38569604086845466,"We have only one training run performed on each model-task pair. As a result, we have no
255"
LIMITATIONS AND FUTURE WORK,0.38697318007662834,"estimation for how consistently observed phenomena appear with the given architectures. We only
256"
LIMITATIONS AND FUTURE WORK,0.388250319284802,"train each model for a maximum of 500K steps. Thus, when a model fails to converge within this
257"
LIMITATIONS AND FUTURE WORK,0.3895274584929757,"window, we lose information on insightful trends that could possibly occur with further training.
258"
LIMITATIONS AND FUTURE WORK,0.39080459770114945,"We do not empirically evaluate the effectiveness of ICL Regression Score or the usability of our
259"
LIMITATIONS AND FUTURE WORK,0.39208173690932313,"provided code platform. We compute no verifying metrics to establish how well ICL Regression
260"
LIMITATIONS AND FUTURE WORK,0.3933588761174968,"Score generalizes or is robust to qualitatively distinct ICL regression tasks. Similarly, we perform no
261"
LIMITATIONS AND FUTURE WORK,0.3946360153256705,"user study on the effectiveness of our code platform, presenting only our own experience.
262"
LIMITATIONS AND FUTURE WORK,0.3959131545338442,"Future Work In this paper we analyze ICL performance for GPT-2-Llama and Llama-Mamba
263"
LIMITATIONS AND FUTURE WORK,0.39719029374201786,"hybrid architectures (9 total) on 6 tasks. Future relevant research could entail 1) expanding our
264"
LIMITATIONS AND FUTURE WORK,0.39846743295019155,"architecture-space and streamlining our training-to-evaluation pipeline by creating an architecture
265"
LIMITATIONS AND FUTURE WORK,0.3997445721583653,"search mechanism, 2) assessing our models on other sets of tasks, such as ones relating to lan-
266"
LIMITATIONS AND FUTURE WORK,0.40102171136653897,"guage modeling or image classification, 3) verifying our results with additional training runs, 4)
267"
LIMITATIONS AND FUTURE WORK,0.40229885057471265,"benchmarking model performance along hardware-related metrics.
268"
REFERENCES,0.40357598978288634,"References
269"
REFERENCES,0.40485312899106,"[1] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
270"
REFERENCES,0.4061302681992337,"models are unsupervised multitask learners. 2019.
271"
REFERENCES,0.4074074074074074,"[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
272"
REFERENCES,0.4086845466155811,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
273"
REFERENCES,0.4099616858237548,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
274"
REFERENCES,0.4112388250319285,"[3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter
275"
REFERENCES,0.4125159642401022,"Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
276"
REFERENCES,0.41379310344827586,"via sequence modeling. CoRR, abs/2106.01345, 2021.
277"
REFERENCES,0.41507024265644954,"[4] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
278"
REFERENCES,0.4163473818646232,"Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.
279"
REFERENCES,0.41762452107279696,"[5] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
280"
REFERENCES,0.41890166028097064,"Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
281"
REFERENCES,0.42017879948914433,"Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,
282"
REFERENCES,0.421455938697318,"Jan Leike, and Ryan Lowe. Training language models to follow instructions with human
283"
REFERENCES,0.4227330779054917,"feedback, 2022.
284"
REFERENCES,0.4240102171136654,"[6] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers
285"
REFERENCES,0.42528735632183906,"learn in-context? a case study of simple function classes. Advances in Neural Information
286"
REFERENCES,0.42656449553001274,"Processing Systems, 35:30583–30598, 2022.
287"
REFERENCES,0.4278416347381865,"[7] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and
288"
REFERENCES,0.42911877394636017,"Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning
289"
REFERENCES,0.43039591315453385,"work? arXiv preprint arXiv:2202.12837, 2022.
290"
REFERENCES,0.43167305236270753,"[8] Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. Is attention required for icl? exploring the
291"
REFERENCES,0.4329501915708812,"relationship between model architecture and in-context learning ability, 2023.
292"
REFERENCES,0.4342273307790549,"[9] Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina
293"
REFERENCES,0.4355044699872286,"Rimsky, Meg Tong, Jesse Mu, Daniel Ford, et al. Many-shot jailbreaking.
294"
REFERENCES,0.4367816091954023,"[10] Aaditya Singh, Stephanie Chan, Ted Moskovitz, Erin Grant, Andrew Saxe, and Felix Hill.
295"
REFERENCES,0.438058748403576,"The transient nature of emergent in-context learning in transformers. Advances in Neural
296"
REFERENCES,0.4393358876117497,"Information Processing Systems, 36, 2024.
297"
REFERENCES,0.44061302681992337,"[11] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai.
298"
REFERENCES,0.44189016602809705,"How do transformers learn in-context beyond simple functions? a case study on learning with
299"
REFERENCES,0.44316730523627074,"representations. arXiv preprint arXiv:2310.10616, 2023.
300"
REFERENCES,0.4444444444444444,"[12] Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau.
301"
REFERENCES,0.44572158365261816,"Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023.
302"
REFERENCES,0.44699872286079184,"[13] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:
303"
REFERENCES,0.4482758620689655,"Provable in-context learning with in-context algorithm selection. Advances in neural information
304"
REFERENCES,0.4495530012771392,"processing systems, 36, 2024.
305"
REFERENCES,0.4508301404853129,"[14] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak,
306"
REFERENCES,0.4521072796934866,"Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative
307"
REFERENCES,0.45338441890166026,"study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024.
308"
REFERENCES,0.454661558109834,"[15] Kartik Ahuja and David Lopez-Paz. A closer look at in-context learning under distribution
309"
REFERENCES,0.4559386973180077,"shifts. arXiv preprint arXiv:2305.16704, 2023.
310"
REFERENCES,0.45721583652618136,"[16] Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning:
311"
REFERENCES,0.45849297573435505,"Architectures and algorithms. arXiv preprint arXiv:2401.12973, 2024.
312"
REFERENCES,0.45977011494252873,"[17] Lucas Weber, Elia Bruni, and Dieuwke Hupkes. The icl consistency test. arXiv preprint
313"
REFERENCES,0.4610472541507024,"arXiv:2312.04945, 2023.
314"
REFERENCES,0.4623243933588761,"[18] Noam Wies, Yoav Levine, and Amnon Shashua.
The learnability of in-context learning.
315"
REFERENCES,0.46360153256704983,"Advances in Neural Information Processing Systems, 36, 2024.
316"
REFERENCES,0.4648786717752235,"[19] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning
317"
REFERENCES,0.4661558109833972,"algorithm is in-context learning? investigations with linear models, 2023.
318"
REFERENCES,0.4674329501915709,"[20] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.
319"
REFERENCES,0.46871008939974457,"What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.
320"
REFERENCES,0.46998722860791825,"[21] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao
321"
REFERENCES,0.47126436781609193,"Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently.
322"
REFERENCES,0.4725415070242657,"arXiv preprint arXiv:2303.03846, 2023.
323"
REFERENCES,0.47381864623243936,"[22] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
324"
REFERENCES,0.47509578544061304,"Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning
325"
REFERENCES,0.4763729246487867,"and induction heads. arXiv preprint arXiv:2209.11895, 2022.
326"
REFERENCES,0.4776500638569604,"[23] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of
327"
REFERENCES,0.4789272030651341,"in-context learning as implicit bayesian inference, 2022.
328"
REFERENCES,0.48020434227330777,"[24] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mord-
329"
REFERENCES,0.48148148148148145,"vintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient
330"
REFERENCES,0.4827586206896552,"descent, 2023.
331"
REFERENCES,0.4840357598978289,"[25] Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh,
332"
REFERENCES,0.48531289910600256,"Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive
333"
REFERENCES,0.48659003831417624,"emergent in-context learning in transformers, 2022.
334"
REFERENCES,0.4878671775223499,"[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
335"
REFERENCES,0.4891443167305236,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
336"
REFERENCES,0.4904214559386973,"processing systems, 30, 2017.
337"
REFERENCES,0.49169859514687103,"[27] David Donoho. Data science at the singularity. arXiv preprint arXiv:2310.00865, 2023.
338"
REFERENCES,0.4929757343550447,"[28] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing
339"
REFERENCES,0.4942528735632184,"attention glitches with flip-flop language modeling, 2023.
340"
REFERENCES,0.4955300127713921,"[29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
341"
REFERENCES,0.49680715197956576,"thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
342"
REFERENCES,0.49808429118773945,"Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
343"
REFERENCES,0.49936143039591313,"language models, 2023.
344"
REFERENCES,0.5006385696040868,"[30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
345"
REFERENCES,0.5019157088122606,"Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
346"
REFERENCES,0.5031928480204342,"Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,
347"
REFERENCES,0.5044699872286079,"Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony
348"
REFERENCES,0.5057471264367817,"Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
349"
REFERENCES,0.5070242656449553,"Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
350"
REFERENCES,0.508301404853129,"Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
351"
REFERENCES,0.5095785440613027,"Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
352"
REFERENCES,0.5108556832694764,"Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-
353"
REFERENCES,0.51213282247765,"qing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
354"
REFERENCES,0.5134099616858238,"Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
355"
REFERENCES,0.5146871008939975,"Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation
356"
REFERENCES,0.5159642401021711,"and fine-tuned chat models, 2023.
357"
REFERENCES,0.5172413793103449,"[31] Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019.
358"
REFERENCES,0.5185185185185185,"[32] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer:
359"
REFERENCES,0.5197956577266922,"Enhanced transformer with rotary position embedding, 2021.
360"
REFERENCES,0.5210727969348659,"[33] AI@Meta. Llama 3 model card. 2024.
361"
REFERENCES,0.5223499361430396,"[34] Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from
362"
REFERENCES,0.5236270753512133,"wandb.com.
363"
REFERENCES,0.524904214559387,"[35] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.
364"
REFERENCES,0.5261813537675607,"[36] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces,
365"
REFERENCES,0.5274584929757343,"2023.
366"
REFERENCES,0.5287356321839081,"[37] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,
367"
REFERENCES,0.5300127713920817,"Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza,
368"
REFERENCES,0.5312899106002554,"Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hack-
369"
REFERENCES,0.5325670498084292,"able transformer modelling library. https://github.com/facebookresearch/xformers,
370"
REFERENCES,0.5338441890166028,"2022.
371"
REFERENCES,0.5351213282247765,"A
Architectures
372"
REFERENCES,0.5363984674329502,"(a) The GPT-2 Architecture
(b) The Llama Architecture
(c) The Mamba architecture"
REFERENCES,0.5376756066411239,"Figure 7: The GPT-2, Llama, and Mamba architectures used in our regression tasks"
REFERENCES,0.5389527458492975,"(a) Llama with the feed-forward
block replaced by a Mamba Mixer
block"
REFERENCES,0.5402298850574713,"(b) Llama with rope embeddings
removed and a Mamba Mixer
prepended to serve as a ""posi-
tional embedder"""
REFERENCES,0.541507024265645,"(c) Llama with the feed-forward
block replaced by a Mamba Mixer
block, rope embeddings removed,
and a Mamba Mixer prepended to
serve as a ""positional embedder"""
REFERENCES,0.5427841634738186,Figure 8: The hybrid architectures as modifications to Llama
REFERENCES,0.5440613026819924,"(a) GPT-2 with the GELU MLP
replaced by a SwiGLU"
REFERENCES,0.545338441890166,"(b) GPT-2 with the absolute po-
sitional encodings removed and
rotary position embeddings in-
cluded in attention"
REFERENCES,0.5466155810983397,"(c) GPT-2 with the Layer Norm
replaced by an RMS Norm"
REFERENCES,0.5478927203065134,"(d) GPT-2 with the GELU MLP
replaced by a SwiGLU and the
Layer Norm replaced by an RMS
Norm"
REFERENCES,0.5491698595146871,"(e) GPT-2 with absolute posi-
tional encodings removed, rotary
position embeddings included in
attention, and the Layer Norm re-
placed by an RMS Norm"
REFERENCES,0.5504469987228607,"(f) GPT-2 with the GELU MLP re-
placed by a SwiGLU, absolute po-
sitional encodings removed, and
rotary position embeddings in-
cluded in attention"
REFERENCES,0.5517241379310345,Figure 9: The hybrid architectures as modifications to GPT-2
REFERENCES,0.5530012771392082,"B
Complete Experimental Results
373"
REFERENCES,0.5542784163473818,"B.1
Linear Regression
374"
REFERENCES,0.5555555555555556,"(a) GPT-2-Llama Hybrid Runs
(b) Llama-Mamba Hybrid Runs"
REFERENCES,0.5568326947637292,"(c) Residuals for GPT-2-Llama Hybrid Runs, taken
against Least Squares"
REFERENCES,0.558109833971903,"(d) Residuals for Llama-Mamba Hybrid Runs,
taken against Least Squares"
REFERENCES,0.5593869731800766,Figure 10: Linear Regression Runs with Residual Plots
REFERENCES,0.5606641123882503,"B.2
Sparse Linear Regression
375"
REFERENCES,0.561941251596424,"(a) GPT-2-Llama Hybrid Runs
(b) Llama-Mamba Hybrid Runs"
REFERENCES,0.5632183908045977,"(c) Residuals for GPT-2-Llama Hybrid Runs, taken
against Lasso with α = 0.001"
REFERENCES,0.5644955300127714,"(d) Residuals for Llama-Mamba
Hybrid Runs, taken against Lasso
with α = 0.001"
REFERENCES,0.565772669220945,Figure 11: Sparse Linear Regression Runs
REFERENCES,0.5670498084291188,"B.3
Decision Trees
376"
REFERENCES,0.5683269476372924,"(a) GPT-2-Llama Hybrid Runs
(b) Llama-Mamba Hybrid Runs"
REFERENCES,0.5696040868454662,Figure 12: Decision Tree Runs
REFERENCES,0.5708812260536399,"B.4
2-Layer NN Regression
377"
REFERENCES,0.5721583652618135,"(a) GPT-2-Llama Hybrid Runs
(b) Llama-Mamba Hybrid Runs"
REFERENCES,0.5734355044699873,Figure 13: 2-Layer NN Regression Runs
REFERENCES,0.5747126436781609,"B.5
Sparse Parity
378"
REFERENCES,0.5759897828863346,"(a) Hybrid and Base Model Runs
(b) Residuals for Hybrid and Base Model Runs"
REFERENCES,0.5772669220945083,Figure 14: Sparse Parity Runs with Residual Plots
REFERENCES,0.578544061302682,"B.6
Vector MQAR
379"
REFERENCES,0.5798212005108557,"(a) GPT-2-Llama Hybrid Training Runs
(b) Llama-Mamba Hybrid Training Runs"
REFERENCES,0.5810983397190294,Figure 15: Vector MQAR Training Runs
REFERENCES,0.5823754789272031,"NeurIPS Paper Checklist
380"
CLAIMS,0.5836526181353767,"1. Claims
381"
CLAIMS,0.5849297573435505,"Question: Do the main claims made in the abstract and introduction accurately reflect the
382"
CLAIMS,0.5862068965517241,"paper’s contributions and scope?
383"
CLAIMS,0.5874840357598978,"Answer: [Yes]
384"
CLAIMS,0.5887611749680716,"Justification: The abstract/introduction briefly covers the limitations of the paper while
385"
CLAIMS,0.5900383141762452,"introducing the main claims/findings/contributions. We reference relevant work we are
386"
CLAIMS,0.5913154533844189,"building off of.
387"
CLAIMS,0.5925925925925926,"Guidelines:
388"
CLAIMS,0.5938697318007663,"• The answer NA means that the abstract and introduction do not include the claims
389"
CLAIMS,0.5951468710089399,"made in the paper.
390"
CLAIMS,0.5964240102171137,"• The abstract and/or introduction should clearly state the claims made, including the
391"
CLAIMS,0.5977011494252874,"contributions made in the paper and important assumptions and limitations. A No or
392"
CLAIMS,0.598978288633461,"NA answer to this question will not be perceived well by the reviewers.
393"
CLAIMS,0.6002554278416348,"• The claims made should match theoretical and experimental results, and reflect how
394"
CLAIMS,0.6015325670498084,"much the results can be expected to generalize to other settings.
395"
CLAIMS,0.6028097062579821,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
396"
CLAIMS,0.6040868454661558,"are not attained by the paper.
397"
LIMITATIONS,0.6053639846743295,"2. Limitations
398"
LIMITATIONS,0.6066411238825032,"Question: Does the paper discuss the limitations of the work performed by the authors?
399"
LIMITATIONS,0.6079182630906769,"Answer: [Yes]
400"
LIMITATIONS,0.6091954022988506,"Justification: We briefly mention some limitations in our analysis and experiments in Section
401"
LIMITATIONS,0.6104725415070242,"5.1. We acknowledge our limited training runs, inexhaustive training horizon, incomplete
402"
LIMITATIONS,0.611749680715198,"evaluation of ICL Regression Score, and no metrics on the usability of our codebase.
403"
LIMITATIONS,0.6130268199233716,"Similarly here we acknowledge that this list of limitations is by no means exhaustive.
404"
LIMITATIONS,0.6143039591315453,"Guidelines:
405"
LIMITATIONS,0.6155810983397191,"• The answer NA means that the paper has no limitation while the answer No means that
406"
LIMITATIONS,0.6168582375478927,"the paper has limitations, but those are not discussed in the paper.
407"
LIMITATIONS,0.6181353767560664,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
408"
LIMITATIONS,0.6194125159642401,"• The paper should point out any strong assumptions and how robust the results are to
409"
LIMITATIONS,0.6206896551724138,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
410"
LIMITATIONS,0.6219667943805874,"model well-specification, asymptotic approximations only holding locally). The authors
411"
LIMITATIONS,0.6232439335887612,"should reflect on how these assumptions might be violated in practice and what the
412"
LIMITATIONS,0.6245210727969349,"implications would be.
413"
LIMITATIONS,0.6257982120051085,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
414"
LIMITATIONS,0.6270753512132823,"only tested on a few datasets or with a few runs. In general, empirical results often
415"
LIMITATIONS,0.6283524904214559,"depend on implicit assumptions, which should be articulated.
416"
LIMITATIONS,0.6296296296296297,"• The authors should reflect on the factors that influence the performance of the approach.
417"
LIMITATIONS,0.6309067688378033,"For example, a facial recognition algorithm may perform poorly when image resolution
418"
LIMITATIONS,0.632183908045977,"is low or images are taken in low lighting. Or a speech-to-text system might not be
419"
LIMITATIONS,0.6334610472541508,"used reliably to provide closed captions for online lectures because it fails to handle
420"
LIMITATIONS,0.6347381864623244,"technical jargon.
421"
LIMITATIONS,0.6360153256704981,"• The authors should discuss the computational efficiency of the proposed algorithms
422"
LIMITATIONS,0.6372924648786717,"and how they scale with dataset size.
423"
LIMITATIONS,0.6385696040868455,"• If applicable, the authors should discuss possible limitations of their approach to
424"
LIMITATIONS,0.6398467432950191,"address problems of privacy and fairness.
425"
LIMITATIONS,0.6411238825031929,"• While the authors might fear that complete honesty about limitations might be used by
426"
LIMITATIONS,0.6424010217113666,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
427"
LIMITATIONS,0.6436781609195402,"limitations that aren’t acknowledged in the paper. The authors should use their best
428"
LIMITATIONS,0.644955300127714,"judgment and recognize that individual actions in favor of transparency play an impor-
429"
LIMITATIONS,0.6462324393358876,"tant role in developing norms that preserve the integrity of the community. Reviewers
430"
LIMITATIONS,0.6475095785440613,"will be specifically instructed to not penalize honesty concerning limitations.
431"
THEORY ASSUMPTIONS AND PROOFS,0.648786717752235,"3. Theory Assumptions and Proofs
432"
THEORY ASSUMPTIONS AND PROOFS,0.6500638569604087,"Question: For each theoretical result, does the paper provide the full set of assumptions and
433"
THEORY ASSUMPTIONS AND PROOFS,0.6513409961685823,"a complete (and correct) proof?
434"
THEORY ASSUMPTIONS AND PROOFS,0.6526181353767561,"Answer: [NA]
435"
THEORY ASSUMPTIONS AND PROOFS,0.6538952745849298,"Justification: There are no theoretical results or claims in this paper, and thus no assumptions
436"
THEORY ASSUMPTIONS AND PROOFS,0.6551724137931034,"and proofs are required.
437"
THEORY ASSUMPTIONS AND PROOFS,0.6564495530012772,"Guidelines:
438"
THEORY ASSUMPTIONS AND PROOFS,0.6577266922094508,"• The answer NA means that the paper does not include theoretical results.
439"
THEORY ASSUMPTIONS AND PROOFS,0.6590038314176245,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
440"
THEORY ASSUMPTIONS AND PROOFS,0.6602809706257982,"referenced.
441"
THEORY ASSUMPTIONS AND PROOFS,0.6615581098339719,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
442"
THEORY ASSUMPTIONS AND PROOFS,0.6628352490421456,"• The proofs can either appear in the main paper or the supplemental material, but if
443"
THEORY ASSUMPTIONS AND PROOFS,0.6641123882503193,"they appear in the supplemental material, the authors are encouraged to provide a short
444"
THEORY ASSUMPTIONS AND PROOFS,0.665389527458493,"proof sketch to provide intuition.
445"
THEORY ASSUMPTIONS AND PROOFS,0.6666666666666666,"• Inversely, any informal proof provided in the core of the paper should be complemented
446"
THEORY ASSUMPTIONS AND PROOFS,0.6679438058748404,"by formal proofs provided in appendix or supplemental material.
447"
THEORY ASSUMPTIONS AND PROOFS,0.669220945083014,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
448"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6704980842911877,"4. Experimental Result Reproducibility
449"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6717752234993615,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
450"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6730523627075351,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
451"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6743295019157088,"of the paper (regardless of whether the code and data are provided or not)?
452"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6756066411238825,"Answer: [Yes]
453"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6768837803320562,"Justification: We discuss training and evaluation in our paper while making our codebase
454"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6781609195402298,"accessible.
455"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6794380587484036,"Guidelines:
456"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6807151979565773,"• The answer NA means that the paper does not include experiments.
457"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6819923371647509,"• If the paper includes experiments, a No answer to this question will not be perceived
458"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6832694763729247,"well by the reviewers: Making the paper reproducible is important, regardless of
459"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6845466155810983,"whether the code and data are provided or not.
460"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.685823754789272,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
461"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6871008939974457,"to make their results reproducible or verifiable.
462"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6883780332056194,"• Depending on the contribution, reproducibility can be accomplished in various ways.
463"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6896551724137931,"For example, if the contribution is a novel architecture, describing the architecture fully
464"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6909323116219668,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
465"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6922094508301405,"be necessary to either make it possible for others to replicate the model with the same
466"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6934865900383141,"dataset, or provide access to the model. In general. releasing code and data is often
467"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6947637292464879,"one good way to accomplish this, but reproducibility can also be provided via detailed
468"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6960408684546615,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
469"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6973180076628352,"of a large language model), releasing of a model checkpoint, or other means that are
470"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.698595146871009,"appropriate to the research performed.
471"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6998722860791826,"• While NeurIPS does not require releasing code, the conference does require all submis-
472"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7011494252873564,"sions to provide some reasonable avenue for reproducibility, which may depend on the
473"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.70242656449553,"nature of the contribution. For example
474"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7037037037037037,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
475"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7049808429118773,"to reproduce that algorithm.
476"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7062579821200511,"(b) If the contribution is primarily a new model architecture, the paper should describe
477"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7075351213282248,"the architecture clearly and fully.
478"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7088122605363985,"(c) If the contribution is a new model (e.g., a large language model), then there should
479"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7100893997445722,"either be a way to access this model for reproducing the results or a way to reproduce
480"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7113665389527458,"the model (e.g., with an open-source dataset or instructions for how to construct
481"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7126436781609196,"the dataset).
482"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7139208173690932,"(d) We recognize that reproducibility may be tricky in some cases, in which case
483"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7151979565772669,"authors are welcome to describe the particular way they provide for reproducibility.
484"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7164750957854407,"In the case of closed-source models, it may be that access to the model is limited in
485"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7177522349936143,"some way (e.g., to registered users), but it should be possible for other researchers
486"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.719029374201788,"to have some path to reproducing or verifying the results.
487"
OPEN ACCESS TO DATA AND CODE,0.7203065134099617,"5. Open access to data and code
488"
OPEN ACCESS TO DATA AND CODE,0.7215836526181354,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
489"
OPEN ACCESS TO DATA AND CODE,0.722860791826309,"tions to faithfully reproduce the main experimental results, as described in supplemental
490"
OPEN ACCESS TO DATA AND CODE,0.7241379310344828,"material?
491"
OPEN ACCESS TO DATA AND CODE,0.7254150702426565,"Answer: [Yes]
492"
OPEN ACCESS TO DATA AND CODE,0.7266922094508301,"Justification: This paper contributes its codebase, which contains sufficient information in
493"
OPEN ACCESS TO DATA AND CODE,0.7279693486590039,"the ReadME for reproducibility. This paper’s ""Methods"" section discusses data generation
494"
OPEN ACCESS TO DATA AND CODE,0.7292464878671775,"for the simple function classes.
495"
OPEN ACCESS TO DATA AND CODE,0.7305236270753512,"Guidelines:
496"
OPEN ACCESS TO DATA AND CODE,0.7318007662835249,"• The answer NA means that paper does not include experiments requiring code.
497"
OPEN ACCESS TO DATA AND CODE,0.7330779054916986,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
498"
OPEN ACCESS TO DATA AND CODE,0.7343550446998723,"public/guides/CodeSubmissionPolicy) for more details.
499"
OPEN ACCESS TO DATA AND CODE,0.735632183908046,"• While we encourage the release of code and data, we understand that this might not be
500"
OPEN ACCESS TO DATA AND CODE,0.7369093231162197,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
501"
OPEN ACCESS TO DATA AND CODE,0.7381864623243933,"including code, unless this is central to the contribution (e.g., for a new open-source
502"
OPEN ACCESS TO DATA AND CODE,0.7394636015325671,"benchmark).
503"
OPEN ACCESS TO DATA AND CODE,0.7407407407407407,"• The instructions should contain the exact command and environment needed to run to
504"
OPEN ACCESS TO DATA AND CODE,0.7420178799489144,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
505"
OPEN ACCESS TO DATA AND CODE,0.7432950191570882,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
506"
OPEN ACCESS TO DATA AND CODE,0.7445721583652618,"• The authors should provide instructions on data access and preparation, including how
507"
OPEN ACCESS TO DATA AND CODE,0.7458492975734355,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
508"
OPEN ACCESS TO DATA AND CODE,0.7471264367816092,"• The authors should provide scripts to reproduce all experimental results for the new
509"
OPEN ACCESS TO DATA AND CODE,0.7484035759897829,"proposed method and baselines. If only a subset of experiments are reproducible, they
510"
OPEN ACCESS TO DATA AND CODE,0.7496807151979565,"should state which ones are omitted from the script and why.
511"
OPEN ACCESS TO DATA AND CODE,0.7509578544061303,"• At submission time, to preserve anonymity, the authors should release anonymized
512"
OPEN ACCESS TO DATA AND CODE,0.7522349936143039,"versions (if applicable).
513"
OPEN ACCESS TO DATA AND CODE,0.7535121328224776,"• Providing as much information as possible in supplemental material (appended to the
514"
OPEN ACCESS TO DATA AND CODE,0.7547892720306514,"paper) is recommended, but including URLs to data and code is permitted.
515"
OPEN ACCESS TO DATA AND CODE,0.756066411238825,"6. Experimental Setting/Details
516"
OPEN ACCESS TO DATA AND CODE,0.7573435504469987,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
517"
OPEN ACCESS TO DATA AND CODE,0.7586206896551724,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
518"
OPEN ACCESS TO DATA AND CODE,0.7598978288633461,"results?
519"
OPEN ACCESS TO DATA AND CODE,0.7611749680715197,"Answer: [Yes]
520"
OPEN ACCESS TO DATA AND CODE,0.7624521072796935,"Justification: These details were explained under the ""Methods"" section are sufficient to
521"
OPEN ACCESS TO DATA AND CODE,0.7637292464878672,"understand our results.
522"
OPEN ACCESS TO DATA AND CODE,0.7650063856960408,"Guidelines:
523"
OPEN ACCESS TO DATA AND CODE,0.7662835249042146,"• The answer NA means that the paper does not include experiments.
524"
OPEN ACCESS TO DATA AND CODE,0.7675606641123882,"• The experimental setting should be presented in the core of the paper to a level of detail
525"
OPEN ACCESS TO DATA AND CODE,0.768837803320562,"that is necessary to appreciate the results and make sense of them.
526"
OPEN ACCESS TO DATA AND CODE,0.7701149425287356,"• The full details can be provided either with the code, in appendix, or as supplemental
527"
OPEN ACCESS TO DATA AND CODE,0.7713920817369093,"material.
528"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7726692209450831,"7. Experiment Statistical Significance
529"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7739463601532567,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
530"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7752234993614304,"information about the statistical significance of the experiments?
531"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.776500638569604,"Answer: [Yes]
532"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7777777777777778,"Justification: Every figure and number presented in this paper has confidence of intervals of
533"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7790549169859514,"95% or 99% (specified in each case).
534"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7803320561941252,"Guidelines:
535"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7816091954022989,"• The answer NA means that the paper does not include experiments.
536"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7828863346104725,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
537"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7841634738186463,"dence intervals, or statistical significance tests, at least for the experiments that support
538"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7854406130268199,"the main claims of the paper.
539"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7867177522349936,"• The factors of variability that the error bars are capturing should be clearly stated (for
540"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7879948914431673,"example, train/test split, initialization, random drawing of some parameter, or overall
541"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.789272030651341,"run with given experimental conditions).
542"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7905491698595147,"• The method for calculating the error bars should be explained (closed form formula,
543"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7918263090676884,"call to a library function, bootstrap, etc.)
544"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7931034482758621,"• The assumptions made should be given (e.g., Normally distributed errors).
545"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7943805874840357,"• It should be clear whether the error bar is the standard deviation or the standard error
546"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7956577266922095,"of the mean.
547"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7969348659003831,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
548"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7982120051085568,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
549"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7994891443167306,"of Normality of errors is not verified.
550"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8007662835249042,"• For asymmetric distributions, the authors should be careful not to show in tables or
551"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8020434227330779,"figures symmetric error bars that would yield results that are out of range (e.g. negative
552"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8033205619412516,"error rates).
553"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8045977011494253,"• If error bars are reported in tables or plots, The authors should explain in the text how
554"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8058748403575989,"they were calculated and reference the corresponding figures or tables in the text.
555"
EXPERIMENTS COMPUTE RESOURCES,0.8071519795657727,"8. Experiments Compute Resources
556"
EXPERIMENTS COMPUTE RESOURCES,0.8084291187739464,"Question: For each experiment, does the paper provide sufficient information on the com-
557"
EXPERIMENTS COMPUTE RESOURCES,0.80970625798212,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
558"
EXPERIMENTS COMPUTE RESOURCES,0.8109833971902938,"the experiments?
559"
EXPERIMENTS COMPUTE RESOURCES,0.8122605363984674,"Answer: [Yes]
560"
EXPERIMENTS COMPUTE RESOURCES,0.8135376756066411,"Justification: The list of all GPU types utilized for the experiments were included, along
561"
EXPERIMENTS COMPUTE RESOURCES,0.8148148148148148,"with the time spent for compute on each of them. Furthermore, we provide a breakdown of
562"
EXPERIMENTS COMPUTE RESOURCES,0.8160919540229885,"the time spent for each experiment type on the GPU that we utilized the most (an A10).
563"
EXPERIMENTS COMPUTE RESOURCES,0.8173690932311622,"Guidelines:
564"
EXPERIMENTS COMPUTE RESOURCES,0.8186462324393359,"• The answer NA means that the paper does not include experiments.
565"
EXPERIMENTS COMPUTE RESOURCES,0.8199233716475096,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
566"
EXPERIMENTS COMPUTE RESOURCES,0.8212005108556832,"or cloud provider, including relevant memory and storage.
567"
EXPERIMENTS COMPUTE RESOURCES,0.822477650063857,"• The paper should provide the amount of compute required for each of the individual
568"
EXPERIMENTS COMPUTE RESOURCES,0.8237547892720306,"experimental runs as well as estimate the total compute.
569"
EXPERIMENTS COMPUTE RESOURCES,0.8250319284802043,"• The paper should disclose whether the full research project required more compute
570"
EXPERIMENTS COMPUTE RESOURCES,0.8263090676883781,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
571"
EXPERIMENTS COMPUTE RESOURCES,0.8275862068965517,"didn’t make it into the paper).
572"
CODE OF ETHICS,0.8288633461047255,"9. Code Of Ethics
573"
CODE OF ETHICS,0.8301404853128991,"Question: Does the research conducted in the paper conform, in every respect, with the
574"
CODE OF ETHICS,0.8314176245210728,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
575"
CODE OF ETHICS,0.8326947637292464,"Answer: [Yes]
576"
CODE OF ETHICS,0.8339719029374202,"Justification: We study ICL using simple function classes and do not use real world data.
577"
CODE OF ETHICS,0.8352490421455939,"No human subjects are involved and there are no direct paths for negative societal impact.
578"
CODE OF ETHICS,0.8365261813537676,"Guidelines:
579"
CODE OF ETHICS,0.8378033205619413,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
580"
CODE OF ETHICS,0.8390804597701149,"• If the authors answer No, they should explain the special circumstances that require a
581"
CODE OF ETHICS,0.8403575989782887,"deviation from the Code of Ethics.
582"
CODE OF ETHICS,0.8416347381864623,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
583"
CODE OF ETHICS,0.842911877394636,"eration due to laws or regulations in their jurisdiction).
584"
BROADER IMPACTS,0.8441890166028098,"10. Broader Impacts
585"
BROADER IMPACTS,0.8454661558109834,"Question: Does the paper discuss both potential positive societal impacts and negative
586"
BROADER IMPACTS,0.8467432950191571,"societal impacts of the work performed?
587"
BROADER IMPACTS,0.8480204342273308,"Answer: [NA]
588"
BROADER IMPACTS,0.8492975734355045,"Justification: Since this paper studies ICL in hybrid models using simple function classes,
589"
BROADER IMPACTS,0.8505747126436781,"there is no direct path to negative applications.
590"
BROADER IMPACTS,0.8518518518518519,"Guidelines:
591"
BROADER IMPACTS,0.8531289910600255,"• The answer NA means that there is no societal impact of the work performed.
592"
BROADER IMPACTS,0.8544061302681992,"• If the authors answer NA or No, they should explain why their work has no societal
593"
BROADER IMPACTS,0.855683269476373,"impact or why the paper does not address societal impact.
594"
BROADER IMPACTS,0.8569604086845466,"• Examples of negative societal impacts include potential malicious or unintended uses
595"
BROADER IMPACTS,0.8582375478927203,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
596"
BROADER IMPACTS,0.859514687100894,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
597"
BROADER IMPACTS,0.8607918263090677,"groups), privacy considerations, and security considerations.
598"
BROADER IMPACTS,0.8620689655172413,"• The conference expects that many papers will be foundational research and not tied
599"
BROADER IMPACTS,0.8633461047254151,"to particular applications, let alone deployments. However, if there is a direct path to
600"
BROADER IMPACTS,0.8646232439335888,"any negative applications, the authors should point it out. For example, it is legitimate
601"
BROADER IMPACTS,0.8659003831417624,"to point out that an improvement in the quality of generative models could be used to
602"
BROADER IMPACTS,0.8671775223499362,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
603"
BROADER IMPACTS,0.8684546615581098,"that a generic algorithm for optimizing neural networks could enable people to train
604"
BROADER IMPACTS,0.8697318007662835,"models that generate Deepfakes faster.
605"
BROADER IMPACTS,0.8710089399744572,"• The authors should consider possible harms that could arise when the technology is
606"
BROADER IMPACTS,0.8722860791826309,"being used as intended and functioning correctly, harms that could arise when the
607"
BROADER IMPACTS,0.8735632183908046,"technology is being used as intended but gives incorrect results, and harms following
608"
BROADER IMPACTS,0.8748403575989783,"from (intentional or unintentional) misuse of the technology.
609"
BROADER IMPACTS,0.876117496807152,"• If there are negative societal impacts, the authors could also discuss possible mitigation
610"
BROADER IMPACTS,0.8773946360153256,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
611"
BROADER IMPACTS,0.8786717752234994,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
612"
BROADER IMPACTS,0.879948914431673,"feedback over time, improving the efficiency and accessibility of ML).
613"
SAFEGUARDS,0.8812260536398467,"11. Safeguards
614"
SAFEGUARDS,0.8825031928480205,"Question: Does the paper describe safeguards that have been put in place for responsible
615"
SAFEGUARDS,0.8837803320561941,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
616"
SAFEGUARDS,0.8850574712643678,"image generators, or scraped datasets)?
617"
SAFEGUARDS,0.8863346104725415,"Answer: [NA]
618"
SAFEGUARDS,0.8876117496807152,"Justification: As this paper discusses ICL on simple function classes, it does not utilize
619"
SAFEGUARDS,0.8888888888888888,"real-world data or models real-world capabilities. Thus, there is no risk for misuse.
620"
SAFEGUARDS,0.8901660280970626,"Guidelines:
621"
SAFEGUARDS,0.8914431673052363,"• The answer NA means that the paper poses no such risks.
622"
SAFEGUARDS,0.89272030651341,"• Released models that have a high risk for misuse or dual-use should be released with
623"
SAFEGUARDS,0.8939974457215837,"necessary safeguards to allow for controlled use of the model, for example by requiring
624"
SAFEGUARDS,0.8952745849297573,"that users adhere to usage guidelines or restrictions to access the model or implementing
625"
SAFEGUARDS,0.896551724137931,"safety filters.
626"
SAFEGUARDS,0.8978288633461047,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
627"
SAFEGUARDS,0.8991060025542784,"should describe how they avoided releasing unsafe images.
628"
SAFEGUARDS,0.9003831417624522,"• We recognize that providing effective safeguards is challenging, and many papers do
629"
SAFEGUARDS,0.9016602809706258,"not require this, but we encourage authors to take this into account and make a best
630"
SAFEGUARDS,0.9029374201787995,"faith effort.
631"
LICENSES FOR EXISTING ASSETS,0.9042145593869731,"12. Licenses for existing assets
632"
LICENSES FOR EXISTING ASSETS,0.9054916985951469,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
633"
LICENSES FOR EXISTING ASSETS,0.9067688378033205,"the paper, properly credited and are the license and terms of use explicitly mentioned and
634"
LICENSES FOR EXISTING ASSETS,0.9080459770114943,"properly respected?
635"
LICENSES FOR EXISTING ASSETS,0.909323116219668,"Answer: [Yes]
636"
LICENSES FOR EXISTING ASSETS,0.9106002554278416,"Justification: We provide credit to the authors of the three codebases that inspired some of
637"
LICENSES FOR EXISTING ASSETS,0.9118773946360154,"the features in our own and cite that their codebases fall under the MIT License for the first
638"
LICENSES FOR EXISTING ASSETS,0.913154533844189,"two, and could not be found for the third. URLs are provided for each codebase as well.
639"
LICENSES FOR EXISTING ASSETS,0.9144316730523627,"Guidelines:
640"
LICENSES FOR EXISTING ASSETS,0.9157088122605364,"• The answer NA means that the paper does not use existing assets.
641"
LICENSES FOR EXISTING ASSETS,0.9169859514687101,"• The authors should cite the original paper that produced the code package or dataset.
642"
LICENSES FOR EXISTING ASSETS,0.9182630906768838,"• The authors should state which version of the asset is used and, if possible, include a
643"
LICENSES FOR EXISTING ASSETS,0.9195402298850575,"URL.
644"
LICENSES FOR EXISTING ASSETS,0.9208173690932312,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
645"
LICENSES FOR EXISTING ASSETS,0.9220945083014048,"• For scraped data from a particular source (e.g., website), the copyright and terms of
646"
LICENSES FOR EXISTING ASSETS,0.9233716475095786,"service of that source should be provided.
647"
LICENSES FOR EXISTING ASSETS,0.9246487867177522,"• If assets are released, the license, copyright information, and terms of use in the
648"
LICENSES FOR EXISTING ASSETS,0.9259259259259259,"package should be provided. For popular datasets, paperswithcode.com/datasets
649"
LICENSES FOR EXISTING ASSETS,0.9272030651340997,"has curated licenses for some datasets. Their licensing guide can help determine the
650"
LICENSES FOR EXISTING ASSETS,0.9284802043422733,"license of a dataset.
651"
LICENSES FOR EXISTING ASSETS,0.929757343550447,"• For existing datasets that are re-packaged, both the original license and the license of
652"
LICENSES FOR EXISTING ASSETS,0.9310344827586207,"the derived asset (if it has changed) should be provided.
653"
LICENSES FOR EXISTING ASSETS,0.9323116219667944,"• If this information is not available online, the authors are encouraged to reach out to
654"
LICENSES FOR EXISTING ASSETS,0.933588761174968,"the asset’s creators.
655"
NEW ASSETS,0.9348659003831418,"13. New Assets
656"
NEW ASSETS,0.9361430395913155,"Question: Are new assets introduced in the paper well documented and is the documentation
657"
NEW ASSETS,0.9374201787994891,"provided alongside the assets?
658"
NEW ASSETS,0.9386973180076629,"Answer: [Yes]
659"
NEW ASSETS,0.9399744572158365,"Justification: Our codebase contains a ReadME providing thorough documentation. Our
660"
NEW ASSETS,0.9412515964240102,"paper also explains high-level functionality of our codebase.
661"
NEW ASSETS,0.9425287356321839,"Guidelines:
662"
NEW ASSETS,0.9438058748403576,"• The answer NA means that the paper does not release new assets.
663"
NEW ASSETS,0.9450830140485313,"• Researchers should communicate the details of the dataset/code/model as part of their
664"
NEW ASSETS,0.946360153256705,"submissions via structured templates. This includes details about training, license,
665"
NEW ASSETS,0.9476372924648787,"limitations, etc.
666"
NEW ASSETS,0.9489144316730523,"• The paper should discuss whether and how consent was obtained from people whose
667"
NEW ASSETS,0.9501915708812261,"asset is used.
668"
NEW ASSETS,0.9514687100893997,"• At submission time, remember to anonymize your assets (if applicable). You can either
669"
NEW ASSETS,0.9527458492975734,"create an anonymized URL or include an anonymized zip file.
670"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9540229885057471,"14. Crowdsourcing and Research with Human Subjects
671"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9553001277139208,"Question: For crowdsourcing experiments and research with human subjects, does the paper
672"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9565772669220945,"include the full text of instructions given to participants and screenshots, if applicable, as
673"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9578544061302682,"well as details about compensation (if any)?
674"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9591315453384419,"Answer: [NA]
675"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9604086845466155,"Justification: There are no crowdsourcing experiments or research with human subjects, and
676"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9616858237547893,"thus no text of instructions or compensation information is included.
677"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9629629629629629,"Guidelines:
678"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9642401021711366,"• The answer NA means that the paper does not involve crowdsourcing nor research with
679"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9655172413793104,"human subjects.
680"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.966794380587484,"• Including this information in the supplemental material is fine, but if the main contribu-
681"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9680715197956578,"tion of the paper involves human subjects, then as much detail as possible should be
682"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693486590038314,"included in the main paper.
683"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9706257982120051,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
684"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9719029374201787,"or other labor should be paid at least the minimum wage in the country of the data
685"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9731800766283525,"collector.
686"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744572158365262,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
687"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757343550446999,"Subjects
688"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9770114942528736,"Question: Does the paper describe potential risks incurred by study participants, whether
689"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9782886334610472,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
690"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979565772669221,"approvals (or an equivalent approval/review based on the requirements of your country or
691"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808429118773946,"institution) were obtained?
692"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821200510855683,"Answer: [NA]
693"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833971902937421,"Justification: As there were no study participants in this study, this information was not
694"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846743295019157,"included in this paper.
695"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9859514687100894,"Guidelines:
696"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872286079182631,"• The answer NA means that the paper does not involve crowdsourcing nor research with
697"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885057471264368,"human subjects.
698"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897828863346104,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
699"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910600255427842,"may be required for any human subjects research. If you obtained IRB approval, you
700"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923371647509579,"should clearly state this in the paper.
701"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936143039591315,"• We recognize that the procedures for this may vary significantly between institutions
702"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948914431673053,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
703"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961685823754789,"guidelines for their institution.
704"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974457215836526,"• For initial submissions, do not include any information that would break anonymity (if
705"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9987228607918263,"applicable), such as the institution conducting the review.
706"
