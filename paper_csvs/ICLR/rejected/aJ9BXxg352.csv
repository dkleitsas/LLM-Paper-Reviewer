Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0012330456226880395,"Randomized smoothing is currently considered the state-of-the-art method to ob-
tain certiï¬ably robust classiï¬ers. Despite its remarkable performance, the method
is associated with various serious problems such as â€œcertiï¬ed accuracy water-
fallsâ€, certiï¬cation vs. accuracy trade-off, or even fairness issues. Input-dependent
smoothing approaches have been proposed with intention of overcoming these
ï¬‚aws. However, we demonstrate that these methods lack formal guarantees and
so the resulting certiï¬cates are not justiï¬ed. We show that the input-dependent
smoothing, in general, suffers from the curse of dimensionality, forcing the vari-
ance function to have low semi-elasticity. On the other hand, we provide a theoret-
ical and practical framework that enables the usage of input-dependent smoothing
even in the presence of the curse of dimensionality, under strict restrictions. We
present one concrete design of the smoothing variance and test it on CIFAR10 and
MNIST. Our design solves some of the problems of classical smoothing and is
formally underlined, yet further improvement of the design is still necessary."
INTRODUCTION,0.002466091245376079,"1
INTRODUCTION"
INTRODUCTION,0.0036991368680641184,"Deep neural networks are one of the dominating recently used machine learning methods. They
achieve state-of-the-art performance in a variety of applications like computer vision, natural lan-
guage processing, and many others. The key property that makes neural networks so powerful is
their expressivity (GÂ¨uhring et al., 2020). However, as a prize, they possess a weakness - a vulnera-
bility against adversarial attacks (Szegedy et al., 2013; Biggio et al., 2013). The adversarial attack
on a sample x is a point xâ€² such that the distance d(xâ€², x) is small, yet the model fâ€™s predictions on
x and xâ€² differ. Such examples are often easy to construct, for example by optimizing for a change
in prediction f(x) (Biggio et al., 2013). Even worse, these attacks are present even if the modelâ€™s
prediction on x is unequivocal."
INTRODUCTION,0.004932182490752158,"This property is highly undesirable because in several sensitive applications, misclassifying a sam-
ple just because it does not follow the natural distribution might lead to serious and harmful conse-
quences. A well-known example would be a sticker placed on a trafï¬c sign, which could possibly
confuse the self-driving car and cause an accident (Eykholt et al., 2018). To prevent this behaviour,
the robustness of classiï¬ers against adversarial examples has begun to be a strongly discussed topic.
Though many methods claim to provide robust classiï¬ers, just some of them are certiï¬ably robust,
i.e. the robustness is mathematically guaranteed. The certiï¬ability turns out to be essential since
more sophisticated attacks can break empirical defenses (Carlini & Wagner, 2017)."
INTRODUCTION,0.006165228113440197,"Currently, the dominating method to achieve the certiï¬able robustness is randomized smoothing
(RS). This clever idea to get rid of adversarial examples using randomization of input was introduced
by Lecuyer et al. (2019) and Li et al. (2019) and fully formalized and improved in Cohen et al.
(2019). Let f be a classiï¬er classifying inputs x âˆˆRN as one of the classes C âˆˆC. Assume
now a random deviation Ïµ âˆ¼N(0, Ïƒ2I). The smoothed classiï¬er g, made of f, is deï¬ned as:
g(x) = arg maxC P(f(x + Ïµ) = C), for C âˆˆC. In other words, the smoothed classiï¬er classiï¬es a
class that has the highest probability under the sampling of f(x + Ïµ). Consequently, an adversarial
attack xâ€² on f is less dangerous for g, because g does not look directly at xâ€², but rather at its whole
neighborhood, in a weighted manner. This way, we can get rid of local artifacts that f possesses â€“
thus the name â€œsmoothingâ€. It turns out, that g enjoys strong robustness properties against attacks"
INTRODUCTION,0.007398273736128237,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.008631319358816275,"bounded by a speciï¬cally computed l2-norm threshold, especially if f is trained under a Gaussian
noise augmentation (Cohen et al., 2019)."
INTRODUCTION,0.009864364981504316,"Unfortunately, since the introduction of the RS, several serious problems were reported to be con-
nected to the technique. Cohen et al. (2019) mention two of them. First is the usage of lower
conï¬dence bounds to estimate the leading classâ€™s probability. With a high probability, this leads to
smaller reported certiï¬ed radiuses in comparison with the true ones. Moreover, it yields a theoret-
ical threshold, which upper-bounds the maximal possible certiï¬ed radius and causes the â€œcertiï¬ed
accuracy waterfallsâ€, signiï¬cantly decreasing the certiï¬ed accuracy. This problem is particularly
pronounced for small levels of the used smoothing variance Ïƒ2, which motivates to use larger vari-
ance. Second, RS possesses a robustness vs. accuracy trade-off problem. The bigger Ïƒ we use as the
smoothing variance, the smaller clean accuracy will the smoothed classiï¬er have. This motivates to
use rather smaller levels of Ïƒ. Third, as pointed out by Mohapatra et al. (2020a), RS smoothens the
decision boundary of f in such a way, that bounded or convex regions begin to shrink as Ïƒ increases,
while the unbounded and anti-convex regions expand. This, as the authors empirically demonstrate,
creates a disbalance in class-wise accuracies of g and causes serious fairness issues. Therefore,
again, the smaller values of Ïƒ are more preferable. See Appendix A for a detailed discussion."
INTRODUCTION,0.011097410604192354,"Clearly, the usage of a global, constant Ïƒ is suboptimal. For the samples close to the decision bound-
ary, we want to use small Ïƒ, so that f and g have similar decision boundaries and the expressivity
of f is not lost (where not necessary). On the other hand, far from the decision boundary of f,
where the probability of the dominant class is close to 1, we need bigger Ïƒ to avoid the severe
under-certiï¬cation (see Appendix A). Together, using a non-constant Ïƒ(x) rather than constant Ïƒ, a
suitable smoothing variance could be used to achieve optimal robustness. Yet there are some works
introducing this concept (see Appendix C), most of them lack mathematical reasoning about the
correctness of their method, which, as we show, turns out to be critical."
INTRODUCTION,0.012330456226880395,"To support this argumentation, we present a toy example. We train a network on a 2D dataset of a
circular shape with the classes being two complementary sectors, where one is of a very small angle.
In Figure 1 we show the difference between constant and input-dependent Ïƒ. Using non-constant
Ïƒ(x) deï¬ned in Equation 1, we obtain an improvement both in terms of the certiï¬ed radiuses as well
as clean accuracy. For more details see Appendix A."
INTRODUCTION,0.013563501849568433,"Figure 1: Motivating toy experiment. The constant Ïƒ = 0.6 and the input-dependent Ïƒ(x) equal
in average to the constant Ïƒ are used. Left: Dataset and the variance function depicted as circles
with the radius equal to Ïƒ(x) and centers at the data points. Middle: Zoomed in part of the dataset
and decision boundaries of the smoothed classiï¬ers with constant Ïƒ (red) and input-dependent Ïƒ(x)
(green). Note that we recover a part of the misclassiï¬ed data points by using a more appropriate
smoothing strength close to the decision boundary. Right: Certiï¬ed accuracy plot. The waterfall
effect vanishes since the points far from the decision boundary are certiï¬ed with a correspondingly
large Ïƒ(x)."
INTRODUCTION,0.014796547472256474,"The main contributions of this work are fourfold. First, we generalize the methodology of Cohen
et al. (2019)â€™s work for the case of input-dependent RS (IDRS), obtaining useful and important
insights about how to use the Neyman-Pearson lemma in this, general, case. Second and most
importantly, we show that the IDRS suffers from the curse of dimensionality in the sense that the
semi-elasticity coefï¬cient r of Ïƒ(x) (that is | log(Ïƒ(x0))âˆ’log(Ïƒ(x1))| â‰¤râˆ¥x0âˆ’x1âˆ¥âˆ€x0, x1 âˆˆRN)
in a high-dimensional setting is restricted to be very small. This means, that even if we wanted to
vary the Ïƒ(x) signiï¬cantly with varying x, we canâ€™t. The maximal reasonable speed of change of
Ïƒ(x) turns out to be almost too small to handle, especially in high dimensions. Third, in contrast,"
INTRODUCTION,0.016029593094944512,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01726263871763255,"we also study the conditions on Ïƒ(x) under which it is applicable in high-dimensional regime and
prepare a theoretical framework necessary to build an efï¬cient certiï¬cation algorithm. We are the
ï¬rst to do so for Ïƒ(x) functions, which are not locally constant (as in Wang et al. (2021)). Finally, we
provide a concrete design of the Ïƒ(x) function and test it extensively and compare it to the classical
RS on the CIFAR10 and MNIST datasets. We discuss to what extent the method treats the issues
mentioned above."
INPUT-DEPENDENT RS AND THE CURSE OF DIMENSIONALITY,0.018495684340320593,"2
INPUT-DEPENDENT RS AND THE CURSE OF DIMENSIONALITY"
INPUT-DEPENDENT RS AND THE CURSE OF DIMENSIONALITY,0.01972872996300863,"Let C be the set of classes, f : RN âˆ’â†’C a classiï¬er (reffered to as the base classiï¬er), Ïƒ : RN âˆ’â†’R
a non-negative function and P(C) a set of distributions over C. Then we call Gf : RN âˆ’â†’P(C) the
smoothed class probability predictor, if Gf(x)C = P(f(x + Ïµ) = C), where Ïµ âˆ¼N(0, Ïƒ(x)2I)
and gf : RN âˆ’â†’C is called smoothed classiï¬er if gf(x) = arg maxC Gf(x)C, for C âˆˆC. We will
omit the subscript f in gf often, since it is usually clear from the context, to which base classiï¬er
the g corresponds. Furthermore, let A := g(x) refer to the most likely class under the random
variable f(N(x, Ïƒ2I)), while B denote the second most likely class. Deï¬ne pA = Gf(x)A and
pB = Gf(x)B as the respective probabilities. It is important to note that in practice, it is impossible
to estimate pA and pB precisely. Instead, pA is estimated as a lower conï¬dence bound (LCB) of the
relative occurence of class A in fâ€™s predictions given certain number of Monte-Carlo samples n and
a conï¬dence level Î± and the estimate is denoted as pA. We use the exact Clopper-Pearson interval
for estimation of the LCB. Similarly for pB. We work with l2-norms denoted as âˆ¥xâˆ¥."
INPUT-DEPENDENT RS AND THE CURSE OF DIMENSIONALITY,0.02096177558569667,"First of all, we summarize the main steps in the derivation of certiï¬ed radius using any method that
relies on a use of Neyman-Pearson lemma."
"FOR
A
POTENTIAL
ADVERSARY",0.02219482120838471,"1. For
a
potential
adversary
xâ€²
specify
the
worst-case
classiï¬er
f âˆ—,
such
that
P(f âˆ—(N(x, Ïƒ2I)) = A) = pA, while P(f âˆ—(N(xâ€², Ïƒ2I)) = B) is maximized.
2. Express the probability Gf âˆ—(xâ€²)B as a function depending on xâ€².
3. Determine the conditions on xâ€² (possibly related to âˆ¥x âˆ’xâ€²âˆ¥) for which this probability is
â‰¤0.5. From these condtions, derive the certiï¬ed radius."
"FOR
A
POTENTIAL
ADVERSARY",0.02342786683107275,"Cohen et al. (2019) proceeded in this way to obtain a tight certiï¬ed radius R =
Ïƒ
2 (Î¦âˆ’1(pA) âˆ’
Î¦âˆ’1(pB)). Unfortunately, their result is not directly applicable to the input-dependent case. The
constant Ïƒ simpliï¬es the derivation of f âˆ—that turns out to be a linear classiï¬er. This is not the case
for non-constant Ïƒ(x) anymore. Therefore, we need to generalize the methodology of Cohen et al.
(2019). We put pB = 1âˆ’pA for simplicity (yet it is not necessary to assume this, see Appendix B.5).
Let x0 be the point to certify, x1 the potential adversary point, Î´ = x1âˆ’x0 the noise and Ïƒ0 = Ïƒ(x0),
Ïƒ1 = Ïƒ(x1) the standard deviations used in x0 and x1 respectively. Furthermore, let fi be a density
and Pi a probability measure corresponding to N(xi, Ïƒ2
i I), i âˆˆ{0, 1}.
Lemma 1. Out of all possible classiï¬ers f such that Gf(x)B â‰¤pB = 1 âˆ’pA, the one, for which
Gf(x + Î´)B is maximized is the one, which predicts class B in a region determined by the likelihood
ratio:"
"FOR
A
POTENTIAL
ADVERSARY",0.02466091245376079,"B =

x âˆˆRN : f1(x)"
"FOR
A
POTENTIAL
ADVERSARY",0.025893958076448828,"f0(x) â‰¥1 r 
,"
"FOR
A
POTENTIAL
ADVERSARY",0.027127003699136867,"where r is ï¬xed, such that P0(B) = pB. Note, that we use B to denote both the class and the region
of that class."
"FOR
A
POTENTIAL
ADVERSARY",0.02836004932182491,"We use this lemma to compute the decision boundary of the worst-case classiï¬er f âˆ—.
Theorem 2. If Ïƒ0 > Ïƒ1, then B is a N-dimensional ball with the center at S> and radius R>:"
"FOR
A
POTENTIAL
ADVERSARY",0.029593094944512947,"S> = x +
Ïƒ2
0
Ïƒ2
0 âˆ’Ïƒ2
1
Î´, R> = s"
"FOR
A
POTENTIAL
ADVERSARY",0.030826140567200986,"Ïƒ2
0Ïƒ2
1
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2 + 2N
Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log
Ïƒ0 Ïƒ1"
"FOR
A
POTENTIAL
ADVERSARY",0.032059186189889025,"
+ 2Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log(r)."
"FOR
A
POTENTIAL
ADVERSARY",0.03329223181257707,"If Ïƒ0 < Ïƒ1, then B is the complement of a N-dimensional ball with the center at S< and radius R<:"
"FOR
A
POTENTIAL
ADVERSARY",0.0345252774352651,"S< = x âˆ’
Ïƒ2
0
Ïƒ2
1 âˆ’Ïƒ2
0
Î´, R< = s"
"FOR
A
POTENTIAL
ADVERSARY",0.035758323057953144,"2Ïƒ4
0 âˆ’Ïƒ2
0Ïƒ2
1
(Ïƒ2
1 âˆ’Ïƒ2
0)2 âˆ¥Î´âˆ¥2 + 2N
Ïƒ2
0Ïƒ2
1
Ïƒ2
1 âˆ’Ïƒ2
0
log
Ïƒ1 Ïƒ0"
"FOR
A
POTENTIAL
ADVERSARY",0.036991368680641186,"
âˆ’2Ïƒ2
0Ïƒ2
1
Ïƒ2
1 âˆ’Ïƒ2
0
log(r)."
"FOR
A
POTENTIAL
ADVERSARY",0.03822441430332922,Under review as a conference paper at ICLR 2022
"FOR
A
POTENTIAL
ADVERSARY",0.03945745992601726,Figure 2: Decision regions of the worst-case classiï¬er f âˆ—. Left: Ïƒ0 > Ïƒ1 Right: Ïƒ0 < Ïƒ1
"FOR
A
POTENTIAL
ADVERSARY",0.040690505548705305,"As we depict on Figure 2, both resulting balls are centered on the line connecting x0, x1. Moreover,
the centers of the balls are always further from x0, than x1 is from x0 (even in the case Ïƒ0 < Ïƒ1).
In both cases, it depends on pA (since r is ï¬xed such that P0(B) = pB) and the ball can, but might
not cover x0 and/or x1. Note that if Ïƒ0 = Ïƒ1, what can happen even in input-dependent regime, the
worst-case classiï¬er is the half-space described by Cohen et al. (2019)."
"FOR
A
POTENTIAL
ADVERSARY",0.04192355117139334,"To compute the probability of a ball under a probability measure with an isotropic Gaussian density
is far more challenging than to compute the probability of a half-space. In fact, there is no closed-
form formula for such probability. However, this probability is connected to the non-central chi-
square distribution (NCCHSQ). More precisely, the probability of an N-dimensional ball centered
at z with radius r under N(0, I) can be expressed as a cumulative distribution fucntion (cdf) of
NCCHSQ with N degrees of freedom, non-centrality parameter âˆ¥zâˆ¥2 and argument r2. With this
knowledge, we can express P0(B) and P1(B) in terms of the cdf of NCCHSQ as follows.
Theorem 3."
"FOR
A
POTENTIAL
ADVERSARY",0.04315659679408138,"P0(B) = Ï‡2
N"
"FOR
A
POTENTIAL
ADVERSARY",0.04438964241676942,"Ïƒ2
0
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2, R2
<,>
Ïƒ2
0 !"
"FOR
A
POTENTIAL
ADVERSARY",0.04562268803945746,", P1(B) = Ï‡2
N"
"FOR
A
POTENTIAL
ADVERSARY",0.0468557336621455,"Ïƒ2
1
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2, R2
<,>
Ïƒ2
1 ! ,"
"FOR
A
POTENTIAL
ADVERSARY",0.04808877928483354,where the sign < or > is chosen according to the inequality between Ïƒ0 and Ïƒ1.
"FOR
A
POTENTIAL
ADVERSARY",0.04932182490752158,"Note, that both Theorem 2 and Theorem 3 work well also for Î´ = 0. In this case, we encounter a
ball centered at x0 = x1 and all the cdf functions become cdf functions of central chi-squared."
"FOR
A
POTENTIAL
ADVERSARY",0.05055487053020962,"We expressed probabilities of the worst-case class Bâ€™s decision region using the cdf of NCCHCSQ.
Now, how do we do the certiï¬cation? We start with the certiï¬cation just for two points, x0 and x1.
We question, under which circumstances can x1 be certiï¬ed from the point of view of x0. Having x0,
pA and Ïƒ0 > Ïƒ1, we can obtain such R, that P0(B) = Ï‡2
N
 
âˆ¥Î´âˆ¥2Ïƒ2
0/(Ïƒ2
0 âˆ’Ïƒ2
1)2, R2
= 1 âˆ’pA =
pB simply by putting it into the quantile function: R2 = Ï‡2
N,qf
 
âˆ¥Î´âˆ¥2Ïƒ2
0/(Ïƒ2
0 âˆ’Ïƒ2
1)2, 1 âˆ’pA

. Then,
we can substitute into P1(B) = Ï‡2
N
 
âˆ¥Î´âˆ¥2Ïƒ2
1/(Ïƒ2
0 âˆ’Ïƒ2
1)2, R2Ïƒ2
0/Ïƒ2
1

. This way, we obtain P1(B)
and can judge, whether P1(B) < 1/2 or not. Similar computation can be done if Ïƒ0 < Ïƒ1. Denote
a := âˆ¥Î´âˆ¥. We can express the P1(B) more simply for Ïƒ0 > Ïƒ1 as"
"FOR
A
POTENTIAL
ADVERSARY",0.051787916152897656,"Î¾>(a) := P1(B) = Ï‡2
N"
"FOR
A
POTENTIAL
ADVERSARY",0.0530209617755857,"
Ïƒ2
1
(Ïƒ2
0 âˆ’Ïƒ2
1)2 a2, Ïƒ2
0
Ïƒ2
1
Ï‡2
N,qf"
"FOR
A
POTENTIAL
ADVERSARY",0.05425400739827373,"
Ïƒ2
0
(Ïƒ2
0 âˆ’Ïƒ2
1)2 a2, 1 âˆ’pA "
"FOR
A
POTENTIAL
ADVERSARY",0.055487053020961775,and for Ïƒ0 < Ïƒ1 as
"FOR
A
POTENTIAL
ADVERSARY",0.05672009864364982,"Î¾<(a) := P1(B) = 1 âˆ’Ï‡2
N"
"FOR
A
POTENTIAL
ADVERSARY",0.05795314426633785,"
Ïƒ2
1
(Ïƒ2
1 âˆ’Ïƒ2
0)2 a2, Ïƒ2
0
Ïƒ2
1
Ï‡2
N,qf"
"FOR
A
POTENTIAL
ADVERSARY",0.059186189889025895,"
Ïƒ2
0
(Ïƒ2
1 âˆ’Ïƒ2
0)2 a2, pA 
."
"FOR
A
POTENTIAL
ADVERSARY",0.06041923551171394,"With this in mind, if we have x0, x1, pA, Ïƒ0, Ïƒ1, then we can certify x1 w.r.t x0 simply by choosing
the correct sign (<, >) and computing Î¾<(âˆ¥x0 âˆ’x1âˆ¥) or Î¾>(âˆ¥x0 âˆ’x1âˆ¥), comparing them with 0.5.
The sample plots of these Î¾ functions can be found in Appendix B."
"FOR
A
POTENTIAL
ADVERSARY",0.06165228113440197,"Now, we are ready to discuss the curse of dimensionality. The problem that arises is that having a
high dimension N and Ïƒ0, Ïƒ1 differing a lot from each other, Î¾ functions are already big at 0, even"
"FOR
A
POTENTIAL
ADVERSARY",0.06288532675709001,Under review as a conference paper at ICLR 2022
"FOR
A
POTENTIAL
ADVERSARY",0.06411837237977805,"Figure 3: Plots depicting tightness of results of Theorem 4. On both ï¬gures, the biggest possible
threshold of Ïƒ1/Ïƒ0 for which the condition in Theorem 4 is satisï¬ed (theoretical threshold) and the
numerically computed threshold for which Î¾>(0) passes the threshold 0.5 (practical threshold) are
depicted. Left: Plot for pA = 0.9, Right: Plot for pA = 0.999."
"FOR
A
POTENTIAL
ADVERSARY",0.06535141800246609,"for considerably small pB. For ï¬xed ratio Ïƒ0/Ïƒ1 and probability pB, with increasing dimension, the
Î¾(0) increases and soon becomes bigger than 0.5. This, together with monotonicity of Î¾ function
yields that any x1 cannot be certiï¬ed w.r.t. x0, if Ïƒ0, Ïƒ1 are used. The more dissimilar the Ïƒ0 and Ïƒ1
are, the smaller the dimension N needs to be for this situation to occur. If we want to certify x1 in
a reasonable distance from x0, we need to use similar Ïƒ0, Ïƒ1. This restricts the variability of Ïƒ(x)
function. We will formalize the curse of dimensionality in the following theorems. More on why
the curse of dimensionality is present is in Appendix B.2.
Theorem 4 (curse of dimensionality). Let x0, x1, pA, Ïƒ0, Ïƒ1, N be as usual. Then, the following
two implications hold:"
"FOR
A
POTENTIAL
ADVERSARY",0.06658446362515413,1. If Ïƒ0 > Ïƒ1 and
"FOR
A
POTENTIAL
ADVERSARY",0.06781750924784218,"log
Ïƒ2
1
Ïƒ2
0"
"FOR
A
POTENTIAL
ADVERSARY",0.0690505548705302,"
+ 1 âˆ’Ïƒ2
1
Ïƒ2
0
< 2 log(1 âˆ’pA) N
,"
"FOR
A
POTENTIAL
ADVERSARY",0.07028360049321825,then x1 is not certiï¬ed w.r.t. x0.
"FOR
A
POTENTIAL
ADVERSARY",0.07151664611590629,2. If Ïƒ0 < Ïƒ1 and
"FOR
A
POTENTIAL
ADVERSARY",0.07274969173859433,"log
Ïƒ2
1
Ïƒ2
0 N âˆ’1 N"
"FOR
A
POTENTIAL
ADVERSARY",0.07398273736128237,"
+ 1 âˆ’Ïƒ2
1
Ïƒ2
0 N âˆ’1"
"FOR
A
POTENTIAL
ADVERSARY",0.0752157829839704,"N
< 2 log(1 âˆ’pA) N
,"
"FOR
A
POTENTIAL
ADVERSARY",0.07644882860665844,"then x1 is not certiï¬ed w.r.t. x0.
Corollary 5 (one-sided simpler bound). Let x0, x1, pA, Ïƒ0, Ïƒ1, N be as usual and assume now Ïƒ0 >
Ïƒ1. Then, if"
"FOR
A
POTENTIAL
ADVERSARY",0.07768187422934648,"Ïƒ1
Ïƒ0
< s 1 âˆ’2 r"
"FOR
A
POTENTIAL
ADVERSARY",0.07891491985203453,"âˆ’log(1 âˆ’pA) N
,"
"FOR
A
POTENTIAL
ADVERSARY",0.08014796547472257,then x1 is not certiï¬ed w.r.t x0.
"FOR
A
POTENTIAL
ADVERSARY",0.08138101109741061,"Note, that both Theorem 4 and Corrolary 5 can be adjusted to the case where we have a separate
estimate pB and do not put pB = 1 âˆ’pA (see Appendix B.5). We must emphasize, that the bounds
obtained in Theorem 4 are very tight. In other words, if the ratio Ïƒ1"
"FOR
A
POTENTIAL
ADVERSARY",0.08261405672009864,"Ïƒ0 is just slightly bigger than the
minimal possible threshold determined in Theorem 4, Î¾>(0) becomes smaller than 0.5 and similarly
for Î¾>(0). The reason for this is, that the only two estimates used in the proof of Theorem 4 are
the estimates on the median, which are very tight and constant with respect to N and the Chernoff
bound, which is generally considered to be tight too and improves for larger N. The tightness is
depicted on Figure 3, where we plot the minimal possible threshold Ïƒ1/Ïƒ0 given by Theorem 4 and
minimal threshold for which Î¾>(0) < 0.5 as a function of N."
"FOR
A
POTENTIAL
ADVERSARY",0.08384710234278668,"To get a better feeling about the concrete numbers, we provide a simple table, which shows the
theoretical threshold values provided by Theorem 4. If Ïƒ1/Ïƒ0 is smaller than the threshold, we are
not able to certify any pair of x0, x1 using Ïƒ0, Ïƒ1."
"FOR
A
POTENTIAL
ADVERSARY",0.08508014796547472,Under review as a conference paper at ICLR 2022
"FOR
A
POTENTIAL
ADVERSARY",0.08631319358816276,"pA
0.9
0.99
0.999
0.99993
MNIST
0.946
0.924
0.908
0.892
CIFAR10
0.973
0.961
0.953
0.945
ImageNet
0.997
0.995
0.994
0.993"
"FOR
A
POTENTIAL
ADVERSARY",0.08754623921085081,"Table 1: Theoretical lower-thresholds for Ïƒ1/Ïƒ0 for different data dimensions and class A probabil-
ities. The ImageNet spatial size is assumed to be 3x256x256."
"FOR
A
POTENTIAL
ADVERSARY",0.08877928483353884,"Results from Table 1 are very restrictive. Assume we have a CIFAR10 sample with pA = 0.999.
For such a probability, constant Ïƒ = 0.5 is more than sufï¬cient to guarantee the certiï¬ed radius of
more than 1. However, in the non-constant regime, to certify R â‰¥1, we ï¬rst need to guarantee that
no sample within the distance of 1 from x0 uses Ïƒ1 < 0.953Ïƒ0. To even strengthen this statement,
note that one needs to guarantee Ïƒ1 to be even much closer to Ïƒ0 in practice. Why? The results of
Theorem 4 lower-bound the Î¾ functions at 0. However, since Î¾ functions are strictly increasing (as
shown in Appendix F), one usually needs Ïƒ0 and Ïƒ1 to be much closer to each other to guarantee Î¾
being smaller than 0.5 at a â‰«0. This not only forces the Ïƒ(x) function to have really small semi-
elasticity but also makes it problematic to deï¬ne a stochastic Ïƒ(x). For more, see Appendix B.2."
"FOR
A
POTENTIAL
ADVERSARY",0.09001233045622688,"To fully understand, how the curse of dimensionality affects the usage of IDRS, we mention two
more signiï¬cant effects. First, with increasing dimension, the average distance between samples
tends to grow as
âˆš"
"FOR
A
POTENTIAL
ADVERSARY",0.09124537607891492,"N. This enables bigger distance to change Ïƒ(x). On the other hand, the average
level of Ïƒ(x) used (like âˆ¼0.12, 0.25, . . . ) needs to be adjusted also as
âˆš"
"FOR
A
POTENTIAL
ADVERSARY",0.09247842170160296,"N with increasing dimen-
sion. The bigger average level of Ïƒ(x) we use, the more is the semi-elasticity of Ïƒ(x) restricted by
Theorem 4 and Theorem 7. All together, these two effects combine in a ï¬nal trend that for Ïƒ0 and
Ïƒ1 being variances used in two random test samples, |Ïƒ0/Ïƒ1 âˆ’1| is restricted to go to 0 as 1/
âˆš"
"FOR
A
POTENTIAL
ADVERSARY",0.093711467324291,"N.
For detailed explanation, see Appendix B.4."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.09494451294697903,"3
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.09617755856966707,"As we discuss above, usage of IDRS is challenging. How can we use Ïƒ(x) and obtain valid, math-
ematically justiï¬ed certiï¬ed radiuses? Fix some design Ïƒ(x). If Ïƒ(x) is not trivial, to obtain a
certiï¬ed radius at x0, we need to go over all the possible adversaries x1 in the neighborhood of x0,
compute Ïƒ1 and Î¾<,>(a). Then, the certiï¬ed radius is the inï¬mum over âˆ¥x0 âˆ’x1âˆ¥for all uncertiï¬ed
x1 points. Of course, this is a priori infeasible. Fortunately, the Î¾ functions possess a property that
helps to simplify this procedure. For convenience, we extend the notation of Î¾ functions such that
Î¾(a, Ïƒ1) additionally denotes the dependance on the Ïƒ1 value.
Theorem 6. Let x0, x1, pA, Ïƒ0 be as usual and let âˆ¥x0 âˆ’x1âˆ¥= R. Then, the following two
statements hold:"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.09741060419235512,"1. Let Ïƒ1 â‰¤Ïƒ0. Then, for all Ïƒ2 : Ïƒ1 â‰¤Ïƒ2 â‰¤Ïƒ0, if Î¾>(R, Ïƒ2) > 0.5, then Î¾>(R, Ïƒ1) > 0.5."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.09864364981504316,"2. Let Ïƒ1 â‰¥Ïƒ0. Then, for all Ïƒ2 : Ïƒ1 â‰¥Ïƒ2 â‰¥Ïƒ0, if Î¾<(R, Ïƒ2) > 0.5, then Î¾<(R, Ïƒ1) > 0.5."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.0998766954377312,"Theorem 6 serves as a kind of monotonicity property. The main gain is that now, for each distance
R from x0, we need to pick just two adversaries â€“ the one with biggest Ïƒ1 (if bigger than Ïƒ0) and
the one with the smallest Ïƒ1 (if smaller than Ïƒ0). If we cannot certify some point x1 at the distance
R from x0, then we will for sure not be able to certify at least one of the two adversaries with the
most extreme Ïƒ1 values."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.10110974106041924,"This does, however, not sufï¬ce for most of the reasonable Ïƒ(x) designs, since it might be still too
hard to determine the two most extreme Ïƒ1â€™s at some distance from x0. Therefore, we need to
assume that our Ïƒ(x) has a bounded semi-elasticity coefï¬cient r. Then we have a guarantee that
Ïƒ(x0) exp(âˆ’ra) â‰¤Ïƒ(x1) â‰¤Ïƒ(x0) exp(ra). Thus, we can assume the worst-case extreme Ïƒ1â€™s for
every distance a from x0. Using this, we guarantee the following certiï¬ed radius.
Theorem 7. Let Ïƒ(x) be r-semi-elastic function and x0, pA, N, Ïƒ0 as usual. Then, the certiï¬ed
radius at x0 guaranteed by our method is"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.10234278668310727,"CR(x0) = max {0, sup {R â‰¥0; Î¾>(R, Ïƒ0 exp(âˆ’rR)) < 0.5 and Î¾<(R, Ïƒ0 exp(rR)) < 0.5}} ."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.10357583230579531,Under review as a conference paper at ICLR 2022
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.10480887792848335,"Note that Theorem 7 can be adjusted to the case where we have a separate estimate pB and do not
put pB = 1 âˆ’pA (see Appendix B.5). Since the bigger the semi-elasticity constant of Ïƒ(x) is, the
worse certiï¬cations we obtain, it is important to estimate the constant tightly. Even with a good
estimate of r, we still get smaller certiï¬ed radiuses in comparison with using the Ïƒ(x) exactly, but
that is a prize that is inevitable for the feasibility of the method."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.1060419235511714,"The practical algorithm is then very easy - we just need to pick sufï¬ciently dense space of possible
radiuses and determine the smallest, for which either Î¾>(R, Ïƒ0 exp(âˆ’rR)) or Î¾<(R, Ïƒ0 exp(rR))
becomes bigger than a half. The only non-trivial part is, how to evaluate the Î¾ functions. For small
values of R, the exp(âˆ’rR) is very close to 1 and from the deï¬nition of Î¾ functions it is obvious
that this results in extremely big inputs to the cdf and quantile function of NCCHSQ. To avoid
numerical problems, we employ a simple hack where we assume thresholds for Ïƒ1 such that for
R small enough, these thresholds are used instead of Ïƒ0 exp(Â±rR)). Unfortunately, the numerical
stability still disables the usage of this method on really high-dimensional datasets like ImageNet.
For more details on implementation, see Appendix D."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.10727496917385944,"4
THE DESIGN OF Ïƒ(x) AND EXPERIMENTS"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.10850801479654747,"The only missing ingredient to ï¬nally being able to use IDRS is the Ïƒ(x) function. As we have seen,
this function has to be r-semi-elastic for rather small r and ideally deterministic. Yet it should at
least roughly fulï¬ll the requirements imposed by the motivation â€“ it should possess big values for
points far from the decision boundary of f and rather small for points close to it. Adhering to these
restrictions, we use the following function:"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.10974106041923551,"Ïƒ(x) = Ïƒb exp ï£« ï£­r ï£« ï£­1 k ï£« ï£­
X"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.11097410604192355,"xiâˆˆNk(x)
âˆ¥x âˆ’xiâˆ¥ ï£¶ ï£¸âˆ’m ï£¶ ï£¸ ï£¶"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.11220715166461159,"ï£¸,
(1)"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.11344019728729964,"for Ïƒb being a base standard deviation, r being the required semi-elasticity, {xi}d
i=1 the training set,
Nk(x) the k nearest neighbors of x and m the normalization constant. Intuitively, if a sample is far
from all other samples, it will be far from the decision boundary, unless the network overï¬ts to this
sample. On the other hand, the dense clusters of samples are more likely to be positioned near the
decision boundary, since such clusters have a high leverage on the networkâ€™s weights, forcing the
decision boundary to adapt well to the geometry of the cluster. To use such a function, however, we
ï¬rst prove that it is indeed r-semi-elastic.
Theorem 8. The Ïƒ(x) deï¬ned in equation 1 is r-semi-elastic."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.11467324290998766,"We test our IDRS and our Ïƒ(x) functions extensively. For both CIFAR10 (Krizhevsky, 2009) and
MNIST (LeCun et al., 1999) datasets, we analyze series of different experimental setups, including
experiments with an input-dependent train-time Gaussian data augmentation. We present a direct
comparison of our method with the constant Ïƒ method using evaluation strategy from Cohen et al.
(2019) (all other experiments, including ablation studies, and the discussion on the hyperparameter
selection are presented in Appendix E). Here, we compare Cohen et al. (2019)â€™s evaluations for
Ïƒ = 0.12, 0.25, 0.50 with our evaluations, setting Ïƒb = Ïƒ, r = 0.01, k = 20, m = 5, 1.5 (for
CIFAR10 and MNIST, respectively), applied on models trained with Gaussian data augmentation,
using constant standard deviation roughly equal to the average test-time Ïƒ(x) or test-time Ïƒ. For
CIFAR10, these levels of train-time standard deviation are Ïƒtr = 0.126, 0.263, 0.53 and for MNIST
Ïƒtr = 0.124, 0.258, 0.517. In this way, the levels of Ïƒ(x) we use in the direct comparison are spread
from the values roughly equal to Cohen et al. (2019)â€™s constant Ïƒ to higher values. The results are
depicted in Figure 4."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.1159062885326757,"From Figure 4 we see that we outperform the constant Ïƒ for small levels of Ïƒ, such as 0.12 or 0.25.
On higher levels of Ïƒ, we are, in general, worse (see explanation in Appendix B.3). The most visible
improvement is in mitigation of the truncation of certiï¬ed accuracy (certiï¬ed accuracy waterfall).
To comment on the other two issues, we provide Tables 2 and 3 with the clean accuracies and class-
wise accuracy standard deviations. These results are averages of 8 independent runs and in Table 2,
the displayed error values are equal to empirical standard deviations."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.11713933415536375,"From Tables 2 and 3, we draw two conclusions - ï¬rst, it is not easy to judge about the robustness
vs. accuracy trade-off, because the differences in clean accuracies are not statistically signiï¬cant in"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.11837237977805179,Under review as a conference paper at ICLR 2022
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.11960542540073983,Figure 4: Comparison of certiï¬ed accuracy plots for Cohen et al. (2019) and our work.
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.12083847102342787,"dataset
Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.01, Ïƒtr increased
C
0.852 Â± 0.002
0.780 Â± 0.013
0.673 Â± 0.008
r = 0.00
C
0.851 Â± 0.006
0.792 Â± 0.004
0.674 Â± 0.018
r = 0.01, Ïƒtr increased
M
0.9912 Â± 0.0003
0.9910 Â± 0.0006
0.9881 Â± 0.0003
r = 0.00
M
0.9914 Â± 0.0004
0.9907 Â± 0.0004
0.9886 Â± 0.0005"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.1220715166461159,"Table 2: Clean accuracies for both input-dependent and constant Ïƒ evaluation strategies on CIFAR10
(C) and MNIST (M)."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.12330456226880394,"dataset
Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.01, Ïƒtr increased
C
0.076
0.099
0.120
r = 0.00
C
0.076
0.097
0.122
r = 0.01, Ïƒtr increased
M
0.00775
0.00777
0.00930
r = 0.00
M
0.00751
0.00778
0.00934"
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.12453760789149199,"Table 3: Class-wise accuracy standard deviations for both input-dependent and constant Ïƒ evaluation
strategies on CIFAR10 (C) and MNIST (M)."
HOW TO USE INPUT-DEPENDENT SMOOTHING PROPERLY,0.12577065351418001,"any of the experiments (not even for CIFAR10 and Ïƒ = 0.25, where the difference is at least pro-
nounced). However, the general trend in Table 2 indicates that the clean accuracies tend to slightly
decrease with increasing rate. Nevertheless, the differences are not large enough to compensate
negatively the fact that we outperform constant Ïƒ in terms of the certiï¬ed accuracies. Second, the
standard deviations of the class-wise accuracies, which serve as a good measure of the impact of
the shrinking phenomenon and subsequent fairness, donâ€™t signiï¬cantly change after applying the
non-constant RS."
RELATED WORK,0.12700369913686807,"5
RELATED WORK"
RELATED WORK,0.1282367447595561,"Since the vulnerability of deep neural networks against adversarial attacks has been noticed by
Szegedy et al. (2013); Biggio et al. (2013) a lot of effort has been put into making neural nets
more robust. There are two types of solutions â€“ empirical and certiï¬ed defenses. While empirical
defenses suggest heuristics to make models robust, certiï¬ed approaches additionally provide a way
to compute a mathematically valid robust radius."
RELATED WORK,0.12946979038224415,Under review as a conference paper at ICLR 2022
RELATED WORK,0.13070283600493218,"One of the most effective empirical defenses, adversarial training (Goodfellow et al., 2014; Kurakin
et al., 2016; Madry et al., 2017), is based on a very intuitive idea to use adversarial examples for
training. Unfortunately, together with adversarial training, other promising empirical defenses were
subsequently broken by more sophisticated adversarial methods (for instance Carlini & Wagner
(2017); Athalye & Carlini (2018); Athalye et al. (2018), among many others)."
RELATED WORK,0.1319358816276202,"Among many certiï¬ed defenses (Tsuzuku et al., 2018; Anil et al., 2019; Hein & Andriushchenko,
2017; Wong & Kolter, 2018; Raghunathan et al., 2018; Mirman et al., 2018; Weng et al., 2018),
one of the most successful yet is RS. While Lecuyer et al. (2019) introduced the method within
the context of differential privacy, Li et al. (2019) proceeded via the knowledge of RÂ´enyi diver-
gences. Possibly the most prominent work on RS is that of Cohen et al. (2019), where authors fully
established RS and proved tight certiï¬cation guarantees."
RELATED WORK,0.13316892725030827,"Later, a lot of authors further worked with RS. The work of Yang et al. (2020) generalizes the cer-
tiï¬cation provided by Cohen et al. (2019), to certiï¬cations with respect to the general lp norms and
provide the optimal smoothing distributions for each of the norms. Other works point out differ-
ent problems or weaknesses of RS like the curse of dimensionality (Kumar et al., 2020; Hayes,
2020; Wu et al., 2021), robustness vs. accuracy trade-off (Gao et al., 2020) or a shrinking phe-
nomenon(Mohapatra et al., 2020a), which yields serious fairness issues (Mohapatra et al., 2020a)."
RELATED WORK,0.1344019728729963,"The work of Mohapatra et al. (2020b) improves RS further by introducing the ï¬rst-order information
about g. In this work, authors not only estimate g(x), but also âˆ‡g(x), making more restrictions on
the possible base models f that might have created g. Zhai et al. (2020) and Salman et al. (2019)
improve the training procedure of f to yield better robustness guarantees of g. Salman et al. (2019)
directly use adversarial training of the base classiï¬er f. Finally, Zhai et al. (2020) introduce so-
called soft smoothing, which enables to compute gradients directly for g and construct a training
method, which optimizes directly for the robustness of g via the gradient descent."
RELATED WORK,0.13563501849568435,"To address several issues connected to randomized smoothing, there have been already four works
that introduce the usage of IDRS. Wang et al. (2021) divide RN into several regions Ri, i âˆˆ
{1, . . . , K} and optimize for Ïƒi, i âˆˆ{1, . . . , K} locally, such that Ïƒi is a most suitable choice
for the region Ri. Yet this work partially solves some problems of randomized smoothing, it also
possesses some practical and philosophical issues (see Appendix C). Alfarra et al. (2020); Eiras et al.
(2021); Chen et al. (2021), suggest to optimize for locally optimal Ïƒi, for each sample xi from the
test set. A similar strategy is proposed by these works in the training phase, with the intention of
obtaining the base model f that is most suitable for the construction of the smoothed classiï¬er g.
They demonstrate, that by using this input-dependent approach, one can overcome some of the main
problems of randomized smoothing. However, as we demonstrate in Appendix C, their methodology
is not valid and therefore their results are not trustworthy."
CONCLUSIONS,0.13686806411837238,"6
CONCLUSIONS"
CONCLUSIONS,0.1381011097410604,"We show in this work that input-dependent randomized smoothing suffers from the curse of dimen-
sionality. In the high-dimensional regime, the usage of input-dependent Ïƒ(x) is being put under
strict constraints. The Ïƒ(x) function is forced to have very small semi-elasticity. This is in con-
ï¬‚ict with some recent works, which have used the input-dependent randomized smoothing without
mathematical justiï¬cation and therefore claim invalid results. It seems that input-dependent random-
ized smoothing has limited potential of improvement over the classical, constant-Ïƒ RS. Moreover,
due to numerical instability, the computation of certiï¬ed radiuses on high-dimensional datasets like
ImageNet remains to be an open challenge."
CONCLUSIONS,0.13933415536374846,"On the other hand, we prepare a ready-to-use mathematically underlined framework for the usage
of the input-dependent RS and show, that it works well for small to medium-sized problems. We
also show via extensive experiments, that our concrete design of the Ïƒ(x) function reasonably treats
the truncation issue connected to constant-Ïƒ RS and is partially capable of mitigating the robust-
ness vs. accuracy one. The most intriguing and promising direction for the future work lies in the
development of new Ïƒ(x) functions, which are capable of even better treatment of the mentioned
issues."
CONCLUSIONS,0.1405672009864365,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.14180024660912455,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.14303329223181258,"Both our theoretical and practical results and experiments are reproducible. In the theoretical part,
we provide all the relevant proofs, insights and all the important reasoning. We use public datasets
for our experiments and our code contains just public, well-known libraries. The code will be
publicly available after the review process. We do not use seeds for stochastic algorithms, but the
level of variance is not large enough to obtain qualitatively different results. Upon request, we are
willing to provide models trained by us to obtain exactly the same results."
REFERENCES,0.1442663378545006,REFERENCES
REFERENCES,0.14549938347718866,"Motasem Alfarra, Adel Bibi, Philip HS Torr, and Bernard Ghanem. Data dependent randomized
smoothing. arXiv preprint arXiv:2012.04351, 2020."
REFERENCES,0.1467324290998767,"Cem Anil, James Lucas, and Roger Grosse. Sorting out Lipschitz function approximation. In Kama-
lika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Confer-
ence on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 291â€“
301. PMLR, 09â€“15 Jun 2019. URL http://proceedings.mlr.press/v97/anil19a.
html."
REFERENCES,0.14796547472256474,"Anish Athalye and Nicholas Carlini. On the robustness of the cvpr 2018 white-box adversarial
example defenses. arXiv preprint arXiv:1804.03286, 2018."
REFERENCES,0.14919852034525277,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International conference on machine
learning, pp. 274â€“283. PMLR, 2018."
REFERENCES,0.1504315659679408,"Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Ë‡SrndiÂ´c, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Hendrik
Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip Ë‡ZeleznÂ´y (eds.), Machine Learning and
Knowledge Discovery in Databases, pp. 387â€“402, Berlin, Heidelberg, 2013. Springer Berlin Hei-
delberg. ISBN 978-3-642-40994-3."
REFERENCES,0.15166461159062886,"Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten de-
tection methods. In Proceedings of the 10th ACM workshop on artiï¬cial intelligence and security,
pp. 3â€“14, 2017."
REFERENCES,0.15289765721331688,"Chen Chen, Kezhi Kong, Peihong Yu, Juan Luque, Tom Goldstein, and Furong Huang.
Insta-
rs: Instance-wise randomized smoothing for improved robustness and accuracy. arXiv preprint
arXiv:2103.04436, 2021."
REFERENCES,0.15413070283600494,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiï¬ed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310â€“1320. PMLR, 2019."
REFERENCES,0.15536374845869297,"Francisco Eiras, Motasem Alfarra, M Pawan Kumar, Philip HS Torr, Puneet K Dokania, Bernard
Ghanem, and Adel Bibi. Ancer: Anisotropic certiï¬cation via sample-wise volume maximization.
arXiv preprint arXiv:2107.04570, 2021."
REFERENCES,0.15659679408138102,"Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classiï¬cation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1625â€“1634, 2018."
REFERENCES,0.15782983970406905,"Yue Gao, Harrison Rosenberg, Kassem Fawaz, Somesh Jha, and Justin Hsu. Analyzing accuracy
loss in randomized smoothing defenses. arXiv preprint arXiv:2003.01595, 2020."
REFERENCES,0.15906288532675708,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.16029593094944514,"Ingo GÂ¨uhring, Mones Raslan, and Gitta Kutyniok. Expressivity of deep neural networks. arXiv
preprint arXiv:2007.04759, 2020."
REFERENCES,0.16152897657213316,Under review as a conference paper at ICLR 2022
REFERENCES,0.16276202219482122,"Jamie Hayes. Extensions and limitations of randomized smoothing for robustness guarantees. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Work-
shops, pp. 786â€“787, 2020."
REFERENCES,0.16399506781750925,"Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classiï¬er
against adversarial manipulation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/e077e1a544eec4f0307cf5c3c721d944-Paper.pdf."
REFERENCES,0.16522811344019728,"Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009. URL http://
www.cs.toronto.edu/Ëœkriz/learning-features-2009-TR.pdf."
REFERENCES,0.16646115906288533,"Aounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensionality on
randomized smoothing for certiï¬able robustness. In International Conference on Machine Learn-
ing, pp. 5458â€“5467. PMLR, 2020."
REFERENCES,0.16769420468557336,"Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016."
REFERENCES,0.16892725030826142,"Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The mnist database of handwritten
digits, 1999. URL http://yann.lecun.com/exdb/mnist/."
REFERENCES,0.17016029593094945,"Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiï¬ed
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656â€“672. IEEE, 2019."
REFERENCES,0.17139334155363747,"Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certiï¬ed adversarial robustness with
additive noise. Advances in Neural Information Processing Systems, 32:9464â€“9474, 2019."
REFERENCES,0.17262638717632553,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017."
REFERENCES,0.17385943279901356,"Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learn-
ing Research, pp. 3578â€“3586. PMLR, 10â€“15 Jul 2018. URL http://proceedings.mlr.
press/v80/mirman18b.html."
REFERENCES,0.17509247842170161,"Jeet Mohapatra, Ching-Yun Ko, Sijia Liu, Pin-Yu Chen, Luca Daniel, et al. Rethinking randomized
smoothing for adversarial robustness. arXiv preprint arXiv:2003.01249, 2020a."
REFERENCES,0.17632552404438964,"Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Higher-
order certiï¬cation for randomized smoothing. Advances in Neural Information Processing Sys-
tems, 33, 2020b."
REFERENCES,0.17755856966707767,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiï¬ed defenses against adversarial exam-
ples. In International Conference on Learning Representations, 2018."
REFERENCES,0.17879161528976573,"Christian Robert. On some accurate bounds for the quantiles of a non-central chi squared distribu-
tion. Statistics & probability letters, 10(2):101â€“106, 1990."
REFERENCES,0.18002466091245375,"Hadi Salman, Jerry Li, Ilya P Razenshteyn, Pengchuan Zhang, Huan Zhang, SÂ´ebastien Bubeck,
and Greg Yang. Provably robust deep learning via adversarially trained smoothed classiï¬ers. In
NeurIPS, 2019."
REFERENCES,0.1812577065351418,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013."
REFERENCES,0.18249075215782984,Under review as a conference paper at ICLR 2022
REFERENCES,0.18372379778051787,"Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama.
Lipschitz-margin training:
Scal-
able certiï¬cation of perturbation invariance for deep neural networks.
In S. Ben-
gio,
H.
Wallach,
H.
Larochelle,
K.
Grauman,
N.
Cesa-Bianchi,
and
R.
Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
485843481a7edacbfce101ecb1e4d2a8-Paper.pdf."
REFERENCES,0.18495684340320592,"Tim Van Erven and Peter Harremos.
RÂ´enyi divergence and kullback-leibler divergence.
IEEE
Transactions on Information Theory, 60(7):3797â€“3820, 2014."
REFERENCES,0.18618988902589395,"Lei Wang, Runtian Zhai, Di He, Liwei Wang, and Li Jian. Pretrain-to-ï¬netune adversarial training
via sample-wise randomized smoothing. 2021. URL https://openreview.net/forum?
id=Te1aZ2myPIu."
REFERENCES,0.187422934648582,"Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and
Inderjit Dhillon. Towards fast computation of certiï¬ed robustness for ReLU networks. In Jennifer
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5276â€“5285. PMLR,
10â€“15 Jul 2018. URL http://proceedings.mlr.press/v80/weng18a.html."
REFERENCES,0.18865598027127004,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286â€“5295. PMLR,
2018."
REFERENCES,0.18988902589395806,"Yihan Wu, Aleksandar Bojchevski, Aleksei Kuvshinov, and Stephan GÂ¨unnemann. Completing the
picture: Randomized smoothing suffers from the curse of dimensionality for a large family of
distributions. In International Conference on Artiï¬cial Intelligence and Statistics, pp. 3763â€“3771.
PMLR, 2021."
REFERENCES,0.19112207151664612,"Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized
smoothing of all shapes and sizes. In International Conference on Machine Learning, pp. 10693â€“
10705. PMLR, 2020."
REFERENCES,0.19235511713933415,"Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh,
and Liwei Wang. Macer: Attack-free and scalable robust training via maximizing certiï¬ed radius.
arXiv preprint arXiv:2001.02378, 2020."
REFERENCES,0.1935881627620222,"A
THE ISSUES OF CONSTANT Ïƒ SMOOTHING"
REFERENCES,0.19482120838471023,"A.1
TOY EXAMPLE"
REFERENCES,0.1960542540073983,"To better demonstrate our ideas, we prepared a two-dimensional simple toy dataset. This dataset
can be seen in Figure 5. The dataset is generated in polar coordinates, having uniform angle and the
distance distributed as a square root of suitable chi-square distribution. The classes are positioned
in a circle sectors, one in a sector with a very sharp angle. The number of training samples is 500
for each class, number of test samples is 100 for each class (except demonstrative ï¬gures, where
we increased it to 300). The model that was trained on this dataset was a simple fully connected
three-layer neural network with ReLU activations and a maximal width of 20."
REFERENCES,0.19728729963008632,"A.2
UNDERCERTIFICATION CAUSED BY THE USE OF LOWER CONFIDENCE BOUNDS"
REFERENCES,0.19852034525277434,"As we mention in Section 1, one can not usually obtain exact values of pA and pB. However, it is
obvious, that for vast majority of evaluated samples, pA < pA and pB > pB. Given the nature of
our certiï¬ed radius, it follows that R < R, where R denotes the certiï¬ed radius coming from the
certiï¬cation procedure with pA and pB, while R here stands for the certiï¬ed radius corresponding
to true values pA, pB."
REFERENCES,0.1997533908754624,"It is, therefore, clear, that we face a certain level of under-certiï¬cation. But how serious under-
certiï¬cation it is? Assume the case with a linear base classiï¬er. Imagine, that we move the point
x further and further away from the decision boundary. Therefore, pA âˆ’â†’1. At some point, the"
REFERENCES,0.20098643649815043,Under review as a conference paper at ICLR 2022
REFERENCES,0.20221948212083848,"probability will be so large, that with high probability, all n samplings in our evaluation of pA will
be classiï¬ed as A, obtaining Ë†pA = 1 - the empirical probability. The lower conï¬dence bound pA is
therefore bounded by having Ë†pA = 1. Thus, from some point, the certiï¬cation will yield the same
pA regardless of the true value of pA. So in practice, we have an upper bound on the certiï¬ed radius
R in the case of the linear boundary. In Figure 7 (left), we see the truncation effect. Using Ïƒ = 1,
from a distance of roughly 4, we can no longer achieve a better certiï¬ed radius, despite its theoretical
value equals the distance. Similarly, if we ï¬x a distance of x from decision boundary and vary Ïƒ, for
very small values of Ïƒ, the value of Î¦âˆ’1(pA) will no longer increase, but the values of Ïƒ will pull R
towards zero. This behaviour is depicted in Figure 7 (right)."
REFERENCES,0.2034525277435265,"We can also look at it differently - what is the ratio between Î¦âˆ’1(pA) and Î¦âˆ’1(pA) for different
values of pA? Since R = ÏƒÎ¦âˆ’1(pA) and R = ÏƒÎ¦âˆ’1(pA), the ratio represents the â€œundercertiï¬-"
REFERENCES,0.20468557336621454,"cation rateâ€. In Figure 6 we plot
Î¦âˆ’1(pA)
Î¦âˆ’1(pA) as a function of pA for two different ranges of values.
The situation is worst for very small and very big values of pA. In the case of very big values, this
can be explained due to extreme nature of Î¦âˆ’1. For small values of pA, it can be explained as a
consequence of a fact, that even small difference between pA and pA will yield big ratio between
Î¦âˆ’1(pA) and Î¦âˆ’1(pA) due to the fact, that these values are close to 0."
REFERENCES,0.2059186189889026,"If we look at the left plot on Figure 8 we see, that the certiï¬ed accuracy plots also possess the
truncations. Above some radius, no sample is certiï¬ed anymore. The problem is obviously more
serious for small values of Ïƒ. On the right plot of Figure 8, we see, that samples far from the decision
boundary are obviously under-certiï¬ed. We can also see, that certiï¬ed radiuses remain constant, even
though in reality they would increase with increasing distance from the decision boundary."
REFERENCES,0.20715166461159062,"All the observations so far motivate us to use rather large values of Ïƒ in order to avoid the truncation
problem. However, as we will see in the next sections, using a large Ïƒ carries a different, yet equally
serious burden."
REFERENCES,0.20838471023427868,"A.3
ROBUSTNESS VS. ACCURACY TRADE-OFF"
REFERENCES,0.2096177558569667,"As we demonstrate in the previous subsection, it is be useful to use large values of Ïƒ to prevent the
under-certiï¬cation. But does it come without a prize? If we have a closer look at Figure 8 (right), we
might notice, that the accuracy on the threshold 0, i.e. â€œclean accuracyâ€, decreases as Ïƒ increases.
This effect has been noticed in the literature (Cohen et al., 2019; Gao et al., 2020; Mohapatra et al.,
2020a) and is called robustness vs. accuracy tradeoff."
REFERENCES,0.21085080147965474,"There are several reasons, why this problem occurs. Generally, changing Ïƒ changes the decision
boundary of g and we might assume, that due to the high complexity of the boundary of f, the
decision boundary of g becomes smoother. If Ïƒ is too large, however, the decision boundary will be
so smooth, that it might lose some amount of the base classiï¬erâ€™s expressivity. Another reason for the
accuracy drop is also the increase in the number of samples, for which the evaluation is abstained.
This is because using big values of Ïƒ makes more classes â€œwithin the reach of our distributionâ€,
making the pA and pA small. If pA < 0.5 and we do not estimate pB but set pB = 1 âˆ’pA, then"
REFERENCES,0.2120838471023428,Figure 5: The toy dataset.
REFERENCES,0.21331689272503082,Under review as a conference paper at ICLR 2022
REFERENCES,0.21454993834771888,"Figure 6: The ratio between certiï¬ed radius if using lower conï¬dence bounds and if using exact
values for the case of linear boundary."
REFERENCES,0.2157829839704069,"Figure 7: Left: Certiï¬ed radius as a function of distance in linear boundary case. The truncation
is due to the use of lower conï¬dence bounds. The parameters are n = 100000, Î± = 0.001, Ïƒ = 1.
Right: Certiï¬ed radius for a point x at ï¬xed distance 1 from linear boundary as a function of used
Ïƒ. The undercertiï¬cation follows from usage of lower conï¬dence bounds."
REFERENCES,0.21701602959309493,"Figure 8: Results of certiï¬cation on toy dataset. Left: Certiï¬ed accuracy for different levels of Ïƒ.
Right: Certiï¬ed radiuses and decision boundary of g visualized directly on test set."
REFERENCES,0.218249075215783,"we are not able to classify the sample as class A, yet we cannot classify it as a different class either,
which forces us to abstain. To demonstrate these results, we computed not only the clean accuracies
of Cohen et al. (2019) evaluations but also the abstention rates. Results are depicted in the Table 4."
REFERENCES,0.21948212083847102,"From the table, it is obvious, that the abstention rate is possibly even bigger cause of accuracy drop
than the â€œclean misclassiï¬cationâ€. This problem can be partially solved if one estimated pB together"
REFERENCES,0.22071516646115907,Under review as a conference paper at ICLR 2022
REFERENCES,0.2219482120838471,"Accuracy
Abstention rate
Misclassiï¬cation rate
Ïƒ = 0.12
0.814
0.038
0.148
Ïƒ = 0.25
0.748
0.086
0.166
Ïƒ = 0.50
0.652
0.166
0.182
Ïƒ = 1.00
0.472
0.29
0.238"
REFERENCES,0.22318125770653513,"Table 4: Accuracies, rates of abstentions and misclassiï¬cation rates of Cohen et al. (2019) evaluation
for different levels of Ïƒ."
REFERENCES,0.22441430332922319,"Figure 9: Heatmaps and decision boudnary of base classiï¬er (top left) and the smoothed classiï¬er
for increasing levels of Ïƒ. As Ïƒ increases, the classiï¬er is more smooth and the decision boundary
recedes."
REFERENCES,0.22564734895191121,"with pA too. In this way, using big Ïƒ yields generally small estimated class probabilities, but since
pA â‰¥pB, the problematic pB â‰¥pA occur just very rarely. Another option is to increase the number
of Monte-Carlo samplings for the classiï¬cation decision, what is almost for free."
REFERENCES,0.22688039457459927,"Yet another reason for the decrease in the accuracy is the so-called shrinking phenomenon, which
we will discuss in the next subsection."
REFERENCES,0.2281134401972873,"In contrast with the truncation effect, the robustness vs. accuracy trade-off motivates the usage of
smaller values of Ïƒ in order to prevent the accuracy loss, which is deï¬nitely a very serious issue."
REFERENCES,0.22934648581997533,"A.4
SHRINKING PHENOMENON"
REFERENCES,0.23057953144266338,"How exactly does the decision boundary of g change, as we change the Ïƒ? For instance, if f is a
linear classiï¬er, then the boundary does not change at all. To answer this question, we employ the
following experiment: For our toy base classiï¬er f on our toy dataset, we increase Ïƒ and plot the
heatmap of f, g, together with its decision boundary. This experiment is depicted on Figure 9. As
we see from the plots, increasing Ïƒ causes several effects. First of all, the heatmap becomes more
and more blurred, what proves, that stronger smoothing implies stronger smoothness."
REFERENCES,0.2318125770653514,"Second, crucial, effect is that the bigger the Ïƒ, the smaller the decision boundary of a submissive
class is. The shrinkage becomes pronounced from Ïƒ = 1. Already for Ïƒ = 4, there is hardly
any decision boundary anymore. Generally, as Ïƒ âˆ’â†’âˆž, g will predict the class with the biggest
volume in the input space (in the case of bounded input space, like in image domain, this is very
well deï¬ned). For extreme values of sigma, the pA will practically just be the ratio between the
volume of A and the actual volume of the input space (for bounded input spaces)."
REFERENCES,0.23304562268803947,Under review as a conference paper at ICLR 2022
REFERENCES,0.2342786683107275,"Following from these results, but also from basic intuition, it seems, that an undesired effect becomes
present as Ïƒ increases - the bounded/convex regions become to shrink, like in Figure 9, while the
unbounded/big/anti-convex regions expand. This is called shrinking phenomenon. Mohapatra et al.
(2020a) investigate this effect rather closely. They deï¬ne the shrinkage and vanishing of regions
formally and prove rigorously, that if Ïƒ âˆ’â†’âˆž, bounded regions, or semi-bounded regions (see
Mohapatra et al. (2020a)) will eventually vanish. We formulate the main result in this direction."
REFERENCES,0.23551171393341552,"Theorem 9. Let us have K the number of classes and the dimension N. Assume, that we have some
bounded decision region D for a speciï¬c class roughly centered around 0. Further assume, that this
is the only region where the class is classiï¬ed. Let R be a smallest radius such that D âŠ‚BR(0).
Then, this decision region will vanish at most for Ïƒ â‰¥R
âˆš K
âˆš N ."
REFERENCES,0.23674475955610358,"Proof. The idea of the proof is not very hard. First, the authors prove, that the smoothed region will
be a subset of the smoothed BR(0). Then, they upper-bound the vanishing threshold of such a ball
in two steps. First, they show, that if 0 is not classiï¬ed as the class, then no other point will be (this
is quite an intuitive geometrical statement. The BR(0) has the biggest probability under N(x, Ïƒ2I)
if x â‰¡0). Second, they upper-bound the threshold for Ïƒ, under which BR(0) will have probability
below 1"
REFERENCES,0.2379778051787916,"K (since they use slightly different setting as Cohen et al. (2019)) for the N(x, Ïƒ2I). Using
some insights about incomplete gamma function, which is known to be also the cdf of central chi-
square distribution, and some other integration tricks, they obtain the resulting bound."
REFERENCES,0.23921085080147966,"Besides Theorem 9, authors also claim many other statements abound shrinking, including shrinking
of semi-bounded regions. Moreover, they also conduct experiments on CIFAR10 and ImageNet to
support their theoretical ï¬ndings. They also point out serious fairness issue that comes out as a
consequence of the shrinkage phenomenon. For increasing levels of Ïƒ, they measure the class-wise
clean accuracy of the smoothed classiï¬er. If f is trained with gaussian data augmentation (what is
known to be a good practice in randomized smoothing), using Ïƒ = 0.12, the worst class cat has test
accuracy of 67%, while the best class automobile attains the accuracy of 92%. The ï¬gures, however,
change drastically, if we use Ïƒ = 1 instead. In this case, the worst predicted class cat has accuracy
of poor 22%, while ship has reasonable accuracy 68%. As authors claim, this is a consequence of
the fact that samples of cat are situated more in bounded, convex regions, that suffer from shrinking,
while samples of ship are mostly placed in expanded regions of anti-convex shape that will expand
as the Ïƒ grows. In addition, the authors also show, that the gaussian data augmentation or adversarial
training will reduce the shrinking phenomenon just partially and for moderate and high values of Ïƒ,
this effect will be present anyway."
REFERENCES,0.2404438964241677,"We must emphasize, that this is a serious fairness issue, that has to be treated before randomized
smoothing can be fully used in practice. For instance, if we trained a neural network to classify
humans into several categories, fairness of classiï¬cation is inevitable and the neural network cannot
be used until this issue is solved."
REFERENCES,0.24167694204685575,"Similarly as the robustness vs. accuracy tradeoff, this issue also motivates to use rather smaller
values of Ïƒ. We see, that it is not possible to address all three problems consistently because they
disagree on whether to use smaller, or bigger values of Ïƒ ."
REFERENCES,0.24290998766954378,"A.5
EXPERIMENTS ON HIGH-DIMENSIONAL TOY DATASET"
REFERENCES,0.2441430332922318,"In this subsection, we present the results of our motivational experiment on a synthetic dataset.
Before reading this section, please read our main text, because we will use the necessary notation of
the paper."
REFERENCES,0.24537607891491986,"The dataset we evaluated our method on is a generalization of the dataset visualized on Figure 1.
The data points from one class lie in a cone of small angle and the points are generated such that
the density is higher near the vertex of the cone (which is put in origin). Points from other class
are generated from a spherically symmetrical distribution (where points sampled into the cone are
excluded) with density again highest in the center (note, that the density peak is more pronounced
than in the case of normal distribution, where the density around the center resembles uniform dis-
tribution). This dataset is chosen so that the Ïƒ(x) function designed in Equation 1 well corresponds
to the geometry of the decision boundary. Moreover it is chosen so that the conic decision region"
REFERENCES,0.2466091245376079,Under review as a conference paper at ICLR 2022
REFERENCES,0.24784217016029594,"Dimension
Ïƒ (Ïƒb)
r
Accuracy
2
0.5
-
0.943
2
0.4
0.2
0.96
2
0.5
0.2
0.943
6
0.5
-
0.946
6
0.4
0.1
0.963
18
1.0
-
0.86
18
0.8
0.05
0.886
60
1.0
-
0.83
60
0.8
0.03
0.85
60
1.0
0.03
0.83
180
2.0
-
0.713
180
1.9
0.01
0.726
400
2.0
-
0.623
400
1.95
0.005
0.623"
REFERENCES,0.24907521578298397,Table 5: Clean accuracies of different evaluations of our toy experiment.
REFERENCES,0.25030826140567203,"will shrink rather fast with increasing Ïƒ. The motivation of this example is to show that if the Ïƒ(x)
function is well-designed, our IDRS can outperform the constant RS considerably."
REFERENCES,0.25154130702836003,"The setup of our experiment is as follows: We evaluate dimensions N = 2, 6, 18, 60, 180, 400.
The Ïƒ used for constant smoothing is Ïƒ = 0.5, 0.5, 1.0, 1.0, 2.0, 2.0 respectively. The Ïƒb used is
0.4, 0.5 for N = 2, 0.4 for N = 6, 0.8 for N = 18, 0.8, 1.0 for N = 60, 1.9 for N = 180 and
1.95 for N = 400. The rates are r = 0.2, 0.1, 0.05, 0.03, 0.01, 0.005 respectively. The training
was executed without data augmentation (because samples from different classes are very close to
each other). Moreover, we have set maximal Ïƒ(x) threshold for numerical purposes, because some
samples were outliers and were way too far from other samples (and if the Ïƒ(x) is way too big, the
method encounteres numerical problems). In this case we set Ïƒ(x) â‰¤5Ïƒb, but we are aware that
also much bigger thresholds would have been possible. We present our comparisons in Figure 10
and Table 5."
REFERENCES,0.2527743526510481,"From both the Figure 10 an Table 5 it is clear that the IDRS can outperform the constant Ïƒ RS con-
siderably, if we use really suitable Ïƒ(x) function. We manage to improve signiï¬cantly the certiï¬ed
radiuses without losing a single correct classiï¬cation. On the other hand, in cases where Ïƒb < Ïƒ,
we outperform constant Ïƒ both in clean accuracy and in certiï¬ed radiuses. This example is synthetic
and designed in our favour. The main message is not how perfect our design of Ïƒ(x) is, but the fact,
that if Ïƒ(x) is designed well, the IDRS can bring real advantages, even in moderate dimensions."
REFERENCES,0.25400739827373614,"B
MORE ON THEORY"
REFERENCES,0.25524044389642414,"B.1
GENERALIZATION OF RESULTS FROM LI ET AL. (2019)"
REFERENCES,0.2564734895191122,"In our main text, we mostly focus on the generalization of the methods from Cohen et al. (2019).
This is because these methods yield tight radiuses and because the application of Neyman-Pearson
lemma is beautiful. However, the methodology from Li et al. (2019) can also be generalized for
the input-dependent RS. To be able to do it, we need some auxiliary statements about the RÂ´enyi
divergence."
REFERENCES,0.25770653514180025,Lemma 10. The RÂ´enyi divergence between two one-dimensional normal distributions is as follows:
REFERENCES,0.2589395807644883,"DÎ±(N(Âµ1, Ïƒ2
1)||N(Âµ0, Ïƒ2
0)) = Î±(Âµ1 âˆ’Âµ2)2"
REFERENCES,0.2601726263871763,"2Ïƒ2Î±
+
1
1 âˆ’Î± log

ÏƒÎ±
Ïƒ1âˆ’Î±
1
ÏƒÎ±
0 
,"
REFERENCES,0.26140567200986436,"provided, that Ïƒ2
Î± := (1 âˆ’Î±)Ïƒ2
1 + Î±Ïƒ2
0 â‰¥0."
REFERENCES,0.2626387176325524,Proof. See Van Erven & Harremos (2014).
REFERENCES,0.2638717632552404,Under review as a conference paper at ICLR 2022
REFERENCES,0.2651048088779285,Figure 10: Certiï¬ed accuracy plots of our multidimensional toy experiments.
REFERENCES,0.26633785450061653,"Note, that this proposition induces some assumptions on how Ïƒ0, Ïƒ1, Î± should be related. If Ïƒ0 > Ïƒ1,
then the required inequality holds for any 1 Ì¸= Î± > 0. If Ïƒ0 < Ïƒ1, then Î± is restricted and we need
to keep that in mind."
REFERENCES,0.2675709001233046,"Lemma 11.
Assume,
we have some one-dimensional distributions P1, P1, . . . , PN
and
Q1, Q2, . . . , QN deï¬ned on common space for pairs with the same index. Then, assuming product
space with product Ïƒ-algebra, we have the following identity:"
REFERENCES,0.2688039457459926,"DÎ±(P1 Ã— P2 Ã— Â· Â· Â· Ã— PN||Q1 Ã— Q2 Ã— Â· Â· Â· Ã— QN) = N
X"
REFERENCES,0.27003699136868065,"i=1
DÎ±(Pi||Qi)."
REFERENCES,0.2712700369913687,Under review as a conference paper at ICLR 2022
REFERENCES,0.2725030826140567,Proof. See Van Erven & Harremos (2014).
REFERENCES,0.27373612823674476,"Using these two propositions, we are now able to derive a formula for RÂ´enyi divergence between
two multivariate isotropic normal distributions:"
REFERENCES,0.2749691738594328,Lemma 12.
REFERENCES,0.2762022194821208,"DÎ±(N(x1, Ïƒ2
1I)||N(x0, Ïƒ2
0I)) =
Î±âˆ¥x0 âˆ’x1âˆ¥2"
REFERENCES,0.27743526510480887,"2Ïƒ2
1 + 2Î±(Ïƒ2
0 âˆ’Ïƒ1
1) + N
log

ÏƒÎ± Ïƒ1 "
REFERENCES,0.2786683107274969,"1 âˆ’Î±
âˆ’N
Î±
1 âˆ’Î± log
Ïƒ0 Ïƒ1 
."
REFERENCES,0.279901356350185,"Proof. Imporant property that is needed here is, that isotropic gaussian distributions factorize to
one-dimensinal independent marignals. In other words:"
REFERENCES,0.281134401972873,"N(x1, Ïƒ2
1I) = N(x11, Ïƒ2
1) Ã— N(x12, Ïƒ2
1) Ã— Â· Â· Â· Ã— N(x1N, Ïƒ2
1),"
REFERENCES,0.28236744759556104,"and analogically for x0. Therefore, using Lemma 11 we see:"
REFERENCES,0.2836004932182491,"DÎ±(N(x1, Ïƒ2
1I)||N(x0, Ïƒ2
0I)) = N
X"
REFERENCES,0.2848335388409371,"i=1
DÎ±(N(x1i, Ïƒ2
1)||N(x0i, Ïƒ2
0))."
REFERENCES,0.28606658446362515,"Now, it sufï¬ces to plug in the formula from Proposition 10 to obtain the required result:"
REFERENCES,0.2872996300863132,"DÎ±(N(x1i, Ïƒ2
1I)||N(x0i, Ïƒ2
0I)) = Î±(x1i âˆ’x2i)2"
REFERENCES,0.2885326757090012,"2Ïƒ2Î±
+
1
1 âˆ’Î± log

ÏƒÎ±
Ïƒ1âˆ’Î±
1
ÏƒÎ±
0 "
REFERENCES,0.28976572133168926,"=
Î±(x1i âˆ’x2i)2"
REFERENCES,0.2909987669543773,"2Ïƒ2
1 + 2Î±(Ïƒ2
0 âˆ’Ïƒ1
1) +
log

ÏƒÎ± Ïƒ1 "
REFERENCES,0.2922318125770654,"1 âˆ’Î±
âˆ’
Î±
1 âˆ’Î± log
Ïƒ0 Ïƒ1 "
REFERENCES,0.2934648581997534,Now it sufï¬ces to sum up over i and the result follows.
REFERENCES,0.29469790382244143,"To obtain the certiï¬ed radius, we also need a result from Li et al. (2019), which gives a guarantee
that two measures on the set of classes will share the modus if the Rnyi divergence between them is
small enough."
REFERENCES,0.2959309494451295,"Lemma 13. Let P = (p1, p2, . . . , pK) and Q = (q1, q2, . . . , qK) two discrete measures on C.
Let pA, pB correspond to two biggest probabilities in distribution P. Let M1(a, b) =
a+b"
AND,0.2971639950678175,"2
and
M1âˆ’Î±(a, b) = ( a1âˆ’Î±+b1âˆ’Î±"
AND,0.29839704069050554,"2
)
1
1âˆ’Î± If"
AND,0.2996300863131936,"DÎ±(Q||P) â‰¤âˆ’log(1 âˆ’2M1(pA, pB) + 2M1âˆ’Î±(pA, pB)),"
AND,0.3008631319358816,then the distributions P and Q agree on the class with maximal assigned probability.
AND,0.30209617755856966,"Proof. This lemma can be proved by directly computing the minimal required DÎ± to be able to
disagree on the maximal class probabilities via a constrained optimization problem (with variables
pi, qi, i âˆˆ{1, . . . , K}), solving KKT conditions. For details, consult Li et al. (2019)."
AND,0.3033292231812577,"Having explicit formula for the Rnyi divergence, we can mimic the methodology of Li et al. (2019)
to obtain the certiï¬ed radius:"
AND,0.30456226880394577,"Theorem 14. Given x0, pA, pB, Ïƒ0, N, the certiï¬ed radius squared for all x1 such that ï¬xed Ïƒ1 is
used is:"
AND,0.30579531442663377,"R2 =
sup
Î±âˆˆSÏƒ0,Ïƒ1"
AND,0.3070283600493218,"2Ïƒ2
1 + 2Î±(Ïƒ2
0 âˆ’Ïƒ1
1)
Î± "
AND,0.3082614056720099,"N
Î±
1 âˆ’Î± log
Ïƒ0 Ïƒ1"
AND,0.3094944512946979,"
âˆ’N
log

ÏƒÎ± Ïƒ1  1 âˆ’Î±"
AND,0.31072749691738594,"âˆ’log(1 âˆ’2M1(pA, pB) + 2M1âˆ’Î±(pA, pB)) ! ,"
AND,0.311960542540074,"where SÏƒ0,Ïƒ1 = R+, if Ïƒ0 > Ïƒ1 and SÏƒ0,Ïƒ1 =

0,
Ïƒ2
1
Ïƒ2
1âˆ’Ïƒ2
0"
AND,0.31319358816276205,"i
if Ïƒ0 < Ïƒ1."
AND,0.31442663378545005,Under review as a conference paper at ICLR 2022
AND,0.3156596794081381,"Proof. Let us ï¬x x1 and assume, that Î± âˆˆSÏƒ0,Ïƒ1. Then, due to post-processing inequality for Renyi
divergence, it follows that"
AND,0.31689272503082616,"DÎ±(f(x1 + N(0, Ïƒ2
1I))||f(x0 + N(0, Ïƒ2
0I))) â‰¤DÎ±(x1 + N(0, Ïƒ2
1I)||x0 + N(0, Ïƒ2
0I))"
AND,0.31812577065351416,"=
Î±âˆ¥x0 âˆ’x1âˆ¥2"
AND,0.3193588162762022,"2Ïƒ2
1 + 2Î±(Ïƒ2
0 âˆ’Ïƒ1
1) + N
log

ÏƒÎ± Ïƒ1 "
AND,0.3205918618988903,"1 âˆ’Î±
âˆ’N
Î±
1 âˆ’Î± log
Ïƒ0 Ïƒ1 
."
AND,0.3218249075215783,"Due to Lemma 13, it sufï¬ces that the following inequality holds for some Î± âˆˆSÏƒ0,Ïƒ1:"
AND,0.32305795314426633,Î±âˆ¥x0 âˆ’x1âˆ¥2
AND,0.3242909987669544,"2Ïƒ2
1 + 2Î±(Ïƒ2
0 âˆ’Ïƒ1
1) + N
log

ÏƒÎ± Ïƒ1 "
AND,0.32552404438964244,"1 âˆ’Î±
âˆ’N
Î±
1 âˆ’Î± log
Ïƒ0 Ïƒ1 
â‰¤"
AND,0.32675709001233044,"âˆ’log(1 âˆ’2M1(pA, pB) + 2M1âˆ’Î±(pA, pB))."
AND,0.3279901356350185,This can be rewritten w.r.t. âˆ¥x0 âˆ’x1âˆ¥2:
AND,0.32922318125770655,"âˆ¥x0 âˆ’x1âˆ¥2 â‰¥2Ïƒ2
1 + 2Î±(Ïƒ2
0 âˆ’Ïƒ1
1)
Î± "
AND,0.33045622688039455,"N
Î±
1 âˆ’Î± log
Ïƒ0 Ïƒ1"
AND,0.3316892725030826,"
âˆ’N
log

ÏƒÎ± Ïƒ1  1 âˆ’Î±"
AND,0.33292231812577067,"âˆ’log(1 âˆ’2M1(pA, pB) + 2M1âˆ’Î±(pA, pB)) ! ."
AND,0.33415536374845867,"The resulting certiï¬ed radius squared is now simply obtained by taking maximum over âˆ¥x0 âˆ’x1âˆ¥2
s.t. âˆƒÎ± âˆˆSÏƒ0,Ïƒ1 such that the preceding inequality holds."
AND,0.3353884093711467,"Note, that this theorem is formulated assuming, that except in x0, we use Ïƒ1 everywhere. It would
require some further work to generalize this for general Ïƒ(x) functions, but to demonstrate the next
point, it is not even necessary. Looking at the expression, we can observe that"
AND,0.3366214549938348,"N
Î±
1 âˆ’Î± log
Ïƒ0 Ïƒ1"
AND,0.33785450061652283,"
âˆ’N
log

ÏƒÎ± Ïƒ1  1 âˆ’Î±"
AND,0.33908754623921084,depends highly on N and even for a ratio of Ïƒ0
AND,0.3403205918618989,"Ïƒ1 close to 1, we already obtain very strong negative
values for high dimensions. The expression log(1 âˆ’2M1(pA, pB) + 2M1âˆ’Î±(pA, pB)) is far less
sensitive w.r.t pA and for large dimensions of N it is easily â€œbeatenâ€ by the ï¬rst expression. There-
fore, the higher the dimension N is, the bigger pA or the closer to 1 the Ïƒ0"
AND,0.34155363748458695,"Ïƒ1 has to be in order to
obtain even valid certiï¬ed radius (not to speak about big). This points out that also the method of Li
et al. (2019) suffers from the curse of dimensionality, as we know it must have done. This method
is not useful for big N, because the conditions on pA, Ïƒ0, Ïƒ1 are so extreme, that barely any inputs
would yield a positive certiï¬ed radius. This fact is depicted in the Figure 11."
AND,0.34278668310727495,"The key reason why this happens if done via RÂ´enyi divergences is that while the divergence
DÎ±(N(x1, Ïƒ2
1I)||N(x0, Ïƒ2
0I)) grows independently of dimension as âˆ¥x0 âˆ’x1âˆ¥grows, it drastically
increases for big N even if x1 = x0! This reï¬‚ects the effect, that if Ïƒ0 Ì¸= Ïƒ1, then the more
dimensions we have, the more dissimilar are N(x1, Ïƒ2
1I) and N(x0, Ïƒ2
0I)). We can think of it as a
consequence of standard fact from statistics that the more data we have, the more conï¬dent statistics
against the null hypothesis Ïƒ0 = Ïƒ1 will we get if the null hypothesis is false. Since isotropic normal
distributions can be actually treated as a sample of one-dimensional normal distributions, this is in
accordance with our multivariate distributions setting."
AND,0.344019728729963,"B.2
THE EXPLANATION OF THE CURSE OF DIMENSIONALITY"
AND,0.34525277435265106,"In the Section 2 we show that input-dependent RS suffers from the curse of dimenisonality. Now we
will elaborate a bit more on this phenomenon and try to explain why it occurs. First, it is obvious
from the Subsection B.1, that also the generalized method of Li et al. (2019) suffers from the curse
of dimensionality, because the RÂ´enyi divergence between two isotropic Gaussians with different
variances grows considerably with respect to dimension. This suggests that the input-dependent RS
might suffer from the curse of dimensionality in general. To motivate this idea even further, we
present this easy observation:"
AND,0.34648581997533906,Under review as a conference paper at ICLR 2022
AND,0.3477188655980271,"Figure 11: The certiï¬ed radius as a function of dimension. Paremeters are pA = 0.99, Ïƒ0 = 1, Ïƒ1 =
0.8"
AND,0.34895191122071517,"Theorem 15. Denote RC to be a certiï¬ed radius given for pA and Ïƒ0 at x0 assuming the constant
Ïƒ0 and following the certiï¬cation of Cohen et al. (2019) 1. Assume, that we do the certiï¬cation for
each x1 by assuming the worst case-classiï¬er as in Theorem 2. Then, for any x0, any function Ïƒ(x)
and any pA, the following inequality holds: R â‰¤RC"
AND,0.35018495684340323,"Proof. Fix x1 and Ïƒ1. From Theorem 2 we know that the worst-case classiï¬er f âˆ—deï¬nes a ball B
such that P0(B) = 1 âˆ’pA. From this it obviously follows, that the linear classiï¬er fl and the linear
space Bl that assume constant Ïƒ0 also for x1 and is the worst-case for Ïƒ0 such that P0(Bl) = 1âˆ’pA
is not worst-case for the case of using Ïƒ1 instead. Therefore, P1(Bl) â‰¤P1(B)."
AND,0.35141800246609123,"Moreover, let PC
1 be a probability measure corresponding to N(x1, Ïƒ0I), i.e. the probability mea-
sure assuming constant Ïƒ0. It is easy to see that PC
1 (Bl) > 0.5 â‡â‡’P1(Bl) > 0.5 because the
probability of a linear half-space under isotropic normal distribution is bigger than half if and only
if the mean is contained in the half-space."
AND,0.3526510480887793,"Assume, for contradiction that R > RC. From that, it exists a particular x1 such that PC
1 (Bl) >
0.5 > P1(B), because otherwise there would be no such point, which would cause R > RC.
However, PC
1 (Bl) > 0.5 =â‡’P1(Bl) > 0.5, thus P1(Bl) > P1(B) and that is contradiction."
AND,0.35388409371146734,"This theorem shows, that we can never achieve a better certiï¬ed radius at x0 using Ïƒ0 and having
probability pA than that, which we would get by Cohen et al. (2019)â€™s certiï¬cation. Of course, this
does not mean, that using non-constant Ïƒ is useless, since Ïƒ0 can vary. The question is, how much
do we lose using non-constant Ïƒ. To get a better intuition, we plot the functions Î¾< and Î¾> under
different setups in Figure 12, together with P1(Bl) from the proof of Theorem 15. From the top row
we can deduce that dimension N has a very signiï¬cant impact on the probabilities and therefore also
on the certiï¬ed radius. We particularly point out the fact, that even Î¾>(0), Î¾<(0) can have signiï¬cant
margin w.r.t. to the probability coming out of linear classiï¬er.2 Already for N = 90, we are not able
to certify pA = 0.99 for rather conservative value of Ïƒ0"
AND,0.35511713933415534,"Ïƒ1 . From middle row we see, that decreasing
Ïƒ0
Ïƒ1 can mitigate this effect strongly. For instance, for Ïƒ0 = 1, Ïƒ1 = 0.95 the difference between
P1(B) and P1(Bl) is almost negotiated. Bottom row compares Î¾>(a), Î¾<(a) and the respective
linear classiï¬er probabilities. We can see, that the case Ïƒ0 < Ïƒ1 might cause stronger restrictions on
our certiï¬cation (yet we deduce it just form the picture)."
AND,0.3563501849568434,"1The â€œCâ€ in the subscript of certiï¬ed radius might come both from â€œconstantâ€ and â€œCohen et. al.â€
2Notice the similarity with RÂ´enyi divergence, which also has positive value even for x0 = x1 if Ïƒ0 Ì¸= Ïƒ1
and then grows rather reasonably with distance."
AND,0.35758323057953145,Under review as a conference paper at ICLR 2022
AND,0.3588162762022195,"Figure 12: Plots of Î¾>(a), Î¾<(a) for different setups. Coding for parameters is: [Ïƒ0, Ïƒ1, N, pB] Top:
Î¾>(a) left, Î¾<(a) right, varying values of N. Center: On the left, Î¾>(a) for varying Ïƒ1, on the right
Î¾>(a) for varying pB. Bottom: Î¾>(a) and Î¾<(a) compared."
AND,0.3600493218249075,"What is the reason for Î¾>(a), Î¾<(a) being so big even at 0? The problem is following: Assume
Ïƒ0 > Ïƒ1. If x0 = x1, the worst-case classiï¬er coming from Lemma 2 will be a ball B centered right
at x0, such that P0(B) = 1 âˆ’pA. If we look at P1(B), we see, that we have the same ball centered
directly at the mean, but the variance of the distribution is smaller. Using spherical symmetry of the
isotropic gaussian distribution, this is equivalent to evaluating the probability of a bigger ball. If we
ï¬x Ïƒ0"
AND,0.36128236744759556,Ïƒ1 and look at the ratio of probabilities P1(B)
AND,0.3625154130702836,"P0(B) with increasing N, the curse of dimensionality
comes into the game. For N = 2, the ratio is not too big. However, if N = 3072, like in CIFAR10,
this ratio is far bigger. This can be intuitively seen from a property of chi-square distribution (which
is present in the case x0 = x1), that while expectation is N, the standard deviation is â€œjustâ€
âˆš 2N,"
AND,0.3637484586929716,"i.e.
âˆš"
AND,0.3649815043156597,"V ar(Ï‡2
N)
E(Ï‡2
N)
âˆ’â†’0 as N âˆ’â†’âˆž."
AND,0.36621454993834773,"B.3
WHY DOES THE INPUT-DEPENDENT SMOOTHING WORK BETTER FOR SMALL Ïƒ VALUES?"
AND,0.36744759556103573,"As can be observed in Section 4 and Appendix E, the bigger the Ïƒb = Ïƒ we use, the harder it is to
keep up to standards of constant smoothing. An interesting question is, why is the usage of small
Ïƒb = Ïƒ helpful for the input-dependent smoothing?"
AND,0.3686806411837238,"Assume ï¬xed Ïƒ, say Ïƒb = Ïƒ = 0.12. The theoretical bound on the certiï¬ed radius given 100000
Monte-Carlo samplings and 0.001 conï¬dence level using constant smoothing is about 0.48. Having"
AND,0.36991368680641185,Under review as a conference paper at ICLR 2022
AND,0.3711467324290999,"Figure 13: Comparison of certiï¬ed radius as a function of distance for constant and input-dependent
smoothing. Left: Ïƒb = Ïƒ = 0.12, right: Ïƒb = Ïƒ = 0.50."
AND,0.3723797780517879,"Ïƒ(x) âˆ¼0.12, we cannot expect much bigger certiï¬ed radius. Therefore, if we follow Theorem 7,
the values of exp(âˆ’rR) and exp(rR) in the critical distance âˆ¼0.5 will be much closer to 1, than
the values of exp(âˆ’rR) and exp(rR) if we used Ïƒb = Ïƒ = 0.50 instead, where the critical values
of R could be much bigger than 0.5. Therefore, the â€œgainâ€ in P1(B) imposed by the curse of
dimensionality, compared to P1(B) assuming constant Ïƒ will not be that severe yet. This means,
that the loss in certiï¬ed radius caused by the curse of dimensionality will be much less pronounced
on the â€œactiveâ€ range of certiï¬ed radiuses (those for which the constant smoothing still works),
compared to using big Ïƒb = Ïƒ. To support this idea, we demonstrate it on Figure 13, where we
depict the certiï¬ed radius as a function of distance from decision boundary, assuming f to be a
linear classiï¬er, using Ïƒb = Ïƒ = 0.12 and Ïƒb = Ïƒ = 0.50 for comparison."
AND,0.37361282367447596,"B.4
HOW DOES THE CURSE OF DIMENSIONALITY AFFECT THE TOTAL POSSIBLE
VARIABILITY OF Ïƒ(x)?"
AND,0.374845869297164,"Fix certain type of task, say RGB image classiï¬cation with images of similar object, but consider
many possible resolutions (dimensions N). Given two random images from the test set (x0, x1),
what is the biggest reasonable value of |Ïƒ(x0)/Ïƒ(x1)âˆ’1|? Theoretically, the expression is bounded
by | exp(Â±râˆ¥x0 âˆ’x1âˆ¥) âˆ’1|, given that r is the semi-elasticity constant of Ïƒ(x). However, the
average distance between two samples from a test set of constant size, but increasing dimension
scales as
âˆš"
AND,0.376078914919852,"N. Therefore, with constant r, this upper-bound increases."
AND,0.37731196054254007,"The increasing distance between samples is, therefore, a countereffect to the curse of dimensionality.
In simple words, we have â€œmore distance to change Ïƒ(x0) to Ïƒ(x1)â€. Even if the r decreased just
as 1/
âˆš"
AND,0.3785450061652281,"N, the increasing distances would cancel the effect of the curse of dimensionaity and as a
result, the maximal reasonable value of |Ïƒ(x0)/Ïƒ(x1) âˆ’1| would remain roughly constant w.r.t. N.
However, we need to take into account another effect. As the dimension increases, also the average
distance of samples from the decision boundary increases. This is because the distances in general
grow with dimension and if we assume that the number of intersections of a line segment between
x0 and x1 with the decision boundary of the network remains roughly constant then the average
distance from the decision boundary grows as
âˆš"
AND,0.3797780517879161,"N too. In order to compensate for this, we need to
adjust the basic level of Ïƒ(x) (which we later call Ïƒb and can be understood as the general offset of
our Ïƒ(x)) as
âˆš"
AND,0.3810110974106042,"N too. This is because the maximal attainable certiï¬ed radius given ï¬xed conï¬dence
level Î± and the number of Monte-Carlo samples is a constant multiple of Ïƒ(x)."
AND,0.38224414303329224,"However, with increased Ïƒ, we need to decrease the semi-elasticity rate r in order to obtain full
certiï¬cations (see also Appendix B.3 for intuition behind this)."
AND,0.3834771886559803,"As a sketch of proof, we provide a simple computation, which tells us the approximate asymptotic
behavior of |Ïƒ(x0)/Ïƒ(x1) âˆ’1|. By Theorem 5 it holds:"
AND,0.3847102342786683,"exp(âˆ’rc
âˆš N) â‰¥ s 1 âˆ’2 r"
AND,0.38594327990135635,"âˆ’log(pB) N
,"
AND,0.3871763255240444,Under review as a conference paper at ICLR 2022
AND,0.3884093711467324,"if we want to be able to predict a certiï¬ed radius of c
âˆš"
AND,0.38964241676942046,"N (though this is just a necessary condition.
For sufï¬ciency, the LHS must be much closer to 1). After simple manipulation, we obtain:"
AND,0.3908754623921085,"r â‰¤âˆ’
1
2c
âˆš N
log  1 âˆ’2 r"
AND,0.3921085080147966,âˆ’log(pB) N !
AND,0.3933415536374846,"âˆ¼âˆ’
1
2c
âˆš"
AND,0.39457459926017263,"N
(âˆ’1)2 r"
AND,0.3958076448828607,"âˆ’log(pB) N
= p"
AND,0.3970406905055487,"âˆ’log(pB) cN
."
AND,0.39827373612823674,So the rate scales as 1/N. Now we have:
AND,0.3995067817509248,|Ïƒ(x0)/Ïƒ(x1) âˆ’1| â‰¤| exp(Â±râˆ¥x0 âˆ’x1âˆ¥) âˆ’1| â‰¤exp(râˆ¥x0 âˆ’x1âˆ¥) âˆ’1 â‰¤ exp( p
AND,0.4007398273736128,âˆ’log(pB)
AND,0.40197287299630086,"cN
C
âˆš"
AND,0.4032059186189889,N) âˆ’1 âˆ¼ p
AND,0.40443896424167697,"âˆ’C log(pB) c
âˆš N
."
AND,0.40567200986436497,"B.5
DOES THE CURSE OF DIMENSIONALITY APPLY IN MULTI-CLASS REGIME?"
AND,0.406905055487053,"In the main text, we presented a setup, where pB is set to be 1âˆ’pA. This is equivalent to pretending
that we have just 2 classes. By not estimating the proper value of pB we lose some amount of
power and the resulting certiï¬ed radius is smaller than it could have been, did we have the pB as
well. This is most pronounced for datasets with many classes. The natural question, therefore, is,
whether we could avoid the curse of dimensionality by properly estimating the pB together with pA.
The answer is no. The problem is that the the theory in Section 2 already implicitly works with the
estimate of pB in a form of 1 âˆ’pA. The theory would work also with any other estimate of pB.
Assuming constant pB, instead of constant pA, as we did in Section 2, will, therefore, yield the same
conclusions. Moreover, there is neither theoretical, nor practical reason, why should pB decrease
with increasing dimension."
AND,0.4081381011097411,"This insight even applies to the question of the usage of input-dependent RS in practice. The as-
sumption pB = 1âˆ’pA is no more important in Section 3 than in Section 2. Therefore, we can apply
our method also for the pB obtained directly by Monte-Carlo sampling for the class B (or by any
other estimation method)."
AND,0.4093711467324291,"C
CONCURRENT WORK"
AND,0.41060419235511714,"As we mention in Section 1, the idea to use input-dependent RS is not new. It has popped out in
years 2020 and 2021 in at least four works from three completely distinct groups of authors, even
though none of these works has been successfully published yet. We ï¬nd it necessary to comment
on all of these works because of two orthogonal reasons. First, it is a good practice to compare our
work with the concurrent work to see what are pros and cons of these similar approaches and to what
extend the approaches differ. Second, we are convinced, that three of these four works claim results,
which are not mathematically valid. We ï¬nd this to be a particularly critical problem in a domain
such as certiï¬able robustness, which is by deï¬nition based on rigorous, mathematical certiï¬cations."
AND,0.4118372379778052,"C.1
THE WORK OF WANG ET AL. (2021)"
AND,0.4130702836004932,"This work, submitted for the ICLR conference 2021 is the only work that seems to be mathematically
functional. In this work, authors have two main contributions â€“ ï¬rst, they propose a two-phase
training, where in the second phase, for each sample xi, roughly the optimal Ïƒi is being found and
then this sample xi is being augmented with this Ïƒi as an augmentation standard deviation. Authors
call this method pretrain to ï¬netune. Second, they provide a speciï¬c version of input-dependent
RS. Essentially, they try to overcome the mathematical problems connected to the usage of non-
constant Ïƒ(x) by splitting the input space in so called robust regions Ri, where the constant Ïƒi is
guaranteed to be used. All the certiï¬ed balls are guaranteed to lie within just one of these robust
regions, making sure that within one certiï¬ed region, constant level of Ïƒ is used. Authors test this
method on CIFAR10 and MNIST and show, that the method can outperform existing state-of-the-art
approaches, mainly on the more complex CIFAR10 dataset."
AND,0.41430332922318125,"However, we make several points, which make the results of this work, as well as the proposed
method less impressive:"
AND,0.4155363748458693,Under review as a conference paper at ICLR 2022
AND,0.41676942046855736,"â€¢ The computational complexity of both their train-time and test-time algorithms seems to
be quite high."
AND,0.41800246609124536,"â€¢ The ï¬nal smoothed classiï¬er depends on the order of the incoming samples. As a conse-
quence, it is not clear, whether the method works well for any permutation of the would-
be tested samples. This creates another adversarial attack possibility - to attack the ï¬nal
smoothed classiï¬er by manipulating the test set so that the order of samples is inappropriate
for the good functionality of the ï¬nal smoothed classiï¬er."
AND,0.4192355117139334,"â€¢ Even more, the fact, that the smoothed classiï¬er depends on the order of the would-be
tested samples makes it necessary, that the same smoothed classiï¬er is used all the time
for some test session in a real-world applications. For instance, a camera recognizing faces
to approve an entry to a high-security building would need to keep the same model for its
whole functional life, because restarting the model would enable attackers to create attacks
on the predictions from the previous session. This might lead to signiï¬cant restrictions on
the practical usability of this method."
AND,0.4204685573366215,"C.2
THE WORKS OF ALFARRA ET AL. (2020) AND EIRAS ET AL. (2021)"
AND,0.4217016029593095,"In these works, similarly as in the work of Chen et al. (2021), authors suggest to optimize in each
test point x for such a Ïƒ(x), that maximizes the certiï¬ed radius given by Zhai et al. (2020), which is
an extension of Cohen et al. (2019)â€™s certiï¬ed radius for soft smoothing. The optimization for Ïƒ(x)
differs but is similar in some respect (as will be discussed)."
AND,0.42293464858199753,"Besides, all three works further propose input-dependent training procedure, for which Ïƒ(x) - the
standard deviation of gaussian data augmentation is also optimized. Altogether, both authors claim
strong improvements over all the previous impactful works like Cohen et al. (2019); Zhai et al.
(2020); Salman et al. (2019). The only signiï¬cant difference between the works of Alfarra et al.
(2020) and Eiras et al. (2021) (which have strong author intersections) is that in Eiras et al. (2021),
authors build upon Alfarra et al. (2020)â€™s work and move from the isotropic smoothing to the
smoothing with some speciï¬c anisotropic distributions."
AND,0.4241676942046856,"As mentioned, authors ï¬rst deviate from the setup of Cohen et al. (2019) and turn to the setup
introduced by Zhai et al. (2020), i.e. they use soft smoothed classiï¬er G deï¬ned as"
AND,0.4254007398273736,"GF (x)C = EÎ´âˆ¼N(0,Ïƒ2I)F(x + Î´)C."
AND,0.42663378545006164,"The key property of soft smoothed classiï¬ers is that the Cohen et al. (2019)â€™s result on certiï¬ed
radius holds for them too."
AND,0.4278668310727497,"Theorem 16 (certiï¬ed radius for soft smoothed classiï¬ers). Let G be the soft smoothed probability
predictor. Let x be s.t.
G(x)A â‰¥EA â‰¥EB â‰¥G(x)B."
AND,0.42909987669543775,"Then, the smoothed classiï¬er g is robust at x with radius R = Ïƒ"
AND,0.43033292231812575,"2 (Î¦âˆ’1(EA) âˆ’Î¦âˆ’1(EB)) = Ïƒ Î¦âˆ’1(EA) + Î¦âˆ’1(1 âˆ’EB)) 2
,"
AND,0.4315659679408138,where Î¦âˆ’1 denotes the quantile function of standard normal distribution.
AND,0.43279901356350187,Proof. Is provided in Zhai et al. (2020).
AND,0.43403205918618987,"Note, that it is, similarly as in the hard randomized smoothing version of this theorem, essential to
provide lower and upper conï¬dence bounds for G(x)A and G(x)B, otherwise we cannot use this
theorem with the required probability that the certiï¬ed radius is valid. Denote G(x, Ïƒ) to be the soft
smoothed classiï¬er using Ïƒ in x. Authors propose to use the following theoretical Ïƒ(x) function:"
AND,0.4352651048088779,"Ïƒ(x) = arg max
Ïƒ>0
Ïƒ"
AND,0.436498150431566,"2 (Î¦âˆ’1(G(x, Ïƒ(x))A) âˆ’Î¦âˆ’1(G(x, Ïƒ(x))B)).
(2)"
AND,0.43773119605425403,"It is of course not possible to optimize for this particular function since it is not known. It is also not
feasible to run the Monte-Carlo sampling for each Ïƒ, because that is too costly and moreover due"
AND,0.43896424167694204,Under review as a conference paper at ICLR 2022
AND,0.4401972872996301,"Algorithm 1 Data dependent certiï¬cation (Alfarra et al., 2020)"
AND,0.44143033292231815,"function OPTIMIZESIGMA(F, x, Î², Ïƒ0, M, K):"
AND,0.44266337854500615,"for k = 0, . . . , K do"
AND,0.4438964241676942,"sample Î´1, . . . , Î´M âˆ¼N(0, I)"
AND,0.44512946979038226,"Ï†(Ïƒk) =
1
M M
P"
AND,0.44636251541307026,"i=1
F(x + ÏƒÎ´i)"
AND,0.4475955610357583,"Ë†EA(Ïƒk) = maxC Ï†(Ïƒk)C
Ë†EB(Ïƒk) = maxCÌ¸=A Ï†(Ïƒk)C
R(Ïƒk) = Ïƒk"
AND,0.44882860665844637,"2 (Î¦âˆ’1( Ë†EA(Ïƒk)) âˆ’Î¦âˆ’1( Ë†EB(Ïƒk)))
Ïƒk+1 â†Ïƒk + Î²âˆ‡ÏƒkR(Ïƒk)
Ïƒâˆ—= ÏƒK
return Ïƒâˆ—"
AND,0.45006165228113443,"to stochasticity, it would lead to discontinuous function. Treatment of this problem is probably the
most pronounced difference between the works of Alfarra et al. (2020) and Chen et al. (2021)."
AND,0.45129469790382243,"Alfarra et al. (2020) use the following easy observation: N(0, Ïƒ2I) â‰¡ÏƒN(0, I). Assume we have"
AND,0.4525277435265105,"Î´i, i âˆˆ{1, . . . , M} be i.i.d. sample from N(0, I). Obviously, G(x, Ïƒ(x))A âˆ¼
1
M M
P"
AND,0.45376078914919854,"i=1
F(x + ÏƒÎ´i)A,"
AND,0.45499383477188654,"since this is just the empirical mean of the theoretical expectation. Then, Expression 2 can be
approximated as:"
AND,0.4562268803945746,"Ïƒ(x) = arg max
Ïƒ>0
Ïƒ 2 "
AND,0.45745992601726265,"Î¦âˆ’1
 
1
M M
X"
AND,0.45869297163995065,"i=1
F(x + ÏƒÎ´i)A !"
AND,0.4599260172626387,"âˆ’Î¦âˆ’1
 
1
M M
X"
AND,0.46115906288532676,"i=1
F(x + ÏƒÎ´i)B !! .
(3)"
AND,0.4623921085080148,"Here, M is the number of Monte-Carlo samplings used to approximate this function. Note, that this
function is a random realization of stochastic process in Ïƒ which is driven by the stochasticity in the
sample Î´i, i âˆˆ{1, . . . , M}. To ï¬nd the maximum of this function, authors furhter propose to use
simple gradient ascent, which is possible due to the simple differentiable form of Expression 3. This
differentiability is one of the main motivations to switch from hard to soft randomized smoothing.
Now, we are able to state the exact optimization algorithm of Alfarra et al. (2020):"
AND,0.4636251541307028,"Note, that being done in this way, this algorithm can be viewed as a stochastic gradient ascent. After
obtaining Ïƒâˆ—â‰¡Ïƒ(x), authors further run the Monte-Carlo sampling to estimate the certiï¬ed radius
exactly as in Cohen et al. (2019), but with Ïƒ(x) instead of some global Ïƒ. Using this algorithm,
authors achieve signiï¬cant improvement over the Cohen et al. (2019)â€™s results, particularly getting
rid of the ï¬rst problem mentioned in Appendix A, the truncation issue. For the results, we refer to
Alfarra et al. (2020). We will now give several comments on this algorithm and this method."
AND,0.4648581997533909,"To begin with, in this optimization, authors do not adjust the estimated expectations and therefore
donâ€™t use lower conï¬dence bounds, but rather raw estimates. This is not incorrect, since these
estimates are not used directly for the estimation of certiï¬ed radius, but it is inconsistent with the
resulting estimation. In other words, authors optimize for a slightly different function than they then
use. The difference is, however, not very big apart from extreme values of EA, where the difference
might be really signiï¬cant."
AND,0.46609124537607893,"To overcome slightly this inconsistence, authors further (without comment) use clamping of the
Ë†EA(Ïƒk) and Ë†EB(Ïƒk) on the interval [0.02, 0.98]. I.e. if Ë†EA(Ïƒk) > 0.98, it will be set to 0.98
and this is also taken into account in the computation of gradients. This way, authors get rid of
the inconvenient issue, that if G(x)A âˆ¼1, then Ë†EA(Ïƒk) âˆ¼1 for Ïƒk âˆ¼0, what might cause very
big value of Î¦âˆ’1( Ë†EA(Ïƒk)), yielding strong inconsistency with what would be obtained, if lower
conï¬dence bound was used instead."
AND,0.46732429099876693,"However, the clamping causes even stronger inconsistence in the end. Note, that if G(x)A âˆ¼1, then
the true value of EA(Ïƒk) would be really close to 1, yielding high values of Î¦âˆ’1(EA(Ïƒk)). This
value would be far better approximated by the lower conï¬dence bound than with the clamping, since"
AND,0.468557336621455,Under review as a conference paper at ICLR 2022
AND,0.46979038224414305,"the lower conï¬dence bound of 1 for M = 100000 and Î± = 0.001 is more than 0.9999, while the
clamped value is just 0.98. This makes small values of Ïƒ highly disadvantageous, since Ïƒ"
AND,0.47102342786683105,"2 âˆ’â†’0 as
Ïƒ âˆ’â†’0, yet Î¦âˆ’1( Ë†EA(Ïƒk)) is being stuck on Î¦âˆ’1(0.98). In other words, this way authors artiï¬cially
force the resulting Ïƒ(x) to be big enough, s.t. E(Ïƒ(x))A â‰¤0.98. This assumption is not commented
in the article and might result in intransparent behaviour."
AND,0.4722564734895191,"Second of all, authors use M = 1 for their experiments. This can be interpreted as using batch size
1 in classical SGD. We suppose that this small batch size is suboptimal since it yields an insanely
high variance of the gradient."
AND,0.47348951911220716,"Third of all, during the search for Ïƒ(x), it is not taken into account, whether the prediction is correct
or not. This is, of course, a scientiï¬cally correct approach, since we cannot look at the label of the
test sample before the very ï¬nal evaluation. However, it is also problematic, since the function in
Expression 2 might attain its optimum in such a Ïƒ(x), which leads to misclassiï¬cation. This could
have been avoided if constant Ïƒ was used instead."
AND,0.4747225647348952,"To further illustrate this issue, assume F(x) = 1(B1(0)), i.e. F(x) predicts class 1 if and only
if âˆ¥xâˆ¥â‰¤1, otherwise predicts class 0. Assume we are certifying x â‰¡0 and assume that Ïƒ0 in
Algorithm 1 is initialized such that class 0 is already dominating. Then, we will have positive
gradient âˆ‡ÏƒkR(Ïƒk) in all steps, because F(ÏƒÎ´i) is obviously non-increasing, so the number of
points classiï¬ed as class 1 for ï¬xed sample Î´i, i âˆˆ1, . . . , M is decreasing, yielding Ë†EA(Ïƒk) non-
decreasing in Ïƒk, while Ïƒk"
AND,0.4759556103575832,"2 strictly increasing in Ïƒk. This way, the Ïƒk will diverge to âˆžfor k âˆ’â†’âˆž.
However, point x â‰¡0 is classiï¬ed as class 1, yielding misclassiï¬cation which is, moreover, assigned
very high certiï¬ed radius."
AND,0.47718865598027127,"This issue is actually even more general - the function in Expression 2 does in most cases (assuming
inï¬nite region RN) not possess global maximum, because usually"
AND,0.4784217016029593,"lim
Ïƒâˆ’â†’âˆž
Ïƒ"
AND,0.4796547472256473,"2 (Î¦âˆ’1(G(x, Ïƒ(x))A) âˆ’Î¦âˆ’1(G(x, Ïƒ(x))B)) = âˆž."
AND,0.4808877928483354,"This can be seen, for instance, easily for the F(x) = 1(B1(0)), but it is the case for any hard
classiï¬er, for which one region becomes to have dominating area as the radius around some x0 goes
to inï¬nity. This is because, if some region becomes to be dominating (for instance if all other regions
are bounded), then Ïƒ"
AND,0.48212083847102344,"2 grows, while Î¦âˆ’1(G(x, Ïƒ(x))A) âˆ’Î¦âˆ’1(G(x, Ïƒ(x))B) either grows too, or
stagnates, making the whole function strictly increasing with sufï¬ciently high slope."
AND,0.4833538840937115,"This issue also throws the hyperparameter K under closer inspection. What is the effect of this hy-
perparameter on the performance of the algorithm? From the previous paragraph, it seems, that this
parameter serves not only as the â€œ scaled number of epochsâ€, but also as some stability parameter,
which, however, does not have theoretical, but rather practical justiï¬cation."
AND,0.4845869297163995,"Another issue is, that the function in Expression 2 might be non-convex and might possess many
different local minima, from which not all (or rather just a few) are actually reasonable. Therefore,
the Algorithm 1 is very sensitive to initialization Ïƒ0."
AND,0.48581997533908755,"However, probably the biggest issue of all is connected to the impossibility result showed in Sec-
tion 2, which shows, that the Algorithm 1 actually yields invalid certiï¬ed radiuses. Why it is so?"
AND,0.4870530209617756,"First of all, we must justify, that our impossibility result is applicable also for the soft randomized
smoothing. This is because classiï¬ers of type F(x)C = 1(x âˆˆRC) for RC being decision region for
class C are among applicable classiï¬ers s.t. G(x, Ïƒ)A = EA. With such classiï¬ers, however, there
is no difference between soft and hard smoothing and moreover EA â‰¡pA from our setup. This way
we can construct the worst-case classiï¬ers F âˆ—exactly as in our setup and therefore the same worst-
case classiï¬ers and subsequent adversarial examples are applicable here as well. In other words,
for ï¬xed value of soft smoothed G(x, Ïƒ)A = EA we can denote pA = EA and ï¬nd the worst-case
hard classiï¬er F deï¬ned as indicator of the worst-case ball, which will yield EB â‰¡P1(B) from
Theorem 3 in some queried point x1."
AND,0.4882860665844636,"As we have seen in previous paragraphs, the resulting Ïƒ(x) yielded in Algorithm 1 is very instable
and stochastic - it depends heavily on F, Ïƒ0, K, Î², M and of course Î´i, i âˆˆ{1, . . . , M} for each
iteration of the for cycle. Now, for instance for CIFAR10 and pA = 0.99, we have the minimal
possible ratio Ïƒ0"
AND,0.48951911220715166,"Ïƒ1 equal to more than 0.96. It is hard to believe, that such instable, highly stochastic
and non-regularized (except for K, Î²) method will yield Ïƒ(x) sufï¬ciently slowly varying such that"
AND,0.4907521578298397,Under review as a conference paper at ICLR 2022
AND,0.4919852034525277,"Figure 14: The theoretical certiï¬ed radius as in Expression 2. The function is monotonically in-
creasing on interval [0, 100] and will further be increasing too."
AND,0.4932182490752158,"within the certiï¬ed radius around x0, there will be no x1 for which Ïƒ1 deviates more than by this
strict threshold from Ïƒ0. This is even more pronounced on ImageNet, where the minimal possible
ratio Ïƒ1"
AND,0.49445129469790383,Ïƒ0 is above 0.99 for any pA or EA.
AND,0.4956843403205919,"Even without the help of curse of dimensionality, we can construct a counterexample for which the
algorithm will not yield valid certiï¬ed radius. Assume again F(x) = 1(B1(0)) and assume modest
dimension N = 2. Assume we try to certify point x0 â‰¡[50, 0]. Then, the theoretical Ïƒ-dependent
function from Equation 2 is depicted on Figure 14."
AND,0.4969173859432799,"We can see, that the resulting Ïƒ(x0) will be as big as our regularizers K and Î² in Algorithm 1 will
allow. Therefore, if we run the algorithm for K high-enough, surely the resulting certiï¬ed radius
will be far bigger than 50. However, if we certify the point x0 â‰¡[0, 0] and we start with Ïƒ0 = 0.25,
for instance, then the Ïƒ-dependent certiï¬ed radius in Expression 2 will be decreasing in this Ïƒ0,
yielding Ïƒ(x0) < 0.25, which will result in classiï¬cation of class 1. This point [0, 0] lies within the
â€œcertiï¬edâ€ range of [50, 0], yet it is not classiï¬ed the same, because, obviously, [50, 0] is classiï¬ed
as class 0. This is therefore a counterexample to the validity of Alfarra et al. (2020)â€™s certiï¬cation
method and their results."
AND,0.49815043156596794,"Note that even though our counterexample is a bit â€œextremeâ€ and one could argue that in practice
such a situation would not occur, we must emphasize, that this counterexample is constructed even
without the help of the curse of dimensionality. In practice, it fully sufï¬ces, that for x0 and some
certiï¬ed radius R in x0, there exists x1 within the range of this certiï¬ed radius, s.t. Ïƒ1 is quite
dissimilar to Ïƒ0. If such situation occurs, then R surely is not a valid certiï¬ed radius."
AND,0.499383477188656,"C.3
THE WORK OF CHEN ET AL. (2021)"
AND,0.5006165228113441,"The methodology of Chen et al. (2021) is rather similar to that of Alfarra et al. (2020). The biggest
difference consists in the optimization of Expression 2."
AND,0.5018495684340321,"Instead of stochastic gradient descent, they use more sophisticated version of grid search - so called
multiple-start fast gradient sign search. Simply speaking, this method ï¬rst generates a set of pairs
(Ïƒ0, s)i, i âˆˆ{1, . . . , K} and then for each of the i runs, it runs a j-while cycle, where in each step j,
it increases Ïƒ2
j = Ïƒ2
0 + js to Ïƒ2
j+1 = Ïƒ2
0 + (j + 1)s and checks, whether the Ïƒ-dependent empirical
certiï¬ed radius in Expression 3 increases or not. If yes, they continue until j is above some threshold
T, if not, they break and report Ïƒi as the Ïƒj from the inner step where while cycle was broken. After
obtaining Ïƒi for i âˆˆ{1, . . . , K}, they choose Ïƒ(x) to be the one, that maximizes Expression 3.
More concretely, their multiple-start fast gradient sign search algorithm looks as follows:"
AND,0.5030826140567201,"It is not entirely clear from the text of Chen et al. (2021), how exactly are lk, sk sampled, but it is
written there, that the interval for l is [1, 16] and for s it is (âˆ’1, 1). Moreover, the authors donâ€™t"
AND,0.5043156596794082,Under review as a conference paper at ICLR 2022
AND,0.5055487053020962,"Algorithm 2 Instance-wise multiple-start FGSS (Chen et al., 2021)"
AND,0.5067817509247842,"function OPTIMIZESIGMA(F, x, Ïƒ0, M, K, T):"
AND,0.5080147965474723,"generate (lk, sk), k âˆˆ{1, . . . , K}
for k = 1, . . . , K do"
AND,0.5092478421701603,"sample Î´1, . . . , Î´M âˆ¼N(0, lkÏƒ2
0I)"
AND,0.5104808877928483,"Ï†(âˆšlkÏƒ0) =
1
M M
P"
AND,0.5117139334155364,"i=1
F(x + Î´i)"
AND,0.5129469790382244,"Ë†EA(âˆšlkÏƒ0) = maxC Ï†(âˆšlkÏƒ0)C
R(âˆšlkÏƒ0) = âˆšlkÏƒ0Î¦âˆ’1( Ë†EA(âˆšlkÏƒ0))
mk = R(âˆšlkÏƒ0)
while lk âˆˆ[1, T] do"
AND,0.5141800246609125,"sample Î´1, . . . , Î´M âˆ¼N(0, (lk + sk)Ïƒ2
0I)"
AND,0.5154130702836005,"Ï†(âˆšlk + skÏƒ0) =
1
M M
P"
AND,0.5166461159062885,"i=1
F(x + Î´i)"
AND,0.5178791615289766,"Ë†EA(âˆšlk + skÏƒ0) = maxC Ï†(âˆšlk + skÏƒ0)C
R(âˆšlk + skÏƒ0) = âˆšlk + skÏƒ0Î¦âˆ’1( Ë†EA(âˆšlk + skÏƒ0))
if R(âˆšlk + skÏƒ0) â‰¥R(âˆšlkÏƒ0) then"
AND,0.5191122071516646,"lk â†lk + sk
mk = R(âˆšlk + skÏƒ0)
else"
AND,0.5203452527743526,"break
Ïƒ(x) =
max
kâˆˆ{1,...,K}mk"
AND,0.5215782983970407,return Ïƒ(x)
AND,0.5228113440197287,"provide the code and from the text, it seems, that they donâ€™t use lower conï¬dence bounds during
the evaluation of certiï¬ed radiuses, what we consider to be a serious mistake (if really the case).
However, we add some comments to this method regardless of the lower conï¬dence bounds."
AND,0.5240443896424167,"Generally, this method possesses most of the disadvantages mentioned in Section C.2. They use the
same function for optimization, the Expression 2 and its empirical version 3. This means, that the
method suffers from having several local optima, having no global optimum in general (and in most
cases with limit inï¬nity). Similarly like before, here is also no control over the correctness of the
prediction, i.e. many or all local optima might lead to misclassiï¬cation."
AND,0.5252774352651048,"On the other hand, in this paper authors use M = 500 (the effective batch size), which is deï¬nitely
more reasonable than M = 1 as in Alfarra et al. (2020). Furthermore, they use multiple initializa-
tions, making the optimization more robust and improving the chances to obtain global, or at least
very good local minimum."
AND,0.5265104808877928,"However, the main problem, the curse of dimensionality yielding invalid results is even more pro-
nounced here. Unlike the â€œcontinuous approachâ€ in Alfarra et al. (2020), here authors for each x0
sample just some discrete grid (more complex, since there are more initializations) of possible val-
ues of Ïƒ(x). For instance, if s = 1, then the smallest possible ratio between two consecutive lâ€™s
in the Algorithm 2 is
âˆš"
AND,0.5277435265104808,"15/4 âˆ¼0.97, making it impossible to certify some x1 w.r.t. x0 if for both
s = 1 and l0 Ì¸= l1 on ImageNet and also for a lot of samples on CIFAR10. Of course, the fact that s
is randomly sampled from (âˆ’1, 1) makes this counter-argumentation more difï¬cult, but it is, again,
highly unlikely that this highly stochastic method without control over Ïƒ(x) would yield function
with sufï¬ciently small semi-elasticity. Therefore also the impressive results of Chen et al. (2021)
are, unfortunately, scientiï¬cally invalid."
AND,0.528976572133169,"D
IMPLEMENTATION DETAILS"
AND,0.530209617755857,"Even though our algorithm is rather easy, there are some perks that should be discussed before one
can safely use it in practice. First, we show the actual Algorithm 3"
AND,0.531442663378545,Under review as a conference paper at ICLR 2022
AND,0.5326757090012331,Algorithm 3 Pseudocode for certiï¬cation and prediction of my method based on Cohen et al. (2019)
AND,0.5339087546239211,"# evaluate g at x0
function PREDICT(f, Ïƒ0, x0, n, Î±):"
AND,0.5351418002466092,"counts â†âˆ’SampleUnderNoise(f, x0, n, Ïƒ0)
Ë†cA, Ë†cB â†âˆ’two top indices in counts
nA, nB â†âˆ’counts(cA), counts(Ë†cB)
if BinomPValue(nA, nA + nB, 0.5) â‰¤Î± then return Ë†cA
else return ABSTAIN"
AND,0.5363748458692972,"# certify the robustness of g around x0
function CERTIFY(f, Ïƒ0, x0, n0, n, Î±):"
AND,0.5376078914919852,"counts0 â†âˆ’SampleUnderNoise(f, x0, n0, Ïƒ0)
Ë†cA â†âˆ’top index in counts0
counts â†âˆ’SampleUnderNoise(f, x0, n, Ïƒ0)
pA â†âˆ’LowerConfBound(counts[Ë†cA], n, 1 âˆ’Î±)
if
pA
>
1/2
then
return
prediction
Ë†cA
and radius
ComputeCertiï¬edRadius(Ïƒ0, r, N, pA, num steps)"
AND,0.5388409371146733,else return ABSTAIN
AND,0.5400739827373613,"function COMPUTECERTIFIEDRADIUS(Ïƒ0, r, N, pA, num steps)"
AND,0.5413070283600493,"radiuses â†âˆ’linspace(num space)
for R in radiuses do"
AND,0.5425400739827374,"Ïƒ11 â†âˆ’Ïƒ0 exp(âˆ’rR)
Ïƒ12 â†âˆ’Ïƒ0 exp(rR)
xi bigger â†Î¾>(R, Ïƒ11)
xi lower â†Î¾<(R, Ïƒ12)
if max{xi bigger, xi lower} > 0.5 then BREAK
return R"
AND,0.5437731196054254,"Note that the function ComputeCertifiedRadius is a bit more complicated than depicted in
Algorithm 3. We donâ€™t use a simple for-loop, but rather quite an efï¬cient search method."
AND,0.5450061652281134,"Theoretically speaking, this algorithm works perfectly. However, in practice, it is a bit problem-
atic. The issue is, that since we use Ïƒ11 and Ïƒ12, which are extremely close to Ïƒ0 for small tested
radiuses R, the NCCHSQ functions will get extremely high inputs, making the results numerically
instable. To prevent this, we use a simple trick. Since the more extreme Ïƒ1 will we assume in
evaluation at particular distance R, the worse for us, we can prevent numerical issues simply by
putting Ïƒt < Ïƒ0 and ÏƒT > Ïƒ0 to be maximal and minimal used Ïƒâ€™s in our evaluation, i.e. the
true Ïƒ used will be min{Ïƒt, Ïƒ0 exp(âˆ’rR)} and max{ÏƒT , Ïƒ0 exp(rR)}. This way, we avoid nu-
merical issues, because we can put Ïƒt, ÏƒT to be s.t.
1
Ïƒ2
0âˆ’Ïƒ2
t is not too big and in the same time
maintainting the correct certiï¬cation thanks to the Lemma 6. The problems of this workarounds
are ï¬rst that it decreases the certiï¬cation power, since it assumes Ïƒ1â€™s that are even worse than the
theoretically guaranteed worst-case possibilities and second, more importantly, that it requires some
engineering to design the Ïƒt, ÏƒT designs. It is submoptimal to put one constant value for these
thresholds, because the numerical problems occur at different ratio thresholds of Ïƒ1/Ïƒ0 for different
class probabilities pA and the dimension N. This requires to design a speciï¬c Ïƒt(pA) and ÏƒT (pA)
functions for each dimension N which we want to apply. For instance, we use Ïƒt(pA) = 0.9993 +
0.001 log10(pB), and ÏƒT (pA) = 1/Ïƒt(pA) for CIFAR10, while for MNIST we use Ïƒt(pA) =
0.9988 + 0.001 log10(pB), and ÏƒT (pA) = 1/Ïƒt(pA). To design such functions, one needs to plot
plot real probability of a ball with fixed variances as fcn of dist func-
tion, which computes the Î¾ functions, for particular N and several values of pA and look, whether
it computes correctly. As an example, we provide such a plots for well and ill working setups on
Figure 15."
AND,0.5462392108508015,"Another performance trick is to not evaluate Î¾ for each Ri, where Ri is i-th grid point of evaluation,
but rather evaluate sequentially Ri2, i.e. just every i2-th point, until we reach value > 0.5 and then
to search just the interval [(i âˆ’1)2, i2], where i is the ï¬rst iteration for which Î¾>(Ri2, Ïƒ1) â‰¥0.5."
AND,0.5474722564734895,Under review as a conference paper at ICLR 2022
AND,0.5487053020961775,"Figure 15: Well and ill working function
plot real probability of a ball with fixed variances as fcn of dist,
which computes the Î¾ functions. The coding is [Ïƒ0, Ïƒ1, N, pA]."
AND,0.5499383477188656,"E
MORE TO EXPERIEMNTS AND ABLATIONS"
AND,0.5511713933415536,"Before we present our further results, we must emphasize that our certiï¬cation procedure is barely
any slower than that of Cohen et al. (2019). More speciï¬cally, given 100000 iterations of monte-
carlo sampling, certiï¬cation of one sample using Cohen et al. (2019)â€™s algorithm on CIFAR10 takes
âˆ¼15 seconds on our machine, while certiï¬cation of a sample using our Algorithm 3 takes 15 âˆ’20
seconds depending on the Ïƒb, r setup. If at least one of Ïƒb and r is not small, then our method runs
practically instantly. If both parameters are small, then one evaluation can take up to 5 seconds
depending on the exact value of parameters and on the pA. Note, that this part of the certiï¬cation
is dimension-independent and therefore can run in the same time also on much higher-dimensional
problems."
AND,0.5524044389642416,"Besides the actual certiï¬cation, we have to compute Ïƒ(x) for each of the test examples. This part of
the algorithm is being executed before the actual certiï¬cation and usually takes around 1 minute on
our machine and on CIFAR10."
AND,0.5536374845869297,"All in all, even in the really worst-case scenario, our method runs at most 1/3-times longer than
the old method on CIFAR10. On MNIST, the ratio between our run and the original run is higher,
since MNIST is smaller-dimensional problem. However, since our part of evaluation is practically
independent of the setup (except the values of Ïƒb and r, which, however, can yield just some upper-
bounded amount of slow-down), our algorithm does not bring any added asymptotic time complex-
ity."
AND,0.5548705302096177,"E.1
HOW TO CHOOSE THE HYPERPARAMETERS?"
AND,0.5561035758323057,"Our design of Ïƒ(x) function deï¬ned in Equation 1 uses several hyperparameters. These are: r for
the rate, m for the scaling, Ïƒb for the base sigma and k for the k-nearest neighbors. How do we
choose these hyperparameters?"
AND,0.5573366214549939,"The m parameter depends on our goals. We can set it so that Ïƒ(x) achieves lowest values at Ïƒb by
setting it so that it is roughly equal to the minimal distance from k nearest neighbors across, for
instance, training samples. Other possibility is to set it so that it is roughly equal to the average
distance from k nearest neighbors, to ensure that the average Ïƒ(x) will roughly correspond to the
Ïƒb."
AND,0.5585696670776819,"The k parameter needs to be set with two objectives in mind. Firstly, it would be unwise to set it too
small, because then the distance from k nearest neighbors would be too noisy. On the other hand, we
donâ€™t want it too high, because then it will not be changing fast enough with changing the position of
x. The suitable value can be obtained by looking at histograms of average distances from k nearest
neighbors and choosing the k for which the histogram is enough scattered, but it is not too small."
AND,0.55980271270037,Under review as a conference paper at ICLR 2022
AND,0.561035758323058,"Figure 16: Comparison of certiï¬ed accuracy plots for Cohen et al. (2019) and our work. For each
plot, the same base model f is used for evaluation."
AND,0.562268803945746,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.00
0.831
0.766
0.658
r = 0.005
0.830
0.766
0.654
r = 0.01
0.828
0.762
0.649
r = 0.015
0.826
-
-"
AND,0.5635018495684341,Table 6: Clean accuracies for Cohenâ€™s models and our non-constant Ïƒ(x) models.
AND,0.5647348951911221,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.00
0.084
0.108
0.131
r = 0.005
0.086
0.112
0.135
r = 0.01
0.088
0.119
0.142"
AND,0.5659679408138101,Table 7: Standard deviations of class-wise accuracies for different levels of Ïƒ and r.
AND,0.5672009864364982,"The r parameter needs to be chosen so that we can have some signiï¬cant advantage over constant
smoothing, but it cannot be too big, because otherwise the curse of dimensionality would apply. The
value can be decided either by trial and error, or by plotting the certiï¬ed radius given linear classiï¬er,
or from Theorem 4, setting the rate low-enough so that within the expected certiï¬ed radius range,
the ratio Ïƒ1"
AND,0.5684340320591862,Ïƒ0 canâ€™t move anywhere near the theoretical thresholds implied by Theorem 4.
AND,0.5696670776818742,"The Ïƒb is the base Ïƒ and should be used according to the level of smoothing variance we want to
use. More discussion on this can be found in Cohen et al. (2019)."
AND,0.5709001233045623,"E.2
COMPARISON WITH COHEN ET AL. (2019) METHODOLOGY ON CIFAR10 DATASETS"
AND,0.5721331689272503,"Here, we compare Cohen et al. (2019)â€™s evaluations for Ïƒ = 0.12, 0.25, 0.50 with our evaluations
directly on models trained by Cohen et al. (2019), setting Ïƒb = Ïƒ, r = 0.005, 0.01 and 0.015 for
Ïƒb = Ïƒ = 0.12, k = 20 and m = 5. In this way, the levels of Ïƒ(x) used in direct comparison will
rise from the values roughly equal to Cohen et al. (2019)â€™s constant Ïƒ to higher values. The results
are depicted in Figure 16."
AND,0.5733662145499383,"Note, that this evaluation is being done on the models trained directly by Cohen et al. (2019) and
therefore the variance of Gaussian data augmentation is not entirely consistent with the optimal
variance that should be used for non-constant Ïƒ, which should be either the same, Ïƒ(x) or constant,
but in average equal to Ïƒ(x). The results are similar as in the Section 4. Note, that for Ïƒb =
Ïƒ = 0.50, the curse of dimensionality becomes most pronounced, as explained in Appendix B.
Further, we provide the Tables 6, 7, where the clean accuracies and class-wise standard deviations
are displayed."
AND,0.5745992601726264,"The results are, again, similar as in the section 4."
AND,0.5758323057953144,Under review as a conference paper at ICLR 2022
AND,0.5770653514180024,"Figure 17: Comparison of certiï¬ed accuracy plots for Cohen et al. (2019) and our work, MNIST.
For each plot, the same base model f is used for evaluation. The term trr stands for train-time rate,
will be discussed later and can be ignored now."
AND,0.5782983970406905,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.00
0.9913
0.9910
0.9888
r = 0.005
0.9914
0.9912
0.9885
r = 0.01
0.9914
0.9910
0.9887
r = 0.02
0.9914
0.9912
0.9876
r = 0.05
0.9914
0.9906
0.9836"
AND,0.5795314426633785,Table 8: Clean accuracies for Cohenâ€™s models and our non-constant Ïƒ(x) models on MNIST.
AND,0.5807644882860666,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.00
0.677
0.729
0.909
r = 0.005
0.659
0.735
0.905
r = 0.01
0.659
0.722
0.9318
r = 0.02
0.659
0.713
0.960
r = 0.05
0.715
0.796
1.159"
AND,0.5819975339087546,"Table 9: Standard deviations of class-wise accuracies for different levels of Ïƒ and r. The printed
values are multiples of 100 of the real standard deviations."
AND,0.5832305795314426,"E.3
COMPARISON WITH COHEN ET AL. (2019) METHODOLOGY ON MNIST DATASETS"
AND,0.5844636251541308,"Here, we present similar comparison as in Subsection E.2, but on MNIST and with models trained
by us. Again, the setup is similar as in the Section 4. We compare Ïƒ = Ïƒb = 0.12, 0.25, 0.50 with
test-time rates r = 0.005, 0.01, 0.02, 0.05 and train-time level of Ïƒ again equal to Ïƒ = Ïƒb. It is
important to note, that we use different normalization constant m in the MNIST case. In CIFAR10,
we set m = 5, in MNIST, the suitable m is 1.5. This way we assure, that the smallest Ïƒ(x) values
in the test set will roughly equal the Ïƒb = Ïƒ. The certiï¬ed accuracy plots are depicted on Figure 17.
We also add the clean accuracy table and class-wise clean accuracies standard deviation table (8, 9)."
AND,0.5856966707768188,"All the results are, again, very similar to those presented in Section 4, even though the gain in
certiï¬ed accuracies is marginally worse, since our evaluations run on models trained with in average
smaller train-time data-augmentation standard deviation Ïƒb = Ïƒ."
AND,0.5869297163995068,"E.4
INVESTIGATION OF THE EFFECT OF TRAINING WITH INPUT-DEPENDENT GAUSSIAN
AUGMENATATION"
AND,0.5881627620221949,"It has been shown by many works, that apart from a good test-time certiï¬cation method, also the
appropriate training plays a very important role in the ï¬nal robustness of our smoothed classiï¬er g.
Already Cohen et al. (2019) realize this and propose to train with gaussian data augmentation with
constant Ïƒ. They experiment with different levels of Ïƒ during training and conclude that training
with the same level of Ïƒ that will be later used in the test time is usually the most suitable option."
AND,0.5893958076448829,Under review as a conference paper at ICLR 2022
AND,0.5906288532675709,"Figure 18: The certiï¬ed accuracies of our procedure on CIFAR10 for Ïƒb = 0.12, 0.25, 0.50, rate
r = 0.01 and training rate trr = 0.0, 0.01, 0.04, 0.1."
AND,0.591861898890259,"The question of best-possible training to boost the certiï¬ed robustness didnâ€™t stay without the in-
terest of different researchers. Both Zhai et al. (2020) and Salman et al. (2019) try to improve the
way of training and propose two different, yet interesting and effective training methods. While
Zhai et al. (2020) manage to incorporate the adversarial robustness into the training loss function,
therefore training directly for the robustness, Salman et al. (2019) propose to use adversarial training
to achieve more robust classiï¬ers."
AND,0.593094944512947,"Both Alfarra et al. (2020) and Chen et al. (2021) already propose to use training with input-dependent
Ïƒ as the variance of gaussian data augmentation. Both of them proceed similarly as during test time
- to obtain training Ïƒ(x), they optimize for such, that would maximize the certiï¬ed accuracy of
training samples."
AND,0.594327990135635,"In this section, we propose and test our own training method. We propose to use again gaussian
data augmentation with input-dependent Ïƒ(x), but we suggest to use the simple Ïƒ(x) deï¬ned in
Equation 1. In other words, we suggest using the same Ïƒ(x) during training as during testing (up to
parametrization, which might differ)."
AND,0.5955610357583231,"Note, that, unlike the certiï¬cation, the training procedure does not require any mathematical analysis
nor certiï¬cation. It is totally up to us how we train the base classiï¬er f and the way of training does
not inï¬‚uence the validity of subsequent certiï¬cation guarantees during test time. However, it is good
to have a reasonable training procedure, because otherwise, we would achieve a satisfactory model
neither in terms of clean accuracy nor in terms of adversarial robustness."
AND,0.5967940813810111,"In the subsequent analysis, we evaluate and compare our certiï¬cation procedures on models trained
with different training parametrizations. For this particular section, we run the comparison only on
the CIFAR10 dataset. For each test-time Ïƒb, r, we evaluate our method with these parameters on
base models f trained with the same Ïƒb, but different level of training rate trr. The training rate
trr plays exactly the same role as the evaluation rate r but is used exclusively during training. Note,
that this makes our Ïƒ(x) different during training and testing since it is parametrized with different
rates."
AND,0.5980271270036991,"On the Figure 18 we plot evaluations on CIFAR10 of our method for rate 0.01, all levels of Ïƒb =
0.12, 0.25, 0.50 and each of these test-time setups is evaluated on 4 different levels of train-time rate
trr = 0.0, 0.01, 0.04, 0.1."
AND,0.5992601726263872,"From the results, we judge, that our training procedure works satisfactorily well. It can generally
outperform the constant Ïƒ training, yet the standard accuracy vs. robustness trade-off is present in
some cases. If we train with small train-time rate, the improvement of the certiï¬ed accuracies is
not pronounced (the case for Ïƒb = Ïƒ = 0.50 is slightly misleading, since such a conï¬guration
is just a result of the variance of clean accuracy w.r.t different traning runs) enough, but we also
donâ€™t lose almost any clean accuracy. Increasing the rate to trr = 0.04 results in much more
pronounced improvements in high certiï¬ed accuracies, yet also comes at a prize of clean accuracy
drop, especially for large Ïƒ levels. Even bigger training rate, such as trr = 0.1 seems to be too big
and does not bring almost any improvement over the rate trr = 0.04, yet loses a large amount of
clean accuracy."
AND,0.6004932182490752,"These results suggest, that the input-dependent training with a carefully chosen training rate for Ïƒ(x)
can lead to signiï¬cant improvements in certiï¬able robustness. However, it is important to note, that"
AND,0.6017262638717632,Under review as a conference paper at ICLR 2022
AND,0.6029593094944513,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
trr = 0.00
0.084
0.107
0.153
trr = 0.01
0.078
0.099
0.126
trr = 0.04
0.068
0.081
0.117
trr = 0.1
0.088
0.099
0.230"
AND,0.6041923551171393,"Table 10: Standard deviations of class-wise accuracies for different levels of Ïƒ and trr, under con-
stant rate r = 0.01."
AND,0.6054254007398274,"the optimal trr seems to be dependent on the Ïƒb, therefore for each value of Ïƒb, some effort has to
be invested to ï¬nd the optimal hyperparameters."
AND,0.6066584463625154,"Besides, we were also interested, whether using an input-dependent Ïƒ(x) during training inï¬‚uences
the class-wise accuracy balance. In Table 10 we report the standard deviations of class-wise accura-
cies."
AND,0.6078914919852034,"We can observe, that unlike the pure input-dependent evaluation, the input-dependent training is
partially capable of mitigating the effects of the shrinking. For instance, the trr = 0.04 for Ïƒb =
0.12 provides obvious improvement in establishing class-wise balance. Similarly successful are
trainings with trr = 0.01 for Ïƒb = 0.12 and both trr = 0.01, 0.04 for Ïƒb = 0.25. Also for
Ïƒ = 0.50 the mitigation is present for small-enough training rates. However, we must emphasize,
that if we use too big training rate, the disbalance between class accuracies will be re-established
and in some cases even magniï¬ed. Therefore, we must be careful to choose the appropriate training
rate for the Ïƒb, r."
AND,0.6091245376078915,"E.5
WHY DO WE NOT COMPARE WITH THE CURRENT STATE-OF-THE-ART?"
AND,0.6103575832305795,"Brieï¬‚y speaking â€“ we could, but we donâ€™t consider it necessary. Since we claim just one type of
improvement over the Cohen et al. (2019)â€™s model (experiment-wise) and donâ€™t claim new state-of-
the-art training method, we didnâ€™t ï¬nd it necessary to measure our strengths with methods of Salman
et al. (2019) and Zhai et al. (2020). It is obvious that we would outperform these methods in the
question of certiï¬ed accuracy waterfalls anyway, since these methods focus on the training phase.
Since we do not outperform Cohen et al. (2019) neither in terms of the clean accuracies nor in terms
of class-wise accuracies, it is not our belief that we would outperform the two modern methods in
these metrics. Moreover, we ï¬nd the comparison with Cohen et al. (2019) most structured, since we
extend the theory built by them."
AND,0.6115906288532675,"E.6
ABLATIONS"
AND,0.6128236744759556,"Even though our results so far might look impressive, we canâ€™t claim that it is fully due to our
particular method until we exclude the possibility, that some different effects play an essential role
in the improvement over Cohen et al. (2019)â€™s work."
AND,0.6140567200986436,"To investigate, whether our particular method dominates the contribution to the performance boost,
we conduct several ablation studies - ï¬rst, we study the variance of our evaluations and trainings,
second, we study the effect of input-dependent test-time randomized smoothing, and third, we study
the effect of input-dependent train-time data augmentation."
AND,0.6152897657213316,"E.6.1
VARIANCE OF THE EVALUATION"
AND,0.6165228113440198,"To ï¬nd out, whether there is a signiï¬cant variance in the evaluation of certiï¬ed radiuses, we conduct
a simple experiment - we train a single model on CIFAR10 and evaluate our method on this model
for the very same setup of parameters multiple times. This way, the only present stochasticity is
in the Monte-Carlo sampling, which inï¬‚uences the evaluation of certiï¬ed radiuses. We pick the
parameters as follows: Ïƒb = 0.50, r = 0.01, trr = 0.0, since the Ïƒb = 0.50 turns out to have
biggest variance in the training. The results are depicted in Figure 19."
AND,0.6177558569667078,"From the results, it is obvious that the variance in the evaluation phase is absolutely negotiable.
Therefore, there is no need to run the same evaluation setup more times."
AND,0.6189889025893958,Under review as a conference paper at ICLR 2022
AND,0.6202219482120839,"Figure 19: The variance of evaluation. Parameters are Ïƒb = 0.50, r = 0.01, trr = 0.0, the evaluated
model is the same for all runs. There are 7 runs on CIFAR10."
AND,0.6214549938347719,"Figure 20: The certiï¬ed accuracies of our procedure on CIFAR10 for Ïƒb = 0.12, 0.25, 0.50, rate
r = 0.01 and training rate trr = 0.0 evaluated on 9 different trained models for each of the setups."
AND,0.6226880394574599,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
accuracy
0.61%
0.40%
1.86%
abstention rate
0.17%
0.34%
0.59%
misclassiï¬cation rate
0.60%
0.24%
1.48%"
AND,0.623921085080148,"Table 11: Standard deviations of clean accuracies, abstention rates and misclassiï¬cation rates for 9
runs of each parameter conï¬guration on CIFAR10."
AND,0.625154130702836,"E.6.2
VARIANCE OF THE TRAINING"
AND,0.6263871763255241,"To estimate the variance of the training, we train several models for one speciï¬c training setup
and evaluate them with the same evaluation setup (knowing, that there is no variance in the eval-
uation phase, this is equivalent to measuring directly the training variance). We pick our classical
non-constant Ïƒ(x) for the evaluation, but we train with constant variance data augmentation. The
concrete parameters we work with are: Ïƒb âˆˆ{0.12, 0.25, 0.50}, r = 0.01, trr = 0.0 and we run 9
trainings for each of these parameter conï¬gurations. Then we run full certiï¬cation to not only see
the variance in clean accuracy, but also the variance in the certiï¬ed radiuses. The results are depicted
in Figure 20."
AND,0.6276202219482121,"From the ï¬gures we see, that the variance of the training is strongly Ïƒb-dependent. Most volatile
clean accuracy is present for the case Ïƒb = 0.50. However, fortunately, the biggest variability is
present for the clean accuracy and the curves seem to be less scattered in the areas of high certi-
ï¬ed radiuses. The concrete standard deviations of clean accuracies are in Table 11. The standard
deviations of clean accuracies for MNIST dataset and the same parameters are in Table 12."
AND,0.6288532675709001,Under review as a conference paper at ICLR 2022
AND,0.6300863131935882,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
accuracy
0.036%
0.042%
0.044%
abstention rate
0.037%
0.027%
0.058%
misclassiï¬cation rate
0.043%
0.029%
0.021%"
AND,0.6313193588162762,"Table 12: Standard deviations of clean accuracies, abstention rates and misclassiï¬cation rates for 8
runs of each parameter conï¬guration on MNIST."
AND,0.6325524044389642,"Since the differences in accuracies of different methods are very subtle, it is hard to obtain statisti-
cally trustworthy results. For instance, given, that the standard deviation 0.4% is the true standard
deviation of the Ïƒb = 0.25 runs, we would need 16 runs to decrease it to a standard deviation of
0.1%, which might be considered to be precise-enough. To do the same in the case of Ïƒb = 0.50 on
CIFAR10, we would roughly need 400 runs to decrease the standard deviation below 0.1%. There-
fore, the results we provide in the subsequent subsections, being the average of â€œjustâ€ 8 runs, have
to be taken just modulo variance in the results, which might still be considerable."
AND,0.6337854500616523,"E.6.3
EFFECT OF INPUT-DEPENDENT EVALUATION"
AND,0.6350184956843403,"In this ablation study, we compare the certiï¬cation method for particular Ïƒb, r = 0.01, trr = 0.0
with the constant-Ïƒ certiï¬cation method with CÏƒb, r = 0.0, trr = 0.0, where C is an appropriate
constant. The motivation behind such an experiment is, that our Ïƒ(x) is generally bigger than Ïƒb, but
originally, we compare this method to constant Ïƒ = Ïƒb evaluation. Therefore, in average, samples
in our method enjoy bigger values of Ïƒ(x). Natural question is, whether we cannot obtain the
same performance boost using just the constant Ïƒ method with CÏƒb > Ïƒb set to such value, which
roughly corresponds to the average of Ïƒ(xi) for xi, i âˆˆ{1, . . . , T} being the test set. The problem
of using bigger CÏƒb is, that we encounter performance drop and more severe case of shrinking,
but we need to check, to what extent is the performance drop present in the input-dependent Ïƒ(x)
method. Comparing the performance drops of larger constant CÏƒb and input-dependent Ïƒ(x), which
is in average larger (but in average the same as the CÏƒb), we will be able to answer, to what degree
is the usage of input-dependent Ïƒ(x) really justiï¬ed. If we remind ourselves, that"
AND,0.6362515413070283,"Ïƒ(x) = Ïƒb exp ï£« ï£­r ï£« ï£­1 k ï£« ï£­
X"
AND,0.6374845869297164,"xiâˆˆNk(x)
âˆ¥x âˆ’xiâˆ¥ ï£¶ ï£¸âˆ’m ï£¶ ï£¸ ï£¶ ï£¸,"
AND,0.6387176325524044,"then we see, that the constant C we are searching for is the average (or rather median) value of exp ï£« ï£­r ï£« ï£­1 k ï£« ï£­
X"
AND,0.6399506781750924,"xiâˆˆNk(x)
âˆ¥x âˆ’xiâˆ¥ ï£¶ ï£¸âˆ’m ï£¶ ï£¸ ï£¶ ï£¸."
AND,0.6411837237977805,"Fortunately, empirically, the mean and median of the above expression are roughly equal for both
CIFAR10 and MNIST, so we are not forced to choose between them. For r = 0.01, m = 5,
we choose the rounded value of C = exp(0.05) on CIFAR10. For r = 0.01, m = 1.5 as in
MNIST, the constant is set to C = 1.035. In the end, the values of CÏƒ used in this experiment are
CÏƒ = 0.126, 0.263, 0.53 for CIFAR10 and CÏƒ = 0.124, 0.258, 0.517 for MNIST. To obtain a fair
comparison, though, we evaluate the input-dependent Ïƒ(x) evaluation strategy on models trained
with constant CÏƒb standard deviation of gaussian augmentation. This is because this level of Ïƒ is
equal to the mean value of the Ïƒ(x) and we believe, that such a training data augmentation standard
deviation is more consistent with our Ïƒ(x) function. We provide the plots of single-run evaluations
of certiï¬ed accuracies for CIFAR10 in Figure 21 and for MNIST in Figure 22. The models on
which we evaluate differ because for the increased constant Ïƒ evaluations we needed to also use an
increased level of the training data augmentation variance."
AND,0.6424167694204685,"From the ï¬gures, it is obvious, that our method is not able to outperform the constant Ïƒ method
using the same mean Ïƒ in terms of certiï¬ed accuracy, not even for our strongest Ïƒb = 0.12. This fact
might not be in general bad news, if we demonstrated, that our method suffers from less pronounced
accuracy drop or less pronounced disbalance in class-wise accuracies. To ï¬nd out, we measure"
AND,0.6436498150431565,Under review as a conference paper at ICLR 2022
AND,0.6448828606658447,"Figure 21: The certiï¬ed accuracies of our procedure on CIFAR10 for Ïƒb = 0.12, 0.25, 0.50, rate
r = 0.01 and constant, yet increased CÏƒb training variance, compared to certiï¬ed accuracies of
the constant Ïƒ method for Ïƒ = Ïƒb = 0.12, 0.25, 0.50 and also Ïƒ = CÏƒb = 0.126, 0.265, 0.53.
Evaluated on a single training."
AND,0.6461159062885327,"Figure 22: The certiï¬ed accuracies of our procedure on MNIST for Ïƒb = 0.12, 0.25, 0.50, rate
r = 0.01 and constant, yet increased CÏƒb training variance, compared to certiï¬ed accuracies of
the constant Ïƒ method for Ïƒ = Ïƒb = 0.12, 0.25, 0.50 and also Ïƒ = CÏƒb = 0.124, 0.258, 0.517.
Evaluated on a single training."
AND,0.6473489519112207,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.01, trs increased
0.852
0.780
0.673
r = 0.00 classical
0.851
0.792
0.674
r = 0.00 increased
0.853
0.780
0.673"
AND,0.6485819975339088,"Table 13: Clean accuracies for both input-dependent and constant Ïƒ evaluation strategies on CI-
FAR10."
AND,0.6498150431565968,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.01, trs increased
0.076
0.099
0.120
r = 0.00 classical
0.076
0.097
0.122
r = 0.00 increased
0.076
0.101
0.123"
AND,0.6510480887792849,"Table 14: Class-wise accuracy standard deviations for both input-dependent and constant Ïƒ evalua-
tion strategies on CIFAR10."
AND,0.6522811344019729,"average accuracies of the evaluation strategies from 8 runs for each, as well as average class-wise
accuracy standard deviations from 8 runs. The results are provided in Tables 13 and 14 for CIFAR10
and Tables 15 and 16 for MNIST."
AND,0.6535141800246609,"As for CIFAR10, except for Ïƒ = Ïƒb = 0.25, the differences in accuracies between different evalua-
tion strategies are so small, that we cannot consider them to be statistically signiï¬cant. Even though
the difference for Ïƒ = Ïƒb = 0.25 is high, it is still not possible to draw some deï¬nite conclusions,
especially for the difference between the input-dependent Ïƒ(x) and the increased constant CÏƒb
evaluations. In general, it is not easy to judge, whether our method possesses some advantage (or
disadvantage) over the increased CÏƒb method in terms of clean accuracy. Similar conclusions can
be drawn in the context of the shrinking phenomenon. Here, the differences are also very small, but"
AND,0.654747225647349,Under review as a conference paper at ICLR 2022
AND,0.655980271270037,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.01, trs increased
0.9913
0.9905
0.9885
r = 0.00 classical
0.9914
0.9907
0.9886
r = 0.00 increased
0.9914
0.9904
0.9885"
AND,0.657213316892725,Table 15: Clean accuracies for both input-dependent and constant Ïƒ evaluation strategies on MNIST.
AND,0.6584463625154131,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
r = 0.01, trs increased
0.00757
0.00798
0.00929
r = 0.00 classical
0.00751
0.00778
0.00934
r = 0.00 increased
0.00750
0.00798
0.00925"
AND,0.6596794081381011,"Table 16: Class-wise accuracy standard deviations for both input-dependent and constant Ïƒ evalua-
tion strategies on MNIST. Printed are multiples of 100 of the real values."
AND,0.6609124537607891,"Figure 23: The certiï¬ed radiuses on CIFAR10 of the non-constant Ïƒ(x) method with rate r = 0.01,
but different training strategies. Used training strategies are input-dependent training with the same
Ïƒ(x) function and constant-Ïƒ training with either Ïƒb or CÏƒb variance level. Evaluations are being
done from single run."
AND,0.6621454993834772,"unlike in the comparison with Cohen et al. (2019) models, where we evaluate our input-dependent
Ïƒ(x) method on classiï¬ers trained with inconsistent data-augmentation variance, here we observe
the general trend, that our method is able to outperform the increased constant CÏƒb evaluation. This
is good news and it conï¬rms our suspicion, that the bad results from Subsection E.2 could come
from the train-test Ïƒ inconsistency."
AND,0.6633785450061652,"The results on MNIST suggest similar conclusions for the accuracy vs. robustness tradeoff. Simi-
larly, the Ïƒ = Ïƒb = 0.12, 0.50 are not telling much, and for Ïƒ = Ïƒb = 0.25, the differences are
still rather small (yet the standard deviation of the results should be âˆ¼0.0001, so it is rather on the
edge). The conclusions for the shrinking phenomenon are a bit more pesimistic than in the case of
CIFAR10. Here we donâ€™t see any improvement over the constant Ïƒ, not even the one with increased
Ïƒ level."
AND,0.6646115906288532,"E.6.4
EFFECT OF INPUT-DEPENDENT TRAINING"
AND,0.6658446362515413,"In this last ablation study, we compare our input-dependent data augmentation for particular Ïƒb, r
and particular training rate trr with the constant CÏƒb data augmentation, where the training rate trr
is set to 0. The strategy for choosing the constant C is exactly the same as in the ï¬rst experiment.
Particularly, we evaluate our method with r = 0.01 and Ïƒb = 0.12, 0.25, 0.50, trained with the
same level of Ïƒb and training rate trr = 0.01 with the evaluations using r = 0.01 and Ïƒb =
0.12, 0.25, 0.50 during test time, while during train time using training rate trr = 0.0, but using the
constant Ïƒ = 0.126, 0.263, 0.53 for CIFAR10 and Ïƒ = 0.124, 0.258, 0.517 for MNIST. This way,
we compensate for the â€œincreased levels of Ïƒ(x)â€ with respect to Ïƒb. We present our comparisons
in the Figure 23 for CIFAR10 and 24 for MNIST, providing the evaluations with r = 0.01, Ïƒb =
0.12, 0.25, 0.50 and the same Ïƒb and trr = 0.0 during train time as a reference."
AND,0.6670776818742293,Under review as a conference paper at ICLR 2022
AND,0.6683107274969173,"Figure 24: The certiï¬ed radiuses on MNIST of the non-constant Ïƒ(x) method with rate r = 0.01,
but different training strategies. Used training strategies are input-dependent training with the same
Ïƒ(x) function and constant-Ïƒ training with either Ïƒb or CÏƒb variance level. Evaluations are being
done from single run."
AND,0.6695437731196054,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
trr = 0.01
0.843
0.780
0.671
trr = 0.00 classical
0.849
0.790
0.670
trr = 0.00 increased
0.852
0.780
0.673"
AND,0.6707768187422934,Table 17: Clean accuracies for both input-dependent and constant Ïƒ training strategies on CIFAR10.
AND,0.6720098643649816,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
trr = 0.01
0.080
0.101
0.121
trr = 0.00 classical
0.080
0.105
0.135
trr = 0.00 increased
0.076
0.099
0.120"
AND,0.6732429099876696,"Table 18: Class-wise accuracy standard deviations for both input-dependent and constant Ïƒ training
strategies on CIFAR10."
AND,0.6744759556103576,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
trr = 0.01
0.9912
0.9910
0.9883
trr = 0.00 classical
0.9914
0.9906
0.9884
trr = 0.00 increased
0.9913
0.9905
0.9885"
AND,0.6757090012330457,Table 19: Clean accuracies for both input-dependent and constant Ïƒ training strategies on MNIST.
AND,0.6769420468557337,"Ïƒ = 0.12
Ïƒ = 0.25
Ïƒ = 0.50
trr = 0.01
0.00757
0.00800
0.00947
trr = 0.00 classical
0.00743
0.00789
0.00929
trr = 0.00 increased
0.00757
0.00798
0.00929"
AND,0.6781750924784217,"Table 20: Class-wise accuracy standard deviations for both input-dependent and constant Ïƒ training
strategies on MNIST."
AND,0.6794081381011098,"The certiï¬ed accuracy results for the CIFAR10 and the MNIST differ a bit. For CIFAR10 training
with rate r = 0.01 is does not overperform the constant CÏƒb training. For Ïƒb = 0.12, the constant
CÏƒb training clearly outperforms the input-dependent training. For Ïƒb = 0.25, these two training
strategies seem to have almost identical performances. For Ïƒb = 0.50, the input-dependent Ïƒ(x)
strategy outperforms the constant Ïƒ ones, but now we know, that it is purely due to the variance in
the training. On the other hand on MNIST, we either have very similar performance or even slightly
outperform the constant Ïƒ training."
AND,0.6806411837237978,Under review as a conference paper at ICLR 2022
AND,0.6818742293464858,"Looking at the accuracy and standard deviation Tables 17, 18, 19 and 20, we can deduce the follow-
ing. In terms of clean accuracy, the input-dependent training strategy performs worst in most of the
cases, even though the differences in performance might not be statistically signiï¬cant. We see, that
we would need far more evaluations to see some clear pattern. However, these results are deï¬nitely
not good news for the use of input-dependent Ïƒ(x) during training."
AND,0.6831072749691739,"In terms of the class-wise accuracy standard deviation, we again see countering results for CIFAR10
and MNIST datasets. For CIFAR10 the input-dependent Ïƒ(x) clearly outperforms the smaller con-
stant Ïƒb training method, particularly for Ïƒb = 0.50. However, the constant CÏƒb method seem to
outperform even the input-dependent Ïƒ(x). For MNIST, the smaller constant Ïƒb outperforms both
other methods, while they are rather similar."
AND,0.6843403205918619,"Together with ï¬ndings from previous sections, these results suggest, that usage of this particular
design of input-dependent Ïƒ(x) might not be worthy until a more precise evaluation is conducted.
However, the combination of input-dependent test-time evaluation with constant, yet increased train-
time augmentation is possibly the strongest combination that can be achieved using input-dependent
sigma at all (especially for CIFAR10)."
AND,0.6855733662145499,"F
PROOFS"
AND,0.686806411837238,"Lemma 17 (Neyman-Pearson). Let X, Y be random vectors in RN with densities x, y. Let h :
RN âˆ’â†’{0, 1} be a random or deterministic function. Then, the following two implications hold:"
AND,0.688039457459926,"1. If S =
n
z âˆˆRN : y(z)"
AND,0.689272503082614,"x(z) â‰¤t
o
for some t > 0 and P(h(X) = 1) â‰¥P(X âˆˆS), then"
AND,0.6905055487053021,P(h(Y ) = 1) â‰¥P(Y âˆˆS).
AND,0.6917385943279901,"2. If S =
n
z âˆˆRN : y(z)"
AND,0.6929716399506781,"x(z) â‰¥t
o
for some t > 0 and P(h(X) = 1) â‰¤P(X âˆˆS), then"
AND,0.6942046855733662,P(h(Y ) = 1) â‰¤P(Y âˆˆS).
AND,0.6954377311960542,Proof. See Cohen et al. (2019).
AND,0.6966707768187423,"Lemma 1: Out of all possible classiï¬ers f such that Gf(x)B â‰¤pB = 1 âˆ’pA, the one, for which
Gf(x + Î´)B is maximized is the one, which predicts class B in a region determined by the likelihood
ratio:"
AND,0.6979038224414303,"B =

x âˆˆRN : f1(x)"
AND,0.6991368680641183,"f0(x) â‰¥1 r 
,"
AND,0.7003699136868065,"where r is ï¬xed, such that P0(B) = pB. Note, that we use B to denote both the class and the region
of that class."
AND,0.7016029593094945,"Proof. Let f be arbitrary classiï¬er. To invoke the Neyman-Pearson Lemma 17, deï¬ne h â‰¡f
(with the only difference, that h goes to {0, 1} instead of {A, B}). Moreover, let S â‰¡B and
X âˆ¼N(x, Ïƒ2
0I), Y âˆ¼N(x + Î´, Ïƒ2
1I). Let also f âˆ—classify S as B. Then obviously, P(X âˆˆS) =
P0(B) = pB. Since Gf(x)B â‰¤pB, we have P(h(X) = 1) â‰¤pB. Using directly the second part of
Neyman-Pearson Lemma 17, this will yield P(Y âˆˆS) â‰¥P(h(Y ) = 1). Rewritten in the words of
our setup, Gf âˆ—(x + Î´) â‰¥Gf(x + Î´)."
AND,0.7028360049321825,"Theorem 2: If Ïƒ0 > Ïƒ1, then B is a N-dimensional ball with the center at S> and radius R>:"
AND,0.7040690505548706,"S> = x +
Ïƒ2
0
Ïƒ2
0 âˆ’Ïƒ2
1
Î´, R> = s"
AND,0.7053020961775586,"Ïƒ2
0Ïƒ2
1
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2 + 2N
Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log
Ïƒ0 Ïƒ1"
AND,0.7065351418002466,"
+ 2Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log(r)."
AND,0.7077681874229347,"If Ïƒ0 < Ïƒ1, then B is the complement of a N-dimensional ball with the center at S< and radius R<:"
AND,0.7090012330456227,"S< = x âˆ’
Ïƒ2
0
Ïƒ2
1 âˆ’Ïƒ2
0
Î´, R< = s"
AND,0.7102342786683107,"2Ïƒ4
0 âˆ’Ïƒ2
0Ïƒ2
1
(Ïƒ2
1 âˆ’Ïƒ2
0)2 âˆ¥Î´âˆ¥2 + 2N
Ïƒ2
0Ïƒ2
1
Ïƒ2
1 âˆ’Ïƒ2
0
log
Ïƒ1 Ïƒ0"
AND,0.7114673242909988,"
âˆ’2Ïƒ2
0Ïƒ2
1
Ïƒ2
1 âˆ’Ïƒ2
0
log(r)."
AND,0.7127003699136868,Under review as a conference paper at ICLR 2022
AND,0.7139334155363748,"Proof. From spherical symmetry of isotropic multivariate normal distribution, it follows, that with-
out loss of generality we can take Î´ â‰¡(a, 0, . . . , 0). With little abuse of notation, let a refer to
(a, 0, . . . , 0) as well as âˆ¥(a, 0, . . . , 0)âˆ¥. With this, the B is a set of all x, for which:"
AND,0.7151664611590629,"f1(x)
f0(x) â‰¥1 r â‡â‡’"
AND,0.7163995067817509,"1
(2Ï€)N/2ÏƒN
0
exp  âˆ’1 2Ïƒ2
0 N
X"
AND,0.717632552404439,"i=1
x2
i !"
AND,0.718865598027127,"â‰¤
r
(2Ï€)N/2ÏƒN
1
exp  âˆ’1 2Ïƒ2
1 """
AND,0.720098643649815,"(x1 âˆ’a)2 + N
X"
AND,0.7213316892725031,"i=2
x2
i #! â‡â‡’"
AND,0.7225647348951911,"1
2Ïƒ2
1 "
AND,0.7237977805178791,"(x1 âˆ’a)2 + N
X"
AND,0.7250308261405672,"i=2
x2
i !"
AND,0.7262638717632552,"âˆ’
1
2Ïƒ2
0 N
X"
AND,0.7274969173859432,"i=1
x2
i â‰¤N log
Ïƒ0 Ïƒ1"
AND,0.7287299630086314,"
+ log(r) â‡â‡’"
AND,0.7299630086313194,"(Ïƒ2
0 âˆ’Ïƒ2
1) N
X"
AND,0.7311960542540074,"i=2
x2
i + (Ïƒ2
0 âˆ’Ïƒ2
1)x2
1 âˆ’2Ïƒ2
0x1a + Ïƒ2
0a2 â‰¤2NÏƒ2
0Ïƒ2
1 log
Ïƒ0 Ïƒ1"
AND,0.7324290998766955,"
+ 2Ïƒ2
0Ïƒ2
1 log(r)"
AND,0.7336621454993835,Now assume Ïƒ0 > Ïƒ1 and continue:
AND,0.7348951911220715,"(Ïƒ2
0 âˆ’Ïƒ2
1) N
X"
AND,0.7361282367447596,"i=2
x2
i + (Ïƒ2
0 âˆ’Ïƒ2
1)x2
1 âˆ’2Ïƒ2
0x1a + Ïƒ2
0a2 â‰¤2NÏƒ2
0Ïƒ2
1 log
Ïƒ0 Ïƒ1"
AND,0.7373612823674476,"
+ 2Ïƒ2
0Ïƒ2
1 log(r) â‡â‡’ N
X"
AND,0.7385943279901356,"i=2
x2
i + x2
1 âˆ’
2Ïƒ2
0
Ïƒ2
0 âˆ’Ïƒ2
1
ax1 +
a2Ïƒ2
0
Ïƒ2
0 âˆ’Ïƒ2
1
â‰¤2N
Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log
Ïƒ0 Ïƒ1"
AND,0.7398273736128237,"
+ 2Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log(r) â‡â‡’"
AND,0.7410604192355117,"
x1 âˆ’
Ïƒ2
0
Ïƒ2
0 âˆ’Ïƒ2
1
a
2
+ N
X"
AND,0.7422934648581998,"i=2
x2
i â‰¤
Ïƒ2
0Ïƒ2
1
(Ïƒ2
0 âˆ’Ïƒ2
1)2 a2 + 2N
Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log
Ïƒ0 Ïƒ1"
AND,0.7435265104808878,"
+ 2Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log(r)"
AND,0.7447595561035758,"Such inequality deï¬nes exactly the ball from the statement of the theorem. On the other hand, if
Ïƒ0 < Ïƒ1:"
AND,0.7459926017262639,"(Ïƒ2
1 âˆ’Ïƒ2
0) N
X"
AND,0.7472256473489519,"i=2
x2
i + (Ïƒ2
1 âˆ’Ïƒ2
0)x2
1 + 2Ïƒ2
0x1a âˆ’Ïƒ2
0a2 â‰¥2NÏƒ2
0Ïƒ2
1 log
Ïƒ1 Ïƒ0"
AND,0.7484586929716399,"
âˆ’2Ïƒ2
0Ïƒ2
1 log(r) â‡â‡’ N
X"
AND,0.749691738594328,"i=2
x2
i + x2
1 +
2Ïƒ2
0
Ïƒ2
1 âˆ’Ïƒ2
0
ax1 âˆ’
a2Ïƒ2
0
Ïƒ2
1 âˆ’Ïƒ2
0
â‰¥2N
Ïƒ2
0Ïƒ2
1
Ïƒ2
1 âˆ’Ïƒ2
0
log
Ïƒ1 Ïƒ0"
AND,0.750924784217016,"
âˆ’2Ïƒ2
0Ïƒ2
1
Ïƒ2
1 âˆ’Ïƒ2
0
log(r) â‡â‡’"
AND,0.752157829839704,"
x1 +
Ïƒ2
0
Ïƒ2
1 âˆ’Ïƒ2
0
a
2
+ N
X"
AND,0.7533908754623921,"i=2
x2
i â‰¥2Ïƒ4
0 âˆ’Ïƒ2
0Ïƒ2
1
(Ïƒ2
1 âˆ’Ïƒ2
0)2 a2 + 2N
Ïƒ2
0Ïƒ2
1
Ïƒ2
1 âˆ’Ïƒ2
0
log
Ïƒ1 Ïƒ0"
AND,0.7546239210850801,"
âˆ’2Ïƒ2
0Ïƒ2
1
Ïƒ2
1 âˆ’Ïƒ2
0
log(r)"
AND,0.7558569667077681,This is exactly the complement of a ball from the second part of the statement of the theorem.
AND,0.7570900123304563,Theorem 3:
AND,0.7583230579531443,"P0(B) = Ï‡2
N"
AND,0.7595561035758323,"Ïƒ2
0
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2, R2
<,>
Ïƒ2
0 !"
AND,0.7607891491985204,", P1(B) = Ï‡2
N"
AND,0.7620221948212084,"Ïƒ2
1
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2, R2
<,>
Ïƒ2
1 ! ,"
AND,0.7632552404438965,where the sign < or > is choosed according to the inequality between Ïƒ0 and Ïƒ1.
AND,0.7644882860665845,"Proof. Assume ï¬rst Ïƒ0 > Ïƒ1. Let us shift the coordinates, such that x +
Ïƒ2
0
Ïƒ2
0âˆ’Ïƒ2
1 Î´ â†âˆ’0. Now, the x"
AND,0.7657213316892725,"will have coordinates âˆ’
Ïƒ2
0
Ïƒ2
0âˆ’Ïƒ2
1 Î´. Assume X âˆ¼N(âˆ’
Ïƒ2
0
Ïƒ2
0âˆ’Ïƒ2
1 Î´, Ïƒ2
0I). To obtain"
AND,0.7669543773119606,"P0(B) = P(X âˆˆB) = P

âˆ¥Xâˆ¥2 <
Ïƒ2
0Ïƒ2
1
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2 + 2N
Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log
Ïƒ0 Ïƒ1"
AND,0.7681874229346486,"
+ 2Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log(r)

,"
AND,0.7694204685573366,Under review as a conference paper at ICLR 2022
AND,0.7706535141800247,"we could almost use NCCHSQ, but we donâ€™t have correct scaling of variance of X. However, for
any regular square matrix Q, it follows that P(X âˆˆB) = P(QX âˆˆQB), where QB is interpreted
as set projection. Therefore, if we choose Q â‰¡
1
Ïƒ0 I, we will get"
AND,0.7718865598027127,"P0(B) = P

âˆ¥X/Ïƒ0âˆ¥2 <
Ïƒ2
1
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2 + 2N
Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log
Ïƒ0 Ïƒ1"
AND,0.7731196054254007,"
+
2Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log(r)

."
AND,0.7743526510480888,"Now, since X/Ïƒ0 âˆ¼N(âˆ’
Ïƒ0
Ïƒ2
0âˆ’Ïƒ2
1 Î´, I), we can use the deï¬nition of NCCHSQ to obtain the ï¬nal:"
AND,0.7755856966707768,"P0(B) = Ï‡2
N"
AND,0.7768187422934648,"
Ïƒ2
0
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2,
Ïƒ2
1
(Ïƒ2
0 âˆ’Ïƒ2
1)2 âˆ¥Î´âˆ¥2 + 2N
Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log
Ïƒ0 Ïƒ1"
AND,0.7780517879161529,"
+
2Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
log(r)

."
AND,0.7792848335388409,"To obtain P1(B), we will do similar calculation, yet we need to compute the offset:"
AND,0.7805178791615289,"x +
Ïƒ2
0
Ïƒ2
0 âˆ’Ïƒ2
1
Î´ âˆ’x âˆ’Î´ =
Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
2
a."
AND,0.781750924784217,"Thus, after shifting coordinates in the same way, our alternative X will be distributed like
X âˆ¼N(âˆ’
Ïƒ2
1
Ïƒ2
0âˆ’Ïƒ2
2 a, Ïƒ1I). Now, the same idea as before will yield the required formula."
AND,0.782983970406905,"In the case of Ïƒ0 < Ïƒ1, we do practically the same thing, yet now, we have to keep in mind, that B
will not be a ball, but its complement, therefore we will obtain â€œ1âˆ’â€ in the formulas."
AND,0.7842170160295932,"Lemma 18. Functions Î¾>(a), Î¾<(a) are continuous on the whole R+. Particularly, they are contin-
uous at 0."
AND,0.7854500616522812,"Proof. Assume for simplicity Ïƒ0 > Ïƒ1 and ï¬x x0, whose position is irrelevant and ï¬x x1 such that
âˆ¥x0 âˆ’x1âˆ¥= am, where am is the point, where we prove the continuity. Note, that Ï‡2
N(Î», x) can be
interpreted (as we have seen) as a probability of an offset ball with radius âˆšx and offset
âˆš"
AND,0.7866831072749692,"Î». Assume
we have a sequence {ai}âˆž
i=1 , ai âˆ’â†’am. Deï¬ne xi to be a point lying on the line deï¬ned by x0, x1
s.t. âˆ¥x0 âˆ’xiâˆ¥= ai. deï¬ne Bai to be the worst-case ball corresponding to ai, xi. Now, without
loss of generality, we can assume, that all Bâ€™s are open. We have already seen from Lemma 2, that
centers of Bi converge to the center of Bm. Deï¬ne Xi = 1(Bi), Xm = 1(Bm)."
AND,0.7879161528976573,"First we need to prove, that ri, radiuses of the balls converge. Assume for contradiction, that ri
do not converge. Without loss of gererality, let rs = lim sup
iâˆ’â†’âˆž
ri > rm. If rs = âˆž, it is trivial to"
AND,0.7891491985203453,"see, that P0(Bi) Ì¸= pB for some i for which ri is too big. If rs < âˆž, consider {aik}âˆž
k=0 to be the
subsequence for which rs is monotonically attained. Deï¬ne Bs the ball with center x1 and radius
rs and Xs = 1(Bs). Then, Xik
a.s.
âˆ’â†’Xs for k âˆ’â†’âˆžand from dominated convergence theorem,
P0(Bik) âˆ’â†’P0(Bs). However, P0(Bs) > P0(Bm) = pB, what is contradiction, since obviously
P0(Bik) Ì¸= pB for some k."
AND,0.7903822441430333,"Since Ïƒ1 is ï¬xed, the P1i, probability measures corresponding to N(xi, Ïƒ1I) are actually the same
probability measure up to a shift. Therefore, P1i(Bi) can be treated as P2( Â¯Bi), where P2 is simply
measure corresponding to N(0, Ïƒ1I) and Â¯Bi is simply Bi shifted accordingly s.t. P1i(Bi) = P2( Â¯Bi)
(and assume, without loss of generality, that for each i, they are shifted such that their centers lie
on a ï¬xed line). Now, since we know, that â€œpositionâ€ of both the centers of the balls and the x1 is
continuous w.r.t a, as can be seen from Lemma 2 (and the radiuses are still ri and converge), we see,
that even 1( Â¯Bi) âˆ’â†’1( Â¯Bm) almost surely. Now, we can simply use dominated convergence theorem
using P2 to obtain P2( Â¯Bi) âˆ’â†’P2( Â¯Bm) and thus P1i(Bi) âˆ’â†’P1m(Bm), what we wanted to prove."
AND,0.7916152897657214,"Note that the proof for the case Ïƒ0 < Ïƒ1 is fully analogous, yet instead of class B probabilities, we
work with class A probabilities to still work with balls and not with less convenient complements."
AND,0.7928483353884094,"Lemma 19. If Î»1 > Î»2, then Ï‡2
N(Î»2
1, x2) â‰¤Ï‡2
N(Î»2
2, x2)."
AND,0.7940813810110974,"Proof. Let us ï¬x N(0, I) and respective measure P and respective density f. From symmetry, the
NCCHCSQ deï¬ned as distribution of âˆ¥Xâˆ¥2 for an offset normal distribution can be as well deï¬ned
as âˆ¥X âˆ’sâˆ¥2 under centralized normal distribution. Deï¬ne B1 a ball with center at (Î»1, 0, . . . , 0)
and radius x and B2 a ball with center at (Î»2, 0, . . . , 0) and radius x. Denote C(B) as the center"
AND,0.7953144266337855,Under review as a conference paper at ICLR 2022
AND,0.7965474722564735,"of a ball B. From deï¬nition of NCCHSQ it now follows, that P(Bi) = Ï‡2
N(Î»2
i , x2), i âˆˆ{1, 2}.
Therefore, it sufï¬ces to show P(B1) â‰¤P(B2)."
AND,0.7977805178791615,Deï¬ne D1 = B1\B2 and D2 = B2\B1. Then we know:
AND,0.7990135635018496,"P(B1) =
Z B1"
AND,0.8002466091245376,"f(z)dz =
Z B1âˆ©B2"
AND,0.8014796547472256,"f(z)dz +
Z B1\B2"
AND,0.8027127003699137,"f(z)dz =
Z B1âˆ©B2"
AND,0.8039457459926017,"f(z)dz +
Z D1"
AND,0.8051787916152897,f(z)dz
AND,0.8064118372379778,"P(B2) =
Z B2"
AND,0.8076448828606658,"f(z)dz =
Z B2âˆ©B1"
AND,0.8088779284833539,"f(z)dz +
Z B2\B1"
AND,0.8101109741060419,"f(z)dz =
Z B2âˆ©B1"
AND,0.8113440197287299,"f(z)dz +
Z D2"
AND,0.812577065351418,"f(z)dz. Thus,"
AND,0.813810110974106,"P(B1) â‰¤P(B2) â‡â‡’
Z D1"
AND,0.815043156596794,"f(z)dz â‰¤
Z D2"
AND,0.8162762022194822,f(z)dz.
AND,0.8175092478421702,Let S = C(B1)+C(B2)
AND,0.8187422934648582,"2
. Deï¬ne a central symmetry M with center S. Let z1 âˆˆD1. Then z1 can be
decomposed as z1 = C(B1) + d, âˆ¥dâˆ¥â‰¤x. Then, z2 := M(z1) = C(B2) âˆ’d from symmetry. This
way, we see, that D1 = M(D2) and D2 = M(D1) under a bijection M which does not distort the
geometry and distances of the euclidean space. Therefore, it sufï¬ces to show:"
AND,0.8199753390875463,"âˆ€z âˆˆD2 : f(z) â‰¥f(M(z)), M(z) âˆˆD1."
AND,0.8212083847102343,"From the monotonicity of f(y) w.r.t âˆ¥yâˆ¥it actually sufï¬ces to show âˆ¥zâˆ¥â‰¤âˆ¥M(z)âˆ¥âˆ€z âˆˆD2. Fix
some z âˆˆD2. By the fact that M is central symmetry and z âˆ’â†’M(z), it is obvious, that z = S + p,
M(z) = S âˆ’p, where p is some vector. Now, using law of cosine, we can write:"
AND,0.8224414303329223,"âˆ¥zâˆ¥2 = âˆ¥Sâˆ¥2 + âˆ¥pâˆ¥2 âˆ’2âˆ¥Sâˆ¥âˆ¥pâˆ¥cos(Î±),"
AND,0.8236744759556104,where Î± is angle between S and âˆ’p. On the other hand:
AND,0.8249075215782984,âˆ¥M(z)âˆ¥2 = âˆ¥Sâˆ¥2 + âˆ¥âˆ’pâˆ¥2 âˆ’2âˆ¥Sâˆ¥âˆ¥âˆ’pâˆ¥cos(Ï€ âˆ’Î±).
AND,0.8261405672009864,"It is obvious from these equations, that"
AND,0.8273736128236745,âˆ¥zâˆ¥â‰¤âˆ¥M(z)âˆ¥â‡â‡’Î± â‰¤Ï€/2 â‡â‡’pT S â‰¤0.
AND,0.8286066584463625,"Here, the crucial observation is, that D1 and D2 are separated by a hyperplane perpendicular to S
(vector), such that S (point) is in this hyperplane. From this it follows:"
AND,0.8298397040690506,"y âˆˆD2 =â‡’yT S â‰¤âˆ¥Sâˆ¥2, y âˆˆD1 =â‡’yT S â‰¥âˆ¥Sâˆ¥2."
AND,0.8310727496917386,"Now, since z = S +p and z âˆˆD2, this implies âˆ¥Sâˆ¥2 â‰¥zT S = âˆ¥Sâˆ¥2 +pT S and thus pT S â‰¤0."
AND,0.8323057953144266,"Lemma 20. Functions Î¾>(a), Î¾<(a) are non-decreasing in a."
AND,0.8335388409371147,"Proof. First assume Ïƒ0 > Ïƒ1 and analyse Î¾>(a). From Lemma 18, we can without loss of generality
assume, that a > 0, since Î¾>(0) is simply the limit for a âˆ’â†’0 and cannot change the monotonicity
status."
AND,0.8347718865598027,"Now, ï¬x a > 0 and deï¬ne x0, x1 s.t. âˆ¥x0 âˆ’x1âˆ¥= a. Denote Ba to be the worst-case ball
corresponding to x0, x1 and P1 as usual. Choose Ïµ <
Ïƒ2
1
Ïƒ2
0âˆ’Ïƒ2
1 a, which is the distance between x1 and
C(Ba), as can be seen from Lemma 2."
AND,0.8360049321824907,"Now, assume x2 lies on line deï¬ned by x0, x1 with âˆ¥x0âˆ’x2âˆ¥= a+Ïµ. Let Ba+Ïµ be the corresponding
worst-case ball and P2 as usual. First observe, that P2(Ba) â‰¥P1(Ba), since âˆ¥x1 âˆ’C(Ba)âˆ¥>
âˆ¥x2 âˆ’C(Ba)âˆ¥. Here we use Ï‡2
N(Î»1, x) â‰¤Ï‡2
N(Î»2, x) if Î»1 > Î»2, what is proved in Lemma 19.
Second, note, that since Ba+Ïµ is the worst-case ball for x2, it follows P2(Ba+Ïµ) â‰¥P2(Ba). Thus,
P1(Ba) â‰¤P2(Ba+Ïµ), but that is exactly Î¾>(a) â‰¤Î¾>(a + Ïµ)."
AND,0.8372379778051788,"To prove Î¾>(a) â‰¤Î¾>(a + Ïµ) also for Ïµ >
Ïƒ2
1
Ïƒ2
0âˆ’Ïƒ2
1 a, it sufï¬ces to consider ï¬nite sequence of points
ai starting at a and ending at a + Ïµ that are â€œclose enough to each otherâ€ such that the respective Ïµi
that codes the shift ai âˆ’â†’ai+1 satisfy Ïµi <
Ïƒ2
1
Ïƒ2
0âˆ’Ïƒ2
1 ai."
AND,0.8384710234278668,Under review as a conference paper at ICLR 2022
AND,0.8397040690505548,"Now, assume Ïƒ0 < Ïƒ1 and analysie Î¾<(a). The proof is similar, but we have to be a bit careful about
some details. Again, ï¬x a > 0, deï¬ne x0, x1 accordingly and all other objects as before, except
now, let us denote Aa = BC
a and Aa+Ïµ = BC
a to be the class A balls which are complements to the
anti-balls B. Again, P2(Aa) â‰¤P1(Aa), since âˆ¥x1 âˆ’C(Aa)âˆ¥< âˆ¥x2 âˆ’C(Aa)âˆ¥. We again used the
monotonicity from Lemma 19. Moreover, P2(Aa+Ïµ) â‰¤P2(Aa), since Ba+Ïµ is the worst-case set
for x2. Therefore, P1(Aa) â‰¥P2(Aa+Ïµ), but after reverting to Bâ€™s, it follows Î¾>(a) â‰¤Î¾>(a + Ïµ)."
AND,0.840937114673243,"Here, we donâ€™t even need to care about âˆ¥Ïµâˆ¥, since the centers of Aâ€™s are on the opposite half-lines
from x0 than x1 and x2."
AND,0.842170160295931,"To prove the main theorem, we need a simple bound on a median of central chi-squared distribution,
shown in Robert (1990) in a more general way.
Lemma 21. For all c â‰¥0,"
AND,0.843403205918619,"N âˆ’1 + c â‰¤Ï‡2
N,qf(c, 0.5) â‰¤Ï‡2
N,qf(0.5) + c."
AND,0.8446362515413071,Proof. See Robert (1990).
AND,0.8458692971639951,"Theorem 4 (the curse of dimensionality): Let x0, x1, pA, Ïƒ0, Ïƒ1, N be as usual. Then, the follow-
ing two implications hold:"
AND,0.8471023427866831,1. If Ïƒ0 > Ïƒ1 and
AND,0.8483353884093712,"log
Ïƒ2
1
Ïƒ2
0"
AND,0.8495684340320592,"
+ 1 âˆ’Ïƒ2
1
Ïƒ2
0
< 2 log(1 âˆ’pA) N
,"
AND,0.8508014796547472,"then x1 is not certiï¬ed w.r.t. x0.
2. If Ïƒ0 < Ïƒ1 and"
AND,0.8520345252774353,"log
Ïƒ2
1
Ïƒ2
0 N âˆ’1 N"
AND,0.8532675709001233,"
+ 1 âˆ’Ïƒ2
1
Ïƒ2
0 N âˆ’1"
AND,0.8545006165228114,"N
< 2 log(1 âˆ’pA) N
,"
AND,0.8557336621454994,then x1 is not certiï¬ed w.r.t. x0.
AND,0.8569667077681874,"Proof. We will ï¬rst prove ï¬rst statement, thus let us assume Ïƒ0 > Ïƒ1. Then P1(B) = Î¾>(âˆ¥x0 âˆ’
x1âˆ¥). From monotonicity of Î¾ showed in Lemma 20, we know Î¾>(âˆ¥x0 âˆ’x1âˆ¥) â‰¥Î¾>(0). We will
show Î¾>(0) > 0.5. We have, using deï¬nition of Î¾> plugging in a = 0:"
AND,0.8581997533908755,"Î¾>(0) = Ï‡2
N"
AND,0.8594327990135635,"Ïƒ2
0
Ïƒ2
1
Ï‡2
N,qf(1 âˆ’pA)

."
AND,0.8606658446362515,"Note, that here, we work with central chi-square cdf and quantile function. In order to show Î¾>(0) >
0.5, it sufï¬ces to show
Ïƒ2
0
Ïƒ2
1
Ï‡2
N,qf(1 âˆ’pA) â‰¥N,"
AND,0.8618988902589396,"because it is well-known, that median of central chi-square distribution is smaller than mean, which
is N, i.e. from strict monotonicity of cdf, we will get Ï‡2
N(N) > 0.5. To show the above inequal-
ity, we will use Chernoff bound on chi-squared, which states the following: If 0 < z < 1, then
Ï‡2
N(zN) â‰¤(z exp(1 âˆ’z))N/2. Putting z â‰¡Ïƒ2
1
Ïƒ2
0 , using chernoff bound we get: Ï‡2
N"
AND,0.8631319358816276,"Ïƒ2
1
Ïƒ2
0
N

â‰¤
Ïƒ2
1
Ïƒ2
0
exp

1 âˆ’Ïƒ2
1
Ïƒ2
0  N"
AND,0.8643649815043156,"2
!< 1 âˆ’pA."
AND,0.8655980271270037,"The last inequality is required to hold. If it holds, then necessarily Ï‡2
N,qf(1 âˆ’pA) > Ïƒ2
1
Ïƒ2
0 N and thus"
AND,0.8668310727496917,"Ïƒ2
0
Ïƒ2
1 Ï‡2
N,qf(1 âˆ’pA) > N. Manipulating the required inequality, we will get exactly"
AND,0.8680641183723797,"log
Ïƒ2
1
Ïƒ2
0"
AND,0.8692971639950678,"
+ 1 âˆ’Ïƒ2
1
Ïƒ2
0
< 2 log(1 âˆ’pA) N
,"
AND,0.8705302096177558,what is the assumption of 1.
AND,0.8717632552404438,Under review as a conference paper at ICLR 2022
AND,0.872996300863132,"Similarly we will also prove the statement 2. Assume Ïƒ0 > Ïƒ1. Like in part 1, we will just prove
Î¾<(0) > 0.5 by using chernoff bound. This time, however, we have:"
AND,0.87422934648582,"Î¾<(0) = 1 âˆ’Ï‡2
N"
AND,0.8754623921085081,"Ïƒ2
0
Ïƒ2
1
Ï‡2
N,qf(pA)

,"
AND,0.8766954377311961,"i.e. we need to prove Ï‡2
N"
AND,0.8779284833538841,"Ïƒ2
0
Ïƒ2
1
Ï‡2
N,qf(pA)

< 1 2."
AND,0.8791615289765722,"The second part of Chernoff bound states: If 1 < z, then Ï‡2
N(zN) â‰¥1 âˆ’(z exp(1 âˆ’z))N/2. Let us"
AND,0.8803945745992602,"choose z â‰¡Ïƒ2
1
Ïƒ2
0
Nâˆ’1"
AND,0.8816276202219482,"N . Then Chernoff bound yields: Ï‡2
N"
AND,0.8828606658446363,"Ïƒ2
1
Ïƒ2
0 N âˆ’1"
AND,0.8840937114673243,"N
N

â‰¥1 âˆ’
Ïƒ2
1
Ïƒ2
0 N âˆ’1"
AND,0.8853267570900123,"N
exp

1 âˆ’Ïƒ2
1
Ïƒ2
0 N âˆ’1 N"
AND,0.8865598027127004,"
!> pA."
AND,0.8877928483353884,"If this holds, then"
AND,0.8890258939580764,"Ïƒ2
1
Ïƒ2
0 N âˆ’1"
AND,0.8902589395807645,"N
N > Ï‡2
N,qf(pA) â‡â‡’Ïƒ2
0
Ïƒ2
1
Ï‡2
N,qf(pA) < N âˆ’1."
AND,0.8914919852034525,"Now, using Lemma 21 for the easy case of central chi-squared, we see: Ï‡2
N(N âˆ’1) < 0.5 and thus Ï‡2
N"
AND,0.8927250308261405,"Ïƒ2
0
Ïƒ2
1
Ï‡2
N,qf(pA)

< 1 2,"
AND,0.8939580764488286,what we wanted to prove.
AND,0.8951911220715166,"Corollary 5 (one-sided simpler bound): Let x0, x1, pA, Ïƒ0, Ïƒ1, N be as usual and assume now
Ïƒ0 > Ïƒ1. Then, if"
AND,0.8964241676942046,"Ïƒ1
Ïƒ0
< s 1 âˆ’2 r"
AND,0.8976572133168927,"âˆ’log(1 âˆ’pA) N
,"
AND,0.8988902589395807,then x1 is not certiï¬ed w.r.t x0.
AND,0.9001233045622689,Proof. We will simply prove
AND,0.9013563501849569,"Ïƒ1
Ïƒ0
< s 1 âˆ’2 r"
AND,0.9025893958076449,âˆ’log(1 âˆ’pA)
AND,0.903822441430333,"N
=â‡’log
Ïƒ2
1
Ïƒ2
0"
AND,0.905055487053021,"
+ 1 âˆ’Ïƒ2
1
Ïƒ2
0
< 2 log(1 âˆ’pA) N
."
AND,0.906288532675709,"Assume expression log(1âˆ’y)+y; 1 > y > 0. From Taylor series, it is apparent, that log(1âˆ’y)+y <
âˆ’y2"
AND,0.9075215782983971,"2 . Therefore, if âˆ’y2"
AND,0.9087546239210851,2 < 2 log(1âˆ’pA)
AND,0.9099876695437731,"N
, then also log(1 âˆ’y) + y < 2 log(1âˆ’pA)"
AND,0.9112207151664612,"N
. Solving for y in the"
AND,0.9124537607891492,"ï¬rst inequality, we get sufï¬cient condition y > 2
q"
AND,0.9136868064118372,âˆ’log(1âˆ’pA)
AND,0.9149198520345253,"N
. Plugging 1 âˆ’Ïƒ2
1
Ïƒ2
0 into y we get:"
AND,0.9161528976572133,"1 âˆ’Ïƒ2
1
Ïƒ2
0
> 2 r"
AND,0.9173859432799013,"âˆ’log(1 âˆ’pA) N
,"
AND,0.9186189889025894,which is very easily manipulated to the inequality from theorem statement.
AND,0.9198520345252774,"Theorem 6: Let x0, x1, pA, Ïƒ0 be as usual and let âˆ¥x0 âˆ’x1âˆ¥= R. Then, the following two
statements hold:"
AND,0.9210850801479655,"1. Let Ïƒ1 â‰¤Ïƒ0. Then, for all Ïƒ2 : Ïƒ1 â‰¤Ïƒ2 â‰¤Ïƒ0, if Î¾>(R, Ïƒ2) > 0.5, then Î¾>(R, Ïƒ1) > 0.5."
AND,0.9223181257706535,"2. Let Ïƒ1 â‰¥Ïƒ0. Then, for all Ïƒ2 : Ïƒ1 â‰¥Ïƒ2 â‰¥Ïƒ0, if Î¾<(R, Ïƒ2) > 0.5, then Î¾>(R, Ïƒ1) > 0.5."
AND,0.9235511713933415,Under review as a conference paper at ICLR 2022
AND,0.9247842170160296,"Proof. We will ï¬rst prove the ï¬rst statement. Denote, as usual in the proofs Bi the worst-case ball
for Ïƒi, Pi the probability associated to N(x1, Ïƒ2
i I). Since Î¾>(R, Ïƒ2) > 0.5 and since it is essentially
P2(B2), we see, that the probability of a ball under normal distribution is bigger than half. This is
obviously possible just if x1 âˆˆB2. From the fact, that B1 is the worst-case ball for Ïƒ1 we see
Î¾>(R, Ïƒ1) = P1(B1) â‰¥P1(B2). It sufï¬ces to show P1(B2) â‰¥P2(B2)."
AND,0.9260172626387176,"This follows, since Ïƒ1 â‰¤Ïƒ2 and x1 âˆˆB2. We know, that we can rescale the space such that"
AND,0.9272503082614056,P1(B2) = P2 Ïƒ2
AND,0.9284833538840938,"Ïƒ1
(B2 âˆ’x1) + x1 
,"
AND,0.9297163995067818,using the fact that Ïƒ just scales the normal distribution. The set Ïƒ2
AND,0.9309494451294698,"Ïƒ1 (B2 âˆ’x1) + x1 is just an image
of B2 via homothety with center x1 and rate Ïƒ2"
AND,0.9321824907521579,Ïƒ1 . So it sufï¬ces to prove P2 Ïƒ2
AND,0.9334155363748459,"Ïƒ1
(B2 âˆ’x1) + x1"
AND,0.9346485819975339,"
â‰¥P2(B2)."
AND,0.935881627620222,"However, obviously Ïƒ2"
AND,0.93711467324291,"Ïƒ1 (B2 âˆ’x1) + x1 âŠƒB2 from convexity of a ball. If, namely, x1 + z âˆˆB2,
then from convexity also x1 + Ïƒ1"
AND,0.938347718865598,"Ïƒ2 z âˆˆB2 and this maps back to x1 + z, thus x1 + z is in an image.
Applying monotonicity of P, we obtain the result."
AND,0.9395807644882861,"Now we will prove the second statement and as usual, let A1, A2, P1, P2 be as usual (A is now the
ball connected to class A). As always, P1(A1) â‰¤P1(A2), so it sufï¬ces to show P2(A2) < 0.5 =â‡’
P1(A2) < 0.5. Now, we need to distinguish two cases. If x1 âˆˆA2, the proof is completely
analogical to the ï¬rst part, but now reasoning on Aâ€™s rather than Bâ€™s. In this case, we will even
get stronger P1(A2) â‰¤P2(A2) just like in the ï¬rst part. If x1 Ì¸âˆˆA2, then it is easy to see, that
indeed P2(A2) < 0.5, yet it is also obvious to see that P1(A2) < 0.5. This ï¬nishes the proof of the
lemma."
AND,0.9408138101109741,"Theorem 7: Let Ïƒ(x) be r-semi-elastic function and x0, pA, N, Ïƒ0 as usual. Then, the certiï¬ed
radius at x0 guaranteed by our method is"
AND,0.9420468557336621,"CR(x0) = max {0, sup {R â‰¥0; Î¾>(R, Ïƒ0 exp(âˆ’rR)) < 0.5 and Î¾<(R, Ïƒ0 exp(rR)) < 0.5}} ."
AND,0.9432799013563502,Proof. This follows easily from Theorem 6
AND,0.9445129469790382,"Lemma 22. Let us have f(x) =
M
P"
AND,0.9457459926017263,"i=1
fi(x)1(x âˆˆRi), where {Ri}M
i=1 is ï¬nite set of regions that"
AND,0.9469790382244143,"divide the RN and {fi}M
i=1, fi : Ri âˆ’â†’R is ï¬nite set of 1-Lipschitz continuous (1-LC) functions.
Moreover assume, that f(x) is continuous. Then, f(x) is 1-LC."
AND,0.9482120838471023,"Proof. We do not assume any nice behaviour from our decision regions, what can make the situation
quite ugly. For instance, regions might not be measurable. However, it will not be a problem for us."
AND,0.9494451294697904,"Fix x1, x2. Consider line segment S = x1 + Î±(x2 âˆ’x1), Î± âˆˆ[0, 1]. Let us instead of points in S
work with numbers in [0, 1] via the Î± encoding. Consider the following coloring C of [0, 1]: Each
point a âˆˆ[0, 1] will be assigned one of M colors according to which region the x1 + a(x2 âˆ’x1)
belongs. Deï¬ne d1 â‰¡0 and d2 = sup{z âˆˆ[0, 1], C(z) = C(d1)}. Thus, d2 is the supremum of all
numbers colored the same color as 0. Then,"
AND,0.9506781750924784,|f(x1 + d2(x2 âˆ’x1)) âˆ’f(x1 + d1(x2 âˆ’x1))| â‰¤(d2 âˆ’d1)âˆ¥x2 âˆ’x1âˆ¥.
AND,0.9519112207151664,"Why? Let {zj}âˆž
j=1 be a non-decreasing sequence s.t. C(zj) = C(0) and zj âˆ’â†’d2. Since f is
continuous, obviously f(x1 + d2(x2 âˆ’x1)) = lim
jâˆ’â†’âˆžf(x1 + zj(x2 âˆ’x1)). Now, since norm and"
AND,0.9531442663378545,absolute value are both continuous functions and since from 1-LC of fC(0) on RC(0) we have
AND,0.9543773119605425,âˆ€j âˆˆN : |f(x1 + zj(x2 âˆ’x1)) âˆ’f(x1 + d1(x2 âˆ’x1))|
AND,0.9556103575832305,"(zj âˆ’d1)âˆ¥x2 âˆ’x1âˆ¥
â‰¤1,"
AND,0.9568434032059187,we also necessarily have
AND,0.9580764488286067,|f(x1 + d2(x2 âˆ’x1)) âˆ’f(x1 + d1(x2 âˆ’x1))|
AND,0.9593094944512947,"(d2 âˆ’d1)âˆ¥x2 âˆ’x1âˆ¥
â‰¤1."
AND,0.9605425400739828,Under review as a conference paper at ICLR 2022
AND,0.9617755856966708,"If d2 = 1, we ï¬nish the construction. If not, distinguish two cases. First assume C(d2) = C(d1). In
this case, take some color C s.t.
âˆƒ{zj}âˆž
j=1 : zj+1 â‰¤zj âˆ€j âˆˆN and zj âˆ’â†’d2 and zj > d2 âˆ€j âˆˆN,
and ï¬x one such {zj}âˆž
j=1. Obviously, C Ì¸= C(0), since d2 is upper-bound on points of color
C(0). Then, deï¬ne d3 = sup{z âˆˆ[0, 1], C(z) = C} and also deï¬ne {zj}âˆž
j=1 : zj+1 â‰¥zj âˆ€j âˆˆ
N and zj âˆ’â†’d3. From continuity of f, we again have f(x1 +d2(x2 âˆ’x1)) = lim
jâˆ’â†’âˆžf(x1 +zj(x2 âˆ’"
AND,0.9630086313193588,"x1)) and similarly f(x1 + d3(x2 âˆ’x1)) = lim
jâˆ’â†’âˆžf(x1 + zj(x2 âˆ’x1)). Again from continuity of"
AND,0.9642416769420469,absolute value and norm and 1-LC of all the partial functions we have:
AND,0.9654747225647349,âˆ€j âˆˆN : |f(x1 + zj(x2 âˆ’x1)) âˆ’f(x1 + zj(x2 âˆ’x1))|
AND,0.966707768187423,"(zj âˆ’zj)âˆ¥x2 âˆ’x1âˆ¥
â‰¤1"
AND,0.967940813810111,"and
|f(x1 + d3(x2 âˆ’x1)) âˆ’f(x1 + d2(x2 âˆ’x1))|"
AND,0.969173859432799,"(d3 âˆ’d2)âˆ¥x2 âˆ’x1âˆ¥
â‰¤1."
AND,0.9704069050554871,"Now assume C(d2) Ì¸= C(d1). Then, we can take as C directly C(d2) and do the same as in the last
paragraph (note, that this case could have implicitly come up in the previous construction too, but
we would need to not take C = C(0) and we ï¬nd this case distinction to be more elegant)."
AND,0.9716399506781751,"If d3 = 1, we ï¬nish the construction. If not, we continue in exactly the same manner as before.
Since the number of colors M is ï¬nite, we will run out of colors in ï¬nite number of steps and thus,
eventually there will be l â‰¤M s.t. dl = 1. The ï¬nal 1-LC is now trivially obtained as follows:"
AND,0.9728729963008631,"|f(x1 + 1(x2 âˆ’x1)) âˆ’f(x1 + 0(x2 âˆ’x1))| = lâˆ’1
X"
AND,0.9741060419235512,"i=1
f(x1 + di+1(x2 âˆ’x1)) âˆ’f(x1 + di(x2 âˆ’x1)) â‰¤ lâˆ’1
X i=1"
AND,0.9753390875462392,"f(x1 + di+1(x2 âˆ’x1)) âˆ’f(x1 + di(x2 âˆ’x1))
 â‰¤ lâˆ’1
X i=1"
AND,0.9765721331689272,(di+1 âˆ’di)âˆ¥x2 âˆ’x1âˆ¥
AND,0.9778051787916153,"= âˆ¥x2 âˆ’x1âˆ¥ lâˆ’1
X"
AND,0.9790382244143033,"i=1
(di+1 âˆ’di) = âˆ¥x2 âˆ’x1âˆ¥"
AND,0.9802712700369913,Theorem 8: The Ïƒ(x) deï¬ned in Equation 1 is r-semi-elastic.
AND,0.9815043156596794,"Proof. Our aim is to prove, that"
AND,0.9827373612823674,"log(Ïƒ(x)) = log(Ïƒb) + r ï£« ï£­1 k ï£« ï£­
X"
AND,0.9839704069050554,"xiâˆˆNk(x)
âˆ¥x âˆ’xiâˆ¥ ï£¶ ï£¸âˆ’m ï£¶ ï£¸"
AND,0.9852034525277436,"is r lipschitz continuous. Obviously, this does not depend neither on log(Ïƒb), nor on âˆ’rm, so we
will focus just on r k
P"
AND,0.9864364981504316,"xiâˆˆNk(x)
âˆ¥x âˆ’xiâˆ¥. Obviously, this function is r lipschitz continuous if and only if 1 k
P"
AND,0.9876695437731196,"xiâˆˆNk(x)
âˆ¥x âˆ’xiâˆ¥is 1-LC."
AND,0.9889025893958077,"Let us ï¬x y âˆˆRN. We will ï¬rst prove âˆ¥x âˆ’yâˆ¥is 1-LC. Let us ï¬x x1, x2. From triangle inequality
we have
âˆ¥x1 âˆ’yâˆ¥âˆ’âˆ¥x2 âˆ’yâˆ¥
 â‰¤âˆ¥x1 âˆ’x2âˆ¥,
what is exactly what we wanted to prove."
AND,0.9901356350184957,"Now ï¬x y1, y2, . . . , yk and x1, x2. Then

1
k k
X"
AND,0.9913686806411838,"i=1
âˆ¥x1 âˆ’yiâˆ¥âˆ’1 k k
X"
AND,0.9926017262638718,"i=1
âˆ¥x2 âˆ’yiâˆ¥
 = 1 k  k
X"
AND,0.9938347718865598,"i=1
âˆ¥x1 âˆ’yiâˆ¥âˆ’âˆ¥x2 âˆ’yiâˆ¥ â‰¤1 k k
X i=1"
AND,0.9950678175092479,"âˆ¥x1 âˆ’yiâˆ¥âˆ’âˆ¥x2 âˆ’yiâˆ¥
 â‰¤1 k k
X"
AND,0.9963008631319359,"i=1
âˆ¥x1 âˆ’x2âˆ¥= 1"
AND,0.9975339087546239,Under review as a conference paper at ICLR 2022
AND,0.998766954377312,"Finally note, that using the k nearest neighbors out of ï¬nite training dataset will divide RN in a
ï¬nite number of regions, where each region is deï¬ned by the set of k nearest neighbors for x in that
region. Note, that the average distance from k nearest neighbors is obviously continuous. Then,
using Lemma 22, the claim follows."
