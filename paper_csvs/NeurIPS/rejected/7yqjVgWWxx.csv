Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007961783439490446,"Discrete diffusion models with absorbing processes have shown promise in lan-
1"
ABSTRACT,0.0015923566878980893,"guage modeling. The key quantities to be estimated are the ratios between the
2"
ABSTRACT,0.0023885350318471337,"marginal probabilities of two transitive states at all timesteps, called the concrete
3"
ABSTRACT,0.0031847133757961785,"score. In this paper, we reveal that the concrete score in absorbing diffusion can be
4"
ABSTRACT,0.003980891719745223,"expressed as conditional probabilities of clean data, multiplied by a time-dependent
5"
ABSTRACT,0.004777070063694267,"scalar in an analytic form. Motivated by the finding, we propose reparameterized
6"
ABSTRACT,0.005573248407643312,"absorbing discrete diffusion (RADD), a dedicated diffusion model that character-
7"
ABSTRACT,0.006369426751592357,"izes the time-independent conditional probabilities. Besides its simplicity, RADD
8"
ABSTRACT,0.007165605095541401,"can reduce the number of function evaluations (NFEs) by caching the output of
9"
ABSTRACT,0.007961783439490446,"the time-independent network when the noisy sample remains unchanged in a
10"
ABSTRACT,0.00875796178343949,"sampling interval. Empirically, RADD is up to 3.5 times faster while consistently
11"
ABSTRACT,0.009554140127388535,"achieving a better performance than the strongest baseline. Built upon the new
12"
ABSTRACT,0.010350318471337579,"factorization of the concrete score, we further prove a surprising result that the
13"
ABSTRACT,0.011146496815286623,"exact likelihood of absorbing diffusion can be rewritten to a simple form (named
14"
ABSTRACT,0.01194267515923567,"denoise cross-entropy) and then estimated efficiently by the Monte Carlo method.
15"
ABSTRACT,0.012738853503184714,"The resulting approach also applies to the original parameterization of the concrete
16"
ABSTRACT,0.013535031847133758,"score. It significantly advances the state-of-the-art discrete diffusion on 5 zero-shot
17"
ABSTRACT,0.014331210191082803,"language modeling benchmarks (measured by perplexity) at the GPT-2 scale.
18"
INTRODUCTION,0.015127388535031847,"1
Introduction
19"
INTRODUCTION,0.01592356687898089,"Auto-regressive models [1, 2, 3] have dominated the area of language modeling for many years. In
20"
INTRODUCTION,0.016719745222929936,"particular, such models significantly benefit from large-scale transformers [4] and training data and
21"
INTRODUCTION,0.01751592356687898,"have achieved remarkable progress [5, 6, 7, 8]. From a probabilistic perspective, the sequential sam-
22"
INTRODUCTION,0.018312101910828025,"pling process of auto-regressive models is inefficient and limits the reasoning ability in nonsequential
23"
INTRODUCTION,0.01910828025477707,"orders [9, 10]. Intrinsically, this is because such models characterize the joint distribution by the
24"
INTRODUCTION,0.019904458598726114,"chain rule of probability, motivating research on developing other types of generative models for text.
25"
INTRODUCTION,0.020700636942675158,"Diffusion models [11, 12, 13] generate data in a coarse-to-fine manner efficiently [14, 15, 16, 17, 18]
26"
INTRODUCTION,0.021496815286624203,"and all dimensions simultaneously, providing an appealing alternative to auto-regressive models.
27"
INTRODUCTION,0.022292993630573247,"Among other efforts [19, 20, 21, 22, 23, 24, 20, 25, 26, 27, 28, 29] (see Section 5 for a comprehensive
28"
INTRODUCTION,0.02308917197452229,"discussion), score entropy discrete diffusion (SEDD) [29] has shown promise in text generation. In
29"
INTRODUCTION,0.02388535031847134,"particular, SEDD has achieved comparable results to auto-regressive models on 5 zero-shot language
30"
INTRODUCTION,0.024681528662420384,"modeling benchmarks at the GPT-2 scale. Meanwhile, SEDD can reduce the number of function
31"
INTRODUCTION,0.025477707006369428,"evaluations (NFEs) in sampling and fulfill text conditioned on prompts at different positions.
32"
INTRODUCTION,0.026273885350318472,"Technically, SEDD employs a discrete-state (absorbing) Markov process that adds noises to data by
33"
INTRODUCTION,0.027070063694267517,"randomly replacing a token with a mask token [M] and then learns a reverse process to denoise from
34"
INTRODUCTION,0.02786624203821656,"an entirely masked sentence. The key quantities to be estimated in SEDD are the ratios between the
35"
INTRODUCTION,0.028662420382165606,"marginal probabilities of two transitive states at all timesteps, called the concrete score. SEDD also
36"
INTRODUCTION,0.02945859872611465,"proposes a “scaling trick” (see details in Section 3) that scales the output of the score estimation by a
37"
INTRODUCTION,0.030254777070063694,"factor. The trick has been proven very effective in practice yet not fully understood in theory [29].
38"
INTRODUCTION,0.03105095541401274,"One of our main contributions is to reveal that the concrete score in absorbing diffusion can be ex-
39"
INTRODUCTION,0.03184713375796178,"pressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic
40"
INTRODUCTION,0.03264331210191083,"form. Our finding theoretically explains the benefits of the scaling trick as a reparameterization
41"
INTRODUCTION,0.03343949044585987,"for better optimization. Motivated by the finding, we propose reparameterized absorbing discrete
42"
INTRODUCTION,0.03423566878980892,"diffusion (RADD), a dedicated diffusion model that characterizes the time-independent conditional
43"
INTRODUCTION,0.03503184713375796,"probabilities by removing the time embedding from the score estimation in SEDD. Besides its
44"
INTRODUCTION,0.035828025477707005,"simplicity, RADD can significantly reduce the NFEs by caching the output of the time-independent
45"
INTRODUCTION,0.03662420382165605,"network when the noisy sample remains unchanged in a sampling interval (see Fig. 1).
46"
INTRODUCTION,0.037420382165605094,"Built upon the new factorization of the concrete score, we further prove a surprising result that the
47"
INTRODUCTION,0.03821656050955414,"exact likelihood of absorbing diffusion can be rewritten to a simple form (named denoise cross-
48"
INTRODUCTION,0.03901273885350318,"entropy, DCE) and then estimated efficiently by the Monte Carlo method. To establish the theory,
49"
INTRODUCTION,0.03980891719745223,"we apply a change of variable from the time t to the probability that a single-dimensional token is
50"
INTRODUCTION,0.04060509554140127,"masked at time t in the forward process. By integrating the probability variable analytically, we show
51"
INTRODUCTION,0.041401273885350316,"that DCE enumerates all orders to decompose the joint distribution auto-regressively and accumulates
52"
INTRODUCTION,0.04219745222929936,"log densities of all conditional distributions in every order, finishing the proof. Such theoretical
53"
INTRODUCTION,0.042993630573248405,"findings enable exact likelihood evaluation and optimization for both the original parameterization of
54"
INTRODUCTION,0.04378980891719745,"absorbing diffusion [29] and the proposed RADD.
55"
INTRODUCTION,0.044585987261146494,"Empirically, RADD is up to 3.5 times faster while consistently achieving a better performance than
56"
INTRODUCTION,0.04538216560509554,"the strongest baseline, i.e. SEDD with the scaling trick [29]. Further, the DCE loss applies to both
57"
INTRODUCTION,0.04617834394904458,"RADD and SEDD for precise likelihood evaluation. It significantly advances the state-of-the-art
58"
INTRODUCTION,0.046974522292993634,"discrete diffusion (i.e. SEDD [29]) on 5 zero-shot language modeling benchmarks (measured by
59"
INTRODUCTION,0.04777070063694268,"perplexity) at the GPT-2 scale. The empirical evidence validates our theoretical findings.
60"
INTRODUCTION,0.04856687898089172,"In summary, this paper has several contributions:
61"
INTRODUCTION,0.04936305732484077,"• Deeper understanding of discrete diffusion: Both the factorization form of the concrete
62"
INTRODUCTION,0.05015923566878981,"score and DCE loss for the exact likelihood computation reveal important yet overlooked
63"
INTRODUCTION,0.050955414012738856,"theoretical properties of absorbing discrete diffusion, which explain the mysterious scaling
64"
INTRODUCTION,0.0517515923566879,"trick, provide practice guidance, and may inspire future work.
65"
INTRODUCTION,0.052547770700636945,"• Simplification: By removing the time conditions, we reparameterize the model to focus on
66"
INTRODUCTION,0.05334394904458599,"a time-independent conditional probability, simplifying the existing model.
67"
INTRODUCTION,0.054140127388535034,"• Efficient sampling: Leveraging the reparameterized form, RADD with a caching strategy
68"
INTRODUCTION,0.05493630573248408,"is consistently faster while achieving a better performance than the strongest competitor.
69"
INTRODUCTION,0.05573248407643312,"• Improved likelihood evaluation: The exact likelihood evaluation approach significantly
70"
INTRODUCTION,0.05652866242038217,"advances the state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks
71"
INTRODUCTION,0.05732484076433121,"(measured by perplexity) at the GPT-2 scale.
72"
BACKGROUND,0.058121019108280256,"2
Background
73"
BACKGROUND,0.0589171974522293,"In this section, we present preliminaries on continuous-time discrete diffusion models. We start with
74"
BACKGROUND,0.059713375796178345,"the one-dimensional case in Section 2.1, followed by the multi-dimensional case in Section 2.2.
75"
SINGLE DIMENSION,0.06050955414012739,"2.1
Single dimension
76"
SINGLE DIMENSION,0.06130573248407643,"Let x denote a single dimensional sample with possible values in {1, . . . , N}. A continuous-time
77"
SINGLE DIMENSION,0.06210191082802548,"discrete Markov chain at time t is characterized by a transition rate matrix Qt as follows
78"
SINGLE DIMENSION,0.06289808917197452,"pt+∆t|t(ˆx|x) =
Qt(x, ˆx)∆t + o(∆t),
ˆx ̸= x,
1 + Qt(x, x)∆t + o(∆t),
ˆx = x,
(2.1)"
SINGLE DIMENSION,0.06369426751592357,"where Qt(x, ˆx) is the (x, ˆx) element of transition rate matrix Qt, denoting the transition rate from
79"
SINGLE DIMENSION,0.06449044585987261,"state x to state ˆx at time t. Equivalently, we can directly define Qt(x, ˆx) as
80"
SINGLE DIMENSION,0.06528662420382166,"Qt(x, ˆx) ="
SINGLE DIMENSION,0.0660828025477707,"(
lim∆t→0
pt+∆t|t(ˆx|x)"
SINGLE DIMENSION,0.06687898089171974,"∆t
,
ˆx ̸= x,
lim∆t→0
pt+∆t|t(x|x)−1"
SINGLE DIMENSION,0.06767515923566879,"∆t
,
ˆx = x.
(2.2)"
SINGLE DIMENSION,0.06847133757961783,"Given the above definition, denote Ps→t(x, ˆx) := pt|s(ˆx|x). The following Kolmogorov’s forward
81"
SINGLE DIMENSION,0.06926751592356688,"equation holds [26, 30]:
82"
SINGLE DIMENSION,0.07006369426751592,"d
dtPs→t = Ps→tQt.
(2.3)"
SINGLE DIMENSION,0.07085987261146497,"In practice [26, 29], Qt is parameterized as σ(t)Q, where σ(t) is a scalar function and Q is a
83"
SINGLE DIMENSION,0.07165605095541401,"constant matrix. In this case, the solution to Eq. (2.3) can be solved analytically as Ps→t =
84"
SINGLE DIMENSION,0.07245222929936306,"exp ((¯σ(t) −¯σ(s))Q), where ¯σ(t) =
R t
0 σ(s)ds and exp is the matrix exponential. Therefore, we
85"
SINGLE DIMENSION,0.0732484076433121,"can directly sample xt from xs in one step for any t > s.
86"
SINGLE DIMENSION,0.07404458598726114,"Further, Q is often designed to diffuse towards a uniform distribution or an absorbing state [M].
87"
SINGLE DIMENSION,0.07484076433121019,"Recent work [20, 26] suggests that the absorbing matrix achieves better empirical performance.
88"
SINGLE DIMENSION,0.07563694267515923,"Besides, as detailed in Section 3, the specific structure of the absorbing matrix can be leveraged
89"
SINGLE DIMENSION,0.07643312101910828,"to improve performance and accelerate sampling. Therefore, we focus on the absorbing matrix as
90"
SINGLE DIMENSION,0.07722929936305732,"follows:
91"
SINGLE DIMENSION,0.07802547770700637,Qabsorb =  
SINGLE DIMENSION,0.07882165605095541,"−1
0
· · ·
0
1
0
−1
· · ·
0
1
...
...
...
...
...
0
0
· · ·
−1
1
0
0
· · ·
0
0 "
SINGLE DIMENSION,0.07961783439490445,"
.
(2.4)"
SINGLE DIMENSION,0.0804140127388535,"The time reversal of the forward process is characterized by a reverse transition rate matrix ˜Qt [31, 32],
92"
SINGLE DIMENSION,0.08121019108280254,"whose element from state x to state ˆx is given by
93"
SINGLE DIMENSION,0.08200636942675159,"˜Qt(x, ˆx) ="
SINGLE DIMENSION,0.08280254777070063,( pt(ˆx)
SINGLE DIMENSION,0.08359872611464968,"pt(x)Qt(ˆx, x),
ˆx ̸= x,
−P"
SINGLE DIMENSION,0.08439490445859872,"k̸=x ˜Qt(x, k),
ˆx = x.
(2.5)"
SINGLE DIMENSION,0.08519108280254777,"Simulating the reverse process requires to learn the reverse transition rate ˜Qt(x, ˆx). As Qt(xt, ˆxt)
94"
SINGLE DIMENSION,0.08598726114649681,"is known, it is sufficient to estimate the concrete score pt(ˆxt)"
SINGLE DIMENSION,0.08678343949044585,"pt(xt) by a score network sθ(xt, t) ≈
95"
SINGLE DIMENSION,0.0875796178343949,[ pt(ˆxt)
SINGLE DIMENSION,0.08837579617834394,"pt(xt)]ˆxt∈X [28]. Denoising score entropy (DSE) [29] is an effective objective to train the score
96"
SINGLE DIMENSION,0.08917197452229299,"network
97 Z T"
SINGLE DIMENSION,0.08996815286624203,"0
E˜x∼pt|0(·|x0)
X"
SINGLE DIMENSION,0.09076433121019108,"y̸=˜x
Qt (˜x, y)

sθ (˜x, t)y −pt|0 (y | x0)"
SINGLE DIMENSION,0.09156050955414012,"pt|0 (˜x | x0) log sθ (˜x, t)y + K
pt|0 (y | x0)"
SINGLE DIMENSION,0.09235668789808917,pt|0 (˜x | x0)
SINGLE DIMENSION,0.09315286624203821,"
dt,"
SINGLE DIMENSION,0.09394904458598727,"(2.6)
where K(a) := a log a −a. In particular, the DSE loss in Eq. (2.6) is an evidence lower bound
98"
SINGLE DIMENSION,0.09474522292993631,"(ELBO) of the negative log-likelihood with an unknown gap. Nevertheless, existing work [29] still
99"
SINGLE DIMENSION,0.09554140127388536,"employs it for training and likelihood evaluation.
100"
SINGLE DIMENSION,0.0963375796178344,"After training, sampling from the model can be understood as discretizing the following process
101"
SINGLE DIMENSION,0.09713375796178345,"d
dtPs→t = Ps→t ˜Qt,
(2.7)"
SINGLE DIMENSION,0.09792993630573249,"where dt is an infinitesimal negative timestep and the concrete score is replaced by the score network.
102"
SINGLE DIMENSION,0.09872611464968153,"Existing samplers include the Euler method, Gillespie method, and Tweedie τ -leaping, as detailed in
103"
SINGLE DIMENSION,0.09952229299363058,"Appendix D.
104"
MULTI-DIMENSION,0.10031847133757962,"2.2
Multi-dimension
105"
MULTI-DIMENSION,0.10111464968152867,"The multi-dimensional cases consider a state space of size d like X d = {1, . . . , n}d. We denote
106"
MULTI-DIMENSION,0.10191082802547771,"the sample as a sequence of one-dimensional data, i.e. x = x1 . . . xd. The transition matrix
107"
MULTI-DIMENSION,0.10270700636942676,"Qt ∈Rnd×nd has an exponential number of possible states, making it expensive to reverse. To
108"
MULTI-DIMENSION,0.1035031847133758,"alleviate this issue, existing work [26, 29] assumes independence between dimensions and each
109"
MULTI-DIMENSION,0.10429936305732485,"dimension is a one-dimensional diffusion process with the same transition rate matrix Qtok
t
∈Rn×n.
110"
MULTI-DIMENSION,0.10509554140127389,"Under the independent assumption, Qt assigns zero values [26, 29] for all sequences with a Hamming
distance larger than 1. According to Eq. (2.4), it is sufficient to model the concrete score between"
MULTI-DIMENSION,0.10589171974522293,"sequences that differ by a Hamming distance of 1, such as ˆxt = x1
t . . . bxi
t . . . xd
t given xt = x1
t · · · xd
t .
Therefore, the score network sθ(·, t) : {1, . . . , n}d →Rd×n is defined as"
MULTI-DIMENSION,0.10668789808917198,"sθ (xt, t)ˆxt = sθ
 
x1
t . . . xi
t . . . xd
t , t

[i, bxi
t] ≈pt
 
x1
t . . . bxi
t . . . xd
t
"
MULTI-DIMENSION,0.10748407643312102,"pt
 
x1
t . . . xi
t . . . xd
t
,"
MULTI-DIMENSION,0.10828025477707007,"which leads to the following expression to estimate the reverse transition rate matrix ˜Qt:
111"
MULTI-DIMENSION,0.10907643312101911,"˜Qt
 
x1
t . . . xi
t . . . xd
t , x1
t . . . bxi
t . . . xd
t

= Qtok
t
 
bxi
t, xi
t
 pt
 
x1
t . . . bxi
t . . . xd
t
"
MULTI-DIMENSION,0.10987261146496816,"pt
 
x1
t . . . xi
t . . . xd
t

(2.8)"
MULTI-DIMENSION,0.1106687898089172,"≈Qtok
t
 
bxi
t, xi
t

sθ
 
x1
t . . . xi
t . . . xd
t , t

[i, bxi
t].
(2.9)"
MULTI-DIMENSION,0.11146496815286625,"Existing samplers assume that each dimension is independent within a small interval ∆t and update
112"
MULTI-DIMENSION,0.11226114649681529,"each dimension in parallel for efficiency [29, 26].
113"
REPARAMETERIZED ABSORBING DISCRETE DIFFUSION,0.11305732484076433,"3
Reparameterized absorbing discrete diffusion
114"
REPARAMETERIZED ABSORBING DISCRETE DIFFUSION,0.11385350318471338,"In Section 3.1, we reveal that the concrete score of absorbing discrete diffusion can be reparameterized
115"
REPARAMETERIZED ABSORBING DISCRETE DIFFUSION,0.11464968152866242,"as conditional distributions of clean data, which enables efficient sampling by caching the output of
116"
REPARAMETERIZED ABSORBING DISCRETE DIFFUSION,0.11544585987261147,"time-independent network (see Section 3.2) and exact likelihood computation (see Section 3.3) by
117"
REPARAMETERIZED ABSORBING DISCRETE DIFFUSION,0.11624203821656051,"applying the change of variable from time to the probability of being masked in a single dimension.
118"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.11703821656050956,"3.1
Parameterizing the concrete score as conditional distributions of clean data
119"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1178343949044586,"A key observation is that only the transition from the masked token to an unmasked token is valid in
120"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.11863057324840764,"the reverse process of an absorbing discrete diffusion. In particular, according to the definition of
121"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.11942675159235669,"the transition matrix of the absorbing process (see Eq. (2.4)), we have Qabsorb(ˆxi
t, xi
t) = 0 for any
122"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12022292993630573,"unmasked xi
t ̸= [M] and ˆxi
t ̸= xi
t. Therefore, the corresponding element in the transition matrix of
123"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12101910828025478,"the reverse process ˜Qt (see Eq. (2.5)) equals zero. Namely,
124"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12181528662420382,"˜Qt
 
x1
t . . . xi
t . . . xd
t , x1
t . . . bxi
t . . . xd
t

= σ(t)Qabsorb  
bxi
t, xi
t
 pt
 
x1
t . . . bxi
t . . . xd
t
"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12261146496815287,"pt
 
x1
t . . . xi
t . . . xd
t
 = 0,
(3.1)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12340764331210191,"for any unmasked state xi
t ̸= [M] and ˆxi
t ̸= xi
t and it is unnecessary to model the corresponding
125"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12420382165605096,"concrete score
pt(x1
t ...bxi
t...xd
t)
pt(x1
t ...xi
t...xd
t). Also, note that the concrete score always takes the value of one if
126"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.125,"ˆxi
t = xi
t. Therefore, we only need to characterize the concrete score for xi
t = [M] and ˆxi
t ̸= [M].
127"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12579617834394904,"Interestingly, in this case, we discover that the concrete score has a simple analytic form w.r.t. to the
128"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1265923566878981,"conditional distributions of clean data, as summarized in the following Theorem 1.
129"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12738853503184713,"Theorem 1. (Analytic concrete score in absorbing case, proof in Appendix B) For xt
=
130"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12818471337579618,"x1
t . . . xi
t . . . xd
t and ˆxt = x1
t . . . bxi
t . . . xd
t , if xi
t = [M] and ˆxi
t ̸= [M], the concrete score at time
131"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12898089171974522,"t can be expressed as a time-independent conditional distribution at time zero multiplied by an
132"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.12977707006369427,"analytic time-dependent term:
133"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1305732484076433,"pt
 
x1
t . . . bxi
t . . . xd
t
"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.13136942675159236,"pt
 
x1
t . . . xi
t . . . xd
t
 =
e−¯σ(t)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1321656050955414,"1 −e−¯σ(t) p0(ˆxi
t|xUM
t
),"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.13296178343949044,"where xUM
t
is the vector consists of all unmasked tokens of xt.
134"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1337579617834395,"One immediate implication of Theorem 1 is to theoretically explain the benefit of the “scaling
135"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.13455414012738853,"trick” in existing work [29] (see Appendix C.2 therein), which significantly improves the practical
136"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.13535031847133758,"performance of discrete diffusion (see Table 2) but has not been fully understood.
137"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.13614649681528662,"In particular, the scaling trick divides the output of the score network sθ by a factor of e¯σ(t) −1.
138"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.13694267515923567,"Equivalently, it reparameterizes sθ(xt, t) as:
139"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1377388535031847,"sθ(xt, t) =
1
e¯σ(t) −1 ˜sθ(xt, t) =
e−¯σ(t)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.13853503184713375,"1 −e−¯σ(t) ˜sθ(xt, t),"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1393312101910828,"0
500
1000
1500
2000
Sampling Steps 0 500 1000 1500 2000"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.14012738853503184,E-NFEs
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1409235668789809,"l = 1024
cached (theory)
uncached
cached (experiment)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.14171974522292993,"Figure 1: Expected number of function eval-
uations (E-NFE) over a different number
of sampling steps.
E-NFE is measured by
Tweedie τ-leaping method with log-linear noise
schedule."
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.14251592356687898,"32
64
128
256
512
1024 2048 4096
NFE 15 30 60 120 150"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.14331210191082802,Generative Perplexity
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.14410828025477707,"SEDD Small (Euler)
SEDD Small (T- )
RADD Small (no cache)
RADD Small (cache)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1449044585987261,"Figure 2: Sample quality measured by per-
plexity (↓). We compare SEDD with Euler and
Tweedie τ-leaping (abbr. T-τ) samplers, and
RADD with Euler sampler. We show E-NFE
for RADD with caching and NEF otherwise."
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.14570063694267515,"where the scaling factor coincides with the time-dependent term in Theorem 1. In the original
140"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1464968152866242,"parameterization, the score network sθ must model the whole time-dependent concrete score. In
141"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.14729299363057324,"contrast, with the scaling trick, the reparameterized score ˜sθ(xt, t) can focus on capturing the clean
142"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1480891719745223,"data distribution p0(ˆxi|xUM
t
) and simplifies learning, according to Theorem 1.
143"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.14888535031847133,"Further, Theorem 1 suggests that it is unnecessary to incorporate the time t in the reparameterized
144"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.14968152866242038,"score, and the reparameterized score ˜sθ(xt, t) should output a valid probability distribution. Mo-
145"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.15047770700636942,"tivated by the insights, we propose reparameterized absorbing discrete diffusion (RADD), which
146"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.15127388535031847,"employs a network cθ(xt) that removes the time condition from the input and takes the softmax as
147"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1520700636942675,"final nonlinearity. Formally, we can write our reparameterization as:
148"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.15286624203821655,"sθ(xt, t) =
e−¯σ(t)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1536624203821656,"1 −e−¯σ(t) cθ(xt).
(3.2)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.15445859872611464,"In practice, we make a minimal modification of the score network in SEDD [29] for simplicity and
149"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1552547770700637,"fairness, detailed in Appendix F.1.
150"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.15605095541401273,"Moreover, RADD also enjoys a more efficient sampling process than SEDD [29] (with or without the
151"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.15684713375796178,"scaling trick) based on its simplified parameterization, as presented below.
152"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.15764331210191082,"3.2
Efficient samplers to reduce NFE by caching cθ(xt)
153"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.15843949044585987,"For the reverse process of an absorbing discrete diffusion, once a token is generated from [M] to
154"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1592356687898089,"an unmasked token, it never transits to another token. Therefore, for a sequence xt of length d, xt
155"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.16003184713375795,"changes at most d times, irrespective of the number of sampling steps D. In the other steps, xt
156"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.160828025477707,"remains in all d dimensions. We highlight that we can cache cθ(xt) naturally without evaluating the
157"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.16162420382165604,"time-independent cθ to reduce the NFE compared to SEDD (see Appendix E for the pseudo-code, ).
158"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1624203821656051,"As shown in Fig. 2, RADD with the caching strategy is more efficient than SEDD given any number
159"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.16321656050955413,"of sampling steps, especially given large sampling steps. This is as expected because the NFE is
160"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.16401273885350318,"limited within the generating sequence length.
161"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.16480891719745222,"Note that the NFEs with the caching strategy is a random variable. To quantify it, we calculate the
162"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.16560509554140126,"expected NFEs (abbr. E-NFEs) required in an analytic form, conditioned on the sampling method,
163"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1664012738853503,"time steps, and noise schedule. Specifically, denote l as the generating sequence length, which does
164"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.16719745222929935,"not equal d generally. Given the sampling time steps {t0 = 0, · · · , tn = T}, let Nk ∈{0, · · · , l}
165"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1679936305732484,"denote the number of changed dimensions of x in [tk−1, tk). Since we perform function evaluation
166"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.16878980891719744,"in [tk−1, tk) only when x changes (i.e. Nk ̸= 0), the NFEs and E-NFEs can expressed as:
167"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1695859872611465,"NFEs(n) = n
X"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17038216560509553,"k=1
I(Nk ̸= 0),
(3.3)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17117834394904458,"E-NFEs(n) = n
X"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17197452229299362,"k=1
E[I(Nk ̸= 0)] = n
X"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17277070063694266,"k=1
P(Nk ̸= 0).
(3.4)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1735668789808917,"For each dimension i, let rk represent the probability that xi changes within the interval [tk−1, tk).
168"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17436305732484075,"As the probability is independent in different dimensions (proof in Appendix D.3), Nk follows a
169"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1751592356687898,"binomial distribution with parameters l and rk. Therefore, Eq. (3.4) can be further simplified as:
170"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17595541401273884,"E-NFEs(n) = n
X"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1767515923566879,"k=1
P(Nk ̸= 0) = n
X"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17754777070063693,"k=1
(1 −(1 −rk)l),
(3.5)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17834394904458598,"which applies to all samplers. Further, rk can be analytically expressed w.r.t. the time steps and
171"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17914012738853502,"noise schedule for both Euler and Tweedie τ-leaping samplers, as detailed in Appendix D.3. Taking
172"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.17993630573248406,"Tweedie τ -leaping method with log-linear noise schedule [29] for example, its E-NFEs is given by:
173"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1807324840764331,"E-NFEs(n) = n
X"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.18152866242038215,"k=1
(1 −(1 −1"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1823248407643312,n)l) = n(1 −(1 −1
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.18312101910828024,"n)l).
(3.6)"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.1839171974522293,"Appendix D.3 provides the proof. As shown in Fig. 1, we plot the curve of Eq. (3.6) in blue, which
174"
PARAMETERIZING THE CONCRETE SCORE AS CONDITIONAL DISTRIBUTIONS OF CLEAN DATA,0.18471337579617833,"agrees with our experiments (the red stars).
175"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.18550955414012738,"3.3
Denoise cross-entropy for exact likelihood evaluation and training
176"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.18630573248407642,"As illustrated in Theorem 1, the concrete score can be understood as a rescaled conditional distribution
177"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.18710191082802546,"on clean data. From this perspective, it is natural to wonder: is it possible to evaluate and optimize
178"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.18789808917197454,"the exact likelihood of the model instead of the ELBO? Surprisingly, the answer is yes for both the
179"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.18869426751592358,"original parameterization [29] and our new parameterization.
180"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.18949044585987262,"Let qθ(x0) denote the model distribution at time zero defined by sθ, or our cθ, which approximates
181"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.19028662420382167,"the true distribution p0(x0). Inspired by the cross-entropy loss in auto-regressive models, we define
182"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.1910828025477707,"the denoising cross-entropy loss LT
DCE(x0) as:
183"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.19187898089171976,"LT
DCE(x0) :=
Z T"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.1926751592356688,"0
E˜x∼pt|0(·|x0)
X"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.19347133757961785,"y̸=˜x
Qt (˜x, y)

−pt|0 (y | x0)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.1942675159235669,"pt|0 (˜x | x0) log sθ (˜x, t)y"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.19506369426751594,"
dt,
(3.7) =
Z T"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.19585987261146498,"0
E˜x∼pt|0(·|x0)
X"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.19665605095541402,"y̸=˜x
Qt (˜x, y)

−pt|0 (y | x0)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.19745222929936307,"pt|0 (˜x | x0) log

e−¯σ(t)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.1982484076433121,1 −e−¯σ(t) cθ(˜x)y
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.19904458598726116,"
dt. (3.8)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.1998407643312102,"Compared with the DSE loss in Eq. (2.6), our DCE loss simply removed the terms sθ (˜x, t)y and
184"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.20063694267515925,"K
 pt|0(y|x0)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2014331210191083,"pt|0(˜x|x0)

, however, it shows that DCE loss exactly equals the negative log-likelihood of qθ(x0)
185"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.20222929936305734,"with a sufficiently long process in absorbing discrete diffusion.
186"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.20302547770700638,"Theorem 2. Suppose {Xt} is a continuous time Markov chain with transition rate matrix Qt =
187"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.20382165605095542,"σ(t)Qabsorb. For a given data x0, if σ(t) satisfies
R ∞
0
σ(τ)dτ = ∞, then the denoising cross-entropy
188"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.20461783439490447,"loss defined in Eq. (3.8) with T →∞exactly equals the negative log-likelihood of x0.
189"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2054140127388535,"L∞
DCE(x0) = −log qθ(x0).
(3.9)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.20621019108280256,"The proof of Theorem 2 consists of three key steps, detailed in Appendix C.1, Appendix C.2
190"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2070063694267516,"and Appendix C.3 respectively. In the first step, we apply a change of variable from t to λ(t) =
191"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.20780254777070065,"1 −e−¯σ(t), which is the probability of a token is masked from 0 to t in the forward process. Further,
192"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2085987261146497,"inspired by the factorization form discovered in Theorem 1, the denoising cross-entropy loss for both
193"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.20939490445859874,"parameterizations can then be rewritten as an integral of λ
194"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.21019108280254778,"L∞
DCE(x0) =
Z 1 0"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.21098726114649682,"1
λE˜x∼pλ(˜x|x0)  X"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.21178343949044587,"˜xi=[M]
−log qθ(xi
0|˜xUM) "
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2125796178343949,"dλ,
(3.10)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.21337579617834396,"where pλ(˜x|x0) is the joint distribution induced by masking each dimension in x0 independently
195"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.214171974522293,"with a probability λ.
196"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.21496815286624205,"In the second step, we demonstrate that the integral w.r.t. λ in Eq. (3.10) can be integrated analytically,
197"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2157643312101911,"and the DSE loss can be rewritten as expectations over the number and positions of masks as follows:
198"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.21656050955414013,"L∞
DCE(x0) = dEk∼U({1,··· ,d})
1
k E˜x∼U( ˜
Xk)  X"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.21735668789808918,"˜xi=[M]
−log qθ(xi
0|˜xUM) "
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.21815286624203822,",
(3.11)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.21894904458598727,"where we denote ˜
Xk := {˜x : ˜x ∈˜
X and ˜x has exact k dimensions masked by [M]} and U(·) as
199"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2197452229299363,"uniform distribution.
200"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.22054140127388536,"Finally, in the third step, we prove that Eq. (3.11) enumerates all orders to decompose the joint
201"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2213375796178344,"distribution auto-regressively and accumulates log densities of all conditional distributions in every
202"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.22213375796178345,"order. Therefore, it is equivalent to the negative log-likelihood of qθ:
203"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2229299363057325,"L∞
DCE(x0) = −log qθ(x0).
(3.12)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.22372611464968153,"Theorem 2 enables exact likelihood computation for both the original model sθ and our cθ, providing
204"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.22452229299363058,"a more accurate measure of model performance. Take cθ for example, Eq. (3.8) can be rewritten as a
205"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.22531847133757962,"form of expectation on t:
206"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.22611464968152867,"LT
DCE(x0) = 1"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2269108280254777,"T Et∼U([0,T ])E˜x∼pt|0(·|x0)
X"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.22770700636942676,"y̸=˜x
Qt (˜x, y)

−pt|0 (y | x0)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2285031847133758,"pt|0 (˜x | x0) log

e−¯σ(t)"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.22929936305732485,"1 −e−¯σ(t) cθ(˜x)y 
."
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.2300955414012739,"(3.13)
Naturally, we can take the Monte Carlo estimation of LT
DCE(x0) by sampling t to approximate
207"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.23089171974522293,"−log qθ(x0) according to Eq. (3.13). In addition, it can be used as an efficient and valid training
208"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.23168789808917198,"target for discrete diffusion models, as an alternative to the ELBO (i.e. DSE loss). For pseudo-code
209"
DENOISE CROSS-ENTROPY FOR EXACT LIKELIHOOD EVALUATION AND TRAINING,0.23248407643312102,"of training, see Appendix E.
210"
EXPERIMENTS,0.23328025477707007,"4
Experiments
211"
EXPERIMENTS,0.2340764331210191,"We present the experimental setups in Section 4.1. We then evaluate the performance of accelerated
212"
EXPERIMENTS,0.23487261146496816,"generation in Section 4.2 and zero-shot perplexity on various language datasets in Section 4.3.
213"
SETTINGS,0.2356687898089172,"4.1
Settings
214"
SETTINGS,0.23646496815286625,"Model.
We use RADD model cθ reparameterzied as described in Section 3.1. Compared with
215"
SETTINGS,0.2372611464968153,"SEDD small model, RADD model has 7M fewer parameters due to the removal of time-condition,
216"
SETTINGS,0.23805732484076433,"which equates to an 8% decrease from the original 90M non-embedding parameters. We trained
217"
SETTINGS,0.23885350318471338,"our RADD model cθ using denoising score entropy and denoising cross entropy, abbreviated as
218"
SETTINGS,0.23964968152866242,"RADD-DSE and RADD-DCE. For SEDD small model, we employed their pre-trained model.
219"
SETTINGS,0.24044585987261147,"Data.
In line with the methodology outlined by SEDD, we trained on the OpenWebText [33]
220"
SETTINGS,0.2412420382165605,"dataset and tested on the LAMBADA, WikiText2, PTB, WikiText103, and One Billion Words
221"
SETTINGS,0.24203821656050956,"datasets [34, 35, 36]. For data splits and data processing, we adopted the same settings and techniques
222"
SETTINGS,0.2428343949044586,"as SEDD, which involves packing sentences to generate uniform-length blocks as model input.
223"
SETTINGS,0.24363057324840764,"Training setup.
We used the same training setup for RADD and SEDD. Specifically, we used a
224"
SETTINGS,0.2444267515923567,"log-linear noise schedule where the expectation of the number of changed tokens at time t is linear
225"
SETTINGS,0.24522292993630573,"with t. For simplicity, we also used the same optimization configuration as SEDD, which can be
226"
SETTINGS,0.24601910828025478,"suboptimal for our RADD model and DCE loss. For more details see Appendix F.
227"
SETTINGS,0.24681528662420382,"Metric.
Following previous work [29], we conduct experiments on unconditional generation and
228"
SETTINGS,0.24761146496815287,"language modeling tasks. For generation, we use perplexity (PPL) on unconditional samples measured
229"
SETTINGS,0.2484076433121019,"by an additional larger language model (i.e. GPT-2 large) to evaluate sample quality. To access
230"
SETTINGS,0.24920382165605096,"inference efficiency, we computed the inference time on a single NVIDIA 4090 GPU with a batch
231"
SETTINGS,0.25,"size of 8 and averaged over 1024 samples. For language modeling tasks, we report the perplexity
232"
SETTINGS,0.25079617834394907,"calculated on the dataset with different models.
233"
SETTINGS,0.2515923566878981,"Table 1: Avarage inference time of a single sample with varying sampling steps. The table
compares the average inference time (in seconds) for the SEDD small model using both Euler and
Tweedie τ-leaping (abbreviated as T-τ) sampling methods, and the RADD small model using the
Euler method with a caching strategy."
SETTINGS,0.25238853503184716,"Methods
Metrics
32
64
128
256
512
1024
2048
4096"
SETTINGS,0.2531847133757962,"SEDD (euler)
Time(s)
0.48
0.87
1.67
3.25
6.41
12.74
25.42
50.86
PPL
155
105
81
66
53
43
35
28"
SETTINGS,0.25398089171974525,"SEDD (T-τ)
Time(s)
0.38
0.68
1.28
2.47
4.85
9.61
19.14
38.20
PPL
151
104
81
65
52
42
34
28"
SETTINGS,0.25477707006369427,"RADD
Time(s)
0.33
0.54
0.94
1.68
2.97
5.15
8.73
14.88
PPL
135
94
72
58
46
37
30
26"
EFFICIENT SAMPLING,0.25557324840764334,"4.2
Efficient sampling
234"
EFFICIENT SAMPLING,0.25636942675159236,"We compare the sample quality measured by perplexity between SEDD and our RADD-DCE model,
235"
EFFICIENT SAMPLING,0.2571656050955414,"as shown in Fig. 2. For a fixed NFE, RADD-DCE with the Euler sampler outperforms SEDD with
236"
EFFICIENT SAMPLING,0.25796178343949044,"multiple samplers. It suggests that RADD with caching accelerates the sampling process and benefits
237"
EFFICIENT SAMPLING,0.2587579617834395,"sample quality at the same time. Besides, the acceleration by cache strategy is particularly significant
238"
EFFICIENT SAMPLING,0.25955414012738853,"with large sampling steps, as analyzed in Section 3.2.
239"
EFFICIENT SAMPLING,0.2603503184713376,"We further compare the running time for the methods in Table 1. Across all sampling steps, RADD
240"
EFFICIENT SAMPLING,0.2611464968152866,"consistently requires the shortest sampling time and outperforms SEDD with different samplers.
241"
EFFICIENT SAMPLING,0.2619426751592357,"Quantitatively, RADD achieves a speed-up of 2.5 ∼3.5 times as shown in Table 1. These results
242"
EFFICIENT SAMPLING,0.2627388535031847,"agree with the analysis of the E-NFEs in Fig. 1, validate the effectiveness of RADD and caching
243"
EFFICIENT SAMPLING,0.2635350318471338,"strategy, and demonstrate the practical implications of our Theorem 1.
244"
EFFICIENT SAMPLING,0.2643312101910828,"According to Eq. (3.11), we can also use RADD as an auto-regressive model to generate samples in
245"
EFFICIENT SAMPLING,0.26512738853503187,"different orders, leading to worse performance as a discrete diffusion, as detailed in Appendix F.4.
246"
EFFICIENT SAMPLING,0.2659235668789809,"We present more sampling details in Appendix F.3. and the generated samples in Appendix G.1.
247"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.26671974522292996,"4.3
Improved zero-shot perplexity on language modeling
248"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.267515923566879,"Following SEDD, we present zero-shot perplexities on the LAMBADA, WikiText2, PTB, Wiki-
249"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.26831210191082805,"Text103, and 1 Billion Words datasets [37] in Table 2 and compare the zero-shot perplexity of our
250"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.26910828025477707,"model with other baseline models [20, 38, 29].
251"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.26990445859872614,"Firstly, we conduct an ablation study of the scaling trick in the middle of the Table 2. With an
252"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.27070063694267515,"absorbing process, the perplexity of the scaled version of SEDD outperforms its unscaled version,
253"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.2714968152866242,"which matches our theoretical discovery in Theorem 1.
254"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.27229299363057324,"Secondly, without any modification of the model, we estimate the exact likelihood of the baseline
255"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.2730891719745223,"model SEDD [29] based on Theorem 2 in Table 2. We observe that perplexity is consistently better
256"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.27388535031847133,"than the ELBO of the strongest discrete diffusion models, which validates our Theorem 2.
257"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.2746815286624204,"Lastly, we report the maximum likelihood training results of RADD in the last row in Table 2. We
258"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.2754777070063694,"observed that RADD-DCE outperforms RADD-DSE, but their performances are slightly worse than
259"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.2762738853503185,"SEDD. This discrepancy could be because we did not search the hyperparameters and directly applied
260"
IMPROVED ZERO-SHOT PERPLEXITY ON LANGUAGE MODELING,0.2770700636942675,"identical optimization configures as SEDD, which may be suboptimal.
261"
RELATED WORK,0.2778662420382166,"5
Related work
262"
RELATED WORK,0.2786624203821656,"Continouous-state diffusion models for text generation.
Several works have been proposed to
263"
RELATED WORK,0.27945859872611467,"apply continuous diffusion to text [19, 21, 22, 23]. Li et al. [19] use an embedding layer to map
264"
RELATED WORK,0.2802547770700637,"discrete tokens to a latent space and learn a continuous-state diffusion on it. Bit Diffusion [22] learns a
265"
RELATED WORK,0.28105095541401276,"continuous diffusion model to generate binary bits of discrete tokens. However, transforming between
266"
RELATED WORK,0.2818471337579618,"these continuous representations and discrete tokens by thresholding may lose information. Bayesian
267"
RELATED WORK,0.28264331210191085,"Flow Network [23] achieves competitive log-likelihood on character-level language modeling tasks
268"
RELATED WORK,0.28343949044585987,"Table 2: Zero-shot language modeling perplexity (↓) on five datasets. † labels the results based
on ELBO which is taken from [20, 38, 29] and ⋆labels the results based on the exact likelihood
implemented by us. In this table, SEDD-U / SEDD-S refer to the unscaled and scaled absorbing
models respectively."
RELATED WORK,0.28423566878980894,"Method
LAMBADA
WikiText2
PTB
WikiText103
1BW"
RELATED WORK,0.28503184713375795,"GPT-2
45.04
42.43
138.43
41.60
75.20"
RELATED WORK,0.285828025477707,"D3PM†
≤93.47
≤77.28
≤200.82
≤75.16
≤138.92
PLAID†
≤57.28
≤51.80
≤142.60
≤50.86
≤91.12
SEDD-Uniform†
≤65.40
≤50.27
≤140.12
≤49.60
≤101.37"
RELATED WORK,0.28662420382165604,"SEDD-U†
≤52.21
≤44.75
≤130.49
≤43.14
≤80.70
SEDD-S†
≤50.92
≤41.84
≤114.24
≤40.62
≤79.29"
RELATED WORK,0.2874203821656051,"SEDD-S⋆(Ours)
50.44
39.91
110.01
39.91
78.01"
RELATED WORK,0.28821656050955413,"RADD-DSE⋆(Ours)
96.62
43.35
125.03
40.34
80.11
RADD-DCE⋆(Ours)
56.67
42.83
116.74
41.02
79.00"
RELATED WORK,0.2890127388535032,"and is proven equivalent to continuous stochastic differential equations trained by denoising score
269"
RELATED WORK,0.2898089171974522,"matching [24]. Such models underperform auto-regressive models on standard text generation tasks.
270"
RELATED WORK,0.2906050955414013,"Discrete-state diffusion models for text generation.
Several discrete-state diffusion models have
271"
RELATED WORK,0.2914012738853503,"been proposed [11, 39, 20]. D3PM [20] proposed a diffusion framework based on any probability
272"
RELATED WORK,0.2921974522292994,"transition matrix and trained with a lower bound of log-likelihood. DiffusionBERT [25] utilizes a
273"
RELATED WORK,0.2929936305732484,"pre-trained BERT [40] as an initialization of diffusion. Furthermore, [26] generalizes the framework
274"
RELATED WORK,0.29378980891719747,"to continuous time by introducing a rate matrix. It is difficult to apply the score matching in such
275"
RELATED WORK,0.2945859872611465,"models because the gradient of the data distribution is undefined. Several works try to generalize the
276"
RELATED WORK,0.29538216560509556,"score matching on discrete data [29, 28, 26, 27]. Meng et al. [28] introduce the concrete score and the
277"
RELATED WORK,0.2961783439490446,"denoising concrete score matching loss. Furthermore, SEDD bridges the discrete state diffusion and
278"
RELATED WORK,0.29697452229299365,"the concrete score by introducing a denoising score entropy loss [29]. By incorporating an absorbing
279"
RELATED WORK,0.29777070063694266,"process, SEDD achieves competitive performance with the auto-regressive models, especially, GPT-2.
280"
CONCLUSION,0.29856687898089174,"6
Conclusion
281"
CONCLUSION,0.29936305732484075,"We introduce RADD, a dedicated discrete diffusion model that characterizes the time-independent
282"
CONCLUSION,0.3001592356687898,"conditional probabilities, built upon a new factorization form of the concrete score. RADD is much
283"
CONCLUSION,0.30095541401273884,"more efficient by reducing the NFEs with a cache strategy while retaining a better performance
284"
CONCLUSION,0.3017515923566879,"than strong baselines. Furthermore, we propose DCE loss and prove it is equivalent to the negative
285"
CONCLUSION,0.30254777070063693,"log-likelihood of absorbing diffusion. When applied to SEDD, DCE significantly advances the
286"
CONCLUSION,0.303343949044586,"state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks at the GPT-2 scale.
287"
CONCLUSION,0.304140127388535,"Limitaition.
Our model has been trained and evaluated primarily on the GPT-2 scale. For broader
288"
CONCLUSION,0.3049363057324841,"applicability, it is essential to explore the effects of scaling on the performance [41], which is left as
289"
CONCLUSION,0.3057324840764331,"future work. The success of diffusion transformers on images [42, 43, 44] and videos [45] suggests
290"
CONCLUSION,0.3065286624203822,"that diffusion models can be scaled up by incorporating transformers.
291"
CONCLUSION,0.3073248407643312,"Another limitation is that our model can only generate full-length outputs, unlike auto-regressive
292"
CONCLUSION,0.30812101910828027,"models that can produce variable-length outputs. This restricts the flexibility of our model in certain
293"
CONCLUSION,0.3089171974522293,"applications. We leave the investigation on this issue as future work.
294"
CONCLUSION,0.30971337579617836,"Social impact. For the current theoretical and experimental scope of this paper, we have not found any
295"
CONCLUSION,0.3105095541401274,"direct social impacts. However, considering future developments, the paper potentially contributes
296"
CONCLUSION,0.31130573248407645,"to the next-generation large language models. In this context, this work could significantly reduce
297"
CONCLUSION,0.31210191082802546,"the inference cost of language models but may also lead to hallucinations, amplify biases and
298"
CONCLUSION,0.31289808917197454,"discrimination in the data, and pose risks of misuse. As with other generative models, addressing
299"
CONCLUSION,0.31369426751592355,"these issues requires further advancements in the field.
300"
REFERENCES,0.3144904458598726,"References
301"
REFERENCES,0.31528662420382164,"[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
302"
REFERENCES,0.3160828025477707,"understanding by generative pre-training. 2018.
303"
REFERENCES,0.31687898089171973,"[2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
304"
REFERENCES,0.3176751592356688,"Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
305"
REFERENCES,0.3184713375796178,"[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
306"
REFERENCES,0.3192675159235669,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
307"
REFERENCES,0.3200636942675159,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
308"
REFERENCES,0.320859872611465,"[4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
309"
REFERENCES,0.321656050955414,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
310"
REFERENCES,0.32245222929936307,"processing systems, 30, 2017.
311"
REFERENCES,0.3232484076433121,"[5] OpenAI. ChatGPT: Optimizing Language Models for Dialogue. November 2022. URL
312"
REFERENCES,0.32404458598726116,"https://openai.com/blog/chatgpt/.
313"
REFERENCES,0.3248407643312102,"[6] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
314"
REFERENCES,0.32563694267515925,"Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
315"
REFERENCES,0.32643312101910826,"technical report. arXiv preprint arXiv:2303.08774, 2023.
316"
REFERENCES,0.32722929936305734,"[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
317"
REFERENCES,0.32802547770700635,"thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
318"
REFERENCES,0.3288216560509554,"Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
319"
REFERENCES,0.32961783439490444,"language models, 2023.
320"
REFERENCES,0.3304140127388535,"[8] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
321"
REFERENCES,0.33121019108280253,"Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.
322"
REFERENCES,0.3320063694267516,"arXiv preprint arXiv:2305.10403, 2023.
323"
REFERENCES,0.3328025477707006,"[9] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz
324"
REFERENCES,0.3335987261146497,"Korbak, and Owain Evans. The reversal curse: Llms trained on"" a is b"" fail to learn"" b is a"".
325"
REFERENCES,0.3343949044585987,"arXiv preprint arXiv:2309.12288, 2023.
326"
REFERENCES,0.3351910828025478,"[10] Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, and Rui Yan. Are we
327"
REFERENCES,0.3359872611464968,"falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse. arXiv
328"
REFERENCES,0.33678343949044587,"preprint arXiv:2311.07468, 2023.
329"
REFERENCES,0.3375796178343949,"[11] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
330"
REFERENCES,0.33837579617834396,"vised learning using nonequilibrium thermodynamics. In International conference on machine
331"
REFERENCES,0.339171974522293,"learning, pages 2256–2265. PMLR, 2015.
332"
REFERENCES,0.33996815286624205,"[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
333"
REFERENCES,0.34076433121019106,"in neural information processing systems, 33:6840–6851, 2020.
334"
REFERENCES,0.34156050955414013,"[13] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
335"
REFERENCES,0.34235668789808915,"Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv
336"
REFERENCES,0.3431528662420382,"preprint arXiv:2011.13456, 2020.
337"
REFERENCES,0.34394904458598724,"[14] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
338"
REFERENCES,0.3447452229299363,"preprint arXiv:2010.02502, 2020.
339"
REFERENCES,0.34554140127388533,"[15] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the
340"
REFERENCES,0.3463375796178344,"optimal reverse variance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503,
341"
REFERENCES,0.3471337579617834,"2022.
342"
REFERENCES,0.3479299363057325,"[16] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential
343"
REFERENCES,0.3487261146496815,"integrator. arXiv preprint arXiv:2204.13902, 2022.
344"
REFERENCES,0.3495222929936306,"[17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver:
345"
REFERENCES,0.3503184713375796,"A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in
346"
REFERENCES,0.35111464968152867,"Neural Information Processing Systems, 35:5775–5787, 2022.
347"
REFERENCES,0.3519108280254777,"[18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
Dpm-
348"
REFERENCES,0.35270700636942676,"solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint
349"
REFERENCES,0.3535031847133758,"arXiv:2211.01095, 2022.
350"
REFERENCES,0.35429936305732485,"[19] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto.
351"
REFERENCES,0.35509554140127386,"Diffusion-lm improves controllable text generation, 2022.
352"
REFERENCES,0.35589171974522293,"[20] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Struc-
353"
REFERENCES,0.35668789808917195,"tured denoising diffusion models in discrete state-spaces. In Advances in Neural Information
354"
REFERENCES,0.357484076433121,"Processing Systems, 2021.
355"
REFERENCES,0.35828025477707004,"[21] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin,
356"
REFERENCES,0.3590764331210191,"Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis
357"
REFERENCES,0.35987261146496813,"Hawthorne, Rémi Leblond, Will Grathwohl, and Jonas Adler. Continuous diffusion for categor-
358"
REFERENCES,0.3606687898089172,"ical data, 2022.
359"
REFERENCES,0.3614649681528662,"[22] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using
360"
REFERENCES,0.3622611464968153,"diffusion models with self-conditioning, 2023.
361"
REFERENCES,0.3630573248407643,"[23] Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian
362"
REFERENCES,0.3638535031847134,"flow networks, 2024.
363"
REFERENCES,0.3646496815286624,"[24] Kaiwen Xue, Yuhao Zhou, Shen Nie, Xu Min, Xiaolu Zhang, Jun Zhou, and Chongxuan Li.
364"
REFERENCES,0.36544585987261147,"Unifying bayesian flow networks and diffusion models through stochastic differential equations,
365"
REFERENCES,0.3662420382165605,"2024.
366"
REFERENCES,0.36703821656050956,"[25] Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusion-
367"
REFERENCES,0.3678343949044586,"bert: Improving generative masked language models with diffusion models. arXiv preprint
368"
REFERENCES,0.36863057324840764,"arXiv:2211.15029, 2022.
369"
REFERENCES,0.36942675159235666,"[26] Andrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and
370"
REFERENCES,0.37022292993630573,"A. Doucet. A continuous time framework for discrete denoising models. In Advances in Neural
371"
REFERENCES,0.37101910828025475,"Information Processing Systems, 2022.
372"
REFERENCES,0.3718152866242038,"[27] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time
373"
REFERENCES,0.37261146496815284,"discrete diffusion models, 2023.
374"
REFERENCES,0.3734076433121019,"[28] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching:
375"
REFERENCES,0.37420382165605093,"Generalized score matching for discrete data, 2023.
376"
REFERENCES,0.375,"[29] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the
377"
REFERENCES,0.37579617834394907,"ratios of the data distribution, 2024.
378"
REFERENCES,0.3765923566878981,"[30] William J Anderson. Continuous-time Markov chains: An applications-oriented approach.
379"
REFERENCES,0.37738853503184716,"Springer Science & Business Media, 2012.
380"
REFERENCES,0.3781847133757962,"[31] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time
381"
REFERENCES,0.37898089171974525,"discrete diffusion models. In The Eleventh International Conference on Learning Representa-
382"
REFERENCES,0.37977707006369427,"tions, 2023.
383"
REFERENCES,0.38057324840764334,"[32] Frank Kelly.
Reversibility and stochastic networks.
1980.
URL https://api.
384"
REFERENCES,0.38136942675159236,"semanticscholar.org/CorpusID:125211322.
385"
REFERENCES,0.3821656050955414,"[33] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/
386"
REFERENCES,0.38296178343949044,"OpenWebTextCorpus, 2019.
387"
REFERENCES,0.3837579617834395,"[34] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,
388"
REFERENCES,0.38455414012738853,"Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA
389"
REFERENCES,0.3853503184713376,"dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual
390"
REFERENCES,0.3861464968152866,"Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
391"
REFERENCES,0.3869426751592357,"1525–1534, Berlin, Germany, August 2016. Association for Computational Linguistics. URL
392"
REFERENCES,0.3877388535031847,"http://www.aclweb.org/anthology/P16-1144.
393"
REFERENCES,0.3885350318471338,"[35] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
394"
REFERENCES,0.3893312101910828,"models, 2016.
395"
REFERENCES,0.39012738853503187,"[36] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and
396"
REFERENCES,0.3909235668789809,"Tony Robinson. One billion word benchmark for measuring progress in statistical language
397"
REFERENCES,0.39171974522292996,"modeling, 2014.
398"
REFERENCES,0.392515923566879,"[37] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
399"
REFERENCES,0.39331210191082805,"models are unsupervised multitask learners. 2019. URL https://api.semanticscholar.
400"
REFERENCES,0.39410828025477707,"org/CorpusID:160025533.
401"
REFERENCES,0.39490445859872614,"[38] Ishaan Gulrajani and Tatsunori Hashimoto. Likelihood-based diffusion language models. In
402"
REFERENCES,0.39570063694267515,"Advances in Neural Information Processing Systems, 2023.
403"
REFERENCES,0.3964968152866242,"[39] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax
404"
REFERENCES,0.39729299363057324,"flows and multinomial diffusion: Learning categorical distributions. Advances in Neural
405"
REFERENCES,0.3980891719745223,"Information Processing Systems, 34:12454–12465, 2021.
406"
REFERENCES,0.39888535031847133,"[40] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
407"
REFERENCES,0.3996815286624204,"deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
408"
REFERENCES,0.4004777070063694,"2018.
409"
REFERENCES,0.4012738853503185,"[41] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
410"
REFERENCES,0.4020700636942675,"Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
411"
REFERENCES,0.4028662420382166,"Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia
412"
REFERENCES,0.4036624203821656,"Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre.
413"
REFERENCES,0.40445859872611467,"Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https:
414"
REFERENCES,0.4052547770700637,"//api.semanticscholar.org/CorpusID:247778764.
415"
REFERENCES,0.40605095541401276,"[42] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth
416"
REFERENCES,0.4068471337579618,"words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on
417"
REFERENCES,0.40764331210191085,"Computer Vision and Pattern Recognition, pages 22669–22679, 2023.
418"
REFERENCES,0.40843949044585987,"[43] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings
419"
REFERENCES,0.40923566878980894,"of the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023.
420"
REFERENCES,0.41003184713375795,"[44] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao,
421"
REFERENCES,0.410828025477707,"Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale.
422"
REFERENCES,0.41162420382165604,"arXiv preprint arXiv:2303.06555, 2023.
423"
REFERENCES,0.4124203821656051,"[45] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min
424"
REFERENCES,0.41321656050955413,"Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled
425"
REFERENCES,0.4140127388535032,"text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.
426"
REFERENCES,0.4148089171974522,"[46] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
427"
REFERENCES,0.4156050955414013,"Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing
428"
REFERENCES,0.4164012738853503,"Systems, 2017. URL https://api.semanticscholar.org/CorpusID:13756489.
429"
REFERENCES,0.4171974522292994,"[47] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
430"
REFERENCES,0.4179936305732484,"deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
431"
REFERENCES,0.41878980891719747,"ence of the North American Chapter of the Association for Computational Linguistics: Human
432"
REFERENCES,0.4195859872611465,"Language Technologies, Volume 1 (Long and Short Papers). Association for Computational
433"
REFERENCES,0.42038216560509556,"Linguistics, 2019.
434"
REFERENCES,0.4211783439490446,"[48] William S. Peebles and Saining Xie. Scalable diffusion models with transformers. In Interna-
435"
REFERENCES,0.42197452229299365,"tional Conference on Computer Vision, 2023.
436"
REFERENCES,0.42277070063694266,"[49] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer
437"
REFERENCES,0.42356687898089174,"with rotary position embedding. Neurocomputing, 2021.
438"
REFERENCES,0.42436305732484075,"A
Proof of Proposition 1
439"
REFERENCES,0.4251592356687898,"Since the different dimensions of the forward diffusion process are independent of each other, we
440"
REFERENCES,0.42595541401273884,"can first analyze the conditional distribution in one dimension. This can be derived directly from
441"
REFERENCES,0.4267515923566879,"Eq. (2.3), but for a better understanding, here we provide a more intuitive proof in the case when
442"
REFERENCES,0.42754777070063693,"Qt = Qabsorb.
443"
REFERENCES,0.428343949044586,"Lemma 1. (Analytic conditional distribution for absorbing case) Suppose {Xt} is a continuous
444"
REFERENCES,0.429140127388535,"time Markov chain with transition rate matrix Qt = σ(t)Qabsorb, given the value x at time zero , the
445"
REFERENCES,0.4299363057324841,"conditional distribution pt|0(xt|x) has the following analytic form:
446"
REFERENCES,0.4307324840764331,"pt|0(xt|x) = 
 "
REFERENCES,0.4315286624203822,"e−¯σ(t),
xt = x,
1 −e−¯σ(t),
xt = [M],
0.
xt ̸= [M] and xt ̸= [M].
(A.1)"
REFERENCES,0.4323248407643312,"Proof. Given the initial value x ∈X = {1, · · · , N}, we have
447"
REFERENCES,0.43312101910828027,"xt =
x,
t < Th,
[M],
t ≥Th,"
REFERENCES,0.4339171974522293,"where Th is the holding time before the system transitions to the next state.
448"
REFERENCES,0.43471337579617836,"Based on the properties of the Qabsorb:
449"
REFERENCES,0.4355095541401274,"pt+∆t|t(x|x) = 1 −(−σ(t)Qabsorb(x, x))∆t + o(∆t).
(A.2)"
REFERENCES,0.43630573248407645,"Partitioning the interval [0, t] into {sk}n
k=0 , make use of Memoryless Property of Continuous-Time
450"
REFERENCES,0.43710191082802546,"Markov Chains:
451"
REFERENCES,0.43789808917197454,"pt|0(x|x) = n
Y"
REFERENCES,0.43869426751592355,"k=1
psk|sk−1(x|x)
(A.3) = n
Y"
REFERENCES,0.4394904458598726,"k=1
(1 −(−σ(tk)Qabsorb(x, x))(sk −sk−1) + o((sk −sk−1)))
(A.4)"
REFERENCES,0.44028662420382164,"= exp( n
X"
REFERENCES,0.4410828025477707,"k=1
ln(1 −(−σ(tk)Qabsorb(x, x))(sk −sk−1) + o((sk −sk−1)))
(A.5)"
REFERENCES,0.44187898089171973,"= exp( n
X"
REFERENCES,0.4426751592356688,"k=1
−(−σ(tk)Qabsorb(x, x))(sk −sk−1) + o((sk −sk−1))).
(A.6)"
REFERENCES,0.4434713375796178,"Let max(sk −sk−1) →0 , we have:
452"
REFERENCES,0.4442675159235669,"pt|0(x|x) = exp(−
Z t"
REFERENCES,0.4450636942675159,"0
−σ(s)Qabsorb(x, x)ds) = exp(−(−Qabsorb(x, x)¯σ(t))).
(A.7)"
REFERENCES,0.445859872611465,"As Qabsorb(x, x) = −1, we have
453"
REFERENCES,0.446656050955414,"pt|0(x|x) = P(Th > t) = e−¯σ(t),
(A.8) 454"
REFERENCES,0.44745222929936307,"pt|0([M]|x) = P(Th > t) = 1 −e−¯σ(t),
(A.9)
455"
REFERENCES,0.4482484076433121,"pt|0(k|x) = 0
if k ̸= [M] and k ̸= x.
(A.10) 456"
REFERENCES,0.44904458598726116,"Proposition 1. (Analytic joint distribution for absorbing case)
457"
REFERENCES,0.4498407643312102,"Suppose {Xt} is a continuous time Markov chain with transition rate matrix Qt = σ(t)Qabsorb. For
458"
REFERENCES,0.45063694267515925,"xt = x1
t · · · xd
t with N1 components as [M] and N2 = d −N1 components as specific value, pt(xt)
459"
REFERENCES,0.45143312101910826,"can be expressed as Eq. (A.11):
460"
REFERENCES,0.45222929936305734,"pt(xt) = [1 −e−¯σ(t)]N1[e−¯σ(t)]N2p0(xUM
t
),
(A.11)"
REFERENCES,0.45302547770700635,"where xUM
t
:= {xk|xk ̸= [M]} represents unmasked part of xUM
t
.
461"
REFERENCES,0.4538216560509554,"Proposition 1 shows that the joint distribution pt(xt) can be expressed as the multiplication of two
462"
REFERENCES,0.45461783439490444,"terms. One is an analytic term only depending on time, the other is a joint distribution of clean data
463"
REFERENCES,0.4554140127388535,"p0(xUM
t
) with N2 dimensions independent of time.
464"
REFERENCES,0.45621019108280253,"Proof. Without loss of generality, let’s assume that the preceding N1 terms of x are all [M], and the
465"
REFERENCES,0.4570063694267516,"remaining N2 terms are fixed at specific values. That is, xt = [M] · · · [M]xN1+1
t
· · · xd
t , and here xk
466"
REFERENCES,0.4578025477707006,"is a fixed value in X.
467"
REFERENCES,0.4585987261146497,"Use the law of total probability and Lemma 1, along with independent property:
468"
REFERENCES,0.4593949044585987,"pt([M] · · · [M]xN1+1
t
· · · xd
t ) =
X"
REFERENCES,0.4601910828025478,"x0∈X d
pt|0([M] · · · [M]xN1+1
t
· · · xd
t |x0)p0(x0) =
X"
REFERENCES,0.4609872611464968,"x1
0∈X,··· ,xd
0∈X
pt|0([M] · · · [M]xN1+1
t
· · · xd
t |x1
0 · · · xd
0)p0(x1
0 · · · xd
0) =
X"
REFERENCES,0.46178343949044587,"x1
0∈X,··· ,xd
0∈X N1
Y"
REFERENCES,0.4625796178343949,"k=1
pk
t|0([M]|xk
0) d
Y"
REFERENCES,0.46337579617834396,"k=N1+1
pk
t|0(xk
t |xk
0)p0(x1
0 · · · xd
0) =
X"
REFERENCES,0.464171974522293,"x1
0∈X,··· ,xN1
0
∈X N1
Y"
REFERENCES,0.46496815286624205,"k=1
pk
t|0([M]|xk
0)[e−¯σ(t)]N2p0(x1
0 · · · xN1
0 xN1+1
t
· · · xd
t ) =
X"
REFERENCES,0.46576433121019106,"x1
0∈X,··· ,xN1
0
∈X
[1 −e−¯σ(t)]N1[e−¯σ(t)]N2p0(x1
0 · · · xN1
0 xN1+1
t
· · · xd
t )"
REFERENCES,0.46656050955414013,"=[1 −e−¯σ(t)]N1[e−¯σ(t)]N2
X"
REFERENCES,0.46735668789808915,"x1
0∈X,··· ,xN1
0
∈X
p0(x1
0 · · · xN1
0 xN1+1
t
· · · xd
t )"
REFERENCES,0.4681528662420382,"=[1 −e−¯σ(t)]N1[e−¯σ(t)]N2p0(xN1+1
t
· · · xd
t )."
REFERENCES,0.46894904458598724,"In the general case, we have:
469"
REFERENCES,0.4697452229299363,"pt(xt) = [1 −e−¯σ(t)]N1[e−¯σ(t)]N2p0(xUM
t
),
which shows that the likelihood of noisy data x at time t equals the likelihood of unmasked part of x
470"
REFERENCES,0.47054140127388533,"at time 0 multiplied by a analytic time-dependent term.
471"
REFERENCES,0.4713375796178344,"B
Proof of Theorem 1
472"
REFERENCES,0.4721337579617834,"Theorem 1. (Analytic concrete score in absorbing case, proof in Appendix B) For xt
=
473"
REFERENCES,0.4729299363057325,"x1
t . . . xi
t . . . xd
t and ˆxt = x1
t . . . bxi
t . . . xd
t , if xi
t = [M] and ˆxi
t ̸= [M], the concrete score at time
474"
REFERENCES,0.4737261146496815,"t can be expressed as a time-independent conditional distribution at time zero multiplied by an
475"
REFERENCES,0.4745222929936306,"analytic time-dependent term:
476"
REFERENCES,0.4753184713375796,"pt
 
x1
t . . . bxi
t . . . xd
t
"
REFERENCES,0.47611464968152867,"pt
 
x1
t . . . xi
t . . . xd
t
 =
e−¯σ(t)"
REFERENCES,0.4769108280254777,"1 −e−¯σ(t) p0(ˆxi
t|xUM
t
),"
REFERENCES,0.47770700636942676,"where xUM
t
is the vector consists of all unmasked tokens of xt.
477"
REFERENCES,0.4785031847133758,"Proof. According to Proposition 1, if xi
t = [M] and ˆxi
t ̸= [M], ˆxUM
t
= (xUM
t
, ˆxi
t),
478"
REFERENCES,0.47929936305732485,"pt(ˆxt)
pt(xt) =[1 −e−¯σ(t)]N1−1[e−¯σ(t)]N2+1p0(ˆxUM
t
)
[1 −e−¯σ(t)]N1[e−¯σ(t)]N2p0(xUM
t
)"
REFERENCES,0.48009554140127386,"=[1 −e−¯σ(t)]N1−1[e−¯σ(t)]N2+1p0(xUM
t
, ˆxi
t)
[1 −e−¯σ(t)]N1[e−¯σ(t)]N2p0(xUM
t
)"
REFERENCES,0.48089171974522293,"=
e−¯σ(t)"
REFERENCES,0.48168789808917195,"1 −e−¯σ(t) p0(ˆxi
t|xUM
t
). 479"
REFERENCES,0.482484076433121,"C
Proof of Theorem 2
480"
REFERENCES,0.48328025477707004,"C.1
Denoising cross-entropy loss by λ
481"
REFERENCES,0.4840764331210191,"According to definition of Qt we can simplify Eq. (3.8) as:
482"
REFERENCES,0.48487261146496813,"L∞
DCE(x0) =
Z ∞"
REFERENCES,0.4856687898089172,"0
E˜x∼pt|0(˜x|x0)  
X"
REFERENCES,0.4864649681528662,"˜xi=[M],j̸=[M]
σ(t)

−
e−¯σ(t)"
REFERENCES,0.4872611464968153,"1 −e−¯σ(t) I(xi
0 = j) log

e−¯σ(t)"
REFERENCES,0.4880573248407643,"1 −e−¯σ(t) cθ(˜x)[i, j]
 dt =
Z ∞"
REFERENCES,0.4888535031847134,"0
E˜x∼pt|0(˜x|x0)  X"
REFERENCES,0.4896496815286624,"˜xi=[M]
σ(t)

−
e−¯σ(t)"
REFERENCES,0.49044585987261147,"1 −e−¯σ(t) log

e−¯σ(t)"
REFERENCES,0.4912420382165605,"1 −e−¯σ(t) cθ(˜x)[i, xi
0]
 dt =
Z ∞"
REFERENCES,0.49203821656050956,"0
E˜x∼pt|0(˜x|x0)  X"
REFERENCES,0.4928343949044586,"˜xi=[M]
σ(t)

−
e−¯σ(t)"
REFERENCES,0.49363057324840764,"1 −e−¯σ(t) log

e−¯σ(t)"
REFERENCES,0.49442675159235666,"1 −e−¯σ(t) qθ(xi
0|˜xUM)
 dt."
REFERENCES,0.49522292993630573,"Define λ(t) = 1 −e−¯σ(t), dλ = σ(t)e−¯σ(t)dt. As ¯σ(t) =
R t
0 σ(τ)dτ, we have λ(0) = 0,
483"
REFERENCES,0.49601910828025475,"limt→∞λ(t) = 1. By a change of variables for the integration variable from t to λ, we can
484"
REFERENCES,0.4968152866242038,"rewrite the above equation as:
485 Z 1 0"
REFERENCES,0.49761146496815284,"1
λE˜x∼pλ(˜x|x0)  X"
REFERENCES,0.4984076433121019,˜xi=[M]
REFERENCES,0.49920382165605093,"
−log(1 −λ"
REFERENCES,0.5,"λ
qθ(xi
0|˜xUM))
 dλ =
Z 1 0"
REFERENCES,0.5007961783439491,"1
λE˜x∼pλ(˜x|x0)
X"
REFERENCES,0.5015923566878981,˜xi=[M]
REFERENCES,0.5023885350318471," 
−log(qθ(xi
0|˜xUM))

dλ +
Z 1 0"
REFERENCES,0.5031847133757962,"1
λE˜x∼pλ(˜x|x0)
X"
REFERENCES,0.5039808917197452,˜xi=[M]
REFERENCES,0.5047770700636943,"
−log(1 −λ"
REFERENCES,0.5055732484076433,"λ
)

dλ."
REFERENCES,0.5063694267515924,"By independence of forward process and Lemma 1, pt|0(˜x|x0) = Qd
i=1 pt|0(˜xi|xi
0) where
486"
REFERENCES,0.5071656050955414,"pt|0(˜xi|xi
0) = 
 "
REFERENCES,0.5079617834394905,"1 −e−¯σ(t)
˜xi = [M],
e−¯σ(t)
˜xi = xi
0,
0
else.
(C.1)"
REFERENCES,0.5087579617834395,"Therefore, pλ(˜x|x0) = Qd
i=1 pλ(˜xi|xi
0) where
487"
REFERENCES,0.5095541401273885,"pλ(˜xi|xi
0) = 
 "
REFERENCES,0.5103503184713376,"λ
˜xi = [M],
1 −λ
˜xi = xi
0,
0
else.
(C.2)"
REFERENCES,0.5111464968152867,"Consider the second term, we have:
488 Z 1 0"
REFERENCES,0.5119426751592356,"1
λE˜x∼pλ(˜x|x0)  X"
REFERENCES,0.5127388535031847,˜xi=[M]
REFERENCES,0.5135350318471338,"
−log(1 −λ"
REFERENCES,0.5143312101910829,"λ
)
 dλ =
Z 1 0"
REFERENCES,0.5151273885350318,"1
λE˜x∼pλ(˜x|x0) "" d
X"
REFERENCES,0.5159235668789809,"i=1
I(˜xi = [M])

−log(1 −λ"
REFERENCES,0.51671974522293,"λ
)
# dλ =
Z 1 0 1
λ "" d
X"
REFERENCES,0.517515923566879,"i=1
pλ(˜xi = [M]|x0)

−log(1 −λ"
REFERENCES,0.518312101910828,"λ
)
# dλ"
REFERENCES,0.5191082802547771,"=d
Z 1"
REFERENCES,0.5199044585987261,"0
−log(1 −λ λ
)dλ"
REFERENCES,0.5207006369426752,"=d (λ log λ + (1 −λ) log(1 −λ)) |1
0."
REFERENCES,0.5214968152866242,"Note that:
lim
λ→0 λ log λ = lim
λ→1(1 −λ) log(1 −λ) = 0,"
REFERENCES,0.5222929936305732,"therefore, (λ log λ + (1 −λ) log(1 −λ))|1
0 = 0.
489"
REFERENCES,0.5230891719745223,"L∞
DCE(x0) =
Z 1 0"
REFERENCES,0.5238853503184714,"1
λE˜x∼pλ(˜x|x0)  X"
REFERENCES,0.5246815286624203,˜xi=[M]
REFERENCES,0.5254777070063694," 
−log(qθ(xi
0|˜xUM))

"
REFERENCES,0.5262738853503185,"dλ.
(C.3)"
REFERENCES,0.5270700636942676,"C.2
Denoising cross-entropy loss by k
490"
REFERENCES,0.5278662420382165,"By Eq. (C.3), we can express the loss in terms of λ.
Given x0, we denote
˜
X
:=
491"
REFERENCES,0.5286624203821656,"{x1
0, [M]} × · · · {xd
0, [M]} as the sample space of ˜x, and define
˜
Xk
:= {˜x :
˜x ∈
˜
X ∧
492"
REFERENCES,0.5294585987261147,"˜x has exact k dimensions with values[M]}. Obviously, | ˜
X| = 2d and | ˜
Xk| = Ck
d . We have:
493 Z 1 0"
REFERENCES,0.5302547770700637,"1
λE˜x∼pλ(˜x|x0)  X"
REFERENCES,0.5310509554140127,˜xi=[M]
REFERENCES,0.5318471337579618," 
−log(qθ(xi
0|˜xUM))

"
REFERENCES,0.5326433121019108,"dλ
(C.4) =
Z 1 0 1
λ X"
REFERENCES,0.5334394904458599,"˜x∈˜
X
pλ(˜x|x0)  X"
REFERENCES,0.5342356687898089,˜xi=[M]
REFERENCES,0.535031847133758," 
−log(qθ(xi
0|˜xUM))

"
REFERENCES,0.535828025477707,"dλ
(C.5) =
Z 1 0 1
λ d
X k=0 X"
REFERENCES,0.5366242038216561,"˜x∈˜
Xk
λk(1 −λ)d−k  X"
REFERENCES,0.5374203821656051,˜xi=[M]
REFERENCES,0.5382165605095541," 
−log(qθ(xi
0|˜xUM))

"
REFERENCES,0.5390127388535032,"dλ
(C.6) =
Z 1 0 1
λ d
X k=1 X"
REFERENCES,0.5398089171974523,"˜x∈˜
Xk
λk(1 −λ)d−k  X"
REFERENCES,0.5406050955414012,˜xi=[M]
REFERENCES,0.5414012738853503," 
−log(qθ(xi
0|˜xUM))

"
REFERENCES,0.5421974522292994,"dλ
(C.7) = d
X k=1 Z 1"
REFERENCES,0.5429936305732485,"0
λk−1(1 −λ)d−kdλ
X"
REFERENCES,0.5437898089171974,"˜x∈˜
Xk  X"
REFERENCES,0.5445859872611465,˜xi=[M]
REFERENCES,0.5453821656050956," 
−log(qθ(xi
0|˜xUM))

"
REFERENCES,0.5461783439490446,"
(C.8) = d
X k=1"
REFERENCES,0.5469745222929936,(k −1)!(d −k)! d! X
REFERENCES,0.5477707006369427,"˜x∈˜
Xk  X"
REFERENCES,0.5485668789808917,˜xi=[M]
REFERENCES,0.5493630573248408," 
−log(qθ(xi
0|˜xUM))

"
REFERENCES,0.5501592356687898,"
(C.9) = d
X k=1"
"KCK
D",0.5509554140127388,"1
kCk
d X"
"KCK
D",0.5517515923566879,"˜x∈˜
Xk  X"
"KCK
D",0.552547770700637,˜xi=[M]
"KCK
D",0.553343949044586," 
−log(qθ(xi
0|˜xUM))

"
"KCK
D",0.554140127388535,".
(C.10)"
"KCK
D",0.5549363057324841,"This can be reformulated in the form of expectation:
494 d
X k=1"
"KCK
D",0.5557324840764332,"1
kCk
d X"
"KCK
D",0.5565286624203821,"˜x∈˜
Xk  X"
"KCK
D",0.5573248407643312,˜xi=[M]
"KCK
D",0.5581210191082803," 
−log(qθ(xi
0|˜xUM))

"
"KCK
D",0.5589171974522293,"
(C.11) = d
X k=1"
"KCK
D",0.5597133757961783,"1
k E˜x∼U( ˜
Xk)  X"
"KCK
D",0.5605095541401274,˜xi=[M]
"KCK
D",0.5613057324840764," 
−log(qθ(xi
0|˜xUM))

"
"KCK
D",0.5621019108280255,"
(C.12)"
"KCK
D",0.5628980891719745,"=dEk∼U({1,··· ,d})
1
k E˜x∼U( ˜
Xk)  X"
"KCK
D",0.5636942675159236,˜xi=[M]
"KCK
D",0.5644904458598726," 
−log(qθ(xi
0|˜xUM))

"
"KCK
D",0.5652866242038217,".
(C.13)"
"KCK
D",0.5660828025477707,"C.3
Exact negative likelihood
495"
"KCK
D",0.5668789808917197,"Let Sd represent the set of all permutations of the integers 1, · · · , d, and let π ∈Sd be one of these
496"
"KCK
D",0.5676751592356688,"permutations. Then, we can express log qθ(x0) as follows:
497"
"KCK
D",0.5684713375796179,"log qθ(x0) = Eπ∼U(Sd) log qθ(x0)
(C.14)"
"KCK
D",0.5692675159235668,"= Eπ∼U(Sd) d
X"
"KCK
D",0.5700636942675159,"l=1
log qθ(xπ(l)
0
|xπ(<l)
0
)
(C.15) = d
X"
"KCK
D",0.570859872611465,"l=1
Eπ∼U(Sd) log qθ(xπ(l)
0
|xπ(<l)
0
)
(C.16) = d
X l=1"
"KCK
D",0.571656050955414,"1
d −l + 1Eπ∼U(Sd) d
X"
"KCK
D",0.572452229299363,"r=l
log qθ(xπ(r)
0
|xπ(<l)
0
)
(C.17) = d
X k=1"
"KCK
D",0.5732484076433121,"1
k Eπ∼U(Sd) d
X"
"KCK
D",0.5740445859872612,"r=d−k+1
log qθ(xπ(r)
0
|xπ(<d−k+1)
0
)
(C.18)"
"KCK
D",0.5748407643312102,"= dEk∼U({1,··· ,d})
1
k Eπ∼U(Sd) d
X"
"KCK
D",0.5756369426751592,"r=d−k+1
log qθ(xπ(r)
0
|xπ(<d−k+1)
0
).
(C.19)"
"KCK
D",0.5764331210191083,"In this context, for a fixed k, the condition xπ(<d−k+1)
0
can be understood as the unmasked part of
498"
"KCK
D",0.5772292993630573,"noisy data ˜xUM. For r = d −k + 1, · · · , d, xπ(r)
0
corresponds to the k items of the masked part.
499"
"KCK
D",0.5780254777070064,"Therefore, we have:
500"
"KCK
D",0.5788216560509554,"Eπ∼U(Sd) d
X"
"KCK
D",0.5796178343949044,"r=d−k+1
log qθ(xπ(r)
0
|xπ(<d−k+1)
0
) = E˜x∼U( ˜
Xk)
X"
"KCK
D",0.5804140127388535,"˜xi=[M]
log qθ(xi
0|˜xUM).
(C.20)"
"KCK
D",0.5812101910828026,"Thus, substituting back, we have:
501"
"KCK
D",0.5820063694267515,"−log qθ(x0) = dEk∼U({1,··· ,d})
1
k E˜x∼U( ˜
Xk)
X"
"KCK
D",0.5828025477707006,"˜xi=[M]
−log qθ(xi
0|˜xUM),
(C.21)"
"KCK
D",0.5835987261146497,"which is exactly Eq. (C.13).
502"
"KCK
D",0.5843949044585988,"This concludes the proof of the exact negative likelihood, showing the equivalence between the
503"
"KCK
D",0.5851910828025477,"expected negative log-likelihood and the denoising cross-entropy formulation.
504"
"KCK
D",0.5859872611464968,"D
Sampling methods of discrete diffusion
505"
"KCK
D",0.5867834394904459,"D.1
Original form in discrete diffusion
506"
"KCK
D",0.5875796178343949,"Euler discrete method
According to the Eq. (2.7), take t = s −∆s and use the Euler method, we
507"
"KCK
D",0.5883757961783439,"can simulate the reverse process by iteratively taking small ∆t Euler steps at time s, calculate the
508"
"KCK
D",0.589171974522293,"reverse transition rate based on sθ(xs, s), and randomly sampling xs−∆s.
509"
"KCK
D",0.589968152866242,"Left term:
510"
"KCK
D",0.5907643312101911,"d
dtPs→t
|t=s−∆s ≈Ps→s−∆s −Ps→s"
"KCK
D",0.5915605095541401,"∆s
.
(D.1)"
"KCK
D",0.5923566878980892,"Right term:
511"
"KCK
D",0.5931528662420382,"Ps→t ˜Qt|t=s−∆s ≈Ps→s ˜Qs.
(D.2)"
"KCK
D",0.5939490445859873,"As Ps→s = I, we have
512"
"KCK
D",0.5947452229299363,"Ps→s−∆s ≈I + ˜Qs∆s.
(D.3)"
"KCK
D",0.5955414012738853,"Rewrite in t and consider a specific input xt, xt−∆t is sampled from the following transition
513"
"KCK
D",0.5963375796178344,"probabilities:
514"
"KCK
D",0.5971337579617835,"pt−∆t|t(xt−∆t|xt) ≈δxtxt−∆t + ˜Qt(xt, xt−∆t)∆t + O(∆t)
(D.4)"
"KCK
D",0.5979299363057324,"≈δxtxt−∆t + ˜Qt(xt, xt−∆t)∆t,
(D.5)"
"KCK
D",0.5987261146496815,"where
515"
"KCK
D",0.5995222929936306,"˜Qt(xt, xt−∆t) ≈"
"KCK
D",0.6003184713375797,"(
Qt(xt−∆t, xt)sθ(xt, t)xt−∆t
xt ̸= xt−∆t,
−P"
"KCK
D",0.6011146496815286,"k̸=xt ˜Qt(xt, k)
xt = xt−∆t.
(D.6)"
"KCK
D",0.6019108280254777,"Tweedie τ -leaping
If we know the analytic form of Ps→t, it is possible to get the closed form
516"
"KCK
D",0.6027070063694268,"of reverse probability Pt→s for any s < t. According to the conditional decomposition of total
517"
"KCK
D",0.6035031847133758,"probability, we have:
518"
"KCK
D",0.6042993630573248,"diag
 
P T
t

Pt→s =
 
diag
 
P T
s

Ps→t
T .
(D.7)"
"KCK
D",0.6050955414012739,"As P T
s Ps→t = P T
t , the following equation holds:
519"
"KCK
D",0.6058917197452229,"Pt→s = diag
 
P T
t
−1 P T
s→tdiag
 
P T
s

= diag
 
P T
t
−1 P T
s→tdiag
 
P T
t P −1
s→t

.
(D.8)"
"KCK
D",0.606687898089172,"Given xt, to get ps|t(xs|xt), we only need to calculate row xt of Pt→s :
520"
"KCK
D",0.607484076433121,"Pt→s(xt, ·) =
1
pt(xt)P T
s→t(xt, ·) ⊙(P T
t P −1
s→t)
(D.9)"
"KCK
D",0.60828025477707,"= P T
s→t(xt, ·) ⊙( P T
t
pt(xt)P −1
s→t) ≈P T
s→t(xt, ·) ⊙(sθ(xt, t)T P −1
s→t).
(D.10)"
"KCK
D",0.6090764331210191,"D.2
Simplified form in reparameterized absorbing discrete diffusion
521"
"KCK
D",0.6098726114649682,"Euler discrete method
For xt
=
[M], given the value of ˆxt, use the Qt(ˆxt, xt)
=
522"
"KCK
D",0.6106687898089171,"σ(t)Qabsorb(ˆxt, xt) and sθ(xt, t)ˆxt =
e−¯σ(t)"
"KCK
D",0.6114649681528662,"1−e−¯σ(t) cθ(xt)ˆxt. Eq. (D.5) can be simplified as:
523"
"KCK
D",0.6122611464968153,pt−∆t|t(ˆxt|[M]) =
"KCK
D",0.6130573248407644,"(
σ(t)
e−¯σ(t)"
"KCK
D",0.6138535031847133,"1−e−¯σ(t) ∆tcθ(xt)ˆxt
if ˆxt ̸= [M],"
"KCK
D",0.6146496815286624,"1 −σ(t)
e−¯σ(t)"
"KCK
D",0.6154458598726115,"1−e−¯σ(t) ∆t
if ˆxt = [M].
(D.11)"
"KCK
D",0.6162420382165605,"For multi-dimension cases, similar results can be obtained:
524"
"KCK
D",0.6170382165605095,"pt−∆t|t(xi
t−∆t|xt) ="
"KCK
D",0.6178343949044586,"(
σ(t)
e−¯σ(t)"
"KCK
D",0.6186305732484076,"1−e−¯σ(t) ∆tcθ(xt)[i, xi
t−∆t]
if xi
t−∆t ̸= [M],"
"KCK
D",0.6194267515923567,"1 −σ(t)
e−¯σ(t)"
"KCK
D",0.6202229299363057,"1−e−¯σ(t) ∆t
if xi
t−∆t = [M].
(D.12)"
"KCK
D",0.6210191082802548,"for all xi
t = [M].
525"
"KCK
D",0.6218152866242038,"Tweedie τ-leaping
Suppose xt = x1
t · · · xd
t has N1 components as [M] and N2 = d −N1
526"
"KCK
D",0.6226114649681529,"components as specific values. Without loss of generality, let’s assume that the preceding N1 terms
527"
"KCK
D",0.6234076433121019,"of xt are all [M], and the remaining N2 terms are fixed at specific values. For 1 ≤i ≤d, given the
528"
"KCK
D",0.6242038216560509,"value of xi
t−∆t ̸= [M], :
529"
"KCK
D",0.625,"pt−∆t|t(xi
t−∆t|xt) = pt,t−∆t(xt, xi
t−∆t)
pt(xt)
.
(D.13)"
"KCK
D",0.6257961783439491,"By Proposition 1:
530"
"KCK
D",0.6265923566878981,"pt(xt) = [1 −e−¯σ(t)]N1[e−¯σ(t)]N2p0(xN1+1
t
· · · xd
t ).
(D.14)"
"KCK
D",0.6273885350318471,"Similar to the proof in Proposition 1:
531"
"KCK
D",0.6281847133757962,"pt,t−∆t(xt, xi
t−∆t) =
X"
"KCK
D",0.6289808917197452,"x0∈X
p(t,t−∆t)|0p(xt, xi
t−∆t|x0)p0(x0) =
X"
"KCK
D",0.6297770700636943,"x1
0∈X,··· ,xd
0∈X
p(t,t−∆t)|0([M] · · · [M]xN1+1
t
· · · xd
t , xi
t−∆t|x1
0 · · · xd
0)p0(x1
0 · · · xd
0) =
X"
"KCK
D",0.6305732484076433,"x1
0∈X,··· ,xd
0∈X
p(t,t−∆t)|0([M], xi
t−∆t|xi
0) N1
Y"
"KCK
D",0.6313694267515924,"k=1,k̸=i
pt|0([M]|xk
0) d
Y"
"KCK
D",0.6321656050955414,"k=N1+1
pt|0(xk|xk
0)p0(x1
0 · · · xd
0) =
X"
"KCK
D",0.6329617834394905,"xk
0∈X,k∈{1,··· ,N1}/{i}
p(t,t−∆t)|0([M], xi
t−∆t|xi
t−∆t) N1
Y"
"KCK
D",0.6337579617834395,"k=1,k̸=i
pt|0([M]|xk
0)[e−¯σ(t)]N2"
"KCK
D",0.6345541401273885,"p0(x1
0 · · · xi−1
0
xi
t−∆txi+1
0
· · · xN1
0 xN1+1
t
· · · xN1+1
t
) =
X"
"KCK
D",0.6353503184713376,"xk
0∈X,k∈{1,··· ,N1}/{i}
(e−¯σ(t−∆t) −e−¯σ(t))(1 −e−¯σ(t))N1−1[e−¯σ(t)]N2"
"KCK
D",0.6361464968152867,"p0(x1
0 · · · xi−1
0
xi
t−∆txi+1
0
· · · xN1
0 xN1+1
t
· · · xd
t )"
"KCK
D",0.6369426751592356,"=(e−¯σ(t−∆t) −e−¯σ(t))(1 −e−¯σ(t))N1−1[e−¯σ(t)]N2p0(xi
t−∆t, xN1+1
t
· · · xd
t )."
"KCK
D",0.6377388535031847,"Note we used the fact that:
532"
"KCK
D",0.6385350318471338,"p(t,t−∆t)|0([M], xi
t−∆t|xi
t−∆t) = pt|t−∆t([M]|xi
t−∆t)pt−∆t|0(xi
t−∆t|xi
t−∆t)"
"KCK
D",0.6393312101910829,= (1 −e−(¯σ(t)−¯σ(t−∆t)))e−¯σ(t−∆t)
"KCK
D",0.6401273885350318,"= e−¯σ(t−∆t) −e−¯σ(t), 533"
"KCK
D",0.6409235668789809,"pt|0([M]|xk
0) = 1 −e−¯σ(t),"
"KCK
D",0.64171974522293,"by dividing the two expressions, we have:
534"
"KCK
D",0.642515923566879,"pt−∆t|t(xi
t−∆t|xt) = e−¯σ(t−∆t) −e−¯σ(t)"
"KCK
D",0.643312101910828,"1 −e¯σ(t)
p0(xi
t−∆t|xN1+1
t
· · · xd
t )
(D.15)"
"KCK
D",0.6441082802547771,≈e−¯σ(t−∆t) −e−¯σ(t)
"KCK
D",0.6449044585987261,"1 −e¯σ(t)
cθ(xt)[i, xi
t−∆t].
(D.16)"
"KCK
D",0.6457006369426752,"In general, for xi
t = [M], we have:
535"
"KCK
D",0.6464968152866242,"pt−∆t|t(xi
t−∆t|xt)"
"KCK
D",0.6472929936305732,"(
≈e−¯σ(t−∆t)−e−¯σ(t)"
"KCK
D",0.6480891719745223,"1−e¯σ(t)
cθ(xt)[i, xi
t−∆t],
xi
t−∆t ̸= [M],"
"KCK
D",0.6488853503184714,= 1−e−¯σ(t−∆t)
"KCK
D",0.6496815286624203,"1−e−¯σ(t)
,
xi
t−∆t = [M].
(D.17)"
"KCK
D",0.6504777070063694,"D.3
Discuss on the expectation of NFE
536"
"KCK
D",0.6512738853503185,"As discussed in Section Appendix D.2, for both the Euler method and Tweedie τ -leaping, the
537"
"KCK
D",0.6520700636942676,"probability pi
t−∆t|t([M]|xt) is only a factor of time which is independent of the other dimensions
538"
"KCK
D",0.6528662420382165,"of xt once given xi
t = [M]. By the Law of Total Probability, it is easy to find that pi
t−∆t|t([M]|[M])
539"
"KCK
D",0.6536624203821656,"is also only a factor of time. Thus, given a specific sampling method and a set of time steps
540"
"KCK
D",0.6544585987261147,"{t0 = 0, · · · , tn = T}, the NFE can be treated as a random variable with a calculable expected value.
541"
"KCK
D",0.6552547770700637,"Let Nk denote the number of dimensions of x which changed in [tk−1, tk), so we have:
542"
"KCK
D",0.6560509554140127,"NFEs(n) = n
X"
"KCK
D",0.6568471337579618,"k=1
I(Nk ̸= 0),
(D.18) 543"
"KCK
D",0.6576433121019108,"E-NFEs(n) = n
X"
"KCK
D",0.6584394904458599,"k=1
E[I(Nk ̸= 0)] = n
X"
"KCK
D",0.6592356687898089,"k=1
P(Nk ̸= 0).
(D.19)"
"KCK
D",0.660031847133758,"For each dimension i, let rk represent the probability that xi changes within the interval [tk−1, tk).
544"
"KCK
D",0.660828025477707,"Consequently, Nk follows a binomial distribution with parameters l and rk, denoted as Nk ∼
545"
"KCK
D",0.6616242038216561,"Binomial(l, rk).
546"
"KCK
D",0.6624203821656051,"E-NFEs(n) = n
X"
"KCK
D",0.6632165605095541,"k=1
P(Nk ̸= 0) = n
X"
"KCK
D",0.6640127388535032,"k=1
(1 −(1 −rk)l).
(D.20)"
"KCK
D",0.6648089171974523,"By definition of rk and property of absorbing diffusion:
547"
"KCK
D",0.6656050955414012,"rk = P(Xi
tk−1 ̸= [M], Xi
tk = [M]|Xi
tn = [M])
(D.21)"
"KCK
D",0.6664012738853503,"= P(Xi
tk−1 ̸= [M]|Xi
tk = [M]) n
Y"
"KCK
D",0.6671974522292994,"l=k+1
P(Xi
tl−1 = [M]|Xi
tl = [M])
(D.22)"
"KCK
D",0.6679936305732485,"= (1 −P(Xi
tk−1 = [M]|Xi
tk = [M])) n
Y"
"KCK
D",0.6687898089171974,"l=k+1
P(Xi
tl−1 = [M]|Xi
tl = [M]).
(D.23)"
"KCK
D",0.6695859872611465,"Eq. (D.23) can be determined given the sampling method and noise schedule.
548"
"KCK
D",0.6703821656050956,"For the Euler method, based on Equation Eq. (D.12), we can derive that:
549"
"KCK
D",0.6711783439490446,"P(Xi
tl−1 = [M]|Xi
tl = [M]) = 1 −σ(tl)
e−¯σ(tl)"
"KCK
D",0.6719745222929936,"1 −e−¯σ(tl) (tl −tl−1).
(D.24)"
"KCK
D",0.6727707006369427,"Therefore, we can express rk as:
550"
"KCK
D",0.6735668789808917,"rk = (σ(tk)
e−¯σ(tk)"
"KCK
D",0.6743630573248408,"1 −e−¯σ(tk) (tk −tk−1)) n
Y"
"KCK
D",0.6751592356687898,"l=k+1
(1 −σ(tl)
e−¯σ(tl)"
"KCK
D",0.6759554140127388,"1 −e−¯σ(tl) (tl −tl−1)).
(D.25)"
"KCK
D",0.6767515923566879,"For Tweedie τ -leaping, By Eq. (D.17), similarly we have:
551"
"KCK
D",0.677547770700637,"P(Xi
tl−1 = [M]|Xi
tl = [M]) = 1 −e−¯σ(tl−1)"
"KCK
D",0.678343949044586,"1 −e−¯σ(tl) ,
(D.26)"
"KCK
D",0.679140127388535,rk = (e−¯σ(tk−1) −e−¯σ(tk)
"KCK
D",0.6799363057324841,"1 −e−¯σ(tk)
) n
Y"
"KCK
D",0.6807324840764332,"l=k+1
(1 −1 −e−¯σ(tl−1)"
"KCK
D",0.6815286624203821,1 −e−¯σ(tl) ) = e−¯σ(tk−1) −e−¯σ(tk)
"KCK
D",0.6823248407643312,"1 −e−¯σ(tn)
.
(D.27)"
"KCK
D",0.6831210191082803,"Specifically, if we adopt a log-linear noise schedule, which implies ¯σ(t) = −log(1 −(1 −ϵ)t) and
552"
"KCK
D",0.6839171974522293,tk = k
"KCK
D",0.6847133757961783,"n, Equation Eq. (D.27) can be simplified to 1"
"KCK
D",0.6855095541401274,"n. Substituting this result into Equation Eq. (D.20),
553"
"KCK
D",0.6863057324840764,"we obtain:
554"
"KCK
D",0.6871019108280255,"E-NFEs(n) = n
X"
"KCK
D",0.6878980891719745,"k=1
(1 −(1 −1"
"KCK
D",0.6886942675159236,n)l) = n(1 −(1 −1
"KCK
D",0.6894904458598726,"n)l).
(D.28)"
"KCK
D",0.6902866242038217,"E
Algorithms for training and inference
555"
"KCK
D",0.6910828025477707,"F
Experimental details
556"
"KCK
D",0.6918789808917197,"F.1
Model details
557"
"KCK
D",0.6926751592356688,"We implemented our RADD model based on SEDD architecture, which is an encoder-only transformer
558"
"KCK
D",0.6934713375796179,"model [46, 47] incorporating time conditioning [48] and using rotary positional encoding [49]. The
559"
"KCK
D",0.6942675159235668,"only difference is that we removed all parts related to time conditioning (i.e. TimeEmbedding,
560"
"KCK
D",0.6950636942675159,"adaLN-zero block [48]) and added a softmax operation at the end of the neural network to ensure the
561"
"KCK
D",0.695859872611465,"output was a valid conditional distribution. Compared with SEDD small model, this modification led
562"
"KCK
D",0.696656050955414,"to a reduction of 7M parameters, equating to an 8% decrease from the original 90M non-embedding
563"
"KCK
D",0.697452229299363,"parameters.
564"
"KCK
D",0.6982484076433121,Algorithm 1 Unconditional Sampling
"KCK
D",0.6990445859872612,"Require: Network cθ, noise schedule σ (total noise ¯σ), time range [0, T], step size ∆t"
"KCK
D",0.6998407643312102,"1: t ←T, xT ←[M] . . . [M]
|
{z
}
d×[M]"
"KCK
D",0.7006369426751592,", ccache ←cθ(xt)"
"KCK
D",0.7014331210191083,"2: while t > 0 do
3:
if Use Euler then
4:
Construct transition densities p(xi
t−∆t|xt) by Eq. (D.12) use ccache
5:
end if
6:
if Use Tweedie τ -leaping then
7:
Construct transition densities p(xi
t−∆t|xt) by Eq. (D.17) use ccache
8:
end if
9:
xi
t−∆t ∼Cat(p(xi
t−∆t|xt)) for all xi
t = [M], xi
t−∆t ←xi
t for all xi
t ̸= [M]
10:
if xt−∆t ̸= xt then
11:
ccache ←cθ(xt)
12:
end if
13:
t ←t −∆t,
14: end while"
"KCK
D",0.7022292993630573,Algorithm 2 Conditional Sampling
"KCK
D",0.7030254777070064,"Require: Network cθ, noise schedule σ (total noise ¯σ), time range [0, T], step size ∆t, Prompt
spaces Ωand tokens T .
1: t ←T, construct xT with xΩ
T = T and x¯Ω
T = [M], ccache ←cθ(xt)
2: while t > 0 do
3:
if Use Euler then
4:
Construct transition densities p(xi
t−∆t|xt) by Eq. (D.12) use ccache
5:
end if
6:
if Use Tweedie τ -leaping then
7:
Construct transition densities p(xi
t−∆t|xt) by Eq. (D.17) use ccache
8:
end if
9:
xi
t−∆t ∼Cat(p(xi
t−∆t|xt)) for all xi
t = [M], xi
t−∆t ←xi
t for all xi
t ̸= [M]
10:
if xt−∆t ̸= xt then
11:
ccache ←cθ(xt)
12:
end if
13:
t ←t −∆t,
14: end while"
"KCK
D",0.7038216560509554,Algorithm 3 Training
"KCK
D",0.7046178343949044,"Require: Network cθ, noise schedule σ (total noise ¯σ), time range [0, T], data distribution pdata"
"KCK
D",0.7054140127388535,"1: repeat
2:
x0 ∼pdata, t ∼U([0, T]).
3:
construct xt by Zi ∼Bernoulli(e−¯σ(t)), xi
t = I(Zi = 1)xi
0 + I(Zi = 0)[M]"
"KCK
D",0.7062101910828026,"4:
Calculate Lθ(xt, x0) = P"
"KCK
D",0.7070063694267515,"xi
t=[M] −σ(t)
e−¯σ(t)"
"KCK
D",0.7078025477707006,"1−e−¯σ(t) log

e−¯σ(t)"
"KCK
D",0.7085987261146497,"1−e−¯σ(t) cθ(xt)[i, xi
0]
"
"KCK
D",0.7093949044585988,"5:
Take gradient descent on ∇θL(xt, x0)
6: until converged"
"KCK
D",0.7101910828025477,"Table 3: Quality of unconditionally generated text evaluated by perplexity (↓). For a fixed model,
the best perplexity is bolded."
"KCK
D",0.7109872611464968,"Method
RADD-DSE
RADD-DCE"
"KCK
D",0.7117834394904459,"Forward
116.94
113.92
Backward
135.39
125.59
Random
114.94
101.23"
"KCK
D",0.7125796178343949,"F.2
Training details
565"
"KCK
D",0.7133757961783439,"Following the settings in [29], we trained our model with the following configuration:
566"
"KCK
D",0.714171974522293,"• Batch Size:512
567"
"KCK
D",0.714968152866242,"• Learning Rate: 3 × 10−4
568"
"KCK
D",0.7157643312101911,"• Exponential Moving Average (EMA):0.9999
569"
"KCK
D",0.7165605095541401,"• Gradient Clipping: Gradient norm clipped to 1
570"
"KCK
D",0.7173566878980892,"• Warmup Schedule: Applied for the first 2500 iterations
571"
"KCK
D",0.7181528662420382,"We utilized 16 V100 32G GPUs or 16 A100 40G GPUs for training. For the A100 40G GPUs, we
572"
"KCK
D",0.7189490445859873,"leveraged flash attention to accelerate the training process. For the V100 32G GPUs, which do not
573"
"KCK
D",0.7197452229299363,"support flash attention or bfloat16, we employed float16 precision and used the Memory-Efficient
574"
"KCK
D",0.7205414012738853,"Attention mechanism available in torch.nn.functional.scaled_dot_product_attention. Additionally, we
575"
"KCK
D",0.7213375796178344,"used gradient checkpointing technique to save memory.
576"
"KCK
D",0.7221337579617835,"F.3
Unconditional generation details
577"
"KCK
D",0.7229299363057324,"We used Tweedie τ -leaping method, which has optimal results with fixed NFE. For SEDD small,
578"
"KCK
D",0.7237261146496815,"we directly used their result. For RADD small, we generated 1000 samples to get the average value
579"
"KCK
D",0.7245222929936306,"following [29].
580"
"KCK
D",0.7253184713375797,"F.4
Further evaluation of generative perplexity
581"
"KCK
D",0.7261146496815286,"As stated in Theorem 1, cθ can be interpreted as a conditional distribution over clean data. A natural
582"
"KCK
D",0.7269108280254777,"idea is to use it directly to generate samples, which is similar to auto-regressive models. However,
583"
"KCK
D",0.7277070063694268,"there are d! kinds of decomposition from joint distribution to conditional distribution, in which we
584"
"KCK
D",0.7285031847133758,"only tested three representative cases:
585"
"KCK
D",0.7292993630573248,"• forward: p(x1 · · · xd) = Qd
k=1 p(xk|x(<k))
586"
"KCK
D",0.7300955414012739,"• backward: p(x1 · · · xd) = Qd
k=1 p(xk|x(>k))
587"
"KCK
D",0.7308917197452229,"• random: π ∼U(Sd), p(x1 · · · xd) = Qd
k=1 p(xπ(k)|xπ(<k))
588"
"KCK
D",0.731687898089172,"Results are shown in Table 3. The perplexity is calculated on average of 1024 samples. For the
589"
"KCK
D",0.732484076433121,"random case, we calculate the average perplexity between different randomly generated π. Generally,
590"
"KCK
D",0.73328025477707,"we find that the perplexity by directly sampling from the conditional distribution is higher than
591"
"KCK
D",0.7340764331210191,"that achieved by Tweedie τ-leaping. Among the different decomposition orders, the random order
592"
"KCK
D",0.7348726114649682,"demonstrated the best performance.
593"
"KCK
D",0.7356687898089171,"G
Additional experimental results
594"
"KCK
D",0.7364649681528662,"G.1
Additional samples
595"
"KCK
D",0.7372611464968153,"In this section, we present the unconditionally and conditionally generated text of RADD-DSE in
596"
"KCK
D",0.7380573248407644,"Fig.3 and Fig.4, respectively. Similarly, the results of RADD-DCE are shown in Fig.?? and Fig.??,
597"
"KCK
D",0.7388535031847133,"respectively.
598"
"KCK
D",0.7396496815286624,"and human face. “And pretty damn conventional mating, so didn’t come to the table like
that with me. (As a character), it would be a pretty solid case to have,” Andra says. “I saw
the way he did it, and I think played a little bit with some of his fans. He wanted me to get
cute a little bit too.”
Advertisement
As in the parking lot, Andra was growing frustrated with the way the cars recently fit in
nicely.
It also makes me feel like some things haven’t changed until around this period of time, in
the future. It’s definitely the future here now — and I’m always dubious about thinking
quite long before Toyota ever introduced a new car. “I think something that perfectly well
fits all the guys,” Andra says. “I therefore could fit more well.”
– Follow Matt Dyckton on Twitter @Mittington.<|endoftext|>Spanish star Christina Rene
got away after a teenage girl told to leave India for an unfamiliar place.
The Russian woman chose Chelsea to stay home, saying the alternative was to get rid of a
condition and die after receiving a medical diagnosis and go back to learning as a nurse.
Chelsea was sentenced to the first arrest for a serious mental health conviction in October,
and was transferred to a man who had taken his place, a 16-year-old man. The bailee’s
Appeal Court had appealed to a court to hear the case she learned from the teenagers.
The Russian Internal Revenue Service found the woman charged with administering an
emergency ward, and although the girl was still waiting for a doctor there, she instead went
to visit another clinic for the treatment of female swi-virus virus.
The court had not ruled out an in-life medical professional. Chelsea never applied to be a
pregnant mother; her formal application assumed she was pregnant, dating from the summer
of 2014.
Out of sound doubts, when her co-boyfriend Chelsea received a proposal to stay abroad for
summer work abroad. Chelsea then applied to win to work and have a place in Russia.
The grant of nearly $5000 Chelsea invited Chelsea out to go see a Moscow clinic. It took
two weeks to find a doctor. These sors’ of Moscow’s federal courts confiscated the grant.
The 17-year-old did not need to go to the hospital where she says she has received everything
she has had in China, of course. Chelsea’s lawyers Andre James Irani gave the court the
doctors he could plead without permission of the young teenager on a regular basis.
The Argentine was put out on bail, prompting a sex psychologist to meet her when she
signed her papers, but no family member was present.
“First thing I wasn’t going to ask three days, but I’m thinking about this months already.
“I’m glad to see that they are waiting for her with their services. She’s already built a good
life. She’s interested in her studies. But feel like she is? I want to be the only person who
has ever been my friend,” she said.
“I took a lot of act, but she is more than never.”
To this day Christina Rene McCourner told the court the situation is between Denmark and
Stockholm syndrome. She says Chelsea has refused to come to term.
“She says that she is talented at medical school. But she’s telling a different story, anyway.
There’s also a case when you just can’t get the CNN-type cowardice yourself going to the
doctor,” she said.
A team of Barcelona has been conducting medical inquiries into both doctors who treat the
sick and those not who use medical services. “How has she been following her visit to Russia
United States and since arriving unable to do so?”
“Each often when she said “All the bills are what may I doctor,” I’ve usually never used my
medicine again,” Chelsea attorneys say.
Life in the wrong world
While far-so in the past two weeks though, nearly speaking only three pregnant women in
Mexico and Europe have requested that Chelsea stop using medical services at them all.
Though many of its applicants have receiving medical assistance in Qatar and other parts of
the world, they believe it means they should go elsewhere, according to the health services
agency.
Chelsea’s received from facility staff dealing with financial responsibility say it is those who
are vulnerable have been out of money for healthcare their life, not who are suffering from
a lack of access. “You might think after you tell that provider is not available, you should
give the load to somebody in Russia or Sweden,” she"
"KCK
D",0.7404458598726115,Figure 3: Unconditionally generated text of RADD-SE.
"KCK
D",0.7412420382165605,"Hi, my name is Shade-Rayhelynis-Neelsons. Interviewer: Two days ago, so let me speak to
you in brief.Drake: Hi, are you studying for graduate school in late July, and somebody is
interviewing you for you for his classes. My first personal quote is: so when they’re doing
class they’re going to a shower, and they expect me to not be part of the shower any way.
I have a take, I want to check when something’s not right and make sure I spot it so that
someone can get it happening. My hair is an important piece of me, and I can be the one
complete human being that when I have a shower and say, ""Okay, I really like my hair, and
this is the thing that I would like and I want to believe something, I should just have an
attitude check."" Do you think anyone else can have time with me? Do I say I don’t?I mean I
get to 7 right the time I go to class, I sit back there and I feel like no one knows what to do.
So, in my case I am not anxious, I’m just saying, ""I feel like I have my hair without letting
go from day to day, I’ve got to feel like I’ve been like the thing they should all be excited
by.""So, the days follow me, you start looking at my pictures, and you realize how pretty
you are. It was just so big this moment for that office, because that was seeing positive
things.I’m starting to grow up and be beautiful too.I mean–I mean it’s kind of fun, now
seeing beautiful girls, especially really in their 20s, come from unusual backgrounds. Back
in the 50s I met people at one of the first places Woosz made mons, asked us to visit charity
walks, and he would buy us a suit. And actually he really wanted to, so I know what the
inspiration is. It’s something that someone could have found out, and where they’re from
– I think it is like everyone is finding ways to relate.This is one of those things I remember
the most about when I went to order the products!For example, it may be taken after the
sporting event, and I don’t know, but they asked us to choose color for our favorite parts
in the group....they asked us to take their favorite color, then they made their eyes for each
skin.The one that was the round head, like that one I chose so much, but it was really a
really painful transition...and I don’t know what touched my skin, so I don’t know what I
would do with it, I will tell you...that was not what I focused on, but I think I may have
pretty much my original o-still darker hair, so I chose to go with the round head which I
did really. Anyway, my job was to get the hair done, and I do most of my hair for school
because I got married and had so many children while I was So. So, I’m always thinking
again not to be confused. I had just been doing niggly since I was a child, and there was
no reason to do it like that. The career had got started, and therefore I was really going
all over the world no less. So one of the things I did was taking of his shirts so I look at
him all in one go, I surprise him.I’ve got some of the most cool things going with those two
of this. You see these all look alike, clean, and awesome–and they used to have a shop in
the....a shop like that, they never sells coats, so that was really helpful to those guys, I feel,
guys going into shop with these guys, and those guys can have the right color, they can
have it that right look.I love my hair, it’s like when I was a year old when I see scent in your
nursery, to get the grandest response, I keep using the smell. I bring it to its level, honestly,
it makes the weight apply.rake: Probably my favorite word to explain, after you explain,
it isn’t hard–in case that’s your thing, I’m already who kind of like going, and I’m able to
get access when I’m trying. I’ve been trying to finish coloring for people, for years, so I
would like to work on it’s own but the colors there to do color pattern are picked for another
reason to put the final color off, that’s like the other one for giving glitter or outline.So I’m
just going to be looking side by side at coloring over and over again. It’s my vision. Each
color is my dream.As you know, a color is just simply something tucked into a dye and what
makes perfect or tail end to a hair is...and that is why I always shampoo twice a day and
shower three times a day."
"KCK
D",0.7420382165605095,Figure 4: Conditionally generated text of RADD-SE. Prompt tokens are highlighted in blue.
"KCK
D",0.7428343949044586,"as well. However, she did not sign.
Gov. Johnluaj said the Amal effectively took the case to the Supreme Court.
""I was wrong to say that they left the Constitution in place and this is basically unconstitu-
tional,” Jackson said. “I don’t think they’re saying that. I’m. That means on the one hand
they’re going to have to intervene, or on the side of the other, they’ll have to intervene. I
can’t think of changing a constitutional decision with them.""
When Gov. Barack Obama announced Tuesday — and a federal court is set to challenge
the way residents of the states violated the law — Attorney General Eric Holder chose to
hold his own hearing. The entire state had a deadline to field a recipient of the letter asking
for comment.
Because hearings were held before, Ohio and 17 states have each had such a case before.
At last night’s hearings, more than four separate arguments were heard by a 52-45 margin
in order to pass the repeal bill by a vote of 63-2 49 to 45. The bill also included Obamacare
legislation and was pushed through Congress after opposition from 24 states. Both brought
in a new governor, popular Gov Sen. Phil Bryant, another Republican in the Senate. Neil
LePage neared an attorney in both cases and refused to find a new insurance secretary.
Both House Leader Mitch McConnell and Republicans said they would repeal the law en-
tirely. The law would have been in the Oval Office of the Logged since 2011.
Former Judge Anthony Teague, the Ohio Chief Judge, found the overwhelming majority
vote in favor of the legislation well in line and said it was a ""needed forward.""
“The things citizenship issues should go from statutes states have to regulations,” he said.
“They’ve got this idea that the courts are getting knocked to their own corner. And once
they see it turn around them, sure as hell they’ll have faith dogged by judges exercising
constitutional rights.”
Attorney Holder, the assistant secretary of state for policy at the Department of Justice,
has discussed the idea that federal courts such as the U.S. Supreme Court should handling
legal issues such as making tough immigration decisions for illegal immigrants.
“The thought process of helping illegal immigrants goes beyond the judicial process. What I
understand. . . criminal immigration measures, gang activity, affirmative action efforts, crim-
inal status,” Holder said. “And things like that, we expect in the state to go to a long way.
There clearly needs to be a criminal justice program on immigration and reform, and we
need to recognize those efforts not to start.”
Holder has also said it is a “real issue” for the constitution if the letter is signed off. He said
it was essentially a message, seeking to show the majority “power of respect and the power
of institutions.”
But the attorney general said he hoped it would be a task to figure out how to start
safeguarding each of their citizens’ rights while restoring their constitutional trust.
“The only thing I think we can really do is have the judges to understand the nature of
the judiciary, how powerful it is to be involved and to interfere with the government with
no accountability on what path they want to move down,” he said.<|endoftext|>Ex76561
molds at the worst parts of the UK economy will tell in the future – most poorer areas of
England have suffered more than any person during the first years of 2008, according to
ex-Home Secretary Jeremy Foot.
A total of 23,000 people of disposable incomes who have five mortgages – more than 4,000
households – will be considered as home buyers, even though published figures will be
different from September, say researchers
Residents of more than 3,000 homes will be the third most likely to die because of jobs
which fell in average terms home ownership during 2008 until the end of this year.
It’s a surprising drop, according to the Wall Street Journal, which says the economy and
the top 1
Professor Jeremy Foot, Home Secretary and the Information Society, raised concerns about
the collapse of the UK’s housing market – down from 39
He said: ""The measure of who ’invested’ at that time, doesn’t include the number of people
or businesses with the assets. There were only three big cities, New York were the other
three?
""This year – it was announced that Royal Bank of Scotland would be the first to close in 20
years – it turned out then that the poorer areas, including by 2009 and 2010, were hammered
hardest,"" Professor Foot said.
Professor Foot’s lecture, which was released today in"
"KCK
D",0.7436305732484076,Figure 5: Unconditionally generated text of RADD-CE.
"KCK
D",0.7444267515923567,"I have a ick of death — that’s where I am. That’s what I need. That need to take something
else. But I don’t have that in my family. And here’s why you might let it go. You don’t
want to really say what I wrote in my story, open your mind and finish it with purposeful
thought. But I can do it if I get cancer.”
“I can not help but accept that you brother-in-law was really on me and that that’s not how
I need to be,” I said with my father’s sad smile. He let it go a few days later, but it didn’t
prevent him from thinking about it. But I made sure he wouldn’t let-in-law leave.
My mother saved my only father for the life. And he was a falsehood, but no doubt. He
saved me.
“My mother thank god. But do you ask outside of me the questions, ask him. He’s dealing
with this and prepare for their perception of bad things he said. They should’t forget him
because the bad thoughts come with it.”
I blackened my father and watched them walk back up to him in front of a good news line.
He stood with us for two days; we stared on from spring to spring.
“It’s OK.” He said.
“I know what you want when you come here.”
“I’ll accept that,” he said. “What I thought to look back was bad. I’m going to let it go
and make an exception in this case. I came here after sharing the story. I’ve been looking
to the world. And I don’t care for anyone in this world, but I want to make it harder for
other people to try to make this mistake.”
He didn’t justify what he meant. My mother drove us the way I trucked him back to his
house, and he chuckled at me in simple words:
“I said to him I need to still be here, but do you want me to have this for the whole day?”
“These are my expectations. If we smile and I’m laughing thank you can I be so happy?”
Outside of my family, there was joy, joy, sorrow. At youngest, most of all, my father also
saw the world. He suffered from hunger, his family lost access to wheelchair and every room.
They fought with him sometimes, and when they needed to rest, he stepped back. I can
only imagine, deep down, his family grew enormously with him.
Our journey with my parents at times had been a combination of things. We were all in
pairs, and my brothers were such. Repeatedly we asked them to tell me when we wouldn’t
last to get that end. And I started missing my trip — never visited neighbors before. And
finally one day I lost my patience. I all wanted to wander down to my father’s house in76561.
On my own, I couldn’t see my beautiful mother again.
I had an ear tumor that was
brainblown away and lost all the ways to make the most of the time again. I took the bed
from the entire bear family along with his three children, and fixed him up on one side and
led him home through the open of the front door with my honey brother. We were hearing
all of the other things I had heard were outlandish stories. Finally, he remained so grateful
for his admiration for my dad.
What was possible my father really didn’t have?
On November 2, 2013, my mother still did not show any empathy for my father. The loyalty
he had was the only expression he had.
On the night where he was arrested, other members of his family noticed that they couldn’t
bribe a co-worker to walk up from his job. That led to him thinking, “I want you to not
be a badexample in your family. You can’t serve an individual who will make up his stupid
demands for you in order to make a profit.” For me, to fulfill the compassion of my own
brother, I’ve become an able example of that, that kids.
With this decision, it was freeing that I had allowed him to try to decide how to make a
better life for his family. I had allowed the animals to have a day off. This is something he
and I can do. His son had thought they could, and it was relief he had been rearranging
that reality. The time our family spends on the house was at seven times a day."
"KCK
D",0.7452229299363057,Figure 6: Conditionally generated text of RADD-CE. Prompt tokens are highlighted in blue.
"KCK
D",0.7460191082802548,"H
License
599"
"KCK
D",0.7468152866242038,"URL and license for existing assets we used are provided in Table 4.
600"
"KCK
D",0.7476114649681529,"Table 4: URL and license for existing assets we used.
Name
URL
License"
"KCK
D",0.7484076433121019,"SEDD
https://github.com/louaaron/Score-Entropy-Discrete-Diffusion
MIT License"
"KCK
D",0.7492038216560509,"NeurIPS Paper Checklist
601"
CLAIMS,0.75,"1. Claims
602"
CLAIMS,0.7507961783439491,"Question: Do the main claims made in the abstract and introduction accurately reflect the
603"
CLAIMS,0.7515923566878981,"paper’s contributions and scope?
604"
CLAIMS,0.7523885350318471,"Answer: [Yes]
605"
CLAIMS,0.7531847133757962,"Justification: The main claims made in the abstract and introduction accurately reflect our
606"
CLAIMS,0.7539808917197452,"paper’s contributions and scope.
607"
CLAIMS,0.7547770700636943,"Guidelines:
608"
CLAIMS,0.7555732484076433,"• The answer NA means that the abstract and introduction do not include the claims
609"
CLAIMS,0.7563694267515924,"made in the paper.
610"
CLAIMS,0.7571656050955414,"• The abstract and/or introduction should clearly state the claims made, including the
611"
CLAIMS,0.7579617834394905,"contributions made in the paper and important assumptions and limitations. A No or
612"
CLAIMS,0.7587579617834395,"NA answer to this question will not be perceived well by the reviewers.
613"
CLAIMS,0.7595541401273885,"• The claims made should match theoretical and experimental results, and reflect how
614"
CLAIMS,0.7603503184713376,"much the results can be expected to generalize to other settings.
615"
CLAIMS,0.7611464968152867,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
616"
CLAIMS,0.7619426751592356,"are not attained by the paper.
617"
LIMITATIONS,0.7627388535031847,"2. Limitations
618"
LIMITATIONS,0.7635350318471338,"Question: Does the paper discuss the limitations of the work performed by the authors?
619"
LIMITATIONS,0.7643312101910829,"Answer: [Yes]
620"
LIMITATIONS,0.7651273885350318,"Justification: We discuss the limitations in the main text.
621"
LIMITATIONS,0.7659235668789809,"Guidelines:
622"
LIMITATIONS,0.76671974522293,"• The answer NA means that the paper has no limitation while the answer No means that
623"
LIMITATIONS,0.767515923566879,"the paper has limitations, but those are not discussed in the paper.
624"
LIMITATIONS,0.768312101910828,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
625"
LIMITATIONS,0.7691082802547771,"• The paper should point out any strong assumptions and how robust the results are to
626"
LIMITATIONS,0.7699044585987261,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
627"
LIMITATIONS,0.7707006369426752,"model well-specification, asymptotic approximations only holding locally). The authors
628"
LIMITATIONS,0.7714968152866242,"should reflect on how these assumptions might be violated in practice and what the
629"
LIMITATIONS,0.7722929936305732,"implications would be.
630"
LIMITATIONS,0.7730891719745223,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
631"
LIMITATIONS,0.7738853503184714,"only tested on a few datasets or with a few runs. In general, empirical results often
632"
LIMITATIONS,0.7746815286624203,"depend on implicit assumptions, which should be articulated.
633"
LIMITATIONS,0.7754777070063694,"• The authors should reflect on the factors that influence the performance of the approach.
634"
LIMITATIONS,0.7762738853503185,"For example, a facial recognition algorithm may perform poorly when image resolution
635"
LIMITATIONS,0.7770700636942676,"is low or images are taken in low lighting. Or a speech-to-text system might not be
636"
LIMITATIONS,0.7778662420382165,"used reliably to provide closed captions for online lectures because it fails to handle
637"
LIMITATIONS,0.7786624203821656,"technical jargon.
638"
LIMITATIONS,0.7794585987261147,"• The authors should discuss the computational efficiency of the proposed algorithms
639"
LIMITATIONS,0.7802547770700637,"and how they scale with dataset size.
640"
LIMITATIONS,0.7810509554140127,"• If applicable, the authors should discuss possible limitations of their approach to
641"
LIMITATIONS,0.7818471337579618,"address problems of privacy and fairness.
642"
LIMITATIONS,0.7826433121019108,"• While the authors might fear that complete honesty about limitations might be used by
643"
LIMITATIONS,0.7834394904458599,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
644"
LIMITATIONS,0.7842356687898089,"limitations that aren’t acknowledged in the paper. The authors should use their best
645"
LIMITATIONS,0.785031847133758,"judgment and recognize that individual actions in favor of transparency play an impor-
646"
LIMITATIONS,0.785828025477707,"tant role in developing norms that preserve the integrity of the community. Reviewers
647"
LIMITATIONS,0.7866242038216561,"will be specifically instructed to not penalize honesty concerning limitations.
648"
THEORY ASSUMPTIONS AND PROOFS,0.7874203821656051,"3. Theory Assumptions and Proofs
649"
THEORY ASSUMPTIONS AND PROOFS,0.7882165605095541,"Question: For each theoretical result, does the paper provide the full set of assumptions and
650"
THEORY ASSUMPTIONS AND PROOFS,0.7890127388535032,"a complete (and correct) proof?
651"
THEORY ASSUMPTIONS AND PROOFS,0.7898089171974523,"Answer: [Yes]
652"
THEORY ASSUMPTIONS AND PROOFS,0.7906050955414012,"Justification: For each theoretical result, the paper provides the full set of assumptions and a
653"
THEORY ASSUMPTIONS AND PROOFS,0.7914012738853503,"complete (and correct) proof. Please see the Appendix for more details.
654"
THEORY ASSUMPTIONS AND PROOFS,0.7921974522292994,"Guidelines:
655"
THEORY ASSUMPTIONS AND PROOFS,0.7929936305732485,"• The answer NA means that the paper does not include theoretical results.
656"
THEORY ASSUMPTIONS AND PROOFS,0.7937898089171974,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
657"
THEORY ASSUMPTIONS AND PROOFS,0.7945859872611465,"referenced.
658"
THEORY ASSUMPTIONS AND PROOFS,0.7953821656050956,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
659"
THEORY ASSUMPTIONS AND PROOFS,0.7961783439490446,"• The proofs can either appear in the main paper or the supplemental material, but if
660"
THEORY ASSUMPTIONS AND PROOFS,0.7969745222929936,"they appear in the supplemental material, the authors are encouraged to provide a short
661"
THEORY ASSUMPTIONS AND PROOFS,0.7977707006369427,"proof sketch to provide intuition.
662"
THEORY ASSUMPTIONS AND PROOFS,0.7985668789808917,"• Inversely, any informal proof provided in the core of the paper should be complemented
663"
THEORY ASSUMPTIONS AND PROOFS,0.7993630573248408,"by formal proofs provided in appendix or supplemental material.
664"
THEORY ASSUMPTIONS AND PROOFS,0.8001592356687898,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8009554140127388,"4. Experimental Result Reproducibility
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8017515923566879,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.802547770700637,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.803343949044586,"of the paper (regardless of whether the code and data are provided or not)?
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.804140127388535,"Answer: [Yes]
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8049363057324841,"Justification: Our paper fully discloses all the information needed to reproduce the main
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8057324840764332,"experiment.
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8065286624203821,"Guidelines:
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8073248407643312,"• The answer NA means that the paper does not include experiments.
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8081210191082803,"• If the paper includes experiments, a No answer to this question will not be perceived
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8089171974522293,"well by the reviewers: Making the paper reproducible is important, regardless of
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8097133757961783,"whether the code and data are provided or not.
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8105095541401274,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8113057324840764,"to make their results reproducible or verifiable.
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8121019108280255,"• Depending on the contribution, reproducibility can be accomplished in various ways.
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8128980891719745,"For example, if the contribution is a novel architecture, describing the architecture fully
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8136942675159236,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8144904458598726,"be necessary to either make it possible for others to replicate the model with the same
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8152866242038217,"dataset, or provide access to the model. In general. releasing code and data is often
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8160828025477707,"one good way to accomplish this, but reproducibility can also be provided via detailed
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8168789808917197,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8176751592356688,"of a large language model), releasing of a model checkpoint, or other means that are
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8184713375796179,"appropriate to the research performed.
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8192675159235668,"• While NeurIPS does not require releasing code, the conference does require all submis-
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8200636942675159,"sions to provide some reasonable avenue for reproducibility, which may depend on the
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.820859872611465,"nature of the contribution. For example
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.821656050955414,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.822452229299363,"to reproduce that algorithm.
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8232484076433121,"(b) If the contribution is primarily a new model architecture, the paper should describe
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8240445859872612,"the architecture clearly and fully.
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8248407643312102,"(c) If the contribution is a new model (e.g., a large language model), then there should
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8256369426751592,"either be a way to access this model for reproducing the results or a way to reproduce
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8264331210191083,"the model (e.g., with an open-source dataset or instructions for how to construct
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8272292993630573,"the dataset).
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8280254777070064,"(d) We recognize that reproducibility may be tricky in some cases, in which case
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8288216560509554,"authors are welcome to describe the particular way they provide for reproducibility.
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8296178343949044,"In the case of closed-source models, it may be that access to the model is limited in
702"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8304140127388535,"some way (e.g., to registered users), but it should be possible for other researchers
703"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8312101910828026,"to have some path to reproducing or verifying the results.
704"
OPEN ACCESS TO DATA AND CODE,0.8320063694267515,"5. Open access to data and code
705"
OPEN ACCESS TO DATA AND CODE,0.8328025477707006,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
706"
OPEN ACCESS TO DATA AND CODE,0.8335987261146497,"tions to faithfully reproduce the main experimental results, as described in supplemental
707"
OPEN ACCESS TO DATA AND CODE,0.8343949044585988,"material?
708"
OPEN ACCESS TO DATA AND CODE,0.8351910828025477,"Answer: [Yes]
709"
OPEN ACCESS TO DATA AND CODE,0.8359872611464968,"Justification: We provide open access to the data and code, with sufficient instructions to
710"
OPEN ACCESS TO DATA AND CODE,0.8367834394904459,"faithfully reproduce the main experimental results, as described in supplemental materials.
711"
OPEN ACCESS TO DATA AND CODE,0.8375796178343949,"Guidelines:
712"
OPEN ACCESS TO DATA AND CODE,0.8383757961783439,"• The answer NA means that paper does not include experiments requiring code.
713"
OPEN ACCESS TO DATA AND CODE,0.839171974522293,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
714"
OPEN ACCESS TO DATA AND CODE,0.839968152866242,"public/guides/CodeSubmissionPolicy) for more details.
715"
OPEN ACCESS TO DATA AND CODE,0.8407643312101911,"• While we encourage the release of code and data, we understand that this might not be
716"
OPEN ACCESS TO DATA AND CODE,0.8415605095541401,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
717"
OPEN ACCESS TO DATA AND CODE,0.8423566878980892,"including code, unless this is central to the contribution (e.g., for a new open-source
718"
OPEN ACCESS TO DATA AND CODE,0.8431528662420382,"benchmark).
719"
OPEN ACCESS TO DATA AND CODE,0.8439490445859873,"• The instructions should contain the exact command and environment needed to run to
720"
OPEN ACCESS TO DATA AND CODE,0.8447452229299363,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
721"
OPEN ACCESS TO DATA AND CODE,0.8455414012738853,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
722"
OPEN ACCESS TO DATA AND CODE,0.8463375796178344,"• The authors should provide instructions on data access and preparation, including how
723"
OPEN ACCESS TO DATA AND CODE,0.8471337579617835,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
724"
OPEN ACCESS TO DATA AND CODE,0.8479299363057324,"• The authors should provide scripts to reproduce all experimental results for the new
725"
OPEN ACCESS TO DATA AND CODE,0.8487261146496815,"proposed method and baselines. If only a subset of experiments are reproducible, they
726"
OPEN ACCESS TO DATA AND CODE,0.8495222929936306,"should state which ones are omitted from the script and why.
727"
OPEN ACCESS TO DATA AND CODE,0.8503184713375797,"• At submission time, to preserve anonymity, the authors should release anonymized
728"
OPEN ACCESS TO DATA AND CODE,0.8511146496815286,"versions (if applicable).
729"
OPEN ACCESS TO DATA AND CODE,0.8519108280254777,"• Providing as much information as possible in supplemental material (appended to the
730"
OPEN ACCESS TO DATA AND CODE,0.8527070063694268,"paper) is recommended, but including URLs to data and code is permitted.
731"
OPEN ACCESS TO DATA AND CODE,0.8535031847133758,"6. Experimental Setting/Details
732"
OPEN ACCESS TO DATA AND CODE,0.8542993630573248,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
733"
OPEN ACCESS TO DATA AND CODE,0.8550955414012739,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
734"
OPEN ACCESS TO DATA AND CODE,0.8558917197452229,"results?
735"
OPEN ACCESS TO DATA AND CODE,0.856687898089172,"Answer: [Yes]
736"
OPEN ACCESS TO DATA AND CODE,0.857484076433121,"Justification: We specify all the training and test details necessary to understand the results
737"
OPEN ACCESS TO DATA AND CODE,0.85828025477707,"in Section 4.1
738"
OPEN ACCESS TO DATA AND CODE,0.8590764331210191,"Guidelines:
739"
OPEN ACCESS TO DATA AND CODE,0.8598726114649682,"• The answer NA means that the paper does not include experiments.
740"
OPEN ACCESS TO DATA AND CODE,0.8606687898089171,"• The experimental setting should be presented in the core of the paper to a level of detail
741"
OPEN ACCESS TO DATA AND CODE,0.8614649681528662,"that is necessary to appreciate the results and make sense of them.
742"
OPEN ACCESS TO DATA AND CODE,0.8622611464968153,"• The full details can be provided either with the code, in appendix, or as supplemental
743"
OPEN ACCESS TO DATA AND CODE,0.8630573248407644,"material.
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8638535031847133,"7. Experiment Statistical Significance
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8646496815286624,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8654458598726115,"information about the statistical significance of the experiments?
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8662420382165605,"Answer: [No]
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8670382165605095,"Justification: Error bars are not reported because it would be too computationally expensive.
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8678343949044586,"Guidelines:
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8686305732484076,"• The answer NA means that the paper does not include experiments.
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8694267515923567,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8702229299363057,"dence intervals, or statistical significance tests, at least for the experiments that support
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8710191082802548,"the main claims of the paper.
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8718152866242038,"• The factors of variability that the error bars are capturing should be clearly stated (for
755"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8726114649681529,"example, train/test split, initialization, random drawing of some parameter, or overall
756"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8734076433121019,"run with given experimental conditions).
757"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8742038216560509,"• The method for calculating the error bars should be explained (closed form formula,
758"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.875,"call to a library function, bootstrap, etc.)
759"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8757961783439491,"• The assumptions made should be given (e.g., Normally distributed errors).
760"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8765923566878981,"• It should be clear whether the error bar is the standard deviation or the standard error
761"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8773885350318471,"of the mean.
762"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8781847133757962,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
763"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8789808917197452,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
764"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8797770700636943,"of Normality of errors is not verified.
765"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8805732484076433,"• For asymmetric distributions, the authors should be careful not to show in tables or
766"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8813694267515924,"figures symmetric error bars that would yield results that are out of range (e.g. negative
767"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8821656050955414,"error rates).
768"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8829617834394905,"• If error bars are reported in tables or plots, The authors should explain in the text how
769"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8837579617834395,"they were calculated and reference the corresponding figures or tables in the text.
770"
EXPERIMENTS COMPUTE RESOURCES,0.8845541401273885,"8. Experiments Compute Resources
771"
EXPERIMENTS COMPUTE RESOURCES,0.8853503184713376,"Question: For each experiment, does the paper provide sufficient information on the com-
772"
EXPERIMENTS COMPUTE RESOURCES,0.8861464968152867,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
773"
EXPERIMENTS COMPUTE RESOURCES,0.8869426751592356,"the experiments?
774"
EXPERIMENTS COMPUTE RESOURCES,0.8877388535031847,"Answer: [Yes]
775"
EXPERIMENTS COMPUTE RESOURCES,0.8885350318471338,"Justification: We provide sufficient information on the computer resources (type of compute
776"
EXPERIMENTS COMPUTE RESOURCES,0.8893312101910829,"workers, memory, time of execution) needed to reproduce the experiments.
777"
EXPERIMENTS COMPUTE RESOURCES,0.8901273885350318,"Guidelines:
778"
EXPERIMENTS COMPUTE RESOURCES,0.8909235668789809,"• The answer NA means that the paper does not include experiments.
779"
EXPERIMENTS COMPUTE RESOURCES,0.89171974522293,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
780"
EXPERIMENTS COMPUTE RESOURCES,0.892515923566879,"or cloud provider, including relevant memory and storage.
781"
EXPERIMENTS COMPUTE RESOURCES,0.893312101910828,"• The paper should provide the amount of compute required for each of the individual
782"
EXPERIMENTS COMPUTE RESOURCES,0.8941082802547771,"experimental runs as well as estimate the total compute.
783"
EXPERIMENTS COMPUTE RESOURCES,0.8949044585987261,"• The paper should disclose whether the full research project required more compute
784"
EXPERIMENTS COMPUTE RESOURCES,0.8957006369426752,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
785"
EXPERIMENTS COMPUTE RESOURCES,0.8964968152866242,"didn’t make it into the paper).
786"
CODE OF ETHICS,0.8972929936305732,"9. Code Of Ethics
787"
CODE OF ETHICS,0.8980891719745223,"Question: Does the research conducted in the paper conform, in every respect, with the
788"
CODE OF ETHICS,0.8988853503184714,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
789"
CODE OF ETHICS,0.8996815286624203,"Answer: [Yes]
790"
CODE OF ETHICS,0.9004777070063694,"Justification: We conduct in the paper conform, in every respect, with the NeurIPS Code of
791"
CODE OF ETHICS,0.9012738853503185,"Ethics.
792"
CODE OF ETHICS,0.9020700636942676,"Guidelines:
793"
CODE OF ETHICS,0.9028662420382165,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
794"
CODE OF ETHICS,0.9036624203821656,"• If the authors answer No, they should explain the special circumstances that require a
795"
CODE OF ETHICS,0.9044585987261147,"deviation from the Code of Ethics.
796"
CODE OF ETHICS,0.9052547770700637,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
797"
CODE OF ETHICS,0.9060509554140127,"eration due to laws or regulations in their jurisdiction).
798"
BROADER IMPACTS,0.9068471337579618,"10. Broader Impacts
799"
BROADER IMPACTS,0.9076433121019108,"Question: Does the paper discuss both potential positive societal impacts and negative
800"
BROADER IMPACTS,0.9084394904458599,"societal impacts of the work performed?
801"
BROADER IMPACTS,0.9092356687898089,"Answer: [Yes]
802"
BROADER IMPACTS,0.910031847133758,"Justification: We discuss the potential positive societal impacts and negative societal impacts
803"
BROADER IMPACTS,0.910828025477707,"in Section 6 of the main text.
804"
BROADER IMPACTS,0.9116242038216561,"Guidelines:
805"
BROADER IMPACTS,0.9124203821656051,"• The answer NA means that there is no societal impact of the work performed.
806"
BROADER IMPACTS,0.9132165605095541,"• If the authors answer NA or No, they should explain why their work has no societal
807"
BROADER IMPACTS,0.9140127388535032,"impact or why the paper does not address societal impact.
808"
BROADER IMPACTS,0.9148089171974523,"• Examples of negative societal impacts include potential malicious or unintended uses
809"
BROADER IMPACTS,0.9156050955414012,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
810"
BROADER IMPACTS,0.9164012738853503,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
811"
BROADER IMPACTS,0.9171974522292994,"groups), privacy considerations, and security considerations.
812"
BROADER IMPACTS,0.9179936305732485,"• The conference expects that many papers will be foundational research and not tied
813"
BROADER IMPACTS,0.9187898089171974,"to particular applications, let alone deployments. However, if there is a direct path to
814"
BROADER IMPACTS,0.9195859872611465,"any negative applications, the authors should point it out. For example, it is legitimate
815"
BROADER IMPACTS,0.9203821656050956,"to point out that an improvement in the quality of generative models could be used to
816"
BROADER IMPACTS,0.9211783439490446,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
817"
BROADER IMPACTS,0.9219745222929936,"that a generic algorithm for optimizing neural networks could enable people to train
818"
BROADER IMPACTS,0.9227707006369427,"models that generate Deepfakes faster.
819"
BROADER IMPACTS,0.9235668789808917,"• The authors should consider possible harms that could arise when the technology is
820"
BROADER IMPACTS,0.9243630573248408,"being used as intended and functioning correctly, harms that could arise when the
821"
BROADER IMPACTS,0.9251592356687898,"technology is being used as intended but gives incorrect results, and harms following
822"
BROADER IMPACTS,0.9259554140127388,"from (intentional or unintentional) misuse of the technology.
823"
BROADER IMPACTS,0.9267515923566879,"• If there are negative societal impacts, the authors could also discuss possible mitigation
824"
BROADER IMPACTS,0.927547770700637,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
825"
BROADER IMPACTS,0.928343949044586,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
826"
BROADER IMPACTS,0.929140127388535,"feedback over time, improving the efficiency and accessibility of ML).
827"
SAFEGUARDS,0.9299363057324841,"11. Safeguards
828"
SAFEGUARDS,0.9307324840764332,"Question: Does the paper describe safeguards that have been put in place for responsible
829"
SAFEGUARDS,0.9315286624203821,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
830"
SAFEGUARDS,0.9323248407643312,"image generators, or scraped datasets)?
831"
SAFEGUARDS,0.9331210191082803,"Answer: [NA]
832"
SAFEGUARDS,0.9339171974522293,"Justification: This paper poses no such risks.
833"
SAFEGUARDS,0.9347133757961783,"Guidelines:
834"
SAFEGUARDS,0.9355095541401274,"• The answer NA means that the paper poses no such risks.
835"
SAFEGUARDS,0.9363057324840764,"• Released models that have a high risk for misuse or dual-use should be released with
836"
SAFEGUARDS,0.9371019108280255,"necessary safeguards to allow for controlled use of the model, for example by requiring
837"
SAFEGUARDS,0.9378980891719745,"that users adhere to usage guidelines or restrictions to access the model or implementing
838"
SAFEGUARDS,0.9386942675159236,"safety filters.
839"
SAFEGUARDS,0.9394904458598726,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
840"
SAFEGUARDS,0.9402866242038217,"should describe how they avoided releasing unsafe images.
841"
SAFEGUARDS,0.9410828025477707,"• We recognize that providing effective safeguards is challenging, and many papers do
842"
SAFEGUARDS,0.9418789808917197,"not require this, but we encourage authors to take this into account and make a best
843"
SAFEGUARDS,0.9426751592356688,"faith effort.
844"
LICENSES FOR EXISTING ASSETS,0.9434713375796179,"12. Licenses for existing assets
845"
LICENSES FOR EXISTING ASSETS,0.9442675159235668,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
846"
LICENSES FOR EXISTING ASSETS,0.9450636942675159,"the paper, properly credited and are the license and terms of use explicitly mentioned and
847"
LICENSES FOR EXISTING ASSETS,0.945859872611465,"properly respected?
848"
LICENSES FOR EXISTING ASSETS,0.946656050955414,"Answer: [Yes]
849"
LICENSES FOR EXISTING ASSETS,0.947452229299363,"Justification: URL and license for existing assets we used are provided in Appendix H.
850"
LICENSES FOR EXISTING ASSETS,0.9482484076433121,"Guidelines:
851"
LICENSES FOR EXISTING ASSETS,0.9490445859872612,"• The answer NA means that the paper does not use existing assets.
852"
LICENSES FOR EXISTING ASSETS,0.9498407643312102,"• The authors should cite the original paper that produced the code package or dataset.
853"
LICENSES FOR EXISTING ASSETS,0.9506369426751592,"• The authors should state which version of the asset is used and, if possible, include a
854"
LICENSES FOR EXISTING ASSETS,0.9514331210191083,"URL.
855"
LICENSES FOR EXISTING ASSETS,0.9522292993630573,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
856"
LICENSES FOR EXISTING ASSETS,0.9530254777070064,"• For scraped data from a particular source (e.g., website), the copyright and terms of
857"
LICENSES FOR EXISTING ASSETS,0.9538216560509554,"service of that source should be provided.
858"
LICENSES FOR EXISTING ASSETS,0.9546178343949044,"• If assets are released, the license, copyright information, and terms of use in the
859"
LICENSES FOR EXISTING ASSETS,0.9554140127388535,"package should be provided. For popular datasets, paperswithcode.com/datasets
860"
LICENSES FOR EXISTING ASSETS,0.9562101910828026,"has curated licenses for some datasets. Their licensing guide can help determine the
861"
LICENSES FOR EXISTING ASSETS,0.9570063694267515,"license of a dataset.
862"
LICENSES FOR EXISTING ASSETS,0.9578025477707006,"• For existing datasets that are re-packaged, both the original license and the license of
863"
LICENSES FOR EXISTING ASSETS,0.9585987261146497,"the derived asset (if it has changed) should be provided.
864"
LICENSES FOR EXISTING ASSETS,0.9593949044585988,"• If this information is not available online, the authors are encouraged to reach out to
865"
LICENSES FOR EXISTING ASSETS,0.9601910828025477,"the asset’s creators.
866"
NEW ASSETS,0.9609872611464968,"13. New Assets
867"
NEW ASSETS,0.9617834394904459,"Question: Are new assets introduced in the paper well documented and is the documentation
868"
NEW ASSETS,0.9625796178343949,"provided alongside the assets?
869"
NEW ASSETS,0.9633757961783439,"Answer: [Yes]
870"
NEW ASSETS,0.964171974522293,"Justification: new assets introduced in this paper are well documented and provided alongside
871"
NEW ASSETS,0.964968152866242,"the assets.
872"
NEW ASSETS,0.9657643312101911,"Guidelines:
873"
NEW ASSETS,0.9665605095541401,"• The answer NA means that the paper does not release new assets.
874"
NEW ASSETS,0.9673566878980892,"• Researchers should communicate the details of the dataset/code/model as part of their
875"
NEW ASSETS,0.9681528662420382,"submissions via structured templates. This includes details about training, license,
876"
NEW ASSETS,0.9689490445859873,"limitations, etc.
877"
NEW ASSETS,0.9697452229299363,"• The paper should discuss whether and how consent was obtained from people whose
878"
NEW ASSETS,0.9705414012738853,"asset is used.
879"
NEW ASSETS,0.9713375796178344,"• At submission time, remember to anonymize your assets (if applicable). You can either
880"
NEW ASSETS,0.9721337579617835,"create an anonymized URL or include an anonymized zip file.
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9729299363057324,"14. Crowdsourcing and Research with Human Subjects
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737261146496815,"Question: For crowdsourcing experiments and research with human subjects, does the paper
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9745222929936306,"include the full text of instructions given to participants and screenshots, if applicable, as
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9753184713375797,"well as details about compensation (if any)?
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761146496815286,"Answer: [NA]
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769108280254777,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777070063694268,"Guidelines:
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785031847133758,"• The answer NA means that the paper does not involve crowdsourcing nor research with
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792993630573248,"human subjects.
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800955414012739,"• Including this information in the supplemental material is fine, but if the main contribu-
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808917197452229,"tion of the paper involves human subjects, then as much detail as possible should be
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981687898089172,"included in the main paper.
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982484076433121,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98328025477707,"or other labor should be paid at least the minimum wage in the country of the data
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840764331210191,"collector.
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848726114649682,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856687898089171,"Subjects
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864649681528662,"Question: Does the paper describe potential risks incurred by study participants, whether
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872611464968153,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880573248407644,"approvals (or an equivalent approval/review based on the requirements of your country or
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888535031847133,"institution) were obtained?
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896496815286624,"Answer: [NA]
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904458598726115,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912420382165605,"Guidelines:
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920382165605095,"• The answer NA means that the paper does not involve crowdsourcing nor research with
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928343949044586,"human subjects.
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936305732484076,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944267515923567,"may be required for any human subjects research. If you obtained IRB approval, you
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952229299363057,"should clearly state this in the paper.
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960191082802548,"• We recognize that the procedures for this may vary significantly between institutions
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968152866242038,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976114649681529,"guidelines for their institution.
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9984076433121019,"• For initial submissions, do not include any information that would break anonymity (if
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992038216560509,"applicable), such as the institution conducting the review.
915"
