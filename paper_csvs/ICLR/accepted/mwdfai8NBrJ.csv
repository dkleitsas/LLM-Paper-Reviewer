Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002898550724637681,"The study of provable adversarial robustness for deep neural networks (DNNs)
has mainly focused on static supervised learning tasks such as image classiﬁca-
tion. However, DNNs have been used extensively in real-world adaptive tasks
such as reinforcement learning (RL), making such systems vulnerable to adver-
sarial attacks as well. Prior works in provable robustness in RL seek to certify
the behaviour of the victim policy at every time-step against a non-adaptive ad-
versary using methods developed for the static setting. But in the real world, an
RL adversary can infer the defense strategy used by the victim agent by observ-
ing the states, actions, etc. from previous time-steps and adapt itself to produce
stronger attacks in future steps (e.g., by focusing more on states critical to the
agent’s performance). We present an efﬁcient procedure, designed speciﬁcally to
defend against an adaptive RL adversary, that can directly certify the total reward
without requiring the policy to be robust at each time-step. Focusing on random-
ized smoothing based defenses, our main theoretical contribution is to prove an
adaptive version of the Neyman-Pearson Lemma – a key lemma for smoothing-
based certiﬁcates – where the adversarial perturbation at a particular time can be
a stochastic function of current and previous observations and states as well as
previous actions. Building on this result, we propose policy smoothing where the
agent adds a Gaussian noise to its observation at each time-step before passing
it through the policy function. Our robustness certiﬁcates guarantee that the ﬁ-
nal total reward obtained by policy smoothing remains above a certain threshold,
even though the actions at intermediate time-steps may change under the attack.
We show that our certiﬁcates are tight by constructing a worst-case scenario that
achieves the bounds derived in our analysis. Our experiments on various environ-
ments like Cartpole, Pong, Freeway and Mountain Car show that our method can
yield meaningful robustness guarantees in practice."
INTRODUCTION,0.005797101449275362,"1
INTRODUCTION"
INTRODUCTION,0.008695652173913044,"Deep neural networks (DNNs) have been widely employed for reinforcement learning (RL) prob-
lems as they enable the learning of policies directly from raw sensory inputs, like images, with min-
imal intervention from humans. From achieving super-human level performance in video-games
(Mnih et al., 2013; Schulman et al., 2015; Mnih et al., 2016), Chess (Silver et al., 2017) and Go
(Silver et al., 2016) to carrying out complex real-world tasks, such as controlling a robot (Levine
et al., 2016) and driving a vehicle (Bojarski et al., 2016), deep-learning based algorithms have not
only established the state of the art, but also become more effortless to train. However, DNNs have
been shown to be susceptible to tiny malicious perturbations of the input designed to completely
alter their predictions (Szegedy et al., 2014; Madry et al., 2018; Goodfellow et al., 2015). In the RL
setting, an attacker may either directly corrupt the observations of an RL agent (Huang et al., 2017;
Behzadan & Munir, 2017; Pattanaik et al., 2018) or act adversarially in the environment (Gleave
et al., 2020) to signiﬁcantly degrade the performance of the victim agent. Most of the adversarial
defense literature has focused mainly on classiﬁcation tasks (Kurakin et al., 2017; Buckman et al.,
2018; Guo et al., 2018; Dhillon et al., 2018; Li & Li, 2017; Grosse et al., 2017; Gong et al., 2017)."
INTRODUCTION,0.011594202898550725,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014492753623188406,"In this paper, we study a defense procedure for RL problems that is provably robust against norm-
bounded adversarial perturbations of the observations of the victim agent."
INTRODUCTION,0.017391304347826087,"Problem setup. A reinforcement learning task is commonly described as a game between an agent
and an environment characterized by the Markov Decision Process (MDP) M = (S, A, T, R, γ),
where S is a set of states, A is a set of actions, T is the transition probability function, R is the one-
step reward function and γ ∈[0, 1] is the discount factor. However, as described in Section 3, our
analysis applies to an even more general setting than MDPs. At each time-step t, the agent makes an
observation ot = o(st) ∈Rd which is a probabilistic function of the current state of the environment,
picks an action at ∈A and receives an immediate reward Rt = R(st, at). We deﬁne an adversary
as an entity that can corrupt the agent’s observations of the environment by augmenting them with
a perturbation ϵt at each time-step t which can depend on the states, actions, observations, etc.,
generated so far. We use ϵ = (ϵ1, ϵ2, . . .) to denote the entire sequence of adversarial perturbations.
The goal of the adversary is to minimize the total reward obtained by the agent policy π while
keeping the overall ℓ2-norm of the perturbation within a budget B. Formally, the adversary seeks to
optimize the following objective:"
INTRODUCTION,0.020289855072463767,"min
ϵ
Eπ "" ∞
X"
INTRODUCTION,0.02318840579710145,"t=0
γtRt #"
INTRODUCTION,0.02608695652173913,", where Rt = R(st, at), at ∼π(·|o(st) + ϵt)"
INTRODUCTION,0.028985507246376812,"s.t. ∥(ϵ1, ϵ2, . . .)∥2 ="
INTRODUCTION,0.03188405797101449,"v
u
u
t ∞
X"
INTRODUCTION,0.034782608695652174,"t=0
∥ϵt∥2
2 ≤B."
INTRODUCTION,0.03768115942028986,"Note that the size of the perturbation ϵt in each time-step t need not be the same and the adversary
may choose to distribute the budget B over different time-steps in a way that allows it to produce a
stronger attack. Also, our formulation accounts for cases when the agent may only partially observe
the state of the environment, making M a Partially Observable Markov Decision Process (POMDP)."
INTRODUCTION,0.04057971014492753,"Objective. Our goal in provably robust RL is to design a policy π such that the total reward in the
presence of a norm-bounded adversary is guaranteed to remain above a certain threshold, i.e.,"
INTRODUCTION,0.043478260869565216,"min
ϵ
Eπ "" ∞
X"
INTRODUCTION,0.0463768115942029,"t=0
γtRt #"
INTRODUCTION,0.04927536231884058,"≥R, s.t. ∥ϵ∥2 ≤B.
(1)"
INTRODUCTION,0.05217391304347826,"In other words, no norm-bounded adversary can lower the expected total reward of the policy π
below a certain threshold. In our discussion, we restrict out focus to ﬁnite-step games that end after
t time-steps. This is a reasonable approximation for inﬁnite games with γ < 1, as for a sufﬁciently
large t, γt becomes negligibly small. For games where γ = 1, Rt must become sufﬁciently small
after a ﬁnite number of steps to keep the total reward ﬁnite."
INTRODUCTION,0.05507246376811594,"Step-wise vs. episodic certiﬁcates. Previous works on robust RL have sought to certify the be-
haviour of the policy function at each time-step of an episode, e.g., the output of a Deep Q-Network
(L¨utjens et al., 2019) and the action taken for a given state (Zhang et al., 2020). Ensuring that the
behaviour of the policy remains unchanged in each step can also certify that the ﬁnal total reward
remains the same under attack. However, if the per-step guarantee fails at even one of the interme-
diate steps, the certiﬁcate on the total reward becomes vacuous or impractical to compute (as noted
in Appendix E of Zhang et al. (2020)). Our approach gets around this issue by directly certifying
the ﬁnal total reward for the entire episode without requiring the policy to be provably robust at each
intermediate step. Also, the threat-model we consider is more general as we allow the adversary to
choose the size of the perturbation for each time-step. Thus, our method can defend against more
sophisticated attacks that focus more on states that are crucial for the victim agent’s performance."
INTRODUCTION,0.057971014492753624,"Technical contributions.
In this paper, we study a defense procedure based on “randomized
smoothing” (Cohen et al., 2019; L´ecuyer et al., 2019; Li et al., 2019; Salman et al., 2019) since
at least in “static” settings, its robustness guarantee scales up to high-dimensional problems and
does not need to make stringent assumptions about the model. We ask: can we utilize the beneﬁts of
randomized smoothing to make a general high-dimensional RL policy provably robust against ad-
versarial attacks? The answer to this question turns out to be non-trivial as the adaptive nature of the
adversary in the RL setting makes it difﬁcult to apply certiﬁcates from the static setting. For exam-
ple, the ℓ2-certiﬁcate by Cohen et al. (2019) critically relies on the clean and adversarial distributions"
INTRODUCTION,0.06086956521739131,Published as a conference paper at ICLR 2022
INTRODUCTION,0.06376811594202898,"(a)
(b) (c) (d)"
INTRODUCTION,0.06666666666666667,"Static (Classification) Setting
Interactive (RL) Setting"
INTRODUCTION,0.06956521739130435,"Figure 1:
The standard Cohen et al. (2019) smoothing-based robustness certiﬁcate relies on the
clean and the adversarial distributions being isometric Gaussians (panel a). However, adding noise
to sequential observations in an RL setting (panels b-d) does not result in an isometric Gaussian
distribution over the space of observations. In all ﬁgures, the distributions associated with clean and
adversarially-perturbed values are shown in blue and red, respectively."
INTRODUCTION,0.07246376811594203,"being isometric Gaussians (Figure 1-a). However, in the RL setting, the adversarial perturbation in
one step might depend on states, actions, observations, etc., of the previous steps, which could in
turn depend on the random Gaussian noise samples added to the observations in these steps. Thus,
the resulting adversarial distribution need not be isometric as in the static setting (Figure 1-(b-d)).
For more details on this example, see Appendix B."
INTRODUCTION,0.07536231884057971,"Our main theoretical contribution is to prove an adaptive version of the Neyman-Pearson
lemma (Neyman & Pearson, 1992) to produce robustness guarantees for RL. We emphasize that
this is not a straightforward extension (refer to Appendix D, E and F for the entire proof). To prove
this fundamental result, we ﬁrst eliminate the effect of randomization in the adversary (Lemma 1)
by converting a general adversary to one where the perturbation at each time-step is a deterministic
function of the previous states, actions, observations, etc., and showing that the modiﬁed adversary
is as strong as the general one. Then, we prove the adaptive Neyman-Pearson lemma where we show
that, in the worst-case, the deterministic adversary can be converted to one that uses up the entire
budget B in the ﬁrst coordinate of the perturbation in the ﬁrst time-step (Lemma 3). Finally, we de-
rive the robustness guarantee under an isometric Gaussian smoothing distribution (Theorem 1). In
section A, we establish the tightness of our certiﬁcates by constructing the worst-case environment-
policy pair which attains our derived bounds. More formally, out of all the environment-policy pairs
that achieve a certain total reward with probability p, we show a worst-case environment-policy pair
and a corresponding adversary such that the probability of achieving the same reward under the
presence of the adversary is minimum. A discussion on the Neyman-Pearson lemma in the context
of randomized smoothing is available in Appendix C."
INTRODUCTION,0.0782608695652174,"Building on these theoretical results, we propose Policy Smoothing, a simple model-agnostic
randomized-smoothing based technique that can provide certiﬁed robustness without increasing the
computational complexity of the agent’s policy. Our main contribution is to show that by augment-
ing the policy’s input by a random smoothing noise, we can achieve provable robustness guarantees
on the total reward under a norm-bounded adversarial attack (Section 4.2). Policy Smoothing does
not need to make assumptions about the agent’s policy function and is also oblivious to the work-
ings of RL environment. Thus, this method can be applied to any RL setting without having to
make restrictive assumptions on the environment or the agent. In section 3, we model the entire
adversarial RL process under Policy Smoothing as a sequence of interactions between a system A,
which encapsulates the RL environment and the agent, and a system B, which captures the addition
of the adversarial perturbation and the smoothing noise to the observations. Our theoretical results
do not require these systems to be Markovian and can thus have potential applications in real-time
decision-making processes that do not necessarily satisfy the Markov property."
INTRODUCTION,0.08115942028985507,"Empirical Results. We use four standard Reinforcement Learning benchmark tasks to evaluate
the effectiveness of our defense and the signiﬁcance of our theoretical results: the Atari games
‘Pong’ and ‘Freeway’ (Mnih et al., 2013) and the classical ‘Cartpole’ and ‘Mountain Car’ control
environments (Barto et al., 1983; Moore, 1990) – see Figure 3. We ﬁnd that our method provides
highly nontrivial certiﬁcates. In particular, on at least two of the tasks, ‘Pong’ and ‘cartpole’, the"
INTRODUCTION,0.08405797101449275,Published as a conference paper at ICLR 2022
INTRODUCTION,0.08695652173913043,"provable lower bounds on the average performances of the defended agents, against any adversary,
exceed the observed average performances of undefended agents under a practical attack."
PRIOR WORK,0.08985507246376812,"2
PRIOR WORK"
PRIOR WORK,0.0927536231884058,"Adversarial RL. Adversarial attacks on RL systems have been extensively studied in recent years.
DNN-based policies have been attacked by either directly corrupting their inputs (Huang et al.,
2017; Behzadan & Munir, 2017; Pattanaik et al., 2018) or by making adversarial changes in the
environment (Gleave et al., 2020). Empirical defenses based on adversarial training, whereby the
dynamics of the RL system is augmented with adversarial noise, have produced good results in
practice (Kamalaruban et al., 2020; Vinitsky et al., 2020). Zhang et al. (2021) propose training
policies together with a learned adversary in an online alternating fashion to achieve robustness to
perturbations of the agent’s observations."
PRIOR WORK,0.09565217391304348,"Robust RL. Prior work by L¨utjens et al. (2019) has proposed a ‘certiﬁed’ defense against adversarial
attacks to observations in deep reinforcement learning, particularly for Deep Q-Network agents.
However, that work essentially only guarantees the stability of the network approximated Q-value at
each time-step of an episode. By contrast, our method provides a bound on the expected true reward
of the agent under any norm-bounded adversarial attack."
PRIOR WORK,0.09855072463768116,"Zhang et al. (2020) certify that the action in each time-step remains unchanged under an adversarial
perturbation of ﬁxed budget for every time-step. This can guarantee that the ﬁnal total reward
obtained by the robust policy remains the same under attack. However, this approach would not be
able to yield any robustness certiﬁcate if even one of the intermediate actions changed under attack.
Our approach gets around this difﬁculty by directly certifying the total reward, letting some of the
intermediate actions of the robust policy to potentially change under attack. For instance, consider
an RL agent playing Atari Pong. The actions taken by the agent when the ball is close to and
approaching the paddle are signiﬁcantly more important than the ones when the ball is far away or
retreating from the paddle. By allowing some of the intermediate actions to potentially change, our
approach can certify for larger adversarial budgets and provide a more ﬁne-grained control over the
desired total-reward threshold. Moreover, we study a more general threat model where the adversary
may allocate different attack budgets for each time-step focusing more on the steps that are crucial
for the agent’s performance, e.g., attacking a Pong agent when the ball is close to the paddle."
PRIOR WORK,0.10144927536231885,"Provable Robustness in Static Settings: Notable provable robustness methods in static settings are
based on interval-bound propagation (Gowal et al., 2018; Huang et al., 2019; Dvijotham et al., 2018;
Mirman et al., 2018), curvature bounds (Wong & Kolter, 2018; Raghunathan et al., 2018; Chiang
et al., 2020; Singla & Feizi, 2019; 2020; 2021), randomized smoothing (Cohen et al., 2019; L´ecuyer
et al., 2019; Li et al., 2019; Salman et al., 2019; Levine & Feizi, 2021), etc. Certiﬁed robustness has
also been extended to problems with structured outputs such as images and sets (Kumar & Goldstein,
2021). Focusing on Gaussian smoothing, Cohen et al. (2019) showed that if a classiﬁer outputs a
class with some probability under an isometric Gaussian noise around an input point, then it will
output that class with high probability at any perturbation of the input within a particular ℓ2 distance.
Kumar et al. (2020) showed how to certify the expectation of softmax scores of a neural network
under Gaussian smoothing by using distributional information about the scores."
PRELIMINARIES AND NOTATIONS,0.10434782608695652,"3
PRELIMINARIES AND NOTATIONS"
PRELIMINARIES AND NOTATIONS,0.1072463768115942,"We model the ﬁnite-step adversarial RL framework as a t-round communication between two sys-
tems A and B (Figure 2). System A represents the RL game. It contains the environment M and
the agent, and when run independently, simulates the interactions between the two for some given
policy π. At each time-step i, it generates a token τi from some set T , which is a tuple of the cur-
rent state si and its observation oi, the action ai−1 in the previous step (and potentially some other
objects that we ignore in this discussion), i.e., τi = (si, ai−1, oi, . . .) ∈S × A × Rd × . . . = T .
For the ﬁrst step, replace the action in τ1 with some dummy element ∗from the action space A.
System B comprises of the adversary and the smoothing distribution which generate an adversarial
perturbation ϵi and a smoothing noise vector δi, respectively, at each time-step i, the sum of which
is denoted by an offset ηi = ϵi + δi ∈Rd."
PRELIMINARIES AND NOTATIONS,0.11014492753623188,Published as a conference paper at ICLR 2022
PRELIMINARIES AND NOTATIONS,0.11304347826086956,Figure 2: Adversarial robustness framework.
PRELIMINARIES AND NOTATIONS,0.11594202898550725,"When both systems are run together in an interactive fashion, in each round i, system A generates
τi as a probabilistic function of τ1, η1, τ2, η2, . . . , τi−1, ηi−1, i.e., τi : (T × Rd)i−1 →∆(T ). τ1 is
sampled from a ﬁxed distribution. It passes τi to B, which generates ϵi as a probabilistic function
of {τj, ηj}i−1
j=1 and τi, i.e., ϵi : (T × Rd)i−1 × T →∆(Rd) and adds a noise vector δi sampled
independently from the smoothing distribution to obtain ηi. It then passes ηi to A for the next round.
After running for t steps, a deterministic or random 0/1-function h is computed over all the tokens
and offsets generated. We are interested in bounding the probability with which h outputs 1 as a
function of the adversarial budget B. In the RL setting, h could be a function indicating whether the
total reward is above a certain threshold or not."
PROVABLY ROBUST RL,0.11884057971014493,"4
PROVABLY ROBUST RL"
ADAPTIVE NEYMAN-PEARSON LEMMA,0.12173913043478261,"4.1
ADAPTIVE NEYMAN-PEARSON LEMMA"
ADAPTIVE NEYMAN-PEARSON LEMMA,0.1246376811594203,"Let X be the random variable representing the tuple z = (τ1, η1, τ2, η2, . . . , τt, ηt) ∈(T × Rd)t
when there is no adversary, i.e., ϵi = 0 and ηi = δi is sampled directly form the smoothing distri-
bution P. Let Y be the random variable representing the same tuple in the presence of a general
adversary ϵ satisfying ∥ϵ∥2 ≤B. Thus, if h(X) = 1 with some probability p, we are interested in
deriving a lower-bound on the probability of h(Y ) = 1 as a function of p and B. Let us now deﬁne
a deterministic adversary ϵdt for which the adversarial perturbation at each step is a deterministic
function of the tokens and offsets of the previous steps and the token generated in the current step.
i.e., ϵdt
i
: (T × Rd)i−1 × T →Rd. Let Y dt be its corresponding random variable. Then, we have
the following lemma that converts a probabilistic adversary into a deterministic one."
ADAPTIVE NEYMAN-PEARSON LEMMA,0.12753623188405797,"Lemma 1 (Reduction to Deterministic Adversaries). For any general adversary ϵ and an Γ ⊆
(T × Rd)t, there exists a deterministic adversary ϵdt such that,"
ADAPTIVE NEYMAN-PEARSON LEMMA,0.13043478260869565,"P[Y dt ∈Γ] ≤P[Y ∈Γ],"
ADAPTIVE NEYMAN-PEARSON LEMMA,0.13333333333333333,where Y dt is the random variable for the distribution deﬁned by the adversary ϵdt.
ADAPTIVE NEYMAN-PEARSON LEMMA,0.13623188405797101,"This lemma says that for any adversary (deterministic or random) and a subset Γ of the space of
z, there exists a deterministic adversary which assigns a lower probability to Γ than the general
adversary. In the RL setting, this means that the probability with which a smoothed policy achieves
a certain reward value under a general adversary is lower-bounded by the probability of the same
under a deterministic adversary. The intuition behind this lemma is that out of all the possible values
that the internal randomness of the adversary may assume, there exists a sequence of values that
assigns the minimum probability to Γ (over the randomness of the environment, policy, smoothing
noise, etc.). We defer the proof to the appendix."
ADAPTIVE NEYMAN-PEARSON LEMMA,0.1391304347826087,"Next, we formulate an adaptive version of the Neyman-Pearson lemma for the case when the smooth-
ing distribution P is an isometric Gaussian N(0, σ2I). If we applied the classical Neyman-Pearson
lemma on the distributions of X and Y dt, it will give us a characterization of the worst-case 0/1
function among the class of functions that achieve a certain probability p of being 1 under the dis-
tribution of X that has the minimum probability of being 1 under Y dt. Let µX and µY dt be the
probability density function of X and Y dt, respectively."
ADAPTIVE NEYMAN-PEARSON LEMMA,0.14202898550724638,Published as a conference paper at ICLR 2022
ADAPTIVE NEYMAN-PEARSON LEMMA,0.14492753623188406,"Lemma 2 (Neyman-Pearson Lemma, 1933). If ΓY dt = {z ∈(T × Rd)t | µY dt(z) ≤qµX(z)}
for some q ≥0 and P[h(X) = 1] ≥P[X ∈ΓY dt], then P[h(Y dt) = 1] ≥P[Y dt ∈ΓY dt]."
ADAPTIVE NEYMAN-PEARSON LEMMA,0.14782608695652175,"For an arbitrary element h in the class of functions Hp = {h | P[h(X) = 1] ≥p}, construct the set
ΓY dt for an appropriate value of q for which P[X ∈ΓY dt] = p. Now, consider a function h′ which
is 1 if its input comes from ΓY dt and 0 otherwise. Then, the above lemma says that the function h′
has the minimum probability of being 1 under Y dt, i.e.,"
ADAPTIVE NEYMAN-PEARSON LEMMA,0.15072463768115943,"h′ = argmin
h∈Hp
P[h(Y dt) = 1]."
ADAPTIVE NEYMAN-PEARSON LEMMA,0.1536231884057971,"This gives us the worst-case function that achieves the minimum probability under an adversarial
distribution. However, in the adaptive setting, ΓY dt could be a very complicated set and obtain-
ing an expression for P[Y dt ∈ΓY dt] might be difﬁcult. To simplify our analysis, we construct a
structured deterministic adversary ϵst which exhausts its entire budget in the ﬁrst coordinate of the
ﬁrst perturbation vector, i.e., ϵst
1 = (B, 0, . . . , 0) and ϵst
i = (0, 0, . . . , 0) for i > 1. Let Y st be the
corresponding random variable and µY st its density function. We formulate the following adaptive
version of the Neyman-Pearson lemma:"
ADAPTIVE NEYMAN-PEARSON LEMMA,0.1565217391304348,"Lemma 3 (Adaptive Neyman-Pearson Lemma). If ΓY st = {z ∈(T ×Rd)t | µY st(z) ≤qµX(z)}
for some q ≥0 and P[h(X) = 1] ≥P[X ∈ΓY st], then P[h(Y dt) = 1] ≥P[Y st ∈ΓY st]."
ADAPTIVE NEYMAN-PEARSON LEMMA,0.15942028985507245,"The key difference from the classical version is that the worst-case set we construct in this lemma
is for the structured adversary and the ﬁnal inequality relates the probability of h outputting 1 under
the adaptive adversary to the probability that the structured adversary assigns to the worst-case set.
It says that for the appropriate value of q for which P[X ∈ΓY st] = p, any function h ∈Hp outputs
1 with at least the probability that Y st assigns to ΓY st. It shows that over all possible functions in
Hp and over all possible adversaries ϵ, the indicator function 1z∈ΓY st and the structured adversary
capture the worst-case scenario where probability of h being 1 under the adversarial distribution is
the minimum. Since both Y st and X are just isometric Gaussian distribution with the same variance
σ2 centered at different points on the ﬁrst coordinate of η1, the set ΓY st is the set of all tuples z for
which {η1}1 is below a certain threshold.1 We use lemmas 1 and 3 to derive the ﬁnal bound on the
probability of h(Y ) = 1 in the following theorem, the proof of which is deferred to the appendix."
ADAPTIVE NEYMAN-PEARSON LEMMA,0.16231884057971013,"Theorem 1 (Robustness Guarantee). For an isometric Gaussian smoothing noise with variance
σ2, if P[h(X) = 1] ≥p, then:"
ADAPTIVE NEYMAN-PEARSON LEMMA,0.16521739130434782,"P[h(Y ) = 1] ≥Φ(Φ−1(p) −B/σ),"
ADAPTIVE NEYMAN-PEARSON LEMMA,0.1681159420289855,where Φ is the standard normal CDF.
ADAPTIVE NEYMAN-PEARSON LEMMA,0.17101449275362318,The above analysis can be adapted to obtain an upper-bound on P[h(Y ) = 1] of Φ(Φ−1(p) + B/σ).
POLICY SMOOTHING,0.17391304347826086,"4.2
POLICY SMOOTHING"
POLICY SMOOTHING,0.17681159420289855,"Building on these results, we develop policy smoothing, a simple model-agnostic randomized-
smoothing based technique that can provide certiﬁed robustness without increasing the computa-
tional complexity of the agent’s policy. Given a policy π, we deﬁne a smoothed policy ¯π as:"
POLICY SMOOTHING,0.17971014492753623,"¯π ( · | o(st)) = π ( · | o(st) + δt) , where δt ∼N(0, σ2I)."
POLICY SMOOTHING,0.1826086956521739,"Our goal is to certify the expected sum of the rewards collected over multiple time-steps under policy
¯π. We modify the technique developed by Kumar et al. (2020) to certify the expected class scores
of a neural network by using the empirical cumulative distribution function (CDF) of the scores
under the smoothing distribution to work for the RL setting. This approach utilizes the fact that
the expected value of a random variable X representing a class score under a Gaussian N(0, σ2I)
smoothing noise can be expressed using its CDF F(.) as below:"
POLICY SMOOTHING,0.1855072463768116,"E[X] =
Z ∞"
POLICY SMOOTHING,0.18840579710144928,"0
(1 −F(x))dx −
Z 0"
POLICY SMOOTHING,0.19130434782608696,"−∞
F(x)dx.
(2)"
POLICY SMOOTHING,0.19420289855072465,1We use {ηi}j to denote the jth coordinate of the vector ηi.
POLICY SMOOTHING,0.19710144927536233,Published as a conference paper at ICLR 2022
POLICY SMOOTHING,0.2,"(b) Pong
(a) Cartpole
(c) Freeway
(d) Mountain Car"
POLICY SMOOTHING,0.2028985507246377,"Figure 3: Environments used in evaluations rendered by OpenAI Gym (Brockman et al., 2016)."
POLICY SMOOTHING,0.20579710144927535,"Given m samples {xi}m
i=1 of the random variable X, let us deﬁne its empirical CDF at a point x,
Fm(x) = |{xi | xi ≤x}|/m, as the fraction of samples that are less than or equal to x. Using
Fm(x), the Dvoretzky–Kiefer–Wolfowitz inequality can produce high-conﬁdence bounds on the
true CDF of X. It says that with probability 1 −α, for α ∈(0, 1], the true CDF F(x) is in the range
[F(x), F(x)], where F(x) = Fm(x) −
p"
POLICY SMOOTHING,0.20869565217391303,"ln(2/α)/2m and F(x) = Fm(x) +
p"
POLICY SMOOTHING,0.21159420289855072,"ln(2/α)/2m. For
an adversarial perturbation of ℓ2-size B, the result of Cohen et al. (2019) bounds the CDF within
[Φ(Φ−1(F(x)) −B/σ), Φ(Φ−1(F(x)) + B/σ)], which in turn bounds E[X] using equation (2)."
POLICY SMOOTHING,0.2144927536231884,"In the RL setting, we can model the total reward as a random variable and obtain its empirical CDF
by playing the game using policy ¯π. As above, we can bound the CDF F(x) of the total reward in
a range [F(x), F(x)] using the empirical CDF. Applying Theorem 1, we can bound the CDF within
[Φ(Φ−1(F(x)) −B/σ), Φ(Φ−1(F(x)) + B/σ)] for an ℓ2 adversary of size B. The function h in
Theorem 1 could represent the CDF F(x) by indicating whether the total reward computed for an
input z ∈(T × Rd)t is below a value x. Finally, equation (2) puts bounds on the expected total
reward under an adversarial attack."
EXPERIMENTS,0.21739130434782608,"5
EXPERIMENTS"
ENVIRONMENTS AND SETUP,0.22028985507246376,"5.1
ENVIRONMENTS AND SETUP"
ENVIRONMENTS AND SETUP,0.22318840579710145,"We tested on four standard environments: the classical cortrol problems ‘Cartpole’ and ‘Mountain
Car’ and the Atari games ‘Pong’ and ‘Freeway.’ We consider three tasks which use a discrete action
space (‘Cartpole’ and the two Atari games) as well as one task that uses a continuous action space
(‘Mountain Car’). For the discrete action space tasks, we use a standard Deep Q-Network (DQN)
(Mnih et al., 2013) model, while for ‘Mountain Car’, we use Deep Deterministic Policy Gradient
(DDPG) (Lillicrap et al., 2016)."
ENVIRONMENTS AND SETUP,0.22608695652173913,"As is common in DQN and DDPG, our agents choose actions based on multiple frames of observa-
tions. In order to apply a realistic threat model, we assume that the adversary acts on each frame only
once when it is ﬁrst observed. The adversarial distortion is then maintained when the same frame is
used in future time-steps. In other words, we consider the observation at time step ot (discussed in
Section 3) to be only the new observation at time t: this means that the adversarial/noise perturbation
ηt, as a ﬁxed vector, continues to be used to select the next action for several subsequent time-steps.
This is a realistic model because we are assuming that the adversary can affect the agent’s observa-
tion of states, not necessarily the agent’s memory of previous observations. As in other works on
smoothing-based defenses (e.g., Cohen et al. (2019)), we add noise during training as well as at test
time. We use DQN and DDPG implementations from the popular stable-baselines3 package (Rafﬁn
et al., 2019): hyperparameters are provided in the appendix. In experiments, we report and certify
for the total non-discounted (γ = 1) reward."
ENVIRONMENTS AND SETUP,0.2289855072463768,"In ‘Cartpole’, the observation vector consists of four kinematic features. We use a simple MLP
model for the Q-network, and tested two variations: one in which the agent uses ﬁve frames of
observation, and one in which the agent uses only a single frame (shown in the appendix)."
ENVIRONMENTS AND SETUP,0.2318840579710145,"In order to show the effectiveness of our technique on tasks involving high-dimensional state ob-
servations, we chose two tasks (‘Pong’ and ‘Freeway’) from the Atari environment, where state
observations are image frames, observed as 84 × 84 pixel greyscale images. For ‘Pong’, we test on
a “one-round” variant of the original environment. In our variant, the game ends after one player,
either the agent or the opponent, scores a goal: the reward is then either zero or one. Note that this is
not a one-timestep episode: it takes typically on the order of 100 timesteps for this to occur. Results"
ENVIRONMENTS AND SETUP,0.23478260869565218,Published as a conference paper at ICLR 2022
ENVIRONMENTS AND SETUP,0.23768115942028986,"Figure 4: Certiﬁed performance for various environments. The certiﬁed lower-bound on the mean
reward is based on a 95% lower conﬁdence interval estimate of the mean reward of the smoothed
model, using 10,000 episodes."
ENVIRONMENTS AND SETUP,0.24057971014492754,"for a full Pong game are presented in the appendix: as explained there, we ﬁnd that the certiﬁcates
unfortunately do not scale with the length of the game. For the ‘Freeway’ game, we play on ‘Hard’
mode and end the game after 250 timesteps."
ENVIRONMENTS AND SETUP,0.24347826086956523,"In order to test on an environment with a continuous action space, we chose the ‘Mountain Car’ en-
vironment. Note that previous certiﬁcation results for reinforcement learning, which certify actions
at individual states rather than certifying the overall reward (Zhang et al., 2020) cannot be applied
to continuous action state problems. In this environment, the observation vector consists of two
kinematic features (position and velocity), and the action is one continuous scalar (acceleration). As
in ‘Cartpole’, we use ﬁve observation frames and a simple MLP policy. We use a slight variant of
the original environment: we do not penalize for fuel cost so the reward is a boolean representing
whether or not the car reaches the destination in the time allotted (999 steps)."
RESULTS,0.2463768115942029,"5.2
RESULTS"
RESULTS,0.2492753623188406,"Certiﬁed lower bounds on the expected total reward, as a function of the total perturbation budget,
are presented in Figure 4. For tasks with zero-one total reward (‘Pong’ and ‘Mountain Car’), the
function to be smoothed represents the total reward: h(·) = R where R is equal to 1 if the agent wins
the round, and 0 otherwise. To compute certiﬁcates on games with continuous scores (‘Cartpole’
and ‘Freeway’), we use CDF smoothing (Kumar et al., 2020): see appendix for technical details."
RESULTS,0.25217391304347825,"In order to evaluate the robustness of both undefended and policy-smoothed agents, we developed
an attack tailored to the threat model deﬁned in 1, where the adversary makes a perturbation to state
observations which is bounded over the entire episode. For DQN agents, as in L¨utjens et al. (2019),
we perturb the observation o such that the perturbation-induced action a′ := arg maxa Q(o + ϵt, a)
minimizes the (network-approximated) Q-value of the true observation Q(o, a′). However, in order
to conserve adversarial budget, we only attack if the gap between attacked q-value Q(o, a′) and the
clean q-value maxa Q(o, a) is sufﬁciently large, exceeding a preset threshold λQ. In practice, this
allows the attacker to concentrate the attack budget only on the time-steps which are critical to the
agent’s performance. When attacking DDPG, where both a Q-value network and a policy network
π are trained and the action is taken according to π, we instead minimize Q(o, π(o + ϵt)) + λ∥ϵt∥2
where the hyperparameter λ plays an analogous role in focusing perturbation budget on “important”
steps, as judged by the effect on the approximated Q-value. Empirical results are presented in
Figure 5. We see that the attacks are effective on the undefended agents (red, dashed lines). In
fact, from comparing Figures 4 and 5, we see that, for the Pong and Cartpole environments, the
undefended performance under attack is worse than the certiﬁed lower bound on the performance of
the policy-smoothed agents under any possible attack: our certiﬁcates are the clearly non-vacuous
for these environments. Further details on the attack optimizations are provided in the appendix."
RESULTS,0.25507246376811593,Published as a conference paper at ICLR 2022
RESULTS,0.2579710144927536,"Figure 5: Empirical robustness of defended and undefended agents. Full details of attacks are
presented in appendix."
RESULTS,0.2608695652173913,"We also present an attempted empirical attack on the smoothed agent, adapting techniques for at-
tacking smoothed classiﬁers from Salman et al. (2019) (solid blue lines). We observed that our
model was highly robust to this attack – signiﬁcantly more robust than guaranteed by our certiﬁcate.
However, it is not clear whether this is due to looseness in the certiﬁcate or to weakness of the attack:
the signiﬁcant practical challenges to attacking smoothed agents are also discussed in the appendix."
CONCLUSION,0.263768115942029,"6
CONCLUSION"
CONCLUSION,0.26666666666666666,"In this work, we extend randomized smoothing to design a procedure that can make any reinforce-
ment learning agent provably robust against adversarial attacks without signiﬁcantly increasing the
complexity of the agent’s policy. We show how to adapt existing theory on randomized smoothing
from static tasks such as classiﬁcation, to the dynamic setting of RL. By proving an adaptive version
of the celebrated Neyman-Pearson Lemma, we show that by adding Gaussian smoothing noise to
the input of the policy, one can certiﬁably defend it against norm-bounded adversarial perturbations
of its input. The policy smoothing technique and its theory covers a wide range of adversaries,
policies and environments. Our analysis is tight, meaning that the certiﬁcates we achieve are best
possible unless restrictive assumptions about the RL game are made. In our experiments, we show
that our method provides meaningful guarantees on the robustness of the defended policies and the
total reward they achieve even in the worst case is higher than an undefended policy. In the future,
the introduction of randomized smoothing to RL could inspire the design of provable robustness
techniques for control problems in dynamic real-world environments and multi-agent RL settings."
CONCLUSION,0.26956521739130435,REPRODUCIBILITY
CONCLUSION,0.27246376811594203,"We supplement our work with accompanying code for reproducing the experimental results, as well
as pre-trained models for a selection of the experiments. Details about setting hyper-parameters and
the environments we test are included in the appendix. Proofs for our theoretical results (lemmas
and main theorem) are also provided in the appendix."
ETHICS STATEMENT,0.2753623188405797,ETHICS STATEMENT
ETHICS STATEMENT,0.2782608695652174,"We present a method to make RL models provably robust under adversarial perturbations. We do
not foresee any immediate ethical concerns associated with our work."
ETHICS STATEMENT,0.2811594202898551,"ACKNOWLEDGEMENTS
This project was supported in part by NSF CAREER AWARD 1942230, a grant from NIST
60NANB20D134, HR001119S0026-GARD-FP-052, HR00112090132, ONR YIP award N00014-
22-1-2271, Army Grant W911NF2120076."
ETHICS STATEMENT,0.28405797101449276,Published as a conference paper at ICLR 2022
REFERENCES,0.28695652173913044,REFERENCES
REFERENCES,0.2898550724637681,"Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson.
Neuronlike adaptive elements
that can solve difﬁcult learning control problems.
IEEE Transactions on Systems, Man, and
Cybernetics, SMC-13(5):834–846, 1983. doi: 10.1109/TSMC.1983.6313077."
REFERENCES,0.2927536231884058,"Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction
attacks. In Petra Perner (ed.), Machine Learning and Data Mining in Pattern Recognition - 13th
International Conference, MLDM 2017, New York, NY, USA, July 15-20, 2017, Proceedings,
volume 10358 of Lecture Notes in Computer Science, pp. 262–275. Springer, 2017. doi: 10.1007/
978-3-319-62416-7\ 19.
URL https://doi.org/10.1007/978-3-319-62416-7_
19."
REFERENCES,0.2956521739130435,"Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,
and Karol Zieba. End to end learning for self-driving cars. CoRR, abs/1604.07316, 2016. URL
http://arxiv.org/abs/1604.07316."
REFERENCES,0.2985507246376812,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016."
REFERENCES,0.30144927536231886,"Jacob Buckman, Aurko Roy, Colin Raffel, and Ian J. Goodfellow. Thermometer encoding: One hot
way to resist adversarial examples. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings,
2018."
REFERENCES,0.30434782608695654,"Ping-yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studer, and Tom Gold-
stein. Certiﬁed defenses for adversarial patches. In 8th International Conference on Learning
Representations, 2020."
REFERENCES,0.3072463768115942,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 1310–1320, Long Beach, California, USA, 09–15 Jun 2019. PMLR."
REFERENCES,0.3101449275362319,"Guneet S. Dhillon, Kamyar Azizzadenesheli, Zachary C. Lipton, Jeremy Bernstein, Jean Kossaiﬁ,
Aran Khanna, and Animashree Anandkumar. Stochastic activation pruning for robust adversarial
defense. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018."
REFERENCES,0.3130434782608696,"Krishnamurthy Dvijotham,
Sven Gowal,
Robert Stanforth,
Relja Arandjelovic,
Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriﬁed learners with learned ver-
iﬁers, 2018."
REFERENCES,0.3159420289855073,"Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversar-
ial policies: Attacking deep reinforcement learning. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
URL https://openreview.net/forum?id=HJgEMpVFwB."
REFERENCES,0.3188405797101449,"Zhitao Gong, Wenlu Wang, and Wei-Shinn Ku. Adversarial and clean data are not twins. CoRR,
abs/1704.04960, 2017."
REFERENCES,0.3217391304347826,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
REFERENCES,0.32463768115942027,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models, 2018."
REFERENCES,0.32753623188405795,"Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick D. McDaniel.
On the (statistical) detection of adversarial examples. CoRR, abs/1702.06280, 2017."
REFERENCES,0.33043478260869563,Published as a conference paper at ICLR 2022
REFERENCES,0.3333333333333333,"Chuan Guo, Mayank Rana, Moustapha Ciss´e, and Laurens van der Maaten. Countering adversarial
images using input transformations. In 6th International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceed-
ings, 2018."
REFERENCES,0.336231884057971,"Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krish-
namurthy Dvijotham, and Pushmeet Kohli. Achieving veriﬁed robustness to symbol substitutions
via interval bound propagation. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 4081–4091,
2019. doi: 10.18653/v1/D19-1419. URL https://doi.org/10.18653/v1/D19-1419."
REFERENCES,0.3391304347826087,"Sandy H. Huang, Nicolas Papernot, Ian J. Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial
attacks on neural network policies. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net,
2017. URL https://openreview.net/forum?id=ryvlRyBKl."
REFERENCES,0.34202898550724636,"Parameswaran Kamalaruban, Yu-Ting Huang, Ya-Ping Hsieh, Paul Rolland, Cheng Shi, and
Volkan Cevher.
Robust reinforcement learning via adversarial training with langevin dy-
namics.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 8127–8138. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
5cb0e249689cd6d8369c4885435a56c2-Paper.pdf."
REFERENCES,0.34492753623188405,"Aounon Kumar and Tom Goldstein. Center smoothing for certiﬁably robust vector-valued functions.
CoRR, abs/2102.09701, 2021. URL https://arxiv.org/abs/2102.09701."
REFERENCES,0.34782608695652173,"Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein. Certifying conﬁdence via ran-
domized smoothing. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
37aa5dfc44dddd0d19d4311e2c7a0240-Abstract.html."
REFERENCES,0.3507246376811594,"Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In
5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-
26, 2017, Conference Track Proceedings, 2017. URL https://openreview.net/forum?
id=BJm4T4Kgx."
REFERENCES,0.3536231884057971,"Mathias L´ecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pp. 656–672, 2019."
REFERENCES,0.3565217391304348,"Alexander Levine and Soheil Feizi. Improved, deterministic smoothing for L1 certiﬁed robustness.
CoRR, abs/2103.10834, 2021. URL https://arxiv.org/abs/2103.10834."
REFERENCES,0.35942028985507246,"Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep vi-
suomotor policies. J. Mach. Learn. Res., 17:39:1–39:40, 2016. URL http://jmlr.org/
papers/v17/15-522.html."
REFERENCES,0.36231884057971014,"Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certiﬁed adversarial robustness with
additive noise. In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,
BC, Canada, pp. 9459–9469, 2019."
REFERENCES,0.3652173913043478,"Xin Li and Fuxin Li. Adversarial examples detection in deep networks with convolutional ﬁlter
statistics. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, Oc-
tober 22-29, 2017, pp. 5775–5783, 2017."
REFERENCES,0.3681159420289855,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learn-
ing. CoRR, abs/1509.02971, 2016."
REFERENCES,0.3710144927536232,Published as a conference paper at ICLR 2022
REFERENCES,0.3739130434782609,"Bj¨orn L¨utjens, Michael Everett, and Jonathan P. How. Certiﬁed adversarial robustness for deep
reinforcement learning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.), 3rd
Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1,
2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pp. 1328–1337.
PMLR, 2019. URL http://proceedings.mlr.press/v100/lutjens20a.html."
REFERENCES,0.37681159420289856,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings, 2018."
REFERENCES,0.37971014492753624,"Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learn-
ing Research, pp. 3578–3586. PMLR, 10–15 Jul 2018. URL http://proceedings.mlr.
press/v80/mirman18b.html."
REFERENCES,0.3826086956521739,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller.
Playing atari with deep reinforcement learning.
CoRR,
abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602."
REFERENCES,0.3855072463768116,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.3884057971014493,"Volodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd
International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-
24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1928–1937. JMLR.org,
2016. URL http://proceedings.mlr.press/v48/mniha16.html."
REFERENCES,0.391304347826087,Andrew W. Moore. Efﬁcient memory-based learning for robot control. 1990.
REFERENCES,0.39420289855072466,"J. Neyman and E. S. Pearson. On the Problem of the Most Efﬁcient Tests of Statistical Hypotheses,
pp. 73–108. Springer New York, New York, NY, 1992. ISBN 978-1-4612-0919-5. doi: 10.1007/
978-1-4612-0919-5 6. URL https://doi.org/10.1007/978-1-4612-0919-5_6."
REFERENCES,0.39710144927536234,"Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In Elisabeth Andr´e, Sven Koenig, Mehdi
Dastani, and Gita Sukthankar (eds.), Proceedings of the 17th International Conference on Au-
tonomous Agents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018,
pp. 2040–2042. International Foundation for Autonomous Agents and Multiagent Systems Rich-
land, SC, USA / ACM, 2018. URL http://dl.acm.org/citation.cfm?id=3238064."
REFERENCES,0.4,"Antonin
Rafﬁn.
Rl
baselines3
zoo.
https://github.com/DLR-RM/
rl-baselines3-zoo, 2020."
REFERENCES,0.4028985507246377,"Antonin Rafﬁn, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dor-
mann. Stable baselines3. https://github.com/DLR-RM/stable-baselines3, 2019."
REFERENCES,0.4057971014492754,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semideﬁnite relaxations for certifying ro-
bustness to adversarial examples. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, NIPS’18, pp. 10900–10910, Red Hook, NY, USA, 2018. Curran
Associates Inc."
REFERENCES,0.40869565217391307,"Hadi Salman, Jerry Li, Ilya P. Razenshteyn, Pengchuan Zhang, Huan Zhang, S´ebastien Bubeck,
and Greg Yang. Provably robust deep learning via adversarially trained smoothed classiﬁers. In
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp.
11289–11300, 2019."
REFERENCES,0.4115942028985507,Published as a conference paper at ICLR 2022
REFERENCES,0.4144927536231884,"John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust re-
gion policy optimization. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, vol-
ume 37 of JMLR Workshop and Conference Proceedings, pp. 1889–1897. JMLR.org, 2015. URL
http://proceedings.mlr.press/v37/schulman15.html."
REFERENCES,0.41739130434782606,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lilli-
crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering
the game of go with deep neural networks and tree search. Nat., 529(7587):484–489, 2016. doi:
10.1038/nature16961. URL https://doi.org/10.1038/nature16961."
REFERENCES,0.42028985507246375,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen
Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforce-
ment learning algorithm. CoRR, abs/1712.01815, 2017. URL http://arxiv.org/abs/
1712.01815."
REFERENCES,0.42318840579710143,"Sahil Singla and Soheil Feizi. Robustness certiﬁcates against adversarial examples for relu networks.
CoRR, abs/1902.01235, 2019."
REFERENCES,0.4260869565217391,"Sahil Singla and Soheil Feizi. Second-order provable defenses against adversarial attacks, 2020."
REFERENCES,0.4289855072463768,"Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. CoRR, abs/2105.11417, 2021. URL
https://arxiv.org/abs/2105.11417."
REFERENCES,0.4318840579710145,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In 2nd International Conference
on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference
Track Proceedings, 2014."
REFERENCES,0.43478260869565216,"Eugene Vinitsky, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and Alexandre M. Bayen.
Robust reinforcement learning using adversarial populations. CoRR, abs/2008.01825, 2020. URL
https://arxiv.org/abs/2008.01825."
REFERENCES,0.43768115942028984,"Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, pp. 5283–5292, 2018."
REFERENCES,0.4405797101449275,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane S. Boning, and Cho-Jui Hsieh. Ro-
bust deep reinforcement learning against adversarial perturbations on observations.
CoRR,
abs/2003.08938, 2020. URL https://arxiv.org/abs/2003.08938."
REFERENCES,0.4434782608695652,"Huan Zhang, Hongge Chen, Duane S Boning, and Cho-Jui Hsieh. Robust reinforcement learning
on state observations with learned optimal adversary. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=sCZbhBvqQaU."
REFERENCES,0.4463768115942029,Published as a conference paper at ICLR 2022
REFERENCES,0.4492753623188406,"A
TIGHTNESS OF THE CERTIFICATE"
REFERENCES,0.45217391304347826,"Here, we present a worst-case environment-policy pair that achieves the bound in Theorem 1, show-
ing that our robustness certiﬁcate is in fact tight. For a given environment M = (S, A, T, R, γ) and
a policy π, let p be a lower-bound on the probability that the total reward obtained by policy π under
Gaussian smoothing (no adversary) with variance σ2 is above a certain threshold ν, i.e., P ""
t
X"
REFERENCES,0.45507246376811594,"i=1
γi−1Ri ≥ν # ≥p."
REFERENCES,0.4579710144927536,"Let Hp be the class of all such environment-policy pairs that cross this reward threshold with proba-
bility at least p. We construct an environment-policy pair (M ′, π′) that achieves the reward threshold
ν with probability Φ(Φ−1(p) −B/σ) under the structured adversary ϵst. Note that, this does not
mean that ϵst is the strongest possible adversary for a general environment-policy pair. It only shows
that the performance of policy π′ in environment M ′ under the adversary ϵst is a lower-bound on the
performance of a general environment-policy pair under a general adversary. Consider a one-step
game with environment M ′ = (S, A, T ′, R′, γ) with a deterministic observation function o of the
state-space and a policy π′ such that π′ returns an action a1 ∈A if the ﬁrst coordinate of o(s1) + η1
is at most ω = {o(s1)}1 +σΦ−1(p) and another action a2 ∈A otherwise. Here {o(s1)}1 represents
the ﬁrst coordinate of o(s1). The environment offers a reward ν if the action in the ﬁrst step is a1 and
0 when it is a2. The game terminates immediately. The probability of the reward being above ν is
equal to the probability of the action being a1. When η1 is sampled from the Gaussian distribution,
this probability is equal to Φ((ω −{o(s1)}1)/σ) = p. Therefore, (M ′, π′) ∈Hp. Under the pres-
ence of the structured adversary ϵst deﬁned in Section 4.1, this probability after smoothing becomes
Φ((ω −{o(s1)}1 −B)/σ) = Φ(Φ−1(p) −B/σ), which is same as the bound in Theorem 1."
REFERENCES,0.4608695652173913,"B
STATIC VS. ADAPTIVE SETTING"
REFERENCES,0.463768115942029,"In this section, we illustrate the difference between the adversarial distributions in the static setting
and the adaptive setting. Naively, one might assume that smoothing-based robustness guarantees
can be applied directly to reinforcement learning, by adding noise to observations. For example, it
seems plausible to use Cohen et al.’s ℓ2 certiﬁcate Cohen et al. (2019), which relies on the overlap
in the distributions of isometric Gaussians with different means, by simply adding Gaussian noise
to each observation (Figure 1-a). However, as we demonstrate with a toy example in Figure 1-
(b-d), the Cohen et al. certiﬁcate cannot be applied directly to the RL setting, because adding
noise to sequential observations does not result in an isometric Gaussian distribution over the space
of observations. This is because the adversarial offset to later observations may be conditioned
on the noise added to previous observations. In 1-(b-d), we consider a two-step episode, and for
simplicity, we consider a case where the ground-truth observations at each step are ﬁxed. At step 1,
the noised distributions of the clean observation o1 and the adversarially-perturbed observation o′
1
are both Gaussians and overlap substantially, similar to in the standard classiﬁcation setting (panel
b). However, we see in panel (c) that the adversarial perturbation ϵ2 added to o2 can depend on the
smoothed value of o′
1. This is because the agent may leak information about the observation that
it receives after smoothing (o1 + η1) to the adversary, for example through its choice of actions.
After smoothing is performed on o2, the adaptive nature of the adversary causes the distribution of
smoothed observations to no longer be an isometric Gaussian in the adversarial case (panel d). The
standard certiﬁcation results therefore cannot be applied."
REFERENCES,0.4666666666666667,"C
NEYMAN–PEARSON LEMMA [1993] IN SMOOTHING"
REFERENCES,0.46956521739130436,"In the context of randomized smoothing, the Neyman–Pearson lemma produces the worst-case deci-
sion boundary of a classiﬁer based on the estimated probability of the top class under the smoothing
distribution. It says that this boundary is a region where the ratio of the probability density functions
of the smoothing distributions at the clean input and the perturbed input is a constant. When the two
distributions are isometric Gaussians, as is the case in static settings like image classiﬁcation, this
boundary takes the form of a hyper-plane (see Appendix A of Cohen et al. (2019)). However, in the
dynamic setting of RL, the smoothing distribution after adding the adversarial perturbation may not"
REFERENCES,0.47246376811594204,Published as a conference paper at ICLR 2022
REFERENCES,0.4753623188405797,"be isometric even if the smoothing noise at each time-step was sampled from an isometric Gaus-
sian distribution (see ﬁgure 1, section ‘Technical contributions’ and Appendix A). So, we formulate
and prove an adaptive version of the Neyman-Pearson lemma to obtain provable robustness in RL
through randomized smoothing."
REFERENCES,0.4782608695652174,"D
PROOF OF LEMMA 1"
REFERENCES,0.4811594202898551,"Statement: For any general adversary ϵ and an Γ ⊆(T × Rd)t, there exists a deterministic adver-
sary ϵdt such that,
P[Y dt ∈Γ] ≤P[Y ∈Γ],
where Y dt is the random variable for the distribution deﬁned by the adversary ϵdt."
REFERENCES,0.48405797101449277,"Proof. Consider a time-step j such that ∀i < j, ϵi is a deterministic function of τ1, η1, τ2,
η2, . . . , τi−1, ηi−1, τi. Let H = {z | z1 = τ1, z2 = η1, z3 = τ2, z4 = η2, . . . , z2j−1 = aj} be the
set of points whose ﬁrst 2j−1 coordinates are ﬁxed to an arbitrary set of values τ1, η1, τ2, η2, . . . , τj.
In the space deﬁned by H, ϵ1, . . . , ϵj−1 are ﬁxed vectors in Rd and ϵj is sampled from a ﬁxed dis-
tribution over the vectors with ℓ2-norm at most Bj
r. Let Y γ
H be the random variable representing the
distribution over points in H deﬁned by the adversary for which ϵj = γ, such that ∥γ∥2 ≤Bj
r. De-
ﬁne an adversary ϵ′, such that, ϵ′
i = ϵi, ∀i ̸= j. Set ϵ′
j to the vector γ that minimizes the probability
that Y γ
H assigns to Γ ∩H, i.e.,
ϵ′
j = arg min
∥γ∥2≤Bj
r
P[Y γ
H ∈Γ ∩H]"
REFERENCES,0.48695652173913045,"The adversary ϵ′ behaves as ϵ up to step j −1. At step j, it sets ϵ′
j to the γ that minimizes the
probability it assigns to Γ ∩H, based on the values τ1, η1, τ2, η2, . . . , τj. After that, it mimics ϵ till
the last time-step t. Therefore, for a given tuple (z1, z2, . . . , z2j−1) = (τ1, η1, τ2, η2, . . . , τj),"
REFERENCES,0.48985507246376814,"P[Y
ϵ′
j
H ∈Γ ∩H] ≤P[Y ∈Γ ∩H]
Since both adversaries are same up to step j −1, their respective distributions over z1, z2, . . . , z2j−1
remains same as well. Therefore, integrating both sides of the above inequality over the space of all
tuples (z1, z2, . . . , z2j−1), we have:
Z
P[Y
ϵ′
j
H ∈Γ ∩H]pY (z1, z2, . . . , z2j−1)dz1dz2 . . . dz2j−1"
REFERENCES,0.4927536231884058,"≤
Z
P[Y ∈Γ ∩H]pY (z1, z2, . . . , z2j−1)dz1dz2 . . . dz2j−1"
REFERENCES,0.4956521739130435,"=⇒P[Y ′ ∈Γ] ≤P[Y ∈Γ],
where Y ′ is the random variable corresponding to ϵ′. Thus, we have constructed an adversary where
the ﬁrst j adversarial perturbations are a deterministic function of the τis and ηis of the previous
rounds. Applying the above step sufﬁciently many times we can construct a deterministic adversary
ϵdt represented by the random variable Y dt such that"
REFERENCES,0.4985507246376812,P[Y dt ∈Γ] ≤P[Y ∈Γ].
REFERENCES,0.5014492753623189,"E
PROOF OF LEMMA 3"
REFERENCES,0.5043478260869565,"Lemma 3 states that the structured adversary characterises the worst-case scenario. Before proving
this lemma, let us ﬁrst show that any deterministic adversary can be converted to one that uses up the
entire budget of B without increasing the probability it assigns to h being one in the worst-case. For
each step i, let us deﬁne a used budget Bi
u = ∥(ϵ1, ϵ2, . . . , ϵi−1)∥2 as the norm of the perturbations
of the previous steps and a remaining budget Bi
r =
p"
REFERENCES,0.5072463768115942,"B2 −(Biu)2 as an upper-bound on the norm
of the perturbations of the remaining steps. Note that, B1
u = 0 and B1
r = B."
REFERENCES,0.5101449275362319,"Consider a version ˜ϵdt of the deterministic adversary that uses up the entire available budget B by
scaling up ϵdt
t
such that its norm is equal to Bt
r, i.e., setting it to ϵdt
t Bt
r/∥ϵdt
t ∥2. Let ˜Y dt be the
random variable representing ˜ϵdt."
REFERENCES,0.5130434782608696,Published as a conference paper at ICLR 2022
REFERENCES,0.5159420289855072,"Lemma. If Γ ˜Y dt = {z ∈(T × Rd)t | µ ˜Y dt(z) ≤qµX(z)} for some q ≥0 and P[h(X) = 1] ≥
P[X ∈Γ ˜Y dt], then P[h(Y dt) = 1] ≥P[ ˜Y dt ∈Γ ˜Y dt]."
REFERENCES,0.518840579710145,"Proof. Consider ΓY dt = {z ∈(T × Rd)t | µY dt(z) ≤q′µX(z)} for some q′ ≥0, such that,
P[X ∈ΓY dt] = p for some lower-bound p on P[h(X) = 1]. Then, by the Neyman-Pearson Lemma
we have that,
P[h(Y dt) = 1] ≥P[Y dt ∈ΓY dt].
Now consider a space H in (T × Rd)t where all but the last element of the tuple z are ﬁxed, i.e.,
H = {z | z1 = τ1, z2 = η1, z3 = τ2, z4 = η2, . . . , z2t−1 = τt} Since, ϵdt is a deterministic
adversary where each ϵdt
i
is a deterministic function of the previous τis and ηis, each ϵdt
i
is also
ﬁxed in H. Therefore, in H, both µX and µY dt are two isometric Gaussians in the space of the ηis
and the set H ∩ΓY dt is a hyperplane. The probability assigned by Y dt to H ∩ΓY dt is proportional
to the distance of the center of the corresponding Gaussian. In the construction of ˜ϵdt, this distance
can only increase, therefore,"
REFERENCES,0.5217391304347826,P[Y dt ∈ΓY dt] ≥P[ ˜Y dt ∈ΓY dt]
REFERENCES,0.5246376811594203,"Now, consider a function hΓY dt(z) which outputs one if z ∈ΓY dt and zero otherwise. Construct
the set Γ ˜Y dt = {z ∈(T × Rd)t | µ ˜Y dt(z) ≤qµX(z)} for some q ≥0 such that,"
REFERENCES,0.527536231884058,P[X ∈Γ ˜Y dt] = p = P[hΓY dt(X) = 1].
REFERENCES,0.5304347826086957,"Then, by the Neyman-Pearson Lemma, we have,"
REFERENCES,0.5333333333333333,P[hΓY dt( ˜Y dt) = 1] ≥P[ ˜Y dt ∈Γ ˜Y dt]
REFERENCES,0.5362318840579711,"or,
P[ ˜Y dt ∈ΓY dt] ≥P[ ˜Y dt ∈Γ ˜Y dt]
(from deﬁnition of hΓY dt)"
REFERENCES,0.5391304347826087,"or,
P[Y dt ∈ΓY dt] ≥P[ ˜Y dt ∈Γ ˜Y dt]"
REFERENCES,0.5420289855072464,"or,
P[h(Y dt) = 1] ≥P[ ˜Y dt ∈Γ ˜Y dt],
(from the above two inequalities)"
REFERENCES,0.5449275362318841,proving the statement of the lemma.
REFERENCES,0.5478260869565217,"Now, we prove lemma 3 below:
Statement: If ΓY st = {z ∈(T × Rd)t | µY st(z) ≤qµX(z)} for some q ≥0 and P[h(X) = 1] ≥
P[X ∈ΓY st], then P[h(Y dt) = 1] ≥P[Y st ∈ΓY st]."
REFERENCES,0.5507246376811594,"Proof. Construct the set Γ ˜Y dt as deﬁned in the above lemma for a q ≥0 such that P[X ∈Γ ˜Y dt] = p,
for some lower-bound p on P[h(X) = 1]. Then,"
REFERENCES,0.553623188405797,P[h(Y dt) = 1] ≥P[ ˜Y dt ∈Γ ˜Y dt]
REFERENCES,0.5565217391304348,"Now consider the structured adversary ϵst in which ϵst
1 = (B, 0, . . . , 0) and ϵst
i = (0, 0, . . . , 0) for
i > 1. Deﬁne the set ΓY st = {z ∈(T × Rd)t | µY st(z) ≤qµX(z)} for the same q as above. Then,
we can show that:"
REFERENCES,0.5594202898550724,"1. P[ ˜Y dt ∈Γ ˜Y dt] = P[Y st ∈ΓY st], and"
REFERENCES,0.5623188405797102,2. P[X ∈Γ ˜Y dt] = P[X ∈ΓY st]
REFERENCES,0.5652173913043478,"which, in turn, prove the statement of the lemma."
REFERENCES,0.5681159420289855,"Let N and Nϵi represent Gaussian distributions centered at origin and ϵi respectively. Then, we can
write µX and µY as below:"
REFERENCES,0.5710144927536231,µX(z) = tY
REFERENCES,0.5739130434782609,"i=1
µTi(τi | τ1, η1, τ2, η2, . . . , τi−1, ηi−1)µN (ηi)"
REFERENCES,0.5768115942028985,µ ˜Y dt(z) = tY
REFERENCES,0.5797101449275363,"i=1
µTi(τi | τ1, η1, τ2, η2, . . . , τi−1, ηi−1)µN˜ϵdt
i (ηi)"
REFERENCES,0.5826086956521739,Published as a conference paper at ICLR 2022
REFERENCES,0.5855072463768116,"where µTi is the conditional probability distribution of token τi given the previous tokens and offsets.
Therefore,"
REFERENCES,0.5884057971014492,µ ˜Y dt(z)
REFERENCES,0.591304347826087,µX(z) = tY i=1
REFERENCES,0.5942028985507246,"µN˜ϵdt
i (ηi)"
REFERENCES,0.5971014492753624,"µN (ηi)
= tY i=1
e"
REFERENCES,0.6,"ηT
i ηi−(ηi−˜ϵdt
i
)T (ηi−˜ϵdt
i
) 2σ2"
REFERENCES,0.6028985507246377,µ ˜Y dt(z)
REFERENCES,0.6057971014492753,"µX(z) ≤q ⇐⇒ t
X"
REFERENCES,0.6086956521739131,"i=1
2ηT
i ˜ϵdt
i −(˜ϵdt
i )T ˜ϵdt
i ≤2σ2 ln q"
REFERENCES,0.6115942028985507,"Consider a round j ≤t such that ˜ϵdt
i
= 0, ∀i > j + 1 and ˜ϵdt
j+1 = (Bj+1
r
, 0, . . . , 0). We can
always ﬁnd such a j as we always have ˜ϵdt
t+1 = (Bt+1
r
, 0, . . . , 0), since Bt+1
r
= 0. Note that,"
REFERENCES,0.6144927536231884,"Bj+1
r
= r"
REFERENCES,0.6173913043478261,"B2 −

Bj+1
u
2
and in turn ˜ϵdt
j+1 are functions of τ1, η1, τ2, η2, . . . , τj and not τj+1."
REFERENCES,0.6202898550724638,"Let H = {z | z1 = τ1, z2 = η1, z3 = τ2, z4 = η2, . . . , z2j−1 = τj} be the set of points whose
ﬁrst 2j −1 coordinates are ﬁxed to an arbitrary set of values τ1, η1, τ2, η2, . . . , τj. For points in
H, all ˜ϵdt
i
for i ≤j + 1 are ﬁxed and for i > j + 1 are set to zero. Let ˜Y dt
H denote the random
variable representing the distribution of points in H deﬁned by the adversary ˜ϵdt (corresponding
random variable ˜Y dt). In the space of ηj, ηj+1, . . . , ηt, this is an isometric Gaussian centered at
(˜ϵdt
j , ˜ϵdt
j+1, 0, . . . , 0). Therefore, Γ ∩H is given by j+1
X"
REFERENCES,0.6231884057971014,"i=1
2ηT
i ˜ϵdt
i −(˜ϵdt
i )T ˜ϵdt
i ≤2σ2 ln t"
REFERENCES,0.6260869565217392,"or,
ηT
j ˜ϵdt
j + ηT
j+1˜ϵdt
j+1 ≤β,
(3)"
REFERENCES,0.6289855072463768,"for some constant β dependent on η1, ˜ϵdt
1 , . . . , ηj−1, ˜ϵdt
j−1, σ and t. The probability assigned by the
Gaussian random variable YH to the half-space deﬁned by (3) is proportional to the distance of the
center of the Gaussian from the hyper-plane in (3), which is equal to:"
REFERENCES,0.6318840579710145,"∥˜ϵdt
j ∥2 + ∥˜ϵdt
j+1∥2 −β
q"
REFERENCES,0.6347826086956522,"∥˜ϵdt
j ∥2 + ∥˜ϵdt
j+1∥2
= (Bj
r)2 −β"
REFERENCES,0.6376811594202898,"Bj
r
,"
REFERENCES,0.6405797101449275,where the equality follows from:
REFERENCES,0.6434782608695652,"∥˜ϵdt
j ∥2 + ∥˜ϵdt
j+1∥2 = ∥˜ϵdt
j ∥2 + (Bj+1
r
)2"
REFERENCES,0.6463768115942029,"= ∥˜ϵdt
j ∥2 + B2 −(Bj+1
u
)2
(from deﬁnition of Bi
r)"
REFERENCES,0.6492753623188405,"= ∥˜ϵdt
j ∥2 + B2 −(∥˜ϵdt
1 ∥2 + ∥˜ϵdt
2 ∥2 + . . . + ∥˜ϵdt
j ∥2)"
REFERENCES,0.6521739130434783,"= B2 −(∥˜ϵdt
1 ∥2 + ∥˜ϵdt
2 ∥2 + . . . + ∥˜ϵdt
j−1∥2)"
REFERENCES,0.6550724637681159,"= B2 −(Bj
u)2 = (Bj
r)2."
REFERENCES,0.6579710144927536,"Now, consider an adversary ˜ϵdt′ such that ˜ϵdt′
i
= ˜ϵdt
i , ∀i ≤j −1, ˜ϵdt′
j
= (Bj
r, 0, . . . , 0), and
˜ϵdt
i
= 0, ∀i > j. Let ˜Y dt′ be the corresponding random variable. Deﬁne Γ ˜Y dt′ similar to Γ ˜Y dt.
Then, Γ ˜Y dt′ ∩H is given by"
REFERENCES,0.6608695652173913,"ηT
j (Bj
r, 0, . . . , 0) ≤β,
(4)"
REFERENCES,0.663768115942029,"which is obtained by replacing ˜ϵdt
j with (Bj
r, 0, . . . , 0) and ˜ϵdt
j+1 with (0, 0, . . . , 0) in inequality (3)
about the origin. Deﬁne ˜Y dt′
H
similar to ˜Y dt
H , and just like ˜Y dt
H , the distribution of ˜Y dt′
H
is also an
isometric Gaussian, but is centered at ((Bj
r, 0, . . . , 0), (0, 0, . . . , 0)). The probability assigned by
this Gaussian distribution to Γ ˜Y dt′ ∩H is proportional to the distance of its center to the hyper-plane
deﬁning the region in (4), which is equal to ((Bj
r)2 −β)/Bj
r. Therefore,"
REFERENCES,0.6666666666666666,"P[ ˜Y dt
H ∈Γ ˜Y dt ∩H] = P[ ˜Y dt′
H
∈Γ ˜Y dt′ ∩H]."
REFERENCES,0.6695652173913044,"The key intuition behind this step is that, for isometric Gaussian smoothing distribution, the worst-
case probability assigned by the adversarial distribution only depends on the magnitude of the per-
turbation and not its direction. Figure 6 illustrates this property for a two-dimensional input space."
REFERENCES,0.672463768115942,Published as a conference paper at ICLR 2022
REFERENCES,0.6753623188405797,"Figure 6: General adversarial perturbation vs. perturbation aligned along the ﬁrst dimension. Blue
and red regions denote where the worst-case function is one and zero respectively."
REFERENCES,0.6782608695652174,"Since both adversaries are same up to step j −1, their respective distributions over z1, z2, . . . , z2j−1
remains same as well, i.e., p ˜Y dt(z1, z2, . . . , z2j−1) = p ˜Y dt′(z1, z2, . . . , z2j−1). Integrating over the
space of all tuples (z1, z2, . . . , z2j−1), we have:
Z
P[ ˜Y dt
H ∈Γ ˜Y dt ∩H]p ˜Y dt(z1, z2, . . . , z2j−1)dz1dz2 . . . dz2j−1"
REFERENCES,0.6811594202898551,"=
Z
P[ ˜Y dt′
H
∈Γ ˜Y dt′ ∩H]p ˜Y dt′(z1, z2, . . . , z2j−1)dz1dz2 . . . dz2j−1"
REFERENCES,0.6840579710144927,"=⇒P[ ˜Y dt
H ∈Γ ˜Y dt] = P[ ˜Y dt′
H
∈Γ ˜Y dt′],"
REFERENCES,0.6869565217391305,"Since the distribution deﬁned by X (with no adversary) over the space of ηis is a Gaussian centered
at origin whose distance to both Γ ˜Y dt ∩H and Γ ˜Y dt′ ∩H is the same (equal to −β/Bj
r), it assigns
the same probability to both (3) and (4). Therefore,"
REFERENCES,0.6898550724637681,P[X ∈Γ ˜Y dt] = P[X ∈Γ ˜Y dt′].
REFERENCES,0.6927536231884058,"Thus, we have constructed an adversary with one less non-zero ϵi. Applying, this step sufﬁciently
many times we can obtain the adversary ϵst such that,"
REFERENCES,0.6956521739130435,"P[ ˜Y dt ∈Γ ˜Y dt] = P[Y st ∈ΓY st]
and
P[X ∈Γ ˜Y dt] = P[X ∈ΓY st]"
REFERENCES,0.6985507246376812,which completes the proof.
REFERENCES,0.7014492753623188,"F
PROOF OF THEOREM 1"
REFERENCES,0.7043478260869566,"Statement: For an isometric Gaussian smoothing noise with variance σ2, if P[h(X) = 1] ≥p,
then:
P[h(Y ) = 1] ≥Φ(Φ−1(p) −B/σ)."
REFERENCES,0.7072463768115942,"Proof. Deﬁne ΓY = {z ∈(T × Rd)t | µY (z) ≤qµX(z)} for an appropriate q such that P[X ∈
ΓY ] = p. Then, by the Neyman-Pearson lemma, we have P[h(Y ) = 1] ≥P[Y ∈ΓY ]. Applying
lemma 1, we know that there exists a deterministic adversary ϵdt represented by random variable
Y dt, such that,
P[h(Y ) = 1] ≥P[Y ∈ΓY ] ≥P[Y dt ∈ΓY ].
(5)"
REFERENCES,0.7101449275362319,"Now deﬁne a function hΓY (z) = 1{z∈ΓY } and a set ΓY dt = {z ∈(T ×Rd)t | µY dt(z) ≤q′µX(z)}
for an appropriate q′ > 0, such that, P[X ∈ΓY dt] = P[hΓY (X) = 1] = p. Applying the Neyman-"
REFERENCES,0.7130434782608696,Published as a conference paper at ICLR 2022
REFERENCES,0.7159420289855073,"Pearson lemma again, we have:
P[hΓY (Y dt) = 1] ≥P[Y dt ∈ΓY dt]"
REFERENCES,0.7188405797101449,"or,
P[Y dt ∈ΓY ] ≥P[Y dt ∈ΓY dt]
(from deﬁnition of hΓY )"
REFERENCES,0.7217391304347827,"or,
P[h(Y ) = 1] ≥P[Y dt ∈ΓY dt]
(from inequality (5))
Deﬁne hΓY dt(z) = 1{z∈ΓY dt}. For the structured adversary ϵst represented by Y st, deﬁne ΓY st =
{z ∈(T × Rd)t | µY st(z) ≤q′′µX(z)} for an appropriate q′′ > 0, such that, P[X ∈ΓY st] =
P[hΓY dt(X) = 1] = p. Applying lemma 3, we have:"
REFERENCES,0.7246376811594203,P[hΓY dt(Y dt) = 1] ≥P[Y st ∈ΓY st]
REFERENCES,0.7275362318840579,"P[Y dt ∈ΓY dt] ≥P[Y st ∈ΓY st]
(from deﬁnition of hΓY dt)"
REFERENCES,0.7304347826086957,"P[h(Y ) = 1] ≥P[Y st ∈ΓY st]
(since P[h(Y ) = 1] ≥P[Y dt ∈ΓY dt])
ΓY st is deﬁned as the set of points z which satisfy:"
REFERENCES,0.7333333333333333,µY st(z)
REFERENCES,0.736231884057971,"µX(z) ≤q′′
or,
µN˜ϵst
1 (η1)"
REFERENCES,0.7391304347826086,"µN (η1)
≤q′′"
REFERENCES,0.7420289855072464,"ηT
1 (B, 0, . . . , 0) ≤β
or,
{η1}1 ≤β/B
for some constant β. This is the set of all tuples z where the ﬁrst coordinate of η1 is below a certain
threshold γ. Since P[X ∈ΓY st] = p,
Φ(γ/σ) = p =⇒γ = σΦ−1(p).
Therefore,"
REFERENCES,0.744927536231884,"P[Y st ∈ΓY st] = Φ
γ −B σ"
REFERENCES,0.7478260869565218,"
= Φ(Φ−1(p) −B/σ)."
REFERENCES,0.7507246376811594,"G
ADDITIONAL CARTPOLE RESULTS"
REFERENCES,0.7536231884057971,"We performed two additional experiment on Cartpole: we tested at larger noise levels, (σ = 0.6 and
0.8) and we tested a variant of the agent architecture. Speciﬁcally, in addition to the agent shown
in the main text, which uses ﬁve frames of observation, we also tested an agent which uses only a
single frame. Unlike the Atari environment, the task is in fact solvable (in the non-adversarial case)
using only one frame: the observation vector represents the complete system state. We computed
certiﬁcates for the policy-smoothed version of this model, and tested attacks on the undefended
version. (We did not test attacks on the smoothed single-frame variant). As we see in Figure 7, we
achieve non-vacuous certiﬁcates in both settings (i.e, at large perturbation sizes, the smoothed agent
is guaranteed to be more robust than the empirical robustness of a non-smoothed agent). However,
observe that the undefended agent in the multi-frame setting is much more vulnerable to adversarial
attack. This is likely because the increased number of total features (20 vs. four) introduces more
complexity of the Q-network, making it more vulnerable against adversarial attack."
REFERENCES,0.7565217391304347,"H
FULL PONG GAME"
REFERENCES,0.7594202898550725,"In Figure 8, we explore a failure case of our technique: we fail to produce non-vacuous certiﬁcates
for a full Pong game, where the game ends after either player scores 21 goals. In particular, while,
for the one-round Pong game, the smoothed agent is provably more robust than the empirical perfor-
mance of the undefended agent, this is clearly not the case for the full game. To understand why our
certiﬁcate is vacuous here, note that in the the “worst-case” environment that our certiﬁcate assumes,
any perturbation will (maximally) affect all future rewards. However, in the multi-round Pong game,
each round of the game is only loosely coupled to the previous rounds (the ball momentum – but
not position – as well as the paddle positions are retained). Therefore, any perturbation can only
have a very limited effect on the total reward. Another way to think about this is to recall that in
smoothing-based certiﬁcates, the noise added to each feature is proportional to the total perturbation
budget of the adversary. In this sort of serial game, the perturbation budget required to attack the
average reward scales with the (square root of the) number of rounds, but the noise tolerance of the
agent does not similarly scale."
REFERENCES,0.7623188405797101,Published as a conference paper at ICLR 2022
REFERENCES,0.7652173913043478,"Figure 7: Additional Cartpole results. Attacks on smoothed agents at all σ for the multiframe agents
are presented in Appendix J"
REFERENCES,0.7681159420289855,"Figure 8: Results for the Full Pong game, compared to the single-round game."
REFERENCES,0.7710144927536232,"I
TRAINING AND CLEAN TEST RESULTS"
REFERENCES,0.7739130434782608,"In Figure 9, we present the clean (non-attacked) test performance for the experiments presented in
the main text, as a function of the smoothing noise σ."
REFERENCES,0.7768115942028986,"In Figure 10, we present the clean training (i.e., validation round) performance as a function of the
training time step and the smoothing noise σ. Note that early stopping was applied: the model from
the best validation round was kept, and only replaced if a strictly better validation performance was
recorded later."
REFERENCES,0.7797101449275362,"• For Cartpole: logs were not kept after the ﬁrst time an evaluation round had a perfect
average score of 200 (this is because the “best model” was saved for this evaluation, and it
would be impossible to beat this score, so training was not continued). However, for other
tasks (i.e. mountain car) logs continued after a perfect evaluation round."
REFERENCES,0.782608695652174,"• For Freeway: as mentioned in Appendix Section L, we trained 5 times at each noise level,
and kept the best of all 5 models. All 5 training curves are shown here for each noise level."
REFERENCES,0.7855072463768116,"J
COMPLETE ATTACK RESULTS"
REFERENCES,0.7884057971014493,"In Figures 11 and 12, we report the empirical robustness under attack for all tested values of λQ:
in the main text, we show only the result for the λQ that represents the strongest attack. Figure 12
also shows the attacks on smoothed agents for all smoothing noises. All attack results are means
over 1000 episodes (except for Mountain Car results, where 250 episodes were used) and error bars
represent the standard error of the mean."
REFERENCES,0.7913043478260869,Published as a conference paper at ICLR 2022
REFERENCES,0.7942028985507247,Figure 9: Clean test performance as a function of smoothing noise σ.
REFERENCES,0.7971014492753623,Figure 10: Clean training performance as a function of smoothing noise σ and training step.
REFERENCES,0.8,Published as a conference paper at ICLR 2022
REFERENCES,0.8028985507246377,"Figure 11: Empirical robustness of undefended agents on for all tested values of λQ (or λ). The
results in the main text are the pointwise minima over λ of these curves."
REFERENCES,0.8057971014492754,Published as a conference paper at ICLR 2022
REFERENCES,0.808695652173913,"Figure 12: Empirical robustness of smoothed agents on for all tested values of σ and λQ (or λ). We
also plot the associated certiﬁcate curves."
REFERENCES,0.8115942028985508,"K
EMPIRICAL ATTACK DETAILS"
REFERENCES,0.8144927536231884,"Our empirical attack on (undefended) RL observations for DQN is described in Algorithm 1. To
summarize, the core of the attack is a standard targeted L2 PGD attack on the Q-value function.
However, because we wish to “save” our total perturbation budget B for use in later steps, some
modiﬁcations are made. First, we only target actions a for which the clean-observation Q-value is
sufﬁciently below (by a gap given by the parameter λQ) the Q-value of the ‘best’ action, which would
be taken in the absence of adversarial attack. Among these possible Targets, we ultimately choose
whichever action will maximally decrease the Q-value, and which the agent can be successfully
be induced to choose within the adversarial budget B. If no such action exists, then the original
observation will be returned, and the entire budget will be saved. In order to preserve budget, the
PGD optimization is stopped as soon as the “decision boundary” is crossed."
REFERENCES,0.8173913043478261,"We use a constant step size η. In order to deal with the variable budget B, we optimize of a number
of iterations which is a constant multiple ν of B η ."
REFERENCES,0.8202898550724638,"For most environments, there is some context used by the Q-value function (i.e, the previous frames)
which is carried over from previous steps, but is not directly being attacked in this round. We need
both the clean version of the context, C, in order to evaluate the “ground-truth” values of the Q-
value function under various actions; as well as the “dirty” version of the context, C′, based on
the adversarial observations which have already been fed to the agent, in order to run the attack
optimization."
REFERENCES,0.8231884057971014,"Our attack for DDPG is described in Algorithm 2. Here, we use the policy π to determine what
action a the agent will take when it observes a corrupted observation o′ (with corrupted context
C′), and use the Q-value function supplied by the DDPG algorithm to determine the “value” of that
action on the ground-truth observation o. Because our goal is to minimize this value, this amounts
to minimizing Q(C; o, π(C′; o′)). In order to ensure that a large amount of L2 “budget” is only used
when the Q value can be substantially minimized, we include a regularization term λ∥o −o′∥2
2."
REFERENCES,0.8260869565217391,Attacks on smoothed agents are described in Appendix M.
REFERENCES,0.8289855072463768,Published as a conference paper at ICLR 2022
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8318840579710145,"1-Round
Full
Multiframe
Single-frame
Pong
Pong
Cartpole
Cartpole
Freeway
Training discount factor γ
0.99
0.99
0.99
0.99
0.99
Total timesteps
10000000
10000000
500000
500000
10000000
Validation interval (steps)
100000
100000
2000
2000
100000
Validation episodes
100
10
10
10
100
Learning Rate
0.0001
0.0001
0.0001
0.00005
0.0001
DQN Buffer Size
10000
10000
100000
100000
10000
DQN steps collected
100000
100000
1000
1000
100000
before learning
Fraction of steps
0.1
0.1
0.16
0.16
0.1
for exploration (linearly"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8347826086956521,"decreasing exp. rate)
Initial exploration rate
1
1
1
1
1
Final exploration rate
0.01
0.01
0
0
0.01
DQN target update
1000
1000
10
10
1000
interval (steps)"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8376811594202899,"Batch size
32
32
1024
1024
32
Training interval (steps)
4
4
256
256
4
Gradient descent steps
1
1
128
128
1
Frames Used
4
4
5
1
4
Training Repeats
1
1
1
1
5
Architecture
CNN*
CNN*
MLP
MLP
CNN*
20×
4×
256×
256×
256×
256×
2
2"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8405797101449275,"Table 1: Training Hyperparameters for DQN models. *CNN refers to the 3-layer convolutional
network deﬁned by the CNNPolicy class in stable-baselines3 (Rafﬁn et al., 2019), based on the
CNN architecture used for Atari games by Mnih et al. (2015). Note that hyperparameters for Atari
games are based on hyperparameters from the stable-baselines3 Zoo package (Rafﬁn, 2020), for a
slightly different (more deterministic) variant of the Pong environment."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8434782608695652,"Note that on image data (i.e., Pong), we do not consider integrality constraints on the observations;
however, we do incorporate box constraints on the pixel values. We also incorporate box constraints
on the kinematic quantities when attacking Mountain Car, but not when attacking Cartpole: the
distinction is that the constraints in Mountain Car represent artiﬁcial constraints on the kinematics
[i.e., the velocity of the car is arbitrarily clipped], while the constraints in Cartpole arise naturally
from the problem setup."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8463768115942029,"L
ENVIRONMENT DETAILS AND HYPERPARAMETERS"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8492753623188406,"For Atari games, we use the “NoFrameskip-v0” variations of these environments with the standard
“AtariPreprocessing” wrapper from the OpenAI Gym (Brockman et al., 2016) package: this provides
This environment also injects non-determinism into the originally-deterministic Atari games, by
adding randomized “stickiness” to the agent’s choice of actions – without this, the state-observation
robustness problem could be trivially solved by memorizing a winning sequence of actions, and
ignoring all observations at test-time."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8521739130434782,"Due to instability in training, for the freeway environment, we trained each model ﬁve times, and
selected the base model based on the performance of validation runs. See training hyperparameters,
Tables 1 and 2. For attack hyperparameters, see Table 3 and 4."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.855072463768116,Published as a conference paper at ICLR 2022
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8579710144927536,"Mountain Car
Training discount factor γ
0.99
Total timesteps
300000
Validation interval (steps)
2000
Validation episodes
10
Learning Rate
0.0001
DDPG Buffer Size
1000000
DDPG steps collected
100
before learning"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8608695652173913,"Batch size
100
Update coefﬁcient τ
0.005
Train frequency
1 per episode
Gradient steps
= episode length
Training action noise
Ornstein Uhlenbeck (σ = 0.5)
Architecture
MLP 2 × 400 × 300 × 1"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.863768115942029,"Table 2: Training Hyperparameters for DDPG models. Hyperparameters are based on hyperpa-
rameters from the stable-baselines3 Zoo package (Rafﬁn, 2020), for the unmodiﬁed Mountain Car
environment."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8666666666666667,"1-Round
Full
Multiframe
Single-frame
Pong
Pong
Cartpole
Cartpole
Freeway
Attack step size η
0.01
0.01
0.01
0.01
0.01
Attack step multiplier ν
2
2
2
2
2
Q-value thresholds λQ searched
.1, .3, .5
.1, .3, .5,
4,6,8,10
0, .05,
0, .06, .12
.9, 1.3, 1.7
.1, 1"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8695652173913043,Table 3: Attack Hyperparameters for DQN models.
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8724637681159421,"Mountain Car
Attack step size η
0.01
Attack steps τ
100
Regularization values λ searched
.001, .0001, .00001"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8753623188405797,Table 4: Attack Hyperparameters for DDPG models.
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8782608695652174,Published as a conference paper at ICLR 2022
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.881159420289855,"Algorithm 1: Empirical Attack on DQN Agents
Input: Q-value function Q, clean prior observation context C, adversarial prior observation
context C′, observation o, budget B, Q-value threshold λQ, step size η, step multiplier ν
Output: Attacked observation oworst, remaining budget B′.
Qclean := maxa∈A Q(C; o, a)
Targets := {a ∈A|Q(C; o, a) ≤Qclean −λQ}
Qworst := Qclean
oworst := o
for a ∈Targets do"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8840579710144928,"o′ := o
inner:
for i in 1, ..., ⌊νB"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8869565217391304,"η ⌋do
if arg maxa′ Q(C′; o′, a′) = a then"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8898550724637682,"if Q(C; o, a) < Qworst then"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8927536231884058,"oworst := o′
Qworst := Q(C; o, a)
end
break inner
end
D := ∇o′ log([SoftMax(Q(C′; o′, ·)]a)
o′ := o′ +
ηD
∥D∥2
if ∥o′ −o∥2 > B then"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8956521739130435,"o′ := o +
B
∥o′−o∥2 (o′ −o)
end
end
end
return oworst,
p"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.8985507246376812,"B2 −∥oworst −o∥2
2"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9014492753623189,"Algorithm 2: Empirical Attack on DDPG Agents
Input: Q-value function Q, policy π, clean prior observation context C, adversarial prior
observation context C′, observation o, budget B, weight parameter λ, step size η, step
count τ
Output: Attacked observation oworst, remaining budget B′.
o′ := o
for i in 1, ..., τ do"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9043478260869565,"D := ∇o′[Q(C; o, π(C′; o′)) + λ∥o′ −o∥2
2]
if ∥D∥2"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9072463768115943,∥o′∥2 ≤0.001 then
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9101449275362319,"break
end
o′ := o′ +
ηD
∥D∥2
if ∥o′ −o∥2 > B then"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9130434782608695,"o′ := o +
B
∥o′−o∥2 (o′ −o)
end
end
return o′,
p"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9159420289855073,"B2 −∥o′ −o∥2
2"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9188405797101449,Published as a conference paper at ICLR 2022
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9217391304347826,"M
ATTACKS ON SMOOTHED AGENTS"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9246376811594202,"In order to attack smoothed agents, we adapted Algorithms 1 and 2 using techniques suggested by
Salman et al. (2019) for attacking smoothed classiﬁers. In particular, whenever the Q-value function
is evaluated or differentiated, we instead evaluate/differentiate the mean output under m = 128
smoothing perturbations. Following Salman et al. (2019), we use the same noise perturbation vectors
at each step during the attack. In the multi-frame case, for the “dirty” context C′, we include the
actually-realized smoothing perturbations used by the agents for previous steps. However, when
determining the “clean” Q-values Q(C; o, a), for the “clean” context C, we use the unperturbed
previous state observations: we then take the average over m smoothing perturbations of both C
and o to determine the clean Q-values. This gives an unbiased estimate for the Q-values of an
undisturbed smoothed agent in this state."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.927536231884058,"When attacking DDPG, in evaluating Q(C; o, π(C′; o′), we average over smoothing perturbations
for both o and o′, in addition to C: this is because both π and Q are trained on noisy samples. Note
that we use independently-sampled noise perturbations on o′ and o."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9304347826086956,"Our attack does not appear to be successful, compared with the lower bound given by our certiﬁcate
(Figures 12). One contributing factor may be that attacking a smoothed agent is more difﬁcult
that attacking a smoothed classiﬁer, for the following reason: a smoothed classiﬁer evaluates the
expected output at test time, while a smoothed agent does not. Thus, while the average Q-value
for the targeted action might be greater than the average Q-value for the clean action, the actual
realization will depend on the speciﬁc realization of the random smoothing vector that the agent
actually uses."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9333333333333333,"N
RUNTIMES AND COMPUTATIONAL ENVIRONMENT"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.936231884057971,"Each experiment is run on an NVIDIA 2080 Ti GPU. Typical training times are shown in Table 5.
Typical clean evaluation times are shown in Table 6. Typical attack times are shown in Table 7."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9391304347826087,"Experiment
Time (hours)
Pong (1-round)
11.1
Pong (Full)
12.0
Cartpole (Multi-frame)
0.27
Cartpole (Single-frame)
0.32
Freeway
14.2
Mountain Car
0.63"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9420289855072463,Table 5: Training times
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9449275362318841,"Experiment
Time (seconds):
Time (seconds):
Experiment
smallest noise σ
largest noise σ
Pong (1-round)
0.46
0.38
Pong (Full)
3.82
4.65
Cartpole (Multi-frame)
0.20
0.13
Cartpole (Single-frame)
0.18
0.12
Freeway
1.36
1.35
Mountain Car
0.67
0.91"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9478260869565217,"Table 6: Evaluation times. Note that the times reported here are per episode: in order to statistically
bound the mean rewards, we performed 10,000 such episode evaluations for each environment."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9507246376811594,"O
CDF SMOOTHING DETAILS"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9536231884057971,"Due to the very general form of our certiﬁcation result (h(·), as a 0/1 function, can represent any
outcome, and we can bound the lower-bound the probability of this outcome), there are a variety of"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9565217391304348,Published as a conference paper at ICLR 2022
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9594202898550724,"Experiment
Time (seconds):
Time (seconds):
Experiment
smallest budget B
largest budget B
Pong (1-round)
1.01
0.68
Pong (Full)
8.84
10.2
Cartpole (Multi-frame)
0.35
0.32
Cartpole (Single-frame)
0.79
0.56
Freeway
2.67
2.80
Mountain Car
44.0
19.6"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9623188405797102,"Table 7: Attack times. Note that the times reported here are per episode: in the paper, we report the
mean of 1000 such episodes."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9652173913043478,"Figure 13: Comparison of certiﬁed bounds on the total reward in Cartpole, using (a) point estimation,
and (b) the DKW inequality to generate empirical bounds."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9681159420289855,"ways we can use the basic result to compute a certiﬁcate for an entire episode entire game. In the
main text, we introduce CDF smoothing Kumar et al. (2020) as one such option. In CDF smoothing
for any threshold value x, we can deﬁne hx(·) as an indicator function for the event that the total
episode reward is greater than x. Then, by the deﬁnition of the CDF function, the expectation of
hx(·) is equal to 1 −F(x), where F(·) is the CDF function of the reward. Then our lower-bound on
the expectation of hx(·) under adversarial attack is in fact an upper-bound on F(x): combining this
with Equation 2 in the main text,"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9710144927536232,"E[X] =
Z ∞"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9739130434782609,"0
(1 −F(x))dx −
Z 0"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9768115942028985,"−∞
F(x)dx,"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9797101449275363,provides a lower bound on the total expectation of the reward under adversarial perturbation.
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9826086956521739,"However, in order to perform this integral from empirical samples, we must bound F(x) at all
points: this requires ﬁrst upper-bounding the non-adversarial CDF function at all x, before apply-
ing our certiﬁcate result. Following Kumar et al. (2020), we accomplish this using the Dvoret-
zky–Kiefer–Wolfowitz inequality (for the Full Pong environment.)"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9855072463768116,"In the case of the Cartpole environment, we explore a different strategy: note that the reward at
each timestep is itself a 0/1 function, so we can deﬁne ht(·) as simply the reward at timestep t.
We can then apply our certiﬁcate result at each timestep independently, and take a sum. Note that
this requires estimating the average reward at each step independently: we use the Clopper-Pearson
method (following Cohen et al. (2019)), and in order to certify in total to the desired 95% conﬁdence
bound, we certify each estimate to (100 - 5/T)% conﬁdence, where T is the total number of timesteps
per episode (= 200)."
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9884057971014493,"However, note that, in the particular case of the cartpole environment, ht(·) = 1 if and only if
we have “survived” to time-step t: in other words, ht(·) is simply an indicator function for the
total reward being ≥t. Therefore in this case, this independent estimation method is equivalent
to CDF smoothing, just using Clopper-Pearson point-estimates of the CDF function rather than
the Dvoretzky–Kiefer–Wolfowitz inequality. In practice, we ﬁnd that this produced slightly better
certiﬁcates for this task. (Figure 13)"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.991304347826087,Published as a conference paper at ICLR 2022
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9942028985507246,"P
ENVIRONMENT LICENSES"
"-ROUND
FULL
MULTIFRAME
SINGLE-FRAME
PONG
PONG
CARTPOLE
CARTPOLE
FREEWAY",0.9971014492753624,"OpenAI Gym Brockman et al. (2016) is Copyright 2016 by OpenAI and provided under the MIT
License. The stable-baselines3 packageRafﬁn et al. (2019) is Copyright 2019 by Antonin Rafﬁn and
also provided under the MIT License."
