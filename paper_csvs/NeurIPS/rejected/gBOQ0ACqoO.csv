Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010559662090813093,"State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion.
1"
ABSTRACT,0.0021119324181626186,"However, they neglect the factor of depth while designing the fusion strategy. In
2"
ABSTRACT,0.0031678986272439284,"this work, we for the first time point out that different modalities play different roles
3"
ABSTRACT,0.004223864836325237,"as depth varies via statistical analysis and visualization. Based on this finding, we
4"
ABSTRACT,0.005279831045406547,"propose a Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy that guides the
5"
ABSTRACT,0.006335797254487857,"weights of point cloud and RGB image modalities by introducing depth encoding
6"
ABSTRACT,0.007391763463569166,"at both global and local levels. Specifically, the Depth-Aware Global Feature
7"
ABSTRACT,0.008447729672650475,"Fusion (DGF) module adaptively adjusts the weights of image Bird’s-Eye-View
8"
ABSTRACT,0.009503695881731784,"(BEV) features in multi-modal global features via depth encoding. Furthermore,
9"
ABSTRACT,0.010559662090813094,"to compensate for the information lost when transferring raw features to the BEV
10"
ABSTRACT,0.011615628299894404,"space, we propose a Depth-Aware Local Feature Fusion (DLF) module, which
11"
ABSTRACT,0.012671594508975714,"adaptively adjusts the weights of original voxel features and multi-view image
12"
ABSTRACT,0.013727560718057022,"features in multi-modal local features via depth encoding. Extensive experiments
13"
ABSTRACT,0.014783526927138331,"on the nuScenes dataset demonstrate that our DH-Fusion method surpasses previous
14"
ABSTRACT,0.01583949313621964,"state-of-the-art methods w.r.t. NDS. Moreover, our DH-Fusion is more robust to
15"
ABSTRACT,0.01689545934530095,"various kinds of corruptions, outperforming previous methods on nuScenes-C w.r.t.
16"
ABSTRACT,0.01795142555438226,"both NDS and mAP.
17"
INTRODUCTION,0.01900739176346357,"1
Introduction
18"
INTRODUCTION,0.02006335797254488,"3D object detection has a wide range of applications in the fields of autonomous driving and robotics.
19"
INTRODUCTION,0.021119324181626188,"A large number of previous works have successfully focused on using a single modality, such as point
20"
INTRODUCTION,0.022175290390707498,"cloud or images, to design efficient 3D object detectors. However, the performance of these detectors
21"
INTRODUCTION,0.023231256599788808,"reaches a bottleneck due to the limitations of modality characteristics. For instance, the point cloud
22"
INTRODUCTION,0.024287222808870117,"modality can only provide rich geometric information while lacks detailed semantic information;
23"
INTRODUCTION,0.025343189017951427,"the image modality can only provide rich texture information while lacks three-dimensional spatial
24"
INTRODUCTION,0.026399155227032733,"information. To address the aforementioned issues, we are highly motivated to obtain comprehensive
25"
INTRODUCTION,0.027455121436114043,"information that represents objects by designing a LiDAR-camera 3D object detector.
26"
INTRODUCTION,0.028511087645195353,"In recent years, LiDAR-camera 3D object detection develops rapidly. Some works [1, 4, 28, 33, 67]
27"
INTRODUCTION,0.029567053854276663,"propose effective methods to integrate information from two modalities at the feature level. However,
28"
INTRODUCTION,0.030623020063357972,"they all overlook an important factor of depth in their fusion strategies. To understand how point
29"
INTRODUCTION,0.03167898627243928,"cloud and image information vary with depth, we first conduct statistical and visualization analysis
30"
INTRODUCTION,0.03273495248152059,"on the nuScenes-mini dataset [3], and find that: (1) The number of points representing objects at
31"
INTRODUCTION,0.0337909186906019,"near range is relatively large, which allows us to accurately determine the object’s location, size, and
32"
INTRODUCTION,0.03484688489968321,"category, even without the aid of images. As shown in Fig. 1a, there is an average of 163.7 points per
33"
INTRODUCTION,0.03590285110876452,"object within 0-10 meters, which is a substantial number. We also visualize a car at 6.8 meters in
34"
INTRODUCTION,0.03695881731784583,"Fig. 1b ①and find it encompasses a considerable number of points, well representing the shape. In
35"
INTRODUCTION,0.03801478352692714,"contrast, some background noise in the image may interfere with detection (Fig. 1b ②). (2) As the
36 5.9e4 10.4 1"
INTRODUCTION,0.03907074973600845,Number of Points Per Object
INTRODUCTION,0.04012671594508976,0-10      10-20      20-30     30-40     40-50
INTRODUCTION,0.04118268215417107,Depth (m) 20 10 30
INTRODUCTION,0.042238648363252376,"40
100 200 1 4.0e3 8.0e3 1.2e4 1.6e4 1.0e6 2.3e7"
INTRODUCTION,0.04329461457233368,"1.0e5
1.6e4 7.8e3 4.1e3 163.9 2.9"
INTRODUCTION,0.044350580781414996,"0.4
0.3"
INTRODUCTION,0.0454065469904963,"Point
Pixel
1.0e7"
INTRODUCTION,0.046462513199577615,Number of Pixels Per Object 150
INTRODUCTION,0.04751847940865892,(a) Statistical chart
INTRODUCTION,0.048574445617740235,"Image
Point Cloud"
M,0.04963041182682154,"6.8m ④ (2) ③ ①
②"
M,0.050686378035902854,42.1m
M,0.05174234424498416,35.3m
M,0.05279831045406547,6.8m
M,0.05385427666314678,42.1m
M,0.054910242872228086,35.3m
M,0.0559662090813094,(b) Visualization
M,0.057022175290390706,"Figure 1: Statistical and visualization analysis on the nuScenes-mini dataset. (a) The average numbers
of points and pixels for each object at different depths. (b) Examples of near-range and long-range
objects in images and point cloud. Points within the bounding boxes are colored red for observation."
M,0.05807814149947202,"depth increases, the number of points representing objects decreases rapidly. As shown in Fig. 1a,
37"
M,0.059134107708553325,"the number of points within 30-50 meters falls below one per object, meaning that many objects are
38"
M,0.06019007391763464,"even not represented by any points, such as the object at 42.1 meters in Fig. 1b ③. In contrast, the
39"
M,0.061246040126715945,"complete objects may still be observed on the image, as in Fig. 1b ④, where the image information
40"
M,0.06230200633579725,"becomes more important. To address the above problems, we propose a feature fusion strategy that
41"
M,0.06335797254487856,"adaptively adjusts the importance of the two modalities based on depth.
42"
M,0.06441393875395987,"Specifically, we propose a novel method for multi-modal 3D object detection, namely Depth-Aware
43"
M,0.06546990496304118,"Hybrid Feature Fusion (DH-Fusion). The innovation lies in adaptively adjusting the weights of
44"
M,0.0665258711721225,"features by introducing depth encoding to hybrid feature fusion at both global and local levels. The
45"
M,0.0675818373812038,"fusion strategy consists of two crucial components: Depth-Aware Global Feature Fusion (DGF)
46"
M,0.06863780359028511,"module and Depth-Aware Local Feature Fusion (DLF) module. In DGF, we take point cloud Bird’s-
47"
M,0.06969376979936642,"Eye-View (BEV) features and image BEV features as inputs, and dynamically adjust the weights of
48"
M,0.07074973600844774,"image BEV features based on depth during fusion by utilizing a global-fusion transformer encoder
49"
M,0.07180570221752904,"with a depth encoder. To compensate for the information lost when transforming raw features to
50"
M,0.07286166842661035,"BEV space, we enhance the fused BEV features at a lower cost by utilizing the original instance
51"
M,0.07391763463569166,"features. In DLF, we obtain 3D boxes by utilizing a Region Proposal Network (RPN). Then, the
52"
D BOXES ARE PROJECTED INTO BOTH LIDAR VOXEL FEATURES AND MULTI-VIEW IMAGE FEATURES TO CROP OUT,0.07497360084477296,"3D boxes are projected into both LiDAR voxel features and multi-view image features to crop out
53"
D BOXES ARE PROJECTED INTO BOTH LIDAR VOXEL FEATURES AND MULTI-VIEW IMAGE FEATURES TO CROP OUT,0.07602956705385427,"corresponding local instance features with more detailed information. Afterward, we take these as
54"
D BOXES ARE PROJECTED INTO BOTH LIDAR VOXEL FEATURES AND MULTI-VIEW IMAGE FEATURES TO CROP OUT,0.07708553326293559,"inputs and dynamically adjust the weights of local multi-view image features and local LiDAR voxel
55"
D BOXES ARE PROJECTED INTO BOTH LIDAR VOXEL FEATURES AND MULTI-VIEW IMAGE FEATURES TO CROP OUT,0.0781414994720169,"features based on depth through the use of a local-fusion transformer encoder with the depth encoder.
56"
D BOXES ARE PROJECTED INTO BOTH LIDAR VOXEL FEATURES AND MULTI-VIEW IMAGE FEATURES TO CROP OUT,0.0791974656810982,"In the end, we update local features for each object on the global feature map to enhance the detailed
57"
D BOXES ARE PROJECTED INTO BOTH LIDAR VOXEL FEATURES AND MULTI-VIEW IMAGE FEATURES TO CROP OUT,0.08025343189017951,"instance information of multi-modal global features for detection.
58"
D BOXES ARE PROJECTED INTO BOTH LIDAR VOXEL FEATURES AND MULTI-VIEW IMAGE FEATURES TO CROP OUT,0.08130939809926083,"Our contributions are summarized as follows.
59"
WE FOR THE FIRST TIME POINT OUT THAT DEPTH IS AN IMPORTANT FACTOR TO CONSIDER WHILE FUSING LIDAR,0.08236536430834214,"1. We for the first time point out that depth is an important factor to consider while fusing LiDAR
60"
WE FOR THE FIRST TIME POINT OUT THAT DEPTH IS AN IMPORTANT FACTOR TO CONSIDER WHILE FUSING LIDAR,0.08342133051742344,"point cloud features and RGB image features for 3D object detection. From our statistical and
61"
WE FOR THE FIRST TIME POINT OUT THAT DEPTH IS AN IMPORTANT FACTOR TO CONSIDER WHILE FUSING LIDAR,0.08447729672650475,"visualization analysis, we can see that image features play different roles as depth varies.
62"
WE PROPOSE A DEPTH-AWARE HYBRID FEATURE FUSION STRATEGY THAT DYNAMICALLY ADJUSTS THE WEIGHTS OF,0.08553326293558607,"2. We propose a depth-aware hybrid feature fusion strategy that dynamically adjusts the weights of
63"
WE PROPOSE A DEPTH-AWARE HYBRID FEATURE FUSION STRATEGY THAT DYNAMICALLY ADJUSTS THE WEIGHTS OF,0.08658922914466737,"features during feature fusion by introducing depth encoding at both global and local levels. The
64"
WE PROPOSE A DEPTH-AWARE HYBRID FEATURE FUSION STRATEGY THAT DYNAMICALLY ADJUSTS THE WEIGHTS OF,0.08764519535374868,"above strategy can obtain high-quality features for detection, fully leveraging the advantages of
65"
WE PROPOSE A DEPTH-AWARE HYBRID FEATURE FUSION STRATEGY THAT DYNAMICALLY ADJUSTS THE WEIGHTS OF,0.08870116156282999,"different modalities at various depths.
66"
WE PROPOSE A DEPTH-AWARE HYBRID FEATURE FUSION STRATEGY THAT DYNAMICALLY ADJUSTS THE WEIGHTS OF,0.0897571277719113,"3. Our method is evaluated on the nuScenes [3] dataset and a more challenging nuScenes-C [13]
67"
WE PROPOSE A DEPTH-AWARE HYBRID FEATURE FUSION STRATEGY THAT DYNAMICALLY ADJUSTS THE WEIGHTS OF,0.0908130939809926,"dataset, outperforming previous multi-modal methods and being robust to various kinds of data
68"
WE PROPOSE A DEPTH-AWARE HYBRID FEATURE FUSION STRATEGY THAT DYNAMICALLY ADJUSTS THE WEIGHTS OF,0.09186906019007392,"corruptions.
69"
RELATED WORK,0.09292502639915523,"2
Related Work
70"
RELATED WORK,0.09398099260823653,"Since our method is based on conducting 3D object detection using data from multiple modalities,
71"
RELATED WORK,0.09503695881731784,"including point cloud and images, we briefly review recent works in the following fields: LiDAR-
72"
RELATED WORK,0.09609292502639916,"based 3D object detection, camera-based 3D object detection, and LiDAR-camera 3D object detection.
73"
RELATED WORK,0.09714889123548047,"2.1
LiDAR-based 3D Object Detection
74"
RELATED WORK,0.09820485744456177,"LiDAR-based 3D object detectors only take the point cloud as input. Based on their different data
75"
RELATED WORK,0.09926082365364308,"representations, they can be divided into point-based [44–46, 64, 65], voxel-based [12, 22, 61, 68, 71],
76"
RELATED WORK,0.1003167898627244,"and point-voxel-based [17, 42, 43] methods. The feature extraction networks of point-based methods
77"
RELATED WORK,0.10137275607180571,"typically extract features directly from the point cloud through a point-based backbone [40], such as
78"
RELATED WORK,0.10242872228088701,"PointRCNN [44]. The voxel-based methods first convert the point cloud into voxels and then extract
79"
RELATED WORK,0.10348468848996832,"voxel features through a 3D sparse convolution network [14], such as VoxelNet [71]. Point-voxel-
80"
RELATED WORK,0.10454065469904963,"based methods like PV-RCNN [42] combine the above two methods to extract and fuse point and
81"
RELATED WORK,0.10559662090813093,"voxel features. The purpose of these approaches is to capture the geometric spatial information of the
82"
RELATED WORK,0.10665258711721225,"point cloud. However, point cloud is sparse and incomplete, lacking detailed texture information,
83"
RELATED WORK,0.10770855332629356,"which greatly limits the detection performance.
84"
RELATED WORK,0.10876451953537487,"2.2
Camera-based 3D Object Detection
85"
RELATED WORK,0.10982048574445617,"Camera-based 3D object detectors only take images as inputs. Depending on the form of inputs,
86"
RELATED WORK,0.11087645195353749,"they can be divided into monocular [2, 24, 32, 41, 47, 55], stereo [6, 25, 30, 48, 70], and multi-view
87"
RELATED WORK,0.1119324181626188,"[19, 27, 56, 62] 3D object detectors. Early works like FCOS3D [55] input a monocular image and
88"
RELATED WORK,0.11298838437170011,"utilize 2D object detectors to directly predict 3D bounding boxes, but these approaches have limited
89"
RELATED WORK,0.11404435058078141,"capability in capturing spatial information. Subsequently, stereo and multi-view 3D object detectors
90"
RELATED WORK,0.11510031678986272,"are proposed to obtain more precise depth information by constructing spatial relationships among
91"
RELATED WORK,0.11615628299894404,"multiple images, such as Stereo RCNN [25] and BEVDet [19]. These methods successfully achieve
92"
RELATED WORK,0.11721224920802534,"purely visual 3D object detection, but they do not perform as well as LiDAR-based methods, because
93"
RELATED WORK,0.11826821541710665,"the spatial depth information provided by images is not as direct and precise as that provided by point
94"
RELATED WORK,0.11932418162618796,"cloud.
95"
RELATED WORK,0.12038014783526928,"2.3
LiDAR-Camera 3D Object Detection
96"
RELATED WORK,0.12143611404435058,"LiDAR-camera 3D object detectors take point cloud and images as inputs, and can be classified
97"
RELATED WORK,0.12249208025343189,"into early-fusion-based [50, 52, 57, 59, 69], intermediate-fusion-based [1, 4, 28, 33, 67], and late-
98"
RELATED WORK,0.1235480464625132,"fusion-based [37, 38] 3D object detectors based on the location of multi-modal information fusion
99"
RELATED WORK,0.1246040126715945,"[36].
100"
RELATED WORK,0.12565997888067582,"Early-fusion-based methods perform at the point level, where the typical approach involves enhancing
101"
RELATED WORK,0.12671594508975711,"the raw point cloud with semantic information extracted from images. PointPainting [50] and Fu-
102"
RELATED WORK,0.12777191129883844,"sionPainting [59] decorate the raw point cloud with semantic scores from 2D semantic segmentation.
103"
RELATED WORK,0.12882787750791974,"Similarly, PointAugmenting [52] enhances the raw point cloud using features extracted from a 2D
104"
RELATED WORK,0.12988384371700107,"semantic segmentation network. However, early-fusion-based methods are sensitive to alignment
105"
RELATED WORK,0.13093980992608237,"errors between the two modalities.
106"
RELATED WORK,0.13199577613516367,"Intermediate-fusion-based methods perform at the feature level. Transfusion [1] first proposes to
107"
RELATED WORK,0.133051742344245,"utilize the transformer for fine-grained fusion from LiDAR BEV features and multi-view image
108"
RELATED WORK,0.1341077085533263,"features. FUTR3D [5] encode each modality using deformable attention [73] in its own coordinate
109"
RELATED WORK,0.1351636747624076,"and concatenate them for fusion. BEVFusion [28, 33] projects both point cloud and images to BEV
110"
RELATED WORK,0.13621964097148892,"space for BEV feature fusion. SparseFusion [58] extracts instance-level features from both two
111"
RELATED WORK,0.13727560718057022,"modalities separately, and fuse them to perform detection. Similarly, ObjectFusion [4] utilizes 3D
112"
RELATED WORK,0.13833157338965152,"proposals from LiDAR modality to extract instance-level features for fusion. CMT [60] proposes
113"
RELATED WORK,0.13938753959873285,"the simultaneous interaction between the object queries and multi-modal features in the transformer
114"
RELATED WORK,0.14044350580781415,"encoder and decoder. IS-Fusion [67] proposes feature fusion at both the instance level and scene
115"
RELATED WORK,0.14149947201689547,"level. The intermediate-fusion-based methods gradually become a mainstream approach due to the
116"
RELATED WORK,0.14255543822597677,"diversity of fusion strategies.
117"
RELATED WORK,0.14361140443505807,"Late-fusion-based methods perform at the bounding box level. Typically, CLOCs [37] obtains 2D and
118"
RELATED WORK,0.1446673706441394,"3D bounding boxes by separately using 2D and 3D object detectors, and then combine them to achieve
119"
RELATED WORK,0.1457233368532207,"more accurate 3D bounding boxes. However, the interaction between modalities in late-fusion-based
120"
RELATED WORK,0.146779303062302,"methods is very limited, which constrains model performance.
121"
RELATED WORK,0.14783526927138332,"These multi-modal methods successfully outperform single-modal methods. However, their feature
122"
RELATED WORK,0.14889123548046462,"fusion methods do not take depth into account. In contrast, our approach introduces depth information
123"
RELATED WORK,0.14994720168954592,"to guide the hybrid feature fusion, boosting the performance of the detector.
124"
RELATED WORK,0.15100316789862725,Multi-view Images
RELATED WORK,0.15205913410770855,LiDAR Point Cloud
D ENCODER,0.15311510031678988,3D Encoder
D ENCODER,0.15417106652587118,2D Encoder … … …
D ENCODER,0.15522703273495247,Decoder
D ENCODER,0.1562829989440338,Detection
D ENCODER,0.1573389651531151,"Head
Global Feature Fusion
Local Feature Fusion"
D ENCODER,0.1583949313621964,Depth Encoder
D ENCODER,0.15945089757127773,"Input Encoding
Depth-Aware Global Feature Fusion
Depth-Aware Local Feature Fusion
Decoding G
O"
D ENCODER,0.16050686378035903,Select
D ENCODER,0.16156282998944033,Select
D ENCODER,0.16261879619852165,Project
D ENCODER,0.16367476240760295,Project
D ENCODER,0.16473072861668428,"View 
Transfer"
D ENCODER,0.16578669482576558,"Compress G
O G
B"
D ENCODER,0.16684266103484688,"G
B
L
O G
B L
B L
O G
Bˆ G
B"
D ENCODER,0.1678986272439282,"M
Select"
D ENCODER,0.1689545934530095,Depth Encoder
D ENCODER,0.1700105596620908,Depth Encoder M
D ENCODER,0.17106652587117213,Sine and
D ENCODER,0.17212249208025343,Cosine De
D ENCODER,0.17317845828933473,"Figure 2: Overview of our method. It introduces depth encoding in both global and local feature
fusion to obtain depth-adaptive multi-modal representations for detection.
is the multiplication
operation, and M is the merge operation."
METHODOLOGY,0.17423442449841606,"3
Methodology
125"
METHODOLOGY,0.17529039070749736,"In this section, we first give an overview of our proposed multi-modal 3D object detector, and then
126"
METHODOLOGY,0.17634635691657866,"provide a detailed introduction to our proposed feature fusion method.
127"
OVERVIEW,0.17740232312565998,"3.1
Overview
128"
OVERVIEW,0.17845828933474128,"We propose a multi-modal 3D object detection method via Depth-Aware Hybrid Feature Fusion
129"
OVERVIEW,0.1795142555438226,"(DH-Fusion). As illustrated in Fig. 2, our approach consists of two important feature fusion modules:
130"
OVERVIEW,0.1805702217529039,"Depth-Aware Global Feature Fusion (DGF) and Depth-Aware Local Feature Fusion (DLF). In the
131"
OVERVIEW,0.1816261879619852,"following, we briefly describe the detection pipeline.
132"
OVERVIEW,0.18268215417106654,"Inputs. First, we take the point cloud P and multi-view images I as inputs, where point cloud
133"
OVERVIEW,0.18373812038014783,"consists of a set of points: P = {P1, P2, · · · , PNl}, and each point has four dimensions: X-axis,
134"
OVERVIEW,0.18479408658922913,"Y-axis, Z-axis, and intensity; the multi-view images comprise Nc images: I = {I1, I2, · · · , INc},
135"
OVERVIEW,0.18585005279831046,"each image captured by its corresponding camera.
136"
OVERVIEW,0.18690601900739176,"Input Encoding. For the point cloud P, we use a 3D encoder to extract raw global voxel features
137"
OVERVIEW,0.18796198521647306,"VG
O; for the multi-view images I, we use a 2D encoder to extract image features of all views IG
O.
138"
OVERVIEW,0.1890179514255544,"Hybrid Feature Fusion. Then, for voxel features VG
O, we compress the height dimension to obtain
139"
OVERVIEW,0.1900739176346357,"point cloud BEV features VG
B ; for image features IG
O, we transform their perspective view to bird’s
140"
OVERVIEW,0.191129883843717,"eye view to obtain image BEV features IG
B. To fully leverage the features from two modalities, we
141"
OVERVIEW,0.1921858500527983,"design a DGF module that aims to dynamically adjust the weights of image BEV features based
142"
OVERVIEW,0.1932418162618796,"on depth values during feature fusion. Please refer to Sec. 3.2 for more details. To compensate
143"
OVERVIEW,0.19429778247096094,"for the information lost when transforming raw features to BEV space, we propose a DLF module
144"
OVERVIEW,0.19535374868004224,"that, based on depth, utilizes the raw features to enhance the detailed information of each object
145"
OVERVIEW,0.19640971488912354,"instance in global multi-modal features. It consists of three processes: local feature selection, local
146"
OVERVIEW,0.19746568109820486,"feature fusion, and merging local features into global features. First, we obtain the local multi-modal
147"
OVERVIEW,0.19852164730728616,"BEV features FL
B, local voxel features VL
O, and local multi-view image features IL
O, by cropping the
148"
OVERVIEW,0.19957761351636746,"corresponding global features based on the 3D boxes obtained from an RPN; then, it dynamically
149"
OVERVIEW,0.2006335797254488,"and individually adjusts the weights of each local feature of VL
O and IL
O based on depth values during
150"
OVERVIEW,0.2016895459345301,"feature fusion; finally, we update local features for each object on the global feature map. Please
151"
OVERVIEW,0.20274551214361142,"refer to Sec. 3.3 for more details. In this way, we obtain enhanced multi-modal global features for
152"
OVERVIEW,0.20380147835269272,"detection.
153"
OVERVIEW,0.20485744456177402,"Decoding. Based on the enhanced multi-modal global features ˆFG
B that contain rich semantic and
154"
OVERVIEW,0.20591341077085534,"spatial information, we utilize a transformer decoder and a detection head to predict the object
155"
OVERVIEW,0.20696937697993664,"categories and 3D bounding boxes.
156"
OVERVIEW,0.20802534318901794,Cross Attention
OVERVIEW,0.20908130939809927,Add & Norm FFN
OVERVIEW,0.21013727560718057,Add & Norm
OVERVIEW,0.21119324181626187,Multiply & Norm
OVERVIEW,0.2122492080253432,"Global-Fusion Transformer Q
K
V G
B G
B"
OVERVIEW,0.2133051742344245,"Depth Encoder G
B"
OVERVIEW,0.21436114044350582,"Figure 3: Illustration of the DGF.
It consists of a global fusion trans-
former with the depth encoder."
OVERVIEW,0.21541710665258712,Cross Attention
OVERVIEW,0.21647307286166842,Add & Norm
OVERVIEW,0.21752903907074975,Depth Encoder
OVERVIEW,0.21858500527983105,Multiply & Norm …
OVERVIEW,0.21964097148891235,Projection
OVERVIEW,0.22069693769799367,Cross Attention
OVERVIEW,0.22175290390707497,Add & Norm
OVERVIEW,0.22280887011615627,Depth Encoder
OVERVIEW,0.2238648363252376,Multiply & Norm … …
OVERVIEW,0.2249208025343189,"Projection
Cat & Conv Merge"
OVERVIEW,0.22597676874340022,"Local Feature Selection
Local-Fusion Transformer"
OVERVIEW,0.22703273495248152,"Q
K
V
Q
K
V"
OVERVIEW,0.22808870116156282,"G
O
G
O L
O G
B L
B L
O G
Bˆ"
OVERVIEW,0.22914466737064415,"Figure 4: Illustration of the DLF. It consists of a local feature
selection module and a local fusion transformer with the
depth encoder."
DEPTH-AWARE GLOBAL FEATURE FUSION,0.23020063357972545,"3.2
Depth-Aware Global Feature Fusion
157"
DEPTH-AWARE GLOBAL FEATURE FUSION,0.23125659978880675,"As shown in Fig. 3, the DGF module consists of a global-fusion transformer with a depth encoder. In
158"
DEPTH-AWARE GLOBAL FEATURE FUSION,0.23231256599788808,"the following, we provide a detailed explanation of each component.
159"
DEPTH ENCODER,0.23336853220696938,"3.2.1
Depth Encoder
160"
DEPTH ENCODER,0.23442449841605068,"We introduce depth encoding (DE) in feature fusion to dynamically adjust the weights of image BEV
161"
DEPTH ENCODER,0.235480464625132,"features during fusion. First, we build a depth matrix M to store the depth value of each position
162"
DEPTH ENCODER,0.2365364308342133,"element pk represented as:
163"
DEPTH ENCODER,0.2375923970432946,"pk = {(xk, yk) : dk}, k ∈[1, n],
(1)"
DEPTH ENCODER,0.23864836325237593,"where (xk, yk) are the positional coordinates, dk is the depth value, and n is the number of elements.
164"
DEPTH ENCODER,0.23970432946145723,"Then, we use Euclidean distance to calculate the distance between every element’s spatial location
165"
DEPTH ENCODER,0.24076029567053855,"(xk, yk) and the ego coordinate element’s location (x n"
DEPTH ENCODER,0.24181626187961985,"2 , y n"
DEPTH ENCODER,0.24287222808870115,"2 ):
166"
DEPTH ENCODER,0.24392819429778248,"dk = E((xk, yk), (x n"
DEPTH ENCODER,0.24498416050686378,"2 , y n"
DEPTH ENCODER,0.24604012671594508,"2 )), k ∈[1, n],
(2)"
DEPTH ENCODER,0.2470960929250264,"where we denote E(·) as the Euclidean distance calculation. The depth matrix M serves as a lookup
167"
DEPTH ENCODER,0.2481520591341077,"table to avoid redundant computation of depth values. Since the size of the BEV features is large and
168"
DEPTH ENCODER,0.249208025343189,"the depth distribution is simple, to avoid introducing additional parameters, the depth encoding De is
169"
DEPTH ENCODER,0.2502639915522703,"obtained by applying sine and cosine functions [49] to the depth matrix.
170"
GLOBAL-FUSION TRANSFORMER,0.25131995776135163,"3.2.2
Global-Fusion Transformer
171"
GLOBAL-FUSION TRANSFORMER,0.25237592397043296,"In the global-fusion transformer, we take the point cloud BEV features VG
B ∈RW ×H×C and image
172"
GLOBAL-FUSION TRANSFORMER,0.25343189017951423,"BEV features IG
B ∈RW ×H×C as inputs, and integrate the depth encoding obtained above by multi-
173"
GLOBAL-FUSION TRANSFORMER,0.25448785638859556,"plying it with the point cloud BEV features, forming the query QG
V = N(VG
B × Conv(De)), where
174"
GLOBAL-FUSION TRANSFORMER,0.2555438225976769,"Conv(·) is a convolution operation to align with the channels of VG
B, and N(·) is a normalization
175"
GLOBAL-FUSION TRANSFORMER,0.2565997888067582,"layer. The image BEV features are queried as the corresponding key KG
I and value V G
I . We utilize
176"
GLOBAL-FUSION TRANSFORMER,0.2576557550158395,"the multi-head cross attention to achieve the interacted feature ˆVG
B based on depth:
177"
GLOBAL-FUSION TRANSFORMER,0.2587117212249208,"ˆVG
B = CA(QG
V , KG
I , V G
I ),
(3)"
GLOBAL-FUSION TRANSFORMER,0.25976768743400214,"where CA(·) indicates the multi-head cross attention. Afterward, we aggregate the information from
178"
GLOBAL-FUSION TRANSFORMER,0.2608236536430834,"both modalities to obtain the fused features FG
B :
179"
GLOBAL-FUSION TRANSFORMER,0.26187961985216474,"FG
B = N(FFN(N(ˆVG
B + VG
B )) + N(ˆVG
B + VG
B )),
(4)"
GLOBAL-FUSION TRANSFORMER,0.26293558606124606,"where N(·) is a normalization layer; FFN(·) specifies a feed-forward network containing two
180"
GLOBAL-FUSION TRANSFORMER,0.26399155227032733,"convolution operations. In this way, we obtain fused features in which the image features play
181"
GLOBAL-FUSION TRANSFORMER,0.26504751847940866,"different roles as the depth varies.
182"
DEPTH-AWARE LOCAL FEATURE FUSION,0.26610348468849,"3.3
Depth-Aware Local Feature Fusion
183"
DEPTH-AWARE LOCAL FEATURE FUSION,0.26715945089757126,"As shown in Fig. 4, the DLF module consists of a local feature selection and a local-fusion transformer
184"
DEPTH-AWARE LOCAL FEATURE FUSION,0.2682154171066526,"with the depth encoder. In the following, we provide a detailed explanation of each component.
185"
LOCAL FEATURE SELECTION,0.2692713833157339,"3.3.1
Local Feature Selection
186"
LOCAL FEATURE SELECTION,0.2703273495248152,"To compensate for the information lost when transforming point cloud features and image features to
187"
LOCAL FEATURE SELECTION,0.2713833157338965,"BEV space, we enhance the instance details of fused BEV features FG
B using instance features from
188"
LOCAL FEATURE SELECTION,0.27243928194297784,"raw voxel features VG
O and multi-view image features IG
O. Specifically, we utilize an RPN to regress
189"
LOCAL FEATURE SELECTION,0.2734952481520591,"t 3D boxes based on the BEV features FG
B . We directly crop the global fused BEV features FG
B
190"
LOCAL FEATURE SELECTION,0.27455121436114044,"based on the regressed 3D boxes to obtain the local fused BEV features FL
B ∈Rc×t. On the other
191"
LOCAL FEATURE SELECTION,0.27560718057022177,"hand, we project the 3D boxes onto the raw voxel features and multi-view image features to obtain
192"
LOCAL FEATURE SELECTION,0.27666314677930304,"their corresponding local features before global fusion, preserving richer information for each object
193"
LOCAL FEATURE SELECTION,0.27771911298838436,"instance. Specifically, we utilize the voxel pooling operation [12], followed by a 3D convolution
194"
LOCAL FEATURE SELECTION,0.2787750791974657,"operation and a linear layer, to extract local voxel features VL
O ∈Rc×t; we transform the 3D boxes
195"
LOCAL FEATURE SELECTION,0.27983104540654696,"from bird’s eye view to perspective view, and utilize the RoI Align operation [15], followed by a
196"
LOCAL FEATURE SELECTION,0.2808870116156283,"linear layer, to extract instance image features IL
O ∈Rc×t. By doing this, we obtain the hybrid
197"
LOCAL FEATURE SELECTION,0.2819429778247096,"(before & after global fusion) local features, which will be sent to the subsequent fusion module.
198"
LOCAL-FUSION TRANSFORMER,0.28299894403379094,"3.3.2
Local-Fusion Transformer
199"
LOCAL-FUSION TRANSFORMER,0.2840549102428722,"In the local-fusion transformer, the weights of each local raw feature are dynamically adjusted based
200"
LOCAL-FUSION TRANSFORMER,0.28511087645195354,"on depth values during feature fusion, and we update local features for each object on the global
201"
LOCAL-FUSION TRANSFORMER,0.28616684266103487,"feature map. Specifically, we take the local multi-modal BEV features FL
B, local voxel features VL
O,
202"
LOCAL-FUSION TRANSFORMER,0.28722280887011614,"and local multi-view image features IL
O as inputs, and integrate the depth encoding by multiplying
203"
LOCAL-FUSION TRANSFORMER,0.28827877507919747,"it with the local multi-modal BEV features, forming the query QL
F. The local multi-view image
204"
LOCAL-FUSION TRANSFORMER,0.2893347412882788,"features and local voxel features are respectively queried as the corresponding key KL
I , KL
V and value
205"
LOCAL-FUSION TRANSFORMER,0.29039070749736007,"V L
I , V L
V . The two multi-head cross-attention modules are utilized to achieve the interacted features
206"
LOCAL-FUSION TRANSFORMER,0.2914466737064414,"ˆQL
F, ˆQL
F
′. Note that the computation process of multi-head cross attention is similar to that described
207"
LOCAL-FUSION TRANSFORMER,0.2925026399155227,"in Sec. 3.2.2 and is omitted here. Afterward, we aggregate the above features:
208"
LOCAL-FUSION TRANSFORMER,0.293558606124604,"ˆFL
B = Conv(Cat( ˆQL
F + FL
B, ˆQL
F
′ + FL
B
′)),
(5)"
LOCAL-FUSION TRANSFORMER,0.2946145723336853,"where Cat(·) is the concatenation operation; Conv(·) is used to align with the feature channels of
209"
LOCAL-FUSION TRANSFORMER,0.29567053854276665,"global fused BEV features FG
B . As a result, we obtain enhanced local features by dynamically calling
210"
LOCAL-FUSION TRANSFORMER,0.2967265047518479,"back rich information in raw modalities at various depths. Afterward, we update the global features
211"
LOCAL-FUSION TRANSFORMER,0.29778247096092925,"FG
B by inserting the enhanced local features at corresponding locations.
212"
EXPERIMENTS,0.2988384371700106,"4
Experiments
213"
EXPERIMENTS,0.29989440337909185,"In this section, we will first introduce the dataset and evaluation metrics, followed by the implementa-
214"
EXPERIMENTS,0.30095036958817317,"tion details. Then, we will compare our method with the state-of-the-art methods on nuScenes and
215"
EXPERIMENTS,0.3020063357972545,"also present results on a more challenging dataset of nuScenes-C with data corruptions. Finally, we
216"
EXPERIMENTS,0.30306230200633577,"will show the ablation studies and qualitative results. More experiments are provided in Appendix
217"
EXPERIMENTS,0.3041182682154171,"A.2.
218"
EXPERIMENTAL SETUP,0.3051742344244984,"4.1
Experimental Setup
219"
EXPERIMENTAL SETUP,0.30623020063357975,"Datasets and evaluation metrics. We evaluate our proposed DH-Fusion on the nuScenes benchmark
220"
EXPERIMENTAL SETUP,0.307286166842661,"[3] and a more challenging dataset of nuScenes-C [13] with data corruptions. nuScenes dataset
221"
EXPERIMENTAL SETUP,0.30834213305174235,"provides 700 scene sequences for training, 150 scene sequences for validation, and 150 scene
222"
EXPERIMENTAL SETUP,0.3093980992608237,"sequences for testing. Each sequence contains 40 frames of 32-beam LiDAR data, and each frame
223"
EXPERIMENTAL SETUP,0.31045406546990495,"has six corresponding images covering a 360-degree field of view. It offers calibration matrices that
224"
EXPERIMENTAL SETUP,0.3115100316789863,"facilitate accurate projection of 3D points onto 2D pixels, and contains 10 object categories that are
225"
EXPERIMENTAL SETUP,0.3125659978880676,"commonly encountered within autonomous driving. nuScenes-C dataset provides 27 corruptions
226"
EXPERIMENTAL SETUP,0.3136219640971489,"with 5 severities on the nuScenes validation set, including corruptions at the weather, sensor, motion,
227"
EXPERIMENTAL SETUP,0.3146779303062302,"object, and alignment level. We use the nuScenes detection scores (NDS) and mean Average Precision
228"
EXPERIMENTAL SETUP,0.31573389651531153,"(mAP) to evaluate our detection results, where NDS is a comprehensive metric in nuScenes that
229"
EXPERIMENTAL SETUP,0.3167898627243928,"combines object translation, scale, orientation, velocity, and attribute errors.
230"
EXPERIMENTAL SETUP,0.31784582893347413,"Implementation details. We implement the proposed DH-Fusion with PyTorch [39] under the
231"
EXPERIMENTAL SETUP,0.31890179514255546,"open-source framework MMDetection3D [10]. Specifically, for the LiDAR branch, we use VoxelNet
232"
EXPERIMENTAL SETUP,0.3199577613516367,"[71] with FPN [61] as the 3D encoder. The voxel size is set to [0.075m, 0.075m, 0.1m], and the range
233"
EXPERIMENTAL SETUP,0.32101372756071805,"of point cloud is [-54m, 54m] along the X-axis, [-54m, 54m] along the Y-axis, and [-3m, 5m] along
234"
EXPERIMENTAL SETUP,0.3220696937697994,"the Z-axis. For the image branch, we use the ResNet18 [16], ResNet50 [16], and SwinTiny [34] with
235"
EXPERIMENTAL SETUP,0.32312565997888065,"FPN [29] as the 2D image encoder of DH-Fusion-light, -base, -large, respectively. Correspondingly,
236"
EXPERIMENTAL SETUP,0.324181626187962,"the resolution of input images is resized to 256 × 704, 320 × 800, and 384 × 1056. Additionally, we
237"
EXPERIMENTAL SETUP,0.3252375923970433,"utilize BEVPoolV2 [18] to obtain image BEV features. Following [33], the feature size W × H is set
238"
EXPERIMENTAL SETUP,0.3262935586061246,"to 180 × 180, the channel C is set to 128, and the channel c is also set to 128. The multi-head cross
239"
EXPERIMENTAL SETUP,0.3273495248152059,"attention is implemented with 8 heads, and the FFN contains 2 MLP layers with a hidden dimension
240"
EXPERIMENTAL SETUP,0.32840549102428723,"of 128. Following [58], the number of regressed 3D boxes t is set to 200. More implementation
241"
EXPERIMENTAL SETUP,0.32946145723336856,"details are provided in Appendix A.1.
242"
COMPARISON TO THE STATE OF THE ART,0.33051742344244983,"4.2
Comparison to the State of the Art
243"
COMPARISON TO THE STATE OF THE ART,0.33157338965153116,"Aiming for a fair comparison, we categorize previous methods based on the types of 2D backbones
244"
COMPARISON TO THE STATE OF THE ART,0.3326293558606125,"into ResNet50-based, SwinTiny-based, and others, and provide three versions of our proposed method,
245"
COMPARISON TO THE STATE OF THE ART,0.33368532206969376,"named DH-Fusion-light, DH-Fusion-base, and DH-Fusion-large. The results are shown in Tab. 1.
246"
COMPARISON TO THE STATE OF THE ART,0.3347412882787751,"(1) Compared with the ResNet50-based methods, our DH-Fusion-base outperforms the top method
247"
COMPARISON TO THE STATE OF THE ART,0.3357972544878564,"FocalFormer3D [7] by up to 1 pp w.r.t. NDS under the same configuration. Specifically, we reach
248"
COMPARISON TO THE STATE OF THE ART,0.3368532206969377,"74.0% w.r.t. NDS and 71.2% w.r.t. mAP on the validation set, and 74.7% w.r.t. NDS and 71.7%
249"
COMPARISON TO THE STATE OF THE ART,0.337909186906019,"w.r.t. mAP on the test set, while maintaining comparable inference speed of 8.7 FPS on a 3090 GPU.
250"
COMPARISON TO THE STATE OF THE ART,0.33896515311510034,"(2) Compared with the SwinTiny-based methods and others, our DH-Fusion-large outperforms the
251"
COMPARISON TO THE STATE OF THE ART,0.3400211193241816,"top method IS-Fusion [67] under the same configuration, and runs 2x faster than it. Specifically, we
252"
COMPARISON TO THE STATE OF THE ART,0.34107708553326294,"reach 74.4% w.r.t. NDS on the validation set, and 75.4% w.r.t. NDS on the test set, while achieving a
253"
COMPARISON TO THE STATE OF THE ART,0.34213305174234426,"faster inference speed of 5.7 FPS on a 3090 GPU, indicating that our proposed method is both more
254"
COMPARISON TO THE STATE OF THE ART,0.34318901795142553,"effective and efficient. (3) Furthermore, our DH-Fusion-light surpasses the typical BEVFusion [33]
255"
COMPARISON TO THE STATE OF THE ART,0.34424498416050686,"by up to 1 pp w.r.t. all metrics using a lighter 2D backbone, and achieves a real-time inference speed
256"
COMPARISON TO THE STATE OF THE ART,0.3453009503695882,"of 13.8 FPS. Overall, our method achieves higher detection accuracy and faster inference speed.
257"
ROBUSTNESS TO CORRUPTIONS,0.34635691657866946,"4.3
Robustness to Corruptions
258"
ROBUSTNESS TO CORRUPTIONS,0.3474128827877508,"We further implement some experiments on the nuScenes-C [13] dataset to evaluate the model’s
259"
ROBUSTNESS TO CORRUPTIONS,0.3484688489968321,"robustness under various corruptions, including changes in weather, data loss or temporal-spatial
260"
ROBUSTNESS TO CORRUPTIONS,0.3495248152059134,"misalignment in multi-modal inputs, etc. The results for different kinds of corruptions are shown
261"
ROBUSTNESS TO CORRUPTIONS,0.3505807814149947,"in Tab. 2, and more detailed results for each fine-grained corruption are shown in Appendix A.2.3.
262"
ROBUSTNESS TO CORRUPTIONS,0.35163674762407604,"We find that our DH-Fusion-light still achieves an average performance of 68.67% w.r.t. NDS and
263"
ROBUSTNESS TO CORRUPTIONS,0.3526927138331573,"63.07% w.r.t. mAP under various corruptions, which only decreases by 4.63 pp w.r.t. NDS and
264"
ROBUSTNESS TO CORRUPTIONS,0.35374868004223864,"6.68 pp w.r.t. mAP, compared to its performance without corruptions. Performance drop is smaller
265"
ROBUSTNESS TO CORRUPTIONS,0.35480464625131997,"than that observed with previous methods including BEVFusion [28] across all kinds of corruptions,
266"
ROBUSTNESS TO CORRUPTIONS,0.3558606124604013,"indicating that our DH-Fusion-light possesses superior robustness. Furthermore, we observe that our
267"
ROBUSTNESS TO CORRUPTIONS,0.35691657866948256,"DH-Fusion-light is particularly robust against weather and object corruptions, where the performance
268"
ROBUSTNESS TO CORRUPTIONS,0.3579725448785639,"drop is less than 3pp. The more stable performance indicates that our method is more friendly to
269"
ROBUSTNESS TO CORRUPTIONS,0.3590285110876452,"practical applications, where data corruption may occur.
270"
ABLATION STUDIES,0.3600844772967265,"4.4
Ablation Studies
271"
ABLATION STUDIES,0.3611404435058078,"We conduct ablation studies to first demonstrate the effect of each component of DH-Fusion, then
272"
ABLATION STUDIES,0.36219640971488914,"to demonstrate the effect of depth encoding in DGF and DLF, and finally to assess the impact of
273"
ABLATION STUDIES,0.3632523759239704,"multiplying depth encoding. All method variants are implemented on the nuScenes validation dataset.
274"
ABLATION STUDIES,0.36430834213305174,"Table 1: Comparisons with the state of the art on the nuScenes validation and test sets. FPS is
measured on a 3090 GPU by default, and * denotes the inference speed on an A100 GPU referred
from the original paper. Note that all results are obtained without any model ensemble or test time
augmentation."
ABLATION STUDIES,0.36536430834213307,"Methods
Present at
Image Size - 2D Backbone
FPS
Validation
Test
NDS mAP NDS mAP
Image Backbone: ResNet50[16]
Trainsfusion [1]
CVPR’22
320 × 800-ResNet50
6.5
71.3
67.5
71.7
68.9
DeepInteraction [66]
NeurIPS’22
448 × 800-ResNet50
1.9
72.4
69.9
73.4
70.8
MSMDFusion [21]
CVPR’23
448 × 800- ResNet50
2.1
72.1
69.7
74.0
71.5
FocalFormer3D [7]
ICCV’23
320 × 800-ResNet50
9.2*
73.1
70.1
73.9
71.6
DH-Fusion-base (Ours)
-
320 × 800-ResNet50
8.7
74.0
71.2
74.7
71.7
Image Backbone: SwinTiny[31]
BEVFusion [28]
NeurIPS’22
448 × 800-SwinTiny
0.7*
71.0
67.9
71.8
69.2
BEVFusion [33]
ICRA’23
256 × 704- SwinTiny
9.6
71.4
68.5
72.9
70.2
ObjectFusion [4]
ICCV’23
256 × 704- SwinTiny
-
72.3
69.8
73.3
71.0
SparseFusion [58]
ICCV’23
256 × 704- SwinTiny
4.4
72.8
70.5
73.8
72.0
IS-Fusion [67]
CVPR’24
384 × 1056-SwinTiny
3.2*
74.0
72.8
75.2
73.0
Image Backbone: Others
AutoAlignV2 [8]
ECCV’22
640 × 1280-CSPNet [51]
4.8*
71.2
67.1
72.4
68.4
UVTR [26]
NeurIPS’22
640 × 1280-ResNet101 [16]
1.8
70.2
65.4
71.1
67.1
FUTR3D [5]
CVPR’23
900 × 1600-VOVNet [23]
3.3*
68.0
64.2
72.1
69.4
UniTR [54]
ICCV’23
256 × 704-DSVT [53]
9.3*
73.3
70.5
74.5
70.9
CMT [60]
ICCV’23
640 × 1600-VOVNet
6.0*
72.9
70.3
74.1
72.0
UniPAD [63]
CVPR’24
900 × 1600-ConvNeXtS [34]
-
73.2
69.9
73.9
71.0
DH-Fusion-large (Ours)
-
384 × 1056-SwinTiny
5.7
74.4
72.3
75.4
72.8
DH-Fusion-light (Ours)
-
256 × 704-ResNet18
13.8
73.3
69.8
74.2
70.9"
ABLATION STUDIES,0.36642027455121434,Table 2: Robustness experiments on nuScenes-C. Numbers are NDS / mAP.
ABLATION STUDIES,0.36747624076029567,"Methods
Corruption
Average
None
Weather
Sensor
Motion
Object
Alignment
FUTR3D [5]
68.05 / 64.17
62.75 / 55.51 63.66 / 56.83 53.16 / 44.43 65.45 / 61.04 62.83 / 57.60 62.82↓5.23 / 56.99↓7.18"
ABLATION STUDIES,0.368532206969377,"TransFusion [1]
69.82 / 66.38
65.42 / 59.37 66.17 / 59.82 51.52 / 41.47 68.28 / 64.38 61.98 / 54.94 63.74↓6.08 / 58.73↓7.65"
ABLATION STUDIES,0.36958817317845827,"BEVFusion [33]
71.40 / 68.45
67.54 / 61.87 67.59 / 61.80 55.19 / 47.30 68.01 / 65.14 63.94 / 58.71 66.06↓5.34 / 61.03↓7.42"
ABLATION STUDIES,0.3706441393875396,"DH-Fusion-light (Ours) 73.30 / 69.75
72.19 / 67.48 69.16 / 62.87 57.07 / 47.52 71.01 / 67.11 67.24 / 62.38 68.67↓4.63 / 63.07↓6.68"
ABLATION STUDIES,0.3717001055966209,"Effect of DGF and DLF. To demonstrate the effect of DGF and DLF, we conduct experiments by
275"
ABLATION STUDIES,0.3727560718057022,"integrating the components one by one into the baseline, BEVFusion [33]. The results are shown
276"
ABLATION STUDIES,0.3738120380147835,"in Tab. 3. We find that our DGF improves the baseline performance by 1.0 pp w.r.t. NDS and 0.9
277"
ABLATION STUDIES,0.37486800422386485,"pp w.r.t. mAP. This demonstrates that dynamically adjusting the weights of the image BEV features
278"
ABLATION STUDIES,0.3759239704329461,"during fusion is effective for 3D object detection. Additionally, our DLF improves the baseline
279"
ABLATION STUDIES,0.37697993664202745,"performance by 1.3 pp w.r.t. NDS and 0.8 pp w.r.t. mAP, which indicates that dynamically adjusting
280"
ABLATION STUDIES,0.3780359028511088,"the weights of the local raw instance features based on depth during fusion effectively compensates
281"
ABLATION STUDIES,0.3790918690601901,"for the information loss caused by the transformation of global features into the BEV feature space.
282"
ABLATION STUDIES,0.3801478352692714,"The results of integrating both components show an improvement of 1.9 pp w.r.t. NDS and 1.3 pp
283"
ABLATION STUDIES,0.3812038014783527,"w.r.t. mAP, well verifying the benefits of dynamically fusing global and local hybrid features based
284"
ABLATION STUDIES,0.382259767687434,"on depth.
285"
ABLATION STUDIES,0.3833157338965153,"Effect of depth encoding in DGF and DLF. To evaluate the effectiveness of our depth encoding,
286"
ABLATION STUDIES,0.3843717001055966,"we conduct experiments where the depth encoding is removed from the DGF and DLF modules,
287"
ABLATION STUDIES,0.38542766631467795,"respectively. The results are shown in Tab. 4. When removing the depth encoding from Baseline+DGF,
288"
ABLATION STUDIES,0.3864836325237592,"the performance drops by 0.6 pp w.r.t. NDS and 0.4 pp w.r.t. mAP. Similarly, when removing the
289"
ABLATION STUDIES,0.38753959873284055,"depth encoding from Baseline+DLF, the performance also decreases by 1.1 pp w.r.t. NDS and 0.9 pp
290"
ABLATION STUDIES,0.3885955649419219,"w.r.t. mAP. These results indicate that our depth encoding is effective. Furthermore, we observe that
291"
ABLATION STUDIES,0.38965153115100315,"removing the depth encoding from the DLF module results in a larger performance drop, suggesting
292"
ABLATION STUDIES,0.3907074973600845,"that depth encoding plays a more crucial role in local feature fusion.
293"
ABLATION STUDIES,0.3917634635691658,"Impact of different operations for depth encoding. We conduct experiments with different
294"
ABLATION STUDIES,0.3928194297782471,"operations of depth encoding, including concatenation, summation, and multiplication. The results
295"
ABLATION STUDIES,0.3938753959873284,"in Tab. 5, show that the multiplication operation consistently outperforms the summation and
296"
ABLATION STUDIES,0.39493136219640973,"concatenation operations w.r.t. both metrics. The superior performance of multiplication can be
297"
ABLATION STUDIES,0.395987328405491,"attributed to its ability to more effectively modulate the feature maps based on depth information.
298"
ABLATION STUDIES,0.39704329461457233,"Unlike summation, which simply shifts the feature values, or concatenation, which increases the
299"
ABLATION STUDIES,0.39809926082365366,"dimensionality without direct interaction, multiplication allows for more interaction between the
300"
ABLATION STUDIES,0.3991552270327349,"Table 3: Ablation studies of each
proposed module."
ABLATION STUDIES,0.40021119324181625,"Baseline DGF DLF
NDS
mAP
!
71.4
68.5
!
!
72.4↑1.0 69.4↑0.9"
ABLATION STUDIES,0.4012671594508976,"!
!
72.7↑1.3 69.3↑0.8"
ABLATION STUDIES,0.40232312565997885,"!
!
!
73.3↑1.9 69.8↑1.3"
ABLATION STUDIES,0.4033790918690602,"Table 4: Ablation studies of depth
encoding (DE) in DGF and DLF."
ABLATION STUDIES,0.4044350580781415,"Methods
NDS
mAP
Baseline + DGF
72.4
69.4
w/o DE
71.8↓0.6 69.0↓0.4"
ABLATION STUDIES,0.40549102428722283,"Baseline + DLF
72.7
69.3
w/o DE
71.6↓1.1 68.4↓0.9"
ABLATION STUDIES,0.4065469904963041,"Table 5: Ablation studies
of different operations for
depth encoding."
ABLATION STUDIES,0.40760295670538543,"Methods
NDS mAP
Summation
72.8
69.2
Concatenation
72.5
68.7
Multiplication
73.3
69.8"
ABLATION STUDIES,0.40865892291446676,"(a) Attention weights
(b) Average map"
ABLATION STUDIES,0.40971488912354803,"Figure 5: Attention weights applied on
BEV image features in DGF vary with
depth."
ABLATION STUDIES,0.41077085533262936,"Point Cloud
BEV Feature"
ABLATION STUDIES,0.4118268215417107,BEVFusion
ABLATION STUDIES,0.41288278775079196,DH-Fusion
ABLATION STUDIES,0.4139387539598733,（Ours）
ABLATION STUDIES,0.4149947201689546,Left Front Image
ABLATION STUDIES,0.4160506863780359,"Figure 6: Qualitative detection results and BEV fea-
tures of BEVFusion and ours. We show the ground
truth boxes in green, and the prediction boxes in blue."
ABLATION STUDIES,0.4171066525871172,"depth encoding and features, leading to better feature representation and ultimately improving the
301"
ABLATION STUDIES,0.41816261879619854,"detection performance.
302"
QUALITATIVE RESULTS,0.4192185850052798,"4.5
Qualitative Results
303"
QUALITATIVE RESULTS,0.42027455121436114,"To better understand how depth encoding affects the feature fusion, in Fig. 5, we plot a curve to
304"
QUALITATIVE RESULTS,0.42133051742344246,"observe how the attention weights applied on the image BEV features in our DGF module vary with
305"
QUALITATIVE RESULTS,0.42238648363252373,"depth, and visualize the average attention map. It is evident that the weights of the image BEV
306"
QUALITATIVE RESULTS,0.42344244984160506,"features stay low in near range, but go up significantly as depth increases when the depth is larger
307"
QUALITATIVE RESULTS,0.4244984160506864,"than 40 meters. This trend supports our hypothesis that the image modality would become more
308"
QUALITATIVE RESULTS,0.42555438225976766,"important as depth increases. In this way, our depth encoding allows the model to dynamically adjust
309"
QUALITATIVE RESULTS,0.426610348468849,"the weights of image BEV features based on depth.
310"
QUALITATIVE RESULTS,0.4276663146779303,"We also compare the detection results of our DH-Fusion method with the baseline BEVFusion [33]
311"
QUALITATIVE RESULTS,0.42872228088701164,"in Fig. 6, where we clearly find that our method better localizes those distant objects compared to
312"
QUALITATIVE RESULTS,0.4297782470960929,"BEVFusion. These results demonstrate that our proposed multi-modal fusion strategy based on depth
313"
QUALITATIVE RESULTS,0.43083421330517424,"is more effective for detection. Besides, we exhibit the corresponding BEV feature maps, where
314"
QUALITATIVE RESULTS,0.43189017951425557,"our method shows a stronger feature response for the foreground objects, especially for distant ones.
315"
QUALITATIVE RESULTS,0.43294614572333684,"That is why our feature fusion strategy can provide higher-quality detection results. More qualitative
316"
QUALITATIVE RESULTS,0.43400211193241817,"results can be found in Appendix A.3.
317"
CONCLUSION,0.4350580781414995,"5
Conclusion
318"
CONCLUSION,0.43611404435058077,"In this paper, we for the first time point out that different modalities play different roles as depth varies
319"
CONCLUSION,0.4371700105596621,"via statistical analysis and visualization. Based on this finding, we propose a feature fusion strategy
320"
CONCLUSION,0.4382259767687434,"for multi-modal 3D object detection, namely Depth-Aware Hybrid Feature Fusion (DH-Fusion), that
321"
CONCLUSION,0.4392819429778247,"dynamically adjusts the weights of features during feature fusion by introducing depth encoding at
322"
CONCLUSION,0.440337909186906,"both global and local levels. Extensive experiments on the nuScenes dataset demonstrate that our
323"
CONCLUSION,0.44139387539598735,"DH-Fusion method surpasses previous state-of-the-art methods w.r.t. NDS. Moreover, our DH-Fusion
324"
CONCLUSION,0.4424498416050686,"is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C
325"
CONCLUSION,0.44350580781414994,"dataset w.r.t. both NDS and mAP. Our method uses an attention-based approach to interact with
326"
CONCLUSION,0.44456177402323127,"the two modalities, making the detection results sensitive to modality loss. We plan to further
327"
CONCLUSION,0.44561774023231254,"explore feature fusion methods that are robust to modality loss. Although our method improves
328"
CONCLUSION,0.44667370644139387,"detection performance, emergency plans still need to be implemented in practical applications to
329"
CONCLUSION,0.4477296726504752,"ensure personnel safety.
330"
REFERENCES,0.44878563885955647,"References
331"
REFERENCES,0.4498416050686378,"[1] Bai, X., Hu, Z., Zhu, X., Huang, Q., Chen, Y., Fu, H., Tai, C.L.: Transfusion: Robust lidar-
332"
REFERENCES,0.4508975712777191,"camera fusion for 3d object detection with transformers. In: CVPR (2022)
333"
REFERENCES,0.45195353748680045,"[2] Brazil, G., Liu, X.: M3d-rpn: Monocular 3d region proposal network for object detection. In:
334"
REFERENCES,0.4530095036958817,"ICCV (2019)
335"
REFERENCES,0.45406546990496305,"[3] Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A., Pan, Y., Baldan,
336"
REFERENCES,0.4551214361140444,"G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous driving. In: CVPR (2020)
337"
REFERENCES,0.45617740232312565,"[4] Cai, Q., Pan, Y., Yao, T., Ngo, C.W., Mei, T.: Objectfusion: Multi-modal 3d object detection
338"
REFERENCES,0.457233368532207,"with object-centric fusion. In: ICCV (2023)
339"
REFERENCES,0.4582893347412883,"[5] Chen, X., Zhang, T., Wang, Y., Wang, Y., Zhao, H.: Futr3d: A unified sensor fusion framework
340"
REFERENCES,0.4593453009503696,"for 3d detection. In: CVPR (2023)
341"
REFERENCES,0.4604012671594509,"[6] Chen, Y., Liu, S., Shen, X., Jia, J.: Dsgn: Deep stereo geometry network for 3d object detection.
342"
REFERENCES,0.4614572333685322,"In: CVPR (2020)
343"
REFERENCES,0.4625131995776135,"[7] Chen, Y., Yu, Z., Chen, Y., Lan, S., Anandkumar, A., Jia, J., Alvarez, J.M.: Focalformer3d:
344"
REFERENCES,0.4635691657866948,"focusing on hard instance for 3d object detection. In: ICCV (2023)
345"
REFERENCES,0.46462513199577615,"[8] Chen, Z., Li, Z., Zhang, S., Fang, L., Jiang, Q., Zhao, F.: Deformable feature aggregation for
346"
REFERENCES,0.4656810982048574,"dynamic multi-modal 3d object detection. In: ECCV (2022)
347"
REFERENCES,0.46673706441393875,"[9] Chiu, H.k., Prioletti, A., Li, J., Bohg, J.: Probabilistic 3d multi-object tracking for autonomous
348"
REFERENCES,0.4677930306230201,"driving. arxiv 2020. arXiv preprint arXiv:2001.05673 (2020)
349"
REFERENCES,0.46884899683210135,"[10] Contributors, M.: MMDetection3D: OpenMMLab next-generation platform for general 3D
350"
REFERENCES,0.4699049630411827,"object detection. https://github.com/open-mmlab/mmdetection3d (2020)
351"
REFERENCES,0.470960929250264,"[11] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical
352"
REFERENCES,0.4720168954593453,"image database. In: CVPR (2009)
353"
REFERENCES,0.4730728616684266,"[12] Deng, J., Shi, S., Li, P., Zhou, W., Zhang, Y., Li, H.: Voxel r-cnn: Towards high performance
354"
REFERENCES,0.47412882787750793,"voxel-based 3d object detection. In: AAAI (2021)
355"
REFERENCES,0.4751847940865892,"[13] Dong, Y., Kang, C., Zhang, J., Zhu, Z., Wang, Y., Yang, X., Su, H., Wei, X., Zhu, J.: Bench-
356"
REFERENCES,0.47624076029567053,"marking robustness of 3d object detection to common corruptions. In: CVPR (2023)
357"
REFERENCES,0.47729672650475186,"[14] Graham, B., Engelcke, M., Van Der Maaten, L.: 3d semantic segmentation with submanifold
358"
REFERENCES,0.4783526927138332,"sparse convolutional networks. In: CVPR (2018)
359"
REFERENCES,0.47940865892291445,"[15] He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn. In: CVPR (2017)
360"
REFERENCES,0.4804646251319958,"[16] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR
361"
REFERENCES,0.4815205913410771,"(2016)
362"
REFERENCES,0.4825765575501584,"[17] Hu, J.S., Kuai, T., Waslander, S.L.: Point density-aware voxels for lidar 3d object detection. In:
363"
REFERENCES,0.4836325237592397,"CVPR (2022)
364"
REFERENCES,0.48468848996832103,"[18] Huang, J., Huang, G.: Bevpoolv2: A cutting-edge implementation of bevdet toward deployment.
365"
REFERENCES,0.4857444561774023,"arXiv:2211.17111 (2022)
366"
REFERENCES,0.48680042238648363,"[19] Huang, J., Huang, G., Zhu, Z., Ye, Y., Du, D.: Bevdet: High-performance multi-camera 3d
367"
REFERENCES,0.48785638859556496,"object detection in bird-eye-view. arXiv:2112.11790 (2021)
368"
REFERENCES,0.48891235480464623,"[20] Huang, J., Ye, Y., Liang, Z., Shan, Y., Du, D.: Detecting as labeling: Rethinking lidar-camera
369"
REFERENCES,0.48996832101372756,"fusion in 3d object detection. arXiv arXiv:2311.07152 (2023)
370"
REFERENCES,0.4910242872228089,"[21] Jiao, Y., Jie, Z., Chen, S., Chen, J., Ma, L., Jiang, Y.G.: Msmdfusion: Fusing lidar and camera
371"
REFERENCES,0.49208025343189016,"at multiple scales with multi-depth seeds for 3d object detection. In: CVPR (2023)
372"
REFERENCES,0.4931362196409715,"[22] Lang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars: Fast encoders
373"
REFERENCES,0.4941921858500528,"for object detection from point clouds. In: CVPR (2019)
374"
REFERENCES,0.4952481520591341,"[23] Lee, Y., Hwang, J.w., Lee, S., Bae, Y., Park, J.: An energy and gpu-computation efficient
375"
REFERENCES,0.4963041182682154,"backbone network for real-time object detection. In: CVPR workshops (2019)
376"
REFERENCES,0.49736008447729674,"[24] Li, B., Ouyang, W., Sheng, L., Zeng, X., Wang, X.: Gs3d: An efficient 3d object detection
377"
REFERENCES,0.498416050686378,"framework for autonomous driving. In: CVPR (2019)
378"
REFERENCES,0.49947201689545934,"[25] Li, P., Chen, X., Shen, S.: Stereo r-cnn based 3d object detection for autonomous driving. In:
379"
REFERENCES,0.5005279831045406,"CVPR (2019)
380"
REFERENCES,0.5015839493136219,"[26] Li, Y., Chen, Y., Qi, X., Li, Z., Sun, J., Jia, J.: Unifying voxel-based representation with
381"
REFERENCES,0.5026399155227033,"transformer for 3d object detection. In: NeurIPS (2022)
382"
REFERENCES,0.5036958817317846,"[27] Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Qiao, Y., Dai, J.: Bevformer: Learning
383"
REFERENCES,0.5047518479408659,"bird’s-eye-view representation from multi-camera images via spatiotemporal transformers. In:
384"
REFERENCES,0.5058078141499472,"ECCV (2022)
385"
REFERENCES,0.5068637803590285,"[28] Liang, T., Xie, H., Yu, K., Xia, Z., Lin, Z., Wang, Y., Tang, T., Wang, B., Tang, Z.: Bevfusion:
386"
REFERENCES,0.5079197465681098,"A simple and robust lidar-camera fusion framework. In: NeurIPS (2022)
387"
REFERENCES,0.5089757127771911,"[29] Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks
388"
REFERENCES,0.5100316789862724,"for object detection. In: CVPR (2017)
389"
REFERENCES,0.5110876451953538,"[30] Liu, Y., Wang, L., Liu, M.: Yolostereo3d: A step back to 2d for efficient stereo 3d detection. In:
390"
REFERENCES,0.5121436114044351,"ICRA. IEEE (2021)
391"
REFERENCES,0.5131995776135164,"[31] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer:
392"
REFERENCES,0.5142555438225976,"Hierarchical vision transformer using shifted windows. In: ICCV (2021)
393"
REFERENCES,0.515311510031679,"[32] Liu, Z., Wu, Z., Tóth, R.: Smoke: Single-stage monocular 3d object detection via keypoint
394"
REFERENCES,0.5163674762407603,"estimation. In: CVPR (2020)
395"
REFERENCES,0.5174234424498416,"[33] Liu, Z., Tang, H., Amini, A., Yang, X., Mao, H., Rus, D.L., Han, S.: Bevfusion: Multi-task
396"
REFERENCES,0.518479408658923,"multi-sensor fusion with unified bird’s-eye view representation. In: ICRA (2023)
397"
REFERENCES,0.5195353748680043,"[34] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In:
398"
REFERENCES,0.5205913410770855,"CVPR (2022)
399"
REFERENCES,0.5216473072861668,"[35] Loshchilov, I.,
Hutter,
F.:
Decoupled weight decay regularization. arXiv preprint
400"
REFERENCES,0.5227032734952481,"arXiv:1711.05101 (2017)
401"
REFERENCES,0.5237592397043295,"[36] Mao, J., Shi, S., Wang, X., Li, H.: 3d object detection for autonomous driving: A comprehensive
402"
REFERENCES,0.5248152059134108,"survey. IJCV (2023)
403"
REFERENCES,0.5258711721224921,"[37] Pang, S., Morris, D., Radha, H.: Clocs: Camera-lidar object candidates fusion for 3d object
404"
REFERENCES,0.5269271383315733,"detection. In: IROS (2020)
405"
REFERENCES,0.5279831045406547,"[38] Pang, S., Morris, D., Radha, H.: Fast-clocs: Fast camera-lidar object candidates fusion for 3d
406"
REFERENCES,0.529039070749736,"object detection. In: WACV (2022)
407"
REFERENCES,0.5300950369588173,"[39] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
408"
REFERENCES,0.5311510031678986,"Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning
409"
REFERENCES,0.53220696937698,"library. In: NeurIPS (2019)
410"
REFERENCES,0.5332629355860612,"[40] Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learning on point
411"
REFERENCES,0.5343189017951425,"sets in a metric space. In: NeurIPS (2017)
412"
REFERENCES,0.5353748680042238,"[41] Qin, Z., Wang, J., Lu, Y.: Monogrnet: A geometric reasoning network for monocular 3d object
413"
REFERENCES,0.5364308342133052,"localization. In: AAAI (2019)
414"
REFERENCES,0.5374868004223865,"[42] Shi, S., Guo, C., Jiang, L., Wang, Z., Shi, J., Wang, X., Li, H.: Pv-rcnn: Point-voxel feature set
415"
ABSTRACT,0.5385427666314678,"abstraction for 3d object detection. In: CVPR (2020)
416"
ABSTRACT,0.5395987328405492,"[43] Shi, S., Jiang, L., Deng, J., Wang, Z., Guo, C., Shi, J., Wang, X., Li, H.: Pv-rcnn++: Point-voxel
417"
ABSTRACT,0.5406546990496304,"feature set abstraction with local vector representation for 3d object detection. IJCV (2022)
418"
ABSTRACT,0.5417106652587117,"[44] Shi, S., Wang, X., Li, H.: Pointrcnn: 3d object proposal generation and detection from point
419"
ABSTRACT,0.542766631467793,"cloud. In: CVPR (2019)
420"
ABSTRACT,0.5438225976768744,"[45] Shi, S., Wang, Z., Shi, J., Wang, X., Li, H.: From points to parts: 3d object detection from point
421"
ABSTRACT,0.5448785638859557,"cloud with part-aware and part-aggregation network. IEEE TPAMI (2020)
422"
ABSTRACT,0.545934530095037,"[46] Shi, W., Rajkumar, R.: Point-gnn: Graph neural network for 3d object detection in a point cloud.
423"
ABSTRACT,0.5469904963041182,"In: CVPR (2020)
424"
ABSTRACT,0.5480464625131996,"[47] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposi-
425"
ABSTRACT,0.5491024287222809,"tion for monocular 3d object detection. In: ICCV (2021)
426"
ABSTRACT,0.5501583949313622,"[48] Sun, J., Chen, L., Xie, Y., Zhang, S., Jiang, Q., Zhou, X., Bao, H.: Disp r-cnn: Stereo 3d object
427"
ABSTRACT,0.5512143611404435,"detection via shape prior guided instance disparity estimation. In: CVPR (2020)
428"
ABSTRACT,0.5522703273495249,"[49] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł.,
429"
ABSTRACT,0.5533262935586061,"Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)
430"
ABSTRACT,0.5543822597676874,"[50] Vora, S., Lang, A.H., Helou, B., Beijbom, O.: Pointpainting: Sequential fusion for 3d object
431"
ABSTRACT,0.5554382259767687,"detection. In: CVPR (2020)
432"
ABSTRACT,0.5564941921858501,"[51] Wang, C.Y., Liao, H.Y.M., Wu, Y.H., Chen, P.Y., Hsieh, J.W., Yeh, I.H.: Cspnet: A new
433"
ABSTRACT,0.5575501583949314,"backbone that can enhance learning capability of cnn. In: CVPR workshops (2020)
434"
ABSTRACT,0.5586061246040127,"[52] Wang, C., Ma, C., Zhu, M., Yang, X.: Pointaugmenting: Cross-modal augmentation for 3d
435"
ABSTRACT,0.5596620908130939,"object detection. In: CVPR (2021)
436"
ABSTRACT,0.5607180570221753,"[53] Wang, H., Shi, C., Shi, S., Lei, M., Wang, S., He, D., Schiele, B., Wang, L.: Dsvt: Dynamic
437"
ABSTRACT,0.5617740232312566,"sparse voxel transformer with rotated sets. In: CVPR (2023)
438"
ABSTRACT,0.5628299894403379,"[54] Wang, H., Tang, H., Shi, S., Li, A., Li, Z., Schiele, B., Wang, L.: Unitr: A unified and efficient
439"
ABSTRACT,0.5638859556494192,"multi-modal transformer for bird’s-eye-view representation. In: ICCV (2023)
440"
ABSTRACT,0.5649419218585006,"[55] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d
441"
ABSTRACT,0.5659978880675819,"object detection. In: ICCV (2021)
442"
ABSTRACT,0.5670538542766631,"[56] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object
443"
ABSTRACT,0.5681098204857444,"detection from multi-view images via 3d-to-2d queries. In: Robot Learning (2022)
444"
ABSTRACT,0.5691657866948258,"[57] Wu, H., Wen, C., Shi, S., Li, X., Wang, C.: Virtual sparse convolution for multimodal 3d object
445"
ABSTRACT,0.5702217529039071,"detection. In: CVPR (2023)
446"
ABSTRACT,0.5712777191129884,"[58] Xie, Y., Xu, C., Rakotosaona, M.J., Rim, P., Tombari, F., Keutzer, K., Tomizuka, M., Zhan, W.:
447"
ABSTRACT,0.5723336853220697,"Sparsefusion: Fusing multi-modal sparse representations for multi-sensor 3d object detection.
448"
ABSTRACT,0.573389651531151,"In: ICCV (2023)
449"
ABSTRACT,0.5744456177402323,"[59] Xu, S., Zhou, D., Fang, J., Yin, J., Bin, Z., Zhang, L.: Fusionpainting: Multimodal fusion with
450"
ABSTRACT,0.5755015839493136,"adaptive attention for 3d object detection. In: ITSC (2021)
451"
ABSTRACT,0.5765575501583949,"[60] Yan, J., Liu, Y., Sun, J., Jia, F., Li, S., Wang, T., Zhang, X.: Cross modal transformer via
452"
ABSTRACT,0.5776135163674763,"coordinates encoding for 3d object dectection. In: ICCV (2023)
453"
ABSTRACT,0.5786694825765576,"[61] Yan, Y., Mao, Y., Li, B.: Second: Sparsely embedded convolutional detection. Sensors (2018)
454"
ABSTRACT,0.5797254487856388,"[62] Yang, C., Chen, Y., Tian, H., Tao, C., Zhu, X., Zhang, Z., Huang, G., Li, H., Qiao, Y., Lu, L.,
455"
ABSTRACT,0.5807814149947201,"et al.: Bevformer v2: Adapting modern image backbones to bird’s-eye-view recognition via
456"
ABSTRACT,0.5818373812038015,"perspective supervision. In: CVPR (2023)
457"
ABSTRACT,0.5828933474128828,"[63] Yang, H., Zhang, S., Huang, D., Wu, X., Zhu, H., He, T., Tang, S., Zhao, H., Qiu, Q., Lin, B.,
458"
ABSTRACT,0.5839493136219641,"He, X., Ouyang, W.: Unipad: A universal pre-training paradigm for autonomous driving. In:
459"
ABSTRACT,0.5850052798310454,"CVPR (2024)
460"
ABSTRACT,0.5860612460401268,"[64] Yang, Z., Sun, Y., Liu, S., Shen, X., Jia, J.: Ipod: Intensive point-based object detector for point
461"
ABSTRACT,0.587117212249208,"cloud. arXiv:1812.05276 (2018)
462"
ABSTRACT,0.5881731784582893,"[65] Yang, Z., Sun, Y., Liu, S., Shen, X., Jia, J.: Std: Sparse-to-dense 3d object detector for point
463"
ABSTRACT,0.5892291446673706,"cloud. In: ICCV (2019)
464"
ABSTRACT,0.590285110876452,"[66] Yang, Z., Chen, J., Miao, Z., Li, W., Zhu, X., Zhang, L.: Deepinteraction: 3d object detection
465"
ABSTRACT,0.5913410770855333,"via modality interaction. In: NeurIPS (2022)
466"
ABSTRACT,0.5923970432946146,"[67] Yin, J., Shen, J., Chen, R., Li, W., Yang, R., Frossard, P., Wang, W.: Is-fusion: Instance-scene
467"
ABSTRACT,0.5934530095036958,"collaborative fusion for multimodal 3d object detection. In: CVPR (2024)
468"
ABSTRACT,0.5945089757127772,"[68] Yin, T., Zhou, X., Krahenbuhl, P.: Center-based 3d object detection and tracking. In: CVPR
469"
ABSTRACT,0.5955649419218585,"(2021)
470"
ABSTRACT,0.5966209081309398,"[69] Yin, T., Zhou, X., Krähenbühl, P.: Multimodal virtual point 3d detection. In: NeurIPS (2021)
471"
ABSTRACT,0.5976768743400211,"[70] You, Y., Wang, Y., Chao, W.L., Garg, D., Pleiss, G., Hariharan, B., Campbell, M., Wein-
472"
ABSTRACT,0.5987328405491025,"berger, K.Q.: Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving.
473"
ABSTRACT,0.5997888067581837,"arXiv:1906.06310 (2019)
474"
ABSTRACT,0.600844772967265,"[71] Zhou, Y., Tuzel, O.: Voxelnet: End-to-end learning for point cloud based 3d object detection.
475"
ABSTRACT,0.6019007391763463,"In: CVPR (2018)
476"
ABSTRACT,0.6029567053854277,"[72] Zhu, B., Jiang, Z., Zhou, X., Li, Z., Yu, G.: Class-balanced grouping and sampling for point
477"
ABSTRACT,0.604012671594509,"cloud 3d object detection. arXiv preprint arXiv:1908.09492 (2019)
478"
ABSTRACT,0.6050686378035903,"[73] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers
479"
ABSTRACT,0.6061246040126715,"for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020)
480"
ABSTRACT,0.6071805702217529,"NeurIPS Paper Checklist
481"
CLAIMS,0.6082365364308342,"1. Claims
482"
CLAIMS,0.6092925026399155,"Question: Do the main claims made in the abstract and introduction accurately reflect the
483"
CLAIMS,0.6103484688489969,"paper’s contributions and scope?
484"
CLAIMS,0.6114044350580782,"Answer: [Yes]
485"
CLAIMS,0.6124604012671595,"Justification: The main claims made in the abstract and introduction accurately reflect the
486"
CLAIMS,0.6135163674762407,"paper’s contributions and scope. The claims are clearly stated and are consistent with the
487"
CLAIMS,0.614572333685322,"theoretical and experimental results presented in the paper.
488"
CLAIMS,0.6156282998944034,"Guidelines:
489"
CLAIMS,0.6166842661034847,"• The answer NA means that the abstract and introduction do not include the claims
490"
CLAIMS,0.617740232312566,"made in the paper.
491"
CLAIMS,0.6187961985216474,"• The abstract and/or introduction should clearly state the claims made, including the
492"
CLAIMS,0.6198521647307286,"contributions made in the paper and important assumptions and limitations. A No or
493"
CLAIMS,0.6209081309398099,"NA answer to this question will not be perceived well by the reviewers.
494"
CLAIMS,0.6219640971488912,"• The claims made should match theoretical and experimental results, and reflect how
495"
CLAIMS,0.6230200633579726,"much the results can be expected to generalize to other settings.
496"
CLAIMS,0.6240760295670539,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
497"
CLAIMS,0.6251319957761352,"are not attained by the paper.
498"
LIMITATIONS,0.6261879619852164,"2. Limitations
499"
LIMITATIONS,0.6272439281942978,"Question: Does the paper discuss the limitations of the work performed by the authors?
500"
LIMITATIONS,0.6282998944033791,"Answer: [Yes]
501"
LIMITATIONS,0.6293558606124604,"Justification: We discuss the limitations of our method, specifically that using an attention-
502"
LIMITATIONS,0.6304118268215417,"based approach to interact with the two modalities makes the detection results sensitive to
503"
LIMITATIONS,0.6314677930306231,"modality loss.
504"
LIMITATIONS,0.6325237592397043,"Guidelines:
505"
LIMITATIONS,0.6335797254487856,"• The answer NA means that the paper has no limitation while the answer No means that
506"
LIMITATIONS,0.6346356916578669,"the paper has limitations, but those are not discussed in the paper.
507"
LIMITATIONS,0.6356916578669483,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
508"
LIMITATIONS,0.6367476240760296,"• The paper should point out any strong assumptions and how robust the results are to
509"
LIMITATIONS,0.6378035902851109,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
510"
LIMITATIONS,0.6388595564941922,"model well-specification, asymptotic approximations only holding locally). The authors
511"
LIMITATIONS,0.6399155227032735,"should reflect on how these assumptions might be violated in practice and what the
512"
LIMITATIONS,0.6409714889123548,"implications would be.
513"
LIMITATIONS,0.6420274551214361,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
514"
LIMITATIONS,0.6430834213305174,"only tested on a few datasets or with a few runs. In general, empirical results often
515"
LIMITATIONS,0.6441393875395988,"depend on implicit assumptions, which should be articulated.
516"
LIMITATIONS,0.6451953537486801,"• The authors should reflect on the factors that influence the performance of the approach.
517"
LIMITATIONS,0.6462513199577613,"For example, a facial recognition algorithm may perform poorly when image resolution
518"
LIMITATIONS,0.6473072861668426,"is low or images are taken in low lighting. Or a speech-to-text system might not be
519"
LIMITATIONS,0.648363252375924,"used reliably to provide closed captions for online lectures because it fails to handle
520"
LIMITATIONS,0.6494192185850053,"technical jargon.
521"
LIMITATIONS,0.6504751847940866,"• The authors should discuss the computational efficiency of the proposed algorithms
522"
LIMITATIONS,0.6515311510031679,"and how they scale with dataset size.
523"
LIMITATIONS,0.6525871172122492,"• If applicable, the authors should discuss possible limitations of their approach to
524"
LIMITATIONS,0.6536430834213305,"address problems of privacy and fairness.
525"
LIMITATIONS,0.6546990496304118,"• While the authors might fear that complete honesty about limitations might be used by
526"
LIMITATIONS,0.6557550158394931,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
527"
LIMITATIONS,0.6568109820485745,"limitations that aren’t acknowledged in the paper. The authors should use their best
528"
LIMITATIONS,0.6578669482576558,"judgment and recognize that individual actions in favor of transparency play an impor-
529"
LIMITATIONS,0.6589229144667371,"tant role in developing norms that preserve the integrity of the community. Reviewers
530"
LIMITATIONS,0.6599788806758183,"will be specifically instructed to not penalize honesty concerning limitations.
531"
THEORY ASSUMPTIONS AND PROOFS,0.6610348468848997,"3. Theory Assumptions and Proofs
532"
THEORY ASSUMPTIONS AND PROOFS,0.662090813093981,"Question: For each theoretical result, does the paper provide the full set of assumptions and
533"
THEORY ASSUMPTIONS AND PROOFS,0.6631467793030623,"a complete (and correct) proof?
534"
THEORY ASSUMPTIONS AND PROOFS,0.6642027455121436,"Answer: [Yes]
535"
THEORY ASSUMPTIONS AND PROOFS,0.665258711721225,"Justification: We provide detailed theoretical statements and formulas along with their
536"
THEORY ASSUMPTIONS AND PROOFS,0.6663146779303062,"descriptions in the paper.
537"
THEORY ASSUMPTIONS AND PROOFS,0.6673706441393875,"Guidelines:
538"
THEORY ASSUMPTIONS AND PROOFS,0.6684266103484688,"• The answer NA means that the paper does not include theoretical results.
539"
THEORY ASSUMPTIONS AND PROOFS,0.6694825765575502,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
540"
THEORY ASSUMPTIONS AND PROOFS,0.6705385427666315,"referenced.
541"
THEORY ASSUMPTIONS AND PROOFS,0.6715945089757128,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
542"
THEORY ASSUMPTIONS AND PROOFS,0.672650475184794,"• The proofs can either appear in the main paper or the supplemental material, but if
543"
THEORY ASSUMPTIONS AND PROOFS,0.6737064413938754,"they appear in the supplemental material, the authors are encouraged to provide a short
544"
THEORY ASSUMPTIONS AND PROOFS,0.6747624076029567,"proof sketch to provide intuition.
545"
THEORY ASSUMPTIONS AND PROOFS,0.675818373812038,"• Inversely, any informal proof provided in the core of the paper should be complemented
546"
THEORY ASSUMPTIONS AND PROOFS,0.6768743400211193,"by formal proofs provided in appendix or supplemental material.
547"
THEORY ASSUMPTIONS AND PROOFS,0.6779303062302007,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6789862724392819,"4. Experimental Result Reproducibility
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6800422386483632,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6810982048574445,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6821541710665259,"of the paper (regardless of whether the code and data are provided or not)?
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6832101372756072,"Answer: [Yes]
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6842661034846885,"Justification: We provide a detailed experimental setup in the paper, and the training and
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6853220696937699,"testing details are provided in the supplementary material to ensure the reproducibility of
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6863780359028511,"our results.
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6874340021119324,"Guidelines:
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6884899683210137,"• The answer NA means that the paper does not include experiments.
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.689545934530095,"• If the paper includes experiments, a No answer to this question will not be perceived
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6906019007391764,"well by the reviewers: Making the paper reproducible is important, regardless of
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6916578669482577,"whether the code and data are provided or not.
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6927138331573389,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6937697993664202,"to make their results reproducible or verifiable.
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6948257655755016,"• Depending on the contribution, reproducibility can be accomplished in various ways.
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6958817317845829,"For example, if the contribution is a novel architecture, describing the architecture fully
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6969376979936642,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6979936642027456,"be necessary to either make it possible for others to replicate the model with the same
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6990496304118268,"dataset, or provide access to the model. In general. releasing code and data is often
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7001055966209081,"one good way to accomplish this, but reproducibility can also be provided via detailed
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7011615628299894,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7022175290390708,"of a large language model), releasing of a model checkpoint, or other means that are
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7032734952481521,"appropriate to the research performed.
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7043294614572334,"• While NeurIPS does not require releasing code, the conference does require all submis-
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7053854276663146,"sions to provide some reasonable avenue for reproducibility, which may depend on the
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.706441393875396,"nature of the contribution. For example
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7074973600844773,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7085533262935586,"to reproduce that algorithm.
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7096092925026399,"(b) If the contribution is primarily a new model architecture, the paper should describe
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7106652587117213,"the architecture clearly and fully.
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7117212249208026,"(c) If the contribution is a new model (e.g., a large language model), then there should
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7127771911298838,"either be a way to access this model for reproducing the results or a way to reproduce
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7138331573389651,"the model (e.g., with an open-source dataset or instructions for how to construct
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7148891235480465,"the dataset).
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7159450897571278,"(d) We recognize that reproducibility may be tricky in some cases, in which case
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7170010559662091,"authors are welcome to describe the particular way they provide for reproducibility.
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7180570221752904,"In the case of closed-source models, it may be that access to the model is limited in
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7191129883843717,"some way (e.g., to registered users), but it should be possible for other researchers
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.720168954593453,"to have some path to reproducing or verifying the results.
588"
OPEN ACCESS TO DATA AND CODE,0.7212249208025343,"5. Open access to data and code
589"
OPEN ACCESS TO DATA AND CODE,0.7222808870116156,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
590"
OPEN ACCESS TO DATA AND CODE,0.723336853220697,"tions to faithfully reproduce the main experimental results, as described in supplemental
591"
OPEN ACCESS TO DATA AND CODE,0.7243928194297783,"material?
592"
OPEN ACCESS TO DATA AND CODE,0.7254487856388595,"Answer: [No]
593"
OPEN ACCESS TO DATA AND CODE,0.7265047518479408,"Justification: We release the experimental details in the paper, and the code will be released
594"
OPEN ACCESS TO DATA AND CODE,0.7275607180570222,"after the paper is accepted.
595"
OPEN ACCESS TO DATA AND CODE,0.7286166842661035,"Guidelines:
596"
OPEN ACCESS TO DATA AND CODE,0.7296726504751848,"• The answer NA means that paper does not include experiments requiring code.
597"
OPEN ACCESS TO DATA AND CODE,0.7307286166842661,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
598"
OPEN ACCESS TO DATA AND CODE,0.7317845828933475,"public/guides/CodeSubmissionPolicy) for more details.
599"
OPEN ACCESS TO DATA AND CODE,0.7328405491024287,"• While we encourage the release of code and data, we understand that this might not be
600"
OPEN ACCESS TO DATA AND CODE,0.73389651531151,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
601"
OPEN ACCESS TO DATA AND CODE,0.7349524815205913,"including code, unless this is central to the contribution (e.g., for a new open-source
602"
OPEN ACCESS TO DATA AND CODE,0.7360084477296727,"benchmark).
603"
OPEN ACCESS TO DATA AND CODE,0.737064413938754,"• The instructions should contain the exact command and environment needed to run to
604"
OPEN ACCESS TO DATA AND CODE,0.7381203801478353,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
605"
OPEN ACCESS TO DATA AND CODE,0.7391763463569165,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
606"
OPEN ACCESS TO DATA AND CODE,0.7402323125659979,"• The authors should provide instructions on data access and preparation, including how
607"
OPEN ACCESS TO DATA AND CODE,0.7412882787750792,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
608"
OPEN ACCESS TO DATA AND CODE,0.7423442449841605,"• The authors should provide scripts to reproduce all experimental results for the new
609"
OPEN ACCESS TO DATA AND CODE,0.7434002111932418,"proposed method and baselines. If only a subset of experiments are reproducible, they
610"
OPEN ACCESS TO DATA AND CODE,0.7444561774023232,"should state which ones are omitted from the script and why.
611"
OPEN ACCESS TO DATA AND CODE,0.7455121436114044,"• At submission time, to preserve anonymity, the authors should release anonymized
612"
OPEN ACCESS TO DATA AND CODE,0.7465681098204857,"versions (if applicable).
613"
OPEN ACCESS TO DATA AND CODE,0.747624076029567,"• Providing as much information as possible in supplemental material (appended to the
614"
OPEN ACCESS TO DATA AND CODE,0.7486800422386484,"paper) is recommended, but including URLs to data and code is permitted.
615"
OPEN ACCESS TO DATA AND CODE,0.7497360084477297,"6. Experimental Setting/Details
616"
OPEN ACCESS TO DATA AND CODE,0.750791974656811,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
617"
OPEN ACCESS TO DATA AND CODE,0.7518479408658922,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
618"
OPEN ACCESS TO DATA AND CODE,0.7529039070749736,"results?
619"
OPEN ACCESS TO DATA AND CODE,0.7539598732840549,"Answer: [Yes]
620"
OPEN ACCESS TO DATA AND CODE,0.7550158394931362,"Justification: We provide a detailed experimental setup in the paper, and the training and
621"
OPEN ACCESS TO DATA AND CODE,0.7560718057022175,"testing details are provided in the supplementary material.
622"
OPEN ACCESS TO DATA AND CODE,0.7571277719112989,"Guidelines:
623"
OPEN ACCESS TO DATA AND CODE,0.7581837381203802,"• The answer NA means that the paper does not include experiments.
624"
OPEN ACCESS TO DATA AND CODE,0.7592397043294614,"• The experimental setting should be presented in the core of the paper to a level of detail
625"
OPEN ACCESS TO DATA AND CODE,0.7602956705385427,"that is necessary to appreciate the results and make sense of them.
626"
OPEN ACCESS TO DATA AND CODE,0.7613516367476241,"• The full details can be provided either with the code, in appendix, or as supplemental
627"
OPEN ACCESS TO DATA AND CODE,0.7624076029567054,"material.
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7634635691657867,"7. Experiment Statistical Significance
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.764519535374868,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7655755015839493,"information about the statistical significance of the experiments?
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7666314677930306,"Answer: [Yes]
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7676874340021119,"Justification: We provide data explanations and statistical methods for obtaining statistical
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7687434002111933,"results in the paper.
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7697993664202746,"Guidelines:
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7708553326293559,"• The answer NA means that the paper does not include experiments.
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7719112988384371,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7729672650475184,"dence intervals, or statistical significance tests, at least for the experiments that support
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7740232312565998,"the main claims of the paper.
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7750791974656811,"• The factors of variability that the error bars are capturing should be clearly stated (for
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7761351636747624,"example, train/test split, initialization, random drawing of some parameter, or overall
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7771911298838438,"run with given experimental conditions).
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.778247096092925,"• The method for calculating the error bars should be explained (closed form formula,
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7793030623020063,"call to a library function, bootstrap, etc.)
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7803590285110876,"• The assumptions made should be given (e.g., Normally distributed errors).
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.781414994720169,"• It should be clear whether the error bar is the standard deviation or the standard error
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7824709609292503,"of the mean.
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7835269271383316,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7845828933474129,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7856388595564942,"of Normality of errors is not verified.
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7866948257655755,"• For asymmetric distributions, the authors should be careful not to show in tables or
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7877507919746568,"figures symmetric error bars that would yield results that are out of range (e.g. negative
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7888067581837381,"error rates).
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7898627243928195,"• If error bars are reported in tables or plots, The authors should explain in the text how
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7909186906019008,"they were calculated and reference the corresponding figures or tables in the text.
655"
EXPERIMENTS COMPUTE RESOURCES,0.791974656810982,"8. Experiments Compute Resources
656"
EXPERIMENTS COMPUTE RESOURCES,0.7930306230200633,"Question: For each experiment, does the paper provide sufficient information on the com-
657"
EXPERIMENTS COMPUTE RESOURCES,0.7940865892291447,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
658"
EXPERIMENTS COMPUTE RESOURCES,0.795142555438226,"the experiments?
659"
EXPERIMENTS COMPUTE RESOURCES,0.7961985216473073,"Answer: [Yes]
660"
EXPERIMENTS COMPUTE RESOURCES,0.7972544878563886,"Justification: We provide hardware computer resources for training and testing.
661"
EXPERIMENTS COMPUTE RESOURCES,0.7983104540654699,"Guidelines:
662"
EXPERIMENTS COMPUTE RESOURCES,0.7993664202745512,"• The answer NA means that the paper does not include experiments.
663"
EXPERIMENTS COMPUTE RESOURCES,0.8004223864836325,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
664"
EXPERIMENTS COMPUTE RESOURCES,0.8014783526927138,"or cloud provider, including relevant memory and storage.
665"
EXPERIMENTS COMPUTE RESOURCES,0.8025343189017952,"• The paper should provide the amount of compute required for each of the individual
666"
EXPERIMENTS COMPUTE RESOURCES,0.8035902851108765,"experimental runs as well as estimate the total compute.
667"
EXPERIMENTS COMPUTE RESOURCES,0.8046462513199577,"• The paper should disclose whether the full research project required more compute
668"
EXPERIMENTS COMPUTE RESOURCES,0.805702217529039,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
669"
EXPERIMENTS COMPUTE RESOURCES,0.8067581837381204,"didn’t make it into the paper).
670"
CODE OF ETHICS,0.8078141499472017,"9. Code Of Ethics
671"
CODE OF ETHICS,0.808870116156283,"Question: Does the research conducted in the paper conform, in every respect, with the
672"
CODE OF ETHICS,0.8099260823653643,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
673"
CODE OF ETHICS,0.8109820485744457,"Answer: [Yes]
674"
CODE OF ETHICS,0.8120380147835269,"Justification: The research conducted in our paper complies with NeurIPS ethical standards
675"
CODE OF ETHICS,0.8130939809926082,"in all aspects.
676"
CODE OF ETHICS,0.8141499472016895,"Guidelines:
677"
CODE OF ETHICS,0.8152059134107709,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
678"
CODE OF ETHICS,0.8162618796198522,"• If the authors answer No, they should explain the special circumstances that require a
679"
CODE OF ETHICS,0.8173178458289335,"deviation from the Code of Ethics.
680"
CODE OF ETHICS,0.8183738120380147,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
681"
CODE OF ETHICS,0.8194297782470961,"eration due to laws or regulations in their jurisdiction).
682"
BROADER IMPACTS,0.8204857444561774,"10. Broader Impacts
683"
BROADER IMPACTS,0.8215417106652587,"Question: Does the paper discuss both potential positive societal impacts and negative
684"
BROADER IMPACTS,0.82259767687434,"societal impacts of the work performed?
685"
BROADER IMPACTS,0.8236536430834214,"Answer: [Yes]
686"
BROADER IMPACTS,0.8247096092925026,"Justification: We discuss that although our method has good performance, practical applica-
687"
BROADER IMPACTS,0.8257655755015839,"tions need to ensure personnel safety.
688"
BROADER IMPACTS,0.8268215417106652,"Guidelines:
689"
BROADER IMPACTS,0.8278775079197466,"• The answer NA means that there is no societal impact of the work performed.
690"
BROADER IMPACTS,0.8289334741288279,"• If the authors answer NA or No, they should explain why their work has no societal
691"
BROADER IMPACTS,0.8299894403379092,"impact or why the paper does not address societal impact.
692"
BROADER IMPACTS,0.8310454065469906,"• Examples of negative societal impacts include potential malicious or unintended uses
693"
BROADER IMPACTS,0.8321013727560718,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
694"
BROADER IMPACTS,0.8331573389651531,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
695"
BROADER IMPACTS,0.8342133051742344,"groups), privacy considerations, and security considerations.
696"
BROADER IMPACTS,0.8352692713833157,"• The conference expects that many papers will be foundational research and not tied
697"
BROADER IMPACTS,0.8363252375923971,"to particular applications, let alone deployments. However, if there is a direct path to
698"
BROADER IMPACTS,0.8373812038014784,"any negative applications, the authors should point it out. For example, it is legitimate
699"
BROADER IMPACTS,0.8384371700105596,"to point out that an improvement in the quality of generative models could be used to
700"
BROADER IMPACTS,0.839493136219641,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
701"
BROADER IMPACTS,0.8405491024287223,"that a generic algorithm for optimizing neural networks could enable people to train
702"
BROADER IMPACTS,0.8416050686378036,"models that generate Deepfakes faster.
703"
BROADER IMPACTS,0.8426610348468849,"• The authors should consider possible harms that could arise when the technology is
704"
BROADER IMPACTS,0.8437170010559663,"being used as intended and functioning correctly, harms that could arise when the
705"
BROADER IMPACTS,0.8447729672650475,"technology is being used as intended but gives incorrect results, and harms following
706"
BROADER IMPACTS,0.8458289334741288,"from (intentional or unintentional) misuse of the technology.
707"
BROADER IMPACTS,0.8468848996832101,"• If there are negative societal impacts, the authors could also discuss possible mitigation
708"
BROADER IMPACTS,0.8479408658922915,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
709"
BROADER IMPACTS,0.8489968321013728,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
710"
BROADER IMPACTS,0.8500527983104541,"feedback over time, improving the efficiency and accessibility of ML).
711"
SAFEGUARDS,0.8511087645195353,"11. Safeguards
712"
SAFEGUARDS,0.8521647307286166,"Question: Does the paper describe safeguards that have been put in place for responsible
713"
SAFEGUARDS,0.853220696937698,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
714"
SAFEGUARDS,0.8542766631467793,"image generators, or scraped datasets)?
715"
SAFEGUARDS,0.8553326293558606,"Answer: [NA]
716"
SAFEGUARDS,0.856388595564942,"Justification: The model of the paper dos not address the issues mentioned in the guidelines.
717"
SAFEGUARDS,0.8574445617740233,"Guidelines:
718"
SAFEGUARDS,0.8585005279831045,"• The answer NA means that the paper poses no such risks.
719"
SAFEGUARDS,0.8595564941921858,"• Released models that have a high risk for misuse or dual-use should be released with
720"
SAFEGUARDS,0.8606124604012672,"necessary safeguards to allow for controlled use of the model, for example by requiring
721"
SAFEGUARDS,0.8616684266103485,"that users adhere to usage guidelines or restrictions to access the model or implementing
722"
SAFEGUARDS,0.8627243928194298,"safety filters.
723"
SAFEGUARDS,0.8637803590285111,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
724"
SAFEGUARDS,0.8648363252375924,"should describe how they avoided releasing unsafe images.
725"
SAFEGUARDS,0.8658922914466737,"• We recognize that providing effective safeguards is challenging, and many papers do
726"
SAFEGUARDS,0.866948257655755,"not require this, but we encourage authors to take this into account and make a best
727"
SAFEGUARDS,0.8680042238648363,"faith effort.
728"
LICENSES FOR EXISTING ASSETS,0.8690601900739177,"12. Licenses for existing assets
729"
LICENSES FOR EXISTING ASSETS,0.870116156282999,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
730"
LICENSES FOR EXISTING ASSETS,0.8711721224920802,"the paper, properly credited and are the license and terms of use explicitly mentioned and
731"
LICENSES FOR EXISTING ASSETS,0.8722280887011615,"properly respected?
732"
LICENSES FOR EXISTING ASSETS,0.8732840549102429,"Answer: [Yes]
733"
LICENSES FOR EXISTING ASSETS,0.8743400211193242,"Justification: We have annotated the cited papers and datasets in our paper.
734"
LICENSES FOR EXISTING ASSETS,0.8753959873284055,"Guidelines:
735"
LICENSES FOR EXISTING ASSETS,0.8764519535374868,"• The answer NA means that the paper does not use existing assets.
736"
LICENSES FOR EXISTING ASSETS,0.877507919746568,"• The authors should cite the original paper that produced the code package or dataset.
737"
LICENSES FOR EXISTING ASSETS,0.8785638859556494,"• The authors should state which version of the asset is used and, if possible, include a
738"
LICENSES FOR EXISTING ASSETS,0.8796198521647307,"URL.
739"
LICENSES FOR EXISTING ASSETS,0.880675818373812,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
740"
LICENSES FOR EXISTING ASSETS,0.8817317845828934,"• For scraped data from a particular source (e.g., website), the copyright and terms of
741"
LICENSES FOR EXISTING ASSETS,0.8827877507919747,"service of that source should be provided.
742"
LICENSES FOR EXISTING ASSETS,0.883843717001056,"• If assets are released, the license, copyright information, and terms of use in the
743"
LICENSES FOR EXISTING ASSETS,0.8848996832101372,"package should be provided. For popular datasets, paperswithcode.com/datasets
744"
LICENSES FOR EXISTING ASSETS,0.8859556494192186,"has curated licenses for some datasets. Their licensing guide can help determine the
745"
LICENSES FOR EXISTING ASSETS,0.8870116156282999,"license of a dataset.
746"
LICENSES FOR EXISTING ASSETS,0.8880675818373812,"• For existing datasets that are re-packaged, both the original license and the license of
747"
LICENSES FOR EXISTING ASSETS,0.8891235480464625,"the derived asset (if it has changed) should be provided.
748"
LICENSES FOR EXISTING ASSETS,0.8901795142555439,"• If this information is not available online, the authors are encouraged to reach out to
749"
LICENSES FOR EXISTING ASSETS,0.8912354804646251,"the asset’s creators.
750"
NEW ASSETS,0.8922914466737064,"13. New Assets
751"
NEW ASSETS,0.8933474128827877,"Question: Are new assets introduced in the paper well documented and is the documentation
752"
NEW ASSETS,0.8944033790918691,"provided alongside the assets?
753"
NEW ASSETS,0.8954593453009504,"Answer: [NA]
754"
NEW ASSETS,0.8965153115100317,"Justification: The paper does not release new assets
755"
NEW ASSETS,0.8975712777191129,"Guidelines:
756"
NEW ASSETS,0.8986272439281943,"• The answer NA means that the paper does not release new assets.
757"
NEW ASSETS,0.8996832101372756,"• Researchers should communicate the details of the dataset/code/model as part of their
758"
NEW ASSETS,0.9007391763463569,"submissions via structured templates. This includes details about training, license,
759"
NEW ASSETS,0.9017951425554382,"limitations, etc.
760"
NEW ASSETS,0.9028511087645196,"• The paper should discuss whether and how consent was obtained from people whose
761"
NEW ASSETS,0.9039070749736009,"asset is used.
762"
NEW ASSETS,0.9049630411826821,"• At submission time, remember to anonymize your assets (if applicable). You can either
763"
NEW ASSETS,0.9060190073917634,"create an anonymized URL or include an anonymized zip file.
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9070749736008448,"14. Crowdsourcing and Research with Human Subjects
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9081309398099261,"Question: For crowdsourcing experiments and research with human subjects, does the paper
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9091869060190074,"include the full text of instructions given to participants and screenshots, if applicable, as
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9102428722280888,"well as details about compensation (if any)?
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.91129883843717,"Answer: [NA]
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9123548046462513,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9134107708553326,"Guidelines:
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.914466737064414,"• The answer NA means that the paper does not involve crowdsourcing nor research with
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9155227032734953,"human subjects.
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9165786694825766,"• Including this information in the supplemental material is fine, but if the main contribu-
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9176346356916578,"tion of the paper involves human subjects, then as much detail as possible should be
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9186906019007391,"included in the main paper.
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9197465681098205,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9208025343189018,"or other labor should be paid at least the minimum wage in the country of the data
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9218585005279831,"collector.
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9229144667370645,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9239704329461457,"Subjects
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.925026399155227,"Question: Does the paper describe potential risks incurred by study participants, whether
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9260823653643083,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9271383315733897,"approvals (or an equivalent approval/review based on the requirements of your country or
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.928194297782471,"institution) were obtained?
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9292502639915523,"Answer: [NA]
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9303062302006336,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9313621964097148,"Guidelines:
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9324181626187962,"• The answer NA means that the paper does not involve crowdsourcing nor research with
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9334741288278775,"human subjects.
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9345300950369588,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9355860612460402,"may be required for any human subjects research. If you obtained IRB approval, you
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9366420274551215,"should clearly state this in the paper.
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9376979936642027,"• We recognize that the procedures for this may vary significantly between institutions
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.938753959873284,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9398099260823654,"guidelines for their institution.
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9408658922914467,"• For initial submissions, do not include any information that would break anonymity (if
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.941921858500528,"applicable), such as the institution conducting the review.
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9429778247096093,"A
Appendix
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9440337909186906,"A.1
Additional Implementation Details
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9450897571277719,"During training, we adopt a one-stage strategy like DAL [20]. The whole pipeline is trained for a
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9461457233368532,"total of 20 epochs with the AdamW optimizer [35] loading from the pre-trained weights from the
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9472016895459345,"ImageNet [11] classification task only. Meanwhile, we use CBGS [72] to resample the training data,
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9482576557550159,"and the one-cycle learning policy with a maximum learning rate of 2.0 × 10−4. The batch size is set
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9493136219640972,"to 8 on 4 3090 RTX GPUs. We adopt random flipping along both X and Y-axis, the random scaling in
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9503695881731784,"[0.95, 1.05], and random rotation in [-π/8, π/8] to augment the LiDAR data, and the random rotation
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9514255543822597,"in [-5.4◦, 5.4◦] and random resizing in [-0.06, 0.44] to augment the images. During evaluation, we
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9524815205913411,"test a single model without any data augmentation on a single 3090 RTX GPU.
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9535374868004224,"A.2
Additional Experiments
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9545934530095037,"A.2.1
3D Multi-Object Tracking Experiments
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.955649419218585,"We evaluate our DH-Fusion on the nuScenes tracking benchmark for 3D multi-object tracking (MOT)
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9567053854276664,"task. Following ObjectFusion [4], we adopt the same tracking-by-detection algorithm that uses
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9577613516367476,"velocity-based closest point distance matching, which is more effective than 3D Kalman filter [9].
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9588173178458289,"For fair comparisons, we report the results of our DH-Fusion-light capable of real-time detection
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9598732840549102,"on the nuScenes validation set, as shown in Tab. 6. We find that our DH-Fusion-light outperforms
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9609292502639916,"BEVFusion [33] and ObjectFusion [4] by 2.0 pp and 0.6 pp w.r.t. AMOTA. These results demonstrate
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9619852164730729,"that our DH-Fusion provides 3D detection boxes of higher quality, benefiting the downstream task of
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9630411826821542,"3D MOT.
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9640971488912354,Table 6: Comparisons on nuScenes validation set for 3D multi-object tracking.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9651531151003168,"Methods
AMOTA ↑AMOTP ↓IDS ↓
TransFusion [1]
71.8
60.3
694
BEVFusion [33]
72.8
59.4
764
ObjectFusion [4]
74.2
54.3
611
DH-Fusion-light (Ours)
74.8
50.3
539"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9662090813093981,"A.2.2
Evaluation at Different Depths
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9672650475184794,"Since our fusion strategy is depth-aware, it is necessary to validate our method at different depths.
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9683210137275607,"Following [4], we categorize annotation and prediction ego distances into three groups: Near (0-
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693769799366421,"20m), Middle (20-30m), and Far (>30m). As shown in Tab. 7, compared to ObjectFusion [4], our
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704329461457233,"DH-Fusion-light consistently improves performance across all depth ranges. Specifically, our method
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9714889123548046,"achieves a 47.1 mAP in the long range (>30m), surpassing ObjectFusion by 5.5 pp w.r.t. mAP. These
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9725448785638859,"results indicate that our method is more effective across different depths, especially in detecting
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9736008447729673,"distant objects.
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746568109820486,Table 7: Comparisons on nuScenes validation set at different depths. The numbers are mAP.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757127771911299,"Methods
Near Middle
Far
TransFusion-L [1]
77.5
60.9
34.8
BEVFusion [33]
79.4
64.9
40.0
ObjectFusion [4]
79.7
65.4
41.6
DH-Fusion-light (Ours)
80.3
66.5
47.1"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9767687434002112,"A.2.3
Detailed Results on the nuScenes-C
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9778247096092925,"We further provide the detailed results of each fine-grained corruption on nuScenes-C in Tab. 8. The
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9788806758183738,"results are highly consistent with the average values of each kind of data corruption.
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799366420274551,"A.3
More Visualization
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809926082365364,"As an extension of Fig. 6 in the manuscript, we provide additional examples of 3D object detection
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820485744456178,"results and BEV features from our baseline, BEVFusion [33], and our DH-Fusion. In various
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831045406546991,"samples, our method consistently achieves higher accuracy and recall in 3D detection results, with
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841605068637803,"stronger feature responses for distant objects compared to BEVFusion. These results demonstrate the
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9852164730728616,"effectiveness of the proposed method in dynamically adjusting the weights of features based on depth
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986272439281943,"during fusion at both global and local levels.
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873284054910243,"Table 8: Comparisons for each corruption level on the nuScenes-C. Corruptions exist in both
modalities by default. (L) means that only the point cloud modality has corruptions, and (C) means
that only the image modality has corruptions. Numbers are NDS / mAP."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883843717001056,"Corruption
FUTR3D
TransFusion
BEVFusion
DH-Fusion
None
68.5 / 64.17
69.82 / 66.38 71.40 / 68.45 73.30 / 69.75"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989440337909187,Weather
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904963041182682,"Snow
61.52 / 52.73 68.29 / 63.30 68.33 / 62.84 71.47 / 65.98
Rain
64.47 / 58.40 69.40 / 65.35 70.14 / 66.13 72.05 / 67.32
Fog
61.20 / 53.19 62.62 / 53.67 62.73 / 54.10 72.13 / 67.24
Sunlight
63.61 / 57.70 61.36 / 55.14 68.95 / 64.42 73.18 / 69.44"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915522703273495,Sensor
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926082365364308,"Density
67.58 / 63.72 69.42 / 65.77 71.01 / 67.79 72.94 / 69.15
Cutout
66.91 / 62.25 68.30 / 63.66 70.09 / 66.18 71.99 / 67.45
Crosstalk
67.17 / 62.66 68.83 / 64.67 70.72 / 67.32 73.23 / 69.55
FOV Lost
45.66 / 26.32 47.89 / 24.63 48.65 / 27.17 43.41 / 20.78
Gaussian (L)
64.10 / 58.94 62.32 / 55.10 65.99 / 60.64 69.04 / 63.51
Uniform (L)
67.28 / 63.21 68.68 / 64.72 70.18 / 66.81 72.54 / 68.79
Impulse (L)
67.47 / 63.42 69.06 / 65.51 70.63 / 67.54 72.75 / 68.91
Gussian (C)
62.92 / 54.96 68.94 / 64.52 69.35 / 64.44 71.55 / 66.16
Uniform (C)
64.43 / 57.61 69.33 / 65.26 70.06 / 65.81 72.46 / 67.99
Impulse (C)
63.07 / 55.16 68.89 / 64.37 69.25 / 64.30 71.66 / 66.41"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936642027455121,Motion
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947201689545935,"Compensation
39.62 / 31.87
25.69 / 9.01
36.76 / 27.57 32.51 / 15.99
Moving Obj.
56.41 / 45.43 60.03 / 51.01 59.42 / 51.63 68.12 / 60.62
Motion Blur
63.44 / 55.99 68.85 / 64.39 69.38 / 64.74 70.58 / 65.95"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957761351636748,Object
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996832101372756,"Local Density
67.62 / 63.60 69.34 / 65.65 70.77 / 67.42 72.48 / 68.87
Local Cutout
66.45 / 61.85 67.97 / 63.33 68.11 / 63.41 69.62 / 64.17
Local Gaussian
66.85 / 62.94 67.96 / 63.76 68.32 / 64.34 71.32 / 67.14
Local Uniform
67.92 / 64.09 69.67 / 66.20 70.68 / 67.58 71.34 / 66.03
Local Impulse
67.89 / 64.02 69.64 / 66.29 70.93 / 67.91 71.83 / 68.15
Shear
61.15 / 55.42 66.43 / 62.32 62.95 / 60.72 68.41 / 65.23
Scale
62.00 / 56.79 67.81 / 64.13 66.00 / 64.57 71.40 / 68.90
Rotation
63.67 / 59.64 67.42 / 63.36 66.31 / 65.13 71.62 / 68.35"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978880675818373,"Alignment
Spatial
67.75 / 63.77 69.72 / 66.22 71.35 / 68.39 71.95 / 69.52
Temporal
57.91 / 51.43 54.23 / 43.65 56.62 / 49.02 62.53 / 55.24
Average
62.82 / 56.99 64.71 / 58.73 66.06 / 61.03 68.67 / 63.07"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989440337909187,"Figure 7: More examples of 3D object detection results and BEV features from BEVFusion
and ours. We show the ground truth boxes in green, and the prediction boxes in blue. We
use red circles to highlight the comparisons of ours with BEVFusion."
