Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018214936247723133,"Deep generative models (DGMs) aim at characterizing the distribution of the train-
1"
ABSTRACT,0.0036429872495446266,"ing set by maximizing the marginal likelihood of inputs in an unsupervised manner,
2"
ABSTRACT,0.00546448087431694,"making them a promising option for unsupervised out-of-distribution (OOD) de-
3"
ABSTRACT,0.007285974499089253,"tection. However, recent works have reported that DGMs often assign higher
4"
ABSTRACT,0.009107468123861567,"likelihoods to OOD data than in-distribution (ID) data, i.e., overestimation, leading
5"
ABSTRACT,0.01092896174863388,"to their failures in OOD detection. Although several pioneer works have tried to
6"
ABSTRACT,0.012750455373406194,"analyze this phenomenon, and some VAE-based methods have also attempted to
7"
ABSTRACT,0.014571948998178506,"alleviate this issue by modifying their score functions for OOD detection, the root
8"
ABSTRACT,0.01639344262295082,"cause of the overestimation in VAE has never been revealed to our best knowl-
9"
ABSTRACT,0.018214936247723135,"edge. To fill this gap, this paper will provide a thorough theoretical analysis on
10"
ABSTRACT,0.020036429872495445,"the overestimation issue of VAE, and reveal that this phenomenon arises from two
11"
ABSTRACT,0.02185792349726776,"Inside-Enemy aspects: 1) the improper design of prior distribution; 2) the gap
12"
ABSTRACT,0.023679417122040074,"of dataset entropies between ID and OOD datasets. Based on these findings, we
13"
ABSTRACT,0.025500910746812388,"propose a novel score function to Alleviate VAE’s Overestimation In unsupervised
14"
ABSTRACT,0.0273224043715847,"OOD Detection, named “AVOID”, which contains two novel techniques, specifi-
15"
ABSTRACT,0.029143897996357013,"cally post-hoc prior and dataset entropy calibration. Experimental results verify
16"
ABSTRACT,0.030965391621129327,"our analysis, demonstrating that the proposed method is effective in alleviating
17"
ABSTRACT,0.03278688524590164,"overestimation and improving unsupervised OOD detection performance.
18"
INTRODUCTION,0.03460837887067395,"1
Introduction
19"
INTRODUCTION,0.03642987249544627,"The detection of out-of-distribution (OOD) data, i.e., identifying data that differ from the in-
20"
INTRODUCTION,0.03825136612021858,"distribution (ID) training set, is crucial for ensuring the reliability and safety of real-world applications
21"
INTRODUCTION,0.04007285974499089,"[1, 2, 3, 4]. While the most commonly used OOD detection methods rely on supervised classifiers
22"
INTRODUCTION,0.04189435336976321,"[5, 6, 7, 8, 9, 10, 11], which require labeled data, the focus of this paper is on designing an unsu-
23"
INTRODUCTION,0.04371584699453552,"pervised OOD detector. Unsupervised OOD detection refers to the task of designing a detector,
24"
INTRODUCTION,0.04553734061930783,"based solely on the unlabeled training data, that can determine whether an input is ID or OOD
25"
INTRODUCTION,0.04735883424408015,"[12, 13, 14, 15, 16, 17, 18]. This unsupervised approach is more practical for real-world scenarios
26"
INTRODUCTION,0.04918032786885246,"where the data lack labels.
27"
INTRODUCTION,0.051001821493624776,"Deep generative models (DGMs) are a highly attractive option for unsupervised OOD detection.
28"
INTRODUCTION,0.052823315118397086,"DGMs, mainly including the auto-regressive model [19, 20], flow model [21, 22], diffusion model
29"
INTRODUCTION,0.0546448087431694,"[23], generative adversarial network [24], and variational autoencoder (VAE) [25], are designed
30"
INTRODUCTION,0.056466302367941715,"to model the distribution of the training set by explicitly or implicitly maximizing the likelihood
31"
INTRODUCTION,0.058287795992714025,"estimation of p(x) for its input x without category label supervision or additional OOD auxiliary
32"
INTRODUCTION,0.060109289617486336,"data. They have achieved great successes in a wide range of applications, such as image and text
33"
INTRODUCTION,0.061930783242258654,"generation. Since generative models are promising at modeling the distribution of the training set,
34"
INTRODUCTION,0.06375227686703097,"they could be seen as an ideal unsupervised OOD detector, where the likelihood of the unseen OOD
35"
INTRODUCTION,0.06557377049180328,"data output by the model should be lower than that of the in-distribution data.
36"
INTRODUCTION,0.06739526411657559,"Unfortunately, developing a flawless unsupervised OOD detector using DGMs is not as easy as it
37"
INTRODUCTION,0.0692167577413479,"seems to be. Recent experiments have revealed a counterfactual phenomenon that directly applying
38"
INTRODUCTION,0.07103825136612021,"the likelihood of generative models as an OOD detector can result in overestimation, i.e., DGMs
39"
INTRODUCTION,0.07285974499089254,"assign higher likelihoods to OOD data than ID data [12, 13, 17, 18]. For instance, a generative
40"
INTRODUCTION,0.07468123861566485,"model trained on the FashionMNIST dataset could assign higher likelihoods to data from the MNIST
41"
INTRODUCTION,0.07650273224043716,"dataset (OOD) than data from the FashionMNIST dataset (ID), as shown in Figure 6(a). Since OOD
42"
INTRODUCTION,0.07832422586520947,"detection can be viewed as a verification of whether a generative model has learned to model the
43"
INTRODUCTION,0.08014571948998178,"distribution of the training set accurately, the counterfactual phenomenon of overestimation not only
44"
INTRODUCTION,0.08196721311475409,"poses challenges to unsupervised OOD detection but also raises doubts about the generative model’s
45"
INTRODUCTION,0.08378870673952642,"fundamental ability in modeling the data distribution. Therefore, it highlights the need for developing
46"
INTRODUCTION,0.08561020036429873,"more effective methods for unsupervised OOD detection and, more importantly, a more thorough
47"
INTRODUCTION,0.08743169398907104,"understanding of the reasons behind the overestimation in deep generative models.
48"
INTRODUCTION,0.08925318761384335,"To develop more effective methods for unsupervised OOD detection, some approaches have modified
49"
INTRODUCTION,0.09107468123861566,"the likelihood to new score functions based on empirical assumptions, such as low- and high-level
50"
INTRODUCTION,0.09289617486338798,"features’ consistency [17, 18] and ensemble approaches [26]. While these methods, particularly the
51"
INTRODUCTION,0.0947176684881603,"VAE-based methods [18], have achieved state-of-the-art (SOTA) performance in unsupervised OOD
52"
INTRODUCTION,0.0965391621129326,"detection, none of them provides a clear explanation for the overestimation issue. To gain insight into
53"
INTRODUCTION,0.09836065573770492,"the overestimation issue in generative models, pioneering works have shown that the overestimation
54"
INTRODUCTION,0.10018214936247723,"issue could arise from the intrinsic model curvature brought by the invertible architecture in flow
55"
INTRODUCTION,0.10200364298724955,"models [27]. However, in contrast to the exact marginal likelihood estimation used in flow and
56"
INTRODUCTION,0.10382513661202186,"auto-regressive models, VAE utilizes a lower bound of the likelihood, making it difficult to analyze.
57"
INTRODUCTION,0.10564663023679417,"Overall, the reasons behind the overestimation issue of VAE are still not fully understood.
58"
INTRODUCTION,0.10746812386156648,"In this paper, we try to address the research gap by providing a theoretical analysis of VAE’s
59"
INTRODUCTION,0.1092896174863388,"overestimation in unsupervised OOD detection. Our contributions can be summarized as follows:
60"
INTRODUCTION,0.1111111111111111,"1. Through theoretical analyses, we are the first to identify two factors that cause the overestima-
61"
INTRODUCTION,0.11293260473588343,"tion issue of VAE: 1) the improper design of prior distribution; 2) the intrinsic gap of dataset
62"
INTRODUCTION,0.11475409836065574,"entropies between ID and OOD datasets;
63"
INTRODUCTION,0.11657559198542805,"2. Focused on these two discovered factors, we propose a new score function, named “AVOID”,
64"
INTRODUCTION,0.11839708561020036,"to alleviate the overestimation issue from two aspects: 1) post-hoc prior for the improper
65"
INTRODUCTION,0.12021857923497267,"design of prior distribution; 2) dataset entropy calibration for the gap of dataset entropies;
66"
EXTENSIVE EXPERIMENTS DEMONSTRATE THAT OUR METHOD CAN EFFECTIVELY IMPROVE THE PERFORMANCE,0.122040072859745,"3. Extensive experiments demonstrate that our method can effectively improve the performance
67"
EXTENSIVE EXPERIMENTS DEMONSTRATE THAT OUR METHOD CAN EFFECTIVELY IMPROVE THE PERFORMANCE,0.12386156648451731,"of VAE-based methods on unsupervised OOD detection, with theoretical guarantee.
68"
PRELIMINARIES,0.12568306010928962,"2
Preliminaries
69"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.12750455373406194,"2.1
Unsupervised Out-of-distribution Detection
70"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.12932604735883424,"In this part, we will first give a problem statement of OOD detection and then we will introduce the
71"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.13114754098360656,"detailed setup for applying unsupervised OOD detection.
72"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.13296903460837886,"Problem statement. While deploying a machine learning system, it is possible to encounter inputs
73"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.13479052823315119,"from unknown distributions that are semantically and/or statistically different from the training data,
74"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.1366120218579235,"and such inputs are referred to as OOD data. Processing OOD data could potentially introduce critical
75"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.1384335154826958,"errors that compromise the safety of the system [1]. Thus, the OOD detection task is to identify these
76"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.14025500910746813,"OOD data, which could be seen as a binary classification task: determining whether an input x is
77"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.14207650273224043,"more likely ID or OOD. It could be formalized as a level-set estimation:
78"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.14389799635701275,"x =
ID,
if
S(x) > λ,
OOD,
if
S(x) ≤λ,
(1)"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.14571948998178508,"where S(x) denotes the score function, i.e., OOD detector, and the threshold λ is commonly chosen
79"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.14754098360655737,"to make a high fraction (e.g., 95%) of ID data is correctly classified [9]. In conclusion, OOD detection
80"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.1493624772313297,"aims at designing the S(x) that could assign higher scores to ID data samples than OOD ones.
81"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.151183970856102,"Setup. Denoting the input space with X, an unlabeled training dataset Dtrain = {xi}N
i=1 containing
82"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.15300546448087432,"of N data points can be obtained by sampling i.i.d. from a data distribution PX . Typically, we treat
83"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.15482695810564662,"the PX as pid, which represents the in-distribution (ID) [17, 27]. With this unlabeled training set,
84"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.15664845173041894,"unsupervised OOD detection is to design a score function S(x) that can determine whether an input
85"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.15846994535519127,"is ID or OOD. This is different from supervised OOD detection, which typically leverages a classifier
86"
UNSUPERVISED OUT-OF-DISTRIBUTION DETECTION,0.16029143897996356,"that is trained on labeled data [4, 7, 9]. We provide a detailed discussion in Appendix A.
87"
VAE-BASED UNSUPERVISED OOD DETECTION,0.1621129326047359,"2.2
VAE-based Unsupervised OOD Detection
88"
VAE-BASED UNSUPERVISED OOD DETECTION,0.16393442622950818,"DGMs could be an ideal choice for unsupervised OOD detection because the estimated marginal
89"
VAE-BASED UNSUPERVISED OOD DETECTION,0.1657559198542805,"likelihood pθ(x) can be naturally used as the score function S(x). Among DGMs, VAE can offer
90"
VAE-BASED UNSUPERVISED OOD DETECTION,0.16757741347905283,"great flexibility and strong representation ability [28], leading to a series of unsupervised OOD
91"
VAE-BASED UNSUPERVISED OOD DETECTION,0.16939890710382513,"detection methods based on VAE that have achieved SOTA performance [17, 18]. Specifically, VAE
92"
VAE-BASED UNSUPERVISED OOD DETECTION,0.17122040072859745,"estimates the marginal likelihood by training with the variational evidence lower bound (ELBO), i.e.,
93"
VAE-BASED UNSUPERVISED OOD DETECTION,0.17304189435336975,"ELBO(x) = Eqϕ(z|x) [log pθ(x|z)] −DKL(qϕ(z|x)||p(z)),
(2)"
VAE-BASED UNSUPERVISED OOD DETECTION,0.17486338797814208,"where the posterior qϕ(z|x) is modeled by an encoder, the reconstruction likelihood pθ(x|z) is
94"
VAE-BASED UNSUPERVISED OOD DETECTION,0.1766848816029144,"modeled by a decoder, and the prior p(z) is set as a Gaussian distribution N(0, I). After well training
95"
VAE-BASED UNSUPERVISED OOD DETECTION,0.1785063752276867,"the VAE, ELBO(x) is an estimation of the p(x), which could be directly seen as the score function
96"
VAE-BASED UNSUPERVISED OOD DETECTION,0.18032786885245902,"S(x) to do OOD detection. But the VAE would suffer from the overestimation issue, which will be
97"
VAE-BASED UNSUPERVISED OOD DETECTION,0.18214936247723132,"introduced in the next section. More details and Related Work can be seen in Appendix B.
98"
VAE-BASED UNSUPERVISED OOD DETECTION,0.18397085610200364,"3
Analysis of VAE’s overestimation in Unsupervised OOD Detection
99"
VAE-BASED UNSUPERVISED OOD DETECTION,0.18579234972677597,"We will first conduct an analysis to identify the factors contributing to VAE’s overestimation, i.e.,
100"
VAE-BASED UNSUPERVISED OOD DETECTION,0.18761384335154827,"the improper design of prior distribution and the gap between ID and OOD datasets’ entropies.
101"
VAE-BASED UNSUPERVISED OOD DETECTION,0.1894353369763206,"Subsequently, we will give a deeper analysis of the first factor to have a better understanding.
102"
VAE-BASED UNSUPERVISED OOD DETECTION,0.1912568306010929,"3.1
Identifying Factors of VAE’s Overestimation Issue
103"
VAE-BASED UNSUPERVISED OOD DETECTION,0.1930783242258652,"Following the common analysis procedure [27], an ideal score function S(x) that could achieve good
104"
VAE-BASED UNSUPERVISED OOD DETECTION,0.19489981785063754,"OOD detection performance is expected to have the following property for any OOD dataset:
105"
VAE-BASED UNSUPERVISED OOD DETECTION,0.19672131147540983,"G = Ex∼pid(x)[S(x)] −Ex∼pood(x)[S(x)] > 0,
(3)"
VAE-BASED UNSUPERVISED OOD DETECTION,0.19854280510018216,"where pid(x) and pood(x) denote the true distribution of the ID and OOD dataset, respectively. A
106"
VAE-BASED UNSUPERVISED OOD DETECTION,0.20036429872495445,"larger gap between these two expectation terms can usually lead to better OOD detection performance.
107"
VAE-BASED UNSUPERVISED OOD DETECTION,0.20218579234972678,"Using the ELBO(x) as the score function S(x), we could give a formal definition of the repeatedly
108"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2040072859744991,"reported VAE’s overestimation issue in the context of unsupervised OOD detection [12, 13, 17, 18].
109"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2058287795992714,"Definition 1 (VAE’s overestimation in unsupervised OOD Detection). Assume we have a VAE
110"
VAE-BASED UNSUPERVISED OOD DETECTION,0.20765027322404372,"trained on a training set and we use the ELBO(x) as the score function to distinguish data points
111"
VAE-BASED UNSUPERVISED OOD DETECTION,0.20947176684881602,"sampled i.i.d. from the in-distribution testing set (pid) and an OOD dataset (pood). When
112"
VAE-BASED UNSUPERVISED OOD DETECTION,0.21129326047358835,"G = Ex∼pid(x)[ELBO(x)] −Ex∼pood(x)[ELBO(x)] ≤0,
(4)"
VAE-BASED UNSUPERVISED OOD DETECTION,0.21311475409836064,"it is called VAE’s overestimation in unsupervised OOD detection.
113"
VAE-BASED UNSUPERVISED OOD DETECTION,0.21493624772313297,"With a clear definition of overestimation, we could now investigate the underlying factors causing
114"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2167577413479053,"the overestimation in VAE. After well training a VAE, we could reformulate the expectation term of
115"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2185792349726776,"ELBO(x) from the perspective of information theory [29] as:
116"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2204007285974499,Ex∼p(x)[ELBO(x)] = Ex∼p(x)[Ez∼qϕ(z|x) log pθ(x|z)] −Ex∼p(x)[DKL(qϕ(z|x)||p(z))]
VAE-BASED UNSUPERVISED OOD DETECTION,0.2222222222222222,"= −Hp(x) −DKL(q(z)||p(z)),
(5)"
VAE-BASED UNSUPERVISED OOD DETECTION,0.22404371584699453,"because we have
117"
VAE-BASED UNSUPERVISED OOD DETECTION,0.22586520947176686,"Ex∼p(x)[Ez∼qϕ(z|x) log pθ(x|z)] = Iq(x, z) + Ep(x) log p(x) = Iq(x, z) −Hp(x),
(6)"
VAE-BASED UNSUPERVISED OOD DETECTION,0.22768670309653916,"Ex∼p(x)[DKL(qϕ(z|x)||p(z))] = Iq(x, z) + DKL(q(z)||p(z)),
(7)"
VAE-BASED UNSUPERVISED OOD DETECTION,0.22950819672131148,"where the Iq(x, z) is mutual information between x and z and the q(z) is the aggregated posterior
118"
VAE-BASED UNSUPERVISED OOD DETECTION,0.23132969034608378,"distribution of the latent variables z, which is defined by q(z) = Ex∼p(x)qϕ(z|x). We leave the
119"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2331511839708561,"detailed definition and derivation in Appendix C.1. Thus, the gap G in Eq. (4) could be rewritten as
120"
VAE-BASED UNSUPERVISED OOD DETECTION,0.23497267759562843,"G = [−Hpid(x) + Hpood(x)] + [−DKL(qid(z)||p(z)) + DKL(qood(z)||p(z))],
(8)"
VAE-BASED UNSUPERVISED OOD DETECTION,0.23679417122040072,"where the dataset entropy Hpid(x)/Hpood(x) is a constant that only depends on the true distribution
121"
VAE-BASED UNSUPERVISED OOD DETECTION,0.23861566484517305,"of ID/OOD dataset; the prior p(z) is typically set as a standard (multivariate) Gaussian distribution
122"
VAE-BASED UNSUPERVISED OOD DETECTION,0.24043715846994534,"N(0, I) to enable reparameterization for efficient gradient descent optimization [25].
123"
VAE-BASED UNSUPERVISED OOD DETECTION,0.24225865209471767,"Through analyzing the most widely used criterion, specifically the expectation of ELBO reformulated
124"
VAE-BASED UNSUPERVISED OOD DETECTION,0.24408014571949,"in Eq. (8), for VAE-based unsupervised OOD detection, we find that there will be two potential
125"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2459016393442623,"factors that lead to the overestimation issue of VAE, i.e., G ≤0:
126"
VAE-BASED UNSUPERVISED OOD DETECTION,0.24772313296903462,"Factor I: The improper design of prior distribution p(z). Several studies have argued that the
127"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2495446265938069,"aggregated posterior distribution of latent variables q(z) cannot always equal N(0, I), particularly
128"
VAE-BASED UNSUPERVISED OOD DETECTION,0.25136612021857924,"when the dataset exhibits intrinsic multimodality [28, 30, 31, 32]. In fact, when q(z) is extremely
129"
VAE-BASED UNSUPERVISED OOD DETECTION,0.25318761384335153,"close to p(z), it is more likely to become trapped in a bad local optimum known as posterior collapse
130"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2550091074681239,"[33, 34, 35], i.e., qϕ(z|x) ≈p(z), resulting in q(z) =
R"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2568306010928962,"x qϕ(z|x)p(x) ≈
R"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2586520947176685,"x p(z)p(x) = p(z). In
131"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2604735883424408,"this situation, the posterior qϕ(z|x) becomes uninformative about the inputs. Thus, the value of
132"
VAE-BASED UNSUPERVISED OOD DETECTION,0.26229508196721313,"DKL(qid(z)||p(z)) could be overestimated, potentially contributing to G ≤0.
133"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2641165755919854,"Factor II: The gap between Hpid(x) and Hpood(x). Considering the dataset’s statistics, such as the
134"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2659380692167577,"variance of pixel values, different datasets exhibit various levels of entropy. It is reasonable that a
135"
VAE-BASED UNSUPERVISED OOD DETECTION,0.2677595628415301,"dataset containing images with richer low-level features and more diverse content is expected to have
136"
VAE-BASED UNSUPERVISED OOD DETECTION,0.26958105646630237,"a higher entropy. As an example, the FashionMNIST dataset should possess higher entropy compared
137"
VAE-BASED UNSUPERVISED OOD DETECTION,0.27140255009107467,"to the MNIST dataset. Therefore, when the entropy of the ID dataset is higher than that of an OOD
138"
VAE-BASED UNSUPERVISED OOD DETECTION,0.273224043715847,"dataset, the value of −Hpid(x) + Hpood(x) is less than 0, potentially leading to overestimation.
139"
MORE ANALYSIS ON FACTOR I,0.2750455373406193,"3.2
More Analysis on Factor I
140"
MORE ANALYSIS ON FACTOR I,0.2768670309653916,"In this part, we will focus on addressing the following question: when is the common design of the
141"
MORE ANALYSIS ON FACTOR I,0.2786885245901639,"prior distribution proper, and when is it not?
142 x1 4 3 2 1 0 1 2 3 4 x2 4 3 2 1 0 1 2 3 4 p(x) 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
MORE ANALYSIS ON FACTOR I,0.28051001821493626,(a) Data distribution p(x)
MORE ANALYSIS ON FACTOR I,0.28233151183970856,"3
2
1
0
1
2
3
z 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40"
MORE ANALYSIS ON FACTOR I,0.28415300546448086,Probability p(z)
MORE ANALYSIS ON FACTOR I,0.2859744990892532,(b) Prior p(z) x1 4 3 2 1 0 1 2 3 4 x2 4 3 2 1 0 1 2 3 4
MORE ANALYSIS ON FACTOR I,0.2877959927140255,Estimated p(x) with p(z) 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16
MORE ANALYSIS ON FACTOR I,0.2896174863387978,(c) Estimated pθ(x)
MORE ANALYSIS ON FACTOR I,0.29143897996357016,"3
2
1
0
1
2
3
z 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40"
MORE ANALYSIS ON FACTOR I,0.29326047358834245,Probability q(z)
MORE ANALYSIS ON FACTOR I,0.29508196721311475,(d) Posterior q(z)
MORE ANALYSIS ON FACTOR I,0.29690346083788705,Figure 1: Visualization of modeling a single-modal data distribution with a linear VAE.
MORE ANALYSIS ON FACTOR I,0.2987249544626594,"When the design of prior is proper? Assuming that we have a dataset consisting of N data points
143"
MORE ANALYSIS ON FACTOR I,0.3005464480874317,"{xi}N
i=1, each of which is sampled from a given d-dimensional data distribution p(x) = N(x|0, Σx)
144"
MORE ANALYSIS ON FACTOR I,0.302367941712204,"as shown in Figure 1(a). Then we construct a linear VAE to estimate p(x), formulated as:
145"
MORE ANALYSIS ON FACTOR I,0.30418943533697634,"p(z) = N(z|0, I)
qϕ(z|x) = N(z|Ax + B, C)
(9)"
MORE ANALYSIS ON FACTOR I,0.30601092896174864,"pθ(x|z) = N(x|Ez + F, σ2I),
where A,B,C,D,E,F, and σ are all learnable parameters and their optimal values can be obtained by
146"
MORE ANALYSIS ON FACTOR I,0.30783242258652094,"the derivation in Appendix C.3. As the estimated distribution pθ(x) depicted in Figure 1(c), we can
147"
MORE ANALYSIS ON FACTOR I,0.30965391621129323,"find that the linear VAE with the optimal parameter values can accurately estimate the p(x) through
148"
MORE ANALYSIS ON FACTOR I,0.3114754098360656,"maximizing ELBO, i.e., the overestimation issue is not present. In this case, Figures 1(b) and 1(d)
149"
MORE ANALYSIS ON FACTOR I,0.3132969034608379,"indicate that the design of the prior distribution is proper, where the posterior q(z) equals prior p(z).
150"
MORE ANALYSIS ON FACTOR I,0.3151183970856102,"When the design of prior is NOT proper? Consider a more complex data distribution, e.g., a mixture
151"
MORE ANALYSIS ON FACTOR I,0.31693989071038253,"of Gaussians, p(x) = PK
k=1 πkN(x|µk, Σk), K = 2 as shown in Figure 2(a), where πk = 1/K
152"
MORE ANALYSIS ON FACTOR I,0.31876138433515483,"and PK
k=1 µk = 0. We construct a dataset consisting of K × N data points, obtained by sampling
153"
MORE ANALYSIS ON FACTOR I,0.3205828779599271,"N data samples {x(k)
i
}N,K
i=1,k=1 from each component Gaussian N(x|µk, Σk). The formulation of
154"
MORE ANALYSIS ON FACTOR I,0.3224043715846995,"p(z), qϕ(z|x), and pθ(x|z) is consistent with those in Eq. (9). More details are in Appendix C.2.
155 x1 6 4 2 0 2 4 6 x2 6 4 2 0 2 4 6 p(x) 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07"
MORE ANALYSIS ON FACTOR I,0.3242258652094718,(a) Data distribution p(x)
MORE ANALYSIS ON FACTOR I,0.32604735883424407,"3
2
1
0
1
2
3
z 0.0 0.1 0.2 0.3 0.4 0.5 0.6"
MORE ANALYSIS ON FACTOR I,0.32786885245901637,Probability
MORE ANALYSIS ON FACTOR I,0.3296903460837887,"q(z)
p(z)"
MORE ANALYSIS ON FACTOR I,0.331511839708561,(b) p(z)&q(z) x1 8 6 4 2 0 2 4 6 8 x2 8 6 4 2 0 2 4 6 8
MORE ANALYSIS ON FACTOR I,0.3333333333333333,Estimated p(x) with p(z) 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035
MORE ANALYSIS ON FACTOR I,0.33515482695810567,(c) ˆpθ(x) with p(z) x1 8 6 4 2 0 2 4 6 8 x2 8 6 4 2 0 2 4 6 8
MORE ANALYSIS ON FACTOR I,0.33697632058287796,Estimated p(x) with q(z) 0.00 0.01 0.02 0.03 0.04
MORE ANALYSIS ON FACTOR I,0.33879781420765026,(d) ˆpθ(x) with q(z)
MORE ANALYSIS ON FACTOR I,0.3406193078324226,"Figure 2: Visualization of modeling a multi-modal data distribution with a linear VAE.
In what follows, we will provide a basic derivation outline for the linear VAE under the multi-modal
156"
MORE ANALYSIS ON FACTOR I,0.3424408014571949,"case. We can first obtain the marginal likelihood ˆpθ(x; E, F, σ) =
R
pθ(x|z)p(z) = N(x|F, EE⊤+
157"
MORE ANALYSIS ON FACTOR I,0.3442622950819672,"σ2I) with the strictly tighter importance sampling on ELBO [36], i.e., learning the optimal generative
158"
MORE ANALYSIS ON FACTOR I,0.3460837887067395,"process. Then, the joint log-likelihood of the observed dataset {x(k)
i
}N,K
i=1,k=1 can be formulated as:
159 L = K
X k=1 N
X"
MORE ANALYSIS ON FACTOR I,0.34790528233151186,"i=1
log ˆpθ(x(k)
i
) = −KNd"
MORE ANALYSIS ON FACTOR I,0.34972677595628415,"2
log(2π) −KN"
MORE ANALYSIS ON FACTOR I,0.35154826958105645,"2
log det(M) −KN"
MORE ANALYSIS ON FACTOR I,0.3533697632058288,"2 tr[M−1S],
(10)"
MORE ANALYSIS ON FACTOR I,0.3551912568306011,"where M = EE⊤+ σ2I and S =
1
KN
PK
k=1
PN
i=1(x(k)
i
−F)(x(k)
i
−F)⊤. After that, we could
160"
MORE ANALYSIS ON FACTOR I,0.3570127504553734,"explore the stationary points of parameters through the ELBO, which can be analytically written as:
161"
MORE ANALYSIS ON FACTOR I,0.3588342440801457,ELBO(x) =
MORE ANALYSIS ON FACTOR I,0.36065573770491804,"L1
z
}|
{
Eqϕ(z|x)[log pθ(x|z)] −"
MORE ANALYSIS ON FACTOR I,0.36247723132969034,"L2
z
}|
{
DKL[qϕ(z|x)||p(z)],
(11)"
MORE ANALYSIS ON FACTOR I,0.36429872495446264,"L1 =
1
2σ2 [−tr(ECE⊤) −(EAx + EB)⊤(EAx + EB) + 2x⊤(EAx + EB) −x⊤x] −d"
MORE ANALYSIS ON FACTOR I,0.366120218579235,"2 log(2πσ2),"
MORE ANALYSIS ON FACTOR I,0.3679417122040073,L2 = 1
MORE ANALYSIS ON FACTOR I,0.3697632058287796,2[−log det(C) + (Ax + B)⊤(Ax + B) + tr(C) −1].
MORE ANALYSIS ON FACTOR I,0.37158469945355194,"The detailed derivation of parameter solutions in Eq. (10) and (11) can be found in Appendix C.4.
162"
MORE ANALYSIS ON FACTOR I,0.37340619307832423,"In conclusion of this case, Figure 2(b) illustrates that q(z) is a multi-modal distribution instead of
163"
MORE ANALYSIS ON FACTOR I,0.37522768670309653,"p(z) = N(z|0, I), i.e., the design of the prior is not proper, which leads to overestimation as seen in
164"
MORE ANALYSIS ON FACTOR I,0.3770491803278688,"Figure 2(c). However, as analyzed in Factor I, we found that the overestimation issue is mitigated
165"
MORE ANALYSIS ON FACTOR I,0.3788706739526412,"when replacing p(z) in the KL term of the ELBO with q(z), which is shown in Figure 2(d).
166"
MORE ANALYSIS ON FACTOR I,0.3806921675774135,"More empirical studies on the improper design of prior. To extend to a more practical and
167"
MORE ANALYSIS ON FACTOR I,0.3825136612021858,"representative case, we used a 3-layer MLP to model qϕ(z|x) and pθ(x|z) with p(z) = N(0, I) on
168"
MORE ANALYSIS ON FACTOR I,0.3843351548269581,"the same dataset of the above multi-modal case. Implementation details are provided in Appendix
169"
MORE ANALYSIS ON FACTOR I,0.3861566484517304,"C.5. After training, we observed that q(z) still differs from p(z), as shown in Figure 3(a). The ELBO
170"
MORE ANALYSIS ON FACTOR I,0.3879781420765027,"still suffers from overestimation, especially in the region near (0, 0), as shown in Figure 3(b).
171"
MORE ANALYSIS ON FACTOR I,0.38979963570127507,"2
1
0
1
2
z 0 50 100 150 200"
MORE ANALYSIS ON FACTOR I,0.39162112932604737,Density
MORE ANALYSIS ON FACTOR I,0.39344262295081966,z ~ q(z|x)
MORE ANALYSIS ON FACTOR I,0.39526411657559196,(a) qid(z) x1 6 4 2 0 2 4 6 x2 6 4 2 0 2 4 6 p(x) 0.00 0.05 0.10 0.15 0.20
MORE ANALYSIS ON FACTOR I,0.3970856102003643,(b) Estimated pθ(x)
MORE ANALYSIS ON FACTOR I,0.3989071038251366,"2000
1800
1600
1400
1200
1000
Posterior z's log-probability in 
(0, I) 0.000 0.002 0.004 0.006 0.008"
MORE ANALYSIS ON FACTOR I,0.4007285974499089,Density
MORE ANALYSIS ON FACTOR I,0.40255009107468126,"FashionMNIST train (ID)
FashionMNIST test (ID)
MNIST test (OOD)"
MORE ANALYSIS ON FACTOR I,0.40437158469945356,(c) FashionMNIST (ID)
MORE ANALYSIS ON FACTOR I,0.40619307832422585,"1600
1400
1200
1000
800
Posterior z's log-probability in 
(0, I)"
MORE ANALYSIS ON FACTOR I,0.4080145719489982,0.0000
MORE ANALYSIS ON FACTOR I,0.4098360655737705,0.0005
MORE ANALYSIS ON FACTOR I,0.4116575591985428,0.0010
MORE ANALYSIS ON FACTOR I,0.4134790528233151,0.0015
MORE ANALYSIS ON FACTOR I,0.41530054644808745,0.0020
MORE ANALYSIS ON FACTOR I,0.41712204007285975,0.0025
MORE ANALYSIS ON FACTOR I,0.41894353369763204,0.0030
MORE ANALYSIS ON FACTOR I,0.4207650273224044,0.0035
MORE ANALYSIS ON FACTOR I,0.4225865209471767,Density
MORE ANALYSIS ON FACTOR I,0.424408014571949,"CIFAR10 train (ID)
CIFAR10 test (ID)
SVHN test (OOD)"
MORE ANALYSIS ON FACTOR I,0.4262295081967213,(d) CIFAR10 (ID)
MORE ANALYSIS ON FACTOR I,0.42805100182149364,"Figure 3:
(a) and (b): visualization of qid(z) and estimated p(x) by ELBO on the multi-modal
data distribution with a non-linear deep VAE; (c) and (d): the density plot of the log-probability of
posterior z, i.e., z ∼qϕ(z|x), in prior N(0, I) on two dataset pairs."
MORE ANALYSIS ON FACTOR I,0.42987249544626593,"Finally, we extend the analysis directly to high-dimensional image data. Since VAE trained on image
172"
MORE ANALYSIS ON FACTOR I,0.43169398907103823,"data needs to be equipped with a higher dimensional latent variable space, it is hard to visualize
173"
MORE ANALYSIS ON FACTOR I,0.4335154826958106,"directly. But please note that, if qid(z) is closer to p(z) = N(0, I), zid ∼qid(z) should occupy
174"
MORE ANALYSIS ON FACTOR I,0.4353369763205829,"the center of latent space N(0, I) and zood ∼qood(z) should be pushed far from the center, leading
175"
MORE ANALYSIS ON FACTOR I,0.4371584699453552,"to p(zid) to be larger than p(zood). However, surprisingly, we found this expected phenomenon
176"
MORE ANALYSIS ON FACTOR I,0.43897996357012753,"does not exist, as shown in Figure 3(c) and 3(d), where the experiments are on two dataset pairs,
177"
MORE ANALYSIS ON FACTOR I,0.4408014571948998,"Fashion-MNIST(ID)/MNIST(OOD) and CIFAR10(ID)/SVHN(OOD). This still suggests that the
178"
MORE ANALYSIS ON FACTOR I,0.4426229508196721,"prior p(z) is improper, even qood(z) for OOD data may be closer to p(z) than qid(z).
179"
MORE ANALYSIS ON FACTOR I,0.4444444444444444,"Brief summary. Through analyzing overestimation scenarios from simple to complex, the answer
180"
MORE ANALYSIS ON FACTOR I,0.44626593806921677,"to the question at the beginning of this part could be: the prior distribution p(z) = N(0, I) is an
181"
MORE ANALYSIS ON FACTOR I,0.44808743169398907,"improper choice for VAE when modeling a complex data distribution p(x), leading to an overestimated
182"
MORE ANALYSIS ON FACTOR I,0.44990892531876137,"DKL(qid(z)||p(z)) and further raising the overestimation issue in unsupervised OOD detection.
183"
MORE ANALYSIS ON FACTOR I,0.4517304189435337,"4
Alleviating VAE’s overestimation in Unsupervised OOD Detection
184"
MORE ANALYSIS ON FACTOR I,0.453551912568306,"In this section, we develop the “AVOID” method to alleviate the influence of two aforementioned
185"
MORE ANALYSIS ON FACTOR I,0.4553734061930783,"factors in Section 3, including i) post-hoc prior and ii) dataset entropy calibration, both of which are
186"
MORE ANALYSIS ON FACTOR I,0.45719489981785066,"implemented in a simple way to inspire related work and can be further investigated for improvement.
187"
POST-HOC PRIOR METHOD FOR FACTOR I,0.45901639344262296,"4.1
Post-hoc Prior Method for Factor I
188"
POST-HOC PRIOR METHOD FOR FACTOR I,0.46083788706739526,"FashionMNIST test (ID)
MNIST test (OOD)
Prior z
(0, I)"
POST-HOC PRIOR METHOD FOR FACTOR I,0.46265938069216755,"Figure 4: The t-SNE visualization of
the latent representations on FashionM-
NIST(ID)/MNIST(OOD) dataset pair."
POST-HOC PRIOR METHOD FOR FACTOR I,0.4644808743169399,"To provide a more insightful view to investigate the re-
189"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4663023679417122,"lationship between qid(z), qood(z), and p(z), we use t-
190"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4681238615664845,"SNE [37] to visualize them in Figure 4. The visualization
191"
POST-HOC PRIOR METHOD FOR FACTOR I,0.46994535519125685,"reveals that p(z) cannot distinguish between the latent
192"
POST-HOC PRIOR METHOD FOR FACTOR I,0.47176684881602915,"variables sampled from qid(z) and qood(z), while qid(z) is
193"
POST-HOC PRIOR METHOD FOR FACTOR I,0.47358834244080145,"clearly distinguishable from qood(z). Therefore, to alle-
194"
POST-HOC PRIOR METHOD FOR FACTOR I,0.47540983606557374,"viate overestimation, we can explicitly modify the prior
195"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4772313296903461,"distribution p(z) in Eq. (8) to force it to be closer to qid(z)
196"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4790528233151184,"and far from qood(z), i.e., decreasing DKL(qid(z)||p(z))
197"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4808743169398907,"and increasing DKL(qood(z)||p(z)).
198"
POST-HOC PRIOR METHOD FOR FACTOR I,0.48269581056466304,"A straightforward modifying approach is to replace p(z)
199"
POST-HOC PRIOR METHOD FOR FACTOR I,0.48451730418943534,"in ELBO with an additional distribution ˆqid(z) that can
200"
POST-HOC PRIOR METHOD FOR FACTOR I,0.48633879781420764,"fit qid(z) well after training, where the target value of
201"
POST-HOC PRIOR METHOD FOR FACTOR I,0.48816029143898,"qid(z) can be acquired by marginalizing qϕ(z|x) over the
202"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4899817850637523,"training set, i.e., qid(z) = Ex∼pid(x)[qϕ(z|x)]. Previous study on distribution matching [30] has
203"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4918032786885246,"developed an LSTM-based method to efficiently fit qid(z) in the latent space, i.e.,
204"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4936247723132969,"ˆqid(z) = T
Y"
POST-HOC PRIOR METHOD FOR FACTOR I,0.49544626593806923,"t=1
q(zt|z<t), where q(zt|z<t) = N(µi, σ2
i ).
(12)"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4972677595628415,"Thus, we could propose a “post-hoc prior” (PHP) method for Factor I, formulated as
205"
POST-HOC PRIOR METHOD FOR FACTOR I,0.4990892531876138,"PHP(x) := Ez∼qϕ(z|x) log pθ(x|z) −DKL(qϕ(z|x)||ˆqid(z)),
(13)
which could lead to better OOD detection performance since it could enlarge the gap G, i.e.,
206"
POST-HOC PRIOR METHOD FOR FACTOR I,0.5009107468123861,"GPHP = [−Hpid(x) + Hpood(x)] + [−DKL(qid(z)||ˆqid(z)] + DKL(qood(z)||ˆqid(z))] > G.
(14)
Please note that PHP can be directly integrated into a trained VAE in a “plug-and-play” manner.
207"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5027322404371585,"4.2
Dataset Entropy Calibration Method for Factor II
208"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5045537340619308,"While the entropy of a dataset is a constant that remains unaffected by different model settings, it is
209"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5063752276867031,"still an essential factor that leads to overestimation. To address this, a straightforward approach is to
210"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5081967213114754,"design a calibration method that ensures the value added to the ELBO of ID data will be larger than
211"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5100182149362478,"that of OOD data. Specifically, we denote the calibration term as C(x), and its expected property
212"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.51183970856102,"could be formulated as
213"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5136612021857924,"Ex∼pid(x)[C(x)] > Ex∼pood(x)[C(x)].
(15)"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5154826958105647,"After adding the calibration C(x) to the ELBO(x), we could obtain the “dataset entropy calibration”
214"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.517304189435337,"(DEC) method for Factor II, formulated as
215"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5191256830601093,"DEC(x) := Ez∼qϕ(z|x) log pθ(x|z) −DKL(qϕ(z|x)||p(z)) + C(x).
(16)"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5209471766848816,"With the property in Eq. (15), we could find that the new gap GDEC becomes larger than the original
216"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5227686703096539,"gap G based solely on ELBO, as GDEC = G +Ex∼pid(x)[C(x)]−Ex∼pood(x)[C(x)] > G, which should
217"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5245901639344263,"alleviate the overestimation and lead to better unsupervised OOD detection performance.
218"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5264116575591985,"0
5
10
15
20
25
Number of singular values (n) 10
8 10
7 10
6 10
5 10
4 10
3 10
2 10
1"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5282331511839709,"Reconstruction error (|xre
x|)"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5300546448087432,"FashionMNIST train
FashionMNIST test
MNIST test
notMNIST test
SVHNGray test
CIFAR10Gray test
Omniglot test
KMNIST test
Constant
Random"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5318761384335154,"Figure 5: Visualization of the
relationship between the num-
ber of singular values and the
reconstruction error."
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5336976320582878,"How to design the calibration C(x)? For the choice of the function
219"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5355191256830601,"C(x), inspired by the previous work [13], we could use image com-
220"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5373406193078324,"pression methods like Singular Value Decomposition (SVD) [38]
221"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5391621129326047,"to roughly measure the complexity of an image, where the images
222"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5409836065573771,"from the same dataset should have similar complexity. An intuitive
223"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5428051001821493,"insight into this could be shown in Figure 5, where the ID dataset’s
224"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5446265938069217,"statistical feature, i.e., the curve, is distinguishable to other datasets.
225"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.546448087431694,"Based on this empirical study, we could first propose a non-scaled
226"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5482695810564663,"calibration function, denoted as Cnon(x). First, we could set the num-
227"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5500910746812386,"ber of singular values as nid, which can achieve the reconstruction
228"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5519125683060109,"error |xrecon −x| = ϵ in the ID training set; then for a test input xi,
229"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5537340619307832,"we use SVD to calculate the smallest ni that could also achieve a
230"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5555555555555556,"smaller reconstruction error ϵ, then Cnon(x) could be formulated as:
231"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5573770491803278,"Cnon(x) =
(ni/nid),
if
ni < nid,
[((nid −(ni −nid))/nid],
if
ni ≥nid,
(17)"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5591985428051002,"which can give the ID dataset a higher expectation Ex∼pid(x)[Cnon(x)] than that of other statistically
232"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5610200364298725,"different OOD datasets. More details to obtain Cnon(x) can be found in Appendix D.
233"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5628415300546448,"4.3
Putting Them Together to Get “AVOID”
234"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5646630236794171,"By combining the post-hoc prior (PHP) method and the dataset entropy calibration (DEC) method,
235"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5664845173041895,"we could develop a new score function, denoted as SAVOID(x):
236"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5683060109289617,"SAVOID(x) := Eqϕ(z|x) [log pθ(x|z)] −DKL(qϕ(z|x)||ˆqid(z)) + C(x).
(18)"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5701275045537341,"To balance the importance of PHP and DEC terms in Eq. (18), we consider to set an appropriate scale
237"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5719489981785064,"for C(x). For the scale of C(x), if it is too small, its effectiveness in alleviating overestimation could be
238"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5737704918032787,"limited. Otherwise, it may hurt the effectiveness of the PHP method since DEC will dominate the value
239"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.575591985428051,"of “AVOID”. Additionally, for statistically similar datasets, i.e., Hpid(x) ≈Hpood(x), the property in
240"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5774134790528234,"Eq. (15) cannot be guaranteed and we may only have Ex∼pid(x)[Cnon(x)] ≈Ex∼pood(x)[Cnon(x)], in
241"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5792349726775956,"which case we could only rely on the PHP method. Thus, an appropriate scale of Ex∼pid(x)[C(x)],
242"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.581056466302368,"named “Cscale”, could be derived by Cscale = Ex∼pid(x)[PHP(x)] ≈Hpid(x), which leads to
243"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5828779599271403,"Ex∼pid(x)[DEC(x)] = −Hpid(x) −DKL(qid(z)||p(z)) + Cscale ≈−DKL(qid(z)||p(z)).
(19)"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5846994535519126,"Thus, when Hpid(x) ≈Hpood(x) and Ex∼pid(x)[C(x)] ≈Ex∼pood(x)[C(x)], the PHP part of “AVOID”
244"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5865209471766849,"could still be helpful to alleviate overestimation.
245"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5883424408014571,"Motivated by the above analysis, we could implement the scaled calibration function, formulated as
246"
DATASET ENTROPY CALIBRATION METHOD FOR FACTOR II,0.5901639344262295,"C(x) = Cnon(x) × Cscale =
(ni/nid) × Cscale,
if
ni < nid,
[((nid −(ni −nid))/nid] × Cscale,
if
ni ≥nid.
(20)"
EXPERIMENTS,0.5919854280510018,"5
Experiments
247"
EXPERIMENTAL SETUP,0.5938069216757741,"5.1
Experimental Setup
248"
EXPERIMENTAL SETUP,0.5956284153005464,"Datasets. In accordance with existing literature [17, 18, 39], we evaluate our method against previous
249"
EXPERIMENTAL SETUP,0.5974499089253188,"works using two standard dataset pairs: FashionMNIST [40] (ID) / MNIST [41] (OOD) and CIFAR10
250"
EXPERIMENTAL SETUP,0.599271402550091,"[42] (ID) / SVHN [43] (OOD). The suffixes “ID” and “OOD” represent in-distribution and out-of-
251"
EXPERIMENTAL SETUP,0.6010928961748634,"distribution datasets, respectively. To more comprehensively assess the generalization capabilities
252"
EXPERIMENTAL SETUP,0.6029143897996357,"of these methods, we incorporate additional OOD datasets, the details of which are available in
253"
EXPERIMENTAL SETUP,0.604735883424408,"Appendix E.1. Notably, datasets featuring the suffix “-G” (e.g., “CIFAR10-G”) have been converted
254"
EXPERIMENTAL SETUP,0.6065573770491803,"to grayscale, resulting in a single-channel format.
255"
EXPERIMENTAL SETUP,0.6083788706739527,"Evaluation and Metrics. We adhere to the previous evaluation procedure [17, 18], where all methods
256"
EXPERIMENTAL SETUP,0.6102003642987249,"are trained using the training split of the in-distribution dataset, and their OOD detection performance
257"
EXPERIMENTAL SETUP,0.6120218579234973,"is assessed on both the testing split of the in-distribution dataset and the OOD dataset. In line
258"
EXPERIMENTAL SETUP,0.6138433515482696,"with previous works [1, 5, 44], we employ evaluation metrics including the area under the receiver
259"
EXPERIMENTAL SETUP,0.6156648451730419,"operating characteristic curve (AUROC ↑), the area under the precision-recall curve (AUPRC ↑),
260"
EXPERIMENTAL SETUP,0.6174863387978142,"and the false positive rate at 80% true positive rate (FPR80 ↓). The arrows indicate the direction of
261"
EXPERIMENTAL SETUP,0.6193078324225865,"improvement for each metric.
262"
EXPERIMENTAL SETUP,0.6211293260473588,"Baselines. Our experiments primarily encompass two comparison aspects: i) evaluating our novel
263"
EXPERIMENTAL SETUP,0.6229508196721312,"score function “AVOID” against previous unsupervised OOD detection methods to determine whether
264"
EXPERIMENTAL SETUP,0.6247723132969034,"it can achieve competitive performance; and ii) comparing “AVOID” with VAE’s ELBO to assess
265"
EXPERIMENTAL SETUP,0.6265938069216758,"whether our method can mitigate overestimation and yield improved performance. For comparisons
266"
EXPERIMENTAL SETUP,0.6284153005464481,"in i, we can categorize the baselines into three groups, as outlined in [18]: “Supervised” includes
267"
EXPERIMENTAL SETUP,0.6302367941712204,"supervised OOD detection methods that utilize in-distribution data labels [1, 5, 9, 45, 46, 47, 48, 49];
268"
EXPERIMENTAL SETUP,0.6320582877959927,"“Auxiliary” refers to methods that employ auxiliary knowledge gathered from OOD data [13, 39, 44];
269"
EXPERIMENTAL SETUP,0.6338797814207651,"and “Unsupervised” encompasses methods without reliance on labels or OOD-specific assumptions
270"
EXPERIMENTAL SETUP,0.6357012750455373,"[14, 17, 18, 26]. For comparisons in ii, we compare our method with a standard VAE [25], which also
271"
EXPERIMENTAL SETUP,0.6375227686703097,"serves as the foundation of our method. Further details regarding these baselines and their respective
272"
EXPERIMENTAL SETUP,0.639344262295082,"categories can be found in Appendix E.2.
273"
EXPERIMENTAL SETUP,0.6411657559198543,"Implementation Details. The VAE’s latent variable z’s dimension is set as 200 for all experiments
274"
EXPERIMENTAL SETUP,0.6429872495446266,"with the encoder and decoder parameterized by a 3-layer convolutional neural network, respectively.
275"
EXPERIMENTAL SETUP,0.644808743169399,"Table 1: The comparisons of our method and other OOD detection methods. The best results achieved
by the methods of the category “Not ensembles” of “Unsupervised” have been bold."
EXPERIMENTAL SETUP,0.6466302367941712,"FashinMNIST(ID)/MNIST(OOD)
CIFAR10(ID)/SVHN(OOD)
Supervised
Auxiliary
Unsupervised
Supervised
Auxiliary
Unsupervised
Method
AUROC↑Mehod
AUROC↑Method
AUROC↑Method
AUROC↑Mehod
AUROC↑Method
AUROC↑
CP [1]
73.4
LR(PC) [39]
99.4
-Ensembles
MD [46]
99.7
LR(PC) [39]
93.0
-Ensembles
CP(Ent) [1]
74.6
LR(BC) [39]
45.5
WAIC(5VAE) [26]
76.6
LMD [47]
27.9
LR(VAE) [39]
26.5
WAIC(5Glow) [26]
99.0
ODIN [45]
75.2
CP(OOD) [39]
87.7
WAIC(5PC) [26]
22.1
EN [6]
98.9
OE [44]
98.4
WAIC(5PC) [26]
62.8
VIB [5]
94.1
CP(Cal) [39]
90.4
-Not Ensembles
iDE [52]
95.7
IC(Glow) [13]
95.0
-Not Ensembles
MD(CNN) [46]
94.2
IC(Glow) [13]
99.8
LRe [14]
98.8
LN[9]
98.4
IC(PC++) [13]
92.9
LRe [14]
87.5
MD(DN) [46]
98.6
IC(PC++) [13]
96.7
HVK [17]
98.4
ODIN [45]
82.9
IC(HVAE) [13]
83.3
HVK [17]
89.1
DE [1]
85.7
LLRada[18]
98.0
GN [49]
76.7
LLRada[18]
94.2
AVOID(ours)
99.2
AVOID(ours)
94.5"
EXPERIMENTAL SETUP,0.6484517304189436,"Table 2: The comparisons of our method with post-hoc prior (denoted as “PHP”) or dataset en-
tropy calibration (denoted as “DEC”) individually and other unsupervised OOD detection methods.
“PHP+DEC"" is equal to our method “AVOID"". Bold numbers are superior results."
EXPERIMENTAL SETUP,0.6502732240437158,"FashinMNIST(ID)/MNIST(OOD)
CIFAR10(ID)/SVHN(OOD)
Method
AUROC↑
AUPRC↑
FPR80↓
Method
AUROC↑
AUPRC↑
FPR80↓
ELBO [25]
23.5
35.6
98.5
ELBO [25]
24.9
36.7
94.6
WAIC(5PC) [26]
22.1
40.1
91.1
WAIC(5PC) [26]
62.8
61.6
65.7
HVK [17]
98.4
98.4
1.3
HVK [17]
89.1
87.5
17.2
LLRada[18]
97.0
97.6
0.9
LLRada[18]
92.6
91.8
11.1
-Ours:
-Ours:
PHP
89.7
90.3
13.3
PHP
39.6
42.6
85.7
DEC
34.1
40.7
92.5
DEC
87.8
89.9
17.8
PHP+DEC
99.2
99.4
0.00
PHP+DEC
94.5
95.3
4.24"
EXPERIMENTAL SETUP,0.6520947176684881,"The reconstruction likelihood distribution is modeled by a discretized mixture of logistics [20]. For
276"
EXPERIMENTAL SETUP,0.6539162112932605,"optimization, we adopt the same Adam optimizer [50] with a learning rate of 1e-3. We train all
277"
EXPERIMENTAL SETUP,0.6557377049180327,"models in comparison by setting the batch size as 128 and the max epoch as 1000. All experiments
278"
EXPERIMENTAL SETUP,0.6575591985428051,"are performed on a PC with an NVIDIA A100 GPU and our code is implemented with PyTorch [51].
279"
EXPERIMENTAL SETUP,0.6593806921675774,"More implementation details can be found in Appendix E.3.
280"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.6612021857923497,"5.2
Comparison with Unsupervised OOD Detection Baselines
281"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.663023679417122,"First, we compare our method with other SOTA baselines in Table 1. The results demonstrate that our
282"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.6648451730418944,"method achieves competitive performance compared to “Supervised” and “Auxiliary” methods and
283"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.6666666666666666,"outperforms “Unsupervised” OOD detection methods. Next, we provide a more detailed comparison
284"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.668488160291439,"with some unsupervised methods, particularly the ELBO of VAE, as shown in Table 2. These
285"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.6703096539162113,"results indicate that our method effectively mitigates overestimation and enhances OOD detection
286"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.6721311475409836,"performance when using VAE as the backbone. Lastly, to assess our method’s generalization
287"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.6739526411657559,"capabilities, we test it on a broader range of datasets, as displayed in Table 3. Experimental results
288"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.6757741347905283,"strongly verify our analysis of the VAE’s overestimation issue and demonstrate that our method
289"
COMPARISON WITH UNSUPERVISED OOD DETECTION BASELINES,0.6775956284153005,"consistently mitigates overestimation, regardless of the type of OOD datasets.
290"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6794171220400729,"5.3
Ablation Study on Verifying the Post-hoc Prior Method
291"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6812386156648452,"To evaluate the effectiveness of the Post-hoc Prior (PHP), we compare it with other unsupervised
292"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6830601092896175,"methods in Table 2. Moreover, we test the PHP method on additional datasets and present the results
293"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6848816029143898,"in Table 4 of Appendix F. The experimental results demonstrate that the PHP method can alleviate
294"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6867030965391621,"the overestimation. To provide a better understanding, we also visualize the density plot of ELBO and
295"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6885245901639344,"PHP for the “FashionMNIST(ID)/MNIST(OOD)” dataset pair in Figures 6(a) and 6(b), respectively.
296"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6903460837887068,"The Log-likelihood Ratio (LLR) methods [17, 18] are the current SOTA unsupervised OOD detection
297"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.692167577413479,"methods that also focus on latent variables. These methods are based on an empirical assumption
298"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6939890710382514,"that the bottom layer latent variables of a hierarchical VAE could learn low-level features and top
299"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6958105646630237,"layers learn semantic features. However, we discovered that while ELBO could already perform
300"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.697632058287796,"well in detecting some OOD data, the LLR method [18] could negatively impact OOD detection
301"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.6994535519125683,"performance to some extent, as demonstrated in Figure 6(c), where the model is trained on MNIST
302"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.7012750455373407,"and detects FashionMNIST as OOD. On the other hand, our method can still maintain comparable
303"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.7030965391621129,"performance since the PHP method can explicitly alleviate overestimation, which is one of the
304"
ABLATION STUDY ON VERIFYING THE POST-HOC PRIOR METHOD,0.7049180327868853,"strengths of our method compared to the SOTA methods.
305"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7067395264116576,"5.4
Ablation Study on Verifying the Dataset Entropy Calibration Method
306"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7085610200364298,"We evaluate the performance of dataset entropy calibration, referred to as “DEC”, in Table 2 and
307"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7103825136612022,"Table 5 of Appendix G. Although the DEC method is simple, our results show that it effectively
308"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7122040072859745,"alleviates overestimation. To better understand DEC, we visualize the calculated C(x) of CIFAR10
309"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7140255009107468,"Table 3: The comparisons of our method “AVOID” and baseline “ELBO” on more datasets. Bold
numbers are superior performance."
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7158469945355191,"ID
FashionMNIST
ID
CIFAR10
OOD
AUROC ↑
AUPRC ↑
FPR80 ↓
OOD
AUROC ↑
AUPRC ↑
PFR80 ↓
ELBO / AVOID (ours)
ELBO / AVOID (ours)
KMNIST
60.03 / 78.71
54.60 / 68.91
61.6 / 48.4
CIFAR100
52.91 / 55.36
51.15 / 72.13
77.42 / 73.93
Omniglot
99.86 / 100.0
99.89 / 100.0
0.00 / 0.00
CelebA
57.27 / 71.23
54.51 / 72.13
69.03 / 54.45
notMNIST
94.12 / 97.72
94.09 / 97.70
8.29 / 2.20
Places365
57.24 / 68.37
56.96 / 69.05
73.13 / 62.64
CIFAR10-G
98.01 / 99.01
98.24 / 99.04
1.20 / 0.40
LFWPeople
64.15 / 67.72
59.71 / 68.81
59.44 / 54.45
CIFAR100-G
98.49 / 98.59
97.49 / 97.87
1.00 / 1.00
SUN
53.14 / 63.09
54.48 / 63.32
79.52 / 68.63
SVHN-G
95.61 / 96.20
96.20 / 97.41
3.00 / 0.40
STL10
49.37 / 64.51
47.79 / 65.50
78.02 / 67.23
CelebA-G
97.33 / 97.87
94.71 / 95.82
3.00 / 0.40
Flowers102
67.68 / 76.83
64.68 / 78.01
57.94 / 46.65
SUN-G
99.16 / 99.32
99.39 / 99.47
0.00 / 0.00
GTSRB
39.50 / 53.06
41.73 / 49.84
86.61 / 73.63
Places365-G
98.92 / 98.89
98.05 / 98.61
0.80 / 0.80
DTD
37.86 / 81.82
40.93 / 62.42
82.22 / 64.24
Const
94.94 / 95.20
97.27 / 97.32
1.80 / 1.70
Const
0.001 / 80.12
30.71 / 89.42
100.0 / 22.38
Random
99.80 / 100.0
99.90 / 100.0
0.00 / 0.00
Random
71.81 / 99.31
82.89 / 99.59
85.71 / 0.000"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7176684881602914,"14
12
10
8
6
4
bits/dim 0.0 0.1 0.2 0.3 0.4"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7194899817850637,Density
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7213114754098361,"FashionMNIST test (ID)
MNIST test (OOD)"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7231329690346083,(a) Density plot of ELBO
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7249544626593807,"22.5
20.0
17.5
15.0
12.5
10.0
7.5
5.0
bits/dim 0.00 0.05 0.10 0.15 0.20"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.726775956284153,Density
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7285974499089253,"FashionMNIST test (ID)
MNIST test (OOD)"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7304189435336976,(b) Density plot of PHP
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.73224043715847,"0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7340619307832422,True Positive Rate
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7358834244080146,MNIST (ID) / FashionMNIST (OOD) ELBO
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7377049180327869,(c) ROC curve of LLR
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7395264116575592,"0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7413479052823315,True Positive Rate
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7431693989071039,MNIST (ID) / FashionMNIST (OOD) ELBO PHP
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7449908925318761,(d) ROC curve of PHP
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7468123861566485,"Figure 6: Density plots and ROC curves. (a): directly using ELBO(x), an estimation of the p(x),
of a VAE trained on FashionMNIST leads to overestimation in detecting MNIST as OOD data; (b):
using PHP method could alleviate the overestimation; (c): SOTA method LLR hurts the performance
when ELBO could already work well; (d): PHP method would not hurt the performance."
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7486338797814208,"(ID) in Figure 7(a) and other OOD datasets in Figure 7(b) when nid = 20. Our results show that
310"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7504553734061931,"the C(x) of CIFAR10 (ID) achieves generally higher values than that of other datasets, which is the
311"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7522768670309654,"underlying reason for its effectiveness in alleviating overestimation. Additionally, we investigate the
312"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7540983606557377,"impact of different nid on OOD detection performance in Figure 7(c), where our results show that the
313"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.75591985428051,"performance is consistently better than ELBO.
314"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7577413479052824,"0
2000
4000
6000
8000
10000
12000 C(x) 0 100 200 300 400"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7595628415300546,Density
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.761384335154827,CIFAR-10 test (ID)
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7632058287795993,(a) C(x) of CIFAR10 (ID)
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7650273224043715,"0
2000
4000
6000
8000
10000
12000 C(x) 0 100 200 300 400 500 600 700"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7668488160291439,Density
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7686703096539163,"LFWPeople
SUN397
GTSRB
CIFAR100
SVHN
Flowers102
DTD
Places365
STL10
CelebA
constant
random"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7704918032786885,(b) C(x) of OOD datasets
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7723132969034608,"5.0
7.5
10.0
12.5
15.0
17.5
20.0
22.5
25.0
nid 30 40 50 60 70 80 90 AUROC"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7741347905282332,"DEC
ELBO"
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7759562841530054,(c) Impact of nid
ABLATION STUDY ON VERIFYING THE DATASET ENTROPY CALIBRATION METHOD,0.7777777777777778,"Figure 7: (a) and (b) are respectively the visualizations of the calculated entropy calibration C(x) of
CIFAR10 (ID) and other OOD datasets, where the C(x) of CIFAR10 (ID) could achieve generally
higher values. (c) is the OOD detection performance of dataset entropy calibration with different nid
settings, which consistently outperforms ELBO."
CONCLUSION,0.7795992714025501,"6
Conclusion
315"
CONCLUSION,0.7814207650273224,"In conclusion, we have identified the underlying factors that lead to VAE’s overestimation in un-
316"
CONCLUSION,0.7832422586520947,"supervised OOD detection: the improper design of the prior and the gap of the dataset entropies
317"
CONCLUSION,0.785063752276867,"between the ID and OOD datasets. With this analysis, we have developed a novel score function
318"
CONCLUSION,0.7868852459016393,"called “AVOID”, which is effective in alleviating overestimation and improving unsupervised OOD
319"
CONCLUSION,0.7887067395264117,"detection. This work may lead a research stream for improving unsupervised OOD detection by
320"
CONCLUSION,0.7905282331511839,"developing more efficient and sophisticated methods aimed at optimizing these revealed factors.
321"
REFERENCES,0.7923497267759563,"References
322"
REFERENCES,0.7941712204007286,"[1] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
323"
REFERENCES,0.7959927140255009,"examples in neural networks. In ICLR, 2017.
324"
REFERENCES,0.7978142076502732,"[2] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-
325"
REFERENCES,0.7996357012750456,"sarial examples. In ICLR, 2015.
326"
REFERENCES,0.8014571948998178,"[3] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High
327"
REFERENCES,0.8032786885245902,"confidence predictions for unrecognizable images. In CVPR, 2015.
328"
REFERENCES,0.8051001821493625,"[4] Hongxin Wei, Lue Tao, Renchunzi Xie, Lei Feng, and Bo An. Open-sampling: Exploring
329"
REFERENCES,0.8069216757741348,"out-of-distribution data for re-balancing long-tailed datasets. In ICML, 2022.
330"
REFERENCES,0.8087431693989071,"[5] Alexander A. Alemi, Ian Fischer, and Joshua V. Dillon. Uncertainty in the variational informa-
331"
REFERENCES,0.8105646630236795,"tion bottleneck. CoRR, abs/1807.00906, 2018.
332"
REFERENCES,0.8123861566484517,"[6] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution
333"
REFERENCES,0.8142076502732241,"detection. In NeurIPS, 2020.
334"
REFERENCES,0.8160291438979964,"[7] Hongxin Wei, Lue Tao, Renchunzi Xie, and Bo An. Open-set label noise can improve robustness
335"
REFERENCES,0.8178506375227687,"against inherent label noise. In NeurIPS, 2022.
336"
REFERENCES,0.819672131147541,"[8] Zhuo Huang, Xiaobo Xia, Li Shen, Bo Han, Mingming Gong, Chen Gong, and Tongliang
337"
REFERENCES,0.8214936247723132,"Liu. Harnessing out-of-distribution examples via augmenting content and style. arXiv preprint
338"
REFERENCES,0.8233151183970856,"arXiv:2207.03162, 2022.
339"
REFERENCES,0.825136612021858,"[9] Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. Mitigating neural
340"
REFERENCES,0.8269581056466302,"network overconfidence with logit normalization. In ICML, 2022.
341"
REFERENCES,0.8287795992714025,"[10] Shuyang Yu, Junyuan Hong, Haotao Wang, Zhangyang Wang, and Jiayu Zhou. Turning the
342"
REFERENCES,0.8306010928961749,"curse of heterogeneity in federated learning into a blessing for out-of-distribution detection. In
343"
REFERENCES,0.8324225865209471,"ICLR, 2023.
344"
REFERENCES,0.8342440801457195,"[11] Ido Galil, Mohammed Dabbah, and Ran El-Yaniv. A framework for benchmarking class-out-of-
345"
REFERENCES,0.8360655737704918,"distribution detection and its application to imagenet. In ICLR, 2023.
346"
REFERENCES,0.8378870673952641,"[12] Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A. DePristo, Joshua V.
347"
REFERENCES,0.8397085610200364,"Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In
348"
REFERENCES,0.8415300546448088,"NeurIPS, 2019.
349"
REFERENCES,0.843351548269581,"[13] Joan Serrà, David Álvarez, Vicenç Gómez, Olga Slizovskaia, José F. Núñez, and Jordi Luque.
350"
REFERENCES,0.8451730418943534,"Input complexity and out-of-distribution detection with likelihood-based generative models. In
351"
REFERENCES,0.8469945355191257,"ICLR, 2020.
352"
REFERENCES,0.848816029143898,"[14] Zhisheng Xiao, Qing Yan, and Yali Amit. Likelihood regret: An out-of-distribution detection
353"
REFERENCES,0.8506375227686703,"score for variational auto-encoder. In NeurIPS, 2020.
354"
REFERENCES,0.8524590163934426,"[15] Lars Maaløe, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIVA: A very deep hierarchy
355"
REFERENCES,0.8542805100182149,"of latent variables for generative modeling. In NeurIPS, 2019.
356"
REFERENCES,0.8561020036429873,"[16] Griffin Floto, Stefan Kremer, and Mihai Nica. The tilted variational autoencoder: Improving
357"
REFERENCES,0.8579234972677595,"out-of-distribution detection. In ICLR, 2023.
358"
REFERENCES,0.8597449908925319,"[17] Jakob D Drachmann Havtorn, Jes Frellsen, Soren Hauberg, and Lars Maaløe. Hierarchical vaes
359"
REFERENCES,0.8615664845173042,"know what they don’t know. In ICML, 2021.
360"
REFERENCES,0.8633879781420765,"[18] Yewen Li, Chaojie Wang, Xiaobo Xia, Tongliang Liu, and Bo An. Out-of-distribution detection
361"
REFERENCES,0.8652094717668488,"with an adaptive likelihood ratio on informative hierarchical vae. In NeurIPS, 2022.
362"
REFERENCES,0.8670309653916212,"[19] Aäron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, Oriol Vinyals,
363"
REFERENCES,0.8688524590163934,"and Alex Graves. Conditional image generation with pixelcnn decoders. In NeurIPS, 2016.
364"
REFERENCES,0.8706739526411658,"[20] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the
365"
REFERENCES,0.8724954462659381,"pixelcnn with discretized logistic mixture likelihood and other modifications. In ICLR, 2017.
366"
REFERENCES,0.8743169398907104,"[21] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In
367"
REFERENCES,0.8761384335154827,"ICLR, 2017.
368"
REFERENCES,0.8779599271402551,"[22] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolu-
369"
REFERENCES,0.8797814207650273,"tions. In NeurIPS, 2018.
370"
REFERENCES,0.8816029143897997,"[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
371"
REFERENCES,0.8834244080145719,"NeurIPS, 2020.
372"
REFERENCES,0.8852459016393442,"[24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
373"
REFERENCES,0.8870673952641166,"Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications
374"
REFERENCES,0.8888888888888888,"of the ACM, 63(11):139–144, 2020.
375"
REFERENCES,0.8907103825136612,"[25] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
376"
REFERENCES,0.8925318761384335,"[26] Hyunsun Choi, Eric Jang, and Alexander A Alemi. Waic, but why? generative ensembles for
377"
REFERENCES,0.8943533697632058,"robust anomaly detection. arXiv preprint arXiv:1810.01392, 2018.
378"
REFERENCES,0.8961748633879781,"[27] Eric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Görür, and Balaji Lakshmi-
379"
REFERENCES,0.8979963570127505,"narayanan. Do deep generative models know what they don’t know? In ICLR, 2019.
380"
REFERENCES,0.8998178506375227,"[28] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma
381"
REFERENCES,0.9016393442622951,"with denoising diffusion gans. In ICLR, 2022.
382"
REFERENCES,0.9034608378870674,"[29] Thomas M Cover. Elements of Information Theory. John Wiley & Sons, 1999.
383"
REFERENCES,0.9052823315118397,"[30] Mihaela Rosca, Balaji Lakshminarayanan, and Shakir Mohamed. Distribution matching in
384"
REFERENCES,0.907103825136612,"variational inference. CoRR, abs/1802.06847, 2018.
385"
REFERENCES,0.9089253187613844,"[31] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep
386"
REFERENCES,0.9107468123861566,"unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015.
387"
REFERENCES,0.912568306010929,"[32] William Feller. On the theory of stochastic processes, with particular reference to applications.
388"
REFERENCES,0.9143897996357013,"In Selected Papers I, pages 769–798. Springer, 2015.
389"
REFERENCES,0.9162112932604736,"[33] Yixin Wang, David M. Blei, and John P. Cunningham. Posterior collapse and latent variable
390"
REFERENCES,0.9180327868852459,"non-identifiability. In NeurIPS, 2021.
391"
REFERENCES,0.9198542805100182,"[34] Adji B. Dieng, Yoon Kim, Alexander M. Rush, and David M. Blei. Avoiding latent variable
392"
REFERENCES,0.9216757741347905,"collapse with generative skip models. In AISTATS, 2019.
393"
REFERENCES,0.9234972677595629,"[35] Yewen Li, Chaojie Wang, Zhibin Duan, Dongsheng Wang, Bo Chen, Bo An, and Mingyuan
394"
REFERENCES,0.9253187613843351,"Zhou. Alleviating “posterior collapse” in deep topic models via policy gradient. In NeurIPS,
395"
REFERENCES,0.9271402550091075,"2022.
396"
REFERENCES,0.9289617486338798,"[36] Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders.
397"
REFERENCES,0.930783242258652,"In ICLR, 2016.
398"
REFERENCES,0.9326047358834244,"[37] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of
399"
REFERENCES,0.9344262295081968,"machine learning research, 9(11), 2008.
400"
REFERENCES,0.936247723132969,"[38] Gilbert W Stewart. On the early history of the singular value decomposition. SIAM Review, 35
401"
REFERENCES,0.9380692167577414,"(4):551–566, 1993.
402"
REFERENCES,0.9398907103825137,"[39] Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A. DePristo, Joshua V.
403"
REFERENCES,0.941712204007286,"Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In
404"
REFERENCES,0.9435336976320583,"NeurIPS, 2019.
405"
REFERENCES,0.9453551912568307,"[40] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
406"
REFERENCES,0.9471766848816029,"benchmarking machine learning algorithms. CoRR, abs/1708.07747, 2017.
407"
REFERENCES,0.9489981785063752,"[41] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
408"
REFERENCES,0.9508196721311475,"applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
409"
REFERENCES,0.9526411657559198,"[42] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
410"
REFERENCES,0.9544626593806922,"Master’s thesis, University of Tront, 2009.
411"
REFERENCES,0.9562841530054644,"[43] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.
412"
REFERENCES,0.9581056466302368,"Reading digits in natural images with unsupervised feature learning. 2011.
413"
REFERENCES,0.9599271402550091,"[44] Dan Hendrycks, Mantas Mazeika, and Thomas G. Dietterich. Deep anomaly detection with
414"
REFERENCES,0.9617486338797814,"outlier exposure. In ICLR, 2019.
415"
REFERENCES,0.9635701275045537,"[45] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image
416"
REFERENCES,0.9653916211293261,"detection in neural networks. In ICLR, 2018.
417"
REFERENCES,0.9672131147540983,"[46] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for
418"
REFERENCES,0.9690346083788707,"detecting out-of-distribution samples and adversarial attacks. In NeurIPS, 2018.
419"
REFERENCES,0.970856102003643,"[47] Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K Varshney, and Dawn Song. Anomalous
420"
REFERENCES,0.9726775956284153,"example detection in deep learning: A survey. IEEE Access, 8:132330–132347, 2020.
421"
REFERENCES,0.9744990892531876,"[48] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
422"
REFERENCES,0.97632058287796,"predictive uncertainty estimation using deep ensembles. In NeurIPS, 2017.
423"
REFERENCES,0.9781420765027322,"[49] Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting
424"
REFERENCES,0.9799635701275046,"distributional shifts in the wild. In NeurIPS, 2021.
425"
REFERENCES,0.9817850637522769,"[50] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,
426"
REFERENCES,0.9836065573770492,"2015.
427"
REFERENCES,0.9854280510018215,"[51] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
428"
REFERENCES,0.9872495446265938,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
429"
REFERENCES,0.9890710382513661,"Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
430"
REFERENCES,0.9908925318761385,"Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
431"
REFERENCES,0.9927140255009107,"high-performance deep learning library. In NeurIPS, 2019.
432"
REFERENCES,0.994535519125683,"[52] Ramneet Kaur, Susmit Jha, Anirban Roy, Sangdon Park, Edgar Dobriban, Oleg Sokolsky, and
433"
REFERENCES,0.9963570127504554,"Insup Lee. idecode: In-distribution equivariance for conformal out-of-distribution detection. In
434"
REFERENCES,0.9981785063752276,"AAAI, 2022.
435"
