Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002824858757062147,"Knowledge distillation is widely used as a means of improving the performance of
a relatively simple “student” model using the predictions from a complex “teacher”
model. Several works have shown that distillation signiﬁcantly boosts the student’s
overall performance; however, are these gains uniform across all data subgroups?
In this paper, we show that distillation can harm performance on certain subgroups,
e.g., classes with few associated samples, compared to the vanilla student trained
using the one-hot labels. We trace this behaviour to errors made by the teacher
distribution being transferred to and ampliﬁed by the student model. To mitigate this
problem, we present techniques which soften the teacher inﬂuence for subgroups
where it is less reliable. Experiments on several image classiﬁcation benchmarks
show that these modiﬁcations of distillation maintain boost in overall accuracy,
while additionally ensuring improvement in subgroup performance."
INTRODUCTION,0.005649717514124294,"1
INTRODUCTION"
INTRODUCTION,0.00847457627118644,"Knowledge distillation is a technique for improving the performance of a “student” model using the
predictions from a “teacher” model. At its core, distillation involves replacing the one-hot training
labels with the teacher’s predicted label distribution. Empirically, distillation has proven successful
for model compression (Bucilˇa et al., 2006; Hinton et al., 2015), improving the performance of a
ﬁxed model architecture (Anil et al., 2018; Furlanello et al., 2018), and semi-supervised learning (Ra-
dosavovic et al., 2018). Theoretically, several works (Lopez-Paz et al., 2016; Mobahi et al., 2020;
Tang et al., 2020; Menon et al., 2020; Zhang & Sabuncu, 2020; Ji & Zhu, 2020; Allen-Zhu & Li,
2020; Zhou et al., 2021; Dao et al., 2021) have studied how distillation affects learning. Put together,
both strands of work further the understanding of when and why distillation helps."
INTRODUCTION,0.011299435028248588,"In this paper, we are similarly motivated to better understand the mechanics of distillation, but pose a
slightly different question: does distillation help all data subgroups uniformly? Or, do its overall gains
come at the expense of degradation of performance on certain subgroups? To our knowledge, there
has been no systematic study (empirical or otherwise) of this question. This consideration is topical
given the study of fairness of machine learning algorithms on under-represented subgroups (Hardt
et al., 2016; Buolamwini & Gebru, 2018; Chzhen et al., 2019; Sagawa et al., 2020a)."
INTRODUCTION,0.014124293785310734,"Our ﬁrst ﬁnding is that even in standard settings — e.g., on image classiﬁcation benchmarks such
as CIFAR — distillation can disproportionately harm performance on subgroups deﬁned by the
individual classes (see Figure 1). To discern the source of this behaviour, we ablate the teacher
and student architectures (§3.2), dataset complexity (§3.3), label frequencies (§3.4). These point to
the potential harms of distillation when the teacher makes conﬁdent mispredictions on a subgroup.
We emphasise that this phenomenon is not simply the teacher’s worst class accuracy translating to
student; instead, distillation ampliﬁes worst class imbalance much more than the teacher."
INTRODUCTION,0.01694915254237288,"Having identiﬁed a potential limitation of distillation, we present two simple techniques to remedy it.
These apply per-subgroup mixing weights between the teacher and one-hot labels, and per-subgroup
margins respectively (§4). Intuitively, these limit the inﬂuence of teacher predictions on subgroups it
models poorly. Experiments on image classiﬁcation benchmarks show that these methods typically
maintain a boost in overall accuracy, while ensuring a more equitable improvement across subgroups."
INTRODUCTION,0.01977401129943503,"In sum, this work provides novel insights into distillation performance, with three main contributions:"
INTRODUCTION,0.022598870056497175,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.025423728813559324,"Baseline
Adaptive mixing
Method 1 0 1 2 3 4"
INTRODUCTION,0.02824858757062147,Accuracy improvement
INTRODUCTION,0.031073446327683617,Subgroup
INTRODUCTION,0.03389830508474576,"Average
Worst"
INTRODUCTION,0.03672316384180791,(a) CIFAR-100-LT.
INTRODUCTION,0.03954802259887006,"Baseline
Adaptive mixing
Method 3 2 1 0"
INTRODUCTION,0.0423728813559322,Accuracy improvement
INTRODUCTION,0.04519774011299435,Subgroup
INTRODUCTION,0.0480225988700565,"Average
Worst"
INTRODUCTION,0.05084745762711865,(b) ImageNet.
INTRODUCTION,0.05367231638418079,"Figure 1: Illustration of the potential deleterious effects of distillation on data subgroups. We train a
ResNet-56 teacher on CIFAR-100-LT, a long-tailed version of CIFAR-100 (Cui et al., 2019; Cao
et al., 2019) where some labels have only a few associated samples, and a ResNet-50 teacher on
ImageNet. For each dataset, we self-distill to a student ResNet of the same depth. On CIFAR-100-
LT, as is often observed, distillation helps the overall accuracy over one-hot student training (∼2%
absolute). However, such gains come at signiﬁcant cost on subgroups deﬁned by the individual
classes: on the ten rarest classes, distillation harms performance by ∼1%. Similarly, on ImageNet,
distillation harms the average accuracy of the worst-10 classes by ∼3%. Our proposed techniques
(§4) can roughly preserve the overall accuracy, while boosting subgroup performance."
INTRODUCTION,0.05649717514124294,"(i) we identify a hitherto unexplored issue with distillation, namely, that its improvements in overall
accuracy may come at the expense of harming accuracy on certain subgroups (§3.1). Such
a ﬁnding is topical given the widespread practical use of generic learning paradigms such as
distillation, and the increasing societal applications of learning systems more broadly.
(ii) we ablate potential sources for the above phenomenon (§3.2, §3.3, §3.4), and in the process
identify certain characteristics of data (e.g., skewed label distributions) where it can manifest.
This systematic empirical analysis aims at understanding a non-trivial phenomenon, rather than
empirically comparing different learning methods.
(iii) we propose two simple modiﬁcations of distillation that mitigate the above problem, based on
applying per-subgroup mixing weights and margins (§4); these perform well empirically (§5)."
INTRODUCTION,0.059322033898305086,"While the paper’s analysis is empirical rather than theoretical, we note that our focus is on systematic
analysis aiming at understanding a non-trivial phenomenon, rather than merely empirical comparison:
Section §3 is devoted to carefully understanding the extent of, and causes for, the non-uniform gains
of distillation. This is in line with works which empirically analyse neural network phenomena, e.g.
Zhang et al. (2017); Müller et al. (2019); Nakkiran et al. (2020); Neyshabur et al. (2020)."
BACKGROUND AND RELATED WORK,0.062146892655367235,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.06497175141242938,"Knowledge distillation. Consider a multi-class classiﬁcation problem over instances X and labels
Y = [L]
.= {1, . . . , L}. Given a training set S = {(xn, yn)}N
n=1 drawn from some distribution P, we
seek a classiﬁer h: X →Y that minimises the misclassiﬁcation error Eavg(h)
.= P(h(x) ̸= y). In
practice, one may learn logits f : X →RL to minimise ˆR(f) = 1"
BACKGROUND AND RELATED WORK,0.06779661016949153,"N
PN
n=1 ℓ(yn, f(xn)), where ℓis a
loss function such as the softmax cross-entropy, which for softmax probabilities py(x) ∝exp(fy(x))
is ℓ(y, f(x))
.= −log py(x). One may then classify the sample via h(x) = arg maxy∈[L] fy(x)."
BACKGROUND AND RELATED WORK,0.07062146892655367,"Knowledge distillation (Bucilˇa et al., 2006; Hinton et al., 2015) employs the logits f t : X →RL of a
“teacher” model to train a “student” model. The latter learns logits f s : X →RL to minimise:"
BACKGROUND AND RELATED WORK,0.07344632768361582,"ˆRdist(f) = 1 N N
X n=1"
BACKGROUND AND RELATED WORK,0.07627118644067797,"
(1 −α) · ℓ(yn, f(xn)) + α ·
X"
BACKGROUND AND RELATED WORK,0.07909604519774012,"y′∈[L]
pt
y′(xn) · ℓ(y′, f(xn))

,
(1)"
BACKGROUND AND RELATED WORK,0.08192090395480225,"where α ∈[0, 1]. Here, one converts the teacher logits to probabilities pt : X →∆L for simplex ∆,
e.g. via a softmax transformation pt
y′(x) ∝exp(f t
y′(x)). The second term smooths the student labels"
BACKGROUND AND RELATED WORK,0.0847457627118644,Under review as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.08757062146892655,"based on the teacher’s conﬁdence that they explain the sample. The ﬁrst term includes the original
label, so as to prevent incorrect teacher predictions from overwhelming the student. One further
important trick is temperature scaling of the teacher logits, so that pt
y′(x) ∝exp(T −1 · f t
y′(x)).
Setting T ≫0 makes pt more uniform, thus preventing overconﬁdent predictions (Guo et al., 2017)."
BACKGROUND AND RELATED WORK,0.0903954802259887,"Average versus subgroup performance. The above exposition treats the misclassiﬁcation error
Eavg(h) as the fundamental performance measure of interest. However, suppose the data contains
subgroups G = {1, . . . , G}. Deﬁning the per-subgroup errors errg(h)
.= P(h(x) ̸= y | g), we have
Eavg(h) = P"
BACKGROUND AND RELATED WORK,0.09322033898305085,"g∈G P(g) · errg(h), which may mask errors on samples with P(g) ∼0 (Sagawa et al.,
2020a;b; Sohoni et al., 2020). To this end, one may instead measure the balanced error (Menon
et al., 2013) Ebal(h)
.= P
g∈G
1
|G| · errg(h) which treats the subgroup distribution as uniform, or
the worst-subgroup error (Sagawa et al., 2020a;b; Sohoni et al., 2020) Emax(h)
.= maxg∈G errg(h),
which focusses on the worst-performing subgroup. An intermediary is the average of the k worst-
performing subgroups (Williamson & Menon, 2019): for ith largest per-subgroup error err[i](h),
Etop−k(h)
.= 1"
BACKGROUND AND RELATED WORK,0.096045197740113,"k
Pk
i=1 err[i](h)."
BACKGROUND AND RELATED WORK,0.09887005649717515,"The deﬁnition of subgroups is a domain-speciﬁc consideration. One special case is where each label
deﬁnes a subgroup (i.e., G = Y), and P(y) is skewed. In such long-tail settings (Buda et al., 2017),
classiﬁers with good average performance can perform poorly on “tail” labels where P(y) ∼0."
BACKGROUND AND RELATED WORK,0.1016949152542373,"Related work. There is limited prior study that dissects distillation’s overall gains per subgroup.
Zhao et al. (2020) showed that in incremental learning settings, distillation can be biased towards
recently observed classes. We show that even in ofﬂine settings, distillation can harm certain classes.
Recently, Zhou et al. (2021) studied the standard aggregate performance Eavg of distillation, which
was tied to a certain subset of “regularisation samples”. By contrast, our primary concern is to
understand the subgroup performance of distillation."
BACKGROUND AND RELATED WORK,0.10451977401129943,"Study of the fairness of machine learning algorithms on under-represented data subgroups has
received recent attention (Dwork et al., 2012; Hardt et al., 2016; Buolamwini & Gebru, 2018; Chzhen
et al., 2019). This has prompted dissection of the performance of techniquess such as dimensionality
reduction (Samadi et al., 2018), increasing model capacity (Sagawa et al., 2020a), and selective
classiﬁcation (Jones et al., 2021). We follow the general spirit of such works, studying a more delicate
setting involving two separate models (the student and teacher), each with their own inductive biases.
We present more discussion of related directions in §6. In a related effort, Hooker et al. (2019)
explore how model compression may harm subgroup performance compared to the original model."
BACKGROUND AND RELATED WORK,0.10734463276836158,"3
ARE DISTILLATION’S GAINS UNIFORM?"
BACKGROUND AND RELATED WORK,0.11016949152542373,"We demonstrate that the gains of distillation are not uniform across subgroups: speciﬁcally, consider-
ing subgroups deﬁned by classes, distillation can harm the student’s performance on the “hardest”
few classes (§3.1). To understand the genesis of this problem, we perform ablations (cf. Table 1) that
establish its existence in settings where there are insufﬁcient samples to model certain classes, either
due to the number of classes being large (§3.3), or the class distribution being skewed (§3.4). We
then identify that the student may amplify the teacher’s errors (§3.6). Finally, we corroborate these
results for a more general notion of subgroup in a fairness dataset (§3.5)."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.11299435028248588,"3.1
DISTILLATION HURTS SUBGROUP PERFORMANCE"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.11581920903954802,"To begin, we consider the effect of distillation on a standard image classiﬁcation benchmark, namely,
ImageNet. We employ a self-distillation (Furlanello et al., 2018) setup, with ResNet-34 teacher and
student models, trained with standard hyperparameter choices (see Appendix B). Following Cho &
Hariharan (2019), we use early stopping on the teacher model. We now ask: what is the impact of
distillation on the student’s overall and per-class performance? The ﬁrst question has an expected
answer: distillation improves the student’s average accuracy by +0.4% (see the Baseline setting in
Table 1). Judged by this conventional metric, distillation is thus a success."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.11864406779661017,"A more nuanced picture emerges when we break down the source of the above improvement. We
compute the per-class accuracies for the one-hot and distillation models, to understand how the overall
gains of distillation are distributed. Figure 2 shows that these gains are non-uniform: distillation"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.12146892655367232,Under review as a conference paper at ICLR 2022
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.12429378531073447,"Setting
Dataset
Avg acc
Worst
subgroup acc
Baseline (§3.1)
ImageNet
+0.39
-0.43"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1271186440677966,"Stronger teacher (§3.2)
ImageNet
+0.17
-1.19"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.12994350282485875,"Long tail (§3.4)
CIFAR-100 LT
+2.17
-1.46
ImageNet LT
+0.21
-0.32"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1327683615819209,"Fairness (§3.5)
UCI Adult
+3.10
-5.94"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.13559322033898305,"Reduce #classes (§3.3)
CIFAR-100
+1.93
+3.33
ImageNet-100
+0.09
+0.06"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1384180790960452,"Table 1: Summary of ﬁndings in the ablation anal-
ysis of distillation’s subgroup performance (§3). In
a range of different settings, distillation is seen to
hurt the hardest subgroup accuracy (worst-k class
accuracy or worst subgroup according to an at-
tribute), despite improving the average accuracy
(upper rows). Decreasing number of labels helps
improve the hardest classes (bottom row)."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.14124293785310735,"0
200
400
600
800
1000
Index k 0.05 0.04 0.03 0.02 0.01 0.00"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1440677966101695,Cumulative accuracy gain
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.14689265536723164,"Figure 2: Cumulative gain of ResNet-34 self-
distillation on ImageNet.
For index k, we
compute the gain in average accuracy over
the k worst classes. While average accuracy
(k = 1000) improves by +0.4%, for k ≤40,
distillation harms over the one-hot model (evi-
denced by the negative gain)."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1497175141242938,"Teacher
Student
Average
accuracy
Worst-10
accuracy
EffnNet
Res-50
+0.17
-1.19
EffnNet
Res-34
0.00
-0.80
EffnNet
Res-18
+0.05
-1.60"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.15254237288135594,"Teacher
Student
Average
accuracy
Worst-10
accuracy
Res-50
Res-50
+0.39
-0.43
Res-50
Res-34
+0.39
-2.05
Res-50
Res-18
-0.09
-1.00"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1553672316384181,"Teacher
Student
Average
accuracy
Worst-10
accuracy
Res-34
Res-34
+0.42
-2.60
Res-34
Res-18
+0.15
-3.60
Res-18
Res-18
-0.13
-3.80"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.15819209039548024,"Table 2: Summary of effect of distillation on different teacher and student architectures considered
for the ImageNet dataset. The comparison is with respect to the one-hot (i.e., non-distilled) student.
We ﬁnd that distillation consistently hurts accuracy of the worst 10 classes."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.16101694915254236,"in fact hurts the worst-k class performance for k ≤40. (See Appendix C for a detailed per-class
breakdown.) Thus, distillation may harm the student on classes that it already ﬁnds difﬁcult. At the
same time we note that distillation does improve many classes, such as the classes with relatively
high accuracies from the one-hot student."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1638418079096045,"Given that average accuracy improves, it is worth asking whether the above is a cause for concern:
does it matter that performance on subgroups corresponding to the “hardest” classes suffers? While
ultimately a domain-speciﬁc consideration, in general exacerbating subgroup errors may lead to
issues from the fairness perspective. Indeed, we shall see that distillation can also harm in settings
where the subgroups correspond to sensitive variables; we discuss this further in §3.5."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.16666666666666666,"At this stage, it is apposite to ask whether the above is an isolated ﬁnding, or indicative of a deeper
issue. We thus study each of the following in turn: (i) does the ﬁnding hold in settings beyond
self-distillation? (ii) does the ﬁnding hold for other datasets, or is it simply due to the idiosyncrasies
of ImageNet? (iii) what are some general characteristics of settings where the problem is manifest?"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1694915254237288,"3.2
IS DISTILLATION BIASED BY THE MODEL SIZE OR ARCHITECTURE?"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.17231638418079095,"Having begun with a self-distillation setup, we now demonstrate that similar ﬁndings hold when
the student and teacher architectures differ. Continuing with the ImageNet dataset, in the second
row in Table 1 we report statistics for the overall average accuracy and average accuracy over the
worst 10 classes when distilled ResNet-50 student from a stronger teacher: Efﬁcient-Net (Tan & Le,
2021) teacher achieving 85.7% accuracy. Again, we see improved average accuracy and harmed
Worst subgroup accuracy, composed of the 10 classes with the lowest accuracy according to teacher’s
performance. In Table 2, we report statistics when varying teacher and student architectures. The
detrimental effect of distillation on hard class performance holds across all scenarios: thus, our earlier
results were not speciﬁc to self-distillation."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1751412429378531,"For self-distillation settings, smaller models appear to incur greater losses on the worst-class error.
When distilling between different architectures (e.g., from ResNet-50 to ResNet-18), we observe that
even average accuracy may not improve, as noted in Cho & Hariharan (2019). There is however no"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.17796610169491525,Under review as a conference paper at ICLR 2022
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1807909604519774,"clear trend between the difference in architectures and drop in worst class performance. Finally, we
note that there is little change in the teacher’s and student’s worst-k classe; see Figure 6 (Appendix)."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.18361581920903955,"3.3
IS DISTILLATION BIASED BY A LARGE NUMBER OF CLASSES?"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1864406779661017,"Having seen that ImageNet consistently demonstrates a performance degradation on certain classes,
we now repeat the same analysis on a smaller image classiﬁcation benchmark. We return to the self-
distillation setup, using ResNet-56 models on CIFAR-100. On this dataset, Table 1 shows a (perhaps
more expected) result: distillation boosts both the average and worst-1 class performance. This
indicates that, at a minimum, the behaviour of distillation’s performance gains are problem-speciﬁc;
on CIFAR, distillation appears a complete win for both the average and subgroup accuracy."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.18926553672316385,"One plausible hypothesis is that the tension between average and subgroup performance only mani-
fests on problems with many labels, which might informally be considered “harder”. To conﬁrm this
further while ﬁxing the dataset, we randomly select 10% of classes from the ImageNet dataset, and
only keep examples corresponding to those classes across the train and validation sets. Consistent
with the results for CIFAR-100, we again ﬁnd that the worst-10 class accuracy is not harmed under
distillation (see the right side of Table 1 where we report results on ResNet-34 self-distillation)."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.192090395480226,"The above indicates that for problems with a few, balanced labels, there may not be a tension between
average and worst-subgroup performance under distillation. However, we now show that even for
problems with relatively few labels, one may harm subgroup performance if there is label imbalance."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.19491525423728814,"3.4
IS DISTILLATION BIASED BY CLASS IMBALANCE?"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.1977401129943503,"We now consider a long-tail setting, where the training label distribution P(y) is highly non-uniform.
Following the long-tail learning literature (Cui et al., 2019; Cao et al., 2019; Kang et al., 2020), we
construct “long-tailed” (LT) versions of the above datasets, wherein the training set is down-sampled
so as to achieve a particular label skew. For ImageNet, we use the long-tailed version from Liu et al.
(2019). For other datasets, we down-sample labels to follow P(y = i) ∝
1
µi for constant µ and
i ∈[L] (Cui et al., 2019). The ratio of the most to least frequent class is set to 100."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.20056497175141244,"From the Long tail setting in Table 1, we note that on both CIFAR-100-LT and ImageNet-LT, accuracy
over the hardest classes drops (we report results from self-distillation using ResNet-56 for CIFAR-100
and ResNet-50 ImageNet). The former is particularly interesting, given that the standard CIFAR-100
shows gains amongst the hardest classes. This provides evidence that for some “harder” problems —
e.g., where there are insufﬁciently many samples from which to model a particular class — there may
be a tension between the average and subgroup performance."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2033898305084746,"3.5
BEYOND CLASSES: OTHER CHOICES OF SUBGROUPS"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2062146892655367,"Our analysis thus far has focused on subgroups deﬁned by classes. This choice is natural for long-
tailed problems, where it is important to ensure good model performance on rare classes (Kang
et al., 2020). In other problems, different choices of subgroups may be appropriate. For example, in
problems arising in fairness, one may deﬁne subgroups based on certain sensitive attributes (e.g., sex,
race). In such settings, does one similarly see varying gains from distillation across subgroups?"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.20903954802259886,"We conﬁrm this can indeed hold on the UCI Adult dataset using random forest models (details in
Appendix C.3). This data involves the task of predicting if an individual’s income is ≥50K or not,
and possesses subgroups deﬁned by the individual’s race and sex. Akin to the preceding results, we
ﬁnd that distillation can signiﬁcantly improve overall accuracy, at the expense of degrading accuracy
on certain rare subgroups, e.g., Black women; see Table 1, and Table 8 (Appendix). This further
corroborates our basic observation on the non-uniform distribution of distillation’s gains."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.211864406779661,"A distinct notion of subgroup was recently considered in Zhou et al. (2021), who identiﬁed the
impact of certain “regularisation samples” on distillation. These are a subset of training samples
which were seen to degrade the overall performance of distillation. It is of interest whether such a
subgroup relates to our previously studied subgroups of “hard” classes; e.g., is there an abundance of
regularisation samples in such subgroups, which might explain the poor performance of distillation? In
Appendix C.4, we study the relationship between regularisation samples, and the per-label subgroups"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.21468926553672316,Under review as a conference paper at ICLR 2022
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2175141242937853,"0
1
2
3
4
5
6
7
8
9
Class bucket 0.0 0.2 0.4 0.6 0.8"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.22033898305084745,Accuracy
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2231638418079096,"student
teacher"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.22598870056497175,(a) Accuracy.
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2288135593220339,"0
1
2
3
4
5
6
7
8
9
Class bucket 0 2 4 6 8"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.23163841807909605,Log-loss
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2344632768361582,"student
teacher"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.23728813559322035,(b) Log-loss.
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2401129943502825,"0
1
2
3
4
5
6
7
8
9
Class bucket 0.75 0.50 0.25 0.00 0.25 0.50 0.75"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.24293785310734464,Margin
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2457627118644068,"student
teacher"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.24858757062146894,(c) Margin.
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2514124293785311,"Figure 3: Logit statistics on CIFAR-100 LT, for the teacher and distilled student under a self-
distillation setup (ResNet-56 →ResNet-56). We show the statistics on 10 class buckets: these are
created by sorting the 100 classes according to the teacher accuracy, and then creating 10 groups
of classes. As expected, the student follows the general trend of the teacher model. Strikingly, we
observe that the teacher model tends to systematically conﬁdently mispredict samples in the higher
buckets, thus incurring a negative margin; such misplaced conﬁdence is largely transferred to the
student, whose accuracy suffers on such buckets. Note that we consider statistics on the test set."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2542372881355932,"from our analysis; we ﬁnd that, in general, these may be complementary notions. We further analyze
the effect of the technique proposed in Zhou et al. (2021) on average and subgroup accuracies in §5."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2570621468926554,"3.6
WHY DOES DISTILLATION HURT CERTAIN SUBGROUPS?"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2598870056497175,"The above has established that in a range of scenarios, distillation can hurt performance on subgroups
deﬁned by individual classes. However, a ﬁrm understanding of why this happens remains elusive. To
study this, we consider ResNet-56 self-distillation on CIFAR-100-LT — which showed a stark gap
between the average and subgroup (i.e., worst-1 class) performance — and dissect the logits of the
teacher and distilled student. (See the Appendix for plots where the teacher and student architectures
differ.) Across classes, we seek to understand: (i) how aligned are the student and teacher accuracies?
(ii) how reliable are the models’ probability estimates? (iii) how do the models’ conﬁdences behave?"
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2627118644067797,"For a test1 example (x, y) and predicted label distribution p(x) ∈∆L, we thus compute each
models’ accuracy, log-loss ℓlog(y, p(x)) = −log py(x), and margin (Koltchinskii & Panchenko,
2002) ℓmarg(y, p(x)) = py(x) −maxy′̸=y p′
y(x). Note that the latter may be negative if the model
predicts the incorrect label for the example. Figure 3 shows these metrics on 10 class buckets: these
are created by sorting the 100 classes according to the teacher accuracy, and then creating 10 buckets
of classes. Within each bucket, we compute the average of the metric speciﬁed above."
DISTILLATION HURTS SUBGROUP PERFORMANCE,0.2655367231638418,"Remarkably, for 5 out of 10 class buckets, average margins are negative, suggesting that the teacher is
often wrong yet conﬁdent in predicting these classes. On these buckets, the student accuracy generally
worsens compared to the teacher. Further, log-loss increases across all buckets (including those
where accuracy improves), indicating reduced conﬁdence in the true class of the distilled student.
This points at a potential source of the poorer performance on the worst-1 accuracy. Recall that
the distilled student’s aim is to mimic the teacher’s logits on the training samples. This is a proxy
to the student’s true goal, which is mimicking these logits on test samples, so as to attain similar
generalisation performance as the teacher. When such generalisation happens, the student can thus
be expected to roughly inherit the teacher’s per-class performance; in settings like the above, this
unfortunately implies it will perform poorly on those classes with negative teacher margin."
IMPROVING SUBGROUP PERFORMANCE UNDER DISTILLATION,0.268361581920904,"4
IMPROVING SUBGROUP PERFORMANCE UNDER DISTILLATION"
IMPROVING SUBGROUP PERFORMANCE UNDER DISTILLATION,0.2711864406779661,"We now study simple means of correcting distillation to prevent the degradation of subgroup per-
formance identiﬁed above. These leverage the insight that the behaviour is potentially a result of"
IMPROVING SUBGROUP PERFORMANCE UNDER DISTILLATION,0.2740112994350282,"1The choice of test, rather than train, example is crucial: an overparameterised teacher will likely correctly
predict all training samples, thus rendering the above statistics of limited use. To leverage the insights from the
above analysis in practice, we shall use a holdout set that can be carved out from the training set."
IMPROVING SUBGROUP PERFORMANCE UNDER DISTILLATION,0.2768361581920904,Under review as a conference paper at ICLR 2022
IMPROVING SUBGROUP PERFORMANCE UNDER DISTILLATION,0.2796610169491525,"the teacher conﬁdently mispredicting on some subgroups. In the following, for concreteness and
simplicity, we focus on subgroups that are given by the individual classes."
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.2824858757062147,"4.1
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS"
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.2853107344632768,"In §3.6, we saw that distillation can hurt on classes where the teacher is inherently inaccurate. Such
inaccuracy may in fact be ampliﬁed by the student, which is hardly desirable. An intuitive ﬁx is to
simply rely less on the teacher for classes where it performs poorly, or is otherwise not conﬁdent;
instead, the student can simply fall back onto the one-hot training labels themselves. Formally, for
per-class mixing weights (α1, . . . , αL) ∈[0, 1]L, the student can minimise"
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.288135593220339,"¯Rdist(f) = 1 N N
X n=1"
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.2909604519774011,"h
(1 −αyn) · ℓ(yn, f(xn)) + αyn ·
X"
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.2937853107344633,"y′∈[L]
pt
y′(x) · ℓ(y′, f(xn))
i
.
(2)"
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.2966101694915254,"This objective introduces a mixing weight αy per-class, which allows us to weigh between teacher
predictions and one-hot labels for each class independently. By contrast, in the standard distillation
setup equation 1 we only have a single weight α that is common for all classes."
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.2994350282485876,"How do we choose the weights αy? In the standard distillation objective equation 1, one only needs
to tune a single scalar α, which is amenable to, e.g., cross-validation. By contrast, equation 2 involves
a single scalar for each label, which makes any attempt at grid search infeasible. Following the
observations in §3.6, we propose the following intuitive setting of αy given teacher predictions pt:"
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.3022598870056497,"αy = max
 
0, Ex|y

γavg(y, pt(x))

γavg(y, pt(x))
.= pt
y(x) −
1
L −1 X"
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.3050847457627119,"y′̸=y
pt
y′(x).
(3)"
DISTILLATION WITH ADAPTIVE MIXING WEIGHTS,0.307909604519774,"In words, Equation 3 places greater faith in the teacher model for those classes which it predicts
correctly with conﬁdence, i.e., with large average margin γavg. When this margin is negative — so
that the teacher is incorrect on average, which can occur on classes that are rare in the training set
— we set αy = 0, and completely ignore the teacher predictions. The above requires estimating the
expectation Ex|y [·], which requires access to a labelled sample. This may be done using a holdout
set; we shall follow this in our subsequent experiments."
DISTILLATION WITH PER-CLASS MARGINS,0.3107344632768362,"4.2
DISTILLATION WITH PER-CLASS MARGINS"
DISTILLATION WITH PER-CLASS MARGINS,0.3135593220338983,"Our second approach for improving distillation on harder classes is to leverage recent developments
in long-tail learning, where the goal is to improve performance on rare classes. Speciﬁcally, Khan
et al. (2018); Cao et al. (2019); Tang et al. (2020); Ren et al. (2020); Menon et al. (2020); Wang et al.
(2021) proposed a variant of the softmax cross-entropy with margins ρyy′ between label pairs:"
DISTILLATION WITH PER-CLASS MARGINS,0.3163841807909605,"ℓ(y, f(x)) = log "
DISTILLATION WITH PER-CLASS MARGINS,0.3192090395480226,"1 +
X"
DISTILLATION WITH PER-CLASS MARGINS,0.3220338983050847,"y′̸=y
ρyy′ · efy′(x)−fy(x) "
DISTILLATION WITH PER-CLASS MARGINS,0.3248587570621469,".
(4)"
DISTILLATION WITH PER-CLASS MARGINS,0.327683615819209,"Intuitively, this penalises predicting label y′ instead of y when ρyy′ is large. For training label
distribution π, Cao et al. (2019) proposed to set ρyy′ ∝exp(π−1/4
y
), so that rare labels receive a
higher weight when misclassiﬁed. Khan et al. (2018); Ren et al. (2020); Menon et al. (2020); Wang
et al. (2021) showed gains with ρyy′ ∝
πy′"
DISTILLATION WITH PER-CLASS MARGINS,0.3305084745762712,"πy , so that rare labels are not confused with common ones."
DISTILLATION WITH PER-CLASS MARGINS,0.3333333333333333,"We adapt such techniques to our setting, with the intuition that we ought to increase the student
penalty for misclassifying those “hard” classes that the teacher has difﬁculty modeling. We thus
choose ρyy′ =
αy′"
DISTILLATION WITH PER-CLASS MARGINS,0.3361581920903955,"αy , where αy is the adaptive per-class mixing weight from the previous section. This
discourages the model from confusing “hard” labels y with “easy” labels y′, when αy′ > αy. To
avoid a division by 0 issue, we add a small offset to αy when it becomes 0."
DISTILLATION WITH PER-CLASS MARGINS,0.3389830508474576,"We may understand the effect of Equation 4 by studying how it impacts the Bayes-optimal student
model predictions, i.e., the optimal predictions in the inﬁnite sample limit, and without a model
capacity restriction. We have the following.
Lemma 1. Let ℓbe the loss of Equation 4, with ρyy′
.=
αy′"
DISTILLATION WITH PER-CLASS MARGINS,0.3418079096045198,αy . Let f ∗be the Bayes-optimal student
DISTILLATION WITH PER-CLASS MARGINS,0.3446327683615819,"scores that minimise Rdist(f)
.= Ex(pt(x))⊤ℓ(f(x)). Then, ∀x ∈X, y ∈[L], f ∗
y (x) = log
pt
y(x) αy ."
DISTILLATION WITH PER-CLASS MARGINS,0.3474576271186441,Under review as a conference paper at ICLR 2022
DISTILLATION WITH PER-CLASS MARGINS,0.3502824858757062,"Table 3: Summary of student’s average accuracy using one-hot and distilled labels. Worst k denotes
accuracy over the worst k classes. Global and adaptive temperatures αy selected using a held out dev
set. The proposed AdaAlpha technique improves both mean and worst class accuracy over vanilla
distillation. For AdaMargin on CIFAR-100 LT and ImageNet LT, we observed divergence during
training, presumably due to this method being sensitive to the selection of hyperparameters, which in
turn are estimated on very small number of examples per class."
DISTILLATION WITH PER-CLASS MARGINS,0.3531073446327684,"Dataset
Method
Per-class accuracy statistics
Mean
Worst-1
Worst-10"
DISTILLATION WITH PER-CLASS MARGINS,0.3559322033898305,"CIFAR-100
One-hot
73.31
45.67
52.12
Distillation
75.24
49.00
54.65
AdaAlpha
75.43
49.33
56.42
AdaMargin
75.15
51.33
56.62"
DISTILLATION WITH PER-CLASS MARGINS,0.3587570621468927,"ImageNet
One-hot
76.38
14.00
23.64
Distillation
76.35
13.00
22.02
AdaAlpha
76.57
13.00
23.22
AdaMargin
76.36
14.00
23.20"
DISTILLATION WITH PER-CLASS MARGINS,0.3615819209039548,"Dataset
Method
Per-class accuracy statistics
Mean
Worst-1
Worst-10
Worst-100"
DISTILLATION WITH PER-CLASS MARGINS,0.3644067796610169,"CIFAR-100 LT
One-hot
43.22
0.00
2.33
N/A
Distillation
45.39
0.00
0.87
N/A
AdaAlpha
48.57
0.67
4.20
N/A
AdaMargin*
Training diverges"
DISTILLATION WITH PER-CLASS MARGINS,0.3672316384180791,"ImageNet LT
One-hot
45.41
0.00
0.00
1.39
Distillation
45.98
0.00
0.00
1.10
AdaAlpha
46.15
0.00
0.00
1.08
AdaMargin*
Training diverges"
DISTILLATION WITH PER-CLASS MARGINS,0.3700564971751412,"Lemma 1 illustrates that using per-class margins encourages the student to mimic the teacher
predictions pt(x), but with an important modiﬁcation: we up-weight the probabilities for classes
that the teacher does poorly on (αy ∼0). Intuitively, this makes it easier for the student to improve
performance on classes with small teacher margin."
DISTILLATION WITH PER-CLASS MARGINS,0.3728813559322034,"Relation to existing work. Previous works varied distillation supervision across examples towards
improving average accuracy. Proposals included weighting samples based on the ratio (Tang et al.,
2019; Zhou et al., 2021), and difference (Zhang et al., 2020) between student and teacher score.
Similarly, Zhou et al. (2020) proposed to only apply distillation on samples the teacher gets correct."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.3757062146892655,"5
RESULTS FOR ADAPTIVE DISTILLATION METHODS"
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.3785310734463277,"We now present results that further corroborate the potential non-uniform gains of distillation, and
the ability to mitigate this with the techniques of the previous section. We emphasise here that our
goal is expressly not to improve over the state-of-the-art in distillation techniques; rather, we wish to
verify the key principles identiﬁed in the preceding study, which considers distillation from a novel
angle (i.e., in terms of subgroup rather than average performance)."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.3813559322033898,"Setup. We report results on the datasets used in §3: CIFAR-100, ImageNet; and long-tailed (LT)
versions of the same. For brevity, we report results under self-distillation. (For results with varying
architectures, see the Appendix.) Thus, for each dataset, we train a one-hot teacher ResNet model,
which is distilled to a student ResNet of the same depth. We use ResNet-56 models for CIFAR, and
ResNet-50 models for all other datasets. We employ the same hyper-parameters as used in §3, except
we use non-early stopped teachers for consistency across datasets; see the Appendix for details."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.384180790960452,"We compare: (i) one-hot training of the student (ii) standard distillation, i.e., minimising Equation 1
(iii) AdaAlpha, our proposed distillation objective with adaptive mixing between one-hot and teacher
labels Equation 2, and α as per Equation 3 (iv) AdaMargin, our proposed distillation objective with
adaptive margins (Equation 4), and ρyy′ =
αy′"
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.3870056497175141,"αy . We summarise each method by reporting: (i) the
standard mean accuracy over all classes; (ii) the accuracy over the worst-1 class; and (iii) the mean
accuracy over worst-10 (and worst-100 for the LT datasets) classes."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.3898305084745763,"For the Ada-* methods, per §4, creating the label-dependent αy requires estimating the generalisation
performance of the teacher. To do this, we create a random holdout split of the training set. For
non-LT datasets, we randomly split into 80% (new train) – 20% (dev). For LT datasets, for each class
we hold out k examples into the dev set (k = 50 for Imagenet-LT, k = 20 for CIFAR-100-LT), or
half of examples for a class if the total number of per class examples is at most 2k. We train an initial
teacher on the new train slice of data, and estimate its per-class performance on the holdout dev slice.
These are used to estimate αy as per, e.g., Equation 3."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.3926553672316384,Table 3 summarises the results for all methods. We make the following observations.
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.3954802259887006,Under review as a conference paper at ICLR 2022
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.3983050847457627,"Table 4: Ablations of design choices in the proposed methods: 1) remove distillation signal from the
bottom 10% of classes, according to conﬁdence; 2) randomly shufﬂe per-class α values; 3) weight
distillation based on student and teacher conﬁdence Zhou et al. (2021)."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.4011299435028249,"Dataset
Method
Per-class accuracy statistics
Mean
Worst-1
Worst-10"
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.403954802259887,"CIFAR-100
AdaAlpha
75.52 ± 0.10
49.33 ± 3.09
56.59 ± 0.44
remove hardest 10%
75.40 ± 0.04
48.33 ± 1.89
55.79 ± 1.19
shufﬂe temperatures
74.56 ± 0.93
46.00 ± 1.91
53.10 ± 1.22
Zhou et al. (2021)
75.14 ± 0.29
45.11 ± 1.66
53.89 ± 0.66"
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.4067796610169492,"AdaAlpha improves mean accuracy over vanilla distillation. The proposed AdaAlpha method
consistently and signiﬁcantly improves standard mean accuracy over vanilla distillation. Thus,
AdaAlpha does not sacriﬁce the gains offered by distillation on average class performance, which
is desirable. Other techniques sometimes perform slightly worse than standard distillation on this
metric; however, as we now see, this is compensated by gains on other important dimensions."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.4096045197740113,"AdaAlpha improves worst-accuracy over distillation. The proposed method consistently improves
the worst-class accuracy compared to standard distillation: Thus, the technique largely fulﬁl their
design goal of improving performance on “hard” classes, while not overly sacriﬁcing average-case
performance. In most cases, these improve both the average and worst-class accuracy, indicating that
softening the teacher inﬂuence can be broadly beneﬁcial."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.4124293785310734,"Comparison of AdaAlpha and AdaMargin. In the Appendix, we report per class statistics for
CIFAR-100 LT. AdaMargin ﬂattens both the margin and log-loss distributions, reducing conﬁdence
on the poorly classiﬁed, tail classes. AdaAlpha consistently increases log-loss across classes, and
improves margins on few buckets, leading to a positive margin on one bucket where all other methods
give negative margins. Intuitively, AdaMargin tries to more aggressively control the worst-class
accuracy; when this succeeds, there is a large payoff, but there is also greater risk of overﬁtting."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.4152542372881356,"Additional ablations. We conﬁrm that the success of AdaAlpha is not immediately replicated by
simpler baselines: (i) remove hardest 10%, which removes the distillation loss component on bottom
10% labels according to the per class margins found using Equation 3. It helps analyze whether there
is any additional gain beyond simply removing teacher’s supervision where it is arguably wrong.
(ii) shufﬂe temperatures, which randomly shufﬂes the per-class αy values used in AdaAlpha. This
determines whether the precise choice of which labels to up- or down-weight is important; (iii) the
adaptive distillation scheme of Zhou et al. (2021), where distillation is weighted differently across
examples depending on the teacher and student scores."
RESULTS FOR ADAPTIVE DISTILLATION METHODS,0.4180790960451977,"In Table 4, we ﬁnd that the ﬁrst two methods work worse than the proposed AdaAlpha method,
indicating that the precise choice of which labels to up- or down-weight is important, and that it does
not sufﬁce to merely ignore the teacher on entire subgroups. The adaptive distillation scheme Zhou
et al. (2021) is also not as effective as AdaAlpha; see Appendix for more such results."
DISCUSSION AND OTHER APPROACHES,0.4209039548022599,"6
DISCUSSION AND OTHER APPROACHES"
DISCUSSION AND OTHER APPROACHES,0.423728813559322,"Our goal of ensuring equitable performance across classes can be seen as encouraging fairness across
subgroups deﬁned by the classes. This is subtly different to the classical fairness literature (Calders &
Verwer, 2010; Dwork et al., 2012; Hardt et al., 2016), wherein the subgroups are deﬁned by certain
sensitive attributes. Broadly, fairness techniques attempt to learn models that predict the target label
accurately, but the subgroup label poorly; these are inadmissible for our setting, wherein the two
labels exactly coincide. Ensuring fairness across subgroups deﬁned by the classes has been studied
in Mohri et al. (2019); Williamson & Menon (2019); Sagawa et al. (2020a), who proposed algorithms
to explicitly minimise the worst-class (as opposed to the average) loss. Adapting such algorithms to
the distillation setting is of interest for future work. More broadly, the intent of the analysis in this
paper is to better understand settings where distillation can implicitly hurt certain under-represented
subgroups."
DISCUSSION AND OTHER APPROACHES,0.4265536723163842,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.4293785310734463,ETHICS STATEMENT
ETHICS STATEMENT,0.4322033898305085,"This work studies settings wherein distillation can harm performance on subgroups. The aim of
such a study is to help identify and mitigate potential adverse effects from distillation in practical
applications where fairness considerations are important. Further, our proposed techniques were
shown to improve the subgroup performance of distillation, and thus could be useful for preventing
such adverse behaviour. As a limitation, we considered natural deﬁnitions of such subgroups, but an
important practical consideration is settings where subgroup membership is unknown."
REPRODUCIBILITY STATEMENT,0.4350282485875706,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.4378531073446328,"For all experiments, we used the default hyperparameters from previous works (see Appendix B for
details)."
REFERENCES,0.4406779661016949,REFERENCES
REFERENCES,0.4435028248587571,"Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. CoRR, abs/2012.09816, 2020. URL https://arxiv.org/
abs/2012.09816."
REFERENCES,0.4463276836158192,"Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E. Dahl, and Geoffrey E.
Hinton. Large scale distributed neural network training through online distillation. In International
Conference on Learning Representations, 2018."
REFERENCES,0.4491525423728814,"Cristian Bucilˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings
of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ’06, pp. 535–541, New York, NY, USA, 2006. ACM."
REFERENCES,0.4519774011299435,"Mateusz Buda, Atsuto Maki, and Maciej A. Mazurowski. A systematic study of the class imbalance
problem in convolutional neural networks. arXiv:1710.05381 [cs, stat], October 2017."
REFERENCES,0.4548022598870056,"Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial
gender classiﬁcation. In Sorelle A. Friedler and Christo Wilson (eds.), Conference on Fairness,
Accountability, and Transparency, volume 81 of Proceedings of Machine Learning Research, pp.
77–91, New York, NY, USA, 23–24 Feb 2018. PMLR."
REFERENCES,0.4576271186440678,"Toon Calders and Sicco Verwer. Three Naive Bayes approaches for discrimination-free classiﬁcation.
Data Mining and Knowledge Discovery, 21(2):277–292, 2010."
REFERENCES,0.4604519774011299,"Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Aréchiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing
Systems 32, pp. 1565–1576, 2019."
REFERENCES,0.4632768361581921,"J. H. Cho and B. Hariharan. On the efﬁcacy of knowledge distillation. In 2019 IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), pp. 4793–4801, 2019."
REFERENCES,0.4661016949152542,"Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. Leverag-
ing labeled and unlabeled data for consistent fair binary classiﬁcation. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 32, pp. 12760–12770. Curran Associates, Inc., 2019."
REFERENCES,0.4689265536723164,"Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on
effective number of samples. In CVPR, 2019."
REFERENCES,0.4717514124293785,"Tri Dao, Govinda M Kamath, Vasilis Syrgkanis, and Lester Mackey. Knowledge distillation as
semiparametric inference. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=m4UCf24r0Y."
REFERENCES,0.4745762711864407,"Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Innovations in Theoretical Computer Science Conference (ITCS), pp. 214–226,
2012."
REFERENCES,0.4774011299435028,Under review as a conference paper at ICLR 2022
REFERENCES,0.480225988700565,"Tommaso Furlanello, Zachary Chase Lipton, Michael Tschannen, Laurent Itti, and Anima Anandku-
mar. Born-again neural networks. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pp. 1602–1611,
2018."
REFERENCES,0.4830508474576271,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 1321–
1330, 2017."
REFERENCES,0.4858757062146893,"Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In
Advances in Neural Information Processing Systems (NIPS), December 2016."
REFERENCES,0.4887005649717514,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016."
REFERENCES,0.4915254237288136,"Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
CoRR, abs/1503.02531, 2015."
REFERENCES,0.4943502824858757,"Sara Hooker, Aaron C. Courville, Yann N. Dauphin, and Andrea Frome. Selective brain damage:
Measuring the disparate impact of model pruning. CoRR, abs/1911.05248, 2019. URL http:
//arxiv.org/abs/1911.05248."
REFERENCES,0.4971751412429379,"Guangda Ji and Zhanxing Zhu. Knowledge distillation in wide neural networks: Risk bound, data
efﬁciency and imperfect teacher. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems,
2020."
REFERENCES,0.5,"Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, and Percy Liang. Selective classiﬁcation
can magnify disparities across groups. In International Conference on Learning Representations,
2021."
REFERENCES,0.5028248587570622,"Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yan-
nis Kalantidis. Decoupling representation and classiﬁer for long-tailed recognition. In Eighth
International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.5056497175141242,"Salman H. Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A. Sohel, and Roberto Togneri.
Cost-sensitive learning of deep feature representations from imbalanced data. IEEE Transactions
on Neural Networks and Learning Systems, 29(8):3573–3587, 2018. doi: 10.1109/TNNLS.2017.
2732482."
REFERENCES,0.5084745762711864,"V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization
error of combined classiﬁers. Ann. Statist., 30(1):1–50, 02 2002."
REFERENCES,0.5112994350282486,"Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large-scale
long-tailed recognition in an open world. In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 2537–2546. Computer
Vision Foundation / IEEE, 2019."
REFERENCES,0.5141242937853108,"D. Lopez-Paz, B. Schölkopf, L. Bottou, and V. Vapnik. Unifying distillation and privileged informa-
tion. In International Conference on Learning Representations (ICLR), November 2016."
REFERENCES,0.5169491525423728,"Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017."
REFERENCES,0.519774011299435,"Aditya Krishna Menon, Harikrishna Narasimhan, Shivani Agarwal, and Sanjay Chawla. On the
statistical consistency of algorithms for binary classiﬁcation under class imbalance. In Proceedings
of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21
June 2013, pp. 603–611, 2013."
REFERENCES,0.5225988700564972,"Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and
Sanjiv Kumar. Long-tail learning via logit adjustment, 2020."
REFERENCES,0.5254237288135594,"Hossein Mobahi, Mehrdad Farajtabar, and Peter L. Bartlett. Self-distillation ampliﬁes regularization
in hilbert space, 2020."
REFERENCES,0.5282485875706214,Under review as a conference paper at ICLR 2022
REFERENCES,0.5310734463276836,"Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Interna-
tional Conference on Machine Learning, 2019."
REFERENCES,0.5338983050847458,"Rafael Müller, Simon Kornblith, and Geoffrey E. Hinton. When does label smoothing help? In
Advances in Neural Information Processing Systems 32, pp. 4696–4705, 2019."
REFERENCES,0.536723163841808,"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations, 2020."
REFERENCES,0.53954802259887,"Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.
What is being transferred in trans-
fer learning?
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 512–523. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
0607f4c705595b911a4f3e7a127b44e0-Paper.pdf."
REFERENCES,0.5423728813559322,"Ilija Radosavovic, Piotr Dollár, Ross B. Girshick, Georgia Gkioxari, and Kaiming He. Data distillation:
Towards omni-supervised learning. In 2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 4119–4128, 2018."
REFERENCES,0.5451977401129944,"Jiawei Ren, Cunjun Yu, shunan sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and hongsheng Li. Balanced
meta-softmax for long-tailed visual recognition. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
4175–4186. Curran Associates, Inc., 2020."
REFERENCES,0.5480225988700564,"S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for
group shifts: On the importance of regularization for worst-case generalization. In International
Conference on Learning Representations (ICLR), 2020a."
REFERENCES,0.5508474576271186,"S. Sagawa, A. Raghunathan, P. W. Koh, and P. Liang. An investigation of why overparameterization
exacerbates spurious correlations. In International Conference on Machine Learning (ICML),
2020b."
REFERENCES,0.5536723163841808,"Samira Samadi, Uthaipon Tantipongpipat, Jamie H Morgenstern, Mohit Singh, and Santosh Vempala.
The price of fair pca: One extra dimension. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31, pp. 10976–10987. Curran Associates, Inc., 2018."
REFERENCES,0.556497175141243,"N. Sohoni, J. Dunnmon, G. Angus, A. Gu, and C. Ré. No subclass left behind: Fine-grained
robustness in coarse-grained classiﬁcation problems. In To appear in Conference on Neural
Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.559322033898305,"Mingxing Tan and Quoc V. Le. Efﬁcientnetv2: Smaller models and faster training, 2021."
REFERENCES,0.5621468926553672,"Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H. Chi, and Sagar Jain.
Understanding and improving knowledge distillation. CoRR, abs/2002.03532, 2020."
REFERENCES,0.5649717514124294,"Shitao Tang, Litong Feng, Wenqi Shao, Zhanghui Kuang, Wayne Zhang, and Zheng Lu. Learning
efﬁcient detector with semi-supervised adaptive distillation. In 30th British Machine Vision Con-
ference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019, pp. 215. BMVA Press, 2019. URL
https://bmvc2019.org/wp-content/uploads/papers/0145-paper.pdf."
REFERENCES,0.5677966101694916,"Grant Van Horn and Pietro Perona. The devil is in the tails: Fine-grained classiﬁcation in the wild.
arXiv preprint arXiv:1709.01450, 2017."
REFERENCES,0.5706214689265536,"Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam,
Pietro Perona, and Serge Belongie. The inaturalist species classiﬁcation and detection dataset. In
2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8769–8778, United
States, December 2018. Institute of Electrical and Electronics Engineers (IEEE). ISBN 978-1-5386-
6421-6. doi: 10.1109/CVPR.2018.00914. URL http://cvpr2018.thecvf.com/. 2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2018 ; Conference
date: 18-06-2018 Through 22-06-2018."
REFERENCES,0.5734463276836158,Under review as a conference paper at ICLR 2022
REFERENCES,0.576271186440678,"Jiaqi Wang, Wenwei Zhang, Yuhang Zang, Yuhang Cao, Jiangmiao Pang, Tao Gong, Kai Chen, Ziwei
Liu, Chen Change Loy, and Dahua Lin. Seesaw loss for long-tailed instance segmentation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021."
REFERENCES,0.5790960451977402,"Robert C. Williamson and Aditya Krishna Menon. Fairness risk measures. In Proceedings of the
36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, pp. 6786–6797, 2019."
REFERENCES,0.5819209039548022,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017."
REFERENCES,0.5847457627118644,"Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang, and Yichen Wei. Prime-
aware adaptive distillation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael
Frahm (eds.), Computer Vision – ECCV 2020, pp. 658–674. Springer International Publishing,
2020. ISBN 978-3-030-58529-7."
REFERENCES,0.5875706214689266,"Zhilu Zhang and Mert R. Sabuncu. Self-distillation as instance-speciﬁc label smoothing. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.5903954802259888,"Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and
fairness in class incremental learning. In 2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 13205–13214. IEEE, 2020."
REFERENCES,0.5932203389830508,"Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang.
Rethinking soft labels for knowledge distillation: A bias–variance tradeoff perspective. In Interna-
tional Conference on Learning Representations, 2021."
REFERENCES,0.596045197740113,"Zaida Zhou, Chaoran Zhuge, Xinwei Guan, and Wen Liu. Channel distillation: Channel-wise
attention for knowledge distillation. CoRR, abs/2006.01683, 2020. URL https://arxiv.
org/abs/2006.01683."
REFERENCES,0.5988700564971752,Under review as a conference paper at ICLR 2022
REFERENCES,0.6016949152542372,"A
PROOFS"
REFERENCES,0.6045197740112994,Proof of Lemma 1. We may write
REFERENCES,0.6073446327683616,"ℓ(y, f(x)) = log "
REFERENCES,0.6101694915254238,"1 +
X y′̸=y αy′"
REFERENCES,0.6129943502824858,"αy
· efy′(x)−fy(x)   = log "
REFERENCES,0.615819209039548,"1 +
X"
REFERENCES,0.6186440677966102,"y′̸=y
eln αy′−ln αy · efy′(x)−fy(x)   = log "
REFERENCES,0.6214689265536724,"1 +
X"
REFERENCES,0.6242937853107344,"y′̸=y
e
¯
fy′(x)−¯
fy(x)  "
REFERENCES,0.6271186440677966,"= −log
e ¯
fy(x)
P"
REFERENCES,0.6299435028248588,"y′∈[L] e
¯
fy′(x) ,"
REFERENCES,0.632768361581921,"where ¯fy(x)
.= fy(x) + ln αy. The population loss is"
REFERENCES,0.635593220338983,"Rdist(f) = Ex

(pt(x))⊤ℓ(f(x))
"
REFERENCES,0.6384180790960452,"= Ex

KL(pt(x) ∥¯ps(x))

,"
REFERENCES,0.6412429378531074,"where ¯ps
y(x) ∝exp( ¯fy(x)). Thus, at optimality we must have ¯ps(x) = pt(x), or ¯fy(x) = log pt
y(x)."
REFERENCES,0.6440677966101694,"By deﬁnition of ¯f, we thus see that fy(x) = log pt
y(x) −log αy = log
pt
y(x) αy ."
REFERENCES,0.6468926553672316,"B
DETAILS OF EXPERIMENTS"
REFERENCES,0.6497175141242938,"B.1
ARCHITECTURE"
REFERENCES,0.652542372881356,"We use ResNet with batch norm (He et al., 2016) for all our experiments with the following conﬁgura-
tions. For CIFAR, we experiment with ResNet-56 and ResNet-32. For ImageNet, we use ResNet-50.
We list the architecture conﬁgurations in terms of (nlayer, nﬁlter, stride) corresponding to each ResNet
block in Table 5."
REFERENCES,0.655367231638418,"Architecture
Conﬁguration: [(nlayer, nﬁlter, stride)]"
REFERENCES,0.6581920903954802,"CIFAR ResNet-32
[(5, 16, 1), (5, 32, 2), (5, 64, 2)]
CIFAR ResNet-56
[(9, 16, 1), (9, 32, 2), (9, 64, 2)]
ImageNet ResNet-18
[(2, 64, 1), (2, 128, 2), (2, 256, 2), (2, 512, 2)]
ImageNet ResNet-34
[(3, 64, 1), (4, 128, 2), (6, 256, 2), (3, 512, 2)]
ImageNet ResNet-50
[(3, 64, 1), (4, 128, 2), (6, 256, 2), (3, 512, 2)]*"
REFERENCES,0.6610169491525424,"Table 5: ResNet Architecture conﬁgurations used in our experiments (He et al., 2016). [*] Note that
ImageNet ResNet-50 uses larger blocks with 3 convolutional layers per residual block compared to
ResNet-18 and 34. We refer to He et al. (2016) for more details."
REFERENCES,0.6638418079096046,"B.2
TRAINING SET"
REFERENCES,0.6666666666666666,"For all datasets, we train using SGD and weight decay 10−4 for CIFAR, and 0.5 × 10−4 for Imagenet
datasets. We have the following dataset speciﬁc settings."
REFERENCES,0.6694915254237288,"CIFAR-100. We train for 450 epochs with an initial learning rate of 1.0, with a linear warmup in the
ﬁrst 15 epochs, and an annealed learning rate schedule. We drop the learning rate by a factor of 10 at
epochs number: 200, 300 and 400. We use a mini-batch size of 1024. We use SGD with Nesterov
momentum of 0.9."
REFERENCES,0.672316384180791,Under review as a conference paper at ICLR 2022
REFERENCES,0.6751412429378532,"0
1
2
3
4
5
6
7
8
9
Tail class sub-bucket 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.6779661016949152,Accuracy Bottom 10%
REFERENCES,0.6807909604519774,"student distill
student one-hot
teacher (early stop)"
REFERENCES,0.6836158192090396,(a) Accuracy (tail classes).
REFERENCES,0.6864406779661016,"0
1
2
3
4
5
6
7
8
9
Tail class sub-bucket 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
REFERENCES,0.6892655367231638,Log-loss Bottom 10%
REFERENCES,0.692090395480226,"student distill
student one-hot
teacher (early stop)"
REFERENCES,0.6949152542372882,(b) Log-loss (tail classes).
REFERENCES,0.6977401129943502,"0
1
2
3
4
5
6
7
8
9
Tail class sub-bucket 0.2 0.1 0.0 0.1"
REFERENCES,0.7005649717514124,Margin
REFERENCES,0.7033898305084746,"student distill
student one-hot
teacher (early stop)"
REFERENCES,0.7062146892655368,(c) Margin (tail classes).
REFERENCES,0.7090395480225988,"Figure 4: Logit statistics for ResNet-50 self-distillation on ImageNet, for the (early-stopped) teacher,
self-distilled student, and one-hot (non-distilled) student. Per Figure 3, we ﬁrst create 10 class buckets.
We zoom in on the “tail” bucket (comprising the 100 “hardest” classes), and further split them into
10 “tail sub-buckets”. As in Figure 3, the teacher is seen to conﬁdently mispredict most samples on
the last few buckets, with such misplaced conﬁdence being transferred to the student."
REFERENCES,0.711864406779661,"For our distillation experiments we train only with the cross-entropy objective against the teacher’s
logits. For each method we ﬁnd the best temperature from the list of values: {1, 2, 3, 4, 5}."
REFERENCES,0.7146892655367232,"ImageNet. We train for 90 epochs with an initial learning rate of 0.8, with a linear warmup in the
ﬁrst 5 epochs, and an annealed learning rate schedule. We drop the learning rate by a factor of 10 at
epochs number: 30, 60 and 80. We use a mini-batch size of 1024."
REFERENCES,0.7175141242937854,"For our distillation experiments we train with the distillation objective as deﬁned in Equation 1 setting
α = 0.2. For each method we ﬁx the temperature to 0.9."
REFERENCES,0.7203389830508474,"Long-tail (LT) datasets. We follow setup as in the non-long tail version, except for the learning rate
schedule, which we change to follow the cosine schedule (Loshchilov & Hutter, 2017)."
REFERENCES,0.7231638418079096,"C
ADDITIONAL EXPERIMENTS"
REFERENCES,0.7259887005649718,We present additional experiments to those in the body.
REFERENCES,0.7288135593220338,"C.1
FURTHER VARYING DATASETS AND MODEL ARCHITECTURES"
REFERENCES,0.731638418079096,"On Imagenet, we summarise statistics for three models: the early-stopped teacher, distilled student,
and the one-hot (non-distilled) student. As with CIFAR-100-LT, we sort classes by teacher accuracy,
and bucket them into 10 groups. Owing to the larger number of labels, we further zoom into the “tail”
bucket (comprising the 100 “hardest” classes), and split them into 10 sub-buckets. From Figure 4,
the distilled student performs worse than its one-hot counterpart on the last bucket; this is in keeping
with our results in Table 1."
REFERENCES,0.7344632768361582,"Figure 5 shows logit statistics for additional settings to considered in the body. On ImageNet-LT, e.g.,
we see again that the margin of the teacher model systematically worsens and becomes negative on
the hardest classes."
REFERENCES,0.7372881355932204,"In Table 6 we report results from the inherently long-tailed iNaturalist 2018 dataset (Van Horn &
Perona, 2017). Our observations made for other considered datasets hold: adaptive margin method
improves over both one hot and plain distillation in terms of the worst class accuracy. We also
observe, how the average accuracy improves."
REFERENCES,0.7401129943502824,"C.2
LOGIT PLOTS UNDER ADA-* METHODS"
REFERENCES,0.7429378531073446,"Figure 7 shows the logit statistics under the proposed AdaMargin and AdaAlpha methods on CIFAR-
100 LT. We see that AdaMargin can generally improve the student margin and accuracy on the hardest
classes, while also reducing the log-loss. This conﬁrms that the gains of the method come from
improving behaviour of the scores on these hard classes."
REFERENCES,0.7457627118644068,Under review as a conference paper at ICLR 2022
REFERENCES,0.748587570621469,"0
1
2
3
4
5
6
7
8
9
Class bucket 0.0 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.751412429378531,Log-loss
REFERENCES,0.7542372881355932,CIFAR-100 res56 -> res56 [T = 3.0]
REFERENCES,0.7570621468926554,"student distill
student one-hot
teacher"
REFERENCES,0.7598870056497176,"0
1
2
3
4
5
6
7
8
9
Class bucket 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.7627118644067796,Margin
REFERENCES,0.7655367231638418,CIFAR-100 res56 -> res56 [T = 3.0]
REFERENCES,0.768361581920904,"student distill
student one-hot
teacher"
REFERENCES,0.7711864406779662,"0
1
2
3
4
5
6
7
8
9
Class bucket 0.0 0.5 1.0 1.5 2.0"
REFERENCES,0.7740112994350282,Log-loss
REFERENCES,0.7768361581920904,Imagenet res50 -> res50 [T = 0.9]
REFERENCES,0.7796610169491526,"student distill
student one-hot
teacher"
REFERENCES,0.7824858757062146,"0
1
2
3
4
5
6
7
8
9
Class bucket 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.7853107344632768,Margin
REFERENCES,0.788135593220339,Imagenet res50 -> res50 [T = 0.9]
REFERENCES,0.7909604519774012,"student distill
student one-hot
teacher"
REFERENCES,0.7937853107344632,"0
1
2
3
4
5
6
7
8
9
Class bucket 0 1 2 3 4 5 6"
REFERENCES,0.7966101694915254,Log-loss
REFERENCES,0.7994350282485876,Imagenet-LT res50 -> res18 [T = 0.9]
REFERENCES,0.8022598870056498,"student distill
student one-hot
teacher"
REFERENCES,0.8050847457627118,"0
1
2
3
4
5
6
7
8
9
Class bucket 0.4 0.2 0.0 0.2 0.4 0.6"
REFERENCES,0.807909604519774,Margin
REFERENCES,0.8107344632768362,Imagenet-LT res50 -> res18 [T = 0.9]
REFERENCES,0.8135593220338984,"student distill
student one-hot
teacher"
REFERENCES,0.8163841807909604,"Figure 5: Logit statistics for the teacher, student with one-hot labels, and student with distilled labels
across: datasets and architectures."
REFERENCES,0.8192090395480226,"0.0
0.2
0.4
0.6
0.8
1.0
One-hot accuracy 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8220338983050848,Distilled accuracy
REFERENCES,0.8248587570621468,"0.2
0.4
0.6
0.8
1.0
One-hot accuracy 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.827683615819209,Distilled accuracy
REFERENCES,0.8305084745762712,"Figure 6: Per-class accuracies for one-hot and self-distilled ResNet-18 (left) and ResNet-34 (right) on
ImageNet. The diagonal denotes classes where both models achieve the same accuracy. Distillation
tends to worsen performance on “hard” classes for the one-hot model, i.e., those with low accuracy
(red rectangle)."
REFERENCES,0.8333333333333334,"C.3
RESULTS ON ADULT DATASET"
REFERENCES,0.8361581920903954,"We report the results of an experiment on the UCI Adult dataset. This data comprises ∼48K
examples, with the target being a binary label denoting whether or not an individual has income
≥50K. The data is mildly imbalanced, with 24% of samples being positive."
REFERENCES,0.8389830508474576,"Inspired by Dao et al. (2021), we consider a random forest based distillation setup: we use a teacher
model that is a random forest classiﬁer comprising 500 trees with a maximum depth of 20, and a
student model that is a random forest regressor comprising 1 tree with a maximum depth of 20. The
teacher model achieves a test (balanced) accuracy of 81.8%."
REFERENCES,0.8418079096045198,Under review as a conference paper at ICLR 2022
REFERENCES,0.844632768361582,"Method
Per-class accuracy statistics
Mean
Worst 20
Top 20%
∆20"
REFERENCES,0.847457627118644,"One-hot
53.00
5.00
100.00
48.00
Distill
52.67
5.00
100.00
47.67
AdaMargin
53.33
8.33
98.33
45.00
AdaAlpha avg
54.67
13.33
100.00
41.33"
REFERENCES,0.8502824858757062,"Table 6: Self-distillation experiments (from Resnet-50 to Resnet-50) on the iNaturalist dataset Van
Horn et al. (2018) with student’s average accuracy using one-hot and distilled labels. Worst 20
denotes accuracy averaged over worst 20 classes ∆20 denotes the difference between the mean
accuracy and the worst 20 classiﬁed classes. The proposed AdaMargin technique improves mean and
worst-class accuracy over both one-hot training and standard distillation."
REFERENCES,0.8531073446327684,"0
1
2
3
4
5
6
7
8
9
Class bucket 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.8559322033898306,Accuracy
REFERENCES,0.8587570621468926,"Model
student distill AdaAlpha
student distill AdaMargin
student distill standard
teacher"
REFERENCES,0.8615819209039548,(a) Accuracy.
REFERENCES,0.864406779661017,"0
1
2
3
4
5
6
7
8
9
Class bucket 0 2 4 6 8"
REFERENCES,0.867231638418079,Log-loss
REFERENCES,0.8700564971751412,"student distill AdaAlpha
student distill AdaMargin
student distill standard
teacher"
REFERENCES,0.8728813559322034,(b) Log-loss.
REFERENCES,0.8757062146892656,"0
1
2
3
4
5
6
7
8
9
Class bucket 0.75 0.50 0.25 0.00 0.25 0.50 0.75"
REFERENCES,0.8785310734463276,Margin
REFERENCES,0.8813559322033898,"student distill AdaAlpha
student distill AdaMargin
student distill standard
teacher"
REFERENCES,0.884180790960452,(c) Margin).
REFERENCES,0.8870056497175142,"Figure 7: Logit statistics for ResNet-56 self-distillation on CIFAR-100 LT, for the teacher, self-
distilled student, and our adaptive methods. Per Figure 3, we create 10 class buckets. AdaMargin
ﬂattens both the margin and log-loss distributions. AdaAlpha increases log loss across classes, while
improving margins on few buckets, including ﬂipping the bucket 5 to have positive margin."
REFERENCES,0.8898305084745762,"Table 7: Difference between distillation and one-hot performance on Adult dataset. Here, subgroups
are deﬁned by the sex and label. ∆refers to the difference between the distilled and one-hot student’s
accuracy on the subgroup."
REFERENCES,0.8926553672316384,"sex
label
∆"
REFERENCES,0.8954802259887006,"Male
0
-2.222
Female
0
0.393
Female
1
2.373
Male
1
8.384"
REFERENCES,0.8983050847457628,"We perform distillation by feeding the student model the teacher’s prediction scores, mixed in with the
binary training labels with a weight α = 0.9. Distillation improves the student’s overall (balanced)
accuracy signiﬁcantly, from 76.2% to 79.3%. However, this gain is not distributed uniformly: using
per-label subgroups, we ﬁnd that distillation helps the positive class by +7.4%, but hurts the negative
class by −1.2%. While by itself suggestive of asymmetry in distillation performance, the data admits
an arguably more natural subgroup creation, based on available sex and sex features. For example,
we ﬁnd that amongst low-income males, distillation hurts by −2.2%; further restricting to those who
are Asian Paciﬁc-Islander, the degradation is −5.9%. This conﬁrms that in scenarios where fairness
may be a consideration, a naïve application of distillation may be inadmissible."
REFERENCES,0.9011299435028248,"C.4
ANALYSIS OF REGULARISATION SAMPLES"
REFERENCES,0.903954802259887,"Recently, Zhou et al. (2021) proposed the notion of regularisation samples to understand how
distillation’s performance can be improved. In brief, such samples correspond to cases where the
teacher’s prediction on the training label is less than the distilled student’s prediction on this label;
these may be shown to correspond to cases where a certain notion of “variance reduction” dominates"
REFERENCES,0.9067796610169492,Under review as a conference paper at ICLR 2022
REFERENCES,0.9096045197740112,"Table 8: Difference between distillation and one-hot performance on Adult dataset. Here, subgroups
are deﬁned by the sex, race, and label. ∆refers to the difference between the distilled and one-hot
student’s accuracy on the subgroup."
REFERENCES,0.9124293785310734,"race
sex
label
∆"
REFERENCES,0.9152542372881356,"Amer-Indian-Eskimo
Female
1
-66.667
Asian-Pac-Islander
Male
0
-5.941
Other
Male
0
-4.347
Black
Female
1
-2.381
White
Male
0
-2.248
Black
Male
0
-1.639
Black
Female
0
-0.140
Asian-Pac-Islander
Female
0
0.000
White
Female
0
0.388
Other
Female
0
2.439
White
Female
1
2.724
Amer-Indian-Eskimo
Female
0
6.349
Amer-Indian-Eskimo
Male
0
6.493
Asian-Pac-Islander
Female
1
7.692
White
Male
1
7.796
Black
Male
1
9.489
Other
Male
1
15.000
Asian-Pac-Islander
Male
1
19.626
Other
Female
1
20.000
Amer-Indian-Eskimo
Male
1
25.000"
REFERENCES,0.9180790960451978,"a notion of “bias reduction”. Given our analysis above of the asymmetric effects of distillation on
certain subgroups, it is natural to consider whether or not these relate to the presence of regularisation
samples in these groups."
REFERENCES,0.9209039548022598,"Figure 8(a) visualises the distribution of regularisation samples inside subgroups deﬁned by 10 label
buckets. where the labels are sorted in descending order of label frequency. Here, we compare the
predicted probabilities of the teacher and ﬁnal distilled student models on all training samples (as
was done in the analysis of Zhou et al. (2021)). Interestingly, we see that the tail buckets tend to have
very few regularisation samples; i.e., for rare labels, the teacher prediction on the training label is
generally higher than that of the distilled student model. We conﬁrm this in Figure 8(b)."
REFERENCES,0.923728813559322,"While the analysis of Zhou et al. (2021) was primarily for training samples — since the aim in
identifying regularisation samples was to mitigate their inﬂuence during training — we may also
identify the breakdown of such samples on test data. Figure 9(a) shows that, compared to the training
set, there are in absolute terms more such samples across nearly every label bucket; however, there is
again no clear correlation between the label bucket and the fraction of such samples. In particular,
the tail bucket is again the one with the fewest regularisation samples. This is corroborated by the
probability scores of the teacher and student in Figure 9(b)."
REFERENCES,0.9265536723163842,"Overall, this results suggest that the existing notion of regularisation samples may not, by themselves,
be sufﬁcient to predict the poor performance of distillation on certain subgroups deﬁned by labels."
REFERENCES,0.9293785310734464,"C.5
IMPACT OF REPEATED DISTILLATION"
REFERENCES,0.9322033898305084,"In the body, we showed that performing distillation once can harm worst-class accuracy. However,
what is the effect of repeating this process, and distilling using the resulting student as a new teacher?
Does the worst-class accuracy get further harmed?"
REFERENCES,0.9350282485875706,"Table 9 shows that on CIFAR-100 LT, repeating distillation can indeed harm worst-class performance,
even though average performance remains roughly similar. This further highlights the potential
tension between average and worst-case performance under distillation."
REFERENCES,0.9378531073446328,Under review as a conference paper at ICLR 2022
REFERENCES,0.940677966101695,"0
1
2
3
4
5
6
7
8
9
Label bucket 0 10 20 30 40 50 60 70 80"
REFERENCES,0.943502824858757,% of regularisation samples
REFERENCES,0.9463276836158192,"(a) Distribution of regularisation samples
per label bucket."
REFERENCES,0.9491525423728814,"0
1
2
3
4
5
6
7
8
9
Label bucket 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9519774011299436,Probability
REFERENCES,0.9548022598870056,variable
REFERENCES,0.9576271186440678,"Teacher
Student"
REFERENCES,0.96045197740113,"(b) Teacher versus distilled student probail-
ities on training label."
REFERENCES,0.963276836158192,"Figure 8: Study of regularisation samples on training set, CIFAR-100 LT."
REFERENCES,0.9661016949152542,"0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
9.0
Label bucket 0 10 20 30 40 50 60 70"
REFERENCES,0.9689265536723164,% of regularisation samples
REFERENCES,0.9717514124293786,"(a) Distribution of regularisation samples
per label bucket."
REFERENCES,0.9745762711864406,"0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
9.0
Label bucket 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
REFERENCES,0.9774011299435028,Probability
REFERENCES,0.980225988700565,variable
REFERENCES,0.9830508474576272,"Teacher
Student"
REFERENCES,0.9858757062146892,"(b) Teacher versus distilled student probail-
ities on test label."
REFERENCES,0.9887005649717514,"Figure 9: Study of regularisation samples on test set, CIFAR-100 LT."
REFERENCES,0.9915254237288136,"Method
Per-class accuracy statistics
Mean
Worst 10
Top 10%
∆10"
REFERENCES,0.9943502824858758,"One-hot
44.16
3.00
87.70
41.16
Distillation 1×
45.49
0.90
88.10
44.59
Distillation 2×
45.22
0.00
88.40
45.22
Distillation 3×
44.80
0.00
87.60
44.80"
REFERENCES,0.9971751412429378,"Table 9: Results of repeated distillation on CIFAR-100 LT. Using a distilled student as teacher for a
subsequent round of distillation is seen to further hurt worst-class accuracy."
