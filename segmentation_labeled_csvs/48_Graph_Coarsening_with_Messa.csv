Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.010869565217391304,"Graph coarsening reduces the size of a large graph to decrease computational load
and memory footprint, while preserving some of its key properties. For instance,
training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic
savings in time and memory. However, GNNs rely on the Message-Passing (MP)
paradigm, and classical spectral preservation guarantees for graph coarsening
do not directly lead to theoretical guarantees when performing naive message-
passing on the coarsened graph. In this work, we propose a new message-passing
operation specific to coarsened graphs, which exhibit theoretical guarantees on
the preservation of the propagated signal. We conduct node classification tasks
on synthetic and real data and observe improved results compared to performing
naive message-passing on the coarsened graph."
INTRODUCTION,0.021739130434782608,"1
Introduction"
INTRODUCTION,0.03260869565217391,"In recent years, several applications in data science and machine learning have produced large-scale
graph data [2, 10]. To handle such massive graphs, researchers have developed general-purpose
graph reduction methods [1], such as graph coarsening [3, 16]. It consists in producing a small
graph from a large graph while retaining some of its key properties,and starts to play an increasingly
prominent role in machine learning applications [3]."
INTRODUCTION,0.043478260869565216,"Graph Neural Networks.
Machine Learning on graphs is now largely done by Graph Neural
Networks (GNNs) [2, 14, 19]. GNNs are deep architectures on graph that rely on the Message-
Passing (MP) paradigm [8]: at each layer, the representation Hl
i ∈Rdl of each node 1 ≤i ≤N, is
updated by aggregating and transforming the representations of its neighbours at the previous layer
{Hl−1
j
}j∈N(i), where N(i) is the neighborhood of i. In most examples, this aggregation can be
represented as a multiplication of the node representation matrix Hl−1 ∈RN×dl−1 by a propagation
matrix S ∈RN×N related to the graph structure, followed by a fully connected transformation. That
is, starting with initial node features H0, the GNN Φθ outputs after k layers:"
INTRODUCTION,0.05434782608695652,"Hl = σ

SHl−1θl

,
Φθ(H0, S) = Hk ,
(1)"
INTRODUCTION,0.06521739130434782,"where σ is an activation function applied element-wise (often ReLU), θl ∈Rdl−1×dl are learned
parameters and θ = {θ1, . . . , θk}. We emphasize here the dependency of the GNN on the propagation
matrix S. Classical choices include mean aggregation S = D−1A or the normalized adjacency
S = D−1"
INTRODUCTION,0.07608695652173914,2 AD−1
INTRODUCTION,0.08695652173913043,"2 , with A the adjacency matrix of the graph and D the diagonal matrix of degrees.
An interesting example is the Simplified Graph Convolution (SGC) model [20], which consists in
removing all the non-linearity (σ = id). SGC reaches quite good performances when compared to
GNNs and due to its simplicity, it has been extensively employed in theoretical analyses [13, 22]."
INTRODUCTION,0.09782608695652174,"In this paper, we consider graph coarsening as a preprocessing step to downstream tasks [6, 11],
in opposition to e.g. graph pooling [21] within a GNN itself, which is often data-driven and fully
differentiable [21]. A primary question is the following: after coarsening, is training a GNN on a
coarsened graph provably close to training it on the original graph?"
INTRODUCTION,0.10869565217391304,"There are many criteria to measure the quality of graph coarsening algorithms [3, 5, 16]. A classical
objective is the preservation of spectral properties of the graph Laplacian [1, 4, 12, 16, 17]. Loukas"
INTRODUCTION,0.11956521739130435,Graph Coarsening with Message-Passing Guarantees
INTRODUCTION,0.13043478260869565,"[16] materializes this by the so-called Restricted Spectral Approximation (RSA, see Sec. 2) property.
Surprisingly, the RSA does not generally lead to guarantees on the MP process at the core of GNNs,
even for very simple signals. In this paper, we address this problem by defining a new propagation
matrix SMP
c
specific to coarsened graphs, which translate the RSA bound to MP guarantees. The
proposed matrix SMP
c
can be computed for any given coarsening and is not specific to the coarsening
algorithm used to produce it , as long as it produces coarsenings with RSA guarantees. To our
knowledge, the only previous work to propose a new propagation matrix for coarsened graphs is [11],
where the authors obtain guarantees for a specific GNN model (APPNP [15]), which is quite different
from generic MP."
INTRODUCTION,0.14130434782608695,"Notations.
For a matrix Q ∈Rn×N, the matrix Q+ ∈RN×n is its pseudo-inverse. For a symmetric
positive semi-definite (p.s.d.) matrix L ∈RN×N, and x ∈RN we denote by ∥x∥L =
√"
INTRODUCTION,0.15217391304347827,"x⊤Lx
the Mahalanobis semi-norm associated to L. For a matrix P ∈RN×N, we denote by ∥P∥=
max∥x∥=1 ∥Px∥the operator norm of P, and ∥P∥L = ∥L
1
2 PL−1"
INTRODUCTION,0.16304347826086957,"2 ∥. For a subspace R, we say that
a matrix P is R-preserving if x ∈R implies Px ∈R. Finally, for a matrix X ∈RN×d, we denote
its columns by X:,i, and define ∥X∥:,L = P"
INTRODUCTION,0.17391304347826086,"i ∥X:,i∥L."
LIT REVIEW,0.18478260869565216,"2
Background on Graph Coarsening"
LIT REVIEW,0.1956521739130435,"A graph G with N nodes is described by its weighted adjacency matrix A ∈RN×N. L ∈RN×N
refers to the symmetric p.s.d. Laplacian of the graph, which can either be the combinatorial Laplacian
L = D −A, or the symmetric normalized Laplacian L = IN −D−1"
LIT REVIEW,0.20652173913043478,2 AD−1
LIT REVIEW,0.21739130434782608,"2 . We denote by
L ∈RN×N a notion of symmetric p.s.d. Laplacian of the graph: classical choices include the
combinatorial Laplacian L = D −A or the symmetric normalized Laplacian L = IN −D−1"
LIT REVIEW,0.22826086956521738,2 AD−1 2 .
LIT REVIEW,0.2391304347826087,"Coarsening matrix.
A coarsening algorithm takes a graph G with N nodes, and produces a
coarsened graph Gc with n < N nodes. Intuitively, nodes in G are grouped in “super-nodes” in Gc,
This mapping can be represented via a coarsening matrix Q ∈Rn×N:"
LIT REVIEW,0.25,"Q =
Qki > 0
if the i-th node of G is mapped to the k-th super-node of Gc
Qki = 0
otherwise"
LIT REVIEW,0.2608695652173913,"The lifting matrix is the pseudo-inverse of the coarsening matrix Q+, and plays the role of inverse
mapping from the coarsened graph to the original one. The coarsening ratio r is: r = 1 −n"
LIT REVIEW,0.2717391304347826,"N . In this
paper, as in the majority of the literature [16], we consider only well-mapped coarsenings, where
nodes in G are mapped to a unique node in Gc (i.e. Q has exactly one non-zero value per column)."
LIT REVIEW,0.2826086956521739,"Restricted Spectral Approximation.
Preserving spectral properties is key in graph coarsening.
For example, this is formalized in [16] with the Restricted Spectral Approximation (RSA), which
measures how much the projection Π = Q+Q is close to the identity for a class of signals. More
precisely, for a signal x ∈RN over the nodes of G, xc ∈Rn and ˜x ∈RN, where"
LIT REVIEW,0.29347826086956524,"xc = Qx,
˜x = Q+xc = Πx,
(2)"
LIT REVIEW,0.30434782608695654,are the coarsened and re-lifted signals. The RSA constant defined below quantifies the RSA.
LIT REVIEW,0.31521739130434784,"Definition 1 (Restricted Spectral Approximation constant) Consider a subspace R ⊂RN, a
Laplacian L, a coarsening matrix Q and its corresponding projection operator Π = Q+Q. The RSA
constant ϵL,Q,R is defined as"
LIT REVIEW,0.32608695652173914,"ϵL,Q,R =
sup
x∈R,∥x∥L=1
∥x −Πx∥L
(3)"
LIT REVIEW,0.33695652173913043,"In other words, the RSA constant measures how much signals in R are preserved by the coarsening-
lifting operation, with respect to the norm ∥·∥L. In practice, R is often chosen a the subspace spanned
by the first eigenvectors of L ordered by increasing eigenvalue. Given some R and Laplacian L, the
goal of a coarsening algorithm is then to produce a coarsening Q with the smallest RSA constant
possible. While the “best” coarsening arg minQ ϵL,Q,R is generally computationally unreachable,
there are many possible heuristic algorithms, often based on greedy merging of nodes [16]."
LIT REVIEW,0.34782608695652173,Graph Coarsening with Message-Passing Guarantees
IMPLEMENTATION/METHODS,0.358695652173913,"3
Message-Passing on coarsened graphs"
IMPLEMENTATION/METHODS,0.3695652173913043,"We have seen that coarsening algorithms generally aim at preserving the spectral properties of the
graph Laplacian, leading to small RSA constants ϵL,Q,R. However, this generally does not directly
translate to guarantees on the MP process materialized by the matrix S. In this section, we propose a
new propagation matrix such that small RSA constants leads to preserved MP, which then leads to
guarantees for training GNNs on coarsened graphs."
IMPLEMENTATION/METHODS,0.3804347826086957,"A new propagation matrix on coarsened graphs.
Given a graph G with a propagation matrix
S and a coarsened graph Gc with a coarsening matrix Q, our goal is to define a propagation matrix
SMP
c
∈Rn×n such that one round of MP on the coarsened signal xc followed by lifting is close to
performing MP in the original graph: Q+SMP
c xc ≈Sx. For instance, assuming that the propagation
matrix S = fS(A) is the output of a function fS of the graph’s adjacency matrix, the most natural
choice, often adopted in the literature [6], is to simply take Sc = fS(Ac), where Ac is the adjacency
matrix of the coarsened graph. However, this does not generally leads to the desired guarantees.
Some authors propose different variant of Sc adapted to specific cases [11, 21] (see Sec. 4), but none
offers generic message-passing guarantees. To address this, we propose a new propagation matrix:"
IMPLEMENTATION/METHODS,0.391304347826087,"SMP
c
= QSQ+ ∈Rn×n .
(4)"
IMPLEMENTATION/METHODS,0.40217391304347827,"Despite the simplicity of this expression, we will see that under some mild hypotheses this choice
indeed leads to preservation guarantees of message-passing for coarsenings with small RSA constants.
An important remark is that, unlike all the examples in the literature, the proposed matrix SMP
c
may
be asymmetric even when S is symmetric, unless Q is orthogonal and Q⊤= Q+, which is not the
case for many classical coarsenings [16]."
IMPLEMENTATION/METHODS,0.41304347826086957,"Message-Passing guarantees.
To state our result, we must make some technical assumptions
relating the choice of Laplacian, the nature of the coarsening, and the propagation matrix S."
IMPLEMENTATION/METHODS,0.42391304347826086,"Assumption 1 Assume that Π and S are both ker(L)-preserving, and that S is R-preserving."
IMPLEMENTATION/METHODS,0.43478260869565216,"Theorem 1 Define SMP
c
as (4). Under Assumption 1, for all x ∈R,"
IMPLEMENTATION/METHODS,0.44565217391304346,"∥Sx −Q+SMP
c xc∥L ≤ϵL,Q,R∥x∥L (CS + CΠ)
(5)"
IMPLEMENTATION/METHODS,0.45652173913043476,where CS := ∥S∥L and CΠ := ∥ΠS∥L.
IMPLEMENTATION/METHODS,0.4673913043478261,"This theorem shows that the RSA error ϵL,Q,R directly translates to an error bound between Sx and
Q+SMP
c xc. To satisfy the assumptions, we choose in our experiment to use GCNconv [14] with
S = D( ˆA)−1"
IMPLEMENTATION/METHODS,0.4782608695652174,2 ˆAD( ˆA)−1
IMPLEMENTATION/METHODS,0.4891304347826087,"2 with A = A + IN, and to compute a coarsening with a good RSA constant
for the “Laplacian” L = (1 + δ)IN −S with small δ > 0 and R spanned by the first eigenvectors of
L (the small δ is used to conveniently reduce the kernel of L to {0}). A broader discussion on the
multiplicative constant can be found in the full paper."
IMPLEMENTATION/METHODS,0.5,"GNN training on coarsened graph.
We now instantiate our message-passing guarantees to GNN
training on coarsened graph, with SGC as a primary example. To fix ideas, we consider a single large
graph G, and a node-level task. Given some node features X ∈RN×d, the goal is to minimize a
loss function J : RN →R+ on the output of a GNN Φθ(X, S) ∈RN (assumed unidimensional for
simplicity) with respect to the parameter θ:"
IMPLEMENTATION/METHODS,0.5108695652173914,"min
θ∈Θ R(θ) with R(θ) := J(Φθ(X, S))
(6)"
IMPLEMENTATION/METHODS,0.5217391304347826,"where Θ is a set of parameters that we assume bounded. Instead, one may want to train on the
coarsened graph Gc, which can be done by minimizing instead"
IMPLEMENTATION/METHODS,0.532608695652174,"Rc(θ) := J(Q+Φθ(Xc, SMP
c ))
(7)"
IMPLEMENTATION/METHODS,0.5434782608695652,"where Xc = QX. That is, the GNN is applied on the coarsened graph, and the output is then lifted to
compute the loss, which is then back-propagated to compute the gradient of θ. We make the following
assumption to state our result."
IMPLEMENTATION/METHODS,0.5543478260869565,"Assumption 2 Assume that there is a constant CJ such that |J(x) −J(x′)| ≤CJ∥x −x′∥L.
Moreover, assume that σ is R-preserving, ∥σ(x) −σ(x′)∥L ≤Cσ∥x −x′∥L, σ and Q+ commute."
IMPLEMENTATION/METHODS,0.5652173913043478,Graph Coarsening with Message-Passing Guarantees
IMPLEMENTATION/METHODS,0.5760869565217391,"The first part is valid for most loss functions, while the R-preserving part is, for now, only verified
for the id activation function and thus the SGC model. Finding non-trivial activation functions that
preserve some subspaces, or replacing this assumption with a more flexible one, are important paths
for future work."
IMPLEMENTATION/METHODS,0.5869565217391305,"Theorem 2 Under Assumptions 1 and 2: for all node features X ∈RN×d such that X:,i ∈R,
denoting by θ⋆= arg minθ∈Θ R(θ) and θc = arg minθ∈Θ Rc(θ), we have"
IMPLEMENTATION/METHODS,0.5978260869565217,"R(θc) −R(θ⋆) ≤CϵL,Q,R∥X∥:,L
(8)"
IMPLEMENTATION/METHODS,0.6086956521739131,"with C = 2CJCk
σCΘ(CS + CΠ) Pk
l=1 ¯Ck−l
Π
Cl−1
S
where ¯CΠ := ∥ΠSΠ∥L and CΘ is a constant that
depends on the parameter set Θ."
RESULTS/EXPERIMENTS,0.6195652173913043,"4
Experiments"
RESULTS/EXPERIMENTS,0.6304347826086957,"Setup.
We choose the propagation matrix from GCNconv [14], that is, S
= fS(A) =
D( ˆA)−1"
RESULTS/EXPERIMENTS,0.6413043478260869,2 ˆAD( ˆA)−1
RESULTS/EXPERIMENTS,0.6521739130434783,"2 with ˆA = A + IN. We report here the result for SGC [20], which satisfies
Assumption 2 (experiment with the true GCNconv [14] in the full paper). As detailed in the previous
section, we take L = (1 + δ)IN −S with δ = 0.001 and R as the K first eigenvectors of L
(K = N/10 in our experiments), ensuring that assumption 1 is satisfied. We adapt the algorithm
from [16] to coarsen the graphs with a good RSA constant ϵL,Q,R, more details in the full paper."
RESULTS/EXPERIMENTS,0.6630434782608695,"On coarsened graphs, we compare five propagation matrices: SMP
c
= QSQ+, our proposed matrix;"
RESULTS/EXPERIMENTS,0.6739130434782609,"Sc = fS(Ac), the naive choice Sdiag
c
= ˆ
D′−1/2(Ac+C) ˆ
D′−1/2, proposed in [11]; Sdiff
c
= QSQ⊤,
roughly inspired by Diffpool [21] and Ssym
c
= (Q+)⊤SQ+, the lifting employed to compute the
adjacency matrix of the coarsened graph Ac = (Q+)⊤AQ+."
RESULTS/EXPERIMENTS,0.6847826086956522,"Node classification.
We perform node classification experiments on real-world graphs, namely
Cora [18], Citeseer [7] and Reddit [9]. For simplicity, we restrict them to their largest connected
component1, since using a connected graph is far more convenient for coarsening algorithms (details
in the full paper). Despite the lifting procedure, training on the coarsened graph results is faster
than using the entire graph (e.g., by approximately 30% for a coarsening ratio of r = 0.5 when
parallelized on GPU). Each classification results is averaged on 10 random training. Results are
reported in Table 1. We observe that the proposed propagation matrix SMP
c
yields better results and is
more stable, especially for high coarsening ratio. The detailed hyper-parameters for each model and
each dataset can be found with additional comments on the results and datasets in the full paper."
RESULTS/EXPERIMENTS,0.6956521739130435,"SGC
Cora
Citeseer
Reddit"
RESULTS/EXPERIMENTS,0.7065217391304348,"r
0.5
0.7
0.5
0.7
0.9
0.99"
RESULTS/EXPERIMENTS,0.717391304347826,"Ssym
c
16.1 ± 3.8
16.4 ± 4.7
18.6 ± 4.6
19.8 ± 5.0
37.1 ± 6.6
3.7 ± 5.5
Sdiff
c
21.8 ± 2.2
13.6 ± 2.8
30.5 ± 0.2
23.1 ± 0.0
18.3 ±0.0
14.9 ± 0.0
Sc
78.7 ± 0.0
74.6 ± 0.1
72.8 ± 0.1
72.5 ± 0.1
87.5 ± 0.1
37.3 ± 0.0
Sdiag
c
78.7 ± 0.1
77.3 ± 0.0
73.4 ± 0.1
73.1 ± 0.4
87.6 ± 0.1
37.3 ± 0.0
SMP
c
(ours)
80.3 ± 0.1
78.5 ± 0.0
74.6 ± 0.1
74.2 ± 0.1
90.2 ±0.0
64.1 ± 0.0
Full Graph
81.6 ± 0.1
73.6 ± 0.0
94.9 ± 0.0"
RESULTS/EXPERIMENTS,0.7282608695652174,Table 1: Accuracy in % for node classification and different coarsening ratio and models.
RESULTS/EXPERIMENTS,0.7391304347826086,1hence the slight difference with other reported results on these datasets
RESULTS/EXPERIMENTS,0.75,Graph Coarsening with Message-Passing Guarantees
REFERENCES,0.7608695652173914,References
REFERENCES,0.7717391304347826,"[1] G. Bravo Hermsdorff and L. Gunderson. A unifying framework for spectrum-preserving graph
sparsification and coarsening. Advances in Neural Information Processing Systems, 32, 2019. 1"
REFERENCES,0.782608695652174,"[2] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veliˇckovi´c. Geometric Deep Learning: Grids,
Groups, Graphs, Geodesics, and Gauges. arXiv:2104.13478, 2021. URL http://arxiv.org/
abs/2104.13478. 1"
REFERENCES,0.7934782608695652,"[3] J. Chen, Y. Saad, and Z. Zhang. Graph coarsening: from scientific computing to machine
learning, volume 79. Springer International Publishing, 2022. ISBN 4032402100282. doi:
10.1007/s40324-021-00282-x. URL https://doi.org/10.1007/s40324-021-00282-x.
1"
REFERENCES,0.8043478260869565,"[4] Y. Chen, R. Yao, Y. Yang, and J. Chen. A gromov-wasserstein geometric view of spectrum-
preserving graph coarsening. In International Conference on Machine Learning, pages 5257–
5281. PMLR, 2023. 1"
REFERENCES,0.8152173913043478,"[5] I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph cuts without eigenvectors a multilevel
approach. IEEE transactions on pattern analysis and machine intelligence, 29(11):1944–1957,
2007. 1"
REFERENCES,0.8260869565217391,"[6] C. Dickens, E. Huang, A. Reganti, J. Zhu, K. Subbian, and D. Koutra. Graph coarsening via
convolution matching for scalable graph neural network training. In Companion Proceedings of
the ACM on Web Conference 2024, pages 1502–1510, 2024. 1, 3"
REFERENCES,0.8369565217391305,"[7] C. L. Giles, K. D. Bollacker, and S. Lawrence. Citeseer: An automatic citation indexing system,
1998. URL www.neci.nj.nec.com. 4"
REFERENCES,0.8478260869565217,"[8] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural Message Passing for
Quantum Chemistry. In International Conference on Machine Learning (ICML), pages 1–14,
2017. ISBN 978-1-4577-0079-8. doi: 10.1002/nme.2457. URL http://arxiv.org/abs/
1704.01212. 1"
REFERENCES,0.8586956521739131,"[9] W. L. Hamilton, Z. Ying, and J. Leskovec. Inductive Representation Learning on Large Graphs.
In NIPS, pages 1024–1034, 2017. 4"
REFERENCES,0.8695652173913043,"[10] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open Graph
Benchmark: Datasets for Machine Learning on Graphs. Neural Information Processing Systems
(NeurIPS), (NeurIPS):1–34, 2020. URL http://arxiv.org/abs/2005.00687. 1"
REFERENCES,0.8804347826086957,"[11] Z. Huang, S. Zhang, C. Xi, T. Liu, and M. Zhou. Scaling up Graph Neural Networks Via Graph
Coarsening, volume 1. Association for Computing Machinery, 2021. ISBN 9781450383325.
doi: 10.1145/3447548.3467256. 1, 2, 3, 4"
REFERENCES,0.8913043478260869,"[12] Y. Jin, A. Loukas, and J. JaJa. Graph coarsening with preserved spectral properties. In
International Conference on Artificial Intelligence and Statistics, pages 4452–4462. PMLR,
2020. 1"
REFERENCES,0.9021739130434783,"[13] N. Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing.
Advances in Neural Information Processing Systems (NeurIPS), 2022. URL http://arxiv.
org/abs/2205.12156. 1"
REFERENCES,0.9130434782608695,"[14] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations, 2016. 1, 3, 4"
REFERENCES,0.9239130434782609,"[15] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate: Graph neural networks
meet personalized PageRank. 7th International Conference on Learning Representations, ICLR
2019, pages 1–15, 2019. 2"
REFERENCES,0.9347826086956522,"[16] A. Loukas. Graph reduction with spectral and cut guarantees. Journal of Machine Learning
Research, 20(116):1–42, 2019. 1, 2, 3, 4"
REFERENCES,0.9456521739130435,"[17] A. Loukas and P. Vandergheynst. Spectrally approximating large graphs with smaller graphs.
In International conference on machine learning, pages 3237–3246. PMLR, 2018. 1"
REFERENCES,0.9565217391304348,"[18] A. K. Mccallum, K. Nigam, J. Rennie, and K. Seymore. Automating the construction of internet
portals with machine learning, 2000. URL www.campsearch.com. 4"
REFERENCES,0.967391304347826,"[19] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural
network model. IEEE transactions on neural networks, 20(1):61–80, 2008. 1"
REFERENCES,0.9782608695652174,Graph Coarsening with Message-Passing Guarantees
REFERENCES,0.9891304347826086,"[20] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger. Simplifying graph convolutional
networks. In International conference on machine learning, pages 6861–6871. PMLR, 2019. 1,
4
[21] Z. Ying. Jiaxuan you, christopher morris, xiang ren, will hamilton, and jure leskovec. hierarchi-
cal graph representation learning with differentiable pooling. Advances in neural information
processing systems, 31:4800–4810, 2018. 1, 3, 4
[22] J. Zhu, R. A. Rossi, A. Rao, T. Mai, N. Lipka, N. K. Ahmed, and D. Koutra. Graph Neural
Networks with Heterophily. 35th AAAI Conference on Artificial Intelligence, AAAI 2021, 12B:
11168–11176, 2021. 1"
