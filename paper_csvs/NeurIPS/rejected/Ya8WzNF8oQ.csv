Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001893939393939394,"Markov Decision Processes (MDPs) deliver a formal framework for modeling
1"
ABSTRACT,0.003787878787878788,"and solving sequential decision-making problems. In this paper, we make several
2"
ABSTRACT,0.005681818181818182,"contributions towards the theoretical understanding of (stochastic) policy gradient
3"
ABSTRACT,0.007575757575757576,"methods for MDPs. The focus lies on proving convergence (rates) of softmax policy
4"
ABSTRACT,0.00946969696969697,"gradient towards global optima in undiscounted finite-time horizon problems, i.e.
5"
ABSTRACT,0.011363636363636364,"γ = 1, without regularization. Such problems are relevant for instance for optimal
6"
ABSTRACT,0.013257575757575758,"stopping or specific supply chain problems. Our estimates must differ significantly
7"
ABSTRACT,0.015151515151515152,"from several recent articles that involve powers of (1 −γ)−1.
8"
ABSTRACT,0.017045454545454544,"The main contributions are the following. For undiscounted finite-time MDPs we
9"
ABSTRACT,0.01893939393939394,"prove asymptotic convergence of policy gradient to a global optimum and derive a
10"
ABSTRACT,0.020833333333333332,"convergence rate using a weak Polyak-Łojasiewicz (PL) inequality. In each decision
11"
ABSTRACT,0.022727272727272728,"epoch, the derived error bound depends linearly on the remaining duration of the
12"
ABSTRACT,0.02462121212121212,"MDP. In the second part of the analysis, we quantify the convergence behavior for
13"
ABSTRACT,0.026515151515151516,"the stochastic version of policy gradient. The analysis yields complexity bounds
14"
ABSTRACT,0.028409090909090908,"for an approximation arbitrarily close to the global optimum with high probability.
15"
ABSTRACT,0.030303030303030304,"As a by-product, our stochastic gradient arguments prove that the plain vanilla
16"
ABSTRACT,0.032196969696969696,"REINFORCE algorithm for softmax policies indeed approximates global optima
17"
ABSTRACT,0.03409090909090909,"for sufficiently large batch sizes.
18"
INTRODUCTION,0.03598484848484849,"1
Introduction
19"
INTRODUCTION,0.03787878787878788,"Policy gradient methods continue to enjoy great popularity in practice due to their model-free nature
20"
INTRODUCTION,0.03977272727272727,"and high flexibility. Despite their far-reaching history (Williams, 1992; Sutton et al., 1999; Konda and
21"
INTRODUCTION,0.041666666666666664,"Tsitsiklis, 1999; Kakade, 2001), there were no proofs for the global convergence of these algorithms
22"
INTRODUCTION,0.043560606060606064,"for a long time. Nevertheless, they have been very successful in many applications, which is why
23"
INTRODUCTION,0.045454545454545456,"numerous variants have been developed in the last few decades, whose convergence analysis, if
24"
INTRODUCTION,0.04734848484848485,"available, is mostly limited to convergence to stationary points (Pirotta et al., 2013; Schulman et al.,
25"
INTRODUCTION,0.04924242424242424,"2015; Papini et al., 2018; Clavera et al., 2018; Shen et al., 2019; Xu et al., 2020b; Huang et al., 2020;
26"
INTRODUCTION,0.05113636363636364,"Xu et al., 2020a; Huang et al., 2022).
27"
INTRODUCTION,0.05303030303030303,"In recent years, notable advancements have been achieved in the convergence analysis towards
28"
INTRODUCTION,0.054924242424242424,"global optima (Fazel et al., 2018; Agarwal et al., 2021; Mei et al., 2020; Bhandari and Russo, 2021,
29"
INTRODUCTION,0.056818181818181816,"2022; Cen et al., 2022; Xiao, 2022; Alfano and Rebeschini, 2023). These achievements are partially
30"
INTRODUCTION,0.058712121212121215,"attributed to the utilization of (weak) gradient domination or Polyak-Łojasiewicz (PL) inequalities
31"
INTRODUCTION,0.06060606060606061,"(Polyak, 1963). As examined in Karimi et al. (2016) a PL-inequality and smoothness implies a
32"
INTRODUCTION,0.0625,"linear convergence rate for gradient descent methods. In certain cases, only a weaker form of the
33"
INTRODUCTION,0.06439393939393939,"PL inequality can be derived, which states that it is only possible to limit the norm of the gradient
34"
INTRODUCTION,0.06628787878787878,"instead of the squared norm of the gradient by the distance to the optimum. Despite this limitation,
35"
INTRODUCTION,0.06818181818181818,"O(1/n)-convergence can still be achieved in some instances.
36"
INTRODUCTION,0.07007575757575757,"The research community has predominantly focused on discounted Markov decision processes
37"
INTRODUCTION,0.07196969696969698,"(MDPs) with infinite time horizon: (S, A, p, r, γ) is an MDP, where S is a finite state space, A
38"
INTRODUCTION,0.07386363636363637,"is a finite action space, p is a transition function such that p(s′|s, a) denotes the probability of
39"
INTRODUCTION,0.07575757575757576,"transitioning from state s to state s′ under action a. The reward function is given by r : S × A →R,
40"
INTRODUCTION,0.07765151515151515,"where R ⊆R is usually assumed to be bounded and positive, and γ ∈(0, 1) is a discount factor. The
41"
INTRODUCTION,0.07954545454545454,"value function under consideration takes the form
42"
INTRODUCTION,0.08143939393939394,"V π(s) = ES0=s,At∼π(·|St),St+1∼p(·|St,At)
h ∞
X"
INTRODUCTION,0.08333333333333333,"t=0
γtr(St, At)
i
,
(1)"
INTRODUCTION,0.08522727272727272,"for all s ∈S. Investigating a stationary policy applied in every time point suffices for discounted
43"
INTRODUCTION,0.08712121212121213,"MDPs (Puterman, 2005, Theorem 6.1.1). Yet, in this paper, we focus on MDPs with finite-time
44"
INTRODUCTION,0.08901515151515152,"horizons and without a discount factor, i.e., γ = 1. There is a prevailing argument that finite-time
45"
INTRODUCTION,0.09090909090909091,"MDPs do not require additional scrutiny as they can be transformed into infinite horizon MDPs.
46"
INTRODUCTION,0.0928030303030303,"However, specific challenges arise in certain scenarios, such as optimal stopping (Li et al., 2009) or
47"
INTRODUCTION,0.0946969696969697,"finite-time inventory control problems (Bhandari and Russo, 2022), where a non-stationary policy
48"
INTRODUCTION,0.09659090909090909,"becomes necessary. Unlike in infinite time horizon MDPs, reducing the problem to stationary policies
49"
INTRODUCTION,0.09848484848484848,"is inadequate for finite-time MDPs, and a new policy must be trained recursively at each time step
50"
INTRODUCTION,0.10037878787878787,"(Puterman, 2005). Our convergence analysis comprises two steps: firstly, we investigate convergence
51"
INTRODUCTION,0.10227272727272728,"at each time step and secondly, we examine the error accumulation through backward induction. A
52"
INTRODUCTION,0.10416666666666667,"detailed discussion of finite-time MDPs is presented in Section 2. There are some recent articles also
53"
INTRODUCTION,0.10606060606060606,"studying policy gradient of finite-time horizon MDPs considering fictitious discount algorithms (Guo
54"
INTRODUCTION,0.10795454545454546,"et al., 2022) or finite-time linear quadratic control problems (Hambly et al., 2021, 2022; Zhang et al.,
55"
INTRODUCTION,0.10984848484848485,"2021).
56"
INTRODUCTION,0.11174242424242424,"We begin with a discussion of relevant results for discounted MDPs that encourage our contributions.
57"
INTRODUCTION,0.11363636363636363,"In Agarwal et al. (2021), the global asymptotic convergence of policy gradient is demonstrated under
58"
INTRODUCTION,0.11553030303030302,"tabular softmax parametrization, and convergence rates are derived using log-barrier regularization
59"
INTRODUCTION,0.11742424242424243,"and natural policy gradient. Building upon this work, Mei et al. (2020) showed the first convergence
60"
INTRODUCTION,0.11931818181818182,"rates for policy gradient using non-uniform PL-inequalities (Mei et al., 2021), specifically for tabular
61"
INTRODUCTION,0.12121212121212122,"softmax parametrization. However, this convergence rate is fundamentally dependent on the discount
62"
INTRODUCTION,0.12310606060606061,"factor, (1−γ)−6, and cannot be readily extrapolated to undiscounted MDPs with finite-time horizons.
63"
INTRODUCTION,0.125,"To bridge this gap, we consider policy gradient under tabular softmax parametrization, but in
64"
INTRODUCTION,0.1268939393939394,"undiscounted MDPs with finite-time horizons and non-stationary policies. In Section 3, we show
65"
INTRODUCTION,0.12878787878787878,"asymptotic convergence to a global optimum and subsequently derive a global convergence rate using
66"
INTRODUCTION,0.13068181818181818,"a weaker form of the PL-inequality. The convergence rate at a fixed time point is linearly depending
67"
INTRODUCTION,0.13257575757575757,"on the remaining duration of the MDP, which is a better property compared to (1 −γ)−6. The
68"
INTRODUCTION,0.13446969696969696,"issue of dependency on γ when it approaches 1 is a significant subject in the context of discounting,
69"
INTRODUCTION,0.13636363636363635,"and various efforts have been made to mitigate this dependency. For instance, employing entropy
70"
INTRODUCTION,0.13825757575757575,"regularization as demonstrated in Mei et al. (2020) or applying mirror descent as described in Xiao
71"
INTRODUCTION,0.14015151515151514,"(2022) can enhance the rate of convergence.
72"
INTRODUCTION,0.14204545454545456,"In the second part of the paper, we abandon the assumption that the exact gradient is known and focus
73"
INTRODUCTION,0.14393939393939395,"on the model free stochastic policy gradient method. For this type of algorithm, very little is known
74"
INTRODUCTION,0.14583333333333334,"even in the discounted case. Agarwal et al. (2021) discussed the approximate natural policy gradient
75"
INTRODUCTION,0.14772727272727273,"for log-linear policies, and Ding et al. (2022) derived complexity bounds for entropy-regularized
76"
INTRODUCTION,0.14962121212121213,"stochastic policy gradient. They use a well-chosen stopping time which measures the distance to the
77"
INTRODUCTION,0.15151515151515152,"set of optimal parameters, and simultaneously guarantees convergence to the regularized optimum
78"
INTRODUCTION,0.1534090909090909,"prior to the occurrence of the stopping time by using a small enough step size and large enough batch
79"
INTRODUCTION,0.1553030303030303,"size. Similar to this idea, we construct a different stopping time in this work, which allows us to
80"
INTRODUCTION,0.1571969696969697,"analyze convergence of the stochastic policy gradient method in the finite, non-stationary case and
81"
INTRODUCTION,0.1590909090909091,"also in the infinite discounted case without regularization. The stopping time we propose measures
82"
INTRODUCTION,0.16098484848484848,"the distance between the policy gradient and stochastic policy gradient trajectories and stops when the
83"
INTRODUCTION,0.16287878787878787,"stochastic gradient differs too far from the exact gradient updates. This allows us to derive complexity
84"
INTRODUCTION,0.16477272727272727,"bounds for an approximation arbitrarily close to the global optimum that does not require a set of
85"
INTRODUCTION,0.16666666666666666,"optimal parameters, which is relevant when considering softmax parametrization.
86"
INTRODUCTION,0.16856060606060605,"To the best of our knowledge, the results presented in this paper provide the first convergence analysis
87"
INTRODUCTION,0.17045454545454544,"of softmax policy gradient in the undiscounted finite-time MDP setting without regularization. We
88"
INTRODUCTION,0.17234848484848486,"note that discussions in Bhandari and Russo (2022) do not apply to softmax parametrization, as they
89"
INTRODUCTION,0.17424242424242425,"assume the existence of optimal parameters in the parameter space.
90"
INTRODUCTION,0.17613636363636365,"The remainder of this manuscript is structured as follows: In Section 2, we discuss finite-time
91"
INTRODUCTION,0.17803030303030304,"MDPs and explain how to solve them using backward induction. In Section 3, we show asymptotic
92"
INTRODUCTION,0.17992424242424243,"convergence to a global optimum and derive the corresponding convergence rate. Moreover, in
93"
INTRODUCTION,0.18181818181818182,"Section 4, we present the results pertaining to finite-time stochastic policy gradient and in Section 5
94"
INTRODUCTION,0.18371212121212122,"we analyze the error accumulation using backward induction for exact and stochastic gradients. In
95"
INTRODUCTION,0.1856060606060606,"Section 6, we provide our findings regarding infinite discounted MDPs, where we derive complexity
96"
INTRODUCTION,0.1875,"bounds for the REINFORCE algorithm.
97"
FINITE-TIME HORIZON MDPS,0.1893939393939394,"2
Finite-time horizon MDPs
98"
FINITE-TIME HORIZON MDPS,0.19128787878787878,"A finite-time MDP is defined by a tuple (H, S, A, r, p) with H = {0, . . . , H −1} decision epochs,
99"
FINITE-TIME HORIZON MDPS,0.19318181818181818,"finite state space S = S0 ∪· · · ∪SH−1, finite action space A = S"
FINITE-TIME HORIZON MDPS,0.19507575757575757,"s∈S As, a reward function
100"
FINITE-TIME HORIZON MDPS,0.19696969696969696,"r : S × A →R and transition function p : S × A →∆(S) with p(Sh+1|s, a) = 1 for every
101"
FINITE-TIME HORIZON MDPS,0.19886363636363635,"h < H −1, s ∈Sh and a ∈As. Let ∆(D) denote the set of all probability measures over a finite
102"
FINITE-TIME HORIZON MDPS,0.20075757575757575,"set D. Due to finite decision epochs, the choice of the action is time dependent, i.e. non-stationary
103"
FINITE-TIME HORIZON MDPS,0.20265151515151514,"policies π = (πh)H−1
h=0 must be considered, where πh : Sh →∆(A) for every h ∈H is such that
104"
FINITE-TIME HORIZON MDPS,0.20454545454545456,"πh(As|s) = 1 for every s ∈Sh. Denote by π(h) = (πk)H−1
k=h the sub-policy of π form h to H −1,
105"
FINITE-TIME HORIZON MDPS,0.20643939393939395,"and define the h-state value function under policy π for every s ∈Sh by
106"
FINITE-TIME HORIZON MDPS,0.20833333333333334,"V
π(h)
h
(s) := E
π(h)
s
h H−1
X"
FINITE-TIME HORIZON MDPS,0.21022727272727273,"k=h
r(Sk, Ak)
i
,
h ∈H,
(2)"
FINITE-TIME HORIZON MDPS,0.21212121212121213,"where E
π(h)
s
is the expectation under the measure such that Sh = s, Ak ∼πk(·|Sk) and Sk+1 ∼
107"
FINITE-TIME HORIZON MDPS,0.21401515151515152,"p(·|Sk, Ak) for h ≤k < H −1. The h-state-action value function for every tuple (s, a) ∈Sh × As
108"
FINITE-TIME HORIZON MDPS,0.2159090909090909,"is defined by
109"
FINITE-TIME HORIZON MDPS,0.2178030303030303,"Q
π(h+1)
h
(s, a) := r(s, a) +
X"
FINITE-TIME HORIZON MDPS,0.2196969696969697,"s′∈Sh+1
p(s′|s, a)V
π(h+1)
h+1
(s′),
h ≤H −2.
(3)"
FINITE-TIME HORIZON MDPS,0.2215909090909091,"Note that Qh is independent of policy πh and for H −1, QH−1(s, a) := r(s, a) independently of
110"
FINITE-TIME HORIZON MDPS,0.22348484848484848,"any policy. Furthermore, define the h-state-action advantage function
111"
FINITE-TIME HORIZON MDPS,0.22537878787878787,"A
π(h)
h
(s, a) := Q
π(h+1)
t
(s, a) −V
π(h)
h
(s),
s ∈Sh, a ∈As.
(4)"
FINITE-TIME HORIZON MDPS,0.22727272727272727,"In the following, we will suppress the dependence of π(h) and write π in the superscripts of Vh, Qh
112"
FINITE-TIME HORIZON MDPS,0.22916666666666666,"and Ah, when the policy is clear out of context. We denote by
113"
FINITE-TIME HORIZON MDPS,0.23106060606060605,"V π
h (µh) := Es∼µh[V π
h (s)]"
FINITE-TIME HORIZON MDPS,0.23295454545454544,"the value function for an initial state distribution µh on Sh in epoch h ∈H. The performance
114"
FINITE-TIME HORIZON MDPS,0.23484848484848486,"difference lemma (Kakade and Langford, 2002) is a useful identity to compare policies. It turns
115"
FINITE-TIME HORIZON MDPS,0.23674242424242425,"out to be very useful to prove convergence of policy gradient methods (Agarwal et al., 2021). For
116"
FINITE-TIME HORIZON MDPS,0.23863636363636365,"finite-time MDPs the following version is proved in the supplementary material:
117"
FINITE-TIME HORIZON MDPS,0.24053030303030304,"Lemma 2.1 (Performance difference lemma). For any h ∈H and for any pair of policies π and π′
118"
FINITE-TIME HORIZON MDPS,0.24242424242424243,"the following holds true for every s ∈Sh:
119"
FINITE-TIME HORIZON MDPS,0.24431818181818182,"V π
h (s) −V π′
h (s) = H−1
X"
FINITE-TIME HORIZON MDPS,0.24621212121212122,"k=h
E
π(h)
Sh=s
h
Aπ′
k (Sk, Ak)
i
."
FINITE-TIME HORIZON MDPS,0.2481060606060606,"In order to address finite-time MDPs it becomes necessary to consider non-stationary policies because
120"
FINITE-TIME HORIZON MDPS,0.25,"the optimal decision at each time point depends on the time horizon until the end of the problem.
121"
FINITE-TIME HORIZON MDPS,0.2518939393939394,"Thus, to solve finite-time MDPs with policy gradient a time-dependent parametrization of the policy
122"
FINITE-TIME HORIZON MDPS,0.2537878787878788,"is required. Consider a parameter space denoted by Θ = Θ0 ×· · ·×ΘH−1, where a policy parameter
123"
FINITE-TIME HORIZON MDPS,0.2556818181818182,"θ = (θ0, . . . , θH−1) ∈Θ includes H different parameters. A parametric policy πθ = (πθh)H−1
h=0 is
124"
FINITE-TIME HORIZON MDPS,0.25757575757575757,"defined such that the policy in epoch h depends only on the parameter θh. It is worth noting that finite-
125"
FINITE-TIME HORIZON MDPS,0.25946969696969696,"time MDPs are typically solved using backward induction as known from dynamic programming
126"
FINITE-TIME HORIZON MDPS,0.26136363636363635,"theory (Puterman, 2005). In order to obtain the optimal solution for a finite-time MDP through
127"
FINITE-TIME HORIZON MDPS,0.26325757575757575,"backward induction the parametrization must have the capability to approximate any deterministic
128"
FINITE-TIME HORIZON MDPS,0.26515151515151514,"policy. This is because deterministic optimal policies exist for finite-time MDPs similar to discounted
129"
FINITE-TIME HORIZON MDPS,0.26704545454545453,"MDPs. These conditions have made the tabular softmax policy a subject of extensive research in
130"
FINITE-TIME HORIZON MDPS,0.2689393939393939,"the context of discounted MDPs, owing to its ability to meet these requirements (Mei et al., 2020;
131"
FINITE-TIME HORIZON MDPS,0.2708333333333333,"Agarwal et al., 2021; Ding et al., 2022). Let Θh = Rdh for all h ∈H, where dh = P"
FINITE-TIME HORIZON MDPS,0.2727272727272727,"s∈Sh |As| the
132"
FINITE-TIME HORIZON MDPS,0.2746212121212121,"number of state-action pairs in epoch h. Then the tabular softmax parametrization is defined to be
133"
FINITE-TIME HORIZON MDPS,0.2765151515151515,"πθ(a|s) =
exp(θ(s, a))
P"
FINITE-TIME HORIZON MDPS,0.2784090909090909,"a′∈A exp(θ(s, a′)),
θ = (θ(s, a))s∈Sh,a∈As ∈Rdh.
(5)"
FINITE-TIME HORIZON MDPS,0.2803030303030303,"In the forthcoming chapters, we will center our convergence analysis on this parametrization. Never-
134"
FINITE-TIME HORIZON MDPS,0.2821969696969697,"theless, we emphasize that the results presented in this section are also valid for any other parametriza-
135"
FINITE-TIME HORIZON MDPS,0.2840909090909091,"tion.
136"
FINITE-TIME HORIZON MDPS,0.2859848484848485,"To solve a finite-time MDP the problem is partitioned into h sub-problems, with each epoch being
137"
FINITE-TIME HORIZON MDPS,0.2878787878787879,"considered separately. Given any fixed policy ˜π, the objective function in epoch h is defined to be the
138"
FINITE-TIME HORIZON MDPS,0.2897727272727273,"h-state value function in state s ∈Sh under the policy (πθh, ˜π(h+1)) := (πθh, ˜πh+1, . . . , ˜πH−1),
139"
FINITE-TIME HORIZON MDPS,0.2916666666666667,"Jh,s(θh) := E
(πθh,˜π(h+1))
Sh=s
h H−1
X"
FINITE-TIME HORIZON MDPS,0.2935606060606061,"k=h
r(Sk, Ak)
i
.
(6)"
FINITE-TIME HORIZON MDPS,0.29545454545454547,"An optimal parameter θ∗
h is then sought such that Jh,s(θ∗
h) = supθ∈Θh Jh,s(θ), for all s ∈Sh.
140"
FINITE-TIME HORIZON MDPS,0.29734848484848486,"In order to attain an optimal policy at each time point, this problem is approached via backward
141"
FINITE-TIME HORIZON MDPS,0.29924242424242425,"induction, and the parametrization ˜π in equation (6) is selected to be the pre-optimized one. Assuming
142"
FINITE-TIME HORIZON MDPS,0.30113636363636365,"that the parametrization is able to approximate an optimal policy (e.g. the softmax parametrization),
143"
FINITE-TIME HORIZON MDPS,0.30303030303030304,"then the backward induction yields optimal parameters θ∗
h, . . . , θ∗
H−1 in the sense that, see Puterman
144"
FINITE-TIME HORIZON MDPS,0.30492424242424243,"(2005, Sec. 4.5),
145"
FINITE-TIME HORIZON MDPS,0.3068181818181818,"Jh,s(θ∗
h) =
sup
θh∈Θh,...,θH−1∈ΘH−1
V πθ
h (s),"
FINITE-TIME HORIZON MDPS,0.3087121212121212,"for all s ∈Sh. To employ the policy gradient method, it is essential to compute the gradient of
146"
FINITE-TIME HORIZON MDPS,0.3106060606060606,"Jh,s(θ) with respect to θ for a given policy ˜π. Notably, the forthcoming policy ˜π can be any policy,
147"
FINITE-TIME HORIZON MDPS,0.3125,"independent of the current parameter θ, which is trained during epoch h. This approach significantly
148"
FINITE-TIME HORIZON MDPS,0.3143939393939394,"deviates from the one used in discounted MDPs, such as in Sutton et al. (1999), where a stationary
149"
FINITE-TIME HORIZON MDPS,0.3162878787878788,"policy is parametrized and utilized at every time step. Despite the differences, a policy gradient
150"
FINITE-TIME HORIZON MDPS,0.3181818181818182,"theorem can still be attained, allowing the gradient of the objective function to be written as an
151"
FINITE-TIME HORIZON MDPS,0.32007575757575757,"expectation.
152"
FINITE-TIME HORIZON MDPS,0.32196969696969696,"Theorem 2.2. For a fixed policy ˜π and h ∈H the gradient of Jh,s(θ) defined in (6) is given by
153"
FINITE-TIME HORIZON MDPS,0.32386363636363635,"∇Jh,s(θ) = ESh=s,Ah∼πθ(·|s)[∇log(πθ(Ah|Sh))Q˜π
h(Sh, Ah)]."
FINITE-TIME HORIZON MDPS,0.32575757575757575,"As for the value function, we denote by Jh(θ) := Es∼µh[Jh,s(θ)] the objective function under some
154"
FINITE-TIME HORIZON MDPS,0.32765151515151514,initial state distribution µh on Sh. Algorithm 1 summarizes policy gradient in finite-time MDPs.
FINITE-TIME HORIZON MDPS,0.32954545454545453,"Algorithm 1: Policy Gradient for finite-time MDPs and non-stationary policies
Result: Approximate policy ˆπ∗≈π∗"
FINITE-TIME HORIZON MDPS,0.3314393939393939,"Initialize θ(0) = (θ(0)
0 , . . . , θ(0)
H−1) ∈Θ
for h = H −1, . . . , 0 do"
FINITE-TIME HORIZON MDPS,0.3333333333333333,"Choose fixed step size ηh and number of training steps Nh
for n = 0, . . . , Nh −1 do"
FINITE-TIME HORIZON MDPS,0.3352272727272727,"Calculate ∇Jh(θ(n)
h ) with fixed policy ˆπ∗after h
θ(n+1)
h
= θ(n)
h
+ ηh∇Jh(θ(n)
h )
end"
FINITE-TIME HORIZON MDPS,0.3371212121212121,"Set ˆπ∗
h = πθ
(Nh)
h
end 155"
FINITE-TIME HORIZON MDPS,0.3390151515151515,"Training each time point separately and having a fixed policy ˜π after h, we state a version of the
156"
FINITE-TIME HORIZON MDPS,0.3409090909090909,"performance difference lemma given this specific setting.
157"
FINITE-TIME HORIZON MDPS,0.3428030303030303,"Corollary 2.3. For any h ∈H and two policies π and π′: If π(h+1) = π′
(h+1), it holds that
158"
FINITE-TIME HORIZON MDPS,0.3446969696969697,"V π
h (s) −V π′
h (s) = E
π(h)
Sh=s
h
Aπ′
h (Sh, Ah)
i
."
CONVERGENCE ANALYSIS OF SOFTMAX POLICY GRADIENT,0.3465909090909091,"3
Convergence Analysis of Softmax Policy Gradient
159"
CONVERGENCE ANALYSIS OF SOFTMAX POLICY GRADIENT,0.3484848484848485,"Before we combine all decision epochs as stated in Algorithm 1, we provide convergence results for
160"
CONVERGENCE ANALYSIS OF SOFTMAX POLICY GRADIENT,0.3503787878787879,"each h ∈H given that the policy after h is fixed and denoted by ˜π. The error analysis over time is
161"
CONVERGENCE ANALYSIS OF SOFTMAX POLICY GRADIENT,0.3522727272727273,"then employed in Section 5.
162"
CONVERGENCE ANALYSIS OF SOFTMAX POLICY GRADIENT,0.3541666666666667,"Assumption 3.1. Throughout the remaining manuscript we assume that the rewards are bounded in
163"
CONVERGENCE ANALYSIS OF SOFTMAX POLICY GRADIENT,0.3560606060606061,"[0, R∗], for some R∗> 0.
164"
ASYMPTOTIC CONVERGENCE,0.35795454545454547,"3.1
Asymptotic convergence
165"
ASYMPTOTIC CONVERGENCE,0.35984848484848486,"The choice of tabular softmax parametrization is particularly convenient as derivatives are simple.
166"
ASYMPTOTIC CONVERGENCE,0.36174242424242425,"Lemma 3.2. Let h ∈H, then the partial derivatives of Jh with respect to θ take the following form
167"
ASYMPTOTIC CONVERGENCE,0.36363636363636365,"∂Jh(θ)
∂θ(s, a) = µ(s)πθ(a|s)A
(πθ,˜π(h+1))
h
(s, a)."
ASYMPTOTIC CONVERGENCE,0.36553030303030304,"Furthermore, Jh is a smooth function with respect to θ. The proof is based on a more general
168"
ASYMPTOTIC CONVERGENCE,0.36742424242424243,"result which proves smoothness for all parametrizations with bounded gradient and Hessian of the
169"
ASYMPTOTIC CONVERGENCE,0.3693181818181818,"log-policy.
170"
ASYMPTOTIC CONVERGENCE,0.3712121212121212,"Proposition 3.3. Let h ∈H and consider the objective function Jh(θ). If there exists G, M > 0
171"
ASYMPTOTIC CONVERGENCE,0.3731060606060606,"such that
172"
ASYMPTOTIC CONVERGENCE,0.375,"||∇log πθ(a|s)||2 ≤G
and
||∇2 log πθ(a|s)||2 ≤M,
for all s ∈Sh, a ∈As, then for any initial state distribution µh of Sh the function Jh(θ) is βh-smooth
173"
ASYMPTOTIC CONVERGENCE,0.3768939393939394,"in θ with βh = (H −h)R∗(G2 + M).
174"
ASYMPTOTIC CONVERGENCE,0.3787878787878788,"Smoothness under these assumptions in the discounted finite-time setting with stationary policy was
175"
ASYMPTOTIC CONVERGENCE,0.3806818181818182,"shown for example in Xu et al. (2020b) and Xu et al. (2020a). We obtain the following smoothness
176"
ASYMPTOTIC CONVERGENCE,0.38257575757575757,"parameter:
177"
ASYMPTOTIC CONVERGENCE,0.38446969696969696,"Lemma 3.4. Let h ∈H, then the h-state value function under softmax parametrization, θ 7→Jh(θ),
178"
ASYMPTOTIC CONVERGENCE,0.38636363636363635,"is βh-smooth with βh = 2(H −h)R∗|A|.
179"
ASYMPTOTIC CONVERGENCE,0.38825757575757575,"We point out that the smoothness parameter is independent of the choice of ˜π. A consequence of the
180"
ASYMPTOTIC CONVERGENCE,0.39015151515151514,"smoothness is the asymptotic convergence of the objective function towards a global maximum. As
181"
ASYMPTOTIC CONVERGENCE,0.39204545454545453,"each epoch is considered separately we just write θn instead of θ(n)
h
until Section 5.
182"
ASYMPTOTIC CONVERGENCE,0.3939393939393939,"Theorem 3.5. Let h ∈H and consider the gradient ascent updates
183"
ASYMPTOTIC CONVERGENCE,0.3958333333333333,"θn+1 = θn + ηh∇Jh(θn)
(7)"
ASYMPTOTIC CONVERGENCE,0.3977272727272727,"for arbitrary θ0 ∈Rdh. We assume that µh(s) > 0 for all s ∈Sh and 0 < ηh ≤
1
βh . Then, for all
184"
ASYMPTOTIC CONVERGENCE,0.3996212121212121,"s ∈Sh, Jh,s(θn) converges to J∗
h,s for n →∞, where J∗
h,s = supθ Jh,s(θ) < ∞.
185"
ASYMPTOTIC CONVERGENCE,0.4015151515151515,"The difficulties that arise from softmax parametrization are the same as discussed in Agarwal et al.
186"
ASYMPTOTIC CONVERGENCE,0.4034090909090909,"(2021) for the infinite time setting: The softmax policy approximates an optimal deterministic policy.
187"
ASYMPTOTIC CONVERGENCE,0.4053030303030303,"Therefore, parameters converge to −∞for suboptimal actions and to ∞for optimal actions. The idea
188"
ASYMPTOTIC CONVERGENCE,0.4071969696969697,"of the proof follows the outline of the discounted MDP setting except for one main distinction: the
189"
ASYMPTOTIC CONVERGENCE,0.4090909090909091,"action-value function Qh is independent of the policy gradient updates such that no limiting process
190"
ASYMPTOTIC CONVERGENCE,0.4109848484848485,"has to be constructed. A detailed proof is provided in B.1.
191"
ASYMPTOTIC CONVERGENCE,0.4128787878787879,"Note that the assumption µh(s) > 0 for all s ∈Sh is necessary for sufficient exploration. The
192"
ASYMPTOTIC CONVERGENCE,0.4147727272727273,"same assumption is needed for the initial distribution of a discounted MDP in Agarwal et al. (2021,
193"
ASYMPTOTIC CONVERGENCE,0.4166666666666667,"Thm. 10). Furthermore, Mei et al. (2020, Prop. 3) have demonstrated the necessity of this assumption.
194"
CONVERGENCE RATE,0.4185606060606061,"3.2
Convergence rate
195"
CONVERGENCE RATE,0.42045454545454547,"In order to derive a convergence rate for tabular softmax parametrized finite-time MDPs we will
196"
CONVERGENCE RATE,0.42234848484848486,"establish a weaker form of the PL-inequality. Therefore, consider for h ∈H a deterministic optimal
197"
CONVERGENCE RATE,0.42424242424242425,"policy π∗
h, given that the policy after h is fixed by ˜π, i.e. for all s ∈Sh,
198"
CONVERGENCE RATE,0.42613636363636365,"π∗
h(·|s) =
argmax
π(·|s): Policy
V
(π,˜π(h+1))
h
(s)."
CONVERGENCE RATE,0.42803030303030304,"Please note here that the optimal policy and also J∗
h,s depend on the choice of ˜π.
199"
CONVERGENCE RATE,0.42992424242424243,"Lemma 3.6 (weak PL-inequality). For the objective Jh it holds that
200"
CONVERGENCE RATE,0.4318181818181818,"∥∇Jh(θ)∥2 ≥min
s∈Sh πθ(a∗
h(s)|s)(J∗
h −Jh(θ)),"
CONVERGENCE RATE,0.4337121212121212,"where a∗
h(s) = argmaxa∈Asπ∗
h(a|s) and J∗
h = supθ Jh(θ).
201"
CONVERGENCE RATE,0.4356060606060606,"The term mins∈S πθ(a∗
h(s)|s) also appears in similar form in the discounted setting in Mei et al.
202"
CONVERGENCE RATE,0.4375,"(2020). The main challenge is to bound this term from below uniformly in θ appearing in the gradient
203"
CONVERGENCE RATE,0.4393939393939394,"ascent updates. Due to asymptotic convergence this can be achieved, where it is necessary to assume
204"
CONVERGENCE RATE,0.4412878787878788,"µh(s) > 0 for all s ∈Sh.
205"
CONVERGENCE RATE,0.4431818181818182,"Lemma 3.7. Let h ∈H, µh(s) > 0 for all s ∈Sh and consider the sequence (θn)n∈N0 generated by
206"
CONVERGENCE RATE,0.44507575757575757,"(7) for arbitrarily initialized θ0 ∈Rdh. Then it holds that ch := infn≥0 mins∈Sh πθn(a∗
h(s)|s) > 0.
207"
CONVERGENCE RATE,0.44696969696969696,"We emphasize that the constant ch is influenced by the initial parameter θ0 thereby making it a
208"
CONVERGENCE RATE,0.44886363636363635,"parameter dependent on the model, as it is also for discounted MDPs in Mei et al. (2020).
209"
CONVERGENCE RATE,0.45075757575757575,"Theorem 3.8. Let h ∈H, µh(s) > 0 for all s ∈Sh and consider the sequence (θn)n∈N0 generated
210"
CONVERGENCE RATE,0.45265151515151514,"by (7) for arbitrarily initialized θ0 ∈Rdh. Define ch := infn≥0 mins∈Sh πθn(a∗
h(s)|s) > 0 by
211"
CONVERGENCE RATE,0.45454545454545453,"Lemma 3.7 and choose step size ηh =
1
βh with βh = 2(H −h)R∗|A|. Then it holds that
212"
CONVERGENCE RATE,0.4564393939393939,"J∗
h −Jh(θn) ≤4(H −h)R∗|A|"
CONVERGENCE RATE,0.4583333333333333,"c2
hn
,"
CONVERGENCE RATE,0.4602272727272727,"where J∗
h = supθ Jh(θ).
213"
CONVERGENCE RATE,0.4621212121212121,"The error bound depends on the time horizon up to the last time point, meaning intuitively that an
214"
CONVERGENCE RATE,0.4640151515151515,"optimal policy for earlier time points in the MDP (smaller h) is harder to achieve and requires a
215"
CONVERGENCE RATE,0.4659090909090909,"longer learning period then later time points (h near to H). Comparing this result to the convergence
216"
CONVERGENCE RATE,0.4678030303030303,"rate for discounted MPDs we note that the linear dependency on the time horizon is less aggressive
217"
CONVERGENCE RATE,0.4696969696969697,"than the factor (1 −γ)−1. In addition, the magnitude of the state space Sh does not have a direct
218"
CONVERGENCE RATE,0.4715909090909091,"impact on the rate. However, the constant ch indirectly introduces a dependency.
219"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.4734848484848485,"4
Convergence Analysis of Stochastic Softmax Policy Gradient
220"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.4753787878787879,"For the rest of this paper we drop the assumption of knowing ∇Jh(θ). In this model-free setting it is
221"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.4772727272727273,"only assumed that trajectories of the finite-time MDP can be simulated. Stochastic policy gradient is
222"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.4791666666666667,"used to train the parameters, where in each iteration the gradient of the objective is approximated
223"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.4810606060606061,"using Monte Carlo estimates. Consider Kh trajectories (si
k, ai
k)H−1
k=h , for i = 1, . . . , Kh, generated
224"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.48295454545454547,"by si
h ∼µh, ai
h ∼πθ
h and ai
k ∼˜πk for h < k < H. The estimator is defined by
225"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.48484848484848486,"b∇JK
h (θ) =
1
Kh Kh
X"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.48674242424242425,"i=1
∇log(πθ(ai
h|si
h)) ˆQh(si
h, ai
h),
(8)"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.48863636363636365,"where ˆQh(si
h, ai
h) = PH−1
k=h r(si
k, ai
k) is an unbiased estimator of the h-state-action value function
226"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.49053030303030304,"in (si
h, ai
h) under policy ˜π. Then the stochastic policy gradient updates for training the parameter θ
227"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.49242424242424243,"are given by
228"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.4943181818181818,"θn+1 = θn + ηh b∇JKh
h
(θ).
(9)
To train an optimal policy with backward induction, ˜π is chosen to be the already trained policies.
229"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.4962121212121212,"As in Section 3 we first restrict our convergence analysis to one time point h given a fixed policy ˜π
230"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.4981060606060606,"after h. The entire stochastic policy gradient algorithm, often called REINFORCE, is summarized in
231"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.5,"Algorithm 2.
232"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.5018939393939394,"Under the softmax parametrization it holds true that b∇JKh
h
(θ) is an unbiased estimator with uniformly
233"
CONVERGENCE ANALYSIS OF STOCHASTIC SOFTMAX POLICY GRADIENT,0.5037878787878788,"bounded variance due to the bounded reward assumption (see Lemma C.1).
234"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5056818181818182,"4.1
Asymptotic convergence to stationary point
235"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5075757575757576,"Using stochastic policy gradient, we obtain almost sure convergence of the value function to a
236"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.509469696969697,"stationary point for decreasing step sizes. Note that, except for this theorem we assume a constant
237"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5113636363636364,"step size.
238"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5132575757575758,"Algorithm 2: REINFORCE with Backward Iteration
Result: Approximate policy ˆπ∗≈π∗"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5151515151515151,"Initialize θ(0) = (θ(0)
0 , . . . , θ(0)
H−1) ∈Θ
for h = H −1, . . . , 0 do"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5170454545454546,"Choose step size ηh, number of training steps Nh and batch size Kh
for n = 0, . . . , Nh −1 do"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5189393939393939,"for i = 1, . . . Kh do"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5208333333333334,"Sample trajectory (si
k, ai
k)H−1
k=h , s.t. si
h ∼µh, ai
h ∼πθ(n)
h
and ai
k ∼ˆπ∗
k for k > h
end
θ(n+1)
h
= θ(n)
h
+ ηh b∇JKh
h
(θ), where b∇JKh
h
(θ) is defined in (8)
end"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5227272727272727,"Set ˆπ∗
h := πθ
(Nh)
h
end"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5246212121212122,"Theorem 4.1. For any h ∈H consider the stochastic process (θn)n≥0 generated by
239"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5265151515151515,"θn+1 = θn + η(n)
h
b∇JKh
h
(θ),"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5284090909090909,"for arbitrary batch size Kh ≥1 and initial θ0 such that E[Jh(θ0)] < ∞. Furthermore, suppose that
240"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5303030303030303,"η(n)
h
is decreasing, such that P"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5321969696969697,"n≥0 η(n)
h
= ∞and P"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5340909090909091,"n≥0
 
η(n)
h
2 < ∞. Then ∇Jh(θn) →0 almost
241"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5359848484848485,"surely for n →∞.
242"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5378787878787878,"With Lemma C.1 and the boundedness of the h-state value functions, this follows directly from the
243"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5397727272727273,"stochastic approximation theorem stated in Bertsekas and Tsitsiklis (2000) (see Proposition C.2 in
244"
ASYMPTOTIC CONVERGENCE TO STATIONARY POINT,0.5416666666666666,"the supplementary material).
245"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5435606060606061,"4.2
Complexity bounds to approximate to global optimum with high probability
246"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5454545454545454,"In the following denote by (¯θn)n≥1 the deterministic sequence generated by policy gradient with
247"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5473484848484849,"exact gradients,
248"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5492424242424242,"¯θn+1 = ¯θn + ηh∇Jh(¯θn).
(10)"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5511363636363636,"Let (θn)n≥0 be the stochastic process from (9) such that the initial parameter agree, θ0 = ¯θ0, and the
249"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.553030303030303,"step size ηh is the same for both processes. The natural filtration of (θn)n≥0 is denoted by (Fn)n≥0.
250"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5549242424242424,"Recall that ch = minn≥0 mins∈S π¯θn(a∗(s)|s) is bounded away from 0 by Lemma 3.7. The idea of
251"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5568181818181818,"the convergence analysis for stochastic softmax policy gradient is to define the following stopping
252"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5587121212121212,"time
253"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5606060606060606,τ := min{n ≥0 : ∥θn −¯θn∥2 ≥ch 4 }.
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5625,"This means, τ is the first time when the stochastic process (θn)n≥0 is too far away from the policy
254"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5643939393939394,"gradient trajectory (¯θn)n≥0. Hence, all challenges encountered in the deterministic case transfer to
255"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5662878787878788,"the stochastic context, indicating that the model dependent constant ch naturally appears in the error
256"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5681818181818182,"bounds of the stochastic case. We emphasize that τ is a stopping time with respect to the filtration
257"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5700757575757576,"(Fn)n≥0 by construction.
258"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.571969696969697,"First, consider the event {n ≤τ}, i.e. ∥θn −¯θn∥2 ≤ch"
COMPLEXITY BOUNDS TO APPROXIMATE TO GLOBAL OPTIMUM WITH HIGH PROBABILITY,0.5738636363636364,"4 . It follows by the
√"
-LIPSCHITZ CONTINUITY,0.5757575757575758,"2-Lipschitz continuity
259"
-LIPSCHITZ CONTINUITY,0.5776515151515151,of θ 7→πθ(a∗(s)|s) (Lemma C.3) that min0≤k≤τ mins∈S πθk(a∗(s)|s) ≥ch
-LIPSCHITZ CONTINUITY,0.5795454545454546,"2 > 0 (Lemma C.4).
260"
-LIPSCHITZ CONTINUITY,0.5814393939393939,"This allows us to use the weak PL-inequality of Lemma 3.6 to derive a convergence rate on the event
261"
-LIPSCHITZ CONTINUITY,0.5833333333333334,"{n ≤τ} in the following sense:
262"
-LIPSCHITZ CONTINUITY,0.5852272727272727,"Lemma 4.2. Suppose µh(s) > 0 for all s ∈Sh, the batch size K(n)
h
≥
9c2
hCh"
-LIPSCHITZ CONTINUITY,0.5871212121212122,"32β2
hN
3
2
h
(1 −
1
2√Nh )n2 is
263"
-LIPSCHITZ CONTINUITY,0.5890151515151515,"increasing for some Nh ≥1 and the step size ηh =
1
βh
√Nh , for fixed h ∈H. Then,
264"
-LIPSCHITZ CONTINUITY,0.5909090909090909,"E

(J∗
h −Jh(θn))1{n≤τ}

≤
16√Nhβh
3(1 −
1
2√Nh )c2
hn."
-LIPSCHITZ CONTINUITY,0.5928030303030303,"Secondly, consider the complementary event {τ ≤n}. We can bound the probability of this event
265"
-LIPSCHITZ CONTINUITY,0.5946969696969697,"by δ for a large enough batch size Kh. The proof is based on a similar result obtained by Ding et al.
266"
-LIPSCHITZ CONTINUITY,0.5965909090909091,"(2022, Lem. 6.3) for discounted MDPs.
267"
-LIPSCHITZ CONTINUITY,0.5984848484848485,"Lemma 4.3. Suppose µh(s) > 0 for all s ∈Sh. Then, for any δ > 0, we have P(τ ≤n) < δ if
268"
-LIPSCHITZ CONTINUITY,0.6003787878787878,Kh ≥16n3Ch
-LIPSCHITZ CONTINUITY,0.6022727272727273,"β2c2
hδ2 and ηh =
1
√nβh .
269"
-LIPSCHITZ CONTINUITY,0.6041666666666666,"We are now ready to formulate the main result of this section.
270"
-LIPSCHITZ CONTINUITY,0.6060606060606061,"Theorem 4.4. Suppose the stochastic policy gradient updates are generated by (9) for arbitrary
271"
-LIPSCHITZ CONTINUITY,0.6079545454545454,"initialization θ0 ∈Rdh. Suppose that µh(s) > 0 for all s ∈Sh and choose for any δ, ϵ > 0,
272"
-LIPSCHITZ CONTINUITY,0.6098484848484849,"(i) the number of training steps Nh ≥
  64βh"
-LIPSCHITZ CONTINUITY,0.6117424242424242,"3δc2
hϵ
2,
273"
-LIPSCHITZ CONTINUITY,0.6136363636363636,"(ii) the step size ηh =
1
βh
√Nh and the batch size Kh = 64N3
hCh
β2c2
hδ2 .
274"
-LIPSCHITZ CONTINUITY,0.615530303030303,"Then, P
 
(J∗
h −Jh(θNh)) ≥ϵ

≤δ.
275"
-LIPSCHITZ CONTINUITY,0.6174242424242424,"It should be noted that the choice of step size ηh and batch size Kh are closely connected and both
276"
-LIPSCHITZ CONTINUITY,0.6193181818181818,"strongly depend on the number of training steps Nh. Specifically, as Nh increases, the batch size
277"
-LIPSCHITZ CONTINUITY,0.6212121212121212,"increases, while the step size tends to decrease to prevent exceeding the stopping time with high
278"
-LIPSCHITZ CONTINUITY,0.6231060606060606,"probability. However, it is possible to increase the batch size even further and simultaneously benefit
279"
-LIPSCHITZ CONTINUITY,0.625,"from choosing a larger step size, or vice versa.
280"
ERROR ANALYSIS OVER TIME,0.6268939393939394,"5
Error Analysis over Time
281"
ERROR ANALYSIS OVER TIME,0.6287878787878788,"In this section, we will first examine the accumulation of error over time for the policy gradient
282"
ERROR ANALYSIS OVER TIME,0.6306818181818182,"Algorithm 1, and secondly, for the stochastic policy gradient Algorithm 2. In both cases the error
283"
ERROR ANALYSIS OVER TIME,0.6325757575757576,accumulates linearly such that an ϵ
ERROR ANALYSIS OVER TIME,0.634469696969697,"H -error in each time point h results in an overall error of ϵ. This
284"
ERROR ANALYSIS OVER TIME,0.6363636363636364,"is due to the additive structure of the rewards and comes naturally from the backward induction of
285"
ERROR ANALYSIS OVER TIME,0.6382575757575758,"dynamic programming for finite-time MDPs.
286"
ERROR ANALYSIS OVER TIME,0.6401515151515151,"Theorem 5.1. Assume that µh(s) > 0 for all h ∈H, s ∈Sh. Let ϵ > 0, the step size ηh =
1
βh and
287"
ERROR ANALYSIS OVER TIME,0.6420454545454546,the batch size Nh = 4(H−h)HR∗|A|
ERROR ANALYSIS OVER TIME,0.6439393939393939,"c2
hϵ
 1"
ERROR ANALYSIS OVER TIME,0.6458333333333334,"µh

∞. Denote by ˆπ∗= (πθN0
0 , . . . , πθ
NH−1
H−1 ) the final policy
288"
ERROR ANALYSIS OVER TIME,0.6477272727272727,"from Algorithm 1, then for all s ∈S0,
289"
ERROR ANALYSIS OVER TIME,0.6496212121212122,"V ∗
0 (s) −V ˆπ∗
0 (s) ≤ϵ."
ERROR ANALYSIS OVER TIME,0.6515151515151515,"For the stochastic policy gradient algorithm, we obtain the following main result:
290"
ERROR ANALYSIS OVER TIME,0.6534090909090909,"Theorem 5.2. Assume that µh(s) > 0 for all h ∈H, s ∈Sh. Let δ, ϵ > 0, the step size ηh =
1
βhNh ,
291"
ERROR ANALYSIS OVER TIME,0.6553030303030303,"number of training steps Nh =
 64βhH2 1"
ERROR ANALYSIS OVER TIME,0.6571969696969697,"µh

∞
3δc2
hϵ
2
and the batch size Kh = 64N 2
hH2Ch
βhc2
hδ2
. Denote by
292"
ERROR ANALYSIS OVER TIME,0.6590909090909091,"ˆπ∗= (πθN0
0 , . . . , πθ
NH−1
H−1 ) the final policy from Algorithm 2, then
293"
ERROR ANALYSIS OVER TIME,0.6609848484848485,"P

∃s ∈S0 : V ∗
0 (s) −V ˆπ∗
0 (s) ≥ϵ

≤δ."
ERROR ANALYSIS OVER TIME,0.6628787878787878,"In both results we observe that the number of training steps in each epoch depends on the constant
294
 1"
ERROR ANALYSIS OVER TIME,0.6647727272727273,"µh

∞= maxs∈S
1
µh(s). The proofs of Section D reveal that this constant occurs to ensure that the
295"
ERROR ANALYSIS OVER TIME,0.6666666666666666,"objective Jh,s(θ(Nh)
h
) is close to J∗
h,s for every s ∈Sh.
296"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6685606060606061,"6
Convergence Analysis of Stochastic Policy Gradient in Infinite Horizons
297"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6704545454545454,"In this final section, we show how to combine the results of Mei et al. (2020) with our stochastic
298"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6723484848484849,"gradient arguments to show that the plain vanilla REINFORCE algorithm without regularization
299"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6742424242424242,"can approximate global maxima if the batch sizes are chosen properly. Our theoretically derived
300"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6761363636363636,"batch sizes are clearly not of practical use but give a first insight why REINFORCE requires large
301"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.678030303030303,"batch sizes to give reasonable approximations. In the following, we consider the discounted MDP
302"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6799242424242424,"setting from Equation (1) with rewards taking values in [0, 1], i.e. R∗= 1, and tabular softmax
303"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6818181818181818,"parametrization πθ from (5) with θ ∈Θ = R|S||A|. The objective function J(θ) := ES0∼µ[V πθ(S0)]
304"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6837121212121212,"is defined for an initial state distribution µ. It is important to highlight that πθ is now a stationary
305"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6856060606060606,"policy used in every epoch. Our arguments rely on the weak PL-inequality for the exact value
306"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6875,"function. Mei et al. (2020) proved that
307"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6893939393939394,∂V πθ(µ) ∂θ
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6912878787878788,"2 ≥

dπ∗
ρ
dπθ
µ"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6931818181818182,"∞
mins∈S πθ(a∗(s)|s)
p"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6950757575757576,"|S|
(V ∗(ρ) −V πθ(ρ)),"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.696969696969697,"where a∗(s) = argmax π∗(·|s) the optimal action in state s and

dπ∗
ρ
dπθ
µ"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.6988636363636364,"∞is the distribution mismatch
308"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7007575757575758,"coefficient introduced in Agarwal et al. (2021). We present an alternative version in Lemma E.2
309"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7026515151515151,"without the constant |S|−1/2. The typical approach to prove convergence of stochastic gradient
310"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7045454545454546,"schemes is to iteratively compare the stochastic gradient update to the deterministic one and then
311"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7064393939393939,"control the error. This is not always possible, but for stochastic softmax policy gradient we show
312"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7083333333333334,"that the error can be controlled for large enough batch sizes. We proceed in a manner similar to
313"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7102272727272727,"Section 4.2. Thus, to state the theorem let us denote by
314"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7121212121212122,"¯θn+1 = ¯θn + η∇J(¯θn),
θn+1 = θn + η b∇JK(θ)
(11)"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7140151515151515,"the policy gradient and stochastic policy gradient schemes.
Also denote by c
:=
315"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7159090909090909,"minn≥0 mins∈S π¯θn(a∗(s)|s) the model dependent constant from the weak PL-inequality of (Mei
316"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7178030303030303,"et al., 2020, Lem. 8). For the algorithm we use the unbiased gradient estimator proposed by Zhang
317"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7196969696969697,"et al. (2020) which the authors used to prove convergence to a stationary point. Our main contribution
318"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7215909090909091,"is the following convergence result towards the global optimum:
319"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7234848484848485,"Theorem 6.1. Let (¯θn)n≥0 and (θn)n≥0 be the (stochastic) policy gradient updates from (11) for
320"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7253787878787878,"arbitrary initial ¯θ0 = θ0 ∈Θ. Suppose µ(s) > 0 for all s ∈S and choose for any δ, ϵ > 0,
321"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7272727272727273,"(i) the number of training steps N ≥
 
258
3ϵδc2(1−γ)3
2,
322"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7291666666666666,"(ii) step size η = (1−γ)3 8
√ N
323"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7310606060606061,"(iii) batch size K = max
n
9(1−γ)4c2C"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7329545454545454,"2048
(
√ N −1"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7348484848484849,"2)

dπ∗
µ
µ

−2"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7367424242424242,"∞, 4(1−γ)6N3C"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7386363636363636,"c2δ2
o
.
324"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.740530303030303,"Then, P
 
(J∗−J(θN)) ≥ϵ

≤δ, where J∗= supθ J(θ).
325"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7424242424242424,"We present more details on the algorithm and the proof in Section E of the supplementary material.
326"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7443181818181818,"We emphasize that the dependency on the distribution mismatch coefficient and the model dependent
327"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7462121212121212,"constant c are unavoidable since the stochastic gradient ascent is derived from the deterministic
328"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7481060606060606,"gradient ascent. To the best of our knowledge, this is the first convergence analysis for stochastic
329"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.75,"policy gradient with softmax parametrization without regularization. So far, Ding et al. (2022) derived
330"
CONVERGENCE ANALYSIS OF STOCHASTIC POLICY GRADIENT IN INFINITE HORIZONS,0.7518939393939394,"complexity bounds for convergence of softmax policy gradient to the entropy-regularized optimum.
331"
CONCLUSION AND FUTURE WORK,0.7537878787878788,"7
Conclusion and Future Work
332"
CONCLUSION AND FUTURE WORK,0.7556818181818182,"In this paper, we have presented a convergence analysis of policy gradient methods for undiscounted
333"
CONCLUSION AND FUTURE WORK,0.7575757575757576,"MDPs with finite-time horizon in the tabular setting. Assuming exact gradients we have obtained an
334"
CONCLUSION AND FUTURE WORK,0.759469696969697,"O(1/n)-convergence rate which is linearly dependent on the time horizon. In the model-free setting
335"
CONCLUSION AND FUTURE WORK,0.7613636363636364,"we have derived complexity bounds to approximate the error to global optima with high probability.
336"
CONCLUSION AND FUTURE WORK,0.7632575757575758,"Moreover, we were able to extend this result to discounted MDPs without regularization.
337"
CONCLUSION AND FUTURE WORK,0.7651515151515151,"In the finite-time case, it would be intriguing to explore policy parametrizations with a smaller
338"
CONCLUSION AND FUTURE WORK,0.7670454545454546,"parameter space as for example log-linear policies. Additionally, investigating modern policy
339"
CONCLUSION AND FUTURE WORK,0.7689393939393939,"gradient algorithms such as TRPO and natural policy gradient within the context of finite-time MDPs
340"
CONCLUSION AND FUTURE WORK,0.7708333333333334,"could further enhance the convergence rate. In the stochastic setting, it is desirable to eliminate the
341"
CONCLUSION AND FUTURE WORK,0.7727272727272727,"model-dependent parameter from the complexity bounds to construct a practicable algorithm. This
342"
CONCLUSION AND FUTURE WORK,0.7746212121212122,"would require an improved convergence analysis of policy gradient with exact gradients.
343"
REFERENCES,0.7765151515151515,"References
344"
REFERENCES,0.7784090909090909,"Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy
345"
REFERENCES,0.7803030303030303,"gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
346"
REFERENCES,0.7821969696969697,"Research, 22(98):1–76, 2021. URL http://jmlr.org/papers/v22/19-736.html.
347"
REFERENCES,0.7840909090909091,"Carlo Alfano and Patrick Rebeschini. Linear convergence for natural policy gradient with log-linear
348"
REFERENCES,0.7859848484848485,"policy parametrization, 2023. URL https://arxiv.org/abs/2209.15382.
349"
REFERENCES,0.7878787878787878,"Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics,
350"
REFERENCES,0.7897727272727273,"Philadelphia, PA, 2017. doi: 10.1137/1.9781611974997.
351"
REFERENCES,0.7916666666666666,"Dimitri P. Bertsekas and John N. Tsitsiklis. Gradient convergence in gradient methods with errors.
352"
REFERENCES,0.7935606060606061,"SIAM Journal on Optimization, 10(3):627–642, 2000. doi: 10.1137/S1052623497331063.
353"
REFERENCES,0.7954545454545454,"Jalaj Bhandari and Daniel Russo. On the linear convergence of policy gradient methods for finite
354"
REFERENCES,0.7973484848484849,"MDPs. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics,
355"
REFERENCES,0.7992424242424242,"volume 130 of Proceedings of Machine Learning Research, pages 2386–2394. PMLR, 13–15 Apr
356"
REFERENCES,0.8011363636363636,"2021. URL https://proceedings.mlr.press/v130/bhandari21a.html.
357"
REFERENCES,0.803030303030303,"Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods, 2022.
358"
REFERENCES,0.8049242424242424,"URL https://arxiv.org/abs/1906.01786.
359"
REFERENCES,0.8068181818181818,"Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence
360"
REFERENCES,0.8087121212121212,"of natural policy gradient methods with entropy regularization. Operations Research, 70(4):
361"
REFERENCES,0.8106060606060606,"2563–2578, 2022. doi: 10.1287/opre.2021.2151.
362"
REFERENCES,0.8125,"Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel.
363"
REFERENCES,0.8143939393939394,"Model-based reinforcement learning via meta-policy optimization. In Proceedings of The 2nd Con-
364"
REFERENCES,0.8162878787878788,"ference on Robot Learning, volume 87 of Proceedings of Machine Learning Research, pages 617–
365"
REFERENCES,0.8181818181818182,"629. PMLR, 29–31 Oct 2018. URL https://proceedings.mlr.press/v87/clavera18a.
366"
REFERENCES,0.8200757575757576,"html.
367"
REFERENCES,0.821969696969697,"Yuhao Ding, Junzi Zhang, and Javad Lavaei. Beyond exact gradients: Convergence of stochastic
368"
REFERENCES,0.8238636363636364,"soft-max policy gradient methods with entropy regularization, 2022. URL https://arxiv.org/
369"
REFERENCES,0.8257575757575758,"abs/2110.10117.
370"
REFERENCES,0.8276515151515151,"Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient
371"
REFERENCES,0.8295454545454546,"methods for the linear quadratic regulator. In Proceedings of the 35th International Conference on
372"
REFERENCES,0.8314393939393939,"Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1467–1476.
373"
REFERENCES,0.8333333333333334,"PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/fazel18a.html.
374"
REFERENCES,0.8352272727272727,"Xin Guo, Anran Hu, and Junzi Zhang. Theoretical guarantees of fictitious discount algorithms for
375"
REFERENCES,0.8371212121212122,"episodic reinforcement learning and global convergence of policy gradient methods. Proceedings
376"
REFERENCES,0.8390151515151515,"of the AAAI Conference on Artificial Intelligence, 36(6):6774–6782, Jun. 2022. doi: 10.1609/aaai.
377"
REFERENCES,0.8409090909090909,"v36i6.20633.
378"
REFERENCES,0.8428030303030303,"Ben Hambly, Renyuan Xu, and Huining Yang. Policy gradient methods for the noisy linear quadratic
379"
REFERENCES,0.8446969696969697,"regulator over a finite horizon. SIAM Journal on Control and Optimization, 59(5):3359–3391,
380"
REFERENCES,0.8465909090909091,"2021. doi: 10.1137/20M1382386.
381"
REFERENCES,0.8484848484848485,"Ben Hambly, Renyuan Xu, and Huining Yang. Policy gradient methods find the nash equilibrium in n-
382"
REFERENCES,0.8503787878787878,"player general-sum linear-quadratic games, 2022. URL https://arxiv.org/abs/2107.13090.
383"
REFERENCES,0.8522727272727273,"Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Momentum-based policy gradient meth-
384"
REFERENCES,0.8541666666666666,"ods. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of
385"
REFERENCES,0.8560606060606061,"Proceedings of Machine Learning Research, pages 4422–4433. PMLR, 13–18 Jul 2020. URL
386"
REFERENCES,0.8579545454545454,"https://proceedings.mlr.press/v119/huang20a.html.
387"
REFERENCES,0.8598484848484849,"Feihu Huang, Shangqian Gao, and Heng Huang. Bregman gradient policy optimization. In In-
388"
REFERENCES,0.8617424242424242,"ternational Conference on Learning Representations, 2022. URL https://openreview.net/
389"
REFERENCES,0.8636363636363636,"forum?id=ZU-zFnTum1N.
390"
REFERENCES,0.865530303030303,"Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
391"
REFERENCES,0.8674242424242424,"Proceedings of the Nineteenth International Conference on Machine Learning, page 267–274.
392"
REFERENCES,0.8693181818181818,"Morgan Kaufmann Publishers Inc., 2002. doi: 10.5555/645531.656005.
393"
REFERENCES,0.8712121212121212,"Sham M Kakade. A natural policy gradient. In Advances in Neural Information Processing Sys-
394"
REFERENCES,0.8731060606060606,"tems, volume 14. MIT Press, 2001. URL https://proceedings.neurips.cc/paper_files/
395"
REFERENCES,0.875,"paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf.
396"
REFERENCES,0.8768939393939394,"Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
397"
REFERENCES,0.8787878787878788,"gradient methods under the Polyak-Łojasiewicz condition. In Machine Learning and Knowledge
398"
REFERENCES,0.8806818181818182,"Discovery in Databases, pages 795–811, Cham, 2016. Springer International Publishing. ISBN
399"
REFERENCES,0.8825757575757576,"978-3-319-46128-1.
400"
REFERENCES,0.884469696969697,"Vijay Konda and John Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
401"
REFERENCES,0.8863636363636364,"Processing Systems, volume 12. MIT Press, 1999. URL https://proceedings.neurips.cc/
402"
REFERENCES,0.8882575757575758,"paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf.
403"
REFERENCES,0.8901515151515151,"Yuxi Li, Csaba Szepesvari, and Dale Schuurmans. Learning exercise policies for american options.
404"
REFERENCES,0.8920454545454546,"In Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics,
405"
REFERENCES,0.8939393939393939,"volume 5 of Proceedings of Machine Learning Research, pages 352–359, Hilton Clearwater Beach
406"
REFERENCES,0.8958333333333334,"Resort, Clearwater Beach, Florida USA, 16–18 Apr 2009. PMLR. URL https://proceedings.
407"
REFERENCES,0.8977272727272727,"mlr.press/v5/li09d.html.
408"
REFERENCES,0.8996212121212122,"Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
409"
REFERENCES,0.9015151515151515,"rates of softmax policy gradient methods. In Proceedings of the 37th International Conference on
410"
REFERENCES,0.9034090909090909,"Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6820–6829.
411"
REFERENCES,0.9053030303030303,"PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/mei20b.html.
412"
REFERENCES,0.9071969696969697,"Jincheng Mei, Yue Gao, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. Leveraging non-uniformity
413"
REFERENCES,0.9090909090909091,"in first-order non-convex optimization. In Proceedings of the 38th International Conference on
414"
REFERENCES,0.9109848484848485,"Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 7555–7564.
415"
REFERENCES,0.9128787878787878,"PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/mei21a.html.
416"
REFERENCES,0.9147727272727273,"Yurii Nesterov. Introductory Lectures on Convex Optimization. Springer New York, NY, 2013. doi:
417"
REFERENCES,0.9166666666666666,"10.1007/978-1-4419-8853-9.
418"
REFERENCES,0.9185606060606061,"Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochas-
419"
REFERENCES,0.9204545454545454,"tic variance-reduced policy gradient. In Proceedings of the 35th International Conference on
420"
REFERENCES,0.9223484848484849,"Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4026–4035.
421"
REFERENCES,0.9242424242424242,"PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/papini18a.html.
422"
REFERENCES,0.9261363636363636,"Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Adaptive step-size for policy gradient meth-
423"
REFERENCES,0.928030303030303,"ods. In Advances in Neural Information Processing Systems, volume 26. Curran Associates,
424"
REFERENCES,0.9299242424242424,"Inc., 2013.
URL https://proceedings.neurips.cc/paper_files/paper/2013/file/
425"
REFERENCES,0.9318181818181818,"f64eac11f2cd8f0efa196f8ad173178e-Paper.pdf.
426"
REFERENCES,0.9337121212121212,"B.T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics
427"
REFERENCES,0.9356060606060606,"and Mathematical Physics, 3(4):864–878, 1963. doi: 10.1016/0041-5553(63)90382-3.
428"
REFERENCES,0.9375,"M.L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
429"
REFERENCES,0.9393939393939394,"Wiley & Sons, 2005. doi: 10.1002/9780470316887.
430"
REFERENCES,0.9412878787878788,"John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
431"
REFERENCES,0.9431818181818182,"policy optimization. In Proceedings of the 32nd International Conference on Machine Learning,
432"
REFERENCES,0.9450757575757576,"volume 37 of Proceedings of Machine Learning Research, pages 1889–1897, Lille, France, 07–09
433"
REFERENCES,0.946969696969697,"Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/schulman15.html.
434"
REFERENCES,0.9488636363636364,"Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi. Hessian aided policy
435"
REFERENCES,0.9507575757575758,"gradient. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of
436"
REFERENCES,0.9526515151515151,"Proceedings of Machine Learning Research, pages 5729–5738. PMLR, 09–15 Jun 2019. URL
437"
REFERENCES,0.9545454545454546,"https://proceedings.mlr.press/v97/shen19d.html.
438"
REFERENCES,0.9564393939393939,"Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
439"
REFERENCES,0.9583333333333334,"for reinforcement learning with function approximation. In Advances in Neural Information
440"
REFERENCES,0.9602272727272727,"Processing Systems, volume 12. MIT Press, 1999. URL https://proceedings.neurips.cc/
441"
REFERENCES,0.9621212121212122,"paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf.
442"
REFERENCES,0.9640151515151515,"Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
443"
REFERENCES,0.9659090909090909,"learning. Machine Learning, 8(3):229–256, 1992. doi: 10.1007/BF00992696.
444"
REFERENCES,0.9678030303030303,"Lin Xiao. On the convergence rates of policy gradient methods. Journal of Machine Learning
445"
REFERENCES,0.9696969696969697,"Research, 23(282):1–36, 2022. URL http://jmlr.org/papers/v23/22-0056.html.
446"
REFERENCES,0.9715909090909091,"Pan Xu, Felicia Gao, and Quanquan Gu. Sample efficient policy gradient methods with recursive
447"
REFERENCES,0.9734848484848485,"variance reduction. In International Conference on Learning Representations, 2020a. URL
448"
REFERENCES,0.9753787878787878,"https://openreview.net/forum?id=HJlxIJBFDr.
449"
REFERENCES,0.9772727272727273,"Pan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variance-
450"
REFERENCES,0.9791666666666666,"reduced policy gradient. In Proceedings of The 35th Uncertainty in Artificial Intelligence Confer-
451"
REFERENCES,0.9810606060606061,"ence, volume 115 of Proceedings of Machine Learning Research, pages 541–551. PMLR, 22–25
452"
REFERENCES,0.9829545454545454,"Jul 2020b. URL https://proceedings.mlr.press/v115/xu20a.html.
453"
REFERENCES,0.9848484848484849,"Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Ba¸sar. Global convergence of policy gradient
454"
REFERENCES,0.9867424242424242,"methods to (almost) locally optimal policies. SIAM Journal on Control and Optimization, 58(6):
455"
REFERENCES,0.9886363636363636,"3586–3612, 2020. doi: 10.1137/19M1288012.
456"
REFERENCES,0.990530303030303,"Kaiqing Zhang, Xiangyuan Zhang, Bin Hu, and Tamer Basar. Derivative-free policy optimization for
457"
REFERENCES,0.9924242424242424,"linear risk-sensitive and robust control design: Implicit regularization and sample complexity. In
458"
REFERENCES,0.9943181818181818,"Advances in Neural Information Processing Systems, volume 34, pages 2949–2964. Curran As-
459"
REFERENCES,0.9962121212121212,"sociates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/
460"
REFERENCES,0.9981060606060606,"file/1714726c817af50457d810aae9d27a2e-Paper.pdf.
461"
