Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002183406113537118,"Per-example gradient norms are a vital ingredient for estimating gradient noise
scale (GNS) with minimal variance. Observing the tensor contractions required to
compute them, we propose a method with minimal FLOPs in 3D or greater tensor
regimes by simultaneously computing the norms while computing the parameter
gradients. Using this method we are able to observe the GNS of different layers at
higher accuracy than previously possible. We find that the total GNS of contempo-
rary transformer models is predicted well by the GNS of only the normalization
layers. As a result, focusing only on the normalization layer, we develop a custom
kernel to compute the per-example gradient norms while performing the Layer-
Norm backward pass with zero throughput overhead. Tracking GNS on only those
layers, we are able to guide a practical batch size schedule that reduces training
time by 18% on a Chinchilla-optimal language model."
INTRODUCTION,0.004366812227074236,"1
Introduction"
INTRODUCTION,0.006550218340611353,"The gradients gathered during the backward pass while training a neural network are typically
inspected via their Frobenius norm, the magnitude of the vector. This gradient vector may be viewed
as the sum of gradients computed over each individual example in the minibatch. Each of these has
its own norm. In this work, we develop a method to access these norms that works at any scale, for
three common layer types in deep learning models: linear, normalization and embedding layers."
INTRODUCTION,0.008733624454148471,"One primary application of a per-example gradient norm is in estimating the Gradient Noise Scale
(GNS) [39], a metric that has been shown to be useful in training large scale models [9]. The
uncertainty of the GNS estimator depends directly on the size of the batch used to compute the small
batch gradient norm as shown in Section 2.1. So, the most precise estimate of the GNS is obtained by
computing the gradient norms for each example in the minibatch: the per-example gradient norm."
INTRODUCTION,0.010917030567685589,"To demonstrate GNS measurement in practice we perform experiments on contemporary language
model architectures, providing a detailed visualisation of the movement of the GNS components
throughout training, presented in Section 4. By inspecting these components it was found that the
GNS of the model is highly correlated between layer types, which we give an intuition for in Figure 1."
INTRODUCTION,0.013100436681222707,"However, the practical utility of measuring GNS with per-example gradient norms is only present if it
can be gathered without affecting training time. Focusing on LayerNorm [4] layers, we note the main
speed bottleneck is the memory I/O when not implemented as a fused kernel. To demonstrate this,
we develop a custom kernel to compute both the backward pass and the per-example gradient norms
at the same time. Using this kernel the throughput overhead of gathering the per-example gradient is
zero, even outperforming PyTorch’s LayerNorm at larger dimensions. We apply this to a practical
batch size schedule case study in Section 5. l1 l2 l3"
INTRODUCTION,0.015283842794759825,Minibatch l1 l2 l3
INTRODUCTION,0.017467248908296942,Minibatch l1 l2 l3
INTRODUCTION,0.019650655021834062,Minibatch l1 l2 l3
INTRODUCTION,0.021834061135371178,Aggregated
INTRODUCTION,0.024017467248908297,"Figure 1: Gradient noise scale (GNS) is typically computed by comparing per-minibatch (aggregated-
across-layers) gradients to gradients “Aggregated” across minibatches. We estimate GNS with lower
variance by making each minibatch a single example, and maintain per-layer GNS estimates. We find
the magnitude of gradients (visualized by the length of red arrows) to be consistent across layers,
enabling overall GNS to be computed very cheaply using only gradient stats from LayerNorm layers."
INTRODUCTION,0.026200873362445413,"To reiterate, the contributions of this work are:"
INTRODUCTION,0.028384279475982533,"• A minimal FLOP algorithm and implementation for computing gradients and per-example
gradient norms of linear layers simultaneously.1"
INTRODUCTION,0.03056768558951965,"• Observations that the measured GNS for LayerNorm layers is highly correlated with the
GNS of the remaining layers."
INTRODUCTION,0.03275109170305677,"• Development of an example kernel to implement tracking the GNS of LayerNorm layers
that does not affect network throughput (tokens/sec)."
INTRODUCTION,0.034934497816593885,"• Demonstration of a real application of GNS tracking in a batch size schedule experiment
that obtains an 18% wall-time speedup in training a Chinchilla-optimal [29] LLM."
BACKGROUND,0.03711790393013101,"2
Background"
GRADIENT NOISE SCALE,0.039301310043668124,"2.1
Gradient Noise Scale"
GRADIENT NOISE SCALE,0.04148471615720524,"GNS is a metric derived from observing a second order Taylor expansion of the change in a loss
function under the following assumption on the noise in the gradient estimate [39],"
GRADIENT NOISE SCALE,0.043668122270742356,"Gest(θ) ∼N

G(θ), 1"
GRADIENT NOISE SCALE,0.04585152838427948,"B Σ(θ)

,
(1)"
GRADIENT NOISE SCALE,0.048034934497816595,"where Gest is the observed gradient, B is the batch size, and θ the parameters of the model. Here,
G is the unobserved “true” gradient and Σ is the covariance of the gradient estimate. The Taylor
expansion mentioned is,"
GRADIENT NOISE SCALE,0.05021834061135371,E[L(θ −ϵGest)] = L(θ) −ϵ|G|2 + 1
GRADIENT NOISE SCALE,0.05240174672489083,"2ϵ2

GT HG + tr(HΣ) B"
GRADIENT NOISE SCALE,0.05458515283842795,"
.
(2)"
GRADIENT NOISE SCALE,0.056768558951965066,"Where ϵ is the learning rate and H is the Hessian of the loss. On the right hand side is a factor that
depends on B. It may be shown [39] that the optimal step size and optimal change in the loss is
achieved when B = Bnoise := tr(HΣ)/GT HG. Averaging this optimal step over an entire run, and
measuring this value by a grid search, yields Bcrit which describes a batch size that meets an optimal
tradeoff between cost and training speed. It is shown by analysis and experiment that Bnoise ≈Bcrit."
GRADIENT NOISE SCALE,0.05895196506550218,"As this depends on the Hessian, which is typically unavailable, McCandlish et al. [39] suggest making
the assumption that the Hessian is diagonal, which yields"
GRADIENT NOISE SCALE,0.0611353711790393,Bsimple = tr(Σ)
GRADIENT NOISE SCALE,0.06331877729257641,"GT G .
(3)"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.06550218340611354,1Similar algorithms for other layer types described in Appendix B
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.06768558951965066,"101
102
103
104
0.000 0.005 0.010 0.015 0.020 0.025"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.06986899563318777,GNS stderr
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.07205240174672489,"l=4, s=1
l=8, s=1
l=16, s=1
l=32, s=1
l=64, s=1
l=128, s=1"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.07423580786026202,"103
104
0.000 0.005 0.010 0.015 0.020 0.025"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.07641921397379912,GNS stderr
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.07860262008733625,"l=128, s=1
l=128, s=2
l=128, s=4
l=128, s=8
l=128, s=16
l=128, s=32
l=128, s=64"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.08078602620087336,"Figure 2: The variance of the GNS estimator for different Bbig (left) and Bsmall (right) sizes. Bbig = l
and Bsmall = s in legends. Stderr is estimated using a jackknife resampling method for ratio
estimators [12]. For the same number of samples processed, a smaller Bsmall always has a lower
standard error, while the size of the large batch, Bbig does not affect the standard error."
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.08296943231441048,"To compute Bsimple McCandlish et al. [39] define the unbiased estimators S and ∥G∥2
2 as:"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.0851528384279476,"∥G∥2
2 :=
1
Bbig −Bsmall"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.08733624454148471,"
Bbig
GBbig
2
2 −Bsmall ∥GBsmall∥2
2

≈GT G
(4)"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.08951965065502183,"S :=
1
1/Bsmall −1/Bbig"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.09170305676855896,"
∥GBsmall∥2
2 −
GBbig
2
2"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.09388646288209607,"
≈tr(Σ),
(5)"
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.09606986899563319,"where Bbig and Bsmall are the batch sizes used to compute the gradients GBbig and GBsmall, respectively
(potentially corresponding to Aggregated and Minibatch gradients as depicted in Figure 1).
GBbig

2 is trivially computed using the gradients accumulated for the optimizer but ∥GBsmall∥2 is not.
One option is to use the gradients communicated between Distributed Data Parallel (DDP) nodes,
but this has two downsides: (1) the variance of the estimate is tied to the DDP configuration and
(2) the estimate is not available in all training configurations. For example, experiments on a single
GPU cannot use this method. One can also access the gradients during gradient accumulation, but
this similarly depends on the training configuration. A full taxonomy of the options for computing
∥GBsmall∥2 is provided in Appendix A."
SIMILAR ALGORITHMS FOR OTHER LAYER TYPES DESCRIBED IN APPENDIX B,0.0982532751091703,"For each observation of
GBbig

2 we may observe multiple ∥GBsmall∥2, typically Bbig/Bsmall of them.
On each step the estimate of ∥GBsmall∥2
2 is therefore a mean over Bbig/Bsmall samples, whose variance
is reduced according to the law of large numbers. However, the GNS is a ratio of the unbiased
estimators in Equations 4 and 5, so it may not be clear how this affects uncertainty in the GNS
estimate. Figure 2 explores this relationship by simulation of a setting where the GNS is set to 1 while
varying Bbig and Bsmall. We find it is always better (less uncertainty) to use the smallest possible
Bsmall to estimate the GNS, while the choice of Bbig is irrelevant."
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.10043668122270742,"2.2
Efficient Per-example Gradient Norms"
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.10262008733624454,"Goodfellow [26] proposes a trick to compute gradient norms for individual examples in a minibatch,
which would provide the minimum variance estimate of the GNS as described in Section 2.1.
Neglecting the original derivation, by writing the desired squared norm as a tensor contraction the
trick may be reproduced automatically via einsum path optimization [49, 15]. The tensor contraction
for per-example gradient norms, n2
b, of a linear layer in the 2D setting is,"
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.10480349344978165,"n2
b =
X"
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.10698689956331878,"i,k
(w′)2
bik =
X"
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.1091703056768559,"i,k
xbixbiy′
bky′
bk,"
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.11135371179039301,"where x are the activations prior to a linear layer, y′ are the gradients of the loss with respect to the
outputs of the linear layer and w′ are the gradients of the loss with respect to the weights of the linear
layer."
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.11353711790393013,"Li et al. [36] extend this trick to the three dimensional case. For inputs X ∈RB×T ×I and outputs
Y ∈RB×T ×K, the per-example gradient norm nb is,"
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.11572052401746726,"n2
b = (w′)2
bik = (
X"
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.11790393013100436,"t
xbtiy′
btk)2 = xbtiy′
btkxbuiy′
buk = ⟨XXT , Y′Y′T ⟩2
F ,"
EFFICIENT PER-EXAMPLE GRADIENT NORMS,0.12008733624454149,"which has O(T 2) memory complexity in the sequence length T.2. Index sets are b ∈[1, B], i ∈
[1, I], k ∈[1, K], t, u ∈[1, T]. At some point, the I/O cost of computing the per-example gradient
norms by computing the full w′
b explicitly will be cheaper. Noting this fact motivated the work in
Section 3 and the practical relationship between these resource costs is explored in Section 3.1."
RELATED WORK,0.1222707423580786,"2.3
Related Work"
RELATED WORK,0.12445414847161572,"Gradient norms
One common motivation for computing per-example gradient norms is for differ-
ential privacy. By bounding the gradient for any single example, we can ensure each example has a
limited impact on the final parameters [45, 36]. Per-example gradient clipping has been performed
with convolutional networks [45] and sequential models, e.g., LLMs [36]. These methods allow
control over per-example gradient norms even when training with large batch sizes. Approaches like
these are implemented in the differential-privacy library Opacus [56], and have support natively in
PyTorch, but are less efficient than the methods proposed in this paper. An alternative mechanism
to manifest per-example gradient norms is to simply use a batch size of one. While not efficient
enough for training large-scale networks, such sequential training may arise in situations such as
reinforcement learning, where per-example gradient clipping has also been performed (to improve
stability [52])."
RELATED WORK,0.12663755458515283,"Gradient noise scale
The Gradient Noise Scale [39] has been widely used for training large-scale
neural networks. For example, Brown et al. [9] note the GNS was measured during training and used
to guide batch sizing when training GPT-3. Dey et al. [19] mention that operating near the critical
batch size, as dictated by the GNS, is important for hyperparameter transfer under the maximal
update parameterization [54]. Even when not explicitly mentioned in publications, open source code
often implements the GNS (e.g., see codebases [21, 13] for GPT-NeoX [7] and Hourglass Diffusion
Transformer [14])."
RELATED WORK,0.12882096069868995,"Measurements similar to the GNS have also been used in a range of prior work to guide batch sizing
for minibatch SGD [10, 17, 5, 55]. Chen et al. [11] show experimentally that wider networks can
be trained using larger batches; they also establish a theoretical connection between wider networks
and gradient variance, albeit for simple two-layer networks. In contrast, Shallue et al. [47] found
empirically that narrower Transformers scale better to larger batch sizes. Smith and Le [50] propose
a noise scale based not on gradient variance, but on the learning rate, dataset size, and batch size
(similar to the notion of temperature in Section 4.1). Zhang et al. [60] find the critical batch size
depends on the choice of optimizer. Faghri et al. [22] introduce a gradient clustering and stratified
sampling approach to minimize minibatch gradient variance, and use this approach as a tool to help
understand optimization."
RELATED WORK,0.13100436681222707,"Gradient variance
Beyond computing the GNS, our method can support other applications where
measuring the distribution of per-example gradients is useful or informative. Gradient variance
has been used to classify the difficulty of examples [1], which can be used, for example, to surface
problematic examples for human auditing. The question of whether gradient distributions tend toward
Gaussian in the (central) limit is of theoretical significance [50], with implications toward the ability
of SGD to escape sharp minima and land in wide basins [63, 41, 48]. Bounded gradient variance is
also assumed in some convergence analysis [8, 62], as noted in [22]."
RELATED WORK,0.1331877729257642,"Perhaps the most familiar use of gradient variance is of course in adaptive optimizers like Adagrad,
Adam, and others that reduce step sizes in high-gradient-noise directions [20, 57, 46, 33, 44]. Hilton
et al. [28, App. C] directly relate Adam second moment statistics to a component-wise version of the
GNS. Optimizers typically estimate gradients jointly across training steps and minibatches, however
vSGD [46] leverages separate components for gradient momentum and for gradient variation across
samples. Zhang et al. [61] find the variance of gradient norms across examples predictive of whether"
RELATED WORK,0.13537117903930132,"2This specific Einstein contraction is not used by Li et al. [36] but appears in the Backpack library [15] We
provide the vector algebra contraction path chosen by Li et al. [36] on the right."
RELATED WORK,0.13755458515283842,"vanilla SGD outperforms adaptive optimizers, however recent work has shown Adam to outperform
SGD even in the (noise-free) full gradient descent setting [34, 35]."
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.13973799126637554,"3
Simultaneous Per-example Gradient Norms"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.14192139737991266,"As described in Section 2, computing GNS requires small batch gradient norms. Typically, these may
be gathered during gradient accumulation or DDP communication.3 However, these methods are
not universally applicable and may not be available in all training configurations. In this section we
describe a method for baking the computation of the per-example gradient norms into the computation
graph, making it universally applicable. The typical tensor contraction used to compute the backward
gradient in a linear layer using the input activations, x, and gradients, g, is,"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.14410480349344978,"w′
k,l =
X
x...kg...l,"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.1462882096069869,"in other words, a sum over vector outer products for every vector in the trailing dimension. In principle,
it is possible to access the intermediate tensor containing the batch dimension w′
bkl = P xb...kgb...l.
This allows us to compute the per-example gradient norms with FLOPs scaling at the same rate as
the normal, non-per-example backward pass (Figure 3), albeit at increased I/O cost due to having to
materialize the intermediate tensor."
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.14847161572052403,"A generic algorithm to compute the per-example gradient norms simultaneously with the weight
gradient in a standard linear layer is provided in Algorithm 1 using einsum for readability and
portability.4 The reason for the correction in step 4 can be seen by considering the gradient of loss
function L with respect to the weights on a single example b, wb,"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.15065502183406113,"∇wb
1
B X"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.15283842794759825,"b
L(xb) = 1"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.15502183406113537,"B ∇wbL(xb),"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.1572052401746725,"computing the squared norm of this will therefore contain a factor of 1/B2, which must be corrected
for."
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.15938864628820962,Algorithm 1 Linear Layer Simultaneous Per-Example Gradient Norm Computation
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.1615720524017467,"Require: gradient tensor g of shape (B, ..., L), input activation tensor x of shape (B, ..., K)
Ensure: weight gradient tensor w′ of shape (K, L), mean of per-example squared norms ∥w′
b∥2
2
1: w′
b ←einsum(‘b...k, b...l →bkl’, x, g)
2: sw ←einsum(‘bkl →b’, w′2
b )
3: w′ ←einsum(‘bkl →kl’, w′
b)
4: ∥w′
b∥2
2 ←1/B × einsum(sw, ‘b →’) × B2 # reduce by mean then apply correction"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.16375545851528384,"5: return w′, ∥w′
b∥2
2"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.16593886462882096,"3.1
FLOPs and I/O Costs"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.16812227074235808,"The computational cost of computing per-example gradient norms can be broken down into FLOPs,
in Figure 3, and I/O, in Figure 4, with matrix multiplication on current devices being potentially
bottlenecked by both. We estimate ideal FLOP and DRAM I/O costs, assuming optimal reuse of
data loaded from DRAM into SRAM with no recomputation. In practice, duplicate computation
may be used to improve wall-clock time and to fit within hardware limitations of the amount of
shared memory available. We compare here against the efficient per-example gradient norm method
described by Li et al. [36], which the authors note is only efficient (in terms of I/O cost) when
2T 2 < PD, where T is the sequence length, P is input and D is output dimension of the linear layer.
This bound is discussed further in Appendix E."
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.1703056768558952,"In terms of FLOPS, Figure 3 shows the simultaneous per-example gradient norms are almost always
preferable, only being more expensive for very short sequence lengths in small models. The reason
for this is shown on the right hand side; the number of FLOPs required to compute the simultaneous
per-example gradient norms is independent of the sequence length."
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.17248908296943233,"3A complete taxonomy for small batch gradient computation is given in Appendix A.
4Additional algorithms for Embedding and LayerNorm layers are described in Appendix B."
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.17467248908296942,"108
109
1010
1011"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.17685589519650655,Parameters 1012 1013 1014 1015 1016 FLOPs
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.17903930131004367,"108
109
1010
1011"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.1812227074235808,Parameters 0.5 1.0 1.5 2.0 2.5
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.18340611353711792,GNS FLOPs / Model FLOPs
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.185589519650655,"Algorithm
Li et al
Simultaneous
Sequence Length
256
1024
2048
4096"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.18777292576419213,"Figure 3: FLOP cost of computing per-example gradient norms. (Left) Total FLOP cost. (Right)
Proportional cost versus one model forward and backward pass. The FLOP cost of Simultaneous
per-example gradient norms is strictly dominant to alternative methods (left) and the ratio of this
additional cost to the FLOP cost of processing the entire model does not depend on context length
(right)."
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.18995633187772926,"108
109
1010
1011"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.19213973799126638,"Parameters 10
1 100 101"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.1943231441048035,IO (proportion of model forward pass IO)
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.1965065502183406,"Algorithm
Li et al
Simultaneous
LN only
Sequence Length
256
1024
2048
4096
8192
16384
32768
65536"
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.19868995633187772,"Figure 4: Total I/O cost of computing per-example gradient norms, assuming gradients and parameters
are stored with 4 bytes of precision. The relative IO cost of Simultaneous per-example gradient
norms is less than Li et al. [36] for very long contexts for all model scales, approximately equivalent
for models of 10B parameters and 4096 context length, and higher for shorter contexts with larger
models. The IO cost of LN (LayerNorm) per-example gradient norms alone is much lower than either
method."
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.20087336244541484,"The I/O cost shown in 4 illustrates a tradeoff in computing the per-example gradient norm. The
simultaneous method is more expensive at large model sizes with short sequence length because it
must act on a large intermediate tensor."
SIMULTANEOUS PER-EXAMPLE GRADIENT NORMS,0.20305676855895197,"To estimate model flops, we use PyTorch’s FLOPCounterMode, which only measures the FLOPs in
matrix multiplications and attention computation, however these make up the vast majority of the
FLOPs in a Transformer model."
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2052401746724891,"4
Gradient Noise Scale in Transformer Language Models"
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2074235807860262,"Using the methods described in previous sections to measure per-example gradient norms and estimate
the GNS, we perform experiments on a 111M parameter Chinchilla-optimal language model [19, 29]
using the OpenWebText dataset [24].5 As the prior work was performed on Pile [23], Appendix C.1
describes an experiment to check the optimality of the Chinchilla model on this dataset. We also
found Flash attention led to numerical instability, which we were able to mitigate with an architectural
modification described in Appendix C.2."
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2096069868995633,"5The code to replicate these experiments may be found at https://github.com/CerebrasResearch/
nanoGNS/tree/main/exact."
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.21179039301310043,"10
3
10
2
10
1
100
101
102"
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.21397379912663755,"||G||2
2 100 101 102 Tr( )"
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.21615720524017468,GNS=100
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2183406113537118,GNS=1000
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2205240174672489,GNS=10000
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.22270742358078602,MLP and Attention
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.22489082969432314,"0
1
2
3
4
5
6
7
8
9
Combined"
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.22707423580786026,"0
2000
4000
6000
8000
Iteration 0 100 200 300 400 500 600 GNS"
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2292576419213974,"10
5
10
4
10
3
10
2"
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2314410480349345,"||G||2
2 10
4 10
3 10
2 Tr( )"
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2336244541484716,GNS=100
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.23580786026200873,GNS=1000
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.23799126637554585,GNS=10000
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.24017467248908297,LayerNorm
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2423580786026201,"0
1
2
3
4
5
6
7
8
9
ln_f
Combined"
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.2445414847161572,"0
2000
4000
6000
8000
Iteration 0 50 100 150 200 250 300 350 GNS"
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.24672489082969432,"Figure 5: GNS phase plot: Linear/Embedding layers are separated from LayerNorm layers by row.
Component estimators of Equations 4 and 5 are shown (left) with the GNS over the course of training
on the (right)."
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.24890829694323144,"All experiments computed per-example gradient norms for all layers in the model with the exception
of the performance results of Sections 5.1 and 5.2, which only computed per-example gradient norms
for the normalization layers. Each experiment was run on Nvidia A10 GPUs, in either 12 or 24 hours
depending on the precision used, Bfloat16 or Float32 respectively. We used the nanoGPT6 codebase
with the layers described in Section 3 added."
GRADIENT NOISE SCALE IN TRANSFORMER LANGUAGE MODELS,0.25109170305676853,"Having an accurate estimate of the GNS statistics ∥G∥2
2 and S allows us to visualize the movement
of both in a phase space during training as shown in Figure 5. LayerNorm layers are separate from
the rest of the network because their statistics are much smaller and to illustrate how the resulting
GNS estimates on the right track each other. To observe these trends in another training regime, see
Figure 14 in Appendix D.1."
THE TEMPERATURE OF TRAINING,0.25327510917030566,"4.1
The Temperature of Training"
THE TEMPERATURE OF TRAINING,0.2554585152838428,"McCandlish et al. [39, App. C] observed that the GNS measurement depends on the batch size and
learning rate used in training. In fact, from the derivation outlined in Section 2.1, the gradient noise
scale is only well-defined at the optimal learning rate. Using a toy model of a quadratic loss function,
they observed that the GNS should be inversely proportional to the temperature, T, a ratio of batch
size B to learning rate ϵ:"
THE TEMPERATURE OF TRAINING,0.2576419213973799,Bnoise ∝Bsimple ∝1 T = B ϵ .
THE TEMPERATURE OF TRAINING,0.259825327510917,"This enables a testable prediction that the GNS will increase with increasing batch size or with
descending learning rate. This prediction was found to accurately describe experiments on a small"
THE TEMPERATURE OF TRAINING,0.26200873362445415,6https://github.com/karpathy/nanoGPT
THE TEMPERATURE OF TRAINING,0.26419213973799127,"5.5
6.0
6.5
7.0
7.5
8.0
8.5
Tokens processed
1e8 102 103 GNS"
THE TEMPERATURE OF TRAINING,0.2663755458515284,"original , 16B"
THE TEMPERATURE OF TRAINING,0.2685589519650655,"/16, B
/4, 4B
/4, B/4"
THE TEMPERATURE OF TRAINING,0.27074235807860264,"Figure 6: During the middle of training a 111M parameter language model on OpenWebText, the
learning rate, ϵ or batch size, B were varied, restarting the run from the same point. This Figure
replicates an experiment from McCandlish et al. [39] showing how varying the ratio causes changes
in the measured GNS, but here only due to changes in the learning rate. Changes in the batch size do
not have the predicted effect."
THE TEMPERATURE OF TRAINING,0.27292576419213976,"convolutional model on the SVHN dataset. We repeat it here in the setting described above in Figure 6.
To match the results of McCandlish et al. [39], all interventions tested should yield the same result.
We find the GNS does indeed react predictably to changes in the learning rate, but the reactions to
changes in the batch size are not predicted by the theory."
GNS CORRELATES BETWEEN LAYER TYPES,0.27510917030567683,"4.2
GNS Correlates Between Layer Types"
GNS CORRELATES BETWEEN LAYER TYPES,0.27729257641921395,"Inspection of Figure 5 suggests the LayerNorm layers produce a similar GNS, when combined, as
the total GNS of the model. Before describing how to quantify this relationship we must first note
that the unbiased estimators ∥G∥2
2 and S are noisy. All GNS figures presented in this paper and other
work smooth both of these estimators, typically with an Exponential Moving Average (EMA) filter,
before computing the GNS ratio.7"
GNS CORRELATES BETWEEN LAYER TYPES,0.2794759825327511,"So, when quantifying the relationship between the GNS of different layers, it must be compared for
different smoothing factors. Here, we show the regression coefficients with respect to the alpha of
the EMA filter in Figure 7. The results show that the GNS of the LayerNorm and Attention layers
are highly predictive of the total GNS of the model. In both cases, the slope is approximately 1.4,
meaning the total GNS is approximately 1.4 times the GNS of the LayerNorm or Attention layers."
GNS CORRELATES BETWEEN LAYER TYPES,0.2816593886462882,"Comparing the quality of this fit versus the quality of prior work’s overall fit of the GNS to the critical
batch size (measured empirically) [39], the quality seems acceptable and we do not need to apply this
1.4x correction factor, rather we just note that the true Bcrit may be greater than the measured Bsimple."
BATCH SIZE SCHEDULING,0.2838427947598253,"5
Batch Size Scheduling"
BATCH SIZE SCHEDULING,0.28602620087336245,"We focus on two concerns that affect the practicality of batch size scheduling. First, measuring the
appropriate batch size without incurring any additional training time. We find this is possible with the
method described in Section 5.1. Second, whether batch size scheduling is effective in practice. We
find it can offer significant savings in the required number of tokens processed in Section 5.2."
UNIVERSAL GNS WITH ZERO OVERHEAD,0.28820960698689957,"5.1
Universal GNS with Zero Overhead"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.2903930131004367,"Capturing a GNS estimate for a linear layer is powerful, but efficiently doing so presents a challenge.
Such an estimate requires accumulating per-example gradients of hidden_size2 across the sequence
dimension, compared to just hidden_size with LayerNorm. This increased size requires using more
complex reductions in the kernel, rather than a simple warp reduction followed by shared-memory
atomic reduction with a final atomic global reduction (as we can implement for LayerNorm per-
example gradients within shared memory). In addition, linear layer kernels are already highly
optimized and require using advanced techniques to keep GPU tensor cores fed with data, so"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.2925764192139738,7The results described in Figure 5 are explored at a 1.3B parameter scale in Appendix D.3.
UNIVERSAL GNS WITH ZERO OVERHEAD,0.29475982532751094,"0
1
2
Tokens
1e9 100 101 102 103 104 GNS"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.29694323144104806,"Attention
LayerNorm
MLP
Embedding
Total"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.29912663755458513,"0.96
0.98
1.00
alpha 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 slope"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.30131004366812225,"0.96
0.98
1.00
alpha 0.96 0.97 0.98 0.99 1.00 r"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3034934497816594,"Figure 7: Regression of total GNS using the GNS of each layer type. (Left) GNS of each layer
type and the total GNS are plotted against the number of tokens processed for varying EMA alpha
settings. (Center & Right) The slope and Pearson’s correlation coefficient of the regression of the
total GNS against the GNS of each layer type, respectively, as a function of the same EMA alpha
values. The total GNS (black) on the left is predicted well by individual layer types as indicated by
the correlation coefficients (right), however the type with slope closest to 1 is LayerNorm (center),
only overestimating the GNS by less than 40% across EMA alpha values."
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3056768558951965,"768
2048
4096
Dimensionality 0 50 100 150 200"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3078602620087336,Average Fwd + Bwd Time (ms)
UNIVERSAL GNS WITH ZERO OVERHEAD,0.31004366812227074,"Implementation
LayerNorm (PyTorch 2.4.0)
Our Kernel"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.31222707423580787,"Figure 8: Comparison of average time taken for a LayerNorm forward and backward pass with gradi-
ent accumulation when using PyTorch’s native implementation versus our custom kernel computing
per-example gradient norms in tandem. Measured on an Nvidia H100 GPU."
UNIVERSAL GNS WITH ZERO OVERHEAD,0.314410480349345,"combining such a kernel with per-example gradient computation - with its own memory overheads
and corresponding available bandwidth reduction - would be a difficult undertaking."
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3165938864628821,"We thus implemented a LayerNorm-specific CUDA kernel that also captures GNS. In experiments
with language models at different scales, illustrated in Figure 8, we find this kernel has practically
zero overhead compared to PyTorch’s LayerNorm implementation. The complete source code for
this kernel is provided with the accompanying code for this paper8."
UNIVERSAL GNS WITH ZERO OVERHEAD,0.31877729257641924,"5.2
Case Study: Batch Size Schedule"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.32096069868995636,"As a case study we continue with the 111M parameter language model on OpenWebText described
above. Over three seeds, we run both a fixed batch size and a batch size schedule that increases
linearly with the number of tokens processed to the original batch size. We vary the batch size during
training by varying the number of gradient accumulation steps."
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3231441048034934,8https://github.com/CerebrasResearch/nanoGNS/tree/main/exact/normgnorm
UNIVERSAL GNS WITH ZERO OVERHEAD,0.32532751091703055,"105
106
107
108
109"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.32751091703056767,Tokens processed 101
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3296943231441048,3 × 100
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3318777292576419,4 × 100
UNIVERSAL GNS WITH ZERO OVERHEAD,0.33406113537117904,6 × 100
UNIVERSAL GNS WITH ZERO OVERHEAD,0.33624454148471616,Train loss
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3384279475982533,"Original
With batch size schedule"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3406113537117904,"101
3 × 100
4 × 100
6 × 100"
UNIVERSAL GNS WITH ZERO OVERHEAD,0.34279475982532753,Train loss 0 1 2 3 4
UNIVERSAL GNS WITH ZERO OVERHEAD,0.34497816593886466,Tokens saved 1e8
UNIVERSAL GNS WITH ZERO OVERHEAD,0.3471615720524017,"Figure 9: (Left) Linear batch size schedule tracking the GNS over 2.2 billion tokens processed. Loss
is plotted over a smoothed range from 3 runs using different Seeds. (Right) The number of tokens
saved over the fixed batch size run to achieve the same loss."
UNIVERSAL GNS WITH ZERO OVERHEAD,0.34934497816593885,"The results of this experiment are shown in Figure 9. The left plot shows the progression of the loss
for both models, with the range of values captured over different seeds. The mean loss for the linear
batch size schedule leads the fixed batch size throughout training. On the right, this lead is quantified
by interpolating the number of tokens saved to achieve the same loss. The precise schedule used is
shown in Figure 15 in Appendix D.2."
LIMITATIONS,0.35152838427947597,"6
Limitations"
LIMITATIONS,0.3537117903930131,"In this paper, we only studied Transformers, which include Normalization sub-layers natively. While
Transformers are ubiquitous in machine learning, there are many models, including variations of
RNNs, CNNs, and state-space models, that do not use such layers conventionally. However, we
note LayerNorm could be added to these networks with very little overhead (in fact, the desire
to normalize activations in RNNs was one of the original motivations for developing LayerNorm;
application of batch normalization [30] to RNNs was “not obvious” [4]). Nevertheless, investigating
LayerNorm-based GNS in these other models requires further work."
LIMITATIONS,0.3558951965065502,"Our work is also part of efforts to improve efficiency and address the increasing costs of training
and tuning large neural networks [6]. We provide both a more-efficient technique for computing
the GNS, and also, by enabling use of GNS statistics, we support compute-efficient training recipes,
such as use of dynamic batch sizes. While some have argued that hyperscalers may re-invest any
efficiency savings into ever-larger models [42], for academic researchers, such savings could allow
pushing the state-of-the-art, while still getting results in a reasonable timeframe. Recent efforts
to enable frontier-model-performance within academic budgets are encouraging, both to reduce
memory [38, 18] and save compute [37, 2]. Of course, even for such economical approaches,
“extensive hyperparameter search” may still be required [31]. There is a growing awareness that
hyperparameter tuning has a negative impact on equity in AI research, as tuning success depends
directly on researcher finances [51]. A correlated trend is to use better training measurements
(such as gradient noise in batch and step size optimizers (Section 2.3)) to reduce dependence on
hyperparameters, and in this way we hope our work can also ultimately improve research equity."
CONCLUSION,0.35807860262008734,"7
Conclusion"
CONCLUSION,0.36026200873362446,"This work set out to provide a practical method for computing the per-example gradient norms
necessary to compute the GNS independent of the training configuration. In the process we discovered
that not all the layers are necessary for a practical estimate of the GNS and that the per-example
gradient norms can be computed for the normalization layers with zero overhead. This enabled
practical experiments, such as a batch size schedule and replicating prior GNS observations. We
are hopeful that democratising access to GNS statistics, on any device, will enable subsequent
discoveries."
REFERENCES,0.3624454148471616,References
REFERENCES,0.3646288209606987,"[1] Chirag Agarwal, Daniel D’souza, and Sara Hooker. 2022. Estimating example difficulty using
variance of gradients. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 10368–10378."
REFERENCES,0.36681222707423583,"[2] Sotiris Anagnostidis, Gregor Bachmann, and Thomas Hofmann. 2023. Navigating Scaling
Laws: Accelerating Vision Transformer’s Training via Adaptive Strategies. arXiv preprint
arXiv:2311.03233 (2023)."
REFERENCES,0.36899563318777295,"[3] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. 2022.
High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the
Representation. arXiv:2205.01445 [stat.ML] https://arxiv.org/abs/2205.01445"
REFERENCES,0.37117903930131,"[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016.
Layer Normalization.
arXiv:1607.06450 [stat.ML]"
REFERENCES,0.37336244541484714,"[5] Lukas Balles, Javier Romero, and Philipp Hennig. 2016. Coupling adaptive batch sizes with
learning rates. arXiv preprint arXiv:1612.05086 (2016)."
REFERENCES,0.37554585152838427,"[6] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.
On the dangers of stochastic parrots: Can language models be too big?. In Proceedings of the
2021 ACM conference on fairness, accountability, and transparency. 610–623."
REFERENCES,0.3777292576419214,"[7] Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,
Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022.
GPT-NeoX-20B: An Open-Source Autoregressive Language Model. In Proceedings of the ACL
Workshop on Challenges & Perspectives in Creating Large Language Models."
REFERENCES,0.3799126637554585,"[8] Léon Bottou, Frank E Curtis, and Jorge Nocedal. 2018. Optimization methods for large-scale
machine learning. SIAM review 60, 2 (2018), 223–311."
REFERENCES,0.38209606986899564,"[9] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
arXiv:2005.14165 [cs.CL] https://arxiv.org/abs/2005.14165"
REFERENCES,0.38427947598253276,"[10] Richard H Byrd, Gillian M Chin, Jorge Nocedal, and Yuchen Wu. 2012. Sample size selection
in optimization methods for machine learning. Mathematical programming 134, 1 (2012),
127–155."
REFERENCES,0.3864628820960699,"[11] Lingjiao Chen, Hongyi Wang, Jinman Zhao, Dimitris Papailiopoulos, and Paraschos Koutris.
2018. The effect of network width on the performance of large-batch training. Advances in
neural information processing systems 31 (2018)."
REFERENCES,0.388646288209607,"[12] Denis Choquet, Pierre L’Ecuyer, and Christian Léger. 1999. Bootstrap Confidence Intervals
for Ratios of Expectations. ACM Trans. Model. Comput. Simul. 9, 4 (oct 1999), 326–348.
https://doi.org/10.1145/352222.352224"
REFERENCES,0.39082969432314413,"[13] Katherine Crowson. 2024. k-diffusion. https://github.com/crowsonkb/k-diffusion.
GitHub repository."
REFERENCES,0.3930131004366812,"[14] Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Z
Kaplan, and Enrico Shippole. 2024. Scalable High-Resolution Pixel-Space Image Synthesis
with Hourglass Diffusion Transformers. arXiv preprint arXiv:2401.11605 (2024)."
REFERENCES,0.3951965065502183,"[15] Felix Dangel, Frederik Kunstner, and Philipp Hennig. 2020. BackPACK: Packing more into
Backprop. In International Conference on Learning Representations. https://openreview.
net/forum?id=BJlrF24twB"
REFERENCES,0.39737991266375544,"[16] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FlashAttention:
Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv:2205.14135 [cs.LG]
https://arxiv.org/abs/2205.14135"
REFERENCES,0.39956331877729256,"[17] Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein. 2016. Big batch SGD: Automated
inference using adaptive batch sizes. arXiv preprint arXiv:1610.05792 (2016)."
REFERENCES,0.4017467248908297,"[18] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient
finetuning of quantized LLMs. Advances in Neural Information Processing Systems 36 (2023)."
REFERENCES,0.4039301310043668,"[19] Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu
Pathria, Marvin Tom, and Joel Hestness. 2023. Cerebras-GPT: Open Compute-Optimal Lan-
guage Models Trained on the Cerebras Wafer-Scale Cluster.
arXiv:2304.03208 [cs.LG]
https://arxiv.org/abs/2304.03208"
REFERENCES,0.40611353711790393,"[20] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online
learning and stochastic optimization. Journal of machine learning research 12, 7 (2011)."
REFERENCES,0.40829694323144106,"[21] EleutherAI. 2024. GPT-NeoX. https://github.com/EleutherAI/gpt-neox.
GitHub
repository."
REFERENCES,0.4104803493449782,"[22] Fartash Faghri, David Duvenaud, David J Fleet, and Jimmy Ba. 2020. A study of gradient
variance in deep learning. arXiv preprint arXiv:2007.04532 (2020)."
REFERENCES,0.4126637554585153,"[23] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The
Pile: An 800GB Dataset of Diverse Text for Language Modeling. arXiv:2101.00027 [cs.CL]"
REFERENCES,0.4148471615720524,"[24] Aaron Gokaslan and Vanya Cohen. 2019. OpenWebText Corpus. http://Skylion007.
github.io/OpenWebTextCorpus."
REFERENCES,0.4170305676855895,"[25] Alicia Golden, Samuel Hsia, Fei Sun, Bilge Acun, Basil Hosmer, Yejin Lee, Zachary DeVito,
Jeff Johnson, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2024. Is Flash Attention
Stable? arXiv:2405.02803 [cs.LG]"
REFERENCES,0.4192139737991266,"[26] Ian
Goodfellow.
2015.
Efficient
Per-Example
Gradient
Computations.
arXiv:1510.01799 [stat.ML] https://arxiv.org/abs/1510.01799"
REFERENCES,0.42139737991266374,"[27] Gavia Gray, Anshul Samar, and Joel Hestness. 2023. Efficient and Approximate Per-Example
Gradient Norms for Gradient Noise Scale. In Workshop on Advancing Neural Network Training:
Computational Efficiency, Scalability, and Resource Optimization (WANT@NeurIPS 2023).
https://openreview.net/forum?id=xINTMAvPQA"
REFERENCES,0.42358078602620086,"[28] Jacob Hilton, Karl Cobbe, and John Schulman. 2022. Batch size-invariance for policy optimiza-
tion. arXiv:2110.00641 [cs.LG]"
REFERENCES,0.425764192139738,"[29] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia
Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent
Sifre. 2022. Training Compute-Optimal Large Language Models. arXiv:2203.15556 [cs.CL]"
REFERENCES,0.4279475982532751,"[30] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network
training by reducing internal covariate shift. In International conference on machine learning.
pmlr, 448–456."
REFERENCES,0.43013100436681223,"[31] Peter Izsak, Moshe Berchansky, and Omer Levy. 2021. How to train BERT with an academic
budget. arXiv preprint arXiv:2104.07705 (2021)."
REFERENCES,0.43231441048034935,"[32] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli
Laine. 2024.
Analyzing and Improving the Training Dynamics of Diffusion Models.
arXiv:2312.02696 [cs.CV] https://arxiv.org/abs/2312.02696"
REFERENCES,0.4344978165938865,"[33] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 (2014)."
REFERENCES,0.4366812227074236,"[34] Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. 2023. Noise
is not the main factor behind the gap between SGD and Adam on transformers, but sign descent
might be. arXiv preprint arXiv:2304.13960 (2023)."
REFERENCES,0.4388646288209607,"[35] Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, and Alberto Bietti. 2024.
Heavy-tailed class imbalance and why Adam outperforms gradient descent on language models.
arXiv preprint arXiv:2402.19449 (2024)."
REFERENCES,0.4410480349344978,"[36] Xuechen Li, Florian Tramèr, Percy Liang, and Tatsunori Hashimoto. 2022. Large Language
Models Can Be Strong Differentially Private Learners. arXiv:2110.05679 [cs.LG] https:
//arxiv.org/abs/2110.05679"
REFERENCES,0.4432314410480349,"[37] Xianhang Li, Zeyu Wang, and Cihang Xie. 2023. CLIPA-v2: Scaling CLIP Training with
81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget; An Extra $4,000 Unlocks 81.8%
Accuracy. arXiv preprint arXiv:2306.15658 (2023)."
REFERENCES,0.44541484716157204,"[38] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and
Sanjeev Arora. 2023. Fine-tuning language models with just forward passes. Advances in
Neural Information Processing Systems 36 (2023), 53038–53075."
REFERENCES,0.44759825327510916,"[39] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. 2018. An Empirical
Model of Large-Batch Training. arXiv:1812.06162 [cs.LG] https://arxiv.org/abs/1812.
06162"
REFERENCES,0.4497816593886463,"[40] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. 2018. Spectral
Normalization for Generative Adversarial Networks.
arXiv:1802.05957 [cs.LG] https:
//arxiv.org/abs/1802.05957"
REFERENCES,0.4519650655021834,"[41] Thanh Huy Nguyen, Umut Simsekli, Mert Gurbuzbalaban, and Gaël Richard. 2019. First exit
time analysis of stochastic gradient descent under heavy-tailed gradient noise. Advances in
neural information processing systems 32 (2019)."
REFERENCES,0.45414847161572053,"[42] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel
Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural
network training. arXiv preprint arXiv:2104.10350 (2021)."
REFERENCES,0.45633187772925765,"[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.
Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9."
REFERENCES,0.4585152838427948,"[44] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2019. On the convergence of Adam and
beyond. arXiv preprint arXiv:1904.09237 (2019)."
REFERENCES,0.4606986899563319,"[45] Gaspar Rochette, Andre Manoel, and Eric W. Tramel. 2019. Efficient Per-Example Gradient
Computations in Convolutional Neural Networks. arXiv:1912.06015 [cs.LG] https://arxiv.
org/abs/1912.06015"
REFERENCES,0.462882096069869,"[46] Tom Schaul, Sixin Zhang, and Yann LeCun. 2013. No more pesky learning rates. In International
conference on machine learning. PMLR, 343–351."
REFERENCES,0.4650655021834061,"[47] Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E Dahl. 2019. Measuring the effects of data parallelism on neural network training.
Journal of Machine Learning Research 20, 112 (2019), 1–49."
REFERENCES,0.4672489082969432,"[48] Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. 2019.
A tail-index analysis of
stochastic gradient noise in deep neural networks. In International Conference on Machine
Learning. PMLR, 5827–5837."
REFERENCES,0.46943231441048033,"[49] Daniel G. A. Smith and Johnnie Gray. 2018. opt_einsum - A Python package for optimizing
contraction order for einsum-like expressions. Journal of Open Source Software 3, 26 (2018),
753. https://doi.org/10.21105/joss.00753"
REFERENCES,0.47161572052401746,"[50] Samuel L Smith and Quoc V Le. 2017. A Bayesian perspective on generalization and stochastic
gradient descent. arXiv preprint arXiv:1710.06451 (2017)."
REFERENCES,0.4737991266375546,"[51] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considera-
tions for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019)."
REFERENCES,0.4759825327510917,"[52] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. 2016.
Dueling network architectures for deep reinforcement learning. In International conference on
machine learning. PMLR, 1995–2003."
REFERENCES,0.4781659388646288,"[53] Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D.
Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-
dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. 2023. Small-scale
proxies for large-scale Transformer training instabilities. arXiv:2309.14322 [cs.LG] https:
//arxiv.org/abs/2309.14322"
REFERENCES,0.48034934497816595,"[54] Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick
Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. 2021. Tuning Large Neural Networks
via Zero-Shot Hyperparameter Transfer. In Advances in Neural Information Processing Systems."
REFERENCES,0.48253275109170307,"[55] Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, and
Peter Bartlett. 2018. Gradient diversity: a key ingredient for scalable distributed learning. In
International Conference on Artificial Intelligence and Statistics. PMLR, 1998–2007."
REFERENCES,0.4847161572052402,"[56] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad,
Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode,
and Ilya Mironov. 2022. Opacus: User-Friendly Differential Privacy Library in PyTorch.
arXiv:2109.12298 [cs.LG]"
REFERENCES,0.4868995633187773,"[57] Matthew D Zeiler. 2012.
Adadelta: an adaptive learning rate method.
arXiv preprint
arXiv:1212.5701 (2012)."
REFERENCES,0.4890829694323144,"[58] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe
Zhang, Jiatao Gu, and Josh Susskind. 2023. Stabilizing Transformer Training by Preventing
Attention Entropy Collapse. arXiv:2303.06296 [cs.LG] https://arxiv.org/abs/2303.
06296"
REFERENCES,0.4912663755458515,"[59] Biao Zhang and Rico Sennrich. 2019.
Root Mean Square Layer Normalization.
arXiv:1910.07467 [cs.LG]"
REFERENCES,0.49344978165938863,"[60] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger Grosse. 2019. Which Algorithmic Choices Matter at Which
Batch Sizes? Insights From a Noisy Quadratic Model. arXiv:1907.04164 [cs.LG]"
REFERENCES,0.49563318777292575,"[61] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi,
Sanjiv Kumar, and Suvrit Sra. 2020. Why are adaptive methods good for attention models?
Advances in Neural Information Processing Systems 33 (2020), 15383–15393."
REFERENCES,0.4978165938864629,"[62] Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. 2022. Adam can
converge without any modification on update rules. Advances in neural information processing
systems 35 (2022), 28386–28399."
REFERENCES,0.5,"[63] Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. 2018. The anisotropic noise
in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization
effects. arXiv preprint arXiv:1803.00195 (2018)."
REFERENCES,0.5021834061135371,"A
Taxonomy"
REFERENCES,0.5043668122270742,Gray et al. [27] included an prior version of this taxonomy in their work.
REFERENCES,0.5065502183406113,"The following taxonomy describes the different methods available to compute the ∥GBsmall∥2
2 necessary
to compute the GNS as described in Section 2.1. “Gradient norm cost” below refers to the cost of
computing the norm of the gradient for all parameters in the model, which is typically orders of
magnitude smaller than the cost of forward or backward passes."
REFERENCES,0.5087336244541485,"• Microbatch: multiple ∥GBsmall∥2
2 are computed over a set of microbatches"
REFERENCES,0.5109170305676856,"– DDP: Each ∥GBsmall∥2
2 is computed before gradients are communicated between DDP
nodes [39].
Pros: Only gradient norm cost.
Cons: Variance tied to number of DDP nodes (see Figure 2), can’t be used on one node.
– Sequential: Each ∥GBsmall∥2
2 are computed sequentially during gradient accumulation.
Pros: Only gradient norm cost.
Cons: Variance tied to the number of gradient accumulation steps."
REFERENCES,0.5131004366812227,"• Subbatch: During gradient accumulation, select ∥GBsmall∥2
2 partway through.
Pros: Only gradient norm cost, easy to implement.
Cons: Higher variance than Microbatch as ∥GBsmall∥2
2 is not averaged.
• Per-example:"
REFERENCES,0.5152838427947598,"Pros: Independent of gradient accumulation or DDP configuration, minimal variance."
REFERENCES,0.517467248908297,– Exact:
REFERENCES,0.519650655021834,"* ∥GBsmall∥2
2 is computed directly by the per-example gradient trick [26, 36].
Pros: Minimal cost in 2D regime.
Cons: Redundant computation required in 3D regime."
REFERENCES,0.5218340611353712,"* ∥GBsmall∥2
2 is computed in tandem with the parameter gradients using the method
described in Section 3.
Pros: No redundant computation.
Cons: Expansion in memory causes slowdowns as described in Section 3.1.
– Approximation: ∥GBsmall∥2
2 is approximated by assuming input activations are normally
distributed with mean zero [27].
Pros: Fewer FLOPs than Exact methods.
Cons: Not exact."
REFERENCES,0.5240174672489083,"All of the methods described above can be measured either online or offline. The description above
focuses on the online case; i.e. measuring the gradient norms during training. To use these methods
offline: run the models without performing weight updates and measure gradient norms the same
way. The estimators of Equation 4 and 5 can then be aggregated using a mean rather than an EMA or
by using a method to estimate measurement uncertainty such as the jackknife mentioned in Figure 2
(described in the context of GNS by Gray et al. [27, App.B]). This can be useful to estimate how
long to run the offline estimate."
REFERENCES,0.5262008733624454,"B
Additional Simultaneous Per-Example Gradient Norm Computations"
REFERENCES,0.5283842794759825,"Algorithms 3 and 2 describe the process for computing the per-example gradient norms for the
embedding and LayerNorm layers, which are typically the remaining layers in Transformer models.
RMSNorm [59] is practically identical to LayerNorm in this case because the parameters the gradient
is computed wrt are in the affine transform, which is the same in both layer types."
REFERENCES,0.5305676855895196,"C
Language Model Experiment Details"
REFERENCES,0.5327510917030568,"As mentioned in the text, the code to run the experiments described in this paper can be found at
https://github.com/CerebrasResearch/nanoGNS/tree/main/exact."
REFERENCES,0.5349344978165939,Algorithm 2 Layernorm Simultaneous Per-Example Gradient Norm Computation
REFERENCES,0.537117903930131,"Require: gradient tensor g of shape (B, ..., K), input activation tensor x of shape (B, ..., K)
Ensure: gamma gradient tensor γ′ of shape (K, ), mean of per-example squared norms ∥γ′
b∥2
2,
gradient tensor β′ of shape (K, ), mean of per-example squared norms
β′
b
2
2
1: γ′
b ←einsum(‘b...k, b...k →bk’, x, g)
2: sγ ←einsum(‘bk →b’, γ′2
b )
3: γ′ ←einsum(‘bk →k’, γ′
b)
4: ∥γ′
b∥2
2 ←1/B × einsum(sγ, ‘b →’) × B2 # reduce by mean then apply correction
5: β′
b ←einsum(‘b...k →bk’, g)
6: sβ ←einsum(‘bk →b’, β′2
b )
7: β′ ←einsum(‘bk →k’, β′
b) 8:"
REFERENCES,0.5393013100436681,"β′
b
2
2 ←1/B × einsum(sβ, ‘b →’) × B2 # reduce by mean then apply correction"
REFERENCES,0.5414847161572053,"9: return γ′, ∥γ′
b∥2
2, β′,
β′
b
2
2"
REFERENCES,0.5436681222707423,Algorithm 3 Embedding Layer Simultaneous Per-Example Gradient Norm Computation
REFERENCES,0.5458515283842795,"Require: gradient tensor g of shape (B, T, D), input id tensor x of shape (B, T), vocabulary size V
Ensure: weight gradient tensor w′ of shape (V, D), mean of per-example squared norms ∥w′
b∥2
2
1: o ←onehot(x, V )
2: w′
b ←einsum(‘btv, btd →bvd’, o, g)
3: sw ←einsum(‘bvd →b’, w′2
b )
4: w′ ←einsum(‘bvd →vd’, w′
b)
5: ∥w′
b∥2
2 ←1/B × einsum(sw, ‘b →’) × B2 # reduce by mean then apply correction"
REFERENCES,0.5480349344978166,"6: return w′, ∥w′
b∥2
2"
REFERENCES,0.5502183406113537,"C.1
Optimality on OpenWebText"
REFERENCES,0.5524017467248908,"We chose to use the Cerebras-GPT [19] recipes for experiments as they are designed to be Chinchilla
optimal. This means that each model size should achieve the lowest possible loss for a given FLOP
budget [29]. However, these recipes were tuned on the Pile dataset [23] and we used the OpenWebText
dataset [24] so that results could be replicated (Pile is no longer publicly available)."
REFERENCES,0.5545851528384279,"To verify that the training protocol is optimal on OpenWebText, we performed a small study to
illustrate how the performance would vary as we vary the size and total tokens trained on. Model size
was varied by changing the hidden size: the 70M model has a hidden size of 576, the 111M model
has a hidden size of 768 and the 161M model has a hidden size of 960. The token budget for each
model size was chosen to keep the total FLOPs constant."
REFERENCES,0.5567685589519651,"The learning rate was varied to observe a minima in the loss at each model scale. The results are
shown in Figure 10. While we found that the learning rate may be increased overall, the 111M
model was found to have the lowest loss of the three models. From these results we conclude that the
training protocol is optimal within this range of model sizes and we assume 111M is good enough. In
other words, a better model might exist between 70M and 161M parameters for this FLOP budget but
it isn’t outside of this range."
REFERENCES,0.5589519650655022,"C.2
Flash Attention Numerical Instability"
REFERENCES,0.5611353711790393,"The experiments described in Sections 4 and 5.2 involve Chinchilla optimal language models at a
111M scale [19]. These experiments were replicated according to the published information. We
encountered diverging runs when executing in bfloat16 Automatic Mixed Precision (AMP) consistent
with the default settings in nanoGPT. These experiments were executed on NVIDIA A10 GPUs for
accessible replication at small scale. By ablation it was found that these runs would diverge:"
REFERENCES,0.5633187772925764,• Regardless of batch size schedule
REFERENCES,0.5655021834061136,"10
3
6 × 10
4"
REFERENCES,0.5676855895196506,Learning Rate 3.225 3.250 3.275 3.300 3.325 3.350 3.375 3.400
REFERENCES,0.5698689956331878,Validation Loss
REFERENCES,0.5720524017467249,"min: 3.230
lr: 1.17e-03"
REFERENCES,0.574235807860262,"min: 3.246
lr: 1.17e-03
min: 3.238
lr: 1.46e-03"
REFERENCES,0.5764192139737991,Model Size
M PARAMS,0.5786026200873362,"111M params
70M params
161M params"
M PARAMS,0.5807860262008734,"Figure 10: The loss of models trained on OpenWebText with 70M, 111M and 161M parameters. The
learning rate was varied to find the minima in the loss at each model scale. The optimal learning rate
for each model size is annotated."
M PARAMS,0.5829694323144105,"• Regardless of hyperparameters: learning rate, weight decay, LayerNorm epsilon or Adam
epsilon [53]
• When using PyTorch’s AMP in bfloat16 precision
• When using Flash attention [16, 25]"
M PARAMS,0.5851528384279476,"This was surprising because prior work had trained these models successfully [19]. In that work the
model was also trained using bfloat16 AMP precision, but it was trained on a Cerebras CS-2 system.
Due to this difference, we suspected the issue was due to a difference between the efficient attention
kernel and the Flash attention kernel in PyTorch."
M PARAMS,0.5873362445414847,"By inspecting the histograms of weights and biases in the Query, Key, Value (QKV) projection
during training, we found that range grew the fastest in block 1 (the second block in the model). In
addition, we observed that the histogram of the query and key projection weights became bimodal
as the gradient norm diverged. This is illustrated in Figure 11. Further analysis of a checkpoint
taken at this point in training focused on the difference between gradients computed using the flash
attention kernel and the nanoGPT pure PyTorch attention implementation using float32 precision. At
initialization the gradients were not significantly different but at the point of divergence there was a
significant difference coinciding with increased parameter norms in that layer."
M PARAMS,0.5895196506550219,"To replicate the issue from scratch, we came up with a simulation from a generic initialization. Inspired
by the teacher-student experiment protocol proposed by Ba et al. [3] (although otherwise unrelated)
we set up a learning task with a “teacher” and “student” model with the same architecture. Both
networks begin with the same weights but we add a small amount of noise to the teacher’s weights.
The student is trained to match the teacher’s output. After experimenting with hyperparameters we
were able to replicated the divergence seen during training9, as illustrated in Figure 12."
M PARAMS,0.5917030567685589,"Using this isolated simulation we were able to test different methods to mitigate the divergence.
Karras et al. [32] suggested that cosine attention could address similar divergences attributed to
self-attention. In Figure 13 we replicated the experiment described in Figure 12 using cosine attention
and found that the divergence no longer occurred."
M PARAMS,0.5938864628820961,"Separately, experimenting with precision ablation found that if float32 precision was used only in
block 1 (2nd) then the divergence would also not occur. Based on this and the above, we found the
following two architectural mitigations for the divergence, in only block 1 (2nd):"
M PARAMS,0.5960698689956332,"• Use cosine attention, i.e. normalize the query and key head vectors before self-attention. OR"
M PARAMS,0.5982532751091703,"9The code for this experiment is available at https://gist.github.com/gaviag-cerebras/
b77aef9de29e859a5e999a582d57f6a2"
M PARAMS,0.6004366812227074,"0.3
0.2
0.1
0.0
0.1
0.2
0.3
Weight Value 0 5000 10000 15000 20000 25000 30000"
M PARAMS,0.6026200873362445,Frequency
M PARAMS,0.6048034934497817,"Mean: -0.0001
Std: 0.1207"
M PARAMS,0.6069868995633187,Query Weights
M PARAMS,0.6091703056768559,"0.4
0.2
0.0
0.2
0.4
Bias Value 0 10 20 30 40 50"
M PARAMS,0.611353711790393,Frequency
M PARAMS,0.6135371179039302,"Mean: -0.0062
Std: 0.2116"
M PARAMS,0.6157205240174672,Query Biases
M PARAMS,0.6179039301310044,"0.4
0.2
0.0
0.2
0.4
Weight Value 0 10000 20000 30000 40000"
M PARAMS,0.6200873362445415,Frequency
M PARAMS,0.6222707423580786,"Mean: -0.0001
Std: 0.1019"
M PARAMS,0.6244541484716157,Key Weights
M PARAMS,0.6266375545851528,"1.0
0.5
0.0
0.5
1.0
Bias Value 0 10 20 30 40 50"
M PARAMS,0.62882096069869,Frequency
M PARAMS,0.631004366812227,"Mean: -0.0178
Std: 0.6204"
M PARAMS,0.6331877729257642,Key Biases
M PARAMS,0.6353711790393013,"0.2
0.1
0.0
0.1
0.2
Weight Value 0 20000 40000 60000 80000"
M PARAMS,0.6375545851528385,100000
M PARAMS,0.6397379912663755,Frequency
M PARAMS,0.6419213973799127,"Mean: -0.0000
Std: 0.0234"
M PARAMS,0.6441048034934498,Value Weights
M PARAMS,0.6462882096069869,0.010 0.0050.000 0.005 0.010 0.015 0.020 0.025
M PARAMS,0.648471615720524,Bias Value 0 20 40 60 80
M PARAMS,0.6506550218340611,Frequency
M PARAMS,0.6528384279475983,"Mean: -0.0001
Std: 0.0033"
M PARAMS,0.6550218340611353,Value Biases
M PARAMS,0.6572052401746725,"0.15
0.10
0.05
0.00
0.05
0.10
0.15
Weight Value 0 20000 40000 60000 80000"
M PARAMS,0.6593886462882096,100000
M PARAMS,0.6615720524017468,120000
M PARAMS,0.6637554585152838,Frequency
M PARAMS,0.665938864628821,"Mean: -0.0000
Std: 0.0181"
M PARAMS,0.6681222707423581,Output Weights
M PARAMS,0.6703056768558951,"0.04
0.02
0.00
0.02
0.04
Bias Value 0 10 20 30 40 50"
M PARAMS,0.6724890829694323,Frequency
M PARAMS,0.6746724890829694,"Mean: 0.0000
Std: 0.0126"
M PARAMS,0.6768558951965066,Output Biases
M PARAMS,0.6790393013100436,"Figure 11: Histograms of weights and biases for the 111M experiment described in Sections 4 and 5.2
from the attention block containing the QKV projection, self-attention and output projection layers.
The histograms for the query and key projection weights and biases are bimodal while the value
projection weights and biases are not."
M PARAMS,0.6812227074235808,"0
500
1000
1500
2000
Iterations 22 24 26 28 30"
M PARAMS,0.6834061135371179,Bias Norm
M PARAMS,0.6855895196506551,Bias Norms
M PARAMS,0.6877729257641921,"Teacher
Flash
Non-Flash"
M PARAMS,0.6899563318777293,"0
500
1000
1500
2000
Iterations 5 10 15 20"
M PARAMS,0.6921397379912664,Distance
M PARAMS,0.6943231441048034,Distances to Teacher
M PARAMS,0.6965065502183406,"Flash to Teacher
Non-Flash to Teacher"
M PARAMS,0.6986899563318777,"0
500
1000
1500
2000
Iterations 0 5 10 15 20"
M PARAMS,0.7008733624454149,Distance
M PARAMS,0.7030567685589519,Flash to Non-Flash Distance
M PARAMS,0.7052401746724891,"0
500
1000
1500
2000
Iterations 0 5 10 15"
M PARAMS,0.7074235807860262,Distance Difference
M PARAMS,0.7096069868995634,Teacher Distance Difference
M PARAMS,0.7117903930131004,"Figure 12: Two “student” networks, identical to the “teacher” network except for the addition of a
small amount of noise to the teacher’s QKV projection bias. As training progresses, the student using
Flash attention diverges for the same inputs. Plots are, clockwise from top left: “Bias Norms” shows
the norms of the bias layer in each of the networks, “Distances to Teacher” shows the L2 distance
from each student to the teacher. “Flash to Non-Flash Distance” shows the L2 distance between the
student using Flash attention and not, “Teacher Distance Difference” is the difference between the
distances to the teacher for both cases."
M PARAMS,0.7139737991266376,"0
100
200
300
400
500
Iterations 19.8 20.0 20.2 20.4 20.6 20.8"
M PARAMS,0.7161572052401747,Bias Norm
M PARAMS,0.7183406113537117,Bias Norms
M PARAMS,0.7205240174672489,"Teacher
Flash
Non-Flash"
M PARAMS,0.722707423580786,"0
100
200
300
400
500
Iterations 4.6 4.7 4.8 4.9 5.0 5.1 5.2"
M PARAMS,0.7248908296943232,Distance
M PARAMS,0.7270742358078602,Distances to Teacher
M PARAMS,0.7292576419213974,"Flash to Teacher
Non-Flash to Teacher"
M PARAMS,0.7314410480349345,"0
100
200
300
400
500
Iterations 0.005 0.010 0.015 0.020"
M PARAMS,0.7336244541484717,Distance
M PARAMS,0.7358078602620087,Flash to Non-Flash Distance
M PARAMS,0.7379912663755459,"0
100
200
300
400
500
Iterations"
M PARAMS,0.740174672489083,0.0006
M PARAMS,0.74235807860262,0.0004
M PARAMS,0.7445414847161572,0.0002
M PARAMS,0.7467248908296943,0.0000
M PARAMS,0.7489082969432315,Distance Difference
M PARAMS,0.7510917030567685,Teacher Distance Difference
M PARAMS,0.7532751091703057,"Figure 13: Replication of the experiment described in Figure 12 using cosine attention instead of
Flash attention. The divergence observed no longer occurs."
M PARAMS,0.7554585152838428,• Use spectral normalization [40] on the QKV projection.
M PARAMS,0.75764192139738,"Critically, only modifying a single layer does not affect the throughput of the model, the observed
Model FLOPs Utilization (MFU) did not decrease by more than 1% in either case. Both of these
bound the norm of the query and key head vectors prior to attention. Spectral normalization achieves
this because the QKV projection is preceded by a LayerNorm layer. Using this mitigation on the
111M model allowed the experiment to be replicated on an NVIDIA A10 GPU and we observed the
same behaviour as running the model more slowly in float32 precision."
M PARAMS,0.759825327510917,"Similar divergences are discussed in prior literature (and in nanoGPT’s issue tracker) but we are
unable to verify that it is the same problem. Wortsman et al. [53] discuss how to build similar
experiments to those described above but do not investigate flash attention specifically. Golden
et al. [25] investigate the numerical stability of Flash attention but neglect to demonstrate a failure
mode that affects real training runs. Zhai et al. [58] focus on the numerical stability of attention in
general and propose a similar mitigation (their method, σReparam, is a scaled version of spectral
normalization) but do not investigate flash attention specifically."
M PARAMS,0.7620087336244541,"It is likely that the mitigation proposed will not work in all cases, such as for larger models. However,
we only needed to replicate at the scale we were working at. The experiments in Figure 12 and
Figure 13 are included to illustrate how bounding the norm of the query and key head vectors seems
to be important for numerical stability. However, this may change in future versions of the flash
attention kernel, these results were obtained with PyTorch 2.4.0."
M PARAMS,0.7641921397379913,"D
Additional GNS Results"
M PARAMS,0.7663755458515283,"D.1
Additional GNS Phase Plot"
M PARAMS,0.7685589519650655,"Figure 14 shows the GNS phase plot for the same model as described in Section 4 but with the linear
batch size schedule described in Section 5.2."
M PARAMS,0.7707423580786026,"D.2
Batch Size Schedule"
M PARAMS,0.7729257641921398,The batch size schedule used in the experiment described in Section 5.2 is shown in Figure 15.
M PARAMS,0.7751091703056768,"D.3
Larger Scale Training"
M PARAMS,0.777292576419214,"To demonstrate that the method scales to larger models, we trained a 1.3B parameter GPT model10 on
OpenWebText using 8 H100 GPUs. The results of this experiment are shown in Figure 16. The left
plot shows the per-example gradient norms for all layers, while the right plot shows the per-example
gradient norms for only the LayerNorm layers. The GNS computed using the traditional DDP method
is also shown for comparison. In Figure 16a we observe that the LayerNorm remains predictive of the
total GNS, as in the 111M model results of Figure 7. When all non-fused simultaneous per-example
gradient norms were collected we observed an MFU of 40% and when only the fused LayerNorm
layers were collected we observed an MFU of 57%."
M PARAMS,0.7794759825327511,"After completing this experiment a bug was discovered in the code that decreased per-example
gradient norms by a constant factor. This caused an underestimation of the GNS. In Figure 16b
this can be seen when we compare the GNS estimated via DDP method. Initially, we assumed
that this constant factor was due a failure of the LayerNorm GNS approximation to larger models.
Unfortunately, we did not have the budget in time or resources to rerun the experiment so we corrected
the results by multiplying by the constant factor observed in the comparison to the DDP method."
M PARAMS,0.7816593886462883,"This may be representative of real world scenarios where a large model is pretrained over many DDP
nodes. As the user has access to two methods to estimate the GNS, they may account for any bias or
slope between the estimates. Then, if it is necessary to continue training on a single node, they can
use the per-example gradient norms to estimate the GNS. Similar techniques can involve enabling
per-example GNS estimation for all layers for a short time, or estimating the GNS offline as described
in Appendix A."
M PARAMS,0.7838427947598253,10Again following the GPT2-like[43] prescription from Dey et al. [19].
M PARAMS,0.7860262008733624,"10
3
10
2
10
1
100
101
102"
M PARAMS,0.7882096069868996,"||G||2
2 100 101 102 Tr( )"
M PARAMS,0.7903930131004366,GNS=100
M PARAMS,0.7925764192139738,GNS=1000
M PARAMS,0.7947598253275109,GNS=10000
M PARAMS,0.7969432314410481,MLP and Attention
M PARAMS,0.7991266375545851,"0
1
2
3
4
5
6
7
8
9
Combined"
M PARAMS,0.8013100436681223,"0
5000 1000015000200002500030000"
M PARAMS,0.8034934497816594,Iteration 0 200 400 600 800 GNS
M PARAMS,0.8056768558951966,"10
5
10
4
10
3
10
2"
M PARAMS,0.8078602620087336,"||G||2
2 10
4 10
3 10
2 10
1 Tr( )"
M PARAMS,0.8100436681222707,GNS=100
M PARAMS,0.8122270742358079,GNS=1000
M PARAMS,0.8144104803493449,GNS=10000
M PARAMS,0.8165938864628821,LayerNorm
M PARAMS,0.8187772925764192,"0
1
2
3
4
5
6
7
8
9
ln_f
Combined"
M PARAMS,0.8209606986899564,"0
5000 1000015000200002500030000"
M PARAMS,0.8231441048034934,Iteration 0 50 100 150 200 250 300 350 GNS
M PARAMS,0.8253275109170306,"Figure 14: GNS phase plot as in Figure 5 but focusing on the batch size schedule described in Sec-
tion 5.2. Linear/Embedding layers are separated from LayerNorm layers by row and the component
estimators of Equations 4 and 5 are plotted (left), with the GNS over the course of training (right)."
M PARAMS,0.8275109170305677,"E
FLOP & I/O Formulae"
M PARAMS,0.8296943231441049,"We use the following formulae in our FLOP and I/O cost estimations, where B = Batch Size,
T = Sequence Length, K = Input Dimension, L = Output Dimension:"
M PARAMS,0.8318777292576419,"Table 1: FLOPs
Algorithm
Weight Gradient
Gradient Norms
Simultaneous
BKL (2T −1) + KL (B −1)
BKL + B (KL −1)
[36]
KL (2BT −1)
BT 2 · (2K + 2L −2) + BT 2"
M PARAMS,0.834061135371179,"Table 2: I/O
Algorithm
Weight Gradient
Gradient Norms
Simultaneous
BKL + BKT + BLT
BKL + B
[36]
BKT + BLT + KL
2BT 2 + B"
M PARAMS,0.8362445414847162,"0.0
0.5
1.0
1.5
2.0
Tokens Processed
1e9 0 20 40 60 80 100 120 140 160"
M PARAMS,0.8384279475982532,Batch Size
M PARAMS,0.8406113537117904,GNS and Batch Size over Tokens Processed (alpha=0.999)
M PARAMS,0.8427947598253275,"GNS
Batch Size"
M PARAMS,0.8449781659388647,"Figure 15: The batch size schedule used and GNS observed in the 111M batch size schedule
experiment illustrated in Figure 15. An aliasing issue is noticeable in the interpolated linear batch
size schedule that was used. This has since been fixed in the published code."
M PARAMS,0.8471615720524017,"0.96
0.98
1.00
alpha 0.5 1.0 1.5 2.0 slope"
M PARAMS,0.8493449781659389,"Attention
LayerNorm
MLP
Embedding"
M PARAMS,0.851528384279476,"0.96
0.98
1.00
alpha 0.4 0.5 0.6 0.7 0.8 0.9 1.0 r"
M PARAMS,0.8537117903930131,(a) Regression analysis repeating Figure 6.
M PARAMS,0.8558951965065502,"101
3 × 100
4 × 100
6 × 100"
M PARAMS,0.8580786026200873,Train Loss 0 250 500 750 1000 GNS
M PARAMS,0.8602620087336245,GNS vs Train Loss
M PARAMS,0.8624454148471615,"DDP (8 nodes)
Per-example (calibrated 2.2x)"
M PARAMS,0.8646288209606987,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Tokens Processed
1e10 0 250 500 750 1000 GNS"
M PARAMS,0.8668122270742358,GNS (alpha=0.99)
M PARAMS,0.868995633187773,"DDP (8 nodes)
Per-example (calibrated 2.2x)"
M PARAMS,0.87117903930131,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Tokens Processed
1e10 1 2 3"
M PARAMS,0.8733624454148472,GNS Ratio (DDP / Per-example)
M PARAMS,0.8755458515283843,Ratio of GNS Traces (alpha=0.99)
M PARAMS,0.8777292576419214,DDP / Per-example
M PARAMS,0.8799126637554585,"(b) Comparison of GNS computed using traditional
DDP methods and per-example gradient norms."
M PARAMS,0.8820960698689956,"Figure 16: 1.3B GPT model train on OpenWebText using 8 H100s, trained twice. (Left) Per-example
gradient norms for all layers were gathered to replicate the analysis in Figure 7. (Right) Per-example
gradient norms were gathered for only LayerNorm layers, then compared to the GNS computed using
traditional DDP methods."
M PARAMS,0.8842794759825328,"Solving the I/O equations above reproduces [36]’s analysis with T =
√ 2
√"
M PARAMS,0.8864628820960698,"KL
2
at the cross-over point
above which simultaneous calculation is more I/O efficient. Solving for FLOPs gives: T = r"
M PARAMS,0.888646288209607,"2KL −1
2K + 2L −1."
M PARAMS,0.8908296943231441,NeurIPS Paper Checklist
CLAIMS,0.8930131004366813,1. Claims
CLAIMS,0.8951965065502183,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The claimed contributions are a method for computing per-example gradient
norms in the forward pass of a neural network and a GNS discovery of correlation between
layers. The method is efficient, see Section 5.1, and the GNS is shown to be predictable, see
Section 4.2.
Guidelines:"
CLAIMS,0.8973799126637555,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8995633187772926,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Limitations are discussed in Section 6.
Guidelines:"
CLAIMS,0.9017467248908297,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9039301310043668,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
CLAIMS,0.9061135371179039,Answer: [Yes]
CLAIMS,0.9082969432314411,"Justification: The main theoretical results concern the FLOP costs of computing per-example
gradient norms, which can be found in Section 3.1. All other theoretical results are summa-
rized from prior work."
CLAIMS,0.9104803493449781,Guidelines:
CLAIMS,0.9126637554585153,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9148471615720524,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9170305676855895,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9192139737991266,"Answer: [Yes]
Justification: The algorithms described in Sections 3 are sufficient to reproduce the
main experimental results. To obtain the performance reported in Section 5.1 the cus-
tom LayerNorm kernel is provided with the shared code.
The code is available at
https://github.com/CerebrasResearch/nanoGNS/tree/main/exact."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9213973799126638,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9235807860262009,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.925764192139738,"some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9279475982532751,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code to implement the cuda kernel described in Section 5.1 is provided
with the submission. The code to implement the per-example gradient norm computations
described in Section 3 and Appendix B is provided in pseudocode in the text and results may
be replicated by using the shared code at https://github.com/CerebrasResearch/
nanoGNS/tree/main/exact.
Guidelines:"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9301310043668122,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9323144104803494,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The experiments conducted in Section 4 use the same settings as Dey et al.
[19], whose code and hyperparameters are available. The only changes from these details,
which were made for numerical stability on GPU, are described in Appendix C.2.
Guidelines:"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9344978165938864,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9366812227074236,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The experiments in Section 4.2 and 5.2 are the only experiments requiring
statistical testing. Section 4.2 reports the Pearson correlation coefficient and Section 5.2
reports the results over three seeds."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9388646288209607,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9410480349344978,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9432314410480349,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9454148471615721,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9475982532751092,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9497816593886463,"Justification: The runtime of each experiment performed in Section 4 is provided in the text.
The code to implement the custom LayerNorm kernel described in Section 5.1 is available
at https://github.com/CerebrasResearch/nanoGNS/tree/main/exact."
EXPERIMENTS COMPUTE RESOURCES,0.9519650655021834,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9541484716157205,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9563318777292577,9. Code Of Ethics
CODE OF ETHICS,0.9585152838427947,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9606986899563319,Answer: [Yes]
CODE OF ETHICS,0.962882096069869,"Justification: This research has no human participants, uses public open datasets and has
generic applications in training deep neural networks."
CODE OF ETHICS,0.9650655021834061,Guidelines:
CODE OF ETHICS,0.9672489082969432,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9694323144104804,10. Broader Impacts
BROADER IMPACTS,0.9716157205240175,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The consequences of this work are difficult to anticipate, we frame it here as
useful for practitioners training deep models but it may also influence differential privacy as
described in Section 2.3.
Guidelines:"
BROADER IMPACTS,0.9737991266375546,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
BROADER IMPACTS,0.9759825327510917,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper does not release any data or models.
Guidelines:"
BROADER IMPACTS,0.9781659388646288,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided relgeneric easing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
BROADER IMPACTS,0.980349344978166,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Section 4 links to the nanoGPT repo, which our experiments are based on.
The code includes sections implementing GNS for DDP that may be found in the code of
Crowson et al. [14] which are properly attributed."
BROADER IMPACTS,0.982532751091703,Guidelines:
BROADER IMPACTS,0.9847161572052402,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
BROADER IMPACTS,0.9868995633187773,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification:
The code shared with the submission at https://github.com/
CerebrasResearch/nanoGNS/tree/main/exact includes instructions for replicating
experiments and notebooks for replicating figures.
Guidelines:"
BROADER IMPACTS,0.9890829694323144,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
BROADER IMPACTS,0.9912663755458515,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
BROADER IMPACTS,0.9934497816593887,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
BROADER IMPACTS,0.9956331877729258,"Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
BROADER IMPACTS,0.9978165938864629,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
