Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011961722488038277,"In this work, we propose a novel adversarial defence mechanism for image classi-
1"
ABSTRACT,0.0023923444976076554,"fication – CARSO – blending the paradigms of adversarial training and adversarial
2"
ABSTRACT,0.0035885167464114833,"purification in a synergistic robustness-enhancing way. The method builds upon
3"
ABSTRACT,0.004784688995215311,"an adversarially-trained classifier, and learns to map its internal representation
4"
ABSTRACT,0.005980861244019139,"associated with a potentially perturbed input onto a distribution of tentative clean
5"
ABSTRACT,0.007177033492822967,"reconstructions. Multiple samples from such distribution are classified by the same
6"
ABSTRACT,0.008373205741626795,"adversarially-trained model, and an aggregation of its outputs finally constitutes the
7"
ABSTRACT,0.009569377990430622,"robust prediction of interest. Experimental evaluation by a well-established bench-
8"
ABSTRACT,0.01076555023923445,"mark of strong adaptive attacks, across different image datasets, shows that CARSO
9"
ABSTRACT,0.011961722488038277,"is able to defend itself against adaptive end-to-end white-box attacks devised for
10"
ABSTRACT,0.013157894736842105,"stochastic defences. Paying a modest clean accuracy toll, our method improves
11"
ABSTRACT,0.014354066985645933,"by a significant margin the state-of-the-art for CIFAR-10, CIFAR-100, and
12"
ABSTRACT,0.01555023923444976,"TINYIMAGENET-200 ℓ∞robust classification accuracy against AUTOATTACK.
13"
INTRODUCTION,0.01674641148325359,"1
Introduction
14"
INTRODUCTION,0.017942583732057416,"Vulnerability to adversarial attacks [8, 57] – i.e. the presence of inputs, usually crafted on purpose,
15"
INTRODUCTION,0.019138755980861243,"capable of catastrophically altering the behaviour of high-dimensional models [9] – constitutes a major
16"
INTRODUCTION,0.02033492822966507,"hurdle towards ensuring the compliance of deep learning systems with the behaviour expected by
17"
INTRODUCTION,0.0215311004784689,"modellers and users, and their adoption in safety-critical scenarios or tightly-regulated environments.
18"
INTRODUCTION,0.022727272727272728,"This is particularly true for adversarially-perturbed inputs, where a norm-constrained perturbation –
19"
INTRODUCTION,0.023923444976076555,"often hardly detectable by human inspection [48, 5] – is added to an otherwise legitimate input, with
20"
INTRODUCTION,0.025119617224880382,"the intention of eliciting an anomalous response [34].
21"
INTRODUCTION,0.02631578947368421,"Given the widespread nature of the issue [30], and the serious concerns raised about the safety and
22"
INTRODUCTION,0.02751196172248804,"reliability of data-learnt models in the lack of an appropriate mitigation [7], adversarial attacks have
23"
INTRODUCTION,0.028708133971291867,"been extensively studied. Yet, obtaining generally robust machine learning (ML) systems remains a
24"
INTRODUCTION,0.029904306220095694,"longstanding issue, and a major open challenge.
25"
INTRODUCTION,0.03110047846889952,"Research in the field has been driven by two opposing, yet complementary, efforts. On the one
26"
INTRODUCTION,0.03229665071770335,"hand, the study of failure modes in existing models and defences, with the goal of understanding
27"
INTRODUCTION,0.03349282296650718,"their origin and developing stronger attacks with varying degrees of knowledge and control over the
28"
INTRODUCTION,0.034688995215311005,"target system [57, 21, 44, 60]. On the other hand, the construction of increasingly capable defence
29"
INTRODUCTION,0.03588516746411483,"mechanisms. Although alternatives have been explored [15, 59, 11, 68], most of the latter is based on
30"
INTRODUCTION,0.03708133971291866,"adequately leveraging adversarial training [21, 42, 58, 49, 23, 31, 54, 62, 18, 47], i.e. training a ML
31"
INTRODUCTION,0.03827751196172249,"model on a dataset composed of (or enriched with) adversarially-perturbed inputs associated with
32"
INTRODUCTION,0.039473684210526314,"their correct, pre-perturbation labels. In fact, adversarial training has been the only technique capable
33"
INTRODUCTION,0.04066985645933014,"of consistently providing an acceptable level of defence [24], while still incrementally improving up
34"
INTRODUCTION,0.041866028708133975,"to the current state-of-the-art [18, 47].
35"
INTRODUCTION,0.0430622009569378,"Another defensive approach is that of adversarial purification [53, 66], where a generative model is
36"
INTRODUCTION,0.04425837320574163,"used – similarly to denoising – to recover a perturbation-free version of the input before classification
37"
INTRODUCTION,0.045454545454545456,"is performed. Nonetheless, such attempts have generally fallen short of expectations due to inherent
38"
INTRODUCTION,0.04665071770334928,"limitations of the generative models used in early attempts [45], or due to decreases in robust
39"
INTRODUCTION,0.04784688995215311,"accuracy1 when attacked end-to-end [25] – resulting in subpar robustness if the defensive structure is
40"
INTRODUCTION,0.04904306220095694,"known to the adversary [60]. More recently, the rise of diffusion-based generative models [28] and
41"
INTRODUCTION,0.050239234449760764,"their use for purification have enabled more successful results of this kind [45, 13] – although at the
42"
INTRODUCTION,0.05143540669856459,"cost of much longer training and inference times, and a much brittler robustness evaluation [13, 38].
43"
INTRODUCTION,0.05263157894736842,"In this work, we design a novel adversarial defence for supervised image classification, dubbed
44"
INTRODUCTION,0.05382775119617225,"CARSO (i.e., Counter-Adversarial Recall of Synthetic Observations). The approach relies on an
45"
INTRODUCTION,0.05502392344497608,"adversarially-trained classifier (called hereinafter simply the classifier), endowed with a stochastic
46"
INTRODUCTION,0.056220095693779906,"generative model (called hereinafter the purifier). Upon classification of a potentially-perturbed input,
47"
INTRODUCTION,0.05741626794258373,"the latter learns to generate – from the tensor2 of (pre)activations registered at neuron level in the
48"
INTRODUCTION,0.05861244019138756,"former – samples from a distribution of plausible, perturbation-free reconstructions. At inference
49"
INTRODUCTION,0.05980861244019139,"time, some of these samples are classified by the very same classifier, and the original input is robustly
50"
INTRODUCTION,0.061004784688995214,"labelled by aggregating its many outputs. This method – to the best of our knowledge the first attempt
51"
INTRODUCTION,0.06220095693779904,"to organically merge the adversarial training and purification paradigms – avoids the vulnerability
52"
INTRODUCTION,0.06339712918660287,"pitfalls typical of the mere stacking of a purifier and a classifier [25], while still being able to take
53"
INTRODUCTION,0.0645933014354067,"advantage of independent incremental improvements to adversarial training or generative modelling.
54"
INTRODUCTION,0.06578947368421052,"An empirical assessment3 of the defence in the ℓ∞white-box setting is provided, using a conditional
55"
INTRODUCTION,0.06698564593301436,"[56, 64] variational autoencoder [32, 50] as the purifier and existing state-of-the-art adversarially
56"
INTRODUCTION,0.06818181818181818,"pre-trained models as classifiers. Such choices are meant to give existing approaches – and the
57"
INTRODUCTION,0.06937799043062201,"adversary attacking our architecture end-to-end as part of the assessment – the strongest advantage
58"
INTRODUCTION,0.07057416267942583,"possible. Yet, in all scenarios considered, CARSO improves significantly the robustness of the
59"
INTRODUCTION,0.07177033492822966,"pre-trained classifier – even against attacks specifically devised to fool stochastic defences like ours.
60"
INTRODUCTION,0.0729665071770335,"Remarkably, with a modest clean accuracy toll, our method improves by a significant margin the
61"
INTRODUCTION,0.07416267942583732,"current state-of-the-art for CIFAR-10 [33], CIFAR-100 [33], and TINYIMAGENET-200 [14] ℓ∞
62"
INTRODUCTION,0.07535885167464115,"robust classification accuracy against AUTOATTACK [17].
63"
INTRODUCTION,0.07655502392344497,"In summary, the paper makes the following contributions:
64"
INTRODUCTION,0.07775119617224881,"• The description of CARSO, a novel adversarial defence method synergistically blending
65"
INTRODUCTION,0.07894736842105263,"adversarial training and adversarial purification;
66"
INTRODUCTION,0.08014354066985646,"• A collection of relevant technical details fundamental to its successful training and use,
67"
INTRODUCTION,0.08133971291866028,"originally developed for the purifier being a conditional variational autoencoder – but
68"
INTRODUCTION,0.08253588516746412,"applicable to more general scenarios as well;
69"
INTRODUCTION,0.08373205741626795,"• Experimental assessment of the method, against standardised benchmark adversarial attacks
70"
INTRODUCTION,0.08492822966507177,"– showing higher robust accuracy w.r.t. to existing state-of-the-art adversarial training and
71"
INTRODUCTION,0.0861244019138756,"purification approaches.
72"
INTRODUCTION,0.08732057416267942,"The rest of the manuscript is structured as follows. In section 2 we provide an overview of selected
73"
INTRODUCTION,0.08851674641148326,"contributions in the fields of adversarial training and purification-based defences – with focus on
74"
INTRODUCTION,0.08971291866028708,"image classification. In section 3, a deeper analysis is given of two integral parts of our experimental
75"
INTRODUCTION,0.09090909090909091,"assessment: PGD adversarial training and conditional variational autoencoders. Section 4 is devoted
76"
INTRODUCTION,0.09210526315789473,"to the intuition behind CARSO, its architectural description, and the relevant technical details that
77"
INTRODUCTION,0.09330143540669857,"allow it to work. Section 5 contains details about the experimental setup, results, comments, and
78"
INTRODUCTION,0.09449760765550239,"limitations. Section 6 concludes the paper and outlines directions of future development.
79"
RELATED WORK,0.09569377990430622,"2
Related work
80"
RELATED WORK,0.09688995215311005,"Adversarial training as a defence
The idea of training a model on adversarially-generated examples
81"
RELATED WORK,0.09808612440191387,"as a way to make it more robust can be traced back to the very beginning of research in the area.
82"
RELATED WORK,0.09928229665071771,"1 The test set accuracy of the frozen-weights trained classifier – computed on a dataset entirely composed of
adversarially-perturbed examples generated against that specific model.
2 Which we call internal representation.
3 Implementation of the method and code for the experiments (based on PyTorch [46], AdverTorch [19], and
ebtorch [4]) can be found in the supplementary materials of the paper."
RELATED WORK,0.10047846889952153,"The seminal [57] proposes to perform training on a mixed collection of clean and adversarial data,
83"
RELATED WORK,0.10167464114832536,"generated beforehand.
84"
RELATED WORK,0.10287081339712918,"The introduction of FGSM [21] enables the efficient generation of adversarial examples along the
85"
RELATED WORK,0.10406698564593302,"training, with a single normalised gradient step. Its iterative counterpart PGD [42] – discussed
86"
RELATED WORK,0.10526315789473684,"in section 3 and Appendix A – significantly improves the effectiveness of adversarial examples
87"
RELATED WORK,0.10645933014354067,"produced, making it still the de facto standard for the synthesis of adversarial training inputs [24].
88"
RELATED WORK,0.1076555023923445,"Further incremental improvements have also been developed, some focused specifically on robustness
89"
RELATED WORK,0.10885167464114832,"assessment (e.g. adaptive-stepsize variants, as in [17]).
90"
RELATED WORK,0.11004784688995216,"The most recent adversarial training protocols further rely on synthetic data to increase the numerosity
91"
RELATED WORK,0.11124401913875598,"of training datapoints [23, 49, 62, 18, 47], and adopt adjusted loss functions to balance robustness and
92"
RELATED WORK,0.11244019138755981,"accuracy [67] or generally foster the learning process [18]. The entire model architecture may also be
93"
RELATED WORK,0.11363636363636363,"tuned specifically for the sake of robustness enhancement [47]. At least some of such ingredients are
94"
RELATED WORK,0.11483253588516747,"often required to reach the current state-of-the-art in robust accuracy via adversarial training.
95"
RELATED WORK,0.11602870813397129,"Purification as a defence
Amongst the first attempts of purification-based adversarial defence,
96"
RELATED WORK,0.11722488038277512,"[25] investigates the use of denoising autoencoders [61] to recover examples free from adversarial
97"
RELATED WORK,0.11842105263157894,"perturbations. Despite its effectiveness in the denoising task, the method may indeed increase
98"
RELATED WORK,0.11961722488038277,"the vulnerability of the system when attacks are generated against it end-to-end. The contextually
99"
RELATED WORK,0.12081339712918661,"proposed improvement adds a smoothness penalty to the reconstruction loss, partially mitigating such
100"
RELATED WORK,0.12200956937799043,"downside [25]. Similar in spirit, [39] tackles the issue by computing the reconstruction loss between
101"
RELATED WORK,0.12320574162679426,"the last-layers representations of the frozen-weights attacked classifier, respectively receiving, as
102"
RELATED WORK,0.12440191387559808,"input, the clean and the tentatively denoised example.
103"
RELATED WORK,0.1255980861244019,"In [52], Generative Adversarial Networks (GANs) [22] learnt on clean data are used at inference time
104"
RELATED WORK,0.12679425837320574,"to find a plausible synthetic example – close to the perturbed input – belonging to the unperturbed
105"
RELATED WORK,0.12799043062200957,"data manifold. Despite encouraging results, the delicate training process of GANs and the existence
106"
RELATED WORK,0.1291866028708134,"of known failure modes [70] limit the applicability of the method. More recently, a similar approach
107"
RELATED WORK,0.1303827751196172,"[27] employing energy-based models [37] suffered from poor sample quality [45].
108"
RELATED WORK,0.13157894736842105,"Purification approaches based on (conditional) variational autoencoders include [29] and [53]. Very
109"
RELATED WORK,0.13277511961722488,"recently, a technique combining variational manifold learning with a test-time iterative purification
110"
RELATED WORK,0.1339712918660287,"procedure has also been proposed [65].
111"
RELATED WORK,0.13516746411483255,"Finally, already-mentioned techniques relying on score- [66] and diffusion- based [45, 13] models
112"
RELATED WORK,0.13636363636363635,"have also been developed, with generally favourable results – often balanced in practice by longer
113"
RELATED WORK,0.1375598086124402,"training and inference times, and a much more fragile robustness assessment [13, 38].
114"
PRELIMINARIES,0.13875598086124402,"3
Preliminaries
115"
PRELIMINARIES,0.13995215311004786,"PGD adversarial training
The task of finding model parameters robust to adversarial perturbations
116"
PRELIMINARIES,0.14114832535885166,"is framed by [42] as a min-max optimisation problem seeking to minimise adversarial risk. The
117"
PRELIMINARIES,0.1423444976076555,"inner optimisation (i.e., the generation of worst-case adversarial examples) is solved by an iterative
118"
PRELIMINARIES,0.14354066985645933,"algorithm – Projected Gradient Descent – interleaving gradient ascent steps in input space with
119"
PRELIMINARIES,0.14473684210526316,"the eventual projection on the shell of an ϵ-ball centred around an input datapoint, thus imposing a
120"
PRELIMINARIES,0.145933014354067,"perturbation strength constraint.
121"
PRELIMINARIES,0.1471291866028708,"In this manuscript, we will use the shorthand notation ϵp to denote ℓp norm-bound perturbations of
122"
PRELIMINARIES,0.14832535885167464,"maximum magnitude ϵ.
123"
PRELIMINARIES,0.14952153110047847,"The formal details of such method are provided in Appendix A.
124"
PRELIMINARIES,0.1507177033492823,"(Conditional) variational autoencoders
Variational autoencoders (VAEs) [32, 50] allow the learn-
125"
PRELIMINARIES,0.1519138755980861,"ing from data of approximate generative latent-variable models of the form p(x, z) = p(x | z)p(z),
126"
PRELIMINARIES,0.15311004784688995,"whose likelihood and posterior are approximately parameterised by deep artificial neural networks
127"
PRELIMINARIES,0.15430622009569378,"(ANNs). The problem is cast as the maximisation of a variational lower bound.
128"
PRELIMINARIES,0.15550239234449761,"In practice, optimisation is performed iteratively – on a loss function given by the linear mixture of
129"
PRELIMINARIES,0.15669856459330145,"data-reconstruction loss and empirical KL divergence w.r.t. a chosen prior, computed on mini-batches
130"
PRELIMINARIES,0.15789473684210525,"of data.
131"
PRELIMINARIES,0.1590909090909091,"Conditional Variational Autoencoders [56, 64] extend VAEs by attaching a conditioning tensor c –
132"
PRELIMINARIES,0.16028708133971292,"expressing specific characteristics of each example – to both x and z during training. This allows the
133"
PRELIMINARIES,0.16148325358851676,"learning of a decoder model capable of conditional data generation.
134"
PRELIMINARIES,0.16267942583732056,"Further details on the functioning of such models are given in Appendix B.
135"
STRUCTURE OF CARSO,0.1638755980861244,"4
Structure of CARSO
136"
STRUCTURE OF CARSO,0.16507177033492823,"The core ideas informing the design of our method are driven more by first principles rather than
137"
STRUCTURE OF CARSO,0.16626794258373206,"arising from specific contingent requirements. This section discusses such ideas, the architectural
138"
STRUCTURE OF CARSO,0.1674641148325359,"details of CARSO, and a group of technical aspects fundamental to its training and inference processes.
139"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.1686602870813397,"4.1
Architectural overview and principle of operation
140"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.16985645933014354,"From an architectural point of view, CARSO is essentially composed of two ANN models – a classifier
141"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.17105263157894737,"and a purifier – operating in close synergy. The former is trained on a given classification task,
142"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.1722488038277512,"whose inputs might be adversarially corrupted at inference time. The latter learns to generate samples
143"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.173444976076555,"from a distribution of potential input reconstructions, tentatively free from adversarial perturbations.
144"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.17464114832535885,"Crucially, the purifier has only access to the internal representation of the classifier – and not even
145"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.17583732057416268,"directly to the perturbed input – to perform its task.
146"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.17703349282296652,"During inference, for each input, the internal representation of the classifier is used by the purifier to
147"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.17822966507177032,"synthesise a collection of tentatively unperturbed input reconstructions. Those are classified by the
148"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.17942583732057416,"same classifier, and the resulting outputs are aggregated into a final robust prediction.
149"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.180622009569378,"There are no specific requirements for the classifier, whose training is completely independent of
150"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.18181818181818182,"the use of the model as part of CARSO. However, training it adversarially improves significantly
151"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.18301435406698566,"the clean accuracy of the overall system, allowing it to benefit from established adversarial training
152"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.18421052631578946,"techniques.
153"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.1854066985645933,"The purifier is also independent of specific architectural choices, provided it is capable of stochastic
154"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.18660287081339713,"conditional data generation at inference time, with the internal representation of the classifier used as
155"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.18779904306220097,"the conditioning set.
156"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.18899521531100477,"In the rest of the paper, we employ a state-of-the-art adversarially pre-trained WIDERESNET model
157"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.1901913875598086,"as the classifier, and a purpose-built conditional variational autoencoder as the purifier, the latter
158"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.19138755980861244,"operating decoder-only during inference. Such choice was driven by the deliberate intention to assess
159"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.19258373205741627,"the adversarial robustness of our method in its worst-case scenario against a white-box attacker, and
160"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.1937799043062201,"with the least advantage compared to existing approaches based solely on adversarial training.
161"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.19497607655502391,"In fact, the decoder of a conditional VAE allows for exact algorithmic differentiability [6] w.r.t. its
162"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.19617224880382775,"conditioning set, thus averting the need for backward-pass approximation [2] in generating end-to-end
163"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.19736842105263158,"adversarial attacks against the entire system, and preventing (un)intentional robustness by gradient
164"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.19856459330143542,"obfuscation [2]. The same cannot be said [13] for more capable and modern purification models,
165"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.19976076555023922,"such as those based e.g. on diffusive processes, whose robustness assessment is still in the process of
166"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.20095693779904306,"being understood [38].
167"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.2021531100478469,"A downside of such choice is represented by the reduced effectiveness of the decoder in the synthesis
168"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.20334928229665072,"of complex data, due to well-known model limitations. In fact, we experimentally observe a modest
169"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.20454545454545456,"increase in reconstruction cost for non-perturbed inputs, which in turn may limit the clean accuracy of
170"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.20574162679425836,"the entire system. Nevertheless, we defend the need for a fair and transparent robustness evaluation,
171"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.2069377990430622,"such as the one provided by the use of a VAE-based purifier, in the evaluation of any novel architecture-
172"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.20813397129186603,"agnostic adversarial defence technique.
173"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.20933014354066987,"A diagram of the whole architecture is shown in Figure 1, and its detailed principles of operation are
174"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.21052631578947367,"recapped below.
175"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.2117224880382775,"Training
At training time, adversarially-perturbed examples are generated against the classifier,
176"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.21291866028708134,"and fed to it. The tensors containing the classifier (pre)activations across the network are then
177"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.21411483253588517,"extracted. Finally, the conditional VAE serving as the purifier is trained on perturbation-free input
178"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.215311004784689,"reconstruction, conditional on the corresponding previously extracted internal representations, and
179"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.21650717703349281,"using pre-perturbation examples as targets.
180"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.21770334928229665,"Figure 1: Schematic representation of the CARSO architecture used in the experimental phase of this
work. The subnetwork bordered by the red dashed line is used only during the training of the purifier.
The subnetwork bordered by the blue dashed line is re-evaluated on different random samples zi and
the resulting individual ˆyi are aggregated into ˆyrob. The classifier f(·; θ) is always kept frozen; the
remaining network is trained on LVAE(x, ˆx). More precise details on the functioning of the networks
are provided in subsection 4.1."
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.21889952153110048,"Upon completion of the training process, the encoder network may be discarded as it will not be used
181"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.22009569377990432,"for inference.
182"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.22129186602870812,"Inference
The example requiring classification is fed to the classifier. Its corresponding internal
183"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.22248803827751196,"representation is extracted and used to condition the generative process described by the decoder
184"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.2236842105263158,"of the VAE. Stochastic latent variables are repeatedly sampled from the original priors, which are
185"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.22488038277511962,"given by an i.i.d. multivariate Standard Normal distribution. Each element in the resulting set of
186"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.22607655502392343,"reconstructed inputs is classified by the same classifier, and the individually predicted class logits are
187"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.22727272727272727,"aggregated. The result of such aggregation constitutes the robust prediction of the input class.
188"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.2284688995215311,"Remarkably, the only link between the initial potentially-perturbed input and the resulting purified
189"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.22966507177033493,"reconstructions (and thus the predicted class) is through the internal representation of the classifier,
190"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.23086124401913877,"which serves as a featurisation of the original input. The whole process is exactly differentiable end-
191"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.23205741626794257,"to-end, and the only potential hurdle to the generation of adversarial attacks against the entire system
192"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.2332535885167464,"is the stochastic nature of the decoding – which is easily tackled by Expectation over Transformation
193"
ARCHITECTURAL OVERVIEW AND PRINCIPLE OF OPERATION,0.23444976076555024,"[3].
194"
A FIRST-PRINCIPLES JUSTIFICATION,0.23564593301435408,"4.2
A first-principles justification
195"
A FIRST-PRINCIPLES JUSTIFICATION,0.23684210526315788,"If we consider a trained ANN classifier, subject to a successful adversarial attack by means of a
196"
A FIRST-PRINCIPLES JUSTIFICATION,0.23803827751196172,"slightly perturbed example, we observe that – both in terms of ℓp magnitude and human perception
197"
A FIRST-PRINCIPLES JUSTIFICATION,0.23923444976076555,"– a small variation on the input side of the network is amplified to a significant amount on the
198"
A FIRST-PRINCIPLES JUSTIFICATION,0.24043062200956938,"output side, thanks to the layerwise processing by the model. Given the deterministic nature of such
199"
A FIRST-PRINCIPLES JUSTIFICATION,0.24162679425837322,"processing at inference time, we speculate that the trace obtained by sequentially collecting the
200"
A FIRST-PRINCIPLES JUSTIFICATION,0.24282296650717702,"(pre)activation values within the network, along the forward pass, constitutes a richer characterisation
201"
A FIRST-PRINCIPLES JUSTIFICATION,0.24401913875598086,"of such an amplification process compared to the knowledge of the input alone. Indeed, as we do, it
202"
A FIRST-PRINCIPLES JUSTIFICATION,0.2452153110047847,"is possible to learn a direct mapping from such featurisation of the input, to a distribution of possible
203"
A FIRST-PRINCIPLES JUSTIFICATION,0.24641148325358853,"perturbation-free input reconstructions – taking advantage of such characterisation.
204"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.24760765550239233,"4.3
Hierarchical input and internal representation encoding
205"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.24880382775119617,"Training a conditional VAE requires [56] that the conditioning set c is concatenated to the input x
206"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.25,"before encoding occurs, and to the sample of latent variables z right before decoding. The same is
207"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2511961722488038,"also true, with the suitable adjustments, for any conditional generative approach where the target and
208"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.25239234449760767,"the conditioning set must be processed jointly.
209"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2535885167464115,"In order to ensure the usability and scalability of CARSO across the widest range of input data and
210"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.25478468899521534,"classifier models, we propose to perform such processing in a hierarchical and partially disjoint
211"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.25598086124401914,"fashion between the input and the conditioning set. In principle, the encoding of x and c can be
212"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.25717703349282295,"performed by two different and independent subnetworks, until some form of joint processing must
213"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2583732057416268,"occur. This allows to retain the overall architectural structure of the purifier, while having finer-grained
214"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2595693779904306,"control over the inductive biases [43] deemed the most suitable for the respective variables.
215"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2607655502392344,"In the experimental phase of our work, we encode the two variables independently. The input
216"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2619617224880383,"is compressed by a multilayer convolutional neural network (CNN). The internal representation –
217"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2631578947368421,"which in our case is composed of differently sized multi-channel images – is processed layer by
218"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.26435406698564595,"layer by independent multilayer CNNs (responsible for encoding local information), whose flattened
219"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.26555023923444976,"outputs are finally concatenated and compressed by a fully-connected layer (modelling inter-layer
220"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.26674641148325356,"correlations in the representation). The resulting compressed input and conditioning set are then
221"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2679425837320574,"further concatenated and jointly encoded by a fully-connected network (FCN).
222"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.26913875598086123,"In order to use the VAE decoder at inference time, the entire compression machinery for the condi-
223"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2703349282296651,"tioning set must be preserved after training, and used to encode the internal representations extracted.
224"
HIERARCHICAL INPUT AND INTERNAL REPRESENTATION ENCODING,0.2715311004784689,"The equivalent input encoder may be discarded instead.
225"
ADVERSARIALLY-BALANCED BATCHES,0.2727272727272727,"4.4
Adversarially-balanced batches
226"
ADVERSARIALLY-BALANCED BATCHES,0.27392344497607657,"Training the purifier in representation-conditional input reconstruction requires having access to
227"
ADVERSARIALLY-BALANCED BATCHES,0.2751196172248804,"adversarially-perturbed examples generated against the classifier, and to the corresponding clean data.
228"
ADVERSARIALLY-BALANCED BATCHES,0.27631578947368424,"Specifically, we use as input a mixture of clean and adversarially perturbed examples, and the clean
229"
ADVERSARIALLY-BALANCED BATCHES,0.27751196172248804,"input as the target.
230"
ADVERSARIALLY-BALANCED BATCHES,0.27870813397129185,"Within each epoch, the training set of interest is shuffled [51, 10], and only a fixed fraction of each
231"
ADVERSARIALLY-BALANCED BATCHES,0.2799043062200957,"resulting batch is adversarially perturbed. Calling ϵ the maximum ℓp perturbation norm bound for
232"
ADVERSARIALLY-BALANCED BATCHES,0.2811004784688995,"the threat model against which the classifier was adversarially pre-trained, the portion of perturbed
233"
ADVERSARIALLY-BALANCED BATCHES,0.2822966507177033,"examples is generated by an even split of FGSMϵ/2, PGDϵ/2, FGSMϵ, and PGDϵ attacks.
234"
ADVERSARIALLY-BALANCED BATCHES,0.2834928229665072,"Any smaller subset of attack types and strengths, or a detailedly unbalanced batch composition,
235"
ADVERSARIALLY-BALANCED BATCHES,0.284688995215311,"always experimentally results in a worse performing purification model. More details justifying such
236"
ADVERSARIALLY-BALANCED BATCHES,0.28588516746411485,"choice are provided in Appendix C.
237"
ROBUST AGGREGATION STRATEGY,0.28708133971291866,"4.5
Robust aggregation strategy
238"
ROBUST AGGREGATION STRATEGY,0.28827751196172247,"At inference time, many different input reconstructions are classified by the classifier, and the
239"
ROBUST AGGREGATION STRATEGY,0.2894736842105263,"respective outputs concur to the settlement of a robust prediction.
240"
ROBUST AGGREGATION STRATEGY,0.29066985645933013,"Calling lα
i the output logit associated with class i ∈{1, . . . , C} in the prediction by the classifier on
241"
ROBUST AGGREGATION STRATEGY,0.291866028708134,"sample α ∈{1, . . . , N}, we adopt the following aggregation strategy:
242"
ROBUST AGGREGATION STRATEGY,0.2930622009569378,"Pi := 1 Z N
Y"
ROBUST AGGREGATION STRATEGY,0.2942583732057416,"α=1
eelα
i"
ROBUST AGGREGATION STRATEGY,0.29545454545454547,"with Pi being the aggregated probability of membership in class i, Z a normalisation constant such
243"
ROBUST AGGREGATION STRATEGY,0.2966507177033493,"that PC
i=1 Pi = 1, and e Euler’s number.
244"
ROBUST AGGREGATION STRATEGY,0.29784688995215314,"Such choice produces a robust prediction much harder to take over in the event that an adversary
245"
ROBUST AGGREGATION STRATEGY,0.29904306220095694,"selectively targets a specific input reconstruction. A heuristic justification for this property is given in
246"
ROBUST AGGREGATION STRATEGY,0.30023923444976075,"Appendix D.
247"
EXPERIMENTAL ASSESSMENT,0.3014354066985646,"5
Experimental assessment
248"
EXPERIMENTAL ASSESSMENT,0.3026315789473684,"Experimental evaluation of our method is carried out in terms of robust and clean image classification
249"
EXPERIMENTAL ASSESSMENT,0.3038277511961722,"accuracy within three different scenarios (a, b and c), determined by the specific classification task.
250"
EXPERIMENTAL ASSESSMENT,0.3050239234449761,"The white-box threat model with a fixed ℓ∞norm bound is assumed throughout, as it generally
251"
EXPERIMENTAL ASSESSMENT,0.3062200956937799,"constitutes the most demanding setup for adversarial defences.
252"
SETUP,0.30741626794258375,"5.1
Setup
253"
SETUP,0.30861244019138756,"Data
The CIFAR-10 [33] dataset is used in scenario (a), the CIFAR-100 [33] dataset is used in
254"
SETUP,0.30980861244019137,"scenario (b), whereas the TINYIMAGENET-200 [14] dataset is used in scenario (c).
255"
SETUP,0.31100478468899523,"Architectures
A WIDERESNET-28-10 model is used as the classifier, adversarially pre-trained on
256"
SETUP,0.31220095693779903,"the respective dataset – the only difference between scenarios being the number of output logits: 10
257"
SETUP,0.3133971291866029,"in scenario (a), 100 in scenario (b), and 200 in scenario (c).
258"
SETUP,0.3145933014354067,"The purifier is composed of a conditional VAE, processing inputs and internal representations in a
259"
SETUP,0.3157894736842105,"partially disjoint fashion, as explained in subsection 4.3. The input is compressed by a two-layer
260"
SETUP,0.31698564593301437,"CNN; the internal representation is instead processed layerwise by independent CNNs (three-layered
261"
SETUP,0.3181818181818182,"in scenarios (a) and (b), four-layered in scenario (c)) whose outputs are then concatenated and
262"
SETUP,0.319377990430622,"compressed by a fully-connected layer. A final two-layer FCN jointly encodes the compressed input
263"
SETUP,0.32057416267942584,"and conditioning set, after the concatenation of the two. A six-layer deconvolutional network is used
264"
SETUP,0.32177033492822965,"as the decoder.
265"
SETUP,0.3229665071770335,"More precise details on all architectures are given in Appendix E.
266"
SETUP,0.3241626794258373,"Outer minimisation
In scenarios (a) and (b), the classifier is trained according to [18]; in scenario
267"
SETUP,0.3253588516746411,"(c), according to [62]. Classifiers were always acquired as pre-trained models, using publicly available
268"
SETUP,0.326555023923445,"weights provided by the respective authors.
269"
SETUP,0.3277511961722488,"The purifier is trained on the VAE loss, using summed pixel-wise channel-wise binary cross-entropy
270"
SETUP,0.32894736842105265,"as the reconstruction cost. Optimisation is performed by RADAM+LOOKAHEAD [41, 69] with a
271"
SETUP,0.33014354066985646,"learning rate schedule that presents a linear warm-up, a plateau phase, and a linear annealing [55].
272"
SETUP,0.33133971291866027,"To promote the learning of meaningful reconstructions during the initial phases of training, the KL
273"
SETUP,0.33253588516746413,"divergence term in the VAE loss is suppressed for an initial number of epochs. Afterwards, it is
274"
SETUP,0.33373205741626794,"linearly modulated up to its actual value, during a fixed number of epochs (β increase) [26]. The
275"
SETUP,0.3349282296650718,"initial and final epochs of such modulation are reported in Table 14.
276"
SETUP,0.3361244019138756,"Additional scenario-specific details are provided in Appendix E.
277"
SETUP,0.3373205741626794,"Inner minimisation
ϵ∞= 8/255 is set as the perturbation norm bound.
278"
SETUP,0.33851674641148327,"Adversarial examples against the purifier are obtained, as explained in subsection 4.4, by FGSMϵ/2,
279"
SETUP,0.3397129186602871,"PGDϵ/2, FGSMϵ, and PGDϵ, in a class-untargeted fashion on the cross-entropy loss. In the case of
280"
SETUP,0.3409090909090909,"PGD, gradient ascent with a step size of α = 0.01 is used.
281"
SETUP,0.34210526315789475,"The complete details and hyperparameters of the attacks are described in Appendix E.
282"
SETUP,0.34330143540669855,"Evaluation
In each scenario, we report the clean and robust test-set accuracy – the latter by means
283"
SETUP,0.3444976076555024,"of AUTOATTACK [17] – of the classifier and the corresponding CARSO architecture.
284"
SETUP,0.3456937799043062,"For the classifier alone, the standard version of AUTOATTACK (AA) is used: i.e., the worst-case
285"
SETUP,0.34688995215311,"accuracy on a mixture of AUTOPGD on the cross-entropy loss [17] with 100 steps, AUTOPGD on
286"
SETUP,0.3480861244019139,"the difference of logits ratio loss [17] with 100 steps, FAB [16] with 100 steps, and the black-box
287"
SETUP,0.3492822966507177,"SQUARE attack [1] with 5000 queries.
288"
SETUP,0.35047846889952156,"In the evaluation of the CARSO architecture, the number of reconstructed samples per input is set to 8,
289"
SETUP,0.35167464114832536,"the logits are aggregated as explained in subsection 4.5, and the output class is finally selected as the
290"
SETUP,0.35287081339712917,"arg max of the aggregation. Due to the stochastic nature of the purifier, robust accuracy is assessed
291"
SETUP,0.35406698564593303,"by a version of AUTOATTACK suitable for stochastic defences (randAA) – composed of AUTOPGD
292"
SETUP,0.35526315789473684,"on the cross-entropy and difference of logits ratio losses, across 20 Expectation over Transformation
293"
SETUP,0.35645933014354064,"(EOT) [3] iterations with 100 gradient ascent steps each.
294"
SETUP,0.3576555023923445,"Computational infrastructure
All experiments were performed on an NVIDIA DGX A100 system.
295"
SETUP,0.3588516746411483,"Training in scenarios (a) and (c) was run on 8 NVIDIA A100 GPUs with 40 GB of dedicated memory
296"
SETUP,0.36004784688995217,"each; in scenario (b) 4 of such devices were used. Elapsed real training time for the purifier in all
297"
SETUP,0.361244019138756,"scenarios is reported in Table 1.
298"
SETUP,0.3624401913875598,Table 1: Elapsed real running time for training the purifier in the different scenarios considered.
SETUP,0.36363636363636365,"Scenario
(a)
(b)
(c)"
SETUP,0.36483253588516745,"Elapsed real training time
159 min
138 min
213 min"
RESULTS AND DISCUSSION,0.3660287081339713,"5.2
Results and discussion
299"
RESULTS AND DISCUSSION,0.3672248803827751,"An analysis of the experimental results is provided in the subsection that follows, whereas their
300"
RESULTS AND DISCUSSION,0.3684210526315789,"systematic exposition is given in Table 2.
301"
RESULTS AND DISCUSSION,0.3696172248803828,"Table 2: Clean (results in italic) and adversarial (results in upright) accuracy for the different models and
datasets used in the respective scenarios. The following abbreviations are used: Scen: scenario considered;
AT/Cl: clean accuracy for the adversarially-pretrained model used as the classifier, when considered alone;
C/Cl: clean accuracy for the CARSO architecture; AT/AA: robust accuracy (by the means of AUTOATTACK) for
the adversarially-pretrained model used as the classifier, when considered alone; C/randAA: robust accuracy for
the CARSO architecture, when attacked end-to-end by AUTOATTACK for randomised defences; Best AT/AA:
best robust accuracy result for the respective dataset (by the means of AUTOATTACK), obtained by adversarial
training alone (any model); Best P/AA: best robust accuracy result for the respective dataset (by the means
of AUTOATTACK), obtained by adversarial purification (any model). Robust accuracies in round brackets are
obtained using the PGD+EOT [38] pipeline, developed for diffusion-based purifiers. The best clean and robust
accuracies per dataset are shown in bold. The clean accuracies for the models referred to in the Best columns
are shown in Table 15 (in Appendix F)."
RESULTS AND DISCUSSION,0.3708133971291866,"Scen.
Dataset
AT/Cl
C/Cl
AT/AA
C/rand-AA
(PGD+EOT)
Best AT/AA
Best P/AA
(PGD+EOT)"
RESULTS AND DISCUSSION,0.37200956937799046,"(a)
CIFAR-10
0.9216
0.8686
0.6773
0.7613
(0.7689)
0.7107
0.7812
(0.6641)"
RESULTS AND DISCUSSION,0.37320574162679426,"(b)
CIFAR-100
0.7385
0.6806
0.3918
0.6665
0.4267
0.4609"
RESULTS AND DISCUSSION,0.37440191387559807,"(c)
TINYIMAGENET-200
0.6519
0.5632
0.3130
0.5356
0.3130"
RESULTS AND DISCUSSION,0.37559808612440193,"Scenario (a)
Comparing the robust accuracy of the classifier model used in scenario (a) [18]
302"
RESULTS AND DISCUSSION,0.37679425837320574,"with that resulting from the inclusion of the same model in the CARSO architecture, we observe
303"
RESULTS AND DISCUSSION,0.37799043062200954,"a +8.4% increase. This is counterbalanced by a −5.6% clean accuracy toll. The same version of
304"
RESULTS AND DISCUSSION,0.3791866028708134,"CARSO further provides a +5.03 robustness increase w.r.t. the current best AT-trained model [47]
305"
RESULTS AND DISCUSSION,0.3803827751196172,"that employs a ∼3× larger RAWIDERESNET-70-16 model.
306"
RESULTS AND DISCUSSION,0.3815789473684211,"In addition, our method provides a remarkable +9.72% increase in robust accuracy w.r.t. to the best
307"
RESULTS AND DISCUSSION,0.3827751196172249,"adversarial purification approach [40], a diffusion-based purifier. However, the comparison is not as
308"
RESULTS AND DISCUSSION,0.3839712918660287,"straightforward. In fact, the paper [40] reports a robust accuracy of 78.12% using AUTOATTACK on
309"
RESULTS AND DISCUSSION,0.38516746411483255,"the gradients obtained via the adjoint method [45]. As noted in [38], such evaluation (which uses the
310"
RESULTS AND DISCUSSION,0.38636363636363635,"version of AUTOATTACK that is unsuitable for stochastic defences) leads to a large overestimation of
311"
RESULTS AND DISCUSSION,0.3875598086124402,"the robustness of diffusive purifiers. As suggested in [38], the authors of [40] re-evaluate the robust
312"
RESULTS AND DISCUSSION,0.388755980861244,"accuracy according to a more suitable pipeline (PGD+EOT, whose hyperparameters are shown in
313"
RESULTS AND DISCUSSION,0.38995215311004783,"Table 12), obtaining a much lower robust accuracy of 66.41%. Consequently, we repeat the same
314"
RESULTS AND DISCUSSION,0.3911483253588517,"evaluation for CARSO and compare the worst-case robustness amongst the two. In line with typical
315"
RESULTS AND DISCUSSION,0.3923444976076555,"AT methods, and unlike diffusive purification, the robustness of CARSO assessed by means of randAA
316"
RESULTS AND DISCUSSION,0.39354066985645936,"is still lower w.r.t. than achieved by PGD+EOT.
317"
RESULTS AND DISCUSSION,0.39473684210526316,"Scenario (b)
Moving to scenario (b), CARSO achieves a robust accuracy increase of +27.47% w.r.t.
318"
RESULTS AND DISCUSSION,0.39593301435406697,"the classifier alone [18], balanced by a 5.79% decrease in clean accuracy. Our approach also improves
319"
RESULTS AND DISCUSSION,0.39712918660287083,"upon the robust accuracy of the best AT-trained model [62] (WIDERESNET-70-16) by 23.98%. In
320"
RESULTS AND DISCUSSION,0.39832535885167464,"the absence of a reliable robustness evaluation by means of PGD+EOT for the best purification-based
321"
RESULTS AND DISCUSSION,0.39952153110047844,"method [40], we still obtain a +20.25% increase in robust accuracy upon its (largely overestimated)
322"
RESULTS AND DISCUSSION,0.4007177033492823,"AA result.
323"
RESULTS AND DISCUSSION,0.4019138755980861,"Scenario (c)
In scenario (c), CARSO improves upon the classifier alone [62] (which is also the
324"
RESULTS AND DISCUSSION,0.40311004784689,"best AT-based approach for TINYIMAGENET-200) by +22.26%. A significant clean accuracy toll is
325"
RESULTS AND DISCUSSION,0.4043062200956938,"imposed by the relative complexity of the dataset, i.e. −8.87%. In this setting, we lack any additional
326"
RESULTS AND DISCUSSION,0.4055023923444976,"purification-based methods.
327"
RESULTS AND DISCUSSION,0.40669856459330145,"Assessing the impact of gradient obfuscation
Although the architecture of CARSO is algorith-
328"
RESULTS AND DISCUSSION,0.40789473684210525,"mically differentiable end-to-end – and the integrated diagnostics of the randAA routines raised no
329"
RESULTS AND DISCUSSION,0.4090909090909091,"warnings during the assessment – we additionally guard against the eventual gradient obfuscation [2]
330"
RESULTS AND DISCUSSION,0.4102870813397129,"induced by our method by repeating the evaluation at ϵ∞= 0.95, verifying that the resulting robust
331"
RESULTS AND DISCUSSION,0.41148325358851673,"accuracy stays below random chance [12]. Results are shown in Table 3.
332"
RESULTS AND DISCUSSION,0.4126794258373206,"Table 3: Robust classification accuracy against AUTOATTACK, for ϵ∞= 0.95, as a way to assess the (lack of)
impact of gradient obfuscation on robust accuracy evaluation."
RESULTS AND DISCUSSION,0.4138755980861244,"Scenario
(a)
(b)
(c)"
RESULTS AND DISCUSSION,0.4150717703349282,"ϵ∞= 0.95 acc.
<0.047
<0.010
≈0.0"
LIMITATIONS AND OPEN PROBLEMS,0.41626794258373206,"5.3
Limitations and open problems
333"
LIMITATIONS AND OPEN PROBLEMS,0.41746411483253587,"In line with recent research aiming at the development of robust defences against multiple perturb-
334"
LIMITATIONS AND OPEN PROBLEMS,0.41866028708133973,"ations [20, 35], our method determines a decrease in clean accuracy w.r.t. the original model on
335"
LIMITATIONS AND OPEN PROBLEMS,0.41985645933014354,"which it is built upon – especially in scenario (c) as the complexity of the dataset increases. This
336"
LIMITATIONS AND OPEN PROBLEMS,0.42105263157894735,"phenomenon is partly dependent on the choice of a VAE as the generative purification model, a
337"
LIMITATIONS AND OPEN PROBLEMS,0.4222488038277512,"requirement for the fairest evaluation possible in terms of robustness.
338"
LIMITATIONS AND OPEN PROBLEMS,0.423444976076555,"Yet, the issue remains open: is it possible to devise a CARSO-like architecture capable of the same
339"
LIMITATIONS AND OPEN PROBLEMS,0.4246411483253589,"– if not better – robust behaviour, which is also competitively accurate on clean inputs? Potential
340"
LIMITATIONS AND OPEN PROBLEMS,0.4258373205741627,"avenues for future research may involve the development of CARSO-like architectures in which
341"
LIMITATIONS AND OPEN PROBLEMS,0.4270334928229665,"representation-conditional data generation is obtained by means of diffusion or score-based models.
342"
LIMITATIONS AND OPEN PROBLEMS,0.42822966507177035,"Alternatively, incremental developments aimed at improving the cross-talk between the purifier and
343"
LIMITATIONS AND OPEN PROBLEMS,0.42942583732057416,"the final classifier may be pursued.
344"
LIMITATIONS AND OPEN PROBLEMS,0.430622009569378,"Lastly, the scalability of CARSO could be strongly improved by determining whether the internal
345"
LIMITATIONS AND OPEN PROBLEMS,0.4318181818181818,"representation used in conditional data generation may be restricted to a smaller subset of layers,
346"
LIMITATIONS AND OPEN PROBLEMS,0.43301435406698563,"while still maintaining the general robustness of the method.
347"
CONCLUSION,0.4342105263157895,"6
Conclusion
348"
CONCLUSION,0.4354066985645933,"In this work, we presented a novel adversarial defence mechanism tightly integrating input purification,
349"
CONCLUSION,0.4366028708133971,"and classification by an adversarially-trained model – in the form of representation-conditional data
350"
CONCLUSION,0.43779904306220097,"purification. Our method is able to improve upon the current state-of-the-art in CIFAR-10, CIFAR-
351"
CONCLUSION,0.43899521531100477,"100, and TINYIMAGENET ℓ∞robust classification, w.r.t. both adversarial training and purification
352"
CONCLUSION,0.44019138755980863,"approaches alone.
353"
CONCLUSION,0.44138755980861244,"Such results suggest a new synergistic strategy to achieve adversarial robustness in visual tasks and
354"
CONCLUSION,0.44258373205741625,"motivate future research on the application of the same design principles to different models and
355"
CONCLUSION,0.4437799043062201,"types of data.
356"
REFERENCES,0.4449760765550239,"References
357"
REFERENCES,0.4461722488038278,"[1]
Maksym Andriushchenko et al. ‘Square Attack: a query-efficient black-box adversarial attack via random
358"
REFERENCES,0.4473684210526316,"search’. In: 16th European Conference on Computer Vision. 2020.
359"
REFERENCES,0.4485645933014354,"[2]
Anish Athalye, Nicholas Carlini and David Wagner. ‘Obfuscated Gradients Give a False Sense of Security:
360"
REFERENCES,0.44976076555023925,"Circumventing Defenses to Adversarial Examples’. In: Proceedings of the International Conference on
361"
REFERENCES,0.45095693779904306,"Machine Learning. 2018.
362"
REFERENCES,0.45215311004784686,"[3]
Anish Athalye et al. ‘Synthesizing Robust Adversarial Examples’. In: Proceedings of the International
363"
REFERENCES,0.4533492822966507,"Conference on Machine Learning. 2018.
364"
REFERENCES,0.45454545454545453,"[4]
Emanuele Ballarin. ebtorch: Collection of PyTorch additions, extensions, utilities, uses and abuses.
365"
REFERENCES,0.4557416267942584,"2024. URL: https://github.com/emaballarin/ebtorch.
366"
REFERENCES,0.4569377990430622,"[5]
Vincent Ballet et al. ‘Imperceptible Adversarial Attacks on Tabular Data’. In: Thirty-third Conference on
367"
REFERENCES,0.458133971291866,"Neural Information Processing Systems, Workshop on Robust AI in Financial Services: Data, Fairness,
368"
REFERENCES,0.45933014354066987,"Explainability, Trustworthiness, and Privacy (Robust AI in FS). 2019.
369"
REFERENCES,0.4605263157894737,"[6]
Atilim Gunes Baydin et al. ‘Automatic differentiation in machine learning: a survey’. In: The Journal of
370"
REFERENCES,0.46172248803827753,"Machine Learning Research 18.153 (2018), pp. 1–43.
371"
REFERENCES,0.46291866028708134,"[7]
Battista Biggio and Fabio Roli. ‘Wild patterns: Ten years after the rise of adversarial machine learning’.
372"
REFERENCES,0.46411483253588515,"In: Pattern Recognition 84 (2018), pp. 317–331.
373"
REFERENCES,0.465311004784689,"[8]
Battista Biggio et al. ‘Evasion Attacks against Machine Learning at Test Time’. In: Proceedings of the
374"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4665071770334928,"2013th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume
375"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4677033492822967,"Part III. 2013.
376"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4688995215311005,"[9]
Luca Bortolussi and Guido Sanguinetti. Intrinsic Geometric Vulnerability of High-Dimensional Artificial
377"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4700956937799043,"Intelligence. 2018. arXiv: 1811.03571.
378"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.47129186602870815,"[10]
Léon Bottou. ‘On-Line Algorithms and Stochastic Approximations’. In: On-Line Learning in Neural
379"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.47248803827751196,"Networks. Cambridge University Press, 1999. Chap. 2.
380"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.47368421052631576,"[11]
Ginevra Carbone et al. ‘Robustness of Bayesian Neural Networks to Gradient-Based Attacks’. In:
381"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4748803827751196,"Advances in Neural Information Processing Systems. 2020.
382"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.47607655502392343,"[12]
Nicholas Carlini et al. On Evaluating Adversarial Robustness. 2019. arXiv: 1902.06705.
383"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4772727272727273,"[13]
Huanran Chen et al. Robust Classification via a Single Diffusion Model. 2023. arXiv: 2305.15241.
384"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4784688995215311,"[14]
Patryk Chrabaszcz, Ilya Loshchilov and Frank Hutter. A Downsampled Variant of ImageNet as an
385"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4796650717703349,"Alternative to the CIFAR datasets. 2017. arXiv: 1707.08819.
386"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.48086124401913877,"[15]
Moustapha Cisse et al. ‘Parseval Networks: Improving Robustness to Adversarial Examples’. In: Pro-
387"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4820574162679426,"ceedings of the International Conference on Machine Learning. 2017.
388"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.48325358851674644,"[16]
Francesco Croce and Matthias Hein. Minimally distorted Adversarial Examples with a Fast Adaptive
389"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.48444976076555024,"Boundary Attack. 2020. arXiv: 1907.02044.
390"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.48564593301435405,"[17]
Francesco Croce and Matthias Hein. ‘Reliable Evaluation of Adversarial Robustness with an Ensemble of
391"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4868421052631579,"Diverse Parameter-free Attacks’. In: Proceedings of the International Conference on Machine Learning.
392"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4880382775119617,"2020.
393"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4892344497607656,"[18]
Jiequan Cui et al. Decoupled Kullback-Leibler Divergence Loss. 2023. arXiv: 2305.13948.
394"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4904306220095694,"[19]
Gavin Weiguang Ding, Luyu Wang and Xiaomeng Jin. AdverTorch v0.1: An Adversarial Robustness
395"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4916267942583732,"Toolbox based on PyTorch. 2019. arXiv: 1902.07623.
396"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.49282296650717705,"[20]
Hadi M. Dolatabadi, Sarah Erfani and Christopher Leckie. ‘ℓ∞-Robustness and Beyond: Unleashing
397"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.49401913875598086,"Efficient Adversarial Training’. In: 18th European Conference on Computer Vision. 2022.
398"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.49521531100478466,"[21]
Ian Goodfellow, Jonathon Shlens and Christian Szegedy. ‘Explaining and Harnessing Adversarial Ex-
399"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4964114832535885,"amples’. In: International Conference on Learning Representations. 2015.
400"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.49760765550239233,"[22]
Ian Goodfellow et al. ‘Generative Adversarial Nets’. In: Advances in Neural Information Processing
401"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.4988038277511962,"Systems. 2014.
402"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5,"[23]
Sven Gowal et al. ‘Improving Robustness using Generated Data’. In: Advances in Neural Information
403"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5011961722488039,"Processing Systems. 2021.
404"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5023923444976076,"[24]
Sven Gowal et al. Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial
405"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5035885167464115,"Examples. 2020. arXiv: 2010.03593.
406"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5047846889952153,"[25]
Shixiang Gu and Luca Rigazio. ‘Towards Deep Neural Network Architectures Robust to Adversarial
407"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5059808612440191,"Examples’. In: Workshop Track of the International Conference on Learning Representations. 2015.
408"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.507177033492823,"[26]
Irina Higgins et al. ‘beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Frame-
409"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5083732057416268,"work’. In: International Conference on Learning Representations. 2017.
410"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5095693779904307,"[27]
Mitch Hill, Jonathan Mitchell and Song-Chun Zhu. ‘Stochastic Security: Adversarial Defense Using Long-
411"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5107655502392344,"Run Dynamics of Energy-Based Models’. In: International Conference on Learning Representations.
412"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5119617224880383,"2021.
413"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5131578947368421,"[28]
Chin-Wei Huang, Jae Hyun Lim and Aaron C Courville. ‘A Variational Perspective on Diffusion-Based
414"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5143540669856459,"Generative Models and Score Matching’. In: Advances in Neural Information Processing Systems. 2021.
415"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5155502392344498,"[29]
Uiwon Hwang et al. ‘PuVAE: A Variational Autoencoder to Purify Adversarial Examples’. In: IEEE
416"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5167464114832536,"Access. 2019.
417"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5179425837320574,"[30]
Andrew Ilyas et al. ‘Adversarial Examples Are Not Bugs, They Are Features’. In: Advances in Neural
418"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5191387559808612,"Information Processing Systems. 2019.
419"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5203349282296651,"[31]
Xiaojun Jia et al. ‘LAS-AT: Adversarial Training With Learnable Attack Strategy’. In: Proceedings of the
420"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5215311004784688,"IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
421"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5227272727272727,"[32]
Diederik P. Kingma and Max Welling. ‘Auto-Encoding Variational Bayes’. In: International Conference
422"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5239234449760766,"on Learning Representations. 2014.
423"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5251196172248804,"[33]
Alex Krizhevsky. ‘Learning Multiple Layers of Features from Tiny Images’. In: 2009.
424"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5263157894736842,"[34]
Alexey Kurakin, Ian J. Goodfellow and Samy Bengio. ‘Adversarial Examples in the Physical World’. In:
425"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.527511961722488,"Artificial Intelligence Safety and Security (2018).
426"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5287081339712919,"[35]
Cassidy Laidlaw, Sahil Singla and Soheil Feizi. ‘Perceptual Adversarial Robustness: Defense Against
427"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5299043062200957,"Unseen Threat Models’. In: International Conference on Learning Representations. 2021.
428"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5311004784688995,"[36]
Yann LeCun and Corinna Cortes. The MNIST handwritten digit database. 2010.
429"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5322966507177034,"[37]
Yann LeCun et al. ‘A tutorial on energy-based learning’. In: Predicting structured data. MIT Press, 2006.
430"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5334928229665071,"Chap. 1.
431"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.534688995215311,"[38]
Minjong Lee and Dongwoo Kim. ‘Robust Evaluation of Diffusion-Based Adversarial Purification’. In:
432"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5358851674641149,"International Conference on Computer Vision. 2024.
433"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5370813397129187,"[39]
Fangzhou Liao et al. ‘Defense Against Adversarial Attacks Using High-Level Representation Guided
434"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5382775119617225,"Denoiser’. In: IEEE Conference on Computer Vision and Pattern Recognition. 2018.
435"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5394736842105263,"[40]
Guang Lin et al. Robust Diffusion Models for Adversarial Purification. 2024. arXiv: 2403.16067.
436"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5406698564593302,"[41]
Liyuan Liu et al. ‘On the Variance of the Adaptive Learning Rate and Beyond’. In: International
437"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5418660287081339,"Conference on Learning Representations. 2020.
438"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5430622009569378,"[42]
Aleksander Madry et al. ‘Towards Deep Learning Models Resistant to Adversarial Attacks’. In: Interna-
439"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5442583732057417,"tional Conference on Learning Representations. 2018.
440"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5454545454545454,"[43]
Tom M. Mitchell. The Need for Biases in Learning Generalizations. Tech. rep. New Brunswick, NJ:
441"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5466507177033493,"Rutgers University, 1980.
442"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5478468899521531,"[44]
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi and Pascal Frossard. ‘DeepFool: A Simple and
443"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5490430622009569,"Accurate Method to Fool Deep Neural Networks’. In: IEEE Conference on Computer Vision and Pattern
444"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5502392344497608,"Recognition. 2016.
445"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5514354066985646,"[45]
Weili Nie et al. ‘Diffusion Models for Adversarial Purification’. In: Proceedings of the International
446"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5526315789473685,"Conference on Machine Learning. 2022.
447"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5538277511961722,"[46]
Adam Paszke et al. ‘PyTorch: An Imperative Style, High-Performance Deep Learning Library’. In:
448"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5550239234449761,"Advances in Neural Information Processing Systems. 2019.
449"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.55622009569378,"[47]
ShengYun Peng et al. Robust Principles: Architectural Design Principles for Adversarially Robust CNNs.
450"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5574162679425837,"2023.
451"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5586124401913876,"[48]
Yao Qin et al. ‘Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recog-
452"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5598086124401914,"nition’. In: Proceedings of the International Conference on Machine Learning. 2019.
453"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5610047846889952,"[49]
Sylvestre-Alvise Rebuffi et al. ‘Data Augmentation Can Improve Robustness’. In: Advances in Neural
454"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.562200956937799,"Information Processing Systems. 2021.
455"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5633971291866029,"[50]
Danilo Jimenez Rezende, Shakir Mohamed and Daan Wierstra. ‘Stochastic Backpropagation and Ap-
456"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5645933014354066,"proximate Inference in Deep Generative Models’. In: Proceedings of the International Conference on
457"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5657894736842105,"Machine Learning. 2014.
458"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5669856459330144,"[51]
Herbert Robbins and Sutton Monro. ‘A Stochastic Approximation Method’. In: The Annals of Mathemat-
459"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5681818181818182,"ical Statistics 22.3 (1951), pp. 400–407.
460"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.569377990430622,"[52]
Pouya Samangouei, Maya Kabkab and Rama Chellappa. ‘Defense-GAN: Protecting Classifiers Against
461"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5705741626794258,"Adversarial Attacks Using Generative Models’. In: International Conference on Learning Representations.
462"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5717703349282297,"2018.
463"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5729665071770335,"[53]
Changhao Shi, Chester Holtz and Gal Mishne. ‘Online Adversarial Purification based on Self-supervised
464"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5741626794258373,"Learning’. In: International Conference on Learning Representations. 2021.
465"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5753588516746412,"[54]
Naman D Singh, Francesco Croce and Matthias Hein. Revisiting Adversarial Training for ImageNet:
466"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5765550239234449,"Architectures, Training and Generalization across Threat Models. 2023. arXiv: 2303.01870.
467"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5777511961722488,"[55]
Leslie N. Smith. ‘Cyclical Learning Rates for Training Neural Networks’. In: IEEE Winter Conference
468"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5789473684210527,"on Applications of Computer Vision. 2017.
469"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5801435406698564,"[56]
Kihyuk Sohn, Honglak Lee and Xinchen Yan. ‘Learning Structured Output Representation using Deep
470"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5813397129186603,"Conditional Generative Models’. In: Advances in Neural Information Processing Systems. 2015.
471"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5825358851674641,"[57]
Christian Szegedy et al. ‘Intriguing properties of neural networks’. In: International Conference on
472"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.583732057416268,"Learning Representations. 2014.
473"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5849282296650717,"[58]
Florian Tramèr and Dan Boneh. ‘Adversarial Training and Robustness for Multiple Perturbations’. In:
474"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5861244019138756,"Advances in Neural Information Processing Systems. 2019.
475"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5873205741626795,"[59]
Florian Tramèr et al. ‘Ensemble Adversarial Training: Attacks and Defenses’. In: International Conference
476"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5885167464114832,"on Learning Representations. 2018.
477"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5897129186602871,"[60]
Florian Tramèr et al. ‘On Adaptive Attacks to Adversarial Example Defenses’. In: Advances in Neural
478"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5909090909090909,"Information Processing Systems. 2020.
479"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5921052631578947,"[61]
Pascal Vincent et al. ‘Extracting and composing robust features with denoising autoencoders’. In: Inter-
480"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5933014354066986,"national Conference on Machine Learning. 2008.
481"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5944976076555024,"[62]
Zekai Wang et al. Better Diffusion Models Further Improve Adversarial Training. 2023. arXiv: 2303.
482"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5956937799043063,"10130.
483"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.59688995215311,"[63]
Han Xiao, Kashif Rasul and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking
484"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5980861244019139,"Machine Learning Algorithms. 2017. arXiv: 1708.07747.
485"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.5992822966507177,"[64]
Xinchen Yan et al. ‘Attribute2Image: Conditional Image Generation from Visual Attributes’. In: Proceed-
486"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6004784688995215,"ings of the European Conference on Computer Vision. 2016.
487"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6016746411483254,"[65]
Zhaoyuan Yang et al. ‘Adversarial Purification with the Manifold Hypothesis’. In: AAAI Conference on
488"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6028708133971292,"Artificial Intelligence. 2024.
489"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.604066985645933,"[66]
Jongmin Yoon, Sung Ju Hwang and Juho Lee. ‘Adversarial Purification with Score-based Generative
490"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6052631578947368,"Models’. In: Proceedings of the International Conference on Machine Learning. 2021.
491"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6064593301435407,"[67]
Hongyang Zhang et al. ‘Theoretically Principled Trade-off between Robustness and Accuracy’. In:
492"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6076555023923444,"Proceedings of the International Conference on Machine Learning. 2019.
493"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6088516746411483,"[68]
M. Zhang, S. Levine and C. Finn. ‘MEMO: Test Time Robustness via Adaptation and Augmentation’. In:
494"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6100478468899522,"Advances in Neural Information Processing Systems. 2022.
495"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.611244019138756,"[69]
Michael Zhang et al. ‘Lookahead Optimizer: k steps forward, 1 step back’. In: Advances in Neural
496"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6124401913875598,"Information Processing Systems. 2019.
497"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6136363636363636,"[70]
Zhaoyu Zhang, Mengyan Li and Jun Yu. ‘On the Convergence and Mode Collapse of GAN’. In: SIG-
498"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6148325358851675,"GRAPH Asia 2018 Technical Briefs. 2018.
499"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6160287081339713,"A
On Projected Gradient Descent adversarial training
500"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6172248803827751,"The task of determining model parameters θ⋆that are robust to adversarial perturbations is cast in
501"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.618421052631579,"[42] as a min-max optimisation problem seeking to minimise adversarial risk, i.e.:
502"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6196172248803827,"θ⋆≈ˆθ⋆:= arg min
θ
E(x,y)∼D"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6208133971291866,"
max
δ∈S L (f (x + δ; θ) , y)
"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6220095693779905,"where D is the distribution on the examples x and the corresponding labels y, f(·; θ) is a model
503"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6232057416267942,"with learnable parameters θ, L is a suitable loss function, and S is the set of allowed constrained
504"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6244019138755981,"perturbations. In the case of ℓp norm-bound perturbations of maximum magnitude ϵ, we can further
505"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6255980861244019,"specify S := {δ | ∥δ∥p ≤ϵ}.
506"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6267942583732058,"The inner optimisation problem is solved, in [42], by Projected Gradient Descent (PGD), an iterative
507"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6279904306220095,"algorithm whose goal is the synthesis of an adversarial perturbation ˆδ = δ(K) after K gradient ascent
508"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6291866028708134,"and projection steps defined as:
509"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6303827751196173,"δ(k+1) ←PS

δ(k) + α sign

∇δ(k)Lce(f(x + δ(k); θ), y)
"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.631578947368421,"where δ(0) is randomly sampled within S, α is a hyperparameter (step size), Lce is the cross-entropy
510"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6327751196172249,"function, and PA is the Euclidean projection operator onto set A, i.e.:
511"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6339712918660287,"PA(a) := arg min
a′∈A
||a −a′||2 ."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6351674641148325,"The outer optimisation is carried out by simply training f(·; θ) on the examples found by PGD against
512"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6363636363636364,"the current model parameters – and their original pre-perturbation labels. The overall procedure just
513"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6375598086124402,"described constitutes PGD adversarial training.
514"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.638755980861244,"B
On the functioning of (conditional) Variational Autoencoders
515"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6399521531100478,"Variational autoencoders (VAEs) [32, 50] learn from data a generative distribution of the form
516"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6411483253588517,"p(x, z) = p(x | z)p(z), where the probability density p(z) represents a prior over latent variable z,
517"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6423444976076556,"and p(x | z) is the likelihood function, which can be used to sample data of interest x, given z.
518"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6435406698564593,"Training is carried out by maximising a variational lower bound, −LVAE(x), on the log-likelihood
519"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6447368421052632,"log p(x) – which is a proxy for the Evidence Lower Bound (ELBO) – i.e.:
520"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.645933014354067,−LVAE(x) := Eq(z | x)[log p(x | z)] −KL(q(z | x)∥p(z))
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6471291866028708,"where q(z | x) ≈p(z | x) is an approximate posterior and KL(·∥·) is the Kullback-Leibler divergence.
521"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6483253588516746,"By parameterising the likelihood with a decoder ANN pθD(x | z; θD) ≈p(x | z), and a possible
522"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6495215311004785,"variational posterior with an encoder ANN qθE(z | x; θE) ≈q(z | x), the parameters θ⋆
D of the
523"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6507177033492823,"generative model that best reproduces the data can be learnt – jointly with θ⋆
E – as:
524"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6519138755980861,"θ⋆
E, θ⋆
D :=
arg min
(θE,θD)
LVAE(x) ="
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.65311004784689,"arg min
(θE,θD)
Ex∼D
h
−Ez∼qθE(z | x;θE) [log pθD(x | z; θD)] + KL(qθE(z | x; θE)∥p(z))
i"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6543062200956937,"where D is the distribution over the (training) examples x.
525"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6555023923444976,"From a practical point of view, optimisation is based on the empirical evaluation of LVAE(x; θ) on
526"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6566985645933014,"mini-batches of data, with the term −Ez∼qθE(z | x;θE) [log pθD(x | z; θD)] replaced by a reconstruction
527"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6578947368421053,"cost
528"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6590909090909091,"LReco(x, x′) ≥0 | LReco(x, x′) = 0 ⇐⇒x = x′ ."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6602870813397129,"The generation of new data according to the fitted model is achieved by sampling from
529"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6614832535885168,"pθ⋆
D(x | z; θ⋆
D)

z∼p(z)"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6626794258373205,"i.e. decoding samples from p(z).
530"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6638755980861244,"The setting is analogous in the case of conditional Variational Autoencoders [56, 64] (see section 3),
531"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6650717703349283,"where conditional sampling is achieved by
532"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.666267942583732,"xcj ∼pθ⋆
D(x | z, c; θ⋆
D)

z∼p(z); c=cj
."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6674641148325359,"C
Justification of Adversarially-balanced batches
533"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6686602870813397,"During the incipient phases of experimentation, preliminary tests were performed with the MNIST
534"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6698564593301436,"[36] and Fashion-MNIST [63] datasets – using a conditional VAE as the purifier, and small FCNs or
535"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6710526315789473,"convolutional ANNs as the classifiers. Adversarial examples were generated against the adversarially
536"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6722488038277512,"pre-trained classifier, and tentatively denoised by the purifier with one sample only. The resulting
537"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6734449760765551,"recovered inputs were classified by the classifier and the overall accuracy was recorded.
538"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6746411483253588,"Importantly, such tests were not meant to assess the end-to-end adversarial robustness of the whole
539"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6758373205741627,"architecture, but only to tune the training protocol of the purifier.
540"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6770334928229665,"Generating adversarial training examples by means of PGD is considered the gold standard [24]
541"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6782296650717703,"and was first attempted as a natural choice to train the purifier. However, in this case, the following
542"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6794258373205742,"phenomena were observed:
543"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.680622009569378,"• Unsatisfactory clean accuracy was reached upon convergence, speculatively a consequence
544"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6818181818181818,"of the VAE having never been trained on clean-to-clean example reconstruction;
545"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6830143540669856,"• Persistent vulnerability to same norm-bound FGSM perturbations was noticed;
546"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6842105263157895,"• Persistent vulnerability to smaller norm-bound FGSM and PGD perturbations was noticed.
547"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6854066985645934,"In an attempt to mitigate such issues, the composition of adversarial examples was adjusted to
548"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6866028708133971,"specifically counteract each of the issues uncovered. The adoption of any smaller subset of attack
549"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.687799043062201,"types or strength, compared to that described in subsection 4.4, resulted in unsatisfactory mitigation.
550"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6889952153110048,"At that point, another problem emerged: if such an adversarial training protocol was carried out in
551"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6901913875598086,"homogeneous batches, each containing the same type and strength of attack (or none at all), the
552"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6913875598086124,"resulting robust accuracy was still partially compromised due to the homogeneous ordering of attack
553"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6925837320574163,"types and strengths across batches.
554"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.69377990430622,"Such observations lead to the final formulation of the training protocol, detailed in subsection 4.4,
555"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6949760765550239,"which mitigates to the best the issues described so far.
556"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6961722488038278,"D
Heuristic justification of the robust aggregation strategy
557"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6973684210526315,"The rationale leading to the choice of the specific robust aggregation strategy described in subsec-
558"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6985645933014354,"tion 4.5 was an attempt to answer the following question: ‘How is it possible to aggregate the results
559"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.6997607655502392,"of an ensemble of classifiers in a way such that it is hard to tilt the balance of the ensemble by
560"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7009569377990431,"attacking only a few of its members?’. The same reasoning can be extended to the reciprocal problem
561"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7021531100478469,"we are trying to solve here, where different input reconstructions obtained from the same potentially
562"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7033492822966507,"perturbed input are classified by the same model (the classifier).
563"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7045454545454546,"Far from providing a satisfactory answer, we can analyse the behaviour of our aggregation strategy
564"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7057416267942583,"as the logit associated with a given model and class varies across its domain, under the effect of
565"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7069377990430622,"adversarial intervention. Comparison with existing (and more popular) probability averaging and
566"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7081339712918661,"logit averaging aggregation strategies should provide a heuristic justification of our choice.
567"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7093301435406698,"We recall our aggregation strategy:
568"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7105263157894737,"Pi := 1 Z N
Y"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7117224880382775,"α=1
eelα
i ."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7129186602870813,"Additionally, we recall logit averaging aggregation
569"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7141148325358851,Pi := 1
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.715311004784689,"Z e
1
N
PN
α=1 lα
i = 1 Z N
Y"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7165071770334929,"α=1
e
1
N lα
i = 1 Z N
Y"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7177033492822966,"α=1
elα
i
! 1 N"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7188995215311005,"and probability averaging aggregation
570"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7200956937799043,"Pi := 1 Z N
X α=1"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7212918660287081,"elα
i
PC
j=1 elα
j = N
X"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.722488038277512,"α=1
elα
i
1
Qα"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7236842105263158,"where Qα = PC
j=1 elα
j .
571"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7248803827751196,"Finally, since lα
i
∈R, ∀lα
i , limx→−∞ex = 0 and e0 = 1, we can observe that elα
i
> 0 and
572"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7260765550239234,"eelα
i > 1, ∀lα
i .
573"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7272727272727273,"Now, we consider a given class i⋆and the classifier prediction on a given input reconstruction α⋆, and
574"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7284688995215312,"study the potential effect of an adversary acting on lα⋆
i⋆. This adversarial intervention can be framed
575"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7296650717703349,"in two complementary scenarios: either the class i⋆is correct and the adversary aims to decrease its
576"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7308612440191388,"membership probability, or the class i⋆is incorrect and the adversary aims to increase its membership
577"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7320574162679426,"probability. In any case, the adversary should comply with the ϵ∞-boundedness of its perturbation on
578"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7332535885167464,"the input.
579"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7344497607655502,"Logit averaging
In the former scenario, the product of elα
i terms can be arbitrarily deflated (up to
580"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7356459330143541,"zero) by lowering the lα⋆
i⋆logit only. In the latter scenario, the logit can be arbitrarily inflated, and
581"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7368421052631579,"such effect is only partially suppressed by normalisation by Z (a sum of 1/N-exponentiated terms).
582"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7380382775119617,"Probability averaging
In the former scenario, although the effect of the deflation of a single logit
583"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7392344497607656,"is bounded by elα⋆
i⋆> 0, two attack strategies are possible: either decreasing the value of lα⋆
i⋆or
584"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7404306220095693,"increasing the value of Qα⋆, giving rise to complex combined effects. In the latter scenario, the
585"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7416267942583732,"reciprocal is possible, i.e. either inflating lα⋆
i⋆or deflating Qα⋆. Normalisation has no effect in both
586"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.742822966507177,"cases.
587"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7440191387559809,"Ours
In the former scenario, the effect of logit deflation on a single product term is bounded by
588"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7452153110047847,"eelα⋆
i⋆> 1, thus exerting only a minimal collateral effect on the product, through a decrease of Z.
589"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7464114832535885,"This effectively prevents aggregation takeover by logit deflation. Similarly to logit averaging, in the
590"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7476076555023924,"latter scenario, the logit can be arbitrarily inflated. However, in this case, the effect of normalisation
591"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7488038277511961,"by Z is much stronger, given its increased magnitude.
592"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.75,"From such a comparison, our aggregation strategy is the only one that strongly prevents adversarial
593"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7511961722488039,"takeover by logit deflation, while still defending well against perturbations targeting logit inflation.
594"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7523923444976076,"E
Architectural details and hyperparameters
595"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7535885167464115,"In the following section, we provide more precise details about the architectures (subsection E.1) and
596"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7547846889952153,"hyperparameters (subsection E.2) used in the experimental phase of our work.
597"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7559808612440191,"E.1
Architectures
598"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.757177033492823,"In the following subsection, we describe the specific structure of the individual parts composing the
599"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7583732057416268,"purifier – in the three scenarios considered. As far as the classifier architectures are concerned, we
600"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7595693779904307,"redirect the reader to the original articles introducing those models (i.e.: [18] for scenarios (a) and
601"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7607655502392344,"(b), [62] for scenario (c)).
602"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7619617224880383,"During training, before being processed by the purifier encoder, input examples are standardised
603"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7631578947368421,"according to the statistics of the respective training dataset.
604"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7643540669856459,"Afterwards, they are fed to the disjoint input encoder (see subsection 4.3), whose architecture is
605"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7655502392344498,"shown in Table 4. The same architecture is used in all scenarios considered.
606"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7667464114832536,"Table 4: Architecture for the disjoint input encoder of the purifier. The same architecture is used in all scenarios
considered. The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The
following abbreviations are used: Conv2D: 2-dimensional convolutional layer; ch_in: number of input channels;
ch_out: number of output channels; ks: kernel size; s: stride; p: padding; b: presence of a learnable bias
term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence of learnable affine transform
coefficients; slope: slope for the activation function in the negative semi-domain."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7679425837320574,Disjoint Input Encoder (all scenarios)
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7691387559808612,"Conv2D(ch_in=3, ch_out=6, ks=3, s=2, p=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=6, ch_out=12, ks=3, s=2, p=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7703349282296651,"The original input is also fed to the classifier. The corresponding internal representation is extracted,
607"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7715311004784688,"preserving its layered structure. In order to improve the scalability of the method, only a subset of
608"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7727272727272727,"classifier layers is used instead of the whole internal representation. Specifically, for each block of
609"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7739234449760766,"the WIDERESNET architecture, only the first layers have been considered; two shortcut layers have
610"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7751196172248804,"also been added for good measure. The exact list of those layers is reported in Table 5.
611"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7763157894736842,"Each extracted layerwise (pre)activation tensor has the shape of a multi-channel image, which is
612"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.777511961722488,"processed – independently for each layer – by a different CNN whose individual architecture is shown
613"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7787081339712919,"in Table 6 (scenarios (a) and (b)) and Table 7 (scenario (c)).
614"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7799043062200957,"The resulting tensors (still having the shape of multi-channel images) are then jointly processed by a
615"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7811004784688995,"fully-connected subnetwork whose architecture is shown in Table 8. The value of fcrepr for the
616"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7822966507177034,"different scenarios considered is shown in Table 13.
617"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7834928229665071,"The compressed input and compressed internal representation so obtained are finally jointly encoded
618"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.784688995215311,"by an additional fully-connected subnetwork whose architecture is shown in Table 9. The output is a
619"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7858851674641149,"tuple of means and standard deviations to be used to sample the stochastic latent code z.
620"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7870813397129187,"The sampler used for the generation of such latent variables z, during the training of the purifier,
621"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7882775119617225,"is a reparameterised [32] Normal sampler z ∼N(µ, σ). During inference, z is sampled by re-
622"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7894736842105263,"parameterisation from the i.i.d Standard Normal distribution z ∼N(0, 1) (i.e. from its original
623"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7906698564593302,"prior).
624"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7918660287081339,"The architectures for the decoder of the purifier are shown in Table 10 (scenarios (a) and (b)) and
625"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7930622009569378,"Table 11 (scenario (c)).
626"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7942583732057417,"E.2
Hyperparameters
627"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7954545454545454,"In the following section, we provide the hyperparameters used for adversarial example generation and
628"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7966507177033493,"optimisation during the training of the purifier, and those related to the purifier model architectures.
629"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7978468899521531,"We also provide the hyperparameters for the PGD+EOT attack, which is used as a complementary
630"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.7990430622009569,"tool for the evaluation of adversarial robustness.
631"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8002392344497608,"Table 5: Classifier model (WIDERESNET-28-10) layer names used as (a subset of) the internal represent-
ation fed to the layerwise convolutional encoder of the purifier. The names reflect those used in the model
implementation."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8014354066985646,All scenarios
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8026315789473685,"layer.0.block.0.conv_0
layer.0.block.0.conv_1
layer.0.block.1.conv_0
layer.0.block.1.conv_1
layer.0.block.2.conv_0
layer.0.block.2.conv_1
layer.0.block.3.conv_0
layer.0.block.3.conv_1
layer.1.block.0.conv_0
layer.1.block.0.conv_1
layer.1.block.0.shortcut
layer.1.block.1.conv_0
layer.1.block.1.conv_1
layer.1.block.2.conv_0
layer.1.block.2.conv_1
layer.1.block.3.conv_0
layer.1.block.3.conv_1
layer.2.block.0.conv_0
layer.2.block.0.conv_1
layer.2.block.0.shortcut
layer.2.block.1.conv_0
layer.2.block.1.conv_1
layer.2.block.2.conv_0
layer.2.block.2.conv_1
layer.2.block.3.conv_0
layer.2.block.3.conv_1"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8038277511961722,"Table 6: Architecture for the layerwise internal representation encoder of the purifier. The architecture shown
in this table is used in scenarios (a) and (b). The architecture is represented layer by layer, from input to output,
in a PyTorch-like syntax. The following abbreviations are used: Conv2D: 2-dimensional convolutional layer;
ch_in: number of input channels; ch_out: number of output channels; ks: kernel size; s: stride; p: padding; b:
presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence
of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain.
The abbreviation [ci] indicates the number of input channels for the (pre)activation tensor of each extracted
layer. The abbreviation ceil indicates the ceiling integer rounding function."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8050239234449761,Layerwise Internal Representation Encoder (scenarios (a) and (b))
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.80622009569378,"Conv2D(ch_in=[ci], ch_out=ceil([ci]/2), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/2), ch_out=ceil([ci]/4), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/4), ch_out=ceil([ci]/8), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8074162679425837,"Table 7: Architecture for the layerwise internal representation encoder of the purifier. The architecture shown
in this table is used in scenario (c). The architecture is represented layer by layer, from input to output, in
a PyTorch-like syntax. The following abbreviations are used: Conv2D: 2-dimensional convolutional layer;
ch_in: number of input channels; ch_out: number of output channels; ks: kernel size; s: stride; p: padding; b:
presence of a learnable bias term; BatchNorm2D: 2-dimensional batch normalisation layer; affine: presence
of learnable affine transform coefficients; slope: slope for the activation function in the negative semi-domain.
The abbreviation [ci] indicates the number of input channels for the (pre)activation tensor of each extracted
layer. The abbreviation ceil indicates the ceiling integer rounding function."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8086124401913876,Layerwise Internal Representation Encoder (scenario (c))
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8098086124401914,"Conv2D(ch_in=[ci], ch_out=ceil([ci]/2), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/2), ch_out=ceil([ci]/4), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/4), ch_out=ceil([ci]/8), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
Conv2D(ch_in=ceil([ci]/8), ch_out=ceil([ci]/16), ks=3, s=1, p=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8110047846889952,"Table 8: Architecture for the fully-connected representation encoder of the purifier. The architecture shown
in this table is used in all scenarios considered. The architecture is represented layer by layer, from input to
output, in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating
its input features; flatten_features: whether the input features are to be flattened before concatenation;
feats_in, feats_out: number of input and output features of a linear layer; b: presence of a learnable bias
term; BatchNorm1D: 1-dimensional batch normalisation layer; affine: presence of learnable affine transform
coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [computed]
indicates that the number of features is computed according to the shape of the concatenated input tensors. The
value of fcrepr for the different scenarios considered is shown in Table 13."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.812200956937799,Fully-Connected Representation Encoder (all scenarios)
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8133971291866029,"Concatenate(flatten_features=True)
Linear(feats_in=[computed], feats_out=fcrepr, b=False)
BatchNorm1D(affine=True)
LeakyReLU(slope=0.2)"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8145933014354066,"Table 9: Architecture for the fully-connected joint encoder of the purifier. The architecture shown in this
table is used in all scenarios considered. The architecture is represented layer by layer, from input to output,
in a PyTorch-like syntax. The following abbreviations are used: Concatenate: layer concatenating its input
features; flatten_features: whether the input features are to be flattened before concatenation; feats_in,
feats_out: number of input and output features of a linear layer; b: presence of a learnable bias term;
BatchNorm1D: 1-dimensional batch normalisation layer; affine: presence of learnable affine transform
coefficients; slope: slope for the activation function in the negative semi-domain. The abbreviation [computed]
indicates that the number of features is computed according to the shape of the concatenated input tensors. The
value of fjoint for the different scenarios considered is shown in Table 13. The last layer of the network
returns a tuple of 2 tensors, each independently processed – from the output of the previous layer – by the two
comma-separated sub-layers."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8157894736842105,Fully-Connected Joint Encoder (all scenarios)
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8169856459330144,"Concatenate(flatten_features=True)
Linear(feats_in=[computed], feats_out=fjoint, b=False)
BatchNorm1D(affine=True)
LeakyReLU(slope=0.2)
( Linear(feats_in=fjoint, feats_out=fjoint, b=True),
Linear(feats_in=fjoint, feats_out=fjoint, b=True) )"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8181818181818182,"Table 10: Architecture for the decoder of the purifier. The architecture shown in this table is used in scenarios
(a) and (b). The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The
following abbreviations are used: Concatenate: layer concatenating its input features; flatten_features:
whether the input features are to be flattened before concatenation; feats_in, feats_out: number of input
and output features of a linear layer; b: presence of a learnable bias term; ConvTranspose2D: 2-dimensional
transposed convolutional layer; ch_in: number of input channels; ch_out: number of output channels; ks:
kernel size; s: stride; p: padding; op: PyTorch parameter ‘output padding’, used to disambiguate the number
of spatial dimensions of the resulting output; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional
batch normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the
activation function in the negative semi-domain. The values of fjoint and fcrepr for the different scenarios
considered are shown in Table 13."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.819377990430622,Decoder (scenarios (a) and (b))
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8205741626794258,"Concatenate(flatten_features=True)
Linear(feats_in=[fjoint+fcrepr], feats_out=2304, b=True)
LeakyReLU(slope=0.2)
Unflatten(256, 3, 3)
ConvTranspose2D(ch_in=256, ch_out=256, ks=3, s=2, p=1, op=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=256, ch_out=128, ks=3, s=2, p=1, op=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=128, ch_out=64, ks=3, s=2, p=1, op=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=64, ch_out=32, ks=3, s=2, p=1, op=0, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=32, ch_out=3, ks=2, s=1, p=1, op=0, b=True)
Sigmoid()"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8217703349282297,"Table 11: Architecture for the decoder of the purifier. The architecture shown in this table is used in scenario
(c). The architecture is represented layer by layer, from input to output, in a PyTorch-like syntax. The following
abbreviations are used: Concatenate: layer concatenating its input features; flatten_features: whether
the input features are to be flattened before concatenation; feats_in, feats_out: number of input and output
features of a linear layer; b: presence of a learnable bias term; ConvTranspose2D: 2-dimensional transposed
convolutional layer; ch_in: number of input channels; ch_out: number of output channels; ks: kernel size;
s: stride; p: padding; op: PyTorch parameter ‘output padding’, used to disambiguate the number of spatial
dimensions of the resulting output; b: presence of a learnable bias term; BatchNorm2D: 2-dimensional batch
normalisation layer; affine: presence of learnable affine transform coefficients; slope: slope for the activation
function in the negative semi-domain. The values of fjoint and fcrepr for the different scenarios considered
are shown in Table 13."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8229665071770335,Decoder (scenario (c))
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8241626794258373,"Concatenate(flatten_features=True)
Linear(feats_in=[fjoint+fcrepr], feats_out=4096, b=True)
LeakyReLU(slope=0.2)
Unflatten(256, 4, 4)
ConvTranspose2D(ch_in=256, ch_out=256, ks=3, s=2, p=1, op=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=256, ch_out=128, ks=3, s=2, p=1, op=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=128, ch_out=64, ks=3, s=2, p=1, op=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=64, ch_out=32, ks=3, s=2, p=1, op=1, b=False)
BatchNorm2D(affine=True)
LeakyReLU(slope=0.2)
ConvTranspose2D(ch_in=32, ch_out=3, ks=3, s=1, p=1, op=0, b=True)
Sigmoid()"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8253588516746412,"Attacks
The hyperparameters used for the adversarial attacks described in subsection 4.4 are shown
632"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8265550239234449,"in Table 12. The value of ϵ∞is fixed to ϵ∞= 8/255. With the only exception of ϵ∞, AUTOATTACK
633"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8277511961722488,"is to be considered a hyperparameter-free adversarial example generator.
634"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8289473684210527,"Table 12: Hyperparameters for the attacks used for training and testing the purifier The FGSM and PDG attacks
refer to the training phase (see subsection 4.4), whereas the PGD+EOT attack [38] refers to the robustness
assessment pipeline. The entry CCE denotes the Categorical CrossEntropy loss function. The ℓ∞threat model is
assumed, and all inputs are linearly rescaled within [0.0, 1.0] before the attack."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8301435406698564,"FGSM
PGD
PGD+EOT"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8313397129186603,"Input clipping
[0.0, 1.0]
[0.0, 1.0]
[0.0, 1.0]
# of steps
1
40
200
Step size
ϵ∞
0.01
0.007
Loss function
CCE
CCE
CCE
# of EoT iterations
1
1
20
Optimiser
SGD
SGD"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8325358851674641,"Architectures
Table 13 contains the hyperparameters that define the model architectures used as
635"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.833732057416268,"part of the purifier, in the different scenarios considered.
636"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8349282296650717,"Table 13: Scenario-specific architectural hyperparameters for the purifier, as referred to in Table 8, Table 9,
Table 10, and Table 11."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8361244019138756,"Scenario (a)
Scenario (b)
Scenario (c)"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8373205741626795,"fcrepr
512
512
768
fjoint
128
128
192"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8385167464114832,"Training
Table 14 collects the hyperparameters governing the training of the purifier in the different
637"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8397129186602871,"scenarios considered.
638"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8409090909090909,"Table 14: Hyperparameters used for training the purifier, grouped by scenario. The entry CCE denotes the
Categorical CrossEntropy loss function. The LR scheduler is stepped after each epoch."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8421052631578947,"All scenarios
Sc. (a)
Sc. (b)
Sc. (c)"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8433014354066986,"Optimiser
RADAM+LOOKAHEAD
RADAM β1
0.9
RADAM β2
0.999
RADAM ϵ
10−8"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8444976076555024,"RADAM Weight Decay
None
LOOKAHEAD averaging decay
0.8
LOOKAHEAD steps
6
Initial LR
5 × 10−9"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8456937799043063,"Loss function
CCE
Sampled reconstructions per input
8"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.84688995215311,"Epochs
200
200
250
LR warm-up epochs
25
25
31
LR plateau epochs
25
25
31
LR annealing epochs
150
250
188
Plateau LR
0.064
0.064
0.0128
Final LR
4.346 × 10−4
4.346 × 10−4
1.378 × 10−4"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8480861244019139,"β increase initial epoch
25
25
32
β increase final epoch
34
34
43
Batch size
5120
2560
1024
Adversarial batch fraction
0.5
0.15
0.01"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8492822966507177,"F
Additional tables
639"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8504784688995215,"The following section contains additional tabular data that may be of interest to the reader.
640"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8516746411483254,"Table 15 reports the respective clean accuracies for the best models available in terms of AUTOAT-
641"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8528708133971292,"TACK robust accuracy, in scenarios (a) and (b). Models are further divided in AT-based and
642"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.854066985645933,"purification-based, so as to match the corresponding columns for robust accuracy shown in Table 2.
643"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8552631578947368,"The best AT-based model for CIFAR-10 is taken from [18], whereas that for CIFAR-100 from [62].
644"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8564593301435407,"Both best purification-based models are taken from [40].
645"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8576555023923444,"The clean and robust accuracies for the best AT-based model on TINYIMAGENET-200 (scenario (c))
646"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8588516746411483,"are already part of Table 2 and we redirect the reader there for such information. We are not aware of
647"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8600478468899522,"any published state-of-the-art adversarial purification-based model for TINYIMAGENET-200.
648"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.861244019138756,"Table 15: Clean accuracy for the best models (by robust accuracy) on the datasets considered in scenarios
(a) and (b), mentioned in Table 2. The following abbreviations are used: Scen: scenario considered; Best
AT/Cl: clean accuracy for the most robust model (by the means of AUTOATTACK) on the respective dataset,
obtained by adversarial training alone; Best P/Cl: clean accuracy for the most robust model (by the means of
AUTOATTACK) on the respective dataset, obtained by adversarial purification alone."
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8624401913875598,"Scen.
Dataset
Best AT/Cl
Best P/Cl"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8636363636363636,"(a)
CIFAR-10
0.9323
0.9082"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8648325358851675,"(b)
CIFAR-100
0.7522
0.6973"
TH EUROPEAN CONFERENCE ON MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES - VOLUME,0.8660287081339713,"NeurIPS Paper Checklist
649"
CLAIMS,0.8672248803827751,"1. Claims
650"
CLAIMS,0.868421052631579,"Question: Do the main claims made in the abstract and introduction accurately reflect the
651"
CLAIMS,0.8696172248803827,"paper’s contributions and scope?
652"
CLAIMS,0.8708133971291866,"Answer: [Yes]
653"
CLAIMS,0.8720095693779905,"Justification: Claims made in the abstract and introduction of the paper accurately reflect
654"
CLAIMS,0.8732057416267942,"its contributions, and those are directly corroborated by experimental analysis. Results and
655"
CLAIMS,0.8744019138755981,"their discussion is available in subsection 5.2 and subsection 5.3.
656"
LIMITATIONS,0.8755980861244019,"2. Limitations
657"
LIMITATIONS,0.8767942583732058,"Question: Does the paper discuss the limitations of the work performed by the authors?
658"
LIMITATIONS,0.8779904306220095,"Answer: [Yes]
659"
LIMITATIONS,0.8791866028708134,"Justification: subsection 5.2 and subsection 5.3 also contain the discussion of potential
660"
LIMITATIONS,0.8803827751196173,"limitations of the method, and open problems it introduces.
661"
THEORY ASSUMPTIONS AND PROOFS,0.881578947368421,"3. Theory Assumptions and Proofs
662"
THEORY ASSUMPTIONS AND PROOFS,0.8827751196172249,"Question: For each theoretical result, does the paper provide the full set of assumptions and
663"
THEORY ASSUMPTIONS AND PROOFS,0.8839712918660287,"a complete (and correct) proof?
664"
THEORY ASSUMPTIONS AND PROOFS,0.8851674641148325,"Answer: [NA]
665"
THEORY ASSUMPTIONS AND PROOFS,0.8863636363636364,"Justification: The paper does not contribute novel theoretical results. Assumptions anyway
666"
THEORY ASSUMPTIONS AND PROOFS,0.8875598086124402,"related to the contribution are clearly stated throughout the paper.
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.888755980861244,"4. Experimental Result Reproducibility
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8899521531100478,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8911483253588517,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8923444976076556,"of the paper (regardless of whether the code and data are provided or not)?
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8935406698564593,"Answer: [Yes]
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8947368421052632,"Justification: Training and evaluation details required to reproduce the experimental results
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.895933014354067,"of the paper are reported in section 5 and Appendix E. Code and data for the reproduction
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8971291866028708,"of all experiments are additionally released as part of Supplementary Material.
675"
OPEN ACCESS TO DATA AND CODE,0.8983253588516746,"5. Open access to data and code
676"
OPEN ACCESS TO DATA AND CODE,0.8995215311004785,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
677"
OPEN ACCESS TO DATA AND CODE,0.9007177033492823,"tions to faithfully reproduce the main experimental results, as described in supplemental
678"
OPEN ACCESS TO DATA AND CODE,0.9019138755980861,"material?
679"
OPEN ACCESS TO DATA AND CODE,0.90311004784689,"Answer: [Yes]
680"
OPEN ACCESS TO DATA AND CODE,0.9043062200956937,"Justification: Code and data for the reproduction of all experiments are released to reviewers
681"
OPEN ACCESS TO DATA AND CODE,0.9055023923444976,"as part of Supplementary Material. The public version of the paper will include instructions
682"
OPEN ACCESS TO DATA AND CODE,0.9066985645933014,"to obtain the same material from a dedicated publicly accessible source.
683"
OPEN ACCESS TO DATA AND CODE,0.9078947368421053,"6. Experimental Setting/Details
684"
OPEN ACCESS TO DATA AND CODE,0.9090909090909091,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
685"
OPEN ACCESS TO DATA AND CODE,0.9102870813397129,"parameters, how they were chosen, type of optimiser, etc.) necessary to understand the
686"
OPEN ACCESS TO DATA AND CODE,0.9114832535885168,"results?
687"
OPEN ACCESS TO DATA AND CODE,0.9126794258373205,"Answer: [Yes]
688"
OPEN ACCESS TO DATA AND CODE,0.9138755980861244,"Justification: Training and evaluation details, including hyperparameters, required to re-
689"
OPEN ACCESS TO DATA AND CODE,0.9150717703349283,"produce the experimental results of the paper are reported in section 5 and Appendix E.
690"
OPEN ACCESS TO DATA AND CODE,0.916267942583732,"Code and data for the reproduction of all experiments are additionally released as part of
691"
OPEN ACCESS TO DATA AND CODE,0.9174641148325359,"Supplementary Material.
692"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9186602870813397,"7. Experiment Statistical Significance
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9198564593301436,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9210526315789473,"information about the statistical significance of the experiments?
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9222488038277512,"Answer: [No]
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9234449760765551,"Justification: As doing adversarial training with 40-steps PGD is roughly 40 times more
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9246411483253588,"computationally demanding than nominal training, unfortunately, we are unable to show
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9258373205741627,"error bars or otherwise quantify statistical errors. In any case, the improvement induced
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9270334928229665,"by our method w.r.t. their state-of-the-art counterparts is well clear of the threshold for
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9282296650717703,"statistical significance.
701"
EXPERIMENTS COMPUTE RESOURCES,0.9294258373205742,"8. Experiments Compute Resources
702"
EXPERIMENTS COMPUTE RESOURCES,0.930622009569378,"Question: For each experiment, does the paper provide sufficient information on the com-
703"
EXPERIMENTS COMPUTE RESOURCES,0.9318181818181818,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
704"
EXPERIMENTS COMPUTE RESOURCES,0.9330143540669856,"the experiments?
705"
EXPERIMENTS COMPUTE RESOURCES,0.9342105263157895,"Answer: [Yes]
706"
EXPERIMENTS COMPUTE RESOURCES,0.9354066985645934,"Justification: Information about computational resources and required time is contained in
707"
EXPERIMENTS COMPUTE RESOURCES,0.9366028708133971,"subsection 5.1 as well as in the supplementary materials.
708"
CODE OF ETHICS,0.937799043062201,"9. Code Of Ethics
709"
CODE OF ETHICS,0.9389952153110048,"Question: Does the research conducted in the paper conform, in every respect, with the
710"
CODE OF ETHICS,0.9401913875598086,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
711"
CODE OF ETHICS,0.9413875598086124,"Answer: [Yes]
712"
CODE OF ETHICS,0.9425837320574163,"Justification: The research conducted in the paper does conform, in every respect, with the
713"
CODE OF ETHICS,0.94377990430622,"NeurIPS Code of Ethics.
714"
BROADER IMPACTS,0.9449760765550239,"10. Broader Impacts
715"
BROADER IMPACTS,0.9461722488038278,"Question: Does the paper discuss both potential positive societal impacts and negative
716"
BROADER IMPACTS,0.9473684210526315,"societal impacts of the work performed?
717"
BROADER IMPACTS,0.9485645933014354,"Answer: [Yes]
718"
BROADER IMPACTS,0.9497607655502392,"Justification: The paper proposes a novel technique to mitigate the problem of adversarial
719"
BROADER IMPACTS,0.9509569377990431,"vulnerability of high-dimensional classifiers. Such vulnerability may pose potential societal
720"
BROADER IMPACTS,0.9521531100478469,"impacts, as discussed in section 1.
721"
SAFEGUARDS,0.9533492822966507,"11. Safeguards
722"
SAFEGUARDS,0.9545454545454546,"Question: Does the paper describe safeguards that have been put in place for responsible
723"
SAFEGUARDS,0.9557416267942583,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
724"
SAFEGUARDS,0.9569377990430622,"image generators, or scraped datasets)?
725"
SAFEGUARDS,0.9581339712918661,"Answer: [NA]
726"
SAFEGUARDS,0.9593301435406698,"Justification: We do not plan to release models or data with a high risk for misuse. Models
727"
SAFEGUARDS,0.9605263157894737,"to be released do not reasonably carry risk for misuse.
728"
LICENSES FOR EXISTING ASSETS,0.9617224880382775,"12. Licenses for existing assets
729"
LICENSES FOR EXISTING ASSETS,0.9629186602870813,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
730"
LICENSES FOR EXISTING ASSETS,0.9641148325358851,"the paper, properly credited and are the license and terms of use explicitly mentioned and
731"
LICENSES FOR EXISTING ASSETS,0.965311004784689,"properly respected?
732"
LICENSES FOR EXISTING ASSETS,0.9665071770334929,"Answer: [Yes]
733"
LICENSES FOR EXISTING ASSETS,0.9677033492822966,"Justification: The creators and/or owners of assets used in the paper are either credited by
734"
LICENSES FOR EXISTING ASSETS,0.9688995215311005,"reference to their original research work, or directly with a link to the preferred landing page
735"
LICENSES FOR EXISTING ASSETS,0.9700956937799043,"for such assets. The use of licensed material is compliant with the respective licenses.
736"
NEW ASSETS,0.9712918660287081,"13. New Assets
737"
NEW ASSETS,0.972488038277512,"Question: Are new assets introduced in the paper well documented and is the documentation
738"
NEW ASSETS,0.9736842105263158,"provided alongside the assets?
739"
NEW ASSETS,0.9748803827751196,"Answer: [Yes]
740"
NEW ASSETS,0.9760765550239234,"Justification: Trained model weights are provided alongside the paper, and their details are
741"
NEW ASSETS,0.9772727272727273,"provided as part of supplementary materials. In case of release to the public, such details
742"
NEW ASSETS,0.9784688995215312,"will be provided contextually to the models.
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9796650717703349,"14. Crowdsourcing and Research with Human Subjects
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808612440191388,"Question: For crowdsourcing experiments and research with human subjects, does the paper
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820574162679426,"include the full text of instructions given to participants and screenshots, if applicable, as
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832535885167464,"well as details about compensation (if any)?
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844497607655502,"Answer: [NA]
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856459330143541,"Justification: No crowdsourcing or research with human subjects has been performed as part
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868421052631579,"of the work of, or leading to, this paper.
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880382775119617,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892344497607656,"Subjects
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904306220095693,"Question: Does the paper describe potential risks incurred by study participants, whether
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916267942583732,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992822966507177,"approvals (or an equivalent approval/review based on the requirements of your country or
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940191387559809,"institution) were obtained?
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952153110047847,"Answer: [NA]
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964114832535885,"Justification: No crowdsourcing or research with human subjects has been performed as part
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976076555023924,"of the work of or leading to this paper - including that potentially requiring IRB approval or
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988038277511961,"equivalent authorisation.
760"
