Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005235602094240838,"We study the problem of transfer learning and fine-tuning in linear models for
both regression and binary classification. In particular, we consider the use of
stochastic gradient descent (SGD) on a linear model initialized with pretrained
weights and using a small training data set from the target distribution. In the
asymptotic regime of large models, we provide an exact and rigorous analysis and
relate the generalization errors (in regression) and classification errors (in binary
classification) for the pretrained and fine-tuned models. In particular, we give
conditions under which the fine-tuned model outperforms the pretrained one. An
important aspect of our work is that all the results are ""universal"", in the sense that
they depend only on the first and second order statistics of the target distribution.
They thus extend well beyond the standard Gaussian assumptions commonly made
in the literature. Furthermore, our universality results extend beyond standard SGD
training to the test error of a classification task trained using ridge regression."
INTRODUCTION,0.0010471204188481676,"1
Introduction"
INTRODUCTION,0.0015706806282722514,"Deep neural networks have revolutionized the way data processing and statistical inference are
conducted. Despite their ground-breaking performance, these models often require a plethora of
training samples, which can make the process of data acquisition expensive. Moreover, with the
advent of increasingly complex deep networks and especially Large Language Models (LLMs), the
process of training from scratch has also become prohibitively expensive. To alleviate the scarcity of
prepared data and the high training costs, various strategies have been proposed. One such method is
fine-tuning in which a network previously trained on a source dataset/task different from the target
dataset/task is leveraged as the initialization point for training on the target dataset/task. In many
practical applications, especially for networks containing billions of parameters, only a subset of
weights are updated to adapt the model to the new task. A particularly attractive method involves
fine-tuning only the last layer of the network. A foundational question would be: How effective is
this procedure? Are there fundamental limits to how much one can achieve by utilization of a model
pretrained on a different distribution?"
INTRODUCTION,0.0020942408376963353,∗Equal Contribution
INTRODUCTION,0.002617801047120419,"In this work, we would like to investigate this problem rigorously through the lens of linear regression
and binary classification in the overparametrized regime, where the number of weights/parameters
exceeds the number of data points. To do so, we analyze the performance of regressors/classifiers
obtained through performing SGD initialized at a weight w0 ∈R𝑑acquired through training on
the source domain. It was shown in Gunasekar et al. [2018] and Azizan and Hassibi [2018] that
gradient descent (GD) and stochastic gradient descent(SGD) iterations converge to the optimal
solution w ∈R𝑑of the following optimization problem:"
INTRODUCTION,0.0031413612565445027,"min
w ∥w −w0∥2
2
(1)"
INTRODUCTION,0.0036649214659685864,"𝑠.𝑡
y = Xw"
INTRODUCTION,0.004188481675392671,"where w0 ∈R𝑑is the initialization point for SGD, X ∈R𝑛×𝑑is the design/data matrix which is
comprised of independent rows reflecting the fact that datapoints (x𝑖, 𝑦𝑖) are sampled independently,
and y ∈R𝑛is the vector of labels. This property of SGD is known as ""Implicit Regularization""
and will serve as the basis for our analysis as the pretraining step is captured by the vector w0.
Understanding this ""interpolating regime"" is key to the theoretical analysis of machine learning
models as most of the deep neural networks, on account of their highly overparametrized nature,
operate in a setting where they are able to attain negligible training error. It is noteworthy that,
even for linear models, characterizing the exact performance of fine-tuning approach has been
somewhat limited and is inhibited furthermore by the Gaussianity assumption on X. One of our
main contributions is to overcome this limitation. In fact, to showcase the ubiquity of our results, we
establish a general universality theorem that applies to a large class of data distributions and extends
beyond the context of Transfer Learning. To go into more detail, we say that Gaussian universality
holds for a data distribution P and a training algorithm 𝑇if the test error obtained by training on
data sampled from P using 𝑇is the same as the test error obtained by training on data sampled
from the Gaussian distribution N (µP, 𝚺P) using 𝑇, where µP and 𝚺P stand for the mean and the
covariance matrix of P respectively. In the context of binary classification, we establish universality
of the classification error with respect to the distribution of each class. In light of the the latter result,
the problem reduces to analyzing the case of the Gaussian design matrices. We allow classes with
different non-scalar covariance matrices 𝚺1 and 𝚺2 and build on the results of Akhtiamov et al. [2024]
to address the problem in this specific scenario."
RELATED WORKS,0.004712041884816754,"1.1
Related Works"
RELATED WORKS,0.005235602094240838,"Transfer Learning has been an active topic of research at least since the 1970’s Bozinovski [2020]. It
is mainly applied in the situations where obtaining data from the true distribution, which we will refer
as the target distribution, is costly but there is a cheap way to access data from a source distribution,
which bears resemblance to the target distribution. The two most popular approaches to transfer
learning consist of instance-based transfer learning and fine-tuning."
RELATED WORKS,0.005759162303664921,"Instance-based transfer learning incorporates the source dataset, along with the target dataset, and
trains the model on this amalgamated dataset. The key insight is that, provided that the source
distribution is close enough to the target distibution and a suitable training scheme is chosen, the final
performance enjoys an improvement. We refer the reader to the landmark workDai et al. [2007] as
well as to the comprehensive overviews covering the empirical advances in this area Tan et al. [2018],
Zhuang et al. [2020]. For theoretical analysis, we would like to emphasize a recent work Jain et al.
[2024] that characterized the scaling laws for ridge regression over the union of the source and target
data in the high dimensional regime."
RELATED WORKS,0.0062827225130890054,"The present manuscript focuses on fine-tuning in which a model is first pretrained on the source
distribution and then fine-tuned using the target data. Dar et al. [2021] analyzes transfer learning for
linear regression assuming Gaussianity of the data. The results obtained in this paper, in particular,
imply that the generalization error obtained in their work is universal, in the sense that they can be
immediately extended to a much broader set of target distributions, which we elaborate on in much
greater detail in the main body of the paper."
RELATED WORKS,0.006806282722513089,"Gerace et al. [2022] consider the problem of binary classification via a two-layer neural network in
a synthetic setting. The source data and the source labels are sampled according to x = 𝑅𝑒𝑙𝑢(Ga)
and 𝑦= 𝑆𝑖𝑔𝑛(a𝑇z) respectively where G, a and z are i.i.d. Gaussian. The target data and labels are
generated by perturbing G and a and then applying the same rules as for the source data. Gerace
et al. [2022] trained the network on the source dataset and then proceeded by only fine-tuning the"
RELATED WORKS,0.007329842931937173,"last layer on the target domain using the cross-entropy loss with an ℓ2 regularizer. The analysis was
conducted using the Replica method. The authors of Gerace et al. [2022] compared their results with
an equivalent random feature model and observed empirically a certain kind of Gaussian universality
for real-world datasets, such as MNIST."
RELATED WORKS,0.007853403141361256,"Extension of results obtained under Gaussianity assumptions to non-Gaussian distributions remains a
pertinent research direction in which the idea of Gaussian universality plays a central role. Montanari
and Nguyen [2017] proved the universality of the generalization error for the elastic net, assuming the
design matrix A is i.i.d subgaussian. Panahi and Hassibi [2017] generalized the result to the problem
of regularized regression with a quadratic loss and a convex separable regularizer which is either
𝑓(·) = ∥· ∥1 (LASSO) or strongly convex. Han and Shen [2023] generalized Panahi and Hassibi
[2017]’s results to non-separable Lipschitz test functions and provided non-asymptotic bounds for
the concentration of solutions. In the random geometry literature, Oymak and Tropp [2018] showed
universality of the embedding dimension of randomized dimension reduction linear maps with i.i.d
entries satisfying certain moment conditions. Abbasi et al. [2019] proved universality of the recovery
threshold for minimizing strongly convex functions under linear measurements constraint, assuming
the rows are iid and the norm of the mean is asymptotically negligible compared to the noise. Lahiry
and Sur [2023], proved the universality of generalization error for ridge regression and LASSO when
the rows are distributed iid with a specific block structure dependence per row, AD where A has zero
mean iid random subgaussian vectors per row and D being diagonal."
RELATED WORKS,0.008376963350785341,"Leveraging universality is not limited to the signal recovery literature. As an example, Hu and Lu
[2022], Bosch et al. [2023], Schröder et al. [2023], have analyzed the performance of the random
feature models by replacing nonlinear functions of Gaussians with the corresponding Gaussian
distribution having the same mean and covariance in single-layer and multiple-layer scenarios,
respectively, through a universality argument, referred to as the Gaussian equivalence principle. In
a more general setting, under subgaussianity assumption on the data, Montanari and Saeed [2022]
proved the universality of the training error for general loss and regularizer and test error when the
regularizer is strongly convex and the loss is convex. Hastie et al. [2022] proved universality for the
the minimal ℓ2-norm linear interpolators for the data generated via a very specific rule x = 𝜎(W𝚺
1
2 z),
where z is i.i.d zero mean and satisfies several other technical properties and 𝚺is a deterministic PSD
matrix. Dandi et al. [2024] considered the universality of mixture distributions in a similar setting to
Montanari and Saeed [2022] and proved the universality of the free energy of the test error."
OUTLINE OF THE PAPER,0.008900523560209424,"1.2
Outline of the Paper"
OUTLINE OF THE PAPER,0.009424083769633508,"In Section 2.1, we formally define the optimization problem for which we aim to establish universality.
Section 2.2 outlines our primary contributions. In Section 3, we present our main universality result,
accompanied by several insightful remarks. Section 4 focuses on our findings related to fine-tuning
in the contexts of linear regression and binary classification. In Section 5, we validate our theoretical
results through empirical experiments. Finally, we conclude with a summary and discussion in
Section 6."
PROBLEM FORMULATION,0.009947643979057591,"2
Problem Formulation"
THE SETTING,0.010471204188481676,"2.1
The Setting"
THE SETTING,0.01099476439790576,"We use bold letters for vectors and matrices. We denote the 𝑖’th largest singular value of matrix A by
𝑠𝑖(A). We consider the proportional regime where 𝑑= Θ(𝑛) and 𝑑"
THE SETTING,0.011518324607329843,"𝑛→𝜅> 1. We refer to Section
A.1 for the other notations and definitions necessary for the rest of this exposition."
THE SETTING,0.012041884816753926,"Given a training dataset {(x𝑖, 𝑦𝑖)}𝑛
𝑖=1 and a model with a fixed architecture, the conventional approach
of learning the weights for the model consists of choosing a loss function L and finding 𝑤minimizing
1
𝑛
Í𝑛
𝑖=1 L(x𝑖, w𝑖, 𝑦𝑖). In this exposition we focus on linear models and loss functions of the form
L(x𝑖, w𝑖, 𝑦𝑖) = ℓ(𝑦𝑖−x𝑇
𝑖w𝑖), where ℓis differentiable and ℓ(0) = 0. Azizan and Hassibi [2018]
characterized behavior of a broad family of optimizers, called stochastic mirror descent (SMD)
algorithms, for linear models in the over-parametrized regime. For a strictly convex function
𝑔: R𝑑→R, the update rule of the SMD with a mirror 𝑔and a learning rate 𝜂> 0 is defined as"
THE SETTING,0.012565445026178011,"∇𝑔(w𝑡) = ∇𝑔(w𝑡−1) −𝜂∇L𝑡(w𝑡−1),
𝑡≥1,
(2)"
THE SETTING,0.013089005235602094,"where 𝜂> 0 is the learning rate and L𝑡is the loss function L evaluated at a point chosen at random
in the dataset corresponding to the 𝑡’th iteration. Due to strict convexity, ∇𝑔(·) defines an invertible
transformation, which is why (2) is indeed a well-defined update rule. Note also that this includes
SGD as a special case, which corresponds to taking 𝑔(·) = ∥· ∥2
2. Azizan and Hassibi [2018] show
that applying SMD initialized at w0 to minimize 1"
THE SETTING,0.013612565445026177,"𝑛
Í𝑛
𝑖=1 ℓ(𝑦𝑖−x𝑇
𝑖w) yields a weight vector defined
by the following optimization problem:"
THE SETTING,0.014136125654450262,"min
w 𝐷𝑔(w, w0)"
THE SETTING,0.014659685863874346,"𝑠.𝑡
𝑦𝑖= x𝑇
𝑖w,
1 ≤𝑖≤𝑛"
THE SETTING,0.015183246073298429,"We construct the random matrix A ∈R𝑛×𝑑by setting its rows equal to x𝑖. Hence, stating in a more
general fashion, we are interested in the analysis of the following optimization problem, with the
main case of interest being when 𝑓is a quadratic:"
THE SETTING,0.015706806282722512,"min
w
𝑓(w)"
THE SETTING,0.016230366492146597,"𝑠.𝑡
y = Aw"
THE SETTING,0.016753926701570682,"As the objective is comprised of minimizing a strongly convex function 𝑓over a closed convex set, it
has a unique minimizer. By using a Lagrange multiplier 𝜆∈R, this convex optimization problem can
be written in the unconstrained form:"
THE SETTING,0.017277486910994764,"Φ(A) := sup
𝜆>0
min
𝑤
𝜆
2 ∥Aw −y∥2
2 + 𝑓(w)
(3)"
THE SETTING,0.01780104712041885,"Considering the objective without the sup𝜆>0 yields the following regularized linear regression
problem for which we will also establish a universality theorem:"
THE SETTING,0.01832460732984293,"Φ𝜆(A) := min
𝑤
𝜆
2 ∥Aw −y∥2
2 + 𝑓(w)
(4)"
THE SETTING,0.018848167539267015,"Note that (4) captures the case of explicit regularization, which is of highest importance whenever
there is a high amount of noise or label corruption present. Furthermore, by deliberately choosing
the regularizer 𝑓, it is possible to obtain solutions that exhibit certain behavior, viz being sparse or
compressed. In order to tackle optimizations such as 3, 4, we prove their equivalence to a problem
with a suitable Gaussian design G which is described in Definition 4 in Section A.1. Our approach
for proving universality will be the Lindeberg approach Lindeberg [1922] Chatterjee [2006]."
OUR CONTRIBUTIONS,0.0193717277486911,"2.2
Our Contributions"
TRANSFER LEARNING,0.019895287958115182,"2.2.1
Transfer Learning"
TRANSFER LEARNING,0.020418848167539267,"Linear Regression We extend the results from Dar et al. [2021] by providing precise expressions for
the generalization error in linear regression tasks. In addition, we demonstrate that the test error is
always lower-bounded by a quantity that is attained only when the covariance matrix of the data is
scalar. Furthermore, we identify specific conditions under which fine-tuning is successful."
TRANSFER LEARNING,0.020942408376963352,"Binary Classification We present the first precise characterization of the classification error for
linear models trained using stochastic gradient descent (SGD) on data drawn from a general mixture
distribution with arbitrary covariance matrices. Moreover, we delineate the regimes in which fine-
tuning proves effective."
UNIVERSALITY,0.021465968586387434,"2.2.2
Universality"
UNIVERSALITY,0.02198952879581152,"Proof of universality for implicit regularization. In the context of SGD and, more generally,
SMD (see 2) with a mirror 𝑓satisfying the assumptions of Theorem 1, we prove universality of the
generalization error as well as of the value of the corresponding implicit regularization objective for a
wide range of data distributions characterized by Assumptions 1. Note that this cannot be reduced to
any known results on universality of constrained objectives as the latter assume that the constraints
are deterministic, i.e the optimization variables belong to some determinitic set C chatacterized by
the constraints. While to analyze SMD one has to deal with constraints of the form Aw = y which
represents a random polytope. To the best of our knowledge, the only other paper dealing with
universality in the context of implicit regularization is Hastie et al. [2022]. They study minimal"
UNIVERSALITY,0.022513089005235604,"ℓ2-norm linear interpolators, but they work with a very specific (random feature) model for data
distributions defined by x = 𝜎(W𝚺
1
2 z), where z is i.i.d zero mean and W is i.i.d. Gaussian. Our
paper generalizes theirs in two non-trivial ways, as it allows for arbitrary smooth convex strongly
convex mirrors 𝑓(w) as well as more general data distributions."
UNIVERSALITY,0.023036649214659685,"Relaxing assumptions for explicit regularization. To the best of the authors’ knowledge, all
previous results on universality assumed either that the data points x ∈R𝑑have i.i.d. entries or
that the rows are independent and x is subgaussian. We relax these assumptions. That is, we
show that, for the quadratic loss function and any strongly convex not necessarily differentiable
regularizer universality holds as long as the rows are i.i.d., all moments of the x up to the 6th satisfy"
UNIVERSALITY,0.02356020942408377,"Ex|(x −Ex)𝑇v|𝑞≤𝐾
∥v∥𝑞
2
𝑑𝑞/2 for v ∈R𝑑and 𝑉𝑎𝑟(x𝑇Cx) →0 for any C of bounded operator norm."
UNIVERSALITY,0.024083769633507852,"Extending universality to mixtures of distributions. Since the main motivation behind this work is
the study of generalization in classification tasks, we focus on data matrices sampled from mixture
distributions. Note that most previous papers on universality, such as Montanari and Saeed [2022],
do not apply to this setting directly. As an illustration to why results of Montanari and Saeed [2022]
cannot just be applied to the mixture distributions directly, consider P = 1"
UNIVERSALITY,0.024607329842931937,"2N (µ, I) + 1"
UNIVERSALITY,0.025130890052356022,"2N (−µ, I)
corresponding to a mixture of two classes with antipodal means and the resulting classification
error depends on ∥µ∥, while P has mean 0 and covariance I meaning that the matching gaussian
distribution N (0, I) does not contain any information about the classes."
UNIVERSALITY,0.025654450261780103,"However, there is one prior paper Dandi et al. [2024] that studies universality specifically in the
context of mixture distributions. It is worth mentioning that their definition of universality is different.
Namely, they prove universality of the expectation with respect to the Gibbs distribution, which
suffices to show universality of the train but not of the test error."
UNIVERSALITY,0.02617801047120419,"Allowing for non-vanishing means. In Definition 2, we assume only that Ea∥a −µ∥2
2 = 𝑂(1),
whereas in Abbasi et al. [2019], Montanari and Saeed [2022] and Dandi et al. [2024] they have
∥µ∥2
2
Ea∥a−µ∥2
2 →0, implying that the norms of the means are negligible compared to the norms of the"
UNIVERSALITY,0.026701570680628273,"data points on average. To our knowledge, our work is the first to tackle it."
MAIN RESULT,0.027225130890052355,"3
Main Result"
MAIN RESULT,0.02774869109947644,"In this section, we present our main universality theorem and before doing so, we list the required
technical assumptions. Consider the following convex optimization problem"
MAIN RESULT,0.028272251308900525,"Φ𝜆(A) := min
𝑤
𝜆
2𝑛∥Aw −y∥2
2 + 1"
MAIN RESULT,0.028795811518324606,"𝑛𝑓(w)
(5)"
MAIN RESULT,0.02931937172774869,"Denote the solution to the optimization problem 5 above by wΦ𝜆(A). Then we assume the following:
Assumptions 1.
1. A is a regular block matrix and G is its matching Gaussian matrix as in
Section A.1."
MAIN RESULT,0.029842931937172777,"2. The regularizer 𝑓(·) is regular also as Section A.1 or there exists a sequence of regular 𝑓𝑚’s
converging uniformly to 𝑓."
MAIN RESULT,0.030366492146596858,3. 𝑓(·) is 𝑀-strongly convex.
MAIN RESULT,0.030890052356020943,"4. Ey∥y∥2
2 = 𝑂(𝑑) with E|𝑦𝑗|𝑞= 𝑂(1) for 𝑗∈[𝑛] and 𝑞∈[6] and is independent of A."
MAIN RESULT,0.031413612565445025,"5. 𝑠min(AA𝑇) = Ω(1) with high probability,"
MAIN RESULT,0.03193717277486911,"6. 𝑉𝑎𝑟(∥y∥2
2) →0"
MAIN RESULT,0.032460732984293195,"7. ∥∇𝑓(wΦ𝜆(A))∥2 ≤𝐾𝑓
√𝑛for a constant 𝐾𝑓independent of 𝜆.
Theorem 1. If Ψ = 𝚽= sup𝜆>0 Φ𝜆, suppose parts (1) - (7) of Assumptions 1 hold and for Ψ = Φ𝜆,
only assume parts (1) - (4) of Assumptions 1. Then"
MAIN RESULT,0.032984293193717276,"1. (Universality of the training error) For any L-Lipschitz 𝑔: R𝑑→R, and every 𝑡1 > 0, 𝑡2 >
𝑡1, 𝑐∈R the following holds:"
MAIN RESULT,0.033507853403141365,• If P(|𝑔(Ψ(G)) −𝑐| > 𝑡1) →0 then P(|𝑔(Ψ(A)) −𝑐| > 𝑡2) →0.
MAIN RESULT,0.034031413612565446,"• Furthermore, if 𝑔has bounded second derivative, then"
MAIN RESULT,0.03455497382198953,"lim
𝑛→∞"
MAIN RESULT,0.03507853403141361,"EA,y𝑔(Ψ(A)) −EG,y𝑔(Ψ(G))
 = 0"
MAIN RESULT,0.0356020942408377,"2. (Universality of the solution) For a function 𝜓with ∇2𝜓⪯𝑞I if either 𝜓is regular
(Definition 1) or there is a sequence of regular functions converging uniformly to 𝜓,"
MAIN RESULT,0.03612565445026178,"• If for every 𝑡> 0, P(|Ψ(G) −𝑐| > 𝑡) →0 then for B ∈{A, G}, there exists 𝑐1 ∈R
such that for any 𝑡1 > 0 we have P(|𝜓(wΨ(B)) −𝑐1| > 𝑡1) →0.
• Furthermore, if 𝜓is bounded"
MAIN RESULT,0.03664921465968586,"lim
𝑛→∞"
MAIN RESULT,0.03717277486910995,"EA,y𝜓(wΨ(A)) −EG,y𝜓(wΨ(G))
 = 0"
MAIN RESULT,0.03769633507853403,"To sum up, Theorem 1 says that the vector of weights wA trained on data sampled from a non-gaussian
distribution A either via SMD with a mirror 𝑓or by minimizing the least squares objective with
regularizer 𝑓, shares many similar characteristics with the vector of weights wG trained on data
sampled from the matching GMM G via the same optimization procedure under certain technical
assumptions on A and 𝑓. By ""similar characteristics"", we mean that for any 𝜓: R𝑑→R with
bounded Hessian, 𝜓(wA) = 𝜓(wG) holds in the limit."
MAIN RESULT,0.03821989528795811,"We would like to point out a few remarks.
Remark 1. Note that 𝜓need not be convex, in fact any linear combination of regular 𝜓is also
regular. An immediate result is the universality of the empirical distribution of the coordinates of
solutions.
Remark 2. The assumption ∥∇𝑓(wΦ𝜆(A))∥2 = 𝑂(√𝑛) in Theorem 1 is satisfied for any function 𝑓
with a locally lipschitz gradient, i.e ∥∇𝑓(u)∥2 ≤𝐾(1+ ∥u∥2). In particular it applies to 𝑓(·) = ∥· ∥2
2.
Remark 3. In both parts of Theorem 1, the regularizer is allowed to be an 𝑀-strongly convex uniform
limit of differentiable convex functions, which implies that 𝑓(w) = 𝑡∥w∥1 + 𝑀∥w∥2
2, 𝑡> 0, 𝑀> 0,
known as the elastic net, is also included in our results.
Remark 4. In Theorem 1, the optimal value of Φ𝜆is only achieved when 𝜆→∞and is not attained
at any finite 𝜆. This means that proving the results for 𝚿= Φ in Theorem 1 requires additional ideas
apart from the case Ψ = Φ𝜆in Theorem 1, as the latter makes use of the boundedness of 𝜆extensively.
To tackle this issue, we present a uniform convergence result in 𝜆which might be of independent
interest.
Remark 5. The condition 𝑠min(AA𝑇) = Ω(1) can be satisfied for a variety of random matrix models.
If for each block of A, A𝑖= ˜A𝑖𝚺1/2
𝑖
where ˜A𝑖has iid entries and 𝑠min(𝚺𝑖) = Ω(1), then by the
Bai-Yin’s law Bai and Yin [2008] the condition is satisfied. The second family is comprised of blocks
with independent and identical rows where the norm of rows have exponentrial concentration. We
prove this in the Appendix. One particular instance will be the distributions satsifying LCP from
Definition 3.
Remark 6. The second assumption in Assumptions 1 is not too restrictive and is in particular
satisfied for 𝜓(w) = P(a𝑇w > 𝑐). We have with high probability for Ψ ∈{Φ𝜆, Φ}"
MAIN RESULT,0.0387434554973822,"lim
𝑛→∞"
MAIN RESULT,0.03926701570680628,"P(a𝑇wΨ(A) > 𝑐) −P(a𝑇wΨ(G) > 𝑐)
 = 0"
MAIN RESULT,0.039790575916230364,"For a independent of A, G but sampled from the same distribution as the rows of A, respectively. Note
that by this argument we have reduced the problem of verifying CLT for a𝑇wΨ(A) to its Gaussian
counterpart, a𝑇wΨ(G). Then, specifically for the applications presented in this paper, we provide a
proof that a𝑇wΨ(G) satisfies CLT w.r.t. to randomness in a with high probability in G. However, in
general the latter CLT condition has to be verified on a case by case basis."
MAIN RESULT,0.04031413612565445,"4
Applications: Transfer Learning"
REGRESSION,0.040837696335078534,"4.1
Regression"
REGRESSION,0.041361256544502616,"We consider the classical problem of recovering the best linear regressor for the following linear
model
y = Xw∗+ z
(6)"
REGRESSION,0.041884816753926704,"Here, w∗is the ground truth, the rows of the data matrix X are i.i.d with Ex𝑖= 0 and Ex𝑖x𝑇
𝑖=: R𝑥,
z is centered and is independent of X and satisfies Ez∥z∥2
2 = 𝜎2𝑛. For a given ˆw, its generalization
error is defined by:"
REGRESSION,0.042408376963350786,𝑒𝑔𝑒𝑛( ˆw) := E𝑥(x𝑇w∗−x𝑇ˆw)2 = ( ˆw −w∗)𝑇R𝑥( ˆw −w∗)
REGRESSION,0.04293193717277487,"Now in order to recover w∗given observations (y, X), we choose to optimize the least squares
objective, minw ∥y −Xw∥2
2, and to do so, we leverage SGD. We would like to investigate how useful
having a pretrained classifier w0 can be for the recovery of w∗. As pointed out earlier, SGD initialized
from w = w0, by its implicit regularization property, converges to the solution ˆw of (1). We denote
𝑒𝑎:= 𝑒𝑔𝑒𝑛(w0); hence the name ""a priori error"", 𝑒𝑎. We provide the assumptions necessary for our
results on linear regression in Section A.2. In what follows we give a precise characterization of the
posterior error, 𝑒𝑝, of the model after training with SGD, in terms of 𝑒𝑎and the other parameters of
the problem. In order to leverage Theorem 1 to establish the universality of the generalization error,
it is sufficient to apply a change of variable w′ := R−1/2
𝑥
w and use 𝜓(w) = ∥w∥2
2 as the test function.
Theorem 2. Under Assumptions 3 in Section A.2, the generalization error of the SGD solution
initialized from w0 converges in probability to"
REGRESSION,0.043455497382198956,𝑒𝑝= 2 −𝜅(1 −𝑡)
REGRESSION,0.04397905759162304,"𝜅(1 −𝑡) −1𝜎2 +
𝑡
𝜅(1 −𝑡) −1𝑒𝑎
(7)"
REGRESSION,0.04450261780104712,"With 𝑡=
∫
4𝑝(𝑟)
(2+𝑟𝜃)2 𝑑𝑟where 𝜃is found through 𝜅−1 𝜅
= 1"
REGRESSION,0.04502617801047121,𝜃𝑆𝑝(−1
REGRESSION,0.04554973821989529,𝜃) (12). And 𝜅> 1 is the proportional
REGRESSION,0.04607329842931937,"constant, i.e for X ∈R𝑛×𝑑, 𝑑"
REGRESSION,0.04659685863874346,"𝑛→𝜅. Moreover, for any distribution 𝑝(𝑟) we have"
REGRESSION,0.04712041884816754,"𝑒𝑝≥
1
𝜅−1𝜎2 + 𝜅−1"
REGRESSION,0.04764397905759162,"𝜅
𝑒𝑎
(8)"
REGRESSION,0.048167539267015703,The lower bound is attained if and only if 𝑝(𝑟) = 𝛿(𝑟−𝑟0) for some 𝑟0 > 0
REGRESSION,0.04869109947643979,"The following remark is immediate:
Remark 7. Theorem 2 entails that, depending on the noise level present in the data, training could
be even potentially harmful. Indeed, if 𝜎2 ≥𝑒𝑎, then 𝑒𝑝≥
1
𝜅−1𝜎2 + 𝜅−1"
REGRESSION,0.049214659685863874,"𝜅𝑒𝑎≥𝜎(2√𝑒𝑎−𝜎) ≥𝑒𝑎
for any 𝜅> 1 and it is therefore more appropriate to use vector w0 instead of performing fine-tuning.
Moreover, if the covariance Rx is scalar, then the converse is true. Namely, if 𝑒𝑎≥𝜎2, then the
best achievable error corresponds to 𝜅∗=
√𝑒𝑎
√𝑒𝑎−𝜎and equals 𝜎(2√𝑒𝑎−𝜎), which is less than 𝑒𝑎.
Therefore, transfer learning contributes to improving the test performance when the variance of the
noise is not too high, but the model has to be fine-tuned on the correct amount of target data."
CLASSIFICATION,0.049738219895287955,"4.2
Classification"
PROBLEM SETTING,0.050261780104712044,"4.2.1
Problem setting"
PROBLEM SETTING,0.050785340314136125,"Consider a binary classification task and let X stand for the data matrix and y denote the vector
of labels where each 𝑦𝑖= ±1 depending on what class the 𝑖-th point x𝑖falls into. We denote
µℓ:= Ex𝑖and 𝚺ℓ:= Ex𝑖x𝑇
𝑖−µℓµ𝑇
ℓ, for the mean and covariance of each datapoint x𝑖, respectively
and ℓ∈{1, 2} depending on the class of x𝑖. After learning a linear classifier w, we assign labels to
new previously unseen points according to y𝑛𝑒𝑤= 𝑠𝑖𝑔𝑛(w𝑇x𝑛𝑒𝑤). Without loss of generality we
will assume that the first 𝑛"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05130890052356021,"2 rows of X are sampled from the first class and the remaining rows are
sampled from the second as we can permute the rows otherwise. Modulo such permutation, it is
straightforward to see that X satisfies parts (1) - (3) of Definition 2. Since the topic of the present
paper is fine-tuning, we assume that the following steps are performed:"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.051832460732984295,"1. Obtain a pre-trained classifier w0
2. Renormalize w0 obtained during the previous step using the target data via"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05235602094240838,"min
𝛼∥y −𝛼Xw0∥2"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05287958115183246,This yields:
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05340314136125655,𝛼= y𝑇Xw
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05392670157068063,"∥Xw∥2
(9)"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05445026178010471,"After finding 𝛼take w′
0 := 𝛼w0. This transform preserves the direction of w0, while setting"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.0549738219895288,its squared magnitude to 𝛼2∥w0∥2 = (y𝑇Xw0)2∥w0∥2
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05549738219895288,"∥Xw0∥4
, which depends only on the direction of
w0 but not on its magnitude anymore. We find applying this transform very meaningful, as
it does not change the classification error for the source data but simplifies learning for the
regression problem defined by the target data.
3. Learn the final classifier from the target data X ∈R𝑛×𝑑and labels y ∈R𝑛using SGD
initialized at w′
0, obtained from the previous step."
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05602094240837696,"We will use the assumptions in Section A.3 to define further details of the classification task we
consider. Note that in practice the main difficulty of the classification task in the over-parametrized
regime (𝑛< 𝑑) arises due to the fact that µ1, µ2, 𝚺1 and 𝚺2 are not known and cannot be estimated
reliably. Nevertheless, we would like to start with characterizing the optimal performance in the
scenario where these are provided to us by an oracle. Lemma 1 in Section A.3 provides such a
characterization under certain symmetry assumptions. In view of Lemma 1 it is natural to introduce
the following assumption:
Assumptions 2. The initialization point w0 satisfies"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05654450261780105,"w0 = 𝑡∗w∗+ 𝑡𝜂η
(10)"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05706806282722513,"where w∗= (𝚺1 + 𝚺2)−1(µ1 −µ2) is the optimal classifier defined as in Lemma 1. η is a noise vector
with ∥η∥2 = 1 −𝑟and for any deterministic matrix C of bounded operator norm it holds that η𝑇Cη
converges in probability to Tr(C) (1−𝑟)"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05759162303664921,"𝑑
. Note that the smaller the ratio 𝑡𝜂"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.0581151832460733,"𝑡∗is, the better performance
w0 has."
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05863874345549738,"Indeed, 10 captures the closeness of the initilization point w0 to the optimal classifier w∗. For
example, in the isotropic case 𝚺1 = 𝚺2 = 𝜎2I, if w0 has the classification error 𝑒𝑎, this means that"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.059162303664921465,we should take 𝑡𝜂
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05968586387434555,"𝑡∗=
𝑑
2𝜎2
√︃"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.060209424083769635,"𝑑(1−𝑟)
𝜎2𝑄−1(𝑒𝑎)2 −2. Finally, we would like to remark that, under notation and
assumptions from Theorem 1, the following corollary is implied by Theorem 1 modulo Theorem 4
presented in the Appendix along with the explanation of the implication:
Corollary 1 (Universality of the classification error for SGD and ridge regression objectives). If
𝑓(w) = ∥w −w0∥2
2, when ∥w0∥2
2 = 𝑂(𝑑) then for a, g independent of A, G but sampled from the
same distribution as their rows, respectively, we have with high probability for Ψ ∈{Φ𝜆, Φ}"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.060732984293193716,"lim
𝑛→∞"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.0612565445026178,"P(a𝑇wΨ(A) > 0) −P(g𝑇wΨ(G) > 0)
 = 0"
ANALYSIS OF THE CLASSIFICATION ERROR,0.061780104712041886,"4.2.2
Analysis of the Classification Error"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06230366492146597,"Theorem 3. Let w0 be an initialization point that satisfies Assumption 2 and X be a data matrix
satisfying Assumptions 4 in Section A.3. Then the classification error of the SGD solution initialized"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06282722513089005,"at 𝛼w0 for 𝛼defined by (9) and trained on X, y is given by 𝑒𝑝= 𝑄  −𝛾+2 2
√︃ 𝜏2−𝛾2 4 !"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06335078534031413,where 𝛾and 𝜏are
ANALYSIS OF THE CLASSIFICATION ERROR,0.06387434554973823,"determined in terms of a 𝜃solving 2
√"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06439790575916231,"2
𝜃√𝑛𝑆𝚺1+𝚺2(−2
√"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06492146596858639,"2
𝜃√𝑛) = 1 −1"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06544502617801047,𝜅whose expressions are provided in the
ANALYSIS OF THE CLASSIFICATION ERROR,0.06596858638743455,"Appendix (cf. Section I, equation 40) with 𝑑"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06649214659685863,𝑛→𝜅> 1.
ANALYSIS OF THE CLASSIFICATION ERROR,0.06701570680628273,"The expressions from Theorem 3 can be simplified drastically in the case of scalar covariance
matrices."
ANALYSIS OF THE CLASSIFICATION ERROR,0.06753926701570681,"Corollary 2. Under the notation from Theorem 3, assume Σ1 = Σ2 = 𝜎2I𝑑"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06806282722513089,"𝑑
and define 𝜌:= 𝑑(1−𝑟)"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06858638743455497,"𝜎2
.
Let 𝑒𝑎be the classification error of w0 initialized according to 10. Then"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06910994764397906,"𝑒𝑝(𝜅, 𝜌)
P−→𝑄"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06963350785340314,"
𝑄−1(𝑒𝑎)2(2𝜅+ 𝜌−2) + 𝜌
√︁"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07015706806282722,"𝜅(𝜅−1)
√︃"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07068062827225131,(2𝜅+ 𝜌)((4𝜅−2)𝑄−1(𝑒𝑎)4 + 𝑄−1(𝑒𝑎)2  2𝜅3 + 𝜅2𝜌−2𝜅(𝜌−1) + 𝜌 + 2𝜅2) ! (11)
ANALYSIS OF THE CLASSIFICATION ERROR,0.0712041884816754,We arrive at the following conclusion summarizing the derivations above.
ANALYSIS OF THE CLASSIFICATION ERROR,0.07172774869109948,"Remark 8.
• If 𝜌= 𝜔(1), then 𝑒𝑝(𝜅, ∞)
P−→𝑄

𝑄−1(𝑒𝑎) +
1
𝑄−1(𝑒𝑎)"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07225130890052356,"√︁
𝜅
𝜅−1"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07277486910994764,"
< 𝑒𝑎and fine-"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07329842931937172,"tuning always succeeds. It is observed that for 𝑒𝑎> 𝑄(1), the worse 𝑒𝑎is, the better 𝑒𝑝
will be."
ANALYSIS OF THE CLASSIFICATION ERROR,0.07382198952879582,"• If 𝜌= Θ(1), then the fine-tuning step may or may not succeed depending on whether
𝑒𝑝(𝜅, 𝜌) < 𝑒𝑎or not. See Section 5.2 for further (empirical) explorations on the usefulness
of the fine-tuning of the pre-trained solution for this regime."
ANALYSIS OF THE CLASSIFICATION ERROR,0.0743455497382199,"• If 𝜌= 𝑜(1), then the classification error of any linear classifier goes to 1"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07486910994764398,2 as 𝑛→∞since it
ANALYSIS OF THE CLASSIFICATION ERROR,0.07539267015706806,"is lower bounded by 𝑄(
√︃"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07591623036649214,"𝜌
2 ) according to Lemma 1. Thus, any kind of learning will fail in
this regime."
ANALYSIS OF THE CLASSIFICATION ERROR,0.07643979057591622,"An interesting regime is where the number of samples is much lower than the number of parameters,
which naturally rises in the context of fine-tuning large models and corresponds to letting 𝜅≫1. For
this regime, we have 𝑒𝑝≈𝑄 "
ANALYSIS OF THE CLASSIFICATION ERROR,0.07696335078534032,𝑄−1(𝑒𝑎) + 1 2
ANALYSIS OF THE CLASSIFICATION ERROR,0.0774869109947644,"
𝜌−1
𝑄−1(𝑒𝑎) −3𝑄−1(𝑒𝑎)
 1 𝜅 !"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07801047120418848,"We observe that if 𝜌< 1, transfer learning always fails independent of value of 𝑒𝑎for 𝜅≫1 and"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07853403141361257,"training would not help with improving performance. On the other hand, if 𝑒𝑎> 𝑄(
√︃ 𝜌−1"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07905759162303665,"3 ) > 𝑄(
√︃"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07958115183246073,"𝜌
2 ),
for large enough 𝜅, transfer learning always achieves an error less than 𝑒𝑎."
NUMERICAL EXPERIMENTS,0.08010471204188482,"5
Numerical experiments"
REGRESSION,0.0806282722513089,"5.1
Regression"
REGRESSION,0.08115183246073299,"To corroborate our findings, we plotted the generalization error of the weight obtained through
running SGD according to the Assumptions 3 in Section 4.1 with respect to 𝜅= 𝑑"
REGRESSION,0.08167539267015707,"𝑛. To do so, we
fixed 𝑑= 1000 and varied 𝑛across different values. We used CVXPY (Grant and Boyd [2014],
Agrawal et al. [2018]) to solve (1) efficiently on a Laptop CPU. To verify the universality of our
results, we initially constructed a centered random matrix X′ with i.i.d components according
to the distributions N (0, 1), 𝐵𝑒𝑟(0.5), and 𝜒2(1) and using a correlation matrix R𝑥we defined
X := X′R1/2
𝑥. On the other hand, we generated R𝑥according to the following three distributions:
single level 𝑝(𝑟) := 𝛿(𝑟−1), bilevel 𝑝(𝑟) := 0.3𝛿(𝑟−1) + 0.7𝛿(𝑟−5) and uniform on the interval
[1, 5]. We specifically consider these cases as they are common in the literature and we use the
parameter 𝜎2 for the component-wise variance of z in 6. Additionally, w0 is chosen according to
Assumptions 3 in such a manner that 𝑒𝑎= 1. In both Figures 1, 2 the blue line represents the
prediction 7 made by Theorem 2, the red line depicts the lower bound 8. The markers showcase the
performance of weights obtained under different distributions as described earlier. It can be seen that
from Figures 1, 2, for the bilevel and uniform distributions, depending on the value of 𝜎, transfer
learning might not be beneficial as discussed in Remark 7. In particular, in Figures 1c and 2c, the
generalization error is always lower bounded by 𝑒𝑎= 1 and only in 𝜅→∞can get close to 1. Finally,
the single-level distribution on R𝑥is always a lower bound for the generalization error of various
distributions on R𝑥."
CLASSIFICATION,0.08219895287958115,"5.2
Classification"
CLASSIFICATION,0.08272251308900523,"Similar to the preceding subsection, we experimented with sampling the entries of X independently
from three different centered distributions: normal, Bernoulli, and 𝜒2. We also sampled the means
µ1 and µ2 from N (0, 1"
CLASSIFICATION,0.08324607329842931,"𝑑I𝑑) with a cross-correlation 𝑟= E[µ1𝑖µ2𝑖] = 0.9 and added them to
the corresponding rows of X. For Figures 3, we fixed 𝜌= 0.8, 2, 5 respectively and plotted the
classification error predicted by Corollary 2 as a solid red line, empirically observed classification
errors for the normal, Bernoulli and 𝜒2 entries as black squares, green circles and red triangles
respectively. The blue lines depict the classification error at the initialization. It can be seen that there
is a close match between the empirical errors between points from different distributions as well as
with the theoretical prediction, thus validating both Theorem 1 and Theorem 3. Note that fixing 𝜌"
CLASSIFICATION,0.08376963350785341,in this setting corresponds to fixing 𝜎2 as 𝜌= 𝑑(1−𝑟)
CLASSIFICATION,0.08429319371727749,"𝜎2
. It is also worth mentioning that fine-tuning
improves performance in the setting of Figure 3c but hurts it for Figure 3a. Moreover, note that
in Figure 3b, although for smaller 𝜅transfer learning hurts, past a certain 𝜅, training improves the
performance. Also we observe that by increasing 𝜌across the three plots, the classification task
becomes easier and fine-tuning improves performance as supported by Remark 8."
CLASSIFICATION,0.08481675392670157,"2
4
6
8
10 0 0.2 0.4 0.6 0.8 1 𝜅"
CLASSIFICATION,0.08534031413612565,Generalization Error N
CLASSIFICATION,0.08586387434554973,"𝐵
𝜒2
𝑒𝑝
Lower Bound"
CLASSIFICATION,0.08638743455497382,(a) 𝜎= 0.01
CLASSIFICATION,0.08691099476439791,"2
4
6
8
10 0.4 0.6 0.8 1 𝜅"
CLASSIFICATION,0.08743455497382199,Generalization Error N
CLASSIFICATION,0.08795811518324607,"𝐵
𝜒2
𝑒𝑝
Lower Bound"
CLASSIFICATION,0.08848167539267016,(b) 𝜎= 0.15
CLASSIFICATION,0.08900523560209424,"2
4
6
8
10 0 50 100 150 𝜅"
CLASSIFICATION,0.08952879581151832,Generalization Error N
CLASSIFICATION,0.09005235602094241,"𝐵
𝜒2(1)"
CLASSIFICATION,0.0905759162303665,"𝑒𝑝
Lower Bound"
CLASSIFICATION,0.09109947643979058,(c) 𝜎= 2
CLASSIFICATION,0.09162303664921466,Figure 1: Generalization error for the bilevel distribution on the covariance of the data
CLASSIFICATION,0.09214659685863874,"2
4
6
8
10 0 0.2 0.4 0.6 0.8 1 𝜅"
CLASSIFICATION,0.09267015706806282,Generalization Error N
CLASSIFICATION,0.09319371727748692,"𝐵
𝜒2
𝑒𝑝
Lower Bound"
CLASSIFICATION,0.093717277486911,(a) 𝜎= 0.01
CLASSIFICATION,0.09424083769633508,"2
4
6
8
10 0.4 0.6 0.8 𝜅"
CLASSIFICATION,0.09476439790575916,Generalization Error N
CLASSIFICATION,0.09528795811518324,"𝐵
𝜒2
𝑒𝑝
Lower Bound"
CLASSIFICATION,0.09581151832460733,(b) 𝜎= 0.15
CLASSIFICATION,0.09633507853403141,"2
4
6
8
10 0 50 100 𝜅"
CLASSIFICATION,0.0968586387434555,Generalization Error N
CLASSIFICATION,0.09738219895287958,"𝐵
𝜒2
𝑒𝑝
Lower Bound"
CLASSIFICATION,0.09790575916230367,(c) 𝜎= 2
CLASSIFICATION,0.09842931937172775,Figure 2: Generalization error for the uniform distribution on the covariance of the data
CLASSIFICATION,0.09895287958115183,"0
5
10
15
20 0.3 0.35 0.4 0.45 𝜅"
CLASSIFICATION,0.09947643979057591,Classification Error N
CLASSIFICATION,0.1,"𝐵
𝜒2
𝑒𝑝"
CLASSIFICATION,0.10052356020942409,"(a) 𝜌= 0.8, 𝑟= 0.9, 𝑒𝑎= 0.3"
CLASSIFICATION,0.10104712041884817,"0
5
10
15
20 0.3 0.32 0.34 0.36 0.38 𝜅"
CLASSIFICATION,0.10157068062827225,"Classification Error 𝑒𝑎
N"
CLASSIFICATION,0.10209424083769633,"𝐵
𝜒2
𝑒𝑝"
CLASSIFICATION,0.10261780104712041,"(b) 𝜌= 2, 𝑟= 0.9, 𝑒𝑎= 0.3"
CLASSIFICATION,0.10314136125654451,"0
5
10
15
20 0.2 0.22 0.24 0.26 0.28 0.3 𝜅"
CLASSIFICATION,0.10366492146596859,Classification Error N
CLASSIFICATION,0.10418848167539267,"𝐵
𝜒2
𝑒𝑝"
CLASSIFICATION,0.10471204188481675,"(c) 𝜌= 5, 𝑟= 0.9, 𝑒𝑎= 0.3"
CLASSIFICATION,0.10523560209424083,Figure 3: Classification error
CONCLUSION AND FUTURE WORK,0.10575916230366492,"6
Conclusion and future work"
CONCLUSION AND FUTURE WORK,0.10628272251308901,"We presented a novel Gaussian universality result and used it to study the problem of transfer learning
in linear models, for both regression and binary classification. In particular, we were able to precisely
relate the performance of the pretrained model to that of the fine-tuned model trained via SGD and,
as a result, identified situations where transfer learning helps and where it or does not. Possible future
directions include investigating other problems where the universality result may be useful, extending
the results to potential functions that are not necessarily convex nor separable, as well as exploring
the implications of universality for objectives with explicit regularization."
REFERENCES,0.1068062827225131,References
REFERENCES,0.10732984293193717,"E. Abbasi, F. Salehi, and B. Hassibi. Universality in learning from linear measurements. Advances in
Neural Information Processing Systems, 32, 2019."
REFERENCES,0.10785340314136126,"A. Agrawal, R. Verschueren, S. Diamond, and S. Boyd. A rewriting system for convex optimization
problems. Journal of Control and Decision, 5(1):42–60, 2018."
REFERENCES,0.10837696335078534,"D. Akhtiamov, D. Bosch, R. Ghane, K. N. Varma, and B. Hassibi. A novel gaussian min-max theorem
and its applications. arXiv preprint arXiv:2402.07356, 2024."
REFERENCES,0.10890052356020942,"N. Azizan and B. Hassibi. Stochastic gradient/mirror descent: Minimax optimality and implicit
regularization. In International Conference on Learning Representations, 2018."
REFERENCES,0.1094240837696335,"Z.-D. Bai and Y.-Q. Yin. Limit of the smallest eigenvalue of a large dimensional sample covariance
matrix. In Advances In Statistics, pages 108–127. World Scientific, 2008."
REFERENCES,0.1099476439790576,"S. G. Bobkov. On concentration of distributions of random weighted sums. Annals of probability,
pages 195–215, 2003."
REFERENCES,0.11047120418848168,"D. Bosch, A. Panahi, and B. Hassibi. Precise asymptotic analysis of deep random feature models. In
The Thirty Sixth Annual Conference on Learning Theory, pages 4132–4179. PMLR, 2023."
REFERENCES,0.11099476439790576,"S. Bozinovski. Reminder of the first paper on transfer learning in neural networks, 1976. Informatica,
44(3), 2020."
REFERENCES,0.11151832460732984,S. Chatterjee. A generalization of the lindeberg principle. 2006.
REFERENCES,0.11204188481675392,"W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for transfer learning. In Proceedings of the 24th
international conference on Machine learning, pages 193–200, 2007."
REFERENCES,0.112565445026178,"Y. Dandi, L. Stephan, F. Krzakala, B. Loureiro, and L. Zdeborová. Universality laws for gaussian
mixtures in generalized linear models. Advances in Neural Information Processing Systems, 36,
2024."
REFERENCES,0.1130890052356021,"Y. Dar, D. LeJeune, and R. G. Baraniuk. The common intuition to transfer learning can win or lose:
Case studies for linear regression. arXiv preprint arXiv:2103.05621, 2021."
REFERENCES,0.11361256544502618,"F. Gerace, L. Saglietti, S. S. Mannelli, A. Saxe, and L. Zdeborová. Probing transfer learning with a
model of synthetic correlated datasets. Machine Learning: Science and Technology, 3(1):015030,
2022."
REFERENCES,0.11413612565445026,"M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 2.1, Mar.
2014."
REFERENCES,0.11465968586387434,"S. Gunasekar, J. Lee, D. Soudry, and N. Srebro. Characterizing implicit bias in terms of opti-
mization geometry. In J. Dy and A. Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pages 1832–1841. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/
v80/gunasekar18a.html."
REFERENCES,0.11518324607329843,"Q. Han and Y. Shen. Universality of regularized regression estimators in high dimensions. The
Annals of Statistics, 51(4):1799–1823, 2023."
REFERENCES,0.11570680628272251,"T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless least
squares interpolation. Annals of statistics, 50(2):949, 2022."
REFERENCES,0.1162303664921466,"H. Hu and Y. M. Lu. Universality laws for high-dimensional learning with random features. IEEE
Transactions on Information Theory, 69(3):1932–1964, 2022."
REFERENCES,0.11675392670157068,"A. Jain, A. Montanari, and E. Sasoglu. Scaling laws for learning with real and surrogate data. arXiv
preprint arXiv:2402.04376, 2024."
REFERENCES,0.11727748691099477,"S. Lahiry and P. Sur. Universality in block dependent linear models with applications to nonparametric
regression. arXiv preprint arXiv:2401.00344, 2023."
REFERENCES,0.11780104712041885,"F. Liese and K.-J. Miescke. Statistical decision theory. In Statistical Decision Theory: Estimation,
Testing, and Selection, pages 1–52. Springer, 2008."
REFERENCES,0.11832460732984293,"J. W. Lindeberg. Eine neue herleitung des exponentialgesetzes in der wahrscheinlichkeitsrechnung.
Mathematische Zeitschrift, 15(1):211–225, 1922."
REFERENCES,0.11884816753926701,"A. Montanari and P.-M. Nguyen. Universality of the elastic net error. In 2017 IEEE International
Symposium on Information Theory (ISIT), pages 2338–2342. IEEE, 2017."
REFERENCES,0.1193717277486911,"A. Montanari and B. N. Saeed. Universality of empirical risk minimization. In Conference on
Learning Theory, pages 4310–4312. PMLR, 2022."
REFERENCES,0.11989528795811519,"S. Oymak and J. A. Tropp. Universality laws for randomized dimension reduction, with applications.
Information and Inference: A Journal of the IMA, 7(3):337–446, 2018."
REFERENCES,0.12041884816753927,"A. Panahi and B. Hassibi. A universal analysis of large-scale regularized least squares solutions.
Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.12094240837696335,"D. Schröder, H. Cui, D. Dmitriev, and B. Loureiro. Deterministic equivalent and error universality
of deep random features learning. In International Conference on Machine Learning, pages
30285–30320. PMLR, 2023."
REFERENCES,0.12146596858638743,"M. E. A. Seddik, C. Louart, M. Tamaazousti, and R. Couillet. Random matrix theory proves that deep
learning representations of gan-data behave as gaussian mixtures. In International Conference on
Machine Learning, pages 8573–8582. PMLR, 2020."
REFERENCES,0.12198952879581151,"C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu. A survey on deep transfer learning. In
Artificial Neural Networks and Machine Learning–ICANN 2018: 27th International Conference
on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III 27, pages
270–279. Springer, 2018."
REFERENCES,0.1225130890052356,"R. Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018."
REFERENCES,0.12303664921465969,"F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He. A comprehensive survey on
transfer learning. Proceedings of the IEEE, 109(1):43–76, 2020."
REFERENCES,0.12356020942408377,"A
Notations, Definitions and Assumptions"
REFERENCES,0.12408376963350785,"A.1
Notations and Definitions"
REFERENCES,0.12460732984293194,"We call a convex function 𝑓: R𝑑→R separable if 𝑓(w) = Í𝑑
𝑖=1 𝑓𝑖(𝑤𝑖), where 𝑓𝑖: R →R are
convex. Examples of such functions include ∥· ∥𝑝
𝑝for every 𝑝≥1. A function 𝑓: R𝑑→R is
𝑀-strongly convex for 𝑀> 0 if for every w ∈R𝑑, 𝑓(w) −𝑀∥w∥2
2 is convex."
REFERENCES,0.12513089005235603,"Functions satisfying the following definition will be instrumental in the statements of our results:
Definition 1. We call a function 𝑓: R𝑑→R regular if it satisfies the following conditions"
REFERENCES,0.1256544502617801,1. 𝑓is convex with 𝑓(0) = 𝑂(𝑑).
REFERENCES,0.1261780104712042,2. 𝑓is three times differentiable.
REFERENCES,0.12670157068062826,"3. 𝑓satisfies the following third-order condition for some 𝐶𝑓> 0: 𝑑
∑︁"
REFERENCES,0.12722513089005236,"𝑖, 𝑗,𝑘=1"
REFERENCES,0.12774869109947645,"𝜕3 𝑓
𝜕𝑤𝑖𝜕𝑤𝑗𝜕𝑤𝑘
𝑣𝑖𝑣𝑗𝑣𝑘≤𝐶𝑓 𝑑
∑︁"
REFERENCES,0.12827225130890052,"𝑖=1
|𝑣𝑖|3"
REFERENCES,0.12879581151832462,"Note that separable functions with bounded third order derivatives satisfy the third assumption of
Definition 1. For the description of the main results on regression and classification we recall several
probability theory concepts."
REFERENCES,0.12931937172774868,"Given a real-valued measure 𝜇, its Stieltjes transform is defined as"
REFERENCES,0.12984293193717278,"𝑆𝜇(𝑧) :=
∫ R"
REFERENCES,0.13036649214659685,"1
𝑟−𝑧𝜇(𝑑𝑟),
𝑧∈H+ ∪R \ 𝑆𝑢𝑝𝑝(𝜇)
(12)"
REFERENCES,0.13089005235602094,"With H+ = {𝑧: 𝐼𝑚(𝑧) > 0}. Moreover, we denote convergence in probability and weak convergence"
REFERENCES,0.13141361256544504,"for measures by
P−→and ⇝respectively. For a random variable 𝑋, we sometimes use 𝑉𝑎𝑟(𝑋) for the
variance of 𝑋. For a convex differentiable function 𝑔, the Bregman divergence is defined by"
REFERENCES,0.1319371727748691,"𝐷𝑔(w, w0) := 𝑔(w) −𝑔(w0) −∇𝑔(w0)𝑇(w −w0)"
REFERENCES,0.1324607329842932,"Next, we will provide a description of the design matrices investigated in this paper. It will be clear
soon that these assumptions are often satisfied in practice.
Definition 2.
2 We call a random matrix A ∈R𝑛×𝑑block-regular if it satisfies the following
properties:"
REFERENCES,0.13298429319371727,"1. A𝑇=  A𝑇
1
A𝑇
2
. . .
A𝑇
𝑘
 with 𝑘being finite and for each A𝑖∈R𝑛𝑖×𝑑, 𝑛𝑖= Θ(𝑛)"
REFERENCES,0.13350785340314136,"2. For 1 ≤𝑖≤𝑘, the rows of A𝑖are distributed independently and identically."
REFERENCES,0.13403141361256546,"3. EA𝑖=: 1µ𝑇
𝑖"
REFERENCES,0.13455497382198953,"4. Let a be any row of A and µ be its mean. Then for any deterministic vector v ∈R𝑑, and"
REFERENCES,0.13507853403141362,"𝑞∈N, 𝑞≤6, there exists a constant 𝐾> 0 such that Ea|(a −µ)𝑇v|𝑞≤𝐾
∥v∥𝑞
2
𝑑𝑞/2"
REFERENCES,0.1356020942408377,"5. For any deterministic matrix C ∈R𝑑×𝑑of bounded operator norm we have 𝑉𝑎𝑟(a𝑇Ca) →0
as 𝑑→∞."
REFERENCES,0.13612565445026178,"6. Each ∥µ𝑖∥2
2 = 𝑂(1)"
REFERENCES,0.13664921465968585,"Note that assumptions 1-3 from Definition 2 are satisfied for the design matrix both in the classification
and regression scenarios, and the assumption ∥µ𝑖∥2
2 = 𝑂(1) is just a matter of normalization. While
assumptions 4 and 5 might appear obscure at first, any a ∈R𝑑satisfying the Lipschitz Concentration
Property (LCP) also satisfies the aforementioned assumptions. Recall that the LCP is defined as
follows."
REFERENCES,0.13717277486910995,"2We would like to point out that in this definition and throughout this work in general, mathematically
speaking, we are dealing with sequences of vectors and matrices with increasing dimensions. So, for example,
assumption 5 can be formally stated as follows: given any sequence of C𝑖∈R𝑑𝑖×𝑑𝑖satisfying ∥C𝑖∥𝑜𝑝≤𝐾for
some constant 𝐾> 0 and all 𝑖> 0, we have that 𝑉𝑎𝑟(a𝑖C𝑖a𝑖) →0 as 𝑖→∞"
REFERENCES,0.13769633507853404,"Definition 3. We say that a distribution a ∈R𝑑satisfies LCP if the following inequality holds for
any Lipschitz function 𝜙: R𝑑→R"
REFERENCES,0.1382198952879581,"P(|𝜙(a) −E𝜙(a)| > 𝑡) ≤2 exp (−
𝑑𝑡2"
REFERENCES,0.1387434554973822,"2∥𝜙∥2
𝐿𝑖𝑝
) ."
REFERENCES,0.13926701570680627,"It is also worth mentioning that, as shown in Seddik et al. [2020], any data distribution from
which one can sample using a GAN satisfies Definition 3. Since it is known that many natural
datasets can be approximated well by GAN-generated data in practice, we thus find this assumption
realistic. These assumptions also bear significance for the instance-based approach to transfer
learning as utilizing GANs can remedy the dearth of data. What is more, these assumptions are not
limited to the subgaussians and include many other subexponential distributions, including 𝜒2 which
appears naturally in many signal processing applications such as Phase Retrieval. In order to tackle
optimizations such as 3, 4, we prove their equivalence to a problem with a suitable Gaussian design
G. For this purpose we will need the following definition:"
REFERENCES,0.13979057591623037,"Definition 4. We call a Gaussian matrix G matching a block-regular A if G is block-regular as well,
EG𝑖= EA𝑖= 1µ𝑇
𝑖and for any row g of G𝑖and any row a of A𝑖, it holds that Egg𝑇= Eaa𝑇."
REFERENCES,0.14031413612565444,"A.2
Assumptions for Theorem 2"
REFERENCES,0.14083769633507853,"Assumptions 3.
1. R𝑥≻0 is diagonal without loss of generality."
REFERENCES,0.14136125654450263,"2. Let ˆ𝑝𝑁be the empirical spectral density of R𝑥, then ˆ𝑝𝑁⇝𝑝for some probability distribu-
tion 𝑝."
REFERENCES,0.1418848167539267,"3. w0 is a white perturbation of true w∗in the R𝑥basis. That is, w0 = w∗+ R−1/2
𝑥
ξ where
Eξ = 0, Eξξ𝑇= 𝑒𝑎"
REFERENCES,0.1424083769633508,𝑑I for a fixed 𝑒𝑎> 0.
REFERENCES,0.14293193717277486,"The third assumption from Assumptions 3 represents the fact that w0 is a perturbation of the ground
truth w∗in the eigenbasis of 𝑅𝑥."
REFERENCES,0.14345549738219895,"A.3
Assumptions for Theorem 3"
REFERENCES,0.14397905759162305,"Assumptions 4.
1. The data matrix X satisfies parts (4)-(6) of Definition 2"
REFERENCES,0.14450261780104712,"2. The means µ1, µ2 are of norm 1 and for any deterministic matrix C of bounded operator
norm it holds that µ𝑇
1 Cµ1, µ𝑇
2 Cµ2 and µ𝑇
1 Cµ2 converge in probability to Tr(C)"
REFERENCES,0.1450261780104712,"𝑑
, Tr(C)"
REFERENCES,0.14554973821989528,"𝑑
and
𝑟Tr(C)"
REFERENCES,0.14607329842931938,"𝑑
respectively."
REFERENCES,0.14659685863874344,"3. 𝚺1, 𝚺2 are diagonal."
REFERENCES,0.14712041884816754,"4. Let ˆ𝑝𝑁be the joint empirical spectral density of 𝚺1, 𝚺2, then ˆ𝑝𝑁⇝𝑝, where the distribu-
tion 𝑝is such that 𝑝(𝑠1, 𝑠2) = 𝑝(𝑠2, 𝑠1) holds for all 𝑠1, 𝑠2."
REFERENCES,0.14764397905759163,"For a discussion of the first assumption from Assumptions 4, see the paragraph after Definition 2.
The second assumption from Assumptions 4 is satisfied, for example, for any random vector of
the form 𝚺1/2z, where 𝚺is an arbitrary 𝑃𝑆𝐷matrix and z is i.i.d. The third assumption postulates
that the means are normalized generic 𝑑-dimensional vectors with an angle arccos 𝑟between them.
The fourth assumption says that 𝚺1 and 𝚺2 are simultaneously diagonalizable and, finally, the fifth
assumption simply introduces notation for the joint density function of the eigenvalues of 𝚺1 and 𝚺2."
REFERENCES,0.1481675392670157,"Lemma 1. Assume that the feature vectors are equally likely to be drawn from class 1 or class 2
and Assumptions 4 hold. Then the optimal classifier is given by w∗= (𝚺1 + 𝚺2)−1(µ1 −µ2) and its
classification error is equal to the following, where 𝑄(·) is the integral of the tail of the standard
normal distribution. 1
2𝑄"
REFERENCES,0.1486910994764398,Tr(𝚺1 + 𝚺2)−1√
REFERENCES,0.14921465968586387,"1 −𝑟
√︁"
REFERENCES,0.14973821989528796,2Tr(𝚺1(𝚺1 + 𝚺2)−2) ! + 1 2𝑄
REFERENCES,0.15026178010471203,Tr(𝚺1 + 𝚺2)−1√
REFERENCES,0.15078534031413612,"1 −𝑟
√︁"
REFERENCES,0.15130890052356022,2Tr(𝚺2(𝚺1 + 𝚺2)−2) !
REFERENCES,0.1518324607329843,"B
Proof of Theorem 1"
REFERENCES,0.15235602094240838,In this section we provide the complete proof of Theorem 1.
REFERENCES,0.15287958115183245,"B.1
Proof of part 1 of Theorem 1 when Ψ = Φ𝜆"
REFERENCES,0.15340314136125655,"We prove the universality of the objective value and then use it to prove the universality of a 𝜓applied
to the optimal solutions. To do so, we construct the perturbed objective. Let"
REFERENCES,0.15392670157068064,"Φ𝜆,𝜖(A) := 1"
REFERENCES,0.1544502617801047,"𝑛min
𝑤∈S𝑤
𝜆
2 ∥Aw −y∥2 + 𝑓(w) + 𝜖𝜓(w)"
REFERENCES,0.1549738219895288,"Where |𝜖| is small enough that 𝜖𝜓(w) + 𝑓(w) is 𝜌-strongly convex. For such 𝜖’s, we will show that
Φ𝜆,𝜖(A) and Φ𝜆,𝜖(B) converge in probability to the same value and use that to deduce a similar
result involving 𝜓(wΦ𝜆,0(A)), 𝜓(wΦ𝜆,0(B)) . A key step is to reduce the proof to upper bounding the
difference of expectations, to do so, the following proposition will be instrumental whose proof is
given in Appendix D."
REFERENCES,0.15549738219895287,"Proposition 1. If for any Lipschitz function 𝑔: R →R and 𝑡1 > 0, P

|𝑔(Φ𝜆,𝜖(B)) −𝑐| > 𝑡1

→0
and for every twice differentiable ˜𝑔: R →R with bounded second derivative, we have that
lim𝑛→∞
EA,y ˜𝑔(Φ𝜆,𝜖(B)) −EB,y ˜𝑔(Φ𝜆,𝜖(A))
 →0 then for any 𝑡2 > 𝑡1."
REFERENCES,0.15602094240837697,"lim
𝑛→∞P

|𝑔(Φ𝜆,𝜖(A)) −𝑐| > 𝑡2

= 0"
REFERENCES,0.15654450261780103,"Thus it suffices to prove lim𝑛→∞
EA,y𝑔(Φ𝜆(B)) −EB,y𝑔(Φ𝜆(A))
 = 0 for every twice differentiable
𝑔with bounded second derivative. As stated earlier, we proceed by Lindeberg’s approach. Fixing 𝜆
and 𝜖, to simplify notation we use Φ(A) for Φ𝜆,𝜖(A). For 0 ≤𝑗≤𝑛, let 𝑞∈N be the largest such
that 𝑗≥Í𝑞
𝑙=1 𝑛𝑙, and let 𝑟= 𝑗−Í𝑞
𝑙=1 𝑛𝑙if , then ˆA 𝑗="
REFERENCES,0.15706806282722513,"(
a1,1
a1,2
. . .
a𝑞,𝑟
b𝑞,𝑟+1
. . .
b𝑘,𝑛𝑘
𝑇
𝑟< 𝑛𝑞+1

a1,1
a1,2
. . .
a𝑞,𝑟
b𝑞+1,1
. . .
b𝑘,𝑛𝑘
𝑇
𝑟= 𝑛𝑞+1"
REFERENCES,0.15759162303664923,It follwos that A = ˆA0 and B = ˆA𝑛. Then we have by a telescopic sum
REFERENCES,0.1581151832460733,"EA,y𝑔(Φ(A)) −EB,y𝑔(Φ(B))
 =
EA,B,y"
REFERENCES,0.1586387434554974,"𝑛−1
∑︁"
REFERENCES,0.15916230366492146,"𝑗=0
𝑔(Φ( ˆA 𝑗)) −𝑔(Φ( ˆA 𝑗−1)) ≤"
REFERENCES,0.15968586387434555,"𝑛−1
∑︁ 𝑗=0"
REFERENCES,0.16020942408376965,"E ˆA 𝑗,y𝑔(Φ( ˆA 𝑗)) −E ˆA 𝑗−1,y𝑔(Φ( ˆA 𝑗−1))"
REFERENCES,0.16073298429319371,Now we define a new matrix by dropping the 𝑗’th row. ˜A𝑗=
REFERENCES,0.1612565445026178,"(
a1,1
a1,2
. . .
a𝑞,𝑟−1
b𝑞,𝑟+1
. . .
b𝑘,𝑛𝑘
𝑇
𝑟< 𝑛𝑞+1

a1,1
a1,2
. . .
a𝑞,𝑟−1
b𝑞+1,1
. . .
b𝑘,𝑛𝑘
𝑇
𝑟= 𝑛𝑞+1"
REFERENCES,0.16178010471204188,Note that
REFERENCES,0.16230366492146597,Φ( ˆA𝑗) = 1
REFERENCES,0.16282722513089004,"𝑛min
𝑤∈S𝑤
𝜆
2 ∥ˆA 𝑗w −y∥2 + 𝑓(w) + 𝜖𝜓(w) = 1"
REFERENCES,0.16335078534031414,"𝑛min
w∈S𝑤
𝜆
2 ∥˜A 𝑗w −˜y∥2+𝜆"
REFERENCES,0.16387434554973823,"2 (a𝑇
𝑗w −𝑦𝑗)2 + 𝑓(w) + 𝜖𝜓(w) =: M(a𝑗)"
REFERENCES,0.1643979057591623,Φ( ˆA 𝑗−1) = 1
REFERENCES,0.1649214659685864,"𝑛min
𝑤∈S𝑤
𝜆
2 ∥ˆA 𝑗w −y∥2 + 𝑓(w) + 𝜖𝜓(w) = 1"
REFERENCES,0.16544502617801046,"𝑛min
w∈S𝑤
𝜆
2 ∥˜A 𝑗w −˜y∥2+𝜆"
REFERENCES,0.16596858638743456,"2 (b𝑇
𝑗w −𝑦𝑗)2 + 𝑓(w) + 𝜖𝜓(w) =: M(b 𝑗)"
REFERENCES,0.16649214659685863,"And we define M(a)
:=
minw∈S𝑤𝓂(a, w).
So, we need to bound
E ˆA 𝑗,y𝑔(Φ( ˆA 𝑗)) −"
REFERENCES,0.16701570680628272,"E ˆA 𝑗−1,y𝑔(Φ( ˆA 𝑗−1))
, to do so, we condition on ˜A 𝑗:
E ˆA 𝑗,y𝑔(Φ( ˆA 𝑗)) −E ˆA 𝑗−1,y𝑔(Φ( ˆA 𝑗−1))
 =
E ˜A 𝑗,˜yEa 𝑗,b𝑗,𝑦𝑗"
REFERENCES,0.16753926701570682,"
𝑔(M(a𝑗)) −𝑔(M(b 𝑗))
 ˜A 𝑗, ˜y
"
REFERENCES,0.16806282722513088,Let us define the optimization whose terms are shared by both of M(a𝑗) and M(b𝑗)
REFERENCES,0.16858638743455498,Θ := 1
REFERENCES,0.16910994764397905,"𝑛min
w∈S𝑤
𝜆
2 ∥˜A 𝑗w −˜y∥2 + 𝑓(w) + 𝜖𝜓(w)"
REFERENCES,0.16963350785340314,"Now note that since the second derivative of 𝑔is bounded, we have
𝑔(Θ) −𝑔(M(a𝑗)) −𝑔′(Θ)(M(a𝑗) −Θ)
 ≤∥𝑔′′∥∞(Θ −M(a𝑗))2
𝑔(Θ) −𝑔(M(b𝑗)) −𝑔′(Θ)(M(b𝑗) −Θ)
 ≤∥𝑔′′∥∞(Θ −M(b𝑗))2
(13)"
REFERENCES,0.17015706806282724,"Adding and subtracting 𝑔(Θ) and taking expectation and moving inside, we have
E ˜A 𝑗,˜yEa𝑗,b 𝑗,𝑦𝑗"
REFERENCES,0.1706806282722513,"
𝑔(M(a𝑗)) −𝑔(M(b𝑗))
 ˜A 𝑗, ˜y
"
REFERENCES,0.1712041884816754,"≤
E ˜A 𝑗,˜yEa𝑗,b 𝑗,𝑦𝑗"
REFERENCES,0.17172774869109947,"
𝑔(Θ) −𝑔(M(a𝑗)) −𝑔′(Θ)(M(a𝑗) −Θ)
 ˜A 𝑗, ˜y
"
REFERENCES,0.17225130890052356,"+
E ˜A 𝑗,˜yEa 𝑗,b 𝑗,𝑦𝑗"
REFERENCES,0.17277486910994763,"
𝑔(Θ) −𝑔(M(b𝑗)) −𝑔′(Θ)(M(b𝑗) −Θ)
 ˜A 𝑗, ˜y
"
REFERENCES,0.17329842931937173,"+
E ˜A 𝑗,˜yEa 𝑗,b 𝑗,𝑦𝑗"
REFERENCES,0.17382198952879582,"
𝑔′(Θ)(M(a𝑗) −M(b𝑗))
 ˜A 𝑗, ˜y
"
REFERENCES,0.1743455497382199,"Using 13, and by the independence of Θ and a𝑗, b𝑗, 𝑦𝑗, we have
E ˜A 𝑗,˜yEa𝑗,b 𝑗,𝑦𝑗"
REFERENCES,0.17486910994764399,"
𝑔(M(a𝑗)) −𝑔(M(b𝑗))
 ˜A 𝑗, ˜y
 ≤
E ˜A𝑗,˜y𝑔′(Θ)Ea 𝑗,b 𝑗,𝑦𝑗"
REFERENCES,0.17539267015706805,"
(M(a𝑗) −M(b 𝑗))
 ˜A 𝑗, ˜y
"
REFERENCES,0.17591623036649215,"+ ∥𝑔′′∥∞E ˜A 𝑗,˜y,a𝑗,𝑦𝑗(Θ −M(a𝑗))2 + ∥𝑔′′∥∞E ˜A 𝑗,˜y,b𝑗,𝑦𝑗(Θ −M(b𝑗))2"
REFERENCES,0.17643979057591622,"≤E ˜A 𝑗,˜y|𝑔′(Θ)|
Ea𝑗,b 𝑗,𝑦𝑗"
REFERENCES,0.1769633507853403,"
(M(a𝑗) −M(b𝑗))
 ˜A 𝑗, ˜y
"
REFERENCES,0.1774869109947644,"+ ∥𝑔′′∥∞

E ˜A 𝑗,˜y,a𝑗,𝑦𝑗(Θ −M(a𝑗))2 + E ˜A 𝑗,˜y,b 𝑗,𝑦𝑗(Θ −M(b𝑗))2"
REFERENCES,0.17801047120418848,"≤∥𝑔′∥∞E ˜A 𝑗,˜y"
REFERENCES,0.17853403141361257,"Ea 𝑗,b𝑗,𝑦𝑗"
REFERENCES,0.17905759162303664,"
(M(a𝑗) −M(b𝑗))
 ˜A 𝑗, ˜y
"
REFERENCES,0.17958115183246073,"+ ∥𝑔′′∥∞

E ˜A 𝑗,˜y,a𝑗,𝑦𝑗(Θ −M(a𝑗))2 + E ˜A𝑗,˜y,b𝑗,𝑦𝑗(Θ −M(b𝑗))2
(14)"
REFERENCES,0.18010471204188483,"Thus we focus on M(a 𝑗), M(𝑏𝑗) conditioned on ˜A 𝑗. From now on, we drop the index 𝑗from ˜A 𝑗,
a𝑗, b𝑗, 𝑦𝑗. Intuitively, we show that by dropping (a𝑇w −𝑦)2 and (b𝑇w −𝑦)2 and approximating
𝑓, 𝜓by their second-order Taylor expansion, the objective values Φ( ˆA 𝑗), Φ( ˆA 𝑗−1) would not change
much. Which implies Φ( ˆA 𝑗) and Φ( ˆA 𝑗) are also close in value."
REFERENCES,0.1806282722513089,Let 𝑢:= arg min 𝜆
REFERENCES,0.181151832460733,"2 ∥˜Aw −˜y∥2 + 𝑓(w) + 𝜖𝜓(w) and it is unique because of strong convexity. Now we
use the second order Taylor expansion of 𝑓+ 𝜖𝜓:"
REFERENCES,0.18167539267015706,𝑓(w) + 𝜖𝜓(w) = 𝑓(u) + 𝜖𝜓(u) + ∇( 𝑓(u) + 𝜖𝜓(u))𝑇(w −u) + 1
REFERENCES,0.18219895287958116,2 (w −u)𝑇∇2( 𝑓(u) + 𝜖𝜓(u))(w −u) + 𝑅(w −u)
REFERENCES,0.18272251308900522,"Where limw→u
𝑅(w−u)"
REFERENCES,0.18324607329842932,"∥w−u∥2 = 0. Let g := ∇( 𝑓(u) + 𝜖𝜓(u)), H := ∇2( 𝑓(u) + 𝜖𝜓(u)). Moreover, let"
REFERENCES,0.1837696335078534,"L(a) := min
w∈S𝑤ℓ(a, w) := 1"
REFERENCES,0.18429319371727748,"𝑛min
w∈S𝑤
𝜆
2 ∥˜Aw −˜y∥2 + 𝜆"
REFERENCES,0.18481675392670158,2 (a𝑇w −𝑦)2
REFERENCES,0.18534031413612564,+ 𝑓(u) + 𝜖𝜓(u) + g𝑇(w −u) + 1
REFERENCES,0.18586387434554974,2 (w −u)𝑇H(w −u)
REFERENCES,0.18638743455497384,"Note that a and b match in distribution up to the second moment, which implies that"
REFERENCES,0.1869109947643979,"Ea,𝑦(a𝑇u −𝑦)2 = Eb,𝑦(b𝑇u −𝑦)2"
REFERENCES,0.187434554973822,"Eaa𝑇𝛀−1a = Ebb𝑇𝛀−1b
By triangle inequality we obtain"
REFERENCES,0.18795811518324607,"E ˜A,˜y"
REFERENCES,0.18848167539267016,"Ea,b,𝑦"
REFERENCES,0.18900523560209423,"
(M(a) −M(b))
 ˜A, ˜y
"
REFERENCES,0.18952879581151832,"≤E ˜A,˜y Ea,𝑦"
REFERENCES,0.19005235602094242,"
(M(a) −L(a))
 ˜A, ˜y
 + E ˜A,˜y Eb,𝑦"
REFERENCES,0.1905759162303665,"
(M(b) −L(b))
 ˜A, ˜y
"
REFERENCES,0.19109947643979058,"+ E ˜A,˜y Ea,𝑦"
REFERENCES,0.19162303664921465,"
L(a) −Θ −𝜆"
REFERENCES,0.19214659685863875,"2𝑛
Ea,𝑦(a𝑇u −𝑦)2"
REFERENCES,0.19267015706806281,1 + 𝜆Eaa𝑇𝛀−1a
REFERENCES,0.1931937172774869,"˜A, ˜y
 + E ˜A,˜y Eb,𝑦"
REFERENCES,0.193717277486911,"
L(b) −Θ −𝜆"
REFERENCES,0.19424083769633507,"2𝑛
Eb,𝑦(b𝑇u −𝑦)2"
REFERENCES,0.19476439790575917,1 + 𝜆Ebb𝑇𝛀−1b
REFERENCES,0.19528795811518324,"˜A, ˜y
"
REFERENCES,0.19581151832460733,"≤E ˜A,˜y,a,𝑦|M(a) −L(a)| + E ˜A,˜y,b,𝑦|M(b) −L(b)|"
REFERENCES,0.19633507853403143,"+ E ˜A,˜y Ea,𝑦"
REFERENCES,0.1968586387434555,"
L(a) −Θ −𝜆"
REFERENCES,0.1973821989528796,"2𝑛
(a𝑇u −𝑦)2"
REFERENCES,0.19790575916230366,1 + 𝜆Eaa𝑇𝛀−1a
REFERENCES,0.19842931937172775,"˜A, ˜y
 + E ˜A,˜y Eb,𝑦"
REFERENCES,0.19895287958115182,"
L(b) −Θ −𝜆"
REFERENCES,0.19947643979057592,"2𝑛
(b𝑇u −𝑦)2"
REFERENCES,0.2,1 + 𝜆Ebb𝑇𝛀−1b
REFERENCES,0.20052356020942408,"˜A, ˜y
"
REFERENCES,0.20104712041884817,"≤E ˜A,˜y,a,𝑦|M(a) −L(a)| + E ˜A,˜y,b,𝑦|M(b) −L(b)| + 𝜆"
REFERENCES,0.20157068062827224,"2𝑛E ˜A,˜y,a,𝑦"
REFERENCES,0.20209424083769634,(a𝑇u −𝑦)2
REFERENCES,0.20261780104712043,"1 + 𝜆a𝑇𝛀−1a
−
(a𝑇u −𝑦)2"
REFERENCES,0.2031413612565445,1 + 𝜆Eaa𝑇𝛀−1a + 𝜆
REFERENCES,0.2036649214659686,"2𝑛E ˜A,˜y,b,𝑦"
REFERENCES,0.20418848167539266,(b𝑇u −𝑦)2
REFERENCES,0.20471204188481676,"1 + 𝜆b𝑇𝛀−1b
−
(b𝑇u −𝑦)2"
REFERENCES,0.20523560209424083,1 + 𝜆Ebb𝑇𝛀−1b  (15)
REFERENCES,0.20575916230366492,"First we will provide an upper bound for the third and fourth terms in 15. As the function 𝑥↦→
1
1+𝑥is
1-Lipschitz and using Cauchy Schwartz we arrive at"
REFERENCES,0.20628272251308902,"𝜆
2𝑛E ˜A,˜y,a,𝑦"
REFERENCES,0.20680628272251309,(a𝑇u −𝑦)2
REFERENCES,0.20732984293193718,"1 + 𝜆a𝑇𝛀−1a
−
(a𝑇u −𝑦)2"
REFERENCES,0.20785340314136125,1 + 𝜆Eaa𝑇𝛀−1a ≤𝜆2
REFERENCES,0.20837696335078534,"2𝑛E ˜A,˜y,a,𝑦
(a𝑇u −𝑦)2(a𝑇𝛀−1a −Eaa𝑇𝛀−1a) (16) ≤𝜆2 2𝑛 √︃"
REFERENCES,0.2089005235602094,"E ˜A,˜y,a,𝑦(a𝑇u −𝑦)4E ˜A,˜y,a,𝑦(a𝑇𝛀−1a −Eaa𝑇𝛀−1a)2 (17)"
REFERENCES,0.2094240837696335,"By Lemma 7, there exists a constant 𝐶1 < ∞such that E ˜A,˜y,a,𝑦(a𝑇u−𝑦)4 < 𝐶1 for any 𝑛. Furthermore,
by the assumptions, lim𝑛→∞E ˜A,˜y,a,𝑦(a𝑇𝛀−1a −Eaa𝑇𝛀−1a)2 = 0."
REFERENCES,0.2099476439790576,"Now for the first and second terms in (15), using Lemma 5, we obtain with probability 1,"
REFERENCES,0.21047120418848167,|M(a) −L(a)| ≤8𝐶𝑓+𝜖𝜓
REFERENCES,0.21099476439790577,"𝑛
∥u −wL∥3
31{∥u −wL∥3 ≤
𝜌
18𝐶𝑓+𝜖𝜓
} + 𝜆"
REFERENCES,0.21151832460732983,"2𝑛(a𝑇u −𝑦)21{∥u −wL∥3 >
𝜌
18𝐶𝑓+𝜖𝜓
}"
REFERENCES,0.21204188481675393,Which implies
REFERENCES,0.21256544502617802,"E ˜A,˜y,a,𝑦|M(a) −L(a)| ≤8𝐶𝑓+𝜖𝜓"
REFERENCES,0.2130890052356021,"𝑛
E ˜A,˜y,a,𝑦"
REFERENCES,0.2136125654450262,"
∥u −wL∥3
31{∥u −wL∥3 ≤
𝜌
18𝐶𝑓+𝜖𝜓
}
 + 𝜆"
REFERENCES,0.21413612565445025,"2𝑛E ˜A,˜y,a,𝑦"
REFERENCES,0.21465968586387435,"
(a𝑇u −𝑦)21{∥u −wL∥3 >
𝜌
18𝐶𝑓+𝜖𝜓
}
"
REFERENCES,0.21518324607329842,We apply Cauchy-Schwarz to each term:
REFERENCES,0.2157068062827225,"E ˜A,˜y,a,𝑦|M(a) −L(a)| ≤8𝐶𝑓+𝜖𝜓"
REFERENCES,0.2162303664921466,"𝑛
E ˜A,˜y,a,𝑦∥u −wL∥3
3 + 𝜆 2𝑛 √︂"
REFERENCES,0.21675392670157068,"E ˜A,˜y,a,𝑦12{∥u −wL∥3 >
𝜌
18𝐶𝑓+𝜖𝜓
}E ˜A,˜y,a,𝑦(a𝑇u −𝑦)4"
REFERENCES,0.21727748691099477,= 8𝐶𝑓+𝜖𝜓
REFERENCES,0.21780104712041884,"𝑛
E ˜A,˜y,a,𝑦∥u −wL∥3
3 + 𝜆 2𝑛 √︂"
REFERENCES,0.21832460732984293,"P ˜A,˜y,a,𝑦(∥u −wL∥3 >
𝜌
18𝐶𝑓+𝜖𝜓
)E ˜A,˜y,a,𝑦(a𝑇u −𝑦)4"
REFERENCES,0.218848167539267,"To deal with the terms involving P, we leverage Markov’s inequality"
REFERENCES,0.2193717277486911,"E ˜A,˜y,a,𝑦|M(a) −L(a)| ≤8𝐶𝑓+𝜖𝜓"
REFERENCES,0.2198952879581152,"𝑛
E ˜A,˜y,a,𝑦∥u −wL∥3
3 + 𝜆"
REFERENCES,0.22041884816753926,"2𝑛
 18𝐶𝑓+𝜖𝜓"
REFERENCES,0.22094240837696336,"𝜌
3/2√︃"
REFERENCES,0.22146596858638742,"E ˜A,˜y,a,𝑦∥u −wL∥3
3E ˜A,˜y,a,𝑦(a𝑇u −𝑦)4"
REFERENCES,0.22198952879581152,"By Lemma 7, there exists a 𝐶2 < ∞such that E ˜A,˜y,a,𝑦∥u−wL∥3
3 ≤𝐶2"
REFERENCES,0.22251308900523561,"𝑑, and recall that E ˜A,˜y,a,𝑦(a𝑇u−
𝑦)4 < 𝐶1 for any 𝑛. Thus"
REFERENCES,0.22303664921465968,"E ˜A,˜y,a,𝑦|M(a) −L(a)| ≤8𝐶2𝐶𝑓+𝜖𝜓"
REFERENCES,0.22356020942408378,"𝑛𝑑
+ 𝜆√𝐶1 2𝑛
√ 𝑑"
REFERENCES,0.22408376963350785, 18𝐶𝑓+𝜖𝜓
REFERENCES,0.22460732984293194,"𝜌
3/2 ≤𝐶𝑎 𝑛
√"
REFERENCES,0.225130890052356,"𝑑
(18)"
REFERENCES,0.2256544502617801,For some constant ˜𝐶= 𝜆√𝐶1
REFERENCES,0.2261780104712042,"2
  18𝐶𝑓+𝜖𝜓"
REFERENCES,0.22670157068062827,"𝜌
3/2 + 8𝐶2𝐶𝑓+𝜖𝜓. Plugging (16) and (18) in (15), yields"
REFERENCES,0.22722513089005236,"E ˜A,˜y"
REFERENCES,0.22774869109947643,"Ea,b,𝑦"
REFERENCES,0.22827225130890053,"
(M(a) −M(b))
 ˜A, ˜y
 ≤
˜𝐶+ ˜𝐶′ 𝑛
√"
REFERENCES,0.22879581151832462,"𝑑
+ 𝜆2 ˜𝐶 2𝑛 √︃"
REFERENCES,0.2293193717277487,"E ˜A,˜y𝑉𝑎𝑟a(a𝑇𝛀−1a) + 𝜆2 ˜𝐶′ 2𝑛 √︃"
REFERENCES,0.22984293193717278,"E ˜A,˜y𝑉𝑎𝑟b(b𝑇𝛀−1b)"
REFERENCES,0.23036649214659685,"For the other terms in (14), we have by Lemmas 4, 7:"
REFERENCES,0.23089005235602095,"∥𝑔′′∥∞E ˜A,˜y,a,𝑦(Θ −M(a))2+∥𝑔′′∥∞E ˜A,˜y,b,𝑦(Θ −M(b))2"
REFERENCES,0.23141361256544501,"≤∥𝑔′′∥∞E ˜A,˜y,a,𝑦
𝜆2"
REFERENCES,0.2319371727748691,"4𝑛2 (a𝑇u −𝑦)2 + ∥𝑔′′∥∞E ˜A,˜y,b,𝑦
𝜆2"
REFERENCES,0.2324607329842932,4𝑛2 (b𝑇u −𝑦)2
REFERENCES,0.23298429319371727,"≤
𝜆2∥𝑔′′∥∞( ˜𝐶1 + ˜𝐶′
1)"
REFERENCES,0.23350785340314137,"4𝑛2
Plugging in (14), we obtain
E ˜A,˜yEa,b,𝑦"
REFERENCES,0.23403141361256544,"
𝑔(M(a)) −𝑔(M(b))
 ˜A, ˜y
"
REFERENCES,0.23455497382198953,≤∥𝑔′∥∞
REFERENCES,0.2350785340314136," ˜𝐶+ ˜𝐶′ 𝑛
√"
REFERENCES,0.2356020942408377,"𝑑
+ 𝜆2 ˜𝐶"
REFERENCES,0.2361256544502618,"2𝑛E ˜A,˜y𝑉𝑎𝑟a(a𝑇𝛀−1a) + 𝜆2 ˜𝐶′"
REFERENCES,0.23664921465968586,"2𝑛E ˜A,˜y𝑉𝑎𝑟b(b𝑇𝛀−1b)

+ 𝜆2∥𝑔′′∥∞( ˜𝐶+ ˜𝐶′) 4𝑛2"
REFERENCES,0.23717277486910995,"Therefore,
EA,y𝑔(Φ(A)) −EB,y𝑔(Φ(B))
 ≤"
REFERENCES,0.23769633507853402,"𝑛−1
∑︁ 𝑗=0"
REFERENCES,0.23821989528795812,"E ˆA 𝑗,y𝑔(Φ( ˆA 𝑗)) −E ˆA 𝑗−1,y𝑔(Φ( ˆA 𝑗−1)) ≤"
REFERENCES,0.2387434554973822,"𝑛−1
∑︁"
REFERENCES,0.23926701570680628,"𝑗=0
∥𝑔′∥∞"
REFERENCES,0.23979057591623038," ˜𝐶+ ˜𝐶′ 𝑛
√"
REFERENCES,0.24031413612565444,"𝑑
+ 𝜆2 ˜𝐶"
REFERENCES,0.24083769633507854,"2𝑛E ˜A 𝑗,˜y𝑗𝑉𝑎𝑟a 𝑗(a𝑇
𝑗𝛀−1a𝑗) + 𝜆2 ˜𝐶′"
REFERENCES,0.2413612565445026,"2𝑛E ˜A 𝑗,˜y𝑗𝑉𝑎𝑟b 𝑗(b𝑇
𝑗𝛀−1b𝑗)

+ 𝜆2∥𝑔′′∥∞( ˜𝐶+ ˜𝐶′) 4𝑛2"
REFERENCES,0.2418848167539267,"= ∥𝑔′∥∞( ˜𝐶+ ˜𝐶′)
√"
REFERENCES,0.2424083769633508,"𝑑
+ 𝜆2∥𝑔′′∥∞( ˜𝐶+ ˜𝐶′) 4𝑛 +"
REFERENCES,0.24293193717277486,"𝑛−1
∑︁"
REFERENCES,0.24345549738219896,"𝑗=0
∥𝑔′∥∞"
REFERENCES,0.24397905759162303,𝜆2 ˜𝐶 2𝑛 √︃
REFERENCES,0.24450261780104712,"E ˜A 𝑗,˜y𝑗𝑉𝑎𝑟a𝑗(a𝑇
𝑗𝛀−1a 𝑗) + 𝜆2 ˜𝐶′ 2𝑛 √︃"
REFERENCES,0.2450261780104712,"E ˜A 𝑗,˜y𝑗𝑉𝑎𝑟b 𝑗(b𝑇
𝑗𝛀−1b𝑗)
"
REFERENCES,0.24554973821989529,Now note that since ∥𝛀∥𝑜𝑝≤1
REFERENCES,0.24607329842931938,"𝜌with probability 1, lim𝑛→∞𝑉𝑎𝑟a 𝑗(a𝑇
𝑗𝛀−1a𝑗) = 0 from assumptions,
thus for every class [ 𝑗], there exists a function 𝜁[ 𝑗](𝑛) that 𝑉𝑎𝑟a𝑗(a𝑇
𝑗𝛀−1a𝑗) ≤𝜁2
[ 𝑗](𝑛) for every
𝑛∈N and lim𝑛→∞𝜁[ 𝑗](𝑛) = 0. Thus
EA,y𝑔(Φ(A)) −EB,y𝑔(Φ(B))
 ≤∥𝑔′∥∞( ˜𝐶+ ˜𝐶′)
√"
REFERENCES,0.24659685863874345,"𝑑
+ 𝜆2∥𝑔′′∥∞( ˜𝐶+ ˜𝐶′)"
REFERENCES,0.24712041884816754,"4𝑛
+ 𝜆2∥𝑔′∥∞( ˜𝐶+ ˜𝐶′) 𝑘
∑︁ 𝑖=1"
REFERENCES,0.2476439790575916,"𝑛𝑖
2𝑛𝜁𝑖(𝑛)"
REFERENCES,0.2481675392670157,"≤∥𝑔′∥∞( ˜𝐶+ ˜𝐶′)
√"
REFERENCES,0.2486910994764398,"𝑑
+ 𝜆2∥𝑔′′∥∞( ˜𝐶+ ˜𝐶′)"
REFERENCES,0.24921465968586387,"4𝑛
+ 𝜆2∥𝑔′∥∞( ˜𝐶+ ˜𝐶′) max
1≤𝑖≤𝑘
𝑛𝑖
2𝑛𝜁𝑖(𝑛)"
REFERENCES,0.24973821989528797,"Now since 𝑘is finite and 𝑛𝑖= Θ(𝑛) for every 1 ≤𝑖≤𝑘, we have that lim𝑛→∞max1≤𝑖≤𝑘
𝑛𝑖
2𝑛𝜁𝑖(𝑛) = 0.
Thus for every 𝜆, 𝜖> 0"
REFERENCES,0.25026178010471206,"lim
𝑛→∞"
REFERENCES,0.25078534031413613,"EA,y𝑔(Φ𝜆,𝜖(A)) −EB,y𝑔(Φ𝜆,𝜖(B))
 = 0"
REFERENCES,0.2513089005235602,and the proof of part 1 of Theorem 1 is concluded.
REFERENCES,0.2518324607329843,"B.2
Proof of part 1 of Theorem 1 when Ψ = Φ"
REFERENCES,0.2523560209424084,"Now to prove the part 1 of Theorem 1, we will combine the following lemma with the previous
section. For the ease of notation, we drop the dependency on 𝜖. Recall that since 𝑓is 𝜌-strongly
convex, we can write"
REFERENCES,0.25287958115183246,"Φ𝑛
𝜆(𝐴) = min
w
𝜆
2 ∥Aw −y∥2
2 + 𝜌"
REFERENCES,0.2534031413612565,"2 ∥w∥2
2 + ℎ(w)"
REFERENCES,0.25392670157068065,For some convex ℎ: R𝑑→R.
REFERENCES,0.2544502617801047,"Lemma 2. If for every 𝜆> 0, we have Φ𝑛
𝜆(𝐴)
P−→𝑐𝜆, then for every Lipschitz function 𝑔: R →R,"
REFERENCES,0.2549738219895288,"we have 𝑔(sup𝜆>0 Φ𝑛
𝜆(𝐴))
P−→𝑔(sup𝜆>0 𝑐𝜆) and 𝑔(sup𝜆>0 Φ𝑛
𝜆(𝐵))
P−→𝑔(sup𝜆>0 𝑐𝜆)"
REFERENCES,0.2554973821989529,"Proof. First note that for a given 𝑡> 0, we have for every Lipschitz function 𝑔"
REFERENCES,0.256020942408377,"P(|𝑔(Φ𝑛
𝜆) −𝑔(𝑐𝜆)| > 𝑡) ≤P(|Φ𝑛
𝜆−𝑐𝜆| > 𝑡 𝐿)"
REFERENCES,0.25654450261780104,Letting 𝛿:= 𝑡
REFERENCES,0.2570680628272251,"𝐿, ˜𝛿1 > 0 , we know that for every 𝜆≥0, there exists 𝑁1 ∈N such that for every 𝑛≥𝑁1"
REFERENCES,0.25759162303664923,"P(|Φ𝑛
𝜆−𝑐𝜆| > 𝛿) < ˜𝛿1"
REFERENCES,0.2581151832460733,"We will show that an 𝑅1 > 0 can be chosen in such a way that there exists 𝑁2 ∈N such that for every
𝑛≥𝑁2"
REFERENCES,0.25863874345549737,"P(sup
𝜆>0
Φ𝑛
𝜆−
sup
0<𝜆<𝑅1
Φ𝑛
𝜆> 𝛿) ≤˜𝛿2"
REFERENCES,0.2591623036649215,"To see this, by the monotonicity of Φ𝑛
𝜆in 𝜆, we have by the fundamental theorem of calculus for a
given 𝑅1"
REFERENCES,0.25968586387434556,"P(sup
𝜆>0
Φ𝑛
𝜆−
sup
0<𝜆<𝑅1
Φ𝑛
𝜆> 𝛿) = P( lim
𝜆→∞Φ𝑛
𝜆−Φ𝑛
𝑅1 > 𝛿) = P
∫∞ 𝑅1"
REFERENCES,0.2602094240837696,"𝜕Φ𝑛
𝜆
𝜕𝜆𝑑𝑠> 𝛿
"
REFERENCES,0.2607329842931937,"By Danskin’s theorem,"
REFERENCES,0.2612565445026178,"P(sup
𝜆>0
Φ𝑛
𝜆−
sup
0<𝜆<𝑅1
Φ𝑛
𝜆> 𝛿) = P
1 𝑛 ∫∞"
REFERENCES,0.2617801047120419,"𝑅1
∥Aw(𝑠) −y∥2𝑑𝑠> 𝛿

(19)"
REFERENCES,0.26230366492146595,"Now for the gradient of the main optimization we have, by 𝑔:= ∇ℎ(w)"
REFERENCES,0.2628272251308901,𝜆A𝑇(Aw −y) + 𝜌w + g = 0
REFERENCES,0.26335078534031414,"Which implies w = (𝜆A𝑇A + 𝜌I)−1(𝜆A𝑇y −g). Hence by matrix inversion lemma,"
REFERENCES,0.2638743455497382,Aw −y = −(I −A(𝜆A𝑇A + 𝜌I)−1𝜆A𝑇)y −A(𝜆A𝑇A + 𝜌I)−1g
REFERENCES,0.2643979057591623,= −𝜌(𝜆AA𝑇+ 𝜌𝐼)−1y −A(𝜆A𝑇A + 𝜌I)−1g = −𝜌(𝜆AA𝑇+ 𝜌𝐼)−1y −(𝜆AA𝑇+ 𝜌𝐼)−1Ag
REFERENCES,0.2649214659685864,By triangle inequality and definition of the operator norm we have
REFERENCES,0.26544502617801047,"∥Aw + y∥2
2 ≤4𝜌2∥(𝜆AA𝑇+ 𝜌𝐼)−1∥2
𝑜𝑝∥y∥2
2 + 4∥(𝜆AA𝑇+ 𝜌𝐼)−1A∥2
𝑜𝑝∥g∥2
2
Whence plugging back in 19 yields"
REFERENCES,0.26596858638743454,"P(sup
𝜆>0
Φ𝑛
𝜆−
sup
0<𝜆<𝑅1
Φ𝑛
𝜆> 𝛿) ≤P
4 𝑛 ∫∞"
REFERENCES,0.26649214659685866,"𝑅1
𝜌2∥(𝑠AA𝑇+ 𝜌𝐼)−1∥2
𝑜𝑝∥y∥2
2 + ∥(𝑠AA𝑇+ 𝜌𝐼)−1A∥2
𝑜𝑝∥g(𝑠)∥2
2𝑑𝑠> 𝛿
"
REFERENCES,0.2670157068062827,"≤P
4𝜌2∥y∥2
2
𝑛 ∫∞"
REFERENCES,0.2675392670157068,"𝑅1
∥(𝑠AA𝑇+ 𝜌𝐼)−1∥2
𝑜𝑝> 𝛿 2 "
REFERENCES,0.2680628272251309,"+ P
4 𝑛 ∫∞"
REFERENCES,0.268586387434555,"𝑅1
∥(𝑠AA𝑇+ 𝜌𝐼)−1A∥2
𝑜𝑝∥g(𝑠)∥2
2𝑑𝑠> 𝛿 2 "
REFERENCES,0.26910994764397905,"For the norm of g, by assumption, there exists a 𝐶𝑔independent of 𝜆, such that
∥g(𝑠) ∥2
2
𝑛
≤𝐶𝑔with
high probability."
REFERENCES,0.2696335078534031,Consider the following the events
REFERENCES,0.27015706806282724,"T𝑦:=
n ∥y∥2
2
𝑛
≤𝐶𝑦
o"
REFERENCES,0.2706806282722513,"T𝑔:=
n ∥g(𝑠)∥2
2
𝑛
≤𝐶𝑔
o"
REFERENCES,0.2712041884816754,"T𝐴:=
n
𝑠min(AA𝑇) ≥𝐶𝐴
o"
REFERENCES,0.2717277486910995,We further have
REFERENCES,0.27225130890052357,"P
4𝜌2∥y∥2
2
𝑛 ∫∞"
REFERENCES,0.27277486910994764,"𝑅1
∥(𝑠AA𝑇+ 𝜌𝐼)−1∥2
𝑜𝑝𝑑𝑠> 𝛿 2"
REFERENCES,0.2732984293193717,"
≤P(T 𝑐
𝑦) + P

4𝜌2𝐶𝑦 ∫∞"
REFERENCES,0.27382198952879583,"𝑅1
∥(𝑠AA𝑇+ 𝜌𝐼)−1∥2
𝑜𝑝𝑑𝑠> 𝛿 2  P
4 𝑛 ∫∞"
REFERENCES,0.2743455497382199,"𝑅1
∥(𝑠AA𝑇+ 𝜌𝐼)−1A∥2
𝑜𝑝∥g(𝑠)∥2
2𝑑𝑠> 𝛿 2"
REFERENCES,0.27486910994764396,"
≤P(T 𝑐
𝑔) + P

4𝐶𝑔 ∫∞"
REFERENCES,0.2753926701570681,"𝑅1
∥(𝑠AA𝑇+ 𝜌𝐼)−1A∥2
𝑜𝑝𝑑𝑠> 𝛿 2  (20)"
REFERENCES,0.27591623036649215,"Thus we need to focus on the operator norm of each term in 20. After diagonalizing and using the
fact that for any two rectangular matrices X, Y with matching dimensions, 𝑠𝑝𝑒𝑐(XY) = 𝑠𝑝𝑒𝑐(YX)"
REFERENCES,0.2764397905759162,"∥(𝜆AA𝑇+ 𝜌𝐼)−1∥2
𝑜𝑝= max
1≤𝑖≤𝑛
1
(𝜆𝑠𝑖(AA𝑇) + 𝜌)2 =
1
(𝜆𝑠min(AA𝑇) + 𝜌)2"
REFERENCES,0.2769633507853403,"∥(𝜆AA𝑇+ 𝜌𝐼)−1A∥2
𝑜𝑝= max
1≤𝑖≤𝑛
𝑠𝑖(AA𝑇)
(𝜆𝑠𝑖(AA𝑇) + 𝜌)2"
REFERENCES,0.2774869109947644,"Now by the event T𝐴, 𝑠min(AA𝑇) ≥𝐶𝐴, then for a large enough 𝑅1 such that 𝜌"
REFERENCES,0.2780104712041885,"𝑅1 ≤𝐶𝐴, then for every
𝜆≥𝑅1"
REFERENCES,0.27853403141361255,"∥(𝜆AA𝑇+ 𝜌𝐼)−1∥2
𝑜𝑝≤
1
(𝜆𝐶𝐴+ 𝜌)2"
REFERENCES,0.27905759162303667,"∥(𝜆AA𝑇+ 𝜌𝐼)−1A∥2
𝑜𝑝≤
𝐶𝐴
(𝜆𝐶𝐴+ 𝜌)2"
REFERENCES,0.27958115183246074,"Bounding the integrals implies
∫∞"
REFERENCES,0.2801047120418848,"𝑅1
∥(𝑠AA𝑇+ 𝜌𝐼)−1∥2
𝑜𝑝𝑑𝑠≤1 𝐶𝐴"
REFERENCES,0.2806282722513089,"1
𝐶𝐴𝑅1 + 𝜌
∫∞"
REFERENCES,0.281151832460733,"𝑅1
∥(𝑠AA𝑇+ 𝜌𝐼)−1A∥2
𝑜𝑝𝑑𝑠≤
1
𝐶𝐴𝑅1 + 𝜌"
REFERENCES,0.28167539267015707,"Summarizing,"
REFERENCES,0.28219895287958113,"P(sup
𝜆>0
Φ𝑛
𝜆−
sup
0<𝜆<𝑅1
Φ𝑛
𝜆> 𝛿) ≤P

4𝜌2𝐶𝑦
𝐶𝐴(𝐶𝐴𝑅1 + 𝜌) > 𝛿 2"
REFERENCES,0.28272251308900526,"
+ P

𝐶𝑔
𝐶𝐴𝑅1 + 𝜌> 𝛿 2"
REFERENCES,0.2832460732984293,"
+ P(T 𝑐
𝑦) + P(T 𝑐
𝑔) + 2P(T 𝑐
𝐴) (21)"
REFERENCES,0.2837696335078534,"Now we choose 𝑅1 to be large enough such that the first two terms vanish. Therefore there exists an
𝑁2 ∈N such that for every 𝑛≥𝑁2, P(T 𝑐
𝑦) + P(T 𝑐
𝑔) + 2P(T 𝑐
𝐴) ≤˜𝛿2. Thus for such 𝑅1 and 𝑁2, we
have"
REFERENCES,0.28429319371727746,"P(sup
𝜆>0
Φ𝑛
𝜆−
sup
0<𝜆<𝑅1
Φ𝑛
𝜆> 𝛿) ≤˜𝛿2"
REFERENCES,0.2848167539267016,Now we can let 𝑅2 > 0 be chosen in such a way that:
REFERENCES,0.28534031413612565,"sup
𝜆>0
𝑐𝜆−
sup
0<𝜆<𝑅2
𝑐𝜆< ˜𝛿"
REFERENCES,0.2858638743455497,"Take the maximum of 𝑅1 and 𝑅2 as 𝑅. By triangle inequality we observe
P(| sup
𝜆>0
Φ𝑛
𝜆−sup
𝜆>0
𝑐𝜆| > 𝛿) ≤P(| sup
0<𝜆<𝑅
Φ𝑛
𝜆−sup
𝜆>0
𝑐𝜆| > 𝛿) + P(sup
0<𝜆
Φ𝑛
𝜆−sup
0<𝜆<𝑅
Φ𝑛
𝜆> 𝛿)"
REFERENCES,0.28638743455497384,"≤P(| sup
0<𝜆<𝑅
Φ𝑛
𝜆−sup
0<𝜆<𝑅
𝑐𝜆| > 𝛿) + P(sup
𝜆>0
𝑐𝜆−sup
0<𝜆<𝑅
𝑐𝜆> 𝛿) + P(sup
0<𝜆
Φ𝑛
𝜆−sup
0<𝜆<𝑅
Φ𝑛
𝜆> 𝛿)"
REFERENCES,0.2869109947643979,"≤P(| sup
0<𝜆<𝑅
Φ𝑛
𝜆−sup
0<𝜆<𝑅
𝑐𝜆| > 𝛿) + 0 + ˜𝛿1"
REFERENCES,0.287434554973822,"≤˜𝛿1 + P( sup
0<𝜆<𝑅
|Φ𝑛
𝜆−𝑐𝜆| > 𝛿)"
REFERENCES,0.2879581151832461,"≤˜𝛿1 + P( sup
0≤𝜆≤𝑅
|Φ𝑛
𝜆−𝑐𝜆| > 𝛿)"
REFERENCES,0.28848167539267017,"First we prove 𝑐𝜆is concave in 𝜆. Then by appealing to the Convexity Lemma (lemma 7.75 in
Liese and Miescke [2008]), the result follows. Note that Φ𝑛
𝜆is concave in 𝜆as it is the pointwise-
minimum of a concave function in 𝜆everywhere. Now to prove the concavity of 𝑐𝜆, for a given pair of
𝜆1, 𝜆2 ∈[0, 𝑅] and 0 < 𝜃< 1, we prove the deterministic event {𝑐𝜃𝜆1+(1−𝜃)𝜆2 −𝜃𝑐𝜆1 −(1−𝜃)𝑐𝜆2 < 0}
has probability zero. First note that
Φ𝑛
𝜃𝜆1+(1−𝜃)𝜆2 −𝜃Φ𝑛
𝜆1 −(1 −𝜃)Φ𝑛
𝜆2 ≥0"
REFERENCES,0.28900523560209423,Thus by union bound for 𝜖> 0
REFERENCES,0.2895287958115183,"P

𝜃𝑐𝜆1+(1 −𝜃)𝑐𝜆2 −𝑐𝜃𝜆1+(1−𝜃)𝜆2 > 𝜖
"
REFERENCES,0.2900523560209424,"≤P

𝜃(𝑐𝜆1 −Φ𝑛
𝜆1) + (1 −𝜃)(𝑐𝜆2 −Φ𝑛
𝜆2) + Φ𝑛
𝜃𝜆1+(1−𝜃)𝜆2 −𝑐𝜃𝜆1+(1−𝜃)𝜆2 > 𝜖
"
REFERENCES,0.2905759162303665,"≤P

𝜃|𝑐𝜆1 −Φ𝑛
𝜆1| + (1 −𝜃)|𝑐𝜆2 −Φ𝑛
𝜆2| + |Φ𝑛
𝜃𝜆1+(1−𝜃)𝜆2 −𝑐𝜃𝜆1+(1−𝜃)𝜆2| > 𝜖
"
REFERENCES,0.29109947643979056,"≤P

𝜃|𝑐𝜆1 −Φ𝑛
𝜆1| > 𝜖 3"
REFERENCES,0.2916230366492147,"
+ P

(1 −𝜃)|𝑐𝜆2 −Φ𝑛
𝜆2| > 𝜖 3"
REFERENCES,0.29214659685863875,"
+ P

|Φ𝑛
𝜃𝜆1+(1−𝜃)𝜆2 −𝑐𝜃𝜆1+(1−𝜃)𝜆2| > 𝜖 3 "
REFERENCES,0.2926701570680628,"Now by taking 𝑛to be large enough, the RHS can be made arbitrarily small for every 𝜖and 𝜃, thus 𝑐𝜆
is concave. Furthermore, since [0, 𝑅] is a compact set, we can appeal to the convexity lemma 7.75 in
Liese and Miescke [2008] and the first claim follows. For the second claim, note that so far we have
proved"
REFERENCES,0.2931937172774869,"Φ𝜆(𝐴)
P−→𝑐𝜆=⇒sup
𝜆>0
Φ𝜆(𝐴)
P−→sup
𝜆>0
𝑐𝜆"
REFERENCES,0.293717277486911,"Then since |Φ𝜆(𝐵) −Φ𝜆(𝐴)|
P−→0 thus Φ𝜆(𝐵)
P−→𝑐𝜆, repeating the same argument this time for
Φ𝜆(𝐵) yields"
REFERENCES,0.2942408376963351,"sup
𝜆>0
Φ𝜆(𝐵)
P−→sup
𝜆>0
𝑐𝜆 □"
REFERENCES,0.29476439790575915,"B.2.1
Sequence of Regularizers"
REFERENCES,0.29528795811518327,"We prove that by constraining ourselves to a large enough compact set Sw, if 𝑓𝑚→𝑓uniformly,
then the universality results also hold. Note for the Φ𝜆case, we have that by setting w = 0 in the
optimization, we have ∥wΦ𝜆(A) ∥2 ≤𝐶𝑤
√𝑛for some 𝐶𝑤> 0 with high probability, thus we can
instead set Sw := 𝐶𝑤
√𝑛and consider the following equivalent constrained optimization problem"
REFERENCES,0.29581151832460734,"˜Φ𝑚
𝜆,𝜖(A) := 1"
REFERENCES,0.2963350785340314,"𝑛min
w∈Sw
𝜆
2 ∥Aw −y∥2
2 + 𝑓𝑚(w) + 𝜖𝜓(w)"
REFERENCES,0.29685863874345547,"Now consider a sequence of regular functions 𝑓𝑚, converging uniformly to 𝑓on Sw. Take 𝑚large
enough such that ∥𝑓−𝑓𝑚∥∞≤𝑛𝛿for any 𝑛which implies | ˜Φ𝑚
𝜆,𝜖(A) −˜Φ𝜆,𝜖(A)| ≤𝛿. Thus for any
𝑡> 0"
REFERENCES,0.2973821989528796,"P
 ˜Φ𝜆,𝜖(A) −𝑐𝜆,𝜖
 > 𝑡

≤P
 ˜Φ𝜆,𝜖(A) −˜Φ𝑚
𝜆,𝜖(A)
 > 𝑡 2"
REFERENCES,0.29790575916230366,"
+ P
 ˜Φ𝑚
𝜆,𝜖(A) −𝑐𝑚
𝜆,𝜖
 > 𝑡 2"
REFERENCES,0.29842931937172773,"
(22)"
REFERENCES,0.29895287958115185,"If ˜Φ𝑚
𝜆,𝜖(A)
P−→𝑐𝑚
𝜆,𝜖, then by (22), we have ˜Φ𝜆,𝜖(A)
P−→𝑐𝜆,𝜖. A similar argument holds for"
REFERENCES,0.2994764397905759,"˜Φ𝑚
𝜖(A) := 1"
REFERENCES,0.3,"𝑛
min
Aw=y,w∈Sw 𝑓𝑚(w) + 𝜖𝜓(w)"
REFERENCES,0.30052356020942406,"Furthermore, subsequent results also hold for 𝜓(wΦ𝑚
𝜆(A)) and 𝜓(wΦ𝜆(A)), 𝜓(wΦ𝑚(A)) and 𝜓(wΦ(A))"
REFERENCES,0.3010471204188482,"B.3
Proof of Part 2 of Theorem 1"
REFERENCES,0.30157068062827225,"B.3.1
Proof of Part 2 of Theorem 1 for a regular test function when Ψ = Φ𝜆"
REFERENCES,0.3020942408376963,"Now we prove the second part of Theorem 1. Since we assumed Φ𝜆,0(A) converges to some 𝑐𝜆,0 in"
REFERENCES,0.30261780104712044,"probability, then for a small enough 𝜖, we also have Φ𝜆,𝜖(A)
P−→𝑐𝜆,𝜖. Therefore we may write by an
application of triangle inequality and the union bound"
REFERENCES,0.3031413612565445,"P

Φ𝜆,𝜖(A) −Φ𝜆,0(A)"
REFERENCES,0.3036649214659686,"𝜖
−𝑐𝜆,𝜖−𝑐𝜆,0 𝜖"
REFERENCES,0.3041884816753927,"> 𝑡

≤P
Φ𝜆,𝜖(A) −𝑐𝜆,𝜖
 > 𝜖𝑡 2"
REFERENCES,0.30471204188481676,"
+ P
Φ𝜆,0(A) −𝑐𝜆,0
 > 𝜖𝑡 2  (23)"
REFERENCES,0.30523560209424083,"For
every
𝜖, 𝑡
>
0,
the
RHS
of
(23)
goes
to
zero.
Similarly
we
have"
REFERENCES,0.3057591623036649,"P

Φ𝜆,0(A)−Φ𝜆,−𝜖(A)"
REFERENCES,0.306282722513089,"𝜖
−𝑐𝜆,0−𝑐𝜆,−𝜖 𝜖"
REFERENCES,0.3068062827225131,"> 𝑡

→0."
REFERENCES,0.30732984293193716,Now we observe that by triangle inequality
REFERENCES,0.3078534031413613,"P
𝜓(wΦ𝜆(A)) −𝑑𝑐𝜆,𝜖 𝑑𝜖 𝜖=0"
REFERENCES,0.30837696335078535,"> 𝑡

≤P

𝜓(wΦ𝜆(A)) > 𝑑𝑐𝜆,𝜖 𝑑𝜖"
REFERENCES,0.3089005235602094,"𝜖=0+𝑡

+ P

𝜓(wΦ𝜆(A)) < 𝑑𝑐𝜆,𝜖 𝑑𝜖"
REFERENCES,0.3094240837696335,"𝜖=0−𝑡
"
REFERENCES,0.3099476439790576,"We take ˆ𝜖to be small enough such that

𝑑𝑐𝜆,𝜖 𝑑𝜖"
REFERENCES,0.3104712041884817,"𝜖=0−𝑐𝜆, ˆ𝜖−𝑐𝜆,0 ˆ𝜖 < 𝑡"
REFERENCES,0.31099476439790574,"2,

𝑑𝑐𝜆,𝜖 𝑑𝜖"
REFERENCES,0.31151832460732987,"𝜖=0−𝑐𝜆,0 −𝑐𝜆,−ˆ𝜖 ˆ𝜖 < 𝑡 2"
REFERENCES,0.31204188481675393,This implies
REFERENCES,0.312565445026178,"P
𝜓(wΦ𝜆(A)) −𝑑𝑐𝜆,𝜖 𝑑𝜖 𝜖=0"
REFERENCES,0.31308900523560207,"> 𝑡

≤P

𝜓(wΦ𝜆(A)) > 𝑐𝜆,0 −𝑐𝜆,−ˆ𝜖"
REFERENCES,0.3136125654450262,"ˆ𝜖
+ 𝑡 2"
REFERENCES,0.31413612565445026,"
+ P

𝜓(wΦ𝜆(A)) < 𝑐𝜆, ˆ𝜖−𝑐𝜆,0 ˆ𝜖
−𝑡 2 "
REFERENCES,0.3146596858638743,"And by Danskin’s theorem, Φ𝜆,𝜖(A) is minimum of a concave function 𝜖, therefore it is also concave
and it is differentiable with respect to 𝜖, we observe that 𝜓(𝑤Φ𝜆(A)) = 𝑑Φ𝜆,𝜖(A) 𝑑𝜖"
REFERENCES,0.31518324607329845,"𝜖=0. Moreover, by
concavity over ˆ𝜖:"
REFERENCES,0.3157068062827225,"Φ𝜆, ˆ𝜖(A) −Φ𝜆,0(A)"
REFERENCES,0.3162303664921466,"ˆ𝜖
≤𝜓(𝑤Φ𝜆(A)) ≤Φ𝜆,0(A) −Φ𝜆,−ˆ𝜖(A)"
REFERENCES,0.31675392670157065,"ˆ𝜖
Whence"
REFERENCES,0.3172774869109948,"P
𝜓(wΦ𝜆(A)) −𝑑𝑐𝜆,𝜖 𝑑𝜖 𝜖=0"
REFERENCES,0.31780104712041884,"> 𝑡

≤P
Φ𝜆,0(A) −Φ𝜆,−ˆ𝜖(A)"
REFERENCES,0.3183246073298429,"ˆ𝜖
> 𝑐𝜆,0 −𝑐𝜆,−ˆ𝜖"
REFERENCES,0.31884816753926704,"ˆ𝜖
+ 𝑡 2 "
REFERENCES,0.3193717277486911,"+ P
Φ𝜆, ˆ𝜖(A) −Φ𝜆,0(A)"
REFERENCES,0.31989528795811517,"ˆ𝜖
< 𝑐𝜆, ˆ𝜖−𝑐𝜆,0 ˆ𝜖
−𝑡 2 "
REFERENCES,0.3204188481675393,"≤P

Φ𝜆,0(A) −Φ𝜆,−ˆ𝜖(A)"
REFERENCES,0.32094240837696336,"ˆ𝜖
−𝑐𝜆,0 −𝑐𝜆,−ˆ𝜖 ˆ𝜖 > 𝑡 2 "
REFERENCES,0.32146596858638743,"+ P

Φ𝜆, ˆ𝜖(A) −Φ𝜆,0(A)"
REFERENCES,0.3219895287958115,"ˆ𝜖
−𝑐𝜆, ˆ𝜖−𝑐𝜆,0 ˆ𝜖 > 𝑡 2 "
REFERENCES,0.3225130890052356,"By (23), the RHS goes to zero, thus 𝜓(wΦ𝜆(A))
P−→𝑑𝑐𝜆,𝜖 𝑑𝜖"
REFERENCES,0.3230366492146597,"𝜖=0. Furthermore, we have"
REFERENCES,0.32356020942408376,"P
𝜓(wΦ𝜆(B)) −𝑑𝑐𝜆,𝜖 𝑑𝜖 𝜖=0"
REFERENCES,0.3240837696335079,"> 𝑡

≤P

Φ𝜆,0(B) −Φ𝜆,−ˆ𝜖(B)"
REFERENCES,0.32460732984293195,"ˆ𝜖
−𝑐𝜆,0 −𝑐𝜆,−ˆ𝜖 ˆ𝜖 > 𝑡 2 "
REFERENCES,0.325130890052356,"+ P

Φ𝜆, ˆ𝜖(B) −Φ𝜆,0(B)"
REFERENCES,0.3256544502617801,"ˆ𝜖
−𝑐𝜆, ˆ𝜖−𝑐𝜆,0 ˆ𝜖 > 𝑡 2 "
REFERENCES,0.3261780104712042,"≤P
Φ𝜆,𝜖(B) −𝑐𝜆,𝜖
 > 𝜖𝑡 2"
REFERENCES,0.3267015706806283,"
+ P
Φ𝜆,−𝜖(B) −𝑐𝜆,−𝜖
 > 𝜖𝑡 2 "
REFERENCES,0.32722513089005234,"+ 2P
Φ𝜆,0(B) −𝑐𝜆,0
 > 𝜖𝑡 2"
REFERENCES,0.32774869109947646,"
(24)"
REFERENCES,0.32827225130890053,"By the result of the first part, we know that the RHS (24) goes to zero as 𝑛→∞. Hence, 𝜓(wΦ𝜆(B))
P−→
𝑑𝑐𝜆,𝜖 𝑑𝜖 𝜖=0."
REFERENCES,0.3287958115183246,"Moreover, if 𝜓is bounded, by definition of convergence in distribution, |EA𝜓(wΦ𝜆(A)) −
EB𝜓(wΦ𝜆(B))| →0."
REFERENCES,0.32931937172774867,"B.3.2
Proof of Part 2 of Theorem 1 for a regular test function for Ψ = Φ"
REFERENCES,0.3298429319371728,Defining
REFERENCES,0.33036649214659686,˜Φ𝜖(A) := 1
REFERENCES,0.3308900523560209,"𝑛
min
Aw=y,w∈Sw 𝑓(w) + 𝜖𝜓(w)"
REFERENCES,0.33141361256544505,"Since 𝑠𝑢𝑝𝜆>0 ˜Φ𝜆,𝜖(A)
P−→𝑠𝑢𝑝𝜆>0𝑐𝜆,𝜖, or equivalently ˜Φ𝜖(A)
P−→𝑐𝜖, we can use the same argu-"
REFERENCES,0.3319371727748691,"ment from previous section and Danskin’s theorem, to observe that 𝜓(w ˜Φ𝜆(A))
P−→
𝑑𝑐𝜖 𝑑𝜖"
REFERENCES,0.3324607329842932,𝜖=0 and
REFERENCES,0.33298429319371725,"𝜓(w ˜Φ𝜆(B))
P−→𝑑𝑐𝜖 𝑑𝜖 𝜖=0."
REFERENCES,0.3335078534031414,"B.3.3
Sequences of regular test functions"
REFERENCES,0.33403141361256544,"By a similar approach, it follows that for a sequence of regular functions 𝜓𝑚converging uniformly to"
REFERENCES,0.3345549738219895,"𝜓, one also has 𝜓(wΦ𝜆)
P−→𝑑𝑐𝜆,𝜖 𝑑𝜖"
REFERENCES,0.33507853403141363,"𝜖=0:= lim𝑚→∞
𝑑𝑐𝑚
𝜖
𝑑𝜖"
REFERENCES,0.3356020942408377,"𝜖=0. Indeed,"
REFERENCES,0.33612565445026177,"P
𝜓(wΦ𝜆(A)) −𝑑𝑐𝜆,𝜖 𝑑𝜖 𝜖=0"
REFERENCES,0.3366492146596859,"> 𝑡

≤P
𝜓(wΦ𝜆(A)) −𝜓𝑚(wΦ𝜆(A))
 > 𝑡 2"
REFERENCES,0.33717277486910996,"
+ P
𝜓𝑚(wΦ𝜆(A)) −𝑑𝑐𝜆,𝜖 𝑑𝜖 𝜖=0 > 𝑡 2 "
REFERENCES,0.337696335078534,"Since we have uniform convergence, for 𝑡"
REFERENCES,0.3382198952879581,"2, there exists an 𝑚0 such that for all 𝑚≥𝑚0 and all 𝑥∈R𝑑,"
REFERENCES,0.3387434554973822,we have |𝜓𝑚(𝑥) −𝜓(𝑥)| < 𝑡
REFERENCES,0.3392670157068063,"2. Thus for such 𝑚’s, P
𝜓(wΦ𝜆(A)) −𝜓𝑚(wΦ𝜆(A))
 > 𝑡 2"
REFERENCES,0.33979057591623035,"
= 0. Then by"
REFERENCES,0.3403141361256545,triangle inequality
REFERENCES,0.34083769633507854,"P
𝜓(wΦ𝜆(A)) −𝑑𝑐𝜆,𝜖 𝑑𝜖 𝜖=0"
REFERENCES,0.3413612565445026,"> 𝑡

≤P
𝜓𝑚(wΦ𝜆(A)) −
𝑑𝑐𝑚
𝜆,𝜖
𝑑𝜖 𝜖=0 > 𝑡 4"
REFERENCES,0.3418848167539267,"
+ P

𝑑𝑐𝜆,𝜖 𝑑𝜖"
REFERENCES,0.3424083769633508,"𝜖=0−
𝑑𝑐𝑚
𝜆,𝜖
𝑑𝜖 𝜖=0 > 𝑡 4 "
REFERENCES,0.34293193717277487,"By assumption P
𝜓𝑚(wΦ𝜆(A)) −
𝑑𝑐𝑚
𝜆,𝜖
𝑑𝜖 𝜖=0 > 𝑡 4"
REFERENCES,0.34345549738219894,"
→0. And by the definition of 𝑑𝑐𝜆,𝜖 𝑑𝜖"
REFERENCES,0.34397905759162306,𝜖=0 the claim
REFERENCES,0.34450261780104713,follows.
REFERENCES,0.3450261780104712,"C
Proof of Corollary 1"
REFERENCES,0.34554973821989526,We will utilize the result of the following Theorem.
REFERENCES,0.3460732984293194,"Theorem 4. (Bobkov [2003]) For an isotropic random vector X ∈R𝑑, and an isotropic gaussian
g ∈R𝑑if P(| ∥X∥2
√"
REFERENCES,0.34659685863874345,"2 −1| ≥𝜖𝑑) ≤𝜖𝑑, then for all 𝛿> 0"
REFERENCES,0.3471204188481675,"𝜈

sup
𝑡∈R"
REFERENCES,0.34764397905759165,"P(X𝑇θ ≤𝑡) −P(g𝑇θ ≤𝑡)
 ≥4𝜖𝑑+ 𝛿

≤4𝑑3/8𝑒−𝑐𝑑𝛿4"
REFERENCES,0.3481675392670157,Where θ ∼𝜈the uniform measure on the unit sphere S𝑑−1
REFERENCES,0.3486910994764398,"Essentially, through CGMT, we observe that when we use a quadratic regularizer or run SGD, the
solution, wΨ(G) converges weakly in distribution to a Gaussian vector, g𝐴𝑂. Whence we conclude
that wΨ(G) behaves the same as a generic vector. Therefore, by choosing X := a −Ea, a row of our
data matrix A, Theorem 4 enables us to prove the universality of the classification error. Note that in
general EwΨ(G) ≠0, but by Assumptions 4, the means of the classes are generic which allows us to
employ Theorem 4 to conclude the proof."
REFERENCES,0.34921465968586385,"D
Lemmata for the proof of Theorem 1"
REFERENCES,0.34973821989528797,"Proposition 2. If for any Lipschitz function 𝑔: R →R and 𝑡1 > 0, P

|𝑔(Φ𝜆,𝜖(B)) −𝑐| > 𝑡1

→0
and for every twice differentiable ˜𝑔: R →R with bounded second derivative, we have that
lim𝑛→∞
EA,y ˜𝑔(Φ𝜆,𝜖(B)) −EB,y ˜𝑔(Φ𝜆,𝜖(A))
 →0 then for any 𝑡2 > 𝑡1."
REFERENCES,0.35026178010471204,"lim
𝑛→∞P

|𝑔(Φ𝜆,𝜖(A)) −𝑐| > 𝑡2

= 0"
REFERENCES,0.3507853403141361,"Proof. First note that P(|Φ𝜆,𝜖(B) −𝑐| > 𝑡) →0, implies P(|𝑔(Φ𝜆,𝜖(B)) −𝑔(𝑐)| > 𝐿𝑡) →0.
Thus it suffices to verify P(|Φ𝜆,𝜖(B) −𝑐| > 𝑡) →0. This follows similar to Panahi and Hassibi
[2017]. Let 𝜉(𝑥) := 21(−∞,−𝑡2]∪[𝑡2,∞) (𝑥) +   𝑡1+𝑡2"
REFERENCES,0.35130890052356023,"2
−(|𝑥| −𝑡2)21(−𝑡2,−𝑡1+𝑡2"
REFERENCES,0.3518324607329843,"2
]∪[ 𝑡1+𝑡2"
REFERENCES,0.35235602094240837,"2
,𝑡2) (𝑥) + (|𝑥| −"
REFERENCES,0.35287958115183243,𝑡1)21(−𝑡1+𝑡2
REFERENCES,0.35340314136125656,"2
,−𝑡1]∪[𝑡1, 𝑡1+𝑡2"
REFERENCES,0.3539267015706806,"2
) (𝑥). Then we have"
REFERENCES,0.3544502617801047,"P

|Φ𝜆,𝜖(A) −𝑐| > 𝑡2

= P

𝜉 Φ𝜆,𝜖(A) −𝑐 > 2
"
REFERENCES,0.3549738219895288,Then by Markov’s and triangle inequality
REFERENCES,0.3554973821989529,"P

|Φ𝜆,𝜖(A) −𝑐| > 𝑡2

≤1"
REFERENCES,0.35602094240837695,"2EA,y𝜉 Φ𝜆,𝜖(A) −𝑐 ≤1 2"
REFERENCES,0.3565445026178011,"EA,y𝜉 Φ𝜆,𝜖(A) −𝑐 −EB,y𝜉 Φ𝜆,𝜖(B) −𝑐 + 1"
REFERENCES,0.35706806282722514,"2EB,y𝜉 Φ𝜆,𝜖(B) −𝑐"
REFERENCES,0.3575916230366492,"Note that 𝜉(𝑥) ≤21(−∞,−𝑡1]∪[𝑡1,∞). Thus 1"
REFERENCES,0.3581151832460733,"2EB,y𝜉 Φ𝜆,𝜖(B) −𝑐 ≤P

|Φ𝜆,𝜖(B) −𝑐| > 𝑡1

. Moreover,
we know that 𝜉(.) has bounded second derivative derivatives almost everywhere and by assumption
on 𝑔′, we would have"
REFERENCES,0.3586387434554974,"P

|Φ𝜆,𝜖(A) −𝑐| > 𝑡2

≤1 2"
REFERENCES,0.35916230366492147,"EA,y𝜉 Φ𝜆,𝜖(A) −𝑐 −EB,y𝜉 Φ𝜆,𝜖(B) −𝑐 + P

|Φ𝜆,𝜖(B) −𝑐| > 𝑡1

→0 □"
REFERENCES,0.35968586387434553,Lemma 3. We have the following:
REFERENCES,0.36020942408376966,1. L(a) −Θ = 𝜆
REFERENCES,0.3607329842931937,"2𝑛
(a𝑇u−𝑦)2"
REFERENCES,0.3612565445026178,1+𝜆a𝑇𝛀−1a
REFERENCES,0.36178010471204186,"2. wL −u = 𝜆
𝑦−a𝑇u
1+𝜆a𝑇𝛀−1a𝛀−1a"
REFERENCES,0.362303664921466,"Proof. We know for Θ, taking derivative yields"
REFERENCES,0.36282722513089005,𝜆˜A𝑇˜Au −𝜆˜A𝑇˜y + g = 0
REFERENCES,0.3633507853403141,This implies −𝜆˜A𝑇˜y + g = −𝜆˜A𝑇˜Au.
REFERENCES,0.36387434554973824,"Note that for L(a), since the objective is quadratic in w, we solve the optimization in closed form,
we have"
REFERENCES,0.3643979057591623,L(a) = 1
REFERENCES,0.3649214659685864,"𝑛min
w
 w𝑇
1 
1
2 (𝜆˜A𝑇˜A + 𝜆aa𝑇+ H)
1
2 (−𝜆˜A𝑇˜y −𝜆𝑦a + g −Hu)
1
2 (−𝜆˜A𝑇˜y −𝜆𝑦a + g −Hu)𝑇
𝑓(u) + 𝜖𝜓(u) −g𝑇u + 1"
REFERENCES,0.36544502617801045,2u𝑇Hu + 𝜆
REFERENCES,0.36596858638743457,2 (∥˜y∥2 + 𝑦2)
REFERENCES,0.36649214659685864," 
w
1 "
REFERENCES,0.3670157068062827,Let 𝛀:= 𝜆˜A𝑇˜A + H. hence
REFERENCES,0.3675392670157068,L(a) = 1 𝑛
REFERENCES,0.3680628272251309,"
𝑓(u) + 𝜖𝜓(u) −g𝑇u + 1"
REFERENCES,0.36858638743455496,2𝑢𝑇Hu + 𝜆
REFERENCES,0.36910994764397903,2 (∥˜y∥2 + 𝑦2) −1
REFERENCES,0.36963350785340315,"2 (𝛀u + 𝜆𝑦𝑎)𝑇(𝛀+ 𝜆𝑎a𝑇)−1(𝛀u + 𝜆𝑦a)
"
REFERENCES,0.3701570680628272,Using Sherman-Morrison as 𝛀≻0:
REFERENCES,0.3706806282722513,"(𝛀u + 𝜆𝑦a)𝑇
𝛀+ 𝜆aa𝑇−1
(𝛀u + 𝜆𝑦a) = (𝛀u + 𝜆𝑦a)𝑇
𝛀−1 −𝜆𝛀−1aa𝑇𝛀−1"
REFERENCES,0.3712041884816754,1 + 𝜆a𝑇𝛀−1a
REFERENCES,0.3717277486910995,"
(𝛀u + 𝜆𝑦a) ="
REFERENCES,0.37225130890052355,"= (𝛀u + 𝜆𝑦a)𝑇
u −𝜆a𝑇u
𝛀−1a"
REFERENCES,0.37277486910994767,"1 + 𝜆a𝑇𝛀−1a
+ 𝜆𝑦𝛀−1a −𝜆2𝑦a𝑇𝛀−1a
𝛀−1a"
REFERENCES,0.37329842931937174,"1 + 𝜆a𝑇𝛀−1a 
="
REFERENCES,0.3738219895287958,"= (𝛀u + 𝜆𝑦a)𝑇

u + 𝜆

𝑦−𝜆𝑦a𝑇𝛀−1a + a𝑇u"
REFERENCES,0.3743455497382199,1 + 𝜆a𝑇𝛀−1a
REFERENCES,0.374869109947644,"
𝛀−1a

= (𝛀u + 𝜆𝑦a)𝑇
u + 𝜆
𝑦−a𝑇u"
REFERENCES,0.37539267015706806,"1 + 𝜆a𝑇𝛀−1a
𝛀−1a
"
REFERENCES,0.37591623036649213,From here we obtain for the solution Thus for the first claim:
REFERENCES,0.37643979057591626,"wL = (𝛀+ 𝜆aa𝑇)−1(𝛀u + 𝜆𝑦a) = u + 𝜆
𝑦−a𝑇u"
REFERENCES,0.3769633507853403,"1 + 𝜆a𝑇𝛀−1a
𝛀−1a"
REFERENCES,0.3774869109947644,Hence we have for the first claim
REFERENCES,0.37801047120418846,"wL −u = 𝜆
𝑦−a𝑇u"
REFERENCES,0.3785340314136126,"1 + 𝜆a𝑇𝛀−1a
𝛀−1a"
REFERENCES,0.37905759162303665,Now we continue with the calculation of the objective value
REFERENCES,0.3795811518324607,"u𝑇𝛀u + 𝜆
𝑦−a𝑇u"
REFERENCES,0.38010471204188484,"1 + 𝜆a𝑇𝛀−1a
a𝑇u + 𝜆𝑦a𝑇u + 𝜆2𝑦
𝑦−a𝑇u"
REFERENCES,0.3806282722513089,"1 + 𝜆a𝑇𝛀−1a
a𝑇𝛀−1a ="
REFERENCES,0.381151832460733,"= u𝑇𝛀u +
𝜆"
REFERENCES,0.38167539267015704,1 + 𝜆a𝑇𝛀−1a
REFERENCES,0.38219895287958117,"
𝑦a𝑇u −(a𝑇u)2 + 𝑦a𝑇u + 𝜆𝑦(a𝑇u)(a𝑇𝛀−1a) + 𝜆𝑦2a𝑇𝛀−1a −𝜆𝑦(a𝑇u)(a𝑇𝛀−1a)

="
REFERENCES,0.38272251308900523,"= u𝑇𝛀u +
𝜆"
REFERENCES,0.3832460732984293,"1 + 𝜆a𝑇𝛀−1a
(2𝑦a𝑇u −(a𝑇u)2 + 𝜆𝑦2a𝑇𝛀−1a) Thus"
REFERENCES,0.3837696335078534,L(a) = 1 𝑛
REFERENCES,0.3842931937172775,"
𝑓(u) + 𝜖𝜓(u) −g𝑇u + 1"
REFERENCES,0.38481675392670156,2u𝑇Hu + 𝜆
REFERENCES,0.38534031413612563,"2 (∥˜y∥2 + 𝑦2) −1 2u𝑇𝛀u −1 2
𝜆"
REFERENCES,0.38586387434554975,"1 + 𝜆a𝑇𝛀−1a
(2𝑦a𝑇u −(a𝑇u)2 + 𝜆𝑦2a𝑇𝛀−1a)
 = 1 𝑛"
REFERENCES,0.3863874345549738,"
𝑓(u) + 𝜖𝜓(u) −g𝑇u −𝜆"
REFERENCES,0.3869109947643979,2u𝑇˜A𝑇˜Au + 𝜆
REFERENCES,0.387434554973822,2 ∥˜y∥2 + 𝜆
REFERENCES,0.3879581151832461,"2
(a𝑇u −𝑦)2"
REFERENCES,0.38848167539267014,1 + 𝜆a𝑇𝛀−1a 
REFERENCES,0.38900523560209427,"Since 𝑔= 𝜆˜A𝑇˜y −𝜆˜A𝑇˜A𝑢, then g𝑇u = 𝜆˜y𝑇˜Au −𝜆u𝑇˜A ˜A𝑢, therefore"
REFERENCES,0.38952879581151834,L(a) = 1 𝑛
REFERENCES,0.3900523560209424,"
𝑓(u) + 𝜖𝜓(u) −𝜆˜y𝑇˜Au + 𝜆u𝑇˜A ˜Au −𝜆"
REFERENCES,0.39057591623036647,2u𝑇˜A𝑇˜Au + 𝜆
REFERENCES,0.3910994764397906,2 ∥˜y∥2 + 𝜆
REFERENCES,0.39162303664921466,"2
(a𝑇u −𝑦)2"
REFERENCES,0.39214659685863873,1 + 𝜆a𝑇𝛀−1a  = 1 𝑛
REFERENCES,0.39267015706806285,"
𝑓(u) + 𝜖𝜓(u) + 𝜆"
REFERENCES,0.3931937172774869,2 ∥˜Au −˜y∥2 + 𝜆
REFERENCES,0.393717277486911,"2
(a𝑇u −𝑦)2"
REFERENCES,0.39424083769633506,1 + 𝜆a𝑇𝛀−1a
REFERENCES,0.3947643979057592,"
= Θ + 𝜆"
REFERENCES,0.39528795811518325,"2𝑛
(a𝑇u −𝑦)2"
REFERENCES,0.3958115183246073,"1 + 𝜆a𝑇𝛀−1a
□"
REFERENCES,0.39633507853403144,Lemma 4. We have the following:
REFERENCES,0.3968586387434555,"1. M(a) −Θ ≤
𝜆
2𝑛(a𝑇u −y)2"
REFERENCES,0.3973821989528796,"2. ℓ(a, w) −L(a) ≥
𝜌
2𝑛∥w −w¯ L∥2
2"
REFERENCES,0.39790575916230364,"3. |𝓂(a, w) −ℓ(a, w)| ≤𝐶𝑓+𝜖𝜓"
REFERENCES,0.39842931937172776,"𝑛
∥w −u∥3
3"
REFERENCES,0.39895287958115183,"Proof. 1. By definition,"
REFERENCES,0.3994764397905759,"M(a) ≤𝓂(a, u) = Θ + 𝜆"
REFERENCES,0.4,2𝑛(a𝑇u −𝑦)2
REFERENCES,0.4005235602094241,"Thus M(a) −Θ ≤
𝜆
2𝑛(a𝑇u −y)2"
REFERENCES,0.40104712041884816,"2. The idea of the proof is by strong convexity. By assumption, 𝑓(w) + 𝜖𝜓(w) for small enough 𝜖is
𝜌-strongly convex. This implies from defintion:"
REFERENCES,0.4015706806282722,"ℓ(a, w) ≥L(a) + 𝜌"
REFERENCES,0.40209424083769635,"2𝑛∥w −wL∥2
2"
REFERENCES,0.4026178010471204,3. By Taylor remainder theorem for some 0 < 𝑡< 1:
REFERENCES,0.4031413612565445,"𝓂(a, w) −ℓ(a, w) = 1 𝑛"
REFERENCES,0.4036649214659686,"
𝑓(w) + 𝜖𝜓(w) −𝑓(u) −𝜖𝜓(u) −𝑔𝑇(w −u) −1"
REFERENCES,0.4041884816753927,"2 (w −u)𝑇H(w −u)
 = 1 𝑛 ∑︁"
REFERENCES,0.40471204188481674,"| 𝛼|=3
𝜕𝛼( 𝑓+ 𝜖𝜓)((1 −𝑡)u + 𝑡w) (w −u) 𝛼 𝛼!"
REFERENCES,0.40523560209424087,"Since 𝑓and 𝜓are separable, 𝜕𝛼( 𝑓+ 𝜖𝜓) = 0 unless 𝛼= 3𝑒𝛼(𝑒𝛼is the standard basis vector in R𝑑.
Then by assumption on the third derivative of 𝑓+ 𝜖𝜓,"
REFERENCES,0.40575916230366493,"|𝓂(a, w) −ℓ(a, w)| = 1 𝑛 ∑︁"
REFERENCES,0.406282722513089,"| 𝛼|=3
𝜕𝛼( 𝑓+ 𝜖𝜓)((1 −𝑡)u + 𝑡w) (w −u) 𝛼 𝛼!  = 1 𝑛  𝑑
∑︁ 𝑖=1"
REFERENCES,0.40680628272251307,𝜕3( 𝑓+ 𝜖𝜓)((1 −𝑡)𝑢𝑖+ 𝑡𝑤𝑖)
REFERENCES,0.4073298429319372,"𝜕𝑤3
𝑖
(w𝑖−𝑢𝑖)3 ≤𝐶𝑓+𝜖𝜓"
REFERENCES,0.40785340314136126,"𝑛
∥w −u∥3
3 □"
REFERENCES,0.4083769633507853,Now we are ready to prove the following key lemma.
REFERENCES,0.40890052356020945,Lemma 5. We have |M(a) −L(a)| ≤min{ 𝜆
REFERENCES,0.4094240837696335,"2𝑛(a𝑇u −𝑦)2, 8𝐶𝑓+𝜖𝜓"
REFERENCES,0.4099476439790576,"𝑛
∥wL −u∥3
3}"
REFERENCES,0.41047120418848165,"Proof. Let wM = arg min 𝓂(a, w). Note that since Θ ≤M(a) and by Lemma 4 we have"
REFERENCES,0.4109947643979058,L(a) −M(a) = L(a) −Θ + Θ −M(a) ≤𝜆
REFERENCES,0.41151832460732984,"2𝑛
(a𝑇u −𝑦)2"
REFERENCES,0.4120418848167539,1 + 𝜆a𝑇𝛀−1a
REFERENCES,0.41256544502617803,"On the other hand, note that 𝓂(a, u) = Θ + 𝜆"
REFERENCES,0.4130890052356021,2𝑛(a𝑇u −𝑦)2 and Θ ≤L(a)
REFERENCES,0.41361256544502617,"M(a) −L(a) = M(a) −𝓂(a, u) + 𝓂(a, u) −Θ + Θ −L(a) ≤𝜆"
REFERENCES,0.41413612565445024,2𝑛(a𝑇u −𝑦)2
REFERENCES,0.41465968586387436,Thus as 𝜆
REFERENCES,0.41518324607329843,"2𝑛(a𝑇u −𝑦)2 ≥
𝜆
2𝑛
(a𝑇u−𝑦)2"
REFERENCES,0.4157068062827225,1+𝜆a𝑇𝛀−1a
REFERENCES,0.4162303664921466,|M(a) −L(a)| ≤𝜆
REFERENCES,0.4167539267015707,2𝑛(a𝑇u −𝑦)2
REFERENCES,0.41727748691099475,"But as we will see, (a𝑇u −𝑦)2 is of order 𝑂(1), which will not suffice. Thus we break down
M(a) −L(a) depending on the distance of 𝑢and wL"
REFERENCES,0.4178010471204188,"M(a) −L(a) = 𝓂(a, wM) −𝓂(a, wL) + 𝓂(a, wL) −ℓ(a, wL)"
REFERENCES,0.41832460732984295,"≤𝓂(a, wL) −ℓ(a, wL) ≤𝐶𝑓+𝜖𝜓"
REFERENCES,0.418848167539267,"𝑛
∥wL −u∥3
3"
REFERENCES,0.4193717277486911,For the other direction
REFERENCES,0.4198952879581152,"L(a) −M(a) = ℓ(a, wL) −ℓ(a, wM) + ℓ(a, wM) −𝓂(a, wM) ≤𝐶𝑓+𝜖𝜓"
REFERENCES,0.42041884816753927,"𝑛
∥wM −u∥3
3"
REFERENCES,0.42094240837696334,"Let 𝑅:= ∥wL −u∥3 and consider the ℓ3 ball centered at wL. Now if ∥wM −wL∥3 ≤𝑅then
∥wM −u∥3 ≤2𝑅by a straightforward application of triangle inequality. Which implies
L(a) −M(a) ≤8𝐶𝑓+𝜖𝜓"
REFERENCES,0.4214659685863874,"𝑛
𝑅3. Now when does ∥wM −wL∥3 ≤𝑅hold? We show that for every point
on the boundary, w, 𝓂(a, w) ≥𝓂(a, wL), this implies that wM is indeed inside the ball. We have"
REFERENCES,0.42198952879581153,"𝓂(a, w) −𝓂(a, wL) ≥ℓ(a, w) −𝐶𝑓+𝜖𝜓"
REFERENCES,0.4225130890052356,"𝑛
∥w −u∥3
3 −ℓ(a, wL) −𝐶𝑓+𝜖𝜓"
REFERENCES,0.42303664921465967,"𝑛
∥wL −u∥3
3 ≥𝜌"
REFERENCES,0.4235602094240838,"2𝑛∥w −wL∥2
2 −𝐶𝑓+𝜖𝜓"
REFERENCES,0.42408376963350786,"𝑛
(∥w −u∥3
3 + 𝑅3)"
REFERENCES,0.4246073298429319,Then using Lemma 4
REFERENCES,0.42513089005235605,"𝓂(a, w) −𝓂(a, wL) ≥𝜌"
REFERENCES,0.4256544502617801,"2𝑛∥w −wL∥2
3 −𝐶𝑓+𝜖𝜓"
REFERENCES,0.4261780104712042,"𝑛
(∥w −u∥3
3 + 𝑅3) ≥𝜌"
REFERENCES,0.42670157068062825,2𝑛𝑅2 −𝐶𝑓+𝜖𝜓
REFERENCES,0.4272251308900524,"𝑛
(8𝑅3 + 𝑅3) = 𝑅2( 𝜌"
REFERENCES,0.42774869109947644,"2𝑛−9𝐶𝑓+𝜖𝜓 𝑛
𝑅)"
REFERENCES,0.4282722513089005,"Now if 𝑅≤
𝜌
18𝐶𝑓+𝜖𝜓, then 𝓂(a, w) −𝓂(a, wL) ≥0. Thus all in all"
REFERENCES,0.42879581151832463,|L(a) −M(a)| ≤8𝐶𝑓+𝜖𝜓
REFERENCES,0.4293193717277487,"𝑛
∥wL −u∥3
3 □"
REFERENCES,0.42984293193717277,Lemma 6. We have the following upper bounds
REFERENCES,0.43036649214659684,"1. ∥u∥2
2 = 𝑂(𝑛) with high probabilty."
REFERENCES,0.43089005235602096,"2. E ˜A,˜yµ𝑇u = 𝑂(1)"
REFERENCES,0.431413612565445,"Proof. 1. Since 𝑓+ 𝜖𝜓is 𝜌-strongly convex, one can decompose 𝑓(w) + 𝜖𝜓(w) = 𝜌"
REFERENCES,0.4319371727748691,"2 ∥w∥2
2 + ℎ(w)
for some convex function ℎ(·). Then"
REFERENCES,0.4324607329842932,"𝜆
2 ∥˜Au −˜y∥2
2 + 𝜌"
REFERENCES,0.4329842931937173,2 ∥u∥2 + ℎ(u) ≤𝜆
REFERENCES,0.43350785340314135,"2 ∥˜y∥2
2 + ℎ(0) = 𝑂(𝑛)"
REFERENCES,0.4340314136125654,"Thus this implies ∥u∥2
2 = 𝑂(𝑛)."
REFERENCES,0.43455497382198954,"2. In a similar fashion, we have for E ˜A,˜yµ𝑇u Θ = 1 𝑛 𝜆"
REFERENCES,0.4350785340314136,"2 ∥˜Au −˜y∥2
2 + 𝑓(u) + 𝜖𝜓(u)

≤1 𝑛 𝜆"
REFERENCES,0.4356020942408377,"2 ∥˜y∥2 + 𝑓(0) + 𝜖𝜓(0)
"
REFERENCES,0.4361256544502618,"Let ˜A𝑇=
 ˆ˜A𝑇
1
ˆ˜A𝑇
2
. . .
ˆ˜A𝑇
𝑘"
REFERENCES,0.43664921465968587,"
. Now we use the definition of ˜A to obtain a lower bound:"
REFERENCES,0.43717277486910994,"𝜆
2 ∥˜Au −˜y∥2
2 + 𝑓(u) + 𝜖𝜓(u) ≥𝜆 2 𝑘
∑︁"
REFERENCES,0.437696335078534,"𝑖=1
∥ˆ˜A𝑖u + 1µ𝑇
𝑖u −ˆ˜y𝑖∥2
2 ≥𝜆"
REFERENCES,0.4382198952879581,"2 ∥ˆ˜A𝑖u + 1µ𝑇
𝑖u −ˆ˜y𝑖∥2
2 ≥𝑛𝑖𝜆"
REFERENCES,0.4387434554973822,"2 (µ𝑇
𝑖u)2 + 𝜆(µ𝑇
𝑖u)1𝑇( ˆ˜A𝑖u −ˆ˜y𝑖)"
REFERENCES,0.43926701570680626,"Thus plugging in w = 0 in the optimization yields an upper bound, which is a quadratic inequality in
µ𝑇
𝑖u: 𝑛𝑖𝜆"
REFERENCES,0.4397905759162304,"2 (µ𝑇
𝑖u)2 + 𝜆(µ𝑇
𝑖u)1𝑇( ˆ˜A𝑖u −ˆ˜y𝑖) −𝜆"
REFERENCES,0.44031413612565445,"2 ∥˜y∥2 −𝑓(0) −𝜖𝜓(0) ≤0
(25)"
REFERENCES,0.4408376963350785,Now note that for a quadratic optimization with 𝑏≥0
REFERENCES,0.44136125654450264,"𝑥2 −𝑎𝑥−𝑏≤0 ⇐⇒𝑎−
√"
REFERENCES,0.4418848167539267,𝑎2 + 4𝑏
REFERENCES,0.4424083769633508,"2
≤𝑥≤𝑎+
√"
REFERENCES,0.44293193717277485,𝑎2 + 4𝑏
WHENCE,0.44345549738219897,"2
Whence"
WHENCE,0.44397905759162304,"|𝑥| ≤|𝑎| +
√"
WHENCE,0.4445026178010471,"𝑎2 + 4𝑏
2
≤|𝑎| + |𝑎| + 2
√"
WHENCE,0.44502617801047123,"𝑏
2
= |𝑎| +
√ 𝑏"
WHENCE,0.4455497382198953,"We apply this result to 25,"
WHENCE,0.44607329842931936,"|µ𝑇
𝑖u| ≤2"
WHENCE,0.44659685863874343,"𝑛𝑖
|1𝑇( ˆ˜A𝑖u −ˆ˜y𝑖)| + 2 𝜆𝑛𝑖 √︂"
WHENCE,0.44712041884816756,"𝜆
2 ∥˜y∥2 + 𝑓(0) + 𝜖𝜓(0) ≤2"
WHENCE,0.4476439790575916,"𝑛𝑖
|1𝑇ˆ˜y𝑖| + 2"
WHENCE,0.4481675392670157,"𝑛𝑖
∥1𝑇ˆ˜A𝑖∥2∥u∥2 + 2 𝜆𝑛𝑖 √︂"
WHENCE,0.4486910994764398,"𝜆
2 ∥˜y∥2 + 𝑓(0) + 𝜖𝜓(0)"
WHENCE,0.4492146596858639,Now taking expectation and using a combination Cauchy-Schwarz and Jensen inequalities yields
WHENCE,0.44973821989528795,"E ˜A,˜y|µ𝑇
𝑖u| ≤2"
WHENCE,0.450261780104712,"𝑛𝑖
E ˜A,˜y|1𝑇ˆ˜y𝑖| + 2 𝑛𝑖 √︃"
WHENCE,0.45078534031413614,"E ˜A,˜y∥1𝑇ˆ˜A𝑖∥2
2E ˜A,˜y∥u∥2
2 + 2 𝜆𝑛𝑖 √︂"
WHENCE,0.4513089005235602,"𝜆
2E ˜A,˜y∥˜y∥2
2 + 𝑓(0) + 𝜖𝜓(0)"
WHENCE,0.4518324607329843,"Now note that E ˆ˜A𝑖= 0, and by the independence of the rows:"
WHENCE,0.4523560209424084,"E ˜A,˜y∥1𝑇ˆ˜A𝑖∥2
2 = 𝑑
∑︁ 𝑗1=1"
WHENCE,0.45287958115183247,"  𝑛
∑︁"
WHENCE,0.45340314136125653,"𝑗2=1
( ˆ˜A𝑖) 𝑗2 𝑗1
2 = 𝑑
∑︁ 𝑗1=1 𝑛
∑︁"
WHENCE,0.4539267015706806,"𝑗2, 𝑗3=1
E ˜A( ˆ˜A𝑖) 𝑗2 𝑗1( ˆ˜A𝑖) 𝑗3 𝑗1 = 𝑑
∑︁"
WHENCE,0.4544502617801047,"𝑗2=1
E ˜A 𝑛
∑︁"
WHENCE,0.4549738219895288,"𝑗1=1
( ˆ˜A𝑖)2
𝑗2 𝑗1"
WHENCE,0.45549738219895286,"By assumption, for each row ˆa𝑖of ˆ˜A𝑖, we have that Eˆa𝑖∥ˆa𝑖∥2
2 = 𝑂(1), which implies E ˜A,˜y∥1𝑇ˆ˜A𝑖∥2
2 =
𝑂(𝑑). Therefore, by the previous part, since ∥u∥2
2 ≤
𝜆
𝜌∥˜y∥2
2 + 2"
WHENCE,0.456020942408377,"𝜌ℎ(0), then E˜y∥u∥2
2 ≤
2
𝜌ℎ(0) +
𝜆
𝜌E˜y∥˜y∥2
2 = 𝑂(𝑑) by assumption on the ˜y. Hence it can be seen that E ˜A,˜yµ𝑇u = 𝑂(1)
□"
WHENCE,0.45654450261780105,"Now we are ready to conclude the proof with the following lemma.
Lemma 7. We have the following"
WHENCE,0.4570680628272251,"1. E ˜A,˜y,a,𝑦(a𝑇u −𝑦)2𝑞= 𝑂(1) for 𝑞∈[3]"
WHENCE,0.45759162303664924,"2. E ˜A,˜y,a,𝑦∥u −wL∥3
3 = 𝑂( 1 𝑑)"
WHENCE,0.4581151832460733,Proof. 1. First note that
WHENCE,0.4586387434554974,"E ˜A,˜y,a,𝑦(a𝑇u −𝑦)2𝑞= E ˜A,˜y,a,𝑦 2𝑞
∑︁ 𝑗=1 2𝑞
𝑗"
WHENCE,0.45916230366492145,"
((a −µ + µ)𝑇u) 𝑗(−𝑦)2𝑞−𝑦"
WHENCE,0.45968586387434557,"Moreover,"
WHENCE,0.46020942408376964,"E ˜A,˜y,a,𝑦(a𝑇u −𝑦)2𝑞≤ 2𝑞
∑︁ 𝑗=1 2𝑞
𝑗"
WHENCE,0.4607329842931937,"
E ˜A,˜y,a,𝑦"
WHENCE,0.4612565445026178,"
|(a −µ + µ)𝑇u| 𝑗|𝑦|2𝑞−𝑦
 ≤ 2𝑞
∑︁ 𝑗=1 2𝑞
𝑗"
WHENCE,0.4617801047120419,"
E ˜A,˜y,a,𝑦"
WHENCE,0.46230366492146596,"
|𝑦|2𝑞−𝑦2𝑗max{|µ𝑇u|, |(a −µ)𝑇u|} 𝑗
 ≤ 2𝑞
∑︁"
WHENCE,0.46282722513089003,"𝑗=1
2 𝑗
2𝑞
𝑗"
WHENCE,0.46335078534031415,"
E ˜A,˜y,a,𝑦"
WHENCE,0.4638743455497382,"
|𝑦|2𝑞−𝑦(|µ𝑇u| 𝑗+ |(a −µ)𝑇u| 𝑗)
 ≤ 2𝑞
∑︁"
WHENCE,0.4643979057591623,"𝑗=1
2 𝑗
2𝑞
𝑗 √︃"
WHENCE,0.4649214659685864,"E|𝑦|4𝑞−2𝑦
√︁"
WHENCE,0.4654450261780105,"E|µ𝑇u|2 𝑗+
√︁"
WHENCE,0.46596858638743455,"E|(a −µ)𝑇u|2𝑗
"
WHENCE,0.4664921465968586,"By assumption E|𝑦|2𝑞−𝑦≤𝐶for some constant 𝐶> 0 that is dimension-independent. Moreover, by"
WHENCE,0.46701570680628274,"our assumption, E|(a −µ)𝑇u| 𝑗≤˜𝐶
∥u∥𝑗
2
𝑑𝑗/2 . Also, from the previous lemma, E|µ𝑇u|2𝑗= 𝑂(1). The
claim follows."
WHENCE,0.4675392670157068,"2. First note that ∥u −wL∥3
3 = 𝜆3
𝑦−a𝑇u
1+𝜆a𝑇𝛀−1a"
WHENCE,0.4680628272251309,"3
∥𝛀−1a∥3
3 and since 𝛀≻0, 1 + 𝜆a𝑇𝛀−1a > 0 hence by
Minkowski’s inequality"
WHENCE,0.468586387434555,"E ˜A,˜y,a,𝑦∥u −wL∥3
3 ≤𝜆3E ˜A,˜y,a,𝑦|𝑎𝑇u −𝑦|3∥𝛀−1a∥3
3
≤𝜆3E ˜A,˜y,a,𝑦|𝑎𝑇u −𝑦|3∥𝛀−1(a −µ + µ)∥3
3
≤8𝜆3E ˜A,˜y,a,𝑦|𝑎𝑇u −𝑦|3 max{∥𝛀−1(a −µ)∥3, ∥𝛀−1µ∥3}3"
WHENCE,0.46910994764397906,"≤8𝜆3E ˜A,˜y,a,𝑦|𝑎𝑇u −𝑦|3∥𝛀−1(a −µ)∥3
3 + 8𝜆3E ˜A,˜y,a,𝑦|𝑎𝑇u −𝑦|3|𝛀−1µ∥3
3"
WHENCE,0.46963350785340313,≤8𝜆3√︃
WHENCE,0.4701570680628272,"E ˜A,˜y,a,𝑦(a𝑇u −𝑦)6E ˜A,˜y,a,𝑦∥𝛀−1(a −µ)∥6
3"
WHENCE,0.4706806282722513,+ 8𝜆3√︃
WHENCE,0.4712041884816754,"E ˜A,˜y,a,𝑦(a𝑇u −𝑦)6E ˜A,˜y,a,𝑦∥𝛀−1µ∥6
3"
WHENCE,0.47172774869109946,"Now since we assumed that the objective is 𝜌-strongly convex, or there exists 𝜌> 0 that ∥𝛀−1∥𝑜𝑝≤
1
𝜌, denoting ω𝑗as the 𝑗th row of 𝛀−1, we have by the inequality between Frobenius and Operator
norm, ∥ω𝑘∥2 ≤1"
WHENCE,0.4722513089005236,𝜌. Now by the assumptions we have
WHENCE,0.47277486910994765,"E ˜A,˜y,a,𝑦∥𝛀−1(a −µ)∥6
3 = E ˜A,˜y,a,𝑦
 𝑑
∑︁"
WHENCE,0.4732984293193717,"𝑗=1
|(a −µ)𝑇ω𝑗|32"
WHENCE,0.4738219895287958,"= E ˜A,˜y 𝑑
∑︁"
WHENCE,0.4743455497382199,"𝑗=1
E𝑎,𝑦|(a −µ)𝑇ω𝑗|6 + 𝑑
∑︁"
WHENCE,0.474869109947644,"𝑗,𝑙=1, 𝑗≠𝑙
E|(a −µ)𝑇ω𝑗|3|(a −µ)𝑇ω𝑙|3 ≤𝐶 𝑑
∑︁ 𝑗=1"
WHENCE,0.47539267015706804,"∥ω𝑗∥6
2
𝑑3
+ 𝐶 𝑑
∑︁"
WHENCE,0.47591623036649217,"𝑗,𝑙=1, 𝑗≠𝑙 √︃"
WHENCE,0.47643979057591623,"Ea|(a −µ)𝑇ω𝑗|6Ea|(a −µ)𝑇ω𝑙|6 ≤𝐶𝑑
1
𝜌6𝑑3 + 𝐶(𝑑2 −𝑑)
1
𝜌6𝑑3 ≤
𝐶
𝜌6𝑑"
WHENCE,0.4769633507853403,We also have for the other term:
WHENCE,0.4774869109947644,"E ˜A,˜y,a,𝑦∥𝛀−1µ∥6
3 ≤E ˜A,˜y,a,𝑦∥𝛀−1µ∥6
2"
WHENCE,0.4780104712041885,"Note that ∥𝛀−1µ∥2 ≤∥𝛀−1/2∥𝑜𝑝∥𝛀−1/2µ∥2. Thus it is sufficient to analyze µ𝑇𝛀−1µ𝑇. Let 𝑡∈R
be an upper bound for µ𝑇𝛀−1µ𝑇, that is µ𝑇𝛀−1µ𝑇≤𝑡. Note that by the Schur complement
property, we need to find the smallest 𝑡> 0 such that 𝛀−1"
WHENCE,0.47853403141361256,"𝑡µµ𝑇⪰0 with holds with high probability.
Recall that 𝛀:= 𝜆˜A𝑇˜A + H and by assumption H ⪰𝜌I, thus it suffices to find the largest 𝑡> 0
that 𝜆˜A𝑇˜A + 𝜌I −1"
WHENCE,0.4790575916230366,"𝑡µµ𝑇⪰0. Let ˜A′ := ˜A −˜M be centered. This problem is equivalent to having
∥( ˜A′ + ˜M)v∥2
2 + 𝜌∥v∥2
2 −1"
WHENCE,0.47958115183246075,𝑡(µ𝑇v)2 ≥0 for every v ∈R𝑑.
WHENCE,0.4801047120418848,Thus we should have
WHENCE,0.4806282722513089,"𝑡≥
(µ𝑇v)2"
WHENCE,0.481151832460733,"∥( ˜A′ + ˜M)v∥2
2 + 𝜌∥v∥2
2"
WHENCE,0.4816753926701571,"Let 𝑡∗:= supv
(µ𝑇v)2"
WHENCE,0.48219895287958114,"∥( ˜A′+ ˜M)v∥2
2+𝜌∥v∥2
2 . We show 𝑡∗= 𝑂(𝑛−𝑠) for some 𝑠> 0 with high probability. Let"
WHENCE,0.4827225130890052,"v∗:= arg max
(µ𝑇v)2"
WHENCE,0.48324607329842934,"∥( ˜A′+ ˜M)v∥2
2+𝜌∥v∥2
2 . This implies"
WHENCE,0.4837696335078534,"∥( ˜A′ + ˜M)v∗∥2
2 ≤1"
WHENCE,0.48429319371727747,"𝑡∗(µ𝑇v∗)2 ≤
∥µ∥2
2
𝑡∗
∥v∗∥2
2"
WHENCE,0.4848167539267016,"On the other hand, by triangle inequality we have"
WHENCE,0.48534031413612566,(µ𝑇v∗)2 ≤1
WHENCE,0.48586387434554973,"𝑛ℓ
∥˜Mv∗∥2
2 ≤4 𝑛ℓ"
WHENCE,0.4863874345549738,"
∥( ˜A′ + ˜M)v∗∥2
2 + ∥˜A′v∗∥2
2 
≤4 𝑛ℓ"
WHENCE,0.4869109947643979," ∥µ∥2
2
𝑡∗
∥v∗∥2
2 + ∥˜A′∥2
𝑜𝑝∥v∗∥2
2 "
WHENCE,0.487434554973822,. Now we have
WHENCE,0.48795811518324606,"𝑡∗=
(µ𝑇v∗)2"
WHENCE,0.4884816753926702,"∥( ˜A′ + ˜M)v∗∥2
2 + 𝜌∥v∗∥2
2
≤ 4
𝑛ℓ"
WHENCE,0.48900523560209425,"
∥µ∥2
2
𝑡∗∥v∗∥2
2 + ∥˜A′∥2
𝑜𝑝∥v∗∥2
2 "
WHENCE,0.4895287958115183,"𝜌∥v∗∥2
2
="
WHENCE,0.4900523560209424,"4∥µ∥2
2
𝑡∗
+ 4∥˜A′∥2
𝑜𝑝
𝜌𝑛ℓ"
WHENCE,0.4905759162303665,We obtain a quadratic inequality in 𝑡∗
WHENCE,0.49109947643979057,"𝑡∗2 −
4∥˜A′∥2
𝑜𝑝
𝜌𝑛ℓ
𝑡∗−
4∥µ∥2
2
𝜌𝑛ℓ
≤0"
WHENCE,0.49162303664921464,Which implies the following upper bound
WHENCE,0.49214659685863876,"µ𝑇𝛀−1µ𝑇≤min{
4∥˜A′∥2
𝑜𝑝
𝜌𝑛ℓ
+ 4 √︄"
WHENCE,0.49267015706806283,"∥µ∥2
2
𝜌𝑛ℓ
, 1 𝜌}"
WHENCE,0.4931937172774869,"According to Lemma (8) ∥˜A′∥𝑜𝑝≤𝑛
1
3 +𝑠with probability 1 −𝐶log(𝑑+𝑛)"
WHENCE,0.493717277486911,"𝑛𝑠
.Thus using the law of total
expectation, we also observe that"
WHENCE,0.4942408376963351,"E ˜A,˜y,a,𝑦∥𝛀−1µ∥6
3 ≤E ˜A,˜y,a,𝑦∥𝛀−1µ∥6
2 ≤E ˜A,˜y,a,𝑦(µ𝑇𝛀−1µ)3 ≤
4𝑛
2
3 +2𝑠"
WHENCE,0.49476439790575916,"𝜌𝑛ℓ
+ 4 √︄"
WHENCE,0.4952879581151832,"∥µ∥2
2
𝜌𝑛ℓ"
WHENCE,0.49581151832460735,"3
+ 𝐶log(𝑑+ 𝑛) 𝜌𝑛𝑠"
WHENCE,0.4963350785340314,For 0 < 𝑠< 1
WHENCE,0.4968586387434555,"6, as ∥µ∥2
2 = 𝑂(1) by assumption, the RHS goes to zero.
□"
WHENCE,0.4973821989528796,"Lemma 8. Let ˜A′ be a centered random matrix following the assumptions. We have ∥˜A′∥𝑜𝑝≤𝐶𝑛
1
3 +𝑠"
WHENCE,0.4979057591623037,"with probability at least 1 −𝐶′ log(𝑑+𝑛) 𝑛𝑠
."
WHENCE,0.49842931937172774,"Proof. First, we provide an upper bound for the expectation E ˜A′ ∥˜A′∥𝑜𝑝using the matrix Bernstein
inequality and the symmetrization technique. First we construct a corresponding hermitian matrix:"
WHENCE,0.4989528795811518,∥˜A′∥𝑜𝑝=
WHENCE,0.49947643979057593,"
0
˜A′
˜A′𝑇
0"
WHENCE,0.5,"
𝑜𝑝
= 𝑛
∑︁"
WHENCE,0.5005235602094241,"𝑖=1
X𝑖 𝑜𝑝"
WHENCE,0.5010471204188481,"With X𝑖:=

0
E𝑖𝑖˜A′
˜A′𝑇E𝑖𝑖
0"
WHENCE,0.5015706806282723,"
where E𝑖𝑖is the all-zero matrix except (𝑖, 𝑖)-entry where it is equal to 1."
WHENCE,0.5020942408376964,"Note that I = Í𝑛
𝑖=1 E𝑖𝑖and E𝑖𝑖˜A′ essentially picks the i’th row of ˜A′, which is ˜a
′𝑇
𝑖. Let {𝜖𝑖}𝑛
𝑖=1 be an
iid sequence of symmetric Bernoulli random variables supported on {±1}, independent of X𝑖’s. Then
by the symmetrization lemma Vershynin [2018], we have"
WHENCE,0.5026178010471204,"E ˜A′ ∥˜A′∥𝑜𝑝= E ˜A′  𝑛
∑︁"
WHENCE,0.5031413612565445,"𝑖=1
X𝑖"
WHENCE,0.5036649214659686,"𝑜𝑝
≤E ˜A′,ϵ  𝑛
∑︁"
WHENCE,0.5041884816753927,"𝑖=1
𝜖𝑖X𝑖 𝑜𝑝"
WHENCE,0.5047120418848168,"Now conditioning on ˜A′ or equivalently X𝑖’s, since 𝜖X𝑖’s are independent zero-mean symmetric
matrices of size (𝑑+ 𝑛) × (𝑑+ 𝑛) with bounded operator norm, we can leverage Matrix Bernstein
inequality Vershynin [2018] to obtain (≲means up to some constant)"
WHENCE,0.5052356020942408,"E ˜A′ ∥˜A′∥𝑜𝑝≲
√︁"
WHENCE,0.5057591623036649,"log(𝑑+ 𝑛)E ˜A′  𝑛
∑︁"
WHENCE,0.506282722513089,"𝑖=1
E𝜖𝑖
 𝜖𝑖X𝑖
2 1/2"
WHENCE,0.506806282722513,"𝑜𝑝
+ log(𝑑+ 𝑛)E ˜A′ max
1≤𝑖≤𝑛"
WHENCE,0.5073298429319372,"X𝑖

𝑜𝑝 =
√︁"
WHENCE,0.5078534031413613,"log(𝑑+ 𝑛)E ˜A′  𝑛
∑︁"
WHENCE,0.5083769633507853,"𝑖=1
X2
𝑖  1/2"
WHENCE,0.5089005235602094,"𝑜𝑝
+ log(𝑑+ 𝑛)E ˜A′ max
1≤𝑖≤𝑛"
WHENCE,0.5094240837696336,"X𝑖

𝑜𝑝 =
√︁"
WHENCE,0.5099476439790576,log(𝑑+ 𝑛)E ˜A′ 
WHENCE,0.5104712041884817,"Í𝑛
𝑖=1 E𝑖𝑖˜A′ ˜A′𝑇E𝑖𝑖
0
0
Í𝑛
𝑖=1 ˜a′
𝑖˜a′𝑇
𝑖  1/2"
WHENCE,0.5109947643979058,"𝑜𝑝
+ log(𝑑+ 𝑛)E ˜A′ max
1≤𝑖≤𝑛"
WHENCE,0.5115183246073298,"X𝑖

𝑜𝑝"
WHENCE,0.512041884816754,"Note that for
X𝑖

𝑜𝑝, we have
X𝑖

𝑜𝑝=
E𝑖𝑖˜A′
𝑜𝑝= max
∥v∥2=1"
WHENCE,0.512565445026178,"E𝑖𝑖˜A′v
2
2 = max
∥v∥2=1(˜a
′𝑇
𝑖v)2 = ∥˜a′
𝑖∥2
2"
WHENCE,0.5130890052356021,"Moreover, using triangle inequality"
WHENCE,0.5136125654450262,"Í𝑛
𝑖=1 E𝑖𝑖˜A′ ˜A′𝑇E𝑖𝑖
0
0
Í𝑛
𝑖=1 ˜a′
𝑖˜a′𝑇
𝑖"
WHENCE,0.5141361256544502,"
𝑜𝑝
≤ 𝑛
∑︁"
WHENCE,0.5146596858638743,"𝑖=1
E𝑖𝑖˜A′ ˜A′𝑇E𝑖𝑖 𝑜𝑝
+ 𝑛
∑︁"
WHENCE,0.5151832460732985,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝 = 𝑛
∑︁"
WHENCE,0.5157068062827225,"𝑖=1
∥˜a′
𝑖∥2
2E𝑖𝑖 𝑜𝑝
+ 𝑛
∑︁"
WHENCE,0.5162303664921466,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝"
WHENCE,0.5167539267015707,"= max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2 + 𝑛
∑︁"
WHENCE,0.5172774869109947,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝"
WHENCE,0.5178010471204189,Combining these results along with Jensen’s inequality yields
WHENCE,0.518324607329843,"E ˜A′ ∥˜A′∥𝑜𝑝≲
√︁"
WHENCE,0.518848167539267,"log(𝑑+ 𝑛)

E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2 + E ˜A′  𝑛
∑︁"
WHENCE,0.5193717277486911,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝"
WHENCE,0.5198952879581152,"1/2
+ log(𝑑+ 𝑛)E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2"
WHENCE,0.5204188481675392,Now to analyze E ˜A′
WHENCE,0.5209424083769634,"Í𝑛
𝑖=1 ˜a′
𝑖˜a′𝑇
𝑖"
WHENCE,0.5214659685863874,"𝑜𝑝
, we use the symmetrization trick again with iid Bernoulli {𝜖′
𝑖}𝑛
𝑖=1"
WHENCE,0.5219895287958115,"and Bernstein’s similar to Vershynin’s E ˜A′  𝑛
∑︁"
WHENCE,0.5225130890052356,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖"
WHENCE,0.5230366492146596,"𝑜𝑝
≤E ˜A′  𝑛
∑︁"
WHENCE,0.5235602094240838,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖
−E˜a′
𝑖˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝
+ 𝑛
∑︁"
WHENCE,0.5240837696335079,"𝑖=1
E˜a′
𝑖˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝"
WHENCE,0.5246073298429319,"≤E ˜A′,ϵ′  𝑛
∑︁"
WHENCE,0.525130890052356,"𝑖=1
𝜖′
𝑖˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝
+ 𝑛
∑︁ 𝑖=1"
WHENCE,0.5256544502617801,"E˜a′
𝑖˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝 ≲
√︁"
WHENCE,0.5261780104712042,"log 𝑛E ˜A′  𝑛
∑︁"
WHENCE,0.5267015706806283,"𝑖=1
E𝜖′
𝑖
 𝜖′
𝑖˜a′
𝑖˜a′𝑇
𝑖
2 1/2"
WHENCE,0.5272251308900524,"𝑜𝑝
+ log 𝑛E ˜A′ max
1≤𝑖≤𝑛"
WHENCE,0.5277486910994764,"˜a′
𝑖˜a′𝑇
𝑖∥𝑜𝑝+ 𝑛
∑︁ 𝑖=1"
WHENCE,0.5282722513089005,"E˜a′
𝑖˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝 =
√︁"
WHENCE,0.5287958115183246,"log 𝑛E ˜A′  𝑛
∑︁"
WHENCE,0.5293193717277487,"𝑖=1
∥˜a′
𝑖∥2
2˜a′
𝑖˜a′𝑇
𝑖  1/2"
WHENCE,0.5298429319371728,"𝑜𝑝
+ log 𝑛E ˜A′ max
1≤𝑖≤𝑛"
WHENCE,0.5303664921465968,"˜a′
𝑖∥2
2 + 𝑛
∑︁ 𝑖=1"
WHENCE,0.5308900523560209,"E˜a′
𝑖˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝 ≲
√︁"
WHENCE,0.5314136125654451,"log 𝑛E ˜A′ max
1≤𝑖≤𝑛"
WHENCE,0.5319371727748691,"˜a′
𝑖∥2  𝑛
∑︁"
WHENCE,0.5324607329842932,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖  1/2"
WHENCE,0.5329842931937173,"𝑜𝑝
+ log 𝑛E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2 + 𝑛
∑︁ 𝑖=1"
WHENCE,0.5335078534031413,"E˜a′
𝑖˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝"
WHENCE,0.5340314136125655,"Now for the first term, applying Cauchy Schwartz yields"
WHENCE,0.5345549738219896,"E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2  𝑛
∑︁"
WHENCE,0.5350785340314136,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖  1/2"
WHENCE,0.5356020942408377,"𝑜𝑝
≤

E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2E ˜A′  𝑛
∑︁"
WHENCE,0.5361256544502618,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝 1/2"
WHENCE,0.5366492146596858,"Moreover, for the last term we have from our assumptions
E˜a′
𝑖˜a′
𝑖˜a′𝑇
𝑖"
WHENCE,0.53717277486911,"𝑜𝑝
= max
∥v∥2=1 E˜a′
𝑖(˜a′𝑇
𝑖v)2 ≤max
∥v∥2=1 𝐶
∥v∥2
2
𝑑
= 𝐶"
WHENCE,0.537696335078534,"𝑑
Thus we have E ˜A′  𝑛
∑︁"
WHENCE,0.5382198952879581,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖"
WHENCE,0.5387434554973822,"𝑜𝑝
≲
√︁"
WHENCE,0.5392670157068062,"log 𝑛

E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2E ˜A′  𝑛
∑︁"
WHENCE,0.5397905759162304,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝"
WHENCE,0.5403141361256545,"1/2
+ log 𝑛E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2 + 𝐶"
WHENCE,0.5408376963350785,"Solving for

E ˜A′"
WHENCE,0.5413612565445026,"Í𝑛
𝑖=1 ˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝"
WHENCE,0.5418848167539267,"1/2
yields"
WHENCE,0.5424083769633508,"
E ˜A′  𝑛
∑︁"
WHENCE,0.5429319371727749,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖 𝑜𝑝"
WHENCE,0.543455497382199,"1/2
≲
√︁"
WHENCE,0.543979057591623,"log 𝑛

E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2"
WHENCE,0.5445026178010471,"1/2
+ 2

log 𝑛E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2 + 𝐶
1/2 Thus E ˜A′  𝑛
∑︁"
WHENCE,0.5450261780104712,"𝑖=1
˜a′
𝑖˜a′𝑇
𝑖"
WHENCE,0.5455497382198953,"𝑜𝑝
≲log 𝑛E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2 + 𝐶"
WHENCE,0.5460732984293194,"Summarising, we have"
WHENCE,0.5465968586387434,"E ˜A′ ∥˜A′∥𝑜𝑝≲
√︁"
WHENCE,0.5471204188481675,"log(𝑑+ 𝑛)

E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2 + log 𝑛E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2 + 𝐶
1/2
+ log(𝑑+ 𝑛)E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2"
WHENCE,0.5476439790575917,"Thus it remains to provide an upper bound for E ˜A′ max1≤𝑖≤𝑛∥˜a′
𝑖∥2
2, we have by Jensen’s

E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2"
WHENCE,0.5481675392670157,"3
≤E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥6
2 ≤ 𝑛
∑︁"
WHENCE,0.5486910994764398,"𝑖=1
E˜a′
𝑖∥˜a′
𝑖∥6
2"
WHENCE,0.5492146596858639,Now by Definition 2 and Holder’s we have
WHENCE,0.5497382198952879,"E˜a′
𝑖∥˜a′
𝑖∥6
2 =
∑︁"
WHENCE,0.550261780104712,"𝛼1,𝛼2,𝛼3
E˜a′
𝑖˜a
′2
𝑖𝛼1 ˜a
′2
𝑖𝛼2 ˜a
′2
𝑖𝛼3 ≤
∑︁"
WHENCE,0.5507853403141362,"𝛼1,𝛼2,𝛼3"
WHENCE,0.5513089005235602,"
E˜a′
𝑖˜a
′6
𝑖𝛼1"
WHENCE,0.5518324607329843,"1/3
E˜a′
𝑖˜a
′6
𝑖𝛼2"
WHENCE,0.5523560209424084,"1/3
E˜a′
𝑖˜a
′6
𝑖𝛼3"
WHENCE,0.5528795811518324,"1/3
≤
∑︁"
WHENCE,0.5534031413612566,"𝛼1,𝛼2,𝛼3"
WHENCE,0.5539267015706806,"𝐶
𝑑3 ≤𝐶"
WHENCE,0.5544502617801047,"Thus
E ˜A′ max
1≤𝑖≤𝑛∥˜a′
𝑖∥2
2 ≤𝐶𝑛
1
3"
WHENCE,0.5549738219895288,"All in all
E ˜A′ ∥˜A′∥𝑜𝑝≲log(𝑑+ 𝑛)𝑛
1
3
Thus by Markov’s inequality for 𝑠> 0"
WHENCE,0.5554973821989528,"P ∥˜A′∥𝑜𝑝≥𝐶𝑛
1
3 +𝑠 ≤E ˜A′ ∥˜A′∥𝑜𝑝"
WHENCE,0.556020942408377,"𝐶𝑛
1
3 +𝑠
≤𝐶′ log(𝑑+ 𝑛)𝑛
1
3"
WHENCE,0.5565445026178011,"𝑛
1
3 +𝑠
= 𝐶′ log(𝑑+ 𝑛) 𝑛𝑠
□"
WHENCE,0.5570680628272251,"Lemma 9. For any function 𝜓: R →R with bounded third derivative, ˜𝜓(w) := Ea𝜓(a𝑇w) is regular"
WHENCE,0.5575916230366492,"Proof. We only need to verify the third order Taylor remainder bound,
∑︁"
WHENCE,0.5581151832460733,"𝛼1,𝛼2,𝛼3"
WHENCE,0.5586387434554974,"𝜕3 ˜𝜓
𝜕𝑤𝛼1𝜕𝑤𝛼2𝜕𝑤𝛼3
𝑣𝛼1𝑣𝛼2𝑣𝛼3 =
∑︁"
WHENCE,0.5591623036649215,"𝛼1,𝛼2,𝛼3
Ea𝜓′′′(a𝑇w)𝑎𝛼1𝑎𝛼2𝑎𝛼3𝑣𝛼1𝑣𝛼2𝑣𝛼3 = Ea(a𝑇v)3𝜓′′′(a𝑇w)"
WHENCE,0.5596858638743456,"By Cauchy Schwartz, we have
∑︁"
WHENCE,0.5602094240837696,"𝛼1,𝛼2,𝛼3"
WHENCE,0.5607329842931937,"𝜕3 ˜𝜓
𝜕𝑤𝛼1𝜕𝑤𝛼2𝜕𝑤𝛼3
𝑣𝛼1𝑣𝛼2𝑣𝛼3 ≤
√︃"
WHENCE,0.5612565445026177,"Ea𝜓′′′(a𝑇w)2
√︃"
WHENCE,0.5617801047120419,Ea(a𝑇v)6
WHENCE,0.562303664921466,"By boundedness of the third derivative of 𝜓, and assumptions
∑︁"
WHENCE,0.56282722513089,"𝛼1,𝛼2,𝛼3"
WHENCE,0.5633507853403141,"𝜕3 ˜𝜓
𝜕𝑤𝛼1𝜕𝑤𝛼2𝜕𝑤𝛼3
𝑣𝛼1𝑣𝛼2𝑣𝛼3 ≤𝐶𝜓
∥v∥3
2
𝑑3/2 ≤𝐶𝜓∥v∥3
3 □"
WHENCE,0.5638743455497383,"E
Generalized CGMT"
WHENCE,0.5643979057591623,"After reducing the analysis to the equivalent gaussian model, one can leverage a new generalization
of the CGMT Akhtiamov et al. [2024] to analyze Φ(G) which is stated as follows and recover the
precise asymptotic properties of the solutions."
WHENCE,0.5649214659685864,"Theorem 5 (Generalized CGMT). Let S𝑤⊂R𝑑, S𝑣1 ⊂R𝑛1, . . . , S𝑣𝑘⊂R𝑛𝑘be compact convex
sets. Denote S𝑣:= S𝑣1 × · · · × S𝑣𝑘, let v ∈S𝑣stand for (v1, . . . , v𝑘) ∈S𝑣1 × · · · × S𝑣𝑘and
𝜓(w, v) : S𝑤× S𝑣→R be convex on S𝑤and concave on S𝑣. Also let 𝚺1, . . . , 𝚺𝑘∈R𝑑×𝑑
be arbitrary PSD matrices. Furthermore, let G1 ∈R𝑛1×𝑑, . . . , G𝑘∈R𝑛𝑘×𝑑, g1, . . . , g𝑘∈R𝑑,
h1 ∈R𝑛1, . . . , h𝑘∈R𝑛𝑘all have i.i.d N (0, 1) components and G = (G1, . . . , G𝑘), g = (g1, . . . , g𝑘),
h = (h1, . . . , h𝑘) be the corresponding 𝑘-tuples. Define"
WHENCE,0.5654450261780105,"Ψ(G) := min
w∈S𝑤max
v∈S𝑣 𝑘
∑︁"
WHENCE,0.5659685863874345,"ℓ=1
v𝑇
ℓGℓ𝚺"
WHENCE,0.5664921465968586,"1
2
ℓw + 𝜓(w, v)
(26)"
WHENCE,0.5670157068062828,"𝜙(g, h) := min
w∈S𝑤max
v∈S𝑣 𝑘
∑︁ ℓ=1"
WHENCE,0.5675392670157068,"
∥vℓ∥2g𝑇
ℓ𝚺"
WHENCE,0.5680628272251309,"1
2
ℓw + ∥𝚺"
WHENCE,0.5685863874345549,"1
2
ℓw∥2h𝑇
ℓvℓ"
WHENCE,0.569109947643979,"
+ 𝜓(w, v)
(27)"
WHENCE,0.5696335078534032,"Let S be an arbitrary open subset of S𝑤and S𝑐= S𝑤\ S. Let 𝜙S be the optimal value of the
optimization 27 when S𝑤is restricted to S. Assume also that there exist 𝜖, 𝛿> 0, ¯𝜙, ¯𝜙S𝑐such that"
WHENCE,0.5701570680628272,• ¯𝜙S𝑐≥¯𝜙+ 3𝛿
WHENCE,0.5706806282722513,"• 𝜙(𝑔, ℎ) < ¯𝜙+ 𝛿with probability at least 1 −𝜖"
WHENCE,0.5712041884816754,• 𝜙S𝑐> ¯𝜙S𝑐−𝛿with probability at least 1 −𝜖
WHENCE,0.5717277486910994,"Then for the optimal solution w of the optimization 26, we have P(wΨ(G) ∈S) ≥1 −2𝑘+1𝜖"
WHENCE,0.5722513089005236,"F
Proof of Theorem 2"
WHENCE,0.5727748691099477,"By leveraging our universality theorem 1 we are ready to analyze the linear regression problem
descrided in 2 in full details"
WHENCE,0.5732984293193717,"Proof. First note that for the ground truth we have 𝑤∗= 𝑎𝑟𝑔𝑚𝑖𝑛E𝑃(𝑦−x𝑇w)2 = R−1
𝑥R𝑥𝑦. By a
simple transformation, the constraint could also be written as"
WHENCE,0.5738219895287958," X
y 
w
−1 
= 0"
WHENCE,0.5743455497382199,"We would like to write for a G with i.i.d entries:
 X
y = GS"
WHENCE,0.574869109947644,Indeed
WHENCE,0.5753926701570681,"S𝑇EG𝑇GS = S𝑇S = E

X𝑇 y𝑇"
WHENCE,0.5759162303664922,"  X
y = E

R𝑥
R𝑥𝑦
R𝑦𝑥
𝑅𝑦 "
WHENCE,0.5764397905759162,Now we can denote S =
WHENCE,0.5769633507853403,"R1/2
𝑥
R−𝑇/2
𝑥
R𝑥𝑦
0
𝑅1/2
Δ !"
WHENCE,0.5774869109947643,"with 𝑅Δ := 𝑅𝑦−R𝑦𝑥R−1
𝑥R𝑥𝑦. And R−𝑇/2
𝑥
R𝑥𝑦= R1/2
𝑥w∗."
WHENCE,0.5780104712041885,Thus we can write the optimization as
WHENCE,0.5785340314136126,"min
w ∥w −w0∥2 𝑠.𝑡
G"
WHENCE,0.5790575916230366,"R1/2
𝑥(w −w∗)
−𝑅1/2
Δ ! = 0"
WHENCE,0.5795811518324607,we can write the optimization as
WHENCE,0.5801047120418849,"min
w ∥w −w0∥2 𝑠.𝑡
G"
WHENCE,0.5806282722513089,"R1/2
𝑥(w −w∗)
−𝑅1/2
Δ ! = 0"
WHENCE,0.581151832460733,"Let u := R1/2
𝑥(w −w∗) and x = w∗−w0, then"
WHENCE,0.5816753926701571,"min
w ∥R−1/2
𝑥
u + x∥2"
WHENCE,0.5821989528795811,"𝑠.𝑡
G

u
−𝑅1/2
Δ 
= 0"
WHENCE,0.5827225130890052,We Lagrange multiplier to bring in the constraint:
WHENCE,0.5832460732984294,"min
u max
v
∥R−1/2
𝑥
u + x∥2 + v𝑇G

u
−𝑅1/2
Δ "
WHENCE,0.5837696335078534,Now we can appeal to CGMT to obtain the AO:
WHENCE,0.5842931937172775,"min
u max
v
∥R−1/2
𝑥
u + x∥2 + ∥v∥(g𝑇u + 𝑔′𝑅1/2
Δ ) + ∥

u
−𝑅1/2
Δ"
WHENCE,0.5848167539267015,"
∥h𝑇v"
WHENCE,0.5853403141361256,"Dropping the 𝑔′𝑅1/2
Δ
term and doing the optimization over the direction of v, yields"
WHENCE,0.5858638743455498,"min
u max
𝛽≥0 ∥R−1/2
𝑥
u + x∥2 + 𝛽∥h∥
√︁"
WHENCE,0.5863874345549738,∥u∥2 + 𝑅Δ + 𝛽g𝑇u
WHENCE,0.5869109947643979,Using the identity √𝑥= min𝜏≥0 𝜏
WHENCE,0.587434554973822,2 + 𝑥2 2𝜏
WHENCE,0.587958115183246,"min
𝜏≥0,u max
𝛽≥0 ∥R−1/2
𝑥
u + x∥2 + 𝛽∥h∥𝜏"
WHENCE,0.5884816753926702,"2
+ 𝛽∥h∥"
WHENCE,0.5890052356020943,2𝜏(∥u∥2 + 𝑅Δ) + 𝛽g𝑇u
WHENCE,0.5895287958115183,Now we obtain a quadratic optimization over 𝑢which could be rewritten as
WHENCE,0.5900523560209424," u𝑇
1
 
R−1
𝑥+ 𝛽√𝑛"
WHENCE,0.5905759162303665,"2𝜏I
R−1/2
𝑥
x + 𝛽"
G,0.5910994764397905,"2 g
x𝑇R−𝑇/2
𝑥
+ 𝛽"
G,0.5916230366492147,"2 g𝑇
∥x∥2"
G,0.5921465968586388,"! 
u
1 "
G,0.5926701570680628,"Doing the optimization over u, we are left with"
G,0.5931937172774869,"min
𝜏≥0 max
𝛽≥0 ∥x∥2 + 𝛽𝜏√𝑛"
G,0.5937172774869109,"2
+ 𝛽√𝑛"
G,0.5942408376963351,"2𝜏𝑅Δ −(R−1/2
𝑥
x + 𝛽"
G,0.5947643979057592,"2 g)𝑇(R−1
𝑥+ 𝛽√𝑛"
G,0.5952879581151832,"2𝜏I)−1(R−1/2
𝑥
x + 𝛽 2 g)"
G,0.5958115183246073,Expanding the third term yields
G,0.5963350785340314,"min
𝜏≥0 max
𝛽≥0 ∥x∥2 + 𝛽𝜏√𝑛"
G,0.5968586387434555,"2
+ 𝛽√𝑛"
G,0.5973821989528796,"2𝜏𝑅Δ −x𝑇R−𝑇/2
𝑥
(R−1
𝑥+ 𝛽√𝑛"
G,0.5979057591623037,"2𝜏I)−1R−1/2
𝑥
x −𝛽2"
G,0.5984293193717277,"4 g𝑇(R−1
𝑥+ 𝛽√𝑛"
G,0.5989528795811518,2𝜏I)−1g
G,0.599476439790576,"Let z := R1/2
𝑥x, we assume Ezz𝑇= ∥z∥2"
G,0.6,"𝑑𝐼, and 𝑒𝑎= x𝑇R𝑥x = ∥z∥2 using concentration:"
G,0.6005235602094241,"min
𝜏≥0 max
𝛽≥0 ∥𝑥∥2 + 𝛽𝜏√𝑛"
G,0.6010471204188481,"2
+ 𝛽√𝑛"
G,0.6015706806282722,2𝜏𝑅Δ −𝑑1
G,0.6020942408376964,"𝑑TrR−𝑇
𝑥(R−1
𝑥+ 𝛽√𝑛"
G,0.6026178010471204,"2𝜏I)−1R−1
𝑥−𝑑𝛽2"
G,0.6031413612565445,"4
1
𝑑Tr(R−1
𝑥+ 𝛽√𝑛"
G,0.6036649214659686,2𝜏I)−1
G,0.6041884816753926,Therefore
G,0.6047120418848168,"min
𝜏≥0 max
𝛽≥0 ∥x∥2 + 𝛽𝜏√𝑛"
G,0.6052356020942409,"2
+ 𝛽√𝑛"
G,0.6057591623036649,2𝜏𝑅Δ −𝑒𝑎
G,0.606282722513089,"∫
𝑝(𝑟)
1"
G,0.6068062827225131,𝑟(1 + 𝛽√𝑛
G,0.6073298429319371,"2𝜏𝑟)
𝑑𝑟−𝑑𝛽2 4"
G,0.6078534031413613,"∫
𝑝(𝑟)
𝑟"
G,0.6083769633507854,1 + 𝛽√𝑛
G,0.6089005235602094,"2𝜏𝑟
𝑑𝑟"
G,0.6094240837696335,"Dropping the constant term ∥x∥2, we arrive at the following scalarized optimization"
G,0.6099476439790575,"min
𝜏≥0 max
𝛽≥0
𝛽𝜏√𝑛"
G,0.6104712041884817,"2
+ 𝛽√𝑛"
G,0.6109947643979058,"2𝜏𝑅Δ −𝑑
∫
𝑝(𝑟) 𝑒𝑎 𝑑+ 𝛽2 4 𝑟2"
G,0.6115183246073298,𝑟(1 + 𝛽√𝑛
G,0.6120418848167539,"2𝜏𝑟)
𝑑𝑟"
G,0.612565445026178,"Let 𝑒′
𝑎:= 𝑒𝑎"
G,0.6130890052356021,"𝑑. To analyze the optimization further, noting that the objective is differentiable w.r.t 𝛽, 𝜏,
by taking derivatives we have at the optimal point: 𝜏√𝑛"
G,0.6136125654450262,"2
+
√𝑛𝑅Δ"
G,0.6141361256544503,"2𝜏
−𝑑
∫
𝑝(𝑟) 𝜏(√𝑛𝑟2𝛽2 + 4𝑟𝜏𝛽−4𝑒′
𝑎
√𝑛)
2(2𝜏+ 𝑟𝛽√𝑛)2
𝑑𝑟= 0 𝛽√𝑛"
G,0.6146596858638743,"2
−𝛽𝑅Δ
√𝑛
2𝜏2
−𝑑
∫
𝑝(𝑟) 𝛽√𝑛(𝛽2𝑟2 + 4𝑒′
𝑎)
2(2𝜏+ 𝑟𝛽√𝑛)2 𝑑𝑟= 0"
G,0.6151832460732984,Rearranging
G,0.6157068062827226,"𝜏2√𝑛+ √𝑛𝑅Δ = 𝑑
∫
𝑝(𝑟) 𝜏2(√𝑛𝑟2𝛽2 + 4𝑟𝜏𝛽−4𝑒′
𝑎
√𝑛)
(2𝜏+ 𝑟𝛽√𝑛)2
𝑑𝑟"
G,0.6162303664921466,"𝜏2√𝑛−√𝑛𝑅Δ = 𝑑
∫
𝑝(𝑟) 𝜏2(√𝑛𝛽2𝑟2 + √𝑛4𝑒′
𝑎)
(2𝜏+ 𝑟𝛽√𝑛)2
𝑑𝑟"
G,0.6167539267015707,Summing and subtracting:
G,0.6172774869109947,"𝑛= 𝑑
∫
𝑝(𝑟) 𝑛𝑟2𝛽2 + 2√𝑛𝑟𝜏𝛽"
G,0.6178010471204188,"(2𝜏+ 𝑟𝛽√𝑛)2 𝑑𝑟= 𝑑
∫
𝑝(𝑟)(1 −2√𝑛𝑟𝜏𝛽+ 4𝜏2"
G,0.618324607329843,(2𝜏+ 𝑟𝛽√𝑛)2 )𝑑𝑟
G,0.618848167539267,"√𝑛𝑅Δ = 𝑑
∫
𝑝(𝑟) 𝜏2(2𝑟𝜏𝛽−4𝑒′
𝑎
√𝑛)
(2𝜏+ 𝑟𝛽√𝑛)2
𝑑𝑟 Thus 𝑑−𝑛"
G,0.6193717277486911,"𝑑
=
∫
𝑝(𝑟) 2√𝑛𝑟𝜏𝛽+ 4𝜏2"
G,0.6198952879581152,"(2𝜏+ 𝑟𝛽√𝑛)2 𝑑𝑟=
∫
𝑝(𝑟)
2𝜏
2𝜏+ 𝑟𝛽√𝑛𝑑𝑟= 2𝜏"
G,0.6204188481675392,𝛽√𝑛𝑆𝑃(−2𝜏
G,0.6209424083769634,𝛽√𝑛+ 𝑖0)
G,0.6214659685863875,"√𝑛𝑅Δ = 𝑑
∫
𝑝(𝑟) 𝜏2(2𝑟𝜏𝛽−4𝑒′
𝑎
√𝑛)
(2𝜏+ 𝑟𝛽√𝑛)2
𝑑𝑟"
G,0.6219895287958115,If one can solve 𝑑−𝑛
G,0.6225130890052356,"𝑑
=
2𝜏
𝛽√𝑛𝑆𝑃(−2𝜏"
G,0.6230366492146597,𝛽√𝑛+ 𝑖0) to find 𝜃:= 𝛽
G,0.6235602094240837,"𝜏, then plugging into the second equation
yields"
G,0.6240837696335079,"√𝑛𝑅2
Δ = 𝑑
∫
𝑝(𝑟) 𝜏2(2𝑟𝜃𝜏2 −4𝑒′
𝑎
√𝑛)
(2𝜏+ 𝑟𝜃𝜏√𝑛)2
𝑑𝑟= 𝑑
∫
𝑝(𝑟) 2𝑟𝜃𝜏2 −4𝑒′
𝑎
√𝑛
(2 + 𝑟𝜃√𝑛)2
𝑑𝑟"
G,0.624607329842932,Which yields the following equation for 𝜏2
G,0.625130890052356,"𝜏2 =
√𝑛"
G,0.6256544502617801,"2𝜃𝑑
∫
𝑝(𝑟)
𝑟
(2+𝑟𝜃√𝑛)2 𝑑𝑟"
G,0.6261780104712041,"
𝑅Δ + 4𝑑𝑒′
𝑎"
G,0.6267015706806283,"∫
𝑝(𝑟)
(2 + 𝑟𝜃√𝑛)2 𝑑𝑟
"
G,0.6272251308900524,"Now note that
𝑑−𝑛"
G,0.6277486910994764,"𝑑
=
∫
𝑝(𝑟)
2𝜏
2𝜏+ 𝑟𝛽√𝑛𝑑𝑟=
∫
𝑝(𝑟)
2
2 + 𝑟𝜃√𝑛𝑑𝑟=
∫
𝑝(𝑟) 2(2 + 𝑟𝜃√𝑛)"
G,0.6282722513089005,(2 + 𝑟𝜃√𝑛)2 𝑑𝑟=
G,0.6287958115183246,"= 2𝜃√𝑛
∫
𝑝(𝑟)
𝑟
(2 + 𝑟𝜃√𝑛)2 𝑑𝑟+ 4
∫
𝑝(𝑟)
(2 + 𝑟𝜃√𝑛)2 𝑑𝑟"
G,0.6293193717277487,"Let 𝛼:= 4
∫
𝑝(𝑟)
(2+𝑟𝜃√𝑛)2 𝑑𝑟, 𝛼′ := 2𝜃√𝑛
∫
𝑝(𝑟)
𝑟
(2+𝑟𝜃√𝑛)2 𝑑𝑟and . Then the expression for 𝜏could be
written as"
G,0.6298429319371728,"𝜏2 =
𝑛
𝑑𝛼′ (𝑅Δ + 𝐵𝑒𝑎)"
G,0.6303664921465969,As 𝛼+ 𝛼′ = 𝑑−𝑛
G,0.6308900523560209,"𝑑, one can write"
G,0.631413612565445,"𝜏2 =
𝑛"
G,0.6319371727748692,𝑑( 𝑑−𝑛
G,0.6324607329842932,"𝑑
−𝛼)
(𝑅Δ + 𝛼𝑒𝑎) =
𝑛
𝑑−𝑛−𝛼𝑑(𝑅Δ + 𝛼𝑒𝑎) =
𝑛
𝑑(1 −𝛼) −𝑛𝑅Δ +
𝛼𝑛
𝑑(1 −𝛼) −𝑛𝑒𝑎"
G,0.6329842931937173,Now the generalization error is
G,0.6335078534031413,𝜏2 −𝑅Δ = 2𝑛−𝑑(1 −𝛼)
G,0.6340314136125654,"𝑑(1 −𝛼) −𝑛𝑅Δ +
𝑛𝐵
𝑑(1 −𝛼) −𝑛𝑒𝑎= (2𝑛−𝑑)𝑅Δ + (𝑑𝑅Δ + 𝑛𝑒𝑎)𝛼"
G,0.6345549738219896,𝑑−𝑛−𝑑𝛼
G,0.6350785340314136,"With 𝛼= 4
∫
𝑝(𝑟)
(2+𝑟𝜃√𝑛)2 𝑑𝑟. Now note that using Cauchy Schwarz applied to
√︁"
G,0.6356020942408377,"𝑝(𝑟) and
2√"
G,0.6361256544502618,"𝑝(𝑟)
2+𝑟𝜃√𝑛 ( 𝑑−𝑛"
G,0.6366492146596858,"𝑑
)2 = (
∫
𝑝(𝑟)
2
2 + 𝑟𝜃√𝑛𝑑𝑟)2 ≤
∫
𝑝(𝑟)𝑑𝑟
∫
4𝑝(𝑟)
(2 + 𝑟𝜃√𝑛)2 𝑑𝑟= 𝛼"
G,0.63717277486911,Thus 𝛼≥( 𝑑−𝑛
G,0.6376963350785341,"𝑑)2. Now for the scalar distribution 𝑝(𝑟) = 𝛿(𝑟−𝑟0), since 2+𝑟0𝜃√𝑛= 2+ 2𝑛"
G,0.6382198952879581,"𝑑−𝑛=
2𝑑
𝑑−𝑛,
then 𝛼= ( 𝑑−𝑛"
G,0.6387434554973822,"𝑑)2. Thus 𝛼achieves its lower bound (uniquely) for the scalar distribution. Now
note that assuming 𝑛, 𝑑, 𝑒𝑎, 𝑅Δ are fixed, the generalization error is increasing in 𝛼, thus the error
expression for the generalization error in the scalar distribution case is a lower bound.
□"
G,0.6392670157068063,"G
Useful Results"
G,0.6397905759162303,"Lemma 10. Given a weight vector w, and assuming the feature vectors are equally likely to be drawn
from class 1 or class 2 and satisfy part 2 of Assumptions 4, the corresponding classification error is
given by"
G,0.6403141361256545,"1
2𝑄(
µ𝑇
1 w
√︁"
G,0.6408376963350786,"w𝑇𝚺1w
) + 1"
G,0.6413612565445026,"2𝑄(−
µ𝑇
2 w
√︁"
G,0.6418848167539267,"w𝑇𝚺2w
)
(28)"
G,0.6424083769633507,"Lemma 11. Assume that the feature vectors are equally likely to be drawn from class 1 or class 2,
Assumptions 4 hold and w0 is defined as in Assumption 2. Then the classification error corresponding
to w0 is equal to"
G,0.6429319371727749,"1
2𝑄
©­­­­
« √"
G,0.643455497382199,"1 −𝑟Tr(𝚺1 + 𝚺2)−1
√︂"
G,0.643979057591623,"2Tr(𝚺1(𝚺1 + 𝚺2)−2) +
𝑡2𝜂
𝑡2∗Tr𝚺1"
G,0.6445026178010471,"ª®®®®
¬ + 1"
G,0.6450261780104712,"2𝑄
©­­­­
« √"
G,0.6455497382198953,"1 −𝑟Tr(𝚺1 + 𝚺2)−1
√︂"
G,0.6460732984293194,"2Tr(𝚺2(𝚺1 + 𝚺2)−2) +
𝑡2𝜂
𝑡2∗Tr𝚺2"
G,0.6465968586387435,"ª®®®®
¬"
G,0.6471204188481675,"H
Proof of Lemma 1"
G,0.6476439790575916,Let us proceed to prove Lemma 1. We will take the gradient of (28) w.r.t. 𝑤and set it to 0:
G,0.6481675392670158,Proof. ∇w
G,0.6486910994764398,"""
1
2𝑄(−
µ𝑇
2 w
√︁"
G,0.6492146596858639,"w𝑇𝚺2w
) # =
1 4
√ 2𝜋 """
G,0.6497382198952879,"exp (−
w𝑇µ2µ𝑇
2 w"
G,0.650261780104712,2w𝑇𝚺2w ) √︁ w𝑇𝚺2w
G,0.6507853403141362,"µ𝑇
2 w"
G,0.6513089005235602,"w𝑇𝚺2wµ2µ𝑇
2 w −w𝑇µ2µ𝑇
2 w𝚺2w"
G,0.6518324607329843,"(w𝑇𝚺2w)2 # = 1 4
√ 2𝜋"
G,0.6523560209424084,"""
1
𝑠2
𝑒𝑥𝑝(−
𝑠2
2
2 )(µ2𝑡2 −𝑠2
2𝚺2w) #"
G,0.6528795811518324,"where 𝑠2 =
µ𝑇
2 w
√︁"
G,0.6534031413612565,"w𝑇𝚺2w
, 𝑡2 =
µ𝑇
2 w w𝑇𝚺2w ∇w"
G,0.6539267015706807,"""
1
2𝑄(
µ𝑇
1 w
√︁"
G,0.6544502617801047,"w𝑇𝚺1w
) # = −
1 4
√ 2𝜋 """
G,0.6549738219895288,"exp (−
w𝑇µ1µ𝑇
1 w"
G,0.6554973821989529,2w𝑇𝚺1w ) √︁ w𝑇𝚺1w
G,0.6560209424083769,"µ𝑇
1 w"
G,0.6565445026178011,"w𝑇𝚺1wµ1µ𝑇
1 w −w𝑇µ1µ𝑇
1 w𝚺1w"
G,0.6570680628272252,"(w𝑇𝚺1w)2 # = −
1 4
√ 2𝜋"
G,0.6575916230366492,"""
1
𝑠1
exp (−
𝑠2
1
2 )(µ1𝑡1 −𝑠2
1𝚺1w) #"
G,0.6581151832460733,"where 𝑠1 =
µ𝑇
1 w
√︁"
G,0.6586387434554973,"w𝑇𝚺1w
, 𝑡1 =
µ𝑇
1 w w𝑇𝚺1w"
G,0.6591623036649215,"Hence, the following equality holds at any optimal w:"
G,0.6596858638743456,"1
𝑠2
exp (−
𝑠2
2
2 )(µ2𝑡2 −𝑠2
2𝚺2w) = 1"
G,0.6602094240837696,"𝑠1
exp (−
𝑠2
1
2 )(µ1𝑡1 −𝑠2
1𝚺1w)"
G,0.6607329842931937,"Therefore, w = (˜𝑠1𝚺1 + ˜𝑠2𝚺2)−1(˜𝑡1µ1 −˜𝑡2µ2), where we have"
G,0.6612565445026178,"˜𝑠1 = 𝑠1 exp (−
𝑠2
1
2 ),
˜𝑠2 = −𝑠2 exp (−
𝑠2
2
2 ),"
G,0.6617801047120419,˜𝑡1 = 𝑡1
G,0.662303664921466,"𝑠1
exp (−
𝑠2
1
2 ),
˜𝑡2 = 𝑡2"
G,0.6628272251308901,"𝑠2
exp (−
𝑠2
2
2 )"
G,0.6633507853403141,Now we can consider the following optimization over 4 scalar variables:
G,0.6638743455497382,"min
˜𝑠1, ˜𝑠2,˜𝑡1,˜𝑡2
1
2𝑄(
µ𝑇
1 w
√︁"
G,0.6643979057591624,"w𝑇𝚺1w
) + 1"
G,0.6649214659685864,"2𝑄(−
µ𝑇
2 w
√︁"
G,0.6654450261780105,"w𝑇𝚺2w
)
(29)"
G,0.6659685863874345,"Note that since the optimization (29) is low-dimensional, by the Convexity Lemma Liese and Miescke
[2008], we can interchange the limit in 𝑛and min, implying that we can analyze the concentrated
optimization instead. We then arrive at the following fixed point equations using Assumptions 4:"
G,0.6664921465968586,"𝑠2
1
𝑡2
1
= Tr(𝚺1(˜𝑠1𝚺1 + ˜𝑠2𝚺2)−2)(˜𝑡2
1 + ˜𝑡2
2 −2˜𝑡1˜𝑡2𝑟)𝑑
(30)"
G,0.6670157068062827,"𝑠2
1
𝑡1
= Tr(˜𝑠1𝚺1 + ˜𝑠2𝚺2)−1(˜𝑡1 −˜𝑡2𝑟)𝑑
(31)"
G,0.6675392670157068,"𝑠2
2
𝑡2
2
= Tr(𝚺2(˜𝑠1𝚺1 + ˜𝑠2𝚺2)−2)(˜𝑡2
1 + ˜𝑡2
2 −2˜𝑡1˜𝑡2𝑟)𝑑
(32)"
G,0.6680628272251309,"𝑠2
2
𝑡2
= Tr(˜𝑠1𝚺1 + ˜𝑠2𝚺2)−1(˜𝑡2 −˜𝑡1𝑟)𝑑
(33)"
G,0.668586387434555,"Note that, since we assume 𝑝is exchangeable in its arguments, the system of equations (30) is
invariant under the transformation (𝑠1, 𝑡1, 𝑠2, 𝑡2) →(−𝑠2, 𝑡2, −𝑠1, 𝑡1). As such, 𝑠1 = −𝑠2 and 𝑡1 = 𝑡2
at the optimal point, implying that w∗= (𝚺1 + 𝚺2)−1(µ1 −µ2) is the optimal classifier, as for the
chosen linear classification procedure the performance in invariant to the rescalings of the classifier
vector and therefore we can completely drop 𝑠1, 𝑠2, 𝑡1, 𝑡2 from the equations. □"
G,0.669109947643979,"I
Proof of Theorem 3"
G,0.6696335078534031,"By Gaussian universality proved in Theorem 1, we analyze the binary classification problem as
follows"
G,0.6701570680628273,Proof. Define
G,0.6706806282722513,"R0 := Eµ1,µ2w0w𝑇
0 = Eµ1,µ2(𝑡∗w∗+ 𝑡𝜂η)(𝑡∗w∗+ 𝑡𝜂η)𝑇=
𝑡2
𝜂(1 −𝑟)"
G,0.6712041884816754,"𝑑
𝐼𝑑+ 𝑡2
∗Eµ1,µ2w∗w𝑇
∗"
G,0.6717277486910995,"=
𝑡2
𝜂(1 −𝑟)"
G,0.6722513089005235,"𝑑
𝐼𝑑+ 2𝑡2
∗
𝑑(1 −𝑟)(𝚺1 + 𝚺2)−2"
G,0.6727748691099477,"Note that the eigenvalue density of R0 is captured by 𝜙(𝑠1, 𝑠2) =
𝑡2
𝜂(1−𝑟)"
G,0.6732984293193718,"𝑑
+ 2𝑡2
∗(1−𝑟)"
G,0.6738219895287958,"𝑑
(𝑠1 + 𝑠2)−2,
assuming (𝑠1, 𝑠2) ∼𝑝, where 𝑝is the joint EPD function of 𝚺1 and 𝚺2."
G,0.6743455497382199,"We would like to start with finding 𝛼. Note that the data matrix X can be decomposed as follows
where G1, G2 are i.i.d. standard normal: X ="
G,0.6748691099476439,"G1𝚺1/2
1
G2𝚺1/2
2 !"
G,0.675392670157068,"+

1
0
0
1"
G,0.6759162303664922," 
µ𝑇
1
µ𝑇
2 "
G,0.6764397905759162,We then have: Xw0 =
G,0.6769633507853403,"G1𝚺1/2
1 w0
G2𝚺1/2
2 w0 !"
G,0.6774869109947644,"+

1
0
0
1"
G,0.6780104712041884," 
µ𝑇
1 w0
µ𝑇
2 w0"
G,0.6785340314136126,"
(34)"
G,0.6790575916230367,"It is immediate to see that the first term of (34) is a centered normally distributed random vector
with covariance diag(w𝑇
0 𝚺1w0, . . . , w𝑇
0 𝚺1w0, w𝑇
0 𝚺2w0, . . . , w𝑇
0 𝚺2w0), and therefore its norm con-"
G,0.6795811518324607,"centrates to
√︃"
G,0.6801047120418848,"𝑛
2 w𝑇
0 (𝚺1 + 𝚺2)w0 = Θ(
√︁"
G,0.680628272251309,Tr(𝚺1 + 𝚺2)∥w0∥) according to our assumptions. The second
G,0.681151832460733,"term of (34) is a deterministic vector of norm Θ(√𝑛∥w0∥
√"
G,0.6816753926701571,1 −𝑟).
G,0.6821989528795811,Similarly:
G,0.6827225130890052,"y𝑇Xw0 =  1𝑇
−1𝑇  
G1𝚺1/2
1 w0
G2𝚺1/2
2 w0 !"
G,0.6832460732984293,"+

1
0
0
1"
G,0.6837696335078534," 
µ𝑇
1 w0
µ𝑇
2 w0"
G,0.6842931937172775,"
=  1𝑇
−1𝑇
 
G1𝚺1/2
1 w0
G2𝚺1/2
2 w0 ! + 𝑛"
G,0.6848167539267016,2 (µ1 −µ2)𝑇w0 (35)
G,0.6853403141361256,"Note that we can drop the first term of (35) compared to the second term. Indeed, the first term is a
centered normal random variable with variance 𝑛"
G,0.6858638743455497,"2 w𝑇
0 (𝚺1 + 𝚺2)w0 = Θ(∥w0∥2Tr(𝚺1 + 𝚺2)) and the
second term is a deterministic quantity of order Θ(𝑛∥w0∥
√"
G,0.6863874345549739,1 −𝑟). Thus
G,0.6869109947643979,𝛼= y𝑇Xw0
G,0.687434554973822,∥Xw0∥2 =
G,0.6879581151832461,"𝑛
2 (µ1 −µ2)𝑇w0
𝑛
2 w𝑇
0 (𝚺1 + 𝚺2)w0 + 𝑛"
G,0.6884816753926701,"2 (µ𝑇
1 w0)2 + 𝑛"
G,0.6890052356020943,"2 (µ𝑇
2 w0)2"
G,0.6895287958115183,"Moreover, by independence of η from µ𝑖we can drop the first term in (µ1 −µ2)𝑇w0 = 𝑡𝜂(µ1 −
µ2)𝑇η + 𝑡∗(µ1 −µ2)𝑇w∗and have"
G,0.6900523560209424,"(µ1 −µ2)𝑇w∗= (µ1 −µ2)𝑇(𝚺1 + 𝚺2)−1(µ1 −µ2)
P−→2(1 −𝑟)Tr(𝚺1 + 𝚺2)−1"
G,0.6905759162303665,On the other hand:
G,0.6910994764397905,"µ𝑇
1 w0 = 𝑡𝜂µ𝑇
1 η + 𝑡∗µ𝑇
1 w∗= 𝑡𝜂µ𝑇
1 η + 𝑡∗µ𝑇
1 (𝚺1 + 𝚺2)−1(µ1 −µ2)"
G,0.6916230366492147,"Which implies that we can drop the cross term in the expansion of (µ𝑇
1 w0)2:"
G,0.6921465968586388,"(µ𝑇
1 w0)2 ≈𝑡2
𝜂(µ𝑇
1 η)2 + 𝑡2
∗

µ𝑇
1 (𝚺1 + 𝚺2)−1(µ1 −µ2)
2 P−→1 −𝑟"
G,0.6926701570680628,"𝑑
𝑡2
𝜂+ 𝑡2
∗(1 −𝑟)2"
G,0.6931937172774869,"𝑑2
Tr2(𝚺1 + 𝚺2)−1"
G,0.693717277486911,"We can drop the first without loss of generality as 𝑡𝜂usually does not grow with 𝑑. This implies that
we have for 𝛼 𝛼="
G,0.694240837696335,"𝑛
2
𝑡∗(1−𝑟)"
G,0.6947643979057592,"𝑑
2Tr(𝚺1 + 𝚺2)−1"
G,0.6952879581151833,"𝑛
2 Tr(R0(𝚺1 + 𝚺2)) + 𝑛𝑡2∗(1−𝑟)2"
G,0.6958115183246073,"𝑑2
Tr2(𝚺1 + 𝚺2)−1 ="
G,0.6963350785340314,"1
2
𝑡∗(1−𝑟)"
G,0.6968586387434555,"𝑑
2Tr(𝚺1 + 𝚺2)−1"
G,0.6973821989528796,"1
2Tr(R0(𝚺1 + 𝚺2)) + 𝑡2∗(1−𝑟)2"
G,0.6979057591623037,"𝑑2
Tr2(𝚺1 + 𝚺2)−1"
G,0.6984293193717277,Then SGD converges to
G,0.6989528795811518,"min
w ∥w −𝛼w0∥2"
G,0.6994764397905759,"𝑠.𝑡
y = Xw"
G,0.7,Introducing a Lagrange multiplier v ∈R𝑛:
G,0.7005235602094241,"min
w max
v
∥w −𝛼w0∥2 + v𝑇G"
G,0.7010471204188482,"𝚺1/2
1
𝚺1/2
2 !"
G,0.7015706806282722,w + v𝑇Mw −v𝑇y
G,0.7020942408376963,"Using Theorem 4 from Akhtiamov et al. [2024],"
G,0.7026178010471205,"min max ∥−𝛼0∥2 + 2
∑︁"
G,0.7031413612565445,"𝑖=1
∥𝑣𝑖∥𝑇
𝑖𝚺1/2
𝑖
+ ∥𝚺1/2
𝑖
∥h𝑇
𝑖𝑖+ 𝑇
𝑖1µ𝑇
𝑖+ 𝑇
𝑖1(−1)𝑖"
G,0.7036649214659686,Denoting 𝛽𝑖:= ∥v𝑖∥and performing optimization over the direction of 𝑣𝑖:
G,0.7041884816753927,"min max
𝛽1,𝛽2≥0 ∥−𝛼0∥2 + 2
∑︁"
G,0.7047120418848167,"𝑖=1
𝛽𝑖(𝑇
𝑖𝚺1/2
𝑖
+ ∥h𝑖∥𝚺1/2
𝑖
∥+ µ𝑇
𝑖1 + (−1)𝑖1∥)"
G,0.7052356020942409,We have
G,0.7057591623036649,"min
w
max
𝛽1,𝛽2≥0 ∥w −𝛼w0∥2 + 2
∑︁"
G,0.706282722513089,"𝑖=1
𝛽𝑖(g𝑇
𝑖𝚺1/2
𝑖
w +
√︂ 𝑛
2 √︃"
G,0.7068062827225131,"∥𝚺1/2
𝑖
w∥2 + (µ𝑇
𝑖w + (−1)𝑖)2)"
G,0.7073298429319371,Applying the square root trick to the norm inside the objective we arrive at:
G,0.7078534031413612,"min
𝜏1,𝜏2≥0,w max
𝛽1,𝛽2≥0 ∥w −𝛼w0∥2 + 2
∑︁"
G,0.7083769633507854,"𝑖=1
𝛽𝑖g𝑇
𝑖𝚺1/2
𝑖
w +
√︂"
G,0.7089005235602094,"𝑛
2
𝛽𝑖𝜏𝑖"
G,0.7094240837696335,"2
+ 𝛽𝑖 2𝜏𝑖 √︂"
G,0.7099476439790576,"𝑛
2 (∥𝚺1/2
𝑖
w∥2 + (µ𝑇
𝑖w + (−1)𝑖)2)"
G,0.7104712041884816,"Introducing 𝛾𝑖’s as the Fenchel duals for (µ𝑇
𝑖w + (−1)𝑖)2 we have:"
G,0.7109947643979058,"min
𝜏1,𝜏2≥0,w
max
𝛽1,𝛽2≥0,𝛾1,𝛾2 ∥w −𝛼w0∥2 + 2
∑︁"
G,0.7115183246073299,"𝑖=1
𝛽𝑖g𝑇
𝑖𝚺1/2
𝑖
w +
√︂"
G,0.7120418848167539,"𝑛
2
𝛽𝑖𝜏𝑖"
G,0.712565445026178,"2
+ 𝛽𝑖 2𝜏𝑖 √︂"
G,0.7130890052356021,"𝑛
2 (∥𝚺1/2
𝑖
w∥2 + µ𝑇
𝑖w𝛾𝑖+ (−1)𝑖𝛾𝑖−𝛾2
𝑖
4 )"
G,0.7136125654450262,Performing optimization over w yields:
G,0.7141361256544503,"min
𝜏1,𝜏2≥0
max
𝛽1,𝛽2≥0,𝛾1,𝛾2 𝛼2∥w0∥2 + 2
∑︁ 𝑖=1 √︂"
G,0.7146596858638743,"𝑛
2
𝛽𝑖𝜏𝑖"
G,0.7151832460732984,"2
+
√︂"
G,0.7157068062827225,"𝑛
2
𝛽𝑖
2𝜏𝑖
(𝛾𝑖(−1)𝑖−𝛾2
𝑖
4 )"
G,0.7162303664921466,"−

−𝛼w0 + 1"
G,0.7167539267015707,"2 (𝛽1𝚺1/2
1 g1 + 𝛽2𝚺1/2
2 g2) + 𝛽1 4𝜏1 √︂"
G,0.7172774869109948,"𝑛
2𝛾1µ1 + 𝛽2 4𝜏2 √︂"
G,0.7178010471204188,"𝑛
2𝛾2µ2
𝑇
𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7183246073298429,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7188481675392671,"𝑛
2𝚺2
−1"
G,0.7193717277486911,"·

−𝛼w0 + 1"
G,0.7198952879581152,"2 (𝛽1𝚺1/2
1 g1 + 𝛽2𝚺1/2
2 g2) + 𝛽1 4𝜏1 √︂"
G,0.7204188481675393,"𝑛
2𝛾1µ1 + 𝛽2 4𝜏2 √︂"
G,0.7209424083769633,"𝑛
2𝛾2µ2
"
G,0.7214659685863875,"We drop the 𝛼2∥w0∥2 from the objective because it does not contain any of the variables the
optimization is performed over and therefore dropping it will not change the optimal values of
𝛽1, 𝛽2, 𝛾1, 𝛾2, 𝜏1, 𝜏2. We obtain:"
G,0.7219895287958115,"min
𝜏1,𝜏2≥0
max
𝛽1,𝛽2≥0,𝛾1,𝛾2 2
∑︁ 𝑖=1 √︂"
G,0.7225130890052356,"𝑛
2
𝛽𝑖𝜏𝑖"
G,0.7230366492146597,"2
+
√︂"
G,0.7235602094240837,"𝑛
2
𝛽𝑖
2𝜏𝑖
(𝛾𝑖(−1)𝑖−𝛾2
𝑖
4 ) −𝛼2w𝑇
0 (𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7240837696335078,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.724607329842932,"𝑛
2𝚺2)−1w0−"
G,0.725130890052356,"−
𝛽2
1
4 g𝑇
1 𝚺𝑇/2
1
(𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7256544502617801,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7261780104712042,"𝑛
2𝚺2)−1𝚺1/2
1 g1 −
𝛽2
2
4 g𝑇
2 𝚺𝑇/2
2
(𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7267015706806282,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7272251308900524,"𝑛
2𝚺2)−1𝚺1/2
2 g2−"
G,0.7277486910994765,"−
𝑛𝛽2
1𝛾2
1
32𝜏2
1
µ𝑇
1 (𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7282722513089005,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7287958115183246,"𝑛
2𝚺2)−1µ1 −
𝑛𝛽2
2𝛾2
2
32𝜏2
2
µ𝑇
2 (𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7293193717277487,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7298429319371728,"𝑛
2𝚺2)−1µ2+ + 𝛽1 2𝜏1 √︂"
G,0.7303664921465969,"𝑛
2𝛾1𝛼w𝑇
0 (𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7308900523560209,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.731413612565445,"𝑛
2𝚺2)−1µ1 + 𝛽2 2𝜏2 √︂"
G,0.7319371727748691,"𝑛
2𝛾2𝛼w𝑇
0 (𝐼+ 𝛽2 2𝜏2 √︂"
G,0.7324607329842932,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7329842931937173,"𝑛
2𝚺2)−1µ2−"
G,0.7335078534031414,−𝑛𝛽1𝛽2𝛾1𝛾2
G,0.7340314136125654,"16𝜏1𝜏2
µ𝑇
1 (𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7345549738219895,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7350785340314137,"𝑛
2𝚺2)−1µ2"
G,0.7356020942408377,The objective above can be rewritten in the integral form in the following way:
G,0.7361256544502618,"min
𝜏1,𝜏2≥0
max
𝛽1,𝛽2≥0,𝛾1,𝛾2 2
∑︁ 𝑖=1 √︂"
G,0.7366492146596859,"𝑛
2
𝛽𝑖𝜏𝑖"
G,0.7371727748691099,"2
+
√︂"
G,0.737696335078534,"𝑛
2
𝛽𝑖
2𝜏𝑖
(𝛾𝑖(−1)𝑖−𝛾2
𝑖
4 ) −𝛼2w𝑇
0 (𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7382198952879581,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7387434554973822,"𝑛
2𝚺2)−1w0−"
G,0.7392670157068063,"−𝑑
∫∫
𝑝(𝑠1, 𝑠2)"
G,0.7397905759162303,"𝑠1𝛽2
1+𝑠2𝛽2
2
4
1 + 𝛽1"
G,0.7403141361256544,"2𝜏1
√︁𝑛"
G,0.7408376963350786,2 𝑠1 + 𝛽2
G,0.7413612565445026,"2𝜏2
√︁𝑛"
G,0.7418848167539267,"2 𝑠2
𝑑𝑠1𝑑𝑠2 −
∫∫
𝑝(𝑠1, 𝑠2)"
G,0.7424083769633508,"𝑛𝛽2
1 𝛾2
1
32𝜏2
1 +
𝑛𝛽2
2 𝛾2
2
32𝜏2
2
1 + 𝛽1"
G,0.7429319371727748,"2𝜏1
√︁𝑛"
G,0.743455497382199,2 𝑠1 + 𝛽2
G,0.7439790575916231,"2𝜏2
√︁𝑛"
G,0.7445026178010471,"2 𝑠2
𝑑𝑠1𝑑𝑠2+ + 𝛽1 2𝜏1 √︂"
G,0.7450261780104712,"𝑛
2𝛾1𝛼w𝑇
0 (𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7455497382198953,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7460732984293194,"𝑛
2𝚺2)−1µ1 + 𝛽2 2𝜏2 √︂"
G,0.7465968586387435,"𝑛
2𝛾2𝛼w𝑇
0 (𝐼+ 𝛽2 2𝜏2 √︂"
G,0.7471204188481675,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7476439790575916,"𝑛
2𝚺2)−1µ2"
G,0.7481675392670157,"−2𝑟
∫∫
𝑝(𝑠1, 𝑠2)"
G,0.7486910994764397,𝑛𝛽1𝛽2𝛾1𝛾2
G,0.7492146596858639,"32𝜏1𝜏2
1 + 𝛽1"
G,0.749738219895288,"2𝜏1
√︁𝑛"
G,0.750261780104712,2 𝑠1 + 𝛽2
G,0.7507853403141361,"2𝜏2
√︁𝑛"
G,0.7513089005235603,"2 𝑠2
𝑑𝑠1𝑑𝑠2"
G,0.7518324607329843,Note that
G,0.7523560209424084,"𝛼2w𝑇
0 (𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7528795811518325,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7534031413612565,"𝑛
2𝚺2)−1w0 →𝛼2Tr(R0(𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7539267015706806,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7544502617801047,"𝑛
2𝚺2)−1)"
G,0.7549738219895288,Moreover
G,0.7554973821989529,Tr(R0(𝐼+ 𝛽1 2𝜏1 √︂
G,0.7560209424083769,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.756544502617801,"𝑛
2𝚺2)−1) →𝑑
∫∫
𝑝(𝑠1, 𝑠2)
𝜙(𝑠1, 𝑠2)"
G,0.7570680628272252,1 + 𝛽1
G,0.7575916230366492,"2𝜏1
√︁𝑛"
G,0.7581151832460733,2 𝑠1 + 𝛽2
G,0.7586387434554974,"2𝜏2
√︁𝑛"
G,0.7591623036649214,"2 𝑠2
𝑑𝑠1𝑑𝑠2 And"
G,0.7596858638743456,"w𝑇
0 (𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7602094240837697,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7607329842931937,"𝑛
2𝚺2)−1µ1 →𝑡∗(1 −𝑟)"
G,0.7612565445026178,"𝑑
Tr((𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7617801047120419,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.762303664921466,"𝑛
2𝚺2)−1(𝚺1 + 𝚺2)−1)"
G,0.7628272251308901,The latter term can be derived in the integral form as:
G,0.7633507853403141,"Tr

(𝐼+ 𝛽1 2𝜏1 √︂"
G,0.7638743455497382,"𝑛
2𝚺1 + 𝛽2 2𝜏2 √︂"
G,0.7643979057591623,"𝑛
2𝚺2)−1(𝚺1 + 𝚺2)−1

→𝑑
∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)−1"
G,0.7649214659685863,1 + 𝛽1
G,0.7654450261780105,"2𝜏1
√︁𝑛"
G,0.7659685863874346,2 𝑠1 + 𝛽2
G,0.7664921465968586,"2𝜏2
√︁𝑛"
G,0.7670157068062827,"2 𝑠2
𝑑𝑠1𝑑𝑠2"
G,0.7675392670157068,"Since 𝑝(𝑠1, 𝑠2) is exchangeable in its arguments, we see that the latter objective remains invariant
under the transformation that swaps (𝛽1, 𝜏1, 𝛾1) with (𝛽2, 𝜏2, −𝛾2). Therefore, 𝛽:= 𝛽1 = 𝛽2,
𝜏:= 𝜏1 = 𝜏2 and 𝛾:= 𝛾2 = −𝛾1 holds at the optimal point. The objective then reduces to the
following:"
G,0.7680628272251309,"min
𝜏≥0 max
𝛽≥0,𝛾 √︂"
G,0.768586387434555,"𝑛
2 𝛽𝜏+
√︂"
G,0.7691099476439791,"𝑛
2
𝛽
𝜏(𝛾−𝛾2"
G,0.7696335078534031,"4 ) −
∫∫
𝑝(𝑠1, 𝑠2)
𝑑𝛼2𝜙(𝑠1, 𝑠2) + 𝑑(𝑠1+𝑠2)𝛽2"
G,0.7701570680628272,"4
+ 𝑛𝛽2𝛾2"
G,0.7706806282722513,16𝜏2 (1 −𝑟) 1 + 𝛽
G,0.7712041884816754,2𝜏(𝑠1 + 𝑠2)√︁𝑛
G,0.7717277486910995,"2
𝑑𝑠1𝑑𝑠2 (36)"
G,0.7722513089005235,"−2𝑡∗(1 −𝑟)𝛼
√︂ 𝑛
2"
G,0.7727748691099476,"∫∫
𝑝(𝑠1, 𝑠2)"
G,0.7732984293193718,"𝛽𝛾
2𝜏(𝑠1 + 𝑠2)−1 1 + 𝛽"
G,0.7738219895287958,"2𝜏
√︁𝑛"
G,0.7743455497382199,"2 (𝑠1 + 𝑠2)
(37)"
G,0.774869109947644,"Note that 𝛾can be found explicitly in terms of 𝛽and 𝜏as the optimization over 𝛾is quadratic. Denote
𝑒0 = ∥𝑤0∥2"
G,0.775392670157068,"𝑑
. To facilitate derivations, let us introduce a few notations. Let 𝐹1( 𝛽"
G,0.7759162303664922,"𝜏, 𝛾) :=
∫∫
𝑝(𝑠1, 𝑠2)
𝑑𝛼2𝜙(𝑠1, 𝑠2) + 𝑛𝛽2𝛾2"
G,0.7764397905759163,16𝜏2 (1 −𝑟) + 2𝑡∗(1 −𝑟)𝛼√︁𝑛
G,0.7769633507853403,"2
𝛽𝛾
2𝜏(𝑠1 + 𝑠2)−1 1 + 𝛽"
G,0.7774869109947644,2𝜏(𝑠1 + 𝑠2)√︁𝑛
G,0.7780104712041885,"2
𝑑𝑠1𝑑𝑠2 𝐹2( 𝛽"
G,0.7785340314136125,"𝜏, 𝛽) := 𝑑
∫∫
𝑝(𝑠1, 𝑠2)"
G,0.7790575916230367,(𝑠1+𝑠2)𝛽2
G,0.7795811518324607,"4
1 + 𝛽"
G,0.7801047120418848,"2𝜏
√︁𝑛"
G,0.7806282722513089,"2 (𝑠1 + 𝑠2)
𝑑𝑠1𝑑𝑠2"
G,0.7811518324607329,Thus the objective is
G,0.7816753926701571,"min
𝜏≥0 max
𝛽≥0,𝛾−𝐹1( 𝛽"
G,0.7821989528795812,"𝜏, 𝛾) −𝐹2( 𝛽"
G,0.7827225130890052,"𝜏, 𝛽) +
√︂"
G,0.7832460732984293,"𝑛
2 𝛽𝜏+
√︂"
G,0.7837696335078534,"𝑛
2
𝛽
𝜏(𝛾−𝛾2 4 )"
G,0.7842931937172775,Now taking derivatives w.r.t 𝜏yields
G,0.7848167539267016,𝜕𝜏𝐹1( 𝛽
G,0.7853403141361257,"𝜏, 𝛾) = −𝛽"
G,0.7858638743455497,"𝜏2 𝜕𝑥𝐹1(𝑥, 𝛾)"
G,0.7863874345549738,𝜕𝜏𝐹2( 𝛽
G,0.7869109947643979,"𝜏, 𝛽) = −𝛽"
G,0.787434554973822,"𝜏2 𝜕𝑥𝐹2(𝑥, 𝛽)"
G,0.7879581151832461,Now for 𝛽:
G,0.7884816753926701,𝜕𝛽𝐹1( 𝛽
G,0.7890052356020942,"𝜏, 𝛾) = 1"
G,0.7895287958115184,"𝜏𝜕𝑥𝐹1(𝑥, 𝛾)"
G,0.7900523560209424,𝜕𝛽𝐹2( 𝛽
G,0.7905759162303665,"𝜏, 𝛽) = 2"
G,0.7910994764397906,𝛽𝐹2( 𝛽
G,0.7916230366492146,"𝜏, 𝛽) + 1"
G,0.7921465968586388,"𝜏𝜕𝑥𝐹2(𝑥, 𝛽)"
G,0.7926701570680629,Setting the derivative of the objective w.r.t 𝜏to 0:
G,0.7931937172774869,"𝛽
𝜏2 𝜕𝑥𝐹1(𝑥, 𝛾) + 𝛽"
G,0.793717277486911,"𝜏2 𝜕𝑥𝐹2(𝑥, 𝛽) +
√︂"
G,0.7942408376963351,"𝑛
2 𝛽−
√︂"
G,0.7947643979057591,"𝑛
2
𝛽
𝜏2 (𝛾−𝛾2"
G,0.7952879581151833,4 ) = 0
G,0.7958115183246073,Multiplying by 𝜏2 and dropping 𝛽yields:
G,0.7963350785340314,"𝜕𝑥𝐹1(𝑥, 𝛾) + 𝜕𝑥𝐹2(𝑥, 𝛽) +
√︂"
G,0.7968586387434555,"𝑛
2𝜏2 −
√︂"
G,0.7973821989528795,"𝑛
2 (𝛾−𝛾2"
G,0.7979057591623037,"4 ) = 0
(38)"
G,0.7984293193717278,Derivative w.r.t. 𝛽 −1
G,0.7989528795811518,"𝜏𝜕𝑥𝐹1(𝑥, 𝛾) −2"
G,0.7994764397905759,𝛽𝐹2( 𝛽
G,0.8,"𝜏, 𝛽) −1"
G,0.8005235602094241,"𝜏𝜕𝑥𝐹2(𝑥, 𝛽) +
√︂"
G,0.8010471204188482,"𝑛
2𝜏+
√︂"
G,0.8015706806282723,"𝑛
2
1
𝜏(𝛾−𝛾2"
G,0.8020942408376963,4 ) = 0
G,0.8026178010471204,"Multiplying by 𝜏,"
G,0.8031413612565445,"−𝜕𝑥𝐹1(𝑥, 𝛾) −2𝜏"
G,0.8036649214659686,𝛽𝐹2( 𝛽
G,0.8041884816753927,"𝜏, 𝛽) −𝜕𝑥𝐹2(𝑥, 𝛽) +
√︂"
G,0.8047120418848167,"𝑛
2𝜏2 +
√︂"
G,0.8052356020942408,"𝑛
2 (𝛾−𝛾2"
G,0.805759162303665,4 ) = 0
G,0.806282722513089,"Therefore the set of equations is:
(
𝜕𝑥𝐹1(𝑥, 𝛾) + 𝜕𝑥𝐹2(𝑥, 𝛽) + √︁𝑛"
G,0.8068062827225131,2 𝜏2 −√︁𝑛
G,0.8073298429319372,2 (𝛾−𝛾2
G,0.8078534031413612,4 ) = 0
G,0.8083769633507853,"−𝜕𝑥𝐹1(𝑥, 𝛾) −2𝜏"
G,0.8089005235602095,𝛽𝐹2( 𝛽
G,0.8094240837696335,"𝜏, 𝛽) −𝜕𝑥𝐹2(𝑥, 𝛽) + √︁𝑛"
G,0.8099476439790576,2 𝜏2 + √︁𝑛
G,0.8104712041884817,2 (𝛾−𝛾2
G,0.8109947643979057,4 ) = 0
G,0.8115183246073299,Summing the equations yields −2𝜏
G,0.8120418848167539,𝛽𝐹2( 𝛽
G,0.812565445026178,"𝜏, 𝛽) + 2
√︂"
G,0.8130890052356021,"𝑛
2𝜏2 = 0 →2"
G,0.8136125654450261,𝛽𝐹2( 𝛽
G,0.8141361256544503,"𝜏, 𝛽) = 2
√︂ 𝑛
2𝜏"
G,0.8146596858638744,Which implies
G,0.8151832460732984,"𝑑
∫∫
𝑝(𝑠1, 𝑠2)"
G,0.8157068062827225,(𝑠1+𝑠2)𝛽
G,0.8162303664921466,"2
1 + 𝛽"
G,0.8167539267015707,"2𝜏
√︁𝑛"
G,0.8172774869109948,"2 (𝑠1 + 𝑠2)
𝑑𝑠1𝑑𝑠2 = 2
√︂ 𝑛
2𝜏"
G,0.8178010471204189,"Multiplying both sides by
√𝑛"
G,0.8183246073298429,"2
𝑑𝜏and introducing 𝜃= 𝛽"
G,0.818848167539267,𝜏we obtain:
G,0.819371727748691,"∫∫
𝑝(𝑠1, 𝑠2)"
G,0.8198952879581152,"𝜃
2
√︁𝑛"
G,0.8204188481675393,"2 (𝑠1 + 𝑠2) 1 + 𝜃 2
√︁𝑛"
G,0.8209424083769633,"2 (𝑠1 + 𝑠2)
)𝑑𝑠1𝑑𝑠2 = 𝑛 𝑑"
G,0.8214659685863874,"This can be rewritten as:
∫∫
𝑝(𝑠1, 𝑠2)(1 −
2
√ 2 2
√"
G,0.8219895287958116,"2 + 𝜃√𝑛(𝑠1 + 𝑠2)
)𝑑𝑠1𝑑𝑠2 = 𝑛 𝑑"
G,0.8225130890052356,We arrive at the following equation for 𝜃: 𝑑−𝑛
G,0.8230366492146597,"𝑑
=
∫∫
𝑝(𝑠1, 𝑠2)
2
√ 2 2
√"
G,0.8235602094240838,"2 + 𝜃√𝑛(𝑠1 + 𝑠2)
𝑑𝑠1𝑑𝑠2
(39)"
G,0.8240837696335078,"The last equation above can be reformulated in terms of the Stiltjes transform as follows: 2
√"
G,0.824607329842932,"2
𝜃√𝑛𝑆𝚺1+𝚺2(−2
√"
G,0.8251308900523561,"2
𝜃√𝑛) = 𝑑−𝑛 𝑑"
G,0.8256544502617801,"After finding 𝜃from the equation above, we proceed to identify the optimal value of 𝛾. We will do so
by taking the derivative of (36) by 𝛾and setting it to 0: √︂"
G,0.8261780104712042,"𝑛
2𝜃(1 −𝛾"
G,0.8267015706806282,"2 ) −
∫∫
𝑝(𝑠1, 𝑠2) 𝑛𝜃2𝛾"
G,0.8272251308900523,"8
(1 −𝑟) 1 + 𝜃"
G,0.8277486910994765,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.8282722513089005,"2
𝑑𝑠1𝑑𝑠2 −𝛼
√︂ 𝑛
2"
G,0.8287958115183246,"∫∫
𝑝(𝑠1, 𝑠2) 𝜃𝑡∗(1 −𝑟)(𝑠1 + 𝑠2)−1 1 + 𝜃 2
√︁𝑛"
G,0.8293193717277487,"2 (𝑠1 + 𝑠2)
= 0"
G,0.8298429319371727,"Dividing both sides by 𝜃, factoring √𝑛out and using (39) we obtain:"
G,0.8303664921465969,"𝛾√𝑛
 1 2
√"
G,0.830890052356021,"2
+
√𝑛𝜃(1 −𝑟) 8
𝑑−𝑛 𝑑"
G,0.831413612565445,"
=
√︂"
G,0.8319371727748691,"𝑛
2 −𝛼
√︂ 𝑛
2"
G,0.8324607329842932,"∫∫
𝑝(𝑠1, 𝑠2) 𝑡∗(1 −𝑟)(𝑠1 + 𝑠2)−1 1 + 𝜃 2
√︁𝑛"
G,0.8329842931937173,2 (𝑠1 + 𝑠2)
G,0.8335078534031414,"Hence, we can find 𝛾via 𝛾= 1
2 + √"
G,0.8340314136125655,2𝑛𝜃(1 −𝑟)(𝑑−𝑛) 8𝑑 !−1
G,0.8345549738219895,"1 −𝛼
∫∫
𝑝(𝑠1, 𝑠2) 𝑡∗(1 −𝑟)(𝑠1 + 𝑠2)−1 1 + 𝜃 2
√︁𝑛"
G,0.8350785340314136,2 (𝑠1 + 𝑠2) !
G,0.8356020942408376,"Finally, to find 𝜏, recall (40)"
G,0.8361256544502618,"𝜕𝑥𝐹1(𝑥, 𝛾) + 𝜕𝑥𝐹2(𝑥, 𝛽) +
√︂"
G,0.8366492146596859,"𝑛
2𝜏2 −
√︂"
G,0.8371727748691099,"𝑛
2 (𝛾−𝛾2"
G,0.837696335078534,4 ) = 0
G,0.8382198952879581,Using 𝐹2 is quadratic in its second argument we have:
G,0.8387434554973822,"𝜕𝑥𝐹1(𝑥, 𝛾) + 𝜏2(𝜕𝑥𝐹2(𝑥, 𝜃) +
√︂"
G,0.8392670157068063,"𝑛
2) =
√︂"
G,0.8397905759162304,"𝑛
2 (𝛾−𝛾2 4 )"
G,0.8403141361256544,"𝜏2(𝜕𝑥𝐹2(𝑥, 𝜃) +
√︂"
G,0.8408376963350785,"𝑛
2) =
√︂"
G,0.8413612565445027,"𝑛
2 (𝛾−𝛾2"
G,0.8418848167539267,"4 ) −𝜕𝑥𝐹1(𝑥, 𝛾)"
G,0.8424083769633508,"𝜏2 = (𝜕𝑥𝐹2(𝑥, 𝜃) +
√︂"
G,0.8429319371727748,"𝑛
2)−1(
√︂"
G,0.8434554973821989,"𝑛
2 (𝛾−𝛾2"
G,0.8439790575916231,"4 ) −𝜕𝑥𝐹1(𝑥, 𝛾))"
G,0.8445026178010471,We have
G,0.8450261780104712,"𝐹1(𝑥, 𝛾) :=
∫∫
𝑝(𝑠1, 𝑠2)
𝑑𝛼2𝜙(𝑠1, 𝑠2) + 𝑛𝑥2𝛾2"
G,0.8455497382198953,"16
(1 −𝑟) + 2𝑡∗(1 −𝑟)𝛼√︁𝑛 2
𝑥𝛾"
G,0.8460732984293193,2 (𝑠1 + 𝑠2)−1 1 + 𝑥
G,0.8465968586387435,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.8471204188481676,"2
𝑑𝑠1𝑑𝑠2 Then"
G,0.8476439790575916,"𝜕𝑥𝐹1(𝑥, 𝛾) = −𝑑𝛼2 2 √︂ 𝑛
2"
G,0.8481675392670157,"∫∫
𝑝(𝑠1, 𝑠2)
𝜙(𝑠1, 𝑠2)(𝑠1 + 𝑠2)"
G,0.8486910994764398,(1 + 𝑥
G,0.8492146596858638,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.849738219895288,2 )2 𝑑𝑠1𝑑𝑠2
G,0.8502617801047121,+𝑛𝑥𝛾2(1 −𝑟) 8
G,0.8507853403141361,"∫∫
𝑝(𝑠1, 𝑠2)
1 1 + 𝑥"
G,0.8513089005235602,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.8518324607329842,"2
𝑑𝑠1𝑑𝑠2"
G,0.8523560209424084,"−𝑛𝑥2𝛾2(1 −𝑟) 32 √︂ 𝑛
2"
G,0.8528795811518325,"∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)"
G,0.8534031413612565,(1 + 𝑥
G,0.8539267015706806,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.8544502617801047,2 )2 𝑑𝑠1𝑑𝑠2
G,0.8549738219895288,"−𝛼𝑡∗(1 −𝑟)𝛾
√︂ 𝑛
2"
G,0.8554973821989529,"∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)−1 1 + 𝑥"
G,0.856020942408377,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.856544502617801,"2
𝑑𝑠1𝑑𝑠2"
G,0.8570680628272251,"+𝛼𝑡∗(1 −𝑟)𝑥𝛾 2
𝑛
2"
G,0.8575916230366493,"∫∫
𝑝(𝑠1, 𝑠2)
1"
G,0.8581151832460733,(1 + 𝑥
G,0.8586387434554974,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.8591623036649214,2 )2 𝑑𝑠1𝑑𝑠2
G,0.8596858638743455,Moreover
G,0.8602094240837697,"𝐹2(𝑥, 𝜃) = 𝑑
∫∫
𝑝(𝑠1, 𝑠2)"
G,0.8607329842931937,(𝑠1+𝑠2) 𝜃2
G,0.8612565445026178,"4
1 + 𝑥 2
√︁𝑛"
G,0.8617801047120419,"2 (𝑠1 + 𝑠2)
𝑑𝑠1𝑑𝑠2 Then"
G,0.8623036649214659,"𝜕𝑥𝐹2(𝑥, 𝜃) = −𝑑𝜃2 8 √︂ 𝑛
2"
G,0.86282722513089,"∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)2"
G,0.8633507853403142,"(1 + 𝑥 2
√︁𝑛"
G,0.8638743455497382,2 (𝑠1 + 𝑠2))2 𝑑𝑠1𝑑𝑠2
G,0.8643979057591623,"Thus, all in all:"
G,0.8649214659685864,"𝜏2 =
√︂"
G,0.8654450261780104,"𝑛
2 −𝑑𝜃2 8 √︂ 𝑛
2"
G,0.8659685863874346,"∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)2"
G,0.8664921465968587,"(1 + 𝜃 2
√︁𝑛"
G,0.8670157068062827,2 (𝑠1 + 𝑠2))2 𝑑𝑠1𝑑𝑠2 −1 √︂
G,0.8675392670157068,"𝑛
2 (𝛾−𝛾2"
G,0.8680628272251308,"4 ) + 𝑑𝛼2 2 √︂ 𝑛
2"
G,0.868586387434555,"∫∫
𝑝(𝑠1, 𝑠2)
𝜙(𝑠1, 𝑠2)(𝑠1 + 𝑠2)"
G,0.8691099476439791,(1 + 𝜃
G,0.8696335078534031,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.8701570680628272,2 )2 𝑑𝑠1𝑑𝑠2
G,0.8706806282722513,−𝑛𝜃𝛾2(1 −𝑟) 8
G,0.8712041884816754,"∫∫
𝑝(𝑠1, 𝑠2)
1 1 + 𝜃"
G,0.8717277486910995,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.8722513089005236,"2
𝑑𝑠1𝑑𝑠2"
G,0.8727748691099476,"+ 𝑛𝜃2𝛾2(1 −𝑟) 32 √︂ 𝑛
2"
G,0.8732984293193717,"∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)"
G,0.8738219895287959,(1 + 𝜃
G,0.8743455497382199,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.874869109947644,2 )2 𝑑𝑠1𝑑𝑠2
G,0.875392670157068,"−𝛼𝑡∗(1 −𝑟)𝛾
√︂ 𝑛
2"
G,0.8759162303664921,"∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)−1 1 + 𝜃"
G,0.8764397905759163,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.8769633507853403,"2
𝑑𝑠1𝑑𝑠2"
G,0.8774869109947644,+ 𝛼𝑡∗(1 −𝑟)𝜃𝛾𝑛 4
G,0.8780104712041885,"∫∫
𝑝(𝑠1, 𝑠2)
1"
G,0.8785340314136125,(1 + 𝜃
G,0.8790575916230366,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.8795811518324608,"2 )2 𝑑𝑠1𝑑𝑠2  𝛾= 1
2 + √"
G,0.8801047120418848,2𝑛𝜃(1 −𝑟)(𝑑−𝑛) 8𝑑 !−1
G,0.8806282722513089,"1 −𝛼
∫∫
𝑝(𝑠1, 𝑠2) 𝑡∗(1 −𝑟)(𝑠1 + 𝑠2)−1 1 + 𝜃 2
√︁𝑛"
G,0.881151832460733,2 (𝑠1 + 𝑠2) ! (40)
G,0.881675392670157,"The generalization error can be found through 𝑄

1−𝛾 2
√"
G,0.8821989528795812,"𝜏2−( 𝛾 2 )2 
."
G,0.8827225130890053,"I.1
Single direction case."
G,0.8832460732984293,In the special case when Σ1 = Σ2 = 𝜎2 I𝑑
G,0.8837696335078534,"𝑑we obtain as R0 =
𝑡2
𝜂(1−𝑟)"
G,0.8842931937172774,"𝑑
𝐼𝑑+ 2 𝑡2
∗
𝑑(1 −𝑟)(𝚺1 + 𝚺2)−2:"
G,0.8848167539267016,𝛼= y𝑇Xw0
G,0.8853403141361257,∥Xw0∥2 =
G,0.8858638743455497,𝑡∗(1−𝑟)
G,0.8863874345549738,"𝑑
Tr(𝚺1 + 𝚺2)−1"
G,0.8869109947643979,"1
2Tr(R0(𝚺1 + 𝚺2)) + 𝑡2∗(1−𝑟)2"
G,0.887434554973822,"𝑑2
Tr2(𝚺1 + 𝚺2)−1"
G,0.8879581151832461,"=
𝑡∗
𝑑
2𝜎2"
G,0.8884816753926702,"1
𝑑𝜎2(𝑡2𝜂+ 𝑡2∗
𝑑2
2𝜎4 ) + 𝑡2∗(1 −𝑟) 𝑑2 4𝜎4"
G,0.8890052356020942,We also have
G,0.8895287958115183,"𝛼=
𝑡∗(1 −𝑟)
𝑑
2𝜎2"
G,0.8900523560209425,𝑡2𝜂(1−𝑟)
G,0.8905759162303665,"2𝑑
Tr 2𝜎2"
G,0.8910994764397906,𝑑I + 𝑡2∗
G,0.8916230366492146,𝑑(1 −𝑟)Tr( 2𝜎2
G,0.8921465968586387,𝑑I)−1 + 𝑡2∗(1−𝑟)2
G,0.8926701570680629,"𝑑2
Tr2( 2𝜎2 𝑑I)−1"
G,0.8931937172774869,"=
𝑡∗(1 −𝑟)
𝑑
2𝜎2"
G,0.893717277486911,𝑡2𝜂(1−𝑟) 𝜎2
G,0.8942408376963351,"𝑑
+ 𝑑𝑡2∗(1−𝑟)"
G,0.8947643979057591,"2𝜎2
+ 𝑡2∗(1−𝑟)2𝑑2"
G,0.8952879581151832,"4𝜎4
=
𝑡∗
𝑑
2𝜎2 𝑡2𝜂𝜎2"
G,0.8958115183246074,"𝑑
+ 𝑑𝑡2∗"
G,0.8963350785340314,2𝜎2 + 𝑡2∗(1−𝑟)𝑑2
G,0.8968586387434555,"4𝜎4
=
𝑡∗
𝑑
2𝜎2 𝜎2"
G,0.8973821989528796,"𝑑(𝑡2𝜂+ 𝑡2∗
𝑑2
2𝜎4 ) + 𝑡2∗(1 −𝑟) 𝑑2 4𝜎4"
G,0.8979057591623036,"Multiplying both sides by 𝑡∗(1 −𝑟)
𝑑
2𝜎2 we arrive at:"
G,0.8984293193717278,Γ := 𝛼𝑡∗(1 −𝑟) 𝑑
G,0.8989528795811519,"2𝜎2 =
1"
G,0.8994764397905759,"𝜎2
𝑑(1−𝑟) (
𝑡2𝜂
𝑡2∗
4𝜎4"
G,0.9,"𝑑2 + 2) + 1
(41)"
G,0.900523560209424,Note that plugging in w0 = 𝑡∗(𝚺1 + 𝚺2)−1(µ1 −µ2) + 𝑡𝜂η for 𝚺1 = 𝚺2 = 𝜎2
G,0.9010471204188482,"𝑑I𝑑into the expression
for the generalization error yields"
G,0.9015706806282723,"𝑒𝑎= 𝑄
©­­
«"
G,0.9020942408376963,"µ𝑇
1 w0
√︃"
G,0.9026178010471204,"w𝑇
0 𝚺1w0 ª®®
¬"
G,0.9031413612565445,"= 𝑄
©­­
«"
G,0.9036649214659686,"𝑡∗(1 −𝑟)
𝑑
2𝜎2
√︃"
G,0.9041884816753927,"𝑡2∗(1 −𝑟)
𝑑
2𝜎2 + 𝜎2"
G,0.9047120418848168,𝑑𝑡2𝜂(1 −𝑟)
G,0.9052356020942408,"ª®®
¬
Taking 𝑄−1 of both sides and then squaring them we arrive at:"
G,0.9057591623036649,"𝑄−1(𝑒𝑎)2 =
𝑡2
∗(1 −𝑟)2 𝑑2 4𝜎4"
G,0.9062827225130891,"𝑡2∗(1 −𝑟)
𝑑
2𝜎2 + 𝜎2"
G,0.9068062827225131,"𝑑𝑡2𝜂(1 −𝑟)
=
𝑡2
∗(1 −𝑟)2 𝑑2 4𝜎4"
G,0.9073298429319372,"𝑡2∗(1 −𝑟)
𝑑
2𝜎2 + 𝜎2"
G,0.9078534031413612,"𝑑𝑡2𝜂(1 −𝑟)
=
1"
G,0.9083769633507853,"𝜎2
(1−𝑟)𝑑(2 + 4 𝜎4"
G,0.9089005235602095,"𝑑2
𝑡2𝜂
𝑡2∗)"
G,0.9094240837696335,This yields that by taking 𝜌:= 𝑑(1−𝑟) 𝜎2 𝑡𝜂
G,0.9099476439790576,"𝑡∗
=
𝑑
2𝜎2"
G,0.9104712041884817,"√︂
𝜌
𝑄−1(𝑒𝑎)2 −2 =
𝜌
2(1 −𝑟)"
G,0.9109947643979057,"√︂
𝜌
𝑄−1(𝑒𝑎)2 −2"
G,0.9115183246073298,We thus obtain from (41):
G,0.912041884816754,"Γ =
1
𝑄−1(𝑒𝑎)−2 + 1 =
1
𝑄−1(𝑒𝑎)−2 + 1 =
𝑄−1(𝑒𝑎)2"
G,0.912565445026178,"𝑄−1(𝑒𝑎)2 + 1 = 1 −
1
𝑄−1(𝑒𝑎)2 + 1
(42)"
G,0.9130890052356021,"We proceed to find 𝜃next. From the general case we had 2
√"
G,0.9136125654450262,"2
𝜃√𝑛𝑆𝚺1+𝚺2(−2
√"
G,0.9141361256544502,"2
𝜃√𝑛) = 𝑑−𝑛 𝑑"
G,0.9146596858638744,"Now plugging for the distribution of 𝚺1, 𝚺2 yields"
G,0.9151832460732985,"1 + 𝜃
√︂"
G,0.9157068062827225,"𝑛
2
𝜎2"
G,0.9162303664921466,"𝑑=
𝑑
𝑑−𝑛 𝜃
√︂"
G,0.9167539267015706,"𝑛
2
𝜎2"
G,0.9172774869109948,"𝑑=
𝑛
𝑑−𝑛"
G,0.9178010471204189,"𝜃=
𝑑
√"
G,0.9183246073298429,"2𝑛
(𝑑−𝑛)𝜎2"
G,0.918848167539267,"Using (41) we can simplify the last expression above: 𝛾= 1
2 + √"
G,0.9193717277486911,2𝑛𝜃(1 −𝑟)(𝑑−𝑛) 8𝑑 !−1
G,0.9198952879581151,"1 −𝛼
∫∫
𝑝(𝑠1, 𝑠2) 𝑡∗(1 −𝑟)(𝑠1 + 𝑠2)−1 1 + 𝜃 2
√︁𝑛"
G,0.9204188481675393,"2 (𝑠1 + 𝑠2) ! = = 1
2 + √"
G,0.9209424083769634,"2𝑛𝜃(1 −𝑟)(𝑑−𝑛) 8𝑑 !−1 1 −
Γ 1 + 𝜃 2
√︁𝑛"
G,0.9214659685863874,"2 (𝑠1 + 𝑠2) ! = ©­
« 1
2 + √"
G,0.9219895287958115,"2𝑛
𝑑
√"
G,0.9225130890052357,"2𝑛
(𝑑−𝑛) 𝜎2 (1 −𝑟)(𝑑−𝑛)"
G,0.9230366492146597,"8𝑑
ª®
¬"
G,0.9235602094240838,"−1
©­
«
1 −
Γ"
G,0.9240837696335078,"1 +
𝑑
√"
G,0.9246073298429319,"2𝑛
2(𝑑−𝑛) 𝜎2
√︁𝑛 2
2𝜎2 𝑑)"
G,0.925130890052356,"ª®
¬
="
G,0.9256544502617801,Thus we can write for 𝛾 𝛾=
G,0.9261780104712042,"1
2 +
𝑛2"
G,0.9267015706806283,𝜎2 (1 −𝑟) 8
G,0.9272251308900523,"!−1 
1 −
Γ
1 +
𝑛
𝑑−𝑛  =
1"
G,0.9277486910994764,2 + 𝑛(1 −𝑟) 4𝜎2
G,0.9282722513089006,"−1 
1 −𝑑−𝑛"
G,0.9287958115183246,"𝑑
(1 −
1
𝑄−1(𝑒𝑎)2 + 1)

="
G,0.9293193717277487,"𝑛
𝑑+ 𝑑−𝑛"
G,0.9298429319371728,"𝑑
1
𝑄−1(𝑒𝑎)2+1
1
2 + 𝜌"
G,0.9303664921465968,"4𝜅
where 𝜌:= 𝑑(1 −𝑟) 𝜎2"
G,0.930890052356021,"Furthermore, we have for 𝜏2"
G,0.9314136125654451,"𝜏2 =
√︂"
G,0.9319371727748691,"𝑛
2 −𝑑𝜃2 8 √︂ 𝑛
2"
G,0.9324607329842932,"∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)2"
G,0.9329842931937172,"(1 + 𝜃 2
√︁𝑛"
G,0.9335078534031414,2 (𝑠1 + 𝑠2))2 𝑑𝑠1𝑑𝑠2 −1 √︂
G,0.9340314136125655,"𝑛
2 (𝛾−𝛾2"
G,0.9345549738219895,"4 ) + 𝑑𝛼2 2 √︂ 𝑛
2"
G,0.9350785340314136,"∫∫
𝑝(𝑠1, 𝑠2)
𝜙(𝑠1, 𝑠2)(𝑠1 + 𝑠2)"
G,0.9356020942408377,(1 + 𝜃
G,0.9361256544502617,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.9366492146596859,2 )2 𝑑𝑠1𝑑𝑠2
G,0.93717277486911,−𝑛𝜃𝛾2(1 −𝑟) 8
G,0.937696335078534,"∫∫
𝑝(𝑠1, 𝑠2)
1 1 + 𝜃"
G,0.9382198952879581,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.9387434554973823,"2
𝑑𝑠1𝑑𝑠2"
G,0.9392670157068063,"+ 𝑛𝜃2𝛾2(1 −𝑟) 32 √︂ 𝑛
2"
G,0.9397905759162304,"∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)"
G,0.9403141361256544,(1 + 𝜃
G,0.9408376963350785,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.9413612565445026,2 )2 𝑑𝑠1𝑑𝑠2
G,0.9418848167539267,"−𝛼𝑡∗(1 −𝑟)𝛾
√︂ 𝑛
2"
G,0.9424083769633508,"∫∫
𝑝(𝑠1, 𝑠2)
(𝑠1 + 𝑠2)−1 1 + 𝜃"
G,0.9429319371727749,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.9434554973821989,"2
𝑑𝑠1𝑑𝑠2"
G,0.943979057591623,+ 𝛼𝑡∗(1 −𝑟)𝜃𝛾𝑛 4
G,0.9445026178010472,"∫∫
𝑝(𝑠1, 𝑠2)
1"
G,0.9450261780104712,(1 + 𝜃
G,0.9455497382198953,2 (𝑠1 + 𝑠2)√︁𝑛
G,0.9460732984293194,"2 )2 𝑑𝑠1𝑑𝑠2 
="
G,0.9465968586387434,Simplifying furthermore yields
G,0.9471204188481676,"𝜏2 =

1 −𝑑 𝑛"
G,0.9476439790575916,"∫∫
𝑝(𝑠1, 𝑠2)"
G,0.9481675392670157,"𝑛2
(𝑑−𝑛)2"
G,0.9486910994764398,"𝑑2
(𝑑−𝑛)2
𝑑𝑠1𝑑𝑠2 −1"
G,0.9492146596858638,"
(𝛾−𝛾2"
G,0.949738219895288,"4 ) +
∫∫
𝛼2𝑝(𝑠1, 𝑠2)(1 −𝑟)2(𝑡2
𝜂+ 𝑡2
∗
𝑑2
2𝜎4 )𝜎2"
G,0.9502617801047121,"𝑑(1 −𝑟)(
𝑑
𝑑−𝑛)2
𝑑𝑠1𝑑𝑠2"
G,0.9507853403141361,"−
𝑛
2𝑑
(𝑑−𝑛) 𝜎2 𝛾2(1 −𝑟) 8"
G,0.9513089005235602,"∫∫
𝑝(𝑠1, 𝑠2) 1"
G,0.9518324607329843,"𝑑
𝑑−𝑛
𝑑𝑠1𝑑𝑠2"
G,0.9523560209424083,"+
𝑛
2𝑛𝑑2
(𝑑−𝑛)2 𝜎4 𝛾2(1 −𝑟) 32"
G,0.9528795811518325,"∫∫
𝑝(𝑠1, 𝑠2) 2𝜎2"
G,0.9534031413612566,"𝑑
(
𝑑
𝑑−𝑛)2 𝑑𝑠1𝑑𝑠2"
G,0.9539267015706806,"−𝛼𝑡∗(1 −𝑟)𝛾
∫∫
𝑝(𝑠1, 𝑠2) 𝑑
2𝜎2"
G,0.9544502617801047,"𝑑
𝑑−𝑛
𝑑𝑠1𝑑𝑠2"
G,0.9549738219895288,"+
𝛼𝑡∗(1 −𝑟)
2𝑑
(𝑑−𝑛) 𝜎2 𝛾𝑛 4"
G,0.9554973821989529,"∫∫
𝑝(𝑠1, 𝑠2)
1"
G,0.956020942408377,"(
𝑑
𝑑−𝑛)2 𝑑𝑠1𝑑𝑠2 
="
G,0.956544502617801,Further manipulation leads to
G,0.9570680628272251,"𝜏2 =
𝜅
𝜅−1"
G,0.9575916230366492,"
𝛾−𝛾2"
G,0.9581151832460733,"4 +
Γ2"
G,0.9586387434554974,𝑄−1(𝑒𝑎)2 (1 −1
G,0.9591623036649215,𝜅)2 −𝜌𝛾2
G,0.9596858638743455,4𝜅+ 𝜌𝛾2
G,0.9602094240837696,8𝜅2 −Γ𝛾(1 −1
G,0.9607329842931938,"𝜅) + (1 −1 𝜅) 1 𝜅Γ𝛾
"
G,0.9612565445026178,"=
𝜅
𝜅−1"
G,0.9617801047120419,"
𝛾−𝛾2"
G,0.962303664921466,"4 +
Γ2"
G,0.96282722513089,𝑄−1(𝑒𝑎)2 (1 −1
G,0.9633507853403142,𝜅)2 −𝜌𝛾2
G,0.9638743455497382,4𝜅+ 𝜌𝛾2
G,0.9643979057591623,8𝜅2 −(1 −1
G,0.9649214659685864,"𝜅)2Γ𝛾
"
G,0.9654450261780104,"=
𝜅
𝜅−1  𝜌"
G,0.9659685863874345,8𝜅2 −𝜌 4𝜅−1 4
G,0.9664921465968587,"
𝛾2 +

1 −(1 −1"
G,0.9670157068062827,"𝜅)2Γ

𝛾+
Γ2"
G,0.9675392670157068,"𝑄−1(𝑒𝑎)2 (1 −1 𝜅)2
"
G,0.9680628272251309,"Recall that Γ = 1 −
1
𝑄−1(𝑒𝑎)2+1 and
Γ
𝑄−1(𝑒𝑎)2 = 1 −Γ. Hence, we obtain:"
G,0.9685863874345549,"𝜏2 =
𝜅
𝜅−1  𝜌"
G,0.9691099476439791,8𝜅2 −𝜌 4𝜅−1 4
G,0.9696335078534032,"
𝛾2 +

1 −(1 −1"
G,0.9701570680628272,"𝜅)2Γ

𝛾+ Γ(1 −Γ)(1 −1"
G,0.9706806282722513,"𝜅)2

(43)"
G,0.9712041884816754,"On the other hand, 𝛾=
1−(1−1"
G,0.9717277486910995,"𝜅)Γ
1
2 + 𝜌"
G,0.9722513089005236,"4𝜅
and Γ = 1 −
1
𝑄−1(𝑒𝑎)2+1 combining these fact with 43 yields 𝑒𝑝= 𝑄"
G,0.9727748691099476,"
𝑄−1(𝑒𝑎)2(2𝜅+ 𝜌−2) + 𝜌
√︁"
G,0.9732984293193717,"𝜅(𝜅−1)
√︃"
G,0.9738219895287958,(2𝜅+ 𝜌)((4𝜅−2)𝑄−1(𝑒𝑎)4 + 𝑄−1(𝑒𝑎)2  2𝜅3 + 𝜅2𝜌−2𝜅(𝜌−1) + 𝜌 + 2𝜅2) ! □
G,0.9743455497382199,NeurIPS Paper Checklist
CLAIMS,0.974869109947644,1. Claims
CLAIMS,0.9753926701570681,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The main results are stated in Sections 3, 4 along with numerical evidence in
Section 5.
Guidelines:"
CLAIMS,0.9759162303664921,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9764397905759162,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of our approach and assumptions are explained in Section 6.
Guidelines:"
CLAIMS,0.9769633507853404,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9774869109947644,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.9780104712041885,"Justification: The theoretical results, assumptions, and definitions are provided in sections
2, 3, 4. Moreover, the complete proof of arguments in the main body is presented in the
Appendices B, F, H, I."
CLAIMS,0.9785340314136126,Guidelines:
CLAIMS,0.9790575916230366,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9795811518324608,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9801047120418848,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9806282722513089,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.981151832460733,"Justification: All the necessary details are given in Section 5. Moreover, the code is attached
to the supplementary materials."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.981675392670157,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9821989528795811,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9827225130890053,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9832460732984293,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code is attached to the supplementary material.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9837696335078534,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
OPEN ACCESS TO DATA AND CODE,0.9842931937172775,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All the necessary details are given in Section 5.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9848167539267015,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
OPEN ACCESS TO DATA AND CODE,0.9853403141361257,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: As the main focus of the paper is the theoretical contribution, the experiments
are conducted using specific data distributions whose statistics are exactly known. (𝜒2,
Bernoulli, Gaussian)
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9858638743455498,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
OPEN ACCESS TO DATA AND CODE,0.9863874345549738,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
OPEN ACCESS TO DATA AND CODE,0.9869109947643979,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: All the necessary details are given in Section 5.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.987434554973822,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
OPEN ACCESS TO DATA AND CODE,0.987958115183246,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: The research conducted in this paper conforms with Neurips Code of Ethics.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9884816753926702,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
OPEN ACCESS TO DATA AND CODE,0.9890052356020942,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Since this paper is concerned with the mathematical foundations of AI.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9895287958115183,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
OPEN ACCESS TO DATA AND CODE,0.9900523560209424,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9905759162303664,11. Safeguards
SAFEGUARDS,0.9910994764397906,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9916230366492147,Answer: [NA]
SAFEGUARDS,0.9921465968586387,Justification: Since this paper is concerned with the mathematical foundations of AI.
SAFEGUARDS,0.9926701570680628,Guidelines:
SAFEGUARDS,0.993193717277487,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.993717277486911,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9942408376963351,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9947643979057592,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9952879581151832,Justification: All the necessary details are given in Section 5.
LICENSES FOR EXISTING ASSETS,0.9958115183246073,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9963350785340314,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9968586387434555,"• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9973821989528796,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Since this paper is concerned with the mathematical foundations of AI.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9979057591623036,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9984293193717277,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Since this paper is concerned with the mathematical foundations of AI.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9989528795811519,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Since this paper is concerned with the mathematical foundations of AI.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9994764397905759,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
