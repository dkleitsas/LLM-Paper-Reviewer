Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016806722689075631,"Since real-world machine systems are running in non-stationary and continually
1"
ABSTRACT,0.0033613445378151263,"changing environments, Continual Test-Time Adaptation (CTTA) task is proposed
2"
ABSTRACT,0.005042016806722689,"to adapt the pre-trained model to continually changing target domains. Recently,
3"
ABSTRACT,0.0067226890756302525,"existing methods mainly focus on model-based adaptation, which aims to leverage
4"
ABSTRACT,0.008403361344537815,"a self-training manner to extract the target domain knowledge. However, pseudo
5"
ABSTRACT,0.010084033613445379,"labels can be noisy and the updated model parameters are uncertain under dynamic
6"
ABSTRACT,0.011764705882352941,"data distributions, leading to error accumulation and catastrophic forgetting in
7"
ABSTRACT,0.013445378151260505,"the continual adaptation process. To tackle these challenges and maintain the
8"
ABSTRACT,0.015126050420168067,"model plasticity, we tactfully design a Visual Domain Adapter (ViDA) for CTTA,
9"
ABSTRACT,0.01680672268907563,"explicitly handling both domain-specific and domain-agnostic knowledge. Specifi-
10"
ABSTRACT,0.018487394957983194,"cally, we first comprehensively explore the different domain representations of the
11"
ABSTRACT,0.020168067226890758,"adapters with trainable high and low-rank embedding space. Then we inject ViDAs
12"
ABSTRACT,0.021848739495798318,"into the pre-trained model, which leverages high-rank and low-rank prototypes to
13"
ABSTRACT,0.023529411764705882,"adapt the current domain distribution and maintain the continual domain-shared
14"
ABSTRACT,0.025210084033613446,"knowledge, respectively. To adapt to the various distribution shifts of each sample
15"
ABSTRACT,0.02689075630252101,"in target domains, we further propose a Homeostatic Knowledge Allotment (HKA)
16"
ABSTRACT,0.02857142857142857,"strategy, which adaptively merges knowledge from each ViDA with different rank
17"
ABSTRACT,0.030252100840336135,"prototypes. Extensive experiments conducted on four widely-used benchmarks
18"
ABSTRACT,0.031932773109243695,"demonstrate that our proposed method achieves state-of-the-art performance in
19"
ABSTRACT,0.03361344537815126,"both classification and segmentation CTTA tasks. In addition, our method can be
20"
ABSTRACT,0.03529411764705882,"regarded as a novel transfer paradigm and showcases promising results in zero-shot
21"
ABSTRACT,0.03697478991596639,"adaptation of foundation models to continual downstream tasks and distributions.
22"
INTRODUCTION,0.03865546218487395,"1
Introduction
23"
INTRODUCTION,0.040336134453781515,"Deep Neural Networks (DNN) have achieved remarkable performance in various computer vision
24"
INTRODUCTION,0.04201680672268908,"tasks, such as classification [22, 14], object detection [48, 63], and segmentation [9, 58], when the test
25"
INTRODUCTION,0.043697478991596636,"data distribution is similar to the training data. However, real-world machine perception systems (i.e.,
26"
INTRODUCTION,0.0453781512605042,"autonomous driving [1, 28]) operate in non-stationary and constantly changing environments, which
27"
INTRODUCTION,0.047058823529411764,"contain heterogeneous and dynamic domain distribution shifts. Applying a pre-trained model in
28"
INTRODUCTION,0.04873949579831933,"these real-world tasks [50] can lead to significant degradation in perception ability on target domains,
29"
INTRODUCTION,0.05042016806722689,"especially when the target distribution changes unexpectedly over time. Therefore, developing
30"
INTRODUCTION,0.052100840336134456,"continual domain adaptation (DA) methods that can enhance the generalization capability of DNNs
31"
INTRODUCTION,0.05378151260504202,"and improve the reliability of machine perception systems in dynamic environments.
32"
INTRODUCTION,0.05546218487394958,"A classical source-free DA task, Test-Time Adaptation [39] (TTA), eases the distribution shift between
33"
INTRODUCTION,0.05714285714285714,"a source domain and a fixed target domain. This is typically achieved through the utilization of
34"
INTRODUCTION,0.058823529411764705,"self-training mechanisms [42, 55]. However, when adapting to continually changing target domains,
35"
INTRODUCTION,0.06050420168067227,"pseudo labels are noisy and the updated model parameters become uncertain, leading to error
36"
INTRODUCTION,0.06218487394957983,"(a) Continual Test-Time Adaptation
(b) Feature representations of different branches
Domain 1 (Fog)
Domain 2 (Night)
Domain 3 (Rain)
Domain 4 (Snow)"
INTRODUCTION,0.06386554621848739,"Ours: Low-rank Branch
The Pre-trained Parameter
Ours: High-rank Branch"
INTRODUCTION,0.06554621848739496,"Domain-agnostic
Domain-specific"
INTRODUCTION,0.06722689075630252,Pre-trained
INTRODUCTION,0.06890756302521009,"Parameter
Low-Rank
High-Rank"
INTRODUCTION,0.07058823529411765,"Domain
agnostic"
INTRODUCTION,0.07226890756302522,Domain
INTRODUCTION,0.07394957983193277,specific
INTRODUCTION,0.07563025210084033,"Online
Adapted 
Network"
INTRODUCTION,0.0773109243697479,Continually Changing Target Environments
INTRODUCTION,0.07899159663865546,Domain 1 (Fog)
INTRODUCTION,0.08067226890756303,Domain 2
INTRODUCTION,0.08235294117647059,(Night)
INTRODUCTION,0.08403361344537816,Domain 3
INTRODUCTION,0.08571428571428572,(Rain)
INTRODUCTION,0.08739495798319327,Domain 4
INTRODUCTION,0.08907563025210084,(Snow)
INTRODUCTION,0.0907563025210084,"Figure 1: The problem and motivation of our method. (a) Our goal is to effectively adapt the source
pre-trained model to continually changing target domains. We propose Visual Domain Adapters
with different domain representations to tackle the error accumulation and catastrophic forgetting
challenges during the continual adaptation process. We leverage ViDAs with high-rank and low-
rank prototypes to adapt current domain distribution and maintain the continual domain-agnostic
knowledge, respectively. (b) we conduct a t-SNE [53] analysis for the different adapter distributions
across four target domains (ACDC). The low-rank branch exhibits a consistent distribution across the
target domains, suggesting that it can effectively disregard the impact of dynamic distribution shifts.
The high-rank branch demonstrates noticeable distribution discrepancies between the various target
domains, suggesting that it primarily focuses on extracting domain-specific knowledge."
INTRODUCTION,0.09243697478991597,"accumulation and catastrophic forgetting. To tackle this problem, Continual Test-Time Adaptation
37"
INTRODUCTION,0.09411764705882353,"(CTTA) has been proposed [57], which addresses a sequence of different distribution shifts over time
38"
INTRODUCTION,0.0957983193277311,"rather than a single shift as in TTA. Furthermore, CTTA also encompasses the efficient zero-shot
39"
INTRODUCTION,0.09747899159663866,"adaptation of foundation models to continual downstream tasks or distributions [2, 29].
40"
INTRODUCTION,0.09915966386554621,"Existing CTTA works [57, 7, 16, 59] have primarily employed model-based and prompt-based ap-
41"
INTRODUCTION,0.10084033613445378,"proaches to extract target domain-specific and domain-invariant knowledge simultaneously. However,
42"
INTRODUCTION,0.10252100840336134,"for model-based methods [57, 7], the noisy pseudo labels are still unreliable and play a limited role in
43"
INTRODUCTION,0.10420168067226891,"avoiding error accumulation, particularly in scenarios with significant distribution gaps. Meanwhile,
44"
INTRODUCTION,0.10588235294117647,"prompt-based methods [16, 59] face difficulties in leveraging soft prompts with limited trainable
45"
INTRODUCTION,0.10756302521008404,"parameters to learn long-term domain-shared knowledge and prevent catastrophic forgetting.
46"
INTRODUCTION,0.1092436974789916,"To tackle these limitations and maintain the model plasticity, we tactfully design a homeostatic
47"
INTRODUCTION,0.11092436974789915,"Visual Domain Adapter (ViDA), shown in Fig. 1 (a), which explicitly manages domain-specific
48"
INTRODUCTION,0.11260504201680673,"and domain-agnostic knowledge in the continual adaptation process. Specifically, we first carefully
49"
INTRODUCTION,0.11428571428571428,"explore the different domain representations of ViDAs with trainable high and low-rank embedding
50"
INTRODUCTION,0.11596638655462185,"space. Our observations reveal that ViDA with a low-rank prototype focuses on domain-agnostic
51"
INTRODUCTION,0.11764705882352941,"feature representation in different domains. As shown in Fig. 1 (b), the prototype distribution of the
52"
INTRODUCTION,0.11932773109243698,"adapter neglects the influence of dynamic distribution shifts. Conversely, ViDA with a high-rank
53"
INTRODUCTION,0.12100840336134454,"prototype concentrates more on extracting domain-specific knowledge, as evidenced by the prototype
54"
INTRODUCTION,0.1226890756302521,"distribution in different target domains showing an obvious discrepancy. We provide a detailed
55"
INTRODUCTION,0.12436974789915967,"explanation of the motivations in Section 3.1.
56"
INTRODUCTION,0.12605042016806722,"This observation motivates us to inject ViDAs into the pre-trained model, which leverages high and
57"
INTRODUCTION,0.12773109243697478,"low-dimension prototype to adapt current domain distribution and maintain the continual domain-
58"
INTRODUCTION,0.12941176470588237,"shared knowledge, respectively. According to the various distribution shift of each sample, we further
59"
INTRODUCTION,0.13109243697478992,"propose a Homeostatic Knowledge Allotment (HKA) strategy to dynamically fuse the knowledge
60"
INTRODUCTION,0.13277310924369748,"from each ViDA with different dimension prototypes. In Fig. 1 (b), HKA adaptively regularizes the
61"
INTRODUCTION,0.13445378151260504,"balance of different feature representations, including original model, domain-specific, and domain-
62"
INTRODUCTION,0.1361344537815126,"agnostic features. During inference, the different domain-represented ViDAs can be projected into
63"
INTRODUCTION,0.13781512605042018,"the pre-trained model by re-parameterization [13], which ensures no extra parameter increase and
64"
INTRODUCTION,0.13949579831932774,"maintain the model plasticity. In addition, through the proposed homeostatic ViDAs, we empower
65"
INTRODUCTION,0.1411764705882353,"the model with domain generalization ability, which achieves a significant improvement (+7.6%) on
66"
INTRODUCTION,0.14285714285714285,"the five unseen target domains of ImageNet-C. In summary, our contributions are as follows:
67"
INTRODUCTION,0.14453781512605043,"• We carefully study the different domain representations of the adapters with high and low-
68"
INTRODUCTION,0.146218487394958,"rank prototypes. And we tactfully design a Visual Domain Adapter (ViDA) for CTTA,
69"
INTRODUCTION,0.14789915966386555,"explicitly managing domain-specific and domain-shared knowledge to tackle the error
70"
INTRODUCTION,0.1495798319327731,"accumulation and catastrophic forgetting problem, respectively.
71"
INTRODUCTION,0.15126050420168066,"• According to the various distribution shift of each sample in the target domains, we further
72"
INTRODUCTION,0.15294117647058825,"propose a Homeostatic Knowledge Allotment (HKA) strategy to dynamically fuse the
73"
INTRODUCTION,0.1546218487394958,"knowledge from each ViDA with different rank prototypes.
74"
INTRODUCTION,0.15630252100840336,"• Our proposed approach outperforms most state-of-the-art methods according to the experi-
75"
INTRODUCTION,0.15798319327731092,"ments on four benchmark datasets, covering classification and segmentation tasks.
76"
INTRODUCTION,0.15966386554621848,"• Our CTTA method provides a novel transfer paradigm and achieves a promising result in
77"
INTRODUCTION,0.16134453781512606,"zero-shot adapting of foundation models to continual downstream distributions. Meanwhile,
78"
INTRODUCTION,0.16302521008403362,"we empower the source model with domain generalization ability through the proposed
79"
INTRODUCTION,0.16470588235294117,"homeostatic ViDAs, achieving a significant improvement on the unseen target domains.
80"
RELATED WORK,0.16638655462184873,"2
Related work
81"
CONTINUAL TEST-TIME ADAPTATION,0.16806722689075632,"2.1
Continual Test-Time Adaptation
82"
CONTINUAL TEST-TIME ADAPTATION,0.16974789915966387,"Test-time adaptation (TTA), also referred to as source-free domain adaptation [6, 34, 40, 60], aims to
83"
CONTINUAL TEST-TIME ADAPTATION,0.17142857142857143,"adapt a source model to an unknown target domain distribution without relying on any source domain
84"
CONTINUAL TEST-TIME ADAPTATION,0.173109243697479,"data. Recent research has explored self-training and entropy regularization techniques to fine-tune the
85"
CONTINUAL TEST-TIME ADAPTATION,0.17478991596638654,"source model [35, 56, 40, 8]. Tent [56] updates the training parameters in batch normalization layers
86"
CONTINUAL TEST-TIME ADAPTATION,0.17647058823529413,"by minimizing entropy. Recently, there has been a surge of interest in performing Transformer-based
87"
CONTINUAL TEST-TIME ADAPTATION,0.1781512605042017,"TTA works [57, 20, 20]. Continual Test-Time Adaptation (CTTA) refers to a scenario where the
88"
CONTINUAL TEST-TIME ADAPTATION,0.17983193277310924,"target domain is not static, presenting additional challenges for traditional TTA methods. The first
89"
CONTINUAL TEST-TIME ADAPTATION,0.1815126050420168,"approach to address this challenging task is introduced in [57], which combines bi-average pseudo
90"
CONTINUAL TEST-TIME ADAPTATION,0.18319327731092436,"labels and stochastic weight reset. While [57, 7] tackles the problem in both classification and
91"
CONTINUAL TEST-TIME ADAPTATION,0.18487394957983194,"segmentation tasks at the model level, [16] introduces the use of visual domain prompts to address
92"
CONTINUAL TEST-TIME ADAPTATION,0.1865546218487395,"the issue at the input level specifically for the classification task. In this paper, we simultaneously
93"
CONTINUAL TEST-TIME ADAPTATION,0.18823529411764706,"focus on both classification tasks and dense prediction tasks.
94"
PARAMETER-EFFICIENT FINE-TUNING,0.1899159663865546,"2.2
Parameter-Efficient Fine-Tuning
95"
PARAMETER-EFFICIENT FINE-TUNING,0.1915966386554622,"Recently, Parameter-Efficient Fine-Tuning (PEFT) has gained significant traction within the field
96"
PARAMETER-EFFICIENT FINE-TUNING,0.19327731092436976,"of natural language processing (NLP) [30, 26, 25, 61, 37, 27, 19, 23, 54, 45]. Adapter-based
97"
PARAMETER-EFFICIENT FINE-TUNING,0.1949579831932773,"models, a form of PEFT, have gained popularity in NLP. They employ bottleneck architecture adapter
98"
PARAMETER-EFFICIENT FINE-TUNING,0.19663865546218487,"modules inserted between layers in pre-trained models. During fine-tuning, only these modules are
99"
PARAMETER-EFFICIENT FINE-TUNING,0.19831932773109243,"updated. Adapter-based models demonstrate dominant performance over other methods in certain
100"
PARAMETER-EFFICIENT FINE-TUNING,0.2,"tasks, sometimes surpassing standard fine-tuning [12]. Inspired by NLP, adapters in visual tasks have
101"
PARAMETER-EFFICIENT FINE-TUNING,0.20168067226890757,"also received widespread attention. In the initial phases of adapter development, residual adapter
102"
PARAMETER-EFFICIENT FINE-TUNING,0.20336134453781513,"modules [46, 47] are proposed to aid in the effective adaptation of convolutional neural networks
103"
PARAMETER-EFFICIENT FINE-TUNING,0.20504201680672268,"across multiple downstream tasks. AdaptFormer [10] enhances the ViT [14] model by replacing
104"
PARAMETER-EFFICIENT FINE-TUNING,0.20672268907563024,"the original multi-layer perceptron (MLP) block with AdaptMLP. AdaptMLP introduces a trainable
105"
PARAMETER-EFFICIENT FINE-TUNING,0.20840336134453782,"down-to-up bottleneck module in a parallel manner, effectively mitigating catastrophic interference
106"
PARAMETER-EFFICIENT FINE-TUNING,0.21008403361344538,"between tasks. VL-Adapter [51] improves the efficiency and performance of adapters by sharing
107"
PARAMETER-EFFICIENT FINE-TUNING,0.21176470588235294,"low-dimensional layers weights to attain knowledge across tasks. Existing methods, as mentioned,
108"
PARAMETER-EFFICIENT FINE-TUNING,0.2134453781512605,"have not addressed the challenges of long-term preservation of domain-agnostic knowledge and
109"
PARAMETER-EFFICIENT FINE-TUNING,0.21512605042016808,"timely exploration of domain-specific knowledge amidst continuous unknown domain variations.
110"
PARAMETER-EFFICIENT FINE-TUNING,0.21680672268907564,"Consequently, there is an urgent demand for an adapter with different domain representations that
111"
PARAMETER-EFFICIENT FINE-TUNING,0.2184873949579832,"can simultaneously tackle the challenges of error accumulation and catastrophic forgetting.
112"
METHOD,0.22016806722689075,"3
Method
113"
METHOD,0.2218487394957983,"In Continual Test-Time Adaptation (CTTA), we pre-train the model qθ(y|x) on the source domain
114"
METHOD,0.2235294117647059,"DS = (YS, XS) and adapt it on multiple target domains DTi = {(XTi)}n
i=1, where n represents
115"
METHOD,0.22521008403361345,"the scale of the continual target datasets. The entire process can not access any source domain
116"
METHOD,0.226890756302521,"data and can only access target domain data once. The distributions of the target domains (i.e.,
117"
METHOD,0.22857142857142856,"DT1, DT2, ..., DTn) are constantly changing over time. Our goal is to adapt the pre-trained model to
118"
METHOD,0.23025210084033612,"target domains and maintain the perception ability of the model on the seen domain distribution.
119"
METHOD,0.2319327731092437,"Our approach proposes a novel Visual Domain Adapter (ViDA) that contains both high and low-
120"
METHOD,0.23361344537815126,"dimensional prototypes. This design allows us to explicitly manage domain-specific and domain-
121"
METHOD,0.23529411764705882,Augment
METHOD,0.23697478991596638,"Target Domain Input
Pre-trained Source Model"
METHOD,0.23865546218487396,Consistency Loss
METHOD,0.24033613445378152,Back-prop
METHOD,0.24201680672268908,"Prediction
Loss function"
METHOD,0.24369747899159663,Student model
METHOD,0.2453781512605042,Homeostatic Knowledge Allotment &
METHOD,0.24705882352941178,"1
2
3
4
5
6
7
8
9 10"
METHOD,0.24873949579831933,Uncertainty Value
METHOD,0.2504201680672269,"1
2
3
4
5
6
7
8
9 10"
METHOD,0.25210084033613445,Data Flow
METHOD,0.253781512605042,Teacher model EMA
METHOD,0.25546218487394956,"ViDAs
 Original 
Linear or Conv
Low-Rank
High-Rank"
METHOD,0.2571428571428571,Feature：
METHOD,0.25882352941176473,"agnostic
specific
�� 
f�×  ��
�ℎ×  �ℎ"
METHOD,0.2605042016806723,Distribution shift
METHOD,0.26218487394957984,"Reflect
Guide �� & �� , Eq.(4)"
METHOD,0.2638655462184874,"(a)
(b)"
METHOD,0.26554621848739496,Eq.(3)
METHOD,0.2672268907563025,Eq.(2) up
METHOD,0.2689075630252101,"down
up down"
METHOD,0.27058823529411763,"Figure 2: The framework of Visual Domain Adapter (ViDA). (a) We inject different domain-
represented ViDAs into either linear or Conv layers of the pre-trained source model. To update
the ViDAs, we construct a teacher-student framework and use a consistency loss (Eq. 5) as the
optimization objective. The student model processes the original image, while the teacher model
processes an augmented version of the same image. In addition to generating predictions, the teacher
model calculates an uncertainty value (Eq. 3), reflecting the distribution shift of each sample in the
target domain. (b) We illustrate the details of the Homeostatic Knowledge Allotment (HKA) strategy,
which aims to dynamically fuse the knowledge from each ViDA with different rank prototypes."
METHOD,0.2722689075630252,"agnostic knowledge, addressing the challenges of error accumulation and catastrophic forgetting in
122"
METHOD,0.2739495798319328,"CTTA. To effectively adapt to the diverse distribution shifts, a Homeostatic Knowledge Allotment
123"
METHOD,0.27563025210084036,"(HKA) strategy is introduced to dynamically fuse the knowledge from different ViDA with different
124"
METHOD,0.2773109243697479,"domain representations. The overall framework is shown in Fig. 2.
125"
MOTIVATION,0.27899159663865547,"3.1
Motivation
126"
MOTIVATION,0.280672268907563,"The Continual Test-Time Adaptation (CTTA) faces significant challenges, primarily due to error
127"
MOTIVATION,0.2823529411764706,"accumulation and catastrophic forgetting [57, 16]. Meanwhile, adapters with different dimension
128"
MOTIVATION,0.28403361344537814,"prototypes demonstrate remarkable effectiveness in addressing these challenges. This encourages us
129"
MOTIVATION,0.2857142857142857,"to take a step further and investigate the principles underlying the use of domain adapters in CTTA.
130"
MOTIVATION,0.28739495798319326,"Adapter with low rank prototype. Our hypothesis regarding the effectiveness of adapters in
131"
MOTIVATION,0.28907563025210087,"mitigating catastrophic forgetting is that their low-rank prototype representation plays a crucial role.
132"
MOTIVATION,0.2907563025210084,"To explore this further, we conduct a t-SNE study [53] on the third transformer block to analyze
133"
MOTIVATION,0.292436974789916,"the feature distributions across four target domains (ACDC). The results are depicted in Fig. 1 (b).
134"
MOTIVATION,0.29411764705882354,"Our analysis reveals that the low-rank adapter exhibits a relatively consistent distribution across the
135"
MOTIVATION,0.2957983193277311,"different target domains, suggesting that its low-rank prototype can effectively disregard the impact
136"
MOTIVATION,0.29747899159663865,"of dynamic distribution shifts and prioritize the extraction of domain-invariant knowledge.
137"
MOTIVATION,0.2991596638655462,"We adopt the domain distance definition proposed by Ben-David [4, 3] and build upon previous
138"
MOTIVATION,0.30084033613445377,"domain transfer research [18] by employing the H-divergence metric to further evaluate the domain
139"
MOTIVATION,0.3025210084033613,"representations of adapters across different target domains. H-divergence between DS and DTi can
140"
MOTIVATION,0.3042016806722689,"be calculated as dH(DS, DTi) = 2 supD∼H | Prx∼DS[D(x) = 1] −Prx∼DTi[D(x) = 1]|, where H
141"
MOTIVATION,0.3058823529411765,"denotes hypothetical space and D denotes discriminator. Similar to [18], calculating the H-divergence
142"
MOTIVATION,0.30756302521008405,"directly is challenging. We adopt the Jensen-Shannon (JS) divergence between two adjacent
143"
MOTIVATION,0.3092436974789916,"domains as an approximation. To investigate the effectiveness of adapters in adapting to continual
144"
MOTIVATION,0.31092436974789917,"target domains, we compare the JS values obtained by using the source model alone, injecting
145"
MOTIVATION,0.3126050420168067,"low-rank adapter, and combining low-high adapters, as illustrated in Fig. 3 (a). Our results indicate
146"
MOTIVATION,0.3142857142857143,"that the feature representation generated by the low-rank adapter exhibits lower divergence compared
147"
MOTIVATION,0.31596638655462184,"to those of the original source model and closely resembles the values of low-high combination.
148"
MOTIVATION,0.3176470588235294,"To provide clearer evidence for our assumption, we have developed an evaluation approach that
149"
MOTIVATION,0.31932773109243695,"directly reflects the extent of domain catastrophic forgetting. Shown in Table 1, after one round of
150"
MOTIVATION,0.32100840336134456,"CTTA on all target domains (ImageNet-C), we utilize the model and adapter from the last target
151"
MOTIVATION,0.3226890756302521,"domain to directly test on previously seen target domains. As expected, the performance degradation
152"
MOTIVATION,0.3243697478991597,"is observed in only 2 out of 15 corruption types, and there is an overall improvement of 1.0% in
153"
MOTIVATION,0.32605042016806723,"the average classification error. These findings further support our assumptions and indicate that
154"
MOTIVATION,0.3277310924369748,"low-rank adapters are more effective in preserving continual domain-shared knowledge.
155"
MOTIVATION,0.32941176470588235,"c6
c7
→
c12
c13
→
c3
c4
→
c9
c10
→
Domain Shifts"
MOTIVATION,0.3310924369747899,(a) Inter-Domain Divergence
MOTIVATION,0.33277310924369746,"c2
c4
c6
c8
c10
c12
c14"
MOTIVATION,0.334453781512605,(b) Intra-class Divergence
MOTIVATION,0.33613445378151263,Domains
MOTIVATION,0.3378151260504202,"Figure 3: c1 to c15 represent the 15 corruption domains in CIFAR10C listed in sequential order. (a)
Low-rank adapter based model effectively mitigates inter-domain divergence than the source model
across all 14 domain shifts. (b) High-rank adapter based model significantly enhances the intra-class
feature aggregation, yielding results that closely approximate those achieved by our ViDA method."
MOTIVATION,0.33949579831932775,"Adapter with high rank prototype. Regarding the domain representation of the adapter with a
156"
MOTIVATION,0.3411764705882353,"high-rank prototype, we propose that it is better suited to address error accumulation in the continual
157"
MOTIVATION,0.34285714285714286,"adaptation process. We verify this by visualizing the prototype distributions between different
158"
MOTIVATION,0.3445378151260504,"domains, as shown in Fig. 1 (b), and observe that there is a clear discrepancy between domains. And
159"
MOTIVATION,0.346218487394958,"the distribution achieves a better aggregation in a single domain. This suggests that high-rank adapters
160"
MOTIVATION,0.34789915966386553,"primarily focus on extracting domain-specific knowledge in continual target domains. Inspired by
161"
MOTIVATION,0.3495798319327731,"intra-cluster dissimilarity proposed by k-means [41], we use normalized intra-class divergence to
162"
MOTIVATION,0.35126050420168065,"further verify the domain representations of high-rank adapters in CIFAR10C. As illustrated in Fig. 3
163"
MOTIVATION,0.35294117647058826,"(b), the high-rank adapter is found to drive down divergence within almost all domains, indicating
164"
MOTIVATION,0.3546218487394958,"that it can better adapt to current domain distribution and extract domain-specific knowledge in
165"
MOTIVATION,0.3563025210084034,"continual target domains. To straightforwardly measure it, we quantitatively evaluate its performance.
166"
MOTIVATION,0.35798319327731093,"As shown in Table 6 Ex2, the classification error rate exhibits a sustained reduction (-4.6%) in the
167"
MOTIVATION,0.3596638655462185,"dynamic target domains with the use of a high-rank adapter. This finding supports our hypothesis
168"
MOTIVATION,0.36134453781512604,"that high-rank adapters can extract more reliable domain-specific knowledge.
169"
VISUAL DOMAIN ADAPTER,0.3630252100840336,"3.2
Visual Domain Adapter
170"
VISUAL DOMAIN ADAPTER,0.36470588235294116,"The above observation motivates us to introduce high-rank and low-rank Visual Domain Adapters
171"
VISUAL DOMAIN ADAPTER,0.3663865546218487,"(ViDAs) into the source pre-trained model, aiming to simultaneously adapt current domain distribution
172"
VISUAL DOMAIN ADAPTER,0.3680672268907563,"and maintain the continual domain-shared knowledge in CTTA.
173"
VISUAL DOMAIN ADAPTER,0.3697478991596639,"The architecture. The design principle of injecting ViDAs into the pre-trained model is simple
174"
VISUAL DOMAIN ADAPTER,0.37142857142857144,"yet effective, which is illustrated in Figure .2 (b). As we can see there are three sub-branches, the
175"
VISUAL DOMAIN ADAPTER,0.373109243697479,"linear (or Conv) layer in the middle branch is identical to the original network, while the right branch
176"
VISUAL DOMAIN ADAPTER,0.37478991596638656,"and left branch are bottleneck structures and separately indicate the high-rank ViDA and low-rank
177"
VISUAL DOMAIN ADAPTER,0.3764705882352941,"ViDA. Specifically, the right branch (high-rank) contains an up-projection layer with parameters
178"
VISUAL DOMAIN ADAPTER,0.37815126050420167,"W h
up ∈Rd×dh, a down-projection layer with parameters W h
down ∈Rdh×d, where dh (i.e., dh = 128)
179"
VISUAL DOMAIN ADAPTER,0.3798319327731092,"is the middle dimension of high-rank prototype and satisfies dh ≥d. There is not any non-linear
180"
VISUAL DOMAIN ADAPTER,0.3815126050420168,"layer in the ViDA. And we utilize the linear layer as the projection layer when the original model
181"
VISUAL DOMAIN ADAPTER,0.3831932773109244,"is transformer architecture and adopt 1 × 1 Conv as the projection layer when the original model is
182"
VISUAL DOMAIN ADAPTER,0.38487394957983195,"a convolution network. In contrast, the left branch (low-rank) first injects a down-projection layer
183"
VISUAL DOMAIN ADAPTER,0.3865546218487395,"with parameters W l
down ∈Rd×dl, then place an up-projection layer with parameters W l
up ∈Rdl×d,
184"
VISUAL DOMAIN ADAPTER,0.38823529411764707,"where dl (i.e., dl = 1) stand for the middle dimension of the low-rank prototype (dl ≪d). For a input
185"
VISUAL DOMAIN ADAPTER,0.3899159663865546,"feature f, the produced features of high-rank ViDA (fh) and low-rank ViDA (fl) are formulated as:
186"
VISUAL DOMAIN ADAPTER,0.3915966386554622,"fh = W h
down · (W h
up · f);
fl = W l
up · (W l
down · f)
(1)
The two-branch bottleneck is connected to the output feature of the original network (fo) through the
187"
VISUAL DOMAIN ADAPTER,0.39327731092436974,"residual connection via scale factors (λh and λl). The fusion knowledge (ff) can be described as:
188"
VISUAL DOMAIN ADAPTER,0.3949579831932773,"ff = fo + λh × fh + λl × fl
(2)
The domain knowledge scale factors (λh and λl) are adaptively obtained through the homeostatic
189"
VISUAL DOMAIN ADAPTER,0.39663865546218485,"knowledge allotment strategy, which is shown in Section 3.3.
190"
VISUAL DOMAIN ADAPTER,0.3983193277310924,"Continual adapting. During the continual adaptation process, we freeze the parameters of the
191"
VISUAL DOMAIN ADAPTER,0.4,"original model (middle branch) and update the high-rank ViDA and low-rank ViDA on the dynamic
192"
VISUAL DOMAIN ADAPTER,0.4016806722689076,"target domains with unsupervised loss. During inference, the different domain-represented ViDAs
193"
VISUAL DOMAIN ADAPTER,0.40336134453781514,"(linear relation) can be projected into the pre-trained model by re-parameterization [13], which
194"
VISUAL DOMAIN ADAPTER,0.4050420168067227,"ensures no extra parameter increase and maintain the plasticity of the original model.
195"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.40672268907563025,"3.3
Homeostatic Knowledge Allotment
196"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.4084033613445378,"Method motivation. In CTTA, the target domain data can only be accessed once and show different
197"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.41008403361344536,"distribution shifts, which makes the efficiency of domain transfer crucial. Moreover, to tackle error
198"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.4117647058823529,"accumulation and catastrophic forgetting effectively, it becomes necessary to extract different domain
199"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.4134453781512605,"knowledge and handle them separately. This requires regularization of the knowledge fusion weight
200"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.4151260504201681,"to ensure efficient capture of relevant domain-specific knowledge without sacrificing the retention of
201"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.41680672268907565,"long-term domain-shared knowledge. HKA design. As depicted in Figure .2 (b), we draw inspiration
202"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.4184873949579832,"from [44, 49, 17] and introduce an uncertainty value to quantify the degree of distribution shift for
203"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.42016806722689076,"each sample. While the confidence score is a common measure to assess prediction reliability, it tends
204"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.4218487394957983,"to fluctuate irregularly and becomes unreliable in scenarios characterized by distribution shifts. To
205"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.4235294117647059,"address this limitation, we employ the MC Dropout technique [15] on linear layers, enabling multiple
206"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.42521008403361343,"forward propagations to obtain m sets of probabilities for each sample. Subsequently, we calculate
207"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.426890756302521,"the uncertainty value U(x) for a given input x, which are formulated as:
208"
HOMEOSTATIC KNOWLEDGE ALLOTMENT,0.42857142857142855,U(x) =
M,0.43025210084033616,"1
m m
X"
M,0.4319327731092437,"i=1
∥pi(y|x) −µ∥2
! 1 2
(3)"
M,0.4336134453781513,"Where pi(y|x) is the predicted probability of the input x in the ith forward propagation and µ is the
209"
M,0.43529411764705883,"average value of m times prediction. To dynamically adjust the scale factors (λh and λl) based on
210"
M,0.4369747899159664,"the uncertainty score, the formulation is as follows:
211"
M,0.43865546218487395,"( λh = 1 + U(x)
λl = 1 −U(x),
U(x) ≥Θ
λh = 1 −U(x)
λl = 1 + U(x),
U(x) < Θ
(4)"
M,0.4403361344537815,"The threshold value of uncertainty is denoted as Θ, where Θ = 0.2. To realize the homeostasis of
212"
M,0.44201680672268906,"different domain knowledge, when facing the sample with a large uncertainty value, we adaptively
213"
M,0.4436974789915966,"increase the fusion weight of domain-specific knowledge (λh). Conversely, if the input has a low
214"
M,0.44537815126050423,"uncertainty value, the fusion weight of domain-agnostic knowledge (λl) will be increased. By
215"
M,0.4470588235294118,"employing the HKA strategy, our approach ensures that the adaptation process effectively captures
216"
M,0.44873949579831934,"relevant domain-specific knowledge while retaining long-term domain-shared knowledge.
217"
OPTIMIZATION OBJECTIVE,0.4504201680672269,"3.4
Optimization Objective
218"
OPTIMIZATION OBJECTIVE,0.45210084033613446,"Following previous CTTA work [57, 16], we leverage the teacher model T to generate the pseudo
219"
OPTIMIZATION OBJECTIVE,0.453781512605042,"labels ey for updating ViDAs. And we adopt consistency loss Lce as the optimization objective.
220"
OPTIMIZATION OBJECTIVE,0.45546218487394957,"Lce(x) = −1 C C
X"
OPTIMIZATION OBJECTIVE,0.45714285714285713,"c
ey(c) log ˆy(c)
(5)"
OPTIMIZATION OBJECTIVE,0.4588235294117647,"Where ˆy is the output of our student model S , C means the number of categories. Same as previous
221"
OPTIMIZATION OBJECTIVE,0.46050420168067224,"works[57, 16], we load the source pre-trained parameters to initialize the weight of both models and
222"
OPTIMIZATION OBJECTIVE,0.46218487394957986,"adopt the exponential moving average (EMA) to update the teacher model with ViDAs.
223"
OPTIMIZATION OBJECTIVE,0.4638655462184874,"T t = αT t−1 + (1 −α)St
(6)"
OPTIMIZATION OBJECTIVE,0.46554621848739497,"Where t is the time step. And we set α = 0.999 [52], which is the updating weight of EMA.
224"
EXPERIMENT,0.4672268907563025,"4
Experiment
225"
EXPERIMENT,0.4689075630252101,"In Section 4.2 and 4.3, we compare our method with other SOTA methods on classification and
226"
EXPERIMENT,0.47058823529411764,"segmentation of CTTA. In Section 4.4, we employ the foundation model [32, 43] as the backbone
227"
EXPERIMENT,0.4722689075630252,"and evaluate the efficacy of our method. In Section 4.5, we further evaluate the domain generalization
228"
EXPERIMENT,0.47394957983193275,"ability of the proposed method. Comprehensive ablation studies are conducted in Section 4.6. More
229"
EXPERIMENT,0.4756302521008403,"quantitative comparisons and qualitative analyses are shown in the supplementary materials.
230"
TASK SETTINGS AND DATASETS,0.4773109243697479,"4.1
Task settings and Datasets
231"
TASK SETTINGS AND DATASETS,0.4789915966386555,"Dataset. We evaluate our method on three classification CTTA benchmarks, including CIFAR10-
232"
TASK SETTINGS AND DATASETS,0.48067226890756304,"to-CIFAR10C(standard), CIFAR100-to-CIFAR100C [33] and ImageNet-to-ImageNet-C [24]. For
233"
TASK SETTINGS AND DATASETS,0.4823529411764706,"segmentation CTTA [57, 59], we evaluate our method on Cityscapes-to-ACDC, where the Cityscapes
234"
TASK SETTINGS AND DATASETS,0.48403361344537815,"dataset [11] serves as the source domain, and the ACDC dataset [50] represents the target domains.
235"
TASK SETTINGS AND DATASETS,0.4857142857142857,"Baselines. We compare the proposed method against two types of CTTA approaches, including
236"
TASK SETTINGS AND DATASETS,0.48739495798319327,"(1)Modal-based: source model [14, 58], Pseudo-label [36], Tent-continual [56], CoTTA [57], and,
237"
TASK SETTINGS AND DATASETS,0.4890756302521008,"SATA [7]. (2) Prompt-based: visual domain prompt [16].
238"
TASK SETTINGS AND DATASETS,0.4907563025210084,"CTTA Task setting. Following [57, 16], in classification CTTA tasks, we sequentially adapt the
239"
TASK SETTINGS AND DATASETS,0.492436974789916,"pre-trained source model to the fifteen target domains with the largest corruption severity (level 5).
240"
TASK SETTINGS AND DATASETS,0.49411764705882355,"The online prediction results were evaluated immediately after encountering the input data. Regarding
241"
TASK SETTINGS AND DATASETS,0.4957983193277311,"segmentation CTTA [57, 59], the source model [58] is an off-the-shelf pre-trained on the Cityscapes
242"
TASK SETTINGS AND DATASETS,0.49747899159663866,"dataset [11]. As for the continual target domains, we utilize the ACDC dataset [50], which consists
243"
TASK SETTINGS AND DATASETS,0.4991596638655462,"of images collected in four unseen visual conditions: Fog, Night, Rain, and Snow. To simulate
244"
TASK SETTINGS AND DATASETS,0.5008403361344538,"continual environmental changes in real-life scenarios, we cyclically repeat the same sequence of
245"
TASK SETTINGS AND DATASETS,0.5025210084033613,"target domains (Fog→Night→Rain→Snow) multiple times.
246"
TASK SETTINGS AND DATASETS,0.5042016806722689,"Implementation Details. In our CTTA experiments, we follow the implementation details specified
247"
TASK SETTINGS AND DATASETS,0.5058823529411764,"in previous works [57, 59] to ensure consistency and comparability. we adopt ViT-base [14] and
248"
TASK SETTINGS AND DATASETS,0.507563025210084,"ResNet [22] as the backbone in classification CTTA. In the case of ViT-base, we resize the input
249"
TASK SETTINGS AND DATASETS,0.5092436974789916,"images to 224x224, while maintaining the original image resolution for other backbones. For
250"
TASK SETTINGS AND DATASETS,0.5109243697478991,"segmentation CTTA, we adopt the pre-trained Segformer-B5 model [58] as the source model. We
251"
TASK SETTINGS AND DATASETS,0.5126050420168067,"down-sample the input size from 1920x1080 to 960x540 for target domain data [57]. The optimizer is
252"
TASK SETTINGS AND DATASETS,0.5142857142857142,"performed using Adam [31] with (β1, β2) = (0.9, 0.999). We set the learning rates to specific values
253"
TASK SETTINGS AND DATASETS,0.5159663865546219,"for each backbone, such as 1e-5 for ViT and 3e-4 for Segformer. To initialize our visual domain
254"
TASK SETTINGS AND DATASETS,0.5176470588235295,"adapters, we train the model with adapters for one epoch on the source domain. We apply a range of
255"
TASK SETTINGS AND DATASETS,0.519327731092437,"image resolution scale factors [0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0] for the augmentation method and
256"
TASK SETTINGS AND DATASETS,0.5210084033613446,"construct the teacher model inputs [57]. All experiments are conducted on NVIDIA A100 GPUs.
257"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5226890756302521,"4.2
The Effectiveness on Classification CTTA
258"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5243697478991597,"Table 1: Classification error rate(%) for ImageNet-to-ImageNet-C online CTTA task. Gain(%)
represents the percentage of improvement in model accuracy compared with the source method."
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5260504201680672,"Backbone
Method
REF"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5277310924369748,Gaussian shot
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5294117647058824,impulse
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5310924369747899,defocus glass
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5327731092436975,motion zoom snow frost fog
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.534453781512605,brightness
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5361344537815126,contrast
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5378151260504201,elastic_trans
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5394957983193277,pixelate jpeg
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5411764705882353,Mean↓Gain
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5428571428571428,ResNet50
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5445378151260504,"Source [21] CVPR2016 97.8 97.1 98.2 81.7 89.8 85.2 78 83.5 77.1 75.9 41.3 94.5 82.5
79.3
68.6
82
0.0
CoTTA [57] CVPR2022 52.9 51.6 51.4 68.3 78.1 57.1 62.0 48.2 52.7 55.3 25.9 90.0 56.4
36.4
35.2
62.7
+19.3
VDP [16]
AAAI2023
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
51.5
+30.5
SATA [7]
2023.4.20
74.1 72.9 71.6 75.7 74.1 64.2 55.5 55.6 62.9 46.6 36.1 69.9 50.6
44.3
48.5
60.1
+21.9"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5462184873949579,ViT-base
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5478991596638656,"Source
ICLR2021 53.0 51.8 52.1 68.5 78.8 58.5 63.3 49.9 54.2 57.7 26.4 91.4 57.5
38.0
36.2
55.8
0.0
Pseudo [36] ICML2013 45.2 40.4 41.6 51.3 53.9 45.6 47.7 40.4 45.7 93.8 98.5 99.9 99.9
98.9
99.6
61.2
-5.4
Tent [56]
ICLR2021 52.2 48.9 49.2 65.8 73 54.5 58.4 44.0 47.7 50.3 23.9 72.8 55.7
34.4
33.9
51.0
+4.8
CoTTA [57] CVPR2022 52.9 51.6 51.4 68.3 78.1 57.1 62.0 48.2 52.7 55.3 25.9 90.0 56.4
36.4
35.2
54.8
+3.6
VDP [16]
AAAI2023 52.7 51.6 50.1 58.1 70.2 56.1 58.1 42.1 46.1 45.8 23.6 70.4 54.9
34.5
36.1
50.0
+5.8
Ours
Proposed
47.7 42.5 42.9 52.2 56.9 45.5 48.9 38.9 42.7 40.7 24.3 52.8 49.1
33.5
33.1
43.4
+12.4
Directly test after adaptation
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→Mean↓Gain
ViT-base
Ours
Proposed
46.2 44.4 45.8 48.9 52.1 45.0 48.6 37.5 41.9 39.5 23.9 49.0 49.0
32.1
32.6
42.4
+13.4"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5495798319327732,"ImageNet-to-ImageNet-C. Given the source model pre-trained on ImageNet, we conduct CTTA
259"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5512605042016807,"on ImageNet-C, which consists of fifteen corruption types that occur sequentially during the test
260"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5529411764705883,"time. Table .1 demonstrates that the majority of methods employing the ViT backbone achieve lower
261"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5546218487394958,"classification errors compared to those using the ResNet50 backbone. For ViT-base, the average
262"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5563025210084034,"classification error is up to 55.8% when we directly test the source model on target domains. In
263"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5579831932773109,"contrast, our method can outperform all previous methods, achieving a 12.4% and 6.6% improvement
264"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5596638655462185,"over the source model and previous SOTA method, respectively. Moreover, our method showcases
265"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.561344537815126,"remarkable performance across the majority of corruption types, highlighting its effective mitigation
266"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5630252100840336,"of error accumulation and its capability for continual adaptation. After completing the entire CTTA
267"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5647058823529412,"process, we evaluate the performance of our method on the seen target domains. As shown in
268"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5663865546218487,"Table 1, the performance degradation is observed in only 2 out of 15 corruption types. Additionally,
269"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5680672268907563,"we achieve an overall improvement of 1.0% in the average classification error. These findings
270"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5697478991596638,"demonstrate that our method successfully preserves continual domain-shared knowledge and avoids
271"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5714285714285714,"catastrophic forgetting during CTTA. In conclusion, our homeostatic ViDAs can extract the different
272"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.573109243697479,"domain knowledge and avoid CTTA main challenges simultaneously.
273"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5747899159663865,"Table 2:
Average error rate (%) for the stan-
dard CIFAR10-to-CIAFAR10C and CIFAR100-
to-CIAFAR100C CTTA task. All results are eval-
uated on the ViT-base, which is fully pre-trained
on the source domain dataset."
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5764705882352941,"Target
Method Source Tent CoTTA VDP Ours"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5781512605042017,"Cifar10C Mean↓
28.2
25.5
24.6
24.1 20.7
Gain↑
0.0
+2.7
+3.6
+4.1 +7.5"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5798319327731093,"Cifar100C Mean↓
35.4
33.2
34.8
35.0 27.3
Gain↑
0.0
+2.2
+0.7
+0.4 +8.1"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5815126050420169,"Table 3: Average error rate (%) for the CIFAR10-
to-CIFAR10C CTTA task. All results are evalu-
ated on the ViT-Base, which uses the pre-trained
encoder parameter of foundation models (DI-
NOv2 [43] and SAM [32])."
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5831932773109244,Backbone Method Source Tent CoTTA Ours
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.584873949579832,"DINOv2
Mean↓
25.0
21.7
29.3
20.2
Gain↑
0.0
+3.2
−4.3
+4.8"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5865546218487395,"SAM
Mean↓
39.3
37.5
39.4
34.1
Gain↑
0.0
+1.8
−0.1
+5.2"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5882352941176471,"To further validate the effectiveness of our method, we conduct experiments on CIFAR10-to-
274"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5899159663865546,"CIFAR10C and CIFAR100-to-CIFAR100C. As illustrated in Table .2, in CIFAR10C, our approach
275"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5915966386554622,"achieved a 3.4% improvement compared to the previous SOTA model. We extend our evaluation to
276"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5932773109243697,"CIFAR100C, which comprises a larger number of categories in each domain. Our approach surpasses
277"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5949579831932773,"all previous methods, which show the same trend as the above CTTA experiments. Therefore, the re-
278"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5966386554621849,"sults prove that our method mitigates the challenges posed by continual distribution shifts, regardless
279"
THE EFFECTIVENESS ON CLASSIFICATION CTTA,0.5983193277310924,"of the number of categories present in each domain.
280"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6,"4.3
The Effectiveness on Segmentation CTTA
281"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6016806722689075,"Table 4: Performance comparison for Cityscape-to-ACDC CTTA. We sequentially repeat the
same sequence of target domains three times. Mean is the average score of mIoU."
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6033613445378151,"Time
t −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
Round
1
2
3
Mean↑Gain
Method
REF
Fog Night Rain Snow Mean↑Fog Night Rain Snow Mean↑Fog Night Rain Snow Mean↑
Source [58] NIPS2021 69.1 40.3 59.7 57.8
56.7
69.1 40.3 59.7 57.8
56.7
69.1 40.3 59.7 57.8
56.7
56.7
/
TENT [55]
ICLR2021 69.0 40.2 60.1 57.3
56.7
68.3 39.0 60.1 56.3
55.9
67.5 37.8 59.6 55.0
55.0
55.7
-1.0
CoTTA [57] CVPR2022 70.9 41.2 62.4 59.7
58.6
70.9 41.1 62.6 59.7
58.6
70.9 41.0 62.7 59.7
58.6
58.6
+1.9
DePT [20]
ICLR2023 71.0 40.8 58.2 56.8
56.5
68.2 40.0 55.4 53.7
54.3
66.4 38.0 47.3 47.2
49.7
53.4
-3.3
VDP [16]
AAAI2023 70.5 41.1 62.1 59.5
58.3
70.4 41.1 62.2 59.4
58.2
70.4 41.0 62.2 59.4
58.2
58.2
+1.5
Ours
Proposed 71.6 43.2 66.0 63.4
61.1
73.2 44.5 67.0 63.9
62.2
73.2 44.6 67.2 64.2
62.3
61.9
+5.2"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6050420168067226,"road
sidew
build
wall
fence
pole
tr.light
tr.sign
veget
terrain
sky
person
rider
car
truck
bus
train
m.bike
bike
n/a."
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6067226890756302,"Snow
Night"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6084033613445378,"Image
CoTTA
VDP
ViDA
Ground Truth"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6100840336134454,"Figure 4: Qualitative comparison of our method with previous SOTA methods on the ACDC dataset.
Our method could better segment different pixel-wise classes such as shown in the white box."
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.611764705882353,"Cityscapes-to-ACDC. To demonstrate the effectiveness of our method in the semantic segmentation
282"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6134453781512605,"CTTA task, we conducted evaluations on four target domains from the ACDC dataset periodically
283"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6151260504201681,"during test time. As presented in Table 4, we observed a gradual decrease in the mIoUs of TENT
284"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6168067226890757,"and DePT over time, indicating the occurrence of catastrophic forgetting. In contrast, our method
285"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6184873949579832,"has a continual improvement of average mIoU (61.1→62.2→62.3) when the same sequence of
286"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6201680672268908,"target domains is repeated. Significantly, the proposed method surpasses the previous state-of-the-art
287"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6218487394957983,"CTTA method [57] by achieving a 3.3% increase in mIoU. This notable improvement showcases our
288"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6235294117647059,"method’s ability to adapt continuously to different target domains in the pixel-level task. In Fig .4,
289"
THE EFFECTIVENESS ON SEGMENTATION CTTA,0.6252100840336134,"our method correctly distinguish the sidewalk from the road, avoiding mis-classification.
290"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.626890756302521,"4.4
Continual Adapting for Foundation Models
291"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6285714285714286,"Foundation models [5] are trained on large-scale datasets, endowing them with powerful generaliza-
292"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6302521008403361,"tion capabilities and the ability to capture representations of common features. However, performing
293"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6319327731092437,"full fine-tuning on the foundation model is time-consuming and economically impractical. Hence, our
294"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6336134453781512,"adaptation method proves valuable by enhancing the continual transfer performance of foundation
295"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6352941176470588,"Table 5:
The domain generalization compar-
isons on ImageNet-C. Results are evaluated on
ViT-base. Mean and Gain(%) represent the per-
formance on unseen target domains."
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6369747899159663,"Directly test on unseen domains Unseen
Method bri. contrast elastic pixelate jpeg Mean↓
Source 26.4
91.4
57.5
38.0
36.2
49.9
Tent
25.8
91.9
57.0
37.2
35.7
49.5
CoTTA 25.3
88.1
55.7
36.4
34.6
48.0
Ours
24.6
68.2
49.8
34.7
34.1
42.3"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6386554621848739,"Table 6:
Average error rate (%) for the
ImageNet-to-ImageNet-C. Results are evaluated
on the ViT. V iDAh and V iDAl represent the
ViDAs with high-rank and low-rank prototypes."
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6403361344537815,"V iDAh
V iDAl
HKA
Mean↓
Ex1
-
-
-
55.8
Ex2
✓
-
-
51.2
Ex3
-
✓
-
50.7
Ex4
✓
✓
-
45.6
Ex5
✓
✓
✓
43.4"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6420168067226891,"models. As indicated in Table. 3, we introduce foundation models as the pre-trained model and adapt
296"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6436974789915967,"them to continual target domains (CIFAR10C). Our approach achieved a performance improvement
297"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6453781512605042,"of 4.8% on the representative image-level foundation model DINOv2 [43] and 5.2% on pixel-level
298"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6470588235294118,"foundation model SAM [32]. Our method consistently and reliably improves the performance of the
299"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6487394957983194,"foundation model on the unseen continual target domains. Note that, we only use the pre-trained
300"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6504201680672269,"encoder of SAM and add a classification head, which is fine-tuned on the source domain. During the
301"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6521008403361345,"inference phase, the ViDAs with a linear relationship can be projected onto the pre-trained foundation
302"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.653781512605042,"model through re-parameterization. This process empowers the foundation model with the learned
303"
CONTINUAL ADAPTING FOR FOUNDATION MODELS,0.6554621848739496,"different domain representations and maintains the model plasticity.
304"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.6571428571428571,"4.5
Domain Generalization on Unseen Continual Domains
305"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.6588235294117647,"To investigate the domain generalization (DG) ability of our method, we follow the leave-one-domain-
306"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.6605042016806723,"out rule [62, 38] to leverage 10/15 domains of ImageNet-C as source domains for model training while
307"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.6621848739495798,"the rest (5/15 domains) are treated as target domains without any form of adaptation. Specifically,
308"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.6638655462184874,"we first use our proposed method to continually adapt the pre-trained model to 10/15 domains of
309"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.6655462184873949,"ImageNet-C without any supervision. Then we directly test on the 5/15 unseen domains. Surprisingly,
310"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.6672268907563025,"our method reduces 7.6% on the average error on unseen domains (Table 5), which has a significant
311"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.66890756302521,"improvement over other methods. The promising results demonstrate that our method possesses DG
312"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.6705882352941176,"ability by effectively extracting domain-agnostic knowledge. This finding provides a new perspective
313"
DOMAIN GENERALIZATION ON UNSEEN CONTINUAL DOMAINS,0.6722689075630253,"on enhancing DG performance. More DG experiments are provided in the supplementary materials.
314"
ABLATION STUDY,0.6739495798319328,"4.6
Ablation study
315"
ABLATION STUDY,0.6756302521008404,"Effectiveness of each component. We conduct the ablation study on ImageNet-to-ImageNet-C
316"
ABLATION STUDY,0.6773109243697479,"CTTA scenario and evaluate the contribution of each component in our method, including high-rank
317"
ABLATION STUDY,0.6789915966386555,"ViDA (V iDAh), low-rank ViDA (V iDAl), and Homeostatic Knowledge Allotment (HKA) strategy.
318"
ABLATION STUDY,0.680672268907563,"As shown in Table .6, Ex1 represents the performance of the source pre-trained model (only 55.8%).
319"
ABLATION STUDY,0.6823529411764706,"In Ex2, by introducing the high-rank ViDA, the average error decrease 4.6%, demonstrating that
320"
ABLATION STUDY,0.6840336134453782,"the high-rank prototype can extract more domain-specific knowledge to adapt in target domains.
321"
ABLATION STUDY,0.6857142857142857,"As illustrated in Ex3, low-rank ViDA gains 5.1% improvement compared to Ex1. The result
322"
ABLATION STUDY,0.6873949579831933,"proves that the domain-share knowledge extracted from low-rank prototypes can also improve the
323"
ABLATION STUDY,0.6890756302521008,"classification ability on continual target domains. Ex4 has a remarkable improvement of 10.2%
324"
ABLATION STUDY,0.6907563025210084,"overall, demonstrating that the two types of ViDA can compensate for each other in the continual
325"
ABLATION STUDY,0.692436974789916,"adaptation process. Ex5 achieves 12.4% improvement in total, showcasing the effectiveness of the
326"
ABLATION STUDY,0.6941176470588235,"HKA strategy in maximizing the CTTA potential of both types of ViDA.
327"
CONCLUSION AND LIMITATIONS,0.6957983193277311,"5
Conclusion and Limitations
328"
CONCLUSION AND LIMITATIONS,0.6974789915966386,"In this paper, we propose a homeostatic Visual Domain Adapter (ViDA) to address error accumulation
329"
CONCLUSION AND LIMITATIONS,0.6991596638655462,"and catastrophic forgetting problems in Continual Test-Time Adaptation (CTTA) tasks. And we
330"
CONCLUSION AND LIMITATIONS,0.7008403361344537,"investigate that the low-rank ViDA can disregard the impact of dynamic distribution shifts and
331"
CONCLUSION AND LIMITATIONS,0.7025210084033613,"prioritize the extraction of domain-invariant knowledge, and the high-rank ViDA can extract more
332"
CONCLUSION AND LIMITATIONS,0.704201680672269,"reliable domain-specific knowledge. Meanwhile, we further propose a Homeostatic Knowledge
333"
CONCLUSION AND LIMITATIONS,0.7058823529411765,"Allotment (HKA) strategy to dynamically fuse the knowledge from each ViDA with different rank
334"
CONCLUSION AND LIMITATIONS,0.7075630252100841,"prototypes. For limitations, the injected ViDAs and teacher-student scheme brings extra parameters
335"
CONCLUSION AND LIMITATIONS,0.7092436974789916,"and computational costs during the continual adaptation process.
336"
REFERENCES,0.7109243697478992,"References
337"
REFERENCES,0.7126050420168067,"[1] Eduardo Arnold, Omar Y Al-Jarrah, Mehrdad Dianati, Saber Fallah, David Oxtoby, and Alex
338"
REFERENCES,0.7142857142857143,"Mouzakitis. A survey on 3d object detection methods for autonomous driving applications.
339"
REFERENCES,0.7159663865546219,"IEEE Transactions on Intelligent Transportation Systems, 20(10):3782–3795, 2019.
340"
REFERENCES,0.7176470588235294,"[2] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual
341"
REFERENCES,0.719327731092437,"prompts for adapting large-scale models. 2022.
342"
REFERENCES,0.7210084033613445,"[3] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-
343"
REFERENCES,0.7226890756302521,"nifer Wortman Vaughan. A theory of learning from different domains. Machine learning,
344"
REFERENCES,0.7243697478991596,"79(1):151–175, 2010.
345"
REFERENCES,0.7260504201680672,"[4] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
346"
REFERENCES,0.7277310924369748,"for domain adaptation. Advances in neural information processing systems, 19, 2006.
347"
REFERENCES,0.7294117647058823,"[5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von
348"
REFERENCES,0.7310924369747899,"Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the
349"
REFERENCES,0.7327731092436974,"opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
350"
REFERENCES,0.7344537815126051,"[6] Malik Boudiaf, Tom Denton, Bart van Merriënboer, Vincent Dumoulin, and Eleni Triantafillou.
351"
REFERENCES,0.7361344537815127,"In search for a generalizable method for source free domain adaptation. 2023.
352"
REFERENCES,0.7378151260504202,"[7] Goirik Chakrabarty, Manogna Sreenivas, and Soma Biswas. Sata: Source anchoring and target
353"
REFERENCES,0.7394957983193278,"alignment network for continual test time adaptation. arXiv preprint arXiv:2304.10113, 2023.
354"
REFERENCES,0.7411764705882353,"[8] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation.
355"
REFERENCES,0.7428571428571429,"ArXiv, abs/2204.10377, 2022.
356"
REFERENCES,0.7445378151260504,"[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
357"
REFERENCES,0.746218487394958,"Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,
358"
REFERENCES,0.7478991596638656,"and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence,
359"
REFERENCES,0.7495798319327731,"40(4):834–848, 2017.
360"
REFERENCES,0.7512605042016807,"[10] Shoufa Chen, GE Chongjian, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping
361"
REFERENCES,0.7529411764705882,"Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. In Advances
362"
REFERENCES,0.7546218487394958,"in Neural Information Processing Systems.
363"
REFERENCES,0.7563025210084033,"[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
364"
REFERENCES,0.7579831932773109,"Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic
365"
REFERENCES,0.7596638655462185,"urban scene understanding. In Proceedings of the IEEE conference on computer vision and
366"
REFERENCES,0.761344537815126,"pattern recognition, pages 3213–3223, 2016.
367"
REFERENCES,0.7630252100840336,"[12] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu,
368"
REFERENCES,0.7647058823529411,"Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale
369"
REFERENCES,0.7663865546218488,"pre-trained language models. Nature Machine Intelligence, pages 1–16, 2023.
370"
REFERENCES,0.7680672268907563,"[13] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun.
371"
REFERENCES,0.7697478991596639,"Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference
372"
REFERENCES,0.7714285714285715,"on computer vision and pattern recognition, pages 13733–13742, 2021.
373"
REFERENCES,0.773109243697479,"[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
374"
REFERENCES,0.7747899159663866,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
375"
REFERENCES,0.7764705882352941,"An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
376"
REFERENCES,0.7781512605042017,"arXiv:2010.11929, 2020.
377"
REFERENCES,0.7798319327731092,"[15] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
378"
REFERENCES,0.7815126050420168,"uncertainty in deep learning. In international conference on machine learning, pages 1050–1059.
379"
REFERENCES,0.7831932773109244,"PMLR, 2016.
380"
REFERENCES,0.7848739495798319,"[16] Yulu Gan, Xianzheng Ma, Yihang Lou, Yan Bai, Renrui Zhang, Nian Shi, and Lin Luo.
381"
REFERENCES,0.7865546218487395,"Decorate the newcomers: Visual domain prompt for continual test time adaptation. arXiv
382"
REFERENCES,0.788235294117647,"preprint arXiv:2212.04145, 2022.
383"
REFERENCES,0.7899159663865546,"[17] Yulu Gan, Mingjie Pan, Rongyu Zhang, Zijian Ling, Lingran Zhao, Jiaming Liu, and Shanghang
384"
REFERENCES,0.7915966386554621,"Zhang. Cloud-device collaborative adaptation to continual changing environments in the
385"
REFERENCES,0.7932773109243697,"real-world. arXiv preprint arXiv:2212.00972, 2022.
386"
REFERENCES,0.7949579831932773,"[18] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François
387"
REFERENCES,0.7966386554621848,"Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural
388"
REFERENCES,0.7983193277310925,"networks. The journal of machine learning research, 17(1):2096–2030, 2016.
389"
REFERENCES,0.8,"[19] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
390"
REFERENCES,0.8016806722689076,"learners. In Joint Conference of the 59th Annual Meeting of the Association for Computational
391"
REFERENCES,0.8033613445378152,"Linguistics and the 11th International Joint Conference on Natural Language Processing,
392"
REFERENCES,0.8050420168067227,"ACL-IJCNLP 2021, pages 3816–3830. Association for Computational Linguistics (ACL), 2021.
393"
REFERENCES,0.8067226890756303,"[20] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and
394"
REFERENCES,0.8084033613445378,"Dimitris N Metaxas. Visual prompt tuning for test-time domain adaptation. arXiv preprint
395"
REFERENCES,0.8100840336134454,"arXiv:2210.04831, 2022.
396"
REFERENCES,0.8117647058823529,"[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
397"
REFERENCES,0.8134453781512605,"Surpassing human-level performance on imagenet classification. international conference on
398"
REFERENCES,0.8151260504201681,"computer vision, 2015.
399"
REFERENCES,0.8168067226890756,"[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
400"
REFERENCES,0.8184873949579832,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
401"
REFERENCES,0.8201680672268907,"pages 770–778, 2016.
402"
REFERENCES,0.8218487394957983,"[23] Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong
403"
REFERENCES,0.8235294117647058,"Bing, and Luo Si. On the effectiveness of adapter-based tuning for pretrained language model
404"
REFERENCES,0.8252100840336134,"adaptation. arXiv preprint arXiv:2106.03164, 2021.
405"
REFERENCES,0.826890756302521,"[24] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
406"
REFERENCES,0.8285714285714286,"corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
407"
REFERENCES,0.8302521008403362,"[25] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
408"
REFERENCES,0.8319327731092437,"Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning
409"
REFERENCES,0.8336134453781513,"for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019.
410"
REFERENCES,0.8352941176470589,"[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
411"
REFERENCES,0.8369747899159664,"Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
412"
REFERENCES,0.838655462184874,"preprint arXiv:2106.09685, 2021.
413"
REFERENCES,0.8403361344537815,"[27] Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu,
414"
REFERENCES,0.8420168067226891,"and Maosong Sun. Knowledgeable prompt-tuning: Incorporating knowledge into prompt
415"
REFERENCES,0.8436974789915966,"verbalizer for text classification. In Proceedings of the 60th Annual Meeting of the Association
416"
REFERENCES,0.8453781512605042,"for Computational Linguistics (Volume 1: Long Papers), pages 2225–2240, 2022.
417"
REFERENCES,0.8470588235294118,"[28] Yu Huang and Yue Chen. Autonomous driving with deep learning: A survey of state-of-art
418"
REFERENCES,0.8487394957983193,"technologies. arXiv preprint arXiv:2006.06091, 2020.
419"
REFERENCES,0.8504201680672269,"[29] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan,
420"
REFERENCES,0.8521008403361344,"and Ser-Nam Lim. Visual prompt tuning. 2022.
421"
REFERENCES,0.853781512605042,"[30] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient
422"
REFERENCES,0.8554621848739495,"low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems,
423"
REFERENCES,0.8571428571428571,"34:1022–1035, 2021.
424"
REFERENCES,0.8588235294117647,"[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
425"
REFERENCES,0.8605042016806723,"arXiv:1412.6980, 2014.
426"
REFERENCES,0.8621848739495799,"[32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,
427"
REFERENCES,0.8638655462184874,"Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv
428"
REFERENCES,0.865546218487395,"preprint arXiv:2304.02643, 2023.
429"
REFERENCES,0.8672268907563025,"[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
430"
REFERENCES,0.8689075630252101,"2009.
431"
REFERENCES,0.8705882352941177,"[34] Jogendra Nath Kundu, Naveen Venkat, Rahul M, and R. Venkatesh Babu. Universal source-free
432"
REFERENCES,0.8722689075630252,"domain adaptation. 2020.
433"
REFERENCES,0.8739495798319328,"[35] Qicheng Lao, Xiang Jiang, and Mohammad Havaei. Hypothesis disparity regularized mutual
434"
REFERENCES,0.8756302521008403,"information maximization, 2020.
435"
REFERENCES,0.8773109243697479,"[36] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for
436"
REFERENCES,0.8789915966386554,"deep neural networks. 2013.
437"
REFERENCES,0.880672268907563,"[37] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient
438"
REFERENCES,0.8823529411764706,"prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
439"
REFERENCES,0.8840336134453781,"[38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier
440"
REFERENCES,0.8857142857142857,"domain generalization. In Proceedings of the IEEE international conference on computer vision,
441"
REFERENCES,0.8873949579831932,"pages 5542–5550, 2017.
442"
REFERENCES,0.8890756302521008,"[39] Jian Liang, Ran He, and Tieniu Tan. A comprehensive survey on test-time adaptation under
443"
REFERENCES,0.8907563025210085,"distribution shifts. arXiv preprint arXiv:2303.15361, 2023.
444"
REFERENCES,0.892436974789916,"[40] Jian Liang, D. Hu, and Jiashi Feng. Do we really need to access the source data? source
445"
REFERENCES,0.8941176470588236,"hypothesis transfer for unsupervised domain adaptation. In ICML, 2020.
446"
REFERENCES,0.8957983193277311,"[41] J MacQueen. Classification and analysis of multivariate observations. In 5th Berkeley Symp.
447"
REFERENCES,0.8974789915966387,"Math. Statist. Probability, pages 281–297. University of California Los Angeles LA USA, 1967.
448"
REFERENCES,0.8991596638655462,"[42] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas
449"
REFERENCES,0.9008403361344538,"Brox, and Jan Hendrik Metzen. Test-time adaptation to distribution shift by confidence maxi-
450"
REFERENCES,0.9025210084033614,"mization and input transformation. arXiv preprint arXiv:2106.14999, 2021.
451"
REFERENCES,0.9042016806722689,"[43] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
452"
REFERENCES,0.9058823529411765,"Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning
453"
REFERENCES,0.907563025210084,"robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.
454"
REFERENCES,0.9092436974789916,"[44] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
455"
REFERENCES,0.9109243697478991,"Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
456"
REFERENCES,0.9126050420168067,"evaluating predictive uncertainty under dataset shift. Advances in neural information processing
457"
REFERENCES,0.9142857142857143,"systems, 32, 2019.
458"
REFERENCES,0.9159663865546218,"[45] Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Zhiyuan Liu, Juanzi Li, Lei Hou,
459"
REFERENCES,0.9176470588235294,"Peng Li, Maosong Sun, et al. Exploring low-dimensional intrinsic task subspace via prompt
460"
REFERENCES,0.9193277310924369,"tuning. arXiv preprint arXiv:2110.07867, 2021.
461"
REFERENCES,0.9210084033613445,"[46] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains
462"
REFERENCES,0.9226890756302522,"with residual adapters. Advances in neural information processing systems, 30, 2017.
463"
REFERENCES,0.9243697478991597,"[47] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Efficient parametrization of multi-
464"
REFERENCES,0.9260504201680673,"domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and
465"
REFERENCES,0.9277310924369748,"Pattern Recognition, pages 8119–8127, 2018.
466"
REFERENCES,0.9294117647058824,"[48] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time
467"
REFERENCES,0.9310924369747899,"object detection with region proposal networks. Advances in neural information processing
468"
REFERENCES,0.9327731092436975,"systems, 28, 2015.
469"
REFERENCES,0.934453781512605,"[49] Subhankar Roy, Martin Trapp, Andrea Pilzer, Juho Kannala, Nicu Sebe, Elisa Ricci, and Arno
470"
REFERENCES,0.9361344537815126,"Solin. Uncertainty-guided source-free domain adaptation. In Computer Vision–ECCV 2022:
471"
REFERENCES,0.9378151260504202,"17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXV, pages
472"
REFERENCES,0.9394957983193277,"537–555. Springer, 2022.
473"
REFERENCES,0.9411764705882353,"[50] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Acdc: The adverse conditions dataset with
474"
REFERENCES,0.9428571428571428,"correspondences for semantic driving scene understanding. In Proceedings of the IEEE/CVF
475"
REFERENCES,0.9445378151260504,"International Conference on Computer Vision, pages 10765–10775, 2021.
476"
REFERENCES,0.946218487394958,"[51] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning
477"
REFERENCES,0.9478991596638655,"for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision
478"
REFERENCES,0.9495798319327731,"and Pattern Recognition, pages 5227–5237, 2022.
479"
REFERENCES,0.9512605042016806,"[52] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged
480"
REFERENCES,0.9529411764705882,"consistency targets improve semi-supervised deep learning results. Learning, 2017.
481"
REFERENCES,0.9546218487394958,"[53] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
482"
REFERENCES,0.9563025210084034,"learning research, 9(11), 2008.
483"
REFERENCES,0.957983193277311,"[54] Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. Spot: Better frozen model
484"
REFERENCES,0.9596638655462185,"adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meeting of the
485"
REFERENCES,0.9613445378151261,"Association for Computational Linguistics (Volume 1: Long Papers), pages 5039–5059, 2022.
486"
REFERENCES,0.9630252100840336,"[55] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent:
487"
REFERENCES,0.9647058823529412,"Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.
488"
REFERENCES,0.9663865546218487,"[56] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, and Trevor Darrell. Tent:
489"
REFERENCES,0.9680672268907563,"Fully test-time adaptation by entropy minimization. In ICLR, 2021.
490"
REFERENCES,0.9697478991596639,"[57] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation.
491"
REFERENCES,0.9714285714285714,"ArXiv, abs/2203.13591, 2022.
492"
REFERENCES,0.973109243697479,"[58] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo.
493"
REFERENCES,0.9747899159663865,"Segformer: Simple and efficient design for semantic segmentation with transformers. Advances
494"
REFERENCES,0.9764705882352941,"in Neural Information Processing Systems, 34:12077–12090, 2021.
495"
REFERENCES,0.9781512605042016,"[59] Senqiao Yang, Jiarui Wu, Jiaming Liu, Xiaoqi Li, Qizhe Zhang, Mingjie Pan, and Shanghang
496"
REFERENCES,0.9798319327731092,"Zhang. Exploring sparse visual prompt for cross-domain semantic segmentation. arXiv preprint
497"
REFERENCES,0.9815126050420168,"arXiv:2303.09792, 2023.
498"
REFERENCES,0.9831932773109243,"[60] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized
499"
REFERENCES,0.984873949579832,"source-free domain adaptation. international conference on computer vision, 2021.
500"
REFERENCES,0.9865546218487395,"[61] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient
501"
REFERENCES,0.9882352941176471,"fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199,
502"
REFERENCES,0.9899159663865547,"2021.
503"
REFERENCES,0.9915966386554622,"[62] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization
504"
REFERENCES,0.9932773109243698,"in vision: A survey. arXiv preprint arXiv:2103.02503, 2021.
505"
REFERENCES,0.9949579831932773,"[63] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:
506"
REFERENCES,0.9966386554621849,"Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,
507"
REFERENCES,0.9983193277310924,"2020.
508"
