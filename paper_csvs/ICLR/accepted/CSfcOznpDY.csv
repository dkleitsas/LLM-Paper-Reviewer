Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0027624309392265192,"Disentangled feature representation is essential for data-efÔ¨Åcient learning. The
feature space of deep models is inherently compositional. Existing Œ≤-VAE-based
methods, which only apply disentanglement regularization to the resulting embed-
ding space of deep models, cannot effectively regularize such compositional fea-
ture space, resulting in unsatisfactory disentangled results. In this paper, we for-
mulate the compositional disentanglement learning problem from an information-
theoretic perspective and propose a recursive disentanglement network (RecurD)
that propagates regulatory inductive bias recursively across the compositional
feature space during disentangled representation learning. Experimental studies
demonstrate that RecurD outperforms Œ≤-VAE and several of its state-of-the-art
variants on disentangled representation learning and enables more data-efÔ¨Åcient
downstream machine learning tasks."
INTRODUCTION,0.0055248618784530384,"1
INTRODUCTION"
INTRODUCTION,0.008287292817679558,"Recent progress in machine learning demonstrates the ability to learn disentangled representations
is essential for data-efÔ¨Åcient learning, such as controllable image generation, image manipulation,
and domain adaptation (Suter et al., 2019; Zhu et al., 2018; Peng et al., 2019; Gabbay & Hoshen,
2021; 2019). Œ≤-VAE (Higgins et al., 2017) and its variants are the most investigated approaches
for disentangled representation learning. Recent works on Œ≤-VAE-based methods introduce various
inductive biases as regularization terms and directly apply them on the resulting embedding space of
deep models, such as the bottleneck capacity constraint (Higgins et al., 2017; Burgess et al., 2018),
total correlation among variables (Kim & Mnih, 2018; Chen et al., 2018), and the mismatch between
aggregated posterior and prior (Kumar et al., 2017), aiming to balance among representation capac-
ity, independence constraints, and reconstruction accuracy. Indeed, as demonstrated by Locatello
et al. (2020; 2019), unsupervised disentanglement is fundamentally impossible without explicit in-
ductive biases on models and data sets."
INTRODUCTION,0.011049723756906077,"However, our study shows that existing Œ≤-VAE-based methods may not be able to learn satisfac-
tory disentangled representations even for fairly trivial cases. This is due to the fact that the feature
spaces of deep models have inherently compositional structures, i.e., each complex feature is a com-
position of primitive features. However, existing methods with regularization terms solely applied to
the resulting embedding space cannot effectively propagate disentangled regularization across such
compositional feature space. As shown in Figure 1, applying the standard Œ≤-VAE to the widely"
INTRODUCTION,0.013812154696132596,"* China and Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University,
Shanghai, China, ‚Ä† Microsoft Research Asia, Shanghai, China, ‚Ä° Department of Computing, Imperial College
London, London, United Kingdom, ¬ß Department of Electrical Engineering and Computer Science, University
of Michigan, Michigan, United States, ¬∂ Department of Computer Science, University of Colorado Boulder,
Boulder, United States, || School of Microelectronics, Fudan University, Shanghai, China, ** The corresponding
author."
INTRODUCTION,0.016574585635359115,"Published as a conference paper at ICLR 2022 ùê¶
ùê≥ ùê±
ùê±‡∑ú"
INTRODUCTION,0.019337016574585635,"Encoder
Decoder"
INTRODUCTION,0.022099447513812154,"(a)
(b)
(c)
Figure 1: Illustration of the negative impact on ignoring the compositional structure of the repre-
sentation space using dSprites dataset: (a) Illustration of z (from embedding space) and m (from
intermediate layers). (b) z is not disentangled sufÔ¨Åciently when m is not disentangled sufÔ¨Åciently.
(c) The disentanglement of z improves, as that of m improves."
INTRODUCTION,0.024861878453038673,"used dataset dSprites (Matthey et al., 2017), we visualize the resulting representation z, as well as
its compositional low-level representations m extracted from the previous layer (as shown in Fig-
ure 1(a)), and evaluate the independence between each pair of m and each pair of z, respectively ‚Ä†.
Figure 1(b) and Figure 1(c) show that the disentanglement quality of low-level features m may im-
pact the resulting representation z in terms of disentanglement quality. This study demonstrates the
potential beneÔ¨Åt to regularize the compositional feature space of deep models during disentangled
representation learning."
INTRODUCTION,0.027624309392265192,"This work aims to tackle the compositional disentanglement learning problem. First, we formu-
late disentangled representation learning from an information-theoretic perspective, and introduce
a new learning objective covering three essential properties for learning disentangled representa-
tions: sufÔ¨Åciency, minimal sufÔ¨Åciency, and disentanglement. Theoretical analysis shows that the
proposed learning objective is a general form of Œ≤-VAE and several of its state-of-the-art variants.
Next, we extend the proposed learning objective to cover the disentanglement representation learn-
ing problem in the compositional feature space. Governed by the proposed learning objective, we
present Recursive Disentanglement Network (RecurD), a compositional disentanglement learning
method, which directs the disentanglement learning process across the compositional feature space
by applying regulatory inductive bias recursively through the feed-forward network. We argue that
the recursive propagation of inductive bias through the feed-forward network imposes a sufÔ¨Åcient
condition of disentangled representation learning. Empirical studies demonstrate that RecurD out-
performs Œ≤-VAE (Higgins et al., 2017) and several other variants of VAE (Burgess et al., 2018; Kim
& Mnih, 2018; Chen et al., 2018; Kumar et al., 2017) on disentangled representation learning and
achieves more data-efÔ¨Åcient learning in downstream machine learning tasks."
COMPOSITIONAL DISENTANGLEMENT LEARNING,0.03038674033149171,"2
COMPOSITIONAL DISENTANGLEMENT LEARNING"
COMPOSITIONAL DISENTANGLEMENT LEARNING,0.03314917127071823,"In this section, we Ô¨Årst formulate disentanglement learning from the information-theoretic perspec-
tive by introducing three key properties and show such formulation is a general form of the opti-
mization objectives of Œ≤-VAE and several of its variants. Next, we extend the principled objective
to the compositional feature space to tackle the compositional disentanglement learning problem."
DISENTANGLEMENT LEARNING OBJECTIVE,0.03591160220994475,"2.1
DISENTANGLEMENT LEARNING OBJECTIVE"
DISENTANGLEMENT LEARNING OBJECTIVE,0.03867403314917127,"The challenge of representation learning can be formulated as Ô¨Ånding a distribution p (z|x) that maps
original data x ‚ààX into a representation z with Ô¨Åxed amount of variables z = {z1, . . . , zn} (Bengio
et al., 2013). The key intuition of z is to capture minimal sufÔ¨Åcient information in a disentangled
manner, given the reconstruction task x ‚âàÀÜx. We denote the representation learning process as a
Markov Chain for which ÀÜx ‚Üíx ‚Üíz, which means z depends on ÀÜx only through x, i.e., p (z|x) =
p (z|x, ÀÜx) (see also, (Cover, 1999; Achille & Soatto, 2018)). The principled properties of z are
deÔ¨Åned as follows:"
DISENTANGLEMENT LEARNING OBJECTIVE,0.04143646408839779,"DeÔ¨Ånition 1. SufÔ¨Åciency: a representation z of x for ÀÜx is sufÔ¨Åcient if I (x, ÀÜx) = I (z, ÀÜx)."
DISENTANGLEMENT LEARNING OBJECTIVE,0.04419889502762431,"‚Ä†The independence between two components ci and cj is measured by the normalized mutual informa-
tion (Chen et al., 2018). Whenever NMI(ci; cj) = I(ci; cj)/H(c) = 0, ci and cj are independent (disen-
tangled)."
DISENTANGLEMENT LEARNING OBJECTIVE,0.04696132596685083,Published as a conference paper at ICLR 2022
DISENTANGLEMENT LEARNING OBJECTIVE,0.049723756906077346,"For the reconstruction task, z is sufÔ¨Åcient if it can successfully reconstruct x by ÀÜx. The difference
between I (x; ÀÜx) and I (z; ÀÜx) is computed as follows: I (x; ÀÜx) ‚àíI (z; ÀÜx) = I (x; ÀÜx|z) = H (ÀÜx|z) ‚àí
H (ÀÜx|x). Given the reconstruction task x ‚âàÀÜx, H (ÀÜx|x) is constant and independent to z, so the
sufÔ¨Åcient property can be optimized by minimizing H (ÀÜx|z) (Federici et al., 2020; Dubois et al.,
2020).
DeÔ¨Ånition 2. Minimal SufÔ¨Åciency: a representation z of x is minimal sufÔ¨Åcient if I (x; z) = I (z; ÀÜx)."
DISENTANGLEMENT LEARNING OBJECTIVE,0.052486187845303865,"A minimal sufÔ¨Åcient z encodes the minimum amount of information about x required to reconstruct
ÀÜx (Cover, 1999; Achille & Soatto, 2018). Since I (z; ÀÜx) equals to I (x; ÀÜx) when z is sufÔ¨Åcient,
the difference is computed as I (x; z) ‚àíI (z; ÀÜx) = I (x; z) ‚àíI (x; ÀÜx). Given the reconstruction
task x ‚âàÀÜx, I (x; ÀÜx) is constant and independent to z, so the minimal sufÔ¨Åciency property can be
optimized by minimizing I (x; z).
DeÔ¨Ånition 3. Disentanglement: a representation denoted as z = {z1, . . . , zn} is disentangled if
P"
DISENTANGLEMENT LEARNING OBJECTIVE,0.055248618784530384,jÃ∏=i I (zi; zj) = 0.
DISENTANGLEMENT LEARNING OBJECTIVE,0.058011049723756904,"From the deÔ¨Ånition of mutual information, I (zi; zj) = H (zi) ‚àíH (zi|zj) denotes the reduction
of uncertainty in zi when zj is observed (Cover, 1999). If any two components zi and zj are
disentangled, changes to zi have no inÔ¨Çuence on zj, which means I (zi; zj) = 0."
DISENTANGLEMENT LEARNING OBJECTIVE,0.06077348066298342,"A representation satisfying all these properties can be found by introducing two Lagrange multipliers
Œª1 and Œª2 for two constrained expected properties with respect to the fundamental sufÔ¨Åciency prop-
erty. The principled objective of disentanglement learning is to minimize the following objective:"
DISENTANGLEMENT LEARNING OBJECTIVE,0.06353591160220995,"L = H (ÀÜx|z) + Œª1I (x; z) + Œª2
X"
DISENTANGLEMENT LEARNING OBJECTIVE,0.06629834254143646,"jÃ∏=i
I (zi, zj) .
(1)"
DISENTANGLEMENT LEARNING OBJECTIVE,0.06906077348066299,"The above objective can be interpreted as the reconstruction error, plus two regularizers that yield
an optimally disentangled representation. The principled objective also helps us analyze and un-
derstand the success of recently developed Œ≤-VAE-based methods. These methods operate with
an encoder with parameter œÜ and a decoder with parameter Œ∏, to induce the joint distributions
q (x, z) = qœÜ (z|x) q (x) and p (x, z) = pŒ∏ (x|z) p (z), respectively, where p (z) is a Ô¨Åxed prior
distribution. The learning objective of Œ≤-VAE contains the reconstruction error and KL divergence
between the variational posterior and prior. To understand the relationship of learning objectives
between Equation 1 and Œ≤-VAE-based methods, we decompose I (x; z) (Kim & Mnih, 2018) and
estimate an upper bound for P"
DISENTANGLEMENT LEARNING OBJECTIVE,0.0718232044198895,"jÃ∏=i I (zi, zj) (Te Sun, 1980; 1975), then we assign different weights
as follows:"
DISENTANGLEMENT LEARNING OBJECTIVE,0.07458563535911603,"Œª1I (x; z) + Œª2
X"
DISENTANGLEMENT LEARNING OBJECTIVE,0.07734806629834254,"jÃ∏=i
I (zi, zj)"
DISENTANGLEMENT LEARNING OBJECTIVE,0.08011049723756906,‚â§ŒªaEx [KL (q (z|x) ‚à•p (z))] + ŒªbKL 
DISENTANGLEMENT LEARNING OBJECTIVE,0.08287292817679558,"q (z) ‚à• n
Y"
DISENTANGLEMENT LEARNING OBJECTIVE,0.0856353591160221,"j=1
q (zj) ! + Œªc n
X"
DISENTANGLEMENT LEARNING OBJECTIVE,0.08839779005524862,"j=1
KL (q (zj) ‚à•p (zj)) .
(2)"
DISENTANGLEMENT LEARNING OBJECTIVE,0.09116022099447514,"As shown in Table 1, the learning objectives of Œ≤-VAE and its four variants can be regarded as
speciÔ¨Åc cases of Equation 1, i.e., they assign different weights to our regularization terms, which
can balance among latent variables capacity, independence constraints, and reconstruction accuracy,
leading to successful disentangled representation learning (Zhao et al., 2017; Li et al., 2020). More
details can be found in Appendix B. However, in these works, the inductive bias toward disentan-
glement is only applied to the embedding space of z, ignoring the need of disentanglement during
feature composition in feed-forward networks."
DISENTANGLEMENT LEARNING OBJECTIVE,0.09392265193370165,"Table 1: Deriving the learning objectives of Œ≤-VAE and its four variants as speciÔ¨Åc cases of Equa-
tion 1."
DISENTANGLEMENT LEARNING OBJECTIVE,0.09668508287292818,"Method
Œ≤-VAE
FactorVAE
Œ≤-TCVAE
DIP-VAE
InfoVAE
Weight
Œªa = Œ≤.
Œªa = 1,
Œªa = 1,
Œªa = 1,
Œªa = 1 ‚àíŒ±,
Relation
Œªb = Œ≥.
Œªb = Œ≤.
Œªb = Œªc = Œª.
Œªb + Œªc = Œ± + Œª ‚àí1."
COMPOSITIONAL OBJECTIVE,0.09944751381215469,"2.2
COMPOSITIONAL OBJECTIVE"
COMPOSITIONAL OBJECTIVE,0.10220994475138122,"Considering an encoder with L layers to encode original data x into disentangled representation z.
Let us denote ml as the input features of the l-layer, which are divided into groups of features, i.e.,
ml = ‚à™jml
j, where ml
j is the j-th feature subset."
COMPOSITIONAL OBJECTIVE,0.10497237569060773,Published as a conference paper at ICLR 2022
COMPOSITIONAL OBJECTIVE,0.10773480662983426,"We formulate the compositional relation between features from two consecutive layers as follows:
ml+1
j
= Layer
 
ml √ó wl
j

. Layer can be applicable to commonly used neural network layers,
e.g., the convolution layer in computer vision tasks. The compositional relation is achieved by a
composition matrix wl ‚ààRdl√ódl+1, so that features in ml are divided into dl+1 groups after passing
through all compositional vectors (wl
js). Note that ml+1
j
is only related to the subset of features
from ml selected by wl
j."
COMPOSITIONAL OBJECTIVE,0.11049723756906077,"Similar to Section 2.1, we assume that the learning process of ml+1 depends on ml through original
data x, denoted as the Markov chain: ml ‚Üíx ‚Üíml+1. It can be written as ml+1 ‚Üíx ‚Üíml
according to the conditional independence implied by the Markovity (Cover, 1999). Consider two
output features ml+1
i
, ml+1
j
‚ààml+1 and their corresponding input feature subsets ml
i, ml
j ‚ààml,
we deÔ¨Åne key notions as follows:
DeÔ¨Ånition 4. Compositional Disentanglement: ml
i and ml
j are disentangled if I
 
ml
i; ml
j

= 0 ."
COMPOSITIONAL OBJECTIVE,0.1132596685082873,"A disentangled representation of ml
i and ml
j may improve the disentanglement quality between
ml+1
i
and ml+1
j
. Similar to DeÔ¨Ånition 3, we can achieve compositional disentanglement by mini-
mizing I
 
ml
i; ml
j

."
COMPOSITIONAL OBJECTIVE,0.11602209944751381,"DeÔ¨Ånition 5. Compositional Minimal SufÔ¨Åciency: Assume that the learning process of ml+1
j
is
denoted by the Markov chain: ml+1
j
‚Üíx ‚Üí
 
ml
i, ml
j

. Given the original data x, an input feature
set ml
j for the output feature ml+1
j
is minimal sufÔ¨Åcient if I
 
x; ml+1
j

= I
 
ml
j; ml+1
j

."
COMPOSITIONAL OBJECTIVE,0.11878453038674033,"For the output feature ml+1
j
, the input feature set ml
j is sufÔ¨Åcient and another input feature set ml
i
is superÔ¨Çuous when ml
j is able to capture all information of ml+1
j
as well as the original data x.
Furthermore, according to Data-Processing Inequality (DPI) (Cover, 1999; Achille & Soatto, 2018)
in the Markov chain, there exists an inequality that:
I
 
x; ml+1
j

‚â•I
 
ml+1
j
; ml
j

+ I
 
ml+1
j
; ml
i

‚àíI
 
ml
i; ml
j

,
(3)"
COMPOSITIONAL OBJECTIVE,0.12154696132596685,"where the difference between I
 
x; ml+1
j

and I
 
ml+1
j
; ml
j

is equivalent to the difference be-
tween I
 
ml+1
j
; ml
i

and I
 
ml
i; ml
j

. Therefore, matching I
 
ml+1
j
; ml
i

to I
 
ml
i; ml
j

can yield
a minimal sufÔ¨Åcient representation ml
j for ml+1
j
. Based on the deÔ¨Ånition of compositional disen-
tanglement, we can optimize the minimal sufÔ¨Åciency by forcing I
 
ml+1
j
; ml
i

to be 0. More details
can be found in Appendix B."
COMPOSITIONAL OBJECTIVE,0.12430939226519337,"To learn disentangled representation via effectively regularizing the compositional feature space, we
augment the principled learning objective (Equation 1) with compositional regularizers. Therefore,
the compositional learning objective for disentangled representation is deÔ¨Åned as follows:"
COMPOSITIONAL OBJECTIVE,0.1270718232044199,"L = H
 
ÀÜx|mL+1"
COMPOSITIONAL OBJECTIVE,0.1298342541436464,"|
{z
}
sufÔ¨Åcient +Œª1 Ô£´ Ô£≠
L
X l=2"
COMPOSITIONAL OBJECTIVE,0.13259668508287292,"dl+1
X"
COMPOSITIONAL OBJECTIVE,0.13535911602209943,"jÃ∏=i
I
 
ml
i; ml+1
j

Ô£∂ Ô£∏"
COMPOSITIONAL OBJECTIVE,0.13812154696132597,"|
{z
}
minimal sufÔ¨Åcient +Œª2 Ô£´"
COMPOSITIONAL OBJECTIVE,0.1408839779005525,"Ô£≠
L+1
X l=2"
COMPOSITIONAL OBJECTIVE,0.143646408839779,"dl+1
X"
COMPOSITIONAL OBJECTIVE,0.1464088397790055,"jÃ∏=i
I
 
ml
i, ml
j

Ô£∂ Ô£∏"
COMPOSITIONAL OBJECTIVE,0.14917127071823205,"|
{z
}
disentangled ,
(4)"
COMPOSITIONAL OBJECTIVE,0.15193370165745856,"where mL+1 denotes the Ô¨Ånal disentangled representation z. Our intuition is that disentangled
learning for compositional feature space could beneÔ¨Åt the disentanglement learning for high-level
representations."
RECURSIVE DISENTANGLEMENT NETWORK,0.15469613259668508,"3
RECURSIVE DISENTANGLEMENT NETWORK"
RECURSIVE DISENTANGLEMENT NETWORK,0.1574585635359116,"We now describe a learning method with the goal of optimizing the compositional disentanglement
learning objective. This method, called Recursive Disentanglement Network (RecurD), propagates
inductive bias (disentanglement) recursively across the compositional feature space."
MODEL ARCHITECTURE,0.16022099447513813,"3.1
MODEL ARCHITECTURE"
MODEL ARCHITECTURE,0.16298342541436464,"As shown in Figure 2, RecurD contains an encoder and a decoder to learn the disentangled repre-
sentation z of data x and to reconstruct ÀÜx, where the encoder contains multiple Recursive Modules
and the decoder is a Deconvolutional Neural Network (Zeiler & Fergus, 2014)."
MODEL ARCHITECTURE,0.16574585635359115,Published as a conference paper at ICLR 2022
MODEL ARCHITECTURE,0.1685082872928177,Figure 2: Recursive Disentanglement Network
MODEL ARCHITECTURE,0.1712707182320442,"The Ô¨Årst Recursive Module of the encoder is implemented by a multi-channel convolutional net-
work (Lawrence et al., 1997) to encode the original image x. Following the notation in Section 2.2,
the output of the 1-st Recursive Module is denoted as m2, which is also the input of 2-nd Re-
cursive Module. As for the l-th (l ‚â•2) Recursive Module, it contains (1) a Router R to learn a
composition matrix wl from the input features ml to decompose ml into subsets; and (2) a Group-
of-Encoders (GoE) layer consisting of n encoders to induce the output feature ml+1."
MODEL ARCHITECTURE,0.17403314917127072,"Inspired by the Gate of Mixture-of-Experts (Shazeer et al., 2017; Fedus et al., 2021) on parameter
selection for each input, we present a Router with TopK to learn the compositional relation. In
detail, the Router takes ml as input and compute composition matrix wl via learning similarity:"
MODEL ARCHITECTURE,0.17679558011049723,"wl = softmax
 
TopK
 
R
 
ml
, k

, R
 
ml
= softmax

ml 
mlT 
‚ó¶v, and"
MODEL ARCHITECTURE,0.17955801104972377,"TopK
 
R
 
ml
, k

=

R
 
ml"
MODEL ARCHITECTURE,0.18232044198895028,"ij ,
ifR
 
ml"
MODEL ARCHITECTURE,0.1850828729281768,"ij is in the top k elements of R
 
ml"
MODEL ARCHITECTURE,0.1878453038674033,"¬∑j
‚àí‚àû,
otherwise. (5)"
MODEL ARCHITECTURE,0.19060773480662985,"Here, R (m) denotes the similarity matrix and v is a learning matrix learned by a linear layer. ‚ó¶
denotes Hadamard product. TopK is the compositional strategy to determine whether input feature
ml
ij belongs to the input feature set of ml+1
j
, and k is a hyperparameter. The GoE layer consists
of dl+1 parallel encoders

Enc1, . . . , Encdl+1
	
to generate the output features ml+1, where each
encoder is implemented by a convolutional neural network with speciÔ¨Åc parameters:"
MODEL ARCHITECTURE,0.19337016574585636,"ml+1
j
= Encj
 
ml √ó wl
j

.
(6)"
MODEL ARCHITECTURE,0.19613259668508287,"Then, ml+1 is obtained by concatenating the outputs from each encoder, which is converted directly
to the input of the l + 1-th Recursive Module. Note that the output of L-th Recursive Module is
the learned disentangled representation z. Finally, the Decoder Dec takes z as input to obtain the
reconstructed input ÀÜx: ÀÜx = Dec (z)."
LEARNING OF RECURD,0.19889502762430938,"3.2
LEARNING OF RECURD"
LEARNING OF RECURD,0.20165745856353592,"As mentioned in Equation 4, the learning objective of RecurD governs the recursive disentanglement
learning across the compositional feature space. As shown by related works, a precise estimation
of mutual information in the high dimensional space is important for accurately estimating the loss.
According to the recent progresses, the mutual information between two representations can be
maximized by using any sample-based differentiable mutual information lower bound. Similar to
the work of Federici et al. (2020); Hjelm et al. (2019); Wen et al. (2020), we utilize MINE es-
timators (Belghazi et al., 2018) to estimate the mutual information. This approach introduces an
auxiliary parametric model Estimator Net which is jointly optimized during the training procedure
using re-parametrized samples from the posterior distribution."
RELATED WORK,0.20441988950276244,"4
RELATED WORK"
RELATED WORK,0.20718232044198895,"Œ≤-VAE-based methods introduce various regularization terms and directly apply them on the result-
ing embedding space. Higgins et al. (2017) proposed Œ≤-VAE method by introducing a constraint
Œ≤ over the KL divergence between the inferred distribution qœÜ(z|x) and its prior‚Äìan isotropic unit
Gaussian. Burgess et al. (2018) found that the network specializes in the factor that contributes most
to a small reconstruction error, with a limited channel capacity. Thus, they proposed the Annealed-
VAE, an extension of Œ≤-VAE, which gradually adds more latent encoding capacity by enforcing the
KL divergence to be at a controllable value C. Kim & Mnih (2018) analyzed the disentanglement"
RELATED WORK,0.20994475138121546,Published as a conference paper at ICLR 2022
RELATED WORK,0.212707182320442,"performance of Œ≤-VAE by breaking down the regularization term into two components. One is Total
Correlation (TC) (Watanabe, 1960), which encourages the marginal distribution of representations
to be factorial. The other is mutual information I(x; z), which reduces the amount of information
about x stored in z. Based on the decomposition, they proposed FactorVAE to relax the regular-
ization of I(x; z) while directly penalizing the TC term. Similarly, Chen et al. (2018) proposed
Œ≤-TCVAE by decomposing the TC penalty as the dependence among variables and the distance be-
tween each variable‚Äôs posterior and prior. Since TC is intractable, both FactorVAE and Œ≤-TCVAE
use approximation methods. Another TC-related approach is DIP-VAE (Kumar et al., 2017), whose
designers argued that the estimation of TC requires additional parameters and suffers from vanishing
gradients. Therefore, DIP-VAE optimizes the moments distance between the aggregated posterior
and a factorized prior instead of estimating TC. The above methods improved over Œ≤-VAE by ap-
plying inductive biases directly to the high-level latent variable space. However, the feature space
of deep models is compositional in nature. Existing variants of Œ≤-VAE cannot effectively apply
disentanglement regularization across such compositional feature space, and yield inferior disentan-
glement in representation learning. Our approach diverges from prior works by taking a principled
information-theoretic approach to formulate and analyze the compositional disentanglement feature
structure."
RELATED WORK,0.2154696132596685,"Recent works on hierarchical VAEs introduce layer-wise disentanglement regularization to learn
conditioning structures across multi-layer latent variables, such as VampPrior (Tomczak & Welling,
2018), Ladder VAEs (S√∏nderby et al., 2016), and NVAE (Vahdat & Kautz, 2020). In these hierar-
chical model structures, e.g., the cross-layer residual connection structure in NVAE and VampPrior,
inter-layer regularization is less of a focus. The latent variables of a preceding layer serve as shared
inputs to the next layer, which introduces information redundancy and hence impairs representa-
tion disentanglement. In contrast, the proposed compositional objective optimizes the statistical
independence of inter-layer and intra-layer latent variables simultaneously, thereby minimizing the
information redundancy of inter-layer information sharing and improving disentanglement quality."
EXPERIMENTS,0.21823204419889503,"5
EXPERIMENTS"
EXPERIMENTS,0.22099447513812154,"This section presents both quantitative and qualitative experiments to evaluate RecurD in terms of
disentanglement quality and data efÔ¨Åciency on downstream tasks."
PERFORMANCE OF DISENTANGLEMENT LEARNING,0.22375690607734808,"5.1
PERFORMANCE OF DISENTANGLEMENT LEARNING"
PERFORMANCE OF DISENTANGLEMENT LEARNING,0.2265193370165746,"We compare the disentanglement learning performance of RecurD with Œ≤-VAE and its state-of-the-
art variants, including Œ≤-VAE (Higgins et al., 2017), Annealed-VAE (Burgess et al., 2018), Factor-
VAE (Kim & Mnih, 2018), Œ≤-TCVAE (Chen et al., 2018), DIP-VAE (Kumar et al., 2017). More
experimental results can be found in Appendix E."
PERFORMANCE OF DISENTANGLEMENT LEARNING,0.2292817679558011,"Datasets: We consider two datasets in which each image is obtained by a deterministic function
of ground-truth factors: dSprites (Matthey et al., 2017) and 3DShapes (Burgess & Kim, 2018).
dSprites contains 737, 280 binary 64 √ó 64 images of 2D shapes with 5 ground truth factors, i.e., 3
shapes, 6 scales, 40 orientations, 32 x-positions, 32 y-position. 3DShapes contains 480, 000 RGB
64√ó64√ó3 images of 3D shapes with 6 ground truth factors, i.e., 4 shapes, 8 scales, 15 orientations,
10 Ô¨Çoor hues, 10 wall hues, 10 object hues. For all experiments, we use a 9:1 training to testing data
ratio, following earlier work (Kumar et al., 2017; Locatello et al., 2019). More details on network
architecture and hyperparameter settings are included in Appendix D."
PERFORMANCE OF DISENTANGLEMENT LEARNING,0.23204419889502761,"Evaluation Metrics for Disentanglement: There is no standard metric for evaluating disentangle-
ment (Zhou et al., 2021; Ridgeway & Mozer, 2018), and most existing metrics involve the estimation
of a variable-factor matrix relating the factors of variation to the learned representations. In the ex-
periments, we consider three widely used metrics: Separated Attribute Predictability (SAP) (Kumar
et al., 2017), Mutual Information Gap (MIG) (Chen et al., 2018) and Disentanglement, Complete-
ness, and Informativeness (DCI) (Eastwood & Williams, 2018). DCI contains three metrics for
disentanglement (DCI-D), Completeness (DCI-C) and Informativeness (DCI-I), respectively. Due
to the space limitation, we present the results of DCI-D and include the results on DCI-C and DCI-
I in the Appendix E. Overall, these metrics can comprehensively evaluate RecurD from different
disentanglement measurements."
PERFORMANCE OF DISENTANGLEMENT LEARNING,0.23480662983425415,Published as a conference paper at ICLR 2022
QUANTITATIVE RESULTS,0.23756906077348067,"5.1.1
QUANTITATIVE RESULTS"
QUANTITATIVE RESULTS,0.24033149171270718,"For quantitative analysis, we conduct three sets of experiments: 1) evaluating the average perfor-
mance of disentanglement score via three evaluation metrics; 2) measuring the trade-off among
reconstruction, minimal sufÔ¨Åciency and disentanglement with mini-batch samples; and 3) analyzing
the inÔ¨Çuence of compositional objective for disentanglement learning via the three properties."
QUANTITATIVE RESULTS,0.2430939226519337,"Table 2 compares the reconstruction error and three widely-used disentanglement metrics of RecurD
and Ô¨Åve other methods on the dSprites and 3DShapes datasets. Compared with all the baselines,
RecurD achieves much lower reconstruction error as well as higher SAP, MIG, and DCI scores in
most of the cases. It is important to point out that the reconstruction error of Œ≤-VAE increases
as Œ≤ increases (stronger disentanglement regularization), which indicates that Œ≤-VAE comprises
reconstruction error for disentanglement. However, RecurD achieves better in both reconstruction
and disentanglement, demonstrating the superiority of the proposed compositional learning objective
and the recursive disentanglement network."
QUANTITATIVE RESULTS,0.24585635359116023,"Figure 3 shows scatter plots relating Minimal SufÔ¨Åciency and Disentanglement to reconstruction er-
ror (SufÔ¨Åciency), where each point represents a mini-batch of data. Here, we compute the minimal
sufÔ¨Åciency score using I(x; z) and present it by the area of each point. In all baseline methods, we
observe that smaller scatters are correlated with higher reconstruction errors and higher disentangle-
ment scores. The reason is that Œ≤-VAE and its variants only place disentanglement regularization on
the embedding space of z which imposes a limit on the capacity of the information channel (Higgins
et al., 2017), so that features which are important for reconstruction but harmful for disentanglement
may lose during training, leading to compromised reconstruction quality. However, RecurD reveals
the necessity of disentanglement during feature composition in feed-forward networks and induces
effective information compression. Therefore, compared to all the baselines, the representations
from RecurD obtain better disentanglement and minimal sufÔ¨Åciency without degrading reconstruc-
tion quality."
QUANTITATIVE RESULTS,0.24861878453038674,"Figure 4 shows the impact of the number of Recursive Modules in disentanglement learning. We
evaluate three RecurD variants on the principled properties, including RecurD 0, RecurD 1 and
RecurD 2 with 1, 2 and 3 Recursive Modules, respectively. And we report the values correspond-"
QUANTITATIVE RESULTS,0.2513812154696133,"Table 2: Three widely used evaluation scores and reconstruction error on the test sets for dSprites and
3Dshapes. Boldface indicates the best results, i.e., reconstruction error or disentanglement scores."
QUANTITATIVE RESULTS,0.2541436464088398,"dataset
dSprites
3DShapes
Reconst. Error
SAP
MIG
DCI-D
Reconst error
SAP
MIG
DCI-D
Œ≤-VAE(Œ≤ = 4)
0.0066
0.0284
0.2617
0.1191
0.0216
0.1463
0.2519
0.4682
Œ≤-VAE(Œ≤ = 16)
0.0094
0.0242
0.2241
0.1820
0.0544
0.1488
0.2629
0.4528
Œ≤-VAE(Œ≤ = 60)
0.0127
0.0445
0.1432
0.1291
0.0624
0.1041
0.2402
0.4317
Annealed-VAE
0.0171
0.0311
0.1177
0.1449
0.0811
0.0730
0.2217
0.4279
Factor-VAE
0.0228
0.0436
0.2594
0.1955
0.0800
0.1331
0.2630
0.4491
Œ≤-TCVAE
0.0162
0.0352
0.1585
0.1774
0.0312
0.0364
0.2070
0.4487
DIP-VAE
0.0213
0.0261
0.0731
0.1038
0.0213
0.2013
0.3108
0.4853
RecurD
0.0047
0.0502
0.2707
0.3841
0.0083
0.1979
0.3105
0.5804"
QUANTITATIVE RESULTS,0.2569060773480663,"Figure 3:
Reconstruction error vs. disentanglement performance. Scatters located at the left top
indicate better performance. The area of each scatter represents the minimal sufÔ¨Åciency score esti-
mated by I(x; z), and smaller area indicates better performance."
QUANTITATIVE RESULTS,0.2596685082872928,Published as a conference paper at ICLR 2022
QUANTITATIVE RESULTS,0.26243093922651933,"ing to the three terms in our compositional learning objective on the training sets. We observe that
RecurD 1 and RecurD 2 perform much better than RecurD 0 on the optimization of minimal
sufÔ¨Åciency and disentanglement. In addition, during the early stage of training, the performance of
RecurD 2 improves much faster than RecurD 1. This experiment demonstrates that the recursive
propagation of inductive bias through the feed-forward network improves disentangled representa-
tion learning."
QUALITATIVE RESULTS,0.26519337016574585,"5.1.2
QUALITATIVE RESULTS"
QUALITATIVE RESULTS,0.26795580110497236,"Figure 4: The performance of RecurD with varying number
of recursive modules on the principled properties."
QUALITATIVE RESULTS,0.27071823204419887,"For qualitative analysis, we present
the latent traversals of RecurD on two
datasets in Figure 5, in which we vary
a single variable learned by an en-
coder in GoE while keeping all oth-
ers Ô¨Åxed. For images in 3DShapes,
the latent traversals show that Re-
curD is able to successfully capture
all the six factors of variation. For
images in dSprites, we observe that
RecurD is able to discover x-position,
y-position and scale (continuous vari-
ables).
More importantly, RecurD
can, to some extent, discover shape
and orientation (discrete variables),
which have been proved to be strug-
gling for many other methods (Ku-
mar et al., 2017; Kim & Mnih, 2018;
Locatello et al., 2019). The reason is that Œ≤-VAE and its variants encourage independence among
variables via controlling the information capacity of z and matching the posterior p(z|x) to an
isotropic unit Gaussian. However, learning discrete variables would require using a discrete prior
instead of Gaussian (Kim & Mnih, 2018). On the other hand, RecurD is able to model both dis-
crete and continuous factors by directly disentangling based on the three principled properties on
the entire compositional feature space, leading to stronger representation capability."
ABLATION AND PARAMETER DEPENDENCE STUDY,0.27348066298342544,"5.1.3
ABLATION AND PARAMETER DEPENDENCE STUDY"
ABLATION AND PARAMETER DEPENDENCE STUDY,0.27624309392265195,"In this section, we evaluate the impacts of hyperparameters in both learning objective and model
architecture. First, we study the impact of compositional learning objective with varying regular-
ization coefÔ¨Åcients (Œª1 and Œª2) of minimal sufÔ¨Åciency and disentanglement. Then, we evaluate the
inÔ¨Çuence of hyperparameter k ‚Äî the group size in the GoE of Recursive Module. Figure 6 (Re-
curD with varying Œª1, Œª2 and k) shows the scatter plots of disentanglement score (SAP) along with
reconstruction error on the test sets of dSprites and 3DShapes."
ABLATION AND PARAMETER DEPENDENCE STUDY,0.27900552486187846,"As shown in Figure 6, larger penalties on both minimal sufÔ¨Åciency and disentanglement yield higher
disentanglement score and lower reconstruction error, demonstrating the importance of the minimal
sufÔ¨Åciency term and the disentanglement term in Equation 1. Note that RecurD with Œª2 = 0 is"
ABLATION AND PARAMETER DEPENDENCE STUDY,0.281767955801105,"Figure 5: First row: original images. Second
row: reconstructions. Remaining rows: recon-
structions of latent traversals."
ABLATION AND PARAMETER DEPENDENCE STUDY,0.2845303867403315,"Figure 6: Ablation study on Œª1, Œª2 and group
size k. Note that RecurD with Œª2 = 0 reduces to
Œ≤-VAE with the compositional architecture."
ABLATION AND PARAMETER DEPENDENCE STUDY,0.287292817679558,Published as a conference paper at ICLR 2022
ABLATION AND PARAMETER DEPENDENCE STUDY,0.2900552486187845,"reduced to a standard Œ≤-VAE with the compositional architecture (as I(x; z) is an lower bound of
KL(p(z|x)‚à•p(z))), which underperforms RecurD, indicating that an explicit penalty on disentan-
glement is important for disentangled representation learning. As for the group size in the GoE ‚Äî
k, it is not surprising that when k increases, RecurD has lower reconstruction errors, with a slightly
inferior disentanglement performance. The reason is that a dense composition of feature space can
help the model maintain sufÔ¨Åcient information but make it difÔ¨Åcult to disentangle latent variables.
RecurD with k = 1 results in poor performance on both reconstruction and disentanglement, in-
dicating that over-emphasizing decomposition in the feature space may fail to preserve sufÔ¨Åcient
information. More results are presented in the Appendix E."
PERFORMANCE OF DOWNSTREAM TASKS,0.292817679558011,"5.2
PERFORMANCE OF DOWNSTREAM TASKS"
PERFORMANCE OF DOWNSTREAM TASKS,0.2955801104972376,"Figure 7:
Performance comparison of RecurD
and baselines on the standard classiÔ¨Åcation task
(left) and the domain generalization task (right).
Each model is trained by varying ratios of training
data. The dotted line represents the performance
of a single-layer neural network."
PERFORMANCE OF DOWNSTREAM TASKS,0.2983425414364641,"In this section, we compare the performance
of RecurD and Ô¨Åve baseline methods by mea-
suring their data efÔ¨Åciency on two downstream
tasks:
a standard classiÔ¨Åcation task on the
MNIST dataset and a domain generalization
task on the MNIST-Rotation dataset (Ghifary
et al., 2015). MNIST-Rotation is a synthetic
dataset consisting of 6 domains, each contain-
ing 1, 000 images of the 10 digits randomly se-
lected from the training set of MNIST, with 6
rotation degrees: 0‚ó¶, 15‚ó¶, 30‚ó¶, 45‚ó¶, 60‚ó¶and
75‚ó¶. For both tasks, we set the training sets of
different proportions: 20%, 40%, 60%, 80%,
and 100%, to evaluate the classiÔ¨Åcation perfor-
mance of RecurD and the baselines with differ-
ent amounts of training data. For the domain
generalization problem, we follow the previous
works (Li et al., 2017; Balaji et al., 2018; Du et al., 2020) with the same train-test split strategy and
the leave-one-domain-out strategy, i.e., we take the samples from one domain as the target domain
for testing, and the samples from the remaining domains as the source domain for training."
PERFORMANCE OF DOWNSTREAM TASKS,0.3011049723756906,"Figure 7 reports the average accuracy of different methods on the same test set. We can observe that
all methods achieve decent data efÔ¨Åciency on both tasks, i.e., without much performance degradation
even with 20% of training data, suggesting that the learning process of disentangled representation
can more effectively capture information from the inputs. However, Œ≤-VAE and its variants do not
achieve satisfactory classiÔ¨Åcation accuracy for either task compared to a single-layer neural network.
The reason is that Œ≤-VAE and its variants obtain the disentangled representation by limiting the ca-
pacity of information channels, thus they tend to only maintain features that contribute more to dis-
entanglement, which may lose informative features that contribute more to classiÔ¨Åcation. Compared
to the baselines, RecurD achieves consistently better performance in both tasks, especially on the
harder domain generalization task. The reason is that RecurD can learn disentangled representations
without sacriÔ¨Åcing the reconstruction performance, conÔ¨Årming the hypothesis that compositional
disentanglement learning yields better generalization and more data-efÔ¨Åcient representations. We
believe that the informative disentangled representations emerge when the right balance is achieved
between sufÔ¨Åcient information preservation and minimal sufÔ¨Åcient information learned in a disen-
tangled manner."
CONCLUSION,0.30386740331491713,"6
CONCLUSION"
CONCLUSION,0.30662983425414364,"This paper has described a solution to the compositional disentangled representation learning prob-
lem. We Ô¨Årst presented a general information-theoretic formulation of disentanglement representa-
tion learning, and then extended it to the compositional feature space. We then described RecurD,
a recursive disentanglement network, which propagates regulatory inductive bias recursively across
the compositional feature space. RecurD outperforms Œ≤-VAE and its state-of-the-art variants on
disentangled representation learning and achieves more data-efÔ¨Åcient learning in downstream tasks."
CONCLUSION,0.30939226519337015,Published as a conference paper at ICLR 2022
REFERENCES,0.31215469613259667,REFERENCES
REFERENCES,0.3149171270718232,"Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal of Machine Learning Research, 19(1):1947‚Äì1980, 2018."
REFERENCES,0.31767955801104975,"Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain gen-
eralization using meta-regularization. Advances in Neural Information Processing Systems, 31:
998‚Äì1008, 2018."
REFERENCES,0.32044198895027626,"Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference
on Machine Learning, pp. 531‚Äì540. PMLR, 2018."
REFERENCES,0.32320441988950277,"Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798‚Äì1828,
2013."
REFERENCES,0.3259668508287293,"Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018."
REFERENCES,0.3287292817679558,"Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-
jardins, and Alexander Lerchner.
Understanding disentangling in beta-vae.
arXiv preprint
arXiv:1804.03599, 2018."
REFERENCES,0.3314917127071823,"Ricky TQ Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentangle-
ment in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018."
REFERENCES,0.3342541436464088,"Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Proceedings of the 30th International Conference on Neural Information Processing Systems, pp.
2180‚Äì2188, 2016."
REFERENCES,0.3370165745856354,"Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999."
REFERENCES,0.3397790055248619,"Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees GM Snoek, and Ling Shao.
Learning to learn with variational information bottleneck for domain generalization. In European
Conference on Computer Vision, pp. 200‚Äì216. Springer, 2020."
REFERENCES,0.3425414364640884,"Yann Dubois, Douwe Kiela, David J Schwab, and Ramakrishna Vedantam. Learning optimal repre-
sentations with the decodable information bottleneck. arXiv preprint arXiv:2009.12789, 2020."
REFERENCES,0.3453038674033149,"Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disen-
tangled representations. In International Conference on Learning Representations, 2018."
REFERENCES,0.34806629834254144,"Marco Federici, Anjan Dutta, Patrick Forr¬¥e, Nate Kushman, and Zeynep Akata. Learning robust
representations via multi-view information bottleneck. arXiv preprint arXiv:2002.07017, 2020."
REFERENCES,0.35082872928176795,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efÔ¨Åcient sparsity. arXiv preprint arXiv:2101.03961, 2021."
REFERENCES,0.35359116022099446,"Sanja Fidler, Sven Dickinson, and Raquel Urtasun. 3d object detection and viewpoint estimation
with a deformable 3d cuboid model. Advances in neural information processing systems, 25:
611‚Äì619, 2012."
REFERENCES,0.356353591160221,"Aviv Gabbay and Yedid Hoshen.
Demystifying inter-class disentanglement.
arXiv preprint
arXiv:1906.11796, 2019."
REFERENCES,0.35911602209944754,"Aviv Gabbay and Yedid Hoshen. Scaling-up disentanglement for image translation. arXiv preprint
arXiv:2103.14017, 2021."
REFERENCES,0.36187845303867405,"Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generaliza-
tion for object recognition with multi-task autoencoders. In Proceedings of the IEEE international
conference on computer vision, pp. 2551‚Äì2559, 2015."
REFERENCES,0.36464088397790057,Published as a conference paper at ICLR 2022
REFERENCES,0.3674033149171271,"Irina Higgins, Lo¬®ƒ±c Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner.
beta-vae: Learning basic visual concepts with a
constrained variational framework. In 5th International Conference on Learning Representations,
ICLR 2017, 2017."
REFERENCES,0.3701657458563536,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. ICLR, 2019."
REFERENCES,0.3729281767955801,"Hyunjik Kim and Andriy Mnih.
Disentangling by factorising.
In International Conference on
Machine Learning, pp. 2649‚Äì2658. PMLR, 2018."
REFERENCES,0.3756906077348066,"Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentan-
gled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017."
REFERENCES,0.3784530386740331,"Steve Lawrence, C Lee Giles, Ah Chung Tsoi, and Andrew D Back. Face recognition: A convolu-
tional neural-network approach. IEEE transactions on neural networks, 8(1):98‚Äì113, 1997."
REFERENCES,0.3812154696132597,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE international conference on computer vision, pp.
5542‚Äì5550, 2017."
REFERENCES,0.3839779005524862,"Yanjun Li, Shujian Yu, Jose C Principe, Xiaolin Li, and Dapeng Wu. Pri-vae: principle-of-relevant-
information variational autoencoders. arXiv preprint arXiv:2007.06503, 2020."
REFERENCES,0.3867403314917127,"Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Sch¬®olkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In international conference on machine learning, pp. 4114‚Äì4124.
PMLR, 2019."
REFERENCES,0.38950276243093923,"Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R¬®atsch, Sylvain Gelly, Bernhard
Sch¬®olkopf, and Olivier Bachem. A sober look at the unsupervised learning of disentangled repre-
sentations and their evaluation. arXiv preprint arXiv:2010.14766, 2020."
REFERENCES,0.39226519337016574,"Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017."
REFERENCES,0.39502762430939226,"William McGill. Multivariate information transmission. Transactions of the IRE Professional Group
on Information Theory, 4(4):93‚Äì111, 1954."
REFERENCES,0.39779005524861877,"Xingchao Peng, Zijun Huang, Ximeng Sun, and Kate Saenko. Domain agnostic learning with dis-
entangled representations. In International Conference on Machine Learning, pp. 5102‚Äì5112.
PMLR, 2019."
REFERENCES,0.4005524861878453,"Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. Advances
in neural information processing systems, 28:1252‚Äì1260, 2015."
REFERENCES,0.40331491712707185,"Karl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the f-statistic
loss. arXiv preprint arXiv:1802.05312, 2018."
REFERENCES,0.40607734806629836,"Huajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang,
and Tarek Abdelzaher. Controlvae: Controllable variational autoencoder. In International Con-
ference on Machine Learning, pp. 8655‚Äì8664. PMLR, 2020."
REFERENCES,0.4088397790055249,"Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538, 2017."
REFERENCES,0.4116022099447514,"Casper Kaae S√∏nderby, Tapani Raiko, Lars Maal√∏e, S√∏ren Kaae S√∏nderby, and Ole Winther. Ladder
variational autoencoders. Advances in neural information processing systems, 29:3738‚Äì3746,
2016."
REFERENCES,0.4143646408839779,"Raphael Suter, Djordje Miladinovic, Bernhard Sch¬®olkopf, and Stefan Bauer. Robustly disentangled
causal mechanisms: Validating deep representations for interventional robustness. In Interna-
tional Conference on Machine Learning, pp. 6056‚Äì6065. PMLR, 2019."
REFERENCES,0.4171270718232044,Published as a conference paper at ICLR 2022
REFERENCES,0.4198895027624309,"Han Te Sun. Linear dependence structure of the entropy space. Inf Control, 29(4):337‚Äì68, 1975."
REFERENCES,0.42265193370165743,"Han Te Sun. Multiple mutual informations and multiple interactions in frequency data. Information
and Control, 46:26‚Äì45, 1980."
REFERENCES,0.425414364640884,"Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on ArtiÔ¨Åcial
Intelligence and Statistics, pp. 1214‚Äì1223. PMLR, 2018."
REFERENCES,0.4281767955801105,"Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint
arXiv:2007.03898, 2020."
REFERENCES,0.430939226519337,"Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of Re-
search and Development, 4(1):66‚Äì82, 1960."
REFERENCES,0.43370165745856354,"Liangjian Wen, Yiji Zhou, Lirong He, Mingyuan Zhou, and Zenglin Xu. Mutual information gradi-
ent estimation for representation learning. arXiv preprint arXiv:2005.01123, 2020."
REFERENCES,0.43646408839779005,"Matthew D Zeiler and Rob Fergus.
Visualizing and understanding convolutional networks.
In
European conference on computer vision, pp. 818‚Äì833. Springer, 2014."
REFERENCES,0.43922651933701656,"Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational
autoencoders. arXiv preprint arXiv:1706.02262, 2017."
REFERENCES,0.4419889502762431,"Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y. Ng, Gunnar E. Carlsson, and Stefano Ermon.
Evaluating the disentanglement of deep generative models through manifold topology. In Inter-
national Conference on Learning Representations, 2021."
REFERENCES,0.4447513812154696,"Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Joshua B Tenen-
baum, and William T Freeman. Visual object networks: Image generation with disentangled 3d
representation. arXiv preprint arXiv:1812.02725, 2018."
REFERENCES,0.44751381215469616,Published as a conference paper at ICLR 2022
REFERENCES,0.45027624309392267,"A
PROPERTY OF MARKOV CHAIN AND MUTUAL INFORMATION"
REFERENCES,0.4530386740331492,This section lists the related properties of Markov chain and mutual information used in this work.
REFERENCES,0.4558011049723757,Markov Chain
REFERENCES,0.4585635359116022,"Consider three random variables a, b and c coming from a joint distribution p(a, b, c), if the con-
ditional distribution of c depends only on a and is conditionally independent of b, the variables can
form a Markov chain in the order denoted as: b ‚Üía ‚Üíc. Specially, the joint distribution can be
written as (Cover, 1999):"
REFERENCES,0.4613259668508287,"p(a, b, c) = p(a)p(b|a)p(c|a).
(7)"
REFERENCES,0.46408839779005523,"Markovity implies a conditional independence between c and b when a is observed, the reason is:"
REFERENCES,0.46685082872928174,"p(c, b|a) = p(a, b, c)"
REFERENCES,0.4696132596685083,"p(a)
= p(c, a)p(b|a)"
REFERENCES,0.4723756906077348,"p(a)
= p(c|a)p(b|a).
(8)"
REFERENCES,0.47513812154696133,"By rewriting the conditional independence, the Markov chain of b ‚Üía ‚Üíc also implies c ‚Üía ‚Üí
b as:"
REFERENCES,0.47790055248618785,"p(b, c|a) = p(a, b, c)"
REFERENCES,0.48066298342541436,"p(a)
= p(b, a)p(c|a)"
REFERENCES,0.48342541436464087,"p(a)
= p(c|a)p(b|a).
(9)"
REFERENCES,0.4861878453038674,Mutual Information
REFERENCES,0.4889502762430939,1. Positivity: I(a; b) ‚â•0; I(a; b|c) ‚â•0.
REFERENCES,0.49171270718232046,"2. Chain Rule: I(a; b, c) = I(a; b) + I(a; c|b)"
REFERENCES,0.494475138121547,3. I(a; b) = H(a) ‚àíH(a|b) = 0 if and only if a and b are independent.
REFERENCES,0.4972375690607735,I(a; b) = H(a) ‚àíH(a | b)
REFERENCES,0.5,"= E

log
1
p(a)"
REFERENCES,0.5027624309392266,"
‚àíE

log
1
p(a|b) "
REFERENCES,0.505524861878453,"= E

log p(a|b)"
REFERENCES,0.5082872928176796,"p(a)
p(b)
p(b) "
REFERENCES,0.511049723756906,"= E

log p(a, b) p(a)b "
REFERENCES,0.5138121546961326,"= D (p(a, b)‚à•p(a)) √ó p(b))
‚â•0 (10)"
REFERENCES,0.5165745856353591,"4. Data-Processing Inequality (DPI): If three random variables a, b and c coming from a joint
distribution p(a, b, c) can form a Markov chain in the order denoted as: b ‚Üía ‚Üíc, then I(b; a) ‚â•
I(b; c). (Theorem 2.8.1 in Cover (1999))"
REFERENCES,0.5193370165745856,"B
LEARNING OBJECTIVES DECOMPOSITION"
REFERENCES,0.5220994475138122,"In this section, we decompose the proposed learning objective to analyze the relationship of learning
objectives between Equation 1 and Œ≤-VAE-based methods. Let p(x) denote the true distribution
of the data, and pœÜ(z|x) and pŒ∏(x|z) denote the unknown distributions that we need to estimate,
parametrized by an encoder with œÜ and a decoder with Œ∏."
REFERENCES,0.5248618784530387,"B.1
DECOMPOSITION OF MINIMAL SUFFICIENCY"
REFERENCES,0.5276243093922652,"Minimal SufÔ¨Åciency is deÔ¨Åned as z can encode the minimum amount information of x required to
reconstruct ÀÜx, optimized by minimize I(x; z). Inspired from the work of Chen et al. (2018), we can"
REFERENCES,0.5303867403314917,Published as a conference paper at ICLR 2022
REFERENCES,0.5331491712707183,decompose the minimal sufÔ¨Åciency by assuming the prior p(z) as a factorized Gaussian as:
REFERENCES,0.5359116022099447,"I (x; z) = Eq(x,z) [KL (q (x, z) ‚à•q (z) p (x))]"
REFERENCES,0.5386740331491713,"= Eq(x,z)"
REFERENCES,0.5414364640883977,"
log
q (x, z)
q (z) p (x)"
REFERENCES,0.5441988950276243,"
= Eq(x,z)"
REFERENCES,0.5469613259668509,"
log q (z|x) p (x)"
REFERENCES,0.5497237569060773,q (z) p (x) 
REFERENCES,0.5524861878453039,= Ep(x) Ô£Æ
REFERENCES,0.5552486187845304,Ô£∞Eq(z|x) Ô£Æ
REFERENCES,0.5580110497237569,"Ô£∞log q (z|x) ‚àílog q (z) + log p (z) ‚àílog p (z) + log
Y"
REFERENCES,0.5607734806629834,"j
q (zj) ‚àílog
Y"
REFERENCES,0.56353591160221,"j
q (zj) Ô£π Ô£ª Ô£π Ô£ª"
REFERENCES,0.5662983425414365,"= Eq(x,z)"
REFERENCES,0.569060773480663,"
log q (z|x) q (z)"
REFERENCES,0.5718232044198895,"
‚àíEq(z) Ô£Æ Ô£∞X"
REFERENCES,0.574585635359116,"j
log q (zj)"
REFERENCES,0.5773480662983426,p (zj) Ô£π
REFERENCES,0.580110497237569,"Ô£ª‚àíEq(z) """
REFERENCES,0.5828729281767956,"log
q (z)
Q"
REFERENCES,0.585635359116022,j q (zj) #
REFERENCES,0.5883977900552486,"= Ep(x) [KL (q (z|x) ‚à•p (z))] ‚àí
X"
REFERENCES,0.5911602209944752,"j
KL (q (zj) ‚à•p (zj)) ‚àíKL Ô£´"
REFERENCES,0.5939226519337016,"Ô£≠q (z) ‚à•
Y"
REFERENCES,0.5966850828729282,"j
q (zj) Ô£∂ Ô£∏ (11)"
REFERENCES,0.5994475138121547,"The Ô¨Årst term is KL divergence between inferred posterior and prior, usually as a penalty term of
Œ≤-VAEs for disentangling. The second term is the dimension-wise KL divergence, which represents
the distance between each latent dimension to the prior. The third term is the Total Correlation
referring to the independence among variables."
REFERENCES,0.6022099447513812,"B.2
ESTIMATION OF DISENTANGLEMENT"
REFERENCES,0.6049723756906077,"Disentanglement is deÔ¨Åned as the independence between any two variables, optimized by minimiz-
ing P
iÃ∏=j I(zi; zj), which is an lower bound of Total Correlation."
REFERENCES,0.6077348066298343,"The concept of Total Correlation (TC) was described by McGill (1954) and formally formulated by
Watanabe (1960) to evaluate the mutual independence of multi-variant variables. Assume a set of z
of random variables z1, . . . , zn. The TC in z is expressed as:"
REFERENCES,0.6104972375690608,"C (z1, . . . , zn) = n
X"
REFERENCES,0.6132596685082873,"i=1
H (zi) ‚àíH (z1, z2, . . . , zn) ,
(12)"
REFERENCES,0.6160220994475138,"where H (z1, z2, . . . , zn) denotes the joint entropy. Furthermore, according to the work of Te Sun
(1980; 1975), the relation between TC and mutual information can be described as:"
REFERENCES,0.6187845303867403,"C (z) = n
X"
REFERENCES,0.6215469613259669,"i=1
H (zi) ‚àíH (z) = ‚àí
X"
REFERENCES,0.6243093922651933,"n‚â•2
‚àÜH (z)
(13)"
REFERENCES,0.6270718232044199,"The general ‚àÜH (z) is deÔ¨Åned as Fano‚Äôs multiple mutual information among variables. In the case
of n = 2, it reduces to Shannon‚Äôs mutual information and in the case of n = 3, it coincides with
McGill‚Äôs mutual information. The formulation of TC can be simpliÔ¨Åed as:"
REFERENCES,0.6298342541436464,"C (z1, . . . , zn) =
X"
REFERENCES,0.6325966850828729,"jÃ∏=i
I (zi; zj) +
X"
REFERENCES,0.6353591160220995,"kÃ∏=jÃ∏=i
‚àÜH (zi; zj; zk) + . . . + ‚àÜH (z1; . . . ; zn)
(14)"
REFERENCES,0.638121546961326,"In
VAEs,
to
measure
dependence
for
multiple
latent
variables,
TC
is
computed
as
DKL

q (z) ‚à•Q"
REFERENCES,0.6408839779005525,"j q (zj)

.
Therefore, the proposed disentanglement P"
REFERENCES,0.643646408839779,"iÃ∏=j I (zi; zj) is a lower
bound of TC."
REFERENCES,0.6464088397790055,Published as a conference paper at ICLR 2022
REFERENCES,0.649171270718232,"B.3
RELATION WITH OTHER WORKS"
REFERENCES,0.6519337016574586,"After decomposing the minimal sufÔ¨Åciency and estimating the disentanglement by TC, we can com-
bine two regularizers as:"
REFERENCES,0.6546961325966851,"Œª1I (x; z) + Œª2
X"
REFERENCES,0.6574585635359116,"jÃ∏=i
I (zi; zj)"
REFERENCES,0.6602209944751382,"‚â§Œª1Ep(x) [KL (q (z|x) ‚à•p (z))] ‚àíŒª12
X"
REFERENCES,0.6629834254143646,"j
KL (q (zj) ‚à•p (zj)) ‚àíŒª13 KL Ô£´"
REFERENCES,0.6657458563535912,"Ô£≠q (z) ‚à•
Y"
REFERENCES,0.6685082872928176,"j
q (zj) Ô£∂ Ô£∏"
REFERENCES,0.6712707182320442,"+ Œª2
X"
REFERENCES,0.6740331491712708,"j
KL (q (zj) ‚à•p (zj))"
REFERENCES,0.6767955801104972,"= Œª1Ep(x) [KL (q (z|x) ‚à•p (z))] + (Œª2 ‚àíŒª12)
X"
REFERENCES,0.6795580110497238,"j
KL (q (zj) ‚à•p (zj)) ‚àíŒª13 KL Ô£´"
REFERENCES,0.6823204419889503,"Ô£≠q (z) ‚à•
Y"
REFERENCES,0.6850828729281768,"j
q (zj) Ô£∂ Ô£∏"
REFERENCES,0.6878453038674033,"= ŒªaEp(x) [KL (q (z|x) ‚à•p (z))] + Œªb
X"
REFERENCES,0.6906077348066298,"j
KL (q (zj) ‚à•p (zj)) + Œªc KL Ô£´"
REFERENCES,0.6933701657458563,"Ô£≠q (z) ‚à•
Y"
REFERENCES,0.6961325966850829,"j
q (zj) Ô£∂ Ô£∏"
REFERENCES,0.6988950276243094,"(15)
Therefore, by assigning different weights to the decomposed term, we can establish the relationship
between the proposed information-theoretic objective and existing Œ≤-VAE variants, which will pro-
vide insights regarding the capabilities and limitations of existing methods, and further motivate the
proposed work."
REFERENCES,0.7016574585635359,"Specially,
the
objective
function
of
Œ≤-TCVAE
is
I (x, z) + KL

q (z) ‚à•Q"
REFERENCES,0.7044198895027625,"j q (zj)

+
P"
REFERENCES,0.7071823204419889,j KL (q (zj) ‚à•p (zj)). If we decompose the I (x; z) of Œ≤-TCVAE as: KL (q (z|x) ‚à•p (z)) ‚àí
REFERENCES,0.7099447513812155,"KL

q (z) ‚à•Q
j q (zj)

‚àíP
j KL (q (zj) ‚à•p (zj)), the regularizer term of Œ≤-TCVAE can be writ-"
REFERENCES,0.712707182320442,"ten as: Œ±KL (q (z|x) ‚à•p (z))+(Œ≤ ‚àíŒ±) KL

q (z) ‚à•Q"
REFERENCES,0.7154696132596685,"j q (zj)

+(Œ≥ ‚àíŒ±) P"
REFERENCES,0.7182320441988951,"j KL (q (zj) ‚à•p (zj)).
Since Œ± = Œ≥ = 1 in Œ≤-TCVAE, therefore when Œªc = 0, the last line of Equation 15 is equivalent to
Œ≤-TCVAE."
REFERENCES,0.7209944751381215,"C
PROOF OF COMPOSITIONAL MINIMAL SUFFICIENCY"
REFERENCES,0.7237569060773481,"In this section, we prove the statements reported in the section 2.2 of the paper."
REFERENCES,0.7265193370165746,Property:
REFERENCES,0.7292817679558011,"(P1): Data-processing inequality in the markov chain: considering the markov chain b ‚Üía ‚Üíc,
then I (b, a) ‚â•I (b; c). (Theorem 2.8.1 (Cover, 1999).)"
REFERENCES,0.7320441988950276,"(P2): Chain rule for mutual information: I (a; b, c) = I (a; b)+I (a; c|b). (Theorem 2.5.1 (Cover,
1999).)"
REFERENCES,0.7348066298342542,"(P3): Decompositon of conditional mutual information. (Proposition B.1. (Federici et al., 2020).)"
REFERENCES,0.7375690607734806,Conditional independence assumptions:
REFERENCES,0.7403314917127072,"The Markov Chain ml+1
j
‚Üíx ‚Üí
 
ml
i, ml
j

, implies the conditional independence between ml+1
j
and
 
ml
i, ml
j

when x is observed."
REFERENCES,0.7430939226519337,Proof:
REFERENCES,0.7458563535911602,"I
 
x; ml+1
j
 (P1)
‚â•I
 
ml+1
j
; ml
i, ml
j
"
REFERENCES,0.7486187845303868,"(P2)
= I
 
ml+1
j
; ml
j

+ I
 
ml+1
j
; ml
i | ml
j
"
REFERENCES,0.7513812154696132,"(P3)
= I
 
ml+1
j
; ml
j

+ I
 
ml+1
j
; ml
i

‚àíI
 
ml
i; ml
j

. (16)"
REFERENCES,0.7541436464088398,Published as a conference paper at ICLR 2022
REFERENCES,0.7569060773480663,"D
EXPERIMENTAL DETAILS"
REFERENCES,0.7596685082872928,"For all baselines, we use a Convolutional Neural Network for the encoder, and a Deconvolutional
Neural Network for the decoder. Specially, for Factor-VAE, we use a 6-layer Multi-Layer Perceptron
for the discriminator, with the leaky ReLU as the activation on per layer, and we set Œ≥ as 6.4. For
Œ≤-VAE, we use a set of Œ≤ = {4, 16, 60}. Annealed-VAE uses Œ≥ = 1000 with a linearly increasing
C from 0.5 nats to a 25.0 nats). Œ≤-TCVAE uses Œ± = Œ≥ = 1 and Œ≤ = 4. DIP-VAE is implemented
by the parameters Œªod = 100 and Œªd = 10."
REFERENCES,0.7624309392265194,"For RecurD, we implement RecurD 0 as the same encoder/decoder architecture with all baselines,
as shown in Table 3. As for RecurD 1 and RecurD 2, we implement the same decoder architec-
ture with all baselines, and the details about the encoder of RecurD 1 and RecurD 2 are shown
in Table 4 and Table 5. Following the previous work, we use negative cross-entropy to compute
reconstruction error, which represents sufÔ¨Åciency in our work. As for the computation of minimal
sufÔ¨Åciency and disentanglement, we implement the MINE estimator by 4-layer Multi-Layer Percep-
tron, similar with the work of Federici et al. (2020). As for the hyperparameter in our model, we vary
Œª1 and Œª2 in the set {0.1, 0.2, 0.5, 1, 2, 5, 10, 50} while Ô¨Åxing Œª1 = 1 and Œª1 = 2 for both datasets.
During training, we use Adam optimiser with learning rate 1e ‚àí4, Œ≤1 = 0.9, Œ≤2 = 0.999 for pa-
rameter updates. Specially, we utilize RecurD 1 on dSprites, 3DCars and 3DShapes, and RecurD 2
on CelebA."
REFERENCES,0.7651933701657458,Table 3: Encoder and Decoder architecture for all baselines and RecurD 0.
REFERENCES,0.7679558011049724,"Input
dSprites (3DShapes): 64 √ó 64√ó 1(3) Images"
REFERENCES,0.7707182320441989,Encoder
REFERENCES,0.7734806629834254,"Conv: k=4, s=2, p=1, channel=32, ReLU
Conv: k=4, s=2, p=1, channel=32, ReLU
Conv: k=4, s=2, p=1, channel=64, ReLU
Conv: k=4, s=2, p=1, channel=64, ReLU
FC: 128 (256)
FC: 2 √ó 8"
REFERENCES,0.7762430939226519,Decoder
REFERENCES,0.7790055248618785,Input: z ‚ààR8
REFERENCES,0.7817679558011049,"FC: 128, ReLU
FC: 4 √ó 4 √ó 64, ReLU
Deconv: k=4, s=2, p=1, channel=64, ReLU
Deconv: k=4, s=2, p=1, channel=32, ReLU
Deconv: k=4, s=2, p=1, channel=32, ReLU
Deconv: k=4, s=2, p=1, channel=1(3), ReLU
Output:
64 √ó 64√ó 1 (3) Reconst. Images"
REFERENCES,0.7845303867403315,"Table 4: Encoder architecture of RecurD 1.
Input
64 √ó 64 √ó 1 (3) Image"
-TH RECURSIVE MODULE,0.787292817679558,0-th Recursive Module
-TH RECURSIVE MODULE,0.7900552486187845,"Router(v)
-"
-TH RECURSIVE MODULE,0.7928176795580111,"GoE
4√ó [Conv: k=4, s=2, p=1, channel=32, ReLU]
Conv: k=4, s=2, p=1, channel=64, ReLU"
-TH RECURSIVE MODULE,0.7955801104972375,"1-th Recursive Module
Router(v)
FC, 8, ReLU"
-TH RECURSIVE MODULE,0.7983425414364641,"GoE
8 √ó
Conv : k=4, s=2, p=1, channel=16, ReLU
FC, 16; FC, 2 √ó 1 "
-TH RECURSIVE MODULE,0.8011049723756906,Published as a conference paper at ICLR 2022
-TH RECURSIVE MODULE,0.8038674033149171,"Table 5: Encoder architecture of RecurD 2.
Input
64 √ó 64√ó1 (3) Image"
-TH RECURSIVE MODULE,0.8066298342541437,"0-th Recursive Module
Router(v)
-
GoE
2√ó [Conv: k=4, s=2, p=1, channel=32, ReLU ]"
-TH RECURSIVE MODULE,0.8093922651933702,"1-th Recursive Module
Router(v)
FC, 4, ReLU
GoE
4 √ó [Conv: k=4, s=2, p=1, channel=16, ReLU]"
-TH RECURSIVE MODULE,0.8121546961325967,"2-th Recursive Module
Router(v)
FC, 16, ReLU"
-TH RECURSIVE MODULE,0.8149171270718232,"GoE
16 √ó
Conv : k=4, s=2, p=1, channel=16, ReLU
FC, 16; FC, 2 √ó 1 "
-TH RECURSIVE MODULE,0.8176795580110497,"E
ADDITIONAL EXPERIMENTS"
-TH RECURSIVE MODULE,0.8204419889502762,"This section reports additional experiments, which draw similar conclusions as that of Section 5."
-TH RECURSIVE MODULE,0.8232044198895028,"E.1
QUANTITATIVE RESULTS"
-TH RECURSIVE MODULE,0.8259668508287292,"The Ô¨Årst set of quantitative results is the supplementary results of disentanglement score DCI-C and
DCI-I on dSprites and 3DShapes. Table 6, Figure 8 and Figure 9 show the results of disentanglement
score with reconstruction error, which also show RecurD can achieve a better trade-off between
reconstruction and disentanglement."
-TH RECURSIVE MODULE,0.8287292817679558,"Figure 8: Reconstruction error vs. disentanglement performance on dSprites. Scatters located at the
left top indicate better performance."
-TH RECURSIVE MODULE,0.8314917127071824,"Figure 9: Reconstruction error vs. disentanglement performance on 3DShapes. Scatters located at
the left top indicate better performance."
-TH RECURSIVE MODULE,0.8342541436464088,"The second set is the performance comparison results with three additional baselines, i.e, Info-
GAN (Chen et al., 2016), Control-VAE and NVAE. InfoGAN (Chen et al., 2016) maximizes the
mutual information between the small subset of the latent variables and the observations to increase
the interpretability of the latent representation. Control-VAE (Shao et al., 2020) dynamically tunes"
-TH RECURSIVE MODULE,0.8370165745856354,Published as a conference paper at ICLR 2022
-TH RECURSIVE MODULE,0.8397790055248618,"Table 6: DCI-C, DCI-I scores and reconstruction error on the test sets for dSprites and 3DShapes.
Bold face indicates the best results, i.e., reconstruction error and disentanglement scores."
-TH RECURSIVE MODULE,0.8425414364640884,"dataset
dSprites
3DShapes"
-TH RECURSIVE MODULE,0.8453038674033149,"Reconst error
DCI-C
DCI-I
Reconst error
DCI-C
DCI-I"
-TH RECURSIVE MODULE,0.8480662983425414,"Œ≤-VAE(Œ≤ = 4)
0.0066
0.1148
0.2181
0.0216
0.4463
0.1982
Œ≤-VAE(Œ≤ = 16)
0.0094
0.1575
0.2087
0.0544
0.4691
0.1828
Œ≤-VAE(Œ≤ = 60)
0.0127
0.1261
0.1769
0.0624
0.4215
0.1641
Annealed-VAE
0.0171
0.1793
0.1663
0.0811
0.4419
0.1682
Factor-VAE
0.0228
0.1375
0.1699
0.0800
0.4419
0.1707
Œ≤-TCVAE
0.0162
0.1988
0.1660
0.0312
0.4516
0.1774
DIP-VAE
0.0213
0.0968
0.1496
0.0213
0.4621
0.1961"
-TH RECURSIVE MODULE,0.850828729281768,"RecurD
0.0047
0.3835
0.2222
0.0083
0.5668
0.2014"
-TH RECURSIVE MODULE,0.8535911602209945,"the weight Œ≤ on the KL term to achieve a good trade-off between disentanglement and reconstruc-
tion quality. NVAE (Vahdat & Kautz, 2020) optimizes high-quality image generation via global
correlation capturing across multi-layer latent variables."
-TH RECURSIVE MODULE,0.856353591160221,"We compare with Info-GAN Control-VAE and NVAE on three datasets, including dSprites,
3Dshapes and 3Dcars. The 3DCars (Reed et al., 2015) exhibits different car models from Fi-
dler et al. (2012) under different camera viewpoints. The evaluation method is the disentanglement
score MIG. Table 7 show that NVAE and VampPrior can achieve comparable reconstruction error
compared to RecurD. However, their disentanglement qualities are not as good as RecurD, because
RecurD additionally regularizes the inter-layer information sharing and alleviates the information re-
dundancy of multiple layers. This experiment demonstrates the superiority of the proposed RecurD
method in disentanglement compared to existing hierarchical VAEs."
-TH RECURSIVE MODULE,0.8591160220994475,"Table 7: MIG score and reconstruction error on the test sets for dSprites, 3DShapes and 3DCars.
Bold face indicates the best results, i.e., reconstruction error and disentanglement scores."
-TH RECURSIVE MODULE,0.861878453038674,"dataset
dSprites
3DShapes
3DCars"
-TH RECURSIVE MODULE,0.8646408839779005,"Reconst. Error
MIG
Reconst. Error
MIG
Reconst. Error
MIG"
-TH RECURSIVE MODULE,0.8674033149171271,"Œ≤-VAE(Œ≤ = 4)
0.0066
0.2617
0.0216
0.2519
0.0376
0.1015
InfoGAN
-
0.1598
-
0.1874
-
0.1083
Control-VAE
0.0102
0.2455
0.0357
0.2630
0.0257
0.1583
NVAE
0.0041
0.0043
0.0078
0.0081
0.0118
0.0034"
-TH RECURSIVE MODULE,0.8701657458563536,"RecurD
0.0047
0.2707
0.0083
0.3105
0.0132
0.1762"
-TH RECURSIVE MODULE,0.8729281767955801,"E.2
ABLATION STUDY"
-TH RECURSIVE MODULE,0.8756906077348067,"In this section, we report the supplementary results of ablation study. The Ô¨Årst is to study the impact
of compositional learning objective with varying regularization coefÔ¨Åcients (Œª1 and Œª2) of minimal
sufÔ¨Åciency and disentanglement. Figure E.2 and Figure E.2 show the scatter plots of disentan-
glement scores along with reconstruction error on the test sets of dSprites, with varying Œª1, Œª2.
Figure 12 and Figure 13 show the results on the 3DShapes."
-TH RECURSIVE MODULE,0.8784530386740331,"The second set is to evaluate the inÔ¨Çuence of hyperparameter k ‚Äî the group size in the GoE of
Recursive Module. Figure E.2 and Figure E.2 show the inÔ¨Çuence of varying k for disentanglement
scores and reconstruction error on the test sets of dSprites. Figure E.2 and Figure E.2 show the
results on the 3DShapes."
-TH RECURSIVE MODULE,0.8812154696132597,"The third set is to evaluate the disentanglement metrics on the preceding layers. We compute MIG
on mL‚àí1 and mL‚àí2 of RecurD on 3DShapes. As shown in Table 8, high-level representations have
higher MIG than that of low-level representations (z > mL‚àí1 > mL‚àí2). These results conÔ¨Årm that"
-TH RECURSIVE MODULE,0.8839779005524862,Published as a conference paper at ICLR 2022
-TH RECURSIVE MODULE,0.8867403314917127,"the recursive propagation of inductive bias through the feed-forward network improves disentangled
representation learning."
-TH RECURSIVE MODULE,0.8895027624309392,"The last set is to evaluate the impact of architecture of Gate-of-Encoders (GoE), we conduct three
variant GoEs, including Linear: GoE is implemented as a linear layer with softmax; Fix: GoE is
implemented as Ô¨Åx assignment, i.e., we split ml in d + 1 equal slices; and Att: GoE is implemented
as a multi-head attention layer (the head is Ô¨Åxed as 8). As shown in Table 9, this study demonstrates
that GoE indeed Ô¨Åts well with the disentanglement process."
-TH RECURSIVE MODULE,0.8922651933701657,Figure 10: Ablation study on Œª1 and Œª2 on dSprites.
-TH RECURSIVE MODULE,0.8950276243093923,Figure 11: Ablation study on Œª1 and Œª2 on dSprites.
-TH RECURSIVE MODULE,0.8977900552486188,"Table 8: Comparison of MIG on z, mL‚àí1 and mL‚àí2."
-TH RECURSIVE MODULE,0.9005524861878453,"mL‚àí2
mL‚àí1
z(mL)
Reconst. Error"
-TH RECURSIVE MODULE,0.9033149171270718,"MIG
0.1081
0.2892
0.3105
0.0083"
-TH RECURSIVE MODULE,0.9060773480662984,Published as a conference paper at ICLR 2022
-TH RECURSIVE MODULE,0.9088397790055248,Figure 12: Ablation study on Œª1 and Œª2 on 3DShapes.
-TH RECURSIVE MODULE,0.9116022099447514,Figure 13: Ablation study on Œª1 and Œª2 on 3DShapes.
-TH RECURSIVE MODULE,0.914364640883978,Figure 14: Ablation study on k on dSprites.
-TH RECURSIVE MODULE,0.9171270718232044,Published as a conference paper at ICLR 2022
-TH RECURSIVE MODULE,0.919889502762431,Figure 15: Ablation study on k on dSprites.
-TH RECURSIVE MODULE,0.9226519337016574,Figure 16: Ablation study on k on 3DShapes.
-TH RECURSIVE MODULE,0.925414364640884,Figure 17: Ablation study on k on 3DShapes.
-TH RECURSIVE MODULE,0.9281767955801105,Published as a conference paper at ICLR 2022
-TH RECURSIVE MODULE,0.930939226519337,Table 9: Ablation study on the GoE architecture.
-TH RECURSIVE MODULE,0.9337016574585635,"Linear
Fix
Att
RecurD"
-TH RECURSIVE MODULE,0.93646408839779,"MIG
0.2539
0.2617
0.2604
0.3105"
-TH RECURSIVE MODULE,0.9392265193370166,"Reconst. Error
0.0176
0.0116
0.0095
0.0083"
-TH RECURSIVE MODULE,0.9419889502762431,"E.3
QUALITATIVE RESULTS"
-TH RECURSIVE MODULE,0.9447513812154696,"In this section, we report the qualitative samples of traversal images on three datasets, including
dSprites(Figure 18), 3DShapes(Figure 19) and CelebA(from Figure 20 to Figure 25)."
-TH RECURSIVE MODULE,0.9475138121546961,"SpeciÔ¨Åcally, on CelebA, we tentatively increase the dimensionality of latent variables of RecurD
(from 16 to 32) by doubling the output of the last layer of each encoder from a single latent variable
to a 2-dimensional variable."
-TH RECURSIVE MODULE,0.9502762430939227,Figure 18: Traversal samples on dSprites.
-TH RECURSIVE MODULE,0.9530386740331491,Figure 19: Traversal samples on 3DShapes.
-TH RECURSIVE MODULE,0.9558011049723757,"Figure 20:
Traversal samples on CelebA
(Azimuth)."
-TH RECURSIVE MODULE,0.9585635359116023,"Figure 21:
Traversal samples on CelebA
(Background Color)."
-TH RECURSIVE MODULE,0.9613259668508287,Published as a conference paper at ICLR 2022
-TH RECURSIVE MODULE,0.9640883977900553,"Figure 22:
Traversal samples on CelebA
(Face Color)."
-TH RECURSIVE MODULE,0.9668508287292817,"Figure 23:
Traversal samples on CelebA
(Face Width)."
-TH RECURSIVE MODULE,0.9696132596685083,"Figure 24:
Traversal samples on CelebA
(Gender)."
-TH RECURSIVE MODULE,0.9723756906077348,"Figure 25:
Traversal samples on CelebA
(Smile)."
-TH RECURSIVE MODULE,0.9751381215469613,"E.4
EVALUATION OF COMPUTATIONAL COSTS"
-TH RECURSIVE MODULE,0.9779005524861878,"In this section, we evaluate the computational complexity of our model on 3DShapes and CelebA.
The evaluation metrics consist of multiply-accumulate operation (MACs), the model parameters
(Params), evaluation time (Eva. time) and converge time (all models converge to the same recon-
struction error as Œ≤-VAE, denoted as Con. time)."
-TH RECURSIVE MODULE,0.9806629834254144,"In terms of parameter efÔ¨Åciency, as shown in Table 10, the recursive disentanglement network itself
only contains 0.826 million parameters (RecurD 2 w/o MINEs), which are comparable to Beta-VAE.
Most of the parameters of RecurD 2 are contributed by using MINE estimator. SpeciÔ¨Åcally, in the
initial implementation (RecurD 2/speciÔ¨Åc MINEs), each pair of outputs of encoders is equipped
with a speciÔ¨Åc MINE model, which contributes 7.214 million parameters. We further optimize the
design by using shared MINE within the same feature category, which can effectively reduce the
total number of parameters down to 1.325 million (RecurD 2/shared MINEs)."
-TH RECURSIVE MODULE,0.9834254143646409,Table 10: Complexity comparison of three models on 3DShapes and CelebA.
-TH RECURSIVE MODULE,0.9861878453038674,"Dataset
Method
MACs(G)
Params(M)
Eva. time(s)
Con. time(s)"
DSHAPES,0.988950276243094,3DShapes
DSHAPES,0.9917127071823204,"RecurD 1
3.466
3.672
0.005124
27.1212
RecurD 2
3.469
3.694
0.005354
23.3197
beta-VAE
3.144
0.769
0.003283
32.5251
Factor-VAE
3.401
4.779
0.003389
40.6231"
DSHAPES,0.994475138121547,CelebA
DSHAPES,0.9972375690607734,"RecurD 1
3.944
7.935
0.01065
29.7421
RecurD 2
3.957
8.040
0.01087
36.6408
RecurD 2 w/o MINEs
2.960
0.826
-
-
RecurD 2/shared MINEs
3.654
1.325
0.01072
36.6402
beta-VAE
3.145
0.769
0.01030
45.2067
Factor-VAE
3.402
4.792
0.01005
48.9939"
