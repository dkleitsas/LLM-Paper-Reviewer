Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0026041666666666665,"Posterior predictive distributions quantify uncertainties ignored by point
estimates. This paper introduces The Neural Testbed, which provides tools
for the systematic evaluation of agents that generate such predictions. Cru-
cially, these tools assess not only the quality of marginal predictions per in-
put, but also joint predictions given many inputs. Joint distributions are of-
ten critical for useful uncertainty quantiﬁcation, but they have been largely
overlooked by the Bayesian deep learning community. We benchmark sev-
eral approaches to uncertainty estimation using a neural-network-based
data generating process. Our results reveal the importance of evaluation
beyond marginal predictions. Further, they reconcile sources of confusion
in the ﬁeld, such as why Bayesian deep learning approaches that generate
accurate marginal predictions perform poorly in sequential decision tasks,
how incorporating priors can be helpful, and what roles epistemic versus
aleatoric uncertainty play when evaluating performance. We also present
experiments on real-world challenge datasets, which show a high correlation
with testbed results, and that the importance of evaluating joint predictive
distributions carries over to real data. As part of this eﬀort, we opensource
The Neural Testbed, including all implementations from this paper."
INTRODUCTION,0.005208333333333333,"1
Introduction"
INTRODUCTION,0.0078125,"Deep learning has emerged as the state-of-the-art approach across a number of application
domains in which agents learn from large amounts of data (LeCun et al., 2015). Neural
networks are increasingly used not only to predict outcomes but also to inform decisions.
Common approaches in deep learning produce point estimates but not uncertainty estimates,
which are often required for eﬀective decision-making. Bayesian deep learning extends the
methodology to produce such uncertainty estimates (MacKay, 1992; Neal, 2012)."
INTRODUCTION,0.010416666666666666,"We consider agents that are trained on data pairs ((Xt, Yt+1) : t = 0, 1, . . . , T −1) and
subsequently generate predictions given new inputs. When presented with an input XT , a
Bayesian neural network can generate a predictive distribution of the outcome YT +1 that is
yet to be observed. This distribution characterizes the agent’s uncertainty about YT +1. We
refer to such a prediction as marginal to distinguish it from a joint predictive distribution over
a list (YT +1, . . . , YT +τ) of prospective outcomes corresponding to inputs (XT , . . . , XT +τ−1)."
INTRODUCTION,0.013020833333333334,"The importance of uncertainty estimation has motivated a great deal of research over recent
years (Kendall & Gal, 2017). This research has produced a variety of agents that learn to
generate predictive distributions. With this proliferation of alternatives, it is increasingly
important to analyze and compare their performance (Filos et al., 2019; Nado et al., 2021).
In this paper, we introduce new tools for systematic evaluation of such agents."
INTRODUCTION,0.015625,"Our tools overcome several limitations faced by previous methods of evaluation. First, by
focusing purely on predictive distributions, we allow for a uniﬁed treatment of approaches
developed within the Bayesian neural network community and beyond. This sidesteps the"
INTRODUCTION,0.018229166666666668,Open source code available at https://anonymous.4open.science/r/neural-testbed-B839.
INTRODUCTION,0.020833333333333332,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0234375,"question of whether any approach ‘is really Bayesian’ (Wilson & Izmailov, 2020). Second, our
tools evaluate the quality of higher-order joint predictions (τ > 1). Until now, the Bayesian
deep learning literature has focused almost exclusively on evaluating marginal predictions
(Wang et al., 2021). Finally, we develop a neural-network-based data generating process
for Bayesian deep learning that can be used to drive insight and algorithm development.
Where research has focused on a small set of challenge datasets, this might introduce bias
through overﬁtting via multiple iterations of algorithm development. We use these tools to
compare hundreds of agent variants. Further, we show that performance on our synthetic
data generating process data is highly correlated with performance on real-world challenge
datasets. We opensource all code used in this paper as The Neural Testbed."
INTRODUCTION,0.026041666666666668,"Our results reconcile several sources of confusion in the ﬁeld. One concerns why particular
approaches developed by the Bayesian deep learning community, such as Bayes-by-backprop,
dropout, and deep ensembles, perform poorly in sequential decision tasks despite faring
well based on evaluation metrics of that community (Osband et al., 2018).
Our results
demonstrate that, while such methods produce accurate marginal predictions, they are no
longer competitive when it comes to high-order joint predictions. Joint predictions play a
critical role in sequential decision-making (Lu et al., 2021)."
INTRODUCTION,0.028645833333333332,"Another puzzling issue is that state-of-the-art methods do not employ domain-speciﬁc priors.
Whether Bayesian deep learning approaches should at all is a subject of controversy (Wenzel
et al., 2020). We show that the beneﬁts of domain-speciﬁc priors can be pronounced when
evaluating high-order joint predictions, even where they are negligible for marginals."
INTRODUCTION,0.03125,"We also help to resolve a point of philosophical debate within the deep learning community:
the importance of epistemic versus aleatoric uncertainty1. The strangeness of this distinc-
tion has even made its way into wider popular culture, as satirized in the XKCD comic of
Figure 1 (Munroe, 2021). For a given parametric model, we can clearly distinguish param-
eter uncertainty from noise, or reducible from irreducible uncertainty. However, from the
perspective of a learning agent, the choice of model is subjective; diﬀerent models can lead
to the same marginal predictions. Our formulation provides a clear and objective way to
assess the quality of predictive distributions, without reliance on this subjective distinction
between knowledge and chance. Crucially, we show that this can be judged via the quality
of joint predictions, but that marginals are not suﬃcient."
INTRODUCTION,0.033854166666666664,Figure 1: Epistemic or aleatoric? Does it matter?
INTRODUCTION,0.036458333333333336,"It is worth mentioning another notable contribution of this work. The quality of a predictive
distribution is commonly assessed in terms of cross-entropy loss. While this measure is well-
deﬁned for both marginal and joint predictions, to the best of our knowledge, the literature
has only addressed computation in the former case. For high-order joint predictions, the
straightforward approach would require computing sums over exponentially many values.
To render this computationally tractable, we developed a novel approximation algorithm
that leverages a random partitioning operation and Monte Carlo simulation. While this
approach is motivated by concepts from high-dimensional geometry (Kaski, 1998; Donoho,
2006), we leave its analysis as a topic for future theoretical research."
INTRODUCTION,0.0390625,"1Epistemic uncertainty relates to knowledge (ancient Greek episteme↔knowledge), as opposed
to aleatoric uncertainty relating to chance (Latin alea↔dice) (Der Kiureghian & Ditlevsen, 2009)."
INTRODUCTION,0.041666666666666664,Under review as a conference paper at ICLR 2022
EVALUATING PREDICTIVE DISTRIBUTIONS,0.044270833333333336,"2
Evaluating predictive distributions"
EVALUATING PREDICTIVE DISTRIBUTIONS,0.046875,"In this section, we introduce notation for the standard supervised learning framework we
will consider (classiﬁcation) as well as our evaluation metric (the KL-loss). We also explain
how we estimate the KL-loss for high-order joint predictions where exact computation is
infeasible, using random partitions and Monte Carlo simulation."
EVALUATING PREDICTIVE DISTRIBUTIONS,0.049479166666666664,"2.1
Kullback–Leibler loss"
EVALUATING PREDICTIVE DISTRIBUTIONS,0.052083333333333336,"Consider a sequence of pairs ((Xt, Yt+1) : t = 0, 1, 2, . . .), where each Xt is a feature vector
and each Yt+1 is its target label. This sequence is i.i.d. conditioned on the environment
E, which produces the data, and which we view as a latent random variable. We consider
an agent that is uncertain about the environment and predicts class labels YT +1:T +τ ≡
(YT +1, . . . , YT +τ) given training data pairs DT ≡((Xt, Yt+1) : t = 0, 1, 2, . . . , T −1) and
unlabelled feature vectors XT :T +τ−1 ≡(XT , . . . , XT +τ−1). From the agent’s perspective,
each feature vector Xt is generated i.i.d from a ﬁxed distribution P(Xt ∈·), and each class
label Yt+1 is then drawn from P(Yt+1 ∈·|E, Xt)."
EVALUATING PREDICTIVE DISTRIBUTIONS,0.0546875,"We describe the agent’s predictions in terms of a generative model, parameterized by a
vector θT that the agent learns from the training data DT . For any inputs XT :T +τ−1, θT
determines a predictive distribution, which could be used to sample imagined outcomes
ˆYT +1:T +τ. We deﬁne the τ th-order expected KL-loss by"
EVALUATING PREDICTIVE DISTRIBUTIONS,0.057291666666666664,"dτ
KL = E

dKL
 
P (YT +1:T +τ ∈·|E, XT :T +τ−1)
|
{z
}
environment likelihood"
EVALUATING PREDICTIVE DISTRIBUTIONS,0.059895833333333336,"P( ˆYT +1:T +τ ∈·|θT , XT :T +τ−1)
|
{z
}
agent likelihood"
EVALUATING PREDICTIVE DISTRIBUTIONS,0.0625,"
(1)"
EVALUATING PREDICTIVE DISTRIBUTIONS,0.06510416666666667,"= −E
h
log

P

ˆYT +1:T +τ = YT +1:T +τ
θT , XT :T +τ−1, YT +1:T +τ
i"
EVALUATING PREDICTIVE DISTRIBUTIONS,0.06770833333333333,"|
{z
}
cross-entropy loss ≡negative log-likelihood + C,"
EVALUATING PREDICTIVE DISTRIBUTIONS,0.0703125,"where C = E [log (P (YT +1:T +τ|E, XT :T +τ−1))] is independent of θT . The expectation is
taken over all random variables, including the environment E, the parameters θT , XT :T +τ−1,
and YT +1:T +τ. Note that dτ
KL is equivalent to the widely used notion of cross-entropy loss,
though oﬀset by a quantity that is independent of θT (Kullback & Leibler, 1951). For τ > 1,
dτ
KL assesses joint rather than the marginal predictions."
MARGINAL VERSUS JOINT PREDICTIONS,0.07291666666666667,"2.2
Marginal Versus Joint Predictions"
MARGINAL VERSUS JOINT PREDICTIONS,0.07552083333333333,"Evaluating an agent’s ability to estimate uncertainty on joint instead of marginal predictions
can result in very diﬀerent answers. We provide a simple example that illustrates the point.
Suppose the data ((Xt, Yt+1) : t = 0, 1, 2, . . .) is generated by repeated tosses of a possibly
biased coin with unknown probability p of heads.2 Let Xt = 0, to indicate that there is no
input, and let each outcome Yt+1 be 0 or 1 to indicate tails or heads, respectively. Consider
two agents that, without any training, predict outcomes. Agent 1 assumes p = 2/3 and
models the outcome of each ﬂip as pure chance. Agent 2 assumes that the coin is fully
biased, meaning that p ∈{0, 1}, but assigns probabilities 1/3 and 2/3 to 0 and 1."
MARGINAL VERSUS JOINT PREDICTIONS,0.078125,"Let ˆY 1
t+1 and ˆY 2
t+1 denote the outcomes imagined by the two agents. Despite their diﬀering
assumptions, the two agents generate identical marginal predictive distributions: P( ˆY 1
t+1 =
0) = P( ˆY 2
t+1 = 0) = 1/3. On the other hand, joint predictions greatly diﬀer for large τ:"
MARGINAL VERSUS JOINT PREDICTIONS,0.08072916666666667,"P( ˆY 1
1 = 0, .., ˆY 1
τ = 0) = 1/3τ ≪1/3 = P( ˆY 2
1 = 0, . . . , ˆY 2
τ = 0)."
MARGINAL VERSUS JOINT PREDICTIONS,0.08333333333333333,"We can say that agent 1 attributes all uncertainty to aleatoric sources and agent 2, epistemic
sources (although as Figure 1 alludes, there are many ways an agent can attribute sources
of uncertainty). Evaluating marginal predictions cannot distinguish between the two pos-
sibilities, though for a speciﬁc prior distribution over p, one agent could be right and the
other wrong. One must evaluate joint predictions to make this distinction."
MARGINAL VERSUS JOINT PREDICTIONS,0.0859375,"2We consider this coin as an illustrative model of more complex binary outcomes, such as whether
a user will click on an ad, or whether a given mortgage will default on payments."
MARGINAL VERSUS JOINT PREDICTIONS,0.08854166666666667,Under review as a conference paper at ICLR 2022
MARGINAL VERSUS JOINT PREDICTIONS,0.09114583333333333,"When it comes to decision-making, this distinction can be critical (Lu et al., 2021). In a
casino, under the ﬁrst agent’s assumption, there is large upside and little risk on repeatedly
betting on heads in the long run. However, if there is a 1/3 chance the coin will always land
tails, as is the case in the second agent’s prediction, there is a ruinous risk to repeatedly
betting heads. Evaluating joint predictions beyond marginals distinguishes these cases."
MARGINAL VERSUS JOINT PREDICTIONS,0.09375,"2.3
Computation of Kullback–Leibler loss"
MARGINAL VERSUS JOINT PREDICTIONS,0.09635416666666667,"In contexts we will consider, it is not possible to compute dτ
KL exactly. As such, we will
approximate dτ
KL via Monte Carlo simulation. This section provides a high level overview of
our approach, we push the full details to Appendix A. Algorithm 1 outlines a basic approach
to estimating dτ
KL with respect to a synthetic data generating process.
The algorithm
samples a set of environments and a training dataset for each environment. For each of
these pairs, the agent is re-initialized, trained, and then tested on N independent test data
τ-samples. Note that each test data τ-sample includes τ data pairs. For each test data
τ-sample, the likelihood of the environment is computed exactly, but that of the agent’s
belief distribution is approximated. The estimate of dτ
KL is taken to be the sample mean of
the log-likelihood-ratios (Algorithm 2)."
MARGINAL VERSUS JOINT PREDICTIONS,0.09895833333333333,Algorithm 1 KL-Loss Computation
MARGINAL VERSUS JOINT PREDICTIONS,0.1015625,"1: for j = 1, 2, . . . , J do
2:
sample environment and training dataset, and train agent
3:
for n = 1, 2, . . . , N do
4:
sample a test data τ-sample with τ feature-label pairs
5:
compute pj,n
▷likelihood of environment
6:
compute ˆpj,n
▷estimated likelihood of agent’s belief distribution"
MARGINAL VERSUS JOINT PREDICTIONS,0.10416666666666667,"7: return
1
JN
PJ
j=1
PN
n=1 log (pj,n/ˆpj,n)
▷estimated log-likelihood-ratio"
MARGINAL VERSUS JOINT PREDICTIONS,0.10677083333333333,"While the likelihood of an environment can be eﬃciently computed, that of an agent’s belief
distribution poses a computational challenge. One approach is to estimate this likelihood
via Monte Carlo simulation (Algorithm 3). This produces unbiased estimates, which can be
accurate when τ is small. However, maintaining accuracy requires the number of samples
to grow exponentially with τ, as discussed in Appendix A.1. To overcome this challenge, we
propose a novel approach that estimates the likelihood of the agent’s beliefs via a combina-
tion of randomized partitioning and Monte Carlo simulation (Algorithm 4) (Kaski, 1998).
We conjecture that, under suitable regularity conditions, this novel approach produces accu-
rate estimates even when τ is large, but leave a formal analysis to future work. Even though
Algorithm 1 is developed for a synthetic data generating process, it is straightforward to
extend it to evaluate agents on real data. We outline our approach to real data in Section
5.1, with full details in Appendix A.2."
BENCHMARK AGENTS,0.109375,"3
Benchmark agents"
BENCHMARK AGENTS,0.11197916666666667,"In this section we outline the baseline agents that we use to benchmark canonical approaches
to uncertainty estimation in deep learning. Table 1 links to papers that introduce these
agents, as well as the hyperparamters that we tuned to optimize their performance via
gridsearch. In each case, we attempt to match ‘canonical’ implementations, which we open
source at https://anonymous.4open.science/r/neural-testbed-B839."
BENCHMARK AGENTS,0.11458333333333333,"In addition to these agent implementations, our opensource project contains all the evalua-
tion code to reproduce the results of this paper. Our code is written in Python and makes use
of Jax internally (Bradbury et al., 2018). However, our evaluation procedure is framework
agnostic, and can equally be used with any Python package including Tensorﬂow, Pytorch
or even SKlearn. Over the course of this paper, we have made extensive use of parallel
computation to facilitate large hyperparameter sweeps over many problems. Nevertheless,
the overall computational cost is relatively low by modern deep learning standards and relies
only on standard CPU. For reference, evaluating the mlp agent across all the problems in"
BENCHMARK AGENTS,0.1171875,Under review as a conference paper at ICLR 2022
BENCHMARK AGENTS,0.11979166666666667,"agent
description
hyperparameters
mlp
Vanilla MLP
L2 decay
ensemble
‘Deep Ensemble’ (Lakshminarayanan et al., 2017)
L2 decay, ensemble size
dropout
Dropout (Gal & Ghahramani, 2016)
L2 decay, network, dropout rate
bbb
Bayes by Backprop (Blundell et al., 2015)
prior mixture, network, early stopping
sgmcmc
Stochastic Langevin MCMC (Welling & Teh, 2011)
learning rate, prior, momentum
ensemble+
Ensemble + prior functions (Osband et al., 2018)
L2 decay, ensemble size, prior scale, bootstrap
hypermodel
Hypermodel (Dwaracherla et al., 2020)
L2 decay, prior, bootstrap, index dimension"
BENCHMARK AGENTS,0.12239583333333333,"Table 1: Summary of benchmark agents, full details in Appendix B."
BENCHMARK AGENTS,0.125,"our testbed and real data requires less than 3 CPU-hours. We view our opensource eﬀort
as one of the major contributions of this work. We provide clear and strong baselines, to-
gether with an objective and accessible method for assessing uncertainty estimates beyond
marginal distributions."
THE NEURAL TESTBED,0.12760416666666666,"4
The Neural Testbed"
THE NEURAL TESTBED,0.13020833333333334,"In this section we introduce the Neural Testbed, a system for assessing and comparing agent
performance. The Testbed implements synthetic data generating processes and streamlines
the process of sampling data, training agents, and evaluating test performance by estimat-
ing KL-loss for marginal and high-order joint predictions. Since independent data can be
generated for each execution, the Testbed can drive insight and multiple iterations of algo-
rithm development without risk of overﬁtting to a ﬁxed dataset. We begin by describing the
simple generative model based around a random 2-layer MLP. We then apply this testbed
to evaluate a comprehensive set of benchmark agents."
SYNTHETIC DATA GENERATING PROCESSES,0.1328125,"4.1
Synthetic data generating processes"
SYNTHETIC DATA GENERATING PROCESSES,0.13541666666666666,"By data generating process, we do not mean only the conditional distribution of data pairs
(Xt, Yt+1)|E but also the distribution of the environment E.
The Testbed considers 2-
dimensional inputs and binary classiﬁcation problems, although the generating processes
can be easily extended to any input dimension and number of classes. The Testbed oﬀers
three data generating processes distinguished by a “temperature” setting, which signiﬁes the
signal-to-noise ratio (SNR) regime of the generated data. The agent can be tuned separately
for each setting. This reﬂects prior knowledge of whether the agent is operating in a high
SNR regime such as image recognition or a low SNR regime such as weather forecasting."
SYNTHETIC DATA GENERATING PROCESSES,0.13802083333333334,"To generate a model, the Testbed samples a 2-hidden-layer ReLU MLP with 2 output units,
which are scaled by 1/temperature and passed through a softmax function to produce class
probabilities. The MLP is sampled according to standard Xavier initialization (Glorot &
Bengio, 2010), with the exception that biases in the ﬁrst layer are drawn from N(0, 1"
SYNTHETIC DATA GENERATING PROCESSES,0.140625,"2). The
inputs (Xt : t = 0, 1, . . .) are drawn i.i.d. from N(0, I). The agent is provided with the data
generating process as prior knowledge."
SYNTHETIC DATA GENERATING PROCESSES,0.14322916666666666,"In Section 2.1, we described KL-loss as a metric for evaluating performance of an agent.
The Neural Testbed estimates KL-loss, with τ ∈{1, 100}, for three temperature settings
and several training dataset sizes. For each value of τ, the KL-losses are averaged to produce
an aggregate performance measure. Further details concerning data generation and agent
evaluation are oﬀered in Appendix A."
PERFORMANCE IN MARGINAL PREDICTIONS,0.14583333333333334,"4.2
Performance in marginal predictions"
PERFORMANCE IN MARGINAL PREDICTIONS,0.1484375,"We begin our evaluation of benchmark approaches to Bayesian deep learning in marginal
predictions (τ = 1). This setting has been the main focus of the Bayesian deep learning
literature. Despite this focus, it is surprising to see in Figure 2 that none of the bench-
mark methods signiﬁcantly outperform a well-tuned MLP baseline according to d1
KL. Of
course, there are many other metrics one might consider, but in this fundamental metric of
prediction quality, the mlp agent presents a baseline that is diﬃcult to outperform."
PERFORMANCE IN MARGINAL PREDICTIONS,0.15104166666666666,Under review as a conference paper at ICLR 2022
PERFORMANCE IN MARGINAL PREDICTIONS,0.15364583333333334,"mlp
ensemble
dropout
bbb
hypermodel
ensemble+
sgmcmc
agent 0.6 0.8 1"
PERFORMANCE IN MARGINAL PREDICTIONS,0.15625,normalized KL estimate tau 1 100
PERFORMANCE IN MARGINAL PREDICTIONS,0.15885416666666666,"Figure 2: Most Bayesian deep learning approaches do not signiﬁcantly outperform a single
MLP in marginal predictions (τ = 1). Once we examine predictive distributions beyond
marginals we see a clear diﬀerence in performance between our benchmark agents (τ = 100).
For each τ, the KL estimates are normalized by the KL of the MLP agent."
PERFORMANCE IN MARGINAL PREDICTIONS,0.16145833333333334,"fixed over testbed
tuned per setting
agent hyperparameters 1 1.25 1.50 1.75 2"
PERFORMANCE IN MARGINAL PREDICTIONS,0.1640625,normalized KL estimate
PERFORMANCE IN MARGINAL PREDICTIONS,0.16666666666666666,bootstrap no yes
PERFORMANCE IN MARGINAL PREDICTIONS,0.16927083333333334,"Figure 3: Agent robustness im-
proves with bootstrapping."
PERFORMANCE IN MARGINAL PREDICTIONS,0.171875,"1
10
100
1000
0 0.1 0.2 0.3 0.4 0.5"
PERFORMANCE IN MARGINAL PREDICTIONS,0.17447916666666666,average KL estimate
PERFORMANCE IN MARGINAL PREDICTIONS,0.17708333333333334,tau: 1 agent
PERFORMANCE IN MARGINAL PREDICTIONS,0.1796875,ensemble
PERFORMANCE IN MARGINAL PREDICTIONS,0.18229166666666666,ensemble+
PERFORMANCE IN MARGINAL PREDICTIONS,0.18489583333333334,"1
10
100
1000
number of training points"
PERFORMANCE IN MARGINAL PREDICTIONS,0.1875,tau: 100
PERFORMANCE IN MARGINAL PREDICTIONS,0.19010416666666666,"Figure 4: The beneﬁts of additive prior functions are
clear in the high tau, low data regime."
PERFORMANCE IN MARGINAL PREDICTIONS,0.19270833333333334,"One of the keys to this result is that all of the agents are able to tune their hyperparameters,
such as L2 weight decay, to the SNR regime and number of training points. This matches the
way deep learning systems are typically implemented in practice, with extensive hyperpa-
rameter tuning on validation data. This methodology has led many practitioners to doubt
the usefulness of automatic tuning procedures such as bootstrap sampling (Nixon et al.,
2020). In Figure 3, we compare the performance of an ensemble+ agent that uses boot-
strapping with and without the ability to tune the hyperparameters per problem setting.
We see that bootstrap sampling is beneﬁcial when the agent is expected to work robustly
over a wide range of problem settings. However, the beneﬁts are no longer apparent when
the agent is allowed to tune its hyperparameters to individual tasks."
PERFORMANCE BEYOND MARGINALS,0.1953125,"4.3
Performance beyond marginals"
PERFORMANCE BEYOND MARGINALS,0.19791666666666666,"One of the key contributions of this paper is to evaluate predictive distributions beyond
marginals. In Figure 2, the red bars show the results of benchmark agents evaluated on
joint predictive distributions with τ = 100. Unlike when evaluating on marginal predic-
tions, where no method signiﬁcantly outperforms a well-tuned MLP, the potential beneﬁts
aﬀorded by Bayesian deep learning become clear when examining higher-order predictive
distributions. Our results refute prior works’ claims that examining dτ
KL beyond marginals
provides little new information (Wang et al., 2021)."
PERFORMANCE BEYOND MARGINALS,0.20052083333333334,"Figure 2 shows that sgmcmc is the top-performing agent overall. This should be reassuring
to the Bayesian deep learning community and beyond. In the limit of large compute this
agent should recover the ‘gold-standard’ of Bayesian inference, and it does indeed perform
best (Welling & Teh, 2011). However, some of the most popular approaches in this ﬁeld
(ensemble, dropout) do not actually provide good approximations to the predictive distri-
bution in τ = 100. In fact, we see that even though Bayesian purists may deride ensemble+
and hypermodels as ‘not really Bayesian’, these methods actually provide much better ap-
proximations to the Bayesian posterior than ‘fully Bayesian’ VI approaches like bbb. We"
PERFORMANCE BEYOND MARGINALS,0.203125,Under review as a conference paper at ICLR 2022
PERFORMANCE BEYOND MARGINALS,0.20572916666666666,"note too that while sgmcmc performs best, it also requires orders of magnitude more com-
putation than competitive methods even in this toy setting (see Appendix C.2). As we
scale to more complex environments, it may therefore be worthwhile to consider alternative
approaches to approximate Bayesian inference."
PERFORMANCE BEYOND MARGINALS,0.20833333333333334,"For insight into where our top agents are able to outperform, we compare ensemble and
ensemble+ under the medium SNR regime in Figures 4 and 5. These methods are identical,
except for the addition of a randomized prior function (Osband et al., 2018). Figure 4 shows
that, although these methods perform similarly in the quality of their marginal predictions
(τ = 1), the addition of a prior function greatly improves the quality of joint predictive
distributions (τ = 100) in the low data regime. Figure 5 provides additional intuition into
how the randomized prior functions are able to drive improved performance. Figure 5a
shows a sampled generative model from our Testbed, with the training data shown in red
and blue circles. Figure 5b shows the mean predictions and 4 randomly sampled ensemble
members from each agent (top=ensemble, bottom=ensemble+). We see that, although the
agents mostly agree in their mean predictions, ensemble+ produces more diverse sampled
outcomes enabled by the addition of randomized prior functions. In contrast, ensemble
produces similar samples, which may explain why its performance is close to baseline mlp."
PERFORMANCE BEYOND MARGINALS,0.2109375,"(a) True model.
(b) Agent samples: only ensemble+ produces diverse decision boundaries."
PERFORMANCE BEYOND MARGINALS,0.21354166666666666,Figure 5: Visualization of the predictions of ensemble and ensemble+ agents.
PERFORMANCE ON REAL DATA,0.21614583333333334,"5
Performance on real data"
PERFORMANCE ON REAL DATA,0.21875,"Section 4 provides a simple, sanitized testbed for clear insight to the eﬃcacy of Bayesian deep
learning techniques. However, most deep learning research is not driven by these sorts of
synthetic generative models, but the ultimate goal of performing well on real datasets. In this
section, we apply the same benchmark agents to a selection of small challenge datasets. We
ﬁnd that, on average, tuning agents for the synthetic problems leads to better performance
on real data. We also ﬁnd that, just as the synthetic testbed, agents that perform similarly
in marginal predictions may be distinguished in the quality of their joint predictions."
DATASETS,0.22135416666666666,"5.1
Datasets"
DATASETS,0.22395833333333334,"We focus on 10 benchmark datasets (3 feature-based, 7 image from pixels) drawn from the
literature including Iris, MNIST, and CIFAR-10 (TFD). This collection is not intended to
be comprehensive, or to include the most challenging large-scale problems, but instead to
represent some canonical real-world data that might reasonably be addressed with the MLP
models of Section 4.1. We apply a basic pre-processing step to each dataset, normalizing
input features and ﬂattening observations. We push full details to Appendix D.1."
DATASETS,0.2265625,"To assess performance in real datasets, we follow a similar procedure as Algorithm 1. The
only diﬀerence is that since it is impossible to compute the likelihood of environment for
real datasets, we compute the negative log-likelihood (NLL) rather than dτ
KL. Appendix A.2
provides further details. Note that NLL and dτ
KL are equivalent for agent comparison since
they diﬀer by a constant (see Equation 1). Furthermore, to allow for more direct comparison
with the synthetic testbed, we also consider variants of each dataset where the number of
training pairs is limited to less than the ‘full’ dataset size."
DATASETS,0.22916666666666666,Under review as a conference paper at ICLR 2022
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.23177083333333334,"5.2
Synthetic data is predictive of real data"
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.234375,"Recall that Figure 2 compares performance across an array of agents, assessed using our syn-
thetic data generating process. Each agent’s hyperparameters were tuned by ﬁrst enumerat-
ing a list of plausibly near-optimal choices and selecting the one that optimizes performance.
Each of our real-world datasets can be viewed as generated by an environment sampled from
an alternative data generating process. A natural question is whether performance on the
synthetic data correlates with performance on the real-world data."
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.23697916666666666,"The table of Figure 6a displays results pertaining to each of our agents. For each agent,
performance for each candidate hyperparameter setting was assessed on synthetic and real
data, and the correlation across these pairs is reported. The left and right columns restrict
attention to datasets with low and high volumes of training data, respectively. If a cor-
relation were equal to 1, the hyperparameter setting that optimizes agent performance on
real data would be identical to that on synthetic data. It is reassuring that the correla-
tions are high, reﬂecting a strong degree of alignment, with the exception of bbb in low data
regime, for which there appear to be pathological outcomes distorting performance for small
training sets. The values in parentheses express 5th and 95th percentile conﬁdence bounds,
measured via the statistical bootstrap."
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.23958333333333334,"Figure 6b plots performance on real versus synthetic data for the high data regime. Each
data point represents one agent-hyperparameter combination. If the correlation were equal
to 1, the combination that performs best on the synthetic data would also perform best on
the real data. It is reassuring that the correlation is large, and the conﬁdence interval be-
tween the 5th and 95th percentiles small. Agent-hyperparameter combinations that perform
better on the testbed tend to perform better on real data as well."
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.2421875,"agent
low data
high data
mlp
0.74 (0.57,0.85)
0.68 (0.38,0.99)
ensemble
0.72 (0.52,0.85)
0.63 (0.34,0.96)
dropout
0.77 (0.68,0.86)
0.78 (0.66,0.87)
bbb
-0.48 (-0.6,-0.35)
0.76 (0.68,0.83)
sgmcmc
0.72 (0.53,0.85)
0.86 (0.79,0.92)
ensemble+
0.85 (0.63,0.98)
0.74 (0.3,0.97)
hypermodel
0.52 (0.17,0.76)
0.33 (0.03,0.59)"
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.24479166666666666,(a) Correlation by agent by data regime.
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.24739583333333334,"0.01
0.03
0.1
0.3
KL on testbed 0.3 0.5 1"
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.25,NLL on real data
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.2526041666666667,"correlation=0.76 (0.67,0.82)"
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.2552083333333333,(b) Correlation in high data regime.
SYNTHETIC DATA IS PREDICTIVE OF REAL DATA,0.2578125,Figure 6: Performance on the Testbed correlates with performance on real datasets.
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2604166666666667,"5.3
Higher order predictions and informative priors"
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2630208333333333,"Our synthetic testbed can be helpful in driving innovations that carry over to real data.
Section 5.2 indicated that performance on the Testbed is correlated with that on real-
world data.
We now repeat the observation from Figure 4 on real data; additive prior
functions can signiﬁcantly improve the accuracy of joint predictive distributions generated by
ensembles. We show this by comparing the performance of ensemble+ with diﬀerent forms
of prior functions on benchmark datasets. We evaluate an ensemble with no prior function
(none), a random MLP prior (MLP), and a random linear function of a 2-dimensional latent
representation as the prior, trained via variational autoencoder (VAE) (Kingma & Welling,
2014). We provide full details in Appendix D.3."
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.265625,"Figure 7 plots the improvement in NLL for the ensemble agent relative to a baseline MLP
(lower is better), and breaks out the result for datasets=MNIST,Iris and τ = 1, 100. We"
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2682291666666667,Under review as a conference paper at ICLR 2022
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2708333333333333,"can see that the results for Iris mirror our synthetic data almost exactly. The results for
MNIST share some qualitative insights, but also reveal some important diﬀerences. For Iris
τ = 1 none of the methods outperform the MLP baseline, but for τ = 100 we see signiﬁcant
beneﬁts to an additive MLP prior in the low data regime. For MNIST τ = 1 we actually see
beneﬁts to ensembles, even without prior functions and even in the high data regime. This
reveals some aspects of this real data that are not captured by our synthetic model, where
we did not see this behaviour. For τ = 100 the random MLP prior gives a slight advantage,
but the eﬀect is much less pronounced. We hypothesize this is because, unlike the testbed,
the MLP prior is not well-matched to the input image data. However, the VAE prior is able
to provide signiﬁcant beneﬁt in the low data regime.3 These beneﬁts also carry over to Iris,
even where random MLPs already provided signﬁcant value. Designing architectures that
oﬀer useful priors for learning agents is an exciting area for future work."
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2734375,"1
10
100 0.3 0.2 0.1 0"
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2760416666666667,NLL ensemble - NLL MLP
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2786458333333333,dataset: iris
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.28125,tau: 1
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2838541666666667,"ensemble
prior MLP VAE none"
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2864583333333333,"1
10
100"
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2890625,dataset: iris
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2916666666666667,tau: 100
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2942708333333333,"1
1e1
1e2
1e3
1e4"
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.296875,dataset: mnist
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.2994791666666667,tau: 1
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.3020833333333333,"1
1e1
1e2
1e3
1e4
number of training points"
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.3046875,dataset: mnist
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.3072916666666667,tau: 100
HIGHER ORDER PREDICTIONS AND INFORMATIVE PRIORS,0.3098958333333333,"Figure 7: Prior functions provide signiﬁcant beneﬁt in the high tau, low data regime, just
like the testbed. However, for image datasets random MLP priors provide relatively little
beneﬁt. Unsupervised pretraining can help to design useful priors in high dimensional data."
CONCLUSION,0.3125,"6
Conclusion"
CONCLUSION,0.3151041666666667,"This paper highlights the need to evaluate predictive distributions beyond marginals. In
addition to this conceptual contribution, we develop a suite of practical computational
tools that can evaluate diverse approaches to uncertainty estimation. Together with these
tools, we provide a neural-network-based data generating process that facilitates research
and iteration beyond a small set of challenge datasets. We package these together as The
Neural Testbed, including a variety of baseline agent implementations. We believe that this
represents an exciting and valuable new benchmark for Bayesian deep learning and beyond."
CONCLUSION,0.3177083333333333,"We have already used this testbed to generate several new insights in this paper. We have
shown many popular Bayesian deep learning approaches perform similarly in marginal pre-
dictions but quite diﬀerently in joint predictions. We reveal the importance of bootstrapping
for parameter robustness, and also help reconcile the observed lack of improvement when
tuned to speciﬁc datasets. We have shown that these insights from synthetic data can carry
over to real datasets; that performance in these settings is correlated, that agents with sim-
ilar marginal predictions can be distinguished by their joint predictions, and that suitable
prior functions can play an important role in driving good performance."
CONCLUSION,0.3203125,"The results in this paper are in some sense preliminary. The grand challenge for Bayesian
deep learning is to provide eﬀective uncertainty estimates in large, rich datasets. While we
have demonstrated beneﬁts to predictive evaluation beyond marginals only in the ‘low data’
regime and small-scale problems, we believe that they will extend more broadly to situations
where new test inputs appear novel relative to training data. As such, we believe our core
insights should carry over to the related problems of nonstationarity and covariate shift that
plague modern deep learning systems. As an agent takes on more and more complex tasks,
it will continue to run into new and unfamiliar settings and uncertain outcomes; as such,
eﬀective predictive distributions will be more important than ever."
WE HYPOTHESIZE THAT APPROPRIATELY INITIALIZED CONVNET ARCHITECTURES MAY BE ABLE TO LEVERAGE,0.3229166666666667,"3We hypothesize that appropriately initialized convnet architectures may be able to leverage
image structure as noted in prior work (Ulyanov et al., 2018)."
WE HYPOTHESIZE THAT APPROPRIATELY INITIALIZED CONVNET ARCHITECTURES MAY BE ABLE TO LEVERAGE,0.3255208333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.328125,References
REFERENCES,0.3307291666666667,"TensorFlow Datasets, a collection of ready-to-use datasets. https://www.tensorflow.org/
datasets."
REFERENCES,0.3333333333333333,"Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight un-
certainty in neural network. In International Conference on Machine Learning, pp. 1613–
1622. PMLR, 2015."
REFERENCES,0.3359375,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax."
REFERENCES,0.3385416666666667,"Thomas Cover and Peter Hart. Nearest neighbor pattern classiﬁcation. IEEE transactions
on information theory, 13(1):21–27, 1967."
REFERENCES,0.3411458333333333,"Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural
safety, 31(2):105–112, 2009."
REFERENCES,0.34375,"David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):
1289–1306, 2006."
REFERENCES,0.3463541666666667,"Vikranth Dwaracherla, Xiuyuan Lu, Morteza Ibrahimi, Ian Osband, Zheng Wen, and Ben-
jamin Van Roy. Hypermodels for exploration. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=ryx6WgStPB."
REFERENCES,0.3489583333333333,"Angelos Filos, Sebastian Farquhar, Aidan N Gomez, Tim GJ Rudner, Zachary Kenton,
Lewis Smith, Milad Alizadeh, Arnoud De Kroon, and Yarin Gal. A systematic compar-
ison of Bayesian deep learning robustness in diabetic retinopathy tasks. arXiv preprint
arXiv:1912.10481, 2019."
REFERENCES,0.3515625,"Jerome H Friedman.
The elements of statistical learning: Data mining, inference, and
prediction. springer open, 2017."
REFERENCES,0.3541666666666667,"Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In International Conference on Machine Learning,
2016."
REFERENCES,0.3567708333333333,"Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedfor-
ward neural networks. In Proceedings of the 13th international conference on artiﬁcial
intelligence and statistics, pp. 249–256, 2010."
REFERENCES,0.359375,"Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian deep ensembles via the
neural tangent kernel. arXiv preprint arXiv:2007.05864, 2020."
REFERENCES,0.3619791666666667,"Matthew D Hoﬀman, Andrew Gelman, et al. The no-u-turn sampler: adaptively setting
path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(1):1593–1623, 2014."
REFERENCES,0.3645833333333333,"Samuel Kaski. Dimensionality reduction by random mapping: fast similarity computation
for clustering. 1998 IEEE International Joint Conference on Neural Networks Proceedings.
IEEE World Congress on Computational Intelligence (Cat. No.98CH36227), 1:413–418
vol.1, 1998."
REFERENCES,0.3671875,"Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for
computer vision?
In Advances in Neural Information Processing Systems, volume 30,
2017."
REFERENCES,0.3697916666666667,"Diederik P Kingma and Max Welling.
Auto-encoding variational Bayes.
International
Conference on Learning Representations, 2014."
REFERENCES,0.3723958333333333,"Solomon Kullback and Richard A Leibler. On information and suﬃciency. The annals of
mathematical statistics, 22(1):79–86, 1951."
REFERENCES,0.375,Under review as a conference paper at ICLR 2022
REFERENCES,0.3776041666666667,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable pre-
dictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems, pp. 6405–6416, 2017."
REFERENCES,0.3802083333333333,"Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521(7553):436,
2015."
REFERENCES,0.3828125,"Xiuyuan Lu, Ian Osband, Benjamin Van Roy, and Zheng Wen. Evaluating probabilistic
inference in deep learning: Beyond marginal predictions. CoRR, abs/2107.09224, 2021.
URL https://arxiv.org/abs/2107.09224."
REFERENCES,0.3854166666666667,"David JC MacKay. A practical Bayesian framework for backpropagation networks. Neural
computation, 4(3):448–472, 1992."
REFERENCES,0.3880208333333333,"Randall Munroe. Xkcd webcomic, 2021. URL https://m.xkcd.com/2440/."
REFERENCES,0.390625,"Kevin P Murphy. Machine Learning: A Probabilistic Perspective. MIT Press, 2012."
REFERENCES,0.3932291666666667,"Zachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael Dusenberry, Sebastian
Farquhar, Angelos Filos, Marton Havasi, Rodolphe Jenatton, Ghassen Jerfel, Jeremiah
Liu, Zelda Mariet, Jeremy Nixon, Shreyas Padhy, Jie Ren, Tim Rudner, Yeming Wen,
Florian Wenzel, Kevin Murphy, D. Sculley, Balaji Lakshminarayanan, Jasper Snoek, Yarin
Gal, and Dustin Tran. Uncertainty Baselines: Benchmarks for uncertainty & robustness
in deep learning. arXiv preprint arXiv:2106.04015, 2021."
REFERENCES,0.3958333333333333,"Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science &
Business Media, 2012."
REFERENCES,0.3984375,"Jeremy Nixon, Balaji Lakshminarayanan, and Dustin Tran. Why are bootstrapped deep
ensembles not better? In ”I Can’t Believe It’s Not Better!”NeurIPS 2020 workshop, 2020."
REFERENCES,0.4010416666666667,"Ian Osband and Benjamin Van Roy. Bootstrapped Thompson sampling and deep explo-
ration. arXiv preprint arXiv:1507.00300, 2015."
REFERENCES,0.4036458333333333,"Ian Osband, John Aslanides, and Albin Cassirer.
Randomized prior functions for deep
reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 8617–8629. Curran Associates, Inc., 2018.
URL http://papers.nips.cc/paper/
8080-randomized-prior-functions-for-deep-reinforcement-learning.pdf."
REFERENCES,0.40625,"Ian Osband, Zheng Wen, Mohammad Asghari, Morteza Ibrahimi, Xiyuan Lu, and Benjamin
Van Roy. Epistemic neural networks. arXiv preprint arXiv:2107.08924, 2021."
REFERENCES,0.4088541666666667,"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011."
REFERENCES,0.4114583333333333,"Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on
machine learning, pp. 63–71. Springer, 2003."
REFERENCES,0.4140625,"Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In
International conference on machine learning, pp. 1530–1538. PMLR, 2015."
REFERENCES,0.4166666666666667,"Bernhard Sch¨olkopf and Alexander J Smola. Learning with kernels: Support vector ma-
chines, regularization, optimization, and beyond. MIT press, 2018."
REFERENCES,0.4192708333333333,"Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse.
Functional variational
Bayesian neural networks. arXiv preprint arXiv:1903.05779, 2019."
REFERENCES,0.421875,"Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 9446–9454, 2018."
REFERENCES,0.4244791666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.4270833333333333,"Chaoqi Wang, Shengyang Sun, and Roger Grosse.
Beyond marginal uncertainty: How
accurately can Bayesian regression models estimate posterior predictive correlations? In
International Conference on Artiﬁcial Intelligence and Statistics, pp. 2476–2484. PMLR,
2021."
REFERENCES,0.4296875,"Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics.
In Proceedings of the 28th international conference on machine learning (ICML-11), pp.
681–688. Citeseer, 2011."
REFERENCES,0.4322916666666667,"Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub ´Swiatkowski, Linh Tran, Stephan
Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin.
How good is the Bayes posterior in deep neural networks really?
arXiv preprint
arXiv:2002.02405, 2020."
REFERENCES,0.4348958333333333,"Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic
perspective of generalization. arXiv preprint arXiv:2002.08791, 2020."
REFERENCES,0.4375,"Max A Woodbury. Inverting modiﬁed matrices. Statistical Research Group, 1950."
REFERENCES,0.4401041666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.4427083333333333,"A
Testbed Pseudocode"
REFERENCES,0.4453125,"We present the testbed pseudocode in this section. Speciﬁcally, Algorithm 2 is the pseu-
docode for our neural testbed, and Algorithm 3 and Algorithm 4 are two diﬀerent ap-
proaches to estimate the likelihood of a test data τ-sample conditioned on an agent’s be-
lief.
Algorithm 3 is based on the standard Monte-Carlo estimation, while Algorithm 4
adopts a random partitioning approach.
The presented testbed pseudocode works for
any prior P(E
∈·) over the environment and any input distribution PX, including
the ones described in Section 4.1.
We also release full code and implementations at
https://anonymous.4open.science/r/neural-testbed-B839."
REFERENCES,0.4479166666666667,"In addition to presenting the testbed pseudocode, we also discuss some core technical issues
in the neural testbed design.
Speciﬁcally, Appendix A.1 discusses how to estimate the
likelihood of an agent’s belief distribution; Appendix A.2 discusses how to extend the testbed
to agent evaluation on real data; ﬁnally, Appendix A.3 explains our choices of experiment
parameters."
REFERENCES,0.4505208333333333,Algorithm 2 Neural Testbed
REFERENCES,0.453125,Require: the testbed requires the following inputs
REFERENCES,0.4557291666666667,"1. prior distribution over the environment P(E ∈·), input distribution PX
2. agent fθ
3. number of training data T, test distribution order τ
4. number of sampled problems J, number of test data samples N
5. parameters for agent likelihood estimation, as is speciﬁed in Algorithm 3 and 4"
REFERENCES,0.4583333333333333,"for j = 1, 2, . . . , J do"
REFERENCES,0.4609375,Step 1: sample environment and training data
REFERENCES,0.4635416666666667,"1. sample environment E ∼P(E ∈·)
2. sample T inputs X0, X1, . . . , XT −1 i.i.d. from PX
3. sample the training labels Y1, . . . , YT conditionally i.i.d. as"
REFERENCES,0.4661458333333333,"Yt+1 ∼P (Y ∈·|E, X = Xt)
∀t = 0, 1, . . . , T −1"
REFERENCES,0.46875,"4. choose the training dataset as DT = {(Xt, Yt+1) , t = 0, . . . , T −1}"
REFERENCES,0.4713541666666667,Step 2: train agent
REFERENCES,0.4739583333333333,"train agent fθT based on training dataset DT
Step 3: compute likelihoods"
REFERENCES,0.4765625,"for n = 1, 2, . . . , N do"
REFERENCES,0.4791666666666667,"1. sample X(n)
T , . . . , X(n)
T +τ−1 i.i.d. from PX
2. generate Y (n)
T +1, . . . , Y (n)
T +τ conditionally independently as"
REFERENCES,0.4817708333333333,"Y (n)
t+1 ∼P

Y ∈·
E, X = X(n)
t

∀t = T, T + 1, . . . , T + τ −1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.484375,3. compute the likelihood under the environment E as
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.4869791666666667,"pj,n = P

Y (n)
T +1:T +τ
E, X(n)
T :T +τ−1

= QT +τ−1
t=T
Pr

Y (n)
t+1
E, X(n)
t
"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.4895833333333333,4. estimate the likelihood conditioned on the agent’s belief
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.4921875,"ˆpj,n ≈P

ˆYT +1:T +τ = Y (n)
T +1:T +τ
θT , X(n)
T :T +τ−1, Y (n)
T +1:T +τ

,"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.4947916666666667,"based on Algorithm 3 or 4 with test data τ-sample

X(n)
T :T +τ−1, Y (n)
T +1:T +τ

."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.4973958333333333,"return
1
JN
PJ
j=1
PN
n=1 log (pj,n/ˆpj,n)"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5,Under review as a conference paper at ICLR 2022
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5026041666666666,Algorithm 3 Monte Carlo Estimation of Likelihood of Agent’s Belief
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5052083333333334,Require:
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5078125,"1. trained agent fθT and number of Monte Carlo samples M
2. test data τ-sample (XT :T +τ−1, YT +1:T +τ)"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5104166666666666,"Step 1: sample M models ˆE1, . . . , ˆEM conditionally i.i.d. from P

ˆE ∈·
fθT
"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5130208333333334,Step 2: estimate ˆp as
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.515625,"ˆp = 1 M M
X"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5182291666666666,"m=1
P

ˆYT +1:T +τ = YT +1:T +τ
 ˆEm, XT :T +τ−1, YT +1:T +τ
"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5208333333333334,return ˆp
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5234375,Algorithm 4 Estimation of Likelihood of Agent’s Belief via Random Partitioning
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5260416666666666,Require:
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5286458333333334,"1. trained agent fθT
2. number of Monte Carlo samples M
3. number of hyperplanes d
4. test data τ-sample (XT :T +τ−1, YT +1:T +τ)
Step 1: sample M models ˆE1, . . . , ˆEM conditionally i.i.d. from P( ˆE ∈·|fθT ); for each
model m = 1, . . . , M, class k, and t = T, . . . , T + τ −1, deﬁne"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.53125,"pm,t,k = P( ˆY (m)
t+1 = k| ˆEm, Xt),"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5338541666666666,"and ℓm,t,k = Φ−1 (pm,t,k), where Φ(·) is the CDF of the standard normal function. For
each model m, deﬁne a vector"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5364583333333334,"ℓm = [ℓm,T,1, ℓm,T,2, . . . , ℓm,T +τ−1,K] ∈ℜKτ"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5390625,"Step 2: sample a d × (Kτ) matrix A and a d-dimensional vector b, with each ele-
ment/component sampled i.i.d. from N(0, 1). For each m = 1, . . . , M, compute"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5416666666666666,"ψm = 1 [Aℓm + b ≥0] ∈{0, 1}d."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5442708333333334,"Step 3: partition the sampled models, with each cell indexed by ψ ∈{0, 1}d and deﬁned
by
Mψ = {m : ψm = ψ}"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.546875,and assign a probability to each cell:
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5494791666666666,qψ = |Mψ| M
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5520833333333334,"Step 4: ∀ψ ∈{0, 1}d and ∀t = T, T + 1, . . . , T + τ −1, estimate the probability of
predicting ˆYt+1 = k conditioned on the cell:"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5546875,"pψ,t,k =

1
|Mψ|
P
m∈Mψ pm,t,k
if |Mψ| > 0
1
if |Mψ| = 0"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5572916666666666,"Step 5: estimate Pr(ˆYt+1:T +τ = Yt+1:T +τ|θT , Xt:T +τ−1, Yt+1:T +τ) as"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5598958333333334,"ˆp =
X"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5625,"ψ∈{0,1}d
qψ"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5651041666666666,"T +τ−1
Y"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5677083333333334,"t=T
pψ,t,Yt+1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5703125,return ˆp
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5729166666666666,Under review as a conference paper at ICLR 2022
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5755208333333334,"A.1
Estimating Likelihood of Agent’s Belief Distribution"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.578125,"We have presented two algorithms to estimate the likelihood of a test data τ-sample condi-
tioned on a trained agent: Algorithm 3 is based on the standard Monte Carlo estimation,
while Algorithm 4 adopts an approach combining random partitioning and Monte Carlo
estimation.
In this subsection, we brieﬂy discuss the pros and cons between these two
algorithms, and provide some general guidelines on how to choose between them."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5807291666666666,"Algorithm 3 produces unbiased estimates of the likelihoods, which is usually accurate when
τ is small (e.g. for τ ≤10). However, maintaining accuracy might require the number of
samples M to grow exponentially with τ. The following is an illustrative example."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5833333333333334,"Example 1 (Uniform belief over deterministic models): Consider a scenario where
the number of class labels is K = 2. We say a model ˆE is deterministic if for any feature
vector Xt,
P( ˆYt+1 = 1 | ˆE, Xt) ∈{0, 1}."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5859375,"Obviously, for any test data τ-sample (XT :T +τ−1, YT +1:T +τ) with YT +1:T +τ ∈{0, 1}τ, under
a deterministic model ˆE, we have"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5885416666666666,"P

ˆYT +1:T +τ = YT +1:T +τ
 ˆE, XT :T +τ−1, YT +1:T +τ

∈{0, 1}."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5911458333333334,"When restricted to the inputs XT :T +τ−1, there are 2τ distinguishable deterministic models.
Assume the agent’s belief distribution is uniform over these 2τ distinguishable deterministic
models, then for any YT +1:T +τ ∈{0, 1}τ, the likelihood of the agent’s belief distribution is"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.59375,"P

ˆYT +1:T +τ = YT +1:T +τ
 θT , XT :T +τ−1, YT +1:T +τ

= 2−τ."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5963541666666666,"Now let’s consider Algorithm 3.
When a model ˆEm is sampled from the agent’s belief
distribution, with probability 2−τ,"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.5989583333333334,"P

ˆYT +1:T +τ = YT +1:T +τ
 ˆEm, XT :T +τ−1, YT +1:T +τ

= 1,"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6015625,"and with probability 1 −2−τ,"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6041666666666666,"P

ˆYT +1:T +τ = YT +1:T +τ
 ˆEm, XT :T +τ−1, YT +1:T +τ

= 0."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6067708333333334,"Consequently, in expectation, we need the number of Monte Carlo samples M = Ω(2τ) to
ensure that the estimate ˆp returned by Algorithm 3 is non-zero."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.609375,"To overcome this challenge, we also propose a novel approach to estimate the likelihood of
agent’s belief via a combination of randomized partitioning and Monte Carlo simulation,
as is presented in Algorithm 4. This approach proceeds as follows. First, M models are
sampled from the agent’s belief distribution. For each sampled model, each test data input
Xt, and each class label k, a predictive probability pm,t,k and its probit ℓm,t,k = Φ−1(pm,t,k)
are computed, where Φ(·) is the CDF of the standard normal distribution. For each sampled
model, we also stack its probits into a probit vector ℓm ∈ℜKτ. Then, d random hyperplanes
are sampled and used to partition ℜKτ into 2d cells. Stacked probit vectors place models
in cells. Predictive distributions of models in each cell are averaged, and the likelihood
is calculated based on these averages, with each cell weighted according to the number of
models it contains."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6119791666666666,"The Neural Testbed applies Algorithm 4 with 2d ≪M. Hence, some cells are assigned many
models. We conjecture that, under suitable regularity conditions, models assigned to the
same cell tend to generate similar predictions. If this is the case, this algorithm produces
accurate estimates even when τ is large. We leave a formal analysis to future work."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6145833333333334,"Finally, we brieﬂy discuss how to choose between Algorithm 3 and Algorithm 4. As a rule of
thumb, we recommend to choose Algorithm 3 for τ < 10 and Algorithm 4 with the number
of hyperplanes d between 5 and 10 for τ ≥10."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6171875,Under review as a conference paper at ICLR 2022
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6197916666666666,"A.2
Agent Evaluation on Real Data"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6223958333333334,"Algorithm 2 (and its simpliﬁed version Algorithm 1) is developed for a synthetic data gener-
ating processes. We now discuss how to extend it to agent evaluation on real data. Consider
a scenario with J real datasets, and each dataset is further partitioned into a training dataset
and a test dataset. The main diﬀerence between this scenario and a synthetic data gener-
ating process is that we cannot compute the likelihood of environment for real data. Thus,
we compute the cross-entropy loss instead (see Equation 1). The computational approach is
similar to Algorithm 1: for each real dataset, we use its training dataset to train an agent.
Then, we sample N test data τ-samples from the test dataset, and estimate the likelihoods
of the agent’s belief distribution. The estimate of the cross-entropy loss is taken to be the
sample mean of the negative log-likelihoods."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.625,"Note that when ranking agents, the cross-entropy loss and dτ
KL will lead to the same order of
agents, since these two losses diﬀer by a constant independent of the agent (see Equation 1)."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6276041666666666,"A.3
Choices of Experiment Parameters"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6302083333333334,"To apply Algorithm 2, we need to specify an input distribution PX and a prior distribution
on the environment P(E ∈·). Recall from Section 4.1 that we consider binary classiﬁcation
problems with input dimension 2. We choose PX = N(0, I), and we consider three envi-
ronment priors distinguished by a temperature parameter that controls the signal-to-noise
ratio (SNR) regime. We sweep over temperatures in {0.01, 0.1, 0.5}. The prior distribution
P(E ∈·) is induced by a distribution over MLPs with 2 hidden layers and ReLU activation.
The MLP is distributed according to standard Xavier initialization, except that biases in
the ﬁrst layer are drawn from N(0, 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6328125,"2). The MLP outputs two units, which are divided
by the temperature parameter and passed through the softmax function to produce class
probabilities. The implementation of this generative model is in our open source code under
the path /generative/factories.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6354166666666666,"We now describe the other parameters we use in the Testbed.
In Algorithm 2, we
pick the order of predictive distributions τ
∈
{1, 100}, training dataset size T
∈
{1, 3, 10, 30, 100, 300, 1000}, number of sampled problems J = 10, and number of testing
data τ-samples N = 1000. We apply Algorithm 3 for evaluation of d1
KL and Algorithm 4
for evaluation of d100
KL. In both Algorithms 3 and 4, we sample M = 1000 models from the
agent. In Algorithm 4, we set the number of hyperplanes d = 7. The speciﬁcation of the
testbed parameters is in our open soucre code under the path /leaderboard/sweep.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6380208333333334,"On real datasets, we apply the same τ ∈{1, 100}, N = 1000, and M = 1000. We set the
number of hyperplanes d = 10 in Algorithm 4."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.640625,Under review as a conference paper at ICLR 2022
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6432291666666666,"B
Agents"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6458333333333334,"In this section, we describe the benchmark agents in Section 3 and the choice of vari-
ous hyperparameters used in the implementation of these agents. The list of agents in-
clude MLP, ensemble, dropout, Bayes by backprop, stochastic Langevin MCMC, ensem-
ble+ and hypermodel.
We will also include other agents such as KNN, random forest,
and deep kernel, but the performance of these agents was worse than the other benchmark
agents, so we chose not to include them in the comparison in Section
4. In each case,
we attempt to match the “canonical” implementation. The complete implementation of
these agents including the hyperparameter sweeps used for the Testbed are available at
https://anonymous.4open.science/r/neural-testbed-B839. We make use of the Epis-
temic Neural Networks notation from (Osband et al., 2021) in our code. We set the de-
fault hyperparameters of each agent to be the ones that minimize the aggregated KL score
dagg
KL = d1
KL + d100
KL/100."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6484375,"B.1
MLP"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6510416666666666,"The mlp agent learns a 2-layer MLP with 50 hidden units in each layer by minimiz-
ing the cross-entropy loss with L2 weight regularization.
The L2 weight decay scale is"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6536458333333334,chosen either to be λ 1
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.65625,"T or λ
d√"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6588541666666666,"β
T
, where d is the input dimension, β is the tempera-
ture of the generative process and T is the size of the training dataset. We sweep over
λ ∈{10−4, 10−3, 10−2, 10−1, 1, 10, 100}. We implement the MLP agent as a special case of
a deep ensemble (B.2). The implementation and hyperparameter sweeps for the mlp agent
can be found in our open source code, as a special case of the ensemble agent, under the
path /agents/factories/ensemble.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6614583333333334,"B.2
Ensemble"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6640625,"We implement the basic “deep ensembles” approach for posterior approximation (Lakshmi-
narayanan et al., 2017). The ensemble agent learns an ensemble of MLPs by minimizing the
cross-entropy loss with L2 weight regularization. The only diﬀerence between the ensemble
members is their independently initialized network weights. We chose the L2 weight scale"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6666666666666666,"to be either λ
1
MT or λ
d√"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6692708333333334,"β
MT , where M is the ensemble size, d is the input dimension, β is
the temperature of the generative process, and T is the size of the training dataset. We
sweep over ensemble size M ∈{1, 3, 10, 30, 100} and λ ∈{10−4, 10−3, 10−2, 10−1, 1, 10, 100}.
We ﬁnd that larger ensembles work better, but this eﬀect is within margin of error after 10
elements. The implementation and hyperparameter sweeps for the ensemble agent can be
found in our open source code under the path /agents/factories/ensemble.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.671875,"B.3
Dropout"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6744791666666666,"We follow Gal & Ghahramani (2016) to build a droput agent for posterior approxima-
tion. The agent applies dropout on each layer of a fully connected MLP with ReLU ac-
tivation and optimizes the network using the cross-entropy loss combined with L2 weight"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6770833333333334,"decay. The L2 weight decay scale is chosen to be either
l2
2T (1 −pdrop) or
d√"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6796875,"βl
T
where pdrop
is the dropping probability, d is the input dimension, β is the temperature of the data
generating process, and T is the size of the training dataset.
We sweep over dropout
rate pdrop ∈{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}, length scale (used for L2 weight decay)
l ∈{0.01, 0.1, 0.3, 1, 3, 10}, number of neural network layers ∈{2, 3}, and hidden layer
size ∈{50, 100}. The implementation and hyperparameter sweeps for the dropout agent
can be found in our open source code under the path /agents/factories/dropout.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6822916666666666,"B.4
Bayes-by-backprop"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6848958333333334,"We follow Blundell et al. (2015) to build a bbb agent for posterior approximation. We con-
sider a scale mixture of two zero-mean Gaussian densities as the prior. The Gaussian densi-
ties have standard deviations σ1 and σ2, and they are mixed with probabilities p and 1 −p,"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6875,Under review as a conference paper at ICLR 2022
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6901041666666666,"respectively. We sweep over σ1 ∈{1, 2, 4}, σ2 ∈{0.25, 0.5, 0.75}, p ∈{0, 0.25, 0.5, 0.75, 1},
learning rate ∈{10−3, 3 × 10−3}, number of training steps ∈{500, 1000, 10000}, number of
neural network layers ∈{2, 3}, hidden layer size ∈{50, 100}, and the ratio of the complexity
cost to the likelihood cost ∈{1, d√β}, where d is the input dimension and β is the tempera-
ture of the data generating process. The implementation and hyperparameter sweeps for the
bbb agent can be found in our open source code under the path /agents/factories/bbb.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6927083333333334,"B.5
Stochastic gradient Langevin dynamics"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6953125,"We follow Welling & Teh (2011) to implement a sgmcmc agent using stochastic gradient
Langevin dynamics (SGLD). We consider two versions of SGLD, one with momentum and
other without the momentum. We consider independent Gaussian prior on the neural net-
work parameters where the prior variance is set to be"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.6979166666666666,"σ2 = λ T dβ ,"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7005208333333334,"where λ is a hyperparameter that is swept over {0.01, 0.1, 0.5, 1}, d is the input dimension,
β is the temperature of the data generating process, and T is the size of the training
dataset. We consider a constant learning rate that is swept over {10−5, 5 × 10−5, 10−4, 5 ×
10−4, 10−3, 5 × 10−3, 10−2}.
For SGLD with momentum, the momentum decay term is
always set to be 0.9. The number of training batches is 5 × 105 with burn-in time of 105
training batches. We save a model every 1000 steps after the burn-in time and use these
models as an ensemble during the evaluation. The implementation and hyperparameter
sweeps for the sgmcmc agent can be found in our open source code under the path /agents/
factories/sgmcmc.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.703125,"B.6
Ensemble+"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7057291666666666,"We implement the ensemble+ agent using deep ensembles with randomized prior func-
tions (Osband et al., 2018) and bootstrap sampling (Osband & Van Roy, 2015).
Sim-
ilar to the vanilla ensemble agent in Section B.2, we consider L2 weight scale to be"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7083333333333334,"either λ
1
MT or λ
d√"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7109375,"β
MT .
We sweep over ensemble size M ∈{1, 3, 10, 30, 100} and λ ∈
{10−4, 10−3, 10−2, 10−1, 1, 10, 100}.
The randomized prior functions are sampled exactly
from the data generating process, and we sweep over prior scaling ∈{0, √β, 1}. In addition,
we sweep over bootstrap type (none, exponential, bernoulli). We ﬁnd that the addition
of randomized prior functions is crucial for improvement in performance over vanilla deep
ensembles in terms of the quality of joint predictions. We also ﬁnd that bootstrap sampling
improves agent robustness, although the advantage is less apparent when one is allowed to
tune the L2 weight decay for each task (see Figure 3). The implementation and hyperpa-
rameter sweeps for the ensemble+ agent can be found in our open source code under the
path /agents/factories/ensemble_plus.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7135416666666666,"B.7
Hypermodel"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7161458333333334,"We follow Dwaracherla et al. (2020) to build a hypermodel agent for posterior approxima-
tion. We consider a linear hypermodel over a 2-layer MLP base model. We sweep over"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.71875,"index dimension ∈{1, 3, 5, 7}. The L2 weight decay is chosen to be either λ 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7213541666666666,"T or λ
d√"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7239583333333334,"β
T
with λ ∈{0.1, 0.3, 1, 3, 10}, where d is the input dimension, β is the temperature of the
data generating process, and T is the size of the training dataset. We chose three diﬀerent
bootstrapping methods of none, exponential, bernoulli. We use an additive prior which is a
linear hypermodel prior over an MLP base model, which is similar to the generating process,
with number of hidden layers in {1, 2}, 10 hidden units in each layer, and prior scale from
{0, √β, 1}. The implementation and hyperparameter sweeps for the hypermodel agent can
be found in our open source code under the path /agents/factories/hypermodel.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7265625,Under review as a conference paper at ICLR 2022
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7291666666666666,"B.8
Non-parametric classifiers"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7317708333333334,"K-nearest neighbors (k-NN) (Cover & Hart, 1967) and random forest classiﬁers (Friedman,
2017) are simple and cheap oﬀ-the-shelf non-parametric baselines (Murphy, 2012; Pedregosa
et al., 2011). The ‘uncertainty’ in these classiﬁers arises merely from the fact that they pro-
duce distributions over the labels and as such we do not expect them to perform well relative
to more principled approaches. Moreover, these methods have no capacity to model dτ
KL for
τ > 1. For the knn agent we swept over the number of neighbors k ∈{1, 5, 10, 30, 50, 100}
and the weighting of the contribution of each neighbor as either uniform or based on distance.
For the random forest agent we swept over the number of trees in the forest {10, 100, 1000},
and the splitting criterion which was either the Gini impurity coeﬃcient or the information
gain. To prevent inﬁnite values in the KL we truncate the probabilities produced by these
classiﬁers to be in the interval [0.01, 0.99]. The implementation and hyperparameter sweeps
for the knn and random forest agents can be found in our open source code under the
paths /agents/factories/knn.py and /agents/factories/random_forest.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.734375,"B.9
Gaussian process with learned kernel"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7369791666666666,"A neural network takes input Xt ∈X and produces output Zt+1 = Wφθ(Xt) + b ∈RK,
where W ∈RK×m is a matrix, b ∈RK is a bias vector, and φθ : X →Rm is the output
of the penultimate layer of the neural network.
In the case of classiﬁcation the output
Zt+1 corresponds to the logits over the class labels, i.e., ˆYt+1 ∝exp(Zt+1). The neural
network should learn a function that maps the input into a space where the classes are
linearly distinguishable. In other words, the mapping that the neural network is learning
can be considered a form of kernel (Sch¨olkopf & Smola, 2018), where the kernel function k :
X × X →R is simply k(X, X′) = φθ(X)⊤φθ(X′). With this in mind, we can take a trained
neural network and consider the learned mapping to be the kernel in a Gaussian process
(GP) (Rasmussen, 2003), from which we can obtain approximate uncertainty estimates.
Concretely, let Φ0:T −1 ∈RT ×m be the matrix corresponding to the φθ(Xt), t = 0, . . . , T −1,
vectors stacked row-wise and let ΦT :T +τ−1 ∈Rτ×m denote the same quantity for the test
set. Fix index i ∈{0, . . . , K −1} to be a particular class index. A GP models the joint
distribution over the dataset to be a multi-variate Gaussian, i.e.,
""
Z(i)
1:T
Z(i)
T +1:T +τ # ∼N"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7395833333333334,"""
µ(i)
1:T
µ(i)
T +1:T +τ #"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7421875,",

σ2I + Φ0:T −1Φ⊤
0:T −1
ΦT :T +τ−1Φ⊤
0:T −1
Φ0:T −1Φ⊤
T :T +τ−1
ΦT :T +τ−1Φ⊤
T :T +τ−1 !"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7447916666666666,"where σ > 0 models the noise in the training data measurement and µ(i)
1:T , µ(i)
T +1:T +τ are
the means under the GP. The conditional distribution is given by"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7473958333333334,"P(Z(i)
T +1:T +τ | Z(i)
1:T , X0:T +τ−1) = N

µ(i)
T +1:T +τ|1:T , ΣT +1:T +τ|1:T
"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.75,"where
ΣT +1:T +τ|1:T = ΦT :T +τ−1Φ⊤
T :T +τ−1 −ΦT :T +τ−1Φ⊤
0:T −1(σ2I + Φ0:T −1Φ⊤
0:T −1)−1Φ0:T −1Φ⊤
T :T +τ−1."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7526041666666666,"and rather than use the GP to compute µ(i)
T +1:T +τ|0:T (which would not be possible since we
do not oberve the true logits) we just take it to be the output of the neural network when
evaluated on the test dataset. The matrix being inverted in the expression for ΣT +1:T +τ|0:T
has dimension T × T, which may be quite large. We use the Sherman-Morrison-Woodbury
identity to rewrite it as follows (Woodbury, 1950)"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7552083333333334,"ΣT +1:T +τ|0:T = ΦT :T +τ−1(I −Φ⊤
0:T −1(σ2I + Φ0:T −1Φ⊤
0:T −1)−1Φ0:T −1)Φ⊤
T :T +τ−1
= σ2ΦT :T +τ−1(σ2I + Φ⊤
0:T −1Φ0:T −1)−1Φ⊤
T :T +τ−1,
which instead involves the inverse of an m × m matrix, which may be much smaller. If we
perform a Cholesky factorization of positive deﬁnite matrix (σ2I + Φ⊤
0:T −1Φ0:T −1) = LL⊤"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7578125,"then the samples for all logits simultaneously can be drawn by ﬁrst sampling ζ ∈Rm×K,
with each entry drawn IID from N(0, 1), then forming
ˆYT +1:T +τ ∝exp(µT +1:T +τ|1:T + σΦT :T +τ−1L−⊤ζ).
The implementation and hyperparameter sweeps for the deep kernel agent can be found
in our open source code under the path /agents/factories/deep_kernel.py."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7604166666666666,Under review as a conference paper at ICLR 2022
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7630208333333334,"B.10
Other agents"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.765625,"In our paper we have made a concerted eﬀort to include representative and canonical agents
across diﬀerent families of Bayesian deep learning and adjacent research. In addition to
these implementations, we performed extensive tuning to make sure that each agent was
given a fair shot. However, with the proliferation of research in this area, it was not possible
for us to evaluate all competiting approaches. We hope that, by opensourcing the Neural
Testbed, we can allow researchers in the ﬁeld to easily assess and compare their agents to
these baselines."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7682291666666666,"For example, we highlight a few recent pieces of research that might be interesting to evaluate
in our setting. Of course, there are many more methods to compare and benchmark. We
leave this open as an exciting area for future research."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7708333333333334,"• Neural Tangent Kernel Prior Functions (He et al., 2020). Proposes a speciﬁc type
of prior function in ensemble+ inspired by connections to the neural tangent kernel."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7734375,"• Functional Variational Bayesian Neural Networks (Sun et al., 2019).
Applies
variational inference directly to the function outputs, rather than weights like bbb."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7760416666666666,"• Variational normalizing ﬂows (Rezende & Mohamed, 2015). Applies variational in-
ference over a more expressive family than bbb."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7786458333333334,"• No U-Turn Sampler (Hoﬀman et al., 2014). Another approach to sgmcmc that attempts
to compute the posterior directly, computational costs can grow large."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.78125,"C
Testbed results"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7838541666666666,"In this section, we provide the complete results of the performance of benchmark agents on
the Testbed, broken down by the temperature setting, which controls the SNR, and the size
of the training dataset. We select the best performing agent within each agent family and
plot d1
KL and d100
KL with the performance of an MLP agent as a reference. We also provide
a plot comparing the training time of diﬀerent agents."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7864583333333334,"C.1
Performance breakdown"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7890625,"Figures 8 and 9 show the KL estimates evaluated on τ = 1 and τ = 100, respectively. For
each agent, for each SNR regime, for each number of training points we plot the average
KL estimate from the Testbed. In each plot, we include the “baseline” mlp agent as a black
dashed line to allow for easy comparison across agents.
A detailed description of these
benchmark agents can be found in Appendix B."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7916666666666666,"C.2
Training time"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7942708333333334,"Figure 10 shows a plot comparing the d100
KL and training time of diﬀerent agents normalized
with that of an MLP. We can see that sgmcmc agent has the best performance, but at the
cost of more training time (computation). Both ensemble+ and hypermodel agents have
similar performance as sgmcmc with lower training time. We trained our agents on CPU
only systems."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.796875,"D
Real data"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.7994791666666666,"This section provides supplementary details regarding the experiments in Section 5. As
before, we include full implementation and source code at https://anonymous.4open.
science/r/neural-testbed-B839."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8020833333333334,"D.1
Datasets"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8046875,"Table 2 outlines the datasets included in our experiments. Unlike to the synthetic testbed,
which evaluates agents over a range of SNR regimes, these datasets are generally all high"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8072916666666666,Under review as a conference paper at ICLR 2022
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8098958333333334,"SNR regime. We can see this since the top-performing agents in the literature are able to
obtain high levels of classiﬁcation accuracy on held out data; something that is impossible
if the underlying system has high levels of noise."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8125,"dataset name
type
# classes
input dimension
# training pairs
iris
structured
3
4
120
wine quality
structured
11
11
3,918
german credit numeric
structured
2
24
800
mnist
image
10
784
60,000
fashion-mnist
image
10
784
60,000
mnist-corrupted/shot-noise
image
10
784
60,000
emnist/letters
image
37
784
88,800
emnist/digits
image
10
784
240,000
cmaterdb
image
10
3,072
5,000
cifar10
image
10
3,072
50,000"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8151041666666666,Table 2: Summary of benchmark datasets used in the paper.
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8177083333333334,"Each of these datasets is provided with a canonical training/test set of speciﬁc sizes. In
order to examine performance in diﬀerent data regimes we augment the default settings of
Table 2 by also examining the performance of agents on these datasets with reduced training
data. In a way that mirrors the testbed sweep of Section 4.1, we also look at settings where
the training data is restricted to T = 1, 10, 100, 1000, 10000 data points respectively."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8203125,"D.2
Correlation"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8229166666666666,"Figure 6 breaks down the correlation in performance between testbeds and real data. For
the purposes of Table 6a we say that T = 1, 10 is the ‘low data’ regime, and the maximum
training dataset size is the ‘high data’ regime. Our results show that, for each agent, for
each data regime, performance of hyperparameters is correlated across settings."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8255208333333334,"One concern might be that while performance on real data overall is highly correlated, that
this might not necessarily be the case for any individual dataset. Or, alternatively, that this
correlation is driven by extremely strong relationships in one dataset that are not present in
others. Figure 11 shows that this is not the case. In fact, for each of the datasets considered
we have strong and positive correlation over agent-hyperparameter pairs.
This gives us
conﬁdence that the results of Figure 6b are robust not only to choice of agent, but also to
some reasonable choice of datasets."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.828125,"D.3
Prior functions"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8307291666666666,"We consider two diﬀerent forms of prior functions for ensemble+: a random MLP of the
input data and a random linear function of a 2-dimensional latent trained via variational
autoencoder (VAE) (Kingma & Welling, 2014). For the MLP prior, we tried both linear
(MLP with no hidden layer) and MLP with hidden layers, and observed that the linear prior
works better. To train the 2-dimensional latent, we considered a 2-layer (128, 64) MLP for
the Gaussian encoder and a 2-layer (64, 128) MLP for the Bernoulli decoder. We trained
the VAE using all unsupervised training data available for each dataset. After training the
VAE for 10,000 steps, we used the output mean of the Gaussian encoder as the latent."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8333333333333334,Under review as a conference paper at ICLR 2022 0 0.2 0.4 0.6
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8359375,KL estimate on tau=1
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8385416666666666,"temperature = 0.01
temperature = 0.1
temperature = 0.5"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8411458333333334,ensemble 0 0.2 0.4 0.6
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.84375,ensemble+ 0 0.2 0.4 0.6
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8463541666666666,hypermodel 0 0.2 0.4 0.6
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8489583333333334,dropout 0 0.2 0.4 0.6
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8515625,sgmcmc 0 0.2 0.4 0.6 bbb 0 0.2 0.4 0.6
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8541666666666666,deep_kernel 0 0.2 0.4 0.6 knn
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8567708333333334,"1
10
100
1000 0 0.2 0.4 0.6"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.859375,"1
10
100
1000
1
10
100
1000
Number of training points"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8619791666666666,random_forest
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8645833333333334,"Figure 8: Performance of benchmark agents on the Testbed evaluated on τ = 1, compared
against the MLP baseline."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8671875,Under review as a conference paper at ICLR 2022 0 20 40 60
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8697916666666666,KL estimate on tau=100
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8723958333333334,"temperature = 0.01
temperature = 0.1
temperature = 0.5"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.875,ensemble 0 20 40 60
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8776041666666666,ensemble+ 0 20 40 60
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8802083333333334,hypermodel 0 20 40 60
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8828125,dropout 0 20 40 60
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8854166666666666,sgmcmc 0 20 40 60 bbb 0 20 40 60
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8880208333333334,deep_kernel 0 20 40 60 knn
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.890625,"1
10
100
1000 0 20 40 60"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8932291666666666,"1
10
100
1000
1
10
100
1000
Number of training points"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8958333333333334,random_forest
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.8984375,"Figure 9: Performance of benchmark agents on the Testbed evaluated on τ = 100, compared
against the MLP baseline."
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9010416666666666,Under review as a conference paper at ICLR 2022
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9036458333333334,"1
3
10
30
Average training time (x MLP training time) 0.6 0.7 0.8 0.9 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.90625,Normalized KL estimate agent bbb
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9088541666666666,dropout
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9114583333333334,ensemble
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9140625,ensemble+
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9166666666666666,hypermodel mlp
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9192708333333334,sgmcmc
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.921875,Figure 10: Normalized KL vs training time of diﬀerent agents
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9244791666666666,"0.01
0.03
0.1
0.3 1 3 5"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9270833333333334,NLL on real data
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9296875,"dataset: cifar10
correlation: 0.46"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9322916666666666,"0.01
0.03
0.1
0.3 0.1 0.3 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9348958333333334,dataset: cmaterdb
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9375,correlation: 0.53
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9401041666666666,"0.01
0.03
0.1
0.3 0.03 0.1 0.3 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9427083333333334,dataset: emnist/digits
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9453125,correlation: 0.75
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9479166666666666,"0.01
0.03
0.1
0.3
0.1 0.3 1 3"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9505208333333334,dataset: emnist/letters
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.953125,correlation: 0.61
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9557291666666666,"0.01
0.03
0.1
0.3 0.3 0.5 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9583333333333334,dataset: fashion_mnist
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9609375,correlation: 0.72
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9635416666666666,"0.01
0.03
0.1
0.3 0.8 0.9 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9661458333333334,dataset: german_credit_numeric
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.96875,correlation: 0.64
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9713541666666666,"0.01
0.03
0.1
0.3 0.3 0.5 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9739583333333334,"dataset: iris
correlation: 0.85"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9765625,"0.01
0.03
0.1
0.3 0.1 0.3 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9791666666666666,"dataset: mnist
correlation: 0.73"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9817708333333334,"0.01
0.03
0.1
0.3 0.1 0.3 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.984375,dataset: mnist_corrupted/shot_noise
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9869791666666666,correlation: 0.69
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9895833333333334,"0.01
0.03
0.1
0.3
KL on testbed 0.5 0.7 1"
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9921875,dataset: wine_quality
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9947916666666666,correlation: 0.75
COMPUTE THE LIKELIHOOD UNDER THE ENVIRONMENT E AS,0.9973958333333334,Figure 11: Correlation in high data regime for diﬀerent datasets.
