Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003076923076923077,"Data over non-Euclidean manifolds, often discretized as surface meshes, naturally
arise in computer graphics and biological and physical systems. In particular,
solutions to partial differential equations (PDEs) over manifolds depend critically
on the underlying geometry. While graph neural networks have been successfully
applied to PDEs, they do not incorporate surface geometry and do not consider
local gauge symmetries of the manifold. Alternatively, recent works on gauge
equivariant convolutional and attentional architectures on meshes leverage the
underlying geometry but underperform in modeling surface PDEs with complex
nonlinear dynamics. To address these issues, we introduce a new gauge equivariant
architecture using nonlinear message passing. Our novel architecture achieves
higher performance than either convolutional or attentional networks on domains
with highly complex and nonlinear dynamics. However, similar to the non-mesh
case, design trade-offs favor convolutional, attentional, or message passing net-
works for different tasks; we investigate in which circumstances our message
passing method provides the most benefit 1."
INTRODUCTION,0.006153846153846154,"1
Introduction"
INTRODUCTION,0.009230769230769232,"Surfaces embedded in 3D space appear in many domains such as computer graphics [1], structural
biology [2, 3], neuroscience [4], climate pattern segmentation [5, 6], and fluid dynamics [7]. Such
objects are naturally Riemannian manifolds but are often discretized into meshes for computational
tractability. Unlike other 3D representations such as point clouds or voxels, meshes encode both the
geometry and topology of the surface."
INTRODUCTION,0.012307692307692308,"One important task that naturally arises over meshes is solving partial differential equations (PDEs)
over surfaces and non-Euclidean manifolds [11–14]. A classic approach to solving surface PDEs is
to model the domain as a mesh and use the finite element method and the variational formulation
[15, 16]. Recently, there has been increasing success in applying deep learning methods to accelerate
solving PDEs and building differentiable surrogates for dynamics models [17–20]. While there are
many effective approaches for gridded data or point clouds, fewer methods exist for leveraging the
geometric structure of meshes. Several approaches simply treat the mesh as a graph by approximating
patches of the manifold as Euclidean [21–23]. However, these approaches do not exploit the geometric
properties of meshes, and Verma et al. [24], de Haan et al. [9] have shown that not retaining this
information leads to poor performance. One way to express such information is to specify the local"
INTRODUCTION,0.015384615384615385,"†Equal advising.
1Project website with code: https://jypark0.github.io/hermes"
INTRODUCTION,0.018461538461538463,"(a) Mesh
GemCNN
EMAN
Hermes"
INTRODUCTION,0.021538461538461538,"Figure 1: Mesh example. The neighbors of vertex b are projected onto the tangent plane to compute
their local orientation (θa, θc) after choosing a reference neighbor (gauge). There are 3 flavors of
gauge equivariant message passing, cf. Fig. 17 in Bronstein et al. [8]. GemCNN [9] computes linear
convolutions using the neighbor node features, EMAN [10] uses linear messages with attention, while
Hermes performs nonlinear message passing. Self-interactions are omitted for clarity."
INTRODUCTION,0.024615384615384615,"gauge symmetries and explicitly model operations that are equivariant to these transformations [6, 9],
by using parallel transport and applying a gauge equivariant filter."
INTRODUCTION,0.027692307692307693,"Although previous work on gauge equivariance for meshes including convolutional (GemCNN [9])
and attentional (EMAN [10]) methods have shown good performance on shape correspondence and
mesh classification tasks, we find they are inadequate for modeling complex dynamics on meshes,
such as solving surface PDEs, which can be highly nonlinear. In this work, we introduce a new
architecture, Hermes, that combines gauge equivariance with nonlinear message passing. As Hermes
can express linear convolutions or attention, Hermes is strictly more expressive than GemCNN and
EMAN. Hermes completes the 3 flavors of gauge equivariant message passing, analogous to graph
neural networks [8]. See Figure 1 for a comparison."
INTRODUCTION,0.03076923076923077,"We evaluate Hermes on several linear and nonlinear partial differential equations, and also on shape
correspondence, object interaction system, and cloth dynamics. Our experiments show that Hermes
outperforms convolutional or attentional counterparts in most domains, particularly on nonlinear
surface PDEs. We find that Hermes is more robust to both the fineness of the mesh and surface
roughness compared to both GemCNN and EMAN. We further investigate when the added model
complexity of nonlinear message passing outweighs the cost. With this work, we hope to help guide
practitioners on when to use nonlinear message passing schemes over linear schemes."
INTRODUCTION,0.033846153846153845,"Our contributions are summarized as follows: 1) we propose a novel gauge equivariant, nonlinear
message passing architecture, Hermes, for learning on meshes, 2) when evaluated on complex and
nonlinear dynamics such as surface PDEs, Hermes outperforms both convolutional and attentional
architectures and can generate realistic prediction rollouts, and 3) we investigate in which situations
nonlinear message passing should be preferred over convolutional or attentional counterparts."
BACKGROUND,0.036923076923076927,"2
Background"
BACKGROUND,0.04,"In this work, we focus on learning signals over meshes. Similar to de Haan et al. [9], our goal
is to define a model in coordinates that are intrinsic to the 2D mesh instead of using the extrinsic
3D coordinates of the embedding space. This avoids dependence on the 3D embedding, which is
irrelevant. However, in order to encode data over the mesh, it is still necessary to make a choice of
local coordinate frame at each vertex. Since this coordinate frame is arbitrary, it follows that the
model should be invariant to change of coordinates frame at each vertex, i.e. gauge equivariant."
DATA OVER MESHES,0.043076923076923075,"2.1
Data over Meshes"
DATA OVER MESHES,0.046153846153846156,"The Mesh Datum
A mesh M consists of (V, E, F), where V is the set of vertices, E = {(i, j)} is
the set of ordered vertex indices i, j connected by an edge, and F = {(i, j, k)} is the set of ordered
vertex indices i, j, k connected by a triangular face. Let Nv denote the set of vertices connected to a
vertex v. We assume that the mesh represents a discrete 2-dimensional manifold embedded in R3
(i.e. a manifold mesh). Let xb ∈R3 be the coordinates of vertex b. We assign a normal vector nb to
each vertex b equal to the normalized average of the surface normals of the faces that contain the
vertex. The associated tangent plane TbM at vertex b is the 2-dimensional affine space through xb
orthogonal to the normal nb."
DATA OVER MESHES,0.04923076923076923,"Local Coordinate Frame
We adopt the strategy of [9] by defining the local coordinate frame at
each vertex in terms of a reference neighboring vertex d ∈Nb. See Figure 1a for a visualization.
The reference neighbor defines a positively oriented orthonormal basis {eb
1, eb
2} for the tangent space
TbM. The orientation of every neighbor v ∈Nb can be represented with polar angles, where θv is
the angle between v and the reference neighbor after projection to the tangent plane. In Figure 1a,
θd = 0 as d is itself the reference neighbor and we show the θv for the other neighbors with respect
to the reference neighbor d. See Appendix A.1 for more information."
DATA OVER MESHES,0.052307692307692305,"Change of Gauge
A different choice of reference neighbor will result in a different positively
oriented orthonormal basis. Any two such bases will be related by an element g ∈SO(2). Here
SO(2) is called the gauge group and g is the gauge transformation. Note that such a change is
local, and a different change gp can be performed at each vertex p ∈V. For example, if we switch
from reference neighbor d to c, then we induce a gauge transformation Rot(ϕ) ∈SO(2) where
ϕ = θc −θd is the angle between d and c. The orientations are then updated θq 7→θq −ϕ, ∀q ∈Np."
DATA OVER MESHES,0.055384615384615386,"Feature Fields on the Mesh
Input, output, and hidden features are encoded as data over the
vertices {fp}p∈V or edges of the mesh {epq}(p,q)∈E. We consider directed graphs and so for the edge
feature epq, p is the source and q is the target node. We situate epq to be at TpM for simplicity as
they are often chosen to be vectors relative to p or the edge distance [25, 18]. A scalar field, such
as temperature, is invariant to change of gauge, and thus fp ∈R transforms according to the trivial
representation ρ0 of SO(2). A vector field over the mesh, representing e.g. a flow across the manifold
would have fp ∈R2 and transform according to the standard rotation representation ρ1 of SO(2) by
2 × 2 matrices. In general, feature fields may be any representation ρn of the gauge group SO(2)."
DATA OVER MESHES,0.05846153846153846,"Parallel Transport
Let fp and fq be feature vectors at nodes p, q, respectively, where q is adjacent
to p. As features at different nodes live in different tangent spaces and have different bases, in order
for the anisotropic kernel to be applied to fq for each q ∈Np, each fq must be parallel transported
to TpM and be in the same gauge. The parallel transporter gq→p first aligns the tangent plane at q
to the tangent plane at p by a 3D rotation and then transforms the gauge at q to the gauge at p by a
planar rotation. Acting on fq with gq→p transports fq to the gauge at p. For the edge features epq, no
parallel transport is required as the features live in TpM. For more details, see Appendix A.2 and
Appendix A.2 of [9]."
FLAVORS OF MESSAGE PASSING,0.06153846153846154,"2.2
Flavors of Message Passing"
FLAVORS OF MESSAGE PASSING,0.06461538461538462,"Meshes are graphs with additional geometric information. Neural networks designed for meshes may
be understood by analogy to graph neural networks (GNNs). As outlined in Bronstein et al. [8], there
are largely three flavors of GNNs: convolutional, attentional, and nonlinear message-passing."
FLAVORS OF MESSAGE PASSING,0.06769230769230769,"The three images on the right of Figure 1 show the three different GNN flavors in the context of
gauge equivariant networks over meshes. To date, both convolutional and attentional GNNs have
gauge equivariant analogues for learning over meshes. Here, we introduce a gauge equivariant mesh
network in the nonlinear message-passing flavor. Let p be the source node (b in Figure 1a), q ∈Np
be the target nodes (one-hop neighborhood), and f the signals over nodes."
FLAVORS OF MESSAGE PASSING,0.07076923076923076,"Convolutional
GemCNN [9] uses convolutions where the kernel Kneigh is applied (anisotropically)
to features at the target nodes. It is comparable to graph convolutional networks [26] but with
anisotropy and gauge equivariance. As convolutions are linear, the messages passed to the source
node are linear, and these messages are aggregated in a permutation invariant way. GemCNN also has
a kernel Kself to model self-interactions before updating the feature at the source node. The gauge
equivariant convolution (GemCNN layer) is defined as"
FLAVORS OF MESSAGE PASSING,0.07384615384615385,"f ′
p = Kselffp +
X"
FLAVORS OF MESSAGE PASSING,0.07692307692307693,"q∈Np Kneigh(θq)ρin(gq→p)fq,
(1)"
FLAVORS OF MESSAGE PASSING,0.08,"where Kneigh(θq) is the kernel for q and ρin(gq→p)fq is the feature vector at q parallel transported to
the gauge at p. Input features fq transform according to ρin and output features f ′
q according to ρout."
FLAVORS OF MESSAGE PASSING,0.08307692307692308,"The filter Kneigh is anisotropic, i.e., it depends on the orientation θq. This is strictly more expressive
than an isotropic filter (as in a GNN) which would simply use the same weights for each neighboring
vertex. However, the dependence on orientation means that without constraints, the convolution
depends on the choice of local coordinate frame. To fix this and impose gauge equivariance, Kneigh
must satisfy K = ρout(g)−1Kρin(g). We also define the kernel to only depend on the orientation and"
FLAVORS OF MESSAGE PASSING,0.08615384615384615,"not on the radial distance of neighboring nodes, as including the radius in the parameterization was
not beneficial [9] and verified in our experiments."
FLAVORS OF MESSAGE PASSING,0.08923076923076922,"Attentional
EMAN [10] extends GemCNN by adding an attention mechanism to the messages
before the aggregation step, analogous to graph attention networks [27]. Gauge equivariant attention
with the self-interaction term is defined as"
FLAVORS OF MESSAGE PASSING,0.09230769230769231,"f ′
p = αppKself
valuefp +
X"
FLAVORS OF MESSAGE PASSING,0.09538461538461539,"q∈Np αpqKneigh
value (θq)ρ(gq→p)fq,
(2)"
FLAVORS OF MESSAGE PASSING,0.09846153846153846,"where αpp, αpq are the attention weights for the self-interaction and neighbor interaction terms (see
Equation (12) in Appendix B.2). Multi-headed attention can be used by concatenating the results of
the individual heads. The attention weights are combined linearly with the neighboring node features
and can be seen as weighted local averaging. Note that due to the separate kernels for the keys,
queries, and values, an EMAN layer uses many more parameters than an equivalent GemCNN layer
with the same dimensions."
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.10153846153846154,"3
Gauge Equivariant Nonlinear Message Passing"
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.10461538461538461,"We propose gauge equivariant nonlinear message passing for meshes, complementing the convolu-
tional and attentional gauge equivariant methods. As is the case for nonlinear message passing GNNs,
our network is designed to be better suited for tasks with complex local interactions. Our network,
named Hermes, consists of a sequence of message passing blocks, each of which contains an edge
network ϕe, an aggregation (we use sum throughout), and a node network ϕn."
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.1076923076923077,"The edge network models neighboring interactions and takes as inputs the source node features
fp, target node features fq where q ∈Np, and edge features epq where (p, q) ∈E. The target
node features are parallel transported to the gauge at p as ρ(gq→p). The network ϕe consists of Ne
gauge equivariant convolutions followed by regular nonlinearities [9] which also preserve gauge
equivariance. The nonlinear messages mpq are then aggregated to mp. The node network models
self-interactions and takes as input mp ⊕fp, where ⊕represents the direct sum, and then applies Nn
gauge equivariant convolutions with Kself kernels and regular nonlinearities. Equations (3)-(7) define
gauge equivariant nonlinear message passing,"
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.11076923076923077,"hpq = fp ⊕ρ(gq→p)fq ⊕epq,
∀(p, q) ∈E
(3)"
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.11384615384615385,"mpq = ϕe(hpq) = σ ◦KNe
neigh(θq) ◦· · · ◦σ ◦K1
neigh(θq)(hpq),
(4)"
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.11692307692307692,"mp =
X"
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.12,"q∈Np
mpq,
∀p ∈V
(5)"
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.12307692307692308,"hp = mp ⊕fp
(6)"
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.12615384615384614,"f ′
p = ϕn(hp) = σ ◦KNn
self ◦· · · ◦σ ◦K1
self(hp),
(7)"
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.12923076923076923,"where hpq and hp are inputs to ϕe and ϕn, σ is the regular nonlinearity, and ◦denotes composition."
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.13230769230769232,"Figure 2 shows the Hermes architecture for one of the datasets used in experiments (wave PDE,
see Section 4.1). For comparison, we also include the GemCNN and EMAN architectures in
Appendix B. A residual connection is included at the end of each HermesBlock for more expressivity,
see Section 5.5 for ablation results."
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.13538461538461538,"Proof of Gauge Equivariance
We explicitly show that Hermes is equivariant to local gauge
transformations in Proposition 1."
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.13846153846153847,Proposition 1. Hermes is equivariant to local gauge transformations gp for any p ∈V of a mesh.
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.14153846153846153,"The proof, provided in Appendix B.3, uses the fact that all of the kernels used in Hermes satisfy the
constraints for gauge equivariance. The nonlinearities are exactly gauge equivariant as the number of
intermediate samples N used in the discrete Fourier transform approaches infinity and approximately
gauge equivariant for finite N (see Section 4 in [9] for more details). Thus Hermes is also gauge
equivariant (in the limit)."
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.14461538461538462,"Hermes combines both nonlinear message passing and gauge equivariance, generalizing GemCNN
and EMAN, and completes the full picture with the 3 flavors of message passing (cf. Figure 1). An"
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.1476923076923077,"Figure 2: Hermes network architecture for the Wave PDE dataset. There are two message blocks,
each with 2 layers for the edge network ϕe and 1 layer for the node network ϕn. To illustrate the
computations within the edge and node networks, we use the example mesh from Figure 1a as input.
In the edge network, we show only the computations for node b for clarity."
GAUGE EQUIVARIANT NONLINEAR MESSAGE PASSING,0.15076923076923077,"important point to note is that nonlinear message passing decouples the effect of the number of layers
in the network (depth) with the receptive field of the graph. For graph convolutions using 1-hop
neighbors, the receptive field of messages is exactly equal to the network depth. In contrast, nonlinear
message passing computes messages using features from neighbors at an arbitrary hop distance away
from the source node before the message is passed to the source node. We hypothesize that this
decoupling is beneficial for tasks with complicated interaction dynamics between node neighbors,
and previous works with graph message passing networks [28, 29] have shown good performance
for abstract graphs (non-spatial graphs) involving objects. Although meshes are inherently spatial,
we show that nonlinear message passing is an important tool for solving PDEs on surfaces and can
outperform either convolutional or attentional mechanisms."
EXPERIMENT DESIGN,0.15384615384615385,"4
Experiment Design"
EXPERIMENT DESIGN,0.15692307692307692,"To evaluate gauge equivariant nonlinear message passing, we consider several different tasks. Our
primary domains are partial differential equations on meshes, but we also consider shape correspon-
dence, object interactions, and cloth simulation. Throughout, we consider triangular meshes as they
are the most common. Further details and visualizations of the domains are provided in Appendix C."
DOMAINS,0.16,"4.1
Domains"
DOMAINS,0.16307692307692306,"Partial Differential Equations on meshes
We consider three linear and nonlinear surface partial
differential equations (PDEs), where the dynamics occur on the surface of an object, represented as a
two dimensional mesh embedded in 3D space. Since solving PDEs on meshes, e.g. heat diffusing over
a surface, naturally depends on the intrinsic mesh geometry, gauge-equivariant nonlinear message
passing is a promising solution. For all three equations, we use example meshes in the PyVista library
[30] and generate 5 trajectories with different initial conditions, see Appendix C for more details."
DOMAINS,0.16615384615384615,"Heat/Wave equation: The heat and wave equations are second-order linear partial differential equa-
tions (see Appendix C.1). Solving the heat equation on a surface mesh can correspond to modeling
the dynamics when an external hot object makes contact at certain points on a thin hollow object.
The wave equation describes how acoustic waves propagate on a thin hollow surface. The dynamics
of both equations are highly dependent on the local mesh geometry."
DOMAINS,0.16923076923076924,"Cahn-Hilliard equation: The Cahn-Hilliard equation [31] describes phase separation in a binary
fluid mixture and is often used to model spinodal decomposition. It is a fourth-order, nonlinear,"
DOMAINS,0.1723076923076923,"time-dependent PDE and is often factored into two coupled second-order equations. The surface
Cahn-Hilliard equation can model real-world applications such as cell proliferation [32] and two-
component vesicles [33]. As a nonlinear PDE, we expect our nonlinear message passing method to
exhibit greater performance than other linear message passing flavors."
DOMAINS,0.1753846153846154,"Shape correspondence
As a standard mesh benchmark, we use the same FAUST dataset used
in previous work [9, 10]. The dataset consists of 80 train and 20 test high resolution scans of 10
humans in 10 different poses. The task is to determine shape correspondence between different
meshes. As the vertices are all registered and represent the same position on the human body, this
task is equivalent to classifying the correct label for each vertex."
DOMAINS,0.17846153846153845,"Object interactions
Inspired by interaction systems [28, 29, 34], we consider complex dynamics
of interacting objects on a mesh. On a coarse triangular mesh with random hills, each object occupies
a vertex and is oriented towards a neighboring vertex. An action can either turn the object left
(changing its orientation), move the object forward, or turn right. Objects cannot move forward
if there is another object at the destination vertex, giving rise to complex interacting dynamics.
Furthermore, we consider the geometry of the mesh such that an object cannot move forward if
the height difference between the current vertex and destination vertex is too high, or if the angle
between the vertex normals is too large. If an object is able to move forward, we parallel transport its
orientation and then choose the nearest neighboring node as its new orientation."
DOMAINS,0.18153846153846154,"FlagSimple
We also include the FlagSimple dataset from [20], which simulates cloth dynamics
of a flag with self-collisions. Unlike the other datasets, the mesh is dynamic where the node positions
change over time. The dataset was created using ArcSim [35] with regular meshing over 400
timesteps. See Appendix A.1 of [20] for more information."
TRAINING DETAILS,0.18461538461538463,"4.2
Training Details"
TRAINING DETAILS,0.18769230769230769,"For the PDE datasets, we report test root mean squared error (RMSE) of the prediction at the next
timestep given the previous 5 timesteps. We use three separate test datasets and evaluate generalization
to future timesteps (test time), unseen initial conditions (test init), and unseen meshes (test
mesh). For test time, we train on timesteps T = 0, . . . , 149 and test on T = 150, . . . , 200. For
test init, we test on trajectories with new initial conditions. For test mesh, we evaluate on
completely unseen meshes to see whether a method overfits to specific mesh geometries."
TRAINING DETAILS,0.19076923076923077,"For baselines, our main comparison is with GemCNN and EMAN to gauge how beneficial nonlinear
message passing is over convolutional or attentional flavors. We also compare against 1) a SOTA
messsage passing, mesh-aware method MeshGraphNet [20], 2) an E(3)-equivariant, non mesh-aware
message passing network EGNN [25], 3) a non-equivariant, mesh-aware baseline SpiralNet++ [36],
and 4) standard non-equivariant GNNs: graph convolutional networks (GCN) [26] and message
passing networks (MPNN) [37, 38]. We tune each architecture to use a similar number of trainable
parameters for a fair comparison, see Table 9 in Appendix D. A more detailed feature comparison of
each method is provided in Table 8 and all other training details are relegated to Appendix D."
RESULTS,0.19384615384615383,"5
Results"
RESULTS,0.19692307692307692,"Table 1 shows the results on the PDE datasets and other results are given in Table 10 in Appendix E.
On Heat, we see that EMAN outperforms GemCNN and Hermes outperforms EMAN significantly.
This coincides with our expectation as attention mechanisms can express convolutions using constant
weights, and Hermes generalizes both linear convolution and attention mechanisms. However, this
does not hold for the Wave dataset, where EMAN is noticeably worse than GemCNN while Hermes
still performs well. On the nonlinear Cahn-Hilliard dataset, we see that EMAN cannot generalize
well to unseen meshes. Overall Hermes achieves an RMSE approximately 3 times lower than that of
GemCNN, and between 2 to 8 times lower than that of EMAN."
RESULTS,0.2,"Compared to other non gauge equivariant baselines, Hermes outperforms all methods, except for
certain cases with MeshGraphNet. Hermes is worse than MeshGraphNet on Heat, outperforms on
Wave, and performs similarly on Cahn-Hilliard. It performs substantially better on all test mesh
datasets, which may indicate that Hermes can generalize to the true dynamics function, rather than
the specific dynamics seen in the training trajectories. In other words, Hermes is better adapted to use
the underlying geometry while MeshGraphNet overfits to specific geometries."
RESULTS,0.20307692307692307,"Table 1: RMSE on PDE domains, using 5 runs. All values are expressed in ×10−3 and gray denotes
95% confidence intervals. Hermes generally outperforms baselines, except for some cases with
MeshGraphNet (MGN). Hermes outperforms MeshGraphNet on all test mesh datasets."
RESULTS,0.20615384615384616,"Hermes
GemCNN
EMAN
GCN
MPNN
MGN
EGNN
SpiralNet++"
RESULTS,0.20923076923076922,"HEAT
Test time
1.18±0.3
3.88±0.8
2.93±0.9
152±1.2
2.66±0.8
0.93±0.2
3.09±1.2
2.82±0.2
Test init
1.16±0.3
3.85±0.8
2.90±0.9
152±0.9
2.63±0.8
0.93±0.2
3.07±1.2
6.44±0.1
Test mesh
1.01±0.3
3.50±0.6
2.47±0.7
127±2.2
2.36±0.7
2.41±1.1
8.96±5.0
22.0±0.3"
RESULTS,0.2123076923076923,"WAVE
Test time
5.43±0.8
12.2±1.5
19.0±3.0
162±5.0
9.07±1.2
6.26±0.9
45.9±6.1
8.88±1.2
Test init
3.72±1.3
7.28±0.7
15.3±3.6
158±5.9
5.24±1.1
4.24±0.6
12.1±3.5
8.47±0.6
Test mesh
3.79±1.3
8.23±0.6
15.8±3.7
164±5.1
6.29±1.3
7.01±1.9
54.5±18
10.8±0.8"
RESULTS,0.2153846153846154,"CAHN-
HILLIARD"
RESULTS,0.21846153846153846,"Test time
3.48±0.9
13.8±12
8.41±4.0
250±7.6
7.25±3.1
4.49±0.6
8.36±1.4
11.6±3.3
Test init
4.23±1.5
14.9±12
8.75±4.0
383±6.0
7.52±3.0
4.64±0.6
10.6±1.1
12.9±2.8
Test mesh
5.41±0.8
14.1±12
43.8±27
391±8.6
7.63±3.0
18.7±7.2
9.38±1.7
13.4±2.5"
RESULTS,0.22153846153846155,"Figure 3: Errors from long-horizon prediction rollouts on unseen meshes, given only initial conditions.
Error bars denote standard error over 5 runs averaged over unseen meshes (2 meshes for heat/wave,
1 for Cahn-Hilliard). GemCNN has exploding errors with increasing t on Heat and Wave. Hermes
generally outperforms GemCNN and EMAN, with the exception of EMAN on Wave."
RESULTS,0.2246153846153846,"On the FlagSimple dataset (Table 10), Hermes outperforms MeshGraphNet considerably, suggesting
that Hermes is not limited to static meshes and can handle temporally changing meshes well. Note
that MeshGraphNet was modified to have a similar number of parameters as Hermes and so the
results are different than reported in Pfaff et al. [20]."
LONG-HORIZON PREDICTION ROLLOUTS,0.2276923076923077,"5.1
Long-horizon prediction rollouts"
LONG-HORIZON PREDICTION ROLLOUTS,0.23076923076923078,"Using a representative random run, we generate predictions from each model autoregressively given
only the initial conditions. We generate predictions on the unseen meshes (test mesh) and roll out the
entire trajectory of the PDE (Tmax = 200). Figure 3 shows how the errors change with the increasing
rollout horizon. For GemCNN, the errors accumulate quickly and the predictions diverge for Heat
and Wave. EMAN performs similarly to Hermes on Heat but underperforms on Cahn-Hilliard. All
methods eventually diverge on Wave; this may be due to the fact that the wave amplitude oscillates
multiple times over the longer horizon and so it may be more difficult to predict the periodic nature."
LONG-HORIZON PREDICTION ROLLOUTS,0.23384615384615384,"Table 2 shows prediction samples at T + 50 generated autoregressively given only initial conditions.
GemCNN fails on the Heat dataset and does not produce the correct spatial patterns for Wave and
Cahn-Hilliard. EMAN performs well on Wave, but fails on Cahn-Hilliard. Hermes gives fairly
realistic predictions on all datasets. See Appendix E.1 for more samples with varying T."
MESH FINENESS,0.23692307692307693,"5.2
Mesh Fineness"
MESH FINENESS,0.24,"Here we investigate how mesh fineness impacts performance with respect to the three flavors of
message passing. Our intuition is that as meshes become finer, the features at each node become more
similar. Thus the dynamics between nodes would become more linear and convolutional approaches
may perform comparably with nonlinear message passing."
MESH FINENESS,0.24307692307692308,"For this experiment, we solve the heat and wave equations on a single mesh (“armadillo”). To
generate different mesh resolutions, we simplify the original mesh (# vertices = 172, 974) to
{348, 867, 1731, 3461, 8650} vertices using quadric decimation [39] (1731 vertices were used for the
main results in Table 1). See Figure 9 in Appendix E.2 for data visualizations. We generate 15 trajec-
tories with Tmax = 100 and use 5 of the 15 trajectories as test init and use T = 81, . . . , 100 of the
remaining 10 trajectories for the test time. As we use a single mesh, we do not test generalization
to unseen meshes. We use 3 random seeds for each method."
MESH FINENESS,0.24615384615384617,"Table 2: Qualitative prediction samples rolled out to the full path using only initial conditions.
GemCNN completely fails on Heat, while EMAN fails on Wave. Hermes predicts the spatial patterns
accurately and outperforms GemCNN and EMAN on all datasets."
MESH FINENESS,0.24923076923076923,"Dataset
GemCNN
EMAN
Hermes
Ground Truth"
MESH FINENESS,0.2523076923076923,"Heat
(T+50)"
MESH FINENESS,0.2553846153846154,"Wave
(T+50)"
MESH FINENESS,0.25846153846153846,"Cahn-Hilliard
(T+50)"
MESH FINENESS,0.26153846153846155,"Figure 4: Performance for varying mesh fineness for heat on the test time (left) and test init (right)
datasets. Error bars denote standard error over 3 runs. The errors for GemCNN and EMAN increase
as the mesh becomes coarser, while Hermes performs similarly throughout."
MESH FINENESS,0.26461538461538464,"Figure 4 shows the results for the heat dataset. RMSE on both the test time and test init datasets
increase for GemCNN and EMAN as the mesh becomes coarser (decreasing number of vertices). This
coincides with the intuition that coarser meshes have more nonlinear dynamics between nodes and
thus contributing to the increase in errors. On the other hand, Hermes still quantitatively outperform
GemCNN and EMAN across all mesh resolutions. It is also robust to varying mesh fineness and has
similar error values throughout."
MESH FINENESS,0.2676923076923077,"On the wave dataset, we find that Hermes still outperforms GemCNN and EMAN at each resolution
though the gap is not as large, see Figure 10 (Appendix E.2). There is a surprising decreasing trend
in errors for all methods as the mesh becomes finer. This indicates that mesh fineness may not be the
only factor at play: it is possible that the type of dynamics (PDE) used and the specific architecture
(e.g. graph receptive field) can affect the results. Another possible explanation is that as we use a
single simplified mesh, there may be some artifacts that affect the wave PDE simulations."
SURFACE ROUGHNESS,0.27076923076923076,"5.3
Surface Roughness"
SURFACE ROUGHNESS,0.27384615384615385,"We investigate whether surface roughness affects the performance gap between GemCNN and
Hermes. We use the same armadillo mesh with 1,731 vertices from before and extract the vertex
normals. The vertex normals are then randomly scaled using a Gaussian distribution N(0, s2), where
s ∈{0.1, 0.5, 1, 1.5, 3}. The vertex coordinates are then modified by adding these scaled normals to
the original coordinates, resulting in different surface roughnesses. See Figure 11 in Appendix E.3
for visualizations. We use the same settings as in the fineness experiments in Section 5.2."
SURFACE ROUGHNESS,0.27692307692307694,"Similarly to the results on mesh fineness, we find that Hermes retains the performance advantage
over GemCNN and EMAN across all roughness scales. Its RMSE errors are also nearly constant"
SURFACE ROUGHNESS,0.28,"Figure 5: Performance for varying surface roughness for heat on the test time (left) and test init
(right) datasets. Error bars denote standard error over 3 runs. Increasing scale increases the surface
roughness of the mesh. Hermes outperforms GemCNN and EMAN across different roughness scales
and is much more robust."
SURFACE ROUGHNESS,0.28307692307692306,Table 3: Mean forward computation time in seconds (ms) during inference on test time PDE datasets.
SURFACE ROUGHNESS,0.28615384615384615,"GemCNN
EMAN
Hermes
GCN
MPNN
MeshGraphNet
EGNN
SpiralNet++"
SURFACE ROUGHNESS,0.28923076923076924,"HEAT (ms)
12.4
19.5
10.8
1.5
1.2
2.2
1.4
0.9
WAVE (ms)
12.5
17.7
10.5
1.4
1.3
2.2
1.3
0.9
CAHN-HILLIARD (ms)
6.9
9.2
6.2
1.9
1.5
2.1
1.6
1.3"
SURFACE ROUGHNESS,0.2923076923076923,"throughout indicating robustness to surface roughness. Surprisingly, GemCNN seems to perform
increasingly better as the surface becomes rougher while EMAN seems to perform roughly equal. On
the wave dataset, Hermes still outperforms other baselines at most roughness scales, see Figure 12 in
Appendix E.3. There does not seem to be any correlation in errors with surface roughness."
COMPUTATION TIME,0.2953846153846154,"5.4
Computation Time"
COMPUTATION TIME,0.29846153846153844,"As the edge and node networks ϕn, ϕe consist of multi-layer perceptrons, one might think Hermes
requires more computation time than its simpler convolutional or attentional counterparts. Table 3
shows that this is not the case; Hermes has a lower computation time than both GemCNN and EMAN.
As we control for a similar number of parameters in each architecture, there are fewer aggregations
in Hermes than in GemCNN or EMAN resulting in a lower average computation time in the forward
pass. For EMAN in particular, due to its attention mechanism, we find that it uses many more
parameters than either GemCNN or Hermes. Thus constraining EMAN to have a similar number of
parameters as GemCNN restricts its expressivity. On the other hand, Hermes is very flexible as it can
use a different number of layers for the edge and node networks. Even though Hermes uses a slightly
lower hidden dimension than GemCNN in all domains, the results show that this does not hamper
model expressivity and it still achieves higher performance. As expected, all three gauge equivariant
methods are significantly more computationally expensive than standard graph neural networks."
RESIDUAL CONNECTION ABLATION,0.30153846153846153,"5.5
Residual connection ablation"
RESIDUAL CONNECTION ABLATION,0.3046153846153846,"As Hermes models the self-influence of nodes, we also test whether a residual connection is necessary.
Table 12 in Appendix E.4 shows the ablation of the residual connection. Having a residual connection
improves performance on Heat, but decreases performance slightly on Wave and Cahn-Hilliard. Thus
the residual connection seems to be task-dependent and should be considered a hyperparameter. In
our experiments, a residual connection was used for each message passing block."
RELATED WORK,0.3076923076923077,"6
Related Work"
RELATED WORK,0.31076923076923074,"Learning over meshes
Several different mesh operators have been studied within computer
graphics, in the context of shape classification [40–42], dense shape correspondence [43, 36], mesh
segmentation [44–46]. Due to the success of graph neural networks (GNNs) [26, 27, 38], several
approaches use GNNs to process meshes [21, 22, 47, 36] and incorporate geometric information via
geodesic convolutions, anisotropic kernels, dual graphs, or spiral operators. Pfaff et al. [20] introduce
a state of the art method for learning simulations with meshes by representing a graph in mesh space
and in world space. In this work, we introduce a novel architecture that combines nonlinear message
passing with explicit equivariance to local gauge transformations."
RELATED WORK,0.31384615384615383,"Gauge Symmetry
Most works on non-Euclidean manifolds have used convolutions on discretized
patches of the manifold, approximating them to be Euclidean. Masci et al. [21] define the patch
operator using local geodesic coordinate systems. Monti et al. [23] use a mixture of parametric
Gaussian kernels. All of these works use (linear) convolutions, unlike our work. Boscaini et al.
[22] uses anisotropic diffusion kernels, but uses the principal curvature direction as the preferred
gauge which may be ill-defined for some shapes. On the other hand, several works have considered
equivariance to gauge symmetry. Cohen et al. [6] proposes gauge equivariant convolutions on the
icosahedron. Most similar to our work is that of [9, 48, 10], which explicitly incorporate gauge
equivariant kernels for meshes. de Haan et al. [9] use convolutions while He et al. [48] and Basu et al.
[10] use attention to discrete and continuous gauge transformations, respectively. In this work, we
complete the picture and propose general nonlinear message passing with gauge equivariance for
meshes. For more in-depth theory and discussion of local gauge equivariance on manifolds, see [49]."
RELATED WORK,0.3169230769230769,"Solving PDEs on surfaces
With the rise of physics-informed neural networks [50–52] and their
increased sample efficiency and performance over classical methods, several recent works have
focused on solving complex partial differential equations with deep learning, e.g. fluid dynamics
[53–55], thermodynamics [56], structural mechanics [57, 58], and material science [59, 60]. Some
approaches learn the finite-dimensional [51, 53, 54] or infinite-dimensional [61, 17, 62] solution
operators. However, there has been less work on applying deep learning to solving PDEs on surfaces
embedded in 3D space using meshes, with some exceptions [63–65]. Fang and Zhan [63] solve the
Laplace-Beltrami equation over a 3D surface with neural networks while Tang et al. [64] propose
an extrinsic approach. However, both methods are mesh-free and only use points and their normals,
discarding any connectivity information. Li et al. [65] extend Fourier neural operator [66] and learns
a diffeomorphic deformation between the input space and the computational mesh. While Geo-FNO
depends on the embedding space of the mesh (e.g. embedding a rough 2D mesh in 3D), our method
works directly on the intrinsic mesh surface. Perhaps most relevant to this paper is [67], which
extends GemCNN [9] in several ways to predict hemodynamics on artery walls. In this work, we
propose a nonlinear message passing architecture for meshes that retains the underlying geometry
and demonstrate their effectiveness in predicting a variety of surface dynamics."
DISCUSSION,0.32,"7
Discussion"
DISCUSSION,0.3230769230769231,"We introduce a novel architecture, Hermes, that performs gauge equivariant, nonlinear message
passing for meshes. Hermes complements convolutional GemCNN and attentional EMAN and
completes development of the 3 flavors of message passing in the gauge equivariant setting. In the
context of meshes, similar to GNNs, there seems to be a tradeoff between simple linear operations
such as convolutions versus nonlinear message passing. Convolutions are computationally efficient
and can perform well on simpler tasks such as shape correspondence, see Table 10 in Appendix E.
However, when the interactions are more complicated such as in surface PDEs, nonlinear message
passing surpass linear schemes. By decoupling the degree of nonlinearity from the depth of the
network and receptive field, Hermes outperforms GemCNN and EMAN significantly on the PDE
datasets and produces realistic predictions given only initial conditions."
DISCUSSION,0.3261538461538461,"A limitation is that Hermes may use more parameters depending on the architectures of the edge
and node networks. In particular, both EMAN and Hermes cannot scale well to meshes with a large
number of vertices naively, and one may need to consider more sophisticated approaches such as
multi-scale or graph expander approaches. Furthermore, the architecture search space for Hermes is
larger than that of GemCNN or EMAN, as one needs to consider different combinations of numbers
of layers in the edge and node networks, along with the number of message passing blocks."
DISCUSSION,0.3292307692307692,"For future work, one direction is to consider different dynamics such as non-stationary or chaotic
dynamics, and other PDEs important in real world applications such as Navier-Stokes for climate or
blood flow. Another direction is to analyze the design space of gauge equivariant networks. While
GNNs have been extensively studied, far less work exists for mesh methods. GNNs are often highly
task-specific and there are many design dimensions (e.g. residual connections, message passing
iterations, etc.) to consider [68]. It would be particularly helpful for practitioners to have guidelines
on when to use gauge equivariance or message passing over simpler approaches. This work aims
to be a first step in this direction by demonstrating Hermes as a good fit for predicting nonlinear
dynamics on meshes."
DISCUSSION,0.3323076923076923,Acknowledgments and Disclosure of Funding
DISCUSSION,0.3353846153846154,"We sincerely thank Vedanshi Shah for the initial data generation code and Pim de Haan for clarifying
parts of GemCNN. This work is supported in part by NSF 2107256 and 2134178. This work was
completed in part using the Discovery cluster, supported by Northeastern University’s Research
Computing team."
REFERENCES,0.3384615384615385,References
REFERENCES,0.3415384615384615,"[1] Mario Botsch, Leif Kobbelt, Mark Pauly, Pierre Alliez, and Bruno Lévy. Polygon mesh
processing. CRC press, 2010."
REFERENCES,0.3446153846153846,"[2] Pablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola, D Boscaini, MM Bronstein,
and BE Correia. Deciphering interaction fingerprints from protein molecular surfaces using
geometric deep learning. Nature Methods, 17(2):184–192, 2020."
REFERENCES,0.3476923076923077,"[3] Freyr Sverrisson, Jean Feydy, Bruno E Correia, and Michael M Bronstein. Fast end-to-end
learning on protein surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 15272–15281, 2021."
REFERENCES,0.3507692307692308,"[4] Franco Dassi, Bree Ettinger, Simona Perotto, and Laura M Sangalli. A mesh simplification
strategy for a spatial regression analysis over the cortical surface of the brain. Applied Numerical
Mathematics, 90:111–131, 2015."
REFERENCES,0.35384615384615387,"[5] Mayur Mudigonda, Sookyung Kim, Ankur Mahesh, Samira Kahou, Karthik Kashinath, Dean
Williams, Vincen Michalski, Travis O’Brien, and Mr Prabhat. Segmenting and tracking extreme
climate events using neural networks. In Deep Learning for Physical Sciences (DLPS) Workshop,
held with NIPS Conference, 2017."
REFERENCES,0.3569230769230769,"[6] Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling.
Gauge equivariant
convolutional networks and the icosahedral cnn. In International conference on Machine
learning, pages 1321–1330. PMLR, 2019."
REFERENCES,0.36,"[7] Kenneth G Powell, Philip L Roe, and James Quirk. Adaptive-mesh algorithms for computational
fluid dynamics. In Algorithmic trends in computational fluid dynamics, pages 303–337. Springer,
1993."
REFERENCES,0.3630769230769231,"[8] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021."
REFERENCES,0.36615384615384616,"[9] Pim de Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge equivariant mesh CNNs:
Anisotropic convolutions on geometric graphs. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=Jnspzp-oIZE."
REFERENCES,0.36923076923076925,"[10] Sourya Basu, Jose Gallego-Posada, Francesco Viganò, James Rowbottom, and Taco Cohen.
Equivariant mesh attention networks. Transactions on Machine Learning Research, 2022. ISSN
2835-8856. URL https://openreview.net/forum?id=3IqqJh2Ycy."
REFERENCES,0.3723076923076923,"[11] Marcelo Bertalmıo, Li-Tien Cheng, Stanley Osher, and Guillermo Sapiro. Variational problems
and partial differential equations on implicit surfaces. Journal of Computational Physics, 174
(2):759–780, 2001."
REFERENCES,0.37538461538461537,"[12] Thomas D Economon, Francisco Palacios, Sean R Copeland, Trent W Lukaczyk, and Juan J
Alonso. Su2: An open-source suite for multiphysics simulation and design. Aiaa Journal, 54
(3):828–846, 2016."
REFERENCES,0.37846153846153846,"[13] Steffen Marburg and Bodo Nolte. Computational acoustics of noise propagation in fluids: finite
and boundary element methods, volume 578. Springer, 2008."
REFERENCES,0.38153846153846155,"[14] Amy Novick-Cohen. Chapter 4 the cahn-hilliard equation. volume 4 of Handbook of Differential
Equations: Evolutionary Equations, pages 201–228. North-Holland, 2008. doi: https://doi.
org/10.1016/S1874-5717(08)00004-2. URL https://www.sciencedirect.com/science/
article/pii/S1874571708000042."
REFERENCES,0.38461538461538464,"[15] Martin Reuter, Franz-Erich Wolter, and Niklas Peinecke. Laplace–beltrami spectra as ‘shape-
dna’of surfaces and solids. Computer-Aided Design, 38(4):342–366, 2006."
REFERENCES,0.38769230769230767,"[16] Gerhard Dziuk and Charles M Elliott. Finite element methods for surface pdes. Acta Numerica,
22:289–396, 2013."
REFERENCES,0.39076923076923076,"[17] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Kaushik Bhattacharya,
Andrew Stuart, Anima Anandkumar, et al. Fourier neural operator for parametric partial
differential equations. In International Conference on Learning Representations, 2021."
REFERENCES,0.39384615384615385,"[18] Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers.
arXiv preprint arXiv:2202.03376, 2022."
REFERENCES,0.39692307692307693,"[19] Rui Wang, Robin Walters, and Rose Yu. Incorporating symmetry into deep dynamics models
for improved generalization. arXiv preprint arXiv:2002.03061, 2020."
REFERENCES,0.4,"[20] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning mesh-
based simulation with graph networks. In International Conference on Learning Representations,
2021."
REFERENCES,0.40307692307692305,"[21] Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. Geodesic con-
volutional neural networks on riemannian manifolds. In Proceedings of the IEEE international
conference on computer vision workshops, pages 37–45, 2015."
REFERENCES,0.40615384615384614,"[22] Davide Boscaini, Jonathan Masci, Emanuele Rodolà, and Michael Bronstein. Learning shape
correspondence with anisotropic convolutional neural networks. Advances in neural information
processing systems, 29, 2016."
REFERENCES,0.40923076923076923,"[23] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and
Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model
cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
5115–5124, 2017."
REFERENCES,0.4123076923076923,"[24] Nitika Verma, Edmond Boyer, and Jakob Verbeek. Feastnet: Feature-steered graph convolutions
for 3d shape analysis. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 2598–2606, 2018."
REFERENCES,0.4153846153846154,"[25] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural
networks. In International conference on machine learning, pages 9323–9332. PMLR, 2021."
REFERENCES,0.41846153846153844,"[26] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In International Conference on Learning Representations, 2017."
REFERENCES,0.42153846153846153,"[27] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.4246153846153846,"[28] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction
networks for learning about objects, relations and physics. Advances in neural information
processing systems, 29, 2016."
REFERENCES,0.4276923076923077,"[29] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural
relational inference for interacting systems. In International conference on machine learning,
pages 2688–2697. PMLR, 2018."
REFERENCES,0.4307692307692308,"[30] Bane Sullivan and Alexander Kaszynski. PyVista: 3D plotting and mesh analysis through a
streamlined interface for the Visualization Toolkit (VTK). Journal of Open Source Software, 4
(37):1450, May 2019. doi: 10.21105/joss.01450. URL https://doi.org/10.21105/joss.
01450."
REFERENCES,0.4338461538461538,"[31] John W Cahn and John E Hilliard. Free energy of a nonuniform system. i. interfacial free energy.
The Journal of chemical physics, 28(2):258–267, 1958."
REFERENCES,0.4369230769230769,"[32] Evgeniy Khain and Leonard M Sander. Generalized cahn-hilliard equation for biological
applications. Physical review E, 77(5):051129, 2008."
REFERENCES,0.44,"[33] Andreas Rätz. A benchmark for the surface cahn–hilliard equation. Applied Mathematics
Letters, 56:65–71, 2016."
REFERENCES,0.4430769230769231,"[34] Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world
models. In International Conference on Learning Representations, 2020."
REFERENCES,0.4461538461538462,"[35] Rahul Narain, Armin Samii, and James F O’brien. Adaptive anisotropic remeshing for cloth
simulation. ACM transactions on graphics (TOG), 31(6):1–10, 2012."
REFERENCES,0.4492307692307692,"[36] Shunwang Gong, Lei Chen, Michael Bronstein, and Stefanos Zafeiriou. Spiralnet++: A fast
and highly efficient mesh convolution operator. In Proceedings of the IEEE/CVF international
conference on computer vision workshops, pages 0–0, 2019."
REFERENCES,0.4523076923076923,"[37] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International conference on machine learning,
pages 1263–1272. PMLR, 2017."
REFERENCES,0.4553846153846154,"[38] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint
arXiv:1806.01261, 2018."
REFERENCES,0.4584615384615385,"[39] Michael Garland and Paul S Heckbert. Surface simplification using quadric error metrics. In
Proceedings of the 24th annual conference on Computer graphics and interactive techniques,
pages 209–216, 1997."
REFERENCES,0.46153846153846156,"[40] Ilya Kostrikov, Zhongshi Jiang, Daniele Panozzo, Denis Zorin, and Joan Bruna. Surface
networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 2540–2548, 2018."
REFERENCES,0.4646153846153846,"[41] Yutong Feng, Yifan Feng, Haoxuan You, Xibin Zhao, and Yue Gao. Meshnet: Mesh neural
network for 3d shape representation. In Proceedings of the AAAI conference on artificial
intelligence, volume 33, pages 8279–8286, 2019."
REFERENCES,0.4676923076923077,"[42] Alon Lahav and Ayellet Tal. Meshwalker: Deep mesh understanding by random walks. ACM
Transactions on Graphics (TOG), 39(6):1–13, 2020."
REFERENCES,0.4707692307692308,"[43] Isaak Lim, Alexander Dielen, Marcel Campen, and Leif Kobbelt. A simple approach to intrinsic
correspondence learning on unstructured 3d meshes. In Proceedings of the European Conference
on Computer Vision (ECCV) Workshops, pages 0–0, 2018."
REFERENCES,0.47384615384615386,"[44] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent convolutions for
dense prediction in 3d. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 3887–3896, 2018."
REFERENCES,0.47692307692307695,"[45] Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or.
Meshcnn: a network with an edge. ACM Transactions on Graphics (TOG), 38(4):1–12, 2019."
REFERENCES,0.48,"[46] Jingwei Huang, Haotian Zhang, Li Yi, Thomas Funkhouser, Matthias Nießner, and Leonidas J
Guibas. Texturenet: Consistent local parametrizations for learning from high-resolution signals
on meshes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 4440–4449, 2019."
REFERENCES,0.48307692307692307,"[47] Francesco Milano, Antonio Loquercio, Antoni Rosinol, Davide Scaramuzza, and Luca Carlone.
Primal-dual mesh convolutional neural networks. Advances in Neural Information Processing
Systems, 33:952–963, 2020."
REFERENCES,0.48615384615384616,"[48] Lingshen He, Yiming Dong, Yisen Wang, Dacheng Tao, and Zhouchen Lin. Gauge equivariant
transformer. Advances in Neural Information Processing Systems, 34:27331–27343, 2021."
REFERENCES,0.48923076923076925,"[49] Maurice Weiler, Patrick Forré, Erik Verlinde, and Max Welling. Coordinate independent
convolutional networks–isometry and gauge equivariant convolutions on riemannian manifolds.
arXiv preprint arXiv:2106.06020, 2021."
REFERENCES,0.49230769230769234,"[50] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational physics, 378:686–707, 2019."
REFERENCES,0.49538461538461537,"[51] Maziar Raissi. Deep hidden physics models: Deep learning of nonlinear partial differential
equations. The Journal of Machine Learning Research, 19(1):932–955, 2018."
REFERENCES,0.49846153846153846,"[52] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In
International conference on machine learning, pages 3208–3216. PMLR, 2018."
REFERENCES,0.5015384615384615,"[53] Xiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady flow ap-
proximation. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining, pages 481–490, 2016."
REFERENCES,0.5046153846153846,"[54] Saakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik.
Prediction of aerodynamic flow fields using convolutional neural networks. Computational
Mechanics, 64:525–545, 2019."
REFERENCES,0.5076923076923077,"[55] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physics-
informed deep learning for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pages 1457–1466, 2020."
REFERENCES,0.5107692307692308,"[56] Shengze Cai, Zhicheng Wang, Sifan Wang, Paris Perdikaris, and George Em Karniadakis.
Physics-informed neural networks for heat transfer problems. Journal of Heat Transfer, 143(6),
2021."
REFERENCES,0.5138461538461538,"[57] Chengping Rao, Hao Sun, and Yang Liu. Physics-informed deep learning for computational
elastodynamics without labeled data. Journal of Engineering Mechanics, 147(8):04021043,
2021."
REFERENCES,0.5169230769230769,"[58] Marco Maurizi, Chao Gao, and Filippo Berto. Predicting stress, strain and deformation fields in
materials and structures with graph neural networks. Scientific Reports, 12(1):21834, 2022."
REFERENCES,0.52,"[59] Ruijin Cang, Hechao Li, Hope Yao, Yang Jiao, and Yi Ren. Improving direct physical properties
prediction of heterogeneous materials from imaging data via convolutional neural network and
a morphology-aware generative model. Computational Materials Science, 150:212–221, 2018."
REFERENCES,0.5230769230769231,"[60] Dehao Liu and Yan Wang. Multi-fidelity physics-constrained neural network and its application
in materials modeling. Journal of Mechanical Design, 141(12), 2019."
REFERENCES,0.5261538461538462,"[61] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,
Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial
differential equations. arXiv preprint arXiv:2003.03485, 2020."
REFERENCES,0.5292307692307693,"[62] Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model
reduction and neural networks for parametric pdes.
The SMAI journal of computational
mathematics, 7:121–157, 2021."
REFERENCES,0.5323076923076923,"[63] Zhiwei Fang and Justin Zhan. A physics-informed neural network framework for pdes on 3d
surfaces: Time independent problems. IEEE Access, 8:26328–26335, 2019."
REFERENCES,0.5353846153846153,"[64] Zhuochao Tang, Zhuojia Fu, and Sergiy Reutskiy. An extrinsic approach based on physics-
informed neural networks for pdes on surfaces. Mathematics, 10(16):2861, 2022."
REFERENCES,0.5384615384615384,"[65] Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural oper-
ator with learned deformations for pdes on general geometries. arXiv preprint arXiv:2207.05209,
2022."
REFERENCES,0.5415384615384615,"[66] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,
Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differen-
tial equations. arXiv preprint arXiv:2010.08895, 2020."
REFERENCES,0.5446153846153846,"[67] Julian Suk, Pim de Haan, Phillip Lippe, Christoph Brune, and Jelmer M Wolterink. Mesh neural
networks for se (3)-equivariant hemodynamics estimation on the artery wall. arXiv preprint
arXiv:2212.05023, 2022."
REFERENCES,0.5476923076923077,"[68] Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. Advances
in Neural Information Processing Systems, 33:17009–17021, 2020."
REFERENCES,0.5507692307692308,"[69] O. Tange. Gnu parallel - the command-line power tool. ;login: The USENIX Magazine, 36(1):
42–47, Feb 2011."
REFERENCES,0.5538461538461539,"[70] Martin Reuter, Silvia Biasotti, Daniela Giorgi, Giuseppe Patanè, and Michela Spagnuolo. Dis-
crete laplace–beltrami operators for shape analysis and segmentation. Computers & Graphics,
33(3):381–390, 2009."
REFERENCES,0.556923076923077,"[71] Will Schroeder, Kenneth M Martin, and William E Lorensen. The visualization toolkit an
object-oriented approach to 3D graphics. Prentice-Hall, Inc., 1998."
REFERENCES,0.56,"[72] A. Logg and G. N. Wells. DOLFIN: automated finite element computing. ACM Transactions
on Mathematical Software, 37, 2010. doi: 10.1145/1731022.1731030."
REFERENCES,0.563076923076923,"[73] Carl T Kelley. Solving nonlinear equations with Newton’s method. SIAM, 2003."
REFERENCES,0.5661538461538461,"[74] Federica Bogo, Javier Romero, Matthew Loper, and Michael J. Black. FAUST: Dataset and
evaluation for 3D mesh registration. In Proceedings IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), Piscataway, NJ, USA, June 2014. IEEE."
REFERENCES,0.5692307692307692,"[75] Ronald J Williams and David Zipser. Gradient-based learning algorithms for recurrent. Back-
propagation: Theory, architectures, and applications, 433:17, 1995."
REFERENCES,0.5723076923076923,"A
Background"
REFERENCES,0.5753846153846154,"A.1
Local Coordinate Frame"
REFERENCES,0.5784615384615385,"As discussed in the main text, the reference neighbor d defines the basis {eb
1, eb
2} for the tangent space
TbM (see Figure 1a). Denote by logb(xd) = ProjTbM(xd) −xb the vector from xb (the coordinates
of b) to the orthogonal projection of xd ∈R3 (the coordinates of d) onto the tangent plane TbM. Let
eb
1 = logb(xd)/∥logb(xd)∥and eb
2 = nb × eb
1. Then the orientation of every neighbor v ∈Nb can
be represented with polar angles, where"
REFERENCES,0.5815384615384616,"θv = atan2

eb
2
⊤logb(v), eb
1
⊤logb(v)

(8)"
REFERENCES,0.5846153846153846,is the angle between v and the reference neighbor after projection to the tangent plane.
REFERENCES,0.5876923076923077,"A.2
Parallel Transport"
REFERENCES,0.5907692307692308,"Let fp and fq be feature vectors at nodes p, q, respectively, where q is adjacent to p. As features at
different nodes live in different tangent spaces and have different bases, in order for the anisotropic
kernel to be applied to fq for each q ∈Np, each fq must be parallel transported to TpM and be in
the same gauge."
REFERENCES,0.5938461538461538,"Let fp and fq be feature vectors at nodes p, q, respectively, where q is adjacent to p. As mentioned in
the main text, the parallel transporter gq→p first aligns the tangent plane TpM to the tangent plane
TqM by a 3D rotation and then transforms the gauge at q to the gauge at p by a planar rotation."
REFERENCES,0.5969230769230769,"Let np, nq be the normal at p and q, and Rqp ∈SO(3) be the 3D rotation required to align TqM and
TpM. Then gq→p ∈[0, 2π) is given by"
REFERENCES,0.6,"gq→p = atan2
 
(Rqpeq
2)⊤ep
1, (Rqpeq
1)⊤ep
1

,
(9)"
REFERENCES,0.6030769230769231,"where (eq
1, eq
2) and (ep
1, ep
2) are the frames at node q and p, respectively."
REFERENCES,0.6061538461538462,"B
Gauge-Equivariant Message Passing Methods"
REFERENCES,0.6092307692307692,"In this section, we provide the architectures of GemCNN and EMAN for the wave dataset to compare
with Hermes (see Figure 2), and prove that Hermes is equivariant to local gauge transformations."
REFERENCES,0.6123076923076923,"B.1
GemCNN"
REFERENCES,0.6153846153846154,"The architecture for GemCNN is given in Figure 6. Note that while the equations in GemCNN
(see Equations 1) allow for self-interactions, the implemented architecture does not actually use
self-interaction kernels, possibly due to residual connections."
REFERENCES,0.6184615384615385,"Figure 6: GemCNN network architecture for the Wave PDE dataset. There are three message
blocks, each with 2 gauge-equivariant convolutional layers. To illustrate the computations within the
convolutional layer, we use the example mesh from Figure 1a and show only the computations for
node b for clarity."
REFERENCES,0.6215384615384615,"B.2
EMAN"
REFERENCES,0.6246153846153846,"For completeness, the full equations for EMAN are given in Equations (10)-(12) (see also Appendix
C.5 in [10])."
REFERENCES,0.6276923076923077,"Figure 7: EMAN network architecture for the Wave PDE dataset. There are three message blocks,
each with 2 attention and gauge-equivariant convolutional layers. To illustrate the computations
within the edge and node networks, we use the example mesh from Figure 1a and show only the
computations for node b for clarity."
REFERENCES,0.6307692307692307,"f ′
p = αppKself
valuefp +
X"
REFERENCES,0.6338461538461538,"q∈Np
αpqKneigh
value (θq)ρ(gq→p)fq,
(10)"
REFERENCES,0.6369230769230769,"αpp = softmax  
"
REFERENCES,0.64,"
Kself
keyfp
⊤
(Kqueryfp)
√Catt "
REFERENCES,0.6430769230769231,"
,
(11)"
REFERENCES,0.6461538461538462,"αpq = softmax  
"
REFERENCES,0.6492307692307693,"
∥q∈Np Kneigh
key (θpq)ρ(gq→p)fq
⊤
(Kqueryfp)
√Catt "
REFERENCES,0.6523076923076923,"
,
(12)"
REFERENCES,0.6553846153846153,"where Ki
key, Ki
value, i ∈{self, neigh} are the usual key and value matrices for the self-interaction and
neighbor interaction terms, and Kquery is the query matrix. The ∥q∈Np denotes a concatenation of the
vectors for all q ∈Np."
REFERENCES,0.6584615384615384,"Figure 7 shows the architecture for EMAN. There is an EMANAttention layer to compute the
attention weights αpq, which are passed to the aggregation function. Similarly to GemCNN, the
implemented architecture does not use self-interaction kernels."
REFERENCES,0.6615384615384615,"B.3
Hermes"
REFERENCES,0.6646153846153846,"We provide the proof of Proposition 1 (stated here again for convenience). Part of the proof uses the
fact that anisotropic convolutions over the tangent space are equivariant under the appropriate kernel
constraint as shown in de Haan et al. [9]."
REFERENCES,0.6676923076923077,Proposition 2. Hermes is equivariant to local gauge transformations gp for any p ∈V of a mesh.
REFERENCES,0.6707692307692308,"Proof. Using Equations (3)-(7), we first consider the simpler case where the edge and node networks
have one gauge-equivariant layer each, i.e. Ne = Nn = 1. Let G = SO(2) be the gauge group.
Let p ∈V be some vertex and q ∈Np be some vertex adjacent to p. The inputs to the edge"
REFERENCES,0.6738461538461539,"network are fp, fq, epq (the source node, target node, and edge features, respectively), which are all
representations of G. With respect to the gauge transformation gp ∈G, they transform as"
REFERENCES,0.676923076923077,"fp 7→ρin(−gp)fp,
ρ(gq→p)fq 7→ρin(−gp)ρ(gq→p)fq,
epq 7→ρin(−gp)epq,"
REFERENCES,0.68,"where fq is first parallel transported to the gauge at p before applying the group action (epq is assumed
to already be in this gauge). As hpq is the direct product of fp, ρ(gq→p)fq, epq (Equation 3), it also
transforms as hpq 7→ρin(−gp)hpq. For the edge network to be equivariant with respect to gp, we need"
REFERENCES,0.683076923076923,"to show that σ

K1
neigh(θq −gp)(ρin(−gp)hpq)

= ρout(−gp)

σ

K1
neigh(θq)hpq

. We compute
the left-hand side"
REFERENCES,0.6861538461538461,"σ
 
K1
neigh(θq −gp)(ρin(−gp)hpq)

= σ
 
ρout(−gp)K1
neigh(θq)ρin(gp)ρin(−gp)hpq
"
REFERENCES,0.6892307692307692,"= ρout(−gp)σ
 
K1
neigh(θq)hpq
"
REFERENCES,0.6923076923076923,"as desired, where we have used the kernel constraint K1
neigh(θq−gp) = ρout(−gp)K1
neigh(θq)ρin(gq→p)
and the fact that σ is equivariant with respect to the ρout representation of G and thus commutes with
ρout(−gp)."
REFERENCES,0.6953846153846154,"For the node network, K1
self does not depend on θq and so the kernel constraint is simpler: K1
self =
ρout(−gp)K1
selfρin(gp). It is easy to see that σ
 
K1
selfρin(−gp)hp

= ρout(−gp)
 
σ
 
K1
selfhp

and we
omit the proof as it is nearly identical to that of the edge network."
REFERENCES,0.6984615384615385,"The remaining part is the aggregation (sum) (Equation 5). As ρ(−gp) is a linear group action
(representation) of G, ρ(−gp)mp = P"
REFERENCES,0.7015384615384616,q∈Np ρ(−gp)mpq.
REFERENCES,0.7046153846153846,"We have so far proved that Hermes is equivariant for Ne = Nn = 1. This suffices to prove for all
Ne ≥1, Nn ≥1. The edge network ϕe is a composition of the gauge-equivariant convolutions
and activations interleaved with each other. The regular nonlinearity is gauge-equivariant when the
number of intermediate samples used in the discrete Fourier transform goes to ∞and is approximately
gauge-equivariant with finite N. As the composition of equivariant functions is also equivariant, ϕe
is equivariant to any gp for any Ne ≥1, and similarly, ϕn is equivariant to any gp for Nn ≥1. Thus
Hermes is equivariant to any local gauge transformation gp ∈G, in the limit N →∞."
REFERENCES,0.7076923076923077,"C
Experiment Domains"
REFERENCES,0.7107692307692308,"C.1
Heat/Wave Equation"
REFERENCES,0.7138461538461538,The heat and wave partial differential equations are given by ∂u
REFERENCES,0.7169230769230769,"∂t = α˜Lu,
(Heat)
∂2u"
REFERENCES,0.72,"∂t2 = c2 ˜Lu,
(Wave)
(13)"
REFERENCES,0.7230769230769231,"where u is the temperature or the displacement for the heat and wave equations respectively, α is
thermal diffusivity, c is a constant, and ˜L is the symmetric cotangent Laplacian [70]. Note that ˜L
is the discrete version of the Laplace-Beltrami operator over a Riemannian manifold. For a scalar
function u, ˜L(u) is given as,"
REFERENCES,0.7261538461538461,"(˜L(u))i =
1
2Ai X"
REFERENCES,0.7292307692307692,"j∈N(i)
(cot αij + cot βij)(uj −ui),
(14)"
REFERENCES,0.7323076923076923,"where N(i) denotes the adjacent vertices of i, αij and βij are the angles opposite edge (i, j), and Ai
is the vertex area of i, where we use the barycentric cell area."
REFERENCES,0.7353846153846154,"We use 8 example meshes from the PyVista software [30] and use 6 for training and 2 for testing
generalization (test mesh) to unseen meshes. We further generate test sets to evaluate generalization
on future timesteps (test time) and random initialization (test init) conditions. We simulate 5 samples
for Tmax = 200 timesteps, using 3 samples from T = 0, . . . , 150 for training and the remaining
T = 151, . . . , 200 as test time samples. The unseen 2 samples from T = 0, . . . , 200 are used for test
init samples. Some meshes contained too many nodes to be computationally feasible so we simplify
these meshes to be less than 5,000 nodes using quadric decimation [71]. We use α = c = 1. See
Tables 4, 5 for visualizations of the meshes and how the features evolve over time."
REFERENCES,0.7384615384615385,"The heat and wave equations were simulated using the finite difference method using the forward
Euler scheme. As each mesh has a different Laplacian, we tune the time discretization dt for each
mesh. To initialize, we randomly generate Gaussian bumps over the mesh (20% of all nodes for
heat, 2.5% for wave). While such initialization is not necessarily realistic, we chose to incorporate
more superposition and interference to generate more diverse dynamics. The task is to predict the
temperature (heat) or displacement (wave) at each vertex at the next time step, given a short history
of values. Both the inputs and outputs are trivial representations."
REFERENCES,0.7415384615384616,"C.2
Cahn-Hilliard Equation"
REFERENCES,0.7446153846153846,"The Cahn-Hilliard equation [31] is often represented by the following two coupled second-order
equations:"
REFERENCES,0.7476923076923077,"∂c
∂t −ML(µ) = 0,
µ −df"
REFERENCES,0.7507692307692307,"dc + λL(c) = 0,
(15)"
REFERENCES,0.7538461538461538,"where c is the fluid concentration, M is the diffusion coefficient, µ is the chemical potential, L is the
Laplacian, f is the double-well free energy function, and λ is a positive constant."
REFERENCES,0.7569230769230769,"We use 4 meshes from the PyVista software [30] and use 3 for training and 1 for testing (test mesh).
As in the heat and wave datasets, we use N = 5 samples with Tmax = 200 timesteps and split the
training and test datasets similarly. Some meshes contained too many nodes to be computationally
feasible so we simplify such meshes using the same procedure as in the heat and wave datasets. See
Tables 6 for visualizations."
REFERENCES,0.76,"The Cahn-Hilliard equation was simulated using the DOLFINx software [72] with time discretization
dt = 5 × 10−6, M = 1, f = 100c2(1 −c2), and we use the θ-method with θ = 0.5. We
randomize λ uniformly from [0.01, 0.02] for each data sample. The initial concentrations are randomly
sampled from N(0.6, 0.05) for each vertex. The solver formulates the coupled second-order PDEs in
variational form, and solves it using the Newton-Krylov method [73]."
REFERENCES,0.7630769230769231,"The task is to predict the concentration at each vertex at the next time step, given a short history of
values. Both the inputs and outputs are trivial representations."
REFERENCES,0.7661538461538462,"C.3
Shape Correspondence"
REFERENCES,0.7692307692307693,"The FAUST dataset [74] consists of 80 train and 20 test samples, where 2 unseen human subjects are
used for testing. Following previous work [9], the inputs are xyz coordinates with representation ρ0
and the network outputs trivial features, followed by linear post-processing layers. The task is shape
correspondence between meshes, where for every mesh, each vertex is registered and denotes the
same position on the body."
REFERENCES,0.7723076923076924,"C.4
Object interactions"
REFERENCES,0.7753846153846153,"In this domain, we consider 50 objects on a mesh with 250 vertices, see Figure 8. Each mesh is
generated parametrically using the random hills generation procedure provided in VTK [71]. For
training, we generate 20 meshes using different random seeds but the same random hills parameters,
randomly initialize the object positions and orientations, and then simulate for a 100 timesteps. At
each timestep, we select the action with weighted probabilities: 10% turn left, 80% move forward, and
10% turn right. For testing, we generate 100 meshes with randomized hill parameters and simulate
forward 20 timesteps. We represent the current state of the system using the vertex coordinates, a
one-hot embedding of object occupancy, and object orientations. The orientations are 2-dimensional
vectors that represent the heading of the object on the tangent place at the current vertex, pointing
towards the position of the neighboring node projected onto the tangent plane."
REFERENCES,0.7784615384615384,"The task is to correctly predict the object occupancy and orientations of the next timestep, given the
current state and action f(st, at) = ˆst+1. The inputs to the graph network are ρ0 features, except
for the orientations, which are ρ1. The graph network outputs both the logits of the occupancy
and the orientation as ρ0 and ρ1 features, respectively. As the occupancy is discrete, we add a
linear post-processing layer after the graph network for only the occupancy outputs and compute the
negative log likelihood given the ground truth occupancy. For orientations, we use root mean square
error (RMSE) from the orientations given by the graph network."
REFERENCES,0.7815384615384615,"Table 4: Heat equation dataset of all meshes used with Tmax = 200. We simulate N = 5 samples for
each mesh. Of the training meshes, we use T = 150, . . . , 200 as the test time dataset and 2 unseen
samples as the test init dataset. The armadillo and urn meshes were used as the test mesh dataset."
REFERENCES,0.7846153846153846,"Object
Split
No. vertices
t=0
t=50
t=200"
REFERENCES,0.7876923076923077,"Armadillo
Test
1,731"
REFERENCES,0.7907692307692308,"Bunny
Train
502"
REFERENCES,0.7938461538461539,"Lucy
Train
2,501"
REFERENCES,0.796923076923077,"Sphere
Train
842"
REFERENCES,0.8,"Spider
Train
4,670"
REFERENCES,0.803076923076923,"Urn
Train
2,454"
REFERENCES,0.8061538461538461,"Woman
Train
2,998"
REFERENCES,0.8092307692307692,"Table 5: Wave equation dataset of all meshes used with Tmax = 200. We simulate N = 5 samples for
each mesh. Of the training meshes, we use T = 150, . . . , 200 as the test time dataset and 2 samples
as the test init dataset. The armadillo and urn meshes were used as the test mesh dataset."
REFERENCES,0.8123076923076923,"Object
Split
No. vertices
t=0
t=50
t=200"
REFERENCES,0.8153846153846154,"Armadillo
Test
1,731"
REFERENCES,0.8184615384615385,"Bunny
Train
502"
REFERENCES,0.8215384615384616,"Lucy
Train
2,501"
REFERENCES,0.8246153846153846,"Sphere
Train
842"
REFERENCES,0.8276923076923077,"Spider
Train
4,670"
REFERENCES,0.8307692307692308,"Urn
Train
2,454"
REFERENCES,0.8338461538461538,"Woman
Train
2,998"
REFERENCES,0.8369230769230769,"Table 6: Cahn-Hilliard equation dataset of all meshes used with Tmax = 200. We simulate N = 5
samples for each mesh. Of the training meshes, we use T = 150, . . . , 200 as the test time dataset and
2 samples as the test init dataset. The armadillo and urn meshes were used as the test mesh dataset."
REFERENCES,0.84,"Object
Split
No. vertices
T=0
T=50
T=200"
REFERENCES,0.8430769230769231,"Bunny
Train
502"
REFERENCES,0.8461538461538461,"Ellipsoid
Train
1,942"
REFERENCES,0.8492307692307692,"Sphere
Train
842"
REFERENCES,0.8523076923076923,"Supertoroid
Test
2,940"
REFERENCES,0.8553846153846154,"Figure 8: Objects interactions on a mesh. Purple denotes an unoccupied vertex and other colors
represent different objects. Images are different random initializations of the underlying mesh and
object position and orientations."
REFERENCES,0.8584615384615385,"D
Training Details and Baselines"
REFERENCES,0.8615384615384616,"For classification tasks such as FAUST or for predicting object occupancy in the Objects dataset,
post-processing linear layers are used. For FAUST, we find that training for longer (400 epochs vs
100 from [9]) benefits all models. We use cross entropy loss for FAUST, a combination of binary
cross entropy and RMSE for Objects, and RMSE for all the PDE datasets. For the PDE datasets, we
use truncated backpropagation through time [75] and roll out the network predictions for 3 timesteps
using a history of 5 timesteps."
REFERENCES,0.8646153846153846,"We use the regular nonlinearity [9] to maintain gauge equivariance, with ReLU activations and 101
samples, for all models. We use a band limit of 4 for the gauge-equivariant kernels. Throughout,"
REFERENCES,0.8676923076923077,"Table 7: Details of architecture and hyperparameters
Dataset
FAUST
Objects
Heat
Wave
Cahn-Hilliard"
REFERENCES,0.8707692307692307,"No. blocks
5
1
1
2
1
No. layers in ϕe per block
2
3
4
2
4
No. layers in ϕn per block
0
0
3
1
1
Hidden representation
⊕2
i=010ρi
⊕2
i=012ρi
⊕2
i=012ρi
⊕2
i=012ρi
⊕2
i=012ρi
No. post processing layers
2
1
0
0
0
Epochs
400
100
100
100
100
Batch size
1
2
1
1
1
Optimizer
Adam
Learning rate
7 × 10−3
5 × 10−4
1 × 10−4
5 × 10−4
5 × 10−3"
REFERENCES,0.8738461538461538,Table 8: Comparison table of relevant graph neural networks and their features.
REFERENCES,0.8769230769230769,"Graph Flavors
Features"
REFERENCES,0.88,"Models
Convolutional
Attentional
Message Passing
E(3)-Equivariant
Gauge-Equivariant
Mesh Aware"
REFERENCES,0.8830769230769231,"GemCNN
✓
✓
✓"
REFERENCES,0.8861538461538462,"EMAN
✓
✓
(w/ RelTan features)
✓
✓"
REFERENCES,0.8892307692307693,"Hermes
✓
✓
✓"
REFERENCES,0.8923076923076924,"GCN
✓
MPNN
✓
MeshGraphNet
✓
✓
EGNN
✓
✓
SpiralNet++
✓
✓"
REFERENCES,0.8953846153846153,"Table 9: Number of learnable parameters
FAUST
Objects
Heat
Wave
Cahn-Hilliard
Flag"
REFERENCES,0.8984615384615384,"GemCNN
1,829,658
161,715
40,337
40,337
29,775
-
EMAN
1,839,476
165,013
40,093
40,093
30,882
-
Hermes
1,837,776
162,976
40,544
40,695
23,122
51,555"
REFERENCES,0.9015384615384615,"GCN
-
-
41,401
41,401
29,401
-
MPNN
-
-
41,449
41,449
29,673
-
MeshGraphNet
-
-
44,993
44,993
35,457
45,313
EGNN
-
-
50,433
50,433
28,609
-
SpiralNet++
-
-
63,841
63,841
27,153
-"
REFERENCES,0.9046153846153846,"L2 regularization of the weights with a coefficient of 1 × 10−5 was used. A cosine learning rate
scheduler was used for the Objects and Cahn-Hilliard datasets."
REFERENCES,0.9076923076923077,"Table 7 contains the architecture details and hyperparameters used for each domain. Table 8 lists
each method’s flavor of message passing and its features. Table 9 shows the number of learnable
parameters used for each method and domain."
REFERENCES,0.9107692307692308,"E
Results"
REFERENCES,0.9138461538461539,"Table 10 shows the results on FAUST, Objects, and FlagSimple datasets. On FAUST, GemCNN
outperforms both EMAN and Hermes. We obtained slightly different results for EMAN than reported
in the paper [10]. On Objects, Hermes outperforms GemCNN and EMAN, albeit slightly. On
FlagSimple, Hermes considerably outperforms MeshGraphNet."
REFERENCES,0.916923076923077,Table 10: Test results for each method using 5 runs. Gray numbers are 95% confidence intervals.
REFERENCES,0.92,"GemCNN
EMAN
Hermes
MeshGraphNet"
REFERENCES,0.9230769230769231,"FAUST
Accuracy (%)
99.1±0.1
97.5±0.4
97.5±0.2
-"
REFERENCES,0.9261538461538461,"OBJECTS
Accuracy (%)
98.8±0.1
96.9±0.5
99.7±0.0
-
RMSE (×10−2)
6.40±0.1
6.43±0.0
6.26±0.0
-"
REFERENCES,0.9292307692307692,"FLAGSIMPLE
RMSE (×10−3)
-
-
5.87±0.0
9.01±0.1"
REFERENCES,0.9323076923076923,"E.1
Long-horizon prediction rollouts"
REFERENCES,0.9353846153846154,"Table 11 shows additional qualitative samples generated autoregressively using only the initial
conditions for T + 10, T + 30, T + 100 timesteps. Hermes predicts large spatial patterns accurately
and outperforms GemCNN and EMAN on all datasets."
REFERENCES,0.9384615384615385,"Table 11: Qualitative predictions rolled out to 10, 30, 100 timesteps using only initial conditions."
REFERENCES,0.9415384615384615,"Timestep
GemCNN
EMAN
Hermes
Ground Truth HEAT T+10 T+30 T+100 WAVE T+10 T+30 T+100"
REFERENCES,0.9446153846153846,CAHN-HILLIARD T+10 T+30 T+100
REFERENCES,0.9476923076923077,"E.2
Mesh Fineness"
REFERENCES,0.9507692307692308,"Figure 9 shows visualizations of the different mesh resolutions used and Figure 10 shows the results
on the wave dataset. Hermes still outperforms GemCNN and EMAN at each resolution. Unlike the
heat dataset (Figure 4), there is an unexpected overall decreasing RMSE trend with a coarser mesh."
REFERENCES,0.9538461538461539,"(a) 348
(b) 867
(c) 1,731
(d) 3,461
(e) 8,650"
REFERENCES,0.9569230769230769,Figure 9: Visualization of different mesh resolutions. The captions denote the number of vertices.
REFERENCES,0.96,"Figure 10: Performance for varying mesh fineness for wave on the test time (left) and test init (right)
datasets. Error bars denote standard error over 3 runs. The number of vertices decreases with an
increasing reduction ratio (see Figure 9)."
REFERENCES,0.963076923076923,"E.3
Surface Roughness"
REFERENCES,0.9661538461538461,"Figure 11 shows visualizations of the different mesh roughness and Figure 12 shows the results on
the wave dataset. Hermes outperforms GemCNN and EMAN at most roughness scales."
REFERENCES,0.9692307692307692,"(a) 0.1
(b) 0.5
(c) 1.0
(d) 1.5
(e) 3.0"
REFERENCES,0.9723076923076923,Figure 11: Visualization of different mesh roughnesses. The captions denote the roughness scale.
REFERENCES,0.9753846153846154,"Figure 12: Performance for varying surface roughness for wave on the test time (left) and test init
(right) datasets. Error bars denote standard error over 3 runs. Increasing scale increases the surface
roughness of the mesh (see Figure 11)."
REFERENCES,0.9784615384615385,"E.4
Ablation on Residual Connection"
REFERENCES,0.9815384615384616,"Table 12 shows the ablation experiment with the residual connection. The results are mixed: having a
residual helps on Heat but not on Wave or Cahn-Hilliard. The decision to have a residual connection
should thus be considered a task-dependent hyperparameter."
REFERENCES,0.9846153846153847,Table 12: Ablation study on residual connection
REFERENCES,0.9876923076923076,"With
residual
Without
residual"
REFERENCES,0.9907692307692307,"HEAT
Test time (×10−3)
1.18±0.3
1.24±0.5
Test init (×10−3)
1.16±0.3
1.22±0.5
Test mesh (×10−3)
1.01±0.3
1.05±0.4"
REFERENCES,0.9938461538461538,"WAVE
Test time (×10−3)
5.43±0.8
5.17±0.6
Test init (×10−3)
3.72±1.3
2.38±0.3
Test mesh (×10−3)
3.79±1.3
2.52±0.4"
REFERENCES,0.9969230769230769,"CAHN-HILLIARD
Test time (×10−3)
4.23±0.9
3.08±0.8
Test init (×10−3)
5.21±1.2
3.53±1.3
Test mesh (×10−3)
5.34±0.7
5.73±1.2"
