Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018181818181818182,"We consider finite episodic Markov decision processes aiming at the entropic risk
1"
ABSTRACT,0.0036363636363636364,"measure (EntRM) of return for risk-sensitive control. We identify two properties of
2"
ABSTRACT,0.005454545454545455,"the EntRM that enable risk-sensitive distributional dynamic programming. We pro-
3"
ABSTRACT,0.007272727272727273,"pose two novel distributional reinforcement learning (DRL) algorithms, including
4"
ABSTRACT,0.00909090909090909,"a model-free one and a model-based one, that implement optimism through two
5"
ABSTRACT,0.01090909090909091,different schemes. We prove that both of them attain ˜O( exp(|β|H)−1
ABSTRACT,0.012727272727272728,"|β|H
H
√"
ABSTRACT,0.014545454545454545,"HS2AT)
6"
ABSTRACT,0.016363636363636365,"regret upper bound, where S is the number of states, A the number of states, H
7"
ABSTRACT,0.01818181818181818,"the time horizon and T the number of total time steps. It matches RSVI2 proposed
8"
ABSTRACT,0.02,"in [22] with a much simpler regret analysis. To the best of our knowledge, this is
9"
ABSTRACT,0.02181818181818182,"the first regret analysis of DRL, which theoretically verifies the efficacy of DRL
10"
ABSTRACT,0.023636363636363636,"for risk-sensitive control. Finally, we improve the existing lower bound by proving
11"
ABSTRACT,0.025454545454545455,a tighter bound of Ω( exp(βH/6)−1
ABSTRACT,0.02727272727272727,"βH
H
√"
ABSTRACT,0.02909090909090909,"SAT) for β > 0 case, which recovers the
12"
ABSTRACT,0.03090909090909091,"tight lower bound Ω(H
√"
ABSTRACT,0.03272727272727273,"SAT) in the risk-neutral setting.
13"
INTRODUCTION,0.034545454545454546,"1
Introduction
14"
INTRODUCTION,0.03636363636363636,"Standard reinforcement learning (RL) [45] seeks to find an optimal policy that maximizes the
15"
INTRODUCTION,0.038181818181818185,"expectation of return. It is also called risk-neutral RL since the objective is the mean functional of
16"
INTRODUCTION,0.04,"the return distribution. However, in some high-stakes applications including finance [15, 6], medical
17"
INTRODUCTION,0.04181818181818182,"treatment [21] and operations [16] etc, the decision-maker tends to be risk-sensitive with the goal of
18"
INTRODUCTION,0.04363636363636364,"maximizing some risk measure of return distribution.
19"
INTRODUCTION,0.045454545454545456,"In this paper, we consider the problem of optimizing the exponential risk measure (EntRM) in the
20"
INTRODUCTION,0.04727272727272727,"episodic and finite MDP setting for risk-sensitive control. The entropic risk measure can trade-off
21"
INTRODUCTION,0.04909090909090909,"between the expectation and the variance, and adjusts the risk-sensitiveness by control a risk parameter
22"
INTRODUCTION,0.05090909090909091,"(see Equation 1). Ever since the seminal work of [29], risk-sensitive RL based on the EntRM has
23"
INTRODUCTION,0.05272727272727273,"been applied across a wide range of domains [43, 37, 27]. Most of the existing approaches, however,
24"
INTRODUCTION,0.05454545454545454,"involve complicated algorithmic design to deal with the non-linearity of the EntRM.
25"
INTRODUCTION,0.056363636363636366,"Distributional reinforcement learning (DRL) [4] has demonstrated its superior performance over
26"
INTRODUCTION,0.05818181818181818,"traditional methods in some difficult tasks [14, 13] under risk-neutral setting. Different from the
27"
INTRODUCTION,0.06,"value-based approaches, it learns the whole return distribution instead of a real-valued value function.
28"
INTRODUCTION,0.06181818181818182,"Given the entire return distribution, it is natural to leverage the distributional information to optimize
29"
INTRODUCTION,0.06363636363636363,"a risk measure other than expectation [13, 44, 33]. Despite of the intrinsic connection between DRL
30"
INTRODUCTION,0.06545454545454546,"and risk-sensitive RL, it is surprising that existing works on risk-sensitive control via DRL approaches
31"
INTRODUCTION,0.06727272727272728,"([13, 34, 1]) lack regret analysis. Consequently, it is challenging to evaluate and improve these DRL
32"
INTRODUCTION,0.06909090909090909,"algorithms in terms of sample-efficiency, which brings about a reasonable question
33"
INTRODUCTION,0.07090909090909091,"Can distributional reinforcement learning attain near-optimal regret for risk-sensitive control?
34"
INTRODUCTION,0.07272727272727272,"In this work, we answer this question positively by providing two DRL algorithms with provably
35"
INTRODUCTION,0.07454545454545454,"regret guarantees. We devise two novel DRL algorithms with principled exploration schemes for
36"
INTRODUCTION,0.07636363636363637,"risk-sensitive control in the tabular MDP setting. In particular, the proposed algorithms implement
37"
INTRODUCTION,0.07818181818181819,"the principle of optimism in the face of uncertainty (OFU) at the distributional level to balance the
38"
INTRODUCTION,0.08,"exploration-exploitation trade-off. By providing the first regret analysis of DRL, we theoretically
39"
INTRODUCTION,0.08181818181818182,"verifies the efficacy of DRL for risk-sensitive control. Therefore, our work bridge the gap between
40"
INTRODUCTION,0.08363636363636363,"DRL and risk-sensitive RL with regard to sample complexity.
41"
INTRODUCTION,0.08545454545454545,"Main contributions.
We summarize our main contributions in the following.
42"
INTRODUCTION,0.08727272727272728,"1. We build a risk-sensitive distributional dynamic programming (RS-DDP) framework. To be more
43"
INTRODUCTION,0.0890909090909091,"specific, we choose the entropic risk measure (EntRM) of the return distribution as our objective. By
44"
INTRODUCTION,0.09090909090909091,"identifying two key properties of EntRM, We establish distributional Bellman optimality equation for
45"
INTRODUCTION,0.09272727272727273,"risk-sensitive control.
46"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.09454545454545454,"2. We propose two DRL algorithms that enforce the OFU principle in a distributional fashion through
47"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.09636363636363636,two different schemes. We provide ˜O( exp(|β|H)−1
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.09818181818181818,"|β|
H
√"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.1,"S2AK) regret upper bound, which matches
48"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.10181818181818182,"the best existing result of RSVI2 in [22]. It is the first regret analysis of DRL algorithm in the
49"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.10363636363636364,"finite episodic MDP in the risk-sensitive setting. Compared to [22], our algorithm does not involve
50"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.10545454545454545,"complicated bonus design, and our analysis are conceptually cleaner and easier to interpret.
51"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.10727272727272727,"3. We fill the gaps in the proof of lower bound in [23]. To the best of our knowledge, [23] only
52"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.10909090909090909,"implies a lower bound Ω( exp(|β|H/2)−1 |β|
√"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.11090909090909092,"K) rather the claimed bound Ω( exp(|β|H/2)−1 |β|
√"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.11272727272727273,"T). The
53"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.11454545454545455,"resulting lower bound is independent of S and A and is loose with a factor of
√"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.11636363636363636,"H. We overcome
54"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.11818181818181818,these issues by proving a tight lower bound of Ω( exp(βH/6)−1
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.12,"βH
H
√"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.12181818181818181,"SAT) for β > 0. Note that the
55"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.12363636363636364,"lower bound is tight in the risk-neutral setting (β →0).
56"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.12545454545454546,"Related work.
Following the paper [4], DRL has witnessed a rapid growth of study in literature
57"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.12727272727272726,"[40, 14, 13, 2, 32]. Most of these works focus on improving the performance in the risk-neutral
58"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.1290909090909091,"setting, with a few exceptions [13, 34, 1]. However, none of these works study the sample complexity.
59"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.13090909090909092,"A rich body of work studies risk-sensitive RL with the EntRM [7, 8, 10, 9, 3, 11, 12, 18, 17, 19,
60"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.13272727272727272,"24, 28, 30, 33, 35, 36, 38, 39, 42, 43]. In particular, [29] is the first to introduce the ERM as risk-
61"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.13454545454545455,"sensitive objective in MDP. However, they either assume known transition and reward or consider
62"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.13636363636363635,"infinite-horizon setting without sample-complexity considerations.
63"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.13818181818181818,"Two works are closely related to ours [23, 22] under precisely the same setting. [23] is the first to
64"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.14,"study the risk-sensitive episodic MDP, which provides the first algorithms and regret guarantees.
65"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.14181818181818182,"Nevertheless, the regret upper bounds contain a dispensable factor of exp(|β|H2). Additionally, their
66"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.14363636363636365,"lower bound proof contains mistakes, and the corrected proof suggests a weaker bound. [22] improves
67"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.14545454545454545,"the algorithm by removing the additional O(exp(|β|H2)) factor. However, the regret analysis is
68"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.14727272727272728,"complicated, and the lower bound is not fixed. A very recent work ([1]) independently proposes a
69"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.14909090909090908,"risk-sensitive DDP framework, but their work is fundamentally different from ours. The risk measure
70"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.1509090909090909,"considered in [1] is the conditional value at risk (CVaR), and they focus on the infinite horizon setting.
71"
WE PROPOSE TWO DRL ALGORITHMS THAT ENFORCE THE OFU PRINCIPLE IN A DISTRIBUTIONAL FASHION THROUGH,0.15272727272727274,"Due to the space limit, we provide detailed comparisons with [23, 22, 1] in Appendix A.
72"
PRELIMINARIES,0.15454545454545454,"2
Preliminaries
73"
PRELIMINARIES,0.15636363636363637,"Notations.
We write [M : N] ≜{M, M + 1, ..., N} and [N] ≜[1 : N] for any positive integers
74"
PRELIMINARIES,0.15818181818181817,"M ≤N. We adopt the convention that Pm
i=n ai ≜0 if n > m and Qm
i=n ai ≜1 if n > m. We
75"
PRELIMINARIES,0.16,"use I{·} to denote the indicator function. For any x ∈R, we define [x]+ ≜max{x, 0}. We define
76"
PRELIMINARIES,0.1618181818181818,"the step function with parameter c as ψc(x) ≜I{x ≥c}. Note that ψc represents the CDF of a
77"
PRELIMINARIES,0.16363636363636364,"deterministic variable taking value c. We denote by D([a, b]), DM and D the set of distributions
78"
PRELIMINARIES,0.16545454545454547,"supported on [a, b], [0, M] and the set of all distributions respectively. For a random variable (r.v.) X,
79"
PRELIMINARIES,0.16727272727272727,"we use E[X] and V[X] to denote its expectation and variance. For two r.v.s, we denote by X⊥Y if
80"
PRELIMINARIES,0.1690909090909091,"X is independent of Y . We use ˜O(·) to denote O(·) omitting logarithmic factors.
81"
PRELIMINARIES,0.1709090909090909,"Episodic MDP.
An episodic MDP is identified by M ≜(S, A, (Ph)h∈[H], (Rh)h∈[H], H), where
82"
PRELIMINARIES,0.17272727272727273,"S is the state space, A the action space, Ph : S × A× →∆(S) the probability transition kernel at
83"
PRELIMINARIES,0.17454545454545456,"step h, Rh : S × A →D([0, 1]) the collection of reward distributions at step h and H the length of
84"
PRELIMINARIES,0.17636363636363636,"one episode. The agent interacts with the environment for K episodes. At the beginning of episode k,
85"
PRELIMINARIES,0.1781818181818182,"Nature selects an initial state sk
1 arbitrarily. In step h, the agent takes action ak
h and observes random
86"
PRELIMINARIES,0.18,"reward Rk
h(sk
h, ak
h) ∼Rh(sk
h, ak
h) and reaches the next state sk
h+1 ∼Ph(·|sk
h, ak
h). The episode
87"
PRELIMINARIES,0.18181818181818182,"terminates at H + 1 with Rk
H+1 = 0, then the agent proceeds to next episode.
88"
PRELIMINARIES,0.18363636363636363,"For each (k, h) ∈[K] × [H], we denote by Hk
h ≜
 
s1
1, a1
1, s1
2, a1
2, . . . , s1
H, a1
H, . . . , sk
h, ak
h

the
89"
PRELIMINARIES,0.18545454545454546,"(random) history up to step h episode k. We define Fk ≜Hk−1
H
as the history up to episode
90"
PRELIMINARIES,0.18727272727272729,"k −1. We describe the interaction between the algorithm and MDP in two levels. In the level of
91"
PRELIMINARIES,0.1890909090909091,"episode, we define an algorithm as a sequence of function A ≜(Ak)k∈[K], each mapping Fk to
92"
PRELIMINARIES,0.19090909090909092,"a policy Ak(Fk) ∈Π. We denote by πk ≜Ak(Fk) the policy at episode k. In the level of step, a
93"
PRELIMINARIES,0.19272727272727272,"(deterministic) policy π is defined as a sequence of functions π = (πh)h∈[H] with πh : S →∆(A).
94"
PRELIMINARIES,0.19454545454545455,"Entropic risk measure.
EntRM is a well-known risk measure in risk-sensitive decision-making,
95"
PRELIMINARIES,0.19636363636363635,"including mathematical finance [25], Markovian decision processes [3]. The EntRM value of a r.v.
96"
PRELIMINARIES,0.19818181818181818,"X ∼F with coefficient β ̸= 0 is defined as
97"
PRELIMINARIES,0.2,Uβ(X) ≜1
PRELIMINARIES,0.2018181818181818,β log(EX∼F [exp(βX)]) = 1
PRELIMINARIES,0.20363636363636364,"β log
Z"
PRELIMINARIES,0.20545454545454545,"R
exp(βx)dF(x)

."
PRELIMINARIES,0.20727272727272728,"With slight abuse of notations, we write Uβ(F) = Uβ(X) for X ∼F. For β with small absolute
98"
PRELIMINARIES,0.20909090909090908,"value, using Taylor’s expansion we have
99"
PRELIMINARIES,0.2109090909090909,Uβ(X) = E[X] + β
PRELIMINARIES,0.21272727272727274,"2 V[X] + O(β2).
(1)"
PRELIMINARIES,0.21454545454545454,"Hence for a decision-maker who aims at maximizing the EntRM value, she tends to be risk-seeking
100"
PRELIMINARIES,0.21636363636363637,"(favoring high uncertainty in X) if β > 0 and risk-averse (favoring low uncertainty in X) if β < 0.
101"
PRELIMINARIES,0.21818181818181817,"|β| controls the risk-sensitivity. It exactly recovers mean as the risk-neutral objective when β →0.
102"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.22,"3
Risk-sensitive Distributional Dynamic Programming
103"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.22181818181818183,"[4, 40] has discussed the infinite-horizon distributional dynamic programming in the risk-neutral
104"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.22363636363636363,"setting, which will be referred to as the classical DDP. There is a big gap between the risk-sensitive
105"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.22545454545454546,"MDP and the risk-neutral one. In this section, we establish the novel DDP framework for risk-sensitive
106"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.22727272727272727,"control.
107"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2290909090909091,"We start with defining the return for a policy π starting from state-action pair (s, a) at step h
108"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2309090909090909,"Zπ
h(s, a) ≜ H
X"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.23272727272727273,"h′=h
Rh′(sh′, ah′), sh = s, ah′ = πh′(sh′), sh′+1 ∼Ph′(·|sh′, ah′)."
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.23454545454545456,"Define Y π
h (s) ≜Zπ
h(s, πh(s)). There are three sources of randomness in Zπ
h(s, a): the reward
109"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.23636363636363636,"Rh(s, a), the transition P π and the next-state return Y π
h+1(sh+1). Denote by νπ
h(s) and ηπ
h(s, a) the
110"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2381818181818182,"cumulative distribution function (CDF) corresponding to Y π
h (s) and Zπ
h(s, a) respectively. To the
111"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.24,"end of risk-sensitive control, we define the action-value function of a policy π at step h as Qπ
h(s, a) ≜
112"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.24181818181818182,"Uβ(Zπ
h(s, a)), i.e. the EntRM value of the return distribution, for each (s, a, h) ∈S × A × [H]. The
113"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.24363636363636362,"value function is defined as V π
h (s) ≜Qπ
h(s, πh(s)) = Uβ(Y π
h (s)).
114"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.24545454545454545,"We focus on the control setting, in which the goal is to find an optimal policy to maximize the value
115"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.24727272727272728,"function, i.e.
116"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.24909090909090909,"π∗≜arg
max
(π1,...,πH)∈Π V π1...πH
1
(s)."
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2509090909090909,"We write π = (π1, ..., πH) to emphasize that it is a multi-stage maximization problem. Direct search
117"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.25272727272727274,"suffers exponential computational complexity. In the risk-neutral case, the principle of optimality
118"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2545454545454545,"holds, i.e.,the optimal policy of tail sub-problem is the tail optimal policy [5]. Therein the multi-stage
119"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.25636363636363635,"maximization problem can be reduced to a multiple single-stage maximization problem. However,
120"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2581818181818182,"the principle does not always hold for general risk measures. For example, the optimal policy for
121"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.26,"CVaR may be non-Markovian/history-dependent ([41]).
122"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.26181818181818184,"We identify two key properties of EntRM, upon which we retain the principle of optimality.
123"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2636363636363636,"Lemma 1. The EntRM satisfies the following properties:
124"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.26545454545454544,"• Additive: X⊥Y ⇒Uβ(X + Y ) = Uβ(X) + Uβ(Y ), ∀X, Y .
125"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2672727272727273,"• Monotonicity-preserving: ∀F1, F2, G ∈D, ∀θ ∈[0, 1],
126"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2690909090909091,Uβ(F2) ≤Uβ(F1) ⇒Uβ((1 −θ)F2 + θG) ≤Uβ((1 −θ)F1 + θG).
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.27090909090909093,"The proof is given in Appendix B. In particular, the additivity entails that the EntRM value of the
127"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2727272727272727,"current return Zπ
h(s, a) equals the sum of the immediate value of Rh(s, a) and the value of the future
128"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.27454545454545454,"return Y π
h (s′), i.e.,
129"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.27636363636363637,"Uβ(Zπ
h(s, a)) = Uβ(Rh(s, a)) + Uβ(Y π
h (s′).
The monotonicity-preserving property together with the additivity suggests that the optimal future
130"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2781818181818182,"return Y ∗
h (s′) consists in the optimal current return Z∗
h(s, a)
131"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.28,"Z∗
h(s, a) = Rh(s, a) + Y ∗
h (s′)."
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2818181818181818,"These observations implies the principle of optimality.
132"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.28363636363636363,"Proposition 1 (Principle of optimality). Let π∗= {π∗
1, π∗
2, ..., π∗
H} be an optimal policy and assume
133"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.28545454545454546,"when we visit some state s using policy π at time-step h with positive probability. Consider the
134"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2872727272727273,"sub-problem defined by the the following maximization problem
135"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.28909090909090907,"max
π∈Π V π
h (s) = Uβ(Rh(s, a)) + Uβ([Phνπ
h+1](s, a))."
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2909090909090909,"Then the truncated optimal policy {π∗
h, π∗
h+1, ..., π∗
H} is optimal for this sub-problem.
136"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2927272727272727,"The proof is given in Appendix E. It further induces the distributional Bellman optimality equation.
137"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.29454545454545455,"Proposition 2 (Distributional Bellman optimality equation). For arbitrary initial state s1, the optimal
138"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.2963636363636364,"policy (π∗
h)h∈[H] is given by the following backward recursions:
139"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.29818181818181816,"ν∗
H+1(s) = ψ0, η∗
h(s, a) = [Phν∗
h+1](s, a) ∗fh(·|s, a),
π∗
h(s) = arg max
a∈A Q∗
h(s, a) = Uβ(η∗
h(s, a)), ν∗
h(s) = η∗
h(s, π∗
h(s)),
(2)"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.3,"where fh(s, a) is the probability density function of Rh(s, a). Furthermore, the sequence (η∗
h)h∈[H]
140"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.3018181818181818,"and (ν∗
h)h∈[H] are the sequence of distributions corresponding to the optimal returns at each step.
141"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.30363636363636365,"The proof is given in Appendix E. For simplicity, we define the distributional Bellman operator
142"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.3054545454545455,"B(P, R) : DS →DS×A with associated model (P, R) = (P(s, a), R(s, a))(s,a)∈S×A as
143"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.30727272727272725,"[B(P, R)ν](s, a) ≜[Pν](s, a) ∗fh(·|s, a), ∀(s, a) ∈S × A."
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.3090909090909091,"Hence we can rewrite Equation 2 in a compact form:
144"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.3109090909090909,"ν∗
H+1(s) = ψ0, η∗
h(s, a) = [B(Ph, Rh)ν∗
h+1](s, a),
π∗
h(s) = arg max
a∈A Uβ(η∗
h(s, a)), ν∗
h(s) = η∗
h(s, π∗
h(s)), ∀(s, a, h) ∈S × A × [H].
(3)"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.31272727272727274,"Finally, we define the regret of an algorithm A interacting with an MDP M for K episodes as
145"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.3145454545454546,"Regret(A , M, K) ≜ K
X"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.31636363636363635,"k=1
V ∗
1 (sk
1) −V πk
h (sk
1)."
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.3181818181818182,"Note that the regret is a random variable since πk is a random quantity.
We denote by
146"
RISK-SENSITIVE DISTRIBUTIONAL DYNAMIC PROGRAMMING,0.32,"E[Regret(A , M, K)] the expected regret. We will omit π and M if it is clear from the context.
147"
ALGORITHM,0.32181818181818184,"4
Algorithm
148"
ALGORITHM,0.3236363636363636,"For a better understanding of the readers, we present our algorithms under the assumption that
149"
ALGORITHM,0.32545454545454544,"the reward is deterministic and known1. The algorithms for the case of random reward are given
150"
ALGORITHM,0.32727272727272727,1The algorithms for random reward enjoy the regret bounds of the same order.
ALGORITHM,0.3290909090909091,"in Appendix C. We denote by {rh(s, a)}(s,a,h)∈S×A×[H] the reward functions. For the case of
151"
ALGORITHM,0.33090909090909093,"deterministic reward, the Bellman update in Equation 2 takes the form
152"
ALGORITHM,0.3327272727272727,"η∗
h(s, a) = [Phν∗
h+1](s, a)(· −rh(s, a)),"
ALGORITHM,0.33454545454545453,"since adding a deterministic reward rh(s, a) corresponds to shifting the distribution [Phν∗
h+1](s, a) by
153"
ALGORITHM,0.33636363636363636,"an amount of rh(s, a). We thus define the distributional Bellman operator B(P, R) : DS →DS×A
154"
ALGORITHM,0.3381818181818182,"with associated model (P, r) = (P(s, a), r(s, a))(s,a)∈S×A as
155"
ALGORITHM,0.34,"[B(P, r)ν](s, a) ≜[Pν](s, a)(· −rh(s, a)), ∀(s, a) ∈S × A."
ALGORITHM,0.3418181818181818,"We propose two DRL algorithms in this section, including a model-free algorithm and a model-based
156"
ALGORITHM,0.34363636363636363,"algorithm. We first introduce the Model- Free Risk-sensitive Optimistic Distribution Iteration
157"
ALGORITHM,0.34545454545454546,"(RODI-MF) in Algorithm 1. For completeness, we introduce some additional notations here. For
158"
ALGORITHM,0.3472727272727273,"two CDFs F and G over reals, we define the supremum distance between them ∥F −G∥∞≜
159"
ALGORITHM,0.3490909090909091,"supx |F(x) −G(x)|. We define the ℓ1 distance between two probability mass functions (PMFs)
160"
ALGORITHM,0.3509090909090909,P and Q as ∥P −Q∥1 ≜P
ALGORITHM,0.3527272727272727,"i |Pi −Qi|. We denote by B∞(F, c) := {G ∈D|∥G −F∥∞≤c}
161"
ALGORITHM,0.35454545454545455,"the supremum norm ball centered at F with radius c. With slight abuse of notations, we denote by
162"
ALGORITHM,0.3563636363636364,"B1(P, c) the l1 norm ball centered at P with radius c.
163"
ALGORITHM OVERVIEW,0.35818181818181816,"4.1
Algorithm overview
164"
RODI-MF,0.36,"4.1.1
RODI-MF
165"
RODI-MF,0.3618181818181818,"In each episode, the algorithm includes the planning phase (Line 4-12) and the interaction phase
166"
RODI-MF,0.36363636363636365,"(Line 13-17).
167"
RODI-MF,0.3654545454545455,"Planning phase.
In a high level, the algorithm implements an optimistic version of approximate
168"
RODI-MF,0.36727272727272725,"DDP from step H + 1 to step 1 in each episode. In Line (5-7), it performs sample-based Bellman
169"
RODI-MF,0.3690909090909091,"update. To make it clear, we introduce the superscript k to the variables of Algorithm 1 in episode k.
170"
RODI-MF,0.3709090909090909,"For example, ηk
h denotes ηh in episode k. Specifically, for those visited state-action pairs, we claim
171"
RODI-MF,0.37272727272727274,"that Line 6 is equivalent to a model-based Bellman update. Denote by Ik
h(s, a) ≜I{(sk
h, ak
h) = (s, a)}.
172"
RODI-MF,0.37454545454545457,"Fix a tuple (s, a, k, h) such that N k
h(s, a) ≥1. We denote by ˆP k
h (·|s, a) the empirical transition
173"
RODI-MF,0.37636363636363634,"model
174"
RODI-MF,0.3781818181818182,"ˆP k
h (s′|s, a) =
1
N k
h(s, a) X"
RODI-MF,0.38,"τ∈[k−1]
Iτ
h(s, a) · I{sτ
h+1 = s′}."
RODI-MF,0.38181818181818183,"Observe that for any ν ∈DS, we have
175"
RODI-MF,0.3836363636363636,"h
ˆP k
h ν
i
(s, a) =
X"
RODI-MF,0.38545454545454544,"s′∈S
ˆP k
h (s′|s, a)ν(s′) =
1
N k
h(s, a) X s′∈S X"
RODI-MF,0.38727272727272727,"τ∈[k−1]
Iτ
h(s, a) · I{sτ
h+1 = s′}ν(s′)"
RODI-MF,0.3890909090909091,"=
1
N k
h(s, a) X"
RODI-MF,0.39090909090909093,"τ∈[k−1]
Iτ
h(s, a) ·
X"
RODI-MF,0.3927272727272727,"s′∈S
I{sτ
h+1 = s′}ν(sτ
h+1)"
RODI-MF,0.39454545454545453,"=
1
N k
h(s, a) X"
RODI-MF,0.39636363636363636,"τ∈[k−1]
Iτ
h(s, a)ν(sτ
h+1)."
RODI-MF,0.3981818181818182,"Hence the update formula in Line 6 of Algorithm 1 can be rewritten as
176"
RODI-MF,0.4,"ηk
h(s, a) =
h
ˆP k
h νk
h+1
i
(s, a)(· −rh(s, a)) =
h
B( ˆP k
h , rh)νk
h+1
i
(s, a),"
RODI-MF,0.4018181818181818,"implying the equivalence to a model-based Bellman update with empirical model ˆP k
h . Alternatively,
177"
RODI-MF,0.4036363636363636,"the unvisited (s, a) remains to be the return distribution corresponding to the highest possible reward
178"
RODI-MF,0.40545454545454546,"H + 1 −h. The algorithm then computes the optimism constants (Line 8) and enforces OFU
179"
RODI-MF,0.4072727272727273,"through the distributional optimism operator ck
h (Line 9) to obtain the optimistically plausible return
180"
RODI-MF,0.4090909090909091,"distribution ηk
h. The choice of ck
h will be discussed later. The optimistic return distributions yields the
181"
RODI-MF,0.4109090909090909,"optimistic value function, from which the algorithm generates the greedy policy πk
h. The policy πk
h
182"
RODI-MF,0.4127272727272727,"will be used in the interaction phase.
183"
RODI-MF,0.41454545454545455,"Interaction phase.
In Line (15-16), the agent interacts with the environment using policy π and
184"
RODI-MF,0.4163636363636364,"updates the counts Nh based on new observations.
185"
RODI-MF,0.41818181818181815,Algorithm 1 RODI-MF
RODI-MF,0.42,"1: Input: T and δ
2: Initialize Nh(·, ·) ←0; ηh(·, ·), νh(·) ←ψH+1−h for all h ∈[H]
3: for k = 1 : K do
4:
for h = H : 1 do
5:
if Nh(·, ·) > 0 then
6:
ηh(·, ·) ←
1
Nh(·,·)
P
τ∈[k−1] Iτ
h(·, ·)νh+1(sτ
h+1)(· −rh(·, ·))
7:
end if
8:
ch(·, ·) ←
q"
S,0.4218181818181818,"2S
Nh(·,·)∨1ι"
S,0.42363636363636364,"9:
ηh(·, ·) ←O∞
ch(·,·)ηh(·, ·)
10:
πh(·) ←arg maxa Uβ(ηh(·, a))
11:
νh(·) ←ηh(·, πh(·))
12:
end for
13:
Receive sk
1
14:
for h = 1 : H do
15:
ak
h ←πh(sk
h) and transit to sk
h+1
16:
Nh(sk
h, ak
h) ←Nh(sk
h, ak
h) + 1
17:
end for
18: end for"
RODI-MB,0.4254545454545455,"4.1.2
RODI-MB
186"
RODI-MB,0.42727272727272725,"We introduce the second algorithm Model- Based Risk-sensitive Optimistic Distribution Iteration
187"
RODI-MB,0.4290909090909091,"(RODI-MB). Algorithm 2 is a model-based algorithm because it requires to explicitly maintaining the
188"
RODI-MB,0.4309090909090909,"empirical transition model in each episode. However, it can be reduced to a non-distributional rein-
189"
RODI-MB,0.43272727272727274,"forcement learning algorithm that deals with the one-dimensional values instead of the distributions,
190"
RODI-MB,0.43454545454545457,"which saves the computational complexity and space complexity. Likewise, the algorithm includes
191"
RODI-MB,0.43636363636363634,"the planning phase (Line 4-10) and the interaction phase (Line 11-15).
192"
RODI-MB,0.4381818181818182,"Planning phase.
Analogous to Algorithm 1, the algorithm also performs approximate DDP together
193"
RODI-MB,0.44,"with the OFU principle. First, it applies the distributional optimistic operator to the empirical transition
194"
RODI-MB,0.44181818181818183,"model ˆP k
h to get the optimistic transition model ˜P k
h . Then the algorithm uses ˜P k
h to execute Bellman
195"
RODI-MB,0.44363636363636366,"update to generate the optimistic return distributions ηk
h. The remaining steps are the same as
196"
RODI-MB,0.44545454545454544,"Algorithm 1.
197"
RODI-MB,0.44727272727272727,"Interaction phase.
In Line (13-14), the agent interacts with the environment using policy πk and
198"
RODI-MB,0.4490909090909091,"updates the counts N k+1
h
and empirical transition model ˆP k+1
h
based on the new observations.
199"
RODI-MB,0.4509090909090909,Algorithm 2 RODI-MB
RODI-MB,0.4527272727272727,"1: Input: T and δ
2: N 1
h(·, ·) ←0; ˆP 1
h(·, ·) ←1"
RODI-MB,0.45454545454545453,"S 1 for all h ∈[H]
3: for k = 1 : K do
4:
νk
H+1(·) ←ψ0
5:
for h = H : 1 do
6:
˜P k
h (·, ·) ←O1
ck
h(·,·) ˆP k
h (·, ·)"
RODI-MB,0.45636363636363636,"7:
ηk
h(·, ·) ←
h
B

˜P k
h , rh

νk
h+1
i
(·, ·)"
RODI-MB,0.4581818181818182,"8:
πk
h(·) ←arg maxa Uβ(ηk
h(·, a))
9:
νk
h(·) ←ηk
h(·, πk
h(·))
10:
end for
11:
Receive sk
1
12:
for h = 1 : H do
13:
ak
h ←πk
h(sk
h) and transit to sk
h+1
14:
Compute N k+1
h
(·, ·) and ˆP k+1
h
(·, ·)
15:
end for
16: end for"
RODI-MB,0.46,Algorithm 3 ROVI
RODI-MB,0.4618181818181818,"1: Input: T and δ
2: N 1
h(·, ·) ←0; ˆP 1
h(·, ·) ←1"
RODI-MB,0.4636363636363636,"S 1 for all h ∈[H]
3: for k = 1 : K do
4:
W k
H+1(·) ←1
5:
for h = H : 1 do
6:
˜P k
h (·, ·) ←O1
ck
h(·,·) ˆP k
h (·, ·)"
RODI-MB,0.46545454545454545,"7:
Jk
h(·, ·) ←eβrh(·,·) h
˜P k
h W k
h+1
i
(·, ·)"
RODI-MB,0.4672727272727273,"Equivalence to ROVI.
Risk-sensitive Optimistic Value Iteration (ROVI) is a non-distributional
200"
RODI-MB,0.4690909090909091,"algorithm that deals with the real-valued value function rather than the distribution. It is motivated by
201"
RODI-MB,0.4709090909090909,"the exponential Bellman equation proposed by [22]. We define the functional exponential EntRM
202"
RODI-MB,0.4727272727272727,"(EERM) Eβ as the EntRM after the exponential transformation
203"
RODI-MB,0.47454545454545455,"Eβ(F) ≜exp(β(Uβ(F))) =
Z"
RODI-MB,0.4763636363636364,"R
exp(βx)dF(x)."
RODI-MB,0.4781818181818182,"Define the exponential value functions Wh(s) ≜Eβ(νh(s)) and Jh(s, a) ≜Eβ(ηh(s, a)) for all
204"
RODI-MB,0.48,"(s, a, h)s. Applying EERM to Equation 3 yields the exponential Bellman equation
205"
RODI-MB,0.4818181818181818,"J∗
h(s, a) = exp(βrh(s, a))[PhW ∗
h+1](s, a),
W ∗
h(s) = sign(β) max
a
sign(β)J∗
h(s, a), W ∗
H+1(s) = 1.
(4)"
RODI-MB,0.48363636363636364,"To verify the equivalence, it is sufficient to show that Jk
h in Algorithm 3 corresponds to the exponential
206"
RODI-MB,0.48545454545454547,"function of ηk
h in Algorithm 2. Observe that Eβ is linear in F, hence it follows that
207"
RODI-MB,0.48727272727272725,"Eβ(ηk
h(s, a)) = Eβ
h
˜P k
h νk
h+1
i
(s, a)(· −rh(s, a))

= exp(βrh(s, a)) ·
h
˜P k
h Eβ(νk
h+1)
i
(s, a)"
RODI-MB,0.4890909090909091,"= exp(βrh(s, a))
h
˜P k
h W k
h+1
i
(s, a) = Jk
h(s, a)."
RODI-MB,0.4909090909090909,"The two algorithms generate the policy sequence in the same way, implying that their trajectories
208"
RODI-MB,0.49272727272727274,"HK
H follow the same distribution. The formal statement is given in Appendix E.
209"
DISTRIBUTIONAL OPTIMISM,0.49454545454545457,"4.2
Distributional Optimism
210"
DISTRIBUTIONAL OPTIMISM,0.49636363636363634,"It is common to add a bonus to the reward to ensure optimism in the risk-neutral setting. Specifically,
211"
DISTRIBUTIONAL OPTIMISM,0.49818181818181817,"the bonus is closely related to the level of uncertainty, which is quantified by the concentration
212"
DISTRIBUTIONAL OPTIMISM,0.5,"inequality. Yet, this type of optimism cannot be adapted to the distributional setup. As one of our
213"
DISTRIBUTIONAL OPTIMISM,0.5018181818181818,"technical novelty, the distributional optimism is introduced for algorithmic design and regret analysis.
214"
DISTRIBUTIONAL OPTIMISM,0.5036363636363637,"In particular, we specify two types of distributional optimism operators, which map a statistically
215"
DISTRIBUTIONAL OPTIMISM,0.5054545454545455,"plausible distribution (either the empirical model or the return distribution) to a optimistically
216"
DISTRIBUTIONAL OPTIMISM,0.5072727272727273,"plausible distribution. Either of them is applied by Algorithm 2 or Algorithm 1.
217"
DISTRIBUTIONAL OPTIMISM,0.509090909090909,"Distributional optimism on the return distribution (in Algorithm 1).
For two CDFs F and G,
218"
DISTRIBUTIONAL OPTIMISM,0.5109090909090909,"we say that F is more optimistic than G (w.r.t. EntRM) if Uβ(F) ≥Uβ(G). This reflects the intuition
219"
DISTRIBUTIONAL OPTIMISM,0.5127272727272727,"that the more optimistic distribution should own larger EntRM value. Following [31], we define the
220"
DISTRIBUTIONAL OPTIMISM,0.5145454545454545,"distributional optimism operator O∞
c : D([a, b]) 7→D([a, b]) with level c ∈(0, 1) as
221"
DISTRIBUTIONAL OPTIMISM,0.5163636363636364,"(O∞
c F)(x) ≜[F(x) −cI[a,b)(x)]+."
DISTRIBUTIONAL OPTIMISM,0.5181818181818182,"The optimistic operator shifts the input F down by at most c over [a, b), and retain the value 1 at b. It
222"
DISTRIBUTIONAL OPTIMISM,0.52,"ensures that O∞
c F remains in D([a, b]) and dominates all the other CDFs in D([a, b]) in the sense
223"
DISTRIBUTIONAL OPTIMISM,0.5218181818181818,"that (O∞
c F)(x) ≤G(x) for any G ∈B∞(F, c). Since EntRM is monotonic, it holds that
224"
DISTRIBUTIONAL OPTIMISM,0.5236363636363637,"Uβ(O∞
c F) ≥Uβ(G), ∀G ∈B∞(F, c)."
DISTRIBUTIONAL OPTIMISM,0.5254545454545455,"Hence O∞
c F is the most optimistic distribution in the infinity ball B∞(F, c). In other words, for
225"
DISTRIBUTIONAL OPTIMISM,0.5272727272727272,"any CDF F and G satisfying ∥F −G∥∞≤c, we have O∞
c G ⪰F. When specialized to the return
226"
DISTRIBUTIONAL OPTIMISM,0.5290909090909091,"distributions, we can apply the distributional optimism operator to the estimated return distribution
227"
DISTRIBUTIONAL OPTIMISM,0.5309090909090909,"ηk
h (Line 9 of Algorithm 1) with the constant ck
h to ensure Uβ(ηk
h(s, a)) ≥Uβ(η∗
h(s, a). The constant
228"
DISTRIBUTIONAL OPTIMISM,0.5327272727272727,"ck
h quantifies uncertainty in the model estimation, i.e.,
 ˆP k
h (s, a) −Ph(s, a)

1.
229"
DISTRIBUTIONAL OPTIMISM,0.5345454545454545,"Distributional optimism on the model (in Algorithm 2).
Given the model, we consider the
optimism among the space of PMFs rather than CDFs. Using the ℓ1 concentration inequality [46],
we get a concentration bound of the empirical PMF of model: with probability at least 1 −δ,"
DISTRIBUTIONAL OPTIMISM,0.5363636363636364,"bP k
h (s, a) −Ph(s, a)

1 ≤ck
h(s, a) = s"
"S
N K",0.5381818181818182,"2S
N k
h(s, a) log 1"
"S
N K",0.54,δ = ˜O s
"S
N K",0.5418181818181819,"2S
N k
h(s, a) ! ."
"S
N K",0.5436363636363636,"We wish to obtain a optimistic transition model ˜P k
h (s, a) from the empirical one bP k
h (s, a). To be more
230"
"S
N K",0.5454545454545454,"specific, the return distribution ηk
h computed from ˜P k
h (s, a) and νk
h+1 should be more optimistic than
231"
"S
N K",0.5472727272727272,"the optimal one η∗
h(s, a) with high probability. We thus define the distributional optimism operator
232"
"S
N K",0.5490909090909091,"O1
c : D(S) 7→D(S) with level c and future return ν ∈DS as
233"
"S
N K",0.5509090909090909,"O1
c

bP(s, a), ν

≜arg
max
P ∈B1( b
P (s,a),c)
Uβ([Pν])."
"S
N K",0.5527272727272727,"The ERM satisfy an interesting property that enables an efficient approach to perform O1
c (see
234"
"S
N K",0.5545454545454546,"Appendix B). The following holds by using the induction
235"
"S
N K",0.5563636363636364,"Uβ
 
ηk
h(s, a)

= rh(s, a) + Uβ
h
˜P k
h νk
h+1
i
[s, a]

≥rh(s, a) + Uβ
 
Phνk
h+1

[s, a]
"
"S
N K",0.5581818181818182,"≥rh(s, a) + Uβ
 
Phν∗
h+1

[s, a]
"
"S
N K",0.56,"= Uβ(η∗
h(s, a)),"
"S
N K",0.5618181818181818,"which verify the optimism of ηk
h(s, a) over η∗
h(s, a).
236"
REGRET ANALYSIS,0.5636363636363636,"5
Regret Analysis
237"
REGRET UPPER BOUNDS,0.5654545454545454,"5.1
Regret upper bounds
238"
REGRET UPPER BOUNDS,0.5672727272727273,"Theorem 1 (Regret upper bound of RODI-MF). For any δ ∈(0, 1), with probability 1 −δ, the regret
239"
REGRET UPPER BOUNDS,0.5690909090909091,"of Algorithm 1 under deterministic reward or Algorithm 4 under random reward is bounded as
240"
REGRET UPPER BOUNDS,0.5709090909090909,"Regret(RODI-MF, K) ≤O
 1"
REGRET UPPER BOUNDS,0.5727272727272728,"|β|LHH
p"
REGRET UPPER BOUNDS,0.5745454545454546,"S2AK log(4SAT/δ)

= ˜O
exp(|β|H) −1"
REGRET UPPER BOUNDS,0.5763636363636364,"|β|
H
√"
REGRET UPPER BOUNDS,0.5781818181818181,"S2AK

."
REGRET UPPER BOUNDS,0.58,"The proof is given in Appendix D.
241"
REGRET UPPER BOUNDS,0.5818181818181818,"Theorem 2 (Regret upper bound of RODI-MB/ROVI). For any δ ∈(0, 1), with probability 1 −δ, the
242"
REGRET UPPER BOUNDS,0.5836363636363636,"regret of Algorithm 1/Algorithm 3 under deterministic reward or Algorithm 4/Algorithm 6 under
243"
REGRET UPPER BOUNDS,0.5854545454545454,"random reward is bounded as
244"
REGRET UPPER BOUNDS,0.5872727272727273,"Regret(RODI-MF, K) = Regret(ROVI, K) ≤O( 1"
REGRET UPPER BOUNDS,0.5890909090909091,"|β|LHH
p"
REGRET UPPER BOUNDS,0.5909090909090909,S2AK log(4SAT/δ))
REGRET UPPER BOUNDS,0.5927272727272728,"= ˜O
exp(|β|H) −1"
REGRET UPPER BOUNDS,0.5945454545454546,"|β|
H
√"
REGRET UPPER BOUNDS,0.5963636363636363,"S2AK

."
REGRET UPPER BOUNDS,0.5981818181818181,"The proof is given in Appendix D. The above results match the best-known results in [22]. In
245"
REGRET UPPER BOUNDS,0.6,"particular, our algorithms attain exponentially improved regret bounds than those of RSVI and RSQ
246"
REGRET UPPER BOUNDS,0.6018181818181818,"in [23] with a factor of exp(|β|H2). By choosing |β| = O(1/H), we can eliminate the exponential
247"
REGRET UPPER BOUNDS,0.6036363636363636,"term and achieve polynomial regret bound akin to the risk-neutral setting.
248"
REGRET UPPER BOUNDS,0.6054545454545455,"Compared to the traditional/non-distributional analysis dealing with one-dimensional values, our
249"
REGRET UPPER BOUNDS,0.6072727272727273,"analysis is distribution-centered, called the distributional analysis. The distributional analysis deals
250"
REGRET UPPER BOUNDS,0.6090909090909091,"with the distributions of the return rather than the risk measure values of the return. For example, it
251"
REGRET UPPER BOUNDS,0.610909090909091,"involves the operations of the distributions, the optimism between different distributions, the error
252"
REGRET UPPER BOUNDS,0.6127272727272727,"caused by estimation of distribution, etc. These distributional aspects fundamentally differ from the
253"
REGRET UPPER BOUNDS,0.6145454545454545,"traditional analysis that deals with the one-dimensional scalars (value functions). Now we recap the
254"
REGRET UPPER BOUNDS,0.6163636363636363,"technical novelty of our analysis in the following.
255"
REGRET UPPER BOUNDS,0.6181818181818182,"Lipschitz continuity and linearity.
We identify two important properties of EERM that establishes
256"
REGRET UPPER BOUNDS,0.62,"the regret upper bounds, including the Lipschitz continuity and linearity. Denote by LM the Lipschitz
257"
REGRET UPPER BOUNDS,0.6218181818181818,"constant of the EERM Eβ : D([0, M]) →R with respect to the infinity norm ∥·∥∞. Lemma 2
258"
REGRET UPPER BOUNDS,0.6236363636363637,"provides a tight Lipschitz constant of EERM. The Lipschitz constant relates the difference between
259"
REGRET UPPER BOUNDS,0.6254545454545455,"distributions to the difference measured by their EERM values.
260"
REGRET UPPER BOUNDS,0.6272727272727273,"Lemma 2 (Lipschitz property of EERM). Eβ is Lipschitz continuous with respect to the supremum
261"
REGRET UPPER BOUNDS,0.6290909090909091,"norm over DM with LM = exp(|β|M) −1. Moreover, LM is tight in terms of both |β| and M.
262"
REGRET UPPER BOUNDS,0.6309090909090909,"Notice that limβ→0 LM = 0, which coincides with the fact that limβ→0 Eβ = 1. The linearity of
263"
REGRET UPPER BOUNDS,0.6327272727272727,"EERM is a key property that sharpens the regret bounds. In contrast, EntRM is non-linear in the
264"
REGRET UPPER BOUNDS,0.6345454545454545,"distribution, which could induce a factor of exp(|β|H) when controlling the error propagation across
265"
REGRET UPPER BOUNDS,0.6363636363636364,"time-steps. It would further lead to a compounding factor of exp(|β|H2) in the regret bound. In
266"
REGRET UPPER BOUNDS,0.6381818181818182,"summary, the Lipschitz continuity property enables the regret upper bounds of DRL algorithms, and
267"
REGRET UPPER BOUNDS,0.64,"the linearity tightens the bound.
268"
REGRET UPPER BOUNDS,0.6418181818181818,"Distributional optimism.
Another technical novelty in our analysis is the optimism in the face of
269"
REGRET UPPER BOUNDS,0.6436363636363637,"uncertainty at the distributional level. The traditional analysis uses the OFU to construct a sequence
270"
REGRET UPPER BOUNDS,0.6454545454545455,"of optimistic value functions. However, our analysis implements the distributional optimism that
271"
REGRET UPPER BOUNDS,0.6472727272727272,"yields a sequence of optimistic return distributions. In particular, we first define a high probability
272"
REGRET UPPER BOUNDS,0.649090909090909,"event, under which the true return distribution concentrates around the estimated one with a certain
273"
REGRET UPPER BOUNDS,0.6509090909090909,"confidence radius. Then we apply the distributional optimism operator to obtain the optimistically
274"
REGRET UPPER BOUNDS,0.6527272727272727,"plausible return distribution and the optimistic EntRM value. Hence the regret can be bounded by the
275"
REGRET UPPER BOUNDS,0.6545454545454545,"surrogate regret, with the optimal EntRM value replaced by
276"
REGRET UPPER BOUNDS,0.6563636363636364,"Regret(K) = K
X k=1"
REGRET UPPER BOUNDS,0.6581818181818182,"1
β log
 
W ∗
1 (sk
1)

−1"
REGRET UPPER BOUNDS,0.66,"β log

W πk
1 (sk
1)

≤1 β K
X"
REGRET UPPER BOUNDS,0.6618181818181819,"k=1
W k
1 (sk
1) −W πk
1 (sk
1). 277"
REGRET UPPER BOUNDS,0.6636363636363637,"Distributional analysis vs. non-distributional analysis.
When analyzing Algorithm 2/Algorithm
278"
REGRET UPPER BOUNDS,0.6654545454545454,"3, proving the regret bound of either algorithm suffices due to their equivalence relation. Since
279"
REGRET UPPER BOUNDS,0.6672727272727272,"Algorithm 3 is a non-distributional algorithm, one may consider using the standard analysis that
280"
REGRET UPPER BOUNDS,0.6690909090909091,"does not involve distributions. However, we show that this induces a factor of
1
|β| exp(|β|H), which
281"
REGRET UPPER BOUNDS,0.6709090909090909,"explodes as |β| →0. We overcome this issue by invoking a novel distributional analysis of Algorithm
282"
REGRET UPPER BOUNDS,0.6727272727272727,"2, leading to the desired factor of
1
|β| (exp(|β|H) −1).
283"
REGRET UPPER BOUNDS,0.6745454545454546,"Although we focus on the algorithms for the deterministic reward in the main text, the regret upper
284"
REGRET UPPER BOUNDS,0.6763636363636364,"bounds also hold for case of random reward. Algorithm 4, Algorithm 5 and Algorithm 6 corresponds
285"
REGRET UPPER BOUNDS,0.6781818181818182,"to Algorithm 1, Algorithm 2 and Algorithm 3 respectively (cf. Appendix C).
286"
REGRET LOWER BOUND,0.68,"5.2
Regret lower bound
287"
REGRET LOWER BOUND,0.6818181818181818,"We provide more details of the mistakes in the lower bound of [23] in Appendix D. The proof of [23]
288"
REGRET LOWER BOUND,0.6836363636363636,"reduces the regret lower bound to the two-armed bandit regret lower bound. Since the two-armed
289"
REGRET LOWER BOUND,0.6854545454545454,"bandit is a special case of MDP with S = 1, A = 2 and H = 1, the reduction-based proof only leads
290"
REGRET LOWER BOUND,0.6872727272727273,"to a lower bound independent of S, A, and H. Instead, our tight lower bound follows a totally different
291"
REGRET LOWER BOUND,0.6890909090909091,"roadmap motivated by [20]. [20] proves the tight minimax lower bound H
√"
REGRET LOWER BOUND,0.6909090909090909,"SAT for risk-neutral
292"
REGRET LOWER BOUND,0.6927272727272727,"MDP. However, the generalization to risk-sensitive MDP is non-trivial. The main technical challenge
293"
REGRET LOWER BOUND,0.6945454545454546,"is due to the non-linearity of EntRM. The proof in [23] heavily relies on the linearity of expectation,
294"
REGRET LOWER BOUND,0.6963636363636364,"allowing the exchange between taking the risk measure (expectation) and the summation. In the
295"
REGRET LOWER BOUND,0.6981818181818182,"risk-sensitive setting, the non-linearity of EntRM requires new proof techniques.
296"
REGRET LOWER BOUND,0.7,"Assumption 1. Assume S ≥6, A ≥2, and there exists an integer d such that S = 3 + Ad−1"
REGRET LOWER BOUND,0.7018181818181818,"A−1 . We
297"
REGRET LOWER BOUND,0.7036363636363636,further assume that H ≥3d and ¯H ≜H
REGRET LOWER BOUND,0.7054545454545454,"3 ≥1.
298"
REGRET LOWER BOUND,0.7072727272727273,Theorem 3 (Tighter lower bound). Assume Assumption 1 holds and β > 0. Let ¯L ≜(1 −1
REGRET LOWER BOUND,0.7090909090909091,"A)(S −
299"
REGRET LOWER BOUND,0.7109090909090909,3) + 1
REGRET LOWER BOUND,0.7127272727272728,"A. Then for any algorithm A , there exists an MDP MA such that for K ≥2 exp(β(H −¯H −
300"
REGRET LOWER BOUND,0.7145454545454546,"d)) ¯H ¯LA we have
301"
REGRET LOWER BOUND,0.7163636363636363,"E[Regret(A , MA , K)] ≥
1
72
√"
REGRET LOWER BOUND,0.7181818181818181,"6
exp(βH/6) −1"
REGRET LOWER BOUND,0.72,"βH
H
√ SAT."
REGRET LOWER BOUND,0.7218181818181818,"The proof is given in Appendix D. Theorem 3 recovers the tight lower bound for standard episodic
302"
REGRET LOWER BOUND,0.7236363636363636,"MDP, implying that the exponential dependence on |β| and H in the upper bounds is indispensable.
303"
REGRET LOWER BOUND,0.7254545454545455,"Yet, it is not clear whether a similar lower bound holds for β < 0, which is left as a future direction.
304"
CONCLUSION,0.7272727272727273,"6
Conclusion
305"
CONCLUSION,0.7290909090909091,"We propose a risk-sensitive distributional dynamic programming framework. We devise two novel
306"
CONCLUSION,0.730909090909091,"DRL algorithms, including a model-free one and a model-based one, which implement the OFU
307"
CONCLUSION,0.7327272727272728,"principle at the distributional level to balance the exploration and exploitation trade-off under the
308"
CONCLUSION,0.7345454545454545,"risk-sensitive setting. We prove that both attain near-optimal regret upper bounds compared with our
309"
CONCLUSION,0.7363636363636363,"improved lower bound.
310"
CONCLUSION,0.7381818181818182,"There are several promising future directions. The current regret upper bound has an additional factor
311
√"
CONCLUSION,0.74,"HS compared with the lower bound. It might be possible to remove the factor by designing new
312"
CONCLUSION,0.7418181818181818,"algorithms or improving the analysis. Besides, it is interesting to extend the DRL algorithm from
313"
CONCLUSION,0.7436363636363637,"tabular MDP to linear function approximation setting. Finally, it will be meaningful to investigate
314"
CONCLUSION,0.7454545454545455,"whether the DDP framework holds for other risk measures.
315"
REFERENCES,0.7472727272727273,"References
316"
REFERENCES,0.7490909090909091,"[1] Mastane Achab and Gergely Neu. Robustness and risk management via distributional dynamic
317"
REFERENCES,0.7509090909090909,"programming. arXiv preprint arXiv:2112.15430, 2021.
318"
REFERENCES,0.7527272727272727,"[2] Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva
319"
REFERENCES,0.7545454545454545,"Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional determin-
320"
REFERENCES,0.7563636363636363,"istic policy gradients. arXiv preprint arXiv:1804.08617, 2018.
321"
REFERENCES,0.7581818181818182,"[3] Nicole Bäuerle and Ulrich Rieder. More risk-sensitive markov decision processes. Mathematics
322"
REFERENCES,0.76,"of Operations Research, 39(1):105–120, 2014.
323"
REFERENCES,0.7618181818181818,"[4] Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforce-
324"
REFERENCES,0.7636363636363637,"ment learning. In International Conference on Machine Learning, pages 449–458. PMLR,
325"
REFERENCES,0.7654545454545455,"2017.
326"
REFERENCES,0.7672727272727272,"[5] Dimitri P Bertsekas et al. Dynamic programming and optimal control: Vol. 1. Athena scientific
327"
REFERENCES,0.769090909090909,"Belmont, 2000.
328"
REFERENCES,0.7709090909090909,"[6] Tomasz R Bielecki, Stanley R Pliska, and Michael Sherris. Risk sensitive asset allocation.
329"
REFERENCES,0.7727272727272727,"Journal of Economic Dynamics and Control, 24(8):1145–1177, 2000.
330"
REFERENCES,0.7745454545454545,"[7] Vivek S Borkar. A sensitivity formula for risk-sensitive cost and the actor–critic algorithm.
331"
REFERENCES,0.7763636363636364,"Systems & Control Letters, 44(5):339–346, 2001.
332"
REFERENCES,0.7781818181818182,"[8] Vivek S Borkar. Q-learning for risk-sensitive control. Mathematics of operations research,
333"
REFERENCES,0.78,"27(2):294–311, 2002.
334"
REFERENCES,0.7818181818181819,"[9] Vivek S Borkar. Learning algorithms for risk-sensitive control. In Proceedings of the 19th
335"
REFERENCES,0.7836363636363637,"International Symposium on Mathematical Theory of Networks and Systems–MTNS, volume 5,
336"
REFERENCES,0.7854545454545454,"2010.
337"
REFERENCES,0.7872727272727272,"[10] Vivek S Borkar and Sean P Meyn. Risk-sensitive optimal control for markov decision processes
338"
REFERENCES,0.7890909090909091,"with monotone cost. Mathematics of Operations Research, 27(1):192–209, 2002.
339"
REFERENCES,0.7909090909090909,"[11] Rolando Cavazos-Cadena and Daniel Hernández-Hernández. Discounted approximations for
340"
REFERENCES,0.7927272727272727,"risk-sensitive average criteria in markov decision chains with finite state space. Mathematics of
341"
REFERENCES,0.7945454545454546,"Operations Research, 36(1):133–146, 2011.
342"
REFERENCES,0.7963636363636364,"[12] Stefano P Coraluppi and Steven I Marcus.
Risk-sensitive, minimax, and mixed risk-
343"
REFERENCES,0.7981818181818182,"neutral/minimax control of markov decision processes. In Stochastic analysis, control, opti-
344"
REFERENCES,0.8,"mization and applications, pages 21–40. Springer, 1999.
345"
REFERENCES,0.8018181818181818,"[13] Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for
346"
REFERENCES,0.8036363636363636,"distributional reinforcement learning. In International conference on machine learning, pages
347"
REFERENCES,0.8054545454545454,"1096–1105. PMLR, 2018.
348"
REFERENCES,0.8072727272727273,"[14] Will Dabney, Mark Rowland, Marc G Bellemare, and Rémi Munos. Distributional reinforcement
349"
REFERENCES,0.8090909090909091,"learning with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence,
350"
REFERENCES,0.8109090909090909,"2018.
351"
REFERENCES,0.8127272727272727,"[15] Mark Davis and Sébastien Lleo. Risk-sensitive benchmarked asset management. Quantitative
352"
REFERENCES,0.8145454545454546,"Finance, 8(4):415–426, 2008.
353"
REFERENCES,0.8163636363636364,"[16] Erick Delage and Shie Mannor. Percentile optimization for markov decision processes with
354"
REFERENCES,0.8181818181818182,"parameter uncertainty. Operations research, 58(1):203–213, 2010.
355"
REFERENCES,0.82,"[17] Giovanni B Di Masi et al. Infinite horizon risk sensitive control of discrete time markov
356"
REFERENCES,0.8218181818181818,"processes with small risk. Systems & control letters, 40(1):15–20, 2000.
357"
REFERENCES,0.8236363636363636,"[18] Giovanni B Di Masi and Lukasz Stettner. Risk-sensitive control of discrete-time markov
358"
REFERENCES,0.8254545454545454,"processes with infinite horizon. SIAM Journal on Control and Optimization, 38(1):61–78, 1999.
359"
REFERENCES,0.8272727272727273,"[19] Giovanni B Di Masi and Łukasz Stettner. Infinite horizon risk sensitive control of discrete time
360"
REFERENCES,0.8290909090909091,"markov processes under minorization property. SIAM Journal on Control and Optimization,
361"
REFERENCES,0.8309090909090909,"46(1):231–252, 2007.
362"
REFERENCES,0.8327272727272728,"[20] Omar Darwiche Domingues, Pierre Ménard, Emilie Kaufmann, and Michal Valko. Episodic
363"
REFERENCES,0.8345454545454546,"reinforcement learning in finite mdps: Minimax lower bounds revisited. In Algorithmic Learning
364"
REFERENCES,0.8363636363636363,"Theory, pages 578–598. PMLR, 2021.
365"
REFERENCES,0.8381818181818181,"[21] Damien Ernst, Guy-Bart Stan, Jorge Goncalves, and Louis Wehenkel. Clinical data based
366"
REFERENCES,0.84,"optimal sti strategies for hiv: a reinforcement learning approach. In Proceedings of the 45th
367"
REFERENCES,0.8418181818181818,"IEEE Conference on Decision and Control, pages 667–672. IEEE, 2006.
368"
REFERENCES,0.8436363636363636,"[22] Yingjie Fei, Zhuoran Yang, Yudong Chen, and Zhaoran Wang. Exponential bellman equation
369"
REFERENCES,0.8454545454545455,"and improved regret bounds for risk-sensitive reinforcement learning. Advances in Neural
370"
REFERENCES,0.8472727272727273,"Information Processing Systems, 34, 2021.
371"
REFERENCES,0.8490909090909091,"[23] Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, and Qiaomin Xie.
Risk-
372"
REFERENCES,0.850909090909091,"sensitive reinforcement learning: Near-optimal risk-sample tradeoff in regret. arXiv preprint
373"
REFERENCES,0.8527272727272728,"arXiv:2006.13827, 2020.
374"
REFERENCES,0.8545454545454545,"[24] Wendell H Fleming and William M McEneaney. Risk-sensitive control on an infinite time
375"
REFERENCES,0.8563636363636363,"horizon. SIAM Journal on Control and Optimization, 33(6):1881–1915, 1995.
376"
REFERENCES,0.8581818181818182,"[25] Hans Föllmer and Alexander Schied. Stochastic finance. In Stochastic Finance. de Gruyter,
377"
REFERENCES,0.86,"2016.
378"
REFERENCES,0.8618181818181818,"[26] Aurélien Garivier, Pierre Ménard, and Gilles Stoltz. Explore first, exploit next: The true shape
379"
REFERENCES,0.8636363636363636,"of regret in bandit problems. Mathematics of Operations Research, 44(2):377–399, 2019.
380"
REFERENCES,0.8654545454545455,"[27] Lars Peter Hansen and Thomas J Sargent. Robustness. In Robustness. Princeton university
381"
REFERENCES,0.8672727272727273,"press, 2011.
382"
REFERENCES,0.8690909090909091,"[28] Daniel Hernández-Hernández and Steven I Marcus. Risk sensitive control of markov processes
383"
REFERENCES,0.8709090909090909,"in countable state space. Systems & control letters, 29(3):147–155, 1996.
384"
REFERENCES,0.8727272727272727,"[29] Ronald A Howard and James E Matheson. Risk-sensitive markov decision processes. Manage-
385"
REFERENCES,0.8745454545454545,"ment science, 18(7):356–369, 1972.
386"
REFERENCES,0.8763636363636363,"[30] Anna Ja´skiewicz. Average optimality for risk-sensitive control with general state space. The
387"
REFERENCES,0.8781818181818182,"annals of applied probability, 17(2):654–675, 2007.
388"
REFERENCES,0.88,"[31] Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to
389"
REFERENCES,0.8818181818181818,"be conservative: Quickly learning a cvar policy. In Proceedings of the AAAI Conference on
390"
REFERENCES,0.8836363636363637,"Artificial Intelligence, volume 34, pages 4436–4443, 2020.
391"
REFERENCES,0.8854545454545455,"[32] Clare Lyle, Marc G Bellemare, and Pablo Samuel Castro. A comparative analysis of expected
392"
REFERENCES,0.8872727272727273,"and distributional reinforcement learning. In Proceedings of the AAAI Conference on Artificial
393"
REFERENCES,0.889090909090909,"Intelligence, volume 33, pages 4504–4511, 2019.
394"
REFERENCES,0.8909090909090909,"[33] Xiaoteng Ma, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao. Dsac: Distributional
395"
REFERENCES,0.8927272727272727,"soft actor critic for risk-sensitive reinforcement learning. arXiv preprint arXiv:2004.14547,
396"
REFERENCES,0.8945454545454545,"2020.
397"
REFERENCES,0.8963636363636364,"[34] Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani.
Conservative offline distributional
398"
REFERENCES,0.8981818181818182,"reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.
399"
REFERENCES,0.9,"[35] Steven I Marcus, Emmanual Fernández-Gaucherand, Daniel Hernández-Hernandez, Stefano
400"
REFERENCES,0.9018181818181819,"Coraluppi, and Pedram Fard. Risk sensitive markov decision processes. In Systems and control
401"
REFERENCES,0.9036363636363637,"in the twenty-first century, pages 263–279. Springer, 1997.
402"
REFERENCES,0.9054545454545454,"[36] Oliver Mihatsch and Ralph Neuneier. Risk-sensitive reinforcement learning. Machine learning,
403"
REFERENCES,0.9072727272727272,"49(2):267–290, 2002.
404"
REFERENCES,0.9090909090909091,"[37] David Nass, Boris Belousov, and Jan Peters. Entropic risk measure in policy search. In 2019
405"
REFERENCES,0.9109090909090909,"IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1101–1106.
406"
REFERENCES,0.9127272727272727,"IEEE, 2019.
407"
REFERENCES,0.9145454545454546,"[38] Takayuki Osogami. Robustness and risk-sensitivity in markov decision processes. Advances in
408"
REFERENCES,0.9163636363636364,"Neural Information Processing Systems, 25:233–241, 2012.
409"
REFERENCES,0.9181818181818182,"[39] Stephen D Patek. On terminating markov decision processes with a risk-averse objective
410"
REFERENCES,0.92,"function. Automatica, 37(9):1379–1386, 2001.
411"
REFERENCES,0.9218181818181819,"[40] Mark Rowland, Marc Bellemare, Will Dabney, Rémi Munos, and Yee Whye Teh. An analysis
412"
REFERENCES,0.9236363636363636,"of categorical distributional reinforcement learning. In International Conference on Artificial
413"
REFERENCES,0.9254545454545454,"Intelligence and Statistics, pages 29–37. PMLR, 2018.
414"
REFERENCES,0.9272727272727272,"[41] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on stochastic
415"
REFERENCES,0.9290909090909091,"programming: modeling and theory. SIAM, 2021.
416"
REFERENCES,0.9309090909090909,"[42] Yun Shen, Wilhelm Stannat, and Klaus Obermayer. Risk-sensitive markov control processes.
417"
REFERENCES,0.9327272727272727,"SIAM Journal on Control and Optimization, 51(5):3652–3672, 2013.
418"
REFERENCES,0.9345454545454546,"[43] Yun Shen, Michael J Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforcement
419"
REFERENCES,0.9363636363636364,"learning. Neural computation, 26(7):1298–1328, 2014.
420"
REFERENCES,0.9381818181818182,"[44] Rahul Singh, Qinsheng Zhang, and Yongxin Chen. Improving robustness via risk averse
421"
REFERENCES,0.94,"distributional reinforcement learning. In Learning for Dynamics and Control, pages 958–968.
422"
REFERENCES,0.9418181818181818,"PMLR, 2020.
423"
REFERENCES,0.9436363636363636,"[45] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
424"
REFERENCES,0.9454545454545454,"2018.
425"
REFERENCES,0.9472727272727273,"[46] Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger.
426"
REFERENCES,0.9490909090909091,"Inequalities for the l1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep,
427"
REFERENCES,0.9509090909090909,"2003.
428"
REFERENCES,0.9527272727272728,"Checklist
429"
REFERENCES,0.9545454545454546,"1. For all authors...
430"
REFERENCES,0.9563636363636364,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
431"
REFERENCES,0.9581818181818181,"contributions and scope? [Yes]
432"
REFERENCES,0.96,"(b) Did you describe the limitations of your work? [Yes]
433"
REFERENCES,0.9618181818181818,"(c) Did you discuss any potential negative societal impacts of your work? [Yes]
434"
REFERENCES,0.9636363636363636,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
435"
REFERENCES,0.9654545454545455,"them? [Yes]
436"
REFERENCES,0.9672727272727273,"2. If you are including theoretical results...
437"
REFERENCES,0.9690909090909091,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
438"
REFERENCES,0.9709090909090909,"(b) Did you include complete proofs of all theoretical results? [Yes]
439"
REFERENCES,0.9727272727272728,"3. If you ran experiments...
440"
REFERENCES,0.9745454545454545,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
441"
REFERENCES,0.9763636363636363,"mental results (either in the supplemental material or as a URL)? [N/A]
442"
REFERENCES,0.9781818181818182,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
443"
REFERENCES,0.98,"were chosen)? [N/A]
444"
REFERENCES,0.9818181818181818,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
445"
REFERENCES,0.9836363636363636,"ments multiple times)? [N/A]
446"
REFERENCES,0.9854545454545455,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
447"
REFERENCES,0.9872727272727273,"of GPUs, internal cluster, or cloud provider)? [N/A]
448"
REFERENCES,0.9890909090909091,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
449"
REFERENCES,0.990909090909091,"(a) If your work uses existing assets, did you cite the creators? [N/A]
450"
REFERENCES,0.9927272727272727,"(b) Did you mention the license of the assets? [N/A]
451"
REFERENCES,0.9945454545454545,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
452 453"
REFERENCES,0.9963636363636363,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
454"
REFERENCES,0.9981818181818182,"using/curating? [N/A]
455"
