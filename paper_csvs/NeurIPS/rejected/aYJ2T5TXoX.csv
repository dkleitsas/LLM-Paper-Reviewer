Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001072961373390558,"Experimental studies are a cornerstone of machine learning (ML) research. A com-
1"
ABSTRACT,0.002145922746781116,"mon, but often implicit, assumption is that the results of a study will generalize
2"
ABSTRACT,0.003218884120171674,"beyond the study itself, e.g. to new data. That is, there is a high probability that
3"
ABSTRACT,0.004291845493562232,"repeating the study under different conditions will yield similar results. Despite the
4"
ABSTRACT,0.00536480686695279,"importance of the concept, the problem of measuring generalizability remains open.
5"
ABSTRACT,0.006437768240343348,"This is probably due to the lack of a mathematical formalization of experimental
6"
ABSTRACT,0.0075107296137339056,"studies. In this paper, we propose such a formalization and develop a quantifiable
7"
ABSTRACT,0.008583690987124463,"notion of generalizability. This notion allows to explore the generalizability of
8"
ABSTRACT,0.009656652360515022,"existing studies and to estimate the number of experiments needed to achieve the
9"
ABSTRACT,0.01072961373390558,"generalizability of new studies. To demonstrate its usefulness, we apply it to two
10"
ABSTRACT,0.011802575107296138,"recently published benchmarks to discern generalizable and non-generalizable
11"
ABSTRACT,0.012875536480686695,"results. We also publish a Python module that allows our analysis to be repeated
12"
ABSTRACT,0.013948497854077254,"for other experimental studies.
13"
INTRODUCTION,0.015021459227467811,"1
Introduction
14"
INTRODUCTION,0.016094420600858368,"Due to the importance of experimental studies, the machine learning (ML) community advocates for
15"
INTRODUCTION,0.017167381974248927,"high methodological standards [20, 12, 13, 17, 8, 31, 32, 44]. Failure to meet these standards can
16"
INTRODUCTION,0.018240343347639486,"have significant consequences, such as the ongoing reproducibility crisis [6, 47, 50, 51, 30].
17"
INTRODUCTION,0.019313304721030045,"Reproducibility is not the only desirable property of a study. For example, the reader expects that the
18"
INTRODUCTION,0.0203862660944206,"best encoders of categorical features identified in [41] will not only remain the best when the study
19"
INTRODUCTION,0.02145922746781116,"is reproduced, but will also outperform their competitors on new datasets. This property of getting
20"
INTRODUCTION,0.022532188841201718,"the same results from different data is known as replicability [46, 48]. Replicability is a special case
21"
INTRODUCTION,0.023605150214592276,"of generalizability, the property of obtaining the same results with any change in the inputs. The
22"
INTRODUCTION,0.02467811158798283,"assumption of generalizability is arguably the main motivation for extensive experimental studies and
23"
INTRODUCTION,0.02575107296137339,"benchmarks. However, existing definitions of generalizability do not quantify how well the results of
24"
INTRODUCTION,0.02682403433476395,"a study can be transferred to other contexts. This hinders the usefulness of such studies and leads to
25"
INTRODUCTION,0.027896995708154508,"confusion. For example, articles [38, 41, 49, 42] and [19, 8, 11, 13, 29, 43] report that the results of
26"
INTRODUCTION,0.028969957081545063,"experimental studies are often contradictory.
27"
INTRODUCTION,0.030042918454935622,"Quantifying generalizability can also help determine the appropriate size of experimental studies. For
28"
INTRODUCTION,0.03111587982832618,"example, one dataset is unlikely to be sufficient to draw far-reaching conclusions, but 106 datasets
29"
INTRODUCTION,0.032188841201716736,"are likely enough. Of course, such large studies are usually not practical: it is crucial to determine the
30"
INTRODUCTION,0.033261802575107295,"minimum amount of data needed to achieve generalizability. This principle also applies to decisions
31"
INTRODUCTION,0.034334763948497854,"other than the number of datasets, such as the choice of quality metric and the initialization seed.
32"
INTRODUCTION,0.03540772532188841,"A notion similar to generalizability is model replicability [1, 22, 23, 24, 33, 36, 37]. A model is
33"
INTRODUCTION,0.03648068669527897,"ρ-replicable if, given i.i.d. samples from the same data distribution, the trained models are the same
34"
INTRODUCTION,0.03755364806866953,"with probability 1 −ρ [33]. Adapting this definition to quantify generalizability is not trivial, as
35"
INTRODUCTION,0.03862660944206009,"it requires formalizing experimental studies. The latter must take into account several aspects: the
36"
INTRODUCTION,0.03969957081545064,"research question, the results of a study, and how to compare the results. Regarding the problem of
37"
INTRODUCTION,0.0407725321888412,"defining the size of experimental studies, the current literature addresses the (crucial, but orthogonal)
38"
INTRODUCTION,0.04184549356223176,"problem of choosing appropriate experimental factors [20, 12, 13, 17, 8, 31, 32, 44]. While these
39"
INTRODUCTION,0.04291845493562232,"studies recommend varying the factors, they do not help decide how many of the factor levels are
40"
INTRODUCTION,0.043991416309012876,"enough.
41"
INTRODUCTION,0.045064377682403435,"Our contributions are as follows:
42"
INTRODUCTION,0.046137339055793994,"1. we formalize experimental studies and their results;
43"
INTRODUCTION,0.04721030042918455,"2. we propose a quantifiable definition of the generalizability of experimental studies;
44"
INTRODUCTION,0.048283261802575105,"3. we develop an algorithm to estimate the size of a study to obtain generalizable results;
45"
INTRODUCTION,0.04935622317596566,"4. we consider two recent experimental studies on categorical encoders [41] and Large Lan-
46"
INTRODUCTION,0.05042918454935622,"guage Models [55] and show how their results may or may not be generalizable.
47"
INTRODUCTION,0.05150214592274678,"5. we will publish the GENEXPY1 Python module to repeat our analysis in other studies.
48"
INTRODUCTION,0.05257510729613734,"Paper outline: Section 2 is related work, Section 3 formalizes experimental studies, Section 4 defines
49"
INTRODUCTION,0.0536480686695279,"generalizability and provides the algorithm to estimate the required size of a study for generalizability,
50"
INTRODUCTION,0.05472103004291846,"Section 5 contains the case studies, Section 6 describes limitations and concludes.
51"
RELATED WORK,0.055793991416309016,"2
Related work
52"
RELATED WORK,0.05686695278969957,"We first discuss the literature related to the motivation we are tackling, i.e., why experimental studies
53"
RELATED WORK,0.05793991416309013,"may not generalize. Second, we overview the existing concept of model replicability, closely related
54"
RELATED WORK,0.059012875536480686,"to our work. Finally, we show other meanings that these words can assume in other domains.
55"
RELATED WORK,0.060085836909871244,"Non-generalizable results.
It is well known that experimental results can significantly vary based
56"
RELATED WORK,0.0611587982832618,"on design choices [38, 41, 49, 42]. Possible reasons include an insufficient number of datasets [19,
57"
RELATED WORK,0.06223175965665236,"41, 3, 12] as well as differences in hyperparameter tuning [13, 41], initialization seed [30], and
58"
RELATED WORK,0.06330472103004292,"hardware [56]. As a result, the statistical benchmarking literature advocates for experimenters to
59"
RELATED WORK,0.06437768240343347,"motivate their design choices [7, 43, 11, 13, 44] and clearly state the conclusions they are attempting
60"
RELATED WORK,0.06545064377682404,"to draw from their study [7, 45].
61"
RELATED WORK,0.06652360515021459,"Replicability and generalizability in ML.
Our work formalizes the definitions of replicability and
62"
RELATED WORK,0.06759656652360516,"generalizability given in [48, 46]. Intuitively, replicable work consists of repeating an experiment
63"
RELATED WORK,0.06866952789699571,"on different data, while generalizable work varies other factors as well — e.g., quality metric,
64"
RELATED WORK,0.06974248927038626,"implementation. A recent line of work, initiated by [33], has linked replicability to model stability: a
65"
RELATED WORK,0.07081545064377683,"ρ-replicable model learns (with probability 1 −ρ) the same parameters from different i.i.d. samples.
66"
RELATED WORK,0.07188841201716738,"This definition has later been adapted and applied to other learning algorithms [23], clustering [24],
67"
RELATED WORK,0.07296137339055794,"reinforcement learning [22, 37], convex optimization [1], and learning rules [36]. Recent efforts
68"
RELATED WORK,0.0740343347639485,"have been bridging the gap between replicability, differential privacy, generalization error, and global
69"
RELATED WORK,0.07510729613733906,"stability [15, 16, 26, 45, 21]. However, these applications remain limited to model replicability.
70"
RELATED WORK,0.07618025751072961,"Replicability and generalizability in Science.
In other fields of Science, generalizability and
71"
RELATED WORK,0.07725321888412018,"replicability take different meanings. In social sciences, generalizability theory is a tool to quantify
72"
RELATED WORK,0.07832618025751073,"the effect of different factors on numerical responses [14]. In medicine, the replicability proposed
73"
RELATED WORK,0.07939914163090128,"in [34] is the probability of observing a positive treatment effect in a meta-study. Although these
74"
RELATED WORK,0.08047210300429185,"concepts are related to generalizability of experimental studies, they are limited to purely numerical
75"
RELATED WORK,0.0815450643776824,"responses or specific study designs.
76"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.08261802575107297,"3
Experiments and experimental studies
77"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.08369098712446352,"An experimental study is a set of experiments comparing the same alternatives under different
78"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.08476394849785408,"experimental conditions. An experimental condition is a tuple of levels of experimental factors, the
79"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.08583690987124463,"parameters defining the experiments. Different factors play different roles in the study: the design
80"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.08690987124463519,"and held-constant factors are fixed by design, while the generalizability of a study is defined in terms
81"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.08798283261802575,"of the allowed-to-vary factors. The study aims at answering a research question, which defines its
82"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.0890557939914163,"scope and goals.
83"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.09012875536480687,1https://anonymous.4open.science/r/genexpy-B94D
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.09120171673819742,"Universe of valid 
experimental conditions
C"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.09227467811158799,LLM1 LLM2 LLM3
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.09334763948497854,"Alternatives 
A pos1 pos2"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.0944206008583691,"pos3
pos4 pos5 pos6"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.09549356223175966,"pos7
pos8
S S"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.09656652360515021,"E(pos5), E(pos7), E(pos8)"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.09763948497854077,"E(pos3), E(pos4), E(pos6)"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.09871244635193133,"Results of 
experiments Study"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.09978540772532189,Similar?
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.10085836909871244,Generalizability
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.10193133047210301,"Figure 1: Two empirical studies on the checkmate-in-one task, cf. Example 3.1."
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.10300429184549356,"Example 3.1. (The “checkmate-in-one” task, cf. Figure 1) An experimenter wants to compare three
84"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.10407725321888411,"Large Language Models (LLMs), the alternatives, on the “checkmate-in-one” task [55, 2, 5, 4, 18].
85"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.10515021459227468,"The assignment is to find the unique checkmating move from a position of pieces on a chessboard: an
86"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.10622317596566523,"LLM succeeds if and only if it outputs the correct move. The experimenter considers two experimental
87"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.1072961373390558,"factors: the number of shots, n, and the initial position on the chessboard, posl. The number of shots
88"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.10836909871244635,"is a design factor, while the initial position is an allowed-to-vary factor. The experimenter wants to
89"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.10944206008583691,"find if LLM1 ranks consistently against the other two LLMs when changing the initial position, for a
90"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.11051502145922747,"fixed number of shots.
91"
EXPERIMENTS AND EXPERIMENTAL STUDIES,0.11158798283261803,"The rest of this section defines the terms introduced above.
92"
EXPERIMENTS,0.11266094420600858,"3.1
Experiments
93"
EXPERIMENTS,0.11373390557939914,"An experiment evaluates all the considered alternatives under a valid experimental condition.
94"
EXPERIMENTS,0.1148068669527897,"Alternatives.
An alternative a ∈A is an object compared in the study, like an LLM in Example 3.1.
95"
EXPERIMENTS,0.11587982832618025,"Here, A is the set of alternatives considered in the study, with cardinality na.
96"
EXPERIMENTS,0.11695278969957082,"Experimental factors.
An experimental factor is anything that could, in principle, affect the result
97"
EXPERIMENTS,0.11802575107296137,"of an experiment. i denotes a factor, Ci the (possibly infinite) set of levels i can take, c ∈Ci a level
98"
EXPERIMENTS,0.11909871244635194,"of i, and I the set of all factors. We adapt Montgomery’s classification of experimental factors [44,
99"
EXPERIMENTS,0.12017167381974249,"Chapter 1] and discern between design factors, held-constant factors, and allowed-to-vary factors.
100"
EXPERIMENTS,0.12124463519313304,"• Design factors, e.g., whether and how to tune the hyperparameters, quality metrics, number
101"
EXPERIMENTS,0.1223175965665236,"of shots, are chosen by the experimenter.
102"
EXPERIMENTS,0.12339055793991416,"• Held-constant factors, e.g., implementation, initialization seed, number of cross-validated
103"
EXPERIMENTS,0.12446351931330472,"folds, may affect the outcome but are not in the scope of the experiment and are fixed by the
104"
EXPERIMENTS,0.1255364806866953,"experimenter.
105"
EXPERIMENTS,0.12660944206008584,"• Allowed-to-vary factors, e.g., “dataset” or “chessboard position” in Example 3.1, may affect
106"
EXPERIMENTS,0.1276824034334764,"the outcome but cannot be held constant: the experimenter expects results to generalize w.r.t.
107"
EXPERIMENTS,0.12875536480686695,"these factors; Iatv denotes them.
108"
EXPERIMENTS,0.1298283261802575,"Experimental conditions.
An experimental condition c is a tuple of levels of experimental factors,
109"
EXPERIMENTS,0.13090128755364808,c = (ci)i∈I ∈C ⊆Q
EXPERIMENTS,0.13197424892703863,"i∈I Ci. We endow C with a probability µ, as we will need to sample from it to
110"
EXPERIMENTS,0.13304721030042918,"define the result of a study in Section ??. The probability space (C, F, µ) is the universe of valid
111"
EXPERIMENTS,0.13412017167381973,"experimental conditions. C may not coincide with Q
i∈I Ci as some experimental conditions may be
112"
EXPERIMENTS,0.1351931330472103,"invalid, i.e., illegal or not of interest. Validity has to be assessed on a case-by-case basis. For instance,
113"
EXPERIMENTS,0.13626609442060086,"in Example 3.1, C = {(posl, n)}l,n, where posl is a legal configuration of pieces on a chessboard
114"
EXPERIMENTS,0.13733905579399142,"and m is the non-negative number of shots.
115"
EXPERIMENTS,0.13841201716738197,"Experimental results.
The experiment function E evaluates the alternatives A under a valid
116"
EXPERIMENTS,0.13948497854077252,"experimental condition c ∈C. Unless necessary, we consider A fixed and omit it in our notation.
117"
EXPERIMENTS,0.1405579399141631,"We require that E : C →Rna is a measurable function, for some fixed A. Finally, the result of an
118"
EXPERIMENTS,0.14163090128755365,"experiment E (A, c) is a ranking on A.
119"
EXPERIMENTS,0.1427038626609442,"Definition 3.1 (Ranking (with ties)). A ranking r on A is a transitive and reflexive binary endorelation
120"
EXPERIMENTS,0.14377682403433475,"on A. Equivalently, r is a totally ordered partition of A into tiers of equivalent alternatives. r(a)
121"
EXPERIMENTS,0.14484978540772533,"denotes the rank of a ∈A, i.e., the position of the tier of a in the ordering. W.l.o.g. (Rna, P(Rna))
122"
EXPERIMENTS,0.1459227467811159,"denotes the measure space of all rankings of na objects, where P indicates the power set.
123"
EXPERIMENTS,0.14699570815450644,"Example 3.1 (Continued). The result of an experiment on (posl, n) is a ranking of the three LLMs,
124"
EXPERIMENTS,0.148068669527897,"according to whether or not they output the checkmating move. Suppose that only LLM1 and LLM2
125"
EXPERIMENTS,0.14914163090128754,"output the correct move. Then E(posl, n) ranks LLM1 and LLM2 tied as best and LLM3 as worst.
126"
EXPERIMENTAL STUDIES,0.15021459227467812,"3.2
Experimental studies
127"
EXPERIMENTAL STUDIES,0.15128755364806867,"A study is defined by its research question Q, i.e., its scope and goals. The scope consists of the
128"
EXPERIMENTAL STUDIES,0.15236051502145923,"alternatives A, the valid experimental conditions C, and the allowed-to-vary factors Iatv. The goal is
129"
EXPERIMENTAL STUDIES,0.15343347639484978,"the kind of conclusions one is attempting to draw from the study. For now, the goal is a statement of
130"
EXPERIMENTAL STUDIES,0.15450643776824036,"interests, i.e., a set of strings.
131"
EXPERIMENTAL STUDIES,0.1555793991416309,"Definition 3.2 (Research question). The research question Q = (A, C, Iatv, goals) is a tuple contain-
132"
EXPERIMENTAL STUDIES,0.15665236051502146,"ing the set of alternatives A, the experimental conditions C, the set of allowed-to-vary-factors Iatv,
133"
EXPERIMENTAL STUDIES,0.157725321888412,"and the goals of the study.
134"
EXPERIMENTAL STUDIES,0.15879828326180256,"Example 3.1 (Continued). The research question of the “checkmate-in-one” study is as follows.
135"
EXPERIMENTAL STUDIES,0.15987124463519314,"The scope is

A = {LLMa}a=1,2,3 , C = {(posl, n)}l,n , Iatv = {“position”})

. The goal is “Does
136"
EXPERIMENTAL STUDIES,0.1609442060085837,"LLM1 rank consistently against the other LLMs?”
137"
EXPERIMENTAL STUDIES,0.16201716738197425,"A crucial element of our formalization is the distinction between ideal and empirical studies. An
138"
EXPERIMENTAL STUDIES,0.1630901287553648,"ideal study exhausts its research question; however, its result is not observable. An empirical study is
139"
EXPERIMENTAL STUDIES,0.16416309012875535,"an observable sample of an ideal study.
140"
IDEAL STUDIES,0.16523605150214593,"3.2.1
Ideal studies
141"
IDEAL STUDIES,0.16630901287553648,"The ideal study on a research question Q = (A, C, Iatv, goals) is the experimental study consist-
142"
IDEAL STUDIES,0.16738197424892703,"ing of an experiment for each valid experimental condition c ∈C. We say that such a study
143"
IDEAL STUDIES,0.1684549356223176,"exhausts Q. Hence, there exists exactly one ideal study on Q. The result of an ideal study is
144"
IDEAL STUDIES,0.16952789699570817,"the probability distribution of the results of its experiments. Recall that the experiment function
145"
IDEAL STUDIES,0.17060085836909872,"E : (C, F, µ) →(Rna, P (Rna)) is measurable.
146"
IDEAL STUDIES,0.17167381974248927,"Definition 3.3 (Result of an ideal study). The result of an ideal study with research question
147"
IDEAL STUDIES,0.17274678111587982,"Q = (A, C, Iatv, goals) is
148"
IDEAL STUDIES,0.17381974248927037,"S (Q) = P : Rna →[0, 1]"
IDEAL STUDIES,0.17489270386266095,"r 7→P (r) := µ
 
E−1(r)

,"
IDEAL STUDIES,0.1759656652360515,"where E−1(r) = {c : E(c) = r} ⊆C is the preimage of r through E.
149"
IDEAL STUDIES,0.17703862660944206,"In general, multiple experiments of a study may yield identical results. Definition 3.3 supports this by
150"
IDEAL STUDIES,0.1781115879828326,"assigning a higher probability mass to results that occur more often.
151"
EMPIRICAL STUDIES,0.1791845493562232,"3.2.2
Empirical studies
152"
EMPIRICAL STUDIES,0.18025751072961374,"Consider again a research question Q = (A, C, Iatv, goals). In practice, as C might be infinite or too
153"
EMPIRICAL STUDIES,0.1813304721030043,"large, one can only run experiments on a sample of valid experimental conditions {cj}N
j=1
iid
∼(C, µ).
154"
EMPIRICAL STUDIES,0.18240343347639484,"The study performed on {cj}N
j=1 is an empirical study on Q, of size N. As for ideal studies, the
155"
EMPIRICAL STUDIES,0.1834763948497854,"result of an empirical study is the probability distribution of the results of its experiments.
156"
EMPIRICAL STUDIES,0.18454935622317598,"Definition 3.4 (Result of an empirical study). The result of an empirical study on Q is
157"
EMPIRICAL STUDIES,0.18562231759656653,"ˆSN (Q) : Rna →[0, 1]"
EMPIRICAL STUDIES,0.18669527896995708,"r 7→#
n
j ∈{cj}N
j=1 : E (A, cj) = r
o
."
EMPIRICAL STUDIES,0.18776824034334763,"Where Q, {cj}N
j=1 is a research question and a set of valid experimental conditions as above.
158"
EMPIRICAL STUDIES,0.1888412017167382,"The result of an empirical study can be thought of as the empirical distribution of a sample following
159"
EMPIRICAL STUDIES,0.18991416309012876,"the distribution of the result of the corresponding ideal study. With a slight abuse of notation,
160"
EMPIRICAL STUDIES,0.19098712446351931,"indicating both the sample and its empirical distribution as ˆSN (Q), we write
161"
EMPIRICAL STUDIES,0.19206008583690987,"ˆSN (Q)
iid
∼S (Q) ."
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.19313304721030042,"4
Generalizability of experimental studies
162"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.194206008583691,"The currently accepted definition of generalizability is the property of two independent studies with
163"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.19527896995708155,"the same research question to yield similar results [46, 48]. Although intuitive, this notion is not
164"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.1963519313304721,"directly applicable as it does not provide a way to measure the generalizability of a study. We now
165"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.19742489270386265,"introduce a quantifiable notion of generalizability of experimental studies, as the probability that any
166"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.1984978540772532,"two empirical studies approximating the same ideal study yield similar results.
167"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.19957081545064378,"Definition 4.1 (Generalizability). Let Q = (A, C, Iatv, κ) be the research question of an ideal study,
168"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.20064377682403434,"let P = S(Q) be the result of that study, and let d be some distance between probability distributions.
169"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2017167381974249,"The generalizability of the ideal study on Q is
170"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.20278969957081544,"Gen (Q; ε, n) := Pn ⊗Pn 
(Xj, Yj)n
j=1 : d(X, Y ) ≤ε

,
(1)"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.20386266094420602,"where ε ∈R+ is a similarity threshold.
171"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.20493562231759657,"As the result of an ideal study is usually unobservable (cf. Section 3.2), we do not know the true
172"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.20600858369098712,"distribution P. However, we can observe the result of an empirical study, ˆPN = ˆSN (Q), which
173"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.20708154506437768,"approximates P under the assumption that the experimental conditions are i.i.d. samples from C. As
174"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.20815450643776823,"the sample size N increases (the empirical study becomes larger), ˆPN converges in distribution to P.
175"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2092274678111588,"Definition (1) requires a distance d between probability distributions. In the next sections, we propose
176"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.21030042918454936,"to use a generalizability based on kernels and Maximum Mean Discrepancy (MMD) [27], as it allows
177"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2113733905579399,"to compute generalizability w.r.t. different research questions. The underlying idea is that we can
178"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.21244635193133046,"capture the goal of a study with an appropriate kernel. We conclude this section with an algorithm to
179"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.21351931330472104,"estimate the number of experimental conditions required to obtain generalizable results.
180"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2145922746781116,"4.1
Similarity between rankings — kernels
181"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.21566523605150215,"Whether two experimental results (i.e., rankings) are similar or not ultimately depends on the goal of
182"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2167381974248927,"the study. For instance, consider two rankings on A = {a1, a2, a3}, r = (1, 2, 3) and r′ = (1, 3, 2),
183"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.21781115879828325,"where ri is the tier of alternative ai. The conclusions drawn from r and r′ are identical if one’s goal is
184"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.21888412017167383,"to find the best alternative, but very different if one’s goal is to obtain an ordering of the alternatives.
185"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.21995708154506438,"One can use kernels to quantify the similarity between experimental results. Kernels are suitable to
186"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.22103004291845493,"formalize the aspects of the result of a study one wants to generalize, i.e., the goals of the study. For
187"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.22210300429184548,"instance, one kernel is suitable to identify the best tier while another kernel focuses on the position of
188"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.22317596566523606,"a specific alternative. In the following, we describe three representative kernels that cover a wide
189"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.22424892703862662,"spectrum of possible goals.
190"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.22532188841201717,"Borda kernel.
The Borda kernel is suitable for goals in the form “Is the alternative a∗consistently
191"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.22639484978540772,"ranked the same?”. It uses the Borda count: the number of alternatives (weakly) dominated by a given
192"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.22746781115879827,"one [9]. For a pair of rankings, we compute the Borda counts of a∗, and then take their difference.
193"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.22854077253218885,"κa∗,ν
b
(r1, r2) = e−ν|b1−b2|,"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2296137339055794,"where bl = {a ∈A : rl(a) ≥rl(a∗)} is the number of alternatives dominated by a∗in rl and ν ∈R
194"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.23068669527896996,"is the kernel bandwidth. The Borda kernel takes values in

e(−νna), 1

. If ν is too large compared to
195"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2317596566523605,"1/|b1−b2|, the kernel is oversensitive and will penalize every deviation too much. On the contrary, if ν
196"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.23283261802575106,"is too small, the kernel is undersensitive and will not penalize deviations unless they are very large.
197"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.23390557939914164,"As |b1 −b2| ∈[0, na], we recommend ν = 1/na.
198"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2349785407725322,"Jaccard kernel.
The Jaccard kernel is suitable for goals in the form “Are the best alternatives
199"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.23605150214592274,"consistently the same ones?”. As it measures the similarity between sets [25, 10], we use it to compare
200"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2371244635193133,"the top-k tiers of two rankings.
201"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.23819742489270387,"κk
j (r1, r2) ="
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.23927038626609443,"r−1
1 ([k]) ∩r−1
2 ([k])

r−1
1 ([k]) ∪r−1
2 ([k])
,"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.24034334763948498,"where r−1([k]) = {a ∈A : r1(a) ≤k} is the set of alternatives whose rank is better than or equal to
202"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.24141630901287553,"k. The Jaccard kernel takes values in [0, 1].
203"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.24248927038626608,"Mallows kernel.
The Mallows kernel is suitable for goals in the form “Are the alternatives ranked
204"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.24356223175965666,"consistently?”. It measures the overall similarity between rankings [35, 40, 39]. We adapt the original
205"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2446351931330472,"definition in [39] for ties,
206"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.24570815450643776,"κν
m(r1, r2) = e−νnd,
where nd = P"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.24678111587982832,"a1,a2∈A |sign (r1(a1) −r1(a2)) −sign (r2(a1) −r2(a2))| is the number of discor-
207"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2478540772532189,"dant pairs and ν ∈R is the kernel bandwidth. If a pair is tied in one ranking but not in the other, one
208"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.24892703862660945,"counts it as half a discordant pair. The Mallows kernel takes values in

exp
 
−2ν
 na
2

, 1

. If ν is
209"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.25,"too large compared to 1/nd, the kernel is oversensitive and it will penalize every deviation too much.
210"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2510729613733906,"On the contrary, if ν is too small, the kernel is undersensitive and will not penalize deviations unless
211"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2521459227467811,"they are very large. As nd ∈

0,
 na
2

, we recommend ν = 1/(
na
2 ).
212"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2532188841201717,"4.2
Distance between distributions — Maximum Mean Discrepancy
213"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2542918454935622,"Having sorted out how to measure the similarity between the results of experiments, we now
214"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2553648068669528,"discuss how to measure the distance between the results of studies. We chose the Maximum Mean
215"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.25643776824034337,"Discrepancy (MMD) [27], for the following reasons. First, MMD is compatible with the kernels
216"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2575107296137339,"described in Section 4.1, i.e., it takes into consideration the goal of the studies. Second, it handles
217"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.25858369098712447,"sparse distributions well; this is needed as empirical studies are typically small compared to the
218"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.259656652360515,"number of all possible rankings, which grows exponentially in the number of alternatives. 2 Finally,
219"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2607296137339056,"it comes with bounds and theoretical guarantees, which we will use in Section 4.3.
220"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.26180257510729615,"Definition 4.2 (MMD (empirical distributions)). Let X be a set with a kernel κ, and let Q1 and Q2
221"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2628755364806867,"be two probability distributions on Rna. Let x = (xi)n
i=1 , y = (yi)m
i=1 be two i.i.d. samples from
222"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.26394849785407726,"Q1 and Q2 respectively. Then,
223"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.26502145922746784,"MMD (x, y)2 := 1 n2 n
X"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.26609442060085836,"i,j=1
κ(xi, xj) + 1 m2 m
X"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.26716738197424894,"i,j=1
κ(yi, yj) −
2
mn X"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.26824034334763946,"i=1...n
j=1...m"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.26931330472103004,"κ(xi, yj)."
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2703862660944206,"Proposition 4.1. MMD takes values in

0,
p"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.27145922746781115,"2 · (κsup −κinf)

, where κsup = supx,y∈X κ(x, y) and
224"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.27253218884120173,"κinf = infx,y∈X κ(x, y).
225"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.27360515021459225,"4.3
How many experiments ensure generalizability?
226"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.27467811158798283,"When designing a study, an experimenter has to decide how many experiments to run in order to
227"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2757510729613734,"obtain generalizable results. In other words, they need to choose a (minimum) sample size n∗that
228"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.27682403433476394,"achieves the desired generalizability α∗and the desired similarity ε∗.
229"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2778969957081545,"n∗= min {n ∈N0 : Gen (P; ε∗, n) ≥α∗} .
(2)"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.27896995708154504,"To estimate n∗we make use of a linear dependency between the logarithms of the sample size n and
230"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2800429184549356,"the logarithm of the α∗-quantile of MMD εα∗
n that we have observed in our experiments.
231"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2811158798283262,"Proposition 4.2. ∀α∗, there exist β0 ≥0, β1 ≤0 s.t.
232"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2821888412017167,"log(n) ≈β1 log

εα∗
n

+ β0
(3)"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2832618025751073,"Appendix A.3.2 provides a proof for a simplified case. Proposition 4.2 suggests that one can use a
233"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2843347639484979,"small set of N preliminary experiments to estimate n∗. One can then iteratively improve that estimate
234"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2854077253218884,"with the results of additional experiments.
235"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.286480686695279,"Our algorithm, shown in detail in Appendix A.3.3, requires specifying the desired generalizability,
236"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2875536480686695,"α∗, and the similarity threshold between the studies results, ε∗. Then, it performs the following steps:
237"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2886266094420601,"2Fubini or ordered Bell numbers, OEIS sequence A000670."
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.28969957081545067,"0.7
0.8
0.9
0.95
0.99
α∗ 10 20 30 40 n∗"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2907725321888412,"0.01
0.05
0.1
0.2
0.3
δ ∗"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2918454935622318,"g1
g2
g3"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2929184549356223,Figure 2: Predicted n∗for categorical encoders.
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2939914163090129,"1. it estimates the α∗-quantile of MMD for all n less than some budget nmax. If there exists an
238"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.29506437768240346,"n less than nmax that satisfies the condition in (2), we return it as n∗;
239"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.296137339055794,"2. it then fits the linear model in (3), computing the coefficients β0 and β1;
240"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.29721030042918456,"3. finally, it outputs n∗= exp
 
β1 log
 
εα∗
n

+ β0

, which satisfies the condition in (2) thanks
241"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.2982832618025751,"to Proposition 4.2.
242"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.29935622317596566,"In practice, choosing ε∗is hardly interpretable as it is a threshold on MMD. To solve this, we propose
243"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.30042918454935624,"choosing ε∗as a function of another parameter δ∗, such that
244"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.30150214592274677,"ε∗(δ∗) =
q"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.30257510729613735,2(κsup −fκ(δ∗)).
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.30364806866952787,"Here, δ∗represents the distance between two rankings as computed by the kernel and fκ is the
245"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.30472103004291845,"function linking the distance to the kernel value. For instance, for the Jaccard kernel, δ∗is simply
246"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.30579399141630903,"the Jaccard coefficient between the top-k tiers of two rankings, fκ(δ∗) = 1 −δ∗, and ε∗(δ∗) =
247
p"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.30686695278969955,"2(1 −(1 −δ∗)). For the Mallows kernel (with our recommendation for ν), δ∗is the fraction of
248"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.30793991416309013,"discordant pairs, fκ(x) = e−x, and ε∗(δ∗) =
p"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.3090128755364807,"2(1 −e−δ∗). As a concrete example, achieving
249"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.31008583690987124,"(α∗= 0.99, δ∗= 0.05)-generalizable results for the Jaccard kernel means that, with probability 0.99,
250"
GENERALIZABILITY OF EXPERIMENTAL STUDIES,0.3111587982832618,"the average Jaccard coefficient between two rankings drawn from the results is 0.95.
251"
CASE STUDIES,0.31223175965665234,"5
Case studies
252"
CASE STUDIES,0.3133047210300429,"5.1
Case Study 1: A benchmark of categorical encoders
253"
CASE STUDIES,0.3143776824034335,"We now evaluate the generalizability of a recent study [41] that analyzes the performance of encoders
254"
CASE STUDIES,0.315450643776824,"for categorical data. The performance of an encoder is approximated by the quality of a model trained
255"
CASE STUDIES,0.3165236051502146,"on the encoded data. The design factors are the model, the tuning strategy for the pipeline, and the
256"
CASE STUDIES,0.31759656652360513,"quality metric for the model, while the only allowed-to-vary factor is the dataset. We impute missing
257"
CASE STUDIES,0.3186695278969957,"values in the results of the study by assigning the worst rank. We evaluate how well the results of the
258"
CASE STUDIES,0.3197424892703863,"study generalize w.r.t. three goals:
259"
CASE STUDIES,0.3208154506437768,"(g1) Find out if One-Hot encoder (a popular encoder) ranks consistently amongst its competitors,
260"
CASE STUDIES,0.3218884120171674,"using the Borda kernel with ν = 1/na.
261"
CASE STUDIES,0.3229613733905579,"(g2) Investigate if some encoders outperform all the others using the Jaccard kernel with k = 1.
262"
CASE STUDIES,0.3240343347639485,"(g3) Evaluate whether the encoders are typically ranked in a similar order, using the Mallows
263"
CASE STUDIES,0.3251072961373391,"kernel with ν = 1/(
na
2 ).
264"
CASE STUDIES,0.3261802575107296,"Figure 2 shows the predicted n∗for different choices of α∗and δ∗, the other one fixed at 0.95
265"
CASE STUDIES,0.3272532188841202,"and 0.05 respectively. The variance in the boxes comes from variance in the design factors. For
266"
CASE STUDIES,0.3283261802575107,"example, the results for the design factors “decision tree, full tuning, accuracy” have a different
267"
CASE STUDIES,0.3293991416309013,"(α∗, δ∗)-generalizability than the results for “SVM, no tuning, accuracy”. We observe on the left
268"
CASE STUDIES,0.33047210300429186,"that — as expected — obtaining generalizable results requires more experiments as the desired
269"
CASE STUDIES,0.3315450643776824,"0.7
0.8
0.9
0.95
0.99
α∗ 0 20 40 60 n∗"
CASE STUDIES,0.33261802575107297,"0.01
0.05
0.1
0.2
0.3
δ ∗"
CASE STUDIES,0.33369098712446355,"g1
g2
g3"
CASE STUDIES,0.33476394849785407,Figure 3: Predicted n∗for LLMs.
CASE STUDIES,0.33583690987124465,"generalizability α∗increases. We can also see that the variance of the boxes increases with α∗. This
270"
CASE STUDIES,0.3369098712446352,"means that the choice of the design factors has a larger influence on the achieved generalizability.
271"
CASE STUDIES,0.33798283261802575,"We observe the same when decreasing δ∗, as it corresponds to a stricter similarity condition on the
272"
CASE STUDIES,0.33905579399141633,"rankings. In the rather extreme cases of α∗= 0.7 or δ∗= 0.3, even less than 10 datasets are enough
273"
CASE STUDIES,0.34012875536480686,"to achieve (α∗, δ∗)-generalizability.
274"
CASE STUDIES,0.34120171673819744,"Consider now goal g2 for two different choices of design factors: (A): “decision tree, full tuning,
275"
CASE STUDIES,0.34227467811158796,"accuracy” and (B): “SVM, full tuning, balanced accuracy”. Furthermore, let (α∗, δ∗) = (0.95, 0.05):
276"
CASE STUDIES,0.34334763948497854,"we estimate n∗= 28 for (A) and n∗= 34 for (B), corresponding to the bottom and top whiskers of
277"
CASE STUDIES,0.3444206008583691,"the corresponding box in Figure 2. As both (A) and (B) were evaluated using n = 30 experiments,
278"
CASE STUDIES,0.34549356223175964,"we conclude that the results of (A) are (barely) (0.95, 0.05)-generalizable, while those of (B) are not.
279"
CASE STUDIES,0.3465665236051502,"Hence, one should run more experiments with fixed factors (B) to make the study generalizable.
280"
CASE STUDIES,0.34763948497854075,"5.2
Case study 2: BIG-bench — A benchmark of Large Language Models
281"
CASE STUDIES,0.3487124463519313,"We now evaluate the generalizability of BIG-bench [55], a collaborative benchmark of Large Lan-
282"
CASE STUDIES,0.3497854077253219,"guage Models (LLMs). The benchmark compares LLMs on different tasks, such as the checkmate-
283"
CASE STUDIES,0.35085836909871243,"in-one task (cf. Example 3.1), and for different numbers of shots. Task and number of shots are the
284"
CASE STUDIES,0.351931330472103,"design factors. Every task has a number of subtasks, which is the allowed-to-vary factor. We stick to
285"
CASE STUDIES,0.3530042918454936,"the preferred scoring for each subtask. As the results have too many missing values to impute them,
286"
CASE STUDIES,0.3540772532188841,"we only consider the experimental conditions where at least 80% of the LLMs had results, and to the
287"
CASE STUDIES,0.3551502145922747,"LLMs whose results cover at least 80% of the conditions.
288"
CASE STUDIES,0.3562231759656652,"Similar to before, we define the three goals as follows:
289"
CASE STUDIES,0.3572961373390558,"(g1) Find out if GPT3 (to date, one of the most popular LLMs) ranks consistently amongst its
290"
CASE STUDIES,0.3583690987124464,"competitors, using the Borda kernel with ν = 1/na.
291"
CASE STUDIES,0.3594420600858369,"(g2) Investigate if some encoders outperform all the others using the Jaccard kernel with k = 1.
292"
CASE STUDIES,0.3605150214592275,"(g3) Evaluate whether the LLMs are typically ranked in a similar order, using the Mallows kernel
293"
CASE STUDIES,0.361587982832618,"with ν = 1/(
na
2 ).
294"
CASE STUDIES,0.3626609442060086,"Figure 3 shows the predicted n∗for different choices of α∗and δ∗, the other one fixed at 0.95 and
295"
CASE STUDIES,0.36373390557939916,"0.05 respectively. Again, the variance in the boxes comes from variance in the design factors, i.e., the
296"
CASE STUDIES,0.3648068669527897,"task and the number of shots. As before, increasing α∗or decreasing δ∗leads to higher n∗. Unlike in
297"
CASE STUDIES,0.36587982832618027,"the previous section, n∗for g2 greatly depends on the combination of fixed factors, as we now detail.
298"
CASE STUDIES,0.3669527896995708,"Consider now goal g2 for two different choices of design factors: (A): “conlang_translation, 0 shots”,
299"
CASE STUDIES,0.36802575107296137,"and (B): “arithmetic, 2 shots”. Furthermore, let (α∗, δ∗) = (0.95, 0.05). For this choice of of
300"
CASE STUDIES,0.36909871244635195,"parameters, we estimate n∗= 44 for (A), corresponding to the top whisker of the corresponding box
301"
CASE STUDIES,0.3701716738197425,"in Figure 2. As the study evaluates (A) on 10 subtasks, it is therefore not (0.95, 0.05)-generalizable.
302"
CASE STUDIES,0.37124463519313305,"In fact, we estimate that this would require 34 more subtasks. For (B), on the other hand, we estimate
303"
CASE STUDIES,0.3723175965665236,"n∗= 1: the best 2-shot LLM for the observed subtasks is always PALM 535B. Hence, the result of a
304"
CASE STUDIES,0.37339055793991416,"single experiment is enough to achieve (0.95, 0.05)-generalizability.
305"
CASE STUDIES,0.37446351931330474,"10
20
30
40
N 0.0 0.2 0.4 0.6 0.8"
CASE STUDIES,0.37553648068669526,relative error
CASE STUDIES,0.37660944206008584,(a) Categorical encoders
CASE STUDIES,0.3776824034334764,"10
20
30
40
N 0.0 0.2 0.4 0.6 0.8"
CASE STUDIES,0.37875536480686695,relative error
CASE STUDIES,0.3798283261802575,"g1
g2
g3"
CASE STUDIES,0.38090128755364805,(b) LLMs
CASE STUDIES,0.38197424892703863,"Figure 4: Relative error in the estimate of n∗against n∗
50."
CASE STUDIES,0.3830472103004292,"Note that, although we correctly estimated n∗= 1 for (B), this estimate relies on 10 preliminary
306"
CASE STUDIES,0.38412017167381973,"experiments. In other words, our algorithm was able to quantify in hindsight that a single experiment
307"
CASE STUDIES,0.3851931330472103,"would have been enough to obtain generalizable results. Of course, however, one cannot trust an
308"
CASE STUDIES,0.38626609442060084,"estimate of n∗based on only one experiment. The next section thus investigates how the number of
309"
CASE STUDIES,0.3873390557939914,"preliminary experiments influences the estimate of n∗.
310"
CASE STUDIES,0.388412017167382,"5.3
How many preliminary experiments?
311"
CASE STUDIES,0.3894849785407725,"This section evaluates the influence of the number of preliminary experiments N on n∗. For each
312"
CASE STUDIES,0.3905579399141631,"study, we consider the design factor combinations for which we have at least 50 experiments. This
313"
CASE STUDIES,0.3916309012875536,"results in 23 out of 48 combinations for the categorical encoders and 9 out of 24 combinations for
314"
CASE STUDIES,0.3927038626609442,"the LLMs. For each of those combinations, we consider the estimate n∗
50 made at N = 50 as the
315"
CASE STUDIES,0.3937768240343348,"ground truth and observe how the estimates of n∗for N < 50 differ. Figure 4 shows the relative error
316"
CASE STUDIES,0.3948497854077253,"|n∗
N−n∗
50|/n∗
50, for different goals: the relative errors behave very differently. For goal g3 (Mallows
317"
CASE STUDIES,0.3959227467811159,"kernel), even n∗
10 is close to n∗
50 for a majority of the design factor combinations. On the contrary,
318"
CASE STUDIES,0.3969957081545064,"one needs 20 to 30 preliminary experiments for goal g1 (Borda kernel). This means that knowing the
319"
CASE STUDIES,0.398068669527897,"goals of a study when performing preliminary experiments can help understand how trustworthy the
320"
CASE STUDIES,0.39914163090128757,"estimate of n∗is.
321"
CONCLUSION,0.4002145922746781,"6
Conclusion
322"
CONCLUSION,0.4012875536480687,"Limitations & future work.
First, we dealt with experimental results as rankings. Other forms of
323"
CONCLUSION,0.40236051502145925,"results, e.g., the absolute performance of alternatives according to some quality measure, will require
324"
CONCLUSION,0.4034334763948498,"the development of appropriate kernels. Second, our approach uses kernels to compute the similarity
325"
CONCLUSION,0.40450643776824036,"of experimental results and MMD the distance between the results of studies. There are, however.
326"
CONCLUSION,0.4055793991416309,"other possible choices. Third, we processed missing evaluations by either dropping them or imputing
327"
CONCLUSION,0.40665236051502146,"them. One could analyze different solutions, for instance by adapting the kernels to missing values.
328"
CONCLUSION,0.40772532188841204,"Fourth, we estimate the distribution of the MMD by sampling multiple times from the results. A
329"
CONCLUSION,0.40879828326180256,"non-asymptotic theory of MMD, at least for some kernels, might yield more insights in improving
330"
CONCLUSION,0.40987124463519314,"this procedure. Fifth, we plan to investigate the possibility of actively selecting experiments to obtain
331"
CONCLUSION,0.41094420600858367,"good estimates of the required size n∗with less preliminary experiments. Sixth and related to the
332"
CONCLUSION,0.41201716738197425,"previous one, we intend to obtain some guarantees on the convergence of n∗to the true value.
333"
CONCLUSION,0.4130901287553648,"Conclusions.
An experimental study is generalizable if, with high probability, its findings will hold
334"
CONCLUSION,0.41416309012875535,"under different experimental conditions, e.g., on unseen datasets. Non-generalizable studies might be
335"
CONCLUSION,0.41523605150214593,"of limited use or even misleading. This study is, to our knowledge, the first to develop a quantifiable
336"
CONCLUSION,0.41630901287553645,"notion for the generalizability of experimental studies. To achieve this, we formalize experiments,
337"
CONCLUSION,0.41738197424892703,"experimental studies and their results — rankings and distributions over rankings. Our approach
338"
CONCLUSION,0.4184549356223176,"allows us to estimate the number of experiments needed to achieve a desired level of generalizability
339"
CONCLUSION,0.41952789699570814,"in new experimental studies. We demonstrate its utility showing generalizable and non-generalizable
340"
CONCLUSION,0.4206008583690987,"results in two recent experimental studies.
341"
CONCLUSION,0.4216738197424893,"Acknowledgments
342"
CONCLUSION,0.4227467811158798,"...
343"
REFERENCES,0.4238197424892704,"References
344"
REFERENCES,0.4248927038626609,"[1]
Kwangjun Ahn et al. “Reproducibility in Optimization: Theoretical Framework and Limits”.
345"
REFERENCES,0.4259656652360515,"In: NeurIPS. 2022.
346"
REFERENCES,0.4270386266094421,"[2]
Scott Alexander. “A very unlikely chess game, 2020”. In: URL https://slatestarcodex.
347"
REFERENCES,0.4281115879828326,"com/2020/01/06/a-very-unlikely-chessgame/.(cited on pp. 29 and 30) ().
348"
REFERENCES,0.4291845493562232,"[3]
Maxime Alvarez et al. “A Revealing Large-Scale Evaluation of Unsupervised Anomaly
349"
REFERENCES,0.4302575107296137,"Detection Algorithms”. In: CoRR abs/2204.09825 (2022).
350"
REFERENCES,0.4313304721030043,"[4]
Prithviraj Ammanabrolu et al. “Bringing stories alive: Generating interactive fiction worlds”.
351"
REFERENCES,0.43240343347639487,"In: Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital
352"
REFERENCES,0.4334763948497854,"Entertainment. Vol. 16. 1. 2020, pp. 3–9.
353"
REFERENCES,0.434549356223176,"[5]
Prithviraj Ammanabrolu et al. “Toward automated quest generation in text-adventure games”.
354"
REFERENCES,0.4356223175965665,"In: arXiv preprint arXiv:1909.06283 (2019).
355"
REFERENCES,0.4366952789699571,"[6]
Monya Baker. “1,500 scientists lift the lid on reproducibility”. In: Nature 533.7604 (2016).
356"
REFERENCES,0.43776824034334766,"[7]
Thomas Bartz-Beielstein et al. “Benchmarking in Optimization: Best Practice and Open
357"
REFERENCES,0.4388412017167382,"Issues”. In: CoRR abs/2007.03488 (2020).
358"
REFERENCES,0.43991416309012876,"[8]
Alessio Benavoli et al. “Time for a Change: a Tutorial for Comparing Multiple Classifiers
359"
REFERENCES,0.4409871244635193,"Through Bayesian Analysis”. In: J. Mach. Learn. Res. 18 (2017), 77:1–77:36.
360"
REFERENCES,0.44206008583690987,"[9]
JC de Borda. “M’emoire sur les’ elections au scrutin”. In: Histoire de l’Acad’emie Royale des
361"
REFERENCES,0.44313304721030045,"Sciences (1781).
362"
REFERENCES,0.44420600858369097,"[10]
Mathieu Bouchard, Anne-Laure Jousselme, and Pierre-Emmanuel Doré. “A proof for the
363"
REFERENCES,0.44527896995708155,"positive definiteness of the Jaccard index matrix”. In: International Journal of Approximate
364"
REFERENCES,0.44635193133047213,"Reasoning 54.5 (2013), pp. 615–626.
365"
REFERENCES,0.44742489270386265,"[11]
Anne-Laure Boulesteix, Rory Wilson, and Alexander Hapfelmeier. “Towards evidence-based
366"
REFERENCES,0.44849785407725323,"computational statistics: lessons from clinical research on the role and design of real-data
367"
REFERENCES,0.44957081545064376,"benchmark studies”. In: BMC Medical Research Methodology 17 (2017), pp. 1–12.
368"
REFERENCES,0.45064377682403434,"[12]
Anne-Laure Boulesteix et al. “A statistical framework for hypothesis testing in real data
369"
REFERENCES,0.4517167381974249,"comparison studies”. In: The American Statistician 69.3 (2015), pp. 201–212.
370"
REFERENCES,0.45278969957081544,"[13]
Xavier Bouthillier et al. “Accounting for Variance in Machine Learning Benchmarks”. In:
371"
REFERENCES,0.453862660944206,"MLSys. mlsys.org, 2021.
372"
REFERENCES,0.45493562231759654,"[14]
Robert L Brennan. “Generalizability theory”. In: Educational Measurement: Issues and Prac-
373"
REFERENCES,0.4560085836909871,"tice 11.4 (1992), pp. 27–34.
374"
REFERENCES,0.4570815450643777,"[15]
Mark Bun et al. “Stability Is Stable: Connections between Replicability, Privacy, and Adaptive
375"
REFERENCES,0.4581545064377682,"Generalization”. In: STOC. ACM, 2023, pp. 520–527.
376"
REFERENCES,0.4592274678111588,"[16]
Zachary Chase, Shay Moran, and Amir Yehudayoff. “Stability and Replicability in Learning”.
377"
REFERENCES,0.46030042918454933,"In: FOCS. IEEE, 2023, pp. 2430–2439.
378"
REFERENCES,0.4613733905579399,"[17]
Giorgio Corani et al. “Statistical comparison of classifiers through Bayesian hierarchical
379"
REFERENCES,0.4624463519313305,"modelling”. In: Mach. Learn. 106.11 (2017), pp. 1817–1837.
380"
REFERENCES,0.463519313304721,"[18]
Sahith Dambekodi et al. “Playing text-based games with common sense”. In: arXiv preprint
381"
REFERENCES,0.4645922746781116,"arXiv:2012.02757 (2020).
382"
REFERENCES,0.4656652360515021,"[19]
Mostafa Dehghani et al. “The Benchmark Lottery”. In: CoRR abs/2107.07002 (2021).
383"
REFERENCES,0.4667381974248927,"[20]
Janez Demsar. “Statistical Comparisons of Classifiers over Multiple Data Sets”. In: J. Mach.
384"
REFERENCES,0.4678111587982833,"Learn. Res. 7 (2006), pp. 1–30.
385"
REFERENCES,0.4688841201716738,"[21]
Peter Dixon et al. “List and Certificate Complexities in Replicable Learning”. In: NeurIPS.
386"
REFERENCES,0.4699570815450644,"2023.
387"
REFERENCES,0.47103004291845496,"[22]
Eric Eaton et al. “Replicable Reinforcement Learning”. In: NeurIPS. 2023.
388"
REFERENCES,0.4721030042918455,"[23]
Hossein Esfandiari et al. “Replicable Bandits”. In: ICLR. OpenReview.net, 2023.
389"
REFERENCES,0.47317596566523606,"[24]
Hossein Esfandiari et al. “Replicable Clustering”. In: NeurIPS. 2023.
390"
REFERENCES,0.4742489270386266,"[25]
Thomas Gärtner, Quoc Viet Le, and Alex J Smola. “A short tour of kernel methods for graphs”.
391"
REFERENCES,0.47532188841201717,"In: Under Preparation (2006).
392"
REFERENCES,0.47639484978540775,"[26]
Badih Ghazi et al. “On User-Level Private Convex Optimization”. In: ICML. Vol. 202. Pro-
393"
REFERENCES,0.47746781115879827,"ceedings of Machine Learning Research. PMLR, 2023, pp. 11283–11299.
394"
REFERENCES,0.47854077253218885,"[27]
Arthur Gretton et al. “A Kernel Method for the Two-Sample-Problem”. In: NIPS. MIT Press,
395"
REFERENCES,0.4796137339055794,"2006, pp. 513–520.
396"
REFERENCES,0.48068669527896996,"[28]
Arthur Gretton et al. “A Kernel Two-Sample Test”. In: J. Mach. Learn. Res. 13 (2012), pp. 723–
397"
REFERENCES,0.48175965665236054,"773.
398"
REFERENCES,0.48283261802575106,"[29]
Odd Erik Gundersen, Kevin L. Coakley, and Christine R. Kirkpatrick. “Sources of Irrepro-
399"
REFERENCES,0.48390557939914164,"ducibility in Machine Learning: A Review”. In: CoRR abs/2204.07610 (2022).
400"
REFERENCES,0.48497854077253216,"[30]
Odd Erik Gundersen et al. “On Reporting Robust and Trustworthy Conclusions from Model
401"
REFERENCES,0.48605150214592274,"Comparison Studies Involving Neural Networks and Randomness”. In: ACM-REP. ACM,
402"
REFERENCES,0.4871244635193133,"2023, pp. 37–61.
403"
REFERENCES,0.48819742489270385,"[31]
Torsten Hothorn et al. “The design and analysis of benchmark experiments”. In: Journal of
404"
REFERENCES,0.4892703862660944,"Computational and Graphical Statistics 14.3 (2005), pp. 675–699.
405"
REFERENCES,0.490343347639485,"[32]
Karl Huppler. “The Art of Building a Good Benchmark”. In: TPCTC. Vol. 5895. Lecture Notes
406"
REFERENCES,0.49141630901287553,"in Computer Science. Springer, 2009, pp. 18–30.
407"
REFERENCES,0.4924892703862661,"[33]
Russell Impagliazzo et al. “Reproducibility in learning”. In: STOC. ACM, 2022, pp. 818–831.
408"
REFERENCES,0.49356223175965663,"[34]
Iman Jaljuli et al. “Quantifying replicability and consistency in systematic reviews”. In:
409"
REFERENCES,0.4946351931330472,"Statistics in Biopharmaceutical Research 15.2 (2023), pp. 372–385.
410"
REFERENCES,0.4957081545064378,"[35]
Yunlong Jiao and Jean-Philippe Vert. “The Kendall and Mallows Kernels for Permutations”.
411"
REFERENCES,0.4967811158798283,"In: IEEE Trans. Pattern Anal. Mach. Intell. 40.7 (2018), pp. 1755–1769.
412"
REFERENCES,0.4978540772532189,"[36]
Alkis Kalavasis et al. “Statistical Indistinguishability of Learning Algorithms”. In: ICML.
413"
REFERENCES,0.4989270386266094,"Vol. 202. Proceedings of Machine Learning Research. PMLR, 2023, pp. 15586–15622.
414"
REFERENCES,0.5,"[37]
Amin Karbasi et al. “Replicability in Reinforcement Learning”. In: NeurIPS. 2023.
415"
REFERENCES,0.5010729613733905,"[38]
Fred Lu, Edward Raff, and James Holt. “A Coreset Learning Reality Check”. In: AAAI. AAAI
416"
REFERENCES,0.5021459227467812,"Press, 2023, pp. 8940–8948.
417"
REFERENCES,0.5032188841201717,"[39]
Colin L Mallows. “Non-null ranking models. I”. In: Biometrika 44.1/2 (1957), pp. 114–130.
418"
REFERENCES,0.5042918454935622,"[40]
Horia Mania et al. “On kernel methods for covariates that are rankings”. In: (2018).
419"
REFERENCES,0.5053648068669528,"[41]
Federico Matteucci, Vadim Arzamasov, and Klemens Böhm. “A benchmark of categorical
420"
REFERENCES,0.5064377682403434,"encoders for binary classification”. In: NeurIPS. 2023.
421"
REFERENCES,0.5075107296137339,"[42]
Duncan C. McElfresh et al. “On the Generalizability and Predictability of Recommender
422"
REFERENCES,0.5085836909871244,"Systems”. In: NeurIPS. 2022.
423"
REFERENCES,0.509656652360515,"[43]
Iven Van Mechelen et al. “A white paper on good research practices in benchmarking: The
424"
REFERENCES,0.5107296137339056,"case of cluster analysis”. In: WIREs Data. Mining. Knowl. Discov. 13.6 (2023).
425"
REFERENCES,0.5118025751072961,"[44]
Douglas C Montgomery. Design and analysis of experiments. John wiley & sons, 2017.
426"
REFERENCES,0.5128755364806867,"[45]
Shay Moran, Hilla Schefler, and Jonathan Shafer. “The Bayesian Stability Zoo”. In: NeurIPS.
427"
REFERENCES,0.5139484978540773,"2023.
428"
REFERENCES,0.5150214592274678,"[46]
Engineering National Academies of Sciences, Medicine, et al. Reproducibility and replicability
429"
REFERENCES,0.5160944206008584,"in science. National Academies Press, 2019.
430"
REFERENCES,0.5171673819742489,"[47]
Roger D Peng. “Reproducible research in computational science”. In: Science 334.6060 (2011),
431"
REFERENCES,0.5182403433476395,"pp. 1226–1227.
432"
REFERENCES,0.51931330472103,"[48]
Joelle Pineau et al. “Improving Reproducibility in Machine Learning Research(A Report from
433"
REFERENCES,0.5203862660944206,"the NeurIPS 2019 Reproducibility Program)”. In: J. Mach. Learn. Res. 22 (2021), 164:1–
434"
REFERENCES,0.5214592274678111,"164:20.
435"
REFERENCES,0.5225321888412017,"[49]
Zhen Qin et al. “RD-Suite: A Benchmark for Ranking Distillation”. In: NeurIPS. 2023.
436"
REFERENCES,0.5236051502145923,"[50]
Edward Raff. “Does the Market of Citations Reward Reproducible Work?” In: ACM-REP.
437"
REFERENCES,0.5246781115879828,"ACM, 2023, pp. 89–96.
438"
REFERENCES,0.5257510729613734,"[51]
Edward Raff. “Research Reproducibility as a Survival Analysis”. In: AAAI. AAAI Press, 2021,
439"
REFERENCES,0.526824034334764,"pp. 469–478.
440"
REFERENCES,0.5278969957081545,"[52]
Isaac J Schoenberg. “Metric spaces and positive definite functions”. In: Transactions of the
441"
REFERENCES,0.528969957081545,"American Mathematical Society 44.3 (1938), pp. 522–536.
442"
REFERENCES,0.5300429184549357,"[53]
Bernhard Schölkopf. “The kernel trick for distances”. In: Advances in neural information
443"
REFERENCES,0.5311158798283262,"processing systems 13 (2000).
444"
REFERENCES,0.5321888412017167,"[54]
J Laurie Snell and John G Kemeny. “Mathematical models in the social sciences”. In: (No
445"
REFERENCES,0.5332618025751072,"Title) (1962).
446"
REFERENCES,0.5343347639484979,"[55]
Aarohi Srivastava et al. “Beyond the Imitation Game: Quantifying and extrapolating the
447"
REFERENCES,0.5354077253218884,"capabilities of language models”. In: CoRR abs/2206.04615 (2022).
448"
REFERENCES,0.5364806866952789,"[56]
Donglin Zhuang et al. “Randomness in Neural Network Training: Characterizing the Impact
449"
REFERENCES,0.5375536480686696,"of Tooling”. In: MLSys. mlsys.org, 2022.
450"
REFERENCES,0.5386266094420601,"A
Details for Section 4
451"
REFERENCES,0.5396995708154506,"A.1
Details for Section 4.1
452"
REFERENCES,0.5407725321888412,"This section contains the proofs to show that the similarities introduced in Section 4.1 are kernels,
453"
REFERENCES,0.5418454935622318,"i.e., symmetric and positive definite functions. As symmetry is a clear property of all of them, we
454"
REFERENCES,0.5429184549356223,"only discuss their positive definiteness. Our proofs for the Borda and Mallows kernels follow that
455"
REFERENCES,0.5439914163090128,"in [35]: we define a distance d on the set of rankings Rna and show that (Rna, d) is isometric to an
456"
REFERENCES,0.5450643776824035,"L2 space. This ensures that d is a conditionally positive definite (c.p.d.) function and, thus, that e−νd
457"
REFERENCES,0.546137339055794,"is positive definite [52, 53]. Our proof for the Jaccard kernel, instead, follows without much effort
458"
REFERENCES,0.5472103004291845,"from previous results. For ease of reading, we restate the definitions as well.
459"
REFERENCES,0.5482832618025751,Definition A.1 (Borda kernel).
REFERENCES,0.5493562231759657,"κa∗,ν
b
(r1, r2) = e−ν|d1−d2|,
(4)"
REFERENCES,0.5504291845493562,"where dl = {a ∈A : rl(a) ≥rl(a∗)} is the number of alternatives dominated by a∗in rl and ν ∈R.
460"
REFERENCES,0.5515021459227468,"Proposition A.1. The Borda kernel as defined in (4) is a kernel.
461"
REFERENCES,0.5525751072961373,"Proof. Define a distance
462"
REFERENCES,0.5536480686695279,d : Rna × Rna →R+
REFERENCES,0.5547210300429185,"(r1, r2) 7→|d1, d2| ,"
REFERENCES,0.555793991416309,"where dl = {a ∈A : rl(a) ≥rl(a∗)} is the number of alternatives dominated by a∗in rl. Now,
463"
REFERENCES,0.5568669527896996,"(Rna, d) is isometric to (R, ∥·∥2) via the map rl 7→dl. Hence, d is c.p.d. and κb is a kernel.
464"
REFERENCES,0.5579399141630901,Definition A.2 (Jaccard kernel).
REFERENCES,0.5590128755364807,"κk
j (r1, r2) ="
REFERENCES,0.5600858369098712,"r−1
1 ([k]) ∩r−1
2 ([k])

r−1
1 ([k]) ∪r−1
2 ([k])
,
(5)"
REFERENCES,0.5611587982832618,"where r−1([k]) = {a ∈A : r1(a) ≤k} is the set of alternatives whose rank is better than or equal to
465"
REFERENCES,0.5622317596566524,"k.
466"
REFERENCES,0.5633047210300429,"Proposition A.2. The Jaccard kernel as defined in (5) is a kernel.
467"
REFERENCES,0.5643776824034334,"Proof. It is already know that the Jaccard coefficients for sets is a kernel [25, 10]. As the Jaccard
468"
REFERENCES,0.5654506437768241,"kernel for rankings is equivalent to the Jaccard coefficient for the k-best tiers of said rankings, the
469"
REFERENCES,0.5665236051502146,"former is also a kernel.
470"
REFERENCES,0.5675965665236051,Definition A.3 (Mallows kernel).
REFERENCES,0.5686695278969958,"κν
m(r1, r2) = e−νnd,
(6)"
REFERENCES,0.5697424892703863,where nd = P
REFERENCES,0.5708154506437768,"a1,a2∈A |sign (r1(a1) −r1(a2)) −sign (r2(a1) −r2(a2))| is the number of discor-
471"
REFERENCES,0.5718884120171673,"dant pairs and ν ∈R is the kernel bandwidth.
472"
REFERENCES,0.572961373390558,"Proposition A.3. The Mallows kernel as defined in (6) is a kernel.
473"
REFERENCES,0.5740343347639485,"Proof. The number of discordant pairs nd is a distance on Rna [54]. Consider now the mapping of a
474"
REFERENCES,0.575107296137339,"ranking into its adjacency matrix,
475"
REFERENCES,0.5761802575107297,"Φ : Rna →{0, 1}na×na"
REFERENCES,0.5772532188841202,"r 7→(sign (r(i) −r(j)))na
i,j=1 ."
REFERENCES,0.5783261802575107,"Then,
476"
REFERENCES,0.5793991416309013,"nd = ∥Φ(r1) −Φ(r2)∥1 = ∥Φ(r1) −Φ(r2)∥2
2
where ∥·∥p indicates the entry-wise matrix p-norm and the equality holds because the entries of the
477"
REFERENCES,0.5804721030042919,"matrices are either 0 or 1. As a consequence, (Rna, nd) is isometric to (Rna×na, ∥·∥2) via Φ. Hence,
478"
REFERENCES,0.5815450643776824,"nd is c.p.d. and κm is a kernel.
479"
REFERENCES,0.5826180257510729,"A.2
Details for Section 4.2
480"
REFERENCES,0.5836909871244635,"Proposition 4.1. MMD takes values in

0,
p"
REFERENCES,0.5847639484978541,"2 · (κsup −κinf)

, where κsup = supx,y∈X κ(x, y) and
481"
REFERENCES,0.5858369098712446,"κinf = infx,y∈X κ(x, y).
482"
REFERENCES,0.5869098712446352,Proof.
REFERENCES,0.5879828326180258,"0 ≤MMDκ (x, y)2 = 1 n2 n
X"
REFERENCES,0.5890557939914163,"i,j=1
κ(xi, xj) + 1 m2 m
X"
REFERENCES,0.5901287553648069,"i,j=1
κ(yi, yj) −
2
mn X"
REFERENCES,0.5912017167381974,"i=1...n
j=1...m"
REFERENCES,0.592274678111588,"κ(xi, yj)
(7) ≤1 n2 n
X"
REFERENCES,0.5933476394849786,"i,j=1
κsup + 1 m2 n
X"
REFERENCES,0.5944206008583691,"i,j=1
κsup −
2
mn X"
REFERENCES,0.5954935622317596,"i=1...n
j=1...m κinf"
REFERENCES,0.5965665236051502,= 2(κsup −κinf) 483
REFERENCES,0.5976394849785408,"A.3
Details for Section 4.3
484"
REFERENCES,0.5987124463519313,"A.3.1
Choice of α∗, ε∗, and δ∗
485"
REFERENCES,0.5997854077253219,"Consider a research question Q = (A, C, Iatv, κ) and the corresponding ideal study with result
486"
REFERENCES,0.6008583690987125,"P. The algorithm introduced in Section 4.3 aims at finding the minimum n∗such that, given two
487"
REFERENCES,0.601931330472103,"independent empirical studies on Q, they achieve similar results. It has two hyperparameters, α∗and
488"
REFERENCES,0.6030042918454935,"ε∗. α∗∈[0, 1] is the generalizability that one wants to achieve from the study, i.e., the probability
489"
REFERENCES,0.6040772532188842,"that two independent realizations of the same ideal study will yield similar results. ε∗∈R+ is a
490"
REFERENCES,0.6051502145922747,"similarity threshold: the results of two empirical studies x, y
iid
∼P are similar if MMDκ(x, y) ≤ε∗.
491"
REFERENCES,0.6062231759656652,"However, as it is, ε∗is not interpretable. Instead, adapting the proof of Proposition 4.1, we can bound
492"
REFERENCES,0.6072961373390557,"MMD by imposing a condition on the kernel, as we’ll now illustrate. The key remark is that we are
493"
REFERENCES,0.6083690987124464,"looking for a condition in the form
494"
REFERENCES,0.6094420600858369,"MMDκ (x, y) ≤ε∗=
q"
REFERENCES,0.6105150214592274,"2(κsup −δ′),"
REFERENCES,0.6115879828326181,"where δ′ ∈[0, κsup] replaces the third summatory in (7). In other terms, we can interpret δ′ as the
495"
REFERENCES,0.6126609442060086,"minimum acceptable value for the average of the kernel, EP2 [κ(x, y)]. We now go a step further and
496"
REFERENCES,0.6137339055793991,"compute δ′ (a condition on the kernel) from δ∗∈[0, 1] (a condition on the rankings). The relation
497"
REFERENCES,0.6148068669527897,"between δ′ and δ∗changes with the kernel, and so does the interpretation of δ∗. For the three kernels
498"
REFERENCES,0.6158798283261803,"we discuss in Section 4.1:
499"
REFERENCES,0.6169527896995708,"• Mallows kernel with ν = 1/(
n
2): δ∗is the fraction of discordant pairs, δ′ = e−δ∗.
500"
REFERENCES,0.6180257510729614,"• Jaccard kernel: δ∗is the intersection over union of the top k tiers, δ′ = 1 −δ∗.
501"
REFERENCES,0.619098712446352,"• Borda kernel with ν = 1/na: δ∗is the difference in relative position of a∗in the rankings,
502"
REFERENCES,0.6201716738197425,"normalized to the length of the rankings, δ′ = e−δ∗
503"
REFERENCES,0.621244635193133,"A.3.2
Proof of proposition 4.2
504"
REFERENCES,0.6223175965665236,"Proposition 4.2. ∀α∗, there exist β0 ≥0, β1 ≤0 s.t.
505"
REFERENCES,0.6233905579399142,"log(n) ≈β1 log

εα∗
n

+ β0
(3)"
REFERENCES,0.6244635193133047,"Proof. We provide a proof replacing MMD with the distribution-free bound defined in [28].
506"
REFERENCES,0.6255364806866953,"Pn ⊗Pn

(Xj, Yj)n
j=1 : MMD(X, Y ) −
2κsup n"
REFERENCES,0.6266094420600858,"
> ε

< exp

−nε2 4κsup "
REFERENCES,0.6276824034334764,"(1)
==⇒Pn ⊗Pn 
(Xj, Yj)n
j=1 : MMD(X, Y ) > ε‘

< exp "
REFERENCES,0.628755364806867,"
−
n

ε′ −

2κsup n
2 4κsup  
"
REFERENCES,0.6298283261802575,"(2)
==⇒Pn ⊗Pn

(Xj, Yj)n
j=1 : MMD(X, Y ) > n−1 2
q"
REFERENCES,0.630901287553648,"−log (1 −α) 4κsup 
+
p 2κsup"
REFERENCES,0.6319742489270386,"
< 1 −α"
REFERENCES,0.6330472103004292,"(3)
==⇒Pn ⊗Pn

(Xj, Yj)n
j=1 : MMD(X, Y ) ≤n−1 2
q"
REFERENCES,0.6341201716738197,"−log (1 −α) 4κsup 
+
p 2κsup 
≥α"
REFERENCES,0.6351931330472103,"where:
507"
REFERENCES,0.6362660944206009,"(1) ε′ = ε +
p"
REFERENCES,0.6373390557939914,"2κsup/n.
508"
REFERENCES,0.6384120171673819,(2) 1 −α = exp 
REFERENCES,0.6394849785407726,"−
n

ε′−
 2κsup n
2 4κsup !"
REFERENCES,0.6405579399141631,and ε‘ = n−1 2  p
REFERENCES,0.6416309012875536,"−log (1 −α) 4κsup + p2κsup

.
509"
REFERENCES,0.6427038626609443,"(3) Take the complementary event.
510"
REFERENCES,0.6437768240343348,"Now,
511"
REFERENCES,0.6448497854077253,"qα
n = n−1 2
q"
REFERENCES,0.6459227467811158,"−log (1 −α) 4κsup 
+
p 2κsup"
REFERENCES,0.6469957081545065,"⇒n = (qα
n)−2
q"
REFERENCES,0.648068669527897,"−4κsup log (1 −α) +
p 2κsup 2"
REFERENCES,0.6491416309012875,"⇒log(n) = −2 log(qα
n) + 2 log
q"
REFERENCES,0.6502145922746781,"−4κsup log (1 −α) +
p 2κsup 
."
REFERENCES,0.6512875536480687,"concluding the proof.
512"
REFERENCES,0.6523605150214592,"Remark. Altohugh theoretically sound, using the abovementioned bound instead of MMD leads to
513"
REFERENCES,0.6534334763948498,"excessively conservative estimates for n∗, roughly one order of magnitude greater than the empirical
514"
REFERENCES,0.6545064377682404,"estimate.
515"
REFERENCES,0.6555793991416309,"A.3.3
Pseudocode for the algorithm
516"
REFERENCES,0.6566523605150214,"Algorithm 1 Compute n∗
N from preliminary study"
REFERENCES,0.657725321888412,"Require: α∗
▷desired generalizability
Require: δ∗
▷similarity threshold on rankings
Require: Q
▷research question, Q = (A, C, Iatv, κ)
Require: N
▷size of preliminary study
Require: nmax
▷maximum sample size to compute MMD
Require: nrep
▷number of repetitions to compute MMD"
REFERENCES,0.6587982832618026,"procedure ESTIMATENSTAR(α∗, δ∗, Q, N, nmax, nrep)"
REFERENCES,0.6598712446351931,"ε∗←compute ε∗from δ∗
▷cf. Appendix A.3
sample {cj}N
j=1
iid
∼C"
REFERENCES,0.6609442060085837,"nmax ←min {nmax, [N/2]}
▷we need two disjoint samples of size nmax from {cj}N
j=1
for n = 1 . . . nmax do"
REFERENCES,0.6620171673819742,"mmds ←empty list
for n = 1 . . . nrep do"
REFERENCES,0.6630901287553648,"sample without replacement (cj)2nmax
j=1 ∼{cj}N
j=1
x ←(cj)nmax
j=1
▷split the disjoint samples"
REFERENCES,0.6641630901287554,"y ←(cj)2nmax
j=nmax
append MMD (x, y) to mmds
end for
εα∗
n ←α∗-quantile of mmds
end for
fit a linear regression log(n) = β1 log
 
εα∗
n

+ β0
n∗
N ←β1 log(ε∗) + β0
return n∗
N
end procedure"
REFERENCES,0.6652360515021459,"procedure RUNEXPERIMENTS(α∗, δ∗, Q, nmax, nrep, step)"
REFERENCES,0.6663090128755365,"N ←step
while n∗> N do"
REFERENCES,0.6673819742489271,"sample {cj}N
j=1
iid
∼C
n∗←ESTIMATENSTAR(α∗, δ∗, Q, N, nmax, nrep)
N ←N + step
end while
end procedure"
REFERENCES,0.6684549356223176,"NeurIPS Paper Checklist
517"
CLAIMS,0.6695278969957081,"1. Claims
518"
CLAIMS,0.6706008583690987,"Question: Do the main claims made in the abstract and introduction accurately reflect the
519"
CLAIMS,0.6716738197424893,"paper’s contributions and scope?
520"
CLAIMS,0.6727467811158798,"Answer: [Yes]
521"
CLAIMS,0.6738197424892703,"Justification: In order, our claims are: the formalization in Section 3; the definition gen-
522"
CLAIMS,0.674892703862661,"eralizability in Section 4; the algorithm for study size in Section 4.3, the case studies in
523"
CLAIMS,0.6759656652360515,"Section 5, and we provide a link to the anonymized GitHub repository for the module.
524"
CLAIMS,0.677038626609442,"Guidelines:
525"
CLAIMS,0.6781115879828327,"• The answer NA means that the abstract and introduction do not include the claims
526"
CLAIMS,0.6791845493562232,"made in the paper.
527"
CLAIMS,0.6802575107296137,"• The abstract and/or introduction should clearly state the claims made, including the
528"
CLAIMS,0.6813304721030042,"contributions made in the paper and important assumptions and limitations. A No or
529"
CLAIMS,0.6824034334763949,"NA answer to this question will not be perceived well by the reviewers.
530"
CLAIMS,0.6834763948497854,"• The claims made should match theoretical and experimental results, and reflect how
531"
CLAIMS,0.6845493562231759,"much the results can be expected to generalize to other settings.
532"
CLAIMS,0.6856223175965666,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
533"
CLAIMS,0.6866952789699571,"are not attained by the paper.
534"
LIMITATIONS,0.6877682403433476,"2. Limitations
535"
LIMITATIONS,0.6888412017167382,"Question: Does the paper discuss the limitations of the work performed by the authors?
536"
LIMITATIONS,0.6899141630901288,"Answer: [Yes]
537"
LIMITATIONS,0.6909871244635193,"Justification: In Section 6.
538"
LIMITATIONS,0.6920600858369099,"Guidelines:
539"
LIMITATIONS,0.6931330472103004,"• The answer NA means that the paper has no limitation while the answer No means that
540"
LIMITATIONS,0.694206008583691,"the paper has limitations, but those are not discussed in the paper.
541"
LIMITATIONS,0.6952789699570815,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
542"
LIMITATIONS,0.6963519313304721,"• The paper should point out any strong assumptions and how robust the results are to
543"
LIMITATIONS,0.6974248927038627,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
544"
LIMITATIONS,0.6984978540772532,"model well-specification, asymptotic approximations only holding locally). The authors
545"
LIMITATIONS,0.6995708154506438,"should reflect on how these assumptions might be violated in practice and what the
546"
LIMITATIONS,0.7006437768240343,"implications would be.
547"
LIMITATIONS,0.7017167381974249,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
548"
LIMITATIONS,0.7027896995708155,"only tested on a few datasets or with a few runs. In general, empirical results often
549"
LIMITATIONS,0.703862660944206,"depend on implicit assumptions, which should be articulated.
550"
LIMITATIONS,0.7049356223175965,"• The authors should reflect on the factors that influence the performance of the approach.
551"
LIMITATIONS,0.7060085836909872,"For example, a facial recognition algorithm may perform poorly when image resolution
552"
LIMITATIONS,0.7070815450643777,"is low or images are taken in low lighting. Or a speech-to-text system might not be
553"
LIMITATIONS,0.7081545064377682,"used reliably to provide closed captions for online lectures because it fails to handle
554"
LIMITATIONS,0.7092274678111588,"technical jargon.
555"
LIMITATIONS,0.7103004291845494,"• The authors should discuss the computational efficiency of the proposed algorithms
556"
LIMITATIONS,0.7113733905579399,"and how they scale with dataset size.
557"
LIMITATIONS,0.7124463519313304,"• If applicable, the authors should discuss possible limitations of their approach to
558"
LIMITATIONS,0.7135193133047211,"address problems of privacy and fairness.
559"
LIMITATIONS,0.7145922746781116,"• While the authors might fear that complete honesty about limitations might be used by
560"
LIMITATIONS,0.7156652360515021,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
561"
LIMITATIONS,0.7167381974248928,"limitations that aren’t acknowledged in the paper. The authors should use their best
562"
LIMITATIONS,0.7178111587982833,"judgment and recognize that individual actions in favor of transparency play an impor-
563"
LIMITATIONS,0.7188841201716738,"tant role in developing norms that preserve the integrity of the community. Reviewers
564"
LIMITATIONS,0.7199570815450643,"will be specifically instructed to not penalize honesty concerning limitations.
565"
THEORY ASSUMPTIONS AND PROOFS,0.721030042918455,"3. Theory Assumptions and Proofs
566"
THEORY ASSUMPTIONS AND PROOFS,0.7221030042918455,"Question: For each theoretical result, does the paper provide the full set of assumptions and
567"
THEORY ASSUMPTIONS AND PROOFS,0.723175965665236,"a complete (and correct) proof?
568"
THEORY ASSUMPTIONS AND PROOFS,0.7242489270386266,"Answer: [Yes]
569"
THEORY ASSUMPTIONS AND PROOFS,0.7253218884120172,"Justification: The proofs are in the Appendix.
570"
THEORY ASSUMPTIONS AND PROOFS,0.7263948497854077,"Guidelines:
571"
THEORY ASSUMPTIONS AND PROOFS,0.7274678111587983,"• The answer NA means that the paper does not include theoretical results.
572"
THEORY ASSUMPTIONS AND PROOFS,0.7285407725321889,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
573"
THEORY ASSUMPTIONS AND PROOFS,0.7296137339055794,"referenced.
574"
THEORY ASSUMPTIONS AND PROOFS,0.73068669527897,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
575"
THEORY ASSUMPTIONS AND PROOFS,0.7317596566523605,"• The proofs can either appear in the main paper or the supplemental material, but if
576"
THEORY ASSUMPTIONS AND PROOFS,0.7328326180257511,"they appear in the supplemental material, the authors are encouraged to provide a short
577"
THEORY ASSUMPTIONS AND PROOFS,0.7339055793991416,"proof sketch to provide intuition.
578"
THEORY ASSUMPTIONS AND PROOFS,0.7349785407725322,"• Inversely, any informal proof provided in the core of the paper should be complemented
579"
THEORY ASSUMPTIONS AND PROOFS,0.7360515021459227,"by formal proofs provided in appendix or supplemental material.
580"
THEORY ASSUMPTIONS AND PROOFS,0.7371244635193133,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7381974248927039,"4. Experimental Result Reproducibility
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7392703862660944,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.740343347639485,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7414163090128756,"of the paper (regardless of whether the code and data are provided or not)?
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7424892703862661,"Answer: [Yes]
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7435622317596566,"Justification: On GitHub.
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7446351931330472,"Guidelines:
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7457081545064378,"• The answer NA means that the paper does not include experiments.
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7467811158798283,"• If the paper includes experiments, a No answer to this question will not be perceived
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7478540772532188,"well by the reviewers: Making the paper reproducible is important, regardless of
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7489270386266095,"whether the code and data are provided or not.
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7510729613733905,"to make their results reproducible or verifiable.
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7521459227467812,"• Depending on the contribution, reproducibility can be accomplished in various ways.
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7532188841201717,"For example, if the contribution is a novel architecture, describing the architecture fully
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7542918454935622,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7553648068669528,"be necessary to either make it possible for others to replicate the model with the same
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7564377682403434,"dataset, or provide access to the model. In general. releasing code and data is often
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7575107296137339,"one good way to accomplish this, but reproducibility can also be provided via detailed
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7585836909871244,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.759656652360515,"of a large language model), releasing of a model checkpoint, or other means that are
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607296137339056,"appropriate to the research performed.
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7618025751072961,"• While NeurIPS does not require releasing code, the conference does require all submis-
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7628755364806867,"sions to provide some reasonable avenue for reproducibility, which may depend on the
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7639484978540773,"nature of the contribution. For example
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7650214592274678,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7660944206008584,"to reproduce that algorithm.
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7671673819742489,"(b) If the contribution is primarily a new model architecture, the paper should describe
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7682403433476395,"the architecture clearly and fully.
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.76931330472103,"(c) If the contribution is a new model (e.g., a large language model), then there should
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7703862660944206,"either be a way to access this model for reproducing the results or a way to reproduce
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714592274678111,"the model (e.g., with an open-source dataset or instructions for how to construct
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7725321888412017,"the dataset).
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7736051502145923,"(d) We recognize that reproducibility may be tricky in some cases, in which case
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746781115879828,"authors are welcome to describe the particular way they provide for reproducibility.
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7757510729613734,"In the case of closed-source models, it may be that access to the model is limited in
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.776824034334764,"some way (e.g., to registered users), but it should be possible for other researchers
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7778969957081545,"to have some path to reproducing or verifying the results.
619"
OPEN ACCESS TO DATA AND CODE,0.778969957081545,"5. Open access to data and code
620"
OPEN ACCESS TO DATA AND CODE,0.7800429184549357,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
621"
OPEN ACCESS TO DATA AND CODE,0.7811158798283262,"tions to faithfully reproduce the main experimental results, as described in supplemental
622"
OPEN ACCESS TO DATA AND CODE,0.7821888412017167,"material?
623"
OPEN ACCESS TO DATA AND CODE,0.7832618025751072,"Answer: [Yes]
624"
OPEN ACCESS TO DATA AND CODE,0.7843347639484979,"Justification: On GitHub.
625"
OPEN ACCESS TO DATA AND CODE,0.7854077253218884,"Guidelines:
626"
OPEN ACCESS TO DATA AND CODE,0.7864806866952789,"• The answer NA means that paper does not include experiments requiring code.
627"
OPEN ACCESS TO DATA AND CODE,0.7875536480686696,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
628"
OPEN ACCESS TO DATA AND CODE,0.7886266094420601,"public/guides/CodeSubmissionPolicy) for more details.
629"
OPEN ACCESS TO DATA AND CODE,0.7896995708154506,"• While we encourage the release of code and data, we understand that this might not be
630"
OPEN ACCESS TO DATA AND CODE,0.7907725321888412,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
631"
OPEN ACCESS TO DATA AND CODE,0.7918454935622318,"including code, unless this is central to the contribution (e.g., for a new open-source
632"
OPEN ACCESS TO DATA AND CODE,0.7929184549356223,"benchmark).
633"
OPEN ACCESS TO DATA AND CODE,0.7939914163090128,"• The instructions should contain the exact command and environment needed to run to
634"
OPEN ACCESS TO DATA AND CODE,0.7950643776824035,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
635"
OPEN ACCESS TO DATA AND CODE,0.796137339055794,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
636"
OPEN ACCESS TO DATA AND CODE,0.7972103004291845,"• The authors should provide instructions on data access and preparation, including how
637"
OPEN ACCESS TO DATA AND CODE,0.7982832618025751,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
638"
OPEN ACCESS TO DATA AND CODE,0.7993562231759657,"• The authors should provide scripts to reproduce all experimental results for the new
639"
OPEN ACCESS TO DATA AND CODE,0.8004291845493562,"proposed method and baselines. If only a subset of experiments are reproducible, they
640"
OPEN ACCESS TO DATA AND CODE,0.8015021459227468,"should state which ones are omitted from the script and why.
641"
OPEN ACCESS TO DATA AND CODE,0.8025751072961373,"• At submission time, to preserve anonymity, the authors should release anonymized
642"
OPEN ACCESS TO DATA AND CODE,0.8036480686695279,"versions (if applicable).
643"
OPEN ACCESS TO DATA AND CODE,0.8047210300429185,"• Providing as much information as possible in supplemental material (appended to the
644"
OPEN ACCESS TO DATA AND CODE,0.805793991416309,"paper) is recommended, but including URLs to data and code is permitted.
645"
OPEN ACCESS TO DATA AND CODE,0.8068669527896996,"6. Experimental Setting/Details
646"
OPEN ACCESS TO DATA AND CODE,0.8079399141630901,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
647"
OPEN ACCESS TO DATA AND CODE,0.8090128755364807,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
648"
OPEN ACCESS TO DATA AND CODE,0.8100858369098712,"results?
649"
OPEN ACCESS TO DATA AND CODE,0.8111587982832618,"Answer: [Yes]
650"
OPEN ACCESS TO DATA AND CODE,0.8122317596566524,"Justification: In Section 5.
651"
OPEN ACCESS TO DATA AND CODE,0.8133047210300429,"Guidelines:
652"
OPEN ACCESS TO DATA AND CODE,0.8143776824034334,"• The answer NA means that the paper does not include experiments.
653"
OPEN ACCESS TO DATA AND CODE,0.8154506437768241,"• The experimental setting should be presented in the core of the paper to a level of detail
654"
OPEN ACCESS TO DATA AND CODE,0.8165236051502146,"that is necessary to appreciate the results and make sense of them.
655"
OPEN ACCESS TO DATA AND CODE,0.8175965665236051,"• The full details can be provided either with the code, in appendix, or as supplemental
656"
OPEN ACCESS TO DATA AND CODE,0.8186695278969958,"material.
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8197424892703863,"7. Experiment Statistical Significance
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8208154506437768,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8218884120171673,"information about the statistical significance of the experiments?
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.822961373390558,"Answer: [Yes]
661"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8240343347639485,"Justification: The boxplots in Section 5 show the variability for the choice of fixed factors.
662"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.825107296137339,"Guidelines:
663"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8261802575107297,"• The answer NA means that the paper does not include experiments.
664"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8272532188841202,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
665"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8283261802575107,"dence intervals, or statistical significance tests, at least for the experiments that support
666"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8293991416309013,"the main claims of the paper.
667"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8304721030042919,"• The factors of variability that the error bars are capturing should be clearly stated (for
668"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8315450643776824,"example, train/test split, initialization, random drawing of some parameter, or overall
669"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8326180257510729,"run with given experimental conditions).
670"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8336909871244635,"• The method for calculating the error bars should be explained (closed form formula,
671"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8347639484978541,"call to a library function, bootstrap, etc.)
672"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8358369098712446,"• The assumptions made should be given (e.g., Normally distributed errors).
673"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8369098712446352,"• It should be clear whether the error bar is the standard deviation or the standard error
674"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8379828326180258,"of the mean.
675"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8390557939914163,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
676"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8401287553648069,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
677"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8412017167381974,"of Normality of errors is not verified.
678"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.842274678111588,"• For asymmetric distributions, the authors should be careful not to show in tables or
679"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8433476394849786,"figures symmetric error bars that would yield results that are out of range (e.g. negative
680"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8444206008583691,"error rates).
681"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8454935622317596,"• If error bars are reported in tables or plots, The authors should explain in the text how
682"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8465665236051502,"they were calculated and reference the corresponding figures or tables in the text.
683"
EXPERIMENTS COMPUTE RESOURCES,0.8476394849785408,"8. Experiments Compute Resources
684"
EXPERIMENTS COMPUTE RESOURCES,0.8487124463519313,"Question: For each experiment, does the paper provide sufficient information on the com-
685"
EXPERIMENTS COMPUTE RESOURCES,0.8497854077253219,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
686"
EXPERIMENTS COMPUTE RESOURCES,0.8508583690987125,"the experiments?
687"
EXPERIMENTS COMPUTE RESOURCES,0.851931330472103,"Answer: [No]
688"
EXPERIMENTS COMPUTE RESOURCES,0.8530042918454935,"Justification: The analysis we showcase in Section 5 executes very fast, requiring in total
689"
EXPERIMENTS COMPUTE RESOURCES,0.8540772532188842,"less than 4 hours on a standard office laptop.
690"
EXPERIMENTS COMPUTE RESOURCES,0.8551502145922747,"Guidelines:
691"
EXPERIMENTS COMPUTE RESOURCES,0.8562231759656652,"• The answer NA means that the paper does not include experiments.
692"
EXPERIMENTS COMPUTE RESOURCES,0.8572961373390557,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
693"
EXPERIMENTS COMPUTE RESOURCES,0.8583690987124464,"or cloud provider, including relevant memory and storage.
694"
EXPERIMENTS COMPUTE RESOURCES,0.8594420600858369,"• The paper should provide the amount of compute required for each of the individual
695"
EXPERIMENTS COMPUTE RESOURCES,0.8605150214592274,"experimental runs as well as estimate the total compute.
696"
EXPERIMENTS COMPUTE RESOURCES,0.8615879828326181,"• The paper should disclose whether the full research project required more compute
697"
EXPERIMENTS COMPUTE RESOURCES,0.8626609442060086,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
698"
EXPERIMENTS COMPUTE RESOURCES,0.8637339055793991,"didn’t make it into the paper).
699"
CODE OF ETHICS,0.8648068669527897,"9. Code Of Ethics
700"
CODE OF ETHICS,0.8658798283261803,"Question: Does the research conducted in the paper conform, in every respect, with the
701"
CODE OF ETHICS,0.8669527896995708,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
702"
CODE OF ETHICS,0.8680257510729614,"Answer: [Yes]
703"
CODE OF ETHICS,0.869098712446352,"Justification:
704"
CODE OF ETHICS,0.8701716738197425,"Guidelines:
705"
CODE OF ETHICS,0.871244635193133,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
706"
CODE OF ETHICS,0.8723175965665236,"• If the authors answer No, they should explain the special circumstances that require a
707"
CODE OF ETHICS,0.8733905579399142,"deviation from the Code of Ethics.
708"
CODE OF ETHICS,0.8744635193133047,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
709"
CODE OF ETHICS,0.8755364806866953,"eration due to laws or regulations in their jurisdiction).
710"
BROADER IMPACTS,0.8766094420600858,"10. Broader Impacts
711"
BROADER IMPACTS,0.8776824034334764,"Question: Does the paper discuss both potential positive societal impacts and negative
712"
BROADER IMPACTS,0.878755364806867,"societal impacts of the work performed?
713"
BROADER IMPACTS,0.8798283261802575,"Answer: [NA]
714"
BROADER IMPACTS,0.880901287553648,"Justification:
715"
BROADER IMPACTS,0.8819742489270386,"Guidelines:
716"
BROADER IMPACTS,0.8830472103004292,"• The answer NA means that there is no societal impact of the work performed.
717"
BROADER IMPACTS,0.8841201716738197,"• If the authors answer NA or No, they should explain why their work has no societal
718"
BROADER IMPACTS,0.8851931330472103,"impact or why the paper does not address societal impact.
719"
BROADER IMPACTS,0.8862660944206009,"• Examples of negative societal impacts include potential malicious or unintended uses
720"
BROADER IMPACTS,0.8873390557939914,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
721"
BROADER IMPACTS,0.8884120171673819,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
722"
BROADER IMPACTS,0.8894849785407726,"groups), privacy considerations, and security considerations.
723"
BROADER IMPACTS,0.8905579399141631,"• The conference expects that many papers will be foundational research and not tied
724"
BROADER IMPACTS,0.8916309012875536,"to particular applications, let alone deployments. However, if there is a direct path to
725"
BROADER IMPACTS,0.8927038626609443,"any negative applications, the authors should point it out. For example, it is legitimate
726"
BROADER IMPACTS,0.8937768240343348,"to point out that an improvement in the quality of generative models could be used to
727"
BROADER IMPACTS,0.8948497854077253,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
728"
BROADER IMPACTS,0.8959227467811158,"that a generic algorithm for optimizing neural networks could enable people to train
729"
BROADER IMPACTS,0.8969957081545065,"models that generate Deepfakes faster.
730"
BROADER IMPACTS,0.898068669527897,"• The authors should consider possible harms that could arise when the technology is
731"
BROADER IMPACTS,0.8991416309012875,"being used as intended and functioning correctly, harms that could arise when the
732"
BROADER IMPACTS,0.9002145922746781,"technology is being used as intended but gives incorrect results, and harms following
733"
BROADER IMPACTS,0.9012875536480687,"from (intentional or unintentional) misuse of the technology.
734"
BROADER IMPACTS,0.9023605150214592,"• If there are negative societal impacts, the authors could also discuss possible mitigation
735"
BROADER IMPACTS,0.9034334763948498,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
736"
BROADER IMPACTS,0.9045064377682404,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
737"
BROADER IMPACTS,0.9055793991416309,"feedback over time, improving the efficiency and accessibility of ML).
738"
SAFEGUARDS,0.9066523605150214,"11. Safeguards
739"
SAFEGUARDS,0.907725321888412,"Question: Does the paper describe safeguards that have been put in place for responsible
740"
SAFEGUARDS,0.9087982832618026,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
741"
SAFEGUARDS,0.9098712446351931,"image generators, or scraped datasets)?
742"
SAFEGUARDS,0.9109442060085837,"Answer: [NA]
743"
SAFEGUARDS,0.9120171673819742,"Justification:
744"
SAFEGUARDS,0.9130901287553648,"Guidelines:
745"
SAFEGUARDS,0.9141630901287554,"• The answer NA means that the paper poses no such risks.
746"
SAFEGUARDS,0.9152360515021459,"• Released models that have a high risk for misuse or dual-use should be released with
747"
SAFEGUARDS,0.9163090128755365,"necessary safeguards to allow for controlled use of the model, for example by requiring
748"
SAFEGUARDS,0.9173819742489271,"that users adhere to usage guidelines or restrictions to access the model or implementing
749"
SAFEGUARDS,0.9184549356223176,"safety filters.
750"
SAFEGUARDS,0.9195278969957081,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
751"
SAFEGUARDS,0.9206008583690987,"should describe how they avoided releasing unsafe images.
752"
SAFEGUARDS,0.9216738197424893,"• We recognize that providing effective safeguards is challenging, and many papers do
753"
SAFEGUARDS,0.9227467811158798,"not require this, but we encourage authors to take this into account and make a best
754"
SAFEGUARDS,0.9238197424892703,"faith effort.
755"
LICENSES FOR EXISTING ASSETS,0.924892703862661,"12. Licenses for existing assets
756"
LICENSES FOR EXISTING ASSETS,0.9259656652360515,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
757"
LICENSES FOR EXISTING ASSETS,0.927038626609442,"the paper, properly credited and are the license and terms of use explicitly mentioned and
758"
LICENSES FOR EXISTING ASSETS,0.9281115879828327,"properly respected?
759"
LICENSES FOR EXISTING ASSETS,0.9291845493562232,"Answer: [Yes]
760"
LICENSES FOR EXISTING ASSETS,0.9302575107296137,"Justification: To the best of our knowledge, we referenced all sources in the appropriate way.
761"
LICENSES FOR EXISTING ASSETS,0.9313304721030042,"Guidelines:
762"
LICENSES FOR EXISTING ASSETS,0.9324034334763949,"• The answer NA means that the paper does not use existing assets.
763"
LICENSES FOR EXISTING ASSETS,0.9334763948497854,"• The authors should cite the original paper that produced the code package or dataset.
764"
LICENSES FOR EXISTING ASSETS,0.9345493562231759,"• The authors should state which version of the asset is used and, if possible, include a
765"
LICENSES FOR EXISTING ASSETS,0.9356223175965666,"URL.
766"
LICENSES FOR EXISTING ASSETS,0.9366952789699571,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
767"
LICENSES FOR EXISTING ASSETS,0.9377682403433476,"• For scraped data from a particular source (e.g., website), the copyright and terms of
768"
LICENSES FOR EXISTING ASSETS,0.9388412017167382,"service of that source should be provided.
769"
LICENSES FOR EXISTING ASSETS,0.9399141630901288,"• If assets are released, the license, copyright information, and terms of use in the
770"
LICENSES FOR EXISTING ASSETS,0.9409871244635193,"package should be provided. For popular datasets, paperswithcode.com/datasets
771"
LICENSES FOR EXISTING ASSETS,0.9420600858369099,"has curated licenses for some datasets. Their licensing guide can help determine the
772"
LICENSES FOR EXISTING ASSETS,0.9431330472103004,"license of a dataset.
773"
LICENSES FOR EXISTING ASSETS,0.944206008583691,"• For existing datasets that are re-packaged, both the original license and the license of
774"
LICENSES FOR EXISTING ASSETS,0.9452789699570815,"the derived asset (if it has changed) should be provided.
775"
LICENSES FOR EXISTING ASSETS,0.9463519313304721,"• If this information is not available online, the authors are encouraged to reach out to
776"
LICENSES FOR EXISTING ASSETS,0.9474248927038627,"the asset’s creators.
777"
NEW ASSETS,0.9484978540772532,"13. New Assets
778"
NEW ASSETS,0.9495708154506438,"Question: Are new assets introduced in the paper well documented and is the documentation
779"
NEW ASSETS,0.9506437768240343,"provided alongside the assets?
780"
NEW ASSETS,0.9517167381974249,"Answer: [Yes]
781"
NEW ASSETS,0.9527896995708155,"Justification: Our Python module is documented on GitHub.
782"
NEW ASSETS,0.953862660944206,"Guidelines:
783"
NEW ASSETS,0.9549356223175965,"• The answer NA means that the paper does not release new assets.
784"
NEW ASSETS,0.9560085836909872,"• Researchers should communicate the details of the dataset/code/model as part of their
785"
NEW ASSETS,0.9570815450643777,"submissions via structured templates. This includes details about training, license,
786"
NEW ASSETS,0.9581545064377682,"limitations, etc.
787"
NEW ASSETS,0.9592274678111588,"• The paper should discuss whether and how consent was obtained from people whose
788"
NEW ASSETS,0.9603004291845494,"asset is used.
789"
NEW ASSETS,0.9613733905579399,"• At submission time, remember to anonymize your assets (if applicable). You can either
790"
NEW ASSETS,0.9624463519313304,"create an anonymized URL or include an anonymized zip file.
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9635193133047211,"14. Crowdsourcing and Research with Human Subjects
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9645922746781116,"Question: For crowdsourcing experiments and research with human subjects, does the paper
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9656652360515021,"include the full text of instructions given to participants and screenshots, if applicable, as
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9667381974248928,"well as details about compensation (if any)?
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9678111587982833,"Answer: [NA]
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9688841201716738,"Justification:
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9699570815450643,"Guidelines:
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971030042918455,"• The answer NA means that the paper does not involve crowdsourcing nor research with
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9721030042918455,"human subjects.
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973175965665236,"• Including this information in the supplemental material is fine, but if the main contribu-
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742489270386266,"tion of the paper involves human subjects, then as much detail as possible should be
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9753218884120172,"included in the main paper.
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763948497854077,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774678111587983,"or other labor should be paid at least the minimum wage in the country of the data
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785407725321889,"collector.
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9796137339055794,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98068669527897,"Subjects
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817596566523605,"Question: Does the paper describe potential risks incurred by study participants, whether
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828326180257511,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9839055793991416,"approvals (or an equivalent approval/review based on the requirements of your country or
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9849785407725322,"institution) were obtained?
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860515021459227,"Answer: [NA]
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871244635193133,"Justification:
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881974248927039,"Guidelines:
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892703862660944,"• The answer NA means that the paper does not involve crowdsourcing nor research with
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990343347639485,"human subjects.
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914163090128756,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924892703862661,"may be required for any human subjects research. If you obtained IRB approval, you
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935622317596566,"should clearly state this in the paper.
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946351931330472,"• We recognize that the procedures for this may vary significantly between institutions
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9957081545064378,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967811158798283,"guidelines for their institution.
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978540772532188,"• For initial submissions, do not include any information that would break anonymity (if
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989270386266095,"applicable), such as the institution conducting the review.
825"
