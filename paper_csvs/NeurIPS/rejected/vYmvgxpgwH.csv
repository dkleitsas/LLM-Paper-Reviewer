Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010822510822510823,"The optimal training configurations of large language models (LLMs) with respect
1"
ABSTRACT,0.0021645021645021645,"to model sizes and compute budgets have been extensively studied. But how to
2"
ABSTRACT,0.003246753246753247,"optimally configure LLMs during inference has not been explored in sufficient
3"
ABSTRACT,0.004329004329004329,"depth. We study compute-optimal inference: designing models and inference
4"
ABSTRACT,0.005411255411255411,"strategies that optimally trade off additional inference-time compute for improved
5"
ABSTRACT,0.006493506493506494,"performance. As a first step towards understanding and designing compute-optimal
6"
ABSTRACT,0.007575757575757576,"inference methods, we assessed the effectiveness and computational efficiency
7"
ABSTRACT,0.008658008658008658,"of multiple inference strategies such as Greedy Search, Majority Voting, Best-of-
8"
ABSTRACT,0.00974025974025974,"N, Weighted Voting, and their variants on two different Tree Search algorithms,
9"
ABSTRACT,0.010822510822510822,"involving different model sizes (e.g., 7B and 34B) and computational budgets. We
10"
ABSTRACT,0.011904761904761904,"found that a smaller language model with a novel tree search algorithm typically
11"
ABSTRACT,0.012987012987012988,"achieves a Pareto-optimal trade-off. These results highlight the potential benefits of
12"
ABSTRACT,0.01406926406926407,"deploying smaller models equipped with more sophisticated decoding algorithms
13"
ABSTRACT,0.015151515151515152,"in end-devices to enhance problem-solving accuracy. For instance, we show that
14"
ABSTRACT,0.016233766233766232,"the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model
15"
ABSTRACT,0.017316017316017316,"on MATH500 while using 2× less FLOPs. Our findings could potentially apply to
16"
ABSTRACT,0.0183982683982684,"any generation task with a well-defined measure of success.
17"
INTRODUCTION,0.01948051948051948,"1
Introduction
18"
INTRODUCTION,0.020562770562770564,"Scaling laws of neural networks [Hestness et al., 2017, Rosenfeld et al., 2019] have been established
19"
INTRODUCTION,0.021645021645021644,"across a range of domains, including language modeling [Kaplan et al., 2020, Hoffmann et al., 2022,
20"
INTRODUCTION,0.022727272727272728,"OpenAI, 2023], image modeling [Henighan et al., 2020, Yu et al., 2022, Peebles and Xie, 2023],
21"
INTRODUCTION,0.023809523809523808,"video modeling [Brooks et al., 2024], reward modeling [Gao et al., 2023], and board games [Jones,
22"
INTRODUCTION,0.024891774891774892,"2021]. These studies have demonstrated how model performance is influenced by both the size of the
23"
INTRODUCTION,0.025974025974025976,"model and the amount of training computation. However, there is limited knowledge on how varying
24"
INTRODUCTION,0.027056277056277056,"the compute during inference affects model performance after the model has been trained.
25"
INTRODUCTION,0.02813852813852814,"To improve the task performance of large language models (LLMs), inference techniques typically
26"
INTRODUCTION,0.02922077922077922,"involve additional computation in a performance maximization step at inference time [Nye et al.,
27"
INTRODUCTION,0.030303030303030304,"2021, Wei et al., 2022, Wang et al., 2022b, Yao et al., 2023, Chen et al., 2024b]. This cost must be
28"
INTRODUCTION,0.031385281385281384,"taken into account for compute-optimal inference. For example, a Monte Carlo Tree Search (MCTS)
29"
INTRODUCTION,0.032467532467532464,"method [Jones, 2021] may improve task performance, but potentially cost much more than simply
30"
INTRODUCTION,0.03354978354978355,"sampling solutions multiple times. Generally speaking, we need a comprehensive understanding of
31"
INTRODUCTION,0.03463203463203463,"how various inference-time methods (e.g., Best-of-N, majority voting) trade off between performance
32"
INTRODUCTION,0.03571428571428571,"and cost. To improve our understanding, this paper presents a thorough empirical evaluation with
33"
INTRODUCTION,0.0367965367965368,"careful analysis over various configurations of representative LLMs and inference algorithms.
34"
INTRODUCTION,0.03787878787878788,"Specifically, we explore how to select an optimal model size (e.g., 7B or 34B) for the policy model
35"
INTRODUCTION,0.03896103896103896,"and an effective inference strategy (e.g., Greedy Search, Majority Voting, Best-of-N, Weighted Voting,
36"
INTRODUCTION,0.04004329004329004,"4
16
64
256
1024
Inference FLOPs per question (×1012) 50 55 60 65 70 75"
INTRODUCTION,0.04112554112554113,Test error
INTRODUCTION,0.04220779220779221,Inference scaling on MATH
INTRODUCTION,0.04329004329004329,"Sampling (7B)
Sampling (34B)
MCTS (7B)
MCTS (34B)"
INTRODUCTION,0.044372294372294376,"4
16
64
256
1024
Inference FLOPs per question (×1012) 50 55 60 65 70 75"
INTRODUCTION,0.045454545454545456,Test error
INTRODUCTION,0.046536796536796536,Inference scaling on MATH
INTRODUCTION,0.047619047619047616,"REBASE (7B)
REBASE (34B)"
INTRODUCTION,0.048701298701298704,"Figure 1: The inference computation scaling laws exhibited in error rate on the MATH500 test set
based on weighted majority voting, where the left figure shows sampling vs. MCTS, and the right
figure shows our proposed REBASE. Clearly, the error rate decreases steadily when the computation
increases, and REBASE exhibits a Pareto-optimal tradeoff during inference."
INTRODUCTION,0.049783549783549784,"and their Tree Search variants) to maximize performance (i.e., accuracy) within a given compute
37"
INTRODUCTION,0.050865800865800864,"budget. We manipulate the inference computation (FLOPs) of a fixed model by generating additional
38"
INTRODUCTION,0.05194805194805195,"tokens through the policy model, sampling further candidate solutions, and ranking them with a
39"
INTRODUCTION,0.05303030303030303,"reward model. We analyze the performance of a family of math-specialized LLMs (i.e., Llemma-7B
40"
INTRODUCTION,0.05411255411255411,"and Llemma-34B [Azerbayev et al., 2023]) fine-tuned on the MetaMath dataset [Yu et al., 2023] and
41"
INTRODUCTION,0.05519480519480519,"measure the error rate on the GSM8K test set [Cobbe et al., 2021a] and MATH500 test set [Hendrycks
42"
INTRODUCTION,0.05627705627705628,"et al., 2021b, Lightman et al., 2023b].
43"
INTRODUCTION,0.05735930735930736,"Our analysis shows that voting-based methods generally outperform the strategy which selects the
44"
INTRODUCTION,0.05844155844155844,"best solution (i.e., Best-of-N), and weighted voting has the most favorable results (Section 4.3,
45"
INTRODUCTION,0.05952380952380952,"Figure 5 & 6). However, neither method shows a desirable behavior at high levels of compute. For
46"
INTRODUCTION,0.06060606060606061,"instance, weighted voting saturates when sampling more than 128 solutions (Figure 1). We have also
47"
INTRODUCTION,0.06168831168831169,"found that the commonly used MCTS method does not perform well with weighted voting, as it often
48"
INTRODUCTION,0.06277056277056277,"yields many unfinished solutions, hence having less votes. To address this issue, we propose a novel
49"
INTRODUCTION,0.06385281385281386,"tree search algorithm, REward BAlanced SEarch (REBASE), which pairs well with weighted voting
50"
INTRODUCTION,0.06493506493506493,"and improves the Pareto-optimal trade-off between accuracy and inference compute. The key idea of
51"
INTRODUCTION,0.06601731601731602,"REBASE is to use a node-quality based reward to control the exploitation and pruning properties of
52"
INTRODUCTION,0.0670995670995671,"tree search, while ensuring enough candidate solutions for voting or selection.
53"
INTRODUCTION,0.06818181818181818,"In our experiments, REBASE consistently outperforms sampling and MCTS methods across all
54"
INTRODUCTION,0.06926406926406926,"settings, models, and tasks. Importantly, we find that REBASE with a smaller language model
55"
INTRODUCTION,0.07034632034632035,"typically achieves a Pareto-optimal trade-off. For instance, we show that the Llemma-7B model can
56"
INTRODUCTION,0.07142857142857142,"achieve competitive accuracy to a Llemma-34B model while using 2× less FLOPs when evaluating
57"
INTRODUCTION,0.07251082251082251,"on MATH500 (Figure 1) or GSM8K (Figure 4). These findings underscore the advantages of using
58"
INTRODUCTION,0.0735930735930736,"smaller models with advanced inference-time algorithms on end-devices to improve problem-solving.
59"
RELATED WORKS,0.07467532467532467,"2
Related Works
60"
RELATED WORKS,0.07575757575757576,"Mathematical Reasoning with LLMs.
Large language models have made significant progress
61"
RELATED WORKS,0.07683982683982683,"in recent years, and have exhibited strong reasoning abilities [Brown et al., 2020, Hoffmann et al.,
62"
RELATED WORKS,0.07792207792207792,"2022, Chowdhery et al., 2022, Lewkowycz et al., 2022]. Mathematical problem solving is a key task
63"
RELATED WORKS,0.07900432900432901,"for measuring LLM reasoning abilities [Cobbe et al., 2021a, Hendrycks et al., 2021b]. [Ling et al.,
64"
RELATED WORKS,0.08008658008658008,"2017] first developed the method of producing step by step solutions that lead to the final answer.
65"
RELATED WORKS,0.08116883116883117,"Later, [Cobbe et al., 2021b] extended the work by finetuning the pre-trained language model on a
66"
RELATED WORKS,0.08225108225108226,"large dataset to solve math word problems, a verifier is trained for evaluating solutions and ranking
67"
RELATED WORKS,0.08333333333333333,"solutions. Nye et al. [2021] train models to use a scratchpad and improve their performance on
68"
RELATED WORKS,0.08441558441558442,"algorithmic tasks. Wei et al. [2022] demonstrate that the reasoning ability of a language model can
69"
RELATED WORKS,0.0854978354978355,"be elicited through the prompting. Subsequent research [Kojima et al., 2022, Lewkowycz et al., 2022,
70"
RELATED WORKS,0.08658008658008658,"Zhou et al., 2022] in reasoning tasks has also highlighted the efficacy of rationale augmentation. We
71"
RELATED WORKS,0.08766233766233766,"choose problem solving in mathematics as the task to study the compute-optimal strategy since it
72"
RELATED WORKS,0.08874458874458875,"allows us to accurately evaluate the problem solving ability of LLMs.
73"
RELATED WORKS,0.08982683982683982,"Figure 2: Illustration of compute-optimal scaling laws in training and inference. The Chinchilla
scaling law shows how to choose a model size and number of training tokens under a training-
compute budget, while ours shows how to choose a model size and an inference strategy under a
inference-compute budget."
RELATED WORKS,0.09090909090909091,"Inference Strategies of LLM Problem Solving.
A variety of inference (also called decoding)
74"
RELATED WORKS,0.09199134199134198,"strategies have been developed to generate sequences with a trained model. Deterministic methods
75"
RELATED WORKS,0.09307359307359307,"such as greedy decoding and beam search [Teller, 2000, Graves, 2012] find highly probable sequences,
76"
RELATED WORKS,0.09415584415584416,"often yielding high quality results but without diversity. Sampling algorithms (e.g., temperature
77"
RELATED WORKS,0.09523809523809523,"sampling [Ackley et al., 1985]) can produce a diverse set of results which are then aggregated to
78"
RELATED WORKS,0.09632034632034632,"achieve higher accuracy (e.g., via majority voting [Wang et al., 2022a]). Recent methods combine
79"
RELATED WORKS,0.09740259740259741,"search algorithms with modern LLMs, including breadth-first or depth-first search [Yao et al., 2023],
80"
RELATED WORKS,0.09848484848484848,"Monte-Carlo Tree Search (MCTS) [Zhang et al., 2023, Zhou et al., 2023, Liu et al., 2024, Choi et al.,
81"
RELATED WORKS,0.09956709956709957,"2023], and Self-evaluation Guided Beam Search [Xie et al., 2023]. All of these methods show that
82"
RELATED WORKS,0.10064935064935066,"using search at inference time can lead to performance gains in various tasks. However, the trade-off
83"
RELATED WORKS,0.10173160173160173,"for the improved performance is the use of compute to perform the search. Analyzing the trade-off
84"
RELATED WORKS,0.10281385281385282,"between compute budget and LLM inference performance remains understudied. In this paper, we
85"
RELATED WORKS,0.1038961038961039,"systematically analyze the trade-off between compute budget and problem-solving performance, and
86"
RELATED WORKS,0.10497835497835498,"propose a tree search method that is empirically Pareto-optimal.
87"
RELATED WORKS,0.10606060606060606,"Process Reward Models.
Process reward models (PRMs) have emerged as a technique to improve
88"
RELATED WORKS,0.10714285714285714,"the reasoning and problem-solving capabilities of LLMs. These models assign rewards to the
89"
RELATED WORKS,0.10822510822510822,"intermediate steps of the LLM generated sequences. PRMs have been shown effective in selecting
90"
RELATED WORKS,0.10930735930735931,"reasoning traces with a low error rate, and for providing rewards in reinforcement learning-style
91"
RELATED WORKS,0.11038961038961038,"algorithms [Uesato et al., 2022, Polu and Sutskever, 2020, Gudibande et al., 2023]. Ma et al. [2023]
92"
RELATED WORKS,0.11147186147186147,"applies the PRM to give rewards on the intermediate steps and guide the multi-step reasoning process.
93"
RELATED WORKS,0.11255411255411256,"The PRM can be either trained on human labeled data [Lightman et al., 2023a] or model-labeled
94"
RELATED WORKS,0.11363636363636363,"synthetic data [Wang et al., 2023]. In our work, we use the PRM as the reward model for selecting
95"
RELATED WORKS,0.11471861471861472,"generated solutions, and for selecting which partial solutions to explore in tree search.
96"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.11580086580086581,"3
An Empirical Analysis of Compute-Optimal Inference for Problem-Solving
97"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.11688311688311688,"We explore the following question: Given a fixed FLOPs budget, how should one select an optimal
98"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.11796536796536797,"model size for the policy model, and an effective inference strategy to maximize performance (i.e.,
99"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.11904761904761904,"accuracy)? To address this, we represent the problem-solving error rate E(N, T) as a function of the
100"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.12012987012987013,"number of model parameters N and the number of generated tokens T. The computational budget C
101"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.12121212121212122,"is a deterministic function FLOPs(N, T), based on N and T. Our goal is to minimize E under the
102"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.12229437229437229,"test-time compute constraint FLOPs(N, T) = C:
103"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.12337662337662338,"Nopt(C), Topt(C) =
arg min
N,T s.t. FLOPs(N,T )=C
E(N, T)
(1)"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.12445887445887446,"where Nopt(C) and Topt(C) denote the optimal allocation of a computational budget C.
104"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.12554112554112554,"Here, the inference computation (FLOPs) for a fixed model can be modulated by generating more
105"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.1266233766233766,"tokens with the policy model, e.g., by sampling additional candidate solutions and subsequently
106"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.1277056277056277,"ranking them using a reward model. We primarily consider sampling and tree-search approaches
107"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.12878787878787878,"with reranking or majority voting as the means to consume more tokens, including Greedy Search,
108"
AN EMPIRICAL ANALYSIS OF COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING,0.12987012987012986,"Majority Voting, Best-of-N, Weighted Voting, and their variants on tree search methods.
109"
INFERENCE STRATEGIES,0.13095238095238096,"3.1
Inference Strategies
110"
SAMPLING,0.13203463203463203,"3.1.1
Sampling
111"
SAMPLING,0.1331168831168831,"Greedy Search. This strategy generates tokens one at a time by selecting the highest probability token
112"
SAMPLING,0.1341991341991342,"at each step, without considering future steps. It is computationally efficient but often suboptimal in
113"
SAMPLING,0.13528138528138528,"terms of diversity.
114"
SAMPLING,0.13636363636363635,"Best-of-n. This strategy, also known as rejection sampling, samples multiple solutions and chooses
115"
SAMPLING,0.13744588744588745,"the one with the highest score given by the reward model.
116"
SAMPLING,0.13852813852813853,"Majority Voting. In this strategy, multiple model outputs are generated, and the final answer to the
117"
SAMPLING,0.1396103896103896,"problem is determined by the most frequently occurring answer in all the outputs.
118"
SAMPLING,0.1406926406926407,"Weighted Majority Voting. This strategy is a variant of majority voting in which the votes are
119"
SAMPLING,0.14177489177489178,"weighted based on the score given by the reward model.
120"
SAMPLING,0.14285714285714285,"3.1.2
Monte Carlo Tree Search (MCTS)
121"
SAMPLING,0.14393939393939395,"Monte Carlo Tree Search (MCTS) has proven effective in domains such as board games where
122"
SAMPLING,0.14502164502164502,"strategic decision-making is required [Silver et al., 2016, 2017, Jones, 2021]. Recent work has shown
123"
SAMPLING,0.1461038961038961,"that adapting MCTS to the context of LLMs can enhance the text generation process [Zhang et al.,
124"
SAMPLING,0.1471861471861472,"2023, Zhou et al., 2023, Liu et al., 2024, Choi et al., 2023, Chen et al., 2024a, Tian et al., 2024,
125"
SAMPLING,0.14826839826839827,"Chen et al., 2024a]. In this context, MCTS is often paired with a value model to score and guide the
126"
SAMPLING,0.14935064935064934,"exploration steps. For additional background, we provide a review of MCTS in Appendix B.
127"
SAMPLING,0.15043290043290045,"Recent work in MCTS or its variants (e.g., Tree of Thoughts [Yao et al., 2023]) mainly focus on
128"
SAMPLING,0.15151515151515152,"improving the performance (e.g., accuracy) on the studied tasks. However, generic comparisons of
129"
SAMPLING,0.1525974025974026,"MCTS with conventional methods like Best-of-n and Majority Voting in terms of computational
130"
SAMPLING,0.15367965367965367,"budget, measured in generated tokens or processing time, are either scarce or indicating inference-
131"
SAMPLING,0.15476190476190477,"time issues. For example, MCTS consumes substantially more resources, often requiring dozens of
132"
SAMPLING,0.15584415584415584,"times more generated tokens than simpler methods. Specifically, a significant portion of the paths
133"
SAMPLING,0.15692640692640691,"in the search tree are used to estimate and select nodes, and these paths do not necessarily become
134"
SAMPLING,0.15800865800865802,"a part of the final candidate solution, although MCTS ensures that the sampled solutions comprise
135"
SAMPLING,0.1590909090909091,"high-quality intermediate steps. In contrast, sampling methods generate multiple solutions in parallel
136"
SAMPLING,0.16017316017316016,"and independently, and all the generated sequences are included in the candidate solutions. However,
137"
SAMPLING,0.16125541125541126,"the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no
138"
SAMPLING,0.16233766233766234,"mechanism for pruning poor steps or exploiting promising ones.
139"
SAMPLING,0.1634199134199134,"This highlights the need for developing a new tree search method that can achieve a comparable (or
140"
SAMPLING,0.1645021645021645,"better) performance as MCTS, and that is computationally less costly, just like weighted majority
141"
SAMPLING,0.16558441558441558,"voting and best-of-n. This need leads to the development of our new method named Reward Balanced
142"
SAMPLING,0.16666666666666666,"SEarch (REBASE), as introduced next.
143"
SAMPLING,0.16774891774891776,"3.1.3
Reward Balanced Search (REBASE)
144"
SAMPLING,0.16883116883116883,"The REBASE tree search method inherits the exploitation and pruning properties of tree search,
145"
SAMPLING,0.1699134199134199,"while using the reward model alone to estimate the nodes’ qualities without additional computation
146"
SAMPLING,0.170995670995671,"for estimating values by sampling children. The efficiency is achieved by constraining the total
147"
SAMPLING,0.17207792207792208,"expansion width of the tree at a certain depth. REBASE balances the expansion width among the
148"
SAMPLING,0.17316017316017315,"nodes at the same depth based on the rewards given by the Process Reward Model (PRM). The details
149"
SAMPLING,0.17424242424242425,"are provided below:
150"
SAMPLING,0.17532467532467533,"Notations. We consider the fine-tuned LLM as a policy πθ. Given a question q and the first k steps
151"
SAMPLING,0.1764069264069264,"of a solution x1, · · · , xk, the (k + 1)-th step is produced by πθ(xk+1|q, x1 · · · xk). When generating
152"
SAMPLING,0.1774891774891775,"solutions using tree search, the root of the tree corresponds to the question q. The node corresponding
153"
SAMPLING,0.17857142857142858,Figure 3: Illustration of one iteration of REward BAlanced SEarch (REBASE).
SAMPLING,0.17965367965367965,"to xk+1 is the child of the node corresponding to xk if it is sampled from πθ(·|q, x1 · · · , xk). The
154"
SAMPLING,0.18073593073593072,"reward of a node n(xk) is determined by the PRM as R(n(xk)) = R(q, x1, · · · , xk).
155"
SAMPLING,0.18181818181818182,"Initialization. Given the question q, balance temperature Tb, and sampling number of solutions N,
156"
SAMPLING,0.1829004329004329,"we sample N instances of the first step for the question, yielding all the nodes of depth 1 in the search
157"
SAMPLING,0.18398268398268397,"tree. We set the sampling budget of depth 0 B0 = N as initialization.
158"
SAMPLING,0.18506493506493507,"Reward modeling and update. In the i-th iteration, the PRM assigns the rewards to all the nodes
159"
SAMPLING,0.18614718614718614,"at depth i. After that, the algorithm examines whether the solutions up to depth i are complete.
160"
SAMPLING,0.18722943722943722,"Supposing there are Ci completed solutions, we update the sampling budget using Bi ←Bi−1 −Ci.
161"
SAMPLING,0.18831168831168832,"If Bi = 0, the process ends, and we obtain N solutions.
162"
SAMPLING,0.1893939393939394,"Exploration balancing and expansion. For all of the nodes nj with reward R(nj) in the depth i of
163"
SAMPLING,0.19047619047619047,"the tree, we calculate the expansion width of the nj as:
164"
SAMPLING,0.19155844155844157,"Wj = Round

Bi
exp (R(nj)/Tb)
P
k exp (R(nk)/Tb)"
SAMPLING,0.19264069264069264,"
.
(2)"
SAMPLING,0.1937229437229437,"Then we sample Wj children for nj for all the nodes in depth i, and start the next iteration.
165"
THEORETICAL ANALYSIS,0.19480519480519481,"3.1.4
Theoretical Analysis
166"
THEORETICAL ANALYSIS,0.1958874458874459,"Before empirically studying the scaling effects of increasing the inference-time compute budget,
167"
THEORETICAL ANALYSIS,0.19696969696969696,"we present two theorems which will help us understand the experimental results later. These two
168"
THEORETICAL ANALYSIS,0.19805194805194806,"theorems give an upper bound on the performance of sampling when fixing the LLM generator.
169"
THEORETICAL ANALYSIS,0.19913419913419914,"We assume the vocabulary is limited and the sequence length is constrained, thus the number of
170"
THEORETICAL ANALYSIS,0.2002164502164502,"possible solutions and answers are finite. The proofs are provided in the Appendix A.
171"
THEORETICAL ANALYSIS,0.2012987012987013,"Theorem 1. Given a test dataset D and a LLM π. |A| is the finite set of all possible answers given
172"
THEORETICAL ANALYSIS,0.20238095238095238,"by LLM, the ground truth function g maps test data d to the true answer. Denote the accuracy of the
173"
THEORETICAL ANALYSIS,0.20346320346320346,"LLM on this dataset with majority over N samples as ACCMV (π, D, N). The accuracy of majority
174"
THEORETICAL ANALYSIS,0.20454545454545456,"voting on the LLM will eventually saturate, i.e.
175"
THEORETICAL ANALYSIS,0.20562770562770563,"lim
N→∞ACCMV (π, D, N) =
P"
THEORETICAL ANALYSIS,0.2067099567099567,d∈D I ((g(d) = arg maxa∈A π(a|d))
THEORETICAL ANALYSIS,0.2077922077922078,"|D|
.
(3)"
THEORETICAL ANALYSIS,0.20887445887445888,"2
4
8
16
32
64
128
256
Inference FLOPs per question (×1012) 15 20 25 30 35"
THEORETICAL ANALYSIS,0.20995670995670995,Test error
THEORETICAL ANALYSIS,0.21103896103896103,Infer. scaling on GSM8K (Weighted Majority)
THEORETICAL ANALYSIS,0.21212121212121213,"Sampling (Llemma-7B)
Sampling (Llemma-34B)
REBASE (Llemma-7B)
REBASE (Llemma-34B)"
THEORETICAL ANALYSIS,0.2132034632034632,"2
4
8
16
32
64
128
256
Inference FLOPs per question (×1012) 15 20 25 30 35"
THEORETICAL ANALYSIS,0.21428571428571427,Test error
THEORETICAL ANALYSIS,0.21536796536796537,Infer. scaling on GSM8K (Best-of-N)
THEORETICAL ANALYSIS,0.21645021645021645,"Sampling (Llemma-7B)
Sampling (Llemma-34B)
REBASE (Llemma-7B)
REBASE (Llemma-34B)"
THEORETICAL ANALYSIS,0.21753246753246752,"Figure 4: The inference computation scaling comparisons across different model sizes. The
left/right panel shows the GSM8K problem-solving error rate on GSM8K based on Weighted
Mjority/Best-of-N."
THEORETICAL ANALYSIS,0.21861471861471862,"where π(x|d) denotes the probability that the LLM answers x given input d and I is the indicator
176"
THEORETICAL ANALYSIS,0.2196969696969697,"function.
177"
THEORETICAL ANALYSIS,0.22077922077922077,"Theorem 2. Assume the reward model assigns an expected reward of R(a) to a ∈A among the
178"
THEORETICAL ANALYSIS,0.22186147186147187,"different solutions generated by LLM that yields a. Given a test dataset D and a LLM π. |A| is the
179"
THEORETICAL ANALYSIS,0.22294372294372294,"finite set of all possible answers given by LLM, the ground truth function g maps test data d to the
180"
THEORETICAL ANALYSIS,0.22402597402597402,"true answer. Denote the accuracy of the LLM on this dataset with weighted majority over N samples
181"
THEORETICAL ANALYSIS,0.22510822510822512,"as ACCW V (π, D, N, R). The accuracy of weighted majority voting on the LLM will eventually
182"
THEORETICAL ANALYSIS,0.2261904761904762,"saturate, i.e.
183"
THEORETICAL ANALYSIS,0.22727272727272727,"lim
N→∞ACCW V (π, D, N, R) =
P"
THEORETICAL ANALYSIS,0.22835497835497837,d∈D I ((g(d) = arg maxa∈A R(a)π(a|d))
THEORETICAL ANALYSIS,0.22943722943722944,"|D|
.
(4)"
THEORETICAL ANALYSIS,0.2305194805194805,"where π(x|d) denotes the probability that the LLM answers x given input d and I denotes the
184"
THEORETICAL ANALYSIS,0.23160173160173161,"indicator function.
185"
THEORETICAL ANALYSIS,0.2326839826839827,"Theorem 2 shows that as long as the reward model assigns higher rewards than the policy for correct an-
186"
THEORETICAL ANALYSIS,0.23376623376623376,"swers versus other answers in expectation, the upper bound of Weighted Majority Voting will be higher
187"
THEORETICAL ANALYSIS,0.23484848484848486,"than Majority Voting since I ((g(d) = arg maxa∈A R(a)π(a|d)) > I ((g(d) = arg maxa∈A π(a|d)).
188"
THEORETICAL ANALYSIS,0.23593073593073594,"We put the figures comparing BoN and Weighted Majority Voting in the main paper and leave the
189"
THEORETICAL ANALYSIS,0.237012987012987,"Majority Voting figures in Appendix D since Majority Voting is dominated by Weighted Majority
190"
THEORETICAL ANALYSIS,0.23809523809523808,"Voting.
191"
EXPERIMENTS,0.23917748917748918,"4
Experiments
192"
SETUP,0.24025974025974026,"4.1
Setup
193"
SETUP,0.24134199134199133,"Datasets.
We conduct experiments on two mathematical problem-solving datasets to investigate
194"
SETUP,0.24242424242424243,"the scaling effects of computation and our REBASE method for both challenging and simpler
195"
SETUP,0.2435064935064935,"problems. Specifically, MATH [Hendrycks et al., 2021a] and GSM8K[Cobbe et al., 2021b] are
196"
SETUP,0.24458874458874458,"datasets containing high school mathematics competition-level problems and grade-school level
197"
SETUP,0.24567099567099568,"mathematical reasoning problems, respectively. Following [Lightman et al., 2023b, Wang et al., 2024,
198"
SETUP,0.24675324675324675,"Sun et al., 2024], we use the MATH500 subset as our test set.
199"
SETUP,0.24783549783549783,"Generators.
We use Llemma-7B and Llemma-34B [Azerbayev et al., 2024] as our base models and
200"
SETUP,0.24891774891774893,"finetune them on the MetaMath dataset [Yu et al., 2024] using full parameter supervised fine-tuning
201"
SETUP,0.25,"(Full-SFT), The detailed finetuning configuration is given in the Appendix. Additionally, we test the
202"
SETUP,0.2510822510822511,"Mistral-7B model to expand our findings across different models.
203"
SETUP,0.25216450216450215,"Reward Model.
All of the experiments use the same Llemma-34B reward model, which we
204"
SETUP,0.2532467532467532,"finetuned on the synthetic process reward modeling dataset, Math-Shepherd [Wang et al., 2024]. We
205"
SETUP,0.25432900432900435,"added a reward head to make the model, enabling it to output a scalar reward at the end of each step.
206"
SETUP,0.2554112554112554,"4
16
64
256
1024
Infer. FLOPs per question (×1012) 50 55 60 65 70 75"
SETUP,0.2564935064935065,Test error
SETUP,0.25757575757575757,Llemma-7B on MATH
SETUP,0.25865800865800864,"Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN"
SETUP,0.2597402597402597,"16
32
64 128 256 5121024
Infer. FLOPs per question (×1012) 50 55 60 65 70"
SETUP,0.26082251082251084,Test error
SETUP,0.2619047619047619,Llemma-34B on MATH
SETUP,0.262987012987013,"Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN"
SETUP,0.26406926406926406,"4
16
64
256
1024
Infer. FLOPs per question (×1012) 55 60 65 70 75"
SETUP,0.26515151515151514,Test error
SETUP,0.2662337662337662,Mistral-7B on MATH
SETUP,0.26731601731601734,"Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN"
SETUP,0.2683982683982684,"Figure 5: The inference computation scaling laws of different models for the problem-solving
error rate on MATH500 test set. The tested models are Llemma-7B (left), Llemma-34B (middle),
& Mistral-7B (right). In the legend, W.M. and BoN refer to Weighted Majority and Best-of-N,
respectively."
SETUP,0.2694805194805195,"Inference Configuration.
For the MATH dataset, we sample 1, 2, 4, 8, 16, 32, 64, 128, and 256
207"
SETUP,0.27056277056277056,"solutions for the 7B models, and 1 to 64 solutions for the 34B Llemma model. For the GSM8K dataset,
208"
SETUP,0.27164502164502163,"we sample 1 to 32 solutions, as it is relatively easier. We use sampling and REBASE to generate
209"
SETUP,0.2727272727272727,"these samples and select the answer through Best-of-N, Majority Voting, and Weighted Voting.
210"
SETUP,0.27380952380952384,"Each configuration is run multiple times to calculate the mean and variance, thereby mitigating the
211"
SETUP,0.2748917748917749,"randomness and ensuring the reliability of our conclusions.
212"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.275974025974026,"4.2
Main Results of Compute-Optimal Inference
213"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.27705627705627706,"In order to compare the compute budgets of 7B and 34B models, we plot the figures with the number
214"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.27813852813852813,"of FLOPs used per question during inference. We compute the inference FLOPs based on the standard
215"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2792207792207792,"formula from [Kaplan et al., 2020].
216"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2803030303030303,"Llemma-7B model achieves competitive accuracy to Llemma-34B model with lower compute
217"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2813852813852814,"budget.
Figures 1 and 4 show the curves of error rates versus total number of inference FLOPs per
218"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2824675324675325,"question. Inference methods with different model sizes are plotted in the same diagram. We found
219"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.28354978354978355,"that Llemma-7B costs approximately 2x less total FLOPs than Llemma-34B under the same method
220"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2846320346320346,"(Sampling, MCTS, REBASE) and task (MATH, GSM8K) while achieving competitive accuracy.
221"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2857142857142857,"This result suggests that, with the same training dataset and model family, training and inference with
222"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.28679653679653677,"a smaller model could be more favorable in terms of compute budget if multiple sampling or search
223"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2878787878787879,"methods are employed.
224"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.288961038961039,"All inference configurations will saturate eventually.
This is expected as Theorem 1 and Theorem
225"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.29004329004329005,"2 show. Also illustrated in Figures 5 and 6, the slope of the erro rate curves start large, then decreases
226"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2911255411255411,"and the curves finally become nearly flat as the number of samples scales, showing the effect of
227"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2922077922077922,"saturation.
228"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.29329004329004327,"Scaling law of compute-optimal inference.
The findings in our experiments are consistent with
229"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2943722943722944,"the Theorem 1 and 2, After a threshold the accruacy of sampling more solutions saturate, we should
230"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.29545454545454547,"scale the model size. We interpolate the smoothed test error rate curve in Figure 1 and Figure 4,
231"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.29653679653679654,"and fit power laws to estimate the optimal model size N and number of generated tokens T for any
232"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2976190476190476,"given amount of compute. We obtained a relationship Nopt ∝Ca and Topt ∝Cb, where a = 1.0
233"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.2987012987012987,"and b = 0.0 for both sampling-based weighted voting and our tree-search method REBASE. Our
234"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.29978354978354976,"fitted curves indicate that the optimal inference strategy is invariant to the amount of compute (e.g.,
235"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.3008658008658009,"re-ranking with 32 sampled solutions or REBASE tree search with a compute budget of 64 for
236"
MAIN RESULTS OF COMPUTE-OPTIMAL INFERENCE,0.30194805194805197,"MATH), and the optimal model size grows linearly with the increased compute budget.
237"
COMPARING REBASE TO OTHER BASELINES,0.30303030303030304,"4.3
Comparing REBASE to Other Baselines
238"
COMPARING REBASE TO OTHER BASELINES,0.3041125541125541,"REBASE is Pareto-optimal.
While MCTS undeperforms Sampling (Fig. 1), from Fig. 1, 4, 5,
239"
COMPARING REBASE TO OTHER BASELINES,0.3051948051948052,"and 6, we found that REBASE consistently outperforms the Sampling method in all settings, when
240"
COMPARING REBASE TO OTHER BASELINES,0.30627705627705626,"2
4
8
16
32
64
Infer. FLOPs per question (×1012) 15 20 25 30 35"
COMPARING REBASE TO OTHER BASELINES,0.30735930735930733,Test error
COMPARING REBASE TO OTHER BASELINES,0.30844155844155846,Llemma-7B on GSM8K
COMPARING REBASE TO OTHER BASELINES,0.30952380952380953,"Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN"
COMPARING REBASE TO OTHER BASELINES,0.3106060606060606,"16
32
64
128
256
Infer. FLOPs per question (×1012) 12 14 16 18 20 22 24"
COMPARING REBASE TO OTHER BASELINES,0.3116883116883117,Test error
COMPARING REBASE TO OTHER BASELINES,0.31277056277056275,Llemma-34B on GSM8K
COMPARING REBASE TO OTHER BASELINES,0.31385281385281383,"Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN"
COMPARING REBASE TO OTHER BASELINES,0.31493506493506496,"2
4
8
16
32
64
Infer. FLOPs per question (×1012) 10 12 14 16 18 20 22 24"
COMPARING REBASE TO OTHER BASELINES,0.31601731601731603,Test error
COMPARING REBASE TO OTHER BASELINES,0.3170995670995671,Mistral-7B on GSM8K
COMPARING REBASE TO OTHER BASELINES,0.3181818181818182,"Sampling W.M.
Sampling BoN
REBASE W.M.
REBASE BoN"
COMPARING REBASE TO OTHER BASELINES,0.31926406926406925,"Figure 6: The inference computation scaling laws of different models for the problem-solving
error rate on GSM8K test set. The tested models are Llemma-7B (left), Llemma-34B (middle),
& Mistral-7B (right). In the legend, W.M. and BoN refer to Weighted Majority and Best-of-N,
respectively."
COMPARING REBASE TO OTHER BASELINES,0.3203463203463203,"Table 1: REBASE with lower compute budget has competitive accuracy against Sampling with
higher compute budget. We use weighted voting to aggreagte the candidate solutions in both Sampling
and REBASE."
COMPARING REBASE TO OTHER BASELINES,0.32142857142857145,"# SAMPLES
FLOPS
MATH500"
COMPARING REBASE TO OTHER BASELINES,0.3225108225108225,MISTRAL-7B
COMPARING REBASE TO OTHER BASELINES,0.3235930735930736,"SAMPLING
256
8.7 × 1014
42.8
REBASE
32
1.36 × 1014
45.0"
COMPARING REBASE TO OTHER BASELINES,0.3246753246753247,LLEMMA-7B
COMPARING REBASE TO OTHER BASELINES,0.32575757575757575,"SAMPLING
256
10.0 × 1014
45.5
REBASE
32
1.48 × 1014
46.8"
COMPARING REBASE TO OTHER BASELINES,0.3268398268398268,LLEMMA-34B
COMPARING REBASE TO OTHER BASELINES,0.32792207792207795,"SAMPLING
64
12.1 × 1014
46.7
REBASE
32
7.08 × 1014
49.2"
COMPARING REBASE TO OTHER BASELINES,0.329004329004329,"fixing the model and the evaluation task. Table 1 shows that REBASE can achieve competitive
241"
COMPARING REBASE TO OTHER BASELINES,0.3300865800865801,"accuracy with even a lower compute budget than the sampling method. This finding is novel, and
242"
COMPARING REBASE TO OTHER BASELINES,0.33116883116883117,"differs from previous tree search works which typically improve the performance at the cost of higher
243"
COMPARING REBASE TO OTHER BASELINES,0.33225108225108224,"computational expense compared to sampling [Chen et al., 2024a, Xie et al., 2023]. Table 2 shows
244"
COMPARING REBASE TO OTHER BASELINES,0.3333333333333333,"that given the same compute budget (sampling 32 solutions for the 7B model and 8 solutions for 34B
245"
COMPARING REBASE TO OTHER BASELINES,0.3344155844155844,"model), using REBASE yields higher accuray than sampling.
246"
COMPARING REBASE TO OTHER BASELINES,0.3354978354978355,"Weaker models gain more from Tree Search.
From Fig. 2, we saw that compared with sampling,
247"
COMPARING REBASE TO OTHER BASELINES,0.3365800865800866,"Mistral-7B, Llemma-7B, Llemma-34B increase 5.3%, 3.3%, 2.6% in MATH and 0.7%, 1.9%, 0.9%
248"
COMPARING REBASE TO OTHER BASELINES,0.33766233766233766,"in GSM8K. The order of accuracy increase is inversely related to the model’s corresponding greedy
249"
COMPARING REBASE TO OTHER BASELINES,0.33874458874458874,"search on those datasets. This suggests that weaker models, as indicated by their lower greedy search
250"
COMPARING REBASE TO OTHER BASELINES,0.3398268398268398,"accuracy, benefit more from tree search methods like REBASE.
251"
COMPARING REBASE TO OTHER BASELINES,0.3409090909090909,"REBASE saturates later than sampling with higher accuray.
From Figure 5 and Figure 6, we
252"
COMPARING REBASE TO OTHER BASELINES,0.341991341991342,"observe that both sampling and REBASE saturate early in GSM8K and relatively late in MATH,
253"
COMPARING REBASE TO OTHER BASELINES,0.3430735930735931,"which we attribute to the difference of the difficulty level. This can be explained through the LLM
254"
COMPARING REBASE TO OTHER BASELINES,0.34415584415584416,"may assign high probability to the true answer in easy problems than those of harder problem, as
255"
COMPARING REBASE TO OTHER BASELINES,0.34523809523809523,"suggested by Theorem 1 and 2 with their proofs A. On MATH (Figure 5), we see that REBASE
256"
COMPARING REBASE TO OTHER BASELINES,0.3463203463203463,"finally saturates with a higher accuracy than sampling. We hypothesize the reason is that REBASE
257"
COMPARING REBASE TO OTHER BASELINES,0.3474025974025974,"samples the truth answer with higher probability than sampling. And as demonstrated by Theorem 1
258"
COMPARING REBASE TO OTHER BASELINES,0.3484848484848485,"and 2, the upper bound becomes higher.
259"
COMPARING REBASE TO OTHER BASELINES,0.3495670995670996,"Table 2: Accuracy of diffrent inference configurations under a specific compute budget. MV, BoN
and WV denote Majority Voting, Best-of-N and Weighted Voting, respectively."
COMPARING REBASE TO OTHER BASELINES,0.35064935064935066,"# SAMPLES
MATH FLOPS
GSM8K FLOPS
MATH500
GSM8K"
COMPARING REBASE TO OTHER BASELINES,0.35173160173160173,MISTRAL-7B
COMPARING REBASE TO OTHER BASELINES,0.3528138528138528,"GREEDY
1
3.4 × 1012
2.3 × 1012
28.6
77.9
SAMPLING + MV
32
109.2 × 1012
72.6 × 1012
36.1
85.7
SAMPLING + BON
32
109.2 × 1012
72.6 × 1012
40.3
89.4
SAMPLING + WV
32
109.2 × 1012
72.6 × 1012
39.7
89.1
REBASE + MV
32
136.2 × 1012
78.9 × 1012
44.1
88.8
REBASE + BON
32
136.2 × 1012
78.9 × 1012
45.4
89.4
REBASE + WV
32
136.2 × 1012
78.9 × 1012
45.0
89.8"
COMPARING REBASE TO OTHER BASELINES,0.3538961038961039,LLEMMA-7B
COMPARING REBASE TO OTHER BASELINES,0.354978354978355,"GREEDY
1
3.92 × 1012
2.3 × 1012
30.0
68.5
SAMPLING + MV
32
125.4 × 1012
73.9 × 1012
41.0
80.0
SAMPLING + BON
32
125.4 × 1012
73.9 × 1012
41.7
85.6
SAMPLING + WV
32
125.4 × 1012
73.9 × 1012
43.5
85.4
REBASE + MV
32
148.0 × 1012
82.6 × 1012
46.1
86.1
REBASE + BON
32
148.0 × 1012
82.6 × 1012
44.1
86.9
REBASE + WV
32
148.0 × 1012
82.6 × 1012
46.8
87.3"
COMPARING REBASE TO OTHER BASELINES,0.3560606060606061,LLEMMA-34B
COMPARING REBASE TO OTHER BASELINES,0.35714285714285715,"GREEDY
1
19.0 × 1012
11.2 × 1012
33.0
78.4
SAMPLING + MV
8
152.3 × 1012
89.7 × 1012
39.9
84.3
SAMPLING + BON
8
152.3 × 1012
89.7 × 1012
40.4
86.7
SAMPLING + WV
8
152.3 × 1012
89.7 × 1012
41.0
86.0
REBASE + MV
8
176.8 × 1012
98.7 × 1012
43.9
86.1
REBASE + BON
8
176.8 × 1012
98.7 × 1012
43.6
86.9
REBASE + WV
8
176.8 × 1012
98.7 × 1012
42.9
86.9"
CONCLUSION & LIMITATIONS,0.3582251082251082,"5
Conclusion & Limitations
260"
CONCLUSION & LIMITATIONS,0.3593073593073593,"In this work, we have conducted a comprehensive empirical analysis of compute-optimal inference
261"
CONCLUSION & LIMITATIONS,0.36038961038961037,"for problem-solving with language models. Our study has revealed several key findings. First, with
262"
CONCLUSION & LIMITATIONS,0.36147186147186144,"an optimal inference configuration, a small language model can achieve competitive accuracy to a
263"
CONCLUSION & LIMITATIONS,0.3625541125541126,"4× larger model while using approximately 2× less total FLOPs under the same inference method
264"
CONCLUSION & LIMITATIONS,0.36363636363636365,"(Sampling, MCTS, REBASE) and task (MATH, GSM8K), suggesting that training and inference
265"
CONCLUSION & LIMITATIONS,0.3647186147186147,"with smaller models could be more favorable in terms of compute budget when combined with
266"
CONCLUSION & LIMITATIONS,0.3658008658008658,"multiple sampling or search strategies. Second, our new REBASE tree-search method consistently
267"
CONCLUSION & LIMITATIONS,0.36688311688311687,"outperforms sampling (and MCTS) across all settings, models, and tasks, achieving competitive
268"
CONCLUSION & LIMITATIONS,0.36796536796536794,"accuracy with even lower compute budget compared to sampling. Our findings highlight the potential
269"
CONCLUSION & LIMITATIONS,0.36904761904761907,"of deploying smaller models equipped with more sophisticated inference strategies like REBASE to
270"
CONCLUSION & LIMITATIONS,0.37012987012987014,"enhance problem-solving accuracy while maintaining computational efficiency.
271"
CONCLUSION & LIMITATIONS,0.3712121212121212,"Limitations
First, our experiments focused specifically on mathematical problem-solving tasks,
272"
CONCLUSION & LIMITATIONS,0.3722943722943723,"and the generalizability of our findings to other domains remains to be explored. Second, we only
273"
CONCLUSION & LIMITATIONS,0.37337662337662336,"investigated a limited range of model scales, primarily focusing on 7B and 34B models. Future
274"
CONCLUSION & LIMITATIONS,0.37445887445887444,"research could extend our analysis to a wider range of model sizes to gain a more comprehensive
275"
CONCLUSION & LIMITATIONS,0.37554112554112556,"understanding of the scaling laws for compute-optimal inference. Finally, our experiments mainly
276"
CONCLUSION & LIMITATIONS,0.37662337662337664,"utilized the MetaMath dataset for training the models. It would be valuable to explore the impact of
277"
CONCLUSION & LIMITATIONS,0.3777056277056277,"different training datasets on the performance and efficiency of compute-optimal inference strategies
278"
CONCLUSION & LIMITATIONS,0.3787878787878788,"for mathematical problem-solving.
279"
REFERENCES,0.37987012987012986,"References
280"
REFERENCES,0.38095238095238093,"David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann
281"
REFERENCES,0.38203463203463206,"machines. Cognitive science, 9(1):147–169, 1985.
282"
REFERENCES,0.38311688311688313,"Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q
283"
REFERENCES,0.3841991341991342,"Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for
284"
REFERENCES,0.3852813852813853,"mathematics. arXiv preprint arXiv:2310.10631, 2023.
285"
REFERENCES,0.38636363636363635,"Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q.
286"
REFERENCES,0.3874458874458874,"Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for
287"
REFERENCES,0.38852813852813856,"mathematics, 2024.
288"
REFERENCES,0.38961038961038963,"Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr,
289"
REFERENCES,0.3906926406926407,"Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.
290"
REFERENCES,0.3917748917748918,"Video generation models as world simulators. 2024. URL https://openai.com/research/
291"
REFERENCES,0.39285714285714285,"video-generation-models-as-world-simulators.
292"
REFERENCES,0.3939393939393939,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal,
293"
REFERENCES,0.395021645021645,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
294"
REFERENCES,0.3961038961038961,"few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.
295"
REFERENCES,0.3971861471861472,"Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision
296"
REFERENCES,0.39826839826839827,"without process, 2024a.
297"
REFERENCES,0.39935064935064934,"Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search
298"
REFERENCES,0.4004329004329004,"useful for llm planning? it depends on the discriminator. arXiv preprint arXiv:2402.10890, 2024b.
299"
REFERENCES,0.4015151515151515,"Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. Kcts: Knowledge-constrained tree
300"
REFERENCES,0.4025974025974026,"search decoding with token-level hallucination detection, 2023.
301"
REFERENCES,0.4036796536796537,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
302"
REFERENCES,0.40476190476190477,"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:
303"
REFERENCES,0.40584415584415584,"Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
304"
REFERENCES,0.4069264069264069,"Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
305"
REFERENCES,0.408008658008658,"Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
306"
REFERENCES,0.4090909090909091,"Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,
307"
REFERENCES,0.4101731601731602,"2021a.
308"
REFERENCES,0.41125541125541126,"Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
309"
REFERENCES,0.41233766233766234,"Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve
310"
REFERENCES,0.4134199134199134,"math word problems. arXiv preprint arXiv:2110.14168, 2021b.
311"
REFERENCES,0.4145021645021645,"Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In
312"
REFERENCES,0.4155844155844156,"International Conference on Machine Learning, pages 10835–10866. PMLR, 2023.
313"
REFERENCES,0.4166666666666667,"Alex Graves. Sequence transduction with recurrent neural networks, 2012.
314"
REFERENCES,0.41774891774891776,"Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine,
315"
REFERENCES,0.41883116883116883,"and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717,
316"
REFERENCES,0.4199134199134199,"2023.
317"
REFERENCES,0.420995670995671,"Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin
318"
REFERENCES,0.42207792207792205,"Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence
319"
REFERENCES,0.4231601731601732,"with apps. arXiv preprint arXiv:2105.09938, 2021a.
320"
REFERENCES,0.42424242424242425,"Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
321"
REFERENCES,0.4253246753246753,"Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In
322"
REFERENCES,0.4264069264069264,"Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track
323"
REFERENCES,0.4274891774891775,"(Round 2), 2021b.
324"
REFERENCES,0.42857142857142855,"Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun,
325"
REFERENCES,0.4296536796536797,"Tom B. Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative
326"
REFERENCES,0.43073593073593075,"modeling. arXiv preprint arXiv:2010.14701, 2020.
327"
REFERENCES,0.4318181818181818,"Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
328"
REFERENCES,0.4329004329004329,"Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
329"
REFERENCES,0.43398268398268397,"empirically. arXiv preprint arXiv:1712.00409, 2017.
330"
REFERENCES,0.43506493506493504,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
331"
REFERENCES,0.43614718614718617,"Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
332"
REFERENCES,0.43722943722943725,"Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
333"
REFERENCES,0.4383116883116883,"Andy L Jones. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113, 2021.
334"
REFERENCES,0.4393939393939394,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
335"
REFERENCES,0.44047619047619047,"Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
336"
REFERENCES,0.44155844155844154,"arXiv preprint arXiv:2001.08361, 2020.
337"
REFERENCES,0.44264069264069267,"Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
338"
REFERENCES,0.44372294372294374,"language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
339"
REFERENCES,0.4448051948051948,"Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-
340"
REFERENCES,0.4458874458874459,"masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative
341"
REFERENCES,0.44696969696969696,"reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.
342"
REFERENCES,0.44805194805194803,"Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
343"
REFERENCES,0.4491341991341991,"Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
344"
REFERENCES,0.45021645021645024,"arXiv:2305.20050, 2023a.
345"
REFERENCES,0.4512987012987013,"Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike,
346"
REFERENCES,0.4523809523809524,"John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step, 2023b.
347"
REFERENCES,0.45346320346320346,"Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gen-
348"
REFERENCES,0.45454545454545453,"eration: Learning to solve and explain algebraic word problems. In Proceedings of the 55th
349"
REFERENCES,0.4556277056277056,"Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
350"
REFERENCES,0.45670995670995673,"158–167, 2017.
351"
REFERENCES,0.4577922077922078,"Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli
352"
REFERENCES,0.4588744588744589,"Celikyilmaz. Don’t throw away your value model! generating more preferable text with value-
353"
REFERENCES,0.45995670995670995,"guided monte-carlo tree search decoding, 2024.
354"
REFERENCES,0.461038961038961,"Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang.
355"
REFERENCES,0.4621212121212121,"Let’s reward step by step: Step-level reward model as the navigators for reasoning, 2023.
356"
REFERENCES,0.46320346320346323,"Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
357"
REFERENCES,0.4642857142857143,"Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:
358"
REFERENCES,0.4653679653679654,"Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,
359"
REFERENCES,0.46645021645021645,"2021.
360"
REFERENCES,0.4675324675324675,"OpenAI. Gpt-4 technical report, 2023.
361"
REFERENCES,0.4686147186147186,"William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of
362"
REFERENCES,0.4696969696969697,"the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023.
363"
REFERENCES,0.4707792207792208,"Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.
364"
REFERENCES,0.47186147186147187,"arXiv preprint arXiv:2009.03393, 2020.
365"
REFERENCES,0.47294372294372294,"Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction
366"
REFERENCES,0.474025974025974,"of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019.
367"
REFERENCES,0.4751082251082251,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
368"
REFERENCES,0.47619047619047616,"Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
369"
REFERENCES,0.4772727272727273,"the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.
370"
REFERENCES,0.47835497835497837,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
371"
REFERENCES,0.47943722943722944,"Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
372"
REFERENCES,0.4805194805194805,"human knowledge. nature, 550(7676):354–359, 2017.
373"
REFERENCES,0.4816017316017316,"Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang
374"
REFERENCES,0.48268398268398266,"Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint
375"
REFERENCES,0.4837662337662338,"arXiv:2403.09472, 2024.
376"
REFERENCES,0.48484848484848486,"Virginia Teller. Speech and language processing: an introduction to natural language processing,
377"
REFERENCES,0.48593073593073594,"computational linguistics, and speech recognition, 2000.
378"
REFERENCES,0.487012987012987,"Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward self-
379"
REFERENCES,0.4880952380952381,"improvement of llms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253,
380"
REFERENCES,0.48917748917748916,"2024.
381"
REFERENCES,0.4902597402597403,"Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
382"
REFERENCES,0.49134199134199136,"Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and
383"
REFERENCES,0.49242424242424243,"outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.
384"
REFERENCES,0.4935064935064935,"Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang
385"
REFERENCES,0.4945887445887446,"Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR,
386"
REFERENCES,0.49567099567099565,"abs/2312.08935, 2023.
387"
REFERENCES,0.4967532467532468,"Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang
388"
REFERENCES,0.49783549783549785,"Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024.
389"
REFERENCES,0.4989177489177489,"Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-
390"
REFERENCES,0.5,"ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.
391"
REFERENCES,0.5010822510822511,"International Conference on Learning Representations (ICLR 2023), 2022a.
392"
REFERENCES,0.5021645021645021,"Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
393"
REFERENCES,0.5032467532467533,"Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.
394"
REFERENCES,0.5043290043290043,"arXiv preprint arXiv:2212.10560, 2022b.
395"
REFERENCES,0.5054112554112554,"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.
396"
REFERENCES,0.5064935064935064,"Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022.
397"
REFERENCES,0.5075757575757576,"Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie.
398"
REFERENCES,0.5086580086580087,"Self-evaluation guided beam search for reasoning, 2023.
399"
REFERENCES,0.5097402597402597,"Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik
400"
REFERENCES,0.5108225108225108,"Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv
401"
REFERENCES,0.5119047619047619,"preprint arXiv:2305.10601, 2023.
402"
REFERENCES,0.512987012987013,"Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
403"
REFERENCES,0.5140692640692641,"Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-
404"
REFERENCES,0.5151515151515151,"rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.
405"
REFERENCES,0.5162337662337663,"Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo
406"
REFERENCES,0.5173160173160173,"Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for
407"
REFERENCES,0.5183982683982684,"large language models. arXiv preprint arXiv:2309.12284, 2023.
408"
REFERENCES,0.5194805194805194,"Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok,
409"
REFERENCES,0.5205627705627706,"Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical
410"
REFERENCES,0.5216450216450217,"questions for large language models, 2024.
411"
REFERENCES,0.5227272727272727,"Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan.
412"
REFERENCES,0.5238095238095238,"Planning with large language models for code generation, 2023.
413"
REFERENCES,0.5248917748917749,"Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language
414"
REFERENCES,0.525974025974026,"agent tree search unifies reasoning acting and planning in language models, 2023.
415"
REFERENCES,0.5270562770562771,"Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,
416"
REFERENCES,0.5281385281385281,"and Jimmy Ba.
Large language models are human-level prompt engineers.
arXiv preprint
417"
REFERENCES,0.5292207792207793,"arXiv:2211.01910, 2022.
418"
REFERENCES,0.5303030303030303,"A
Proofs of Theorem 1 and 2
419"
REFERENCES,0.5313852813852814,"A.1
Proof of Theorem 1
420"
REFERENCES,0.5324675324675324,"Proof. Suppose the possible answers of the LLM are x1, x2, x3, · · · , x|A|, with π(x1|d) >
421"
REFERENCES,0.5335497835497836,"π(x2|d) > · · · > π(x|A||d). After sampling N solutions from the LLM, we denote the occurence of
422"
REFERENCES,0.5346320346320347,"xi as f(xi), the probability that x1 is not the most frequent output is
423"
REFERENCES,0.5357142857142857,"P(f(x1) ̸= arg max
x
f(x))
(5)"
REFERENCES,0.5367965367965368,"With Union bound, we get
424"
REFERENCES,0.5378787878787878,"P(x1 ̸= arg max
x
f(x))
(6) ≤ |A|
X"
REFERENCES,0.538961038961039,"i=2
P(f(x1) ≤f(xi))
(7)"
REFERENCES,0.54004329004329,"≤|A|P(f(x1) ≤f(x2))
(8)
=|A| (1 −P (f(x1) ≥f(x2)))
(9)"
REFERENCES,0.5411255411255411,"≤|A|

1 −P

f(x1) ≥π(x1|d) + π(x2|d)"
N,0.5422077922077922,"2
N

P

f(x2) ≤π(x1|d) + π(x2|d)"
N,0.5432900432900433,"2
N

(10)"
N,0.5443722943722944,"≤|A|

1 −

1 −e−
δ2
1
2 π(x1|d)N
 
1 −e−
δ2
2
2+δ2 π(x2|d)N

(11)"
N,0.5454545454545454,"≤|A|CN
for some C < 1.
(12)"
N,0.5465367965367965,"Where (11) is by Chernoff Bound, δ1 = π(x1|d)−π(x2|d)"
N,0.5476190476190477,"2π(x1|d)
and δ2 = π(x1|d)−π(x2|d)"
N,0.5487012987012987,"2π(x2|d)
. As N →∞, we
425"
N,0.5497835497835498,"have
426"
N,0.5508658008658008,"f(x) =
M(x|N) = 1
if x = arg maxa∈A π(a|d)
M(x|N) = 0
otherwise .
(13)"
N,0.551948051948052,"Where M(x|N) denotes the probability that majority voting over N sampled solutions gives x. The
427"
N,0.553030303030303,"proof of original theorem is automatically completed by (13).
428"
N,0.5541125541125541,"A.2
Proof of Theorem 2
429"
N,0.5551948051948052,"Proof. The proof is similar to the Theorem 1, We rank x1, x2, · · · , x|A| with R(x1)f(x1) > · · · >
430"
N,0.5562770562770563,"R(x|A|f(x|A|). Denotes w(xi) as the the total weights of answer xi after sampling N solutions. As
431"
N,0.5573593073593074,"N →∞, w(xi) →R(xi)f(xi). Same as proof in theorem 1, we have
432"
N,0.5584415584415584,"P(x1 ̸= arg max
x
f(x))
(14)"
N,0.5595238095238095,"≤|A|P(w(x1) ≤w(x2))
(15)
=|A| (1 −P (w(x1) ≥w(x2)))
(16)"
N,0.5606060606060606,"≤|A|

1 −P

w(x1) ≥v(x1) + v(x2)"
N,0.5616883116883117,"2
N

P

w(x2) ≤v(x1) + v(x2)"
N,0.5627705627705628,"2
N

.
(17)"
N,0.5638528138528138,"Where v(x) = R(x)π(x|d), the remaining proof completely follows Theorem 1.
433"
N,0.564935064935065,"B
MCTS Details
434"
N,0.566017316017316,"The MCTS process can be represented as the following steps:
435"
N,0.5670995670995671,"Selection
The process begins at the root node. Here, the algorithm recursively selects the child
436"
N,0.5681818181818182,"node that offers the highest Upper Confidence Bound applied to Trees (UCT) value, continuing until
437"
N,0.5692640692640693,"a node is reached that has not been expanded. The UCT is calculated using the formula
438"
N,0.5703463203463204,UCT(s) = Q(s) + C s
N,0.5714285714285714,ln (N(Parent(s)))
N,0.5725108225108225,"N(s)
.
(18)"
N,0.5735930735930735,"Table 3: Fine-tuning Hyper-parameters: LR refers to the learning rate, BS refers to the batch size.
Llemma-7B and LLemma-34B are the generators we use in our experiments, RM is short for Reward
Model."
N,0.5746753246753247,"Model
# Epoch
Dataset
BS
LR
Max Seq Length
Dtype"
N,0.5757575757575758,"Llemma-7B
1
MetaMath
128
8E-6
1024
FP32
Llemma-34B
1
MetaMath
128
8E-6
768
FP32
Llemma-34B RM
2
Math-Shepherd
128
1E-5
768
BF16"
N,0.5768398268398268,"4
16
64
256
1024
Infer. FLOPs per question (×1012) 50 55 60 65 70 75"
N,0.577922077922078,Test error
N,0.579004329004329,Llemma-7B on MATH
N,0.5800865800865801,"Sampling M.V.
REBASE M.V."
N,0.5811688311688312,"16
32
64 128 256 5121024
Infer. FLOPs per question (×1012) 50 55 60 65 70"
N,0.5822510822510822,Test error
N,0.5833333333333334,Llemma-34B on MATH
N,0.5844155844155844,"Sampling M.V.
REBASE M.V."
N,0.5854978354978355,"4
16
64
256
1024
Infer. FLOPs per question (×1012) 55 60 65 70 75"
N,0.5865800865800865,Test error
N,0.5876623376623377,Mistral-7B on MATH
N,0.5887445887445888,"Sampling M.V.
REBASE M.V."
N,0.5898268398268398,"Figure 7: The inference computation scaling laws of different models for the problem-solving
error rate on MATH test set. The tested models are Llemma-7B (left), Llemma-34B (middle), &
Mistral-7B (right). In the legend, M.V. refer to Majority Voting."
N,0.5909090909090909,"Where Q(s) represents the quality score of node s, N(s) is the number of visits to node s, and C is a
439"
N,0.591991341991342,"constant determining the level of exploration.
440"
N,0.5930735930735931,"Expansion and evaluation
Upon reaching a non-terminal node s, the node is expanded by gener-
441"
N,0.5941558441558441,"ating multiple child nodes. Each child node c is then evaluated using a value function V (c), which
442"
N,0.5952380952380952,"predicts the potential quality of continuing the sequence from node c.
443"
N,0.5963203463203464,"Backpropagation
After evaluation, the algorithm updates the UCT values and the visit counts for
444"
N,0.5974025974025974,"all nodes along the path from the selected node back to the root. For any node n in this path, the
445"
N,0.5984848484848485,"updates are made as follows:
446"
N,0.5995670995670995,"N(n) ←N(n) + 1,"
N,0.6006493506493507,Q(n) ←(N(n) −1) Q(n) + V (s)
N,0.6017316017316018,"N(n)
."
N,0.6028138528138528,"C
Hyper-parameters
447"
N,0.6038961038961039,"Finetuning
We put all the hyperparameters of fine-tuned models in the table 3. We preprocess the
448"
N,0.604978354978355,"MetaMath Dataset to make the solutions in a stepwise format.
449"
N,0.6060606060606061,"Inference
For all the inference strategies, the temperature of the LLM is set to 1.0. Max tokens
450"
N,0.6071428571428571,"for the output is 1024 and max tokens for one step is 256. For REBASE, we chose the balance
451"
N,0.6082251082251082,"temperature (the softmax temperature in the REBASE algorithm) as Tb = 0.1. For MCTS, we set
452"
N,0.6093073593073594,"C in the UCT value as 1 and we expand 4, 8, 16 children for the root, 2 children for other selected
453"
N,0.6103896103896104,"nodes with total 32, 64, 128 expansions respectively.
454"
N,0.6114718614718615,"D
Supplementary Figures
455"
N,0.6125541125541125,"In the main part of paper, there isn’t enough space for showing the scaling effects of Majority Voting,
456"
N,0.6136363636363636,"we append the figures about Majority Voting and Majority Voting v.s. Weighted Majority Voting
457"
N,0.6147186147186147,"2
4
8
16
32
64
Infer. FLOPs per question (×1012) 15 20 25 30 35"
N,0.6158008658008658,Test error
N,0.6168831168831169,Llemma-7B on GSM8K
N,0.6179653679653679,"Sampling M.V.
REBASE M.V."
N,0.6190476190476191,"16
32
64
128
256
Infer. FLOPs per question (×1012) 12 14 16 18 20 22 24"
N,0.6201298701298701,Test error
N,0.6212121212121212,Llemma-34B on GSM8K
N,0.6222943722943723,"Sampling M.V.
REBASE M.V."
N,0.6233766233766234,"2
4
8
16
32
64
Infer. FLOPs per question (×1012) 12 14 16 18 20 22 24"
N,0.6244588744588745,Test error
N,0.6255411255411255,Mistral-7B on GSM8K
N,0.6266233766233766,"Sampling M.V.
REBASE M.V."
N,0.6277056277056277,"Figure 8: The inference computation scaling laws of different models for the problem-solving
error rate on GSM8K test set. The tested models are Llemma-7B (left), Llemma-34B (middle), &
Mistral-7B (right). In the legend, M.V. refer to Majority Voting."
N,0.6287878787878788,"4
16
64
256
1024
Infer. FLOPs per question (×1012) 50 55 60 65 70 75"
N,0.6298701298701299,Test error
N,0.6309523809523809,Llemma-7B on MATH
N,0.6320346320346321,"Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M."
N,0.6331168831168831,"16
32
64 128 256 5121024
Infer. FLOPs per question (×1012) 50 55 60 65 70"
N,0.6341991341991342,Test error
N,0.6352813852813853,Llemma-34B on MATH
N,0.6363636363636364,"Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M."
N,0.6374458874458875,"4
16
64
256
1024
Infer. FLOPs per question (×1012) 55 60 65 70 75"
N,0.6385281385281385,Test error
N,0.6396103896103896,Mistral-7B on MATH
N,0.6406926406926406,"Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M."
N,0.6417748917748918,"Figure 9: The inference computation scaling laws of different models for the problem-solving
error rate on MATH test set. The tested models are Llemma-7B (left), Llemma-34B (middle), &
Mistral-7B (right). In the legend, M.V. and W.M. refer to Majority Voting and Weighted Majority,
respectively."
N,0.6428571428571429,"2
4
8
16
32
64
Infer. FLOPs per question (×1012) 15 20 25 30 35"
N,0.6439393939393939,Test error
N,0.645021645021645,Llemma-7B on GSM8K
N,0.6461038961038961,"Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M."
N,0.6471861471861472,"16
32
64
128
256
Infer. FLOPs per question (×1012) 12 14 16 18 20 22 24"
N,0.6482683982683982,Test error
N,0.6493506493506493,Llemma-34B on GSM8K
N,0.6504329004329005,"Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M."
N,0.6515151515151515,"2
4
8
16
32
64
Infer. FLOPs per question (×1012) 10 12 14 16 18 20 22 24"
N,0.6525974025974026,Test error
N,0.6536796536796536,Mistral-7B on GSM8K
N,0.6547619047619048,"Sampling M.V.
Sampling W.M.
REBASE M.V.
REBASE W.M."
N,0.6558441558441559,"Figure 10: The inference computation scaling laws of different models for the problem-solving
error rate on GSM8K test set. The tested models are Llemma-7B (left), Llemma-34B (middle), &
Mistral-7B (right). In the legend, M.V. and W.M. refer to Majority Voting and Weighted Majority,
respectively."
N,0.6569264069264069,"(Fig. 7, 8 ,9, 10) in this appendix. The experiments show that although the gap between Majority
458"
N,0.658008658008658,"Voting and Weighted Majority Voting on sampling is huge. This gap becomes much smaller if we
459"
N,0.6590909090909091,"apply REBASE. This phenomenon can be caused by the selection ability of tree search like REBASE.
460"
N,0.6601731601731602,"Once REBASE already samples solutions with high rewards, conducing weighted majority voting
461"
N,0.6612554112554112,"gains less since the sampled solutions may all have relatively high and stable rewards compared with
462"
N,0.6623376623376623,"those of sampling.
463"
N,0.6634199134199135,"NeurIPS Paper Checklist
464"
CLAIMS,0.6645021645021645,"1. Claims
465"
CLAIMS,0.6655844155844156,"Question: Do the main claims made in the abstract and introduction accurately reflect the
466"
CLAIMS,0.6666666666666666,"paper’s contributions and scope?
467"
CLAIMS,0.6677489177489178,"Answer: [Yes]
468"
CLAIMS,0.6688311688311688,"Justification: In Abstract and Introduction, we claim that we investigate the compute-optimal
469"
CLAIMS,0.6699134199134199,"inference: designing models and inference strategies that optimally trade off additional
470"
CLAIMS,0.670995670995671,"inference-time compute for improved performance.
471"
CLAIMS,0.672077922077922,"Guidelines:
472"
CLAIMS,0.6731601731601732,"• The answer NA means that the abstract and introduction do not include the claims
473"
CLAIMS,0.6742424242424242,"made in the paper.
474"
CLAIMS,0.6753246753246753,"• The abstract and/or introduction should clearly state the claims made, including the
475"
CLAIMS,0.6764069264069265,"contributions made in the paper and important assumptions and limitations. A No or
476"
CLAIMS,0.6774891774891775,"NA answer to this question will not be perceived well by the reviewers.
477"
CLAIMS,0.6785714285714286,"• The claims made should match theoretical and experimental results, and reflect how
478"
CLAIMS,0.6796536796536796,"much the results can be expected to generalize to other settings.
479"
CLAIMS,0.6807359307359307,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
480"
CLAIMS,0.6818181818181818,"are not attained by the paper.
481"
LIMITATIONS,0.6829004329004329,"2. Limitations
482"
LIMITATIONS,0.683982683982684,"Question: Does the paper discuss the limitations of the work performed by the authors?
483"
LIMITATIONS,0.685064935064935,"Answer: [Yes]
484"
LIMITATIONS,0.6861471861471862,"Justification: The discussion is in the last section of the main paper.
485"
LIMITATIONS,0.6872294372294372,"Guidelines:
486"
LIMITATIONS,0.6883116883116883,"• The answer NA means that the paper has no limitation while the answer No means that
487"
LIMITATIONS,0.6893939393939394,"the paper has limitations, but those are not discussed in the paper.
488"
LIMITATIONS,0.6904761904761905,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
489"
LIMITATIONS,0.6915584415584416,"• The paper should point out any strong assumptions and how robust the results are to
490"
LIMITATIONS,0.6926406926406926,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
491"
LIMITATIONS,0.6937229437229437,"model well-specification, asymptotic approximations only holding locally). The authors
492"
LIMITATIONS,0.6948051948051948,"should reflect on how these assumptions might be violated in practice and what the
493"
LIMITATIONS,0.6958874458874459,"implications would be.
494"
LIMITATIONS,0.696969696969697,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
495"
LIMITATIONS,0.698051948051948,"only tested on a few datasets or with a few runs. In general, empirical results often
496"
LIMITATIONS,0.6991341991341992,"depend on implicit assumptions, which should be articulated.
497"
LIMITATIONS,0.7002164502164502,"• The authors should reflect on the factors that influence the performance of the approach.
498"
LIMITATIONS,0.7012987012987013,"For example, a facial recognition algorithm may perform poorly when image resolution
499"
LIMITATIONS,0.7023809523809523,"is low or images are taken in low lighting. Or a speech-to-text system might not be
500"
LIMITATIONS,0.7034632034632035,"used reliably to provide closed captions for online lectures because it fails to handle
501"
LIMITATIONS,0.7045454545454546,"technical jargon.
502"
LIMITATIONS,0.7056277056277056,"• The authors should discuss the computational efficiency of the proposed algorithms
503"
LIMITATIONS,0.7067099567099567,"and how they scale with dataset size.
504"
LIMITATIONS,0.7077922077922078,"• If applicable, the authors should discuss possible limitations of their approach to
505"
LIMITATIONS,0.7088744588744589,"address problems of privacy and fairness.
506"
LIMITATIONS,0.70995670995671,"• While the authors might fear that complete honesty about limitations might be used by
507"
LIMITATIONS,0.711038961038961,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
508"
LIMITATIONS,0.7121212121212122,"limitations that aren’t acknowledged in the paper. The authors should use their best
509"
LIMITATIONS,0.7132034632034632,"judgment and recognize that individual actions in favor of transparency play an impor-
510"
LIMITATIONS,0.7142857142857143,"tant role in developing norms that preserve the integrity of the community. Reviewers
511"
LIMITATIONS,0.7153679653679653,"will be specifically instructed to not penalize honesty concerning limitations.
512"
THEORY ASSUMPTIONS AND PROOFS,0.7164502164502164,"3. Theory Assumptions and Proofs
513"
THEORY ASSUMPTIONS AND PROOFS,0.7175324675324676,"Question: For each theoretical result, does the paper provide the full set of assumptions and
514"
THEORY ASSUMPTIONS AND PROOFS,0.7186147186147186,"a complete (and correct) proof?
515"
THEORY ASSUMPTIONS AND PROOFS,0.7196969696969697,"Answer: [Yes]
516"
THEORY ASSUMPTIONS AND PROOFS,0.7207792207792207,"Justification: It’s in Appendix.
517"
THEORY ASSUMPTIONS AND PROOFS,0.7218614718614719,"Guidelines:
518"
THEORY ASSUMPTIONS AND PROOFS,0.7229437229437229,"• The answer NA means that the paper does not include theoretical results.
519"
THEORY ASSUMPTIONS AND PROOFS,0.724025974025974,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
520"
THEORY ASSUMPTIONS AND PROOFS,0.7251082251082251,"referenced.
521"
THEORY ASSUMPTIONS AND PROOFS,0.7261904761904762,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
522"
THEORY ASSUMPTIONS AND PROOFS,0.7272727272727273,"• The proofs can either appear in the main paper or the supplemental material, but if
523"
THEORY ASSUMPTIONS AND PROOFS,0.7283549783549783,"they appear in the supplemental material, the authors are encouraged to provide a short
524"
THEORY ASSUMPTIONS AND PROOFS,0.7294372294372294,"proof sketch to provide intuition.
525"
THEORY ASSUMPTIONS AND PROOFS,0.7305194805194806,"• Inversely, any informal proof provided in the core of the paper should be complemented
526"
THEORY ASSUMPTIONS AND PROOFS,0.7316017316017316,"by formal proofs provided in appendix or supplemental material.
527"
THEORY ASSUMPTIONS AND PROOFS,0.7326839826839827,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7337662337662337,"4. Experimental Result Reproducibility
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7348484848484849,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7359307359307359,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.737012987012987,"of the paper (regardless of whether the code and data are provided or not)?
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7380952380952381,"Answer: [Yes]
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7391774891774892,"Justification: We introduce our method in Section 3 and the hyperparameters are introduced
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7402597402597403,"in the Appendix.
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7413419913419913,"Guidelines:
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7424242424242424,"• The answer NA means that the paper does not include experiments.
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7435064935064936,"• If the paper includes experiments, a No answer to this question will not be perceived
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7445887445887446,"well by the reviewers: Making the paper reproducible is important, regardless of
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7456709956709957,"whether the code and data are provided or not.
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7467532467532467,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7478354978354979,"to make their results reproducible or verifiable.
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7489177489177489,"• Depending on the contribution, reproducibility can be accomplished in various ways.
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75,"For example, if the contribution is a novel architecture, describing the architecture fully
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7510822510822511,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7521645021645021,"be necessary to either make it possible for others to replicate the model with the same
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7532467532467533,"dataset, or provide access to the model. In general. releasing code and data is often
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7543290043290043,"one good way to accomplish this, but reproducibility can also be provided via detailed
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7554112554112554,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7564935064935064,"of a large language model), releasing of a model checkpoint, or other means that are
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7575757575757576,"appropriate to the research performed.
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7586580086580087,"• While NeurIPS does not require releasing code, the conference does require all submis-
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597402597402597,"sions to provide some reasonable avenue for reproducibility, which may depend on the
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7608225108225108,"nature of the contribution. For example
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7619047619047619,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.762987012987013,"to reproduce that algorithm.
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7640692640692641,"(b) If the contribution is primarily a new model architecture, the paper should describe
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7651515151515151,"the architecture clearly and fully.
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7662337662337663,"(c) If the contribution is a new model (e.g., a large language model), then there should
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7673160173160173,"either be a way to access this model for reproducing the results or a way to reproduce
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7683982683982684,"the model (e.g., with an open-source dataset or instructions for how to construct
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7694805194805194,"the dataset).
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7705627705627706,"(d) We recognize that reproducibility may be tricky in some cases, in which case
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7716450216450217,"authors are welcome to describe the particular way they provide for reproducibility.
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7727272727272727,"In the case of closed-source models, it may be that access to the model is limited in
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7738095238095238,"some way (e.g., to registered users), but it should be possible for other researchers
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7748917748917749,"to have some path to reproducing or verifying the results.
567"
OPEN ACCESS TO DATA AND CODE,0.775974025974026,"5. Open access to data and code
568"
OPEN ACCESS TO DATA AND CODE,0.7770562770562771,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
569"
OPEN ACCESS TO DATA AND CODE,0.7781385281385281,"tions to faithfully reproduce the main experimental results, as described in supplemental
570"
OPEN ACCESS TO DATA AND CODE,0.7792207792207793,"material?
571"
OPEN ACCESS TO DATA AND CODE,0.7803030303030303,"Answer: [Yes]
572"
OPEN ACCESS TO DATA AND CODE,0.7813852813852814,"Justification: We only used open-source models in this work. The code will be released.
573"
OPEN ACCESS TO DATA AND CODE,0.7824675324675324,"Guidelines:
574"
OPEN ACCESS TO DATA AND CODE,0.7835497835497836,"• The answer NA means that paper does not include experiments requiring code.
575"
OPEN ACCESS TO DATA AND CODE,0.7846320346320347,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
576"
OPEN ACCESS TO DATA AND CODE,0.7857142857142857,"public/guides/CodeSubmissionPolicy) for more details.
577"
OPEN ACCESS TO DATA AND CODE,0.7867965367965368,"• While we encourage the release of code and data, we understand that this might not be
578"
OPEN ACCESS TO DATA AND CODE,0.7878787878787878,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
579"
OPEN ACCESS TO DATA AND CODE,0.788961038961039,"including code, unless this is central to the contribution (e.g., for a new open-source
580"
OPEN ACCESS TO DATA AND CODE,0.79004329004329,"benchmark).
581"
OPEN ACCESS TO DATA AND CODE,0.7911255411255411,"• The instructions should contain the exact command and environment needed to run to
582"
OPEN ACCESS TO DATA AND CODE,0.7922077922077922,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
583"
OPEN ACCESS TO DATA AND CODE,0.7932900432900433,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
584"
OPEN ACCESS TO DATA AND CODE,0.7943722943722944,"• The authors should provide instructions on data access and preparation, including how
585"
OPEN ACCESS TO DATA AND CODE,0.7954545454545454,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
586"
OPEN ACCESS TO DATA AND CODE,0.7965367965367965,"• The authors should provide scripts to reproduce all experimental results for the new
587"
OPEN ACCESS TO DATA AND CODE,0.7976190476190477,"proposed method and baselines. If only a subset of experiments are reproducible, they
588"
OPEN ACCESS TO DATA AND CODE,0.7987012987012987,"should state which ones are omitted from the script and why.
589"
OPEN ACCESS TO DATA AND CODE,0.7997835497835498,"• At submission time, to preserve anonymity, the authors should release anonymized
590"
OPEN ACCESS TO DATA AND CODE,0.8008658008658008,"versions (if applicable).
591"
OPEN ACCESS TO DATA AND CODE,0.801948051948052,"• Providing as much information as possible in supplemental material (appended to the
592"
OPEN ACCESS TO DATA AND CODE,0.803030303030303,"paper) is recommended, but including URLs to data and code is permitted.
593"
OPEN ACCESS TO DATA AND CODE,0.8041125541125541,"6. Experimental Setting/Details
594"
OPEN ACCESS TO DATA AND CODE,0.8051948051948052,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
595"
OPEN ACCESS TO DATA AND CODE,0.8062770562770563,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
596"
OPEN ACCESS TO DATA AND CODE,0.8073593073593074,"results?
597"
OPEN ACCESS TO DATA AND CODE,0.8084415584415584,"Answer: [Yes]
598"
OPEN ACCESS TO DATA AND CODE,0.8095238095238095,"Justification: We used the standard training and test splits or MATH and GSM8K and report
599"
OPEN ACCESS TO DATA AND CODE,0.8106060606060606,"the hyperparameters in the appendix.
600"
OPEN ACCESS TO DATA AND CODE,0.8116883116883117,"Guidelines:
601"
OPEN ACCESS TO DATA AND CODE,0.8127705627705628,"• The answer NA means that the paper does not include experiments.
602"
OPEN ACCESS TO DATA AND CODE,0.8138528138528138,"• The experimental setting should be presented in the core of the paper to a level of detail
603"
OPEN ACCESS TO DATA AND CODE,0.814935064935065,"that is necessary to appreciate the results and make sense of them.
604"
OPEN ACCESS TO DATA AND CODE,0.816017316017316,"• The full details can be provided either with the code, in appendix, or as supplemental
605"
OPEN ACCESS TO DATA AND CODE,0.8170995670995671,"material.
606"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8181818181818182,"7. Experiment Statistical Significance
607"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8192640692640693,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
608"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8203463203463204,"information about the statistical significance of the experiments?
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8214285714285714,"Answer: [Yes]
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8225108225108225,"Justification: The error bars are included in our figures.
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8235930735930735,"Guidelines:
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8246753246753247,"• The answer NA means that the paper does not include experiments.
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8257575757575758,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8268398268398268,"dence intervals, or statistical significance tests, at least for the experiments that support
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.827922077922078,"the main claims of the paper.
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.829004329004329,"• The factors of variability that the error bars are capturing should be clearly stated (for
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8300865800865801,"example, train/test split, initialization, random drawing of some parameter, or overall
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8311688311688312,"run with given experimental conditions).
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8322510822510822,"• The method for calculating the error bars should be explained (closed form formula,
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8333333333333334,"call to a library function, bootstrap, etc.)
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8344155844155844,"• The assumptions made should be given (e.g., Normally distributed errors).
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8354978354978355,"• It should be clear whether the error bar is the standard deviation or the standard error
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8365800865800865,"of the mean.
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8376623376623377,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8387445887445888,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8398268398268398,"of Normality of errors is not verified.
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8409090909090909,"• For asymmetric distributions, the authors should be careful not to show in tables or
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.841991341991342,"figures symmetric error bars that would yield results that are out of range (e.g. negative
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8430735930735931,"error rates).
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8441558441558441,"• If error bars are reported in tables or plots, The authors should explain in the text how
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8452380952380952,"they were calculated and reference the corresponding figures or tables in the text.
632"
EXPERIMENTS COMPUTE RESOURCES,0.8463203463203464,"8. Experiments Compute Resources
633"
EXPERIMENTS COMPUTE RESOURCES,0.8474025974025974,"Question: For each experiment, does the paper provide sufficient information on the com-
634"
EXPERIMENTS COMPUTE RESOURCES,0.8484848484848485,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
635"
EXPERIMENTS COMPUTE RESOURCES,0.8495670995670995,"the experiments?
636"
EXPERIMENTS COMPUTE RESOURCES,0.8506493506493507,"Answer: [Yes]
637"
EXPERIMENTS COMPUTE RESOURCES,0.8517316017316018,"Justification: All the experiments are conducted on 8× H100 GPUs.
638"
EXPERIMENTS COMPUTE RESOURCES,0.8528138528138528,"Guidelines:
639"
EXPERIMENTS COMPUTE RESOURCES,0.8538961038961039,"• The answer NA means that the paper does not include experiments.
640"
EXPERIMENTS COMPUTE RESOURCES,0.854978354978355,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
641"
EXPERIMENTS COMPUTE RESOURCES,0.8560606060606061,"or cloud provider, including relevant memory and storage.
642"
EXPERIMENTS COMPUTE RESOURCES,0.8571428571428571,"• The paper should provide the amount of compute required for each of the individual
643"
EXPERIMENTS COMPUTE RESOURCES,0.8582251082251082,"experimental runs as well as estimate the total compute.
644"
EXPERIMENTS COMPUTE RESOURCES,0.8593073593073594,"• The paper should disclose whether the full research project required more compute
645"
EXPERIMENTS COMPUTE RESOURCES,0.8603896103896104,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
646"
EXPERIMENTS COMPUTE RESOURCES,0.8614718614718615,"didn’t make it into the paper).
647"
CODE OF ETHICS,0.8625541125541125,"9. Code Of Ethics
648"
CODE OF ETHICS,0.8636363636363636,"Question: Does the research conducted in the paper conform, in every respect, with the
649"
CODE OF ETHICS,0.8647186147186147,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
650"
CODE OF ETHICS,0.8658008658008658,"Answer: [Yes]
651"
CODE OF ETHICS,0.8668831168831169,"Justification: Yes, we conform with the NeurIPS Code of Ethics.
652"
CODE OF ETHICS,0.8679653679653679,"Guidelines:
653"
CODE OF ETHICS,0.8690476190476191,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
654"
CODE OF ETHICS,0.8701298701298701,"• If the authors answer No, they should explain the special circumstances that require a
655"
CODE OF ETHICS,0.8712121212121212,"deviation from the Code of Ethics.
656"
CODE OF ETHICS,0.8722943722943723,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
657"
CODE OF ETHICS,0.8733766233766234,"eration due to laws or regulations in their jurisdiction).
658"
BROADER IMPACTS,0.8744588744588745,"10. Broader Impacts
659"
BROADER IMPACTS,0.8755411255411255,"Question: Does the paper discuss both potential positive societal impacts and negative
660"
BROADER IMPACTS,0.8766233766233766,"societal impacts of the work performed?
661"
BROADER IMPACTS,0.8777056277056277,"Answer: [NA]
662"
BROADER IMPACTS,0.8787878787878788,"Justification: We do not find significant positive societal impacts and negative societal
663"
BROADER IMPACTS,0.8798701298701299,"impacts of our work.
664"
BROADER IMPACTS,0.8809523809523809,"Guidelines:
665"
BROADER IMPACTS,0.8820346320346321,"• The answer NA means that there is no societal impact of the work performed.
666"
BROADER IMPACTS,0.8831168831168831,"• If the authors answer NA or No, they should explain why their work has no societal
667"
BROADER IMPACTS,0.8841991341991342,"impact or why the paper does not address societal impact.
668"
BROADER IMPACTS,0.8852813852813853,"• Examples of negative societal impacts include potential malicious or unintended uses
669"
BROADER IMPACTS,0.8863636363636364,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
670"
BROADER IMPACTS,0.8874458874458875,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
671"
BROADER IMPACTS,0.8885281385281385,"groups), privacy considerations, and security considerations.
672"
BROADER IMPACTS,0.8896103896103896,"• The conference expects that many papers will be foundational research and not tied
673"
BROADER IMPACTS,0.8906926406926406,"to particular applications, let alone deployments. However, if there is a direct path to
674"
BROADER IMPACTS,0.8917748917748918,"any negative applications, the authors should point it out. For example, it is legitimate
675"
BROADER IMPACTS,0.8928571428571429,"to point out that an improvement in the quality of generative models could be used to
676"
BROADER IMPACTS,0.8939393939393939,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
677"
BROADER IMPACTS,0.895021645021645,"that a generic algorithm for optimizing neural networks could enable people to train
678"
BROADER IMPACTS,0.8961038961038961,"models that generate Deepfakes faster.
679"
BROADER IMPACTS,0.8971861471861472,"• The authors should consider possible harms that could arise when the technology is
680"
BROADER IMPACTS,0.8982683982683982,"being used as intended and functioning correctly, harms that could arise when the
681"
BROADER IMPACTS,0.8993506493506493,"technology is being used as intended but gives incorrect results, and harms following
682"
BROADER IMPACTS,0.9004329004329005,"from (intentional or unintentional) misuse of the technology.
683"
BROADER IMPACTS,0.9015151515151515,"• If there are negative societal impacts, the authors could also discuss possible mitigation
684"
BROADER IMPACTS,0.9025974025974026,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
685"
BROADER IMPACTS,0.9036796536796536,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
686"
BROADER IMPACTS,0.9047619047619048,"feedback over time, improving the efficiency and accessibility of ML).
687"
SAFEGUARDS,0.9058441558441559,"11. Safeguards
688"
SAFEGUARDS,0.9069264069264069,"Question: Does the paper describe safeguards that have been put in place for responsible
689"
SAFEGUARDS,0.908008658008658,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
690"
SAFEGUARDS,0.9090909090909091,"image generators, or scraped datasets)?
691"
SAFEGUARDS,0.9101731601731602,"Answer: [NA]
692"
SAFEGUARDS,0.9112554112554112,"Justification:
693"
SAFEGUARDS,0.9123376623376623,"Guidelines:
694"
SAFEGUARDS,0.9134199134199135,"• The answer NA means that the paper poses no such risks.
695"
SAFEGUARDS,0.9145021645021645,"• Released models that have a high risk for misuse or dual-use should be released with
696"
SAFEGUARDS,0.9155844155844156,"necessary safeguards to allow for controlled use of the model, for example by requiring
697"
SAFEGUARDS,0.9166666666666666,"that users adhere to usage guidelines or restrictions to access the model or implementing
698"
SAFEGUARDS,0.9177489177489178,"safety filters.
699"
SAFEGUARDS,0.9188311688311688,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
700"
SAFEGUARDS,0.9199134199134199,"should describe how they avoided releasing unsafe images.
701"
SAFEGUARDS,0.920995670995671,"• We recognize that providing effective safeguards is challenging, and many papers do
702"
SAFEGUARDS,0.922077922077922,"not require this, but we encourage authors to take this into account and make a best
703"
SAFEGUARDS,0.9231601731601732,"faith effort.
704"
LICENSES FOR EXISTING ASSETS,0.9242424242424242,"12. Licenses for existing assets
705"
LICENSES FOR EXISTING ASSETS,0.9253246753246753,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
706"
LICENSES FOR EXISTING ASSETS,0.9264069264069265,"the paper, properly credited and are the license and terms of use explicitly mentioned and
707"
LICENSES FOR EXISTING ASSETS,0.9274891774891775,"properly respected?
708"
LICENSES FOR EXISTING ASSETS,0.9285714285714286,"Answer: [Yes]
709"
LICENSES FOR EXISTING ASSETS,0.9296536796536796,"Justification: We use the proper citations.
710"
LICENSES FOR EXISTING ASSETS,0.9307359307359307,"Guidelines:
711"
LICENSES FOR EXISTING ASSETS,0.9318181818181818,"• The answer NA means that the paper does not use existing assets.
712"
LICENSES FOR EXISTING ASSETS,0.9329004329004329,"• The authors should cite the original paper that produced the code package or dataset.
713"
LICENSES FOR EXISTING ASSETS,0.933982683982684,"• The authors should state which version of the asset is used and, if possible, include a
714"
LICENSES FOR EXISTING ASSETS,0.935064935064935,"URL.
715"
LICENSES FOR EXISTING ASSETS,0.9361471861471862,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
716"
LICENSES FOR EXISTING ASSETS,0.9372294372294372,"• For scraped data from a particular source (e.g., website), the copyright and terms of
717"
LICENSES FOR EXISTING ASSETS,0.9383116883116883,"service of that source should be provided.
718"
LICENSES FOR EXISTING ASSETS,0.9393939393939394,"• If assets are released, the license, copyright information, and terms of use in the
719"
LICENSES FOR EXISTING ASSETS,0.9404761904761905,"package should be provided. For popular datasets, paperswithcode.com/datasets
720"
LICENSES FOR EXISTING ASSETS,0.9415584415584416,"has curated licenses for some datasets. Their licensing guide can help determine the
721"
LICENSES FOR EXISTING ASSETS,0.9426406926406926,"license of a dataset.
722"
LICENSES FOR EXISTING ASSETS,0.9437229437229437,"• For existing datasets that are re-packaged, both the original license and the license of
723"
LICENSES FOR EXISTING ASSETS,0.9448051948051948,"the derived asset (if it has changed) should be provided.
724"
LICENSES FOR EXISTING ASSETS,0.9458874458874459,"• If this information is not available online, the authors are encouraged to reach out to
725"
LICENSES FOR EXISTING ASSETS,0.946969696969697,"the asset’s creators.
726"
NEW ASSETS,0.948051948051948,"13. New Assets
727"
NEW ASSETS,0.9491341991341992,"Question: Are new assets introduced in the paper well documented and is the documentation
728"
NEW ASSETS,0.9502164502164502,"provided alongside the assets?
729"
NEW ASSETS,0.9512987012987013,"Answer: [Yes]
730"
NEW ASSETS,0.9523809523809523,"Justification: We use the proper citations.
731"
NEW ASSETS,0.9534632034632035,"Guidelines:
732"
NEW ASSETS,0.9545454545454546,"• The answer NA means that the paper does not release new assets.
733"
NEW ASSETS,0.9556277056277056,"• Researchers should communicate the details of the dataset/code/model as part of their
734"
NEW ASSETS,0.9567099567099567,"submissions via structured templates. This includes details about training, license,
735"
NEW ASSETS,0.9577922077922078,"limitations, etc.
736"
NEW ASSETS,0.9588744588744589,"• The paper should discuss whether and how consent was obtained from people whose
737"
NEW ASSETS,0.95995670995671,"asset is used.
738"
NEW ASSETS,0.961038961038961,"• At submission time, remember to anonymize your assets (if applicable). You can either
739"
NEW ASSETS,0.9621212121212122,"create an anonymized URL or include an anonymized zip file.
740"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9632034632034632,"14. Crowdsourcing and Research with Human Subjects
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9642857142857143,"Question: For crowdsourcing experiments and research with human subjects, does the paper
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9653679653679653,"include the full text of instructions given to participants and screenshots, if applicable, as
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9664502164502164,"well as details about compensation (if any)?
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9675324675324676,"Answer: [NA]
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9686147186147186,"Justification: No crowdsourcing experiments are used.
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9696969696969697,"Guidelines:
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9707792207792207,"• The answer NA means that the paper does not involve crowdsourcing nor research with
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9718614718614719,"human subjects.
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9729437229437229,"• Including this information in the supplemental material is fine, but if the main contribu-
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974025974025974,"tion of the paper involves human subjects, then as much detail as possible should be
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751082251082251,"included in the main paper.
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761904761904762,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772727272727273,"or other labor should be paid at least the minimum wage in the country of the data
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783549783549783,"collector.
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794372294372294,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805194805194806,"Subjects
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816017316017316,"Question: Does the paper describe potential risks incurred by study participants, whether
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826839826839827,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837662337662337,"approvals (or an equivalent approval/review based on the requirements of your country or
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848484848484849,"institution) were obtained?
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9859307359307359,"Answer: [NA]
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987012987012987,"Justification: No human subjects are involved.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880952380952381,"Guidelines:
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891774891774892,"• The answer NA means that the paper does not involve crowdsourcing nor research with
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902597402597403,"human subjects.
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913419913419913,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924242424242424,"may be required for any human subjects research. If you obtained IRB approval, you
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935064935064936,"should clearly state this in the paper.
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945887445887446,"• We recognize that the procedures for this may vary significantly between institutions
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956709956709957,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967532467532467,"guidelines for their institution.
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978354978354979,"• For initial submissions, do not include any information that would break anonymity (if
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989177489177489,"applicable), such as the institution conducting the review.
774"
