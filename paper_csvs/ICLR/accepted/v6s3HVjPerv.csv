Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029069767441860465,"A variety of methods exist to explain image classiﬁcation models. However, it
remains unclear whether they provide any beneﬁt to users over simply compar-
ing various inputs and the model’s respective predictions. We conducted a user
study (N=240) to test how such a baseline explanation technique performs against
concept-based and counterfactual explanations. To this end, we contribute a syn-
thetic dataset generator capable of biasing individual attributes and quantifying
their relevance to the model. In a study, we assess if participants can identify the
relevant set of attributes compared to the ground-truth. Our results show that the
baseline outperformed concept-based explanations. Counterfactual explanations
from an invertible neural network performed similarly as the baseline. Still, they
allowed users to identify some attributes more accurately. Our results highlight the
importance of measuring how well users can reason about biases of a model, rather
than solely relying on technical evaluations or proxy tasks. We open-source our
study and dataset so it can serve as a blue-print for future studies."
INTRODUCTION,0.005813953488372093,"1
INTRODUCTION"
INTRODUCTION,0.00872093023255814,"Deep neural networks have been widely adopted in many domains. Yet, for some applications, their
use may be limited by how little we understand which features are relevant. Whether engineer or
user, insurance company, or regulatory body; all require reliable information about what the model
has learned or why the model provides a certain output. Numerous methods have been proposed to
explain deep neural networks (Gilpin et al., 2018; Molnar et al., 2020)."
INTRODUCTION,0.011627906976744186,"Ultimately, to evaluate whether such explanations are helpful, we need user studies (Doshi-Velez
& Kim, 2017; Wortman Vaughan & Wallach, 2020). In fact, some studies provided evidence that
interpretable ML techniques may be helpful to ﬁnd biases or spurious correlations (Ribeiro et al.,
2016b; Adebayo et al., 2020a). However, a substantial body of work shows that they may not be as
helpful as claimed (Kaur et al., 2020; Alqaraawi et al., 2020; Chu et al., 2020; Shen & Huang, 2020).
Consequently, it seems that in real-world applications, biases are often found by simply inspecting
the model’s predictions rather than applying interpretable ML. A recent example is the Twitter image
cropping algorithm: it was the users who discovered that it favored white people over people of color
(Yee et al., 2021). In this work, we ask: do modern interpretability methods enable users to discover
biases better than by simply inspecting input/output pairs?"
INTRODUCTION,0.014534883720930232,"To investigate this question empirically, we ﬁrst propose TWO4TWO: a synthetic dataset depicting
two abstract animals. Its data-generating factors can be correlated with the binary target class, thereby
creating arbitrarily strong biases. We designed a baseline explanation technique for bias discovery
using only the model’s output: input images are arranged in a grid grouped by the model’s logit
predictions. This design allows a user to inspect all attributes that potentially predict the target class."
INTRODUCTION,0.01744186046511628,"In an initial user study (N=50), we validated that participants were struggling to ﬁnd both biases
contained in our dataset using this technique. Hence, more elaborate methods can improve over
the baseline on this dataset. In the main study (N=240), we compared the baseline against two
state-of-the-art explanations: automatically-discovered concepts and counterfactual interpolations
generated with an invertible neural network."
INTRODUCTION,0.020348837209302327,Published as a conference paper at ICLR 2022
INTRODUCTION,0.023255813953488372,"(a) Baseline
(b) Invertible Neural Networks
(c) Concepts (Zhang et al., 2021)"
INTRODUCTION,0.02616279069767442,"Figure 1: We tested whether users can identify the class-relevant features of images showing two types
of animals. We biased attributes like the animal’s color to be predictive of the class and investigated
whether explanation techniques enabled users to discover these biases. We tested a simple baseline (a)
which shows random samples grouped by the model’s output logit, counterfactual samples generated
by an invertible neural network (b), and automatically discovered concepts (c). A participant viewed
only one of the above conditions."
INTRODUCTION,0.029069767441860465,"We found that none of these explanations outperformed the baseline, even though some features were
identiﬁed more accurately with counterfactuals. The textual justiﬁcations of participants revealed
several usability issues in all methods. This highlights the necessity to validate any claims about the
beneﬁts of explanation techniques in user studies."
INTRODUCTION,0.03197674418604651,This work represents substantial empirical novelty and signiﬁcance in the ﬁeld of interpretable ML:
INTRODUCTION,0.03488372093023256,"• The TWO4TWO dataset generator provides control over features and biases. It is designed
speciﬁcally for human subject studies and to challenge existing interpretability approaches,
• Methods to quantify ground-truth feature importance when the data-generating process is
known,
• A study design that provides a uniﬁed approach to evaluating interpretable vision methods
on the task of bias discovery. It is suitable for lay-users and includes several measures to
ensure high-quality crowd-sourced responses,
• A strong and simple baseline explanation technique using only the model output, which we
propose as a benchmark for future studies,
• We open-source our dataset, explanation techniques, model, study design, including instruc-
tions and videos to support replicating our results as well as adapting our design to other
explanation techniques."
RELATED WORK,0.0377906976744186,"2
RELATED WORK"
RELATED WORK,0.040697674418604654,"Interpretable ML for Vision
Different explanation approaches have been proposed: saliency maps
(Bach et al., 2015; Ancona et al., 2018; Sundararajan et al., 2017), example-based explanations (Cai
et al., 2019), counterfactual examples (Singla et al., 2020), activation-concept approaches (Kim et al.,
2018), or models with built-in interpretability (Chen et al., 2019; Brendel & Bethge, 2018). For a
detailed review about the ﬁeld, we refer to (Gilpin et al., 2018; Molnar et al., 2020)."
RELATED WORK,0.0436046511627907,"Our work focuses on counterfactual explanations and automatically-discovered concepts. Counterfac-
tual explanations are samples that change the model output, e.g., ﬂip the output class (Wachter et al.,
2018). We generated counterfactuals with invertible neural networks (INNs) (Jacobsen et al., 2018;
Kingma & Dhariwal, 2018). This approach has recently gained momentum (Hvilshøj et al., 2021;
Dombrowski et al., 2021; Mackowiak et al., 2020). Previous works have also used GANs and VAEs
for counterfactual generation (Goyal et al., 2019; Mertes et al., 2020; Sauer & Geiger, 2021; Singla
et al., 2020; Liu et al., 2019; Baumgartner et al., 2018; Chang et al., 2019). The main advantage of
using INNs for counterfactuals is that the generative function is perfectly aligned with the forward
function, as an analytic inverse exists."
RELATED WORK,0.046511627906976744,"Concepts represent abstract properties, which can be used to explain a model. For example, the
classiﬁcation of an image as ”zebra” could be explained by a pronounced similarity to the ”stripe”
concept. This similarity is determined by the dot product of the network’s internal activations with a
concept vector. TCAV (Kim et al., 2018) required manually deﬁned concepts. Recent works proposed
to discover concepts automatically (Ghorbani et al., 2019; Zhang et al., 2021)."
RELATED WORK,0.04941860465116279,Published as a conference paper at ICLR 2022
RELATED WORK,0.05232558139534884,"Animal Color
Block Shape"
RELATED WORK,0.055232558139534885,"Body Posture
Background
Legs stretched out
Head peeking out"
RELATED WORK,0.05813953488372093,"Peeky
Stretchy"
RELATED WORK,0.061046511627906974,"Figure 2: The left panel depicts the main difference between Peeky and Stretchy: the legs’ position.
While Peeky shows one pair of legs moved inwards, Stretchy’s legs are moved outwards. TWO4TWO
offers different attributes: animal color, background color, the shape of the blocks and the animal’s
body posture. All of which can be controlled and biased separately."
RELATED WORK,0.06395348837209303,"User Studies for Interpretability
Previous works with the task of bias discovery have mainly
evaluated saliency maps and used datasets with a single, simple bias, e.g. background Adebayo
et al. (2020a); Ribeiro et al. (2016a) or image watermarks Kim et al. (2018). User studies for
concept-based methods tested only the accessibility of the explanations by asking users to assign
images to a concept (Zhang et al., 2021; Ghorbani et al., 2019). Counterfactual explanations have
been evaluated by Mertes et al. (2020) on a forward-prediction task. We thus believe that we are the
ﬁrst to extensively test counterfactual-based and concept-based explanations on bias discovery using
a challenging dataset. Recently, a study on exemplary-based explanations focused on understanding
internal activations of a neural network (Borowski et al., 2020). It showed that for this task, examples
could be more beneﬁcial than complex feature visualizations (Olah et al., 2017). Similarly, there is
evidence that participants often rely on model predictions rather than on explanations (Alqaraawi
et al., 2020; Adebayo et al., 2020a)."
RELATED WORK,0.06686046511627906,"Synthetic Datasets for Interpretable Vision
Datasets with known ground-truth biases have been
proposed before. BAM is an artiﬁcial dataset (Yang & Kim, 2019) where spurious background
correlations are introduced by pasting segmented objects on different textures, e.g. dogs on bamboo
forests. However, the resulting images are unsuitable for user studies as they look artiﬁcial and make
it easy for participants to suspect that the background is important. Additionally, it would be difﬁcult
to introduce more than one bias. A limitation that also the synthetic dataset in (Chen et al., 2018)
shares. The synthetic dataset created by Arras et al. (2021) created a dataset to technically evaluate
saliency methods on a visual question answering task technically. TWO4TWO is the ﬁrst dataset
designed explicitly for human subject evaluations. To the best of our knowledge, we provide the ﬁrst
uniﬁed approach to evaluate interpretable vision on a bias-discovery task."
RELATED WORK,0.06976744186046512,"3
TWO4TWO: DATASETS WITH KNOWN FEATURE IMPORTANCE"
RELATED WORK,0.07267441860465117,"Dataset Description
Datasets generated with TWO4TWO consist of two abstract animal classes,
called Peeky and Stretchy. Both consist of eight blocks: four for the spine and four for the legs. For
both animals, one pair of legs is always at an extended position. The other pair moves parallel to the
spine inward and outward. The attribute legs’ position, a scalar in [0,1], controls the position. At a
value of 0.5, the pair of legs are at the same vertical position as the last block of the spine. Peekies
have a leg position ≤0.52 which means legs are moved mostly inwards to the body center. In the
same fashion, Stretchies are extended outwards, legs’ position ≥0.48. We added some ambiguity
to ensure a model has an incentive to use possible biases. Therefore, Peekies and Stretchies are
equally likely for a legs’ position between 0.48 and 0.52. It is also difﬁcult for humans to tell if the
legs are outward or inwards in this range. Besides the legs’ position, the dataset has the following
parameters: body posture (bending and three rotation angles), position, animal color (from red to
blue), blocks’ shape (from cubes to spheres), and background color (from red to blue). Each can be
changed arbitrarily and continuously (See Appendix Table 5)."
RELATED WORK,0.0755813953488372,"When designing the dataset, we wanted to ensure that (1) participants can become experts within a
few minutes of training, (2) it allows for the creation of multiple biases that are difﬁcult to ﬁnd, and
(3) that it provides a challenge for existing interpretability methods. Goal (1) is met as participants
can be instructed using only a few examples (see the tutorial video in Appendix C). The high number
of controllable attributes achieve Goal (2). We biased the attributes such that they do not stand out,
which we validated in the ﬁrst user study. Goal (3) is met by spatially overlapping attributes and long-"
RELATED WORK,0.07848837209302326,Published as a conference paper at ICLR 2022
RELATED WORK,0.08139534883720931,"0
1
Legs' Position 0.0 0.5 1.0"
RELATED WORK,0.08430232558139535,Background
RELATED WORK,0.0872093023255814,"0
1
Legs' Position 0.0 0.5 1.0 Shape"
RELATED WORK,0.09011627906976744,"0
1
Legs' Position 0.0 0.5 1.0 Color"
RELATED WORK,0.09302325581395349,"Figure 3: The joint distributions of legs’ position and the attributes background (left), shape (middle),
and color (right). Datapoints are yellow for Peekies and blue for Stretchies. The background is not
biased. The shape is biased for legs’ position lower than (0.45) or greater (0.55), but is uniform in
the center. The color contains additional predictive information about the target class, as it allow
to discriminate between Peeky and Stretchy where the legs’ position overlaps. However, for more
extreme arms’ positions the color is uniform and not biased."
RELATED WORK,0.09593023255813954,"range image dependencies. Spatially overlapping attributes, like color and shape, directly challenge
saliency map explanations. Long-range image dependencies, like the legs’ positions relative to the
spine, can not be explained when analyzing patches separately as done in (Chen et al., 2019; Brendel
& Bethge, 2018). Both properties are common in real-world datasets: For example, race and gender
in facial datasets are encoded by spatially overlapping features. Long-range image dependencies are
particularly relevant for pose estimation and visual reasoning (Johnson et al., 2017)."
RELATED WORK,0.09883720930232558,"Introducing Biases
For our studies’ dataset, we sampled the block’s shape in a non-predictive
biased fashion. This means that for legs’ positions that clearly showed a Peeky [0, 0.45] most blocks
were rather cubic, while for legs’ positions that clearly showed a Stretchy [0.55, 1] most blocks were
rather round. However, for the legs’ positions between [0.45, 0.55] the blocks shape was uniformly
distributed. In particular, in the even narrower interval [0.48, 0.52] where a classiﬁer can only be as
good as random guessing, the block’s shape does not provide any additional information about the
target class. In Figure 3, we show the joint distribution of the block’s shape and legs’ position."
RELATED WORK,0.10174418604651163,"We sampled the animals’ color to be predictive for the target class. At the small interval where the
legs overlap [0.48; 0.52], we distributed the animal color to provide additional class information.
Stretchies were more likely to be red, and Peekies were more likely to be blue. Outside of this
centered interval, the color gradually became uniformly distributed (see Figure 3). Hence, color
was more equally distributed than the shape, making the color bias harder to detect visually. The
remaining attributes, background color and body posture, were sampled independently of the class,
and we expected our model to ignore them."
RELATED WORK,0.10465116279069768,"Measuring Ground-Truth Feature Importance
Even if a dataset contains biases, it is unclear
how relevant they will be to a neural network after training. Feature importance also depends on the
network architecture, the optimization process, and even the weight initialization. As TWO4TWO
allows us to change any parameter in isolation, we can directly compare the model prediction between
two images that differ in only one parameter. For these two images, we measured both the median
absolute logit change and also for how many samples the predicted class was ﬂipped. Both measures
quantify how inﬂuential each parameter is (see Table 1). As expected, the legs’ position had a strong
inﬂuence on the prediction. The model relied more on animal color than on the blocks’ shape, which
is expected as the color contains additional information about the class. Surprisingly, the prediction
ﬂip for unrelated attributes such as background was only slightly lower than for blocks’ shape."
RELATED WORK,0.10755813953488372,"To analyze this further, we calculated a linear ﬁt for each parameter change to the logit change.
We reported the coefﬁcient of determination R2, which indicates how much of the variance in
the prediction can be explained linearly by the analyzed property. While the unrelated properties
sometimes ﬂip a prediction, the direction of that ﬂip is random (R2 ≈0). In contrast, the biased
parameters inﬂuence predictions in a directed fashion, with animal color (R2=0.751) being clearly
more directed than blocks’ shape (R2=0.307)."
MODEL AND EVALUATED METHODS,0.11046511627906977,"4
MODEL AND EVALUATED METHODS"
MODEL AND EVALUATED METHODS,0.11337209302325581,"As discussed in section 3, TWO4TWO was designed to challenge existing interpretability methods,
e.g., saliency map explanations and patch-based models. We selected two methods that might provide"
MODEL AND EVALUATED METHODS,0.11627906976744186,Published as a conference paper at ICLR 2022
MODEL AND EVALUATED METHODS,0.11918604651162791,"Table 1: Importance of the data generating factors to the model’s prediction. Prediction Flip quantiﬁes
how often the model’s prediction changes the sign when changing the attribute. The Mean Logit
Change reports the median of the absolute change in logit values. The R2 score is calculated on an
ordinary least squares from the changes of each factor to the changes in the model’s logit. For more
attributes, see Appendix Table 3."
MODEL AND EVALUATED METHODS,0.12209302325581395,"Factor
Prediction Flip [%]
Median Logit Change
R2"
MODEL AND EVALUATED METHODS,0.125,"Legs’ Position
41.680
2.493
0.933
Color
7.080
0.886
0.751
Shape
3.920
0.577
0.307
Background
2.640
0.523
0.006
Rotation Yaw
3.480
0.669
0.001
Bending
3.640
0.605
0.000"
MODEL AND EVALUATED METHODS,0.12790697674418605,"the user with the necessary information: counterfactuals generated with an invertible neural network
(INN) and concept-based explanations (Zhang et al., 2021)."
MODEL AND EVALUATED METHODS,0.1308139534883721,"INN Counterfactuals
We trained an INN using both a supervised and an unsupervised objective
(Dinh et al., 2016; 2015). To predict the target class, the model ﬁrst applies the forward function ϕ to
map a data point x to a feature vector z = ϕ(x). Then, a linear classiﬁer takes those features z and
predicts the logit score f(x) = wT z + b. Any input can be reconstructed from the feature vector by
applying the inverse function x = ϕ−1(z). The model has a test accuracy of 96.7%. Further details
can be found in Appendix A.2. The baseline and concept techniques are also applied to this model."
MODEL AND EVALUATED METHODS,0.13372093023255813,"To create a counterfactual example ˜x for a data point x, we can exploit the linearity of the classiﬁer.
Moving along the weight vector w, i.e., adding w to the features z, changes the model’s prediction.
By controlling the step size with a scalar α, we can directly quantify the change in the logit value
∆y = αwT w. The modiﬁed feature vector z + αw can be inverted back to the input domain,
resulting in a counterfactual ˜x = ϕ−1(z + αw) which visualizes the changes introduced by a step
αw in z-space. The INN’s explanations are visualized in a grid where each row shows a single
counterfactual interpolation (see Figure 1b)."
MODEL AND EVALUATED METHODS,0.13662790697674418,"Automatically-discovered Concepts
We adapted the NMF approach of Zhang et al. (2021) to our
speciﬁc network architecture. Because the network’s internal representations also contain negative
values, we used matrix factorization instead of NMF. We generated the concepts using layer 342 (from
a total of 641 layers). The layer has a feature map resolution of 8x8. This choice represents a trade-off
between enough spatial resolution and high-level information. We ran the matrix factorization with
10 components and selected the ﬁve components that correlated most with the logit score (r is in the
range [0.21, 0.34])."
MODEL AND EVALUATED METHODS,0.13953488372093023,"Our presentation of concept-based explanations was very similar to (Zhang et al., 2021): we visualized
concepts with ﬁve exemplary images per row and highlighted regions corresponding to a concept.
Since our classiﬁer is binary, a negative contribution for Stretchy actually means a positive contribution
for Peeky. Hence, we could have characterized a concept as more Peeky and more Stretchy, to make
the design similar to the other two explanation techniques. However, as the concepts did not strongly
correlate with the model’s output, presenting them as class-related could confuse participants: a
more Peeky column would have contained some images showing Stretchies and vice versa. Thus,
we presented them separately in two consecutive rows (See Figure 1c). Presenting concepts in this
fashion gives them a fair chance in the study because participants rated the relevance of each attribute
for the model rather than for each class separately."
HUMAN SUBJECT STUDY,0.14244186046511628,"5
HUMAN SUBJECT STUDY"
HUMAN SUBJECT STUDY,0.14534883720930233,"We share the view of Doshi-Velez & Kim (2017) and Wortman Vaughan & Wallach (2020) that
user-testing of explanation techniques is a crucial but challenging endeavor. As our second main
contribution, we propose and conduct a user study based on the Two4Two dataset which can act as a
blue-print for future investigations. Our design has been iterated in over ten pilot studies and proposes
solutions to common problems that arise when evaluating explanation techniques on crowd-sourcing
platforms with lay participants."
HUMAN SUBJECT STUDY,0.14825581395348839,Published as a conference paper at ICLR 2022
DESIGN CONSIDERATIONS,0.1511627906976744,"5.1
DESIGN CONSIDERATIONS"
DESIGN CONSIDERATIONS,0.15406976744186046,"Data without Prior Domain Knowlege
We speciﬁcally designed the Two4Two dataset to avoid
overburdening participants, as might be the case with other types of data. Within a few minutes,
participants can easily become domain experts. Since the data is unknown to them prior to the study,
we avoid introducing any prior domain knowledge as a confounding factor, which can be an issue
(Alqaraawi et al., 2020)."
DESIGN CONSIDERATIONS,0.1569767441860465,"Manageable but not Oversimpliﬁed Tasks
We propose the task of bias-discovery: participants
had to rate features as either relevant or irrelevant to a model. The task directly reﬂects users’
perception of feature importance. Furthermore, bias-discovery has the advantage of being suitable
for lay participants. At the same time, it is also grounded in the model’s behavior. This is an
advantage over tasks used in several previous studies, which only evaluated whether explanations
were accessible to users, e.g. by identifying the target property smiling using image interpolations
(Singla et al., 2020) or assigning images to a concept class (Zhang et al., 2021; Ghorbani et al., 2019).
However, these tasks are an oversimpliﬁcation and cannot measure any insights the users gained
about the model. In contrast, Alqaraawi et al. (2020) employed the task of forward prediction of a
neural network. This requires substantial model understanding and is very challenging, as reﬂected
by the participants’ low accuracy. Assessing trust in a human-in-the-loop task, despite its realistic
setting, has the disadvantage that trust is inﬂuenced by many factors which are difﬁcult to control for
(Lee & See, 2004; Springer et al., 2017). Another approach is to asks participants to assess whether
a model is ﬁt for deployment (Ribeiro et al., 2016b; Adebayo et al., 2020b). However, in our own
pilots studies, users deemed a model ﬁt for deployment even if they knew it was biased."
DESIGN CONSIDERATIONS,0.15988372093023256,"Baseline Explanation Technique To quantify whether an explanation is beneﬁcial for users, it must
be compared to an alternative explanation. In this work, we argue that a very simple and reasonable
alternative for users is to inspect the model’s logits assigned to a set of input images. We designed
such a baseline explanation as shown in Figure 1a. After several design iterations, we settled for a
visually dense image grid with 5 columns sorted by the logit score, each column covering 20% of the
logit values. The columns were labeled very certain for Peeky/Stretchy, certain for Peeky/Stretchy,
and as unsure. Pilot studies showed that participants’ attention is limited. We thus decided to display
a total of 50 images, i.e. an image grid of 10 rows. The number of images was held constant between
explanation techniques to ensure the same amount of visual information and a fair comparison. In
this work, we focused on binary classiﬁcations. For a multi-class setting, one could adapt the baseline
by contrasting one class verses another class."
DESIGN CONSIDERATIONS,0.16279069767441862,"High Response Quality
We took extensive measures to ensure participants understood their task
and the explanation techniques. Participants were required to watch three professionally-spoken
tutorial videos, each under four minutes long. The videos explained, on a high level, the Two4Two
dataset, machine learning and how to use an assigned explanation technique to discover relevant
features. To avoid inﬂuencing participants, we prototyped idealized explanations using images from
TWO4TWO. The explanations showed different biases than those in the study. Each video was
followed by a written summary and set of multiple choice comprehension questions After failing
such a test once, participants could study the video and summary again. When failing a test for a
second time, participants were excluded from the study. We also excluded participants if their written
answers reﬂected a serious misunderstanding of the task, indicated by very short answers copied
for all attributes or reasoning that is very different from the tutorial. We recruited participants from
Proliﬁc who are ﬂuent in English, hold an academic degree and have an approval rate of ≥90%. To
ensure they are also motivated, we compensated them with an average hourly pay of £11.45 which
included a bonus of £0.40 per correct answer."
EXPERIMENTAL DESIGN,0.16569767441860464,"5.2
EXPERIMENTAL DESIGN"
EXPERIMENTAL DESIGN,0.1686046511627907,"We conducted two online user studies. Before starting the data collection, we formulated our
hypotheses, chose appropriate statistical tests, and pre-registered our studies (see Appendix D).
This way, we follow the gold-standard of deﬁning the statistical analysis before the data collection,
thus ensuring that our statistical results are reliable (Cockburn et al., 2018). The ﬁrst study (N=50)
analyzed whether the task was challenging enough that other methods could potentially improve over
the baseline. We tested if at least one bias in our model (either the animal’s color or the blocks’ shape)
was difﬁcult to ﬁnd using the baseline technique. Consequently, we used a within-subject design."
EXPERIMENTAL DESIGN,0.17151162790697674,Published as a conference paper at ICLR 2022
EXPERIMENTAL DESIGN,0.1744186046511628,"Table 2: The mean accuracy for each attribute by condition. Ncollected provide the number of
participants collected and Nﬁltered the number of remaining participants after the ﬁltering. Stars mark
statistical signiﬁcance."
EXPERIMENTAL DESIGN,0.17732558139534885,"Condition
Ncollected
Nﬁltered
Overall
Legs
Color
Backgr.
Shape
Posture"
EXPERIMENTAL DESIGN,0.18023255813953487,"Study 1 (Baseline)
50
43
73.4
86.0
48.8
86.0
74.4
72.1"
EXPERIMENTAL DESIGN,0.18313953488372092,"Study 2
240
192
67.0
78.2
58.9
66.8
73.1
59.1
INN
80
62
84.5
***100.0
*82.3
*79.0
90.3
71.0
Baseline
80
71
80.8
85.9
59.2
95.8
93.0
70.4
Concepts
80
59
32.2
45.8
32.2
18.6
32.2
32.2"
EXPERIMENTAL DESIGN,0.18604651162790697,"In the second study (N=240), we evaluated the two explanation techniques described in Section
4 against the baseline using a between-subjects design. Participants were randomly, but equally
assigned to one of the explanation techniques. We speciﬁed two directed hypotheses. We expected
participants in the INN condition to perform better than those in baseline, because the baseline does
not clearly highlight relevant features, whereas interpolations highlight features in isolation. We
expected participants viewing concepts to perform worse than those in the baseline, due to their
inability to highlight spatially overlapping features."
EXPERIMENTAL DESIGN,0.18895348837209303,"For both studies, participants completed a tutorial phase ﬁrst. Using their assigned explanations,
they then assessed the relevance of ﬁve attributes: legs’ position relative to the spine, animal color,
background, rotation or bending, and blocks’ shape. The questions were formulated as: ”How
relevant is <attribute> for the system?”, and participants had to choose between irrelevant or
relevant. The percentage of correct answers (accuracy) served as our primary metric. Participants
also had to write a short, fully-sentenced justiﬁcation for their answers. For links to the study, see
Appendix C."
RESULTS,0.19186046511627908,"5.3
RESULTS"
RESULTS,0.19476744186046513,"0.0
0.1
0.2
0.3
0.4"
RESULTS,0.19767441860465115,"0
1
2
3
4
5
#Correct"
RESULTS,0.2005813953488372,Proportion
RESULTS,0.20348837209302326,"BASE
CON
INN"
RESULTS,0.2063953488372093,"Figure 4: The proportion of correct
answers for baseline (BASE), con-
cepts (CON), and INN."
RESULTS,0.20930232558139536,"Data Exclusions
As stated in the preregistration, we auto-
matically excluded all participants who withdrew their consent,
failed one of the comprehension questions twice, skipped a
video, or exceeded Proliﬁc’s time limit for completion. If
a participant was excluded, a new participant’s place was
made available until the pre-registered number of completed
responses was reached. We excluded 63 study respondents for
the ﬁrst study, and 145 for the second study in this fashion.
We ensured that all participants were naive about the dataset.
Once they participated in a study, they were blacklisted for
future studies."
RESULTS,0.21220930232558138,"For completed studies, two annotators independently marked the participants’ written answers
and excluded those with copy and paste answers or indications of grave misunderstandings of the
instructions. Participants were labeled as: include, unsure, or exclude. Both anotators had an
agreement of κ = 0.545 for the ﬁrst study and κ = 0.643 for the second (measured include vs.
unsure and exclude). Disagreements were solved by discussion. In total, we excluded 7 participants
from the ﬁrst study (14%) and 48 participants from the second study (20%)."
RESULTS,0.21511627906976744,"First Study
For the accepted 43 participants, we used two-sided exact McNemar tests on their
answers about the relevance of the legs position compared with animal color (ﬁrst test) and back-
ground (second test). Participants found the color bias less often than the legs’ positions (P <0.0001).
The success rate for the color attribute was 49% vs. 86% for legs. The shape bias was not sig-
niﬁcantly harder to ﬁnd than the legs’ positions and was identiﬁed correctly with 74% accuracy
(P=0.3036). Hence, we conﬁrmed our hypothesis and concluded that other methods still have room
for improvement over the baseline."
RESULTS,0.2180232558139535,"Second Study
In the second study, we evaluated 192 valid participant responses (62 INN, 71
BASE, 59 CON). We expected data to be different from the normal distribution, and a Shapiro-Wilk
test for all conditions conﬁrmed this (P < 0.001). We depict the number of correct answers per
condition in Figure 4. A Kruskal-Wallis test showed a signiﬁcant differences in accuracy scores
between conditions (P < 0.001). For focused comparisons, we used two Wilcoxon-rank-sum"
RESULTS,0.22093023255813954,Published as a conference paper at ICLR 2022
RESULTS,0.2238372093023256,"tests with Bonferroni correction to correct for multiple comparisons. The accuracy scores differed
signiﬁcantly between the baseline and concept conditions (P <0.001, r=0.778). The performance
of participants using concepts was rather poor, with only 31.7% accuracy, considering that random
answers would yield a score of 50%. For concepts, not a single attribute surpassed the 50% barrier.
We found no signiﬁcant difference when comparing the baseline and counterfactuals (P=0.441,
r=0.091). Their mean accuracies are close, with 80.8% for baseline and 84.5% for counterfactuals.
INN counterfactuals helped users to discover the main attribute, legs’ position, (P <0.001) and
color bias (P=0.033) more reliably.1 However, counterfactuals performed signiﬁcantly worse for
the background attribute (P=0.033), while for blocks’ shape and position we found no signiﬁcant
difference (for both, P=1)."
RESULTS,0.22674418604651161,"Qualitative Results
To understand how participants integrated the explanation techniques into their
reasoning, we analyzed the textual answers of each feature qualitatively. Two annotators ﬁrst applied
open coding to the answers. They performed another pass of closed coding after agreeing on a subset
of the relevant codes, on which the following analysis is based. Overall, the participants perceived
the task as challenging, as they expressed being unsure about their answers (N=71)."
RESULTS,0.22965116279069767,"We designed our image grid to show both possible classes and provide information about the model’s
certainty. We found that many participants integrated this additional source of information into their
reasoning. This was especially prevalent in the baseline condition (N=51). Participants particularly
focused on the columns ’very certain Peeky’ and ’very certain Stretchy’, as well as on the column
’unsure’. While this may have helped conﬁrm or reject their own hypotheses, it sometimes led to
confusion; for example, when an image that exhibited a pronounced leg position, and therefore could
easily be identiﬁed as Peeky or Stretchy, was classiﬁed by the model as ’unsure’ (N=14)."
RESULTS,0.23255813953488372,"Across conditions, we also observed that participants expect that all images needed to support
a hypothesis. ”The animals are in different colors, there are blue stretchy and also blue peeky
animals, If the color was relevant peeky/stretchy would be in one color etc” (P73, BASE). Across
conditions, most participants that applied such deterministic reasoning failed to ﬁnd the color bias. In
contrast, other participants applied more probabilistic reasoning, which helped them deal with such
contradictions: ”Peeky is more likely to be blue in colour, whereas Stretchy is more likely to be pink.
This is not always true (e.g. the shapes can be white in colour at either ends of the spectrum) but it
might be somewhat relevant to help the system decide” (P197, INN)."
RESULTS,0.23546511627906977,"Another observed strategy of participants was to reference how often they saw evidence for the
relevance of a feature (N=35), which was very prevalent in the concepts condition (N=20). Especially
concepts were rather difﬁcult for participants to interpret. A common issue was that they expected a
relevant feature to be highlighted completely and consistently (N=38). Several instances show that
participants struggled to interpret how a highlighted region can explain the relevance of a feature, ”If
this [the legs position] were relevant I would have expected the system to highlight only the portion
of the image that contains the legs and spine. (e.g. only the legs and one block of the spine at each
end). Instead, every image had minimally the entire animal highlighted” (P82, CON). Furthermore,
spatially overlaping features were another cause of confusion: ”there are rows in which the animal is
highlighted but not the background so it could be because of color, shape or rotation” (P157, CON)"
RESULTS,0.23837209302325582,"Participants erred more often for the background in the INN condition than for the baseline. We
conducted an analysis to investigate this issue. We found that 29 participants stated that they perceived
no changes in the background of the counterfactuals and hence considered this feature irrelevant.
Another 21 participants noted that they saw such a change, which let 12 of them to believe its
a relevant feature. ”The background color changes in every case, it’s also a little subtle but it
does” (P205). Another 9 participants decided that the changes were too subtle to be relevant. ”The
background colour does not change an awful lot along each row, maybe in a couple of rows it changes
slightly but I do not feel the change is signiﬁcant enough that this is a relevant factor in the machine
decision” (P184)."
RESULTS,0.24127906976744187,"Do Counterfactuals Highlight Irrelevant Features?
Indeed, subtle perceptual changes in back-
ground color are present (Figure 1b). To quantify these changes, we decided to use an objective
observer: a convolutional neural network. We trained a MobileNetV2 (Sandler et al., 2018) to predict"
RESULTS,0.2441860465116279,"1The statistical analysis of the attributes for INN vs. baseline was not pre-registered. The reported p-values
for the attributes were corrected for eight tests (including the pre-registered tests) using the Holm–Bonferroni
method."
RESULTS,0.24709302325581395,Published as a conference paper at ICLR 2022
RESULTS,0.25,"−5
0
5
Logit 0 1"
RESULTS,0.25290697674418605,Legs’ Position
RESULTS,0.2558139534883721,"−5
0
5
Logit 0 1 Color"
RESULTS,0.25872093023255816,"−5
0
5
Logit 0 1"
RESULTS,0.2616279069767442,Background
RESULTS,0.26453488372093026,"−5
0
5
Logit 0 1 Shape"
RESULTS,0.26744186046511625,"Figure 5: Attribute changes along counterfactual interpolations as measured by an observer convnet.
Each line corresponds to a single sample whose logit score is modiﬁed through linear interpolations
in the classiﬁer space."
RESULTS,0.2703488372093023,"the parameter values of individual attributes of an image (e.g., background color, object color, etc.)
using a completely unbiased version of TWO4TWO. After training, the model could predict the
parameter values almost exactly (MSE < 0.0022, see Table 7). We then used this model to evaluate
the parameter values of counterfactual INN interpolations, each spanning 99% of the logit distribution.
We visualize the predictions of MobileNetV2 in Figure 5. All predictive properties (legs’ position,
body color, blocks’ shape) are changed by the counterfactuals consistently. For the background, the
changes are subtle but present. We also quantiﬁed the change in parameters using the difference
between the maximum and minimum predicted value per individual interpolation (See Table 6). This
supports the ﬁnding that relevant attributes change the most – legs’ position: 0.662; shapes: 0.624;
color: 0.440. The background changes less with 0.045, which seems enough to give some participants
a false impression about its relevance."
CONCLUSION,0.27325581395348836,"6
CONCLUSION"
CONCLUSION,0.2761627906976744,"Contributions
We contribute a dataset with full control over the biases it contains and methods to
verify the feature importances of a given model. The dataset was speciﬁcally designed for user studies
and we contribute a carefully crafted study design as a template for future empirical evaluations of
interpretable vision. Our design includes a simple, yet powerful baseline technique that relies on
the model’s outputs only. While interpretability methods had room to improve over the baseline, we
showed that two state-of-the-art methods did not perform signiﬁcantly better. Our results emphasize
that any explanation technique needs to be evaluated in extensive user studies."
CONCLUSION,0.27906976744186046,"Limitations
Due to budget constraints, we limited the number of factors in our experimental design
(external vs. internal validity trade-off). Our study introduced a predictive bias for the animal’s color
and a non-predictive bias for the blocks’ shape. It remains unclear how our results may have changed
for a different dataset conﬁguration: certain biases could exhibit different visual saliency. It remains
also left for future work to determine which visual interface design is optimal for a given method.
Furthermore, our study design restricted participants to make binary choices and provide textual
justiﬁcations – limiting our understanding of the participants issues."
CONCLUSION,0.2819767441860465,"Take-Aways
We were surprised by the performance of the two tested techniques. Users had
problems interpreting the automatically-discovered concepts and could not identify the relevant
attributes. As we expected, explaining spatially overlapping features by highlighting important
regions limited the concepts’ expressiveness. On the other hand, INN counterfactuals also did not
perform signiﬁcantly better than the baseline. Still, counterfactuals were more helpful to discover
the strongest bias in the model. However, some participants rated the relevance of the background
incorrectly, as slight changes in the interpolations were still salient enough. It is therefore important
for future work to develop counterfactuals that alter only relevant attributes."
CONCLUSION,0.28488372093023256,"We presented a user study on a synthetic dataset. We believe that the results also have implications
for natural image data. When we created Two4Two, our objective was to translate challenges faced
on ”real” computer vision data (like spatially overlapping features) into an abstract domain. Although
some properties of photorealistic datasets are lost in this abstraction, a method performing poorly on
TWO4TWO would likely not perform well on a natural dataset with spatially overlapping features."
CONCLUSION,0.2877906976744186,"Outlook
The user study was a reality check for two interpretability methods. Such studies can guide
technical innovations by identifying areas where users still struggle with current explanation methods.
They are laborious and expensive, but at the same time, they are crucial for future interpretability
research. Future work could focus on creating a more realistic, controllable dataset, e.g. using
augmented reality (Alhaija et al., 2018). By open-sourcing our videos, study, and code we encourage
the community to take on the challenge to beat the simple baseline."
CONCLUSION,0.29069767441860467,Published as a conference paper at ICLR 2022
ACKNOWLEDGMENTS,0.2936046511627907,"7
ACKNOWLEDGMENTS"
ACKNOWLEDGMENTS,0.29651162790697677,"We thank Jonas K¨ohler and Benjamin Wild for their feedback on the manuscript. We also thank our
reviewers for their time. LS was supported by the Elsa-von-Neumann Scholarship of the state of
Berlin. MS and PW were funded by the German Federal Ministry of Education and Research (BMBF)
- NR 16DII113. In addition, the work was partially funded by the the German Federal Ministry
of Education and Research (BMBF), under grant number 16DHB4018. We thank the Center for
Information Services and High Performance Computing (ZIH) at Dresden University of Technology
and the HPC Service of ZEDAT, Freie Universit¨at Berlin, for generous allocations of computation
time (Bennett et al., 2020)."
ETHICS STATEMENT,0.29941860465116277,"8
ETHICS STATEMENT"
ETHICS STATEMENT,0.3023255813953488,"In our human subject evaluation, it was important for us to pay crowd workers £11.45 per hour which
is above UK minimum wage of £8.91. The crowd workers consented to the use of their answers in
the study. No personal-data was collected. Our organization does not require an approval of online
user studies through an ethics review board."
REPRODUCIBILITY STATEMENT,0.30523255813953487,"9
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.3081395348837209,"Our work brings some additional challenges to reproducibility beyond compared to other machine
learning research. The results of the human subject study depend on the dataset, model and even the
presentations in the video tutorials. We decided to tackle this challenge by ﬁrst open-sourcing our
dataset, model, videos, code, and even the study itself. For now, we share the links to the study and
videos in the Appendix. We will share an machine-readable export of the study, the source-code, and
the model with our reviewers via OpenReview once the discussion period start and will make them
public at a later point in time."
REPRODUCIBILITY STATEMENT,0.311046511627907,"In total, we spent around 5000 GBP including prestudies, software licenses, and our two studies in the
paper. We estimate the costs of reproducing the studies in our work at around 2500 GBP excluding
software licenses. For a study comparing only one condition against the baseline, we would estimate
the costs to be around 1400 GBP."
REFERENCES,0.313953488372093,REFERENCES
REFERENCES,0.3168604651162791,"J. Adebayo, Michael Muelly, I. Liccardi, and Been Kim. Debugging tests for model explanations.
ArXiv, abs/2011.05429, 2020a."
REFERENCES,0.31976744186046513,"Julius Adebayo, Michael Muelly, Ilaria Liccardi, and Been Kim. Debugging tests for model explana-
tions, 2020b."
REFERENCES,0.3226744186046512,"Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars M. Mescheder, Andreas Geiger, and Carsten
Rother. Augmented reality meets computer vision: Efﬁcient data generation for urban driving
scenes. International Journal of Computer Vision, 126:961–972, 2018."
REFERENCES,0.32558139534883723,"Ahmed Alqaraawi, Martin Schuessler, Philipp Weiß, Enrico Costanza, and Nadia Berthouze. Evaluat-
ing saliency map explanations for convolutional neural networks: A user study. In Proceedings of
the 25th International Conference on Intelligent User Interfaces, IUI ’20, pp. 263–274, New York,
NY, USA, 2020. Association for Computing Machinery. doi: 10.1145/3377325.3377519."
REFERENCES,0.32848837209302323,"Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and Markus Gross. Towards better understanding of
gradient-based attribution methods for deep neural networks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=Sy21R9JAW."
REFERENCES,0.3313953488372093,"Leila Arras, Ahmed Osman, and Wojciech Samek. Clevr-xai: A benchmark dataset for the ground
truth evaluation of neural network explanations. Information Fusion, 2021. ISSN 1566-2535. doi:
https://doi.org/10.1016/j.inffus.2021.11.008. URL https://www.sciencedirect.com/
science/article/pii/S1566253521002335."
REFERENCES,0.33430232558139533,Published as a conference paper at ICLR 2022
REFERENCES,0.3372093023255814,"Sebastian Bach, Alexander Binder, Gr´egoire Montavon, Frederick Klauschen, Klaus-Robert M¨uller,
and Wojciech Samek. On pixel-wise explanations for non-linear classiﬁer decisions by layer-wise
relevance propagation. PLoS ONE, 10(7), 2015. doi: 10.1371/journal.pone.0130140."
REFERENCES,0.34011627906976744,"Christian F Baumgartner, Lisa M Koch, Kerem Can Tezcan, Jia Xi Ang, and Ender Konukoglu.
Visual feature attribution using wasserstein gans. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 8309–8319, 2018."
REFERENCES,0.3430232558139535,"Loris Bennett, Bernd Melchers, and Boris Proppe. Curta: A general-purpose high-performance
computer at zedat, freie universit¨at berlin. http://dx.doi.org/10.17169/refubium-26754, 2020."
REFERENCES,0.34593023255813954,"Judy Borowski, Roland S. Zimmermann, Judith Schepers, Robert Geirhos, Thomas S. A. Wallis,
Matthias Bethge, and Wieland Brendel. Exemplary natural images explain cnn activations better
than feature visualizations, 2020."
REFERENCES,0.3488372093023256,"Wieland Brendel and Matthias Bethge. Approximating cnns with bag-of-local-features models works
surprisingly well on imagenet. In International Conference on Learning Representations, 2018."
REFERENCES,0.35174418604651164,"Carrie J. Cai, Jonas Jongejan, and Jess Holbrook. The effects of example-based explanations in a
machine learning interface. In Proceedings of the 24th International Conference on Intelligent
User Interfaces, IUI ’19, pp. 258–262, New York, NY, USA, 2019. Association for Computing
Machinery. ISBN 9781450362726. doi: 10.1145/3301275.3302289. URL https://doi.org/
10.1145/3301275.3302289."
REFERENCES,0.3546511627906977,"Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and D. Duvenaud. Explaining image classiﬁers
by counterfactual generation. In ICLR, 2019."
REFERENCES,0.35755813953488375,"Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks
like that: deep learning for interpretable image recognition. In Advances in neural information
processing systems, pp. 8930–8941, 2019."
REFERENCES,0.36046511627906974,"Yuxin Chen, Oisin Mac Aodha, Shihan Su, Pietro Perona, and Yisong Yue. Near-optimal machine
teaching via explanatory teaching sets. In Amos Storkey and Fernando Perez-Cruz (eds.), Pro-
ceedings of the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics,
volume 84 of Proceedings of Machine Learning Research, pp. 1970–1978. PMLR, 09–11 Apr
2018. URL https://proceedings.mlr.press/v84/chen18g.html."
REFERENCES,0.3633720930232558,"Eric Chu, Deb Roy, and Jacob Andreas. Are visual explanations useful? a case study in model-in-the-
loop prediction, 2020."
REFERENCES,0.36627906976744184,"Andy Cockburn, Carl Gutwin, and Alan Dix. HARK No More: On the Preregistration of CHI
Experiments, pp. 1–12. Association for Computing Machinery, New York, NY, USA, 2018. ISBN
9781450356206. URL https://doi.org/10.1145/3173574.3173715."
REFERENCES,0.3691860465116279,"Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. CoRR, abs/1410.8516, 2015."
REFERENCES,0.37209302325581395,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016."
REFERENCES,0.375,"Ann-Kathrin Dombrowski, Jan E Gerken, and Pan Kessel. Diffeomorphic explanations with normal-
izing ﬂows. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit
Likelihood Models, 2021. URL https://openreview.net/forum?id=ZBR9EpEl6G4."
REFERENCES,0.37790697674418605,"Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.
arXiv: 1702.08608, 2017."
REFERENCES,0.3808139534883721,"Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based
explanations. In Advances in Neural Information Processing Systems, pp. 9277–9286, 2019."
REFERENCES,0.38372093023255816,"Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal.
Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th
International Conference on data science and advanced analytics (DSAA), pp. 80–89. IEEE, 2018."
REFERENCES,0.3866279069767442,Published as a conference paper at ICLR 2022
REFERENCES,0.38953488372093026,"Yash Goyal, Amir Feder, Uri Shalit, and Been Kim. Explaining classiﬁers with causal concept effect
(cace). arXiv preprint arXiv:1907.07165, 2019."
REFERENCES,0.39244186046511625,"Frederik Hvilshøj, Alexandros Iosiﬁdis, and Ira Assent. Ecinn: Efﬁcient counterfactuals from
invertible neural networks, 2021."
REFERENCES,0.3953488372093023,"J¨orn-Henrik Jacobsen, Arnold W.M. Smeulders, and Edouard Oyallon. i-revnet: Deep invertible
networks. In International Conference on Learning Representations, 2018. URL https://
openreview.net/forum?id=HJsjkMb0Z."
REFERENCES,0.39825581395348836,"Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and
Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual
reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
2901–2910, 2017."
REFERENCES,0.4011627906976744,"Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Vaughan Jen-
nifer Wortman. Interpreting interpretability: Understanding data scientists’ use of interpretabil-
ity tools for machine learning. In Proceedings of the 2020 CHI Conference on Human Fac-
tors in Computing Systems, CHI ’20, pp. 1–14, New York, NY, USA, 2020. Association
for Computing Machinery.
ISBN 9781450367080.
doi: 10.1145/3313831.3376219.
URL
https://doi.org/10.1145/3313831.3376219."
REFERENCES,0.40406976744186046,"Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al.
Interpretability beyond feature attribution: Quantitative testing with concept activation vectors
(tcav). In International Conference on Machine Learning, pp. 2673–2682, 2018."
REFERENCES,0.4069767441860465,"Diederik P. Kingma and P. Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. ArXiv,
abs/1807.03039, 2018."
REFERENCES,0.40988372093023256,"John D. Lee and Katrina A. See. Trust in Automation: Designing for Appropriate Reliance. Human
Factors, 46(1):50–80, March 2004. ISSN 0018-7208. doi: 10.1518/hfes.46.1.50 30392."
REFERENCES,0.4127906976744186,"Shusen Liu, Bhavya Kailkhura, Donald Loveland, and Yong Han. Generative counterfactual intro-
spection for explainable deep learning, 2019."
REFERENCES,0.41569767441860467,"Radek Mackowiak, Lynton Ardizzone, Ullrich K¨othe, and Carsten Rother. Generative classiﬁers as a
basis for trustworthy computer vision. arXiv preprint arXiv:2007.15036, 2020."
REFERENCES,0.4186046511627907,"Silvan Mertes, Tobias Huber, Katharina Weitz, Alexander Heimerl, and Elisabeth Andr´e. Ganterfac-
tual - counterfactual explanations for medical non-experts using generative adversarial learning,
2020."
REFERENCES,0.42151162790697677,"Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl. Interpretable Machine Learning – A Brief
History, State-of-the-Art and Challenges. In ECML PKDD 2020 Workshops, Communications in
Computer and Information Science, pp. 417–431, Cham, 2020. Springer International Publishing.
ISBN 978-3-030-65965-3. doi: 10.1007/978-3-030-65965-3 28."
REFERENCES,0.42441860465116277,"Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2(11):e7,
2017."
REFERENCES,0.4273255813953488,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”: Explaining the
predictions of any classiﬁer. Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016a."
REFERENCES,0.43023255813953487,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”: Explaining the
predictions of any classiﬁer, 2016b."
REFERENCES,0.4331395348837209,"Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks. 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 4510–4520, 2018."
REFERENCES,0.436046511627907,"Axel Sauer and Andreas Geiger. Counterfactual generative networks, 2021."
REFERENCES,0.438953488372093,Published as a conference paper at ICLR 2022
REFERENCES,0.4418604651162791,"Hua Shen and Ting-Hao Kenneth Huang. How Useful Are the Machine-Generated Interpretations
to General Users? A Human Evaluation on Guessing the Incorrectly Predicted Labels.
In
Proceedings of the Eighth AAAI Conference on Human Computation and Crowdsourcing (HCOMP-
20), volume 8, pp. 168–172, Virtual, October 2020. AAAI Press. ISBN 978-1-57735-848-0."
REFERENCES,0.44476744186046513,"Sumedha Singla, Brian Pollack, Junxiang Chen, and Kayhan Batmanghelich.
Explanation by
progressive exaggeration. In International Conference on Learning Representations, 2020."
REFERENCES,0.4476744186046512,"Aaron Springer, Victoria Hollis, and Steve Whittaker. Dice in the Black Box: User Experiences with
an Inscrutable Algorithm. AAAI Spring Symposium Series, 2017."
REFERENCES,0.45058139534883723,"Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319–3328.
JMLR.org, 2017."
REFERENCES,0.45348837209302323,"Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening
the black box: Automated decisions and the gdpr. Harvard Journal of Law & Technology, 31(2),
2018."
REFERENCES,0.4563953488372093,"Jennifer Wortman Vaughan and Hanna Wallach. A human-centered agenda for intelligible machine
learning.
This is a draft version of a chapter in a book to be published in the 2020 - 21
timeframe., November 2020. URL https://www.microsoft.com/en-us/research/
publication/a-human-centered-agenda-for-intelligible-machine-
learning/."
REFERENCES,0.45930232558139533,"Mengjiao Yang and Been Kim. Benchmarking attribution methods with relative feature importance,
2019."
REFERENCES,0.4622093023255814,"Kyra Yee, U. Tantipongpipat, and Shubhanshu Mishra. Image cropping on twitter: Fairness metrics,
their limitations, and the importance of representation, design, and agency. ArXiv, abs/2105.08667,
2021."
REFERENCES,0.46511627906976744,"Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A. Ehinger, and Benjamin I. P. Rubinstein.
Invertible concept-based explanations for cnn models with non-negative concept activation vectors,
2021."
REFERENCES,0.4680232558139535,Published as a conference paper at ICLR 2022
REFERENCES,0.47093023255813954,"Table 3: Importance of the data generating factors to the model’s prediction. For the R2 score,
we ﬁtted an ordinary least squares from the factors’ deltas to the deltas of the model’s logits and
then report the coefﬁcient of determination (R2). The Mean Logit Change reports the median of
the absolute change in logit values. The Prediction Flip column quantiﬁes how often the model’s
prediction changed the sign when changing the attribute."
REFERENCES,0.4738372093023256,"Factor
Prediciton Flip [%]
Median Logit Change
R2"
REFERENCES,0.47674418604651164,"Legs’ Position
41.680
2.493
0.933
Color
7.080
0.886
0.751
Shape
3.920
0.577
0.307
Position Y
2.960
0.597
0.007
Background
2.640
0.523
0.006
Rotation Yaw
3.480
0.669
0.001
Rotation Roll
2.260
0.413
0.001
Bending
3.640
0.605
0.000
Rotation Pitch
3.500
0.627
0.000
Position X
3.380
0.581
0.000"
REFERENCES,0.4796511627906977,"Table 4: Norm quantiﬁes the length of feature map changes (∆h = f(x) −f(ˆx)) after resampling
the different data generative factors. Angle w. Clas. quantiﬁes the mean angle (in degrees) between
∆h and the classiﬁer weight w."
REFERENCES,0.48255813953488375,"Factor
Norm
Norm Std.
Angle w. Clas.
Angle Std."
REFERENCES,0.48546511627906974,"Legs’ Position
70.8
2.6
79.9
1.9
Color
53.8
2.4
85.3
1.3
Shape
66.9
2.1
88.5
1.5
Position Y
68.1
2.8
89.7
1.7
Background
60.3
2.6
89.7
1.5
Bending
68.9
2.6
89.8
1.7
Rotation Yaw
68.5
2.7
89.8
1.7
Rotation Roll
55.2
2.9
89.9
1.2
Position X
68.4
3.0
89.9
1.6
Rotation Pitch
69.5
2.5
90.0
1.5"
REFERENCES,0.4883720930232558,"A
APPENDIX: TECHNICAL DETAILS"
REFERENCES,0.49127906976744184,"A.1
TWO4TWO DATASET DETAILS"
REFERENCES,0.4941860465116279,"Factor
Range
Distribution
Biased
Additional Class Information"
REFERENCES,0.49709302325581395,"Legs’ Position
[0, 1]
Uniform with overlap
Yes
-
Color
[0, 1]
See Figure 3
Yes
Yes
Shape
[0, 1]
See Figure 3
Yes
No
Position Y
[-0.8, 0]
Uniform
No
No
Position X
[-0.8, 0]
Uniform
No
No
Background
[0.05, 0.95]
Uniform
No
No
Rotation Yaw
[0, 2π]
Uniform
No
No
Rotation Roll
[−π/4, π/4]
Truncated Normal(0, 0.03π/4)
No
No
Rotation Pitch
[−π/6, π/6]
Truncated Normal(0, π / 8)
No
No
Bending
[−π/10, π/10]
Truncated Normal(0, π / 20)
No
No"
REFERENCES,0.5,"Table 5: Distribution of each attribute in the study’s dataset. Biased denotes whether an attribute is
unequally distributed for the two classes. Additional Class Information show if an attribute contains
any additional information about the target class not already given by the legs’ position."
REFERENCES,0.502906976744186,Published as a conference paper at ICLR 2022
REFERENCES,0.5058139534883721,Table 6: Two4Two: Effect of interpolating along the weight vector.
REFERENCES,0.5087209302325582,"Attribute
Mean Maximal Change
Std."
REFERENCES,0.5116279069767442,"Legs’ Position
0.662
0.140
Color
0.440
0.190
Shapes
0.624
0.208
Bending
0.059
0.042
Background
0.045
0.044
Rotation Pitch
0.186
0.126
Rotation Yaw
0.102
0.182
Rotation Roll
0.003
0.001
Position X
0.105
0.078
Position Y
0.103
0.078"
REFERENCES,0.5145348837209303,"−5
0
5
Logit 0 1"
REFERENCES,0.5174418604651163,Legs’ Position (a)
REFERENCES,0.5203488372093024,"−5
0
5
Logit 0 1 Color (b)"
REFERENCES,0.5232558139534884,"−5
0
5
Logit 0 1"
REFERENCES,0.5261627906976745,Background (c)
REFERENCES,0.5290697674418605,"−5
0
5
Logit 0 1 Shape (d)"
REFERENCES,0.5319767441860465,"−5
0
5
Logit −0.25 0.00 0.25"
REFERENCES,0.5348837209302325,Bending (e)
REFERENCES,0.5377906976744186,"−5
0
5
Logit −2 0"
REFERENCES,0.5406976744186046,Rotation Yaw (f)
REFERENCES,0.5436046511627907,"−5
0
5
Logit −0.5 0.0"
REFERENCES,0.5465116279069767,Position X (g)
REFERENCES,0.5494186046511628,"−5
0
5
Logit −0.75 −0.50 −0.25"
REFERENCES,0.5523255813953488,Position Y (h)
REFERENCES,0.5552325581395349,"Figure 6: All attribute values as predicted by an observer convnet for sequences of counterfactual
interpolations. Each line corresponds to a single sample whose logit score is modiﬁed through linear
interpolations in classiﬁer space."
REFERENCES,0.5581395348837209,Published as a conference paper at ICLR 2022
IMPORT DATACLASSES,0.561046511627907,1 import dataclasses
IMPORT NUMPY AS NP,0.563953488372093,2 import numpy as np
IMPORT NUMPY AS NP,0.5668604651162791,3 import matplotlib.pyplot as plt 4
IMPORT NUMPY AS NP,0.5697674418604651,5 from two4two.blender import render
IMPORT NUMPY AS NP,0.5726744186046512,"6 from two4two.bias import Sampler, Continouos"
IMPORT NUMPY AS NP,0.5755813953488372,7 from two4two.scene_parameters import SceneParameters 8
IMPORT NUMPY AS NP,0.5784883720930233,9 @dataclasses.dataclass
IMPORT NUMPY AS NP,0.5813953488372093,10 class RotationBiasSampler(Sampler):
IMPORT NUMPY AS NP,0.5843023255813954,"11
""""""A rotation-biased sampler. 12"
IMPORT NUMPY AS NP,0.5872093023255814,"13
The rotation is sampled conditionally depending on the object type."
IMPORT NUMPY AS NP,0.5901162790697675,"14
Positive rotations for peaky and negative rotations for stretchy."
IMPORT NUMPY AS NP,0.5930232558139535,"15
"""""" 16"
IMPORT NUMPY AS NP,0.5959302325581395,"17
obj_rotation_yaw: Continouos = dataclasses.field("
IMPORT NUMPY AS NP,0.5988372093023255,"18
default_factory=lambda: {"
IMPORT NUMPY AS NP,0.6017441860465116,"19
’peaky’: np.random.uniform(-np.pi / 4, 0),"
IMPORT NUMPY AS NP,0.6046511627906976,"20
’stretchy’: np.random.uniform(0, np.pi / 4), 21
}) 22"
IMPORT NUMPY AS NP,0.6075581395348837,23 # sample a 4 images
IMPORT NUMPY AS NP,0.6104651162790697,24 sampler = RotationBiasSampler()
IMPORT NUMPY AS NP,0.6133720930232558,25 params = [sampler.sample() for _ in range(4)]
IMPORT NUMPY AS NP,0.6162790697674418,"26 for img, mask, param in render(params):"
IMPORT NUMPY AS NP,0.6191860465116279,"27
plt.imshow(img)"
IMPORT NUMPY AS NP,0.622093023255814,"28
plt.title(f""{param.obj_name}: {param.obj_rotation_yaw}"")"
IMPORT NUMPY AS NP,0.625,"29
plt.show()"
IMPORT NUMPY AS NP,0.627906976744186,"Listing 1: Source code example to create a biased sampler. High positive rotations are predictive of
Stretchy and low negative rotations of Peaky."
IMPORT NUMPY AS NP,0.6308139534883721,"A.2
ARCHITECTURE OF THE INVERTIBLE NEURAL NETWORK"
IMPORT NUMPY AS NP,0.6337209302325582,"Our model is based on the Glow architecture (Kingma & Dhariwal, 2018) and contains 7 blocks. A
block is a collection of 32 ﬂow steps, followed by a down-sampling layer, and ends with a fade-out
layer. A single ﬂow step consists of actnorm, invertible 1 × 1 convolution and afﬁne coupling layer.
The down-sampling keeps all dimensions, e.g. a shape of (h, w, c) becomes (h/2, w/2, 4c). The
fade-out layer maps removes half of the channels. The out-faded channels are than mapped to a
standard normal distribution to compute the unsupervised loss. For generating counterfactuals, the
out-faded values are not thrown away but rather stored to be used when computing the inverse."
IMPORT NUMPY AS NP,0.6366279069767442,"The model is trained using a supervised loss and an unsupervised objective. In total our model had
687 layers and 261 million parameters. The classiﬁer used the output of layer 641. The remaining
layers 642-687 were optimized using the standard unsupervised ﬂow objective. For the ﬁrst 641
layers, we also trained on the classiﬁer’s supervised loss."
IMPORT NUMPY AS NP,0.6395348837209303,"Let ϕ denote the ﬁrst 641 layers and µ : Rn 7→Rn the last. We train ϕ both on a supervised loss
from the classiﬁer f(x) and an unsupervised loss from matching the prior distribution N(0, I) and
the log determinante of the Jacobian. µ is only trained on the unsupervised loss:"
IMPORT NUMPY AS NP,0.6424418604651163,"arg
min
θϕ,θµ,θf Lun(µ ◦ϕ(x)) + β Lsup(wT ϕ(x) + b, ytrue).
(1)"
IMPORT NUMPY AS NP,0.6453488372093024,"For the supervised loss Lsup, we use the binary cross entropy. As unsupervised loss Lun, we use the
commonly used standard ﬂow loss obtained from the change of variables trick Dinh et al. (2016).
The unsupervised loss ensures that inverting the function results in realistic looking images and can
also be seen as a regularization."
IMPORT NUMPY AS NP,0.6482558139534884,The layer 342 used for the concept explanations is an afﬁne coupling layer.
IMPORT NUMPY AS NP,0.6511627906976745,Published as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.6540697674418605,"Table 7: Test performance of the supervised trained model MobileNet-V2 measured using a mean
squared error (MSE)."
IMPORT NUMPY AS NP,0.6569767441860465,"Attribute
Test MSE"
IMPORT NUMPY AS NP,0.6598837209302325,"Legs’ Position
0.0008912
Bending
0.0001915
Background
0.0001128
Color
0.0005297
Rotation Pitch
0.0009235
Rotation Roll
0.0005622
Rotation Yaw
0.002243
Position X
0.0004451
Position Y
0.0003912
Shapes
0.001102"
IMPORT NUMPY AS NP,0.6627906976744186,"A.3
SUPERVISED MOBILENET-V2"
IMPORT NUMPY AS NP,0.6656976744186046,"We used a MobileNet-V2 to predict the attributes values. The models test mean squared errors are
denoted in Table 7."
IMPORT NUMPY AS NP,0.6686046511627907,Published as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.6715116279069767,"B
LINKS TO MODEL, DATASET AND STUDY"
IMPORT NUMPY AS NP,0.6744186046511628,"Model export:
https://f002.backblazeb2.com/file/iclr2022/do_users_
benefit_from_interpretable_vision_model.tar.gz
Unbiased dataset:
https://f002.backblazeb2.com/file/iclr2022/two4two_
obj_color_and_spherical_finer_search_spherical_
uniform_0.33_uniform_0.15_unbiased.tar
Biased dataset:
https://f002.backblazeb2.com/file/iclr2022/two4two_
obj_color_and_spherical_finer_search_spherical_
uniform_0.33_uniform_0.15.tar
Export of study:
https://f002.backblazeb2.com/file/iclr2022/ICLR2022_
Export_Do_Users_Benefit_From_Interpretable_Vision.qsf
PDF print-out of the study:
https://f002.backblazeb2.com/file/iclr2022/ICLR2022_
Export_Do_Users_Benefit_From_Interpretable_Vision.pdf"
IMPORT NUMPY AS NP,0.6773255813953488,"C
USER-STUDY LINKS AND VIDEOS"
IMPORT NUMPY AS NP,0.6802325581395349,"The studies can be accessed on the Qualtrics platform (with anonymized consent form) under the
following links:"
IMPORT NUMPY AS NP,0.6831395348837209,"Baseline condition:
https://wznbm.qualtrics.com/jfe/form/SV_
7Umdmdaq8EHVRm6
INN condition:
https://wznbm.qualtrics.com/jfe/form/SV_
dneHADG7BxjVurc
Concepts condition:
https://wznbm.qualtrics.com/jfe/form/SV_
0PWErBQmGL0lobk"
IMPORT NUMPY AS NP,0.686046511627907,The tutorial videos can be viewed under the following links:
IMPORT NUMPY AS NP,0.688953488372093,"Introduction Tutorial for Peeky and Stretchy:
https://f002.backblazeb2.com/file/iclr2022/Intro_
Peeky_Stretchy.mp4
Second Introduction Tutorial for ML and Biases:
https://f002.backblazeb2.com/file/iclr2022/Second_
Intro_ML.mp4
Tutorial for baseline condition:
https://f002.backblazeb2.com/file/iclr2022/condition_
BASE.mp4
Tutorial for concept condition:
https://f002.backblazeb2.com/file/iclr2022/condition_
CONCEPTS.mp4
Tutorial for INN condition:
https://f002.backblazeb2.com/file/iclr2022/condition_
INN.mp4"
IMPORT NUMPY AS NP,0.6918604651162791,Published as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.6947674418604651,"D
USER-STUDY PREREGISTRATION AND HYPOTHESIS"
IMPORT NUMPY AS NP,0.6976744186046512,The Preregistrations can also be viewed under the following URLs:
IMPORT NUMPY AS NP,0.7005813953488372,"• Validation of TWO4TWO: https://aspredicted.org/blind.php?x=/62X_
15J"
IMPORT NUMPY AS NP,0.7034883720930233,"• Study 2: Concepts vs. Baseline and INN vs. Baseline: https://aspredicted.org/
blind.php?x=/7XN_77P"
IMPORT NUMPY AS NP,0.7063953488372093,"We also paid the participants in both studies the more lucrative tariff included in the preregistration of
study 2, e.g. for third comprehension task: 3.50GBP and so on."
IMPORT NUMPY AS NP,0.7093023255813954,"D.1
VALIDATION OF TWO4TWO"
IMPORT NUMPY AS NP,0.7122093023255814,1) Have any data been collected for this study already?
IMPORT NUMPY AS NP,0.7151162790697675,"No, no data have been collected for this study yet."
IMPORT NUMPY AS NP,0.7180232558139535,2) What’s the main question being asked or hypothesis being tested in this study?
IMPORT NUMPY AS NP,0.7209302325581395,"This study investigates whether users identify biases learned by a neural network. The neural networks
task is to discriminate between two abstract animals (”Peeky” and ”Stretchy”). Each participant is
presented with predictions of the system in a 10x5 image grid."
IMPORT NUMPY AS NP,0.7238372093023255,"After an initial tutorial phase, the participants have to ﬁnd biases in the model. They do this by scoring
different characteristics as relevant or irrelevant. The characteristics are: ”legs position relative to the
spine (LEGS)”, ”object color (COLOR)”, ”background (BACK)”, ”rounded or rectangular shape of
the blocks (SHAPE)”, and ”rotation and bending (ROT)”."
IMPORT NUMPY AS NP,0.7267441860465116,"The main research question is whether we succeeded in creating a model that contains at least one
bias that is hard to detect, i.e. either COLOR or SHAPE should be harder to detect than LEGS."
IMPORT NUMPY AS NP,0.7296511627906976,HB: Participants can identify the biases in COLOR or SHAPE less frequently than LEGS.
IMPORT NUMPY AS NP,0.7325581395348837,3) Describe the key dependent variable(s) specifying how they will be measured.
IMPORT NUMPY AS NP,0.7354651162790697,Participant will answer the following questions:
IMPORT NUMPY AS NP,0.7383720930232558,"• LEGS: How relevant is the legs position relative to the spine for the system?: Relevant /
Irrelevant"
IMPORT NUMPY AS NP,0.7412790697674418,• COLOR: How relevant is the color of the animal for the system? Relevant / Irrelevant
IMPORT NUMPY AS NP,0.7441860465116279,• BACK: How relevant is the background of the animal for the system? Relevant / Irrelevant
IMPORT NUMPY AS NP,0.747093023255814,"• SHAPE: How relevant is the rounded or rectangular shape of the animal’s blocks for the
system? Relevant / Irrelevant"
IMPORT NUMPY AS NP,0.75,"• ROT: How relevant is the rotation and bending of the animal for the system? Relevant /
Irrelevant"
IMPORT NUMPY AS NP,0.752906976744186,"The ground truth answer is that LEGS, COLOR, SHAPE are relevant while BACK and ROT are
irrelevant. Our ﬁrst dependent variable is the number of times the head position was selected as
relevant. Our second dependent variable is the number of times the color of the animal was selected
as relevant. Our third dependent variable is the number of times the rounded or rectangular shape of
the animal’s blocks was selected as relevant."
IMPORT NUMPY AS NP,0.7558139534883721,4) How many and which conditions will participants be assigned to?
IMPORT NUMPY AS NP,0.7587209302325582,"Our study follows a within-subject design and has only one condition. We ﬁrst show the participants
introductory videos about the two abstract animals, the machine learning system, and some guidance
on how to interpret the predictions of the system. Each video is accompanied by a written summary.
We then show the predictions of the system in a grid of images: 10 sorted rows of 5 images drawn
from the validation set (50 original images). Each of the ﬁve columns represents the neural netwoks’s
logit range. Similarly rated images are assigned to the same column."
IMPORT NUMPY AS NP,0.7616279069767442,Published as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.7645348837209303,5) Specify exactly which analyses you will conduct to examine the main question/hypothesis.
IMPORT NUMPY AS NP,0.7674418604651163,"We will conduct two exact one-sided McNemar-tests with LEGS acting as our control: one between
SHAPE and LEGS and a second between COLOR and LEGS. We will use a one-sided test as we
expect that SHAPE and COLOR are harder to identify. The signiﬁcance level of both tests will be
Bonferroni adjusted to α = 0.025."
IMPORT NUMPY AS NP,0.7703488372093024,"6) Describe exactly how outliers will be deﬁned and handled, and your precise rule(s) for excluding
observations."
IMPORT NUMPY AS NP,0.7732558139534884,"We reject participants with low effort responses or who failed to understand the dataset, machine
learning concept, or explanation method. We have implemented hard-coded exclusion criteria directly
in the survey (implemented with Qulatrics and Proliﬁc)."
IMPORT NUMPY AS NP,0.7761627906976745,• did not ﬁnish experiment at all or in under 77 minutes
IMPORT NUMPY AS NP,0.7790697674418605,"• did not watch tutorial videos completely (there are 3 videos) or failed a multiple-choice
comprehension test twice (there are four such tests), unless participants explicitly ask us to
retake the study"
IMPORT NUMPY AS NP,0.7819767441860465,• using a device smaller than a tablet (min. 600 px in width or height)
IMPORT NUMPY AS NP,0.7848837209302325,• provided answers about relevant characteristics in under 30 seconds
IMPORT NUMPY AS NP,0.7877906976744186,• withdrawn data consent / returned task on Proliﬁc
IMPORT NUMPY AS NP,0.7906976744186046,"• circumvented Qualtrics protection against retaking the entire survey again (ﬁrst complete
submission will be counted)"
IMPORT NUMPY AS NP,0.7936046511627907,"We do not plan to exclude any participants who passed all of the above criteria unless the qualitative
answers reveal a serious misunderstanding of the study instructions that the multiple choice tests did
not cover. We will report such exclusions in detail in the Appendix."
IMPORT NUMPY AS NP,0.7965116279069767,7) How many observations will be collected or what will determine sample size?
IMPORT NUMPY AS NP,0.7994186046511628,"No need to justify decision, but be precise about exactly how the number will be determined. 50
participants from Proliﬁc with the background:"
IMPORT NUMPY AS NP,0.8023255813953488,• Fluent in English
IMPORT NUMPY AS NP,0.8052325581395349,• Hold an academic degree
IMPORT NUMPY AS NP,0.8081395348837209,• Proliﬁc approval rate of at least 90%
IMPORT NUMPY AS NP,0.811046511627907,• Did not participate in pilot studies
IMPORT NUMPY AS NP,0.813953488372093,• Passed hard coded exclusion criteria (see 8).
IMPORT NUMPY AS NP,0.8168604651162791,"We pay participants max. 8.00 GBP (6.00 GBP base salary + 2.00 GBP max bonus). For those failing
any comprehension questions or not watching the video, we pay:"
IMPORT NUMPY AS NP,0.8197674418604651,• First comprehension task: no compensation
IMPORT NUMPY AS NP,0.8226744186046512,• Second comprehension task: 0.5 GBP
IMPORT NUMPY AS NP,0.8255813953488372,• Third comprehension task: 1.75 GBP
IMPORT NUMPY AS NP,0.8284883720930233,• Failed to watch ﬁrst video: no compensation
IMPORT NUMPY AS NP,0.8313953488372093,• Failed to watch second video: 1 GBP
IMPORT NUMPY AS NP,0.8343023255813954,• Failed to watch third video: 2 GBP
IMPORT NUMPY AS NP,0.8372093023255814,"8) Anything else you would like to pre-register? (e.g., secondary analyses, variables collected for
exploratory purposes, unusual analyses planned?) We ask participants to answer three multiple choice
comprehension tests in the form of true/false statements to ensure that they understood the task and
the dataset. We also ask them to provide some free-text justiﬁcation of why they chose a relevant /
irrelevant rating to the questions in Section 3."
IMPORT NUMPY AS NP,0.8401162790697675,Published as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.8430232558139535,"D.2
STUDY 2: CONCEPTS VS. BASELINE AND INN VS. BASELINE"
IMPORT NUMPY AS NP,0.8459302325581395,1) Have any data been collected for this study already?
IMPORT NUMPY AS NP,0.8488372093023255,"No, no data have been collected for this study yet."
IMPORT NUMPY AS NP,0.8517441860465116,2) What’s the main question being asked or hypothesis being tested in this study?
IMPORT NUMPY AS NP,0.8546511627906976,"This study investigates whether users identify biases learned by a neural network. The neural networks
task is to discriminate between two abstract animals (”Peeky” and ”Stretchy”). Each participant is
presented one of three different explanation methods: baseline (B), counterfactuals obtained using
invertible neural networks (CF) and prototypes (P)."
IMPORT NUMPY AS NP,0.8575581395348837,"Each participant is randomly assigned to a method. After an initial tutorial phase, the participants have
to ﬁnd biases in the model. They do this by scoring different characteristics as relevant or irrelevant.
The characteristics are: ”legs position relative to the spine (LEGS)”, ”object color (COLOR)”,
”background (BACK)”, ”rounded or rectangular shape of the blocks (SHAPE)”, and ”rotation and
bending (ROT)”."
IMPORT NUMPY AS NP,0.8604651162790697,"The main question of our study is whether the participants can correctly identify relevant and irrelevant
attributes using these explanation methods (B, CF, P). This is reﬂected by two hypotheses:"
IMPORT NUMPY AS NP,0.8633720930232558,H1: Participants identify relevant and irrelevant attributes with less accuracy using P compared to B.
IMPORT NUMPY AS NP,0.8662790697674418,"H2: Participants identify relevant and irrelevant attributes with higher accuracy using CF compared
to B."
IMPORT NUMPY AS NP,0.8691860465116279,3) Describe the key dependent variable(s) specifying how they will be measured.
IMPORT NUMPY AS NP,0.872093023255814,Participant will answer the following questions:
IMPORT NUMPY AS NP,0.875,"• LEGS: How relevant is the legs position relative to the spine for the system?: Relevant /
Irrelevant
• COLOR: How relevant is the color of the animal for the system? Relevant / Irrelevant
• BACK: How relevant is the background of the animal for the system? Relevant / Irrelevant
• SHAPE: How relevant is the rounded or rectangular shape of the animal’s blocks for the
system? Relevant / Irrelevant
• ROT: How relevant is the rotation and bending of the animal for the system? Relevant /
Irrelevant"
IMPORT NUMPY AS NP,0.877906976744186,"The ground truth answer is that LEGS, COLOR, SHAPE are relevant while BACK and ROT are
irrelevant. Our dependent variable is the percentage of correctly answered questions per participant
(accuracy, which is computed as (true positives + true negatives)/number of total answers)."
IMPORT NUMPY AS NP,0.8808139534883721,4) How many and which conditions will participants be assigned to?
IMPORT NUMPY AS NP,0.8837209302325582,"We run a between-subject study, with randomly but equally assigned participants to 1 of 3 conditions.
We ﬁrst show introductory videos about the two abstract animals, the machine learning system,
the explanation technique and some guidance on how to interpret the technique. Each video is
accompanied by a written summary. We then show a grid of (10x5) images:"
IMPORT NUMPY AS NP,0.8866279069767442,"1. B: NN predictions explained with 10 sorted rows of 5 images drawn from the validation set (50
original images). Each of the ﬁve columns represents a score range. Similarly rated images are
assigned to the same column."
IMPORT NUMPY AS NP,0.8895348837209303,"2.CF: Same grid layout as B, but the NN is explained by counterfactual interpolations. Each row
contains interpolations which change the prediction of the NN to ﬁt the designated score. Original
images are used as starting points but are not shown."
IMPORT NUMPY AS NP,0.8924418604651163,"3.P: We found concepts based on the work by (Zhang et al., 2020). Each row shows a set of relevant
concepts. We only used concepts correlated with at least r=0.2 with the model logit values. In total,
we display 10 rows where each row contains a concept. Each row contains a set of 5 example images
for which the concept is relevant."
IMPORT NUMPY AS NP,0.8953488372093024,"(Zhang et al., 2020) https://arxiv.org/abs/2006.15417"
IMPORT NUMPY AS NP,0.8982558139534884,Published as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.9011627906976745,5) Specify exactly which analyses you will conduct to examine the main question/hypothesis.
IMPORT NUMPY AS NP,0.9040697674418605,"We will compute the accuracy scores for each participant and then compare the accuracy scores
between the conditions. We expect the data to be non-normally distributed, and will test this
assumption using a Shapiro-Wilk test with a signiﬁcance level of α = 0.05. If our assumption is true,
we plan to conduct a Kruskal-Wallis test, followed by post-hoc analysis using Wilcoxon’s-rank-sum
tests for focused comparison between the groups CF and B (expecting higher accuracy in CF) and P
and B (expecting lower accuracy in P)."
IMPORT NUMPY AS NP,0.9069767441860465,"If the data is normally distributed, we will conduct a one-way ANOVA with planned contrasts, if the
following assumptions of ANOVAs are met:"
IMPORT NUMPY AS NP,0.9098837209302325,"• Homogeneity of the variance of the population (assessed with a Levene-Test with a signiﬁ-
cance level of α = 0.05.)"
IMPORT NUMPY AS NP,0.9127906976744186,"If the homogeneity of variance assumption of ANOVA is violated (assessed with a Levene-Test with
a signiﬁcance level of α = 0.05.), we plan to perform Welch’s Anova."
IMPORT NUMPY AS NP,0.9156976744186046,"6) Describe exactly how outliers will be deﬁned and handled, and your precise rule(s) for excluding
observations."
IMPORT NUMPY AS NP,0.9186046511627907,"We reject participants with low effort responses or who failed to understand the dataset, machine
learning concept, or explanation method. We have implemented hard-coded exclusion criteria directly
in the survey (implemented with Qulatrics and Proliﬁc)."
IMPORT NUMPY AS NP,0.9215116279069767,• did not ﬁnish experiment at all or in under 77 minutes
IMPORT NUMPY AS NP,0.9244186046511628,"• did not watch the tutorial videos completely (there are 3 videos) or failed a multiple-choice
comprehension test twice (there are four such tests), unless participants explicitly ask us to
retake the study"
IMPORT NUMPY AS NP,0.9273255813953488,• using a device smaller than a tablet (min. 600 px in width or height)
IMPORT NUMPY AS NP,0.9302325581395349,• provided answers about relevant characteristics in under 30 seconds
IMPORT NUMPY AS NP,0.9331395348837209,• withdrawn data consent / returned task on Proliﬁc
IMPORT NUMPY AS NP,0.936046511627907,"• circumvented Qualtrics protection against retaking the entire survey again (ﬁrst complete
submission will be counted)"
IMPORT NUMPY AS NP,0.938953488372093,"We do not plan to exclude any participants who passed all of the above criteria unless the qualitative
answers reveal a serious misunderstanding of the study instructions that the multiple choice tests did
not cover. We will report such exclusions in detail in the Appendix."
IMPORT NUMPY AS NP,0.9418604651162791,"7) How many observations will be collected or what will determine sample size? No need to justify
decision, but be precise about exactly how the number will be determined."
IMPORT NUMPY AS NP,0.9447674418604651,240 (80 per condition) participants from Proliﬁc with the background:
IMPORT NUMPY AS NP,0.9476744186046512,• Fluent in English
IMPORT NUMPY AS NP,0.9505813953488372,"• First, we sample participants with an academic degree. If we do not reach the desired
participant number, which is likely given the limited availability of such subjects, we will
supplement with participants with an academic degree in other subjects. All participants
will be randomly and equally split into the 4 conditions."
IMPORT NUMPY AS NP,0.9534883720930233,• Proliﬁc approval rate of at least 90%
IMPORT NUMPY AS NP,0.9563953488372093,• Did not participate in pilot studies
IMPORT NUMPY AS NP,0.9593023255813954,• Passed hard coded exclusion criteria (see 8).
IMPORT NUMPY AS NP,0.9622093023255814,"We pay participants max. 6.50 GBP (4.50 GBP base salary + 2.00 GBP max bonus). For those failing
any comprehension questions or not watching the video, we pay:"
IMPORT NUMPY AS NP,0.9651162790697675,• First comprehension task: 0.5 GBP
IMPORT NUMPY AS NP,0.9680232558139535,• Second comprehension task: 1.75 GBP
IMPORT NUMPY AS NP,0.9709302325581395,Published as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.9738372093023255,• Third comprehension task: 3.50GBP
IMPORT NUMPY AS NP,0.9767441860465116,• Failed to watch ﬁrst video: no compensation
IMPORT NUMPY AS NP,0.9796511627906976,• Failed to watch second video: 1 GBP
IMPORT NUMPY AS NP,0.9825581395348837,• Failed to watch third video: 2 GBP
IMPORT NUMPY AS NP,0.9854651162790697,"8) Anything else you would like to pre-register? (e.g., secondary analyses, variables collected for
exploratory purposes, unusual analyses planned?)"
IMPORT NUMPY AS NP,0.9883720930232558,"In a previous study, we collected 50 responses for the baseline condition only (Preregistration
#75056)). We do not plan to use the data for this study."
IMPORT NUMPY AS NP,0.9912790697674418,"We ask participants to answer three multiple choice comprehension tests in the form of true/false
statements to ensure that they understood the task and the dataset. We also ask them to provide some
free-text justiﬁcation of why they chose a relevant / irrelevant rating to the questions in Section 3."
IMPORT NUMPY AS NP,0.9941860465116279,"Additionally, we ask the participants about their machine learning expertise level. Participants can
rate their expertise as: complete novice, some expertise, or expert in the topic. We plan to use
descriptive statistics to see how accuracies change per condition for each expertise level and how
expertise was distributed within our sample."
IMPORT NUMPY AS NP,0.997093023255814,"We are also planning a qualitative thematic analysis of the open-text questions in our survey via open
and axial coding, with the aim of understanding how participants integrated explanations in their
reasoning about the relevance of attributes."
