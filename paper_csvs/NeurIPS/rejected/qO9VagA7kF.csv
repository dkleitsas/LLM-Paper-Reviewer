Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001953125,"Post-training quantization (PTQ) has emerged as a promising technique for mit-
1"
ABSTRACT,0.00390625,"igating memory consumption and computational costs in large language models
2"
ABSTRACT,0.005859375,"(LLMs). However, a systematic examination of various quantization schemes,
3"
ABSTRACT,0.0078125,"model families, and quantization bit precision has been absent from the literature.
4"
ABSTRACT,0.009765625,"In this paper, we conduct a comprehensive analysis of these factors by investigating
5"
ABSTRACT,0.01171875,"the effects of PTQ on weight-only, activation-only, and weight-and-activation quan-
6"
ABSTRACT,0.013671875,"tization using diverse methods such as round-to-nearest (RTN), GPTQ, ZeroQuant,
7"
ABSTRACT,0.015625,"and their variants. We apply these methods to two distinct model families with
8"
ABSTRACT,0.017578125,"parameters ranging from 125M to 176B. Our contributions include: (1) a sensitivity
9"
ABSTRACT,0.01953125,"analysis revealing that activation quantization is generally more susceptible to
10"
ABSTRACT,0.021484375,"weight quantization, with smaller models often outperforming larger models in
11"
ABSTRACT,0.0234375,"terms of activation quantization; (2) an evaluation and comparison of existing PTQ
12"
ABSTRACT,0.025390625,"methods to optimize model size reduction while minimizing the impact on accuracy,
13"
ABSTRACT,0.02734375,"revealing that none of the current methods can achieve the original model quality
14"
ABSTRACT,0.029296875,"for quantization with either INT4-weight or INT4-weight-and-INT8-activation;
15"
ABSTRACT,0.03125,"(3) based on these insights, we propose an optimized method called Low-Rank
16"
ABSTRACT,0.033203125,"Compensation (LoRC), which employs low-rank matrices to enhance model quality
17"
ABSTRACT,0.03515625,"recovery with a minimal increase in model size.
18"
INTRODUCTION,0.037109375,"1
Introduction
19"
INTRODUCTION,0.0390625,"Large language models (LLMs) like Codex [15] and ChatGPT [24] have demonstrated breakthrough
20"
INTRODUCTION,0.041015625,"performance across various benchmarks, such as natural language understanding and generation, and
21"
INTRODUCTION,0.04296875,"are now integrated into everyday applications. However, efficiently serving LLMs has become a
22"
INTRODUCTION,0.044921875,"pressing concern due to their significant memory consumption and computational demands. Unlike
23"
INTRODUCTION,0.046875,"classification or diffusion models, LLMs present unique challenges, as they involve two distinct
24"
INTRODUCTION,0.048828125,"phases: prompt and generation. The prompt phase is primarily compute-bound, while the generation
25"
INTRODUCTION,0.05078125,"phase, with low batch size and KV cache, is mainly memory-bound [26].
26"
INTRODUCTION,0.052734375,"As the progression of hardware bandwidth lags behind that of computational demand [14], the resource
27"
INTRODUCTION,0.0546875,"demands of extra-large models such as MT-NLG-530B [30]—which necessitates the deployment of
28"
INTRODUCTION,0.056640625,"multiple nodes for operation—escalate, adding to the complexities of cross-node communication.
29"
INTRODUCTION,0.05859375,"This has emphasized the urgency to curtail both the size and computational expense of Large Language
30"
INTRODUCTION,0.060546875,"Models (LLMs). An increasingly effective solution to these issues is post-training quantization (PTQ).
31"
INTRODUCTION,0.0625,"This method aids in the reduction of training prerequisites while simultaneously lowering the bit
32"
INTRODUCTION,0.064453125,"precision of weights and activations to either INT4 or INT8.
33"
INTRODUCTION,0.06640625,"While the effectiveness of post-training quantization (PTQ) has been underscored in a number of
34"
INTRODUCTION,0.068359375,"recent studies [36, 12, 35, 7], a comprehensive, systematic investigation into several key dimensions
35"
INTRODUCTION,0.0703125,"of this technique remains to be undertaken. Specifically, the extant literature falls short in providing
36"
INTRODUCTION,0.072265625,"0.0
0.2
0.4
0.6
0.8
1.0
Model-size (Bit numbers in log scale)
1e12 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5 30.0"
INTRODUCTION,0.07421875,"Model-quality (Perplexity, decreasing in y-axis)"
INTRODUCTION,0.076171875,OPT (Quality and size tradeoff)
INTRODUCTION,0.078125,"0.0
0.5
1.0
1.5
2.0
1e11 10 12 14 16 18 20"
INTRODUCTION,0.080078125,"PTQ
RTN"
INTRODUCTION,0.08203125,"FP16/INT8
LoRC"
INTRODUCTION,0.083984375,"0.0
0.5
1.0
1.5
2.0
2.5
Model-size (Bit numbers in log scale)
1e12 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5 30.0"
INTRODUCTION,0.0859375,"Model-quality (Perplexity, decreasing in y-axis)"
INTRODUCTION,0.087890625,BLOOM (Quality and size tradeoff)
INTRODUCTION,0.08984375,"0
1
2
3
4
5 1e10 15.0 17.5 20.0 22.5 25.0 27.5 30.0"
INTRODUCTION,0.091796875,"PTQ
RTN"
INTRODUCTION,0.09375,"FP16/INT8
LoRC"
INTRODUCTION,0.095703125,"Figure 1: The model size and quality trade-off of different quantization methods on models from
OPT and BLOOM families. Here PTQ (with fine-grained quantization) represents the method
from [36, 12], RTN means the naive round-to-nearest baseline (with fine-grained quantization as
well), and FP16/INT8 is used as the no-accuracy-loss baseline. LoRC is our proposed method that
works seamless with PTQ. Note that we drop all diverged points for better visualization. For all
detailed numbers, please see Appendix E."
INTRODUCTION,0.09765625,"thorough coverage of the functionality of various PTQ methods or the sensitivity of disparate models.
37"
INTRODUCTION,0.099609375,"Moreover, despite current quantization methods demonstrating promising results in the reduction of
38"
INTRODUCTION,0.1015625,"model sizes, the question persists as to whether these methods are achieving their optimal potential in
39"
INTRODUCTION,0.103515625,"minimizing Large Language Models (LLMs) sizes.
40"
INTRODUCTION,0.10546875,"With these observations in mind, our study sets forth to address two salient questions: (1) When
41"
INTRODUCTION,0.107421875,"subjected to quantization, do LLMs of varying sizes and pretraining data exhibit similar behavior? (2)
42"
INTRODUCTION,0.109375,"Are existing quantization methods truly leveraging their full potential in reducing the sizes of LLMs?
43"
INTRODUCTION,0.111328125,"Contribution.
To elucidate these queries, we undertake an exhaustive examination of the impact
44"
INTRODUCTION,0.11328125,"of PTQ on weight-only, activation-only, and combined weight-and-activation quantization. This
45"
INTRODUCTION,0.115234375,"investigation incorporates a range of PTQ methods, including round-to-nearest (RTN), GPTQ [12],
46"
INTRODUCTION,0.1171875,"ZeroQuant [36], and their respective variants. To broaden the scope of our analysis, we focus on
47"
INTRODUCTION,0.119140625,"two distinct model families, OPT [40] and BLOOM [28], spanning model sizes from 125M to a
48"
INTRODUCTION,0.12109375,"massive 176B. Our code will be made available for reproduction. In summary, we make the following
49"
INTRODUCTION,0.123046875,"contributions:
50"
INTRODUCTION,0.125,"(1) We provide a thorough sensitivity analysis to demonstrate that a) Activation quantization is
51"
INTRODUCTION,0.126953125,"generally more sensitive to weight quantization; Smaller models usually have better activation
52"
INTRODUCTION,0.12890625,"quantization performance than the relative larger model. b) Different model families show different
53"
INTRODUCTION,0.130859375,"INT8 activation quantization behaviors; Particularly for large models, BLOOM-176B has small
54"
INTRODUCTION,0.1328125,"accuracy drops (about 1 perplexity or PPL) but OPT-30B and -66B experience worse performance.
55"
INTRODUCTION,0.134765625,"(2) We carry out a detailed evaluation and comparison of current PTQ methods, utilizing optimal
56"
INTRODUCTION,0.13671875,"configurations to maximize model size reduction while minimizing accuracy impact. We found that
57"
INTRODUCTION,0.138671875,"the current existing method can barely achieve less than 0.1 PPL points degradation for quantization
58"
INTRODUCTION,0.140625,"with either INT4-weight or INT4-weight-and-INT8-activation (W4A8). To recover the 0.1 PPL, we
59"
INTRODUCTION,0.142578125,"strive to push the boundaries of employing fine-grained quantization (FGQ) techniques. We observe
60"
INTRODUCTION,0.14453125,"FGQ is able to recovered points degradation of <0.1 PPL for large models (>13B) for INT4 weight
61"
INTRODUCTION,0.146484375,"quantization, but there are still non-negligible model quality drops.
62"
INTRODUCTION,0.1484375,"(3) Based on the above understanding, we further optimize existing methods and introduce a technique
63"
INTRODUCTION,0.150390625,"called Low Rank Compensation (LoRC), which employs low-rank matrix factorization on the
64"
INTRODUCTION,0.15234375,"quantization error matrix. Complementary to FGQ, LoRC plays a crucial role in enhancing the full
65"
INTRODUCTION,0.154296875,"model quality recovery, while there is little increase of the model size.
66"
INTRODUCTION,0.15625,"In Figure 1, we provide model size and quality trade-offs for both OPT and BLOOM families.
67"
INTRODUCTION,0.158203125,"As can be seen, using LoRC on top of PTQ methods from [36, 12] and fine-grained quantization,
68"
INTRODUCTION,0.16015625,"we set a new quantization Pareto frontier for LLMs. Meanwhile, we recommend the following
69"
INTRODUCTION,0.162109375,"setting for quantizing LLMs with LoRC (Note that activation quantization should be only applied if
70"
INTRODUCTION,0.1640625,"necessary): (1) For larger models (>10B), fine-grained (block size 64–256) 4-bit weight quantization
71"
INTRODUCTION,0.166015625,"plus 8-bit activation quantization (block size 64–256) with PTQ can be used for real deployment; (2)
72"
INTRODUCTION,0.16796875,"For middle-size models (<10B and >1B), per-row INT8 quantization plus fine-grained (block size
73"
INTRODUCTION,0.169921875,"64–256) INT8 activation quantization can be used with PTQ from [12, 36]; (3) For smaller models
74"
INTRODUCTION,0.171875,"(<1B), per-row W8A8 (INT8 weight and INT8 activation) RTN is enough based on [36].
75"
RELATED WORK,0.173828125,"2
Related Work
76"
RELATED WORK,0.17578125,"Different quantization methods [29, 38, 9, 41, 1, 8, 31, 19] for transformer-based models [32] have
77"
RELATED WORK,0.177734375,"been explored for a while. However, most of those works need quantization-aware finetuning or
78"
RELATED WORK,0.1796875,"even expensive quantization-aware knowledge distillation [17]. Due to the cost of training/finetuning
79"
RELATED WORK,0.181640625,"LLMs [25, 18, 31, 34, 33], it is a challenge for practitioners/researchers to do finetuning/distillation
80"
RELATED WORK,0.18359375,"on those LLMs, particularly for models like GPT-3-175B [4] and BLOOM-176B [28].
81"
RELATED WORK,0.185546875,"Post-training quantization (PTQ) [37, 3] is an alternative way to quantize the model with no/minimal
82"
RELATED WORK,0.1875,"finetuning requirement. Along this line, several recent works focus on LLMs (beyond the million-
83"
RELATED WORK,0.189453125,"parameter scale). [36] proposes vector-based INT8 quantization with layer-by-layer knowledge
84"
RELATED WORK,0.19140625,"distillation to overcome the training cost and quantization error introduced by LLMs. [6] uses similar
85"
RELATED WORK,0.193359375,"vector-based INT8 quantization weight plus mixed-precision (INT8/FP16) quantization for activation
86"
RELATED WORK,0.1953125,"to overcome the sensitivity of activation quantization. However, the inference speed of [6] is generally
87"
RELATED WORK,0.197265625,"even slower than FP16 baseline [2] due to the difficulty of implementing mixed-precision calculation
88"
RELATED WORK,0.19921875,"within a single tensor. More recently, [12] extends OBQ [10, 16, 21] on LLMs for INT4 weight-only
89"
RELATED WORK,0.201171875,"quantization and shows great efficiency on quantization and latency, and [35] shows the outliers
90"
RELATED WORK,0.203125,"from activations can be smoothed out by migrating the quantization difficulty from activations to its
91"
RELATED WORK,0.205078125,"associated weights. However, [35] can only work for W8A8 quantization as lower weight precision
92"
RELATED WORK,0.20703125,"(INT4) itself already leads to significant accuracy degradation, and the accuracy drop is larger than
93"
RELATED WORK,0.208984375,"0.1 PPL points, which as discussed in the later section is sub-optimal. [7] shows the scaling law of
94"
RELATED WORK,0.2109375,"weight-only quantization with the simplest round-to-nearest baseline, but it does not consider the
95"
RELATED WORK,0.212890625,"weight-and-activation quantization and/or the above PTQ optimization methods. As can be seen
96"
RELATED WORK,0.21484375,"from Figure 1, by using PTQ optimization methods, the model quality can be significantly improved.
97"
RELATED WORK,0.216796875,"Please also see Appendix E for more detailed numbers.
98"
RELATED WORK,0.21875,"Different than existing works, our paper extensively tests the effect of (1) different quantization
99"
RELATED WORK,0.220703125,"schemes, e.g., symmetric and asymmetric quantization, (2) different PTQ methods, e.g., [36, 12],
100"
RELATED WORK,0.22265625,"(3) different model families, e.g., [28, 40], (4) different quantization coverage, e.g., weight-only
101"
RELATED WORK,0.224609375,"and weight-and-activation quantization, and (5) other discussions, e.g., the effect of quantization
102"
RELATED WORK,0.2265625,"granularity. As such, we provide a much more comprehensive understanding of post-training
103"
RELATED WORK,0.228515625,"quantization for large language models compared to the previous works.
104"
RELATED WORK,0.23046875,"3
Would different model families behave similarly on quantization?
105"
RELATED WORK,0.232421875,"There are mainly two categories of PTQ for LLMs, i.e., weight-only quantization [12] and weight-
106"
RELATED WORK,0.234375,"and-activation quantization [6, 36, 35]. In the latter, it is uniformly observed across all studies that
107"
RELATED WORK,0.236328125,"activation quantization demonstrates greater sensitivity than weight quantization. However, prior
108"
RELATED WORK,0.23828125,"research tends to concentrate on a single (family) model to emphasize the necessity of their proposed
109"
RELATED WORK,0.240234375,"quantization technique. A comprehensive and systematic evaluation of this PTQ methodology,
110"
RELATED WORK,0.2421875,"particularly the sensitivity of weight/activation quantization for varying model sizes and distinct
111"
RELATED WORK,0.244140625,"model families, has yet to be undertaken. Hence, we conduct an examination on both the OPT [40]
112"
RELATED WORK,0.24609375,"and BLOOM [28] families to elucidate the quantization sensitivity of weight and activation.
113"
RELATED WORK,0.248046875,"Table 1: Classification of quantization sensi-
tivity (or quantization loss). The sensitivity
increases from Class-1 to Class-3."
RELATED WORK,0.25,"Class
Class-1
Class-2
Class-3"
RELATED WORK,0.251953125,"PPL Degradation
≤0.1
>0.1 & ≤0.5
>0.5"
RELATED WORK,0.25390625,"Sensitivity setting. We use the zero-shot validation
114"
RELATED WORK,0.255859375,"perplexity (PPL) differential on three datasets, namely,
115"
RELATED WORK,0.2578125,"Wikitext-2 [23], PTB [22], and C4 [27], before and
116"
RELATED WORK,0.259765625,"after the quantization of these LLMs to illustrate their
117"
RELATED WORK,0.26171875,"sensitivity, as PPL is significantly correlated to zero-
118"
RELATED WORK,0.263671875,"shot/few-shot accuracy measurement [7]. Specifically,
119"
RELATED WORK,0.265625,"a higher PPL drop indicates enhanced quantization sen-
120"
RELATED WORK,0.267578125,"sitivity. For simplicity, we also categorize quantization sensitivity (or quantization loss) into three
121"
RELATED WORK,0.26953125,"different classes as depicted in Table 1. Notably, the threshold is chosen because when the model
122"
RELATED WORK,0.271484375,"size approximately doubles (e.g., 13B vs. 30B, and 30B vs. 66B), the PPL improvement is about 0.5
123"
RELATED WORK,0.2734375,"(see Table 2). The sensitivity (or loss) incrementally increases as the class number ascends. From
124"
RELATED WORK,0.275390625,"a practical standpoint, we favor lower quantization sensitivity (accuracy loss), making Class-1 the
125"
RELATED WORK,0.27734375,"optimal-loss post-training quantization.
126"
RELATED WORK,0.279296875,"We employ both symmetric and asymmetric quantization to gauge the quantization sensitivity and
127"
RELATED WORK,0.28125,"highlight the advantage of asymmetric quantization. Particularly, we implement per-row quantiza-
128"
RELATED WORK,0.283203125,"tion [12] for weight quantization and per-token quantization for activation [36].
129"
RELATED WORK,0.28515625,"Robustness of Weight-only Quantization for Large Models. The results of weight-only quanti-
130"
RELATED WORK,0.287109375,"zation in OPT and BLOOM models are summarized in Table 2. INT8 weight-only quantization,
131"
RELATED WORK,0.2890625,"either symmetric or asymmetric, results in negligible accuracy loss (less than 0.05, i.e., Class-1).
132"
RELATED WORK,0.291015625,"Consequently, for tasks oriented towards generation, FP16 weight can simply be replaced with INT8
133"
RELATED WORK,0.29296875,"weight to reduce memory usage. For INT4 quantization, the asymmetric method outperforms the
134"
RELATED WORK,0.294921875,"symmetric approach in accuracy, attributable to its superior utilization of the quantization range.
135"
RELATED WORK,0.296875,"Interestingly, larger models exhibit better tolerance to low-precision quantization (i.e., INT4) than
136"
RELATED WORK,0.298828125,"smaller models, with a few exceptions such as OPT-66B.1 Particularly, BLOOM-176B shows PPL
137"
RELATED WORK,0.30078125,"degradation (around 0.3 points) in Class-2, which could explain why the large GLM-130B [39] can
138"
RELATED WORK,0.302734375,"operate with INT4 weight-only quantization out of the box with acceptable accuracy impact.
139"
RELATED WORK,0.3046875,Table 2: Average PPL of OPT and BLOOM (BLM). See Table E.1 for all results.
RELATED WORK,0.306640625,"Precision
OPT-6.7b
OPT-13b
OPT-30b
OPT-66b
BLM-1.7b
BLM-3b
BLM-7.1b
BLM-176b"
RELATED WORK,0.30859375,"W16-A16
11.90
11.22
10.70
10.33
20.43
17.58
14.96
10.90"
RELATED WORK,0.310546875,"W8sym-A16
11.90
11.22
10.70
10.33
20.43
17.59
14.97
10.90
W8asym-A16
11.90
11.22
10.70
10.33
20.45
17.59
14.97
10.90"
RELATED WORK,0.3125,"W4sym-A16
14.36
12.73
11.77
97.05
23.18
19.36
16.27
11.28
W4asym-A16
13.44
12.09
11.52
31.52
22.47
19.01
15.90
11.20"
RELATED WORK,0.314453125,"W16-A8sym
26.04
3171.49
2048.21
2638.09
20.68
17.73
15.28
12.10
W16-A8asym
12.62
15.36
23.57
561.35
20.52
17.65
15.14
11.62"
RELATED WORK,0.31640625,"Challenge Encountered in Activation Quantization for Large Models. Activation quantization
140"
RELATED WORK,0.318359375,"has consistently proven more difficult than weight quantization [36, 6], as illustrated in Table 2. When
141"
RELATED WORK,0.3203125,"compared to weight-only quantization, activation-only quantization indicates that asymmetric quanti-
142"
RELATED WORK,0.322265625,"zation can significantly improved performance over symmetric quantization. Moreover, contrary to
143"
RELATED WORK,0.32421875,"weight-only quantization, smaller models typically exhibit better tolerance to activation quantization,
144"
RELATED WORK,0.326171875,"as their hidden dimension is smaller and the activation dynamic range is also narrower than larger
145"
RELATED WORK,0.328125,"models [36]. It should be noted that for models larger than 10B, all fall into Class-3, indicating a
146"
RELATED WORK,0.330078125,"degradation of more than 0.5 PPL points.
147"
RELATED WORK,0.33203125,"The last two rows of Table 2 show that different model families exhibit significantly different
148"
RELATED WORK,0.333984375,"behaviors. BLOOM does not exhibit divergence issues even up to a model size of 176B, whereas OPT
149"
RELATED WORK,0.3359375,"displays very poor performance from a model size of 6.7B (larger models with INT8 activation have
150"
RELATED WORK,0.337890625,"even worse PPL). This could again be attributed to the Layer Norm issue within the OPT-family1.
151"
RELATED WORK,0.33984375,"Findings 1 on Sensitivity Analysis. (1) INT8 weight-only quantization can serve as a stan-
dard method for reducing memory costs in LLMs, with negligible degradation in accuracy.
(2) INT4 weight-only quantization for small models results in substantial accuracy degra-
dation (Class-3), but this effect lessens as the model size increases (Class-2). (3) Contrary
to (2), INT8 activation results in minimal accuracy drops for small models (Class-1) but
larger models exhibit greater drops (Class-3). (4) With INT8 activation, BLOOM shows no
divergence issues up to a model size of 176B, whereas OPT performs poorly from ≥6.7B
model sizes. 152"
RELATED WORK,0.341796875,"1[12] discovered that OPT-66B has a high proportion of dead neurons in the early layers, which might
influence the compression capability. We also identify another potential reason: the Layer Norm of the OPT-
family is not well trained (except OPT-350M), with the weight and the bias being all 1’s and 0’s, respectively."
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.34375,"4
Are existing quantization methods optimally harnessing the potential to
153"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.345703125,"minimize LLMs sizes?
154"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.34765625,"Numerous lightweight optimization-based methods have been proposed, which update the model
155"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.349609375,"weights during quantization. These methods such as [36, 12, 35], unlike quantization-aware training,
156"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.3515625,"only require a small portion of the training data and a limited training time. Particularly, GPTQ [12]
157"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.353515625,"and ZeroQuant [36], have proven to be effective and efficient in terms of GPU resources, time cost,
158"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.35546875,"and data usage for INT4 weight quantization.2 In this work, we focus on the variants of GPTQ and
159"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.357421875,"ZeroQuant as well as the most straightforward baseline, round-to-nearest neighborhood (RTN).
160"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.359375,"RTN directly applies PTQ on the trained data and follows the procedure detailed in Section A to
161"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.361328125,"perform the quantization. Specifically, for symmetric quantization, we set S = max(abs(x)) and
162"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.36328125,"Z = 0; for asymmetric quantization, we set S = max(x) −min(x) and Z = min(x).
163"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.365234375,"GPTQ extends the OBQ [10]. It tries to optimize the following non-linear least square problem,
164"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.3671875,"min ˆ
W ∥Wx−ˆWx∥2
2 where W is the weight, x is the activation, and ˆW is a quantized weight. GPTQ
165"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.369140625,"employs second-order methods to obtain a closed-form solution. In addition, the quantization for each
166"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.37109375,"weight matrix is performed column-/row-wisely and the quantization errors from previous columns
167"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.373046875,"will be passed to those columns not yet quantized. See[10, 12] for more details.
168"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.375,"ZQ-Global is the original method proposed in [36], where authors treat each layer as a small neural
169"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.376953125,"network (a.k.a., subnetwork) and use the FP16 subnetwork as the teacher model to distill the quantized
170"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.37890625,"one with a few hundred iterations, i.e., minˆθ |fθ(x) −fˆθ(x)|22, where θ is a set of weights, ˆθ is
171"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.380859375,"the quantized version, fθ is the subnetwork with parameters θ, and x is the input. Thus, it can
172"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.3828125,"significantly reduce the GPU resource requirement and time cost.
173"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.384765625,"ZQ-Local is an extension mode of ZQ-Global for further GPU requirement reduction and training
174"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.38671875,"cost reduction. Particularly, instead of using each transformer layer as the subnetwork, we treat each
175"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.388671875,"linear layer as the subnetwork. This method can be viewed as an iterative first-order optimization
176"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.390625,"method (e.g., SGD) to solve min ˆ
W ∥Wx −ˆWx∥2
2.
177"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.392578125,"Experimental Setup. We compare the four methods mentioned above on weight-only and weight-
178"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.39453125,"and-activation quantization. As weight quantization is always static (i.e., it does not change during
179"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.396484375,"inference), there is virtually no system performance difference between symmetric and asymmetric
180"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.3984375,"quantization.3 We use asymmetric quantization for better accuracy, and the conclusions would hold
181"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.400390625,"similarly for symmetric quantization. For parameters used for GPTQ, ZQ-Local, and ZQ-Global,
182"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.40234375,"please refer to Appendix B. An interesting finding for ZeroQuant is that the hyperparameters (e.g.,
183"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.404296875,"learning rate and its scheduler) provided in the original work [36] are sub-optimal. In this work,
184"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.40625,"we find the best configurations for ZQ-Local and ZQ-Global and denote them as ZQ-Local∗and
185"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.408203125,"ZQ-Global∗, respectively, with the best tuned results. To ensure consistent and comparable results,
186"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.41015625,"we set a fixed random seed for our experiments. In the context of post-training quantization, varying
187"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.412109375,"the random seed has minimal impact on the final results, as indicated in more detail in Table B.1.
188"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.4140625,"Evaluation of Weight-only Quantization. The results from weight-only quantization using OPT and
189"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.416015625,"Bloom are presented in Table 3. The findings indicate that the larger models tend to be less sensitive
190"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.41796875,"to INT4 weight-only quantization. This observation holds true across all methods (RTN, GPTQ,
191"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.419921875,"ZQ-Local∗, and ZQ-Global∗) with the exception of OPT-66B, which shows greater degradation than
192"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.421875,"OPT-30B. It is noteworthy that light-weight optimization-based methods significantly outperform the
193"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.423828125,"RTN baseline in terms of accuracy. For instance, these methods substantially reduce the degradation
194"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.42578125,"in perplexity of OPT-30B/66B compared to baseline. Most quantized models with parameters greater
195"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.427734375,"than 6.7B fall under Class II, indicating their potential for real-world applications. For instance, the
196"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.4296875,"quality of INT4 OPT-30B (66B) is superior to that of INT8 OPT-13B (30B).
197"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.431640625,"Among the optimization-based methods, ZQ-Global∗generally performs better on smaller models
198"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.43359375,"(those with fewer than 1B parameters), while GPTQ excels on larger models. ZQ-Local∗does not
199"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.435546875,"outperform GPTQ or ZQ-Global∗-— a reasonable outcome given that GPTQ employs a closed-form
200"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.4375,"solution to solve the non-linear quadratic problem and ZQ-Global∗optimizes a larger subnetwork.
201"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.439453125,"The inferior performance of ZQ-Global∗compared to GPTQ for larger models is unexpected since
202"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.44140625,"ZQ-Global∗optimizes an entire transformer layer while GPTQ only optimizes a single linear layer.
203"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.443359375,"2We tested the method proposed by [35] but did not find it better than others for INT4 weight quantization.
3The bias term (a.k.a., the zero point) can be simply fused into the previous activation quantization kernel [36]."
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.4453125,"Table 3: The evaluation results of different PTQ methods on OPT and BLOOM (BLM) with
asymmmetric quantization on weight or (and) activation. See more details in Table E.3 and Table E.6."
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.447265625,"Precision
Method
OPT-6.7b
OPT-13b
OPT-30b
OPT-66b
BLM-1.7b
BLM-3b
BLM-7.1b
BLM-176b"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.44921875,"W16A16
11.90
11.22
10.70
10.33
20.43
17.58
14.96
10.90 W4A16"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.451171875,"RTN
13.44
12.09
11.52
31.52
22.47
19.01
15.90
11.20
GPTQ
12.28
11.42
10.78
10.52
21.58
18.33
15.50
11.02
ZQ-Local∗
12.46
11.64
11.05
10.79
21.70
18.50
15.55
11.11
ZQ-Global∗
12.38
11.62
11.04
10.68
21.38
18.33
15.52
11.05 W4A8"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.453125,"RTN
14.80
26.36
86.26
815.00
22.75
19.17
16.19
12.22
GPTQ
13.88
17.28
20.71
648.69
21.71
18.44
15.75
11.86
ZQ-Local∗
13.24
14.23
18.53
16.32
21.86
18.66
15.75
11.19
ZQ-Global∗
13.17
13.07
14.65
37.82
21.43
18.39
15.58
11.49"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.455078125,"A plausible explanation is that larger models are more sensitive to weight updates, necessitating more
204"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.45703125,"advanced fine-tuning methods.
205"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.458984375,"Evaluation of Weight and Activation Quantization. The evaluation results for existing methods
206"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.4609375,"using W4A8 quantization are presented in Table 3. The three light-weight optimization-based
207"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.462890625,"methods outperform RTN significantly, underscoring their efficacy. However, all of the results fall
208"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.46484375,"into either Class-2 or Class-3. This suggests that for certain applications, it might be more beneficial
209"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.466796875,"to use smaller models with fewer parameters rather than larger, quantized models.
210"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.46875,"Among quantization-based methods, ZQ-Global∗and ZQ-Local∗generally outperform GPTQ, which
211"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.470703125,"is anticipated given that GPTQ was originally designed for weight-only quantization. ZQ-Global∗
212"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.47265625,"performs better than ZQ-Local∗in most cases except for the two largest models, OPT-66B and
213"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.474609375,"Bloom-176B, despite having larger trainable parameters in one step. This again signifies the need for
214"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.4765625,"a more suitable and advanced optimization method for large language models (LLMs).
215"
ARE EXISTING QUANTIZATION METHODS OPTIMALLY HARNESSING THE POTENTIAL TO,0.478515625,"Finding 2 on Comparisons. (1) GPTQ typically performs better for weight-only quantization,
while ZeroQuant (including both ZQ-Global∗and ZQ-Local∗) yields superior results for
weight and activation quantization. (2) The tested optimization-based methods cannot achieve
Class-1 quantization error for either INT4 weight-only or W4A8 quantization with the
exception of GPTQ on OPT-30B with weight-only quantization. 216"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.48046875,"4.1
Fine-grained Quantization and Its Evaluation
217"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.482421875,"With PTQ and row-wise quantization, achieving Class-1 quantization error is challenging for both
218"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.484375,"weight-only and weight-and-activation quantization. Generally, utilizing a smaller model with INT8
219"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.486328125,"weight is more advantageous than employing a model that is twice as large with INT4 weight.
220"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.48828125,"One potential solution to this issue is the implementation of finer-grained quantization schemes [5],
221"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.490234375,"where every k elements possess their own scaling factor and/or zero point. This approach can
222"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.4921875,"significantly reduce quantization error. In the extreme case, where every single element has its own
223"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.494140625,"scaling factor, the original FP16 number can be precisely recovered. Importantly, block-k quantization
224"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.49609375,"can be implemented on modern GPUs, one of the most prevalent deep learning architectures, since
225"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.498046875,"the compute unit (streaming multiprocessor) of GPUs processes tiles of data (e.g., 128 by 128 tiling
226"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.5,"size) for matrix computation.
227"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.501953125,"Although fine-grained quantization can substantially narrow the gap between the quantized tensor
228"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.50390625,"and its floating-point counterpart, the application of RTN still results in a non-trivial accuracy gap.
229"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.505859375,"Consequently, we build upon fine-grained quantization by employing existing optimization-based
230"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.5078125,"methods to further enhance accuracy. Specifically, we utilize GPTQ and ZQ-Global for all models
231"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.509765625,"and settings and apply ZQ-Local to OPT-66B and Bloom-176B. For the hyperparameters used in
232"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.51171875,"ZQ-Global and ZQ-Local, we select the top three identified in Section 4 for all models, except for
233"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.513671875,"Bloom-176B, for which we only use the top-performing hyperparameter to reduce training costs.
234"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.515625,"4-bit Weight Quantization.
We hereby present the W4A16 results for OPT and BLOOM, as
235"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.517578125,"delineated in Table 4, corresponding to an array of quantization block sizes. The performance
236"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.51953125,"sees a significant improvement with smaller block sizes compared to per-row quantization. The
237"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.521484375,"point of diminishing returns, however, varies for different model sizes. For example, smaller mod-
238"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.5234375,"els (such as OPT-6.7B and BLOOM-1.7b) continue to see substantial gains until the block size
239"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.525390625,"reduces to 32. In contrast, for larger models (those exceeding 10B, with OPT-66B as the excep-
240"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.52734375,"Table 4: Results of W4asym-A16 quantization with various block-size out of the best result from
optimization-based methods on OPT and BLOOM (BLM). See Table E.15 and Table E.16 for full
results including RTN. N/A means that the block size is not divisible by the hidden size."
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.529296875,"Block-size
OPT-6.7b
OPT-13b
OPT-30b
OPT-66b
BLM-1.7b
BLM-3b
BLM-7.1b
BLM-176b"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.53125,"W16A16
11.90
11.22
10.70
10.33
20.43
17.58
14.96
10.90
Per-row
12.28
11.42
10.78
10.52
21.38
18.33
15.50
11.02"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.533203125,"1024
12.16
11.36
10.75
10.52
31.03
N/A
15.24
10.96
512
12.08
11.32
10.73
10.52
20.93
17.99
15.20
10.95
256
12.05
11.28
10.74
10.50
20.95
17.97
15.18
10.95
128
12.10
11.28
10.74
10.44
20.92
17.90
15.17
10.94
32
12.03
11.28
10.72
10.41
20.82
17.88
15.16
10.95"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.53515625,"Table 5: OPT W4asym-A8 with various block-size out of the best result from GPTQ, ZQ-Local, and
ZQ-Global on OPT and BLOOM (BLM). See Table E.20 for full results including RTN."
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.537109375,"Precision
block-size (W|A)
OPT-6.7b
OPT-13b
OPT-30b
OPT-66b
BLM-1.7b
BLM-3b
BLM-7.1b
BLM-176b"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.5390625,"W4A16
128 | NA
12.10
11.28
10.74
10.44
20.92
17.90
15.17
10.94"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.541015625,"W4A8
Case-1: per-row | per-row
13.17
13.07
14.65
16.32
21.43
18.39
15.58
11.19
Case-2: per-row | 128
12.29
11.45
10.80
10.61
21.59
18.31
15.52
11.03
Case-3: 128 | 128
12.04
11.31
10.75
10.45
21.27
17.86
15.19
10.96"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.54296875,"tion), the benefits derived from smaller block sizes wane rapidly around block-256/512. Most
241"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.544921875,"crucially, for models equal to or larger than 13B, a smaller quantization block size results in quanti-
242"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.546875,"zation error being classified under Class-1, indicating virtually negligible degradation in accuracy.
243"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.548828125,"Table 6: BLOOM-176B with different quan-
tization block sizes on activation.
Here
weight is asymmetrically quantized with
block size 128. See more in Table E.22."
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.55078125,"A8 Block Size
1024
512
256
128
32"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.552734375,"PPL
10.98
10.97
10.95
10.95
10.95 244"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.5546875,"Activation Quantization (W4A8). To comprehend
245"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.556640625,"the benefits of fine-grained quantization on activation,
246"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.55859375,"we analyze the quantization between per-row and a
247"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.560546875,"block size of 128, with INT4 weight, as highlighted in
248"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.5625,"Table 5. For models of considerable size, specifically
249"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.564453125,"those equal to or exceeding 1B, the application of such
250"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.56640625,"fine-grained activation quantization (Case-1) results in a
251"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.568359375,"substantial reduction in quantization error compared to per-row activation (Case-2). By implementing
252"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.5703125,"fine-grained activation quantization with weight quantization (Case-3), we are able to almost restore
253"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.572265625,"the performance to the level of their W4A16 counterparts.
254"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.57421875,"Furthermore, we detail the impacts of varying activation quantization block sizes in Table 6 on
255"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.576171875,"BLOOM-176B, with INT4 weight. A trend of superior accuracy is observed with smaller block
256"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.578125,"sizes in contrast to larger ones. However, the enhancement in performance reaches a saturation point
257"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.580078125,"when the size smaller or equal to 256, which corresponds to the range of values INT8 can represent.
258"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.58203125,"Despite INT8’s capability to signify 256 distinct values, activation quantization errors persist due to
259"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.583984375,"the application of uniform quantization.
260"
FINE-GRAINED QUANTIZATION AND ITS EVALUATION,0.5859375,"Finding 3 on FGQ. (1) Larger models (≥10B) are capable of attaining Class-1 error for 4-bit
quantization. These models can leverage low-precision quantization as the model size with
INT4 is similar to an INT8 model that is half its size, with improved accuracy. On the other
hand, smaller models (≤10B) typically reach only Class-2 or Class-3 error levels. (2) For
larger models (>10B), the difference between fine-grained weight-and-activation quantization
and fine-grained weight-only quantization is insignificant. (3) The advantage of fine-grained
activation quantization fades for larger models when the block size reaches 256. 261"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.587890625,"5
Proposed Method to Further Push the Limit of Post-training Quantization
262"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.58984375,"Building on the investigation and conclusions drawn from previous sections, it has become apparent
263"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.591796875,"that there is still a need for an advanced methodology to further refine the existing methods, with
264"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.59375,"the objective of fully realizing the original fp16 PPL quality. In this section, we introduce a simple
265"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.595703125,"yet effective method called LoRC (Low Rank Compensation) to optimize the current existing
266"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.59765625,"quantization error and further bridge the gap between the quality of the original model and its
267"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.599609375,"quantized counterparts.
268"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.6015625,"Table 7: W#asym-A16 quantization with # being 4-bit, 3-bit and 2-bit on OPT and BLOOM (BLM)."
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.603515625,"Bits
LoRC
Coarse-grained weight quantization (per-row block-size)
Fine-grained quantization on weight (256 block-size )
OPT-6.7b
OPT-13b
OPT-30b
OPT-66b
BLM-176b
OPT-6.7b
OPT-13b
OPT-30b
OPT-66b
BLM-176b"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.60546875,"W8A16
11.90
11.22
10.70
10.33
10.90
11.90
11.22
10.70
10.33
10.90"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.607421875,"W4A16
✗
12.28
11.42
10.78
10.78
11.02
12.05
11.28
10.74
10.50
10.95
✓
12.10
11.36
10.76
10.34
10.98
11.99
11.29
10.70
10.29
10.93"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.609375,"W3A16
✗
14.18
12.43
11.28
17.77
49.46
12.79
11.63
10.9
11.34
11.13
✓
13.00
11.90
11.14
10.63
11.30
12.40
11.57
10.83
10.42
11.08"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.611328125,"W2A16
✗
120.56
40.17
25.74
225.45
Explode
23.13
15.55
12.68
308.49
12.64
✓
24.17
18.53
14.39
13.01
14.15
16.27
14.30
12.37
11.54
12.21"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.61328125,"LoRC is inspired by the employment of low-rank matrix factorization on the quantization error matrix
269"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.615234375,"E := W −ˆW, where W represents the original weight and ˆW is the quantized weight. LoRC
270"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.6171875,"approximates the error E with ˆE = ˆU ˆV by using two low-rank matrices ˆU and ˆV . This results in a
271"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.619140625,"more accurate approximation of the original weight matrix W by ˆWlorc = ˆW + ˆE, thereby reducing
272"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.62109375,"quantization errors: ∥W −ˆW∥≥∥W −ˆWlorc∥. LoRC consists of two steps:
273"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.623046875,"Step I: Implement Singular Value Decomposition (SVD) on the error matrix E = UΣV , where
274"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.625,"U ∈Rdin×din and V ∈Rdout×dout are unitary matrices, and Σ ∈Rdin×dout is a diagonal matrix with its
275"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.626953125,"diagonal elements ordered in a descending manner.
276"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.62890625,"Step II: We formulate the matrix ˆE = ˆU ˆV where ˆU = Um(Σm)
1
2 and ˆV = (Σm)
1
2 Vm. Here,
277"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.630859375,"Um = U:,1:m ∈Rdin×m, Vm = V1:m,: ∈Rm×dout, and Σm = Σ1:m,1:m ∈Rm×m.
278"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.6328125,"The objective of LoRC is to achieve a good approximation of the error matrix E using low-rank
279"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.634765625,"matrices, with minimal impact on the increase in model size. For instance, consider the standard
280"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.63671875,"transformer models [32], where each layer is comprised of a multi-headed attention (MHA) module
281"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.638671875,"and a multi-linear perception (MLP) module. Let h represent the hidden dimension and l the number
282"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.640625,"of layers. The total number of parameters is 12lh2 as each layer contains 4h2 for MHA (for key,
283"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.642578125,"query, value, and projection matrices), and 8h2 for MLP (two matrices of sizes h × 4h and 4h × h).
284"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.64453125,"With the addition of low-rank LoRC to the six matrices in each layer, the total number of parameters
285"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.646484375,"for l layers would amount to 18hml.4 Consequently, the ratio of parameters added to the existing
286"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.6484375,"model is 3m/2h. It’s important to note that the low-rank dimension m can be as small as 4 or 8
287"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.650390625,"(which we will discuss in detail in a later section) while the standard hidden dimension h ≥768,
288"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.65234375,"making the number 3m/2h ≤0.016.
289"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.654296875,"Significantly, LoRC can be viewed as a supplementary feature to existing quantization methodologies
290"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.65625,"such as RTN, GPTQ, and ZeroQuant-local/Global, and can be seamlessly integrated with FGQ.
291"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.658203125,"We have conducted experiments to evaluate the performance of LoRC on both OPT and BLOOM,
292"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.66015625,"applying 4-bit, 3-bit, and 2-bit weights by setting the activation to FP16.5 Based on the discoveries
293"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.662109375,"in the preceding sections, we utilize the GPTQ quantization strategy. To gain a comprehensive
294"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.6640625,"understanding of LoRC, we include the results with and without the application of FGQ. The datasets
295"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.666015625,"and hyperparameters are consistent with those detailed in earlier sections.
296"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.66796875,"Table 8: Results of W4asym A16 quantization
with LoRC approximating ˆE = ˆU ˆV on OPT
model family. ˆU and ˆV can be represented with
FP16 or INT8, of which the performance are rep-
resented below. There is hardly any difference
between FP16 and INT8."
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.669921875,"LoRC
Coarse-grained weight quantization
Fain-grained weight Quantization
ˆU, ˆV
6.7b
13b
30b
66b
6.7b
13b
30b"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.671875,"FP16
12.08
11.35
10.76
10.31
11.993
11.290
10.703
INT8
12.10
11.36
10.76
10.34
11.987
11.290
10.700"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.673828125,"Evaluation Results. The findings are showcased in
297"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.67578125,"Table 7, split into two sections: coarse-grained weight
298"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.677734375,"quantization (per-row) and fine-grained quantization
299"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.6796875,"(block-size 256). Notably, we observe that the two
300"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.681640625,"low-rank matrices, ˆU and ˆV , can be quantized to 8-bit
301"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.68359375,"without any performance discrepancy (Table 8). Thus,
302"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.685546875,"the two low-rank matrices for LoRC in Table 7 are
303"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.6875,"INT8 with a low-rank dimension of m = 8.
304"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.689453125,"Several key observations can be made. Firstly, LoRC
305"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.69140625,"consistently boosts performance across all bit sizes and block sizes, as indicated by the lower
306"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.693359375,"perplexity scores when LoRC is activated. Secondly, the enhancement brought about by LoRC
307"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.6953125,"becomes more substantial as the bit size diminishes, especially noticeable for W2A16, which
308"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.697265625,"displays a markedly greater impact compared to W4A16 and W3A16 in most scenarios. Lastly, the
309"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.69921875,"4In the MHA module, LoRC contributes 2hm to each of key, query, value, and the projection matrices. In the
MLP module, LoRC contributes 8hm and 2hm respectively to the matrices of dimensions h × 4h and 4h × h.
5For INT8 Activation, please see Table E.23, the observation for FP16 holds similarly for INT8 Activation."
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.701171875,"combination of fine-grained quantization with LoRC yields the most impressive results, underscoring
310"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.703125,"the efficacy of LoRC when integrated with FGQ. Overall, the results emphasize the benefits of using
311"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.705078125,"LoRC for enhanced performance in weight quantization and its compatibility with FGQ. Notably,
312"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.70703125,"recovering the last 0.05-0.1 perplexity can be challenging, but with LoRC, we are able to nearly
313"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.708984375,"recover the original model quality for INT4 quantization.
314"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.7109375,"Table 9: W4A16 quantization with LoRC
by varying the low-rank dimension m."
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.712890625,"LoRC-dim m
OPT-1.3b
OPT-6.7b
OPT-30b"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.71484375,"m = 0 basline
15.95
12.06
10.73"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.716796875,"m = 1
15.93
12.01
10.73
m = 4
15.73
12.00
10.72
m = 8
15.76
11.99
10.70
m = 16
15.74
12.00
10.69
m = 32
15.71
12.01
10.69
Figure 2: Eigenvalues of the Error matrix E for W4A16"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.71875,"Ablation Study on the Low Rank Dimension m. An essential aspect of the LoRC method is on the
315"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.720703125,"optimal low-rank dimension, denoted as m, explained in Step II. To explore this, we varied m in the
316"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.72265625,"range of 1, 4, 8, 16, and 32 for OPT-1.3b/6.7b/30b models, and applied W4A16 GPTQ quantization.
317"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.724609375,"The outcomes are depicted in Table 9, indicating that the enhancements achieved through LoRC
318"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.7265625,"begin to plateau as the dimension m surpasses 4. The most optimal performance for OPT-6.7b is
319"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.728515625,"realized when m = 8.
320"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.73046875,"This observation may seem counterintuitive initially, as one might anticipate that larger LoRC
321"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.732421875,"dimensions would yield more significant improvements. To gain a more comprehensive understanding,
322"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.734375,"we conducted an analysis of the eigenvalues of the actual error matrix E = W −ˆW for each matrix.
323"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.736328125,"By randomly selecting 20 matrices from MHA and MLP layers, we plotted the eigenvalues of E as a
324"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.73828125,"curve, depicted in Figure 2. The two plots reveal a rapid flattening of eigenvalues after index 8, which
325"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.740234375,"elucidates why increasing the LoRC dimension does not considerably enhance performance. Hence,
326"
PROPOSED METHOD TO FURTHER PUSH THE LIMIT OF POST-TRAINING QUANTIZATION,0.7421875,"a sensible dimension for ˆU and ˆV in the LoRC methodology could be 8.6
327"
DISCUSSION,0.744140625,"6
Discussion
328"
DISCUSSION,0.74609375,"Conclusion.
In this work, we provide a comprehensive study of post-training quantization (PTQ) on
329"
DISCUSSION,0.748046875,"large language models with different PTQ methods (e.g., RTN, GPTQ, ZeroQuant), and with different
330"
DISCUSSION,0.75,"quantization coverage (weight-only and weight-and-activation quantization), etc. We find that PTQ
331"
DISCUSSION,0.751953125,"methods are critical to improving the quantized model quality, and that fine-grained quantization
332"
DISCUSSION,0.75390625,"(FGQ) can bring acceptable accuracy and model size trade-off. Finally, we introduced an optimization
333"
DISCUSSION,0.755859375,"technique called Low Rank Compensation (LoRC), which works synergistically with PTQ and FGQ,
334"
DISCUSSION,0.7578125,"playing a crucial role in enhancing full model quality recovery with a minimal increase in model size.
335"
DISCUSSION,0.759765625,"Limitation.
Despite quantizing over 10,000 experiments, our study was constrained by our com-
336"
DISCUSSION,0.76171875,"puting resources. This restriction made us choose between diversifying the model sizes and varying
337"
DISCUSSION,0.763671875,"the tasks. We strategically limited our datasets to WikiText, PTB, and C4 to concentrate on a broad
338"
DISCUSSION,0.765625,"range of quantization methods. Consequently, our general findings are more robust concerning the
339"
DISCUSSION,0.767578125,"two model families and three datasets examined in this paper. However, caution should be exercised
340"
DISCUSSION,0.76953125,"when generalizing these findings to tasks that are dissimilar to those covered in this study.
341"
DISCUSSION,0.771484375,"Future Opportunity.
Throughout the paper, we see several unresolved problems from current
342"
DISCUSSION,0.7734375,"quantization schemes and/or algorithms, and we find potential directions for LLM compression: (1)
343"
DISCUSSION,0.775390625,"Although we use fine-grained quantization schemes in the paper, the real implementation is missing.
344"
DISCUSSION,0.77734375,"How to efficiently implement odd bit precision is also challenging. [12] demonstrated that 3-bit can
345"
DISCUSSION,0.779296875,"achieve better throughput in the generation phase by packing all 3-bit numbers in continuous memory
346"
DISCUSSION,0.78125,"space. However, this method is sub-optimal as the dequantization step needs to connect bits from
347"
DISCUSSION,0.783203125,"different bytes. One possible way to implement odd bits, e.g., 5 bits, is to use two integer matrices
348"
DISCUSSION,0.78515625,"with INT4 and INT1. During the dequantization stage, we couple the two matrices together. (2) How
349"
DISCUSSION,0.787109375,"to combine PTQ with other lightweight compression techniques, e.g., post-training pruning [20, 11],
350"
DISCUSSION,0.7890625,"is an interesting direction to further reduce the memory consumption and compute cost.
351"
DISCUSSION,0.791015625,"6Please note that this observation is only true for PTQ. If one uses quantize-aware training (QAT) and let ˆU
and ˆV updated during QAT, we arrive at contrasting conclusions. For more details, please refer to Appendix D."
REFERENCES,0.79296875,"References
352"
REFERENCES,0.794921875,"[1] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and
353"
REFERENCES,0.796875,"Irwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701,
354"
REFERENCES,0.798828125,"2020.
355"
REFERENCES,0.80078125,"[2] Big-Science. Bloom inference. https://github.com/huggingface/transformers-blo
356"
REFERENCES,0.802734375,"om-inference/tree/main/bloom-inference-scripts, 2022.
357"
REFERENCES,0.8046875,"[3] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming
358"
REFERENCES,0.806640625,"the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021.
359"
REFERENCES,0.80859375,"[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
360"
REFERENCES,0.810546875,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
361"
REFERENCES,0.8125,"few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
362"
REFERENCES,0.814453125,"[5] Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov,
363"
REFERENCES,0.81640625,"Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, et al. Pushing the limits of
364"
REFERENCES,0.818359375,"narrow precision inferencing at cloud scale with microsoft floating point. Advances in neural
365"
REFERENCES,0.8203125,"information processing systems, 33:10271–10281, 2020.
366"
REFERENCES,0.822265625,"[6] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix
367"
REFERENCES,0.82421875,"multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.
368"
REFERENCES,0.826171875,"[7] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws.
369"
REFERENCES,0.828125,"arXiv preprint arXiv:2212.09720, 2022.
370"
REFERENCES,0.830078125,"[8] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar-
371"
REFERENCES,0.83203125,"mendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.
372"
REFERENCES,0.833984375,"[9] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou,
373"
REFERENCES,0.8359375,"and Armand Joulin. Training with quantization noise for extreme fixed-point compression.
374"
REFERENCES,0.837890625,"arXiv preprint arXiv:2004.07320, 2020.
375"
REFERENCES,0.83984375,"[10] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-
376"
REFERENCES,0.841796875,"training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022.
377"
REFERENCES,0.84375,"[11] Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot.
378"
REFERENCES,0.845703125,"arXiv preprint arXiv:2301.00774, 2023.
379"
REFERENCES,0.84765625,"[12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training
380"
REFERENCES,0.849609375,"quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.
381"
REFERENCES,0.8515625,"[13] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.
382"
REFERENCES,0.853515625,"A survey of quantization methods for efficient neural network inference.
arXiv preprint
383"
REFERENCES,0.85546875,"arXiv:2103.13630, 2021.
384"
REFERENCES,0.857421875,"[14] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer. Ai and
385"
REFERENCES,0.859375,"memory wall. RiseLab Medium Post, 2021.
386"
REFERENCES,0.861328125,"[15] GitHub. Github copilot. https://github.com/features/copilot/, 2021.
387"
REFERENCES,0.86328125,"[16] Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain
388"
REFERENCES,0.865234375,"surgeon. In Advances in neural information processing systems, pages 164–171, 1993.
389"
REFERENCES,0.8671875,"[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.
390"
REFERENCES,0.869140625,"Workshop paper in NIPS, 2014.
391"
REFERENCES,0.87109375,"[18] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and
392"
REFERENCES,0.873046875,"Qun Liu.
Tinybert: Distilling bert for natural language understanding.
arXiv preprint
393"
REFERENCES,0.875,"arXiv:1909.10351, 2019.
394"
REFERENCES,0.876953125,"[19] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert:
395"
REFERENCES,0.87890625,"Integer-only bert quantization. In International conference on machine learning, pages 5506–
396"
REFERENCES,0.880859375,"5518. PMLR, 2021.
397"
REFERENCES,0.8828125,"[20] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer, and
398"
REFERENCES,0.884765625,"Amir Gholami. A fast post-training pruning framework for transformers. arXiv preprint
399"
REFERENCES,0.88671875,"arXiv:2204.09656, 2022.
400"
REFERENCES,0.888671875,"[21] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
401"
REFERENCES,0.890625,"information processing systems, pages 598–605, 1990.
402"
REFERENCES,0.892578125,"[22] Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank.
403"
REFERENCES,0.89453125,"Using Large Corpora, page 273, 1994.
404"
REFERENCES,0.896484375,"[23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
405"
REFERENCES,0.8984375,"models. In International Conference on Learning Representations, 2017.
406"
REFERENCES,0.900390625,"[24] OpenAI. Openai chatgpt. https://openai.com/blog/chatgpt/, 2022.
407"
REFERENCES,0.90234375,"[25] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and
408"
REFERENCES,0.904296875,"quantization. arXiv preprint arXiv:1802.05668, 2018.
409"
REFERENCES,0.90625,"[26] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm
410"
REFERENCES,0.908203125,"Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling
411"
REFERENCES,0.91015625,"transformer inference. arXiv preprint arXiv:2211.05102, 2022.
412"
REFERENCES,0.912109375,"[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
413"
REFERENCES,0.9140625,"Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
414"
REFERENCES,0.916015625,"text-to-text transformer, 2019.
415"
REFERENCES,0.91796875,"[28] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,
416"
REFERENCES,0.919921875,"Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A
417"
REFERENCES,0.921875,"176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,
418"
REFERENCES,0.923828125,"2022.
419"
REFERENCES,0.92578125,"[29] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W
420"
REFERENCES,0.927734375,"Mahoney, and Kurt Keutzer. Q-BERT: Hessian based ultra low precision quantization of bert.
421"
REFERENCES,0.9296875,"In AAAI, pages 8815–8821, 2020.
422"
REFERENCES,0.931640625,"[30] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari,
423"
REFERENCES,0.93359375,"Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using
424"
REFERENCES,0.935546875,"deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language
425"
REFERENCES,0.9375,"model. arXiv preprint arXiv:2201.11990, 2022.
426"
REFERENCES,0.939453125,"[31] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai
427"
REFERENCES,0.94140625,"Wong. Compression of generative pre-trained language models via quantization. arXiv preprint
428"
REFERENCES,0.943359375,"arXiv:2203.10705, 2022.
429"
REFERENCES,0.9453125,"[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
430"
REFERENCES,0.947265625,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
431"
REFERENCES,0.94921875,"processing systems, pages 5998–6008, 2017.
432"
REFERENCES,0.951171875,"[33] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding
433"
REFERENCES,0.953125,"int4 quantization for transformer models: Latency speedup, composability, and failure cases.
434"
REFERENCES,0.955078125,"arXiv preprint arXiv:2301.12017, 2023.
435"
REFERENCES,0.95703125,"[34] Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme compression
436"
REFERENCES,0.958984375,"for pre-trained transformers made simple and efficient. arXiv preprint arXiv:2206.01859, 2022.
437"
REFERENCES,0.9609375,"[35] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant:
438"
REFERENCES,0.962890625,"Accurate and efficient post-training quantization for large language models. arXiv preprint
439"
REFERENCES,0.96484375,"arXiv:2211.10438, 2022.
440"
REFERENCES,0.966796875,"[36] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong
441"
REFERENCES,0.96875,"He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.
442"
REFERENCES,0.970703125,"arXiv preprint arXiv:2206.01861, 2022.
443"
REFERENCES,0.97265625,"[37] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. Gobo: Quantizing
444"
REFERENCES,0.974609375,"attention-based nlp models for low latency and energy efficient inference. In 2020 53rd Annual
445"
REFERENCES,0.9765625,"IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 811–824. IEEE,
446"
REFERENCES,0.978515625,"2020.
447"
REFERENCES,0.98046875,"[38] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT: Quantized 8bit bert.
448"
REFERENCES,0.982421875,"arXiv preprint arXiv:1910.06188, 2019.
449"
REFERENCES,0.984375,"[39] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,
450"
REFERENCES,0.986328125,"Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv
451"
REFERENCES,0.98828125,"preprint arXiv:2210.02414, 2022.
452"
REFERENCES,0.990234375,"[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
453"
REFERENCES,0.9921875,"Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
454"
REFERENCES,0.994140625,"transformer language models. arXiv preprint arXiv:2205.01068, 2022.
455"
REFERENCES,0.99609375,"[41] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert:
456"
REFERENCES,0.998046875,"Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812, 2020.
457"
