Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004608294930875576,"Vision transformers have delivered tremendous success in representation learning.
This is primarily due to effective token mixing through self-attention. However,
this scales quadratically with the number of pixels, which becomes infeasible for
high-resolution inputs. To cope with this challenge, we propose Adaptive Fourier
Neural Operator (AFNO) as an efﬁcient token mixer that learns to mix in the
Fourier domain. AFNO is based on a principled foundation of operator learning
which allows us to frame token mixing as a continuous global convolution with-
out any dependence on the input resolution. This principle was previously used
to design FNO, which solves global convolution efﬁciently in the Fourier domain
and has shown promise in learning challenging PDEs. To handle challenges in vi-
sual representation learning such as discontinuities in images and high resolution
inputs, we propose principled architectural modiﬁcations to FNO which results in
memory and computational efﬁciency. This includes imposing a block-diagonal
structure on the channel mixing weights, adaptively sharing weights across to-
kens, and sparsifying the frequency modes via soft-thresholding and shrinkage.
The resulting model is highly parallel with a quasi-linear complexity and has lin-
ear memory in the sequence size. AFNO outperforms self-attention mechanisms
for few-shot segmentation in terms of both efﬁciency and accuracy. For Cityscapes
segmentation with the Segformer-B3 backbone, AFNO can handle a sequence size
of 65k and outperforms other self-attention mechanisms. Code is available1."
INTRODUCTION,0.009216589861751152,"1
INTRODUCTION"
INTRODUCTION,0.013824884792626729,"Figure 1: Parameter count and mIoU for Segformer,
Swin, and other models at different scales. AFNO con-
sistently outperforms other mixers (see Section 5.7)."
INTRODUCTION,0.018433179723502304,"Vision
transformers
have
recently
shown
promise in producing rich contextual represen-
tations for recognition and generation tasks.
However, a major challenge is posed by long
sequences from high resolution images and
videos. Here, long-range and multiway depen-
dencies are crucial to understand the composi-
tionality and relationships among the objects in
a scene. A key component for the effective-
ness of transformers is attributed to proper mix-
ing of tokens. Finding a good mixer is how-
ever challenging as it needs to scale with the
sequence size, and systematically generalize to
downstream tasks."
INTRODUCTION,0.02304147465437788,"Recently, there has been extensive research to
ﬁnd good token mixers; see e.g., Tay et al.
(2020b) and references therein.
The origi-
nal self-attention imposes graph structures, and
uses the similarity among the tokens to capture"
INTRODUCTION,0.027649769585253458,"∗Joint ﬁrst authors, contributed equally. The ﬁrst author has done this work during internship at NVIDIA,
and the second author was leading the project. 1 Code: github.com/jtguibas/AdaptiveFourierNeuralOperator."
INTRODUCTION,0.03225806451612903,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03686635944700461,"Figure 2: The multi-layer transformer network with FNO, GFN, and AFNO mixers. GFNet performs element-
wise matrix multiplication with separate weights across channels (k). FNO performs full matrix multiplica-
tion that mixes all the channels. AFNO performs block-wise channel mixing using MLP along with soft-
thresholding. The symbols h, w, d, and k refer to the height, width, channel size, and block count, respectively."
INTRODUCTION,0.041474654377880185,"Models
Complexity (FLOPs)
Parameter Count
Interpretation
Self-Attention
N 2d + 3Nd2
3d2
Graph Global Conv.
GFN
Nd + Nd log N
Nd
Depthwise Global Conv.
FNO
Nd2 + Nd log N
Nd2
Global Conv.
AFNO (ours)
Nd2/k + Nd log N
(1 + 4/k)d2 + 4d
Adaptive Global Conv."
INTRODUCTION,0.04608294930875576,"Table 1:
Complexity, parameter count, and interpretation for FNO, AFNO, GFN, and Self-Attention. N :=
hw, d, and k refer to the sequence size, channel size, and block count, respectively."
INTRODUCTION,0.05069124423963134,"the long-range dependencies Vaswani et al. (2017); Dosovitskiy et al. (2020) . It is parameter efﬁ-
cient and adaptive, but suffers from a quadratic complexity in the sequence size. To achieve efﬁcient
mixing with linear complexity, several approximations have been introduced for self-attention; see
Section 2. These approximations typically compromise accuracy for the sake of efﬁciency. For
instance, long-short (LS) transformer aggregates a long-range attention with dynamic projection to
model distant correlations and a short-term attention to capture local correlations Zhu et al. (2021).
Long range dependencies are modeled in low dimensions, which can limit expressiveness."
INTRODUCTION,0.055299539170506916,"More recently, alternatives have been introduced for self-attention that relax the graph assumption
for efﬁcient mixing. Instead, they leverage the geometric structures using Fourier transform Rao
et al. (2021); Lee-Thorp et al. (2021). For instance, the Global Filter Networks (GFN) proposes
depthwise global convolution for token mixing that enjoys an efﬁcient implementation in the Fourier
domain Rao et al. (2021). GFN mainly involves three steps: (i) spatial token mixing via fast Fourier
transform (FFT); (ii) frequency gating; and (iii) inverse FFT for token demixing. GFN however
lacks adaptivity and expressiveness at high resolutions since the parameter count grows with the
sequence size, and no channel mixing is involved in (ii)."
INTRODUCTION,0.059907834101382486,"Our Approach. To address these shortcomings, we frame token mixing as operator learning that
learns mappings between continuous functions in inﬁnite dimensional spaces. We treat tokens as
continuous elements in the function space, and model token mixing as continuous global convolu-
tion, which captures global relationships in the geometric space. One way to solve global convolu-
tion efﬁciently is through FFT. More generally, we compose such global convolution operations with
nonlinearity such as ReLU to learn any general non-linear operator. This forms the basis for design-
ing Fourier Neural operators (FNOs) which has shown promise in solving PDEs Li et al. (2020a).
We thus adopt FNO as a starting point for designing efﬁcient token mixing."
INTRODUCTION,0.06451612903225806,"Designing AFNO. Adapting FNO from PDEs to vision needs several design modiﬁcations. Images
have high-resolution content with discontinuities due to edges and other structures. The channel
mixing in standard FNO incurs a quadratic complexity in the channel size. To control this com-"
INTRODUCTION,0.06912442396313365,Published as a conference paper at ICLR 2022
INTRODUCTION,0.07373271889400922,"plexity, we impose a block-diagonal structure on the channel mixing weights. Also, to enhance
generalization, inspired by sparse regression, we sparsify the frequencies via soft-thresholding Tib-
shirani (1996). Also, for parameter efﬁciency, our MLP layer shares weights across tokens (see
Table 1). We term the resulting model as adaptive FNO (AFNO)."
INTRODUCTION,0.07834101382488479,"We perform extensive experiments with pretraining vision transformers for upstream classiﬁcation
and inpainting that are then ﬁnetuned for downstream segmentation. Compared with the state-of-the-
art, our AFNO using the ViT-B backbone outperforms existing GFN, LS, and self-attention for few-
shot segmentation in terms of both efﬁciency and accuracy, e.g, compared with self-attention, AFNO
achieves slightly better accuracy while being 30% more efﬁcient. For Cityscapes segmentation
with the Segformer-B3 backbone, AFNO achieves state-of-the-art and beats previous methods, e.g.
AFNO achieves more than 2% better mIoU compared with efﬁcient self-attention Xie et al. (2021),
and is also competitive with GFN and LS."
INTRODUCTION,0.08294930875576037,Key Contributions. Our main contributions are summarized as follows:
INTRODUCTION,0.08755760368663594,"• We establish a link between operator learning and high-resolution token mixing and adapt FNO
from PDEs as an efﬁcient mixer with a quasi-linear complexity in the sequence length."
INTRODUCTION,0.09216589861751152,"• We design AFNO in a principled way to improve its expressiveness and generalization by impos-
ing block-diagonal structure, adaptive weight-sharing, and sparsity."
INTRODUCTION,0.0967741935483871,"• We conduct experiments for pretraining and ﬁnetuning. AFNO outperforms existing mixers for
few-shot segmentation. For Cityscapes segmentation with the Segformer-B3 backbone, AFNO
(sequence: 65k) achieves state-of-the-art, e.g., with 2% gain over the efﬁcient self-attention."
RELATED WORKS,0.10138248847926268,"2
RELATED WORKS"
RELATED WORKS,0.10599078341013825,"Our work is at the intersection of operator learning and efﬁcient transformers. Since the inception
of transformers, there have been several works to improve the efﬁciency of self-attention. We divide
them into three lines of work based on the structural constraints."
RELATED WORKS,0.11059907834101383,"Graph-Based Mixers primarily focus on ﬁnding efﬁcient surrogates to approximate self-attention.
Those include: (i) sparse attentions that promote predeﬁned sparse patterns; see e.g., sparse trans-
former Child et al. (2019), image transformer Parmar et al. (2018), axial transformer Ho et al. (2019),
and longformer Beltagy et al. (2020); (ii) low-rank attention that use linear sketching such as lin-
formers Wang et al. (2020), long-short transformers Lian et al. (2021), Nystr¨omformer Xiong et al.
(2021); (iii) kernel methods that approximate attention with ensemble of kernels such as performer
Choromanski et al. (2020), linear transformer Katharopoulos et al. (2020), and random feature at-
tention Peng et al. (2021); and (iv) clustering-based methods such as reformer Kitaev et al. (2020),
routing transformer Roy et al. (2021), and Sinkhorn transformer Tay et al. (2020a). These surrogates
however compromise accuracy for efﬁciency."
RELATED WORKS,0.1152073732718894,"MLP-Based Mixers relax the graph similarity constraints of the self-attention and spatially mix
tokens using MLP projections. The original MLP-mixer Tolstikhin et al. (2021) achieves similar
accuracy as self-attention. It is further accelerated by ResMLP Touvron et al. (2021) that replaces
the layer norm with the afﬁne transforms. gMLP Liu et al. (2021a) also uses an additional gating
to weight tokens before mixing. This class of methods however lack scalability due to quadratic
complexity of MLP projection, and their parameter inefﬁciency for high resolution images."
RELATED WORKS,0.11981566820276497,"Fourier-Based Mixers apply the Fourier transform to spatially mix tokens. FNet Lee-Thorp et al.
(2021) resembles the MLP-mixer with token mixer simply being pre-ﬁxed DFT. No ﬁltering is
done to adapt the data distribution. Global ﬁlter networks (GFNs) Rao et al. (2021) however learn
Fourier ﬁlters to perform depthwise global convolution, where no channel mixing is involved. Also,
GFN ﬁlters lack adaptivity that could negatively impact generalization. In contrast, our proposed
AFNO performs global convolution with dynamic ﬁltering and channel mixing that leads to better
expressivity and generalization."
RELATED WORKS,0.12442396313364056,"Operator Learning deals with mapping from functions to functions and commonly used for PDEs.
Operator learning can be deployed in computer vision as images are RGB-valued functions on a
2D plane. This continuous generalization allows us to permeate beneﬁts from operators. Recent
advances in operator learning include DeepONet Lu et al. (2019) that learns the coefﬁcients and
basis of the operators, and neural operators Kovachki et al. (2021) that are parameterized by integral"
RELATED WORKS,0.12903225806451613,Published as a conference paper at ICLR 2022
RELATED WORKS,0.1336405529953917,"operators. In this work, we adopt Fourier neural operators Li et al. (2020a) that implement global
convolution via FFT which has been very successful for solving nonlinear and chaotic PDEs."
PRELIMINARIES AND PROBLEM STATEMENT,0.1382488479262673,"3
PRELIMINARIES AND PROBLEM STATEMENT"
PRELIMINARIES AND PROBLEM STATEMENT,0.14285714285714285,"Consider a 2D image that is divided into a h × w grid of small and non-overlapping patches. Each
patch is represented as a d-dimensional token, and the image can be represented as a token tensor
X ∈Rh×w×d. Treating image as a token sequence, transformers then aim to learn a contextual
embedding that transfers well to downstream tasks. To end up with a rich representation, the tokens
need to be effectively mixed over the layers."
PRELIMINARIES AND PROBLEM STATEMENT,0.14746543778801843,"Self-attention is an effective mixing that learns the graph similarity among tokens. It however scales
quadratically with the sequence size, which impedes training high resolution images. Our goal is
then to ﬁnd an alternative mixing strategy that achieves favorable scaling trade-offs in terms of
computational complexity, memory, and downstream transfer accuracy."
KERNEL INTEGRATION,0.15207373271889402,"3.1
KERNEL INTEGRATION"
KERNEL INTEGRATION,0.15668202764976957,"The self-attention mechanism can be written as a kernel integration (Tsai et al., 2019; Cao, 2021;
Kovachki et al., 2021). For the input tensor X we denote the (n, m)-th token as xn,m ∈Rd. For
notation convenience, we index the token sequence as X[s] := X[ns, ms] for some s, t ∈[hw].
Deﬁne also N := hw as the sequence length. The self-attention mixing is then deﬁned as follows:"
KERNEL INTEGRATION,0.16129032258064516,Deﬁnition 1 (Self Attention). Att : RN×d →RN×d
KERNEL INTEGRATION,0.16589861751152074,"Att(X) := softmax
XWq(XWk)⊤ √ d"
KERNEL INTEGRATION,0.17050691244239632,"
XWv
(1)"
KERNEL INTEGRATION,0.17511520737327188,"where Wq, Wk, Wv ∈Rd×d are the query, key, and value matrices, respectively. Deﬁne K :=
softmax(⟨XWq, XWk⟩/
√"
KERNEL INTEGRATION,0.17972350230414746,"d) as the N × N score array with ⟨·, ·⟩being inner product in Rd. We
then treat self-attention as an asymmetric matrix-valued kernel κ : [N]×[N] →Rd×d parameterized
as κ[s, t] = K[s, t]·Wv (where K[s, t] is scalar valued and “·” is scalar-matrix multiplication). Then
the self-attention can be viewed as a kernel summation."
KERNEL INTEGRATION,0.18433179723502305,"Att(X)[s] := N
X"
KERNEL INTEGRATION,0.1889400921658986,"t=1
X[t]κ[s, t]
∀s ∈[N].
(2)"
KERNEL INTEGRATION,0.1935483870967742,"Where X[t]κ[s, t] = X[t]K[s, t]Wv = K[s, t]X[t]Wv. This kernel summation can be extended
to continuous kernel integrals. The input tensor X is no longer a ﬁnite-dimensional vector in the
Euclidean space X ∈RN×d, but rather a spatial function in the function space X ∈(D, Rd) deﬁned
on domain D ⊂R2 which is the physical space of the images. In this continuum formulation,
the neural network becomes an operator that acts on the input functions. This brings us efﬁcient
characterization originating from operator learning."
KERNEL INTEGRATION,0.19815668202764977,"Deﬁnition 2 (Kernel Integral). We deﬁne the kernel integral operator K : (D, Rd) →(D, Rd) as"
KERNEL INTEGRATION,0.20276497695852536,"K(X)(s) =
Z"
KERNEL INTEGRATION,0.2073732718894009,"D
κ(s, t)X(t) dt
∀s ∈D.
(3)"
KERNEL INTEGRATION,0.2119815668202765,"with a continuous kernel function κ : D × D →Rd×d Li et al. (2020b). For the special case of the
Green’s kernel κ(s, t) = κ(s −t), the integral leads to global convolution deﬁned below."
KERNEL INTEGRATION,0.21658986175115208,"Deﬁnition 3 (Global Convolution). Assuming κ(s, t) = κ(s −t), the kernel operator admits"
KERNEL INTEGRATION,0.22119815668202766,"K(X)(s) =
Z"
KERNEL INTEGRATION,0.22580645161290322,"D
κ(s −t)X(t) dt
∀s ∈D.
(4)"
KERNEL INTEGRATION,0.2304147465437788,"The convolution is a smaller complexity class of operation compared to integration. The Green’s
kernel has beneﬁcial regularization effect but it is also expressive enough to capture global interac-
tions. Furthermore, the global convolution can be efﬁciently implemented by the FFT."
KERNEL INTEGRATION,0.2350230414746544,Published as a conference paper at ICLR 2022
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.23963133640552994,"3.2
FOURIER NEURAL OPERATOR AS TOKEN MIXER"
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.24423963133640553,"The class of shift-equivariant kernels has a desirable property that they can be decomposed as a linear
combination of eigen functions. Eigen transforms have a magical property where according to the
convolution theorem Soliman & Srinath (1990), global convolution in the spatial domain amounts
to multiplication in the eigen transform domain. A popular example of such eigen functions is the
Fourier transform. Accordingly, one can deﬁne the Fourier neural operator (FNO) Li et al. (2020a)."
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.2488479262672811,"Deﬁnition 4 (Fourier Neural Operator). For the continuous input X ∈D and kernel κ, the kernel
integral at token s is found as"
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.2534562211981567,"K(X)(s) = F−1 
F(κ) · F(X)

(s)
∀s ∈D,"
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.25806451612903225,"where · denotes matrix multiplication, and F, F−1 denote the continous Fourier transform and its
inverse, respectively."
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.2626728110599078,"Discrete FNO. Inspired by FNO, for images with ﬁnite dimension on a discrete grid, our idea is to
mix tokens using the discrete Fourier transform (DFT). For the input token tensor X ∈Rh×w×d,
deﬁne the complex-valued weight tensor W := DFT(κ) ∈Ch×w×d×d to parameterize the kernel.
FNO mixing then entails the following operations per token (m, n) ∈[h] × [w]"
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.2672811059907834,"step (1). token mixing
zm,n = [DFT(X)]m,n
step (2). channel mixing
˜zm,n = Wm,nzm,n"
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.271889400921659,"step (3). token demixing
ym,n = [IDFT( ˜Z)]m,n"
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.2764976958525346,"Local Features. DFT assumes a global convolution applied on periodic images, which is not typ-
ically true for real-world images. To compensate local features and non-periodic boundaries, we
can add a residual term xm,n (can also be parameterized as a simple local convolution) to the token
demixing step 3 in FNO; see also Wen et al. (2021)."
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.28110599078341014,"Resolution Invariance. The FNO model is invariant to the discretization h, w. It parameterizes the
tokens function via Fourier bases which are invariant to the underlying resolution. Thus, after train-
ing on one resolution it can be directly evaluated at another resolution (zero-shot super-resolution).
Further, the FNO model encodes the higher-frequency information in the channel dimension. Thus,
even after truncating the higher frequency modes zm,n, FNO can still output the full spectrum."
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.2857142857142857,"It is also important to recognize step (2) of FNO, where d × d weight matrix Wm,n mixes the
channels. This implies mixed-channel global convolution. Note that the concurrent GFN work Rao
et al. (2021) is a special case of FNO, when Wm,n is diagonal and the channels are separable."
FOURIER NEURAL OPERATOR AS TOKEN MIXER,0.2903225806451613,"The FNO incurs O(N log(N)d2) complexity, and thus quasi-linear in the sequence size. The pa-
rameter count is however O(Nd2) as each token has its own channel mixing weights, which poorly
scales with the image resolution. In addition, the weights Wm,n are static, which can negatively
impact the generalization. The next section enhances FNO to cope with these shortcomings.x"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.29493087557603687,"4
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.2995391705069124,This section ﬁxes the shortcomings of FNO for images to improve scalability and robustness.
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.30414746543778803,"Block-Diagonal Structure on W. FNO involves d × d weight matrices for each token. That results
in O(Nd2) parameter count that could be prohibitive. To reduce the paramater count we impose a
block diagonal structure on W, where it is divided into k weight blocks of size d/k × d/k. The
kernel then operates independently on each block as follows"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3087557603686636,"˜z(ℓ)
m,n = W (ℓ)
m,nz(ℓ)
m,n,
ℓ= 1, . . . , k
(5)"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.31336405529953915,"The block diagonal weights are both interpretable and computationally parallelizable. In essence,
each block can be interpreted a head as in multi-head self-attention, which projects into a subspace
of the data. The number of blocks should be chosen properly so each subspace has a sufﬁciently
large dimension. In the special case, that the block size is one, FNO coincides with the GFN kernels.
Moreover, the multiplications in (5) are performed independently, which is quite parallelizable."
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.31797235023041476,Published as a conference paper at ICLR 2022
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3225806451612903,def AFNO(x)
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3271889400921659,"bias = x
x = RFFT2(x)
x = x.reshape(b, h, w//2+1, k, d/k)
x = BlockMLP(x)
x = x.reshape(b, h, w//2+1, d)
x = SoftShrink(x)
x = IRFFT2(x)
return x + bias"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3317972350230415,"x = Tensor[b, h, w, d]
W_1, W_2 = ComplexTensor[k, d/k, d/k]
b_1, b_2 = ComplexTensor[k, d/k]"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.33640552995391704,def BlockMLP(x):
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.34101382488479265,"x = MatMul(x, W_1) + b_1
x = ReLU(x)
return MatMul(x, W_2) + b_2"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3456221198156682,Figure 3: Pseudocode for AFNO with adaptive weight sharing and adaptive masking.
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.35023041474654376,"Weight Sharing. Another caveat with FNO is that the weights are static and once learned they will
not be adaptively changed for the new samples. Inspired by self-attention we want the tokens to be
adaptive. In addition, static weights are independent across tokens, but we want the tokens interact
and decide about passing certain low and high frequency modes. To this end, we adopt a two-layer
perceptron that is supposed to approximate any function for a sufﬁciently large hidden layer. For
(n, m)-th token, it admits"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3548387096774194,"˜zm,n = MLP(zm,n) = W2σ(W1zm,n) + b
(6)"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.35944700460829493,"Note, that the weights W1, W2, b are shared for all tokens, and thus the parameter count can be
signiﬁcantly reduced."
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3640552995391705,"Soft-Thresholding and Shrinkage. Images are inherently sparse in the Fourier domain, and most
of the energy is concentrated around low frequency modes. Thus, one can adaptively mask the
tokens according to their importance towards the end task. This can use the expressivity towards
representing the important tokens. To sparsify the tokens, instead of linear combination as in (5),
we use the nonlinear LASSO Tibshirani (1996) channel mixing as follow"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3686635944700461,"min ∥˜zm,n −Wm,nzm,n∥2 + λ∥˜zm,n∥1
(7)"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.37327188940092165,This can be solved via soft-thresholding and shrinkage operation
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3778801843317972,"˜zm,n = Sλ(Wm,nzm,n)
(8)"
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3824884792626728,"that is deﬁned as Sλ(x) = sign(x) max{|x| −λ, 0}, where λ is a tuning parameter that controls
the sparsity. It is also worth noting that the promoted sparsity can also regularize the network and
improve the robustness."
ADAPTIVE FOURIER NEURAL OPERATORS FOR TRANSFORMERS,0.3870967741935484,"With the aforementioned modiﬁcations, the overall AFNO mixer module is shown in Fig 1 along
with the pseudo code in Fig 2. Also, for the sake of comparison, the AFNO is compared against
FNO, GFN, and self-attention in Table 1 in terms of interpretation, memory, and complexity."
EXPERIMENTS,0.391705069124424,"5
EXPERIMENTS"
EXPERIMENTS,0.39631336405529954,"We conduct extensive experiments to demonstrate the merits of our proposed AFNO transformer.
Namely, 1) we evaluate the efﬁciencyy-accuracy trade-off between AFNO and alternative mixing
mechanisms on inpainting and classiﬁcation pretraining tasks; and then 2) measure performance
on few-shot semantic segmentation with inpainting pretraining; and 3) evaluate the performance of
AFNO in high resolution settings with semantic segmentation. Our experiments cover a wide-range
of datasets, including ImageNet-1k, CelebA-Faces, LSUN-Cats, ADE-Cars, and Cityscapes as in
Deng et al. (2009); Liu et al. (2015); Yu et al. (2015); Krause et al. (2013); Cordts et al. (2016)."
EXPERIMENTS,0.4009216589861751,"5.1
IMAGENET-1K INPAINTING"
EXPERIMENTS,0.4055299539170507,"We conduct image inpainting experiments which compare AFNO to other competitive mixing mech-
anisms. The image inpainting task is deﬁned as follows: given an input image X of size [h, w, d],
where h, w, d denote height, width, and channels respectively, we randomly mask pixel intensities to
zero based on a uniformly random walk. The loss function used to train the model is mean squared
error between the original image and the reconstruction. We measure performance via the Peak
Signal-to-Noise Ratio (PSNR) and structural similarity index measure (SSIM) between the ground
truth and the reconstruction. More details about the experiments are provided in the appendix."
EXPERIMENTS,0.41013824884792627,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.4147465437788018,"Backbone
Mixer
Params
GFLOPs
Latency(sec)
SSIM
PSNR(dB)
ViT-B/4
Self-Attention
87M
357.2
1.2
0.931
27.06
ViT-B/4
LS
87M
274.2
1.4
0.920
26.18
ViT-B/4
GFN
87M
177.8
0.7
0.928
26.76
ViT-B/4
AFNO (ours)
87M
257.2
0.8
0.931
27.05"
EXPERIMENTS,0.41935483870967744,"Table 2:
Inpainting PSNR and SSIM for ImageNet-1k validation data. AFNO matches the performance of
Self-Attention despite using signiﬁcantly less FLOPs."
EXPERIMENTS,0.423963133640553,"Inpainting Results. PSNR and SSIM are reported in Table 2 for AFNO versus alternative mix-
ers. It appears that AFNO is competitive with self-attention. However, AFNO uses signiﬁcantly
less GFLOPs than Self-Attention. Compared to both LS and GFN, AFNO acheives signiﬁcantly
better PSNR and SSIM. More importantly, AFNO achieves favorable downstream transfer, which is
elaborated in the next section for few-shot segmentation."
FEW SHOT SEGMENTATION,0.42857142857142855,"5.2
FEW SHOT SEGMENTATION"
FEW SHOT SEGMENTATION,0.43317972350230416,"After pretraining on image inpainting, we evaluate the few-shot sematic segmentation performance
of the models. We construct three few-shot segmentation datasets by selecting training and valida-
tion images from CelebA-Faces, ADE-Cars, and LSUN-Cats as in Zhang et al. (2021b). The model
is trained using cross-entropy loss. We measure mIoU over the validation set. More details about
the experiments are deferred to the Appendix."
FEW SHOT SEGMENTATION,0.4377880184331797,"Backbone
Mixer
Params
GFLOPs
LSUN-Cats
ADE-Cars
CelebA-Faces
ViT-B/4
Self-Attention
87M
357.2
35.57
49.26
56.91
ViT-B/4
LS
87M
274.2
20.29
29.66
41.36
ViT-B/4
GFN
87M
177.8
34.52
47.84
55.21
ViT-B/4
AFNO (ours)
87M
257.2
35.73
49.60
55.75"
FEW SHOT SEGMENTATION,0.4423963133640553,"Table 3: Few-shot segmentation mIoU for AFNO versus alternative mixers. AFNO surpasses Self-Attention
for 2/3 datasets while using less ﬂops."
FEW SHOT SEGMENTATION,0.4470046082949309,"Few-Shot Segmentation Results. Results are reported in Table 3. It is evident that AFNO performs
on par with self-attention. Furthermore, for out-of-domain datasets such as ADE-Cars or LSUN-
Cats it slightly outperforms self-attention, which is partly attributed to the sparsity regularization
endowed in AFNO."
CITYSCAPES SEGMENTATION,0.45161290322580644,"5.3
CITYSCAPES SEGMENTATION"
CITYSCAPES SEGMENTATION,0.45622119815668205,"To test the scalability of AFNO for high resolution images with respect to alternative mixers, we
evaluate high-resolution (1024 × 1024) semantic segmentation for the Cityscapes dataset. We use
the SegFormer-B3 backbone which is a hiearchical vision transformer Xie et al. (2021). We train the
model using the cross-entropy loss and measure performance via reporting mIoU over the validation
set. More details about the experiments and the model are available in the Appendix."
CITYSCAPES SEGMENTATION,0.4608294930875576,"Backbone
Mixer
Params
Total GFLOPs
Mixer GFLOPs
mIoU
Segformer-B3/4
SA
45M
N/A
825.7
N/A
Segformer-B3/4
Efﬁcient SA
45M
380.7
129.9
79.7
Segformer-B3/4
LS
45M
409.1
85.0
80.5
Segformer-B3/4
GFN
45M
363.4
2.6
80.4
Segformer-B3/4
AFNO-100% (ours)
45M
440.0
23.7
80.9
Segformer-B3/4
AFNO-25% (ours)
45M
429.0
12.4
80.4"
CITYSCAPES SEGMENTATION,0.46543778801843316,"Table 4: mIoU and FLOPs for Cityscapes segmentation at 1024 × 1024 resolution. Note, both the mixer and
total FLOPs are included. For GFN and AFNO, the MLP layers are the bottleneck for the complexity. Also,
AFNO-25% only keeps 25% of the low frequency modes, while AFNO-100% keeps all the modes. Results for
self-attention cannot be obtained due to the long sequence length in the ﬁrst few layers."
CITYSCAPES SEGMENTATION,0.4700460829493088,Published as a conference paper at ICLR 2022
CITYSCAPES SEGMENTATION,0.47465437788018433,"Cityscapes Segmentation Results. We report the ﬁnal numbers for Cityscapes semantic segmenta-
tion in Table 4. AFNO-100% outperforms all other methods in terms of mIoU. Furthermore, we ﬁnd
that the AFNO-25% model which truncates 75% of high frequency modes during ﬁnetuning only
loses 0.05 mIoU and is competitive with the other mixers. It is important to note that the majority
of computations is spent for the MLP layers after the attention module."
CITYSCAPES SEGMENTATION,0.4792626728110599,"5.4
IMAGENET-1K CLASSIFICATION"
CITYSCAPES SEGMENTATION,0.4838709677419355,"We run image classiﬁcation experiments with the AFNO mixer module using the ViT backbone
on ImageNet-1K dataset containing 1.28M training images and 50K validation images from 1, 000
classes at 224 × 224 resolution. We measure performance via reporting top-1 and top-5 valida-
tion accuracy along with theoretical FLOPs of the model. More details about the experiments are
provided in the appendix."
CITYSCAPES SEGMENTATION,0.48847926267281105,"Backbone
Mixer
Params
GFLOPs
Top-1 Accuracy
Top-5 Accuracy
ViT-S/4
LS
16M
15.8
80.87
95.31
ViT-S/4
GFN
16M
6.1
78.77
94.4
ViT-S/4
AFNO (ours)
16M
15.3
80.89
95.39"
CITYSCAPES SEGMENTATION,0.4930875576036866,Table 5: ImageNet-1K classiﬁcation efﬁciencyy-accuracy trade-off when the input resolution is 224 × 224.
CITYSCAPES SEGMENTATION,0.4976958525345622,"Classiﬁcation Results. The classiﬁcation accuracy for different token mixers are listed in Table
5. It can be observed that AFNO outperforms GFN by more than 2% top-1 accuracy thanks to
the adaptive weight sharing which allows for a larger channel size. Furthermore, our experiments
demonstrate that AFNO is competitive with LS for classiﬁcation."
ABLATION STUDIES,0.5023041474654378,"5.5
ABLATION STUDIES"
ABLATION STUDIES,0.5069124423963134,"We also conduct experiments to investigate how different components of AFNO contribute to per-
formance."
ABLATION STUDIES,0.511520737327189,"Figure 4: Ablations for the sparsity thresholds and block count measured by inpainting validation PSNR. The
results suggest that soft thresholding and blocks are effective"
ABLATION STUDIES,0.5161290322580645,"Sparsity Threshold. We vary the sparsity threshold λ from 0 to 10. For each λ we pretrain the
network ﬁrst, and then ﬁnetune for few-shot segmentation on the CelebA-Faces dataset. We report
both the inpainting PSNR from pretraining and the segmentation mIoU. The results are shown in
Figure 3. λ = 0 corresponds to no sparsity. It is evident that the PSNR/mIoU peaks at λ = 0.01,
indicating that the sparsity is effective. We also compare to hard thresholding (always removing
higher frequencies as in FNO) in Table 6. We truncate 65% of the higher frequencies for both
inpainting pretraining and few-shot segmentation ﬁnetuning."
ABLATION STUDIES,0.5207373271889401,"Number of Blocks. We vary the number of blocks used when we impose structure on our weights
W. To make the comparison fair, we simultaneously adjust the hidden size so the overall parameter
count of the model are equal. We vary the number of blocks from 1 to 64 and measure the resulting
inpainting PSNR on ImageNet-1K. It is seen that 8 blocks achieves the best PSNR. This shows that
blocking is effective."
ABLATION STUDIES,0.5253456221198156,"Impact of Adaptive Weights. We evaluate how removing adaptive weights and instead using static
weights affects the performance of AFNO for ImageNet-1K inpainting and few-shot segmentation."
ABLATION STUDIES,0.5299539170506913,Published as a conference paper at ICLR 2022
ABLATION STUDIES,0.5345622119815668,"The results are presented in Table 6. The results suggest that adaptive weights are crucial to AFNO’s
performance."
ABLATION STUDIES,0.5391705069124424,"Backbone
Mixer
Parameter Count
PSNR
CelebA-Faces mIoU
ViT-XS/4
FNO
16M
24.8
39.27
ViT-XS/4
AFNO [Non-Adaptive Weights]
16M
25.1
44.04
ViT-XS/4
AFNO [Hard Thresholding 35%]
16M
23.58
34.17
ViT-XS/4
AFNO
16M
25.69
49.49"
ABLATION STUDIES,0.543778801843318,"Table 6: Ablations for AFNO versus FNO, AFNO without adaptive weights, and hard thresholding. Results
are on inpainting pretraining with 10% of ImageNet along with few-show segmentation mIoU on CelebA-
Faces. Hard thresholding only keeps 35% of low frequency modes. AFNO demonstrates superior performance
for the same parameter count in both tasks."
ABLATION STUDIES,0.5483870967741935,"Comparison to FNO. To show that AFNO’s modiﬁcations ﬁx the shortcomings of FNO for images,
we directly compare AFNO and FNO on ImageNet-1K inpainting pretraining and few-shot segmen-
tation on CelebA-Faces. The results are also presented in Table 6. The results suggest that AFNO’s
modiﬁcations are crucial to performance in both tasks."
COMPARISON WITH DIFFERENT TRUNKS AT DIFFERENT SCALES,0.5529953917050692,"5.6
COMPARISON WITH DIFFERENT TRUNKS AT DIFFERENT SCALES"
COMPARISON WITH DIFFERENT TRUNKS AT DIFFERENT SCALES,0.5576036866359447,"In order to provide more extensive comparison with the state-of-the-art efﬁcient transformers we
have included experiments for different trunks at different scales. Since the primary motivation of
this work is to deal with high resolution vision, we focus on the task of Cityscapes semantic segmen-
tation a the benchmark that is a challenging task due to the high 1024×2048 resolution of images.
For the trunks we adopt: i) the Segformer Xie et al. (2021) backbones B0, B1, B2, B3, under three
different mixers namely AFNO, GFN and efﬁcient self-attention (ESA); ii) Swin backbones Liu
et al. (2021b) (T, S, B), and iii) ResNet He et al. (2016) and MobileNetV2 Sandler et al. (2018).
Results for LS and self-attention are not reported due to instability issues with half-precision train-
ing and quadratic memory usage with sequence length respectively. Note that efﬁcient self-attention
is self-attention but with a sequence reduction technique introduced in Wang et al. (2021). It is
meant to be a cheap approximation to self-attention. Numbers for ResNet and MobileNetV2 are
directly adopted from Xie et al. (2021). For training Segformer we use the same recipe as discussed
in Section A.3 which consists of pretraining on ImageNet-1K classiﬁcation for 300 epochs. For
training Swin, we use pretrained classiﬁcation checkpoints available from the original authors and
then combine Swin with the Segformer head Xie et al. (2021). We use the same training recipe as
the Segformer models for Swin."
COMPARISON WITH DIFFERENT TRUNKS AT DIFFERENT SCALES,0.5622119815668203,"The mIoU scores are listed in Fig. 1 versus the parameter size. It is ﬁrst observed that AFNO
outperforms other mixers when using the same Segformer backbone under the same parameter size.
Also, when using AFNO with the hierarchical segformer backbone, it consistently outperforms the
Swin backbone for semantic segmentation."
CONCLUSIONS,0.5668202764976958,"6
CONCLUSIONS"
CONCLUSIONS,0.5714285714285714,"We leverage the geometric structure of images in order to build an efﬁcient token mixer in compar-
ison to self-attention. Inspired by global convolution, we borrow Fourier Neural Operators (FNO)
from PDEs for mixing tokens and propose principled architectural modiﬁcations to adapt FNO for
images. Speciﬁcally, we impose a block diagonal structure on the weights, adaptive weight shar-
ing, and sparsify the frequency with soft-thresholding and shrinkage. We call the proposed mixer
Adaptive Fourier Neural Operator (AFNO) and it incurs quasi-linear complexity in sequence length.
Our experiments indicate favorable accuracy-efﬁciency trade-off for few-shot segmentation, and
competitive high-resolution segmentation compared with state-of-the-art. There are still important
avenues to explore for the future work such as exploring alterantives for the DFT such as the Wavelet
transform to better capture locality as in Gupta et al. (2021)."
CONCLUSIONS,0.576036866359447,Published as a conference paper at ICLR 2022
REFERENCES,0.5806451612903226,REFERENCES
REFERENCES,0.5852534562211982,"Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150, 2020."
REFERENCES,0.5898617511520737,"Shuhao Cao. Choose a transformer: Fourier or galerkin. arXiv preprint arXiv:2105.14995, 2021."
REFERENCES,0.5944700460829493,"Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019."
REFERENCES,0.5990783410138248,"Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794, 2020."
REFERENCES,0.6036866359447005,"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban
scene understanding. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3213–3223, 2016."
REFERENCES,0.6082949308755761,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.6129032258064516,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.6175115207373272,"Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differen-
tial equations. arXiv preprint arXiv:2109.13459, 2021."
REFERENCES,0.6221198156682027,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.6267281105990783,"Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-
mensional transformers. arXiv preprint arXiv:1912.12180, 2019."
REFERENCES,0.631336405529954,"Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International Conference on Ma-
chine Learning, pp. 5156–5165. PMLR, 2020."
REFERENCES,0.6359447004608295,"Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. arXiv
preprint arXiv:2001.04451, 2020."
REFERENCES,0.6405529953917051,"Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces.
arXiv preprint arXiv:2108.08481, 2021."
REFERENCES,0.6451612903225806,"Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained
categorization.
In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013."
REFERENCES,0.6497695852534562,"James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with
fourier transforms. arXiv preprint arXiv:2105.03824, 2021."
REFERENCES,0.6543778801843319,"Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential
equations. arXiv preprint arXiv:2010.08895, 2020a."
REFERENCES,0.6589861751152074,"Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differ-
ential equations. arXiv preprint arXiv:2003.03485, 2020b."
REFERENCES,0.663594470046083,Published as a conference paper at ICLR 2022
REFERENCES,0.6682027649769585,"Dongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao. As-mlp: An axial shifted mlp architecture
for vision. arXiv preprint arXiv:2107.08391, 2021."
REFERENCES,0.6728110599078341,"Hanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint
arXiv:2105.08050, 2021a."
REFERENCES,0.6774193548387096,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021b."
REFERENCES,0.6820276497695853,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.6866359447004609,"Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for iden-
tifying differential equations based on the universal approximation theorem of operators. arXiv
preprint arXiv:1910.03193, 2019."
REFERENCES,0.6912442396313364,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In International Conference on Machine Learning, pp. 4055–
4064. PMLR, 2018."
REFERENCES,0.695852534562212,"Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.
Random feature attention. arXiv preprint arXiv:2103.02143, 2021."
REFERENCES,0.7004608294930875,"Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global ﬁlter networks for
image classiﬁcation. arXiv preprint arXiv:2107.00645, 2021."
REFERENCES,0.7050691244239631,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguis-
tics, 9:53–68, 2021."
REFERENCES,0.7096774193548387,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510–4520, 2018."
REFERENCES,0.7142857142857143,"Samir S Soliman and Mandyam D Srinath. Continuous and discrete signals and systems. Englewood
Cliffs, 1990."
REFERENCES,0.7188940092165899,"Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In
International Conference on Machine Learning, pp. 9438–9447. PMLR, 2020a."
REFERENCES,0.7235023041474654,"Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv
preprint arXiv:2009.06732, 2020b."
REFERENCES,0.728110599078341,"Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267–288, 1996."
REFERENCES,0.7327188940092166,"Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An
all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021."
REFERENCES,0.7373271889400922,"Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard
Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Herv´e J´egou. Resmlp: Feedforward
networks for image classiﬁcation with data-efﬁcient training. arXiv preprint arXiv:2105.03404,
2021."
REFERENCES,0.7419354838709677,"Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Transformer dissection: A uniﬁed understanding of transformer’s attention via
the lens of kernel. arXiv preprint arXiv:1908.11775, 2019."
REFERENCES,0.7465437788018433,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.7511520737327189,Published as a conference paper at ICLR 2022
REFERENCES,0.7557603686635944,"Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.7603686635944701,"Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,
and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without
convolutions. arXiv preprint arXiv:2102.12122, 2021."
REFERENCES,0.7649769585253456,"Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. U-
fno–an enhanced fourier neural operator based-deep learning model for multiphase ﬂow. arXiv
preprint arXiv:2109.03697, 2021."
REFERENCES,0.7695852534562212,"Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Seg-
former: Simple and efﬁcient design for semantic segmentation with transformers. arXiv preprint
arXiv:2105.15203, 2021."
REFERENCES,0.7741935483870968,"Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and
Vikas Singh. Nystr\” omformer: A nystr\” om-based algorithm for approximating self-attention.
arXiv preprint arXiv:2102.03902, 2021."
REFERENCES,0.7788018433179723,"Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.
Lsun: Construction of
a large-scale image dataset using deep learning with humans in the loop.
arXiv preprint
arXiv:1506.03365, 2015."
REFERENCES,0.783410138248848,"Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Laﬂeche, Adela Barriuso, Antonio
Torralba, and Sanja Fidler. Datasetgan: Efﬁcient labeled data factory with minimal human effort.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
10145–10155, 2021a."
REFERENCES,0.7880184331797235,"Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Laﬂeche, Adela Barriuso, Antonio
Torralba, and Sanja Fidler. Datasetgan: Efﬁcient labeled data factory with minimal human effort.
In CVPR, 2021b."
REFERENCES,0.7926267281105991,"Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar,
and Bryan Catanzaro. Long-short transformer: Efﬁcient transformers for language and vision.
arXiv preprint arXiv:2107.02192, 2021."
REFERENCES,0.7972350230414746,Published as a conference paper at ICLR 2022
REFERENCES,0.8018433179723502,"A
APPENDIX"
REFERENCES,0.8064516129032258,This section includes visualizations of AFNO as well as the details of the experiments.
REFERENCES,0.8110599078341014,"A.1
VISUALIZATION OF AFNO"
REFERENCES,0.815668202764977,"To gain insight into how AFO works, we produce visualizations of AFNO’s weights and represen-
tations. In particular, we visualize the spectral clustering of the tokens and sparsity masks from soft
thresholding."
REFERENCES,0.8202764976958525,"AFNO clustering versus other mixers. We show the clustering of tokens after each transformer
layer. For a 10-layer transformer pretrained with 10% of ImageNet-1k, we apply spectral clustering
on the intermediate features when we use k-NN kernel with k = 10 and the number of clusters is also
set to 4. It appears that AFNO clusters are as good as self-attention. AFNO clusters seem to be more
aligned with the image objects than GFN ones. Also, long-short transformer seems not to preserve
the objectness in the last layers. This observation is consistent with the the few-shot segmentation
results in section 5.2 that show the superior performance of AFNO over the alternatives."
REFERENCES,0.8248847926267281,"SA
GFN
LS
AFNO"
REFERENCES,0.8294930875576036,"Figure 5: Spectral clustering of tokens for different token mixers. From top to bottom, it shows the input and
the layers 2, 4, 6, 8, 10 for the inpainting pretrained model."
REFERENCES,0.8341013824884793,"Sparsity Masks. We also explore how the sparsity mask affects the magnitude of the values in
tokens. We calculate the fraction of values over the channel dimension and blocks that have been
masked to zero by the Softshrink function. As one can see in Figure 6, it is clear that the input images
are very sparse in the Fourier domain. Furthermore, this sparsity suggests that we can aggressively
truncate higher frequencies and maintain performance."
REFERENCES,0.8387096774193549,"Figure 6: Log magnitude of tokens (56×56) after soft-thresholding and shrinkage (λ = 0.1) and the sparsity
mask averaged over channels for inpainting pretrained network. Left to right shows layers 1 to 5, respectively."
REFERENCES,0.8433179723502304,"A.2
INPAINTING"
REFERENCES,0.847926267281106,"We use the ViT-B/4 backbone for the ImageNet-1K inpainting experiments. The ViT-B backbone has
12 layers and is described in more detail in the original paper Dosovitskiy et al. (2020). Importantly,
we use a 4x4 patch size to model the long sequence size setting."
REFERENCES,0.8525345622119815,• Self-Attention Vaswani et al. (2017) uses 16 attention heads and a hidden size of 768.
REFERENCES,0.8571428571428571,Published as a conference paper at ICLR 2022
REFERENCES,0.8617511520737328,"• Long-Short transformer Zhu et al. (2021) (LS) uses a window-size of 4 and dynamic pro-
jection rank of 8 with a hidden size of 768."
REFERENCES,0.8663594470046083,• Global Filter Network Rao et al. (2021) (GFN) uses a hidden size of 768.
REFERENCES,0.8709677419354839,"• Adaptive Fourier Neural Operator (AFNO) uses 1 block, a hidden dimension of 750, and a
sparsity threshold of 0.1, and a 1D convolution layer as the bias."
REFERENCES,0.8755760368663594,"The training procedure can be summarized as follows. Given an image x ∈R3×224×224 from
ImageNet-1K, we randomly mask pixel intensities to zero by initially sampling uniformly from
{(i, j)}h,w
i,j=1 and then apply the transition function T((i, j)) = UniformSample({(i −1, j), (i +
1, j), (i, j −1), (i, j + 1)}) for 3136 steps. We use a linear projection layer at the end to convert
tokens into a reconstructed image. Our loss function computes the mean squared error between
the ground truth and reconstruction of the masked pixels. We measure performance via the Peak
Signal-to-Noise Ratio (PSNR) and structural similarity index measure (SSIM) between the ground
truth and the reconstruction."
REFERENCES,0.880184331797235,"We train for 100 epochs using the Adam optimizer with a learning rate of 10−4 for self-attention
and 10−3 for all the other mechanisms using the cosine-decay schedule to a minimum learning rate
of 10−5. We use gradient clipping threshold of 1.0 and weight-decay of 0.01."
REFERENCES,0.8847926267281107,"A.3
CITYSCAPES SEGMENTATION"
REFERENCES,0.8894009216589862,"We use the SegFormer-B3 backbone for the Cityscapes segmentation experiments. The SegFormer-
B3 backbone is a four-stage architecture which reduces the sequence size and increase the hidden
size as you progress through the network. The model is described in more detail in Xie et al. (2021).
More details about how the mixing mechanisms are combined with SegFormer-B3 is described
below. All models do not modify the number layers in each stage which is [3, 4, 18, 3]."
REFERENCES,0.8940092165898618,"• Efﬁcient Self-Attention uses a hidden size of [64, 128, 320, 512] and [1, 2, 5, 8] for the
four stages"
REFERENCES,0.8986175115207373,"• Global Filter Network uses a hidden size of [128, 256, 440, 512] in order to match the
parameter count of the other networks. Because GFN is not resolution invaraint, we use
bilinear interpolation in the forward pass to make the ﬁlters match the input resolution of
the tokens."
REFERENCES,0.9032258064516129,"• Long-Short uses a hidden size of [128, 256, 360, 512] to match the parameter count of the
other networks and [1, 2, 5, 8] attention heads."
REFERENCES,0.9078341013824884,"• Adaptive Fourier Neural Operator uses [208, 288, 440, 512] to match the parameter count
of the other networks. It uses [1, 2, 5, 8] blocks in the four stages."
REFERENCES,0.9124423963133641,"We pretrain the SegFormer-B3 backbone on ImageNet-1K classiﬁcation for 300 epochs. Our setup
consists of using the Adam optimizer, a learning rate of 10−3 with cosine decay to 10−5, weight
regularization of 0.05, a batch size of 1024, gradient clipping threshold of 1.0, and learning rate
warmup for 6250 iterations. We then ﬁnetune these models on Cityscapes for 450 epochs using a
learning rate of 1.2 · 10−4. We train on random 1024x1024 crops and also evaluate at 1024x1024."
REFERENCES,0.9170506912442397,"A.4
FEW-SHOT SEGMENTATION"
REFERENCES,0.9216589861751152,"The models used for few-shot segmentation are described in B.1. We use the inpainting pretrained
models and ﬁnetune them on few-shot segmentation on CelebA-Faces, ADE-Cars, and LSUN-Cats
at 224x224 resolution. The samples for the few-shot dataset are selected as done in DatasetGAN in
Zhang et al. (2021a). To train the network, we use the per-pixel cross entropy loss."
REFERENCES,0.9262672811059908,"We ﬁnetune the models on the few-shot datasets for 2000 epochs with a learning rate of 10−4 for
self-attention and 10−3 for other mixers. We use no gradient clipping or weight decay. We measure
validation performance every 100 epochs and report the maximum across the entire training run."
REFERENCES,0.9308755760368663,Published as a conference paper at ICLR 2022
REFERENCES,0.9354838709677419,"A.5
CLASSIFICATION"
REFERENCES,0.9400921658986175,"The models for classiﬁcation are based on the Global Filter Network GFN-XS models but with 4x4
patch size. In particular, we utilize 12 transformer layers and adjust the hidden size and attention-
speciﬁc hyperparameters to reach a parameter count of 16M. Due to some attention mechanisms not
being able to support class tokens, we use global average pooling at the last layer to produce output
softmax probabilities for the 1, 000 classes in ImageNet-1k. More details about each of the models
is provided below."
REFERENCES,0.9447004608294931,• Self-Attention Vaswani et al. (2017) uses 12 attention heads and a hidden size of 324.
REFERENCES,0.9493087557603687,"• Long-Short transformer Zhu et al. (2021) (LS) uses a window-size of 4 and dynamic pro-
jection rank of 8 with a hidden size of 312."
REFERENCES,0.9539170506912442,"• Global Filter Network Rao et al. (2021) (GFN) uses a hidden size of 245. The hidden size
is smaller due to the need to make all the models have the same parameter count."
REFERENCES,0.9585253456221198,"• Adaptive Fourier Neural Operator (AFNO) uses 16 blocks, a hidden dimension of 384,
sparsity threshold of 0.1, and a 1D convolution layer as the bias."
REFERENCES,0.9631336405529954,"We trained for 300 epochs with Adam optimizer and cross-entropy loss using the learning rate of
(BatchSize/512)×5×10−4 for the models. We also use ﬁve epochs of linear learning-rate warmup,
and after a cosine-decay schedule to the minimum value 10−5. Along with this, the gradient norm
is clipped not to exceed 1.0 and weight-decay regularization is set to 0.05."
REFERENCES,0.967741935483871,"A.6
ABLATION"
REFERENCES,0.9723502304147466,"For the ablation studies, we use a backbone we denote ViT-XS which refers to models which only
have 5 layers and have attention-specifc hyperparameters adjusted to reach a parameter count of
16M. Details of these models are described below."
REFERENCES,0.9769585253456221,"• For FNO, we use a hidden size of 64 and ﬁve layers to make the parameter count 16M."
REFERENCES,0.9815668202764977,"• For AFNO with Static Weights, we use a hidden size of 124, four blocks, and a sparsity
theshold of 0.01."
REFERENCES,0.9861751152073732,"• For AFNO-35%, we hard threshold and only keep the bottom 35% frequencies. In practice,
this means we keep 32/56 frequncies of the tokens along each spatial dimension. We use a
hidden size of 124, four blocks and no sparsity theshold."
REFERENCES,0.9907834101382489,"• For AFNO, we use a hidden size of 584, four blocks, and a sparsity theshold of 0.01."
REFERENCES,0.9953917050691244,"These models are inpaint pretrained on a randomly chosen subset of only 10% of ImageNet-1K
and trained for 100 epochs. For ﬁnetuning, we use the same setup as the few-shot segmentation
experiments described in A.4. We only evaluate on the CelebA-Faces dataset."
