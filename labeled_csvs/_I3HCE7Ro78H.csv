Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0028653295128939827,"Adversarial Training using a strong ﬁrst-order adversary (PGD) is the gold stan-
dard for training Deep Neural Networks that are robust to adversarial examples.
We show that, contrary to the general understanding of the method, the gradient at
an optimal adversarial example may increase, rather than decrease, the adversari-
ally robust loss. This holds independently of the learning rate. More precisely, we
provide a counterexample to a corollary of Danskin’s Theorem presented in the
seminal paper of Madry et al. (2018) which states that a solution of the inner max-
imization problem can yield a descent direction for the adversarially robust loss.
Based on a correct interpretation of Danskin’s Theorem, we propose Danskin’s
Descent Direction (DDi) and we verify experimentally that it provides better di-
rections than those obtained by a PGD adversary. Using the CIFAR10 dataset we
further provide a real world example showing that our method achieves a steeper
increase in robustness levels in the early training stages of smooth-activation net-
works without BatchNorm, and is more stable than the PGD baseline. As a lim-
itation, PGD training of ReLU+BatchNorm networks still performs better, but
current theory is unable to explain this."
INTRODUCTION,0.0057306590257879654,"1
INTRODUCTION"
INTRODUCTION,0.008595988538681949,"Adversarial Training (AT) (Goodfellow et al., 2015; Madry et al., 2018) has become the de-facto
algorithm used to train Neural Networks that are robust to adversarial examples (Szegedy et al.,
2014). Variations of AT together with data augmentation yield the best-performing models in public
benchmarks (Croce et al., 2020). Despite lacking optimality guarantees for the inner-maximization
problem, the simplicity and performance of AT are enough reasons to embrace its heuristic nature."
INTRODUCTION,0.011461318051575931,"From an optimization perspective, the consensus is that AT is a sound algorithm: based on Dan-
skin’s Theorem, Madry et al. (2018, Corollary C.2) posit that by ﬁnding a maximizer of the inner
non-concave maximization problem, i.e., an optimal adversarial example, one can obtain a descent
direction for the adversarially robust loss. What if this is not true? are we potentially overlooking
issues in its algorithmic framework?"
INTRODUCTION,0.014326647564469915,"As mentioned in (Dong et al., 2020, Section 2.3), Corollary C.2 in Madry et al. (2018) can be consid-
ered the theoretical optimization foundation of the non-convex non-concave min-max optimization
algorithms that we now collectively refer to as Adversarial Training. It justiﬁes the two-stage struc-
ture of the training loop: ﬁrst we ﬁnd one approximately optimal adversarial example and then we
update the model using the gradient (with respect to the model parameters) at the perturbed input."
INTRODUCTION,0.017191977077363897,"The only drawbacks of a ﬁrst-order adversary seem to be its computational complexity and its ap-
proximate suboptimal solver nature. Ignoring the computational complexity issue, suppose we have
access to a theoretical oracle that provides a single solution of the inner-maximization problem. In
such idealized setting, can we safely assume AT is decreasing the adversarially robust loss on the
data sample? According to the aforementioned theoretical results, it would appear so."
INTRODUCTION,0.02005730659025788,"In this work, we scrutinize the optimization paradigm on which Adversarial Training (AT) has been
founded, and we posit that ﬁnding multiple solutions of the inner-maximization problem is necessary"
INTRODUCTION,0.022922636103151862,*These authors contributed equally to this work
INTRODUCTION,0.025787965616045846,Published as a conference paper at ICLR 2023
INTRODUCTION,0.02865329512893983,"−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
Contour plot −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 2.000 3.000 4.000 4.000 5.000 5.000 6.000 6.000 7.000 7.000 7.000 7.000"
INTRODUCTION,0.03151862464183381,"DDi
PGD (a)"
INTRODUCTION,0.034383954154727794,"0
10
20
30
40
50
Iteration 1.0 1.2 1.4 1.6 1.8 2.0 2.2"
INTRODUCTION,0.03724928366762178,Robust Loss
INTRODUCTION,0.04011461318051576,"DDi
PGD (b)"
INTRODUCTION,0.04297994269340974,"0
25
50
75
100
125
150
175
200
Epoch 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50"
INTRODUCTION,0.045845272206303724,Robust accuracy
INTRODUCTION,0.04871060171919771,"DDi
PGD (c)"
INTRODUCTION,0.05157593123209169,"Figure 1: (a) and (b): comparison of our method (DDi) and the single-adversarial-example method
(PGD) on a synthetic min-max problem. Using a single example may increase the robust loss. DDi
computes 10 examples and can avoid this. (c): similar improvement over PGD training shown on
CIFAR10, where DDi with 10 examples speeds up convergence. More details in Section 5"
INTRODUCTION,0.054441260744985676,"to ﬁnd good descent directions of the adversarially robust loss. In doing so, we hope to improve
our understanding of the non-convex/non-concave min-max optimization problem that underlies the
Adversarial Training methodology, and potentially improving its performance."
INTRODUCTION,0.05730659025787966,"Our contributions: We present two counterexamples to Madry et al. (2018, Corollary C.2), the mo-
tivation behind AT. They show that using the gradient (with respect to the parameters of the model)
evaluated at a single solution of the inner-maximization problem, can increase the robust loss, i.e.,
it can harm the robustness of the model. In particular, in counterexample 2 many descent directions
exist, but they cannot be found if we only compute a single solution of the inner-maximization prob-
lem. In Section 2 we explain that the ﬂaw in the proof is due to a misunderstanding of the directional
derivative notion that is used in the original work of Danskin (1966)."
INTRODUCTION,0.06017191977077364,"Based on our ﬁndings, we propose Danskin’s Descent Direction (DDi, Algorithm 1). It aims to
overcome the problems of the single adversarial example paradigm of AT by exploiting multiple
adversarial examples, obtaining better update directions for the network. For a data-label pair, DDi
ﬁnds the steepest descent direction for the robust loss, assuming that (i) there exists a ﬁnite number
of solutions of the inner-maximization problem and (ii) they can be found with ﬁrst-order methods."
INTRODUCTION,0.06303724928366762,"In Section 5 we verify experimentally that: (i) it is unrealistic to assume a unique solution of the
inner-maximization problem, hence making a case for our method DDi, (ii) our method can achieve
more stable descent dynamics than the vanilla AT method in synthetic scenarions and (iii) on the
CIFAR10 dataset DDi is more stable and achieves higher robustness levels in the early stages of
traning, compared with a PGD adversary of equivalent complexity. This is observed in a setting
where the conditions of Danskin’s Theorem holds, i.e., using differentiable activation functions and
removing BatchNorm. As a limitation, PGD training of ReLU+BatchNorm networks still performs
better, but there is no theory explaining this. The code to reproduce our results will be available at
https://github.com/LIONS-EPFL/ddi_at."
INTRODUCTION,0.0659025787965616,"Remark.
The fact that (Madry et al., 2018, Corollary C.2) is false, might be well-known in the
optimization ﬁeld. In the convex setting it corresponds to the common knowledge that a negative
subgradient of a non-smooth convex function might not be a descent direction c.f., (Boyd, 2014,
Section 2.1). However, we believe this is not well-known in the AT community given that (i)
its practical implications i.e., methods deriving steeper descent updates using multiple adversarial
examples, have not been previously introduced, and (ii) the results in Madry et al. (2018) have been
central in the development of AT. Hence, our contribution can be understood as raising awareness
about the issue, and demonstrating its practical implications for AT."
LIT REVIEW,0.06876790830945559,"2
A COUNTEREXAMPLE TO MADRY ET AL. (2018, COROLLARY C.2)"
LIT REVIEW,0.07163323782234957,"Preliminaries. Let θ ∈Rd be the parameters of a model, (x, y) ∼D a data-label distribution, δ a
perturbation in a compact set S0 and L a loss function. The optimization objective of AT is:"
LIT REVIEW,0.07449856733524356,"min
θ
ρ(θ),
where ρ(θ) := E(x,y)∼D"
LIT REVIEW,0.07736389684813753,"
max
δ∈S0 L(θ, x + δ, y)

(1)"
LIT REVIEW,0.08022922636103152,Published as a conference paper at ICLR 2023
LIT REVIEW,0.0830945558739255,"In this setting ρ(θ) is referred to as the adversarial loss or robust loss. In order to optimize Eq. (1)
via iterative ﬁrst-order methods, we need access to an stochastic gradient of the adversarial loss ρ or
at least, the weaker notion of stochastic descent direction i.e., a direction along which the function"
LIT REVIEW,0.08595988538681948,"φ(θ) :=
max
δ∈S:=Sk
0 ("
LIT REVIEW,0.08882521489971347,"g(θ, δ) := 1 k k
X"
LIT REVIEW,0.09169054441260745,"i=1
L(θ, xi + δi, yi) ) (2)"
LIT REVIEW,0.09455587392550144,"decreases in value.
We have collected the perturbations δi ∈S0 on the batch {(xi, yi)}k
i=1 as the
columns of a matrix δ = [δ1, . . . , δk] ∈S := Sk
0 which is also a compact set. To obtain a descent
direction for partial maximization functions like φ we resort to Danskin’s Theorem:
Theorem 1 (Danskin (1966)). Let S be a compact topological space, and let g : Rd × S be a
continuous function such that g(·, δ) is differentiable for all δ ∈S and ∇θg(θ, δ) is continuous on
Rd × S. Let
φ(θ) := max
δ∈S g(θ, δ),
S⋆(θ) := arg max
δ∈S
g(θ, δ)
(3)"
LIT REVIEW,0.09742120343839542,"Let γ ∈Rd with ∥γ∥2 = 1 be an arbitrary unit vector. The directional derivative Dγφ(θ) of φ in
the direction γ at the point θ exists, and is given by the formula"
LIT REVIEW,0.10028653295128939,"Dγφ(θ) =
max
δ∈S⋆(θ)⟨γ, ∇θg(θ, δ)⟩
(4)"
LIT REVIEW,0.10315186246418338,"Remark.
γ ̸= 0 is called a descent direction of φ at θ if and only if Dγφ(θ) < 0, i.e., if the
directional derivative is strictly negative."
LIT REVIEW,0.10601719197707736,"Corollary 1 is an equivalent rephrasing of Madry et al. (2018, Corollary C.2.), and was originally
claimed to be a consequence of Theorem 1. Unfortunately counterexample 1 shows that the corollary
is false. As Theorem 1 (Danskin’s Theorem) is true, this means that there is some mistake in the
proof of the corollary provided in Madry et al. (2018).
Corollary 1. Let δ⋆∈S⋆(θ). If −∇θg(θ, δ⋆) ̸= 0, then it is a descent direction for φ at θ.
Counterexample 1. Let S := [−1, 1] and g(θ, δ) = θδ. The conditions of Danskin’s theorem
clearly hold in this case, and
φ(θ) :=
max
δ∈[−1,1] θδ = |θ|.
(5)"
LIT REVIEW,0.10888252148997135,"Note that at θ = 0, we have S⋆(0) = [−1, 1]. Choosing δ = 1 ∈S⋆(0) we have that g(θ, 1) = θ
and so −∇θg(0, 1) = −1 ̸= 0. Hence, Corollary 1 would imply that −1 is a descent direction for
φ(θ) = |θ|. However, θ = 0 is a global minimizer of the absolute value function, which means that
there exists no descent direction. This is a contradiction."
LIT REVIEW,0.11174785100286533,"To cast more clarity on why Corollary 1 is false, we explain what is the mistake in the proof provided
in Madry et al. (2018). The main issue is the deﬁnition of the directional derivative, a concept in
multivariable calculus that is deﬁned in slightly different ways in the literature.
Deﬁnition 1. Let φ : Rd →R. For a nonzero vector γ ∈Rd, the one-sided directional derivative
of φ in the direction γ at the point θ is deﬁned as the one-sided limit:"
LIT REVIEW,0.11461318051575932,"Dγφ(θ) := lim
t→0+
φ(θ + tγ) −φ(θ)"
LIT REVIEW,0.1174785100286533,"t∥γ∥2
(6)"
LIT REVIEW,0.12034383954154727,The two-sided directional derivative is deﬁned as the two-sided limit:
LIT REVIEW,0.12320916905444126,"ˆDγφ(θ) := lim
t→0
φ(θ + tγ) −φ(θ)"
LIT REVIEW,0.12607449856733524,"t∥γ∥2
(7)"
LIT REVIEW,0.12893982808022922,"Unfortunately, it is not always clear which one of the two notions is meant when the term directional
derivative is used. Indeed, as our notation suggests, the one-sided deﬁnition Eq. (6) is the one used
in the statement of Danskin’s Theorem (Danskin, 1966). However, the proof of Corollary 1 provided
in Madry et al. (2018) mistakenly assumes the two-sided deﬁnition Eq. (7), and inadvertently uses
the following property that holds for ˆDγφ(θ) (Eq. (7)) but not for Dγφ(θ) (Eq. (6)):"
LIT REVIEW,0.1318051575931232,"Lemma 1. For the two-sided directional derivative deﬁnition (7) it holds that −ˆDγφ(θ) =
ˆD−γφ(θ) provided that ˆDγ exists. In particular, if ˆDγφ(θ) > 0 then ˆD−γφ(θ) < 0. However
this is not true for the one-sided directional derivative (6), as the example φ(θ) = |θ| at θ = 0
shows (both directional derivatives are strictly positive)."
LIT REVIEW,0.1346704871060172,Published as a conference paper at ICLR 2023
LIT REVIEW,0.13753581661891118,"We provide a proof of this fact in Appendix E. The (ﬂawed) proof of Corollary 1 provided in Madry
et al. (2018) starts by noting that for a solution δ of the inner-maximization problem, the directional
derivative in the direction γ = ∇θg(θ, δ) is positive, as implied by Danskin’s Theorem:"
LIT REVIEW,0.14040114613180515,"Dγφ(θ) =
max
δ∈S⋆(θ)⟨γ, ∇θg(θ, δ)⟩≥⟨∇θg(θ, δ), ∇θg(θ, δ)⟩= ∥∇θg(θ, δ)∥2 > 0
(8)"
LIT REVIEW,0.14326647564469913,"assuming that ∇θg(θ, δ) is non-zero. The mistake in the proof lies in concluding that D−γφ(θ) < 0.
Following Lemma 1, this property does not hold for the one-sided directional derivative deﬁnition
Eq. (6), the one used in Danskin’s Theorem."
LIT REVIEW,0.14613180515759314,"3
A COUNTEREXAMPLE AT A POINT THAT IS NOT LOCALLY OPTIMAL"
LIT REVIEW,0.1489971346704871,"The question remains whether a slightly modiﬁed version of Corollary 1 holds true: it might be
the case that by adding some mild assumption, we exclude all possible counterexamples. In the
particular case of counterexample 1, θ = 0 is a local optimum of the function φ(θ) = |θ|. At
such points, descent directions do not exist. However, in the trajectory of an iterative optimization
algorithm we are mostly concerned with non-locally-optimal points. Hence, we explore whether
adding the assumption that θ is not locally optimal can make Corollary 1 true. Unfortunately, we
will show that this is not the case."
LIT REVIEW,0.1518624641833811,"To this end we construct a family of counterexamples to Corollary 1 with the following properties:
(i) there exists a descent direction at a point θ (that is, θ is not locally optimal) and (ii), it does not
coincide with −∇θg(θ, δ), for any optimal δ ∈S⋆(θ). Moreover, all the directions −∇θg(θ, δ) are
in fact ascent directions i.e., they lead to an increase in the function φ(θ).
Counterexample 2. Let S := [0, 1] and let u, v ∈R2 be unit vectors such that −1 < ⟨u, v⟩< 0.
That is, u and v form an obtuse angle. Let"
LIT REVIEW,0.15472779369627507,"g(θ, δ) = δ⟨θ, u⟩+ (1 −δ)⟨θ, v⟩+ δ(δ −1)
(9)"
LIT REVIEW,0.15759312320916904,"Clearly, the function satisﬁes all conditions of Theorem 1. At θ = 0, we have that S⋆(0) =
arg maxδ∈[0,1] δ(δ −1) = {0, 1}. At δ = 0 we have ∇θg(θ, 0) = ∇θ⟨θ, v⟩= v and at δ = 1
we have ∇θg(θ, 1) = ∇θ⟨θ, u⟩= u. We compute the value of the directional derivatives in the
negative direction of such vectors. According to Danskin’s Theorem we have"
LIT REVIEW,0.16045845272206305,"D−vφ(0) = max
δ∈{0,1}⟨−v, ∇θg(θ, δ)⟩= max(⟨−v, v⟩, ⟨−v, u⟩) ≥−⟨v, u⟩> 0
(10)"
LIT REVIEW,0.16332378223495703,"where −⟨v, u⟩> 0 holds by construction. Analogously, D−uφ(0) > 0. This means that all such
directions are ascent directions. However, for the direction γ = −(u + v) we have"
LIT REVIEW,0.166189111747851,"Dγφ(θ) ∝max
δ∈{0,1}⟨−(u + v), ∇θg(θ, δ)⟩"
LIT REVIEW,0.16905444126074498,"= max(⟨−u −v, u⟩, ⟨−u −v, v⟩) = −1 −⟨u, v⟩< 0
(11)"
LIT REVIEW,0.17191977077363896,"where the last inequality also follows by construction. Hence, −(u + v) is a descent direction."
LIT REVIEW,0.17478510028653296,"As counterexample 2 shows, Adversarial Training has the following problem: even if we are able to
compute one solution of the inner-maximization problem δ ∈S it can be the case that moving in the
direction −∇θg(θ, δ) increases the robust training loss i.e., the classiﬁer becomes less, rather than
more, robust. This can happen at any stage, independently of the local optimality of θ."
LIT REVIEW,0.17765042979942694,"For a non-locally-optimal θ ∈Rd, the construction of the counterexamples relies on the following:
if for any gradient computed at one inner-max solution, there exist another gradient (at a different
inner-max solution) forming an obtuse angle, then no single inner-max solution yields a descent
direction. Consequently, it sufﬁces to ensure that for any gradient that can be found by solving
the inner problem, there exists another one that has a negative inner product with it. Precisely, our
counterexample 2 is carefully crafted so that this property holds."
IMPLEMENTATION/METHODS,0.18051575931232092,"4
DANSKIN’S DESCENT DIRECTION"
IMPLEMENTATION/METHODS,0.1833810888252149,"Danskin’s Theorem implies that the directional derivative depends on all the solutions of the inner-
max problem S⋆(θ) c.f., Eq. (4). One possible issue in Adversarial Training is relying on a single"
IMPLEMENTATION/METHODS,0.18624641833810887,Published as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.18911174785100288,"solution, as it does not necessarily lead to a descent direction c.f. counterexample 2. To ﬁx this,
we design an algorithm that uses multiple adversarial perturbations per data sample. In theory, we
can obtain the steepest descent direction for the robust loss on a batch {(xi, yi) : i = 1, . . . , k} by
solving the following min-max problem:"
IMPLEMENTATION/METHODS,0.19197707736389685,"γ⋆∈arg min
γ:∥γ∥2=1
max
δ∈S⋆(θ)⟨γ, ∇θg(θ, δ)⟩,
g(θ, δ) := 1 k k
X"
IMPLEMENTATION/METHODS,0.19484240687679083,"i=1
L(θ, xi + δi, yi)
(12)"
IMPLEMENTATION/METHODS,0.1977077363896848,"On the one hand, if the set of maximizers S⋆(θ) is inﬁnite, Eq. (12) would be out of reach for
computationally tractable methods. On the other hand, the solution is trivial if there is a single
maximizer , but we verify experimentally in Section 5 that such assumption is wrong in practice. In
conclusion, a compromise has to be made in order to devise an tractable algorithm that is relevant in
practical scenarios. First, we assume that the set of optimal adversarial perturbations is ﬁnite:"
IMPLEMENTATION/METHODS,0.20057306590257878,"S⋆(θ) := arg max
δ∈S
g(θ, δ) = S⋆
m(θ) ={δ(1), . . . , δ(m)},
m ≥1, m ∈Z
(13)"
IMPLEMENTATION/METHODS,0.2034383954154728,"Under such assumption, it is possible to compute the steepest descent direction in Eq. (12) efﬁciently.
Theorem 2. Let ∆m be the m-dimensional simplex i.e., α ≥0, Pm
i=1 αi = 1.
Suppose
that S⋆(θ) = S⋆
m(θ) :={δ(1), . . . , δ(m)} and denote by ∇θg(θ, S⋆
m(θ)) the matrix with columns
∇θg(θ, δ(i)) for i = 1, . . . , m. As long as θ is not a local minimizer of the robust loss φ(θ) =
maxδ∈S g(θ, δ), then the steepest descent direction of φ at θ can be computed as:"
IMPLEMENTATION/METHODS,0.20630372492836677,"γ⋆:= −∇θg(θ, S⋆
m(θ))α⋆"
IMPLEMENTATION/METHODS,0.20916905444126074,"∥∇θg(θ, S⋆m(θ))α⋆∥,
α⋆∈arg min
α∈∆m ∥∇θg(θ, S⋆
m(θ))α∥2
2
(14)"
IMPLEMENTATION/METHODS,0.21203438395415472,"We present the proof of Theorem 2 in Appendix C. We now relax our initial ﬁniteness assumption
Eq. (13), as it might not hold in practice. We show that it might sufﬁce to approximate the (possibly
inﬁnite) set of maximizers S⋆(θ) with a ﬁnite set S⋆
m(θ). If the direction γ⋆deﬁned in Eq. (14)
satisﬁes an additional inequality involving the ﬁnite set S⋆
m(θ), it will be a certiﬁed descent direction.
Theorem 3. Suppose that ∇θg(θ, δ) is L-Lipschitz as a function of δ, i.e., ∥∇θg(θ, δ) −
∇θg(θ, δ′)∥2 ≤L∥δ −δ′∥2. Let S⋆(θ) be the set of solutions of the inner maximization prob-
lem, and let S⋆
m(θ) := {δ(1), . . . , δ(m)} be a ﬁnite set that ϵ-approximates S⋆(θ) in the following
sense: for any δ ∈S⋆(θ) there exists δ(i) ∈S⋆
m(θ) such that ∥δ −δ(i)∥2 ≤ϵ. Let γ⋆be as in
Eq. (14). If maxδ∈S⋆
m(θ)⟨γ⋆, ∇θg(θ, δ)⟩< −Lϵ then γ⋆is a descent direction for φ at θ."
IMPLEMENTATION/METHODS,0.2148997134670487,"The Lipschitz gradient assumption in Theorem 3 is standard in the optimization literature. We
provide a proof of Theorem 3 in Appendix D. This results motivate Danskin’s Descent Direction
(Algorithm 1). We assume an oracle providing a ﬁnite set of adversarial perturbations S⋆
m(θ) that
satiﬁes the approximation assumption in Theorem 3. In particular, this does not require solving
the inner-maximization problem to optimality, which is out of reach for computationally tractable
methods and requires expensive branch-and-bound or MIP techniques (Zhang et al., 2022; Tjeng
et al., 2019; Palma et al., 2021; Wang et al., 2021). Given S⋆
m(θ), we compute γ⋆as in Eq. (14),
which corresponds to Line 7 of Algorithm 1. If the values of L and ϵ in Theorem 3 are not available
(they might be hard to compute), we cannot certify that γ⋆is a descent direction. However, note that
given a set of adversarial examples S⋆
m(θ), γ⋆is still the best choice as it ensures we improve the
loss on all elements of S⋆
m(θ)."
IMPLEMENTATION/METHODS,0.2177650429799427,"The optimization problem deﬁning α⋆and γ⋆can be solved to arbitrary accuracy efﬁciently: It
corresponds to the minimization of a smooth objective subject to the convex constraint α ∈∆m.
We use the accelerated PGD algorithm proposed in (Parikh et al., 2014, section 4.3) and pair it with
the efﬁcient simplex projection algorithm given in Duchi et al. (2008). As the problem is smooth,
a ﬁxed step-size choice guarantees convergence. We set it as the inverse of the spectral norm of
∇θg(θ, S⋆(θ))⊤∇θg(θ, S⋆(θ)) and run the algorithm for a ﬁxed number of iterations. Alternatively,
one can consider Frank-Wolfe with away steps (Lacoste-Julien & Jaggi, 2015)."
IMPLEMENTATION/METHODS,0.22063037249283668,"In practice, the theoretical oracle algorithm that computes the set S⋆
m(θ) is replaced by heuristics
like performing multiple runs of the Fast Gradient Sign Method (FGSM) or Iterative FGSM (Ku-
rakin et al., 2017) (referred to as PGD in Madry et al. (2018)). The complexity of an iteration in
Algorithm 1 depends on this choice. In Section 5 we explore different choices and how it affects the
the performance of the method."
IMPLEMENTATION/METHODS,0.22349570200573066,Published as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.22636103151862463,Algorithm 1 Danskin’s Descent Direction (DDi)
IMPLEMENTATION/METHODS,0.22922636103151864,"1: Input: Batch size k ≥1, number of adversarial examples m, initial iterate θ0 ∈Rd, number of
iterations T ≥1, step-sizes {βt}T
t=1.
2: for t = 0 to T −1 do
3:
Draw (x1, y1), . . . , (xk, yk) from data distribution D
4:
g(θ, δ) ←1"
IMPLEMENTATION/METHODS,0.23209169054441262,"k
Pk
i=1 L(θ, x + δi, yi)
5:
δ(1), . . . , δ(m) ←MAXIMIZEδ∈Sg(θt, δ)
▷Using a heuristic like PGD
6:
M ←

∇θg(θt, δ(i)) : i = 1, . . . , m

∈Rd×m"
IMPLEMENTATION/METHODS,0.2349570200573066,"7:
α⋆←MINIMIZEα∈∆m∥Mα∥2
2
▷To ϵ-suboptimality
8:
γ⋆←
Mα⋆
∥Mα⋆∥2
9:
θt+1 ←θt + βtγ⋆"
IMPLEMENTATION/METHODS,0.23782234957020057,"10: end for
11: return θT"
RESULTS/EXPERIMENTS,0.24068767908309455,"5
EXPERIMENTS"
RESULTS/EXPERIMENTS,0.24355300859598855,"5.1
EXISTENCE OF MULTIPLE OPTIMAL ADVERSARIAL SOLUTIONS"
RESULTS/EXPERIMENTS,0.24641833810888253,"This section provides evidence that the set of optimal adversarial examples for a given sample is not
a singleton. The hypothesis is tested by using a ResNet-18 pretrained on CIFAR10 and computing
multiple randomly initialized PGD-7 attacks for each image with ε =
8
255. We compute all pairwise
ℓ2-distances between attacks for a given image and plot a joint histogram for 10 examples in Fig-
ure 2. There is a clear separation away from zero for all pairwise distances indicating that the attacks
are indeed distinct in the input space. Additionally, we plot a histogram over the adversarial losses
for each image. An example is provided in Figure 2, which is corroborated by similar results for
other images (see Figure 6§B). We ﬁnd that the adversarial losses all concentrate with low variance
far away from the clean loss. This conﬁrms that all perturbations are in fact both strong and distinct."
RESULTS/EXPERIMENTS,0.2492836676217765,"0.0
0.2
0.4
0.6
0.8
1.0
∥δ −δ′∥2 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
RESULTS/EXPERIMENTS,0.2521489971346705,Density
RESULTS/EXPERIMENTS,0.25501432664756446,"0.56
0.58
0.60
0.62
0.64
Loss 0 200 400 600 800 1000"
RESULTS/EXPERIMENTS,0.25787965616045844,Density
RESULTS/EXPERIMENTS,0.2607449856733524,"Clean loss
Adversarial loss"
RESULTS/EXPERIMENTS,0.2636103151862464,"Figure 2: Non-uniqueness of an optimal adversarial perturbation. (left) Pairwise ℓ2-distances be-
tween PGD-based perturbations are bounded away from zero by a large margin, showing that they
are distinct. (right) The losses of multiple perturbations on the same sample concentrate around a
value much larger than the clean loss (see Fig. 7 for zoomed-in version)."
RESULTS/EXPERIMENTS,0.2664756446991404,"5.2
EXPLORING THE OPTIMIZATION LANDSCAPE OF DDI AND STANDARD ADVERSARIAL
TRAINING"
RESULTS/EXPERIMENTS,0.2693409742120344,"Having established that there exist multipe adversarial examples, we now show that the gradients
computed can exhibit the behaviors discussed in Section 3. In a ﬁrst synthetic example we borrow
from (Orabona, 2019, Chapter 6), we consider the function g(θ, δ) = δ

θ2
1 + (θ2 + 1)2
+ (1 −
δ)

θ2
1 + (θ2 −1)2
where θ ∈R2 and δ ∈[0, 1]. As can be seen from Figure 1a and Figure 1b,
following a gradient computed at a single example leads to a increase in the objective and an unstable
optimization behavior despite the use of a decaying step-size."
RESULTS/EXPERIMENTS,0.2722063037249284,Published as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.27507163323782235,"In a second synthetic examples, we consider robust binary classiﬁcation with a feed-forward neural
network on a synthetic 2-dimensional dataset, trained with batch gradient descent. We observe
that during training, after an initial phase where all gradients computed at different perturbations
point roughly in the same direction, we begin to observe pairs of gradients with negative inner-
products (see Figure 3 (left)). That means that following one of those gradients would lead to an
increase of the robust loss, as shown by the different optimization behavior (see Figure 3 (center)).
Therefore, the beneﬁts DDi kick in later in training, once the loss has stabilized and the inner-
solver starts outputting gradients with negative inner products. Indeed, we see that in the middle of
training (iteration 250), DDi ﬁnds a descent direction of the (linearized) robust objective, whereas
all individual gradients lead to an increase."
RESULTS/EXPERIMENTS,0.27793696275071633,"0
100
200
300
400
500
Iteration 0 5 10 15 20 25 30"
RESULTS/EXPERIMENTS,0.2808022922636103,Negative inner-products count
RESULTS/EXPERIMENTS,0.2836676217765043,"DDi
PGD"
RESULTS/EXPERIMENTS,0.28653295128939826,"0
100
200
300
400
500
Iteration 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70"
RESULTS/EXPERIMENTS,0.28939828080229224,Robust Loss
RESULTS/EXPERIMENTS,0.2922636103151863,"DDi
PGD"
RESULTS/EXPERIMENTS,0.29512893982808025,"0.000
0.002
0.004
0.006
0.008
0.010
Step along normalized direction"
RESULTS/EXPERIMENTS,0.2979942693409742,0.3664
RESULTS/EXPERIMENTS,0.3008595988538682,0.3666
RESULTS/EXPERIMENTS,0.3037249283667622,0.3668
RESULTS/EXPERIMENTS,0.30659025787965616,0.3670
RESULTS/EXPERIMENTS,0.30945558739255014,0.3672
RESULTS/EXPERIMENTS,0.3123209169054441,0.3674
RESULTS/EXPERIMENTS,0.3151862464183381,Robust Loss
RESULTS/EXPERIMENTS,0.31805157593123207,"PGD 0
PGD 1
PGD 2
PGD 3
PGD 4
PGD 5
PGD 6
PGD 7
PGD 8
PGD 9
DDi"
RESULTS/EXPERIMENTS,0.3209169054441261,"Figure 3: Count of negative inner products pairs among the 10 gradients computed per iteration(left),
corresponding robust loss behavior along optimization (center). At iteration 250, comparison of the
direction obtained by DDi and individual gradients.(right)."
RESULTS/EXPERIMENTS,0.3237822349570201,"5.3
ACCURACY/ROBUSTNESS COMPARISON OF DDI VS ADVERSARIAL TRAINING"
RESULTS/EXPERIMENTS,0.32664756446991405,"We compare the robust test and training error of Adversarial Training vs our proposed method DDi,
on the CIFAR10 benchmark. As baseline we use ℓ∞-PGD with ϵ = 8/255, α = 2/255, ninner = 7.
We train a ResNet18 with SGD, using the settings from Pang et al. (2021), Table 1 except for some
modiﬁcations noted below. This means SGD with hyperparameters lr= 0.1, momentum=0.0 (not
the default 0.9, we explain why below), batch size= 128 and weight decay= 5e −4. We
run for 200 epochs, no warmup, decreasing lr by a factor of 0.1 at 50% and 75% of the epochs."
RESULTS/EXPERIMENTS,0.32951289398280803,"Satisfying theoretical assumptions:
Real world architectures are often not covered by theory
while simple toy examples are often far removed from practice. To demonstrate the real world
impact of our results, we therefore study a setting where the conditions of Danskin’s Theorem hold,
but which also uses standard building blocks used by practitioners, speciﬁcally replacing ReLU
with CELU(Barron, 2017), replacing BatchNorm (BN) (Ioffe & Szegedy, 2015) with GroupNorm
(GN) (Wu & He, 2018) and removing momentum. This ensures differentiability, removes intra-
batch dependencies and ensures each update depends only on the descent direction found at that
step respecively. We present more detailed justiﬁcation in Appendix B.2 due to space constraints
and additionally show an ablation study on the effect of our modiﬁcations in (Section 5.3) 1."
RESULTS/EXPERIMENTS,0.332378223495702,"Our main results can be seen in Section 5.3. The robust accuracy of the DDi-trained model increases
much more rapidly in the early stages, it increases more after the ﬁrst drop in the learning rate, and
is more stable when compared to the baseline. Section 5.3 also gives evidence that our method has
(generally positive or neutral) effects in all settings. Using ReLU instead of CELU re-introduces
the characteristic bump in robust accuracy that has led to early stopping becoming standard practice
in robust training. It also diminishes the beneﬁt of DDi, but DDi remains on par with PGD in terms
of training speed and decays slightly less towards the end of the training. Adding momentum does
not help either method in terms of training speed and makes them behave almost identically."
RESULTS/EXPERIMENTS,0.335243553008596,"Finally, BN seems to signiﬁcantly ease the optimisation for both methods, raising overall perfor-
mance and amplifying the bump on both methods. Here, PGD actually reaches a higher maximum
robust accuracy and rises faster initially, but then converges to a lower value. This implies that some
beneﬁts of DDi remain even outside the setting covered by the theory."
RESULTS/EXPERIMENTS,0.33810888252148996,"1It is worth noting that the early stopping robust accuracy we achieve in ablations approximately matches
that reported in Engstrom et al. (2019) on resnet50"
RESULTS/EXPERIMENTS,0.34097421203438394,Published as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.3438395415472779,"0
25
50
75
100
125
150
175
200
Epoch 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50"
RESULTS/EXPERIMENTS,0.3467048710601719,Robust accuracy
RESULTS/EXPERIMENTS,0.3495702005730659,"DDi
PGD"
RESULTS/EXPERIMENTS,0.3524355300859599,"0
25
50
75
100
125
150
175
200
Epoch 0.1 0.2 0.3 0.4 0.5"
RESULTS/EXPERIMENTS,0.3553008595988539,Robust accuracy
RESULTS/EXPERIMENTS,0.35816618911174786,"DDi-Theory
PGD-Theory
DDi-BN-NonSmooth
DDi-NonSmooth
DDi-NonSmooth+Mom
PGD-BN-NonSmooth
PGD-NonSmooth
PGD-NonSmooth+Mom"
RESULTS/EXPERIMENTS,0.36103151862464183,"Figure 4: (left) Evolution of the robust accuracy on the CIFAR10 validation set, using a standard
PGD-20 adversary for evaluation and DDi/PGD-7 during training. (right) an ablation testing the
effect of adding the elements not covered by theory (BN,ReLU,momentum) back into our setting."
RESULTS/EXPERIMENTS,0.3638968481375358,"Although these are promising results indicating that DDi can give real world beneﬁts in terms of
iterations and reduce the need for early stopping, it is worth asking whether once could get the same
beneﬁt with a simpler or cheaper method. The ﬁnal robust accuracies obtained are very close, and
the increased convergence rate in terms of steps comes at a more than 10x slowdown due to having
to perform 10 independent forward-backward passes and then solving an additional inner problem.
Additionally, it could be argued that these results are to be expected and trivial: we are spending 10x
the compute to get 10x the gradients."
RESULTS/EXPERIMENTS,0.3667621776504298,"One might even say there is no need to solve the inner product and a simpler method to select the
best adversary would sufﬁce. In Fig. 5a we address these concerns by comparing Section 5.3 to the
results of the following variants attempting to match the computational complexity: PGD-70 runs a
single PGD adversary for 10x the number of steps, PGD-70 −1"
RESULTS/EXPERIMENTS,0.36962750716332377,"t runs a single PGD adversary for
10x the number of steps, using a 1/t learning rate decay after leaving the ”standard” PGD regime
(i.e. after 8 adversary steps) to converge closer to an optimal adversarial example, PGD-max-10
runs ten parallel, independent PGD adversaries for each image and select the adversarial example
that induces the largest loss. Finally, PGD-min-10 runs ten parallel, independent PGD adversaries
for each image, then computes the gradients and selects the one with the lowest norm.This is an
approximation of DDi that avoids solving Line 7 in Algorithm 1."
RESULTS/EXPERIMENTS,0.37249283667621774,"In Fig. 5b we create a DDi variant based on the FAST adversary (Wong et al., 2020) (using ϵ =
8/255, α = 10/255). Using PGD for the evaluation attack, we compare against vanilla FAST in
our setting (no BN, momentum and using CELU) as well as a FAST-max-10 variant analoguous to
PGD-max-10. As we can see in Fig. 5a, every step of the pipeline of DDi seems to be necessary, with
none of the PGD variants achieving the fast initial rise in robustness. PGD-70 −1"
RESULTS/EXPERIMENTS,0.3753581661891118,"t and PGD-min-10
reach a higher ﬁnal robust accuracy, which we attribute to the higher quality adversarial example
and informed selection respectively. This is corroborated in Fig. 5b. Using a single step adversary
is sufﬁcient to speed up convergence in the early stages of training, but does not reach the same ﬁnal
robust accuracy."
RESULTS/EXPERIMENTS,0.37822349570200575,"PGD and DDi seem to behave similarly in the later stages of training. We would suggest a compu-
tationally cheaper DDi variant which uses single ascent steps (FAST) in the beginning of training
and PGD in the later stages. In any case, the bulk of the overhead lies in the subroutine in Line 7
of Algorithm 1. A faster approximate solution could also speed up the method signiﬁcantly. Such
incremental improvements are left for future work Neverthelss, in Appendix B.4 we explore some
modiﬁcations that can reduce the runtime of Algorithm 1 by at least 70% while retaining its beneﬁts."
LIT REVIEW,0.38108882521489973,"6
RELATED WORK"
LIT REVIEW,0.3839541547277937,"Wang et al. (2019) derive suboptimality bounds for the robust training problem, under a locally
strong concavity assumption on the inner-maximization problem. However, such results do not
extend to Neural Networks, as the inner-maximization problem is not strongly concave, in general."
LIT REVIEW,0.3868194842406877,Published as a conference paper at ICLR 2023
LIT REVIEW,0.38968481375358166,"0
25
50
75
100
125
150
175
200
Epoch 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50"
LIT REVIEW,0.39255014326647564,Robust accuracy
LIT REVIEW,0.3954154727793696,"DDi
PGD"
LIT REVIEW,0.3982808022922636,PGD-70-1
LIT REVIEW,0.40114613180515757,"t
PGD-max-10
PGD-70
PGD-min-10 (a)"
LIT REVIEW,0.4040114613180516,"0
25
50
75
100
125
150
175
200
Epoch 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50"
LIT REVIEW,0.4068767908309456,Robust accuracy
LIT REVIEW,0.40974212034383956,"DDi-FAST
FAST
FAST-max-10 (b)"
LIT REVIEW,0.41260744985673353,"Figure 5: (a) Ablations comparing PGD-variants matching the number of adversarial gradients/steps
used for DDi. (b) Ablation over single-step adversaries (FAST/DDi-FAST)."
LIT REVIEW,0.4154727793696275,"In contrast, we do not make unrealistic assumptions like strong concavity, and we deal with the
existence multiple solutions of the inner-maximization problem."
LIT REVIEW,0.4183381088825215,"In Nouiehed et al. (2019), it is shown that if the inner-maximization problem is unconstrained and
satisﬁes the PL-condition, it is differentiable, and the gradient can be computed after obtaining a
single solution of the problem. However, in the robust learning problem the adversary is usually
constrained to a compact set, and the PL condition does not hold generically. This renders such
assumptions hard to justify in the AT setting."
LIT REVIEW,0.42120343839541546,"Tramer & Boneh (2019); Maini et al. (2020) study robustness to multiple perturbation types, which
might appear similar to our approach, but is not. Such works strike to train models that are simul-
taneously robust against ℓ∞- and ℓ2-bounded perturbations, for example. In contrast, we focus on a
single perturbation type, and we study how to use multiple adversarial examples of the same sample
to improve the update directions of the network parameters."
LIT REVIEW,0.42406876790830944,"Finally, we back our claim that the falseness of Madry et al. (2018, Corollary C.2.) is not well-
known in the literature on Adversarial Training. For example, such result is included in the textbook
(Vorobeychik et al., 2018, Proposition 8.1). It has also been either reproduced or mentioned in
conference papers like Liu et al. (2020, Section 2), Viallard et al. (2021, Appendix B), Wei & Ma
(2020, Section 5) and possibly many others. This supports our claim that raising awareness about
the mistake in the proof is an important contribution."
CONCLUSION/DISCUSSION,0.4269340974212034,"7
CONCLUSION"
CONCLUSION/DISCUSSION,0.4297994269340974,"In this paper we presented a formal proof, counter examples and evidence about the real world im-
pact of the fact that a foundational corollary of the Adversarial Training literature is in fact false.
Raising awareness about an incorrect claim that has been present in the Adversarial Training lit-
erature may provide opportunities to develop improved variants of the method. Indeed, we see
some improvents in an implementable algorithm that align with our theoretical arguments: DDi
exploits multiple approximate solutions of the inner-maximization problem, yields better updates
for the parameters of the network and improves the optimization dynamics. However, it is impor-
tant to remember the limitations and opportunities for future work: our algorithm requires multiple
forward-backward passes and one additional optimization problem. Reducing the overhead over the
vanlla PGD method would certainly make our results truly practical."
CONCLUSION/DISCUSSION,0.4326647564469914,"Non-smooth activations and the use of Batch Normalization or momentum still falls outside the
scope of existing theory but might achieve better performance in benchmarks. To date, this requires
using precise hyperparameters and tricks like early-stopping, that have only been found to work
a-posteriori through extensive trial and error. Since we observe lower decay even in such setting,
future work extending the analysis to cover this case might help alleviate this cost."
CONCLUSION/DISCUSSION,0.4355300859598854,Published as a conference paper at ICLR 2023
CONCLUSION/DISCUSSION,0.4383954154727794,ACKNOWLEDGMENTS
CONCLUSION/DISCUSSION,0.44126074498567336,"This work is funded (in part) through a PhD fellowship of the Swiss Data Science Center, a joint
venture between EPFL and ETH Zurich. Igor Krawczuk, Leello Dadi, Thomas Pethick and Volkan
Cevher acknowledge funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme (grant agreement n◦725594 - time-
data). This work was supported by the Swiss National Science Foundation (SNSF) under grant
number 200021 205011."
CONCLUSION/DISCUSSION,0.44412607449856734,"This work is licensed under a Creative Commons “Attribution 3.0 Un-
ported” license."
REFERENCES,0.4469914040114613,REFERENCES
REFERENCES,0.4498567335243553,"Jonathan T. Barron. Continuously differentiable exponential linear units, 2017. URL https:
//arxiv.org/abs/1704.07483. 7, 14"
REFERENCES,0.45272206303724927,"Philipp Benz, Chaoning Zhang, and In So Kweon. Batch normalization increases adversarial vulner-
ability and decreases adversarial transferability: A non-robust feature perspective. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pp. 7818–7827, 2021. 14"
REFERENCES,0.45558739255014324,"Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normal-
ization. Advances in neural information processing systems, 31, 2018. 14"
REFERENCES,0.4584527220630373,"Stephen Boyd.
Subgradient methods.
https://web.stanford.edu/class/ee364b/
lectures/subgrad_method_notes.pdf, 2014. [Online; accessed 27-September-2022].
2"
REFERENCES,0.46131805157593125,"Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale
image recognition without normalization. In International Conference on Machine Learning, pp.
1059–1071. PMLR, 2021. 14"
REFERENCES,0.46418338108882523,"Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flam-
marion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adver-
sarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020. 1"
REFERENCES,0.4670487106017192,"John M. Danskin. The theory of max-min, with applications. SIAM Journal on Applied Mathe-
matics, 14(4):641–664, 1966. doi: 10.1137/0114053. URL https://doi.org/10.1137/
0114053. 2, 3"
REFERENCES,0.4699140401146132,"Yinpeng Dong, Zhijie Deng, Tianyu Pang, Jun Zhu, and Hang Su. Adversarial distributional train-
ing for robust deep learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 8270–8283. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/5de8a36008b04a6167761fa19b61aa6c-Paper.pdf. 1"
REFERENCES,0.47277936962750716,"Shiv Ram Dubey, Satish Kumar Singh, and Bidyut Baran Chaudhuri.
Activation functions in
deep learning: A comprehensive survey and benchmark. Neurocomputing, 503:92–108, 2022.
ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2022.06.111. URL https://www.
sciencedirect.com/science/article/pii/S0925231222008426. 14"
REFERENCES,0.47564469914040114,"John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto
the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference
on Machine learning, pp. 272–279, 2008. 5"
REFERENCES,0.4785100286532951,"Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness
(python library), 2019. URL https://github.com/MadryLab/robustness. 7"
REFERENCES,0.4813753581661891,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-
ings, 2015. URL http://arxiv.org/abs/1412.6572. 1"
REFERENCES,0.48424068767908307,Published as a conference paper at ICLR 2023
REFERENCES,0.4871060171919771,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015. 7, 14"
REFERENCES,0.4899713467048711,"Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann, Ming Zhou, and Klaus
Neymeyr. Exponential convergence rates for batch normalization: The power of length-direction
decoupling in non-convex optimization. In The 22nd International Conference on Artiﬁcial Intel-
ligence and Statistics, pp. 806–815. PMLR, 2019. 14"
REFERENCES,0.49283667621776506,"Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In
International Conference on Learning Representations, 2017. URL https://openreview.
net/forum?id=BJm4T4Kgx. 5"
REFERENCES,0.49570200573065903,"Simon Lacoste-Julien and Martin Jaggi.
On the global linear convergence of frank-wolfe opti-
mization variants. In Proceedings of the 28th International Conference on Neural Information
Processing Systems - Volume 1, NIPS’15, pp. 496–504, Cambridge, MA, USA, 2015. MIT Press.
5"
REFERENCES,0.498567335243553,"Sijia Liu, Songtao Lu, Xiangyi Chen, Yao Feng, Kaidi Xu, Abdullah Al-Dujaili, Mingyi Hong, and
Una-May O’Reilly. Min-max optimization without gradients: Convergence and applications to
black-box evasion and poisoning attacks. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 6282–6293. PMLR, 13–18 Jul 2020. URL https://proceedings.
mlr.press/v119/liu20j.html. 9"
REFERENCES,0.501432664756447,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018. 1, 2, 3, 4, 5, 9"
REFERENCES,0.504297994269341,"Pratyush Maini, Eric Wong, and Zico Kolter. Adversarial robustness against the union of mul-
tiple perturbation models. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learn-
ing Research, pp. 6640–6650. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.
press/v119/maini20a.html. 9"
REFERENCES,0.5071633237822349,"Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn.
Solving a class of non-convex min-max games using iterative ﬁrst order methods.
In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alche Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
25048eb6a33209cb5a815bff0cf6887c-Paper.pdf. 9"
REFERENCES,0.5100286532951289,"Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213,
2019. 6"
REFERENCES,0.5128939828080229,"Alessandro De Palma, Harkirat Behl, Rudy R Bunel, Philip Torr, and M. Pawan Kumar. Scaling the
convex barrier with active sets. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=uQfOy7LrlTR. 5"
REFERENCES,0.5157593123209169,"Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial
training. In International Conference on Learning Representations, 2021. URL https://
openreview.net/forum?id=Xb8xvrtB8Ce. 7"
REFERENCES,0.5186246418338109,"Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends R⃝in Optimization,
1(3):127–239, 2014. 5"
REFERENCES,0.5214899713467048,"Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization?
Advances in neural information processing systems, 31, 2018.
14"
REFERENCES,0.5243553008595988,"Maurice Sion. On general minimax theorems. Paciﬁc Journal of Mathematics, 8(1):171 – 176,
1958. doi: pjm/1103040253. URL https://doi.org/. 16"
REFERENCES,0.5272206303724928,Published as a conference paper at ICLR 2023
REFERENCES,0.5300859598853869,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Good-
fellow, and Rob Fergus.
Intriguing properties of neural networks.
In Yoshua Bengio and
Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.
URL http:
//arxiv.org/abs/1312.6199. 1"
REFERENCES,0.5329512893982808,"Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with
mixed integer programming. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=HyGIdiRqtm. 5"
REFERENCES,0.5358166189111748,"Florian Tramer and Dan Boneh.
Adversarial training and robustness for multiple perturba-
tions.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alch´e-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
5d4ae76f053f8f2516ad12961ef7fe97-Paper.pdf. 9"
REFERENCES,0.5386819484240688,"Paul Viallard, Pascal Germain, Amaury Habrard, and Emilie Morvant. Self-bounding majority vote
learning algorithms by the direct minimization of a tight pac-bayesian c-bound. In Nuria Oliver,
Fernando P´erez-Cruz, Stefan Kramer, Jesse Read, and Jose A. Lozano (eds.), Machine Learning
and Knowledge Discovery in Databases. Research Track, pp. 167–183, Cham, 2021. Springer
International Publishing. ISBN 978-3-030-86520-7. 9"
REFERENCES,0.5415472779369628,"Yevgeniy Vorobeychik, Murat Kantarcioglu, and Ronald Brachman. Adversarial Machine Learning.
Morgan & Claypool Publishers, 2018. ISBN 1681733951. 9"
REFERENCES,0.5444126074498568,"Haotao Wang, Aston Zhang, Shuai Zheng, Xingjian Shi, Mu Li, and Zhangyang Wang. Removing
batch normalization boosts adversarial training. In International Conference on Machine Learn-
ing, pp. 23433–23445. PMLR, 2022. 14"
REFERENCES,0.5472779369627507,"Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.
Beta-CROWN: Efﬁcient bound propagation with per-neuron split constraints for neural network
robustness veriﬁcation. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.),
Advances in Neural Information Processing Systems, 2021. URL https://openreview.
net/forum?id=ahYIlRBeCFw. 5"
REFERENCES,0.5501432664756447,"Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu.
On the
convergence and robustness of adversarial training. In Kamalika Chaudhuri and Ruslan Salakhut-
dinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 6586–6595. PMLR, 09–15 Jun 2019. URL
https://proceedings.mlr.press/v97/wang19i.html. 8"
REFERENCES,0.5530085959885387,"Colin Wei and Tengyu Ma. Improved sample complexities for deep neural networks and robust
classiﬁcation via an all-layer margin. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=HJe_yR4Fwr. 9"
REFERENCES,0.5558739255014327,"Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=BJx040EFvH. 8"
REFERENCES,0.5587392550143266,"Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), pp. 3–19, 2018. 7, 14"
REFERENCES,0.5616045845272206,"Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang.
A uniﬁed analysis of stochastic
momentum methods for deep learning.
In Proceedings of the Twenty-Seventh International
Joint Conference on Artiﬁcial Intelligence, IJCAI-18, pp. 2955–2961. International Joint Con-
ferences on Artiﬁcial Intelligence Organization, 7 2018. doi: 10.24963/ijcai.2018/410. URL
https://doi.org/10.24963/ijcai.2018/410. 14"
REFERENCES,0.5644699140401146,"Huan Zhang, Shiqi Wang, Kaidi Xu, Yihan Wang, Suman Jana, Cho-Jui Hsieh, and Zico Kolter.
A branch and bound framework for stronger adversarial attacks of ReLU networks.
In Ka-
malika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato
(eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of
Proceedings of Machine Learning Research, pp. 26591–26604. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/zhang22ae.html. 5"
REFERENCES,0.5673352435530086,Published as a conference paper at ICLR 2023
OTHER,0.5702005730659025,"A
MORE ON COUNTEREXAMPLES"
OTHER,0.5730659025787965,"Here we give more details on the construction of the counterexamples. First observe that for a given
point θ0, and a direction γ, if there exists a δ0 ∈S⋆(θ0) such that ⟨γ, ∇θg(θ0, δ)⟩> 0, then γ is not
a descent direction since Dγφ(θ0) ≥0."
OTHER,0.5759312320916905,"In order to ensure that no descent directions can be recovered by solving the inner-maximization,
it sufﬁces to guarantee that for any δ
∈
S⋆(θ0), there exists δ′
∈
S⋆(θ0) such that
⟨∇θg(θ0, δ′), ∇θg(θ0, δ)⟩< 0. This way, neither −∇θg(θ0, δ) nor −∇θg(θ0, δ′) would be descent
directions."
OTHER,0.5787965616045845,"It easy to generate instances verifying the above using linear functions. More formally, by taking
any family of vectors V = {v1, . . . , vn} such that for any i ∈{1, . . . , n} there exists j ∈{1, . . . , n}
such that ⟨vi, vj⟩< 0, we can construct the objective g(θ, δ) = P δiv⊤
i (θ −θ0) −H(δ), where δ is
in the n-dimensional Simplex and H is the Shannon entropy. Solving the inner-maximization would
yield any one of the vectors {v1, . . . , vn}, and by construction, none of them are descent directions."
OTHER,0.5816618911174785,"B
EXPERIMENTS"
OTHER,0.5845272206303725,"B.1
MULTIPLE ATTACKS"
OTHER,0.5873925501432665,"0.625
0.650
0.675
0.700
0.725
0.750
0.775
0.800
0.825
Loss 0 50 100 150 200 250"
OTHER,0.5902578796561605,Density
OTHER,0.5931232091690545,"Clean loss
Adversarial loss"
OTHER,0.5959885386819485,"1.2
1.3
1.4
1.5
1.6
1.7
Loss 0 10 20 30 40 50 60"
OTHER,0.5988538681948424,Density
OTHER,0.6017191977077364,"Clean loss
Adversarial loss"
OTHER,0.6045845272206304,"0.70
0.75
0.80
0.85
0.90
0.95
Loss 0 50 100 150 200 250 300"
OTHER,0.6074498567335244,Density
OTHER,0.6103151862464183,"Clean loss
Adversarial loss"
OTHER,0.6131805157593123,"0.58
0.59
0.60
0.61
0.62
Loss 0 200 400 600 800"
OTHER,0.6160458452722063,Density
OTHER,0.6189111747851003,"Clean loss
Adversarial loss"
OTHER,0.6217765042979942,"0.46
0.48
0.50
0.52
0.54
0.56
0.58
Loss 0 200 400 600 800 1000 1200 1400"
OTHER,0.6246418338108882,Density
OTHER,0.6275071633237822,"Clean loss
Adversarial loss"
OTHER,0.6303724928366762,"0.90
0.92
0.94
0.96
0.98
1.00
1.02
1.04
Loss 0 100 200 300 400 500 600 700"
OTHER,0.6332378223495702,Density
OTHER,0.6361031518624641,"Clean loss
Adversarial loss"
OTHER,0.6389684813753582,"0.700
0.725
0.750
0.775
0.800
0.825
0.850
Loss 0 25 50 75 100 125 150 175 200"
OTHER,0.6418338108882522,Density
OTHER,0.6446991404011462,"Clean loss
Adversarial loss"
OTHER,0.6475644699140402,"0.44
0.46
0.48
0.50
0.52
0.54
0.56
Loss 0 20 40 60 80 100"
OTHER,0.6504297994269341,Density
OTHER,0.6532951289398281,"Clean loss
Adversarial loss"
OTHER,0.6561604584527221,"0.975
1.000
1.025
1.050
1.075
1.100
1.125
1.150
Loss 0 50 100 150 200"
OTHER,0.6590257879656161,Density
OTHER,0.66189111747851,"Clean loss
Adversarial loss"
OTHER,0.664756446991404,"Figure 6: The losses of multiple perturbations on 9 different example all concentrate around a value
much larger than the clean loss. See Section 5.1 for experimental details. The histograms have been
enlarged in Figure 7."
OTHER,0.667621776504298,Published as a conference paper at ICLR 2023
OTHER,0.670487106017192,"0.831
0.832
0.833
0.834
0.835
0.836
Loss 0 50 100 150 200 250"
OTHER,0.673352435530086,Density
OTHER,0.6762177650429799,Adversarial loss
OTHER,0.6790830945558739,"1.660
1.665
1.670
1.675
1.680
1.685
Loss 0 10 20 30 40 50 60"
OTHER,0.6819484240687679,Density
OTHER,0.6848137535816619,Adversarial loss
OTHER,0.6876790830945558,"0.934
0.935
0.936
0.937
0.938
0.939
0.940
Loss 0 50 100 150 200 250 300"
OTHER,0.6905444126074498,Density
OTHER,0.6934097421203438,Adversarial loss
OTHER,0.6962750716332379,"0.6220
0.6225
0.6230
0.6235
0.6240
Loss 0 200 400 600 800"
OTHER,0.6991404011461319,Density
OTHER,0.7020057306590258,Adversarial loss
OTHER,0.7048710601719198,"0.5764
0.5766
0.5768
0.5770
0.5772
0.5774
0.5776
Loss 0 200 400 600 800 1000 1200 1400"
OTHER,0.7077363896848138,Density
OTHER,0.7106017191977078,Adversarial loss
OTHER,0.7134670487106017,"1.0465
1.0470
1.0475
1.0480
1.0485
Loss 0 100 200 300 400 500 600 700"
OTHER,0.7163323782234957,Density
OTHER,0.7191977077363897,Adversarial loss
OTHER,0.7220630372492837,"0.859
0.860
0.861
0.862
0.863
0.864
0.865
Loss 0 25 50 75 100 125 150 175 200"
OTHER,0.7249283667621776,Density
OTHER,0.7277936962750716,Adversarial loss
OTHER,0.7306590257879656,"0.5425
0.5450
0.5475
0.5500
0.5525
0.5550
0.5575
0.5600
0.5625
Loss 0 20 40 60 80 100"
OTHER,0.7335243553008596,Density
OTHER,0.7363896848137536,Adversarial loss
OTHER,0.7392550143266475,"1.146
1.147
1.148
1.149
1.150
1.151
1.152
1.153
1.154
Loss 0 50 100 150 200"
OTHER,0.7421203438395415,Density
OTHER,0.7449856733524355,Adversarial loss
OTHER,0.7478510028653295,"Figure 7: The losses of multiple perturbations on 9 different example all concentrate around a value
much larger than the clean loss (see Figure 6 for comparison with the clean loss)."
OTHER,0.7507163323782235,"B.2
JUSTIFYING OUR MODIFICATIONS"
OTHER,0.7535816618911175,"For Danskin’s Theorem Theorem 1 to hold, we require the function to be differentiable. To satisfy
differentiability, we replace ReLU with CELU (Barron, 2017) , which has been found to have
comparable performance and sometimes outperform ReLU (Dubey et al., 2022)."
OTHER,0.7564469914040115,"To operate on individual images and remove the batch-wise correlations across samples we replace
BatchNorm (BN) (Ioffe & Szegedy, 2015) with GroupNorm (GN) (Wu & He, 2018)2."
OTHER,0.7593123209169055,"Finally, to make each update depend only on the current state, we set momentum = 0.0. Since
momentum is standard practice in the CV community and works like Yan et al. (2018) argue that it
can improve generalisation, we rely on our ablation to show that removing it is safe."
OTHER,0.7621776504297995,"B.3
FURTHER DETAILS ON SYNTHETIC EXPERIMENTS"
OTHER,0.7650429799426934,"The synthetic experiment in Fig. 1a is conducted with the following settings.
The inner-
maximization is approximated with 10 steps of projected gradient ascent in order to match the tradi-
tional AT setting. The outer iterations have a decaying 0.5
√"
OTHER,0.7679083094555874,"k step-size schedule. We observe the same
erratic behavior for PGD with a ﬁxed outer stepsize, while DDi consitently remains well-behaved."
OTHER,0.7707736389684814,"The synthetic experiment in Fig. 3 is conducted on a dataset of size 100 in dimension 2 where the
coordinates are standard Gaussian. The neural network is a 2-layer network with ELU activation with
a hidden layer of width 2. The inner solver is PGD with 10 steps with stepsize 0.1 and optimizes"
OTHER,0.7736389684813754,"2 There are whole lines of work studying the effects of BN (Bjorck et al., 2018; Santurkar et al., 2018;
Kohler et al., 2019) as well as removing it altogether(Brock et al., 2021). It has also been found to interact
with adversarial robustness in Wang et al. (2022) and Benz et al. (2021), the latter also ﬁnds GN to be a well
performing alternative, justifying our choice."
OTHER,0.7765042979942693,Published as a conference paper at ICLR 2023
OTHER,0.7793696275071633,"over the unit cube. The outer step-size is 0.01 and the weights are optimized with full batch gradient
descent."
OTHER,0.7822349570200573,"The linear approximation at iteration 250 of the robust loss consits of taking the 10 adversarial
examples computed at iteration 250 and approximating it with
˜φ(θ) = max
δ1...δ10 φ(θ250) + ⟨∇θg(θ250, δi), θ −θ250⟩"
OTHER,0.7851002865329513,"Interestingly we do not observe the same drastic improvement over PGD when observing the non-
linearized loss at iteration 250."
OTHER,0.7879656160458453,"0.000
0.002
0.004
0.006
0.008
0.010
Step along normalized direction"
OTHER,0.7908309455587392,0.5762
OTHER,0.7936962750716332,0.5764
OTHER,0.7965616045845272,0.5766
OTHER,0.7994269340974212,0.5768
OTHER,0.8022922636103151,0.5770
OTHER,0.8051575931232091,0.5772
OTHER,0.8080229226361032,0.5774
OTHER,0.8108882521489972,0.5776
OTHER,0.8137535816618912,Robust Loss
OTHER,0.8166189111747851,"PGD 0
PGD 1
PGD 2
PGD 3
PGD 4
PGD 5
PGD 6
PGD 7
PGD 8
PGD 9
DDi"
OTHER,0.8194842406876791,"B.4
IMPROVING THE RUNNING TIME"
OTHER,0.8223495702005731,"0
20
40
60
80
100
120
140
Epochs 0.15 0.20 0.25 0.30 0.35 0.40 0.45"
OTHER,0.8252148997134671,Robust accuracy
OTHER,0.828080229226361,"DDi-celu-GN-nomom-norm
PGD-Theory
DDi-FAST-decay-comb
DDi-FAST-decay
DDi-PGD-decay-comb
DDi-PGD-decay
DDi-PGD-comb (a)"
OTHER,0.830945558739255,"0
20000
40000
60000
80000
100000
120000
Wallclock runtime (s) 0.15 0.20 0.25 0.30 0.35 0.40 0.45"
OTHER,0.833810888252149,Robust accuracy
OTHER,0.836676217765043,"DDi-celu-GN-nomom-norm
PGD-Theory
DDi-FAST-decay-comb
DDi-FAST-decay
DDi-PGD-decay-comb
DDi-PGD-decay
DDi-PGD-comb (b)"
OTHER,0.839541547277937,"Figure 8: (a) Epoch evolution of a more efﬁcient implementation of DDi. (b) Wallclock evolution
of the same methods."
OTHER,0.8424068767908309,"While the focus of this paper is not to obtain a state-of-the-art method, it does matter whether it is
feasible to efﬁciently capture the beneﬁt of DDi. The naive implementation has about a 10 −12
times overhead compared to PGD, mainly due to three bottlenecks (in descending impact)"
OTHER,0.8452722063037249,"1. for k-DDi, generating k adversarial examples with PGD as the base attack involves a k-
times overhead
2. then k separate gradient samples need to be computed on these adversarial examples, which
involes k forward-backward passes
3. ﬁnally, one additional optimization problem needs to be solved."
OTHER,0.8481375358166189,"While steps 1) and 2) can be somewhat parallelized, they still cause a massive increase in com-
pute and memory. We therefore adopt two heuristic approaches to speed up the algorithm while
(hopefully) maintaining it’s beneﬁts:"
OTHER,0.8510028653295129,"1. since later in training the beneﬁts of DDi appear to diminish, we linearly decay the number
of gradients sampled k from 10 down to 1 along the 200 epochs (referred to as decay)"
OTHER,0.8538681948424068,Published as a conference paper at ICLR 2023
OTHER,0.8567335243553008,"2. we also adopt a method of creating k unique batches from only 2 independent adversarial
attacks (described below in Appendix B.4.1, referred to as comb)."
OTHER,0.8595988538681948,"We evaluate this method using both PGD and FAST as base attacks and show the results in Fig. 8a
and Fig. 8b. As can be seen, DA-PGD-decay-comb and DA-PGD-comb both enjoy a massive
speedup in wallclock time (reducing the 12× overhead to about 3×) while retaining the improved
per-step progress of base DDi."
OTHER,0.8624641833810889,"B.4.1
COMBINATORIAL BATCH CONSTRUCTION"
OTHER,0.8653295128939829,"Suppose we have a batch of data-label pairs (xi, yi) of size B. In order to construct k ≤2B
different gradients by computing only 2 adversarial examples per data sample xi in the batch we do
the following:"
OTHER,0.8681948424068768,"1. for each i = 1, . . . , B compute δi,0, δi,1 two adversarial examples using the data-label pair
(xi, yi) in the batch.
2. for each j = 1, . . . , k repeat the following steps:
3. Deﬁne ∆= [ ] as an empty list.
4. generate a random bitvector b ⊆{0, 1}B of length B
5. when bi is 0 we append δi,0 to ∆, otherwise when bi is 1 we append δi,1 to ∆.
6. compute the gradient w.r.t. the network parameters using the perturbations in ∆"
OTHER,0.8710601719197708,"While this still incurs overhead of computing k gradients, it greatly reduces running time as seen
in Fig. 8b and could further improved by e.g. reusing gradients from past epochs to construct the
examples."
OTHER,0.8739255014326648,"C
PROOF OF THEOREM 2."
OTHER,0.8767908309455588,"The steepest descent direction is computed, following Eq. (4) as:"
OTHER,0.8796561604584527,"γ⋆∈arg min
γ:∥γ∥2=1
Dγφ(θ) = arg min
γ:∥γ∥2=1
max
δ∈S⋆
m(θ)⟨γ, ∇θg(θ, δ)⟩
(15)"
OTHER,0.8825214899713467,"Whenever θ is not a local optimum, there exists a non-zero descent direction. In this case we can
relax the constraint that ∥γ∥2 = 1 to ∥γ∥2 ≤1 without changing the solutions or optimal value of
(15), which is strictly negative:"
OTHER,0.8853868194842407,"min
γ:∥γ∥2=1 max
δ∈S⋆
m(θ)⟨γ, ∇θg(θ, δ)⟩=
min
γ:∥γ∥2≤1 max
δ∈S⋆
m(θ)⟨γ, ∇θg(θ, δ)⟩< 0
(16)"
OTHER,0.8882521489971347,"We can now transform (15) into a bilinear convex-concave min-max problem, subject to convex and
compact constraints:"
OTHER,0.8911174785100286,"γ⋆∈arg min
γ:∥γ∥2≤1
Dγφ(θ) = arg min
γ:∥γ∥2≤1
max
δ∈S⋆
m(θ)⟨γ, ∇θg(θ, δ)⟩"
OTHER,0.8939828080229226,"= arg min
γ:∥γ∥2≤1
max
i=1,...,m γ⊤∇θg(θ, δ(i))"
OTHER,0.8968481375358166,"= arg min
γ:∥γ∥2≤1
max
α∈∆m γ⊤∇θg(θ, S⋆
m(θ))α (17)"
OTHER,0.8997134670487106,"By Sion’s minimax Theorem Sion (1958), we can solve Eq. (17) by swapping the operator order:"
OTHER,0.9025787965616046,"min
γ:∥γ∥2≤1 max
α∈∆m γ⊤∇θg(θ, S⋆
m(θ))α = max
α∈∆m
min
γ:∥γ∥2≤1 γ⊤∇θg(θ, S⋆
m(θ))α"
OTHER,0.9054441260744985,"= max
α∈∆m −∥∇θg(θ, S⋆
m(θ))α∥2"
OTHER,0.9083094555873925,"= −min
α∈∆m ∥∇θg(θ, S⋆
m(θ))α∥2 < 0 (18)"
OTHER,0.9111747851002865,"Finally, by noting that squaring the objective function in the right-hand side of Eq. (18) does not
change the set of solutions, we arrive at the formula for α⋆in Eq. (14). Indeed for a solution α⋆to"
OTHER,0.9140401146131805,Published as a conference paper at ICLR 2023
OTHER,0.9169054441260746,this problem we have
OTHER,0.9197707736389685,"arg min
γ:∥γ∥2≤1
max
α∈∆m γ⊤∇θg(θ, S⋆
m(θ))α = arg min
γ:∥γ∥2≤1
γ⊤∇θg(θ, S⋆
m(θ))α⋆"
OTHER,0.9226361031518625,"= −∇θg(θ, S⋆
m(θ))α⋆"
OTHER,0.9255014326647565,"∥∇θg(θ, S⋆m(θ))α⋆∥ (19)"
OTHER,0.9283667621776505,where the denominator is nonnegative as the optimal objective value is nonzero c.f. Eq. (18).
OTHER,0.9312320916905444,"D
PROOF OF THEOREM 3."
OTHER,0.9340974212034384,"For any δ ∈S⋆(θ) let i(δ) ∈{1, . . . , m} be such that ∥δ(i(δ)) −δ∥2 ≤ϵ. That is, we map any
maximizer δ to an index i ∈{1, . . . , m} such that the corresponding perturbation δ(i) in the ﬁnite
set S⋆
m(θ) is at most at an ϵ distance. This map can be constructed by the assumption on S⋆
m(θ)."
OTHER,0.9369627507163324,For any γ such that ∥γ∥2 = 1 we have
OTHER,0.9398280802292264,"⟨γ, ∇θg(θ, δ)⟩= ⟨γ, ∇θg(θ, δ) −∇θg(θ, δ(i(δ))⟩+ ⟨γ, ∇θg(θ, δ(i(δ)))⟩"
OTHER,0.9426934097421203,"≤∥γ∥2
|{z}
=1"
OTHER,0.9455587392550143,"∥∇θg(θ, δ) −∇θg(θ, δ(i(δ)))∥
|
{z
}
≤L∥δ−δ(i(δ))∥≤Lϵ"
OTHER,0.9484240687679083,"+⟨γ, ∇θg(θ, δ(i(δ)))⟩"
OTHER,0.9512893982808023,"≤⟨γ, ∇θg(θ, δ(i(δ)))⟩+ Lϵ"
OTHER,0.9541547277936963,"≤
sup
δ∈S⋆
m(θ)
⟨γ, ∇θg(θ, δ(i))⟩+ Lϵ (20)"
OTHER,0.9570200573065902,Taking the supremum over δ ∈S⋆(θ) on the left-hand-side we obtain
OTHER,0.9598853868194842,"Dγφ(θ) :=
sup
δ∈S⋆(θ)
⟨γ, ∇θg(θ, δ)⟩≤
sup
δ∈S⋆
m(θ)
⟨γ, ∇θg(θ, δ(i))⟩+ Lϵ
(21)"
OTHER,0.9627507163323782,"Hence if the supremum on the right-hand-side is strictly smaller than −Lϵ we have that Dγφ(θ) < 0,
which yields the desired result."
OTHER,0.9656160458452722,"E
PROOF OF LEMMA 1"
OTHER,0.9684813753581661,Assume the limit that deﬁnes ˆDγφ(θ) exists (and is ﬁnite).
OTHER,0.9713467048710601,"ˆD−γφ(θ) = lim
t→0
φ(θ + t(−γ)) −φ(θ)"
OTHER,0.9742120343839542,t∥−γ∥2
OTHER,0.9770773638968482,"= lim
t→0
φ(θ + (−t)γ) −φ(θ)"
OTHER,0.9799426934097422,−(−t)∥γ∥2
OTHER,0.9828080229226361,"=
lim
(−t)→0
φ(θ + (−t)γ) −φ(θ)"
OTHER,0.9856733524355301,−(−t)∥γ∥2
OTHER,0.9885386819484241,"= lim
s→0 −φ(θ + sγ) −φ(θ)"
OTHER,0.9914040114613181,"s∥γ∥2
(let s = (−t))"
OTHER,0.994269340974212,"= −lim
s→0
φ(θ + sγ) −φ(θ)"
OTHER,0.997134670487106,"s∥γ∥2
= −ˆDγφ(θ) (22)"
