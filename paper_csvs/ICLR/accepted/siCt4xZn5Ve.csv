Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007911392405063291,"Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the
key challenges in deep learning, especially for overparametrized models, where
the local minimizers of the loss function L can form a manifold. Intuitively, with a
sufﬁciently small learning rate η, SGD tracks Gradient Descent (GD) until it gets
close to such manifold, where the gradient noise prevents further convergence. In
such regime, Blanc et al. (2020) proved that SGD with label noise locally decreases
a regularizer-like term, the sharpness of loss, tr[∇2L]. The current paper gives a
general framework for such analysis by adapting ideas from Katzenberger (1991). It
allows in principle a complete characterization for the regularization effect of SGD
around such manifold—i.e., the ”implicit bias”—using a stochastic differential
equation (SDE) describing the limiting dynamics of the parameters, which is
determined jointly by the loss function and the noise covariance. This yields some
new results: (1) a global analysis of the implicit bias valid for η−2 steps, in contrast
to the local analysis of Blanc et al. (2020) that is only valid for η−1.6 steps and (2)
allowing arbitrary noise covariance. As an application, we show with arbitrary
large initialization, label noise SGD can always escape the kernel regime and only
requires O(κ ln d) samples for learning an κ-sparse overparametrized linear model
in Rd (Woodworth et al., 2020), while GD initialized in the kernel regime requires
Ω(d) samples. This upper bound is minimax optimal and improves the previous
eO(κ2) upper bound (HaoChen et al., 2020)."
INTRODUCTION,0.0015822784810126582,"1
INTRODUCTION"
INTRODUCTION,0.0023734177215189874,"The implicit bias underlies the generalization ability of machine learning models trained by stochastic
gradient descent (SGD). But it still remains a mystery to mathematically characterize such bias. We
study SGD in the following formulation
xη(k + 1) = xη(k) −η(∇L(xη(k)) +
√"
INTRODUCTION,0.0031645569620253164,"Ξ · σξk(xη(k)))
(1)"
INTRODUCTION,0.003955696202531646,"where η is the learning rate (LR), L
:
RD
→
R is the training loss and σ(x)
=
[σ1(x), σ2(x), . . . , σΞ(x)] ∈RD×Ξ is a deterministic noise function. Here ξk is sampled uniformly
from {1, 2, . . . , Ξ} and it satisﬁes Eξk[σξk(x)] = 0, ∀x ∈Rd and k."
INTRODUCTION,0.004746835443037975,"It is widely believed that large LR (or equivalently, small batch size) helps SGD ﬁnd better minima.
For instance, some previous works argued that large noise enables SGD to select a ﬂatter attraction
basin of the loss landscape which potentially beneﬁts generalization (Li et al., 2019c; Jastrzebski et al.,
2017). However, there is also experimental evidence (Li et al., 2020b) that small LR also has equally
good implicit bias (albeit with higher training time), and that is the case studied here. Presumably
low LR precludes SGD jumping between different basins since under general conditions this should
require Ω(exp(1/η)) steps (Shi et al., 2020). In other words, there should be a mechanism to reach
better generalization while staying within a single basin. For deterministic GD similar mechanisms"
INTRODUCTION,0.005537974683544304,Published as a conference paper at ICLR 2022
INTRODUCTION,0.006329113924050633,"(a) Taylor Expansion of ∇L
(b) Normal Space Dynamics
(c) Tangent Space Dynamics"
INTRODUCTION,0.007120253164556962,Figure 1: Illustration for limiting ﬂow in R2. Γ is an 1D manifold of minimizers of loss L.
INTRODUCTION,0.007911392405063292,"have been demonstrated in simple cases (Soudry et al., 2018; Lyu & Li, 2019) and referred to as
implicit bias of gradient descent. The current paper can be seen as study of implicit bias of Stochastic
GD, which turns out to be quite different, mathematically."
INTRODUCTION,0.00870253164556962,"Recent work (Blanc et al., 2020) shed light on this direction by analyzing effects of stochasticity in
the gradient. For sufﬁciently small LR, SGD will reach and be trapped around some manifold of local
minimizers, denoted by Γ (see Figure 2). The effect is shown to be an implicit deterministic drift in a
direction corresponding to lowering a regularizer-like term along the manifold. They showed SGD
with label noise locally decreases the sharpness of loss, tr[∇2L], by Θ(η0.4) in η−1.6 steps. However,
such an analysis is actually local, since the natural time scale of analysis should be η−2, not η−1.6."
INTRODUCTION,0.00949367088607595,"The contribution of the current paper is a more general and global analysis of this type. We introduce
a more powerful framework inspired by the classic paper (Katzenberger, 1991)."
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.010284810126582278,"1.1
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.011075949367088608,"We start with an intuitive description of the implicit regularization effect described in Blanc et al.
(2020). For simpliﬁcation, we show it for the canonical SDE approximation (See Section B.1 for more
details) of SGD (1) (Li et al., 2017; Cheng et al., 2020). Here W(t) is the standard Ξ-dimensional
Brownian motion. The only property about label noise SGD we will use is that the noise covariance
σσ⊤(x) = ∇2L(x) for every x in the manifold Γ (See derivation in Section 5)."
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.011867088607594937,"d ˜Xη(t) = −η∇L( ˜Xη(t))dt + η · σ( ˜Xη(t))dW(t).
(2)"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.012658227848101266,"Suppose ˜Xη(0) is already close to some local minimizer point X∗∈Γ. The goal is to show ˜Xη(t)
will move in the tangent space and steadily decrease tr[∇2L]. At ﬁrst glance, this seems impossible
as the gradient ∇L vanishes around Γ, and the noise has zero mean, implying SGD should be like
random walk instead of a deterministic drift. The key observation of Blanc et al. (2020) is that the
local dynamics of ˜Xη(t) is completely different in tangent space and normal space — the fast random
walk in normal space causes ˜Xη(t) to move slowly (with velocity Θ(η2)) but deterministically
in certain direction. To explain this, letting ∆(t) = ˜Xη(t) −X∗, Taylor expansion of (2) gives
d∆(t) ≈−η∇2L(X∗)∆dt + ησ(X∗)dW(t), meaning ∆is behaving like an Ornstein-Uhlenbeck
(OU) process locally in the normal space. Its mixing time is Θ(η−1) and the stationary distribution is
the standard multivariate gaussian in the normal space scaled by √η (see Figure 1b), because noise
covariance σσ⊤= ∇2L. Though this OU process itself doesn’t form any regularization, it activates
the second order Taylor expansion of ∇L(X∗+ ∆(t)), i.e., −1"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.013449367088607595,"2∂2(∇L)(X∗)[∆(t), ∆(t)], creating
a Θ(η2) velocity in the tangent space. Since there is no push back force in the tangent space, the
small velocity accumulates over time, and in a longer time scale of Ω(η−1), the time average of the
stochastic velocity is roughly the same as the expected velocity when ∆is sampled from its stationary
distribution. This simpliﬁes the expression of the velocity in tangent space to η2"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.014240506329113924,"2 ∇T tr[∇2L] (see
Figure 1c), where ∇T means the gradient is only taken in the tangent space."
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.015031645569620253,"However, the above approach only gives a local analysis for O(η−1.6) time, where the total movement
due to implicit regularization is O(η2−1.6) = O(η0.4) and thus is negligible when η →0. In order
to get a non-trivial limiting dynamics when η →0, a global analysis for Ω(η−2) steps is necessary
and it cannot be done by Taylor expansion with a single reference point. Recent work by Damian
et al. (2021) glues analyses of multiple local phases into a global guarantee that SGD ﬁnds a (ϵ, γ)-
stationary point for the regularized loss, but still doesn’t show convergence for trajectory when η →0
and cannot deal with general noise types, e.g., noise lying in the tangent space of the manifold. The"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.015822784810126583,Published as a conference paper at ICLR 2022
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.01661392405063291,"main technical difﬁculty here is that it’s not clear how to separate the slow and fast dynamics in
different spaces and how to only take limit for the slow dynamics, especially when shifting to a new
reference point in the Taylor series calculation."
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.01740506329113924,"1.2
OUR APPROACH: SEPARATING THE SLOW FROM THE FAST"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.01819620253164557,"In this work, we tackle this problem via a different angle. First, since the anticipated limiting
dynamics is of speed Θ(η2), we change the time scaling to accelerate (2) by η−2 times, which yields"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.0189873417721519,"dXη(t) = −η−1∇L(Xη(t))dt + σ(Xη(t))dW(t).
(3)"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.01977848101265823,"The key idea here is that we only need to track the slow dynamic, or equivalently, some projection of
X onto the manifold Γ, Φ(X). Here Φ : RD →Γ is some function to be speciﬁed and hopefully we
can simplify the dynamics (3) via choosing suitable Φ. To track the dynamics of Φ(Xη), we apply
Ito’s lemma (a.k.a. stochastic chain rule, see Lemma A.9) to Equation (3), which yields"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.020569620253164556,dΦ(Xη(t)) = −η−1∂Φ(Xη(t))∇L(Xη(t))dt + ∂Φ(Xη(t))σ(Xη(t))dW(t) + 1 2 XD
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.021360759493670885,"i,j=1 ∂ijΦ(Xη(t))(σ(Xη(t))σ(Xη(t))⊤)ijdt."
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.022151898734177215,"Note the ﬁrst term −η−1∂Φ(Xη)∇L(Xη) is going to diverge to ∞when η →0, so a natural choice
for Φ is to kill the ﬁrst term. Further note −∂Φ(X)∇L(X) is indeed the directional derivative of Φ
at X towards −∇L, killing the ﬁrst term becomes equivalent to making Φ invariant under Gradient
Flow (GF) of −∇L(X)! Thus it sufﬁces to take Φ(X) to be the limit of GF starting at X. (Formally
deﬁned in Section 3; see Lemma C.2 for a proof of ∂Φ(X)∇L(X) ≡0.)"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.022943037974683545,"Also intuitively Xη will be inﬁnitely close to Γ, i.e., d(Xη(t), Γ) →0 for any t > 0 as η →0, so we
have Φ(Xη) ≈Xη. Thus we can rewrite the above equation as"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.023734177215189875,dXη(t) ≈∂Φ(Xη(t))σ(Xη(t))dW(t) + 1 2 XD
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.0245253164556962,"i,j=1 ∂ijΦ(Xη(t))(σ(Xη(t))σ(Xη(t))⊤)ijdt, (4)"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.02531645569620253,and the solution of (4) shall converge to that of the following (in an intuitive sense):
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.02610759493670886,dX(t) = ∂Φ(X(t))σ(X(t))dW(t) + 1 2 XD
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.02689873417721519,"i,j=1 ∂ijΦ(X(t))(σ(X(t))σ(X(t))⊤)ijdt,
(5)"
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.027689873417721517,"The above argument for SDE was ﬁrst formalized and rigorously proved by Katzenberger (1991). It
included an extension of the analysis to the case of asymptotic continuous dynamics (Theorem 4.1)
including SGD with inﬁnitesimal LR, but the result is weaker in this case and no convergence is
shown. Another obstacle for applying this analysis is that 2nd order partial derivatives of Φ are
unknown. We solve these issues in Section 4 and our main result Theorem 4.6 gives a clean and
complete characterization for the implicit bias of SGD with inﬁnitesimal LR in Θ(η−2) steps. Finally,
our Corollary 5.2 shows (5) gives exactly the same regularization as tr[∇2L] for label noise SGD."
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.028481012658227847,The main contributions of this paper are summarized as follows.
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.029272151898734177,"1. In Section 4, we propose a mathematical framework to study the implicit bias of SGD with
inﬁnitesimal LR. Our main theorem (Theorem 4.6) gives the limiting diffusion of SGD with LR η
for Θ(η−2) steps as η →0 and allows any covariance structure.
2. In Section 5, we give limiting dynamics of SGD with isotropic noise and label noise.
3. In Section 6, we show for any initialization, SGD with label noise achieves O(κ ln d) sample
complexity for learning a κ-sparse overparametrized linear model (Woodworth et al., 2020). In
this case, the implicit regularizer is a data-dependent weighted ℓ1 regularizer, meaning noise
can help reduce the norm and even escape the kernel regime. The O(κ ln d) rate is minimax
optimal (Raskutti et al., 2012) and improves over ˜O(κ2) upper bound by HaoChen et al. (2020).
In contrast, vanilla GD requires Ω(d) samples to generalize in the kernel regime.
For technical contributions, we rigorously prove the convergence of GF for OLM (Lemma 6.3),
unlike many existing implicit bias analyses which have to assume the convergence. We also prove
the convergence of limiting ﬂow to the global minimizer of the regularizer (Lemma 6.5) by a
trajectory analysis via our framework. It cannot be proved by previous results (Blanc et al., 2020;
Damian et al., 2021), as they only assert convergence to stationary point in the best case."
INTUITIVE EXPLANATION OF REGULARIZATION EFFECT DUE TO SGD,0.030063291139240507,Published as a conference paper at ICLR 2022
RELATED WORKS,0.030854430379746837,"2
RELATED WORKS"
RELATED WORKS,0.03164556962025317,"Loss Landscape of Overparametrized Models A phenomenon known as mode connectivity has
been observed that local minimizers of the loss function of a neural network are connected by
simple paths (Freeman & Bruna, 2016; Garipov et al., 2018; Draxler et al., 2018), especially for
overparametrized models (Venturi et al., 2018; Liang et al., 2018; Nguyen et al., 2018; Nguyen, 2019).
Later this phenomanon is explained under generic assumptions by Kuditipudi et al. (2019). Moreover,
it has been proved that the local minimizers of an overparametrized network form a low-dimensional
manifold (Cooper, 2018; 2020) which possibly has many components. Fehrman et al. (2020) proved
the convergence rate of SGD to the manifold of local minimizers starting in a small neighborhood."
RELATED WORKS,0.03243670886075949,"Implicit Bias in Overparametrized Models Algorithmic regularization has received great attention
in the community (Arora et al., 2018; 2019a; Gunasekar et al., 2018b;a;b; Soudry et al., 2018; Li
et al., 2018; 2020a). In particular, the SGD noise is widely believed to be a promising candidate for
explaining the generalization ability of modern neural networks (LeCun et al., 2012; Keskar et al.,
2016; Hoffer et al., 2017; Zhu et al., 2018; Li et al., 2019a). Beyond the size of noise (Li et al.,
2019c; Jastrzebski et al., 2017), the shape and class of the noise also play an important role (Wen
et al., 2019; Wu et al., 2020). It is shown by HaoChen et al. (2020) that parameter-dependent
noise will bias SGD towards a low-complexity local minimizer. Similar implicit bias has also been
studied for overparametrized nonlinear statistical models by Fan et al. (2020). Several existing
works (Vaskevicius et al., 2019; Woodworth et al., 2020; Zhao et al., 2019) have shown that for the
quadratically overparametrized linear model, i.e., w = u⊙2−v⊙2 or w = u⊙v, gradient descent/ﬂow
from small initialization implicitly regularizes ℓ1 norm and provides better generalization when the
groundtruth is sparse. This is in sharp contrast to the kernel regime, where neural networks trained by
gradient descent behaves like kernel methods (Daniely, 2017; Jacot et al., 2018; Yang, 2019). This
allows one to prove convergence to zero loss solutions in overparametrized settings (Li & Liang,
2018; Du et al., 2018; Allen-Zhu et al., 2019b;a; Du et al., 2019; Zou et al., 2020), where the learnt
function minimizes the corresponding RKHS norm (Arora et al., 2019b; Chizat et al., 2018)."
RELATED WORKS,0.03322784810126582,"Modelling Stochastic First-Order Methods with Itˆo SDE Apart from the discrete-time analysis,
another popular approach to study SGD is through the continuous-time lens using SDE (Li et al.,
2017; 2019b; Cheng et al., 2020). Such an approach is often more elegant and can provide fruitful
insights like the linear scaling rule (Krizhevsky, 2014; Goyal et al., 2017) and the intrinsic learning
rate (Li et al., 2020b). A recent work by Li et al. (2021) justiﬁes such SDE approximation. Xie et al.
(2020) gave a heuristic derivation explaining why SGD favors ﬂat minima with SDE approximation.
Wojtowytsch (2021) showed that the invariant distribution of the canonical SDE approximation
of SGD will collapse to some manifold of minimizers and in particular, favors ﬂat minima. By
approximating SGD using a SDE with slightly modiﬁed covariance for the overparametrized linear
model, Pesme et al. (2021) relates the strength of implicit regularization to training speed."
NOTATION AND PRELIMINARIES,0.03401898734177215,"3
NOTATION AND PRELIMINARIES"
NOTATION AND PRELIMINARIES,0.03481012658227848,"Given loss L, the GF governed by L can be described through a mapping φ : RD × [0, ∞) →RD"
NOTATION AND PRELIMINARIES,0.03560126582278481,"satisfying φ(x, t) = x −
R t
0 ∇L(φ(x, s))ds. We further denote the limiting mapping Φ(x) =
limt→∞φ(x, t) whenever the limit exists. We denote 1ξ ∈RΞ as the one-hot vector where ξ-th
coordinate is 1, and 1 the all 1 vector. See Appendix A for a complete clariﬁcation of notations."
MANIFOLD OF LOCAL MINIMIZERS,0.03639240506329114,"3.1
MANIFOLD OF LOCAL MINIMIZERS"
MANIFOLD OF LOCAL MINIMIZERS,0.037183544303797465,"Assumption 3.1. Assume that the loss L : RD →R is a C3 function, and that Γ is a (D −M)-
dimensional C2-submanifold of RD for some integer 0 ≤M ≤D, where for all x ∈Γ, x is a local
minimizer of L and rank(∇2L(x)) = M."
MANIFOLD OF LOCAL MINIMIZERS,0.0379746835443038,"Assumption 3.2. Assume that U is an open neighborhood of Γ satisfying that gradient ﬂow starting
in U converges to some point in Γ, i.e., ∀x ∈U, Φ(x) ∈Γ. (Then Φ is C2 on U by Falconer (1983).)"
MANIFOLD OF LOCAL MINIMIZERS,0.038765822784810125,"When does such a manifold exist? The vast overparametrization in modern deep learning is a major
reason for the set of global minimizers to appear as a Riemannian manifold (possibly with multiple
connected components), instead of isolated ones. Suppose all global minimizers interpolate the"
MANIFOLD OF LOCAL MINIMIZERS,0.03955696202531646,Published as a conference paper at ICLR 2022
MANIFOLD OF LOCAL MINIMIZERS,0.040348101265822785,"training dataset, i.e., ∀x ∈RD, L(x) = minx′∈RD L(x′) implies fi(x) = yi for all i ∈[n], then by
preimage theorem (Banyaga & Hurtubise, 2013), the manifold Γ := {x ∈RD | fi(x) = yi, ∀i ∈[n]}
is of dimension D −n if the Jacobian matrix [∇f1(x), . . . , ∇fn(x)] has rank n for all x ∈Γ. Note
this condition is equivalent to that NTK at x has full rank, which is very common in literature."
LIMITING DIFFUSION OF SGD,0.04113924050632911,"4
LIMITING DIFFUSION OF SGD"
LIMITING DIFFUSION OF SGD,0.041930379746835444,"In Section 4.1 we ﬁrst recap the main result of Katzenberger (1991). In Section 4.2 we derive the
closed-form expressions of ∂Φ and ∂2Φ. We present our main result in Section 4.3. We remark that
sometimes we omit the dependency on t to make things clearer."
LIMITING DIFFUSION OF SGD,0.04272151898734177,"4.1
RECAP OF KATZENBERGER’S THEOREM"
LIMITING DIFFUSION OF SGD,0.043512658227848104,"Let {An}n≥1 be a sequence of integrators, where each An : R →R is a non-decreasing function
with An(0) = 0. Let {Zn}n≥1 be a sequence of R|Ξ|-valued stochastic processes deﬁned on R.
Given loss function L and noise covariance function σ, we consider the following stochastic process:"
LIMITING DIFFUSION OF SGD,0.04430379746835443,"Xn(t) = X(0) +
Z t"
LIMITING DIFFUSION OF SGD,0.04509493670886076,"0
σ(Xn(s)dZn(s) +
Z t"
LIMITING DIFFUSION OF SGD,0.04588607594936709,"0
−∇L(Xn(s))dAn(s)
(6)"
LIMITING DIFFUSION OF SGD,0.04667721518987342,"In particular, when the integrator sequence {An}n≥1 increases inﬁnitely fast, meaning that ∀ϵ >
0, inft≥0(An(t + ϵ) −An(t)) →∞as n →∞, we call (6) a Katzenberger process."
LIMITING DIFFUSION OF SGD,0.04746835443037975,"One difﬁculty for directly studying the limiting dynamics of Xn(t) is that the point-wise limit as
n →∞become discontinuous at t = 0 if X(0) /∈Γ. The reason is that clearly limn→∞Xn(0) =
X(0), but for any t > 0, since {An}n≥1 increases inﬁnitely fast, one can prove limn→∞Xn(t) ∈Γ!
To circumvent this issue, we consider Yn(t) = Xn(t) −φ(X(0), An(t)) + Φ(X(0)). Then for each
n ≥1, we have Yn(0) = Φ(X(0)) and limn→∞Yn(t) = limn→∞Xn(t). Thus Yn(t) has the same
limit on (0, ∞) as Xn(t), but the limit of the former is further continuous at t = 0."
LIMITING DIFFUSION OF SGD,0.048259493670886076,"Theorem 4.1 (Informal version of Theorem B.7, Katzenberger 1991). Suppose the loss L, manifold Γ
and neighborhood U satisﬁes Assumptions 3.1 and 3.2. Let {Xn}n≥1 be a sequence of Katzenberger
process with {An}n≥1, {Zn}n≥1. Let Yn(t) = Xn(t) −φ(X(0), An(t)) + Φ(X0). Under technical
assumptions, it holds that if (Yn, Zn) converges to some (Y, W) in distribution, where {W(t)}t≥0 is
the standard Brownian motion, then Y stays on Γ and admits"
LIMITING DIFFUSION OF SGD,0.0490506329113924,"Y (t) = Y (0) +
Z t"
LIMITING DIFFUSION OF SGD,0.049841772151898736,"0
∂Φ(Y )σ(Y )dW(s) + 1 2 XD i,j=1 Z t"
LIMITING DIFFUSION OF SGD,0.05063291139240506,"0
∂ijΦ(Y )(σ(Y )σ(Y )⊤)ijds.
(7)"
LIMITING DIFFUSION OF SGD,0.051424050632911396,"Indeed, SGD (1) can be rewritten into a Katzenberger process as in the following lemma."
LIMITING DIFFUSION OF SGD,0.05221518987341772,"Lemma 4.2. Let {ηn}∞
n=1 be any positive sequence with limn→∞ηn = 0, An(t) = ηn⌊t/η2
n⌋,"
LIMITING DIFFUSION OF SGD,0.05300632911392405,"and Zn(t) = ηn
P⌊t/η2
n⌋
k=1
√"
LIMITING DIFFUSION OF SGD,0.05379746835443038,Ξ(1ξk −1
LIMITING DIFFUSION OF SGD,0.05458860759493671,"Ξ1), where ξ1, ξ2, . . .
i.i.d.
∼Unif([Ξ]). Then with the same
initialization Xn(0) = xηn(0) ≡X(0), Xn(kη2
n) deﬁned by (6) is a Katzenberger process and is
equal to xηn(k) deﬁned in (1) with LR equal to ηn for all k ≥1. Moreover, the counterpart of (7) is"
LIMITING DIFFUSION OF SGD,0.055379746835443035,"Y (t) = Φ(X(0)) +
Z t"
LIMITING DIFFUSION OF SGD,0.05617088607594937,"0
∂Φ(Y )σ(Y )dW(s) + 1 2 Z t"
LIMITING DIFFUSION OF SGD,0.056962025316455694,"0
∂2Φ(Y )[Σ(Y )]ds,
(8)"
LIMITING DIFFUSION OF SGD,0.05775316455696203,where Σ ≡σσ⊤and {W(t)}t≥0 is a Ξ-dimensional standard Brownian motion.
LIMITING DIFFUSION OF SGD,0.058544303797468354,"However, there are two obstacles preventing us from directly applying Theorem 4.1 to SGD. First,
the stochastic integral in (8) depends on the derivatives of Φ, ∂Φ and ∂ijΦ, but Katzenberger (1991)
did not give their dependency on loss L. To resolve this, we explicitly calculate the derivatives of Φ
on Γ in terms of the derivatives of L in Section 4.2."
LIMITING DIFFUSION OF SGD,0.05933544303797468,"The second difﬁculty comes from the convergence of (Yn, Zn) which we assume as granted for
brevity in Theorem 4.1. In fact, the full version of Theorem 4.1 (see Theorem B.7) concerns the
stopped version of Yn with respect to some compact K ⊂U, i.e., Y µn(K)
n
(t) = Yn(t ∧µn(K))
where µn(K) is the stopping time of Yn leaving K. As noted in Katzenberger (1991), we need the"
LIMITING DIFFUSION OF SGD,0.060126582278481014,Published as a conference paper at ICLR 2022
LIMITING DIFFUSION OF SGD,0.06091772151898734,"convergence of µn(K) for Y µn(K)
n
to converge, which is a strong condition and difﬁcult to prove in
our cases. We circumvent this issue by proving Theorem B.9, a user-friendly interface for the original
theorem in Katzenberger (1991), and it only requires the information about the limiting diffusion.
Building upon these, we present our ﬁnal result as Theorem 4.6."
CLOSED-FORM EXPRESSION OF THE LIMITING DIFFUSION,0.061708860759493674,"4.2
CLOSED-FORM EXPRESSION OF THE LIMITING DIFFUSION"
CLOSED-FORM EXPRESSION OF THE LIMITING DIFFUSION,0.0625,"We can calculate the derivatives of Φ by relating to those of L. Here the key observation is the
invariance of Φ along the trajectory of GF.
Lemma 4.3. For any x ∈Γ, ∂Φ(x) ∈RD×D is the projection matrix onto tangent space Tx(Γ)."
CLOSED-FORM EXPRESSION OF THE LIMITING DIFFUSION,0.06329113924050633,"To express the second-order derivatives compactly, we introduce the notion of Lyapunov operator.
Deﬁnition 4.4 (Lyapunov Operator). For a symmetric matrix H, we deﬁne WH = {Σ ∈RD×D |
Σ = Σ⊤, HH†Σ = Σ = ΣHH†} and Lyapunov Operator LH : WH →WH as LH(Σ) =
H⊤Σ + ΣH. It’s easy to verify L−1
H is well-deﬁned on WH."
CLOSED-FORM EXPRESSION OF THE LIMITING DIFFUSION,0.06408227848101265,"Lemma 4.5. Let x be any point in Γ and Σ = Σ(x) = σσ⊤(x) ∈RD×D be the noise covariance
at x1. Then Σ can be decomposed as Σ = Σ∥+ Σ⊥+ Σ∥,⊥+ Σ⊥,∥, where Σ∥:= ∂ΦΣ∂Φ,
Σ⊥:= (ID −∂Φ)Σ(ID −∂Φ) and Σ∥,⊥= Σ⊤
⊥,∥= ∂ΦΣ(ID −∂Φ) are the noise covariance in
tangent space, normal space and across both spaces, respectively. Then it holds that"
CLOSED-FORM EXPRESSION OF THE LIMITING DIFFUSION,0.06487341772151899,"∂2Φ[Σ] =(∇2L)†∂2(∇L)

Σ∥

−∂Φ∂2(∇L)

L−1
∇2L(Σ⊥)

+2∂Φ∂2(∇L)

(∇2L)†Σ⊥,∥

.
(9)"
MAIN RESULT,0.06566455696202532,"4.3
MAIN RESULT"
MAIN RESULT,0.06645569620253164,"Now we are ready to present our main result. It’s a direct combination of Theorem B.9 and Lemma 4.5.
Theorem 4.6. Suppose the loss function L, the manifold of local minimizer Γ and the open neighbor-
hood U satisfy Assumptions 3.1 and 3.2, and xη(0) = x(0) ∈U for all η > 0. If SDE (10) has a
global solution Y with Y (0) = x(0) and Y never leaves U, i.e., P[Y (t) ∈U, ∀t ≥0] = 1, then for
any T > 0, xη(⌊T/η2⌋) converges in distribution to Y (T) as η →0."
MAIN RESULT,0.06724683544303797,dY (t) = Σ
MAIN RESULT,0.0680379746835443,"1
2
∥(Y )dW(t)
|
{z
}
Tangent Noise + 1"
MAIN RESULT,0.06882911392405064,"2∇2L(Y )†∂2(∇L)(Y )

Σ∥(Y )

dt
|
{z
}
Tangent Noise Compensation + 1"
MAIN RESULT,0.06962025316455696,"2∂Φ(Y )

∂2(∇L)(Y )

∇2L(Y )†Σ⊥,∥(Y )
"
MAIN RESULT,0.07041139240506329,"|
{z
}
Mixed Regularization"
MAIN RESULT,0.07120253164556962,"−∂2(∇L)(Y )

L−1
∇2L(Σ⊥(Y ))
"
MAIN RESULT,0.07199367088607594,"|
{z
}
Normal Regularization 
dt, (10)"
MAIN RESULT,0.07278481012658228,"where Σ ≡σσ⊤and Σ∥, Σ⊥, Σ⊥,∥are deﬁned in Lemma 4.5."
MAIN RESULT,0.07357594936708861,"Based on the above theorem, the limiting dynamics of SGD can be understood as follows: (a) the
tangent noise, Σ1/2
∥
(Y )dW(t), is preserved, and the second term of (10) can be viewed as the
necessary tangent noise compensation for the limiting dynamics to stay on Γ. Indeed, Lemma C.7
shows that the value of the second term only depends on Γ itself, i.e., it’s same for all loss L which
locally deﬁnes the same Γ. (b) The noise in the normal space is killed since the limiting dynamics
always stay on Γ. However, its second order effect (Itˆo correction term) takes place as a vector ﬁeld
on Γ, which induces the Noise Regularization and Mixed Regularization term, corresponding to
the mixed and normal noise covariance respectively.
Remark 4.7. In Appendix B.4 we indeed prove a stronger version of Theorem 4.6 that the sample
paths of SGD converge in distribution, i.e., let ˜xη(t) = xη(⌊t/η2⌋), then ˜xη weakly converges to
Y on [0, T]. Moreover, we only assume the existence of a global solution for ease of presentation.
As long as there exists a compact K ⊆Γ such that Y stays in K on [0, T] with high probability,
Theorem B.9 still provides the convergence of SGD iterates (stopped at the boundary of K) before
time T with high probability."
IMPLICATIONS AND EXAMPLES,0.07436708860759493,"5
IMPLICATIONS AND EXAMPLES"
IMPLICATIONS AND EXAMPLES,0.07515822784810126,"In this section, we derive the limiting dynamics for two notable noise types, where we ﬁx the expected
loss L and the noise distribution, and only drive η to 0. The proofs are deferred into Appendix C.3."
IMPLICATIONS AND EXAMPLES,0.0759493670886076,"1For notational convenience, we drop dependency on x."
IMPLICATIONS AND EXAMPLES,0.07674050632911393,Published as a conference paper at ICLR 2022
IMPLICATIONS AND EXAMPLES,0.07753164556962025,"Type I: Isotropic Noise. Isotropic noise means Σ(x) ≡ID for any x ∈Γ (Shi et al., 2020). The
following theorem shows that the limiting diffusion with isotropic noise can be viewed as a Brownian
Motion plus Riemannian Gradient Flow with respect to the pseudo-determinant of ∇2L.
Corollary 5.1 (Limiting Diffusion for Isotropic Noise). If Σ ≡ID on Γ, SDE (10) is then"
IMPLICATIONS AND EXAMPLES,0.07832278481012658,dY (t) = ∂Φ(Y )dW + 1
IMPLICATIONS AND EXAMPLES,0.07911392405063292,"2∇2L(Y )†∂2(∇L)(Y ) [∂Φ(Y )] dt
|
{z
}
Brownian Motion on Manifold −1"
IMPLICATIONS AND EXAMPLES,0.07990506329113924,"2∂Φ(Y )∇(ln |∇2L(Y )|+)dt
|
{z
}
Normal Regularization (11)"
IMPLICATIONS AND EXAMPLES,0.08069620253164557,"where |∇2L(Y )|+ = limα→0
|∇2L(Y )+αID|
αD−rank(∇2L(Y )) is the pseudo-determinant of ∇2L(Y ). |∇2L(Y )|+ is
also equal to the sum of log of non-zero eigenvalue values of ∇2L(Y )."
IMPLICATIONS AND EXAMPLES,0.0814873417721519,"Type II: Label Noise. When doing SGD for ℓ2-regression on dataset {(zi, yi)}n
i=1, adding label
noise (Blanc et al., 2020; Damian et al., 2021) means replacing the true label at iteration k, yik,
by a fresh noisy label ˜yik := yik + δk, where δk
i.i.d.
∼Unif{−δ, δ} for some constant δ > 0. Then
the corresponding loss becomes 1"
IMPLICATIONS AND EXAMPLES,0.08227848101265822,"2(fik(x) −˜yik)2, where fik(x) is the output of the model with
parameter x on data zik. So the label noise SGD update is"
IMPLICATIONS AND EXAMPLES,0.08306962025316456,"xk+1 = xk −η/2 · ∇x (fik(xk) −yik + δik)2 = xk −η(fik(xk) −yik + δk)∇xfik(xk).
(12)"
IMPLICATIONS AND EXAMPLES,0.08386075949367089,Suppose the model can achieve the global minimum of the loss L(x) := 1
IMPLICATIONS AND EXAMPLES,0.08465189873417721,"2E[(fi(x) −˜yi)2] at x∗,
then the model must interpolate the whole dataset, i.e., fi(x∗) = yi for all i ∈[n], and thus here the
manifold Γ is a subset of {x ∈RD | fi(x) = yi, ∀i ∈[n]}. Here the key property of the label noise
used in previous works is Σ(x) = δ2"
IMPLICATIONS AND EXAMPLES,0.08544303797468354,"n
Pn
i=1 ∇xfi(x)∇xfi(x)⊤= δ2∇2L(x). Lately, Damian et al.
(2021) further generalizes the analysis to other losses, e.g., logistic loss and exponential loss, as long
as they satisfy Σ(x) = c∇2L(x) for some constant c > 0."
IMPLICATIONS AND EXAMPLES,0.08623417721518987,"In sharp contrast to the delicate discrete-time analysis in Blanc et al. (2020) and Damian et al. (2021),
the following corollary recovers the same result but with much simpler analysis – taking derivatives
is all you need. Under our framework, we no longer need to do Taylor expansion manually nor
carefully control the inﬁnitesimal variables of different orders together. It is also worth mentioning
that our framework immediately gives a global analysis of Θ(η−2) steps for SGD, far beyond the
local coupling analysis in previous works. In Section 6, we will see how such global analysis allows
us to prove a concrete generalization upper bound in a non-convex problem, the overparametrized
linear model (Woodworth et al., 2020; HaoChen et al., 2020).
Corollary 5.2 (Limiting Flow for Label Noise). If Σ ≡c∇2L on Γ for some constant c > 0,
SDE (10) can be simpliﬁed into (13) where the regularization is from the noise in the normal space."
IMPLICATIONS AND EXAMPLES,0.08702531645569621,"dY (t) = −1/4 · ∂Φ(Y (t))∇tr[c∇2L(Y (t))]dt.
(13)"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.08781645569620253,"6
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE
In this section, we show provable beneﬁt of label noise in generalization using our framework
(Theorem B.7) in a concrete setting, the overparametrized linear models (OLM) (Woodworth
et al., 2020). While the existing implicit regularization results for Gradient Flow often relates the
generalization quality to initialization, e.g., Woodworth et al. (2020) shows that for OLM, small
initialization corresponds to the rich regime and prefers solutions with small ℓ1 norm while large
initialization corresponds to the kernel regime and prefers solutions with small ℓ2 norm, our result
Theorem 6.1 surprisingly proves that even if an OLM is initialized in the kernel regime, label noise
SGD can still help it escape and then enter the rich regime by minimizing its weighted ℓ1 norm. When
the groundtruth is κ-sparse, this provides a eO(κ ln d) vs Ω(d) sample complexity separation between
SGD with label noise and GD when both initialized in the kernel regime. Here d is the dimension of
the groundtruth. The lower bound for GD in the kernel regime is folklore, but for completeness, we
state the result as Theorem 6.7 in Section 6.3 and append its proof in Appendix D.6.
Theorem 6.1. In the setting of OLM, suppose the groundtruth is κ-sparse and n ≥Ω(κ ln d) training
data are sampled from either i.i.d. Gaussian or Boolean distribution. Then for any initialization xinit
(except a zero-measure set) and any ϵ > 0, there exist η0, T > 0 such that for any η < η0, OLM
trained with label noise SGD (12) with LR equal to η for ⌊T/η2⌋steps returns an ϵ-optimal solution,
with probability of 1 −e−Ω(n) over the randomness of the training dataset."
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.08860759493670886,Published as a conference paper at ICLR 2022
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.0893987341772152,The proof roadmap of Theorem 6.1 is the following:
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09018987341772151,"1. Show Assumption 3.1 is satisﬁed, i.e., the set of local minimizers, Γ, is indeed a manifold
and the hessian ∇2L(x) is non-degenerate on Γ (by Lemma 6.2);
2. Show Assumption 3.2 is satisﬁed, i.e., Φ(U) ⊂Γ (by Lemma 6.3);
3. Show the limiting ﬂow (13) converges to the minimizer of the regularizer (by Lemma 6.5);
4. Show the minimizer of the regularizer recovers the groundtruth (by Lemma 6.6)."
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09098101265822785,"Our setting is more general than HaoChen et al. (2020), which assumes w∗∈{0, 1}d and their
reparametrization can only express positive linear functions, i.e., w = u⊙2. Their eO(κ2) rate is
achieved with a delicate three phase LR schedule, while our O(κ ln d) rate only uses a constant LR."
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09177215189873418,"Setting: Let {(zi, yi)}i∈[n] be the training dataset where z1, . . . , zn
i.i.d.
∼Unif({±1}d) or N(0, Id)
and each yi = ⟨zi, w∗⟩for some unknown w∗∈Rd. We assume that w∗is κ-sparse for some κ < d.
Denote x =
 u
v

∈RD = R2d, and we will use x and (u, v) exchangeably as the parameter of
functions deﬁned on RD in the sequel. For each i ∈[n], deﬁne fi(x) = fi(u, v) = z⊤
i (u⊙2 −v⊙2).
Then we ﬁt {(zi, yi)}i∈[n] with an overparametrized model through the following loss function:"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.0925632911392405,"L(x) = L(u, v) = 1 n
Xn"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09335443037974683,"i=1 ℓi(u, v),
where ℓi(u, v) = 1"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09414556962025317,"2(fi(u, v) −yi)2.
(14)"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.0949367088607595,It is straightforward to verify that ∇2L(x) = 4
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09572784810126582,"n
Pn
i=1
  zi⊙u
−zi⊙v
  zi⊙u
−zi⊙v
⊤, ∀x ∈Γ. For simplicity, we
deﬁne Z = (z1, . . . , zn)⊤∈Rn×d and Y = (y1, . . . , yn)⊤∈Rn. Consider the following manifold:"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09651898734177215,"Γ =

x = (u⊤, v⊤)⊤∈U : Z(u⊙2 −v⊙2) = Y
	
,
where U = (R \ {0})D.
(15)"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09731012658227849,"We verify that the above loss function L and manifold Γ satisfy Assumption 3.1 by Lemma 6.2, and
that the neighborhood U and Γ satisfy Assumption 3.2 by Lemma 6.3.
Lemma 6.2. Consider the loss L deﬁned in (14) and manifold Γ deﬁned in (15). If data is full
rank, i.e., rank(Z) = n, then it holds that (a). Γ is a smooth manifold of dimension D −n; (b).
rank(∇2L(x)) = n for all x ∈Γ. In particular, rank(Z) = n holds with probability 1 for Gaussian
distribution and with probability 1 −cd for Boolean distribution for some constant c ∈(0, 1).
Lemma 6.3. Consider the loss function L deﬁned in (14), manifold Γ and its open neighborhood
deﬁned in (15). For gradient ﬂow dxt"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.0981012658227848,"dt = −∇L(xt) starting at any x0 ∈U, it holds that Φ(x0) ∈Γ.
Remark 6.4. In previous works (Woodworth et al., 2020; Azulay et al., 2021), the convergence of
gradient ﬂow is only assumed. Recently Pesme et al. (2021) proved it for a speciﬁc initialization, i.e.,
uj = vj = α, ∀j ∈[n] for some α > 0. Lemma 6.3 completely removes the technical assumption."
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09889240506329114,"Therefore, by the result in the previous section, the implicit regularizer on the manifold is R(x) =
tr(Σ(x)) = tr(δ2∇2L(x)). Without loss of generality, we take δ = 1. Hence, it follows that"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.09968354430379747,R(x) = 4 n XD j=1 Xn
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.10047468354430379,"i=1 z2
i,j

(u2
j + v2
j ).
(16)"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.10126582278481013,The limiting behavior of label noise SGD is described by a Riemannian gradient ﬂow on Γ as follows:
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.10205696202531646,"dxt = −1/4 · ∂Φ(xt)∇R(xt)dt, with x0 = Φ(xinit) ∈Γ.
(17)"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.10284810126582279,"The goal is to show that the above limiting ﬂow will converge to the underlying groundtruth x∗=
 u∗ v∗
"
PROVABLE GENERALIZATION BENEFIT WITH LABEL NOISE,0.10363924050632911,"where (u∗, v∗) = ([w∗]⊙1/2
+
, [−w∗]⊙1/2
+
)."
LIMITING FLOW CONVERGES TO MINIMIZERS OF REGULARIZER,0.10443037974683544,"6.1
LIMITING FLOW CONVERGES TO MINIMIZERS OF REGULARIZER"
LIMITING FLOW CONVERGES TO MINIMIZERS OF REGULARIZER,0.10522151898734178,"In this subsection we show limiting ﬂow (13) starting from anywhere on Γ converges to the minimizer
of regularizer R (by Lemma 6.5). The proof contains two parts: (a) the limiting ﬂow converges;
(b) the limit point of the ﬂow cannot be sub-optimal stationary points. These are indeed the most
technical and difﬁcult parts of proving the O(κ ln d) upper bound, where the difﬁculty comes from
the fact that the manifold Γ is not compact, and the stationary points of the limiting ﬂow are in fact all
located on the boundary of Γ. However, the limiting ﬂow itself is not even deﬁned on the boundary
of the manifold Γ. Even if we can extend ∂Φ(·)∇R(·) continuously to entire RD, the continuous
extension is not everywhere differentiable."
LIMITING FLOW CONVERGES TO MINIMIZERS OF REGULARIZER,0.1060126582278481,"Thus the non-compactness of Γ brings challenges for both (a) and (b). For (a), the convergence
for standard gradient ﬂow is often for free, as long as the trajectory is bounded and the objective is"
LIMITING FLOW CONVERGES TO MINIMIZERS OF REGULARIZER,0.10680379746835443,Published as a conference paper at ICLR 2022
LIMITING FLOW CONVERGES TO MINIMIZERS OF REGULARIZER,0.10759493670886076,"analytic or smooth and semialgebraic. The latter ensures the so-called Kurdyka-Łojasiewicz (KL)
inequality (Lojasiewicz, 1963), which implies ﬁnite trajectory length and thus the convergence.
However, since our ﬂow does not satisfy those nice properties, we have to show that the limiting ﬂow
satisﬁes Polyak-Łojasiewicz condition (a special case of KL condition) (Polyak, 1964) via careful
calculation (by Lemma D.16)."
LIMITING FLOW CONVERGES TO MINIMIZERS OF REGULARIZER,0.10838607594936708,"For (b), the standard analysis based on center stable manifold theorem shows that gradient descent/ﬂow
converges to strict saddle (stationary point with at least one negative eigenvalue in hessian) only for a
zero-measure set of initialization (Lee et al., 2016; 2017). However, such analyses cannot deal with
the case where the ﬂow is not differentiable at the sub-optimal stationary point. To circumvent this
issue, we prove the non-convergence to sub-optimal stationary points with a novel approach: we show
that for any stationary point x, whenever there exists a descent direction of the regularizer R at x, we
can construct a potential function which increases monotonically along the ﬂow around x, while the
potential function is equal to −∞at x, leading to a contradiction. (See proof of Lemma 6.5.)
Lemma 6.5. Let {xt}t≥0 ⊆RD be generated by the ﬂow deﬁned in (17) with any initialization
x0 ∈Γ. Then x∞= limt→∞xt exists. Moreover, x∞= x∗is the optimal solution of (18)."
MINIMIZER OF THE REGULARIZER RECOVERS THE SPARSE GROUNDTRUTH,0.10917721518987342,"6.2
MINIMIZER OF THE REGULARIZER RECOVERS THE SPARSE GROUNDTRUTH
Note 1"
MINIMIZER OF THE REGULARIZER RECOVERS THE SPARSE GROUNDTRUTH,0.10996835443037975,"n
Pn
i=1 z2
i,j = 1 when zi,j
iid
∼Unif{−1, 1}, and we can show minimizing R(x) on Γ, (18),
is equivalent to ﬁnding the minimum ℓ1 norm solution of Equation (14). Standard results in sparse
recovery imply that minimum ℓ1 norm solution recovers with the sparse groundtruth. The gaussian
case is more complicated but still can be proved with techniques from Tropp (2015)."
MINIMIZER OF THE REGULARIZER RECOVERS THE SPARSE GROUNDTRUTH,0.11075949367088607,"minimize
R(x) = 4 n Xd j=1 Xn"
MINIMIZER OF THE REGULARIZER RECOVERS THE SPARSE GROUNDTRUTH,0.1115506329113924,"i=1 z2
i,j

(u2
j + v2
j ),"
MINIMIZER OF THE REGULARIZER RECOVERS THE SPARSE GROUNDTRUTH,0.11234177215189874,"subject to
Z(u⊙2 −v⊙2) = Zw∗.
(18)"
MINIMIZER OF THE REGULARIZER RECOVERS THE SPARSE GROUNDTRUTH,0.11313291139240507,"Lemma 6.6. Let z1, . . . , zn
i.i.d.
∼Unif({±1}d) or N(0, Id). Then there exist some constants C, c > 0
such that if n ≥Cκ ln d, then with probability at least 1 −e−cn, the optimal solution of (18), (ˆu, ˆv),
is unique up to sign ﬂips of each coordinate and recovers the groundtruth, i.e., ˆu⊙2 −ˆv⊙2 = w∗."
LOWER BOUND FOR GRADIENT DESCENT IN THE KERNEL REGIME,0.11392405063291139,"6.3
LOWER BOUND FOR GRADIENT DESCENT IN THE KERNEL REGIME"
LOWER BOUND FOR GRADIENT DESCENT IN THE KERNEL REGIME,0.11471518987341772,"In this subsection we show GD needs at least Ω(d) samples to learn OLM, when initialized in the
kernel regime. This lower bound holds for all learning rate schedules and numbers of steps. This is in
sharp contrast to the eO(κ ln d) sample complexity upper bound of SGD with label noise. Following
the setting of kernel regime in (Woodworth et al., 2020), we consider the limit of u0 = v0 = α1,
with α →∞. It holds that fi(u0, v0) = 0 and ∇fi(u0, v0) = [αzi, −αzi] for each i ∈[n].
Standard convergence analysis for NTK (Neural Tangent Kernel, Jacot et al. (2018)) shows that upon
convergence, the distance traveled by parameter converges to 0, and thus the learned model shall
converge in function space, so is the generalization performance. For ease of illustration, we directly
consider the lower bound for test loss when the NTK is ﬁxed throughout the training."
LOWER BOUND FOR GRADIENT DESCENT IN THE KERNEL REGIME,0.11550632911392406,"Theorem 6.7. Assume z1, . . . , zn
i.i.d.
∼N(0, Id) and yi = z⊤
i w∗, for all i ∈[n]. Deﬁne the loss
with linearized model as L(x) = Pn
i=1(fi(x0) + ⟨∇fi(x0), x −x0⟩−yi)2, where x =
 u
v

and
x0 =
 u0
v0

= α
 1
1

. Then for any groundtruth w∗, any learning rate schedule {ηt}t≥1, and any ﬁxed
number of steps T, the expected ℓ2 loss of x(T) is at least (1 −n"
LOWER BOUND FOR GRADIENT DESCENT IN THE KERNEL REGIME,0.11629746835443038,"d ) ∥w∗∥2
2, where x(T) is the T-th
iterate of GD on L, i.e., x(t + 1) = x(t) −ηt∇L(x(t)), for all t ≥0."
CONCLUSION AND FUTURE WORK,0.11708860759493671,"7
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.11787974683544304,"We propose a mathematical framework to study the implicit bias of SGD with inﬁnitesimal LR. We
show that with arbitrary noise covariance, Θ(η−2) steps of SGD converge to a limiting diffusion
on certain manifold of local minimizer, as the LR η →0. For speciﬁc noise types, this allows
us to recover and strengthen results regarding implicit bias in previous works with much simpler
analysis. In particular, we show a sample complexity gap between label noise SGD and GD in the
kernel regime for a overparametrized linear model, justifying the generalization beneﬁt of SGD. For
the future work, we believe our framework can be applied to analyze the implicit bias of SGD in
more complex models towards better understanding of the algorithmic regularization induced by
stochasticity. It will be valuable to extend our method to other stochastic optimization algorithms,
e.g., ADAM, SGD with momentum."
CONCLUSION AND FUTURE WORK,0.11867088607594936,Published as a conference paper at ICLR 2022
CONCLUSION AND FUTURE WORK,0.1194620253164557,ACKNOWLEDGEMENT
CONCLUSION AND FUTURE WORK,0.12025316455696203,"We thank Yangyang Li for pointing us to Katzenberger (1991). We also thank Wei Zhan and Jason
Lee for helpful discussions."
CONCLUSION AND FUTURE WORK,0.12104430379746836,"The authors acknowledge support from NSF, ONR, Simons Foundation, Schmidt Foundation, Mozilla
Research, Amazon Research, DARPA and SRC. ZL is also supported by Microsoft Research PhD
Fellowship."
REFERENCES,0.12183544303797468,REFERENCES
REFERENCES,0.12262658227848101,"Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. Advances in neural information processing systems,
2019a."
REFERENCES,0.12341772151898735,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019b."
REFERENCES,0.12420886075949367,"Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning, pp.
244–253. PMLR, 2018."
REFERENCES,0.125,"Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019a."
REFERENCES,0.12579113924050633,"Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322–332. PMLR, 2019b."
REFERENCES,0.12658227848101267,"Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake Woodworth, Nathan Srebro, Amir
Globerson, and Daniel Soudry. On the implicit bias of initialization shape: Beyond inﬁnitesimal
mirror descent. arXiv preprint arXiv:2102.09769, 2021."
REFERENCES,0.127373417721519,"Augustin Banyaga and David Hurtubise. Lectures on Morse homology, volume 29. Springer Science
& Business Media, 2013."
REFERENCES,0.1281645569620253,"Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, 2013."
REFERENCES,0.12895569620253164,"Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural
networks driven by an ornstein-uhlenbeck like process. In Conference on learning theory, pp.
483–513. PMLR, 2020."
REFERENCES,0.12974683544303797,"Xiang Cheng, Dong Yin, Peter Bartlett, and Michael Jordan. Stochastic gradient and langevin
processes. In International Conference on Machine Learning, pp. 1810–1819. PMLR, 2020."
REFERENCES,0.1305379746835443,"Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
arXiv preprint arXiv:1812.07956, 2018."
REFERENCES,0.13132911392405064,"Y Cooper. The critical locus of overparameterized neural networks. arXiv preprint arXiv:2005.04210,
2020."
REFERENCES,0.13212025316455697,"Yaim Cooper.
The loss landscape of overparameterized neural networks.
arXiv preprint
arXiv:1804.10200, 2018."
REFERENCES,0.13291139240506328,"Alex Damian, Tengyu Ma, and Jason Lee. Label noise sgd provably prefers ﬂat global minimizers.
arXiv preprint arXiv:2106.06530, 2021."
REFERENCES,0.1337025316455696,"Amit Daniely. Sgd learns the conjugate kernel class of the network. arXiv preprint arXiv:1702.08503,
2017."
REFERENCES,0.13449367088607594,"Manfredo P Do Carmo. Riemannian geometry. Springer Science & Business Media, 2013."
REFERENCES,0.13528481012658228,Published as a conference paper at ICLR 2022
REFERENCES,0.1360759493670886,"Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in
neural network energy landscape. In International conference on machine learning, pp. 1309–1318.
PMLR, 2018."
REFERENCES,0.13686708860759494,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–1685.
PMLR, 2019."
REFERENCES,0.13765822784810128,"Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018."
REFERENCES,0.13844936708860758,"K. J. Falconer. Differentiation of the limit mapping in a dynamical system. Journal of the London
Mathematical Society, s2-27(2):356–372, 1983. ISSN 0024-6107. doi: 10.1112/jlms/s2-27.2.356."
REFERENCES,0.13924050632911392,"Jianqing Fan, Zhuoran Yang, and Mengxin Yu. Understanding implicit regularization in over-
parameterized nonlinear statistical model. arXiv preprint arXiv:2007.08322, 2020."
REFERENCES,0.14003164556962025,"Benjamin Fehrman, Benjamin Gess, and Arnulf Jentzen. Convergence rates for the stochastic gradient
descent method for non-convex objective functions. Journal of Machine Learning Research, 21,
2020."
REFERENCES,0.14082278481012658,"C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectiﬁed network optimization.
arXiv preprint arXiv:1611.01540, 2016."
REFERENCES,0.14161392405063292,"Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson.
Loss surfaces, mode connectivity, and fast ensembling of dnns. arXiv preprint arXiv:1802.10026,
2018."
REFERENCES,0.14240506329113925,"Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017."
REFERENCES,0.14319620253164558,"Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp. 1832–1841.
PMLR, 2018a."
REFERENCES,0.1439873417721519,"Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1–10. IEEE, 2018b."
REFERENCES,0.14477848101265822,"Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the implicit
bias of the noise covariance. arXiv preprint arXiv:2006.08680, 2020."
REFERENCES,0.14556962025316456,"Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the gen-
eralization gap in large batch training of neural networks. arXiv preprint arXiv:1705.08741,
2017."
REFERENCES,0.1463607594936709,"Andrew Holbrook. Differentiating the pseudo determinant. Linear Algebra and its Applications, 548:
293–304, 2018."
REFERENCES,0.14715189873417722,"Elton P Hsu. Stochastic analysis on manifolds. Number 38. American Mathematical Soc., 2002."
REFERENCES,0.14794303797468356,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018."
REFERENCES,0.14873417721518986,"Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors inﬂuencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017."
REFERENCES,0.1495253164556962,"Jeff Kahn, J´anos Koml´os, and Endre Szemer´edi. On the probability that a random±1-matrix is
singular. Journal of the American Mathematical Society, 8(1):223–240, 1995."
REFERENCES,0.15031645569620253,"Ioannis Karatzas and Steven Shreve. Brownian motion and stochastic calculus, volume 113. springer,
2014."
REFERENCES,0.15110759493670886,Published as a conference paper at ICLR 2022
REFERENCES,0.1518987341772152,"Gary Shon Katzenberger. Solutions of a stochastic differential equation forced onto a manifold by a
large drift. The Annals of Probability, pp. 1587–1628, 1991."
REFERENCES,0.15268987341772153,"Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016."
REFERENCES,0.15348101265822786,"Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint
arXiv:1404.5997, 2014."
REFERENCES,0.15427215189873417,"Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, and
Rong Ge. Explaining landscape connectivity of low-cost solutions for multilayer nets. arXiv
preprint arXiv:1906.06247, 2019."
REFERENCES,0.1550632911392405,"Yann A LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M¨uller. Efﬁcient backprop. In
Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012."
REFERENCES,0.15585443037974683,"Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on learning theory, pp. 1246–1257. PMLR, 2016."
REFERENCES,0.15664556962025317,"Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin
Recht. First-order methods almost always avoid saddle points. arXiv preprint arXiv:1710.07406,
2017."
REFERENCES,0.1574367088607595,"Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. arXiv preprint arXiv:1902.00621, 2019a."
REFERENCES,0.15822784810126583,"Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modiﬁed equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pp. 2101–2110. PMLR,
2017."
REFERENCES,0.15901898734177214,"Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modiﬁed equations and dynamics of stochastic
gradient algorithms i: Mathematical foundations. The Journal of Machine Learning Research, 20
(1):1474–1520, 2019b."
REFERENCES,0.15981012658227847,"Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018."
REFERENCES,0.1606012658227848,"Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory,
pp. 2–47. PMLR, 2018."
REFERENCES,0.16139240506329114,"Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019c."
REFERENCES,0.16218354430379747,"Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent
for matrix factorization: Greedy low-rank learning. In International Conference on Learning
Representations, 2020a."
REFERENCES,0.1629746835443038,"Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with traditional
optimization analyses: The intrinsic learning rate. Advances in Neural Information Processing
Systems, 33, 2020b."
REFERENCES,0.16376582278481014,"Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efﬁcient than
fully-connected nets? arXiv preprint arXiv:2010.08515, 2020c."
REFERENCES,0.16455696202531644,"Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic
differential equations (sdes). arXiv preprint arXiv:2102.12470, 2021."
REFERENCES,0.16534810126582278,"Shiyu Liang, Ruoyu Sun, Yixuan Li, and Rayadurgam Srikant. Understanding the loss surface of
neural networks for binary classiﬁcation. In International Conference on Machine Learning, pp.
2835–2843. PMLR, 2018."
REFERENCES,0.1661392405063291,"Stanislaw Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les ´equations
aux d´eriv´ees partielles, 117(87-89):2, 1963."
REFERENCES,0.16693037974683544,Published as a conference paper at ICLR 2022
REFERENCES,0.16772151898734178,"Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019."
REFERENCES,0.1685126582278481,"Andrew Y Ng. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings
of the twenty-ﬁrst international conference on Machine learning, pp. 78, 2004."
REFERENCES,0.16930379746835442,"Quynh Nguyen. On connected sublevel sets in deep learning. In International Conference on Machine
Learning, pp. 4790–4799. PMLR, 2019."
REFERENCES,0.17009493670886075,"Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias Hein. On the loss landscape of a class
of deep neural networks with no bad local valleys. arXiv preprint arXiv:1809.10749, 2018."
REFERENCES,0.17088607594936708,Lawrence M. Perko. Differential equations and dynamical systems. 2001.
REFERENCES,0.17167721518987342,"Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear
networks: a provable beneﬁt of stochasticity. arXiv preprint arXiv:2106.09524, 2021."
REFERENCES,0.17246835443037975,"David Pollard. Convergence of stochastic processes. Springer Science & Business Media, 2012."
REFERENCES,0.17325949367088608,"Boris T Polyak. Gradient methods for solving equations and inequalities. USSR Computational
Mathematics and Mathematical Physics, 4(6):17–32, 1964."
REFERENCES,0.17405063291139242,"Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax-optimal rates for sparse additive
models over kernel classes via convex programming. Journal of machine learning research, 13(2),
2012."
REFERENCES,0.17484177215189872,"Bin Shi, Weijie J Su, and Michael I Jordan. On learning rates and schr\” odinger operators. arXiv
preprint arXiv:2004.06977, 2020."
REFERENCES,0.17563291139240506,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822–2878, 2018."
REFERENCES,0.1764240506329114,"Joel A Tropp. Convex recovery of a structured signal from independent random linear measurements.
In Sampling Theory, a Renaissance, pp. 67–101. Springer, 2015."
REFERENCES,0.17721518987341772,"Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse
recovery. Advances in Neural Information Processing Systems, 32:2972–2983, 2019."
REFERENCES,0.17800632911392406,"Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in two-layer neural network
optimization landscapes. arXiv preprint arXiv:1802.06384, 2018."
REFERENCES,0.1787974683544304,"Yeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and Jimmy Ba. Interplay
between optimization and generalization of stochastic gradient descent with covariance noise.
arXiv preprint arXiv:1902.08234, 2019."
REFERENCES,0.17958860759493672,"Ward Whitt. Stochastic-process limits: an introduction to stochastic-process limits and their applica-
tion to queues. Springer Science & Business Media, 2002."
REFERENCES,0.18037974683544303,"Stephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type. part ii:
Continuous time analysis. arXiv preprint arXiv:2106.02588, 2021."
REFERENCES,0.18117088607594936,"Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In
Conference on Learning Theory, pp. 3635–3673. PMLR, 2020."
REFERENCES,0.1819620253164557,"Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On the
noisy gradient descent that generalizes as sgd. In International Conference on Machine Learning,
pp. 10367–10376. PMLR, 2020."
REFERENCES,0.18275316455696203,"Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics:
Stochastic gradient descent escapes from sharp minima exponentially fast.
arXiv preprint
arXiv:2002.03495, 2020."
REFERENCES,0.18354430379746836,Published as a conference paper at ICLR 2022
REFERENCES,0.1843354430379747,"Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019."
REFERENCES,0.185126582278481,"Peng Zhao, Yun Yang, and Qiao-Chu He.
Implicit regularization via hadamard product over-
parametrization in high-dimensional linear regression. arXiv preprint arXiv:1903.09367, 2019."
REFERENCES,0.18591772151898733,"Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. arXiv
preprint arXiv:1803.00195, 2018."
REFERENCES,0.18670886075949367,"Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning, 109(3):467–492, 2020."
REFERENCES,0.1875,Published as a conference paper at ICLR 2022
REFERENCES,0.18829113924050633,"A
PRELIMINARIES ON STOCHASTIC PROCESSES"
REFERENCES,0.18908227848101267,"We ﬁrst clarify the notations in this paper. For any integer k, we denote Ck as the set of the k
times continuously differentiable functions. We denote a ∧b = min{a, b}. For any vector u, v
and α ∈R, we deﬁne [u ⊙v]i = uivi and [v⊙α]i = vα
i . For any matrix A, we denote its pseudo
inverse by A†. For mapping F : RD →RD, we denote the Jacobian of F at x by ∂F(x) ∈RD×D
where the (i, j)-th entry is ∂jFi(x). We also use ∂F(x)[u] and ∂2F(x)[u, v] to denote the ﬁrst
and second order directional derivative of F at x along the derivation of u (and v). We abuse the
notation of ∂2F by viewing it a linear mapping deﬁned on RD ⊗RD ∼= RD2, in the sense that
∂2F(x)[Σ] = PD
i,j=1 ∂2F(x)[ei, ej]Σij, for any Σ ∈RD×D. For any submanifold Γ ⊂RD and
x ∈Γ, we denote by Tx(Γ) the tangent space of Γ at x and T ⊥
x (Γ) the normal space of Γ at x."
REFERENCES,0.189873417721519,"Next, we review a few basics of stochastic processes that will be useful for proving our results, so
that our paper will be self-contained. We refer the reader to classics like Karatzas & Shreve (2014);
Billingsley (2013); Pollard (2012) for more systematic derivations."
REFERENCES,0.1906645569620253,"Throughout the rest of this section, let E be a Banach space equipped with norm ∥· ∥, e.g., (R, | · |)
and (RD, ∥· ∥2)."
REFERENCES,0.19145569620253164,"A.1
C `ADL `AGFUNCTION AND METRIC"
REFERENCES,0.19224683544303797,"Deﬁnition A.1 (C`adl`agfunction). Let T ∈[0, ∞]. A function g : [0, T) →E is c`adl`ag if for all
t ∈[0, T) it is right-continuous at t and its left limit g(t−) exists. Let DE[0, T) be the set of all
c`adl`agfunction mapping [0, T) into E. We also use DE[0, T) to denote the set of all continuous
function mapping [0, T) into E. By deﬁnition, CE[0, T) ⊂DE[0, T).
Deﬁnition A.2 (Continuity modulus). For any function f : [0, ∞) →E and any interval I ⊆[0, ∞),
we deﬁne"
REFERENCES,0.1930379746835443,"ω(f; I) = sup
s,t∈I
∥f(s) −f(t)∥."
REFERENCES,0.19382911392405064,"For any N ∈N and θ > 0, we further deﬁne the continuity modulus of continuous f as"
REFERENCES,0.19462025316455697,"ωN(f, θ) =
sup
0≤t≤t+θ≤N
{ω(f; [t, t + θ])}."
REFERENCES,0.19541139240506328,"Moreover, the continuity modulus of c`adl`agf ∈DE[0, ∞) is deﬁned as"
REFERENCES,0.1962025316455696,"ω′
N(f, θ) = inf

max
i≤r ω(f; [ti−1, ti) : 0 ≤t0 < · · · < tr = N, inf
i<r(ti −ti−1) ≥θ

."
REFERENCES,0.19699367088607594,"Deﬁnition A.3 (Jump). For any g ∈DE[0, T), we deﬁne the jump of g at t to be"
REFERENCES,0.19778481012658228,∆g(t) = g(t) −g(t−).
REFERENCES,0.1985759493670886,"For any δ > 0, we deﬁne hδ : [0, ∞) →[0, ∞) by"
REFERENCES,0.19936708860759494,"hδ(r) =
0
if r ≤δ
1 −δ/r
if r ≥δ ."
REFERENCES,0.20015822784810128,"We then further deﬁne Jδ : DRD[0, ∞) →DRD[0, ∞) (Katzenberger, 1991) as"
REFERENCES,0.20094936708860758,"Jδ(g)(t) =
X"
REFERENCES,0.20174050632911392,"0<s≤t
hδ(∥∆g(s)∥)∆g(s).
(19)"
REFERENCES,0.20253164556962025,"Deﬁnition A.4 (Skorohod metric on DE[0, ∞)). For each ﬁnite T > 0 and each pair of functions
f, g ∈DE[0, ∞), deﬁne dT (f, g) as the inﬁmum of all those values of δ for which there exist grids
0 ≤t0 < t1 < · · · < tm and 0 < s0 < s1 < · · · < · · · < sm, with tk, sk ≥T, such that
|ti −si| ≤δ for i = 0, . . . , k, and"
REFERENCES,0.20332278481012658,"∥f(t) −g(s)∥≤δ
if (t, s) ∈[ti, ti+1) × [si, si+1)"
REFERENCES,0.20411392405063292,"for i = 0, . . . , k −1. The Skorohod metric on DE[0, ∞) is deﬁned to be"
REFERENCES,0.20490506329113925,"d(f, g) = ∞
X"
REFERENCES,0.20569620253164558,"T =1
2−T min{1, dT (f, g)}."
REFERENCES,0.2064873417721519,Published as a conference paper at ICLR 2022
REFERENCES,0.20727848101265822,"A.2
STOCHASTIC PROCESSES AND STOCHASTIC INTEGRAL"
REFERENCES,0.20806962025316456,"Let (Ω, F, {Ft}t≥0, P) be a ﬁltered probability space.
Deﬁnition A.5 (Cross variation). Let X and Y be two {Ft}t≥0-adapted stochastic processes such
that X has sample paths in DRD×e[0, ∞) and Y has samples paths in DRe[0, ∞), then the cross
variation of X and Y on (0, t], denoted by [X, Y ](t), is deﬁned to be the limit of m−1
X"
REFERENCES,0.2088607594936709,"i=0
(X(ti+1) −X(ti))(Y (ti+1) −Y (ti))"
REFERENCES,0.20965189873417722,"in probability as the mesh size of 0 = t0 < t1 < · · · < tm = t goes to 0, if it exists. Moreover, for Y
itself, we write"
REFERENCES,0.21044303797468356,"[Y ] = e
X"
REFERENCES,0.21123417721518986,"i=1
[Yi, Yi]"
REFERENCES,0.2120253164556962,"Deﬁnition A.6 (Martingale). Let {X(t)}t≥0 be a {Ft}t≥0-adapted stochastic process. If for all
0 ≤s ≤t, it holds that"
REFERENCES,0.21281645569620253,"E[X(t) | Fs] = X(s),"
REFERENCES,0.21360759493670886,"then X is called a martingale.
Deﬁnition A.7 (Local martingale). Let {X(t)}t≥0 be a {Ft}t≥0-adapted stochastic process. If there
exists a sequence of {Ft}t≥0-stopping time, {τk}k≥0, such that"
REFERENCES,0.2143987341772152,"• P[τk < τk+1] = 1, P[limk→∞τk = ∞] = 1,"
REFERENCES,0.21518987341772153,"• and {Xτk(t)}t≥0 is a {Ft}t≥0-adapted martingale,"
REFERENCES,0.21598101265822786,"then X is called a local martingale.
Deﬁnition A.8 (Semimartingale). Let {X(t)}t≥0 be a {Ft}t≥0-adapted stochastic process. If there
exists a local martingale {M(t)}t≥0 and a c`adl`ag{Ft}t≥0-adapted process {A(t)}t≥0 with bounded
total variation that X(t) = M(t) + A(t), then X is called a semimartingale.
Lemma A.9 (Itˆo’s Lemma). Let {X(t)}t≥0 be deﬁned through the following Itˆo drift-diffusion
process:"
REFERENCES,0.21677215189873417,dX(t) = µ(t)dt + σ(t)dW(t).
REFERENCES,0.2175632911392405,"where {W(t)}t≥0 is the standard Brownian motion. Then for any twice differentiable function f, it
holds that"
REFERENCES,0.21835443037974683,"df(t, X(t)) =
∂f"
REFERENCES,0.21914556962025317,∂t + (∇xf)⊤µt + 1
REFERENCES,0.2199367088607595,"2 tr[σ⊤∇2
xfσ]

dt + (∇xf)⊤σ(t)dW(t)."
REFERENCES,0.22072784810126583,"A.3
WEAK CONVERGENCE FOR STOCHASTIC PROCESSES"
REFERENCES,0.22151898734177214,"Let (DE[0, ∞), A, d) be a metric space equipped with a σ-algebra A and the Skorohod metric deﬁned
in the previous subsection."
REFERENCES,0.22231012658227847,"Let {Xn}n≥0 be a sequence of stochastic processes on a sequence of probability spaces
{(Ωn, Fn, Pn)}n≥0 such that each Xn has sample paths in DE[0, ∞). Also, let X be a stochas-
tic process on (Ω, F, P) with sample paths on DE[0, ∞).
Deﬁnition A.10 (Weak convergence). A sequence of stochastic process{Xn}n≥0 is said to converge
in distribution or weakly converge to X (written as Xn ⇒X) if and only if for all A-measurable,
bounded, and continuous function f : DE[0, ∞) →R, it holds that"
REFERENCES,0.2231012658227848,"lim
n→∞E [f(Xn)] = E [f(X)] .
(20)"
REFERENCES,0.22389240506329114,"Though we deﬁne weak convergence for a countable sequence of stochastic processes, but it is still
valid if we index the stochastic processes by real numbers, e.g., {Xη}η≥0, and consider the weak
convergence of Xη as η →0. This is because the convergence in (20) is for a sequence of real
numbers, which is also well-deﬁned if we replace limn→∞by limη→0."
REFERENCES,0.22468354430379747,Published as a conference paper at ICLR 2022
REFERENCES,0.2254746835443038,"Deﬁnition A.11 (δ-Prohorov distance). Let δ > 0. For any two probability measures P and Q on a
metric space with metric d, let (X, Y ) be a coupling such that P is the marginalized law of X and Q
that of Y . We deﬁne"
REFERENCES,0.22626582278481014,"ρδ(P, Q) = inf{ϵ > 0 : ∃(X, Y ), P[d(X, Y ) ≥ϵ] ≤δ}."
REFERENCES,0.22705696202531644,Note this distance is not a metric because it does not satisfy triangle inequality.
REFERENCES,0.22784810126582278,"Deﬁnition A.12 (Prohorov metric). For any two probability measures P and Q on a metric space
with metric d, let (X, Y ) be a coupling such that P is the marginalized law of X and Q that of Y .
Denote the marginal laws of X and Y by L(X) and L(Y ) respectively. We deﬁne the Prohorov
metric as"
REFERENCES,0.2286392405063291,"ρ(P, Q) = inf{ϵ > 0 : ∃(X, Y ), L(X) = P, L(Y ) = Q, P[d(X, Y ) ≥ϵ] ≤ϵ}."
REFERENCES,0.22943037974683544,"It can be shown that Xn ⇒X is equivalent to limn→∞ρ(Xn, X) = 0."
REFERENCES,0.23022151898734178,"Theorem A.13 (Skorohod Representation Theorem). Suppose Pn, n = 1, 2, . . . and P are proba-
bility measures on E such that Pn ⇒P. Then there is a probability space (Ω, F, P) on which are
deﬁned E-valued random variables Xn, n = 1, 2, . . . and X with distributions Pn and P respectively,
such that limn→∞Xn = X a.s."
REFERENCES,0.2310126582278481,"The main convergence result in Katzenberger (1991) (Theorem B.7) are in the sense of Skorohod
metric in Deﬁnition A.4, which is harder to understand and use compared to the more common
uniform metric (Deﬁnition A.14). However, convergence in Skorohod metric and uniform metric
indeed coincide with each other when the limit is in CRD[0, ∞), i.e., the continuous functions."
REFERENCES,0.23180379746835442,"Deﬁnition A.14 (Uniform metric on DE[0, ∞)). For each ﬁnite T > 0 and each pair of functions
f, g ∈DE[0, T), the uniform metric is deﬁned to be"
REFERENCES,0.23259493670886075,"dU(f, g; T) =
sup
t∈[0,T )
∥f(t) −g(t)∥."
REFERENCES,0.23338607594936708,"The uniform metric on DE[0, ∞) is deﬁned to be"
REFERENCES,0.23417721518987342,"dU(f, g) = ∞
X"
REFERENCES,0.23496835443037975,"T =1
2−T min{1, dU(f, g; T)}."
REFERENCES,0.23575949367088608,"Lemma A.15 (Problem 7, Section 5, Pollard (2012)). If Xn ⇒X in the Skorohod sense, and X has
sample paths in CRD[0, ∞), then Xn ⇒X in the uniform metric."
REFERENCES,0.23655063291139242,"Remark A.16. We shall note the uniform metric deﬁned above is weaker than supt∈[0,∞) ∥f(t) −
g(t)∥. Convergence in the uniform metric on [0, ∞] deﬁned in Deﬁnition A.14 is equivalent to
convergence in the uniform metric on each compact set [0, T] for T ∈N+. The same holds for the
Skorohod topology."
REFERENCES,0.23734177215189872,"B
LIMITING DIFFUSION OF SGD"
REFERENCES,0.23813291139240506,"In this section, we give a complete derivation of the limiting diffusion of SGD. Here we use ⇒to
denote the convergence in distribution. For any U ⊆RD, we denote by ˚U its interior. For linear
space S, we use S⊥to denote its orthogonal complement."
REFERENCES,0.2389240506329114,"First, as mentioned in Assumption 3.2, we verify that the mapping Φ is C2 in Lemma B.1. In
Appendix B.1 we discuss how different time scalings could affect the coefﬁcients in SDE (2)
and (3). Then we check the necessary conditions for applying the results in Katzenberger (1991)
in Appendix B.2 and recap the corresponding theorem for the asymptotically continuous case
in Appendix B.3. Finally, we provide a user-friendly interface for Katzenberger’s theorem in
Appendix B.4."
REFERENCES,0.23971518987341772,"Lemma B.1 (Implication of Falconer (1983)). Under Assumption 3.2, Φ is C2 on U."
REFERENCES,0.24050632911392406,"Proof of Lemma B.1. Applying Theorem 5.1 of Falconer (1983) with f(·) = φ(·, 1) sufﬁces."
REFERENCES,0.2412974683544304,Published as a conference paper at ICLR 2022
REFERENCES,0.24208860759493672,"B.1
APPROXIMATING SGD BY SDE"
REFERENCES,0.24287974683544303,"Let’s ﬁrst clarify how we derive the SDEs, (2) and (3), that approximate SGD (1) under different
time scalings. Recall W(t) is Ξ-dimensional Brownian motion and that σ(X) : RD →RD×Ξ is a
deterministic noise function. As proposed by Li et al. (2017), one approach to approximate SGD (1)
by SDE is to consider the following SDE:"
REFERENCES,0.24367088607594936,"dX(t) = −∇L(X(t))dt + √ησ(X(t))dW(t),"
REFERENCES,0.2444620253164557,"where the time correspondence is t = kη, i.e., X(kη) ≈xη(k)."
REFERENCES,0.24525316455696203,"Now rescale the above SDE by considering ˜X(t) = X(tη), which then yields"
REFERENCES,0.24604430379746836,"d ˜X(t) = dX(tη) = −∇L(X(tη))d(tη) + √ησ(X(tη))dW(tη)
= −η∇L(X(tη))dt + √ησ(X(tη))dW(tη)."
REFERENCES,0.2468354430379747,"Now we deﬁne W ′(t) =
1
√ηW(tη), and it’s easy to verify that W ′(t) is also a Ξ-dimensional"
REFERENCES,0.247626582278481,"brownian motion, which means W ′ d= W, i.e., W and W ′ have the same sample paths in CRd[0, ∞).
Thus"
REFERENCES,0.24841772151898733,d ˜X(t) = −η∇L(X(tη))dt + ησ(X(tη))dW ′(t)
REFERENCES,0.24920886075949367,"= −η∇L( ˜X(t))dt + ησ( ˜X(t))dW ′(t),"
REFERENCES,0.25,"where the time correspondence is t = k, i.e., ˜X(k) ≈xη(k). The above SDE is exactly the same
as (2)."
REFERENCES,0.25079113924050633,"Then, to accelerate the above SDE by η−2 times, let’s deﬁne ¯X(t) = ˜X(t/η2). Then it follows that"
REFERENCES,0.25158227848101267,d ¯X(t) = d ˜X(t/η2) = −η∇L( ˜X(t/η2))dt/η2 + ησ( ˜X(t/η2))dW(t/η2) = −1
REFERENCES,0.252373417721519,"η ∇L( ¯X(t))dt + σ( ¯X(t))d
 
ηW(t/η2)
"
REFERENCES,0.25316455696202533,"Again note that ηW(t/η2)
d= W(t) in sample paths and thus is also a Ξ-Brownian motion. Here the
time correspondence is t = kη2, i.e., evolving for constant time with the above SDE approximates
Ω(1/η2) steps of SGD. In this way, we derive SDE (3) in the main context."
REFERENCES,0.25395569620253167,"B.2
NECESSARY CONDITIONS"
REFERENCES,0.254746835443038,"Below we collect the necessary conditions imposed on {Zn}n≥1 and {An}n≥1 in Katzenberger
(1991). Recall that we consider the following stochastic process"
REFERENCES,0.2555379746835443,"Xn(t) = X(0) +
Z t"
REFERENCES,0.2563291139240506,"0
σ(Xn(s))dZn(s) −
Z t"
REFERENCES,0.25712025316455694,"0
∇L(Xn(s))dAn(s)."
REFERENCES,0.2579113924050633,"For any stopping time τ, the stopped process is deﬁned as Xτ
n(t) = Xn(t ∧τ). For any compact
K ⊂U, we deﬁne the stopping time of Xn leaving K as λn(K) = inf{t ≥0 | Xn(t−) /∈
˚
K or Xn(t) /∈˚
K}.
Condition B.2. The integrator sequence {An}n≥1 is asymptotically continuous: sup
t>0
|An(t) −"
REFERENCES,0.2587025316455696,"An(t−)| ⇒0 where An(t−) = lims→t−An(s) is the left limit of An at t.
Condition B.3. The integrator sequence {An}n≥1 increases inﬁnitely fast: ∀ϵ > 0, inf
t≥0(An(t + ϵ) −"
REFERENCES,0.25949367088607594,"An(t)) ⇒∞.
Condition B.4 (Eq.(5.1), Katzenberger 1991). For every T > 0, as n →∞, it holds that"
REFERENCES,0.2602848101265823,"sup
0<t≤T ∧λn(K)
∥∆Zn(t)∥2 ⇒0."
REFERENCES,0.2610759493670886,"Condition B.5 (Condition 4.2, Katzenberger 1991). For each n ≥1, let Yn be a {Fn
t }-
semimartingale with sample paths in DRD[0, ∞). Assume that for some δ > 0 (allowing δ = ∞) and
every n ≥1 there exist stopping times {τ m
n
| m ≥1} and a decomposition of Yn −Jδ(Yn)"
REFERENCES,0.26186708860759494,Published as a conference paper at ICLR 2022
REFERENCES,0.2626582278481013,"into a local martingale Mn plus a ﬁnite variation process Fn such that P[τ m
n
≤m] ≤1/m,
{[Mn](t ∧τ m
n ) + Tt∧τ m
n (Fn)}n≥1 is uniformly integrable for every t ≥0 and m ≥1, and"
REFERENCES,0.2634493670886076,"lim
γ→0 lim sup
n→∞P

sup
0≤t≤T
(Tt+γ(Fn) −Tt(Fn)) > ϵ

= 0,"
REFERENCES,0.26424050632911394,"for every ϵ > 0 and T > 0, where Tt(·) denotes total variation on the interval [0, t].
Lemma B.6. For SGD iterates deﬁned using the notation in Lemma 4.2, the sequences {An}n≥1
and {Zn}n≥1 satisfy Condition B.2, B.3, B.4 and B.5."
REFERENCES,0.2650316455696203,Proof of Lemma B.6. Condition B.2 is obvious from the deﬁnition of {An}n≥1.
REFERENCES,0.26582278481012656,"Next, for any ϵ > 0 and t ∈[0, T], we have"
REFERENCES,0.2666139240506329,"An(t + ϵ) −An(t) = ηn ·
t + ϵ η2n"
REFERENCES,0.2674050632911392,"
−ηn ·
 t η2n"
REFERENCES,0.26819620253164556,"
≥t + ϵ −η2
n
ηn
−t"
REFERENCES,0.2689873417721519,"ηn
= ϵ −η2
n
ηn
,"
REFERENCES,0.2697784810126582,"which implies that inf0≤t≤T (An(t + ϵ) −An(t)) > ϵ/(2ηn) for small enough ηn. Then taking
n →∞yields the Condition B.3."
REFERENCES,0.27056962025316456,"For Condition B.4, note that"
REFERENCES,0.2713607594936709,"∆Zn(t) =
ηn
√"
REFERENCES,0.2721518987341772,Ξ(1ξk −1
REFERENCES,0.27294303797468356,"Ξ1)
if t = k · η2
n,
0
otherwise."
REFERENCES,0.2737341772151899,"Therefore, we have ∥∆Zn(t)∥2 ≤2ηn
√"
REFERENCES,0.2745253164556962,"Ξ for all t > 0. This implies that ∥∆Zn(t)∥2 →0 uniformly
over t > 0 as n →∞, which veriﬁes Condition B.4."
REFERENCES,0.27531645569620256,"We proceed to verify Condition B.5. By the deﬁnition of Zn, we know that {Zn(t)}t≥0 is a
jump process with independent increments and thus is a martingale. Therefore, by decomposing
Zn = Mn + Fn with Mn being a local martingale and Fn a ﬁnite variation process, we must have
Fn = 0 and Mn is Zn itself. It then sufﬁces to show that [Mn](t ∧τ m
n ) is uniformly integrable for
every t ≥0 and m ≥1. Since Mn is a pure jump process, we have"
REFERENCES,0.27610759493670883,"[Mn](t ∧τ m
n ) =
X"
REFERENCES,0.27689873417721517,"0<s≤t∧τ m
n
∥∆Mn(s)∥2
2 ≤
X"
REFERENCES,0.2776898734177215,"0<s≤t
∥∆Mn(s)∥2
2 ="
REFERENCES,0.27848101265822783,"⌊t/η2
n⌋
X k=1 ηn
√"
REFERENCES,0.27927215189873417,"Ξ

1ξk −1 Ξ1
 2 2
≤4Ξ"
REFERENCES,0.2800632911392405,"⌊t/η2
n⌋
X"
REFERENCES,0.28085443037974683,"k=1
η2
n ≤4Ξt."
REFERENCES,0.28164556962025317,"This implies that [Mη](t ∧τ m
η ) is universally bounded by 4t, and thus [Mη](t ∧τ m
η ) is uniformly
integrable. This completes the proof."
REFERENCES,0.2824367088607595,"Lemma 4.2. Let {ηn}∞
n=1 be any positive sequence with limn→∞ηn = 0, An(t) = ηn⌊t/η2
n⌋,"
REFERENCES,0.28322784810126583,"and Zn(t) = ηn
P⌊t/η2
n⌋
k=1
√"
REFERENCES,0.28401898734177217,Ξ(1ξk −1
REFERENCES,0.2848101265822785,"Ξ1), where ξ1, ξ2, . . .
i.i.d.
∼Unif([Ξ]). Then with the same
initialization Xn(0) = xηn(0) ≡X(0), Xn(kη2
n) deﬁned by (6) is a Katzenberger process and is
equal to xηn(k) deﬁned in (1) with LR equal to ηn for all k ≥1. Moreover, the counterpart of (7) is"
REFERENCES,0.28560126582278483,"Y (t) = Φ(X(0)) +
Z t"
REFERENCES,0.28639240506329117,"0
∂Φ(Y )σ(Y )dW(s) + 1 2 Z t"
REFERENCES,0.28718354430379744,"0
∂2Φ(Y )[Σ(Y )]ds,
(8)"
REFERENCES,0.2879746835443038,where Σ ≡σσ⊤and {W(t)}t≥0 is a Ξ-dimensional standard Brownian motion.
REFERENCES,0.2887658227848101,"Proof of Lemma 4.2. For any n ≥1, it sufﬁces to show that given Xn(kη2
n) = xηn(k), we further
have Xn((k + 1)η2
n) = xηn(k + 1). By the deﬁnition of Xn(t), we have"
REFERENCES,0.28955696202531644,"Xn((k + 1)η2
n) −Xn(kη2
n)"
REFERENCES,0.2903481012658228,"= −
Z (k+1)η2
n"
REFERENCES,0.2911392405063291,"kη2n
∇L(Xn(t))dAn(t) +
Z (k+1)η2
n"
REFERENCES,0.29193037974683544,"kη2n
σ(Xn(t))dZn(t)"
REFERENCES,0.2927215189873418,"= −∇L(Xn(kη2
n))(An((k + 1)η2
n) −An(kη2
n)) + σ(Xn(kη2
n))(Zn((k + 1)η2
n) −Zn(kη2
n))"
REFERENCES,0.2935126582278481,"= −ηn∇L(Xn(kη2
n)) + ηn
√"
REFERENCES,0.29430379746835444,"Ξσξk(Xn(kη2
n))"
REFERENCES,0.2950949367088608,"= −ηn∇L(xηn(k)) + ηn
√"
REFERENCES,0.2958860759493671,Ξσξk(xηn(k)) = xηn(k + 1) −xηn(k)
REFERENCES,0.29667721518987344,Published as a conference paper at ICLR 2022
REFERENCES,0.2974683544303797,"where the second equality is because An(t) and Zn(t) are constant on interval [kη2
n, (k +1)η2
n). This
conﬁrms the alignment between {Xn(kη2
n)}k≥1 and {xηn(k)}k≥1."
REFERENCES,0.29825949367088606,"For the second claim, note that σ(x)EZn(t) ≡0 for all x ∈RD, t ≥0 (since the noise has zero-
expectation) and that {Zn(t) −EZn(t)}t≥0 will converge in distribution to a Brownian motion by
the classic functional central limit theorem (see, for example, Theorem 4.3.5 in Whitt (2002)). Thus,
the limiting diffusion of Xn as n →∞can be obtained by substituting Z with the standard Brownian
motion W in (22). This completes the proof."
REFERENCES,0.2990506329113924,"B.3
KATZENBERGER’S THEOREM FOR ASYMPTOTICALLY CONTINUOUS CASE"
REFERENCES,0.2998417721518987,"The full Katzenberger’s theorem deals with a more general case, which only requires the sequence of
intergrators to be asymptotically continuous, thus including SDE (3) and SGD (1) with η goes to 0."
REFERENCES,0.30063291139240506,"To describe the results in Katzenberger (1991), we ﬁrst introduce some deﬁnitions. For each n ≥1, let
(Ωn, Fn, {Fn
t }t≥0, P) be a ﬁltered probability space, Zn an Re-valued cadlag {Fn
t }-semimartingale
with Zn(0) = 0 and An a real-valued cadlag {Fn
t }-adapted nondecreasing process with An(0) = 0.
Let σn : U →M(D, e) be continuous with σn →σ uniformly on compact subsets of U. Let Xn be
an RD-valued cadlag {Fn
t }-semimartingale satisfying, for all compact K ⊂U,"
REFERENCES,0.3014240506329114,"Xn(t) = X(0) +
Z t"
REFERENCES,0.3022151898734177,"0
σ(Xn)dZn +
Z t"
REFERENCES,0.30300632911392406,"0
−∇L(Xn)dAn
(21)"
REFERENCES,0.3037974683544304,"for all t ≤λn(K) where λn(K) = inf{t ≥0 | Xn(t−) /∈˚
K or Xn(t) /∈˚
K} is the stopping time
of Xn leaving K.
Theorem B.7 (Theorem 6.3, Katzenberger 1991). Suppose X(0) ∈U, Assumptions 3.1 and 3.2,
Condition B.2, B.3, B.4 and B.5 hold. For any compact K ⊂U, deﬁne µn(K) = inf{t ≥
0 | Yn(t−) /∈˚
K or Yn(t) /∈˚
K}, then the sequence {(Y µn(K)
n
, Zµn(K)
n
, µn(K)} is relatively
compact in DRD×e[0, ∞) × [0, ∞). If (Y, Z, µ) is a limit point of this sequence under the skorohod
metric (Deﬁnition A.4), then (Y, Z) is a continuous semimartingale, Y (t) ∈Γ for every t ≥0 a.s.,
µ ≥inf{t ≥0 | Y (t) /∈˚
K} a.s. and Y (t) admits"
REFERENCES,0.3045886075949367,"Y (t) = Y (0) +
Z t∧µ"
REFERENCES,0.30537974683544306,"0
∂Φ(Y (s))σ(Y (s))dZ(s) + 1 2 D
X i,j=1 e
X k,l=1 Z t∧µ"
REFERENCES,0.3061708860759494,"0
∂ijΦ(Y (s))σ(Y (s))ikσ(Y (s))jld[Zk, Zl](s).
(22)"
REFERENCES,0.3069620253164557,"We note that by Lemma A.15, convergence in distribution under skorohod metric is equivalent to
convergence in distribution under uniform metric Deﬁnition A.14, therefore in the rest of the paper
we will only use the uniform metric in the rest of the paper, e.g., whenever we mention Prohorov
metric and δ-Prohorov distance, the underlying metric is the uniform metric."
REFERENCES,0.307753164556962,"B.4
A USER-FRIENDLY INTERFACE FOR KATZENBERGER’S THEOREM"
REFERENCES,0.30854430379746833,"Based on the Lemma B.6, we can immediately apply Theorem B.7 to obtain the following limiting
diffusion of SGD.
Theorem B.8. Let the manifold Γ and its open neighborhood U satisfy Assumptions 3.1 and 3.2. Let
K ⊂U be any compact set and ﬁx some x0 ∈K. Consider the SGD formulated in Lemma 4.2 where
Xηn(0) ≡x0. Deﬁne
Yηn(t) = Xηn(t) −φ(Xηn(0), Aηn(t)) + Φ(Xηn(0))"
REFERENCES,0.30933544303797467,"and µηn(K) = min{t ∈N | Yηn(t) /∈˚
K}. Then the sequence {(Y µηn(K)
ηn
, Zηn, µηn(K))}n≥1 is
relatively compact in DRD×Rn[0, ∞) × [0, ∞]. Moreover, if (Y, Z, µ) is a limit point of this sequence,
it holds that Y (t) ∈Γ a.s for all t ≥0, µ ≥inf{t ≥0 | Y (t) /∈˚
K} and Y (t) admits"
REFERENCES,0.310126582278481,"Y (t) =
Z t∧µ"
REFERENCES,0.31091772151898733,"s=0
∂Φ(Y (s))σ(Y (s))dW(s) +
Z t∧µ s=0 1
2 D
X"
REFERENCES,0.31170886075949367,"i,j=1
∂ijΦ(Y (s))(σ(Y (s))σ(Y (s))⊤)ijds (23)"
REFERENCES,0.3125,Published as a conference paper at ICLR 2022
REFERENCES,0.31329113924050633,where {W(s)}s≥0 is the standard Brownian motion and σ(·) is as deﬁned in Lemma 4.2.
REFERENCES,0.31408227848101267,"However, the above theorem is hard to parse and cannot be directly applied if we want to fur-
ther study the implicit bias of SGD through this limiting diffusion. Therefore, we develop a
user-friendly interface to it in below. In particular, Theorem 4.6 is the a special case of Theo-
rem B.9. In Theorem 4.6, we replace ∂Φ(Y (t))σ(Y (t)) with Σ"
REFERENCES,0.314873417721519,"1
2
∥(Y (t)) to simplify the equation,"
REFERENCES,0.31566455696202533,"since ∂Φ(Y (t))σ(Y (t)) (∂Φ(Y (t))σ(Y (t)))⊤= Σ∥(Y (t)) and thus this change doesn’t affect the
distribution of the sample paths of the solution.
Theorem B.9. Under the same setting as Theorem B.8, we change the integer index back to η > 0
with a slight abuse of notation. For any stopping time µ and stochastic process {Y (t)}t≥0 such that
µ ≥inf{t ≥0 | Y (t) /∈˚
K}, Y (0) = Φ(x0) and that (Y, µ) satisfy Equation (23) for some standard
Brownian motion W. For any compact set K ⊆U and T > 0, deﬁne µ(K) = inf{t ≥0 | Y (t) /∈
˚
K} and δ = P(µ(K) ≤T). Then for any ϵ > 0, it holds for all sufﬁciently small LR η that:"
REFERENCES,0.31645569620253167,"ρ2δ(Y µη(K)∧T
η
, Y µ(K)∧T ) ≤ϵ,
(24)"
REFERENCES,0.317246835443038,"which means there is a coupling between the distribution of the stopped processes Y µη(K)∧T
η
and
Y µ(K)∧T , such that the uniform metric between them is smaller than ϵ with probability at least 1−2δ.
In other words, limη→0 ρ2δ(Y µη(K)∧T
η
, Y µ(K)∧T ) = 0."
REFERENCES,0.3180379746835443,"Moreover, when {Y (t)}t≥0 is a global solution to the following limiting diffusion"
REFERENCES,0.3188291139240506,"Y (t) =
Z t"
REFERENCES,0.31962025316455694,"s=0
∂Φ(Y (s))σ(Y (s))dW(s) +
Z t s=0 1
2 D
X"
REFERENCES,0.3204113924050633,"i,j=1
∂ijΦ(Y (s))(σ(Y (s))σ(Y (s))⊤)ijds"
REFERENCES,0.3212025316455696,"and Y never leaves U, i.e. P[∀t ≥0, Y (t) ∈U] = 1, it holds that Y T
η converges in distribution to
Y T as η →0 for any ﬁxed T > 0."
REFERENCES,0.32199367088607594,"For clarity, we break the proof of Theorem B.9 into two parts, devoted to the two claims respectively."
REFERENCES,0.3227848101265823,"Proof of the ﬁrst claim of Theorem B.9. First, Theorem B.8 guarantees there exists a stopping time
˜µ and a stochastic process {eY (t)}t≥0 such that"
REFERENCES,0.3235759493670886,"1. (eY , ˜µ) satisﬁes Equation (23);"
REFERENCES,0.32436708860759494,2. eY ∈Γ a.s.;
REFERENCES,0.3251582278481013,"3. ˜µ ≥˜µ(K) := inf{t ≥0 | eY (t) /∈˚
K}."
REFERENCES,0.3259493670886076,"The above conditions imply that eY ˜µ(K) ∈Γ a.s.. Since the coefﬁcients in Equation (23) are locally
Lipschitz, we claim that (eY ˜µ(K), ˜µ(K))
d= (Y µ(K), µ(K)). To see this, note that for any compact
K ⊆U, the noise function σ, ∂Φ and ∂2Φ are all Lipschitz on K, thus we can extend their deﬁnitions
to RD such that the resulting functions are still locally Lipschitz. Based on this extension, applying
classic theorem on weak uniqueness (e.g., Theorem 1.1.10, Hsu 2002) to the extended version of
Equation (23) yields the equivalence in law. Thus we only need to prove the ﬁrst claim for eY ."
REFERENCES,0.32674050632911394,"Let ET be the event such that ˜µ(K) > T on ET . Then restricted on ET , we have eY (T ∧˜µ) =
eY (T ∧˜µ(K)) as ˜µ ≥˜µ(K) holds a.s. We ﬁrst prove the claim for any convergent subsequence of
{Yη}η>0."
REFERENCES,0.3275316455696203,"Now, let {ηm}m≥1 be a sequence of LRs such that ηm →0 and Y µηm(K)
ηm
⇒eY ˜µ as m →∞.
By applying the Skorohod representation theorem, we can put {Yηm}m≥1 and eY under the same
probability space such that Y µηm(K)
ηm
→eY ˜µ a.s. in the Skorohod metric, or equivalently the uniform
metric (since eY ˜µ is continuous) i.e.,"
REFERENCES,0.32832278481012656,"dU(Y µηm(K)
ηm
, eY ˜µ) →0, a.s.,"
REFERENCES,0.3291139240506329,Published as a conference paper at ICLR 2022
REFERENCES,0.3299050632911392,"which further implies that for any ϵ > 0, there exists some N > 0 such that for all m > N,"
REFERENCES,0.33069620253164556,"P
h
dU(Y µηm(K)∧T
ηm
, eY ˜µ∧T ) ≥ϵ
i
≤δ."
REFERENCES,0.3314873417721519,"Restricted on ET , we have dU(Y µηm(K)∧T
ηm
, eY ˜µ∧T ) = dU(Y µηm(K)∧T
ηm
, eY ˜µ(K)∧T ), and it follows
that for all m > N,"
REFERENCES,0.3322784810126582,"P
h
dU(Y µηm(K)∧T
ηm
, eY ˜µ(K)∧T ) ≥ϵ
i
≤P
h
{dU(Y µηm(K)∧T
ηm
, eY ˜µ(K)∧T ) ≥ϵ} ∩ET
i
+ P [Ec
T ]"
REFERENCES,0.33306962025316456,"= P
h
{dU(Y µηm(K)∧T
ηm
, eY ˜µ∧T ) ≥ϵ} ∩ET
i
+ P[Ec
T ]"
REFERENCES,0.3338607594936709,"≤P
h
dU(Y µηm(K)∧T
ηm
, eY ˜µ∧T ) ≥ϵ
i
+ P[Ec
T ] ≤2δ."
REFERENCES,0.3346518987341772,"By
the
deﬁnition
of
the
Prohorov
metric
in
Deﬁnition
A.12,
we
then
get
ρ2δ(Y µηm(K)∧T
ηm
, eY ˜µ(K)∧T ) ≤ϵ for all m > N. Therefore, we have"
REFERENCES,0.33544303797468356,"lim
m→∞ρ2δ(Y µηm(K)∧T
ηm
, eY ˜µ(K)∧T ) = 0."
REFERENCES,0.3362341772151899,"Now we claim that it indeed holds that limη→0 ρ2δ(Y µη(K)∧T
η
, eY ˜µ(K)∧T ) = 0. We prove this
by contradiction. Suppose otherwise, then there exists some ϵ > 0 such that for all η0 > 0,
there exists some η < η0 with ρ2δ(Y µη(K)∧T
η
, eY ˜µ(K)∧T ) > ϵ.
Consequently, there is a se-
quence {ηm}m≥1 satisfying limm→∞ηm = 0 and ρ2δ(Y µηm(K)
ηm
, eY ˜µ(K)∧T ) > ϵ for all m. Since
{(Y µηm(K)∧T
ηm
, Zηm, µηm(K))}m≥1 is relatively compact, there exists a subsequence (WLOG, as-
sume it is the original sequence itself) converging to (eY ˜µ∧T , W, ˜µ) in distribution. However, repeating
the exactly same argument as above, we would have ρ2δ(Y µηm(K)∧T
ηm
, eY ˜µ(K)∧T ) ≤ϵ for all sufﬁ-
ciently large m, which is a contradiction. This completes the proof."
REFERENCES,0.3370253164556962,"Proof of the second claim of Theorem B.9. We will ﬁrst show there exists a sequence of compact
set {Km}m≥1 such that ∪∞
m=1Km = U and Km ⊆Km+1. For m ∈N+, we deﬁne Hm =
U \(B1/m(0)+RD \U) and Km = Hm ∩Bm(0). By deﬁnition it holds that ∀m < m′, Hm ⊆Hm′
and Km ⊆Km′. Moreover, since Km is bounded and closed, Km is compact for every m. Now
we claim ∪∞
m=1Km = U. Note that ∪∞
m=1Km = ∪∞
m=1Hm ∩Bm(0) = ∪∞
m=1Hm. ∀x ∈U,
since U is open, we know dU(x, RD \ U) > 0, thus there exists m0 ∈N+, such that ∀m ≥m0,
x /∈(B1/m(0) + RD \ U) and thus x ∈Hm, which implies x ∈∪∞
m=1Hm. On the other hand,
∀x ∈RD \ U, it holds that x ∈(B1/m(0) + RD \ U) for all m ∈N+, thus x /∈Hm ⊂Km."
REFERENCES,0.33781645569620256,"Therefore, since Y ∈U and is continuous almost surely, random variables limm→∞µ(Km) = ∞
a.s., which implies µ(Km) converges to ∞in distribution, i,e,, ∀δ > 0, T > 0, ∃m ∈N+, such that
∀K ⊇Km, it holds P[µ(K) ≤T] ≤δ."
REFERENCES,0.33860759493670883,"Now we will show for any T > 0 and ϵ > 0, there exists η0 such that ρϵ(Y T , Y T
η ) ≤ϵ for all
η ≤η0. Fixing any T > 0, for any ϵ > 0, let δ = ϵ"
REFERENCES,0.33939873417721517,"4, then from above we know exists compact
set K, such that P(µ(K) ≤T) ≤δ. We further pick K′ = K + B2ϵ′(0), where ϵ′ can be any real
number satisfying 0 < ϵ′ < ϵ and K′ ⊆U. Such ϵ′ exists since U is open. Note K ⊆K′, we have
P(µ(K′) ≤T) ≤P(µ(K) ≤T) ≤δ. Thus by the ﬁrst claim of Theorem B.9, there exists η0 > 0,
such that for all η ≤η0, we have ρ2δ(Y µη(K′)∧T
η
, Y µ(K′)∧T ) ≤2−⌈T ⌉ϵ′."
REFERENCES,0.3401898734177215,"Note that ρδ(Y µ(K)∧T , Y µ(K′)∧T ) = 0, so we have for all η ≤η0,"
REFERENCES,0.34098101265822783,"ρ3δ(Y µ(K)∧T , Y µη(K′)∧T
η
) ≤2−⌈T ⌉ϵ′."
REFERENCES,0.34177215189873417,"By the deﬁnition of δ-Prohorov distance in Deﬁnition A.11, we can assume (Y µ(K)∧T , Y µη(K′)∧T
η
)
is already the coupling such that P
h
dU(Y µ(K)∧T , Y µη(K′)∧T
η
) ≥2−⌈T ⌉ϵ′i
≤3δ. Below we want"
REFERENCES,0.3425632911392405,"to show ρ3δ(Y µ(K)∧T , Y T
η ) ≤2−⌈T ⌉ϵ′. Note that for all t ≥0, Y µ(K)∧T (t) ∈K, thus we know if"
REFERENCES,0.34335443037974683,Published as a conference paper at ICLR 2022
REFERENCES,0.34414556962025317,"µη(K′) ≤T, then"
REFERENCES,0.3449367088607595,"dU(Y µ(K)∧T , Y µη(K′)∧T
η
) ≥2−⌈T ⌉Y µ(K)∧T (µη(K′)) −Y µη(K′)∧T
η
(µη(K′)

2
≥2−⌈T ⌉dU(K, Rd/K′)"
REFERENCES,0.34572784810126583,≥2−⌈T ⌉ϵ′.
REFERENCES,0.34651898734177217,"On the other hand, if µη(K′) > T, then Y T
η
= Y µη(K′)∧T
η
.
Thus we can conclude that"
REFERENCES,0.3473101265822785,"dU(Y µ(K)∧T , Y T
η ) ≥2−⌈T ⌉ϵ′ implies dU(Y µ(K)∧T , Y µη(K′)∧T
η
) ≥2−⌈T ⌉ϵ′. Therefore, we further
have"
REFERENCES,0.34810126582278483,"P
h
dU(Y µ(K)∧T , Y T
η ) ≥2−⌈T ⌉ϵ′i
≤P
h
dU(Y µ(K)∧T , Y µη(K′)∧T
η
) ≥2−⌈T ⌉ϵ′i
≤3δ,"
REFERENCES,0.34889240506329117,"that is,"
REFERENCES,0.34968354430379744,"ρ3δ(Y µ(K)∧T , Y T
η ) ≤2−⌈T ⌉ϵ′."
REFERENCES,0.3504746835443038,"Finally, since ρδ(Y T , Y µ(K)∧T ) = 0, we have for all η ≤η0,"
REFERENCES,0.3512658227848101,"ρϵ(Y T , Y T
η ) = ρ4δ(Y T , Y T
η ) ≤ρ3δ(Y µ(K)∧T , Y T
η ) + ρδ(Y T , Y µ(K)∧T ) ≤2−⌈T ⌉ϵ′ + 0 ≤ϵ,"
REFERENCES,0.35205696202531644,which completes the proof.
REFERENCES,0.3528481012658228,"Now, we provide the proof of Theorem 4.6 as a direct application of Theorem B.9."
REFERENCES,0.3536392405063291,"Proof of Theorem 4.6. We ﬁrst prove that Y never leaves Γ, i.e., P[Y (t) ∈Γ, ∀t ≥0] = 1. By the
result of Theorem B.8, we know that for each compact set K ⊂Γ, Y µ(K) stays on Γ almost surely,
where µ(K) := inf{t ≥0 | eY (t) /∈˚
K} is the earliest time that Y leaves K. In other words, for
all compact set K ⊂Γ, P[∃t ≥0, Y (t) /∈Γ, Y (t) ∈K] = 0. Let {Km}m≥1 be any sequence of
compact sets such that ∪m≥1Km = U and Km ⊂U, e.g., the ones constructed in the proof of the
second claim of Theorem B.9. Therefore, we have"
REFERENCES,0.35443037974683544,"P[∃t ≥0, Y (t) /∈Γ] = P[∃t ≥0, Y (t) /∈Γ, Y (t) ∈U] ≤ ∞
X"
REFERENCES,0.3552215189873418,"m=1
P[∃t ≥0, Y (t) /∈Γ, Y (t) ∈Km] = 0,"
REFERENCES,0.3560126582278481,which means Y always stays on Γ.
REFERENCES,0.35680379746835444,"Then recall the decomposition of Σ = Σ∥+ Σ⊥+ Σ∥,⊥+ Σ⊥,∥as deﬁned in Lemma 4.5. Since Y
never leaves Γ, by Lemma 4.5, we can rewrite Equation (10) as"
REFERENCES,0.3575949367088608,"dY (t) = Σ1/2
∥
dW(t) + ∂2Φ(Y (t))[Σ(Y (t))]dt"
REFERENCES,0.3583860759493671,"= ∂Φ(Y (t))σ(Y (t))dW(t) + 1 2 D
X"
REFERENCES,0.35917721518987344,"i,j=1
∂ijΦ(Y (t))(σ(Y (t))σ(Y (t))⊤)ijdt"
REFERENCES,0.3599683544303797,"where the second equality follows from the deﬁnition that Σ∥= ∂ΦΣ∂Φ = ∂Φσσ⊤∂Φ. This
coincides with the formulation of the limiting diffusion in Theorem B.9. Therefore, further combining
Lemma 4.2 and the second part of Theorem B.9, we obtain the desired result."
REFERENCES,0.36075949367088606,"Remark B.10. Our result suggests that for tiny LR η, SGD dynamics have two phases. In Phase I of
Θ(1/η) steps, the SGD iterates move towards the manifold Γ of local minimizers along GF. Then
in Phase II which is of Θ(1/η2) steps, the SGD iterates stay close to Γ and diffuse approximately
according to (10). See Figure 2 for an illustration of this two-phase dynamics. However, since the
length of Phase I gets negligible compared to that of Phase II when η →0, Theorem 4.6 only reﬂects
the time scaling of Phase II."
REFERENCES,0.3615506329113924,"C
EXPLICIT FORMULA OF THE LIMITING DIFFUSION"
REFERENCES,0.3623417721518987,"In this section, we demonstrate how to compute the derivatives of Φ by relating to those of the loss
function L, and then present the explicit formula of the limiting diffusion."
REFERENCES,0.36313291139240506,Published as a conference paper at ICLR 2022
REFERENCES,0.3639240506329114,"Figure 2: Illustration for two-phase dynamics of SGD with the same example as in Figure 1 . Γ is an
1D manifold of minimizers of loss L."
REFERENCES,0.3647151898734177,"C.1
EXPLICIT EXPRESSION OF THE DERIVATIVES"
REFERENCES,0.36550632911392406,"For any x
∈
Γ, we choose an orthonormal basis of Tx(Γ) as {v1, . . . , vD−M}.
Let
{vD−M+1, . . . , vD} be an orthonormal basis of T ⊥
x (Γ) so that {vi}i∈[D] is an orthonormal basis of
RD.
Lemma C.1. For any x ∈Γ and any v ∈Tx(Γ), it holds that ∇2L(x)v = 0."
REFERENCES,0.3662974683544304,"Proof. For any x ∈Tx(Γ), let {x(t)}t≥0 be a parametrized smooth curve on Γ such that x(0) = x
and dx(t)"
REFERENCES,0.3670886075949367,"dt

t=0 = v. Then ∇L(xt) = 0 for all t. Thus 0 = d∇L(xt)"
REFERENCES,0.36787974683544306,"dt

t=0 = ∇2L(x)v."
REFERENCES,0.3686708860759494,"Lemma C.2. For any x ∈RD, it holds that ∂Φ(x)∇L(x) = 0 and"
REFERENCES,0.3694620253164557,"∂2Φ(x)[∇L(x), ∇L(x)] = −∂Φ(x)∇2L(x)∇L(x)."
REFERENCES,0.370253164556962,"Proof. Fixing any x ∈RD, let dx(t)"
REFERENCES,0.37104430379746833,"dt
= −∇L(x(t)) be initialized at x(0) = x. Since Φ(x(t)) =
Φ(x) for all t ≥0, we have"
REFERENCES,0.37183544303797467,"d
dtΦ(x(t)) = −∂Φ(x(t))∇L(x(t)) = 0."
REFERENCES,0.372626582278481,"Evaluating the above equation at t = 0 yields ∂Φ(x)∇L(x) = 0. Moreover, take the second order
derivative and we have
d2"
REFERENCES,0.37341772151898733,"dt2 Φ(xt) = −∂2Φ(x(t))
dx(t)"
REFERENCES,0.37420886075949367,"dt , ∇L(x(t))

−∂Φ(x(t))∇2L(x(t))dx(t)"
REFERENCES,0.375,"dt
= 0."
REFERENCES,0.37579113924050633,Evaluating at t = 0 completes the proof.
REFERENCES,0.37658227848101267,"Now we can prove Lemma 4.3, restated in below.
Lemma 4.3. For any x ∈Γ, ∂Φ(x) ∈RD×D is the projection matrix onto tangent space Tx(Γ)."
REFERENCES,0.377373417721519,"Proof of Lemma 4.3. For any v ∈Tx(Γ), let {v(t), t ≥0} be a parametrized smooth curve on Γ
such that v(0) = x and dv(t)"
REFERENCES,0.37816455696202533,"dt

t=0 = v. Since v(t) ∈Γ for all t ≥0, we have Φ(v(t)) = v(t), and
thus
dv(t) dt"
REFERENCES,0.37895569620253167,"t=0
= d"
REFERENCES,0.379746835443038,"dtΦ(v(t))

t=0
= ∂Φ(x)dv(t) dt t=0
."
REFERENCES,0.3805379746835443,This implies that ∂Φ(x)v = v for all v ∈Tx(Γ).
REFERENCES,0.3813291139240506,"Next, for any u ∈T ⊥
x (Γ) and t ≥0, consider expanding ∇L(x + t∇2L(x)†u) at t = 0:"
REFERENCES,0.38212025316455694,"∇L
 
x + t∇2L(x)†u

= ∇2L(x) · t∇2L(x)†u + o(t)"
REFERENCES,0.3829113924050633,= tu + o(t)
REFERENCES,0.3837025316455696,Published as a conference paper at ICLR 2022
REFERENCES,0.38449367088607594,"where the second equality follows from the assumption that ∇2L(x) is full-rank when restricted on
T ⊥
x (Γ). Then since ∂Φ is continuous, it follows that"
REFERENCES,0.3852848101265823,"lim
t→0
∂Φ(x + t∇2L(x)†u)∇L(x + t∇2L(x)†u)"
REFERENCES,0.3860759493670886,"t
= lim
t→0 ∂Φ(x + t∇2L(x)†)(u + o(1))"
REFERENCES,0.38686708860759494,= ∂Φ(x)u.
REFERENCES,0.3876582278481013,"By Lemma C.2, we have ∂Φ(x + t(∇2L(x))†u))∇L(x + t(∇2L(x))†u) = 0 for all t > 0, which
then implies that ∂Φ(x)u = 0 for all u ∈T ⊥
x (Γ)."
REFERENCES,0.3884493670886076,"Therefore, under the basis {vi, . . . , vN}, ∂Φ(x) is given by"
REFERENCES,0.38924050632911394,"∂Φ(x) =

ID−M
0
0
0"
REFERENCES,0.3900316455696203,"
∈RD×D,"
REFERENCES,0.39082278481012656,"that is, the projection matrix onto Tx(Γ)."
REFERENCES,0.3916139240506329,"Lemma C.3. For any x ∈Γ, it holds that ∂Φ(x)∇2L(x) = 0."
REFERENCES,0.3924050632911392,Proof. It directly follows from Lemma C.1 and Lemma 4.3.
REFERENCES,0.39319620253164556,"Next, we proceed to compute the second-order derivatives."
REFERENCES,0.3939873417721519,"Lemma C.4. For any x ∈Γ, u ∈RD and v ∈Tx(Γ), it holds that"
REFERENCES,0.3947784810126582,"∂2Φ(x)[v, u] = −∂Φ(x)∂2(∇L)(x)[v, ∇2L(x)†u] −∇2L(x)†∂2(∇L)(x)[v, ∂Φ(x)u]."
REFERENCES,0.39556962025316456,"Proof of Lemma C.4. Consider a parametrized smooth curve {v(t)}t≥0 on Γ such that v(0) = x and
dv(t)"
REFERENCES,0.3963607594936709,"dt

t=0 = v. We deﬁne P(t) = ∂Φ(v(t)), P ⊥(t) = ID −P(t) and H(t) = ∇2L(v(t)) for all
t ≥0. By Lemma C.1 and 4.3, we have"
REFERENCES,0.3971518987341772,"P ⊥(t)H(t) = H(t)P ⊥(t) = H(t),
(25)"
REFERENCES,0.39794303797468356,"Denote the derivative of P(t), P ⊥(t) and H(t) with respect to t as P ′(t), (P ⊥)′(t) and H′(t). Then
differentiating with respect to t, we have"
REFERENCES,0.3987341772151899,"(P ⊥)′(t)H(t) = H′(t) −P ⊥(t)H′(t) = P(t)H′(t).
(26)"
REFERENCES,0.3995253164556962,"Then combining (25) and (26) and evaluating at t = 0, we have"
REFERENCES,0.40031645569620256,"P ′(0)H(0) = −(P ⊥)′(0)H(0) = −P(0)H′(0)
(27)"
REFERENCES,0.40110759493670883,We can decompose P ′(0) and H(0) as follows
REFERENCES,0.40189873417721517,"P ′(0) =

P ′
11(0)
P ′
12(0)
P ′
21(0)
P ′
22(0)"
REFERENCES,0.4026898734177215,"
,
H(0) =

0
0
0
H22(0)"
REFERENCES,0.40348101265822783,"
,
(28)"
REFERENCES,0.40427215189873417,"where P ′
11(0) ∈R(D−M)×(D−M) and H22 is the hessian of L restricted on T ⊥
x (Γ). Also note that"
REFERENCES,0.4050632911392405,"P(0)H′(0)P ⊥(0) =

ID−M
0
0
0"
REFERENCES,0.40585443037974683," 
H′
11(0)
H′
12(0)
H′
21(0)
H′
22(0)"
REFERENCES,0.40664556962025317," 
0
0
0
IM "
REFERENCES,0.4074367088607595,"=

0
H′
12(0)
0
0 
,"
REFERENCES,0.40822784810126583,and thus by (28) we have
REFERENCES,0.40901898734177217,"P ′(0)H(0) =

0
P ′
12(0)H22(0)
0
P ′
22(0)H22(0)"
REFERENCES,0.4098101265822785,"
=

0
−H′
12(0)
0
0 
."
REFERENCES,0.41060126582278483,"This implies that we must have P ′
22(0) = 0 and P ′
12(0)H22(0) = H′
12(0). Similarly, by taking
transpose in (28), we also have H22(0)P ′
21(0) = −H′
21(0)."
REFERENCES,0.41139240506329117,Published as a conference paper at ICLR 2022
REFERENCES,0.41218354430379744,"It then remains to determine the value of P ′
11(0). Note that since P(t)P(t) = P(t), we have
P ′(t)P(t) + P(t)P ′(t) = P ′(t), evaluating at t = 0 yields"
REFERENCES,0.4129746835443038,"2P ′
11(0) = P ′
11(0)."
REFERENCES,0.4137658227848101,"Therefore, we must have P ′
11(0) = 0. Combining the above results, we obtain"
REFERENCES,0.41455696202531644,P ′(0) = −P(0)H′(0)H(0)† −H(0)†H′(0)P(0).
REFERENCES,0.4153481012658228,"Finally, recall that P(t) = ∂Φ(v(t)), and thus"
REFERENCES,0.4161392405063291,P ′(0) = d
REFERENCES,0.41693037974683544,"dt∂Φ(v(t))

t=0
= ∂2Φ(x)[v]."
REFERENCES,0.4177215189873418,"Similarly, we have H′(0) = ∂2(∇L)(x)[v], and it follows that"
REFERENCES,0.4185126582278481,∂2Φ(x)[v] = −∂Φ(x)∂2(∇L)(x)[v]∇2L(x)† −∇2L(x)†∂2(∇L)(x)[v]∂Φ(x).
REFERENCES,0.41930379746835444,"Lemma C.5. For any x ∈Γ and u ∈T ⊥
x (Γ), it holds that"
REFERENCES,0.4200949367088608,∂2Φ(x)[uu⊤+ ∇2L(x)†uu⊤∇2L(x)] = −∂Φ(x)∂2(∇L)(x)[∇2L(x)†uu⊤].
REFERENCES,0.4208860759493671,"Proof of Lemma C.5. For any u ∈T ⊥
x (Γ), we deﬁne u(t) = x + t∇2L(x)†u for t ≥0. By Taylor
approximation, we have"
REFERENCES,0.42167721518987344,"∇L(u(t)) = t∇2L(x)∇2L(x)†u + o(t) = tu + o(t)
(29) and"
REFERENCES,0.4224683544303797,"∇2L(u(t)) = ∇2L(x) + t∂2(∇L)(x)[∇2L(x)†u] + o(t).
(30)"
REFERENCES,0.42325949367088606,"Combine (29) and (30) and apply Lemma C.2, and it follows that"
REFERENCES,0.4240506329113924,"0 = ∂2Φ(u(t))[∇L(u(t)), ∇L(u(t))] + ∂Φ(u(t))∇2L(u(t))∇L(u(t))"
REFERENCES,0.4248417721518987,= t2∂2Φ(u(t))[u + o(1)](u + o(1)) + t2∂Φ(u(t))∂2(∇L)(x)[∇2L(x)†u](u + o(1))
REFERENCES,0.42563291139240506,+ t2 ∂Φ(u(t))
REFERENCES,0.4264240506329114,"t
∇2L(x)(u + o(1))"
REFERENCES,0.4272151898734177,= t2∂2Φ(u(t))[u + o(1)](u + o(1)) + t2∂Φ(u(t))∂2(∇L)(x)[∇2L(x)†u](u + o(1))
REFERENCES,0.42800632911392406,+ t2 ∂Φ(u(t)) −∂Φ(x)
REFERENCES,0.4287974683544304,"t
∇2L(x)(u + o(1))"
REFERENCES,0.4295886075949367,"where the last equality follows from Lemma C.3. Dividing both sides by t2 and letting t →0, we get"
REFERENCES,0.43037974683544306,∂2Φ(x)[u]u + ∂Φ(x)∂2(∇L)(x)[∇2L(x)†u]u + ∂2Φ(x)[∇2L(x)†u]∇2L(x)u = 0.
REFERENCES,0.4311708860759494,Rearranging the above equation completes the proof.
REFERENCES,0.4319620253164557,"With the notion of Lyapunov Operator in Deﬁnition 4.4, Lemma C.5 can be further simpliﬁed into
Lemma C.6.
Lemma C.6. For any x ∈Γ and Σ ∈span{uu⊤| u ∈T ⊥
x (Γ)},"
REFERENCES,0.432753164556962,"⟨∂2Φ(x), Σ⟩= −∂Φ(x)∂2(∇L)(x)[L−1
∇2L(x)(Σ)].
(31)"
REFERENCES,0.43354430379746833,"Proof of Lemma C.6. Let A = uu⊤+ ∇2L(x)†uu⊤∇2L(x) and B = ∇2L(x)†uu⊤. The key
observation is that A + A⊤= L∇2L(x)(B + B⊤). Therefore, by Lemma C.5, it holds that"
REFERENCES,0.43433544303797467,∂2Φ(x)[L∇2L(x)(B+B⊤)] = ∂2Φ(x)[A+A⊤] = 2∂Φ(x)∂2(∇L)(x)[B] = ∂Φ(x)∂2(∇L)(x)[B+B⊤].
REFERENCES,0.435126582278481,"Since ∇2L(x)† is full-rank when restricted to T ⊥
x (Γ), we have span{∇2L(x)†uu⊤+uu⊤∇2L(x)† |
u ∈T ⊥
x (Γ)} = span{uu⊤| u ∈T ⊥
x (Γ)}. Thus by the linearity of above equation, we can replace
B + B⊤by any Σ ∈span{uu⊤| u ∈T ⊥
x (Γ)}, resulting in the desired equation."
REFERENCES,0.43591772151898733,Then Lemma 4.5 directly follows from Lemma C.4 and C.5.
REFERENCES,0.43670886075949367,Published as a conference paper at ICLR 2022
REFERENCES,0.4375,"C.2
TANGENT NOISE COMPENSATION ONLY DEPENDENDS ON THE MANIFOLD ITSELF"
REFERENCES,0.43829113924050633,"Here we show that the second term of (10), i.e., the tangent noise compensation for the limiting
dynamics to stay on Γ, only depends on Γ itself.
Lemma C.7. For any x ∈Γ, suppose there exist a neighborhood Ux of x and two loss func-
tions L and L′ that deﬁne the same manifold Γ locally in Ux, i.e., Γ ∩Ux = {x | ∇L(x) =
0} = {x | ∇L′(x) = 0}. Then for any v ∈Tx(Γ), it holds that (∇2L(x))†∂2(∇L)(x) [v, v] =
(∇2L′(x))†∂2(∇L′)(x) [v, v]."
REFERENCES,0.43908227848101267,Proof of Lemma C.7. Let {v(t)}t≥0 be a smooth curve on Γ with v(0) = x and dv(t)
REFERENCES,0.439873417721519,"dt

t=0 = v.
Since v(t) stays on Γ, we have ∇L(v(t)) = 0 for all t ≥0. Taking derivative for two times yields
∂2(∇L)(v(t))[ dv(t)"
REFERENCES,0.44066455696202533,"dt , dv(t)"
REFERENCES,0.44145569620253167,dt ] + ∇2L(v(t)) d2v(t)
REFERENCES,0.442246835443038,"dt2
= 0. Evaluating it at t = 0 and multiplying both
sides by ∇2L(x)†, we get"
REFERENCES,0.4430379746835443,"∇2L(x)†∂2(∇L)(x) [v, v] = −∇2L(x)†∇2L(x)d2v(t) dt2"
REFERENCES,0.4438291139240506,"t=0
= −∂Φ(x)d2v(t) dt2 t=0
."
REFERENCES,0.44462025316455694,"Since ∂Φ(x) is the projection matrix onto Tx(Γ) by Lemma 4.3, it does not depend on L, so
analogously we also have ∇2L′(x)†∂2(∇L′)(x) [v, v] = −∂Φ(x) d2v(t)"
REFERENCES,0.4454113924050633,"dt2

t=0 as well. The proof is"
REFERENCES,0.4462025316455696,thus completed. Note that ∂Φ(x) d2v(t)
REFERENCES,0.44699367088607594,"dt2

t=0 is indeed the second fundamental form for v at x, and
the value won’t change if we choose another parametric smooth curve with a different second-order
time derivative. (See Chapter 6 in Do Carmo (2013) for a reference.)"
REFERENCES,0.4477848101265823,"C.3
PROOF OF RESULTS IN SECTION 5"
REFERENCES,0.4485759493670886,"Now we are ready to give the missing proofs in Section 5 which yield explicit formula of the limiting
diffusion for label noise and isotropic noise.
Corollary 5.1 (Limiting Diffusion for Isotropic Noise). If Σ ≡ID on Γ, SDE (10) is then"
REFERENCES,0.44936708860759494,dY (t) = ∂Φ(Y )dW + 1
REFERENCES,0.4501582278481013,"2∇2L(Y )†∂2(∇L)(Y ) [∂Φ(Y )] dt
|
{z
}
Brownian Motion on Manifold −1"
REFERENCES,0.4509493670886076,"2∂Φ(Y )∇(ln |∇2L(Y )|+)dt
|
{z
}
Normal Regularization (11)"
REFERENCES,0.45174050632911394,"where |∇2L(Y )|+ = limα→0
|∇2L(Y )+αID|
αD−rank(∇2L(Y )) is the pseudo-determinant of ∇2L(Y ). |∇2L(Y )|+ is
also equal to the sum of log of non-zero eigenvalue values of ∇2L(Y )."
REFERENCES,0.4525316455696203,"Proof of Corollary 5.1. Set Σ∥= ∂Φ, Σ⊥= ID −∂Φ and Σ⊥,∥= Σ∥,⊥= 0 in the decomposition
of Σ by Lemma 4.5, and we need to show ∂Φ∇(ln |Σ|+) = ∂2(∇L)[(∇2L)†]."
REFERENCES,0.45332278481012656,"Holbrook (2018) shows that the gradient of pseudo-inverse determinant satisﬁes ∇|A|+ = |A|+A†.
Thus we have for any vector v ∈RD,

v, ∇ln |∇2L|+

=
D
|∇2L|+∇2L"
REFERENCES,0.4541139240506329,"|∇2L|+
, ∂2(∇L)[v]
E
=

∇2L, ∂2(∇L)[v]

= ∂2(∇L)[v, ∇2L] =

v, ∂2(∇L)[(∇2L)†]

, which completes the proof."
REFERENCES,0.4549050632911392,"Corollary 5.2 (Limiting Flow for Label Noise). If Σ ≡c∇2L on Γ for some constant c > 0,
SDE (10) can be simpliﬁed into (13) where the regularization is from the noise in the normal space."
REFERENCES,0.45569620253164556,"dY (t) = −1/4 · ∂Φ(Y (t))∇tr[c∇2L(Y (t))]dt.
(13)"
REFERENCES,0.4564873417721519,"Proof of Corollary 5.2. Since Σ = c∇2L, here we have Σ⊥= Σ and Σ∥, Σ⊥,∥, Σ∥,⊥= 0. Thus it
sufﬁces to show that 2∂2(∇L)

L−1
∇2L(Σ⊥)

= ∇tr[∇2L]. Note that for any v ∈RD,"
REFERENCES,0.4572784810126582,"v⊤∇tr[∇2L] =

ID, ∂2(∇L)[v]

=

ID −∂Φ, ∂2(∇L)[v]

,
(32)"
REFERENCES,0.45806962025316456,"where the second equality is because the the tangent space of symmetric rank-n matrices at ∇2L is
{A∇2L + ∇2LA⊤| A ∈RD×D}, and every element in this tangent space has zero inner-product
with ∂Φ by Lemma 4.3. Also note that L−1
∇2L(∇2L) = 1"
REFERENCES,0.4588607594936709,"2(ID −∂Φ), thus

ID −∂Φ, ∂2(∇L)[v]

=
2

L−1
∇2L(∇2L), ∂2(∇L)[v]

= 2v⊤∂2(∇L)[L−1
∇2L(∇2L)]."
REFERENCES,0.4596518987341772,Published as a conference paper at ICLR 2022
REFERENCES,0.46044303797468356,"C.4
EXAMPLE: k-PHASE MOTOR"
REFERENCES,0.4612341772151899,"We also give an example with rigorous proof where the implicit bias induced by noise in the normal
space cannot be characterized by a ﬁxed regularizer, which was ﬁrst discovered by Damian et al.
(2021) but was only veriﬁed via experiments."
REFERENCES,0.4620253164556962,"Note the normal regularization in both cases of label noise and isotropic noise induces Riemmanian
gradient ﬂow against some regularizer, it’s natural to wonder if the limiting ﬂow induced by the
normal noise can always be characterized by certain regularizer. Interestingly, Damian et al. (2021)
answers this question negatively via experiments in their Section E.2. We adapt their example into
the following one, and rigorously prove the limiting ﬂow moves around a cycle at a constant speed
and never stops using our framework."
REFERENCES,0.46281645569620256,"Suppose dimension D = k + 2 ≥5. For each x ∈RD, we decompose x =
  x1:2
x3:D

where x1:2 ∈R2"
REFERENCES,0.46360759493670883,"and x3:D ∈RD−2. Let Qθ ∈R2×2 be the rotation matrix of angle θ, i.e., Qθ =
  cos θ −sin θ
sin θ
cos θ

and
the loss L(x) := 1"
REFERENCES,0.46439873417721517,"8(∥x1:2∥2
2 −1)2 + 1"
PD,0.4651898734177215,"2
PD
j=3(2 +

Qj−3
α
v, x1:2

x2
j, where α =
2π
D−2 and v is any
vector in R2 with unit norm. Here the manifold is given by Γ := {x | L(x) = 0} = {x ∈RD |
x2
1 + x2
2 = 1, xj = 0, ∀j = 3, . . . , D}."
PD,0.46598101265822783,"The basic idea is that we can add noise in the ‘auxiliary dimensions’ for j = 3, . . . , D to get the
regularization force on the circle {x2
1 + x2
2 = 1}, and the goal is to make the vector ﬁeld induced
by the normal regularization always point to the same direction, say anti-clockwise. However, this
cannot be done with a single auxiliary dimension because from the analysis for label noise, we know
when L−1
∇2L(Σ⊥) is identity, the normal regularization term in Equation (10) has 0 path integral along
the unit circle and thus it must have both directions. The key observation here is that we can align the
magnitude of noise with the strength of the regularization to make the path integral positive. By using
k ≥3 auxiliary dimensions, we can further ensure the normal regularization force is anti-clockwise
and of constant magnitude, which is reminiscent of how a three-phase induction motor works."
PD,0.46677215189873417,"Lemma C.8. Let Σ ∈RD×D be given by Σij(x) = (1+

Qj−3
α
v, Q−π/2x1:2

)(2+

Qj−3
α
v, x1:2

),
if i = j ≥3 or 0 otherwise, then the solution of SDE (10) is the following (33) , which implies that
Y (t) moves anti-clockwise with a constant angular speed of (D −2)/2."
PD,0.4675632911392405,"Y1:2(t) = Qt(D−2)/2Y1:2(0)
and
Y3:D(t) ≡0.
(33)"
PD,0.46835443037974683,"Proof of Lemma C.8. Note that for any x ∈Γ, it holds that"
PD,0.46914556962025317," 
∇2L(x)
 ij = 
 "
PD,0.4699367088607595,"2 +

Qj−3
α
v, x1:2

if i = j ≥3,
xixj
if i, j ∈{1, 2},
0
otherwise.
(34)"
PD,0.47072784810126583,"Then clearly Σ only brings about noise in the normal space, and speciﬁcally, it holds that
L−1
∇2L(x)(Σ(x)) = diag(0, 0, 1 +

Q0
αv, Q−π/2x1:2

, . . . , 1 +

QD−3
α
v, Q−π/2x1:2

). Further note
that, by the special structure of the hessian in (34) and Lemma C.3, for any x ∈Γ, we have
∂Φ(x) = (x2, −x1, 0, . . . , 0)⊤(x2, −x1, 0, . . . , 0) =
 Q−π/2x1:2
0
 Q−π/2x1:2
0
⊤. Combining these"
PD,0.47151898734177217,Published as a conference paper at ICLR 2022
PD,0.4723101265822785,"facts, the dynamics of the ﬁrst two coordinates in SDE (10) can be simpliﬁed into"
PD,0.47310126582278483,dx1:2(t)
PD,0.47389240506329117,"dt
= −
1"
PD,0.47468354430379744,"2∂Φ(x(t))∂2(∇L)(x(t))[L−1
∇2L(Σ(x(t))]
 1:2 = −1"
PD,0.4754746835443038,"2Q−π/2x1:2x⊤
1:2Q⊤
−π/2 D
X j=3"
PD,0.4762658227848101," 
1 +

Qj−3
α
v, Q−π/2x1:2

∇1:2(∂jjL)(x) = −1"
PD,0.47705696202531644,2Q−π/2x1:2 *
PD,0.4778481012658228,"Q−π/2x1:2, D
X j=3"
PD,0.4786392405063291," 
1 +

Qj−3
α
v, Q−π/2x1:2

Qj−3
α
v + = −1"
PD,0.47943037974683544,2Q−π/2x1:2   *
PD,0.4802215189873418,"Q−π/2x1:2, D
X"
PD,0.4810126582278481,"j=3
Qj−3
α
v + + D
X j=3"
PD,0.48180379746835444,"Qj−3
α
v, Q−π/2x1:2
2
  = −1"
PD,0.4825949367088608,2Q−π/2x1:2
PD,0.4833860759493671,"
0 + D −2 2"
PD,0.48417721518987344,"Q−π/2x1:2
2
2"
PD,0.4849683544303797,"
= D −2"
PD,0.48575949367088606,"2
Qπ/2x1:2,"
PD,0.4865506329113924,"where the second to the last equality follows from the property of Qα and the last equality follows
from the fact that ∥x1:2∥2
2 = 1 for all x ∈Γ. Note we require k ≥3 (or D ≥5) to allow
PD
j=3

Qj−3
α
v, Q−π/2x1:2
2 = D−2"
PD,0.4873417721518987,"2
Q−π/2x1:2
2
2. On the other hand, we have dx3:D(t)"
PD,0.48813291139240506,"dt
= 0 as
∂Φ kills the movement on that component."
PD,0.4889240506329114,The proof is completed by noting that the solution of x1:2 is
PD,0.4897151898734177,"x1:2(t) = exp

t · D −2"
PD,0.49050632911392406,"2
Qπ/2"
PD,0.4912974683544304,"
x1:2(0),"
PD,0.4920886075949367,"and by Lemma C.9,"
PD,0.49287974683544306,"exp

t · D −2"
PD,0.4936708860759494,"2
Qπ/2"
PD,0.4944620253164557,"
= (exp(Qπ/2))"
PD,0.495253164556962,"t(D−2) 2
= Q"
PD,0.49604430379746833,t(D−2)
PD,0.49683544303797467,"2
1
= Q t(D−2) 2
."
PD,0.497626582278481,"Lemma C.9. exp(
  0 −1
1
0

) =
  cos 1 −sin 1
sin 1
cos 1

."
PD,0.49841772151898733,"Proof. By deﬁnition, for matrix A =
  0 −1
1
0

, exp(A) = P∞
t=0
At"
PD,0.49920886075949367,"t! . Note A2 = −I, A3 = −A and
A4 = I, and by using this pattern, we can easily check that ∞
X t=0 At t! ="
PD,0.5,"P∞
i=0(−1)i
1
(2i)!
−P∞
i=0(−1)i
1
(2i+1)!
P∞
i=0(−1)i
1
(2i+1)!
P∞
i=0(−1)i
1
(2i)! !"
PD,0.5007911392405063,"=

cos 1
−sin 1
sin 1
cos 1 
."
PD,0.5015822784810127,"D
PROOF OF RESULTS IN SECTION 6"
PD,0.502373417721519,"In this section, we present the missing proofs in Section 6 regarding the overparametrized linear
model."
PD,0.5031645569620253,"For convenience, for any p, r ≥0 and u ∈RD, we denote by Bp
r(u) the ℓp norm ball of radius r
centered at u. We also denote vi:j = (vi, vi+1, . . . , vj)⊤for i, j ∈[D]."
PD,0.5039556962025317,"D.1
PROOF OF THEOREM 6.1"
PD,0.504746835443038,"In this subsection, we provide the proof of Theorem 6.1.
Theorem 6.1. In the setting of OLM, suppose the groundtruth is κ-sparse and n ≥Ω(κ ln d) training
data are sampled from either i.i.d. Gaussian or Boolean distribution. Then for any initialization xinit
(except a zero-measure set) and any ϵ > 0, there exist η0, T > 0 such that for any η < η0, OLM
trained with label noise SGD (12) with LR equal to η for ⌊T/η2⌋steps returns an ϵ-optimal solution,
with probability of 1 −e−Ω(n) over the randomness of the training dataset."
PD,0.5055379746835443,Published as a conference paper at ICLR 2022
PD,0.5063291139240507,"Proof of Theorem 6.1. First, by Lemma 6.6, it holds with probability at least 1 −e−Ω(n) that the
solution to (18), x∗, is unique up to and satisﬁes |x∗| = ψ(w∗). Then on this event, for any ϵ > 0,
by Lemma 6.5, there exists some T > 0 such that xT given by the Riemannian gradient ﬂow (17)
satisﬁes that xT is an ϵ/2-optimal solution of the OLM. For this T, by Theorem 4.6, we know that
the ⌊T/η2⌋-th SGD iterate, xη(⌊T/η2⌋), satisﬁes ∥xη(⌊T/η2⌋) −xT ∥2 ≤ϵ/2 with probability at
least 1 −e−Ω(n) for all sufﬁciently small η > 0, and thus xη(⌊T/η2⌋) is an ϵ-optimal solution of
the OLM. Finally, the validity of applying Theorem 4.6 is guaranteed by Lemma 6.2 and 6.3. This
completes the proof."
PD,0.507120253164557,"In the following subsections, we provide the proofs of all the components used in the above proof."
PD,0.5079113924050633,"D.2
PROOF OF LEMMA 6.2"
PD,0.5087025316455697,"Recall that for each i ∈[n] fi(x) = f(u, v) = z⊤
i (u⊙2 −v⊙2), ∇fi(x) = 2
 zi⊙u
zi⊙v

, and K(x) =
(Kij(x))i,j∈[n] where each Kij(x) = ⟨∇fi(x), ∇fj(x)⟩. Then"
PD,0.509493670886076,"∇2ℓi(x) = 2

zi ⊙u
−zi ⊙v"
PD,0.5102848101265823,"  
(zi ⊙u)⊤
−(zi ⊙v)⊤
+ (fi(u, v) −yi) · diag(zi, zi)."
PD,0.5110759493670886,"So for any x ∈Γ, it holds that"
PD,0.5118670886075949,"∇2L(x) = 2 n n
X i=1"
PD,0.5126582278481012,"
zi ⊙u
−zi ⊙v"
PD,0.5134493670886076,"  
(zi ⊙u)⊤
−(zi ⊙v)⊤
.
(35)"
PD,0.5142405063291139,"Lemma D.1. For any ﬁxed x ∈RD, suppose {∇fi(x)}i∈[n] is linearly independent, then K(x) is
full-rank."
PD,0.5150316455696202,"Proof of Lemma D.1. Suppose otherwise, then there exists some λ ∈Rn such that λ ̸= 0 and
λ⊤K(x)λ = 0. However, note that"
PD,0.5158227848101266,"λ⊤K(x)λ = n
X"
PD,0.5166139240506329,"i,j=1
λiλjKij(x) = n
X"
PD,0.5174050632911392,"i,j=1
λiλj⟨∇fi(x), ∇fj(x)⟩ =  n
X"
PD,0.5181962025316456,"i=1
λi∇fi(x)  2 2
,"
PD,0.5189873417721519,"which implies that Pn
i=1 λi∇fi(x) = 0. This is a contradiction since by assumption {∇fi(x)}i∈[n]
is linearly independent."
PD,0.5197784810126582,"Lemma 6.2. Consider the loss L deﬁned in (14) and manifold Γ deﬁned in (15). If data is full
rank, i.e., rank(Z) = n, then it holds that (a). Γ is a smooth manifold of dimension D −n; (b).
rank(∇2L(x)) = n for all x ∈Γ. In particular, rank(Z) = n holds with probability 1 for Gaussian
distribution and with probability 1 −cd for Boolean distribution for some constant c ∈(0, 1)."
PD,0.5205696202531646,"Proof of Lemma 6.2. (1) By preimage theorem (Banyaga & Hurtubise, 2013), it sufﬁces to check the
jacobian [∇f1(x), . . . , ∇fn(x)] = 2[
  z1⊙u
−z1⊙v

, . . . ,
  zn⊙u
−zn⊙v

] is full rank. Similarly, for the second
claim, due to (35). it is also equivalent to show that {
  zi⊙u
−zi⊙v

}i∈[n] is of rank n."
PD,0.5213607594936709,"Since
 u
v

∈Γ ⊂U, each coordinate is non-zero, thus we only need to show that {zi}i∈[n] is of rank
n. This happens with probability 1 in the Gaussian case, and probability at least 1 −cd for some
constant c ∈(0, 1) by Kahn et al. (1995). This completes the proof."
PD,0.5221518987341772,Published as a conference paper at ICLR 2022
PD,0.5229430379746836,"D.3
PROOF OF LEMMA 6.3"
PD,0.5237341772151899,"We ﬁrst establish some auxiliary results. The following lemma shows the PL condition along the
trajectory of gradient ﬂow."
PD,0.5245253164556962,"Lemma D.2. Along the gradient ﬂow generated by −∇L, it holds that ∥∇L(x(t))∥2
≥
16"
PD,0.5253164556962026,"n λmin(ZZ⊤) · mini∈[d] |ui(0)vi(0)|L(x(t)), ∀t ≥0."
PD,0.5261075949367089,"To prove Lemma D.2, we need the following invariance along the gradient ﬂow.
Lemma D.3. Along the gradient ﬂow generated by −∇L, uj(t)vj(t) stays constant for all j ∈[d].
Thus, sign(uj(t)) = sign(uj(0)) and sign(vj(t)) = sign(vj(0)) for any j ∈[d]."
PD,0.5268987341772152,Proof of Lemma D.3.
PD,0.5276898734177216,"∂
∂t(uj(t)vj(t)) = ∂uj(t)"
PD,0.5284810126582279,"∂t
· vj(t) + uj(t) · ∂vj(t)"
PD,0.5292721518987342,"∂t
= ∇uL(u(t), v(t))j · vj(t) + uj(t) · ∇vL(u(t), v(t))j = 2 n n
X"
PD,0.5300632911392406,"i=1
(fi(u(t), v(t)) −yi)zi,juj(t)vj(t) −2uj(t) n n
X"
PD,0.5308544303797469,"i=1
(fi(u(t), v(t)) −yi)zi,jvj(t) = 0."
PD,0.5316455696202531,"Therefore, any sign change of uj(t), vj(t) would enforce uj(t) = 0 or vj(t) = 0 for some t > 0
since uj(t), vj(t) are continuous in time t. This immediately leads to a contradiction to the invariance
of uj(t)vj(t)."
PD,0.5324367088607594,We then can prove Lemma D.2.
PD,0.5332278481012658,Proof of Lemma D.2. Note that
PD,0.5340189873417721,"∥∇L(x)∥2
2 = 1 n2 n
X"
PD,0.5348101265822784,"i,j=1
(fi(x) −yi)(fj(x) −yj) ⟨∇fi(x), ∇fj(x)⟩ ≥1 n2 n
X"
PD,0.5356012658227848,"i=1
(fi(x) −yi)2λmin(K(x)) = 2"
PD,0.5363924050632911,"nL(x)λmin(K(x)),"
PD,0.5371835443037974,"where K(x) is a n × n p.s.d. matrix with Kij(x) = ⟨∇fi(x), ∇fj(x)⟩. Below we lower bound
λmin(K(x)), the smallest eigenvalue of K(x). Note that Kij(x(t)) = 4 Pd
h=1 zi,hzj,h((uh(t))2 +
(vh(t))2), and we have"
PD,0.5379746835443038,K(x(t)) = 4Zdiag((u(t))⊙2 + (v(t))⊙2)Z⊤⪰8Zdiag(|u(t) ⊙v(t)|)Z⊤
PD,0.5387658227848101,"(∗)
= 8Zdiag(|u(0) ⊙v(0)|)Z⊤⪰8 min
i∈[d] |ui(0)vi(0)|ZZT"
PD,0.5395569620253164,"where (∗) is by Lemma D.3. Thus λmin(K(x(t)) ≥8 mini∈[d] |ui(0)vi(0)|λmin(ZZT ) for all t ≥0,
which completes the proof."
PD,0.5403481012658228,"We also need the following characterization of the manifold Γ.
Lemma D.4. All the stationary points in U are global minimizers, i.e., Γ = {x ∈U | ∇L(x) = 0}."
PD,0.5411392405063291,"Proof of Lemma D.4. Since Γ is the set of local minimizers, each x in Γ must satisfy ∇L(x) = 0. The
other direction is proved by noting that rank({zi}i∈[n]) = n, which implies rank({∇fi(x)}i∈[n]) =
n."
PD,0.5419303797468354,"Now, we are ready to prove Lemma 6.3 which is restated below."
PD,0.5427215189873418,Published as a conference paper at ICLR 2022
PD,0.5435126582278481,"Lemma 6.3. Consider the loss function L deﬁned in (14), manifold Γ and its open neighborhood
deﬁned in (15). For gradient ﬂow dxt"
PD,0.5443037974683544,"dt = −∇L(xt) starting at any x0 ∈U, it holds that Φ(x0) ∈Γ."
PD,0.5450949367088608,Proof of Lemma 6.3. It sufﬁces to prove gradient ﬂow dx(t)
PD,0.5458860759493671,"dt
= −∇L(x(t)) converges when t →∞,
as long as x(0) ∈U. Whenever it converges, it must converge to a stationary point in U. The proof
will be completed by noting that all stationary point of L in U belongs to Γ (Lemma D.4)."
PD,0.5466772151898734,"Below we prove limt→∞x(t) exists. Denote C =
16"
PD,0.5474683544303798,"n mini∈[d] |ui(0)vi(0)|λmin(ZZ⊤), then it
follows from Lemma D.2 that

dx(t) dt"
PD,0.5482594936708861,"= ∥∇L(x(t))∥≤∥∇L(x(t))∥2
2
p"
PD,0.5490506329113924,"CL(x(t))
= −dL(x(t)) dt
p"
PD,0.5498417721518988,"L(x(t))
= −
1
2
√ C d
p"
PD,0.5506329113924051,"L(x(t)) dt
."
PD,0.5514240506329114,"Thus the total GF trajectory length is bounded by
R ∞
t=0 dx(t)"
PD,0.5522151898734177,"dt
 dt ≤
R ∞
t=0 −
1
2
√ C
d
√"
PD,0.553006329113924,L(x(t))
PD,0.5537974683544303,"ddt
dt ≤"
PD,0.5545886075949367,"L(x(0)) 2
√"
PD,0.555379746835443,"C , where the last inequality uses that L is non-negative over RD. Therefore, the GF must
converge."
PD,0.5561708860759493,"D.4
PROOF OF RESULTS IN SECTION 6.2"
PD,0.5569620253164557,"Without loss of generality, we will assume P"
PD,0.557753164556962,"i=1 z2
i,j > 0 for all j ∈[d], because otherwise we can
just delete the unused coordinate, since there won’t be any update in the parameter corresponding to
that coordinate. Moreover, in both gaussian and boolean setting, it can be shown that with probability
1, P"
PD,0.5585443037974683,"i=1 z2
i,j > 0 for all j ∈[d]."
PD,0.5593354430379747,"To study the optimal solution to (18), we consider the corresponding d-dimensional convex program
in terms of w ∈Rd, which has been studied in Tropp (2015):"
PD,0.560126582278481,"minimize
R(w) = 4 n d
X j=1 n
X"
PD,0.5609177215189873,"i=1
z2
i,j ! |wj|,"
PD,0.5617088607594937,"subject to
Zw = Zw∗. (36)"
PD,0.5625,"Here we slightly abuse the notation of R and the parameter dimension will be clear from the context.
We can relate the optimal solution to (18) to that of (36) via a canonical parametrization deﬁned as
follows."
PD,0.5632911392405063,"Deﬁnition D.5 (Canonical Parametrization). For any w ∈Rd, we deﬁne
 u
v

= ψ(w) = ([w⊤]⊙1/2
+
,
[−w⊤]⊙1/2
+
)⊤as the canonical parametrization of w. Clearly, it holds that u⊙2 −v⊙2 = w."
PD,0.5640822784810127,"Indeed, we can show that if (36) has a unique optimal solution, it immediately follows that the optimal
solution to (18) is also unique up to sign ﬂips of each coordinate, as summarized in the lemma below."
PD,0.564873417721519,"Lemma D.6. Suppose the optimal solution to (36) is unique and equal to w∗. Then the optimal
solution to (18) is also unique up to sign ﬂips of each coordinate. In particular, one of them is given
by (˜u∗, ˜v∗) = ψ(w∗), that is, the canonical parametrization of w∗."
PD,0.5656645569620253,"Proof of Lemma D.6. Let (ˆu, ˆv) be any optimal solution of (18) and we deﬁne ˆw = ˆu⊙2 −ˆv⊙2,
which is also feasible to (36). By the optimality of w∗, we have d
X j=1 n
X"
PD,0.5664556962025317,"i=1
z2
i,j !"
PD,0.567246835443038,"|w∗
j | ≤ d
X j=1 n
X"
PD,0.5680379746835443,"i=1
z2
i,j !"
PD,0.5688291139240507,"| ˆwj| ≤ d
X j=1 n
X"
PD,0.569620253164557,"i=1
z2
i,j !"
PD,0.5704113924050633,"(ˆu2
j + ˆv2
j ).
(37)"
PD,0.5712025316455697,"On the other hand, (˜u∗, ˜v∗) = ψ(w∗) is feasible to (18). Thus, it follows from the optimality of (ˆu, ˆv)
that d
X j=1 n
X"
PD,0.571993670886076,"i=1
z2
i,j !"
PD,0.5727848101265823,"(ˆu2
j + ˆv2
j ) ≤ d
X j=1 n
X"
PD,0.5735759493670886,"i=1
z2
i,j !"
PD,0.5743670886075949,"((˜u∗
j)2 + (˜v∗
j )2) = d
X j=1 n
X"
PD,0.5751582278481012,"i=1
z2
i,j !"
PD,0.5759493670886076,"|w∗
j |.
(38)"
PD,0.5767405063291139,Published as a conference paper at ICLR 2022
PD,0.5775316455696202,"Combining (37) and (38) yields d
X j=1 n
X"
PD,0.5783227848101266,"i=1
z2
i,j !"
PD,0.5791139240506329,"(ˆu2
j + ˆv2
j ) = d
X j=1 n
X"
PD,0.5799050632911392,"i=1
z2
i,j !"
PD,0.5806962025316456,"|w∗
j | = d
X j=1 n
X"
PD,0.5814873417721519,"i=1
z2
i,j !"
PD,0.5822784810126582,"|ˆu2
j −ˆv2
j |
(39)"
PD,0.5830696202531646,"which implies that ˆu⊙2 −ˆv⊙2 is also an optimal solution of (36). Since w∗is the unique optimal
solution to (36), we have ˆu⊙2 −ˆv⊙2 = w∗. Moreover, by (39), we must have ˆu⊙2 = [w∗]+ and
ˆu⊙2 = [w∗]+, otherwise the equality would not hold. This completes the proof."
PD,0.5838607594936709,"Therefore, the unique optimality of (18) can be reduced to that of (36). In the sequel, we show that the
latter holds for both Boolean and Gaussian random vectors. We divide Lemma 6.6 into to Lemma D.8
and D.7 for clarity."
PD,0.5846518987341772,"Lemma D.7 (Boolean Case). Let z1, . . . , zn
i.i.d.
∼Unif({±1}d). There exist some constants C, c > 0
such that if the sample size n satisﬁes"
PD,0.5854430379746836,n ≥C[κ ln(d/κ) + κ]
PD,0.5862341772151899,"then with probability at least 1 −e−cn2, the optimal solution of (18), (ˆu, ˆv), is unique up to sign ﬂips
of each coordinate and recovers the groundtruth, i.e., ˆu⊙2 −ˆv⊙2 = w∗."
PD,0.5870253164556962,"Proof of Lemma D.7. By the assumption that z1, . . . , zn
i.i.d.
∼Unif({±1}d), we have Pn
i=1 z2
i,j = n
for all j ∈[d]. Then (36) is equivalent to the following optimization problem:"
PD,0.5878164556962026,"minimize
g(w) = ∥w∥1,"
PD,0.5886075949367089,"subject to
Zw = Z((u∗)⊙2 −(v∗)⊙2).
(40)"
PD,0.5893987341772152,"This model exactly ﬁts the Example 6.2 in Tropp (2015) with σ = 1 and α = 1/
√"
THEN APPLYING,0.5901898734177216,"2. Then applying
Equation (4.2) and Theorem 6.3 in Tropp (2015), (40) has a unique optimal solution equal to
(u∗)⊙2 −(v∗)⊙2 with probability at least 1 −e−ch2 for some constant c > 0, given that the sample
size satisﬁes"
THEN APPLYING,0.5909810126582279,n ≥C(κ ln(d/κ) + κ + h)
THEN APPLYING,0.5917721518987342,"for some absolute constant C > 0. Choosing h =
n
2C and then adjusting the choices of C, c
appropriately yield the desired result. Finally, applying Lemma D.6 ﬁnishes the proof."
THEN APPLYING,0.5925632911392406,The Gaussian case requires more careful treatment.
THEN APPLYING,0.5933544303797469,"Lemma D.8 (Gaussian Case). Let z1, . . . , zn
i.i.d.
∼N(0, Id). There exist some constants C, c > 0
such that if the sample size satisﬁes"
THEN APPLYING,0.5941455696202531,"n ≥Cκ ln d,"
THEN APPLYING,0.5949367088607594,"then with probability at least 1 −(2d + 1)e−cn, the optimal solution of (18), (ˆu, ˆv), is unique up to
sign ﬂips of each coordinate of ˆu and ˆv and recovers the groundtruth, i.e., ˆu⊙2 −ˆv⊙2 = w∗."
THEN APPLYING,0.5957278481012658,"Proof of Lemma D.8. Since z1, . . . , zn
i.i.d.
∼N(0, Id), we have P "" n
X"
THEN APPLYING,0.5965189873417721,"i=1
z2
i,j ∈[n/2, 3n/2], ∀j ∈[d] #"
THEN APPLYING,0.5973101265822784,≥1 −2de−cn
THEN APPLYING,0.5981012658227848,"for some constant c > 0, and we denote this event by En. Therefore, on En, we have 2 D
X"
THEN APPLYING,0.5988924050632911,"j=1
(u2
j + v2
j ) ≤R(x) ≤6 D
X"
THEN APPLYING,0.5996835443037974,"j=1
(u2
j + v2
j )"
THEN APPLYING,0.6004746835443038,"or equivalently,"
THEN APPLYING,0.6012658227848101,2(∥u⊙2∥1 + ∥v⊙2∥1) ≤R(x) ≤6(∥u⊙2∥1 + v⊙2∥1).
THEN APPLYING,0.6020569620253164,Published as a conference paper at ICLR 2022
THEN APPLYING,0.6028481012658228,"Deﬁne w∗= (u∗)⊙2 −(v∗)⊙2, and (36) is equivalent to the following convex optimization problem"
THEN APPLYING,0.6036392405063291,"minimize
g(w) = 4 n d
X j=1 n
X"
THEN APPLYING,0.6044303797468354,"i=1
z2
i,j !"
THEN APPLYING,0.6052215189873418,"|wj + w∗
j |,"
THEN APPLYING,0.6060126582278481,"subject to
Zw = 0. (41)"
THEN APPLYING,0.6068037974683544,"The point w = 0 is feasible for (41), and we claim that this is the unique optimal solution when n is
large enough. In detail, assume that there exists a non-zero feasible point w for (41) in the descent
cone (Tropp, 2015) D(g, w∗) of g, then"
THEN APPLYING,0.6075949367088608,"λmin(Z; D(g, w∗)) ≤∥Zw∥2"
THEN APPLYING,0.6083860759493671,"∥w∥2
= 0"
THEN APPLYING,0.6091772151898734,"where the equality follows from that w is feasible.
Therefore, we only need to show that
λmin(Z; D(g, x∗)) is bounded from below for sufﬁciently large n."
THEN APPLYING,0.6099683544303798,"On En, it holds that g belongs to the following function class G = 
"
THEN APPLYING,0.6107594936708861,"h : Rd →R | h(w) = d
X"
THEN APPLYING,0.6115506329113924,"j=1
υj|wj|, υ ∈Υ 
"
THEN APPLYING,0.6123417721518988,"with Υ = {υ ∈Rd : υj ∈[2, 6], ∀j ∈[d]}."
THEN APPLYING,0.6131329113924051,"We identify gυ ∈G with υ ∈Υ, then D(g, w∗) ⊆∪υ∈ΥD(gυ, w∗)) := DΥ, which further implies
that"
THEN APPLYING,0.6139240506329114,"λmin(Z; D(g, w∗)) ≥λmin(Z; DΥ)."
THEN APPLYING,0.6147151898734177,"Recall the deﬁnition of minimum conic singular value (Tropp, 2015):"
THEN APPLYING,0.615506329113924,"λmin(Z; DΥ) =
inf
p∈DΥ∩Sd−1
sup
q∈Sn−1⟨q, Zp⟩."
THEN APPLYING,0.6162974683544303,where Sn−1 denotes the unit sphere in Rn. Applying the same argument as in Tropp (2015) yields
THEN APPLYING,0.6170886075949367,"P

λmin(Z; DΥ) ≥
√"
THEN APPLYING,0.617879746835443,"n −1 −w(DΥ) −h

≥1 −e−h2/2."
THEN APPLYING,0.6186708860759493,"Take the intersection of this event with En, and we obtain from a union bound that"
THEN APPLYING,0.6194620253164557,"λmin(Z; D(g, w∗)) ≥
√"
THEN APPLYING,0.620253164556962,"n −1 −w(DΥ) −h
(42)"
THEN APPLYING,0.6210443037974683,"with probability at least 1 −e−h2/2 −2de−cn. It remains to determine w(DΥ), which is deﬁned as"
THEN APPLYING,0.6218354430379747,"w(DΥ) = Ez∼N(0,Id) """
THEN APPLYING,0.622626582278481,"sup
p∈DΥ∩Sd−1⟨z, p⟩ #"
THEN APPLYING,0.6234177215189873,"= Ez∼N(0,Id) """
THEN APPLYING,0.6242088607594937,"sup
υ∈Υ
sup
p∈D(gυ,x∗)∩Sd−1⟨z, p⟩ #"
THEN APPLYING,0.625,".
(43)"
THEN APPLYING,0.6257911392405063,"Without loss of generality, we assume that w∗= (w∗
1, . . . , w∗
κ, 0, . . . , 0)⊤with w∗
1, . . . , w∗
κ > 0,
otherwise one only needs to specify the signs and the nonzero set of w∗in the sequel. For any υ ∈Υ
and any p ∈D(gυ, w∗) ∩Sd−1, there exists some τ > 0 such that gυ(w∗+ τ · p) ≤gυ(w∗), i.e., d
X"
THEN APPLYING,0.6265822784810127,"j=1
υj|w∗
j + τpj| ≤ d
X"
THEN APPLYING,0.627373417721519,"j=1
υj|w∗
j |"
THEN APPLYING,0.6281645569620253,"which further implies that τ d
X"
THEN APPLYING,0.6289556962025317,"j=κ+1
υj|pj| ≤ κ
X"
THEN APPLYING,0.629746835443038,"j=1
υj(|w∗
j | −|w∗
j −τpj|) ≤τ κ
X"
THEN APPLYING,0.6305379746835443,"j=1
υj|pj|"
THEN APPLYING,0.6313291139240507,"where the second inequality follows from the triangle inequality. Then since each υj ∈[2, 6], it
follows that d
X"
THEN APPLYING,0.632120253164557,"j=κ+1
|pj| ≤3 κ
X"
THEN APPLYING,0.6329113924050633,"j=1
|pj|."
THEN APPLYING,0.6337025316455697,Published as a conference paper at ICLR 2022
THEN APPLYING,0.634493670886076,"Note that this holds for all ξ ∈Ξ simultaneously. Now let us denote p1:κ = (p1, . . . , pκ) ∈Rκ
and p(κ+1):d = (pκ+1, . . . , pd) ∈Rd−κ, and similarly for other d-dimensional vectors. Then for all
p ∈DΥ ∩Sd−1, by Cauchy-Schwartz inequality, we have"
THEN APPLYING,0.6352848101265823,∥p(κ+1):d∥1 ≤3∥p1:κ∥1 ≤3√κ∥p1:κ∥2.
THEN APPLYING,0.6360759493670886,"Thus, for any z ∈Rd and any p ∈DΥ ∩Sd−1, it follows that"
THEN APPLYING,0.6368670886075949,"⟨z, p⟩= ⟨z1:κ, p1:κ⟩+ ⟨z(κ+1):d, p(κ+1):d⟩"
THEN APPLYING,0.6376582278481012,"≤∥z1:κ∥2∥p1:κ∥2 + ∥p(κ+1):d∥1 ·
max
j∈{κ+1,...,d} |zj|"
THEN APPLYING,0.6384493670886076,"≤∥z1:κ∥2∥p1:κ∥2 + 3√κ∥p1:κ∥2 ·
max
j∈{κ+1,...,d} |zj|"
THEN APPLYING,0.6392405063291139,"≤∥z1:κ∥2 + 3√κ ·
max
j∈{κ+1,...,d} |zj|"
THEN APPLYING,0.6400316455696202,"where the last inequality follows from the fact that p ∈Sd−1. Therefore, combine the above inequality
with (43), and we obtain that"
THEN APPLYING,0.6408227848101266,"w(DΥ) ≤E

∥z1:κ∥2 + 3√κ ·
max
j∈{κ+1,...,d} |zj|
"
THEN APPLYING,0.6416139240506329,"≤√κ + 3√κ · E

max
j∈{κ+1,...,d} |zj|

.
(44)"
THEN APPLYING,0.6424050632911392,"where the second inequality follows from the fact that E[∥z1:κ∥2] ≤
p"
THEN APPLYING,0.6431962025316456,"E[∥z1:κ∥2
2] = √κ. To bound
the second term in (44), applying Lemma D.9, it follows from (44) that"
THEN APPLYING,0.6439873417721519,"w(DΥ) ≤√κ + 3
p"
THEN APPLYING,0.6447784810126582,"2κ ln(2(d −κ)).
(45)"
THEN APPLYING,0.6455696202531646,"Therefore, combining (45) and (42), we obtain"
THEN APPLYING,0.6463607594936709,"λmin(Z; D(g, w∗)) ≥
√"
THEN APPLYING,0.6471518987341772,"n −1 −√κ −3
p"
THEN APPLYING,0.6479430379746836,2κ ln(2(d −κ)) −h.
THEN APPLYING,0.6487341772151899,"Therefore, choosing h = √n −1/2, as long as n satisﬁes that n ≥C(κ ln d) for some constant
C > 0, we have λmin(Z; D(g, w∗)) > 0 with probability at least 1 −(2d + 1)e−cn. Finally, the
uniqueness of the optimal solution to (18) in this case follows from Lemma D.6."
THEN APPLYING,0.6495253164556962,"Lemma D.9. Let z ∼N(0, Id), then it holds that E

maxi∈[d] |zi|

≤
p"
THEN APPLYING,0.6503164556962026,2 ln(2d).
THEN APPLYING,0.6511075949367089,"Proof of Lemma D.9. Denote M = maxi∈[d] |zi|. For any λ > 0, by Jensen’s inequality, we have"
THEN APPLYING,0.6518987341772152,"eλ·E[M] ≤E

eλM
= E

max
i∈[d] eλ|zi|

≤ d
X"
THEN APPLYING,0.6526898734177216,"i=1
E
h
eλ|zi|i
."
THEN APPLYING,0.6534810126582279,"Note that E[eλ|zi|] ≤2·E[eλzi]. Thus, by the expression of the Gaussian moment generating function,
we further have"
THEN APPLYING,0.6542721518987342,"eλ·E[M] ≤2 d
X"
THEN APPLYING,0.6550632911392406,"i=1
E

eλzi
= 2deλ2/2,"
THEN APPLYING,0.6558544303797469,from which it follows that
THEN APPLYING,0.6566455696202531,"E[M] ≤ln(2d) λ
+ λ 2 ."
THEN APPLYING,0.6574367088607594,"Choosing λ =
p"
THEN APPLYING,0.6582278481012658,2 ln(2d) yields the desired result.
THEN APPLYING,0.6590189873417721,Published as a conference paper at ICLR 2022
THEN APPLYING,0.6598101265822784,"D.5
PROOF OF LEMMA 6.5"
THEN APPLYING,0.6606012658227848,"Instead of studying the convergence of the Riemannian gradient ﬂow directly, it is more convenient
to consider it in the ambient space RD. To do so, we deﬁne a Lagrange function L(x; λ) =
R(x) + Pn
i=1 λi(fi(x) −yi) for λ ∈Rn. Based on this Lagrangian, we can continuously extend
∂Φ(x)∇R(x) to the whole space RD. In speciﬁc, we can ﬁnd a continuous function F : RD →RD
such that F(·)|Γ = ∂Φ(·)∇R(·). Such an F can be implicitly constructed via the following lemma.
Lemma D.10. The ℓ2 norm has a unique minimizer among {∇xL(x; λ) | λ ∈Rn} for any ﬁxed
x ∈RD. Thus we can deﬁne F : RD →RD by F(x) = argming∈{∇xL(x;λ)|λ∈Rn} ∥g∥2. Moreover,
it holds that ⟨F(x), ∇fi(x)⟩= 0 for all i ∈[n]."
THEN APPLYING,0.6613924050632911,"Proof of Lemma D.10. Fix any x ∈RD. Note that {∇xL(x; λ) | λ ∈Rn} is the subspace spanned
by {∇fi(x)}i∈[n] shifted by ∇R(x), thus there is unique minimizer of the ℓ2 norm in this set. This
implies that F(x) = argming∈{∇xL(x;λ)|λ∈Rn} ∥g∥2 is well-deﬁned."
THEN APPLYING,0.6621835443037974,"To show the second claim, denote h(λ) = ∥∇xL(x; λ)∥2
2/2, which is a quadratic function of λ ∈Rn.
Then we have"
THEN APPLYING,0.6629746835443038,"∇h(λ) =  
"
THEN APPLYING,0.6637658227848101,"⟨∇R(x), ∇f1(x)⟩
...
⟨∇R(x), ∇fn(x)⟩  
+  
"
THEN APPLYING,0.6645569620253164,"Pn
i=1 λi⟨∇f1(x), ∇fi(x)⟩
...
Pn
i=1 λi⟨∇fn(x), ∇fi(x)⟩  
=  
"
THEN APPLYING,0.6653481012658228,"⟨∇R(x), ∇f1(x)⟩
...
⟨∇R(x), ∇fn(x)⟩ "
THEN APPLYING,0.6661392405063291,"
+ K(x)λ."
THEN APPLYING,0.6669303797468354,"For any λ such that ∇xL(x; λ) = F(x), we must have ∇h(λ) = 0 by the deﬁnition of F(x), which
by the above implies"
THEN APPLYING,0.6677215189873418,"(K(x)λ)i = −⟨∇R(x), ∇fi(x)⟩
for all i ∈[n]."
THEN APPLYING,0.6685126582278481,"Therefore, we further have"
THEN APPLYING,0.6693037974683544,"⟨F(x), ∇fi(x)⟩= ⟨∇R(x), ∇fi(x)⟩+ n
X"
THEN APPLYING,0.6700949367088608,"j=1
λj⟨∇fi(x), ∇fj(x)⟩= ⟨∇R(x), ∇fi(x)⟩+ (K(x)λ)i = 0"
THEN APPLYING,0.6708860759493671,for all i ∈[n]. This ﬁnishes the proof.
THEN APPLYING,0.6716772151898734,"Hence, with any initialization x(0) ∈Γ, the limiting ﬂow (17) is equivalent to the following dynamics dx(t)"
THEN APPLYING,0.6724683544303798,"dt
= −1"
THEN APPLYING,0.6732594936708861,"4F(x(t)).
(46)"
THEN APPLYING,0.6740506329113924,"Thus Lemma 6.5 can be proved by showing that the above x(t) converges to x∗as t →∞. We ﬁrst
present a series of auxiliary results in below.
Lemma D.11 (Implications for F(x) = 0). Let F : RD →RD be as deﬁned in Lemma D.10. For
any x =
 u
v

∈RD such that F(x) = 0, it holds that for each j ∈[d], either uj = 0 or vj = 0."
THEN APPLYING,0.6748417721518988,"Proof. Since F(x) = 0, it holds for all j ∈[d] that,"
THEN APPLYING,0.6756329113924051,0 = ∂R
THEN APPLYING,0.6764240506329114,"∂uj
(x) + n
X"
THEN APPLYING,0.6772151898734177,"i=1
λi(x) ∂fi"
THEN APPLYING,0.678006329113924,"∂uj
(x) = 2uj ""
4
n n
X"
THEN APPLYING,0.6787974683544303,"i=1
z2
i,j + n
X"
THEN APPLYING,0.6795886075949367,"i=1
λi(x)zi,j # ,"
THEN APPLYING,0.680379746835443,0 = ∂R
THEN APPLYING,0.6811708860759493,"∂vj
(x) + n
X"
THEN APPLYING,0.6819620253164557,"i=1
λi(x) ∂fi"
THEN APPLYING,0.682753164556962,"∂vj
(x) = 2vj ""
4
n n
X"
THEN APPLYING,0.6835443037974683,"i=1
z2
i,j − n
X"
THEN APPLYING,0.6843354430379747,"i=1
λi(x)zi,j # ."
THEN APPLYING,0.685126582278481,"If there exists some j ∈[d] such that uj ̸= 0 and vj ̸= 0, then it follows from the above two identities
that n
X"
THEN APPLYING,0.6859177215189873,"i=1
z2
i,j = 0"
THEN APPLYING,0.6867088607594937,"which happens with probability 0 in both the Boolean and Gaussian case. Therefore, we must have
uj = 0 or vj = 0 for all j ∈[d]."
THEN APPLYING,0.6875,Published as a conference paper at ICLR 2022
THEN APPLYING,0.6882911392405063,Lemma D.12. Let F : RD →RD be as deﬁned in Lemma D.10. Then F is continuous on RD.
THEN APPLYING,0.6890822784810127,"Proof. Case I. We ﬁrst consider the simpler case of any ﬁxed x∗∈U = (R \ {0})D, assuming that
K(x∗) is full-rank. Lemma D.10 implies that for any λ ∈Rn such that ∇xL(x∗; λ) = F(x∗), we
have"
THEN APPLYING,0.689873417721519,K(x∗)λ = −[∇f1(x) . . . ∇fn(x)]⊤∇R(x).
THEN APPLYING,0.6906645569620253,Thus such λ is unique and given by
THEN APPLYING,0.6914556962025317,λ(x∗) = −K(x∗)−1[∇f1(x) . . . ∇fn(x)]⊤∇R(x).
THEN APPLYING,0.692246835443038,"Since K(x) is continuous around x∗, there exists a sufﬁciently small δ > 0 such that for any
x ∈Bδ(x∗), K(x) is full-rank, which further implies that K(x)−1 is also continuous in Bδ(x).
Therefore, by the above characterization of λ, we see that λ(x) is continuous for x ∈Bδ(x∗), and so
is F(x) = ∇R(x) + Pn
i=1 λi(x)∇fi(x)."
THEN APPLYING,0.6930379746835443,"Case II. Next, we consider all general x∗∈RD. Here for simplicity, we reorder the coordinates as
x = (u1, v1, u2, v2, . . . , ud, vd) with a slight abuse of notation. Without loss of generality, ﬁx any
x∗such that for some q ∈[d], (ui(0))2 + (vi(0))2 > 0 for all i = 1, . . . , q and u∗
i = v∗
i = 0 for
all i = q + 1, . . . , d. Then ∇R(x∗) and {∇fi(x∗)}i∈[n] only depend on {zi,j}i∈[n],j∈[q], and for all
i ∈[n], it holds that"
THEN APPLYING,0.6938291139240507,(∇R(x∗))(2q+1):D = (∇fi(x∗))(2q+1):D = 0.
THEN APPLYING,0.694620253164557,"Note that if we replace {∇fi(x)}i∈[n] by any ﬁxed and invertible linear transform of itself, it would
not affect the deﬁnition of F(x). In speciﬁc, we can choose an invertible matrix Q ∈Rn×n
such that, for some q′ ∈[q], (˜z1, . . . , ˜zn) = (z1, . . . , zn)Q satisﬁes that {˜zi,1:q}i∈[q′] is linearly"
THEN APPLYING,0.6954113924050633,"independent and ˜zi,1:q = 0 for all i = q′ + 1, . . . , n. We then consider
h
∇˜f1(x), . . . , ∇˜fn(x)
i
="
THEN APPLYING,0.6962025316455697,"[∇f1(x), . . . , ∇fn(x)] Q and the corresponding F(x). For notational simplicity, we assume that Q
can be chosen as the identity matrix, so that (z1, . . . , zn) itself satisﬁes the above property, and we
repeat it here for clarity"
THEN APPLYING,0.696993670886076,"{zi,1:q}i∈[q′] is linearly independent and ˜zi,1:q = 0 for all i = q′ + 1, . . . , n.
(47)"
THEN APPLYING,0.6977848101265823,This further implies that
THEN APPLYING,0.6985759493670886,"(∇fi(x))1:(2q) = 0,
for all i ∈{q′ + 1, . . . , n} and x ∈RD.
(48)"
THEN APPLYING,0.6993670886075949,"In the sequel, we use λ for n-dimensional vectors and ¯λ for q′-dimensional vectors. Denote2"
THEN APPLYING,0.7001582278481012,"λ(x) ∈argmin
λ∈Rn"
THEN APPLYING,0.7009493670886076,"∇R(x) + n
X"
THEN APPLYING,0.7017405063291139,"i=1
λi∇fi(x) 2
,"
THEN APPLYING,0.7025316455696202,"¯λ(x) ∈argmin
¯λ∈Rq′  "
THEN APPLYING,0.7033227848101266,"∇R(x) + q′
X"
THEN APPLYING,0.7041139240506329,"i=1
¯λi∇(fi(x)  "
THEN APPLYING,0.7049050632911392,1:(2q) 2 .
THEN APPLYING,0.7056962025316456,"Then due to (47) and (48), we have "
THEN APPLYING,0.7064873417721519,"∇R(x∗) + q′
X"
THEN APPLYING,0.7072784810126582,"i=1
¯λi(x∗)∇fi(x∗)  "
THEN APPLYING,0.7080696202531646,1:(2q) 2 =
THEN APPLYING,0.7088607594936709,"∇R(x∗) + n
X"
THEN APPLYING,0.7096518987341772,"i=1
λi(x)∇fi(x∗)"
THEN APPLYING,0.7104430379746836,"2
= ∥F(x∗)∥2. (49)"
THEN APPLYING,0.7112341772151899,"2We do not care about the speciﬁc choice of λ(x) or ¯λ(x) when there are multiple candidates, and we only
need their properties according to Lemma D.10, so they can be arbitrary. Also, the minimum of ℓ2-norm of an
afﬁne space can always be attained so argmin exists."
THEN APPLYING,0.7120253164556962,Published as a conference paper at ICLR 2022
THEN APPLYING,0.7128164556962026,"On the other hand, for any x ∈RD, by (48), we have"
THEN APPLYING,0.7136075949367089,"
∇R(x) + q′
X"
THEN APPLYING,0.7143987341772152,"i=1
¯λi(x)∇fi(x)
"
THEN APPLYING,0.7151898734177216,1:(2q) 2
THEN APPLYING,0.7159810126582279,"= min
λ∈Rn  "
THEN APPLYING,0.7167721518987342,"∇R(x) + n
X"
THEN APPLYING,0.7175632911392406,"i=1
λi(x)∇fi(x) !"
THEN APPLYING,0.7183544303797469,1:(2q) 2 ≤ 
THEN APPLYING,0.7191455696202531,"
∇R(x) + n
X"
THEN APPLYING,0.7199367088607594,"i=1
λi(x)∇fi(x)
"
THEN APPLYING,0.7207278481012658,1:(2q)
THEN APPLYING,0.7215189873417721,"2
= ∥F1:(2q)(x)∥2"
THEN APPLYING,0.7223101265822784,≤∥F(x)∥2 ≤
THEN APPLYING,0.7231012658227848,"∇R(x) + n
X"
THEN APPLYING,0.7238924050632911,"i=1
λi(x∗)∇fi(x)"
THEN APPLYING,0.7246835443037974,"2
(50)"
THEN APPLYING,0.7254746835443038,"where the ﬁrst and third inequalities follow from the deﬁnition of F(x). Let x →x∗, by the continuity
of ∇R(x) and {∇fi(x)}i∈[n], we have"
THEN APPLYING,0.7262658227848101,"lim
x→x∗"
THEN APPLYING,0.7270569620253164,"∇R(x) + n
X"
THEN APPLYING,0.7278481012658228,"i=1
λi(x∗)∇fi(x) 2
="
THEN APPLYING,0.7286392405063291,"∇R(x∗) + n
X"
THEN APPLYING,0.7294303797468354,"i=1
λi(x∗)∇fi(x∗)"
THEN APPLYING,0.7302215189873418,"2
(51)"
THEN APPLYING,0.7310126582278481,"Denote ˜K(x) = ( ˜Kij(x))(i,j)∈[q′]2 = (⟨∇fi(x)1:(2q), ∇fi(x)1:(2q)⟩)(i,j)∈[q′]2. By applying the
same argument as in Case I, since ˜K(x∗) is full-rank, it also holds that limx→x∗¯λ(x) = ¯λ(x∗), and
thus"
THEN APPLYING,0.7318037974683544,"lim
x→x∗ "
THEN APPLYING,0.7325949367088608,"
∇R(x) + q′
X"
THEN APPLYING,0.7333860759493671,"i=1
¯λi(x)∇fi(x)1:(2q) 2 = "
THEN APPLYING,0.7341772151898734,"
∇R(x) + q′
X"
THEN APPLYING,0.7349683544303798,"i=1
¯λi(x∗)∇fi(x∗)
"
THEN APPLYING,0.7357594936708861,1:(2q) 2
THEN APPLYING,0.7365506329113924,. (52)
THEN APPLYING,0.7373417721518988,"Combing (49), (50), (51) and (52) yields"
THEN APPLYING,0.7381329113924051,"lim
x→x∗∥F1:(2q)(x)∥2 = lim
x→x∗min
λ∈Rn "
THEN APPLYING,0.7389240506329114,"
∇R(x) + n
X"
THEN APPLYING,0.7397151898734177,"i=1
λi∇fi(x)
"
THEN APPLYING,0.740506329113924,1:(2q)
THEN APPLYING,0.7412974683544303,"2
= ∥F(x∗)∥2.
(53)"
THEN APPLYING,0.7420886075949367,"Moreover, since ∥F(2q+1):D(x)∥2 =
q"
THEN APPLYING,0.742879746835443,"∥F(x)∥2
2 −∥F1:(2q)(x)∥2
2, we also have"
THEN APPLYING,0.7436708860759493,"lim
x→x∗∥F(2q+1):D(x)∥2 = 0.
(54)"
THEN APPLYING,0.7444620253164557,"It then remains to show that limx→x∗F1:(2q)(x) = F1:(2q)(x∗), which directly follows from
limx→x∗λ1:q′(x) = λ1:q′(x∗) = ¯λ(x∗)."
THEN APPLYING,0.745253164556962,"Now, for any ϵ > 0, due to the convergence of ¯λ(x) and that ˜K(x∗) ≻0, we can pick a sufﬁciently
small δ1 such that for some constant α > 0 and all x ∈Bδ1(x∗), it holds that ∥¯λ(x)−¯λ(x∗)∥2 ≤ϵ/2
and"
THEN APPLYING,0.7460443037974683,"
∇R(x) + q′
X"
THEN APPLYING,0.7468354430379747,"i=1
¯λi∇fi(x)
"
THEN APPLYING,0.747626582278481,1:(2q)  2 2 ≥ 
THEN APPLYING,0.7484177215189873,"
∇R(x) + q′
X"
THEN APPLYING,0.7492088607594937,"i=1
¯λi(x)∇fi(x)
"
THEN APPLYING,0.75,1:(2q)  2 2
THEN APPLYING,0.7507911392405063,"+ α∥¯λ −¯λ(x)∥2
2. (55)"
THEN APPLYING,0.7515822784810127,"for all ¯λ ∈Rp, where the inequality follows from the strong convexity. Meanwhile, due to (48), we
have"
THEN APPLYING,0.752373417721519,"lim
x→x∗ "
THEN APPLYING,0.7531645569620253,"
∇R(x) + q′
X"
THEN APPLYING,0.7539556962025317,"i=1
λi(x)∇fi(x)
"
THEN APPLYING,0.754746835443038,1:(2q) 2
THEN APPLYING,0.7555379746835443,"= lim
x→x∗ "
THEN APPLYING,0.7563291139240507,"
∇R(x) + n
X"
THEN APPLYING,0.757120253164557,"i=1
λi(x)∇fi(x)
"
THEN APPLYING,0.7579113924050633,1:(2q) 2 = 
THEN APPLYING,0.7587025316455697,"
∇R(x) + q′
X"
THEN APPLYING,0.759493670886076,"i=1
¯λi(x∗)∇fi(x∗)
"
THEN APPLYING,0.7602848101265823,1:(2q) 2
THEN APPLYING,0.7610759493670886,"= lim
x→x∗ "
THEN APPLYING,0.7618670886075949,"
∇R(x) + q′
X"
THEN APPLYING,0.7626582278481012,"i=1
¯λi(x)∇fi(x)
"
THEN APPLYING,0.7634493670886076,1:(2q) 2 .
THEN APPLYING,0.7642405063291139,Published as a conference paper at ICLR 2022
THEN APPLYING,0.7650316455696202,"where the second equality follows from (53) and the second equality is due to (52). Therefore, we
can pick a sufﬁciently small δ2 such that"
THEN APPLYING,0.7658227848101266,"
∇R(x) + q′
X"
THEN APPLYING,0.7666139240506329,"i=1
λi(x)∇fi(x)
"
THEN APPLYING,0.7674050632911392,1:(2q) 2 ≤ 
THEN APPLYING,0.7681962025316456,"
∇R(x) + q′
X"
THEN APPLYING,0.7689873417721519,"i=1
¯λi(x)∇fi(x)
"
THEN APPLYING,0.7697784810126582,1:(2q) 2 + αϵ2
THEN APPLYING,0.7705696202531646,"4
(56)"
THEN APPLYING,0.7713607594936709,"for all x ∈Bδ2(x∗). Setting δ = min(δ1, δ2), it follows from (55) and (56) that"
THEN APPLYING,0.7721518987341772,∥λ1:q′(x) −¯λ(x)∥2 ≤ϵ
THEN APPLYING,0.7729430379746836,"2,
for all x ∈Bδ(x∗)."
THEN APPLYING,0.7737341772151899,"Recall that we already have ∥¯λ(x) −¯λ(x∗)∥≤ϵ/2, and thus"
THEN APPLYING,0.7745253164556962,∥λ1:q′(x) −λ(x∗)1:q′∥2 = ∥λ1:q′(x) −¯λ(x∗)∥2 ≤∥λ1:q′(x) −¯λ(x)∥2 + ∥¯λ(x) −¯λ(x∗)∥2 ≤ϵ
THEN APPLYING,0.7753164556962026,"for all x ∈Bδ(x∗). Therefore, we see that limx→x∗λ1:q′(x) = λ(x∗)1:q′."
THEN APPLYING,0.7761075949367089,"Finally, it follows from the triangle inequality that"
THEN APPLYING,0.7768987341772152,∥F(x) −F(x∗)∥2 ≤ 
THEN APPLYING,0.7776898734177216,"
F(x) −F(x∗)
"
THEN APPLYING,0.7784810126582279,1:(2q)
THEN APPLYING,0.7792721518987342,"2
+ ∥F(2q+1):D(x)∥2 + ∥F(2q+1):D(x∗)∥2
|
{z
}
0 =  "
THEN APPLYING,0.7800632911392406,"∇R(x) + q′
X"
THEN APPLYING,0.7808544303797469,"i=1
λi(x)∇fi(x) −∇R(x∗) − q′
X"
THEN APPLYING,0.7816455696202531,"i=1
λi(x∗)∇fi(x∗)  "
THEN APPLYING,0.7824367088607594,1:(2q) 2
THEN APPLYING,0.7832278481012658,"+ ∥F(2q+1):D(x)∥2 ≤  q′
X"
THEN APPLYING,0.7840189873417721,"i=1
λi(x)∇fi(x) −λi(x∗)∇fi(x∗) 2"
THEN APPLYING,0.7848101265822784,+ ∥∇R(x) −∇R(x∗)∥2 + ∥F(2q+1):D(x)∥2
THEN APPLYING,0.7856012658227848,"where, as x →x∗, the ﬁrst term vanishes by the convergence of λ1:q′(x) and the continuity of each
∇fi(x), the second term converges to 0 by the continuity of ∇R(x) and the third term vanishes
by (54). Therefore, we conclude that"
THEN APPLYING,0.7863924050632911,"lim
x→x∗F(x) = F(x∗),"
THEN APPLYING,0.7871835443037974,"that is, F is continuous."
THEN APPLYING,0.7879746835443038,"Lemma D.13. For any initialization x∗∈Γ, the Riemmanian Gradient Flow (17) (or equivalently,
(46)) is deﬁned on [0, ∞)."
THEN APPLYING,0.7887658227848101,"Proof of Lemma D.13. Let [0, T) be the right maximal interval of existence of the solution of
Riemannian gradient glow and suppose T ̸= ∞. Since R(x(t)) is monotone decreasing, thus
R(x(t)) is upper bounded by R(x(0)) and therefore ∥∇R(x(t))∥is also upper bounded. Since
 dx(t)"
THEN APPLYING,0.7895569620253164,"dt

2 ≤∥∇R(x(t))∥2 for any t < T, the left limit x(T−) := limτ→T −x(τ) must exist. By"
THEN APPLYING,0.7903481012658228,"Corollary 1, Perko (2001), x(T−) belongs to boundary of U, i.e., uj(T−) = 0 or vj(T−) = 0 for
some j ∈[d] by Lemma D.11. By the deﬁnition of the Riemannian gradient ﬂow in (17), we have"
THEN APPLYING,0.7911392405063291,"d
dt(uj(t)vj(t)) =
 vj(t)e⊤
j
uj(t)e⊤
j
 dx(t) dt = −1"
THEN APPLYING,0.7919303797468354,"4
 vj(t)e⊤
j
uj(t)e⊤
j

F(x(t))."
THEN APPLYING,0.7927215189873418,"By the expression of F(x(t)) = ∇R(x(t)) + Pn
i=1 λi(x(t))∇fi(x(t)), we then have"
THEN APPLYING,0.7935126582278481,"d
dt(uj(t)vj(t)) = − ""
2
n n
X"
THEN APPLYING,0.7943037974683544,"i=1
z2
i,j + 1 2 n
X"
THEN APPLYING,0.7950949367088608,"i=1
λi(x(t))zi,j #"
THEN APPLYING,0.7958860759493671,"uj(t)vj(t) − ""
2
n n
X"
THEN APPLYING,0.7966772151898734,"i=1
z2
i,j −1 2 n
X"
THEN APPLYING,0.7974683544303798,"i=1
λi(x(t))zi,j #"
THEN APPLYING,0.7982594936708861,uj(t)vj(t) = −
N,0.7990506329113924,"4
n n
X"
N,0.7998417721518988,"i=1
z2
i,j !"
N,0.8006329113924051,uj(t)vj(t).
N,0.8014240506329114,Denote sj = 4
N,0.8022151898734177,"n
Pn
i=1 z2
i,j. It follows that |uj(t)vj(t)| = |uj(0)vj(0)|e−sjt for all t ∈[0, T). Taking
the limit we have |uj(T−)vj(T−)| ≥|uj(0)vj(0)|e−sjT > 0. Contradiction with T ̸= ∞!"
N,0.803006329113924,Published as a conference paper at ICLR 2022
N,0.8037974683544303,"Before showing that F satisﬁes the PL condition, we need the following two intermediate results.
Given two points u and v in Rd, we say u weakly dominate v (written as u ≤v) if and only if
ui ≤vi, for all i ∈[d]. Given two subsets A and B of RD, we say A weakly dominates B if and
only if for any point v in B, there exists a point u ∈A such that u ≤v."
N,0.8045886075949367,"Lemma D.14. For some q ∈[D], let S be any q-dimensional subspace of RD and P = {u ∈RD |
ui ≥0, ∀i ∈[D]}. Let u⋆be an arbitrary point in P and Q = P ∩(u⋆+ S). Then there exists a
radius r > 0, such that B1
r(0) ∩Q weakly dominates Q, where B1
r(0) is the ℓ1-norm ball of radius r
centered at 0."
N,0.805379746835443,"As a direct implication, for any continuous function f : P →R, which is coordinate-wise non-
decreasing, minx∈U f(x) can always be achieved."
N,0.8061708860759493,"Proof of Lemma D.14. We will prove by induction on the environment dimension D. For the base
case of D = 1, either S = {0} or S = R, and it is straight-forward to verify the desired for both
scenarios."
N,0.8069620253164557,"Suppose the proposition holds for D −1, below we show it holds for D. For each i ∈[D], we apply
the proposition with D −1 to Q ∩{u ∈P | ui = 0} (which can be seen as a subset of RD−1), and
let ri be the corresponding ℓ1 radius. Set r = maxi∈[D] ri, and we show that choosing the radius to
be r sufﬁces."
N,0.807753164556962,"For any v ∈Q, we take a random direction in S, denoted by ω. If ω ≥0 or ω ≤0, we denote by y
the ﬁrst intersection (i.e., choosing the smallest λ) between the line {v −λ|ω|}λ≥0 and the boundary
of U, i.e., ∪D
i=1{z ∈RD | zi = 0}. Clearly y ≤v. By the induction hypothesis, there exists a
u ∈B1
r(0) ∩Q such that u ≤y. Thus u ≤v and meets our requirement."
N,0.8085443037974683,"If ω has different signs across its coordinates, we take y1, y2 to be the ﬁrst intersections of the line
{v −λ|ω|}λ∈R and the boundary of U in directions of λ > 0 and λ < 0, respectively. Again by
the induction hypothesis, there exist u1, u2 ∈B1
r(0) ∩Q such that u1 ≤y1 and u2 ≤y2. Since v
lies in the line connecting u1 and u2, there exists some h ∈[0, 1] such that v = (1 −h)u1 + hu2.
It then follows that (1 −h)u1 + hu2 ≤(1 −h)y1 + hy2 = v. Now since Q is convex, we have
(1 −h)u1 + hu2 ∈Q, and by the triangle inequality it also holds that ∥(1 −h)u1 + hu2∥1 ≤r, so
(1 −h)u1 + hu2 ∈B1
r(0) ∩Q. Therefore, we conclude that B1
r(0) ∩Q weakly dominates Q, and
thus the proposition holds for D. This completes the proof by induction."
N,0.8093354430379747,"Lemma D.15. For some q ∈[D], let S be any q-dimensional subspace of RD and P = {u ∈RD |
ui ≥0, ∀i ∈[D]}. Let u⋆be an arbitrary point in P and Q = P ∩(u⋆+ S). Then there exists
a constant c ∈(0, 1] such that for any sufﬁciently small radius r > 0, c · Q weakly dominates
P ∩(u⋆+ S + B2
r(0)), where B2
r(0) is the ℓ2-norm ball of radius r centered at 0."
N,0.810126582278481,"Proof of Lemma D.15. We will prove by induction on the environment dimension D. For the base
case of D = 1, either S = {0} or S = R. S = R is straight-forward; for the case S = {0}, we just
need to ensure c|u⋆| ≤|u⋆| −r, and it sufﬁces to pick r = |u⋆| and c = 0.5."
N,0.8109177215189873,"Suppose the proposition holds for D −1, below we show it holds for D. For each i ∈[D], we
ﬁrst consider the intersection between P ∩(u⋆+ S + B2
r(0)) and Hi := {u ∈RD | ui = 0}. Let
ui be an arbitrary point in P ∩(u⋆+ S) ∩Hi, then P ∩(u⋆+ S) ∩Hi = P ∩(ui + S) ∩Hi =
P ∩(ui + S ∩Hi). Furthermore, there exists {αi}i∈[D] which only depends on S and satisﬁes
P ∩(u∗+S +B2
r(0))∩Hi ⊂P ∩(ui +S ∩Hi +B2
αir(0)∩Hi). Applying the induction hypothesis
to P ∩(ui + S ∩Hi + B2
αir(0) ∩Hi), we know there exists a c > 0 such that for sufﬁciently small r,
c(P ∩(u⋆+S)∩Hi) = c(P ∩(ui +S ∩Hi)) weakly dominates P ∩(ui +S ∩Hi +B2
αir(0)∩Hi)."
N,0.8117088607594937,"For any point v in Q and any z ∈B2
r(0), we take a random direction in S, denoted by ω. If ω ≥0
or ω ≤0, we denote by y the ﬁrst intersection between {v + z −λ|ω|}λ≥0 and the boundary of U.
Clearly y ≤v. Since y ∈P ∩(u⋆+ S + B2
r(0)) ∩Hi ⊂P ∩(ui + S ∩Hi + B2
αir(0) ∩Hi), by
the induction hypothesis, there exists a u ∈c(P ∩(u⋆+ S) ∩Hi) such that u ≤y. Thus z ≤v + z
and z ∈c(P ∩(u⋆+ S)) = c · Q."
N,0.8125,"If ω has different signs across its coordinates, we take y1, y2 to be the ﬁrst intersections of the line
{v + z −λ|ω|}λ∈R and the boundary of U in directions of λ > 0 and λ < 0, respectively. By the
induction hypothesis, there exist u1, u2 ∈c · Q such that u1 ≤y1 and u2 ≤y2. Since v + z lies"
N,0.8132911392405063,Published as a conference paper at ICLR 2022
N,0.8140822784810127,"in the line connecting u1 and u2, there exists some h ∈[0, 1] such that v + z = (1 −h)y1 + hy2.
It then follows that (1 −h)u1 + hu2 ≤(1 −h)y1 + hy2 = v + z. Since Q is convex, we have
(1−h)u1 +hu2 ∈cQ. Therefore, we conclude that cQ∩Q weakly dominates P ∩(u⋆+S +B2
r(0))
for all sufﬁciently small r, and thus the proposition holds for D. This completes the proof by
induction."
N,0.814873417721519,"Lemma D.16. (Polyak-Łojasiewicz condition for F.) For any x∗such that L(x∗) = 0, i.e., x∗∈Γ,
there exist a neighbourhood U ′ of x∗and a constant c > 0, such that ∥F(x)∥2
2 ≥c · max(R(x) −
R(x∗), 0) for all x ∈U ′ ∩Γ. Note this requirement is only non-trivial when ∥F(x∗)∥2 = 0 since F
is continuous."
N,0.8156645569620253,"Proof of Lemma D.16. It sufﬁces to show the PL condition for {x | F(x) = 0}. We need to show
for any x∗satisfying F(x∗) = 0, there exist some ϵ > 0 and C > 0, such that for all x ∈Γ ∩B2
ϵ (x∗)
with R(x) > R(x∗), it holds that ∥F(x)∥2
2 ≥C(R(x) −R(x∗))."
N,0.8164556962025317,"Case I.
We ﬁrst prove the case where x =
 u
v

itself is a canonical parametrization of w =
u⊙2 −v⊙2, i.e., ujvj = 0 for all j ∈[d]. Since x∗satisﬁes ∇F(x∗) = 0, by Lemma D.11, we have
x∗= ψ(w∗) where w∗= (u∗)⊙2 −(v∗)⊙2. In this case, we can rewrite both R and F as functions of
w ∈Rd. In detail, we deﬁne R′(w) = R(ψ(w)) and F ′(w) = F(ψ(w)) for all w ∈Rd. For any w
in a sufﬁciently small neighbourhood of w∗, it holds that sign(wj) = sign(w∗
j ) for all j ∈[q]. Below
we show that for each possible sign pattern of w(q+1):d, there exists some constant C which admits
the PL condition in the corresponding orthant. Then we take the minimum of all C from different
orthant and the proof is completed. W.L.O.G., we assume that wj ≥0, for all j = q + 1 . . . , d."
N,0.817246835443038,"We temporarily reorder the coordinates as x = (u1, v1, u2, v2, . . . , ud, vd)⊤. Recall that Z =
[z1, . . . , zn]⊤is a n-by-d matrix, and we have"
N,0.8180379746835443,"∥F ′(w)∥2
2 = min
λ∈Rn

(a −sign(w) ⊙Z⊤λ)⊙2, |w|

,"
N,0.8188291139240507,where a = 8
N,0.819620253164557,"n
Pn
i=1 z⊙2
i
∈Rd. Since F(x∗) = 0, there must exist λ∗∈Rn, such that the ﬁrst 2q
coordinates of ∇R(x∗) + Pn
i=1 λ∗
i ∇fi(x∗) are equal to 0. As argued in the proof of Lemma D.12,
we can assume the ﬁrst q′ rows of Z are linear independent on the ﬁrst q coordinates for some q′ ∈[q]."
N,0.8204113924050633,"In other words, Z can be written as

ZA
ZB
0
ZD"
N,0.8212025316455697,"
where ZA ∈Rq′×q. We further denote λa := λ1:q′,"
N,0.821993670886076,"λb := λ(q′+1):n, wa := w1:q and wb := w(q+1):d for convenience, then we have"
N,0.8227848101265823,"∥F ′(w)∥2
2 = min
λ∈Rn

(a1 + sign(wa) ⊙Z⊤
Aλa)⊙2, |wa|

+

(a2 + Z⊤
Bλa + Z⊤
Dλb)⊙2, wb

. (57)"
N,0.8235759493670886,"Since every w in Γ is a global minimizer, R′(w) = R′(w)+Pn
i=1 λ∗
i (z⊤
i w −yi) := g⊤w +R′(w∗),
where g = sign(w) ⊙a + Z⊤λ∗. Similarly we deﬁne ga := g1:q and gb := g(q+1):d. It holds that
ga = 0 and we assume ZDgb = 0 without loss of generality, because this can always be done by
picking suitable λ∗
i for i = q′ + 1, . . . , n. (We have such freedom on λ∗
q′+1:n because they doesn’t
affect the ﬁrst 2q coordinates.)"
N,0.8243670886075949,"We denote λa −λ∗
a by ∆λa, then since 0 = ga = sign(wa) ⊙a1 + Z⊤
Aλ∗
a, we further have

(a1 + sign(wa) ⊙Z⊤
Aλa)⊙2, |wa|

=

(a1 + sign(wa) ⊙Z⊤
Aλ∗
a + sign(wa) ⊙Z⊤
A∆λa)⊙2, |wa|"
N,0.8251582278481012,"=

(sign(wa) ⊙Z⊤
A∆λa)⊙2, |wa|

."
N,0.8259493670886076,"On the other hand, we have gb = sign(wb) ⊙a2 + Z⊤
Bλ∗
a + Z⊤
Dλ∗
b = a2 + Z⊤
Bλ∗
a + Z⊤
Dλ∗
b by the
assumption that each coordinate of wb is non-negative. Combining this with the above identity, we
can rewrite Equation (57) as:"
N,0.8267405063291139,"∥F ′(w)∥2
2 = min
λ∈RD

(Z⊤
A∆λa)⊙2, |wa|

+

(gb + Z⊤
B∆λa + Z⊤
Dλb)⊙2, wb

.
(58)"
N,0.8275316455696202,"Now suppose R′(w) −R′(w∗) = g⊤
b wb = δ for some sufﬁciently small δ (which can be controlled
by ϵ). We will proceed in the following two cases separately."
N,0.8283227848101266,Published as a conference paper at ICLR 2022
N,0.8291139240506329,"• Case I.1:
∥∆λa∥2
= Ω(
√"
N,0.8299050632911392,"δ).
Since ZA has full row rank,
(Z⊤
A∆λa)⊙2
1
=
(Z⊤
A∆λa)
2
2 ≥∥∆λa∥2
2 λ2
min(ZA) is lower-bounded. On the other hand, we can choose ϵ
small enough such that ∀i ∈[q]|(wa)2
i | ≥1"
N,0.8306962025316456,"2(w∗
a)2
i . Thus the ﬁrst term of Equation (58) is
lower bounded by ∥∆λa∥2
2 λ2
min(ZA) · mini∈[q]
1
2(w∗
a)2
i = Ω(δ) = Ω(R′(w) −R′(w∗))."
N,0.8314873417721519,"• Case I.2: ∥∆λa∥2 = O(
√"
N,0.8322784810126582,"δ). Let u = gb+Z⊤
B∆λa+Z⊤
Dλb, then we have u ∈S+B2
c
√ δ(0)"
N,0.8330696202531646,"for some constant c > 0, where S = {gb + Z⊤
Dλb | λb ∈Rn−q′}. By Lemma D.14, there
exists some constant c0 ≥1, such that
1
c0 · S weakly dominates S + B2
c
√"
N,0.8338607594936709,δ(0). Thus we
N,0.8346518987341772,"have ∥F ′(w)∥2
2 ≥infu∈S+Bc
√"
N,0.8354430379746836,"δ(0)

u⊙2, wb

≥infu∈1"
N,0.8362341772151899,"c0 ·S

s⊙2, wb

, where the last step
is because each coordinate of wb is non-negative."
N,0.8370253164556962,"Let A be the orthogonal complement of span(ZD, gb), i.e., the spanned space of columns
of ZD and gb, we know wb ∈
δ
∥gb∥2
2 gb + A, since ZDwb = ZDw2
∗= 0 and g⊤
b wb = δ.
Therefore,"
N,0.8378164556962026,"inf
w:R′(w)−R′(w∗)=δ>0
∥F ′(w)∥2
2
R′(w) −R′(w∗) ≥
inf
wb:R′(w)−R′(w∗)=δ>0
inf
u∈1 c0 ·S"
N,0.8386075949367089,"D
u⊙2, wb δ E ≥1"
N,0.8393987341772152,"c2
0
inf
wb∈
δ
∥gb∥2
2
gb+A,wb≥0,u∈S"
N,0.8401898734177216,"u⊙2, wb

.
(59)"
N,0.8409810126582279,"Note

u⊙2, wb

is a monotone non-decreasing function in the ﬁrst joint orthant, i.e.,
{(u, wb) ∈Rd × Rd−q′ | u ≥0, wb ≥0}, thus by Lemma D.15 the inﬁnimum can
be achieved by some ﬁnite (u, wb) in the joint ﬁrst orthant. Applying the same argument to
each other orthant of u ∈Rd, we conclude that the right-hand-side of (59) can be achieved."
N,0.8417721518987342,"On the other hand, we have u⊤wb = δ > 0 for all wb ∈
δ
∥gb∥2
2 gb + A and u ∈S, by
ZDgb = 0 and the deﬁnition of A. This implies there exists at least one i ∈[d −q′]
such that w2,iui > 0, which further implies

u⊙2, wb

> 0. Therefore, we conclude that
∥F ′(w)∥2
2 = Ω(R′(w) −R′(w0))."
N,0.8425632911392406,"Case II.
Next, for any general x =
 u
v

, we deﬁne w = u⊙2 −v⊙2 and m = min{u⊙2, v⊙2},
where min is taken coordinate-wise. Then we can rewrite ∥F(x)∥2
2 as"
N,0.8433544303797469,"∥F(x)∥2
2 = min
λ∈Rn "
N,0.8441455696202531,"
a
a"
N,0.8449367088607594,"
+

Z
−Z"
N,0.8457278481012658,"
λ

⊙

u
v  2 2"
N,0.8465189873417721,"= min
λ∈Rn "
N,0.8473101265822784,"
a
a"
N,0.8481012658227848,"
+

Z
−Z"
N,0.8488924050632911,"
λ
⊙2
⊙

u⊙2 v⊙2"
N,0.8496835443037974,"
1"
N,0.8504746835443038,"= min
λ∈Rn "
N,0.8512658227848101,"
a
a"
N,0.8520569620253164,"
+

Z
−Z"
N,0.8528481012658228,"
λ
⊙2
⊙

ψ(w)⊙2 +

m
m"
N,0.8536392405063291,"
1"
N,0.8544303797468354,"≥min
λ∈Rn "
N,0.8552215189873418,"
a
a"
N,0.8560126582278481,"
+

Z
−Z"
N,0.8568037974683544,"
λ
⊙2
⊙ψ(w)⊙2

1
+ min
λ∈Rn "
N,0.8575949367088608,"
a
a"
N,0.8583860759493671,"
+

Z
−Z"
N,0.8591772151898734,"
λ
⊙2
⊙

m
m"
N,0.8599683544303798,"
1"
N,0.8607594936708861,"= min
λ∈Rn "
N,0.8615506329113924,"
a
a"
N,0.8623417721518988,"
+

Z
−Z"
N,0.8631329113924051,"
λ

⊙ψ(w) 2"
N,0.8639240506329114,"2
+ min
λ∈Rn "
N,0.8647151898734177,"
a
a"
N,0.865506329113924,"
+

Z
−Z"
N,0.8662974683544303,"
λ

⊙
√m
√m  2 2
."
N,0.8670886075949367,Published as a conference paper at ICLR 2022
N,0.867879746835443,"Then applying the result for the previous case yields the following for some constant C ∈(0, 1):"
N,0.8686708860759493,"∥F(x)∥2
2 ≥C(R(ψ(w)) −R(ψ(w∗)) + min
λ∈Rn "
N,0.8694620253164557,"
a
a"
N,0.870253164556962,"
+

Z
−Z"
N,0.8710443037974683,"
λ

⊙
√m
√m  2"
N,0.8718354430379747,"2
= C(R(ψ(w)) −R(x∗) + 2

a⊙2, m"
N,0.872626582278481,"≥C(R(ψ(w)) −R(x∗) + 2 min
i∈[d] ai ⟨a, m⟩"
N,0.8734177215189873,"= C(R(ψ(w)) −R(x∗) + min
i∈[d] ai(R(x) −R(ψ(w)))"
N,0.8742088607594937,"≥min

C, min
i∈[d] ai"
N,0.875,"
(R(x) −R(x∗)),"
N,0.8757911392405063,"where the ﬁrst equality follows from the fact that x∗= ψ(w∗) and the last inequality is due to the
fact that both R(ψ(w) −R(ψ(w∗)) and R(x) −R(ψ(w)) are non-negative. This completes the
proof."
N,0.8765822784810127,"Now, based on the PL condition, we can show that (17) indeed converges.
Lemma D.17. The trajectory of the ﬂow deﬁned in (17) has ﬁnite length, i.e.,
R ∞
t=0 ∥dx"
N,0.877373417721519,"dt ∥2dt < ∞
for any x∗∈Γ. Moreover, x(t) converges to some x(∞) when t →∞with F(x(∞)) = 0."
N,0.8781645569620253,"Proof of Lemma D.17. Note that along the Riemannian gradient ﬂow, R(x(t)) is non-increasing,
thus ∥x(t)∥2 is bounded over time and {x(t)}t≥0 has at least one limit point, which we will call x∗.
Therefore, R(x∗) is a limit point of R(x(t)), and again since R(x(t)) is non-increasing, it follows
that R(x(t)) ≥R(x∗) and limt→∞R(x(t)) = R(x∗). Below we will show limt→∞x(t) = x∗."
N,0.8789556962025317,Note that dR(x(t))
N,0.879746835443038,"dt
=
D
∇R(x(t)), dx(t)"
N,0.8805379746835443,"dt
E
= −

∇R(x(t)), 1"
N,0.8813291139240507,"4F(x(t))

= −1"
N,0.882120253164557,"4 ∥F(x(t))∥2
2 where
the last equality applies Lemma D.10. By Lemma D.16, there exists a neighbourhood of x∗, U ′,
in which PL condition holds of F. Since x∗is a limit point, there exists a time T0, such that
xT0 ∈U. Let T1 = inft≥T0{x(t) /∈U ′} (which is equal to ∞if x(t) ∈U ′ for all t ≥T0).
Since x(t) is continuous in t and U is open, we know T1 > T0 and for all t ∈[T0, T1), we have
∥F(x(t))∥2 ≥√c(R(x(t)) −R(x∗))1/2."
N,0.8829113924050633,"Thus it holds that for t ∈[T0, T1),"
N,0.8837025316455697,d(R(x(t)) −R(x∗))
N,0.884493670886076,"dt
≤−
√c"
N,0.8852848101265823,"4 (R(x(t)) −R(x∗))1/2 ∥F(x(t))∥2 ,"
N,0.8860759493670886,"that is,"
N,0.8868670886075949,d(R(x(t)) −R(x∗))1/2
N,0.8876582278481012,"dt
≤−
√c"
N,0.8884493670886076,8 ∥F(x(t))∥2 .
N,0.8892405063291139,"Therefore, we have
Z T1"
N,0.8900316455696202,"t=T0
∥F(x(t))∥2 dt ≤8
√c(R(x(T0)) −R(x∗))1/2.
(60)"
N,0.8908227848101266,"Thus if we pick T0 such that R(x(T0)) −R(x∗) is sufﬁciently small, R(T1) will remain in U, which
implies that T1 cannot be ﬁnite and has to be ∞. Therefore, Equation (60) shows that the trajectory
of x(t) is of ﬁnite length, so x(∞) := limt→∞x(t) exists and is equal to x∗. As a by-product, F(x∗)
must be 0."
N,0.8916139240506329,"Finally, collecting all the above lemmas, we are able to prove Lemma 6.5. In Lemma D.17 we already
show the convergence of x(t) as t →∞, the main part of the proof of Lemma 6.5 is to show the
x(∞) cannot be sub-optimal stationary points of R on Γ, the closure of Γ. The key idea here is that
we can construct a different potential φ for each such sub-optimal stationary point x∗, such that (1)
φ(xt) is locally increasing in a sufﬁciently neighborhood of x∗and (2) limx→x∗φ(x) = −∞.
Lemma 6.5. Let {xt}t≥0 ⊆RD be generated by the ﬂow deﬁned in (17) with any initialization
x0 ∈Γ. Then x∞= limt→∞xt exists. Moreover, x∞= x∗is the optimal solution of (18)."
N,0.8924050632911392,Published as a conference paper at ICLR 2022
N,0.8931962025316456,"Proof of Lemma 6.5. We will prove by contradiction. Suppose x(∞) =
 u(∞)
v(∞)

= limt→∞x(t) is
not the optimal solution to (18). Denote w(t) = (u(t))⊙2 −(v(t))⊙2, then w(∞) = limt→∞w(t)
is not the optimal solution to (36). Thus we have R(w(t)) > R(w∗). Without loss of generality,
suppose there is some q ∈[d] such that (ui(∞))2 + (vi(∞))2 > 0 for all i = 1, . . . , q and
ui(∞) = vi(∞) = 0 for all i = q + 1, . . . , d. Again, as argued in the proof of Lemma D.12, we can
assume that, for some q′ ∈[q],"
N,0.8939873417721519,"{zi,1:q}i∈[q′] is linearly independent and zi,1:q = 0 for all i = q′ + 1, . . . , n.
(61)"
N,0.8947784810126582,"Since both w(∞) and w∗satisfy the constraint that Zw(∞) = Zw∗= Y , we further have"
N,0.8955696202531646,"0 = ⟨zi, w(∞)⟩= ⟨zi, w∗⟩= ⟨zi,(q+1):d, w∗
(q+1):d⟩,
for all i = q′ + 1, . . . , n.
(62)"
N,0.8963607594936709,Consider a potential function ϕ : U →R deﬁned as
N,0.8971518987341772,"ϕ(x) = ϕ(u, v) = d
X"
N,0.8979430379746836,"j=q+1
w∗
j

ln(uj)21{w∗
j > 0} −ln(vj)21{w∗
j < 0}

."
N,0.8987341772151899,"Clearly limt→∞ϕ(x(t)) = −∞if limt→∞x(t) = x(∞). Below we will show contradiction if
x(∞) is suboptimal. Consider the dynamics of ϕ(x) along the Riemannian gradient ﬂow: dϕ"
N,0.8995253164556962,"dt (x(t)) =

∇ϕ(x(t)), dx(t) dt"
N,0.9003164556962026,"= −

∇ϕ(x(t)), 1"
N,0.9011075949367089,"4F(x(t))

(63)"
N,0.9018987341772152,"where F is deﬁned previously in Lemma D.10. Recall the deﬁnition of F, and we have"
N,0.9026898734177216,"⟨∇ϕ(x(t)), F(x(t))⟩= *"
N,0.9034810126582279,"∇ϕ(x(t)), 1"
N,0.9042721518987342,"4∇R(x(t)) + 1 4 q′
X"
N,0.9050632911392406,"i=1
λi(x(t))∇fi(x(t)) +"
N,0.9058544303797469,"|
{z
}
I1 + *"
N,0.9066455696202531,"∇ϕ(x(t)), 1 4 n
X"
N,0.9074367088607594,"i=q′+1
λi(x(t))∇fi(x(t)) +"
N,0.9082278481012658,"|
{z
}
I2"
N,0.9090189873417721,".
(64)"
N,0.9098101265822784,"To show ⟨∇ϕ(x(t)), F(x(t))⟩< 0, we analyze I1 and I2 separately. By the deﬁnition of ϕ(x), we
have"
N,0.9106012658227848,"∇ϕ(x) = d
X"
N,0.9113924050632911,"j=q+1
2w∗
j"
N,0.9121835443037974,"1{w∗
j > 0}
uj
· ej −1{w∗
j < 0}
vj
· eD+j "
N,0.9129746835443038,"where ej is the j-th canonical base of Rd. Recall that ∇fi(x) = 2
  zi⊙u
−zi⊙v

, and we further have I2 = n
X"
N,0.9137658227848101,"i=q′+1
λi(x(t)) d
X"
N,0.9145569620253164,"j=q+1
w∗
j"
N,0.9153481012658228,"1{w∗
j > 0}
uj
⟨ej, zi ⊙u⟩+ 1{w∗
j < 0}
vj
⟨ej, zi ⊙v⟩
 = n
X"
N,0.9161392405063291,"i=q′+1
λi(x(t)) d
X"
N,0.9169303797468354,"j=q+1
w∗
j"
N,0.9177215189873418,"1{w∗
j > 0}
uj
zi,juj + 1{w∗
j < 0}
vj
zi,jvj  = n
X"
N,0.9185126582278481,"i=q′+1
λi(x(t)) d
X"
N,0.9193037974683544,"j=q+1
w∗
j zi,j = n
X"
N,0.9200949367088608,"i=q′+1
λi(x(t))⟨zi,(q+1):d, w∗
(q+1):d⟩= 0
(65)"
N,0.9208860759493671,where the last equality follows from (62).
N,0.9216772151898734,"Next, we show that I1 < 0 by utilizing the fact that w∗−w(∞) is a descent direction of R′(w). For
w ∈Rd, deﬁne ˜fi(w) = z⊤
i w and"
N,0.9224683544303798,"˜R(w) = R(w) + q′
X"
N,0.9232594936708861,"i=1
λi(x(∞))( ˜fi(w) −yi)."
N,0.9240506329113924,Published as a conference paper at ICLR 2022
N,0.9248417721518988,"Clearly, for any w ∈RD satisfying Zw = Y , it holds that ˜fi(w) −yi = 0 for each i ∈[n], and thus
R(w) = ˜R(w). In particular, we have ˜R(w(∞)) = R(w(∞)) > R(w∗) = ˜R(w∗). Since ˜R(w) is a
convex function, it follows that ˜R(w(∞)+s(w∗−w(∞))) ≤s ˜R(w∗)+(1−s) ˜R(∞) < ˜R(w(∞))
for all 0 < s ≤1, which implies d ˜
R
dt (w(∞) + s(w∗−w(∞)))|s=0 < −2c < 0+ for some constant
c > 0. Note that, for small enough s > 0, we have"
N,0.9256329113924051,"R(w(∞) + s(w∗−w(∞))) = 4 n d
X j=1 n
X"
N,0.9264240506329114,"i=1
z2
i,j !"
N,0.9272151898734177,"|wj(∞) + s(w∗
j −wj(∞))| = 4 n q
X j=1 n
X"
N,0.928006329113924,"i=1
z2
i,j !"
N,0.9287974683544303,"sign(wj(∞))(wj(∞) + s(w∗
j −wj(∞))) + 4 n d
X j=q+1 n
X"
N,0.9295886075949367,"i=1
z2
i,j !"
N,0.930379746835443,"s|w∗
j |."
N,0.9311708860759493,"Therefore, we can compute the derivative with respect to s at s = 0 as"
N,0.9319620253164557,−2c > d ˜R
N,0.932753164556962,"dt (w(∞) + s(w∗−w(∞)))

s=0 = 4 n q
X j=1 n
X"
N,0.9335443037974683,"i=1
z2
i,j !"
N,0.9343354430379747,"sign(wj(∞))(w∗
j −wj(∞)) + 4 n d
X j=q+1 n
X"
N,0.935126582278481,"i=1
z2
i,j !"
N,0.9359177215189873,"|w∗
j | + q′
X"
N,0.9367088607594937,"i=1
λi(x(∞))z⊤
i (w∗−wj(∞)) = 4 n q
X j=1 n
X"
N,0.9375,"i=1
z2
i,j !"
N,0.9382911392405063,"sign(wj(∞))(w∗
j −w(∞)) + 4 n d
X j=q+1 n
X"
N,0.9390822784810127,"i=1
z2
i,j !"
N,0.939873417721519,"|w∗
j | + q
X"
N,0.9406645569620253,"j=1
(w∗
j −wj(∞)) q′
X"
N,0.9414556962025317,"i=1
λi(x(∞))zi,j + d
X"
N,0.942246835443038,"j=q+1
w∗
j q′
X"
N,0.9430379746835443,"i=1
λi(x(∞))zi,j
(66)"
N,0.9438291139240507,"where the second equality follows from the fact that w(q+1):d(∞) = 0. Since x(t) converges to
x(∞), we must have F(x(∞)) = 0, which implies that for each j ∈{1, . . . , q},"
N,0.944620253164557,0 = ∂R
N,0.9454113924050633,"∂uj
(x(∞)) + q′
X"
N,0.9462025316455697,"i=1
λi(x(∞)) ∂fi"
N,0.946993670886076,"∂uj
(x(∞)) = 2uj(∞)  4 n n
X"
N,0.9477848101265823,"i=1
z2
i,j + q′
X"
N,0.9485759493670886,"i=1
λi(x(∞))zi,j  ,"
N,0.9493670886075949,0 = ∂R
N,0.9501582278481012,"∂vj
(x(∞)) + q′
X"
N,0.9509493670886076,"i=1
λi(x(∞)) ∂fi"
N,0.9517405063291139,"∂vj
(x(∞)) = 2vj(∞)  4 n n
X"
N,0.9525316455696202,"i=1
z2
i,j − q′
X"
N,0.9533227848101266,"i=1
λi(x(∞))zi,j  ."
N,0.9541139240506329,Combining the above two equalities yields
N,0.9549050632911392,"4
n n
X"
N,0.9556962025316456,"i=1
z2
i,j = −sign(wj(∞)) q′
X"
N,0.9564873417721519,"i=1
λi(x(∞))zi,j,
for all j ∈[q]."
N,0.9572784810126582,"Apply the above identity together with (66), and we obtain −2c > q
X"
N,0.9580696202531646,"j=1
−sign(wj(∞))2(w∗
j −w(∞)) q′
X"
N,0.9588607594936709,"i=1
λi(x(∞))zi,j + 4 n d
X j=q+1 n
X"
N,0.9596518987341772,"i=1
z2
i,j !"
N,0.9604430379746836,"|w∗
j | + q
X"
N,0.9612341772151899,"j=1
(w∗
j −wj(∞)) q′
X"
N,0.9620253164556962,"i=1
λi(x(∞))zi,j + d
X"
N,0.9628164556962026,"j=q+1
w∗
j q′
X"
N,0.9636075949367089,"i=1
λi(x(∞))zi,j = 4 n d
X j=q+1 n
X"
N,0.9643987341772152,"i=1
z2
i,j !"
N,0.9651898734177216,"|w∗
j | + d
X"
N,0.9659810126582279,"j=q+1
w∗
j q′
X"
N,0.9667721518987342,"i=1
λi(x(∞))zi,j
(67)"
N,0.9675632911392406,Published as a conference paper at ICLR 2022
N,0.9683544303797469,"On the other hand, by directly evaluating ∇R(x(t)) and each ∇fi(x(t)), we can compute I1 as I1 = d
X j=q+1"
N,0.9691455696202531,"w∗
j 1{w∗
j > 0}
uj(t)  2 n n
X"
N,0.9699367088607594,"i=1
z2
i,juj(t) + 1 2 q′
X"
N,0.9707278481012658,"i=1
λi(x(t))zi,juj(t)   − d
X j=q+1"
N,0.9715189873417721,"w∗
j 1{w∗
j < 0}
vj(t)  2 n n
X"
N,0.9723101265822784,"i=1
z2
i,jvj(t) −1 2 q′
X"
N,0.9731012658227848,"i=1
λi(x(t))zi,jvj(t)   = 2 n d
X j=q+1 n
X"
N,0.9738924050632911,"i=1
z2
i,j !"
N,0.9746835443037974,"|w∗
j | + 1 2 d
X"
N,0.9754746835443038,"j=q+1
w∗
j q′
X"
N,0.9762658227848101,"i=1
λi(x(t))zi,j = 2 n d
X j=q+1 n
X"
N,0.9770569620253164,"i=1
z2
i,j !"
N,0.9778481012658228,"|w∗
j | + 1 2 d
X"
N,0.9786392405063291,"j=q+1
w∗
j q′
X"
N,0.9794303797468354,"i=1
λi(x(∞))zi,j + 1 2 d
X"
N,0.9802215189873418,"j=q+1
w∗
j q′
X"
N,0.9810126582278481,"i=1
(λi(x(t)) −λi(x(∞))) zi,j."
N,0.9818037974683544,"We already know that λ1:q′(x) is continuous at x(∞) by the proof of Lemma D.12, so the third term
converges to 0 as x(t) tends to x(∞). Now, applying (67), we immediately see that there exists some
δ > 0 such that I1 < −c for x(t) ∈Bδ(x(∞)). As we have shown in the above that I2 = 0, it then
follows from (63) and (64) that dϕ"
N,0.9825949367088608,"dt (x(t)) > c,
for all x(t) ∈Bδ(x(∞)).
(68)"
N,0.9833860759493671,"Since limt→∞x(t) = x(∞), there exists some T > 0 such that x(t) ∈Bδ(x(∞)) for all t > T. By
the proof ofLemma D.13, we know that ϕ(x(T)) > −∞, then it follows from (68) that"
N,0.9841772151898734,"lim
t→∞ϕ(x(t)) = ϕ(x(T)) +
Z ∞ T"
N,0.9849683544303798,dϕ(x(t))
N,0.9857594936708861,"dt
dt > ϕ(x(T)) +
Z ∞"
N,0.9865506329113924,"T
cdt = ∞"
N,0.9873417721518988,which is a contradiction. This ﬁnishes the proof.
N,0.9881329113924051,"D.6
PROOF OF THEOREM 6.7"
N,0.9889240506329114,Here we present the lower bound on the sample complexity of GD in the kernel regime.
N,0.9897151898734177,"Theorem 6.7. Assume z1, . . . , zn
i.i.d.
∼N(0, Id) and yi = z⊤
i w∗, for all i ∈[n]. Deﬁne the loss
with linearized model as L(x) = Pn
i=1(fi(x0) + ⟨∇fi(x0), x −x0⟩−yi)2, where x =
 u
v

and
x0 =
 u0
v0

= α
 1
1

. Then for any groundtruth w∗, any learning rate schedule {ηt}t≥1, and any ﬁxed
number of steps T, the expected ℓ2 loss of x(T) is at least (1 −n"
N,0.990506329113924,"d ) ∥w∗∥2
2, where x(T) is the T-th
iterate of GD on L, i.e., x(t + 1) = x(t) −ηt∇L(x(t)), for all t ≥0."
N,0.9912974683544303,"Proof of Theorem 6.7. We ﬁrst simplify the loss function by substituting x′ = x −x(0), so corre-
spondingly x′
0 = 0 and we consider L′(x′) := L(x) = (⟨∇fi(x(0)), x′⟩−yi)2. We can think as if
GD is performed on L′(x′). For simplicity, we still use the x and L(x) notation in below."
N,0.9920886075949367,"In order to show test loss lower bound against a single ﬁxed target function, we must take the
properties of the algorithm into account. The proof is based on the observation that GD is rotationally
equivariant (Ng, 2004; Li et al., 2020c) as an iterative algorithm, i.e., if one rotates the entire data
distribution (including both the training and test data), the expected loss of the learned function
remains the same. Since the data distribution and initialization are invariant under any rotation, it
means the expected loss of x(T) with ground truth being w∗is the same as the case where the ground
truth is uniformly randomly sampled from all vectors of ℓ2-norm ∥w∗∥2."
N,0.992879746835443,Thus the test loss of x(T) is
N,0.9936708860759493,"Ez

(⟨∇fz(x(0)), x(T)⟩−⟨z, w∗⟩)2
= Ez
h
(⟨z, w∗−(u(T) −v(T))⟩)2i
= ∥w∗−(u(T) −v(T))∥2
2 . (69)"
N,0.9944620253164557,Published as a conference paper at ICLR 2022
N,0.995253164556962,"Note x(T) ∈span{∇fx(x(0))}, which is at most an n-dimensional space spanned by the gradients
of model output at x(0), so is u(T) −v(T). We denote the corresponding space for u(T) −v(T)
by S, so dim(S) ≤n and it holds that ∥w∗−(u(T) −v(T))∥2
2 ≥∥(ID −PS)w∗∥2
2, where PS is
projection matrix onto space S."
N,0.9960443037974683,The expected test loss is lower bounded by
N,0.9968354430379747,"Ew∗
h
Ezi
h
∥w∗−(u(T) −v(T))∥2
2
ii
= Ezi
h
Ew∗
h
∥w∗−(u(T) −v(T))∥2
2
ii"
N,0.997626582278481,"≥
min
{zi}i∈[n]
Ew∗
h
∥(ID −PS)w∗∥2
2
i"
N,0.9984177215189873,"≥

1 −n d"
N,0.9992088607594937,"
∥w∗∥2
2 ."
