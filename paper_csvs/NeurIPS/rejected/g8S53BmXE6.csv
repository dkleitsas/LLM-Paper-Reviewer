Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002136752136752137,"Predicting the structure of interacting chains is crucial for understanding biological
1"
ABSTRACT,0.004273504273504274,"systems and developing new drugs. Large-scale Pre-trained Protein Language
2"
ABSTRACT,0.00641025641025641,"models (PLMs), such as ESM-2, have shown an impressive ability to extract
3"
ABSTRACT,0.008547008547008548,"biologically meaningful representations for protein contact and structure prediction.
4"
ABSTRACT,0.010683760683760684,"In this paper, we show that ESMFold, which has been successful in computing
5"
ABSTRACT,0.01282051282051282,"accurate atomic structures for single-chain proteins, can be adapted to predict the
6"
ABSTRACT,0.014957264957264958,"heterodimer structures in a lightweight manner. We propose Linker-tuning, which
7"
ABSTRACT,0.017094017094017096,"learns a continuous prompt to connect the two chains in a dimer before running
8"
ABSTRACT,0.019230769230769232,"it as a single sequence in ESMFold. Experiment results show that our method is
9"
ABSTRACT,0.021367521367521368,"significantly better than the ESMFold-Linker baseline, with relative improvements
10"
ABSTRACT,0.023504273504273504,"of +28.13% and +54.55% in DockQ score on the i.i.d heterodimer test set and
11"
ABSTRACT,0.02564102564102564,"the out-of-distribution (OOD) test set HeteroTest2, respectively. Notably, on the
12"
ABSTRACT,0.027777777777777776,"antibody heavy chain light chain (VH-VL) test set, our method successfully predicts
13"
ABSTRACT,0.029914529914529916,"all the heavy chain light chain docking interfaces, with 46/68 medium-quality and
14"
ABSTRACT,0.03205128205128205,"22/68 high-quality predictions, while being 9× faster than AF-Multimer.
15"
INTRODUCTION,0.03418803418803419,"1
Introduction
16"
INTRODUCTION,0.03632478632478633,"Proteins are large biomolecules essential to life. They are sequences compromised of 20 types of
17"
INTRODUCTION,0.038461538461538464,"amino acids and fold into three-dimensional (3D) structures to carry out functions. Predicting the
18"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.0405982905982906,"3D structures of proteins from amino acid sequences is a long-standing challenge in computational
19"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.042735042735042736,"biology. It is important for the mechanical understanding of protein functions as well as for designing
20"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.04487179487179487,"new drugs. In 2021, AlphaFold2 (AF2) strikes a huge success in solving this challenge, achieving
21"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.04700854700854701,"near experimental accuracy on protein structure prediction [1]. However, this system heavily relies
22"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.049145299145299144,"on Multiple Sequence Alignments (MSAs) to extract the evolutionary information, but MSAs are not
23"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.05128205128205128,"always available or high quality, especially for orphan proteins and fast-evolving antibodies [2].
24"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.053418803418803416,"Inspired by the success of transformer language models in the field of Natural Language Processing
25"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.05555555555555555,"(NLP), there is a line of work resorting to large-scale PLMs for protein structure prediction [2, 3, 4, 5].
26"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.057692307692307696,"These PLM-based models, such as ESMFold [3], take only amino acid sequences as input, eliminating
27"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.05982905982905983,"the need for MSAs. Powered by PLMs, they show strong abilities in capturing protein structure
28"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.06196581196581197,"information [6, 7]. And they are able to predict protein 3D structures at the atomic level with high
29"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.0641025641025641,"accuracy while being an order of magnitude faster than AF2. However, these models are developed
30"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.06623931623931624,"for predicting the structures of single-chain proteins and it is not clear how to use them to predict
31"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.06837606837606838,"multi-chain protein structures.
32"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.07051282051282051,"To adapt these models for protein complex prediction, some researchers have proposed to use a
33"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.07264957264957266,"poly-Glycine linker to join chains and input the linked sequences to the model to predict complex
34"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.07478632478632478,"structures [8, 9]. The rationale is that the model should identify the linker segment as unstructured
35"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.07692307692307693,"and fold the linked sequence in a similar way to multiple chains. Experimental result on AF2 shows
36"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.07905982905982906,"that this approach is simple yet effective. However, for the PLM-based models, whether a linker is
37"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.0811965811965812,"effective or not for protein complex prediction remains unexplored. In the work of ESMFold, they
38"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.08333333333333333,"briefly mention that they use a 25-residue poly-Glycine linker (denoted as G25 in the following) to
39"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.08547008547008547,"join different chains for a specific protein complex example [3]. But they do not test the performance
40"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.0876068376068376,"of the linker systematically. Based on existing work, we would like to investigate the following
41"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.08974358974358974,"questions in this paper: 1) How well can a G25 linker perform on protein complex prediction? 2)
42"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.09188034188034189,"Can we optimize the linker to achieve a better result? And how?
43"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.09401709401709402,"Viewing proteins as the language of life, linkers in fact are the same things as prompts in natural
44"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.09615384615384616,"language. Inspired by prompt engineering [10, 11] in NLP, we propose Linker-tuning, which is to
45"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.09829059829059829,"automatically learn a linker for the PLM-based model ESMFold on the task of heterodimeric protein
46"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.10042735042735043,"structure prediction. Our goal is to find a linker that can link the two chains of a heterodimer so the
47"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.10256410256410256,"structure prediction model can fold it in a similar way as a single-chain protein. How to best achieve
48"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.1047008547008547,"this goal, however, is non-trivial and remains under-explored for the complicated protein structure
49"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.10683760683760683,"prediction model. Through preliminary analysis, we find that it is better to place linker optimization
50"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.10897435897435898,"at the Folding Module instead of at the PLM, which is different from intuition.
51"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.1111111111111111,"Considering ESMFold is a model with large-scale pre-trained PLM ESM2 that scales up to 15B
52"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.11324786324786325,"parameters, to accelerate the linker learning procedure, we train and select our model on a proxy
53"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.11538461538461539,"task called distogram prediction [12], a task that aims to predict inter-residue distance bins in the 3D
54"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.11752136752136752,"space for each pair of residues in a given protein. After training, we test our learned linker on the 3D
55"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.11965811965811966,"structure prediction task on three datasets to investigate the generalization ability of our method.
56"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.12179487179487179,"In summary, our main contributions are as follows:
57"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.12393162393162394,"• We propose Linker-tuning, a lightweight adaptation method that automatically learns a
58"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.12606837606837606,"linker in the continuous space to adapt the single-chain ESMFold for heterodimer structure
59"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.1282051282051282,"prediction.
60"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.13034188034188035,"• We show that our method outperforms the ESMFold-Linker baseline by large margins on
61"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.13247863247863248,"both contact and structure prediction tasks on the heterodimer test set.
62"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.1346153846153846,"• We find that our method generalizes well to predict heterodimers with low sequence similarity
63"
D STRUCTURES OF PROTEINS FROM AMINO ACID SEQUENCES IS A LONG-STANDING CHALLENGE IN COMPUTATIONAL,0.13675213675213677,"and antibody VH-VL complex.
64"
BIOLOGICAL BACKGROUND,0.1388888888888889,"2
Biological background
65"
BIOLOGICAL BACKGROUND,0.14102564102564102,"Linker
In biology, linkers are short amino acid sequences created in nature to separate multiple
66"
BIOLOGICAL BACKGROUND,0.14316239316239315,"domains in a single protein [13]. Biologists have found that linkers rich in Glycine act as independent
67"
BIOLOGICAL BACKGROUND,0.1452991452991453,"units and do not affect the function of the individual proteins to which they attach [14, 15]. Therefore,
68"
BIOLOGICAL BACKGROUND,0.14743589743589744,"we can use the Glycine-rich linker to join interacting chains to make it a single sequence, hoping it
69"
BIOLOGICAL BACKGROUND,0.14957264957264957,"folds in the way they suppose to. Grounded in biological principles, we further extend the natural
70"
BIOLOGICAL BACKGROUND,0.1517094017094017,"discrete linkers to virtual continuous linkers for better protein complex structure prediction.
71"
BIOLOGICAL BACKGROUND,0.15384615384615385,"Distogram and contact map
The 3D structure of a protein is expressed as (x, y, z) coordinates of
72"
BIOLOGICAL BACKGROUND,0.15598290598290598,"the residues’ atoms in the form of a pdb file [16]. The distance between two residues in a protein
73"
BIOLOGICAL BACKGROUND,0.1581196581196581,"3D structure is defined as the Euclidean distance between their Cβ atoms (Cα for Glycine). Binning
74"
BIOLOGICAL BACKGROUND,0.16025641025641027,"all the inter-residue distances in a protein into k distance bins, we can obtain the distogram matrix
75"
BIOLOGICAL BACKGROUND,0.1623931623931624,"[12]. For a protein with L residues, the distogram d is an L × L matrix, with entry dij referring to
76"
BIOLOGICAL BACKGROUND,0.16452991452991453,"the distance category of residue i and j. In a coarser granularity, we can compute the contact map
77"
BIOLOGICAL BACKGROUND,0.16666666666666666,"c ∈RL×L, where cij = 1 means the distance between residue i and j is less than or equal to 8Å.
78"
BIOLOGICAL BACKGROUND,0.16880341880341881,"For protein complexes, we are especially interested in the inter-chain contact maps where the contacts
79"
BIOLOGICAL BACKGROUND,0.17094017094017094,"are formed by two residues from different chains. The inter-chain contact map reflects the interface
80"
BIOLOGICAL BACKGROUND,0.17307692307692307,"of interacting proteins, which is essential for predicting the 3D structure of the complex.
81"
RELATED WORK,0.1752136752136752,"3
Related work
82"
PROTEIN STRUCTURE PREDICTION,0.17735042735042736,"3.1
Protein structure prediction
83"
PROTEIN STRUCTURE PREDICTION,0.1794871794871795,"Single-chain protein structure prediction
In recent years, single-chain protein structure prediction
84"
PROTEIN STRUCTURE PREDICTION,0.18162393162393162,"has attracted increasing attention from researchers in the Artificial Intelligence (AI) community,
85"
PROTEIN STRUCTURE PREDICTION,0.18376068376068377,"mainly due to the ground-breaking success of the deep learning model AF2. Deep learning based
86"
PROTEIN STRUCTURE PREDICTION,0.1858974358974359,"protein structure prediction methods can be classified into two main categories: 1) MSA-based
87"
PROTEIN STRUCTURE PREDICTION,0.18803418803418803,"methods, such as AF2, that take protein sequences and MSAs as input and predict 3D structures
88"
PROTEIN STRUCTURE PREDICTION,0.19017094017094016,"[1, 17, 18]; 2) PLM-based methods, such as ESMFold, that take only protein sequences as input
89"
PROTEIN STRUCTURE PREDICTION,0.19230769230769232,"and predict 3D structures [3, 2, 4, 5, 19, 20, 21, 22, 23]. PLM-based methods do not rely on MSAs,
90"
PROTEIN STRUCTURE PREDICTION,0.19444444444444445,"which are time-consuming in searching homologs and not always available for some proteins like
91"
PROTEIN STRUCTURE PREDICTION,0.19658119658119658,"orphan proteins. Instead, they adopt large-scale pre-trained PLMs to learn evolutionary and structural
92"
PROTEIN STRUCTURE PREDICTION,0.1987179487179487,"meaningful representations for 3D structure prediction. In this work, we build our method upon PLM-
93"
PROTEIN STRUCTURE PREDICTION,0.20085470085470086,"based methods. Specifically, we adopt ESMFold [3] as the backbone since its code and pre-trained
94"
PROTEIN STRUCTURE PREDICTION,0.202991452991453,"weights are all released and convenient to use. The overall architecture of ESMFold contains two
95"
PROTEIN STRUCTURE PREDICTION,0.20512820512820512,"parts: 1) ESM2: a PLM pre-trained with masked language modeling objective and scales up to 15B
96"
PROTEIN STRUCTURE PREDICTION,0.20726495726495728,"parameters; 2) Folding Module: contains Folding Trunk (similar to Evoformer in AF2) and Structure
97"
PROTEIN STRUCTURE PREDICTION,0.2094017094017094,"Module (same as the one in AF2), which are responsible for structure folding.
98"
PROTEIN STRUCTURE PREDICTION,0.21153846153846154,"Multi-chain protein structure prediction
In biology, multi-chain proteins are protein complexes
99"
PROTEIN STRUCTURE PREDICTION,0.21367521367521367,"formed by interacting single-chain proteins where the interactions are driven by the same physical
100"
PROTEIN STRUCTURE PREDICTION,0.21581196581196582,"forces as protein folding [24]. Recently, there is a line of work repurposing single-chain AF2 for
101"
PROTEIN STRUCTURE PREDICTION,0.21794871794871795,"protein complex structure prediction. The methods can be summarized into two main categories:
102"
PROTEIN STRUCTURE PREDICTION,0.22008547008547008,"1) input-adapted methods that provide AF2 with pseudo-multimer inputs either by adding a large
103"
PROTEIN STRUCTURE PREDICTION,0.2222222222222222,"number to the residue_index between chains to indicate chain break [25, 26, 27, 28] or using a linker
104"
PROTEIN STRUCTURE PREDICTION,0.22435897435897437,"to join chains [8, 9]; and 2) training-adapted methods that retrain AF2 on multimeric proteins, such
105"
PROTEIN STRUCTURE PREDICTION,0.2264957264957265,"as AF-Multimer, the state-of-the-art (SOTA) method [29]. On the one hand, the two types of methods
106"
PROTEIN STRUCTURE PREDICTION,0.22863247863247863,"either do not update any parameters, or update all parameters of the base model, while our method
107"
PROTEIN STRUCTURE PREDICTION,0.23076923076923078,"falls in between, adding only a tiny number of extra parameters to the base model. On the other hand,
108"
PROTEIN STRUCTURE PREDICTION,0.2329059829059829,"existing work mainly focuses on the MSA-based method AF2, with little attention being paid to the
109"
PROTEIN STRUCTURE PREDICTION,0.23504273504273504,"PLM-based methods. In this work, we focus on adapting the PLM-based methods for two-chain
110"
PROTEIN STRUCTURE PREDICTION,0.23717948717948717,"protein structure prediction, which has not yet been explored.
111"
PROMPT ENGINEERING,0.23931623931623933,"3.2
Prompt engineering
112"
PROMPT ENGINEERING,0.24145299145299146,"In the NLP community, with the rise of large-scale pre-trained language models (LMs) such as GPT-3
113"
PROMPT ENGINEERING,0.24358974358974358,"[30], “pre-train, prompt, and predict"" has become a prevalent paradigm to steer the LM to perform a
114"
PROMPT ENGINEERING,0.24572649572649571,"wide range of downstream tasks [10]. In this paradigm, the downstream tasks are reformulated in a
115"
PROMPT ENGINEERING,0.24786324786324787,"form that is similar to the LM pre-training task using a textual prompt [30, 31]. The key challenge in
116"
PROMPT ENGINEERING,0.25,"prompt-based learning is to find the right prompt for a specific task, termed “prompt engineering"".
117"
PROMPT ENGINEERING,0.25213675213675213,"There is a line of work that automatically search the right prompts for downstream tasks [32, 33]. In
118"
PROMPT ENGINEERING,0.25427350427350426,"particular, instead of natural language prompts, some researchers propose to use continuous prompts,
119"
PROMPT ENGINEERING,0.2564102564102564,"directly performing prompting in the embedding space of the LM [34, 11]. In their experiment,
120"
PROMPT ENGINEERING,0.25854700854700857,"continuous prompts achieve strong results in both language understanding and generation tasks. In
121"
PROMPT ENGINEERING,0.2606837606837607,"this work, we follow the idea of continuous prompting, searching for the linkers in the continuous
122"
PROMPT ENGINEERING,0.26282051282051283,"space.
123"
PROMPT ENGINEERING,0.26495726495726496,"4
Method: Linker-tuning
124"
PROMPT ENGINEERING,0.2670940170940171,"To adapt the single-chain model for multi-chain protein structure prediction, we propose a lightweight
125"
PROMPT ENGINEERING,0.2692307692307692,"adaptation method called Linker-tuning and a novel weighted distogram loss. The basic idea of our
126"
PROMPT ENGINEERING,0.27136752136752135,"method is to optimize linkers, i.e., prompts, in the embedding space of ESMFold.
127"
PROBLEM FORMULATION,0.27350427350427353,"4.1
Problem formulation
128"
PROBLEM FORMULATION,0.27564102564102566,"Continuous linker tuning of ESMFold for protein complex structure prediction is a continuous opti-
129"
PROBLEM FORMULATION,0.2777777777777778,"mization problem. Our goal is to find a linker that maximizes the performance of ESMFold on protein
130"
PROBLEM FORMULATION,0.2799145299145299,"complex prediction. To be specific, we first denote training data as Dtrain = {(x1, y1), ..., (xn, yn)}
131"
PROBLEM FORMULATION,0.28205128205128205,"where xi = (xA
i , xB
i ) and xA
i , xB
i represent the amino acid sequences of two chains, yi is the structure
132"
PROBLEM FORMULATION,0.2841880341880342,"of protein xi. For a specified linker length L, the linker optimization problem is defined as follows:
133"
PROBLEM FORMULATION,0.2863247863247863,"l∗= arg min
l∈EL"
N,0.28846153846153844,"1
n n
X"
N,0.2905982905982906,"i=1
L(xi, yi, l)
(1)"
N,0.29273504273504275,"Figure 1: Overview of Linker-tuning method with ESMFold as backbone. (A) Training. Based
on ESMFold (shown in blue colors), we add a linker embedding module EL (shown in yellow colors)
with linker length L. Given a protein with multiple chains, we add the linker specified in the linker
embedding module between each chain before running it as a single chain through the ESMFold
model. The model outputs a distogram with the linker part removed. We use a weighted distogram
loss as the objective function to train the linker embedding module while freezing all the parameters
in ESMFold. (B) Inference. After training, ESMFold with our linker embedding module can be
treated as a whole black box model, denoted as ESMFold-Linker*. The input for this model is
just protein sequences. And the model outputs a predicted distogram as well as all the atoms’ 3D
coordinates for the protein."
N,0.2948717948717949,"where l denotes a linker, EL ⊂RL×d denotes a specific embedding space with embedding dimension
134"
N,0.297008547008547,"of d, L(xi, yi, l) denotes complex structure prediction loss w.r.t. protein (xi, yi) using linker l.
135"
N,0.29914529914529914,"Therefore, the linker optimization is placed at the task level instead of at the instance level.
136"
MODEL ARCHITECTURE,0.30128205128205127,"4.2
Model architecture
137"
MODEL ARCHITECTURE,0.3034188034188034,"Our method is implemented based on ESMFold, a PLM-based strong structure prediction model.
138"
MODEL ARCHITECTURE,0.3055555555555556,"As shown in Figure 1, we place the continuous linker at Folding Module of ESMFold, which takes
139"
MODEL ARCHITECTURE,0.3076923076923077,"both the sequence representation from ESM2 and the amino acid sequence as input. There are two
140"
MODEL ARCHITECTURE,0.30982905982905984,"main reasons that motivate us to place the continuous linker at Folding Module instead of at ESM2.
141"
MODEL ARCHITECTURE,0.31196581196581197,"First, we can utilize the pre-trained distogram head while avoiding backpropagating to the giant
142"
MODEL ARCHITECTURE,0.3141025641025641,"ESM2 model. If we put it on the ESM2 side, the combined depth of training will go up to 104
143"
MODEL ARCHITECTURE,0.3162393162393162,"layers, making it easily suffer from gradient vanishing and exploding. Second, preliminary analysis
144"
MODEL ARCHITECTURE,0.31837606837606836,"on inter-chain contact prediction (shown in Table 4) shows that using Folding Module on top of
145"
MODEL ARCHITECTURE,0.32051282051282054,"ESM2-3B increases prediction precision dramatically over ESM2-3B while ESM2-3B just performs
146"
MODEL ARCHITECTURE,0.32264957264957267,"slightly better ESM2-650M, implying that Folding Module is more sensitive to structure prediction
147"
MODEL ARCHITECTURE,0.3247863247863248,"and easier to control.
148"
MODEL ARCHITECTURE,0.3269230769230769,"We implement a plug-in linker embedding module, which contains L × d learnable parameters where
149"
MODEL ARCHITECTURE,0.32905982905982906,"d is the embedding dimension of Folding Module. During training, only the linker embedding module
150"
MODEL ARCHITECTURE,0.3311965811965812,"is trainable, while all the original parameters in ESMFold are frozen. Therefore, ESM2 is just a
151"
MODEL ARCHITECTURE,0.3333333333333333,"sequence feature extractor that generates features for Folding Module. As shown in Figure 1(A), we
152"
MODEL ARCHITECTURE,0.33547008547008544,"first use a poly-Glycine linker of the same length as the continuous linker to join different chains for
153"
MODEL ARCHITECTURE,0.33760683760683763,"the ESM2 input. Then we obtain the protein sequence representation and input it to Folding Module
154"
MODEL ARCHITECTURE,0.33974358974358976,"along with the chains connected by the continuous linker. Finally, the distogram head outputs a
155"
MODEL ARCHITECTURE,0.3418803418803419,"probability distribution pD
ij ∈R64 of each residue pair (i, j) on 64 distance bins, which is used for
156"
MODEL ARCHITECTURE,0.344017094017094,"computing the loss function. After training, we view ESMFold and the linker embedding module as
157"
MODEL ARCHITECTURE,0.34615384615384615,"a whole and name it as ESMFold-Linker*. As shown in Figure 1(B), it can be used to predict the
158"
MODEL ARCHITECTURE,0.3482905982905983,"distograms as well as the 3D coordinates of all the residues for multi-chain protein sequences.
159"
WEIGHTED DISTOGRAM LOSS,0.3504273504273504,"4.3
Weighted distogram loss
160"
WEIGHTED DISTOGRAM LOSS,0.3525641025641026,"Intuitively, to predict the structure of a protein complex, we need to know two things: 1) the structures
161"
WEIGHTED DISTOGRAM LOSS,0.3547008547008547,"of each chain, on which ESMFold has been trained; and 2) the interaction interface between chains,
162"
WEIGHTED DISTOGRAM LOSS,0.35683760683760685,"which ESMFold has never seen before. Therefore, we propose to weight the intra-chain predictions
163"
WEIGHTED DISTOGRAM LOSS,0.358974358974359,"and inter-chain predictions differently, with a focus on learning better interface between chains.
164"
WEIGHTED DISTOGRAM LOSS,0.3611111111111111,"Formally, let NA, NB be the number of residues in two chains in a protein complex, N = NA + NB
165"
WEIGHTED DISTOGRAM LOSS,0.36324786324786323,"be the total number of residues in the protein complex. Let yij ∈R64 denote the one-hot labels of
166"
WEIGHTED DISTOGRAM LOSS,0.36538461538461536,"the 3D space distance bins between residue pair (i, j) and pij ∈R64 be the corresponding predicted
167"
WEIGHTED DISTOGRAM LOSS,0.36752136752136755,"probability. We define a weighted distogram loss for a protein complex as follows:
168"
WEIGHTED DISTOGRAM LOSS,0.3696581196581197,"L(x, y, l) = L1(xA, yA) + L1(xB, yB) + λL2(x, y, l)
(2)"
WEIGHTED DISTOGRAM LOSS,0.3717948717948718,"where L1(., .) denotes the single-chain distogram loss given as follows:
169"
WEIGHTED DISTOGRAM LOSS,0.37393162393162394,"L1(xA, yA) = −
2
NA(NA + 1) NA
X i=1 NA
X j≥i"
X,0.37606837606837606,"64
X"
X,0.3782051282051282,"b=1
yijblog(pD
ijb)
(3)"
X,0.3803418803418803,"and L2(x, y, l) denotes the inter-chain distogram loss defined as follows:
170"
X,0.38247863247863245,"L2(x, y, l) = −
1
NANB NA
X i=1 NB
X j=1"
X,0.38461538461538464,"64
X"
X,0.38675213675213677,"b=1
yijblog(pD
ijb)
(4)"
X,0.3888888888888889,"and λ ≥2 is a hyperparameter controlling the attention we place on the interface of a protein complex.
171"
X,0.391025641025641,"In our method, we use the weighted distogram loss as the training objective and validation metric.
172"
EXPERIMENTS,0.39316239316239315,"5
Experiments
173"
EXPERIMENT SETTING,0.3952991452991453,"5.1
Experiment setting
174"
EXPERIMENT SETTING,0.3974358974358974,"Datasets
We mainly perform experiments on heteromers of two chains. For training, we use the
175"
EXPERIMENT SETTING,0.3995726495726496,"dataset from APOC [35], which contains heterodimers released in the Protein Data Bank (PDB)
176"
EXPERIMENT SETTING,0.4017094017094017,"before 2018-09-30. After filtering out similar sequences at a 40% sequence identity threshold, it
177"
EXPERIMENT SETTING,0.40384615384615385,"is split into train/valid/test1 sets by CDPred [36]. We further filter out those proteins that contain
178"
EXPERIMENT SETTING,0.405982905982906,"missing Cβ coordinates (Cα for Glycine) in the pdb file. The resulting train/valid/test sample sizes are
179"
EXPERIMENT SETTING,0.4081196581196581,"2,946/193/172, respectively. The average number of residues in the test set is 367, with a maximum
180"
EXPERIMENT SETTING,0.41025641025641024,"of 998. Furthermore, we use the largest blind test set HeteroTest22 from CDPred, which contains
181"
EXPERIMENT SETTING,0.41239316239316237,"55 heterodimers released in PDB between 2021-09-01 to 2021-10-20 [36]. The average number of
182"
EXPERIMENT SETTING,0.41452991452991456,"residues is 505, with a maximum of 979. In addition, we use the antibody VH-VL test set from
183"
EXPERIMENT SETTING,0.4166666666666667,"XtrimoDock [37]. It contains 68 samples released in PDB after 2022-02-01. Each sample consists of
184"
EXPERIMENT SETTING,0.4188034188034188,"one heavy and one light chain, forming the fragment variable region (Fv), which is a critical part of
185"
EXPERIMENT SETTING,0.42094017094017094,"antigen binding. The average number of residues is 231, with a range of [223, 244].
186"
EXPERIMENT SETTING,0.4230769230769231,"Models
We use ESMFold-v13 as our backbone model. ESMFold-v1 consists of a 3B ESM2 model
187"
EXPERIMENT SETTING,0.4252136752136752,"and a 670M Folding Module, which is the largest yet publicly available ESMFold checkpoint. For the
188"
EXPERIMENT SETTING,0.42735042735042733,"Linker-tuning method, the linker length L is set to 25, equal to the length of the manual poly-Glycine
189"
EXPERIMENT SETTING,0.42948717948717946,"linker. So the plug-in linker embedding module contains 0.027M parameters. We initialize the linker
190"
EXPERIMENT SETTING,0.43162393162393164,"embedding using the embedding of Glycine. During training, only the linker embedding module
191"
EXPERIMENT SETTING,0.4337606837606838,"is trainable, while all the original parameters in ESMFold are frozen. The hyperparameter λ in the
192"
EXPERIMENT SETTING,0.4358974358974359,"1https://github.com/BioinfoMachineLearning/CDPred/tree/main/example/training_datalists
2https://zenodo.org/record/6647564#.ZDWvMuxBxhE
3https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_3B_v1.pt"
EXPERIMENT SETTING,0.43803418803418803,"weighted distogram loss is set to 4. We train the model on a single Nvidia A100 80GB GPU with
193"
EXPERIMENT SETTING,0.44017094017094016,"batch_size=1 and num_epoch=15. The protein sequences in the training set are cropped to 225
194"
EXPERIMENT SETTING,0.4423076923076923,"residues to fit in GPU memory using the multi-chain cropping algorithm from AF-Multimer [29].
195"
EXPERIMENT SETTING,0.4444444444444444,"The number of recycles is set to 1 during training to reduce computation. We use Adam optimizer
196"
EXPERIMENT SETTING,0.4465811965811966,"with a learning rate of 5e-4. We select the best model based on the validation weighted distogram
197"
EXPERIMENT SETTING,0.44871794871794873,"loss. During inference, the number of recycles is set to 3.
198"
EXPERIMENT SETTING,0.45085470085470086,"Baselines
We compare our method with several baselines and one SOTA model as follows:
199"
EXPERIMENT SETTING,0.452991452991453,"• ESMFold-Linker: ESMFold-v1 with chains joined by the G25 linker as input.
200"
EXPERIMENT SETTING,0.4551282051282051,"• ESMFold-Gap: ESMFold-v1 with residue_index_offset set to 512.
201"
EXPERIMENT SETTING,0.45726495726495725,"• AlphaFold-Linker [29]: AF2 with a 21 residue repeated Glycine-Glycine-Serine linker.
202"
EXPERIMENT SETTING,0.4594017094017094,"• HDOCK [38]: rigid docking with single chains predicted by AF2.
203"
EXPERIMENT SETTING,0.46153846153846156,"• AF-Multimer(v3 best) [29]: AF-Multimer contains five models that are trained on all protein
204"
EXPERIMENT SETTING,0.4636752136752137,"structures released in PDB before 2021-09-30. We take the best prediction from the five
205"
EXPERIMENT SETTING,0.4658119658119658,"AF-Multimer models.
206"
EXPERIMENT SETTING,0.46794871794871795,"Metrics
For protein complex 3D structure prediction, we use DockQ [39] to evaluate the quality
207"
EXPERIMENT SETTING,0.4700854700854701,"of the predicted interfaces. As defined by Critical Assessment of PRediction Interactions (CAPRI),
208"
EXPERIMENT SETTING,0.4722222222222222,"interfaces with DockQ < 0.23 means incorrect prediction, interfaces with 0.23 ≤DockQ < 0.49
209"
EXPERIMENT SETTING,0.47435897435897434,"means acceptable prediction, 0.49 ≤DockQ < 0.80 means medium quality prediction, and DockQ ≥
210"
EXPERIMENT SETTING,0.47649572649572647,"0.80 means high-quality prediction. To evaluate the whole predicted protein complex structure rather
211"
EXPERIMENT SETTING,0.47863247863247865,"than the interfaces, we adopt two commonly used global structure metrics, namely, Root Mean
212"
EXPERIMENT SETTING,0.4807692307692308,"Squared Deviation (RMSD), and Template-Modeling Score (TM-Score) [40]. Besides, we use the
213"
EXPERIMENT SETTING,0.4829059829059829,"top-k precision as an evaluation metric for inter-chain contact prediction. We set k = Ns/5, where
214"
EXPERIMENT SETTING,0.48504273504273504,"Ns is the minimum chain length for a given protein complex.
215"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.48717948717948717,"5.2
General heterodimer structure prediction
216"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.4893162393162393,"Table 1 shows the protein complex structure prediction results of our methods and the baselines on
217"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.49145299145299143,"the heterodimer test set and HeteroTest2. On the i.i.d. heterodimer test set, ESMFold-Linker achieves
218"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.4935897435897436,"a 0.32 DockQ score and a 0.76 TM-score on average. By optimizing the linker, our model, i.e.,
219"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.49572649572649574,"ESMFold-Linker*, achieves a 0.36 DockQ score and a 0.79 TM-score on average on the same test
220"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.49786324786324787,"set, outperforming the ESMFold-Linker baseline by 13.61% and 3.28%, respectively. Interestingly,
221"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5,"the gain of the interface quality (13.61%) is much larger than the gain of the whole structure quality
222"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5021367521367521,"(3.28%), indicating that our learned linker mainly improves the interfaces more than the overall
223"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5042735042735043,"structures. We further improve the ESMFold-Linker* by incorporating a large chain break, which
224"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5064102564102564,"adds a large number to the residue index in Folding Module. And the model ESMFold-Linker*-
225"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5085470085470085,"Gap achieves a 0.41 DockQ score and 0.80 TM-score, outperforming ESMFold-Linker by 28.13%
226"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5106837606837606,"and 5.26%, respectively. On the OOD test set HeteroTest2, we observe similar results. ESMFold-
227"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5128205128205128,"Linker*-Gap surpasses ESMFold-Linker by 54.55% DockQ score and 4.82% TM-score, respectively,
228"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5149572649572649,"suggesting that our learned linker can generalize well to OOD data.
229"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5170940170940171,"Compared to AlphaFold-Linker, a model that takes linked sequences and MSAs as input, our best
230"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5192307692307693,"model ESMFold-Linker*-Gap achieves similar DockQ scores on both test sets, with lower values in
231"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5213675213675214,"RMSD. Meanwhile, it outperforms the classic docking method HDOCK with AF2 predicted chains
232"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5235042735042735,"as input in terms of DockQ score and RMSD. Furthermore, we compare it with the SOTA model
233"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5256410256410257,"AF-Multimer.4 From Table 1, we can see there is still a large gap between our method and the
234"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5277777777777778,"AF-Multimer(v1 best) on HeteroTest2. There are three main reasons responsible for this gap: 1) The
235"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5299145299145299,"base model for AF-Multimer is AF2, which is a model stronger than ESMFold in general, especially
236"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.532051282051282,"for those proteins that have high-quality MSAs; 2) AF-Multimer is a fully fine-tuned version of AF2
237"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5341880341880342,"on a larger protein complex structure dataset while our model is a prompt tuning method trained only
238"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5363247863247863,"on the heterodimer dataset; 3) AF-Multimer ensembles five models, while we only use one model.
239"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5384615384615384,"However, our method is able to predict some proteins that are hard for both ESMFold-Linker and
240"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5405982905982906,"AF-Multimer. As shown in Figure 2, ESMFold-Linker* successfully predicts the interface of the
241"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5427350427350427,"4We use AF-Multimer v1 here because of the overlapping training data of AF-Multimer v3 and HeteroTest2.
Since AF-Multimer v1 contains the heterodimer test set in its training data, we do not report the performance."
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5448717948717948,Table 1: Structure prediction results on Heterodimer data.
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5470085470085471,"Heterodimer test
HeteroTest2"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5491452991452992,"DockQ↑
RMSD↓
TM-score↑
DockQ↑
RMSD↓
TM-score↑"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5512820512820513,"ESMFold-Linker
0.32
±0.34
10.76
±8.68
0.76
±0.19
0.11
±0.20
20.10
±10.31
0.62
±0.19"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5534188034188035,"ESMFold-Gap
0.34
±0.35
10.37
±8.89
0.77
±0.19
0.11
±0.21
20.17
±11.70
0.63
±0.19"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5555555555555556,"AlphaFold-Linker
0.42
±0.40
9.38
±9.46
0.83
±0.17
0.17
±0.32
20.95
±11.93
0.71
±0.18"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5576923076923077,"HDOCK
0.36
±0.38
9.74
±8.66
0.81
±0.17
0.15
±0.29
19.49
±11.72
0.68
±0.18"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5598290598290598,"ESMFold-Linker*(ours)
0.36
±0.35
9.19
±8.04
0.79
±0.19
0.14
±0.23
19.03
±10.93
0.65
±0.20"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.561965811965812,"ESMFold-Linker*-Gap(ours)
0.41
±0.35
8.59
±8.39
0.80
±0.19
0.17
±0.25
18.53
±11.27
0.65
±0.20"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5641025641025641,"AF-multimer(v1 best)
0.30
±0.35
15.07
±11.78
0.73
±0.20"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5662393162393162,"Figure 2: Comparison of predicted structure quality and inference time of heterodimer 7D7F_AD by
ESMFold-Linker, ESMFold-Linker*(ours), and AF-Multimer(v3 best). 7D7F is a membrane protein
comprising 917 residues in the A and D chains. Structures are drawn using Protein Imager [41]. Gray
indicates the ground truth structure."
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5683760683760684,"membrane protein 7D7F_AD with a DockQ score of 0.39 while ESMFold-Linker and AF-Multimer
242"
GENERAL HETERODIMER STRUCTURE PREDICTION,0.5705128205128205,"cannot predict the interface correctly.
243"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5726495726495726,"5.3
Antibody heavy chain light chain docking
244"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5747863247863247,"We further test our method on antibodies, an important type of protein in designing new drugs.
245"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5769230769230769,"Particularly, we focus on the heavy chain and the light chain docking. Table 2 shows the structure
246"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5790598290598291,"prediction results of MSA-free methods (first two methods) and MSA-based methods (last four
247"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5811965811965812,"methods) on the VH-VL test set. As an MSA-free model, ESMFold-Linker predicts all the interfaces
248"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5833333333333334,"successfully with an average DockQ score of 0.737, better than the classical docking method HDOCK.
249"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5854700854700855,"But it still lacks behind AlphaFold-Linker. For the three linker-based models, the distributions of their
250"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5876068376068376,"DockQ scores are shown in Figure 3. Equipped with the optimized linker, ESMFold-Linker* achieves
251"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5897435897435898,"an average DockQ score of 0.753, with 7 more high-quality interface predictions than ESMFold-
252"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5918803418803419,"Linker and 5 more high-quality interface predictions than AlphaFold-Linker. This result indicates
253"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.594017094017094,"that our learned linker trained on the general heterodimer dataset generalizes well to antibody data.
254"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5961538461538461,"Although the interface prediction performance of our method still falls behind AF-Multimer(v3 best),
255"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.5982905982905983,"the gap in the DockQ score is much smaller compared to the case in HeteroTest2. Besides, it is
256"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6004273504273504,"quite close in TM-score to XtrimoDock [37], which is trained on an antibody-antigen dataset. Given
257"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6025641025641025,"our method only requires sequences as input, it can be a potentially useful model in the scenario of
258"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6047008547008547,"antibody design where the evolving antibody might not have MSAs.
259"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6068376068376068,Table 2: Structure prediction results on VH-VL.
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6089743589743589,"DockQ↑
RMSD↓
TM-score↑"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6111111111111112,"ESMFold-Linker
0.737
±0.084
1.459
±0.474
0.955
±0.019"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6132478632478633,"ESMFold-Linker*(ours)
0.753
±0.083
1.388
±0.498
0.959
±0.019"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6153846153846154,"HDOCK
0.705
±0.202
2.0318
±2.405
0.926
±0.101"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6175213675213675,"AlphaFold-Linker
0.746
±0.089
1.4068
±0.520
0.957
±0.021"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6196581196581197,"AF-multimer (v3 best)
0.779
±0.091
1.287
±0.518
0.963
±0.020"
ANTIBODY HEAVY CHAIN LIGHT CHAIN DOCKING,0.6217948717948718,"XtrimoDock
0.775
±0.021
1.264
±0.572
0.965
±0.097
Figure 3: Boxplot of DockQ on VH-VL."
ANALYSIS AND DISCUSSION,0.6239316239316239,"6
Analysis and Discussion
260"
ANALYSIS AND DISCUSSION,0.6260683760683761,Table 3: Inference time. Time
ANALYSIS AND DISCUSSION,0.6282051282051282,"ESMFold-Gap
3 min
ESMFold-Linker
4 min
ESMFold-Linker*
4 min"
ANALYSIS AND DISCUSSION,0.6303418803418803,"AF-Multimer
36 min"
ANALYSIS AND DISCUSSION,0.6324786324786325,"ESMFold-Linker* is 9× faster than AF-Multimer in inference
261"
ANALYSIS AND DISCUSSION,0.6346153846153846,"We report the structure inference time of the MSA-free methods
262"
ANALYSIS AND DISCUSSION,0.6367521367521367,"(ESMFold-Gap, ESMFold-Linker, and ESMFold-Linker*) and the
263"
ANALYSIS AND DISCUSSION,0.6388888888888888,"SOTA MSA-based model AF-Multimer on the VH-VL dataset using
264"
ANALYSIS AND DISCUSSION,0.6410256410256411,"A100 80G GPU. Table 3 shows the total model inference time on
265"
ANALYSIS AND DISCUSSION,0.6431623931623932,"the VH-VL test set, where AF-Multimer’s time is only for one
266"
ANALYSIS AND DISCUSSION,0.6452991452991453,"model, excluding the time of MSA search. As shown in Table 3, on
267"
ANALYSIS AND DISCUSSION,0.6474358974358975,"the VH-VL test set with an average sequence length of 231, both
268"
ANALYSIS AND DISCUSSION,0.6495726495726496,"ESMFold-Linker and ESMFold-Linker* take 4 minutes to run the
269"
ANALYSIS AND DISCUSSION,0.6517094017094017,"inference, which is 9× faster than AF-Multimer.
270"
ANALYSIS AND DISCUSSION,0.6538461538461539,"Large chain break or linker, or both?
We perform an ablation study on ESMFold with chain break
271"
ANALYSIS AND DISCUSSION,0.655982905982906,"and linker to better understand the contribution of each operation. Table 4 shows the comparison
272"
ANALYSIS AND DISCUSSION,0.6581196581196581,"of inter-chain contact prediction precision of ESMFold-based methods on the heterodimer test set
273"
ANALYSIS AND DISCUSSION,0.6602564102564102,"and HeteroTest2.5 As shown in Table 4, it is hard to tell whether ESMFold-Linker or ESMFold-Gap
274"
ANALYSIS AND DISCUSSION,0.6623931623931624,"is better. However, combining the two (ESMFold-Linker-Gap) provides significant performance
275"
ANALYSIS AND DISCUSSION,0.6645299145299145,"gains over using either operation alone on both datasets. We observe similar effects in our method
276"
ANALYSIS AND DISCUSSION,0.6666666666666666,"when incorporating chain break with the optimized linker. Compared to using a chain break, the
277"
ANALYSIS AND DISCUSSION,0.6688034188034188,"major limitation of using a linker is that it increases the computation cost (shown in Table 3). But
278"
ANALYSIS AND DISCUSSION,0.6709401709401709,"we can enjoy the advantage of a large degree of freedom for improvement and better performance.
279"
ANALYSIS AND DISCUSSION,0.6730769230769231,"Empirically, combining the two gives a better performance than just using each of them.
280"
ANALYSIS AND DISCUSSION,0.6752136752136753,Table 4: Comparison of inter-chain contact prediction results on Heterodimer data.
ANALYSIS AND DISCUSSION,0.6773504273504274,"(%)
Heterodimer test
HeteroTest2"
ANALYSIS AND DISCUSSION,0.6794871794871795,"top Ns/5
top Ns/2
top Ns
top Ns/5
top Ns/2
top Ns"
ANALYSIS AND DISCUSSION,0.6816239316239316,"ESM2-650M-Linker
12.02
9.89
8.33
ESM2-3B-Linker
12.14
10.86
8.89"
ANALYSIS AND DISCUSSION,0.6837606837606838,"ESMFold-Linker
49.88
47.04
40.64
23.00
18.92
13.72
ESMFold-Gap
51.15
48.13
40.82
22.09
18.21
13.08
ESMFold-Linker*(ours)
57.55
53.04
44.37
27.11
22.14
15.46
ESMFold-Linker-Gap
57.72
53.44
45.41
25.20
19.84
14.66
ESMFold-Linker*-Gap(ours)
60.40
56.27
48.00
28.00
23.69
17.26"
ANALYSIS AND DISCUSSION,0.6858974358974359,"The learned linker allows more chain twist while rarely interacting with the chains
In Figure 4,
281"
ANALYSIS AND DISCUSSION,0.688034188034188,"we visualize the predicted contact maps of two proteins with the linker inside to understand how the
282"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.6901709401709402,"5The contact map probabilities are obtained from the predicted distogram probabilities by summing the
probability mass in each distribution below 8.25Å."
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.6923076923076923,Figure 4: Contact maps of viral proteins 7VYR_HL (A) and 7WPE_YZ (B).
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.6944444444444444,"linker interacts with the chains. The two proteins are 7VYR_HL and 7WPE_YZ, corresponding to a
283"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.6965811965811965,"good case (0.77 DockQ score) and a bad case (0.01 DockQ score) in our model ESMFold-Linker*.
284"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.6987179487179487,"As shown in Figure 4, both the G25 linker (middle) and our learned linker (right) seem to rarely
285"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7008547008547008,"interact with the protein chains in both cases. This result indicates that ESMFold is able to recognize
286"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7029914529914529,"the linker part as a disordered region and fold the connected sequences as multi-domain proteins.
287"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7051282051282052,"Furthermore, there are more predicted contacts using the learned linker than using the G25 linker in
288"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7072649572649573,"both cases. This result suggests that the learned linker allows the connecting chains to freely twist
289"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7094017094017094,"and rotate to recruit binding partners more than the manual linker.
290"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7115384615384616,"Limitations
Our method has some limitations. First, if the base model (ESMFold-v1) is not good
291"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7136752136752137,"at predicting a certain type of protein complexes, such as the heterodimers in HeteroTest2, adding an
292"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7158119658119658,"optimized linker can not make it a strong model for that type of data since the trainable parameter size
293"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.717948717948718,"is very small. Second, our method is tested on heterodimers, whether it generalizes to homodimers or
294"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7200854700854701,"multi-chain proteins is unknown. Third, the linker is only optimized at the Folding Module, while the
295"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7222222222222222,"linker at ESM2 remains constant. And the linker length is treated as a hyperparameter, which can be
296"
THE CONTACT MAP PROBABILITIES ARE OBTAINED FROM THE PREDICTED DISTOGRAM PROBABILITIES BY SUMMING THE,0.7243589743589743,"further optimized to improve performance and speed.
297"
CONCLUSIONS AND FUTURE WORK,0.7264957264957265,"7
Conclusions and future work
298"
CONCLUSIONS AND FUTURE WORK,0.7286324786324786,"The use of prompts in protein structure prediction models is not always clear due to the high
299"
CONCLUSIONS AND FUTURE WORK,0.7307692307692307,"complexity of models and a general lack of biological knowledge for AI researchers. In this work,
300"
CONCLUSIONS AND FUTURE WORK,0.7329059829059829,"we have proposed Linker-tuning, a prompt tuning method to adapt the single-chain pre-trained
301"
CONCLUSIONS AND FUTURE WORK,0.7350427350427351,"ESMFold for heterodimer structure prediction. As proof-of-concept, we showcase that we can place
302"
CONCLUSIONS AND FUTURE WORK,0.7371794871794872,"a soft prompt in ESMFold. The task is reformulated as a pre-trained task itself under the biological
303"
CONCLUSIONS AND FUTURE WORK,0.7393162393162394,"prior. Experiments show that merely tuning a prompt on ESMFold can significantly improve the
304"
CONCLUSIONS AND FUTURE WORK,0.7414529914529915,"predicted complex structure quality over the discrete prompt handcrafted with strong biological
305"
CONCLUSIONS AND FUTURE WORK,0.7435897435897436,"insight. Hopefully, our work can inspire more work on AI for Protein Science.
306"
CONCLUSIONS AND FUTURE WORK,0.7457264957264957,"There are two directions for future work. Firstly, we would like to extend our work to antibody-
307"
CONCLUSIONS AND FUTURE WORK,0.7478632478632479,"antigen structure prediction, a critical task with direct relevance to drug design. Secondly, we are
308"
CONCLUSIONS AND FUTURE WORK,0.75,"going to explore structural-aware antibody design using our method since it is efficient and fast. By
309"
CONCLUSIONS AND FUTURE WORK,0.7521367521367521,"pursuing these directions, our objective is to make progressive contributions towards the development
310"
CONCLUSIONS AND FUTURE WORK,0.7542735042735043,"of effective drugs for disease treatment and pain relief.
311"
REFERENCES,0.7564102564102564,"References
312"
REFERENCES,0.7585470085470085,"[1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn
313"
REFERENCES,0.7606837606837606,"Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure
314"
REFERENCES,0.7628205128205128,"prediction with alphafold. Nature, 596(7873):583–589, 2021.
315"
REFERENCES,0.7649572649572649,"[2] Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu,
316"
REFERENCES,0.7670940170940171,"Qi Xie, Bonnie Berger, et al. High-resolution de novo structure prediction from primary sequence. BioRxiv,
317"
REFERENCES,0.7692307692307693,"pages 2022–07, 2022.
318"
REFERENCES,0.7713675213675214,"[3] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa,
319"
REFERENCES,0.7735042735042735,"Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale
320"
REFERENCES,0.7756410256410257,"of evolution enable accurate structure prediction. BioRxiv, 2022.
321"
REFERENCES,0.7777777777777778,"[4] Xiaomin Fang, Fan Wang, Lihang Liu, Jingzhou He, Dayong Lin, Yingfei Xiang, Xiaonan Zhang, Hua Wu,
322"
REFERENCES,0.7799145299145299,"Hui Li, and Le Song. Helixfold-single: Msa-free protein structure prediction by using protein language
323"
REFERENCES,0.782051282051282,"model as an alternative. arXiv preprint arXiv:2207.13921, 2022.
324"
REFERENCES,0.7841880341880342,"[5] Ratul Chowdhury, Nazim Bouatta, Surojit Biswas, Christina Floristean, Anant Kharkar, Koushik Roy,
325"
REFERENCES,0.7863247863247863,"Charlotte Rochereau, Gustaf Ahdritz, Joanna Zhang, George M Church, et al. Single-sequence protein
326"
REFERENCES,0.7884615384615384,"structure prediction using a language model and deep learning. Nature Biotechnology, 40(11):1617–1623,
327"
REFERENCES,0.7905982905982906,"2022.
328"
REFERENCES,0.7927350427350427,"[6] Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer pro-
329"
REFERENCES,0.7948717948717948,"tein language models are unsupervised structure learners. In International Conference on Learning
330"
REFERENCES,0.7970085470085471,"Representations, 2020.
331"
REFERENCES,0.7991452991452992,"[7] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle
332"
REFERENCES,0.8012820512820513,"Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsu-
333"
REFERENCES,0.8034188034188035,"pervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences,
334"
REFERENCES,0.8055555555555556,"118(15):e2016239118, 2021.
335"
REFERENCES,0.8076923076923077,"[8] Junsu Ko and Juyong Lee. Can alphafold2 predict protein-peptide complex structures accurately? BioRxiv,
336"
REFERENCES,0.8098290598290598,"2021.
337"
REFERENCES,0.811965811965812,"[9] Tomer Tsaban, Julia K Varga, Orly Avraham, Ziv Ben-Aharon, Alisa Khramushin, and Ora Schueler-
338"
REFERENCES,0.8141025641025641,"Furman. Harnessing protein folding neural networks for peptide–protein docking. Nature communications,
339"
REFERENCES,0.8162393162393162,"13(1):1–12, 2022.
340"
REFERENCES,0.8183760683760684,"[10] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,
341"
REFERENCES,0.8205128205128205,"prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM
342"
REFERENCES,0.8226495726495726,"Computing Surveys, 55(9):1–35, 2023.
343"
REFERENCES,0.8247863247863247,"[11] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT
344"
REFERENCES,0.8269230769230769,"understands, too. arXiv preprint arXiv:2103.10385, 2021.
345"
REFERENCES,0.8290598290598291,"[12] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli
346"
REFERENCES,0.8311965811965812,"Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein structure prediction
347"
REFERENCES,0.8333333333333334,"using potentials from deep learning. Nature, 577(7792):706–710, 2020.
348"
REFERENCES,0.8354700854700855,"[13] Vishnu Priyanka Reddy Chichili, Veerendra Kumar, and Jayaraman Sivaraman. Linkers in the structural
349"
REFERENCES,0.8376068376068376,"biology of protein–protein interactions. Protein science, 22(2):153–167, 2013.
350"
REFERENCES,0.8397435897435898,"[14] Athena D Nagi and Lynne Regan. An inverse correlation between loop length and stability in a four-helix-
351"
REFERENCES,0.8418803418803419,"bundle protein. Folding and Design, 2(1):67–75, 1997.
352"
REFERENCES,0.844017094017094,"[15] Janet E Deane, Daniel P Ryan, Margaret Sunde, Megan J Maher, J Mitchell Guss, Jane E Visvader, and
353"
REFERENCES,0.8461538461538461,"Jacqueline M Matthews. Tandem lim domains provide synergistic binding in the lmo4: Ldb1 complex.
354"
REFERENCES,0.8482905982905983,"The EMBO journal, 23(18):3589–3598, 2004.
355"
REFERENCES,0.8504273504273504,"[16] Stephen K Burley, Helen M Berman, Charmi Bhikadiya, Chunxiao Bi, Li Chen, Luigi Di Costanzo, Cole
356"
REFERENCES,0.8525641025641025,"Christie, Jose M Duarte, Shuchismita Dutta, Zukang Feng, et al. Protein data bank: the single global
357"
REFERENCES,0.8547008547008547,"archive for 3d macromolecular structure data. Nucleic Acids Research, 47(D1), 2018.
358"
REFERENCES,0.8568376068376068,"[17] Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee,
359"
REFERENCES,0.8589743589743589,"Jue Wang, Qian Cong, Lisa N Kinch, R Dustin Schaeffer, et al. Accurate prediction of protein structures
360"
REFERENCES,0.8611111111111112,"and interactions using a three-track neural network. Science, 373(6557):871–876, 2021.
361"
REFERENCES,0.8632478632478633,"[18] Jianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchinnikov, and David Baker.
362"
REFERENCES,0.8653846153846154,"Improved protein structure prediction using predicted interresidue orientations. Proceedings of the National
363"
REFERENCES,0.8675213675213675,"Academy of Sciences, 117(3):1496–1503, 2020.
364"
REFERENCES,0.8696581196581197,"[19] Wenkai Wang, Zhenling Peng, and Jianyi Yang. Single-sequence protein structure prediction using
365"
REFERENCES,0.8717948717948718,"supervised transformer protein language models. Nature Computational Science, 2(12):804–814, 2022.
366"
REFERENCES,0.8739316239316239,"[20] Yining Wang, Xumeng Gong, Shaochuan Li, Bing Yang, YiWu Sun, Chuan Shi, Hui Li, Yangang Wang,
367"
REFERENCES,0.8760683760683761,"Cheng Yang, and Le Song. xtrimoabfold: De novo antibody structure prediction without msa. arXiv
368"
REFERENCES,0.8782051282051282,"preprint arXiv:2212.00735v3, 2022.
369"
REFERENCES,0.8803418803418803,"[21] Jinhua Zhu, Zhenyu He, Ziyao Li, Guolin Ke, and Linfeng Zhang. Uni-fold musse: De novo protein
370"
REFERENCES,0.8824786324786325,"complex prediction with protein language models. bioRxiv, pages 2023–02, 2023.
371"
REFERENCES,0.8846153846153846,"[22] Brennan Abanades, Wing Ki Wong, Fergus Boyles, Guy Georges, Alexander Bujotzek, and Charlotte M
372"
REFERENCES,0.8867521367521367,"Deane. Immunebuilder: Deep-learning models for predicting the structures of immune proteins. bioRxiv,
373"
REFERENCES,0.8888888888888888,"pages 2022–11, 2022.
374"
REFERENCES,0.8910256410256411,"[23] Yining Wang, Xumeng Gong, Shaochuan Li, Bing Yang, Yiwu Sun, Yujie Luo, Hui Li, and Le Song. Fast de
375"
REFERENCES,0.8931623931623932,"novo antibody structure prediction with atomic accuracy. Cancer Research, 83(7_Supplement):4296–4296,
376"
REFERENCES,0.8952991452991453,"2023.
377"
REFERENCES,0.8974358974358975,"[24] Ozlem Keskin, Attila Gursoy, Buyong Ma, and Ruth Nussinov. Principles of protein- protein interactions:
378"
REFERENCES,0.8995726495726496,"what are the preferred ways for proteins to interact? Chemical reviews, 108(4):1225–1244, 2008.
379"
REFERENCES,0.9017094017094017,"[25] Ian R Humphreys, Jimin Pei, Minkyung Baek, Aditya Krishnakumar, Ivan Anishchenko, Sergey Ovchin-
380"
REFERENCES,0.9038461538461539,"nikov, Jing Zhang, Travis J Ness, Sudeep Banjade, Saket R Bagde, et al. Computed structures of core
381"
REFERENCES,0.905982905982906,"eukaryotic protein complexes. Science, 374(6573):eabm4805, 2021.
382"
REFERENCES,0.9081196581196581,"[26] Patrick Bryant, Gabriele Pozzati, and Arne Elofsson. Improved prediction of protein-protein interactions
383"
REFERENCES,0.9102564102564102,"using alphafold2. Nature communications, 13(1):1265, 2022.
384"
REFERENCES,0.9123931623931624,"[27] Mu Gao, Davi Nakajima An, Jerry M Parks, and Jeffrey Skolnick. Af2complex predicts direct physical
385"
REFERENCES,0.9145299145299145,"interactions in multimeric proteins with deep learning. Nature communications, 13(1):1744, 2022.
386"
REFERENCES,0.9166666666666666,"[28] Milot Mirdita, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin
387"
REFERENCES,0.9188034188034188,"Steinegger. Colabfold: making protein folding accessible to all. Nature methods, 19(6):679–682, 2022.
388"
REFERENCES,0.9209401709401709,"[29] Richard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green,
389"
REFERENCES,0.9230769230769231,"Augustin Žídek, Russ Bates, Sam Blackwell, Jason Yim, et al. Protein complex prediction with alphafold-
390"
REFERENCES,0.9252136752136753,"multimer. BioRxiv, pages 2021–10, 2022.
391"
REFERENCES,0.9273504273504274,"[30] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
392"
REFERENCES,0.9294871794871795,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
393"
REFERENCES,0.9316239316239316,"Advances in neural information processing systems, 33:1877–1901, 2020.
394"
REFERENCES,0.9337606837606838,"[31] Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classification and natural
395"
REFERENCES,0.9358974358974359,"language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for
396"
REFERENCES,0.938034188034188,"Computational Linguistics: Main Volume, pages 255–269, 2021.
397"
REFERENCES,0.9401709401709402,"[32] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting
398"
REFERENCES,0.9423076923076923,"knowledge from language models with automatically generated prompts. In Proceedings of the 2020
399"
REFERENCES,0.9444444444444444,"Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222–4235, 2020.
400"
REFERENCES,0.9465811965811965,"[33] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners.
401"
REFERENCES,0.9487179487179487,"In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the
402"
REFERENCES,0.9508547008547008,"11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages
403"
REFERENCES,0.9529914529914529,"3816–3830, 2021.
404"
REFERENCES,0.9551282051282052,"[34] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In
405"
REFERENCES,0.9572649572649573,"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
406"
REFERENCES,0.9594017094017094,"International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–
407"
REFERENCES,0.9615384615384616,"4597, 2021.
408"
REFERENCES,0.9636752136752137,"[35] Mu Gao and Jeffrey Skolnick. Apoc: large-scale identification of similar protein pockets. Bioinformatics,
409"
REFERENCES,0.9658119658119658,"29(5):597–604, 2013.
410"
REFERENCES,0.967948717948718,"[36] Zhiye Guo, Jian Liu, Jeffrey Skolnick, and Jianlin Cheng. Prediction of inter-chain distance maps of
411"
REFERENCES,0.9700854700854701,"protein complexes with 2d attention-based deep neural networks. Nature Communications, 13(1):6963,
412"
REFERENCES,0.9722222222222222,"2022.
413"
REFERENCES,0.9743589743589743,"[37] Yujie Luo, Shaochuan Li, Yiwu Sun, Ruijia Wang, Tingting Tang, Beiqi Hongdu, Xingyi Cheng, Chuan
414"
REFERENCES,0.9764957264957265,"Shi, Hui Li, and Le Song. xtrimodock: Rigid protein docking via cross-modal representation learning and
415"
REFERENCES,0.9786324786324786,"spectral algorithm. bioRxiv, pages 2023–02, 2023.
416"
REFERENCES,0.9807692307692307,"[38] Yumeng Yan, Huanyu Tao, Jiahua He, and Sheng-You Huang. The hdock server for integrated protein–
417"
REFERENCES,0.9829059829059829,"protein docking. Nature protocols, 15(5):1829–1852, 2020.
418"
REFERENCES,0.9850427350427351,"[39] Sankar Basu and Björn Wallner. Dockq: a quality measure for protein-protein docking models. PloS one,
419"
REFERENCES,0.9871794871794872,"11(8):e0161879, 2016.
420"
REFERENCES,0.9893162393162394,"[40] Yang Zhang and Jeffrey Skolnick. Scoring function for automated assessment of protein structure template
421"
REFERENCES,0.9914529914529915,"quality. Proteins: Structure, Function, and Bioinformatics, 57(4):702–710, 2004.
422"
REFERENCES,0.9935897435897436,"[41] Gianluca Tomasello, Ilaria Armenia, and Gianluca Molla. The protein imager: a full-featured online
423"
REFERENCES,0.9957264957264957,"molecular viewer interface with server-side hq-rendering capabilities. Bioinformatics, 36(9):2909–2911,
424"
REFERENCES,0.9978632478632479,"2020.
425"
