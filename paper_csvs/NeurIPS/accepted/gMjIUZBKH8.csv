Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00546448087431694,"The neural network structures of generative models and their corresponding infen-
rence models paired in variational autoencoders (VAEs) play a critical role in the
models’ generative performance. However, powerful VAE network structures are
hand-crafted and fixed prior to training, resulting in a one-size-fits-all approach
that requires heavy computation to tune for given data. Moreover, existing VAE
regularization methods largely overlook the importance of network structures and
fail to prevent overfitting in deep VAE models with cascades of hidden layers. To
address these issues, we propose a Bayesian inference framework that automati-
cally adapts VAE network structures to data and prevent overfitting as they grow
deeper. We model the number of hidden layers with a beta process to infer the
most plausible encoding/decoding network depths warranted by data and perform
layer-wise dropout regularization with a conjugate Bernoulli process. We develop
a scalable estimator that performs joint inference on both VAE network structures
and latent variables. Our experiments show that the inference framework effectively
prevents overfitting in both shallow and deep VAE models, yielding state-of-the-art
performance. We demonstrate that our framework is compatible with different
types of VAE backbone networks and can be applied to various VAE variants,
further improving their performance."
INTRODUCTION,0.01092896174863388,"1
Introduction"
INTRODUCTION,0.01639344262295082,"The inference models and the generative models paired in variational autoencoders (VAEs) are com-
monly constructed with neural networks, i.e., encoding networks and decoding networks, respectively
[1, 2, 3]. Extensive research efforts show that well-designed encoding/decoding network structures
for VAE models can constantly achieve state-of-the-art generative performance compared to other
generative models [4, 5, 6]. However, powerful VAE network structures are hand-crafted and fixed
prior to training. The issue with fixed network structures is that shallow ones limit VAE models’
expressiveness, whereas overly deep networks are slow to use and prone to overfitting. Traditional
model selection by training different VAE network structures for given data is difficult since finding
optimal hyperparameters for each candidate structure is a daunting task, and training large VAE
structures requires significant computation. On the other hand, network structure adaptation methods
for discriminative model settings [7, 8, 9] cannot be straightforwardly applied to address the unique
challenge posed by VAE estimation along with the latent variables."
INTRODUCTION,0.02185792349726776,"Although the network structures play a critical role in the performance of VAE models, they are largely
overlooked by current VAE regularization methods. This renders their failure to prevent overfitting
when the network structures grow deeper. Amortized inference regularization (AIR) proposes two
approaches: injecting random noise to the VAE objective for inference or directly restricting the
inference models to a set of smooth functions [10, 11]. Another approach in VAE regularization"
INTRODUCTION,0.0273224043715847,∗Corresponding author
INTRODUCTION,0.03278688524590164,"(a)
(b)
(c)"
INTRODUCTION,0.03825136612021858,"Figure 1: (a): Demonstration of the VAE network structure inference framework. Beta processes
induce infinite number of hidden layers for encoding/decodineg networks, and its conjugate Bernoulli
process prunes the neurons in each layer with a layer-wise activation probability from the beta process.
Filled circles indicate activated neurons, corresponding to a sample of 1 from the Bernoulli process,
while empty circles correspond to deactivated neurons, corresponding to a sample of 0. (b) and
(c): Two settings of the stick-breaking constructions of beta process. The sticks on top are random
draws from the process, which act as the layer-wise activation probabilities. A stick location δfl
corresponds to a hidden-layer function fl ∈F, and the height denotes its activation probability πl.
The bottom shows the conjugate Bernoulli processes to activate or deactivate neurons in each layer
(column-wisely)."
INTRODUCTION,0.04371584699453552,"incorporates additional constraints to the VAE objective, which enforces a similarity between original
input and its semantic preserving transformation in the latent representation [12, 13]. Nonparametric
Bayesian inference such as Indian buffet process deep generative models (IBP-DGM) [14] and beta-
Bernoulli process VAE (BB-VAE) [15] address overfitting by inferring the dimensionality of VAE
latent variables using an Indian buffet process, specifically, a marginalized beta-Bernoulli process
prior. While these regularization methods are effective for shallow network structures, they fail to
prevent overfitting in deep VAE models."
INTRODUCTION,0.04918032786885246,"We propose a novel Bayesian inference framework that automatically adapt VAE network structures
by inferring the most plausible encoding/decoding network depths based on the given data, as
demonstrated in Figure 1. To achieve this, we employ beta processes [16, 17] to model the number
of hidden layers in the encoding/decoding networks, allowing for infinite depths. A conjugate
Bernoulli process is utilized to prune the neurons in each layer based on layer-wise activation
probabilities generated by the beta process. In addition, to enable efficient joint inference on both the
network structures and the latent variables, we extend multiply importance weighted autoencoder
(MIWAE) [18] by introducing an additional sample size used for Monte Carlo estimation of the
network structures to be tuned. Our theoretical and empirical analysis shows that this novel gradient
estimation scheme leads to a tight lower bound with high signal-to-noise ratio of parameter gradients."
INTRODUCTION,0.0546448087431694,"In summary, our contributions are: i) We propose AdaVAE, a novel VAE structural adaptation
strategy based on Bayesian model selection to enhance model performance. ii) We introduce a
scalable estimator that facilitates joint inference on both encoding/decoding network structures and
latent variables. iii) We conduct a comprehensive analysis of AdaVAE’s regularization capabilities
and demonstrate its ability to effectively mitigate overfitting in both shallow and deep VAE models and
achieve state-of-the-art performance. iv) We showcase the versatility of AdaVAE by demonstrating
its compatibility with different types of VAE backbone networks. It can also be readily applied to
various VAE variants, thereby enhancing their performance."
RELATED WORKS,0.060109289617486336,"2
Related Works"
RELATED WORKS,0.06557377049180328,"Variational autoencoders (VAEs) have gained popularity as generative models across a wide range
of applications [19, 20, 21, 22, 23, 24]. Extensive research efforts have focused on enhancing the
performance of VAEs [25, 26, 27, 28, 29, 30, 31, 32]. Notably, well-designed encoding/decoding
neural network structures constantly yield state-of-the-art generative performance [4, 5, 6, 33].
Ladder-VAE (LVAE) employs a shared top-down dependency structure in both the inference and
generative models to facilitate information sharing between layers [4]. SkipVAE employs skip
connections that connect latent variables to each layer in the generative model, mitigating posterior"
RELATED WORKS,0.07103825136612021,"collapse [34]. BIVA consists of a skip-connected generative model and an inference model formed
by a bidirectional stochastic inference path [5]. NVAE designs expressive neural network structures
tailored for VAEs based on deep residual networks [6]. Furthermore, advancements in VAE network
structure design have also empowered recent works on hybrid models based on VAEs [35, 36, 37, 38]."
RELATED WORKS,0.07650273224043716,"Current VAE regularization methods often overlook the overfitting effect caused by deep network
structures [12, 10, 15, 11, 13]. Amortized inference regularization (AIR) re-interprets the amortized
inference model as a regularization for maximum likelihood training. AIR encourages the smoothness
of the encoding network to restrict the model capacity, effectively mitigating overfitting and improving
predictive performance [10]. Specifically, AIR proposes a denoising variational autoencoder (DVAE)
that modifies the VAE objective using random perturbation training. An alternative AIR technique
is weight-normalized inference VAE (WNI-VAE) that directly restricts the encoding networks to a
set of smooth functions and achieves comparable performance to DVAE. Both approaches exhibit
a lower inference gap than standard VAEs [39]. Consistency regularization for VAE (CR-VAE)
extends the regularization techniques employed in semi-supervised learning [40, 41] to tackle the
inconsistency problem of the inference models by enforcing the latent representations of an image and
its semantic-preserving transformation to be similar, thereby yielding robust latent representations
accounting for data variation [12]."
RELATED WORKS,0.08196721311475409,"IBP-DGM [14] and BB-VAE[15] as nonparametric Bayesian inference applied to VAE regularization
focus on inferring the dimensionality of latent variable via an Indian Buffet Process (IBP) prior. The
IBP prior is derived by marginalization over a beta process, resulting in a binary vector that masks
the VAE latent variables. Without accommodating network structures, latent variable regularization
alone is inadequate for mitigating overfitting."
EFFICIENT VAE ESTIMATORS,0.08743169398907104,"3
Efficient VAE Estimators"
EFFICIENT VAE ESTIMATORS,0.09289617486338798,"Maximum likelihood estimation of a vector of parameters θ of a deep generative model pθ(x, h) with
x denoting observed variables and h denoting latent variables is intractable in general due to the
marginalization over h. Amortized variational inference optimizes an evidence lower bound (ELBO)
Lθ,ϕ(x) on the log marginal likelihood log pθ(x) = log
R
pθ(x, h)dh by introducing a variational
distribution qϕ(h|x):"
EFFICIENT VAE ESTIMATORS,0.09836065573770492,"log pθ(x) ≥
Z
qϕ(h|x) log pθ(x, h)"
EFFICIENT VAE ESTIMATORS,0.10382513661202186,"qϕ(h|x) dh
(1)"
EFFICIENT VAE ESTIMATORS,0.1092896174863388,= Eqϕ(h|x)[log pθ(x|h)] −KL[qϕ(h|x)||pθ(h)]
EFFICIENT VAE ESTIMATORS,0.11475409836065574,"= Lθ,ϕ(x)"
EFFICIENT VAE ESTIMATORS,0.12021857923497267,"where ϕ denotes variational parameters. For VAEs, qϕ(h|x) denotes an inference model commonly
constructed with a neural network. pθ(x|h) denotes a generative model that can also be parameterized
by a neural network. The ELBO is optimized with gradient-based methods via reparameterization
trick using stochastic Monte Carlo estimators of ∇Lθ,ϕ [1]."
EFFICIENT VAE ESTIMATORS,0.12568306010928962,"IWAE obtains a tighter lower bound using K-sample importance weighting estimate of the log
marginal likelihood [42]:"
EFFICIENT VAE ESTIMATORS,0.13114754098360656,"Lθ,ϕ(x) =Eqϕ(h|x) "" log 1 K K
X k=1"
EFFICIENT VAE ESTIMATORS,0.1366120218579235,pθ(x|hk)pθ(hk)
EFFICIENT VAE ESTIMATORS,0.14207650273224043,qϕ(hk|x) # (2)
EFFICIENT VAE ESTIMATORS,0.14754098360655737,"where hk ∼qϕ(h|x). It shows that the bound gets tighter with increasing K. However, [18] presents
theoretical and empirical evidence that increasing the importance weighted sample size K to tighten
the bound degrades signal-to-noise ratio (SNR) of parameter gradients estimates for the encoding
network, and hurts the learning process. A new estimator (MIWAE) is thus introduced to address the
issue of diminishing SNR, and its gradient estimate is:"
EFFICIENT VAE ESTIMATORS,0.15300546448087432,"∆M,K = 1 M M
X"
EFFICIENT VAE ESTIMATORS,0.15846994535519127,"m=1
∇θ,ϕ log 1 K K
X k=1"
EFFICIENT VAE ESTIMATORS,0.16393442622950818,"pθ(x|hm,k)pθ(hm,k)"
EFFICIENT VAE ESTIMATORS,0.16939890710382513,"qϕ(hm,k|x)
(3)"
EFFICIENT VAE ESTIMATORS,0.17486338797814208,"where hm,k ∼qϕ(hm,k|x). For a fixed budget M × K of total number of hidden variable samples,
the number of samples M reduces the variance in estimating the ELBO gradient. K is the importance
sample size as in IWAE."
VAE STRUCTURE INFERENCE FRAMEWORK,0.18032786885245902,"4
VAE Structure Inference Framework"
VAE STRUCTURE INFERENCE FRAMEWORK,0.18579234972677597,"Traditional model selection cannot effectively adapt pre-determined VAE network structures to data
without incurring significant computation overhead. We thus propose AdaVAE that enables joint
inference on the structures of the encoding/decoding networks using stochastic processes [43, 9] and
latent variables, and optimizes VAE objective without requiring additional expensive computation."
FORMULATION OF THE INFERENCE MODEL,0.1912568306010929,"4.1
Formulation of the Inference Model"
FORMULATION OF THE INFERENCE MODEL,0.19672131147540983,"Let the prior over the latent variables h be a zero-mean isotropic multivariate Gaussian pθ(h) =
N(h; 0, I). We formulate the inference model by letting the variational distribution over h be a
multivariate Gaussian with a diagonal covariance structure:"
FORMULATION OF THE INFERENCE MODEL,0.20218579234972678,"qϕ(h|x, Z) = N(h; µ, σ2I)
(4)"
FORMULATION OF THE INFERENCE MODEL,0.20765027322404372,"where the mean µ and the standard deviation σ are outputs of the encoding neural network Fϕ(x)
with the variational parameters ϕ. The binary matrix Z = [zol ∈{0, 1}] denotes the network
structural variable, as in Figure 1(a)."
FORMULATION OF THE INFERENCE MODEL,0.21311475409836064,"Let fl denote the l-th hidden layer of Fϕ(x) composed of neurons (i.e., non-linear activation functions)
f(·). The encoding network Fϕ(x) has the form:"
FORMULATION OF THE INFERENCE MODEL,0.2185792349726776,"fl = f(Wlfl−1) ⊙z·l + fl−1
l ∈{1, 2, ..., ∞}
(5)"
FORMULATION OF THE INFERENCE MODEL,0.22404371584699453,"where ϕ = {Wl ∈RO×O}, and Wl is the layer l’s weight matrix. ⊙denotes element-wise
multiplication of two vectors, so that we drop out the l-th layer’s outputs by multiplying them
elementwisely by the column vector z·l of Z. Each random variable zol takes the value 1 with
πl ∈[0, 1] indicating activation probability of the l-th layer, as in Figures 1 (b),(c). O is the maximum
number of neurons in a layer that is set to be the same for all hidden layers. We have skip connections
to propagate the outputs of the hidden layers to the output layer. Note that when this network structure
is used as the decoding network for the generative model pθ(x|h), then h and x are swapped, and θ
denotes the weights parameters. The output-layer can be readily replaced with a logistic function and
the normal distribution with a Bernoulli distribution for binary data."
BETA PROCESS PRIOR OVER LAYER NUMBER,0.22950819672131148,"4.2
Beta Process Prior over Layer Number"
BETA PROCESS PRIOR OVER LAYER NUMBER,0.23497267759562843,A beta process B = P
BETA PROCESS PRIOR OVER LAYER NUMBER,0.24043715846994534,"l πlδfl, where δfl is a unit point mass at fl, is a completely random measure
over countably infinite set of pairs (fl, πl) [16], where fl ∈F denotes a hidden-layer function and πl is
its activation probability πl ∈[0, 1]. Its conjugate Bernoulli process can be defined as Zo· ∼BeP(B),
where Zo· = P"
BETA PROCESS PRIOR OVER LAYER NUMBER,0.2459016393442623,"l zolδfl is at the same locations δfl as B where zol are independent Bernoulli variables
with πl being the probability of zol = 1. As in Eqn. (5), zol = 1 activates the o’th neuron in layer l.
Computationally, we employ the stick-breaking construction [17] of beta process and its conjugate
Bernoulli process as"
BETA PROCESS PRIOR OVER LAYER NUMBER,0.25136612021857924,"zol ∼Ber(πl),
πl = lY"
BETA PROCESS PRIOR OVER LAYER NUMBER,0.2568306010928962,"j=1
νj,
νl ∼Beta(α, β)
(6)"
BETA PROCESS PRIOR OVER LAYER NUMBER,0.26229508196721313,"where νl are sequentially drawn from a beta distribution. The hyperparameters α and β can be set to
balance the network depth and width. Specifically, Figure 1 (b) demonstrates that if β > α > 1, the
network structure prior favors shallower but wider network structures with the first few layer-wise
activation probabilities being high. If α > β > 1, activation probabilities tend to be low over larger
number of active layers, and the prior prefers a deeper but narrower network, as in Figure 1 (c)."
BETA PROCESS PRIOR OVER LAYER NUMBER,0.2677595628415301,We thus define the prior over the encoding network structural variable Z as
BETA PROCESS PRIOR OVER LAYER NUMBER,0.273224043715847,"pα,β(Z, ν) = pα,β(ν)p(Z|ν) = ∞
Y"
BETA PROCESS PRIOR OVER LAYER NUMBER,0.2786885245901639,"l=1
Beta(νl|α, β) O
Y"
BETA PROCESS PRIOR OVER LAYER NUMBER,0.28415300546448086,"o=1
Ber(zol|πl)
(7)"
BETA PROCESS PRIOR OVER LAYER NUMBER,0.2896174863387978,"To enable asymmetric encoding/decoding network structures, we independently apply the prior to
both networks. Further analysis on symmetric constraints can be found in the Appendix."
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.29508196721311475,"4.3
Joint Inference on VAE Network Structures and Latent Variables"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3005464480874317,We first expand the overall marginal likelihood over the VAE structures Z as
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.30601092896174864,"log pα,β(x) = KL[q(Z, ν|{at}T
t=1, {bt}T
t=1)||pα,β(Z, ν|x)] + L{at},{bt}(x)
(8)"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3114754098360656,"where the first RHS term denotes a Kullback-Leibler (KL) divergence of the approximate variational
distribution from the true posterior of the VAE network structural variables. We specify the variational
distribution as"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.31693989071038253,"q(Z, ν|{at}T
t=1, {bt}T
t=1) = T
Y"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3224043715846995,"t=1
Beta(νt|at, bt) O
Y"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.32786885245901637,"o=1
ConBer(zot|πt)
(9)"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3333333333333333,"where πt = Qt
j=1 νj, and {at, bt}T
t=1 are the variational parameters. T denotes a truncation level for
the maximum number of hidden layers [17]. We also relax the constraint of the discrete variables
by reparameterizing the Bernoulli distribution into a concrete Bernoulli distribution ConBer(zot|πt)
[44, 45]. This allows us to efficiently backpropagate the parameter gradients of the estimator while
generating network structure samples."
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.33879781420765026,The second RHS term in Eqn. (8) denotes the ELBO to the overall marginal likelihood:
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3442622950819672,"L{at},{bt}(x) =
Z
q(Z, ν|{at}T
t=1{bt}T
t=1)(log pθ(x|Z) + log pα,β(Z, ν)"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.34972677595628415,"−log q(Z, ν|{at}T
t=1{bt}T
t=1))dZdν
(10)
The term log pθ(x|Z) in Eqn. (10) is the marginal likelihood over the latent variable h, which is an
extension of Eqn. (1), in terms of conditioning on the structure variable Z. Thus, the ELBO to the
marginal likelihood log pθ(x|Z) is:"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3551912568306011,"log pθ(x|Z) ≥
Z
qϕ(h|x, Z) log pθ(x, h|Z)"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.36065573770491804,"qϕ(h|x, Z)dh"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.366120218579235,"= Eqϕ(h|x,Z)[log pθ(x|h, Z)] −KL[qϕ(h|x, Z)||pθ(h|Z)]"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.37158469945355194,"= Lθ,ϕ(x|Z)
(11)"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3770491803278688,"Lemma 1 Let Q(h|x) =
R
qϕ(h|x, Z)q(Z, ν)dZdν be the variational distribution of the latent
variable h marginalizing over Z, then"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3825136612021858,"Eq(Z,ν)Eqϕ(h|x,Z) log pθ(x, h|Z)"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3879781420765027,"qϕ(h|x, Z) ≤EQ(h|x) log pθ(x, h|Z)"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.39344262295081966,"Q(h|x)
(12)"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.3989071038251366,"Lemma 1 indicates that the overall ELBO we derived on the left-hand side in Eqn. (12) bounds the
lower bound to the marginal likelihood over h. The proof is in the appendix. In particular, q(Z, ν) is
essentially a non-explicit mixing distribution [46]. It allows the variational distribution Q(h|x) to
take complex form, and results in more informative latent representation."
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.40437158469945356,"We adopt Monte Carlo estimation of the expectations over both Z in Eqn. (10) and h in Eqn. (11)
to estimate the overall ELBO. In particular, we extend the MIWAE estimator in Eqn. (3), and
introduce three sample sizes to tune: the number of samples S used for Monte Carlo estimation of the
expectation over the VAE network structure variable Z, the number of samples Ms used for Monte
Carlo estimation of the gradient of the latent variable ELBO conditioned on the structure samples in
Eqn. (11), and the number of importance samples Ks used for estimation of the expectation over the
latent variables h. We thus express our gradient estimate in the general form as"
JOINT INFERENCE ON VAE NETWORK STRUCTURES AND LATENT VARIABLES,0.4098360655737705,"∆S,M,K = 1 S S
X s=1"
MS,0.41530054644808745,"1
Ms Ms
X"
MS,0.4207650273224044,"m=1
∇θ,ϕ log 1 Ks Ks
X k=1"
MS,0.4262295081967213,"pθ(x|hm,k, Zs)pθ(hm,k|Zs)"
MS,0.43169398907103823,"qϕ(hm,k|x, Zs)
(13)"
MS,0.4371584699453552,"where hm,k ∼qϕ(h|x, Z). When S = 1 our estimator becomes equivalent to the MIWAE objective
in Eqn. (3). Since our estimator generates latent variable samples conditioned on network structure
samples, increasing S will not impact the SNR of the gradient estimator in Eqn. (13)."
MS,0.4426229508196721,"Theorem 1 Let LS be the lower bound with S structure samples of Zs ∼q(Z, ν), then:"
MS,0.44808743169398907,"LS ≤LS+1 ≤log pα,β(x), LS = Eq(Z,ν)Eqϕ(h|x,Z) log[ 1 S X s"
MS,0.453551912568306,"pθ(x, h|Zs)
qϕ(h|x, Zs)]
(14)"
MS,0.45901639344262296,"Figure 2: Left: Evolution of the encoding/decoding network structures visualized through layer-wise
activation probabilities πl (top) and neuron activations Z (bottom) with a reconstructed sample.
Right: the top shows the median change of the number of active layers over training epochs and the
percentage of activated neurons in the truncation. The bottom shows the convergence of the proposed
estimator in terms of negative log-likelihood (−LL) for different structure sample size S."
MS,0.4644808743169399,"Table 1: Test performance in negative log-likelihood (−LL) mean ±1 standard deviation (lower the
better) over 4 runs with random initialization. The overall best result on each dataset is bolded."
MS,0.46994535519125685,"Dataset
(M,K)
S=1
S=2
S=4
S=8
MNIST
(8,8)
82.25±0.05
82.50±0.00
82.60±0.10
82.30±0.10
(4,16)
82.47±0.45
82.33±0.13
82.52±0.02
83.02±0.20
Omniglot
(8,8)
107.10±0.10
106.45±0.10
106.55±0.30
106.34±0.01
(4,16)
108.12±0.16
107.15±0.08
107.35±0.40
108.30±0.50
Caltech101
(8,8)
116.83±1.57
114.94±0.45
114.00±0.42
113.54±0.40
(4,16)
116.30±1.11
114.55±1.18
113.02±0.34
112.53±0.42"
MS,0.47540983606557374,"Proof of this theorem is in the appendix. The theorem shows the convergence of our estimator.
Specifically, increasing S leads to a tighter lower bound for the overall marginal likelihood. Training
an encoding/decoding network with depth L and width M, the time complexity is Tc = O(NBLM 2)
with N training examples and B epochs. Our method is linearly scalable as STc. With a proper
thresholding, the number of active layers L is relatively small in each sample."
EXPERIMENTS,0.4808743169398907,"5
Experiments"
EXPERIMENTS,0.48633879781420764,"We analyze the behavior of our inference framework across various tasks. We study how AdaVAE
facilitates the evolution of encoding/decoding network structures for inferring the most plausible
depth from the given data, while generating expressive latent representations. Next, we explore the
impact of the structure sample size S on the convergence of the proposed estimator in Eqn.(13).
Then we show that AdaVAE effectively mitigates overfitting in both shallow and deep network
settings, leading to state-of-the-art performance on benchmark datasets. Finally, we demonstrate the
framework’s compatibility with different types of backbone networks and VAE variants. 2"
ADAPTIVE VAE NETWORK STRUCTURES,0.4918032786885246,"5.1
Adaptive VAE Network Structures"
ADAPTIVE VAE NETWORK STRUCTURES,0.4972677595628415,"AdaVAE enables us to perform joint inference on both encoding/decoding network structures and
latent variables. To investigate how network structures evolve during training epochs, we set the
truncation level T = 25 on MLP backbone nets with tanh non-linearities. We analyze adaVAE’s
behavior on 28×28 binarized MNIST images [47], employing structure sample sizes S = {1, 2, 4, 8}.
We run the experiments 3 times and averaging the outcomes. Figure 2 Left shows the evolution of the
encoding/decoding network structures for one trial with S = 8. AdaVAE initializes multiple hidden"
ADAPTIVE VAE NETWORK STRUCTURES,0.5027322404371585,2Implementation details are in the Appendix. Codes are provided.
ADAPTIVE VAE NETWORK STRUCTURES,0.5081967213114754,"Figure 3: The performance of VAE regularization methods changes with network depths. Our
proposed method effectively prevents overfitting for both small and large truncations T, consistently
achieving the best performance. In contrast, for 1 ≤L ≤5, the performance of other regularization
methods initially improves but then starts to decline, suggesting they suffer from overfitting even for
shallow structures."
ADAPTIVE VAE NETWORK STRUCTURES,0.5136612021857924,"Table 2: The best performance of our method and the VAE regularization methods in Figure 3. We
also demonstrate the compatibility of our framework."
ADAPTIVE VAE NETWORK STRUCTURES,0.5191256830601093,"MNIST
Omniglot
Caltech 101
-LL ↓
MI ↑
-LL ↓
MI ↑
-LL ↓
MI ↑
MIWAE [18]
86.11±0.01
9.13±0.01
110.61±0.10
8.98±0.00
116.19±0.08
7.64±0.01
MIWAE+DO
90.99±0.01
9.13±0.01
110.89±0.01
8.98±0.00
116.00±0.40
7.63±0.01
DVAE [10]
87.67±0.17
9.07±0.00
112.04±0.13
8.96±0.00
113.71±0.43
7.53±0.01
CR-VAE [12]
87.67±0.05
9.03±0.01
109.94±0.19
8.97±0.00
117.46±0.10
7.37±0.01
IBP-DGM [14]
92.24±0.75
-
124.01±1.62
-
135.23±0.54
-
BB-VAE [15]
91.55±0.69
-
124.47±0.58
-
135.18±0.83
-
Ours
82.30±0.10
9.13±0.01
106.34±0.01
8.98±0.00
113.54±0.40
7.67±0.02
Ours+DVAE
83.30±0.20
9.07±0.02
107.80±0.40
8.96±0.00
111.92±0.37
7.63±0.00
Ours+CR-VAE
85.20±0.20
9.03±0.01
105.60±0.05
8.97±0.00
108.93±1.40
7.51±0.02"
ADAPTIVE VAE NETWORK STRUCTURES,0.5245901639344263,"layers with sparsely activated neurons, gradually converging to fewer fully activated layers. Figure
2 right top presents the medians of the number of active hidden layers in the encoding/decoding
networks, as well as the percentage of activated neurons in the truncation changing over epochs. The
encoding network structure stabilizes at two active layers, while the decoding network settles at six
active layers. The decoding network tends to have more active layers compared to the encoding
network on Omniglot as well (see Appendix). Figure 2 right bottom shows the generative performance
assessed by negative log-likelihood (−LL) converge faster with an increased structure sample size S,
which is consistent with Theorem 1."
EFFECT OF STRUCTURE SAMPLE SIZES,0.5300546448087432,"5.2
Effect of Structure Sample Sizes"
EFFECT OF STRUCTURE SAMPLE SIZES,0.5355191256830601,"We assess our estimator using three benchmark datasets: MNIST [47], Omniglot [48], and Caltech101
Silhouettes [49]. In each minibatch, we set a budget of S×Ms×Ks = 64 total latent variable samples
for each datapoint. We examine four settings of the VAE structure sample size S = {1, 2, 4, 8}, along
with latent variable sample sizes (M, K) = {(8, 8), (4, 16)} as in [18]. The truncation level is T = 25
with a maximum width O = 200. The distribution over the output from the decoding networks is
factorized Bernoulli. Table 1 indicates larger values of S generally yield better performance. For
MNIST, there is no statistically significant difference between S = 1 and S = 8. Among the two
structure sample sizes, the best importance sample configuration is (M, K) = (8, 8)."
ON PREVENTING OVERFITTING,0.5409836065573771,"5.3
On Preventing Overfitting"
ON PREVENTING OVERFITTING,0.546448087431694,"To assess the performance of our method across varying truncation level T, we compare with
existing VAE regularization methods: Denoising VAE (DVAE) [10], CR-VAE [12], and BB-VAE
[15], MIWAE with dropout (MIWAE+DO), along with vanilla MIWAE [18]. as in Figure 3. All
methods share the same maximum width of O = 200 and a latent variable dimensionality of 50. For"
ON PREVENTING OVERFITTING,0.5519125683060109,"Figure 4: Visualization of the latent representation via t-SNE embeddings for the MNIST dataset. The
embeddings are colored based on class labels. DVAE and CR-VAE combined with our framework
result in better representations."
ON PREVENTING OVERFITTING,0.5573770491803278,"Figure 5: Influence of the maximum number of neurons per layer O on our method. When O is
small (e.g., O ≤100), we tend to have shallower encoding/decoding networks. As O becomes
reasonably large (e.g., O ≥100), it tends not to have significant influence on the depth. Meanwhile,
the percentages of activated neurons in the truncation become stable."
ON PREVENTING OVERFITTING,0.5628415300546448,"IBP-DGM and BB-VAE, we set the maximum dimensionality of the latent variables to 100, following
the setup of [15]. In Figure 3, we set depth L for the regularization methods and truncation level T
for our method over the range L = T = {1, 2, 3, 4, 5, 10, 15, 20, 25}, allowing us to compare their
effectiveness in both shallow (i.e., L = T ≤5) and deep (i.e., L = T ≥5) VAE network structures."
ON PREVENTING OVERFITTING,0.5683060109289617,"Figure 3 shows that when L = T ≤5, the VAE regularization methods can mitigate overfitting.
However, even for the shallow cases as L = T = {4, 5}, the performance of these methods is
affected by overfitting, as evidenced by the “U”-shaped performance curves, indicating the classical
variance-bias trade-off. Despite our method exhibiting slight underfitting issues for L = T ≤2, it
still outperforms other methods. For deep VAE structures with T/L = {10, 15, 20, 25}, Figure 3
shows that our method’s performance is minimally impacted by large truncations. This robustness
indicates our method’s ability to mitigate overfitting for large VAE structure settings. In contrast, as
the depth L increases, the steadily increasing negative log-likelihood (−LL) for other regularization
methods suggests their inability to prevent overfitting in deep VAE models. Overall, AdaVAE
consistently delivers superior performance for both shallow and deep VAE structures. An analysis of
the computational time required by our framework and its comparison to the baselines is presented in
the Appendix."
ON PREVENTING OVERFITTING,0.5737704918032787,"Table 2 highlights our superiority over other methods in terms of density estimation and mutual
information (MI) [50]. Additionally, when incorporating our framework with DVAE through the
addition of Gaussian noise to the input or with CR-VAE by integrating the consistency regularization
term into our estimator, we observe enhanced performance on Omniglot and Caltech101 datasets.
Furthermore, in Figure 4, we visualize the latent representations of the VAE methods. DVAE and CR-
VAE when combined with our framework result in well-separated clusters, indicating that application
of our framework allows VAEs to learn a meaningful latent representation. A quantitative evaluation
of the latent representations is presented in the Appendix by analyzing the performance of VAE
methods on a downstream classification task."
EFFECTS OF THE MAXIMUM WIDTH O,0.5792349726775956,"5.4
Effects of the Maximum Width O"
EFFECTS OF THE MAXIMUM WIDTH O,0.5846994535519126,"We further investigate the influence of the width O (i.e., the maximum number of neurons per
layer). Figure 5 shows the evolution of the medians of the number of active hidden layers (i.e., the
hidden layers with activated neurons) as O increases. When O ≤100, we tend to have shallower"
EFFECTS OF THE MAXIMUM WIDTH O,0.5901639344262295,"(a)
(b)
(c)
(d)"
EFFECTS OF THE MAXIMUM WIDTH O,0.5956284153005464,"Figure 6: Our framework’s performance on different VAE backbone networks. (a) and (b) show
VAE and β-VAE [51] with convolutional layers on MNIST and Omniglot datasets. (c) and (d) show
VGAEs [52] with graph convolutional layers on Cora and Citeseer datasets."
EFFECTS OF THE MAXIMUM WIDTH O,0.6010928961748634,"encoding/decoding networks with a lower percentage of activated neurons in the truncation. However,
when O is reasonably large as O ≥100, it has no influence on the network depth. In particular, the
percentage of activated neurons in the whole truncation also remains relatively stable. This suggests
that our method can automatically balance network depth and width to maintain the best performance."
APPLICATION TO VAE BACKBONE NETWORKS,0.6065573770491803,"5.5
Application to VAE Backbone Networks"
APPLICATION TO VAE BACKBONE NETWORKS,0.6120218579234973,"We demonstrate adaVAE’s efficacy by applying it to VAEs with different types of encoding/decoding
backbone networks. To infer the number of convolutional layers in a convolutional VAE (cVAE)
using the beta process, we readily mask the convolution channels in layer l with z·l. Figure 6 (a) and
(b) show that by adapting network structures to the data we improve the overall performance of cVAE
and β-cVAE [51]. The adaptive backbone networks effectively prevent overfitting for deep structure
settings (i.e., T/L ≥15)."
APPLICATION TO VAE BACKBONE NETWORKS,0.6174863387978142,"Variational graph autoencoder (VGAE) [52] encodes graph nodes to latent embeddings with graph
convolutional (GC) layers and re-creates the input graph from the embeddings by predicting the
existence of an edge between the nodes. We combine our framework with VGAE by elementwisely
multiplying the GC channels (i.e., the feature vectors) of layer l with z·l in both its encoding/decoding
networks. We compare the hybrid method’s performance with graph autoencoders (GAEs) [52] and
vanilla VGAEs on two benchmark graph-structured datasets: Cora [53] and Citeseer [54]. The AUC
scores of link prediction over varying numbers of GC layer settings are shown in Figure 6 (c) and (d).
Our framework enables the VGAE to maintain its best performance for all the network depth settings
by automatically adapting its structures to the data, whereas the performance of GAE and vanilla
VGAE drops with the increase of the layer numbers (e.g., T/L = {6, 8, 10})."
APPLICATION TO VAE VARIANTS,0.6229508196721312,"5.6
Application to VAE Variants"
APPLICATION TO VAE VARIANTS,0.6284153005464481,"Table 3: Performance comparison of VAE variants with and without
our inference framework in terms of −LL, MI, and KL divergence."
APPLICATION TO VAE VARIANTS,0.6338797814207651,"Methods
-LL ↓
MI ↑
KL ↑
β-cVAE (β = 2) [51]
106.50±0.12
8.46±0.01
18.01±0.21
Ours + β-cVAE (β = 2)
102.45±0.07
8.47±0.01
20.65±0.02
LVAE [4]
136.50±1.50
8.43±0.02
20.05±0.21
Ours+LVAE
121.48±0.67
8.50±0.01
21.00±0.47
SkipVAE [34]
112.68±0.80
8.50±0.01
22.82±0.79
Ours+SkipVAE
108.00±0.04
8.50±0.01
28.26±0.10
NVAE [6]
98.83±0.17
8.51±0.01
34.97±0.13
Ours+NVAE
99.10±0.20
8.51±0.01
37.85±0.50"
APPLICATION TO VAE VARIANTS,0.639344262295082,"We assess the performance of
our inference framework by
leveraging it to adapt the net-
work structures of VAE vari-
ants to data. Specifically, for
β-VAE, we apply layer-wise bi-
nary masks z·l to the convolu-
tional channels and infer the
layer numbers using the beta
process. In the case of Ladder-
VAE (LVAE) [4], we adjust its
depth by applying layer-wise
binary masks to its determin-
istic layers in the bottom-up
dependency structures and add
skip connections between the stochastic layers. For SkipVAE [34], we model its depth by employing
layer-wise binary masks and skip connections in both its encoding/decoding networks. The expressive
network structures of NVAE [6] consists of multiple blocks of convolutional layers. We apply our"
APPLICATION TO VAE VARIANTS,0.644808743169399,"framework to infer the number of blocks for a light NVAE version without top-down dependency
between the blocks. The detailed settings of these VAE variants and additional results can be found
in Appendix. The results in Table 3 on FashionMNIST [55] demonstrate that by inferring the encod-
ing/decoding network structures we significantly improve the density estimation performance of the
VAE variants. Our framework also boosts their ability to mitigate posterior collapse as indicated by
MI and KL divergence."
CONCLUSION,0.6502732240437158,"6
Conclusion"
CONCLUSION,0.6557377049180327,"We present a Bayesian inference framework and a scalable estimator that automatically adapts VAE
network structures to data. The experiments demonstrate its effectiveness in preventing overfitting
for both shallow and deep structure settings. Moreover, AdaVAE exhibits promising applications
across various types of VAE backbone networks and VAE variants, including those with hierarchical
structures such as LVAE. Notably, AdaVAE enhances the generative performance of these models
without requiring pre-determined network structures prior to training. Our future work entails relaxing
the constraint of truncation levels by incorporating the Russian roulette method [56] and scaling up
the inference for large images."
ACKNOWLEDGEMENTS,0.6612021857923497,"7
Acknowledgements"
ACKNOWLEDGEMENTS,0.6666666666666666,"This material is based upon work supported by the National Science Foundation under NSF Award
No.2045804 and Award No.1850492. We are thankful to Kishan KC for helpful discussion. We
acknowledge Research Computing at the Rochester Institute of Technology [57] for providing
computational resources."
REFERENCES,0.6721311475409836,References
REFERENCES,0.6775956284153005,"[1] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the International
Conference on Learning Representations (ICLR), 2014."
REFERENCES,0.6830601092896175,"[2] Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. In International Conference on Machine Learning (ICML), pages
1278–1286. PMLR, 2014."
REFERENCES,0.6885245901639344,"[3] Jyoti Aneja, Alex Schwing, Jan Kautz, and Arash Vahdat. A contrastive learning approach for training
variational autoencoder priors. In Advances in Neural Information Processing Systems, volume 34, pages
480–493, 2021."
REFERENCES,0.6939890710382514,"[4] Casper Kaae S, Tapani Raiko, Lars Maalø e, Søren Kaae Sø nderby, and Ole Winther. Ladder variational
autoencoders. In Advances in Neural Information Processing Systems (NeurIPS), volume 29, 2016."
REFERENCES,0.6994535519125683,"[5] Lars Maalœ, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIVA: A very deep hierarchy of latent
variables for generative modeling. In Advances in Neural Information Processing Systems (NeurIPS),
volume 32, 2019."
REFERENCES,0.7049180327868853,"[6] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In Advances in Neural
Information Processing Systems (NeurIPS), volume 33, pages 19667–19679, 2020."
REFERENCES,0.7103825136612022,"[7] Corinna Cortes, Xavier Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. AdaNet: Adaptive
structural learning of artificial neural networks. In Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 874–883. PMLR,
2017."
REFERENCES,0.7158469945355191,"[8] Konstantinos Panousis, Sotirios Chatzis, and Sergios Theodoridis. Nonparametric Bayesian deep networks
with local competition. In International Conference on Machine Learning, pages 4980–4988. PMLR,
2019."
REFERENCES,0.7213114754098361,"[9] Kishan KC, Rui Li, and Mohammad Mahdi Gilany. Joint inference for neural network depth and dropout
regularization. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, 2021."
REFERENCES,0.726775956284153,"[10] Rui Shu, Hung H Bui, Shengjia Zhao, Mykel J Kochenderfer, and Stefano Ermon. Amortized inference
regularization. In Advances in Neural Information Processing Systems (NeurIPS), volume 31, 2018."
REFERENCES,0.73224043715847,"[11] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing
robust features with denoising autoencoders. In Proceedings of the 25th International Conference on
Machine Learning (ICML), pages 1096–1103, 2008."
REFERENCES,0.7377049180327869,"[12] Samarth Sinha and Adji Bousso Dieng. Consistency regularization for variational auto-encoders. In
Advances in Neural Information Processing Systems (NeurIPS), volume 34, 2021."
REFERENCES,0.7431693989071039,"[13] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-encoders:
Explicit invariance during feature extraction. In International Conference on Machine Learning (ICML),
2011."
REFERENCES,0.7486338797814208,"[14] Sotirios P Chatzis. Indian buffet process deep generative models for semi-supervised classification. In 2018
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2456–2460.
IEEE, 2018."
REFERENCES,0.7540983606557377,"[15] Rachit Singh, Jeffrey Ling, and Finale Doshi-Velez. Structured variational autoencoders for the beta-
bernoulli process. In NIPS 2017 Workshop on Advances in Approximate Bayesian Inference, 2017."
REFERENCES,0.7595628415300546,"[16] Romain Thibaux and Michael I Jordan. Hierarchical beta processes and the Indian buffet process. In
Artificial Intelligence and Statistics (AISTATS), pages 564–571. PMLR, 2007."
REFERENCES,0.7650273224043715,"[17] John W Paisley, Aimee K Zaas, Christopher W Woods, Geoffrey S Ginsburg, and Lawrence Carin. A
stick-breaking construction of the beta process. In International Conference on Machine Learning (ICML),
2010."
REFERENCES,0.7704918032786885,"[18] Tom Rainforth, Adam Kosiorek, Tuan Anh Le, Chris Maddison, Maximilian Igl, Frank Wood, and
Yee Whye Teh. Tighter variational bounds are not necessarily better. In International Conference on
Machine Learning (ICML), pages 4277–4285. PMLR, 2018."
REFERENCES,0.7759562841530054,"[19] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexan-
der Lerchner. Towards a definition of disentangled representations. arXiv preprint arXiv:1812.02230,
2018."
REFERENCES,0.7814207650273224,"[20] Fabian Falck, Haoting Zhang, Matthew Willetts, George Nicholson, Christopher Yau, and Chris C Holmes.
Multi-facet clustering variational autoencoders. In Advances in Neural Information Processing Systems
(NeurIPS), volume 34, pages 8676–8690, 2021."
REFERENCES,0.7868852459016393,"[21] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement
in variational autoencoders. In Advances in Neural Information Processing Systems (NeurIPS), volume 31,
2018."
REFERENCES,0.7923497267759563,"[22] Abhishek Kumar and Ben Poole. On implicit regularization in β-VAEs. In International Conference on
Machine Learning (ICML), pages 5480–5490. PMLR, 2020."
REFERENCES,0.7978142076502732,"[23] Dazhong Shen, Chuan Qin, Chao Wang, Hengshu Zhu, Enhong Chen, and Hui Xiong. Regularizing
variational autoencoder with diversity and uncertainty awareness. arXiv preprint arXiv:2110.12381, 2021."
REFERENCES,0.8032786885245902,"[24] Vaibhav Saxena, Jimmy Ba, and Danijar Hafner. Clockwork variational autoencoders. In Advances in
Neural Information Processing Systems (NeurIPS), volume 34, pages 29246–29257, 2021."
REFERENCES,0.8087431693989071,"[25] Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoencoders. In
Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research (PMLR),
pages 1086–1094. PMLR, 2018."
REFERENCES,0.8142076502732241,"[26] Joe Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. In Proceedings of the 35th
International Conference on Machine Learning (ICML), volume 80 of Proceedings of Machine Learning
Research, pages 3403–3412. PMLR, 10–15 Jul 2018."
REFERENCES,0.819672131147541,"[27] Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In Proceedings of
The 33rd International Conference on Machine Learning (ICML), volume 48 of Proceedings of Machine
Learning Research (PMLR), pages 324–333, 20–22 Jun 2016."
REFERENCES,0.825136612021858,"[28] Yingzhen Li and Richard E Turner. Rényi divergence variational inference. In Advances in Neural
Information Processing Systems (NeurIPS), volume 29, 2016."
REFERENCES,0.8306010928961749,"[29] Arash Vahdat, Evgeny Andriyash, and William Macready. Dvae#: Discrete variational autoencoders with
relaxed boltzmann priors. In Advances in Neural Information Processing Systems (NeurIPS), volume 31,
2018."
REFERENCES,0.8360655737704918,"[30] Ali Razavi, Aäron van den Oord, Ben Poole, and Oriol Vinyals. Preventing posterior collapse with
delta-vaes. In 7th International Conference on Learning Representations, (ICLR) 2019, New Orleans, LA,
USA, May 6-9, 2019, 2019."
REFERENCES,0.8415300546448088,"[31] James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don't blame the elbo! a linear
vae perspective on posterior collapse. In Advances in Neural Information Processing Systems (NeurIPS),
volume 32, 2019."
REFERENCES,0.8469945355191257,"[32] Alexej Klushyn, Nutan Chen, Richard Kurle, Botond Cseke, and Patrick van der Smagt. Learning
hierarchical priors in vaes. In Advances in Neural Information Processing Systems, volume 32, 2019."
REFERENCES,0.8524590163934426,"[33] Jakob D. Havtorn, Jes Frellsen, Søren Hauberg, and Lars Maaløe. Hierarchical VAEs know what they
don’t know. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference
on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research (PMLR), pages
4117–4128, 18–24 Jul 2021."
REFERENCES,0.8579234972677595,"[34] Adji B. Dieng, Yoon Kim, Alexander M. Rush, and David M. Blei. Avoiding latent variable collapse
with generative skip models. In Proceedings of the Twenty-Second International Conference on Artificial
Intelligence and Statistics (AISTATS), Proceedings of Machine Learning Research, pages 2397–2405, 2019."
REFERENCES,0.8633879781420765,"[35] Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved
variational inference with inverse autoregressive flow. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information Processing Systems (NeurIPS), volume 29, 2016."
REFERENCES,0.8688524590163934,"[36] Vaden Masrani, Tuan Anh Le, and Frank Wood. The thermodynamic variational objective. In Advances in
Neural Information Processing Systems (NeurIPS), volume 32, 2019."
REFERENCES,0.8743169398907104,"[37] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2.
In Advances in Neural Information Processing Systems, volume 32, 2019."
REFERENCES,0.8797814207650273,"[38] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In Advances
in Neural Information Processing Systems, volume 34, pages 11287–11302, 2021."
REFERENCES,0.8852459016393442,"[39] Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoencoders. In
International Conference on Machine Learning (ICML), pages 1078–1086. PMLR, 2018."
REFERENCES,0.8907103825136612,"[40] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In Advances in
Neural Information Processing Systems (NeurIPS), volume 27, 2014."
REFERENCES,0.8961748633879781,"[41] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations
and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing
Systems (NeurIPS), volume 29, 2016."
REFERENCES,0.9016393442622951,"[42] Yuri Burda, Roger B Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In Proceedings
of the International Conference on Learning Representations (ICLR), 2016."
REFERENCES,0.907103825136612,"[43] Tamara Broderick, Michael I Jordan, and Jim Pitman. Beta processes, stick-breaking and power laws.
Bayesian Analysis, 7(2):439–476, 2012."
REFERENCES,0.912568306010929,"[44] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of
discrete random variables. arXiv preprint arXiv:1611.00712, 2016."
REFERENCES,0.9180327868852459,"[45] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016."
REFERENCES,0.9234972677595629,"[46] Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In International Conference on
Machine Learning (ICML), pages 5660–5669. PMLR, 2018."
REFERENCES,0.9289617486338798,"[47] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, volume 86 (11), page 2278–2324, 2014."
REFERENCES,0.9344262295081968,"[48] Brenden M. Lake, Russ R. Salakhutdinov, and Josh Tenenbaum.
One-shot learning by inverting a
compositional causal process. In Advances in Neural Information Processing Systems (NeurIPS), 2013."
REFERENCES,0.9398907103825137,"[49] Benjamin Marlin, Kevin Swersky, Bo Chen, and Nando Freitas. Inductive principles for restricted
boltzmann machine learning. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the
Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), volume 9, pages
509–516. PMLR, 13–15 May 2010."
REFERENCES,0.9453551912568307,"[50] Matthew D Hoffman and Matthew J Johnson. ELBO surgery: Yet another way to carve up the variational
evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NeurIPS, volume 1,
2016."
REFERENCES,0.9508196721311475,"[51] Irina Higgins, Loïc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained
variational framework. In Proceedings of the International Conference on Learning Representations
(ICLR), 2017."
REFERENCES,0.9562841530054644,"[52] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308,
2016."
REFERENCES,0.9617486338797814,"[53] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construc-
tion of internet portals with machine learning. Information Retrieval, 3:127–163, 2000."
REFERENCES,0.9672131147540983,"[54] C Lee Giles, Kurt D Bollacker, and Steve Lawrence. CiteSeer: An automatic citation indexing system. In
Proceedings of the third ACM Conference on Digital Libraries, pages 89–98, 1998."
REFERENCES,0.9726775956284153,"[55] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.9781420765027322,"[56] Kai Xu, Akash Srivastava, and Charles Sutton. Variational russian roulette for deep bayesian nonparamet-
rics. In International Conference on Machine Learning (ICML), pages 6963–6972. PMLR, 2019."
REFERENCES,0.9836065573770492,[57] Research Computing at Rochester Institute of Technology. https://doi.org/10.34788/0S3G-QD15.
REFERENCES,0.9890710382513661,"A
Appendix"
REFERENCES,0.994535519125683,This section is in the supplemental material.
