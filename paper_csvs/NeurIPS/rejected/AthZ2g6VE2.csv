Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009587727708533077,"In Distributed optimization and Learning, and even more in the modern framework
1"
ABSTRACT,0.0019175455417066154,"of federated learning, communication, which is slow and costly, is critical. We
2"
ABSTRACT,0.0028763183125599234,"introduce LoCoDL, a communication-efficient algorithm that leverages the two
3"
ABSTRACT,0.003835091083413231,"popular and effective techniques of Local training, which reduces the communi-
4"
ABSTRACT,0.004793863854266539,"cation frequency, and Compression, in which short bitstreams are sent instead of
5"
ABSTRACT,0.005752636625119847,"full-dimensional vectors of floats. LoCoDL works with a large class of unbiased
6"
ABSTRACT,0.006711409395973154,"compressors that includes widely-used sparsification and quantization methods.
7"
ABSTRACT,0.007670182166826462,"LoCoDL provably benefits from local training and compression and enjoys a doubly-
8"
ABSTRACT,0.00862895493767977,"accelerated communication complexity, with respect to the condition number of
9"
ABSTRACT,0.009587727708533078,"the functions and the model dimension, in the general heterogenous regime with
10"
ABSTRACT,0.010546500479386385,"strongly convex functions. This is confirmed in practice, with LoCoDL outperform-
11"
ABSTRACT,0.011505273250239693,"ing existing algorithms.
12"
INTRODUCTION,0.012464046021093002,"1
Introduction
13"
INTRODUCTION,0.013422818791946308,"Performing distributed computations is now pervasive in all areas of science. Notably, Federated
14"
INTRODUCTION,0.014381591562799617,"Learning (FL) consists in training machine learning models in a distributed and collaborative way
15"
INTRODUCTION,0.015340364333652923,"(Koneˇcný et al., 2016a,b; McMahan et al., 2017; Bonawitz et al., 2017). The key idea in this rapidly
16"
INTRODUCTION,0.016299137104506232,"growing field is to exploit the wealth of information stored on distant devices, such as mobile phones
17"
INTRODUCTION,0.01725790987535954,"or hospital workstations. The many challenges to face in FL include data privacy and robustness
18"
INTRODUCTION,0.01821668264621285,"to adversarial attacks, but communication-efficiency is likely to be the most critical (Kairouz et al.,
19"
INTRODUCTION,0.019175455417066157,"2021; Li et al., 2020a; Wang et al., 2021). Indeed, in contrast to the centralized setting in a datacenter,
20"
INTRODUCTION,0.020134228187919462,"in FL the clients perform parallel computations but also communicate back and forth with a distant
21"
INTRODUCTION,0.02109300095877277,"orchestrating server. Communication typically takes place over the internet or cell phone network,
22"
INTRODUCTION,0.02205177372962608,"and can be slow, costly, and unreliable. It is the main bottleneck that currently prevents large-scale
23"
INTRODUCTION,0.023010546500479387,"deployment of FL in mass-market applications.
24"
INTRODUCTION,0.023969319271332695,"Two strategies to reduce the communication burden have been popularized by the pressing needs
25"
INTRODUCTION,0.024928092042186004,"of FL: 1) Local Training (LT), which consists in reducing the communication frequency. That is,
26"
INTRODUCTION,0.02588686481303931,"instead of communicating the output of every computation step involving a (stochastic) gradient call,
27"
INTRODUCTION,0.026845637583892617,"several such steps are performed between successive communication rounds. 2) Communication
28"
INTRODUCTION,0.027804410354745925,"Compression (CC), in which compressed information is sent instead of full-dimensional vectors.
29"
INTRODUCTION,0.028763183125599234,"We review the literature of LT and CC in Section 1.2.
30"
INTRODUCTION,0.029721955896452542,"We propose a new randomized algorithm named LoCoDL, which features LT and unbiased CC
31"
INTRODUCTION,0.030680728667305847,"for communication-efficient FL and distributed optimization. It is variance-reduced (Hanzely &
32"
INTRODUCTION,0.031639501438159155,"Richtárik, 2019; Gorbunov et al., 2020a; Gower et al., 2020), so that it converges to an exact solution.
33"
INTRODUCTION,0.032598274209012464,"It provably benefits from the two mechanisms of LT and CC: the communication complexity is doubly
34"
INTRODUCTION,0.03355704697986577,"accelerated, with a better dependency on the condition number of the functions and on the dimension
35"
INTRODUCTION,0.03451581975071908,"of the model.
36"
PROBLEM AND MOTIVATION,0.03547459252157239,"1.1
Problem and Motivation
37"
PROBLEM AND MOTIVATION,0.0364333652924257,"We study distributed optimization problems of the form
38"
PROBLEM AND MOTIVATION,0.037392138063279005,"min
x∈Rd
1
n n
X"
PROBLEM AND MOTIVATION,0.038350910834132314,"i=1
fi(x) + g(x),
(1)"
PROBLEM AND MOTIVATION,0.039309683604985615,"where d ≥1 is the model dimension and the functions fi : Rd →R and g : Rd →R are smooth,
39"
PROBLEM AND MOTIVATION,0.040268456375838924,"so their gradients will be called. We consider the server-client model in which n ≥1 clients
40"
PROBLEM AND MOTIVATION,0.04122722914669223,"do computations in parallel and communicate back and forth with a server. The private function
41"
PROBLEM AND MOTIVATION,0.04218600191754554,"fi is owned by and stored on client i ∈[n] := {1, . . . , n}. Problem (1) models empirical risk
42"
PROBLEM AND MOTIVATION,0.04314477468839885,"minimization, of utmost importance in machine learning (Sra et al., 2011; Shalev-Shwartz & Ben-
43"
PROBLEM AND MOTIVATION,0.04410354745925216,"David, 2014). More generally, minimizing a sum of functions appears in virtually all areas of science
44"
PROBLEM AND MOTIVATION,0.045062320230105465,"and engineering. Our goal is to solve Problem (1) in a communication-efficient way, in the general
45"
PROBLEM AND MOTIVATION,0.046021093000958774,"heterogeneous setting in which the functions fi, as well as g, can be arbitrarily different: we do not
46"
PROBLEM AND MOTIVATION,0.04697986577181208,"make any assumption on their similarity whatsoever.
47"
PROBLEM AND MOTIVATION,0.04793863854266539,"We consider in this work the strongly convex setting — an analysis with nonconvex functions would
48"
PROBLEM AND MOTIVATION,0.0488974113135187,"certainly require very different proof techniques, which we currently do not know how to derive. That
49"
PROBLEM AND MOTIVATION,0.04985618408437201,"is, the following holds:
50"
PROBLEM AND MOTIVATION,0.05081495685522531,"Assumption 1.1 (strongly convex functions). The functions fi and g are all L-smooth and µ-strongly
51"
PROBLEM AND MOTIVATION,0.05177372962607862,"convex, for some 0 < µ ≤L.1 Then we denote by x⋆the solution of the strongly convex problem
52"
PROBLEM AND MOTIVATION,0.052732502396931925,"(1), which exists and is unique. We define the condition number κ := L"
PROBLEM AND MOTIVATION,0.053691275167785234,"µ .
53"
PROBLEM AND MOTIVATION,0.05465004793863854,"Problem (1) can be viewed as the minimization of the average of the n functions (fi + g), which can
54"
PROBLEM AND MOTIVATION,0.05560882070949185,"be performed using calls to ∇(fi +g) = ∇fi +∇g. We do not use this straightforward interpretation.
55"
PROBLEM AND MOTIVATION,0.05656759348034516,"Instead, let us illustrate the interest of having the additional function g in (1), using 4 different
56"
PROBLEM AND MOTIVATION,0.05752636625119847,"viewpoints. We stress that we can handle the case g = 0, as discussed in Section 3.1.
57"
PROBLEM AND MOTIVATION,0.058485139022051776,"• Viewpoint 1: regularization. The function g can be a regularizer. For instance, if the functions fi
58"
PROBLEM AND MOTIVATION,0.059443911792905084,"are convex, adding g = µ"
PROBLEM AND MOTIVATION,0.06040268456375839,"2 ∥· ∥2 for a small µ > 0 makes the problem µ-strongly convex.
59"
PROBLEM AND MOTIVATION,0.061361457334611694,"• Viewpoint 2: shared dataset. The function g can model the cost of a common dataset, or a piece
60"
PROBLEM AND MOTIVATION,0.062320230105465,"thereof, that is known to all clients.
61"
PROBLEM AND MOTIVATION,0.06327900287631831,"• Viewpoint 3: server-aided training. The function g can model the cost of a core dataset, known
62"
PROBLEM AND MOTIVATION,0.06423777564717162,"only to the server, which makes calls to ∇g. This setting has been investigated in several works, with
63"
PROBLEM AND MOTIVATION,0.06519654841802493,"the idea that using a small auxiliary dataset representative of the global data distribution, the server
64"
PROBLEM AND MOTIVATION,0.06615532118887824,"can correct for the deviation induced by partial participation (Zhao et al., 2018; Yang et al., 2021,
65"
PROBLEM AND MOTIVATION,0.06711409395973154,"2023). We do not focus on this setting, because we deal with the general heterogeneous setting in
66"
PROBLEM AND MOTIVATION,0.06807286673058485,"which g and the fi are not meant to be similar in any sense, and in our work g is handled by the
67"
PROBLEM AND MOTIVATION,0.06903163950143816,"clients, not by the server.
68"
PROBLEM AND MOTIVATION,0.06999041227229147,"• Viewpoint 4: a new mathematical and algorithmic principle. This is the idea that led to the
69"
PROBLEM AND MOTIVATION,0.07094918504314478,"construction of LoCoDL, and we detail it in Section 2.1.
70"
PROBLEM AND MOTIVATION,0.07190795781399809,"In LoCoDL, the clients make all gradient calls; that is, Client i makes calls to ∇fi and ∇g.
71"
STATE OF THE ART,0.0728667305848514,"1.2
State of the Art
72"
STATE OF THE ART,0.0738255033557047,"We review the latest developments on communication-efficient algorithms for distributed learn-
73"
STATE OF THE ART,0.07478427612655801,"ing, making use of LT, CC, or both. Before that, we note that we should distinguish uplink, or
74"
STATE OF THE ART,0.07574304889741132,"clients-to-server, from downlink, or server-to-clients, communication. Uplink is usually slower than
75"
STATE OF THE ART,0.07670182166826463,"downlink communication, since uploading different messages in parallel to the server is slower than
76"
STATE OF THE ART,0.07766059443911794,"broadcasting the same message to an arbitrary number of clients. This can be due to cache memory
77"
STATE OF THE ART,0.07861936720997123,"and aggregation speed constraints of the server, as well as asymmetry of the service provider’s
78"
STATE OF THE ART,0.07957813998082454,"systems or protocols used on the internet or cell phone network. In this work, we focus on the
79"
STATE OF THE ART,0.08053691275167785,"uplink communication complexity, which is the bottleneck in practice. Indeed, the goal is to
80"
STATE OF THE ART,0.08149568552253116,"1A differentiable function f : Rd →R is said to be L-smooth if ∇f is L-Lipschitz continuous; that is, for
every x ∈Rd and y ∈Rd, ∥∇f(x) −∇f(y)∥≤L∥x −y∥(the norm is the Euclidean norm throughout the
paper). f is said to be µ-strongly convex if f −µ"
STATE OF THE ART,0.08245445829338446,2 ∥· ∥2 is convex.
STATE OF THE ART,0.08341323106423777,"exploit parallelism to obtain better performance when n increases. Precisely, with LoCoDL, the uplink
81"
STATE OF THE ART,0.08437200383509108,"communication complexity decreases from O
 
d√κ log ϵ−1
when n is small to O
 √"
STATE OF THE ART,0.08533077660594439,"d√κ log ϵ−1
82"
STATE OF THE ART,0.0862895493767977,"when n is large, where the condition number κ is defined in Assumption 1.1, see Corollary 3.2. Many
83"
STATE OF THE ART,0.087248322147651,"works have considered bidirectional compression, which consists in compressing the messages sent
84"
STATE OF THE ART,0.08820709491850431,"both ways (Gorbunov et al., 2020b; Philippenko & Dieuleveut, 2020; Liu et al., 2020; Philippenko &
85"
STATE OF THE ART,0.08916586768935762,"Dieuleveut, 2021; Condat & Richtárik, 2022; Gruntkowska et al., 2023; Tyurin & Richtárik, 2023b)
86"
STATE OF THE ART,0.09012464046021093,"but to the best of our knowledge, this has no impact on the downlink complexity, which cannot be
87"
STATE OF THE ART,0.09108341323106424,"reduced further than O
 
d√κ log ϵ−1
, just because there is no parallelism to exploit in this direction.
88"
STATE OF THE ART,0.09204218600191755,"Thus, we focus our analysis on theoretical and algorithmic techniques to reduce the uplink commu-
89"
STATE OF THE ART,0.09300095877277086,"nication complexity, which we call communication complexity in short, and we ignore downlink
90"
STATE OF THE ART,0.09395973154362416,"communication.
91"
STATE OF THE ART,0.09491850431447747,"Communication Compression (CC) consists in applying some lossy scheme that compresses vectors
92"
STATE OF THE ART,0.09587727708533078,"into messages of small bit size, which are communicated. For instance, the well-known rand-k
93"
STATE OF THE ART,0.09683604985618409,"compressor selects k coordinates of the vector uniformly at random, for some k ∈[d] := {1, . . . , d}.
94"
STATE OF THE ART,0.0977948226270374,"k can be as small as 1, in which case the compression factor is d, which can be huge. Some
95"
STATE OF THE ART,0.0987535953978907,"compressors, such as rand-k, are unbiased, whereas others are biased; we refer to Beznosikov et al.
96"
STATE OF THE ART,0.09971236816874401,"(2020); Albasyoni et al. (2020); Horváth et al. (2022); Condat et al. (2022b) for several examples and
97"
STATE OF THE ART,0.10067114093959731,"a discussion of their properties. The introduction of DIANA by Mishchenko et al. (2019) was a major
98"
STATE OF THE ART,0.10162991371045062,"milestone, as this algorithm converges linearly with the large class of unbiased compressors defined
99"
STATE OF THE ART,0.10258868648130393,"in Section 1.3 and also considered in LoCoDL. The communication complexity O
 
dκ log ϵ−1
of
100"
STATE OF THE ART,0.10354745925215723,"the basic Gradient Descent (GD) algorithm is reduced with DIANA to O
 
(κ + d) log ϵ−1
when n
101"
STATE OF THE ART,0.10450623202301054,"is large, see Table 2. DIANA was later extended in several ways (Horváth et al., 2022; Gorbunov
102"
STATE OF THE ART,0.10546500479386385,"et al., 2020a; Condat & Richtárik, 2022). An accelerated version of DIANA called ADIANA based
103"
STATE OF THE ART,0.10642377756471716,"on Nesterov Accelerated GD has been proposed (Li et al., 2020b) and further analyzed in He et al.
104"
STATE OF THE ART,0.10738255033557047,"(2023); it has the state-of-the-art theoretical complexity.
105"
STATE OF THE ART,0.10834132310642378,"Algorithms converging linearly with biased compressors have also been proposed, such as EF21
106"
STATE OF THE ART,0.10930009587727708,"(Richtárik et al., 2021; Fatkhullin et al., 2021; Condat et al., 2022b), but the acceleration potential is
107"
STATE OF THE ART,0.11025886864813039,"less understood than with unbiased compressors. Algorithms with CC such as MARINA (Gorbunov
108"
STATE OF THE ART,0.1112176414189837,"et al., 2021) and DASHA (Tyurin & Richtárik, 2023a) have been proposed for nonconvex optimization,
109"
STATE OF THE ART,0.11217641418983701,"but their analysis requires a different approach and there is a gap in the achievable performance: their
110"
STATE OF THE ART,0.11313518696069032,"complexity depends on ωκ
√n instead of ωκ"
STATE OF THE ART,0.11409395973154363,"n with DIANA, where ω characterizes the compression error
111"
STATE OF THE ART,0.11505273250239693,"variance, see (2). Therefore, we focus on the convex setting and leave the nonconvex study for future
112"
STATE OF THE ART,0.11601150527325024,"work.
113"
STATE OF THE ART,0.11697027804410355,"Local Training (LT) is a simple but remarkably efficient idea: the clients perform multiple Gradient
114"
STATE OF THE ART,0.11792905081495686,"Descent (GD) steps, instead of only one, between successive communication rounds. The intuition
115"
STATE OF THE ART,0.11888782358581017,"behind is that this leads to the communication of richer information, so that the number of com-
116"
STATE OF THE ART,0.11984659635666348,"munication rounds to reach a given accuracy is reduced. We refer to Mishchenko et al. (2022) for
117"
STATE OF THE ART,0.12080536912751678,"a comprehensive review of LT-based algorithms, which include the popular FedAvg and Scaffold
118"
STATE OF THE ART,0.12176414189837009,"algorithms of McMahan et al. (2017) and Karimireddy et al. (2020), respectively. Mishchenko et al.
119"
STATE OF THE ART,0.12272291466922339,"(2022) made a breakthrough by proposing Scaffnew, the first LT-based variance-reduced algorithm
120"
STATE OF THE ART,0.1236816874400767,"that not only converges linearly to the exact solution in the strongly convex setting, but does so with
121"
STATE OF THE ART,0.12464046021093,"accelerated communication complexity O(d√κ log ϵ−1). In Scaffnew, communication can occur
122"
STATE OF THE ART,0.12559923298178333,"randomly after every iteration, but occurs only with a small probability p. Thus, there are in average
123"
STATE OF THE ART,0.12655800575263662,"p−1 local steps between successive communication rounds. The optimal dependency on √κ (Scaman
124"
STATE OF THE ART,0.12751677852348994,"et al., 2019) is obtained with p = 1/√κ. LoCoDL has the same probabilistic LT mechanism as
125"
STATE OF THE ART,0.12847555129434324,"Scaffnew but does not revert to it when compression is disabled, because of the additional function g
126"
STATE OF THE ART,0.12943432406519656,"and tracking variables y and v. A different approach to LT was developed by Sadiev et al. (2022a)
127"
STATE OF THE ART,0.13039309683604985,"with the APDA-Inexact algorithm, and generalized to handle partial participation by Grudzie´n et al.
128"
STATE OF THE ART,0.13135186960690318,"(2023) with the 5GCS algorithm: in both algorithms, the local GD steps form an inner loop in order
129"
STATE OF THE ART,0.13231064237775647,"to compute a proximity operator inexactly.
130"
STATE OF THE ART,0.1332694151486098,"Combining LT and CC while retaining their benefits is very challenging. In our strongly convex and
131"
STATE OF THE ART,0.1342281879194631,"heterogeneous setting, the methods Qsparse-local-SGD (Basu et al., 2020) and FedPAQ (Reisizadeh
132"
STATE OF THE ART,0.13518696069031638,"et al., 2020) do not converge linearly. FedCOMGATE features LT + CC and converges linearly
133"
STATE OF THE ART,0.1361457334611697,"(Haddadpour et al., 2021), but its complexity O(dκ log ϵ−1) does not show any acceleration. We can
134"
STATE OF THE ART,0.137104506232023,"mention that random reshuffling, a technique that can be seen as a type of LT, has been combined with
135"
STATE OF THE ART,0.13806327900287632,"CC in Sadiev et al. (2022b); Malinovsky & Richtárik (2022). Recently, Condat et al. (2022a) managed
136"
STATE OF THE ART,0.13902205177372962,"to design a specific compression technique compatible with the LT mechanism of Scaffnew, leading
137"
STATE OF THE ART,0.13998082454458294,"to CompressedScaffnew, the first LT + CC algorithm exhibiting a doubly-accelerated complexity,
138"
STATE OF THE ART,0.14093959731543623,"namely O
  √"
STATE OF THE ART,0.14189837008628955,"d√κ+ d√κ
√n +d

log ϵ−1
, as reported in Table 2. However, CompressedScaffnew uses
139"
STATE OF THE ART,0.14285714285714285,"a specific linear compression scheme that requires shared randomness; that is, all clients have to agree
140"
STATE OF THE ART,0.14381591562799617,"on a random permutation of the columns of the global compression pattern. No other compressor can
141"
STATE OF THE ART,0.14477468839884947,"be used, which notably rules out any type of quantization.
142"
A GENERAL CLASS OF UNBIASED RANDOM COMPRESSORS,0.1457334611697028,"1.3
A General Class of Unbiased Random Compressors
143"
A GENERAL CLASS OF UNBIASED RANDOM COMPRESSORS,0.14669223394055608,"For every ω ≥0, we define the U(ω) as the set of random compression operators C : Rd →Rd that
144"
A GENERAL CLASS OF UNBIASED RANDOM COMPRESSORS,0.1476510067114094,"are unbiased, i.e. E[C(x)] = x, and satisfy, for every x ∈Rd,
145"
A GENERAL CLASS OF UNBIASED RANDOM COMPRESSORS,0.1486097794822627,"E
h
∥C(x) −x∥2i
≤ω ∥x∥2 .
(2)"
A GENERAL CLASS OF UNBIASED RANDOM COMPRESSORS,0.14956855225311602,"In addition, given a collection (Ci)n
i=1 of compression operators in U(ω) for some ω ≥0, in order
146"
A GENERAL CLASS OF UNBIASED RANDOM COMPRESSORS,0.15052732502396932,"to characterize their joint variance, we introduce the constant ωav ≥0 such that, for every xi ∈Rd,
147"
A GENERAL CLASS OF UNBIASED RANDOM COMPRESSORS,0.15148609779482264,"i ∈[n], we have
148 E  "
N,0.15244487056567593,"1
n n
X i=1"
N,0.15340364333652926," 
Ci(xi) −xi
 2 ≤ωav n n
X"
N,0.15436241610738255,"i=1
∥xi∥2 .
(3)"
N,0.15532118887823587,"The inequality (3) is not an additional assumption: it is satisfied with ωav = ω by convexity of the
149"
N,0.15627996164908917,"squared norm. But the convergence rate will depend on ωav, which is typically much smaller than ω.
150"
N,0.15723873441994246,"In particular, if the compressors Ci are mutually independent, the variance of their sum is the sum of
151"
N,0.15819750719079578,"their variances, and (3) is satisfied with ωav = ω"
N,0.15915627996164908,"n.
152"
CHALLENGE AND CONTRIBUTIONS,0.1601150527325024,"1.4
Challenge and Contributions
153"
CHALLENGE AND CONTRIBUTIONS,0.1610738255033557,"This work addresses the following question: Can we combine LT and CC with any compressors in
154"
CHALLENGE AND CONTRIBUTIONS,0.16203259827420902,"the generic class U(ω) defined in the previous section, and fully benefit from both techniques by
155"
CHALLENGE AND CONTRIBUTIONS,0.1629913710450623,"obtaining a doubly-accelerated communication complexity?
156"
CHALLENGE AND CONTRIBUTIONS,0.16395014381591563,"We answer this question in the affirmative. LoCoDL has the same probabilistic LT mechanism as
157"
CHALLENGE AND CONTRIBUTIONS,0.16490891658676893,"Scaffnew and features CC with compressors in U(ω) with arbitrarily large ω ≥0, with proved linear
158"
CHALLENGE AND CONTRIBUTIONS,0.16586768935762225,"convergence under Assumption 1.1, without further requirements. By choosing the communication
159"
CHALLENGE AND CONTRIBUTIONS,0.16682646212847554,"probability and the variance ω appropriately, double acceleration is obtained. Thus, LoCoDL achieves
160"
CHALLENGE AND CONTRIBUTIONS,0.16778523489932887,"the same theoretical complexity as CompressedScaffnew, but allows for a large class of compressors
161"
CHALLENGE AND CONTRIBUTIONS,0.16874400767018216,"instead of the cumbersome permutation-based compressor of the latter. In particular, with compressors
162"
CHALLENGE AND CONTRIBUTIONS,0.16970278044103548,"performing sparsification and quantization, LoCoDL outperforms existing algorithms, as we show by
163"
CHALLENGE AND CONTRIBUTIONS,0.17066155321188878,"experiments in Section 4. This is remarkable, since ADIANA, based on Nesterov acceleration and
164"
CHALLENGE AND CONTRIBUTIONS,0.1716203259827421,"not LT, has an even better theoretical complexity when n is larger than d, see Table 2, but this is not
165"
CHALLENGE AND CONTRIBUTIONS,0.1725790987535954,"reflected in practice: ADIANA is clearly behind LoCoDL in our experiments. Thus, LoCoDL sets new
166"
CHALLENGE AND CONTRIBUTIONS,0.17353787152444872,"standards in terms of communication efficiency.
167"
PROPOSED ALGORITHM LOCODL,0.174496644295302,"2
Proposed Algorithm LoCoDL
168"
PROPOSED ALGORITHM LOCODL,0.17545541706615533,"2.1
Principle: Double Lifting of the Problem to a Consensus Problem
169"
PROPOSED ALGORITHM LOCODL,0.17641418983700863,"In LoCoDL, every client stores and updates two local model estimates. They will all converge to the
170"
PROPOSED ALGORITHM LOCODL,0.17737296260786195,"same solution x⋆of (1). This construction comes from two ideas.
171"
PROPOSED ALGORITHM LOCODL,0.17833173537871524,"Local steps with local models. In algorithms making use of LT, such as FedAvg, Scaffold and
172"
PROPOSED ALGORITHM LOCODL,0.17929050814956854,"Scaffnew, the clients store and update local model estimates xi. When communication occurs, an
173"
PROPOSED ALGORITHM LOCODL,0.18024928092042186,"estimate of their average is formed by the server and broadcast to all clients. They all resume their
174"
PROPOSED ALGORITHM LOCODL,0.18120805369127516,"computations with this new model estimate.
175"
PROPOSED ALGORITHM LOCODL,0.18216682646212848,"Compressing the difference between two estimates. To implement CC, a powerful idea is to
176"
PROPOSED ALGORITHM LOCODL,0.18312559923298177,"compress not the vectors themselves, but difference vectors that converge to zero. This way, the
177"
PROPOSED ALGORITHM LOCODL,0.1840843720038351,"algorithm is variance-reduced; that is, the compression error vanishes at convergence. The technique
178"
PROPOSED ALGORITHM LOCODL,0.1850431447746884,"of compressing the difference between a gradient vector and a control variate is at the core of
179"
PROPOSED ALGORITHM LOCODL,0.1860019175455417,"Table 1: Communication complexity in number of communication rounds to reach ϵ-accuracy for
linearly-converging algorithms allowing for CC with independent compressors in U(ω) for any ω ≥0.
Since the compressors are independent, ωav = ω"
PROPOSED ALGORITHM LOCODL,0.186960690316395,"n. We provide the leading asymptotic factor and
ignore log factors such as log ϵ−1. The state of the art is highlighted in green."
PROPOSED ALGORITHM LOCODL,0.18791946308724833,"Algorithm
Com. complexity in # rounds
case ω = O(n)
case ω = Θ(n)
DIANA
(1 + ω"
PROPOSED ALGORITHM LOCODL,0.18887823585810162,"n)κ + ω
κ + ω
κ + ω
EF21
(1 + ω)κ
(1 + ω)κ
(1 + ω)κ"
GCS-CC,0.18983700862895495,5GCS-CC
GCS-CC,0.19079578139980824,"
1+√ω+ ω
√n
√κ + ω
(1+√ω)√κ + ω
(1+√ω)√κ + ω"
GCS-CC,0.19175455417066156,"ADIANA1

1+ ω3/4"
GCS-CC,0.19271332694151486,"n1/4 + ω
√n
√κ + ω

1+ ω3/4"
GCS-CC,0.19367209971236818,"n1/4
√κ + ω
(1+√ω)√κ + ω"
GCS-CC,0.19463087248322147,"ADIANA2

1 +
ω
√n
√κ + ω"
GCS-CC,0.1955896452540748,"
1 +
ω
√n
√κ + ω
(1+√ω)√κ + ω"
GCS-CC,0.1965484180249281,"lower bound2

1 +
ω
√n
√κ + ω"
GCS-CC,0.1975071907957814,"
1 +
ω
√n
√κ + ω
(1+√ω)√κ + ω"
GCS-CC,0.1984659635666347,LoCoDL
GCS-CC,0.19942473633748803,"
1+√ω+ ω
√n
√κ + ω(1+ ω"
GCS-CC,0.20038350910834132,"n)
(1+√ω)√κ + ω
(1+√ω)√κ + ω"
GCS-CC,0.20134228187919462,"1This is the complexity derived in the original paper Li et al. (2020b).
2This is the complexity derived by a refined analysis in the preprint He et al. (2023), where a matching lower
bound is also derived."
GCS-CC,0.20230105465004794,"Table 2: (Uplink) communication complexity in number of reals to reach ϵ-accuracy for linearly-
converging algorithms allowing for CC, with an optimal choice of unbiased compressors. We provide
the leading asymptotic factor and ignore log factors such as log ϵ−1. The state of the art is highlighted
in green."
GCS-CC,0.20325982742090123,"Algorithm
complexity in # reals case n=O(d)
DIANA
(1 + d"
GCS-CC,0.20421860019175456,"n)κ + d
d
nκ + d
EF21
dκ
dκ"
GCS-CC,0.20517737296260785,5GCS-CC √
GCS-CC,0.20613614573346117,"d+ d
√n
√κ + d
d
√n
√κ + d"
GCS-CC,0.20709491850431447,ADIANA
GCS-CC,0.2080536912751678,"
1 +
d
√n
√κ + d
d
√n
√κ + d"
GCS-CC,0.20901246404602108,CompressedScaffnew √
GCS-CC,0.2099712368168744,"d+ d
√n
√κ + d
d
√n
√κ + d"
GCS-CC,0.2109300095877277,"FedCOMGATE
dκ
dκ"
GCS-CC,0.21188878235858102,LoCoDL √
GCS-CC,0.21284755512943432,"d+ d
√n
√κ + d
d
√n
√κ + d"
GCS-CC,0.21380632790028764,"algorithms such as DIANA and EF21. Here, we want to compress differences between model
180"
GCS-CC,0.21476510067114093,"estimates, not gradient estimates. That is, we want Client i to compress the difference between xi and
181"
GCS-CC,0.21572387344199426,"another model estimate that converges to the solution x⋆as well. We see the need of an additional
182"
GCS-CC,0.21668264621284755,"model estimate that plays the role of an anchor for compression. This is the variable y common to all
183"
GCS-CC,0.21764141898370087,"clients in LoCoDL, which compress xi −y and send these compressed differences to the server.
184"
GCS-CC,0.21860019175455417,"Combining the two ideas. Accordingly, an equivalent reformulation of (1) is the consensus problem
185"
GCS-CC,0.2195589645254075,"with n + 1 variables
186"
GCS-CC,0.22051773729626079,"min
x1,...,xn,y
1
n n
X"
GCS-CC,0.2214765100671141,"i=1
fi(xi) + g(y) s.t. x1 = · · · = xn = y."
GCS-CC,0.2224352828379674,"The primal–dual optimality conditions are x1 = · · · = xn = y, 0 = ∇fi(xi) −ui ∀i ∈[n],
187"
GCS-CC,0.2233940556088207,"0 = ∇g(y) −v, and 0 = u1 + · · · + un + nv (dual feasibility), for some dual variables u1, . . . , un, v
188"
GCS-CC,0.22435282837967402,"introduced in LoCoDL, that always satisfy the dual feasibility condition.
189"
DESCRIPTION OF LOCODL,0.2253116011505273,"2.2
Description of LoCoDL
190"
DESCRIPTION OF LOCODL,0.22627037392138064,"LoCoDL is a randomized primal–dual algorithm, shown as Algorithm 1. At every iteration, for every
191"
DESCRIPTION OF LOCODL,0.22722914669223393,"i ∈[n] in parallel, Client i first constructs a prediction ˆxt
i of its updated local model estimate, using
192"
DESCRIPTION OF LOCODL,0.22818791946308725,"a GD step with respect to fi corrected by the dual variable ut
i. It also constructs a prediction ˆyt of
193"
DESCRIPTION OF LOCODL,0.22914669223394055,"the updated model estimate, using a GD step with respect to g corrected by the dual variable vt.
194"
DESCRIPTION OF LOCODL,0.23010546500479387,Algorithm 1 LoCoDL
DESCRIPTION OF LOCODL,0.23106423777564716,"1: input: stepsizes γ > 0, χ > 0, ρ > 0; probability p ∈(0, 1]; variance factor ω ≥0; local initial
estimates x0
1, . . . , x0
n ∈Rd, initial estimate y0 ∈Rd, initial control variates u0
1, . . . , u0
n ∈Rd"
DESCRIPTION OF LOCODL,0.23202301054650049,and v ∈Rd such that 1
DESCRIPTION OF LOCODL,0.23298178331735378,"n
Pn
i=1 u0
i + v0 = 0.
2: for t = 0, 1, . . . do
3:
for i = 1, . . . , n, at clients in parallel, do
4:
ˆxt
i := xt
i −γ∇fi(xt
i) + γut
i
5:
ˆyt := yt −γ∇g(yt) + γvt // the clients store and update identical copies of yt, vt, ˆyt"
DESCRIPTION OF LOCODL,0.2339405560882071,"6:
flip a coin θt ∈{0, 1} with Prob(θt = 1) = p
7:
if θt = 1 then
8:
dt
i := Ct
i
 
ˆxt
i −ˆyt"
DESCRIPTION OF LOCODL,0.2348993288590604,"9:
send dt
i to the server
10:
at server: aggregate ¯dt :=
1
2n
Pn
j=1 dt
j and broadcast ¯dt to all clients"
DESCRIPTION OF LOCODL,0.23585810162991372,"11:
xt+1
i
:= (1 −ρ)ˆxt
i + ρ(ˆyt + ¯dt)
12:
ut+1
i
:= ut
i +
pχ
γ(1+2ω)
  ¯dt −dt
i
"
DESCRIPTION OF LOCODL,0.236816874400767,"13:
yt+1 := ˆyt + ρ ¯dt"
DESCRIPTION OF LOCODL,0.23777564717162034,"14:
vt+1 := vt +
pχ
γ(1+2ω) ¯dt"
DESCRIPTION OF LOCODL,0.23873441994247363,"15:
else
16:
xt+1
i
:= ˆxt
i, yt+1 = ˆyt, ut+1
i
:= ut
i, vt+1 := vt"
DESCRIPTION OF LOCODL,0.23969319271332695,"17:
end if
18:
end for
19: end for"
DESCRIPTION OF LOCODL,0.24065196548418025,"Since g is known by all clients, they all maintain and update identical copies of the variables y and
195"
DESCRIPTION OF LOCODL,0.24161073825503357,"v. If there is no communication, which is the case with probability 1 −p, xi and y are updated
196"
DESCRIPTION OF LOCODL,0.24256951102588686,"with these predicted estimates, and the dual variables ui and v are unchanged. If communication
197"
DESCRIPTION OF LOCODL,0.24352828379674019,"occurs, which is the case with probability p, the clients compress the differences ˆxt
i −ˆyt and send
198"
DESCRIPTION OF LOCODL,0.24448705656759348,"these compressed vectors to the server, which forms ¯dt equal to one half of their average. Then the
199"
DESCRIPTION OF LOCODL,0.24544582933844677,"variables xi are updated using a convex combination of the local predicted estimates ˆxt
i and the global
200"
DESCRIPTION OF LOCODL,0.2464046021093001,"but noisy estimate ˆyt + ¯dt. y is updated similarly. Finally, the dual variables are updated using the
201"
DESCRIPTION OF LOCODL,0.2473633748801534,"compressed differences minus their weighted average, so that the dual feasibility condition remains
202"
DESCRIPTION OF LOCODL,0.2483221476510067,"satisfied. The model estimates xt
i, ˆxt
i, yt, ˆyt all converge to x⋆, so that their differences, as well as
203"
DESCRIPTION OF LOCODL,0.24928092042186,"the compressed differences as a consequence of (2), converge to zero. This is the key property that
204"
DESCRIPTION OF LOCODL,0.25023969319271333,"makes the algorithm variance-reduced. We consider the following assumption.
205"
DESCRIPTION OF LOCODL,0.25119846596356665,"Assumption 2.1 (class of compressors). In LoCoDL the compressors Ct
i are all in U(ω) for some
206"
DESCRIPTION OF LOCODL,0.2521572387344199,"ω ≥0. Moreover, for every i ∈[n], i′ ∈[n], t ≥0, t′ ≥0, Ct
i and Ct′
i′ are independent if t ̸= t′ (Ct
i
207"
DESCRIPTION OF LOCODL,0.25311601150527324,"and Ct
i′ at the same iteration t need not be independent). We define ωav ≥0 such that for every t ≥0,
208"
DESCRIPTION OF LOCODL,0.25407478427612656,"the collection (Ct
i)n
i=1 satisfies (3).
209"
DESCRIPTION OF LOCODL,0.2550335570469799,"Remark 2.2 (partial participation). LoCoDL allows for a form of partial participation if we set ρ = 1.
210"
DESCRIPTION OF LOCODL,0.25599232981783315,"Indeed, in that case, at steps 11 and 13 of the algorithm, all local variables xi as well as the common
211"
DESCRIPTION OF LOCODL,0.2569511025886865,"variable y are overwritten by the same up-to-date model ˆyt + ¯dt. So, it does not matter that for
212"
DESCRIPTION OF LOCODL,0.2579098753595398,"a non-participating client i with dt
i = 0, the ˆxt′
i were not computed for the t′ ≤t since its last
213"
DESCRIPTION OF LOCODL,0.2588686481303931,"participation, as they are not used in the process. However, a non-participating client should still
214"
DESCRIPTION OF LOCODL,0.2598274209012464,"update its local copy of y at every iteration. This can be done when ∇g is much cheaper to compute
215"
DESCRIPTION OF LOCODL,0.2607861936720997,"that ∇fi, as is the case with g = µ"
DESCRIPTION OF LOCODL,0.26174496644295303,"2 ∥· ∥2. A non-participating client can be completely idle for a
216"
DESCRIPTION OF LOCODL,0.26270373921380635,"certain period of time, but when it resumes participating, it should receive the last estimates of x, y
217"
DESCRIPTION OF LOCODL,0.2636625119846596,"and v from the server as it lost synchronization.
218"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.26462128475551294,"3
Convergence and Complexity of LoCoDL
219"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.26558005752636626,"Theorem 3.1 (linear convergence of LoCoDL). Suppose that Assumptions 1.1 and 2.1 hold. In
220"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2665388302972196,"LoCoDL, suppose that 0 < γ < 2"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.26749760306807285,"L, 2ρ −ρ2(1 + ωav) −χ ≥0. For every t ≥0, define the Lyapunov
221"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2684563758389262,"function
222"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2694151486097795,"Ψt := 1 γ n
X i=1"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.27037392138063276,"xt
i −x⋆2 + n
yt −x⋆2
!"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2713326941514861,"+ γ(1 + 2ω) p2χ n
X i=1"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2722914669223394,"ut
i −u⋆
i
2 + n
vt −v⋆2
! ,"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.27325023969319273,"(4)
where v⋆:= ∇g(x⋆) and u⋆
i := ∇fi(x⋆). Then LoCoDL converges linearly: for every t ≥0,
223"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.274209012464046,"E

Ψt
≤τ tΨ0,
where
τ := max

(1 −γµ)2, (1 −γL)2, 1 −
p2χ
1 + 2ω"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2751677852348993,"
< 1.
(5)"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.27612655800575264,"In addition, for every i ∈[n], (xt
i)t∈N and (yt)t∈N converge to x⋆, (ut
i)t∈N converges to u⋆
i , and
224"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.27708533077660596,"(vt)t∈N converges to v⋆, almost surely.
225"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.27804410354745923,"We place ourselves in the conditions of Theorem 3.1. We observe that in (5), the larger χ, the better,
226"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.27900287631831255,"so given ρ we should set χ = 2ρ −ρ2(1 + ωav). Then, choosing ρ to maximize χ yields
227"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2799616490891659,"χ = ρ =
1
1 + ωav
.
(6)"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2809204218600192,We now study the complexity of LoCoDL with χ and ρ chosen as in (6) and γ = Θ( 1
CONVERGENCE AND COMPLEXITY OF LOCODL,0.28187919463087246,"L). We remark
228"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2828379674017258,"that LoCoDL has the same rate τ ♯:= max(1 −γµ, γL −1)2 as mere distributed gradient descent, as
229"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2837967401725791,"long as p−1, ω and ωav are small enough to have 1−
p2χ
1+2ω ≤τ ♯. This is remarkable: communicating
230"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.28475551294343243,"with a low frequency and compressed vectors does not harm convergence at all, until some threshold.
231"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2857142857142857,"The iteration complexity of LoCoDL to reach ϵ-accuracy, i.e. E[Ψt] ≤ϵΨ0, is
232"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.286673058485139,"O

κ + (1 + ωav)(1 + ω) p2"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.28763183125599234,"
log ϵ−1

.
(7)"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.28859060402684567,"By choosing
233"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.28954937679769893,p = min r
CONVERGENCE AND COMPLEXITY OF LOCODL,0.29050814956855225,"(1 + ωav)(1 + ω) κ
, 1 ! ,
(8)"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2914669223394056,"the iteration complexity becomes O
 
κ + ω(1 + ωav)

log ϵ−1
and the communication complexity
234"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.29242569511025884,"in number of communication rounds is p times the iteration complexity, that is
235 O
p"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.29338446788111217,"κ(1 + ωav)(1 + ω) + ω(1 + ωav)

log ϵ−1
."
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2943432406519655,"If the compressors are mutually independent, ωav = ω"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2953020134228188,"n and the communication complexity can be
236"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2962607861936721,"equivalently written as
237"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2972195589645254,"O

1 + √ω + ω
√n"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.2981783317353787," √κ + ω

1 + ω n"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.29913710450623204,"
log ϵ−1

,"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3000958772770853,"as shown in Table 1.
238"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.30105465004793863,"Let us consider the example of independent rand-k compressors, for some k ∈[d]. We have
239 ω = d"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.30201342281879195,"k −1. Therefore, the communication complexity in numbers of reals is k times the complexity
240"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3029721955896453,"in number of rounds; that is, O
√"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.30393096836049854,"kd +
d
√n
 √κ + d
 
1 +
d
kn

log ϵ−1
. We can now choose
241"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.30488974113135187,k to minimize this complexity: with k = ⌈d
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3058485139022052,"n⌉, it becomes O
√"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3068072866730585,"d +
d
√n
 √κ + d

log ϵ−1
, as
242"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3077660594439118,"shown in Table 2. Let us state this result:
243"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3087248322147651,"Corollary 3.2. In the conditions of Theorem 3.1, suppose in addition that the compressors Ct
i are
244"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3096836049856184,independent rand-k compressors with k = ⌈d
CONVERGENCE AND COMPLEXITY OF LOCODL,0.31064237775647174,n⌉. Suppose that γ = Θ( 1
CONVERGENCE AND COMPLEXITY OF LOCODL,0.311601150527325,"L), χ = ρ =
n
n−1+d/k, and
245"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.31255992329817833,p = min r
CONVERGENCE AND COMPLEXITY OF LOCODL,0.31351869606903165,dk(n −1) + d2
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3144774688398849,"nk2κ
, 1 ! .
(9)"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.31543624161073824,"Then the uplink communication complexity in number of reals of LoCoDL is
246 O
√"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.31639501438159157,"d√κ + d√κ
√n + d

log ϵ−1

.
(10)"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3173537871524449,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Number of Communicated Bits
1e6 10
5 10
4 10
3 10
2 10
1 100"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.31831255992329816,"f(x)
f*"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3192713326941515,"LoCoDL: Rand-2
LoCoDL: Natural
LoCoDL: Rand-2 + Natural
LoCoDL: l1-select
ADIANA: Rand-30
ADIANA: Natural
ADIANA: Rand-30 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3202301054650048,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Number of Communicated Bits
1e6 10
5 10
4 10
3 10
2 10
1 100"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3211888782358581,"f(x)
f*"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3221476510067114,"LoCoDL: Rand-1
LoCoDL: Natural
LoCoDL: Rand-1 + Natural
LoCoDL: l1-select
ADIANA: Rand-30
ADIANA: Natural
ADIANA: Rand-30 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3231064237775647,"(a) n = 87
(b) n = 288"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.32406519654841803,"Figure 1: Comparison of several algorithms with several compressors on logistic regression with the
‘a5a’ dataset from the LibSVM, which has d = 122 and 6,414 data points. We chose different values
of n to illustrate the two regimes n < d and n > d, as discussed at the end of Section 3."
CONVERGENCE AND COMPLEXITY OF LOCODL,0.32502396931927136,"This is the same complexity as CompressedScaffnew (Condat et al., 2022a). However, it is obtained
247"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3259827420901246,"with simple independent compressors, which is much more practical than the permutation-based
248"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.32694151486097794,"compressors with shared randomness of CompressedScaffnew. Moreover, this complexity can be
249"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.32790028763183127,"obtained with other types of compressors, and further reduced, when reasoning in number of bits and
250"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3288590604026846,"not only reals, by making use of quantization (Albasyoni et al., 2020), as we illustrate by experiments
251"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.32981783317353786,"in the next section.
252"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3307766059443912,"We can distinguish 2 regimes:
253"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3317353787152445,"1. In the “large d small n” regime, i.e. n = O(d), the communication complexity of LoCoDL in (10)
254"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3326941514860978,"becomes O

d√κ
√n + d

log ϵ−1
. This is the state of the art, as reported in Table 2.
255"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3336529242569511,"2. In the “large n small d” regime, i.e. n = Ω(d), the communication complexity of LoCoDL in (10)
256"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3346116970278044,"becomes O
√"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.33557046979865773,"d√κ + d

log ϵ−1
. If n is even larger with n = Ω(d2), ADIANA achieves the even
257"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.336529242569511,"better complexity O
 
(√κ + d) log ϵ−1
.
258"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3374880153403643,"Yet, in the experiments we ran with different datasets and values of d, n, κ, LoCoDL outperforms the
259"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.33844678811121764,"other algorithms, including ADIANA, in all cases.
260"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.33940556088207097,"3.1
The Case g = 0
261"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.34036433365292423,"We have assumed the presence of a function g in Problem (1), whose gradient is called by all clients.
262"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.34132310642377756,"In this section, we show that we can handle the case where such a function is not available. So, let
263"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3422818791946309,us assume that we want to minimize 1
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3432406519654842,"n
Pn
i=1 fi, with the functions fi satisfying Assumption 1.1.
264"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.34419942473633747,We now define the functions ˜fi := fi −µ
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3451581975071908,4 ∥·∥2 and ˜g := µ
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3461169702780441,"4 ∥·∥2. They are all ˜L-smooth and ˜µ-
265"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.34707574304889743,"strongly convex, with ˜L := L −µ"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3480345158197507,2 and ˜µ := µ
CONVERGENCE AND COMPLEXITY OF LOCODL,0.348993288590604,"2 . Moreover, it is equivalent to minimize 1"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.34995206136145734,"n
Pn
i=1 fi
266 or 1"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.35091083413231067,"n
Pn
i=1 ˜fi + ˜g. We can then apply LoCoDL to the latter problem. At Step 5, we simply have
267"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.35186960690316393,yt −γ∇˜g(yt) = (1 −γµ
CONVERGENCE AND COMPLEXITY OF LOCODL,0.35282837967401726,"2 )yt. The rate in (5) applies with L and µ replaced by ˜L and ˜µ, respectively.
268"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3537871524448706,"Since κ ≤˜κ :=
˜L
˜µ ≤2κ, the asymptotic complexities derived above also apply to this setting. Thus,
269"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.3547459252157239,"the presence of g in Problem (1) is not restrictive at all, as the only property of g that matters is that it
270"
CONVERGENCE AND COMPLEXITY OF LOCODL,0.35570469798657717,"has the same amount of strong convexity as the fis.
271"
EXPERIMENTS,0.3566634707574305,"4
Experiments
272"
EXPERIMENTS,0.3576222435282838,"We evaluate the performance of our proposed method LoCoDL and compare it with several other
273"
EXPERIMENTS,0.3585810162991371,"methods that also allow for CC and converge linearly to x⋆. We also include GradSkip (Maranjyan
274"
EXPERIMENTS,0.3595397890699904,"et al., 2023) and Scaffold (McMahan et al., 2017) in our comparisons. We focus on a regularized
275"
EXPERIMENTS,0.3604985618408437,"logistic regression problem, which has the form (1) with
276"
EXPERIMENTS,0.36145733461169705,"fi(x) = 1 m m
X"
EXPERIMENTS,0.3624161073825503,"s=1
log

1+exp
 
−bi,sa⊤
i,sx

+ µ"
EXPERIMENTS,0.36337488015340363,"2 ∥x∥2
(11)"
EXPERIMENTS,0.36433365292425696,and g = µ
EXPERIMENTS,0.3652924256951103,"2 ∥x∥2, where n is the number of clients, m is the number of data points per client, ai,s ∈Rd
277"
EXPERIMENTS,0.36625119846596355,"and bi,s ∈{−1, +1} are the data samples, and µ is the regularization parameter, set so that κ = 104.
278"
EXPERIMENTS,0.36720997123681687,"For all algorithms other than LoCoDL, for which there is no function g, the functions fi in (11) have
279"
EXPERIMENTS,0.3681687440076702,"a twice higher µ, so that the problem remains the same.
280"
EXPERIMENTS,0.3691275167785235,"We considered several datasets from the LibSVM library (Chang & Lin, 2011) (3-clause BSD license).
281"
EXPERIMENTS,0.3700862895493768,"We show the results with the ‘a5a’ dataset in Figure 1 and with other datasets in the Appendix. We
282"
EXPERIMENTS,0.3710450623202301,"prepared each dataset by first shuffling it, then distributing it equally among the n clients (since m
283"
EXPERIMENTS,0.3720038350910834,"in (11) is an integer, the remaining datapoints were discarded). We used four different compression
284"
EXPERIMENTS,0.37296260786193675,"operators in the class U(ω), for some ω ≥0:
285"
EXPERIMENTS,0.37392138063279,"• rand-k for some k ∈[d], which communicates 32k + k⌈log2(d)⌉bits. Indeed, the k randomly
286"
EXPERIMENTS,0.37488015340364333,"chosen values are sent in the standard 32-bits IEEE floating-point format, and their locations are
287"
EXPERIMENTS,0.37583892617449666,encoded with k⌈log2(d)⌉additional bits. We have ω = d
EXPERIMENTS,0.37679769894535,"k −1.
288"
EXPERIMENTS,0.37775647171620325,"• Natural Compression (Horváth et al., 2022), a form of quantization in which floats are encoded
289"
EXPERIMENTS,0.37871524448705657,into 9 bits instead of 32 bits. We have ω = 1
EXPERIMENTS,0.3796740172579099,"8.
290"
EXPERIMENTS,0.38063279002876316,"• A combination of rand-k and Natural Compression, in which the k chosen values are encoded
291"
EXPERIMENTS,0.3815915627996165,"into 9 bits, which yields a total of 9k + k⌈log2(d)⌉bits. We have ω = 9d"
EXPERIMENTS,0.3825503355704698,"8k −1.
292"
EXPERIMENTS,0.3835091083413231,"• The l1-selection compressor, defined as C(x) = sign(xj)∥x∥1ej, where j is chosen randomly in
293"
EXPERIMENTS,0.3844678811121764,"[d], with the probability of choosing j′ ∈[d] equal to |xj′|/∥x∥1, and ej is the j-th standard unit basis
294"
EXPERIMENTS,0.3854266538830297,"vector in Rd. sign(xj)∥x∥1 is sent as a 32-bits float and the location of j is indicated with ⌈log2(d)⌉,
295"
EXPERIMENTS,0.38638542665388304,"so that this compressor communicates 32 + ⌈log2(d)⌉bits. Like with rand-1, we have ω = d −1.
296"
EXPERIMENTS,0.38734419942473636,"The compressors at different clients are independent, so that ωav = ω"
EXPERIMENTS,0.3883029721955896,"n in (3).
297"
EXPERIMENTS,0.38926174496644295,"We can see that LoCoDL, when combined with rand-k and Natural Compression, converges faster
298"
EXPERIMENTS,0.39022051773729627,"than all other algorithms, with respect to the total number of communicated bits per client. We
299"
EXPERIMENTS,0.3911792905081496,"chose two different numbers n of clients, one with n < d and another one with n > 2d, since
300"
EXPERIMENTS,0.39213806327900286,"the compressor of CompressedScaffnew is different in the two cases n < 2d and n > 2d (Condat
301"
EXPERIMENTS,0.3930968360498562,"et al., 2022a). LoCoDL outperforms CompressedScaffnew in both cases. As expected, all methods
302"
EXPERIMENTS,0.3940556088207095,"exhibit faster convergence with larger n. Remarkably, ADIANA, which has the best theoretical
303"
EXPERIMENTS,0.3950143815915628,"complexity for large n, improves upon DIANA but is not competitive with the LT-based methods
304"
EXPERIMENTS,0.3959731543624161,"CompressedScaffnew, 5GCS-CC, and LoCoDL. This illustrates the power of doubly-accelerated
305"
EXPERIMENTS,0.3969319271332694,"methods based on a successful combination of LT and CC. In this class, our new proposed LoCoDL
306"
EXPERIMENTS,0.39789069990412274,"algorithm shines. For all algorithms, we used the theoretical parameter values given in their available
307"
EXPERIMENTS,0.39884947267497606,"convergence results (Corollary 3.2 for LoCoDL). We tried to tune the parameter values, such as k in
308"
EXPERIMENTS,0.3998082454458293,"rand-k and the (average) number of local steps per round, but this only gave minor improvements.
309"
EXPERIMENTS,0.40076701821668265,"For instance, ADIANA in Figure 1 was a bit faster with the best value of k = 20 than with k = 30.
310"
EXPERIMENTS,0.40172579098753597,"Increasing the learning rate γ led to inconsistent results, with sometimes divergence.
311"
CONCLUSION,0.40268456375838924,"5
Conclusion
312"
CONCLUSION,0.40364333652924256,"We have proposed LoCoDL, which combines a probabilistic Local Training mechanism similar to the
313"
CONCLUSION,0.4046021093000959,"one of Scaffnew and Communication Compression with a large class of unbiased compressors. This
314"
CONCLUSION,0.4055608820709492,"successful combination makes LoCoDL highly communication-efficient, with a doubly accelerated
315"
CONCLUSION,0.40651965484180247,"complexity with respect to the model dimension d and the condition number of the functions.
316"
CONCLUSION,0.4074784276126558,"In practice, LoCoDL outperforms other algorithms, including ADIANA, which has an even better
317"
CONCLUSION,0.4084372003835091,"complexity in theory obtained from Nesterov acceleration and not Local Training. This again
318"
CONCLUSION,0.40939597315436244,"shows the relevance of the popular mechanism of Local Training, which has been widely adopted in
319"
CONCLUSION,0.4103547459252157,"Federated Learning. A venue for future work is to implement bidirectional compression (Liu et al.,
320"
CONCLUSION,0.411313518696069,"2020; Philippenko & Dieuleveut, 2021). We will also investigate extensions of our method with calls
321"
CONCLUSION,0.41227229146692235,"to stochastic gradient estimates, with or without variance reduction, as well as partial participation.
322"
CONCLUSION,0.41323106423777567,"These two features have been proposed for Scaffnew in Malinovsky et al. (2022) and Condat et al.
323"
CONCLUSION,0.41418983700862894,"(2023), but they are challenging to combine with generic compression.
324"
REFERENCES,0.41514860977948226,"References
325"
REFERENCES,0.4161073825503356,"Albasyoni, A., Safaryan, M., Condat, L., and Richtárik, P. Optimal gradient compression for
326"
REFERENCES,0.4170661553211889,"distributed and federated learning. preprint arXiv:2010.03246, 2020.
327"
REFERENCES,0.41802492809204217,"Basu, D., Data, D., Karakus, C., and Diggavi, S. N. Qsparse-Local-SGD: Distributed SGD With Quan-
328"
REFERENCES,0.4189837008628955,"tization, Sparsification, and Local Computations. IEEE Journal on Selected Areas in Information
329"
REFERENCES,0.4199424736337488,"Theory, 1(1):217–226, 2020.
330"
REFERENCES,0.42090124640460214,"Bertsekas, D. P. Convex optimization algorithms. Athena Scientific, Belmont, MA, USA, 2015.
331"
REFERENCES,0.4218600191754554,"Beznosikov, A., Horváth, S., Richtárik, P., and Safaryan, M. On biased compression for distributed
332"
REFERENCES,0.4228187919463087,"learning. preprint arXiv:2002.12410, 2020.
333"
REFERENCES,0.42377756471716205,"Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H. B., Patel, S., Ramage, D., Segal,
334"
REFERENCES,0.4247363374880153,"A., and Seth, K. Practical secure aggregation for privacy-preserving machine learning. In Proc. of
335"
REFERENCES,0.42569511025886864,"the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 1175–1191,
336"
REFERENCES,0.42665388302972196,"2017.
337"
REFERENCES,0.4276126558005753,"Chang, C.-C. and Lin, C.-J.
LIBSVM: A library for support vector machines.
ACM Trans-
338"
REFERENCES,0.42857142857142855,"actions on Intelligent Systems and Technology, 2:27:1–27:27, 2011.
Software available at
339"
REFERENCES,0.42953020134228187,"http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm.
340"
REFERENCES,0.4304889741131352,"Condat, L. and Richtárik, P. MURANA: A generic framework for stochastic variance-reduced
341"
REFERENCES,0.4314477468839885,"optimization. In Proc. of the conference Mathematical and Scientific Machine Learning (MSML),
342"
REFERENCES,0.4324065196548418,"PMLR 190, 2022.
343"
REFERENCES,0.4333652924256951,"Condat, L. and Richtárik, P. RandProx: Primal-dual optimization algorithms with randomized
344"
REFERENCES,0.4343240651965484,"proximal updates. In Proc. of International Conference on Learning Representations (ICLR), 2023.
345"
REFERENCES,0.43528283796740175,"Condat, L., Agarský, I., and Richtárik, P. Provably doubly accelerated federated learning: The first
346"
REFERENCES,0.436241610738255,"theoretically successful combination of local training and compressed communication. preprint
347"
REFERENCES,0.43720038350910834,"arXiv:2210.13277, 2022a.
348"
REFERENCES,0.43815915627996166,"Condat, L., Li, K., and Richtárik, P. EF-BV: A unified theory of error feedback and variance reduction
349"
REFERENCES,0.439117929050815,"mechanisms for biased and unbiased compression in distributed optimization. In Proc. of Conf.
350"
REFERENCES,0.44007670182166825,"Neural Information Processing Systems (NeurIPS), 2022b.
351"
REFERENCES,0.44103547459252157,"Condat, L., Agarský, I., Malinovsky, G., and Richtárik, P. TAMUNA: Doubly accelerated federated
352"
REFERENCES,0.4419942473633749,"learning with local training, compression, and partial participation. preprint arXiv:2302.09832 pre-
353"
REFERENCES,0.4429530201342282,"sented at the Int. Workshop on Federated Learning in the Age of Foundation Models in Conjunction
354"
REFERENCES,0.4439117929050815,"with NeurIPS 2023, 2023.
355"
REFERENCES,0.4448705656759348,"Fatkhullin, I., Sokolov, I., Gorbunov, E., Li, Z., and Richtárik, P. EF21 with bells & whistles: Practical
356"
REFERENCES,0.4458293384467881,"algorithmic extensions of modern error feedback. preprint arXiv:2110.03294, 2021.
357"
REFERENCES,0.4467881112176414,"Gorbunov, E., Hanzely, F., and Richtárik, P. A unified theory of SGD: Variance reduction, sampling,
358"
REFERENCES,0.4477468839884947,"quantization and coordinate descent. In Proc. of 23rd Int. Conf. Artificial Intelligence and Statistics
359"
REFERENCES,0.44870565675934804,"(AISTATS), PMLR 108, 2020a.
360"
REFERENCES,0.44966442953020136,"Gorbunov, E., Kovalev, D., Makarenko, D., and Richtárik, P. Linearly converging error compensated
361"
REFERENCES,0.4506232023010546,"SGD. In Proc. of Conf. Neural Information Processing Systems (NeurIPS), 2020b.
362"
REFERENCES,0.45158197507190795,"Gorbunov, E., Burlachenko, K., Li, Z., and Richtárik, P. MARINA: Faster non-convex distributed
363"
REFERENCES,0.45254074784276127,"learning with compression. In Proc. of 38th Int. Conf. Machine Learning (ICML), pp. 3788–3798,
364"
REFERENCES,0.4534995206136146,"2021.
365"
REFERENCES,0.45445829338446786,"Gower, R. M., Schmidt, M., Bach, F., and Richtárik, P. Variance-reduced methods for machine
366"
REFERENCES,0.4554170661553212,"learning. Proc. of the IEEE, 108(11):1968–1983, November 2020.
367"
REFERENCES,0.4563758389261745,"Grudzie´n, M., Malinovsky, G., and Richtárik, P. Can 5th Generation Local Training Methods Support
368"
REFERENCES,0.4573346116970278,"Client Sampling? Yes! In Proc. of Int. Conf. Artificial Intelligence and Statistics (AISTATS), 2023.
369"
REFERENCES,0.4582933844678811,"Gruntkowska, K., Tyurin, A., and Richtárik, P. EF21-P and friends: Improved theoretical communi-
370"
REFERENCES,0.4592521572387344,"cation complexity for distributed optimization with bidirectional compression. In Proc. of 40th Int.
371"
REFERENCES,0.46021093000958774,"Conf. Machine Learning (ICML), 2023.
372"
REFERENCES,0.46116970278044106,"Haddadpour, F., Kamani, M. M., Mokhtari, A., and Mahdavi, M. Federated learning with compression:
373"
REFERENCES,0.4621284755512943,"Unified analysis and sharp guarantees. In Proc. of Int. Conf. Artificial Intelligence and Statistics
374"
REFERENCES,0.46308724832214765,"(AISTATS), PMLR 130, pp. 2350–2358, 2021.
375"
REFERENCES,0.46404602109300097,"Hanzely, F. and Richtárik, P. One method to rule them all: Variance reduction for data, parameters
376"
REFERENCES,0.4650047938638543,"and many new methods. preprint arXiv:1905.11266, 2019.
377"
REFERENCES,0.46596356663470756,"He, Y., Huang, X., and Yuan, K. Unbiased compression saves communication in distributed optimiza-
378"
REFERENCES,0.4669223394055609,"tion: When and how much? preprint arXiv:2305.16297, 2023.
379"
REFERENCES,0.4678811121764142,"Horváth, S., Ho, C.-Y., Horváth, L., Sahu, A. N., Canini, M., and Richtárik, P. Natural compression
380"
REFERENCES,0.46883988494726747,"for distributed deep learning. In Proc. of the conference Mathematical and Scientific Machine
381"
REFERENCES,0.4697986577181208,"Learning (MSML), PMLR 190, 2022.
382"
REFERENCES,0.4707574304889741,"Horváth, S., Kovalev, D., Mishchenko, K., Stich, S., and Richtárik, P. Stochastic distributed learning
383"
REFERENCES,0.47171620325982744,"with gradient quantization and variance reduction. Optimization Methods and Software, 2022.
384"
REFERENCES,0.4726749760306807,"Kairouz, P. et al. Advances and open problems in federated learning. Foundations and Trends in
385"
REFERENCES,0.473633748801534,"Machine Learning, 14(1–2), 2021.
386"
REFERENCES,0.47459252157238735,"Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S. U., and Suresh, A. T. SCAFFOLD:
387"
REFERENCES,0.47555129434324067,"Stochastic controlled averaging for federated learning. In Proc. of 37th Int. Conf. Machine Learning
388"
REFERENCES,0.47651006711409394,"(ICML), pp. 5132–5143, 2020.
389"
REFERENCES,0.47746883988494726,"Koneˇcný, J., McMahan, H. B., Ramage, D., and Richtárik, P. Federated optimization: distributed
390"
REFERENCES,0.4784276126558006,"machine learning for on-device intelligence. arXiv:1610.02527, 2016a.
391"
REFERENCES,0.4793863854266539,"Koneˇcný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., and Bacon, D. Federated
392"
REFERENCES,0.48034515819750717,"learning: Strategies for improving communication efficiency. In NIPS Private Multi-Party Machine
393"
REFERENCES,0.4813039309683605,"Learning Workshop, 2016b. arXiv:1610.05492.
394"
REFERENCES,0.4822627037392138,"Li, T., Sahu, A. K., Talwalkar, A., and Smith, V. Federated learning: Challenges, methods, and future
395"
REFERENCES,0.48322147651006714,"directions. IEEE Signal Processing Magazine, 3(37):50–60, 2020a.
396"
REFERENCES,0.4841802492809204,"Li, Z., Kovalev, D., Qian, X., and Richtárik, P. Acceleration for compressed gradient descent in
397"
REFERENCES,0.4851390220517737,"distributed and federated optimization. In Proc. of 37th Int. Conf. Machine Learning (ICML),
398"
REFERENCES,0.48609779482262705,"volume PMLR 119, 2020b.
399"
REFERENCES,0.48705656759348037,"Liu, X., Li, Y., Tang, J., and Yan, M. A double residual compression algorithm for efficient distributed
400"
REFERENCES,0.48801534036433364,"learning. In Proc. of Int. Conf. Artificial Intelligence and Statistics (AISTATS), PMLR 108, pp.
401"
REFERENCES,0.48897411313518696,"133–143, 2020.
402"
REFERENCES,0.4899328859060403,"Malinovsky, G. and Richtárik, P. Federated random reshuffling with compression and variance
403"
REFERENCES,0.49089165867689355,"reduction. preprint arXiv:arXiv:2205.03914, 2022.
404"
REFERENCES,0.49185043144774687,"Malinovsky, G., Yi, K., and Richtárik, P. Variance reduced ProxSkip: Algorithm, theory and
405"
REFERENCES,0.4928092042186002,"application to federated learning. In Proc. of Conf. Neural Information Processing Systems
406"
REFERENCES,0.4937679769894535,"(NeurIPS), 2022.
407"
REFERENCES,0.4947267497603068,"Maranjyan, A., Safaryan, M., and Richtárik, P. Gradskip: Communication-accelerated local gradient
408"
REFERENCES,0.4956855225311601,"methods with better computational complexity, 2023.
409"
REFERENCES,0.4966442953020134,"McMahan, H. B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. Communication-efficient
410"
REFERENCES,0.49760306807286675,"learning of deep networks from decentralized data. In Proc. of Int. Conf. Artificial Intelligence and
411"
REFERENCES,0.49856184084372,"Statistics (AISTATS), PMLR 54, 2017.
412"
REFERENCES,0.49952061361457334,"Mishchenko, K., Gorbunov, E., Takáˇc, M., and Richtárik, P. Distributed learning with compressed
413"
REFERENCES,0.5004793863854267,"gradient differences. arXiv:1901.09269, 2019.
414"
REFERENCES,0.50143815915628,"Mishchenko, K., Malinovsky, G., Stich, S., and Richtárik, P. ProxSkip: Yes! Local Gradient Steps
415"
REFERENCES,0.5023969319271333,"Provably Lead to Communication Acceleration! Finally!
In Proc. of the 39th International
416"
REFERENCES,0.5033557046979866,"Conference on Machine Learning (ICML), July 2022.
417"
REFERENCES,0.5043144774688398,"Philippenko, C. and Dieuleveut, A. Artemis: tight convergence guarantees for bidirectional compres-
418"
REFERENCES,0.5052732502396932,"sion in federated learning. preprint arXiv:2006.14591, 2020.
419"
REFERENCES,0.5062320230105465,"Philippenko, C. and Dieuleveut, A. Preserved central model for faster bidirectional compression in
420"
REFERENCES,0.5071907957813998,"distributed settings. In Proc. of Conf. Neural Information Processing Systems (NeurIPS), 2021.
421"
REFERENCES,0.5081495685522531,"Reisizadeh, A., Mokhtari, A., Hassani, H., Jadbabaie, A., and Pedarsani, R.
FedPAQ: A
422"
REFERENCES,0.5091083413231065,"communication-efficient federated learning method with periodic averaging and quantization.
423"
REFERENCES,0.5100671140939598,"In Proc. of Int. Conf. Artificial Intelligence and Statistics (AISTATS), pp. 2021–2031, 2020.
424"
REFERENCES,0.5110258868648131,"Richtárik, P., Sokolov, I., and Fatkhullin, I. EF21: A new, simpler, theoretically better, and practically
425"
REFERENCES,0.5119846596356663,"faster error feedback. In Proc. of 35th Conf. Neural Information Processing Systems (NeurIPS),
426"
REFERENCES,0.5129434324065196,"2021.
427"
REFERENCES,0.513902205177373,"Sadiev, A., Kovalev, D., and Richtárik, P. Communication acceleration of local gradient methods via
428"
REFERENCES,0.5148609779482263,"an accelerated primal-dual algorithm with an inexact prox. In Proc. of Conf. Neural Information
429"
REFERENCES,0.5158197507190796,"Processing Systems (NeurIPS), 2022a.
430"
REFERENCES,0.5167785234899329,"Sadiev, A., Malinovsky, G., Gorbunov, E., Sokolov, I., Khaled, A., Burlachenko, K., and Richtárik, P.
431"
REFERENCES,0.5177372962607862,"Federated optimization algorithms with random reshuffling and gradient compression. preprint
432"
REFERENCES,0.5186960690316395,"arXiv:2206.07021, 2022b.
433"
REFERENCES,0.5196548418024928,"Scaman, K., Bach, F., Bubeck, S., Lee, Y. T., and Massoulié, L. Optimal convergence rates for convex
434"
REFERENCES,0.5206136145733461,"distributed optimization in networks. Journal of Machine Learning Research, 20:1–31, 2019.
435"
REFERENCES,0.5215723873441994,"Shalev-Shwartz, S. and Ben-David, S. Understanding machine learning: From theory to algorithms.
436"
REFERENCES,0.5225311601150527,"Cambridge University Press, 2014.
437"
REFERENCES,0.5234899328859061,"Sra, S., Nowozin, S., and Wright, S. J. Optimization for Machine Learning. The MIT Press, 2011.
438"
REFERENCES,0.5244487056567594,"Tyurin, A. and Richtárik, P. DASHA: Distributed nonconvex optimization with communication
439"
REFERENCES,0.5254074784276127,"compression, optimal oracle complexity, and no client synchronization. In Proc. of International
440"
REFERENCES,0.5263662511984659,"Conference on Learning Representations (ICLR), 2023a.
441"
REFERENCES,0.5273250239693192,"Tyurin, A. and Richtárik, P. 2Direction: Theoretically faster distributed training with bidirectional
442"
REFERENCES,0.5282837967401726,"communication compression. In Proc. of Conf. Neural Information Processing Systems (NeurIPS),
443"
REFERENCES,0.5292425695110259,"2023b.
444"
REFERENCES,0.5302013422818792,"Wang, J. et al. A field guide to federated optimization. preprint arXiv:2107.06917, 2021.
445"
REFERENCES,0.5311601150527325,"Yang, H., Fang, M., and Liu, J. Achieving linear speedup with partial worker participation in non-IID
446"
REFERENCES,0.5321188878235859,"federated learning. In Proc. of International Conference on Learning Representations (ICLR),
447"
REFERENCES,0.5330776605944392,"2021.
448"
REFERENCES,0.5340364333652924,"Yang, H., Qiu, P., Khanduri, P., and Liu, J. On the efficacy of server-aided federated learning against
449"
REFERENCES,0.5349952061361457,"partial client participation. preprint https://openreview.net/forum?id=Dyzhru5NO3u, 2023.
450"
REFERENCES,0.535953978906999,"Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V. Federated learning with non-iid data.
451"
REFERENCES,0.5369127516778524,"preprint arXiv:1806.00582, 2018.
452"
REFERENCES,0.5378715244487057,"Appendix
453"
REFERENCES,0.538830297219559,"A
Proof of Theorem 3.1
454"
REFERENCES,0.5397890699904123,"We define the Euclidean space X := Rd and the product space X := X n+1 endowed with the
455"
REFERENCES,0.5407478427612655,"weighted inner product
456"
REFERENCES,0.5417066155321189,"⟨x, x′⟩X := n
X"
REFERENCES,0.5426653883029722,"i=1
⟨xi, x′
i⟩+ n⟨y, y′⟩,
∀x = (x1, . . . , xn, y), x′ = (x′
1, . . . , x′
n, y′).
(12)"
REFERENCES,0.5436241610738255,"We define the copy operator 1 : x ∈X 7→(x, . . . , x, x) ∈X and the linear operator
457"
REFERENCES,0.5445829338446788,"S : x ∈X 7→1¯x, with ¯x = 1"
N,0.5455417066155321,"2n n
X"
N,0.5465004793863855,"i=1
xi + ny !"
N,0.5474592521572388,".
(13)"
N,0.548418024928092,"S is the orthogonal projector in X onto the consensus line {x ∈X : x1 = · · · = xn = y}. We also
458"
N,0.5493767976989453,"define the linear operator
459"
N,0.5503355704697986,"W := Id−S : x = (x1, . . . , xn, y) ∈X 7→(x1−¯x, . . . , xn−¯x, y−¯x), with ¯x = 1"
N,0.551294343240652,"2n n
X"
N,0.5522531160115053,"i=1
xi + ny ! ,"
N,0.5532118887823586,"(14)
where Id denotes the identity. W is the orthogonal projector in X onto the hyperplane {x ∈X :
460"
N,0.5541706615532119,"x1 + · · · + xn + ny = 0}, which is orthogonal to the consensus line. As such, it is self-adjoint,
461"
N,0.5551294343240653,"positive semidefinite, its eigenvalues are (1, . . . , 1, 0), its kernel is the consensus line, and its spectral
462"
N,0.5560882070949185,"norm is 1. Also, W 2 = W. Note that we can write W in terms of the differences di = xi −y and
463"
N,0.5570469798657718,"¯d =
1
2n
Pn
i=1 di:
464"
N,0.5580057526366251,"W : x = (x1, . . . , xn, y) 7→
 
d1 −¯d, . . . , dn −¯d, −¯d

.
(15)"
N,0.5589645254074784,"Since for every x = (x1, . . . , xn, y), Wx = 0 := (0, . . . , 0, 0) if and only if x1 = · · · = xn = y,
465"
N,0.5599232981783318,"we can reformulate the problem (1) as
466"
N,0.5608820709491851,"min
x=(x1,...,xn,y)∈X f(x)
s.t.
Wx = 0,
(16)"
N,0.5618408437200384,"where f(x) := Pn
i=1 fi(xi) + ng(y). Note that in X, f is L-smooth and µ-strongly convex, and
467"
N,0.5627996164908916,"∇f(x) =
 
∇f1(x1), . . . ∇fn(xn), ∇g(y)

.
468 469"
N,0.5637583892617449,"Let t ≥0.
We also introduce vector notations for the variables of the algorithm: xt :=
470"
N,0.5647171620325983,"(xt
1, . . . , xt
n, yt), ˆxt := (ˆxt
1, . . . , ˆxt
n, ˆyt), ut := (ut
1, . . . , ut
n, vt), u⋆:= (u⋆
1, . . . , u⋆
n, v⋆), wt :=
471"
N,0.5656759348034516,"xt −γ∇f(xt), w⋆:= x⋆−γ∇f(x⋆), where x⋆:= 1x⋆is the unique solution to (16). We also
472"
N,0.5666347075743049,"define ¯xt :=
1
2n(Pn
i=1 ˆxt
i + nˆyt) and λ :=
pχ
γ(1+2ω).
473"
N,0.5675934803451582,"Then we can write the iteration of LoCoDL as
474
"
N,0.5685522531160115,"ˆxt := xt −γ∇f(xt) + γut = wt + γut
flip a coin θt ∈{0, 1} with Prob(θt = 1) = p
if θt = 1
dt :=
 
Ct
1(ˆxt
1 −ˆyt), . . . , Ct
n(ˆxt
n −ˆyt), 0
"
N,0.5695110258868649,"¯dt :=
1
2n
Pn
j=1 dt
j
xt+1 := (1 −ρ)ˆxt + ρ1(ˆyt + ¯dt)
ut+1 := ut + λ
 
1 ¯dt −dt
= ut −λWdt
else
xt+1 := ˆxt"
N,0.5704697986577181,"ut+1 := ut
end if (17)"
N,0.5714285714285714,"We denote by Ft the σ-algebra generated by the collection of X-valued random variables
475"
N,0.5723873441994247,"x0, u0, . . . , xt, ut.
476"
N,0.573346116970278,"Since we suppose that Su0 = 0 and we have SWdt′ = 0 in the update of u, we have Sut′ = 0 for
477"
N,0.5743048897411314,"every t′ ≥0.
478"
N,0.5752636625119847,"If θt = 1, we have
479"
N,0.576222435282838,"ut+1 −u⋆2
X =
ut −u⋆2
X + λ2 Wdt2
X −2λ⟨ut −u⋆, Wdt⟩X"
N,0.5771812080536913,"=
ut −u⋆2
X + λ2 dt2
X −λ2 Sdt2
X −2λ⟨ut −u⋆, dt⟩X ,"
N,0.5781399808245445,"because Sut = Su⋆= 0, so that ⟨ut −u⋆, Sdt⟩X = 0.
480"
N,0.5790987535953979,"The variance inequality (2) satisfied by the compressors Ct
i is equivalent to E
h
∥Ct
i(x)∥2i
≤(1 +
481"
N,0.5800575263662512,"ω) ∥x∥2, so that
482"
N,0.5810162991371045,"E
hdt2
X | Ft, θt = 1
i
≤(1 + ω)
ˆxt −1ˆyt2
X ."
N,0.5819750719079578,"Also,
483"
N,0.5829338446788112,"E

dt | Ft, θt = 1

= ˆxt −1ˆyt."
N,0.5838926174496645,"Thus,
484"
N,0.5848513902205177,"E
hut+1 −u⋆2
X | Fti
= (1 −p)
ut −u⋆2
X + pE
hut+1 −u⋆2
X | Ft, θt = 1
i"
N,0.585810162991371,"≤
ut −u⋆2
X + pλ2(1 + ω)
ˆxt −1ˆyt2
X −pλ2E
hSdt2
X | Ft, θt = 1
i"
N,0.5867689357622243,"−2pλ⟨ut −u⋆, ˆxt −1ˆyt⟩X"
N,0.5877277085330777,"=
ut −u⋆2
X + pλ2(1 + ω)
ˆxt −1ˆyt2
X −pλ2E
hSdt2
X | Ft, θt = 1
i"
N,0.588686481303931,"−2pλ⟨ut −u⋆, ˆxt⟩X ."
N,0.5896452540747843,"Moreover,
E
h
∥Sdt∥2
X | Ft, θt = 1
i
≥
∥E[Sdt | Ft, θt = 1]∥2
X
=
∥Sˆxt −1ˆyt∥2
X
and
485"
N,0.5906040268456376,"∥ˆxt −1ˆyt∥2
X = ∥Sˆxt −1ˆyt∥2
X + ∥W ˆxt∥2
X , so that
486"
N,0.5915627996164909,"E
hut+1 −u⋆2
X | Fti
≤
ut −u⋆2
X + pλ2(1 + ω)
ˆxt −1ˆyt2
X −pλ2 Sˆxt −1ˆyt2"
N,0.5925215723873442,"−2pλ⟨ut −u⋆, ˆxt⟩X"
N,0.5934803451581975,"=
ut −u⋆2
X + pλ2ω
ˆxt −1ˆyt2
X + pλ2 W ˆxt2 −2pλ⟨ut −u⋆, ˆxt⟩X ."
N,0.5944391179290508,"From the Peter–Paul inequality ∥a + b∥2 ≤2∥a∥2 + 2∥b∥2 for any a and b, we have
487"
N,0.5953978906999041,"ˆxt −1ˆyt2
X = n
X i=1"
N,0.5963566634707574,"ˆxt
i −ˆyt2 = n
X i=1"
N,0.5973154362416108,"(ˆxt
i −¯xt) −(ˆyt −¯xt)
2 ≤ n
X i=1"
N,0.5982742090124641,"
2
ˆxt
i −¯xt)
2 + 2
ˆyt −¯xt2 = 2 n
X i=1"
N,0.5992329817833174,"ˆxt
i −¯xt)
2 + n
ˆyt −¯xt2
!"
N,0.6001917545541706,"= 2
ˆxt −1¯xt2
X = 2
W ˆxt2
X .
(18)"
N,0.6011505273250239,"Hence,
488"
N,0.6021093000958773,"E
hut+1 −u⋆2
X | Fti
≤
ut −u⋆2
X + pλ2(1 + 2ω)
W ˆxt2
X −2pλ⟨ut −u⋆, ˆxt⟩X ."
N,0.6030680728667306,"On the other hand,
489"
N,0.6040268456375839,"E
hxt+1 −x⋆2
X | Ft, θ = 1
i
= (1 −ρ)2 ˆxt −x⋆2
X + ρ2E
h1(ˆyt + ¯dt) −x⋆2
X | Ft, θ = 1
i"
N,0.6049856184084372,"+ 2ρ(1 −ρ)

ˆxt −x⋆, 1
 
ˆyt + E
 ¯dt | Ft, θ = 1
 
−x⋆ X ."
N,0.6059443911792906,"We have E
 ¯dt | Ft, θ = 1

=
1
2n
Pn
i=1 ˆxt
i −1"
N,0.6069031639501438,"2 ˆyt = ¯xt −ˆyt, so that
490"
N,0.6078619367209971,"1
 
ˆyt + E
 ¯dt | Ft, θ = 1

= 1¯xt = Sˆxt."
N,0.6088207094918504,"In addition,
491"
N,0.6097794822627037,"ˆxt −x⋆, Sˆxt −x⋆"
N,0.610738255033557,"X =

ˆxt −x⋆, S(ˆxt −x⋆)"
N,0.6116970278044104,"X =
S(ˆxt −x⋆)
2
X ."
N,0.6126558005752637,"Moreover,
492"
N,0.613614573346117,"E
h1(ˆyt + ¯dt) −x⋆2
X | Ft, θ = 1
i
=
1
 
ˆyt + E
 ¯dt | Ft, θ = 1

−x⋆2
X"
N,0.6145733461169702,"+ E
h1
  ¯dt −E
 ¯dt | Ft, θ = 1
2
X | Ft, θ = 1
i"
N,0.6155321188878236,"=
Sˆxt −x⋆2
X"
N,0.6164908916586769,"+ 2nE
h ¯dt −E
 ¯dt | Ft, θ = 1
2 | Ft, θ = 1
i"
N,0.6174496644295302,"and, using (3),
493"
N,0.6184084372003835,"E
h ¯dt −E
 ¯dt | Ft, θ = 1
2 | Ft, θ = 1
i
≤ωav"
N,0.6193672099712368,"4n n
X i=1"
N,0.6203259827420902,"ˆxt
i −ˆyt2 ≤ωav"
N,0.6212847555129435,2n
N,0.6222435282837967,"W ˆxt2
X ,"
N,0.62320230105465,"where the second inequality follows from (18). Hence,
494"
N,0.6241610738255033,"E
hxt+1 −x⋆2
X | Ft, θ = 1
i
≤(1 −ρ)2 ˆxt −x⋆2
X + ρ2 Sˆxt −x⋆2
X + ρ2ωav
W ˆxt2
X"
N,0.6251198465963567,"+ 2ρ(1 −ρ)
S(ˆxt −x⋆)
2
X
= (1 −ρ)2 ˆxt −x⋆2
X + ρ2ωav
W ˆxt2
X
+ (2ρ −ρ2)
S(ˆxt −x⋆)
2
X
= (1 −ρ)2 ˆxt −x⋆2
X + ρ2ωav
W ˆxt2
X"
N,0.62607861936721,"+ (2ρ −ρ2)
ˆxt −x⋆2
X −
W ˆxt2
X "
N,0.6270373921380633,"=
ˆxt −x⋆2
X −
 
2ρ −ρ2 −ρ2ωav
 W ˆxt2
X"
N,0.6279961649089166,"and
495"
N,0.6289549376797698,"E
hxt+1 −x⋆2
X | Fti
= (1 −p)
ˆxt −x⋆2
X + pE
hxt+1 −x⋆2
X | Ft, θt = 1
i"
N,0.6299137104506232,"≤
ˆxt −x⋆2
X −p
 
2ρ −ρ2(1 + ωav)
 W ˆxt2
X ."
N,0.6308724832214765,"Furthermore,
496"
N,0.6318312559923298,"ˆxt −x⋆2
X =
wt −w⋆2
X + γ2 ut −u⋆2
X + 2γ⟨wt −w⋆, ut −u⋆⟩X"
N,0.6327900287631831,"=
wt −w⋆2
X −γ2 ut −u⋆2
X + 2γ⟨ˆxt −x⋆, ut −u⋆⟩X"
N,0.6337488015340365,"=
wt −w⋆2
X −γ2 ut −u⋆2
X + 2γ⟨ˆxt, ut −u⋆⟩X ,"
N,0.6347075743048898,"which yields
497"
N,0.6356663470757431,"E
hxt+1 −x⋆2
X | Fti
≤
wt −w⋆2
X −γ2 ut −u⋆2
X + 2γ⟨ˆxt, ut −u⋆⟩X"
N,0.6366251198465963,"−p
 
2ρ −ρ2(1 + ωav)
 W ˆxt2
X ."
N,0.6375838926174496,"Hence, with λ =
pχ
γ(1+2ω),
498"
N,0.638542665388303,"1
γ E
hxt+1 −x⋆2
X | Fti
+ γ(1 + 2ω)"
N,0.6395014381591563,"p2χ
E
hut+1 −u⋆2
X | Fti ≤1 γ"
N,0.6404602109300096,"wt −w⋆2
X −γ
ut −u⋆2
X + 2⟨ˆxt, ut −u⋆⟩X −p"
N,0.6414189837008629,"γ
 
2ρ −ρ2(1 + ωav)
 W ˆxt2
X"
N,0.6423777564717162,+ γ(1 + 2ω) p2χ
N,0.6433365292425696,"ut −u⋆2
X + pχ γ"
N,0.6442953020134228,"W ˆxt2
X −2⟨ut −u⋆, ˆxt⟩X = 1 γ"
N,0.6452540747842761,"wt −w⋆2
X + γ(1 + 2ω) p2χ"
N,0.6462128475551294,"
1 −
p2χ
1 + 2ω"
N,0.6471716203259827," ut −u⋆2
X −p"
N,0.6481303930968361,"γ
 
2ρ −ρ2(1 + ωav) −χ
 W ˆxt2
X ."
N,0.6490891658676894,"Therefore, assuming that 2ρ −ρ2(1 + ωav) −χ ≥0,
499"
N,0.6500479386385427,"E

Ψt+1 | Ft
≤1 γ"
N,0.6510067114093959,"wt −w⋆2
X +

1 −
p2χ
1 + 2ω"
N,0.6519654841802492, γ(1 + 2ω) p2χ
N,0.6529242569511026,"ut −u⋆2
X ."
N,0.6538830297219559,"According to Condat & Richtárik (2023, Lemma 1),
500"
N,0.6548418024928092,"wt −w⋆2
X =
(Id −γ∇f)xt −(Id −γ∇f)x⋆2
X
≤max(1 −γµ, γL −1)2 xt −x⋆2
X ."
N,0.6558005752636625,"Hence,
501"
N,0.6567593480345159,"E

Ψt+1 | Ft
≤max

(1 −γµ)2, (1 −γL)2, 1 −
p2χ
1 + 2ω"
N,0.6577181208053692,"
Ψt.
(19)"
N,0.6586768935762224,"Using the tower rule, we can unroll the recursion in (19) to obtain the unconditional expectation of
502"
N,0.6596356663470757,"Ψt+1.
503"
N,0.660594439117929,"Using classical results on supermartingale convergence (Bertsekas, 2015, Proposition A.4.5), it
504"
N,0.6615532118887824,"follows from (19) that Ψt →0 almost surely. Almost sure convergence of xt and ut follows.
505"
N,0.6625119846596357,"B
Additional Experiments
506"
N,0.663470757430489,"The results for the experiments in Section 4 with the ‘diabetes’ dataset from the LibSVM library
507"
N,0.6644295302013423,"(Chang & Lin, 2011) are shown in Figure 2. The results with the ‘w1a’ and ‘australian’ datasets, for
508"
N,0.6653883029721956,"the same logistic regression problem with κ = 104, are shown in Figures 3 and 4.
509"
N,0.6663470757430489,"Consistent with our previous findings, LoCoDL outperforms the other algorithms in terms of commu-
510"
N,0.6673058485139022,"nication efficiency.
511"
N,0.6682646212847555,"0
50000
100000
150000
200000
250000
300000
350000
400000
Number of Communicated Bits 10
5 10
4 10
3 10
2 10
1 100"
N,0.6692233940556088,"f(x)
f*"
N,0.6701821668264621,"LoCoDL: Rand-2
LoCoDL: Natural
LoCoDL: Rand-2 + Natural
LoCoDL: l1-select
ADIANA: Rand-2
ADIANA: Natural
ADIANA: Rand-2 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
N,0.6711409395973155,"0
50000
100000
150000
200000
250000
300000
350000
400000
Number of Communicated Bits 10
5 10
4 10
3 10
2 10
1 100"
N,0.6720997123681688,"f(x)
f*"
N,0.673058485139022,"LoCoDL: Rand-1
LoCoDL: Natural
LoCoDL: Rand-1 + Natural
LoCoDL: l1-select
ADIANA: Rand-2
ADIANA: Natural
ADIANA: Rand-2 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
N,0.6740172579098753,"(a) n = 6
(b) n = 37"
N,0.6749760306807286,"0
50000
100000
150000
200000
250000
300000
350000
400000
Number of Communicated Bits 10
5 10
4 10
3 10
2 10
1 100"
N,0.675934803451582,"f(x)
f*"
N,0.6768935762224353,"LoCoDL: Rand-1
LoCoDL: Natural
LoCoDL: Rand-1 + Natural
LoCoDL: l1-select
ADIANA: Rand-2
ADIANA: Natural
ADIANA: Rand-2 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
N,0.6778523489932886,(c) n = 73
N,0.6788111217641419,"Figure 2: Comparison of several algorithms with several compressors on logistic regression with the
‘diabetes’ dataset from the LibSVM, which has d = 8 and 768 data points. We chose different values
of n to illustrate the three regimes n < d, n > d, n > d2, as discussed at the end of Section 3."
N,0.6797698945349953,"0
50000
100000
150000
200000
250000
300000
Number of Communicated Bits 10
5 10
4 10
3 10
2 10
1 100"
N,0.6807286673058485,"f(x)
f*"
N,0.6816874400767018,"LoCoDL: Rand-2
LoCoDL: Natural
LoCoDL: Rand-2 + Natural
LoCoDL: l1-select
ADIANA: Rand-3
ADIANA: Natural
ADIANA: Rand-3 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
N,0.6826462128475551,"0
50000
100000
150000
200000
250000
300000
Number of Communicated Bits 10
5 10
4 10
3 10
2 10
1 100"
N,0.6836049856184084,"f(x)
f*"
N,0.6845637583892618,"LoCoDL: Rand-1
LoCoDL: Natural
LoCoDL: Rand-1 + Natural
LoCoDL: l1-select
ADIANA: Rand-3
ADIANA: Natural
ADIANA: Rand-3 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
N,0.6855225311601151,"(a) n = 9
(b) n = 41"
N,0.6864813039309684,"0
50000
100000
150000
200000
250000
300000
Number of Communicated Bits 10
5 10
4 10
3 10
2 10
1 100"
N,0.6874400767018217,"f(x)
f*"
N,0.6883988494726749,"LoCoDL: Rand-1
LoCoDL: Natural
LoCoDL: Rand-1 + Natural
LoCoDL: l1-select
ADIANA: Rand-3
ADIANA: Natural
ADIANA: Rand-3 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
N,0.6893576222435283,(c) n = 225
N,0.6903163950143816,"Figure 3: Comparison of several algorithms with various compressors on logistic regression with the
‘australian’ dataset from the LibSVM, which has d = 14 and 690 data points. We chose different
values of n to illustrate the three regimes: n < d, n > d, n > d2, as discussed at the end of Section 3."
N,0.6912751677852349,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Number of Communicated Bits
1e6 10
5 10
4 10
3 10
2 10
1 100"
N,0.6922339405560882,"f(x)
f*"
N,0.6931927133269415,"LoCoDL: Rand-4
LoCoDL: Natural
LoCoDL: Rand-4 + Natural
LoCoDL: l1-select
ADIANA: Rand-75
ADIANA: Natural
ADIANA: Rand-75 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
N,0.6941514860977949,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Number of Communicated Bits
1e6 10
5 10
4 10
3 10
2 10
1 100"
N,0.6951102588686481,"f(x)
f*"
N,0.6960690316395014,"LoCoDL: Rand-1
LoCoDL: Natural
LoCoDL: Rand-1 + Natural
LoCoDL: l1-select
ADIANA: Rand-75
ADIANA: Natural
ADIANA: Rand-75 + Natural
ADIANA: l1-select
DIANA: Rand-1
DIANA: Natural
DIANA: Rand-1 + Natural
DIANA: l1-select
5GCS-CC: Rand-1
5GCS-CC: Natural
5GCS-CC: Rand-1 + Natural
5GCS-CC: l1-select
CompressedScaffnew: s=2
GradSkip
Scaffold"
N,0.6970278044103547,"(a) n = 87
(b) n = 619"
N,0.697986577181208,"Figure 4: Comparison of several algorithms with various compressors on logistic regression with the
‘w1a’ dataset from the LibSVM, which has d = 300 and 2,477 data points. We chose different values
of n to illustrate the two regimes, n < d and n > d, as discussed at the end of Section 3."
N,0.6989453499520614,"NeurIPS Paper Checklist
512"
CLAIMS,0.6999041227229147,"1. Claims
513"
CLAIMS,0.700862895493768,"Question: Do the main claims made in the abstract and introduction accurately reflect the
514"
CLAIMS,0.7018216682646213,"paper’s contributions and scope?
515"
CLAIMS,0.7027804410354745,"Answer: [Yes]
516"
CLAIMS,0.7037392138063279,"Justification: our contribution is the unique combination of the two key mechanisms of
517"
CLAIMS,0.7046979865771812,"local training and compression, as mentioned in the title and detailed in the abstract and
518"
CLAIMS,0.7056567593480345,"introduction.
519"
CLAIMS,0.7066155321188878,"Guidelines:
520"
CLAIMS,0.7075743048897412,"• The answer NA means that the abstract and introduction do not include the claims
521"
CLAIMS,0.7085330776605945,"made in the paper.
522"
CLAIMS,0.7094918504314478,"• The abstract and/or introduction should clearly state the claims made, including the
523"
CLAIMS,0.710450623202301,"contributions made in the paper and important assumptions and limitations. A No or
524"
CLAIMS,0.7114093959731543,"NA answer to this question will not be perceived well by the reviewers.
525"
CLAIMS,0.7123681687440077,"• The claims made should match theoretical and experimental results, and reflect how
526"
CLAIMS,0.713326941514861,"much the results can be expected to generalize to other settings.
527"
CLAIMS,0.7142857142857143,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
528"
CLAIMS,0.7152444870565676,"are not attained by the paper.
529"
LIMITATIONS,0.716203259827421,"2. Limitations
530"
LIMITATIONS,0.7171620325982742,"Question: Does the paper discuss the limitations of the work performed by the authors?
531"
LIMITATIONS,0.7181208053691275,"Answer: [Yes]
532"
LIMITATIONS,0.7190795781399808,"Justification: the limitations are discussed in the conclusion.
533"
LIMITATIONS,0.7200383509108341,"Guidelines:
534"
LIMITATIONS,0.7209971236816874,"• The answer NA means that the paper has no limitation while the answer No means that
535"
LIMITATIONS,0.7219558964525408,"the paper has limitations, but those are not discussed in the paper.
536"
LIMITATIONS,0.7229146692233941,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
537"
LIMITATIONS,0.7238734419942474,"• The paper should point out any strong assumptions and how robust the results are to
538"
LIMITATIONS,0.7248322147651006,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
539"
LIMITATIONS,0.725790987535954,"model well-specification, asymptotic approximations only holding locally). The authors
540"
LIMITATIONS,0.7267497603068073,"should reflect on how these assumptions might be violated in practice and what the
541"
LIMITATIONS,0.7277085330776606,"implications would be.
542"
LIMITATIONS,0.7286673058485139,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
543"
LIMITATIONS,0.7296260786193672,"only tested on a few datasets or with a few runs. In general, empirical results often
544"
LIMITATIONS,0.7305848513902206,"depend on implicit assumptions, which should be articulated.
545"
LIMITATIONS,0.7315436241610739,"• The authors should reflect on the factors that influence the performance of the approach.
546"
LIMITATIONS,0.7325023969319271,"For example, a facial recognition algorithm may perform poorly when image resolution
547"
LIMITATIONS,0.7334611697027804,"is low or images are taken in low lighting. Or a speech-to-text system might not be
548"
LIMITATIONS,0.7344199424736337,"used reliably to provide closed captions for online lectures because it fails to handle
549"
LIMITATIONS,0.7353787152444871,"technical jargon.
550"
LIMITATIONS,0.7363374880153404,"• The authors should discuss the computational efficiency of the proposed algorithms
551"
LIMITATIONS,0.7372962607861937,"and how they scale with dataset size.
552"
LIMITATIONS,0.738255033557047,"• If applicable, the authors should discuss possible limitations of their approach to
553"
LIMITATIONS,0.7392138063279002,"address problems of privacy and fairness.
554"
LIMITATIONS,0.7401725790987536,"• While the authors might fear that complete honesty about limitations might be used by
555"
LIMITATIONS,0.7411313518696069,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
556"
LIMITATIONS,0.7420901246404602,"limitations that aren’t acknowledged in the paper. The authors should use their best
557"
LIMITATIONS,0.7430488974113135,"judgment and recognize that individual actions in favor of transparency play an impor-
558"
LIMITATIONS,0.7440076701821668,"tant role in developing norms that preserve the integrity of the community. Reviewers
559"
LIMITATIONS,0.7449664429530202,"will be specifically instructed to not penalize honesty concerning limitations.
560"
THEORY ASSUMPTIONS AND PROOFS,0.7459252157238735,"3. Theory Assumptions and Proofs
561"
THEORY ASSUMPTIONS AND PROOFS,0.7468839884947267,"Question: For each theoretical result, does the paper provide the full set of assumptions and
562"
THEORY ASSUMPTIONS AND PROOFS,0.74784276126558,"a complete (and correct) proof?
563"
THEORY ASSUMPTIONS AND PROOFS,0.7488015340364333,"Answer: [Yes]
564"
THEORY ASSUMPTIONS AND PROOFS,0.7497603068072867,"Justification: the proofs are in the appendix.
565"
THEORY ASSUMPTIONS AND PROOFS,0.75071907957814,"Guidelines:
566"
THEORY ASSUMPTIONS AND PROOFS,0.7516778523489933,"• The answer NA means that the paper does not include theoretical results.
567"
THEORY ASSUMPTIONS AND PROOFS,0.7526366251198466,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
568"
THEORY ASSUMPTIONS AND PROOFS,0.7535953978907,"referenced.
569"
THEORY ASSUMPTIONS AND PROOFS,0.7545541706615532,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
570"
THEORY ASSUMPTIONS AND PROOFS,0.7555129434324065,"• The proofs can either appear in the main paper or the supplemental material, but if
571"
THEORY ASSUMPTIONS AND PROOFS,0.7564717162032598,"they appear in the supplemental material, the authors are encouraged to provide a short
572"
THEORY ASSUMPTIONS AND PROOFS,0.7574304889741131,"proof sketch to provide intuition.
573"
THEORY ASSUMPTIONS AND PROOFS,0.7583892617449665,"• Inversely, any informal proof provided in the core of the paper should be complemented
574"
THEORY ASSUMPTIONS AND PROOFS,0.7593480345158198,"by formal proofs provided in appendix or supplemental material.
575"
THEORY ASSUMPTIONS AND PROOFS,0.7603068072866731,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7612655800575263,"4. Experimental Result Reproducibility
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7622243528283796,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.763183125599233,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7641418983700863,"of the paper (regardless of whether the code and data are provided or not)?
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7651006711409396,"Answer: [Yes]
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7660594439117929,"Justification: the pseudo-code of our proposed algorithm is given. It is short and easy to
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7670182166826462,"implement. The parameter values for the experiments are provided.
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7679769894534996,"Guidelines:
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7689357622243528,"• The answer NA means that the paper does not include experiments.
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7698945349952061,"• If the paper includes experiments, a No answer to this question will not be perceived
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7708533077660594,"well by the reviewers: Making the paper reproducible is important, regardless of
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7718120805369127,"whether the code and data are provided or not.
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7727708533077661,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7737296260786194,"to make their results reproducible or verifiable.
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746883988494727,"• Depending on the contribution, reproducibility can be accomplished in various ways.
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.775647171620326,"For example, if the contribution is a novel architecture, describing the architecture fully
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7766059443911792,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7775647171620326,"be necessary to either make it possible for others to replicate the model with the same
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7785234899328859,"dataset, or provide access to the model. In general. releasing code and data is often
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7794822627037392,"one good way to accomplish this, but reproducibility can also be provided via detailed
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7804410354745925,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7813998082454459,"of a large language model), releasing of a model checkpoint, or other means that are
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7823585810162992,"appropriate to the research performed.
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7833173537871524,"• While NeurIPS does not require releasing code, the conference does require all submis-
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7842761265580057,"sions to provide some reasonable avenue for reproducibility, which may depend on the
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.785234899328859,"nature of the contribution. For example
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7861936720997124,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7871524448705657,"to reproduce that algorithm.
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.788111217641419,"(b) If the contribution is primarily a new model architecture, the paper should describe
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7890699904122723,"the architecture clearly and fully.
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7900287631831256,"(c) If the contribution is a new model (e.g., a large language model), then there should
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7909875359539789,"either be a way to access this model for reproducing the results or a way to reproduce
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7919463087248322,"the model (e.g., with an open-source dataset or instructions for how to construct
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7929050814956855,"the dataset).
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7938638542665388,"(d) We recognize that reproducibility may be tricky in some cases, in which case
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7948226270373921,"authors are welcome to describe the particular way they provide for reproducibility.
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7957813998082455,"In the case of closed-source models, it may be that access to the model is limited in
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7967401725790988,"some way (e.g., to registered users), but it should be possible for other researchers
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7976989453499521,"to have some path to reproducing or verifying the results.
615"
OPEN ACCESS TO DATA AND CODE,0.7986577181208053,"5. Open access to data and code
616"
OPEN ACCESS TO DATA AND CODE,0.7996164908916586,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
617"
OPEN ACCESS TO DATA AND CODE,0.800575263662512,"tions to faithfully reproduce the main experimental results, as described in supplemental
618"
OPEN ACCESS TO DATA AND CODE,0.8015340364333653,"material?
619"
OPEN ACCESS TO DATA AND CODE,0.8024928092042186,"Answer: [Yes]
620"
OPEN ACCESS TO DATA AND CODE,0.8034515819750719,"Justification: The data used in the experiments are publicly available. The code is provided.
621"
OPEN ACCESS TO DATA AND CODE,0.8044103547459253,"Guidelines:
622"
OPEN ACCESS TO DATA AND CODE,0.8053691275167785,"• The answer NA means that paper does not include experiments requiring code.
623"
OPEN ACCESS TO DATA AND CODE,0.8063279002876318,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
624"
OPEN ACCESS TO DATA AND CODE,0.8072866730584851,"public/guides/CodeSubmissionPolicy) for more details.
625"
OPEN ACCESS TO DATA AND CODE,0.8082454458293384,"• While we encourage the release of code and data, we understand that this might not be
626"
OPEN ACCESS TO DATA AND CODE,0.8092042186001918,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
627"
OPEN ACCESS TO DATA AND CODE,0.8101629913710451,"including code, unless this is central to the contribution (e.g., for a new open-source
628"
OPEN ACCESS TO DATA AND CODE,0.8111217641418984,"benchmark).
629"
OPEN ACCESS TO DATA AND CODE,0.8120805369127517,"• The instructions should contain the exact command and environment needed to run to
630"
OPEN ACCESS TO DATA AND CODE,0.8130393096836049,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
631"
OPEN ACCESS TO DATA AND CODE,0.8139980824544583,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
632"
OPEN ACCESS TO DATA AND CODE,0.8149568552253116,"• The authors should provide instructions on data access and preparation, including how
633"
OPEN ACCESS TO DATA AND CODE,0.8159156279961649,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
634"
OPEN ACCESS TO DATA AND CODE,0.8168744007670182,"• The authors should provide scripts to reproduce all experimental results for the new
635"
OPEN ACCESS TO DATA AND CODE,0.8178331735378715,"proposed method and baselines. If only a subset of experiments are reproducible, they
636"
OPEN ACCESS TO DATA AND CODE,0.8187919463087249,"should state which ones are omitted from the script and why.
637"
OPEN ACCESS TO DATA AND CODE,0.8197507190795782,"• At submission time, to preserve anonymity, the authors should release anonymized
638"
OPEN ACCESS TO DATA AND CODE,0.8207094918504314,"versions (if applicable).
639"
OPEN ACCESS TO DATA AND CODE,0.8216682646212847,"• Providing as much information as possible in supplemental material (appended to the
640"
OPEN ACCESS TO DATA AND CODE,0.822627037392138,"paper) is recommended, but including URLs to data and code is permitted.
641"
OPEN ACCESS TO DATA AND CODE,0.8235858101629914,"6. Experimental Setting/Details
642"
OPEN ACCESS TO DATA AND CODE,0.8245445829338447,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
643"
OPEN ACCESS TO DATA AND CODE,0.825503355704698,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
644"
OPEN ACCESS TO DATA AND CODE,0.8264621284755513,"results?
645"
OPEN ACCESS TO DATA AND CODE,0.8274209012464045,"Answer: [Yes]
646"
OPEN ACCESS TO DATA AND CODE,0.8283796740172579,"Justification: We provide these details.
647"
OPEN ACCESS TO DATA AND CODE,0.8293384467881112,"Guidelines:
648"
OPEN ACCESS TO DATA AND CODE,0.8302972195589645,"• The answer NA means that the paper does not include experiments.
649"
OPEN ACCESS TO DATA AND CODE,0.8312559923298178,"• The experimental setting should be presented in the core of the paper to a level of detail
650"
OPEN ACCESS TO DATA AND CODE,0.8322147651006712,"that is necessary to appreciate the results and make sense of them.
651"
OPEN ACCESS TO DATA AND CODE,0.8331735378715245,"• The full details can be provided either with the code, in appendix, or as supplemental
652"
OPEN ACCESS TO DATA AND CODE,0.8341323106423778,"material.
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.835091083413231,"7. Experiment Statistical Significance
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8360498561840843,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8370086289549377,"information about the statistical significance of the experiments?
656"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.837967401725791,"Answer: [No]
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8389261744966443,"Justification: the variability with respect to different random realizations plays a minor role
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8398849472674976,"in the performance.
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.840843720038351,"Guidelines:
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8418024928092043,"• The answer NA means that the paper does not include experiments.
661"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8427612655800575,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
662"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8437200383509108,"dence intervals, or statistical significance tests, at least for the experiments that support
663"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8446788111217641,"the main claims of the paper.
664"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8456375838926175,"• The factors of variability that the error bars are capturing should be clearly stated (for
665"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8465963566634708,"example, train/test split, initialization, random drawing of some parameter, or overall
666"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8475551294343241,"run with given experimental conditions).
667"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8485139022051774,"• The method for calculating the error bars should be explained (closed form formula,
668"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8494726749760306,"call to a library function, bootstrap, etc.)
669"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.850431447746884,"• The assumptions made should be given (e.g., Normally distributed errors).
670"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8513902205177373,"• It should be clear whether the error bar is the standard deviation or the standard error
671"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8523489932885906,"of the mean.
672"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8533077660594439,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
673"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8542665388302972,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
674"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8552253116011506,"of Normality of errors is not verified.
675"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8561840843720039,"• For asymmetric distributions, the authors should be careful not to show in tables or
676"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"figures symmetric error bars that would yield results that are out of range (e.g. negative
677"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8581016299137104,"error rates).
678"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8590604026845637,"• If error bars are reported in tables or plots, The authors should explain in the text how
679"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8600191754554171,"they were calculated and reference the corresponding figures or tables in the text.
680"
EXPERIMENTS COMPUTE RESOURCES,0.8609779482262704,"8. Experiments Compute Resources
681"
EXPERIMENTS COMPUTE RESOURCES,0.8619367209971237,"Question: For each experiment, does the paper provide sufficient information on the com-
682"
EXPERIMENTS COMPUTE RESOURCES,0.862895493767977,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
683"
EXPERIMENTS COMPUTE RESOURCES,0.8638542665388304,"the experiments?
684"
EXPERIMENTS COMPUTE RESOURCES,0.8648130393096836,"Answer: [Yes]
685"
EXPERIMENTS COMPUTE RESOURCES,0.8657718120805369,"Justification: we do not focus on the computation time but on the number of communicated
686"
EXPERIMENTS COMPUTE RESOURCES,0.8667305848513902,"bits, which is independent from the hardware. So the experiments can be reproduced on any
687"
EXPERIMENTS COMPUTE RESOURCES,0.8676893576222435,"machine.
688"
EXPERIMENTS COMPUTE RESOURCES,0.8686481303930969,"Guidelines:
689"
EXPERIMENTS COMPUTE RESOURCES,0.8696069031639502,"• The answer NA means that the paper does not include experiments.
690"
EXPERIMENTS COMPUTE RESOURCES,0.8705656759348035,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
691"
EXPERIMENTS COMPUTE RESOURCES,0.8715244487056567,"or cloud provider, including relevant memory and storage.
692"
EXPERIMENTS COMPUTE RESOURCES,0.87248322147651,"• The paper should provide the amount of compute required for each of the individual
693"
EXPERIMENTS COMPUTE RESOURCES,0.8734419942473634,"experimental runs as well as estimate the total compute.
694"
EXPERIMENTS COMPUTE RESOURCES,0.8744007670182167,"• The paper should disclose whether the full research project required more compute
695"
EXPERIMENTS COMPUTE RESOURCES,0.87535953978907,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
696"
EXPERIMENTS COMPUTE RESOURCES,0.8763183125599233,"didn’t make it into the paper).
697"
CODE OF ETHICS,0.8772770853307766,"9. Code Of Ethics
698"
CODE OF ETHICS,0.87823585810163,"Question: Does the research conducted in the paper conform, in every respect, with the
699"
CODE OF ETHICS,0.8791946308724832,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
700"
CODE OF ETHICS,0.8801534036433365,"Answer: [Yes]
701"
CODE OF ETHICS,0.8811121764141898,"Justification:
702"
CODE OF ETHICS,0.8820709491850431,"Guidelines:
703"
CODE OF ETHICS,0.8830297219558965,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
704"
CODE OF ETHICS,0.8839884947267498,"• If the authors answer No, they should explain the special circumstances that require a
705"
CODE OF ETHICS,0.8849472674976031,"deviation from the Code of Ethics.
706"
CODE OF ETHICS,0.8859060402684564,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
707"
CODE OF ETHICS,0.8868648130393096,"eration due to laws or regulations in their jurisdiction).
708"
BROADER IMPACTS,0.887823585810163,"10. Broader Impacts
709"
BROADER IMPACTS,0.8887823585810163,"Question: Does the paper discuss both potential positive societal impacts and negative
710"
BROADER IMPACTS,0.8897411313518696,"societal impacts of the work performed?
711"
BROADER IMPACTS,0.8906999041227229,"Answer: [NA]
712"
BROADER IMPACTS,0.8916586768935763,"Justification: This is a theoretical work whose goal is to advance the foundations of opti-
713"
BROADER IMPACTS,0.8926174496644296,"mization and machine learning. The positive impact of developing more efficient algorithms
714"
BROADER IMPACTS,0.8935762224352828,"is clear and does not need to be discussed. We do not see any particular negative impact.
715"
BROADER IMPACTS,0.8945349952061361,"Guidelines:
716"
BROADER IMPACTS,0.8954937679769894,"• The answer NA means that there is no societal impact of the work performed.
717"
BROADER IMPACTS,0.8964525407478428,"• If the authors answer NA or No, they should explain why their work has no societal
718"
BROADER IMPACTS,0.8974113135186961,"impact or why the paper does not address societal impact.
719"
BROADER IMPACTS,0.8983700862895494,"• Examples of negative societal impacts include potential malicious or unintended uses
720"
BROADER IMPACTS,0.8993288590604027,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
721"
BROADER IMPACTS,0.900287631831256,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
722"
BROADER IMPACTS,0.9012464046021093,"groups), privacy considerations, and security considerations.
723"
BROADER IMPACTS,0.9022051773729626,"• The conference expects that many papers will be foundational research and not tied
724"
BROADER IMPACTS,0.9031639501438159,"to particular applications, let alone deployments. However, if there is a direct path to
725"
BROADER IMPACTS,0.9041227229146692,"any negative applications, the authors should point it out. For example, it is legitimate
726"
BROADER IMPACTS,0.9050814956855225,"to point out that an improvement in the quality of generative models could be used to
727"
BROADER IMPACTS,0.9060402684563759,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
728"
BROADER IMPACTS,0.9069990412272292,"that a generic algorithm for optimizing neural networks could enable people to train
729"
BROADER IMPACTS,0.9079578139980825,"models that generate Deepfakes faster.
730"
BROADER IMPACTS,0.9089165867689357,"• The authors should consider possible harms that could arise when the technology is
731"
BROADER IMPACTS,0.909875359539789,"being used as intended and functioning correctly, harms that could arise when the
732"
BROADER IMPACTS,0.9108341323106424,"technology is being used as intended but gives incorrect results, and harms following
733"
BROADER IMPACTS,0.9117929050814957,"from (intentional or unintentional) misuse of the technology.
734"
BROADER IMPACTS,0.912751677852349,"• If there are negative societal impacts, the authors could also discuss possible mitigation
735"
BROADER IMPACTS,0.9137104506232023,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
736"
BROADER IMPACTS,0.9146692233940557,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
737"
BROADER IMPACTS,0.9156279961649089,"feedback over time, improving the efficiency and accessibility of ML).
738"
SAFEGUARDS,0.9165867689357622,"11. Safeguards
739"
SAFEGUARDS,0.9175455417066155,"Question: Does the paper describe safeguards that have been put in place for responsible
740"
SAFEGUARDS,0.9185043144774688,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
741"
SAFEGUARDS,0.9194630872483222,"image generators, or scraped datasets)?
742"
SAFEGUARDS,0.9204218600191755,"Answer: [NA]
743"
SAFEGUARDS,0.9213806327900288,"Justification:
744"
SAFEGUARDS,0.9223394055608821,"Guidelines:
745"
SAFEGUARDS,0.9232981783317353,"• The answer NA means that the paper poses no such risks.
746"
SAFEGUARDS,0.9242569511025887,"• Released models that have a high risk for misuse or dual-use should be released with
747"
SAFEGUARDS,0.925215723873442,"necessary safeguards to allow for controlled use of the model, for example by requiring
748"
SAFEGUARDS,0.9261744966442953,"that users adhere to usage guidelines or restrictions to access the model or implementing
749"
SAFEGUARDS,0.9271332694151486,"safety filters.
750"
SAFEGUARDS,0.9280920421860019,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
751"
SAFEGUARDS,0.9290508149568553,"should describe how they avoided releasing unsafe images.
752"
SAFEGUARDS,0.9300095877277086,"• We recognize that providing effective safeguards is challenging, and many papers do
753"
SAFEGUARDS,0.9309683604985618,"not require this, but we encourage authors to take this into account and make a best
754"
SAFEGUARDS,0.9319271332694151,"faith effort.
755"
LICENSES FOR EXISTING ASSETS,0.9328859060402684,"12. Licenses for existing assets
756"
LICENSES FOR EXISTING ASSETS,0.9338446788111218,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
757"
LICENSES FOR EXISTING ASSETS,0.9348034515819751,"the paper, properly credited and are the license and terms of use explicitly mentioned and
758"
LICENSES FOR EXISTING ASSETS,0.9357622243528284,"properly respected?
759"
LICENSES FOR EXISTING ASSETS,0.9367209971236817,"Answer: [Yes]
760"
LICENSES FOR EXISTING ASSETS,0.9376797698945349,"Justification: we use the LibSVM and mention the 3-clause BSD license.
761"
LICENSES FOR EXISTING ASSETS,0.9386385426653883,"Guidelines:
762"
LICENSES FOR EXISTING ASSETS,0.9395973154362416,"• The answer NA means that the paper does not use existing assets.
763"
LICENSES FOR EXISTING ASSETS,0.9405560882070949,"• The authors should cite the original paper that produced the code package or dataset.
764"
LICENSES FOR EXISTING ASSETS,0.9415148609779482,"• The authors should state which version of the asset is used and, if possible, include a
765"
LICENSES FOR EXISTING ASSETS,0.9424736337488016,"URL.
766"
LICENSES FOR EXISTING ASSETS,0.9434324065196549,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
767"
LICENSES FOR EXISTING ASSETS,0.9443911792905082,"• For scraped data from a particular source (e.g., website), the copyright and terms of
768"
LICENSES FOR EXISTING ASSETS,0.9453499520613614,"service of that source should be provided.
769"
LICENSES FOR EXISTING ASSETS,0.9463087248322147,"• If assets are released, the license, copyright information, and terms of use in the
770"
LICENSES FOR EXISTING ASSETS,0.947267497603068,"package should be provided. For popular datasets, paperswithcode.com/datasets
771"
LICENSES FOR EXISTING ASSETS,0.9482262703739214,"has curated licenses for some datasets. Their licensing guide can help determine the
772"
LICENSES FOR EXISTING ASSETS,0.9491850431447747,"license of a dataset.
773"
LICENSES FOR EXISTING ASSETS,0.950143815915628,"• For existing datasets that are re-packaged, both the original license and the license of
774"
LICENSES FOR EXISTING ASSETS,0.9511025886864813,"the derived asset (if it has changed) should be provided.
775"
LICENSES FOR EXISTING ASSETS,0.9520613614573347,"• If this information is not available online, the authors are encouraged to reach out to
776"
LICENSES FOR EXISTING ASSETS,0.9530201342281879,"the asset’s creators.
777"
NEW ASSETS,0.9539789069990412,"13. New Assets
778"
NEW ASSETS,0.9549376797698945,"Question: Are new assets introduced in the paper well documented and is the documentation
779"
NEW ASSETS,0.9558964525407478,"provided alongside the assets?
780"
NEW ASSETS,0.9568552253116012,"Answer: [NA]
781"
NEW ASSETS,0.9578139980824545,"Justification:
782"
NEW ASSETS,0.9587727708533078,"Guidelines:
783"
NEW ASSETS,0.959731543624161,"• The answer NA means that the paper does not release new assets.
784"
NEW ASSETS,0.9606903163950143,"• Researchers should communicate the details of the dataset/code/model as part of their
785"
NEW ASSETS,0.9616490891658677,"submissions via structured templates. This includes details about training, license,
786"
NEW ASSETS,0.962607861936721,"limitations, etc.
787"
NEW ASSETS,0.9635666347075743,"• The paper should discuss whether and how consent was obtained from people whose
788"
NEW ASSETS,0.9645254074784276,"asset is used.
789"
NEW ASSETS,0.965484180249281,"• At submission time, remember to anonymize your assets (if applicable). You can either
790"
NEW ASSETS,0.9664429530201343,"create an anonymized URL or include an anonymized zip file.
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9674017257909875,"14. Crowdsourcing and Research with Human Subjects
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9683604985618408,"Question: For crowdsourcing experiments and research with human subjects, does the paper
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693192713326941,"include the full text of instructions given to participants and screenshots, if applicable, as
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702780441035475,"well as details about compensation (if any)?
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9712368168744008,"Answer: [NA]
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9721955896452541,"Justification:
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9731543624161074,"Guidelines:
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9741131351869607,"• The answer NA means that the paper does not involve crowdsourcing nor research with
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975071907957814,"human subjects.
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760306807286673,"• Including this information in the supplemental material is fine, but if the main contribu-
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769894534995206,"tion of the paper involves human subjects, then as much detail as possible should be
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9779482262703739,"included in the main paper.
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9789069990412272,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798657718120806,"or other labor should be paid at least the minimum wage in the country of the data
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808245445829339,"collector.
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817833173537871,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827420901246404,"Subjects
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837008628954937,"Question: Does the paper describe potential risks incurred by study participants, whether
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846596356663471,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856184084372004,"approvals (or an equivalent approval/review based on the requirements of your country or
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865771812080537,"institution) were obtained?
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987535953978907,"Answer: [NA]
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884947267497604,"Justification:
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894534995206136,"Guidelines:
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904122722914669,"• The answer NA means that the paper does not involve crowdsourcing nor research with
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913710450623202,"human subjects.
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923298178331735,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932885906040269,"may be required for any human subjects research. If you obtained IRB approval, you
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942473633748802,"should clearly state this in the paper.
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952061361457335,"• We recognize that the procedures for this may vary significantly between institutions
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961649089165868,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99712368168744,"guidelines for their institution.
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980824544582934,"• For initial submissions, do not include any information that would break anonymity (if
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990412272291467,"applicable), such as the institution conducting the review.
825"
