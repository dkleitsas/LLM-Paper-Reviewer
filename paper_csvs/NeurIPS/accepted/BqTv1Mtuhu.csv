Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0024509803921568627,"Transformer-based architectures have recently exhibited remarkable successes
across different domains beyond just powering large language models.
How-
ever, existing approaches typically focus on predictive accuracy and computa-
tional cost, largely ignoring certain other practical issues such as robustness to
contaminated samples. In this paper, by re-interpreting the self-attention mecha-
nism as a non-parametric kernel density estimator, we adapt classical robust ker-
nel density estimation methods to develop novel classes of transformers that are
resistant to adversarial attacks and data contamination. We first propose meth-
ods that down-weight outliers in RKHS when computing the self-attention oper-
ations. We empirically show that these methods produce improved performance
over existing state-of-the-art methods, particularly on image data under adversar-
ial attacks. Then we leverage the median-of-means principle to obtain another
efficient approach that results in noticeably enhanced performance and robust-
ness on language modeling and time series classification tasks. Our methods can
be combined with existing transformers to augment their robust properties, thus
promising to impact a wide variety of applications."
INTRODUCTION,0.004901960784313725,"1
Introduction"
INTRODUCTION,0.007352941176470588,"While there have been notable advancements, the robustness of the standard attention module re-
mains an unresolved issue in the literature. In this paper, our goal is to reinforce the attention mech-
anism and construct a comprehensive framework for robust transformer models. To achieve this, we
first revisit the interpretation of self-attention in transformers, viewing it through the prism of the
Nadaraya-Watson (NW) estimator (Nadaraya, 1964) in a non-parametric regression context. Within
the transformer paradigm, the NW estimator is constructed based on the kernel density estimators
(KDE) of the keys and queries. However, these KDEs are not immune to the issue of sample con-
tamination (Kim & Scott, 2012). By conceptualizing the KDE as a solution to the kernel regression
problem within a Reproducing Kernel Hilbert Space (RKHS), we can utilize a range of state-of-the-
art robust KDE techniques, such as those based on robust kernel regression and median-of-mean
estimators. This facilitates the creation of substantially more robust self-attention mechanisms. The
resulting suite of robust self-attention can be adapted to a variety of transformer architectures and
tasks across different data modalities. We carry out exhaustive experiments covering vision, lan-
guage modeling, and time-series classification. The results demonstrate that our approaches can
uphold comparable accuracy on clean data while exhibiting improved performance on contaminated
data. Crucially, this is accomplished without introducing any extra parameters."
INTRODUCTION,0.00980392156862745,"Related Work on Robust Transformers: Vision Transformer (ViT) models (Dosovitskiy et al.,
2020; Touvron et al., 2021b) have recently demonstrated impressive performance across various vi-
sion tasks, positioning themselves as a compelling alternative to CNNs. A number of studies (e.g.,
Subramanya et al., 2022; Paul & Chen, 2022; Bhojanapalli et al., 2021; Mahmood et al., 2021; Mao
et al., 2022; Zhou et al., 2022) have proposed strategies to bolster the resilience of these models
against common adversarial attacks on image data, thereby enhancing their generalizability across
diverse datasets. For instance, Mahmood et al. (2021) provided empirical evidence of ViT’s vulnera-
bility to white-box adversarial attacks, while demonstrating that a straightforward ensemble defense
could achieve remarkable robustness without compromising accuracy on clean data. Zhou et al.
(2022) suggested fully attentional networks to enhance self-attention, achieving state-of-the-art ac-
curacy on corrupted images. Furthermore, Mao et al. (2022) conducted a robustness analysis on
various ViT building blocks, proposing position-aware attention scaling and patch-wise augmenta-
tion to enhance the model’s robustness and accuracy. However, these investigations are primarily
geared toward vision-related tasks, which restricts their applicability across different data modali-
ties. As an example, the position-based attention from Mao et al. (2022) induces a bi-directional
information flow, which is limiting for position-sensitive datasets such as text or sequences. These
methods also introduce additional parameters. Beyond these vision-focused studies, robust trans-
formers have also been explored in fields like text analysis and social media. Yang et al. (2022)
delved into table understanding and suggested a robust, structurally aware table-text encoding archi-
tecture to mitigate the effects of row and column order perturbations. Liu et al. (2021a) proposed
a robust end-to-end transformer-based model for crisis detection and recognition. Furthermore, Li
et al. (2020) developed a unique attention mechanism to create a robust neural text-to-speech model
capable of synthesizing both natural and stable audios. We have noted that these methods vary in
their methodologies, largely due to differences in application domains, and therefore limiting their
generalizability across diverse contexts."
INTRODUCTION,0.012254901960784314,"Other Theoretical Frameworks for Attention Mechanisms: Attention mechanisms in transform-
ers have been recently studied from different perspectives. Tsai et al. (2019) show that attention can
be derived from smoothing the inputs with appropriate kernels. Katharopoulos et al. (2020); Choro-
manski et al. (2021); Wang et al. (2020) further linearize the softmax kernel in attention to attain
a family of efficient transformers with both linear computational and memory complexity. These
linear attentions are proven in Cao (2021) to be equivalent to a Petrov-Galerkin projection (Reddy,
2004), thereby indicating that the softmax normalization in dot-product attention is sufficient but not
necessary. Other frameworks for analyzing transformers that use ordinary/partial differential equa-
tions include Lu et al. (2019); Sander et al. (2022). In addition, the Gaussian mixture model and
graph-structured learning have been utilized to study attentions and transformers (Tang & Matteson,
2021; Gabbur et al., 2021; Zhang & Feng, 2021; Wang et al., 2018; Kreuzer et al., 2021). Nguyen
et al. (2022c) has linked the self-attention mechanism with a non-parametric regression perspective,
which offers enhanced interpretability of Transformers. Our approach draws upon this viewpoint,
but focuses instead on how it can lead to robust solutions."
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.014705882352941176,"2
Self-Attention Mechanism from a Non-parametric Regression Perspective"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.01715686274509804,"Assume we have the key and value vectors {kj, vj}j∈[N] that is collected from the data generating
process v = f(k) + ε, where ε is some noise vectors with E[ε] = 0, and f is the function that we
want to estimate. We consider a random design setting where the key vectors {kj}j∈[N] are i.i.d.
samples from the distribution p(k), and we use p(v, k) to denote the joint distribution of (v, k)
defined by the data generating process. Our target is to estimate f(q) for any new queries q. The
NW estimator provides a non-parametric approach to estimating the function f, the main idea is that"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.0196078431372549,"f(k) = E[v|k] =
Z"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.022058823529411766,"RD v · p(v|k)dv =
Z"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.024509803921568627,"RD
v · p(v, k)"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.02696078431372549,"p(k)
dv,
(1)"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.029411764705882353,"where the first equation comes from the fact that E[ε] = 0, the second equation comes from the
definition of conditional expectation, and the last equation comes from the definition of conditional
density. To provide an estimation of f, we just need to obtain estimations for both the joint density
function p(v, k) and the marginal density function p(k). KDE is commonly used for the density
estimation problem (Rosenblatt, 1956; Parzen, 1962), which requires a kernel kσ with the bandwidth
parameter σ satisfies
R"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.031862745098039214,"RD kσ(x −x′)dx = 1, ∀x′, and estimate the density as"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.03431372549019608,"ˆpσ(v, k) = 1 N X"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.03676470588235294,"j∈[N]
kσ ([v, k] −[vj, kj])
ˆpσ(k) = 1 N X"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.0392156862745098,"j∈[N]
kσ(k −kj),
(2)"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.041666666666666664,"where [v, k] denotes the concatenation of v and k. Specifically, when kσ is the isotropic Gaussian
kernel, we have ˆpσ(v, k) =
1
N
P"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.04411764705882353,"j∈[N] kσ(v −vj)kσ(k −kj). Combine this with Eq. (1) and Eq.
(2), we can obtain the NW estimator of the function f as"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.04656862745098039,bfσ(k) =
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.049019607843137254,"P
j∈[N] vjkσ(k −kj)
P"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.051470588235294115,"j∈[N] kσ(k −kj) .
(3)"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.05392156862745098,"Furthermore, it is not hard to show that if the keys {kj}j∈[N] are normalized, the self-
attention mechanism bfσ(qi) in Eq. (3) is exactly the standard Softmax attention bfσ(qi)
=
P"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.056372549019607844,"j∈[N] softmax
 
q⊤kj/σ2
vj. Such an assumption on the normalized key {kj}j∈[N] can be mild,
as in practice we always have a normalization step on the key to stabilizing the training of the trans-
former (Schlag et al., 2021). If we choose σ2 =
√"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.058823529411764705,"D, where D is the dimension of q and kj, then
bfσ(qi) = hi. As a result, the self-attention mechanism in fact performs a non-parametric regression
with NW-estimator and isotropic Gaussian kernel when the keys are normalized."
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.061274509803921566,"Figure 1: The contour plots illustrate the den-
sity estimation of the two-dimensional query
vector embedding within a transformer’s atten-
tion layer.
The left plot employs the regular
KDE method, as defined in Eq. (4), whereas the
right plot utilizes a robustified version of the
KDE method, which enhances KDE’s robust-
ness against outliers."
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.06372549019607843,"KDE as a Regression Problem in RKHS
We
start with the formal definition of the RKHS. The
space Hk = {f | f : X →R} is the RKHS
associated with kernel k, where k : X × X →R,
if it is a Hilbert space with inner product ⟨·, ·⟩Hk
and following properties:"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.0661764705882353,"• k(x, ·) ∈Hk, ∀x ∈X;"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.06862745098039216,"• ∀f ∈Hk, f(x) = ⟨f, k(x, ·)⟩Hk. Aka
the reproducing property."
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.07107843137254902,"With slightly abuse of notation,
we define
kσ(x, x′) = kσ(x −x′).
By the definition
of the RKHS and the KDE estimator, we know
ˆpσ =
1
N
P"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.07352941176470588,"j∈[N] kσ(xj, ·) ∈Hkσ, and can be
viewed as the optimal solution of the following
least-square regression problem in RKHS:"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.07598039215686274,"ˆpσ = arg min
p∈Hkσ X j∈[N]"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.0784313725490196,"1
N ∥kσ(xj, ·) −p∥2
Hkσ .
(4) … … …"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.08088235294117647,Clean Data
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.08333333333333333,PGD Attack
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.0857843137254902,Brightness
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.08823529411764706,"Image Patches
Attention Weight Factor"
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.09068627450980392,"Later that year, she auctioned a breakfast in Mayfair, 
London, where she raised around £4000 for the Pratham 
NGO, which helps children's primary education."
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.09313725490196079,"Later that year, she auctioned a breakfast in Mayfair, 
London, where she AAA around £4000 for the Pratham 
AAA, which helps children's primary education."
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.09558823529411764,"Figure 2: The application of Transformers with robust KDE attention on image and text is shown.
(Left) The robust KDE self-attention generates varying weight factors for image patch embeddings
under adversarial attacks or data corruption. The adversely impacted regions that would other-
wise lead to incorrect predictions are down-weighted, ensuring enhanced accuracy and robustness.
(Right) In the field of language modeling, the weight factors lend significance to essential keywords
(highlighted in red). In the face of word swap attacks, the fortified self-attention mechanism, par-
ticularly when utilizing the medians-of-means principle, is proficient in disregarding or reducing
the importance of less consequential words (marked in green). Consequently, this results in a more
resilient procedure during self-attention computations."
SELF-ATTENTION MECHANISM FROM A NON-PARAMETRIC REGRESSION PERSPECTIVE,0.09803921568627451,"Note that, in Eq. (4), the same weight factor 1/N is applied uniformly to each error term
∥kσ(xj, ·) −p∥2
Hkσ .
This approach functions effectively if there are no outliers in the set
{kσ(xj, ·)}j∈[N]. However, when outliers are present (for instance, when there is some j such
that ∥kσ(xj, ·)∥Hkσ ≫∥kσ(xi, ·)∥Hkσ , ∀i ∈[N], i ̸= j), the error attributable to these outliers will
overwhelmingly influence the total error, leading to a significant deterioration in the overall density
estimation. We illustrate the robustness issue of the KDE in Figure 1. The view that KDE is sus-
ceptible to outliers, coupled with the non-parametric understanding of the self-attention mechanism,
implies a potential lack of robustness in Transformers when handling outlier-rich data. We now of-
fer a fresh perspective on this robustness issue, introducing a universal framework that is applicable
across diverse data modalities."
ROBUST TRANSFORMERS THAT EMPLOY ROBUST KERNEL DENSITY ESTIMATORS,0.10049019607843138,"3
Robust Transformers that Employ Robust Kernel Density Estimators"
ROBUST TRANSFORMERS THAT EMPLOY ROBUST KERNEL DENSITY ESTIMATORS,0.10294117647058823,"Drawing on the non-parametric regression formulation of self-attention, we derive multiple robust
variants of the NW-estimator and demonstrate their applicability in fortifying existing Transformers.
We propose two distinct types of robust self-attention mechanisms and delve into the properties of
each, potentially paving the way for Transformer variants that are substantially more robust."
DOWN-WEIGHTING OUTLIERS IN RKHS,0.1053921568627451,"3.1
Down-weighting Outliers in RKHS"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.10784313725490197,"Inspired by robust regression (Fox & Weisberg, 2002), a direct approach to achieving robust KDE
involves down-weighting outliers in the RKHS. More specifically, we substitute the least-square loss
in Eq. (4) with a robust loss function ρ, resulting in the following formulation:"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.11029411764705882,"ˆprobust = arg min
p∈Hkσ X"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.11274509803921569,"j∈[N]
ρ
 
∥kσ(xj, ·) −p∥Hkσ

=
X"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.11519607843137254,"j∈[N]
ωjkσ(xj, ·).
(5)"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.11764705882352941,"Examples of the robust loss function ρ include the Huber loss (Huber, 1992), Hampel loss (Hampel
et al., 1986), Welsch loss (Welsch & Becker, 1975) and Tukey loss (Fox & Weisberg, 2002). We
empirically evaluate different loss functions in our experiments. The critical step here is to estimate
the set of weights ω = (ω1, · · · , ωN) ∈∆N, with each ωj ∝ψ
 
∥kσ(xj, ·) −ˆprobust∥Hkσ

, where"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.12009803921568628,ψ(x) := ρ′(x)
DOWN-WEIGHTING OUTLIERS IN RKHS,0.12254901960784313,"x . Since ˆprobust is defined via ω, and ω also depends on ˆprobust, one can address this
circular definition problem via an alternative updating algorithm proposed by Kim & Scott (2012).
The algorithm starts with randomly initialized ω(0) ∈∆n, and performs alternative updates between
ˆprobust and ω until the optimal ˆprobust is reached at the fixed point (see details in Appendix A)."
DOWN-WEIGHTING OUTLIERS IN RKHS,0.125,"However, while this technique effectively diminishes the influence of outliers, it also comes with
noticeable drawbacks. Firstly, it necessitates the appropriate selection of the robust loss function,
which may entail additional effort to understand the patterns of outliers. Secondly, the iterative"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.12745098039215685,Algorithm 1 Procedure of Computing Attention Vector of Transformer-RKDE/SPKDE/MoM
DOWN-WEIGHTING OUTLIERS IN RKHS,0.12990196078431374,"1: Input: Q = {qi}i∈[N], K = {kj}j∈[N], V = {vl}l∈[N], initial weights ω(0)"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.1323529411764706,"2: Normalize K = {kj}j∈[N] along the head dimension.
3: Compute kernel function between each pair of sequence: kσ(Q, K) = {kσ(qi −kj)}i,j∈[N].
4: (Optional) apply attention mask on kσ(Q, K).
5: [MoM] Randomly sample B subsets I1, . . . , IB of size S, obtain the median block Il such that
1
S
P"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.13480392156862744,"j∈Il kσ(qi −kj) = median{ 1 S
P"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.13725490196078433,"j∈I1 kσ(qi −kj), . . . , 1 S
P"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.13970588235294118,j∈IB kσ(qi −kj)}
DOWN-WEIGHTING OUTLIERS IN RKHS,0.14215686274509803,"6: [RKDE] Update weights ω(0) for marginal/joint density by ω(1)
j
= ψ"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.14460784313725492,"kσ(kj,·)−ˆp(k)
robust(k)

Hkσ !"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.14705882352941177,"P
j∈[N] ψ"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.14950980392156862,"kσ(kj,·)−ˆp(k)
robust(k)

Hkσ !."
DOWN-WEIGHTING OUTLIERS IN RKHS,0.15196078431372548,7: [SPKDE] Obtain optimal weights ω for marginal/joint density via solving equation (7).
DOWN-WEIGHTING OUTLIERS IN RKHS,0.15441176470588236,"8: [RKDE, SPKDE] Obtain robust self-attention vector
bhi ="
DOWN-WEIGHTING OUTLIERS IN RKHS,0.1568627450980392,"P
j∈[N] vjωjoint
j
kσ(qi−kj)
P
j∈[N] ωmarginal
j
kσ(qi−kj)."
DOWN-WEIGHTING OUTLIERS IN RKHS,0.15931372549019607,9: [MoM] Obtain attention vector bhi =
DOWN-WEIGHTING OUTLIERS IN RKHS,0.16176470588235295,"P
j∈Il vjkσ(qi−kj)
P
j∈Il kσ(qi−kj) ."
DOWN-WEIGHTING OUTLIERS IN RKHS,0.1642156862745098,"updates might not successfully converge to the optimal solution. A better alternative is to assign
higher weights to high-density regions and reduce the weights for atypical samples. The original
KDE is scaled and projected to its nearest weighted KDE according to the L2 norm. Similar concepts
have been studied by Scaled and Projected KDE (SPKDE) (Vandermeulen & Scott, 2014), which
offer an improved set of weights that better defend against outliers in the RKHS space. Specifically,
given the scaling factor β > 1, and let CN
σ be the convex hull of kσ(x1, ·), . . . , kσ(xN, ·) ∈Hkσ,
i.e., the space of weighted KDEs, the optimal density ˆprobust is given by"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.16666666666666666,"ˆprobust = arg min
p∈CN
σ  β
N X"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.16911764705882354,"j∈[N]
kσ(xj, ·) −p  2 Hkσ ,
(6)"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.1715686274509804,"which is guaranteed to have a unique minimizer since we are projecting in a Hilbert space and
CN
σ is closed and convex. Notice that, by definition, ˆprobust can also be represented as ˆprobust =
P"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.17401960784313725,"j∈[N] ωjkσ(xj, ·), ω ∈∆N, which is same as the formulation in Eq. (5). Then Eq. (6) can be
written as a quadratic programming (QP) problem over ω:"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.17647058823529413,"min
ω
ω⊤Gω −2q⊤ω,
subject to ω ∈∆N,
(7)"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.17892156862745098,where G is the Gram matrix of {xj}j∈[N] with kσ and q = G1 β
DOWN-WEIGHTING OUTLIERS IN RKHS,0.18137254901960784,"N . Since kσ is a positive-definite
kernel and each xi is unique, the Gram matrix G is also positive-definite. As a result, this QP
problem is convex, and we can leverage commonly used solvers to efficiently obtain the solution
and the optimal density ˆprobust."
DOWN-WEIGHTING OUTLIERS IN RKHS,0.18382352941176472,"Robust Self-Attention Mechanism
We now introduce the robust self-attention mechanism that
down-weights atypical samples. We consider the density estimator of the joint distribution and the
marginal distribution when using isotropic Gaussian kernel:"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.18627450980392157,"ˆprobust(v, k) =
X"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.18872549019607843,"j∈[N]
ωjoint
j
kσ([vj, kj], [v, k]),
ˆprobust(k) =
X"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.19117647058823528,"j∈[N]
ωmarginal
j
kσ(kj, k).
(8)"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.19362745098039216,"Following the non-parametric regression formulation of self-attention in Eq. (3), we obtain the
robust self-attention mechanism as bhi = P"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.19607843137254902,"j∈[N] vjωjoint
j
kσ(qi −kj)
P"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.19852941176470587,"j∈[N] ωmarginal
j
kσ(qi −kj)
,
(9)"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.20098039215686275,"where ωjoint and ωmarginal are obtained via either alternative updates or the QP solver. We term Trans-
formers whose density from the non-parametric regression formulation of self-attention employs
Eq. (5) and Eq. (6) as Transformer-RKDE and Transformer-SPKDE, respectively. Figure 2 presents
an example of the application of the attention weight factor during the learning process from image
and text data. The derived weight factor can potentially emphasize elements relevant to the class
while reducing the influence of detrimental ones. Note that, the computation of {ωmarginal
j
}j∈[N]"
DOWN-WEIGHTING OUTLIERS IN RKHS,0.2034313725490196,"and {ωjoint
j
}j∈[N] are separate as ωjoint
j
involves both keys and values vectors. During the empirical
evaluation, we concatenate the keys and values along the head dimension to obtain the weights for
the joint density ˆprobust(v, k) and only use the key vectors for obtaining the set of weights for the
marginal ˆprobust(k). In addition, ωmarginal, ωjoint ∈Rj×i for i, j = 1, . . . , N are 2-dimensional ma-
trices that include the pairwise weights between each position of the sequence and the rest of the
positions. The weights are initialized uniformly across a certain sequence length dimension. For
experiments related to language modeling, we can leverage information from the attention mask to
initialize the weights on the unmasked part of the sequence."
DOWN-WEIGHTING OUTLIERS IN RKHS,0.20588235294117646,"Limitations
While constructing attention weight factors proves effective, they necessitate itera-
tive algorithms to calculate the set of weights when computing self-attention at each layer, resulting
in increased overall complexity. Moreover, the foundational contamination model for these meth-
ods is the classical Huber contamination model (Huber, 2011), which requires assumptions about
contamination distributions and parameters that may not be universally applicable, especially in dis-
crete settings. To address these limitations, we propose a novel approach that circumvents these
computational constraints while effectively fostering robust self-attention mechanisms."
ROBUST SELF-ATTENTION VIA MEDIAN-OF-MEANS PRINCIPLE,0.20833333333333334,"3.2
Robust Self-Attention via Median-of-Means Principle"
ROBUST SELF-ATTENTION VIA MEDIAN-OF-MEANS PRINCIPLE,0.2107843137254902,"The Median-of-Means (MoM) principle (Jerrum et al., 1986; Alon et al., 1996) is one other way
to construct robust estimators. Rather than taking the average of all the observations, the sample
is split into several blocks over which the median is computed. The MoM principle also improved
the robustness of KDE and demonstrated its statistical performance under a less restrictive outlier
framework (Humbert et al., 2022). More importantly, it can be easily adapted to self-attention.
Specifically, we randomly divide the keys {kj}N
j=1 into B subsets I1, . . . , IB of equal size, namely,
|I1| = |I2| = . . . = |IB| = S. Then, the robust estimator of p(k) takes the following form:
ˆprobust(k) ∝median {ˆpσ,I1(k), . . . , ˆpσ,IB(k)} ,
(10)
where we define ˆpσ,Il(k) = 1 S
P"
ROBUST SELF-ATTENTION VIA MEDIAN-OF-MEANS PRINCIPLE,0.21323529411764705,"j∈Il kσ(k −kj) for any l ∈[B]. Similarly, the robust estimator of
p(v, k) is as follows:
ˆprobust(v, k) ∝median {ˆpσ,I1(v, k), . . . , ˆpσ,IB(v, k)} ,
(11)
where ˆpσ,Il(v, k) =
1
S
P"
ROBUST SELF-ATTENTION VIA MEDIAN-OF-MEANS PRINCIPLE,0.21568627450980393,"j∈Il kσ(v −vj)kσ(k −kj) for any l ∈[B]. We now propose the self-
attention mechanism utilizing the median-of-means principle."
ROBUST SELF-ATTENTION VIA MEDIAN-OF-MEANS PRINCIPLE,0.2181372549019608,"Median-of-Means Self-Attention Mechanism
Given the robust estimators in Eq. (10) and (11),
we can consider the following robust estimation of the attention: bhi ="
"S
P",0.22058823529411764,"1
S
P"
"S
P",0.22303921568627452,j∈Il vjkσ(qi −kj)
"S
P",0.22549019607843138,"median {ˆpσ,I1(qi −k), . . . , ˆpσ,IB(qi −k)},
(12)"
"S
P",0.22794117647058823,"where Il is the block such that ˆpσ(qi −k) achieves its median value in equation (11). In this context,
the random subsets apply to input sequences rather than individual data points, distinguishing this
approach from stochastic batches. It’s worth noting that our proposed attention mechanism assumes
that key and query vectors achieve their median on the same block. Consequently, we apply the me-
dian block Il obtained from the denominator into the numerator, rather than considering the median
over all potential blocks, which leads to a process that is faster than computing median blocks on
both sides. Moreover, the original MoM principle mandates that each subset be non-overlapping,
i.e. Il1 ∩Il2 = ∅for any 1 ≤l1 ̸= l2 ≤B. However, for structured, high-dimensional data,
dividing into non-overlapping blocks may result in the model only gaining a partial perspective of
the dataset, leading to sub-optimal performance. Therefore, we construct each subset by sampling
with replacement from the original dataset, maintaining the sequential relationship thereafter. In
particular, we have found this strategy to be effective in discrete contexts, such as identifying and
filtering out aberrant words in a sentence. As illustrated in the bottom right segment of Figure 2,
under a word swap attack, the MoM robust attention retains the subsequence that is most relevant to
the content, while discarding unhelpful parts. The downside of MoM self-attention is also apparent:
since the attention mechanism only accesses a portion of the sequence, it is likely to result in subop-
timal performance on clean datasets. The simplicity of MoM allows for easy integration with many
existing models. We initially demonstrate that the MoM self-attention mechanism can enhance the
recent state-of-the-art FourierFormer (Nguyen et al., 2022c). The theoretical explanation for the
ability to remove outliers using MoM-Fourier attention can be found in Appendix C."
"S
P",0.23039215686274508,"Table 1:
Perplexity (PPL) and negative likelihood loss (NLL) of our methods (lower part) and baselines
(upper part) on WikiText-103. The best results are highlighted in bold font and the second best are highlighted
in underline. On clean data, Transformer-SPKDE achieves better PPL and NLL than other baselines. Under
random swap with outlier words., Transformers with MoM self-attention show much better performance."
"S
P",0.23284313725490197,"Method (small version)
Clean Data
Word Swap
Valid PPL/Loss
Test PPL/Loss
Valid PPL/Loss
Test PPL/Loss
Transformer (Vaswani et al., 2017b)
33.15/3.51
34.29/3.54
72.28/4.45
74.56/4.53
Performer (Choromanski et al., 2021)
32.35/3.48
33.49/3.51
71.64/4.42
73.48/4.49
Transformer-MGK (Nguyen et al., 2022b)
32.28/3.47
33.21/3.51
69.78/4.38
71.03/4.41
FourierFormer (Nguyen et al., 2022c)
31.86/3.44
32.85/3.49
65.76/4.32
68.33/4.36
Transformer-RKDE (Huber)
31.22/3.42
32.29/3.47
52.14/3.92
55.68/3.99
Transformer-RKDE (Hampel)
31.24/3.42
32.35/3.48
55.61/3.98
57.92/4.03
Transformer-SPKDE
31.05/3.41
32.18/3.46
51.36/3.89
54.97/3.96
Transformer-MoM
33.56/3.52
34.68/3.55
48.29/3.82
52.14/3.92
FourierFormer-MoM
32.26/3.47
33.14/3.50
47.66/3.81
50.96/3.85"
INCORPORATING ROBUST SELF-ATTENTION MECHANISMS INTO TRANSFORMERS,0.23529411764705882,"3.3
Incorporating Robust Self-Attention Mechanisms into Transformers"
INCORPORATING ROBUST SELF-ATTENTION MECHANISMS INTO TRANSFORMERS,0.23774509803921567,"Computational Efficiency
The two types of robust attention mechanisms proposed above have
their own distinct advantages. To expedite the computation for Transformer-RKDE and obtain the at-
tention weight factor in a more efficient manner, we employ a single-step iteration on the alternative
updates to approximate the optimal set of weights. Empirical results indicate that this one-step itera-
tion can produce sufficiently precise results. For Transformer-SPKDE, as the optimal set of weights
is acquired via the QP solver, it demands more computation time but yields superior performance
on both clean and contaminated data. As an alternative to weight-based methods, Transformer-
MoM offers significantly greater efficiency while providing competitive performance, particularly
with text data. The complete procedure for computing the attention vector for Transformer-RKDE,
Transformer-SPKDE, and Transformer-MoM is detailed in Algorithm 1."
INCORPORATING ROBUST SELF-ATTENTION MECHANISMS INTO TRANSFORMERS,0.24019607843137256,"Training and Inference
We incorporate our robust attention mechanisms into both training and
inference stages of Transformers. Given the uncertainty about data cleanliness, there’s a possibility
of encountering contaminated samples at either stage. Therefore, it is worthwhile to defend against
outliers throughout the entire process. In the training context, our methods modify the computation
of attention vectors across each Transformer layer, making them less susceptible to contamina-
tion from outliers. This entire process remains nonparametric, with no introduction of additional
model parameters. However, the resulting attention vectors, whether shaped by re-weighting or the
median-of-means principle, diverge from those generated by the standard softmax attention. This
distinction influences the model parameters learned during training. During inference, the test data
undergoes a similar procedure to yield robust attention vectors. This ensures protection against po-
tential outlier-induced disruptions within the test sequence. However, if we assume the availability
of a clean training set — where contamination arises solely from distribution shifts, adversarial at-
tacks, or data poisoning during inference — it is sufficient to restrict the application of the robust
attention mechanism to just the inference phase. This could considerably reduce the computational
time required for robust attention vector calculation during training. In our empirical evaluation, we
engaged the robust attention mechanism throughout both phases and recorded the associated compu-
tational time during training. We found that on standard datasets like ImageNet-1K, WikiText-103,
and the UEA time-series classification, infusing the robust attention led to a drop in training loss.
This suggests that training data itself may contain inherent noise or outliers."
EXPERIMENTAL RESULTS,0.2426470588235294,"4
Experimental Results"
EXPERIMENTAL RESULTS,0.24509803921568626,"In this section, we provide empirical validation of the benefits of integrating our proposed robust
KDE attention mechanisms (Transformer-RKDE/SPKDE/MoM) into Transformer base models. We
compare these with the standard softmax Transformer across multiple datasets representing differ-
ent modalities. These include language modeling on the WikiText-103 dataset (Merity et al., 2016)
(Section 4.1) and image classification on ImageNet (Russakovsky et al., 2015; Deng et al., 2009).
Furthermore, we assess performance across multiple robustness benchmarks, namely ImageNet-C
(Hendrycks & Dietterich, 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-O (Hendrycks
et al., 2021b), ImageNet-R (Hendrycks et al., 2021a), and ImageNet-Sketch (Wang et al., 2019)
(Section 4.2), as well as UEA time-series classification (Section 4.3). Our proposed robust trans-"
EXPERIMENTAL RESULTS,0.24754901960784315,"Table 2: Top-1 and top-5 accuracy (%) on ImageNet. The best results are highlighted in bold font and the
second best are highlighted in underlines. RVT (Mao et al., 2022) and DeiT (Touvron et al., 2021b) achieve
better results on clean data; meanwhile, Transformers incorporating robust self-attention hold stronger defense
under different adversarial attacks while still achieving competitive performance on the original ImageNet."
EXPERIMENTAL RESULTS,0.25,"Method
Clean Data
FGSM
PGD
SPSA
Top 1
Top 5
Top 1
Top 5
Top 1
Top 5
Top 1
Top 5
ViT (Dosovitskiy et al., 2020)
72.23
91.13
52.61
82.26
41.84
76.49
48.34
79.36
DeiT (Touvron et al., 2021b)
74.32
93.72
53.24
84.07
41.72
76.43
49.56
80.14
RVT (Mao et al., 2022)
74.37
93.89
53.67
84.11
43.39
77.26
51.43
80.98
FourierFormer (Nguyen et al., 2022c)
73.25
91.66
53.08
83.95
41.34
76.19
48.79
79.57
ViT-RKDE (Huber)
72.83
91.44
55.83
85.89
44.15
79.06
52.42
82.03
ViT-RKDE (Hampel)
72.94
91.63
55.92
85.97
44.23
79.16
52.48
82.07
ViT-SPKDE
73.22
91.95
56.03
86.12
44.51
79.47
52.64
82.33
ViT-MoM
71.94
91.08
55.76
85.23
43.78
78.85
49.38
80.02
FourierFormer-MoM
72.58
91.34
53.25
84.12
41.38
76.41
48.82
79.68"
EXPERIMENTAL RESULTS,0.25245098039215685,"formers are compared with state-of-the-art models, including Performer (Choromanski et al., 2021),
MGK (Nguyen et al., 2022a), RVT (Mao et al., 2022), and FourierFormer (Nguyen et al., 2022c).
All experiments were conducted on machines with 4 NVIDIA A-100 GPUs. For each experiment,
Transformer-RKDE/SPKDE/MoM were compared with other baselines under identical hyperpa-
rameter configurations."
ROBUST LANGUAGE MODELING,0.2549019607843137,"4.1
Robust Language Modeling"
ROBUST LANGUAGE MODELING,0.25735294117647056,"WikiText-103 is a language modeling dataset that contains collection of tokens extracted from good
and featured articles from Wikipedia, which is suitable for models that can leverage long-term de-
pendencies. We follow the standard configurations in Merity et al. (2016); Schlag et al. (2021) and
splits the training data into L-word independent long segments. During evaluation, we process the
text sequence using a sliding window of size L and feed into the model with a batch size of 1. The
last position of the sliding window is used for computing perplexity except in the first segment,
where all positions are evaluated as in Al-Rfou et al. (2019); Schlag et al. (2021)."
ROBUST LANGUAGE MODELING,0.25980392156862747,"In our experiments, we utilized the small and medium (shown in Appendix E) language models
developed by Schlag et al. (2021). We configured the dimensions of key, value, and query to 128,
and set the training and evaluation context length to 256. We contrasted our methods with Performer
(Choromanski et al., 2021), Transformer-MGK (Nguyen et al., 2022a) and FourierFormer (Nguyen
et al., 2022c), which have demonstrated competitive performance. For self-attention, we allocated
8 heads for our methods and Performer, and 4 for Transformer-MGK. The dimension of the feed-
forward layer was set to 2048, with the number of layers established at 16. To prevent numerical
instability, we used the log-sum-exp trick in equation (3) when calculating the attention probability
vector through the Gaussian kernel. We employed similar tactics when computing the attention
weight factor of Transformer-RKDE, initially obtaining the weights in log space, followed by the
log-sum-exp trick to compute robust self-attention as outlined in equation (9). For Transformer-
MoM, the sampled subset sequences constituted 80% of the length of the original sequence."
ROBUST LANGUAGE MODELING,0.2622549019607843,"Table 1 presents the validation and test perplexity (PPL) for several methods. Both Transformer-
RKDE and SPKDE models exhibit an improvement over baseline PPL and NLL on both validation
and test sets. However, MoM-based models display slightly higher perplexity, a result of using only
a portion of the sequence. When the dataset is subjected to a word swap attack, which randomly sub-
stitutes selected keywords with a generic token “AAA” during evaluation, our method, particularly
MoM-based robust attention, yields significantly better results. This is particularly evident when
filtering out infrequent words, where the median trick has proven its effectiveness. We also noticed
superior robustness in RKDE/SPKDE-based robust attention compared to other baseline methods
that were not protected from the attack. Our implementation of the word swap is based on the pub-
licly available TextAttack code by Morris et al. (2020)1. We employed a greedy search method with
constraints on stop-word modifications provided by the TextAttack library."
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.2647058823529412,"4.2
Image Classification under Adversarial Attacks and Data Corruptions"
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.26715686274509803,"For initial simplicity, we utilize the original ViT (tiny) as our base model. However, our methods are
versatile and can be integrated with more advanced base models to enhance their robustness. As our"
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.2696078431372549,1Implementation available at github.com/QData/TextAttack
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.27205882352941174,"Table 3: We evaluated the performance of proposed models across multiple robustness benchmarks, using
appropriate evaluation metrics for each. In the majority of cases, our methods outperformed the baselines."
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.27450980392156865,"Dataset
ImageNet-C
ImageNet-A
ImageNet-O
ImageNet-R
ImageNet-Sketch
Metric
mCE↓
Top-1 Acc↑
AUPR↑
Top-1 Err Rate↓
Top-1 Acc↑
ViT
71.14
0.18
18.31
96.89
37.13
DeiT
70.26
0.73
19.56
94.23
41.68
RVT
68.57
9.45
22.14
63.12
50.07
FourierFormer
71.07
0.69
18.47
94.15
38.36
ViT-RKDE (Huber)
68.69
6.98
25.61
64.33
45.63
ViT-RKDE (Hampel)
68.55
6.34
26.14
62.16
45.76
ViT-SPKDE
68.34
8.29
28.42
61.39
50.13
ViT-MoM
69.11
2.29
28.08
76.44
36.92
FourierFormer-MoM
70.62
2.42
28.86
74.15
39.44"
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.2769607843137255,"1
2
3
4
5
6
Purturbation Budget 10 20 30 40 50 60"
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.27941176470588236,Top-1 Accuracy FGSM
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.2818627450980392,"ViT
DeiT
Fourier
RVT
F-MoM
RKDE
SPKDE
ViT-MoM (a)"
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.28431372549019607,"1
2
3
4
5
6
Purturbation Budget 0 10 20 30 40"
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.2867647058823529,Top-1 Accuracy PGD (b)
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.28921568627450983,"1
2
3
4
5
6
Purturbation Budget 20 30 40 50"
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.2916666666666667,Top-1 Accuracy SPSA (c)
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.29411764705882354,"Figure 3: The top-1 classification accuracy v.s. perturbation budget × 255 curves on ImageNet against three
untargeted attack methods under the l∞norm. The proposed set of ViT with robust self-attention mechanisms
shows stronger defense under all attack methods with different perturbation budgets."
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.2965686274509804,"approaches do not alter the model architecture, each model employs 5.7M parameters. We have also
implemented leading-edge methods, including DeiT with hard distillation (Touvron et al., 2021b),
FourierFormer (Nguyen et al., 2022c), and Robust Vision Transformer (RVT) (Mao et al., 2022), as
our baselines. It’s important to note that, for a fair comparison with RVT, we only incorporated its
position-aware attention scaling without further architectural modifications. Consequently, the re-
sulting RVT model comprises approximately 7.2M parameters. To evaluate adversarial robustness,
we utilized adversarial examples generated by untargeted white-box attacks, which included the
single-step attack method FGSM (Goodfellow et al., 2014), multi-step attack method PGD (Madry
et al., 2017), and score-based black-box attack method SPSA (Uesato et al., 2018). These attacks
were applied to the entire validation set of ImageNet. Each attack distorts the input image with a
perturbation budget ϵ = 1/255 under l∞norm, while the PGD attack uses 20 steps with a step
size of α = 0.15. In addition, we assessed our methods on multiple robustness benchmarks, which
include images derived from the original ImageNet through algorithmic corruptions or outliers."
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.29901960784313725,"Figure 4: Comparison of averaged computation speed (measured
by iteration per second) and maximum GPU memory (measured
by the CUDA max memory allocated function) on ImageNet
classification task. The results are measured under the Trans-
former base models with different capacities: Tiny (5.7M param-
eters), Small (22M), and Base (86M)."
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.3014705882352941,"Table 2 presents the results under ad-
versarial attack. On clean ImageNet,
our performance aligns closely with
RVT and DeiT, the leading perform-
ers. Notably, our methods outperform
RVT under several adversarial attack
types, particularly when employing
the ViT-SPKDE method. Figure 3 il-
lustrates the relationship between ac-
curacy and perturbation budget across
three attack methods.
We observe
that transformers equipped with robust
self-attention mechanisms offer sig-
nificantly enhanced defense capabili-
ties across different perturbation bud-
gets, with their advantages amplifying
as the level of perturbation increases,
as expected. We provide more ablation
studies in Appendix E that explore dif-"
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.30392156862745096,"ferent design choices for each proposed robust KDE attention. Table 3 displays the results across
multiple robustness benchmarks, employing appropriate evaluation metrics for each. In most in-
stances, the best performance is achieved by our proposed methods, which clearly improve upon
existing baselines. Additionally, Figure 4 demonstrates the scalability of our methods as model size
increases. We have excluded the SPKDE-based method from this analysis due to its heightened
computational demands. The results indicate that both the computation speed and memory increase
as model capacity expands for all methods. The MoM-based robust attention closely mirrors the
vanilla Transformer, while the RKDE-based robust attention, with one-step approximation, also
demonstrates scalability with larger models."
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.30637254901960786,"Table 4: A comparison of classification accuracy using the UEA Time Series Classification Archive. Trans-
formers that incorporate our proposed robust attention mechanisms outperform the existing baselines."
IMAGE CLASSIFICATION UNDER ADVERSARIAL ATTACKS AND DATA CORRUPTIONS,0.3088235294117647,"Model/Dataset
Ethanol
Heart
PEMS-SF
Spoken
UWave
Transformer
33.70
75.77
82.66
99.33
84.45
FourierFormer
36.12
76.42
86.70
99.00
86.66
Transformer-RKDE (Huber)
34.72
75.84
84.28
99.28
86.49
Transformer-SPKDE
36.09
76.29
86.02
99.36
88.14
Transformer-MoM
38.41
73.24
86.75
97.64
82.97
FourierFormer-MoM
39.89
74.11
87.63
98.12
85.43"
UEA TIME SERIES CLASSIFICATION,0.3112745098039216,"4.3
UEA Time Series Classification"
UEA TIME SERIES CLASSIFICATION,0.3137254901960784,"Lastly, we conducted experiments using five datasets from the UEA Time-Series Classification
Archive (Bagnall et al., 2018), and compared the outcomes across various methodologies (Table
4). The baseline implementation and datasets were adapted from Wu et al. (2022). Our findings
indicate that our proposed approaches can notably enhance classification accuracy."
CONCLUSION AND FUTURE WORK,0.3161764705882353,"5
Conclusion and Future Work"
CONCLUSION AND FUTURE WORK,0.31862745098039214,"In this work, we explored the link between the dot-product self-attention mechanism and non-
parametric kernel regression. This led to the development of a family of fortified transformers,
which leverage robust KDE as an alternative to dot-product attention, mitigating the impacts of con-
taminated samples. We proposed two variants of robust self-attention mechanisms designed to either
down-weight or filter out potential corrupted data, both of which can be seamlessly integrated into
commonly used transformer models. As our ongoing effort, we are exploring more efficient tech-
niques for estimating the weight set for robust KDE in extremely large models, and incorporating
regularization strategies to mitigate the instability of kernel regression with outliers."
ACKNOWLEDGMENT,0.32107843137254904,"6
Acknowledgment"
ACKNOWLEDGMENT,0.3235294117647059,"Xing Han and Joydeep Ghosh acknowledge support from Intuit Inc. Nhat Ho acknowledges support
from the NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning."
REFERENCES,0.32598039215686275,References
REFERENCES,0.3284313725490196,"Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L. Character-level language modeling
with deeper self-attention.
In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 3159–3166, 2019."
REFERENCES,0.33088235294117646,"Alon, N., Matias, Y., and Szegedy, M. The space complexity of approximating the frequency mo-
ments. In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, pp.
20–29, 1996."
REFERENCES,0.3333333333333333,"Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. In Inter-
national Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=
ByxZX20qFQ."
REFERENCES,0.33578431372549017,"Bagnall, A., Dau, H. A., Lines, J., Flynn, M., Large, J., Bostrom, A., Southam, P., and Keogh, E.
The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075,
2018."
REFERENCES,0.3382352941176471,"Bhojanapalli, S., Chakrabarti, A., Glasner, D., Li, D., Unterthiner, T., and Veit, A. Understanding
robustness of transformers for image classification. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 10231–10241, 2021."
REFERENCES,0.34068627450980393,"Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,
P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural
information processing systems, 33:1877–1901, 2020."
REFERENCES,0.3431372549019608,"Cao, S. Choose a transformer: Fourier or galerkin. Advances in Neural Information Processing
Systems, 34, 2021."
REFERENCES,0.34558823529411764,"Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and
Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling. Advances in
neural information processing systems, 34:15084–15097, 2021."
REFERENCES,0.3480392156862745,"Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transform-
ers. arXiv preprint arXiv:1904.10509, 2019."
REFERENCES,0.35049019607843135,"Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P.,
Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking
attention with performers. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=Ua6zuk0WRH."
REFERENCES,0.35294117647058826,"Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R. Transformer-xl: Attentive
language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019."
REFERENCES,0.3553921568627451,"Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. Universal transformers. In In-
ternational Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=HyzdRiR9Y7."
REFERENCES,0.35784313725490197,"Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–
255. Ieee, 2009."
REFERENCES,0.3602941176470588,"Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
BERT: Pre-training of deep bidirec-
tional transformers for language understanding. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota,
June 2019. Association for Computational Linguistics.
doi: 10.18653/v1/N19-1423.
URL
https://aclanthology.org/N19-1423."
REFERENCES,0.3627450980392157,"Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,
M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."
REFERENCES,0.36519607843137253,"Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,
M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16
words: Transformers for image recognition at scale. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy."
REFERENCES,0.36764705882352944,"Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., and Feichtenhofer, C. Multiscale vision
transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
6824–6835, 2021."
REFERENCES,0.3700980392156863,"Fox, J. and Weisberg, S. Robust regression. An R and S-Plus companion to applied regression, 91,
2002."
REFERENCES,0.37254901960784315,"Gabbur, P., Bilkhu, M., and Movellan, J. Probabilistic attention for interactive segmentation. Ad-
vances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.375,"Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572, 2014."
REFERENCES,0.37745098039215685,"Hampel, F. R., Ronchetti, E. M., Rousseeuw, P., and Stahel, W. A. Robust statistics: the approach
based on influence functions. Wiley-Interscience; New York, 1986."
REFERENCES,0.3799019607843137,"Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions
and perturbations. arXiv preprint arXiv:1903.12261, 2019."
REFERENCES,0.38235294117647056,"Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli,
S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution gen-
eralization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
8340–8349, 2021a."
REFERENCES,0.38480392156862747,"Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
15262–15271, 2021b."
REFERENCES,0.3872549019607843,"Huber, P. J. Robust estimation of a location parameter. In Breakthroughs in statistics, pp. 492–518.
Springer, 1992."
REFERENCES,0.3897058823529412,"Huber, P. J. Robust statistics. In International encyclopedia of statistical science, pp. 1248–1251.
Springer, 2011."
REFERENCES,0.39215686274509803,"Humbert, P., Le Bars, B., and Minvielle, L. Robust kernel density estimation with median-of-means
principle. In International Conference on Machine Learning, pp. 9444–9465. PMLR, 2022."
REFERENCES,0.3946078431372549,"Janner, M., Li, Q., and Levine, S. Offline reinforcement learning as one big sequence modeling
problem. Advances in neural information processing systems, 34:1273–1286, 2021."
REFERENCES,0.39705882352941174,"Jerrum, M. R., Valiant, L. G., and Vazirani, V. V. Random generation of combinatorial structures
from a uniform distribution. Theoretical computer science, 43:169–188, 1986."
REFERENCES,0.39950980392156865,"Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive
transformers with linear attention. In International Conference on Machine Learning, pp. 5156–
5165. PMLR, 2020."
REFERENCES,0.4019607843137255,"Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., and Shah, M. Transformers in vision: A
survey. ACM Computing Surveys (CSUR), 2021."
REFERENCES,0.40441176470588236,"Kim, J. and Scott, C. Robust kernel density estimation. The Journal of Machine Learning Research,
13:2529–2565, 2012."
REFERENCES,0.4068627450980392,"Kreuzer, D., Beaini, D., Hamilton, W., L´etourneau, V., and Tossou, P. Rethinking graph transformers
with spectral attention. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.40931372549019607,"Li, N., Liu, Y., Wu, Y., Liu, S., Zhao, S., and Liu, M. Robutrans: A robust transformer-based text-
to-speech model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pp. 8228–8235, 2020."
REFERENCES,0.4117647058823529,"Lin, T., Wang, Y., Liu, X., and Qiu, X. A survey of transformers. arXiv preprint arXiv:2106.04554,
2021."
REFERENCES,0.41421568627450983,"Liu, J., Singhal, T., Blessing, L. T., Wood, K. L., and Lim, K. H. Crisisbert: a robust transformer for
crisis classification and contextual crisis embedding. In Proceedings of the 32nd ACM Conference
on Hypertext and Social Media, pp. 133–141, 2021a."
REFERENCES,0.4166666666666667,"Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L.,
and Stoyanov, V.
Roberta: A robustly optimized bert pretraining approach.
arXiv preprint
arXiv:1907.11692, 2019."
REFERENCES,0.41911764705882354,"Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierar-
chical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 10012–10022, 2021b."
REFERENCES,0.4215686274509804,"Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., and Hu, H. Video swin transformer. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2022."
REFERENCES,0.42401960784313725,"Lu, Y., Li, Z., He, D., Sun, Z., Dong, B., Qin, T., Wang, L., and Liu, T.-Y.
Understanding
and improving transformer from a multi-particle dynamic system point of view. arXiv preprint
arXiv:1906.02762, 2019."
REFERENCES,0.4264705882352941,"Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models
resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017."
REFERENCES,0.42892156862745096,"Mahmood, K., Mahmood, R., and Van Dijk, M. On the robustness of vision transformers to adver-
sarial examples. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 7838–7847, 2021."
REFERENCES,0.43137254901960786,"Mao, X., Qi, G., Chen, Y., Li, X., Duan, R., Ye, S., He, Y., and Xue, H.
Towards robust vi-
sion transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 12042–12051, 2022."
REFERENCES,0.4338235294117647,"Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint
arXiv:1609.07843, 2016."
REFERENCES,0.4362745098039216,"Morris, J. X., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D., and Qi, Y.
Textattack: A frame-
work for adversarial attacks, data augmentation, and adversarial training in nlp. arXiv preprint
arXiv:2005.05909, 2020."
REFERENCES,0.4387254901960784,"Nadaraya, E. A. On estimating regression. Theory of Probability & Its Applications, 9(1):141–142,
1964."
REFERENCES,0.4411764705882353,"Nguyen, T., Nguyen, T., Do, H., Nguyen, K., Saragadam, V., Pham, M., Nguyen, K., Ho, N., and
Osher, S. Improving transformer with an admixture of attention heads. In Advances in Neural
Information Processing Systems, 2022a."
REFERENCES,0.44362745098039214,"Nguyen, T., Nguyen, T., Le, D., Nguyen, K., Tran, A., Baraniuk, R., Ho, N., and Osher, S. Improving
transformers with probabilistic attention keys. In International Conference on Machine Learning,
2022b."
REFERENCES,0.44607843137254904,"Nguyen, T., Pham, M., Nguyen, T., Nguyen, K., Osher, S. J., and Ho, N. Fourierformer: Transformer
meets generalized Fourier integral theorem. Advances in Neural Information Processing Systems,
2022c."
REFERENCES,0.4485294117647059,"Parzen, E. On estimation of a probability density function and mode. The annals of mathematical
statistics, 33(3):1065–1076, 1962."
REFERENCES,0.45098039215686275,"Paul, S. and Chen, P.-Y.
Vision transformers are robust learners.
In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 36, pp. 2071–2081, 2022."
REFERENCES,0.4534313725490196,"Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by
generative pre-training. OpenAI report, 2018."
REFERENCES,0.45588235294117646,"Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are
unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.4583333333333333,"Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,
Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervi-
sion. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021."
REFERENCES,0.46078431372549017,"Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu,
P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of
Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/20-074.html."
REFERENCES,0.4632352941176471,"Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-
shot text-to-image generation. In International Conference on Machine Learning, pp. 8821–8831.
PMLR, 2021."
REFERENCES,0.46568627450980393,"Reddy, J. An introduction to the finite element method, volume 1221. McGraw-Hill New York,
2004."
REFERENCES,0.4681372549019608,"Rosenblatt, M. Remarks on some nonparametric estimates of a density function. The annals of
mathematical statistics, pp. 832–837, 1956."
REFERENCES,0.47058823529411764,"Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla,
A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal
of computer vision, 115(3):211–252, 2015."
REFERENCES,0.4730392156862745,"Sander, M. E., Ablin, P., Blondel, M., and Peyr´e, G. Sinkformers: Transformers with doubly stochas-
tic attention. In International Conference on Artificial Intelligence and Statistics, pp. 3515–3530.
PMLR, 2022."
REFERENCES,0.47549019607843135,"Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers.
In International Conference on Machine Learning, pp. 9355–9366. PMLR, 2021."
REFERENCES,0.47794117647058826,"Subramanya, A., Saha, A., Koohpayegani, S. A., Tejankar, A., and Pirsiavash, H. Backdoor attacks
on vision transformers. arXiv preprint arXiv:2206.08477, 2022."
REFERENCES,0.4803921568627451,"Tang, B. and Matteson, D. S. Probabilistic transformer for time series analysis. In Beygelzimer,
A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing
Systems, 2021. URL https://openreview.net/forum?id=HfpNVDg3ExA."
REFERENCES,0.48284313725490197,"Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. arXiv preprint
arXiv:2009.06732, 2020."
REFERENCES,0.4852941176470588,"Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J´egou, H. Training data-efficient
image transformers & distillation through attention. In International Conference on Machine
Learning, pp. 10347–10357. PMLR, 2021a."
REFERENCES,0.4877450980392157,"Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J´egou, H. Training data-efficient
image transformers & distillation through attention. In International Conference on Machine
Learning, pp. 10347–10357. PMLR, 2021b."
REFERENCES,0.49019607843137253,"Tsai, Y.-H. H., Bai, S., Yamada, M., Morency, L.-P., and Salakhutdinov, R.
Transformer dis-
section: An unified understanding for transformer’s attention via the lens of kernel.
In Pro-
ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.
4344–4353, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:
10.18653/v1/D19-1443. URL https://aclanthology.org/D19-1443."
REFERENCES,0.49264705882352944,"Uesato, J., O’donoghue, B., Kohli, P., and Oord, A. Adversarial risk and the dangers of evaluating
against weak attacks. In International Conference on Machine Learning, pp. 5025–5034. PMLR,
2018."
REFERENCES,0.4950980392156863,"Vandermeulen, R. A. and Scott, C. Robust kernel density estimation by scaling and projection in
hilbert space. Advances in Neural Information Processing Systems, 27, 2014."
REFERENCES,0.49754901960784315,"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polo-
sukhin, I. Attention is all you need. In Advances in neural information processing systems, pp.
5998–6008, 2017a."
REFERENCES,0.5,"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and
Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H.,
Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing
Systems, volume 30. Curran Associates, Inc., 2017b. URL https://proceedings.neurips.cc/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf."
REFERENCES,0.5024509803921569,"Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning robust global representations by penalizing
local predictive power. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.5049019607843137,"Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.5073529411764706,"Wang, X., Girshick, R., Gupta, A., and He, K. Non-local neural networks. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pp. 7794–7803, 2018."
REFERENCES,0.5098039215686274,"Welsch, R. E. and Becker, R. A. Robust non-linear regression using the dogleg algorithm. Technical
report, National Bureau of Economic Research, 1975."
REFERENCES,0.5122549019607843,"Wu, H., Wu, J., Xu, J., Wang, J., and Long, M. Flowformer: Linearizing transformers with conser-
vation flows. In International Conference on Machine Learning, 2022."
REFERENCES,0.5147058823529411,"Yang, J., Gupta, A., Upadhyay, S., He, L., Goel, R., and Paul, S. Tableformer: Robust transformer
modeling for table-text encoding. arXiv preprint arXiv:2203.00274, 2022."
REFERENCES,0.5171568627450981,"Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., and Le, Q. V. Xlnet: Generalized
autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019."
REFERENCES,0.5196078431372549,"Zhang, S. and Feng, Y.
Modeling concentrated cross-attention for neural machine translation
with Gaussian mixture model.
In Findings of the Association for Computational Linguis-
tics: EMNLP 2021, pp. 1401–1411, Punta Cana, Dominican Republic, November 2021. As-
sociation for Computational Linguistics.
doi: 10.18653/v1/2021.findings-emnlp.121.
URL
https://aclanthology.org/2021.findings-emnlp.121."
REFERENCES,0.5220588235294118,"Zhou, D., Yu, Z., Xie, E., Xiao, C., Anandkumar, A., Feng, J., and Alvarez, J. M. Understanding
the robustness in vision transformers. In International Conference on Machine Learning, pp.
27378–27394. PMLR, 2022."
REFERENCES,0.5245098039215687,"Supplementary Material of “Designing Robust Transformers
using Robust Kernel Density Estimation”"
REFERENCES,0.5269607843137255,"A
The Non-parametric Regression Perspective of Self-Attention"
REFERENCES,0.5294117647058824,"Given an input sequence X = [x1, . . . , xN]⊤∈RN×Dx of N feature vectors, the self-attention
mechanism transforms it into another sequence H := [h1, · · · , hN]⊤∈RN×Dv as follows:"
REFERENCES,0.5318627450980392,"hi =
X"
REFERENCES,0.5343137254901961,"j∈[N]
softmax
q⊤
i kj
√ D"
REFERENCES,0.5367647058823529,"
vj, for i = 1, . . . , N.
(13)"
REFERENCES,0.5392156862745098,"The vectors qi, kj and vj are the query, key and value vectors, respectively. They are computed as
follows:
[q1, q2, . . . , qN]⊤:= Q = XW ⊤
Q ∈RN×D,"
REFERENCES,0.5416666666666666,"[k1, k2, . . . , kN]⊤:= K = XW ⊤
K ∈RN×D,"
REFERENCES,0.5441176470588235,"[v1, v2, . . . , vN]⊤:= V = XW ⊤
V ∈RN×Dv, (14)"
REFERENCES,0.5465686274509803,"where WQ, WK ∈RD×Dx, WV ∈RDv×Dx are the weight matrices. Equation (13) can be written
in the following equivalent matrix form:"
REFERENCES,0.5490196078431373,"H = softmax
QK⊤ √ D"
REFERENCES,0.5514705882352942,"
V ,
(15)"
REFERENCES,0.553921568627451,"where the softmax function is applied to each row of the matrix (QK⊤)/
√"
REFERENCES,0.5563725490196079,"D. Equation (15) is
also called the “softmax attention”. Assume we have the key and value vectors {kj, vj}j∈[N] that
is collected from the data generating process
v = f(k) + ε,
(16)
where ε is some noise vectors with E[ε] = 0, and f is the function that we want to estimate. If
{kj}j∈[N] are i.i.d. samples from the distribution p(k), and p(v, k) is the joint distribution of (v, k)
defined by equation (16), we have"
REFERENCES,0.5588235294117647,"f(k) = E[v|k] =
Z"
REFERENCES,0.5612745098039216,"RD v · p(v|k)dv =
Z"
REFERENCES,0.5637254901960784,"RD
v · p(v, k)"
REFERENCES,0.5661764705882353,"p(k)
dv,
(17)"
REFERENCES,0.5686274509803921,"We need to obtain estimations for both the joint density function p(v, k) and the marginal density
function p(k) to obtain function f, one popular approach is the kernel density estimation:"
REFERENCES,0.571078431372549,"ˆpσ(v, k) = 1 N X"
REFERENCES,0.5735294117647058,"j∈[N]
kσ ([v, k] −[vj, kj])
(18)"
REFERENCES,0.5759803921568627,ˆpσ(k) = 1 N X
REFERENCES,0.5784313725490197,"j∈[N]
kσ(k −kj),
(19)"
REFERENCES,0.5808823529411765,"where [v, k] denotes the concatenation of v and k. kσ could be isotropic Gaussian kernel: kσ(x −
x′) = exp
 
−∥x −x′∥2/(2σ2)

, we have"
REFERENCES,0.5833333333333334,"ˆpσ(v, k) = 1 N X"
REFERENCES,0.5857843137254902,"j∈[N]
kσ(v −vj)kσ(k −kj).
(20)"
REFERENCES,0.5882352941176471,"Combining equations (19), (20), and (17), we obtain the NW estimator of the function f as"
REFERENCES,0.5906862745098039,"bfσ(k) =
Z"
REFERENCES,0.5931372549019608,"RD
v · ˆpσ(v, k)"
REFERENCES,0.5955882352941176,"ˆpσ(k)
dv
(21) =
Z RD v · P"
REFERENCES,0.5980392156862745,j∈[N] kσ(v −vj)kσ(k −kj) P
REFERENCES,0.6004901960784313,"j∈[N] kσ(k −kj)
dv = P"
REFERENCES,0.6029411764705882,"j∈[N] kσ(k −kj)
R
v · kσ(v −vj)dv
P"
REFERENCES,0.6053921568627451,j∈[N] kσ(k −kj) = P
REFERENCES,0.6078431372549019,"j∈[N] vjkσ(k −kj)
P"
REFERENCES,0.6102941176470589,"j∈[N] kσ(k −kj) .
(22)"
REFERENCES,0.6127450980392157,"Now we show how the self-attention mechanism is related to the NW estimator.
If the keys
{kj}j∈[N] are normalized"
REFERENCES,0.6151960784313726,bfσ(q) = P
REFERENCES,0.6176470588235294,"j∈[N] vj exp
 
−∥q −kj∥2/2σ2 P"
REFERENCES,0.6200980392156863,j∈[N] exp (−∥q −kj∥2/2σ2) = P
REFERENCES,0.6225490196078431,"j∈[N] vj exp

−
 
∥q∥2 + ∥kj∥2
/2σ2
exp
 
q⊤kj/σ2 P"
REFERENCES,0.625,"j∈[N] exp [−(∥q∥2 + ∥kj∥2) /2σ2] exp (q⊤kj/σ2) =
X j∈[N]"
REFERENCES,0.6274509803921569,"exp
 
q⊤kj/σ2
P
j∈[N] exp(q⊤kj/σ2)
vj =
X"
REFERENCES,0.6299019607843137,"j∈[N]
softmax
 
q⊤kj/σ2
vj.
(23)"
REFERENCES,0.6323529411764706,Then estimating the softmax attention is equivalent to estimating bfσ(q).
REFERENCES,0.6348039215686274,"B
Details on Leveraging Robust KDE on Transformers"
REFERENCES,0.6372549019607843,"For simplicity, we use the Huber loss function as the demonstrating example, which is defined as
follows:"
REFERENCES,0.6397058823529411,"ρ(x) :=

x2/2,
0 ≤x ≤a
ax −a2/2,
a < x,
(24)"
REFERENCES,0.6421568627450981,where a is a constant. The solution to this robust regression problem has the following form:
REFERENCES,0.6446078431372549,"Proposition 1. Assume the robust loss function ρ is non-decreasing in [0, ∞], ρ(0) = 0 and
limx→0
ρ(x)"
REFERENCES,0.6470588235294118,"x
= 0. Define ψ(x) := ρ′(x)"
REFERENCES,0.6495098039215687,"x
and assume ψ(0) = limx→0
ρ′(x)"
REFERENCES,0.6519607843137255,"x
exists and finite. Then
the optimal ˆprobust can be written as"
REFERENCES,0.6544117647058824,"ˆprobust =
X"
REFERENCES,0.6568627450980392,"j∈[N]
ωjkσ(xj, ·),"
REFERENCES,0.6593137254901961,"where ω = (ω1, · · · , ωN) ∈∆N, with each ωj ∝ψ
 
∥kσ(xj, ·) −ˆprobust∥Hkσ

. Here ∆n denotes
the n-dimensional probability simplex."
REFERENCES,0.6617647058823529,"Proof. The proof of Proposition 1 is mainly adapted from the proof in Kim & Scott (2012). Here,
we provide proof of completeness. For any p ∈Hkσ, we denote"
REFERENCES,0.6642156862745098,J(p) = 1 N X
REFERENCES,0.6666666666666666,"j∈[N]
ρ
 
∥kσ(xj, ·) −p∥Hkσ

."
REFERENCES,0.6691176470588235,"Then we have the following lemma regarding the Gateaux differential of J and a necessary condition
for ˆprobust to be optimal solution of the robust loss objective function in equation (5)."
REFERENCES,0.6715686274509803,"Lemma 1. Given the assumptions on the robust loss function ρ in Proposition 1, the Gateaux dif-
ferential of J at p ∈Hkσ with incremental h ∈Hkσ, defined as δJ(p; h), is"
REFERENCES,0.6740196078431373,"δJ(p; h) := lim
τ→0
J(p + τh) −J(p)"
REFERENCES,0.6764705882352942,"τ
= −⟨V (p), h⟩Hkσ ,"
REFERENCES,0.678921568627451,where the function V : Hkσ →Hkσ is defined as:
REFERENCES,0.6813725490196079,V (p) = 1 N X
REFERENCES,0.6838235294117647,"j∈[N]
ψ
 
∥kσ(xj, ·) −p∥Hkσ

(kσ(xj, ·) −p)."
REFERENCES,0.6862745098039216,A necessary condition for ˆprobust is V (ˆprobust) = 0.
REFERENCES,0.6887254901960784,"The proof of Lemma 1 can be found in Lemma 1 of Kim & Scott (2012). Based on the necessary
condition for ˆprobust in Lemma 1, i.e., V (ˆprobust) = 0, we have"
N,0.6911764705882353,"1
N X"
N,0.6936274509803921,"j∈[N]
ψ
 
∥kσ(xj, ·) −ˆprobust∥Hkσ

(kσ(xj, ·) −ˆprobust) = 0."
N,0.696078431372549,Direct algebra indicates that ˆprobust = P
N,0.6985294117647058,"j∈[N] ωjkσ(xj, ·) where ω = (ω1, · · · , ωN) ∈∆N, and
ωj ∝ψ
 
∥kσ(xj, ·) −ˆprobust∥Hkσ

. As a consequence, we obtain the conclusion of the proposition."
N,0.7009803921568627,"For the Huber loss function, we have that"
N,0.7034313725490197,"ψ(x) :=

1,
0 ≤x ≤a
a/x,
a < x."
N,0.7058823529411765,"Hence, when the error ∥kσ(xj, ·), · −ˆprobust∥Hkσ is over the threshold a, the final estimator will
down-weight the importance of kσ(xj, ·). This is in sharp contrast with the standard KDE method,
which will assign uniform weights to all of the kσ(xj, ·). As we mentioned in the main paper, the
estimator provided in Proposition 1 is circularly defined, as ˆprobust is defined via ω, and ω depends on
ˆprobust. Such an issue can be addressed by estimating ω with an iterative algorithm termed as kernel-
ized iteratively re-weighted least-squares (KIRWLS). The algorithm starts with randomly initialized
ω(0) ∈∆n, and perform the following iterative updates between two steps:"
N,0.7083333333333334,"ˆp(k)
robust =
X"
N,0.7107843137254902,"j∈[N]
ω(k−1)
i
kσ(xj, ·),
ω(k)
j
=
ψ
kσ(xj, ·) −ˆp(k)
robust

Hkσ  P"
N,0.7132352941176471,"j∈[N] ψ
kσ(xj, ·) −ˆp(k)
robust

Hkσ"
N,0.7156862745098039,".
(25)"
N,0.7181372549019608,"Note that, the optimal ˆprobust is the fixed point of this iterative update, and the KIRWLS algorithm
converges under standard regularity conditions. Furthermore, one can directly compute the term
kσ(xj, ·) −ˆp(k)
robust

Hkσ
via the reproducing property:"
N,0.7205882352941176,"kσ(xj, ·) −ˆp(k)
robust

2"
N,0.7230392156862745,"Hkσ
= −2
X"
N,0.7254901960784313,"m∈[N]
ω(k−1)
m
kσ(xm, xj) + kσ(xj, xj) +
X"
N,0.7279411764705882,"m∈[N],n∈[N]
ω(k−1)
m
ω(k−1)
n
kσ(xm, xn).
(26)"
N,0.7303921568627451,"Therefore, the weights can be updated without mapping the data to the Hilbert space."
N,0.7328431372549019,"C
Fourier Attention with Median of Means"
N,0.7352941176470589,"We introduce the Fourier Attention coupled with the Median of Means (MoM) principle and show
how this is robust to outliers. For any given function ϕ : R →R and radius R, we randomly divide
the keys {ki}i∈[N] into B subsets I1, . . . , IB of equal size where |I1| = |I2| = · · · = |IB| = S.
Define ˆpR,Im(ql) = 1"
N,0.7377450980392157,"S
P
i∈Im
QD
j=1 ϕ( sin(R(qlj−kij))"
N,0.7401960784313726,"R(qlj−kij)
), then the MoM Fourier attention is defined
as ˆhl ="
"S
P",0.7426470588235294,"1
S
P"
"S
P",0.7450980392156863,"i∈Im vi
QD
j=1 ϕ( sin(R(qlj−kij))"
"S
P",0.7475490196078431,"R(qij−klj)
)"
"S
P",0.75,"median{ˆpR,I1(ql), . . . , ˆpR,IB(ql)} ,
(27)"
"S
P",0.7524509803921569,"where Im is the block such that ˆpR(ql, k) achieves its median value. To shed light into the robustness
of Transformers that use Eq. (27) as the attention mechanism, we demonstrate that the estimator
ˆpR(q) = median{ˆpR,I1(q), . . . , ˆpR,IB(q)} is a robust estimator of the density function p(q) of the
keys. We first introduce a few notations that are useful for stating this result. Denote C = {1 ≤
i ≤N : ki is clean} and O = {1 ≤i ≤N : ki is outlier}. Then, we have C ∩O = ∅and
C ∪O = {1, 2, . . . , N}. The following result establishes a high probability upper bound on the
sup-norm between bpR(q) and p(q)."
"S
P",0.7549019607843137,"Theorem 1. Assume that the function ϕ satisfies
R
ϕ(sin(z)/z)zjdz = 0 for all 1 ≤j ≤m
and
R
|ϕ(sin(z)/z)||z|m+1dz < ∞for some m ∈N. Furthermore, the density function p(q)
satisfies supq |p(q)| < ∞. The number of blocks B and the number of outliers |O| are such that
B > (2 + δ)|O| where δ is the failure probability. Then, with ∆=
1
2+δ −|O|"
"S
P",0.7573529411764706,"B for the radius R
sufficiently large and δ sufficiently small, with probability at least 1 −exp(−2∆2B) we find that"
"S
P",0.7598039215686274,"∥ˆpR −p∥∞≤C(
1
Rm+1 + r"
"S
P",0.7622549019607843,"BRD log R log(2/δ) N
)"
"S
P",0.7647058823529411,where C is some universal constant.
"S
P",0.7671568627450981,"Remark 1. The result of Theorem 1 indicates by choosing R = O(N −
1
2(m+1)+D ), the rate of ˆpR to p
under the supremum norm is O(N −
m+1
2(m+1)+D ). With that choice of R, when N approaches infinity,
the MoM estimator ˆpR is a consistent estimator of the clean distribution p of the keys. This confirms
the validity of using ˆpR to robustify p and similarly the usage of MoM Fourier attention Eq. (27) as
a robust attention for Transformers."
"S
P",0.7696078431372549,"Proof. From the formulation of the MoM estimator bpR(q), we obtain the following inequality"
"S
P",0.7720588235294118,"{sup
q |ˆpR(q) −p(q)| ≥ϵ} ⊂{sup
q B
X"
"S
P",0.7745098039215687,"b=1
1{|ˆpR,Ib(q)−p(q)|≥ϵ} ≥B 2 }"
"S
P",0.7769607843137255,"This bound indicates that to bound P(∥ˆpR(q) −p(q)∥∞
≥
ϵ), it is sufficient to bound
P({supq
PB
b=1 1{|ˆpR,Ib(q)−p(q)|≥ϵ} ≥B"
"S
P",0.7794117647058824,"2 }). Indeed, for each 1 ≤b ≤B, we find that"
"S
P",0.7818627450980392,"1{|ˆpR,Ib(q)−p(q)|≥ϵ} ≤1{supq{|ˆpR,Ib(q)−p(q)|≥ϵ}."
"S
P",0.7843137254901961,"Therefore, we have B
X"
"S
P",0.7867647058823529,"b=1
1{|ˆpR,Ib(q)−p(q)|≥ϵ} ≤ B
X"
"S
P",0.7892156862745098,"b=1
1{supq{|ˆpR,Ib(q)−p(q)|≥ϵ},"
"S
P",0.7916666666666666,"which leads to supq
PB
b=1 1{|ˆpR,Ib(q)−p(q)|≥ϵ} ≤PB
b=1 1{supq{|ˆpR,Ib(q)−p(q)|≥ϵ}. This inequality
shows that"
"S
P",0.7941176470588235,"P({sup
q B
X"
"S
P",0.7965686274509803,"b=1
1{|ˆpR,Ib(q)−p(q)|≥ϵ} ≥B"
"S
P",0.7990196078431373,"2 }) ≤P( B
X"
"S
P",0.8014705882352942,"b=1
1{supq{|ˆpR,Ib(q)−p(q)|≥ϵ})."
"S
P",0.803921568627451,"To ease the presentation, we denote Wb = 1{supq{|ˆpR,Ib(q)−p(q)|≥ϵ} and B = {1 ≤b ≤B :
Ib ∩O = ∅}. Then, the following inequalities hold B
X"
"S
P",0.8063725490196079,"b=1
1{supq{|ˆpR,Ib(q)−p(q)|≥ϵ} =
X"
"S
P",0.8088235294117647,"b∈B
Wb +
X"
"S
P",0.8112745098039216,"b∈Bc
Wb ≤
X"
"S
P",0.8137254901960784,"b∈B
Wb + |O| ≤
X"
"S
P",0.8161764705882353,"b∈B
(Wb −E[Wb]) + B · P(sup
q |ˆpR,I1(q) −p(q)| > ϵ) + |O|,"
"S
P",0.8186274509803921,"where we assume without loss of generality that 1 ∈B, which is possible due to the assumption that
B > (2 + δ)|O|. We now prove the following uniform concentration bound:"
"S
P",0.821078431372549,Lemma 2. Assume that ϕ(z) ≤C for all |z| ≤1 for some universal constant C. We have
"S
P",0.8235294117647058,"P(sup
q |ˆpR,I1(q) −p(q)| ≥C(
1
Rm+1 + s"
"S
P",0.8259803921568627,RD log R log(2/δ)
"S
P",0.8284313725490197,"|I1|
)) ≤δ."
"S
P",0.8308823529411765,"Proof of Lemma 2. By the triangle inequality, we have"
"S
P",0.8333333333333334,"sup
q |ˆpR,I1(q) −p(q)| ≤sup
q |ˆpR,I1(q) −E[ˆpR,I1(q)]| + sup
q |E[ˆpR,I1(q)] −p(q)|."
"S
P",0.8357843137254902,"To bound supq |ˆpR,I1(q) −E[ˆpR,I1(q)]|, we use Bernstein’s inequality along with the brack-
eting entropy under L1 norm in the space of queries q.
In particular, we denote Yi
=
RD
AD
QD
j=1 ϕ( sin(R(qj−kij))"
"S
P",0.8382352941176471,"R(qj−kij)
) where A =
R"
"S
P",0.8406862745098039,R ϕ( sin(z)
"S
P",0.8431372549019608,"z
)dz for all i ∈I1.
Since sin(R(qj −
kij))/(R(qj −kij)) ≤1 for all 1 ≤j ≤D, we obtain that |Yi| ≤CDRD/AD where C is the
constant such that ϕ(z) ≤C when |z| ≤1. Furthermore, E[|Yi|]"
"S
P",0.8455882352941176,"By choose ϵ = C(
1
Rm+1 +
q"
"S
P",0.8480392156862745,RD log R log(2/δ)
"S
P",0.8504901960784313,"|I1|
)), then we find that"
"S
P",0.8529411764705882,"P(sup
q |ˆpR,I1(q) −p(q)| > ϵ) ≤
δ
2(2 + δ)"
"S
P",0.8553921568627451,Collecting the above inequalities leads to
"S
P",0.8578431372549019,"P({sup
q B
X"
"S
P",0.8602941176470589,"b=1
1{|ˆpR,Ib(q)−p(q)|≥ϵ} ≥B"
"S
P",0.8627450980392157,"2 }) ≤exp(−2B∆2),"
"S
P",0.8651960784313726,"where ∆=
1
2+δ −|O|"
"S
P",0.8676470588235294,"B . As a consequence, we obtain the conclusion of the theorem."
"S
P",0.8700980392156863,"D
Dataset Information"
"S
P",0.8725490196078431,"WikiText-103
The dataset1 contains around 268K words and its training set consists of about 28K
articles with 103M tokens, this corresponds to text blocks of about 3600 words. The validation set
and test sets consist of 60 articles with 218K and 246K tokens respectively."
"S
P",0.875,"ImageNet
We use the full ImageNet dataset that contains 1.28M training images and 50K vali-
dation images. The model learns to predict the class of the input image among 1000 categories. We
report the top-1 and top-5 accuracy on all experiments. The following ImageNet variants are test
sets that are used to evaluate model performance."
"S
P",0.8774509803921569,"ImageNet-C
For robustness on common image corruptions, we use ImageNet-C (Hendrycks &
Dietterich, 2019) which consists of 15 types of algorithmically generated corruptions with five levels
of severity. ImageNet-C uses the mean corruption error (mCE) as a metric: the smaller mCE means
the more robust the model under corruption."
"S
P",0.8799019607843137,"ImageNet-A
This dataset contains real-world adversarially filtered images that fool current Ima-
geNet classifiers. A 200-class subset of the original ImageNet-1K’s 1000 classes is selected so that
errors among these 200 classes would be considered egregious, which cover most broad categories
spanned by ImageNet-1K."
"S
P",0.8823529411764706,"ImageNet-O
This dataset contains adversarially filtered examples for ImageNet out-of-
distribution detectors. The dataset contains samples from ImageNet-22K but not from ImageNet-
1K, where samples that are wrongly classified as an ImageNet-1K class with high confidence by a
ResNet-50 are selected. We use AUPR (area under precision-recall) as the evaluation metric."
"S
P",0.8848039215686274,"ImageNet-R
This dataset contains various artistic renditions of object classes from the original
ImageNet dataset, which is discouraged by the original ImageNet. ImageNet-R contains 30,000
image renditions for 200 ImageNet classes, where a subset of the ImageNet-1K classes is chosen."
"S
P",0.8872549019607843,"ImageNet-Sketch
This dataset contains 50,000 images, 50 images for each of the 1000 ImageNet
classes. The dataset is constructed with Google Image queries “sketch of xxx”, where xxx is the
standard class name. The search is only performed within the “black and white” color scheme."
"S
P",0.8897058823529411,1www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/
"S
P",0.8921568627450981,"E
Ablation Studies"
"S
P",0.8946078431372549,"In this section, we provide additional results and ablation studies that focus on different design
choices for the proposed robust KDE attention mechanisms. The detailed experimental settings can
be found in the caption of each table."
"S
P",0.8970588235294118,"Table 5:
Perplexity (PPL) and negative likelihood loss (NLL) of our methods (lower part) and baselines
(upper part) on WikiText-103 using a medium version of Transformer. The best results are highlighted in bold
font and the second best are highlighted in underline. On clean data, Transformer-SPKDE achieves better PPL
and NLL than other baselines. Under random swap with outlier words, Transformers with MoM self-attention
show much better performance."
"S
P",0.8995098039215687,"Method (median version)
Clean Data
Word Swap
Valid PPL/Loss
Test PPL/Loss
Valid PPL/Loss
Test PPL/Loss
Transformer (Vaswani et al., 2017b)
27.90/3.32
29.60/3.37
65.36/4.31
68.12/4.36
Performer (Choromanski et al., 2021)
27.34/3.31
29.51/3.36
64.72/4.30
67.43/4.34
Transformer-MGK (Nguyen et al., 2022b)
27.28/3.31
29.24/3.36
64.46/4.30
67.31/4.33
FourierFormer (Nguyen et al., 2022c)
26.51/3.29
28.01/3.33
63.74/4.28
65.27/4.31
Transformer-RKDE (Huber)
26.12/3.28
27.89/3.32
49.37/3.85
51.22/3.89
Transformer-RKDE (Hampel)
25.87/3.27
27.44/3.31
48.62/3.83
51.03/3.88
Transformer-SPKDE
25.76/3.27
27.35/3.31
46.91/3.79
49.14/3.84
Transformer-MoM
28.26/3.34
29.98/3.38
45.35/3.75
47.92/3.81
FourierFormer-MoM
27.13/3.31
29.02/3.36
43.23/3.71
44.97/3.74"
"S
P",0.9019607843137255,"Table 6: Test PPL/NLL loss versus the parameter a of Huber loss function defined in Eq. (24)
(upper) and Hampel loss function (Kim & Scott, 2012) (lower; we use 2×a and 3×a as parameters
b and c) on original and word-swapped Wiki-103 dataset. The best results are highlighted in bold
font and the second best are highlighted in underline. We choose a = 0.4 in rest of the experiments."
"S
P",0.9044117647058824,"Robust Loss Parameter
0.1
0.2
0.4
0.6
0.8
1"
"S
P",0.9068627450980392,"Clean Data
32.92/3.48
32.87/3.48
32.29/3.47
32.38/3.48
32.46/3.48
32.48/3.48"
"S
P",0.9093137254901961,"Word Swap
55.82/3.99
55.97/3.99
55.68/3.99
56.89/4.01
57.26/4.01
57.37/4.01"
"S
P",0.9117647058823529,"Clean Data
32.67/3.48
32.32/3.48
32.35/3.48
32.47/3.48
32.53/3.48
32.58/3.48"
"S
P",0.9142156862745098,"Word Swap
58.02/4.03
57.86/4.03
57.92/4.03
58.24/4.04
58.37/4.04
58.43/4.04"
"S
P",0.9166666666666666,"Table 7: Top-1 classification accuracy on ImageNet versus the parameter a of Huber loss function
defined in Eq. (24) under different settings. The best results are highlighted in bold font and the
second best are highlighted in underline. We choose a = 0.2 in rest of the experiments."
"S
P",0.9191176470588235,"Huber Loss Parameter
0.1
0.2
0.4
0.6
0.8
1"
"S
P",0.9215686274509803,"Clean Data
71.45
72.83
71.62
71.07
70.65
70.34"
"S
P",0.9240196078431373,"FGSM
56.72
55.83
55.34
54.87
54.02
52.98"
"S
P",0.9264705882352942,"PGD
46.37
44.15
43.87
43.25
42.69
41.96"
"S
P",0.928921568627451,"SPSA
52.38
52.42
51.69
51.34
50.97
48.22"
"S
P",0.9313725490196079,"Imagenet-C
45.37
45.58
45.63
45.26
44.63
43.76"
"S
P",0.9338235294117647,"Table 8: Top-1 classification accuracy on ImageNet versus the parameter a of Hampel loss function
defined in Kim & Scott (2012) under different settings. We use 2 × a and 3 × a as parameters b and
c. The best results are highlighted in bold font and the second best are highlighted in underline. We
choose a = 0.2 in rest of the experiments."
"S
P",0.9362745098039216,"Hampel Loss Parameter
0.1
0.2
0.4
0.6
0.8
1"
"S
P",0.9387254901960784,"Clean Data
71.63
72.94
71.84
71.23
70.87
70.41"
"S
P",0.9411764705882353,"FGSM
56.42
55.92
55.83
55.66
54.97
53.68"
"S
P",0.9436274509803921,"PGD
45.18
44.23
43.89
43.62
43.01
42.34"
"S
P",0.946078431372549,"SPSA
52.96
52.48
52.13
51.46
50.92
50.23"
"S
P",0.9485294117647058,"Imagenet-C
44.76
45.61
46.04
46.13
45.82
45.31"
"S
P",0.9509803921568627,"Table 9: Top-1 classification accuracy on ImageNet versus the parameter β of SPKDE defined in
Eq. (6) under different settings. β =
1
1−ε > 1, where ε is the percentage of anomalous samples. A
larger β indicates a more robust model. The best results are highlighted in bold font and the second
best are highlighted in underline. We choose β = 1.4 in rest of the experiments."
"S
P",0.9534313725490197,"β
1.05
1.2
1.4
1.6
1.8
2"
"S
P",0.9558823529411765,"Clean Data
74.25
73.56
73.22
73.01
72.86
72.64"
"S
P",0.9583333333333334,"FGSM
53.69
55.08
56.03
55.37
54.21
53.86"
"S
P",0.9607843137254902,"PGD
42.31
43.68
44.51
44.32
44.17
43.71"
"S
P",0.9632352941176471,"SPSA
51.29
52.02
52.64
52.84
52.16
51.39"
"S
P",0.9656862745098039,"Imagenet-C
44.68
45.49
44.76
44.21
43.96
43.33"
"S
P",0.9681372549019608,"Table 10: Top-1 classification accuracy on ImageNet versus the number of iterations of the KIRWLS
algorithm in Eq. (25) employed in Transformer-RKDE. Since the increased number of iterations
does not lead to significant improvements of performance while the computational cost is much
higher, we use the single-step iteration of the KIRWLS algorithm in Transformer-RKDE."
"S
P",0.9705882352941176,"Huber Loss
Hampel Loss"
"S
P",0.9730392156862745,"Iteration #
1
2
3
5
1
2
3
5"
"S
P",0.9754901960784313,"Clean Data
72.83
72.91
72.95
72.98
72.94
72.99
73.01
73.02"
"S
P",0.9779411764705882,"FGSM
55.83
55.89
55.92
55.94
55.92
55.96
55.97
55.99"
"S
P",0.9803921568627451,"PGD
44.15
44.17
44.17
44.18
44.23
44.26
44.28
44.31"
"S
P",0.9828431372549019,"SPSA
52.42
52.44
52.45
52.45
52.48
52.53
52.55
52.56"
"S
P",0.9852941176470589,"Imagenet-C
45.58
45.61
45.62
45.62
45.61
45.66
45.68
45.71"
"S
P",0.9877450980392157,"Table 11: Computation time (measured by seconds per iteration) of baseline methods, Transformer-
SPKDE, Transformer-MoM and Transformer-RKDE with different number of KIRWLS iterations.
Transformer-SPKDE requires longer time since it directly obtains the optimal set of weights via the
QP solver."
"S
P",0.9901960784313726,"Iterations of KIRWLS
DeiT
RVT
SPKDE
MoM-KDE
1
2
3
5"
"S
P",0.9926470588235294,"Time (s/it)
0.43
0.51
0.68
0.84
0.35
0.41
1.45
0.37"
"S
P",0.9950980392156863,"(a)
(b)
(c)
(d)"
"S
P",0.9975490196078431,"Figure 5: Contour plots of density estimation of the 2-dimensional query vector embedding in an
attention layer of the transformer when using (b) KDE (Eq. (4)) and (c) RKDE after one iteration
of Eq. (25) with Huber loss (Eq. (24)), (d) KDE with median-of-means principle (Eq. (10)), where
(a) is the true density function. We draw 1000 samples (gray circles) from a multivariate normal
density and 100 outliers (red cross) from a gamma distribution as the contaminating density. RKDE
and KDE with the median-of-means principle can be less affected by contaminated samples when
computing self-attention as nonparametric regression."
