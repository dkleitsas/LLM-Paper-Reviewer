Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009250693802035153,"Missing values are a common problem that poses signiﬁcant challenges to data
1"
ABSTRACT,0.0018501387604070306,"analysis and machine learning. This problem necessitates the development of an
2"
ABSTRACT,0.0027752081406105457,"effective imputation method to ﬁll in the missing values accurately, thereby en-
3"
ABSTRACT,0.0037002775208140612,"hancing the overall quality and utility of the datasets. Existing imputation meth-
4"
ABSTRACT,0.004625346901017576,"ods, however, fall short of considering the ‘missingness’ information in the data
5"
ABSTRACT,0.005550416281221091,"during initialization and modeling the entangled feature and sample correlations
6"
ABSTRACT,0.0064754856614246065,"explicitly during the learning process, thus leading to inferior performance. We
7"
ABSTRACT,0.0074005550416281225,"propose M3-Impute, which aims to leverage the missingness information and such
8"
ABSTRACT,0.008325624421831638,"correlations with novel masking schemes. M3-Impute ﬁrst models the data as a
9"
ABSTRACT,0.009250693802035153,"bipartite graph and uses an off-the-shelf graph neural network, equipped with a
10"
ABSTRACT,0.010175763182238668,"reﬁned initialization process, to learn node embeddings. They are then optimized
11"
ABSTRACT,0.011100832562442183,"through M3-Impute’s novel feature correlation unit (FCU) and sample correlation
12"
ABSTRACT,0.012025901942645698,"unit (SCU) that enable explicit consideration of feature and sample correlations
13"
ABSTRACT,0.012950971322849213,"for imputation. Experiment results on 15 benchmark datasets under three different
14"
ABSTRACT,0.013876040703052728,"missing patterns show the effectiveness of M3-Impute by achieving 13 best and 2
15"
ABSTRACT,0.014801110083256245,"second-best MAE scores on average.
16"
INTRODUCTION,0.01572617946345976,"1
Introduction
17"
INTRODUCTION,0.016651248843663275,"Missing values in a dataset are a pervasive issue in real-world data analysis. They arise for various
18"
INTRODUCTION,0.01757631822386679,"reasons, ranging from the limitations of data collection methods to errors during data transmission
19"
INTRODUCTION,0.018501387604070305,"and storage. Since many data analysis algorithms cannot directly handle missing values, the most
20"
INTRODUCTION,0.01942645698427382,"common way to deal with them is to discard the corresponding samples or features with missing
21"
INTRODUCTION,0.020351526364477335,"values, which would compromise the quality of data analysis. To tackle this problem, missing value
22"
INTRODUCTION,0.02127659574468085,"imputation algorithms have been proposed to preserve all samples and features by imputing missing
23"
INTRODUCTION,0.022201665124884366,"values with estimated ones based on the observed values in the dataset, so that the dataset can be
24"
INTRODUCTION,0.02312673450508788,"analyzed as a complete one without losing any information.
25"
INTRODUCTION,0.024051803885291396,"The imputation of missing values usually requires modeling of correlations between different fea-
26"
INTRODUCTION,0.02497687326549491,"tures and samples. Feature-wise correlations help predict missing values from other observed fea-
27"
INTRODUCTION,0.025901942645698426,"tures in the same sample, while sample-wise correlations help predict them in one sample from other
28"
INTRODUCTION,0.02682701202590194,"similar samples. It is thus important to jointly model the feature-wise and sample-wise correlations
29"
INTRODUCTION,0.027752081406105456,"in the dataset. In addition, the prediction of missing values also largely depends on the ‘missingness’
30"
INTRODUCTION,0.028677150786308975,"of the data, i.e., whether a certain feature value is observed or not in the dataset. Speciﬁcally, the
31"
INTRODUCTION,0.02960222016651249,"missingness information directly determines which observed feature values can be used for imputa-
32"
INTRODUCTION,0.030527289546716005,"tion. For example, even if two samples are closely related, it may be less effective to use them for
33"
INTRODUCTION,0.03145235892691952,"imputation if they have missing values in exactly the same features. It still remains a challenging
34"
INTRODUCTION,0.032377428307123035,"problem how to jointly model feature-wise and sample-wise correlations with such data missingness.
35"
INTRODUCTION,0.03330249768732655,"Among existing methods for missing value imputation, statistical methods [4, 9, 14, 16, 18, 19, 22,
36"
INTRODUCTION,0.034227567067530065,"28, 30, 31, 37, 43] extract data correlations with statistical models, which are generally not ﬂexible
37"
INTRODUCTION,0.03515263644773358,"in handling mixed data types and struggles to scale up to large datasets. Learning-based imputation
38"
INTRODUCTION,0.036077705827937095,"methods [10, 24, 27, 29, 33, 42, 50, 51, 53], instead, take advantage of the strong expressiveness
39"
INTRODUCTION,0.03700277520814061,"and scalability of machine/deep learning algorithms to model data correlations. However, most of
40"
INTRODUCTION,0.037927844588344126,"them are still built upon the raw tabular data structure as is, which greatly restricts them from jointly
41"
INTRODUCTION,0.03885291396854764,"modeling the feature-wise and sample-wise correlations. In light of this, graph-based methods [52,
42"
INTRODUCTION,0.039777983348751156,"54] have been proposed to model the raw data as a bipartite graph, with samples and features being
43"
INTRODUCTION,0.04070305272895467,"two different types of nodes. A sample node and a feature node are connected if the feature value
44"
INTRODUCTION,0.041628122109158186,"is observed in that sample. The missing values are then predicted as the inner product between
45"
INTRODUCTION,0.0425531914893617,"the embeddings of the corresponding sample and feature nodes. However, this simple prediction
46"
INTRODUCTION,0.043478260869565216,"does not consider the speciﬁc missingness information as mentioned above. For instance, the target
47"
INTRODUCTION,0.04440333024976873,"feature to impute may have different correlations with features in the samples which have different
48"
INTRODUCTION,0.045328399629972246,"kinds of missingness; however, the same feature-node embedding is still used for their imputation.
49"
INTRODUCTION,0.04625346901017576,"A similar issue also arises for sample-node embeddings.
50"
INTRODUCTION,0.04717853839037928,"In this work, we address these problems by proposing M3-Impute, a mask-guided representation
51"
INTRODUCTION,0.04810360777058279,"learning method for missing value imputation. The key idea behind M3-Impute is to explicitly
52"
INTRODUCTION,0.04902867715078631,"utilize the data-missingness information as model input with our proposed novel masking schemes
53"
INTRODUCTION,0.04995374653098982,"so that it can accurately learn feature-wise and sample-wise correlations in the presence of different
54"
INTRODUCTION,0.05087881591119334,"kinds of data missingness. M3-Impute ﬁrst builds a bipartite graph from the data as used in [52].
55"
INTRODUCTION,0.05180388529139685,"In the embedding initialization for graph representation learning, however, we not only use the the
56"
INTRODUCTION,0.05272895467160037,"relationships between samples and their associated features but also the missingness information so
57"
INTRODUCTION,0.05365402405180388,"as to initialize the embeddings of samples and features jointly and effectively. We then propose novel
58"
INTRODUCTION,0.0545790934320074,"feature correlation unit (FCU) and sample correlation unit (SCU) in M3-Impute to explicitly take
59"
INTRODUCTION,0.05550416281221091,"feature-wise and sample-wise correlations into account for imputation. FCU learns the correlations
60"
INTRODUCTION,0.056429232192414434,"between the target missing feature and observed features within each sample, which are then further
61"
INTRODUCTION,0.05735430157261795,"updated via a soft mask on the sample missingness information. SCU then computes the sample-
62"
INTRODUCTION,0.058279370952821465,"wise correlations with another soft mask on the missingness information for each pair of samples
63"
INTRODUCTION,0.05920444033302498,"that have values to impute. We then integrate the output embeddings of FCU and SCU to estimate
64"
INTRODUCTION,0.060129509713228495,"the missing values in a dataset. We carry out extensive experiments on 15 open datasets. The results
65"
INTRODUCTION,0.06105457909343201,"show that M3-Impute outperforms state-of-the-art methods in 13 of the 15 datasets on average under
66"
INTRODUCTION,0.061979648473635525,"three different settings of missing value patterns, achieving up to 11.47% improvement in MAE
67"
INTRODUCTION,0.06290471785383904,"compared to the second-best method.
68"
RELATED WORK,0.06382978723404255,"2
Related Work
69"
RELATED WORK,0.06475485661424607,"Statistical methods:
These imputation approaches include joint modeling with expectation-
70"
RELATED WORK,0.06567992599444958,"maximization (EM) [9, 16, 22], k-nearest neighbors (kNN) [14, 43], and matrix completion [5,
71"
RELATED WORK,0.0666049953746531,"6, 18, 32]. However, joint modeling with EM and matrix completion often lack the ﬂexibility to
72"
RELATED WORK,0.06753006475485661,"handle data with mixed modalities, while kNN faces scalability issues due to its high computational
73"
RELATED WORK,0.06845513413506013,"complexity. In contrast, M3-Impute is scalable and adaptive to different data distributions.
74"
RELATED WORK,0.06938020351526364,"Learning-based methods: Iterative imputation frameworks [1, 2, 15, 20, 23, 24, 35, 41, 44, 45],
75"
RELATED WORK,0.07030527289546716,"such as MICE [45] and HyperImpute [23], have been extensively studied. These iterative frame-
76"
RELATED WORK,0.07123034227567067,"works apply different imputation methods for each feature and iteratively estimate missing val-
77"
RELATED WORK,0.07215541165587419,"ues until convergence.
In addition, for deep neural network learners, both generative mod-
78"
RELATED WORK,0.0730804810360777,"els [27, 29, 36, 50, 51, 53], such as GAIN [50] and MIWAE [29], and discriminative mod-
79"
RELATED WORK,0.07400555041628122,"els [10, 24, 48], such AimNet [48], have also been proposed. However, these methods are built
80"
RELATED WORK,0.07493061979648474,"upon raw tabular data structures, which fall short of capturing the complex correlations in features,
81"
RELATED WORK,0.07585568917668825,"samples, and their combination [54]. In contrast, M3-Impute is based on the bipartite graph model-
82"
RELATED WORK,0.07678075855689177,"ing of the data, which is more suitable for learning the data correlations for imputation.
83"
RELATED WORK,0.07770582793709528,"Graph neural network-based methods: GNN-based methods [40, 52, 54] are proposed to address
84"
RELATED WORK,0.0786308973172988,"the drawbacks mentioned above due to their effectiveness in modeling complex relations between
85"
RELATED WORK,0.07955596669750231,"entities. Among them, GRAPE [52] transforms tabular data into a bipartite graph where features are
86"
RELATED WORK,0.08048103607770583,"one type of node and samples are the other. A sample node is connected to a feature node only if the
87"
RELATED WORK,0.08140610545790934,"corresponding feature value is present. This transformation allows the imputation task to be framed
88"
RELATED WORK,0.08233117483811286,"as a link prediction problem, where the inner product of the learned node embeddings is computed
89"
RELATED WORK,0.08325624421831637,"as the predicted values. IGRM [54] further enhances the bipartite graph by explicitly introducing
90"
RELATED WORK,0.0841813135985199,"linkages between sample nodes to facilitate message propagation between samples. However, these
91"
RELATED WORK,0.0851063829787234,"methods do not effectively encode the missingness information of different samples and features into
92 SCU"
RELATED WORK,0.08603145235892692,"FCU
Data"
RELATED WORK,0.08695652173913043,Feat 1 Feat 2 Feat 3 Feat 4
RELATED WORK,0.08788159111933395,"0.1
0.24
N/A
0.59
0.4
N/A
0.2
0.59"
RELATED WORK,0.08880666049953746,"0.1
0.3
0.3
N/A"
RELATED WORK,0.08973172987974098,Graph Neural
RELATED WORK,0.09065679925994449,"Network
(e.g., GraphSAGE) MLP I3 ... ... ... ... ..."
RELATED WORK,0.09158186864014801,Initalization
RELATED WORK,0.09250693802035152,LEGEND
RELATED WORK,0.09343200740055504,"Sample-wise Correlation
Existing Edge
Data Flow"
RELATED WORK,0.09435707678075855,"Initialized Node
Sample Node
Feature Node
Known Mask"
RELATED WORK,0.09528214616096208,Feature-wise Correlation
RELATED WORK,0.09620721554116558,Weighted Sum
RELATED WORK,0.0971322849213691,Figure 1: Overview of the M3-Impute model.
RELATED WORK,0.09805735430157261,"the imputation process, which can impair their imputation accuracy. In contrast, M3-Impute enables
93"
RELATED WORK,0.09898242368177614,"explicit modeling of missingness information through novel masking schemes so that feature-wise
94"
RELATED WORK,0.09990749306197964,"and sample-wise correlations can be accurately captured in the imputation process.
95"
RELATED WORK,0.10083256244218317,"3
M3-Impute
96"
OVERVIEW,0.10175763182238667,"3.1
Overview
97"
OVERVIEW,0.1026827012025902,"We here provide an overview of M3-Impute to impute the missing value of feature f for a given
98"
OVERVIEW,0.1036077705827937,"sample s, as depicted in Figure 1. Initially, the data matrix with missing values is modeled as an
99"
OVERVIEW,0.10453283996299723,"undirected bipartite graph, and the missing value is imputed by predicting the edge weight ˆesf of
100"
OVERVIEW,0.10545790934320073,"its corresponding missing edge (Section 3.2). M3-Impute next employs a GNN model, such as
101"
OVERVIEW,0.10638297872340426,"GraphSAGE [17], on the bipartite graph to learn the embeddings of samples and features. These
102"
OVERVIEW,0.10730804810360776,"embeddings, along with the known masks of the data matrix (used to indicate which feature values
103"
OVERVIEW,0.10823311748381129,"are available in each sample), are then input into our novel feature correlation unit (FCU) and
104"
OVERVIEW,0.1091581868640148,"sample correlation unit (SCU), which shall be explained in Section 3.3 and Section 3.4, to obtain
105"
OVERVIEW,0.11008325624421832,"feature-wise and sample-wise correlations, respectively. Finally, M3-Impute takes the feature-wise
106"
OVERVIEW,0.11100832562442182,"and sample-wise correlations into a multi-layer perceptron (MLP) to predict the missing feature
107"
OVERVIEW,0.11193339500462535,"value ˆesf (Section 3.5). The whole process, including the embedding generation, is trained in an
108"
OVERVIEW,0.11285846438482887,"end-to-end manner.
109"
INITIALIZATION UNIT,0.11378353376503238,"3.2
Initialization Unit
110"
INITIALIZATION UNIT,0.1147086031452359,"Let A ∈Rn×m be an n × m matrix that consists of n data samples and m features, where Aij
111"
INITIALIZATION UNIT,0.11563367252543941,"denotes the j-th feature value of the i-th data sample. We introduce an n × m mask matrix M ∈
112"
INITIALIZATION UNIT,0.11655874190564293,"{0, 1}n×m for A to indicate that the value of Aij is observed when Mij = 1. In other words, the
113"
INITIALIZATION UNIT,0.11748381128584644,"goal of imputation here is to predict the missing feature values Aij for i and j such that Mij = 0.
114"
INITIALIZATION UNIT,0.11840888066604996,"We deﬁne the masked data matrix D to be D = A ⊙M, where ⊙is the Hadamard product, i.e., the
115"
INITIALIZATION UNIT,0.11933395004625347,"element-wise multiplication of two matrices.
116"
INITIALIZATION UNIT,0.12025901942645699,"As used in recent studies [52, 54], we model the masked data matrix D as a bipartite graph and tackle
117"
INITIALIZATION UNIT,0.1211840888066605,"the missing value imputation problem as a link prediction task on the bipartite graph. Speciﬁcally,
118"
INITIALIZATION UNIT,0.12210915818686402,"D is modeled as an undirected bipartite graph G = (S ∪F, E), where S = {s1, s2, . . . , sn} is the
119"
INITIALIZATION UNIT,0.12303422756706753,"set of ‘sample’ nodes and F = {f1, f2, . . . , fm} is the set of ‘feature’ nodes. Also, E is the set
120"
INITIALIZATION UNIT,0.12395929694727105,"of edges that only exist between sample node s and feature node f when Dsf ̸= 0, and each edge
121"
INITIALIZATION UNIT,0.12488436632747456,"(s, f) ∈E is associated with edge weight esf, which is given by esf = Dsf. Then, the missing
122"
INITIALIZATION UNIT,0.12580943570767808,"value imputation problem becomes, for any missing entries in D (where Dsf = 0), to predict their
123"
INITIALIZATION UNIT,0.1267345050878816,"corresponding edge weights by developing a learnable mapping F(·), i.e.,
124"
INITIALIZATION UNIT,0.1276595744680851,"ˆesf = F(G, (s, f) ̸∈E).
(1)"
INITIALIZATION UNIT,0.12858464384828863,"The recent studies that use the bipartite graph modeling [52, 54] initialize all sample node embed-
125"
INITIALIZATION UNIT,0.12950971322849214,"dings as all-one vectors and feature node embeddings as one-hot vectors, which have a value 1 in the
126"
INITIALIZATION UNIT,0.13043478260869565,"positions representing their respective features and 0’s elsewhere. We observe, however, that such an
127"
INITIALIZATION UNIT,0.13135985198889916,"initialization does not effectively utilize the information from the masked data matrix, which leads
128"
INITIALIZATION UNIT,0.1322849213691027,"to inferior imputation accuracy, as shall be demonstrated in Section 4.3. Thus, in M3-Impute, we
129"
INITIALIZATION UNIT,0.1332099907493062,"propose to initialize each sample node embedding based on its associated (initial) feature embed-
130"
INITIALIZATION UNIT,0.1341350601295097,"dings instead of initializing them separately. While the feature embeddings are randomly initialized,
131"
INITIALIZATION UNIT,0.13506012950971322,"the sample node embeddings are initialized in a way that reﬂects the embeddings of the features
132"
INITIALIZATION UNIT,0.13598519888991675,"whose values are available in their corresponding samples.
133"
INITIALIZATION UNIT,0.13691026827012026,"Let h0
f be the initial embedding of feature f, which is a randomly initialized d-dimensional vector,
134"
INITIALIZATION UNIT,0.13783533765032377,"and deﬁne H0
F = [h0
f1h0
f2 . . . h0
fm] ∈Rd×m. Also, let ds ∈Rm be the s-th column vector of D⊤,
135"
INITIALIZATION UNIT,0.13876040703052728,"which is a vector of the feature values of sample s, and let ms ∈Rm be its corresponding mask
136"
INITIALIZATION UNIT,0.1396854764107308,"vector, i.e., ms = cols(M⊤), where cols(·) denotes the s-th column vector of the matrix. We then
137"
INITIALIZATION UNIT,0.14061054579093432,"initialize the embedding h0
s of each sample node s as follows:
138"
INITIALIZATION UNIT,0.14153561517113783,"h0
s = ϕ
(
H0
F
[
ds + ϵ(1 −ms)
])
,
(2)"
INITIALIZATION UNIT,0.14246068455134134,"where 1 ∈Rm is an all-one vector, and ϕ(·) is an MLP. Note that the term ds +ϵ(1−ms) indicates
139"
INITIALIZATION UNIT,0.14338575393154487,"a vector that consists of observable feature values of s and some small positive values ϵ in the places
140"
INITIALIZATION UNIT,0.14431082331174838,"where the feature values are unavailable (masked out).
141"
FEATURE CORRELATION UNIT,0.1452358926919519,"3.3
Feature Correlation Unit
142"
FEATURE CORRELATION UNIT,0.1461609620721554,"To improve the accuracy of missing value imputation, we aim to fully exploit feature correlations
143"
FEATURE CORRELATION UNIT,0.14708603145235893,"which often appear in the datasets. While the feature correlations are naturally captured by GNNs,
144"
FEATURE CORRELATION UNIT,0.14801110083256244,"we observe that there is still room for improvement. We propose FCU as an integral component of
145"
FEATURE CORRELATION UNIT,0.14893617021276595,"M3-Impute to fully exploit the feature correlations.
146"
FEATURE CORRELATION UNIT,0.1498612395929695,"To impute the missing value of feature f for a given sample s, FCU begins by computing the
147"
FEATURE CORRELATION UNIT,0.150786308973173,"feature ‘context’ vector of sample s in the embedding space that reﬂects the correlations between
148"
FEATURE CORRELATION UNIT,0.1517113783533765,"the target missing feature f and observed features. Let hf ∈Rd be the learned embedding vector
149"
FEATURE CORRELATION UNIT,0.15263644773358,"of feature f from the GNN, and let HF be the d × m matrix that consists of all the learned feature
150"
FEATURE CORRELATION UNIT,0.15356151711378355,"embedding vectors. We ﬁrst obtain dot-product similarities between feature f and all the features
151"
FEATURE CORRELATION UNIT,0.15448658649398705,"in the embedding space, i.e., H⊤
F hf. We then mask out the similarity values with respect to non-
152"
FEATURE CORRELATION UNIT,0.15541165587419056,"observed features in sample s. Here, instead of applying the mask vector ms of sample s directly,
153"
FEATURE CORRELATION UNIT,0.15633672525439407,"we use a learnable ‘soft’ mask vector, denoted by m′
s, which is deﬁned to be m′
s = σ1(ms) ∈Rm,
154"
FEATURE CORRELATION UNIT,0.1572617946345976,"where σ1(·) is an MLP with the GELU activation function [21]. In other words, we obtain feature-
155"
FEATURE CORRELATION UNIT,0.15818686401480112,"wise similarities with respect to sample s, denoted by rf
s, as follows:
156"
FEATURE CORRELATION UNIT,0.15911193339500462,"rf
s = σ2
(
(H⊤
F hf) ⊙m′
s
)
∈Rd,
(3)"
FEATURE CORRELATION UNIT,0.16003700277520813,"where σ2(·) denotes another MLP with the GELU activation function.
FCU next obtains the
157"
FEATURE CORRELATION UNIT,0.16096207215541167,"Hadamard product between the learned embedding vector of sample s, hs, and the feature-wise
158"
FEATURE CORRELATION UNIT,0.16188714153561518,"similarities with respect to sample s, rf
s, to learn their joint representations in a multiplicative man-
159"
FEATURE CORRELATION UNIT,0.16281221091581868,"ner. Speciﬁcally, FCU obtains the feature context vector of sample s, denoted by cf
s, as follows:
160"
FEATURE CORRELATION UNIT,0.1637372802960222,"cf
s = σ3
(
hs ⊙rf
s
)
∈Rd,
(4)"
FEATURE CORRELATION UNIT,0.16466234967622573,"where σ3(·) is also an MLP with the GELU activation function. That is, FCU fuses the represen-
161"
FEATURE CORRELATION UNIT,0.16558741905642924,"tation vector of s and the vector that has embedding similarity values between the target feature f
162"
FEATURE CORRELATION UNIT,0.16651248843663274,"and the available features in s through the effective use of the soft mask m′
s. From (3) and (4), the
163"
FEATURE CORRELATION UNIT,0.16743755781683625,"operations of FCU can be written as
164"
FEATURE CORRELATION UNIT,0.1683626271970398,"cf
s = FCU(hs, ms, HF ) = σ3
(
hs ⊙σ2
(
(H⊤
F hf) ⊙σ1(ms)
))
.
(5)"
SAMPLE CORRELATION UNIT,0.1692876965772433,"3.4
Sample Correlation Unit
165"
SAMPLE CORRELATION UNIT,0.1702127659574468,"To measure similarities between s and other samples, a common approach would be to use the
166"
SAMPLE CORRELATION UNIT,0.1711378353376503,"dot product or cosine similarity between their embedding vectors. This approach, however, fails
167"
SAMPLE CORRELATION UNIT,0.17206290471785385,"to take into account the observability or availability of each feature in a sample.
It also does
168"
SAMPLE CORRELATION UNIT,0.17298797409805736,"not capture the fact that different observed features are of different importance to the target fea-
169"
SAMPLE CORRELATION UNIT,0.17391304347826086,"ture to impute when it comes to measuring the similarities. We introduce SCU as another inte-
170"
SAMPLE CORRELATION UNIT,0.17483811285846437,"gral component of M3-Impute to compute the sample ‘context’ vector of sample s by incorpo-
171"
SAMPLE CORRELATION UNIT,0.1757631822386679,"rating the embedding vectors of its similar samples as well as different weights of observed fea-
172"
SAMPLE CORRELATION UNIT,0.17668825161887142,"tures. SCU works based on the two novel masking schemes, which shall be explained shortly.
173 SCU"
SAMPLE CORRELATION UNIT,0.17761332099907493,"FCU
FCU"
SAMPLE CORRELATION UNIT,0.17853839037927843,dot product
SAMPLE CORRELATION UNIT,0.17946345975948197,weighted sum MLP
SAMPLE CORRELATION UNIT,0.18038852913968548,masking
SAMPLE CORRELATION UNIT,0.18131359851988899,Figure 2: SCU. 174
SAMPLE CORRELATION UNIT,0.1822386679000925,"Suppose we are to impute the missing value of feature f for a given sample
175"
SAMPLE CORRELATION UNIT,0.18316373728029603,"s. SCU aims to leverage the information from the samples that are similar
176"
SAMPLE CORRELATION UNIT,0.18408880666049954,"to s. As a ﬁrst step to this end, we create a subset of samples P ⊂S that
177"
SAMPLE CORRELATION UNIT,0.18501387604070305,"are similar to s. Speciﬁcally, we randomly choose and put a sample into
178"
SAMPLE CORRELATION UNIT,0.18593894542090658,"P with probability that is proportional to the cosine similarity between s
179"
SAMPLE CORRELATION UNIT,0.1868640148011101,"and the sample. This operation is repeated without replacement until P
180"
SAMPLE CORRELATION UNIT,0.1877890841813136,"reaches a given size.
181"
SAMPLE CORRELATION UNIT,0.1887141535615171,"Mutual Sample Masking: Given a subset of samples P that include s,
182"
SAMPLE CORRELATION UNIT,0.18963922294172064,"we ﬁrst compute the pairwise similarities between s and other samples in
183"
SAMPLE CORRELATION UNIT,0.19056429232192415,"the subset P. While they are computed in a similar way to FCU, we only
184"
SAMPLE CORRELATION UNIT,0.19148936170212766,"consider the commonly observed features (or the common ones that have
185"
SAMPLE CORRELATION UNIT,0.19241443108233117,"feature values) in both s and its peer p ∈P \ {s}, to calculate their pair-
186"
SAMPLE CORRELATION UNIT,0.1933395004625347,"wise similarity in the sense that the missing value of feature f is inferred.
187"
SAMPLE CORRELATION UNIT,0.1942645698427382,"Speciﬁcally, we compute the pairwise similarity between s and p∈P \{s},
188"
SAMPLE CORRELATION UNIT,0.19518963922294172,"which is denoted by sim(s, p | f), as follows:
189"
SAMPLE CORRELATION UNIT,0.19611470860314523,"sim(s, p | f) = FCU(hs, mp, Hf) · FCU(hp, ms, Hf) ∈R,
(6)
where hs and hp are the learned embedding vectors of samples s and p from the GNN, respectively,
190"
SAMPLE CORRELATION UNIT,0.19703977798334876,"and ms and mp are their respective mask vectors. Note that the multiplication in the RHS of (6) is
191"
SAMPLE CORRELATION UNIT,0.19796484736355227,"the dot product.
192"
SAMPLE CORRELATION UNIT,0.19888991674375578,"Irrelevant Feature Masking: After we obtain the pairwise similarities between s and other samples
193"
SAMPLE CORRELATION UNIT,0.1998149861239593,"in P, it would be natural to consider a weighted sum of their corresponding embedding vectors, i.e.,
194
∑"
SAMPLE CORRELATION UNIT,0.20074005550416282,"p∈P\{s} sim(s, p | f) hp, in imputing the value of the target feature f. However, we observe that
195"
SAMPLE CORRELATION UNIT,0.20166512488436633,"hp contains the information from the features whose values are available in p as well as possibly
196"
SAMPLE CORRELATION UNIT,0.20259019426456984,"other features as it is learned via the so-called neighborhood aggregation mechanism that is central
197"
SAMPLE CORRELATION UNIT,0.20351526364477335,"to GNNs, but some of the features may be irrelevant in inferring the value of feature f. Thus, instead
198"
SAMPLE CORRELATION UNIT,0.20444033302497688,"of using {hp} directly, we introduce a d-dimensional mask vector rf
p for hp, which is to mask out
199"
SAMPLE CORRELATION UNIT,0.2053654024051804,"potentially irrelevant feature information in hp, when it comes to imputing the value of feature f.
200"
SAMPLE CORRELATION UNIT,0.2062904717853839,"Speciﬁcally, it is deﬁned by
201"
SAMPLE CORRELATION UNIT,0.2072155411655874,"rf
p = σ4 ([mp; mf]) ∈Rd,
(7)
where mf is an m-dimensional one-hot vector that has a value 1 in the place of feature f and 0’s
202"
SAMPLE CORRELATION UNIT,0.20814061054579094,"elsewhere, [· ; ·] denotes the vector concatenation operation, and σ4(·) is an MLP with the GELU
203"
SAMPLE CORRELATION UNIT,0.20906567992599445,"activation function. Note that the rationale behind the design of rf
p is to embed the information on
204"
SAMPLE CORRELATION UNIT,0.20999074930619796,"the features whose values are present in p as well as the information on the target feature f to impute.
205"
SAMPLE CORRELATION UNIT,0.21091581868640147,"The mask rf
p is then applied to hp to obtain the masked embedding vector of p as follows:
206"
SAMPLE CORRELATION UNIT,0.211840888066605,"ϕp(hp, rf
p) = σ5
(
hp ⊙rf
p
)
∈Rd,
(8)"
SAMPLE CORRELATION UNIT,0.2127659574468085,"where σ5(·) is also an MLP with the GELU activation function. Once we have the masked embed-
207"
SAMPLE CORRELATION UNIT,0.21369102682701202,"ding vectors of samples (excluding s) in P, we ﬁnally compute the sample context vector of sample
208"
SAMPLE CORRELATION UNIT,0.21461609620721553,"s, denoted by zf
s, which is a weighted sum of the masked embedding vectors with weights being the
209"
SAMPLE CORRELATION UNIT,0.21554116558741906,"pairwise similarity values, i.e.,
210"
SAMPLE CORRELATION UNIT,0.21646623496762257,"zf
s = σ6  
∑"
SAMPLE CORRELATION UNIT,0.21739130434782608,"p∈P\{s}
sim(s, p | f) ϕp(hp, rf
p) "
SAMPLE CORRELATION UNIT,0.2183163737280296,"∈Rd,
(9)"
SAMPLE CORRELATION UNIT,0.21924144310823312,"where σ6(·) is again an MLP with the GELU activation function. From (6)–(9), the operations of
211"
SAMPLE CORRELATION UNIT,0.22016651248843663,"SCU can be written as
212"
SAMPLE CORRELATION UNIT,0.22109158186864014,"zf
s = SCU(HP, MP, HF ) = σ6  
∑"
SAMPLE CORRELATION UNIT,0.22201665124884365,"p∈P\{s}
sim(s, p | f) σ5 (hp ⊙σ4 ([mp; mf])) "
SAMPLE CORRELATION UNIT,0.22294172062904719,",
(10)"
SAMPLE CORRELATION UNIT,0.2238667900092507,"where HP = {hp, p ∈P} and MP = {mp, p ∈P}.
213"
SAMPLE CORRELATION UNIT,0.2247918593894542,Algorithm 1 Forward computation of M3-Impute to impute the value of feature f for sample s.
SAMPLE CORRELATION UNIT,0.22571692876965774,"1: Input: Bipartite graph G, initial feature node embeddings H0
F , GNN model (e.g., GraphSAGE) GNN(·),
known mask matrix M, and a subset of samples P ⊂S.
2: Output: Predicted missing feature value ˆesf.
3: Obtain initial sample node embeddings H0
S according to Equation (2).
4: HS, HF = GNN(H0
S, H0
F , G).
▷Perform graph representation learning
5: cf
s = FCU(hs, ms, HF ).
6: zf
s = SCU(HP, MP, HF ).
7: Predict the missing feature value ˆesf using Equation (11)."
IMPUTATION,0.22664199814986125,"3.5
Imputation
214"
IMPUTATION,0.22756706753006475,"For a given sample s, to impute the missing value of feature f, M3-Impute obtains its feature context
215"
IMPUTATION,0.22849213691026826,"vector cf
s and sample context vector zf
s through FCU and SCU, respectively, which are then used
216"
IMPUTATION,0.2294172062904718,"for imputation. Speciﬁcally, it is done by predicting the corresponding edge weight ˆesf as follows:
217"
IMPUTATION,0.2303422756706753,"ˆesf = ϕα
(
(1 −α)cf
s + αzf
s
)
,
(11)"
IMPUTATION,0.23126734505087881,"where ϕα(·) denotes an MLP with a non-linear activation function (i.e., ReLU for continuous values
218"
IMPUTATION,0.23219241443108232,"and softmax for discrete ones), and α is a learnable scalar parameter. This scalar parameter α is
219"
IMPUTATION,0.23311748381128586,"introduced to strike a balance between leveraging feature-wise correlation and sample-wise correla-
220"
IMPUTATION,0.23404255319148937,"tion. It is necessary because the quality of zf
s relies on the quality of the samples chosen in P, so
221"
IMPUTATION,0.23496762257169287,"overly relying on zf
s would backﬁre if their quality is not as desired. To address this problem, instead
222"
IMPUTATION,0.23589269195189638,"of employing a ﬁxed weight α, we make α learnable and adaptive in determining the weights for
223"
IMPUTATION,0.23681776133209992,"cf
s and zf
s . Note that this kind of learnable parameter approach has been widely adopted in natural
224"
IMPUTATION,0.23774283071230343,"language processing [26, 34, 38, 46] and computer vision [8, 55, 56], showing superior performance
225"
IMPUTATION,0.23866790009250693,"to its ﬁxed counterpart. In M3-Impute, the scalar parameter α is learned based on the similarity
226"
IMPUTATION,0.23959296947271044,"values between s and its peer samples p ∈P \ {s} as follows:
227"
IMPUTATION,0.24051803885291398,"α = ϕγ
(
∥"
IMPUTATION,0.2414431082331175,"p∈P\{s}
sim (s, p | f)
)
,
(12)"
IMPUTATION,0.242368177613321,"where ∥represents the concatenation operation, and ϕγ(·) is an MLP with the activation function
228"
IMPUTATION,0.2432932469935245,"γ(x) = 1 −1 / e|x|. The overall operation of M3-Impute is summarized in Algorithm 1. To learn
229"
IMPUTATION,0.24421831637372804,"network parameters, we use cross-entropy loss and mean square error loss for imputing discrete and
230"
IMPUTATION,0.24514338575393155,"continuous feature values, respectively.
231"
EXPERIMENTS,0.24606845513413506,"4
Experiments
232"
EXPERIMENT SETUP,0.24699352451433856,"4.1
Experiment Setup
233"
EXPERIMENT SETUP,0.2479185938945421,"Datasets: We conduct experiments on 15 open datasets. These real-world datasets consist of mixed
234"
EXPERIMENT SETUP,0.2488436632747456,"data types with both continuous and discrete values and cover different domains including civil
235"
EXPERIMENT SETUP,0.24976873265494912,"engineering (CONCRETE, ENERGY), physics and chemistry (YACHT), thermal dynamics (NAVAL),
236"
EXPERIMENT SETUP,0.2506938020351526,"etc. Since the datasets are fully observed, we introduce missing values by applying a randomly
237"
EXPERIMENT SETUP,0.25161887141535616,"generated mask to the data matrix. Speciﬁcally, as used in prior studies [23, 24], we apply three
238"
EXPERIMENT SETUP,0.25254394079555964,"masking generation schemes, namely missing completely at random (MCAR), missing at random
239"
EXPERIMENT SETUP,0.2534690101757632,"(MAR), and missing not at random (MNAR).1 We use MCAR with a missing ratio of 30%, unless
240"
EXPERIMENT SETUP,0.2543940795559667,"otherwise speciﬁed. We follow the preprocessing steps adopted in [52, 54] to scale feature values
241"
EXPERIMENT SETUP,0.2553191489361702,"to [0, 1] with a MinMax scaler [25]. Due to the space limit, we below present the results of eight
242"
EXPERIMENT SETUP,0.25624421831637373,"datasets that are used in Grape [52] and report the other results in Appendix.
243"
EXPERIMENT SETUP,0.25716928769657726,"Baseline models: M3-Impute is compared against popular and state-of-the-art imputation methods,
244"
EXPERIMENT SETUP,0.25809435707678074,"including statistical methods, deep generative methods, and graph-based methods listed as follows:
245"
EXPERIMENT SETUP,0.2590194264569843,"MEAN: It imputes the missing value ˆesf as the mean of observed values in feature f from all
246"
EXPERIMENT SETUP,0.2599444958371878,"the samples. K-nearest neighbors (kNN) [43]: It imputes the missing value ˆesf using the kNNs
247"
EXPERIMENT SETUP,0.2608695652173913,"that have observed values in feature f with weights that are based on the Euclidean distance to
248"
EXPERIMENT SETUP,0.26179463459759483,"sample s. Multivariate imputation by chained equations (Mice) [45]: This method runs multiple
249"
EXPERIMENT SETUP,0.2627197039777983,"regressions where each missing value is modeled upon the observed non-missing values. Iterative
250"
EXPERIMENT SETUP,0.26364477335800185,1More details about the datasets and mask generation for missing values can be found in Appendix.
EXPERIMENT SETUP,0.2645698427382054,Table 1: Imputation accuracy in MAE. MAE scores are enlarged by 10 times.
EXPERIMENT SETUP,0.26549491211840887,"Yacht
Wine
Concrete
Housing
Energy
Naval
Kin8nm
Power"
EXPERIMENT SETUP,0.2664199814986124,"Mean
2.09
0.98
1.79
1.85
3.10
2.31
2.50
1.68
Svd [18]
2.46
0.92
1.94
1.53
2.24
0.50
3.67
2.33
Spectral [30]
2.64
0.91
1.98
1.46
2.26
0.41
2.80
2.13
Mice [45]
1.68
0.77
1.34
1.16
1.53
0.20
2.50
1.16
kNN [43]
1.67
0.72
1.16
0.95
1.81
0.10
2.77
1.38
Gain [50]
2.26
0.86
1.67
1.23
1.99
0.46
2.70
1.31
Miwae [29]
4.68
1.00
1.81
3.81
2.79
2.37
2.57
1.74
Grape [52]
1.46
0.60
0.75
0.64
1.36
0.07
2.50
1.00
Miracle [24]
42.97
1.13
1.71
42.23
41.43
0.17
2.49
1.15
HyperImpute [23]
1.76
0.67
0.84
0.82
1.32
0.04
2.58
1.06"
EXPERIMENT SETUP,0.26734505087881594,"M3-Impute
1.33
0.60
0.71
0.60
1.32
0.06
2.50
0.99"
EXPERIMENT SETUP,0.2682701202590194,"SVD (Svd) [18]: It imputes missing values by solving a matrix completion problem with iterative
251"
EXPERIMENT SETUP,0.26919518963922295,"low-rank singular value decomposition. Spectral regularization algorithm (Spectral) [30]: This
252"
EXPERIMENT SETUP,0.27012025901942643,"matrix completion algorithm uses the nuclear norm as a regularizer and imputes missing values with
253"
EXPERIMENT SETUP,0.27104532839962997,"iterative soft-thresholded SVD. Miwae [29]: It works based on an autoencoder generative model
254"
EXPERIMENT SETUP,0.2719703977798335,"trained to maximize a potentially tight lower bound of the log-likelihood of the observed data and
255"
EXPERIMENT SETUP,0.272895467160037,"Monte Carlo techniques for imputation. Miracle [24]: It uses the imputation results from naive
256"
EXPERIMENT SETUP,0.2738205365402405,"methods such as MEAN and reﬁnes them iteratively by learning a missingness graph (m-graph) and
257"
EXPERIMENT SETUP,0.27474560592044406,"regularizing an imputation function. Gain [50]: This method trains a data imputation generator with
258"
EXPERIMENT SETUP,0.27567067530064754,"a generalized generative adversarial network in which the discriminator aims to distinguish between
259"
EXPERIMENT SETUP,0.2765957446808511,"real and imputed values. Grape [52]: It models the data as a bipartite graph and imputes missing
260"
EXPERIMENT SETUP,0.27752081406105455,"values by predicting the weights of the missing edges, each of which is done based on the inner
261"
EXPERIMENT SETUP,0.2784458834412581,"product between the embeddings of its corresponding sample and feature nodes. HyperImpute [23]:
262"
EXPERIMENT SETUP,0.2793709528214616,"HyperImpute is a framework that conducts an extensive search among a set of imputation methods,
263"
EXPERIMENT SETUP,0.2802960222016651,"selecting the optimal imputation method with ﬁne-tuned parameters for each feature in the dataset.
264"
EXPERIMENT SETUP,0.28122109158186864,"Model conﬁgurations: Parameters of M3-Impute are updated by the Adam optimizer with a learn-
265"
EXPERIMENT SETUP,0.2821461609620722,"ing rate of 0.001 for 40,000 epochs. For graph representation learning, we use a variant of Graph-
266"
EXPERIMENT SETUP,0.28307123034227566,"SAGE [17], which not only learns node embeddings but also edge embeddings via the neighborhood
267"
EXPERIMENT SETUP,0.2839962997224792,"aggregation mechanism, as similarly used in [52]. We consider its three-layer GNN model. We em-
268"
EXPERIMENT SETUP,0.2849213691026827,"ploy mean-pooling as the aggregation function and use ReLU as the activation function for the GNN
269"
EXPERIMENT SETUP,0.2858464384828862,"layers. We set the embedding dimension d to 128. It is known that randomly dropping out a subset
270"
EXPERIMENT SETUP,0.28677150786308975,"of observable edges during training improves the model’s generalization ability. We also leverage
271"
EXPERIMENT SETUP,0.28769657724329323,"the observation and randomly drop 50% of observable edges during training. For each experiment,
272"
EXPERIMENT SETUP,0.28862164662349676,"we conduct ﬁve runs with different random seeds and report the average results.
273"
OVERALL PERFORMANCE,0.2895467160037003,"4.2
Overall Performance
274"
OVERALL PERFORMANCE,0.2904717853839038,"We ﬁrst compare the feature imputation performance of M3-Impute with popular and state-of-the-
275"
OVERALL PERFORMANCE,0.2913968547641073,"art imputation methods. As shown in Table 1, M3-Impute achieves the lowest imputation MAE
276"
OVERALL PERFORMANCE,0.2923219241443108,"for six out of the eight examined datasets and the second-best MAE scores in the other two, which
277"
OVERALL PERFORMANCE,0.29324699352451433,"validates the effectiveness of M3-Impute. For KIN8NM dataset, M3-Impute underperforms Miracle.
278"
OVERALL PERFORMANCE,0.29417206290471787,"It is mainly because each feature in KIN8NM is independent of the others, so none of the observed
279"
OVERALL PERFORMANCE,0.29509713228492135,"features can help impute missing feature values. For NAVAL dataset, the only model that outperforms
280"
OVERALL PERFORMANCE,0.2960222016651249,"M3-Impute is HyperImpute [23]. In the NAVAL dataset, nearly every feature exhibits a strong linear
281"
OVERALL PERFORMANCE,0.2969472710453284,"correlation with the other features, i.e., every pair of features has correlation coefﬁcient close to
282"
OVERALL PERFORMANCE,0.2978723404255319,"one. This allows HyperImpute to readily select a linear model from its model pool for each feature
283"
OVERALL PERFORMANCE,0.29879740980573544,"to impute. Nonetheless, M3-Impute exhibits overall superior performance to the baselines as it
284"
OVERALL PERFORMANCE,0.299722479185939,"can be well adapted to each dataset that possesses different amounts of correlations over features
285"
OVERALL PERFORMANCE,0.30064754856614245,"and samples. In other words, M3-Impute beneﬁts from explicitly incorporating feature-wise and
286"
OVERALL PERFORMANCE,0.301572617946346,"sample-wise correlations together with our carefully designed mask schemes. Furthermore, we
287"
OVERALL PERFORMANCE,0.30249768732654947,"evaluate the performance of M3-Impute under MAR and MNAR settings. We observe that M3-
288"
OVERALL PERFORMANCE,0.303422756706753,"Impute consistently outperforms all the baselines under all datasets and achieves a larger margin in
289"
OVERALL PERFORMANCE,0.30434782608695654,"the improvement compared to the case with MCAR setting. This implies that M3-Impute is also
290"
OVERALL PERFORMANCE,0.30527289546716,"effective in handling different patterns of missing values in the input data. Comprehensive results
291"
OVERALL PERFORMANCE,0.30619796484736356,"are provided in Appendix.
292"
OVERALL PERFORMANCE,0.3071230342275671,Table 2: Ablation study. M3-Uniform stands for M3-Impute with the uniform sampling strategy.
OVERALL PERFORMANCE,0.3080481036077706,"Yacht
Wine
Concrete
Housing
Energy
Naval
Kin8nm
Power"
OVERALL PERFORMANCE,0.3089731729879741,"HyperImpute
1.76 ± .03
0.67 ± .01
0.84 ± .02
0.82 ± .01
1.32 ± .02
0.04 ± .00
2.58 ± .05
1.06 ± .01
Grape
1.46 ± .01
0.60 ± .00
0.75 ± .01
0.64 ± .01
1.36 ± .01
0.07 ± .00
2.50 ± .00
1.00 ± .00"
OVERALL PERFORMANCE,0.3098982423681776,Architecture
OVERALL PERFORMANCE,0.3108233117483811,"Init Only
1.43 ± .01
0.60 ± .00
0.74 ± .00
0.63 ± .01
1.35 ± .01
0.06 ± .00
2.50 ± .00
0.99 ± .00
Init+FCU
1.35 ± .01
0.61 ± .00
0.72 ± .03
0.61 ± .02
1.32 ± .00
0.07 ± .01
2.50 ± .00
0.99 ± .00
Init+SCU
1.37 ± .01
0.60 ± .00
0.73 ± .00
0.63 ± .01
1.30 ± .00
0.09 ± .01
2.50 ± .00
1.00 ± .00
M3-Impute
1.33 ± .04
0.60 ± .00
0.71 ± .01
0.60 ± .00
1.32 ± .01
0.06 ± .00
2.50 ± .00
0.99 ± .00"
OVERALL PERFORMANCE,0.31174838112858466,Sampling Strategy
OVERALL PERFORMANCE,0.31267345050878814,"M3-Uniform
1.34 ± .01
0.60 ± .00
0.73 ± .01
0.61 ± .00
1.31 ± .00
0.06 ± .00
2.50 ± .00
0.99 ± .00"
ABLATION STUDY,0.3135985198889917,"4.3
Ablation Study
293"
ABLATION STUDY,0.3145235892691952,"To study the effectiveness of three integral components of M3-Impute, we consider three variants of
294"
ABLATION STUDY,0.3154486586493987,"M3-Impute, each with a subset of the components, namely initialization only (Init Only), initializa-
295"
ABLATION STUDY,0.31637372802960223,"tion + FCU (Init + FCU), and initialization + SCU (Init + SCU). The performance of these variants
296"
ABLATION STUDY,0.3172987974098057,"are evaluated against the top-performing imputation baselines such as Grape and HyperImpute. As
297"
ABLATION STUDY,0.31822386679000925,"shown in Table 2, the three variants derived from M3-Impute achieve lower MAE values than both
298"
ABLATION STUDY,0.3191489361702128,"baselines in most datasets, demonstrating the effectiveness of our novel components in M3-Impute.
299"
ABLATION STUDY,0.32007400555041626,"Speciﬁcally, for initialization only, the key difference between M3-Impute and Grape lies in our
300"
ABLATION STUDY,0.3209990749306198,"reﬁned initialization process of feature-node and sample-node embeddings. The reduced MAE val-
301"
ABLATION STUDY,0.32192414431082333,"ues observed by the Init Only variant demonstrate that our proposed initialization process is more
302"
ABLATION STUDY,0.3228492136910268,"effective in utilizing information between samples and their associated features, including missing
303"
ABLATION STUDY,0.32377428307123035,"ones, as compared to the basic initialization used in [52]. In addition, we observe that when FCU or
304"
ABLATION STUDY,0.32469935245143383,"SCU is incorporated, MAE values are further reduced for most datasets. This validates that explicitly
305"
ABLATION STUDY,0.32562442183163737,"modeling feature-wise or sample-wise correlations through our novel masking schemes can improve
306"
ABLATION STUDY,0.3265494912118409,"imputation accuracy. When all the three components are combined together as in M3-Impute, they
307"
ABLATION STUDY,0.3274745605920444,"work synergistically to lower MAE values, validating the efﬁcacy of explicit consideration of both
308"
ABLATION STUDY,0.3283996299722479,"sample-wise and feature-wise correlations (in addition to the reﬁned initialization process) for miss-
309"
ABLATION STUDY,0.32932469935245146,"ing data imputation.
310"
ROBUSTNESS,0.33024976873265494,"4.4
Robustness
311"
ROBUSTNESS,0.33117483811285847,"Missing ratio: In practice, datasets may possess different missing ratios. To validate the model’s
312"
ROBUSTNESS,0.33209990749306195,"robustness under such circumstances, we evaluate the performance of M3-Impute and other baseline
313"
ROBUSTNESS,0.3330249768732655,"models with varying missing ratios, i.e., 0.1, 0.3, 0.5, and 0.7. Figure 3 shows their performance. We
314"
ROBUSTNESS,0.333950046253469,"use the MAE of HyperImpute (HI) as the reference performance and offset the performance of each
315"
ROBUSTNESS,0.3348751156336725,"model by MAEx −MAEHI, where x represents the considered model. For clarity, we here only
316"
ROBUSTNESS,0.33580018501387604,"report the results of four top-performing models. As shown in Figure 3, M3-Impute outperforms
317"
ROBUSTNESS,0.3367252543940796,"other baseline models for almost all the cases, especially under YACHT, CONCRETE, ENERGY,
318"
ROBUSTNESS,0.33765032377428306,"and HOUSING datasets. It is worth noting that modeling feature correlations in these datasets is
319"
ROBUSTNESS,0.3385753931544866,"particularly challenging due to the presence of considerable amounts of weakly correlated features,
320"
ROBUSTNESS,0.33950046253469013,"along with a few strongly correlated ones. Nonetheless, FCU and SCU in M3-Impute were able
321"
ROBUSTNESS,0.3404255319148936,"to better capture such correlations with our efﬁcient masking schemes, thereby resulting in a large
322"
ROBUSTNESS,0.34135060129509714,"improvement in imputation accuracy. In addition, for KIN8NM dataset, M3-Impute ties with the
323"
ROBUSTNESS,0.3422756706753006,"second-best model, Grape. As mentioned in Section 4.2, each feature in KIN8NM is independent
324"
ROBUSTNESS,0.34320074005550416,"of the others, so none of the observed features can help impute missing feature values. For NAVAL
325"
ROBUSTNESS,0.3441258094357077,"dataset, where each feature strongly correlates with the others, M3-Impute surpasses Grape but falls
326"
ROBUSTNESS,0.3450508788159112,"short of HyperImpute, due to the same reason as discussed above. Overall, M3-Impute is robust to
327"
ROBUSTNESS,0.3459759481961147,"various missing ratios. Comprehensive results for all the baseline models can be found in Appendix.
328"
ROBUSTNESS,0.34690101757631825,"Sampling strategy in SCU: While SCU uses a sampling strategy based on pairwise cosine similari-
329"
ROBUSTNESS,0.34782608695652173,"ties to construct a subset of samples P, the simplest sampling strategy to build P would be to choose
330"
ROBUSTNESS,0.34875115633672527,"samples uniformly at random without replacement (M3-Uniform). Intuitively, this approach cannot
331"
ROBUSTNESS,0.34967622571692875,"identify similar peer samples accurately and thus would lead to inferior performance. Nonetheless,
332"
ROBUSTNESS,0.3506012950971323,"as shown in Table 2, even with this naive uniform sampling strategy, M3-Uniform still outperforms
333"
ROBUSTNESS,0.3515263644773358,"the two leading imputation baselines.
334 Wine Yacht Naval Power"
ROBUSTNESS,0.3524514338575393,Energy
ROBUSTNESS,0.35337650323774283,Kin8nm
ROBUSTNESS,0.35430157261794637,Housing
ROBUSTNESS,0.35522664199814985,Concrete 0.5 0.0 0.5
ROBUSTNESS,0.3561517113783534,Missing Ratio 0.1 Wine Yacht Naval Power
ROBUSTNESS,0.35707678075855687,Energy
ROBUSTNESS,0.3580018501387604,Kin8nm
ROBUSTNESS,0.35892691951896394,Housing
ROBUSTNESS,0.3598519888991674,Concrete 0.5 0.0 0.5
ROBUSTNESS,0.36077705827937095,Missing Ratio 0.3 Wine Yacht Naval Power
ROBUSTNESS,0.3617021276595745,Energy
ROBUSTNESS,0.36262719703977797,Kin8nm
ROBUSTNESS,0.3635522664199815,Housing
ROBUSTNESS,0.364477335800185,Concrete 0.5 0.0 0.5
ROBUSTNESS,0.3654024051803885,Missing Ratio 0.5 Wine Yacht Naval Power
ROBUSTNESS,0.36632747456059206,Energy
ROBUSTNESS,0.36725254394079554,Kin8nm
ROBUSTNESS,0.3681776133209991,Housing
ROBUSTNESS,0.3691026827012026,Concrete 0.5 0.0 0.5
ROBUSTNESS,0.3700277520814061,Missing Ratio 0.7
ROBUSTNESS,0.3709528214616096,MAE Offset by HyperImpute
ROBUSTNESS,0.37187789084181316,"MICE
KNN
GRAPE
M3-Impute"
ROBUSTNESS,0.37280296022201664,Figure 3: Model performance vs. missing ratios. MAE scores are offset by HyperImpute [23].
ROBUSTNESS,0.3737280296022202,"Size of P in SCU: Intuitively, neither an excessively small nor overly large size of the sample subset
335"
ROBUSTNESS,0.37465309898242366,"P is optimal. Too few peer samples leave SCU with insufﬁcient information to learn sample-wise
336"
ROBUSTNESS,0.3755781683626272,"correlations, while too many peer samples may include quite a few dissimilar ones, which may
337"
ROBUSTNESS,0.37650323774283073,"introduce signiﬁcant noise to the computation of SCU and thus degrade the performance. Table 3
338"
ROBUSTNESS,0.3774283071230342,"shows the performance of M3-Impute with varying numbers of peer samples. In general, the trends
339"
ROBUSTNESS,0.37835337650323775,"agree with our intuition. Although the optimal size varies across different datasets, we observe that
340"
ROBUSTNESS,0.3792784458834413,"having the number of peer samples to be 5 to 10 achieves the overall best imputation accuracy.
341"
ROBUSTNESS,0.38020351526364476,Table 3: MAE scores for varying peer-sample size (|P|−1) and different values of ϵ.
ROBUSTNESS,0.3811285846438483,"Yacht
Wine
Concrete
Housing
Energy
Naval
Kin8nm
Power"
ROBUSTNESS,0.3820536540240518,"Peer = 1
1.34 ± .00
0.60 ± .00
0.73 ± .00
0.61 ± .01
1.32 ± .00
0.06 ± .00
2.5 ± .00
0.99± .00
Peer = 2
1.35 ± .01
0.61 ± .00
0.72 ± .01
0.59 ± .01
1.32 ± .00
0.06 ± .00
2.5 ± .00
1.00 ± .00
Peer = 5
1.33 ± .04
0.60 ± .00
0.71 ± .01
0.60 ± .00
1.32 ± .01
0.06 ± .00
2.5 ± .00
0.99± .00
Peer = 10
1.33 ± .01
0.61 ± .00
0.71 ± .01
0.60 ± .01
1.31 ± .01
0.07 ± .00
2.5 ± .00
1.00 ± .00
Peer = 15
1.34 ± .00
0.61 ± .00
0.72 ± .01
0.60 ± .00
1.31 ± .00
0.07 ± .00
2.5 ± .00
0.99 ± .00
Peer = 20
1.34 ± .04
0.61 ± .00
0.72 ± .01
0.60 ± .01
1.31 ± .00
0.07 ± .00
2.5 ± .00
1.00 ± .00"
ROBUSTNESS,0.3829787234042553,"ϵ = 0
1.34 ± .01
0.61 ± .00
0.71 ± .01
0.60 ± .01
1.30 ± .00
0.06 ± .00
2.50 ± .00
0.99 ± .00
ϵ = 10−5
1.31 ± .01
0.61 ± .00
0.71 ± .00
0.60 ± .01
1.30 ± .00
0.07 ± .00
2.50 ± .00
1.00 ± .00
ϵ = 10−4
1.33 ± .04
0.60 ± .00
0.71 ± .01
0.60 ± .00
1.30 ± .00
0.06 ± .00
2.50 ± .00
0.99 ± .00
ϵ = 10−3
1.33 ± .04
0.60 ± .00
0.72 ± .01
0.60 ± .01
1.30 ± .00
0.07 ± .01
2.50 ± .00
0.99 ± .00"
ROBUSTNESS,0.38390379278445885,"Initialization parameter ϵ: We also evaluate whether a non-zero value of ϵ in the initialization
342"
ROBUSTNESS,0.38482886216466233,"process of M3-Impute indeed lead to an improvement in imputation accuracy. As shown in Table 3,
343"
ROBUSTNESS,0.38575393154486587,"for YACHT and WINE datasets, the introduction of a non-zero value of ϵ results in lower MAE scores.
344"
ROBUSTNESS,0.3866790009250694,"Another insight that we have from Table 3 is that ϵ should not be set too large, as a large value of ϵ
345"
ROBUSTNESS,0.3876040703052729,"might impose incorrect weights to the features with missing values. We observe that it is an overall
346"
ROBUSTNESS,0.3885291396854764,"good choice to set ϵ to 1×10−5 or 1×10−4.
347"
CONCLUSION,0.3894542090656799,"5
Conclusion
348"
CONCLUSION,0.39037927844588344,"We have presented M3-Impute, a mask-guided representation learning for missing data imputation.
349"
CONCLUSION,0.391304347826087,"M3-Impute improved the initialization process by considering the relationships between samples and
350"
CONCLUSION,0.39222941720629045,"their associated features (including missing ones) even in initializing the embeddings. In addition,
351"
CONCLUSION,0.393154486586494,"for more effective representation learning, we introduced two novel components in M3-Impute –
352"
CONCLUSION,0.3940795559666975,"FCU and SCU, which learn feature-wise and sample-wise correlations, respectively, to capture data
353"
CONCLUSION,0.395004625346901,"correlations explicitly and leverage them for imputation. Extensive experiment results demonstrate
354"
CONCLUSION,0.39592969472710454,"the effectiveness of M3-Impute. M3-Impute achieves overall superior performance to popular and
355"
CONCLUSION,0.396854764107308,"state-of-the-art methods on 15 open datasets, with 13 best and two second-best MAE scores on
356"
CONCLUSION,0.39777983348751156,"average under three different settings of missing value patterns.
357"
REFERENCES,0.3987049028677151,"References
358"
REFERENCES,0.3996299722479186,"[1] Melissa J Azur, Elizabeth A Stuart, Constantine Frangakis, and Philip J Leaf. Multiple imputa-
359"
REFERENCES,0.4005550416281221,"tion by chained equations: what is it and how does it work? International journal of methods
360"
REFERENCES,0.40148011100832565,"in psychiatric research, 20(1):40–49, 2011.
361"
REFERENCES,0.4024051803885291,"[2] Jaap Brand. Development, implementation and evaluation of multiple imputation strategies for
362"
REFERENCES,0.40333024976873266,"the statistical analysis of incomplete data sets. 1999.
363"
REFERENCES,0.40425531914893614,"[3] Thomas Brooks, D. Pope, and Michael Marcolini. Airfoil self-noise. https://doi.org/10.
364"
REFERENCES,0.4051803885291397,"24432/C5VW2C, March 2014.
365"
REFERENCES,0.4061054579093432,"[4] Lane F Burgette and Jerome P Reiter. Multiple imputation for missing data via sequential
366"
REFERENCES,0.4070305272895467,"regression trees. American journal of epidemiology, 172(9):1070–1076, 2010.
367"
REFERENCES,0.40795559666975023,"[5] Jian-Feng Cai, Emmanuel J Candès, and Zuowei Shen. A singular value thresholding algorithm
368"
REFERENCES,0.40888066604995377,"for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010.
369"
REFERENCES,0.40980573543015725,"[6] Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization.
370"
REFERENCES,0.4107308048103608,"Communications of the ACM, 55(6):111–119, 2012.
371"
REFERENCES,0.4116558741905643,"[7] Paulo Cortez, Antonio Cerdeira, Fernando Almeida, Telmo Matos, and José Reis. Wine quality.
372"
REFERENCES,0.4125809435707678,"https://doi.org/10.24432/C56S3T, October 2009.
373"
REFERENCES,0.41350601295097134,"[8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. De-
374"
REFERENCES,0.4144310823311748,"formable convolutional networks. In 2017 IEEE International Conference on Computer Vision
375"
REFERENCES,0.41535615171137835,"(ICCV), pages 764–773, 2017.
376"
REFERENCES,0.4162812210915819,"[9] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete
377"
REFERENCES,0.41720629047178537,"data via the em algorithm. Journal of the royal statistical society: series B (methodological),
378"
REFERENCES,0.4181313598519889,"39(1):1–22, 1977.
379"
REFERENCES,0.41905642923219244,"[10] Tianyu Du, Luca Melis, and Ting Wang. Remasker: Imputing tabular data with masked au-
380"
REFERENCES,0.4199814986123959,"toencoding. In The Twelfth International Conference on Learning Representations, 2024.
381"
REFERENCES,0.42090656799259946,"[11] Dheeru Dua and Casey Graff. Uci machine learning repository, 2017.
382"
REFERENCES,0.42183163737280294,"[12] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression.
383"
REFERENCES,0.4227567067530065,"Ann. Statist., 32(1):407–499, 2004.
384"
REFERENCES,0.42368177613321,"[13] Ronald A Fisher. The use of multiple measurements in taxonomic problems. Annals of eugen-
385"
REFERENCES,0.4246068455134135,"ics, 7(2):179–188, 1936.
386"
REFERENCES,0.425531914893617,"[14] Pedro J García-Laencina, José-Luis Sancho-Gómez, and Aníbal R Figueiras-Vidal. Pattern
387"
REFERENCES,0.42645698427382056,"classiﬁcation with missing data: a review. Neural Computing and Applications, 19:263–282,
388"
REFERENCES,0.42738205365402404,"2010.
389"
REFERENCES,0.4283071230342276,"[15] Andrew Gelman. Parameterization and bayesian modeling. Journal of the American Statistical
390"
REFERENCES,0.42923219241443106,"Association, 99(466):537–545, 2004.
391"
REFERENCES,0.4301572617946346,"[16] Zoubin Ghahramani and Michael Jordan. Supervised learning from incomplete data via an em
392"
REFERENCES,0.43108233117483813,"approach. Advances in neural information processing systems, 6, 1993.
393"
REFERENCES,0.4320074005550416,"[17] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on
394"
REFERENCES,0.43293246993524515,"large graphs. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob
395"
REFERENCES,0.4338575393154487,"Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information
396"
REFERENCES,0.43478260869565216,"Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,
397"
REFERENCES,0.4357076780758557,"December 4-9, 2017, Long Beach, CA, USA, pages 1024–1034, 2017.
398"
REFERENCES,0.4366327474560592,"[18] Trevor Hastie, Rahul Mazumder, Jason D. Lee, and Reza Zadeh. Matrix completion and low-
399"
REFERENCES,0.4375578168362627,"rank svd via fast alternating least squares. J. Mach. Learn. Res., 16(1):33673402, jan 2015.
400"
REFERENCES,0.43848288621646625,"[19] Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh.
Matrix completion and
401"
REFERENCES,0.43940795559666973,"low-rank svd via fast alternating least squares. The Journal of Machine Learning Research,
402"
REFERENCES,0.44033302497687327,"16(1):3367–3402, 2015.
403"
REFERENCES,0.4412580943570768,"[20] David Heckerman, David Maxwell Chickering, Christopher Meek, Robert Rounthwaite, and
404"
REFERENCES,0.4421831637372803,"Carl Kadie. Dependency networks for inference, collaborative ﬁltering, and data visualization.
405"
REFERENCES,0.4431082331174838,"Journal of Machine Learning Research, 1(Oct):49–75, 2000.
406"
REFERENCES,0.4440333024976873,"[21] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with
407"
REFERENCES,0.44495837187789083,"gaussian error linear units. CoRR, abs/1606.08415, 2016.
408"
REFERENCES,0.44588344125809437,"[22] James Honaker, Gary King, and Matthew Blackwell. Amelia ii: A program for missing data.
409"
REFERENCES,0.44680851063829785,"Journal of statistical software, 45:1–47, 2011.
410"
REFERENCES,0.4477335800185014,"[23] Daniel Jarrett, Bogdan Cebere, Tennison Liu, Alicia Curth, and Mihaela van der Schaar. Hy-
411"
REFERENCES,0.4486586493987049,"perimpute: Generalized iterative imputation with automatic model selection.
In Kamalika
412"
REFERENCES,0.4495837187789084,"Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, edi-
413"
REFERENCES,0.45050878815911194,"tors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,
414"
REFERENCES,0.4514338575393155,"Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 9916–9937.
415"
REFERENCES,0.45235892691951896,"PMLR, 2022.
416"
REFERENCES,0.4532839962997225,"[24] Trent Kyono, Yao Zhang, Alexis Bellot, and Mihaela van der Schaar. MIRACLE: causally-
417"
REFERENCES,0.45420906567992597,"aware imputation via learning missing data mechanisms.
In Marc’Aurelio Ranzato, Alina
418"
REFERENCES,0.4551341350601295,"Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Ad-
419"
REFERENCES,0.45605920444033304,"vances in Neural Information Processing Systems 34: Annual Conference on Neural Informa-
420"
REFERENCES,0.4569842738205365,"tion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 23806–
421"
REFERENCES,0.45790934320074006,"23817, 2021.
422"
REFERENCES,0.4588344125809436,"[25] Jure Leskovec, Anand Rajaraman, and Jeffrey D. Ullman. Mining of Massive Datasets, 2nd
423"
REFERENCES,0.4597594819611471,"Ed. Cambridge University Press, 2014.
424"
REFERENCES,0.4606845513413506,"[26] Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao, and Rui Yan.
425"
REFERENCES,0.4616096207215541,"VMSMO: Learning to generate multimodal summary for video-based news articles. In Bonnie
426"
REFERENCES,0.46253469010175763,"Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference
427"
REFERENCES,0.46345975948196116,"on Empirical Methods in Natural Language Processing (EMNLP), pages 9360–9369, Online,
428"
REFERENCES,0.46438482886216464,"November 2020. Association for Computational Linguistics.
429"
REFERENCES,0.4653098982423682,"[27] Steven Cheng-Xian Li, Bo Jiang, and Benjamin M. Marlin. Misgan: Learning from incom-
430"
REFERENCES,0.4662349676225717,"plete data with generative adversarial networks. In 7th International Conference on Learning
431"
REFERENCES,0.4671600370027752,"Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
432"
REFERENCES,0.46808510638297873,"[28] Roderick JA Little and Donald B Rubin. Statistical analysis with missing data, volume 793.
433"
REFERENCES,0.4690101757631822,"John Wiley & Sons, 2019.
434"
REFERENCES,0.46993524514338575,"[29] Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: deep generative modelling and imputation
435"
REFERENCES,0.4708603145235893,"of incomplete data sets. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceed-
436"
REFERENCES,0.47178538390379277,"ings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,
437"
REFERENCES,0.4727104532839963,"Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages
438"
REFERENCES,0.47363552266419984,"4413–4423. PMLR, 2019.
439"
REFERENCES,0.4745605920444033,"[30] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for
440"
REFERENCES,0.47548566142460685,"learning large incomplete matrices. J. Mach. Learn. Res., 11:22872322, aug 2010.
441"
REFERENCES,0.47641073080481033,"[31] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms
442"
REFERENCES,0.47733580018501387,"for learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287–
443"
REFERENCES,0.4782608695652174,"2322, 2010.
444"
REFERENCES,0.4791859389454209,"[32] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms
445"
REFERENCES,0.4801110083256244,"for learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287–
446"
REFERENCES,0.48103607770582796,"2322, 2010.
447"
REFERENCES,0.48196114708603144,"[33] Boris Muzellec, Julie Josse, Claire Boyer, and Marco Cuturi. Missing data imputation using
448"
REFERENCES,0.482886216466235,"optimal transport. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th Inter-
449"
REFERENCES,0.4838112858464385,"national Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
450"
REFERENCES,0.484736355226642,"Research, pages 7130–7140. PMLR, 13–18 Jul 2020.
451"
REFERENCES,0.4856614246068455,"[34] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive
452"
REFERENCES,0.486586493987049,"summarization. In International Conference on Learning Representations, 2018.
453"
REFERENCES,0.48751156336725254,"[35] Trivellore E Raghunathan, James M Lepkowski, John Van Hoewyk, Peter Solenberger, et al.
454"
REFERENCES,0.4884366327474561,"A multivariate technique for multiply imputing missing values using a sequence of regression
455"
REFERENCES,0.48936170212765956,"models. Survey methodology, 27(1):85–96, 2001.
456"
REFERENCES,0.4902867715078631,"[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
457"
REFERENCES,0.49121184088806663,"resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Com-
458"
REFERENCES,0.4921369102682701,"puter Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022,
459"
REFERENCES,0.49306197964847365,"pages 10674–10685. IEEE, 2022.
460"
REFERENCES,0.4939870490286771,"[37] Joseph L Schafer. Analysis of incomplete multivariate data. CRC press, 1997.
461"
REFERENCES,0.49491211840888066,"[38] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with
462"
REFERENCES,0.4958371877890842,"pointer-generator networks. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of
463"
REFERENCES,0.4967622571692877,"the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
464"
REFERENCES,0.4976873265494912,"Papers), pages 1073–1083, Vancouver, Canada, July 2017. Association for Computational Lin-
465"
REFERENCES,0.49861239592969475,"guistics.
466"
REFERENCES,0.49953746530989823,"[39] V. Sigillito, S. Wing, L. Hutton, and K. Baker. Ionosphere. https://doi.org/10.24432/
467"
REFERENCES,0.5004625346901017,"C5W01B, December 1988.
468"
REFERENCES,0.5013876040703052,"[40] Indro Spinelli, Simone Scardapane, and Aurelio Uncini.
Missing data imputation with
469"
REFERENCES,0.5023126734505088,"adversarially-trained graph convolutional networks. Neural Networks, 129:249–260, 2020.
470"
REFERENCES,0.5032377428307123,"[41] Daniel J Stekhoven and Peter Bühlmann. Missforestnon-parametric missing value imputation
471"
REFERENCES,0.5041628122109159,"for mixed-type data. Bioinformatics, 28(1):112–118, 2012.
472"
REFERENCES,0.5050878815911193,"[42] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon.
CSDI: conditional score-
473"
REFERENCES,0.5060129509713228,"based diffusion models for probabilistic time series imputation.
In Marc’Aurelio Ranzato,
474"
REFERENCES,0.5069380203515264,"Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors,
475"
REFERENCES,0.5078630897317299,"Advances in Neural Information Processing Systems 34: Annual Conference on Neural Infor-
476"
REFERENCES,0.5087881591119334,"mation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 24804–
477"
REFERENCES,0.509713228492137,"24816, 2021.
478"
REFERENCES,0.5106382978723404,"[43] Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshi-
479"
REFERENCES,0.5115633672525439,"rani, David Botstein, and Russ B. Altman. Missing value estimation methods for dna microar-
480"
REFERENCES,0.5124884366327475,"rays. Bioinformatics, 17(6):520–525, 2001.
481"
REFERENCES,0.513413506012951,"[44] Stef Van Buuren, Jaap PL Brand, Catharina GM Groothuis-Oudshoorn, and Donald B Rubin.
482"
REFERENCES,0.5143385753931545,"Fully conditional speciﬁcation in multivariate imputation. Journal of statistical computation
483"
REFERENCES,0.515263644773358,"and simulation, 76(12):1049–1064, 2006.
484"
REFERENCES,0.5161887141535615,"[45] Stef van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained
485"
REFERENCES,0.517113783533765,"equations in r. Journal of Statistical Software, 45(3):167, 2011.
486"
REFERENCES,0.5180388529139686,"[46] Wenbo Wang, Yang Gao, Heyan Huang, and Yuxiang Zhou. Concept pointer network for
487"
ABSTRACT,0.5189639222941721,"abstractive summarization. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, ed-
488"
ABSTRACT,0.5198889916743756,"itors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-
489"
ABSTRACT,0.5208140610545791,"cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
490"
ABSTRACT,0.5217391304347826,"IJCNLP), pages 3076–3085, Hong Kong, China, November 2019. Association for Computa-
491"
ABSTRACT,0.5226641998149861,"tional Linguistics.
492"
ABSTRACT,0.5235892691951897,"[47] William H. Wolberg, Olvi L. Mangasarian, and W. Nick Street. Breast cancer wisconsin (diag-
493"
ABSTRACT,0.5245143385753932,"nostic). https://doi.org/10.24432/C5DW2B, October 1995.
494"
ABSTRACT,0.5254394079555966,"[48] Richard Wu, Aoqian Zhang, Ihab Ilyas, and Theodoros Rekatsinas. Attention-based learning
495"
ABSTRACT,0.5263644773358002,"for missing data imputation in holoclean.
Proceedings of Machine Learning and Systems,
496"
ABSTRACT,0.5272895467160037,"2:307–325, 2020.
497"
ABSTRACT,0.5282146160962072,"[49] I-Cheng Yeh. Blood transfusion service center. https://doi.org/10.24432/C5GS39, Oc-
498"
ABSTRACT,0.5291396854764108,"tober 2008.
499"
ABSTRACT,0.5300647548566142,"[50] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GAIN: missing data imputation
500"
ABSTRACT,0.5309898242368177,"using generative adversarial nets. In Jennifer G. Dy and Andreas Krause, editors, Proceedings
501"
ABSTRACT,0.5319148936170213,"of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
502"
ABSTRACT,0.5328399629972248,"Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Re-
503"
ABSTRACT,0.5337650323774283,"search, pages 5675–5684. PMLR, 2018.
504"
ABSTRACT,0.5346901017576319,"[51] Seongwook Yoon and Sanghoon Sull. GAMIN: generative adversarial multiple imputation net-
505"
ABSTRACT,0.5356151711378353,"work for highly missing data. In 2020 IEEE/CVF Conference on Computer Vision and Pattern
506"
ABSTRACT,0.5365402405180388,"Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8453–8461. Computer
507"
ABSTRACT,0.5374653098982424,"Vision Foundation / IEEE, 2020.
508"
ABSTRACT,0.5383903792784459,"[52] Jiaxuan You, Xiaobai Ma, Daisy Yi Ding, Mykel J. Kochenderfer, and Jure Leskovec. Handling
509"
ABSTRACT,0.5393154486586494,"missing data with graph representation learning. In Hugo Larochelle, Marc’Aurelio Ranzato,
510"
ABSTRACT,0.5402405180388529,"Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Infor-
511"
ABSTRACT,0.5411655874190564,"mation Processing Systems 33: Annual Conference on Neural Information Processing Systems
512"
ABSTRACT,0.5420906567992599,"2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
513"
ABSTRACT,0.5430157261794635,"[53] Shuhan Zheng and Nontawat Charoenphakdee. Diffusion models for missing value imputation
514"
ABSTRACT,0.543940795559667,"in tabular data. CoRR, abs/2210.17128, 2022.
515"
ABSTRACT,0.5448658649398704,"[54] Jiajun Zhong, Ning Gui, and Weiwei Ye. Data imputation with iterative graph reconstruction.
516"
ABSTRACT,0.545790934320074,"In Brian Williams, Yiling Chen, and Jennifer Neville, editors, Thirty-Seventh AAAI Conference
517"
ABSTRACT,0.5467160037002775,"on Artiﬁcial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of
518"
ABSTRACT,0.547641073080481,"Artiﬁcial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artiﬁcial
519"
ABSTRACT,0.5485661424606846,"Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 11399–11407.
520"
ABSTRACT,0.5494912118408881,"AAAI Press, 2023.
521"
ABSTRACT,0.5504162812210915,"[55] Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, and Jifeng Dai. An empirical study of
522"
ABSTRACT,0.5513413506012951,"spatial attention mechanisms in deep networks. In Proceedings of the IEEE/CVF International
523"
ABSTRACT,0.5522664199814986,"Conference on Computer Vision (ICCV), October 2019.
524"
ABSTRACT,0.5531914893617021,"[56] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable,
525"
ABSTRACT,0.5541165587419057,"better results. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
526"
ABSTRACT,0.5550416281221091,"(CVPR), pages 9300–9308, 2019.
527"
ABSTRACT,0.5559666975023126,"A
Appendix
528"
ABSTRACT,0.5568917668825162,"Table 4: Overview of Datasets.
Concrete
Housing
Wine
Yacht
Energy
Kin8nm
Naval
Power"
ABSTRACT,0.5578168362627197,"# Samples
1030
506
1599
308
768
8192
11934
9568
# Features
8
13
11
6
8
8
16
4"
ABSTRACT,0.5587419056429233,"0
1
2
3
4
5
6
7"
ABSTRACT,0.5596669750231268,"0
1
2
3
4
5
6
7"
ABSTRACT,0.5605920444033302,Concrete
ABSTRACT,0.5615171137835338,"0
1
2
3
4
5
6
7"
ABSTRACT,0.5624421831637373,"0
1
2
3
4
5
6
7"
ABSTRACT,0.5633672525439408,Energy 0 1 2 3 4 5 6 7 8 9 10 11 12
ABSTRACT,0.5642923219241444,"0
1
2
3
4
5
6
7
8
9
10
11
12"
ABSTRACT,0.5652173913043478,Housing
ABSTRACT,0.5661424606845513,"0
1
2
3
4
5
6
7"
ABSTRACT,0.5670675300647549,"0
1
2
3
4
5
6
7"
ABSTRACT,0.5679925994449584,Kim8nm
ABSTRACT,0.5689176688251619,"0
2
4
6
8
10 12 14 16"
ABSTRACT,0.5698427382053654,"0
2
4
6
8
10
12
14
16 Naval"
ABSTRACT,0.5707678075855689,"0
1
2
3"
ABSTRACT,0.5716928769657724,"0
1
2
3 Power"
ABSTRACT,0.572617946345976,0 1 2 3 4 5 6 7 8 9 10
ABSTRACT,0.5735430157261795,"0
1
2
3
4
5
6
7
8
9
10 Wine"
ABSTRACT,0.574468085106383,"0
1
2
3
4
5"
ABSTRACT,0.5753931544865865,"0
1
2
3
4
5 Yacht 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00"
ABSTRACT,0.57631822386679,Figure 4: Pearson correlation coefﬁcients of UCI datasets.
ABSTRACT,0.5772432932469935,"In this section, we discuss further experimental details. We ﬁrst give an overview of the dataset
529"
ABSTRACT,0.5781683626271971,"details in Section A.1, followed by the implementation of different missing types and present corre-
530"
ABSTRACT,0.5790934320074006,"sponding imputation performance under MAR and MNAR settings (Section A.2). We then provide
531"
ABSTRACT,0.580018501387604,"the comprehensive results of the robustness experiments (Section A.3). Finally, we extend our eval-
532"
ABSTRACT,0.5809435707678076,"uation of M3-Impute to seven additional datasets (Section A.4) and elaborate on the computational
533"
ABSTRACT,0.5818686401480111,"resources in Section A.5.
534"
ABSTRACT,0.5827937095282146,"A.1
Dataset Details
535"
ABSTRACT,0.5837187789084182,"Table 4 presents the statistics of the eight UCI datasets [11] used throughout Section 4. Figure 4 il-
536"
ABSTRACT,0.5846438482886216,"lustrates the Pearson correlation coefﬁcients among the features. In the Kin8nm dataset, all features
537"
ABSTRACT,0.5855689176688251,"are linearly independent, whereas the Naval dataset exhibits strong correlations among its features.
538"
ABSTRACT,0.5864939870490287,"Under the MCAR setting, M3-Impute performs comparably to the baseline imputation methods on
539"
ABSTRACT,0.5874190564292322,"these two datasets (shown in Table 1). However, in real-world scenarios, features are not always
540"
ABSTRACT,0.5883441258094357,"entirely independent or strongly correlated. In the other six datasets, we observe a mix of weakly
541"
ABSTRACT,0.5892691951896393,"correlated features along with a few that are strongly correlated. In these cases, M3-Impute consis-
542"
ABSTRACT,0.5901942645698427,"tently outperforms all baseline methods.
543"
ABSTRACT,0.5911193339500462,"A.2
Detailed Results of Different Missing Types
544"
ABSTRACT,0.5920444033302498,"We adopt the same procedure outlined in [52, 54] to generate missing values under different settings.
545"
ABSTRACT,0.5929694727104533,"• MCAR: A n × m matrix is sampled from a uniform distribution. Positions with values no greater
546"
ABSTRACT,0.5938945420906568,"than the ratio of missingness are viewed as missing and the remaining positions are observable.
547"
ABSTRACT,0.5948196114708603,"• MAR: First, a subset of features is randomly selected to be fully observed. Then, these remaining
548"
ABSTRACT,0.5957446808510638,"features have values removed according to a logistic model with random weights, using the fully
549"
ABSTRACT,0.5966697502312673,"observed feature values as input. The desired rate of missingness is achieved by adjusting the bias
550"
ABSTRACT,0.5975948196114709,"term.
551"
ABSTRACT,0.5985198889916744,"• MNAR: This is done by ﬁrst apply the MAR mechanism above. Then, the remaining feature
552"
ABSTRACT,0.599444958371878,"values are masked out by the MCAR mechanism.
553"
ABSTRACT,0.6003700277520814,Table 5: MAE scores under MAR setting.
ABSTRACT,0.6012950971322849,"Yacht
Wine
Concrete
Housing
Energy
Naval
Kin8nm
Power"
ABSTRACT,0.6022201665124884,"Mean
2.20
1.09
1.79
2.02
3.26
2.75
2.49
1.81
Svd [18]
2.64
1.04
2.32
1.71
3.68
0.52
2.69
2.37
Spectral [30]
3.06
0.91
2.12
1.84
2.88
1.29
3.56
3.37
Mice [45]
1.79
0.79
1.27
1.22
1.12
0.27
2.51
1.16
Knn [43]
1.69
0.66
0.89
0.89
1.61
0.07
2.94
1.11
Gain [50]
2.07
1.13
1.87
0.92
2.26
0.91
2.93
1.42
Miwae [29]
3.47
1.04
1.87
3.79
3.82
3.78
2.57
2.07
Grape [52]
1.20
0.60
0.77
0.66
1.05
0.07
2.49
1.06
Miracle [24]
44.33
1.70
3.08
48.63
38.20
48.77
2.82
0.86
HyperImpute [23]
2.06
0.78
1.30
1.05
1.11
1.01
3.07
1.07"
ABSTRACT,0.603145235892692,"M3-Impute
1.09
0.60
0.77
0.60
0.98
0.07
2.49
1.01"
ABSTRACT,0.6040703052728955,Table 6: MAE scores under MNAR setting.
ABSTRACT,0.6049953746530989,"Yacht
Wine
Concrete
Housing
Energy
Naval
Kin8nm
Power"
ABSTRACT,0.6059204440333025,"Mean
2.18
1.04
1.80
1.95
3.17
2.60
2.49
1.76
Svd [18]
2.61
1.06
2.24
1.58
3.55
0.53
2.69
2.27
Spectral [30]
2.75
1.01
1.86
1.60
2.50
1.35
3.34
3.14
Mice [45]
1.91
0.77
1.37
1.22
1.57
0.21
2.50
1.08
Knn [43]
1.92
0.75
1.15
0.95
1.96
0.08
3.06
1.65
Gain [50]
2.34
0.92
1.80
1.08
1.92
1.12
2.78
1.22
Miwae [29]
3.77
1.02
1.86
3.80
2.74
3.79
2.58
1.93
Grape [52]
1.23
0.61
0.73
0.61
1.16
0.08
2.46
1.02
Miracle [24]
43.57
1.03
2.15
46.17
39.37
46.50
2.64
1.06
HyperImpute [23]
1.95
0.72
0.88
0.85
1.19
0.85
2.71
1.09"
ABSTRACT,0.606845513413506,"M3-Impute
1.15
0.60
0.68
0.54
1.09
0.08
2.46
1.00"
ABSTRACT,0.6077705827937095,"In addition to the results for MCAR setting presented in Table 4.2, Table 5 and Table 6 present the
554"
ABSTRACT,0.6086956521739131,"MAE scores under MAR and MNAR settings, respectively. M3-Impute consistently outperforms all
555"
ABSTRACT,0.6096207215541165,"baseline methods in both scenarios.
556"
ABSTRACT,0.61054579093432,"A.3
Robustness against Various Ratios of Missingness
557"
ABSTRACT,0.6114708603145236,"Table 8 presents the performance of various imputation methods across different ratios of missing-
558"
ABSTRACT,0.6123959296947271,"ness. M3-Impute achieves the lowest MAE scores in most cases and the second-best MAE scores in
559"
ABSTRACT,0.6133209990749307,"the remaining ones.
560"
ABSTRACT,0.6142460684551342,"A.4
Further Evaluation on Seven Additional Datasets
561"
ABSTRACT,0.6151711378353376,"Table 7: Overview of seven additional datasets.
airfoil
blood
wine-white
ionosphere
breast
iris
diabetes"
ABSTRACT,0.6160962072155411,"# Samples
1503
748
4899
351
569
150
442
# Features
6
4
12
34
30
4
10"
ABSTRACT,0.6170212765957447,"In this experiment, we further evaluate M3-Impute on seven datasets: Airfoil [3], Blood [49], Wine-
562"
ABSTRACT,0.6179463459759482,"White [7], Ionosphere [39], Breast Cancer [47], Iris [13], and Diabetes [12]. An overview of dataset
563"
ABSTRACT,0.6188714153561518,"details is provided in Table 7, and feature correlations are illustrated in Figure 5. We simulate
564"
ABSTRACT,0.6197964847363552,"missingness in data under MCAR, MAR, and MNAR conditions, each with a missing ratio of 0.3.
565"
ABSTRACT,0.6207215541165587,"Results are demonstrated in Table 9. Across all three types of missingness, M3-Impute achieves ﬁve
566"
ABSTRACT,0.6216466234967623,"best and two second-best MAE scores on average.
567"
ABSTRACT,0.6225716928769658,"0
1
2
3
4
5"
ABSTRACT,0.6234967622571693,"0
1
2
3
4
5"
ABSTRACT,0.6244218316373727,Airfoil
ABSTRACT,0.6253469010175763,"0
1
2
3
4"
ABSTRACT,0.6262719703977798,"0
1
2
3
4 Blood"
ABSTRACT,0.6271970397779834,"0
3
6
9 12 15 18 21 24 27 30"
ABSTRACT,0.6281221091581869,"0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30"
ABSTRACT,0.6290471785383904,Breast
ABSTRACT,0.6299722479185939,0 1 2 3 4 5 6 7 8 9 10
ABSTRACT,0.6308973172987974,"0
1
2
3
4
5
6
7
8
9
10"
ABSTRACT,0.6318223866790009,Diabetes
ABSTRACT,0.6327474560592045,0 3 6 9 1215182124273033
ABSTRACT,0.633672525439408,"0
3
6
9
12
15
18
21
24
27
30
33"
ABSTRACT,0.6345975948196114,Ionosphere
ABSTRACT,0.635522664199815,"0
1
2
3
4"
ABSTRACT,0.6364477335800185,"0
1
2
3
4 Iris"
ABSTRACT,0.637372802960222,0 1 2 3 4 5 6 7 8 9 1011
ABSTRACT,0.6382978723404256,"0
1
2
3
4
5
6
7
8
9
10
11"
ABSTRACT,0.6392229417206291,Wine-White 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
ABSTRACT,0.6401480111008325,Figure 5: Pearson correlation coefﬁcient of 7 extra datasets.
ABSTRACT,0.6410730804810361,"A.5
Computational Resources
568"
ABSTRACT,0.6419981498612396,"All our experiments are conducted on a GPU server running Ubuntu 22.04, with PyTorch 2.1.0
569"
ABSTRACT,0.6429232192414431,"and CUDA 12.1. We train and test M3-Impute using a single NVIDIA A100 80G GPU. With the
570"
ABSTRACT,0.6438482886216467,"experimental setup described in Section 4.1, the total runtime (including both training and testing)
571"
ABSTRACT,0.6447733580018501,"for each of the ﬁve repeated runs ranged from 1 to 5 hours, depending on the scale of the datasets.
572"
ABSTRACT,0.6456984273820536,Table 8: MAE scores across different levels of missingness.
ABSTRACT,0.6466234967622572,"Yacht
Wine
Concrete
Housing
Energy
Naval
Kin8nm
Power"
ABSTRACT,0.6475485661424607,Missing 10%
ABSTRACT,0.6484736355226642,"Mean
2.22 ± 0.05
0.96 ± 0.02
1.81 ± 0.02
1.84 ± 0.01
3.09 ± 0.07
2.30 ± 0.01
2.50 ± 0.01
1.68 ± 0.00
Svd
1.92 ± 0.16
0.88 ± 0.03
2.04 ± 0.04
1.69 ± 0.11
1.75 ± 0.10
0.34 ± 0.00
5.04 ± 0.06
2.26 ± 0.04
Spectral
2.24 ± 0.12
0.76 ± 0.02
1.84 ± 0.05
1.28 ± 0.04
1.76 ± 0.08
0.38 ± 0.01
2.71 ± 0.02
1.77 ± 0.02
Mice
1.38 ± 0.13
0.62 ± 0.01
0.97 ± 0.04
0.98 ± 0.04
1.28 ± 0.07
0.13 ± 0.00
2.50 ± 0.01
1.01 ± 0.01
Knn
1.40 ± 0.17
0.49 ± 0.01
0.58 ± 0.05
0.74 ± 0.04
1.42 ± 0.05
0.03 ± 0.00
2.53 ± 0.01
1.26 ± 0.00
Gain
2.30 ± 0.04
0.83 ± 0.04
1.62 ± 0.05
1.16 ± 0.05
1.95 ± 0.05
0.45 ± 0.01
2.74 ± 0.02
1.22 ± 0.00
Miwae
4.57 ± 0.09
0.98 ± 0.01
1.85 ± 0.03
3.78 ± 0.10
2.77 ± 0.16
2.36 ± 0.00
2.56 ± 0.00
1.74 ± 0.00
Grape
1.00 ± 0.00
0.48 ± 0.00
0.45 ± 0.01
0.49 ± 0.00
1.19 ± 0.00
0.05 ± 0.00
2.49 ± 0.00
0.85 ± 0.03
Miracle
44.77 ± 0.05
0.97 ± 0.19
1.91 ± 0.07
43.90 ± 0.33
41.43 ± 0.34
0.12 ± 0.00
2.48 ± 0.00
1.07 ± 0.05
HyperImpute
1.50 ± 0.11
0.52 ± 0.00
0.51 ± 0.04
0.75 ± 0.04
1.18 ± 0.05
0.06 ± 0.04
2.50 ± 0.00
0.84± 0.00"
ABSTRACT,0.6493987049028677,"M3-Impute
0.96 ± 0.00
0.47 ± 0.01
0.41 ± 0.01
0.45 ± 0.00
1.15 ± 0.00
0.05 ± 0.00
2.49 ± 0.00
0.84 ± 0.01"
ABSTRACT,0.6503237742830712,"Yacht
Wine
Concrete
Housing
Energy
Naval
Kin8nm
Power"
ABSTRACT,0.6512488436632747,Missing 30%
ABSTRACT,0.6521739130434783,"Mean
2.09 ± 0.04
0.98 ± 0.01
1.79 ± 0.01
1.85 ± 0.00
3.10 ± 0.04
2.31 ± 0.00
2.50 ± 0.00
1.68 ± 0.00
Svd
2.46 ± 0.16
0.92 ± 0.01
1.94 ± 0.02
1.53 ± 0.03
2.24 ± 0.06
0.50 ± 0.00
3.67 ± 0.06
2.33 ± 0.01
Spectral
2.64 ± 0.11
0.91 ± 0.01
1.98 ± 0.04
1.46 ± 0.03
2.26 ± 0.09
0.41 ± 0.00
2.80 ± 0.01
2.13 ± 0.01
Mice
1.68 ± 0.05
0.77 ± 0.00
1.34 ± 0.01
1.16 ± 0.03
1.53 ± 0.04
0.20 ± 0.01
2.50 ± 0.00
1.16 ± 0.01
Knn
1.67 ± 0.02
0.72 ± 0.00
1.16 ± 0.03
0.95 ± 0.01
1.81 ± 0.03
0.10 ± 0.00
2.77 ± 0.01
1.38 ± 0.01
Gain
2.26 ± 0.11
0.86 ± 0.00
1.67 ± 0.03
1.23 ± 0.02
1.99 ± 0.03
0.46 ± 0.02
2.70 ± 0.00
1.31 ± 0.05
Miwae
4.68 ± 0.16
1.00 ± 0.00
1.81 ± 0.01
3.81 ± 0.04
2.79 ± 0.04
2.37 ± 0.00
2.57 ± 0.00
1.74 ± 0.00
Grape
1.46 ± 0.01
0.60 ± 0.00
0.75 ± 0.01
0.64 ± 0.01
1.36 ± 0.01
0.07 ± 0.00
2.50 ± 0.00
1.00 ± 0.00
Miracle
42.97 ± 0.53
1.13 ± 0.00
1.71 ± 0.05
42.23 ± 0.31
41.43 ± 0.34
0.17 ± 0.00
2.49 ± 0.00
1.15 ± 0.01
HyperImpute
1.76 ± 0.03
0.67 ± 0.01
0.84 ± 0.02
0.82 ± 0.01
1.32 ± 0.02
0.04 ± 0.00
2.58 ± 0.05
1.06 ± 0.01"
ABSTRACT,0.6530989824236818,"M3-Impute
1.33 ± 0.04
0.60 ± 0.00
0.71 ± 0.01
0.60 ± 0.00
1.32 ± 0.01
0.06 ± 0.00
2.50 ± 0.00
0.99 ± 0.00"
ABSTRACT,0.6540240518038853,"Yacht
Wine
Concrete
Housing
Energy
Naval
Kin8nm
Power"
ABSTRACT,0.6549491211840888,Missing 50%
ABSTRACT,0.6558741905642923,"Mean
2.12 ± 0.02
0.98 ± 0.01
1.81 ± 0.01
1.84 ± 0.01
3.08 ± 0.02
2.31 ± 0.00
2.50 ± 0.00
1.67 ± 0.00
Svd
3.00 ± 0.11
1.18 ± 0.00
2.19 ± 0.01
1.88 ± 0.01
2.88 ± 0.04
0.87 ± 0.00
3.30 ± 0.01
2.92 ± 0.02
Spectral
3.17 ± 0.13
1.13 ± 0.00
2.31 ± 0.01
1.76 ± 0.03
3.03 ± 0.02
0.46 ± 0.00
3.02 ± 0.00
2.98 ± 0.02
Mice
1.99 ± 0.08
0.83 ± 0.00
1.59 ± 0.03
1.33 ± 0.02
2.13 ± 0.12
0.31 ± 0.01
2.50 ± 0.00
1.32 ± 0.01
Knn
2.08 ± 0.02
0.98 ± 0.01
1.40 ± 0.02
1.37 ± 0.01
2.21 ± 0.01
0.76 ± 0.01
2.65 ± 0.00
1.80 ± 0.01
Gain
2.33 ± 0.03
1.18 ± 0.15
2.20 ± 0.17
1.43 ± 0.09
2.58 ± 0.09
0.56 ± 0.03
2.86 ± 0.06
1.36 ± 0.00
Miwae
4.57 ± 0.06
1.01 ± 0.01
1.85 ± 0.02
3.79 ± 0.01
2.83 ± 0.05
2.38 ± 0.00
2.58 ± 0.00
1.73 ± 0.00
Grape
1.89 ± 0.02
0.75 ± 0.01
1.24 ± 0.00
0.83 ± 0.01
1.63 ± 0.01
0.09 ± 0.00
2.50 ± 0.00
1.19 ± 0.00
Miracle
40.77 ± 0.34
1.08 ± 0.00
2.00 ± 0.08
39.40 ± 0.33
37.40 ± 0.22
0.24 ± 0.00
2.82 ± 0.06
1.29 ± 0.00
HyperImpute
2.07 ± 0.11
0.85 ± 0.00
1.33 ± 0.08
1.06 ± 0.11
1.70 ± 0.05
0.07 ± 0.00
2.96 ± 0.04
1.29 ± 0.01"
ABSTRACT,0.6567992599444958,"M3-Impute
1.74 ± 0.01
0.74 ± 0.00
1.19 ± 0.02
0.79 ± 0.01
1.57 ± 0.00
0.08 ± 0.00
2.50 ± 0.00
1.19 ± 0.00"
ABSTRACT,0.6577243293246994,"Yacht
Wine
Concrete
Housing
Energy
Naval
Kin8nm
Power"
ABSTRACT,0.6586493987049029,Missing 70%
ABSTRACT,0.6595744680851063,"Mean
2.16 ± 0.06
0.99 ± 0.00
1.81 ± 0.01
1.83 ± 0.02
3.08 ± 0.01
2.31 ± 0.00
2.50 ± 0.00
1.67 ± 0.00
Svd
3.78 ± 0.06
1.63 ± 0.02
2.53 ± 0.03
2.58 ± 0.07
3.65 ± 0.09
1.56 ± 0.00
3.58 ± 0.00
3.88 ± 0.01
Spectral
4.17 ± 0.10
1.67 ± 0.02
2.75 ± 0.01
2.59 ± 0.05
4.00 ± 0.03
1.04 ± 0.00
3.73 ± 0.01
4.33 ± 0.01
Mice
2.21 ± 0.10
0.93 ± 0.01
1.72 ± 0.02
1.54 ± 0.04
2.71 ± 0.15
0.53 ± 0.00
2.62 ± 0.08
1.46 ± 0.00
Knn
2.62 ± 0.08
1.05 ± 0.00
1.60 ± 0.01
1.43 ± 0.02
2.54 ± 0.04
1.08 ± 0.00
2.84 ± 0.01
2.73 ± 0.00
Gain
3.07 ± 0.08
1.61 ± 0.15
2.84 ± 0.04
3.09 ± 0.04
3.83 ± 0.15
1.07 ± 0.02
3.31 ± 0.21
1.51 ± 0.05
Miwae
4.56 ± 0.07
1.02 ± 0.00
1.84 ± 0.01
3.78 ± 0.02
3.02 ± 0.07
2.38 ± 0.00
2.58 ± 0.00
1.72 ± 0.00
Grape
2.14 ± 0.01
0.88 ± 0.01
1.64 ± 0.02
1.12 ± 0.01
2.10 ± 0.01
0.17 ± 0.00
2.49 ± 0.00
1.37 ± 0.00
Miracle
38.37 ± 0.38
1.03 ± 0.00
2.45 ± 0.21
36.23 ± 0.21
33.93 ± 0.17
0.53 ± 0.00
3.09 ± 0.02
1.92 ± 0.04
HyperImpute
2.49 ± 0.08
0.92 ± 0.02
1.71 ± 0.01
1.12 ± 0.13
2.16 ± 0.06
0.15 ± 0.00
3.15 ± 0.03
1.54 ± 0.02"
ABSTRACT,0.6604995374653099,"M3-Impute
2.14 ± 0.00
0.87 ± 0.00
1.56 ± 0.01
1.08 ± 0.00
2.05 ± 0.00
0.17 ± 0.00
2.49 ± 0.00
1.37 ± 0.00"
ABSTRACT,0.6614246068455134,Table 9: MAE scores on seven additional datasets
ABSTRACT,0.6623496762257169,"airfoil
blood
wine-white
ionosphere
breast
iris
diabetes MCAR"
ABSTRACT,0.6632747456059205,"Mean
2.32 ± 0.05
1.14 ± 0.01
0.76 ± 0.00
2.01 ± 0.03
1.06 ± 0.00
2.15 ± 0.09
1.78 ± 0.03
Svd
2.76 ± 0.05
0.97 ± 0.04
0.87 ± 0.00
1.26 ± 0.03
0.58 ± 0.00
1.70 ± 0.07
1.76 ± 0.02
Spectral
2.30 ± 0.07
0.94 ± 0.03
0.78 ± 0.01
1.38 ± 0.02
0.38 ± 0.00
1.48 ± 0.13
1.48 ± 0.03
Mice
1.97 ± 0.04
0.69 ± 0.01
0.61 ± 0.01
1.37 ± 0.03
0.34 ± 0.01
1.07 ± 0.09
1.29 ± 0.05
Knn
2.18 ± 0.04
0.93 ± 0.01
0.64 ± 0.01
1.07 ± 0.03
0.53 ± 0.01
1.54 ± 0.22
1.71 ± 0.04
Gain
2.22 ± 0.06
1.26 ± 0.04
0.73 ± 0.01
1.50 ± 0.01
0.51 ± 0.01
1.29 ± 0.07
1.47 ± 0.06
Miracle
2.13 ± 0.05
43.17 ± 0.05
0.60 ± 0.00
37.70 ± 0.22
35.07 ± 0.41
45.13 ± 0.42
41.00 ± 0.14
Grape
1.16 ± 0.02
0.68 ± 0.00
0.52 ± 0.00
1.08 ± 0.01
0.37 ± 0.00
0.82 ± 0.00
1.31 ± 0.00
Miwae
2.36 ± 0.06
2.03 ± 0.05
0.77 ± 0.00
5.14 ± 0.06
1.89 ± 0.02
4.60 ± 0.17
5.05 ± 0.04
HyperImpute
1.09 ± 0.02
0.63 ± 0.02
0.55 ± 0.00
1.18 ± 0.04
0.33 ± 0.01
1.04 ± 0.11
1.17 ± 0.02"
ABSTRACT,0.6641998149861239,"M3-Impute
1.09 ± 0.03
0.67 ± 0.00
0.52 ± 0.00
1.01 ± 0.01
0.36 ± 0.01
0.82 ± 0.00
1.29 ± 0.01 MAR"
ABSTRACT,0.6651248843663274,"Mean
2.33 ± 0.14
0.91 ± 0.02
0.87 ± 0.01
2.02 ± 0.08
1.13 ± 0.03
1.99 ± 0.25
1.74 ± 0.33
Svd
2.99 ± 0.83
0.91 ± 0.07
0.78 ± 0.05
1.40 ± 0.08
0.61 ± 0.03
1.85 ± 0.42
2.09 ± 0.02
Spectral
2.01 ± 0.60
1.22 ± 0.36
0.99 ± 0.23
1.50 ± 0.02
0.46 ± 0.04
1.62 ± 0.13
1.32 ± 0.20
Mice
2.16 ± 0.28
1.00 ± 0.40
0.63 ± 0.04
1.43 ± 0.08
0.32 ± 0.07
0.85 ± 0.09
1.33 ± 0.23
Knn
1.59 ± 0.70
0.90 ± 0.25
0.53 ± 0.02
1.09 ± 0.03
0.53 ± 0.03
0.91 ± 0.08
1.43 ± 0.23
Gain
2.29 ± 0.09
1.01 ± 0.15
0.65 ± 0.11
1.71 ± 0.10
0.69 ± 0.05
1.25 ± 0.04
1.34 ± 0.04
Miracle
2.08 ± 0.26
42.30 ± 0.22
1.05 ± 0.05
26.60 ± 0.37
39.53 ± 0.17
49.60 ± 1.14
41.83 ± 0.09
Grape
1.57 ± 0.02
0.29 ± 0.01
0.48 ± 0.00
1.17 ± 0.03
0.39 ± 0.00
0.86 ± 0.02
1.12 ± 0.01
Miwae
2.56 ± 0.01
2.03 ± 0.03
0.69 ± 0.01
6.10 ± 0.04
2.17 ± 0.03
3.46 ± 0.13
4.26 ± 0.06
HyperImpute
1.21 ± 0.21
0.88 ± 0.33
0.57 ± 0.08
1.30 ± 0.03
0.34 ± 0.02
1.05 ± 0.11
1.46 ± 0.10"
ABSTRACT,0.666049953746531,"M3-Impute
1.54 ± 0.02
0.28 ± 0.01
0.48 ± 0.00
1.07 ± 0.01
0.37 ± 0.01
0.82 ± 0.03
1.07 ± 0.00 MNAR"
ABSTRACT,0.6669750231267345,"Mean
2.36 ± 0.11
0.98 ± 0.05
0.82 ± 0.01
2.04 ± 0.06
1.11 ± 0.02
2.06 ± 0.09
1.77 ± 0.20
Svd
2.98 ± 0.52
0.98 ± 0.09
0.82 ± 0.04
1.36 ± 0.07
0.60 ± 0.03
1.66 ± 0.20
1.93 ± 0.02
Spectral
2.64 ± 0.18
1.40 ± 0.18
0.88 ± 0.13
1.46 ± 0.02
0.41 ± 0.03
1.35 ± 0.11
1.51 ± 0.13
Mice
2.07 ± 0.14
0.76 ± 0.17
0.62 ± 0.02
1.44 ± 0.07
0.33 ± 0.02
0.99 ± 0.11
1.27 ± 0.16
Knn
2.11 ± 0.27
1.04 ± 0.12
0.60 ± 0.02
1.12 ± 0.03
0.55 ± 0.02
1.53 ± 0.52
1.60 ± 0.17
Gain
2.21 ± 0.05
1.09 ± 0.06
0.69 ± 0.01
1.55 ± 0.03
0.62 ± 0.02
1.26 ± 0.04
1.43 ± 0.06
Miracle
1.72 ± 0.08
42.90 ± 0.14
0.59 ± 0.01
30.70 ± 0.57
37.30 ± 0.29
47.37 ± 0.90
41.60 ± 0.37
Grape
1.46 ± 0.03
0.42 ± 0.00
0.49 ± 0.00
1.15 ± 0.01
0.38 ± 0.00
0.89 ± 0.02
1.21 ± 0.01
Miwae
2.47 ± 0.03
1.99 ± 0.04
0.72 ± 0.00
5.66 ± 0.02
2.05 ± 0.00
3.98 ± 0.32
4.62 ± 0.08
HyperImpute
1.23 ± 0.04
0.82 ± 0.18
0.58 ± 0.05
1.28 ± 0.02
0.36 ± 0.03
1.07 ± 0.07
1.30 ± 0.19"
ABSTRACT,0.667900092506938,"M3-Impute
1.46 ± 0.01
0.41 ± 0.00
0.49 ± 0.00
1.06 ± 0.02
0.36 ± 0.01
0.87 ± 0.00
1.19 ± 0.00"
ABSTRACT,0.6688251618871416,"NeurIPS Paper Checklist
573"
CLAIMS,0.669750231267345,"1. Claims
574"
CLAIMS,0.6706753006475485,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
575"
CLAIMS,0.6716003700277521,"paper’s contributions and scope?
576"
CLAIMS,0.6725254394079556,"Answer: [Yes]
577"
CLAIMS,0.6734505087881592,"Justiﬁcation: In the abstract and introduction sections, we clearly deﬁne the scope of this
578"
CLAIMS,0.6743755781683626,"paper, focusing on missing value imputation. We propose M3-Impute, a mask-guided im-
579"
CLAIMS,0.6753006475485661,"putation method designed to compute feature-wise and sample-wise correlations based on
580"
CLAIMS,0.6762257169287696,"missing data patterns. A concise summary of the experimental results is provided at the
581"
CLAIMS,0.6771507863089732,"end of both sections.
582"
CLAIMS,0.6780758556891767,"Guidelines:
583"
CLAIMS,0.6790009250693803,"• The answer NA means that the abstract and introduction do not include the claims
584"
CLAIMS,0.6799259944495837,"made in the paper.
585"
CLAIMS,0.6808510638297872,"• The abstract and/or introduction should clearly state the claims made, including the
586"
CLAIMS,0.6817761332099908,"contributions made in the paper and important assumptions and limitations. A No or
587"
CLAIMS,0.6827012025901943,"NA answer to this question will not be perceived well by the reviewers.
588"
CLAIMS,0.6836262719703978,"• The claims made should match theoretical and experimental results, and reﬂect how
589"
CLAIMS,0.6845513413506013,"much the results can be expected to generalize to other settings.
590"
CLAIMS,0.6854764107308048,"• It is ﬁne to include aspirational goals as motivation as long as it is clear that these
591"
CLAIMS,0.6864014801110083,"goals are not attained by the paper.
592"
LIMITATIONS,0.6873265494912119,"2. Limitations
593"
LIMITATIONS,0.6882516188714154,"Question: Does the paper discuss the limitations of the work performed by the authors?
594"
LIMITATIONS,0.6891766882516188,"Answer: [Yes]
595"
LIMITATIONS,0.6901017576318224,"Justiﬁcation: In Section 4.2, we discussed two cases of MAE degradation for the KIN8NM
596"
LIMITATIONS,0.6910268270120259,"and NAVAL datasets. It is mainly because 1. Each feature in KIN8NM is independent of the
597"
LIMITATIONS,0.6919518963922294,"others, so none of the observed features can help impute missing feature values. 2. In the
598"
LIMITATIONS,0.692876965772433,"NAVAL dataset, nearly every feature exhibits a strong linear correlation with the other fea-
599"
LIMITATIONS,0.6938020351526365,"tures. While it is true that M3-Impute does not achieve the best MAE on these two datasets,
600"
LIMITATIONS,0.6947271045328399,"our model has outperformed all the other baselines on the majority of datasets. This demon-
601"
LIMITATIONS,0.6956521739130435,"strates the unique strengths of graph modeling in M3-Impute over tabular data modeling in
602"
LIMITATIONS,0.696577243293247,"baselines like Hyperimpute. In real-world scenarios, the correlation structure of datasets is
603"
LIMITATIONS,0.6975023126734505,"often unpredictable, and such extreme cases are relatively rare. Thus, we design a scheme
604"
LIMITATIONS,0.6984273820536541,"to handle general cases for data imputation tasks. The empirical evidence suggests that our
605"
LIMITATIONS,0.6993524514338575,"approach has been quite successful and exhibits overall superior performance to the base-
606"
LIMITATIONS,0.700277520814061,"lines as it can be well adapted to each dataset that possesses different levels of correlations
607"
LIMITATIONS,0.7012025901942646,"over features and samples.
608"
LIMITATIONS,0.7021276595744681,"Guidelines:
609"
LIMITATIONS,0.7030527289546716,"• The answer NA means that the paper has no limitation while the answer No means
610"
LIMITATIONS,0.7039777983348752,"that the paper has limitations, but those are not discussed in the paper.
611"
LIMITATIONS,0.7049028677150786,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
612"
LIMITATIONS,0.7058279370952821,"• The paper should point out any strong assumptions and how robust the results are to
613"
LIMITATIONS,0.7067530064754857,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
614"
LIMITATIONS,0.7076780758556892,"model well-speciﬁcation, asymptotic approximations only holding locally). The au-
615"
LIMITATIONS,0.7086031452358927,"thors should reﬂect on how these assumptions might be violated in practice and what
616"
LIMITATIONS,0.7095282146160962,"the implications would be.
617"
LIMITATIONS,0.7104532839962997,"• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
618"
LIMITATIONS,0.7113783533765032,"only tested on a few datasets or with a few runs. In general, empirical results often
619"
LIMITATIONS,0.7123034227567068,"depend on implicit assumptions, which should be articulated.
620"
LIMITATIONS,0.7132284921369103,"• The authors should reﬂect on the factors that inﬂuence the performance of the ap-
621"
LIMITATIONS,0.7141535615171137,"proach. For example, a facial recognition algorithm may perform poorly when image
622"
LIMITATIONS,0.7150786308973173,"resolution is low or images are taken in low lighting. Or a speech-to-text system might
623"
LIMITATIONS,0.7160037002775208,"not be used reliably to provide closed captions for online lectures because it fails to
624"
LIMITATIONS,0.7169287696577243,"handle technical jargon.
625"
LIMITATIONS,0.7178538390379279,"• The authors should discuss the computational efﬁciency of the proposed algorithms
626"
LIMITATIONS,0.7187789084181314,"and how they scale with dataset size.
627"
LIMITATIONS,0.7197039777983348,"• If applicable, the authors should discuss possible limitations of their approach to ad-
628"
LIMITATIONS,0.7206290471785384,"dress problems of privacy and fairness.
629"
LIMITATIONS,0.7215541165587419,"• While the authors might fear that complete honesty about limitations might be used by
630"
LIMITATIONS,0.7224791859389454,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
631"
LIMITATIONS,0.723404255319149,"limitations that aren’t acknowledged in the paper. The authors should use their best
632"
LIMITATIONS,0.7243293246993524,"judgment and recognize that individual actions in favor of transparency play an impor-
633"
LIMITATIONS,0.7252543940795559,"tant role in developing norms that preserve the integrity of the community. Reviewers
634"
LIMITATIONS,0.7261794634597595,"will be speciﬁcally instructed to not penalize honesty concerning limitations.
635"
THEORY ASSUMPTIONS AND PROOFS,0.727104532839963,"3. Theory Assumptions and Proofs
636"
THEORY ASSUMPTIONS AND PROOFS,0.7280296022201665,"Question: For each theoretical result, does the paper provide the full set of assumptions and
637"
THEORY ASSUMPTIONS AND PROOFS,0.72895467160037,"a complete (and correct) proof?
638"
THEORY ASSUMPTIONS AND PROOFS,0.7298797409805735,"Answer: [NA]
639"
THEORY ASSUMPTIONS AND PROOFS,0.730804810360777,"Justiﬁcation: This paper does not present theoretical results. We do not assume that the
640"
THEORY ASSUMPTIONS AND PROOFS,0.7317298797409806,"data is missing under MCAR, MAR, or MNAR conditions for M3-Impute to be effective.
641"
THEORY ASSUMPTIONS AND PROOFS,0.7326549491211841,"Instead, M3-Impute demonstrates robust performance across all three settings.
642"
THEORY ASSUMPTIONS AND PROOFS,0.7335800185013877,"Guidelines:
643"
THEORY ASSUMPTIONS AND PROOFS,0.7345050878815911,"• The answer NA means that the paper does not include theoretical results.
644"
THEORY ASSUMPTIONS AND PROOFS,0.7354301572617946,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
645"
THEORY ASSUMPTIONS AND PROOFS,0.7363552266419982,"referenced.
646"
THEORY ASSUMPTIONS AND PROOFS,0.7372802960222017,"• All assumptions should be clearly stated or referenced in the statement of any theo-
647"
THEORY ASSUMPTIONS AND PROOFS,0.7382053654024052,"rems.
648"
THEORY ASSUMPTIONS AND PROOFS,0.7391304347826086,"• The proofs can either appear in the main paper or the supplemental material, but if
649"
THEORY ASSUMPTIONS AND PROOFS,0.7400555041628122,"they appear in the supplemental material, the authors are encouraged to provide a
650"
THEORY ASSUMPTIONS AND PROOFS,0.7409805735430157,"short proof sketch to provide intuition.
651"
THEORY ASSUMPTIONS AND PROOFS,0.7419056429232193,"• Inversely, any informal proof provided in the core of the paper should be comple-
652"
THEORY ASSUMPTIONS AND PROOFS,0.7428307123034228,"mented by formal proofs provided in appendix or supplemental material.
653"
THEORY ASSUMPTIONS AND PROOFS,0.7437557816836263,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7446808510638298,"4. Experimental Result Reproducibility
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7456059204440333,"Question: Does the paper fully disclose all the information needed to reproduce the main
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7465309898242368,"experimental results of the paper to the extent that it affects the main claims and/or conclu-
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7474560592044404,"sions of the paper (regardless of whether the code and data are provided or not)?
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7483811285846439,"Answer: [Yes]
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7493061979648473,"Justiﬁcation: In Section 3, we explain the computational pipeline of the proposed model
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7502312673450509,"in detail and provide a pseudo-code to better outline the methodology. The experimental
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7511563367252544,"setup is comprehensively described in Section 4.1. In addition, supplementary materials
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7520814061054579,"include our complete codebase to reproduce the results presented in this paper, including
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7530064754856615,"the model implementation, training and testing pipeline, conﬁguration ﬁles, and execution
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7539315448658649,"scripts.
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7548566142460684,"Guidelines:
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.755781683626272,"• The answer NA means that the paper does not include experiments.
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567067530064755,"• If the paper includes experiments, a No answer to this question will not be perceived
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.757631822386679,"well by the reviewers: Making the paper reproducible is important, regardless of
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7585568917668826,"whether the code and data are provided or not.
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.759481961147086,"• If the contribution is a dataset and/or model, the authors should describe the steps
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7604070305272895,"taken to make their results reproducible or veriﬁable.
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7613320999074931,"• Depending on the contribution, reproducibility can be accomplished in various ways.
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7622571692876966,"For example, if the contribution is a novel architecture, describing the architecture
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7631822386679001,"fully might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation,
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7641073080481036,"it may be necessary to either make it possible for others to replicate the model with
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7650323774283071,"the same dataset, or provide access to the model. In general. releasing code and data
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7659574468085106,"is often one good way to accomplish this, but reproducibility can also be provided via
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7668825161887142,"detailed instructions for how to replicate the results, access to a hosted model (e.g., in
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7678075855689177,"the case of a large language model), releasing of a model checkpoint, or other means
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7687326549491211,"that are appropriate to the research performed.
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7696577243293247,"• While NeurIPS does not require releasing code, the conference does require all sub-
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7705827937095282,"missions to provide some reasonable avenue for reproducibility, which may depend
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7715078630897317,"on the nature of the contribution. For example
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7724329324699353,"(a) If the contribution is primarily a new algorithm, the paper should make it clear
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7733580018501388,"how to reproduce that algorithm.
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7742830712303422,"(b) If the contribution is primarily a new model architecture, the paper should describe
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7752081406105458,"the architecture clearly and fully.
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7761332099907493,"(c) If the contribution is a new model (e.g., a large language model), then there should
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7770582793709528,"either be a way to access this model for reproducing the results or a way to re-
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7779833487511564,"produce the model (e.g., with an open-source dataset or instructions for how to
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7789084181313598,"construct the dataset).
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7798334875115633,"(d) We recognize that reproducibility may be tricky in some cases, in which case au-
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7807585568917669,"thors are welcome to describe the particular way they provide for reproducibility.
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7816836262719704,"In the case of closed-source models, it may be that access to the model is limited in
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.782608695652174,"some way (e.g., to registered users), but it should be possible for other researchers
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7835337650323775,"to have some path to reproducing or verifying the results.
697"
OPEN ACCESS TO DATA AND CODE,0.7844588344125809,"5. Open access to data and code
698"
OPEN ACCESS TO DATA AND CODE,0.7853839037927844,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
699"
OPEN ACCESS TO DATA AND CODE,0.786308973172988,"tions to faithfully reproduce the main experimental results, as described in supplemental
700"
OPEN ACCESS TO DATA AND CODE,0.7872340425531915,"material?
701"
OPEN ACCESS TO DATA AND CODE,0.788159111933395,"Answer: [Yes]
702"
OPEN ACCESS TO DATA AND CODE,0.7890841813135985,"Justiﬁcation: In the supplementary material, we provide the complete code for our model,
703"
OPEN ACCESS TO DATA AND CODE,0.790009250693802,"including the scripts for experiments and evaluations, as well as the execution scripts used
704"
OPEN ACCESS TO DATA AND CODE,0.7909343200740055,"in the experiments. We have also included the data preprocessed by us, along with the
705"
OPEN ACCESS TO DATA AND CODE,0.7918593894542091,"download links for publicly available datasets. We will release the formatted codebase on
706"
OPEN ACCESS TO DATA AND CODE,0.7927844588344126,"GitHub following the conclusion of the anonymity period.
707"
OPEN ACCESS TO DATA AND CODE,0.793709528214616,"Guidelines:
708"
OPEN ACCESS TO DATA AND CODE,0.7946345975948196,"• The answer NA means that paper does not include experiments requiring code.
709"
OPEN ACCESS TO DATA AND CODE,0.7955596669750231,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
710"
OPEN ACCESS TO DATA AND CODE,0.7964847363552267,"public/guides/CodeSubmissionPolicy) for more details.
711"
OPEN ACCESS TO DATA AND CODE,0.7974098057354302,"• While we encourage the release of code and data, we understand that this might not
712"
OPEN ACCESS TO DATA AND CODE,0.7983348751156337,"be possible, so No is an acceptable answer. Papers cannot be rejected simply for not
713"
OPEN ACCESS TO DATA AND CODE,0.7992599444958371,"including code, unless this is central to the contribution (e.g., for a new open-source
714"
OPEN ACCESS TO DATA AND CODE,0.8001850138760407,"benchmark).
715"
OPEN ACCESS TO DATA AND CODE,0.8011100832562442,"• The instructions should contain the exact command and environment needed to run to
716"
OPEN ACCESS TO DATA AND CODE,0.8020351526364478,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
717"
OPEN ACCESS TO DATA AND CODE,0.8029602220166513,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
718"
OPEN ACCESS TO DATA AND CODE,0.8038852913968547,"• The authors should provide instructions on data access and preparation, including how
719"
OPEN ACCESS TO DATA AND CODE,0.8048103607770583,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
720"
OPEN ACCESS TO DATA AND CODE,0.8057354301572618,"• The authors should provide scripts to reproduce all experimental results for the new
721"
OPEN ACCESS TO DATA AND CODE,0.8066604995374653,"proposed method and baselines. If only a subset of experiments are reproducible, they
722"
OPEN ACCESS TO DATA AND CODE,0.8075855689176689,"should state which ones are omitted from the script and why.
723"
OPEN ACCESS TO DATA AND CODE,0.8085106382978723,"• At submission time, to preserve anonymity, the authors should release anonymized
724"
OPEN ACCESS TO DATA AND CODE,0.8094357076780758,"versions (if applicable).
725"
OPEN ACCESS TO DATA AND CODE,0.8103607770582794,"• Providing as much information as possible in supplemental material (appended to the
726"
OPEN ACCESS TO DATA AND CODE,0.8112858464384829,"paper) is recommended, but including URLs to data and code is permitted.
727"
OPEN ACCESS TO DATA AND CODE,0.8122109158186864,"6. Experimental Setting/Details
728"
OPEN ACCESS TO DATA AND CODE,0.81313598519889,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
729"
OPEN ACCESS TO DATA AND CODE,0.8140610545790934,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
730"
OPEN ACCESS TO DATA AND CODE,0.8149861239592969,"results?
731"
OPEN ACCESS TO DATA AND CODE,0.8159111933395005,"Answer: [Yes]
732"
OPEN ACCESS TO DATA AND CODE,0.816836262719704,"Justiﬁcation: Experimental setup is detailed in Section 4.1 and Appendix A.2. We also
733"
OPEN ACCESS TO DATA AND CODE,0.8177613320999075,"explore the hyperparameters utilized in M3-Impute. Results are presented in Table 3.
734"
OPEN ACCESS TO DATA AND CODE,0.818686401480111,"Guidelines:
735"
OPEN ACCESS TO DATA AND CODE,0.8196114708603145,"• The answer NA means that the paper does not include experiments.
736"
OPEN ACCESS TO DATA AND CODE,0.820536540240518,"• The experimental setting should be presented in the core of the paper to a level of
737"
OPEN ACCESS TO DATA AND CODE,0.8214616096207216,"detail that is necessary to appreciate the results and make sense of them.
738"
OPEN ACCESS TO DATA AND CODE,0.8223866790009251,"• The full details can be provided either with the code, in appendix, or as supplemental
739"
OPEN ACCESS TO DATA AND CODE,0.8233117483811286,"material.
740"
OPEN ACCESS TO DATA AND CODE,0.8242368177613321,"7. Experiment Statistical Signiﬁcance
741"
OPEN ACCESS TO DATA AND CODE,0.8251618871415356,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropri-
742"
OPEN ACCESS TO DATA AND CODE,0.8260869565217391,"ate information about the statistical signiﬁcance of the experiments?
743"
OPEN ACCESS TO DATA AND CODE,0.8270120259019427,"Answer: [Yes]
744"
OPEN ACCESS TO DATA AND CODE,0.8279370952821462,"Justiﬁcation: We conduct all the experiments over ﬁve runs and report the mean MAE
745"
OPEN ACCESS TO DATA AND CODE,0.8288621646623496,"scores, along with the standard deviations.
746"
OPEN ACCESS TO DATA AND CODE,0.8297872340425532,"Guidelines:
747"
OPEN ACCESS TO DATA AND CODE,0.8307123034227567,"• The answer NA means that the paper does not include experiments.
748"
OPEN ACCESS TO DATA AND CODE,0.8316373728029602,"• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
749"
OPEN ACCESS TO DATA AND CODE,0.8325624421831638,"dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
750"
OPEN ACCESS TO DATA AND CODE,0.8334875115633672,"the main claims of the paper.
751"
OPEN ACCESS TO DATA AND CODE,0.8344125809435707,"• The factors of variability that the error bars are capturing should be clearly stated (for
752"
OPEN ACCESS TO DATA AND CODE,0.8353376503237743,"example, train/test split, initialization, random drawing of some parameter, or overall
753"
OPEN ACCESS TO DATA AND CODE,0.8362627197039778,"run with given experimental conditions).
754"
OPEN ACCESS TO DATA AND CODE,0.8371877890841813,"• The method for calculating the error bars should be explained (closed form formula,
755"
OPEN ACCESS TO DATA AND CODE,0.8381128584643849,"call to a library function, bootstrap, etc.)
756"
OPEN ACCESS TO DATA AND CODE,0.8390379278445883,"• The assumptions made should be given (e.g., Normally distributed errors).
757"
OPEN ACCESS TO DATA AND CODE,0.8399629972247918,"• It should be clear whether the error bar is the standard deviation or the standard error
758"
OPEN ACCESS TO DATA AND CODE,0.8408880666049954,"of the mean.
759"
OPEN ACCESS TO DATA AND CODE,0.8418131359851989,"• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
760"
OPEN ACCESS TO DATA AND CODE,0.8427382053654024,"ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
761"
OPEN ACCESS TO DATA AND CODE,0.8436632747456059,"Normality of errors is not veriﬁed.
762"
OPEN ACCESS TO DATA AND CODE,0.8445883441258094,"• For asymmetric distributions, the authors should be careful not to show in tables or
763"
OPEN ACCESS TO DATA AND CODE,0.845513413506013,"ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
764"
OPEN ACCESS TO DATA AND CODE,0.8464384828862165,"error rates).
765"
OPEN ACCESS TO DATA AND CODE,0.84736355226642,"• If error bars are reported in tables or plots, The authors should explain in the text how
766"
OPEN ACCESS TO DATA AND CODE,0.8482886216466234,"they were calculated and reference the corresponding ﬁgures or tables in the text.
767"
EXPERIMENTS COMPUTE RESOURCES,0.849213691026827,"8. Experiments Compute Resources
768"
EXPERIMENTS COMPUTE RESOURCES,0.8501387604070305,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
769"
EXPERIMENTS COMPUTE RESOURCES,0.851063829787234,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
770"
EXPERIMENTS COMPUTE RESOURCES,0.8519888991674376,"the experiments?
771"
EXPERIMENTS COMPUTE RESOURCES,0.8529139685476411,"Answer: [Yes]
772"
EXPERIMENTS COMPUTE RESOURCES,0.8538390379278445,"Justiﬁcation: We train and test M3-Impute on a single Nvidia A100 80G GPU (Detailed
773"
EXPERIMENTS COMPUTE RESOURCES,0.8547641073080481,"setup described in A.5). With the experimental setup described in Section 4.1, the total
774"
EXPERIMENTS COMPUTE RESOURCES,0.8556891766882516,"running time (including training and testing) for one of the ﬁve repeated runs varies from 1
775"
EXPERIMENTS COMPUTE RESOURCES,0.8566142460684552,"to 5 hours, depending on the scale of the datasets.
776"
EXPERIMENTS COMPUTE RESOURCES,0.8575393154486587,"Guidelines:
777"
EXPERIMENTS COMPUTE RESOURCES,0.8584643848288621,"• The answer NA means that the paper does not include experiments.
778"
EXPERIMENTS COMPUTE RESOURCES,0.8593894542090657,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
779"
EXPERIMENTS COMPUTE RESOURCES,0.8603145235892692,"or cloud provider, including relevant memory and storage.
780"
EXPERIMENTS COMPUTE RESOURCES,0.8612395929694727,"• The paper should provide the amount of compute required for each of the individual
781"
EXPERIMENTS COMPUTE RESOURCES,0.8621646623496763,"experimental runs as well as estimate the total compute.
782"
EXPERIMENTS COMPUTE RESOURCES,0.8630897317298798,"• The paper should disclose whether the full research project required more compute
783"
EXPERIMENTS COMPUTE RESOURCES,0.8640148011100832,"than the experiments reported in the paper (e.g., preliminary or failed experiments
784"
EXPERIMENTS COMPUTE RESOURCES,0.8649398704902868,"that didn’t make it into the paper).
785"
CODE OF ETHICS,0.8658649398704903,"9. Code Of Ethics
786"
CODE OF ETHICS,0.8667900092506938,"Question: Does the research conducted in the paper conform, in every respect, with the
787"
CODE OF ETHICS,0.8677150786308974,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
788"
CODE OF ETHICS,0.8686401480111008,"Answer: [Yes]
789"
CODE OF ETHICS,0.8695652173913043,"Justiﬁcation: Our paper adheres to the NeurIPS Code of Ethics in every respect. 1. We
790"
CODE OF ETHICS,0.8704902867715079,"ensured fair wages for all human participants involved in our study, abiding by regional
791"
CODE OF ETHICS,0.8714153561517114,"minimum hourly rates. 2. Our research methodology adhered to institutional protocols for
792"
CODE OF ETHICS,0.8723404255319149,"human subjects and data privacy. 3. We obtained informed consent from all participants
793"
CODE OF ETHICS,0.8732654949121184,"and minimized exposure of personally identiﬁable information. 4. The datasets used are
794"
CODE OF ETHICS,0.8741905642923219,"publicly available and have not been deprecated, with all copyrights respected. 5. We
795"
CODE OF ETHICS,0.8751156336725254,"have transparently communicated the societal impact of our research, considering potential
796"
CODE OF ETHICS,0.876040703052729,"misuse and its effects on discrimination, surveillance, and environmental impact. 6. We
797"
CODE OF ETHICS,0.8769657724329325,"have also reﬂected on the biases in our models and datasets and taken steps to mitigate them.
798"
CODE OF ETHICS,0.877890841813136,"7. Our data and models are documented and released with appropriate licenses, and we’ve
799"
CODE OF ETHICS,0.8788159111933395,"employed secure data storage and distribution practices. 8. We ensured legal compliance
800"
CODE OF ETHICS,0.879740980573543,"and provided all necessary elements for the reproducibility of our research.
801"
CODE OF ETHICS,0.8806660499537465,"Guidelines:
802"
CODE OF ETHICS,0.8815911193339501,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
803"
CODE OF ETHICS,0.8825161887141536,"• If the authors answer No, they should explain the special circumstances that require a
804"
CODE OF ETHICS,0.883441258094357,"deviation from the Code of Ethics.
805"
CODE OF ETHICS,0.8843663274745606,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
806"
CODE OF ETHICS,0.8852913968547641,"eration due to laws or regulations in their jurisdiction).
807"
BROADER IMPACTS,0.8862164662349676,"10. Broader Impacts
808"
BROADER IMPACTS,0.8871415356151712,"Question: Does the paper discuss both potential positive societal impacts and negative
809"
BROADER IMPACTS,0.8880666049953746,"societal impacts of the work performed?
810"
BROADER IMPACTS,0.8889916743755781,"Answer: [NA]
811"
BROADER IMPACTS,0.8899167437557817,"Justiﬁcation: The method proposed in this work is only applicable for missing value impu-
812"
BROADER IMPACTS,0.8908418131359852,"tation and is unlikely to have a negative social impact.
813"
BROADER IMPACTS,0.8917668825161887,"Guidelines:
814"
BROADER IMPACTS,0.8926919518963923,"• The answer NA means that there is no societal impact of the work performed.
815"
BROADER IMPACTS,0.8936170212765957,"• If the authors answer NA or No, they should explain why their work has no societal
816"
BROADER IMPACTS,0.8945420906567992,"impact or why the paper does not address societal impact.
817"
BROADER IMPACTS,0.8954671600370028,"• Examples of negative societal impacts include potential malicious or unintended uses
818"
BROADER IMPACTS,0.8963922294172063,"(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
819"
BROADER IMPACTS,0.8973172987974098,"(e.g., deployment of technologies that could make decisions that unfairly impact spe-
820"
BROADER IMPACTS,0.8982423681776133,"ciﬁc groups), privacy considerations, and security considerations.
821"
BROADER IMPACTS,0.8991674375578168,"• The conference expects that many papers will be foundational research and not tied
822"
BROADER IMPACTS,0.9000925069380203,"to particular applications, let alone deployments. However, if there is a direct path to
823"
BROADER IMPACTS,0.9010175763182239,"any negative applications, the authors should point it out. For example, it is legitimate
824"
BROADER IMPACTS,0.9019426456984274,"to point out that an improvement in the quality of generative models could be used to
825"
BROADER IMPACTS,0.902867715078631,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
826"
BROADER IMPACTS,0.9037927844588344,"that a generic algorithm for optimizing neural networks could enable people to train
827"
BROADER IMPACTS,0.9047178538390379,"models that generate Deepfakes faster.
828"
BROADER IMPACTS,0.9056429232192414,"• The authors should consider possible harms that could arise when the technology is
829"
BROADER IMPACTS,0.906567992599445,"being used as intended and functioning correctly, harms that could arise when the
830"
BROADER IMPACTS,0.9074930619796485,"technology is being used as intended but gives incorrect results, and harms following
831"
BROADER IMPACTS,0.9084181313598519,"from (intentional or unintentional) misuse of the technology.
832"
BROADER IMPACTS,0.9093432007400555,"• If there are negative societal impacts, the authors could also discuss possible mitiga-
833"
BROADER IMPACTS,0.910268270120259,"tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
834"
BROADER IMPACTS,0.9111933395004626,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
835"
BROADER IMPACTS,0.9121184088806661,"feedback over time, improving the efﬁciency and accessibility of ML).
836"
SAFEGUARDS,0.9130434782608695,"11. Safeguards
837"
SAFEGUARDS,0.913968547641073,"Question: Does the paper describe safeguards that have been put in place for responsible
838"
SAFEGUARDS,0.9148936170212766,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
839"
SAFEGUARDS,0.9158186864014801,"image generators, or scraped datasets)?
840"
SAFEGUARDS,0.9167437557816837,"Answer: [NA]
841"
SAFEGUARDS,0.9176688251618872,"Justiﬁcation: The model we propose does not carry the risk of misuse; the datasets were
842"
SAFEGUARDS,0.9185938945420906,"selected under fair use conditions, from publicly available sources with undisputed licenses.
843"
SAFEGUARDS,0.9195189639222942,"Therefore, our work does not require additional safeguard protections.
844"
SAFEGUARDS,0.9204440333024977,"Guidelines:
845"
SAFEGUARDS,0.9213691026827012,"• The answer NA means that the paper poses no such risks.
846"
SAFEGUARDS,0.9222941720629048,"• Released models that have a high risk for misuse or dual-use should be released with
847"
SAFEGUARDS,0.9232192414431082,"necessary safeguards to allow for controlled use of the model, for example by re-
848"
SAFEGUARDS,0.9241443108233117,"quiring that users adhere to usage guidelines or restrictions to access the model or
849"
SAFEGUARDS,0.9250693802035153,"implementing safety ﬁlters.
850"
SAFEGUARDS,0.9259944495837188,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
851"
SAFEGUARDS,0.9269195189639223,"should describe how they avoided releasing unsafe images.
852"
SAFEGUARDS,0.9278445883441259,"• We recognize that providing effective safeguards is challenging, and many papers do
853"
SAFEGUARDS,0.9287696577243293,"not require this, but we encourage authors to take this into account and make a best
854"
SAFEGUARDS,0.9296947271045328,"faith effort.
855"
LICENSES FOR EXISTING ASSETS,0.9306197964847364,"12. Licenses for existing assets
856"
LICENSES FOR EXISTING ASSETS,0.9315448658649399,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
857"
LICENSES FOR EXISTING ASSETS,0.9324699352451434,"the paper, properly credited and are the license and terms of use explicitly mentioned and
858"
LICENSES FOR EXISTING ASSETS,0.9333950046253469,"properly respected?
859"
LICENSES FOR EXISTING ASSETS,0.9343200740055504,"Answer: [Yes]
860"
LICENSES FOR EXISTING ASSETS,0.9352451433857539,"Justiﬁcation: In our research, we have carefully credited all the code and data used, provid-
861"
LICENSES FOR EXISTING ASSETS,0.9361702127659575,"ing explicit citations for each. The licenses for this code and data are notably permissive,
862"
LICENSES FOR EXISTING ASSETS,0.937095282146161,"including MIT, BSD 3-clause, and CC BY 4.0. In accordance with these licenses, we have
863"
LICENSES FOR EXISTING ASSETS,0.9380203515263644,"properly acknowledged the contributions of the original authors.
864"
LICENSES FOR EXISTING ASSETS,0.938945420906568,"Guidelines:
865"
LICENSES FOR EXISTING ASSETS,0.9398704902867715,"• The answer NA means that the paper does not use existing assets.
866"
LICENSES FOR EXISTING ASSETS,0.940795559666975,"• The authors should cite the original paper that produced the code package or dataset.
867"
LICENSES FOR EXISTING ASSETS,0.9417206290471786,"• The authors should state which version of the asset is used and, if possible, include a
868"
LICENSES FOR EXISTING ASSETS,0.9426456984273821,"URL.
869"
LICENSES FOR EXISTING ASSETS,0.9435707678075855,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
870"
LICENSES FOR EXISTING ASSETS,0.9444958371877891,"• For scraped data from a particular source (e.g., website), the copyright and terms of
871"
LICENSES FOR EXISTING ASSETS,0.9454209065679926,"service of that source should be provided.
872"
LICENSES FOR EXISTING ASSETS,0.9463459759481961,"• If assets are released, the license, copyright information, and terms of use in the pack-
873"
LICENSES FOR EXISTING ASSETS,0.9472710453283997,"age should be provided. For popular datasets, paperswithcode.com/datasets has
874"
LICENSES FOR EXISTING ASSETS,0.9481961147086031,"curated licenses for some datasets. Their licensing guide can help determine the li-
875"
LICENSES FOR EXISTING ASSETS,0.9491211840888066,"cense of a dataset.
876"
LICENSES FOR EXISTING ASSETS,0.9500462534690102,"• For existing datasets that are re-packaged, both the original license and the license of
877"
LICENSES FOR EXISTING ASSETS,0.9509713228492137,"the derived asset (if it has changed) should be provided.
878"
LICENSES FOR EXISTING ASSETS,0.9518963922294172,"• If this information is not available online, the authors are encouraged to reach out to
879"
LICENSES FOR EXISTING ASSETS,0.9528214616096207,"the asset’s creators.
880"
NEW ASSETS,0.9537465309898242,"13. New Assets
881"
NEW ASSETS,0.9546716003700277,"Question: Are new assets introduced in the paper well documented and is the documenta-
882"
NEW ASSETS,0.9555966697502313,"tion provided alongside the assets?
883"
NEW ASSETS,0.9565217391304348,"Answer: [Yes]
884"
NEW ASSETS,0.9574468085106383,"Justiﬁcation: We have detailed the new datasets employed in this research in Appendix A.4.
885"
NEW ASSETS,0.9583718778908418,"We commit to making these datasets publicly accessible following the anonymity period to
886"
NEW ASSETS,0.9592969472710453,"foster transparency and reproducibility.
887"
NEW ASSETS,0.9602220166512488,"Guidelines:
888"
NEW ASSETS,0.9611470860314524,"• The answer NA means that the paper does not release new assets.
889"
NEW ASSETS,0.9620721554116559,"• Researchers should communicate the details of the dataset/code/model as part of their
890"
NEW ASSETS,0.9629972247918593,"submissions via structured templates. This includes details about training, license,
891"
NEW ASSETS,0.9639222941720629,"limitations, etc.
892"
NEW ASSETS,0.9648473635522664,"• The paper should discuss whether and how consent was obtained from people whose
893"
NEW ASSETS,0.96577243293247,"asset is used.
894"
NEW ASSETS,0.9666975023126735,"• At submission time, remember to anonymize your assets (if applicable). You can
895"
NEW ASSETS,0.967622571692877,"either create an anonymized URL or include an anonymized zip ﬁle.
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685476410730804,"14. Crowdsourcing and Research with Human Subjects
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.969472710453284,"Question: For crowdsourcing experiments and research with human subjects, does the pa-
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703977798334875,"per include the full text of instructions given to participants and screenshots, if applicable,
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971322849213691,"as well as details about compensation (if any)?
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722479185938946,"Answer: [NA]
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973172987974098,"Justiﬁcation: Our work does not involve crowdsourcing nor research with human subjects.
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9740980573543015,"Guidelines:
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9750231267345051,"• The answer NA means that the paper does not involve crowdsourcing nor research
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9759481961147086,"with human subjects.
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768732654949122,"• Including this information in the supplemental material is ﬁne, but if the main contri-
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777983348751156,"bution of the paper involves human subjects, then as much detail as possible should
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787234042553191,"be included in the main paper.
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9796484736355227,"• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805735430157262,"tion, or other labor should be paid at least the minimum wage in the country of the
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814986123959297,"data collector.
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824236817761333,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833487511563367,"Subjects
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842738205365402,"Question: Does the paper describe potential risks incurred by study participants, whether
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851988899167438,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861239592969473,"approvals (or an equivalent approval/review based on the requirements of your country or
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870490286771508,"institution) were obtained?
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879740980573543,"Answer: [NA]
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888991674375578,"Justiﬁcation: Our work does not involve crowdsourcing nor research with human subjects.
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898242368177613,"Guidelines:
920"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907493061979649,"• The answer NA means that the paper does not involve crowdsourcing nor research
921"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916743755781684,"with human subjects.
922"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925994449583718,"• Depending on the country in which research is conducted, IRB approval (or equiva-
923"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935245143385754,"lent) may be required for any human subjects research. If you obtained IRB approval,
924"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944495837187789,"you should clearly state this in the paper.
925"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953746530989824,"• We recognize that the procedures for this may vary signiﬁcantly between institutions
926"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996299722479186,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
927"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972247918593895,"guidelines for their institution.
928"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981498612395929,"• For initial submissions, do not include any information that would break anonymity
929"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990749306197965,"(if applicable), such as the institution conducting the review.
930"
