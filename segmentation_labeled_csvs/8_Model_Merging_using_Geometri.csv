Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.006993006993006993,"Training high-performing large language models (LLMs) from scratch is an expen-
sive and complex task. Model merging techniques offer a more computationally
efficient alternative, where pretrained LLMs are fine-tuned on specific tasks and
then combined to produce a versatile model capable of handling a broad range
of tasks, including reasoning, coding, mathematics, conversation, and tool usage.
Unlike traditional fine-tuning or ensemble methods, our approach to merging is
less computationally intensive. We represent each fine-tuned model with a ""Task
Vector"" relative to a pretrained ""Base LLM"" , derived from the LoRA (Low Rank
Adaptation) weights of the fine-tuned models. By computing the geometric me-
dian of these task vectors in high-dimensional space using Weiszfeld’s iterative
algorithm and adding it to the ""Base LLM"" weights, we create a unified model that
generalizes effectively across tasks. This efficient method achieves state-of-the-art
performance on benchmark tests while reducing computational demands.
Code available at https://github.com/iMmOrTaL2121/geometric_median_
llm_merging.git."
INTRODUCTION,0.013986013986013986,"1
Introduction"
INTRODUCTION,0.02097902097902098,"Model merging, [9] or model fusion, combines the parameters of multiple models with unique
strengths into a single, unified model. Unlike ensemble methods, which require high memory and
processing power, model merging consolidates knowledge into one streamlined model, reducing
computational costs, memory usage, and latency. This efficient technique enhances generalization
across tasks and is ideal for resource-constrained or low-latency environments, as it does not require
access to the original training data or extensive computation and training."
INTRODUCTION,0.027972027972027972,"Figure 1: An illustration of the ensemble learning paradigm versus the model merging paradigm. (a)
T separate models for T tasks, (b) Assemble T separate models for T tasks, (c) A merged model for T
tasks."
INTRODUCTION,0.03496503496503497,"Combining multiple fine-tuned LLMs with a pretrained ""Base LLM"" can yield a model capable
of excelling across a wide range of tasks, including reasoning, coding, mathematics, conversation,
and tool use. In our approach, we introduce a method that represents each fine-tuned LLM as a"
INTRODUCTION,0.04195804195804196,"""task vector"" [4] relative to the Base LLM, utilizing LoRA [2] (Low-Rank Adaptation) weights. By
computing the geometric median of these task vectors through Weiszfeld’s iterative algorithm [1]
and adding it to the Base LLM, we effectively merge the capabilities of different models, achieving
strong performance on benchmark tests."
LIT REVIEW,0.04895104895104895,"2
Background"
LIT REVIEW,0.055944055944055944,"2.1
Task Vectors"
LIT REVIEW,0.06293706293706294,"For our understanding, [4] a ""task"" is instantiated by a dataset and a loss function used for fine-tuning.
Let θpre ∈Rd be the weights of a pre-trained base model, and θft
t ∈Rd the corresponding weights
after fine-tuning on task t. The task vector τt ∈Rd is given by the element-wise difference between
θft
t and θpre, i.e.,
τt = θft
t −θpre."
LIT REVIEW,0.06993006993006994,Figure 2:
LIT REVIEW,0.07692307692307693,"For all operations, the model weights obtained by applying τnew are given by"
LIT REVIEW,0.08391608391608392,"θnew = θ + λτnew,"
LIT REVIEW,0.09090909090909091,"where the scaling term λ is determined using held-out validation sets. In our approach, we set λ = 1."
LIT REVIEW,0.0979020979020979,"2.2
LoRA (Low-Rank Adaptation)"
LIT REVIEW,0.1048951048951049,"LoRA [2]is a method used to efficiently fine-tune large language models (LLMs) by injecting
trainable low-rank matrices into the model’s architecture. In our approach, we have specifically used
PEFT [7](Parameter Efficient Fine-Tuned) models for model merging . This reduces the number of
parameters that need to be updated during fine-tuning, making the process of finetuning faster and
more memory-efficient, while maintaining performance on downstream tasks."
LIT REVIEW,0.11188811188811189,LoRA modifies a weight matrix W in the neural network as follows:
LIT REVIEW,0.11888111888111888,W ′ = W + ∆W
LIT REVIEW,0.1258741258741259,"where W is the frozen pretrained weight matrix of the base model, and ∆W is the low-rank adaptation
learned during fine-tuning."
LIT REVIEW,0.13286713286713286,"∆W is expressed as:
∆W = BA"
LIT REVIEW,0.13986013986013987,"Here, A ∈Rr×d and B ∈Rd×r , where r is much smaller than the dimension d of the original
weight matrix W. This decomposition reduces the number of trainable parameters from d2 to 2dr,
making the process fast and highly memory efficient."
LIT REVIEW,0.14685314685314685,"• LoRA A: This is a low-rank matrix of size r × d that maps the lower-dimensional represen-
tation back to the original dimension d."
LIT REVIEW,0.15384615384615385,"• LoRA B: This is a low-rank matrix of size d × r that is learned during fine-tuning. It
projects the input (or intermediate representations) to a lower-dimensional space of rank r."
IMPLEMENTATION/METHODS,0.16083916083916083,"3
Method"
IMPLEMENTATION/METHODS,0.16783216783216784,"The composition ∆W = BA allows for an efficient update of the original weight matrix W without
needing to directly update W itself . The product of these two low-rank matrices approximates the
weight update that would normally be learned by directly updating W."
IMPLEMENTATION/METHODS,0.17482517482517482,"In our approach, the fine-tuned LLM’s we used had 23 encoder blocks and 23 decoder blocks, and
each encoder or decoder block has LoRA A weights with dimension A ∈R16×2048 and LoRA B
weights with dimension B ∈R2048×16 ."
IMPLEMENTATION/METHODS,0.18181818181818182,"3.1
Computing weights of merged LLM"
IMPLEMENTATION/METHODS,0.1888111888111888,"A transformer block has many encoder and decoder blocks [8]. Also from section 2.2 , we can
say that the matrix ∆W = BA is the corresponding task vector of a given fine-tuned LLM for a
given encoder/decoder block ( parameter matrix of the W ∈R2048×2048) . Similarly we find task
vectors for all fine-tuned LLMs for the same block. We flatten all these task vector matrices to a
high dimensional vector R1×4194304 and then find the geometric median of all these flattened vectors
treating each of these vectors as a point in multidimensional space to get a ""net task vector"" for that
block which is finally added to the corresponding block in the pretrained base LLM after reshaping
to the original size R2048×2048 of block’s parameter .We are finding the geometric median , so that
our model is able to optimally perform in all the tasks."
IMPLEMENTATION/METHODS,0.1958041958041958,"3.2
Geometric Median"
IMPLEMENTATION/METHODS,0.20279720279720279,"The geometric median [1] is a point in multidimensional space that minimizes the sum of distances to
a set of given points. It generalizes the concept of the median from 1D to higher dimensions."
IMPLEMENTATION/METHODS,0.2097902097902098,"Mathematically,given a set of points X = {x1, x2, . . . , xn} in Euclidean space Rd, the geometric
median M is the point that minimizes the sum of Euclidean distances to the given points:"
IMPLEMENTATION/METHODS,0.21678321678321677,"M = arg min
y∈Rd n
X"
IMPLEMENTATION/METHODS,0.22377622377622378,"i=1
∥y −xi∥
(1)"
IMPLEMENTATION/METHODS,0.23076923076923078,where:
IMPLEMENTATION/METHODS,0.23776223776223776,"• M is the geometric median (the point to be found).
• ∥y −xi∥denotes the Euclidean distance between point y and xi."
IMPLEMENTATION/METHODS,0.24475524475524477,"3.3
Weiszfeld Iterative Algorithm"
IMPLEMENTATION/METHODS,0.2517482517482518,"The Weiszfeld algorithm [1] is an iterative method used to compute the geometric median of a set of
points in multi-dimensional space. The geometric median minimizes the sum of Euclidean distances
from a point to a set of given points. This is different from the arithmetic mean, which minimizes the
sum of squared distances. Mathematically, the objective function being minimized is:"
IMPLEMENTATION/METHODS,0.25874125874125875,"f(x) = n
X"
IMPLEMENTATION/METHODS,0.26573426573426573,"i=1
wi∥x −pi∥"
IMPLEMENTATION/METHODS,0.2727272727272727,Where:
IMPLEMENTATION/METHODS,0.27972027972027974,"• x is the point to be found.
• pi are the given points.
• wi are optional weights (if present, they give different importance to the distances from the
point x to each pi).In our approach, we have taken wi=1 for all i .
• ∥x −pi∥is the Euclidean distance between the point x and the point pi."
IMPLEMENTATION/METHODS,0.2867132867132867,"In our approach, we iteratively approximate the geometric median until either 300 iterations are
reached, or the loss function satisfies the condition ."
IMPLEMENTATION/METHODS,0.2937062937062937,"|f(x)| < ϵ
(ϵ = 10−8, in our case)"
RESULTS/EXPERIMENTS,0.3006993006993007,"4
Results"
RESULTS/EXPERIMENTS,0.3076923076923077,"We evaluated our approach using multiple fine-tuned LLMs across various NLP tasks, including
text classification, question answering, text generation, text-to-text generation, sentence similarity,
comprehension and understanding, search and retrieval, natural language generation, paraphrasing,
extraction, and process understanding. The table 1 compares the performance of merging various
number of models using two methods: GeoMed, which computes the geometric median of task
vectors, and WeightAvg, a baseline method that computes the average of all task vectors. The results
indicate that as the number of models merged increases, the evaluation metrics show a substantial
improvement, highlighting the ability of the merged large language model (LLM) to generalize across
multiple tasks. Our method demonstrates effective merging, achieving robust performance across
diverse tasks and indicating enhanced adaptability and generalization."
RESULTS/EXPERIMENTS,0.3146853146853147,"Merge Method
Rouge1
Rouge2
RougeL
RougeLsum
BLEU
GeoMed (20)
0.365169018
0.168960446
0.311908921
0.321912100
0.085560895
WeightAvg (20)
0.363765969
0.167650662
0.310080274
0.309277511
0.083514798
GeoMed (15)
0.363746168
0.168716755
0.310760179
0.311333478
0.083514798
WeightAvg (15)
0.3647378596
0.167141560
0.310224524
0.310449948
0.083514798
GeoMed (6)
0.362287888
0.166269069
0.319156270
0.319113298
0.083994781
WeightAvg (6)
0.361478874
0.169796169
0.310843997
0.311794364
0.082005513
GeoMed (2)
0.355095178
0.164083656
0.317939794
0.299276737
0.089607033
WeightAvg (2)
0.353798629
0.164048906
0.316438994
0.298291665
0.079670733"
RESULTS/EXPERIMENTS,0.32167832167832167,"Table 1: Performance metrics (ROUGE and BLEU) for different model configurations. As shown
in Appendix A.7, several finetuned models were used for various tasks such as Text Classification,
Question Answering, and Sentence Similarity etc."
RESULTS/EXPERIMENTS,0.32867132867132864,"The metrics used are ROUGE and BLEU [3]. We obtained these results by applying our method to
the XSum dataset, which is used for text summarization. This is one of many NLP tasks, and our
method generalizes well across this task."
CONCLUSION/DISCUSSION,0.3356643356643357,"5
Conclusion"
CONCLUSION/DISCUSSION,0.34265734265734266,"We propose an efficient model merging technique using geometric median computation over task
vectors, offering a resource-friendly alternative to traditional fine-tuning and ensemble methods.
Representing each fine-tuned model as a ""Task Vector"" relative to a pretrained ""Base LLM"" and
leveraging LoRA weights, we capture specialized knowledge effectively. By computing the geometric
median with Weiszfeld’s iterative algorithm, we create a unified model that excels across tasks with
reduced computational demands. This method achieves state-of-the-art results on benchmark tests,
combining high performance with scalability."
REFERENCES,0.34965034965034963,References
REFERENCES,0.35664335664335667,"[1] Amir Beck and Shoham Sabach. Weiszfeld’s method: Old and new results. Journal of Optimiza-
tion Theory and Applications, 164:1–40, 2015."
REFERENCES,0.36363636363636365,"[2] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685, 2021."
REFERENCES,0.3706293706293706,"[3] Taojun Hu and Xiao-Hua Zhou. Unveiling llm evaluation focused on metrics: Challenges and
solutions. arXiv preprint arXiv:2404.09135, 2024."
REFERENCES,0.3776223776223776,"[4] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt,
Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint
arXiv:2212.04089, 2022."
REFERENCES,0.38461538461538464,"[5] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out, pages 74–81, 2004."
REFERENCES,0.3916083916083916,"[6] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics, pages 311–318, 2002."
REFERENCES,0.3986013986013986,"[7] George Pu, Anirudh Jain, Jihan Yin, and Russell Kaplan. Empirical analysis of the strengths and
weaknesses of peft techniques for llms. arXiv preprint arXiv:2304.14999, 2023."
REFERENCES,0.40559440559440557,"[8] A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.4125874125874126,"[9] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao.
Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities.
arXiv preprint arXiv:2408.07666, 2024."
OTHER,0.4195804195804196,"A
Appendix / supplemental material"
OTHER,0.42657342657342656,"A.1
Geometric Median"
OTHER,0.43356643356643354,"In geometry, the geometric median [1] of a discrete point set in a Euclidean space is the point
minimizing the sum of distances to the sample points. This generalizes the median, which has the
property of minimizing the sum of distances or absolute differences for one-dimensional data. It
provides a measure of central tendency in higher dimensions.The geometric median minimizes the
sum of the L2 distances of the samples."
OTHER,0.4405594405594406,"The special case of the problem for three points in the plane (that is, m = 3 and n = 2 in the definition
below) is sometimes also known as Fermat’s problem .Its solution is known as the Fermat point of
the triangle formed by the three sample points. The geometric median may in turn be generalized to
the problem of minimizing the sum of weighted distances, known as the Weber problem ."
OTHER,0.44755244755244755,"Figure 3: Given a (proper) triangle, formed by three points a1, a2 and a3, construct three equilateral
triangles such that each contains one of the edges from the triangle a1a2a3. Then, circumscribe each
equilateral triangle. The unique point of intersection of these three circles is the point that yields the
minimum distance to the points a1, a2, and a3; it is called “the Torricelli Point” and denoted by x∗."
OTHER,0.45454545454545453,"A.2
The Fermat-Weber Problem"
OTHER,0.46153846153846156,"The Fermat-Weber problem, described verbally at the beginning of the paper, can be formulated
mathematically as the problem of seeking x ∈Rd that solves min
x ("
OTHER,0.46853146853146854,"f(x) = m
X"
OTHER,0.4755244755244755,"i=1
ωi∥x −ai∥ )"
OTHER,0.4825174825174825,",
(FW)"
OTHER,0.48951048951048953,"where ωi > 0, i = 1, 2, . . . , m are given weights and the vectors a1, a2, . . . , am ∈Rd are given
anchors."
OTHER,0.4965034965034965,"To understand the result that Weiszfeld aimed to prove, let us first write down the expression of the
gradient of the objective function of problem (FW):"
OTHER,0.5034965034965035,"∇f(x) = m
X"
OTHER,0.5104895104895105,"i=1
ωi
x −ai
∥x −ai∥,
x /∈A,
(2)"
OTHER,0.5174825174825175,"where A = {a1, a2, . . . , am} denotes the set of anchors. Note that the gradient is only defined on
points different from the anchors.
Theorem 1 (Weiszfeld’s original result). [1] Suppose that the anchors are not collinear."
OTHER,0.5244755244755245,(a) Problem (FW) has a unique optimal solution.
OTHER,0.5314685314685315,"(b) Let x∗be the optimal solution of problem (FW). If x∗/∈A, then"
OTHER,0.5384615384615384,"∇f(x∗) = m
X"
OTHER,0.5454545454545454,"i=1
ωi
x∗−ai
∥x∗−ai∥= 0.
(3)"
OTHER,0.5524475524475524,"If x∗= ai for some i ∈{1, 2, . . . , m}, then the following inequality holds: m
X"
OTHER,0.5594405594405595,"j=1
j̸=i"
OTHER,0.5664335664335665,"ωj
x∗−aj
∥x∗−aj∥≤ωi.
(4)"
OTHER,0.5734265734265734,"As noted in the introduction, the theorem was not new and was already established by Sturm in 1884.
The anchors a1, a2, . . . , am are said to be collinear if they reside on the same line, i.e., there exist
y, d ∈Rd and t1, t2, . . . , tm ∈R such that ai = y + tid for all i = 1, 2, . . . , m."
OTHER,0.5804195804195804,"A.3
Weiszfeld’s Method"
OTHER,0.5874125874125874,"Theorem 2 (Optimality Condition). [1] The optimality condition ∇f(x∗) = 0 is the necessary and
sufficient condition for unconstrained convex minimization problems at points where the objective
function is differentiable. When anchors are not collinear, the optimal solution is unique and satisfies
the fixed-point relation."
OTHER,0.5944055944055944,"A.4
Fixed-Point Iteration"
OTHER,0.6013986013986014,The optimal solution can be expressed as:
OTHER,0.6083916083916084,"x∗=
Pm
i=1 ωiai/∥x∗−ai∥
Pm
i=1 ωi/∥x∗−ai∥,
(5)"
OTHER,0.6153846153846154,"or in operator form:
x∗= T(x∗),
(6)
where the operator T : Rd \ A →Rd is defined by:"
OTHER,0.6223776223776224,"T(x) :=
Pm
i=1 ωiai/∥x −ai∥
Pm
i=1 ωi/∥x −ai∥.
(7)"
OTHER,0.6293706293706294,"We have thus shown that, for any y ∈Rd \ A,"
OTHER,0.6363636363636364,"y = T(y)
if and only if
∇f(y) = 0.
(8)"
OTHER,0.6433566433566433,"Weiszfeld’s Method
Initialization. x0 ∈Rd \ A.
General Step (k = 0, 1, ...):
xk+1 = T(xk),
(9)
where T(xk) represents the transformation defined in the method."
OTHER,0.6503496503496503,"A.5
Algorithm"
OTHER,0.6573426573426573,"Algorithm 1 Weiszfeld Algorithm for Minimizing f(x) = Pn
i=1 wi∥x −pi∥"
OTHER,0.6643356643356644,"Input: Points pi ∈Rd, i = 1, . . . , n, with optional weights wi > 0. Initial guess x(0) ∈Rd."
OTHER,0.6713286713286714,"1: k ←0
2: repeat
3:
if x(k) ̸= pi for all i ∈{1, . . . , n} then
4:
Compute the weighted average:"
OTHER,0.6783216783216783,x(k+1) ←
OTHER,0.6853146853146853,"Pn
i=1
wi
∥x(k)−pi∥pi
Pn
i=1
wi
∥x(k)−pi∥"
OTHER,0.6923076923076923,"5:
else
6:
Apply a small perturbation to avoid division by zero.
7:
end if
8:
k ←k + 1
9: until convergence (i.e., ∥x(k+1) −x(k)∥< ϵ for a small ϵ)
Output: Geometric median x(k+1)."
OTHER,0.6993006993006993,"A.6
Evaluation Metrics"
OTHER,0.7062937062937062,"We mostly use ROUGE, BLEU, Brevity Penalty as our Evaluation Metrics for summarization,question
answering and text-generation tasks, etc.[3]"
OTHER,0.7132867132867133,"A.6.1
ROUGE (Recall-Oriented Understudy for Gisting Evaluation)"
OTHER,0.7202797202797203,"ROUGE [5] is mostly recall-focused, meaning it evaluates how much of the reference (ground truth)
content is captured by the generated content. There are different variations:"
OTHER,0.7272727272727273,"ROUGE-N:
Measures the overlap of n-grams between the generated text and reference text."
OTHER,0.7342657342657343,ROUGE-N = P
OTHER,0.7412587412587412,"ngram∈reference Countmatch(ngram)
P"
OTHER,0.7482517482517482,"ngram∈reference Count(ngram)
(10)"
OTHER,0.7552447552447552,"where Countmatch(ngram) is the number of n-grams that match between the generated text and
reference, and Count(ngram) is the total count of n-grams in the reference."
OTHER,0.7622377622377622,"ROUGE-L:
Based on the longest common subsequence (LCS), ROUGE-L captures sentence
fluency and grammatical structure."
OTHER,0.7692307692307693,"ROUGE-L = LCS(X, Y )"
OTHER,0.7762237762237763,"Length(Y )
(11)"
OTHER,0.7832167832167832,"where LCS(X, Y ) is the length of the longest common subsequence between the generated text X
and reference Y , and Length(Y ) is the length of the reference text."
OTHER,0.7902097902097902,"A.6.2
BLEU (Bilingual Evaluation Understudy)"
OTHER,0.7972027972027972,"BLEU [6] is primarily a precision-focused metric, measuring how much of the generated text matches
the reference text on an n-gram basis. It’s especially popular in machine translation."
OTHER,0.8041958041958042,"Formula:
BLEU score combines n-gram precision for multiple n-grams (e.g., unigrams, bigrams,
etc.) and takes the geometric mean of these scores, followed by a brevity penalty (discussed below)."
OTHER,0.8111888111888111,"For N-gram precision (up to 4-grams, typically): Pn = P"
OTHER,0.8181818181818182,"ngram∈candidate Countclip(ngram)
P"
OTHER,0.8251748251748252,"ngram∈candidate Count(ngram)
(12)"
OTHER,0.8321678321678322,"where Countclip(ngram) is the clipped count of each n-gram in the candidate that appears in the
reference (capped at the reference’s count)."
OTHER,0.8391608391608392,The BLEU score is then calculated as:
OTHER,0.8461538461538461,"BLEU = Brevity Penalty × exp N
X"
OTHER,0.8531468531468531,"n=1
wn log Pn ! (13)"
OTHER,0.8601398601398601,"where wn is the weight for each n-gram, often set equally (e.g., wn = 1"
OTHER,0.8671328671328671,4 for 4-gram BLEU).
OTHER,0.8741258741258742,"A.6.3
Brevity Penalty (BP)"
OTHER,0.8811188811188811,"BLEU’s Brevity Penalty [6] penalizes short translations that could artificially inflate precision scores.
It ensures that a good BLEU score reflects both the precision of n-grams and the length of the
generated output."
OTHER,0.8881118881118881,"Brevity Penalty (BP) = 
 "
OTHER,0.8951048951048951,"1
if c > r"
OTHER,0.9020979020979021,"exp

1 −r"
OTHER,0.9090909090909091,"c

if c ≤r
(14)"
OTHER,0.916083916083916,where c is the length of the generated text (candidate) and r is the length of the reference text.
OTHER,0.9230769230769231,"• If c > r, there’s no penalty (BP = 1).
• If c ≤r, a penalty proportional to the ratio r"
OTHER,0.9300699300699301,"c is applied, reducing the BLEU score for overly
short candidates."
OTHER,0.9370629370629371,"A.6.4
Brevity Penalty in Language Generation Tasks"
OTHER,0.9440559440559441,"In language generation tasks, like summarization or translation, the brevity penalty controls how
much shorter generated outputs are penalized relative to a reference. Here’s what each direction
typically achieves:"
OTHER,0.951048951048951,"• More brevity penalty: A higher penalty (often achieved by setting a lower brevity penalty
score) means the model will be discouraged from generating shorter responses and thus
produce longer, more complete outputs. This can be useful when concise outputs risk leaving
out important information.
• Less brevity penalty: A lower penalty (or higher score) allows shorter responses, which
can be ideal for concise outputs where detail isn’t as critical, like summaries or simpler
responses."
OTHER,0.958041958041958,"In summarization tasks, lower brevity penalties often encourage conciseness. For tasks needing fuller
detail (like certain types of translations or responses), higher brevity penalties are better to avoid
cutting off important information. In tools like BLEU or ROUGE, one can often adjust brevity to
fine-tune according to the goal."
OTHER,0.965034965034965,"A.7
Finetuned Models Used"
OTHER,0.972027972027972,"No.
Finetuned Model
1
lorahub/flan_t5_xl-dbpedia_14_given_list_what_category_does_the_paragraph_belong_to
2
lorahub/flan_t5_xl-wiki_qa_Topic_Prediction_Question_Only
3
lorahub/flan_t5_xl-anli_r2
4
lorahub/flan_t5_xl- web_questions_question_answer
5
lorahub/flan_t5_xl-duorc_SelfRC_question_answering
6
lorahub/flan_t5_xl-adversarial_qa_dbert_question_context_answer
7
lorahub/flan_t5_xl-wiki_qa_Is_This_True_
8
lorahub/flan_t5_xl-gem_e2e_nlg
9
lorahub/flan_t5_xl-wiki_hop_original_explain_relation
10
lorahub/flan_t5_xl-duorc_SelfRC_title_generation
11
lorahub/flan_t5_xl-glue_mrpc
12
lorahub/flan_t5_xl-glue_cola
13
lorahub/flan_t5_xl-wiki_bio_comprehension
14
lorahub/flan_t5_xl-wiki_bio_key_content
15
lorahub/flan_t5_xl-wiki_bio_guess_person
16
lorahub/flan_t5_xl-wiki_bio_who
17
lorahub/flan_t5_xl-wiki_qa_found_on_google
18
lorahub/flan_t5_xl-gem_web_nlg_en
19
lorahub/flan_t5_xl-duorc_ParaphraseRC_extract_answer
20
lorahub/flan_t5_xl-duorc_SelfRC_extract_answer
21
lorahub/flan_t5_xl-wiqa_what_might_be_the_last_step_of_the_process"
OTHER,0.9790209790209791,Table 2: List of Finetuned Models Used to obtain results of table 1
OTHER,0.986013986013986,"Number of models
Base Model
Finetuned Models
20
google/flan-t5-xl
(1)(2)(3)(4)(5)(6)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17)(18)(19)(20)(21)
15
google/flan-t5-xl
(1)(2)(3)(4)(5)(6)(8)(9)(10)(13)(14)(18)(19)(20)(21)
6
google/flan-t5-xl
(8)(9)(10)(18)(19)(20)
2
google/flan-t5-xl
(7)(8)"
OTHER,0.993006993006993,"Table 3: As shown in Table 2, several finetuned models were used for various tasks such as Text
Classification, Question Answering, and Sentence Similarity etc."
