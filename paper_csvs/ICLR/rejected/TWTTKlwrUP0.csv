Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.006211180124223602,"In the past decade, there has been exponentially growing interest in the use of
observational data collected as a part of routine healthcare practice to determine
the effect of a treatment with causal inference models. Validation of these mod-
els, however, has been a challenge because the ground truth is unknown: only
one treatment-outcome pair for each person can be observed. There have been
multiple efforts to ﬁll this void using synthetic data where the ground truth can
be generated. However, to date, these datasets have been severely limited in their
utility either by being modeled after small non-representative patient populations,
being dissimilar to real target populations, or only providing known effects for
two cohorts (treated vs control). In this work, we produced a large-scale and real-
istic synthetic dataset that supports multiple hypertension treatments, by modeling
after a nationwide cohort of more than 250, 000 hypertension patients’ multi-year
history of diagnoses, medications, and laboratory values. We designed a data
generation process by combining an adapted ADS-GAN model for ﬁctitious pa-
tient information generation and a neural network for treatment outcome genera-
tion. Wasserstein distance of 0.35 demonstrates that our synthetic data follows a
nearly identical joint distribution to the patient cohort used to generate the data.
Our dataset provides ground truth effects for about 30 hypertension treatments on
blood pressure outcomes. Patient privacy was a primary concern for this study;
the ϵ-identiﬁability metric, which estimates the probability of actual patients being
identiﬁed, is 0.008%, ensuring that our synthetic data cannot be used to identify
any actual patients. To demonstrate its usage, we tested the bias in causal effect
estimation of ﬁve well-established models using this dataset. The approach we
used can be readily extended to other types of diseases in the clinical domain, and
to datasets in other domains as well."
INTRODUCTION,0.012422360248447204,"1
INTRODUCTION"
INTRODUCTION,0.018633540372670808,"In health care, studying the causal effect of treatments on patients is critical to advance personal-
ized medicine. Observing an association between a drug (exposure or treatment) and subsequent
adverse or beneﬁcial event (outcome) is not enough to claim the treatment is indeed the cause of the
observed outcome due to the existence of confounding variables, deﬁned as factors that affect both
the treatments and outcomes. Randomized clinical trials (RCTs) have been the gold standard for
estimating causal relationships between intervention and outcome. However, RCTs are sometimes
not feasible due to logistical, ethical, or ﬁnancial considerations. Further, randomized experiments
may not always be generalizable, due to the restricted population used in the experiments. In the
past decade, observational data has become a viable alternative to RCTs to infer causal treatment
effects due to both the increasingly available patient data captured in Electronic Health Records
(EHRs) (Henry et al. (2016)) and the remarkable advances of machine learning techniques and ca-
pabilities. Typically, EHRs capture potential confounding factors such as race, gender, geographic
location, eventual proxies of social determinants of health, as well as medical characteristics such
as comorbidities and laboratory results."
INTRODUCTION,0.024844720496894408,"Many causal inference models have been proposed to estimate treatment effects from observational
data. Validation of these models with realistic benchmarks, however, remains a fundamental chal-"
INTRODUCTION,0.031055900621118012,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.037267080745341616,"lenge due to three reasons. First, the ground truth of treatment effects in a realistic setting is un-
known. In real world, we can not compute the treatment effect by directly comparing the potential
outcomes of different treatments because of the fundamental problem of causal inference: for a
given patient and treatment, we can only observe the factual, deﬁned as the patient outcome for the
given treatment, but not the counterfactual, deﬁned as the patient outcome if the treatment had been
different. Second, legal and ethical issues around un-consented patient data and privacy created a
signiﬁcant barrier in accessing routinely EHRs by the machine learning community. In order to mit-
igate the legal and ethical risks of sharing sensitive information, de-identiﬁcation of patient records
is a commonly used practice. However, previous work has shown that de-identiﬁcation is not sufﬁ-
cient for avoiding re-identiﬁcation through linkage with other identiﬁable datasets Sweeney (1997);
Emam et al. (2011); Malin & Sweeney (2004). Third, most publicly available datasets support either
binary or very few treatments, while there has been growing literature developing techniques with
multiple treatments in recent years (Lopez & Gutman (2017))."
INTRODUCTION,0.043478260869565216,"To address these challenges, in this work we generated a large-scale and realistic patient dataset that
mimics real patient data distributions, supports multiple treatments, and provides ground truth for
the effects of these treatments. The datasets we generated are synthetic patients with hypertension
modeled on a massive nationwide cohort patients’ history of diagnoses, medications, and laboratory
values. We designed a data generation process by adapting an Anonymization Through Data Synthe-
sis Using Generative Adversarial Networks (ADS-GAN by Yoon et al. (2020))model for ﬁctitious
patient information generation and using a neural network for treatment outcome generation. The
synthetic dataset demonstrates strong similarity to the original dataset as measured by the Wasser-
stein distance. In addition, we ensure that the original patients’ privacy is preserved so that our
dataset can be made available to the research community to evaluate causal inference models."
INTRODUCTION,0.049689440993788817,"We demonstrated the use of the synthetic data by applying our dataset to evaluate ﬁve models:
one inverse probability treatment weighting (IPTW) model, one propensity matching model, one
propensity score stratiﬁcation model all introduced by Rosenbaum & Rubin (1983), and two models
in the doubly robust family (Foster & Syrgkanis (2020))."
INTRODUCTION,0.055900621118012424,"To our knowledge, this is the ﬁrst large scale dataset that mimics real data joint distributions with
multiple treatments and known causal effects. The approach we used can be readily extended to
other types of diseases in the clinical domain, and to datasets in other domains as well."
INTRODUCTION,0.062111801242236024,"The rest of this paper is organized as follows. In Section 2, we discuss related works. The details of
our method are presented in Section 4. In Section 5, we discuss the evaluation metrics for the quality
of the data and results of evaluating several established causal inference models with the data. We
discuss the limitations of the work in Section 6 and conclude the paper in Section 7."
RELATED WORK,0.06832298136645963,"2
RELATED WORK"
RELATED WORK,0.07453416149068323,"Our work is related to several existing works on publicly available databases, ﬁctitious patient record
creations, and data generation processes."
PUBLICLY AVAILABLE DATASETS,0.08074534161490683,"2.1
PUBLICLY AVAILABLE DATASETS"
PUBLICLY AVAILABLE DATASETS,0.08695652173913043,"First used in (Hill (2011)), the Infant Health and Development Program (IHDP) is a randomized con-
trolled study designed to evaluate the effect of home visit from specialist doctors on the cognitive
test scores of premature infants. It contains 747 subjects and 25 variables that describe the charac-
teristics of the infants and their mothers. The Jobs dataset by LaLonde (1986) is a benchmark used
by the causal inference community, where the treatment is job training and the outcomes are income
and employment status after training. The Twins dataset, originally used for evaluating causal infer-
ence in (Louizos et al. (2017); Yao et al. (2018)), consists of samples from twin births in the U.S.
between the years 1989 and 1991 provided in (Almond et al. (2005)). The Annual Atlantic Causal
Inference Conference (ACIC) data challenge provides an opportunity to compare causal inference
methodologies across a variety of data generating processes."
PUBLICLY AVAILABLE DATASETS,0.09316770186335403,"Some of the above mentioned datasets do not provide true causal effects. Others are small in size
so the models validated on such datasets may perform very differently in a more general real-world
setting. All the datasets created for ACIC challenge were designed speciﬁcally for competitions. The
covariates in the data are either drawn from publicly available databases, or simulated. In the former"
PUBLICLY AVAILABLE DATASETS,0.09937888198757763,Under review as a conference paper at ICLR 2022
PUBLICLY AVAILABLE DATASETS,0.10559006211180125,"case, the datasets are limited by small populations (Du et al. (2021)) and arbitrarily designed data
generation processes, which did not aim to capture any real-world causal relationships (Karavani
et al. (2019)). In the latter case, the distribution of the data is not realistic, i.e, dissimilar to the
distribution of any real dataset."
EHR DATA GENERATION,0.11180124223602485,"2.2
EHR DATA GENERATION"
EHR DATA GENERATION,0.11801242236024845,"Walonoski et al. (2018) generated synthetic EHRs based on publicly available information. The
focus of their work is on generating the life cycle of a patient and how a disease evolves over
time. Goncalves et al. (2020) evaluated three synthetic data generation models–probabilistic models,
classiﬁcation-based imputation models, and generative adversarial neural networks–in generating
realistic EHR data. Tucker et al. (2020) used a Bayesian network model to generate synthetic data
based on the Clinical Practice Research Datalink (CPRD) in the UK. Benaim et al. (2020) evaluated
synthetic data produced from 5 contemporary studies using MDClone. Wang et al. (2021) proposed a
framework to generate and evaluate synthetic health care data, and the key requirements of synthetic
data for multiple purposes. All of these works focus on EHR data generation producing patient
variables but without ground truth for causal effects. In contrast, the focus of our work is not only
on generating patient variables, but producing ground truth for causal effects as well."
POTENTIAL OUTCOME GENERATION,0.12422360248447205,"2.3
POTENTIAL OUTCOME GENERATION"
POTENTIAL OUTCOME GENERATION,0.13043478260869565,"To validate their models, many researchers such as (Schuler & Rose (2017)) created synthetic co-
variates and produced potential outcomes with a designed data generation process. Such datasets
are not designed to approximate any real data distributions. Franklin et al. (2014); Neal et al. (2020)
generated potential outcomes from covariates with known causal effects, but without any regard to
patient privacy. We addressed the critical issue of patient privacy concerns so our data can be made
available for the research community to evaluate their models."
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA,0.13664596273291926,"3
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA"
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA,0.14285714285714285,"To make our synthetic data realistic, we generate the data based on a real-world patient claim
database from a large insurance company in the United States. This database contains 5 billion
insurance claims (diagnoses, procedures, and drug prescriptions or reﬁlls) and lab test results from
56.4 million patients who subscribed to the company’s service within a 5-year time period between
December 2014 and December 2020. From this database, we extracted a subset of patients affected
by hypertension. We chose hypertension because there are a large number of related claims, making
it easier to learn the data distribution. In addition, since it is a condition affecting nearly half of
adults in the United States (116 million, or 47%), our generated dataset can be directly used for
clinical researchers to develop and evaluate their models for this important disease. Patients are
included in the dataset if they have a medical claim indicating hypertension (ICD code I10, I11.9,
I12.9, and I13.10) or currently treated with anti-hypertensive medications. We exclude patients from
the dataset if they are age <18 or age >85, affected by white coat hypertension, secondary hyperten-
sion, malignant cancers, dementia, or are pregnant. After applying the above mentioned inclusion
and exclusion criteria, we have about 1.6 million patients included in this study. We further exclude
patients treated with a combination of drugs rather than a single drug. We then rank the drugs by the
number of patients treated with each drug, and only keep patients treated with one of the 28 most
popular drugs. These ﬁltering steps produce about 262, 000 patients in the study. The distribution of
this dataset is then learned and used to generate synthetic patients, viewed as samples drawn from
the learned distribution."
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA,0.14906832298136646,"The patients’ diagnoses and treatment history and how their conditions evolve over time are captured
by trajectory data consisting of labs, diagnoses and their corresponding dates. For the convenience of
data processing and analysis, we convert the trajectory data into tabular data with rows representing
different patients (samples) and columns representing patient features (variables) including patient
demographics, diagnoses, medications and labs. In Table. 1, we list and brieﬂy describe these 60
patient variables: 2 variables (F1) describing the systolic blood pressure before the treatment and the
date it was measured, 2 variables (F2) describing the same metric but after the treatment, 3 variables
(F3) indicating current and prior drug usage and reﬁll information, 4 variables (F4) describing patient"
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA,0.15527950310559005,Under review as a conference paper at ICLR 2022
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA,0.16149068322981366,"Family var.
Var. names
Description"
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA,0.16770186335403728,"F1
date-, lab-
First lab result and date
F2
date+, lab+
Second lab result and date
F3
drugs, prior drugs, last reﬁll
Drugs’ info
F4
age, gndr cd, race cd, ethncty cd
Age/Gender/Ethnicity
F5
lab measurement results and date
11 lab measurements and date
F6
safety morbs, morbs prior
Current and previous comorbidities
F7
zip cd, total pop, p female, median income etc
Zip code and related statistics
F8
trajectory index, mcid
Meta-information"
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA,0.17391304347826086,"Table 1: The patient claim dataset on hypertension contains 1, 618, 363 observations and 60 vari-
ables. Above the family of such variables are listed and brieﬂy described."
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA,0.18012422360248448,"basic information (age, gender, ethnicity), 30 variables (F5) indicating laboratory measurements, 2
variables (F6) indicating the presence or absence of comorbid conditions deﬁned by the Charlson
Comorbidity Index (Charlson et al. (1987)), 15 variables (F7) describing the patient’s zip code, the
racial makeup and income levels in the patient’s zip code tabulation area (ZCTA), 2 variables (F8)
indicating meta information."
PATIENT CLAIM DATA AND INCLUSION EXCLUSION CRITERIA,0.18633540372670807,"In this study, we are interested in the causal effects of anti-hypertensive drugs (current drugs of F3)
on patient outcomes, measured as the difference between the ﬁrst (F1) and second lab results (F2)."
APPROACH,0.19254658385093168,"4
APPROACH"
APPROACH,0.19875776397515527,"To generate the synthetic data, we ﬁrst generate the patient variables using an adapted ADS-GAN
model, then generate the treatment outcomes using a neural network. Our approach can be concep-
tually decomposed into four steps described below."
APPROACH,0.20496894409937888,"4.1
STEP 1: DATA PREPROCESSING"
APPROACH,0.2111801242236025,"Since we generate the synthetic data from the patient claim data extracted in Section 3, in this step
we preprocess the patient claim data and prepare it for subsequent steps. Described in Table 1, this
dataset are of mixed data types: integers (e.g., age), ﬂoats (e.g., lab values), categorical values (e.g.,
drugs), and dates. Further, the values and dates of a lab test are missing for some patients if the
lab test was not ordered by the doctors for these patients. In this step, we clean, one-hot encode,
transform and standardize the data so that all the variable are transformed into numerical values in
[0, 1] range. We also add a binary feature for each lab test to indicate missing lab values and dates.
The resulted dataset has 221 features and we call it the original dataset, to be distinguished from the
synthetic dataset. The details of this step are described in Appendix A.1."
APPROACH,0.21739130434782608,"4.2
STEP 2: GENERATION OF OBSERVED VARIABLES USING ADS-GAN"
APPROACH,0.2236024844720497,"In this step we generate synthetic patients characterized by the same variables as listed in Table 1.
We want to achieve two goals in this step: to make the synthetic data as realistic as possible and to
make sure the probability of identifying any actual patients in the original dataset from the synthetic
dataset is very low. We quantitatively deﬁne the identiﬁability in Deﬁnition 2, and the realisticity as
the Wasserstein distance (Gulrajani et al. (2017)) between the distribution of the synthetic dataset
and that of the real dataset it is modeled after."
APPROACH,0.22981366459627328,"There is a trade-off between identiﬁability and realisticity of generated data.
Frameworks like
MedGan (Choi et al. (2018)) and WGAN-GP (Arjovsky et al. (2017)) do not explicitly deﬁne and
allow to control the identiﬁability levels. Therefore, we evaluated generative models that allow to
explicitly control such trade-off, e.g. the ADS-GAN (Yoon et al. (2020)), PATE-GAN (Jordon et al.
(2019)) and DP-GAN (Xie et al. (2018)). ADS-GAN proved to consistently outperform the bench-
marks across the entire range of identiﬁability levels on both the MAGGIC and the three UNOS
transplant datasets. It is also based on a measurable deﬁnition for identiﬁability. Another advan-
tage of ADS-GAN is the use of Wasserstein distance to measure the similarity between two high"
APPROACH,0.2360248447204969,Under review as a conference paper at ICLR 2022
APPROACH,0.2422360248447205,"dimensional joint distributions. We ﬁnally selected ADS-GAN and adapted it to generate the patient
variables in our study."
APPROACH,0.2484472049689441,"Let us denote the original dataset obtained in Section 4.1 as D = {xi}N
i=1, where xi ∈X ⊆Rd and"
APPROACH,0.2546583850931677,"xi =

x(1)
i , x(2)
i
. . . , x(d)
i

, with x(j)
i
∈X (j) ⊆R representing the j-th feature of a patient. N is
the number of samples and d is the number of features of each sample."
APPROACH,0.2608695652173913,"Let PX denote the underlying distribution from which each xi is drawn and let z ∼PZ be drawn
from a multi-variate Gaussian distribution. The framework of ADS-GAN is to train a generator
G : X × Z →X and a discriminator D : X →R in an adversarial fashion: the generator G which
produces synthetic patient variables ˆxi = G (x, z) where ˆxi ∈ˆX ⊆Rd ensures that the synthetic
dataset ˆD = {ˆxi} are not too close to D as measured by the ϵ-identiﬁability deﬁned below; on the
other hand, the discriminator D which measures the distance between two distributions ensures that
the distribution of generated variables P ˆ
X is indistinguishable from the distribution of real variables
PX.
Deﬁnition 1. We deﬁne the weighted Euclidean distance U (xi, xj) between xi and xj as"
APPROACH,0.2670807453416149,"U (xi, xj) = ∥w (xi −xj)∥,"
APPROACH,0.2732919254658385,where w is a weight vector. Then ri is deﬁned as
APPROACH,0.2795031055900621,"ri =
min
xj∈D/xi U (xi, xj) ,"
APPROACH,0.2857142857142857,"where D /xj represents the dataset D without xi. From the deﬁnition, ri is the weighted minimum
distance between xi and any other observations in D."
APPROACH,0.2919254658385093,We similarly deﬁne ˆri as
APPROACH,0.2981366459627329,"ˆri = min
ˆxj∈ˆ
D
U (xi, ˆxj) ,"
APPROACH,0.30434782608695654,which is the weighted minimum distance between xi and the synthetic samples in ˆD.
APPROACH,0.3105590062111801,Deﬁnition 2. The ϵ-identiﬁability of dataset D from ˆD is deﬁned as
APPROACH,0.3167701863354037,"ϵ = I

D, ˆD

= 1 N X i"
APPROACH,0.32298136645962733,"
1(ri>ˆri)
"
APPROACH,0.32919254658385094,where 1 is an indicator function.
APPROACH,0.33540372670807456,"To calculate the weight vector w, we ﬁrst calculate the discrete entropy of the j-th feature, i.e."
APPROACH,0.3416149068322981,"H

X(j)
= −
X"
APPROACH,0.34782608695652173,"x(j)∈X (j)
P

X(j) = x(j)
log
h
P

X(j) = x(j)i"
APPROACH,0.35403726708074534,"The weight for a feature is then calculated as the inverse of H
 
X(j)
. In reality, if a patient can be
re-identiﬁed, the re-identiﬁcation is more likely through rare characteristics or medical conditions of
a patient. Calculating the weight this way ensures that the rare features of a patient are given more
weight, correctly reﬂecting the risk of re-identiﬁcation associated with different features."
APPROACH,0.36024844720496896,"We base the discriminator D on Wasserstein GAN with gradient penalty (Gulrajani et al. (2017))
(WGAN-GP), which adopts Wasserstein distance between P ˆ
X and PX, and deﬁnes the loss LD for
the discriminator D as"
APPROACH,0.36645962732919257,"LD= Ex∼PX,ˆx∼P ˆ
X
h
D (x) −D (ˆx) −µ (∥∇˜xD (˜x)∥2 −1)2i
(1)"
APPROACH,0.37267080745341613,"where ˜x belongs to a random interpolation distribution between PX and P ˆ
X and µ is a further hyper-
parameter that we set to 10 based on previous work (Gulrajani et al. (2017)). We implement both
the generator and the discriminator using multi-layer perceptrons."
APPROACH,0.37888198757763975,"To train the generator G, we need to compute the ϵ-identiﬁability by computing ri and ˆri for every
sample, which is computationally expensive. To solve the problem, Yoon et al. (2020) made a
simplifying assumption that G (x, z) is the closest data point to x. However, this assumption can be
violated during the training of the network that maximizes the distance between G (x, z) and x."
APPROACH,0.38509316770186336,Under review as a conference paper at ICLR 2022
APPROACH,0.391304347826087,"We here introduce a contrastive loss (triplet ranking loss, Schroff et al. (2015)) item, which is deﬁned
as
Ucon (x, x′, z) = max (0, U (x, G (x, z)) −U (x′, G (x, z))) .
(2)"
APPROACH,0.39751552795031053,"Then, the ﬁnal identiﬁability loss function LI is
LI =Ex∼PX,z∼Pz [−U (x, G (x, z))] + βEx,x′∼PX [Ucon (x, x′, z)] .
(3)
Similar to (Yoon et al. (2020)), this loss also assumes that G (x, z) is the closest data point to x.
However, a penalty on the loss function will be imposed if this assumption is violated when the
generated sample G (x, z) is closer to x′, a randomly drawn sample from dataset D, than to x. The
strength of the penalty term is controlled by β."
APPROACH,0.40372670807453415,"In the ﬁnal optimization problem, we minimize G and maximize D simultaneously, written as
G∗, D∗= arg min
G max
D [LD + λLI]
(4)"
APPROACH,0.40993788819875776,"where λ is a hyper-parameter that controls the trade-off between the two objectives. Once trained,
the adapted ADS-GAN model can be used to produce synthetic data set ˆD."
APPROACH,0.4161490683229814,"4.3
STEP 3: DATA GENERATION MODEL AND CAPTURED CAUSAL EFFECTS"
APPROACH,0.422360248447205,"We need to determine a data generation model and proper treatment effects to use in the data gen-
eration process to produce the potential outcomes of the synthetic data, i.e, the factuals and coun-
terfactuals. Many researchers use arbitrary functions and arbitrary treatment effects. For example,
Schuler & Rose (2017) used a linear function as the data generation process and set the treatment
effects arbitrarily. To improve upon such an approach, in this work we train a neural network model
from the original dataset aiming to capture the mapping from patient covariates to outcomes as well
as the treatment effects. The captured treatment effects are not necessarily the true effects, but serve
as the ground truth in the synthetic data when the data is used to evaluate causal inference models
because the patient outcomes in the synthetic data are generated from these causal effects."
APPROACH,0.42857142857142855,"We ﬁrst partition the domain of observed variables X ⊆Rd into the covariate domain XC ⊆Rdc,
the treatment domain XT ⊆Rdt and the outcome domain Xo ⊆R, so that d ≥dc + dt + 1. The
covariates are all the patient variables excluding drugs, prior drugs, zip code, and lab+. Treatments
are the drugs. Outcome is the difference between lab+ and lab-. In this step, each treatment ti ∈XC
is one-hot encoded and represented by a dt dimensional vector, where dt is the number of treatments.
Given a collection of N observations, we use Yi (ti) ∈R to denote the potential outcome of the i-th
individual, if treated with the treatment ti ∈XC. We assume that (Yi, ti, xi) ∈R × XC × XC
are independent and identically distributed, and that the three fundamental assumptions for causal
inference (Rosenbaum & Rubin (1983)) in Appendix A.2 are satisﬁed."
APPROACH,0.43478260869565216,"Following (Lopez & Gutman (2017); Shalit et al. (2017)), given x ∈XC and ti, t0 ∈XC, where t0
is the placebo, the individual-level treatment effect (ITE) of ti can be deﬁned as
τti (x) :=E [Y (ti) −Y (t0) |x)] .
(5)
Hence, the population average treatment effect for treatment ti can be deﬁned as"
APPROACH,0.4409937888198758,"ATEti =E [Y (ti) −Y (t0)] =
Z"
APPROACH,0.4472049689440994,"XC
τti (x) p (x) dx.
(6)"
APPROACH,0.453416149068323,"So the data generation process can be modeled as Y = Ω(x, t), where Ω: XC × T →R. The
true form of Ωis unknown and can be complicated. Here we make a simplifying assumption that
the representation learned from the covariate domain is separated from the representation learned
from the treatment domain. Speciﬁcally, let Φ : XC →R be a representation function and R be the
representation space. We deﬁne Q : R×T →R so that Ω(x, ti) = Q (Φ (x) , ti). Also, we denote
with τ Q
ti (x) the treatment effect estimate of Q."
APPROACH,0.45962732919254656,"With simpliﬁed Ω, we propose a neural network architecture shown in Figure 1 that is able to
capture Ω, Φ, and at the same time, calculate the treatment effects. For the covariate domain XC,
the network is a fully connected feed-forward neural network with Relu as the activation function for
all the neurons. For the treatment domain XT , the inputs are encoded treatments directly connected
to a neuron with a linear activation. The loss function is the standard mean square error (MSE). We
apply a dropout to all the layers and apply a gentle L2 regularization to all the weights of the neural
network."
APPROACH,0.4658385093167702,Under review as a conference paper at ICLR 2022
APPROACH,0.4720496894409938,"Figure 1: Neural network architec-
ture for patient outcome generation
and causal effect calculation."
APPROACH,0.4782608695652174,"The model Ωis trained on the original dataset described in sec-
tion 3, where we have one factual for each observation. Due
to the separation of the covariate domain and treatment do-
main, and with the particular architecture of the ANN shown
in Figure 1, the neural network weights for treatment connec-
tions can be interpreted as the causal treatment effects. Since
there is no interaction between the covariates and treatments,
the individual treatment effects and population average treat-
ment effects are the same. Indeed, suppose wi is the weight for
treatment ti, one can show that wi is indeed the τti in Equation
5 and the ATEti in Equation 6."
APPROACH,0.484472049689441,"4.4
STEP 4: GENERATION OF
FACTUALS AND COUNTERFACTUALS ON SYNTHETIC DATA"
APPROACH,0.4906832298136646,"The domain of variables and all its partitions are the same for
the real dataset D and the synthetic dataset ˆD = { ˆxi : ˆxi = G (xi, z) , xi ∈D, z ∼PZ}N
i=1. Hence,
the neural network trained on the original dataset in Step 4.3 can be fed with the synthetic patient
variables generated in Step 4.2. The neural network outputs are served as the treatment outcomes
for the synthetic data."
APPROACH,0.4968944099378882,"Once trained, this neural network is capable of generating all factual and counterfactual treatment
outcomes for the synthetic data. For any synthetic patient with covariate ˆxj ∈Xc, the potential
outcome of any treatment ti ∈XC can be generated as ˆYj (ti) = Ω( ˆxj, ti) = Ω(Φ ( ˆxj) , ti) .
However, instead of generating the potential outcomes of all possible treatments in XC, in this work
we only generate two potential outcomes for each patient: the factual outcome corresponding to the
treatment produced by the ADS-GAN model, and the counterfactual outcome if the patient had not
received any treatment. The reason that we do not want to produce all the counterfactual outcomes
is that we want to limit the treatment for each patient only to the one generated by the ADS-GAN
model, in order to preserve the treatment assignment mechanism learned from the original dataset."
APPROACH,0.5031055900621118,"There is a distinction between the assumptions made in Section 4.3 in determining the treatment
effects and the assumptions that our synthetic dataset actually satisﬁes. Speciﬁcally, our synthetic
dataset satisﬁes the SUTVA and unconfoundedness assumption, as we did not model the interac-
tions between patients and we provided all the patient variables in the dataset used to generate the
outcomes. Whether it satisﬁes the positivity assumption, however, depends on the original dataset
because the patient assignment mechanism of the synthetic data is learned from the original dataset."
RESULTS,0.5093167701863354,"5
RESULTS"
RESULTS,0.515527950310559,"In this section we evaluate the quality of our synthetic dataset. We show that there is strong sim-
ilarity in both marginal and joint data distributions between the original and synthetic dataset, and
that patient privacy is preserved. We evaluate several causal inference models using our dataset to
demonstrate the usage of it."
ANALYSIS OF SYNTHETIC AND ORIGINAL DATA DISTRIBUTIONS,0.5217391304347826,"5.1
ANALYSIS OF SYNTHETIC AND ORIGINAL DATA DISTRIBUTIONS"
ANALYSIS OF SYNTHETIC AND ORIGINAL DATA DISTRIBUTIONS,0.5279503105590062,"We ﬁrst show how well the generated synthetic data preserves the joint distribution of the original
data. We trained the adapted ADS-GAN model for 10,000 epochs and the outcome neural network
for 20,000 epochs to generate the synthetic dataset. We calculated the Wasserstein distance (Vil-
lani (2008)) between the joint distribution of this synthetic data and that of the original data to be
0.35. To put this value in the correct perspective, we measured the Wasserstein distance between
the original dataset and a randomly generated dataset of the same dimensions. This serves as the
baseline scenario. In addition, we randomly split the original dataset into two datasets and measured
the Wasserstein distance between them, which is essentially the Wasserstein distance between the
dataset and itself and serves as the best case scenario. Our results show that the Wasserstein distance
between the synthetic and original data distribution (0.35) is almost as good as that in the best case
scenario (0.17), and far more lower than the value in the baseline scenario (8.6)."
ANALYSIS OF SYNTHETIC AND ORIGINAL DATA DISTRIBUTIONS,0.5341614906832298,Under review as a conference paper at ICLR 2022
ANALYSIS OF SYNTHETIC AND ORIGINAL DATA DISTRIBUTIONS,0.5403726708074534,"Figure 2: Heatmaps of correlation matrices of patient variables for the original (left) and synthetic
data (right), respectively."
ANALYSIS OF SYNTHETIC AND ORIGINAL DATA DISTRIBUTIONS,0.546583850931677,"Figure 3: Comparison of marginal distribution of lab values between original and synthetic data. The
three horizontal dotted lines in each violin plot from top to the bottom represent the third quartile,
median, and the ﬁrst quartile respectively."
ANALYSIS OF SYNTHETIC AND ORIGINAL DATA DISTRIBUTIONS,0.5527950310559007,"We then compare the joint distributions visually. In Fig. 2, the correlation among all patient attributes
in the original (synthetic) dataset is visualized by the heatmap on the left (right). In the heatmap,
the brighter the color of a pixel is, the more correlated the two variables are with each other. The
diagonal is the brightest in the map, as each pixel on the diagonal represents the correlation between a
variable and itself. The two heatmaps show almost identical patterns, indicating the joint distribution
of the original data is well preserved in the synthetic dataset."
ANALYSIS OF SYNTHETIC AND ORIGINAL DATA DISTRIBUTIONS,0.5590062111801242,"In Fig. 3, we compare qualitatively the marginal distributions of individual variables of the generated
synthetic data (orange) with the related ones from the original data (blue). The ﬁgure shows strong
similarity between the original and synthetic dataset in both basic statistical summaries (e.g., median
and quartiles) and overall shape of these distributions."
IDENTIFIABILITY OF SYNTHETIC DATA,0.5652173913043478,"5.2
IDENTIFIABILITY OF SYNTHETIC DATA"
IDENTIFIABILITY OF SYNTHETIC DATA,0.5714285714285714,"Since the synthetic dataset we generated in this study is meant to be made public, patient privacy
has to be preserved to ensure that no actual patients in the original dataset source can be identiﬁed
through the synthetic dataset. All the synthetic samples in our dataset are conceptually drawn from a
distribution, so no single piece of information about any actual patients is directly carried over to our
dataset. We further calculate the ϵ-identiﬁability as deﬁned in Deﬁnition 2 to be 0.008%, indicating
that the risk of any actual patient being identiﬁed is statistically zero."
IDENTIFIABILITY OF SYNTHETIC DATA,0.577639751552795,"5.3
RESULTS OF TESTING CAUSAL INFERENCE ALGORITHMS ON THE DATASET."
IDENTIFIABILITY OF SYNTHETIC DATA,0.5838509316770186,"In this section, we evaluate the bias in causal treatment effect estimate of ﬁve well established models
on our synthetic dataset: two models in the doubly robust (DR) family, one propensity score strati-
ﬁcation, one propensity matching, and one inverse probability treatment weighting (IPTW) model.
Doubly robust approaches adopt an outcome regression model to estimate the treatment outcome
and a propensity model to estimate the probability of a patient being assigned with a treatment. In
both the DR models we tested, random forest is used as the outcome regression model. The dif-
ference between the two DR models is in the propensity model, which can be logistic regression
(DR-LR) or random forest (DR-RF) classiﬁcation. We use Microsoft DoWhy (Sharma & Kiciman"
IDENTIFIABILITY OF SYNTHETIC DATA,0.5900621118012422,Under review as a conference paper at ICLR 2022
IDENTIFIABILITY OF SYNTHETIC DATA,0.5962732919254659,"(2020)) and EconML (Battocchi et al. (2019)) causal inference packages for the implementation.
When calculating the causal effect of a treatment, we remove the counter-factuals of this treatment
from the dataset to prevent the problem becoming trivial."
IDENTIFIABILITY OF SYNTHETIC DATA,0.6024844720496895,"We adopted four metrics to evaluate the models: the Spearman’s rank correlation coefﬁcient to
measure how well the models preserve the rank of the drugs by their treatment effects, Kendall
rank coefﬁcient similar to Spearman′s coefﬁcient but based on concordant and discordant pairs,
correlation between the estimated effects and the ground truth, and ﬁnally the R2 to measure how
much variance of the ground truth can be explained by the estimate."
IDENTIFIABILITY OF SYNTHETIC DATA,0.6086956521739131,"To estimate how models perform in a real-world setting, we generated an additional dataset consist-
ing of all patient variables of the original dataset and patient outcomes generated from the trained
outcome neural network with patient variables and treatments of the original dataset as its inputs.
We call this dataset the hybrid dataset because part of the data comes from the original dataset and
part of the data is generated. We run the ﬁve causal inference models on the two datasets and re
reported all the results in Appendix A.3."
IDENTIFIABILITY OF SYNTHETIC DATA,0.6149068322981367,"The results on the hybrid dataset show that the algorithms can be grouped into two categories in
terms of their performance: the propensity stratiﬁcation, propensity matching, and two doubly robust
models are in the ﬁrst group and the IPTW model is in the second group. The models in the ﬁrst
group produced much better results than the IPTW model. The results on the synthetic dataset show
a similar pattern. The variance of the performance metrics of the models in the ﬁrst group is larger
than that on the hybrid dataset, but all these models perform much better than the IPTW model.
Investigating why some models perform better than others on the two datasets is out of scope of this
work. Here we show that the synthetic data preserves the relative performance of different models
that would be achieved in a more realistic setting, simulated by the hybrid dataset."
DISCUSSION,0.6211180124223602,"6
DISCUSSION"
DISCUSSION,0.6273291925465838,"In general, inclusion and exclusion criteria applied to the data may introduce selection bias. Our
work was designed with a target trial in mind in which patients are recruited at an initial qualifying
measurement and then followed up after treatment assignments. We believe this minimizes the
impact of selection bias from conditioning on the inclusion and exclusion criteria in our original
data. In this work, we only produced one dataset for hypertension and evaluated ﬁve causal inference
models. We leave it to future work to produce synthetic datasets for other diseases and evaluate and
compare other causal inference models. Because hypertension affects almost half of adults in the
United States, a synthetic dataset on hypertension is of signiﬁcant value by itself. For simplicity, in
this study we did not consider treatment modiﬁers, i.e, interactions between treatments and patient
variables. It is an interesting and important topic which we plan to address in the future."
CONCLUSION,0.6335403726708074,"7
CONCLUSION"
CONCLUSION,0.639751552795031,"To validate machine learning models, researchers have traditionally relied on labeled data, i.e.
ground truth. Due to the fundamental problem of causal inference, however, the lack of realistic
clinical data with ground truth makes it difﬁcult to evaluate causal inference models. In this work,
we produced a large-scale and realistic synthetic data by adapting an ADS-GAN model to generate
patient variables and using a neural network to produce patient outcomes. The data we generated
supports multiple treatments with known treatment effects. We demonstrated that this synthetic
dataset preserves patient privacy and has strong similarity to the original dataset it is modeled after.
We believe that it will facilitate the evaluation, understanding and improvement of causal inference
models, especially with respect to how they perform in real-world scenarios."
REPRODUCIBILITY STATEMENT,0.6459627329192547,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.6521739130434783,"To contribute to reproducibility, we provide in the supplementary material the code for replicating
experiments. In section 3 we provide detailed description of the original real-world dataset and
related inclusion and exclusion criteria. In section A.1 we provide a detailed description of the
preprocessing procedure to make the generation process transparent and reproducible. Finally, we
report the the link to download our synthetic dataset in the ﬁnal paper."
REPRODUCIBILITY STATEMENT,0.6583850931677019,Under review as a conference paper at ICLR 2022
REFERENCES,0.6645962732919255,REFERENCES
REFERENCES,0.6708074534161491,"Douglas Almond, Kenneth Y. Chay, and David S. Lee. The costs of low birth weight. The Quar-
terly Journal of Economics, 120(3):1031–1083, 2005. URL https://EconPapers.repec.
org/RePEc:oup:qjecon:v:120:y:2005:i:3:p:1031-1083. 2.1"
REFERENCES,0.6770186335403726,"M. Arjovsky, S. Chintala, , and L. Bottou. Wasserstein generative adversarial networks. In Interna-
tional Conference on Machine Learning, 2017. 4.2"
REFERENCES,0.6832298136645962,"Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna Oprescu, and Vasilis
Syrgkanis. EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estima-
tion. https://github.com/microsoft/EconML, 2019. Version 0.x. 5.3"
REFERENCES,0.6894409937888198,"Anat Reiner Benaim, Ronit Almog, Yuri Gorelik, Irit Hochberg, Laila Nassar, Tanya Mashiach,
Mogher Khamaisi, Yael Lurie, Zaher S Azzam, Johad Khoury, et al. Analyzing medical research
results based on synthetic data and their relation to real data results: systematic comparison from
ﬁve observational studies. JMIR medical informatics, 8(2):e16492, 2020. 2.2"
REFERENCES,0.6956521739130435,"Mary E Charlson, Peter Pompei, Kathy L Ales, and C Ronald MacKenzie. A new method of clas-
sifying prognostic comorbidity in longitudinal studies: development and validation. Journal of
chronic diseases, 40(5):373–383, 1987. 3"
REFERENCES,0.7018633540372671,"Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, and Jimeng Sun.
Generating multi-label discrete patient records using generative adversarial networks, 2018. 4.2"
REFERENCES,0.7080745341614907,"Xin Du, Lei Sun, Wouter Duivesteijn, Alexander Nikolaev, and Mykola Pechenizkiy. Adversarial
balancing-based representation learning for causal effect inference with observational data. Data
Mining and Knowledge Discovery, pp. 1–26, 2021. 2.1"
REFERENCES,0.7142857142857143,"Khaled El Emam, David L. Buckeridge, Robyn Tamblyn, Angelica Neisa, Elizabeth Jonker, and
Aman Verma. The re-identiﬁcation risk of canadians from longitudinal demographics. BMC
Medical Informatics Decis. Mak., 11:46, 2011. doi: 10.1186/1472-6947-11-46. URL https:
//doi.org/10.1186/1472-6947-11-46. 1"
REFERENCES,0.7204968944099379,"Dylan J. Foster and Vasilis Syrgkanis. Orthogonal statistical learning, 2020. 1"
REFERENCES,0.7267080745341615,"Jessica M Franklin, Sebastian Schneeweiss, Jennifer M Polinski, and Jeremy A Rassen.
Plas-
mode simulation for the evaluation of pharmacoepidemiologic methods in complex healthcare
databases. Computational statistics & data analysis, 72:219–226, 2014. 2.3"
REFERENCES,0.7329192546583851,"Andre Goncalves, Priyadip Ray, Braden Soper, Jennifer Stevens, Linda Coyle, and Ana Paula Sales.
Generation and evaluation of synthetic patient data. BMC medical research methodology, 20(1):
1–40, 2020. 2.2"
REFERENCES,0.7391304347826086,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf. 4.2, 4.2,
4.2"
REFERENCES,0.7453416149068323,"J Henry, Yuriy Pylypchuk, Talisha Searcy, and Vaishali Patel. Adoption of electronic health record
systems among us non-federal acute care hospitals: 2008–2015. ONC data brief, 35:1–9, 2016. 1"
REFERENCES,0.7515527950310559,"Jennifer L. Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 20(1):217–240, 01 2011. doi: 10.1198/jcgs.2010.08162. URL https:
//doi.org/10.1198/jcgs.2010.08162. 2.1"
REFERENCES,0.7577639751552795,"James Jordon, Jinsung Yoon, and M. Schaar. Pate-gan: Generating synthetic data with differential
privacy guarantees. In ICLR, 2019. 4.2"
REFERENCES,0.7639751552795031,"Ehud Karavani, Tal El-Hay, Yishai Shimoni, and Chen Yanover. Comment: Causal inference com-
petitions: Where should we aim? Statistical Science, 34(1):86–89, 2019. 2.1"
REFERENCES,0.7701863354037267,Under review as a conference paper at ICLR 2022
REFERENCES,0.7763975155279503,"Robert LaLonde. Evaluating the econometric evaluations of training programs with experimen-
tal data. American Economic Review, 76(4):604–20, 1986. URL https://EconPapers.
repec.org/RePEc:aea:aecrev:v:76:y:1986:i:4:p:604-20. 2.1"
REFERENCES,0.782608695652174,"Michael J. Lopez and Roee Gutman. Estimation of causal effects with multiple treatments: A review
and new ideas. Statistical Science, 32(3), Aug 2017. ISSN 0883-4237. doi: 10.1214/17-sts612.
URL http://dx.doi.org/10.1214/17-STS612. 1, 4.3"
REFERENCES,0.7888198757763976,"Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, and Max Welling. Causal
effect inference with deep latent-variable models, 2017. 2.1"
REFERENCES,0.7950310559006211,"Bradley Malin and Latanya Sweeney. How (not) to protect genomic data privacy in a distributed
network: Using trail re-identiﬁcation to evaluate and design anonymity protection systems. J. of
Biomedical Informatics, 37(3):179–192, June 2004. ISSN 1532-0464. doi: 10.1016/j.jbi.2004.
04.005. URL https://doi.org/10.1016/j.jbi.2004.04.005. 1"
REFERENCES,0.8012422360248447,"Brady Neal, Chin-Wei Huang, and Sunand Raghupathi. Realcause: Realistic causal inference bench-
marking. arXiv preprint arXiv:2011.15007, 2020. 2.3"
REFERENCES,0.8074534161490683,"Paul R. Rosenbaum and Donald B. Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1):41–55, 04 1983. ISSN 0006-3444. doi: 10.1093/
biomet/70.1.41. URL https://doi.org/10.1093/biomet/70.1.41. 1, 4.3"
REFERENCES,0.8136645962732919,"Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face
recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 815–823, 2015. 4.2"
REFERENCES,0.8198757763975155,"Megan S Schuler and Sherri Rose. Targeted maximum likelihood estimation for causal inference in
observational studies. American journal of epidemiology, 185(1):65–73, 2017. 2.3, 4.3"
REFERENCES,0.8260869565217391,"Uri Shalit, Fredrik D. Johansson, and David Sontag. Estimating individual treatment effect: general-
ization bounds and algorithms. In Proceedings of the 34th International Conference on Machine
Learning, pp. 3076–3085, 2017. 4.3"
REFERENCES,0.8322981366459627,"Amit Sharma and Emre Kiciman. Dowhy: An end-to-end library for causal inference, 2020. 5.3"
REFERENCES,0.8385093167701864,"Latanya Sweeney. Weaving technology and policy together to maintain conﬁdentiality. The Journal
of Law, Medicine & Ethics, 25(2-3):98–110, 1997. doi: 10.1111/j.1748-720X.1997.tb01885.
x.
URL https://doi.org/10.1111/j.1748-720X.1997.tb01885.x.
PMID:
11066504. 1"
REFERENCES,0.84472049689441,"Allan Tucker, Zhenchen Wang, Ylenia Rotalinti, and Puja Myles. Generating high-ﬁdelity synthetic
patient data for assessing machine learning healthcare software. NPJ digital medicine, 3(1):1–13,
2020. 2.2"
REFERENCES,0.8509316770186336,"C. Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.
5.1"
REFERENCES,0.8571428571428571,"Jason Walonoski, Mark Kramer, Joseph Nichols, Andre Quina, Chris Moesel, Dylan Hall, Carlton
Duffett, Kudakwashe Dube, Thomas Gallagher, and Scott McLachlan. Synthea: An approach,
method, and software mechanism for generating synthetic patients and the synthetic electronic
health care record. Journal of the American Medical Informatics Association, 25(3):230–238,
2018. 2.2"
REFERENCES,0.8633540372670807,"Zhenchen Wang, Puja Myles, and Allan Tucker. Generating and evaluating cross-sectional synthetic
electronic healthcare data: Preserving data utility and patient privacy. Computational Intelligence,
37(2):819–851, 2021. 2.2"
REFERENCES,0.8695652173913043,"Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private generative
adversarial network, 2018. 4.2"
REFERENCES,0.8757763975155279,Under review as a conference paper at ICLR 2022
REFERENCES,0.8819875776397516,"Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang.
Repre-
sentation learning for treatment effect estimation from observational data.
In S. Ben-
gio,
H.
Wallach,
H.
Larochelle,
K.
Grauman,
N.
Cesa-Bianchi,
and
R.
Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
a50abba8132a77191791390c3eb19fe7-Paper.pdf. 2.1"
REFERENCES,0.8881987577639752,"Jinsung Yoon, Lydia N. Drumright, and Mihaela van der Schaar. Anonymization through data syn-
thesis using generative adversarial networks (ADS-GAN). IEEE J. Biomed. Health Informatics,
24(8):2378–2388, 2020. doi: 10.1109/JBHI.2020.2980262. URL https://doi.org/10.
1109/JBHI.2020.2980262. 1, 4.2, 4.2, 4.2"
REFERENCES,0.8944099378881988,Under review as a conference paper at ICLR 2022
REFERENCES,0.9006211180124224,"A
APPENDIX"
REFERENCES,0.906832298136646,"A.1
STEP 1: DETAILED DESCRIPTION OF DATA PREPROCESSING"
REFERENCES,0.9130434782608695,"Here we provide a description of the preprocessing procedure to clean and transform the patient
claim data described in Table 1. The goal of this step is to prepare the data ready for subsequent
steps."
REFERENCES,0.9192546583850931,"We ﬁrst one-hot encode all the categorical variables in the table, such as gender and race. For each
lab test there are two variables: lab values and lab dates. Since all the lab dates were after the ’date-’
date, which was the date when the patient’s systolic blood pressure was measured before treatment,
we convert all the lab dates to the number of days since ’date-’, excluding holidays and weekends.
The reason for excluding these dates in the convert ion is that we eventually reverse the exact process
to convert the synthetic lab dates in number of days back to the original date format, we make sure
that such synthetic dates do not fall on holidays and weekends. Note that the exclusion of holidays
and weekends does not apply to the prescription dates, assuming prescriptions can be ﬁlled on any
day. When we transform the ’date-’ variable itself, we arbitrarily pick a date proceeding all the
’date-’ values, and convert the ’date-’ values to the number of days since this picked date, excluding
all holidays and weekends."
REFERENCES,0.9254658385093167,"There are many missing lab dates and values, as expected. If doctors order a certain lab test for a
small portion of patients, the lab test and dates for the majority of the patients would be missing.
We ﬁll the missing values with the mean of the values that are present. However, this induces a
data distribution that signiﬁcantly deviates from Gaussian distribution: the probability of the mean
is much higher than the probability of other values. Our experiments showed that learning such a
distribution was difﬁcult. To address this problem, we created a binary indicator for each lab test,
which is set to 1 if the lab test is not missing, and 0 otherwise. The intuition is that it is easier for the
model to learn not only the distribution of the indicator, but also the join distribution of the indicator
and its corresponding lab results, thus enhancing the synthetic data’s ability to more faithfully reﬂect
the original data distribution. Another beneﬁt of doing this is to directly preserve the information on
whether a lab test was ordered for each patient. Our experiments show that this approach works well
in practice. The ﬁnal step of data preprocessing is to standardize all the values to the [0, 1] range."
REFERENCES,0.9316770186335404,"A.2
FUNDAMENTAL ASSUMPTIONS FOR TREATMENT EFFECT ESTIMATIONS"
REFERENCES,0.937888198757764,"We use t ∈{t1, t2, . . .} = XC to denote treatments, Yi (ti) ∈R to denote the potential outcome if
the patient is treated with the treatment ti ∈XC, and xi to denote the variables for patient i."
REFERENCES,0.9440993788819876,"SUTVA (Stable Unit Treatment Value Assumption) Potential outcomes for one individual are
unaffected by the treatment of others."
REFERENCES,0.9503105590062112,"Unconfoundedness The treatment distribution is conditional independent of the potential outcomes
given covariates, i.e. (∀i)
 
ti ⊥⊥
 
Yi (t1) , Yi (t2) , . . . , Yi
 
t|XC|

|xi

."
REFERENCES,0.9565217391304348,"Positivity Every individual has a non-zero probability to receive each one of the treatments, for a
given level of covariates, i.e. (∀i) (∀s) (s ∈T ∧0 < P (ti = s |xi ) < 1)."
REFERENCES,0.9627329192546584,"A.3
RESULTS OF EVALUATING DIFFERENT CAUSAL INFERENCE MODELS"
REFERENCES,0.968944099378882,Under review as a conference paper at ICLR 2022
REFERENCES,0.9751552795031055,"spearmanr
kendalltau
correlation
R2 score
Propensity Stratiﬁcation
0.99
0.92
0.99
0.99
Propensity Matching
0.92
0.8
0.98
0.94
Doubly Robust - RF
0.95
0.93
0.99
0.95
Doubly Robust - LR
0.99
0.96
0.99
0.99
IPTW
0.59
0.48
-0.46
-5.9"
REFERENCES,0.9813664596273292,"Table 2: Model evaluation on hybrid data. The results show that the ﬁrst four models perform much
better than the IPTW model."
REFERENCES,0.9875776397515528,"spearmanr
kendalltau
correlation
R2 score
Propensity Stratiﬁcation
0.81
0.68
0.87
0.6
Propensity Matching
0.52
0.37
0.47
-11
Doubly Robust - RF
0.92
0.78
0.97
0.94
Doubly Robust - LR
0.74
0.63
0.88
0.75
IPTW
0.09
0.04
0.06
-34"
REFERENCES,0.9937888198757764,"Table 3: Model evaluation on synthetic data. The results show a pattern similar to the table for the
hybrid dataset: the ﬁrst four models perform much better than the IPTW model. The performance
variance of the ﬁrst four models is larger than that on the hybrid dataset."
