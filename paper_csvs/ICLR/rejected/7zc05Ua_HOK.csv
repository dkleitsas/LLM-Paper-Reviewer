Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00390625,"There has been great interest in enhancing the robustness of neural network classi-
Ô¨Åers to defend against adversarial perturbations through adversarial training, while
balancing the trade-off between robust accuracy and standard accuracy. We propose
a novel adversarial training framework that learns to reweight the loss associated
with individual training samples based on a notion of class-conditioned margin,
with the goal of improving robust generalization. Inspired by MAML-based ap-
proaches, we formulate weighted adversarial training as a bilevel optimization
problem where the upper-level task corresponds to learning a robust classiÔ¨Åer, and
the lower-level task corresponds to learning a parametric function that maps from
a sample‚Äôs multi-class margin to an importance weight. Extensive experiments
demonstrate that our approach improves both clean and robust accuracy compared
to related techniques and state-of-the-art baselines."
INTRODUCTION,0.0078125,"1
INTRODUCTION"
INTRODUCTION,0.01171875,"While neural networks have been extremely successful in tasks such as image classiÔ¨Åcation and
speech recognition, recent work (Szegedy et al., 2014; Goodfellow et al., 2015) has demonstrated that
neural network classiÔ¨Åers can be arbitrarily fooled by small, adversarially-chosen perturbations of
their input. Notably, Su et al. (2017) demonstrated that neural network classiÔ¨Åers which can correctly
classify ‚Äúclean‚Äù images may be vulnerable to targeted attacks, e.g., misclassify those same images
when only a single pixel is changed."
INTRODUCTION,0.015625,"Recent work has shown a common failing among techniques that uniformly encourage robustness.
In particular, there exists an intrinsic tradeoff between robustness and accuracy (Zhang et al., 2019).
Bao et al. (2020) investigate this tradeoff from the perspective of classiÔ¨Åcation-callibrated loss theory.
Rice et al. (2020) empirically showed that during adversarial training, networks often irreversibly
lose robustness after training for a short time. They dubbed this phenomenon adversarial overÔ¨Åtting
while proposing early stopping as a remedy. The signiÔ¨Åcance of label noise and memorization in the
context of adversarial overÔ¨Åtting was demonstrated in Sanyal et al. (2021)‚Äîin particular that poor
training samples induce fragility to adversarial perturbations due to the tendency of neural networks
to interpolate the training data. Methods based on weight and logit smoothing have been proposed as
an alternative to early stopping (Chen et al., 2021; Cohen et al., 2019; Salman et al., 2019)."
INTRODUCTION,0.01953125,"In another line of work, Geometry-Aware Instance Reweighted Adversarial Training (GAIRAT; Zhang
et al. (2021)), Weighted Margin-aware Minimax Risk (WMMR; Zeng et al. (2021)), and Margin-
Aware Instance reweighting Learning (MAIL; Wang et al. (2021)) were developed to address adver-
sarial overÔ¨Åtting by controlling the inÔ¨Çuence of training examples via importance / loss weighting.
Intuitively, the samples assigned a low weight correspond to samples on which the classiÔ¨Åer is already
sufÔ¨Åciently robust. However, existing methods rely on rigid approximations of the margin and employ
heuristic weighting schemes that rely on careful choices of hyperparameters."
INTRODUCTION,0.0234375,"This work builds upon these observations. We present BiLAW (Bilevel Learnable Adversarial
reWeighting), an approach that explicitly learns a parametric function (e.g. represented by a small
feed-forward network) that assigns weights to the loss suffered by a classiÔ¨Åer, associated with
individual training samples. The sample weights are learned as a function of the classiÔ¨Åer multiclass
margins of the sample, according to the weights‚Äô effect on robust generalization. We employ a
bi-level optimization formulation Bracken & McGill (1973) and leverage a validation set, where
the upper-level objective corresponds to learning the parameters of a robust classiÔ¨Åer, while the"
INTRODUCTION,0.02734375,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03125,"lower-level objective corresponds to learning a function that predicts sample weights that improve
improve robustness on a validation set. Our approach alternates between iteratively updating the
parametric sample weights and updating the classiÔ¨Åer network parameters."
INTRODUCTION,0.03515625,"Contributions We note that while sample weighting (Zhang et al., 2021; Yi et al., 2021; Wang et al.,
2021) has been investigated in the context of robust training, as far as we know this is the Ô¨Årst work
to explore a method to use a validation set to learn weights to induce robust generalization. Our
contributions consist of the following:"
INTRODUCTION,0.0390625,"1. Our approach performs sample re-weighting during training. We propose a bilevel optimiza-
tion formulation to learn a mapping from multi-class margins to weights according to the
robust loss suffered by a classiÔ¨Åer on a validation set."
INTRODUCTION,0.04296875,"2. We show that when our weighting function corresponds to a neural network, the magnitude
of a sample‚Äôs weight directly corresponds to the vulnerability of the classiÔ¨Åer at that sample."
INTRODUCTION,0.046875,"3. We evaluate the practical performance of BiLAW on MNIST (LeCun & Cortes, 2010), F-
MNIST (Xiao et al., 2017), CIFAR-10, and CIFAR-100. (Krizhevsky et al.) and demonstrate
that it improves clean accuracy up to 6% and robust test accuracy by up to 5% compared to
TRADES and other state-of-the-art sample reweighting methods on CIFAR-10."
INTRODUCTION,0.05078125,"This paper is organized as follows. In Section 2, we review the notation and background of cost-aware
and robust classiÔ¨Åcation. In Section 3 we describe our method in full, including the sample weighting
method. In Section 4, we provide ablative experiments and demonstrate the efÔ¨Åcacy of our framework
by comparing clean and robust test performance on MNIST, F-MNIST, CIFAR-10, and CIFAR-100
to adversarial training and two recent, state-of-the-art sample weighting methods."
PRELIMINARIES AND RELATED WORK,0.0546875,"2
PRELIMINARIES AND RELATED WORK"
PRELIMINARIES AND RELATED WORK,0.05859375,"In this section, we brieÔ¨Çy present background terminology pertaining to adversarially robust classiÔ¨Å-
cation, sample reweighting and bilevel optimization."
PRELIMINARIES AND RELATED WORK,0.0625,"Notations
An ReLU network is a neural network such that all nonlinear activations are ReLU
functions, where we denote the ReLU activation by œÉ : R ‚ÜíR, œÉ(x) = max{0, x}. Informally, we
deÔ¨Åne œÉ : Rd ‚ÜíRd by œÉ(x) = [œÉ(x1), . . . , œÉ(xd)]. Let fŒ∏ : Rd ‚Üí[0, 1]k be a feedforward ReLU
network with l hidden layers and weights Œ∏; for example, f may map from a d-dimensional image to
a k-dimensional vector corresponding to likelihoods for k classes."
PRELIMINARIES AND RELATED WORK,0.06640625,"Given a training set of m sample-label pairs (xi, yi) drawn from a training data distribution D, we
associate a weight wi with each training sample. Informally, these weights characterize the effect
of the sample on the generalization of the network (i.e. samples with large weights promote robust
generalization and visa versa). Given a loss function ‚Ñì: Rk √ó Rk ‚ÜíR, we denote the empirical
weighted training loss suffered by a network with parameters Œ∏ on m training samples with weights
w to be Ltr(Œ∏, w) = Pm
i=1 wi‚Ñì(yi, fŒ∏(xi)) such that wi ‚â•0 and P"
PRELIMINARIES AND RELATED WORK,0.0703125,"i wi = 1. For brevity, we write
‚Ñìi(Œ∏) = ‚Ñì(yi, fŒ∏(xi)). Additionally, if w is left unspeciÔ¨Åed, L corresponds to the unweighted mean
over empirical losses. Likewise, the unweighted validation loss of n samples is denoted Lval(Œ∏)."
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.07421875,"2.1
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING"
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.078125,"Consider the network fŒ∏ : Rd ‚ÜíRk, where the input is d-dimensional and the output is a k-
dimensional vector of likelihoods, with j-th entry corresponding to the likelihood the image belongs
to the j-th class. The associated classiÔ¨Åcation is then c(x) = arg maxj‚àà[1,k] fŒ∏,j(x). In adversarial
machine learning, we are not just concerned that the classiÔ¨Åcation be correct, but we also want to
be robust against adversarial examples, i.e. small perturbations to the input which may change the
classiÔ¨Åcation to an incorrect class. We deÔ¨Åne the notion of œµ-robustness below:"
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.08203125,"DeÔ¨Ånition 1 (œµ-robust) fŒ∏ is called œµ-robust with respect to norm p at x if the classiÔ¨Åcation is
consistent for a small ball of radius œµ around x:"
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.0859375,"c(x + Œ¥) = c(x), ‚àÄŒ¥ : ||Œ¥||p ‚â§œµ.
(1)"
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.08984375,Under review as a conference paper at ICLR 2022
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.09375,"Note that the œµ-robustness of fŒ∏ at x is intimately related to the uniform and local Lipschitz smoothness
of fŒ∏ around x. Recall that a function f has Ô¨Ånite, global Lipschitz constant L > 0 with respect to
norm || ¬∑ ||, if
‚àÉL ‚â•0 s.t. |f(x) ‚àíf(x‚Ä≤)| ‚â§L ¬∑ ||x ‚àíx‚Ä≤||, ‚àÄx, x‚Ä≤ ‚ààX.
(2)"
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.09765625,"An immediate consequence of Eq. 1 and Eq. 2 is that if fŒ∏ is uniformly L-Lipschitz, then fŒ∏ is œµ-robust
at x with œµ =
1
2L(Pa ‚àíPb) where Pa is the likelihood of the most likely outcome, and Pb is the
likelihood of the second most likely outcome (Salman et al., 2019). The piecewise linearity of ReLU
networks facilitates the extension of this consequence to the locally Lipschitz regime. L corresponds
to the norm of the afÔ¨Åne map characterized by f conditioned on input x. These properties were
previously used to characterize the robustness of a network at a sample (and the weight associated
with the sample) (Zeng et al., 2021; Yi et al., 2021; Wang et al., 2021)."
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.1015625,"The minimal ‚Ñìp-norm perturbation Œ¥‚àó
p required to switch an sample‚Äôs label is given by the solution to
the following optimization problem:"
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.10546875,"Œ¥‚àó
p = arg min ||Œ¥||p
s.t.
c(x) Ã∏= c(x + Œ¥)."
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.109375,"A signiÔ¨Åcant amount of existing work relies on a Ô¨Årst-order approximations and H√∂lder‚Äôs inequality to
recover Œ¥‚àó, justifying the popularity of inducing robustness by controlling global and local Lipschitz
constants. More concretely, given a ‚Ñìp norm and radius œµ, a typical goal of robust machine learning is
to learn classiÔ¨Åers that minimize the robust loss on a training dataset:"
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.11328125,"min
Œ∏
E(x,y)‚àºD"
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.1171875,"
max
||Œ¥||p‚â§œµ ‚Ñì(y, fŒ∏(x + Œ¥)) 
."
ROBUST CLASSIFICATION AND ADVERSARIAL OVERFITTING,0.12109375,"For brevity we will denote the robust analogue of a loss L as ÀÜL, indicating this is the robust counterpart
of L, differentiated by the ‚Äúinner‚Äù maximization problem."
MARGIN-AWARE REWEIGHTING,0.125,"2.2
MARGIN-AWARE REWEIGHTING"
MARGIN-AWARE REWEIGHTING,0.12890625,"In the framework of cost-sensitive learning, weights are assigned to the loss associated with individual
samples. The goal of cost-sensitive learning is to minimize the empirical weighted training loss:"
MARGIN-AWARE REWEIGHTING,0.1328125,"min
Œ∏
Ltr(Œ∏, w) := m
X"
MARGIN-AWARE REWEIGHTING,0.13671875,"i=1
wi‚Ñìi(Œ∏)."
MARGIN-AWARE REWEIGHTING,0.140625,"Previous work in cost-sensitive learning for adversarial robustness (Zhang et al., 2020; 2021; Zeng
et al., 2021) typically substitutes the robust loss ÀÜLtr for Ltr and largely focuses on designing heuristic
functions of various notions of margin to use for the sample weight wi."
MARGIN-AWARE REWEIGHTING,0.14453125,"For example, in GAIRAT (Zhang et al., 2021; 2020), the margin is deÔ¨Åned as the least number
of PGD steps, denoted Œ∫, that leads the classiÔ¨Åer to make an incorrect prediction. The sample‚Äôs
weight is computed as œâGAIRAT(xi) = 1"
MARGIN-AWARE REWEIGHTING,0.1484375,"2(1 + tanh(Œª + 5(1 ‚àí2Œ∫/K))) with hyperparameters K and
Œª. A small Œ∫ indicates that the sample lies close to the decision boundary. Larger Œ∫ values imply
that associated samples lie far from the decision boundary, and are therefore more robust, requiring
smaller weights. However, due to the non-linearity of the loss-surface in practice, PGD-based attacks
with Ô¨Ånite iterations may suffer from the same issues that plague standard iterative Ô¨Årst-order methods
in non-convex settings. In other words, Œ∫ is heavily dependent on the optimization path taken by PGD.
This is demonstrated by GAIRAT‚Äôs vulnerability on sophisticated attacks, e.g. AutoAttack (Croce &
Hein, 2020b)."
MARGIN-AWARE REWEIGHTING,0.15234375,"Zhang et al. (2020) deÔ¨Åne the margin as the difference between the loss of a network suffered at a
clean sample and its adversarial variant. Zeng et al. (2021); Wang et al. (2021) propose a deÔ¨Ånition
of margin corresponding to taking differences between logits, as follows."
MARGIN-AWARE REWEIGHTING,0.15625,"DeÔ¨Ånition 2 ((Zeng et al., 2021; Wang et al., 2021)) The margin of a classiÔ¨Åer fŒ∏ on sample
(xi, yi) is the difference between the classiÔ¨Åer‚Äôs conÔ¨Ådence in the correct label yi and the maximal
probability of an incorrect label t, margin(fŒ∏, xi, yi) = p(f(xi) = yi) ‚àímaxtÃ∏=yip(fŒ∏(xi) = t)."
MARGIN-AWARE REWEIGHTING,0.16015625,"Given this deÔ¨Ånition, Zeng et al. (2021); Wang et al. (2021) propose to use an exponential (WMMR)
and sigmoidal (MAIL) functions respectively: œâWMMR(xi) = exp(‚àíŒ±m) with parameter Œ±, and"
MARGIN-AWARE REWEIGHTING,0.1640625,Under review as a conference paper at ICLR 2022
MARGIN-AWARE REWEIGHTING,0.16796875,"œâMAIL(xi) = sigmoid(‚àíŒ≥(m ‚àíŒ≤)) with parameters Œ≥ and Œ≤. WMMR and MAIL rely on the local
linearity of ReLU networks and the fact that for samples near the margin, the relative scale of
predicted class-likelihoods directly corresponds to the distance to the decision boundary. However,
similarly to GAIRAT‚Äôs Œ∫, even for samples very close to the decision boundary, simple functions of
the difference between class likelihoods may not necessarily correspond to the true distance to the
decision boundary. Here we propose a more Ô¨Åne-grained notion of margin, the multi-class margin,
and design a method to learn a mapping between the margin at a sample and its associated weight."
MARGIN-AWARE REWEIGHTING,0.171875,"Previous work has explored theoretical notions of a multi-class margin. For example, Zou (2005)
deÔ¨Åned the margin vector in the context of boosting as a proxy for a vector of conditional class
probabilities. However, this notion of margin is unaware of the true class of a sample. In contrast,
the multi-class margin proposed by Saberian & Vasconcelos (2019); Cortes et al. (2013) are both
closely related to Wang et al. (2021); Zeng et al. (2021), i.e. deÔ¨Åned as the minimal distance between
an arbitrary predicted logit and the logit of the true class."
MARGIN-AWARE REWEIGHTING,0.17578125,"In Fig. 1 we explore the relationship between the logits of a network evaluated at a clean sample
and the predicted class of the adversarially perturbed variant. Methods which rely on the canonical
notions of margin reasonably assume that samples at which a classiÔ¨Åer is vulnerable have small
margin according to Def. 2, i.e. the magnitude of the smallest difference between the logits of any
class and the logit corresponding the true class is small. However, we demonstrate in Fig. 1(b) for
both vulnerable and robust classiÔ¨Åers that while the majority of predictions made by networks on
adversarial samples do correspond to the classes with minimal margin, a signiÔ¨Åcant number do not. In
other words, the class for which the margin is smallest does not always correspond to the adversarial
class. Furthermore, this issue is exacerbated for robust networks as shown by the difference in count
distribution between networks whose relative robustness varies."
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.1796875,"2.3
BI-LEVEL OPTIMIZATION AND META-LEARNING"
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.18359375,"Bilevel optimization, Ô¨Årst introduced by Bracken & McGill (1973) is a framework for mathematical
optimization involving nested optimization problems. A typical bilevel optimization problem takes
on the following form:"
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.1875,"min
x‚ààRp Œ¶(x) := f(x, y‚àó(x))
s.t.
y‚àó‚ààarg min
y‚ààRp
g(x, y),
(3)"
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.19140625,"where f and g are respectively denoted the upper-level and lower-level objectives. The goal of the
framework is to minimize the primary objective Œ¶(x) with respect to x where y‚àó(x) is obtained by
solving the lower-level minimization problem. The framework of bilevel optimization has seen wide
adoption by the machine learning community‚Äîin particular in the context of hyperparameter tuning
(Jenni & Favaro, 2018; Ren et al., 2018) and meta-learning (Finn et al., 2017; Rajeswaran et al.,
2019). Our proposed algorithm has some similarity to the meta-learning literature (Finn et al., 2017;
Rusu et al., 2019; Rajeswaran et al., 2019; Eshratifar et al., 2018). Most notably, the Model-Agnostic
Meta-Learning MAML) algorithm by Finn et al. (2017) incorporates gradient information for the
meta-learning setting. The application of meta-learning as an instance of bilevel optimization has
been explored in the context of sample reweighting. In particular, Ren et al. (2018); Jenni & Favaro
(2018); Shu et al. (2019) proposed methods to address learning with noisy labels by reweighting
the gradients associated with the losses at individual samples based on balancing performance on a
curated validation set and the corrupted training set."
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.1953125,"3
BILAW: LEARNING SAMPLES WEIGHTS FOR ADVERSARIAL TRAINING"
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.19921875,"In this section, we propose BiLAW, a new learning framework for robust training. There are 2 main
novelties in our new learning scheme compared to existing robust training methods: First, we consider
a more reasonable assumption leveraging the concept of multi-class margin in robust training, where
good weights should be aware of both the margin associated with each class, as well as the true
class associated with the sample. Second, as opposed to related work which deÔ¨Ånes an explicit
formula for the weights dependent on the margin, we propose to learn the weights as part of training
the classiÔ¨Åcation model. SpeciÔ¨Åcally, we deÔ¨Åne the weights as a function of a multi-class margin,
and parameterize this function using a small auxiliary network. We formulate this as a bi-level
optimization problem and learn the weights iteratively alongside the neural network parameters."
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.203125,Under review as a conference paper at ICLR 2022
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.20703125,"(a)
(b)
Class 2"
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.2109375,Class 1
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.21484375,Class 3
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.21875,Decision boundary
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.22265625,Decision boundary
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.2265625,Œî- margin
BI-LEVEL OPTIMIZATION AND META-LEARNING,0.23046875,"Figure 1: (a) Diagram of multiclass margin. Larger samples denote samples that should be assigned
large weight, e.g., are misclassiÔ¨Åed or close to the decision boundary. Green (red) arrows denote
entries in the multiclass margin vector for a correctly (incorrectly) classiÔ¨Åed sampled. (b) Sorted logit
order and frequency of adversarial classiÔ¨Åcation. Number of instances where the prediction of an
adversarial sample corresponds to its i-th largest logit in CIFAR-10 (ignoring the 0-th logit/samples
for which the prediction does not change). Colors represent the perturbation budget used during
adversarial training (i.e. varying degrees of robustness). Perturbations are computed using ‚Ñì‚àû-PGD
with 10 iterations and a maximum budget of 0.031."
MULTI-CLASS MARGIN REWEIGHTING,0.234375,"3.1
MULTI-CLASS MARGIN REWEIGHTING"
MULTI-CLASS MARGIN REWEIGHTING,0.23828125,"We extend the logit-based deÔ¨Ånitions of margin applied in Zeng et al. (2021); Wang et al. (2021) and
deÔ¨Åne the multi-class margin of a classiÔ¨Åer at a sample as follows."
MULTI-CLASS MARGIN REWEIGHTING,0.2421875,"DeÔ¨Ånition 3 The multi-class margin of a classiÔ¨Åer fŒ∏ on sample (xi, yi), denoted ‚àÜ: [0, 1]k ‚Üí
[‚àí1, 1]k, is a k-dimensional vector whose j-th entry, ‚àÜ(j)(fŒ∏(xi), yi), is the difference between
the classiÔ¨Åer‚Äôs conÔ¨Ådence in the correct label yi and the classiÔ¨Åer‚Äôs conÔ¨Ådence in label j,
‚àÜ(j)(fŒ∏(xi), yi) = p(fŒ∏(xi) = yi) ‚àíp(fŒ∏(xi) = j)."
MULTI-CLASS MARGIN REWEIGHTING,0.24609375,"For brevity we denote ‚àÜ(fŒ∏(xi), yi) as ‚àÜi. Note that the multi-class margin exhibits two qualities:"
MULTI-CLASS MARGIN REWEIGHTING,0.25,"1. Correct/incorrect classiÔ¨Åcation is implicit in the deÔ¨Ånition, as negative values indicate an
incorrect classiÔ¨Åcation.
2. The true class of the sample is also implicit‚Äîi.e. the index with element zero (assuming the
sample does not lie exactly on a decision boundary separating the true class from another)."
MULTI-CLASS MARGIN REWEIGHTING,0.25390625,"In particular, we highlight the second quality. Prior work has demonstrated that the distribution
of predictions made on adversarial samples is not necessarily uniform over all classes (Abbasi &
Gagn√©, 2017). In other words, vulnerable samples and their associated adversarial perturbations
may concentrate about certain classes more than others. We demonstrate in the results that networks
exhibit non-uniform robustness per-class."
MULTI-CLASS MARGIN REWEIGHTING,0.2578125,"To learn the sample weights as a function of the multiclass margin, we construct an auxiliary neural
network with a single hidden layer, whose parameters are denoted ¬µ and whose inputs are the multi-
class margins. The weight of the i-th training sample is then computed as wi = œâ¬µ(‚àÜi). In general,
we denote the function used to map from margin to weight œâ(¬∑). A question that arises is what loss
function should be used to train this auxiliary network. We design a bilevel optimization approach
leveraging the validation set to learn the auxiliary network parameters ¬µ."
BILEVEL OPTIMIZATION,0.26171875,"3.2
BILEVEL OPTIMIZATION"
BILEVEL OPTIMIZATION,0.265625,"We exploit a validation set to jointly learn a parametric weighting function œâ¬µ on the train-
ing samples and a classiÔ¨Åer which jointly minimize the associated weighted robust error. Let
ÀÜLtr(Œ∏t, wt) = Pm
i=1 wt,iÀÜ‚Ñìi(Œ∏t), where wt,i = œâ¬µt(‚àÜi) be the weighted robust training loss with
respect to parameters Œ∏t and ¬µt at time t. Additionally, wt,i ‚â•0 and Pmb
i=1 wt,y = 1. Intuitively, the"
BILEVEL OPTIMIZATION,0.26953125,Under review as a conference paper at ICLR 2022
BILEVEL OPTIMIZATION,0.2734375,"samples with high weights should improve robust generalization‚Äîthis is quantiÔ¨Åed by the robust
error evaluated on a held-out validation set. Let ÀÜLval(Œ∏t) = 1"
BILEVEL OPTIMIZATION,0.27734375,"n
Pn
i=1 ÀÜ‚Ñìi(Œ∏t) be the unweighted robust
validation loss associated with Œ∏t. Following the meta-learning principle, we seek weights such that
the minimizer of the weighted robust training loss maximizes robust accuracy on the unweighted
validation set‚Äîi.e. solve the following bilevel optimization problem:"
BILEVEL OPTIMIZATION,0.28125,"arg min
Œ∏
ÀÜLtr(Œ∏, œâ¬µ‚àó(‚àÜ))
s.t. ¬µ‚àó‚ààarg min
¬µ
ÀÜLval(Œ∏)
(4)"
BILEVEL OPTIMIZATION,0.28515625,"The workÔ¨Çow of the reweighting procedure is illustrated in Fig. 2(A) and the reweighting algorithm
is presented in Alg. 1. We provide a high-level overview of the procedure below."
BILEVEL OPTIMIZATION,0.2890625,"Step 2:
ùùÅùíï= ùêåùêÄùêåùêã(ùùÅùíï(ùüè, ùúΩ,ùíï)"
BILEVEL OPTIMIZATION,0.29296875,Step 1
BILEVEL OPTIMIZATION,0.296875,"ùúΩ,ùíï= ùêÜùêÉ(ùúΩùíï(ùüè, ùíòùíï(ùüè)"
BILEVEL OPTIMIZATION,0.30078125,"A
Iteration t: Update parametric weights ùë§(ùúá)"
BILEVEL OPTIMIZATION,0.3046875,Meta-update
BILEVEL OPTIMIZATION,0.30859375,Unweighted
BILEVEL OPTIMIZATION,0.3125,"loss ùìõùíóùíÇùíç
ùë•8
ùúÉ:,
ùë•;"
BILEVEL OPTIMIZATION,0.31640625,"ùë§;,:(< ùúÉt-1"
BILEVEL OPTIMIZATION,0.3203125,Weighted
BILEVEL OPTIMIZATION,0.32421875,loss ùìõùíïùíì ùúát-1
BILEVEL OPTIMIZATION,0.328125,"‚Ñìùíä,ùíï(ùüè Œî;"
BILEVEL OPTIMIZATION,0.33203125,Step 3:
BILEVEL OPTIMIZATION,0.3359375,"ùúΩùíï= ùêÜùêÉ(ùúΩùíï(ùüè, ùíòùíï)"
BILEVEL OPTIMIZATION,0.33984375,"B
Iteration t: Update network parameters ùúÉ ùë•; ùë§;,: ùúÉt-1"
BILEVEL OPTIMIZATION,0.34375,Weighted
BILEVEL OPTIMIZATION,0.34765625,loss ùìõùíïùíì ùúát
BILEVEL OPTIMIZATION,0.3515625,"‚Ñìùíä,ùíï(ùüè Œî;"
BILEVEL OPTIMIZATION,0.35546875,"Figure 2: BiLAW Framework. (A) MAML-inspired sample weighting. Step 1: intermediate
parameters ÀúŒ∏t are computed by pseudo-update of Œ∏t. Step 2: Validation loss gradients (calculated
via back-propagation through the weighted training loss) are used to update the parameters of the
auxiliary weighting network ¬µt. (B) Step 3: network parameters Œ∏t updated using new weights wt."
BILEVEL OPTIMIZATION,0.359375,"Our approach is composed of three steps. Steps 1 and 2 rely on the MAML-trick (Finn et al., 2017),
which substitutes one-step updates ¬µt for ¬µ‚àóand iteratively solves the upper-level problem. In this
context, ¬µt is updated according to the gradient of the unweighted robust validation loss with respect
to the sample weights. We note that this method necessitates computation of a pseudo-update in order
to compute this gradient:"
BILEVEL OPTIMIZATION,0.36328125,"Step 1 Pseudo update of classiÔ¨Åer parameters ÀúŒ∏t (Step 1 in Fig. 2, line 5 in Alg. 1)"
BILEVEL OPTIMIZATION,0.3671875,"ÀúŒ∏t = Œ∏t ‚àíŒ≤‚àáŒ∏ ÀÜLtr(Œ∏t, wt‚àí1)
(5)"
BILEVEL OPTIMIZATION,0.37109375,The pseudo parameters ÀúŒ∏t are then used as a surrogate for Œ∏t in optimizing for ¬µ:
BILEVEL OPTIMIZATION,0.375,"Step 2 Update parameters ¬µt of the auxiliary network (Step 2 in Fig. 2, line 6 in Alg. 1)"
BILEVEL OPTIMIZATION,0.37890625,"¬µt = ¬µt‚àí1 ‚àíŒ±Œ≤ mn m
X j=1 Ô£´ Ô£≠
n
X i=1"
BILEVEL OPTIMIZATION,0.3828125,"‚àÇÀÜ‚Ñìval
i (ÀúŒ∏)
‚àÇÀúŒ∏ ÀúŒ∏t"
BILEVEL OPTIMIZATION,0.38671875,"!‚ä§‚àÇÀÜ‚Ñìtr
j(Œ∏)
‚àÇŒ∏ Œ∏t‚àí1 Ô£∂ Ô£∏‚àÇw ‚àÇ¬µ"
BILEVEL OPTIMIZATION,0.390625,"¬µt
,
(6)"
BILEVEL OPTIMIZATION,0.39453125,"where Œ± and Œ≤ are the step size used in the pseudo and auxiliary network updates, respectively."
BILEVEL OPTIMIZATION,0.3984375,"Step 3 Update parameters of classiÔ¨Åer network (Step 3 in Fig. 2, line 8 in Alg. 1)"
BILEVEL OPTIMIZATION,0.40234375,"Œ∏t+1 = Œ∏t ‚àíŒ≤‚àáŒ∏ ÀÜLtr(Œ∏t, wt)
(7)"
BILEVEL OPTIMIZATION,0.40625,"One interpretation of this procedure is that we take a pseudo-step using Œ∏t‚àí1 and ¬µt‚àí1 (Step 1),
calculate the best auxiliary network parameters ¬µt in hindsight that improve generalization, by
minimizing the validation loss with ÀúŒ∏t, (Step 2), and then derive the ‚Äúreal‚Äù update for Œ∏t‚àí1 by
minimizing the weighted training loss using the new weights ¬µt (Step 3). The detailed derivation of the"
BILEVEL OPTIMIZATION,0.41015625,gradient update is provided in the appendix. Note that the term 1
BILEVEL OPTIMIZATION,0.4140625,"n
Pn
i=1 "
BILEVEL OPTIMIZATION,0.41796875,"‚àÇÀÜ‚Ñìval
i (ÀúŒ∏)
‚àÇŒ∏ ÀúŒ∏t"
BILEVEL OPTIMIZATION,0.421875,"!‚ä§
‚àÇÀÜ‚Ñìtr
j(Œ∏)
‚àÇŒ∏"
BILEVEL OPTIMIZATION,0.42578125,"Œ∏t‚àí1
in Eq. (6) represents the correlation between the gradient of the j-th training sample computed on
the training loss and the average gradient of the validation data calculated on the robust validation
loss. As a consequence, if the gradient of the loss with respect to the network parameters at time t
for training sample j is aligned with the average gradient of the meta-loss, it will be considered a
beneÔ¨Åcial sample for generalization and its weight will be increased. Conversely, the weight of the
sample is suppressed if the gradient is anticorrelated with the average validation set-gradient."
BILEVEL OPTIMIZATION,0.4296875,Under review as a conference paper at ICLR 2022
BILEVEL OPTIMIZATION,0.43359375,Algorithm 1 BiLAW training procedure
BILEVEL OPTIMIZATION,0.4375,"Input: Training data D, validation-data set ÀÜD, max iterations T, learning rates Œ±, Œ≤
Output: ClassiÔ¨Åer parameters Œ∏"
BILEVEL OPTIMIZATION,0.44140625,"1: t ‚Üê0
2: Initialize Œ∏0, ¬µ0, w0 = œâ¬µ0(‚àÜ)
3: for t ‚â§T do
4:
(X, y) ‚àºD, ( ÀÜ
X, ÀÜy) ‚àºÀÜD
5:
ÀúŒ∏t ‚ÜêŒ∏t ‚àíŒ≤ ¬∑ ‚àáŒ∏ ÀÜLtr|ÀúŒ∏t,wt
6:
¬µt+1 = ¬µt ‚àíŒ±‚àá¬µ ÀÜLval|Œ∏t,wt
‚ñ∑compute ‚àá¬µ ÀÜLval|ÀúŒ∏t,wt via backpropagation according to Eq. 6
7:
compute wt+1 = œâ¬µt+1(‚àÜ)
‚ñ∑compute ‚àÜwith respect to Œ∏t according to Def 3
8:
Œ∏t+1 = Œ∏t ‚àíŒ≤‚àáŒ∏ ÀÜLtr|Œ∏t,wt+1
9: end for
10: return Œ∏T"
EXPERIMENTS,0.4453125,"4
EXPERIMENTS"
EXPERIMENTS,0.44921875,"In this section, we evaluate the efÔ¨Åcacy of our framework on a variety of datasets, and demonstrate
that our technique improves robustness while preserving clean accuracy. We introduce three variants
based on our reweighting technique:"
EXPERIMENTS,0.453125,"1) Non-parametric reweighting: we learn weights using the weighted adversarial cross-entropy loss
where the weight wj,t for sample j at iteration t is proportional to the correlation between the training"
EXPERIMENTS,0.45703125,loss gradient and the average validation loss gradient: 1
EXPERIMENTS,0.4609375,"n
Pn
i=1 "
EXPERIMENTS,0.46484375,"‚àÇÀÜ‚Ñìval
i (ÀúŒ∏)
‚àÇŒ∏ ÀúŒ∏t"
EXPERIMENTS,0.46875,"!‚ä§
‚àÇÀÜ‚Ñìtr
j(Œ∏)
‚àÇŒ∏"
EXPERIMENTS,0.47265625,"Œ∏t‚àí1
."
EXPERIMENTS,0.4765625,"2) BiLAW (Parametric reweighting, Sec. 3) trained using the weighted adversarial cross-entropy loss."
EXPERIMENTS,0.48046875,"3) BiLAW can be modiÔ¨Åed to BiLAW-TRADES: Parametric reweighting trained with the TRADES
loss (Zhang et al., 2019), i.e. we solve min
Œ∏ X"
EXPERIMENTS,0.484375,"i
‚Ñì(fŒ∏(xi), yi) + 1/Œª(wi ¬∑ KL(fŒ∏(xi), fŒ∏(xi + Œ¥))),
(8)"
EXPERIMENTS,0.48828125,"where ‚Ñì() corresponds to the standard cross-entropy loss, KL corresponds to the KL-divergence, Œ¥
corresponds to an adversarially perturbation, and wi = œâ¬µ(‚àÜi): the parametric map applied to the
multi-class margin of fŒ∏ at xi. For all experiments, we set 1/Œª = 6, and deÔ¨Åne œâ to be a single hidden-
layer fully connected ReLU network with 128 hidden units and a sigmoid activation. Furthermore, to
enforce aforementioned constraints, we normalize the weights per-batch‚Äîi.e. wi = wi/ P j wj."
PERFORMANCE EVALUATION,0.4921875,"4.1
PERFORMANCE EVALUATION"
PERFORMANCE EVALUATION,0.49609375,"We evaluate the performance of our approach compared to plain training, adversarial training
(AT) (Madry et al., 2018), GAIRAT (Zhang et al., 2021), WMMR (Zeng et al., 2021), and
MAIL (Wang et al., 2021). All experiments are run on a single RTX 2080 Ti. When applying
our approach and variants, two validation sets of size 1000 are extracted from the training set: one is
used to learn the auxiliary network parameters, and the second is used for early stopping. This results
in a smaller training set for BiLAW, while the training sets of competing methods are unaltered."
PERFORMANCE EVALUATION,0.5,"In Table 1, we evaluate BiLAW using two relatively small networks on two datasets: MNIST (LeCun
& Cortes, 2010) and Fashion MNIST (Xiao et al., 2017). Tiny-CNN is a convolutional network with
2 convolutional and 2 dense layers. FC1 corresponds to a single hidden layer feedforward network
with 1024 hidden units. The details of the architectures are given in the Appendix. We consider
robustness with respect to ‚Ñì‚àûdistance. We use three criteria: clean test accuracy (clean), robust test
accuracy (PGD) for a given threshold œµ and AutoAttack (AA). Robust test accuracy is computed
using Projected Gradient Descent (PGD) (Madry et al., 2018) with 20 iterations. In all testcases, our
method matches the performance of GAIRAT and out-performs the other methods for clean and PGD
accuracy and we out-perform all reweighting methods on AA accuracy. However, we note the overall
distribution of both clean and robust accuracy is tight. We note a potential drawback of reweighting"
PERFORMANCE EVALUATION,0.50390625,Under review as a conference paper at ICLR 2022
PERFORMANCE EVALUATION,0.5078125,"algorithms: the MNIST and F-MNIST datasets contain a non-trivial number of misclassiÔ¨Åed samples
which can inÔ¨Çuence performance (M√ºller & Markert, 2019). For algorithms which perform weighted
training, possible large weights on outliers or mislabeled examples may inÔ¨Çuence classiÔ¨Åcation
performance. We will investigate this in the context of adversarial training in future work."
PERFORMANCE EVALUATION,0.51171875,"Table 1: MNIST/F-MNIST comparison for plain, AT, GAIRAT, WMMR (Œ±train = 0.1, Œ±test = 2),
MAIL (Œ≥ = 5, Œ≤ = 0.05) and BiLAW using standard robust loss. Best result is underlined and
bolded and second best is bolded."
PERFORMANCE EVALUATION,0.515625,"Tiny-CNN
FC1
perturbation: ‚Ñì‚àû
perturbation: ‚Ñì2
perturbation: ‚Ñì‚àû
perturbation: ‚Ñì2
Clean
PGD
AA
Clean
PGD
AA
Clean
PGD
AA
Clean
PGD
AA
MNIST
œµ = 0.1
œµ = 0.3
œµ = 0.1
œµ = 0.3
plain
99.1
21.7
9.1
99.2
96.9
36.4
98.4
1.7
0.0
98.3
90.3
16.1
AT
99.0
95.9
93.7
99.1
98.2
96.1
98.4
92.9
90.4
8.8
97.4
95.3
GAIRAT
99.1
96.7
91.1
99.2
98.8
90.3
99.0
93.2
89.7
98.8
97.6
89.2
WMMR
98.8
94.3
90.2
99.0
98.5
91.7
98.9
92.8
89.4
98.2
97.2
89.8
MAIL
98.6
95.1
91.4
98.7
98.6
95.4
98.4
93.1
91.3
98.1
97.4
94.2
BiLAW(ours)
99.2
96.7
91.7
99.2
98.9
95.4
99.1
93.1
91.6
98.6
97.6
94.4
F-MNIST
œµ = 0.1
œµ = 0.3
œµ = 0.1
œµ = 0.3
plain
89.6
1.5
0.0
89.7
42.9
0.0
98.5
0.0
0.0
89.3
57.2
0.0
AT
86.4
70.1
68.3
91.9
79.6
77.9
87.0
68.7
66.3
89.8
80.1
76.0
GAIRAT
86.4
77.6
64.3
92.3
81.1
70.3
87.1
70.2
61.4
91.1
81.0
70.4
WMMR
86.2
77.3
64.1
92.1
80.6
71.4
86.9
68.4
61.3
91.1
78.4
70.9
MAIL
86.4
76.9
68.6
92.2
80.5
76.2
90.1
69.3
66.4
90.6
79.3
75.9
BiLAW(ours)
86.6
77.4
68.8
92.4
81.3
76.6
87.3
70.6
66.7
91.4
80.9
76.1"
PERFORMANCE EVALUATION,0.51953125,"Table 2: CIFAR-10 comparison for AT, GAIRAT, WMMR, non-parametric weighting, and BiLAW
with standard adversarial training (BiLAW) and with TRADES loss (BiLAW-TRADES). We perform
AA on 1000 samples in the test set. Best result is underlined and bolded and second best is bolded."
PERFORMANCE EVALUATION,0.5234375,"Small-CNN
WRN-10-32
perturbation: ‚Ñì‚àû
perturbation: ‚Ñì‚àû
perturbation: ‚Ñì‚àû
perturbation: ‚Ñì‚àû
Clean
PGD
AA
Clean
PGD
AA
Clean
PGD
AA
Clean
PGD
AA
CIFAR-10
œµ = 0.0078
œµ = 0.031
œµ = 0.0078
œµ = 0.031
GAIRAT
79.0
54.7
48.1
79.0
55.6
40.7
86.4
73.6
63.1
84.7
56.8
43.4
WMMR
78.7
58.9
51.2
81.7
49.1
39.1
85.9
70.9
67.4
80.6
49.5
40.6
MAIL
76.8
64.3
59.2
81.9
53.3
40.6
84.3
74.1
73.7
83.2
53.7
52.0
AT
78.7
58.7
56.6
79.6
45.6
42.9
85.9
71.3
69.5
85.9
52.0
48.0
TRADES (1/Œª = 6)
79.2
58.9
56.8
78.9
54.8
51.7
84.6
73.9
73.1
83.1
53.9
52.1
Non-parametric weighting
79.7
60.0
47.3
81.3
52.2
40.6
86.4
73.7
62.3
86.6
52.8
42.9
BiLAW (ours)
79.7
63.6
56.7
80.4
55.4
45.3
87.1
74.2
71.3
87.4
58.6
51.4
BiLAW-TRADES (ours)
79.1
64.8
61.5
80.2
56.2
52.6
86.2
74.8
74.2
86.1
58.9
53.6"
PERFORMANCE EVALUATION,0.52734375,"Table 3: CIFAR-100 comparison for AT,
GAIRAT, WMMR, and BiLAW with the
TRADES loss. We perform AA on 1000
samples in the test set. The best result
is underlined and bolded and the second
best is bolded."
PERFORMANCE EVALUATION,0.53125,"WRN-32-10
perturbation: ‚Ñì‚àû
Clean
PGD
AA
CIFAR-100
œµ = 0.031
BiLAW-TRADES (ours)
62.8
31.4
27.2
AT
57.9
28.9
24.7
TRADES (1/Œª = 1)
62.4
25.3
22.2
TRADES (1/Œª = 6)
56.5
30.9
26.9
GAIRAT
60.2
30.4
22.6
WMMR
56.1
29.7
25.8
MAIL
62.4
30.9
26.8"
PERFORMANCE EVALUATION,0.53515625,"In Table 2, we evaluate our method using the two architec-
tures used in Zhang et al. (2021) on CIFAR-10 (Krizhevsky
et al.): a 6-layer convolutional network and a Wide-Resnet-
10-32 (WRN-32-10) (Zagoruyko & Komodakis, 2016),
with details provided in the Appendix.
We run each
method for 100 epochs with training and validation batch
sizes set to 128 using SGD + momentum. A standard learn-
ing rate schedule is implemented with the initial learning
rate of 0.1 divided by 10 at Epoch 30 and 60, respectively.
BiLAW strictly outperforms AT with respect to both clean
and robust accuracy and generally outperforms GAIRAT
and WMMR with respect to clean and robust accuracy on
CIFAR-10 (up to 10%). In particular, BiLAW consistently
achieves superior clean test accuracy in all testcases, ex-
cept for the ‚Ñì‚àûsmall-CNN (œµ = 0.031). On the WRN ‚Ñì‚àû
case, we maintain and outperform relevant methods with respect to both PGD-based and AA-based
robust accuracy while achieving superior clean test accuracy. We demonstrate that when used in
conjunction with TRADES, BiLAW preserves and improves robustness to AA attacks by 1.5% in
contrast to TRADES, while signiÔ¨Åcantly enhancing clean test accuracy by up to 3% and PGD attacks
by up to 5%. We also note that parametric reweighting as opposed to the non-parametric version
signiÔ¨Åcantly improves robust accuracy. On CIFAR-100 (Table 3) BiLAW-TRADES out-performs all
other methods. Our results demonstrate the effectiveness of using a held-out validation set to learn
the sample weights compared to heuristic reweighting schemes."
PERFORMANCE EVALUATION,0.5390625,Under review as a conference paper at ICLR 2022
PERFORMANCE EVALUATION,0.54296875,"In the Appendix we conduct three ablative experiments to analyze the effect of (1) the capacity of the
auxiliary weighting network, (2) the Ô¨Åxed TRADES coefÔ¨Åcient and (3) the input to the weighting
network, on the performance of BiLAW."
TRAINING SAMPLE WEIGHTS,0.546875,"4.2
TRAINING SAMPLE WEIGHTS"
TRAINING SAMPLE WEIGHTS,0.55078125,"We investigate the correspondence between weights and samples, and ask the question: what are the
properties of training examples with high/low weights? Fig. 3 provides evidence that supports our
claim that samples for which the auxiliary network predicts high weights correspond to vulnerable, or
difÔ¨Åcult samples close to the decision boundary. In Fig. 3(a)-(b), we plot the distribution of weights
for each class, as well the associated confusion matrix of predictions made by a robust classiÔ¨Åer
(trained with BiLAW) on adversarial samples. We note that the distribution of weights matches the
distribution of misclassiÔ¨Åed adversarial examples. For example, in Fig. 3(a), samples of the ‚Äòship‚Äô and
‚Äòautomobile‚Äô class are assigned a higher number of smaller weights and they are typically classiÔ¨Åed
correctly as in Fig. 3b. In contrast, birds, cats, and other animals have a higher number of samples
assigned large weight and are more frequently misclassiÔ¨Åed. (c)"
TRAINING SAMPLE WEIGHTS,0.5546875,"(a)
(b) (d)"
TRAINING SAMPLE WEIGHTS,0.55859375,"Figure 3: (a) Weight distribution and examples of CIFAR-10 samples. (b) Adversarial confusion
matrix of a robust network on CIFAR-10. (c) ‚ÄúEasy‚Äù CIFAR-10 samples with low weight are correctly
classiÔ¨Åed. (d) ‚ÄúHard‚Äù CIFAR-10 samples with high weight are typically incorrectly classiÔ¨Åed."
TRAINING SAMPLE WEIGHTS,0.5625,"In Fig. 3(c), we provide several examples of test samples that are assigned low weight. These images
are typically very clear, involving a centered object and plain background. In Fig. 3(d), we provide
a set of test samples assigned high weights. Many of these images are challenging for humans to
identify, even when uncorrupted by adversarial noise. For example, the second and Ô¨Åfth image
are pictures of cats and birds with unusual pose. The seventh, eighth, and ninth image are nearly
impossible to identify due to complex backgrounds or obscured objects. Additionally, the second,
third, eighth, and tenth images consist of multiple objects that could confuse the network or facilitate
more effective adversarial attacks. Thus, high weights can be used to identify adversarially vulnerable
samples, and automatically differentiate easy and challenging samples in existing datasets."
CONCLUSION,0.56640625,"5
CONCLUSION"
CONCLUSION,0.5703125,"We have introduced BiLAW, a new robust training method that learns classiÔ¨Åers that are robust to
norm-bounded adversarial attacks and is inspired by the hypothesis that reweighting via bilevel opti-
mization offers a remedy to the issue of adversarial overÔ¨Åtting. We demonstrate that our method learns
robust networks that out-performs competing methods,including related, geometrically-motivated
techniques. Notably, BiLAW does not rely on complicated heuristics to assign weights, and we
demonstrate the learned weights are interpretable. Future work involves improving scalability and
investigating whether the auxiliary network might be used to detect adversarial corruptions."
CONCLUSION,0.57421875,Under review as a conference paper at ICLR 2022
REFERENCES,0.578125,REFERENCES
REFERENCES,0.58203125,"Mahdieh Abbasi and Christian Gagn√©. Robustness to adversarial examples through an ensemble of
specialists. ArXiv, abs/1702.06856, 2017."
REFERENCES,0.5859375,"Han Bao, Clay Scott, and Masashi Sugiyama. Calibrated surrogate losses for adversarially robust
classiÔ¨Åcation. In Proceedings of Thirty Third Conference on Learning Theory, volume 125
of Proceedings of Machine Learning Research, pp. 408‚Äì451. PMLR, 09‚Äì12 Jul 2020. URL
http://proceedings.mlr.press/v125/bao20a.html."
REFERENCES,0.58984375,"Jerome Bracken and James T. McGill. Mathematical programs with optimization problems in
the constraints. Operations Research, 21(1):37‚Äì44, 1973. ISSN 0030364X, 15265463. URL
http://www.jstor.org/stable/169087."
REFERENCES,0.59375,"Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overÔ¨Åtting
may be mitigated by properly learned smoothening. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=qZzy5urZw9."
REFERENCES,0.59765625,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. CertiÔ¨Åed adversarial robustness via randomized
smoothing. volume 97 of Proceedings of Machine Learning Research, pp. 1310‚Äì1320, Long
Beach, California, USA, 09‚Äì15 Jun 2019. PMLR."
REFERENCES,0.6015625,"Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Multi-class classiÔ¨Åcation with maximum
margin multiple kernel. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the
30th International Conference on Machine Learning, volume 28 of Proceedings of Machine
Learning Research, pp. 46‚Äì54, Atlanta, Georgia, USA, 17‚Äì19 Jun 2013. PMLR. URL https:
//proceedings.mlr.press/v28/cortes13.html."
REFERENCES,0.60546875,"Francesco Croce and Matthias Hein. Provable robustness against all adversarial lp-perturbations
for p ‚â•1. In International Conference on Learning Representations, 2020a. URL https:
//openreview.net/forum?id=rklk_ySYPB."
REFERENCES,0.609375,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML, 2020b."
REFERENCES,0.61328125,"Amir Erfan Eshratifar, David Eigen, and Massoud Pedram. Gradient agreement as an optimization
objective for meta-learning. CoRR, abs/1810.08178, 2018. URL http://arxiv.org/abs/
1810.08178."
REFERENCES,0.6171875,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning,
volume 70, pp. 1126‚Äì1135. PMLR, 06‚Äì11 Aug 2017. URL http://proceedings.mlr.
press/v70/finn17a.html."
REFERENCES,0.62109375,"Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015."
REFERENCES,0.625,"Simon Jenni and Paolo Favaro. Deep bilevel learning. In Vittorio Ferrari, Martial Hebert, Cristian
Sminchisescu, and Yair Weiss (eds.), Computer Vision ‚Äì ECCV 2018, pp. 632‚Äì648, Cham, 2018.
Springer International Publishing. ISBN 978-3-030-01249-6."
REFERENCES,0.62890625,"Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced
Research). URL http://www.cs.toronto.edu/~kriz/cifar.html."
REFERENCES,0.6328125,"Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/."
REFERENCES,0.63671875,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018."
REFERENCES,0.640625,"Nicolas Michael M√ºller and Karla Markert. Identifying mislabeled instances in classiÔ¨Åcation datasets.
CoRR, abs/1912.05283, 2019. URL http://arxiv.org/abs/1912.05283."
REFERENCES,0.64453125,Under review as a conference paper at ICLR 2022
REFERENCES,0.6484375,"Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with
implicit gradients. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
072b030ba126b2f4b2374f342be9ed44-Paper.pdf."
REFERENCES,0.65234375,"Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. In ICML, 2018."
REFERENCES,0.65625,"Leslie Rice, Eric Wong, and J. Zico Kolter. OverÔ¨Åtting in adversarially robust deep learning. CoRR,
abs/2002.11569, 2020. URL https://arxiv.org/abs/2002.11569."
REFERENCES,0.66015625,"Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,
and Raia Hadsell. Meta-learning with latent embedding optimization. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
BJgklhAcK7."
REFERENCES,0.6640625,"Mohammad Saberian and Nuno Vasconcelos. Multiclass boosting: Margins, codewords, losses,
and algorithms. Journal of Machine Learning Research, 20(137):1‚Äì68, 2019. URL http:
//jmlr.org/papers/v20/17-137.html."
REFERENCES,0.66796875,"Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya P. Razenshteyn, and S√©bastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classiÔ¨Åers. CoRR,
abs/1906.04584, 2019."
REFERENCES,0.671875,"Amartya Sanyal, Puneet K. Dokania, Varun Kanade, and Philip Torr.
How benign is benign
overÔ¨Åtting ? In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=g-wu9TMPODo."
REFERENCES,0.67578125,"Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-
net: Learning an explicit mapping for sample weighting. In NeurIPS, 2019."
REFERENCES,0.6796875,"Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural
networks. CoRR, abs/1710.08864, 2017."
REFERENCES,0.68359375,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv, abs/1312.6199, 2014."
REFERENCES,0.6875,"Qizhou Wang, Feng Liu, Bo Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan Zhou, and
Masashi Sugiyama. Probabilistic margins for instance reweighting in adversarial training. CoRR,
abs/2106.07904, 2021. URL https://arxiv.org/abs/2106.07904."
REFERENCES,0.69140625,"Eric Wong and Zico J. Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. International Conference on Machine Learning (ICML), 2018."
REFERENCES,0.6953125,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. CoRR, abs/1708.07747, 2017."
REFERENCES,0.69921875,"Mingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Zhi-Ming Ma. Reweighting augmented
samples by minimizing the maximal expected loss. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=9G5MIc-goqB."
REFERENCES,0.703125,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.
URL http://arxiv.org/abs/1605.07146."
REFERENCES,0.70703125,"Huimin Zeng, Chen Zhu, Tom Goldstein, and Furong Huang. Are adversarial examples created
equal? a learnable weighted minimax risk for robustness under non-uniform attacks. In AAAI,
2021."
REFERENCES,0.7109375,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. volume 97 of Proceedings of
Machine Learning Research, pp. 7472‚Äì7482, Long Beach, California, USA, 09‚Äì15 Jun 2019."
REFERENCES,0.71484375,Under review as a conference paper at ICLR 2022
REFERENCES,0.71875,"Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankan-
halli. Attacks which do not kill training make adversarial learning stronger. In ICML, 2020."
REFERENCES,0.72265625,"Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli.
Geometry-aware instance-reweighted adversarial training. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=iAX0l6Cz8ub."
REFERENCES,0.7265625,"Hui Zou. The margin vector , admissible loss and multi-class margin-based classiÔ¨Åers. 2005."
REFERENCES,0.73046875,"A
APPENDIX"
REFERENCES,0.734375,"A.1
DERIVATION OF META GRADIENT"
REFERENCES,0.73828125,In this section we derive the update rule for the parameters of the auxiliary network in Eq. 6:
REFERENCES,0.7421875,"¬µt = ¬µt‚àí1 ‚àíŒ±Œ≤ mn m
X j=1 Ô£´ Ô£≠
n
X i=1"
REFERENCES,0.74609375,"‚àÇÀÜ‚Ñìval
i (ÀúŒ∏)
‚àÇÀúŒ∏ ÀúŒ∏t"
REFERENCES,0.75,"!‚ä§‚àÇÀÜ‚Ñìtr
j(Œ∏)
‚àÇŒ∏ Œ∏t‚àí1 Ô£∂ Ô£∏‚àÇw ‚àÇ¬µ ¬µt
, Let"
REFERENCES,0.75390625,"ÀÜLtr(Œ∏t, w) = 1 m m
X"
REFERENCES,0.7578125,"j=1
wj ÀÜ‚Ñìj(Œ∏t)"
REFERENCES,0.76171875,"be the robust training loss with respect to parameters Œ∏ at time t and example weight wj for the
j-th training example. Let ÀÜLval(Œ∏t) = 1"
REFERENCES,0.765625,"n
Pn
i=1 ÀÜ‚Ñìi(Œ∏t) be the associated unweighted validation loss.
Following the meta-learning framework, we to minimize this loss via gradient descent."
REFERENCES,0.76953125,‚àÇÀÜLval(ÀúŒ∏)
REFERENCES,0.7734375,"‚àÇ¬µ
= 1 n n
X i"
REFERENCES,0.77734375,"‚àÇÀÜ‚Ñìval
i (ÀúŒ∏)
‚àÇ¬µ = 1 n n
X i"
REFERENCES,0.78125,"‚àÇÀÜ‚Ñìval
i (ÀúŒ∏)
‚àÇÀúŒ∏
‚àÇÀúŒ∏
‚àÇw
‚àÇw ‚àÇ¬µ"
REFERENCES,0.78515625,To compute ‚àÇÀúŒ∏
REFERENCES,0.7890625,"‚àÇw, we can apply the MAML technique and differentiate through the pseudo update
(recall, ÀúŒ∏t = GDtr(Œ∏t‚àí1, wt‚àí1) := Œ∏t‚àí1 ‚àíŒ±‚àáŒ∏Ltr,t‚àí1(Œ∏t‚àí1, w)). For example, a single gradient
descent step:"
REFERENCES,0.79296875,"‚àÇÀúŒ∏
‚àÇw = ‚àÇ"
REFERENCES,0.796875,"‚àÇw(Œ∏t‚àí1 ‚àíŒ±‚àáŒ∏Ltr,t‚àí1(Œ∏t‚àí1, w)) = Œ±
m m
X"
REFERENCES,0.80078125,"i=1
‚àáŒ∏ÀÜ‚Ñìtr
t‚àí1(Œ∏t‚àí1) !"
REFERENCES,0.8046875,So the complete update is:
REFERENCES,0.80859375,"¬µt = ¬µt‚àí1 ‚àíŒ±Œ≤ mn m
X j=1 n
X i=1"
REFERENCES,0.8125,"‚àÇÀÜ‚Ñìval
i (ÀúŒ∏)
‚àÇÀúŒ∏  ‚ä§ ÀúŒ∏t"
REFERENCES,0.81640625,"‚àÇÀÜ‚Ñìtr
j(Œ∏)
‚àÇŒ∏ Œ∏t‚àí1 ‚àÇw ‚àÇ¬µ ¬µt !"
REFERENCES,0.8203125,"A.2
MAIN EXPERIMENTS"
REFERENCES,0.82421875,"A.2.1
ARCHITECTURES"
REFERENCES,0.828125,"We abbreviate one hidden layer fully connected network with 1024 hidden units with FC1. The
tiny-CNN convolutional architecture that we use is identical to that of Wong & Kolter (2018); Croce
& Hein (2020a) ‚Äîconsisting of two convolutional layers with 16 and 32 Ô¨Ålters of size 4√ó4 and stride
2, followed by a fully connected layer with 100 hidden units. For all experiments we use training
and validation batch sizes of 128 and we train all models for 100 epochs. Moreover, we use SGD
with a piecewise constant learning rate schedule with initial learning rate of 0.1. The learning rate is
divided by 10 at epochs 30 and 60 respectively. On all datasets (MNIST, F-MNIST, CIFAR-10, and"
REFERENCES,0.83203125,Under review as a conference paper at ICLR 2022
REFERENCES,0.8359375,Table 4: Architectures for main experiments for number of classes nc.
REFERENCES,0.83984375,"FC1
tiny-CNN
small-CNN"
REFERENCES,0.84375,"FC(1024)
Conv(16, 4 √ó 4, 2)
small-CNN-BLOCK(64)
ReLU
ReLU
small-CNN-BLOCK(128)
FC(nc)
Conv(32, 4 √ó 4, 2)
small-CNN-BLOCK(196)
ReLU
FC(256)
FC(100)
ReLU
ReLU
FC(nc)
FC(nc)"
REFERENCES,0.84765625,Table 5: Architectures for main experiments for number of classes nc.
REFERENCES,0.8515625,small-CNN-BLOCK(c)
REFERENCES,0.85546875,"Conv(c, 3 √ó 3, 1)"
REFERENCES,0.859375,BatchNorm
REFERENCES,0.86328125,"ReLU
Conv(c, 3 √ó 3, 1)"
REFERENCES,0.8671875,BatchNorm
REFERENCES,0.87109375,"ReLU
MaxPool(2 √ó 2)"
REFERENCES,0.875,"CIFAR-100) we restrict the input to be in the range [0, 1]. On the CIFAR-10 dataset, following Zhang
et al. (2021), we apply random crops and random mirroring of the images as data augmentation
during training. We perform adversarial training using the PGD attack of Madry et al. (2018). During
training, we perform 10 iterations of the PGD attack for all datasets. During evaluation, we use
20 iterations for all datasets. Following Zhang et al. (2021), the step size is the perturbation radius
divided by 4."
REFERENCES,0.87890625,"A.3
ABLATION STUDY"
REFERENCES,0.8828125,"In this section, we evaluate variations of our technique on CIFAR-10 using the WRN-32-10 archi-
tecture and ‚Ñì‚àûwith œµ = 0.031. First, we explore how the capacity of the auxiliary reweighting
technique inÔ¨Çuences the performance of our method. Next, we evaluate our network in combination
with TRADES for various values of 1/Œª. Finally, we demonstrate the advantage of the multi-class
margin over alternative inputs mapping to the sample weights‚Äîe.g. using the class-unaware margin
(Def. 1), the adversarial loss ‚àÜadv, and the difference between the adversarial loss and the clean loss
at a sample ‚àÜdiff."
REFERENCES,0.88671875,Table 6: Ablation experiments: capacity of the axuiliary weight prediction network
REFERENCES,0.890625,"Capacity of œâ
CIFAR10
Clean
PGD
PGD - Clean
128 hidden units
86.1
58.9
27.2
64
83.6
57.4
26.2
64 ‚àí64
85.8
58.6
27.1
256
85.7
57.7
28"
REFERENCES,0.89453125,"In Table 6, we evaluate the inÔ¨Çuence of the auxiliary network capacity, i.e. the choice of œâ. As
with training robust classiÔ¨Åers, the architecture of the network inÔ¨Çuences the clean-robust tradeoff,
with smaller capacity networks reducing the gap between clean and robust performance, and larger
networks increasing the gap."
REFERENCES,0.8984375,"In Table 7, we show relative robustness of our approach combined with TRADES to the choice of
1/Œª. In particular, we maintain an important advantage of TRADES: the ability to easily control the
robustness tradeoff by controlling 1/Œª."
REFERENCES,0.90234375,Under review as a conference paper at ICLR 2022
REFERENCES,0.90625,Table 7: Ablation experiments: TRADES coefÔ¨Åcient 1/Œª
REFERENCES,0.91015625,"1/Œª
CIFAR10
Clean
PGD
1/Œª = 6
86.1
58.9
1/Œª = 1
87.4
52.5
1/Œª = 5
86.9
57.6
1/Œª = 10
83.8
57.9"
REFERENCES,0.9140625,"Table 8: Ablation experiments: ‚àÜ, input to the auxiliary network"
REFERENCES,0.91796875,"Network input
CIFAR10
Clean
PGD
multiclass margin (Def. 3)
86.1
58.9
margin (Def. 2)
84.1
54.6
ÀÜ‚Ñì
86.9
57.9
ÀÜ‚Ñì‚àí‚Ñì
85.4
53.8"
REFERENCES,0.921875,"In Table 8, we show that the choice of input to the auxiliary neural network to predict the sample
weights has a signiÔ¨Åcant impact. In particular, we show the necessity of using the multi-class margin
to achieve superior clean and robust test accuracy. Surprisingly, conditioning the weight on the robust
loss also leads to good performance, better than the margin , and employing a learnable map for either
the class-aware and class-unaware outperforms heuristic methods (e.g., WMMR and MAIL)."
REFERENCES,0.92578125,"A.4
CIFAR-10 EXAMPLE WEIGHTS"
REFERENCES,0.9296875,"In Fig 4 we recover the predictions made by a small-CNN trained with BiLAW. We then use principal
component analysis (PCA) to project 10-dimensional predicted class likelihoods into 2-dimensions
and plot the corresponding embeddings. The color denotes the degree of the robustness of each
data point. Samples which are assigned larger weight are darker. As expected, these samples
associated with high weights lie close to the decision boundary and are more likely to improve robust
generalization."
REFERENCES,0.93359375,"In Fig 5 we investigate the dynamics of predicted weights by visualizing the progression of weights
predicted at margins for training samples and their adversarial variants. We observe (1) the dynamics
of the weights seem to be determined largely by the learning rate of the classiÔ¨Åer (i.e. the Ô¨Årst
adjustment to the learning rate happens around epoch 20), (2) the majority of weights predicted
for clean samples are low (i.e. most clean samples are easy), and (3) the variance of the weight
distribution is quite tight for adversarial samples."
REFERENCES,0.9375,"In Fig 6 we compare weights computed via the GAIRAT and MAIL heuristics to weights predicted
via BiLAW and show a positive correlation. In particular, BiLAW may be considered a generalization
of the MAIL heuristic that additionally incorporates multi-class margin information. The similar-
ity between margin-based weight estimators BiLAW and MAIL is evident, while the PGD-based
GAIRAT weighting heuristic emphasizes a bimodal weight distribution."
REFERENCES,0.94140625,"Replicating (Fig. 3), we plot samples with small and large weight for competitive methods GAIRAT
and MAIL. As with out method, samples associated with small weights appear to be ‚Äúeasy‚Äù and visa
versa."
REFERENCES,0.9453125,"A.5
MNIST WEIGHT EXAMPLES"
REFERENCES,0.94921875,"We plot MNIST samples with small and large weight. As with CIFAR-10 (Fig. 3), samples associated
with small weights appear to be ‚Äúeasy‚Äù in the sense that the digits are neatly written. On the other
hand, digits associate with high weight are easily confused and often involve the occurrence or lack
of occurrence of spaces between strokes that deÔ¨Åne certain digits (e.g. 3, 5, 0, 9, and 8)."
REFERENCES,0.953125,Under review as a conference paper at ICLR 2022
REFERENCES,0.95703125,"(a)
(b)"
REFERENCES,0.9609375,"Figure 4: Two orientations of a 3-d plot of PCA applied to the model‚Äôs likelihood predictions
on training samples of three classes from the CIFAR-10 dataset (blue: car, red: plane, & green:
ship). The weight of individual samples (denoted by the shade) correlates with the margin/degree of
robustness."
REFERENCES,0.96484375,"(a)
(b)"
REFERENCES,0.96875,"Figure 5: (a) Progression of weights associated with a subset of clean training samples (b) Progression
of weights associated with a subset of adversarially perturbed training samples"
REFERENCES,0.97265625,"(a)
(b)
(c)"
REFERENCES,0.9765625,"(d)
(e)"
REFERENCES,0.98046875,"Figure 6: (a) BiLAW weight distributions per-class for CIFAR-10 samples. (b) MAIL weight
distributions per-class for CIFAR-10 samples. (c) Scatter plot of MAIL weight vs. BiLAW weight
for a robust network. (b) GAIRAT weight distributions per-class for CIFAR-10 samples. (c) Scatter
plot of GAIRAT weight vs. BiLAW weight for a robust network."
REFERENCES,0.984375,Under review as a conference paper at ICLR 2022 (a) (b)
REFERENCES,0.98828125,"Figure 7: Examples taken from CIFAR-10 and weighted using GAIRAT (Zhang et al., 2021). (a)
Samples with low weight. (b) Samples with high weight. (a) (b)"
REFERENCES,0.9921875,"Figure 8: Examples taken from CIFAR-10 and weighted using MAIL (Wang et al., 2021). (a)
Samples with low weight. (b) Samples with high weight. (a) (b)"
REFERENCES,0.99609375,Figure 9: Examples taken from MNIST. (a) Samples with low weight. (b) Samples with high weight.
