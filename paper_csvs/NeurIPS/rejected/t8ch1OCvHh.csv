Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009276437847866419,"Modern large language models (LLMs) have established state-of-the-art perfor-
1"
ABSTRACT,0.0018552875695732839,"mance through architectural improvements, but still require significant computa-
2"
ABSTRACT,0.0027829313543599257,"tional cost for inference. In an effort to reduce the inference cost, post-training
3"
ABSTRACT,0.0037105751391465678,"quantization (PTQ) has become a popular approach, quantizing weights and acti-
4"
ABSTRACT,0.00463821892393321,"vations to lower precision, such as INT8. In this paper, we reveal the challenges
5"
ABSTRACT,0.0055658627087198514,"of activation quantization in GLU variants [40], which are widely used in feed-
6"
ABSTRACT,0.006493506493506494,"forward network (FFN) of modern LLMs, such as LLaMA family. The problem is
7"
ABSTRACT,0.0074211502782931356,"that severe local quantization errors, caused by excessive magnitudes of activation
8"
ABSTRACT,0.008348794063079777,"in GLU variants, significantly degrade the performance of the quantized LLM. We
9"
ABSTRACT,0.00927643784786642,"denote these activations as activation spikes. Our further observations provide a
10"
ABSTRACT,0.01020408163265306,"systematic pattern of activation spikes: 1) The activation spikes occur in the FFN of
11"
ABSTRACT,0.011131725417439703,"specific layers, particularly in the early and late layers, 2) The activation spikes are
12"
ABSTRACT,0.012059369202226345,"dedicated to a couple of tokens, rather than being shared across a sequence. Based
13"
ABSTRACT,0.012987012987012988,"on our observations, we propose two empirical methods, Quantization-free Module
14"
ABSTRACT,0.013914656771799629,"(QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during
15"
ABSTRACT,0.014842300556586271,"quantization. Our extensive experiments validate the effectiveness of the proposed
16"
ABSTRACT,0.015769944341372914,"methods for the activation quantization, especially with coarse-grained scheme, of
17"
ABSTRACT,0.016697588126159554,"latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR,
18"
ABSTRACT,0.017625231910946195,"and Gemma. In particular, our methods enhance the current alleviation techniques
19"
ABSTRACT,0.01855287569573284,"(e.g., SmoothQuant) that fail to control the activation spikes.1
20"
INTRODUCTION,0.01948051948051948,"1
Introduction
21"
INTRODUCTION,0.02040816326530612,"Large language models (LLMs) have become a key paradigm in natural language processing, acceler-
22"
INTRODUCTION,0.021335807050092765,"ating the release of variations within the community [49, 58]. Furthermore, latest LLMs establish
23"
INTRODUCTION,0.022263450834879406,"state-of-the-art performance by training with increased scale, as well as by adopting architectural
24"
INTRODUCTION,0.023191094619666047,"improvements such as GLU [40], RoPE [41], GQA [2], and MoE [21]. Especially, GLU (Gated
25"
INTRODUCTION,0.02411873840445269,"Linear Unit) variants (e.g., SwiGLU, GeGLU) has been adopted in the most of modern LLM archi-
26"
INTRODUCTION,0.02504638218923933,"tectures (e.g., LLaMA family [46]), due to training efficiency [31, 40]. Although LLMs broaden
27"
INTRODUCTION,0.025974025974025976,"foundational capabilities in natural language tasks and potential for various applications, billions of
28"
INTRODUCTION,0.026901669758812616,"parameters in the large models impose considerable computational costs on end users in practice. To
29"
INTRODUCTION,0.027829313543599257,"reduce GPU memory requirements and accelerate inference speed, post-training quantization (PTQ)
30"
INTRODUCTION,0.0287569573283859,"offers an affordable solution by quantizing weights and activations into a lower precision (e.g., INT8)
31"
INTRODUCTION,0.029684601113172542,"without a need for expensive retraining steps [17, 19, 30]. However, recent studies have revealed that
32"
INTRODUCTION,0.030612244897959183,"large magnitude values at certain coordinates exist in the activations of LLMs, which are often called
33"
INTRODUCTION,0.03153988868274583,"outliers, posing a key challenge in activation quantization [1, 12, 50, 51]. Another line of works
34"
INTRODUCTION,0.032467532467532464,"attempts to explain the role of outlier values in the attention mechanism [9, 42]. Nevertheless, current
35"
INTRODUCTION,0.03339517625231911,"research on the impact of evolving LLM architectures on the outliers remains insufficient.
36"
INTRODUCTION,0.03432282003710575,1Code is available at https://anonymous.4open.science/r/activation-spikes-EDF0.
INTRODUCTION,0.03525046382189239,"In this paper, we present our discovery that the GLU architecture in the feed-forward network (FFN)
37"
INTRODUCTION,0.036178107606679034,"generates excessively large activation values, which are responsible for significant local quantization
38"
INTRODUCTION,0.03710575139146568,"errors. Specifically, we observe that these problematic activation values occur in specific linear
39"
INTRODUCTION,0.038033395176252316,"layers and are dedicated to a couple of tokens, which will be discussed in Section 3. To distinguish
40"
INTRODUCTION,0.03896103896103896,"the excessive GLU activations from the outliers, we refer to them as activation spikes. In light of
41"
INTRODUCTION,0.039888682745825604,"our observations, we propose two empirical methods to mitigate the impact of activation spikes
42"
INTRODUCTION,0.04081632653061224,"on quantization: Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP). QFeM
43"
INTRODUCTION,0.041743970315398886,"aims to partially exclude quantization for linear layers (or modules) where large quantization errors
44"
INTRODUCTION,0.04267161410018553,"occur, instead of quantizing the entire linear modules in the LLM. By scoring the extent of scale
45"
INTRODUCTION,0.04359925788497217,"disparity, QFeM selects linear modules to exclude. On the other hand, QFeP identifies the prefix that
46"
INTRODUCTION,0.04452690166975881,"triggers activation spikes and preserves its context as a key-value (KV) cache, thereby preventing the
47"
INTRODUCTION,0.045454545454545456,"recurrence of activation spikes in subsequent tokens. It is noteworthy that both QFeM and QFeP rely
48"
INTRODUCTION,0.04638218923933209,"on calibration results to capture activation spikes in advance, without any modifications to the target
49"
INTRODUCTION,0.04730983302411874,"LLM. This indicates that our methods can be integrated into any existing quantization methods.
50"
INTRODUCTION,0.04823747680890538,"In our comprehensive experiments, we demonstrate that recently released LLMs incorporating
51"
INTRODUCTION,0.04916512059369202,"GLU variants struggle with activation spikes when applying activation quantization. Consequently,
52"
INTRODUCTION,0.05009276437847866,"the proposed methods, QFeM and QFeP, substantially enhance the performance of the primitive
53"
INTRODUCTION,0.05102040816326531,"quantization method, the round-to-nearest (RTN) method. Furthermore, we observe that current
54"
INTRODUCTION,0.05194805194805195,"outlier alleviation methods [50, 51] are exposed to the activation spikes and benefit from our proposed
55"
INTRODUCTION,0.05287569573283859,"methods. Compared to the strong baseline of fine-grained activation quantization [55], our methods
56"
INTRODUCTION,0.05380333951762523,"show competitive performance, achieving reduced latency and memory footprint.
57"
INTRODUCTION,0.05473098330241188,"In summary, the contributions of our work are as follows:
58"
INTRODUCTION,0.055658627087198514,"• We find that the GLU architecture in modern LLMs systematically generates excessive activation
59"
INTRODUCTION,0.05658627087198516,"values, which are responsible for significant performance degradation in activation quantization.
60"
INTRODUCTION,0.0575139146567718,"• Based on our observations, we propose two empirical methods, QFeM and QFeP, which effectively
61"
INTRODUCTION,0.05844155844155844,"exclude the activation spikes during quantization, with negligible computational overhead and
62"
INTRODUCTION,0.059369202226345084,"compatibility with any existing quantization techniques.
63"
INTRODUCTION,0.06029684601113173,"• Our extensive experimental results validate the detrimental impact of the activation spikes on activa-
64"
INTRODUCTION,0.061224489795918366,"tion quantization, while our proposed methods consistently enhance the quantization performance.
65"
RELATED WORKS,0.06215213358070501,"2
Related Works
66"
RELATED WORKS,0.06307977736549165,"Outlier Values in LLMs.
Previously, outlier values have been observed in the transformer-based
67"
RELATED WORKS,0.0640074211502783,"language models such as BERT [14] and early GPT [36] models through numerous studies [8, 24,
68"
RELATED WORKS,0.06493506493506493,"27, 35, 45]. Since the advent of LLMs [10, 57] rooted in the GPT, recent studies by [1, 12, 51] have
69"
RELATED WORKS,0.06586270871985157,"tackled the existence of outlier values in LLMs. According to them, these outliers exhibit a large
70"
RELATED WORKS,0.06679035250463822,"magnitude of values at the shared dimensions of hidden states across tokens. More recently, [9, 42]
71"
RELATED WORKS,0.06771799628942486,"explain that the outliers attribute to the vertical pattern in the attention mechanism [25, 52], which
72"
RELATED WORKS,0.0686456400742115,"influences the performance of LLMs. In particular, [42] claims a different type of outlier existing in
73"
RELATED WORKS,0.06957328385899815,"the hidden states of specific tokens. However, prior studies merely focus on the superficial hidden
74"
RELATED WORKS,0.07050092764378478,"states between the decoder layers. Our work provides a module-level investigation where quantization
75"
RELATED WORKS,0.07142857142857142,"is applied practically, focusing on different LLM architectures.
76"
RELATED WORKS,0.07235621521335807,"Post-training Quantization for LLMs.
Post-training quantization (PTQ) refers to the quantization
77"
RELATED WORKS,0.07328385899814471,"of a neural network model to low precision, such as INT8, without additional parameter updates [17,
78"
RELATED WORKS,0.07421150278293136,"19]. Especially for LLMs, this approach cost-effectively achieves inference with low memory usage
79"
RELATED WORKS,0.075139146567718,"and faster inference latency by quantizing the weights and activations used in matrix multiplication
80"
RELATED WORKS,0.07606679035250463,"(e.g., linear layer). However, because of the challenges in activation quantization of LLMs, many
81"
RELATED WORKS,0.07699443413729128,"recent works are mainly focused on the weight-only quantization [11, 13, 15, 23, 26, 39, 54].
82"
RELATED WORKS,0.07792207792207792,"Otherwise, the activation quantization faces inherent outliers, which hinder accurate quantization
83"
RELATED WORKS,0.07884972170686456,"by reducing representation resolution. To address this challenge, [12] proposes a mixed-precision
84"
RELATED WORKS,0.07977736549165121,"quantization method where the outlier dimensions are computed in high precision. [50, 51] approach
85"
RELATED WORKS,0.08070500927643785,"migration of scale from activation to weights to alleviate the scale of outlier activations. Along this
86"
RELATED WORKS,0.08163265306122448,"line of research, we propose to enhance the activation quantization based on our observations.
87"
RELATED WORKS,0.08256029684601113,Activation
RELATED WORKS,0.08348794063079777,Spikes
RELATED WORKS,0.08441558441558442,Activation
RELATED WORKS,0.08534322820037106,Spikes
RELATED WORKS,0.0862708719851577,"(a) GLU-implemented LLMs
(b) Non GLU-implemented LLMs"
RELATED WORKS,0.08719851576994433,"Figure 1: Calibration results on GLU-implemented and non GLU-implemented LLMs. We present
the maximum magnitudes of input activations for each linear modules and layer-wise hidden states.
For more results on different LLMs, see Appendix A.2, A.3."
RELATED WORKS,0.08812615955473098,"3
Activation Spikes: Excessive Magnitude of GLU Activations
88"
RELATED WORKS,0.08905380333951762,"For clarity, ""hidden states"" refer to the output tensor of a transformer layer (or block), while ""input
89"
RELATED WORKS,0.08998144712430427,"activations"" or ""activations"" denote the input tensor of a linear layer (or module) in the remain of this
90"
RELATED WORKS,0.09090909090909091,"paper. Recent work [42] has investigated a novel type of outlier existing in the hidden states across
91"
RELATED WORKS,0.09183673469387756,"modern LLMs. Although these outliers of hidden states play a crucial role in the attention mechanism
92"
RELATED WORKS,0.09276437847866419,"[9, 42, 52], their relationship with input activations for quantization has not been fully explored.
93"
RELATED WORKS,0.09369202226345083,"Importantly, because recent LLMs adopt Pre-LN [4, 53], which normalizes hidden states before self-
94"
RELATED WORKS,0.09461966604823747,"attention and feed-forward network (FFN) blocks, the scale of hidden states does not reflect the scale
95"
RELATED WORKS,0.09554730983302412,"of input activations within the transformer block. Therefore, we focus on the input activations fed into
96"
RELATED WORKS,0.09647495361781076,"each linear module within the transformer block to connect to activation quantization. Specifically, we
97"
RELATED WORKS,0.09740259740259741,"examine the four linear (projection) layers: query (parallel to key and value), out, up (parallel to
98"
RELATED WORKS,0.09833024118738404,"gate), and down modules. For detailed illustration of Pre-LN transformer, please see Appendix D.1.
99"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.09925788497217068,"3.1
Existence of Activation Spikes in GLU Variants
100"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10018552875695733,"To analyze the input activations, we employ a calibration method, which is used to estimate the
101"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10111317254174397,"quantization factors such as scale and zero-point. For the calibration data, we use 512 samples
102"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10204081632653061,"randomly collected from the C4 [37] training dataset. Afterwards, we feed each sample into the LLM
103"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10296846011131726,"and monitor each hidden state and input activation through the decoder layers. To estimate the scale
104"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.1038961038961039,"factor, we use absolute maximum value. The tested LLMs are listed in Appendix A.1.
105"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10482374768089053,"GLU-implemented LLMs exhibit activation spikes at specific layers.
In Figure 1a, we display
106"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10575139146567718,"the calibrated scale factors for the LLMs that implement GLU variants (e.g., SwiGLU, GeGLU).
107"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10667903525046382,"Across models, we observe a shared pattern of scale from the results. Within the early and late
108"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10760667903525047,"layers, the down modules in the FFN show noticeable magnitudes of input activations. Note that
109"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10853432282003711,"these input activations are derived from the Hadamard Product within GLU. Thus, the GLU variants
110"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.10946196660482375,"generate activation spikes at the specific layers. Interestingly, we notice a high correlation between the
111"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11038961038961038,"emergence of activation spikes and intermediate hidden states of large scale. This indicates that the
112"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11131725417439703,"FFN contributes to amplifying the hidden states via the addition operation in the residual connection
113"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11224489795918367,"[18]. Once the magnitude of the hidden states is exploded, it persists through layers until encounter
114"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11317254174397032,"the activation spikes at late layers.
115"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11410018552875696,"Non GLU-implemented LLMs show modest scale distribution.
Figure 1b illustrates the cali-
116"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.1150278293135436,"bration results for LLMs with the original feed-forward implementation in Transformer [48]. We
117"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11595547309833024,"observe that the LLMs continue to generate the large-scale hidden states, regardless of the GLU
118"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11688311688311688,"implementation. This corresponds to the observations in [42]. More importantly, our module-level
119"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11781076066790352,"results elaborate that the scale of hidden states is not transferable to the input activations of inner
120"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11873840445269017,"linear modules. Instead, we reveal that GLU variants are associated with the hidden states and
121"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.11966604823747681,"generate activation spikes. This clarifies the quantization challenge of the GLU-implemented LLMs
122"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.12059369202226346,"concentrated in the early and late layers. Because excessive scales of activation spikes have the
123"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.12152133580705009,"potential to hinder the accurate quantization, we conduct an in-depth analysis to better understand
124"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.12244897959183673,"these activation spikes in the following sections.
125"
EXISTENCE OF ACTIVATION SPIKES IN GLU VARIANTS,0.12337662337662338,"BOS
It
'
s
snow \n
It
'
s
snow \n 0 500"
K,0.12430426716141002,1k
K,0.12523191094619665,1.5k
K,0.1261595547309833,LLaMA-2-7B Layer 2 feed-forward.down
K,0.12708719851576994,"BOS
It
'
s
snow \n
It
'
s
snow \n 0"
K,0.1280148423005566,2k
K,0.12894248608534323,4k
K,0.12987012987012986,6k
K,0.13079777365491652,LLaMA-2-70B Layer 9 feed-forward.down
K,0.13172541743970315,"Per-token Scale
Per-tensor Scale"
K,0.1326530612244898,Activation Magnitude
K,0.13358070500927643,"Figure 2: Token-wise scales in a specific layer with an activation spike. When quantizing the input
activations using a per-tensor scale, the scale of the activation spike dominates the scales of the other
tokens. For more examples, see Appendix D.2."
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.1345083487940631,"3.2
Token-level Scale Analysis within Activation Spikes
126"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.13543599257884972,"In the previous section, we observed the excessive scale of the input activations derived from GLU
127"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.13636363636363635,"activation. When quantizing the input activations, the variance of input activation scales for each
128"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.137291280148423,"token affects the quantization performance [55]. To delve into the disparity between token-wise
129"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.13821892393320964,"scales in the activation spikes, we unroll them through the sequence of tokens. Figure 2 illustrates
130"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.1391465677179963,"the individual input activation scales where the activation spike appears. Given a token sequence,
131"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.14007421150278293,"the large magnitudes of input activations are observed in a couple of tokens, such as the BOS token,
132"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.14100185528756956,"newline (\n), and apostrophe ('). These specific tokens coincide with the observations of [42], which
133"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.14192949907235622,"suggests that such tokens exhibit massive values in the hidden states. Thus, the activation spike is
134"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.14285714285714285,"associated with the process of assigning a special role to these tokens in later transformer layers.
135"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.1437847866419295,"However, the excessive scale of specific token hinders the estimation of scale factor for the other
136"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.14471243042671614,"tokens, such as in per-tensor quantization. Additionally, the largest scale is dedicated to the first
137"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.1456400742115028,"instance of the specified token, while the following usage exhibits a modest scale. This phenomenon
138"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.14656771799628943,"makes the quantization more complicated, as the activation spikes dynamically occur depending on
139"
TOKEN-LEVEL SCALE ANALYSIS WITHIN ACTIVATION SPIKES,0.14749536178107606,"the current input sequence.
140"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.14842300556586271,"3.3
Effect of Quantization on Activation Spikes
141"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.14935064935064934,"We explore the impact of local quantization errors caused by activation spikes on LLM outputs. To
142"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.150278293135436,"identify the layers where activation spikes occur, we utilize a ratio between the maximum and median
143"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.15120593692022263,"values of the token-wise input activation scales, instead of using the maximum scale value alone.
144"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.15213358070500926,"The max-median ratio for linear layer m can be formulated as r(m) =
max(S(m))
median(S(m)), where S(m)
145"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.15306122448979592,"represents the token-wise input activation scales incoming to module m ∈M. This max-median
146"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.15398886827458255,"ratio captures the extent to which maximum scale dominate the other token scales. For comparison,
147"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.1549165120593692,"we choose the activation quantization targets as the top-4, middle-4, and bottom-4 modules, based on
148"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.15584415584415584,"the max-median ratio in descending order. Then, we evaluate the perplexity and mean-squared error
149"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.1567717996289425,"(MSE) using the calibration dataset. Here, the MSE is calculated for the last hidden states between
150"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.15769944341372913,"the original (FP16) and partially quantized LLM. As shown in Table 1, quantization on the top-4 rated
151"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.15862708719851576,"modules solely degrades the LLM performance by significant margins, while the other cases exhibit
152"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.15955473098330242,"negligible performance changes. We consider these quantization-sensitive input activations (inter alia
153"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.16048237476808905,"activation spikes) to be the quantization bottleneck, which, in this paper, refers to the quantization
154"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.1614100185528757,"error caused by outliers.
155"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.16233766233766234,"Furthermore, the activation spikes are conditioned on the specific context of the input sequence as
156"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.16326530612244897,"discussed in Section 3.2. Altogether, such dynamic bottlenecks must be handled with caution to
157"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.16419294990723562,"enhance the quantization performance of LLMs.
158"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.16512059369202226,Table 1: Perplexity and MSE of partial activation quantization of LLMs
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.1660482374768089,"Model
Perplexity(↓)
MSE(↓)"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.16697588126159554,"FP16
Top 4
Middle 4
Bottom 4
Top 4
Middle 4
Bottom 4"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.1679035250463822,"LLaMA-2-7B
7.37
11.77
7.38
7.40
1908.80
1.03
12.90
LLaMA-2-13B
6.84
15.09
6.84
6.84
4762.11
0.91
10.38
Mistral-7B
8.35
69.45
8.35
8.36
218.60
0.02
0.18
Gemma-7B
10.85
85.83
10.94
10.87
213.93
1.60
1.07"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.16883116883116883,Offline Inference
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.16975881261595546,"𝑥!
𝑥""
𝑥#
𝑥$
𝑥%"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.17068645640074212,Quantization-free Module (QFeM) 𝑟! 139
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.17161410018552875,"3
2
0.9"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.1725417439703154,"3
3
2
0.9 4136"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.17346938775510204,"3
2
1.3"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.17439703153988867,Quantization-free Prefix (QFeP)
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.17532467532467533,"Layer 𝑖
Layer 𝑗
Layer 𝑘"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.17625231910946196,KV Cache INT8
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.17717996289424862,quantization-free module
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.17810760667903525,quantization-free module
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.1790352504638219,"𝑥!
𝑥""
𝑥# FP16"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.17996289424860853,"𝑥$
𝑥%
𝑥& INT8 𝑞/𝑘/𝑣"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.18089053803339517,"𝑜𝑢𝑡
𝑢𝑝/𝑔𝑎𝑡𝑒 𝑑𝑜𝑤𝑛 𝑞/𝑘/𝑣"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.18181818181818182,"𝑜𝑢𝑡
𝑢𝑝/𝑔𝑎𝑡𝑒 𝑑𝑜𝑤𝑛 𝑞/𝑘/𝑣"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.18274582560296845,"𝑜𝑢𝑡
𝑢𝑝/𝑔𝑎𝑡𝑒 𝑑𝑜𝑤𝑛 𝑟! 139"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.1836734693877551,"3
2
0.9"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.18460111317254174,"3
3
2
0.9 4136"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.18552875695732837,"3
2
1.3 𝑟!"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.18645640074211503,"2
3
2
0.9"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.18738404452690166,"3
3
2
0.9"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.18831168831168832,"2
3
2
1.3"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.18923933209647495,"Input Sequence:
quantization-free prefix"
EFFECT OF QUANTIZATION ON ACTIVATION SPIKES,0.1901669758812616,"Figure 3: Overview of QFeM and QFeP. (Left): QFeM excludes the modules whose r(m) is larger than
the hyperparameter α from quantization. (Right): QFeP computes in advance the prefix of activation
spikes and utilizes solely their KV cache during the quantization phase, effectively preventing further
activation spikes in subsequent sequences."
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19109461966604824,"4
Mitigating Quantization Quality Degradation Based on the Observation
159"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19202226345083487,"To address the quantization bottleneck, our approach is based on the deterministic occurrence patterns
160"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19294990723562153,"of activation spikes. First, we utilize the observation that bottlenecks occur at a few specific layers.
161"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19387755102040816,"This implies that naive full quantization of LLMs is affected by these bottlenecks. Second, we exploit
162"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19480519480519481,"the phenomenon that the activation spike is derived from the first occurrence of specific tokens. Thus,
163"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19573283858998144,"the planned occurrence prevents recurrence in the subsequent and possibly future tokens. In the
164"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19666048237476808,"following sections, we propose two methods inspired the above insights.
165"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19758812615955473,"4.1
Quantization-free Module (QFeM)
166"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19851576994434136,"In the full quantization of LLM, all linear layers within the LLM are quantized. Among these
167"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.19944341372912802,"linear layers, we propose omitting the quantization of input activations for linear layers where
168"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.20037105751391465,"significant quantization errors are caused by activation spikes. To be noted, increasing the number of
169"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2012987012987013,"unquantized modules exhibits a trade-off between the inference latency and the model performance.
170"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.20222634508348794,"Thus, determining which module should be quantized (or left unquantized) is crucial to retain the
171"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.20315398886827457,"efficacy of quantization. Here, we use the max-median ratio r(m) and define a set of unquantized
172"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.20408163265306123,"modules, denoted as Munq, where the ratio r(m) of each linear layer is larger than threshold α. For
173"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.20500927643784786,"instance, all linear layers in M are quantized if α = ∞. For clarity, we treat sibling linear layers,
174"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.20593692022263452,"such as query-key-value, as a single linear layer. To control the impact of activation quantization only,
175"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.20686456400742115,"we leave the weight parameters in unquantized linear layers as INT8 and dequantize them into FP16
176"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2077922077922078,"during matrix multiplication with the incoming activations, operating as weight-only quantization.
177 1 2 4 8 16 32 64 128 256 inf"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.20871985157699444,Threshold 0 50 100 150 10 15
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.20964749536178107,"20
|Munq|"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.21057513914656772,Perplexity
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.21150278293135436,"Figure 4: Trade-off between perplex-
ity (stands for performance) and |Munq|
(stands for latency) according to the
threshold α for LLaMA-2-13B model."
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.212430426716141,"Optimizing the threshold α.
To calculate the activation
178"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.21335807050092764,"scale ratio for each linear layer, we first gather token-wise
179"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.21428571428571427,"input activation scales from the calibration examples dis-
180"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.21521335807050093,"cussed in Section 3.1. Exceptionally, for FFN experts in
181"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.21614100185528756,"the mixture of experts (MoE) architectures like the Mix-
182"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.21706864564007422,"tral model [21], calibration is performed separately. After
183"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.21799628942486085,"determining these ratios, we use binary search to set the
184"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2189239332096475,"threshold value α, balancing inference latency and perfor-
185"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.21985157699443414,"mance degradation. As a metric, we assess performance
186"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22077922077922077,"through perplexity measured on the same calibration ex-
187"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22170686456400743,"amples. For example, the relationship between threshold
188"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22263450834879406,"value α and its impact on performance is depicted in Fig-
189"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22356215213358072,"ure 4, demonstrating how full quantization can degrade
190"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22448979591836735,"performance. Rather than fully quantizing, we identify an
191"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22541743970315398,"optimal threshold by finding the intersection of two performance curves; in Figure 4, this threshold is
192"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22634508348794063,"approximately 16. Details on the QFeM implementation are provided in Table 2.
193"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22727272727272727,"4.2
Quantization-free Prefix (QFeP)
194"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22820037105751392,"Orthogonal to the QFeM, we propose Quantization-free Prefix (QFeP) that mitigates the quantization
195"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.22912801484230055,"errors by precomputing the prefix (or short prompt) corresponding to activation spikes. This method
196"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2300556586270872,"is based on the observations presented in Section 3.2, which indicate that significant quantization
197"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23098330241187384,"errors result from the overestimated scale factor of the first instance within the restricted token
198"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23191094619666047,"set. Inspired by this occurrence pattern of activation spikes, we aim to construct a prefix which
199"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23283858998144713,"stabilizes the quantization scale factor of the tokens that come after the prefix. In other words,
200"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23376623376623376,"once the prefix is fixed at the beginning, the activation spikes consistently occur within the prefix.
201"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23469387755102042,"Afterward, we employ key-value (KV) caching mechanism to process the activation spikes in advance.
202"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23562152133580705,"In practice, KV cache is utilized to optimize the decoding speed of causal language models by storing
203"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23654916512059368,"precomputed key and value states of the previous tokens [32, 34]. This approach provides a bypass
204"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23747680890538034,"of the quantization including activation spikes, while preserving the context of prefix through the
205"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23840445269016697,"KV cache. The KV cache for the prefix is precomputed once through the offline inference of LLM
206"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.23933209647495363,"without quantization. Then, this KV cache is exploited in the quantization phases, such as calibration
207"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24025974025974026,"or dynamic quantization, even for quantized inference. The process of QFeP is illustrated in Figure 3.
208"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24118738404452691,"Prefix Search.
To form a prefix of explicit activation spike, we first identify candidate token that
209"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24211502782931354,"represent the activation spike at the linear layer with the highest max-median ratio r(m). For instance,
210"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24304267161410018,"the candidate token can be apostrophe (') token for LLaMA-2-70B model, as highlighted in red in
211"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24397031539888683,"Figure 2. Once the candidate token is identified, we search the middle context token for between
212"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24489795918367346,"the BOS token and the candidate token in the prefix. This middle context provides dummy context,
213"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24582560296846012,"which is required to activate the candidate token. To find the middle context, we design a template
214"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24675324675324675,"[B, T1, C1, T2, C2] where B, Ti, and Ci denote the BOS token, context token, and candidate token in
215"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24768089053803338,"the vocabulary V , respectively. Then, we select the context token T where C1 triggers an activation
216"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24860853432282004,"spikes, while later instance of the same token C2 does not. When the context token for the activation
217"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.24953617810760667,"spikes is varied, we choose the token that maximizes the activation scale ratio between the C1 and
218"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2504638218923933,"C2. Finally, we prepare the KV cache for searched prefix of [B, T, C]. Note that the latter sequence
219"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.25139146567717996,"in the template can be replaced with sequences from dataset instead of repetition.
220"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2523191094619666,"Table 2: Specifications for QFeM and QFeP used
in experiments. |M| denotes the total number of
linear layers in the LLM, and |Munq| represents
the number of unquantized layers for QFeM."
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2532467532467532,"Model
Prefix
α
|Munq|/|M|"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2541743970315399,"LLaMA-2-7B
[BOS] all .
6.68
17 / 128
LLaMA-2-13B
[BOS] then ,
12.91
6 / 160
LLaMA-2-70B
[BOS] I ’
9.16
25 / 320
Mistral-7B
[BOS] how \n
49.00
3 / 128
Mixtral-8x7B
[BOS] ). \n
4.03
191 / 608
SOLAR-10.7B
[BOS] a 1
6.48
11 / 192
Gemma-7B
[BOS] . Più
10.65
5 / 112
LLaMA-3-8B
[BOS] - nd
6.64
6 / 128
LLaMA-3-70B
[BOS] and ,
78.37
3 / 320"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.25510204081632654,"Implementation Details.
During the prefix
221"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2560296846011132,"search phase, we exploit the calibration dataset
222"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2569573283858998,"used in Section 3.1. For the candidate tokens, we
223"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.25788497217068646,"consider the tokens with the top three largest in-
224"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2588126159554731,"put activation magnitudes. Then, we search for
225"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2597402597402597,"the middle context token among top 200 most fre-
226"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2606679035250464,"quent tokens in the calibration dataset, which is
227"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.26159554730983303,"the subset of the vocabulary V . Finally, with the
228"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2625231910946197,"search result, we prepare the KV cache for the
229"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2634508348794063,"target model in FP16 precision. Exceptionally, for
230"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.26437847866419295,"the Mixtral [21] model, we use the scale of output
231"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2653061224489796,"hidden states instead of input activations, as the
232"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.2662337662337662,"tokens are divided sparsely in a mixture of experts
233"
MITIGATING QUANTIZATION QUALITY DEGRADATION BASED ON THE OBSERVATION,0.26716141001855287,"architecture. Table 2 presents the searched prefix.
234"
EXPERIMENTS,0.2680890538033395,"5
Experiments
235"
EXPERIMENTAL SETUP,0.2690166975881262,"5.1
Experimental Setup
236"
EXPERIMENTAL SETUP,0.2699443413729128,"Models.
Our proposed methods, QFeM and QFeP, aim to mitigate the quantization bottleneck,
237"
EXPERIMENTAL SETUP,0.27087198515769945,"which is discussed in Section 3.3, caused by the activation spikes, especially in the GLU variants. To
238"
EXPERIMENTAL SETUP,0.2717996289424861,"validate the efficiency proposed methods, we tested publicly released LLMs that were implemented
239"
EXPERIMENTAL SETUP,0.2727272727272727,"with GLU, according to their paper and source code. We recognize recent LLMs, including LLAMA-
240"
EXPERIMENTAL SETUP,0.27365491651205937,"2-{7B, 13B, 70B} [47], LLaMA-3-{7B, 70B}, Mistral-7B [20], Mixtral-8x7B [21], SOLAR-10.7B
241"
EXPERIMENTAL SETUP,0.274582560296846,"[22], and Gemma-7B [43], utilize the GLU architecture. The LLMs with original FFN are not
242"
EXPERIMENTAL SETUP,0.2755102040816326,"covered, as they suffer from the existing outliers rather than activation spikes. All models are sourced
243"
EXPERIMENTAL SETUP,0.2764378478664193,"from the huggingface-hub2 repository.
244"
EXPERIMENTAL SETUP,0.27736549165120594,2https://huggingface.co/models
EXPERIMENTAL SETUP,0.2782931354359926,"Table 3: Perplexity and zero-shot evaluation for the quantization on LLaMA-2 models. FP16 denotes
the original model precision, and W8A8 denotes the model quantized to INT8 for both weights and
activations."
EXPERIMENTAL SETUP,0.2792207792207792,"Method
WikiText-2
(ppl↓)
PIQA
(acc↑)
LAMBADA
(acc↑)
HellaSwag
(acc↑)
WinoGrande
(acc↑)
Avg
(acc↑)"
EXPERIMENTAL SETUP,0.28014842300556586,LLaMA-2-7B
EXPERIMENTAL SETUP,0.2810760667903525,"FP16
5.268
78.18%
73.67%
57.13%
69.46%
69.61%"
EXPERIMENTAL SETUP,0.2820037105751391,"W8A8
8.634
72.80%
62.27%
49.57%
63.69%
62.08%
+QFeM
5.758[-2.876]
78.02%
73.86%
56.32%
68.35%
69.14%[+7.06]
+QFeP
5.758[-2.876]
76.44%
73.57%
55.55%
69.22%
68.69%[+6.61]
+QFeM+QFeP
5.573[-3.061]
77.86%
74.58%
56.05%
69.38%
69.47%[+7.39]"
EXPERIMENTAL SETUP,0.2829313543599258,LLaMA-2-13B
EXPERIMENTAL SETUP,0.28385899814471244,"FP16
4.789
79.49%
76.54%
60.20%
72.38%
72.15%"
EXPERIMENTAL SETUP,0.2847866419294991,"W8A8
34.089
70.13%
49.66%
42.65%
58.72%
55.29%
+QFeM
5.241[-28.848]
77.58%
75.68%
59.13%
72.61%
71.25%[+15.96]
+QFeP
6.000[-28.089]
77.53%
73.94%
57.23%
70.96%
69.91%[+14.62]
+QFeM+QFeP
5.126[-28.963]
78.51%
75.86%
59.44%
72.61%
71.61%[+16.32]"
EXPERIMENTAL SETUP,0.2857142857142857,LLaMA-2-70B
EXPERIMENTAL SETUP,0.28664192949907236,"FP16
3.218
81.45%
79.45%
65.29%
80.43%
76.65%"
EXPERIMENTAL SETUP,0.287569573283859,"W8A8
8.055
74.05%
70.27%
55.21%
67.96%
66.87%
+QFeM
3.830[-4.225]
81.23%
77.66%
64.15%
78.14%
75.30%[+8.43]
+QFeP
6.007[-2.048]
77.64%
73.26%
63.40%
76.16%
72.62%[+5.75]
+QFeM+QFeP
3.708[-4.347]
81.23%
77.82%
64.65%
77.11%
75.20%[+8.33] 50 60 70"
EXPERIMENTAL SETUP,0.2884972170686456,Accuracy(%)
EXPERIMENTAL SETUP,0.2894248608534323,Mistral-7B 73 74 75
EXPERIMENTAL SETUP,0.29035250463821893,Mixtral-8x7B 65.0 67.5 70.0 72.5
EXPERIMENTAL SETUP,0.2912801484230056,SOLAR-10.7B 40 50 60 70
EXPERIMENTAL SETUP,0.2922077922077922,Gemma-7B 55 60 65 70
EXPERIMENTAL SETUP,0.29313543599257885,LLaMA-3-8B 30 40 50 60 70 80
EXPERIMENTAL SETUP,0.2940630797773655,LLaMA-3-70B
EXPERIMENTAL SETUP,0.2949907235621521,"W8A8
QFeM
QFeP
QFeM+QFeP
FP16"
EXPERIMENTAL SETUP,0.29591836734693877,"Figure 5: The average accuracy of zero-shot evaluation on other GLU-implemented LLMs. Most
models recover significantly compared to W8A8, with performance close to FP16."
EXPERIMENTAL SETUP,0.29684601113172543,"Quantization.
In the experiments, we quantize both the input activations and the weights of linear
245"
EXPERIMENTAL SETUP,0.29777365491651203,"layers for INT8 matrix multiplication operations. Note that in Table 2, |M| denotes the total number
246"
EXPERIMENTAL SETUP,0.2987012987012987,"of linear modules targeted for quantization. In these linear layers, we opt for dynamic per-tensor
247"
EXPERIMENTAL SETUP,0.29962894248608535,"quantization as the quantization scheme of input activations, and per-channel quantization for weights,
248"
EXPERIMENTAL SETUP,0.300556586270872,"respectively. Regarding both input activations and weights, we symmetrically quantize the range
249"
EXPERIMENTAL SETUP,0.3014842300556586,"using the absolute maximum value as the scale estimation function. For comparison, we use FP16
250"
EXPERIMENTAL SETUP,0.30241187384044527,"and per-token activation quantization [55] as baselines. We refer the reader to Appendix B for Batch
251"
EXPERIMENTAL SETUP,0.3033395176252319,"Matrix-Multiplication (BMM) quantization, which involves quantizing tensors in the self-attention.
252"
EXPERIMENTAL SETUP,0.3042671614100185,"Evaluations.
We evaluate the quantized LLMs with two metrics: zero-shot evaluation accuracy
253"
EXPERIMENTAL SETUP,0.3051948051948052,"and perplexity. For zero-shot evaluation, we use the four datasets: PIQA [7], LAMBADA [33],
254"
EXPERIMENTAL SETUP,0.30612244897959184,"HellaSwag [56], and WinoGrande [38]. We utilize the lm-evaluation-harness library [16] to evaluate
255"
EXPERIMENTAL SETUP,0.3070500927643785,"zero-shot tasks. To measure perplexity, we use the WikiText-2 [28] dataset. In all cases, we use the
256"
EXPERIMENTAL SETUP,0.3079777365491651,"[BOS] token as the starting token for each input sequence by default.
257"
MAIN RESULTS,0.30890538033395176,"5.2
Main Results
258"
MAIN RESULTS,0.3098330241187384,"LLaMA-2 Models.
We report the evaluation results of quantization on LLaMA-2 models in Table 3.
259"
MAIN RESULTS,0.310760667903525,"Compared to FP16 precision, quantizing both weights and activations (W8A8) degrades the overall
260"
MAIN RESULTS,0.3116883116883117,"performance. The results demonstrate that our proposed methods resolve the activation spikes
261"
MAIN RESULTS,0.31261595547309834,"and, surprisingly, restore the performance of the W8A8 close to that of FP16. For example, the
262"
MAIN RESULTS,0.313543599257885,"LLaMA-2 7B model achieves less than a 1% performance drop from FP16. It is worth noting that the
263"
MAIN RESULTS,0.3144712430426716,"Table 4: Evaluation of outlier alleviation methods with QFeM and QFeP. We report perplexity on
WikiText-2 and averaged accuracy of four zero-shot tasks. The same quantization scheme for used
on both SQ and OSP. Per-tensor weight quantization results are provided in Appendix C.1."
MAIN RESULTS,0.31539888682745826,"Method
LLaMA-2-7B
LLaMA-2-13B
LLaMA-2-70B"
MAIN RESULTS,0.3163265306122449,"ppl(↓)
acc(↑)
ppl(↓)
acc(↑)
ppl(↓)
acc(↑)"
MAIN RESULTS,0.3172541743970315,"SQ [51]
9.907
61.08%
34.869
59.45%
8.800
70.25%
+QFeM
5.534
69.65%
5.118
71.23%
3.599
75.93%
+QFeP
5.715
68.66%
6.551
69.33%
5.228
74.07%"
MAIN RESULTS,0.3181818181818182,"OSP [50]
38.490
59.90%
5.148
71.29%
3.827
75.52%
+QFeM
5.493
69.37%
5.099
71.37%
3.559
75.92%
+QFeP
5.642
68.95%
5.144
71.05%
3.752
75.36%"
MAIN RESULTS,0.31910946196660483,"proposed QFeM and QFeP improve at comparable levels. This indicates that the activation spikes
264"
MAIN RESULTS,0.3200371057513915,"present a direct cause of the significant decrease in quantization performance. Because the proposed
265"
MAIN RESULTS,0.3209647495361781,"methods are orthogonal, the performance slightly increases when incorporating both QFeM and QFeP
266"
MAIN RESULTS,0.32189239332096475,"compared to applying them individually.
267"
MAIN RESULTS,0.3228200371057514,"Other GLU-implemented LLMs.
For other LLMs that incorporate GLU, we investigated the
268"
MAIN RESULTS,0.323747680890538,"effectiveness of our methods in mitigating the quantization bottleneck. As can be seen in Figure 5,
269"
MAIN RESULTS,0.3246753246753247,"our methods consistently remedy the performance drop caused by activation spikes. Noticeably,
270"
MAIN RESULTS,0.32560296846011133,"the Mixtral model demonstrates robustness towards the performance degradation. This indicates
271"
MAIN RESULTS,0.32653061224489793,"that the mixture of experts architecture, which divides the MLP experts by tokens, helps to alleviate
272"
MAIN RESULTS,0.3274582560296846,"the impact of the activation spikes. Meanwhile, addressing the activation spikes is not a sufficient
273"
MAIN RESULTS,0.32838589981447125,"complement for the Gemma model compared to other models. We attribute this to the choice of
274"
MAIN RESULTS,0.3293135435992579,"activation function among GLU variants; specifically, Gemma uses GeGLU, while other models
275"
MAIN RESULTS,0.3302411873840445,"employ SwiGLU.
276"
COMBINING OUTLIER ALLEVIATION METHODS,0.33116883116883117,"5.3
Combining Outlier Alleviation Methods
277"
COMBINING OUTLIER ALLEVIATION METHODS,0.3320964749536178,"While our method focuses on the activation spikes, the inherent outlier values in the input activations
278"
COMBINING OUTLIER ALLEVIATION METHODS,0.33302411873840443,"remain. Here, we combine the prior outlier alleviation methods, such as SmoothQuant (SQ) [51]
279"
COMBINING OUTLIER ALLEVIATION METHODS,0.3339517625231911,"and OutlierSuppressionPlus (OSP) [50], to further improve the quantization error. In practice, our
280"
COMBINING OUTLIER ALLEVIATION METHODS,0.33487940630797774,"methods are utilized during the scale calibration phase of alleviation methods to mitigate the impact
281"
COMBINING OUTLIER ALLEVIATION METHODS,0.3358070500927644,"of activation spikes on scale migration between activations and weights. Table 4 demonstrates the
282"
COMBINING OUTLIER ALLEVIATION METHODS,0.336734693877551,"evaluation results of applying the outlier alleviation methods solely and combining them with our
283"
COMBINING OUTLIER ALLEVIATION METHODS,0.33766233766233766,"methods. We find that there are cases where the alleviation method fails to recover the performance
284"
COMBINING OUTLIER ALLEVIATION METHODS,0.3385899814471243,"when quantizing the activations with per-tensor scheme.3 This indicates that alleviating the outlier
285"
COMBINING OUTLIER ALLEVIATION METHODS,0.3395176252319109,"scales, including the activation spikes, is challenging. With the QFeM, the activation spikes are
286"
COMBINING OUTLIER ALLEVIATION METHODS,0.3404452690166976,"excluded, and the accurate alleviation is enabled. In addition, the QFeP also benefits from the SQ
287"
COMBINING OUTLIER ALLEVIATION METHODS,0.34137291280148424,"method, as seen in the case of LLaMA-2 70B. Exceptionally, the OSP successfully addresses the
288"
COMBINING OUTLIER ALLEVIATION METHODS,0.3423005565862709,"activation spikes in the 13B and 70B cases.
289"
ABLATION STUDY,0.3432282003710575,"5.4
Ablation Study
290 65 70"
ABLATION STUDY,0.34415584415584416,Accuracy(%)
ABLATION STUDY,0.3450834879406308,LLaMA-2-7B 50 60 70
ABLATION STUDY,0.3460111317254174,Mistral-7B 65 70
ABLATION STUDY,0.3469387755102041,SOLAR-10.7B
ABLATION STUDY,0.34786641929499074,Random BOS
ABLATION STUDY,0.34879406307977734,"QFeP (w/o context)
QFeP (w/ context)"
ABLATION STUDY,0.349721706864564,"Figure 6: Prefix ablation. Y-axis represents
averaged accuracy of four zero-shot tasks."
ABLATION STUDY,0.35064935064935066,"For the QFeP, we designed a length-three prefix for
291"
ABLATION STUDY,0.3515769944341373,"the KV cache, including the BOS token, context to-
292"
ABLATION STUDY,0.3525046382189239,"ken, and extra token for activation spike. Because the
293"
ABLATION STUDY,0.3534322820037106,"KV cache consumes the capacity of the pretrained se-
294"
ABLATION STUDY,0.35435992578849723,"quence position, it raises a question about the length
295"
ABLATION STUDY,0.35528756957328383,"of the prefix. Therefore, we conduct ablation study
296"
ABLATION STUDY,0.3562152133580705,"for different prefixes for the KV cache. For the pre-
297"
ABLATION STUDY,0.35714285714285715,"fixes, we prepare random, BOS only, and both QFeP without and with the context token. We illustrate
298"
ABLATION STUDY,0.3580705009276438,"the results of ablation study in Figure 6. In all cases, the random prefix showcases the lowest perfor-
299"
ABLATION STUDY,0.3589981447124304,"mance. While the KV cache with the BOS token demonstrates inconsistent performance, our QFeP
300"
ABLATION STUDY,0.35992578849721707,"3In their papers, the activations of LLaMA models are quantized using only a per-token scheme."
ABLATION STUDY,0.3608534322820037,"270
280
290
300
310
320
Latency (ms) 30 40 50 60 70"
ABLATION STUDY,0.36178107606679033,Accuracy (%)
ABLATION STUDY,0.362708719851577,"LLaMA-2-13B
(GPU=RTX4090, token length=2000)"
ABLATION STUDY,0.36363636363636365,Qauntization Scheme
ABLATION STUDY,0.3645640074211503,"FP16
AQ1
AQ2
AQ2 + QFeP
AQ2 + QFeM
AQ3
AQ3 + QFeP
AQ3 + QFeM"
ABLATION STUDY,0.3654916512059369,"1350
1400
1450
1500
Latency (ms) 50 55 60 65 70 75"
ABLATION STUDY,0.36641929499072357,"LLaMA-2-70B
(GPU=A100, token length=2000)"
ABLATION STUDY,0.3673469387755102,Qauntization Scheme
ABLATION STUDY,0.3682745825602968,"FP16
AQ1
AQ2
AQ2 + QFeP
AQ2 + QFeM
AQ3
AQ3 + QFeP
AQ3 + QFeM"
ABLATION STUDY,0.3692022263450835,"Figure 7: Accuracy-latency comparison of different activation quan-
tization schemes: dynamic per-token (AQ1), dynamic per-tensor
(AQ2), and static per-tensor (AQ3)."
ABLATION STUDY,0.37012987012987014,Table 5: Memory footprint.
ABLATION STUDY,0.37105751391465674,"Method
SeqLen"
K,0.3719851576994434,"1K
2K"
K,0.37291280148423006,LLaMA-2-7B
K,0.3738404452690167,"AQ1
8185MiB
9516MiB
AQ2
8148MiB
9474MiB
+QFeP
8149MiB
9478MiB
+QFeM
8148MiB
9474MiB"
K,0.3747680890538033,LLaMA-2-70B
K,0.37569573283859,"AQ1
67756MiB
69037MiB
AQ2
67648MiB
68820MiB
+QFeP
67651MiB
68822MiB
+QFeM
67838MiB
68819MiB"
K,0.37662337662337664,"consistently shows significant improvement. Importantly, the results imply that the sufficient prefix
301"
K,0.37755102040816324,"for the models exhibits differences. However, we emphasize that our KV design for QFeP shows
302"
K,0.3784786641929499,"improvements by large margins across all models.
303"
COMPUTATIONAL COST ANALYSIS,0.37940630797773656,"5.5
Computational Cost Analysis
304"
COMPUTATIONAL COST ANALYSIS,0.3803339517625232,"The proposed methods require additional resources to evict the activation spikes. Therefore, we ana-
305"
COMPUTATIONAL COST ANALYSIS,0.3812615955473098,"lyze the computational costs of the methods and compare them in various schemes. For comparison,
306"
COMPUTATIONAL COST ANALYSIS,0.3821892393320965,"we evaluate different activation quantization schemes: dynamic per-token, dynamic per-tensor, and
307"
COMPUTATIONAL COST ANALYSIS,0.38311688311688313,"static per-tensor, denoted as AQ1, AQ2, and AQ3, respectively. This distinction establishes strong
308"
COMPUTATIONAL COST ANALYSIS,0.38404452690166974,"baselines and demonstrates the potential of the methods. To calibrate the static scales, we estimate
309"
COMPUTATIONAL COST ANALYSIS,0.3849721706864564,"the absolute maximum value using the calibration dataset, which is used in Section 3.1.
310"
COMPUTATIONAL COST ANALYSIS,0.38589981447124305,"Inference Latency.
For each setting, we present the accuracy of the zero-shot tasks and inference
311"
COMPUTATIONAL COST ANALYSIS,0.3868274582560297,"latency of the fixed token sequence, as shown in Figure 7. While the fine-grained scheme (AQ1) shows
312"
COMPUTATIONAL COST ANALYSIS,0.3877551020408163,"a negligible accuracy drop, the counterparts (AQ2, AQ3) degrade with the quantization bottleneck.
313"
COMPUTATIONAL COST ANALYSIS,0.38868274582560297,"However, by applying our methods, the coarse-grained schemes achieve a competitive performance
314"
COMPUTATIONAL COST ANALYSIS,0.38961038961038963,"gain. For example, the combination of AQ2 and QFeM demonstrates the performance close to
315"
COMPUTATIONAL COST ANALYSIS,0.39053803339517623,"the AQ1 but with faster latency. The results signify that addressing the quantization bottleneck
316"
COMPUTATIONAL COST ANALYSIS,0.3914656771799629,"is important to accelerate the inference latency with coarser granularity. Specifically, the naive
317"
COMPUTATIONAL COST ANALYSIS,0.39239332096474955,"static quantization (AQ3), the fastest scheme, exhibits a significant decline. We hope that our work
318"
COMPUTATIONAL COST ANALYSIS,0.39332096474953615,"contributes to the future works, which address the remaining challenges in static quantization.
319"
COMPUTATIONAL COST ANALYSIS,0.3942486085343228,"Memory Footprint.
In Table 5, we record the maximum memory footprint of our methods. For
320"
COMPUTATIONAL COST ANALYSIS,0.39517625231910947,"QFeP, the additional memory is consistently required for the preserved KV cache. However, this
321"
COMPUTATIONAL COST ANALYSIS,0.3961038961038961,"memory overhead is much smaller than that used in the fine-grained quantization (AQ1), as QFeM
322"
COMPUTATIONAL COST ANALYSIS,0.3970315398886827,"utilizes only three tokens for the cache. Contrary to QFeP, QFeM shows inconsistent memory
323"
COMPUTATIONAL COST ANALYSIS,0.3979591836734694,"utilization. For example, the 7B model with QFeM exhibits memory usage similar to AQ2, while the
324"
COMPUTATIONAL COST ANALYSIS,0.39888682745825604,"70B model with QFeM incur additional consumption for a sequence length of 1K. This is attributed to
325"
COMPUTATIONAL COST ANALYSIS,0.39981447124304265,"the use of W8A16 for the unquantization modules in QFeM. To tailor the memory usage or inference
326"
COMPUTATIONAL COST ANALYSIS,0.4007421150278293,"speed, an alternative strategy can be utilized for QFeM, such as applying fine-grained activation
327"
COMPUTATIONAL COST ANALYSIS,0.40166975881261596,"quantization to the unquantization modules instead of using W8A16.
328"
CONCLUSION,0.4025974025974026,"6
Conclusion
329"
CONCLUSION,0.4035250463821892,"We explore the quantization challenge of GLU activations for modern LLMs. We find that the GLU
330"
CONCLUSION,0.4044526901669759,"variants generates excessive activation scales, which cause significant quantization bottlenecks at
331"
CONCLUSION,0.40538033395176254,"the specific layers. Based on the systematic generation pattern of the activation spikes, we propose
332"
CONCLUSION,0.40630797773654914,"methods that address the spikes in a layer-wise (QFeM) and token-wise manner (QFeP). In the
333"
CONCLUSION,0.4072356215213358,"experiments, we confirm that the proposed methods effectively resolve the quantization bottlenecks
334"
CONCLUSION,0.40816326530612246,"and result in a large performance gain. We expect that our work sheds light on the potential challenges
335"
CONCLUSION,0.4090909090909091,"in future studies regarding quantization and facilitates the development of efficient LLM systems.
336"
REFERENCES,0.4100185528756957,"References
337"
REFERENCES,0.4109461966604824,"[1] Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen Gou, Phil
338"
REFERENCES,0.41187384044526903,"Blunsom, Ahmet Üstün, and Sara Hooker. Intriguing properties of quantization at scale.
339"
REFERENCES,0.41280148423005564,"Advances in Neural Information Processing Systems, 36:34278–34294, 2023.
340"
REFERENCES,0.4137291280148423,"[2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and
341"
REFERENCES,0.41465677179962895,"Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head
342"
REFERENCES,0.4155844155844156,"checkpoints. arXiv preprint arXiv:2305.13245, 2023.
343"
REFERENCES,0.4165120593692022,"[3] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxan-
344"
REFERENCES,0.4174397031539889,"dra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin
345"
REFERENCES,0.41836734693877553,"Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867,
346"
REFERENCES,0.41929499072356213,"2023.
347"
REFERENCES,0.4202226345083488,"[4] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.
348"
REFERENCES,0.42115027829313545,"In International Conference on Learning Representations, 2018.
349"
REFERENCES,0.42207792207792205,"[5] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth
350"
REFERENCES,0.4230055658627087,"Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 b
351"
REFERENCES,0.42393320964749537,"technical report. arXiv preprint arXiv:2402.17834, 2024.
352"
REFERENCES,0.424860853432282,"[6] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,
353"
REFERENCES,0.42578849721706863,"Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward
354"
REFERENCES,0.4267161410018553,"Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In
355"
REFERENCES,0.42764378478664195,"International Conference on Machine Learning, pages 2397–2430. PMLR, 2023.
356"
REFERENCES,0.42857142857142855,"[7] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-
357"
REFERENCES,0.4294990723562152,"ical commonsense in natural language. In Proceedings of the AAAI conference on artificial
358"
REFERENCES,0.43042671614100186,"intelligence, volume 34, pages 7432–7439, 2020.
359"
REFERENCES,0.4313543599257885,"[8] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming
360"
REFERENCES,0.4322820037105751,"the challenges of efficient transformer quantization. In Marie-Francine Moens, Xuanjing
361"
REFERENCES,0.4332096474953618,"Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on
362"
REFERENCES,0.43413729128014844,"Empirical Methods in Natural Language Processing, pages 7947–7969, Online and Punta Cana,
363"
REFERENCES,0.43506493506493504,"Dominican Republic, November 2021. Association for Computational Linguistics.
364"
REFERENCES,0.4359925788497217,"[9] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
Quantizable transformers:
365"
REFERENCES,0.43692022263450836,"Removing outliers by helping attention heads do nothing. Advances in Neural Information
366"
REFERENCES,0.437847866419295,"Processing Systems, 36, 2024.
367"
REFERENCES,0.4387755102040816,"[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
368"
REFERENCES,0.4397031539888683,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
369"
REFERENCES,0.44063079777365494,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
370"
REFERENCES,0.44155844155844154,"[11] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantiza-
371"
REFERENCES,0.4424860853432282,"tion of large language models with guarantees. Advances in Neural Information Processing
372"
REFERENCES,0.44341372912801486,"Systems, 36, 2024.
373"
REFERENCES,0.44434137291280146,"[12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix
374"
REFERENCES,0.4452690166975881,"multiplication for transformers at scale. Advances in Neural Information Processing Systems,
375"
REFERENCES,0.4461966604823748,"35:30318–30332, 2022.
376"
REFERENCES,0.44712430426716143,"[13] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh
377"
REFERENCES,0.44805194805194803,"Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized
378"
REFERENCES,0.4489795918367347,"representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078,
379"
REFERENCES,0.44990723562152135,"2023.
380"
REFERENCES,0.45083487940630795,"[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
381"
REFERENCES,0.4517625231910946,"deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
382"
REFERENCES,0.45269016697588127,"2018.
383"
REFERENCES,0.4536178107606679,"[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training
384"
REFERENCES,0.45454545454545453,"quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.
385"
REFERENCES,0.4554730983302412,"[16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles
386"
REFERENCES,0.45640074211502785,"Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas
387"
REFERENCES,0.45732838589981445,"Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,
388"
REFERENCES,0.4582560296846011,"Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework
389"
REFERENCES,0.45918367346938777,"for few-shot language model evaluation, 12 2023.
390"
REFERENCES,0.4601113172541744,"[17] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.
391"
REFERENCES,0.461038961038961,"A survey of quantization methods for efficient neural network inference. In Low-Power Com-
392"
REFERENCES,0.4619666048237477,"puter Vision, pages 291–326. Chapman and Hall/CRC, 2022.
393"
REFERENCES,0.46289424860853434,"[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
394"
REFERENCES,0.46382189239332094,"networks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The
395"
REFERENCES,0.4647495361781076,"Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 630–645. Springer, 2016.
396"
REFERENCES,0.46567717996289426,"[19] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
397"
REFERENCES,0.46660482374768086,"Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
398"
REFERENCES,0.4675324675324675,"efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer
399"
REFERENCES,0.4684601113172542,"vision and pattern recognition, pages 2704–2713, 2018.
400"
REFERENCES,0.46938775510204084,"[20] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
401"
REFERENCES,0.47031539888682744,"Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
402"
REFERENCES,0.4712430426716141,"Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
403"
REFERENCES,0.47217068645640076,"[21] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
404"
REFERENCES,0.47309833024118736,"Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,
405"
REFERENCES,0.474025974025974,"et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.
406"
REFERENCES,0.4749536178107607,"[22] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeon-
407"
REFERENCES,0.47588126159554733,"woo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language
408"
REFERENCES,0.47680890538033394,"models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166, 2023.
409"
REFERENCES,0.4777365491651206,"[23] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W
410"
REFERENCES,0.47866419294990725,"Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint
411"
REFERENCES,0.47959183673469385,"arXiv:2306.07629, 2023.
412"
REFERENCES,0.4805194805194805,"[24] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. BERT busters: Out-
413"
REFERENCES,0.48144712430426717,"lier dimensions that disrupt transformers. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto
414"
REFERENCES,0.48237476808905383,"Navigli, editors, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,
415"
REFERENCES,0.48330241187384043,"pages 3392–3405, Online, August 2021. Association for Computational Linguistics.
416"
REFERENCES,0.4842300556586271,"[25] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark
417"
REFERENCES,0.48515769944341375,"secrets of BERT. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings
418"
REFERENCES,0.48608534322820035,"of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
419"
REFERENCES,0.487012987012987,"International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
420"
REFERENCES,0.48794063079777367,"4365–4374, Hong Kong, China, November 2019. Association for Computational Linguistics.
421"
REFERENCES,0.48886827458256027,"[26] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.
Awq:
422"
REFERENCES,0.4897959183673469,"Activation-aware weight quantization for llm compression and acceleration. arXiv preprint
423"
REFERENCES,0.4907235621521336,"arXiv:2306.00978, 2023.
424"
REFERENCES,0.49165120593692024,"[27] Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao. Positional artefacts propagate through masked
425"
REFERENCES,0.49257884972170685,"language model embeddings. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, edi-
426"
REFERENCES,0.4935064935064935,"tors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics
427"
REFERENCES,0.49443413729128016,"and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long
428"
REFERENCES,0.49536178107606677,"Papers), pages 5312–5327, Online, August 2021. Association for Computational Linguistics.
429"
REFERENCES,0.4962894248608534,"[28] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
430"
REFERENCES,0.4972170686456401,"models. arXiv preprint arXiv:1609.07843, 2016.
431"
REFERENCES,0.49814471243042674,"[29] Javaheripi Mojan and Bubeck Sébastien. Phi-2: The surprising power of small language models,
432"
REFERENCES,0.49907235621521334,"2023.
433"
REFERENCES,0.5,"[30] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen,
434"
REFERENCES,0.5009276437847866,"and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint
435"
REFERENCES,0.5018552875695733,"arXiv:2106.08295, 2021.
436"
REFERENCES,0.5027829313543599,"[31] Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Fevry, Michael Matena,
437"
REFERENCES,0.5037105751391465,"Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding,
438"
REFERENCES,0.5046382189239332,"Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer modifications transfer across
439"
REFERENCES,0.5055658627087198,"implementations and applications? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia,
440"
REFERENCES,0.5064935064935064,"and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in
441"
REFERENCES,0.5074211502782932,"Natural Language Processing, pages 5758–5773, Online and Punta Cana, Dominican Republic,
442"
REFERENCES,0.5083487940630798,"November 2021. Association for Computational Linguistics.
443"
REFERENCES,0.5092764378478665,"[32] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
444"
REFERENCES,0.5102040816326531,"and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint
445"
REFERENCES,0.5111317254174397,"arXiv:1904.01038, 2019.
446"
REFERENCES,0.5120593692022264,"[33] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,
447"
REFERENCES,0.512987012987013,"Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA
448"
REFERENCES,0.5139146567717996,"dataset: Word prediction requiring a broad discourse context. In Katrin Erk and Noah A. Smith,
449"
REFERENCES,0.5148423005565863,"editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguis-
450"
REFERENCES,0.5157699443413729,"tics (Volume 1: Long Papers), pages 1525–1534, Berlin, Germany, August 2016. Association
451"
REFERENCES,0.5166975881261595,"for Computational Linguistics.
452"
REFERENCES,0.5176252319109462,"[34] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan
453"
REFERENCES,0.5185528756957328,"Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.
454"
REFERENCES,0.5194805194805194,"Proceedings of Machine Learning and Systems, 5, 2023.
455"
REFERENCES,0.5204081632653061,"[35] Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice Dell’Orletta. Outlier dimensions
456"
REFERENCES,0.5213358070500927,"that disrupt transformers are driven by frequency. In Yoav Goldberg, Zornitsa Kozareva, and Yue
457"
REFERENCES,0.5222634508348795,"Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages
458"
REFERENCES,0.5231910946196661,"1286–1304, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational
459"
REFERENCES,0.5241187384044527,"Linguistics.
460"
REFERENCES,0.5250463821892394,"[36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
461"
REFERENCES,0.525974025974026,"Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
462"
REFERENCES,0.5269016697588126,"[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
463"
REFERENCES,0.5278293135435993,"Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified
464"
REFERENCES,0.5287569573283859,"text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.
465"
REFERENCES,0.5296846011131725,"[38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
466"
REFERENCES,0.5306122448979592,"adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.
467"
REFERENCES,0.5315398886827458,"[39] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng
468"
REFERENCES,0.5324675324675324,"Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantiza-
469"
REFERENCES,0.5333951762523191,"tion for large language models. arXiv preprint arXiv:2308.13137, 2023.
470"
REFERENCES,0.5343228200371057,"[40] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
471"
REFERENCES,0.5352504638218923,"[41] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
472"
REFERENCES,0.536178107606679,"Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
473"
REFERENCES,0.5371057513914657,"[42] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language
474"
REFERENCES,0.5380333951762524,"models. arXiv preprint arXiv:2402.17762, 2024.
475"
REFERENCES,0.538961038961039,"[43] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya
476"
REFERENCES,0.5398886827458256,"Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open
477"
REFERENCES,0.5408163265306123,"models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.
478"
REFERENCES,0.5417439703153989,"[44] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially
479"
REFERENCES,0.5426716141001855,"usable llms, 2023. Accessed: 2023-05-05.
480"
REFERENCES,0.5435992578849722,"[45] William Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions in trans-
481"
REFERENCES,0.5445269016697588,"former language models obscure representational quality. In Marie-Francine Moens, Xuanjing
482"
REFERENCES,0.5454545454545454,"Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on
483"
REFERENCES,0.5463821892393321,"Empirical Methods in Natural Language Processing, pages 4527–4546, Online and Punta Cana,
484"
REFERENCES,0.5473098330241187,"Dominican Republic, November 2021. Association for Computational Linguistics.
485"
REFERENCES,0.5482374768089053,"[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
486"
REFERENCES,0.549165120593692,"thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
487"
REFERENCES,0.5500927643784786,"and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
488"
REFERENCES,0.5510204081632653,"[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
489"
REFERENCES,0.551948051948052,"Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
490"
REFERENCES,0.5528756957328386,"foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
491"
REFERENCES,0.5538033395176253,"[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
492"
REFERENCES,0.5547309833024119,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
493"
REFERENCES,0.5556586270871985,"processing systems, 30, 2017.
494"
REFERENCES,0.5565862708719852,"[49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
495"
REFERENCES,0.5575139146567718,"Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
496"
REFERENCES,0.5584415584415584,"language models. arXiv preprint arXiv:2206.07682, 2022.
497"
REFERENCES,0.5593692022263451,"[50] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo,
498"
REFERENCES,0.5602968460111317,"and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models
499"
REFERENCES,0.5612244897959183,"by equivalent and effective shifting and scaling. In Houda Bouamor, Juan Pino, and Kalika
500"
REFERENCES,0.562152133580705,"Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language
501"
REFERENCES,0.5630797773654916,"Processing, pages 1648–1665, Singapore, December 2023. Association for Computational
502"
REFERENCES,0.5640074211502782,"Linguistics.
503"
REFERENCES,0.564935064935065,"[51] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
504"
REFERENCES,0.5658627087198516,"SmoothQuant: Accurate and efficient post-training quantization for large language models. In
505"
REFERENCES,0.5667903525046383,"Proceedings of the 40th International Conference on Machine Learning, 2023.
506"
REFERENCES,0.5677179962894249,"[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming
507"
REFERENCES,0.5686456400742115,"language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.
508"
REFERENCES,0.5695732838589982,"[53] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
509"
REFERENCES,0.5705009276437848,"Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.
510"
REFERENCES,0.5714285714285714,"In International Conference on Machine Learning, pages 10524–10533. PMLR, 2020.
511"
REFERENCES,0.5723562152133581,"[54] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive study
512"
REFERENCES,0.5732838589981447,"on post-training quantization for large language models. arXiv preprint arXiv:2303.08302,
513"
REFERENCES,0.5742115027829313,"2023.
514"
REFERENCES,0.575139146567718,"[55] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong
515"
REFERENCES,0.5760667903525046,"He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.
516"
REFERENCES,0.5769944341372912,"Advances in Neural Information Processing Systems, 35:27168–27183, 2022.
517"
REFERENCES,0.577922077922078,"[56] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a
518"
REFERENCES,0.5788497217068646,"machine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, edi-
519"
REFERENCES,0.5797773654916512,"tors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
520"
REFERENCES,0.5807050092764379,"pages 4791–4800, Florence, Italy, July 2019. Association for Computational Linguistics.
521"
REFERENCES,0.5816326530612245,"[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
522"
REFERENCES,0.5825602968460112,"Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
523"
REFERENCES,0.5834879406307978,"transformer language models. arXiv preprint arXiv:2205.01068, 2022.
524"
REFERENCES,0.5844155844155844,"[58] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
525"
REFERENCES,0.5853432282003711,"Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv
526"
REFERENCES,0.5862708719851577,"preprint arXiv:2303.18223, 2023.
527"
REFERENCES,0.5871985157699443,"A
Additional Calibration Results
528"
REFERENCES,0.588126159554731,"In this section, we provide details of LLMs when performing calibration, which is the step during
529"
REFERENCES,0.5890538033395176,"quantization where the FP16 ranges are computed (Appendix A.1), and additional calibration results
530"
REFERENCES,0.5899814471243042,"(Appendix A.2, A.3).
531"
REFERENCES,0.5909090909090909,"A.1
Detailed Specification of LLMs
532"
REFERENCES,0.5918367346938775,"In Section 3.1, we have performed the calibration method on various LLMs. We observe the
533"
REFERENCES,0.5927643784786641,"calibration results by categorizing based on the presence of GLU in the LLMs. Table 6 shows the
534"
REFERENCES,0.5936920222634509,"detailed structures of the LLMs. We refer notations for feed-forward implementiation from [40]. In
535"
REFERENCES,0.5946196660482375,"the case of GLU-implemented LLMs, which is LLaMA-2, LLaMA-3, Mistral, Mixtral, SOLAR,
536"
REFERENCES,0.5955473098330241,"StableLM-2, and Gemma, most models have SwiGLU for FFN activation, while only Gemma has
537"
REFERENCES,0.5964749536178108,"GeGLU. On the other hand, in non GLU-implemented LLMs, most of them utilize GeLU for FFN
538"
REFERENCES,0.5974025974025974,"activation, with the exception of OPT, which uses ReLU.
539"
REFERENCES,0.5983302411873841,"Table 6: Architecture specification of LLMs. We categorize them into two groups depending on
whether GLU is implemented in the FFN. All LLMs in the table use Pre-LN for the LayerNorm
position."
REFERENCES,0.5992578849721707,"Model
Size
FFN Activation
Normalization
PE
Vocabulary Size"
REFERENCES,0.6001855287569573,"GLU-implemented LLMs:
LLaMA-2 [47]
7B, 13B, 70B
SwiGLU
RMSNorm
RoPE
32000
LLaMA-3
8B, 70B
SwiGLU
RMSNorm
RoPE
128256
Mistral [20]
7B
SwiGLU
RMSNorm
RoPE
32000
Mixtral [21]
8x7B
SwiGLU
RMSNorm
RoPE
32000
SOLAR [22]
10.7B
SwiGLU
RMSNorm
RoPE
32000
StableLM-2 [5]
12B
SwiGLU
LayerNorm
RoPE
100352
Gemma [43]
7B
GeGLU
RMSNorm
RoPE
256000"
REFERENCES,0.601113172541744,"Non GLU-implemented LLMs:
OPT [57]
6.7B, 13B, 30B, 66B
ReLU
LayerNorm
Learned
50272
MPT [44]
7B, 30B
GeLU
LayerNorm
ALiBi
50432
Pythia [6]
6.9B, 12B
GeLU
LayerNorm
RoPE
50432, 50688
Falcon [3]
7B, 40B
GeLU
LayerNorm
RoPE
65024
Phi-2 [29]
2.7B
GeLU
LayerNorm
RoPE
51200"
REFERENCES,0.6020408163265306,"A.2
Other Calibration Results on GLU-implementation
540"
REFERENCES,0.6029684601113172,"Figure 8, 9 show the calibration result examples for various GLU-implemented LLMs that are not
541"
REFERENCES,0.6038961038961039,"shown in the models in Figure 1a. In most GLU-implemented LLMs, we observe that the input
542"
REFERENCES,0.6048237476808905,"activations have large values near the first and last layers. Unlike the typical GLU-implemented LLM
543"
REFERENCES,0.6057513914656771,"architecture, Mixtral is composed of 8 feed-forward blocks in the single FFN, containing multiple
544"
REFERENCES,0.6066790352504638,"gate linear units [21]. According to this structure, we can observe that one of the gates spikes in value
545"
REFERENCES,0.6076066790352505,"in Figure 8.
546"
REFERENCES,0.608534322820037,"1
8
16
24
32
Layers 0"
K,0.6094619666048238,3k
K,0.6103896103896104,6k
K,0.6113172541743971,Activation Magnitude
K,0.6122448979591837,Mixtral-8x7B
K,0.6131725417439703,"q_proj
o_proj
experts.0.w2
experts.0.w3
experts.1.w2
experts.1.w3
experts.2.w2
experts.2.w3
experts.3.w2
experts.3.w3"
K,0.614100185528757,"experts.4.w2
experts.4.w3
experts.5.w2
experts.5.w3
experts.6.w2
experts.6.w3
experts.7.w2
experts.7.w3
hidden_states"
K,0.6150278293135436,Figure 8: Calibration results on GLU-implemented LLMs (Mixtral-8x7B).
K,0.6159554730983302,"1
10
20
30
40
Layers 0 500"
K,0.6168831168831169,1k
K,0.6178107606679035,Activation Magnitude
K,0.6187384044526901,LLaMA-2-13B
K,0.6196660482374768,"1
20
40
60
80
Layers 0"
K,0.6205936920222634,5k
K,0.62152133580705,10k
K,0.6224489795918368,15k
K,0.6233766233766234,LLaMA-2-70B
K,0.62430426716141,"1
8
16
24
32
Layers 0 300 600"
K,0.6252319109461967,LLaMA-3-8B
K,0.6261595547309833,"1
20
40
60
80
Layers 0 300 600"
K,0.62708719851577,LLaMA-3-70B
K,0.6280148423005566,"1
12
24
36
48
Layers 0"
K,0.6289424860853432,1k
K,0.6298701298701299,2k
K,0.6307977736549165,Activation Magnitude
K,0.6317254174397031,SOLAR-10.7B
K,0.6326530612244898,"1
10
20
30
40
Layers 0"
K,0.6335807050092764,1k
K,0.634508348794063,2k
K,0.6354359925788498,StableLM-2-12B
K,0.6363636363636364,"1
7
14
21
28
Layers 0"
K,0.637291280148423,1k
K,0.6382189239332097,2k
K,0.6391465677179963,Gemma-7B
K,0.640074211502783,"self-attn.q/k/v
self-attn.out
feed-forward.up/gate
feed-forward.down
hidden states"
K,0.6410018552875696,Figure 9: Calibration results on GLU-implemented LLMs.
K,0.6419294990723562,Figure 10: Calibration results on Non GLU-implemented LLMs.
K,0.6428571428571429,"A.3
Other Calibration Results on Non GLU-implementation
547"
K,0.6437847866419295,"Figure 10 shows the calibration result examples for various non GLU-implemented LLMs that were
548"
K,0.6447124304267161,"not shown in the models in Figure 1b. There are no activation spikes on non GLU-implemented
549"
K,0.6456400742115028,"LLMs.
550"
K,0.6465677179962894,"B
BMM Quantization
551"
K,0.647495361781076,"To achieve faster inference latency, BMM operations in the self-attention also can be computed as
552"
K,0.6484230055658627,"INT8 operation [51]. This requires a quantization on the query, key, and value states including the
553"
K,0.6493506493506493,"cached context. Because activation spikes produce a large magnitude of latent values, it is important
554"
K,0.650278293135436,"to confirm the extent of quantization errors from KV quantization. This confirmation is necessary to
555"
K,0.6512059369202227,"gain advantages from BMM quantization. In Table 7, we examine the impact of BMM quantization on
556"
K,0.6521335807050093,"the W8A8 and QFeM. Regardless of the BMM quantization, the QFeM method consistently improves
557"
K,0.6530612244897959,"the quantization bottleneck. For example, the 13B and 70B models maintain their performance,
558"
K,0.6539888682745826,"while the 7B model shows a slight decrease. However, this decrease appears to be due to inherent
559"
K,0.6549165120593692,"quantization errors rather than a quantization bottleneck from activation spikes. As a result, we
560"
K,0.6558441558441559,"confirm that our QFeM method effectively improves the overall performance even in the BMM
561"
K,0.6567717996289425,"quantization scenario.
562"
K,0.6576994434137291,Table 7: BMM quantization results.
K,0.6586270871985158,"Model
Method
BMM Quantization
No
Yes"
B,0.6595547309833024,"7B
W8A8
62.08%
61.66%
+QFeP
68.69%
68.30%"
B,0.660482374768089,"13B
W8A8
55.29%
55.43%
+QFeP
69.91%
69.77%"
B,0.6614100185528757,"70B
W8A8
66.87%
66.75%
+QFeP
72.62%
72.69%"
B,0.6623376623376623,"C
Supplementary Experiment Results
563"
B,0.6632653061224489,"C.1
Additional Results for Combining Outlier Alleviation Methods
564"
B,0.6641929499072357,"In Table 8, we provide additional results for Section 5.3 with coarse-grained quantization (i.e.,
565"
B,0.6651205936920223,"per-tensor quantization) scheme for weight quantization. Compared to the results obtained with per-
566"
B,0.6660482374768089,"channel weight quantization in Table 4, these results elucidate the negative impact of activation spikes
567"
B,0.6669758812615956,"on the performance of outlier alleviation methods. Furthermore, this suggests that the performance of
568"
B,0.6679035250463822,"OSP method resort to the weight quantization scheme. Nevertheless, the proposed methods, QFeM
569"
B,0.6688311688311688,"and QFeP, consistently improve the effectiveness of outlier alleviation methods by mitigating the
570"
B,0.6697588126159555,"impact of activation spikes.
571"
B,0.6706864564007421,"Table 8: Evaluation of outlier alleviation methods with QFeM and QFeP. We report perplexity on
WikiText-2 and averaged accuracy of four zero-shot tasks. Compared to Table 4, per-tensor weight
quantization and dynamic per-tensor activation quantization are used."
B,0.6716141001855288,"Method
LLaMA-2-7B
LLaMA-2-13B
LLaMA-2-70B"
B,0.6725417439703154,"ppl(↓)
acc(↑)
ppl(↓)
acc(↑)
ppl(↓)
acc(↑)"
B,0.673469387755102,"SQ [51]
24.661
56.87%
120.966
53.06%
8.435
67.08%
+QFeM
6.016
67.74%
5.464
70.04%
4.015
74.18%
+QFeP
6.122
67.22%
10.473
68.17%
5.998
72.54%"
B,0.6743970315398887,"OSP [50]
9.131
63.61%
8.997
64.03%
6.492
71.13%
+QFeM
5.951
68.65%
5.284
70.67%
4.434
73.30%
+QFeP
5.821
68.25%
5.868
67.96%
4.976
73.57%"
B,0.6753246753246753,"D
Miscellaneous
572"
B,0.6762523191094619,"D.1
Transformer Architecture.
573"
B,0.6771799628942486,"In Figure 11, we illustrate the Pre-LN transformer architecture and each sub-modules. We highlight
574"
B,0.6781076066790352,"with the same color the linear modules that accept identical input activations. Note that the hidden
575"
B,0.6790352504638218,"states are normalized before forwarding into the query and up linear modules.
576 BMM"
B,0.6799628942486086,"Softmax BMM 𝑜𝑢𝑡 𝑞
𝑘
𝑣"
B,0.6808905380333952,Hadamard
B,0.6818181818181818,Product 𝑑𝑜𝑤𝑛
B,0.6827458256029685,"𝑔𝑎𝑡𝑒
𝑢𝑝 𝜎 𝑑𝑜𝑤𝑛 𝑢𝑝 𝜎"
B,0.6836734693877551,"(a)
Transformer Block"
B,0.6846011131725418,(Pre-LN)
B,0.6855287569573284,"(b)
Self-Attention"
B,0.686456400742115,"(c)
Feed-Forward (GLU)"
B,0.6873840445269017,LayerNorm
B,0.6883116883116883,LayerNorm
B,0.6892393320964749,Self-Attention
B,0.6901669758812616,Feed-Forward Add Add
B,0.6910946196660482,"(d)
Feed-Forward"
B,0.6920222634508348,(Non-GLU)
B,0.6929499072356216,"Figure 11: An illustration of Pre-LN transformer block and its sub-modules. Two feed-forward
implementation, GLU and Non-GLU, are visualized in (c) and (d) respectively. In feed-forward
network, σ denotes non-linear activation function, such as GeLU. We highlight the linear modules
where input activations are quantized."
B,0.6938775510204082,"D.2
Additional Results for Token-level Scale Analysis
577"
B,0.6948051948051948,"We provide additional results for token-level scale analysis (Section 3.2). In Figure 12 and Figure 13,
578"
B,0.6957328385899815,"the token for the activation spikes behind the BOS token does not exhibit the excessive activation
579"
B,0.6966604823747681,"scale.
580"
B,0.6975881261595547,"BOS
\n
It
'
s
snow
\n
It
'
s
cold
\n 0 500"
K,0.6985157699443414,1k
K,0.699443413729128,1.5k
K,0.7003710575139147,Activation Magnitude
K,0.7012987012987013,LLaMA-2-7B Layer 2 feed-forward.down
K,0.7022263450834879,"Figure 12: Token-wise scales analysis for LLaMA-2-7B. The newline token behind the BOS token
does not exhibit the activation spikes."
K,0.7031539888682746,"BOS
'
It
'
s
snow
\n
It
'
s
cold
\n 0"
K,0.7040816326530612,2k
K,0.7050092764378478,4k
K,0.7059369202226345,6k
K,0.7068645640074211,Activation Magnitude
K,0.7077922077922078,LLaMA-2-7B Layer 2 feed-forward.down
K,0.7087198515769945,"Figure 13: Token-wise scales from the unrolled activation spike of LLaMA-2-70B. The newline
token behind the BOS token does not exhibit the activation spikes."
K,0.7096474953617811,"NeurIPS Paper Checklist
581"
CLAIMS,0.7105751391465677,"1. Claims
582"
CLAIMS,0.7115027829313544,"Question: Do the main claims made in the abstract and introduction accurately reflect the
583"
CLAIMS,0.712430426716141,"paper’s contributions and scope?
584"
CLAIMS,0.7133580705009277,"Answer: [Yes]
585"
CLAIMS,0.7142857142857143,"Justification: We clarify our research scope and contributions in abstract and introduction.
586"
CLAIMS,0.7152133580705009,"Guidelines:
587"
CLAIMS,0.7161410018552876,"• The answer NA means that the abstract and introduction do not include the claims
588"
CLAIMS,0.7170686456400742,"made in the paper.
589"
CLAIMS,0.7179962894248608,"• The abstract and/or introduction should clearly state the claims made, including the
590"
CLAIMS,0.7189239332096475,"contributions made in the paper and important assumptions and limitations. A No or
591"
CLAIMS,0.7198515769944341,"NA answer to this question will not be perceived well by the reviewers.
592"
CLAIMS,0.7207792207792207,"• The claims made should match theoretical and experimental results, and reflect how
593"
CLAIMS,0.7217068645640075,"much the results can be expected to generalize to other settings.
594"
CLAIMS,0.7226345083487941,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
595"
CLAIMS,0.7235621521335807,"are not attained by the paper.
596"
LIMITATIONS,0.7244897959183674,"2. Limitations
597"
LIMITATIONS,0.725417439703154,"Question: Does the paper discuss the limitations of the work performed by the authors?
598"
LIMITATIONS,0.7263450834879406,"Answer: [No]
599"
LIMITATIONS,0.7272727272727273,"Justification: The limitation of our work is that our methods are based on the observations
600"
LIMITATIONS,0.7282003710575139,"without theoretical validation. However, our extensive experimental results validate the
601"
LIMITATIONS,0.7291280148423006,"effectiveness of our methods.
602"
LIMITATIONS,0.7300556586270872,"Guidelines:
603"
LIMITATIONS,0.7309833024118738,"• The answer NA means that the paper has no limitation while the answer No means that
604"
LIMITATIONS,0.7319109461966605,"the paper has limitations, but those are not discussed in the paper.
605"
LIMITATIONS,0.7328385899814471,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
606"
LIMITATIONS,0.7337662337662337,"• The paper should point out any strong assumptions and how robust the results are to
607"
LIMITATIONS,0.7346938775510204,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
608"
LIMITATIONS,0.735621521335807,"model well-specification, asymptotic approximations only holding locally). The authors
609"
LIMITATIONS,0.7365491651205937,"should reflect on how these assumptions might be violated in practice and what the
610"
LIMITATIONS,0.7374768089053804,"implications would be.
611"
LIMITATIONS,0.738404452690167,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
612"
LIMITATIONS,0.7393320964749536,"only tested on a few datasets or with a few runs. In general, empirical results often
613"
LIMITATIONS,0.7402597402597403,"depend on implicit assumptions, which should be articulated.
614"
LIMITATIONS,0.7411873840445269,"• The authors should reflect on the factors that influence the performance of the approach.
615"
LIMITATIONS,0.7421150278293135,"For example, a facial recognition algorithm may perform poorly when image resolution
616"
LIMITATIONS,0.7430426716141002,"is low or images are taken in low lighting. Or a speech-to-text system might not be
617"
LIMITATIONS,0.7439703153988868,"used reliably to provide closed captions for online lectures because it fails to handle
618"
LIMITATIONS,0.7448979591836735,"technical jargon.
619"
LIMITATIONS,0.7458256029684601,"• The authors should discuss the computational efficiency of the proposed algorithms
620"
LIMITATIONS,0.7467532467532467,"and how they scale with dataset size.
621"
LIMITATIONS,0.7476808905380334,"• If applicable, the authors should discuss possible limitations of their approach to
622"
LIMITATIONS,0.74860853432282,"address problems of privacy and fairness.
623"
LIMITATIONS,0.7495361781076066,"• While the authors might fear that complete honesty about limitations might be used by
624"
LIMITATIONS,0.7504638218923934,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
625"
LIMITATIONS,0.75139146567718,"limitations that aren’t acknowledged in the paper. The authors should use their best
626"
LIMITATIONS,0.7523191094619666,"judgment and recognize that individual actions in favor of transparency play an impor-
627"
LIMITATIONS,0.7532467532467533,"tant role in developing norms that preserve the integrity of the community. Reviewers
628"
LIMITATIONS,0.7541743970315399,"will be specifically instructed to not penalize honesty concerning limitations.
629"
THEORY ASSUMPTIONS AND PROOFS,0.7551020408163265,"3. Theory Assumptions and Proofs
630"
THEORY ASSUMPTIONS AND PROOFS,0.7560296846011132,"Question: For each theoretical result, does the paper provide the full set of assumptions and
631"
THEORY ASSUMPTIONS AND PROOFS,0.7569573283858998,"a complete (and correct) proof?
632"
THEORY ASSUMPTIONS AND PROOFS,0.7578849721706865,"Answer: [NA]
633"
THEORY ASSUMPTIONS AND PROOFS,0.7588126159554731,"Justification: We propose empirical methods based on our observation, rather than theoretical
634"
THEORY ASSUMPTIONS AND PROOFS,0.7597402597402597,"analysis.
635"
THEORY ASSUMPTIONS AND PROOFS,0.7606679035250464,"Guidelines:
636"
THEORY ASSUMPTIONS AND PROOFS,0.761595547309833,"• The answer NA means that the paper does not include theoretical results.
637"
THEORY ASSUMPTIONS AND PROOFS,0.7625231910946196,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
638"
THEORY ASSUMPTIONS AND PROOFS,0.7634508348794063,"referenced.
639"
THEORY ASSUMPTIONS AND PROOFS,0.764378478664193,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
640"
THEORY ASSUMPTIONS AND PROOFS,0.7653061224489796,"• The proofs can either appear in the main paper or the supplemental material, but if
641"
THEORY ASSUMPTIONS AND PROOFS,0.7662337662337663,"they appear in the supplemental material, the authors are encouraged to provide a short
642"
THEORY ASSUMPTIONS AND PROOFS,0.7671614100185529,"proof sketch to provide intuition.
643"
THEORY ASSUMPTIONS AND PROOFS,0.7680890538033395,"• Inversely, any informal proof provided in the core of the paper should be complemented
644"
THEORY ASSUMPTIONS AND PROOFS,0.7690166975881262,"by formal proofs provided in appendix or supplemental material.
645"
THEORY ASSUMPTIONS AND PROOFS,0.7699443413729128,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7708719851576994,"4. Experimental Result Reproducibility
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7717996289424861,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7727272727272727,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7736549165120594,"of the paper (regardless of whether the code and data are provided or not)?
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.774582560296846,"Answer: [Yes]
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7755102040816326,"Justification: We precisely describe the process of the proposed methods in their respec-
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7764378478664193,"tive subsections. The models (LLMs) and datasets used in the experiments are publicly
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7773654916512059,"accessible.
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7782931354359925,"Guidelines:
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7792207792207793,"• The answer NA means that the paper does not include experiments.
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7801484230055659,"• If the paper includes experiments, a No answer to this question will not be perceived
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7810760667903525,"well by the reviewers: Making the paper reproducible is important, regardless of
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7820037105751392,"whether the code and data are provided or not.
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7829313543599258,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7838589981447124,"to make their results reproducible or verifiable.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7847866419294991,"• Depending on the contribution, reproducibility can be accomplished in various ways.
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7857142857142857,"For example, if the contribution is a novel architecture, describing the architecture fully
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7866419294990723,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.787569573283859,"be necessary to either make it possible for others to replicate the model with the same
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7884972170686456,"dataset, or provide access to the model. In general. releasing code and data is often
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7894248608534323,"one good way to accomplish this, but reproducibility can also be provided via detailed
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7903525046382189,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7912801484230055,"of a large language model), releasing of a model checkpoint, or other means that are
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7922077922077922,"appropriate to the research performed.
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7931354359925789,"• While NeurIPS does not require releasing code, the conference does require all submis-
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7940630797773655,"sions to provide some reasonable avenue for reproducibility, which may depend on the
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7949907235621522,"nature of the contribution. For example
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7959183673469388,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7968460111317254,"to reproduce that algorithm.
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7977736549165121,"(b) If the contribution is primarily a new model architecture, the paper should describe
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7987012987012987,"the architecture clearly and fully.
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7996289424860853,"(c) If the contribution is a new model (e.g., a large language model), then there should
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.800556586270872,"either be a way to access this model for reproducing the results or a way to reproduce
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8014842300556586,"the model (e.g., with an open-source dataset or instructions for how to construct
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8024118738404453,"the dataset).
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8033395176252319,"(d) We recognize that reproducibility may be tricky in some cases, in which case
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8042671614100185,"authors are welcome to describe the particular way they provide for reproducibility.
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8051948051948052,"In the case of closed-source models, it may be that access to the model is limited in
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8061224489795918,"some way (e.g., to registered users), but it should be possible for other researchers
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8070500927643784,"to have some path to reproducing or verifying the results.
686"
OPEN ACCESS TO DATA AND CODE,0.8079777365491652,"5. Open access to data and code
687"
OPEN ACCESS TO DATA AND CODE,0.8089053803339518,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
688"
OPEN ACCESS TO DATA AND CODE,0.8098330241187384,"tions to faithfully reproduce the main experimental results, as described in supplemental
689"
OPEN ACCESS TO DATA AND CODE,0.8107606679035251,"material?
690"
OPEN ACCESS TO DATA AND CODE,0.8116883116883117,"Answer: [Yes]
691"
OPEN ACCESS TO DATA AND CODE,0.8126159554730983,"Justification: We provide accessible URL in the abstract.
692"
OPEN ACCESS TO DATA AND CODE,0.813543599257885,"Guidelines:
693"
OPEN ACCESS TO DATA AND CODE,0.8144712430426716,"• The answer NA means that paper does not include experiments requiring code.
694"
OPEN ACCESS TO DATA AND CODE,0.8153988868274582,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
695"
OPEN ACCESS TO DATA AND CODE,0.8163265306122449,"public/guides/CodeSubmissionPolicy) for more details.
696"
OPEN ACCESS TO DATA AND CODE,0.8172541743970315,"• While we encourage the release of code and data, we understand that this might not be
697"
OPEN ACCESS TO DATA AND CODE,0.8181818181818182,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
698"
OPEN ACCESS TO DATA AND CODE,0.8191094619666048,"including code, unless this is central to the contribution (e.g., for a new open-source
699"
OPEN ACCESS TO DATA AND CODE,0.8200371057513914,"benchmark).
700"
OPEN ACCESS TO DATA AND CODE,0.8209647495361782,"• The instructions should contain the exact command and environment needed to run to
701"
OPEN ACCESS TO DATA AND CODE,0.8218923933209648,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
702"
OPEN ACCESS TO DATA AND CODE,0.8228200371057514,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
703"
OPEN ACCESS TO DATA AND CODE,0.8237476808905381,"• The authors should provide instructions on data access and preparation, including how
704"
OPEN ACCESS TO DATA AND CODE,0.8246753246753247,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
705"
OPEN ACCESS TO DATA AND CODE,0.8256029684601113,"• The authors should provide scripts to reproduce all experimental results for the new
706"
OPEN ACCESS TO DATA AND CODE,0.826530612244898,"proposed method and baselines. If only a subset of experiments are reproducible, they
707"
OPEN ACCESS TO DATA AND CODE,0.8274582560296846,"should state which ones are omitted from the script and why.
708"
OPEN ACCESS TO DATA AND CODE,0.8283858998144712,"• At submission time, to preserve anonymity, the authors should release anonymized
709"
OPEN ACCESS TO DATA AND CODE,0.8293135435992579,"versions (if applicable).
710"
OPEN ACCESS TO DATA AND CODE,0.8302411873840445,"• Providing as much information as possible in supplemental material (appended to the
711"
OPEN ACCESS TO DATA AND CODE,0.8311688311688312,"paper) is recommended, but including URLs to data and code is permitted.
712"
OPEN ACCESS TO DATA AND CODE,0.8320964749536178,"6. Experimental Setting/Details
713"
OPEN ACCESS TO DATA AND CODE,0.8330241187384044,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
714"
OPEN ACCESS TO DATA AND CODE,0.8339517625231911,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
715"
OPEN ACCESS TO DATA AND CODE,0.8348794063079777,"results?
716"
OPEN ACCESS TO DATA AND CODE,0.8358070500927643,"Answer: [Yes]
717"
OPEN ACCESS TO DATA AND CODE,0.8367346938775511,"Justification: We provide the hyperparameter settings in Table 2.
718"
OPEN ACCESS TO DATA AND CODE,0.8376623376623377,"Guidelines:
719"
OPEN ACCESS TO DATA AND CODE,0.8385899814471243,"• The answer NA means that the paper does not include experiments.
720"
OPEN ACCESS TO DATA AND CODE,0.839517625231911,"• The experimental setting should be presented in the core of the paper to a level of detail
721"
OPEN ACCESS TO DATA AND CODE,0.8404452690166976,"that is necessary to appreciate the results and make sense of them.
722"
OPEN ACCESS TO DATA AND CODE,0.8413729128014842,"• The full details can be provided either with the code, in appendix, or as supplemental
723"
OPEN ACCESS TO DATA AND CODE,0.8423005565862709,"material.
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8432282003710575,"7. Experiment Statistical Significance
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8441558441558441,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8450834879406308,"information about the statistical significance of the experiments?
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8460111317254174,"Answer: [No]
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8469387755102041,"Justification: The proposed methods rely on the sample size of the calibration dataset.
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8478664192949907,"Nevertheless, we are convinced that the sample size used in the experiments is sufficient for
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8487940630797773,"achieving reliable and consistent calibration results.
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.849721706864564,"Guidelines:
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8506493506493507,"• The answer NA means that the paper does not include experiments.
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8515769944341373,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.852504638218924,"dence intervals, or statistical significance tests, at least for the experiments that support
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8534322820037106,"the main claims of the paper.
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8543599257884972,"• The factors of variability that the error bars are capturing should be clearly stated (for
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8552875695732839,"example, train/test split, initialization, random drawing of some parameter, or overall
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8562152133580705,"run with given experimental conditions).
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"• The method for calculating the error bars should be explained (closed form formula,
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8580705009276438,"call to a library function, bootstrap, etc.)
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8589981447124304,"• The assumptions made should be given (e.g., Normally distributed errors).
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.859925788497217,"• It should be clear whether the error bar is the standard deviation or the standard error
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8608534322820037,"of the mean.
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8617810760667903,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.862708719851577,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8636363636363636,"of Normality of errors is not verified.
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8645640074211502,"• For asymmetric distributions, the authors should be careful not to show in tables or
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.865491651205937,"figures symmetric error bars that would yield results that are out of range (e.g. negative
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8664192949907236,"error rates).
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8673469387755102,"• If error bars are reported in tables or plots, The authors should explain in the text how
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8682745825602969,"they were calculated and reference the corresponding figures or tables in the text.
752"
EXPERIMENTS COMPUTE RESOURCES,0.8692022263450835,"8. Experiments Compute Resources
753"
EXPERIMENTS COMPUTE RESOURCES,0.8701298701298701,"Question: For each experiment, does the paper provide sufficient information on the com-
754"
EXPERIMENTS COMPUTE RESOURCES,0.8710575139146568,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
755"
EXPERIMENTS COMPUTE RESOURCES,0.8719851576994434,"the experiments?
756"
EXPERIMENTS COMPUTE RESOURCES,0.87291280148423,"Answer: [Yes]
757"
EXPERIMENTS COMPUTE RESOURCES,0.8738404452690167,"Justification: We provide a computational cost analysis in Section 5.5.
758"
EXPERIMENTS COMPUTE RESOURCES,0.8747680890538033,"Guidelines:
759"
EXPERIMENTS COMPUTE RESOURCES,0.87569573283859,"• The answer NA means that the paper does not include experiments.
760"
EXPERIMENTS COMPUTE RESOURCES,0.8766233766233766,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
761"
EXPERIMENTS COMPUTE RESOURCES,0.8775510204081632,"or cloud provider, including relevant memory and storage.
762"
EXPERIMENTS COMPUTE RESOURCES,0.87847866419295,"• The paper should provide the amount of compute required for each of the individual
763"
EXPERIMENTS COMPUTE RESOURCES,0.8794063079777366,"experimental runs as well as estimate the total compute.
764"
EXPERIMENTS COMPUTE RESOURCES,0.8803339517625232,"• The paper should disclose whether the full research project required more compute
765"
EXPERIMENTS COMPUTE RESOURCES,0.8812615955473099,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
766"
EXPERIMENTS COMPUTE RESOURCES,0.8821892393320965,"didn’t make it into the paper).
767"
CODE OF ETHICS,0.8831168831168831,"9. Code Of Ethics
768"
CODE OF ETHICS,0.8840445269016698,"Question: Does the research conducted in the paper conform, in every respect, with the
769"
CODE OF ETHICS,0.8849721706864564,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
770"
CODE OF ETHICS,0.885899814471243,"Answer: [Yes]
771"
CODE OF ETHICS,0.8868274582560297,"Justification: We have reviewed the code of ethics.
772"
CODE OF ETHICS,0.8877551020408163,"Guidelines:
773"
CODE OF ETHICS,0.8886827458256029,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
774"
CODE OF ETHICS,0.8896103896103896,"• If the authors answer No, they should explain the special circumstances that require a
775"
CODE OF ETHICS,0.8905380333951762,"deviation from the Code of Ethics.
776"
CODE OF ETHICS,0.891465677179963,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
777"
CODE OF ETHICS,0.8923933209647495,"eration due to laws or regulations in their jurisdiction).
778"
BROADER IMPACTS,0.8933209647495362,"10. Broader Impacts
779"
BROADER IMPACTS,0.8942486085343229,"Question: Does the paper discuss both potential positive societal impacts and negative
780"
BROADER IMPACTS,0.8951762523191095,"societal impacts of the work performed?
781"
BROADER IMPACTS,0.8961038961038961,"Answer: [NA]
782"
BROADER IMPACTS,0.8970315398886828,"Justification:
783"
BROADER IMPACTS,0.8979591836734694,"Guidelines:
784"
BROADER IMPACTS,0.898886827458256,"• The answer NA means that there is no societal impact of the work performed.
785"
BROADER IMPACTS,0.8998144712430427,"• If the authors answer NA or No, they should explain why their work has no societal
786"
BROADER IMPACTS,0.9007421150278293,"impact or why the paper does not address societal impact.
787"
BROADER IMPACTS,0.9016697588126159,"• Examples of negative societal impacts include potential malicious or unintended uses
788"
BROADER IMPACTS,0.9025974025974026,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
789"
BROADER IMPACTS,0.9035250463821892,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
790"
BROADER IMPACTS,0.9044526901669759,"groups), privacy considerations, and security considerations.
791"
BROADER IMPACTS,0.9053803339517625,"• The conference expects that many papers will be foundational research and not tied
792"
BROADER IMPACTS,0.9063079777365491,"to particular applications, let alone deployments. However, if there is a direct path to
793"
BROADER IMPACTS,0.9072356215213359,"any negative applications, the authors should point it out. For example, it is legitimate
794"
BROADER IMPACTS,0.9081632653061225,"to point out that an improvement in the quality of generative models could be used to
795"
BROADER IMPACTS,0.9090909090909091,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
796"
BROADER IMPACTS,0.9100185528756958,"that a generic algorithm for optimizing neural networks could enable people to train
797"
BROADER IMPACTS,0.9109461966604824,"models that generate Deepfakes faster.
798"
BROADER IMPACTS,0.911873840445269,"• The authors should consider possible harms that could arise when the technology is
799"
BROADER IMPACTS,0.9128014842300557,"being used as intended and functioning correctly, harms that could arise when the
800"
BROADER IMPACTS,0.9137291280148423,"technology is being used as intended but gives incorrect results, and harms following
801"
BROADER IMPACTS,0.9146567717996289,"from (intentional or unintentional) misuse of the technology.
802"
BROADER IMPACTS,0.9155844155844156,"• If there are negative societal impacts, the authors could also discuss possible mitigation
803"
BROADER IMPACTS,0.9165120593692022,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
804"
BROADER IMPACTS,0.9174397031539888,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
805"
BROADER IMPACTS,0.9183673469387755,"feedback over time, improving the efficiency and accessibility of ML).
806"
SAFEGUARDS,0.9192949907235621,"11. Safeguards
807"
SAFEGUARDS,0.9202226345083488,"Question: Does the paper describe safeguards that have been put in place for responsible
808"
SAFEGUARDS,0.9211502782931354,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
809"
SAFEGUARDS,0.922077922077922,"image generators, or scraped datasets)?
810"
SAFEGUARDS,0.9230055658627088,"Answer: [NA]
811"
SAFEGUARDS,0.9239332096474954,"Justification:
812"
SAFEGUARDS,0.924860853432282,"Guidelines:
813"
SAFEGUARDS,0.9257884972170687,"• The answer NA means that the paper poses no such risks.
814"
SAFEGUARDS,0.9267161410018553,"• Released models that have a high risk for misuse or dual-use should be released with
815"
SAFEGUARDS,0.9276437847866419,"necessary safeguards to allow for controlled use of the model, for example by requiring
816"
SAFEGUARDS,0.9285714285714286,"that users adhere to usage guidelines or restrictions to access the model or implementing
817"
SAFEGUARDS,0.9294990723562152,"safety filters.
818"
SAFEGUARDS,0.9304267161410018,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
819"
SAFEGUARDS,0.9313543599257885,"should describe how they avoided releasing unsafe images.
820"
SAFEGUARDS,0.9322820037105751,"• We recognize that providing effective safeguards is challenging, and many papers do
821"
SAFEGUARDS,0.9332096474953617,"not require this, but we encourage authors to take this into account and make a best
822"
SAFEGUARDS,0.9341372912801484,"faith effort.
823"
LICENSES FOR EXISTING ASSETS,0.935064935064935,"12. Licenses for existing assets
824"
LICENSES FOR EXISTING ASSETS,0.9359925788497218,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
825"
LICENSES FOR EXISTING ASSETS,0.9369202226345084,"the paper, properly credited and are the license and terms of use explicitly mentioned and
826"
LICENSES FOR EXISTING ASSETS,0.937847866419295,"properly respected?
827"
LICENSES FOR EXISTING ASSETS,0.9387755102040817,"Answer: [Yes]
828"
LICENSES FOR EXISTING ASSETS,0.9397031539888683,"Justification: We cite the models and dataset used in the experiments (see Section 5.1).
829"
LICENSES FOR EXISTING ASSETS,0.9406307977736549,"Guidelines:
830"
LICENSES FOR EXISTING ASSETS,0.9415584415584416,"• The answer NA means that the paper does not use existing assets.
831"
LICENSES FOR EXISTING ASSETS,0.9424860853432282,"• The authors should cite the original paper that produced the code package or dataset.
832"
LICENSES FOR EXISTING ASSETS,0.9434137291280148,"• The authors should state which version of the asset is used and, if possible, include a
833"
LICENSES FOR EXISTING ASSETS,0.9443413729128015,"URL.
834"
LICENSES FOR EXISTING ASSETS,0.9452690166975881,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
835"
LICENSES FOR EXISTING ASSETS,0.9461966604823747,"• For scraped data from a particular source (e.g., website), the copyright and terms of
836"
LICENSES FOR EXISTING ASSETS,0.9471243042671614,"service of that source should be provided.
837"
LICENSES FOR EXISTING ASSETS,0.948051948051948,"• If assets are released, the license, copyright information, and terms of use in the
838"
LICENSES FOR EXISTING ASSETS,0.9489795918367347,"package should be provided. For popular datasets, paperswithcode.com/datasets
839"
LICENSES FOR EXISTING ASSETS,0.9499072356215214,"has curated licenses for some datasets. Their licensing guide can help determine the
840"
LICENSES FOR EXISTING ASSETS,0.950834879406308,"license of a dataset.
841"
LICENSES FOR EXISTING ASSETS,0.9517625231910947,"• For existing datasets that are re-packaged, both the original license and the license of
842"
LICENSES FOR EXISTING ASSETS,0.9526901669758813,"the derived asset (if it has changed) should be provided.
843"
LICENSES FOR EXISTING ASSETS,0.9536178107606679,"• If this information is not available online, the authors are encouraged to reach out to
844"
LICENSES FOR EXISTING ASSETS,0.9545454545454546,"the asset’s creators.
845"
NEW ASSETS,0.9554730983302412,"13. New Assets
846"
NEW ASSETS,0.9564007421150278,"Question: Are new assets introduced in the paper well documented and is the documentation
847"
NEW ASSETS,0.9573283858998145,"provided alongside the assets?
848"
NEW ASSETS,0.9582560296846011,"Answer: [NA]
849"
NEW ASSETS,0.9591836734693877,"Justification:
850"
NEW ASSETS,0.9601113172541744,"Guidelines:
851"
NEW ASSETS,0.961038961038961,"• The answer NA means that the paper does not release new assets.
852"
NEW ASSETS,0.9619666048237476,"• Researchers should communicate the details of the dataset/code/model as part of their
853"
NEW ASSETS,0.9628942486085343,"submissions via structured templates. This includes details about training, license,
854"
NEW ASSETS,0.963821892393321,"limitations, etc.
855"
NEW ASSETS,0.9647495361781077,"• The paper should discuss whether and how consent was obtained from people whose
856"
NEW ASSETS,0.9656771799628943,"asset is used.
857"
NEW ASSETS,0.9666048237476809,"• At submission time, remember to anonymize your assets (if applicable). You can either
858"
NEW ASSETS,0.9675324675324676,"create an anonymized URL or include an anonymized zip file.
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9684601113172542,"14. Crowdsourcing and Research with Human Subjects
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693877551020408,"Question: For crowdsourcing experiments and research with human subjects, does the paper
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703153988868275,"include the full text of instructions given to participants and screenshots, if applicable, as
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9712430426716141,"well as details about compensation (if any)?
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9721706864564007,"Answer: [NA]
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9730983302411874,"Justification:
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974025974025974,"Guidelines:
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749536178107606,"• The answer NA means that the paper does not involve crowdsourcing nor research with
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758812615955473,"human subjects.
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768089053803339,"• Including this information in the supplemental material is fine, but if the main contribu-
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777365491651205,"tion of the paper involves human subjects, then as much detail as possible should be
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786641929499073,"included in the main paper.
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795918367346939,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805194805194806,"or other labor should be paid at least the minimum wage in the country of the data
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814471243042672,"collector.
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823747680890538,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833024118738405,"Subjects
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842300556586271,"Question: Does the paper describe potential risks incurred by study participants, whether
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851576994434137,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860853432282004,"approvals (or an equivalent approval/review based on the requirements of your country or
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987012987012987,"institution) were obtained?
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879406307977736,"Answer: [NA]
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888682745825603,"Justification:
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897959183673469,"Guidelines:
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907235621521335,"• The answer NA means that the paper does not involve crowdsourcing nor research with
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916512059369202,"human subjects.
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925788497217068,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935064935064936,"may be required for any human subjects research. If you obtained IRB approval, you
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944341372912802,"should clearly state this in the paper.
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953617810760668,"• We recognize that the procedures for this may vary significantly between institutions
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9962894248608535,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972170686456401,"guidelines for their institution.
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981447124304267,"• For initial submissions, do not include any information that would break anonymity (if
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990723562152134,"applicable), such as the institution conducting the review.
893"
