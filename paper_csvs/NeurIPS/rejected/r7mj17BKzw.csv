Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000999000999000999,"Numerous quantum algorithms operate under the assumption that classical data
1"
ABSTRACT,0.001998001998001998,"has already been converted into quantum states, a process termed Quantum State
2"
ABSTRACT,0.002997002997002997,"Preparation (QSP). However, achieving precise QSP requires a circuit depth that
3"
ABSTRACT,0.003996003996003996,"scales exponentially with the number of qubits, making it a substantial obstacle in
4"
ABSTRACT,0.004995004995004995,"harnessing quantum advantage. Recent research suggests using a Parameterized
5"
ABSTRACT,0.005994005994005994,"Quantum Circuit (PQC) to approximate a target state, offering a more scalable
6"
ABSTRACT,0.006993006993006993,"solution with reduced circuit depth compared to precise QSP. Despite this, the need
7"
ABSTRACT,0.007992007992007992,"for iterative updates of circuit parameters results in a lengthy runtime, limiting its
8"
ABSTRACT,0.008991008991008992,"practical application. To overcome this challenge, we introduce SuperEncoder,
9"
ABSTRACT,0.00999000999000999,"a pre-trained classical neural network model designed to directly estimate the
10"
ABSTRACT,0.01098901098901099,"parameters of a PQC for any given quantum state. By eliminating the need for
11"
ABSTRACT,0.011988011988011988,"iterative parameter tuning, SuperEncoder represents a pioneering step towards
12"
ABSTRACT,0.012987012987012988,"iteration-free approximate QSP.
13"
INTRODUCTION,0.013986013986013986,"1
Introduction
14"
INTRODUCTION,0.014985014985014986,"Quantum Computing (QC) leverages quantum mechanics principles to address classically intractable
15"
INTRODUCTION,0.015984015984015984,"problems [47, 36]. Various quantum algorithms have been developed, encompassing quantum-
16"
INTRODUCTION,0.016983016983016984,"enhanced linear algebra [15, 48, 45], Quantum Machine Learning (QML) [26, 19, 1, 33, 50, 3],
17"
INTRODUCTION,0.017982017982017984,"quantum-enhanced partial differential equation solvers [31, 13], etc. A notable caveat is that those
18"
INTRODUCTION,0.01898101898101898,"algorithms assume that classical data has been efficiently loaded into a specific quantum state, a
19"
INTRODUCTION,0.01998001998001998,"process known as Quantum State Preparation (QSP).
20"
INTRODUCTION,0.02097902097902098,"However, the realization of QSP presents significant challenges. Ideally, we expect each element of
21"
INTRODUCTION,0.02197802197802198,"the classical data to be precisely transformed into an amplitude of the corresponding quantum state.
22"
INTRODUCTION,0.022977022977022976,"This precise QSP is also known as Amplitude Encoding (AE). However, a critical yet unresolved
23"
INTRODUCTION,0.023976023976023976,"problem of AE is that the required circuit depth grows exponentially with respect to the number of
24"
INTRODUCTION,0.024975024975024976,"qubits [34, 41, 29, 46, 49]. Extensive efforts have been made to alleviate this issue, but they fail to
25"
INTRODUCTION,0.025974025974025976,"address it fundamentally. For example, while some methods introduce ancillary qubits for shallower
26"
INTRODUCTION,0.026973026973026972,"circuit [57, 56, 2], they may encounter an exponential number of ancillary qubits. Other methods aim
27"
INTRODUCTION,0.027972027972027972,"at preparing special quantum states with lower circuit depth, being only effective for either sparse
28"
INTRODUCTION,0.028971028971028972,"states [12, 32] or states with some special distributions [14, 17]. To summarize, realizing AE for
29"
INTRODUCTION,0.029970029970029972,"arbitrary quantum states still remains non-scalable due to its exponential resource requirement with
30"
INTRODUCTION,0.030969030969030968,"respect to the number of qubits. Moreover, in the Noisy Intermediate-Scale Quantum (NISQ) era [42],
31"
INTRODUCTION,0.03196803196803197,"hardware has limited qubit lifetimes and confronts a high risk of decoherence errors when executing
32"
INTRODUCTION,0.03296703296703297,"deep circuits, further exacerbating the problem of AE.
33"
INTRODUCTION,0.03396603396603397,"In fact, precise QSP is unrealistic in the present NISQ era due to the inherent errors of quantum
34"
INTRODUCTION,0.03496503496503497,"devices. Hence, iteration-based Approximate Amplitude Encoding (AAE) emerges as a promising
35"
INTRODUCTION,0.03596403596403597,"technique [59, 35, 52]. Specifically, AAE constructs a quantum circuit with tunable parameters, then
36"
INTRODUCTION,0.03696303696303696,"it iteratively updates the parameters to approximate a target quantum state. Since the updating of
37"
INTRODUCTION,0.03796203796203796,"parameters can be guided by states obtained from noisy devices, AAE is robust to noises, becoming
38"
INTRODUCTION,0.03896103896103896,"especially suitable for NISQ applications. More importantly, AAE has been shown to have shallow
39"
INTRODUCTION,0.03996003996003996,"circuit depth [35, 52], making it more scalable than AE.
40"
INTRODUCTION,0.04095904095904096,"4
6
8
Number of Qubits
0.00 0.25 0.50 0.75 1.00"
INTRODUCTION,0.04195804195804196,Normalized Time
INTRODUCTION,0.04295704295704296,"99.21%
99.72%
99.84%"
INTRODUCTION,0.04395604395604396,"Ttotal −TAAE
TAAE"
INTRODUCTION,0.04495504495504495,"Figure 1: Breakdown of normalized run-
time for QNN inference. Original data
are listed in Table 1."
INTRODUCTION,0.04595404595404595,"Unfortunately, AAE possesses a drawback that signifi-
41"
INTRODUCTION,0.04695304695304695,"cantly undermines its potential advantages — the lengthy
42"
INTRODUCTION,0.04795204795204795,"runtime stemming from iterative optimizations of param-
43"
INTRODUCTION,0.04895104895104895,"eters. For example, when a Quantum Neural Network
44"
INTRODUCTION,0.04995004995004995,"(QNN) [3] is trained and deployed, the runtime of AAE
45"
INTRODUCTION,0.05094905094905095,"dominates the inference time as we demonstrated in Fig. 1.
46"
INTRODUCTION,0.05194805194805195,"Since loading classical data into quantum states becomes
47"
INTRODUCTION,0.052947052947052944,"the bottleneck, the potential advantage of QNN dimin-
48"
INTRODUCTION,0.053946053946053944,"ishes no matter how efficient the computations are done
49"
INTRODUCTION,0.054945054945054944,"on quantum devices.
50"
INTRODUCTION,0.055944055944055944,"Compared to AAE, AE employs a pre-defined arithmetic
51"
INTRODUCTION,0.056943056943056944,"decomposition procedure to construct a circuit, thereby
52"
INTRODUCTION,0.057942057942057944,"becoming much faster than AAE at runtime. Therefore,
53"
INTRODUCTION,0.058941058941058944,"it is natural to ask: can we realize both fast and scalable
54"
INTRODUCTION,0.059940059940059943,"methods for arbitrary QSP? This is precisely the question
55"
INTRODUCTION,0.060939060939060936,"we tackle in this paper. Overall, we present three major
56"
INTRODUCTION,0.061938061938061936,"contributions.
57"
INTRODUCTION,0.06293706293706294,"• Given a Parameterized Quantum Circuit (PQC) U(θ) that approximates a target quantum state,
58"
INTRODUCTION,0.06393606393606394,"with θ the parameter vector. We show that there exists a deterministic transformation f that could
59"
INTRODUCTION,0.06493506493506493,"map an arbitrary state |d⟩to its corresponding parameters θ. Consequently, the parameters can be
60"
INTRODUCTION,0.06593406593406594,"designated by f without time-intensive iterations.
61"
INTRODUCTION,0.06693306693306693,"• We show that the mapping f is learnable by utilizing a classical neural network model, which
62"
INTRODUCTION,0.06793206793206794,"we term as SuperEncoder. With SuperEncoder, you can have your cake and eat it too, i.e.,
63"
INTRODUCTION,0.06893106893106893,"simultaneously realizing fast and scalable QSP. We develop a prototype model and shed light on
64"
INTRODUCTION,0.06993006993006994,"insights into its training methodology.
65"
INTRODUCTION,0.07092907092907093,"• We verify the effectiveness of SuperEncoder on both synthetic dataset and representative down-
66"
INTRODUCTION,0.07192807192807193,"stream tasks, paving the way toward iteration-free approximate quantum state preparation.
67"
PRELIMINARIES,0.07292707292707293,"2
Preliminaries
68"
PRELIMINARIES,0.07392607392607392,"In this section, we commence with some basic concepts about quantum computing [36], and then
69"
PRELIMINARIES,0.07492507492507493,"proceed to a brief retrospect of existing QSP methods.
70"
QUANTUM COMPUTATION,0.07592407592407592,"2.1
Quantum Computation
71"
QUANTUM COMPUTATION,0.07692307692307693,"We use Dirac notation throughout this paper. A pure quantum state is defined by a vector |·⟩named
72"
QUANTUM COMPUTATION,0.07792207792207792,"‘ket’, with the unit length. A state can be written as |ψ⟩= PN
j=1 αj|j⟩with P"
QUANTUM COMPUTATION,0.07892107892107893,"j |αj|2 = 1, where
73"
QUANTUM COMPUTATION,0.07992007992007992,"|j⟩denotes a computational basis state and N represents the dimension of the complex vector
74"
QUANTUM COMPUTATION,0.08091908091908091,"space. Density operators describe more general quantum states. Given a mixture of m pure states
75"
QUANTUM COMPUTATION,0.08191808191808192,"{|ψi⟩}m
i=1 with probabilities pi and Pm
i pi = 1, the density operator ρ denotes the mixed state as
76"
QUANTUM COMPUTATION,0.08291708291708291,"ρ = Pm
i=1 pi|ψi⟩⟨ψi| with Tr(ρ) = 1, where ⟨·| refers to the conjugate transpose of |·⟩. Generally,
77"
QUANTUM COMPUTATION,0.08391608391608392,"we use the term fidelity to describe the similarity between an erroneous quantum state and its
78"
QUANTUM COMPUTATION,0.08491508491508491,"corresponding correct state.
79"
QUANTUM COMPUTATION,0.08591408591408592,"The fundamental unit of quantum computation is the quantum bit, or qubit. A qubit’s state can be
80"
QUANTUM COMPUTATION,0.08691308691308691,"expressed as ψ = α|0⟩+ β|1⟩. Given n qubits, the state is generalized to |ψ⟩= P2n"
QUANTUM COMPUTATION,0.08791208791208792,"j |j⟩, where
81"
QUANTUM COMPUTATION,0.08891108891108891,"|j⟩= |j1j2 · · · jn⟩with jk the state of kth qubit in computational basis, and j = Pn
k=1 2n−kjk.
82"
QUANTUM COMPUTATION,0.0899100899100899,"Applying quantum operations evolves a system from one state to another. Generally, these operations
83"
QUANTUM COMPUTATION,0.09090909090909091,"can be categorized into quantum gates and measurements. Typical single-qubit gates include the
84"
QUANTUM COMPUTATION,0.0919080919080919,"Pauli gates X ≡[ 0 1
1 0 ], Y ≡
 0 −i
i
0

, Z ≡
 1
0
0 −1

. These gates have associated rotation operations
85"
QUANTUM COMPUTATION,0.09290709290709291,"RP (θ) ≡e−iθP/2, where θ is the rotation angle and P ∈{X, Y, Z}1. Muti-qubit operations create
86"
QUANTUM COMPUTATION,0.0939060939060939,"1In this paper, Rz, Ry are equivalent to RZ, RY ."
QUANTUM COMPUTATION,0.09490509490509491,"entanglement between qubits, allowing one qubit to interfere with others. In this work, we focus on
87"
QUANTUM COMPUTATION,0.0959040959040959,"the controlled-NOT (CNOT) gate, with the mathematical form of CNOT ≡|0⟩⟨0| ⊗I2 + |1⟩⟨1| ⊗X.
88"
QUANTUM COMPUTATION,0.0969030969030969,"Quantum measurements extract classical information from quantum states, which is described by
89"
QUANTUM COMPUTATION,0.0979020979020979,a collection {Mm} with P
QUANTUM COMPUTATION,0.0989010989010989,"m M †
mMm = I. Here, m refers to the measurement outcomes that may
90"
QUANTUM COMPUTATION,0.0999000999000999,"occur in the experiment, with a probability of p(m) = ⟨ψ|M †
mMm|ψ⟩. The post-measurement state
91"
QUANTUM COMPUTATION,0.1008991008991009,"of the system becomes Mm|ψ⟩/p(m).
92"
QUANTUM COMPUTATION,0.1018981018981019,"A quantum circuit is the graphical representation of a series of quantum operations, which can be
93"
QUANTUM COMPUTATION,0.1028971028971029,"mathematically represented by a unitary matrix U. In the NISQ era, PQC plays an important role
94"
QUANTUM COMPUTATION,0.1038961038961039,"as it underpins variational quantum algorithms [11, 39]. Typical PQC has the form of U(θ) =
95
Q"
QUANTUM COMPUTATION,0.1048951048951049,"i Ui(θi)Vi, where θ is its parameter vector, Ui(θi) = e−iθiPi/2 with Pi denoting a Pauli gate, and
96"
QUANTUM COMPUTATION,0.10589410589410589,"Vi denotes a fixed gate such as CNOT. For example, a PQC composed of Ry gates and CNOT gates
97"
QUANTUM COMPUTATION,0.1068931068931069,"is depicted in Fig. 2.
98"
QUANTUM COMPUTATION,0.10789210789210789,"|0⟩
Ry(θ0)
Ry(θ4)"
QUANTUM COMPUTATION,0.1088911088911089,"|0⟩
Ry(θ1)
Ry(θ5)"
QUANTUM COMPUTATION,0.10989010989010989,"|0⟩
Ry(θ2)
Ry(θ6)"
QUANTUM COMPUTATION,0.1108891108891109,"|0⟩
Ry(θ3)
Ry(θ7)"
QUANTUM COMPUTATION,0.11188811188811189,"Block # 0
Block # 1"
QUANTUM COMPUTATION,0.11288711288711288,Approximated state of |d⟩
QUANTUM COMPUTATION,0.11388611388611389,"Figure 2: An example PQC with two blocks, with each block consisting of a rotation layer (filled
blue) plus an entangler layer (filled red)."
QUANTUM STATE PREPARATION,0.11488511488511488,"2.2
Quantum State Preparation
99"
QUANTUM STATE PREPARATION,0.11588411588411589,"Successful execution of many quantum algorithms requires an initial step of loading classical data
100"
QUANTUM STATE PREPARATION,0.11688311688311688,"into a quantum state [5, 15], a process known as quantum state preparation. This procedure involves
101"
QUANTUM STATE PREPARATION,0.11788211788211789,"implementing a quantum circuit to evolve a system to a designated state. Here, we focus on amplitude
102"
QUANTUM STATE PREPARATION,0.11888111888111888,"encoding and formalize its procedure as follows. Let d be a real-valued N-dimensional classical
103"
QUANTUM STATE PREPARATION,0.11988011988011989,"vector, AE encodes d into the amplitudes of an n-qubit quantum state |d⟩, where N = 2n. More
104"
QUANTUM STATE PREPARATION,0.12087912087912088,"specifically, the data quantum state is represented by |d⟩= PN−1
j=0 dj|j⟩, where dj denotes the jth
105"
QUANTUM STATE PREPARATION,0.12187812187812187,"element of the vector d, and |j⟩refers to a computational basis state. The main objective is to generate
106"
QUANTUM STATE PREPARATION,0.12287712287712288,"a quantum circuit U that initializes an n-qubit system by U|0⟩⊗n = PN−1
j=0 αj|j⟩, whose amplitudes
107"
QUANTUM STATE PREPARATION,0.12387612387612387,"{αj} are equal to {dj}. It is widely recognized that constructing such a circuit generally necessitates
108"
QUANTUM STATE PREPARATION,0.12487512487512488,"a circuit depth that scales exponentially with n [34, 41]. This property makes AE impractical in
109"
QUANTUM STATE PREPARATION,0.1258741258741259,"current NISQ era, as decoherence errors [23] can severely dampen the effectiveness of AE as the
110"
QUANTUM STATE PREPARATION,0.12687312687312688,"number of qubits increases [52].
111"
QUANTUM STATE PREPARATION,0.12787212787212787,"In response to the inherent noisy nature of current devices, approximate amplitude encoding has
112"
QUANTUM STATE PREPARATION,0.12887112887112886,"emerged as a promising technique [59, 35, 52]. Specifically, AAE utilizes a PQC (a.k.a. ansatz) to
113"
QUANTUM STATE PREPARATION,0.12987012987012986,"approximate the target quantum state by iteratively updating the parameters of circuit, following
114"
QUANTUM STATE PREPARATION,0.13086913086913088,"a similar procedure of other variational quantum algorithms [39, 11]. AAE has been shown to be
115"
QUANTUM STATE PREPARATION,0.13186813186813187,"more advantageous for NISQ devices due to its ability to mitigate coherent errors through flexible
116"
QUANTUM STATE PREPARATION,0.13286713286713286,"adjustment of circuit parameters, coupled with its lower circuit depth [52]. We denote an ansatz as
117"
QUANTUM STATE PREPARATION,0.13386613386613386,"U(θ), where θ refers to a vector of tunable parameters for optimizations. A typical ansatz consists
118"
QUANTUM STATE PREPARATION,0.13486513486513488,"of several blocks of operations with the same structure. For example, a two-block ansatz with 4
119"
QUANTUM STATE PREPARATION,0.13586413586413587,"qubits is shown in Fig. 2, where the rotation layer is composed of single-qubit rotational gates
120"
QUANTUM STATE PREPARATION,0.13686313686313686,"Ry(θr) = e−iθrY/2, and the entangler layer comprises CNOT gates. Note that the entangler layer is
121"
QUANTUM STATE PREPARATION,0.13786213786213786,"configurable and hardware-native, which means that we can apply CNOT gates to physically adjacent
122"
QUANTUM STATE PREPARATION,0.13886113886113885,"qubits, thereby eliminating the necessity of additional SWAP gates to overcome the topological
123"
QUANTUM STATE PREPARATION,0.13986013986013987,"constraints [27]. This type of PQC is also known as hardware-efficient ansatz [20], being widely
124"
QUANTUM STATE PREPARATION,0.14085914085914086,"adopted in previous studies of AAE [59, 35, 52].
125"
SUPERENCODER,0.14185814185814186,"3
SuperEncoder
126"
MOTIVATION,0.14285714285714285,"3.1
Motivation
127"
MOTIVATION,0.14385614385614387,"Although AAE can potentially realize high fidelity QSP with O(poly(n)) circuit depth [35] with n
128"
MOTIVATION,0.14485514485514486,"the number of qubits, it requires repetitive online tuning of parameters to approximate the target
129"
MOTIVATION,0.14585414585414586,"state, which may result in an excessively long runtime that undermines its feasibility. Specifically, we
130"
MOTIVATION,0.14685314685314685,"could consider a simple application scenario in QML. The workflow with AAE is depicted in Fig. 3a.
131"
MOTIVATION,0.14785214785214784,"During the inference stage, we must iteratively update the parameters of the AAE ansatz for each
132"
MOTIVATION,0.14885114885114886,"input classical data vector, which may greatly dampen the performance. To quantify this impact, we
133"
MOTIVATION,0.14985014985014986,"measure the runtime of AAE-based data loading and the total runtime of model inference. As one can
134"
MOTIVATION,0.15084915084915085,"observe from Table 1, AAE dominates the runtime, thereby becoming the performance bottleneck.
135"
MOTIVATION,0.15184815184815184,"n
TAAE (s)
Ttotal −TAAE (s)
4
5.0086
0.0397
6
20.1810
0.0573
8
59.4193
0.0978
Table 1: Performance overhead of AAE. We break down the averaged inference runtime per sample
from the MNIST dataset. TAAE denotes time spent on loading classical data into quantum state using
AAE, and Ttotal refers to total runtime."
MOTIVATION,0.15284715284715283,"The necessity of time-intensive iterations is grounded in the following assumption — Given an
136"
MOTIVATION,0.15384615384615385,"arbitrary quantum state |ψ⟩, there does not exist a deterministic transformation f : |ψ⟩→θ, where
137"
MOTIVATION,0.15484515484515485,"θ refers to the vector of parameters enabling a PQC to prepare an approximated state of |ψ⟩. This
138"
MOTIVATION,0.15584415584415584,"assumption seems intuitively correct given the randomness of target states. However, we argue that a
139"
MOTIVATION,0.15684315684315683,"universal mapping f exists for any arbitrary data state |ψ⟩. Taking a little thought of AE, we see that
140"
MOTIVATION,0.15784215784215785,"it implies the following conclusion: given an arbitrary state |ψ⟩, there exists an universal arithmetic
141"
MOTIVATION,0.15884115884115885,"decomposition procedure g : |ψ⟩→U satisfying U|0⟩= |ψ⟩. Inspired by this deterministic
142"
MOTIVATION,0.15984015984015984,"transformation, it is natural to ask: is there an universal transformation g′ : |ψ⟩→U ′ satisfying
143"
MOTIVATION,0.16083916083916083,"E(U ′|0⟩, |ψ⟩) ≤ϵ? Here E denotes the deviation between the prepared state by a circuit U ′ and the
144"
MOTIVATION,0.16183816183816183,"target state, and ϵ refers to certain acceptable error threshold. Since the structure of PQC in AAE
145"
MOTIVATION,0.16283716283716285,"is the same for any target state, U ′ is determined by θ. Then, the problem is reduced to exploring
146"
MOTIVATION,0.16383616383616384,"the existence of f : |ψ⟩→θ. Should f exist, the overhead of online iterations could be eliminated,
147"
MOTIVATION,0.16483516483516483,"resulting in a novel QSP method being both fast and scalable.
148"
MOTIVATION,0.16583416583416583,Parameterized Quantum Circuit (PQC) PQC
MOTIVATION,0.16683316683316685,Classical Data
MOTIVATION,0.16783216783216784,Data Encoding AAE
MOTIVATION,0.16883116883116883,Quantum State
MOTIVATION,0.16983016983016982,Quantum Neural
MOTIVATION,0.17082917082917082,Network Layers
MOTIVATION,0.17182817182817184,Classification Result
MOTIVATION,0.17282717282717283,Iteration #0
MOTIVATION,0.17382617382617382,Iteration #1
MOTIVATION,0.17482517482517482,(a) Inference process of AAE.
MOTIVATION,0.17582417582417584,"Parameterized Quantum Circuit (PQC)
Classical Data"
MOTIVATION,0.17682317682317683,"Data Encoding
SuperEncoder"
MOTIVATION,0.17782217782217782,Quantum State
MOTIVATION,0.17882117882117882,Quantum Neural
MOTIVATION,0.1798201798201798,Network Layers
MOTIVATION,0.18081918081918083,Classification Result
MOTIVATION,0.18181818181818182,(b) Inference process of SuperEncoder.
MOTIVATION,0.18281718281718282,Figure 3: Comparison between AAE and SuperEncoder.
DESIGN METHODOLOGY,0.1838161838161838,"3.2
Design Methodology
149"
DESIGN METHODOLOGY,0.1848151848151848,"Let |ψ⟩be the target state, and U(θ) be the PQC used in AAE with θ the optimized parameters.
150"
DESIGN METHODOLOGY,0.18581418581418582,"Our goal is to develop a model, termed SuperEncoder, to approximate the mapping f : |ψ⟩→θ.
151"
DESIGN METHODOLOGY,0.18681318681318682,"Referring back to the scenario in QML, the workflow with SuperEncoder becomes iteration-free, as
152"
DESIGN METHODOLOGY,0.1878121878121878,"depicted in Fig. 3b.
153"
DESIGN METHODOLOGY,0.1888111888111888,"Since neural networks could be used to approximate any continuous function [6], a natural solution is
154"
DESIGN METHODOLOGY,0.18981018981018982,"to use a neural network to approximate f. Specifically, we adopt a Multi-Layer Perceptron (MLP) as
155"
DESIGN METHODOLOGY,0.19080919080919082,"the backbone model for approximating f. However, training this model is nontrivial. Particularly, we
156"
DESIGN METHODOLOGY,0.1918081918081918,"find it challenging to design a proper loss function. In the remainder of this section, we explore three
157"
DESIGN METHODOLOGY,0.1928071928071928,"different designs and analyze their performance.
158"
DESIGN METHODOLOGY,0.1938061938061938,"(a) Target state.
(b) SuperEncoder-L1 (c) SuperEncoder-L3"
DESIGN METHODOLOGY,0.19480519480519481,"Figure 4: Virtualization of states generated by SuperEncoder trained with different loss functions. L2
is omitted as it produces very similar results to L3."
DESIGN METHODOLOGY,0.1958041958041958,"The first and most straightforward method is parameter-oriented training — setting the loss function
159"
DESIGN METHODOLOGY,0.1968031968031968,"L1 as the MSE between the target parameters θ from AAE and the output parameters ˆθ from
160"
DESIGN METHODOLOGY,0.1978021978021978,"SuperEncoder. To evaluate the performance of L1, we train a SuperEncoder using MNIST dataset,
161"
DESIGN METHODOLOGY,0.19880119880119881,"and test if it could load a test digit image into a quantum state with high fidelity. All images are
162"
DESIGN METHODOLOGY,0.1998001998001998,"downsampled and normalized into 4-qubit states for quick evaluation.
163"
DESIGN METHODOLOGY,0.2007992007992008,"L1
L2
L3
0.6208
0.9873
0.9908"
DESIGN METHODOLOGY,0.2017982017982018,"Table 2: Fidelity comparison be-
tween SuperEncoders trained with
different loss functions."
DESIGN METHODOLOGY,0.20279720279720279,"Unfortunately, results in Table 2 show that L1 achieves poor
164"
DESIGN METHODOLOGY,0.2037962037962038,"performance. The average fidelity of prepared quantum states
165"
DESIGN METHODOLOGY,0.2047952047952048,"is only 0.6208. As demonstrated in Fig. 4, L1 generates a state
166"
DESIGN METHODOLOGY,0.2057942057942058,"that losses the patterns of the original state. Additionally, utiliz-
167"
DESIGN METHODOLOGY,0.20679320679320679,"ing L1 implies that we need to first generate target parameters
168"
DESIGN METHODOLOGY,0.2077922077922078,"using AAE, of which the long runtime hinders pre-training on
169"
DESIGN METHODOLOGY,0.2087912087912088,"larger datasets. Consequently, required is a more effective loss
170"
DESIGN METHODOLOGY,0.2097902097902098,"function design without involving AAE.
171"
DESIGN METHODOLOGY,0.21078921078921078,"0
200
400
600
800
Step 0.00 0.25 0.50 0.75 1.00 Loss"
DESIGN METHODOLOGY,0.21178821178821178,"L1
L2
L3"
DESIGN METHODOLOGY,0.2127872127872128,"Figure 5: Convergence of dif-
ferent loss functions."
DESIGN METHODOLOGY,0.2137862137862138,"To address this challenge, we propose a state-oriented training
172"
DESIGN METHODOLOGY,0.21478521478521478,"methodology, which employs quantum states as targets to guide
173"
DESIGN METHODOLOGY,0.21578421578421578,"optimizations. Specifically, we may apply ˆθ to the circuit and exe-
174"
DESIGN METHODOLOGY,0.21678321678321677,"cute it to obtain the prepared state ˆψ. Then it is possible to calculate
175"
DESIGN METHODOLOGY,0.2177822177822178,"the difference between ˆψ and ψ as the loss to optimize SuperEncoder.
176"
DESIGN METHODOLOGY,0.21878121878121878,"In contrast to parameter-oriented training, this approach applies to
177"
DESIGN METHODOLOGY,0.21978021978021978,"larger datasets as it decouples the training procedure from AAE. We
178"
DESIGN METHODOLOGY,0.22077922077922077,"utilize two different state-oriented metrics, the first being the MSE
179"
DESIGN METHODOLOGY,0.2217782217782218,"between ˆψ and ψ, denoted as L2, and the second is the fidelity of
180"
DESIGN METHODOLOGY,0.22277722277722278,"ˆψ relative to ψ, expressed as L3 = 1 −|⟨ˆψ|ψ⟩|2 [25]. Results in
181"
DESIGN METHODOLOGY,0.22377622377622378,"Table 2 show that L2 and L3 achieve remarkably higher fidelity than
182"
DESIGN METHODOLOGY,0.22477522477522477,"L1. Besides, we observe that L3 prepares a state very similar to the
183"
DESIGN METHODOLOGY,0.22577422577422576,"target one (Fig. 4), verifying that state-oriented training is more effective than parameter-oriented
184"
DESIGN METHODOLOGY,0.22677322677322678,"training.
185"
DESIGN METHODOLOGY,0.22777222777222778,"Landscape Analysis. To understand the efficacy of these loss functions, we further analyze their
186"
DESIGN METHODOLOGY,0.22877122877122877,"landscapes following previous studies [28, 40, 18]. To gain insight from the landscape, we plot Fig. 6
187"
DESIGN METHODOLOGY,0.22977022977022976,"using the same scale and color gradients [18]. Compared to state-oriented losses (L2 and L3), L1 has
188"
DESIGN METHODOLOGY,0.23076923076923078,"a largely flat landscape with non-decreasing minima, thus the model struggles to explore a viable
189"
DESIGN METHODOLOGY,0.23176823176823177,"path towards a lower loss value, a similar pattern can also be observed in Fig. 5. In contrast, L2
190"
DESIGN METHODOLOGY,0.23276723276723277,"0
20
40
60
80 100
0 20 40 60 80 100 0.0 0.5 1.0 1.5 2.0"
DESIGN METHODOLOGY,0.23376623376623376,(a) L1
DESIGN METHODOLOGY,0.23476523476523475,"0
20
40
60
80 100
0 20 40 60 80 100 0.0 0.5 1.0 1.5 2.0"
DESIGN METHODOLOGY,0.23576423576423577,(b) L2
DESIGN METHODOLOGY,0.23676323676323677,"0
20
40
60
80 100
0 20 40 60 80 100 0.0 0.5 1.0 1.5 2.0"
DESIGN METHODOLOGY,0.23776223776223776,(c) L3
DESIGN METHODOLOGY,0.23876123876123875,Figure 6: Landscape virtualization of different loss functions.
DESIGN METHODOLOGY,0.23976023976023977,"and L3 have much lower minima and successfully converge to smaller loss values. Furthermore, we
191"
DESIGN METHODOLOGY,0.24075924075924077,"observe from Fig. 6 that L3 has a wider minima than L2, which may indicate a better generalization
192"
DESIGN METHODOLOGY,0.24175824175824176,"capability [40].
193"
DESIGN METHODOLOGY,0.24275724275724275,"Gradient Analysis. Based on the landscape analysis, we adopt L3 as the loss function to train
194"
DESIGN METHODOLOGY,0.24375624375624375,"SuperEncoder. We note that L3 can be written as 1 −⟨ψ| ˆψ⟩⟨ˆψ|ψ⟩. If ˆρ is a pure state, it is equivalent
195"
DESIGN METHODOLOGY,0.24475524475524477,"to | ˆψ⟩⟨ˆψ|. Then L3 is given by L3 = 1 −⟨ψ|ˆρ|ψ⟩.
196"
DESIGN METHODOLOGY,0.24575424575424576,"This re-formalization is important as only the mixed state ˆρ could be obtained in noisy environments.
197"
DESIGN METHODOLOGY,0.24675324675324675,"Suppose an n-qubit circuit is parameterized by m parameters ˆθ = [ˆθ1, . . . , ˆθk, . . . , ˆθm]. Let W be
198"
DESIGN METHODOLOGY,0.24775224775224775,"the weight matrix of MLP, with k, l the element indices. We analyze the gradient of L3 w.r.t. Wk,l to
199"
DESIGN METHODOLOGY,0.24875124875124874,"showcase its feasibility in different quantum computing environments.
200"
DESIGN METHODOLOGY,0.24975024975024976,"∇Wk,lL3 = ∂L3"
DESIGN METHODOLOGY,0.25074925074925075,"∂Wk,l
= −⟨ψ|
∂ˆρ
∂Wk,l
|ψ⟩"
DESIGN METHODOLOGY,0.2517482517482518,= −⟨ψ|  
DESIGN METHODOLOGY,0.25274725274725274,"Pm
j=1
∂ˆρ1,1"
DESIGN METHODOLOGY,0.25374625374625376,"∂θj
∂θj
∂Wk,l
· · ·
Pm
j=1
∂ˆρ1,N"
DESIGN METHODOLOGY,0.2547452547452547,"∂θj
∂θj
∂Wk,l
...
...
...
Pm
j=1
∂ˆρN,1"
DESIGN METHODOLOGY,0.25574425574425574,"∂θj
∂θj
∂Wk,l
· · ·
Pm
j=1
∂ˆρN,N"
DESIGN METHODOLOGY,0.25674325674325676,"∂θj
∂θj
∂Wk,l "
DESIGN METHODOLOGY,0.25774225774225773,"|ψ⟩,
(1)"
DESIGN METHODOLOGY,0.25874125874125875,"The calculation of
∂θj
∂Wk,l can be easily done on classical devices using backpropagation supported by
201"
DESIGN METHODOLOGY,0.2597402597402597,"automatic differentiation frameworks. Therefore, we only focus on ∂ˆρi,j"
DESIGN METHODOLOGY,0.26073926073926074,"∂θk . In a simulation environ-
202"
DESIGN METHODOLOGY,0.26173826173826176,"ment, the calculation of ˆρ is conducted via noisy quantum circuit simulation, which is essentially a
203"
DESIGN METHODOLOGY,0.2627372627372627,"series of tensor operations on state vectors. Therefore, the calculation of ∂ˆρi,j"
DESIGN METHODOLOGY,0.26373626373626374,"∂θk is compatible with
204"
DESIGN METHODOLOGY,0.2647352647352647,"backpropagation. The situation on real devices becomes more complicated. On real devices, the
205"
DESIGN METHODOLOGY,0.26573426573426573,"mixed state ˆρ is reconstructed through quantum tomography [7] based on classical shadow [55, 16].
206"
DESIGN METHODOLOGY,0.26673326673326675,"Here, for notion simplicity, we denote the process of classical shadow as a transformation S, and
207"
DESIGN METHODOLOGY,0.2677322677322677,"denote the measurement expectations of the ansatz as U(ˆθ). Thus the reconstructed density ma-
208"
DESIGN METHODOLOGY,0.26873126873126874,"trix is given by ˆρ = S(U(ˆθ)). Then the gradient of ˆρi,j with respect to ˆθk is P
u
∂ˆρi,j
∂U(ˆθ)
∂U(ˆθ)"
DESIGN METHODOLOGY,0.26973026973026976,"∂ˆθk .
209"
DESIGN METHODOLOGY,0.2707292707292707,"Here
∂ˆρi,j
∂U(ˆθ) can be efficiently calculated on classical devices using backpropagation, as S operates
210"
DESIGN METHODOLOGY,0.27172827172827174,"on expectation values on classical devices. However, U(ˆθ) involves state evolution on quantum
211"
DESIGN METHODOLOGY,0.2727272727272727,"devices, where back-propagation is impossible due to the No-Cloning theorem [36]. Fortunately,
212"
DESIGN METHODOLOGY,0.27372627372627373,"it is possible to utilize the parameter shift rule [8, 4, 53] to calculate ∂U(ˆθ)"
DESIGN METHODOLOGY,0.27472527472527475,"∂θk . In this way, the
213"
DESIGN METHODOLOGY,0.2757242757242757,gradients of the circuit function U with respect to θj are ∂U(ˆθ)
DESIGN METHODOLOGY,0.27672327672327673,"∂θk
=
1
2 (U(θ+) −U(θ−)), where
214"
DESIGN METHODOLOGY,0.2777222777222777,"θ+ = [θ1, . . . , θk + π"
DESIGN METHODOLOGY,0.2787212787212787,"2 , . . . , θm], θ−= [θ1, . . . , θk −π"
DESIGN METHODOLOGY,0.27972027972027974,"2 , . . . , θm]. To summarize, training SuperEn-
215"
DESIGN METHODOLOGY,0.2807192807192807,"coder with L3 is theoretically feasible on both simulators and real devices.
216"
NUMERICAL RESULTS,0.2817182817182817,"4
Numerical Results
217"
EXPERIMENT SETUP,0.2827172827172827,"4.1
Experiment Setup
218"
EXPERIMENT SETUP,0.2837162837162837,"Datasets. To train a SuperEncoder for arbitrary quantum states, we need a dataset comprising a wide
219"
EXPERIMENT SETUP,0.28471528471528473,"range of quantum states with different distributions. To our knowledge, there is no dataset dedicated
220"
EXPERIMENT SETUP,0.2857142857142857,"for this special purpose. A natural solution is to use readily available datasets from classical machine
221"
EXPERIMENT SETUP,0.2867132867132867,"learning domains (e.g., ImageNet [9], Places [58], SQuAD [44]) by normalizing them to quantum
222"
EXPERIMENT SETUP,0.28771228771228774,"states. However, QSP is essential in various application scenarios besides QML. The classical data to
223"
EXPERIMENT SETUP,0.2887112887112887,"be loaded may not only contain natural images or languages but also contain arbitrary data (e.g., in
224"
EXPERIMENT SETUP,0.2897102897102897,"HHL algorithm [15]). Therefore, we construct a training dataset adapted from FractalDB-60 [21] with
225"
EXPERIMENT SETUP,0.2907092907092907,"60k samples, a formula-driven dataset originally designed for computer vision without any natural
226"
EXPERIMENT SETUP,0.2917082917082917,"images. We also construct a separate dataset to test the performance of QSP, which consists of data
227"
EXPERIMENT SETUP,0.29270729270729273,"sampled from different statistical distributions, including uniform, normal, log-normal, exponential,
228"
EXPERIMENT SETUP,0.2937062937062937,"and Dirichlet distributions, with 3000 samples per distribution. Hereafter we refer this dataset as the
229"
EXPERIMENT SETUP,0.2947052947052947,"synthetic dataset.
230"
EXPERIMENT SETUP,0.2957042957042957,"Platforms. We implement SuperEncoder using PennyLane [34], PyTorch [37] and Qiskit [43].
231"
EXPERIMENT SETUP,0.2967032967032967,"Simulations are done on a Ubuntu server with 768 GB memory, two 32-core Intel(R) Xeon(R) Silver
232"
EXPERIMENT SETUP,0.2977022977022977,"4216 CPU with 2.10 GHz, and 2 NVIDIA A-100 GPUs. IBM quantum cloud platform2 is adopted to
233"
EXPERIMENT SETUP,0.2987012987012987,"evaluate the performance on real quantum devices.
234"
EXPERIMENT SETUP,0.2997002997002997,"Metrics. We evaluate SuperEncoder and compare it to AE and AAE in terms of runtime, scalability,
235"
EXPERIMENT SETUP,0.3006993006993007,"and fidelity. Runtime refers to how long it takes to prepare a quantum state. Scalability refers to how
236"
EXPERIMENT SETUP,0.3016983016983017,"the circuit depth grows with the number of qubits. Fidelity evaluates the similarity between prepared
237"
EXPERIMENT SETUP,0.3026973026973027,"quantum states and target quantum states. Specifically, the fidelity for two mixed states given by
238"
EXPERIMENT SETUP,0.3036963036963037,"density matrices ρ and ˆρ is defined as F(ρ, ˆρ) = Tr
 p√ρˆρ√ρ
2 ∈[0, 1]. A larger F indicates a
239"
EXPERIMENT SETUP,0.3046953046953047,"better fidelity.
240"
EXPERIMENT SETUP,0.30569430569430567,"Implementation. We implement SuperEncoder using an MLP consisting of two hidden layers.
241"
EXPERIMENT SETUP,0.3066933066933067,"The dimensions of input and output layers are respectively set to 2n and m, where n refers to the
242"
EXPERIMENT SETUP,0.3076923076923077,"number of qubits and m refers to the number of parameters. We adopt L3 as the loss function.
243"
EXPERIMENT SETUP,0.3086913086913087,"Training data are down-sampled, flattened, and normalized to 2n-dimensional state vectors. We
244"
EXPERIMENT SETUP,0.3096903096903097,"adopt the hardware efficient ansatz [20] (Fig. 2) as the backbone of quantum circuits and use the
245"
EXPERIMENT SETUP,0.3106893106893107,"same structure for AAE. Given a target state, a pre-trained SuperEncoder model is invoked to
246"
EXPERIMENT SETUP,0.3116883116883117,"generate parameters and thus the circuit for QSP. While for AAE, we employ online iterations for
247"
EXPERIMENT SETUP,0.3126873126873127,"each state. For AE, the arithmetic decomposition method in PennyLane [34, 4] is adopted. We
248"
EXPERIMENT SETUP,0.31368631368631367,"defer more details about implementation to Appendix A. Our framework is open-source at https:
249"
EXPERIMENT SETUP,0.3146853146853147,"//anonymous.4open.science/r/SuperEncoder-A733 with detailed instructions to reproduce
250"
EXPERIMENT SETUP,0.3156843156843157,"our results.
251"
EVALUATION ON SYNTHETIC DATASET,0.3166833166833167,"4.2
Evaluation on Synthetic Dataset
252"
EVALUATION ON SYNTHETIC DATASET,0.3176823176823177,"For simplicity and without loss of generality, we focus our discussion on the results of 4-qubit QSP
253"
EVALUATION ON SYNTHETIC DATASET,0.31868131868131866,"tasks. The outcomes for larger quantum states are detailed in Appendix B.1. The parameters of both
254"
EVALUATION ON SYNTHETIC DATASET,0.3196803196803197,"AAE and SuperEncoder are optimized based on ideal quantum circuit simulation.
255"
EVALUATION ON SYNTHETIC DATASET,0.3206793206793207,"Runtime. The runtime and fidelity results, evaluated on the synthetic dataset, are presented in Table 3.
256"
EVALUATION ON SYNTHETIC DATASET,0.32167832167832167,"We observe that SuperEncoder runs faster than AAE by orders of magnitudes and has a similar
257"
EVALUATION ON SYNTHETIC DATASET,0.3226773226773227,"runtime to AE, affirming that SuperEncoder effectively overcomes the main drawback of AAE.
258"
EVALUATION ON SYNTHETIC DATASET,0.32367632367632365,"AE
AAE
SuperEncoder
Fidelity
Runtime
Fidelity
Runtime
Fidelity
Runtime
Uniform
0.9996
0.9731
Normal
0.9992
0.8201
Log-normal
0.9993
0.9421
Exponential
0.9996
0.9464
Dirichlet
0.9995
0.9737
Average
1.0000
0.0162 s
0.9994
5.0201 s
0.9310
0.0397 s
Table 3: Comparison between AE, AAE and SuperEncoder in terms of runtime and fidelity."
EVALUATION ON SYNTHETIC DATASET,0.3246753246753247,2https://quantum-computing.ibm.com/
EVALUATION ON SYNTHETIC DATASET,0.3256743256743257,"4
6
8
Number of Qubits 0 250 500 750 1000"
EVALUATION ON SYNTHETIC DATASET,0.32667332667332666,Circuit Depth
EVALUATION ON SYNTHETIC DATASET,0.3276723276723277,"AE
AAE/SuperEncoder"
EVALUATION ON SYNTHETIC DATASET,0.32867132867132864,(a) Scaling of circuit depth w.r.t. # qubits.
EVALUATION ON SYNTHETIC DATASET,0.32967032967032966,"4
6
8
Number of Qubits 0.00 0.25 0.50 0.75 1.00"
EVALUATION ON SYNTHETIC DATASET,0.3306693306693307,Fidelity
EVALUATION ON SYNTHETIC DATASET,0.33166833166833165,0.0049
EVALUATION ON SYNTHETIC DATASET,0.33266733266733267,"AE
AAE
SuperEncoder"
EVALUATION ON SYNTHETIC DATASET,0.3336663336663337,(b) Fidelity of different QSP methods on ibm_osaka.
EVALUATION ON SYNTHETIC DATASET,0.33466533466533466,"Figure 7: Comparison between AE, AAE, and SuperEncoder in terms of circuit depth and fidelity on
real devices."
EVALUATION ON SYNTHETIC DATASET,0.3356643356643357,"Scalability. Although AE runs fast, it exhibits poor scalability since the circuit depth grows exponen-
259"
EVALUATION ON SYNTHETIC DATASET,0.33666333666333664,"tially with the number of qubits. The depth of AAE is empirically determined by increasing depth
260"
EVALUATION ON SYNTHETIC DATASET,0.33766233766233766,"until the final fidelity does not increase, same depth is adopted for SuperEncoder. We deter the details
261"
EVALUATION ON SYNTHETIC DATASET,0.3386613386613387,"of determining the depth of AAE/SuperEncoder to Appendix A. As shown in Fig. 7a, the depth of
262"
EVALUATION ON SYNTHETIC DATASET,0.33966033966033965,"AE grows fast and becomes much larger than AAE/SuperEncoder, e.g., the depth of AE for a 8-qubit
263"
EVALUATION ON SYNTHETIC DATASET,0.34065934065934067,"state is 984, whereas the depth of AAE/SuperEncoder is only 120.
264 . . . . . . . . . . . ."
EVALUATION ON SYNTHETIC DATASET,0.34165834165834164,"Encoder Block
U(ϕ0)
U(ϕ1)
U(ϕm)"
EVALUATION ON SYNTHETIC DATASET,0.34265734265734266,"AE
AAE
SuperEncoder
97.15%
98.01%
97.87%"
EVALUATION ON SYNTHETIC DATASET,0.3436563436563437,"Figure 8: Schematic of a QNN (above)
and test accuracies of QSP methods on
the QML task (below)."
EVALUATION ON SYNTHETIC DATASET,0.34465534465534464,"Fidelity. From Table 3, it is evident that SuperEncoder ex-
265"
EVALUATION ON SYNTHETIC DATASET,0.34565434565434566,"periences notable fidelity degradation when compared with
266"
EVALUATION ON SYNTHETIC DATASET,0.34665334665334663,"AAE and AE. Specifically, the average fidelity of SuperEn-
267"
EVALUATION ON SYNTHETIC DATASET,0.34765234765234765,"coder is 0.9307, whereas AAE and AE achieve higher av-
268"
EVALUATION ON SYNTHETIC DATASET,0.34865134865134867,"erage fidelities of 0.9994 and 1.0, respectively. Note that,
269"
EVALUATION ON SYNTHETIC DATASET,0.34965034965034963,"although AE demonstrates the highest fidelity under ideal
270"
EVALUATION ON SYNTHETIC DATASET,0.35064935064935066,"simulation, its performance deteriorates significantly in
271"
EVALUATION ON SYNTHETIC DATASET,0.3516483516483517,"noisy environments. Fig. 7b presents the performance of
272"
EVALUATION ON SYNTHETIC DATASET,0.35264735264735264,"these three QSP methods on quantum states with 4, 6, and
273"
EVALUATION ON SYNTHETIC DATASET,0.35364635364635366,"8 qubits on the ibm_osaka machine. While the fidelity
274"
EVALUATION ON SYNTHETIC DATASET,0.3546453546453546,"of AE is higher than AAE/SuperEncoder on the 4-qubit
275"
EVALUATION ON SYNTHETIC DATASET,0.35564435564435565,"and 6-qubit states, its fidelity on the 8-qubit state is only
276"
EVALUATION ON SYNTHETIC DATASET,0.35664335664335667,"0.0049, becoming much lower than AAE/SuperEncoder.
277"
EVALUATION ON SYNTHETIC DATASET,0.35764235764235763,"This decline is primarily attributed to its large circuit depth as shown in Fig. 7a.
278"
APPLICATION TO DOWNSTREAM TASKS,0.35864135864135865,"4.3
Application to Downstream Tasks
279 q0 : QSP"
APPLICATION TO DOWNSTREAM TASKS,0.3596403596403596,"QPE
QPE inv q1 : q2 : q3 : q4 : R q5 : q6 : q7 : q8 : q9 : a0 : a1 : a2 : q10 :"
APPLICATION TO DOWNSTREAM TASKS,0.36063936063936064,Figure 9: Schematic of HHL.
APPLICATION TO DOWNSTREAM TASKS,0.36163836163836166,"Quantum Machine Learning. We first apply SuperEncoder to
280"
APPLICATION TO DOWNSTREAM TASKS,0.3626373626373626,"a QML task. MNIST dataset is adopted for demonstration, we
281"
APPLICATION TO DOWNSTREAM TASKS,0.36363636363636365,"extract a sub-dataset composed on digits 3 and 6 for evaluation.
282"
APPLICATION TO DOWNSTREAM TASKS,0.3646353646353646,"The quantum circuit that implements a QNN is depicted in Fig. 8,
283"
APPLICATION TO DOWNSTREAM TASKS,0.36563436563436563,"which consists of an encoder block and m entangler layers. Here
284"
APPLICATION TO DOWNSTREAM TASKS,0.36663336663336665,"the encoder block is implemented via QSP circuits, either AE, AAE,
285"
APPLICATION TO DOWNSTREAM TASKS,0.3676323676323676,"or SuperEncoder, of which the parameters are frozen during the
286"
APPLICATION TO DOWNSTREAM TASKS,0.36863136863136864,"training of QNN. The test results are shown in Fig. 8, we observe
287"
APPLICATION TO DOWNSTREAM TASKS,0.3696303696303696,"that SuperEncoder achieves similar performance with AAE and AE.
288"
APPLICATION TO DOWNSTREAM TASKS,0.3706293706293706,"The reason lies in the fact that classification tasks can be robust to
289"
APPLICATION TO DOWNSTREAM TASKS,0.37162837162837165,"noises. Consequently, approximate QSP (AAE and SuperEncoder)
290"
APPLICATION TO DOWNSTREAM TASKS,0.3726273726273726,"with a certain degree of fidelity loss is tolerable.
291"
APPLICATION TO DOWNSTREAM TASKS,0.37362637362637363,"HHL Algorithm. Besides QML, quantum-enhanced linear algebra
292"
APPLICATION TO DOWNSTREAM TASKS,0.37462537462537465,"algorithms are another important set of applications that heavily rely
293"
APPLICATION TO DOWNSTREAM TASKS,0.3756243756243756,"on QSP. The most famous algorithm is the HHL algorithm [15]. The
294"
APPLICATION TO DOWNSTREAM TASKS,0.37662337662337664,"problem can be defined as, given a matrix A ∈CN×N, and a vector b ∈CN, find x ∈CN satisfying
295"
APPLICATION TO DOWNSTREAM TASKS,0.3776223776223776,"Ax = b. A typical implementation of HHL utilizes the circuit depicted in Fig. 9. The outline of
296"
APPLICATION TO DOWNSTREAM TASKS,0.3786213786213786,"HHL is as follows. (i) Apply a QSP circuit to prepare the quantum state |b⟩. (ii) Apply Quantum
297"
APPLICATION TO DOWNSTREAM TASKS,0.37962037962037964,"Phase Estimation [10] (QPE) to estimate the eigenvalue of A (iii) Apply conditioned rotation gates
298"
APPLICATION TO DOWNSTREAM TASKS,0.3806193806193806,"on ancillary qubits based on the eigenvalues (R). (iv) Apply an inverse QPE (QPE_inv) and measure
299"
APPLICATION TO DOWNSTREAM TASKS,0.38161838161838163,"the ancillary qubits to reconstruct the solution vector x. Note that, HHL does not return the solution x
300"
APPLICATION TO DOWNSTREAM TASKS,0.3826173826173826,"itself, but rather an approximation of the expectation value of some operator M associated with x, e.g.,
301"
APPLICATION TO DOWNSTREAM TASKS,0.3836163836163836,"x†Mx. Here, we adopt an optimized version of HHL proposed by Vazquez et al. [51] for evaluation.
302"
APPLICATION TO DOWNSTREAM TASKS,0.38461538461538464,"To compare the performance between different QSP methods, we construct linear equations with
303"
APPLICATION TO DOWNSTREAM TASKS,0.3856143856143856,"fixed matrix A and operator M, while we sample different vectors from our synthetic dataset as b.
304"
APPLICATION TO DOWNSTREAM TASKS,0.3866133866133866,"Results are concluded in Table 4. Unlike QML, HHL expects precise QSP, thus we take the results
305"
APPLICATION TO DOWNSTREAM TASKS,0.3876123876123876,"from AE as the ground truth values and compare the relative error between AAE/SuperEncoder and
306"
APPLICATION TO DOWNSTREAM TASKS,0.3886113886113886,"AE. The relative error of SuperEncoder is 2.4094%, while the error of AAE is only 0.3326%.
307"
DISCUSSION AND FUTURE WORK,0.38961038961038963,"4.4
Discussion and Future Work
308"
DISCUSSION AND FUTURE WORK,0.3906093906093906,"AE
AAE
SuperEncoder
b0
0.7391
0.7404
0.7355
b1
0.7449
0.7445
0.7544
b2
0.7492
0.7469
0.8134
b3
0.7164
0.7099
0.7223
b4
0.7092
0.7076
0.7155
Avg err
0.3326%
2.4094%"
DISCUSSION AND FUTURE WORK,0.3916083916083916,"Table 4: Performance of different QSP
methods in HHL algorithm. ‘Avg err’ de-
notes the average relative errors between
AAE/SuperEncoder and AE."
DISCUSSION AND FUTURE WORK,0.3926073926073926,"The results of our evaluation can be concluded in two folds.
309"
DISCUSSION AND FUTURE WORK,0.3936063936063936,"(i) SuperEncoder effectively eliminates the iteration over-
310"
DISCUSSION AND FUTURE WORK,0.3946053946053946,"head of AAE, thereby becoming both fast and scalable.
311"
DISCUSSION AND FUTURE WORK,0.3956043956043956,"However, it has a notable degradation in fidelity. (ii) The
312"
DISCUSSION AND FUTURE WORK,0.3966033966033966,"impact of fidelity degradation varies across different down-
313"
DISCUSSION AND FUTURE WORK,0.39760239760239763,"stream applications. For QML, the fidelity degradation is
314"
DISCUSSION AND FUTURE WORK,0.3986013986013986,"affordable as long as the prepared states are distinguish-
315"
DISCUSSION AND FUTURE WORK,0.3996003996003996,"able across different classes. However, algorithms like
316"
DISCUSSION AND FUTURE WORK,0.4005994005994006,"HHL rely on precise QSP to produce the best result. In
317"
DISCUSSION AND FUTURE WORK,0.4015984015984016,"these algorithms, SuperEncoder suffers from higher error
318"
DISCUSSION AND FUTURE WORK,0.4025974025974026,"ratio than AAE.
319"
DISCUSSION AND FUTURE WORK,0.4035964035964036,"Note that, the current evaluation results may not reflect the
320"
DISCUSSION AND FUTURE WORK,0.4045954045954046,"actual performance of SuperEncoder on real NISQ devices.
321"
DISCUSSION AND FUTURE WORK,0.40559440559440557,"Recent work has shown that AAE achieves significantly better fidelity than AE does [52]. This is due
322"
DISCUSSION AND FUTURE WORK,0.4065934065934066,"to the intrinsic noise awareness of AAE, as it could obtain states from noisy devices to guide updating
323"
DISCUSSION AND FUTURE WORK,0.4075924075924076,"parameters with better robustness. In essence, the proposed SuperEncoder possesses the same nature
324"
DISCUSSION AND FUTURE WORK,0.4085914085914086,"as AAE. Unfortunately, although the noise-robustness of AAE can be evaluated on a small set of test
325"
DISCUSSION AND FUTURE WORK,0.4095904095904096,"samples, it is difficult to perform noise-aware training for SuperEncoder as it requires a large dataset
326"
DISCUSSION AND FUTURE WORK,0.41058941058941056,"for pre-training. Consequently, SuperEncoder relies on huge amounts of interactions with noisy
327"
DISCUSSION AND FUTURE WORK,0.4115884115884116,"devices, thereby becoming extremely time-consuming. As a result, the effectiveness of SuperEncoder
328"
DISCUSSION AND FUTURE WORK,0.4125874125874126,"in noisy environments remains largely unexplored, which we leave for future exploration. More
329"
DISCUSSION AND FUTURE WORK,0.41358641358641357,"discussion about this perspective is in Appendix C.
330"
RELATED WORK,0.4145854145854146,"5
Related Work
331"
RELATED WORK,0.4155844155844156,"Besides QSP, there are other methods for loading classical data into quantum states. These methods
332"
RELATED WORK,0.4165834165834166,"can be roughly regarded as quantum feature embedding primarily used in QML, which maps classical
333"
RELATED WORK,0.4175824175824176,"data to a completely different distribution encoded in quantum states. A widely used embedding
334"
RELATED WORK,0.41858141858141856,"method is known as angle embedding. Li et al. have proven that this method has a concentration issue,
335"
RELATED WORK,0.4195804195804196,"which means that the encoded states may become indistinguishable as the circuit depth increases [26].
336"
RELATED WORK,0.4205794205794206,"Lei et al. proposed an automatic design framework for efficient quantum feature embedding, resolving
337"
RELATED WORK,0.42157842157842157,"the issue of concentration [24]. The central idea of this framework is to search for the most efficient
338"
RELATED WORK,0.4225774225774226,"circuit architecture for a given classical input, which is also known as Quantum Architecture Search
339"
RELATED WORK,0.42357642357642356,"(QAS) [38, 30, 54]. While the application scenario of quantum feature embedding is largely limited
340"
RELATED WORK,0.4245754245754246,"to QML, QSP has broader usage in general quantum applications, distinguishing SuperEncoder from
341"
RELATED WORK,0.4255744255744256,"all aforementioned work.
342"
CONCLUSION,0.42657342657342656,"6
Conclusion
343"
CONCLUSION,0.4275724275724276,"In this work, we propose SuperEncoder, a neural network-based QSP framework. Instead of iteratively
344"
CONCLUSION,0.42857142857142855,"tuning the circuit parameters to approximate each quantum state, as is done in AAE, we adopt a
345"
CONCLUSION,0.42957042957042957,"different approach by directly learning the relationship between target quantum states and the required
346"
CONCLUSION,0.4305694305694306,"circuit parameters. SuperEncoder combines the scalable circuit architecture of AAE with the fast
347"
CONCLUSION,0.43156843156843155,"runtime of AE, as verified by a comprehensive evaluation on both synthetic dataset and downstream
348"
CONCLUSION,0.4325674325674326,"applications.
349"
REFERENCES,0.43356643356643354,"References
350"
REFERENCES,0.43456543456543456,"[1] Amira Abbas, David Sutter, Christa Zoufal, Aurélien Lucchi, Alessio Figalli, and Stefan
351"
REFERENCES,0.4355644355644356,"Woerner. The power of quantum neural networks. Nature Computational Science, 1(6):403–409,
352"
REFERENCES,0.43656343656343655,"2021.
353"
REFERENCES,0.43756243756243757,"[2] Israel F Araujo, Daniel K Park, Teresa B Ludermir, Wilson R Oliveira, Francesco Petruccione,
354"
REFERENCES,0.4385614385614386,"and Adenilton J Da Silva. Configurable sublinear circuits for quantum state preparation.
355"
REFERENCES,0.43956043956043955,"Quantum Information Processing, 22(2):123, 2023.
356"
REFERENCES,0.4405594405594406,"[3] Johannes Bausch. Recurrent quantum neural networks. Advances in neural information
357"
REFERENCES,0.44155844155844154,"processing systems, 33:1368–1379, 2020.
358"
REFERENCES,0.44255744255744256,"[4] Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, Shahnawaz Ahmed, Vishnu
359"
REFERENCES,0.4435564435564436,"Ajith, M Sohaib Alam, Guillermo Alonso-Linaje, B AkashNarayanan, Ali Asadi, et al. Pen-
360"
REFERENCES,0.44455544455544455,"nylane: Automatic differentiation of hybrid quantum-classical computations. arXiv preprint
361"
REFERENCES,0.44555444555444557,"arXiv:1811.04968, 2018.
362"
REFERENCES,0.44655344655344653,"[5] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth
363"
REFERENCES,0.44755244755244755,"Lloyd. Quantum machine learning. Nature, 549(7671):195–202, 2017.
364"
REFERENCES,0.4485514485514486,"[6] Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural
365"
REFERENCES,0.44955044955044954,"networks with arbitrary activation functions and its application to dynamical systems. IEEE
366"
REFERENCES,0.45054945054945056,"Transactions on Neural Networks, 6(4):911–917, 1995.
367"
REFERENCES,0.4515484515484515,"[7] Marcus Cramer, Martin B Plenio, Steven T Flammia, Rolando Somma, David Gross, Stephen D
368"
REFERENCES,0.45254745254745254,"Bartlett, Olivier Landon-Cardinal, David Poulin, and Yi-Kai Liu. Efficient quantum state
369"
REFERENCES,0.45354645354645357,"tomography. Nature communications, 1(1):149, 2010.
370"
REFERENCES,0.45454545454545453,"[8] Gavin E Crooks. Gradients of parameterized quantum gates using the parameter-shift rule and
371"
REFERENCES,0.45554445554445555,"gate decomposition. arXiv preprint arXiv:1905.13311, 2019.
372"
REFERENCES,0.4565434565434565,"[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
373"
REFERENCES,0.45754245754245754,"scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
374"
REFERENCES,0.45854145854145856,"recognition, pages 248–255. Ieee, 2009.
375"
REFERENCES,0.4595404595404595,"[10] Uwe Dorner, Rafal Demkowicz-Dobrzanski, Brian J Smith, Jeff S Lundeen, Wojciech
376"
REFERENCES,0.46053946053946054,"Wasilewski, Konrad Banaszek, and Ian A Walmsley. Optimal quantum phase estimation.
377"
REFERENCES,0.46153846153846156,"Physical review letters, 102(4):040403, 2009.
378"
REFERENCES,0.46253746253746253,"[11] Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A quantum approximate optimization
379"
REFERENCES,0.46353646353646355,"algorithm. arXiv preprint arXiv:1411.4028, 2014. https://doi.org/10.48550/arXiv.
380"
REFERENCES,0.4645354645354645,"1411.4028.
381"
REFERENCES,0.46553446553446554,"[12] Niels Gleinig and Torsten Hoefler. An efficient algorithm for sparse quantum state preparation.
382"
REFERENCES,0.46653346653346656,"In 2021 58th ACM/IEEE Design Automation Conference (DAC), pages 433–438. IEEE, 2021.
383"
REFERENCES,0.4675324675324675,"[13] Javier Gonzalez-Conde, Ángel Rodríguez-Rozas, Enrique Solano, and Mikel Sanz. Simulating
384"
REFERENCES,0.46853146853146854,"option price dynamics with exponential quantum speedup. arXiv preprint arXiv:2101.04023,
385"
REFERENCES,0.4695304695304695,"2021.
386"
REFERENCES,0.47052947052947053,"[14] Javier Gonzalez-Conde, Thomas W Watts, Pablo Rodriguez-Grasa, and Mikel Sanz. Efficient
387"
REFERENCES,0.47152847152847155,"quantum amplitude encoding of polynomial functions. Quantum, 8:1297, 2024.
388"
REFERENCES,0.4725274725274725,"[15] Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems
389"
REFERENCES,0.47352647352647353,"of equations. Physical Review Letters, 103(15):150502, 2009. https://doi.org/10.1103/
390"
REFERENCES,0.4745254745254745,"PhysRevLett.103.150502.
391"
REFERENCES,0.4755244755244755,"[16] Hsin-Yuan Huang. Learning quantum states from their classical shadows. Nature Reviews
392"
REFERENCES,0.47652347652347654,"Physics, 4(2):81–81, 2022.
393"
REFERENCES,0.4775224775224775,"[17] Jason Iaconis, Sonika Johri, and Elton Yechao Zhu. Quantum state preparation of normal
394"
REFERENCES,0.4785214785214785,"distributions using matrix product states. npj Quantum Information, 10(1):15, 2024.
395"
REFERENCES,0.47952047952047955,"[18] Christian Cmehil-Warn Jacob Hansen. Loss landscapes. In ICLR Blog Track, 2022. https://loss-
396"
REFERENCES,0.4805194805194805,"landscapes.github.io/Loss-Landscapes-Blog/2022/12/01/loss-landscapes/.
397"
REFERENCES,0.48151848151848153,"[19] Weiwen Jiang, Jinjun Xiong, and Yiyu Shi. A co-design framework of neural networks and
398"
REFERENCES,0.4825174825174825,"quantum circuits towards quantum advantage. Nature Communications, 12(1):579, 2021.
399"
REFERENCES,0.4835164835164835,"https://doi.org/10.1038/s41467-020-20729-5.
400"
REFERENCES,0.48451548451548454,"[20] Abhinav Kandala, Antonio Mezzacapo, Kristan Temme, Maika Takita, Markus Brink, Jerry M.
401"
REFERENCES,0.4855144855144855,"Chow, and Jay M. Gambetta. Hardware-efficient variational quantum eigensolver for small
402"
REFERENCES,0.4865134865134865,"molecules and quantum magnets. Nature, 549(7671):242–246, September 2017.
403"
REFERENCES,0.4875124875124875,"[21] Hirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto, Eisuke Yamagata, Ryosuke Yamada,
404"
REFERENCES,0.4885114885114885,"Nakamasa Inoue, Akio Nakamura, and Yutaka Satoh. Pre-training without natural images. In
405"
REFERENCES,0.48951048951048953,"Proceedings of the Asian Conference on Computer Vision, 2020.
406"
REFERENCES,0.4905094905094905,"[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
407"
REFERENCES,0.4915084915084915,"arXiv:1412.6980, 2014.
408"
REFERENCES,0.4925074925074925,"[23] Philip Krantz, Morten Kjaergaard, Fei Yan, Terry P Orlando, Simon Gustavsson, and William D
409"
REFERENCES,0.4935064935064935,"Oliver. A quantum engineer’s guide to superconducting qubits. Applied physics reviews, 6(2),
410"
REFERENCES,0.4945054945054945,"2019.
411"
REFERENCES,0.4955044955044955,"[24] Cong Lei, Yuxuan Du, Peng Mi, Jun Yu, and Tongliang Liu. Neural auto-designer for enhanced
412"
REFERENCES,0.4965034965034965,"quantum kernels. In The Twelfth International Conference on Learning Representations, 2023.
413"
REFERENCES,0.4975024975024975,"[25] Nelson Leung, Mohamed Abdelhafez, Jens Koch, and David Schuster. Speedup for quantum
414"
REFERENCES,0.4985014985014985,"optimal control from automatic differentiation based on graphics processing units. Physical
415"
REFERENCES,0.4995004995004995,"Review A, 95(4):042318, 2017. https://doi.org/10.1103/PhysRevA.95.042318.
416"
REFERENCES,0.5004995004995005,"[26] Guangxi Li, Ruilin Ye, Xuanqiang Zhao, and Xin Wang. Concentration of data encoding in
417"
REFERENCES,0.5014985014985015,"parameterized quantum circuits. Advances in Neural Information Processing Systems, 35:19456–
418"
REFERENCES,0.5024975024975025,"19469, 2022.
419"
REFERENCES,0.5034965034965035,"[27] Gushu Li, Yufei Ding, and Yuan Xie. Tackling the qubit mapping problem for nisq-era quantum
420"
REFERENCES,0.5044955044955045,"devices.
In Proceedings of the Twenty-Fourth International Conference on Architectural
421"
REFERENCES,0.5054945054945055,"Support for Programming Languages and Operating Systems, pages 1001–1014, 2019. https:
422"
REFERENCES,0.5064935064935064,"//doi.org/10.1145/3297858.3304023.
423"
REFERENCES,0.5074925074925075,"[28] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss
424"
REFERENCES,0.5084915084915085,"landscape of neural nets. Advances in neural information processing systems, 31, 2018.
425"
REFERENCES,0.5094905094905094,"[29] Gui-Lu Long and Yang Sun. Efficient scheme for initializing a quantum register with an
426"
REFERENCES,0.5104895104895105,"arbitrary superposed state. Physical Review A, 64(1):014303, 2001.
427"
REFERENCES,0.5114885114885115,"[30] Xudong Lu, Kaisen Pan, Ge Yan, Jiaming Shan, Wenjie Wu, and Junchi Yan. Qas-bench:
428"
REFERENCES,0.5124875124875125,"rethinking quantum architecture search and a benchmark. In International Conference on
429"
REFERENCES,0.5134865134865135,"Machine Learning, pages 22880–22898. PMLR, 2023.
430"
REFERENCES,0.5144855144855145,"[31] Michael Lubasch, Jaewoo Joo, Pierre Moinier, Martin Kiffner, and Dieter Jaksch. Variational
431"
REFERENCES,0.5154845154845155,"quantum algorithms for nonlinear problems. Physical Review A, 101(1):010301, 2020.
432"
REFERENCES,0.5164835164835165,"[32] Rui Mao, Guojing Tian, and Xiaoming Sun. Towards optimal circuit size for sparse quantum
433"
REFERENCES,0.5174825174825175,"state preparation. arXiv e-prints, pages arXiv–2404, 2024.
434"
REFERENCES,0.5184815184815185,"[33] Kosuke Mitarai, Makoto Negoro, Masahiro Kitagawa, and Keisuke Fujii. Quantum circuit
435"
REFERENCES,0.5194805194805194,"learning. Physical Review A, 98(3):032309, 2018.
436"
REFERENCES,0.5204795204795205,"[34] Mikko Möttönen, JJ Vartiainen, Ville Bergholm, and Martti M Salomaa. Transformation of
437"
REFERENCES,0.5214785214785215,"quantum states using uniformly controlled rotations. Quantum Information and Computation, 5,
438"
REFERENCES,0.5224775224775224,"2005.
439"
REFERENCES,0.5234765234765235,"[35] Kouhei Nakaji, Shumpei Uno, Yohichi Suzuki, Rudy Raymond, Tamiya Onodera, Tomoki
440"
REFERENCES,0.5244755244755245,"Tanaka, Hiroyuki Tezuka, Naoki Mitsuda, and Naoki Yamamoto. Approximate amplitude
441"
REFERENCES,0.5254745254745254,"encoding in shallow parameterized quantum circuits and its application to financial market
442"
REFERENCES,0.5264735264735265,"indicators. Physical Review Research, 4(2):023136, 2022.
443"
REFERENCES,0.5274725274725275,"[36] Michael A Nielsen and Isaac L Chuang. Quantum computation and quantum information. 2010.
444"
REFERENCES,0.5284715284715285,"[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
445"
REFERENCES,0.5294705294705294,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
446"
REFERENCES,0.5304695304695305,"style, high-performance deep learning library. Advances in neural information processing
447"
REFERENCES,0.5314685314685315,"systems, 32, 2019.
448"
REFERENCES,0.5324675324675324,"[38] Yash J. Patel, Akash Kundu, Mateusz Ostaszewski, Xavier Bonet-Monroig, Vedran Dunjko,
449"
REFERENCES,0.5334665334665335,"and Onur Danaci. Curriculum reinforcement learning for quantum architecture search under
450"
REFERENCES,0.5344655344655345,"hardware errors. In The Twelfth International Conference on Learning Representations, 2024.
451"
REFERENCES,0.5354645354645354,"[39] Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J
452"
REFERENCES,0.5364635364635365,"Love, Alán Aspuru-Guzik, and Jeremy L O’brien. A variational eigenvalue solver on a photonic
453"
REFERENCES,0.5374625374625375,"quantum processor. Nature communications, 5(1):4213, 2014. https://doi.org/10.1038/
454"
REFERENCES,0.5384615384615384,"ncomms5213.
455"
REFERENCES,0.5394605394605395,"[40] Henning Petzka, Michael Kamp, Linara Adilova, Cristian Sminchisescu, and Mario Boley.
456"
REFERENCES,0.5404595404595405,"Relative flatness and generalization. Advances in neural information processing systems,
457"
REFERENCES,0.5414585414585414,"34:18420–18432, 2021.
458"
REFERENCES,0.5424575424575424,"[41] Martin Plesch and ˇCaslav Brukner. Quantum-state preparation with universal gate decomposi-
459"
REFERENCES,0.5434565434565435,"tions. Physical Review A, 83(3):032302, 2011.
460"
REFERENCES,0.5444555444555444,"[42] John Preskill. Quantum computing in the NISQ era and beyond. Quantum, 2:79, 2018.
461"
REFERENCES,0.5454545454545454,"[43] Qiskit contributors. Qiskit: An open-source framework for quantum computing, 2023.
462"
REFERENCES,0.5464535464535465,"[44] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions
463"
REFERENCES,0.5474525474525475,"for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
464"
REFERENCES,0.5484515484515484,"[45] Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. Prediction by linear regression on a
465"
REFERENCES,0.5494505494505495,"quantum computer. Physical Review A, 94(2):022342, 2016.
466"
REFERENCES,0.5504495504495505,"[46] Vivek V Shende, Stephen S Bullock, and Igor L Markov. Synthesis of quantum logic circuits. In
467"
REFERENCES,0.5514485514485514,"Proceedings of the 2005 Asia and South Pacific Design Automation Conference, pages 272–275,
468"
REFERENCES,0.5524475524475524,"2005.
469"
REFERENCES,0.5534465534465535,"[47] Peter W Shor. Polynomial-time algorithms for prime factorization and discrete logarithms
470"
REFERENCES,0.5544455544455544,"on a quantum computer. SIAM review, 41(2):303–332, 1999. https://doi.org/10.1137/
471"
REFERENCES,0.5554445554445554,"S0036144598347011.
472"
REFERENCES,0.5564435564435565,"[48] Siddarth Srinivasan, Carlton Downey, and Byron Boots. Learning and inference in hilbert space
473"
REFERENCES,0.5574425574425574,"with quantum graphical models. Advances in Neural Information Processing Systems, 31, 2018.
474"
REFERENCES,0.5584415584415584,"[49] Xiaoming Sun, Guojing Tian, Shuai Yang, Pei Yuan, and Shengyu Zhang. Asymptotically
475"
REFERENCES,0.5594405594405595,"optimal circuit depth for quantum state preparation and general unitary synthesis.
IEEE
476"
REFERENCES,0.5604395604395604,"Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2023.
477"
REFERENCES,0.5614385614385614,"[50] Jinkai Tian, Xiaoyu Sun, Yuxuan Du, Shanshan Zhao, Qing Liu, Kaining Zhang, Wei Yi, Wan-
478"
REFERENCES,0.5624375624375625,"rong Huang, Chaoyue Wang, Xingyao Wu, et al. Recent advances for quantum neural networks
479"
REFERENCES,0.5634365634365635,"in generative learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.
480"
REFERENCES,0.5644355644355644,"[51] Almudena Carrera Vazquez, Ralf Hiptmair, and Stefan Woerner. Enhancing the quantum linear
481"
REFERENCES,0.5654345654345654,"systems algorithm using richardson extrapolation. ACM Transactions on Quantum Computing,
482"
REFERENCES,0.5664335664335665,"3(1):1–37, 2022.
483"
REFERENCES,0.5674325674325674,"[52] Hanrui Wang, Yilian Liu, Pengyu Liu, Jiaqi Gu, Zirui Li, Zhiding Liang, Jinglei Cheng,
484"
REFERENCES,0.5684315684315684,"Yongshan Ding, Xuehai Qian, Yiyu Shi, et al. Robuststate: Boosting fidelity of quantum state
485"
REFERENCES,0.5694305694305695,"preparation via noise-aware variational training. arXiv preprint arXiv:2311.16035, 2023.
486"
REFERENCES,0.5704295704295704,"[53] David Wierichs, Josh Izaac, Cody Wang, and Cedric Yen-Yu Lin. General parameter-shift rules
487"
REFERENCES,0.5714285714285714,"for quantum gradients. Quantum, 6:677, 2022.
488"
REFERENCES,0.5724275724275725,"[54] Wenjie Wu, Ge Yan, Xudong Lu, Kaisen Pan, and Junchi Yan. Quantumdarts: differentiable
489"
REFERENCES,0.5734265734265734,"quantum architecture search for variational quantum algorithms. In International Conference
490"
REFERENCES,0.5744255744255744,"on Machine Learning, pages 37745–37764. PMLR, 2023.
491"
REFERENCES,0.5754245754245755,"[55] Ting Zhang, Jinzhao Sun, Xiao-Xu Fang, Xiao-Ming Zhang, Xiao Yuan, and He Lu. Ex-
492"
REFERENCES,0.5764235764235764,"perimental quantum state measurement with classical shadows.
Physical Review Letters,
493"
REFERENCES,0.5774225774225774,"127(20):200501, 2021.
494"
REFERENCES,0.5784215784215784,"[56] Xiao-Ming Zhang, Man-Hong Yung, and Xiao Yuan. Low-depth quantum state preparation.
495"
REFERENCES,0.5794205794205795,"Physical Review Research, 3(4):043200, 2021.
496"
REFERENCES,0.5804195804195804,"[57] Jian Zhao, Yu-Chun Wu, Guang-Can Guo, and Guo-Ping Guo. State preparation based on
497"
REFERENCES,0.5814185814185814,"quantum phase estimation. arXiv preprint arXiv:1912.05335, 2019.
498"
REFERENCES,0.5824175824175825,"[58] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A
499"
REFERENCES,0.5834165834165834,"10 million image database for scene recognition. IEEE transactions on pattern analysis and
500"
REFERENCES,0.5844155844155844,"machine intelligence, 40(6):1452–1464, 2017.
501"
REFERENCES,0.5854145854145855,"[59] Christa Zoufal, Aurélien Lucchi, and Stefan Woerner. Quantum generative adversarial networks
502"
REFERENCES,0.5864135864135864,"for learning and loading random distributions. npj Quantum Information, 5(1):103, 2019.
503"
REFERENCES,0.5874125874125874,"The structure of our Appendix is as follows. Appendix A provides more details of implementing
504"
REFERENCES,0.5884115884115884,"SuperEncoder. Appendix B provides additional numerical results to illustrate the impact of state
505"
REFERENCES,0.5894105894105894,"sizes, model architectures, and training datasets. Appendix C analyzes the estimated runtime of
506"
REFERENCES,0.5904095904095904,"training SuperEncoder on real devices.
507"
REFERENCES,0.5914085914085914,"A
Implementation Details
508"
REFERENCES,0.5924075924075924,"In this section, we elaborate the missing details of SuperEncoder in the main text.
509"
REFERENCES,0.5934065934065934,"The overarching workflow of SuperEncoder is illustrated in Fig. 10. The target quantum states are
510"
REFERENCES,0.5944055944055944,"input to the MLP model. Then, the MLP model generates predicted parameters based on the target
511"
REFERENCES,0.5954045954045954,"states. Afterwards, the parameters are applied to the PQC to obtain the prepared quantum states.
512"
REFERENCES,0.5964035964035964,"Finally, we calculate the loss based on the prepared states and target states and optimize the weights
513"
REFERENCES,0.5974025974025974,"of MLP through backpropagation.
514"
REFERENCES,0.5984015984015985,Target State MLP
REFERENCES,0.5994005994005994,Circuit Parameters PQC
REFERENCES,0.6003996003996004,Prepared State Loss
REFERENCES,0.6013986013986014,Figure 10: Detailed workflow of SuperEncoder.
REFERENCES,0.6023976023976024,"The settings of MLP and PQC are as follows.
515"
REFERENCES,0.6033966033966034,"MLP. As listed in Table 5, we implement a two-layer MLP. Each layer consists of 512 neurons. We
516"
REFERENCES,0.6043956043956044,"employ Tanh as the activation functions since θ represents the angles of rotation gates, ranging from
517"
REFERENCES,0.6053946053946054,"−π to π.
518"
REFERENCES,0.6063936063936064,"Linear
Input
(batch_size, 2n)
Output
(batch_size, 512)"
REFERENCES,0.6073926073926074,"Tanh
Input
(batch_size, 512)
Output
(batch_size, 512)"
REFERENCES,0.6083916083916084,"Linear
Input
(batch_size, 512)
Output
(batch_size, dim(θ))"
REFERENCES,0.6093906093906094,"Tanh
Input
(batch_size, dim(θ))
Output
(batch_size, dim(θ))
Table 5: MLP based SuperEncoder. n refers to the number of qubits. θ denotes the parameter vector."
REFERENCES,0.6103896103896104,"PQC. The circuit structure is the same with the one depicted in Fig. 2, except that the number of
519"
REFERENCES,0.6113886113886113,"blocks is determined dynamically through empirical examinations. Specifically, we utilize AAE to
520"
REFERENCES,0.6123876123876124,"approximate a target state while increasing the number of blocks. The number of blocks is designated
521"
REFERENCES,0.6133866133866134,"when the resulting state fidelity no longer increases. For example, Fig. 11 demonstrates how fidelity
522"
REFERENCES,0.6143856143856143,"changes while increasing the number of blocks. As one can observe, the fidelity converges when the
523"
REFERENCES,0.6153846153846154,"number of layers is larger than 8. Hence, the number of layers is set to be 8 for 4-qubit quantum
524"
REFERENCES,0.6163836163836164,"states. We follow the same procedure to set the number of blocks for other state sizes. Each block
525"
REFERENCES,0.6173826173826173,"has the same structure, consisting of a rotation layer and an entangler layer. Given an n-qubit system,
526"
REFERENCES,0.6183816183816184,"a rotation layer comprises n Ry gates, each operating on a distinct qubit. The entangler layer is
527"
REFERENCES,0.6193806193806194,"composed of two CNOT layers. The first CNOT layer applies CNOT gates to {(q0, q1), (q2, q3), . . . },
528"
REFERENCES,0.6203796203796204,"and the second CNOT layer applies CNOT gates to {(q1, q2), (q3, q4), . . . }. Hence, the depth of
529"
REFERENCES,0.6213786213786214,"a block is 3. Let l be the number of blocks; then the dimension of the parameter vector is given
530"
REFERENCES,0.6223776223776224,"by dim(θ) = n × l, and the depth of AAE/SuperEncoder is 3 × l. We conclude the settings of
531"
REFERENCES,0.6233766233766234,"AAE/SuperEncoder used throughout this study in Table 6.
532"
REFERENCES,0.6243756243756243,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Number of Blocks 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6253746253746254,Fidelity
REFERENCES,0.6263736263736264,Figure 11: Fidelity vs. # blocks for 4-qubit states using AAE.
REFERENCES,0.6273726273726273,"Number of Qubits
4
6
8
Number of Blocks
8
20
40
Depth
24
60
120
Table 6: Number of blocks and corresponding depth of AAE/SuperEncoder."
REFERENCES,0.6283716283716284,"The hyperparameters for training SuperEncoder and optimizing AAE are as follows.
533"
REFERENCES,0.6293706293706294,"Training Hyperparameters for SuperEncoder. Throughout our experiments, the number of epochs
534"
REFERENCES,0.6303696303696303,"are consistently set to be 10. For 4-qubit states, we set bath_size to 32, while we set it 64 for
535"
REFERENCES,0.6313686313686314,"6-qubit and 8-qubit states. We adopt Adam optimizer [22] with a learning rate of 3e-3 and a weight
536"
REFERENCES,0.6323676323676324,"decay of 1e-5.
537"
REFERENCES,0.6333666333666333,"Hyperparameters for AAE. To optimize the parameters of AAE, we also use the Adam optimizer,
538"
REFERENCES,0.6343656343656343,"with a learning rate of 1e-2 and zero weight decay. For all quantum states, we train the AAE for 100
539"
REFERENCES,0.6353646353646354,"steps.
540"
REFERENCES,0.6363636363636364,"B
More Numerical Results
541"
REFERENCES,0.6373626373626373,"B.1
Results on Larger Quantum States
542"
REFERENCES,0.6383616383616384,"In line with the main text, we train the SuperEncoder for 6-qubit and 8-qubit quantum states using
543"
REFERENCES,0.6393606393606394,"FractalDB-60 as the training dataset. Then we evaluate the performance of SuperEncoder on the
544"
REFERENCES,0.6403596403596403,"synthetic test datasets. As shown in Table 7, the average fidelity on 6-qubit and 8-qubit states are
545"
REFERENCES,0.6413586413586414,"0.8655 and 0.7624 respectively. In Appendix B.2, B.3, we discuss potential optimizations to alleviate
546"
REFERENCES,0.6423576423576424,"this performance degradation.
547"
REFERENCES,0.6433566433566433,"Dataset
n = 4
n = 6
n = 8
Uniform
0.9731
0.9254
0.8648
Normal
0.8201
0.7457
0.6075
Log-normal
0.9421
0.8575
0.7122
Exponential
0.9464
0.8757
0.7613
Dirichlet
0.9737
0.9232
0.8663
Avg
0.9310
0.8655
0.7624
Avg-AAE
0.9994
0.9964
0.9910
Table 7: Performance evaluation on larger quantum states (6-qubit and 8-qubit). The last separate
row shows the results of AAE for comparison."
REFERENCES,0.6443556443556444,"B.2
Impact of Model Architecture
548"
REFERENCES,0.6453546453546454,"As a preliminary investigation, the optimal model architecture for SuperEncoder still requires further
549"
REFERENCES,0.6463536463536463,"exploration. Currently, we have set the size of the hidden units at a constant 512 (Table 5). However,
550"
REFERENCES,0.6473526473526473,"as the number of qubits, n, increases, a wider network architecture may become necessary. To
551"
REFERENCES,0.6483516483516484,"showcase the impact of model width, we adjust the size to 4 × 2n for 6-qubit states and 16 × 2n for
552"
REFERENCES,0.6493506493506493,"8-qubit states, and compare their performance with the original settings, as shown in Table 8. As
553"
REFERENCES,0.6503496503496503,"evident from the results, this simple adjustment significantly enhances the fidelity of SuperEncoder,
554"
REFERENCES,0.6513486513486514,"suggesting that there is substantial potential to boost SuperEncoder’s performance by developing a
555"
REFERENCES,0.6523476523476524,"more tailored network architecture.
556"
REFERENCES,0.6533466533466533,"n = 6
n = 8
Dataset
h = 512
h = 4 × 26
h = 512
h = 16×28"
REFERENCES,0.6543456543456544,"Uniform
0.9254
0.9267
0.8648
0.8821
Normal
0.7457
0.7580
0.6075
0.6401
Log-normal
0.8575
0.8608
0.7122
0.7294
Exponential
0.8757
0.8732
0.7613
0.7781
Dirichlet
0.9232
0.9261
0.8663
0.8805
Avg
0.8655
0.8690
0.7624
0.7820
Table 8: Impact of increasing network width. Here h refers to the size of hidden units."
REFERENCES,0.6553446553446554,"B.3
Impact of Training Datasets
557"
REFERENCES,0.6563436563436563,"In addition to refining the model architecture, the development of a specially designed dataset for
558"
REFERENCES,0.6573426573426573,"pre-training SuperEncoder is essential. Currently, the dataset utilized is FractalDB [21], which is
559"
REFERENCES,0.6583416583416584,"originally designed for computer vision tasks. However, given the wide range of applications of QSP,
560"
REFERENCES,0.6593406593406593,"there is a need to accommodate diverse types of classical data from various domains. Therefore, how
561"
REFERENCES,0.6603396603396603,"to create a comprehensive dataset that could fully unleash the potential of SuperEncoder remains an
562"
REFERENCES,0.6613386613386614,"open question. While developing a pre-trained model that performs well in all kinds of applications
563"
REFERENCES,0.6623376623376623,"may be challenging, we advocate for a strategy that combines pre-training with fine-tuning for the
564"
REFERENCES,0.6633366633366633,"practical deployment of SuperEncoder, similar to the approach used with foundation models in
565"
REFERENCES,0.6643356643356644,"classical machine learning. To substantiate this approach, we have compiled a separate dataset that
566"
REFERENCES,0.6653346653346653,"encompasses a variety of statistical distributions not limited to those utilized for evaluation (but with
567"
REFERENCES,0.6663336663336663,"different settings). As demonstrated in Table 9, after fine-tuning, the performance of SuperEncoder
568"
REFERENCES,0.6673326673326674,"improves by approximately 0.03.
569"
REFERENCES,0.6683316683316683,"Dataset
Pre-training
Pre-training+Finetuning
Uniform
0.9731
0.9909
Normal
0.8201
0.8879
Log-normal
0.9421
0.9717
Exponential
0.9464
0.9729
Dirichlet
0.9737
0.9903
Avg
0.9310
0.9627
Table 9: Fidelity improvements after fine-tuning SuperEncoder using a dataset consisting of different
distributions."
REFERENCES,0.6693306693306693,"C
Runtime Estimation for Training on Real Devices
570"
REFERENCES,0.6703296703296703,"Although we have theoretically analyzed the feasibility of training SuperEncoder using states from
571"
REFERENCES,0.6713286713286714,"real devices (Section 3.2), its practical implementation poses significant challenges. Specifically,
572"
REFERENCES,0.6723276723276723,"state-of-the-art quantum tomography techniques, such as classical shadow [55, 16], require numerous
573"
REFERENCES,0.6733266733266733,"snapshots, each measuring a distinct observable.
574"
REFERENCES,0.6743256743256744,"To train SuperEncoder, each sample in the training dataset necessitates one classical shadow to obtain
575"
REFERENCES,0.6753246753246753,"the prepared state. For instance, with the FractalDB-60 dataset, one training epoch requires 60,000
576"
REFERENCES,0.6763236763236763,"classical shadows. Our experiments on the IBM cloud platform reveal an average runtime of 3.02
577"
REFERENCES,0.6773226773226774,"seconds per circuit job excluding queuing time. Suppose the number of snapshots is 1000, then the
578"
REFERENCES,0.6783216783216783,"total runtime to train SuperEncoder for 10 epochs is about 1,812,000,000 seconds3, roughly 57 years,
579"
REFERENCES,0.6793206793206793,"making the process prohibitively expensive and time-consuming.
580"
REFERENCES,0.6803196803196803,"However, quantum tomography is under active investigation, and we expect more efficient techniques
581"
REFERENCES,0.6813186813186813,"to emerge for acquiring noisy quantum states from real devices. Additionally, with the advancement
582"
REFERENCES,0.6823176823176823,"of quantum computing system, future systems may have tightly integrated quantum-classical hetero-
583"
REFERENCES,0.6833166833166833,"geneous architectures (shorter runtime per job) while being capable of executing numerous quantum
584"
REFERENCES,0.6843156843156843,"circuits in parallel (jobs within a classical shadow can execute in parallel). Hence, we anticipate the
585"
REFERENCES,0.6853146853146853,"training of SuperEncoder to be feasible in the future.
586"
REFERENCES,0.6863136863136863,310 × 1000 × 60000 × 3.02
REFERENCES,0.6873126873126874,"NeurIPS Paper Checklist
587"
CLAIMS,0.6883116883116883,"1. Claims
588"
CLAIMS,0.6893106893106893,"Question: Do the main claims made in the abstract and introduction accurately reflect the
589"
CLAIMS,0.6903096903096904,"paper’s contributions and scope?
590"
CLAIMS,0.6913086913086913,"Answer: [Yes]
591"
CLAIMS,0.6923076923076923,"Justification: This work aims at training-free approximate quantum state preparation. As
592"
CLAIMS,0.6933066933066933,"claimed in the abstract and introduction.
593"
CLAIMS,0.6943056943056943,"Guidelines:
594"
CLAIMS,0.6953046953046953,"• The answer NA means that the abstract and introduction do not include the claims
595"
CLAIMS,0.6963036963036963,"made in the paper.
596"
CLAIMS,0.6973026973026973,"• The abstract and/or introduction should clearly state the claims made, including the
597"
CLAIMS,0.6983016983016983,"contributions made in the paper and important assumptions and limitations. A No or
598"
CLAIMS,0.6993006993006993,"NA answer to this question will not be perceived well by the reviewers.
599"
CLAIMS,0.7002997002997003,"• The claims made should match theoretical and experimental results, and reflect how
600"
CLAIMS,0.7012987012987013,"much the results can be expected to generalize to other settings.
601"
CLAIMS,0.7022977022977023,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
602"
CLAIMS,0.7032967032967034,"are not attained by the paper.
603"
LIMITATIONS,0.7042957042957043,"2. Limitations
604"
LIMITATIONS,0.7052947052947053,"Question: Does the paper discuss the limitations of the work performed by the authors?
605"
LIMITATIONS,0.7062937062937062,"Answer: [Yes]
606"
LIMITATIONS,0.7072927072927073,"Justification: SuperEncoder sacrifices fidelity, as discussed in Section 4.4.
607"
LIMITATIONS,0.7082917082917083,"Guidelines:
608"
LIMITATIONS,0.7092907092907093,"• The answer NA means that the paper has no limitation while the answer No means that
609"
LIMITATIONS,0.7102897102897103,"the paper has limitations, but those are not discussed in the paper.
610"
LIMITATIONS,0.7112887112887113,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
611"
LIMITATIONS,0.7122877122877123,"• The paper should point out any strong assumptions and how robust the results are to
612"
LIMITATIONS,0.7132867132867133,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
613"
LIMITATIONS,0.7142857142857143,"model well-specification, asymptotic approximations only holding locally). The authors
614"
LIMITATIONS,0.7152847152847153,"should reflect on how these assumptions might be violated in practice and what the
615"
LIMITATIONS,0.7162837162837162,"implications would be.
616"
LIMITATIONS,0.7172827172827173,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
617"
LIMITATIONS,0.7182817182817183,"only tested on a few datasets or with a few runs. In general, empirical results often
618"
LIMITATIONS,0.7192807192807192,"depend on implicit assumptions, which should be articulated.
619"
LIMITATIONS,0.7202797202797203,"• The authors should reflect on the factors that influence the performance of the approach.
620"
LIMITATIONS,0.7212787212787213,"For example, a facial recognition algorithm may perform poorly when image resolution
621"
LIMITATIONS,0.7222777222777222,"is low or images are taken in low lighting. Or a speech-to-text system might not be
622"
LIMITATIONS,0.7232767232767233,"used reliably to provide closed captions for online lectures because it fails to handle
623"
LIMITATIONS,0.7242757242757243,"technical jargon.
624"
LIMITATIONS,0.7252747252747253,"• The authors should discuss the computational efficiency of the proposed algorithms
625"
LIMITATIONS,0.7262737262737263,"and how they scale with dataset size.
626"
LIMITATIONS,0.7272727272727273,"• If applicable, the authors should discuss possible limitations of their approach to
627"
LIMITATIONS,0.7282717282717283,"address problems of privacy and fairness.
628"
LIMITATIONS,0.7292707292707292,"• While the authors might fear that complete honesty about limitations might be used by
629"
LIMITATIONS,0.7302697302697303,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
630"
LIMITATIONS,0.7312687312687313,"limitations that aren’t acknowledged in the paper. The authors should use their best
631"
LIMITATIONS,0.7322677322677322,"judgment and recognize that individual actions in favor of transparency play an impor-
632"
LIMITATIONS,0.7332667332667333,"tant role in developing norms that preserve the integrity of the community. Reviewers
633"
LIMITATIONS,0.7342657342657343,"will be specifically instructed to not penalize honesty concerning limitations.
634"
THEORY ASSUMPTIONS AND PROOFS,0.7352647352647352,"3. Theory Assumptions and Proofs
635"
THEORY ASSUMPTIONS AND PROOFS,0.7362637362637363,"Question: For each theoretical result, does the paper provide the full set of assumptions and
636"
THEORY ASSUMPTIONS AND PROOFS,0.7372627372627373,"a complete (and correct) proof?
637"
THEORY ASSUMPTIONS AND PROOFS,0.7382617382617382,"Answer: [Yes]
638"
THEORY ASSUMPTIONS AND PROOFS,0.7392607392607392,"Justification: All these necessary contents for theoretical results are included in Section 3.2.
639"
THEORY ASSUMPTIONS AND PROOFS,0.7402597402597403,"Guidelines:
640"
THEORY ASSUMPTIONS AND PROOFS,0.7412587412587412,"• The answer NA means that the paper does not include theoretical results.
641"
THEORY ASSUMPTIONS AND PROOFS,0.7422577422577422,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
642"
THEORY ASSUMPTIONS AND PROOFS,0.7432567432567433,"referenced.
643"
THEORY ASSUMPTIONS AND PROOFS,0.7442557442557443,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
644"
THEORY ASSUMPTIONS AND PROOFS,0.7452547452547452,"• The proofs can either appear in the main paper or the supplemental material, but if
645"
THEORY ASSUMPTIONS AND PROOFS,0.7462537462537463,"they appear in the supplemental material, the authors are encouraged to provide a short
646"
THEORY ASSUMPTIONS AND PROOFS,0.7472527472527473,"proof sketch to provide intuition.
647"
THEORY ASSUMPTIONS AND PROOFS,0.7482517482517482,"• Inversely, any informal proof provided in the core of the paper should be complemented
648"
THEORY ASSUMPTIONS AND PROOFS,0.7492507492507493,"by formal proofs provided in appendix or supplemental material.
649"
THEORY ASSUMPTIONS AND PROOFS,0.7502497502497503,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7512487512487512,"4. Experimental Result Reproducibility
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7522477522477522,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7532467532467533,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7542457542457542,"of the paper (regardless of whether the code and data are provided or not)?
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7552447552447552,"Answer: [Yes]
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7562437562437563,"Justification: Our code is open-source with instructions to reproduce our results, as described
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7572427572427572,"in Section 4.1. We also describe the details of experiment settings in Appendix A.
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7582417582417582,"Guidelines:
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7592407592407593,"• The answer NA means that the paper does not include experiments.
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7602397602397603,"• If the paper includes experiments, a No answer to this question will not be perceived
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7612387612387612,"well by the reviewers: Making the paper reproducible is important, regardless of
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7622377622377622,"whether the code and data are provided or not.
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7632367632367633,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7642357642357642,"to make their results reproducible or verifiable.
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7652347652347652,"• Depending on the contribution, reproducibility can be accomplished in various ways.
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7662337662337663,"For example, if the contribution is a novel architecture, describing the architecture fully
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7672327672327672,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7682317682317682,"be necessary to either make it possible for others to replicate the model with the same
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7692307692307693,"dataset, or provide access to the model. In general. releasing code and data is often
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7702297702297702,"one good way to accomplish this, but reproducibility can also be provided via detailed
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7712287712287712,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7722277722277723,"of a large language model), releasing of a model checkpoint, or other means that are
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7732267732267732,"appropriate to the research performed.
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7742257742257742,"• While NeurIPS does not require releasing code, the conference does require all submis-
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7752247752247752,"sions to provide some reasonable avenue for reproducibility, which may depend on the
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7762237762237763,"nature of the contribution. For example
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7772227772227772,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7782217782217782,"to reproduce that algorithm.
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7792207792207793,"(b) If the contribution is primarily a new model architecture, the paper should describe
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7802197802197802,"the architecture clearly and fully.
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7812187812187812,"(c) If the contribution is a new model (e.g., a large language model), then there should
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7822177822177823,"either be a way to access this model for reproducing the results or a way to reproduce
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7832167832167832,"the model (e.g., with an open-source dataset or instructions for how to construct
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7842157842157842,"the dataset).
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852147852147852,"(d) We recognize that reproducibility may be tricky in some cases, in which case
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7862137862137862,"authors are welcome to describe the particular way they provide for reproducibility.
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7872127872127872,"In the case of closed-source models, it may be that access to the model is limited in
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7882117882117882,"some way (e.g., to registered users), but it should be possible for other researchers
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7892107892107892,"to have some path to reproducing or verifying the results.
689"
OPEN ACCESS TO DATA AND CODE,0.7902097902097902,"5. Open access to data and code
690"
OPEN ACCESS TO DATA AND CODE,0.7912087912087912,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
691"
OPEN ACCESS TO DATA AND CODE,0.7922077922077922,"tions to faithfully reproduce the main experimental results, as described in supplemental
692"
OPEN ACCESS TO DATA AND CODE,0.7932067932067932,"material?
693"
OPEN ACCESS TO DATA AND CODE,0.7942057942057942,"Answer: [Yes]
694"
OPEN ACCESS TO DATA AND CODE,0.7952047952047953,"Justification: See Section 4.1.
695"
OPEN ACCESS TO DATA AND CODE,0.7962037962037962,"Guidelines:
696"
OPEN ACCESS TO DATA AND CODE,0.7972027972027972,"• The answer NA means that paper does not include experiments requiring code.
697"
OPEN ACCESS TO DATA AND CODE,0.7982017982017982,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
698"
OPEN ACCESS TO DATA AND CODE,0.7992007992007992,"public/guides/CodeSubmissionPolicy) for more details.
699"
OPEN ACCESS TO DATA AND CODE,0.8001998001998002,"• While we encourage the release of code and data, we understand that this might not be
700"
OPEN ACCESS TO DATA AND CODE,0.8011988011988012,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
701"
OPEN ACCESS TO DATA AND CODE,0.8021978021978022,"including code, unless this is central to the contribution (e.g., for a new open-source
702"
OPEN ACCESS TO DATA AND CODE,0.8031968031968032,"benchmark).
703"
OPEN ACCESS TO DATA AND CODE,0.8041958041958042,"• The instructions should contain the exact command and environment needed to run to
704"
OPEN ACCESS TO DATA AND CODE,0.8051948051948052,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
705"
OPEN ACCESS TO DATA AND CODE,0.8061938061938062,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
706"
OPEN ACCESS TO DATA AND CODE,0.8071928071928072,"• The authors should provide instructions on data access and preparation, including how
707"
OPEN ACCESS TO DATA AND CODE,0.8081918081918081,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
708"
OPEN ACCESS TO DATA AND CODE,0.8091908091908092,"• The authors should provide scripts to reproduce all experimental results for the new
709"
OPEN ACCESS TO DATA AND CODE,0.8101898101898102,"proposed method and baselines. If only a subset of experiments are reproducible, they
710"
OPEN ACCESS TO DATA AND CODE,0.8111888111888111,"should state which ones are omitted from the script and why.
711"
OPEN ACCESS TO DATA AND CODE,0.8121878121878122,"• At submission time, to preserve anonymity, the authors should release anonymized
712"
OPEN ACCESS TO DATA AND CODE,0.8131868131868132,"versions (if applicable).
713"
OPEN ACCESS TO DATA AND CODE,0.8141858141858141,"• Providing as much information as possible in supplemental material (appended to the
714"
OPEN ACCESS TO DATA AND CODE,0.8151848151848152,"paper) is recommended, but including URLs to data and code is permitted.
715"
OPEN ACCESS TO DATA AND CODE,0.8161838161838162,"6. Experimental Setting/Details
716"
OPEN ACCESS TO DATA AND CODE,0.8171828171828172,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
717"
OPEN ACCESS TO DATA AND CODE,0.8181818181818182,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
718"
OPEN ACCESS TO DATA AND CODE,0.8191808191808192,"results?
719"
OPEN ACCESS TO DATA AND CODE,0.8201798201798202,"Answer: [Yes]
720"
OPEN ACCESS TO DATA AND CODE,0.8211788211788211,"Justification: We illustrate the experimental settings in Section 4.1, and provides additional
721"
OPEN ACCESS TO DATA AND CODE,0.8221778221778222,"details in Appendix A.
722"
OPEN ACCESS TO DATA AND CODE,0.8231768231768232,"Guidelines:
723"
OPEN ACCESS TO DATA AND CODE,0.8241758241758241,"• The answer NA means that the paper does not include experiments.
724"
OPEN ACCESS TO DATA AND CODE,0.8251748251748252,"• The experimental setting should be presented in the core of the paper to a level of detail
725"
OPEN ACCESS TO DATA AND CODE,0.8261738261738262,"that is necessary to appreciate the results and make sense of them.
726"
OPEN ACCESS TO DATA AND CODE,0.8271728271728271,"• The full details can be provided either with the code, in appendix, or as supplemental
727"
OPEN ACCESS TO DATA AND CODE,0.8281718281718282,"material.
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8291708291708292,"7. Experiment Statistical Significance
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8301698301698301,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8311688311688312,"information about the statistical significance of the experiments?
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8321678321678322,"Answer: [No]
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331668331668332,"Justification: Throughout our experiments, we set the random seed to be fixed for all libraries
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8341658341658341,"we used.
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8351648351648352,"Guidelines:
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8361638361638362,"• The answer NA means that the paper does not include experiments.
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8371628371628371,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8381618381618382,"dence intervals, or statistical significance tests, at least for the experiments that support
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8391608391608392,"the main claims of the paper.
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8401598401598401,"• The factors of variability that the error bars are capturing should be clearly stated (for
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8411588411588412,"example, train/test split, initialization, random drawing of some parameter, or overall
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8421578421578422,"run with given experimental conditions).
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8431568431568431,"• The method for calculating the error bars should be explained (closed form formula,
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8441558441558441,"call to a library function, bootstrap, etc.)
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8451548451548452,"• The assumptions made should be given (e.g., Normally distributed errors).
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8461538461538461,"• It should be clear whether the error bar is the standard deviation or the standard error
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8471528471528471,"of the mean.
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8481518481518482,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8491508491508492,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8501498501498501,"of Normality of errors is not verified.
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8511488511488512,"• For asymmetric distributions, the authors should be careful not to show in tables or
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8521478521478522,"figures symmetric error bars that would yield results that are out of range (e.g. negative
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8531468531468531,"error rates).
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8541458541458542,"• If error bars are reported in tables or plots, The authors should explain in the text how
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8551448551448552,"they were calculated and reference the corresponding figures or tables in the text.
755"
EXPERIMENTS COMPUTE RESOURCES,0.8561438561438561,"8. Experiments Compute Resources
756"
EXPERIMENTS COMPUTE RESOURCES,0.8571428571428571,"Question: For each experiment, does the paper provide sufficient information on the com-
757"
EXPERIMENTS COMPUTE RESOURCES,0.8581418581418582,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
758"
EXPERIMENTS COMPUTE RESOURCES,0.8591408591408591,"the experiments?
759"
EXPERIMENTS COMPUTE RESOURCES,0.8601398601398601,"Answer: [Yes]
760"
EXPERIMENTS COMPUTE RESOURCES,0.8611388611388612,"Justification: We describe the computer resources used in this paper in Section 4.1.
761"
EXPERIMENTS COMPUTE RESOURCES,0.8621378621378621,"Guidelines:
762"
EXPERIMENTS COMPUTE RESOURCES,0.8631368631368631,"• The answer NA means that the paper does not include experiments.
763"
EXPERIMENTS COMPUTE RESOURCES,0.8641358641358642,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
764"
EXPERIMENTS COMPUTE RESOURCES,0.8651348651348651,"or cloud provider, including relevant memory and storage.
765"
EXPERIMENTS COMPUTE RESOURCES,0.8661338661338661,"• The paper should provide the amount of compute required for each of the individual
766"
EXPERIMENTS COMPUTE RESOURCES,0.8671328671328671,"experimental runs as well as estimate the total compute.
767"
EXPERIMENTS COMPUTE RESOURCES,0.8681318681318682,"• The paper should disclose whether the full research project required more compute
768"
EXPERIMENTS COMPUTE RESOURCES,0.8691308691308691,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
769"
EXPERIMENTS COMPUTE RESOURCES,0.8701298701298701,"didn’t make it into the paper).
770"
CODE OF ETHICS,0.8711288711288712,"9. Code Of Ethics
771"
CODE OF ETHICS,0.8721278721278721,"Question: Does the research conducted in the paper conform, in every respect, with the
772"
CODE OF ETHICS,0.8731268731268731,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
773"
CODE OF ETHICS,0.8741258741258742,"Answer: [Yes]
774"
CODE OF ETHICS,0.8751248751248751,"Justification: We have read the code of ethics and followed its requirements.
775"
CODE OF ETHICS,0.8761238761238761,"Guidelines:
776"
CODE OF ETHICS,0.8771228771228772,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
777"
CODE OF ETHICS,0.8781218781218781,"• If the authors answer No, they should explain the special circumstances that require a
778"
CODE OF ETHICS,0.8791208791208791,"deviation from the Code of Ethics.
779"
CODE OF ETHICS,0.8801198801198801,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
780"
CODE OF ETHICS,0.8811188811188811,"eration due to laws or regulations in their jurisdiction).
781"
BROADER IMPACTS,0.8821178821178821,"10. Broader Impacts
782"
BROADER IMPACTS,0.8831168831168831,"Question: Does the paper discuss both potential positive societal impacts and negative
783"
BROADER IMPACTS,0.8841158841158842,"societal impacts of the work performed?
784"
BROADER IMPACTS,0.8851148851148851,"Answer: [NA]
785"
BROADER IMPACTS,0.8861138861138861,"Justification: This work has no societal impact.
786"
BROADER IMPACTS,0.8871128871128872,"Guidelines:
787"
BROADER IMPACTS,0.8881118881118881,"• The answer NA means that there is no societal impact of the work performed.
788"
BROADER IMPACTS,0.8891108891108891,"• If the authors answer NA or No, they should explain why their work has no societal
789"
BROADER IMPACTS,0.8901098901098901,"impact or why the paper does not address societal impact.
790"
BROADER IMPACTS,0.8911088911088911,"• Examples of negative societal impacts include potential malicious or unintended uses
791"
BROADER IMPACTS,0.8921078921078921,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
792"
BROADER IMPACTS,0.8931068931068931,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
793"
BROADER IMPACTS,0.8941058941058941,"groups), privacy considerations, and security considerations.
794"
BROADER IMPACTS,0.8951048951048951,"• The conference expects that many papers will be foundational research and not tied
795"
BROADER IMPACTS,0.8961038961038961,"to particular applications, let alone deployments. However, if there is a direct path to
796"
BROADER IMPACTS,0.8971028971028971,"any negative applications, the authors should point it out. For example, it is legitimate
797"
BROADER IMPACTS,0.8981018981018981,"to point out that an improvement in the quality of generative models could be used to
798"
BROADER IMPACTS,0.8991008991008991,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
799"
BROADER IMPACTS,0.9000999000999002,"that a generic algorithm for optimizing neural networks could enable people to train
800"
BROADER IMPACTS,0.9010989010989011,"models that generate Deepfakes faster.
801"
BROADER IMPACTS,0.9020979020979021,"• The authors should consider possible harms that could arise when the technology is
802"
BROADER IMPACTS,0.903096903096903,"being used as intended and functioning correctly, harms that could arise when the
803"
BROADER IMPACTS,0.9040959040959041,"technology is being used as intended but gives incorrect results, and harms following
804"
BROADER IMPACTS,0.9050949050949051,"from (intentional or unintentional) misuse of the technology.
805"
BROADER IMPACTS,0.906093906093906,"• If there are negative societal impacts, the authors could also discuss possible mitigation
806"
BROADER IMPACTS,0.9070929070929071,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
807"
BROADER IMPACTS,0.9080919080919081,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
808"
BROADER IMPACTS,0.9090909090909091,"feedback over time, improving the efficiency and accessibility of ML).
809"
SAFEGUARDS,0.9100899100899101,"11. Safeguards
810"
SAFEGUARDS,0.9110889110889111,"Question: Does the paper describe safeguards that have been put in place for responsible
811"
SAFEGUARDS,0.9120879120879121,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
812"
SAFEGUARDS,0.913086913086913,"image generators, or scraped datasets)?
813"
SAFEGUARDS,0.9140859140859141,"Answer: [NA]
814"
SAFEGUARDS,0.9150849150849151,"Justification: This paper poses no such risks as our released model and datasets are only
815"
SAFEGUARDS,0.916083916083916,"able to be used for quantum state preparation.
816"
SAFEGUARDS,0.9170829170829171,"Guidelines:
817"
SAFEGUARDS,0.9180819180819181,"• The answer NA means that the paper poses no such risks.
818"
SAFEGUARDS,0.919080919080919,"• Released models that have a high risk for misuse or dual-use should be released with
819"
SAFEGUARDS,0.9200799200799201,"necessary safeguards to allow for controlled use of the model, for example by requiring
820"
SAFEGUARDS,0.9210789210789211,"that users adhere to usage guidelines or restrictions to access the model or implementing
821"
SAFEGUARDS,0.922077922077922,"safety filters.
822"
SAFEGUARDS,0.9230769230769231,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
823"
SAFEGUARDS,0.9240759240759241,"should describe how they avoided releasing unsafe images.
824"
SAFEGUARDS,0.9250749250749251,"• We recognize that providing effective safeguards is challenging, and many papers do
825"
SAFEGUARDS,0.926073926073926,"not require this, but we encourage authors to take this into account and make a best
826"
SAFEGUARDS,0.9270729270729271,"faith effort.
827"
LICENSES FOR EXISTING ASSETS,0.9280719280719281,"12. Licenses for existing assets
828"
LICENSES FOR EXISTING ASSETS,0.929070929070929,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
829"
LICENSES FOR EXISTING ASSETS,0.9300699300699301,"the paper, properly credited and are the license and terms of use explicitly mentioned and
830"
LICENSES FOR EXISTING ASSETS,0.9310689310689311,"properly respected?
831"
LICENSES FOR EXISTING ASSETS,0.932067932067932,"Answer: [Yes]
832"
LICENSES FOR EXISTING ASSETS,0.9330669330669331,"Justification: We use an open-source dataset FractalDB, we cite the original paper and
833"
LICENSES FOR EXISTING ASSETS,0.9340659340659341,"indicates the version we use in Section 4.1.
834"
LICENSES FOR EXISTING ASSETS,0.935064935064935,"Guidelines:
835"
LICENSES FOR EXISTING ASSETS,0.936063936063936,"• The answer NA means that the paper does not use existing assets.
836"
LICENSES FOR EXISTING ASSETS,0.9370629370629371,"• The authors should cite the original paper that produced the code package or dataset.
837"
LICENSES FOR EXISTING ASSETS,0.938061938061938,"• The authors should state which version of the asset is used and, if possible, include a
838"
LICENSES FOR EXISTING ASSETS,0.939060939060939,"URL.
839"
LICENSES FOR EXISTING ASSETS,0.9400599400599401,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
840"
LICENSES FOR EXISTING ASSETS,0.9410589410589411,"• For scraped data from a particular source (e.g., website), the copyright and terms of
841"
LICENSES FOR EXISTING ASSETS,0.942057942057942,"service of that source should be provided.
842"
LICENSES FOR EXISTING ASSETS,0.9430569430569431,"• If assets are released, the license, copyright information, and terms of use in the
843"
LICENSES FOR EXISTING ASSETS,0.9440559440559441,"package should be provided. For popular datasets, paperswithcode.com/datasets
844"
LICENSES FOR EXISTING ASSETS,0.945054945054945,"has curated licenses for some datasets. Their licensing guide can help determine the
845"
LICENSES FOR EXISTING ASSETS,0.9460539460539461,"license of a dataset.
846"
LICENSES FOR EXISTING ASSETS,0.9470529470529471,"• For existing datasets that are re-packaged, both the original license and the license of
847"
LICENSES FOR EXISTING ASSETS,0.948051948051948,"the derived asset (if it has changed) should be provided.
848"
LICENSES FOR EXISTING ASSETS,0.949050949050949,"• If this information is not available online, the authors are encouraged to reach out to
849"
LICENSES FOR EXISTING ASSETS,0.9500499500499501,"the asset’s creators.
850"
NEW ASSETS,0.951048951048951,"13. New Assets
851"
NEW ASSETS,0.952047952047952,"Question: Are new assets introduced in the paper well documented and is the documentation
852"
NEW ASSETS,0.9530469530469531,"provided alongside the assets?
853"
NEW ASSETS,0.954045954045954,"Answer: [Yes]
854"
NEW ASSETS,0.955044955044955,"Justification: We submit our assets in zip file and also put them on the anonymous github
855"
NEW ASSETS,0.9560439560439561,"repository, we have included a README file with detailed descriptions.
856"
NEW ASSETS,0.957042957042957,"Guidelines:
857"
NEW ASSETS,0.958041958041958,"• The answer NA means that the paper does not release new assets.
858"
NEW ASSETS,0.9590409590409591,"• Researchers should communicate the details of the dataset/code/model as part of their
859"
NEW ASSETS,0.9600399600399601,"submissions via structured templates. This includes details about training, license,
860"
NEW ASSETS,0.961038961038961,"limitations, etc.
861"
NEW ASSETS,0.962037962037962,"• The paper should discuss whether and how consent was obtained from people whose
862"
NEW ASSETS,0.9630369630369631,"asset is used.
863"
NEW ASSETS,0.964035964035964,"• At submission time, remember to anonymize your assets (if applicable). You can either
864"
NEW ASSETS,0.965034965034965,"create an anonymized URL or include an anonymized zip file.
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9660339660339661,"14. Crowdsourcing and Research with Human Subjects
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967032967032967,"Question: For crowdsourcing experiments and research with human subjects, does the paper
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.968031968031968,"include the full text of instructions given to participants and screenshots, if applicable, as
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9690309690309691,"well as details about compensation (if any)?
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97002997002997,"Answer: [NA]
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971028971028971,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972027972027972,"Guidelines:
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973026973026973,"• The answer NA means that the paper does not involve crowdsourcing nor research with
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974025974025974,"human subjects.
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975024975024975,"• Including this information in the supplemental material is fine, but if the main contribu-
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760239760239761,"tion of the paper involves human subjects, then as much detail as possible should be
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977022977022977,"included in the main paper.
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978021978021978,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790209790209791,"or other labor should be paid at least the minimum wage in the country of the data
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98001998001998,"collector.
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981018981018981,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820179820179821,"Subjects
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983016983016983,"Question: Does the paper describe potential risks incurred by study participants, whether
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984015984015984,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985014985014985,"approvals (or an equivalent approval/review based on the requirements of your country or
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986013986013986,"institution) were obtained?
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987012987012987,"Answer: [NA]
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988011988011988,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989010989010989,"Guidelines:
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99000999000999,"• The answer NA means that the paper does not involve crowdsourcing nor research with
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991008991008991,"human subjects.
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920079920079921,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993006993006993,"may be required for any human subjects research. If you obtained IRB approval, you
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994005994005994,"should clearly state this in the paper.
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995004995004995,"• We recognize that the procedures for this may vary significantly between institutions
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996003996003996,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997002997002997,"guidelines for their institution.
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998001998001998,"• For initial submissions, do not include any information that would break anonymity (if
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.999000999000999,"applicable), such as the institution conducting the review.
899"
