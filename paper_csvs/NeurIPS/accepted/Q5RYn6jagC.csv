Section,Section Appearance Order,Paragraph
PRINCETON NEUROSCIENCE INSTITUTE,0.0,"1Princeton Neuroscience Institute
2Department of Psychology, Princeton University
3Department of Computer Science, EPFL
4Department of Cognitive Science, Dartmouth College
5Department of Computer Science, Princeton University
6Microsoft Research
* Equal contribution"
ABSTRACT,0.0047169811320754715,Abstract
ABSTRACT,0.009433962264150943,"Recent work has documented striking heterogeneity in the performance of state-
of-the-art vision language models (VLMs), including both multimodal language
models and text-to-image models. These models are able to describe and generate
a diverse array of complex, naturalistic images, yet they exhibit surprising failures
on basic multi-object reasoning tasks – such as counting, localization, and simple
forms of visual analogy – that humans perform with near perfect accuracy. To
better understand this puzzling pattern of successes and failures, we turn to theo-
retical accounts of the binding problem in cognitive science and neuroscience, a
fundamental problem that arises when a shared set of representational resources
must be used to represent distinct entities (e.g., to represent multiple objects in an
image), necessitating the use of serial processing to avoid interference. We find that
many of the puzzling failures of state-of-the-art VLMs can be explained as arising
due to the binding problem, and that these failure modes are strikingly similar to
the limitations exhibited by rapid, feedforward processing in the human brain."
INTRODUCTION,0.014150943396226415,"1
Introduction"
INTRODUCTION,0.018867924528301886,"Recent progress in training large-scale neural networks on internet-scale datasets has led to the
creation of AI systems with capabilities rivaling human performance across a broad range of complex
tasks. Most recently, this has given rise to an array of vision language models (VLMs), including
multimodal language models such as GPT-4v that can generate text descriptions of multimodal text
and image inputs [1], and text-to-image models such as DALL-E 3 that can generate images from
natural language descriptions [24]. However, despite the considerable success of VLMs across many
tasks, these models still perform poorly on several surprisingly simple multi-object reasoning tasks –
such as counting [23, 25, 40], relational image generation [7], relational scene understanding [15, 31],
and simple visual analogy tasks [20, 38] – on which humans achieve near perfect accuracy."
INTRODUCTION,0.02358490566037736,"Drawing from theoretical work both in cognitive science and neuroscience, we turn to the binding
problem [9, 10, 29, 33, 36] as a potential explanation for these limitations. ‘Binding’ refers to the
ability to associate one feature of an object (e.g., its color) with the other features of that object (e.g.,
its shape and location), and the ‘binding problem’ refers to the question of how the brain accomplishes
this without interference between the features for different objects. It is widely recognized that the"
INTRODUCTION,0.02830188679245283,"human visual system relies on serial processing to solve this problem, iteratively directing attention to
individual objects so as to avoid interference [28, 33], and that binding errors arise when it is forced
to rely on rapid, parallel visual processing [14, 19, 33]. For example, when human participants are not
able to effectively deploy serial processing (e.g., because attention is overloaded, or because speeded
judgments are required), they are susceptible to so-called illusory conjunctions (e.g., mistakenly
identifying a red square in an image that contains a green square and a red circle) [32]."
INTRODUCTION,0.0330188679245283,"In this work, we test the hypothesis that the failures exhibited by VLMs on multi-object reasoning
tasks are due to representational interference resulting from an inability to manage the binding
problem. We first investigate two classic tasks from the cognitive science literature, visual search [33]
and numerical estimation [14, 19] (i.e., counting), finding that a wide range of VLMs (including 5
multimodal language models and 4 text-to-image models) exhibit stark capacity constraints similar to
those displayed by human observers when forced to make speeded responses. Importantly, although
these effects are more pronounced for scenes with more objects, they cannot be explained entirely as
a function of the number of objects in a scene. Instead, we find that performance is best explained
by the probability of interference given the specific distribution of features and their conjunctions
within a scene. Motivated by this observation, we develop a novel scene description benchmark that
systematically varies the likelihood of interference, finding that this quantity is highly predictive of
binding errors."
INTRODUCTION,0.03773584905660377,"We also apply these insights to better understand the limitations of VLMs in visual analogy tasks,
introducing a simple input pre-processing technique to reduce the potential for representational inter-
ference. We show that this technique improves the performance of VLMs on the task, suggesting that
their original limitation on this task may be due to a more basic difficulty with processing multi-object
scenes, rather than an inability to process relations. Finally, we discuss the normative factors that
underlie the binding problem [2, 9, 22], highlighting the role of compositional representations, which
are useful for generalization, but introduce the potential for interference when shared representations
are used to process multiple objects at the same time. We argue that, surprisingly, the binding
failures exhibited by VLMs imply the presence of compositional representations. Overall, these
results highlight the usefulness of cognitive science in helping to understand the limits of large-scale
generative models, and suggest the presence of a common set of principles that govern information
processing in both artificial systems and human cognition."
VISUAL SEARCH,0.04245283018867924,"2
Visual Search"
VISUAL SEARCH,0.04716981132075472,"Extensive prior work in cognitive psychology has tested how people process scenes involving
multiple objects and under what conditions their performance degrades. These studies demonstrate
that performance is not driven solely by the number of objects present in a scene, but also depends on
the likelihood of interference among objects given the specific distribution of features and feature
conjunctions from which they are composed. This can be seen most directly in research on visual
search, where participants are typically tasked with identifying a specific object within a multi-object
array. A classic pattern of results arises from a comparison of two conditions: disjunctive and
conjunctive search [33]. In disjunctive search (depicted on the left side of Figure 1), the array consists
of distractor objects that share one feature with the target (e.g., the distractors are all circles) but differ
in a second feature (e.g., the distractors are all red circles in the 2D task variant). Since one of the
feature values (the color green) is uniquely assigned to the target object, the distractors present little
interference and therefore task performance is invariant to the number of distractors. This condition
is therefore sometimes referred to as “popout” search, as the target immediately stands out from
the distractors, and the task can thus be performed rapidly without the need for serial processing.
Conversely, in conjunctive search (depicted in the middle of Figure 1), there are two types of distractor
objects that each share one feature with the target (e.g., half of the distractors are red L-shapes and
the other half are green T-shapes). In this case, the target (a green L-shape) possesses no unique
feature that easily distinguishes it from the distractors, leading to a significant degree of interference
between the distractors and the target. One way to mitigate this is the use of serial search to identify
the target. This is suggested by ubiquitiously observed increases in reaction time as a function of the
number of distractors, as well as the observation that when participants are prevented from engaging
in serial search (e.g., by forcing participants to respond quickly), task performance degrades rapidly
as more objects are added to the scene."
VISUAL SEARCH,0.05188679245283019,"Disjunctive search
Conjunctive search"
D SPRITES,0.05660377358490566,2D Sprites
D OBJECTS,0.06132075471698113,3D Objects
D OBJECTS,0.0660377358490566,"Figure 1: Visual search tasks and results. Example trials for the 2D (top) and 3D (bottom) variants
of the disjunctive (left/red column) and conjunctive (middle/blue column) search conditions. Perfor-
mance for 2D and 3D task variants are plotted on the right. Results reflect aggregate performance
for all four VLMs (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5; see Supplementary
Figure 5 for separate model results). Error bars denote 95% binomial confidence intervals."
METHODS,0.07075471698113207,"2.1
Methods"
METHODS,0.07547169811320754,"We tested the extent to which VLMs demonstrate similar capacity constraints to humans in visual
search tasks. We evaluated four multimodal language models – GPT-4v, GPT-4o, Gemini Ultra
1.5, and Claude Sonnet 3.5 – on a task involving disjunctive and conjunctive search conditions.1We
generated datasets involving either 2D sprites or 3D scenes created in Blender [6] (similar to those
found in the CLEVR dataset [13]). The datasets were designed to evaluate the ability of the model to
detect the presence of a target object among multiple distractors. In half of the images, a target was
present, while in the other half, no target was present."
METHODS,0.08018867924528301,"Each image contained between 4 and 50 distractors. For the disjunctive search task, these consisted
of non-overlapping red circles (for the 2D dataset) or green spheres (for the 3D dataset) of a uniform
size. Half of the images additionally contained a target object, which was a green circle (for the 2D
dataset) or a red sphere (for the 3D dataset). For the conjunctive search task, the 2D dataset consisted
of images in which the distractors were either red L-shapes or green T-shapes (randomly selected
with equal probability). Half of the images additionally contained a target object, which was a green
L-shape. The 3D dataset consisted of images in which the distractors were either green spheres or red
cubes (randomly selected with equal probability). Half of the images additionally contained a target
object, which was a red sphere. Each of the datasets (2D disjunctive, 2D conjunctive, 3D disjunctive,
3D conjunctive) contained 1000 images."
RESULTS,0.08490566037735849,"2.2
Results"
RESULTS,0.08962264150943396,"We measured the performance of each model by calculating, for each condition, how detection
accuracy varied as a function of the number of distractors2. The results indicate that performance in
the disjunctive search (i.e., popout) condition was perfect, and invariant to the number of distractors.
That is, regardless of the number of distractors, all models achieved perfect accuracy in this condition."
RESULTS,0.09433962264150944,"1We also evaluated an open-source multimodal language model – Llava 1.5 – but performance was very low
for these tasks. These results are presented in Supplementary Figure 6.
2When studying visual search in human participants, a reaction time (RT) paradigm is typically employed,
measuring the time required to locate the target object. Because RT measures cannot be straightforwardly
obtained from VLMs, we instead measure accuracy, which is sometimes used in human behavioral studies that
employ speeded responses. [18]"
RESULTS,0.09905660377358491,"In contrast, in the conjunctive search condition performance was inversely related to the number
of objects: for 5 objects, all models displayed an accuracy of ∼90%, but as the number of objects
increased, performance dropped substantially. These results were consistently observed for both
the 2D (top panel of Figure 1) and 3D (bottom panel of Figure 1) datasets. These results were also
replicated in an alternative version of the disjunctive search task, in which target and distractor colors
were varied between trials (Supplementary Figure 7)."
RESULTS,0.10377358490566038,"The results of these experiments suggest that multimodal language models demonstrate human-like
capacity constraints in their ability to perform visual search in multi-object settings. It is important to
emphasize that these capacity constraints are not driven solely by the number of objects present within
a scene. Like humans, these models demonstrate capacity constraints only in the task conditions that
are impacted by interference between the target and distractor objects, consistent with the hypothesis
that these capacity constraints arise as a consequence of the binding problem."
NUMERICAL ESTIMATION,0.10849056603773585,"3
Numerical Estimation"
NUMERICAL ESTIMATION,0.11320754716981132,"To assess the generality of the human-like capacity constraints observed for VLMs in visual process-
ing, we investigated a simple numerical estimation task (i.e., counting) that has been widely studied
in cognitive psychology. Although human observers can precisely count a very large number of items
when allowed to explicitly process those items one at a time, their ability to rapidly estimate the
number of items in a display is subject to a severe capacity constraint. Studies have found that the
number of objects that can be reliably estimated without explicit serial counting (sometimes referred
to as “subitizing”) is somewhere between 4 and 6 [14, 17, 19, 27, 34]. To determine whether VLMs
are subject to similar constraints, we evaluated both multimodal language models (GPT-4v, GPT-4o,
Gemini Ultra 1.5, Claude Sonnet 3.5, and Llava 1.5) and text-to-image models (Stable Diffusion Ultra,
DALL-E 3, Google Parti, and Google Muse) on a numerical estimation task involving variations
of both the number and type of objects. We found that VLMs, across a variety of stimulus and
model types, display strikingly similar quantitative capacity limits to those observed in human vision.
We also found that these capacity constraints were strongly affected by the variability of features
present in an image. This effect is consistent with the hypothesis that these constraints arise due to
representational interference: given that objects are represented with a shared set of representational
resources, greater feature variability leads to less overlap in the use of these resources, and therefore
less opportunities for interference and binding errors."
METHODS,0.1179245283018868,"3.1
Methods"
METHODS,0.12264150943396226,"We generated datasets involving both 2D sprites and 3D objects, varying the number of objects per
image between 1 and 20. We explored four conditions with varying levels of feature entropy (i.e.,
feature variability): a low-entropy condition in which all objects in an image had the same color
and shape; two medium-entropy conditions in which all objects in an image had the same shape but
unique colors, or vice versa; and a high-entropy condition in which all objects in an image had unique
colors and shapes. We prompted the multimodal language models to describe the image and then
state the number of objects present in it. To test the text-to-image models, we generated a dataset
comprising 100 distinct categories, evenly split between common foods (50 categories) and animals
(50 categories). We tasked these models with generating images from each category, for which the
number of instances of each object ranged from 1 to 10. To assess their ability to generate images
with the exact number of objects requested, we conducted a human evaluation study. Participants
were asked to count and report the number of objects visible in each generated image. The collected
human judgments were then used to quantify the model’s accuracy. See Appendix C for further
details on human evaluations."
RESULTS,0.12735849056603774,"3.2
Results"
RESULTS,0.1320754716981132,"We measured performance by calculating, for each condition, how accuracy varied with the number
of objects present in the scene. The results indicated that, regardless of the type of stimuli used (2D
vs. 3D shapes, or animals vs. food), and across two fundamentally different types of vision language
model (multimodal language models and text-to-image models), VLMs displayed human-like capacity
limits (Figure 2). For both multimodal language models and T2I models, accuracy was very high for
scenes involving a relatively small number of objects (1-5), but dropped sharply for scenes involving"
D SPRITES,0.13679245283018868,"2D Sprites
3D Objects"
D SPRITES,0.14150943396226415,"High entropy
Medium entropy
Low Entropy"
D SPRITES,0.14622641509433962,"Foods
Animals"
OBJECTS,0.1509433962264151,3 Objects
OBJECTS,0.15566037735849056,6 Objects VLMs T2Is
OBJECTS,0.16037735849056603,"Figure 2: Numerical estimation tasks and results. Top left: Examples of images generated by
text-to-image (T2I) models for different numbers and categories of objects. Top right: Performance
of T2I models as a function of the number and category of objects. Results reflect an aggregate
of four models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse). Bottom left:
Examples of images (featuring either 2D or 3D objects) used to evaluate numerosity estimation.
Feature entropy was varied in four conditions (low entropy, high entropy, and two medium entropy
conditions). Bottom middle: Numerosity estimation results for four multimodal language models
(GPT-4v, GPT-4o, Gemini Ultra 1.5, Claude Sonnet 3.5; see Supplementary Figure 6 for results
with Llava-1.5). Bottom right: Numerosity estimation results plotted as a function of the number of
objects in an image, aggregated across all four models (see Supplementary Figure 8 for individual
model results). Error bars for all plots reflect 95% binomial confidence intervals."
OBJECTS,0.1650943396226415,"6 or more objects. Moreover, the multimodal language models exhibited performance consistent
with our hypothesis that capacity limits arise due to representational interference across objects
(i.e., the binding problem), with overall performance highest in the high-entropy condition (lowest
interference), lowest in the low-entropy condition (highest interference), and intermediate in between
these two extremes in the medium-entropy conditions. Though there are slight differences between
the capacity limits exhibited by these two classes of models, it is striking that they both fall within
the subitizing limit of human vision, especially when considering the significant differences in both
architecture and training procedures. Furthermore, the effect of feature entropy on these capacity"
OBJECTS,0.16981132075471697,"limits strongly suggests that they are driven by representational interference, arising due an inability
to manage the binding problem. To further investigate this hypothesis, we next turned to a novel scene
description benchmark that allowed us to systematically investigate the likelihood of representational
interference, thus enabling a direct test of the extent to which this factor is responsible for the
shortcomings of VLMs."
SCENE DESCRIPTION,0.17452830188679244,"4
Scene Description"
SCENE DESCRIPTION,0.1792452830188679,"Theoretical accounts of the binding problem [9, 33] posit that capacity limits in rapid visual processing
arise as a consequence of interference between representations. Given a scene containing multiple
objects, and a set of shared features with which to represent those objects, the likelihood of interference
will tend to increase as a function of the number of objects in the scene (without the availability
of a mechanism for binding features together, e.g., serial processing). However, as emphasized in
our experiments on visual search and numerosity estimation, interference is not driven solely by the
number of objects, but is also strongly influenced by the specific feature conjunctions present within
a scene."
SCENE DESCRIPTION,0.18396226415094338,"We developed a novel scene description task to further investigate the extent to which VLM per-
formance is driven by representational interference. The task is illustrated in Figure 3a. For each
image, the likelihood of representational interference was quantified as the number of feature triplets
present in that image. A feature triplet is defined as any set of three objects for which one pair shares
a feature, and another pair shares a different feature. For instance, {green X, green triangle, yellow
triangle} is a feature triplet, because the feature ‘green’ is shared by two objects (the green X and
the green triangle), and the feature ‘triangle’ is shared by two objects (the green triangle and the
yellow triangle). Without the ability to accurately bind these features together at the level of objects,
such feature triplets create opportunities for representational interference, and thus lead to illusory
conjunctions. For instance, the feature triplet {green X, green triangle, yellow triangle} may lead
to the erroneous identification of a yellow X. We studied the extent to which the presence of such
feature triplets can account for scene description performance in VLMs."
METHODS,0.18867924528301888,"4.1
Methods"
METHODS,0.19339622641509435,"As in the previous tasks, we generated datasets involving either 2D sprites or 3D objects. Each scene
contained a variable number of objects (10-15 objects for the 2D dataset and 8-12 objects for the
3D dataset), and we systematically varied the number of feature triplets present in each scene. For
example, the scene depicted in Figure 3a contains three feature triplets (illustrated by the dashed
lines). VLMs (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5) were prompted to provide
a description of the objects in JSON format (see Appendix B for more details). We also generated
prompts describing similar scenes (but involving real-world objects) and tested the ability of the T2I
models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse) to accurately generate
these scenes (as assessed by human evaluation; see Appendix C for more details). To obtain a
representative sampling of scenes with different triplet counts, we systematically varied the diversity
of colors and shapes across trials. This approach ensured adequate sampling of trials with different
feature combinations and their associated triplet counts. To ensure reliable performance estimates,
we excluded from analysis any triplet counts represented by fewer than 20 trials across all conditions.
For the T2I experiments, we additionally excluded trials where models generated more than three
extraneous objects not specified in the prompt, as these represented significant deviations from the
intended scene structure."
RESULTS,0.19811320754716982,"4.2
Results"
RESULTS,0.2028301886792453,"We measured scene description performance by calculating how the number of errors (quantified
as the edit distance between the true description of the scene and the model’s description of the
scene) varies as a function of the number of objects present in the scene, and the number of feature
triplets. The results (Figure 3) confirmed our prediction that performance should vary as a function
of the number of triplets. Across multiple stimulus types (2D and 3D objects), and model types
(both multimodal language models and text-to-image models), the largest number of errors occurred
in the trials where the risk of binding errors was highest (i.e., the trials with the largest number of
feature triplets), consistent with the hypothesis that errors would be driven primarily by the formation a. c.
d. 1 2
3 b."
RESULTS,0.20754716981132076,"Figure 3: Scene description task and results. A) Example image used in 2D scene description
task, illustrating the concept of feature triplets: sets of three objects where one pair of objects shares
a feature, and another pair shares a different feature. This example contains three feature triplets,
demarcated by the dashed lines. 3D scenes were also investigated. B) Scene description results for
text-to-image (T2I) models) as a function of the number of feature triplets. C) 2D scene description
results for multimodal language models as a function of the number of feature triplets. Left panel
illustrates the results aggregated across four models (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude
Sonnet 3.5). Right panel illustrates the results aggregated across scenes with different numbers of
objects. D) 3D scene description results. Error bars represent the standard error of the mean."
RESULTS,0.21226415094339623,"of illusory conjunctions. These results also showed a pattern of increasing errors as a function of
the number of objects, consistent with a general capacity limit on the number of objects that can
be accurately represented at the same time. Overall, these results suggest that the capacity limits
displayed by VLMs are best explained by an inability to manage the binding problem. In the next
section, we apply these insights to better understand the limited visual reasoning capabilities of
VLMs."
VISUAL ANALOGY,0.2169811320754717,"5
Visual Analogy"
VISUAL ANALOGY,0.22169811320754718,"An open question in studying the performance of VLMs is the extent to which these models can
solve analogical reasoning tasks. These tasks are of particular interest given their centrality in human
higher-order cognition [11] and their use as measures of human intelligence [30]. Recent work has
demonstrated that LLMs have an impressive ability to solve a range of text-based analogical reasoning
tasks [37], but initial tests of VLMs have suggested that they often struggle to solve comparable
visual forms of these tasks, sometimes performing well below human participants [20, 38]."
VISUAL ANALOGY,0.22641509433962265,"This leads to the question of why, given the success of LLMs on text-based problems in this domain,
VLMs do not display comparable success in solving analogy tasks. One possible explanation,"
VISUAL ANALOGY,0.23113207547169812,Uniﬁed Condition
VISUAL ANALOGY,0.2358490566037736,Decomposed Condition
VISUAL ANALOGY,0.24056603773584906,"Figure 4: Visual analogy task. The Unified and Decomposed conditions present the same object
pairs, but in the Decomposed condition it is broken up across three images. The correct target pair
must share both relations (shape and color) with the source pair, so the correct answer in this example
is Target Pair 2 because it satisfies both the ‘same shape’ and ‘different color’ relations."
VISUAL ANALOGY,0.24528301886792453,"suggested by our investigation of the binding problem, is that the difficulty displayed by VLMs on
visual reasoning tasks may stem from a more basic difficulty with processing multi-object scenes.
In other words, regardless of whether VLMs have the capacity for abstract reasoning necessary to
solve analogy tasks, they will likely struggle to solve visual analogy tasks simply because these tasks
involve processing multi-object scenes. To distinguish between these two failure modes (inability to
process abstract relations vs. inability to process multi-object scenes), we performed experiments
using a visual analogy task in which the visual processing demands were explicitly manipulated, and
measured the ability of multimodal language models to perform both abstract relational tasks and
basic object-level tasks."
METHODS,0.25,"5.1
Methods"
METHODS,0.25471698113207547,"We generated 200 trials from a simple relational match-to-sample (RMTS) task (Figure 4) using the
same 2D sprite stimuli from the previous experiments. We selected a subset of 8 easily recognizable
shapes and colors to generate a set of 64 stimuli. For each trial, we chose the source pair by sampling
two objects that shared at least one of the two feature dimensions. We then selected the target pairs
by sampling two pairs of objects: one which matched the source pair exactly along its relations (the
correct target) and one which shared only one of the relations with the source pair (the incorrect
target). We manipulated visual processing demands by investigating two conditions, one in which
the source and target pairs were presented in a single image (the “unified” condition), and one in
which the source and target pairs were presented as separate images presented in sequence (the
“decomposed” condition), thereby reducing the chance of binding errors."
METHODS,0.25943396226415094,"We assessed the performance of four VLMs (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet
3.5) in four tasks: identification of the correct target pair in the full RMTS task (Analogy), decoding
of single features from specific individual objects (Single Feature Decoding Task), comprehensive
decoding of all features in a given problem (Full Feature Decoding), and decoding of relations
between object pairs (Relation Decoding)."
RESULTS,0.2641509433962264,"5.2
Results"
RESULTS,0.2688679245283019,"We found that performance on this task was highly variable aross VLMs, with some models (Claude
Sonnet) showing nearly perfect performance on all tasks (Table 3) and other models (Gemini Ultra)
showing poor performance on most tasks (Table 4). These results are consistent with recent work
showing mixed success on visual analogy problems [38], and at odds with work claiming that VLMs
have no capacity for visual analogy [20]. Interestingly, we also found that many models also struggled
on more basic tasks such as identifying the features of the objects present in the image, or identifying
the relations for individual pairs of objects."
RESULTS,0.27358490566037735,Table 1: Visual analogy results: GPT-4v.
RESULTS,0.2783018867924528,"Unified Accuracy
Decomposed Accuracy
Accuracy
95% CI
Accuracy
95% CI
Analogy
91%
(86%, 94%)
99%
(97%, 99%)
Relation decoding
80%
(73%, 84%)
91%
(86%, 94%)
Full feature decoding
85%
(79%, 89%)
100%
(100%, 100%)
Single feature decoding
95%
(92%, 98%)
98%
(96%, 99%)"
RESULTS,0.2830188679245283,Table 2: Visual analogy results: GPT-4o.
RESULTS,0.28773584905660377,"Unified Accuracy
Decomposed Accuracy
Accuracy
95% CI
Accuracy
95% CI
Analogy
99%
(96%, 100%)
100%
(100%, 100%)
Relation decoding
88%
(82%, 92%)
98%
(95%, 99%)
Full feature decoding
97%
(84%, 100%)
100%
(100%, 100%)
Single feature decoding
86%
(81%, 91%)
94%
(81%, 91%)"
RESULTS,0.29245283018867924,Table 3: Visual analogy results: Claude Sonnet 3.5.
RESULTS,0.2971698113207547,"Unified Accuracy
Decomposed Accuracy
Accuracy
95% CI
Accuracy
95% CI
Analogy
100%
(100%, 100%)
100%
(100%, 100%)
Relation decoding
92%
(88%, 95%)
99%
(97%, 100%)
Full feature decoding
100%
(100%, 100%)
100%
(100%, 100%)
Single feature decoding
100%
(100%, 100%)
100%
(100%, 100%)"
RESULTS,0.3018867924528302,Table 4: Visual analogy results: Gemini Ultra 1.5.
RESULTS,0.30660377358490565,"Unified Accuracy
Decomposed Accuracy
Accuracy
95% CI
Accuracy
95% CI
Analogy
56%
(49%, 64%)
60%
(53%, 67%)
Relation decoding
84%
(79%, 89%)
89%
(83%, 93%)
Full feature decoding
100%
(100%, 100%)
100%
(100%, 100%)
Single feature decoding
81%
(74%, 85%)
83%
(78%, 88%)"
RESULTS,0.3113207547169811,"Most importantly, we found that performance was significantly improved in the Decomposed con-
dition (involving separate images for each pair of objects) as compared with the Unified condition
(involving a single image with all object pairs). This was the case across all tasks, and for all models,
except for the cases in which performance was already at ceiling for the Unified condition. Taken
together, these results suggest that the poor performance of VLMs on visual analogy tasks may be
a consequence of a more general limitation with processing multi-object scenes, arising due to an
inability to manage the binding problem."
DISCUSSION,0.3160377358490566,"6
Discussion"
DISCUSSION,0.32075471698113206,"We have presented a series of experiments aimed at understanding the limits of vision language
models in processing multi-object scenes. Our results suggest that these limitations can all be
understood as arising from an inability to manage the binding problem, a fundamental problem
associated with compositional coding identified by classic work in cognitive science [10, 33]."
DISCUSSION,0.32547169811320753,"Recent theoretical work has formalized this problem within a normative framework [9], sug-
gesting that it arises due to a tension between the learning of compositional representations,
and the shared use of such representations to encode multiple objects at the same time.
To
illustrate this, consider two different schemes for representing multi-object scenes: a conjunc-"
DISCUSSION,0.330188679245283,"tive scheme, involving dedicated representations for every possible conjunction of features (e.g.,
{redsquare, greencircle, bluetriangle, greensquare, . . .}) vs. a compositional scheme, involving
the dynamic combination of shared features across different dimensions (e.g., combining the features
{red} and {triangle} to represent a red triangle). The compositional scheme enables efficient use of
finite neural resources, and offers major benefits in terms of generalization (e.g., anything learned
about red triangles can then be readily generalized to support inferences about other red objects), but,
without a mechanism for dynamically keeping track of the bindings between features, this scheme
leads to severe interference, giving rise to the capacity limits observed in cognitive processing. One
surprising implication of this perspective is that the binding errors exhibited by VLMs suggest that
they have developed compositional representations, perhaps as a consequence of being forced to
generalize by their immense and highly diverse training corpora. Without the use of such compo-
sitional representations (i.e., if VLMs employed a conjunctive coding scheme), there would be no
interference between the representations for different objects, and thus there would be no binding
problem. The presence of the binding problem, therefore, implies the presence of compositional
representations in VLMs."
DISCUSSION,0.33490566037735847,"It is worth considering how VLMs might be improved so as to enable them to cope with the binding
problem. One might think that this could be accomplished through additional fine-tuning on multi-
object tasks. However, the theoretical perspective outlined above suggests that, to the extent this
alleviates the binding problem, it would do so by eliminating the use of compositional representations,
which would then have negative impacts on generalization. The question then is how VLMs might be
enhanced to solve the binding problem, while preserving the benefits of compositional representations.
The most obvious possibility here is to augment VLMs with mechanisms for serial processing of
images, of the sort that enable human reasoners to manage the binding problem by selectively
attending to individual objects one at a time [22, 33]. A number of methods have been proposed for
sequential reasoning over images [12, 35], though none of these methods have yet been deployed
at the scale of VLMs. An alternative approach (which may be unique to artificial systems) involves
the use of slot-based methods for object-centric representation learning [4, 10, 16], which have been
shown to dramatically improve performance in visual reasoning tasks without requiring sequential
processing of images [8, 21], but which have also not been scaled to the level of current VLMs. It
remains to be seen whether and how these techniques might contribute to improved reasoning in
future VLMs, or whether new approaches will be needed to enable human-like visual reasoning."
LIMITATIONS & FUTURE DIRECTIONS,0.33962264150943394,"6.1
Limitations & Future Directions"
LIMITATIONS & FUTURE DIRECTIONS,0.3443396226415094,"This study has several limitations that should be considered when interpreting the results. First, we
limited our analysis to a relatively small set of tasks. The tasks in our study were selected to illustrate
the different settings in which the binding problem may impact performance, while grounding our
analysis in well known tasks from cognitive science that have been used to index such capacity
constraints in humans. Future work may examine a broader set of tasks such as matrix reasoning
tasks [3, 26, 39] that are more diagnostic of the reasoning failures arising due to issues with binding.
Second, we primarily investigated propietary VLMs, for which we do not have detailed knowledge of
their training data or architecture, or the ability to directly investigate their internal representations.
We chose to focus on these models because they reflect the best-performing current set of VLMs
(our experiments with the open-source Llava-1.5 yielded very poor performance on all tasks), but
continued progress in the development of open-source VLMs should make it possible to investigate
open-source models in future work. Finally, our work is focused particularly on characterizing the
capacity constraints of VLMs arising due to issues with feature binding. While we propose a naive
approach for improving performance by selectively processing sub-images independently, future
work may explore more flexible methods for decomposing complex, multi-object reasoning tasks,
especially by exploiting methods for object-centric representation learning [4, 5, 10, 16]."
ACKNOWLEDGEMENTS,0.3490566037735849,"7
Acknowledgements"
ACKNOWLEDGEMENTS,0.35377358490566035,"This work was supported in part by Microsoft Azure credits provided to Princeton University. D.C.
and T.G. are supported by the National Science Foundation Graduate Research Fellowship Program
(NSF GRFP)."
REFERENCES,0.3584905660377358,References
REFERENCES,0.3632075471698113,"[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
J. Altenschmidt, S. Altman, S. Anadkat, et al.
Gpt-4 technical report.
arXiv preprint
arXiv:2303.08774, 2023.
[2] O. Barak, M. Rigotti, and S. Fusi. The sparseness of mixed selectivity neurons controls the
generalization–discrimination trade-off. Journal of Neuroscience, 33(9):3844–3856, 2013.
[3] D. Barrett, F. Hill, A. Santoro, A. Morcos, and T. Lillicrap. Measuring abstract reasoning in
neural networks. In International conference on machine learning, pages 511–520. PMLR,
2018.
[4] C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Ler-
chner.
Monet: Unsupervised scene decomposition and representation.
arXiv preprint
arXiv:1901.11390, 2019.
[5] D. Chen, S. Cahyawijaya, J. Liu, B. Wang, and P. Fung. Subobject-level image tokenization,
2024.
[6] B. O. Community. Blender - a 3D modelling and rendering package. Blender Foundation,
Stichting Blender Foundation, Amsterdam, 2018.
[7] C. Conwell and T. Ullman. Testing relational understanding in text-guided image generation.
arXiv preprint arXiv:2208.00005, 2022.
[8] D. Ding, F. Hill, A. Santoro, M. Reynolds, and M. Botvinick. Attention over learned object
embeddings enables complex visual reasoning. Advances in neural information processing
systems, 34:9112–9124, 2021.
[9] S. M. Frankland, T. Webb, and J. D. Cohen. No coincidence, george: Capacity-limits as the
curse of compositionality. 2021.
[10] K. Greff, S. Van Steenkiste, and J. Schmidhuber. On the binding problem in artificial neural
networks. arXiv preprint arXiv:2012.05208, 2020.
[11] K. J. Holyoak. Analogy and relational reasoning. The Oxford handbook of thinking and
reasoning, pages 234–259, 2012.
[12] D. A. Hudson and C. D. Manning. Compositional attention networks for machine reasoning.
arXiv preprint arXiv:1803.03067, 2018.
[13] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick.
Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901–
2910, 2017.
[14] E. L. Kaufman, M. W. Lord, T. W. Reese, and J. Volkmann. The discrimination of visual number.
The American journal of psychology, 62(4):498–525, 1949.
[15] M. Lewis, N. V. Nayak, P. Yu, Q. Yu, J. Merullo, S. H. Bach, and E. Pavlick. Does clip bind
concepts? probing compositionality in large image models. arXiv preprint arXiv:2212.10537,
2022.
[16] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Doso-
vitskiy, and T. Kipf. Object-centric learning with slot attention. Advances in neural information
processing systems, 33:11525–11538, 2020.
[17] G. Mandler and B. J. Shebo. Subitizing: an analysis of its component processes. Journal of
experimental psychology: general, 111(1):1, 1982.
[18] B. McElree and M. Carrasco. The temporal dynamics of visual search: evidence for parallel
processing in feature and conjunction searches. Journal of Experimental Psychology: Human
Perception and Performance, 25(6):1517, 1999.
[19] G. A. Miller. The magical number seven, plus or minus two: Some limits on our capacity for
processing information. Psychological review, 63(2):81, 1956.
[20] M. Mitchell, A. B. Palmarini, and A. Moskvichev. Comparing humans, gpt-4, and gpt-4v on
abstraction and reasoning tasks. arXiv preprint arXiv:2311.09247, 2023.
[21] S. S. Mondal, T. Webb, and J. D. Cohen. Learning to reason over visual objects. arXiv preprint
arXiv:2303.02260, 2023."
REFERENCES,0.36792452830188677,"[22] S. Musslick, A. Saxe, A. N. Hoskin, Y. Sagiv, D. Reichman, G. Petri, and J. D. Cohen. On the
rational boundedness of cognitive control: Shared versus separated representations. 2023.
[23] P. Rahmanzadehgervi, L. Bolton, M. R. Taesiri, and A. T. Nguyen. Vision language models are
blind. arXiv preprint arXiv:2407.06581, 2024.
[24] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image
generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.
[25] S. Rane, A. Ku, J. M. Baldridge, I. Tenney, T. L. Griffiths, and B. Kim. Can generative
multimodal models count to ten?
In ICLR 2024 Workshop on Reliable and Responsible
Foundation Models.
[26] J. C. Raven. Progressive matrices: A perceptual test of intelligence, individual form. London:
Lewis, 1938.
[27] S. K. Revkin, M. Piazza, V. Izard, L. Cohen, and S. Dehaene. Does subitizing reflect numerical
estimation? Psychological science, 19(6):607–614, 2008.
[28] P. R. Roelfsema. Solving the binding problem: Assemblies form when neurons enhance their
firing rate—they don’t need to oscillate or synchronize. Neuron, 111(7):1003–1019, 2023.
[29] A. L. Roskies. The binding problem. Neuron, 24(1):7–9, 1999.
[30] R. E. Snow, P. C. Kyllonen, B. Marshalek, et al. The topography of ability and learning
correlations. Advances in the psychology of human intelligence, 2(S 47):103, 1984.
[31] T. Thrush, R. Jiang, M. Bartolo, A. Singh, A. Williams, D. Kiela, and C. Ross. Winoground:
Probing vision and language models for visio-linguistic compositionality, 2022.
[32] A. Treisman and H. Schmidt. Illusory conjunctions in the perception of objects. Cognitive
psychology, 14(1):107–141, 1982.
[33] A. M. Treisman and G. Gelade. A feature-integration theory of attention. Cognitive psychology,
12(1):97–136, 1980.
[34] L. M. Trick and Z. W. Pylyshyn. Why are small and large numbers enumerated differently? a
limited-capacity preattentive stage in vision. Psychological review, 101(1):80, 1994.
[35] M. Vaishnav and T. Serre. Gamr: A guided attention model for (visual) reasoning. arXiv
preprint arXiv:2206.04928, 2022.
[36] C. Von Der Malsburg. The correlation theory of brain function. In Models of neural networks:
Temporal aspects of coding and information processing in biological systems, pages 95–119.
Springer, 1994.
[37] T. Webb, K. J. Holyoak, and H. Lu. Emergent analogical reasoning in large language models.
Nature Human Behaviour, 7(9):1526–1541, 2023.
[38] E. Yiu, M. Qraitem, C. Wong, A. N. Majhi, Y. Bai, S. Ginosar, A. Gopnik, and K. Saenko.
Kiva: Kid-inspired visual analogies for testing large multimodal models. arXiv preprint
arXiv:2407.17773, 2024.
[39] C. Zhang, F. Gao, B. Jia, Y. Zhu, and S.-C. Zhu. Raven: A dataset for relational and analogical
visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 5317–5327, 2019.
[40] C. Zhang and S. Wang. Good at captioning, bad at counting: Benchmarking gpt-4v on earth
observation data. arXiv preprint arXiv:2401.17600, 2024."
REFERENCES,0.37264150943396224,"A
Appendix: Supplementary Figures"
D RESULTS,0.37735849056603776,2D Results
D RESULTS,0.38207547169811323,3D Results
D RESULTS,0.3867924528301887,"Figure 5: Visual search model results. Individual model performance for 2D and 3D visual search
tasks. Error bars denote 95% binomial confidence intervals."
D RESULTS,0.3915094339622642,"Visual Search
Numerosity Estimation"
D RESULTS,0.39622641509433965,"Figure 6: LLaVA 1.5 Performance. Performance of LLaVA 1.5 on the visual search and numerosity
estimation tasks. Performance was substantially weaker than all other models evaluated."
D RESULTS,0.4009433962264151,"Figure 7: Visual search results with additional control experiment. Results for Claude Sonnet
3.5 on 2D visual search tasks, including disjunctive and conjunctive conditions, and an additional
disjunctive search condition (‘Disjunctive Search Control’) in which target and distractor colors were
varied between trials."
D RESULTS,0.4056603773584906,"2D Results
3D Results"
D RESULTS,0.41037735849056606,"Figure 8: Numerosity estimation model results. Individual model performance for 2D and 3D
numerosity estimation tasks. Error bars denote 95% binomial confidence intervals."
D RESULTS,0.41509433962264153,"B
Appendix: Prompts for Vision-Language Model Experiments"
D RESULTS,0.419811320754717,"B.1
Numerical Estimation"
D RESULTS,0.42452830188679247,1. 2D VLM Prompt
D RESULTS,0.42924528301886794,"You are
presented
with an image
containing
several
objects. Your
task is to
accurately
count
the
number
of
objects
in the
image. Follow
these
instructions
carefully:
1. Begin by
describing
each
object in the
image.
2.
Conclude
your
response
by
providing
the
total
count of
objects
as an integer
, enclosed
in square
brackets. Only
the
number
should be
enclosed
in
square
brackets."
D RESULTS,0.4339622641509434,2. 3D VLM Prompt
D RESULTS,0.4386792452830189,"You are
presented
with an image
containing
several
shapes. Your
task is to
accurately
count
the
number
of shapes in the
image. Follow
these
instructions
carefully:
1. Begin by
describing
each
shape in the
image.
2.
Conclude
your
response
by
providing
the
total
count of shapes as an integer ,
enclosed
in square
brackets. Only
the
number
should be
enclosed
in
square
brackets."
D RESULTS,0.44339622641509435,3. T2I Prompt
D RESULTS,0.4481132075471698,"Render
an image
with
exactly {n} { object_name }, each
distinctly
separated
and
easily
countable , arranged
against a uniform
background
in
photorealistic
style."
D RESULTS,0.4528301886792453,"B.2
Visual Search"
D RESULTS,0.45754716981132076,1. 2D Disjunctive Search Prompt
D RESULTS,0.46226415094339623,"You are
presented
with an image
containing
several
shapes. Your
task is to
determine
if there
are any
green
shapes in the
image. Follow
these
steps
carefully:
1.
Describe
each
shape in the image , noting
their
color.
2.
Conclude
your
response
by
stating [True] if there
are any
green
shapes , or [
False] if there
are
none. Enclose
your
final
answer in square
brackets , as
shown."
D RESULTS,0.4669811320754717,2. 3D Disjunctive Search Prompt
D RESULTS,0.4716981132075472,"You are
presented
with an image
containing
several
objects. Your
task is to
determine
if there
are any red
objects
in the
image. Follow
these
steps
carefully:
1.
Describe
each
object in the image , noting
their
color.
2.
Conclude
your
response
by
stating [True] if there
are any red objects , or [
False] if there
are
none. Enclose
your
final
answer in square
brackets , as
shown."
D RESULTS,0.47641509433962265,3. 2D Disjunctive Search (Variable Color) Prompt
D RESULTS,0.4811320754716981,"You are
presented
with an image
containing
several
shapes. Your
task is to
determine
if all the
shapes
are the
same
color. Follow
these
steps
carefully:
1.
Describe
each
shape in the image , noting
their
color.
2.
Conclude
your
response
by
stating [True] if all
shapes
are the
same color ,
or [False] if there is an ""oddball"" shape
that is a different
color. Enclose
your
final
answer
in square
brackets , as shown."
D RESULTS,0.4858490566037736,4. 2D Conjunctive Search
D RESULTS,0.49056603773584906,"You are
presented
with an image
containing a set of letters , specifically
the
letters
’L’ and ’T’. These
letters
will
appear in
either
red or green.
Your
task is to
determine
if there
are any
green ’L’s in the
image. Follow
these
steps
carefully:
1.
Describe
each
shape in the image , noting
their
color.
2.
Conclude
your
response
by
stating [True] if the
letter
’L’ appears
in green ,
or [False] if there
are no green ’L’s. Enclose
your
final
answer
in
square
brackets , as shown."
D RESULTS,0.49528301886792453,5. 3D Conjunctive Search Prompt
D RESULTS,0.5,"You are
presented
with an image
containing a set of objects , specifically
spheres
and
cubes. These
objects
will
appear in either
red or green.
Your
task is to
determine
if there
are any red
spheres
in the
image. Follow
these
steps
carefully:
1.
Describe
each
object in the image , noting
their
color.
2.
Conclude
your
response
by
stating [True] if a red
sphere is present , or [
False] if there
are
none. Enclose
your
final
answer in square
brackets , as
shown."
D RESULTS,0.5047169811320755,"B.3
Scene Description"
D RESULTS,0.5094339622641509,1. 2D VLM Prompt
D RESULTS,0.5141509433962265,"The
following
image
contains
multiple
simple , colored
objects.
The
possible
shapes
that
may be
present
in the
image
are: <airplane , triangle ,
cloud , X-shape , umbrella , pentagon , heart , star , circle , square , spade ,
scissors , infinity , check mark , right -arrow >.
The
possible
colors
that
may be
present
in the
image
are: <red , magenta , salmon
, green , lime , olive , blue , teal , yellow , purple , brown , gray , black , cyan ,
orange >.
Describe
each
object in the
image in the
form of a JSON
object
detailing
the
color
and
shape of each
item.
You
must
answer
only
with
the
json
array of objects , without
any
additional
information
or text.
For
example , if the
image
contains a purple
check mark , two
green
scissors , one
orange
right -arrow , and a teal
infinity
sign
you
would
write: ["
D RESULTS,0.5188679245283019,"{"" shape "": ""check
mark"", ""color "": ""purple ""},
{"" shape "": ""scissors"", ""color "": ""green ""},
{"" shape "": ""scissors"", ""color "": ""green ""},
{"" shape "": ""right -arrow"", ""color "": ""orange ""},
{"" shape "": ""infinity"", ""color "": ""teal ""}
]"
D RESULTS,0.5235849056603774,2. 3D VLM Prompt
D RESULTS,0.5283018867924528,"The
following
image
contains
multiple
simple , colored
objects.
The
possible
shapes
that
may be
present
in the
image
are: <cone , cylinder , bowl
, donut , sphere , cube , droplet , bowling -pin , coil , crown , snowman , spikey -
ball >.
The set of
colors
that
may be
present
in the
image
are: <red , green , blue ,
yellow , purple , light
green , gray , black , light blue , pink , teal , brown >.
Describe
each
object in the
image in the
form of a JSON
object , detailing
the
color
and
shape of each
item.
You
must
answer
only
with
the
json
array of objects , without
any
additional
information
or text.
For
example , if the
image
contains a brown cube , two
green
donuts , and a cyan
spikey -ball , you
would
write: ["
D RESULTS,0.5330188679245284,"{"" shape "": ""cube"", ""color "": ""brown ""},
{"" shape "": ""donut"", ""color "": ""green ""},
{"" shape "": ""donut"", ""color "": ""green ""},
{"" shape "": ""spikey -ball"", ""color "": ""cyan ""}
]"
D RESULTS,0.5377358490566038,3. T2I Prompt
D RESULTS,0.5424528301886793,"Render
an image in
photorealistic
style
with
exactly { objects_string } arranged
against a uniform
background , each
distinctly
separated. Include
only
these
objects
in the
image
and
nothing
else."
D RESULTS,0.5471698113207547,"B.4
Relational Match to Sample (RMTS)"
D RESULTS,0.5518867924528302,1. Full Task – Unified Condition
D RESULTS,0.5566037735849056,"The
following
image
depicts a trial of a relational
match to sample
task
with
two
features: shape
and
color.
There
are
three
pairs of
objects
relevant
to your
task: the
source
pair (the
top
pair of
objects), target
pair #1 (the
pair of
objects
on bottom
left),
and
target
pair
(the
pair of
objects
on the
bottom
right).
Now , given
the
source
pair
and the two
target
pairs , identify
the
matching
target
pair. To
accomplish
this task , you can use the
following
steps:
1.
Identify
the
features
of the
objects
in each
pair (i.e. shape
and
color).
2.
Identify
the
relations
over
the
features
of each
pair.
3.
Determine
which
target
pair
shares
the
same
relations
with
the
source
pair
-- exactly
one
target
pair
will
share
the
same
relations
with
the
source.
4.
Conclude
with
the
integer
of the
correct
target
pair
wrapped
in square
brackets. For
example , if
target
pair #1
matches
the
source
pair , return
[1]."
D RESULTS,0.5613207547169812,2. Full Task – Decomposed Condition
D RESULTS,0.5660377358490566,"The
following
images
depict a trial of a relational
match to sample
task
with
two
features: shape
and
color.
There
are
three
pairs of
objects
relevant
to your
task: the
source pair , target
pair #1, and
target
pair
#2.
Now , given
the
source
pair
and the two
target
pairs , identify
the
matching
target
pair. To
accomplish
this task , you can use the
following
steps:
1.
Identify
the
features
of the
objects
in each
pair (i.e. shape
and
color).
2.
Identify
the
relations
over
the
features
of each
pair.
3.
Determine
which
target
pair
shares
the
same
relations
with
the
source
pair
-- exactly
one
target
pair
will
share
the
same
relations
with
the
source.
4.
Conclude
with
the
integer
of the
correct
target
pair
wrapped
in square
brackets. For
example , if
target
pair #1
matches
the
source
pair , return
[1]."
D RESULTS,0.5707547169811321,3. Relation Decoding – Unified condition
D RESULTS,0.5754716981132075,"Are the two
objects
in the {pair} pair (the {pair_loc} pair) the
same {relation
}?
Your
answer
should be [True] if the
objects
have
the
same {relation} and [False
] if they
have a different {relation }.
Ensure
that
your
final
answer is
wrapped
in square
brackets."
D RESULTS,0.5801886792452831,4. Relation Decoding – Decomposed condition
D RESULTS,0.5849056603773585,"Are the two
objects
in the {pair} pair (the {pair_loc} pair) the
same {relation
}?
Your
answer
should be [True] if the
objects
have
the
same {relation} and [False
] if they
have a different {relation }.
Ensure
that
your
final
answer is
wrapped
in square
brackets."
D RESULTS,0.589622641509434,5. Single Feature Decoding – Unified condition
D RESULTS,0.5943396226415094,What is the {feature} of the { object_loc } ({ object_ind }) object in the {pair}
D RESULTS,0.5990566037735849,"pair? Only
provide
the {feature }.
Your
response
should be a single
word
answer
wrapped
in square
brackets.
For
instance , if I ask for the
color of a red object , you
should
return [red]
and
nothing
else. If I ask for the
shape of an object
that is a circle , you
should
return [circle ].
- Valid
shapes
include: triangle , cloud , cross , heart , circle , square.
- Valid
colors
include: red , green , blue , darkorange , purple , and
gray."
D RESULTS,0.6037735849056604,6. Single Feature Decoding – Decomposed condition
D RESULTS,0.6084905660377359,What is the {feature} of the { object_loc } ({ object_ind }) object in the {pair}
D RESULTS,0.6132075471698113,"pair? Only
provide
the {feature }.
Your
response
should be a single
word
answer
wrapped
in square
brackets.
For
instance , if I ask for the
color of a red object , you
should
return [red]
and
nothing
else. If I ask for the
shape of an object
that is a circle , you
should
return [circle ].
- Valid
shapes
include: triangle , cloud , cross , heart , circle , square.
- Valid
colors
include: red , green , blue , darkorange , purple , and
gray."
D RESULTS,0.6179245283018868,7. All Feature Decoding – Unified condition
D RESULTS,0.6226415094339622,"Examine
the
image
provided , which
depicts
six basic , colored
shapes
arranged
into
three
distinct
pairs of
objects: the
source
pair at the top , target
pair
#1 on the
bottom
left , and
target
pair #2 on the
bottom
right.
For
each pair , identify
the
shapes
as
follows: ""object1"" refers to the left -
most
object in the pair , and the ""object2"" to the right -most
object in the
pair.
Return
the
color
and
shape of each
object in the
trial in the
json
format
described
below.
- Valid
shapes: triangle , cloud , cross , heart , circle , square.
- Valid
colors: red , green , blue , darkorange , purple , and
gray."
D RESULTS,0.6273584905660378,"Your
response
should be in the
following
format:
{"
D RESULTS,0.6320754716981132,source: {
D RESULTS,0.6367924528301887,"source_object1 : {shape: circle , color: purple},
source_object2 : {shape: circle , color: purple}
},
target1: {"
D RESULTS,0.6415094339622641,"target1_object1 : {shape: triangle , color: brown},
target1_object2 : {shape: triangle , color: brown}
},
{"
D RESULTS,0.6462264150943396,"target2_object1 : {shape: square , color: green},
target2_object2 : {shape: square , color: black}
}
}"
D RESULTS,0.6509433962264151,Response:
D RESULTS,0.6556603773584906,8. All Feature Decoding – Decomposed condition
D RESULTS,0.660377358490566,"Examine
the
three
images
provided , which
depict
six basic , colored
shapes
arranged
into
three
distinct
pairs of
objects: the
source pair , target
pair
#1, and
target
pair
#2.
For
each pair , identify
the
shapes
as
follows: ""object1"" refers to the left -
most
object in the pair , and the ""object2"" to the right -most
object in the
pair.
Return
the
color
and
shape of each
object in the
trial in the
json
format
described
below.
- Valid
shapes: triangle , cloud , cross , heart , circle , square.
- Valid
colors: red , green , blue , darkorange , purple , and
gray."
D RESULTS,0.6650943396226415,"Your
response
should be in the
following
format:
{"
D RESULTS,0.6698113207547169,source: {
D RESULTS,0.6745283018867925,"source_object1 : {shape: circle , color: purple},
source_object2 : {shape: circle , color: purple}
},
target1: {"
D RESULTS,0.6792452830188679,"target1_object1 : {shape: triangle , color: brown},
target1_object2 : {shape: triangle , color: brown}
},
{"
D RESULTS,0.6839622641509434,"target2_object1 : {shape: square , color: green},
target2_object2 : {shape: square , color: black}
}
}"
D RESULTS,0.6886792452830188,Response:
D RESULTS,0.6933962264150944,"C
Appendix: Human Evaluations"
D RESULTS,0.6981132075471698,"(a) Binding task evaluation
(b) Counting task evaluation"
D RESULTS,0.7028301886792453,Figure 9: Human Evaluation
D RESULTS,0.7075471698113207,"Human evaluations of the text-to-image variants of the counting and binding tasks were conducted
on Prolific. Participants were asked to count the number of objects in each T2I counting task image,
or asked to match objects to provided labels in each T2I scene description task image. They were
also asked to list extraneous objects (generated objects not in the input prompt) when evaluating the
binding images. Participants were paid a total estimated wage of approximately $12/hour, and total
compensation for the entirety of the human evaluations was approximately $600. All participants
provided informed consent. The study was approved by the Princeton University IRB."
D RESULTS,0.7122641509433962,"D
Appendix: Computational Resources"
D RESULTS,0.7169811320754716,"Experiments were conducted on closed source models, and therefore did not require any specialized
hardware. Generation of the 3D variants of the task were more computationally intensive and were
performed in parallel on a local cluster, with 16GB RAM allocated per CPU per job."
D RESULTS,0.7216981132075472,NeurIPS Paper Checklist
CLAIMS,0.7264150943396226,1. Claims
CLAIMS,0.7311320754716981,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: All claims made in the abstract and introduction are supported by behavioral
evaluation or models and comparisons to previously published work in cognitive science.
Guidelines:"
CLAIMS,0.7358490566037735,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.7405660377358491,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We added a dedicated limitations paragraph to the Discussion (Section 6).
Guidelines:"
CLAIMS,0.7452830188679245,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.75,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]"
CLAIMS,0.7547169811320755,"Justification: Our work does not involve any novel assumptions that require formal justifica-
tion."
CLAIMS,0.7594339622641509,Guidelines:
CLAIMS,0.7641509433962265,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7688679245283019,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7735849056603774,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7783018867924528,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7830188679245284,"Justification: All details regarding reproducing our experiments and controls are thoroughly
described in the methods sections in the text and the Appendix. Furthermore, code for
reproducing our experiments in the zip file attached to the OpenReview submission."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7877358490566038,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7924528301886793,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.7971698113207547,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8018867924528302,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8066037735849056,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8113207547169812,"Justification: All experiment code is provided in the submission’s attached zip file. We will
provide a documented version of the code upon publication."
OPEN ACCESS TO DATA AND CODE,0.8160377358490566,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8207547169811321,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8254716981132075,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8301886792452831,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8349056603773585,Answer: [NA]
OPEN ACCESS TO DATA AND CODE,0.839622641509434,Justification: All methods for testing models is specified in the text and Appendix.
OPEN ACCESS TO DATA AND CODE,0.8443396226415094,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8490566037735849,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8537735849056604,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8584905660377359,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8632075471698113,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8679245283018868,Justification: We report error bars and significance tests for all experiments.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8726415094339622,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8773584905660378,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8820754716981132,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8867924528301887,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Details on compute are listed in the Appendix (Section C).
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8915094339622641,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8962264150943396,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We reviewed the Code of Ethics and confirm that this paper conforms with it.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9009433962264151,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9056603773584906,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: These are discussed in Section 6.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.910377358490566,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9150943396226415,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9198113207547169,11. Safeguards
SAFEGUARDS,0.9245283018867925,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9292452830188679,Answer: [NA]
SAFEGUARDS,0.9339622641509434,Justification: The paper poses no such risks as we do not release new models.
SAFEGUARDS,0.9386792452830188,Guidelines:
SAFEGUARDS,0.9433962264150944,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9481132075471698,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9528301886792453,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9575471698113207,"Answer: [Yes]
Justification: All models used in the present work are available through commercial APIs,
and dataset generation code is shared in the submission."
LICENSES FOR EXISTING ASSETS,0.9622641509433962,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9669811320754716,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9716981132075472,"• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9764150943396226,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We release all datasets for GPT-4v and all images generated by DALL-E
3 at the following anonymized OSF repository: https://osf.io/xq9j8/?view_only=
57ed8b3838f549758686be8e239c2e5e
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9811320754716981,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9858490566037735,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [Yes]
We include details about human evaluations in the Appendix C.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9905660377358491,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [Yes]
Justification: IRB approval was obtained for our studies, and we include a discussion of
potential risks in the human evaluations in Appendix C.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9952830188679245,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
