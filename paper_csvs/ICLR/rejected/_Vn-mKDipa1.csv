Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001968503937007874,"Hierarchical forecasting is a key problem in many practical multivariate forecasting
applications - the goal is to simultaneously predict a large number of correlated
time series that are arranged in a pre-speciﬁed aggregation hierarchy. The main
challenge is to exploit the hierarchical correlations to simultaneously obtain good
prediction accuracy for time series at different levels of the hierarchy. In this
paper, we propose a new approach for hierarchical forecasting which consists
of two components. First, decomposing the time series along a global set of
basis time series and modeling hierarchical constraints using the coefﬁcients of
the basis decomposition. And second, using a linear autoregressive model with
coefﬁcients that vary with time. Unlike past methods, our approach is scalable
(inference for a speciﬁc time series only needs access to its own history) while
also modeling the hierarchical structure via (approximate) coherence constraints
among the time series forecasts. We experiment on several public datasets and
demonstrate signiﬁcantly improved overall performance on forecasts at different
levels of the hierarchy, compared to existing state-of-the-art hierarchical models."
INTRODUCTION,0.003937007874015748,"1
INTRODUCTION"
INTRODUCTION,0.005905511811023622,"Multivariate time series forecasting is a key problem in many domains such as retail demand
forecasting (B¨ose et al., 2017), ﬁnancial predictions (Zhou et al., 2020), power grid optimization
(Hyndman & Fan, 2009), road trafﬁc modeling (Li et al., 2017), and online ads optimization (Cui
et al., 2011). In many of these setting, the problem involves simultaneously forecasting a large
number of possibly correlated time series for various downstream applications. In the retail domain,
the time series may capture sales of items in a product inventory, and in power grids, the time series
may correspond to energy consumption in a household. Often, these time series are arranged in a
natural multi-level hierarchy - for example in retail forecasting, items are grouped into subcategories
and categories, and arranged in a product taxonomy. In the case of power consumption forecasting,
individual households are grouped into neighborhoods, counties, and cities. The hierarchical structure
among the time series can usually be represented as a tree, with the leaf nodes corresponding to
time series at the ﬁnest granularity, and the edges representing parent-child relationships. Figure 1
illustrates a typical hierarchy in the retail forecasting domain for time series of product sales."
INTRODUCTION,0.007874015748031496,"Figure 1: An example hierarchy
for retail demand forecasting. The
blue triangle represents the sub-
tree rooted at the node Store1 with
leaves denoted by Item i."
INTRODUCTION,0.00984251968503937,"In such settings, it is often required to obtain good forecasts,
not just for the leaf level time-series (ﬁne grained forecasts),
but also for the aggregated time-series corresponding to higher
level nodes (coarse gained forecasts). Furthermore, for inter-
pretability and business decision making purposes, it is often
desirable to obtain predictions that are roughly coherent or con-
sistent (Hyndman et al., 2011) with respect to the hierarchy tree.
This means that the predictions for each parent time-series is
equal to the sum of the predictions for its children time-series.
More importantly, incorporating coherence constraints in a hi-
erarchical forecasting model captures the natural inductive bias
in most hierarchical datasets, where the ground truth parent and
children time series indeed adhere to additive constraints. For
example, total sales of a product category is equal to the sum
of sales of all items in that category."
INTRODUCTION,0.011811023622047244,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.013779527559055118,"Some standard approaches for hierarchical forecasting include bottom-up aggregation, or
reconciliation-based approaches. Bottom-Up aggregation involves training a model to obtain pre-
dictions for the leaf nodes, and then aggregate up along the hierarchy tree to obtain predictions for
higher-level nodes. Reconciliation methods (Ben Taieb & Koo, 2019; Taieb et al., 2017; Van Erven &
Cugliari, 2015; Hyndman et al., 2016; Wickramasuriya et al., 2015; 2020; Panagiotelis et al., 2020)
make use of a trained model to obtain predictions for all nodes of the tree, and then, in a separate
post-processing phase, reconcile (or modify) them using various optimization formulations to obtain
coherent predictions. Both of these approaches suffer from shortcomings in term of either aggregating
noise as one moves to higher level predictions (bottom-up aggregation), or not jointly optimizing the
forecasting predictions along with the coherence constraints (for instance, reconciliation)."
INTRODUCTION,0.015748031496062992,"At the same time, there have been several recent advances on using Deep Neural Network models for
multivariate forecasting, including Recurrent Neural Network (RNN), Convolutional Neural Network
(CNN) architectures (Salinas et al., 2020; Oreshkin et al., 2019; Rangapuram et al., 2018; Benidis
et al., 2020), and models designed for multivariate time series based on dimensionality reduction
techniques (Sen et al., 2019; Wang et al., 2019; Nguyen & Quanz, 2021; Salinas et al., 2019; Rasul
et al., 2020; de B´ezenac et al., 2020), that have been shown to outperform classical time-series
models such as autoregressive and exponential smoothing models (McKenzie, 1984; Hyndman et al.,
2008; Hyndman & Athanasopoulos, 2018), especially for large datasets. However, most of these
approaches do not explicitly address the question of how to model the hierarchical relationships in
the dataset. Deep forecasting models based on Graph Neural Networks (GNN) (Bai et al., 2020; Cao
et al., 2020; Yu et al., 2017; Li et al., 2017; Wu et al., 2020) do offer a general framework for learning
on graph-structured data. However it is well known (Bojchevski et al., 2020) that GNNs are hard to
scale for learning on graphs with a very large number of nodes, which in real-world settings such as
retail forecasting, could involve hundreds of thousands of time series. More importantly, a desirable
practical feature for multi-variate forecasting models is to let the prediction of future values for a
particular time series only require as input historical data from that time series (along with covariates),
without requiring access to historical data from all other time series in the hierarchy. This allows for
scalable training and inference of such models using mini-batch gradient descent, without requiring
each batch to contain all the time series in the hierarchy. This is often not possible for GNN-based
forecasting models, which require batch sizes of the order of the number of time series."
INTRODUCTION,0.017716535433070866,"Problem Statement: Based on the above motivations, our goal is to design a hierarchical forecasting
model with the following requirements: 1) The model can be trained using a single-stage pipeline
on all the time series data, without any separate post-processing, 2) The model captures the additive
coherence constraints along the edges of the hierarchy, 3) The model is efﬁciently trainable on large
datasets, without requiring, for instance, batch sizes that scale with the number of time series."
INTRODUCTION,0.01968503937007874,"We propose a principled methodology to address all these above requirements for hierarchical fore-
casting. Our model comprises of two components, both of which can support coherence constraints.
The ﬁrst component is purely a function of the historical values of a time series, without distin-
guishing between the individual time series themselves in any other way. Coherence constraints
on such a model correspond to imposing an additivity property on the prediction function - which
constrains the model to be a linear autoregressive model. However, crucially, our model uses time-
varying autoregressive coefﬁcients that can themselves be nonlinear functions of the timestamp and
other global features (linear versions of time-varying AR have been historically used to deal with
non-stationary signals (Sharman & Friedlander, 1984)). We will refer to this component as the
time-varying autoregressive model."
INTRODUCTION,0.021653543307086614,"The second component focuses on modeling the global temporal patterns in the dataset through
identifying a small set of temporal global basis functions. The basis time-series, when combined
in different ways, can express the individual dynamics of each time series. In our model, the basis
time-series are encoded in a trained seq-2-seq model (Sutskever et al., 2014) model in a functional
form. Each time series is then associated with a learned embedding vector that speciﬁes the weights
for decomposition along these basis functions. Predicting a time series into the future using this
model then just involves extrapolating the global basis functions and combining them using its weight
vector, without explicitly using the past values of that time series. The coherence constraints therefore
only impose constraints on the embedding vectors of each time series, which can be easily modeled
by a hierarchical regularization function. We call this component a basis decomposition model. As we
will see, this part of the model is only approximately coherent unless the embedding constraints hold
exactly. In particular, in this paper, we focus on improving model accuracy rather than preserving"
INTRODUCTION,0.023622047244094488,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.025590551181102362,"exact coherency. In Section A.2, we also provide theoretical justiﬁcation for how such hierarchical
regularization using basis decomposition results in improved prediction accuracy."
INTRODUCTION,0.027559055118110236,"We experimentally evaluate our model on multiple publicly available hierarchical forecasting datasets.
We compare our approach to state-of-the-art (non-hierarchical) deep forecasting models, GNN-based
models and reconciliation models, and show that our approach can obtain consistently more accurate
predictions at all levels of the hierarchy tree."
RELATED WORK ON DEEP HIERARCHICAL MODELS,0.02952755905511811,"2
RELATED WORK ON DEEP HIERARCHICAL MODELS"
RELATED WORK ON DEEP HIERARCHICAL MODELS,0.031496062992125984,"In addition to the works referenced in the previous section, we now discuss a few papers that are more
relevant to our approach. Speciﬁcally, we discuss some recent deep hierarchical forecasting methods
that do not require a post-processing reconciliation step. Hierarchical forecasting methods can be
roughly divided into two categories: point forecasters and probabilistic forecasters. Mishchenko
et al. (2019) propose a point-forecasting approach which imposes coherency on a base model via
ℓ2 regularization on the predictions. Gleason (2020) extend the idea further to impose the hierarchy
on an embedding space rather than the predictions directly. SHARQ (Han et al., 2021) follows a
similar ℓ2 regularization based approach as Mishchenko et al. (2019), and also extends the idea to
probabilistic forecasting. Their model is trained separately for each of the hierarchical levels starting
from the leaf level, thus requiring a separate prediction model for each level."
RELATED WORK ON DEEP HIERARCHICAL MODELS,0.03346456692913386,"Probabilistic forecasting methods include, Hier-E2E (Rangapuram et al., 2021) which produces
perfectly coherent forecasts by using a projection operation on base predictions from a DeepVAR
model (Salinas et al., 2019). It requires the whole hierarchy of time series to be fed as input to the
model leading to a large number of parameters, and hence does not scale well to large hierarchies.
Yanchenko et al. (2021) take a fully Bayesian approach by modelling the hierarchy using conditional
distributions."
PROBLEM SETTING,0.03543307086614173,"3
PROBLEM SETTING"
PROBLEM SETTING,0.03740157480314961,"We are given a set of N coherent time series of length T, arranged in a pre-deﬁned hierarchy
consisting of N nodes. At time step t, the time series data can be represented as a vector yt ∈RN
denoting the time series values of all N nodes. We compactly denote the set of time series for all
T steps as a matrix Y = [y1, · · · , yT ]⊤∈RT ×N. Also deﬁne y(i) as the ith column of the matrix
Y denoting all time steps of the i th time series, and y(i)
t
as the t th value of the i th time series.
We compactly denote the H-step history of Y by YH = [yt−H, · · · , yt−1]⊤∈RH×N and the
H-step history of y(i) by y(i)
H = [y(i)
t−H, · · · , y(i)
t−1] ∈RH. Similarly deﬁne the F-step future of Y
as YF = [yt, · · · , yt+F −1]⊤∈RF ×N. We use theb· notation to denote predicted values, for example
bYF, byF and byt."
PROBLEM SETTING,0.03937007874015748,"Time series forecasts can often be improved by using features as input to the model along with
historical time series. The features often evolve with time, for example, categorical features such as
type of holiday, or continuous features such as time of the day. We denote the matrix of such features
by X ∈RT ×D, where the t th row denotes the D-dimensional feature vector at the t time step. For
simplicity, we assume that the features are global, meaning that they are shared across all time series.
We similarly deﬁne XH and XF as above."
PROBLEM SETTING,0.04133858267716536,"Hierarchically Coherent Time Series.
We assume that the time series data are coherent, that is,
they satisfy the sum constraints over the hierarchy. The time series at each node of the hierarchy is
the equal to the sum of the time series of its children, or equivalently, equal to the sum of the leaf time
series of the sub-tree rooted at that node. Figure 1 shows an example of a sub-tree rooted at a node."
PROBLEM SETTING,0.04330708661417323,"As a result of aggregation, the data can have widely varying scales with the values at higher level
nodes being magnitudes higher than the leaf level nodes. It is well known that neural network training
is more efﬁcient if the data are similarly scaled. Hence, in this paper, we work with rescaled time
series data. The time series at each node is downscaled by the number of leaves in the sub-tree rooted
at the node, so that now they satisfy mean constraints rather than sum constraints described above.
Denote by L(p), the set of leaf nodes of the sub-tree rooted at p. Hierarchically coherent data satisfy"
PROBLEM SETTING,0.045275590551181105,Under review as a conference paper at ICLR 2022
PROBLEM SETTING,0.047244094488188976,"the following data mean property,"
PROBLEM SETTING,0.04921259842519685,"y(p) =
1
|L(p)| X"
PROBLEM SETTING,0.051181102362204724,"i∈L(p)
y(i)
(Data Mean Property).
(1)"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.0531496062992126,"4
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.05511811023622047,"We now introduce the two components in our model, namely the time-varying AR model and the basis
decomposition model. As mentioned in the introduction a combination of these two components
satisfy the three requirements in our problem statement. In particular, we shall see that the coherence
property plays a central role in both the components. For simplicity, in this section, all our equations
will be for forecasting one step into the future (F = 1), even though all the ideas trivially extend to
multi-step forecasting. The deﬁning equation of our model can be written as,"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.05708661417322835,"by(i)
F = f(y(i)
H , XH, XF, ZH, θi)"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.05905511811023622,"=

y(i)
H , a(XH, XF, ZH)"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.0610236220472441,"|
{z
}
Time varying AR (TVAR)"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.06299212598425197,"+

θi, b(XH, XF, ZH)"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.06496062992125984,"|
{z
}
Basis decomposition (BD) .
(2)"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.06692913385826772,"In the above equation, ZH is a latent state vector that contains some summary temporal informa-
tion about the whole dataset, and θi is the embedding/weight vector for time-series i in the basis
decomposition model. ZH can be a relatively low-dimensional temporally evolving variable that
represents some information about the global state of the dataset at a particular time. We use the
Non-Negative Matrix Factorization (NMF) algorithm by Gillis & Vavasis (2013) to select a small set
of representative time series that encode the global state. If the indices of the selected representative
time-series is denoted by {i1, · · · , iR} (R denotes the rank of the factorization), then we deﬁne
Z = [Y (i1), · · · , Y (iR)] ∈RT ×R. Note that we only feed the past values ZH as input to the model,
since future values are not available during forecasting. Also, note that the ﬁnal basis time-series can
be a non-linear function of ZH. In our experiments, R is tuned but is always much much less than N.
a and b are functions not dependent on y(i)
H and θi. We will provide more details as we delve into
individual components."
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.0688976377952756,"Time-Varying AR (TVAR): The ﬁrst part of the expression in equation 2 denoted by Time Varying
AR (TVAR) resembles a linear auto-regressive model with coefﬁcients a(XH, XF, ZH) ∈RH, that
are a function of the input features, and thus can change with time. The AR parameters of this
model are shared across all time series and hence do not encode any time-series speciﬁc information,
a drawback that is overcome by the Basis Decomposition part of our model. This component is
coherent by design because it is a shared linear AR model. However, even though the AR weights
are shared across all the time-series at a given time-point, they can crucially change with time, thus
lending more ﬂexibility to the model."
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.07086614173228346,"Implementation: In order to model the sequential nature of the data, we use an LSTM encoder
to encode the past XH and ZH. Then, we use a fully connected (FC) decoder for predicting the
auto-regressive weights. Similar to Wen et al. (2017)’s multi-horizon approach, we use a different
head of the decoder for each future time step resulting in a F-headed decoder producing F-step
predictions for TVAR weights. The decoder also takes as input the future covariates XF if available.
The produced weights are then multiplied (inner product) to the history to produce the ﬁnal TVAR
predictions. We illustrate this architecture in Figure 2 (right)."
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.07283464566929133,"Basis Decomposition (BD) with Hierarchical Regularization: Now we come to the second part
of our model in equation 2. As discussed before, this part of the model has per time-series adaptivity,
as different time-series can have different embeddings. It resembles an expansion of the time series
on a set of basis functions b(XH, XF, ZH) ∈RK, with the basis weights/embedding for time series
i denoted by θi ∈RK. Both the basis functions and the time series speciﬁc weights are learned from
the data, rather than ﬁxing a speciﬁc form such as Fourier or Wavelet basis."
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.07480314960629922,"The idea of using a basis has also been recently invoked in the time-series literature (Sen et al., 2019;
Wang et al., 2019). The basis recovered in the implementation of Wang et al. (2019) is allowed to
vary for each individual time-series and therefore is not a true basis. Sen et al. (2019) do explicitly
recover an approximate basis in the training set through low-rank matrix factorization regularized"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.07677165354330709,Under review as a conference paper at ICLR 2022
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.07874015748031496,"by a deep global predictive model alternatingly trained on the basis vectors, thus not amenable to
end-to-end optimization. We shall see that our model can be trained in an end-to-end manner."
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.08070866141732283,"Embedding Regularization for Approximate Coherency: The TVAR part of our model is coherent
by design due to its linearity. The BD model however requires the embeddings of the time-series to
satisfy the mean property along the hierarchy. This directly translates to coherency of the predictions
due to linearity with respect to θ."
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.08267716535433071,"θp =
1
|L(p)| X"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.08464566929133858,"i∈L(p)
θi
(Embedding Mean Property),
(3)"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.08661417322834646,We impose this constraint approximately via an ℓ2 regularization on the embedding.
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.08858267716535433,"Ereg(θ) = N
X p=1 X"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.09055118110236221,"i∈L(p)
∥θp −θi∥2
2.
(4)"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.09251968503937008,"The purpose of this regularizer is two fold. Firstly, we observe that, when the leaf embeddings are
kept ﬁxed, the regularizer is minimized when the embeddings satisfy the mean property (3), thus
encouraging coherency in the predictions. Secondly, it also encodes the inductive bias present in the
data corresponding to the hierarchical additive constraints. We provide some theoretical justiﬁcation
for this hierarchical regularization in Section 5."
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.09448818897637795,"Implementation: As before, we use an LSTM encoder to encode the past XH and ZH. Then, we use
the encoding from the encoder along with the future features XF (sequential in nature) and pass them
through an LSTM decoder to yield the F-step basis predictions which are then multiplied with the
embedding (inner product) to produce the ﬁnal BD predictions. We illustrate this architecture in the
top right of Figure 2. Thus, a functional representation of the basis time-series is implicitly maintained
within the trained weights of the basis generating seq-2-seq model. Note that the embeddings are also
trained in our end-to-end model. We illustrate this architecture in Figure 2 (left)."
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.09645669291338582,"We emphasize that the main ideas in our model are agnostic to the speciﬁc type of neural network
architecture used. For our experiments, we speciﬁcally use an LSTM architecture (Hochreiter &
Schmidhuber, 1997) for the encoder and decoder. Other types of architectures including transformers
(Vaswani et al., 2017) and temporal convolution networks (Borovykh et al., 2017) can also be used."
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.0984251968503937,"Loss Function: During training, we minimize the mean absolute error (MAE) of the predictions
along with the embedding regularization term introduced above (our method generalizes to other
losses too, such as mean square error, or mean absolute percentage error). For regularization weight
λE, and by(i)
F deﬁned as Eq (2), and Θ denoting the trainable parameters of a, b, our training loss
function is,
ℓ(Θ, θ) =
X i X"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.10039370078740158,"F
|y(i)
F −by(i)
F |"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.10236220472440945,"|
{z
}
Prediction loss"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.10433070866141732,"+
λEEreg(θ)
|
{z
}
Embedding regularization .
(5)"
HIERARCHICALLY REGULARIZED DEEP FORECASTING - HIRED,0.1062992125984252,"Note that the time-series dependent part of the loss function can be easily mini-batched and the
embeddings are not memory intensive."
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.10826771653543307,"5
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.11023622047244094,"In this section, we theoretically analyze the beneﬁts of modeling hierarchical constraints in a much
simpliﬁed setting, and show how it can result in provably improved accuracy, under some assumptions.
Since analyzing our actual deep non-linear model for an arbitrary hierarchical set of time series can
be complex, we make some simplifying assumptions to the problem and model. We assume that
all the time series in the dataset is a linear combination of a small set of basis time series. That is,
Y = Bθ + w, where B ∈RT ×K denotes the set of basis vectors, θ = [θ1, · · · , θN] ∈RK×N
denotes the set of weight vectors used in the linear combination for each time series, and w ∈RT ×N
denotes the noise matrix sampled i.i.d as w ∼N(0, σ2) for the leaf nodes. A classical example of
such a basis set can be a small subset of Fourier or Wavelet basis (Strang, 1993; van den Oord et al.,
2016) that is relevant to the dataset. Note that we ignore the TVAR model for the sake of analysis and
focus mainly on the BD model which includes the hierarchical regularization."
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.11220472440944881,Under review as a conference paper at ICLR 2022
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.1141732283464567,"Figure 2: In this ﬁgure we show the architectures of our two model components separately. On the
left we show the BD model, where the seq-2-seq model implicitly maintains the basis in a functional
form. Note that the time-series speciﬁc weights {θi} are also trained. On the right, we show the
TVAR model. The fully connected decoder has a different prediction head for each future time-point."
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.11614173228346457,Algorithm 1: Basis Recovery
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.11811023622047244,"Input: Observed y, basis dict ¯
B,
regularization parameter λL
Output: Estimated basis B
bα0 ←
argmin
α∈Rn
1
2T ∥y0 −¯
Bα∥2
2 + λL∥α∥1"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.12007874015748031,"Estimate support bS = {i | |bα0| > 0}
Estimate true basis B ←¯
BbS"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.1220472440944882,"Algorithm 2: Parameter Recovery
Input: Observed time series y, estimated basis B,
regularization parameter λE
Output: Estimated parameters θ
bθ0 ←argminθ0
1
T ∥y0 −Bθ0∥2
2
for n ∈L(0) do"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.12401574803149606,"bθn ←argminθn
1
T ∥yn−Bθn∥2
2+λE∥bθ0−θn∥2
2.
end"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.12598425196850394,"In this section, we consider a 2-level hierarchy of time-series, consisting of a single root node (indexed
by 0) with L children (denoted by L(0)). We will also assume that instead of learning the K basis
vectors B from scratch, the K basis vectors are assumed to come from a much larger dictionary
¯
B ∈RT ×D of D (≫K) vectors that is ﬁxed and known to the model. While the original problem
learns the basis and the coefﬁcients θ simultaneously, in this case the goal is to select the basis from
among a larger dictionary, and learn the coefﬁcients θ ."
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.1279527559055118,"We analyze this problem, and show that under the reasonable assumption of the parent embedding θ0
being close to all the children embeddings θn, using the hierarchical constraints can result in a mean-
square error at the leaf nodes that is a multiplicative factor L smaller than the optimal mean-square
error of any model that does not use the hierarchical constraints. Our proposed HIRED model, when
applied in this setting would result in the following (hierarchically) regularized regression problem:"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.12992125984251968,"min
θ
1
NT ∥y −Bθ∥2
2 + λ
X"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.13188976377952755,"n∈L(0)
∥θ0 −θn∥2
2.
(6)"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.13385826771653545,"For the sake of analysis, we instead consider a two-stage version, described in Algorithm 1 and
Algorithm 2: we ﬁrst recover the support of the basis using Basis Pursuit (Chen et al., 2001). We
then estimate the parameters of the root node, which is then plugged-in to solve for the parameters
of the children node. We also deﬁne the baseline (unregularized) optimization problem for the leaf
nodes that does not use any hierarchical information, as"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.13582677165354332,"˜θn = argmin
θn"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.1377952755905512,"1
T ∥yn −Bθn∥2
2
∀n ∈L(0).
(7)"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.13976377952755906,"The basis support recovery follows from standard analysis (Wainwright, 2009) detailed in Lemma 1 in
the Appendix. We focus on the performance of Algorithm 2 here. The following theorem bounds the
error of the unregularized (˜θn) and the hierarchically-regularized (bθn, see Algorithm 2) optimization
solutions. A proof of the theorem can be found in Appendix A.2.
Theorem 1. Suppose the rows of B are norm bounded as ∥Bi∥2 ≤r, and ∥θn −θ0∥2 ≤β. Deﬁne
Σ = BT B/T as the empirical covariance matrix. For λE = σ2K"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.14173228346456693,"T β2 , eθn and bθn can be bounded as,"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.1437007874015748,"E∥eθn −θn∥2
Σ ≤σ2K"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.14566929133858267,"T
,
E∥bθn −θn∥2
Σ ≤3σ2K"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.14763779527559054,"T
1
1 +
σ2K
T r2β2
+ 6σ2K"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.14960629921259844,"TL .
(8)"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.1515748031496063,Under review as a conference paper at ICLR 2022
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.15354330708661418,"In ﬁxed design linear regression ∥bθn −θn∥2
Σ = ∥B(bθn −θn)∥2 is the population squared error (see
Appendix A.5 for a bound on the parameter estimation error). The gains due to the regularization can
be understood by considering the case when β is upper bounded by a sufﬁciently small quantity. Note
that an upper bound on β essentially implies that the children time-series have structural similarities
as further elaborated in Appendix A.5. We show that the above assumption yields a smaller upper
bound on the error. In fact, if β = o(
p"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.15551181102362205,"K/T), then the numerator 1 +
σ2K
T r2β2 in Eq. (8) is ω(1)"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.15748031496062992,"resulting in E∥bθn −θn∥2
Σ = o( σ2K"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.1594488188976378,T ) which decays faster than σ2K
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.16141732283464566,"T . Furthermore, if β is even smaller
as β = O(
p"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.16338582677165353,"K/LT), then following similar calculations, E∥bθn −θn∥2
Σ = O( σ2K"
THEORETICAL JUSTIFICATION FOR HIERARCHICAL MODELING,0.16535433070866143,"LT ) which is again
smaller than the unregularized bound."
EXPERIMENTS,0.1673228346456693,"6
EXPERIMENTS"
EXPERIMENTS,0.16929133858267717,"We implemented our proposed model in Tensorﬂow (Abadi et al., 2016) and compared against
multiple baselines on popular hierarchical time-series datasets."
EXPERIMENTS,0.17125984251968504,"Datasets. We experimented with three hierarchical forecasting datasets - Two retail forecasting
datasets, M5 (M5, 2020) and Favorita (Favorita, 2017); and the Tourism (Tourism, 2019) dataset
consisting of tourist count data. The history length and forecast horizon (H, F) were set to (28, 7),
(28, 7) and (24, 4), for Favorita, M5 and Tourism respectively. More information can be found in
Appendix B.2. We divide each of the datasets into training, validation and test sets, with details on
the splits provided in Appendix B.3."
EXPERIMENTS,0.1732283464566929,"Baselines. We compare our proposed approach HIRED with the following baseline models: (i) RNN
- we use a seq-2-seq model shared across all the time series, (ii) DeepGLO (Sen et al., 2019), (iii)
DCRNN (Li et al., 2017), a GNN based approach where we feed the hierarchy tree as the input graph,
(iv) Deep Factors (DF) (Wang et al., 2019), (v) L2Emb (Gleason, 2020), which is an improvement
over Mishchenko et al. (2019), (vi) SHARQ (Han et al., 2021), (vii) HierE2E (Rangapuram et al.,
2021) - the standard implementation produces probabilistic forecasts, we however adapt it to point
forecasts by using their projection step on top of a seq-2-seq model. We use code publicly released
by the authors for DeepGLO1 and DCRNN2. We implemented our own version of DeepFactors,
HierE2E, and L2Emb for a fair comparison, since the ofﬁcial implementations either make rolling
probabilistic forecasts, or use a different set of covariates. Additionally, we also compare with the
recent ERM (Ben Taieb & Koo, 2019) reconciliation method applied to the base forecasts from
the RNN model, denoted as (vii) RNN+ERM. It has been shown in (Ben Taieb & Koo, 2019) to
outperform many previous reconciliation techniques such as MinT (Wickramasuriya et al., 2019). For
a fair comparison, we use the same Mean Absolute Error (MAE) loss for all the compared methods,
and make sure that all the models have approximately the same number of parameters. Further details
about the baselines and training parameters can be found in Appendix B."
EXPERIMENTS,0.17519685039370078,"Metrics. We compare the accuracy of the various approaches with respect to two metrics: (i) weighted
absolute percentage error (WAPE), and (ii) symmetric mean absolute percentage error (SMAPE).
A description of these metrics can be found in Appendix B.1. We report the metrics on the test
data, for each level of the hierarchy (with level 0 denoting the root) in Table 1. As a measure of the
aggregate performance across all the levels, we also report the mean of the metrics in all the levels of
the hierarchy denoted by Mean."
RESULTS,0.17716535433070865,"6.1
RESULTS"
RESULTS,0.17913385826771652,"Table 1 shows the averaged test metrics for M5, Favorita, and Tourism datasets. We present only the
Mean metrics for the Tourism dataset due to lack of space. Complete results with conﬁdence intervals
for all the three datasets can be be found in Appendix B.6."
RESULTS,0.18110236220472442,"We ﬁnd that for all three datasets, our proposed model either yields the smallest error or close to the
smallest error across most metrics and most levels. In particular, we ﬁnd that our proposed method
achieves the smallest errors in the mean column for all datasets in terms of WAPE and SMAPE, thus
indicating good performance generally across all levels. We ﬁnd that RNN+ERM in general, yields"
RESULTS,0.1830708661417323,"1https://github.com/rajatsen91/deepglo
2https://github.com/liyaguang/DCRNN/"
RESULTS,0.18503937007874016,Under review as a conference paper at ICLR 2022
RESULTS,0.18700787401574803,"Table 1: The tables show the WAPE/SMAPE test metrics for the M5, Tourism, and Favorita datasets,
averaged over 10 runs. We present only the Mean metrics for the Tourism dataset due to lack of
space. A complete set of results including the standard deviations can be found in Appendix B.6.
In-coherent and coherent baselines are separated by a horizontal line."
RESULTS,0.1889763779527559,"M5
Level 0
Level 1
Level 2
Level 3
Mean"
RESULTS,0.19094488188976377,"HIRED
0.048 / 0.048
0.055 / 0.053
0.072 / 0.077
0.279 / 0.511
0.113 / 0.172"
RESULTS,0.19291338582677164,"RNN
0.059 / 0.059
0.083 / 0.083
0.085 / 0.098
0.282 / 0.517
0.127 / 0.189"
RESULTS,0.19488188976377951,"DF
0.055 / 0.056
0.061 / 0.060
0.076 / 0.085
0.272 / 0.501
0.116 / 0.176"
RESULTS,0.1968503937007874,"DeepGLO
0.077 / 0.081
0.087 / 0.092
0.099 / 0.113
0.278 / 0.538
0.135 / 0.206"
RESULTS,0.19881889763779528,"DCRNN
0.078 / 0.079
0.096 / 0.092
0.165 / 0.193
0.282 / 0.512
0.156 / 0.219"
RESULTS,0.20078740157480315,"L2Emb
0.055 / 0.056
0.064 / 0.063
0.080 / 0.092
0.269 / 0.501
0.117 / 0.178"
RESULTS,0.20275590551181102,"SHARQ
0.093 / 0.096
0.071 / 0.062
0.099 / 0.094
0.277 / 0.528
0.135 / 0.195"
RESULTS,0.2047244094488189,"RNN+ERM
0.052 / 0.052
0.066 / 0.071
0.084 / 0.104
0.286 / 0.520
0.122 / 0.187"
RESULTS,0.20669291338582677,"Hier-E2E
0.152 / 0.160
0.152 / 0.158
0.152 / 0.181
0.396 / 0.615
0.213 / 0.278"
RESULTS,0.20866141732283464,"Tourism
Mean"
RESULTS,0.2106299212598425,"HIRED
0.186 / 0.322"
RESULTS,0.2125984251968504,"RNN
0.211 / 0.333"
RESULTS,0.21456692913385828,"DF
0.204 / 0.334"
RESULTS,0.21653543307086615,"DeepGLO
0.199 / 0.346"
RESULTS,0.21850393700787402,"DCRNN
0.281 / 0.392"
RESULTS,0.2204724409448819,"L2Emb
0.215 / 0.342"
RESULTS,0.22244094488188976,"SHARQ
0.229 / 0.378"
RESULTS,0.22440944881889763,"RNN+ERM
0.251 / 0.417"
RESULTS,0.2263779527559055,"Hier-E2E
0.208 / 0.340"
RESULTS,0.2283464566929134,"Favorita
Level 0
Level 1
Level 2
Level 3
Mean"
RESULTS,0.23031496062992127,"HIRED
0.061 / 0.061
0.094 / 0.182
0.127 / 0.267
0.210 / 0.322
0.123 / 0.208"
RESULTS,0.23228346456692914,"RNN
0.067 / 0.068
0.114 / 0.197
0.134 / 0.290
0.203 / 0.339
0.130 / 0.223"
RESULTS,0.234251968503937,"DF
0.064 / 0.064
0.110 / 0.194
0.135 / 0.291
0.213 / 0.343
0.130 / 0.223"
RESULTS,0.23622047244094488,"DeepGLO
0.098 / 0.088
0.126 / 0.197
0.156 / 0.338
0.226 / 0.404
0.151 / 0.256"
RESULTS,0.23818897637795275,"DCRNN
0.080 / 0.080
0.120 / 0.212
0.134 / 0.328
0.204 / 0.389
0.134 / 0.252"
RESULTS,0.24015748031496062,"L2Emb
0.070 / 0.070
0.114 / 0.199
0.136 / 0.276
0.207 / 0.321
0.132 / 0.216"
RESULTS,0.2421259842519685,"SHARQ
0.088 / 0.085
0.142 / 0.199
0.156 / 0.335
0.230 / 0.404
0.154 / 0.256"
RESULTS,0.2440944881889764,"RNN+ERM
0.056 / 0.058
0.103 / 0.185
0.129 / 0.283
0.220 / 0.348
0.127 / 0.219"
RESULTS,0.24606299212598426,"Hier-E2E
0.120 / 0.125
0.206 / 0.334
0.247 / 0.448
0.409 / 0.573
0.245 / 0.370"
RESULTS,0.24803149606299213,"an improvement over the base RNN predictions for the higher levels closer to the root node (Levels 0
and 1), while, worsening at the lower levels. DCRNN, despite using the hierarchy as a graph also
does not perform as well as our approach, especially in Tourism and M5 Datasets - possibly due
to the fact that a GNN is not the most effective way to model the tree hierarchies. We notice that
HierE2E performs reasonably well for the smaller Tourism while performing badly for the larger M5
and Favorita datasets - a possible explanation being that this is a VAR model that requires much more
parameters to scale to large datasets. Therefore, for HierE2E we perform experiments with 50× more
parameters for M5 and Favorita and report the results in Table 4 in the appendix, showing that while
the results improve, it still performs much worse than our model. Overall, we ﬁnd that our proposed
method consistently works better or at par with the other baselines at all hierarchical levels."
RESULTS,0.25,"Ablation study. Next, we perform an ablation study of our proposed model to understand the effects
of its various components, the results of which are presented in Table 2. We compare our proposed
model, to the same model without any regularization (set λE = 0 in Eq (5)), and a model consisting
of only TVAR. We ﬁnd that both these components in our model are important, and result in improved
accuracy in most metrics."
RESULTS,0.25196850393700787,"Table 2: We report the test WAPE/SMAPE metrics for an ablation study on the M5 dataset, for each
of the components in the HiReD model. We compare our model with two ablated variants: ﬁrst, we
remove the regularization (λE = 0), and second, we remove the BD component (TVAR only)."
RESULTS,0.25393700787401574,"M5 Abl
Level 0
Level 1
Level 2
Level 3
Mean"
RESULTS,0.2559055118110236,"HIRED
0.048 / 0.048
0.055 / 0.053
0.072 / 0.077
0.279 / 0.511
0.113 / 0.172"
RESULTS,0.2578740157480315,"λE = 0
0.054 / 0.054
0.058 / 0.056
0.074 / 0.078
0.279 / 0.513
0.116 / 0.175"
RESULTS,0.25984251968503935,"TVAR only
0.050 / 0.049
0.064 / 0.065
0.084 / 0.086
0.288 / 0.520
0.122 / 0.180"
RESULTS,0.2618110236220472,Under review as a conference paper at ICLR 2022
RESULTS,0.2637795275590551,"Figure 3: Left: Plots of the basis generated on the
validation set of the M5 dataset over 35 days. Right:
We plot the true time series over the same time period,
and compare it with the predicted time series, AR
predictions and BD predictions."
RESULTS,0.265748031496063,"L0
L1
L2
L3"
RESULTS,0.2677165354330709,Favorita
RESULTS,0.26968503937007876,"HiReD
0.004
0.004
0.003
-
λE = 0
0.012
0.013
0.010
-
RNN
0.043
0.044
0.042
- M5"
RESULTS,0.27165354330708663,"HiReD
0.030
0.034
0.034
-
λE = 0
0.035
0.040
0.039
-
RNN
0.042
0.057
0.047
-"
RESULTS,0.2736220472440945,Tourism
RESULTS,0.2755905511811024,"HiReD
0.092
0.079
0.066
0.060
λE = 0
0.085
0.082
0.067
0.059
RNN
0.097
0.089
0.082
0.083"
RESULTS,0.27755905511811024,"Figure 4:
Coherency metric for all our
datasets, at all hierarchical levels. Leaf node
metrics are identically zero, and hence not
reported in the table. Leaf nodes for Favorita
and M5 are denoted by L3. Tourism has 5
hierarchical levels and hence L3 values are
reported in this case."
RESULTS,0.2795275590551181,"Coherence. We also compare the coherence of our predictions to that of the RNN model and an
ablated model with λE = 0. Speciﬁcally, for each node p we measure the deviation of our forecast
from c(p) = 1/L(p) P"
RESULTS,0.281496062992126,"i∈L(p) by(i), the mean of the leaf node predictions of the corresponding sub-
tree. Perfectly coherent predictions will have a zero deviation from this quantity. In Table 4, we
report the WAPE metric between the predictions from our model by and c, for each of the hierarchical
levels. The leaf level predictions are trivially coherent. We ﬁnd that our proposed model consistently
produces more coherent predictions compared to both the models, indicating that our hierarchical
regularization indeed encourages coherency in predictions, in addition to improving accuracy."
RESULTS,0.28346456692913385,"Basis Visualization. We visualize the basis generated by the BD model for the M5 validation set
in Figure 3 (left). We notice that the bases capture various global temporal patterns in the dataset.
In particular, most of the bases have a period of 7, indicating that they represent weekly patterns.
We also show the predictions made from the various components of our model at all hierarchical
levels, in Figure 3 (right). We notice that the predictions from the BD part closely resemble the
general patterns of the true time series values, where as the AR model adds further adjustments to the
predictions, including a constant bias, for most time series. For the leaf level (Level 3) predictions
however, the ﬁnal prediction is dominated by the AR model indicating that global temporal patterns
may be less useful in this case."
CONCLUSION,0.2854330708661417,"7
CONCLUSION"
CONCLUSION,0.2874015748031496,"In this paper, we proposed a method for hierarchical time series forecasting, consisting of two
components, the TVAR model, and the BD model. The TVAR model is coherent by design, whereas
we regularize the BD model to impose approximate coherency. Our model is fully differentiable and
is trainable via SGD, while also being scalable with respect to the number of nodes in the hierarchy.
Furthermore, it also does not require any additional pre-processing steps."
CONCLUSION,0.28937007874015747,"We empirically evaluated our method on three benchmark datasets and showed that our model
consistently improved over state of the art baselines for most levels of the hierarchy. We perform an
ablation study to justify the important components of our model and also show empirically that our
forecasts are more coherent than the RNN baseline. Lastly, we also visualize the learned basis and
observe that they capture various global temporal patterns."
CONCLUSION,0.29133858267716534,"In this work, we aimed at maximizing the overall performance without emphasizing performance at
individual hierarchical levels. For future work, we plan to treat this is as a multi-objective problem
with the aim of understanding the performance tradeoffs at various levels of the hierarchy. Finally,
we also plan to extend our current model to probabilistic forecasting."
CONCLUSION,0.2933070866141732,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.2952755905511811,"Reproducibility statement: We provide code for our model along with instructions in the supple-
mentary ﬁle. The instructions are provided in the readme.txt ﬁle inside the zipped folder. Our
code includes links to public datasets and code for preprocessing them. We also ﬁx the random seed
in our code in order for it to be reproducible. However, exact results may depend on the version of
the python libraries, type of GPU and compute used."
REFERENCES,0.297244094488189,REFERENCES
REFERENCES,0.2992125984251969,"Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-scale
machine learning. In 12th USENIX symposium on operating systems design and implementation
(OSDI 16), pp. 265–283, 2016."
REFERENCES,0.30118110236220474,"Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. Adaptive graph convolutional recurrent
network for trafﬁc forecasting. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17804–17815. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/ce1aad92b939420fc17005e5461e6f48-Paper.pdf."
REFERENCES,0.3031496062992126,"Souhaib Ben Taieb and Bonsoo Koo. Regularized regression for hierarchical forecasting without
unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 1337–1347, 2019."
REFERENCES,0.3051181102362205,"Konstantinos Benidis, Syama Sundar Rangapuram, Valentin Flunkert, Bernie Wang, Danielle Maddix,
Caner Turkmen, Jan Gasthaus, Michael Bohlke-Schneider, David Salinas, Lorenzo Stella, et al.
Neural forecasting: Introduction and literature overview. arXiv preprint arXiv:2004.10240, 2020."
REFERENCES,0.30708661417322836,"Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek
R´ozemberczki, Michal Lukasik, and Stephan G¨unnemann. Scaling graph neural networks with
approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 2464–2473, 2020."
REFERENCES,0.3090551181102362,"Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. Conditional time series forecasting
with convolutional neural networks. arXiv preprint arXiv:1703.04691, 2017."
REFERENCES,0.3110236220472441,"Joos-Hendrik B¨ose, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Dustin Lange, David Salinas,
Sebastian Schelter, Matthias Seeger, and Yuyang Wang. Probabilistic demand forecasting at scale.
Proceedings of the VLDB Endowment, 10(12):1694–1705, 2017."
REFERENCES,0.31299212598425197,"Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong
Xu, Jing Bai, Jie Tong, and Qi Zhang. Spectral temporal graph neural network for multivariate
time-series forecasting. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 17766–17778. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
cdf6581cb7aca4b7e19ef136c6e601a5-Paper.pdf."
REFERENCES,0.31496062992125984,"Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis
pursuit. SIAM review, 43(1):129–159, 2001."
REFERENCES,0.3169291338582677,"Ying Cui, Ruofei Zhang, Wei Li, and Jianchang Mao. Bid landscape forecasting in online ad exchange
marketplace. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 265–273, 2011."
REFERENCES,0.3188976377952756,"Emmanuel de B´ezenac, Syama Sundar Rangapuram, Konstantinos Benidis, Michael Bohlke-
Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, and Tim Januschowski.
Normalizing kalman ﬁlters for multivariate time series analysis. In NeurIPS, 2020."
REFERENCES,0.32086614173228345,"Favorita.
Favorita
forecasting
dataset.
https://www.kaggle.com/c/
favorita-grocery-sales-forecast, 2017."
REFERENCES,0.3228346456692913,"Nicolas Gillis and Stephen A Vavasis. Fast and robust recursive algorithmsfor separable nonnegative
matrix factorization. IEEE transactions on pattern analysis and machine intelligence, 36(4):
698–714, 2013."
REFERENCES,0.3248031496062992,Under review as a conference paper at ICLR 2022
REFERENCES,0.32677165354330706,"Jeffrey L Gleason. Forecasting hierarchical time series with a regularized embedding space. San
Diego, 7, 2020."
REFERENCES,0.328740157480315,"Xing Han, Sambarta Dasgupta, and Joydeep Ghosh. Simultaneously reconciled quantile forecasting
of hierarchically related time series. In International Conference on Artiﬁcial Intelligence and
Statistics, pp. 190–198. PMLR, 2021."
REFERENCES,0.33070866141732286,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.33267716535433073,"Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pp. 9–1. JMLR Workshop and Conference Proceedings, 2012."
REFERENCES,0.3346456692913386,"Rob Hyndman, Anne B Koehler, J Keith Ord, and Ralph D Snyder. Forecasting with exponential
smoothing: the state space approach. Springer Science & Business Media, 2008."
REFERENCES,0.33661417322834647,"Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018."
REFERENCES,0.33858267716535434,"Rob J Hyndman and Shu Fan. Density forecasting for long-term peak electricity demand. IEEE
Transactions on Power Systems, 25(2):1142–1153, 2009."
REFERENCES,0.3405511811023622,"Rob J Hyndman, Roman A Ahmed, George Athanasopoulos, and Han Lin Shang. Optimal com-
bination forecasts for hierarchical time series. Computational statistics & data analysis, 55(9):
2579–2589, 2011."
REFERENCES,0.3425196850393701,"Rob J Hyndman, Alan J Lee, and Earo Wang. Fast computation of reconciled forecasts for hierarchical
and grouped time series. Computational statistics & data analysis, 97:16–32, 2016."
REFERENCES,0.34448818897637795,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.3464566929133858,"Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network:
Data-driven trafﬁc forecasting. arXiv preprint arXiv:1707.01926, 2017."
REFERENCES,0.3484251968503937,"M5.
M5
forecasting
dataset.
https://www.kaggle.com/c/
m5-forecasting-accuracy/, 2020."
REFERENCES,0.35039370078740156,"ED McKenzie. General exponential smoothing and the equivalent arma process. Journal of Forecast-
ing, 3(3):333–344, 1984."
REFERENCES,0.35236220472440943,"Konstantin Mishchenko, Mallory Montgomery, and Federico Vaggi. A self-supervised approach
to hierarchical forecasting with applications to groupwise synthetic controls. arXiv preprint
arXiv:1906.10586, 2019."
REFERENCES,0.3543307086614173,"Nam Nguyen and Brian Quanz. Temporal latent auto-encoder: A method for probabilistic multivariate
time series forecasting. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35,
pp. 9117–9125, 2021."
REFERENCES,0.3562992125984252,"Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis
expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437,
2019."
REFERENCES,0.35826771653543305,"Anastasios Panagiotelis, Puwasala Gamakumara, George Athanasopoulos, Rob J Hyndman, et al.
Probabilistic forecast reconciliation: Properties, evaluation and score optimisation. Monash
econometrics and business statistics working paper series, 26:20, 2020."
REFERENCES,0.36023622047244097,"Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and
Tim Januschowski. Deep state space models for time series forecasting. Advances in neural
information processing systems, 31:7785–7794, 2018."
REFERENCES,0.36220472440944884,"Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus,
and Tim Januschowski. End-to-end learning of coherent probabilistic forecasts for hierarchical
time series. In International Conference on Machine Learning, pp. 8832–8843. PMLR, 2021."
REFERENCES,0.3641732283464567,Under review as a conference paper at ICLR 2022
REFERENCES,0.3661417322834646,"Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann, and Roland Vollgraf. Mul-
tivariate probabilistic time series forecasting via conditioned normalizing ﬂows. arXiv preprint
arXiv:2002.06103, 2020."
REFERENCES,0.36811023622047245,"David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus. High-
dimensional multivariate forecasting with low-rank gaussian copula processes. arXiv preprint
arXiv:1910.03002, 2019."
REFERENCES,0.3700787401574803,"David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic
forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):
1181–1191, 2020."
REFERENCES,0.3720472440944882,"Rajat Sen, Hsiang-Fu Yu, and Inderjit Dhillon. Think globally, act locally: A deep neural network
approach to high-dimensional time series forecasting. arXiv preprint arXiv:1905.03806, 2019."
REFERENCES,0.37401574803149606,"K Sharman and Benjamin Friedlander. Time-varying autoregressive modeling of a class of nonsta-
tionary signals. In ICASSP’84. IEEE International Conference on Acoustics, Speech, and Signal
Processing, volume 9, pp. 227–230. IEEE, 1984."
REFERENCES,0.37598425196850394,"Gilbert Strang. Wavelet transforms versus fourier transforms. Bulletin of the American Mathematical
Society, 28(2):288–305, 1993."
REFERENCES,0.3779527559055118,"Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
Advances in Neural Information Processing Systems (NIPS 2014), 2014."
REFERENCES,0.3799212598425197,"Souhaib Ben Taieb, James W Taylor, and Rob J Hyndman. Coherent probabilistic forecasts for
hierarchical time series. In International Conference on Machine Learning, pp. 3348–3357. PMLR,
2017."
REFERENCES,0.38188976377952755,"Tourism.
Tourism forecasting dataset.
https://robjhyndman.com/publications/
mint/, 2019."
REFERENCES,0.3838582677165354,"A¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. In 9th ISCA Speech Synthesis Workshop, pp. 125–125, 2016."
REFERENCES,0.3858267716535433,"Tim Van Erven and Jairo Cugliari. Game-theoretically optimal reconciliation of contemporaneous
hierarchical time series forecasts. In Modeling and stochastic learning for forecasting in high
dimensions, pp. 297–317. Springer, 2015."
REFERENCES,0.38779527559055116,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017."
REFERENCES,0.38976377952755903,"Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using
ℓ1-constrained quadratic programming (lasso). IEEE transactions on information theory, 55(5):
2183–2202, 2009."
REFERENCES,0.39173228346456695,"Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019."
REFERENCES,0.3937007874015748,"Yuyang Wang, Alex Smola, Danielle Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.
Deep factors for forecasting. In International Conference on Machine Learning, pp. 6607–6617.
PMLR, 2019."
REFERENCES,0.3956692913385827,"Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A multi-horizon
quantile recurrent forecaster. NeurIPS Time Series Workshop, 2017."
REFERENCES,0.39763779527559057,"Shanika L Wickramasuriya, George Athanasopoulos, Rob J Hyndman, et al. Forecasting hierarchical
and grouped time series through trace minimization. Department of Econometrics and Business
Statistics, Monash University, 105, 2015."
REFERENCES,0.39960629921259844,"Shanika L Wickramasuriya, George Athanasopoulos, and Rob J Hyndman. Optimal forecast rec-
onciliation for hierarchical and grouped time series through trace minimization. Journal of the
American Statistical Association, 114(526):804–819, 2019."
REFERENCES,0.4015748031496063,Under review as a conference paper at ICLR 2022
REFERENCES,0.4035433070866142,"Shanika L Wickramasuriya, Berwin A Turlach, and Rob J Hyndman. Optimal non-negative forecast
reconciliation. Statistics and Computing, 30(5):1167–1182, 2020."
REFERENCES,0.40551181102362205,"Kevin W Wilson, Bhiksha Raj, and Paris Smaragdis. Regularized non-negative matrix factorization
with temporal dependencies for speech denoising. In Interspeech, pp. 411–414, 2008."
REFERENCES,0.4074803149606299,"Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Connecting
the dots: Multivariate time series forecasting with graph neural networks. In Proceedings of the
26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
753–763, 2020."
REFERENCES,0.4094488188976378,"Anna K Yanchenko, Di Daniel Deng, Jinglan Li, Andrew J Cron, and Mike West. Hierarchical
dynamic modeling for individualized bayesian forecasting. arXiv preprint arXiv:2101.03408,
2021."
REFERENCES,0.41141732283464566,"Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A deep
learning framework for trafﬁc forecasting. arXiv preprint arXiv:1709.04875, 2017."
REFERENCES,0.41338582677165353,"Dawei Zhou, Lecheng Zheng, Yada Zhu, Jianbo Li, and Jingrui He. Domain adaptive multi-modality
neural attention network for ﬁnancial forecasting. In Proceedings of The Web Conference 2020, pp.
2230–2240, 2020."
REFERENCES,0.4153543307086614,Under review as a conference paper at ICLR 2022
REFERENCES,0.41732283464566927,"A
THEORY"
REFERENCES,0.41929133858267714,"A.1
SUPPORT RECOVERY"
REFERENCES,0.421259842519685,"Lemma 1. Suppose B satisﬁes the lower eigenvalue condition (Assumption 1 in Appendix A.3) with
parameter Cmin and the mutual incoherence condition (Assumption 2 in Appendix A.3) with parameter
γ. Also assume that the columns of the basis pool ¯
B are normalized so that maxj∈Sc ∥¯
B(j)∥≤
√"
REFERENCES,0.42322834645669294,"T,
and the true parameter θ0 of the root satisﬁes"
REFERENCES,0.4251968503937008,∥θ0∥∞≥λL
REFERENCES,0.4271653543307087,"Σ−1
∞+
4σ
√LCmin"
REFERENCES,0.42913385826771655,"
,
(9)"
REFERENCES,0.4311023622047244,"where |||A|||∞= maxi
P"
REFERENCES,0.4330708661417323,"j |Aij| denotes matrix operator ℓ∞norm, and Σ = BT B/T denotes the"
REFERENCES,0.43503937007874016,empirical covariance matrix. Then for λL ≥2 γ q
REFERENCES,0.43700787401574803,2σ2 log d
REFERENCES,0.4389763779527559,"LT
, with probability least 1 −4 exp(−c1Tλ2)"
REFERENCES,0.4409448818897638,"(for some constant c1), the support bS = {i | |bα0| > 0} recovered from the Lasso solution (see
Algorithm 1) is equal to the true support S."
REFERENCES,0.44291338582677164,"Proof. We are given a pool of basis vectors ¯
B from which the observed data is generated using
a subset of K columns which we have denoted by B in the text. We denote the correct subset of
columns by S and recover them from the observed data using basis pursuit - also known as the
support recovery problem in the literature. Given the observed data and the pool of basis vectors ¯
B,
we recover the support from the following regression problem corresponding to the root node time
series.
y0 = ¯
Bα + w0,
w0 ∼N(0, σ2I/L),
(10)
where α is K-sparse with the non-zero indices at S, and the non-zero values equal θ0 - the true
parameters of the root node. Here we have used the fact that the root node has a 1/L times smaller
variance due to aggregation. The true support S can be recovered from the observed data y0, by
solving the sparse regression problem (Lasso) given in Algorithm 2. A number of standard Lasso
assumptions are needed to ensure that the support is identiﬁable, and that the non-zero parameters
are large enough to be estimated. Assuming that ¯
BS (= B) and α satisfy all the assumptions of
Theorem 2, the theorem ensures that the true support S is recovered with high probability."
REFERENCES,0.4448818897637795,"A.2
PROOF OF THEOREM 1 - ERROR BOUNDS FOR REGULARIZED ESTIMATORS"
REFERENCES,0.4468503937007874,"For this proof, we assume that the support S is recovered and the true basis functions B are known
with high probability (see Section A.1). We divide the proof into multiple steps."
REFERENCES,0.44881889763779526,"Step I:
By Corollary 1, the OLS estimate bθ0 (see Algorithm 2) of parameters of the root node and
the OLS estimate eθn (see Eq. (7)) can be bounded as,"
REFERENCES,0.4507874015748031,"E[∥bθ0 −θ0∥2
Σ] ≤σ2K"
REFERENCES,0.452755905511811,"TL ,
E[∥eθn −θn∥2
Σ] ≤σ2K"
REFERENCES,0.4547244094488189,"T
∀n ∈L(0).
(11)"
REFERENCES,0.4566929133858268,"Step II:
Next, using change of variables, we notice that the ridge regression loss for the child nodes
(see Algorithm 2) is equivalent to the following."
REFERENCES,0.45866141732283466,"bψn = argmin
ψn"
REFERENCES,0.46062992125984253,"1
T ∥yn −Bbθ0 −Bψn∥2
2 + λ∥ψn∥2
2
∀n ∈L(0),
(12)"
REFERENCES,0.4625984251968504,"where ψn = θn −bθ0. The ﬁnal estimate for the child parameters can be written as a sum of the ψn
estimate and the root node estimate, bθn = bψn + bθ0. We also consider a related problem that will help
us in computing the errors bounds."
REFERENCES,0.4645669291338583,"eψn = argmin
ψn"
REFERENCES,0.46653543307086615,"1
T ∥yn −Bθ0 −Bψn∥2
2 + λ∥ψn∥2
2
∀n ∈L(0).
(13)"
REFERENCES,0.468503937007874,"Here we have replaced bθ0 with the true value θ0. Note that this regression problem cannot be solved
in practice since we do not have access to the true value of θ0. We will only use it to assist in the"
REFERENCES,0.4704724409448819,Under review as a conference paper at ICLR 2022
REFERENCES,0.47244094488188976,"analysis. Now we will bound the difference between the estimates bψn and eψn. The closed form
solution for ridge regression is well known in the literature."
REFERENCES,0.4744094488188976,"bψn
=
T −1(Σ + λI)−1BT (yn −Bbθ0)
eψn
=
T −1(Σ + λI)−1BT (yn −Bθ0),"
REFERENCES,0.4763779527559055,"where Σ = BT B/T, as deﬁned earlier. The norm of the difference of the estimates can be bounded
as
bψn −eψn
=
T −1(Σ + λI)−1BT B(eθ0 −bθ0)"
REFERENCES,0.47834645669291337,"=⇒∥bψn −eψn∥2
Σ
=
(eθ0 −bθ0)T Σ(Σ + λI)−1Σ(Σ + λI)−1Σ(eθ0 −bθ0)"
REFERENCES,0.48031496062992124,"=
(eθ0 −bθ0)T Σ(Σ + λI)−1Σ(Σ + λI)−1Σ(eθ0 −bθ0)"
REFERENCES,0.4822834645669291,"=
(eθ0 −bθ0)T V D

λ3
i
(λi + λ)2"
REFERENCES,0.484251968503937,"
V T (eθ0 −bθ0)."
REFERENCES,0.4862204724409449,"Here we have used an eigen-decomposition of the symmetric sample covariance matrix as Σ =
V D[λi]V T . We use the notation D[λi] to denote a diagonal matrix with values λi on the diagonal.
The above can be further upper bounded using the fact that λi ≤λi + λ."
REFERENCES,0.4881889763779528,"∥bψn −eψn∥2
Σ ≤(eθ0 −bθ0)T V D[λi]V T (eθ0 −bθ0) = ∥eθ0 −bθ0∥2
Σ.
(14)"
REFERENCES,0.49015748031496065,"Step III:
Now we will bound the error on eψn and ﬁnally use it in the next step with triangle
inequality to prove our result. Note that yn −Bθ0 = B(θn −θ0) + wn. Therefore, we can see from
Eq. (13) that eψn is an estimate for θn −θ0. Using the fact that ∥θn −θ0∥2 ≤β and corollary 1, eψn
can be bounded as,"
REFERENCES,0.4921259842519685,"E[∥eψ −(θn −θ0)∥2
Σ] ≤
r2β2σ2K
Tr2β2 + σ2K .
(15)"
REFERENCES,0.4940944881889764,"Finally using triangle inequality, we bound the error of our estimate bθn."
REFERENCES,0.49606299212598426,"∥bθn −θn∥2
Σ
=
∥bψn + bθ0 −θn∥2
Σ
(Using the decomposition from Step II)."
REFERENCES,0.49803149606299213,"≤
3

∥bψn −eψn∥2
Σ + ∥eψn −(θn −θ0)∥2
Σ + ∥bθ0 −θ0∥2
Σ
"
REFERENCES,0.5,(Using triangle and Cauchy-Schwartz inequality)
REFERENCES,0.5019685039370079,"≤
3

∥eψn −(θn −θ0)∥2
Σ + 2∥bθ0 −θ0∥2
Σ

(Using Eq. (14))."
REFERENCES,0.5039370078740157,"Taking the expectation of the both sides, and using Eq. (11) and (15), we get the desired result."
REFERENCES,0.5059055118110236,"E∥bθn −θn∥2
Σ ≤3
r2β2σ2K
Tr2β2 + σ2K + 6σ2K TL ."
REFERENCES,0.5078740157480315,"A.3
REVIEW OF SPARSE LINEAR REGRESSION"
REFERENCES,0.5098425196850394,"We consider the following sparse recovery problem. We are given data (X, y) ∈Rn×d × Rn
following the observation model y = Xθ∗+ w, where w ∼N(0, σ2I), and θ∗is supported in the
indices indexed by a set S (S-sparse). We estimate θ∗using the following Lagrangian Lasso program,"
REFERENCES,0.5118110236220472,"bθ ∈argmin
θ∈Rn  1"
REFERENCES,0.5137795275590551,"2n∥y −Xθ∥2
2 + λn∥θ∥1"
REFERENCES,0.515748031496063,"
(16)"
REFERENCES,0.5177165354330708,"We consider the ﬁxed design setting, where the matrix X is ﬁxed and not sampled randomly.
Following (Wainwright, 2009), we make the following assumptions required for recovery of the true
support S of θ∗.
Assumption 1 (Lower eigenvalue). The smallest eigenvalue of the sample covariance sub-matrix
indexed by S is bounded below: Λmin"
REFERENCES,0.5196850393700787,"XT
S XS n"
REFERENCES,0.5216535433070866,"
≥Cmin > 0
(17)"
REFERENCES,0.5236220472440944,Under review as a conference paper at ICLR 2022
REFERENCES,0.5255905511811023,"Assumption 2 (Mutual incoherence). There exists some γ ∈(0, 1] such that
XT
ScXS(XT
S XS)−1
∞≤1 −γ,
(18)
where |||A|||∞= maxi
P"
REFERENCES,0.5275590551181102,"j |Aij| denotes matrix operator ℓ∞norm.
Theorem 2 (Support Recovery, Wainwright (2009)). Suppose the design matrix satisﬁes assumptions
1 and 2. Also assume that the design matrix has its n-dimensional columns normalized so that
maxj∈Sc ∥Xj∥2 ≤√n. Then for λn satisfying, λn ≥2 γ r"
REFERENCES,0.5295275590551181,2σ2 log d
REFERENCES,0.531496062992126,"n
,
(19)"
REFERENCES,0.5334645669291339,"the Lasso solution bθ satisﬁes the following properties with a probability of at least 1−4 exp(−c1nλ2
n):"
REFERENCES,0.5354330708661418,"1. The Lasso has a unique optimal solution bθ with its support contained within the true support
S(bθ) ⊆S(θ∗) and satisﬁes the ℓ∞bound"
REFERENCES,0.5374015748031497,"∥bθS −θ∗
S∥∞≤λn ""  "
REFERENCES,0.5393700787401575,"XT
S XS n −1 "
REFERENCES,0.5413385826771654,"∞
+
4σ
√Cmin #"
REFERENCES,0.5433070866141733,"|
{z
}
g(λn)"
REFERENCES,0.5452755905511811,",
(20)"
REFERENCES,0.547244094488189,"2. If in addition, the minimum value of the regression vector θ∗is lower bounded by g(λn),
then it recovers the exact support."
REFERENCES,0.5492125984251969,"A.4
REVIEW OF RIDGE REGRESSION"
REFERENCES,0.5511811023622047,"In this section we review the relevant background from (Hsu et al., 2012) on ﬁxed design ridge
regression. As usual, we assume data (X, y) ∈Rn×d × Rn following the observation model
y = Xθ∗+ w, where w ∼N(0, σ2I). Deﬁne the ridge estimator bθ as the minimizer of the ℓ2
regularized mean squared error,"
REFERENCES,0.5531496062992126,"bθ ∈argmin
θ∈Rn  1"
REFERENCES,0.5551181102362205,"n∥y −Xθ∥2
2 + λ∥θ∥2
2"
REFERENCES,0.5570866141732284,"
(21)"
REFERENCES,0.5590551181102362,"We denote the sample covariance matrix by Σ = XT X/n. Then for any parameter θ, the expected
ℓ2 prediction error is given by, ∥θ −θ∗∥2
Σ = ∥X(θ −θ∗)∥2
2/n. We also assume the standard ridge
regression setting of bounded ∥θ∗∥2 ≤B. We have the following proposition from Hsu et al. (2012)
on expected error bounds for ridge regression.
Proposition 1 (Hsu et al. (2012)). For any regularization parameter λ > 0, the expected prediction
loss can be upper bounded as"
REFERENCES,0.5610236220472441,"E[∥bθ −θ∗∥2
Σ] ≤
X j"
REFERENCES,0.562992125984252,"λj
(λj/λ + 1)2 θ∗
j
2 + σ2 n X j"
REFERENCES,0.5649606299212598,"
λj
λj + λ"
REFERENCES,0.5669291338582677,"2
,
(22)"
REFERENCES,0.5688976377952756,where λi denote the eigenvalues of the empirical covariance matrix Σ.
REFERENCES,0.5708661417322834,"Using the fact that λj ≤tr (Σ), and x/(x + c) is increasing in x for x ≥0, the above bound can be
simpliﬁed as,"
REFERENCES,0.5728346456692913,"E[∥bθ −θ∗∥2
Σ]
≤
tr (Σ)
(tr (Σ) /λ + 1)2 ∥θ∗∥2 + σ2d n"
REFERENCES,0.5748031496062992,"
tr (Σ)
tr (Σ) + λ 2"
REFERENCES,0.5767716535433071,"≤
tr (Σ) B2λ2 + tr (Σ)2 σ2d/n"
REFERENCES,0.5787401574803149,(tr (Σ) + λ)2
REFERENCES,0.5807086614173228,"Assuming that the covariate vectors Xi are norm bounded as ∥Xi∥2 ≤r, and using the fact that
tr (Σ) ≤r2, gives us the following corollary."
REFERENCES,0.5826771653543307,Corollary 1. When choosing λ = σ2d
REFERENCES,0.5846456692913385,"nB2 , the prediction loss can be upper bounded as,"
REFERENCES,0.5866141732283464,"E[∥bθ −θ∗∥2
Σ] ≤
r2B2σ2d
nr2B2 + σ2d.
(23)"
REFERENCES,0.5885826771653543,The usual ordinary least squares bound of σ2d
REFERENCES,0.5905511811023622,"n can be derived when considering the limit B →∞,
corresponding to λ = 0."
REFERENCES,0.59251968503937,Under review as a conference paper at ICLR 2022
REFERENCES,0.594488188976378,"A.5
FURTHER DISCUSSION ON THEOREM 1"
REFERENCES,0.5964566929133859,"Upper bound ∥θn −θ0∥2 ≤β:
The upper bound β essentially bounds the distance between sibling
leaf embeddings belonging to the same parent. This is directly related to an upper bound on the
distance between the parent embedding θ0 and the leaf embeddings θn, as θn is essentially the mean
of the leaf nodes (mean property). In many practical scenarios, the children time series of a parent
may not have too different seasonal trends (for example power consumption of houses in the same
neighborhood, or sales of items under the same category) resulting in the parent time series following
similar trends as well."
REFERENCES,0.5984251968503937,"Bounding ∥θn −θ0∥2:
In most theoretical analyses of linear regression (Wainwright, 2019), the
main quantity of interest is the prediction error ∥θn −θ0∥Σ rather than the parameter estimation error
∥θn−θ0∥2, as the former is directly related to the performance metric of the model. However, a bound
on the parameter estimation error can be easily established using the property that ∥θn −θ0∥2 ≤
∥θn −θ0∥Σ/√Cmin, where Cmin is the lower bound on the smallest eigenvalue of the sample
covariance matrix as deﬁned in Assumption 1 in Section A.3."
REFERENCES,0.6003937007874016,"B
FURTHER EXPERIMENTAL DETAILS"
REFERENCES,0.6023622047244095,"B.1
ACCURACY METRICS"
REFERENCES,0.6043307086614174,"In this section we deﬁne the evaluation metrics used in this paper. Denote the true values by y and
the predicted values by by, both n-dimensional vectors."
REFERENCES,0.6062992125984252,"1. Symmetric mean absolute percent error SMAPE = 2 n
P"
REFERENCES,0.6082677165354331,"i
|byi−yi|
|yi|+|byi|."
REFERENCES,0.610236220472441,2. Weighted absolute percentage error WAPE = P
REFERENCES,0.6122047244094488,"i |byi−yi|
P"
REFERENCES,0.6141732283464567,"i |yi|
."
REFERENCES,0.6161417322834646,"B.2
DATASET DETAILS"
REFERENCES,0.6181102362204725,We use three publicly available benchmark datasets for our experiments.
REFERENCES,0.6200787401574803,"1. The M5 dataset3 consists of time series data of product sales from 10 Walmart stores in three US
states. The data consists of two different hierarchies: the product hierarchy and store location
hierarchy. For simplicity, in our experiments we use only the product hierarchy consisting of 3k
nodes and 1.8k time steps. The validation scores are computed using the predictions from time
steps 1843 to 1877, and test scores on steps 1878 to 1913."
REFERENCES,0.6220472440944882,"2. The Favorita dataset4 is a similar dataset, consisting of time series data from Corporaci´on Favorita,
a South-American grocery store chain. As above, we use the product hierarchy, consisting of 4.5k
nodes and 1.7k time steps. The validation scores are computed using the predictions from time
steps 1618 to 1652, and test scores on steps 1653 to 1687."
REFERENCES,0.6240157480314961,"3. The Australian Tourism dataset5 consists of monthly domestic tourist count data in Australia
across 7 states which are sub-divided into regions, sub-regions, and visit-type. The data consists
of around 500 nodes and 230 time steps. The validation scores are computed using the predictions
from time steps 122 to 156, and test scores on steps 157 to 192."
REFERENCES,0.6259842519685039,"For the three datasets, all the time-series (corresponding to both leaf and higher-level nodes) of the
hierarchy that we used are present in the training data."
REFERENCES,0.6279527559055118,"B.3
TRAINING DETAILS"
REFERENCES,0.6299212598425197,"Table 3 presents all the hyperparameters used in our proposed model. All models were trained via
SGD using the Adam optimizer (Kingma & Ba, 2014), and the training data was standardized to mean
zero and unit variance. The datasets were split into train, validation, and test sets, the sizes of which"
REFERENCES,0.6318897637795275,"3https://www.kaggle.com/c/m5-forecasting-accuracy/
4https://www.kaggle.com/c/favorita-grocery-sales-forecasting/
5https://robjhyndman.com/publications/mint/"
REFERENCES,0.6338582677165354,Under review as a conference paper at ICLR 2022
REFERENCES,0.6358267716535433,"Table 3: Final model hyperparameters for various datasets tuned using the Mean WAPE metric on the
validation set."
REFERENCES,0.6377952755905512,"Model hyperparameters
M5
Favorita
Tourism"
REFERENCES,0.639763779527559,"LSTM hidden dim
42
24
14
Embedding dim K
8
8
6
NMF rank R
12
4
6
Multi-Horizon decoder hidden dim
24
16
12
Embedding regularization λE
3.4e-6
4.644e-4
7.2498e-8
History length H and forecast horizon F
(28, 7)
(28, 7)
(24, 4)
No. of rolling val/test windows
5
5
3
Initial learning rate
0.004
0.002
0.07
Decay rate and decay interval
(0.5, 6)
(0.5, 6)
(0.5, 6)
Early stopping patience
10
10
10
Training epochs
40
40
40
Batch size
512
512
512
Total #params
80k
80k
8k"
REFERENCES,0.6417322834645669,"are given in the Table 3. We used learning rate decay and early stopping using the Mean WAPE score
on the validation set, with a patience of 10 for all models. We tuned the model hyper-parameters
using the same metric. The various model hyper-parameters are given in Table 3."
REFERENCES,0.6437007874015748,"All our experiments were implemented in Tensorﬂow 2, and run on a Titan Xp GPU with 12GB of
memory. The computing server we used, had 256GB of memory, and 32 CPU cores, however, our
code did not seem to use more than 10GB of memory and 4 CPU cores."
REFERENCES,0.6456692913385826,"Mini-Batching:
During each training iteration, we sample a minibatch of time series for computing
the gradients. For constructing a minibatch, ﬁrst a time window (of length H + F) is uniformly
randomly selected from the training time steps, after which a subset of nodes is sampled from the
hierarchy tree. The time series data corresponding to this subset of nodes and the sampled time
window constitutes our minibatch."
REFERENCES,0.6476377952755905,"Predicting for new time steps:
Once trained, our model can be used to predict for new datapoints
without any retraining. However, in practice it may be beneﬁcial to retrain the model with newer data
to improve performance, even though this is not a constraint for our proposed approach."
REFERENCES,0.6496062992125984,"Knowledge of the hierarchy:
Our proposed approach requires the hierarchy to be known during
training to be able to regularize the embeddings. In scenarios where new aggregations are presented
during test time, a reasonable prediction can be produced using the following strategy: the embedding
of the new aggregation is set to the mean of the embeddings of nodes in that aggregation - the rest of
the model remains the same. Predictions for unseen aggregations are made in the same way as with
seen aggregations. This idea is beyond the scope of the current paper, thus left for future exploration."
REFERENCES,0.6515748031496063,"B.4
FURTHER DETAILS ABOUT GLOBAL STATE Z"
REFERENCES,0.6535433070866141,"In many practical scenarios, the evolving global state of the set of time series may not be captured
by the global covariates X only. For instance, when there is an overall increase/decrease in sales
across all time series, it is captured in the past values Y rather than X. As a result, it may be
required to feed in past values of time series to the BD model. However, since we cannot feed in the
whole set of time series (order of 1000s) without leading to scalability issues, we choose a small
set of representative time series using Non-negative Matrix Factorization, thus approximating the
global state. NMF assumes that the columns of the time series matrix Y ∈RT ×N lies in the convex
set spanned by a small subset of columns. However, this assumption may not hold true for many
datasets, and therefore, most NMF algorithms compute an approximate factorization. We use this
aforementioned subset of columns as an approximation to the global state. We would also like to"
REFERENCES,0.655511811023622,Under review as a conference paper at ICLR 2022
REFERENCES,0.65748031496063,"emphasize that our prediction model only sees the past values ZH as input since the future time series
values are unknown."
REFERENCES,0.6594488188976378,"While NMF is one of the choices for Z, it is deﬁnitely not the only choice. Another option, PCA (or
equivalently SVD), may lead to latent vectors which do not have any temporal dependencies thus
requiring additional temporal regularization (Sen et al., 2019; Wilson et al., 2008). One may also use
more sophisticated methods such as Temporal Latent Auto-Encoders (Nguyen & Quanz, 2021). We
leave this idea for future exploration."
REFERENCES,0.6614173228346457,"B.5
BASELINES"
REFERENCES,0.6633858267716536,"Further details about the baselines we compare with are provided as below. For a fair comparison, we
use the the same Mean Absolute Error loss functions for training, and ensure that all the baseline
models have similar number of parameters."
REFERENCES,0.6653543307086615,"1. RNN: We use an LSTM decoder and encoder to implement a seq-2-seq model shared across
all the time series, trained using mean absolute error loss."
REFERENCES,0.6673228346456693,"2. DeepGLO (Sen et al., 2019): We use the implementation released by the authors on Github6.
We modify the loss function, data handling and evaluation to adapt to our setting."
REFERENCES,0.6692913385826772,"3. DCRNN (Li et al., 2017): DCRNN being a GNN based approach requires a correlation
graph as input. We use the ofﬁcial implementation released by the authors7 and provide the
hierarchy tree as the input graph. The implementation uses MAE loss by default."
REFERENCES,0.6712598425196851,"4. Deep Factors (DF) (Wang et al., 2019): The original implementation released by the authors
makes rolling probabilistic forecasts. We implement our own version in Tensorﬂow using
an LSTM encoder for the global model (producing point predictions) as in the original
implementation, while leaving out the probabilistic local model. We manually tune the
hyper-parameters for each of the datasets on the validation set."
REFERENCES,0.6732283464566929,"5. L2Emb (Gleason, 2020): We implement this model using an LSTM decoder and encoder
with MAE as the main training loss. In addition, we also use node embeddings which are fed
as input to the encoder and decoder, and regularized according to the hierarchy as described
by Gleason (2020)."
REFERENCES,0.6751968503937008,"6. SHARQ (Han et al., 2021): We were not able to ﬁnd an ofﬁcial release of SHARQ. We
implemented it using an LSTM based seq-to-seq model, with layer-wise training as described
in the paper. We used MAE as the data ﬁt loss function and the default squared error
reconciliation loss regularizer (See Han et al. (2021) for terminology)."
REFERENCES,0.6771653543307087,"7. HierE2E (Rangapuram et al., 2021): The ofﬁcial implementation produces rolling proba-
bilistic forecasts. We adapt it to point forecasts by using the proposed projection step on
outputs from a seq-2-seq model."
REFERENCES,0.6791338582677166,"8. RNN+ERM (Ben Taieb & Koo, 2019): This approach involves learning a sparse projection
matrix from data, resulting in coherent predictions. We tune the sparsity parameter using the
model accuracy on the validation set."
REFERENCES,0.6811023622047244,"B.6
MORE RESULTS"
REFERENCES,0.6830708661417323,"Table 4 show the test metrics averaged over 10 independent runs on the three datasets along with the
standard deviations."
REFERENCES,0.6850393700787402,"6https://github.com/rajatsen91/deepglo
7https://github.com/liyaguang/DCRNN/"
REFERENCES,0.687007874015748,Under review as a conference paper at ICLR 2022
REFERENCES,0.6889763779527559,"Table 4: WAPE/SMAPE test metrics for all the three datasets, averaged over 10 runs. The standard
deviations are shown in the parenthesis. We bold the smallest mean in each column and anything that
comes within two standard deviations."
REFERENCES,0.6909448818897638,"M5
Level 0
Level 1
Level 2
Level 3
Mean"
REFERENCES,0.6929133858267716,"HIRED
0.048
(0.0011) /
0.048
(0.0011)"
REFERENCES,0.6948818897637795,"0.055
(0.0006) /
0.053
(0.0006)"
REFERENCES,0.6968503937007874,"0.072
(0.0007) /
0.077
(0.0006)"
REFERENCES,0.6988188976377953,"0.279
(0.0003) /
0.511
(0.0012)"
REFERENCES,0.7007874015748031,"0.113
(0.0005) /
0.172
(0.0006)"
REFERENCES,0.702755905511811,"RNN
0.059
(0.002) /
0.059
(0.003)"
REFERENCES,0.7047244094488189,"0.083
(0.013) /
0.083
(0.011)"
REFERENCES,0.7066929133858267,"0.085
(0.002) /
0.098
(0.004)"
REFERENCES,0.7086614173228346,"0.282
(0.006) /
0.517
(0.007)"
REFERENCES,0.7106299212598425,"0.127
(0.005) /
0.189
(0.005)"
REFERENCES,0.7125984251968503,"DF
0.055
(0.001) /
0.056
(0.001)"
REFERENCES,0.7145669291338582,"0.061
(0.001) /
0.060
(0.001)"
REFERENCES,0.7165354330708661,"0.076
(0.001) /
0.085
(0.002)"
REFERENCES,0.718503937007874,"0.272
(0.000) /
0.501
(0.002)"
REFERENCES,0.7204724409448819,"0.116
(0.001) /
0.176
(0.001)"
REFERENCES,0.7224409448818898,"DeepGLO
0.077
(0.0003) /
0.081
(0.0004)"
REFERENCES,0.7244094488188977,"0.087
(0.0003) /
0.092
(0.0004)"
REFERENCES,0.7263779527559056,"0.099
(0.0003) /
0.113
(0.0003)"
REFERENCES,0.7283464566929134,"0.278
(0.0001) /
0.538
(0.0001)"
REFERENCES,0.7303149606299213,"0.135
(0.0003) /
0.206
(0.0003)"
REFERENCES,0.7322834645669292,"DCRNN
0.078
(0.006) /
0.079
(0.007)"
REFERENCES,0.734251968503937,"0.096
(0.005) /
0.092
(0.004)"
REFERENCES,0.7362204724409449,"0.165
(0.003) /
0.193
(0.007)"
REFERENCES,0.7381889763779528,"0.282
(0.000) /
0.512
(0.000)"
REFERENCES,0.7401574803149606,"0.156
(0.002) /
0.219
(0.003)"
REFERENCES,0.7421259842519685,"L2Emb
0.055
(0.0016) /
0.056
(0.001)"
REFERENCES,0.7440944881889764,"0.064
(0.0014) /
0.063
(0.001)"
REFERENCES,0.7460629921259843,"0.080
(0.0011) /
0.092
(0.001)"
REFERENCES,0.7480314960629921,"0.269
(0.0003) /
0.501
(0.003)"
REFERENCES,0.75,"0.117
(0.0009) /
0.178
(0.001)"
REFERENCES,0.7519685039370079,"SHARQ
0.093
(0.002) /
0.096
(0.002)"
REFERENCES,0.7539370078740157,"0.071
(0.004) /
0.062
(0.003)"
REFERENCES,0.7559055118110236,"0.099
(0.002) /
0.094
(0.001)"
REFERENCES,0.7578740157480315,"0.277
(0.000) /
0.528
(0.000)"
REFERENCES,0.7598425196850394,"0.135
(0.001) /
0.195
(0.001)"
REFERENCES,0.7618110236220472,"RNN+ERM
0.052
(0.001) /
0.052
(0.001)"
REFERENCES,0.7637795275590551,"0.066
(0.001) /
0.071
(0.002)"
REFERENCES,0.765748031496063,"0.084
(0.001) /
0.104
(0.002)"
REFERENCES,0.7677165354330708,"0.286
(0.002) /
0.520
(0.004)"
REFERENCES,0.7696850393700787,"0.122
(0.001) /
0.187
(0.001)"
REFERENCES,0.7716535433070866,"Hier-E2E
0.152
(0.002) /
0.160
(0.002)"
REFERENCES,0.7736220472440944,"0.152
(0.002) /
0.158
(0.002)"
REFERENCES,0.7755905511811023,"0.152
(0.002) /
0.181
(0.002)"
REFERENCES,0.7775590551181102,"0.396
(0.001) /
0.615
(0.002)"
REFERENCES,0.7795275590551181,"0.213
(0.002) /
0.278
(0.002)"
REFERENCES,0.781496062992126,"Hier-E2E Large
0.047
(0.002) /
0.050
(0.003)"
REFERENCES,0.7834645669291339,"0.057
(0.001) /
0.063
(0.001)"
REFERENCES,0.7854330708661418,"0.067
(0.001) /
0.080
(0.001)"
REFERENCES,0.7874015748031497,"0.347
(0.001) /
0.573
(0.001)"
REFERENCES,0.7893700787401575,"0.130
(0.001) /
0.192
(0.001)"
REFERENCES,0.7913385826771654,"Favorita
Level 0
Level 1
Level 2
Level 3
Mean"
REFERENCES,0.7933070866141733,"HIRED
0.061
(0.002) /
0.061
(0.002)"
REFERENCES,0.7952755905511811,"0.094
(0.001) /
0.182
(0.002)"
REFERENCES,0.797244094488189,"0.127
(0.001) /
0.267
(0.003)"
REFERENCES,0.7992125984251969,"0.210
(0.000) /
0.322
(0.004)"
REFERENCES,0.8011811023622047,"0.123
(0.001) /
0.208
(0.002)"
REFERENCES,0.8031496062992126,"RNN
0.067
(0.004) /
0.068
(0.003)"
REFERENCES,0.8051181102362205,"0.114
(0.003) /
0.197
(0.004)"
REFERENCES,0.8070866141732284,"0.134
(0.002) /
0.290
(0.005)"
REFERENCES,0.8090551181102362,"0.203
(0.001) /
0.339
(0.005)"
REFERENCES,0.8110236220472441,"0.130
(0.002) /
0.223
(0.004)"
REFERENCES,0.812992125984252,"DF
0.064
(0.003) /
0.064
(0.004)"
REFERENCES,0.8149606299212598,"0.110
(0.002) /
0.194
(0.003)"
REFERENCES,0.8169291338582677,"0.135
(0.002) /
0.291
(0.007)"
REFERENCES,0.8188976377952756,"0.213
(0.001) /
0.343
(0.007)"
REFERENCES,0.8208661417322834,"0.130
(0.002) /
0.223
(0.004)"
REFERENCES,0.8228346456692913,"DeepGLO
0.098
(0.001) /
0.088
(0.001)"
REFERENCES,0.8248031496062992,"0.126
(0.001) /
0.197
(0.001)"
REFERENCES,0.8267716535433071,"0.156
(0.001) /
0.338
(0.001)"
REFERENCES,0.8287401574803149,"0.226
(0.001) /
0.404
(0.001)"
REFERENCES,0.8307086614173228,"0.151
(0.001) /
0.256
(0.001)"
REFERENCES,0.8326771653543307,"DCRNN
0.080
(0.004) /
0.080
(0.005)"
REFERENCES,0.8346456692913385,"0.120
(0.001) /
0.212
(0.002)"
REFERENCES,0.8366141732283464,"0.134
(0.000) /
0.328
(0.000)"
REFERENCES,0.8385826771653543,"0.204
(0.000) /
0.389
(0.000)"
REFERENCES,0.8405511811023622,"0.134
(0.001) /
0.252
(0.001)"
REFERENCES,0.84251968503937,"L2Emb
0.070
(0.003) /
0.070
(0.003)"
REFERENCES,0.844488188976378,"0.114
(0.002) /
0.199
(0.004)"
REFERENCES,0.8464566929133859,"0.136
(0.001) /
0.276
(0.006)"
REFERENCES,0.8484251968503937,"0.207
(0.001) /
0.321
(0.007)"
REFERENCES,0.8503937007874016,"0.132
(0.002) /
0.216
(0.004)"
REFERENCES,0.8523622047244095,"SHARQ
0.088
(0.002) /
0.085
(0.002)"
REFERENCES,0.8543307086614174,"0.142
(0.001) /
0.199
(0.001)"
REFERENCES,0.8562992125984252,"0.156
(0.001) /
0.335
(0.001)"
REFERENCES,0.8582677165354331,"0.230
(0.000) /
0.404
(0.000)"
REFERENCES,0.860236220472441,"0.154
(0.000) /
0.256
(0.000)"
REFERENCES,0.8622047244094488,"RNN+ERM
0.056
(0.002) /
0.058
(0.002)"
REFERENCES,0.8641732283464567,"0.103
(0.001) /
0.185
(0.003)"
REFERENCES,0.8661417322834646,"0.129
(0.001) /
0.283
(0.005)"
REFERENCES,0.8681102362204725,"0.220
(0.001) /
0.348
(0.005)"
REFERENCES,0.8700787401574803,"0.127
(0.001) /
0.219
(0.003)"
REFERENCES,0.8720472440944882,"Hier-E2E
0.120
(0.005) /
0.125
(0.006)"
REFERENCES,0.8740157480314961,"0.206
(0.003) /
0.334
(0.005)"
REFERENCES,0.8759842519685039,"0.247
(0.002) /
0.448
(0.006)"
REFERENCES,0.8779527559055118,"0.409
(0.007) /
0.573
(0.014)"
REFERENCES,0.8799212598425197,"0.245
(0.004) /
0.370
(0.007)"
REFERENCES,0.8818897637795275,"Hier-E2E Large
0.082
(0.002) /
0.077
(0.002)"
REFERENCES,0.8838582677165354,"0.168
(0.003) /
0.263
(0.010)"
REFERENCES,0.8858267716535433,"0.190
(0.002) /
0.360
(0.003)"
REFERENCES,0.8877952755905512,"0.314
(0.002) /
0.440
(0.001)"
REFERENCES,0.889763779527559,"0.189
(0.002) /
0.285
(0.003)"
REFERENCES,0.8917322834645669,"Tourism
Level 0
Level 1
Level 2
Level 3
Level 4
Mean"
REFERENCES,0.8937007874015748,"HIRED
0.059
(0.001) /
0.061
(0.001)"
REFERENCES,0.8956692913385826,"0.125
(0.001) /
0.162
(0.003)"
REFERENCES,0.8976377952755905,"0.172
(0.001) /
0.225
(0.002)"
REFERENCES,0.8996062992125984,"0.229
(0.001) /
0.376
(0.004)"
REFERENCES,0.9015748031496063,"0.347
(0.001) /
0.786
(0.007)"
REFERENCES,0.9035433070866141,"0.186
(0.001) /
0.322
(0.002)"
REFERENCES,0.905511811023622,"RNN
0.110
(0.001) /
0.106
(0.001)"
REFERENCES,0.90748031496063,"0.148
(0.001) /
0.164
(0.002)"
REFERENCES,0.9094488188976378,"0.188
(0.001) /
0.231
(0.001)"
REFERENCES,0.9114173228346457,"0.240
(0.000) /
0.385
(0.006)"
REFERENCES,0.9133858267716536,"0.369
(0.001) /
0.782
(0.012)"
REFERENCES,0.9153543307086615,"0.211
(0.001) /
0.333
(0.002)"
REFERENCES,0.9173228346456693,"DF
0.097
(0.003) /
0.096
(0.002)"
REFERENCES,0.9192913385826772,"0.141
(0.002) /
0.170
(0.002)"
REFERENCES,0.9212598425196851,"0.187
(0.001) /
0.240
(0.002)"
REFERENCES,0.9232283464566929,"0.241
(0.001) /
0.380
(0.002)"
REFERENCES,0.9251968503937008,"0.355
(0.000) /
0.783
(0.014)"
REFERENCES,0.9271653543307087,"0.204
(0.001) /
0.334
(0.003)"
REFERENCES,0.9291338582677166,"DeepGLO
0.089
(0.0002) /
0.079
(0.0002)"
REFERENCES,0.9311023622047244,"0.126
(0.0001) /
0.158
(0.0001)"
REFERENCES,0.9330708661417323,"0.179
(0.0001) /
0.218
(0.0001)"
REFERENCES,0.9350393700787402,"0.234
(0.0001) /
0.372
(0.0001)"
REFERENCES,0.937007874015748,"0.364
(0.0001) /
0.900
(0.0002)"
REFERENCES,0.9389763779527559,"0.199
(0.0001) /
0.346
(0.0001)"
REFERENCES,0.9409448818897638,"DCRNN
0.187
(0.003) /
0.171
(0.003)"
REFERENCES,0.9429133858267716,"0.231
(0.002) /
0.248
(0.003)"
REFERENCES,0.9448818897637795,"0.258
(0.001) /
0.279
(0.002)"
REFERENCES,0.9468503937007874,"0.293
(0.001) /
0.398
(0.001)"
REFERENCES,0.9488188976377953,"0.434
(0.000) /
0.865
(0.000)"
REFERENCES,0.9507874015748031,"0.281
(0.000) /
0.392
(0.001)"
REFERENCES,0.952755905511811,"L2Emb
0.114
(0.007) /
0.115
(0.007)"
REFERENCES,0.9547244094488189,"0.153
(0.002) /
0.180
(0.004)"
REFERENCES,0.9566929133858267,"0.192
(0.002) /
0.244
(0.002)"
REFERENCES,0.9586614173228346,"0.245
(0.001) /
0.385
(0.002)"
REFERENCES,0.9606299212598425,"0.372
(0.002) /
0.789
(0.010)"
REFERENCES,0.9625984251968503,"0.215
(0.002) /
0.342
(0.003)"
REFERENCES,0.9645669291338582,"SHARQ
0.100
(0.005) /
0.104
(0.005)"
REFERENCES,0.9665354330708661,"0.164
(0.002) /
0.209
(0.001)"
REFERENCES,0.968503937007874,"0.217
(0.003) /
0.260
(0.002)"
REFERENCES,0.9704724409448819,"0.265
(0.003) /
0.386
(0.001)"
REFERENCES,0.9724409448818898,"0.399
(0.003) /
0.931
(0.004)"
REFERENCES,0.9744094488188977,"0.229
(0.001) /
0.378
(0.001)"
REFERENCES,0.9763779527559056,"RNN+ERM
0.078
(0.005) /
0.079
(0.005)"
REFERENCES,0.9783464566929134,"0.155
(0.003) /
0.206
(0.006)"
REFERENCES,0.9803149606299213,"0.225
(0.004) /
0.291
(0.006)"
REFERENCES,0.9822834645669292,"0.307
(0.006) /
0.498
(0.008)"
REFERENCES,0.984251968503937,"0.488
(0.009) /
1.013
(0.010)"
REFERENCES,0.9862204724409449,"0.251
(0.005) /
0.417
(0.006)"
REFERENCES,0.9881889763779528,"Hier-E2E
0.110
(0.002) /
0.113
(0.002)"
REFERENCES,0.9901574803149606,"0.143
(0.002) /
0.161
(0.003)"
REFERENCES,0.9921259842519685,"0.187
(0.002) /
0.232
(0.003)"
REFERENCES,0.9940944881889764,"0.240
(0.001) /
0.371
(0.004)"
REFERENCES,0.9960629921259843,"0.358
(0.001) /
0.824
(0.003)"
REFERENCES,0.9980314960629921,"0.208
(0.001) /
0.340
(0.002)"
