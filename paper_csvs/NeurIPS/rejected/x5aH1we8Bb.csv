Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001941747572815534,"Deep neural networks (DNNs) have been proven extremely susceptible to adver-
1"
ABSTRACT,0.003883495145631068,"sarial examples, which raises special safety-critical concerns for DNN-based au-
2"
ABSTRACT,0.005825242718446602,"tonomous driving stacks (i.e., 3D object detection). Although there are extensive
3"
ABSTRACT,0.007766990291262136,"works on image-level attacks, most are restricted to 2D pixel spaces, and such at-
4"
ABSTRACT,0.009708737864077669,"tacks are not always physically realistic in our 3D world. Here we present Adv3D,
5"
ABSTRACT,0.011650485436893204,"the first exploration of modeling adversarial examples as Neural Radiance Fields
6"
ABSTRACT,0.013592233009708738,"(NeRFs). Advances in NeRF provide photorealistic appearances and 3D accurate
7"
ABSTRACT,0.015533980582524271,"generation, yielding a more realistic and realizable adversarial example. We train
8"
ABSTRACT,0.017475728155339806,"our adversarial NeRF by minimizing the surrounding objects‚Äô confidence predicted
9"
ABSTRACT,0.019417475728155338,"by 3D detectors on the training set. Then we evaluate Adv3D on the unseen valida-
10"
ABSTRACT,0.021359223300970873,"tion set and show that it can cause a large performance reduction when rendering
11"
ABSTRACT,0.02330097087378641,"NeRF in any sampled pose. To generate physically realizable adversarial examples,
12"
ABSTRACT,0.02524271844660194,"we propose primitive-aware sampling and semantic-guided regularization that en-
13"
ABSTRACT,0.027184466019417475,"able 3D patch attacks with camouflage adversarial texture. Experimental results
14"
ABSTRACT,0.02912621359223301,"demonstrate that the trained adversarial NeRF generalizes well to different poses,
15"
ABSTRACT,0.031067961165048542,"scenes, and 3D detectors. Finally, we provide a defense method to our attacks that
16"
ABSTRACT,0.03300970873786408,"involves adversarial training through data augmentation.
17"
INTRODUCTION,0.03495145631067961,"1
Introduction
18"
INTRODUCTION,0.036893203883495145,"The perception system of self-driving cars heavily rely on DNNs to process input data and comprehend
19"
INTRODUCTION,0.038834951456310676,"the environment. Although DNNs have exhibited great improvements in performance, they have been
20"
INTRODUCTION,0.040776699029126215,"found vulnerable to adversarial examples [2,15,24,41]. These adversarial examples crafted by adding
21"
INTRODUCTION,0.04271844660194175,"imperceptible perturbations to input data, can lead DNNs to make wrong predictions. Motivated
22"
INTRODUCTION,0.04466019417475728,"by the safety-critical nature of self-driving cars, we aim to explore the possibility of generating
23"
INTRODUCTION,0.04660194174757282,"physically realizable adversarial examples to disrupt 3D detectors in driving scenarios, and further
24"
INTRODUCTION,0.04854368932038835,"improve the robustness of 3D detectors through adversarial training.
25"
INTRODUCTION,0.05048543689320388,"The 2D pixel perturbations (digital attacks) [15,41] have been proven effective in attacking DNNs in
26"
INTRODUCTION,0.05242718446601942,"various computer vision tasks [13,53,56]. However, these 2D pixel attacks are restricted to digital
27"
INTRODUCTION,0.05436893203883495,"space and are difficult to realize in our 3D world. To address this challenge, several works have
28"
INTRODUCTION,0.05631067961165048,"proposed physical attacks. For example, Athalye et al. [2] propose the framework of Expectation Over
29"
INTRODUCTION,0.05825242718446602,"Transformation (EOT) to improve the attack robustness over 3D transformation. Other researchers
30"
INTRODUCTION,0.06019417475728155,"generate adversarial examples beyond image space through differentiable rendering, as seen in [54,59].
31"
INTRODUCTION,0.062135922330097085,"These methods show great promise for advancing the field of 3D adversarial attacks and defense but
32"
INTRODUCTION,0.06407766990291262,"are still limited in synthetic environments.
33"
INTRODUCTION,0.06601941747572816,"Given the safety-critical demand for self-driving cars, several works have proposed physically
34"
INTRODUCTION,0.06796116504854369,"realizable attacks and defense methods in driving scenarios. For example, Cao et al. [5,6] propose to
35"
INTRODUCTION,0.06990291262135923,"learn 3D-aware adversarial attacks capable of generating adversarial mesh to attack 3D detectors.
36"
INTRODUCTION,0.07184466019417475,"Methods
Transferability
Adv. Type
Additional Requirements"
INTRODUCTION,0.07378640776699029,"Cao et al. [5,6]
Poses
3D Mesh
Model, Annotation
Tu et al. [43,44]
Poses, Scenes
3D Mesh
Model, Annotation
Xie et al. [57]
Scenes, Categories
2D Patch
Model, Annotation"
INTRODUCTION,0.07572815533980583,"Adv3D
Poses, Scenes, Categories
3D NeRF
Model"
INTRODUCTION,0.07766990291262135,Table 1: Comparison with prior works of adversarial attack in autonomous driving.
INTRODUCTION,0.07961165048543689,"However, their works only consider learning a 3D adversarial example for a few specific frames.
37"
INTRODUCTION,0.08155339805825243,"Thus, the learned example is not universal and may not transfer to other scenes. To mitigate this
38"
INTRODUCTION,0.08349514563106795,"problem, Tu et al. [43,44] propose to learn a transferable adversary that is placed on top of a vehicle.
39"
INTRODUCTION,0.0854368932038835,"Such an adversary can be used in any scene to hide the attacked object from 3D detectors. However,
40"
INTRODUCTION,0.08737864077669903,"reproducing their attack in our physical world can be challenging since their adversary must have
41"
INTRODUCTION,0.08932038834951456,"direct contact with the attacked object. We list detailed comparisons of prior works in Tab. 1.
42"
INTRODUCTION,0.0912621359223301,"To address the above challenges and generate 3D adversarial examples in driving scenarios, we build
43"
INTRODUCTION,0.09320388349514563,"Adv3D upon recent advances in NeRF [35] that provide both differentiable rendering and realistic
44"
INTRODUCTION,0.09514563106796116,"synthesis. In order to generate physically realizable attacks, we model Adv3D in a patch-attack [40]
45"
INTRODUCTION,0.0970873786407767,"manner and use an optimization-based approach that starts with a realistic NeRF object [26] to learn
46"
INTRODUCTION,0.09902912621359224,"its 3D adversarial texture. We optimize the adversarial texture to minimize the predicted confidence
47"
INTRODUCTION,0.10097087378640776,"of all objects in the scenes, while keeping shape unchanged. During the evaluation, we render the
48"
INTRODUCTION,0.1029126213592233,"input agnostic NeRF in randomly sampled poses, then we paste the rendered patch onto the unseen
49"
INTRODUCTION,0.10485436893203884,"validation set to evaluate the attack performance. Owing to the transferability to poses and scenes, our
50"
INTRODUCTION,0.10679611650485436,"adversarial examples can be executed without prior knowledge of the scene and do not need direct
51"
INTRODUCTION,0.1087378640776699,"contact with the attacked objects, thus making for more feasible attacks compared with [43,44,57,62].
52"
INTRODUCTION,0.11067961165048544,"Finally, we provide thorough evaluations of Adv3D on camera-based 3D object detection with the
53"
INTRODUCTION,0.11262135922330097,"nuScenes [4] dataset. Our contributions are summarized as follows:
54"
INTRODUCTION,0.1145631067961165,"‚Ä¢ We introduce Adv3D, the first exploration of formulating adversarial examples as NeRF to
55"
INTRODUCTION,0.11650485436893204,"attack 3D detectors in autonomous driving. Adv3D provides 3D-aware and photorealistic
56"
INTRODUCTION,0.11844660194174757,"synthesis that was previously unavailable.
57"
INTRODUCTION,0.1203883495145631,"‚Ä¢ By incorporating the proposed primitive-aware sampling and semantic-guided regularization,
58"
INTRODUCTION,0.12233009708737864,"Adv3D generates adversarial examples with enhanced physical realism and realizability.
59"
INTRODUCTION,0.12427184466019417,"‚Ä¢ We conduct extensive real-world experiments and demonstrate the transferability of our
60"
INTRODUCTION,0.1262135922330097,"adversarial examples across unseen environments and detectors.
61"
RELATED WORK,0.12815533980582525,"2
Related Work
62"
ADVERSARIAL ATTACK,0.13009708737864079,"2.1
Adversarial Attack
63"
ADVERSARIAL ATTACK,0.13203883495145632,"DNNs are known to be vulnerable to adversarial attacks, where a small perturbation in the input data
64"
ADVERSARIAL ATTACK,0.13398058252427184,"can cause drastic changes in the output predictions. Szegedy et al. [41] first discovered that adversarial
65"
ADVERSARIAL ATTACK,0.13592233009708737,"examples, generated by adding visually imperceptible perturbations to the original images, make
66"
ADVERSARIAL ATTACK,0.1378640776699029,"DNNs predict a wrong category with high confidence. These vulnerabilities were also discovered in
67"
ADVERSARIAL ATTACK,0.13980582524271845,"object detection and semantic segmentation [30,56]. Moreover, DPatch [30] proposes transferable
68"
ADVERSARIAL ATTACK,0.141747572815534,"patch-based attacks by compositing a small patch to the input image. However, perturbing image
69"
ADVERSARIAL ATTACK,0.1436893203883495,"pixels alone does not guarantee that adversarial examples can be created in the physical world. To
70"
ADVERSARIAL ATTACK,0.14563106796116504,"address this issue, several works have performed physical attacks [3, 8, 18, 23, 46, 52, 58, 61] and
71"
ADVERSARIAL ATTACK,0.14757281553398058,"exposed real-world threats. For example, Athalye et al. [2] generated robust 3D adversarial objects
72"
ADVERSARIAL ATTACK,0.14951456310679612,"by introducing the Expectation Over Transformation (EOT) method. Cheng et al. [11] developed an
73"
ADVERSARIAL ATTACK,0.15145631067961166,"adversarial patch with physical-oriented transformations to attack a depth estimation network. In our
74"
ADVERSARIAL ATTACK,0.1533980582524272,"work, we mainly aim to generate 3D adversarial examples for 3D object detection in driving scenarios.
75"
ROBUSTNESS IN AUTONOMOUS DRIVING,0.1553398058252427,"2.2
Robustness in Autonomous Driving
76"
ROBUSTNESS IN AUTONOMOUS DRIVING,0.15728155339805824,"With the safety-critical nature, it is necessary to pay special attention to robustness in autonomous
77"
ROBUSTNESS IN AUTONOMOUS DRIVING,0.15922330097087378,"driving systems [47]. LiDAR-Adv [6] proposes to learn input-specific adversarial point clouds to
78"
ROBUSTNESS IN AUTONOMOUS DRIVING,0.16116504854368932,"fool LiDAR detectors. Tu et al. [44] produces generalizable point clouds that can be placed on a
79"
ROBUSTNESS IN AUTONOMOUS DRIVING,0.16310679611650486,"vehicle roof to hide it. Furthermore, several work [1,5,43] try to attack a multi-sensor fusion system
80"
ROBUSTNESS IN AUTONOMOUS DRIVING,0.1650485436893204,"by optimizing 3D mesh through differentiable rendering. We compare our method with prior works
81"
ROBUSTNESS IN AUTONOMOUS DRIVING,0.1669902912621359,"in Tab. 1. Our method demonstrates stronger transferability and fewer requirements than prior works.
82"
IMAGE SYNTHESIS USING NERF,0.16893203883495145,"2.3
Image Synthesis using NeRF
83"
IMAGE SYNTHESIS USING NERF,0.170873786407767,"NeRF [35] enables photorealistic synthesis in a 3D-aware manner. Recent advances [45,60] in NeRF
84"
IMAGE SYNTHESIS USING NERF,0.17281553398058253,"allow for control over materials, illumination, and 6D pose of objects. Additionally, NeRF‚Äôs rendering
85"
IMAGE SYNTHESIS USING NERF,0.17475728155339806,"comes directly from real-world reconstruction, providing more physically accurate and photorealistic
86"
IMAGE SYNTHESIS USING NERF,0.1766990291262136,"synthesis than previous mesh-based methods that relied on human handicrafts. Moreover, volumetric
87"
IMAGE SYNTHESIS USING NERF,0.1786407766990291,"rendering [19] enables NeRF to perform accurate and efficient gradient computation compared with
88"
IMAGE SYNTHESIS USING NERF,0.18058252427184465,"dedicated renderers in mesh-based differentiable rendering [9,21,29].
89"
IMAGE SYNTHESIS USING NERF,0.1825242718446602,"Recently, there has been tremendous progress in driving scene simulation using NeRF. Block-
90"
IMAGE SYNTHESIS USING NERF,0.18446601941747573,"NeRF [42] achieves city-scale reconstruction by modeling the blocks of cities with several isolated
91"
IMAGE SYNTHESIS USING NERF,0.18640776699029127,"NeRFs to increase capacity. FEGR [51] learns to intrinsically decompose the driving scene for
92"
IMAGE SYNTHESIS USING NERF,0.1883495145631068,"applications such as relighting. Lift3D [26] use NeRF to generate new objects and augment them to
93"
IMAGE SYNTHESIS USING NERF,0.19029126213592232,"driving datasets, demonstrating the capability of NeRF to improve downstream task performance. The
94"
IMAGE SYNTHESIS USING NERF,0.19223300970873786,"driving scene simulation provides a perfect test bed to evaluate the effectiveness of self-driving cars.
95"
IMAGE SYNTHESIS USING NERF,0.1941747572815534,"Our method is related to Lift3D, but aims to understand and improve the robustness of 3D detectors.
96"
PRELIMINARY,0.19611650485436893,"3
Preliminary
97"
PRELIMINARY,0.19805825242718447,"3.1
Camera-based 3D Object Detection in Autonomous Driving
98"
PRELIMINARY,0.2,"Camera-based 3D object detection is the fundamental task in autonomous driving. Without loss of
99"
PRELIMINARY,0.20194174757281552,"generality, we focus on evaluating the robustness of camera-based 3D detectors.
100"
PRELIMINARY,0.20388349514563106,"The 3D detectors process image data and aim to predict 3D bounding boxes of all surrounding objects.
101"
PRELIMINARY,0.2058252427184466,"The parameterization of a 3D bounding box can be written as b = {R, t, s, c}, where R ‚ààSO(3) is
102"
PRELIMINARY,0.20776699029126214,"the rotation of the box, t = (x, y, z) indicate translation of the box center, s = (l, w, h) represent the
103"
PRELIMINARY,0.20970873786407768,"size (length, width, and height) of the box, and c is the confidence of the predicted box.
104"
PRELIMINARY,0.21165048543689322,"The network structure of camera-based 3D object detectors can be roughly categorized into FoV-
105"
PRELIMINARY,0.21359223300970873,"based (front of view) and BEV-based (bird‚Äôs eye view). FoV-based methods [48,49,50] can be easily
106"
PRELIMINARY,0.21553398058252426,"built by adding 3D attribute branches to 2D detectors. BEV-based methods [38,39] typically convert
107"
PRELIMINARY,0.2174757281553398,"2D image feature to BEV feature using camera parameters, then directly detect objects on BEV
108"
PRELIMINARY,0.21941747572815534,"planes. We refer readers to recent surveys [25,31] for more detail.
109"
DIFFERENTIABLE RENDERING USING NERF,0.22135922330097088,"3.2
Differentiable Rendering using NeRF
110"
DIFFERENTIABLE RENDERING USING NERF,0.22330097087378642,"Our method leverages the differentiable rendering scheme proposed by NeRF [35]. NeRF parame-
111"
DIFFERENTIABLE RENDERING USING NERF,0.22524271844660193,"terizes the volumetric density and color as a function of input coordinates. NeRF uses multi-layer
112"
DIFFERENTIABLE RENDERING USING NERF,0.22718446601941747,"perceptron (MLP) or hybrid neural representations [7,14,36] to represent this function. For each
113"
DIFFERENTIABLE RENDERING USING NERF,0.229126213592233,"pixel on an image, a ray r(t) = ro + rd ¬∑ t is cast from the camera‚Äôs origin ro and passes through the
114"
DIFFERENTIABLE RENDERING USING NERF,0.23106796116504855,"direction of the pixel rd at distance t. In a ray, we uniformly sample K points from the near plane
115"
DIFFERENTIABLE RENDERING USING NERF,0.23300970873786409,"tnear to the far plane tfar, the kth distance is thus calculated as tk = tnear + (tfar ‚àítnear) ¬∑ k/K.
116"
DIFFERENTIABLE RENDERING USING NERF,0.23495145631067962,"For any queried point r(tk) on the ray, the network takes its position r (tk) and predicts the per-point
117"
DIFFERENTIABLE RENDERING USING NERF,0.23689320388349513,"color ck and density œÑk with:
118"
DIFFERENTIABLE RENDERING USING NERF,0.23883495145631067,"(ck, œÑk) = Network (r (tk)) .
(1)
Note that we omit the direction term as suggested by [16]. The final predicted color of each pixel
119"
DIFFERENTIABLE RENDERING USING NERF,0.2407766990291262,"C(r) is computed by approximating the volume rendering integral using numerical quadrature [34]:
120"
DIFFERENTIABLE RENDERING USING NERF,0.24271844660194175,"C(r) = K‚àí1
X"
DIFFERENTIABLE RENDERING USING NERF,0.2446601941747573,"k=0
Tk (1 ‚àíexp (‚àíœÑk (tk+1 ‚àítk))) ck,"
DIFFERENTIABLE RENDERING USING NERF,0.24660194174757283,"with
Tk = exp  ‚àí
X"
DIFFERENTIABLE RENDERING USING NERF,0.24854368932038834,"k‚Ä≤<k
œÑk‚Ä≤ (tk‚Ä≤+1 ‚àítk‚Ä≤) ! . (2)"
DIFFERENTIABLE RENDERING USING NERF,0.2504854368932039,"We build our NeRF upon Lift3D [26]. Lift3D is a 3D generation framework that generates photoreal-
121"
DIFFERENTIABLE RENDERING USING NERF,0.2524271844660194,"istic objects by fitting multi-view images synthesized by 2D generative modes [20] using NeRF. The
122"
DIFFERENTIABLE RENDERING USING NERF,0.2543689320388349,"network of Lift3D is a conditional NeRF with additional latent code input, which controls the shape
123 ùë•ùë• ùë¶ùë¶ ùëßùëß Shape"
DIFFERENTIABLE RENDERING USING NERF,0.2563106796116505,Texture
DIFFERENTIABLE RENDERING USING NERF,0.258252427184466,NeRF network
DIFFERENTIABLE RENDERING USING NERF,0.26019417475728157,"Sampled
Original Image"
DIFFERENTIABLE RENDERING USING NERF,0.2621359223300971,"Pretrained
3D Detectors"
DIFFERENTIABLE RENDERING USING NERF,0.26407766990291265,"Pose Sampling
Primitive-aware Sampling"
DIFFERENTIABLE RENDERING USING NERF,0.26601941747572816,Attacked Image
DIFFERENTIABLE RENDERING USING NERF,0.26796116504854367,Optimize
DIFFERENTIABLE RENDERING USING NERF,0.26990291262135924,Step1: Adversary Generation
DIFFERENTIABLE RENDERING USING NERF,0.27184466019417475,Step2: Gradient Propagation
DIFFERENTIABLE RENDERING USING NERF,0.2737864077669903,"Image 
Composition"
DIFFERENTIABLE RENDERING USING NERF,0.2757281553398058,"Objective: 
minimize all the 
confidences"
DIFFERENTIABLE RENDERING USING NERF,0.27766990291262134,Patch Rendered by NeRF
DIFFERENTIABLE RENDERING USING NERF,0.2796116504854369,"Figure 1: Adv3D aims to generate 3D adversarial examples that consistently perform attacks under
different poses during rendering. We initialize adversarial examples from Lift3D [26]. During
training, we optimize the texture latent codes of NeRF to minimize the detection confidence of
all surrounding objects. During inference, we evaluate the performance reduction of pasting the
adversarial patch rendered using randomly sampled poses on the validation set."
DIFFERENTIABLE RENDERING USING NERF,0.2815533980582524,"and texture of the rendered object. The conditional NeRF in Lift3D is a tri-plane parameterized [7]
124"
DIFFERENTIABLE RENDERING USING NERF,0.283495145631068,"generator. With its realistic generation and 3D controllability, Lift3D has demonstrated that the train-
125"
DIFFERENTIABLE RENDERING USING NERF,0.2854368932038835,"ing data generated by NeRF can help to improve downstream task performance. To further explore
126"
DIFFERENTIABLE RENDERING USING NERF,0.287378640776699,"and exploit the satisfactory property of NeRF, we present a valuable and important application in
127"
DIFFERENTIABLE RENDERING USING NERF,0.28932038834951457,"this work: we leverage the NeRF-generated data to investigate and improve the robustness of the
128"
DIFFERENTIABLE RENDERING USING NERF,0.2912621359223301,"perception system in self-driving cars.
129"
METHOD,0.29320388349514565,"4
Method
130"
METHOD,0.29514563106796116,"We illustrate the pipeline of Adv3D in Fig. 1. We aim to learn a transferable adversarial example in 3D
131"
METHOD,0.2970873786407767,"detection that, when rendered in any pose (i.e., location and rotation), can effectively hide surrounding
132"
METHOD,0.29902912621359223,"objects from 3D detectors in any scenes by lowering their confidence. In Sec. 4.1, to improve the
133"
METHOD,0.30097087378640774,"physical realizability of adversarial examples, we propose (1) Primitive-aware sampling to enable
134"
METHOD,0.3029126213592233,"3D patch attacks. (2) Disentangle NeRF that provides feasible geometry, and (3) Semantic-guided
135"
METHOD,0.3048543689320388,"regularization that enables camouflage adversarial texture. To enhance the transferability across poses
136"
METHOD,0.3067961165048544,"and scenes, we formulate the learning paradigm of Adv3D within the EOT framework [2] in Sec. 4.3.
137"
METHOD,0.3087378640776699,"4.1
3D Adversarial Example Generation
138"
METHOD,0.3106796116504854,"We use a gradient-based method to train our adversarial examples. The training pipeline involves 4
139"
METHOD,0.312621359223301,"steps: (i) randomly sampling the pose of an adversarial example, (ii) rendering the example in the
140"
METHOD,0.3145631067961165,"sampled pose, (iii) pasting the rendered patch into the original image of the training set, and finally,
141"
METHOD,0.31650485436893205,"(iv) computing the loss and optimizing the latent codes. During inference, we discard the (iv) step.
142"
POSE SAMPLING,0.31844660194174756,"4.1.1
Pose Sampling
143"
POSE SAMPLING,0.32038834951456313,"To achieve adversarial attack in arbitrary object poses, we apply Expectation of Transformation
144"
POSE SAMPLING,0.32233009708737864,"(EOT) [2] by randomly sampling object poses. The poses of adversarial examples are parameterized
145"
POSE SAMPLING,0.32427184466019415,"as 3D boxes b that are restricted to a predefined ground plane in front of the camera. We model the
146"
POSE SAMPLING,0.3262135922330097,"ground plane as a uniform distribution B in a specific range that is detailed in the supplement. During
147"
POSE SAMPLING,0.32815533980582523,"training, we independently sample the rendering poses of adversarial examples, and approximate the
148"
POSE SAMPLING,0.3300970873786408,"expectation by taking the average loss over the whole batch.
149"
PRIMITIVE-AWARE SAMPLING,0.3320388349514563,"4.1.2
Primitive-aware Sampling
150"
PRIMITIVE-AWARE SAMPLING,0.3339805825242718,"We model the primitive of adversarial examples as NeRF tightly bound by 3D boxes, in order to enable
151"
PRIMITIVE-AWARE SAMPLING,0.3359223300970874,"non-contact and physically realizable attacks. During volume rendering, we compute the intersection
152"
PRIMITIVE-AWARE SAMPLING,0.3378640776699029,"of rays r(t) with the sampled pose b = {R, t, s} ‚ààB, finding the first hit point and the last hit point
153"
PRIMITIVE-AWARE SAMPLING,0.33980582524271846,"of box (tnear, tfar) by the AABB-ray intersection algorithm [33]. We then sample our points inside
154"
PRIMITIVE-AWARE SAMPLING,0.341747572815534,"the range (tnear, tfar) to reduce large unnecessary samples and avoid contact with the environment.
155"
PRIMITIVE-AWARE SAMPLING,0.34368932038834954,"(tnear, tfar) = Intersect(r, b),
(3)"
PRIMITIVE-AWARE SAMPLING,0.34563106796116505,"r‚Ä≤(tk) = Àúr(tnear) + (Àúr(tfar) ‚àíÀúr(tnear)) ¬∑ k/K,
(4)
Àúr(t) = Transform(r(t), b),
(5)
where Àúr(t) is the sampled points with additional global to local transformation. Specifically, we use a
156"
PRIMITIVE-AWARE SAMPLING,0.34757281553398056,"3D affine transformation to map original sampled points r(t) = ro + rd ¬∑ t into a canonical space
157"
PRIMITIVE-AWARE SAMPLING,0.34951456310679613,"Àúr = {x, y, z} ‚àà[‚àí1, 1]. This ensures that all the sampled points regardless of their distance from the
158"
PRIMITIVE-AWARE SAMPLING,0.35145631067961164,"origin, are transformed to the range [‚àí1, 1], thus providing a compact input representation for NeRF
159"
PRIMITIVE-AWARE SAMPLING,0.3533980582524272,"network. The transformation is given by:
160"
PRIMITIVE-AWARE SAMPLING,0.3553398058252427,"Transform(r, b) = s‚àí1 ¬∑ (R‚àí1 ¬∑ r ‚àít),
(6)
where b = {R, t, s}, R ‚ààSO(3) is rotation matrix of the box, t, s ‚ààR3 indicate translation and
161"
PRIMITIVE-AWARE SAMPLING,0.3572815533980582,"scale vector that move and scale the unit cube to desired location and size. The parameters of b are
162"
PRIMITIVE-AWARE SAMPLING,0.3592233009708738,"sampled from a pre-defined distribution B detailed in the supplement.
163"
PRIMITIVE-AWARE SAMPLING,0.3611650485436893,"Then, the points lied in [‚àí1, 1] are projected to exactly cover the tri-plane features z for interpolation.
164"
PRIMITIVE-AWARE SAMPLING,0.36310679611650487,"Finally, a small MLP takes the interpolated features as input and predicts RGB and density:
165"
PRIMITIVE-AWARE SAMPLING,0.3650485436893204,"(ck, œÑk) = MLP(Interpolate(z, r‚Ä≤ (tk))).
(7)
The primitive-aware sampling enables patch attacks [40] in a 3D-aware manner by lifting the 2D
166"
PRIMITIVE-AWARE SAMPLING,0.36699029126213595,"patch to a 3D box, enhancing the physical realizability by ensuring that the adversarial example only
167"
PRIMITIVE-AWARE SAMPLING,0.36893203883495146,"has a small modification to the original 3D environment.
168"
DISENTANGLED NERF PARAMETERIZATION,0.37087378640776697,"4.1.3
Disentangled NeRF Parameterization
169"
DISENTANGLED NERF PARAMETERIZATION,0.37281553398058254,"The original parameterization of NeRF combines the shape and texture into a single MLP, resulting in
170"
DISENTANGLED NERF PARAMETERIZATION,0.37475728155339805,"an entangled shape and texture generation. Since shape variation is challenging to reproduce in the real
171"
DISENTANGLED NERF PARAMETERIZATION,0.3766990291262136,"world, we disentangle shape and texture generation and only set the texture as adversarial examples.
172"
DISENTANGLED NERF PARAMETERIZATION,0.3786407766990291,"We obtain texture latents ztex. and shape latents zshape from the Lift3D. During volume rendering,
173"
DISENTANGLED NERF PARAMETERIZATION,0.38058252427184464,"we disentangle shape and texture generation by separately predicting RGB and density:
174"
DISENTANGLED NERF PARAMETERIZATION,0.3825242718446602,"ck = Network(ztex., r‚Ä≤ (tk)),
œÑk = Network(zshape, r‚Ä≤ (tk)),
(8)
where zshape is fixed and ztexture is being optimized. Our disentangled parametrization can also
175"
DISENTANGLED NERF PARAMETERIZATION,0.3844660194174757,"be seen as a geometry regularization in [43,44] but keeps geometry unchanged as a usual vehicle,
176"
DISENTANGLED NERF PARAMETERIZATION,0.3864077669902913,"leading to a more realizable adversarial example.
177"
SEMANTIC-GUIDED REGULARIZATION,0.3883495145631068,"4.1.4
Semantic-guided Regularization
178"
SEMANTIC-GUIDED REGULARIZATION,0.39029126213592236,"Setting the full part of the vehicle as adversarial textures is straightforward, but not always feasible in
179"
SEMANTIC-GUIDED REGULARIZATION,0.39223300970873787,"the real world. To improve the physical realizability, we propose to optimize individual semantic parts,
180"
SEMANTIC-GUIDED REGULARIZATION,0.3941747572815534,"such as doors and windows of a vehicle. Specifically, as shown in Fig. 2 (d, e)), we only set a specific
181"
SEMANTIC-GUIDED REGULARIZATION,0.39611650485436894,"part of the vehicle as adversarial texture while maintaining others unchanged. This semantic-guided
182"
SEMANTIC-GUIDED REGULARIZATION,0.39805825242718446,"regularization leads to a camouflage adversarial texture that is less likely spotted in the real world.
183"
SEMANTIC-GUIDED REGULARIZATION,0.4,"To achieve this, we add a semantic branch to Lift3d [26] to predict semantic part labels of the
184"
SEMANTIC-GUIDED REGULARIZATION,0.40194174757281553,"sampled points. We re-train Lift3d by fitting multi-view images and semantic labels generated by
185"
SEMANTIC-GUIDED REGULARIZATION,0.40388349514563104,"editGAN [28]. Using semantic-guided regularization, we maintain the original texture and adversarial
186"
SEMANTIC-GUIDED REGULARIZATION,0.4058252427184466,"part texture at the same time but only optimize the adversarial part texture while leaving the original
187"
SEMANTIC-GUIDED REGULARIZATION,0.4077669902912621,"texture unchanged. This approach allows us to preserve a large majority of parts as usual, but to alter
188"
SEMANTIC-GUIDED REGULARIZATION,0.4097087378640777,"only the specific parts that are adversarial (see Fig. 2 (b, c)). Potential attackers can easily print the
189"
SEMANTIC-GUIDED REGULARIZATION,0.4116504854368932,"adversarial sticker and stick it on the semantic part of vehicles to hide surrounding objects.
190"
SEMANTIC-GUIDED REGULARIZATION,0.41359223300970877,"In our implementation, we query the NeRF network twice, one for the adversarial texture and the
191"
SEMANTIC-GUIDED REGULARIZATION,0.4155339805825243,"other for the original texture. Then, we replace the part of original texture with the adversarial texture
192"
SEMANTIC-GUIDED REGULARIZATION,0.4174757281553398,"indexed by semantic labels in the point space.
193"
GRADIENT PROPAGATION,0.41941747572815535,"4.2
Gradient Propagation
194"
GRADIENT PROPAGATION,0.42135922330097086,"After rendering the adversarial examples, we paste the adversarial patch into the original image
195"
GRADIENT PROPAGATION,0.42330097087378643,"through image composition. The attacked image can be expressed as I1 √ó M + I2 √ó (1 ‚àíM) where
196"
GRADIENT PROPAGATION,0.42524271844660194,"I1 and I2 are the patch and original image, M is foreground mask predicted by NeRF. Next, the
197"
GRADIENT PROPAGATION,0.42718446601941745,"attacked images are fed to pretrained and fixed 3D detectors to compute the objective and back-
198"
GRADIENT PROPAGATION,0.429126213592233,"propagate the gradients. Since both the rendering and detection pipelines are differentiable, Adv3D
199"
GRADIENT PROPAGATION,0.43106796116504853,"allows gradients from the objective to flow into the texture latent codes during optimization.
200"
GRADIENT PROPAGATION,0.4330097087378641,"(a)
(b)
(c)
(d)
(e)"
GRADIENT PROPAGATION,0.4349514563106796,"Figure 2: Rendered results of our adversarial examples. (a) Image and semantic label of an instance
predicted by NeRF. (b) Top: our example without semantic-guided regularization. Bottom: our
example with semantic-guided regularization. (c) Multi-view consistent synthesis of our examples.
(d,e) The texture transfer results of side and back part adversary to other vehicles."
LEARNING PARADIGM,0.4368932038834951,"4.3
Learning Paradigm
201"
LEARNING PARADIGM,0.4388349514563107,"We formulate our learning paradigm as EOT [2] that finds adversarial texture latent codes by
202"
LEARNING PARADIGM,0.4407766990291262,"minimizing the expectation of a binary cross-entropy loss over sampled poses and original images:
203"
LEARNING PARADIGM,0.44271844660194176,"ztex. = arg min
ztex. Eb‚àºBEx‚àºX [‚àílog(1 ‚àíP(I(x, b, ztex.))],
(9)"
LEARNING PARADIGM,0.4446601941747573,"where b is the rendering pose sampled from the predefined distribution of ground plane B, x is the
204"
LEARNING PARADIGM,0.44660194174757284,"original image sampled from the training set X, I(x, b, ztex.) is the attacked image that composited
205"
LEARNING PARADIGM,0.44854368932038835,"by the original image x and the adversarial patch rendered using pose b and texture latent code
206"
LEARNING PARADIGM,0.45048543689320386,"ztex., and P(I(¬∑)) represents the confidence of all proposals predicted by detectors. We approximate
207"
LEARNING PARADIGM,0.4524271844660194,"the expectation by averaging the objective of the independently sampled batch. The objective is a
208"
LEARNING PARADIGM,0.45436893203883494,"binary cross-entropy loss that minimizes the confidence of all predicted bounding boxes, including
209"
LEARNING PARADIGM,0.4563106796116505,"adversarial objects and normal objects.
210"
LEARNING PARADIGM,0.458252427184466,"Built within the framework of EOT, Adv3D helps to improve the transferability and robustness of
211"
LEARNING PARADIGM,0.4601941747572815,"adversarial examples over the sampling parameters (poses and scenes here). This means that the
212"
LEARNING PARADIGM,0.4621359223300971,"attack can be performed without prior knowledge of the scene and are able to disrupt models across
213"
LEARNING PARADIGM,0.4640776699029126,"different poses and times in a non-contact manner.
214"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.46601941747572817,"4.4
Adversarial Defense by Data Augmentation
215"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.4679611650485437,"Toward defenses against our adversarial attack, we also study adversarial training to improve the
216"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.46990291262135925,"robustness of 3D detectors. Adversarial training is typically performed by adding image perturbations
217"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.47184466019417476,"using a few PGD steps [32,55] during the training of networks. However, our adversarial example is
218"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.47378640776699027,"too expensive to generate for the bi-level loop of the min-max optimization objective. Thus, instead of
219"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.47572815533980584,"generating adversarial examples from scratch at every iteration, we directly leverage the transferable
220"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.47766990291262135,"adversarial examples to augment the training set. We use the trained adversarial example to locally
221"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.4796116504854369,"store a large number of rendered images to avoid repeated computation. During adversarial training,
222"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.4815533980582524,"we randomly paste the rendered adversarial patch into the training images with a probability of 30%,
223"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.48349514563106794,"while remaining others unchanged. We provide experimental results in Sec. 5.4.
224"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.4854368932038835,"Models
Backbone
Type
Clean NDS
Adv NDS
Clean mAP
Adv mAP"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.487378640776699,"FCOS3D [48]
ResNet101
FoV
0.3770
0.2674
0.2980
0.1272
PGD-Det [49]
ResNet101
FoV
0.3934
0.2694
0.3174
0.1321
DETR3D [50]
ResNet101
FoV
0.4220
0.2755
0.3470
0.1336
BEVDet [17]
ResNet50
BEV
0.3822
0.2247
0.3076
0.1325
BEVFormer-Tiny [27]
ResNet50
BEV
0.3540
0.2264
0.2524
0.1217
BEVFormer-Base [27]
ResNet101
BEV
0.5176
0.3800
0.4167
0.2376"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.4893203883495146,"Table 2: Comparison of different detectors under our attack. Clean NDS and mAP denote evaluation
using original validation data. Adv NDS and mAP denote evaluation using attacked data. Pred. GT Ego"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.4912621359223301,"Clean Images
Attacked Images
Clean/Attacked BEV Visualization (a) (b) (c)"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.49320388349514566,"Figure 3: Visualization of BEVDet [17] prediction on nuScenes validation set under our attacks.
The visualization threshold is set at 0.6. The adversarial NeRF can hide surrounding objects by
minimizing their predicted confidence in a non-contact manner (making the yellow boxes disappear)."
EXPERIMENTS,0.49514563106796117,"5
Experiments
225"
EXPERIMENTS,0.4970873786407767,"In this section, we first describe the training detail of our adversarial attacks. Then we present the
226"
EXPERIMENTS,0.49902912621359224,"experiments of semantic-guided regularization in Sec. 5.1, the analysis of 3D attack in Sec. 5.2, the
227"
EXPERIMENTS,0.5009708737864078,"transferability across detectors in Sec. 5.3, and our adversarial defense method in Sec. 5.4.
228"
EXPERIMENTS,0.5029126213592233,"Dataset
We conduct our experiments on the widely used nuScenes dataset [4]. This dataset is
229"
EXPERIMENTS,0.5048543689320388,"collected using 6 surrounded-view cameras that cover the full 360¬∞ field of view around the ego-
230"
EXPERIMENTS,0.5067961165048543,"vehicle. The dataset contains 700 scenes for training and 150 scenes for validation. In our work, we
231"
EXPERIMENTS,0.5087378640776699,"train our adversarial examples on the training set and evaluate performance drop on the validation set.
232"
EXPERIMENTS,0.5106796116504855,"Training
We implement our methods using PyTorch [37] and MMDetection3D [12]. All detectors
233"
EXPERIMENTS,0.512621359223301,"are resumed from checkpoints available on their open-source repositories to match the original
234"
EXPERIMENTS,0.5145631067961165,"performance exactly. We only select one instance from Lift3D [26] as the initialization of examples.
235"
EXPERIMENTS,0.516504854368932,"We conduct our experiments using 8 NVIDIA A100 80G GPUs. We use the Adam optimizer [22] with
236"
EXPERIMENTS,0.5184466019417475,"a learning rate of 1e-3 for texture latents. In practice, we optimize texture latents on the training set for
237"
EXPERIMENTS,0.5203883495145631,"five epochs with the same batch size as used during training detectors. We do not use any regularization
238"
EXPERIMENTS,0.5223300970873787,"except for semantic-guided regularization. In all experiments without specified, we render two
239"
EXPERIMENTS,0.5242718446601942,"adversarial examples per image. We ablate the number of rendered adversaries in the supplement.
240"
EXPERIMENTS,0.5262135922330097,"Target Detectors and Metrics
We evaluate the robustness of six representative detectors. Three are
241"
EXPERIMENTS,0.5281553398058253,"FoV-based, and three are BEV-based. The FoV-based methods are FCOS3D [48], PGD-Det [49] and
242"
EXPERIMENTS,0.5300970873786408,"DETR3D [50]. The BEV-based methods are BEVDet [17], BEVFormer-Tiny [27] and BEVFormer-
243"
EXPERIMENTS,0.5320388349514563,"Base. Following prior work [57], we evaluate the performance drop on the validation set after the
244"
EXPERIMENTS,0.5339805825242718,"attack. Specifically, we use the Mean Average Precision (mAP) and nuScenes Detection Score
245"
EXPERIMENTS,0.5359223300970873,"(NDS) [4] to evaluate the performance of 3D detectors.
246"
EXPERIMENTS,0.537864077669903,"Quantitative Results
We provide the experimental results of adversarial attacks in Tab. 2. The
247"
EXPERIMENTS,0.5398058252427185,"attacks are conducted in a full-part manner without semantic-guided regularization to investigate the
248"
EXPERIMENTS,0.541747572815534,"upper limit of attack performance. We found that, in spite of FoV-based or BEV-based, they display
249"
EXPERIMENTS,0.5436893203883495,"similar robustness. Meanwhile, we see a huge improvement of robustness by utilizing a stronger
250"
EXPERIMENTS,0.545631067961165,"backbone (ResNet101 versus ResNet50) when comparing BEVFormer-Base with BEVFormer-Tiny.
251"
EXPERIMENTS,0.5475728155339806,"We hope these results will inspire researchers to develop 3D detectors with enhanced robustness.
252"
EXPERIMENTS,0.5495145631067961,"Visualization Results
We visualize our attack results with semantic-guided regularization in Fig. 3
253"
EXPERIMENTS,0.5514563106796116,"(a,b), and without regularization in Fig. 3 (c). The disappearance of detected objects is caused by their
254"
EXPERIMENTS,0.5533980582524272,"lower confidence scores. For example, the confidence predicted by detectors in Fig. 3 (a) have declined
255"
EXPERIMENTS,0.5553398058252427,"Semantic Part
NDS
mAP"
EXPERIMENTS,0.5572815533980583,"Clean
0.382
0.307
No Part
0.302
0.234
Full Parts
0.224
0.132
Part of Front
0.267
0.148
Part of Side
0.265
0.149
Part of Rear
0.268
0.151"
EXPERIMENTS,0.5592233009708738,"Table 3: Attack results of different se-
mantic parts."
EXPERIMENTS,0.5611650485436893,"Data
Adv train
NDS
mAP"
EXPERIMENTS,0.5631067961165048,"Clean val
‚úó
0.304
0.248
Clean val
‚úì
0.311
0.255"
EXPERIMENTS,0.5650485436893203,"Adv val ‚Ä†
‚úó
0.224
0.132
Adv val ‚Ä†
‚úì
0.264
0.181"
EXPERIMENTS,0.566990291262136,"Adv val ¬ß
‚úì
0.228
0.130"
EXPERIMENTS,0.5689320388349515,Table 4: Results of adversarial training.
EXPERIMENTS,0.570873786407767,"from 0.6 to 0.4, and are therefore filtered out by the visualization threshold of 0.6. In Fig. 3 (a), we
256"
EXPERIMENTS,0.5728155339805825,"find that our adversarial NeRF is realistic enough to be detected by a 3D detector if it doesn‚Äôt display
257"
EXPERIMENTS,0.574757281553398,"much of the adversarial texture. However, once the vehicle shows a larger area of the adversarial
258"
EXPERIMENTS,0.5766990291262136,"texture as seen in Fig. 3 (b), it will hide all objects including itself due to our untargeted objective.
259"
SEMANTIC PARTS ANALYSIS,0.5786407766990291,"5.1
Semantic Parts Analysis
260"
SEMANTIC PARTS ANALYSIS,0.5805825242718446,"In Tab. 3, we provide experiments on the impact of different semantic parts on attack performance.
261"
SEMANTIC PARTS ANALYSIS,0.5825242718446602,"Specifically, we focused on three salient parts of the vehicle: the front, side, and rear. Our results
262"
SEMANTIC PARTS ANALYSIS,0.5844660194174758,"show that compared with adversarial attacks using full parts, the semantic-guided regularization
263"
SEMANTIC PARTS ANALYSIS,0.5864077669902913,"leads to a slightly lower performance drop, but remains a realistic appearance and less likely spotted
264"
SEMANTIC PARTS ANALYSIS,0.5883495145631068,"adversarial texture as illustrated in Fig. 2 (b).
265"
SEMANTIC PARTS ANALYSIS,0.5902912621359223,"Since we do not have access to annotation during training, we additionally conduct ""No Part""
266"
SEMANTIC PARTS ANALYSIS,0.5922330097087378,"experiment that no part of the texture is adversarial, to evaluate the impact of the collision and
267"
SEMANTIC PARTS ANALYSIS,0.5941747572815534,"occlusion. We acknowledge that part of performance degradation can be attributed to the occlusion to
268"
SEMANTIC PARTS ANALYSIS,0.596116504854369,"original objects and the false positive prediction of adversarial objects (see Fig. 3 (a)), since we do
269"
SEMANTIC PARTS ANALYSIS,0.5980582524271845,"not update the ground truth of adversarial objects to the validation set.
270"
SEMANTIC PARTS ANALYSIS,0.6,"5.2
Effectiveness of 3D-aware attack
271"
SEMANTIC PARTS ANALYSIS,0.6019417475728155,"To validate the effectiveness of our 3D attacks, we ablate the impact of different poses on the attack
272"
SEMANTIC PARTS ANALYSIS,0.6038834951456311,"performance. In Fig. 4 (a), we divide the BEV plane into 10 √ó 10 bins ranging from x ‚àà[‚àí5m, 5m]
273"
SEMANTIC PARTS ANALYSIS,0.6058252427184466,"and z ‚àà[10m, 15m]. We then evaluate the relative mAP drop (percentage) of BEVDet [17] by
274"
SEMANTIC PARTS ANALYSIS,0.6077669902912621,"sampling one adversarial example inside the bin per image, while keeping rotation randomly sampled.
275"
SEMANTIC PARTS ANALYSIS,0.6097087378640776,"Similarly, we conduct experiments of 30 uniform rotation bins ranging from [0, 2œÄ] in Fig. 4 (b).
276"
SEMANTIC PARTS ANALYSIS,0.6116504854368932,"The experimental results demonstrate that all aspects of location and rotation achieve a valid attack
277"
SEMANTIC PARTS ANALYSIS,0.6135922330097088,"(performance drop > 30%), thereby proving the transferability of poses in our 3D-aware attack.
278"
SEMANTIC PARTS ANALYSIS,0.6155339805825243,"A finding that contrasts with prior work [44] is the impact of near and far locations in z axis. Our
279"
SEMANTIC PARTS ANALYSIS,0.6174757281553398,"adversarial example is more effective in the near region compared with the far region, while Tu
280"
SEMANTIC PARTS ANALYSIS,0.6194174757281553,"et al. [44] display a roughly uniform distribution in all regions. We hypothesize that the attack
281"
SEMANTIC PARTS ANALYSIS,0.6213592233009708,"performance is proportional to the area of the rendered patch, which is highly related to the location
282"
SEMANTIC PARTS ANALYSIS,0.6233009708737864,3 0 . 0
SEMANTIC PARTS ANALYSIS,0.625242718446602,3 2 . 5
SEMANTIC PARTS ANALYSIS,0.6271844660194175,3 5 . 0
SEMANTIC PARTS ANALYSIS,0.629126213592233,3 7 . 5
SEMANTIC PARTS ANALYSIS,0.6310679611650486,4 0 . 0
SEMANTIC PARTS ANALYSIS,0.6330097087378641,4 2 . 5
SEMANTIC PARTS ANALYSIS,0.6349514563106796,4 5 . 0
SEMANTIC PARTS ANALYSIS,0.6368932038834951,4 7 . 5 ùë•ùë• ùëßùëß ‚àí5ùëöùëö 10ùëöùëö 15ùëöùëö % 5ùëöùëö 0
SEMANTIC PARTS ANALYSIS,0.6388349514563106,"1
2 ùúãùúã ùë•ùë•"
SEMANTIC PARTS ANALYSIS,0.6407766990291263,"ùëßùëß
34.0 36.0 38.0 40.0 42.0 44.0 46.0 %"
SEMANTIC PARTS ANALYSIS,0.6427184466019418,"(a) Location
(b) Rotation ùúÉùúÉ"
SEMANTIC PARTS ANALYSIS,0.6446601941747573,Relative Performance Drop
SEMANTIC PARTS ANALYSIS,0.6466019417475728,"Figure 4: To examine the 3D-aware property of our adversarial examples, we ablate the relative
performance drop by sampling adversarial examples within different bins of location and rotation."
SEMANTIC PARTS ANALYSIS,0.6485436893203883,"XXXXXXXXX
Target
Source
Clean
FCOS3D
PGD-Det
DETR3D
BEVDet
BEVFormer"
SEMANTIC PARTS ANALYSIS,0.6504854368932039,"FCOS3D [48]
0.298
0.124
0.141
0.144
0.176
0.158
PGD-Det [49]
0.317
0.172
0.131
0.150
0.186
0.172
DETR3D [50]
0.347
0.188
0.170
0.133
0.212
0.198
BEVDet [17]
0.307
0.148
0.145
0.140
0.132
0.140
BEVFormer [27]
0.252
0.175
0.155
0.136
0.177
0.124"
SEMANTIC PARTS ANALYSIS,0.6524271844660194,"Table 5: Transferability of our attack to unseen detectors. We evaluate the robustness of target
detectors using an adversarial example trained on source detectors. Reported in mAP."
SEMANTIC PARTS ANALYSIS,0.654368932038835,"of objects. Similar findings are also displayed in rotation. The vehicle that poses vertically to the ego
283"
SEMANTIC PARTS ANALYSIS,0.6563106796116505,"vehicle results in a larger rendered area, thus better attack performance.
284"
TRANSFERABILITY ACROSS DIFFERENT DETECTORS,0.658252427184466,"5.3
Transferability Across Different Detectors
285"
TRANSFERABILITY ACROSS DIFFERENT DETECTORS,0.6601941747572816,"In Tab. 5, we evaluate the transferability of adversarial examples across different detectors. To
286"
TRANSFERABILITY ACROSS DIFFERENT DETECTORS,0.6621359223300971,"this end, we train a single adversarial example of each detector separately, then use the example to
287"
TRANSFERABILITY ACROSS DIFFERENT DETECTORS,0.6640776699029126,"evaluate the performance drop of other detectors. We show that there is a high degree of transferability
288"
TRANSFERABILITY ACROSS DIFFERENT DETECTORS,0.6660194174757281,"between different models. Among them, we observe that DETR3D [50] appears to be more resilient
289"
TRANSFERABILITY ACROSS DIFFERENT DETECTORS,0.6679611650485436,"to adversarial attacks than other detectors. We hypothesize this can be attributed to the sparsity of the
290"
TRANSFERABILITY ACROSS DIFFERENT DETECTORS,0.6699029126213593,"query-based method. During the projection of 3D query to the 2D image plane, only a single point of
291"
TRANSFERABILITY ACROSS DIFFERENT DETECTORS,0.6718446601941748,"the feature is indexed by interpolation, thus fewer areas of adversarial features will be sampled. This
292"
TRANSFERABILITY ACROSS DIFFERENT DETECTORS,0.6737864077669903,"finding may have insightful implications for the development of more robust 3D detectors in the future.
293"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.6757281553398058,"5.4
Adversarial Defense by Data Augmentation
294"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.6776699029126214,"We present the results of adversarial training in Tab. 4. The symbol ‚Ä† indicates attacks using the same
295"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.6796116504854369,"adversarial example used in adversarial training, while ¬ß indicates a different example. We observe
296"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.6815533980582524,"that incorporating adversarial training improves not only the robustness against the seen adversarial
297"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.683495145631068,"examples, but also the clean performance. However, we also note that our adversarial training is
298"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.6854368932038835,"not capable of transferring to unseen adversarial examples trained in the same way, mainly due to
299"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.6873786407766991,"the fixed adversarial example during adversarial training. Furthermore, we hope that future work
300"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.6893203883495146,"can conduct in-depth investigations and consider handling the bi-level loop of adversarial training in
301"
ADVERSARIAL DEFENSE BY DATA AUGMENTATION,0.6912621359223301,"order to better defend against adversarial attacks.
302"
LIMITATION AND FUTURE WORK,0.6932038834951456,"6
Limitation and Future Work
303"
LIMITATION AND FUTURE WORK,0.6951456310679611,"Learning to Sample and Attack
As we do not have access to the dataset annotations, we can not
304"
LIMITATION AND FUTURE WORK,0.6970873786407767,"model the explicit relationship between adversarial and normal objects to avoid collision, and the
305"
LIMITATION AND FUTURE WORK,0.6990291262135923,"collision itself can cause a performance drop (""No Parts"" in Tab. 3). Future work can apply geometry-
306"
LIMITATION AND FUTURE WORK,0.7009708737864078,"aware composition [10] to mitigate this problem. Additionally, future research can explore learning
307"
LIMITATION AND FUTURE WORK,0.7029126213592233,"to predict optimal poses of adversarial objects to maximize the effectiveness of attacks.
308"
LIMITATION AND FUTURE WORK,0.7048543689320388,"Potential Harmful Consequences
The trained adversarial examples have the potential to induce
309"
LIMITATION AND FUTURE WORK,0.7067961165048544,"serious traffic accidents in driving scenarios. However, our work is not intended to cause disruptions in
310"
LIMITATION AND FUTURE WORK,0.7087378640776699,"autonomous driving systems. Instead, our goal is to use the examples to gain a deeper understanding
311"
LIMITATION AND FUTURE WORK,0.7106796116504854,"of the systems and improve their robustness. We hope our work will draw more attention of the
312"
LIMITATION AND FUTURE WORK,0.7126213592233009,"community to further verify and enhance the robustness of autonomous driving systems.
313"
CONCLUSION,0.7145631067961165,"7
Conclusion
314"
CONCLUSION,0.7165048543689321,"In this paper, we propose Adv3D, the first attempt to model adversarial examples as NeRF. Adv3D
315"
CONCLUSION,0.7184466019417476,"enhances the physical realizability of attacks through our proposed primitive-aware sampling and
316"
CONCLUSION,0.7203883495145631,"semantic-guided regularization. Compared with prior works of adversarial examples in autonomous
317"
CONCLUSION,0.7223300970873786,"driving, our examples are more threatening in practice as we carry non-contact attacks, have feasible
318"
CONCLUSION,0.7242718446601941,"3D shapes as usual vehicles, and display camouflage adversarial texture. Extensive experimental
319"
CONCLUSION,0.7262135922330097,"results also demonstrate that Adv3d transfers well to different poses, scenes, and detectors. We hope
320"
CONCLUSION,0.7281553398058253,"our work provides valuable insights for creating more realistic evaluations to investigate and improve
321"
CONCLUSION,0.7300970873786408,"the robustness of autonomous driving systems.
322"
REFERENCES,0.7320388349514563,"References
323"
REFERENCES,0.7339805825242719,"[1] Mazen Abdelfattah, Kaiwen Yuan, Z Jane Wang, and Rabab Ward. Adversarial attacks on camera-lidar
324"
REFERENCES,0.7359223300970874,"models for 3d car detection. In IROS, 2021. 3
325"
REFERENCES,0.7378640776699029,"[2] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples.
326"
REFERENCES,0.7398058252427184,"2018. 1, 2, 4, 6
327"
REFERENCES,0.7417475728155339,"[3] Tom B Brown, Dandelion Man√©, Aurko Roy, Mart√≠n Abadi, and Justin Gilmer. Adversarial patch. arXiv
328"
REFERENCES,0.7436893203883496,"preprint arXiv:1712.09665, 2017. 2
329"
REFERENCES,0.7456310679611651,"[4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan,
330"
REFERENCES,0.7475728155339806,"Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving.
331"
REFERENCES,0.7495145631067961,"In CVPR, 2020. 2, 7
332"
REFERENCES,0.7514563106796116,"[5] Yulong Cao, Ningfei Wang, Chaowei Xiao, Dawei Yang, Jin Fang, Ruigang Yang, Qi Alfred Chen, Mingyan
333"
REFERENCES,0.7533980582524272,"Liu, and Bo Li. Invisible for both camera and lidar: Security of multi-sensor fusion based perception in
334"
REFERENCES,0.7553398058252427,"autonomous driving under physical-world attacks. In IEEE Symposium on Security and Privacy (SP), 2021.
335"
REFERENCES,0.7572815533980582,"1, 2, 3
336"
REFERENCES,0.7592233009708738,"[6] Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan Liu, and Bo Li. Adversarial
337"
REFERENCES,0.7611650485436893,"objects against lidar-based autonomous driving systems. arXiv preprint arXiv:1907.05418, 2019. 1, 2
338"
REFERENCES,0.7631067961165049,"[7] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio
339"
REFERENCES,0.7650485436893204,"Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient
340"
REFERENCES,0.7669902912621359,"geometry-aware 3D generative adversarial networks. In CVPR, 2022. 3, 4
341"
REFERENCES,0.7689320388349514,"[8] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Chau. Shapeshifter: Robust physical
342"
REFERENCES,0.7708737864077669,"adversarial attack on faster r-cnn object detector. In ECML PKDD, 2019. 2
343"
REFERENCES,0.7728155339805826,"[9] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler.
344"
REFERENCES,0.7747572815533981,"Learning to predict 3d objects with an interpolation-based differentiable renderer. NeurIPS, 32, 2019. 3
345"
REFERENCES,0.7766990291262136,"[10] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie
346"
REFERENCES,0.7786407766990291,"Xue, Ersin Yumer, and Raquel Urtasun. Geosim: Realistic video simulation via geometry-aware composi-
347"
REFERENCES,0.7805825242718447,"tion for self-driving. In CVPR, 2021. 9
348"
REFERENCES,0.7825242718446602,"[11] Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, Dongfang Liu, and Xiangyu
349"
REFERENCES,0.7844660194174757,"Zhang. Physical attack on monocular depth estimation with optimal adversarial patches. In ECCV, 2022. 2
350"
REFERENCES,0.7864077669902912,"[12] MMDetection3D Contributors. MMDetection3D: OpenMMLab next-generation platform for general 3D
351"
REFERENCES,0.7883495145631068,"object detection. https://github.com/open-mmlab/mmdetection3d, 2020. 7
352"
REFERENCES,0.7902912621359224,"[13] Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, and Jun Zhu. Benchmarking
353"
REFERENCES,0.7922330097087379,"adversarial robustness on image classification. In CVPR, 2020. 1
354"
REFERENCES,0.7941747572815534,"[14] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa.
355"
REFERENCES,0.7961165048543689,"Plenoxels: Radiance fields without neural networks. In CVPR, 2022. 3
356"
REFERENCES,0.7980582524271844,"[15] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
357"
REFERENCES,0.8,"ICLR, 2015. 1
358"
REFERENCES,0.8019417475728156,"[16] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d aware generator
359"
REFERENCES,0.8038834951456311,"for high-resolution image synthesis. In ICLR, 2022. 3
360"
REFERENCES,0.8058252427184466,"[17] Junjie Huang, Guan Huang, Zheng Zhu, Ye Yun, and Dalong Du. Bevdet: High-performance multi-camera
361"
REFERENCES,0.8077669902912621,"3d object detection in bird-eye-view. arXiv preprint arXiv:2112.11790, 2021. 6, 7, 8, 9
362"
REFERENCES,0.8097087378640777,"[18] Lifeng Huang, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan L. Yuille, Changqing Zou, and Ning Liu.
363"
REFERENCES,0.8116504854368932,"Universal physical camouflage attacks on object detectors. In CVPR, 2020. 2
364"
REFERENCES,0.8135922330097087,"[19] James T Kajiya and Brian P Von Herzen. Ray tracing volume densities. ACM SIGGRAPH computer
365"
REFERENCES,0.8155339805825242,"graphics, 18(3), 1984. 3
366"
REFERENCES,0.8174757281553398,"[20] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
367"
REFERENCES,0.8194174757281554,"improving the image quality of StyleGAN. In CVPR, 2020. 3
368"
REFERENCES,0.8213592233009709,"[21] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In CVPR, 2018. 3
369"
REFERENCES,0.8233009708737864,"[22] Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv preprint
370"
REFERENCES,0.8252427184466019,"arXiv:1412.6980, 2014. 7
371"
REFERENCES,0.8271844660194175,"[23] Stepan Komkov and Aleksandr Petiushko. Advhat: Real-world adversarial attack on arcface face id system.
372"
REFERENCES,0.829126213592233,"In ICPR. IEEE, 2021. 2
373"
REFERENCES,0.8310679611650486,"[24] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In
374"
REFERENCES,0.8330097087378641,"ICLR Workshop, 2017. 1
375"
REFERENCES,0.8349514563106796,"[25] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Enze Xie, Zhiqi Li,
376"
REFERENCES,0.8368932038834952,"Hanming Deng, Hao Tian, Xizhou Zhu, Li Chen, Yulu Gao, Xiangwei Geng, Jia Zeng, Yang Li, Jiazhi
377"
REFERENCES,0.8388349514563107,"Yang, Xiaosong Jia, Bohan Yu, Yu Qiao, Dahua Lin, Si Liu, Junchi Yan, Jianping Shi, and Ping Luo.
378"
REFERENCES,0.8407766990291262,"Delving into the devils of bird‚Äôs-eye-view perception: A review, evaluation and recipe. arXiv preprint
379"
REFERENCES,0.8427184466019417,"arXiv:2209.05324, 2022. 3
380"
REFERENCES,0.8446601941747572,"[26] Leheng Li, Qing Lian, Luozhou Wang, Ningning Ma, and Ying-Cong Chen. Lift3d: Synthesize 3d training
381"
REFERENCES,0.8466019417475729,"data by lifting 2d gan to 3d generative radiance field. In CVPR, 2023. 2, 3, 4, 5, 7
382"
REFERENCES,0.8485436893203884,"[27] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai.
383"
REFERENCES,0.8504854368932039,"Bevformer: Learning bird‚Äôs-eye-view representation from multi-camera images via spatiotemporal trans-
384"
REFERENCES,0.8524271844660194,"formers. ECCV, 2022. 6, 7, 9
385"
REFERENCES,0.8543689320388349,"[28] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. Editgan:
386"
REFERENCES,0.8563106796116505,"High-precision semantic image editing. In NeurIPS, 2021. 5
387"
REFERENCES,0.858252427184466,"[29] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for image-
388"
REFERENCES,0.8601941747572815,"based 3d reasoning. In ICCV, 2019. 3
389"
REFERENCES,0.8621359223300971,"[30] Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai Li, and Yiran Chen. Dpatch: An adversarial patch
390"
REFERENCES,0.8640776699029126,"attack on object detectors. arXiv preprint arXiv:1806.02299, 2018. 2
391"
REFERENCES,0.8660194174757282,"[31] Yuexin Ma, Tai Wang, Xuyang Bai, Huitong Yang, Yuenan Hou, Yaming Wang, Y. Qiao, Ruigang Yang,
392"
REFERENCES,0.8679611650485437,"Dinesh Manocha, and Xinge Zhu. Vision-centric bev perception: A survey. 2022. 3
393"
REFERENCES,0.8699029126213592,"[32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
394"
REFERENCES,0.8718446601941747,"deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 6
395"
REFERENCES,0.8737864077669902,"[33] Alexander Majercik, Cyril Crassin, Peter Shirley, and Morgan McGuire. A ray-box intersection algorithm
396"
REFERENCES,0.8757281553398059,"and efficient dynamic voxel rendering. Journal of Computer Graphics Techniques Vol, 7(3), 2018. 4
397"
REFERENCES,0.8776699029126214,"[34] Nelson Max. Optical models for direct volume rendering. IEEE TVCG, 1995. 3
398"
REFERENCES,0.8796116504854369,"[35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren
399"
REFERENCES,0.8815533980582524,"Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 2, 3
400"
REFERENCES,0.883495145631068,"[36] Thomas M√ºller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives
401"
REFERENCES,0.8854368932038835,"with a multiresolution hash encoding. ACM ToG, 2022. 3
402"
REFERENCES,0.887378640776699,"[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
403"
REFERENCES,0.8893203883495145,"Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
404"
REFERENCES,0.8912621359223301,"learning library. NeurIPS, 32, 2019. 7
405"
REFERENCES,0.8932038834951457,"[38] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly
406"
REFERENCES,0.8951456310679612,"unprojecting to 3d. In ECCV, 2020. 3
407"
REFERENCES,0.8970873786407767,"[39] Cody Reading, Ali Harakeh, Julia Chae, and Steven L. Waslander. Categorical depth distributionnetwork
408"
REFERENCES,0.8990291262135922,"for monocular 3d object detection. CVPR, 2021. 3
409"
REFERENCES,0.9009708737864077,"[40] Abhijith Sharma, Yijun Bian, Phil Munz, and Apurva Narayan. Adversarial patch attacks and defences in
410"
REFERENCES,0.9029126213592233,"vision-based tasks: A survey. arXiv preprint arXiv:2206.08304, 2022. 2, 5
411"
REFERENCES,0.9048543689320389,"[41] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
412"
REFERENCES,0.9067961165048544,"Rob Fergus. Intriguing properties of neural networks. ICLR, 2014. 1, 2
413"
REFERENCES,0.9087378640776699,"[42] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan,
414"
REFERENCES,0.9106796116504854,"Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In
415"
REFERENCES,0.912621359223301,"CVPR, 2022. 3
416"
REFERENCES,0.9145631067961165,"[43] James Tu, Huichen Li, Xinchen Yan, Mengye Ren, Yun Chen, Ming Liang, Eilyan Bitar, Ersin Yumer, and
417"
REFERENCES,0.916504854368932,"Raquel Urtasun. Exploring adversarial robustness of multi-sensor perception systems in self driving. arXiv
418"
REFERENCES,0.9184466019417475,"preprint arXiv:2101.06784, 2021. 2, 3, 5
419"
REFERENCES,0.920388349514563,"[44] James Tu, Mengye Ren, Sivabalan Manivasagam, Ming Liang, Bin Yang, Richard Du, Frank Cheng, and
420"
REFERENCES,0.9223300970873787,"Raquel Urtasun. Physically realizable adversarial examples for lidar object detection. In CVPR, 2020. 2, 5, 8
421"
REFERENCES,0.9242718446601942,"[45] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and Pratul P Srinivasan.
422"
REFERENCES,0.9262135922330097,"Ref-nerf: Structured view-dependent appearance for neural radiance fields. In CVPR, 2022. 3
423"
REFERENCES,0.9281553398058252,"[46] Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu Tang, and Xianglong Liu. Dual attention
424"
REFERENCES,0.9300970873786408,"suppression attack: Generate adversarial camouflage in physical world. In CVPR, 2021. 2
425"
REFERENCES,0.9320388349514563,"[47] Jingkang Wang, Ava Pun, James Tu, Sivabalan Manivasagam, Abbas Sadat, Sergio Casas, Mengye Ren,
426"
REFERENCES,0.9339805825242719,"and Raquel Urtasun. Advsim: Generating safety-critical scenarios for self-driving vehicles. CVPR, 2021. 2
427"
REFERENCES,0.9359223300970874,"[48] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin. FCOS3D: Fully convolutional one-stage monocular
428"
REFERENCES,0.9378640776699029,"3d object detection. In ICCV Workshop, 2021. 3, 6, 7, 9
429"
REFERENCES,0.9398058252427185,"[49] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin. Probabilistic and Geometric Depth: Detecting
430"
REFERENCES,0.941747572815534,"objects in perspective. In CoRL, 2021. 3, 6, 7, 9
431"
REFERENCES,0.9436893203883495,"[50] Yue Wang, Vitor Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, , and Justin M. Solomon. Detr3d:
432"
REFERENCES,0.945631067961165,"3d object detection from multi-view images via 3d-to-2d queries. In CoRL, 2021. 3, 6, 7, 9
433"
REFERENCES,0.9475728155339805,"[51] Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan Gojcic,
434"
REFERENCES,0.9495145631067962,"Wenzheng Chen, and Sanja Fidler. Neural fields meet explicit geometric representations for inverse
435"
REFERENCES,0.9514563106796117,"rendering of urban scenes. In CVPR, June 2023. 3
436"
REFERENCES,0.9533980582524272,"[52] Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, and Yu Wang. Physical adversarial
437"
REFERENCES,0.9553398058252427,"attack on vehicle detector in the carla simulator. arXiv preprint arXiv:2007.16118, 2020. 2
438"
REFERENCES,0.9572815533980582,"[53] Chong Xiang, Charles R Qi, and Bo Li. Generating 3d adversarial point clouds. In CVPR, 2019. 1
439"
REFERENCES,0.9592233009708738,"[54] Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu. Meshadv: Adversarial meshes for visual
440"
REFERENCES,0.9611650485436893,"recognition. In CVPR, 2019. 1
441"
REFERENCES,0.9631067961165048,"[55] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial
442"
REFERENCES,0.9650485436893204,"examples improve image recognition. In CVPR, 2020. 6
443"
REFERENCES,0.9669902912621359,"[56] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples
444"
REFERENCES,0.9689320388349515,"for semantic segmentation and object detection. In CVPR, 2017. 1, 2
445"
REFERENCES,0.970873786407767,"[57] Shaoyuan Xie, Zichao Li, Zeyu Wang, and Cihang Xie. On the adversarial robustness of camera-based 3d
446"
REFERENCES,0.9728155339805825,"object detection. arXiv preprint arXiv:2301.10766, 2023. 2, 7
447"
REFERENCES,0.974757281553398,"[58] Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi
448"
REFERENCES,0.9766990291262136,"Wang, and Xue Lin. Adversarial t-shirt! evading person detectors in a physical world. In ECCV, 2020. 2
449"
REFERENCES,0.9786407766990292,"[59] Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi-Keung Tang, and
450"
REFERENCES,0.9805825242718447,"Alan L Yuille. Adversarial attacks beyond the image space. In CVPR, 2019. 1
451"
REFERENCES,0.9825242718446602,"[60] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T
452"
REFERENCES,0.9844660194174757,"Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ToG,
453"
REFERENCES,0.9864077669902913,"2021. 3
454"
REFERENCES,0.9883495145631068,"[61] Yang Zhang, Hassan Foroosh, Philip David, and Boqing Gong. Camou: Learning physical vehicle
455"
REFERENCES,0.9902912621359223,"camouflages to adversarially attack detectors in the wild. In International Conference on Learning
456"
REFERENCES,0.9922330097087378,"Representations, 2019. 2
457"
REFERENCES,0.9941747572815534,"[62] Zijian Zhu, Yichi Zhang, Hai Chen, Yinpeng Dong, Shu Zhao, Wenbo Ding, Jiachen Zhong, and Shibao
458"
REFERENCES,0.996116504854369,"Zheng. Understanding the robustness of 3d object detection with bird‚Äôs-eye-view representations in
459"
REFERENCES,0.9980582524271845,"autonomous driving. arXiv preprint arXiv:2303.17297, 2023. 2
460"
