Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00411522633744856,"Stein variational inference is a technique for approximate Bayesian inference that
has recently gained popularity because it combines the scalability of variational
inference (VI) with the ﬂexibility of non-parametric inference methods. While there
has been considerable progress in developing algorithms for Stein VI, integration in
existing probabilistic programming languages (PPLs) with an easy-to-use interface
is currently lacking. EinSteinVI is a lightweight composable library that integrates
the latest Stein VI methods with the PPL NumPyro (Phan et al., 2019). EinSteinVI
also provides our novel algorithm ELBO-within-Stein to support the use of custom
inference programs (guides), in addition to implementations of a wide range of
kernels, non-linear scaling of the repulsion force (Wang & Liu, 2019b) and second-
order gradient updates using matrix-valued kernels (Wang et al., 2019b). We
illustrate EinSteinVI using toy examples and show results on par with or better than
existing state-of-the-art methods for real-world problems. These include Bayesian
neural networks for regression and a Stein-mixture deep Markov model, which
also shows EinSteinVI scales to large models with more than 125,000 parameters."
INTRODUCTION,0.00823045267489712,"1
INTRODUCTION"
INTRODUCTION,0.012345679012345678,"Interest in Bayesian deep learning has surged due to the need for quantifying the uncertainty of
predictions obtained from machine learning algorithms (Wilson & Izmailov, 2020; Wilson, 2020).
The idea behind Bayesian inference is to describe observed data x using a model with latent variables
z. The goal is to infer a posterior distribution p(z|x) over the latent variables given a model describing
the joint distribution p(z, x) = p(x|z)p(z). We obtain the posterior by following the rules of Bayesian
inference:
p(z|x) = Z−1p(x|z)p(z)
where Z =
R"
INTRODUCTION,0.01646090534979424,"z p(x|z)p(z)dz is the normalization constant. For most practical models, the nor-
malization constant lacks an analytic solution or requires an infeasible number of computations,
complicating the Bayesian inference problem."
INTRODUCTION,0.0205761316872428,"Variational Inference (VI) techniques (Blei et al., 2017; Hoffman et al., 2013; Ranganath et al., 2014)
aim to approximate the posterior distribution. VI poses a family of distributions over the latent
variables q(z) ∈Q and ﬁnds
arg min
q∈Q
D(q(z)|p(z|x)),"
INTRODUCTION,0.024691358024691357,"where D is a divergence1 between the variational distribution q, also called the guide, and the true
posterior distribution p(z|x). The Kullback-Leibler divergence is a typical choice. VI often provides
good approximations that capture uncertainty and scales to millions of data points by a suitable choice
of Q and inference method."
INTRODUCTION,0.02880658436213992,"Stein VI is a family of VI techniques for approximate Bayesian inference based on Stein’s method (see
Anastasiou et al. (2021) for an overview) that is gaining popularity since it combines the scalability of
traditional VI with the ﬂexibility of non-parametric particle-based methods. Stein variational gradient
descent (SVGD) (Liu & Wang, 2016) is a recent Stein VI technique which uses a set of particles
{zi}N
i=1 as the approximating distribution q(z). As a particle-based method, SVGD is well suited for
capturing correlations between latent variables. The technique preserves the scalability of traditional"
INTRODUCTION,0.03292181069958848,1An asymmetric distance that might satisfy the triangle inequality.
INTRODUCTION,0.037037037037037035,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0411522633744856,"VI approaches while offering the ﬂexibility and modeling scope of techniques such as Markov chain
Monte Carlo (MCMC). SVGD is good at capturing multi-modality (Liu & Wang, 2016; Wang & Liu,
2019a), and has useful theoretical interpretations such as a set of particles following a gradient ﬂow
(Liu, 2017) or in terms of the properties of kernels (Liu & Wang, 2018)."
INTRODUCTION,0.04526748971193416,"Many advanced inference methods based on SVGD have been recently developed; these include non-
linear Stein (Wang & Liu, 2019a), factorized graphical models (Zhuo et al., 2018; Wang et al., 2018a),
matrix-valued kernels (Wang et al., 2019a) and support for higher-order gradient-based optimization
(Detommaso et al., 2018). These techniques have extended the scope of SVGD, allowing more ﬂexible
and accurate approximations of the posterior distribution. While algorithmic power is growing, a
distinct lack of integration with a general probabilistic programming language (PPL) framework
remains. Such integration would solve one of the most prominent limitations of PPLs with traditional
VI: their lack of ﬂexibility in capturing rich correlations in the approximated posterior."
INTRODUCTION,0.04938271604938271,"The main problem with SVGD is that it suffers from the curse of dimensionality: the number of
particles required to adequately represent a posterior distribution is exponential in its dimensionality.
Nalisnick & Smyth (2017) suggest resolving this by using Stein mixtures and propose an inference
algorithm that uses differentiable non-centered parameterization (DNCP) (Kingma & Welling, 2014)
and importance weighted Monte Carlo gradients (Burda et al., 2015)."
INTRODUCTION,0.053497942386831275,"In this article we propose a novel algorithm for inference of Stein mixtures called ELBO-within-Stein.
Unlike prior work on Stein mixtures (Nalisnick & Smyth, 2017), ELBO-within-Stein i) is simple to
implement, ii) only has the learning rate as a tuneable hyper-parameters, iii) requires only one gradient
evaluation of the ELBO for each particle (10 is typical for Stein mixtures) and iv) supports the use
of a tailored guide. We validate our algorithm experimentally, showing signiﬁcant improvements in
accuracy. ELBO-within-Stein is the core algorithm of our Stein VI library called EinSteinVI library.
Our library further includes SVGD as a special case of ELBO-within-Stein and all the advanced
methods mentioned above. The only other PPL with Stein VI methods is PyMC3 (Salvatier et al.,
2016). However, PyMC3 does not include any of the advanced Stein VI methods mentioned above or
Stein mixture methods."
INTRODUCTION,0.05761316872427984,"The EinSteinVI library extends the NumPyro PPL (Bingham et al., 2019; Phan et al., 2019). NumPyro
is a universal probabilistic programming language (van de Meent et al., 2018) for Python, which
allows arbitrary code to be executed in both its model and guide. The computational backend of
NumPyro is Jax (Frostig et al., 2018), which gives access to the powerful program optimization and
parallelizability provided by the Jax compiler. As EinSteinVI works with arbitrary guides, NumPyro
is a well suited language for embedding EinSteinVI. This is because NumPyro i) is embedded in
Python, the de facto programming language for data science, ii) includes the necessary data-structures
for tracking random variables in both model and guide, iii) features SVI with an API that is highly
suitable for EinSteinVI, and iv) beneﬁts computationally from Jax. Our extensions include SVGD,
Stein-mixtures formulated as ELBO-within-Stein, and the advanced methods mentioned above."
INTRODUCTION,0.06172839506172839,"Concretely, our contributions are:"
INTRODUCTION,0.06584362139917696,"• ELBO-within-Stein (EinSteinVI). A novel algorithm for Stein mixtures that only requires a
single gradient evaluation of the ELBO per particle."
INTRODUCTION,0.06995884773662552,"• A general library extension to NumPyro, called EinSteinVI. EinSteinVI allows SVGD to
work with custom guide programs based on ELBO-within-Stein optimization. The library is
compositional with NumPyro features, including support for deep learning, loss functions
(ELBO, Rényi ELBO (Li & Turner, 2016), custom losses), and optimization methods, thus
making it possible for EinSteinVI to grow organically with NumPyro development."
INTRODUCTION,0.07407407407407407,"• Integration of recent developments in Stein variational inference which includes: non-
linear optimization (Wang & Liu, 2019a), a wealth of kernels (Liu & Wang, 2016; 2018;
Gorham & Mackey, 2017), matrix-valued kernels (Wang et al., 2019a) supporting higher-
order optimization, an (experimental) update based on Stein point MCMC (Chen et al.,
2019a), and factorization based on conditional independence between elements in the model
(graphical kernels) (Wang et al., 2018b)."
INTRODUCTION,0.07818930041152264,"• A series of examples that demonstrate EinSteinVI and the synergy between different Stein
VI techniques. The examples include a novel Stein-mixture version of the deep Markov"
INTRODUCTION,0.0823045267489712,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.08641975308641975,"model (SM-DMM), Bayesian neural networks for regression. An examples that explore
kernels and higher-order optimization is available in the appendix."
INTRODUCTION,0.09053497942386832,"The paper proceeds as follows. We ﬁrst present the background of SVGD in the context of our
integrated implementation in Section 2. In Section 3 we introduce Stein mixtures and our algorithm
ELBO-within-Stein. We discuss the general details of the implementation of EinSteinVI in Section 4.
Next, we discuss related work in Section 5. In Section 6 we present various examples using
EinSteinVI, and ﬁnally, we summarize our results and discuss future work in Section 7."
STEIN VARIATIONAL GRADIENT DESCENT,0.09465020576131687,"2
STEIN VARIATIONAL GRADIENT DESCENT"
STEIN VARIATIONAL GRADIENT DESCENT,0.09876543209876543,"The core idea of SVGD is to perform inference by approximating the target posterior distribution
p(z|x) by an empirical distribution qZ(z) = N −1 P"
STEIN VARIATIONAL GRADIENT DESCENT,0.102880658436214,"i δzi(z) based on a set of particles Z = {zi}N
i=1.
Here, δx(y) represents the Dirac delta measure, which is equal to 1 if x = y and 0 otherwise. One
could thus see the approximating distribution qZ(z) as a mixture of point estimates, each represented
by a particle zi ∈Z. The idea is to minimize the Kullback-Leibler divergence DKL(qZ(z) ∥p(z|x))
between the approximation and the true posterior by iteratively updating the particles using the Stein
forces:
zi ←zi + ϵSZ(zi)"
STEIN VARIATIONAL GRADIENT DESCENT,0.10699588477366255,where ϵ is the learning rate and SZ denotes the Stein forces.
STEIN VARIATIONAL GRADIENT DESCENT,0.1111111111111111,"The Two Forces of SVGD
Stein VI consists of two forces which work additively under the form
SZ = S+
Z + S−
Z , where the attractive force is given by"
STEIN VARIATIONAL GRADIENT DESCENT,0.11522633744855967,"S+
Z(zi) = Ezj∼qZ(z)[k(zi, zj)∇zj log p(zj|x)]"
STEIN VARIATIONAL GRADIENT DESCENT,0.11934156378600823,"and the repulsive force by
S−
Z (zi) = Ezj∼qZ(z)[∇zjk(zi, zj)]."
STEIN VARIATIONAL GRADIENT DESCENT,0.12345679012345678,"Here k : Rd × Rd →R is a kernel. The attractive force can be seen as pushing the particles towards
the modes of the true posterior distribution, smoothed by some kernel. For an example of a kernel,
consider the radial basis function (RBF) kernel k(zi, zj) = exp
 
−1"
STEIN VARIATIONAL GRADIENT DESCENT,0.12757201646090535,"h ∥zi −zj ∥2
2

with bandwidth
parameter h, chosen as
1
log nmed(z)."
STEIN VARIATIONAL GRADIENT DESCENT,0.13168724279835392,"The repulsive force moves particles away from each other, ensuring that they do not col-
lapse to the same mode.
For example, with the RBF kernel, the repulsive force becomes
Ezj∼qZ(z)[−exp
 
−1"
STEIN VARIATIONAL GRADIENT DESCENT,0.13580246913580246,"h ∥zi −zj ∥2
2
 2"
STEIN VARIATIONAL GRADIENT DESCENT,0.13991769547325103,"h
P
ℓ(ziℓ−zjℓ)], which has high kernel values for particles
that are close, causing them to repel."
STEIN VARIATIONAL GRADIENT DESCENT,0.1440329218106996,"SVGD works with unnormalized distributions p as the normalization constant becomes additive
in the log-posterior log p(zi|x) = −log Z + log p(x|z) + log p(z) and is thus not required for the
calculation of the gradient. This property is desirable as normalizing p is often computationally
expensive."
STEIN VARIATIONAL GRADIENT DESCENT,0.14814814814814814,"Non-linear Stein
In non-linear Stein (Wang & Liu, 2019a), the repulsive force can be scaled
by a factor λ, resulting in SZ = S+
Z + λS−
Z . This approach is useful when dealing with multi-
modal distributions. It is also useful in cases where the repulsive force vanishes compared to the
likelihood, which happens for large datasets (X). Scaling the repulsive force by a constant λ = c(|X|)
proportional (e.g. c = 0.1 or c = 0.01) to the size of the dataset |X| addresses this issue and can be
chosen by cross-validation on a subset of the data."
STEIN VARIATIONAL GRADIENT DESCENT,0.1522633744855967,"Matrix-valued kernels
The choice of kernels can be extended to matrix-valued ones (Wang et al.,
2019a), K : Rd × Rd →Rd×d, in which case the Stein forces become"
STEIN VARIATIONAL GRADIENT DESCENT,0.15637860082304528,"S+
Z(zi) = Ezj∼qZ(z)[K(zi, zj)∇zj log p(zj|x)]"
STEIN VARIATIONAL GRADIENT DESCENT,0.16049382716049382,"and
S−
Z (zi) = Ezj∼qZ(z)[K(zi, zj)∇zj]"
STEIN VARIATIONAL GRADIENT DESCENT,0.1646090534979424,Under review as a conference paper at ICLR 2022
STEIN VARIATIONAL GRADIENT DESCENT,0.16872427983539096,"where the standalone del ∇zj in the repulsive force represents the vector

∂
∂zj,1 , . . . ,
∂
∂zj,d"
STEIN VARIATIONAL GRADIENT DESCENT,0.1728395061728395,"
. This
results in
(K(zi, zj)∇zj)ℓ=
X"
STEIN VARIATIONAL GRADIENT DESCENT,0.17695473251028807,"k
∇kKℓ,k(zi, zj)."
STEIN VARIATIONAL GRADIENT DESCENT,0.18106995884773663,"The advantage of matrix-valued kernels is that they allow preconditioning2 using the Hessian or
Fisher Information matrix, which can capture local curvature and thus achieve better optima and
convergence rate than standard SVGD. Furthermore, it is easy to represent graphical kernels (Wang
et al., 2018a) using matrix kernels, e.g. K = diag({K(ℓ)}ℓ) where the set of variables are partitioned
with each their own local kernel K(ℓ)."
STEIN MIXTURES,0.18518518518518517,"3
STEIN MIXTURES"
STEIN MIXTURES,0.18930041152263374,"The Stein-mixture was proposed by Nalisnick & Smyth (2017) to resolve the representation issue
for SVGD when the target distribution has high dimensionality. For SVGD, the number of particles
needed to represent a distribution adequately grows exponentially with its dimensionality. As the
computational complexity for the update rule in SVGD is quadratic in the number of particles, the
exponential growth quickly becomes computationally intractable."
STEIN MIXTURES,0.1934156378600823,"Stein-mixtures are hierarchical variational models (Ranganath et al., 2016b) such that the posterior
of the second tier variational distribution is q(z|X) = 1"
STEIN MIXTURES,0.19753086419753085,"N
PN
k=1 δ(zk). The guide is the joint model
q(θ, z) = q(θ|z)q(z), with q(θ|z) the ﬁrst tier distribution and q(z) its prior. The second tier posterior
q(z|X) is optimized using SVGD, so that the marginal posterior is q(θ; z) =
1
N
PN
k=1 q(θ|z), a
restricted mixture."
STEIN MIXTURES,0.20164609053497942,"Nalisnick & Smyth (2017) showed that the Stein force for a mixture approximation of the posterior
p(z|Z) is given by"
STEIN MIXTURES,0.205761316872428,"SZ(zi) = Ezj∼qZ(z)

k(zi, zj)Eθl∼q(θ|zj) [p(X, θl)/q(θl|zj)]

+"
STEIN MIXTURES,0.20987654320987653,"+ Ezj∼qZ(z)

k(zi, zj)∇zj log q(zj)

+"
STEIN MIXTURES,0.2139917695473251,"+ S−
Z (zi),"
STEIN MIXTURES,0.21810699588477367,"where log ∇zjq(zj) acts as a regularizer on the variational parameters which they assume is sufﬁ-
ciently small to be dropped. To evaluate Ezj∼qZ(z) [p(X, θ)/q(θ|zj)], Nalisnick & Smyth (2017) use
DNCP and importance weighted Monte Carlo gradients to obtain the black-box update,"
STEIN MIXTURES,0.2222222222222222,"SZ(zi) = Ezj∼qZ(z) """
STEIN MIXTURES,0.22633744855967078,"k(zi, zj) S
X"
STEIN MIXTURES,0.23045267489711935,"s=1
˜ws∇zj log"
STEIN MIXTURES,0.2345679012345679,"p(X, ˆθs)"
STEIN MIXTURES,0.23868312757201646,q(ˆθs|zj) !#
STEIN MIXTURES,0.24279835390946503,"+ S−
Z (zi),
(1)"
STEIN MIXTURES,0.24691358024691357,"where ˜ws is an importance weight for sample ˆθs = q(zj, ξ), ξ ∼p0 (i.e. the guide)."
STEIN MIXTURES,0.25102880658436216,"ELBO-within-Stein
In ELBO-within-Stein, we replace the weighted average over S samples in
Equation (1) by a single loss (L). If we choose L to be an f-divergence, such as the ELBO, we can
reduce the loss bias by averaging multiple samples. However, ELBO-within-Stein only computes
one gradient of L per particle regardless of the number of samples we use to estimate L. This is
different from Nalisnick & Smyth (2017) who estimate Eθl∼q(θ|zj) [p(X, θl)/q(θl|zj)] by averaging
over gradients, therefore computes a gradient and an importance weighting for each DNCP sample
rather than per particle. The difference makes ELBO-within-Stein computationally cheaper as the
complexity of adding a gradient is O(n) whereas adding a sample to estimate L is O(1). The Stein
force in ELBO-within-Stein is given by"
STEIN MIXTURES,0.2551440329218107,"SZ(zi) = Ezj∼qZ(z)

k(zi, zj)∇zjL(zj)

+ S−
Z (zi)
(2)"
STEIN MIXTURES,0.25925925925925924,where L is the ELBO.
STEIN MIXTURES,0.26337448559670784,"2Using preconditioner matrix Q, such that Q−1M has a lower condition number than M."
STEIN MIXTURES,0.2674897119341564,Under review as a conference paper at ICLR 2022
COMPOSITIONAL IMPLEMENTATION USING NUMPYRO,0.2716049382716049,"4
COMPOSITIONAL IMPLEMENTATION USING NUMPYRO"
COMPOSITIONAL IMPLEMENTATION USING NUMPYRO,0.2757201646090535,"EinSteinVI integrates with the existing NumPyro API by adding the Stein VI interface, which closely
mimicks NumPyro’s SVI interface. Mimicking the SVI interface makes programs that use SVI in
NumPyro trivial to convert to EinSteinVI (see Figure 1)."
COMPOSITIONAL IMPLEMENTATION USING NUMPYRO,0.27983539094650206,"Below, we discuss the key features of EinSteinVI, which include re-initializable guides, EinSteinVI’s
core algorithm, and the new kernel interface."
RE-INITIALIZABLE GUIDES,0.2839506172839506,"4.1
RE-INITIALIZABLE GUIDES"
RE-INITIALIZABLE GUIDES,0.2880658436213992,"The Stein VI interface requires that the initialization is different for each parameter in an inference
program. The reason is that different Stein particles need to be initialized to different values in order
for optimization to work correctly and to avoid that all particles collapse into the posterior mode."
RE-INITIALIZABLE GUIDES,0.29218106995884774,"To support re-initializable guides, we provide the ReinitGuide interface, which requires implement-
ing a function find_params that accepts a list of random number generator (RNG) keys in addition
to the arguments for the guide and returns a set of freshly initialized parameters for each RNG key."
RE-INITIALIZABLE GUIDES,0.2962962962962963,"The WrappedGuide class provides a guide written as a function. WrappedGuide makes a callable
guide re-initializable. It works by running the provided guide multiple times and reinitializing the
parameters using NumPyro’s interface as follows:"
RE-INITIALIZABLE GUIDES,0.3004115226337449,• WrappedGuide runs the guide transforming each parameter to unconstrained space.
RE-INITIALIZABLE GUIDES,0.3045267489711934,"• It replaces the values of the parameters with values provided by a NumPyro initializa-
tion strategy, e.g., init_to_uniform(r), which initializes each parameter with a uniform
random value in the range [−r; r]."
RE-INITIALIZABLE GUIDES,0.30864197530864196,"• It saves the parameter values for each particle and the required inverse transformations to
constrained space to run the model correctly."
RE-INITIALIZABLE GUIDES,0.31275720164609055,"We also allow parameters without reinitialization in order to support neural network libraries like
stax 3 that have their own initializers."
RE-INITIALIZABLE GUIDES,0.3168724279835391,"The Stein VI interface will correctly wrap the guide during initialization, so from a user perspective
the syntax for guides follows the API of SVI."
STEIN VI IN NUMPYRO,0.32098765432098764,"4.2
STEIN VI IN NUMPYRO"
STEIN VI IN NUMPYRO,0.32510288065843623,"The integration of Stein VI into NumPyro requires handling transformations between the parameter
representation of NumPyro4 and the vectorized Stein particles that EinSteinVI operates on. For this,
we rely on Jax PyTrees5 which converts back and forth between Python collections and a ﬂattened
vectorized representation."
STEIN VI IN NUMPYRO,0.3292181069958848,"Algorithm 1 shows the core algorithm of EinSteinVI. EinSteinVI updates the standard variational
model parameters φ and guide parameters ψ by averaging the loss over the Stein particles. For the
Stein parameters, the process is more elaborate. First, we convert the set of individual parameters
to a monolithic vector-encoded particle using Jax PyTrees. The monolithic particle represents the
particles as a ﬂattened and stacked Jax array. Then we compute a kernel based on the vector-encoded
Stein particle; this is delegated to the kernel interface as the computation is kernel-dependent."
STEIN VI IN NUMPYRO,0.3333333333333333,"We apply Jax’s vmap operator (Frostig et al., 2018; Phan et al., 2019) to compute the Stein forces for
each particle in a vectorized manner. This is done in unconstrained space so the Stein force must
the corrected by the Jacobian of the bijection between constrained and unconstrained space. Doing
this directly on the Jax on the monolithic particle incurs a massive memory overhead in the adjoint.
However, as NumPyro registers a bijection for each distribution parameter we can eliminate the
overhead by computing the Jacobian on the Jax representations of the individual parameters. The
operation is embarrassingly parallel and so we again use a vmap operator with a nested tree_map to"
STEIN VI IN NUMPYRO,0.3374485596707819,"3https://jax.readthedocs.io/en/latest/jax.experimental.stax.html
4A dictionary mapping parameters to their values, which can be arbitrary Python type
5https://jax.readthedocs.io/en/latest/pytrees.html"
STEIN VI IN NUMPYRO,0.34156378600823045,Under review as a conference paper at ICLR 2022
STEIN VI IN NUMPYRO,0.345679012345679,"compute the desired Jacobians. Note that Algorithm 1 presents how EinSteinVI works with scalar
kernels and does not account for the different features presented in Section 2."
STEIN VI IN NUMPYRO,0.3497942386831276,"Finally, we convert the monolithic Stein particle to their non-vectorized dictionary-based form and
return the expected changes for standard- and Stein-parameters."
STEIN VI IN NUMPYRO,0.35390946502057613,"Input: Classical VI parameters φ and ψ, Stein parameters {θi}i, model pφ(z, x), guide qθ,ψ(z),
loss L, kernel interface KI.
Output: Parameter changes based on classical VI (∆φ, ∆ψ) and Stein VI forces ({∆θi}i)."
STEIN VI IN NUMPYRO,0.35802469135802467,"procedure EINSTEIN(φ, ψ, {θi}i, pφ, qθ,ψ)"
STEIN VI IN NUMPYRO,0.36213991769547327,"∆φ ←Eθ[∇φL(pφ, qθ,ψ)]
∆ψ ←Eθ[∇ψL(pφ, qθ,ψ)]
{ai}i ←PyTreeFlatten({θi}i)
k ←KI({ai}i)"
STEIN VI IN NUMPYRO,0.3662551440329218,"procedure EINSTEINFORCES(ai)
▷Calculate forces per particle for higher-order
vmap function.
θi ←PYTREERESTORE(ai)
∆ai ←P"
STEIN VI IN NUMPYRO,0.37037037037037035,"aj k(aj, ai)∇aiL(pφ, qθi,ψ) + ∇aik(aj, ai)
return ∆ai
end procedure"
STEIN VI IN NUMPYRO,0.37448559670781895,"{∆ai}i ←VMap({ai}i, EINSTEINFORCES)
{∆θi}i ←PYTREERESTORE({∆ai}i)
return ∆φ, ∆ψ, {∆θi}i
end procedure"
STEIN VI IN NUMPYRO,0.3786008230452675,Algorithm 1: EinSteinVI
KERNEL INTERFACE,0.38271604938271603,"4.3
KERNEL INTERFACE"
KERNEL INTERFACE,0.3868312757201646,"The Kernel interface is straightforward. To extend the interface users must implement the compute
function, which accepts as input the current set of particles, the mapping between model parameters
and particles, and the loss function L and returns a differentiable kernel k. All kernels are currently
static, but the interface could be extended to stateful kernels, allowing conjugate gradients or quasi-
Newton optimization. Appendix A gives the complete list of kernels in EinSteinVI. We are planning
to extend EinSteinVI with probability product kernels Nalisnick & Smyth (2017), which take into
account the information geometry of the ﬁrst tier variational distributions in Stein mixtures."
RELATED WORK,0.39094650205761317,"5
RELATED WORK"
RELATED WORK,0.3950617283950617,"There has been a proliferation of deep probabilistic programming languages (Ge et al., 2018; Bingham
et al., 2019; Salvatier et al., 2016; Tran et al., 2016; Cusumano-Towner et al., 2019; Dillon et al., 2017)
based on tensor frameworks featuring automatic differentiation and supporting various inference
techniques. However, only PyMC3 (Salvatier et al., 2016) includes inference using SteinVI."
RELATED WORK,0.3991769547325103,"In PyMC3 the inference techniques that use Stein’s method include SVGD with a scalar RBF-kernel,
amortized SVGD (Wang et al., 2016; Feng et al., 2017), and Operator Variational Inference (OVI)
(Ranganath et al., 2016a)."
RELATED WORK,0.40329218106995884,"SVGD in PyMC3 manipulates particles to directly capture the target distribution (Liu & Wang, 2016).
In EinSteinVI, SVGD is a special case of Stein mixtures where the guides are point mass distributions.
Because we have guide programs, EinSteinVI allows arbitrary computations to transform random
variables in the guide. This is not possible with SVGD in PyMC3."
RELATED WORK,0.4074074074074074,"Amortized SVGD (Feng et al., 2017) trains a stochastic network to draw samples from a target
distribution. The network is iteratively adjusted so that the output changes in the direction of the
Stein variational gradient (the same gradient as used in SVGD). In comparison, EinSteinVI transports
a ﬁxed set of particles (each parameterizing a guide) to the target distribution. Amortized SVGD is
not in EinSteinVI as its extension to arbitrary guides is an open problem."
RELATED WORK,0.411522633744856,Under review as a conference paper at ICLR 2022
RELATED WORK,0.4156378600823045,"def model():
sample('x', NormalMixture(jnp.array([1 / 3, 2 / 3]),
jnp.array([-2.0, 2.0]),
jnp.array([1.0, 1.0])))"
RELATED WORK,0.41975308641975306,"(a) 1D Gaussian mixture model
svi = SVI(
model,
AutoNormal(model),
Adagrad(step_size=1.0),
Trace_ELBO()
)"
RELATED WORK,0.42386831275720166,"results = svi.run(rng_key,
num_iterations)"
RELATED WORK,0.4279835390946502,(b) SVI
RELATED WORK,0.43209876543209874,"stein = SteinVI(
model,
AutDelta(model),
Adagrad(step_size=1.0),
Trace_ELBO(),
RBFKernel(),
)
results = stein.run(rng_key,
num_iterations)"
RELATED WORK,0.43621399176954734,(c) SVGD with EinSteinVI
RELATED WORK,0.4403292181069959,Figure 1: 1D Gaussian mixture model in NumPyro.
RELATED WORK,0.4444444444444444,"OVI optimizes operator objectives, which take functions of functions to a non-negative number.
Ranganath et al. (2016a) include an operator objective based on the Langevin-Stein operator (Anas-
tasiou et al., 2021). This is the same operator used for the kernelized Stein discrepancy (Liu et al.,
2016) which also underlies SVGD. Unlike EinSteinVI, Amortized SVDG and SVGD, OPV is not a
particle-based method."
EXAMPLES,0.448559670781893,"6
EXAMPLES"
EXAMPLES,0.45267489711934156,"We illustrate the features of EinSteinVI on two toy examples, namely a 1D mixture of Gaussians
below, and 2D mixtures of Gaussian in Appendix B. We demonstrate EinSteinVI on real-world
examples and show that EinSteinVI tends to outperform alternative methods. These examples include
regression with Bayesian neural networks and deep Markov models. All experimental code, notebook
tutorials and the EinSteinVI package itself are available through an Anonymized URL."
D GAUSSIAN MIXTURE,0.4567901234567901,"1D Gaussian mixture
To demonstrate the two modes of VI with EinSteinVI, we consider the 1D
Gaussian mixture 1/3N(−2, 1) + 2/3N(2, 1) (see Figure 1 and Figure 2). The Gaussian target
mixture is bi-modal and well-suited for the nonparametric nature of SVGD and Stein mixtures.
Figure 2 shows that both SVGD and the Stein-mixture naturally capture the bi-modality of the target
distribution, compared to SVI with a simple Gaussian guide. Note the reduction in particles required
to estimate the target when using Stein mixtures compared to SVGD. Also, note that the Stein-mixture
overestimates the variance and slightly pertubates the locations. The error seen at the right mode
for the Stein-mixture with two particles is due to the uniform weighting of the particles in SVGD
(the target posterior is approximated with an empirical distribution, see Section 2), and as such is
algorithmic. The Stein-mixture will therefore not be able to exactly capture the mixing components
of a target mixture model with one particle per component. However, with more particles the mixture
can be approximated better as demonstrated using three particles."
D GAUSSIAN MIXTURE,0.4609053497942387,"Bayesian Neural Networks
We compare SVGD and non-linear Stein in EinSteinVI with the
implementation of SVGD by Liu & Wang (2016)6 (without model selection), and amortized SVGD
(using the Theano backend7, Al-Rfou et al. (2016)) on Bayesian neural networks (BNN) for regression.
The PyMC3 documentation clearly states amortized SVGD is experimental and is not suggested to
be used. We include its experimental results in Appendix B for completeness and note that our
ﬁndings reﬂect the warning in the PyMC3 documentation. We use an RBF-kernel in EinSteinVI for
fair comparison with Liu & Wang (2016) and PyMC3. For non-linear Stein we determined the best
repulsion factor λ∗by a grid search for λ ∈{10−2, 10−1, 101, 102}. For SVGD PyMC3 we use a"
D GAUSSIAN MIXTURE,0.46502057613168724,"6https://github.com/dilinwang820/Stein-Variational-Gradient-Descent/blob/master/
python/bayesian_nn.pyy
7PyMC4, which uses Jax as its backend, does not include SVGD at the time of writing."
D GAUSSIAN MIXTURE,0.4691358024691358,Under review as a conference paper at ICLR 2022
D GAUSSIAN MIXTURE,0.4732510288065844,"(a) SVI
(b) SVGD (RBF kernel)
(c) Two particle Stein-
mixture (linear kernel)"
D GAUSSIAN MIXTURE,0.4773662551440329,"(d) Three particle Stein-
mixture (RBF kernel)"
D GAUSSIAN MIXTURE,0.48148148148148145,"Figure 2: The blue dashed line is the target pdf, while the solid green line is the density of the
particles. We estimate the particle density for SVGD with Gaussian kernel density estimation. We use
100 particles for SVGD, and two or three particles for the Stein-mixture. SVI uses a Gaussian guide."
D GAUSSIAN MIXTURE,0.48559670781893005,Table 1: Average test RMSE and time for inference for the UCI regression benchmarks.
D GAUSSIAN MIXTURE,0.4897119341563786,"Test RMSE
Time
EinStein
PyMC3
Liu & Wang (2016)
EinStein
PyMC3
Dataset
SVGD
NL-Stein
SVGD
SVGD
SVGD
SVGD
Boston
2.610 ± 0.044
2.492 ± 0.028
6.059 ± 0.244
2.990 ± 0.013
5.758s ± 0.048s
1m6.396s ± 0.301s
Concrete
4.175 ± 0.025
4.782 ± 0.047
6.255 ± 0.390
5.927 ± 0.021
5.602s ± 0.048s
1m5.845s ± 0.278s
Energy
0.321 ± 0.004
0.375 ± 0.007
3.494 ± 1.500
1.251 ± 0.017
5.562s ± 0.053s
1m6.175s ± 0.41s
Kin8nm
0.073 ± 0.000
0.076 ± 0.000
0.164 ± 0.040
0.115 ± 0.001
6.141s ± 0.04s
2m40.106 ± 0.298s
Naval
0.001 ± 0.000
0.001 ± 0.000
0.029 ± 0.015
0.008 ± 0.001
6.51s ± 0.048s
3m51.05s ± 0.385s
Power
3.952 ± 0.004
3.907 ± 0.006
5.389 ± 2.219
4.215 ± 0.002
6.314s ± 0.054s
2m51.654s ± 0.566s
Protein
4.509 ± 0.006
4.654 ± 0.006
5.921 ± 2.559
4.846 ± 0.002
9.436s ± 0.032s
11m11.9s ± 0.399s
Wine
0.599 ± 0.006
0.596 ± 0.005
0.769 ± 0.212
0.604 ± 0.001
5.614s ± 0.052s
1m7.767s ± 0.286s
Yacht
0.531 ± 0.009
0.246 ± 0.013
8.341 ± 3.744
0.94 ± 0.03
5.519s ± 0.07s
1m4.748s ± 0.392s
Year
8.662 ± 0.002
8.701 ± 0.002
135.316
8.895 ± 0.001
1m37.569s ± 0.07s
4h19m36s"
D GAUSSIAN MIXTURE,0.49382716049382713,"temperature8 of ten. Like Liu & Wang (2016) we use a BNN with one hidden layer of size ﬁfty and
RELU activation. We put a Gamma(1, 0.1) prior on the precision of the neurons and the likelihood.
For all versions, we use 100 particles and 2000 iterations. We use a mini-batch of 1000 for Year and
100 for the rest. All measurements are repeated ten times and obtained on a GPU9 for EinSteinVI and
PyMC3, except for Year with PyMC3 which was only run once. We do not report times for Liu & Wang
(2016) because only the CPU version of their code could be executed without irresolvable issues."
D GAUSSIAN MIXTURE,0.49794238683127573,"Table 1 shows the performance in terms of the average root mean squared error (RMSE) on the test
set. We ﬁnd that EinSteinVI achieves signiﬁcantly better RMSE than Liu & Wang (2016) and that
both systems outperform SVGD in PyMC3. The times in Table 1 measure how long it takes to infer
parameters. Table 1 excludes the ﬁrst run for two reasons: i) that run will ﬁll the GPU caches, and ii)
Jax will trace the programs. As a result, this run is more costly than subsequent ones. The times for
the ﬁrst run are given in Table 2. By running EinSteinVI for more iterations, we can amortize the
initial cost."
D GAUSSIAN MIXTURE,0.5020576131687243,"Stein Mixture Deep Markov Model
Music generation requires a model to learn complex temporal
dependencies to achieve local consistency between notes. The Stein-mixture deep Markov model
(SM-DMM) is a deep Markov model that uses a mixture of Stein particles to estimate distributions
over model parameters. We consider a vectorized version of the DMM (Jankowiak & Karaletsos,
2019) for the generation of polyphonic music using the JSB chorales data set."
D GAUSSIAN MIXTURE,0.5061728395061729,"The SM-DMM model consists of two feed-forward neural (FNN) networks. The Transition network
takes care of the conditional dependencies between subsequent latent states in the Markov chain. It
consist of two layers with hidden dimension 200 and ReLU activation on the ﬁrst layer and sigmoid"
D GAUSSIAN MIXTURE,0.5102880658436214,"8We found the performance to be very sensitive to the choice of temperature.
9Quadro RTX 6000 with Cuda V11.4.120"
D GAUSSIAN MIXTURE,0.51440329218107,Table 2: Time for ﬁrst repetition with EinStein for UCI regression benchmarks.
D GAUSSIAN MIXTURE,0.5185185185185185,"Dataset
Boston
Concrete
Energy
Kin8nm
Naval
Power
Protein
Wine
Yacht
Year
Time
41.665s
41.642s
41.591s
43.592s
44.2570s
44.058s
47.87s
41.9s
41.409s
2m18.19s"
D GAUSSIAN MIXTURE,0.522633744855967,Under review as a conference paper at ICLR 2022
D GAUSSIAN MIXTURE,0.5267489711934157,"Table 3: Test negative log- likelihood (lower is better) on Polyphonic Music Generation (JSB) dataset.
Baseline results from Krishnan et al. (2016). ISN-DMM and ISN-DMM-Aug (Krishnan et al., 2016),
TSBN and HMSBN (Gan et al., 2015)"
D GAUSSIAN MIXTURE,0.5308641975308642,"ISN-DMM
ISN-DMM-Aug
HMSBN
TSBN
SM-DMM
NLL (a)
6.926
6.773
8.0473
-
-45.983
NLL (b)
6.856
6.692
7.9970
7.48
-46.066"
D GAUSSIAN MIXTURE,0.5349794238683128,"on the second. The Emitter network is a three layers FNN which produces a likelihood at each time
step using the current latent state. The layers have hidden dimensions 100, 100, and 88, respectively.
The variational distribution is of the form QN
n=1 q(zn
1:Tn|f(X1:Tn)) where the parametrized feature
function f1:Tn is a one layered gated recurrent unit (Chung et al., 2014), with hidden dimension 150.
The total number of parameters in the DMM is 128,654, so that with ﬁve particles EinSteinVI is
optimizing 643,270 parameters for the SM-DMM model."
D GAUSSIAN MIXTURE,0.5390946502057613,"We train the SM-DMM using the Adam optimizer (Kingma & Ba, 2014) with a learning rate of
10−5, using an RBF kernel and ﬁve Stein particles for four thousand epochs on the polyphonic music
generation dataset JSB chorals (Boulanger-Lewandowski et al., 2012). We follow Krishnan et al.
(2016) and report two version NLL of a)"
D GAUSSIAN MIXTURE,0.5432098765432098,"PN
i=1 −p(xi|θ)
PN
i=1 Ti
and b)
1
N
PN
i=1
−p(xi|θ)"
D GAUSSIAN MIXTURE,0.5473251028806584,"Ti
, where Ti is the
length of the ith sequence. In Table 3 we report NLL(a) and NLL(b) on a held-out test set of JSB.
Compared to baseline methods SM-DMM achieves a signiﬁcant improvement using Stein mixtures.
We see similar improvements in the test ELBO, Jankowiak & Karaletsos (2019) reports a test ELBO
of -6.82 nats on the JSB dataset for their approach, SM-DMM with EinSteinVI achieves an test
ELBO of 45.10 (higher is better)."
SUMMARY,0.551440329218107,"7
SUMMARY"
SUMMARY,0.5555555555555556,"EinSteinVI provides the latest techniques for Stein VI as an extension to Numpyro. Our results
indicate that the library is substantially faster and more expressive than other available libraries
for Stein VI. EinSteinVI provides a familiar and efﬁcient interface for practitioners working with
the Pyro/NumPyro PPL and provides a uniﬁed code base to researchers for benchmarking new
developments in Stein VI."
SUMMARY,0.5596707818930041,"Possible further extensions include (a) supporting updates inspired by Stein Points (Chen et al., 2018;
2019b), (b) extending the kernel interface with probability product kernels (Jebara et al., 2004) to
account for information geometry when using Stein-mixtures, (c) adding full support for NumPyro’s
automatic enumeration features (Obermeyer et al., 2019) when guides are used, and (d) stateful matrix
kernels for conjugate gradient or quasi-Newton optimization. Stein Points updates could handle cases
where initialization of particles is sub-optimal by initializing from a Stein point MCMC chain and
interleave MCMC updates to get better mode hopping properties."
SUMMARY,0.5637860082304527,ACKNOWLEDGEMENTS
REFERENCES,0.5679012345679012,REFERENCES
REFERENCES,0.5720164609053497,"Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,
Nicolas Ballas, Frédéric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, et al.
Theano: A python framework for fast computation of mathematical expressions. arXiv e-prints,
pp. arXiv–1605, 2016. URL https://arxiv.org/abs/1605.02688."
REFERENCES,0.5761316872427984,"Andreas Anastasiou, Alessandro Barp, François-Xavier Briol, Bruno Ebner, Robert E. Gaunt, Fatemeh
Ghaderinezhad, Jackson Gorham, Arthur Gretton, Christophe Ley, Qiang Liu, Lester Mackey,
Chris. J. Oates, Gesine Reinert, and Yvik Swan. Stein’s method meets statistics: A review of some
recent developments. arXiv preprint arXiv:2105.03481, 2021. URL https://arxiv.org/abs/
2105.03481."
REFERENCES,0.5802469135802469,"Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis
Karaletsos, Rohit Singh, Paul A. Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep"
REFERENCES,0.5843621399176955,Under review as a conference paper at ICLR 2022
REFERENCES,0.588477366255144,"universal probabilistic programming. J. Mach. Learn. Res., 20:28:1–28:6, 2019. URL http:
//jmlr.org/papers/v20/18-403.html."
REFERENCES,0.5925925925925926,"David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association, 112(518):859–877, 2017. doi: 10.1080/01621459.
2017.1285773. URL https://doi.org/10.1080/01621459.2017.1285773."
REFERENCES,0.5967078189300411,"Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependen-
cies in high-dimensional sequences: Application to polyphonic music generation and transcription.
arXiv preprint arXiv:1206.6392, 2012."
REFERENCES,0.6008230452674898,"Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015. URL https://arxiv.org/abs/1509.00519."
REFERENCES,0.6049382716049383,"Wilson Ye Chen, Lester W. Mackey, Jackson Gorham, François-Xavier Briol, and Chris J. Oates.
Stein points. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,
2018, volume 80 of Proceedings of Machine Learning Research, pp. 843–852. PMLR, 2018. URL
http://proceedings.mlr.press/v80/chen18f.html."
REFERENCES,0.6090534979423868,"Wilson Ye Chen, Alessandro Barp, François-Xavier Briol, Jackson Gorham, Mark Girolami, Lester
Mackey, and Chris Oates. Stein point markov chain monte carlo. In International Conference
on Machine Learning, pp. 1011–1021. PMLR, 2019a. URL https://arxiv.org/abs/1905.
03673."
REFERENCES,0.6131687242798354,"Wilson Ye Chen, Alessandro Barp, François-Xavier Briol, Jackson Gorham, Mark A. Girolami,
Lester W. Mackey, and Chris J. Oates. Stein Point Markov Chain Monte Carlo. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97
of Proceedings of Machine Learning Research, pp. 1011–1021. PMLR, 2019b. URL http:
//proceedings.mlr.press/v97/chen19b.html."
REFERENCES,0.6172839506172839,"Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
URL https://arxiv.org/abs/1412.3555."
REFERENCES,0.6213991769547325,"Marco F. Cusumano-Towner, Feras A. Saad, Alexander K. Lew, and Vikash K. Mansinghka. Gen: A
general-purpose probabilistic programming system with programmable inference. In Proceedings
of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation,
PLDI 2019, pp. 221–236, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-6712-7. doi:
10.1145/3314221.3314642. URL http://doi.acm.org/10.1145/3314221.3314642."
REFERENCES,0.6255144032921811,"Gianluca Detommaso, Tiangang Cui, Youssef M. Marzouk, Alessio Spantini, and Robert Scheichl. A
Stein variational Newton method. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen
Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018,
NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pp. 9187–9197, 2018. URL http:
//papers.nips.cc/paper/8130-a-stein-variational-newton-method."
REFERENCES,0.6296296296296297,"Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore,
Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorﬂow distributions. arXiv
preprint arXiv:1711.10604, 2017."
REFERENCES,0.6337448559670782,"Yihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized stein variational
gradient descent. arXiv preprint arXiv:1707.06626, 2017. URL https://arxiv.org/abs/
1707.06626."
REFERENCES,0.6378600823045267,"Roy Frostig, Matthew Johnson, and Chris Leary. Compiling machine learning programs via high-level
tracing. In SysML 2018, 2018. URL http://www.sysml.cc/doc/2018/146.pdf."
REFERENCES,0.6419753086419753,"Zhe Gan, Chunyuan Li, Ricardo Henao, David E Carlson, and Lawrence Carin. Deep temporal
sigmoid belief networks for sequence modeling. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama,"
REFERENCES,0.6460905349794238,Under review as a conference paper at ICLR 2022
REFERENCES,0.6502057613168725,"and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Cur-
ran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
95151403b0db4f75bfd8da0b393af853-Paper.pdf."
REFERENCES,0.654320987654321,"Hong Ge, Kai Xu, and Zoubin Ghahramani. Turing: a language for ﬂexible probabilistic inference.
In International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2018, 9-11 April
2018, Playa Blanca, Lanzarote, Canary Islands, Spain, pp. 1682–1690, 2018.
URL http:
//proceedings.mlr.press/v84/ge18b.html."
REFERENCES,0.6584362139917695,"Jackson Gorham and Lester W. Mackey. Measuring sample quality with kernels. In Doina Precup
and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine
Learning Research, pp. 1292–1301. PMLR, 2017. URL http://proceedings.mlr.press/
v70/gorham17a.html."
REFERENCES,0.6625514403292181,"Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A
kernel two-sample test. The Journal of Machine Learning Research, 13(1):723–773, 2012. URL
https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf."
REFERENCES,0.6666666666666666,"Matthew D. Hoffman, David M. Blei, Chong Wang, and John W. Paisley. Stochastic variational
inference. J. Mach. Learn. Res., 14(1):1303–1347, 2013. URL http://dl.acm.org/citation.
cfm?id=2502622."
REFERENCES,0.6707818930041153,"Martin Jankowiak and Theofanis Karaletsos. Pathwise derivatives for multivariate distributions.
In Kamalika Chaudhuri and Masashi Sugiyama (eds.), The 22nd International Conference on
Artiﬁcial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan,
volume 89 of Proceedings of Machine Learning Research, pp. 333–342. PMLR, 2019. URL
http://proceedings.mlr.press/v89/jankowiak19a.html."
REFERENCES,0.6748971193415638,"Tony Jebara, Risi Kondor, and Andrew Howard. Probability product kernels. The Journal of Ma-
chine Learning Research, 5:819–844, 2004. URL https://www.jmlr.org/papers/volume5/
jebara04a/jebara04a.pdf."
REFERENCES,0.6790123456790124,"Diederik Kingma and Max Welling. Efﬁcient gradient-based inference through transformations
between bayes nets and neural nets. In International Conference on Machine Learning, pp.
1782–1790. PMLR, 2014. URL http://proceedings.mlr.press/v32/kingma14.pdf."
REFERENCES,0.6831275720164609,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. URL https://arxiv.org/abs/1412.6980."
REFERENCES,0.6872427983539094,"Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state
space models. arXiv preprint arXiv:1609.09869, 2016. URL https://arxiv.org/abs/1609.
09869."
REFERENCES,0.691358024691358,"Yingzhen Li and Richard E Turner.
Rényi divergence variational inference.
arXiv preprint
arXiv:1602.02311, 2016. URL https://arxiv.org/abs/1602.02311."
REFERENCES,0.6954732510288066,"Qiang Liu. Stein variational gradient descent as gradient ﬂow. In Isabelle Guyon, Ulrike von Luxburg,
Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 3115–
3123, 2017. URL http://papers.nips.cc/paper/6904-stein-variational-gradient-
descent-as-gradient-flow."
REFERENCES,0.6995884773662552,"Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference
algorithm. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on
Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2370–
2378, 2016. URL http://papers.nips.cc/paper/6338-stein-variational-gradient-
descent-a-general-purpose-bayesian-inference-algorithm."
REFERENCES,0.7037037037037037,Under review as a conference paper at ICLR 2022
REFERENCES,0.7078189300411523,"Qiang Liu and Dilin Wang. Stein variational gradient descent as moment matching. In Samy Bengio,
Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada,
pp. 8868–8877, 2018. URL http://papers.nips.cc/paper/8101-stein-variational-
gradient-descent-as-moment-matching."
REFERENCES,0.7119341563786008,"Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-ﬁt tests.
In International conference on machine learning, pp. 276–284. PMLR, 2016."
REFERENCES,0.7160493827160493,"Eric Nalisnick and Padhraic Smyth. Variational inference with stein mixtures. In Advances in Approx-
imate Bayesian Inference, NIPS 2017 Workshop, 2017. URL http://approximateinference.
org/2017/accepted/NalisnickSmyth2017.pdf."
REFERENCES,0.720164609053498,"Fritz Obermeyer, Eli Bingham, Martin Jankowiak, Du Phan, and Jonathan P Chen. Functional
tensors for probabilistic programming. arXiv preprint arXiv:1910.10775, 2019. URL https:
//arxiv.org/abs/1910.10775."
REFERENCES,0.7242798353909465,"Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable effects for ﬂexible and accelerated
probabilistic programming in numpyro. In Program Transformations for Machine Learning,
NeurIPS 2019 Workshop, 2019."
REFERENCES,0.7283950617283951,"Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In Proceedings
of the Seventeenth International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2014,
Reykjavik, Iceland, April 22-25, 2014, volume 33 of JMLR Workshop and Conference Proceedings,
pp. 814–822. JMLR.org, 2014. URL http://proceedings.mlr.press/v33/ranganath14.
html."
REFERENCES,0.7325102880658436,"Rajesh Ranganath, Dustin Tran, Jaan Altosaar, and David Blei. Operator variational inference.
Advances in Neural Information Processing Systems, 29:496–504, 2016a.
URL https:
//proceedings.neurips.cc/paper/2016/file/d947bf06a885db0d477d707121934ff8-
Paper.pdf."
REFERENCES,0.7366255144032922,"Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In International
Conference on Machine Learning, pp. 324–333. PMLR, 2016b."
REFERENCES,0.7407407407407407,"John Salvatier, Thomas V. Wiecki, and Christopher Fonnesbeck. Probabilistic programming in
python using pymc3. PeerJ Comput. Sci., 2:e55, 2016. doi: 10.7717/peerj-cs.55. URL https:
//doi.org/10.7717/peerj-cs.55."
REFERENCES,0.7448559670781894,"Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja Rudolph, Dawen Liang, and David M.
Blei. Edward: A library for probabilistic modeling, inference, and criticism. arXiv preprint
arXiv:1610.09787, 2016."
REFERENCES,0.7489711934156379,"Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. An introduction to
probabilistic programming. CoRR, abs/1809.10756, 2018. URL http://arxiv.org/abs/1809.
10756."
REFERENCES,0.7530864197530864,"Dilin Wang and Qiang Liu. Nonlinear stein variational gradient descent for learning diversiﬁed
mixture models. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 6576–6585, Long Beach, California, USA, 09–15 Jun 2019a. PMLR. URL
http://proceedings.mlr.press/v97/wang19h.html."
REFERENCES,0.757201646090535,"Dilin Wang and Qiang Liu. Nonlinear stein variational gradient descent for learning diversiﬁed
mixture models. In International Conference on Machine Learning, pp. 6576–6585. PMLR, 2019b.
URL http://proceedings.mlr.press/v97/wang19h.html."
REFERENCES,0.7613168724279835,"Dilin Wang, Yihao Feng, and Qiang Liu. Learning to sample using stein discrepancy. In NIPS
workshop on bayesian deep learning, volume 192, 2016. URL http://bayesiandeeplearning.
org/2016/papers/BDL_21.pdf."
REFERENCES,0.7654320987654321,Under review as a conference paper at ICLR 2022
REFERENCES,0.7695473251028807,"Dilin Wang, Zhe Zeng, and Qiang Liu. Stein variational message passing for continuous graphical
models. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,
2018, volume 80 of Proceedings of Machine Learning Research, pp. 5206–5214. PMLR, 2018a.
URL http://proceedings.mlr.press/v80/wang18l.html."
REFERENCES,0.7736625514403292,"Dilin Wang, Zhe Zeng, and Qiang Liu. Stein variational message passing for continuous graphical
models. In International Conference on Machine Learning, pp. 5219–5227. PMLR, 2018b. URL
http://proceedings.mlr.press/v80/wang18l.html."
REFERENCES,0.7777777777777778,"Dilin Wang, Ziyang Tang, Chandrajit Bajaj, and Qiang Liu.
Stein variational gradient de-
scent with matrix-valued kernels.
In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 32:
Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp.
7834–7844, 2019a.
URL http://papers.nips.cc/paper/8998-stein-variational-
gradient-descent-with-matrix-valued-kernels."
REFERENCES,0.7818930041152263,"Dilin Wang, Ziyang Tang, Chandrajit Bajaj, and Qiang Liu.
Stein variational gradient
descent with matrix-valued kernels.
Advances in Neural Information Processing Sys-
tems, 32:7834, 2019b.
URL https://proceedings.neurips.cc/paper/2019/file/
5dcd0ddd3d918c70d380d32bce4e733a-Paper.pdf."
REFERENCES,0.7860082304526749,"Andrew Gordon Wilson. The case for bayesian deep learning. arXiv preprint arXiv:2001.10995,
2020."
REFERENCES,0.7901234567901234,"Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective
of generalization. arXiv preprint arXiv:2002.08791, 2020."
REFERENCES,0.7942386831275721,"Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, and Bo Zhang. Message passing Stein
variational gradient descent. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 6013–
6022. PMLR, 2018. URL http://proceedings.mlr.press/v80/zhuo18a.html."
REFERENCES,0.7983539094650206,Under review as a conference paper at ICLR 2022
REFERENCES,0.8024691358024691,"A
APPENDIX"
REFERENCES,0.8065843621399177,"Kernel
Deﬁnition
Detail
Type
Reference"
REFERENCES,0.8106995884773662,"Radial Basis
Function (RBF)"
REFERENCES,0.8148148148148148,exp( 1
REFERENCES,0.8189300411522634,"h ∥x −y ∥2
2)
scalar
Liu & Wang
(2016)"
REFERENCES,0.823045267489712,exp( 1
REFERENCES,0.8271604938271605,"h(x −y))
vector
Pyro10
Inverse Multi-
Quadratic
(IMQ)
(c2+ ∥x −y ∥2
2)β
β ∈(−1, 0) and c > 0
scalar
Gorham &
Mackey
(2017)"
REFERENCES,0.831275720164609,"Random
Feature
Expansion"
REFERENCES,0.8353909465020576,"Ew[φ(x, w)φ(y, w)]"
REFERENCES,0.8395061728395061,"φ(x, w) =
√"
REFERENCES,0.8436213991769548,2 cos( 1
REFERENCES,0.8477366255144033,"hw⊤
1 x + w0)
where w0 ∼Unif(0, 2π)
and w1 ∼N(0, 1)"
REFERENCES,0.8518518518518519,"scalar
Liu & Wang
(2018)"
REFERENCES,0.8559670781893004,"Linear
x⊤y + 1
scalar
Liu & Wang
(2018)"
REFERENCES,0.8600823045267489,"Mixture
P"
REFERENCES,0.8641975308641975,"i wiki(x, y)
{ki}i individual kernels,
weights wi"
REFERENCES,0.8683127572016461,"scalar,
vector,
matrix"
REFERENCES,0.8724279835390947,"Liu & Wang
(2018)"
REFERENCES,0.8765432098765432,"Scalar-based
Matrix
k(x, y)I
k scalar-valued kernel
matrix
Wang et al.
(2019a)
Vector-based
Matrix
diag(k(x, y))
k vector-valued kernel
matrix
Wang et al.
(2019a)"
REFERENCES,0.8806584362139918,"Graphical
diag({K(ℓ)(x, y)}ℓ)
{K(ℓ)}ℓmatrix-valued
kernels, each for a unique
partition of latent variables
matrix
Wang et al.
(2019a)"
REFERENCES,0.8847736625514403,"Constant Pre-
conditioned
Q−1"
REFERENCES,0.8888888888888888,"2 K(Q
1
2 x, Q
1
2 y)Q−1 2"
REFERENCES,0.8930041152263375,"K is an inner matrix-valued
kernel and Q is a
preconditioning matrix like
the Hessian −∇2
¯z log p(¯z|x)
or Fischer information
−Ez∼qZ(z)[∇2
z log p(z|x)]
matrices"
REFERENCES,0.897119341563786,"matrix
Wang et al.
(2019a)"
REFERENCES,0.9012345679012346,"Anchor Point
Precondi-
tioned"
REFERENCES,0.9053497942386831,"Pm
ℓ=1 KQℓ(x, y)wℓ(x)wℓ(y)"
REFERENCES,0.9094650205761317,"{aℓ}m
ℓ=1 is a set of anchor
points, Qℓ= Q(aℓ) is a
preconditioning matrix for
each anchor point, KQℓis
an inner kernel conditioned
using Qℓ, and wℓ(x) =
softmaxℓ({N(x|aℓ′, Q−1
ℓ′ )}ℓ′)"
REFERENCES,0.9135802469135802,"matrix
Wang et al.
(2019a)"
REFERENCES,0.9176954732510288,"B
EXPERIMENTS"
REFERENCES,0.9218106995884774,"Star Distribution
To illustrate the kernels included in EinSteinVI, we approximate the ""star
distribution"", follow Wang et al. (2019b) (see Figure 3). The star distribution is constructed as a 2D
Gaussian mixture,"
REFERENCES,0.9259259259259259,"p(x) = 1 K K
X"
REFERENCES,0.9300411522633745,"k=1
N(x|µk, Σk)
µ1 = [0, 1.5], µk = Ukµ1
Σ1 = diag([1, 1"
REFERENCES,0.934156378600823,"100]), Σk = UkΣ1Uk"
REFERENCES,0.9382716049382716,where Uk is a rotation matrix given by
REFERENCES,0.9423868312757202,"Uk =

cos(θ)
−sin(θ)
sin(θ)
cos(θ)"
REFERENCES,0.9465020576131687,"
,
(k ∈[1, .., K])θ = 2kπ/K."
REFERENCES,0.9506172839506173,"We use ﬁfty particles and Adagrad with the learning rate that yields the best Maximum Mean
Discrepancy (MMD) (Gretton et al., 2012) with a RBF kernel. To compute the MMD, we use 10,000"
REFERENCES,0.9547325102880658,Under review as a conference paper at ICLR 2022
REFERENCES,0.9588477366255144,"Initial
Linear
IMQ
RBF"
REFERENCES,0.9629629629629629,"Random Feature
Mixture
Matrix Const."
REFERENCES,0.9670781893004116,"0
200
400
600
800
1000
Iterations 0.02 0.04 0.06 0.08 0.10 0.12 MMD"
REFERENCES,0.9711934156378601,"Linear
IMQ
RBF
Random Feature
Mixture
Matrix Const."
REFERENCES,0.9753086419753086,"Figure 3: Particle positions for EinSteinVI with different kernels after 1000 iterations with a point
mass guide, starting from particle positions given in the upper left frame labelled Initial. The MMD,
bottom right frame, is evaluated using the RBF kernel."
REFERENCES,0.9794238683127572,"Table 4: Average test RMSE and time for inference for the UCI regression benchmarks with amortized
SVGD."
REFERENCES,0.9835390946502057,"RMSE
Time
Boston
87230.469 ± 54219.534
5m46s ± 3s
Concrete
2458.250 ± 754.653
6m3s ± 2s
Energy
2458.250 ± 754.653
5m45s ± 3s
Kin8nm
302.218 ± 43.818
19m54s ± 5s
Naval
0.386 ± 0.128
28m39s ± 3s
Power
3333.796 ± 517.241
21m38s ± 6s
Protein
64.791 ± 12.181
1h32m28s ± 1m1s
Wine
12.929 ± 6.202
7m21s ± 3s
Yacht
2822.983 ± 944.449
5m35s ± 4s"
REFERENCES,0.9876543209876543,"samples from the 2D gaussian mixture. Keeping these choices ﬁxed, we vary the type of kernel, using
a point mass distribution as guide."
REFERENCES,0.9917695473251029,"We consider the scalar (Rd →R) linear kernel, the inverse multiquadric (IMQ) kernel, the RBF
kernel and the random feature kernel; details are given in appendix A. In addition, we use a mixture
kernel, which is a uniform mixture of the linear and the random feature kernels. We also use the
matrix version of the RBF kernel with a constant preconditioning, using the Hessian matrix for
preconditioning. We see that including curvature information by means of the Hessian leads to faster
convergence in EinSteinVI and that the matrix RBF with preconditioning yields the lowest MMD."
REFERENCES,0.9958847736625515,"Bayesian Neural Network
The amortized SVGD in PyMC3 is an experimental implementation
and the documentation clearly discourages its use. For completeness we include the performance
here. The temperature and learning rate were determined by a grid search on the Boston data set.
All measurements are repeated ﬁve times. In Table 4 we report test RMSE and time the the UCI
regression benchmark. We did not include the Year data set as it takes over 21 hours to run. The
RMSE for amortized SVGD is poor and highly variable across all data sets, except Naval. However,
even for Naval the RMSE is two orders of magnitude greater than SVGD in EinSteinVI and Liu &
Wang (2016)."
