Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0020408163265306124,"Discrete-continuous hybrid action space is a natural setting in many practical
problems, such as robot control and game AI. However, most previous Reinforce-
ment Learning (RL) works only demonstrate the success in controlling with either
discrete or continuous action space, while seldom take into account the hybrid
action space. One naive way to address hybrid action RL is to convert the hybrid
action space into a uniﬁed homogeneous action space by discretization or con-
tinualization, so that conventional RL algorithms can be applied. However, this
ignores the underlying structure of hybrid action space and also induces the scala-
bility issue and additional approximation difﬁculties, thus leading to degenerated
results. In this paper, we propose Hybrid Action Representation (HyAR) to learn
a compact and decodable latent representation space for the original hybrid action
space. HyAR constructs the latent space and embeds the dependence between
discrete action and continuous parameter via an embedding table and conditional
Variational Auto-Encoder (VAE). To further improve the effectiveness, the action
representation is trained to be semantically smooth through unsupervised environ-
mental dynamics prediction. Finally, the agent then learns its policy with con-
ventional DRL algorithms in the learned representation space and interacts with
the environment by decoding the hybrid action embeddings to the original action
space. We evaluate HyAR in a variety of environments with discrete-continuous
action space. The results demonstrate the superiority of HyAR when compared
with previous baselines, especially for high-dimensional action spaces."
INTRODUCTION,0.004081632653061225,"1
INTRODUCTION"
INTRODUCTION,0.006122448979591836,"Deep Reinforcement learning (DRL) has recently shown a great success in a variety of decision-
making problems that involve controls with either discrete actions, such as Go (Silver et al., 2016)
and Atari (Mnih et al., 2015), or continuous actions, such as robot control (Schulman et al., 2015;
Lillicrap et al., 2015). However, in contrast to these two kinds of homogeneous action space, many
real-world scenarios requires more complex controls with discrete-continuous hybrid action space,
e.g., Robot Soccer (Masson et al., 2016) and Games (Xiong et al., 2018). For example, in robot
soccer, the agent not only needs to choose whether to shoot or pass the ball (i.e., discrete actions)
but also the associated angle and force (i.e., continuous parameters). Such a hybrid action is also
called parameterized action in some previous works (Hausknecht & Stone, 2016; Massaroli et al.,
2020). Unfortunately, most conventional RL algorithms cannot deal with such a heterogeneous
action space directly, thus preventing the application of RL in these kinds of practical problems."
INTRODUCTION,0.00816326530612245,"To deal with hybrid action space, the most straightforward approach is to convert the heterogeneous
space into a homogeneous one through discretization or continualization. However, it is apparent"
INTRODUCTION,0.01020408163265306,"∗Equal contribution.
†Corresponding authors: Yan Zheng (yanzheng@tju.edu.cn) , Jianye Hao (jianye.hao@tju.edu.cn) and Zhen
Wang(w-zhen@nwpu.edu.cn)."
INTRODUCTION,0.012244897959183673,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014285714285714285,"that discretizing continuous parameter space suffers from the scalability issue due to the exponen-
tially exploring number of discretized actions; while casting all discrete actions into a continuous
dimension produces a piecewise-function action subspace, resulting in additional difﬁculties in ap-
proximation and generalization. To overcome these problems, a few recent works propose speciﬁc
policy structures to learn DRL policies over the original hybrid action space directly. Parameterized
Action DDPG (PADDPG) (Hausknecht & Stone, 2016) makes use of a DDPG (Lillicrap et al., 2015)
structure where the actor is modiﬁed to output a uniﬁed continuous vector as the concatenation of
values for all discrete actions and all corresponding continuous parameters. By contrast, Hybrid
PPO (HPPO) (Fan et al., 2019) uses multiple policy heads consisting of one for discrete actions
and the others for corresponding continuous parameter of each discrete action separately. A very
similar idea is also adopted in (Peng & Tsuruoka, 2019). These methods are convenient to imple-
ment and are demonstrated to effective in simple environments with low-dimensional hybrid action
space. However, PADDPG and HPPO neglect the dependence between discrete and continuous
components of hybrid actions, thus can be problematic since the dependence is vital to identifying
the optimal hybrid actions in general. Besides, the modeling of all continuous parameter dimen-
sions all the time introduces redundancy in computation and policy learning, and may also have the
scalability issue when the hybrid action space becomes high-dimensional."
INTRODUCTION,0.0163265306122449,"Algorithm
Scalability
Stationarity
Dependence
Latent"
INTRODUCTION,0.018367346938775512,"PADDPG
%
!
%
%
HPPO
%
!
%
%
PDQN
%
!
!
%
HHQN
!
%
!
%"
INTRODUCTION,0.02040816326530612,"HyAR (Ours)
!
!
!
!"
INTRODUCTION,0.022448979591836733,"Table 1: A comparison on algorithmic properties of ex-
isting methods for discrete-continous hybrid action RL."
INTRODUCTION,0.024489795918367346,"To model the dependence, Parameterized
DQN (PDQN) (Xiong et al., 2018) pro-
poses a hybrid structure of DQN (Mnih
et al., 2015) and DDPG. The discrete pol-
icy is represented by a DQN which ad-
ditionally takes as input all the continu-
ous parameters output by the DDPG ac-
tor; while the DQN also serves as the critic
of DDPG. Some variants of such a hybrid
structure are later proposed in (Delalleau
et al., 2019; Ma et al., 2021; Bester et al.,
2019). Due to the DDPG actor’s modeling of all parameters, PDQN also have the redundancy and
potential scalablity issue. In an upside-down way, Hierarchical Hybrid Q-Network (HHQN) (Fu
et al., 2019) models the dependent hybrid-action policy with a two-level hierarchical structure. The
high level is for the discrete policy and the selected discrete action serves as the condition (in analogy
to subgoal) which the low-level continuous policy conditions on. This can be viewed as a special
two-agent cooperative game where the high level and low level learn to coordinate at the optimal
hybrid actions. Although the hierarchical structure (Wei et al., 2018) seems to be natural, it suffers
from the high-level non-stationarity caused by off-policy learning dynamics (Wang et al., 2020),
i.e., a discrete action can no longer induce the same transition in historical experiences due to the
change of the low-level policy. In contrast, PADDPG, HPPO and PDQN are stationary in this sense
since they all learn an overall value function and policy thanks to their special structures, which is
analogous to learning a joint policy in the two-agent game. All the above works focus on policy
learning over original hybrid action space. As summarized in Table 1, none of them is able to offer
three desired properties, i.e., scalability, stationarity and action dependence, at the same time."
INTRODUCTION,0.026530612244897958,Latent Space Env
INTRODUCTION,0.02857142857142857,"Latent 
Action"
INTRODUCTION,0.030612244897959183,"Latent Policy
 Decode"
INTRODUCTION,0.0326530612244898,Original  Hybrid
INTRODUCTION,0.03469387755102041,Action
INTRODUCTION,0.036734693877551024,"Discrete
Continuous"
INTRODUCTION,0.03877551020408163,State  Reward
INTRODUCTION,0.04081632653061224,"Figure 1:
An overview of HyAR. Agent
learns a latent policy in the latent represen-
tation space of discrete-continuous actions.
The selected latent action is then decoded
into the original space so as to interact with
the environment."
INTRODUCTION,0.04285714285714286,"In this paper, we propose a novel framework for hy-
brid action RL, called Hybrid Action Representation
(HyAR), to achieve all three properties in Table 1.
A conceptual overview of HyAR is shown in Fig. 1.
The main idea is to construct a uniﬁed and decodable
representation space for original discrete-continuous
hybrid actions, among which the agent learns a la-
tent policy. Then, the selected latent action is de-
coded back to the original hybrid action space so as
to interact with the environment. HyAR is inspired
by recent advances in Representation Learning in
DRL. Action representation learning has shown the
potentials in boosting learning performance (Whit-
ney et al., 2020), reducing large discrete action space
(Chandak et al., 2019), improving generalization in
ofﬂine RL (Zhou et al., 2020) and so on. Different
from these works, to the best knowledge, we are the"
INTRODUCTION,0.044897959183673466,Published as a conference paper at ICLR 2022
INTRODUCTION,0.04693877551020408,"ﬁrst to propose representation learning for discrete-continuous hybrid actions, which consist of het-
erogeneous and dependent action components."
INTRODUCTION,0.04897959183673469,"In HyAR, we maintain a continuous vector for each discrete action in a learnable embedding table;
then a conditional Variational Auto-encoder (VAE) (Kingma & Welling, 2014) that conditions on
the state and the embedding of discrete action is used to construct the latent representation space
for the associated continuous parameters. Different from HHQN, the conditional VAE models and
embeds the dependence in an implicit fashion. The learned representation space are compact and
thus scalable, while also provides convenient decoding by nearest-neighbor lookup of the embed-
ding table and the VAE decoder. Moreover, we utilize the unsupervised environmental dynamics to
learn dynamics predictive hybrid action representation. Such a representation space can be seman-
tically smooth, i.e., hybrid action representations that are close in the space have similar inﬂuence
on environmental dynamics, thus beneﬁts hybrid action RL in the representation space. With the
constructed action representation space, we use TD3 algorithm (Fujimoto et al., 2018) for the latent
policy learning. To ensure the effectiveness, we further propose two mechanisms: latent space con-
straint and representation shift correction to deal with unreliable latent representations and outdated
off-policy action representation experiences respectively. In our experiments, we evaluate HyAR
in a few representative environments with hybrid action space, as well as several new and more
challenging benchmarks."
INTRODUCTION,0.05102040816326531,Our main contributions are summarized below:
INTRODUCTION,0.053061224489795916,"• We propose a novel and generic framework for discrete-continuous hybrid action RL by
leveraging representation learning of hybrid action space for the ﬁrst time.
• We propose an unsupervised method of learning a compact and decodable representation
space for discrete-continuous hybrid actions, along with two mechanisms to improve the
effectiveness of latent policy learning.
• Our algorithm consistently outperforms prior algorithms in representative hybrid-action
benchmarks, especially demonstrating signiﬁcant superiority when the hybrid action space
becomes larger. For reproducibility, codes are provided in the supplementary material."
PRELIMINARIES,0.05510204081632653,"2
PRELIMINARIES"
PRELIMINARIES,0.05714285714285714,"Markov
Decision
Process
Consider
a
standard
Markov
Decision
Process
(MDP)
⟨S, A, P, R, γ, T⟩, deﬁned with a state set S, an action set A, transition function P : S×A×S →R,
reward function R : S × A →R, discounted factor γ ∈[0, 1) and horizon T. The agent interacts
with the MDP by performing its policy π : S →A. The objective of an RL agent is to optimize
its policy to maximize the expected discounted cumulative reward J(π) = Eπ[PT
t=0 γtrt], where
s0 ∼ρ0 (s0) the initial state distribution, at ∼π (st), st+1 ∼P (st+1 | st, at) and rt = R (st, at)."
PRELIMINARIES,0.05918367346938776,"The state-action value function Qπ is deﬁned as Qπ(s, a) = Eπ
hPT
t=0 γtrt | s0 = s, a0 = a
i
."
PRELIMINARIES,0.061224489795918366,"Parameterized Action MDP
In this paper, we focus on a Parameterized Action Markov Decision
Process (PAMDP) ⟨S, H, P, R, γ, T⟩(Masson et al., 2016). PAMDP is an extension of stardard
MDP with a discrete-continuous hybrid action space H deﬁned as:"
PRELIMINARIES,0.06326530612244897,"H = {(k, xk) | xk ∈Xk for all k ∈K} ,
(1)"
PRELIMINARIES,0.0653061224489796,"where K = {1, · · · , K} is the discrete action set, Xk is the corresponding continuous parameter
set for each k ∈K. We call any pair of k, xk as a hybrid action and also call H as hybrid action
space for short in this paper. In turn, we have state transition function P : S × H × S →R, reward
function R : S × H →R, agent’s policy π : S →H and hybrid-action value function Qπ(s, k, xk)."
PRELIMINARIES,0.0673469387755102,"Conventional RL algorithms are not compatible with hybrid action space H. Typical policy represen-
tations such as Multinomial distribution or Gaussian distribution can not model the heterogeneous
components among the hybrid action. Implicit policies derived by action value functions, often
adopted in value-based algorithms, also fail due to intractable maximization over inﬁnite hybrid ac-
tions. In addition, there exists the dependence between discrete actions and continuous parameters,
as a discrete action k determines the valid parameter space Xk associated with it. In other words,
the same parameter paired with different discrete actions can be signiﬁcantly different in semantics.
This indicate that in principle an optimal hybrid-action policy can not determine the continuous
parameters beforehand the discrete action is selected."
PRELIMINARIES,0.06938775510204082,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.07142857142857142,Reinforcement Learning with Hybrid Action Representation
PRELIMINARIES,0.07346938775510205,"𝑠, 𝑘, 𝑥𝑘, 𝑒, 𝑧𝑥, 𝑟, 𝑠′"
PRELIMINARIES,0.07551020408163266,Policy
PRELIMINARIES,0.07755102040816327,"Environment
Embedding Table"
PRELIMINARIES,0.07959183673469387,"Original Actions
Latent Actions"
PRELIMINARIES,0.08163265306122448,Representation Decoding
PRELIMINARIES,0.0836734693877551,Autoencoder (𝑥𝑘) (e) (𝑧𝑥)
PRELIMINARIES,0.08571428571428572,"(𝑘)
discrete"
PRELIMINARIES,0.08775510204081632,"continuous
continuous"
PRELIMINARIES,0.08979591836734693,discrete
PRELIMINARIES,0.09183673469387756,Representation Training
PRELIMINARIES,0.09387755102040816,（for continuous parameters)
PRELIMINARIES,0.09591836734693877,"( for discrete actions) 𝒛𝒙D
E"
PRELIMINARIES,0.09795918367346938,Hybrid Action Representation Model
PRELIMINARIES,0.1,"Decoder
Encoder 𝑥𝑘"
PRELIMINARIES,0.10204081632653061,"𝑠, 𝑘, 𝑥𝑘, 𝑒, 𝑧𝑥, 𝑟, 𝑠′"
PRELIMINARIES,0.10408163265306122,"𝑘look up ⊗
⊗ 𝜇𝑥 σx ෤𝑥𝑘"
PRELIMINARIES,0.10612244897959183,"ሚ𝛿𝑠,𝑠′"
PRELIMINARIES,0.10816326530612246,Embedding Table
PRELIMINARIES,0.11020408163265306,"s
𝑒𝜁,𝑘 𝑧𝑥"
PRELIMINARIES,0.11224489795918367,":    Multi-layer Perceptron (MLP)
⨂:   Element-wise product operation"
PRELIMINARIES,0.11428571428571428,Predictor
PRELIMINARIES,0.11632653061224489,"Figure 2: Illustrations of: (left) the framework DRL with HyAR; and (right) overall workﬂow of
hybrid action representation model, consisting of an embedding table and a conditional VAE."
PRELIMINARIES,0.11836734693877551,"3
HYBRID ACTION REPRESENTATION (HYAR)"
PRELIMINARIES,0.12040816326530612,"As mentioned in previous sections, it is non-trivial for an RL agent to learn with discrete-continuous
hybrid action space efﬁciently due to the heterogeneity and action dependence. Naive solutions by
converting the hybrid action space into either a discrete or a continuous action space can result in
degenerated performance due to the scalability issue and additional approximation difﬁculty. Previ-
ous efforts concentrate on proposing speciﬁc policy structures (Hausknecht & Stone, 2016; Fu et al.,
2019) that are feasible to learn hybrid-action policies directly over original hybrid action space.
However, these methods fail in providing the three desired properties: scalability, stationarity and
action dependence simultaneously (See Tab. 1)."
PRELIMINARIES,0.12244897959183673,"Inspired by recent advances in Representation Learning for RL (Whitney et al., 2020; Chandak
et al., 2019), we propose Hybrid Action Representation (HyAR), a novel framework that converts
the original hybrid-action policy learning into a continuous policy learning problem among the la-
tent action representation space. The intuition behind HyAR is that discrete action and continuous
parameter are heterogeneous in their original representations but they jointly inﬂuence the environ-
ment; thus we can assume that hybrid actions lie on a homogeneous manifold that is closely related
to environmental dynamics semantics. In the following, we introduce an unsupervised approach of
constructing a compact and decodable latent representation space to approximate such a manifold."
DEPENDENCE-AWARE ENCODING AND DECODING,0.12448979591836734,"3.1
DEPENDENCE-AWARE ENCODING AND DECODING"
DEPENDENCE-AWARE ENCODING AND DECODING,0.12653061224489795,"A desired latent representation space for hybrid actions should take the dependence between the two
heterogeneous components into account. Moreover, we need the representation space to be decod-
able, i.e., the latent actions selected by a latent policy can be mapped back to the original hybrid
actions so as to interact with the environment. To this end, we propose dependence-aware encoding
and decoding of hybrid action. The overall workﬂow is depicted in the right of Fig. 2. We establish
an embedding table Eζ ∈RK×d1 with learnable parameter ζ to represent the K discrete actions,
where each row eζ,k = Eζ(k) (with k being the row index) is a d1-dimensional continuous vector
for the discrete action k. Then, we use a conditional Variational Auto-Encoder (VAE) (Kingma &
Welling, 2014) to construct the latent representation space for the continuous parameters. In spe-
ciﬁc, for a hybrid action k, xk and a state s, the encoder qφ(z | xk, s, eζ,k) parameterized by φ takes
s and the embedding eζ,k as condition, and maps xk into the latent variable z ∈Rd2. With the
same condition, the decoder pψ(˜xk | z, s, eζ,k) parameterized by ψ then reconstructs the continuous
parameter ˜xk from z. In principle, the conditional VAE can be trained by maximizing the variational
lower bound (Kingma & Welling, 2014)."
DEPENDENCE-AWARE ENCODING AND DECODING,0.12857142857142856,"More concretely, we adopt a Gaussian latent distribution N(µx, σx) for qφ(z | xk, s, eζ,k) where
µx, σx are the mean and standard deviation outputted by the encoder.
For any latent variable
sample z ∼N(µx, σx), the decoder decodes it deterministicly, i.e., ˜xk = pψ(z, s, eζ,k) =
gψ1 ◦fψ0(z, s, eζ,k), where fψ0 is a transformation network and gψ1 is a fully-connected layer for
reconstruction. We use ψ = ∪i∈{0,1,2}ψi to denote the parameters of both the decoder network and
prediction network (introduced later in Sec. 3.2) as well as the transformation network they share.
With a batch of states and original hybrid actions from buffer D, we train the embedding table Eζ
and the conditional VAE qφ, pψ together by minimizing the loss function LVAE below:"
DEPENDENCE-AWARE ENCODING AND DECODING,0.1306122448979592,"LVAE(φ, ψ, ζ) = Es,k,xk∼D,z∼qφ
h
∥xk −˜xk∥2
2 + DKL
 
qφ(· | xk, s, eζ,k)∥N(0, I)
i
,
(2)"
DEPENDENCE-AWARE ENCODING AND DECODING,0.1326530612244898,Published as a conference paper at ICLR 2022
DEPENDENCE-AWARE ENCODING AND DECODING,0.1346938775510204,"where the ﬁrst term is the L2-norm square reconstruction error and the second term is the Kullback-
Leibler divergence DKL between the variational posterior of latent representation z and the standard
Gaussian prior. Note ˜xk is differentiable with respect to ψ, ζ and φ through reparameterization trick
(Kingma & Welling, 2014)."
DEPENDENCE-AWARE ENCODING AND DECODING,0.13673469387755102,"The embedding table and conditional VAE jointly construct a compact and decodable hybrid action
representation space (∈Rd1+d2) for hybrid actions. We highlight that this is often much smaller
than the joint action space RK+P"
DEPENDENCE-AWARE ENCODING AND DECODING,0.13877551020408163,"k |Xk| considered in previous works (e.g., PADDPG, PDQN and
HPPO), especially when K or P"
DEPENDENCE-AWARE ENCODING AND DECODING,0.14081632653061224,"k |Xk| is large. In this sense, HyAR is expected to be more scalable
when compared in Tab. 1. Moreover, the conditional VAE embeds the dependence of continuous
parameter on corresponding discrete action in the latent space; and allows to avoid the redundancy
of outputting all continuous parameters at any time (i.e., R
P"
DEPENDENCE-AWARE ENCODING AND DECODING,0.14285714285714285,"k |Xk|). This resembles the conditional
structure adopted by HHQN (Fu et al., 2019) while HyAR is free of the non-stationary issue thanks
to learning a single policy in the hybrid representation space."
DEPENDENCE-AWARE ENCODING AND DECODING,0.14489795918367346,"For any latent variables e ∈Rd1 and zx ∈Rd2, they can be decoded into hybrid action k, xk conve-
niently by nearest-neighbor lookup of the embedding table along with the VAE decoder. Formally,
we summarize the encoding and decoding process below:"
DEPENDENCE-AWARE ENCODING AND DECODING,0.1469387755102041,"Encoding: eζ,k = Eζ(k), zx ∼qφ(· | xk, s, eζ,k)
for s, k, xk
Decoding: k = gE(e) = arg mink′∈K ∥eζ,k′ −e∥2, xk = pψ(zx, s, eζ,k)
for s, e, zx
(3)"
DYNAMICS PREDICTIVE REPRESENTATION,0.1489795918367347,"3.2
DYNAMICS PREDICTIVE REPRESENTATION"
DYNAMICS PREDICTIVE REPRESENTATION,0.1510204081632653,"In the above, we introduce how to construct a compact and decodable latent representation space for
original hybrid actions. However, the representation space learned by pure reconstruction of VAE
may be pathological in the sense that it is not discriminative to how hybrid actions have different
inﬂuence on the environment, similarly studied in (Grosnit et al., 2021). Therefore, such a represen-
tation space may be ineffective when involved in the learning of a RL policy and value functions,
as these functions highly depends on the knowledge of environmental dynamics. To this end, we
make full use of environmental dynamics and propose an unsupervised learning loss based on state
dynamics prediction to further reﬁne the hybrid action representation."
DYNAMICS PREDICTIVE REPRESENTATION,0.15306122448979592,"Intuitively, the dynamics predictive representation learned is semantically smooth. In other words,
hybrid action representations that are closer in the space reﬂects similar inﬂuence on environmental
dynamics of their corresponding original hybrid actions. Therefore, in principle such a representa-
tion space can be superior in the approximation and generalization of RL policy and value functions,
than that learned purely from VAE reconstruction. The beneﬁts of dynamics predictive representa-
tion are also demonstrated in (Whitney et al., 2020) (Schwarzer et al., 2020)."
DYNAMICS PREDICTIVE REPRESENTATION,0.15510204081632653,"As shown in the right of Fig. 2, HyAR adopts a subnetwork hψ2 that is cascaded after the transfor-
mation network fφ0 of the conditional VAE decoder (called cascaded structure or cascaded head
below) to produce the prediction of the state residual of transition dynamics. For any transition sam-
ple (s, k, xk, s′), the state residual is denoted by δs,s′ = s′ −s. With some abuse of notation (i.e.,
pψ = hψ2 ◦fψ0 here), the prediction ˜δs,s′ is produced as follows, which completes Eq. 3:"
DYNAMICS PREDICTIVE REPRESENTATION,0.15714285714285714,"Prediction: ˜δs,s′ = pψ(zx, s, eζ,k)
for s, e, zx
(4)"
DYNAMICS PREDICTIVE REPRESENTATION,0.15918367346938775,Then we minimize the L2-norm square prediction error:
DYNAMICS PREDICTIVE REPRESENTATION,0.16122448979591836,"LDyn(φ, ψ, ζ) = Es,k,xk,s′
h
∥˜δs,s′ −δs,s′∥2
2
i
.
(5)"
DYNAMICS PREDICTIVE REPRESENTATION,0.16326530612244897,"Our structure choice of cascaded prediction head is inspired by (Azabou et al., 2021). The reason
behind this is that dynamics prediction could be more complex than continuous action reconstruc-
tion, thus usual parallel heads for both reconstruction and the state residual prediction followed by
the same latent features may have interference in optimizing individual objectives and hinder the
learning of the shared representation."
DYNAMICS PREDICTIVE REPRESENTATION,0.1653061224489796,"So far, we derive the ultimate training loss for hybrid action representation as follows:
LHyAR(φ, ψ, ζ) = LVAE(φ, ψ, ζ) + βLDyn(φ, ψ, ζ),
(6)"
DYNAMICS PREDICTIVE REPRESENTATION,0.1673469387755102,"where β is a hyper-parameter that weights the dynamics predictive representation loss. Note that
the ultimate loss depends on reward-agnostic data of environmental dynamics, which is dense and
usually more convenient to obtain (Stooke et al., 2021; Yarats et al., 2021; Erraqabi et al., 2021)."
DYNAMICS PREDICTIVE REPRESENTATION,0.16938775510204082,Published as a conference paper at ICLR 2022
DYNAMICS PREDICTIVE REPRESENTATION,0.17142857142857143,Algorithm 1: HyAR-TD3
DYNAMICS PREDICTIVE REPRESENTATION,0.17346938775510204,"1 Initialize actor πω and critic networks Qθ1, Qθ2 with random parameters ω, θ1, θ2
2 Initialize discrete action embedding table Eζ and conditional VAE qφ, pψ with random parameters ζ, φ, ψ"
PREPARE REPLAY BUFFER D,0.17551020408163265,3 Prepare replay buffer D
PREPARE REPLAY BUFFER D,0.17755102040816326,4 repeat Stage 1
PREPARE REPLAY BUFFER D,0.17959183673469387,"5
Update ζ and φ, ψ using samples in D
▷see Eq. 6"
PREPARE REPLAY BUFFER D,0.1816326530612245,6 until reaching maximum warm-up training times;
PREPARE REPLAY BUFFER D,0.1836734693877551,7 repeat Stage 2
PREPARE REPLAY BUFFER D,0.18571428571428572,"8
for t ←1 to T do"
PREPARE REPLAY BUFFER D,0.18775510204081633,"9
// select latent actions in representation space"
PREPARE REPLAY BUFFER D,0.18979591836734694,"10
e, zx = πω(s) + ϵe, with ϵe ∼N(0, σ)"
PREPARE REPLAY BUFFER D,0.19183673469387755,"11
// decode into original hybrid actions"
PREPARE REPLAY BUFFER D,0.19387755102040816,"12
k = gE(e), xk = pψ(zx, s, eζ,k)
▷see Eq. 3"
PREPARE REPLAY BUFFER D,0.19591836734693877,"13
Execute (k, xk), observe rt and new state s′"
PREPARE REPLAY BUFFER D,0.19795918367346937,"14
Store {s, k, xk, e, zx, r, s′} in D"
SAMPLE A MINI-BATCH OF N EXPERIENCE FROM D,0.2,"15
Sample a mini-batch of N experience from D"
SAMPLE A MINI-BATCH OF N EXPERIENCE FROM D,0.20204081632653062,"16
Update Qθ1, Qθ2
▷see Eq. 7"
SAMPLE A MINI-BATCH OF N EXPERIENCE FROM D,0.20408163265306123,"17
Update πω with policy gradient
▷see Eq. 8"
REPEAT,0.20612244897959184,"18
repeat"
REPEAT,0.20816326530612245,"19
Update ζ and φ, ψ using samples in D
▷see Eq. 6"
REPEAT,0.21020408163265306,"20
until reaching maximum representation training times;"
REPEAT,0.21224489795918366,21 until reaching maximum total environment steps;
DRL WITH HYBRID ACTION REPRESENTATION,0.21428571428571427,"4
DRL WITH HYBRID ACTION REPRESENTATION"
DRL WITH HYBRID ACTION REPRESENTATION,0.2163265306122449,"In previous section, we introduce the construction of a compact, decodable and semantically smooth
hybrid action representation space. As the conceptual overview in Fig. 1, the next thing is to learn a
latent RL policy in the representation space. In principle, our framework is algorithm-agnostic and
any RL algorithms for continuous control can be used for implementation. In this paper, we adopt
model-free DRL algorithm TD3 (Fujimoto et al., 2018) for demonstration. Though there remains
the chance to build a world model based on hybrid action representation, we leave the study on
model-based RL with HyAR for future work."
DRL WITH HYBRID ACTION REPRESENTATION,0.21836734693877552,"TD3 is popular deterministic-policy Actor-Critic algorithm which is widely demonstrated to be ef-
fective in continuous control. As illustrated in the left of Fig. 2, with the learned hybrid action
representation space, the actor network parameterizes a latent policy πω with parameter ω that out-
puts the latent action vector, i.e., e, zx = πω(s) where e ∈Rd1, zx ∈Rd2. The latent action can
be decoded according to Eq. 3 and obtain the corresponding hybrid action k, xk. The double critic
networks Qθ1, Qθ2 take as input the latent action to approximate hybrid-action value function Qπω,
i.e., Qθi=1,2(s, e, zx) ≈Qπω(s, k, xk). With a buffer of collected transition sample (s, e, zx, r, s′),
the critics are trained by Clipped Double Q-Learning, with the loss function below for i = 1, 2:"
DRL WITH HYBRID ACTION REPRESENTATION,0.22040816326530613,"LCDQ(θi) = Es,e,zx,r,s′
h
(y −Qθi(s, e, zx))2i
,
where y = r + γ min
j=1,2 Q¯θj (s′, π¯ω(s′)) ,
(7)"
DRL WITH HYBRID ACTION REPRESENTATION,0.22244897959183674,"where ¯θj=1,2, ¯ω are the target network parameters. The actor (latent policy) is updated with Deter-
ministic Policy Gradient (Silver et al., 2014) as follows:"
DRL WITH HYBRID ACTION REPRESENTATION,0.22448979591836735,"∇ωJ(ω) = Es

∇πω(s)Qθ1(s, πω(s))∇ωπω(s)

.
(8)"
DRL WITH HYBRID ACTION REPRESENTATION,0.22653061224489796,"Algorithm 1 describes the pseudo-code of HyAR-TD3, containing two major stages: 1 warm-up
stage and 2 training stage. In the warm-up stage, the hybrid action representation models are
pre-trained using a prepared replay buffer D (line 4-6). The parameters the embedding table and
conditional VAE is updated by minimizing the VAE and dynamics prediction loss. Note that the
proposed algorithm has no requirement on how the buffer D is prepared and here we simply use
a random policy for the environment interaction and data generation by default. In the learning
stage, given a environment state, the latent policy outputs a latent action perturbed by a Gaussian
exploration noise, with some abuse of notions e, zx (line 10). The latent action is decoded into
original hybrid action so as to interact with the environment, after which the collected transition
sample is stored in the replay buffer (line 12-14). Then, the latent policy learning is preformed
using the data sampled from D (line 15-17). It is worth noting that the action representation model"
DRL WITH HYBRID ACTION REPRESENTATION,0.22857142857142856,Published as a conference paper at ICLR 2022
DRL WITH HYBRID ACTION REPRESENTATION,0.23061224489795917,"is updated concurrently in the training stage to make continual adjustment to the change of data
distribution (line 19-21)."
DRL WITH HYBRID ACTION REPRESENTATION,0.23265306122448978,Latent Space
DRL WITH HYBRID ACTION REPRESENTATION,0.23469387755102042,Outlier
DRL WITH HYBRID ACTION REPRESENTATION,0.23673469387755103,Boundary
DRL WITH HYBRID ACTION REPRESENTATION,0.23877551020408164,"(a) Representation unreliability
(b) Representation shift"
DRL WITH HYBRID ACTION REPRESENTATION,0.24081632653061225,"Before
After"
DRL WITH HYBRID ACTION REPRESENTATION,0.24285714285714285,"Figure 3: Illustrations of representation unreliability and
representation shift. Dots denote the hybrid action rep-
resentations selected by policy (red) and known by en-
coder (blue). Gray lines form the areas, among which
representations can be well decoded and estimated."
DRL WITH HYBRID ACTION REPRESENTATION,0.24489795918367346,"One signiﬁcant distinction of DRL with
HyAR described above compared with
conventional DRL is that, the hybrid ac-
tion representation space is learned from
ﬁnite samples that are drawn from a mov-
ing data distribution.
The induced un-
reliability and shift of learned represen-
tations can severely cripple the perfor-
mance of learned latent policy if they are
not carefully handled. Hence, we propose
two mechanisms to deal with the above
two considerations as detailed below."
DRL WITH HYBRID ACTION REPRESENTATION,0.24693877551020407,"Latent Space Constraint (LSC)
As the latent representation space is constructed by ﬁnite hybrid
action samples, some areas in the latent space can be highly unreliable in decoding as well as Q-
value estimation. Similar evidences are also founded in (Zhou et al., 2020; Notin et al., 2021). In
Fig. 3(a), the latent action representations inside the boundary can be well decoded and estimated the
values, while the outliers cannot. Once the latent policy outputs outliers, which can be common in
the early learning stage, the unreliability can quickly deteriorate the policy and lead to bad results.
Therefore, we propose to constrain the action representation space of the latent policy inside a
reasonable area adaptively. In speciﬁc, we re-scale each dimension of the output of latent policy
(i.e., [−1, 1]d1+d2 by tanh activation) to a bounded range [blower, bupper]. For a number of s, k, xk
collected previously, the bounds blower, bupper are obtained by calculating the c-percentage central
range where c ∈[0, 100]. We empirically demonstrate the importance of LSC. See more details in
Appendix A & C.3."
DRL WITH HYBRID ACTION REPRESENTATION,0.24897959183673468,"Representation Shift Correction (RSC)
As in Algorithm 1, the hybrid action representation
space is continuously optimized along with RL process. Thus, the representation distribution of
original hybrid actions in the latent space can shift after a certain learning interval (Igl et al., 2020).
Fig. 3(b) illustrates the shift (denoted by different shapes). This negatively inﬂuences the value
function learning since the outdated latent action representation no longer reﬂects the same transi-
tion at present. To handle this, we propose a representation relabeling mechanism. In speciﬁc, for
each mini-batch training in Eq.7, we check the semantic validity of hybrid action representations in
current representation space and relabel the invalid ones with the latest representations. In this way,
the policy learning is always performed on latest representations, so that the issue of representation
shift can be alleviated. Empirically evaluations demonstrate the superiority of relabeling techniques
in achieving a better performance with a lower variance. See more details in Appendix A & C.3."
EXPERIMENTS,0.2510204081632653,"5
EXPERIMENTS"
EXPERIMENTS,0.2530612244897959,"We evaluate HyAR in various hybrid action environments against representative prior algorithms.
Then, a detailed ablation study is conducted to verify the contribution of each component in HyAR.
Moreover, we provide visual analysis for better understandings of HyAR."
EXPERIMENT SETUPS,0.25510204081632654,"5.1
EXPERIMENT SETUPS"
EXPERIMENT SETUPS,0.2571428571428571,"Benchmarks
Fig. 4 visualizes the evaluation benchmarks, including the Platform and Goal from
(Masson et al., 2016), Catch Point from (Fan et al., 2019), and a newly designed Hard Move speciﬁc
to the evaluation in larger hybrid action space. We also build a complex version of Goal, called
Hard Goal. All benchmarks have hybrid actions and require the agent to select reasonable actions
to complete the task. See complete description of benchmarks in Appendix B.1."
EXPERIMENT SETUPS,0.25918367346938775,"Baselines
Four state-of-the-art approaches are selected as baselines: HPPO (Fan et al., 2019),
PDQN (Xiong et al., 2018), PADDPG (Hausknecht & Stone, 2016), HHQN (Fu et al., 2019). In
addition, for a comprehensive study, we extend the baselines which consists of DDPG to their TD3
variants, denoted by PDQN-TD3, PATD3, HHQN-TD3. Last, we use HyAR-DDPG and HyAR-"
EXPERIMENT SETUPS,0.2612244897959184,Published as a conference paper at ICLR 2022
EXPERIMENT SETUPS,0.26326530612244897,Target area agent
EXPERIMENT SETUPS,0.2653061224489796,"n  actuators 
   to choose"
EXPERIMENT SETUPS,0.2673469387755102,"（d）Hard Move
（a）Platform
（b）Goal"
EXPERIMENT SETUPS,0.2693877551020408,Valid catch distance
EXPERIMENT SETUPS,0.2714285714285714,Target point
EXPERIMENT SETUPS,0.27346938775510204,Current action: move agent
EXPERIMENT SETUPS,0.2755102040816326,（c）Catch Point
EXPERIMENT SETUPS,0.27755102040816326,"Figure 4: Benchmarks with discrete-continuous actions: (a) the agent selects a discrete action (run,
hop, leap) and the corresponding continuous parameter (horizontal displacement) to reach the goal;
(b) The agent selects a discrete strategy (move, shoot) and the continuous 2-D coordinate to score;
(c) The agent selects a discrete action (move, catch) and the continuous parameter (direction) to grab
the target point; (d) The agent has n equally spaced actuators. It can choose whether each actuator
should be on or off (thus 2n combination in total) and determine the corresponding continuous
parameter for each actuator (moving distance) to reach the target area."
EXPERIMENT SETUPS,0.2795918367346939,"ENV
HPPO
PADDPG
PDQN
HHQN
HyAR-DDPG
PATD3
PDQN-TD3
HHQN-TD3
HyAR-TD3"
EXPERIMENT SETUPS,0.2816326530612245,"PPO-based
DDPG-based
TD3-based"
EXPERIMENT SETUPS,0.2836734693877551,"Goal
0.0 ± 0.0
0.05 ± 0.10
0.70 ± 0.07
0.0±0.0
0.53±0.02
0.0±0.0
0.71±0.10
0.0±0.0
0.78±0.03
Hard Goal
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0±0.0
0.30±0.08
0.44±0.05
0.06±0.07
0.01±0.01
0.60±0.07
Platform
0.80 ± 0.02
0.36 ± 0.06
0.93 ± 0.05
0.46±0.25
0.87±0.06
0.94±0.10
0.93±0.03
0.62±0.23
0.98±0.01
Catch Point
0.69 ± 0.09
0.82 ± 0.06
0.77 ± 0.07
0.31±0.06
0.89±0.01
0.82±0.10
0.89±0.07
0.27±0.05
0.90±0.03
Hard Move (n = 4)
0.09 ± 0.02
0.03 ± 0.01
0.69 ± 0.07
0.39±0.14
0.91±0.03
0.66±0.13
0.85±0.10
0.52±0.17
0.93±0.02
Hard Move (n = 6)
0.05 ± 0.01
0.04 ± 0.01
0.41 ± 0.05
0.32±0.17
0.91±0.04
0.04±0.02
0.74±0.08
0.29±0.13
0.92±0.04
Hard Move (n = 8)
0.04 ± 0.01
0.06 ± 0.03
0.04 ± 0.01
0.05±0.02
0.85±0.06
0.06±0.02
0.05±0.01
0.05±0.02
0.89±0.03
Hard Move (n = 10)
0.05 ± 0.01
0.04 ± 0.01
0.06 ± 0.02
0.04±0.01
0.82±0.06
0.07±0.02
0.05±0.02
0.05±0.02
0.75±0.05"
EXPERIMENT SETUPS,0.2857142857142857,"Table 2: Comparisons of the baselines regarding the average performance at the end of training
process with the corresponding standard deviation over 5 runs. Values in bold indicate the best
results in each environment."
EXPERIMENT SETUPS,0.28775510204081634,"TD3 to denote our implementations of DRL with HyAR based on DDPG and TD3. For a fair
comparison, the network architecture (i.e., DDPG and TD3) used in associated baselines are the
same. For all experiments, we give each baseline the same training budget. For our algorithms, we
use a random strategy to interact with the environment for 5000 episodes during the warm-up stage.
For each experiment, we run 5 trials and report the average results. Complete details of setups are
provided in Appendix B."
PERFORMANCE EVALUATION,0.2897959183673469,"5.2
PERFORMANCE EVALUATION"
PERFORMANCE EVALUATION,0.29183673469387755,"To conduct a comprehensive comparison, all baselines implemented based on either DDPG or TD3
are reported. To counteract implementation bias, codes of PADDPG, PDQN, and HHQN are directly
adopted from prior works. Comparisons in terms of the averaged results are summarized in Tab. 2,
where bold numbers indicate the best result. Overall, we have the following ﬁndings. HyAR-TD3
and HyAR-DDPG show the better results and lower variance than the others. Moreover, the advan-
tage of HyAR is more obvious in environments in larger hybrid action space (e.g., Hard Goal & Hard
Move). Taking Hard Move for example, as the action space grows exponentially, the performance
of HyAR is steady and barely degrades, while the others deteriorate rapidly. Similar results can be
found in Goal and Hard Goal environments. This is due to the superiority of HyAR of utilizing the
hybrid action representation space, among which the latent policy can be learned based on compact
semantics. These results not only reveal the effectiveness of HyAR in achieving better performance,
but also the scalability and generalization."
PERFORMANCE EVALUATION,0.2938775510204082,"In almost all environments, HyAR outperforms other baselines for both the DDPG-based and TD3-
based cases. The exceptions are in Goal and Platform environments, where PDQN performs slightly
better than HyAR-DDPG. We hypothesize that this is because the hybrid action space of these two
environments is relatively small. For such environments, the learned latent action space could be
sparse and noisy, which in turn degrades the performance. One evidence is that the conservative
(underestimation) nature in TD3 could compensate and alleviates this issue, achieving signiﬁcant
improvements (HyAR-TD3 v.s. HyAR-DDPG). Fig. 5 renders the learning curves, where HyAR-
TD3 outperforms other baselines in both the ﬁnal performance and learning speed across all environ-
ments. Similar results are observed in DDPG-based comparisons and can be found in Appendix C.1.
In addition, HyAR-TD3 shows good generalization across environments, while the others more or"
PERFORMANCE EVALUATION,0.29591836734693877,Published as a conference paper at ICLR 2022
PERFORMANCE EVALUATION,0.2979591836734694,"(a) Goal
(b) Hard Goal
(c) Platform
(d) Catch Point"
PERFORMANCE EVALUATION,0.3,"(e) Hard Move (n=4)
(f) Hard Move (n=6)
(g) Hard Move (n=8)
(h) Hard Move (n=10)"
PERFORMANCE EVALUATION,0.3020408163265306,Average Episodic Reward
PERFORMANCE EVALUATION,0.3040816326530612,"HPPO
PDQN-TD3
PATD3
HHQN-TD3
HyAR-TD3 (Ours)"
PERFORMANCE EVALUATION,0.30612244897959184,"Figure 5: Comparisons of algorithms in different environments. The x- and y-axis denote the envi-
ronment steps (×105) and average episode reward over recent 100 episodes. The curve and shade
denote the mean and a standard deviation over 5 runs."
PERFORMANCE EVALUATION,0.3081632653061224,"less fail in some environments (e.g., HPPO, PATD3, and HHQN-TD3 fail in Fig. 5(a) and PDQN-
TD3 fails in Fig. 5(b)). Moreover, when environments become complex (Fig. 5(e-h)), HyAR-TD3
still achieves steady and better performance, particularly demonstrating the effectiveness of HyAR
in high-dimensional hybrid action space."
ABLATION STUDY AND VISUAL ANALYSIS,0.31020408163265306,"5.3
ABLATION STUDY AND VISUAL ANALYSIS"
ABLATION STUDY AND VISUAL ANALYSIS,0.3122448979591837,"We further evaluate the contribution of the major components in HyAR: the two mechanisms for
latent policy learning, i.e., latent space constraint (LSC) and representation shift correction (RSC),
and the dynamics predictive representation loss. We brieﬂy conclude our results as follows. For
LSC, properly constraining the output space of the latent policy is critical to performance; otherwise,
both loose and conservative constraints dramatically lead to performance degradation. RSC and
dynamics predictive representation loss show similar efﬁcacy: they improve both learning speed
and convergence results, additionally with a lower variance. Such superiority is more signiﬁcant in
the environment when hybrid actions are more semantically different (e.g., Goal). We also conduct
ablation studies on other factors along with hyperparameter analysis. See complete details and
ablation results in Appendix C.2 & C.3. t-SNE 100 50 0 50 100"
ABLATION STUDY AND VISUAL ANALYSIS,0.3142857142857143,(a) Goal t-SNE 100 50 0 50 100
ABLATION STUDY AND VISUAL ANALYSIS,0.3163265306122449,(b) Hard Move (n = 8)
ABLATION STUDY AND VISUAL ANALYSIS,0.3183673469387755,"Figure 6: 2D t-SNE visualizations of learned represen-
tation for original hybrid actions, colored by 1D t-SNE
of the corresponding environmental impact."
ABLATION STUDY AND VISUAL ANALYSIS,0.32040816326530613,"Finally, we adopt t-SNE (Maaten & Hin-
ton, 2008) to visualize the learned hybrid
action representations, i.e., (e, zx), in a 2D
plane. We color each action based on its
impact on the environment i.e., ˜δs,s′. As
shown in Fig. 6, we observe that actions
with a similar impact on the environment
are relatively closer in the latent space.
This demonstrates the dynamics predic-
tive representation loss is helpful for deriv-
ing dynamics-aware representation for fur-
ther improving the learning performance,
efﬁcacy, and stability (see results in Ap-
pendix C.2 & C.4)"
CONCLUSION,0.3224489795918367,"6
CONCLUSION"
CONCLUSION,0.32448979591836735,"In this paper, we propose Hybrid Action Representation (HyAR) for DRL agents to efﬁciently learn
with discrete-continuous hybrid action space. HyAR use an unsupervised method to derive a com-
pact and decodable representation space for discrete-continuous hybrid actions. HyAR can be easily
extended with modern DRL methods to leverage additional advantages. Our experiments demon-
strate the superiority of HyAR regarding performance, learning speed and robustness in most hybrid
action environment, especially in high-dimensional action spaces."
CONCLUSION,0.32653061224489793,Published as a conference paper at ICLR 2022
CONCLUSION,0.32857142857142857,ACKNOWLEDGMENTS
CONCLUSION,0.3306122448979592,"The work is supported by the National Science Fund for Distinguished Young Scholars (Grant No.:
62025602), the National Natural Science Foundation of China (Grant Nos.: 11931015, 62106172),
the XPLORER PRIZE, and the New Generation of Artiﬁcial Intelligence Science and Technology
Major Project of Tianjin (Grant No.: 19ZXZNGX00010)."
REFERENCES,0.3326530612244898,REFERENCES
REFERENCES,0.3346938775510204,"M. Azabou, M. G. Azar, R. Liu, C. H. Lin, E. C. Johnson, K. B. Nair, M. D., K. B. Hengen, W. G.
Roncal, M. V., and E. Dyer. Mine your own view: Self-supervised learning through across-sample
prediction. CoRR, abs/2102.10106, 2021."
REFERENCES,0.336734693877551,"C. J. Bester, S. D. James, and G. D. Konidaris.
Multi-pass q-networks for deep reinforcement
learning with parameterised action spaces. CoRR, abs/1905.04388, 2019."
REFERENCES,0.33877551020408164,"Y. Chandak, G. Theocharous, J. Kostas, S. M. Jordan, and P. S. Thomas. Learning action represen-
tations for reinforcement learning. In ICML, volume 97, pp. 941–950, 2019."
REFERENCES,0.3408163265306122,"O. Delalleau, M. Peter, E. Alonso, and A. Logut. Discrete and continuous action representation for
practical RL in video games. CoRR, abs/1912.11077, 2019."
REFERENCES,0.34285714285714286,"A. Erraqabi, M. Zhao, M. C. Machado, Y. Bengio, S. Sukhbaatar, L. Denoyer, and A. Lazaric.
Exploration-driven representation learning in reinforcement learning. In Unsupervised Reinforce-
ment Learning workshop on ICML, 2021."
REFERENCES,0.3448979591836735,"Z. Fan, R. Su, W. Zhang, and Y. Yu. Hybrid actor-critic reinforcement learning in parameterized
action space. In IJCAI, pp. 2279–2285, 2019."
REFERENCES,0.3469387755102041,"H. Fu, H. Tang, J. Hao, Z. Lei, Y. Chen, and C. Fan. Deep multi-agent reinforcement learning with
discrete-continuous hybrid action spaces. In IJCAI, pp. 2329–2335, 2019."
REFERENCES,0.3489795918367347,"S. Fujimoto, H. v. Hoof, and D. Meger. Addressing function approximation error in actor-critic
methods. In ICML, volume 80, pp. 1582–1591, 2018."
REFERENCES,0.3510204081632653,"A. Grosnit, R. Tutunov, A. Maraval, R. Grifﬁths, A. Cowen-Rivers, L. Yang, L. Zhu, W. Lyu,
Z. Chen, J. Wang, J. Peters, and H. Bou-Ammar. High-dimensional bayesian optimisation with
variational autoencoders and deep metric learning. CoRR, abs/2106.03609, 2021."
REFERENCES,0.35306122448979593,"X. Hao, W. Wang, H. Mao, Y. Yang, D. Li, Y. Zheng, Z. Wang, and J. Hao. Api: Boosting multi-
agent reinforcement learning via agent-permutation-invariant networks. CoRR, abs/2203.05285,
2022."
REFERENCES,0.3551020408163265,"M. Hausknecht and P. Stone. Deep reinforcement learning in parameterized action space. ICLR,
2016."
REFERENCES,0.35714285714285715,"M. Igl, G. Farquhar, J. Luketina, W. Boehmer, and S. Whiteson. The impact of non-stationarity on
generalisation in deep reinforcement learning. CoRR, abs/2006.05826, 2020."
REFERENCES,0.35918367346938773,"D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014."
REFERENCES,0.36122448979591837,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."
REFERENCES,0.363265306122449,"T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Contin-
uous control with deep reinforcement learning. In ICLR, 2015."
REFERENCES,0.3653061224489796,"J. Ma, S. Yao, G. Chen, J. Song, and J. Ji. Distributed reinforcement learning with self-play in
parameterized action space. In 2021 IEEE International Conference on Systems, pp. 1178–1185,
2021."
REFERENCES,0.3673469387755102,"L. V. D. Maaten and G. E. Hinton. Visualizing data using t-SNE. Journal of Machine Learning
Research, 9:2579–2605, 2008."
REFERENCES,0.3693877551020408,Published as a conference paper at ICLR 2022
REFERENCES,0.37142857142857144,"S. Massaroli, M. Poli, S. Bakhtiyarov, A. Yamashita, H. Asama, and J. Park.
Neural ordinary
differential equation value networks for parametrized action spaces. In ICLR 2020 Workshop on
Integration of Deep Neural Models and Differential Equations, 2020."
REFERENCES,0.373469387755102,"W. Masson, P. Ranchod, and G. D. Konidaris. Reinforcement learning with parameterized actions.
In AAAI, pp. 1934–1940, 2016."
REFERENCES,0.37551020408163266,"V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A.
Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforce-
ment learning. Nature, 518(7540):529–533, 2015."
REFERENCES,0.37755102040816324,"M. Neunert, A. Abdolmaleki, M. Wulfmeier, T. Lampe, J. T. Springenberg, R. Hafner, F. Romano,
J. Buchli, N. Heess, and M. A. Riedmiller. Continuous-discrete reinforcement learning for hybrid
control in robotics. In CoRL, volume 100, pp. 735–751, 2019."
REFERENCES,0.3795918367346939,"P. Notin, J. M. Hern´andez-Lobato, and Y. Gal. Improving black-box optimization in VAE latent
space using decoder uncertainty. CoRR, abs/2107.00096, 2021."
REFERENCES,0.3816326530612245,"L. Peng and Y. Tsuruoka. Improving action branching for deep reinforcement learning with a multi-
dimensional hybrid action space. In The 24th Game Programming Workshop, 2019."
REFERENCES,0.3836734693877551,"J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimization. In
ICML, pp. 1889–1897, 2015."
REFERENCES,0.38571428571428573,"J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-dimensional continuous control
using generalized advantage estimation. In ICLR, 2016."
REFERENCES,0.3877551020408163,"M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. C. Courville, and P. Bachman. Data-efﬁcient
reinforcement learning with momentum predictive representations. CoRR, abs/2007.05929, 2020."
REFERENCES,0.38979591836734695,"R. Shen, Y. Zheng, J. Hao, Z. Meng, Y. Chen, C. Fan, and Y. Liu. Generating behavior-diverse game
ais with evolutionary multi-objective deep reinforcement learning. In IJCAI, pp. 3371–3377,
2020."
REFERENCES,0.39183673469387753,"D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A. Riedmiller. Deterministic policy
gradient algorithms. In ICML, pp. 387–395, 2014."
REFERENCES,0.39387755102040817,"D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever,
T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of
go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016."
REFERENCES,0.39591836734693875,"A. Stooke, K. Lee, P. Abbeel, and M. Laskin. Decoupling representation learning from reinforce-
ment learning. In ICML, volume 139, pp. 9870–9879, 2021."
REFERENCES,0.3979591836734694,"J. Sun, Y. Zheng, J. Hao, Z. Meng, and Y. Liu. Continuous multiagent control using collective
behavior entropy for large-scale home energy management. In AAAI, pp. 922–929, 2020."
REFERENCES,0.4,"H. Tang, Z. Meng, G. Chen, P. Chen, C. Chen, Y. Yang, L. Zhang, W. Liu, and J. Hao. Foresee then
evaluate: Decomposing value estimation with latent future prediction. In AAAI, pp. 9834–9842,
2021."
REFERENCES,0.4020408163265306,"R. Wang, R. Yu, B. An, and Z. Rabinovich. I2hrl: Interactive inﬂuence-based hierarchical reinforce-
ment learning. In IJCAI, pp. 3131–3138, 2020."
REFERENCES,0.40408163265306124,"E. Wei, D. Wicke, and S. Luke. Hierarchical approaches for reinforcement learning in parameterized
action space. In 2018 AAAI Spring Symposia, 2018."
REFERENCES,0.4061224489795918,"W. F. Whitney, R. Agarwal, K. Cho, and A. Gupta. Dynamics-aware embeddings. In ICLR, 2020."
REFERENCES,0.40816326530612246,"J. Xiong, Q. Wang, Z. Yang, P. Sun, L. Han, Y. Zheng, H. Fu, T. Zhang, J. Liu, and H. Liu.
Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid
action space. CoRR, abs/1810.06394, 2018."
REFERENCES,0.41020408163265304,Published as a conference paper at ICLR 2022
REFERENCES,0.4122448979591837,"D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Reinforcement learning with prototypical represen-
tations. In ICML, volume 139, pp. 11920–11931, 2021."
REFERENCES,0.4142857142857143,"Y. Zheng, Z. Meng, J. Hao, Z. Zhang, T. Yang, and C. Fan. A deep bayesian policy reuse approach
against non-stationary agents. In NeurIPS, pp. 962–972, 2018."
REFERENCES,0.4163265306122449,"Y. Zheng, X. Xie, T. Su, L. Ma, J. Hao, Z. Meng, Y. Liu, R. Shen, Y. Chen, and C. Fan. Wuji:
Automatic online combat game testing using evolutionary deep reinforcement learning. In ASE,
pp. 772–784, 2019."
REFERENCES,0.41836734693877553,"Y. Zheng, J. Hao, Z. Zhang, Z. Meng, T. Yang, Y. Li, and C. Fan. Efﬁcient policy detecting and
reusing for non-stationarity in markov games. AAMAS, 35(1):1–29, 2021a."
REFERENCES,0.4204081632653061,"Y. Zheng, Y. Liu, X. Xie, Y. Liu, L. Ma, J. Hao, and Y. Liu. Automatic web testing using curiosity-
driven reinforcement learning. In ICSE, pp. 423–435, 2021b."
REFERENCES,0.42244897959183675,"Y. Zheng, Z. Yan, K. Chen, J. Sun, Y. Xu, and Y. Liu. Vulnerability assessment of deep reinforcement
learning models for power system topology optimization. IEEE Transactions on Smart Grid, 12
(4):3613–3623, 2021c."
REFERENCES,0.42448979591836733,"W. Zhou, S. Bajracharya, and D. Held. PLAS: latent action space for ofﬂine reinforcement learning.
CoRR, abs/2011.07213, 2020."
REFERENCES,0.42653061224489797,Published as a conference paper at ICLR 2022
REFERENCES,0.42857142857142855,"A
DETAILED FOR LATENT SPACE CONSTRAINT (LSC) AND
REPRESENTATION SHIFT CORRECTION(RSC)"
REFERENCES,0.4306122448979592,"Latent Space Constraint (LSC)
As we can see in Fig. 3(a), the latent action representations inside
the boundary can be well decoded and estimated the values, while the outliers cannot. Therefore, the
most critical problem for latent space constraint (LSC) is to ﬁnd a reasonable latent space boundary.
Simply re-scale policy’s outputs in a ﬁxed bounded area [−b, b] could lose some important infor-
mation and make the latent space unstable (Zhou et al., 2020; Notin et al., 2021) . We propose a
mechanism to constrain the action representation space of the latent policy inside a reasonable area
adaptively. In speciﬁc, we re-scale each dimension of the output of latent policy (i.e., [−1, 1]d1+d2
by tanh activation) to a bounded range [blower, bupper]. At intervals (actually concurrent with the up-
dates of the hybrid action representation models), we ﬁrst sample M transitions s, k, xk from buffer,
then we obtain the corresponding latent action representations with current representation models.
In this way, we will get M different latent variable values in each dimension. We sort the latent
variable of each dimension and calculate the c-percentage central range, i.e., let the c"
QUANTILE AND,0.4326530612244898,"2 quantile and
1 −c"
QUANTILE AND,0.4346938775510204,"2 quantile of the range to be blower and bupper of the current latent variable. We called c as latent
select range where c ∈[0, 100]. With the decrease of c, the constrained latent action representation
space becomes smaller. The experiment on the value of latent select range c is in Appendix C.3."
QUANTILE AND,0.43673469387755104,"Representation Shift Correction (RSC)
Since the hybrid action representation space is continu-
ously optimized along with the RL learning, the representation distribution of original hybrid actions
in the latent space can shift after a certain learning interval (Igl et al., 2020). Fig. 3(b) visualizes
the shifting (denoted by different shapes). This negatively inﬂuences the value function learning
since the outdated latent action representation no longer reﬂects the same transition at present. To
handle this, we propose a representation relabeling mechanism. In speciﬁc, we feed the batch of
stored original hybrid actions to our representation models to obtain the latest latent representations,
for each mini-batch training in Eq.7. For latent discrete action e, if it can not be mapped to the
corresponding original action k in the latest embedding table, we will relabel e through looking up
the table with stored original discrete action k, i.e., e ←eζ,k + N(0, 0.1). The purpose of adding
noise N(0, 0.1) is to ensure the diversity of the relabeled action representations. For latent contin-
uous action zx, we ﬁrst obtain ˜δs,s′ through the latest decoder pψ(zx, s, eζ,k). Then we verify if
∥˜δs,s′ −δs,s′∥2
2 > δ0 (threshold value δ0 is set to be 4 ∗ˆLDyn, where ˆLDyn is the moving empiri-
cal loss), i.e., the case that indicates that the historical representations has no longer semantically
consistent (with respect to environmental dynamics) under current representation models. Then zx
will be relabeled by the latest latent representations z ∼qφ(· | xk, s, eζ,k). In this way, the policy
learning is always performed on latest representations, so that the issue of representation distribution
shift can be effectively alleviated. The experiment on relabeling techniques is in Appendix C.3."
QUANTILE AND,0.4387755102040816,"Limitation and Potential Improvements of LSC and RSC
The ideas of LSC and RSC are gen-
eral. In our work, both LSC and RSC are easy to implement and empirically demonstrated to be
effective. We also consider that there remains some room to further improve these two mechanisms
in some more challenging cases. For LSC, one potential drawback may be that the central range
constraint on per dimension adopted in our work, is suboptimal to the case if the distribution of
learned action representation of original hybrid actions is multi-modal. In such cases, the central
range constraint may be less effective and can be further improved. For RSC, it requires additional
memory and computation cost. However, in our experiments, such memory and computation cost
is relatively negligible. The dimensionality of each original hybrid action sample is relatively small
compared to the state sample stored; the additional wall-clock time cost of HyAR (with RSC) to
HyAR without RSC is about 6% (tested in Goal)."
QUANTILE AND,0.44081632653061226,"B
EXPERIMENTAL DETAILS"
QUANTILE AND,0.44285714285714284,"B.1
SETUPS"
QUANTILE AND,0.4448979591836735,"Our codes are implemented with Python 3.7.9 and Torch 1.7.1. All experiments were run on a
single NVIDIA GeForce GTX 2080Ti GPU. Each single training trial ranges from 4 hours to 10
hours, depending on the algorithms and environments. For more details of our code can refer to the
HyAR.zip in the supplementary results."
QUANTILE AND,0.44693877551020406,Published as a conference paper at ICLR 2022
QUANTILE AND,0.4489795918367347,"Layer
Actor Network (π(s))
Critic Network (Q(s, a) or V (s))"
QUANTILE AND,0.45102040816326533,"Fully Connected
(state dim, 256)
(state dim + RK+P"
QUANTILE AND,0.4530612244897959,"k |Xk|, 256) or
(state dim + R
P"
QUANTILE AND,0.45510204081632655,"k |Xk| , 256) or (state dim, 256)
Activation
ReLU
ReLU"
QUANTILE AND,0.45714285714285713,"Fully Connected
(256, 256)
(256, 256)
Activation
ReLU
ReLU"
QUANTILE AND,0.45918367346938777,"Fully Connected
(256, RK ) and (256, R
P"
QUANTILE AND,0.46122448979591835,"k |Xk| )
(256, 1)
or (256, R
P"
QUANTILE AND,0.463265306122449,"k |Xk|)
or (256, RK )
Activation
Tanh
None"
QUANTILE AND,0.46530612244897956,Table 3: Network structures for the actor network and the critic network (Q-network or V -network).
QUANTILE AND,0.4673469387755102,"Benchmark Environments We conduct our experiments on several hybrid action environments and
detailed experiment description is below."
QUANTILE AND,0.46938775510204084,"• Platform (Masson et al., 2016): The agent need to reach the ﬁnal goal while avoiding the
enemy or falling into the gap. The agent need to select the discrete action (run, hop, leap)
and determine the corresponding continuous action (horizontal displacement) simultane-
ously to complete the task. The horizon of an episode is 20."
QUANTILE AND,0.4714285714285714,"• Goal (Masson et al., 2016): The agent shoots the ball into the gate to win. Three types
of hybrid actions are available to the agent including kick-to(x,y), shoot-goal-left(h), shoot-
goal-right(h). The continuous action parameters position (x, y) and position (h) along the
goal line are quit different. Furthermore, We built a complex version of the goal environ-
ment, called Hard Goal. We redeﬁned the shot-goal action and split it into ten parameter-
ized actions by dividing the goal line equidistantly. The continuous action parameters of
each shot action will be mapped to a region in the goal line. The horizon of an episode is
50."
QUANTILE AND,0.47346938775510206,"• Catch Point (Fan et al., 2019): The agent should catch the target point (orange) in limited
opportunity (10 chances). There are two hybrid actions move and catch. Move is param-
eterized by a continuous action value which is a directional variable and catch is to try to
catch the target point. The horizon of an episode is 20."
QUANTILE AND,0.47551020408163264,"• Hard Move (designed by us): The agent needs to control n equally spaced actuators to
reach target area (orange). Agent can choose whether each actuator should be on or off.
Thus, the size of the action set is exponential in the number of actuators that is 2n. Each
actuator controls the moving distance in its own direction. n controls the scale of the action
space. As n increases, the dimension of the action will increase. The horizon of an episode
is 25."
QUANTILE AND,0.4775510204081633,"B.2
NETWORK STRUCTURE"
QUANTILE AND,0.47959183673469385,"Our
PATD3
is
implemented
with
reference
to
github.com/sfujim/TD3
(TD3
source-code).
PADDPG
and
PDQN
are
implemented
with
reference
to
https://github.com/cycraig/MP-DQN. For a fair comparison, all the baseline methods
have the same network structure (except for the speciﬁc components to each algorithm) as our
HyAR-TD3 implementation. For PDQN, PADDPG, we introduce a Passthrough Layer (Masson
et al., 2016) to the actor networks to initialise their action-parameter policies to the same linear
combination of state variables.
HPPO paper does not provide open source-code and thus we
implemented it by ourselves according to the guidance provided in their paper. For HPPO, the
discrete actor and continuous actor do not share parameters (better than share parameters in our
experiments)."
QUANTILE AND,0.4816326530612245,"As shown in Tab. 3, we use a two-layer feed-forward neural network of 256 and 256 hidden units
with ReLU activation (except for the output layer) for the actor network for all algorithms. For
PADDPG, PDQN and HHQN, the critic denotes the Q-network. For HPPO, the critic denotes the
V -network. Some algorithms (PATD3, PADDPG, HHQN) output two heads at the last layer of the
actor network, one for discrete action and another for continuous action parameters."
QUANTILE AND,0.48367346938775513,Published as a conference paper at ICLR 2022
QUANTILE AND,0.4857142857142857,"The structure of HyAR is shown in Tab. 4. We introduced element-wise product operation (Tang
et al., 2021) and cascaded head structure (Azabou et al., 2021) to our HyAR model. More details
about their effects are in Appendix C.3."
QUANTILE AND,0.48775510204081635,"Model Component
Layer (Name)
Structure"
QUANTILE AND,0.4897959183673469,"Discrete Action Embedding Table Eζ
Parameterized Table
(Rd1, RK)"
QUANTILE AND,0.49183673469387756,"Conditional Encoder Network
Fully Connected (encoding)
(RXk, 256)
qφ (z | xk, s, eζ,k)
Fully Connected (condition)
(state dim + Rd1, 256)
Element-wise Product
ReLU (encoding) · ReLU(condition)
Fully Connected
(256, 256)
Activation
ReLU
Fully Connected (mean)
(256, Rd2)
Activation
None
Fully Connected (log std)
(256, Rd2)
Activation
None"
QUANTILE AND,0.49387755102040815,"Conditional Decoder & Prediction Network
Fully Connected (latent)
(Rd2 , 256)
pψ(xk, ˜δs,s′ | zk, s, eζ,k)
Fully Connected (condition)
(state dim + Rd1, 256)
Element-wise Product
ReLU(decoding) · ReLU(condition)
Fully Connected
(256, 256)
Activation
ReLU
Fully Connected (reconstruction)
(256, RXk)
Activation
None
Fully Connected
(256, 256)
Activation
ReLU
Fully Connected (prediction)
(256, state dim)
Activation
None"
QUANTILE AND,0.4959183673469388,"Table 4: Network structures for the hybrid action representation (HyAR) including, the discrete
action embedding table and the conditional VAE."
QUANTILE AND,0.49795918367346936,"B.3
HYPERPARAMETER"
QUANTILE AND,0.5,"For all our experiments, we use the raw state and reward from the environment and no normalization
or scaling are used. No regularization is used for the actor and the critic in all algorithms. An
exploration noise sampled from N(0, 0.1) (Fujimoto et al., 2018) is added to all baseline methods
when select action. The discounted factor is 0.99 and we use Adam Optimizer (Kingma & Ba,
2015) for all algorithms. Tab. 5 shows the common hyperparamters of algorithms used in all our
experiments."
QUANTILE AND,0.5020408163265306,"Hyperparameter
HPPO
PADDPG
PDQN
HHQN
PATD3
PDQN-TD3
HHQN-TD3
HyAR-DDPG
HyAR-TD3"
QUANTILE AND,0.5040816326530613,"Actor Learning Rate
1·10−4
1·10−4
1·10−4
1·10−4
3·10−4
3·10−4
3·10−4
1·10−4
3·10−4"
QUANTILE AND,0.5061224489795918,"Critic Learning Rate
1·10−3
1·10−3
1·10−3
1·10−3
3·10−4
3·10−4
3·10−4
1·10−3
3·10−4"
QUANTILE AND,0.5081632653061224,"Representation Model Learning Rate
-
-
-
-
-
-
-
1·10−4
1·10−4"
QUANTILE AND,0.5102040816326531,"Discount Factor
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
Optimizer
Adam
Adam
Adam
Adam
Adam
Adam
Adam
Adam
Adam
Target Actor Update Rate
-
1·10−3
1·10−3
1·10−3
5·10−3
5·10−3
5·10−3
1·10−3
5·10−3"
QUANTILE AND,0.5122448979591837,"Target Critic Update Rate
-
1·10−2
1·10−2
1·10−2
5·10−3
5·10−3
5·10−3
5·10−3
5·10−3
Exploration Policy
N(0, 0.1)
N(0, 0.1)
N(0, 0.1)
N(0, 0.1)
N(0, 0.1)
N(0, 0.1)
N(0, 0.1)
N(0, 0.1)
N(0, 0.1)
Batch Size
128
128
128
128
128
128
128
128
128
Buffer Size
105
105
105
105
105
105
105
105
105
Actor Epoch
2
-
-
-
-
-
-
-
-
Critic Epoch
10
-
-
-
-
-
-
-
-"
QUANTILE AND,0.5142857142857142,"Table 5: A comparison of common hyperparameter choices of algorithms.We use ‘-’ to denote the
‘not applicable’ situation."
QUANTILE AND,0.5163265306122449,"B.4
ADDITIONAL IMPLEMENTATION DETAILS"
QUANTILE AND,0.5183673469387755,"Training setup:
For HPPO, the actor network and the critic network are updated every 2 and 10
episodes respectively for all environment. The clip range of HPPO algorithm is set to 0.2 and we use
GAE (Schulman et al., 2016) for stable policy gradient. For DDPG-based, the actor network and the
critic network is updated every 1 environment step. For TD3-based, the critic network is updated
every 1 environment step and the actor network is updated every 2 environment step."
QUANTILE AND,0.5204081632653061,"The discrete action embedding table is initialized randomly by drawing each dimension from the
uniform distribution U(−1, 1) before representation pre-training. The latent action dim (discrete or"
QUANTILE AND,0.5224489795918368,Published as a conference paper at ICLR 2022
QUANTILE AND,0.5244897959183673,Algorithm 2: HyAR-DDPG
QUANTILE AND,0.5265306122448979,"1 Initialize actor πω and critic networks Qθ with random parameters ω, θ, and the corresponding target
network parameters ¯ω, ¯θ"
QUANTILE AND,0.5285714285714286,"2 Initialize discrete action embedding table Eζ and conditional VAE qφ, pψ with random parameters ζ, φ, ψ"
PREPARE REPLAY BUFFER D,0.5306122448979592,3 Prepare replay buffer D
PREPARE REPLAY BUFFER D,0.5326530612244897,4 repeat Stage 1
PREPARE REPLAY BUFFER D,0.5346938775510204,"5
Update ζ and φ, ψ using samples in D
▷see Eq. 6"
PREPARE REPLAY BUFFER D,0.536734693877551,6 until reaching maximum warm-up training times;
PREPARE REPLAY BUFFER D,0.5387755102040817,7 repeat Stage 2
PREPARE REPLAY BUFFER D,0.5408163265306123,"8
for t ←1 to T do"
PREPARE REPLAY BUFFER D,0.5428571428571428,"9
// select latent actions in representation space"
PREPARE REPLAY BUFFER D,0.5448979591836735,"10
e, zx = πω(s) + ϵe, with ϵe ∼N(0, σ)"
PREPARE REPLAY BUFFER D,0.5469387755102041,"11
// decode into original hybrid actions"
PREPARE REPLAY BUFFER D,0.5489795918367347,"12
k = gE(e), xk = pψ(zx, s, eζ,k)
▷see Eq. 3"
PREPARE REPLAY BUFFER D,0.5510204081632653,"13
Execute (k, xk), observe rt and new state s′"
PREPARE REPLAY BUFFER D,0.5530612244897959,"14
Store {s, k, xk, e, zx, r, s′} in D"
SAMPLE A MINI-BATCH B OF N EXPERIENCE FROM D,0.5551020408163265,"15
Sample a mini-batch B of N experience from D"
SAMPLE A MINI-BATCH B OF N EXPERIENCE FROM D,0.5571428571428572,"16
Update critic by minimizing empirical loss ˆLQ(θ) = N −1 P"
SAMPLE A MINI-BATCH B OF N EXPERIENCE FROM D,0.5591836734693878,"B (y −Qθ(s, e, zx))2, where
y = r + γQ¯θ (s′, π¯ω(s′))"
UPDATE ACTOR BY THE DETERMINISTIC POLICY GRADIENT,0.5612244897959183,"17
Update actor by the deterministic policy gradient"
UPDATE ACTOR BY THE DETERMINISTIC POLICY GRADIENT,0.563265306122449,"18
∇ωJ(ω) = N −1 P"
UPDATE ACTOR BY THE DETERMINISTIC POLICY GRADIENT,0.5653061224489796,"s∈B

∇πω(s)Qθ(s, πω(s))∇ωπω(s)

."
REPEAT,0.5673469387755102,"19
repeat"
REPEAT,0.5693877551020409,"20
Update ζ and φ, ψ using samples in D
▷see Eq. 6"
REPEAT,0.5714285714285714,"21
until reaching maximum representation training times;"
REPEAT,0.573469387755102,22 until reaching maximum total environment steps;
REPEAT,0.5755102040816327,"continuous latent action) default value is 6. We set the KL weight in representation loss LVAE as 0.5
and dynamics predictive representation loss weight β as 10 (default). More details about dynamics
predictive representation loss weight are in Appendix C.2."
REPEAT,0.5775510204081633,"For the warm-up stage, we run 5000 episodes (please refer to Tab. 7 for the corresponding environ-
ment steps in different environments) for experience collection. We then pre-train the hybrid action
representation model (discrete action embedding table and conditional VAE) for 5000 batches with
batch size 64, after which we start the training of the latent policy. The representation models (the
embedding table and conditional VAE) are trained every 10 episodes for 1 batches with batch size 64
for the rest of RL training. See Appendix D for different choices of warm-up training and subsequent
training of the hybrid action representation."
REPEAT,0.5795918367346938,"B.5
DDPG-BASED HYAR ALGORITHM"
REPEAT,0.5816326530612245,"Additionally, we implemented HyAR with DDPG (Lillicrap et al., 2015), called HyAR-DDPG. The
pseudo-code of complete algorithm is shown in Algorithm 2. Results of DDPG-based experimental
comparisons can be found in Appendix C.1."
REPEAT,0.5836734693877551,"C
COMPLETE LEARNING CURVES AND ADDITIONAL EXPERIMENTS"
REPEAT,0.5857142857142857,"C.1
LEARNING CURVES FOR DDPG-BASED COMPARISONS"
REPEAT,0.5877551020408164,"Fig. 7 visualizes the learning curves of DDPG-based comparisons, where HyAR-DDPG outperforms
other baselines in both the ﬁnal performance and learning speed in most environments. Besides the
learning speed, HyAR-DDPG also achieves the best generalization as HyAR-TD3 across different
environments. When the environments become complex (shown in Fig. 7(e-h)), HyAR-DDPG still
achieves steady and better performance than the others, particularly demonstrating the effectiveness
and generalization of HyAR in high-dimensional hybrid action spaces."
REPEAT,0.5897959183673469,Published as a conference paper at ICLR 2022
REPEAT,0.5918367346938775,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 0.0 0.2 0.4 0.6 0.8"
REPEAT,0.5938775510204082,Avg Episode Reward
REPEAT,0.5959183673469388,"HPPO
PDQN
PADDPG
HHQN
HyAR-DDPG"
REPEAT,0.5979591836734693,(a) Goal
REPEAT,0.6,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 25 20 15 10 5 0 5 10 15"
REPEAT,0.6020408163265306,Avg Episode Reward
REPEAT,0.6040816326530613,"HPPO
PDQN
PADDPG
HHQN
HyAR-DDPG"
REPEAT,0.6061224489795919,(b) Hard Goal
REPEAT,0.6081632653061224,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Time Steps (1e5) 0.0 0.2 0.4 0.6 0.8 1.0"
REPEAT,0.610204081632653,Avg Episode Reward
REPEAT,0.6122448979591837,"HPPO
PDQN
PADDPG
HHQN
HyAR-DDPG"
REPEAT,0.6142857142857143,(c) Platform
REPEAT,0.6163265306122448,"0
2
4
6
8
10
Time Steps (1e5) 30 20 10 0 10"
REPEAT,0.6183673469387755,Avg Episode Reward
REPEAT,0.6204081632653061,"HPPO
PDQN
PADDPG
HHQN
HyAR-DDPG"
REPEAT,0.6224489795918368,(d) Catch Point
REPEAT,0.6244897959183674,"0
2
4
6
8
10
Time Steps (1e5) 80 60 40 20 0"
REPEAT,0.6265306122448979,Avg Episode Reward
REPEAT,0.6285714285714286,"HPPO
PDQN
PADDPG
HHQN
HyAR-DDPG"
REPEAT,0.6306122448979592,(e) Hard Move (n = 4)
REPEAT,0.6326530612244898,"0
2
4
6
8
10
Time Steps (1e5) 40 30 20 10 0 10"
REPEAT,0.6346938775510204,Avg Episode Reward
REPEAT,0.636734693877551,"HPPO
PDQN
PADDPG
HHQN
HyAR-DDPG"
REPEAT,0.6387755102040816,(f) Hard Move (n = 6)
REPEAT,0.6408163265306123,"0
2
4
6
8
10
Time Steps (1e5) 60 50 40 30 20 10 0"
REPEAT,0.6428571428571429,Avg Episode Reward
REPEAT,0.6448979591836734,"HPPO
PDQN
PADDPG
HHQN
HyAR-DDPG"
REPEAT,0.6469387755102041,(g) Hard Move (n = 8)
REPEAT,0.6489795918367347,"0
2
4
6
8
10
Time Steps (1e5) 50 40 30 20 10 0"
REPEAT,0.6510204081632653,Avg Episode Reward
REPEAT,0.6530612244897959,"HPPO
PDQN
PADDPG
HHQN
HyAR-DDPG"
REPEAT,0.6551020408163265,(h) Hard Move (n = 10)
REPEAT,0.6571428571428571,"Figure 7: DDPG-based comparisons of related baselines in different environments. The x- and y-
axis denote the environment steps (×105) and average episode reward over recent 100 episodes. The
results are averaged using 5 runs, while the solid line and shaded represent the mean value and a
standard deviation respectively."
REPEAT,0.6591836734693878,"C.2
LEARNING CURVES FOR THE DYNAMICS PREDICTIVE REPRESENTATION"
REPEAT,0.6612244897959184,"Fig. 8 shows the learning curves of HyAR-TD3 with dynamics predictive representation loss
(Fig. 8(a-b)) and the inﬂuence of dynamics predictive representation loss weight β on algorithm
performance (Fig. 8(c)). We can easily ﬁnd that the representation learned by dynamics predictive
representation loss is better than without dynamics predictive representation loss. For the weight
β of dynamics predictive representation loss, we search the candidate set {0.1, 1, 5, 10, 20}. The
results show that the performance of the algorithm gradually improves with the increase of weight
β, reaches the best when β = 10 and then goes down as further increase of β. We can conclude that
the dynamics predictive representation loss is helpful for deriving an environment-awareness repre-
sentation for further improving the learning performance, efﬁcacy, and stability. More experiments
on representation visualization are in Appendix C.4."
REPEAT,0.6632653061224489,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 10 0 10 20 30 40"
REPEAT,0.6653061224489796,Avg Episode Reward
REPEAT,0.6673469387755102,"HyAR-TD3 (with dynamics prediction)
HyAR-TD3 (without dynamics prediction)"
REPEAT,0.6693877551020408,(a) Goal
REPEAT,0.6714285714285714,"0
2
4
6
8
10
Time Steps (1e5) 50 40 30 20 10 0 10"
REPEAT,0.673469387755102,Avg Episode Reward
REPEAT,0.6755102040816326,"HyAR-TD3 (with dynamics prediction)
HyAR-TD3 (without dynamics prediction)"
REPEAT,0.6775510204081633,(b) Hard Move (n = 8)
REPEAT,0.6795918367346939,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 20 10 0 10 20 30 40"
REPEAT,0.6816326530612244,Avg Episode Reward
REPEAT,0.6836734693877551,"Beta = 0
Beta = 0.1
Beta = 1
Beta = 5
Beta = 10
Beta = 20"
REPEAT,0.6857142857142857,(c) Goal
REPEAT,0.6877551020408164,"Figure 8: Learning curves for variants of dynamics predictive representation in HyAR. (a) and (b)
show the comparison between HyAR-TD3 with and without dynamics prediction auxiliary loss; (c)
shows the effects of the balancing weight β in Eq.6. The results are averaged using 5 runs, while the
solid line and shaded represent the mean value and a standard deviation respectively."
REPEAT,0.689795918367347,"C.3
LEARNING CURVES AND TABLE FOR THE RESULTS IN ABLATION STUDY"
REPEAT,0.6918367346938775,"As brieﬂy discussed in Sec. 5.3, we conduct detailed ablation and parameter analysis experiments
on the key components of the algorithm, including:"
REPEAT,0.6938775510204082,"• element-wise product (Tang et al., 2021) (v.s. concat) operation;"
REPEAT,0.6959183673469388,"• cascaded head (Azabou et al., 2021) (v.s. parallel head) structure;"
REPEAT,0.6979591836734694,Published as a conference paper at ICLR 2022
REPEAT,0.7,"• latent select range c ∈{80, 90, 96, 100}, for the latent space constraint (LSC) mechanism;
• action representation relabeling, corresponding to representation shift correction (RSC);
• latent action dim d1 = d2 ∈{3, 6, 12};"
REPEAT,0.7020408163265306,"Fig. 9 shows the learning curves of HyAR-TD3 and its variants for ablation studies, corresponding
to the results in Tab. 6."
REPEAT,0.7040816326530612,"First, we can observe that element-wise product achieves better performance than concatenation
(Fig. 9(a,e)). As similarly discovered in (Tang et al., 2021), we hypothesize that the explicit relation
between the condition and representation imposed by element wise product forces the conditional
VAE to learn more effective hidden features. Second, the signiﬁcance of cascaded head is demon-
strated by its superior performance over parallel head (Fig. 9(a,e)) which means cascaded head can
better output two different features. Third, representation relabeling shows an apparent improve-
ment (Fig. 9(b,f)) which show that representation shift leads to data invalidation in the experience
buffer which will affect RL training. Fourth, a reasonable latent select range plays an important
role in algorithm learning (Fig. 9(c,g)). Only constrain the action representation space of the latent
policy inside a reasonable area (both large and small will fail), can the algorithm learn effectively
and reliably. These experimental results supports our analysis above."
REPEAT,0.7061224489795919,"We also analyse the inﬂuence of latent action dim d1, d2 for RL (Fig. 9(d,h)). In the low-dimensional
hybrid action environment, we should choose a moderate value (e.g., 6). While for high-dimensional
environment, larger value may be better (e.g., 12). The insight behind is that the proper dimension-
ality of latent action representation may be comparable (or more compact) to the dimensionality
of state (ranging from 4 to 17 dimensions in different environments in our experiments). This is
because the latent action representation should reﬂect the semantics of original hybrid action, i.e.,
the state residual."
REPEAT,0.7081632653061225,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 20 10 0 10 20 30 40"
REPEAT,0.710204081632653,Avg Episode Reward
REPEAT,0.7122448979591837,"HyAR-TD3
HyAR-TD3 (Concat)
HyAR-TD3 (Parallel)"
REPEAT,0.7142857142857143,(a) Operator & Structure
REPEAT,0.7163265306122449,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 10 0 10 20 30 40"
REPEAT,0.7183673469387755,Avg Episode Reward
REPEAT,0.7204081632653061,"HyAR-TD3
HyAR-TD3 (No relabel)"
REPEAT,0.7224489795918367,(b) Representation relabeling
REPEAT,0.7244897959183674,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 20 10 0 10 20 30 40"
REPEAT,0.726530612244898,Avg Episode Reward
REPEAT,0.7285714285714285,"HyAR-TD3 (select range = 96%)
HyAR-TD3 (select range = 100%)
HyAR-TD3 (select range = 90%)
HyAR-TD3 (select range = 80%)"
REPEAT,0.7306122448979592,(c) Latent select range
REPEAT,0.7326530612244898,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 20 10 0 10 20 30 40"
REPEAT,0.7346938775510204,Avg Episode Reward
REPEAT,0.736734693877551,"HyAR-TD3 (latent dim = 6)
HyAR-TD3 (latent dim = 3)
HyAR-TD3 (latent dim = 12)"
REPEAT,0.7387755102040816,(d) Latent action dim
REPEAT,0.7408163265306122,"0
2
4
6
8
10
Time Steps (1e5) 50 40 30 20 10 0 10"
REPEAT,0.7428571428571429,Avg Episode Reward
REPEAT,0.7448979591836735,"HyAR-TD3
HyAR-TD3 (Concat)
HyAR-TD3 (Parallel)"
REPEAT,0.746938775510204,(e) Operator & Structure
REPEAT,0.7489795918367347,"0
2
4
6
8
10
Time Steps (1e5) 50 40 30 20 10 0 10"
REPEAT,0.7510204081632653,Avg Episode Reward
REPEAT,0.753061224489796,"HyAR-TD3
HyAR-TD3 (No relabel)"
REPEAT,0.7551020408163265,(f) Representation relabeling
REPEAT,0.7571428571428571,"0
2
4
6
8
10
Time Steps (1e5) 50 40 30 20 10 0 10"
REPEAT,0.7591836734693878,Avg Episode Reward
REPEAT,0.7612244897959184,"HyAR-TD3 (select range = 96%)
HyAR-TD3 (select range = 100%)
HyAR-TD3 (select range = 90%)
HyAR-TD3 (select range = 80%)"
REPEAT,0.763265306122449,(g) Latent select range
REPEAT,0.7653061224489796,"0
2
4
6
8
10
Time Steps (1e5) 50 40 30 20 10 0 10"
REPEAT,0.7673469387755102,Avg Episode Reward
REPEAT,0.7693877551020408,"HyAR-TD3 (latent dim = 6)
HyAR-TD3 (latent dim = 3)
HyAR-TD3 (latent dim = 12)"
REPEAT,0.7714285714285715,(h) Latent action dim
REPEAT,0.773469387755102,"Figure 9: Learning curves of ablation studies for HyAR (i.e., element-wise + cascaded head +
representation relabeling + latent select range = 96% + latent action dim = 6) corresponding to
Tab. 6. From top to bottom is Goal and Hard Move (n = 8) environment. The shaded region
denotes standard deviation of average evaluation over 5 runs."
REPEAT,0.7755102040816326,"C.4
REPRESENTATION VISUAL ANALYSIS"
REPEAT,0.7775510204081633,"In order to further analyze the hybrid action representation, we visualize the learned hybrid action
representations. Fig. 10 and Fig. 11 shows the t-SNE visualization for HyAR in Goal and Hard
Move (n = 8) environment."
REPEAT,0.7795918367346939,"As we can see from Fig. 10, we adopt t-SNE to cluster the latent continuous actions, i.e., (zx),
outputted by the latent policy, and color each action based on latent discrete actions i.e., (e). We
can conclude that latent continuous actions can be clustered by latent discrete actions, but there are
multiple modes in the global range. Our dependence-aware representation model makes good use
of this relationship that the choice of continuous action parameters is depend on discrete actions."
REPEAT,0.7816326530612245,Published as a conference paper at ICLR 2022
REPEAT,0.7836734693877551,"Operation
Structure
Result"
REPEAT,0.7857142857142857,"Elem.-Wise Prod.
Concat.
Cascaded
Parallel
Latent Select Range c
Latent Action Dim
Relabeling
Dynamics Predictive
Results (Goal)
Results (Hard Move)"
REPEAT,0.7877551020408163,"✓
✓
96%
6
✓
✓
0.78 ± 0.03
0.89 ± 0.03"
REPEAT,0.789795918367347,"✓
✓
96%
6
✓
✓
0.66± 0.10
0.83± 0.04"
REPEAT,0.7918367346938775,"✓
✓
96%
6
✓
✓
0.71 ± 0.04
0.80± 0.13"
REPEAT,0.7938775510204081,"✓
✓
96%
6
✓
0.66 ± 0.07
0.83± 0.08"
REPEAT,0.7959183673469388,"✓
✓
100%
6
✓
✓
0.62 ± 0.11
0.78± 0.13
✓
✓
90%
6
✓
✓
0.61 ± 0.04
0.78± 0.08
✓
✓
80%
6
✓
✓
0.08 ± 0.17
0.56± 0.12"
REPEAT,0.7979591836734694,"✓
✓
96%
3
✓
✓
0.59 ± 0.09
0.58± 0.16
✓
✓
96%
12
✓
✓
0.65 ± 0.09
0.90 ± 0.04"
REPEAT,0.8,"✓
✓
96%
6
✓
0.55 ± 0.15
0.84 ± 0.05"
REPEAT,0.8020408163265306,"Table 6: Ablation of our algorithm across each contribution in Goal and Hard Move (n = 8).
Results are average success rates at the end of training process over 5 runs. ± corresponds to a
standard deviation. The corresponding episode reward learning curves are shown in Fig. 9."
REPEAT,0.8040816326530612,"For the dynamics predictive representation loss, we adopt t-SNE to cluster the latent actions, i.e.,
(e, zx), outputted by the latent policy, and color each action based on its impact on the environment
(i.e., δs,s′). As shown in Fig. 11, we observe that actions with a similar impact on the environment
are relatively closer in the latent space. This demonstrates the dynamics predictive representation
loss is helpful for deriving an environment-awareness representation for further improving the learn-
ing performance, efﬁcacy, and stability. t-SNE 100 50 0 50 100"
REPEAT,0.8061224489795918,(a) Goal t-SNE 100 50 0 50 100
REPEAT,0.8081632653061225,(b) Hard Move (n = 8)
REPEAT,0.810204081632653,"Figure 10: t-SNE visualization diagram of continuous action embedding zx, color coded by discrete
action embedding e. The continuous actions related to the same discrete actions are mapped to the
similar regions of the representation space. t-SNE 100 50 0 50 100"
REPEAT,0.8122448979591836,(a) Goal t-SNE 100 50 0 50 100
REPEAT,0.8142857142857143,(b) Hard Move (n = 8)
REPEAT,0.8163265306122449,"Figure 11: t-SNE visualization diagram of hybrid action embedding pair (e, zx), color coded by
˜δs,s′. The hybrid actions with a similar impact on the environment are relatively closer in the latent
space."
REPEAT,0.8183673469387756,Published as a conference paper at ICLR 2022
REPEAT,0.8204081632653061,"D
ADDITIONAL EXPERIMENTS"
REPEAT,0.8224489795918367,"In this section, we conduct some additional experimental results for a further study of HyAR from
different perspectives:"
REPEAT,0.8244897959183674,"• We provide the exact number of samples used in the warm-up stage (i.e., stage 1 in Al-
gorithm 1 in each environment in Tab. 7. The numbers of warm-up environment steps are
about 5%−10% of the total environment steps in our original experiments."
REPEAT,0.826530612244898,"• Moreover, we also conduct some experiments to further reduce the number of samples
used in the warm-up stage (at most 80% off). See the colored results in Tab. 7. HyAR can
achieve comparable performance with < 3% samples of the total environment steps."
REPEAT,0.8285714285714286,"• We also provide Fig. 12 for another view of the learning curves where the number of sam-
ples used in the warm-up stage is also counted for HyAR."
REPEAT,0.8306122448979592,"• We conduct additional experiments to compare HyAR-TD3 and HyAR-TD3 with ﬁxed
hybrid action representation space trained in the warm-up stage in the environments Plat-
form, Goal, Hard Goal, Catch Point and Hard Move (n = 8). The results are provided in
Fig. 13, demonstrating the necessity of subsequent training of the representation trained in
the warm-up stage."
REPEAT,0.8326530612244898,"• We further conduct experiments to study the effects of different choices of training fre-
quency of subsequent representation training, in the environments Goal, Hard Goal. The
results are provided in Fig. 13, demonstrating a moderate training frequency works best."
REPEAT,0.8346938775510204,"Environment
Number of Warm-up Env. Steps
Number of Total
original
new
Env. Steps"
REPEAT,0.8367346938775511,"Goal
20000 (0.067|0.78)
5000 (0.017|0.75)
300000"
REPEAT,0.8387755102040816,"Hard Goal
20000 (0.067|0.60)
5000 (0.017|0.55)
300000"
REPEAT,0.8408163265306122,"Platform
10000 (0.05|0.98)
5000 (0.025|0.96)
200000"
REPEAT,0.8428571428571429,"Catch Point
100000 (0.1|0.90)
20000 (0.02|0.82)
1000000"
REPEAT,0.8448979591836735,"Hard Move (n=4)
100000 (0.1|0.93)
20000 (0.02|0.91)
1000000"
REPEAT,0.8469387755102041,"Hard Move (n=6)
100000 (0.1|0.92)
20000 (0.02|0.92)
1000000"
REPEAT,0.8489795918367347,"Hard Move (n=8)
100000 (0.1|0.89)
20000 (0.02|0.83)
1000000"
REPEAT,0.8510204081632653,"Hard Move (n=10)
100000 (0.1|0.75)
20000 (0.02|0.70)
1000000"
REPEAT,0.8530612244897959,"Table 7: The exact number of samples used in warm-up stage training in different environments.
The column of ‘original’ denotes what is done in our experiments; the column of ‘new’ denotes ad-
ditional experiments we conduct with fewer warm-up samples (and proportionally fewer warm-up
training). For each entry x(y|z), x is the number of samples (environment steps), y denotes the per-
centage
number of warm-up environment steps
number of total environment steps during the training process, and z denotes the corresponding performance
of HyAR-TD3 as evaluated in Tab. 2. Conclusion: The numbers of warm-up environment steps are
about 5%−10% of the total environment steps in our original experiments. The number of warm-
up environment steps can be further reduced by at most 80% off (thus leading to < 3% of the total
environment steps) while comparable performance of our algorithm remains."
REPEAT,0.8551020408163266,"E
ADDITIONAL DISCUSSION ON DISCRETE-CONTINUOUS HYBRID ACTION
SPACE"
REPEAT,0.8571428571428571,"To the best of our knowledge, most RL problems with discrete-continuous hybrid action spaces can
be formulated by Parameterized Action MDP (PAMDP) described in Sec. 2. These hybrid action
spaces can be roughly divided into two categories: structured hybrid action space (mainly considered
in our paper) and non-structured hybrid action space. In non-structured hybrid action space, there is
no dependence among different parts of hybrid action. For a mixed type of the above two, we may
conclude it in the case of structured hybrid action space."
REPEAT,0.8591836734693877,"To be speciﬁc, learning with non-structured hybrid action spaces can be viewed as a special case of
PAMDP. It can be similarly formulated with the deﬁnition of discrete action space K (p.s., this can
be a joint discrete action space when there are multiple discrete dimensions) and the deﬁnition of"
REPEAT,0.8612244897959184,Published as a conference paper at ICLR 2022
REPEAT,0.863265306122449,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 10 0 10 20 30 40"
REPEAT,0.8653061224489796,Avg Episode Reward
REPEAT,0.8673469387755102,"HPPO
PDQN-TD3
PATD3
HHQN-TD3
HyAR-TD3"
REPEAT,0.8693877551020408,(a) goal
REPEAT,0.8714285714285714,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 10 0 10 20 30"
REPEAT,0.8734693877551021,Avg Episode Reward
REPEAT,0.8755102040816326,"HPPO
PDQN-TD3
PATD3
HHQN-TD3
HyAR-TD3"
REPEAT,0.8775510204081632,(b) Hard Goal
REPEAT,0.8795918367346939,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Time Steps (1e5) 0.0 0.2 0.4 0.6 0.8 1.0"
REPEAT,0.8816326530612245,Avg Episode Reward
REPEAT,0.8836734693877552,"HPPO
PDQN-TD3
PATD3
HHQN-TD3
HyAR-TD3"
REPEAT,0.8857142857142857,(c) platform
REPEAT,0.8877551020408163,"0
2
4
6
8
10
Time Steps (1e5) 40 30 20 10 0 10"
REPEAT,0.889795918367347,Avg Episode Reward
REPEAT,0.8918367346938776,"HPPO
PDQN-TD3
PATD3
HHQN-TD3
HyAR-TD3"
REPEAT,0.8938775510204081,(d) catch point
REPEAT,0.8959183673469387,"0
2
4
6
8
10
Time Steps (1e5) 40 30 20 10 0 10"
REPEAT,0.8979591836734694,Avg Episode Reward
REPEAT,0.9,"HPPO
PDQN-TD3
PATD3
HHQN-TD3
HyAR-TD3"
REPEAT,0.9020408163265307,(e) Hard Move (n=4)
REPEAT,0.9040816326530612,"0
2
4
6
8
10
Time Steps (1e5) 50 40 30 20 10 0 10"
REPEAT,0.9061224489795918,Avg Episode Reward
REPEAT,0.9081632653061225,"HPPO
PDQN-TD3
PATD3
HHQN-TD3
HyAR-TD3"
REPEAT,0.9102040816326531,(f) Hard Move (n=6)
REPEAT,0.9122448979591836,"0
2
4
6
8
10
Time Steps (1e5) 70 60 50 40 30 20 10 0 10"
REPEAT,0.9142857142857143,Avg Episode Reward
REPEAT,0.9163265306122449,"HPPO
PDQN-TD3
PATD3
HHQN-TD3
HyAR-TD3"
REPEAT,0.9183673469387755,(g) Hard Move (n=8)
REPEAT,0.9204081632653062,"0
2
4
6
8
10
Time Steps (1e5) 60 50 40 30 20 10 0"
REPEAT,0.9224489795918367,Avg Episode Reward
REPEAT,0.9244897959183673,"HPPO
PDQN-TD3
PATD3
HHQN-TD3
HyAR-TD3"
REPEAT,0.926530612244898,(h) Hard Move (n=10)
REPEAT,0.9285714285714286,"Figure 12: Another view of the learning curves shown in Fig. 5 where the number of samples
used in warm-up training (i.e., stage 1 in Algorithm 1) is also counted for HyAR. Comparisons of
algorithms in different environments. The x- and y-axis denote the environment steps and average
episode reward over recent 100 episodes. The results are averaged using 5 runs, while the solid line
and shaded represent the mean value and a standard deviation respectively."
REPEAT,0.9306122448979591,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time Steps (1e5) 10 0 10 20 30 40"
REPEAT,0.9326530612244898,Avg Episode Reward
REPEAT,0.9346938775510204,"Unfixed HyAR
Fixed HyAR"
REPEAT,0.936734693877551,(a) Goal
REPEAT,0.9387755102040817,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Time Steps (1e5) 10 0 10 20 30"
REPEAT,0.9408163265306122,Avg Episode Reward
REPEAT,0.9428571428571428,"Unfixed HyAR
Fixed HyAR"
REPEAT,0.9448979591836735,(b) Hard Goal
REPEAT,0.9469387755102041,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Time Steps (1e5) 0.2 0.4 0.6 0.8 1.0"
REPEAT,0.9489795918367347,Avg Episode Reward
REPEAT,0.9510204081632653,"Unfixed HyAR
Fixed HyAR"
REPEAT,0.9530612244897959,(c) Platform
REPEAT,0.9551020408163265,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Time Steps (1e5) 40 30 20 10 0 10"
REPEAT,0.9571428571428572,Avg Episode Reward
REPEAT,0.9591836734693877,"Unfixed HyAR
Fixed HyAR"
REPEAT,0.9612244897959183,(d) Catch Point
REPEAT,0.963265306122449,"0
2
4
6
8
10
Time Steps (1e5) 50 40 30 20 10 0 10"
REPEAT,0.9653061224489796,Avg Episode Reward
REPEAT,0.9673469387755103,"Unfixed HyAR
Fixed HyAR"
REPEAT,0.9693877551020408,(e) Hard Move(n=8)
REPEAT,0.9714285714285714,"Figure 13: Comparison between HyAR-TD3 (denoted by Unﬁxed HyAR) and HyAR-TD3 with
ﬁxed hybrid action representation space trained in warm-up stage (denoted by Fixed HyAR) in dif-
ferent environments. The x- and y-axis denote the environment steps and average episode reward
over recent 100 episodes. The results are averaged using 5 runs, while the solid line and shaded
represent the mean value and a standard deviation respectively. Conclusion: Unﬁxed HyAR outper-
forms Fixed HyAR across all the environments; while Fixed HyAR performs very poorly in Goal
and Hard Goal. We conjecture that in these environments, random policy is quite limited in collect-
ing effective and meaning hybrid actions in these environments thus the learned ﬁxed representation
space is not able to support the emergence of effective latent policy."
REPEAT,0.9734693877551021,"a uniform continuous action space X rather than Xk since there is no structural dependence in this
case. A few examples and analogies can be found in Robotic control (Neunert et al., 2019) where
several gears or switchers need to be selected in addition to other continuous control parameters, or"
REPEAT,0.9755102040816327,Published as a conference paper at ICLR 2022
REPEAT,0.9775510204081632,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Time Steps (1e5) 10 0 10 20 30 40"
REPEAT,0.9795918367346939,Avg Episode Reward
REPEAT,0.9816326530612245,"Fixed
1/50ep
1/20ep
1/10ep
2/10ep
5/10ep
10/10ep"
REPEAT,0.9836734693877551,(a) Goal
REPEAT,0.9857142857142858,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Time Steps (1e5) 10 0 10 20 30"
REPEAT,0.9877551020408163,Avg Episode Reward
REPEAT,0.9897959183673469,"Fixed
1/50ep
1/20ep
1/10ep
2/10ep
5/10ep
10/10ep"
REPEAT,0.9918367346938776,(b) Hard Goal
REPEAT,0.9938775510204082,"Figure 14: The effects of different choices of training frequency of subsequent representation train-
ing of HyAR-TD3 in the environments Goal, Hard Goal. For example, 1/10ep denotes that the
hybrid action representation is trained every 10 episodes for 1 batches with batch size 64. The black
dashed lines denote the convergence results of Fixed HyAR in Figure 13. The x- and y-axis denote
the environment steps and average episode reward over recent 100 episodes. The results are aver-
aged using 5 runs, while the solid line and shaded represent the mean value and a standard deviation
respectively. Conclusion: all conﬁgurations of subsequent training outperform Fixed HyAR; while
a moderate training frequency works best."
REPEAT,0.9959183673469387,"several dimensions of continuous parameters are discretized. In this view, our algorithm proposed
under PAMDP is applicable to both structured and non-structured hybrid action spaces."
REPEAT,0.9979591836734694,"However, discrete-continuous hybrid action spaces are not well studied in DRL. Most existing works
evaluate proposed algorithms in relatively low-dimensional and simple environments. Such environ-
ments may reﬂect some signiﬁcant nature of discrete-continuous hybrid action control problem yet
are far away from the scale and complexity of real-world scenarios, e.g., more complex robots with
natural discrete and (dependent) continuous control (beyond the basic robot locomotion in like pop-
ular MuJoCo RL suite), RL-based software testing (Zheng et al., 2021b; 2019), game AI generation
(Shen et al., 2020), recommendation systems (e.g., e-shop, music, news), and industrial production
(e.g., production process control & smart grid control) (Sun et al., 2020; Zheng et al., 2021c). We
believe that the emergence of such environments will beneﬁt the community of discrete-continuous
hybrid action RL study. To evaluate and develop our algorithm on multiagent scenarios (Hao et al.,
2022; Zheng et al., 2021a; 2018) with more complex and practical hybrid action spaces is one of our
main future directions."
