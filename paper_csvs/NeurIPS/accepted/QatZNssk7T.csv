Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.005813953488372093,"In adaptive data analysis, a mechanism gets n i.i.d. samples from an unknown
distribution D, and is required to provide accurate estimations to a sequence of
adaptively chosen statistical queries with respect to D. Hardt and Ullman [2014]
and Steinke and Ullman [2015a] showed that, in general, it is computationally hard
to answer more than Θ(n2) adaptive queries, assuming the existence of one-way
functions.
However, these negative results strongly rely on an adversarial model that signifi-
cantly advantages the adversarial analyst over the mechanism, as the analyst, who
chooses the adaptive queries, also chooses the underlying distribution D. This
imbalance raises questions with respect to the applicability of the obtained hardness
results – an analyst who has complete knowledge of the underlying distribution D
would have little need, if at all, to issue statistical queries to a mechanism which
only holds a finite number of samples from D.
We consider more restricted adversaries, called balanced, where each such ad-
versary consists of two separate algorithms: The sampler who is the entity that
chooses the distribution and provides the samples to the mechanism, and the analyst
who chooses the adaptive queries, but has no prior knowledge of the underlying
distribution (and hence has no a priori advantage with respect to the mechanism).
We improve the quality of previous lower bounds by revisiting them using an
efficient balanced adversary, under standard public-key cryptography assumptions.
We show that these stronger hardness assumptions are unavoidable in the sense
that any computationally bounded balanced adversary that has the structure of all
known attacks, implies the existence of public-key cryptography."
INTRODUCTION,0.011627906976744186,"1
Introduction"
INTRODUCTION,0.01744186046511628,"Statistical validity is a widely recognized crucial feature of modern science. Lack of validity –
popularly known as the replication crisis in science poses a serious threat to the scientific process
and also to the public’s trust in scientific findings."
INTRODUCTION,0.023255813953488372,"One of the factors leading to the replication crisis is the inherent adaptivity in the data analysis
process. To illustrate adaptivity and its effect, consider a data analyst who is testing a specific research"
INTRODUCTION,0.029069767441860465,"hypothesis. The analyst gathers data, evaluates the hypothesis empirically, and often finds that their
hypothesis is not supported by the data, leading to the formulation and testing of more hypotheses.
If these hypotheses are tested and formed based on the same data (as acquiring fresh data is often
expensive or even impossible), then the process is of adaptive data analysis (ADA) because the
choice of hypotheses depends on the data. However, ADA no longer aligns with classical statistical
theory, which assumes that hypotheses are selected independently of the data (and preferably before
gathering data). ADA may lead to overfitting and hence false discoveries."
INTRODUCTION,0.03488372093023256,"Statistical validity under ADA is a fundamental problem in statistics, that has received only partial
answers. A recent line of work, initiated by Dwork et al. [2015c] and includes [Hardt and Ullman,
2014, Dwork et al., 2015a,b, Steinke and Ullman, 2015a,b, Bassily et al., 2016, Rogers et al., 2016,
Russo and Zou, 2016, Smith, 2017, Feldman and Steinke, 2017, Nissim et al., 2018, Feldman and
Steinke, 2018, Shenfeld and Ligett, 2019, Jung et al., 2020, Fish et al., 2020, Dagan and Kur, 2022,
Kontorovich et al., 2022, Dinur et al., 2023, Blanc, 2023] has resulted in new insights into ADA
and robust paradigms for guaranteeing statistical validity in ADA. A major objective of this line of
work is to design optimal mechanisms M that initially obtain a dataset S containing n i.i.d. samples
from an unknown distribution D, and then answers adaptively chosen queries with respect to D.
Importantly, all of M’s answers must be accurate with respect to the underlying distribution D, not
just w.r.t. the empirical dataset S. The main question is how to design an efficient mechanism that
provides accurate estimations to adaptively chosen statistical queries, where the goal is to maximize
the number of queries M can answer. This objective is achieved by providing both upper- and
lower-bound constructions, where the lower-bound constructions demonstrate how an adversarial
analyst making a small number of queries to an arbitrary M can invariably force M to err. The setting
for these lower-bound proofs is formalized as a two-player game between a mechanism M and an
adversary A as in Game 1.1."
INTRODUCTION,0.040697674418604654,Game 1.1 (ADA game between a mechanism M and an adversarial analyst A).
INTRODUCTION,0.046511627906976744,"• M gets a dataset S of n i.i.d. samples from an unknown distribution D over X.
• For i = 1, . . . , ℓ:"
INTRODUCTION,0.05232558139534884,"– A sends a query qi : X 7→[−1, 1] to M.
– M sends an answer yi ∈[−1, 1] to A.
(As A and M are stateful, qi and yi may depend on the previous messages.)
M fails if ∃i ∈[ℓ] s.t. |yi −Ex∼D[qi(x)]| > 1/10."
INTRODUCTION,0.05813953488372093,"A question that immediately arises from the description of Game 1.1 is to whom should the distribution
D be unknown, and how to formalize this lack of knowledge. Ideally, the mechanism M should
succeed with high probability for every unknown distribution D and against any adversary A."
INTRODUCTION,0.06395348837209303,"In prior work, this property was captured by letting the adversary choose the distribution D at the
outset of Game 1.1. Namely, the adversary A can be seen as a pair of algorithms (A1, A2), where
A1 chooses the distribution D and sends a state st to A2 (which may contain the entire view of A1),
and after that, M and A2(st) interacts in Game 1.1. In this adversarial model, Hardt and Ullman
[2014] and Steinke and Ullman [2015a] showed that, assuming the existence of one way functions,
it is computationally hard to answer more than Θ(n2) adaptive queries. These results match the
state-of-the-art constructions [Dwork et al., 2015c,a,b, Steinke and Ullman, 2015b, Bassily et al.,
2016, Feldman and Steinke, 2017, 2018, Dagan and Kur, 2022, Blanc, 2023].1 In fact, each such
negative result was obtained by constructing a single adversary A that fails all efficient mechanisms.
This means that, in general, it is computationally hard to answer more than Θ(n2) adaptive queries
even when the analyst’s algorithm is known to the mechanism. On the other hand, in each of these
negative results, the adversarial analyst has a significant advantage over the mechanism – their ability
to select the distribution D. This allows the analyst to inject random trapdoors in D (e.g., keys of an
encryption scheme) which are then used in forcing a computationally limited mechanism to fail, as
the mechanism does not get a hold of the trapdoor information."
INTRODUCTION,0.06976744186046512,"1Here is an example of a mechanism that handles ˜Θ(n2) adaptive queries using differential privacy: Given
a query qi, the mechanism returns an answer yi = 1 n
P"
INTRODUCTION,0.0755813953488372,"x∈S x + νi where the νi’s are independent Gaussian
noises, each with standard deviation of ˜O(
√"
INTRODUCTION,0.08139534883720931,"ℓ/n). The noises guarantee that the entire process is “private
enough"" for avoiding overfitting in the ADA game, and accuracy is obtained whenever ℓ= ˜O(n2)."
INTRODUCTION,0.0872093023255814,"For most applications, the above adversarial model seems to be too strong. For instance, a data analyst
who is testing research hypotheses usually has no knowledge advantage about the distribution that
the mechanism does not have. In this typical setting, even if the underlying distribution D happens
to have a trapdoor, if the analyst recovers the trapdoor then the mechanism should also be able to
recover it and hence disable its adversarial usage."
INTRODUCTION,0.09302325581395349,"In light of this observation, we could hope that in a balanced setting, where the underlying distribution
is unknown to both the mechanism and the analyst, it would always be possible for M to answer
more than O(n2) adaptive queries. To explore this possibility, we introduce what we call a balanced
adversarial model.
Definition 1.2 (Balanced Adversary). A balanced adversary A consists of two isolated algorithms:
The sampler A1, which chooses a distribution D and provides i.i.d. samples to the mechanism M, and
the analyst A2, which asks the adaptive queries. No information is transferred from A1 to A2. See
Game 1.3."
INTRODUCTION,0.09883720930232558,"Game 1.3 (The ADA game between a mechanism M and a balanced adversary A = (A1, A2))."
INTRODUCTION,0.10465116279069768,"• A1 chooses a distribution D over X (specified by a sampling algorithm) and provides n
i.i.d. samples S to M (by applying the sampling algorithm n times).
/* A1 does not provide A2 with any information */
• M and A2 play Game 1.1 (with respect to D and S)."
INTRODUCTION,0.11046511627906977,M fails if and only if it fails in Game 1.1.
INTRODUCTION,0.11627906976744186,"Note that the difference between the balanced model and the previous (imbalanced) one is whether
A1 can send a state to A2 after choosing the distribution D (in the imbalanced model it is allowed, in
contrast to the balanced model).2"
INTRODUCTION,0.12209302325581395,"We remark that the main advantage of the balanced model comes when considering a publicly known
sampler A1 (as we do throughout this work). This way, A1 captures the common knowledge that both
the mechanism M and the analyst A2 have about the underlying distribution D.
Question 1.4. Do the lower-bounds proved in prior work hold also for balanced adversaries?"
INTRODUCTION,0.12790697674418605,"In this work we answer Question 1.4 in the positive. We do that using a publicly known analyst
A2 (which even makes it stronger than what is required for a lower bound). I.e., even though the
sampler A1 and the analyst A2 are publicly known and cannot communicate with each other, they fail
any computationally bounded mechanism. However, our lower-bound is based on stronger hardness
assumptions than in prior work, namely, we use public-key cryptography."
OUR RESULTS,0.13372093023255813,"1.1
Our Results"
OUR RESULTS,0.13953488372093023,"Our first result is a construction of a balanced adversary forcing any computationally bounded
mechanim to fail in Game 1.3.
Theorem 1.5 (Computational lower bound, informal). There exists an efficient balanced adversary
A = (A1, A2) that fails any computationally bounded mechanism M using Θ(n2) adaptive queries.
Moreover, it does so by choosing a distribution over a small domain."
OUR RESULTS,0.14534883720930233,"Our construction in Theorem 1.5 uses the structure of previous attacks of Hardt and Ullman [2014] and
Steinke and Ullman [2015a], but relies on a stronger hardness assumption of public-key cryptography.
We prove that this is unavoidable.
Theorem 1.6 (The necessity of public-key cryptography for proving Theorem 1.5, informal). Any
computationally bounded balanced adversary that follows the structure of all currently known attacks,
implies the existence of public-key cryptography (in particular, a key-agreement protocol)."
OUR RESULTS,0.1511627906976744,"In Section 1.3 we provide proof sketches of Theorems 1.5 and 1.6, where the formal statements
appear in Sections 3 and 4 (respectively) and the formal proofs appear in the supplementary material."
OUR RESULTS,0.1569767441860465,"2An additional (minor) difference is that we chose in our model to let A1 also provide the i.i.d. samples
to M. This is only useful for Theorem 1.6 as we need there that choosing D and sampling from D are both
computationally efficient (which are simply captured by saying that A1 is computationally bounded)."
OUR RESULTS,0.16279069767441862,"Potential Consequences for the Information Theoretic Setting.
Theorem 1.6 has immediate
implication to the information theoretic setting, and allow for some optimism regarding the possibility
of constructing an inefficient mechanism that answers many adaptive queries."
OUR RESULTS,0.1686046511627907,"It is known that an inefficient mechanism can answer exponentially many adaptive queries, but such
results have a strong dependency on the domain size. For instance, the Private Multiplicative Weights"
OUR RESULTS,0.1744186046511628,"algorithm of Hardt and Rothblum [2010] can answer 2
˜
O

n/√"
OUR RESULTS,0.18023255813953487,"log|X|

adaptive queries accurately.
However, this result is not useful whenever n ≤O(
p"
OUR RESULTS,0.18604651162790697,"log|X|). Indeed, Steinke and Ullman [2015a]
showed that this dependency is unavoidable in general, by showing that large domain can be used for
constructing a similar, unconditional, adversary that fails any computationally unbounded mechanism
after Θ(n2) queries. Our Theorem 1.6 implies that such an attack cannot be implemented in
the balanced setting, which gives the first evidence that there might be a separation between the
computational and information theoretic setting under the balanced adversarial model (in contrast
with the imbalanced model).
Corollary 1.7. There is no balanced adversary that follows the structure of all currently known
attacks, and fails any (computationally unbounded) mechanism."
OUR RESULTS,0.19186046511627908,"In order to see why Corollary 1.7 holds, suppose that we could implement such kind of attack
using a balanced adversary. Then by Theorem 1.6, this would imply that we could construct an
information-theoretic key agreement protocol (i.e., a protocol between two parties that agree on a key
that is secret from the eyes of a computationally unbounded adversary that only sees the transcript
of the execution). But since the latter does not exist, we conclude that such a balanced adversary
does not exists either. In other words, we do not have a negative result that rules out the possibility of
constructing an inefficient mechanism that can answer many adaptive queries of a balanced adversary,
and we know that if a negative result exists, then by Theorem 1.6 it cannot follow the structure of
Hardt and Ullman [2014], Steinke and Ullman [2015a]."
OUR RESULTS,0.19767441860465115,"1.2
Comparison with Elder [2016]"
OUR RESULTS,0.20348837209302326,"The criticism about the lower bounds of Hardt and Ullman [2014], Steinke and Ullman [2015a] is not
new and prior work has attempted at addressing them with only partial success."
OUR RESULTS,0.20930232558139536,"For example, Elder [2016] presented a similar “balanced"" model (called “Bayesian ADA""), where
both the analyst and the mechanism receive a prior P which is a family of distributions, and then the
distribution D is drawn according to P (unknown to both the mechanism and the analyst)."
OUR RESULTS,0.21511627906976744,"From an information theoretic point of view, this model is equivalent to ours when the sampler A1 is
publicly known, since A1 simply defines a prior. But from a computational point of view, defining
the sampling process (i.e., sampling D and the i.i.d. samples from it) in an algorithmic way is better
when we would like to focus on computationally bounded samplers."
OUR RESULTS,0.22093023255813954,"Elder [2016] only focused on the information-theoretic setting. His main result is that a certain family
of mechanisms (ones that only use the posterior means) cannot answer more than ˜O(n4) adaptive
queries. This, however, does not hold for any mechanism’s strategy. In particular, it does not apply to
general computationally efficient mechanisms. Our negative result is quantitatively stronger (n2 vs
n4) and it applies for all computationally efficient mechanisms.3"
OUR RESULTS,0.22674418604651161,"Table 1 summarizes the comparison between Theorem 1.5 and the prior lower bounds (ignoring
computational hardness assumptions)."
TECHNIQUES,0.23255813953488372,"1.3
Techniques"
TECHNIQUES,0.23837209302325582,"We follow a similar technique to that used in Hardt and Ullman [2014] and Steinke and Ullman
[2015a], i.e., a reduction to a restricted set of mechanisms, called natural.
Definition 1.8 (Natural mechanism [Hardt and Ullman, 2014]). A mechanism M is natural if, when
given a sample S = (x1, . . . , xn) ∈X n and a query q: X →[−1, 1], M returns an answer that is a
function solely of (q(x1), . . . , q(xn)). In particular, M does not evaluate q on other data points of its
choice."
TECHNIQUES,0.2441860465116279,"3Our result is not directly comparable to that of Elder [2016], because our negative result does not say anything
for non-efficient mechanisms, while his result does rule out a certain family of non-efficient mechanisms."
TECHNIQUES,0.25,"Balanced?
Class of Mechanisms
# of Queries
Dimension (log|X|)
Steinke and Ullman [2015a]
No
PPT Algorithms
˜O(n2)
no(1)"
TECHNIQUES,0.2558139534883721,"Steinke and Ullman [2015a]
No
All
˜O(n2)
O(n2)
Elder [2016]
Yes
Certain Family
˜O(n4)
˜O(n4)
Theorem 1.5
Yes
PPT Algorithms
˜O(n2)
no(1)
Table 1: Comparison between the lower bounds for adaptive data analysis."
TECHNIQUES,0.2616279069767442,"Hardt and Ullman [2014] and Steinke and Ullman [2015a] showed that there exists an adversarial
analyst eA that fails any natural mechanism M, even when M is computationally unbounded, and even
when D is chosen to be the uniform distribution over {1, 2, . . . , m = 2000n} (I.e., D is known to
everyone). While general mechanisms could simply use the knowledge of the distribution to answer
any query, natural mechanisms are more restricted, and can only provide answers based on the n-size
dataset S that they get. The restriction to natural mechanisms allowed Steinke and Ullman [2015a] to
use interactive fingerprinting codes, which enable to reveal S using Θ(n2) adaptive queries when the
answers are accurate and correlated with S."
TECHNIQUES,0.26744186046511625,"To construct an attacker A that fails any computationally bounded mechanism (and not just natural
mechanisms), prior work forced the mechanism to behave naturally by using a private-key encryption
scheme. More specifically, the adversary first samples m secret keys sk1, . . . , skm, and then defines D
to be the uniform distribution over the pairs {(j, skj)}m
j=1. The adversary then simulates an adversary
eA which fails natural mechanisms as follows: a query ˜q: [m] →[−1, 1] issued by eA is translated
by A to a set of m encryptions {ctj}m
j=1 where each ctj is an encryption of ˜q(j) under the key skj.
These encryptions define a new query q that on input (j, sk), outputs the decryption of ctj under the
key sk. However, since M is computationally bounded and has only the secret keys that are part of
its dataset S, it can only decrypt the values of ˜q on points in S, yielding that it effectively behaves
naturally."
TECHNIQUES,0.27325581395348836,"Note that the above attack A is imbalanced as it injects the secret keys sk1, . . . , skm into D and then
uses these keys when it forms queries. In other words, even though the attacker A is known to the
mechanism, A is able to fail M by creating a secret correlation between its random coins and the
distribution D."
BALANCED ADVERSARY VIA IDENTITY BASED ENCRYPTION SCHEME,0.27906976744186046,"1.3.1
Balanced Adversary via Identity Based Encryption Scheme"
BALANCED ADVERSARY VIA IDENTITY BASED ENCRYPTION SCHEME,0.28488372093023256,"For proving Theorem 1.5, we replace the private-key encryption scheme with a public-key primitive
called identity-based encryption (IBE) scheme [Shamir, 1984, Cocks, 2001, Boneh and Franklin,
2001]. Such a scheme enables to produce m secret keys sk1, . . . , skm along with a master public
key mpk. Encrypting a message to a speific identity j ∈[m] only requires mpk, but decrypting a
message for identity j must be done using its secret key skj. Using an IBE scheme we can achieve
a reduction to natural mechanisms via a balanced adversary A = (A1, A2) as follows: A1 samples
keys mpk, sk1, . . . , skm according to the IBE scheme, and defines D to be the uniform distribution
over the triplets {(j, mpk, skj)}m
j=1. The analyst A2, which does not know the keys, first asks queries
of the form q(j, mpk, sk) = mpkk for every bit k of mpk in order to reveal it. Then, it follows a
strategy as in the previous section, i.e., it simulates an adversary eA which foils natural mechanisms
by translating each query query ˜q: [m] →[−1, 1] issued by eA by encrypting each ˜q(j) for identity j
using mpk. Namely, the IBE scheme allowed the analyst to implement the attack of Hardt and Ullman
[2014] and Steinke and Ullman [2015a], but without having to know the secret keys sk1, . . . , skm."
BALANCED ADVERSARY VIA IDENTITY BASED ENCRYPTION SCHEME,0.29069767441860467,"We can implement the IBE scheme using a standard public-key encryption scheme: in the sampling
process, we sample m independent pairs of public and secret keys {(pkj, skj)}m
j=1 of the original
scheme, and define mpk = (pk1, . . . , pkm). When encrypting a message for identity j ∈[m],
we could simply encrypt it using pkj (part of mpk), which can only be decrypted using skj. The
disadvantage of this approach is the large master public key mpk that it induces. Applying the
encryption scheme with security parameter of λ, the master key mpk will be of size λ · m and not just
λ as the sizes of the secret keys. This means that implementing our balanced adversary with such an
encryption scheme would result with a distribution over a large domain X, which would not rule out
the possibility to construct a mechanism for distributions over smaller domains. Yet, Döttling and"
BALANCED ADVERSARY VIA IDENTITY BASED ENCRYPTION SCHEME,0.29651162790697677,"Garg [2021] showed that it is possible to construct a fully secure IBE scheme using a small mpk of
size only O(λ · log m) under standard hardness assumptions (e.g., the Computational Diffie Helman
problem [Diffie and Hellman, 1976]4 or the hardness of factoring)."
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.3023255813953488,"1.3.2
Key-Agreement Protocol via Balanced Adversary"
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.3081395348837209,"In order to prove Theorem 1.6, we first explain what type of adversaries the theorem applies to. Recall
that in all known attacks (including ours), the adversary A wraps a simpler adversary eA that fails
natural mechanisms. In particular, the wrapper A has two key properties:"
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.313953488372093,1. A knows Ex∼D[qℓ(x)] for the last query qℓthat it asks (becuase it equals to 1
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.31976744186046513,"m
Pm
j=1 ˜qℓ(j),
where ˜qℓis the wrapped query which is part of A’s view), and
2. If the mechanism attempts to behave accurately in the first ℓ−1 rounds (e.g., it answers the
empirical mean 1 n
P"
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.32558139534883723,"x∈S q(x) for every query q), then A, as a wrapper of eA, will be able to
ask a last query qℓthat would fail any computationally bounded last-round strategy for the
mechanism."
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.3313953488372093,"We next show that any computationally bounded balanced adversary A that has the above two
properties, can be used for constructing a key-agreement protocol. That is, a protocol between two
computationally bounded parties P1 and P2 that enable them to agree on a value which cannot be
revealed by a computationally bounded adversary who only sees the transcript of the execution. See
Protocol 1.9."
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.3372093023255814,"Protocol 1.9 (Key-Agreement Protocol (P1, P2) via a balanced adversary A = (A1, A2)).
Input: 1n. Let ℓ= ℓ(n) and X = X(n) be the number queries and the domain that is used by the
adversary A.
Operation:"
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.3430232558139535,"• P1 emulates A1 on input n for obtaining a distribution D over X (specified by a sampling
procedure), and samples n i.i.d. samples S.
• P2 initializes an emulation of A2 on input n.
• For i = 1 to ℓ:"
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.3488372093023256,"1. P2 receives the ith query qi from the emulated A2 and sends it to P1.
2. P1 computes yi = 1 n
P"
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.3546511627906977,"x∈S qi(x), and sends it to P2."
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.36046511627906974,"3. P2 sends yi as the ith answer to the emulated A2.
• P1 and P2 agree on Ex∼D[qℓ(x)]."
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.36627906976744184,"The agreement of Protocol 1.9 relies on the ability of P1 and P2 to compute Ex∼D[qℓ(x)]. Indeed, P1
can accurately estimate it using the access to the sampling procedure, and P2 can compute it based
on the view of the analyst A2 (follows by Property 1)."
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.37209302325581395,"To prove the secrecy guarantee of Protocol 1.9, assume towards a contradiction that there exists a com-
putationally bounded adversary G that given the transcript of the execution, can reveal Ex∼D[qℓ(x)].
Now consider the following mechanism for the ADA game: In the first ℓ−1 queries, answer the em-
pirical mean, but in the last query, apply G on the transcript and answer its output. By the assumption
on G, the mechanism will be able to accurately answer the last query, in contradiction to Property 2."
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.37790697674418605,"We note that Property 1 can be relaxed by only requiring that A is able to provide a “good enough""
estimation of Ex∼D[qℓ(x)]. Namely, as long as the estimation provided in Property 1 is better than
the estimation that an adversary can obtain in Property 2 (we prove that an nΩ(1) multiplicative
gap suffices), this would imply that Protocol 1.9 is a weak key-agreement protocol, which can be
amplified to a fully secure one using standard techniques."
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.38372093023255816,"We also note that by requiring in Game 1.3 that A1 samples from D according to the sampling
procedure, we implicitly assume here that sampling from D can be done efficiently (because A1 is"
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.38953488372093026,"4CDH is hard with respect to a group G of order p, if given a random generator g along with ga and gb, for
uniformly random a, b ∈[p], as inputs, the probability that a PPT algorithm can compute gab is negligible."
KEY-AGREEMENT PROTOCOL VIA BALANCED ADVERSARY,0.3953488372093023,"assumed to be computationally bounded). Our reduction to key-agreement relies on this property,
since if sampling from D could not be done efficiently, then P1 would not have been a computationally
bounded algorithm."
PERSPECTIVE OF PUBLIC KEY CRYPTOGRAPHY,0.4011627906976744,"1.4
Perspective of Public Key Cryptography"
PERSPECTIVE OF PUBLIC KEY CRYPTOGRAPHY,0.4069767441860465,"Over the years, cryptographic research has proposed solutions to many different cryptographic tasks
under a growing number of (unproven) computational hardness assumptions. To some extent, this state
of affairs is unavoidable, since the security of almost any cryptographic primitive implies the existence
of one-way functions [Impagliazzo and Luby, 1989] (which in particular implies that P ̸= NP). Yet,
all various assumptions can essentially be divided into two main types: private key cryptography and
public key cryptography [Impagliazzo, 1995]. The former type is better understood: A series of works
have shown that the unstructured form of hardness guaranteed by one-way functions is sufficient
to construct many complex and structured primitives such as pseudorandom generators [Håstad
et al., 1999], pseudorandom functions [Goldreich et al., 1986] and permutations [Luby and Rackoff,
1988], commitment schemes [Naor, 1991, Haitner et al., 2009], universal one-way hash functions
[Rompel, 1990], zero-knowledge proofs [Goldreich et al., 1987], and more. However, reductions are
much less common outside the one-way functions regime, particularly when constructing public-key
primitives. In the famous work of Impagliazzo and Rudich [1989] they gave the first evidence that
public key cryptography assumptions are strictly stronger than one-way functions, by showing that
key-agreement, which enables two parties to exchange secret messages over open channels, cannot
be constructed from one-way functions in a black-box way."
PERSPECTIVE OF PUBLIC KEY CRYPTOGRAPHY,0.4127906976744186,"Our work shows that a balanced adversary for the ADA game that has the structure of all known
attacks, is a primitive that belongs to the public-key cryptography type. In particular, if public-key
cryptography does not exist, it could be possible to construct a computationally bounded mechanism
that can handle more than Θ(n2) adaptive queries of a balanced adversary (i.e., we currently do not
have a negative result that rules out this possibility)."
OTHER RELATED WORK,0.4186046511627907,"1.5
Other Related Work"
OTHER RELATED WORK,0.42441860465116277,"Nissim et al. [2018] presented a variant of the lower bound of Steinke and Ullman [2015a] that aims
to reduce the number of queries used by the attacker. However, their resulting lower bound only holds
for a certain family of mechanisms, and it does not rule out all computationally efficient mechanisms."
OTHER RELATED WORK,0.43023255813953487,"Dinur et al. [2023] revisited and generalized the lower bounds of Hardt and Ullman [2014] and
Steinke and Ullman [2015a] by showing that they are a consequence of a space bottleneck rather than
a sampling bottleneck. Yet, as in the works by Hardt, Steinke, and Ullman, the attack by Dinur et
al. relies on the ability to choose the underlying distribution D and inject secret trapdoors in it, and
hence it utilizes an imbalanced adversary."
OTHER RELATED WORK,0.436046511627907,"Recently, lower bounds constructions for the ADA problem were used as a tool for constructing
(conditional) lower bounds for other problems, such as the space complexity of adaptive streaming
algorithms [Kaplan et al., 2021] and the time complexity of dynamic algorithms Beimel et al. [2022].
Our lower bound for the ADA problem is qualitatively stronger than previous lower bounds (as
the adversary we construct has less power). Thus, our lower bound could potentially yield new
connections and constructions in additional settings."
CONCLUSION AND OPEN PROBLEMS,0.4418604651162791,"1.6
Conclusion and Open Problems"
CONCLUSION AND OPEN PROBLEMS,0.4476744186046512,"In this work we present the balanced adversarial model for the ADA problem, and show that the
existence of a balanced adversary that has the structure of all previously known attacks is equivalent
to the existence of public-key cryptography. Yet, we do not know what is the truth outside of the
public-key cryptography world. Can we present a different type of efficient attack that is based on
weaker hardness assumptions (like one-way functions)? Or is it possible to construct an efficient
mechanism that answer more than Θ(n2) adaptive queries assuming that public-key cryptography
does not exist? We also leave open similar questions regarding the information theoretic case. We
currently do not know whether it is possible to construct an unbounded mechanism that answers
exponential number of queries for any distribution D (regardless of its domain size)."
CONCLUSION AND OPEN PROBLEMS,0.45348837209302323,"In a broader perspective, lower bounds such as ours show that no general solution exists for a problem.
They often use unnatural inputs or distributions and rely on cryptographic assumptions. They are
important as guidance for how to proceed with a problem, e.g., search for mechanisms that would
succeed if the underlying distribution is from a ""nice"" family of distributions."
PRELIMINARIES,0.45930232558139533,"2
Preliminaries"
NOTATIONS,0.46511627906976744,"2.1
Notations"
NOTATIONS,0.47093023255813954,"We use calligraphic letters to denote sets and distributions, uppercase for random variables, and
lowercase for values and functions. For n ∈N, let [n] = {1, 2, . . . , n}. Let neg(n) stand for a
negligible function in n, i.e., a function ν(n) such that for every constant c > 0 and large enough n it
holds that ν(n) < n−c. For n ∈N we denote by 1n the n-size string 1 . . . 1 (n times). Let PPT stand
for probabilistic polynomial time. We say that a pair of algorithms A = (A1, A2) is PPT if both A1
and A2 are PPT algorithms."
DISTRIBUTIONS AND RANDOM VARIABLES,0.47674418604651164,"2.2
Distributions and Random Variables"
DISTRIBUTIONS AND RANDOM VARIABLES,0.48255813953488375,"Given a distribution D, we write x ∼D, meaning that x is sampled according to D. For a multiset
S, we denote by US the uniform distribution over S, and let x ←S denote that x ∼US. For
a distribution D and a value n ∈N, we denote by Dn the distribution of n i.i.d. samples from
D. For a distribution D over X and a query q: X →[−1, 1], we abuse notation and denote
q(D) := Ex∼D[q(x)], and similarly for S = (x1, . . . , xn) ∈X ∗we abuse notation and denote
q(S) := Ex←S[q(x)] = 1"
DISTRIBUTIONS AND RANDOM VARIABLES,0.4883720930232558,"n
Pn
i=1 xi."
CRYPTOGRAPHIC PRIMITIVES,0.4941860465116279,"2.3
Cryptographic Primitives"
KEY AGREEMENT PROTOCOLS,0.5,"2.3.1
Key Agreement Protocols"
KEY AGREEMENT PROTOCOLS,0.5058139534883721,"The most basic public-key cryptographic primitive is a (1-bit) key-agreement protocol, defined below."
KEY AGREEMENT PROTOCOLS,0.5116279069767442,"Definition 2.1 (key-agreement protocol). Let π be a two party protocol between two interactive PPT
algorithms P1 and P2, each outputs 1-bit. Let π(1n) denote a random execution of the protocol
on joint input 1n (the security parameter), and let O1
n, O2
n and Tn denote the random variables of
P1’s output, P2’s output, and the transcript (respectively) in this execution. We say that π is an
(α, β)-key-agreement protocol if the following holds for any PPT (“eavesdropper”) A and any n ∈N:"
KEY AGREEMENT PROTOCOLS,0.5174418604651163,"Agreement: Pr

O1
n = O2
n

≥α(n), and"
KEY AGREEMENT PROTOCOLS,0.5232558139534884,"Secrecy: Pr

A(Tn) = O1
n

≤β(n)."
KEY AGREEMENT PROTOCOLS,0.5290697674418605,"We say that π is a fully-secure key-agreement protocol if it is an (1 −neg(n), 1/2 + neg(n))-key-
agreement protocol."
IDENTITY-BASED ENCRYPTION,0.5348837209302325,"2.3.2
Identity-Based Encryption"
IDENTITY-BASED ENCRYPTION,0.5406976744186046,"An Identity-Based Encryption (IBE) scheme [Shamir, 1984, Cocks, 2001, Boneh and Franklin, 2001]
consists of four PPT algorithms (Setup, KeyGen, Encrypt, Decrypt) defined as follows:"
IDENTITY-BASED ENCRYPTION,0.5465116279069767,"Setup(1λ): given the security parameter λ, it outputs a master public key mpk and a master secret
key msk."
IDENTITY-BASED ENCRYPTION,0.5523255813953488,"KeyGen(msk, id): given the master secret key msk and an identity id ∈[n], it outputs a decryption
key skid."
IDENTITY-BASED ENCRYPTION,0.5581395348837209,"Encrypt(mpk, id, m): given the master public key mpk, and identity id ∈[n] and a message m, it
outputs a ciphertext ct."
IDENTITY-BASED ENCRYPTION,0.563953488372093,"Decrypt(skid, ct): given a secret key skid for identity id and a ciphertext ct, it outputs a string m."
IDENTITY-BASED ENCRYPTION,0.5697674418604651,The following are the properties of such an encryption scheme:
IDENTITY-BASED ENCRYPTION,0.5755813953488372,"Completeness: For all security parameter λ, identity id ∈[n] and a message m, with probability 1
over (mpk, msk) ∼Setup(1λ) and skid ∼KeyGen(msk, id) it holds that
Decrypt(skid, Encrypt(mpk, id, m)) = m"
IDENTITY-BASED ENCRYPTION,0.5813953488372093,"Security: For any PPT adversary A = (A1, A2) it holds that:"
IDENTITY-BASED ENCRYPTION,0.5872093023255814,"Pr[INDIBE
A
(1λ) = 1] ≤1/2 + neg(λ)"
IDENTITY-BASED ENCRYPTION,0.5930232558139535,"where INDIBE
A
is shown in Experiment 2.2.5"
IDENTITY-BASED ENCRYPTION,0.5988372093023255,"Experiment 2.2 (INDIBE
A
(1λ))."
IDENTITY-BASED ENCRYPTION,0.6046511627906976,"1. (mpk, msk) ∼Setup(1λ)."
IDENTITY-BASED ENCRYPTION,0.6104651162790697,"2. (id∗, (m0
1, . . . , m0
k), (m1
1, . . . , m1
k), st) ∼AKeyGen(msk,·)
1
(mpk) where
m0
i
 =
m1
i
 for
every i ∈[k] and for each query id by A1 to KeyGen(msk, ·) we have that id ̸= id∗.
3. Sample b ←{0, 1}.
4. Sample ct∗
i ∼Encrypt(mpk, id∗, mb
i) for every i ∈[k]."
IDENTITY-BASED ENCRYPTION,0.6162790697674418,"5. b′
∼
AKeyGen(msk,·)
2
(mpk, (ct∗
1, . . . ct∗
k), st) where for each query id by A2 to
KeyGen(msk, ·) we have that id ̸= id∗.
6. Output 1 if b = b′ and 0 otherwise."
IDENTITY-BASED ENCRYPTION,0.622093023255814,"Namely, the adversary chooses two sequences of messages (m0
1, . . . , m0
k) and (m1
1, . . . , m1
k), and
gets encryptions of either the first sequence or the second one, where the encryptions made for
identity id∗that the adversary does not hold its key (not allowed to query KeyGen on input id∗). The
security requirement means that she cannot distinguish between the two cases (except with negligible
probability).
Theorem 2.3 (Döttling and Garg [2021]). Assume that the Computational Diffie-Hellman (CDH)
Problem is hard. Then there exists an IBE scheme E = (Setup, KeyGen, Encrypt, Decrypt) for n
identities such that given a security parameter λ, the master keys and each decryption key are of size
O(λ · log n).6"
BALANCED ADAPTIVE DATA ANALYSIS,0.627906976744186,"2.4
Balanced Adaptive Data Analysis"
BALANCED ADAPTIVE DATA ANALYSIS,0.6337209302325582,"As described in the introduction, the mechanism plays a game with a balanced adversary that
consists of two (isolated) algorithms: a sampler A1, which chooses a distribution D over a domain
X and provides n i.i.d. samples to M, and an analyst A2, which asks the adaptive queries about the
distribution. Let ADAn,ℓ,X [M, A = (A1, A2)] denote Game 1.3 on public inputs n - the number of
samples, ℓ- the number of queries, and X - the domain. We denote by output 1 the case that M fails
in the game, and 0 otherwise. Since this work mainly deals with computationally bounded algorithms
that we would like to model as PPT algorithms, we provide n and ℓin unary representation. We also
assume for simplicity that X is finite, which allows to represent each element as a binary vector of
dimension ⌈log|X|⌉, and we provide the dimension in unary representation as well."
BALANCED ADAPTIVE DATA ANALYSIS,0.6395348837209303,"All previous negative results (Hardt and Ullman [2014], Steinke and Ullman [2015a], and Dinur et al.
[2023]) where achieved by reduction to a restricted family of mechanisms, called natural mechanisms
(Definition 1.8). These are algorithms that can only evaluate the query on the sample points they are
given."
BALANCED ADAPTIVE DATA ANALYSIS,0.6453488372093024,"For natural mechanisms (even unbounded ones), the following was proven.
Theorem 2.4 (Hardt and Ullman [2014], Steinke and Ullman [2015a]). There exists a pair of PPT
algorithms eA = (eA1, eA2) such that for every natural mechanism eM and every large enough n and
ℓ= Θ(n2) it holds that"
BALANCED ADAPTIVE DATA ANALYSIS,0.6511627906976745,"Pr
h
ADAn,ℓ,X=[2000n][ eM, eA] = 1
i
> 3/4.
(1)"
BALANCED ADAPTIVE DATA ANALYSIS,0.6569767441860465,"5The IBE security experiment is usually described as Experiment 2.2 with k = 1 (i.e., encrypting a single
message). Yet, it can be extended to any sequence of messages using a simple reduction.
6The construction can also be based on the hardness of factoring."
BALANCED ADAPTIVE DATA ANALYSIS,0.6627906976744186,"In particular, eA1 always chooses the uniform distribution over [2000n], and eA2 uses only queries
over the range {−1, 0, 1}."
CONSTRUCTING A BALANCED ADVERSARY VIA IBE,0.6686046511627907,"3
Constructing a Balanced Adversary via IBE"
CONSTRUCTING A BALANCED ADVERSARY VIA IBE,0.6744186046511628,"We prove that if an IBE scheme exists, then there is an efficient reduction to natural mechanisms that
holds against any PPT mechanism, yielding a general lower bound for the computational case."
CONSTRUCTING A BALANCED ADVERSARY VIA IBE,0.6802325581395349,"Theorem 3.1 (Restatement of Theorem 1.5). Assume the existence of an IBE scheme E that supports
m = 2000n identities with security parameter λ = λ(n) s.t. n ≤poly(λ) (e.g., λ = n0.1) using keys
of length k = k(n). Then there exists a PPT balanced adversary A = (A1, A2) and ℓ= Θ(n2) + k
such that for every PPT mechanism M it holds that"
CONSTRUCTING A BALANCED ADVERSARY VIA IBE,0.686046511627907,"Pr
h
ADAn,ℓ,X=[m]×{0,1}2k[M, A] = 1
i
> 3/4 −neg(n)."
CONSTRUCTING A BALANCED ADVERSARY VIA IBE,0.6918604651162791,"The proof of the theorem is given in the supplementary material. Note that we use a domain X with
log|X| = 2k+log n+O(1), and by applying Theorem 2.3, the lower bound holds for k = O(λ·log n)
under the CDH hardness assumption."
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.6976744186046512,"4
Reduction to Natural Mechanisms Implies Key Agreement"
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7034883720930233,"We prove that any PPT balanced adversary A = (A1, A2) that has the structure of all known lower
bounds (Hardt and Ullman [2014], Steinke and Ullman [2015a], Dinur et al. [2023] and ours in
Section 3), can be used to construct a key-agreement protocol."
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7093023255813954,"All known constructions use an adversary A that wraps the adversary eA for the natural mechanisms
case (Theorem 2.4) by forcing every mechanism M to behave naturally using cryptography. In
particular, they all satisfy Properties 1 and 2 from Section 1.3.2."
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7151162790697675,"The formal statement is given in the following theorem. The proof is provided in the supplementary
material."
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7209302325581395,"Theorem 4.1 (Restatement of Theorem 1.6). Assume the existence of a PPT adversary A = (A1, A2)
and functions ℓ= ℓ(n) ≤poly(n) and X = X(n) with log|X| ≤poly(n) such that the following
holds: Let n ∈N and consider a random execution of ADAn,ℓ,X [M, A] where M is the mechanism
that given a sample S and a query q, answers the empirical mean q(S). Let Dn and Qn be the (r.v.’s
of the) values of D and q = qℓ(the last query) in the execution (respectively), let Tn be the transcript
of the execution between the analyst A2 and the mechanism M (i.e., the queries and answers), and let
Vn be the view of A2 at the end of the execution (without loss of generality, its input, random coins
and the transcript). Assume that"
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7267441860465116,"1. ∃PPT algorithm F s.t. ∀n ∈N : Pr

|F(Vn) −Qn(Dn)| ≤n−1/10
≥1 −neg(n), and"
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7325581395348837,2. ∀PPT algorithm G and ∀n ∈N : Pr[|G(Tn) −Qn(Dn)| ≤1/10] ≤1/4 + neg(n).
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7383720930232558,Then using A and F it is possible to construct a fully-secure key-agreement protocol.
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7441860465116279,"Note that Assumption 1 in Theorem 4.1 formalizes the first property in which the analyst knows a
good estimation of the true answer, and the PPT algorithm F is the assumed knowledge extractor.
Assumption 2 in Theorem 4.1 formalizes the second property which states that the mechanism, which
answers the empirical mean along the way, will fail in the last query, no matter how it chooses to act
(this behavior is captured with the PPT algorithm G), and moreover, it is enough to assume that this
requirement only holds with respect to to transcript of the execution, and not with respect to the view
of the mechanism."
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.75,"We refer to the full version of the paper, given in the supplementary material, for all the missing
proofs."
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7558139534883721,Acknowledgments and Disclosure of Funding
REDUCTION TO NATURAL MECHANISMS IMPLIES KEY AGREEMENT,0.7616279069767442,"Kobbi Nissim is partially supported by NSF grant No. CNS-2001041 and a gift to Georgetown
University. Uri Stemmer is partially supported by the Israel Science Foundation (grant 1871/19) and
by Len Blavatnik and the Blavatnik Family foundation. Eliad Tsfadia is partially supported by the
Fulbright Program and a gift to Georgetown University."
REFERENCES,0.7674418604651163,References
REFERENCES,0.7732558139534884,"R. Bassily, K. Nissim, A. D. Smith, T. Steinke, U. Stemmer, and J. Ullman. Algorithmic stability for
adaptive data analysis. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2016, pages 1046–1059, 2016."
REFERENCES,0.7790697674418605,"A. Beimel, H. Kaplan, Y. Mansour, K. Nissim, T. Saranurak, and U. Stemmer. Dynamic algorithms
against an adaptive adversary: generic constructions and lower bounds. In STOC, pages 1671–1684.
ACM, 2022."
REFERENCES,0.7848837209302325,"G. Blanc. Subsampling suffices for adaptive data analysis. CoRR, abs/2302.08661, 2023."
REFERENCES,0.7906976744186046,"D. Boneh and M. K. Franklin. Identity-based encryption from the weil pairing. In J. Kilian, editor,
Advances in Cryptology - CRYPTO 2001, 21st Annual International Cryptology Conference,
Proceedings, volume 2139, pages 213–229, 2001."
REFERENCES,0.7965116279069767,"C. C. Cocks. An identity based encryption scheme based on quadratic residues. In Cryptography and
Coding, 8th IMA International Conference, Proceedings, volume 2260, pages 360–363, 2001."
REFERENCES,0.8023255813953488,"Y. Dagan and G. Kur. A bounded-noise mechanism for differential privacy. In Conference on
Learning Theory, volume 178, pages 625–661. PMLR, 2022."
REFERENCES,0.8081395348837209,"W. Diffie and M. E. Hellman. New directions in cryptography. IEEE Trans. Inf. Theory, 22(6):
644–654, 1976."
REFERENCES,0.813953488372093,"I. Dinur, U. Stemmer, D. P. Woodruff, and S. Zhou. On differential privacy and adaptive data analysis
with bounded space. CoRR, abs/2302.05707, 2023."
REFERENCES,0.8197674418604651,"N. Döttling and S. Garg. Identity-based encryption from the diffie-hellman assumption. J. ACM, 68
(3):14:1–14:46, 2021."
REFERENCES,0.8255813953488372,"C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. Roth. Generalization in adaptive
data analysis and holdout reuse. In Advances in Neural Information Processing Systems (NIPS),
2015a."
REFERENCES,0.8313953488372093,"C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. Roth. The reusable holdout:
Preserving validity in adaptive data analysis. Science, 349(6248):636–638, 2015b."
REFERENCES,0.8372093023255814,"C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. L. Roth. Preserving statistical
validity in adaptive data analysis. In STOC, pages 117–126. ACM, 2015c."
REFERENCES,0.8430232558139535,"S. Elder. Challenges in bayesian adaptive data analysis. CoRR, abs/1604.02492, 2016. URL
http://arxiv.org/abs/1604.02492."
REFERENCES,0.8488372093023255,"V. Feldman and T. Steinke. Generalization for adaptively-chosen estimators via stable median. In
Proceedings of the 30th Conference on Learning Theory, COLT 2017, volume 65, pages 728–757.
PMLR, 2017."
REFERENCES,0.8546511627906976,"V. Feldman and T. Steinke. Calibrating noise to variance in adaptive data analysis. In Conference On
Learning Theory, COLT 2018, volume 75, pages 535–544. PMLR, 2018."
REFERENCES,0.8604651162790697,"B. Fish, L. Reyzin, and B. I. P. Rubinstein. Sampling without compromising accuracy in adaptive
data analysis. In Algorithmic Learning Theory, ALT 2020, volume 117, pages 297–318. PMLR,
2020."
REFERENCES,0.8662790697674418,"O. Goldreich, S. Goldwasser, and S. Micali. How to construct random functions. Journal of the ACM,
33(4):792–807, 1986."
REFERENCES,0.872093023255814,"O. Goldreich, S. Micali, and A. Wigderson. How to play any mental game or A completeness theorem
for protocols with honest majority. In Proceedings of the 19th Annual ACM Symposium on Theory
of Computing, STOC 1987, pages 218–229, 1987."
REFERENCES,0.877906976744186,"I. Haitner, M. Nguyen, S. J. Ong, O. Reingold, and S. Vadhan. Statistically hiding commitments and
statistical zero-knowledge arguments from any one-way function. SIAM Journal on Computing,
39(3):1153–1218, 2009."
REFERENCES,0.8837209302325582,"M. Hardt and G. Rothblum. A multiplicative weights mechanism for privacy-preserving data analysis.
In Proc. 51st Foundations of Computer Science (FOCS), pages 61–70. IEEE, 2010."
REFERENCES,0.8895348837209303,"M. Hardt and J. Ullman. Preventing false discovery in interactive data analysis is hard. In FOCS,
pages 454–463, 2014."
REFERENCES,0.8953488372093024,"J. Håstad, R. Impagliazzo, L. A. Levin, and M. Luby. A pseudorandom generator from any one-way
function. SIAM Journal on Computing, 28(4):1364–1396, 1999."
REFERENCES,0.9011627906976745,"R. Impagliazzo. A personal view of average-case complexity. In Proceedings of the Tenth Annual
Structure in Complexity Theory Conference, pages 134–147. IEEE Computer Society, 1995."
REFERENCES,0.9069767441860465,"R. Impagliazzo and M. Luby. One-way functions are essential for complexity based cryptography
(extended abstract). In 30th Annual Symposium on Foundations of Computer Science, FOCS 1989,
pages 230–235, 1989."
REFERENCES,0.9127906976744186,"R. Impagliazzo and S. Rudich. Limits on the provable consequences of one-way permutations. In
Annual ACM Symposium on Theory of Computing (STOC), pages 44–61, 1989."
REFERENCES,0.9186046511627907,"C. Jung, K. Ligett, S. Neel, A. Roth, S. Sharifi-Malvajerdi, and M. Shenfeld. A new analysis of
differential privacy’s generalization guarantees. In ITCS, volume 151 of LIPIcs, pages 31:1–31:17.
Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2020."
REFERENCES,0.9244186046511628,"H. Kaplan, Y. Mansour, K. Nissim, and U. Stemmer. Separating adaptive streaming from oblivious
streaming using the bounded storage model. In CRYPTO (3), volume 12827 of Lecture Notes in
Computer Science, pages 94–121. Springer, 2021."
REFERENCES,0.9302325581395349,"A. Kontorovich, M. Sadigurschi, and U. Stemmer. Adaptive data analysis with correlated observations.
In ICML, volume 162 of Proceedings of Machine Learning Research, pages 11483–11498. PMLR,
2022."
REFERENCES,0.936046511627907,"M. Luby and C. Rackoff. How to construct pseudorandom permutations from pseudorandom
functions. SIAM Journal on Computing, 17(2):373–386, 1988."
REFERENCES,0.9418604651162791,"M. Naor. Bit commitment using pseudorandomness. Journal of Cryptology, 4(2):151–158, 1991."
REFERENCES,0.9476744186046512,"K. Nissim, A. D. Smith, T. Steinke, U. Stemmer, and J. R. Ullman. The limits of post-selection
generalization. In NeurIPS, pages 6402–6411, 2018."
REFERENCES,0.9534883720930233,"R. M. Rogers, A. Roth, A. D. Smith, and O. Thakkar. Max-information, differential privacy, and
post-selection hypothesis testing. In IEEE 57th Annual Symposium on Foundations of Computer
Science, FOCS 2016, pages 487–494, 2016."
REFERENCES,0.9593023255813954,"J. Rompel. One-way functions are necessary and sufficient for secure signatures. In Annual ACM
Symposium on Theory of Computing (STOC), pages 387–394, 1990."
REFERENCES,0.9651162790697675,"D. Russo and J. Zou. Controlling bias in adaptive data analysis using information theory. In
Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS
2016, volume 51, pages 1232–1240. JMLR.org, 2016."
REFERENCES,0.9709302325581395,"A. Shamir.
Identity-based cryptosystems and signature schemes.
In Advances in Cryptology,
Proceedings of CRYPTO 1984, volume 196, pages 47–53, 1984."
REFERENCES,0.9767441860465116,"M. Shenfeld and K. Ligett. A necessary and sufficient stability notion for adaptive generalization. In
NeurIPS, pages 11481–11490, 2019."
REFERENCES,0.9825581395348837,"A. D. Smith. Information, privacy and stability in adaptive data analysis. CoRR, abs/1706.00820,
2017."
REFERENCES,0.9883720930232558,"T. Steinke and J. Ullman. Interactive fingerprinting codes and the hardness of preventing false
discovery. In COLT, pages 1588–1628, 2015a."
REFERENCES,0.9941860465116279,"T. Steinke and J. Ullman. Between pure and approximate differential privacy. CoRR, abs/1501.06095,
2015b."
