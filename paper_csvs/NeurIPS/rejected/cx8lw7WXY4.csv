Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002145922746781116,"Texture editing is a crucial task in 3D modeling that allows users to automatically
1"
ABSTRACT,0.004291845493562232,"manipulate the surface properties of 3D models. However, the inherent complexity
2"
ABSTRACT,0.006437768240343348,"of 3D models and the ambiguous text description lead to the challenge in this
3"
ABSTRACT,0.008583690987124463,"task. To address this challenge, we propose ITEM3D, an illumination-aware model
4"
ABSTRACT,0.01072961373390558,"for automatic 3D object editing according to the text prompts. Leveraging the
5"
ABSTRACT,0.012875536480686695,"power of the diffusion model, ITEM3D takes the rendered images as the bridge
6"
ABSTRACT,0.015021459227467811,"of text and 3D representation, and further optimizes the disentangled texture and
7"
ABSTRACT,0.017167381974248927,"environment map. Previous methods adopt the absolute editing direction namely
8"
ABSTRACT,0.019313304721030045,"score distillation sampling (SDS) as the optimization objective, which unfortunately
9"
ABSTRACT,0.02145922746781116,"results in the noisy appearance and text inconsistency. To solve the problem caused
10"
ABSTRACT,0.023605150214592276,"by the ambiguous text, we introduce a relative editing direction, an optimization
11"
ABSTRACT,0.02575107296137339,"objective defined by the noise difference between the source and target texts, to
12"
ABSTRACT,0.027896995708154508,"release the semantic ambiguity between the texts and images. Additionally, we
13"
ABSTRACT,0.030042918454935622,"gradually adjust the direction during optimization to further address the unexpected
14"
ABSTRACT,0.032188841201716736,"deviation in the texture domain. Qualitative and quantitative experiments show that
15"
ABSTRACT,0.034334763948497854,"our ITEM3D outperforms SDS-based methods on various 3D objects. We also
16"
ABSTRACT,0.03648068669527897,"perform text-guided relighting to show explicit control over lighting.
17"
INTRODUCTION,0.03862660944206009,"1
Introduction
18"
INTRODUCTION,0.0407725321888412,"Texture editing is an important task in 3D modeling that involves manipulating the surface properties
19"
INTRODUCTION,0.04291845493562232,"of 3D models to create a visually fantastic and appealing appearance according to the user’s ideas.
20"
INTRODUCTION,0.045064377682403435,"With the increasing applications of 3D models in entertainment and e-shopping, how to automatically
21"
INTRODUCTION,0.04721030042918455,"generate and edit the texture of a 3D model without manual effort becomes an appealing task in the
22"
INTRODUCTION,0.04935622317596566,"field of 3D vision. However, this task is challenging due to the complexity of 3D models and the
23"
INTRODUCTION,0.05150214592274678,"special representation of the texture.
24"
INTRODUCTION,0.0536480686695279,"To sufficiently handle the above applications, it would be desirable if a texture editing method can
25"
INTRODUCTION,0.055793991416309016,"fulfill the following aspects: 1) Realism: The generated textures should give rise to realistic and
26"
INTRODUCTION,0.05793991416309013,"visually natural 2D images after rendering. It requires generative models to capture the complex
27"
INTRODUCTION,0.060085836909871244,"patterns and structures present in the textures of the 3D model. 2) Relighting: The relighting ability
28"
INTRODUCTION,0.06223175965665236,"allows adjusting the lighting conditions of the edited model to be consistent with the changes made to
29"
INTRODUCTION,0.06437768240343347,"its texture. 3) Efficiency: Texture editing should be efficient and scalable. This requires the use of
30"
INTRODUCTION,0.06652360515021459,"fast and memory-efficient generative models that can generate high-quality textures in a short time.
31"
INTRODUCTION,0.06866952789699571,"Recent advances have demonstrated the effectiveness of generative models in synthesizing high-
32"
INTRODUCTION,0.07081545064377683,"quality textures that are both visually pleasing and semantically meaningful. The use of generative
33"
INTRODUCTION,0.07296137339055794,"adversarial networks (GANs) (50; 2; 43; 7) has shown promising results in producing textures with
34"
INTRODUCTION,0.07510729613733906,"intricate patterns and complex structures. Other approaches, such as texture synthesis via direct
35"
INTRODUCTION,0.07725321888412018,"optimization (8; 9; 40; 54) or neural style transfer (3; 14; 48; 24), have also been explored for their
36"
INTRODUCTION,0.07939914163090128,"ability to generate textures with specific artistic styles. However, the capacity of these models is still
37"
INTRODUCTION,0.0815450643776824,"unable to meet the need of real-world applications, which requires high-quality and diverse textures.
38"
INTRODUCTION,0.08369098712446352,"Meanwhile, recent researches (6; 34; 25; 20; 21; 17; 23) on the diffusion models have emerged as a
39"
INTRODUCTION,0.08583690987124463,"powerful new family of generative methods, which achieve impressive generation results in natural
40"
INTRODUCTION,0.08798283261802575,"images and videos, inspiring us to introduce the awesome power into the task of 3D modeling.
41"
INTRODUCTION,0.09012875536480687,"However, directly applying the diffusion model to 3D objects is a non-trivial task due to the following
42"
INTRODUCTION,0.09227467811158799,"reasons. 1) The gap between the 3D representation and natural images. Existing diffusion
43"
INTRODUCTION,0.0944206008583691,"models are typically trained with natural images, making the pre-trained diffusion model lack prior
44"
INTRODUCTION,0.09656652360515021,"knowledge in the 3D domain. Moreover, due to the complexity of the 3D model, it would be
45"
INTRODUCTION,0.09871244635193133,"difficult to simultaneously edit shape, appearance, and shading, sometimes leading to conflicts in
46"
INTRODUCTION,0.10085836909871244,"optimization goals. Therefore, directly editing the 3D representation may cause extreme semantic
47"
INTRODUCTION,0.10300429184549356,"bias and destruction of inherent 3D topology. 2) The learning misdirection of text description. It is
48"
INTRODUCTION,0.10515021459227468,"hard for text prompts to exactly describe the target images at the pixel level, leading to an ambiguous
49"
INTRODUCTION,0.1072961373390558,"direction when taking the rendered images as the bridge.
50"
INTRODUCTION,0.10944206008583691,"To solve these problems, we present an efficient model, dubbed ITEM3D, which can generate visually
51"
INTRODUCTION,0.11158798283261803,"natural texture corresponding to the text prompt generated by users. Instead of directly applying the
52"
INTRODUCTION,0.11373390557939914,"diffusion model for texture editing in the 2D space, we adopt rendered images as the intermediary that
53"
INTRODUCTION,0.11587982832618025,"bridges the text prompts and the appearance of 3D models. Apart from the appearance, the lighting
54"
INTRODUCTION,0.11802575107296137,"and shading are also key components influencing the rendering results. Therefore, we represent the
55"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.12017167381974249,"3D model into a triangular mesh and a set of disentangled materials consisting of the texture and an
56"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.1223175965665236,"environment map using nvdiffrec (29), which achieves a balance for representing both appearance
57"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.12446351931330472,"and shading.
58"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.12660944206008584,"To optimize a texture and an environment map with the diffusion model, a naive idea is to adopt
59"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.12875536480686695,"the score distillation sampling (SDS) like 2D diffusion-based editing methods, which represents
60"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.13090128755364808,"the absolute direction. Unfortunately, the absolute direction often leads to noisy details and an
61"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.13304721030042918,"inconsistent appearance, due to the ambiguous description of the text prompt for the target images.
62"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.1351931330472103,"Inspired by the recent improvement (13), we replace the absolute editing direction led by the score
63"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.13733905579399142,"distillation sampling with a relative editing direction determined by two predicted noises under the
64"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.13948497854077252,"condition of the source text and the target text respectively, as illustrated in Fig. 1 (a). In this way,
65"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.14163090128755365,"our model enables us to edit the texture in obedience to the text while bypassing the inconsistency
66"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.14377682403433475,"problem by releasing the ambiguous description. It is ideal that the intermediate states between the
67"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.1459227467811159,"source and target text can give relatively accurate descriptions for arbitrary rendered images during
68"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.148068669527897,"the optimization, like the green straight lines in Fig. 1 (b). However, the optimization in the texture
69"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.15021459227467812,"domain actually shows an unexpected offset of the appearance in rendered images, leading to the
70"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.15236051502145923,"deviation from the determined direction, like the red line in Fig. 1 (b). To reduce the deviation caused
71"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.15450643776824036,"by the texture projection, we gradually adjust the editing direction during the optimization, as green
72"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.15665236051502146,"fold lines shown in Fig. 1 (b). With the advent of the textural-inversion model, it can be easy to
73"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.15879828326180256,"automatically correct the description as the change of the texture and its rendered images.
74"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.1609442060085837,"Thanks to the proposed solutions, our method overcomes the challenges of domain gap and learning
75"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.1630901287553648,"misdirection, fulfilling all three requirements of texture editing. In summary, our contributions are:
76"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.16523605150214593,"• We design an efficient optimization pipeline to edit the texture and environment map obedient
77"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.16738197424892703,"to the text prompt, directly empowering the downstream application in the industrial pipeline.
78"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.16952789699570817,"• We introduce the relative direction to the 3D texture optimization, releasing the problem of
79"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.17167381974248927,"noisy details and inconsistent appearance caused by the semantic ambiguity between the texts
80"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.17381974248927037,"and images.
81"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.1759656652360515,"• We propose to gradually adjust the relative direction guided by the source and target texts
82"
D MODEL INTO A TRIANGULAR MESH AND A SET OF DISENTANGLED MATERIALS CONSISTING OF THE TEXTURE AND AN,0.1781115879828326,"which addresses the unexpected deviation from the determined direction caused by the texture.
83"
RELATED WORK,0.18025751072961374,"2
Related Work
84"
RELATED WORK,0.18240343347639484,"3D Model Representation. From the perspective of 3D representations, traditional methods typically
85"
RELATED WORK,0.18454935622317598,"exploit point clouds or meshes to estimate depth maps (1; 38; 11) or employ a voxel grid and estimate
86"
RELATED WORK,0.18669527896995708,"the corresponding occupancy and color (39; 4). However, these methods are often limited to the
87"
RELATED WORK,0.1888412017167382,"memory requirement, which results in excessive runtime. With the development of computer vision,
88"
RELATED WORK,0.19098712446351931,"neural implicit representations are brought up and leverage differentiable rendering to reconstruct
89"
RELATED WORK,0.19313304721030042,"Figure 1: Motivation. (a) Previous methods (31; 6) with SDS Loss to directly guide the optimization
leads to ambiguous details due to the bias between texts and images (red line), while our method in-
troduces the relative direction between source and target texts to the optimization process, eliminating
the bias and improving the rendering results (green line). (b) The optimization in the texture domain
gives rise to the deviation of the target direction (red line), thus we gradually adjust the direction to
fine-tune the optimization (fold green line)."
RELATED WORK,0.19527896995708155,"3D geometry with appearance. Neural Radiance Field (27) and followup methods (52; 47; 28; 5;
90"
RELATED WORK,0.19742489270386265,"49; 51; 33; 22), utilize volumetric representations and a neural encoded field to compute radiance
91"
RELATED WORK,0.19957081545064378,"by ray marching. While these NeRF-based methods synthesize high-fidelity rendering results, the
92"
RELATED WORK,0.2017167381974249,"quality of the generated geometry is limited due to the ambiguity of volume rendering. Meanwhile,
93"
RELATED WORK,0.20386266094420602,"surface-based methods (30; 46; 10) optimize the underlying surface directly. These methods usually
94"
RELATED WORK,0.20600858369098712,"rely on volumetric representation and utilize an implicit surface by converging the volumetric
95"
RELATED WORK,0.20815450643776823,"representation (30) or constructing a field that converse SDF into density (46; 10). Though surface-
96"
RELATED WORK,0.21030042918454936,"based methods achieve better geometry than NeRF-based methods, they require excessive computation
97"
RELATED WORK,0.21244635193133046,"runtime since they rely too much on the ray-marching mechanism. Apart from implicit neural
98"
RELATED WORK,0.2145922746781116,"representations, there also exist approaches that utilize explicit surface representations to estimate
99"
RELATED WORK,0.2167381974248927,"explicit mesh from images. To extend such methods that originally built upon a fixed mesh topology,
100"
RELATED WORK,0.21888412017167383,"DMTet (41) employs a differentiable marching tetrahedral layer and optimizes the surface mesh
101"
RELATED WORK,0.22103004291845493,"directly. Nvdiffrec (29) further extends DMTet by jointly optimizing mesh topology, materials, and
102"
RELATED WORK,0.22317596566523606,"lighting. ITEM3D leverages an explicit mesh representation and optimizes texture and environment
103"
RELATED WORK,0.22532188841201717,"map. By supporting the decomposition of shape, materials and lighting, ITEM3D supports texture
104"
RELATED WORK,0.22746781115879827,"editing while preserving the topology by design. Additionally, ITEM3D employs an efficient
105"
RELATED WORK,0.2296137339055794,"differentiable rasterization pipeline for faster optimization.
106"
RELATED WORK,0.2317596566523605,"2D Diffusion-based Image Editing. Owing to the remarkable generalization ability of the diffusion
107"
RELATED WORK,0.23390557939914164,"model, a growing number of works (23; 17; 35; 36; 42) emerged to create customized images with
108"
RELATED WORK,0.23605150214592274,"specific styles or objects, as well as stunning images based on text descriptions. All these methods
109"
RELATED WORK,0.23819742489270387,"rely on the diffusion process by either fine-tuning the diffusion model or refining the target embedding
110"
RELATED WORK,0.24034334763948498,"to reach the desired image domain. SDEdit (23) denoises the noisy image through a diffusion process
111"
RELATED WORK,0.24248927038626608,"under the given description. DDIB (42) first converts the input image into a latent representation
112"
RELATED WORK,0.2446351931330472,"using origin text and subsequently translates the latent into the desired image with the target text.
113"
RELATED WORK,0.24678111587982832,"ControlNet (53) trains a controlling module to augment images with additional conditions that
114"
RELATED WORK,0.24892703862660945,"improve the controllability of the editing process. DiffusionCLIP (18) fine-tunes the diffusion model,
115"
RELATED WORK,0.2510729613733906,"which translate the image from a pretrained domain to a target text domain. Imagic (17) fine-tunes
116"
RELATED WORK,0.2532188841201717,"both the text embeddings and the diffusion model to ensure more stable editing. Unlike these methods
117"
RELATED WORK,0.2553648068669528,"that optimize in 2D image space, our ITEM3D utilizes the pre-trained diffusion model as a prior for
118"
RELATED WORK,0.2575107296137339,"3D texture optimization.
119"
RELATED WORK,0.259656652360515,"3D Text-guided Generation. With the advent of large text-image models, i.e., the CLIP, recent
120"
RELATED WORK,0.26180257510729615,"works (45; 37; 16; 15; 26) have made impressive progress on 3D text-driven synthesis. The majority
121"
RELATED WORK,0.26394849785407726,"of methods adopt the optimization procedures supervised by the CLIP similarity (32). Specifically,
122"
RELATED WORK,0.26609442060085836,"CLIP-NeRF (45) proposes a unified framework to manipulate NeRF, guided by a text prompt or an
123"
RELATED WORK,0.26824034334763946,"example image. Similarly, CLIP-Mesh applies the explicit textured mesh as a 3D representation, able
124"
RELATED WORK,0.2703862660944206,"to deform the shape along with its texture corresponding to the text. Apart from the CLIP-based
125"
RELATED WORK,0.27253218884120173,"method, the diffusion model (35) recently inspires huge breakthroughs in 3D text-guided generation.
126"
RELATED WORK,0.27467811158798283,"Latent-NeRF (25) utilizes the score distillation sampling to bring the NeRF representation to the
127"
RELATED WORK,0.27682403433476394,"latent space, showing impressive generation results of the combination between diffusion model and
128"
RELATED WORK,0.27896995708154504,"NeRF. TEXTure (34) takes an iterative scheme to paint a 3D model from different viewpoints based
129"
RELATED WORK,0.2811158798283262,"on a pre-trained depth-to-image diffusion model. Fantasia3D (6) leverages the disentangled modeling
130"
RELATED WORK,0.2832618025751073,"and learns the geometry and appearance supervised by the score distillation sampling. However,
131"
RELATED WORK,0.2854077253218884,"these SDS-based methods often produce non-detailed and blurry outputs due to noisy gradients. In
132"
RELATED WORK,0.2875536480686695,"contrast, our ITEM3D uses the relative direction to eliminate the semantic ambiguity of the target
133"
RELATED WORK,0.28969957081545067,"prompt towards the texture.
134"
METHOD,0.2918454935622318,"3
Method
135"
OVERVIEW,0.2939914163090129,"3.1
Overview
136"
OVERVIEW,0.296137339055794,"Given a set of multi-view images I = {I1, ..., In}, we aim to reconstruct the 3D model with both
137"
OVERVIEW,0.2982832618025751,"geometry and texture, and then edit the texture under the guidance of text prompts. To this end, we
138"
OVERVIEW,0.30042918454935624,"design a zero-shot differentiable framework that optimizes the disentangled materials of the object,
139"
OVERVIEW,0.30257510729613735,"i.e., texture map and environment map. We first leverage a differentiable rendering model R to
140"
OVERVIEW,0.30472103004291845,"represent the 3D model as an accurate shape and surface materials with texture and environment map
141"
OVERVIEW,0.30686695278969955,"Sec. 3.2). For further editing of appearance, we utilize the diffusion model to guide the direction
142"
OVERVIEW,0.3090128755364807,"of the texture optimization given the target text prompt. To solve the problem of ambiguous and
143"
OVERVIEW,0.3111587982832618,"noisy details, we introduce the relative direction of source text and target text into the optimization
144"
OVERVIEW,0.3133047210300429,"(Sec. 3.3). Moreover, we gradually adjust relative direction to address the challenges of deviation
145"
OVERVIEW,0.315450643776824,"caused by the unbalanced optimization in the texture domain (Sec. 3.4). The overview of our method
146"
OVERVIEW,0.31759656652360513,"is demonstrated in Fig. 2.
147"
OVERVIEW,0.3197424892703863,"3.2
3D Model Representation
148"
OVERVIEW,0.3218884120171674,"To accomplish editing the appearance of the 3D model via text prompt, we disentangle the 3D model
149"
OVERVIEW,0.3240343347639485,"into a triangular mesh and a set of spatially varying materials. The disentanglement thus allows us
150"
OVERVIEW,0.3261802575107296,"to edit the texture directly while keeping the geometry invariant. The material model we employed
151"
OVERVIEW,0.3283261802575107,"combines a diffuse term, a specular term and a normal term. A four-channel texture is provided for the
152"
OVERVIEW,0.33047210300429186,"diffuse parameters kd, where the optional fourth channel α represents the transparency. Meanwhile,
153"
OVERVIEW,0.33261802575107297,"the specular term is described by a roughness factor r, a metalness factor m and a sheen factor o
154"
OVERVIEW,0.33476394849785407,"that is unused in our model. These values (o, r, m) are stored in another texture map korm. The
155"
OVERVIEW,0.3369098712446352,"normal term in our representation is a tangent space normal map n, which is utilized to capture the
156"
OVERVIEW,0.33905579399141633,"high-frequency details of the appearance. In order to handle texturing effectively during optimization,
157"
OVERVIEW,0.34120171673819744,"we utilize volumetric texturing and access our texture by the world space position x. We tackle the
158"
OVERVIEW,0.34334763948497854,"challenge of the impractical cubic growth in memory usage of volumetric textures for our target
159"
OVERVIEW,0.34549356223175964,"resolution by leveraging a multi-layer perceptron (MLP) to encode the material parameters into a
160"
OVERVIEW,0.34763948497854075,"compact representation. Specifically, given a world space position x, we compute the base color,
161"
OVERVIEW,0.3497854077253219,"kd, the specular parameters, korm and a tangent space normal perturbation n, the mapping is thus
162"
OVERVIEW,0.351931330472103,"formulated as x →(kd, korm, n). With the introduction of this mapping, for a fixed topology, the
163"
OVERVIEW,0.3540772532188841,"textures are initialized by sampling the MLP on the mesh surface and then optimized efficiently.
164"
OVERVIEW,0.3562231759656652,"Following the rendering equation of the image-based lighting model, we compute the radiance L in
165"
OVERVIEW,0.3583690987124464,"direction ωo by:
166"
OVERVIEW,0.3605150214592275,"L (ωo) =
Z"
OVERVIEW,0.3626609442060086,"Ω
Li (ωi) f (ωi, ωo) (ωi · n) dωi,
(1)"
OVERVIEW,0.3648068669527897,"where Li is the incident radiance from direction ωi , the f is the BSDF and n is the intersection
167"
OVERVIEW,0.3669527896995708,"normal of the corresponding integral domain Ω. Specifically, we adopt the Cook-Torrance microfacet
168"
OVERVIEW,0.36909871244635195,"specular shading model as the BSDF in our rendering equation:
169"
OVERVIEW,0.37124463519313305,"f (ωi, ωo) =
D G F
4 (ωo · n) (ωi · n).
(2)"
OVERVIEW,0.37339055793991416,"The term D here represents the GGX (44) normal distribution while the term G is the geometric atten-
170"
OVERVIEW,0.37553648068669526,"uation and F is the Fresnel term respectively. Furthermore, we employ the split-sum approximation
171"
OVERVIEW,0.3776824034334764,"for its efficiency and the rendering equation Eq. (1) can be formulated as:
172"
OVERVIEW,0.3798283261802575,"L (ωo) ≈
Z"
OVERVIEW,0.38197424892703863,"Ω
f (ωi, ωo) (ωi · n) dωi Z"
OVERVIEW,0.38412017167381973,"Ω
Li (ωi) D (ωi, ωo) (ωi · n) dωi.
(3)"
OVERVIEW,0.38626609442060084,"Figure 2: Pipeline of texture editing. We render the 3D model with mesh, texture, and environment
map into 2D images which are then added with noise ϵ. We then use the source text and the target
text as the conditions to denoise via two U-Nets. The difference between the two predicted noises
serve as the relative direction to guide the optimization of the materials of the 3D model, i.e., texture
and environment map."
OVERVIEW,0.388412017167382,"The first term of this product only relies on the parameters (ωi · n) and the roughness r of the BSDF,
173"
OVERVIEW,0.3905579399141631,"which are precomputed and stored in a 2D lookup texture. Meanwhile, the second term is the integral
174"
OVERVIEW,0.3927038626609442,"of the radiance with the specular normal distribution function D expressed in Eq. (2), which is also
175"
OVERVIEW,0.3948497854077253,"precomputed and stored by a filtered cubemap. Owing to the precomputation and lookup mechanism,
176"
OVERVIEW,0.3969957081545064,"the rendering process is then accelerated. In order to learn the environment lighting from 2D image
177"
OVERVIEW,0.39914163090128757,"observations, we employ a differentiable shading model to represent this split-sum approximation.
178"
OVERVIEW,0.4012875536480687,"The cube map in our case can be represented as trainable parameters, which are initialized as the
179"
OVERVIEW,0.4034334763948498,"preintegrated lighting.
180"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4055793991416309,"3.3
Relative Direction Based Optimization
181"
RELATIVE DIRECTION BASED OPTIMIZATION,0.40772532188841204,"Our goal is to enable users to edit the appearance of 3D models using natural language descriptions.
182"
RELATIVE DIRECTION BASED OPTIMIZATION,0.40987124463519314,"To accomplish this, the directional idea is to utilize the diffusion model that has been pre-trained in
183"
RELATIVE DIRECTION BASED OPTIMIZATION,0.41201716738197425,"2D images as knowledge prior to guide the editing of texture. Naively, we could use Score Distillation
184"
RELATIVE DIRECTION BASED OPTIMIZATION,0.41416309012875535,"Sampling (SDS) loss,
185"
RELATIVE DIRECTION BASED OPTIMIZATION,0.41630901287553645,"∇θLSDS(ϕ, x = R(θ)) = Et,ϵ"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4184549356223176,"
w(t)
 
ϵω
ϕ(zt; y, t) −ϵ
 ∂x ∂θ"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4206008583690987,"
,
(4)"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4227467811158798,"where x is the rendered images, t is the sampled time step, zt is the t time step latent, w(t) is the
186"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4248927038626609,"weighting function that equals ∂zt/∂x, y is the text condition, ϵω
ϕ(zt; y, t) is the predicted noise
187"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4270386266094421,"through classifier-free guidance, and ϵ ∈N(0, I) is the noise added to the rendered images. The
188"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4291845493562232,"gradient of SDS loss gives an editing direction for our texture optimization, determined by the text
189"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4313304721030043,"prompt y. However, the SDS loss may cause the destruction of original image content with noisy
190"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4334763948497854,"details, because the text prior typically cannot faithfully reflect the information of the image. It is
191"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4356223175965665,"known that the entropy of an RGB image is significantly larger than that of a text prompt. As a
192"
RELATIVE DIRECTION BASED OPTIMIZATION,0.43776824034334766,"consequence, the misdescription inevitably arises when taking the text prompt as the prior to restore
193"
RELATIVE DIRECTION BASED OPTIMIZATION,0.43991416309012876,"the high-quality image from the same-scale noise. Therefore, even for a text prompt y0 describing the
194"
RELATIVE DIRECTION BASED OPTIMIZATION,0.44206008583690987,"original images, there exists a deviation related to the optimized texture θ between the added noise ϵ
195"
RELATIVE DIRECTION BASED OPTIMIZATION,0.44420600858369097,"and the predicted noise ϵω
ϕ (zt; y0, t), which can be simply expressed as,
196"
RELATIVE DIRECTION BASED OPTIMIZATION,0.44635193133047213,"Dbias(θ, ...) ∝||ϵω
ϕ (zt; y0, t) −ϵ||.
(5)"
RELATIVE DIRECTION BASED OPTIMIZATION,0.44849785407725323,"Thus, the gradient leads to a bias term from the original input image, which can be expressed as,
197"
RELATIVE DIRECTION BASED OPTIMIZATION,0.45064377682403434,"⃗nbias = ∂Dbias(θ, ...)"
RELATIVE DIRECTION BASED OPTIMIZATION,0.45278969957081544,"∂θ
=
 
ϵω
ϕ (zt; y0, t) −ϵ
 ∂x"
RELATIVE DIRECTION BASED OPTIMIZATION,0.45493562231759654,"∂θ .
(6)"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4570815450643777,"Moreover, for an arbitrary text prompt ytgt describing the target editing texture, it could be considered
198"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4592274678111588,"that there exists a term of expected editing direction and a term of bias discussed above,
199"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4613733905579399," 
ϵω
ϕ (zt; ytgt, t) −ϵ
 ∂x"
RELATIVE DIRECTION BASED OPTIMIZATION,0.463519313304721,"∂θ = ⃗ntgt + ⃗nbias.
(7)"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4656652360515021,"As a result, the ⃗nbias gives rise to the misdirection for the optimization procedure.
200"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4678111587982833,"To address these issues, it is ideal to find the accurate editing direction ⃗ntgt, while the term of ⃗nbias
201"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4699570815450644,"is hard to estimate due to the diverse input images. To mitigate the gap, it is natural to take the text
202"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4721030042918455,"guidance as a relative direction rather than an absolute direction, enabling us to eliminate the term of
203"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4742489270386266,"⃗nbias. The absolute direction of the source ⃗nsrc and the target ⃗ntgt can be expressed as,
204"
RELATIVE DIRECTION BASED OPTIMIZATION,0.47639484978540775,"⃗nsrc =
 
ϵω
ϕ (zt; y0, t) −ϵ
 ∂x"
RELATIVE DIRECTION BASED OPTIMIZATION,0.47854077253218885,"∂θ −⃗nbias,
(8)"
RELATIVE DIRECTION BASED OPTIMIZATION,0.48068669527896996,"⃗ntgt =
 
ϵω
ϕ (zt; ytgt, t) −ϵ
 ∂x"
RELATIVE DIRECTION BASED OPTIMIZATION,0.48283261802575106,"∂θ −⃗nbias,
(9)"
RELATIVE DIRECTION BASED OPTIMIZATION,0.48497854077253216,"where ⃗nsrc is actually the ⃗0 giving no extra information to the input images. Inspired by the CLIP-
205"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4871244635193133,"directional loss improved by the StyleGAN-Nada (12) and the denoising loss proposed by the recent
206"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4892703862660944,"work (31), we utilize the difference between the source ⃗nsrc and the target ⃗ntgt as the relative direction
207"
RELATIVE DIRECTION BASED OPTIMIZATION,0.49141630901287553,"of the target, which can be presented as,
208"
RELATIVE DIRECTION BASED OPTIMIZATION,0.49356223175965663,"⃗ntgt = ⃗ntgt −⃗nsrc =
 
ϵω
ϕ (zt, ytgt, t) −ϵω
ϕ (x, y0, t)
 ∂x"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4957081545064378,"∂θ .
(10)"
RELATIVE DIRECTION BASED OPTIMIZATION,0.4978540772532189,"Therefore, the final gradient utilized for optimizing the texture can be presented as,
209"
RELATIVE DIRECTION BASED OPTIMIZATION,0.5,"∇θLRDL(ϕ, x = R(θ)) = Et,ϵ"
RELATIVE DIRECTION BASED OPTIMIZATION,0.5021459227467812,"
w(t)
 
ϵω
ϕ(zt; ytgt, t) −ϵω
ϕ(zt; y0, t)
 ∂x ∂θ"
RELATIVE DIRECTION BASED OPTIMIZATION,0.5042918454935622,"
,
(11)"
DIRECTION ADJUSTMENT,0.5064377682403434,"3.4
Direction Adjustment
210"
DIRECTION ADJUSTMENT,0.5085836909871244,"Different from the gradual transition in the nature image domain, the optimization of the texture
211"
DIRECTION ADJUSTMENT,0.5107296137339056,"domain unfortunately shows an unexpected offset of the appearance in rendered images, due to
212"
DIRECTION ADJUSTMENT,0.5128755364806867,"the complex projection in differentiable rendering. The inherent reason is that the complexity of
213"
DIRECTION ADJUSTMENT,0.5150214592274678,"rendering leads to unbalanced optimization for the texture, with some parts under-tuning and other
214"
DIRECTION ADJUSTMENT,0.5171673819742489,"parts over-tuning. This appearance offset can be seen in some parts of the rendered image, leading
215"
DIRECTION ADJUSTMENT,0.51931330472103,"to the inconsistency between the source text and the rendered images in the median period of the
216"
DIRECTION ADJUSTMENT,0.5214592274678111,"optimization procedure. It is known that a source image with an inconsistent text description means
217"
DIRECTION ADJUSTMENT,0.5236051502145923,"an optimization misdirection which leads to an unknown change in the editing results. Similar to the
218"
DIRECTION ADJUSTMENT,0.5257510729613734,"known problem, if a rendered image during the median optimization hops out the direction between
219"
DIRECTION ADJUSTMENT,0.5278969957081545,"the source text and the target text, it can be considered as the inconsistent description for the source
220"
DIRECTION ADJUSTMENT,0.5300429184549357,"image when we take the current median point as a relative beginning point. The original editing
221"
DIRECTION ADJUSTMENT,0.5321888412017167,"direction is give by,
222"
DIRECTION ADJUSTMENT,0.5343347639484979,"⃗nori = ⃗ntgt −⃗nsrc.
(12)
If the optimization continues along the original direction, a more severe deviation can be attached to
223"
DIRECTION ADJUSTMENT,0.5364806866952789,"the optimization procedure.
224"
DIRECTION ADJUSTMENT,0.5386266094420601,"To avoid the misdirection caused by the texture domain, we propose to adjust the editing direction,
225"
DIRECTION ADJUSTMENT,0.5407725321888412,"specifically the source text prompt, during our optimization process of the texture map. The adjusted
226"
DIRECTION ADJUSTMENT,0.5429184549356223,"direction ∆ˆTi can be represented as,
227"
DIRECTION ADJUSTMENT,0.5450643776824035,"∆ˆTi = ⃗ˆni −⃗ˆni−1 = B(Ii) −B(Ii−1),
(13)
where i is the optimization iteration and B (·) expresses the inverse text generated by a pre-trained
228"
DIRECTION ADJUSTMENT,0.5472103004291845,"language-image model BLIP-v2 (19).
229"
DIRECTION ADJUSTMENT,0.5493562231759657,"As shown in the Fig. 1 (b), the direction is continually adjusted during the optimization so that the
230"
DIRECTION ADJUSTMENT,0.5515021459227468,"new global direction ⃗ni can be written as,
231"
DIRECTION ADJUSTMENT,0.5536480686695279,"⃗ni = ⃗nori +
X"
DIRECTION ADJUSTMENT,0.555793991416309,"j≤i
∆ˆTj.
(14)"
DIRECTION ADJUSTMENT,0.5579399141630901,"By adjusting the optimization direction step by step, we achieve more delicate and controllable
232"
DIRECTION ADJUSTMENT,0.5600858369098712,"editing, which can be seen in Sec. 4.3.
233"
DIRECTION ADJUSTMENT,0.5622317596566524,"Figure 3: Qualitative comparison on NeRF synthetic dataset. The results of both textures and
rendered images are presented. Our method synthesizes more realistic objects which better correspond
to text instructions."
EXPERIMENTS,0.5643776824034334,"4
Experiments
234"
IMPLEMENTATION DETAILS,0.5665236051502146,"4.1
Implementation Details
235"
IMPLEMENTATION DETAILS,0.5686695278969958,"Dataset. In the experiments, we mainly evaluate our model on the NeRF Synthetic (27) dataset. The
236"
IMPLEMENTATION DETAILS,0.5708154506437768,"NeRF synthetic dataset consists of 8 path-traced scenes with multi-view images which we reconstruct
237"
IMPLEMENTATION DETAILS,0.572961373390558,"into our textural mesh-based representation via nvdiffrec (29). Besides, we adopt 3D objects from
238"
IMPLEMENTATION DETAILS,0.575107296137339,"Keenan’s 3D model repository.
239"
IMPLEMENTATION DETAILS,0.5772532188841202,"Experiment Setup. We optimize the 3D model on one RTX A6000 GPU with 48G memory. The
240"
IMPLEMENTATION DETAILS,0.5793991416309013,"optimization procedure lasts about average 500 iterations with 8 minutes for each 3D model. We use
241"
IMPLEMENTATION DETAILS,0.5815450643776824,"the Adam optimizer for both the texture and the environment map with an initial learning rate of 0.01
242"
IMPLEMENTATION DETAILS,0.5836909871244635,"which gradually decreases to 1/10 every 5k iterations during the training process.
243"
COMPARISON WITH BASELINE,0.5858369098712446,"4.2
Comparison with Baseline
244"
COMPARISON WITH BASELINE,0.5879828326180258,"Qualitative Comparison. We compare ITEM3D with the optimization method based on the SDS
245"
COMPARISON WITH BASELINE,0.5901287553648069,"loss. Specifically, Fig. 3 shows the results of editing texture and rendered image with the guidance
246"
COMPARISON WITH BASELINE,0.592274678111588,"of text prompts. While SDS-based method could edit textures along the direction of text prompt,
247"
COMPARISON WITH BASELINE,0.5944206008583691,"their rendered images show the unrealistic appearance, sometimes overfitting to the text. In contrast,
248"
COMPARISON WITH BASELINE,0.5965665236051502,"Table 1: Quantitative Comparisons. We report two CLIP-based scores, i.e., global score and
directional score to evaluate the semantic quality of rendered images. ‘-’ indicates not available.
Our ITEM3D achieves better results than the SDS-based method. Besides, the inferiority of the
performance without direction adjustment also shows the effect of this designed component."
COMPARISON WITH BASELINE,0.5987124463519313,"Method
Origin (Ref.)
ITEM3D
SDS-based
w/o dir. adjustment"
COMPARISON WITH BASELINE,0.6008583690987125,"Global Score↑
0.31
0.32
0.30
0.30
Directional Score↑
-
0.23
0.18
0.16"
COMPARISON WITH BASELINE,0.6030042918454935,"Table 2: User study conducted with 33 participants. Each participant scores based on two evaluation
criteria, i.e., photorealism and text consistency. The range of scores is from 1 to 5, where 1 represents
worst and 5 represents best."
COMPARISON WITH BASELINE,0.6051502145922747,"Method
Origin (Ref.)
ITEM3D
SDS-based"
COMPARISON WITH BASELINE,0.6072961373390557,"Photorealism ↑
4.18
3.77
2.77
Text Consistency ↑
-
4.11
2.45"
COMPARISON WITH BASELINE,0.6094420600858369,"the texture edited by our ITEM3D can render realistic images with high quality, while remaining
249"
COMPARISON WITH BASELINE,0.6115879828326181,"consistent with the input text prompt. The comparison indicates the effectiveness of the introduced
250"
COMPARISON WITH BASELINE,0.6137339055793991,"relative direction of optimization and further direction adjustment. Besides, it can be noticed that
251"
COMPARISON WITH BASELINE,0.6158798283261803,"our methods support segmentation-aware editing. Although the diffusion model lacks the capacity
252"
COMPARISON WITH BASELINE,0.6180257510729614,"of recognizing the semantics in the texture map, it enables to edit the specific part of texture
253"
COMPARISON WITH BASELINE,0.6201716738197425,"corresponding to a text prompt describing partial change. For example, with the prompt “A ficus with
254"
COMPARISON WITH BASELINE,0.6223175965665236,"blue pot”, the change in the texture precisely reflects to the part of the pot in the rendered images. It
255"
COMPARISON WITH BASELINE,0.6244635193133047,"proves that the gradients can accurately back-propagate to the corresponding parts of the texture map
256"
COMPARISON WITH BASELINE,0.6266094420600858,"via the rendered images.
257"
COMPARISON WITH BASELINE,0.628755364806867,"Quantitative Comparison. Moreover, we conduct a quantitative comparison in the Tab. 1. To
258"
COMPARISON WITH BASELINE,0.630901287553648,"evaluate the semantic consistency, we choose objects from Keenan’s 3D Model Repository, render
259"
COMPARISON WITH BASELINE,0.6330472103004292,"their 512×512 RGB images after texture editing, and further compute the CLIP-Score of the rendered
260"
COMPARISON WITH BASELINE,0.6351931330472103,"image and corresponding target text. CLIP-score contains two parts, i.e., global score and directional
261"
COMPARISON WITH BASELINE,0.6373390557939914,"score. Global score measures the similarity between the target text and the editing images, and
262"
COMPARISON WITH BASELINE,0.6394849785407726,"directional score measures the similarity between two editing directions of text prompts and images,
263"
COMPARISON WITH BASELINE,0.6416309012875536,"which are expressed as which can be presented as,
264"
COMPARISON WITH BASELINE,0.6437768240343348,"Scoreglobal =
Ttgt · Itgt
∥Ttgt∥∥Itgt∥,
Scoredirection =
∆T · ∆I
∥∆T∥∥∆T∥,
(15)"
COMPARISON WITH BASELINE,0.6459227467811158,"where Ttgt and Itgt are the embedding of target text and edited image encoded by the CLIP encoder,
265"
COMPARISON WITH BASELINE,0.648068669527897,"and ∆T and ∆I are expressed as,
266"
COMPARISON WITH BASELINE,0.6502145922746781,"∆T = Ttgt −Tsrc,
∆I = Itgt −Isrc.
(16)"
COMPARISON WITH BASELINE,0.6523605150214592,"As illustrated in Tab. 1, our method achieves better results than the SDS-based method.
267"
COMPARISON WITH BASELINE,0.6545064377682404,"User Study. Additionally, we perform a user study in Tab. 2 to further assess the quality of editing
268"
COMPARISON WITH BASELINE,0.6566523605150214,"objects. Users are required to rate on a scale of 1 to 5, based on the following questions: (1) Are
269"
COMPARISON WITH BASELINE,0.6587982832618026,"the edited objects realistic and natural (Photorealism)? (2) Are the edited objects accurately reflect
270"
COMPARISON WITH BASELINE,0.6609442060085837,"the target text’s semantics (Text Consistency)? As presented in Tab. 2, the results demonstrate the
271"
COMPARISON WITH BASELINE,0.6630901287553648,"superior quality with higher realism and more text consistency of our proposed method as compared
272"
COMPARISON WITH BASELINE,0.6652360515021459,"to the baselines.
273"
DIRECTION ADJUSTMENT,0.6673819742489271,"4.3
Direction Adjustment
274"
DIRECTION ADJUSTMENT,0.6695278969957081,"In this section, we further study the necessity of direction adjustment. We perform the ablation
275"
DIRECTION ADJUSTMENT,0.6716738197424893,"study in Fig. 4. Without the adjustment for the relative optimization direction, the texture shows
276"
DIRECTION ADJUSTMENT,0.6738197424892703,"a wired change that the duck gradually generates two heads and the color seems partially yellow
277"
DIRECTION ADJUSTMENT,0.6759656652360515,"and partially red. When applying the gradual adjustment, the duck bypasses the unnatural change
278"
DIRECTION ADJUSTMENT,0.6781115879828327,"and smoothly achieves the target appearance. The example of cattle shows a similar trend. In this
279"
DIRECTION ADJUSTMENT,0.6802575107296137,"experiment, it can be noticed that there exists unbalanced optimization for different parts of the
280"
DIRECTION ADJUSTMENT,0.6824034334763949,"Figure 4: Ablation study of direction adjustment. The results without adjustment show a wired
appearance, i.e., dual heads and quadruple eyes. When applying gradual adjustment, the unrealistic
artifacts are released, in result of natural appearance."
DIRECTION ADJUSTMENT,0.6845493562231759,"Figure 5: Relighting results under the condition of an illumination-aware text prompt. Keeping the
texture constant, ITEM3D has capacity of explicit control over the lighting under the guidance of
prompt related to the environment map."
DIRECTION ADJUSTMENT,0.6866952789699571,"texture. The optimization scheme of simple pieces of texture converges quickly, while more complex
281"
DIRECTION ADJUSTMENT,0.6888412017167382,"modifications require longer time, which in turn over-tunes easy parts leading to poor results. We
282"
DIRECTION ADJUSTMENT,0.6909871244635193,"also compute the two CLIP score for the results without direction adjustment in Tab. 1. It shows that
283"
DIRECTION ADJUSTMENT,0.6931330472103004,"the adjustment indeed helps to maintain the major semantics.
284"
ILLUMINATION-AWARE EDITING,0.6952789699570815,"4.4
Illumination-aware Editing
285"
ILLUMINATION-AWARE EDITING,0.6974248927038627,"The disentangled representation of environment map empowers ITEM3D to explicitly control the light-
286"
ILLUMINATION-AWARE EDITING,0.6995708154506438,"ing under the guidance of a text prompt aiming to relight the 3D model. The results of illumination-
287"
ILLUMINATION-AWARE EDITING,0.7017167381974249,"aware editing are demonstrated in Fig. 5. As shown, given the prompt including lighting information
288"
ILLUMINATION-AWARE EDITING,0.703862660944206,"such as “sunrise”, “bright light”, and “dazzling light”, ITEM3D enables to edit the environment map
289"
ILLUMINATION-AWARE EDITING,0.7060085836909872,"along the direction led by the prompt. It is valuable to prove that the lighting condition of a 3D model
290"
ILLUMINATION-AWARE EDITING,0.7081545064377682,"can be learned solely from the text through the bridge of rendered 2D images.
291"
CONCLUSION AND LIMITATIONS,0.7103004291845494,"5
Conclusion and Limitations
292"
CONCLUSION AND LIMITATIONS,0.7124463519313304,"In conclusion, our ITEM3D model presents an efficient solution to the challenging task of texture
293"
CONCLUSION AND LIMITATIONS,0.7145922746781116,"editing for 3D models. By leveraging the power of diffusion models, ITEM3D is capable to optimize
294"
CONCLUSION AND LIMITATIONS,0.7167381974248928,"the texture and environment map under the guidance of text prompts. To address the semantic
295"
CONCLUSION AND LIMITATIONS,0.7188841201716738,"ambiguity between text prompts and images, we replace the traditional score distillation sampling
296"
CONCLUSION AND LIMITATIONS,0.721030042918455,"(SDS) with a relative editing direction. We further propose a gradual direction adjustment during the
297"
CONCLUSION AND LIMITATIONS,0.723175965665236,"optimization procedure, solving the unbalanced optimization in the texture.
298"
CONCLUSION AND LIMITATIONS,0.7253218884120172,"Despite the promising editing results, our ITEM3D still remains several limitations which should be
299"
CONCLUSION AND LIMITATIONS,0.7274678111587983,"solved in future work. The major limitation is that there remains irremovable noise in some samples.
300"
CONCLUSION AND LIMITATIONS,0.7296137339055794,"Because of the synthesis mechanism of the diffusion model, our ITEM3D extremely depends on the
301"
CONCLUSION AND LIMITATIONS,0.7317596566523605,"denoising ability of the pre-trained U-Net. Another limitation is that the adjustment by the source
302"
CONCLUSION AND LIMITATIONS,0.7339055793991416,"description is non-essential. Our further work aims to explore the learning scheme to solve the
303"
CONCLUSION AND LIMITATIONS,0.7360515021459227,"problem of unbalanced optimization in the texture.
304"
REFERENCES,0.7381974248927039,"References
305"
REFERENCES,0.740343347639485,"[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M. Seitz, and
306"
REFERENCES,0.7424892703862661,"Richard Szeliski. Building rome in a day. Commun. ACM, 54(10):105–112, 2011.
307"
REFERENCES,0.7446351931330472,"[2] Urs Bergmann, Nikolay Jetchev, and Roland Vollgraf. Learning texture manifolds with the periodic spatial
308"
REFERENCES,0.7467811158798283,"GAN. In ICML, volume 70, pages 469–477, 2017.
309"
REFERENCES,0.7489270386266095,"[3] Sema Berkiten, Maciej Halber, Justin Solomon, Chongyang Ma, Hao Li, and Szymon Rusinkiewicz.
310"
REFERENCES,0.7510729613733905,"Learning detail transfer based on geometric features. Comput. Graph. Forum, 36(2):361–373, 2017.
311"
REFERENCES,0.7532188841201717,"[4] Jeremy S De Bonet and Paul Viola. Poxels: Probabilistic voxelized volume reconstruction. In ICCV, 1999.
312"
REFERENCES,0.7553648068669528,"[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In
313"
REFERENCES,0.7575107296137339,"ECCV, pages 333–350, 2022.
314"
REFERENCES,0.759656652360515,"[6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance
315"
REFERENCES,0.7618025751072961,"for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023.
316"
REFERENCES,0.7639484978540773,"[7] Zhuo Chen, Xudong Xu, Yichao Yan, Ye Pan, Wenhan Zhu, Wayne Wu, Bo Dai, and Xiaokang Yang.
317"
REFERENCES,0.7660944206008584,"Hyperstyle3d: Text-guided 3d portrait stylization via hypernetworks. arXiv preprint arXiv:2304.09463,
318"
REFERENCES,0.7682403433476395,"2023.
319"
REFERENCES,0.7703862660944206,"[8] Alexei A. Efros and Thomas K. Leung. Texture synthesis by non-parametric sampling. In ICCV, pages
320"
REFERENCES,0.7725321888412017,"1033–1038, 1999.
321"
REFERENCES,0.7746781115879828,"[9] Anna Frühstück, Ibraheem Alhashim, and Peter Wonka. Tilegan: synthesis of large-scale non-homogeneous
322"
REFERENCES,0.776824034334764,"textures. ACM Trans. Graph., 38(4):58:1–58:11, 2019.
323"
REFERENCES,0.778969957081545,"[10] Qiancheng Fu, Qingshan Xu, Yew Soon Ong, and Wenbing Tao. Geo-neus: Geometry-consistent neural
324"
REFERENCES,0.7811158798283262,"implicit surfaces learning for multi-view reconstruction. NeurIPS, 35:3403–3416, 2022.
325"
REFERENCES,0.7832618025751072,"[11] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE Trans. Pattern
326"
REFERENCES,0.7854077253218884,"Anal. Mach. Intell., 32(8):1362–1376, 2010.
327"
REFERENCES,0.7875536480686696,"[12] Rinon Gal, Or Patashnik, Haggai Maron, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-
328"
REFERENCES,0.7896995708154506,"nada: Clip-guided domain adaptation of image generators. ACM Trans. Graph., 41(4):141:1–141:13,
329"
REFERENCES,0.7918454935622318,"2022.
330"
REFERENCES,0.7939914163090128,"[13] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090,
331"
REFERENCES,0.796137339055794,"2023.
332"
REFERENCES,0.7982832618025751,"[14] Amir Hertz, Rana Hanocka, Raja Giryes, and Daniel Cohen-Or. Deep geometric texture synthesis. ACM
333"
REFERENCES,0.8004291845493562,"Trans. Graph., 39(4):108, 2020.
334"
REFERENCES,0.8025751072961373,"[15] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object
335"
REFERENCES,0.8047210300429185,"generation with dream fields. In CVPR, pages 867–876, 2022.
336"
REFERENCES,0.8068669527896996,"[16] Nikolay Jetchev.
Clipmatrix:
Text-controlled creation of 3d textured meshes.
arXiv preprint
337"
REFERENCES,0.8090128755364807,"arXiv:2109.12922, 2021.
338"
REFERENCES,0.8111587982832618,"[17] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal
339"
REFERENCES,0.8133047210300429,"Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276,
340"
REFERENCES,0.8154506437768241,"2022.
341"
REFERENCES,0.8175965665236051,"[18] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for
342"
REFERENCES,0.8197424892703863,"robust image manipulation. In CVPR, pages 2426–2435, 2022.
343"
REFERENCES,0.8218884120171673,"[19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training
344"
REFERENCES,0.8240343347639485,"with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.
345"
REFERENCES,0.8261802575107297,"[20] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,
346"
REFERENCES,0.8283261802575107,"Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. arXiv
347"
REFERENCES,0.8304721030042919,"preprint arXiv:2211.10440, 2022.
348"
REFERENCES,0.8326180257510729,"[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.
349"
REFERENCES,0.8347639484978541,"Zero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328, 2023.
350"
REFERENCES,0.8369098712446352,"[22] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovitskiy, and
351"
REFERENCES,0.8390557939914163,"Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. arXiv
352"
REFERENCES,0.8412017167381974,"preprint arXiv:2008.02268, 2020.
353"
REFERENCES,0.8433476394849786,"[23] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image
354"
REFERENCES,0.8454935622317596,"synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.
355"
REFERENCES,0.8476394849785408,"[24] Tom Mertens, Jan Kautz, Jiawen Chen, Philippe Bekaert, and Frédo Durand. Texture transfer using
356"
REFERENCES,0.8497854077253219,"geometry correlation. In Proceedings of the Eurographics Symposium on Rendering Techniques, pages
357"
REFERENCES,0.851931330472103,"273–284, 2006.
358"
REFERENCES,0.8540772532188842,"[25] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-
359"
REFERENCES,0.8562231759656652,"guided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022.
360"
REFERENCES,0.8583690987124464,"[26] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural
361"
REFERENCES,0.8605150214592274,"stylization for meshes. In CVPR, pages 13492–13502, 2022.
362"
REFERENCES,0.8626609442060086,"[27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.
363"
REFERENCES,0.8648068669527897,"Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, pages 99–106, 2020.
364"
REFERENCES,0.8669527896995708,"[28] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives
365"
REFERENCES,0.869098712446352,"with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1–15, 2022.
366"
REFERENCES,0.871244635193133,"[29] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Müller,
367"
REFERENCES,0.8733905579399142,"and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In CVPR, pages
368"
REFERENCES,0.8755364806866953,"8280–8290, 2022.
369"
REFERENCES,0.8776824034334764,"[30] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and
370"
REFERENCES,0.8798283261802575,"radiance fields for multi-view reconstruction. arXiv preprint arXiv:2104.10078, 2021.
371"
REFERENCES,0.8819742489270386,"[31] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.
372"
REFERENCES,0.8841201716738197,"arXiv preprint arXiv:2209.14988, 2022.
373"
REFERENCES,0.8862660944206009,"[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
374"
REFERENCES,0.8884120171673819,"Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
375"
REFERENCES,0.8905579399141631,"natural language supervision. In ICML, pages 8748–8763, 2021.
376"
REFERENCES,0.8927038626609443,"[33] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance
377"
REFERENCES,0.8948497854077253,"fields with thousands of tiny mlps. arXiv preprint arXiv:2103.13744, 2021.
378"
REFERENCES,0.8969957081545065,"[34] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided
379"
REFERENCES,0.8991416309012875,"texturing of 3d shapes. arXiv preprint arXiv:2302.01721, 2023.
380"
REFERENCES,0.9012875536480687,"[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
381"
REFERENCES,0.9034334763948498,"image synthesis with latent diffusion models. In CVPR, pages 10684–10695, 2022.
382"
REFERENCES,0.9055793991416309,"[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
383"
REFERENCES,0.907725321888412,"Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-
384"
REFERENCES,0.9098712446351931,"image diffusion models with deep language understanding. NeurIPS, 35:36479–36494, 2022.
385"
REFERENCES,0.9120171673819742,"[37] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Ka-
386"
REFERENCES,0.9141630901287554,"mal Rahimi Malekshan.
Clip-forge: Towards zero-shot text-to-shape generation.
In CVPR, pages
387"
REFERENCES,0.9163090128755365,"18603–18613, 2022.
388"
REFERENCES,0.9184549356223176,"[38] Johannes L. Schönberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view
389"
REFERENCES,0.9206008583690987,"selection for unstructured multi-view stereo. In ECCV, volume 9907, pages 501–518, 2016.
390"
REFERENCES,0.9227467811158798,"[39] Steven M. Seitz and Charles R. Dyer. Photorealistic scene reconstruction by voxel coloring. Int. J. Comput.
391"
REFERENCES,0.924892703862661,"Vis., 35(2):151–173, 1999.
392"
REFERENCES,0.927038626609442,"[40] Omry Sendik and Daniel Cohen-Or.
Deep correlations for texture synthesis.
ACM Trans. Graph.,
393"
REFERENCES,0.9291845493562232,"36(5):161:1–161:15, 2017.
394"
REFERENCES,0.9313304721030042,"[41] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a
395"
REFERENCES,0.9334763948497854,"hybrid representation for high-resolution 3d shape synthesis. NeurIPS, 34:6087–6101, 2021.
396"
REFERENCES,0.9356223175965666,"[42] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-to-
397"
REFERENCES,0.9377682403433476,"image translation. In ICLR, 2022.
398"
REFERENCES,0.9399141630901288,"[43] Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, and Yebin Liu. IDE-3D: interactive
399"
REFERENCES,0.9420600858369099,"disentangled editing for high-resolution 3d-aware portrait synthesis. ACM Trans. Graph., 41(6):270:1–
400"
REFERENCES,0.944206008583691,"270:10, 2022.
401"
REFERENCES,0.9463519313304721,"[44] Bruce Walter, Stephen R. Marschner, Hongsong Li, and Kenneth E. Torrance. Microfacet models for
402"
REFERENCES,0.9484978540772532,"refraction through rough surfaces. In Proceedings of the Eurographics Symposium on Rendering Techniques,
403"
REFERENCES,0.9506437768240343,"pages 195–206, 2007.
404"
REFERENCES,0.9527896995708155,"[45] Can Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image
405"
REFERENCES,0.9549356223175965,"driven manipulation of neural radiance fields. In CVPR, pages 3835–3844, 2022.
406"
REFERENCES,0.9570815450643777,"[46] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang.
Neus:
407"
REFERENCES,0.9592274678111588,"Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint
408"
REFERENCES,0.9613733905579399,"arXiv:2106.10689, 2021.
409"
REFERENCES,0.9635193133047211,"[47] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. Nerf–: Neural radiance
410"
REFERENCES,0.9656652360515021,"fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021.
411"
REFERENCES,0.9678111587982833,"[48] Zhizhong Wang, Lei Zhao, Haibo Chen, Ailin Li, Zhiwen Zuo, Wei Xing, and Dongming Lu. Texture
412"
REFERENCES,0.9699570815450643,"reformer: Towards fast and universal interactive texture transfer. In AAAI, pages 2624–2632, 2022.
413"
REFERENCES,0.9721030042918455,"[49] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. Nex:
414"
REFERENCES,0.9742489270386266,"Real-time view synthesis with neural basis expansion. In CVPR, pages 8534–8543, 2021.
415"
REFERENCES,0.9763948497854077,"[50] Wenqi Xian, Patsorn Sangkloy, Varun Agrawal, Amit Raj, Jingwan Lu, Chen Fang, Fisher Yu, and James
416"
REFERENCES,0.9785407725321889,"Hays. Texturegan: Controlling deep image synthesis with texture patches. In CVPR, pages 8456–8465,
417"
REFERENCES,0.98068669527897,"2018.
418"
REFERENCES,0.9828326180257511,"[51] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time
419"
REFERENCES,0.9849785407725322,"rendering of neural radiance fields. In ICCV, pages 5752–5761, 2021.
420"
REFERENCES,0.9871244635193133,"[52] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural
421"
REFERENCES,0.9892703862660944,"radiance fields. arXiv preprint arXiv:2010.07492, 2020.
422"
REFERENCES,0.9914163090128756,"[53] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv
423"
REFERENCES,0.9935622317596566,"preprint arXiv:2302.05543, 2023.
424"
REFERENCES,0.9957081545064378,"[54] Yang Zhou, Zhen Zhu, Xiang Bai, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Non-stationary
425"
REFERENCES,0.9978540772532188,"texture synthesis by adversarial expansion. ACM Trans. Graph., 37(4):49, 2018.
426"
