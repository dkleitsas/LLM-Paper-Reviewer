Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0014970059880239522,"A common setting in multitask reinforcement learning (RL) demands that an agent
rapidly adapt to various stationary reward functions randomly sampled from a
fixed distribution. In such situations, the successor representation (SR) is a pop-
ular framework which supports rapid policy evaluation by decoupling a policy’s
expected discounted, cumulative state occupancies from a specific reward function.
However, in the natural world, sequential tasks are rarely independent, and instead
reflect shifting priorities based on the availability and subjective perception of re-
warding stimuli. Reflecting this disjunction, in this paper we study the phenomenon
of diminishing marginal utility and introduce a novel state representation, the λ
representation (λR) which, surprisingly, is required for policy evaluation in this
setting and which generalizes the SR as well as several other state representations
from the literature. We establish the λR’s formal properties and examine its nor-
mative advantages in the context of machine learning, as well as its usefulness for
studying natural behaviors, particularly foraging."
INTRODUCTION,0.0029940119760479044,"1
Introduction"
INTRODUCTION,0.004491017964071856,Stimulus Presentations
INTRODUCTION,0.005988023952095809,Cumulative Reward
INTRODUCTION,0.0074850299401197605,"Constant Utility
Diminishing Utility"
INTRODUCTION,0.008982035928143712,Figure 1.1: Diminishing rewards.
INTRODUCTION,0.010479041916167664,"The second ice cream cone rarely tastes as good as the first,
and once all the most accessible brambles have been picked,
the same investment of effort yields less fruit. In everyday
life, the availability and our enjoyment of stimuli is sensitive
to our past interactions with them. Thus, to evaluate different
courses of action and act accordingly, we might expect our
brains to form representations sensitive to the non-stationarity
of rewards. Evidence in fields from behavioral economics
[1, 2] to neuroscience [3] supports this hypothesis. Surprisingly,
however, most of reinforcement learning (RL) takes place under
the assumptions of the Markov Decision Process [MDP; 4],
where rewards and optimal decision-making remain stationary."
INTRODUCTION,0.011976047904191617,"In this paper, we seek to bridge this gap by studying the phenomenon of diminishing marginal
utility [5] in the context of RL. Diminishing marginal utility (DMU) is the subjective phenomenon
by which repeated exposure to a rewarding stimulus reduces the perceived utility one experiences.
While DMU is thought to have its roots in the maintenance of homeostatic equilibrium (too much
ice cream can result in a stomach ache), it also manifests itself in domains in which the collected
rewards are abstract, such as economics ($10 vs. $0 is perceived as a bigger difference in value than
$1,010 vs. $1,000), where it is closely related to risk aversion [6, 7]. While DMU is well-studied in
other fields, relatively few RL studies have explored diminishing reward functions [8, 9], and, to our"
INTRODUCTION,0.01347305389221557,"knowledge, none contain a formal analysis of DMU within RL. Here, we seek to characterize both its
importance and the challenge it poses for current RL approaches (Section 3)."
INTRODUCTION,0.014970059880239521,"Surprisingly, we find that evaluating policies under diminishing rewards requires agents to learn
a novel state representation which we term the λ representation (λR, Section 4). The λR gener-
alizes several state representations from the RL literature: the successor representation [SR; 10],
the first-occupancy representation [FR; 11], and the forward-backward representation [FBR; 12],
adapting them for non-stationary environments. Interestingly, despite the non-stationarity of the
underlying reward functions, we show that the λR still admits a Bellman recursion, allowing for
efficient computation via dynamic programming (or approximate dynamic programming) and prove
its convergence. We demonstrate the scalability of the λR to large and continuous state spaces
(Section 4.1), show that it supports policy evaluation, improvement, and composition (Section 5), and
show that the behavior it induces is consistent with optimal foraging theory (Section 6)."
PRELIMINARIES,0.016467065868263474,"2
Preliminaries"
PRELIMINARIES,0.017964071856287425,"Standard RL
In standard RL, the goal of the agent is to act within its environment so as to
maximize its discounted cumulative reward for some task T [13]. Typically, T is modeled as a
discounted MDP, T = (S, A, p, r, γ, µ0), where S is the state space, A is the set of available actions,
p : S × A 7→P(S) is the transition kernel (where P(S) is the space of probability distributions
over S), r : S 7→R is the reward function, γ ∈[0, 1) is a discount factor, and µ0 ∈P(S) is the
distribution over initial states. Note that the reward function is also frequently defined over state-
action pairs (s, a) or triples (s, a, s′), but to simplify notation we mostly consider state-based rewards.
All of the following analysis, unless otherwise noted, can easily be extended to the (s, a)- or (s, a, s′)-
dependent cases. The goal of the agent is to maximize its expected return, or discounted cumulative
reward P"
PRELIMINARIES,0.019461077844311378,"t γtr(st). The role of the discount factor is twofold: from a theoretical standpoint, it
ensures that this sum is finite for bounded rewards, and it induces myopia in the agent’s behavior.
To simplify notation, we will frequently write r(st) ≜rt and r ∈R|S| as the vector of rewards for
each state. The agent acts according to a stationary policy π : S 7→P(A). For finite MDPs, we
can describe the expected transition probabilities under π using a |S| × |S| matrix P π such that
P π
s,s′ = pπ(s′|s) ≜P"
PRELIMINARIES,0.020958083832335328,"a p(s′|s, a)π(a|s). Given π and a reward function r, the expected return
associated with taking action a in state s is"
PRELIMINARIES,0.02245508982035928,"Qπ
r (s, a) = Eπ "" ∞
X"
PRELIMINARIES,0.023952095808383235,"k=0
γkrt+k
st = s, at = a #"
PRELIMINARIES,0.025449101796407185,"= Es′∼pπ(·|s) [rt + γQπ
r (s′, π(s′))] .
(2.1)"
PRELIMINARIES,0.02694610778443114,"The Qπ
r are called the state-action values or simply the Q-values of π. The expectation Eπ [·] is
taken with respect to the randomness of both the policy and the transition dynamics. For simplicity
of notation, from here onwards we will write expectations of the form Eπ [·|st = s, at = a] as
Eπ [·|st, at]. The recursive form in Eq. (2.1) is called the Bellman equation, and it makes the process
of estimating Qπ
r —termed policy evaluation—tractable via dynamic programming. In particular,
successive applications of the Bellman operator T πQ ≜r +γP πQ are guaranteed to converge to the
true value function Qπ for any initial real-valued |S|×|A| matrix Q. Once a policy has been evaluated,
policy improvement identifies a new policy π′ such that Qπ
r (s, a) ≥Qπ′
r (s, a), ∀(s, a) ∈Qπ
r (s, a).
Helpfully, such a policy can be defined as π′(s) ∈argmaxa Qπ
r (s, a)."
PRELIMINARIES,0.02844311377245509,"Value Decomposition
Often, it can be useful for an agent to evaluate the policies it’s learned on
new reward functions. In order to do so efficiently, it can make use of the successor representation
[SR; 10], which decomposes the value function as follows:"
PRELIMINARIES,0.029940119760479042,V π(s) = Eπ  X
PRELIMINARIES,0.03143712574850299,"k≥0
γkrt+k st "
PRELIMINARIES,0.03293413173652695,= rT Eπ  X
PRELIMINARIES,0.0344311377245509,"k≥0
γk1(st+k) st  "
PRELIMINARIES,0.03592814371257485,"|
{z
}"
PRELIMINARIES,0.0374251497005988,≜M π(s)
PRELIMINARIES,0.038922155688622756,",
(2.2)"
PRELIMINARIES,0.040419161676646706,"where 1(st) is a one-hot representation of state st and M π, the SR, is an |S| × |S| matrix such that"
PRELIMINARIES,0.041916167664670656,"M π(s, s′) ≜Eπ  X"
PRELIMINARIES,0.04341317365269461,"k≥0
γk1(st+k = s′) st  = Eπ """
PRELIMINARIES,0.04491017964071856,"1(st = s′) + γM π(st+1, s′) st #"
PRELIMINARIES,0.04640718562874251,".
(2.3)"
PRELIMINARIES,0.04790419161676647,"Because the SR satisfies a Bellman recursion, it can be learned in a similar manner to the value
function, with the added benefit that if the transition kernel p is fixed, it can be used to instantly
evaluate a policy on a task determined by any reward vector r. The SR can also be conditioned on
actions, M π(s, a, s′), in which case multiplication by r produces the Q-values of π. The SR was
originally motivated by the idea that a representation of state in the brain should be dependent on the
similarity of future paths through the environment, and there is evidence that SR-like representations
are present in the hippocampus [14]. A representation closely related to the SR is the first-occupancy
representation [FR; 11], which modifies the SR by only counting the first visit to each state:"
PRELIMINARIES,0.04940119760479042,"F π(s, s′) ≜Eπ  X"
PRELIMINARIES,0.05089820359281437,"k≥0
γk1(st+k = s′, s′ /∈{st, . . . , st+k−1}) st "
PRELIMINARIES,0.05239520958083832,".
(2.4)"
PRELIMINARIES,0.05389221556886228,"The FR can be used in the same way as the SR, with the difference that the values it computes are
predicated on ephemeral rewards—those that are consumed or vanish after the first visit to a state."
PRELIMINARIES,0.05538922155688623,"Policy Composition
If the agent has access to a set of policies Π = {π} and their associated SRs
(or FRs) and is faced by a new task determined by reward vector r, it can instantly evaluate these
policies by simply multiplying: {Qπ(s, a) = rTM π(s, a, ·)}. This process is termed generalized
policy evaluation [GPE; 15]. Similarly, generalized policy improvement (GPI) is defined as the
identification of a new policy π′ such that Qπ′(s, a) ≥supπ∈Π Qπ(s, a) ∀s, a ∈S × A. Combining
both provides a way for an agent to efficiently compose its policies Π—that is, to combine them to
produce a new policy without additional learning. This process, which we refer to as GPE+GPI,
produces the following policy, which is guaranteed to perform at least as well as any policy in Π [16]:"
PRELIMINARIES,0.05688622754491018,"π′(s) ∈argmax
a∈A
max
π∈Π rTM π(s, a, ·).
(2.5)"
DIMINISHING MARGINAL UTILITY,0.058383233532934134,"3
Diminishing Marginal Utility"
DIMINISHING MARGINAL UTILITY,0.059880239520958084,"Problem Statement
Motivated by DMU’s importance in decision-making, our goal is to understand
RL in the context of the following class of non-Markov reward functions:"
DIMINISHING MARGINAL UTILITY,0.061377245508982034,"rλ(s, t) = λ(s)n(s,t)¯r(s),
λ(s) ∈[0, 1],
(3.1)"
DIMINISHING MARGINAL UTILITY,0.06287425149700598,"where n(s, t) ∈N is the agent’s visit count at s up to time t and ¯r(s) describes the reward at the
first visit to s. λ(s) therefore encodes the extent to which reward diminishes after each visit to s.
Note that for λ(s) = λ = 1 we recover the usual stationary reward given by ¯r, and so this family of
rewards strictly generalizes the stationary Markovian rewards typically used in RL."
DIMINISHING MARGINAL UTILITY,0.06437125748502993,"DMU is Challenging
An immediate question when considering reward functions of this form is
whether or not we can still define a Bellman equation over the resulting value function. If this is the
case, standard RL approaches still apply. However, the following result shows otherwise."
DIMINISHING MARGINAL UTILITY,0.0658682634730539,"Lemma 3.1 (Impossibility; Informal). Given a reward function of the form Eq. (3.1), it is impossible
to define a Bellman equation solely using the resulting value function and immediate reward."
DIMINISHING MARGINAL UTILITY,0.06736526946107785,"We provide a more precise statement, along with proofs for all theoretical results, in Appendix B. This
result means that we can’t write the value function corresponding to rewards of the form Eq. (3.1)
recursively only in terms of rewards and value in an analogous manner to Eq. (2.1). Nonetheless, we
found that it is in fact possible to derive a recursive relationship in this setting, but only by positing a
novel state representation that generalizes the SR and the FR, which we term the λ representation
(λR). In the following sections, we define the λR, establish its formal properties, and demonstrate its
necessity for RL problems with diminishing rewards."
DIMINISHING MARGINAL UTILITY,0.0688622754491018,"4
The λ Representation"
DIMINISHING MARGINAL UTILITY,0.07035928143712575,"A Representation for DMU
We now the derive the λR by decomposing the value function for
rewards of the form Eq. (3.1) and show that it admits a Bellman recursion. To simplify notation, we"
DIMINISHING MARGINAL UTILITY,0.0718562874251497,"use a single λ for all states, but the results below readily apply to non-uniform λ. We have"
DIMINISHING MARGINAL UTILITY,0.07335329341317365,"V π(s) = E "" ∞
X"
DIMINISHING MARGINAL UTILITY,0.0748502994011976,"k=0
γkrλ(st+k, k)
st = s #"
DIMINISHING MARGINAL UTILITY,0.07634730538922156,"= ¯rT E "" ∞
X"
DIMINISHING MARGINAL UTILITY,0.07784431137724551,"k=0
γkλnt(st+k,k)1(st+k)
st = s #"
DIMINISHING MARGINAL UTILITY,0.07934131736526946,"|
{z
}"
DIMINISHING MARGINAL UTILITY,0.08083832335329341,"≜Φπ
λ(s)"
DIMINISHING MARGINAL UTILITY,0.08233532934131736,",
(4.1)"
DIMINISHING MARGINAL UTILITY,0.08383233532934131,"where we call Φπ
λ(s) the λ representation (λR), and nt(s, k) ≜Pk−1
j=0 1(st+j = s), is the number
of times state s is visited from time t up to—but not including—time t + k. Formally:"
DIMINISHING MARGINAL UTILITY,0.08532934131736528,"Definition 4.1 (λR). For an MDP with finite S and λ ∈[0, 1]|S|, the λ representation is given by
Φπ
λ such that"
DIMINISHING MARGINAL UTILITY,0.08682634730538923,"Φπ
λ(s, s′) ≜E "" ∞
X"
DIMINISHING MARGINAL UTILITY,0.08832335329341318,"k=0
λ(s′)nt(s′,k)γk1(st+k = s′)
st = s # (4.2)"
DIMINISHING MARGINAL UTILITY,0.08982035928143713,"where nt(s, k) ≜Pk−1
j=0 1(st+j = s) is the number of times state s is visited from time t until time
t + k −1."
DIMINISHING MARGINAL UTILITY,0.09131736526946108,"We can immediately see that for λ = 0, the λR recovers the FR (we take 00 = 1), and for λ = 1,
it recovers the SR. For λ ∈(0, 1), the λR interpolates between the two, with higher values of λ
reflecting greater persistence of reward in a given state or state-action pair and lower values of λ
reflecting more ephemeral rewards."
DIMINISHING MARGINAL UTILITY,0.09281437125748503,The λR admits a recursive relationship:
DIMINISHING MARGINAL UTILITY,0.09431137724550898,"Φπ
λ(s, s′) = E "" ∞
X"
DIMINISHING MARGINAL UTILITY,0.09580838323353294,"k=0
λnt(s′,k)γk1(st+k = s′)
st = s #"
DIMINISHING MARGINAL UTILITY,0.09730538922155689,"(i)
= E """
DIMINISHING MARGINAL UTILITY,0.09880239520958084,"1(st = s′) + λnt(s′,1)γ ∞
X"
DIMINISHING MARGINAL UTILITY,0.10029940119760479,"k=1
λnt+1(s′,k)γk−11(st+k = s′)
st = s #"
DIMINISHING MARGINAL UTILITY,0.10179640718562874,"= 1(st = s′)(1 + γλEst+1∼pπΦπ
λ(st+1, s′)) + γ(1 −1(st = s′))Est+1∼pπΦπ
λ(st+1, s′),
(4.3)"
DIMINISHING MARGINAL UTILITY,0.10329341317365269,"where (i) follows from nt(s′, k) = nt(s′, 1)+nt+1(s′, k−1). A more detailed derivation is provided
in Appendix A. Thus, we can define a tractable Bellman operator:"
DIMINISHING MARGINAL UTILITY,0.10479041916167664,"Definition 4.2 (λR Operator). Let Φ ∈R|S|×|S| be an arbitrary real-valued matrix, and let Gπ
λ
denote the λR Bellman operator for π, such that"
DIMINISHING MARGINAL UTILITY,0.1062874251497006,"Gπ
λΦ ≜I ⊙
 
11T + γλP πΦ

+ γ(11T −I) ⊙P πΦ,
(4.4)"
DIMINISHING MARGINAL UTILITY,0.10778443113772455,"where ⊙denotes elementwise multiplication and I is the |S| × |S| identity matrix. In particular, for
a stationary policy π, Gπ
λΦπ
λ = Φπ
λ."
DIMINISHING MARGINAL UTILITY,0.1092814371257485,"The following result establishes that successive applications of Gπ
λ converge to the λR."
DIMINISHING MARGINAL UTILITY,0.11077844311377245,"Proposition 4.1 (Convergence). Under the conditions assumed above, set Φ(0) = (1 −λ)I. For
k = 1, 2, . . . , suppose that Φ(k+1) = Gπ
λΦ(k). Then"
DIMINISHING MARGINAL UTILITY,0.1122754491017964,"|(Φ(k) −Φπ
λ)s,s′| ≤γk+1"
DIMINISHING MARGINAL UTILITY,0.11377245508982035,1 −λγ .
DIMINISHING MARGINAL UTILITY,0.11526946107784432,"While such analysis is fairly standard in MDP theory, it is noteworthy that the analysis extends to this
case despite Lemma 3.1. That is, for a ethologically relevant class of non-Markovian reward functions
[17], it is possible to define a Markovian Bellman operator and prove that repeated applications of
it converge to the desired representation. Furthermore, unlike in the stationary reward case, where
decomposing the value function in terms of the reward and the SR is “optional” to perform prediction
or control, in this setting the structure of the problem requires that this representation be learned. To
get an intuition for the λR consider the simple gridworld presented in Fig. 4.1, where we visualize
the λR for varying values of λ. For λ = 0, the representation recovers the FR, encoding the expected
discount at first occupancy of each state, while as λ increases, effective occupancy is accumulated
accordingly at states which are revisited. We offer a more detailed visualization in Fig. F.2."
DIMINISHING MARGINAL UTILITY,0.11676646706586827,"= 0.0 (FR)
= 0.5
= 1.0 (SR) 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
DIMINISHING MARGINAL UTILITY,0.11826347305389222,"Figure 4.1: The λR interpolates between the FR and the SR. We visualize the λRs of the bottom
left state for the depicted policy for λ ∈{0.0, 0.5, 1.0}."
CONTINUOUS STATE SPACES,0.11976047904191617,"4.1
Continuous State Spaces"
CONTINUOUS STATE SPACES,0.12125748502994012,"When the state space is large or continuous, it becomes impossible to use a tabular representation,
and we must turn to function approximation. There are several ways to approach this, each with their
own advantages and drawbacks."
CONTINUOUS STATE SPACES,0.12275449101796407,"Feature-Based Representations
For the SR and the FR, compatibility with function approximation
is most commonly achieved by simply replacing the indicator functions in their definitions with a
base feature function ϕ : S 7→RD to create successor features [SFs; 15, 16] and first-occupancy
features [FFs; 11], respectively. The intuition in this case is that ϕ for the SR and the FR is just a
one-hot encoding of the state (or state-action pair), and so for cases when |S| is too large, we can
replace it with a compressed representation. That is, we have the following definition
Definition 4.3 (SFs). Let ϕ : S 7→RD be a base feature function. Then, the successor feature (SF)
representation φ1 : S × A 7→RD is defined as φπ
1(s, a) ≜Eπ
hP∞
k=0 γkϕ(st+k)
st, at
i
for all
s, a ∈S × A."
CONTINUOUS STATE SPACES,0.12425149700598802,"One key fact to note, however, is that due to this compression, all notion of state “occupancy” is lost
and these representations instead measure feature accumulation. For any feasible λ then, it is most
natural to define these representations using their recursive forms:
Definition 4.4 (λF). For λ ∈[0, 1] and bounded base features ϕ : S 7→[0, 1]D, the λ-feature (λF)
representation of state s is given by φπ
λ such that"
CONTINUOUS STATE SPACES,0.12574850299401197,"φπ
λ(s) ≜ϕ(s) ⊙(1 + γλEs′∼pπ(·|s)φπ
λ(s′)) + γ(1 −ϕ(s)) ⊙Es′∼pπ(·|s)φπ
λ(s′).
(4.5)"
CONTINUOUS STATE SPACES,0.12724550898203593,"In order to maintain their usefulness for policy evaluation, the main requirement of the base features
is that the reward should lie in their span. That is, a given feature function ϕ is most useful for an
associated set of reward functions R, given by"
CONTINUOUS STATE SPACES,0.12874251497005987,"R = {r | ∃w ∈RD s.t. r(s, a) = wTϕ(s, a) ∀s, a ∈S × A}.
(4.6)
However, Barreto et al. [18] demonstrate that good performance can still be achieved for an arbitrary
reward function as long as it’s sufficiently close to some r ∈R."
CONTINUOUS STATE SPACES,0.13023952095808383,"Set-Theoretic Formulations
As noted above, computing expectations of accumulated abstract
features is unsatisfying because it requires that we lose the occupancy-based interpretation of these
representations. It also restricts the agent to reward functions which lie within R. An alternative
approach to extending the SR to continuous MDPs that avoids this issue is the successor measure
[SM; 19], which treats the distribution of future states as a measure over S:"
CONTINUOUS STATE SPACES,0.1317365269461078,"M π(s, a, X) ≜ ∞
X"
CONTINUOUS STATE SPACES,0.13323353293413173,"k=0
γkP(st+k ∈X | st = s, at = a, π)
∀X ⊂S measurable,
(4.7)"
CONTINUOUS STATE SPACES,0.1347305389221557,"which can be expressed in the discrete case as M π = I + γP πM π. In the continuous case, matrices
are replaced by their corresponding measures. Note that SFs can be recovered by integrating:
φπ
1(s, a) =
R"
CONTINUOUS STATE SPACES,0.13622754491017963,"s′ M π(s, a, ds′)ϕ(s′). We can define an analogous object for the λR as follows"
CONTINUOUS STATE SPACES,0.1377245508982036,"Φπ
λ(s, X) ≜ ∞
X"
CONTINUOUS STATE SPACES,0.13922155688622753,"k=0
λnt(X,k)γkP(st+k ∈X | st = s, π)
(4.8) g"
CONTINUOUS STATE SPACES,0.1407185628742515,true = 0.5
CONTINUOUS STATE SPACES,0.14221556886227546,"0
2
4
6
8
Iterations 0 1 2 3 4 5"
CONTINUOUS STATE SPACES,0.1437125748502994,Q-Error
CONTINUOUS STATE SPACES,0.14520958083832336,Dynamic Programming
CONTINUOUS STATE SPACES,0.1467065868263473,"0.5
1.0"
CONTINUOUS STATE SPACES,0.14820359281437126,"0
500
1000
1500
Episodes 0.04 0.06 0.08 0.10 0.12"
CONTINUOUS STATE SPACES,0.1497005988023952,Tabular TD Learning
CONTINUOUS STATE SPACES,0.15119760479041916,"0
500
1000
1500
2000
Episodes 0.0 0.5 1.0 1.5 2.0"
F TD LEARNING,0.15269461077844312,"2.5
F TD Learning"
F TD LEARNING,0.15419161676646706,"Figure 5.1: The λR is required for accurate policy evaluation. Policy evaluation of the policy
depicted in Fig. 4.1 using dynamic programming, tabular TD learning, and λF TD learning produces
the most accurate value estimates when using the λR with λ = λtrue. Results are averaged over
three random seeds. Shading indicates standard error."
F TD LEARNING,0.15568862275449102,"where nt(X, k) ≜Pk−1
j=0 δst+j(X). However, this is not a measure because it fails to satisfy additivity
for λ < 1, i.e., for measurable disjoint sets A, B ⊆S, Φπ
λ(s, A ∪B) < Φπ
λ(s, A) + Φπ
λ(s, B)
(Lemma B.4). For this reason, we call Eq. (4.8) the λ operator (λO). We then minimize the following
squared Bellman error loss for Φπ
λ (dropping sub/superscripts for concision):"
F TD LEARNING,0.15718562874251496,"L(Φ) = Est,st+1∼ρ,s′∼µ
h
(φ(st, s′) −γ ¯φ(st+1, s′))2i
−2Est∼ρ[φ(st, st)]"
F TD LEARNING,0.15868263473053892,"+ 2γ(1 −λ)Est,st+1∼ρ[µ(st)φ(st, st) ¯φ(st+1, st)],
(4.9)"
F TD LEARNING,0.1601796407185629,"where ρ and µ are densities over S and Φπ
λ(s) ≜φπ
λ(s)diag(µ) in the discrete case, with φπ
λ
parametrized by neural network. ¯· indicates a stop-gradient operation, i.e., a target network. A
detailed derivation and discussion are given in Appendix H. While ρ can be any training distribution
of transitions we can sample from, we require an analytic expression for µ. Eq. (4.9) recovers the SM
loss of Touati and Ollivier [12] when λ = 1."
F TD LEARNING,0.16167664670658682,"5
Policy Evaluation, Learning, and Composition under DMU"
F TD LEARNING,0.1631736526946108,"In the following sections, we experimentally validate the formal properties of the λR and explore its
usefulness for solving RL problems with DMU. The majority of our experiments center on navigation
tasks, as we believe this is the most natural setting for studying behavior under diminishing rewards.
However, in Appendix I we also explore potential for the λR’s use other areas, such as continuous
control, even when rewards do not diminish. There is also the inherent question of whether the agent
has access to λ. In a naturalistic context, λ can be seen as an internal variable that the agent likely
knows, especially if the agent has experienced the related stimulus before. Therefore, in subsequent
experiments, treating λ as a ""given"" can be taken to imply the agent has prior experience with the
relevant stimulus. Further details for all experiments can be found in Appendix E."
POLICY EVALUATION,0.16467065868263472,"5.1
Policy Evaluation"
POLICY EVALUATION,0.1661676646706587,"In Section 4, we showed that in order to perform policy evaluation under DMU, an agent is required
to learn the λR. In our first experimental setting, we verify this analysis empirically for the policy
depicted in Fig. 4.1 with a rewarded location in the state indicated by a g in Fig. 5.1 with λtrue = 0.5.
We then compare the performance for agents using different values of λ across dynamic programming
(DP), tabular TD learning, and λF TD learning with Laplacian features. For the latter two, we use
a linear function approximator with a one-hot encoding of the state as the base feature function.
We then compute the Q-values using the λR with λ ∈{0.5, 1.0} (with λ = 1 corresponding to the
SR) and compare the resulting value estimates to the true Q-values. Consistent with our theoretical
analysis, Fig. 5.1 shows that the λR with λ = λtrue is required to produce accurate value estimates."
POLICY LEARNING,0.16766467065868262,"5.2
Policy Learning"
POLICY LEARNING,0.1691616766467066,"To demonstrate that λR is useful in supporting policy improvement under diminishing rewards,
we implemented modified forms of Q-learning [20] (which we term Qλ-learning) and advantage"
POLICY LEARNING,0.17065868263473055,true = 0.5
POLICY LEARNING,0.1721556886227545,"0
100
200
300
400
500
Episodes 0 5 10 15 20 25 30"
POLICY LEARNING,0.17365269461077845,Return
POLICY LEARNING,0.1751497005988024,Q -Learning
POLICY LEARNING,0.17664670658682635,"0.0
0.5
1.0"
POLICY LEARNING,0.1781437125748503,"0
2000
4000
6000
Episodes 0 5 10 15 20 25"
POLICY LEARNING,0.17964071856287425,RNN A2C-
POLICY LEARNING,0.18113772455089822,"Figure 5.2: The λR is required for strong performance. We apply a tabular Q-learning-style
algorithm and deep actor-critic algorithm to policy optimization in the TwoRooms domain. The blue
locations indicate reward, and the black triangle shows the agent’s position. Results are averaged
over three random seeds. Shading indicates standard error."
POLICY LEARNING,0.18263473053892215,"actor-critic [A2C; 21] and applied them to the TwoRooms domain from the NeuroNav benchmark
task set [22] (see Fig. 5.2). For a transition (st, at, st+1), we define the following operator:"
POLICY LEARNING,0.18413173652694612,"T ⋆
λ Φ ≜1(st) ⊙(1 + λγΦ(st+1, at+1)) + γ(1 −1(st)) ⊙Φ(st+1, at+1),
(5.1)"
POLICY LEARNING,0.18562874251497005,"where at+1 = argmaxa Qλ(st, a) = argmaxa rTΦ(st, a). This is an improvement operator for
Qλ. The results in Fig. 5.2 show that Qλ-learning outperforms standard Q-learning (λ = 1) for
diminishing rewards, and that the “correct” λ produces the best performance. To implement A2C
with a λR critic, we modified the standard TD target in a similar manner as follows:"
POLICY LEARNING,0.18712574850299402,"TλV (st) = r(st) + γ(V (st+1) + (λ −1)wT(ϕ(st) ⊙φλ(st+1))),
(5.2)"
POLICY LEARNING,0.18862275449101795,"where ϕ were one-hot state encodings, and the policy, value function, and λF were output heads
of a shared LSTM network [23]. Note this target is equivalent to Definition 4.4 multiplied by the
reward (derivation in Appendix E). Fig. 5.2 shows again that correct value targets lead to improved
performance. Videos of agent behavior can be found at lambdarepresentation.github.io."
POLICY COMPOSITION,0.19011976047904192,"5.3
Policy Composition"
POLICY COMPOSITION,0.19161676646706588,"As we’ve seen, DMU problems of this form have an interesting property wherein solving one task
requires the computation of a representation which on its own is task agnostic. In the same way that
the SR and FR facilitate generalization across reward functions, the λR facilitates generalization
across reward functions with different ¯rs.The following result shows that there is a benefit to having
the “correct” λ for a given resource.
Theorem 5.1 (GPI). Let {Mj}n
j=1 ⊆M and M ∈M be a set of tasks in an environment M and
let Qπ∗
j denote the action-value function of an optimal policy of Mj when executed in M. Assume
that the agent uses diminishing rate ˆλ that may differ from the true environment diminishing rate λ.
Given estimates ˜Qπj such that ∥Qπ∗
j −˜Qπj∥∞≤ϵ for all j, define"
POLICY COMPOSITION,0.19311377245508982,"π(s) ∈argmax
a
max
j
˜Qπj(s, a). Then,"
POLICY COMPOSITION,0.19461077844311378,"Qπ(s, a) ≥max
j
Qπ∗
j (s, a) −
1
1 −γ"
POLICY COMPOSITION,0.19610778443113772,"
2ϵ + |λ −ˆλ|∥r∥∞+ γ(1 −λ)r(s, a) 1 −λγ 
."
POLICY COMPOSITION,0.19760479041916168,"Note that for λ = 1, we recover the original GPI bound due to Barreto et al. [18] with an additional
term quantifying error accrued if incorrectly assuming λ < 1."
POLICY COMPOSITION,0.19910179640718562,"Tabular Navigation
We can see this result reflected empirically in Fig. 5.3, where we consider
the following experimental set-up in the classic FourRooms domain [24]. The agent is assumed to
be given or have previously acquired four policies {π0, π1, π2, π3} individually optimized to reach
rewards located in each of the four rooms of the environment. There are three reward locations
{g0, g1, g2} scattered across the rooms, each with its own initial reward and all with λ = 0.5. At the"
POLICY COMPOSITION,0.20059880239520958,"0.0
0.5
1.0
15.0 17.5 20.0 22.5 25.0"
POLICY COMPOSITION,0.20209580838323354,"Return = 0.5 s0
g0 g1 g2 = 0.0 s0
g0 g1 g2 = 1.0 s0
g0 g1 g2 0 5 10 15 20 25 30 35 40 t"
POLICY COMPOSITION,0.20359281437125748,"Tabular GPI
Example Trajectories"
POLICY COMPOSITION,0.20508982035928144,"Figure 5.3: Tabular GPI. (Left) Average returns obtained by agents performing GPE+GPI using
λRs with λ ∈{0.0, 0.5, 1.0} over 50 episodes. Error bars indicate standard error. (Right) Sample
trajectories. Agents with λ set too high overstay in rewarding states, and those with λ too low leave
too early."
POLICY COMPOSITION,0.20658682634730538,"0.0
0.5
1.0
30 32 34 36 38 40"
POLICY COMPOSITION,0.20808383233532934,Return
POLICY COMPOSITION,0.20958083832335328,"Values
= 0.0"
POLICY COMPOSITION,0.21107784431137724,"Values
= 0.5"
POLICY COMPOSITION,0.2125748502994012,"Values
= 1.0
Pixel-based GPI
Feature Analysis
Example Observation"
POLICY COMPOSITION,0.21407185628742514,"Figure 5.4: Pixel-Based GPI. Performance is strongest for agents using the correct λ = 0.5. PCA
on the learned features in each underlying environment state shows that the λFs capture the value-
conditioned structure of the environment."
POLICY COMPOSITION,0.2155688622754491,"beginning of each episode, an initial state s0 is sampled uniformly from the set of available states.
An episode terminates either when the maximum reward remaining in any of the goal states is less
than 0.1 or when the maximum number of steps H = 40 is reached (when λ = 1, the latter is the
only applicable condition). For each of the four policies, we learn λRs with λ ∈{0.0, 0.5, 1.0} using
dynamic programming and record the returns obtained while performing GPE+GPI with each of
these representations over 50 episodes. Bellman error curves for the λRs are shown in Fig. F.3, and
demonstrate that convergence is faster for lower λ. In the left panel of Fig. 5.3, we can indeed see that
using the correct λ (0.5) nets the highest returns. Example trajectories for each agent λ are shown in
the remaining panels."
POLICY COMPOSITION,0.21706586826347304,"Pixel-Based Navigation
We verified that the previous result is reflected in larger scales by repeating
the experiment in a partially-observed version of FourRooms in which the agent receives 128 × 128
RGB egocentric observations of the environment (Fig. 5.4, left) with H = 50. In this case, the
agent learns λFs for each policy for λ ∈{0.0, 0.5, 1.0}, where each λF is parameterized by a
feedforward convolutional network with the last seven previous frames stacked to account for the
partial observability. The base features were Laplacian eigenfunctions normalized to [0, 1], which
which were shown by Touati et al. [25] to perform the best of all base features for SFs across a range
of environments including navigation."
UNDERSTANDING NATURAL BEHAVIOR,0.218562874251497,"6
Understanding Natural Behavior"
UNDERSTANDING NATURAL BEHAVIOR,0.22005988023952097,"Naturalistic environments often exhibit diminishing reward and give insight into animal behavior.
The problem of foraging in an environment with multiple diminishing food patches (i.e., reward
states) is of interest in behavioral science [26–28]. The cornerstone of foraging theory is the
marginal value theorem [MVT; 28, 29], which states that the optimal time to leave a patch is when
the patch’s reward rate matches the average reward rate of the environment. However, the MVT
does not describe which patch to move to once an agent leaves its current patch. We show that
the λO recovers MVT-like behavior in discrete environments and improves upon the MVT by
not only predicting when agents should leave rewarding patches, but also where they should go.
Moreover, we provide a scheme for learning λ alongside the λO using feedback from the environment."
UNDERSTANDING NATURAL BEHAVIOR,0.2215568862275449,"Figure 6.1: λO trained via FB. a) λ values of two agents in FourRooms, one which learns λ and one
which does not. b) Performance of the two agents from (a). Learning λ improves performance. c)
Reward structure and starting state of the asymmetric environment. d) Trajectory of an agent with
λ = 1. The optimal strategy is to reach the high reward state and exploit it ad infinitum. e) Trajectory
of an agent with λ = 0.1. The optimal strategy is to exploit each reward state for a few time steps
before moving to the next reward state."
UNDERSTANDING NATURAL BEHAVIOR,0.22305389221556887,"To learn the λO, we take inspiration from the FBR [12] and use the following parametrization:
Φπz
λ (s, a, s′) = F(s, a, z)⊤B(s′)µ(s′) and πz(s) = argmaxa F(s, a, z)⊤z, where µ is a density
with full support on S, e.g., uniform. We then optimize using the the loss in Eq. (4.9) under this
parameterization (details in Appendix E). Given a reward function r : S →R at evaluation time,
the agent acts according to argmaxa F(s, a, zR)⊤zR, where zR = Es∼µ[r(s)B(s)]. Because the
environment is non-stationary, zR has to be re-computed at every time step. To emulate a more
realistic foraging task, the agent learns λ by minimizing the loss in Eq. (H.1) in parallel with the loss"
UNDERSTANDING NATURAL BEHAVIOR,0.2245508982035928,"L(λ) = Est,st+1∼ρ
h
1(st = st+1) (r(st+1) −λr(st))2i
,"
UNDERSTANDING NATURAL BEHAVIOR,0.22604790419161677,"which provably recovers the correct value of λ provided that ρ is sufficiently exploratory. In
Appendix H.1 we provide experiments showing that using an incorrect value of λ leads to poor
performance on tabular tasks. In Fig. 6.1 we show that the agent learns the correct value of λ,
increasing its performance. We illustrate the behavior of the λO in an asymmetric environment that
has one large reward state on one side and many small reward states (with higher total reward) on
the other. Different values of λ lead to very different optimal foraging strategies, which the λO
recovers and exhibits MVT-like behavior (see Appendix H.2 for a more detailed analysis). Our hope
is that the λR may provide a framework for new theoretical studies of foraging behavior and possibly
mechanisms for posing new hypotheses. For example, an overly large λ may lead to overstaying in
depleted patches, a frequently observed phenomenon [30]."
CONCLUSION,0.2275449101796407,"7
Conclusion"
CONCLUSION,0.22904191616766467,"Limitations
Despite its advantages, there are several drawbacks to the representation which are
a direct result of the challenge of the DMU setting. First, the λR is only useful for transfer across
diminishing reward functions when the value of λ at each state is consistent across tasks. In natural
settings, this is fairly reasonable, as λ can be thought of as encoding the type of the resource available
at each state (i.e., each resource has its own associated decay rate). Second, as noted in Section 4.1,
the λR does not admit a measure-theoretic formulation, which makes it challenging to define a
principled, occupancy-based version compatible with continuous state spaces. Third, the λR is a
prospective representation, and so while it is used to correctly evaluate a policy’s future return under
DMU, it is not inherently memory-based and so performs this evaluation as if the agent hasn’t visited
locations with diminishing reward before. Additional mechanisms (i.e., recurrence or frame-stacking)
are necessary to account for previous visits. Finally, the λR is dependent on an episodic task setting
for rewards to reset, as otherwise the agent would eventually consume all reward in the environment.
An even more natural reward structure would include a mechanism for reward replenishment in
addition to depletion. We describe several such candidates in Appendix J, but leave a more thorough
investigation of this issue to future work."
CONCLUSION,0.23053892215568864,"In this work, we aimed to lay the groundwork for understanding policy evaluation, learning, and
composition under diminishing rewards. To solve such problems, we introduced—and showed the
necessity of—the λ representation, which generalizes the SR and FR. We demonstrated its usefulness
for rapid policy evaluation and by extension, composition, as well as control. We believe the λR
represents a useful step in the development of state representations for naturalistic environments."
REFERENCES,0.23203592814371257,References
REFERENCES,0.23353293413173654,[1] Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk.
REFERENCES,0.23502994011976047,"Econometrica, 47(2):263–292, 1979."
REFERENCES,0.23652694610778444,"[2] Matthew Rabin. Risk aversion and expected-utility theory: a calibration theorem. Econometrica,
68(5):1281–1292, 2000. doi: 10.2307/2999450."
REFERENCES,0.23802395209580837,"[3] Alex Pine, Ben Seymour, Jonathan P Roiser, Peter Bossaerts, Karl J Friston, H Valerie Curran,
and Raymond J Dolan. Encoding of marginal utility across time in the human brain. Journal of
Neuroscience, 29(30):9575–9581, 2009."
REFERENCES,0.23952095808383234,"[4] Martin L. Puterman. Markov decision processes: discrete stochastic dynamic programming.
John Wiley and Sons, 2010."
REFERENCES,0.2410179640718563,[5] Hermann Heinrich Gossen and Rudolph C Blitz. The laws of human relations and the rules of
REFERENCES,0.24251497005988024,"human action derived therefrom. Mit Press Cambridge, MA, 1983."
REFERENCES,0.2440119760479042,[6] Kenneth J Arrow. The theory of risk-bearing: small and great risks. Journal of risk and
REFERENCES,0.24550898203592814,"uncertainty, 12:103–111, 1996."
REFERENCES,0.2470059880239521,"[7] John W Pratt. Risk aversion in the small and in the large. In Uncertainty in economics, pages
59–79. Elsevier, 1978."
REFERENCES,0.24850299401197604,"[8] Nathan Wispinski, Andrew Butcher, Kory Wallace Mathewson, Craig S Chapman, Matthew
Botvinick, and Patrick M. Pilarski. Adaptive patch foraging in deep reinforcement learning
agents. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https:
//openreview.net/forum?id=a0T3nOP9sB."
REFERENCES,0.25,"[9] Sergey Shuvaev, Sarah Starosta, Duda Kvitsiani, Adam Kepecs, and Alexei Koulakov. R-
learning in actor-critic model offers a biologically relevant mechanism for sequential decision-
making. Advances in neural information processing systems, 33:18872–18882, 2020."
REFERENCES,0.25149700598802394,"[10] Peter Dayan. Improving generalization for temporal difference learning: The successor repre-
sentation. Neural Computation, 5(4):613–624, 1993. doi: 10.1162/neco.1993.5.4.613."
REFERENCES,0.25299401197604793,"[11] Ted Moskovitz, Spencer R Wilson, and Maneesh Sahani. A first-occupancy representation for
reinforcement learning. In International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=JBAZe2yN6Ub."
REFERENCES,0.25449101796407186,[12] Ahmed Touati and Yann Ollivier. Learning one representation to optimize all rewards. Advances
REFERENCES,0.2559880239520958,"in Neural Information Processing Systems, 34:13–23, 2021."
REFERENCES,0.25748502994011974,"[13] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT
Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.
html."
REFERENCES,0.25898203592814373,"[14] Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. The hippocampus as
a predictive map. Nature neuroscience, 20(11):1643–1653, 2017."
REFERENCES,0.26047904191616766,"[15] Andre Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforcement
learning with generalized policy updates. Proceedings of the National Academy of Sciences,
117(48):30079–30087, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1907370117. URL https:
//www.pnas.org/content/117/48/30079."
REFERENCES,0.2619760479041916,"[16] André Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt,
and David Silver. Successor features for transfer in reinforcement learning. Advances in neural
information processing systems, 30, 2017."
REFERENCES,0.2634730538922156,[17] Maor Gaon and Ronen Brafman. Reinforcement learning with non-markovian rewards. In
REFERENCES,0.26497005988023953,"Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 3980–3987,
2020."
REFERENCES,0.26646706586826346,"[18] André Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel
Mankowitz, Augustin Žídek, and Rémi Munos. Transfer in deep reinforcement learning using
successor features and generalised policy improvement, 2019."
REFERENCES,0.2679640718562874,[19] Léonard Blier and Yann Ollivier. The description length of deep learning models. Advances in
REFERENCES,0.2694610778443114,"Neural Information Processing Systems, 31, 2018."
REFERENCES,0.27095808383233533,"[20] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279–292, 1992."
REFERENCES,0.27245508982035926,"[21] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
ment learning. In International conference on machine learning, pages 1928–1937. PMLR,
2016."
REFERENCES,0.27395209580838326,"[22] Arthur Juliani, Samuel Barnett, Brandon Davis, Margaret Sereno, and Ida Momennejad. Neuro-
nav: a library for neurally-plausible reinforcement learning. arXiv preprint arXiv:2206.03312,
2022."
REFERENCES,0.2754491017964072,"[23] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9
(8):1735–1780, 1997."
REFERENCES,0.27694610778443113,"[24] Richard S. Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A
framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112
(1):181–211, 1999.
URL https://www.sciencedirect.com/science/article/pii/
S0004370299000521."
REFERENCES,0.27844311377245506,"[25] Ahmed Touati, Jérémy Rapin, and Yann Ollivier. Does zero-shot reinforcement learning
exist?
In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=MYEap_OcQI."
REFERENCES,0.27994011976047906,"[26] Benjamin Y Hayden, John M Pearson, and Michael L Platt. Neuronal basis of sequential
foraging decisions in a patchy environment. Nature neuroscience, 14(7):933–939, 2011."
REFERENCES,0.281437125748503,"[27] Tehrim Yoon, Robert B Geary, Alaa A Ahmed, and Reza Shadmehr. Control of movement vigor
and decision making during foraging. Proceedings of the National Academy of Sciences, 115
(44):E10476–E10485, 2018."
REFERENCES,0.28293413173652693,"[28] Anthony S Gabay and Matthew AJ Apps. Foraging optimally in social neuroscience: computa-
tions and methodological considerations. Social Cognitive and Affective Neuroscience, 16(8):
782–794, 2021."
REFERENCES,0.2844311377245509,"[29] Eric L. Charnov. Optimal foraging, the marginal value theorem. Theoretical Population Biology,
9(2):129–136, 1976. ISSN 0040-5809."
REFERENCES,0.28592814371257486,"[30] Peter Nonacs. State dependent behavior and the marginal value theorem. Behavioral Ecology,
12(1):71–83, 2001."
REFERENCES,0.2874251497005988,"[31] Ruosong Wang, Hanrui Zhang, Devendra Singh Chaplot, Denis Garagi´c, and Ruslan Salakhut-
dinov. Planning with submodular objective functions. arXiv preprint arXiv:2010.11863, 2020."
REFERENCES,0.28892215568862273,"[32] Manish Prajapat, Mojmír Mutn`y, Melanie N Zeilinger, and Andreas Krause. Submodular
reinforcement learning. arXiv preprint arXiv:2307.13372, 2023."
REFERENCES,0.2904191616766467,"[33] Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is
enough for convex mdps, 2021."
REFERENCES,0.29191616766467066,"[34] Eitan Altman. Constrained Markov decision processes. Routledge, 2021."
REFERENCES,0.2934131736526946,"[35] Ted Moskovitz, Brendan O’Donoghue, Vivek Veeriah, Sebastian Flennerhag, Satinder Singh,
and Tom Zahavy. Reload: Reinforcement learning with optimistic ascent-descent for last-iterate
convergence in constrained mdps. In International Conference on Machine Learning, pages
25303–25336. PMLR, 2023."
REFERENCES,0.2949101796407186,"[36] Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva,
Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al. Behaviour suite for
reinforcement learning. arXiv preprint arXiv:1908.03568, 2019."
REFERENCES,0.2964071856287425,"[37] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint"
REFERENCES,0.29790419161676646,"arXiv:1607.06450, 2016."
REFERENCES,0.2994011976047904,"[38] Martin Riedmiller.
Neural fitted q iteration–first experiences with a data efficient neural
reinforcement learning method. In Machine Learning: ECML 2005: 16th European Conference
on Machine Learning, Porto, Portugal, October 3-7, 2005. Proceedings 16, pages 317–328.
Springer, 2005."
REFERENCES,0.3008982035928144,[39] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. Advances in neural
REFERENCES,0.3023952095808383,"information processing systems, 5, 1992."
REFERENCES,0.30389221556886226,"[40] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, 2018."
REFERENCES,0.30538922155688625,"[41] Ted Moskovitz, Michael Arbel, Ferenc Huszar, and Arthur Gretton. Efficient wasserstein natural
gradients for reinforcement learning, 2020."
REFERENCES,0.3068862275449102,"[42] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor, 2018."
REFERENCES,0.3083832335329341,"[43] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym, 2016."
REFERENCES,0.30988023952095806,"[44] Ted Moskovitz, Jack Parker-Holder, Aldo Pacchiano, Michael Arbel, and Michael Jordan. Tacti-
cal optimism and pessimism for deep reinforcement learning. Advances in Neural Information
Processing Systems, 34:12849–12863, 2021."
REFERENCES,0.31137724550898205,"[45] Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006."
REFERENCES,0.312874251497006,Appendix: A State Representation for Diminishing Rewards
REFERENCES,0.3143712574850299,"GIFs of navigation agents can be found at lambdarepresentation.github.io
and in the supplementary material."
REFERENCES,0.3158682634730539,"A
Derivation of λR Recursion"
REFERENCES,0.31736526946107785,We provide a step-by-step derivation of the λR recursion in Eq. (4.3):
REFERENCES,0.3188622754491018,"Φπ
λ(s, s′) = E "" ∞
X"
REFERENCES,0.3203592814371258,"k=0
λnt(s′,k)γk1(st+k = s′)
st = s # = E """
REFERENCES,0.3218562874251497,"1(st = s′) + ∞
X"
REFERENCES,0.32335329341317365,"k=1
λnt(s′,k)γk1(st+k = s′)
st = s #"
REFERENCES,0.3248502994011976,"(i)= E """
REFERENCES,0.3263473053892216,"1(st = s′) + λnt(s′,1)γ ∞
X"
REFERENCES,0.3278443113772455,"k=1
λnt+1(s′,k)γk−11(st+k = s′)
st = s #"
REFERENCES,0.32934131736526945,"(ii)
= E """
REFERENCES,0.33083832335329344,"1(st = s′) + 1(st = s′)λγ ∞
X"
REFERENCES,0.3323353293413174,"k=1
λnt+1(s′,k)γk−11(st+k = s′)"
REFERENCES,0.3338323353293413,"+ γ(1 −1(st = s′)) ∞
X"
REFERENCES,0.33532934131736525,"k=1
λnt+1(s′,k)γk−11(st+k = s′)
st = s #"
REFERENCES,0.33682634730538924,"= 1(st = s′) + 1(st = s′)λγEst+1∼pπΦπ
λ(st+1, s′)
+ γ(1 −1(st = s′))Est+1∼pπΦπ
λ(st+1, s′)
= 1(st = s′)(1 + γλEst+1∼pπΦπ
λ(st+1, s′))
+ γ(1 −1(st = s′))Est+1∼pπΦπ
λ(st+1, s′),
(A.1)"
REFERENCES,0.3383233532934132,"where (i) is because nt(s′, k) = nt(s′, 1) + nt+1(s′, k) and (ii) is because"
REFERENCES,0.3398203592814371,"λnt(s′,1) = λ1(st=s′) = 1(st = s′)λ + (1 −1(st = s′))."
REFERENCES,0.3413173652694611,"B
Theoretical Analysis"
REFERENCES,0.34281437125748504,"Here, we provide proofs for the theoretical results in the main text."
REFERENCES,0.344311377245509,"Lemma 3.1 (Impossibility; Informal). Given a reward function of the form Eq. (3.1),
it is impossible to define a Bellman equation solely using the resulting value function
and immediate reward."
REFERENCES,0.3458083832335329,"Before providing the formal statement and proof for Lemma 3.1, we introduce a
definition for a Bellman operator."
REFERENCES,0.3473053892215569,"Definition B.1 (Bellman Operator). A Bellman operator is a contractive operator
R|S| →R|S| that depends solely on ¯r, one-step expectations under pπ, and learning
hyperparameters (in our case γ and λ)."
REFERENCES,0.34880239520958084,We can now give a formal statement of Lemma 3.1:
REFERENCES,0.3502994011976048,"Lemma B.1 (Impossibility; Formal). Assume that |S| > 1. Then, there does not
exist a Bellman operator T with fixed point V π."
REFERENCES,0.35179640718562877,"Proof. Assume for a contradiction that T is a Bellman operator. By the Banach
fixed-point theorem, V π must be the unique fixed point of T. Hence, TV π must take
on the following form (see the proof of Lemma 3.1 in Appendix B): for s ∈S,
(TV π)(s) = ¯r(s) + γEs′∼pπ(·|s)V π(s′) + γ(λ −1)¯r(s)Es′∼pπ(·|s)Φπ
λ(s′, s).
For the assumption to hold, there must exist a function f such that, for any s ∈S,
Es′∼pπ(·|s)Φπ
λ(s′, s) = Epπ(·|s)f(¯r, Vπ, γ, λ, s).
Now, by definition,"
REFERENCES,0.3532934131736527,"V π(s) =
X"
REFERENCES,0.35479041916167664,"s′∈S
Φπ
λ(s, s′)¯r(s′)."
REFERENCES,0.3562874251497006,"¯r is a vector in R|S|, so as long as S > 1, ¯r⊥is non-trivial. Fix any ¯r, Vπ, s. Pick a
vector w ∈¯r⊥\ {0} and define
˜Φπ
λ(s, s′) := Φπ
λ(s, s′) + w(s′)
for any s, s′ ∈S. Note that
X"
REFERENCES,0.35778443113772457,"s′∈S
˜Φπ
λ(s, s′)¯r(s′) =
X"
REFERENCES,0.3592814371257485,"s′∈S
Φπ
λ(s, s′)¯r(s′) +
X"
REFERENCES,0.36077844311377244,"s′∈S
w(s′)¯r(s′) = V π(s),"
REFERENCES,0.36227544910179643,"as w ⊥¯r. However,"
REFERENCES,0.36377245508982037,"Es′∼pπ(·|s) ˜Φπ
λ(s′, s) =
X"
REFERENCES,0.3652694610778443,"s′∈S
pπ(s′|s)Φπ
λ(s′, s) +
X"
REFERENCES,0.36676646706586824,"s′∈S
pπ(s′|s)w(s)."
REFERENCES,0.36826347305389223,"The final term evaluates to w(s). Because w ̸= 0, there must exist some s such that
w(s) ̸= 0. For this s, we have a single input (¯r, Vπ, γ, λ, s) to f that corresponds to
two distinct outputs: Es′∼pπ(·|s)Φπ
λ(s′, s) and Es′∼pπ(·|s) ˜Φπ
λ(s′, s)."
REFERENCES,0.36976047904191617,"Hence, f is a one-to-many mapping: for fixed input, there is more than one output.
Therefore, f is not a function, yielding a contradiction."
REFERENCES,0.3712574850299401,"The following establishes Gπ
λ as a contraction.
Lemma B.2 (Contraction). Let Gπ
λ be the operator as defined in Definition 4.2 for
some stationary policy π. Then for any two matrices Φ, Φ′ ∈R|S|×|S|,
|Gπ
λΦ(s, s′) −Gπ
λΦ′(s, s′)| ≤γ|Φ(s, s′) −Φ′(s, s′)|."
REFERENCES,0.3727544910179641,"Proof. We have
|(Gπ
λΦ −Gπ
λΦ′)s,s′| = |(I ⊙(11T + γλP πΦ) + γ(11T −I) ⊙P πΦ"
REFERENCES,0.37425149700598803,"−I ⊙(11T + γλP πΦ′) −γ(11T −I) ⊙P πΦ′)s,s′|"
REFERENCES,0.37574850299401197,"= |(I ⊙γλP π(Φ −Φ′) + γ(11T −I) ⊙P π(Φ −Φ′))s,s′|"
REFERENCES,0.3772455089820359,"= |((I ⊙λ11T + 11T −I) ⊙γP π(Φ −Φ′))s,s′|"
REFERENCES,0.3787425149700599,"(i)
≤|(γP π(Φ −Φ′))s,s′|
= γ|(P π(Φ −Φ′))s,s′|
≤γ|(Φ −Φ′)s,s′|,"
REFERENCES,0.38023952095808383,where (i) comes from using λ ≤1 and simplifying.
REFERENCES,0.38173652694610777,"Note that we can actually get a tighter contraction factor of λγ for s = s′. Given
this contractive property, we can prove its convergence with the use of the following
lemma.
Lemma B.3 (Max λR). The maximum possible value of Φπ
λ(s, s′) is"
REFERENCES,0.38323353293413176,1(s = s′) + (1 −1(s = s′))γ
REFERENCES,0.3847305389221557,"1 −λγ
."
REFERENCES,0.38622754491017963,"Proof. For s = s′,"
REFERENCES,0.38772455089820357,"Φπ
λ(s, s) = 1 + λγEst+1∼pπ(·|st)Φπ
λ(st+1, s)."
REFERENCES,0.38922155688622756,"This is just the standard SR recursion with discount factor λγ, so the maximum is ∞
X"
REFERENCES,0.3907185628742515,"k=0
(λγ)k =
1
1 −λγ .
(B.1)"
REFERENCES,0.39221556886227543,"For s ̸= s′, 1(st = s′) = 0, so"
REFERENCES,0.3937125748502994,"Φπ
λ(s, s′) = γEst+1∼pπ(·|st)Φπ
λ(st+1, s′)."
REFERENCES,0.39520958083832336,"Observe that Φπ
λ(s, s) ≥Φπ
λ(s, s′) for s′ ̸= s, so the maximum is attained for
st+1 = s′. We can then use the result for s = s′ to get"
REFERENCES,0.3967065868263473,"Φπ
λ(s, s′) = γ

1
1 −λγ"
REFERENCES,0.39820359281437123,"
.
(B.2)"
REFERENCES,0.3997005988023952,Combining Eq. (B.1) and Eq. (B.2) yields the desired result.
REFERENCES,0.40119760479041916,"Proposition 4.1 (Convergence). Under the conditions assumed above, set Φ(0) =
(1 −λ)I. For k = 1, 2, . . . , suppose that Φ(k+1) = Gπ
λΦ(k). Then"
REFERENCES,0.4026946107784431,"|(Φ(k) −Φπ
λ)s,s′| ≤
γk+1"
REFERENCES,0.4041916167664671,1 −λγ .
REFERENCES,0.405688622754491,"Proof. Using the notation Xs,s′ = X(s, s′) for a matrix X:"
REFERENCES,0.40718562874251496,"|(Φ(k) −Φπ
λ)s,s′| = |(Gk
λΦ(0) −Gk
λΦπ
λ)s,s′|"
REFERENCES,0.4086826347305389,"= |(Gk
λΦ(0) −Φπ
λ)s,s′|"
REFERENCES,0.4101796407185629,"(i)
≤γk|(Φ(0) −Φπ
λ)s,s′|"
REFERENCES,0.4116766467065868,"(ii)
= γkΦπ
λ(s, s′)"
REFERENCES,0.41317365269461076,"(iii)
≤
γk+1 1 −λγ (B.3)"
REFERENCES,0.41467065868263475,"where (i) is due to Lemma B.2, (ii) is because Φ(0)(s, s′) = 0 for s ̸= s′, and (iii)
is due to Lemma B.3."
REFERENCES,0.4161676646706587,"Lemma B.4 (Subadditivity). For any s ∈S, policy π, λ ∈[0, 1), and disjoint
measurable sets A, B ⊆S,"
REFERENCES,0.4176646706586826,"Φπ
λ(s, A ∪B) < Φπ
λ(s, A) + Φπ
λ(s, B)."
REFERENCES,0.41916167664670656,"Proof. Note that for disjoint sets A, B, we have nt(A∪B, k) = nt(A, k)+nt(B, k).
Hence, conditioned on some policy π and st = s,"
REFERENCES,0.42065868263473055,"λnt(A∪B,k)P(st+k ∈A ∪B) = λnt(A,k)λnt(B,k)P(st+k ∈A) + λnt(A,k)λnt(B,k)P(st+k ∈B)"
REFERENCES,0.4221556886227545,"≤λnt(A,k)P(st+k ∈A) + λnt(B,k)P(st+k ∈B),"
REFERENCES,0.4236526946107784,"where the first line follows from P(st+k ∈A ∪B) = P(st+k ∈A) + P(st+k ∈B).
Equality holds over all A, B, t, k if and only if λ = 1. Summing over k yields the
result."
REFERENCES,0.4251497005988024,"B.1
Proof of Theorem 5.1"
REFERENCES,0.42664670658682635,"We first prove two results, which rely throughout on the fact that Φλ(s, a, s′) ≤
1
1−λγ
for all s, a, s′, which follows from Lemma B.3. For simplicity, we also assume
throughout that all rewards are non-negative, but this assumption can easily be
dropped by taking absolute values of rewards. The proofs presented here borrow
ideas from those of [16]."
REFERENCES,0.4281437125748503,"Lemma B.5. Let {Mj}n
j=1 ⊆M and M ∈M be a set of tasks in an environment M
with diminishing rate λ and let Qπ∗
j denote the action-value function of an optimal
policy of Mj when executed in M. Given estimates ˜Qπj such that ∥Qπ∗
j −˜Qπj∥∞≤ϵ
for all j, define"
REFERENCES,0.4296407185628742,"π(s) ∈argmax
a
max
j
˜Qπj(s, a). Then,"
REFERENCES,0.4311377245508982,"Qπ(s, a) ≥max
j
Qπ∗
j (s, a) −
1
1 −γ"
REFERENCES,0.43263473053892215,"
2ϵ + γ(1 −λ)r(s, a) 1 −λγ 
,"
REFERENCES,0.4341317365269461,where r denotes the reward function of M.
REFERENCES,0.4356287425149701,"Proof. Define ˜Qmax(s, a) := maxj ˜Qπj(s, a) and Qmax(s, a) := maxj Qπ∗
j (s, a). Let
T ν denote the Bellman operator of a policy ν in task M. For all (s, a) ∈S × A and"
REFERENCES,0.437125748502994,"all j,"
REFERENCES,0.43862275449101795,"T π
i ˜Qmax(s, a) = r(s, a) + γ
X"
REFERENCES,0.44011976047904194,"s′
p(s′|s, a)

(λ −1)ri(s, a)Φπ(s′, π(s′), s) + ˜Qmax(s′, π(s′))
"
REFERENCES,0.4416167664670659,"= r(s, a) + γ
X"
REFERENCES,0.4431137724550898,"s′
p(s′|s, a)

(λ −1)ri(s, a)Φπ(s′, π(s′), s) + max
b
˜Qmax(s′, b)
"
REFERENCES,0.44461077844311375,"≥r(s, a) + γ
X"
REFERENCES,0.44610778443113774,"s′
p(s′|s, a)

(λ −1)ri(s, a)Φπ(s′, π(s′), s) + max
b
Qmax(s′, b)

−γϵ"
REFERENCES,0.4476047904191617,"≥r(s, a) + γ
X"
REFERENCES,0.4491017964071856,"s′
p(s′|s, a)
 
(λ −1)ri(s, a)Φπ(s′, π(s′), s) + Qmax(s′, π∗
j(s′))

−γϵ"
REFERENCES,0.4505988023952096,"≥r(s, a) + γ
X"
REFERENCES,0.45209580838323354,"s′
p(s′|s, a)

(λ −1)ri(s, a)Φπ(s′, π(s′), s) + Q
π∗
j
i (s′, π∗
j(s′))

−γϵ"
REFERENCES,0.4535928143712575,"= r(s, a) + γ
X"
REFERENCES,0.4550898203592814,"s′
p(s′|s, a)

(λ −1)ri(s, a)Φπ∗
j (s′, π∗
j(s′), s) + Q
π∗
j
i (s′, π∗
j(s′))

−γϵ"
REFERENCES,0.4565868263473054,"+ γ(λ −1)r(s, a)
X"
REFERENCES,0.45808383233532934,"s′
p(s′|s, a)
 
Φπ(s′, π(s′), s) −Φπ∗
j (s′, π∗
j(s′), s)
"
REFERENCES,0.4595808383233533,"≥T
π∗
j
i Q
π∗
j
i (s, a) −γϵ −γ(1 −λ)r(s, a) 1 −λγ"
REFERENCES,0.46107784431137727,"= Q
π∗
j
i (s, a) −γϵ −γ(1 −λ)r(s, a)"
REFERENCES,0.4625748502994012,"1 −λγ
."
REFERENCES,0.46407185628742514,"This holds for any j, so"
REFERENCES,0.4655688622754491,"T π ˜Qmax(s, a) ≥max
j
Q
π∗
j
i (s, a) −γϵ −γ(1 −λ)r(s, a) 1 −λγ"
REFERENCES,0.46706586826347307,"= Qmax(s, a) −γϵ −γ(1 −λ)r(s, a) 1 −λγ"
REFERENCES,0.468562874251497,"≥˜Qmax(s, a) −ϵ −γϵ −γ(1 −λ)r(s, a)"
REFERENCES,0.47005988023952094,"1 −λγ
."
REFERENCES,0.47155688622754494,"Next, note that for any c ∈R,"
REFERENCES,0.47305389221556887,"T π( ˜Qmax(s, a) + c) = T π ˜Qmax(s, a) + γ
X"
REFERENCES,0.4745508982035928,"s′
p(s′|s, a)c"
REFERENCES,0.47604790419161674,"= T π ˜Qmax(s, a) + γc."
REFERENCES,0.47754491017964074,"Putting everything together, and using the fact that T ν is monotonic and contractive,
Qπ
i (s, a) = lim
k→∞(T π)k ˜Qmax(s, a)"
REFERENCES,0.47904191616766467,"≥lim
k→∞"
REFERENCES,0.4805389221556886,"""
˜Qmax(s, a) −

ϵ(1 + γ) −γ(1 −λ)r(s, a) 1 −λγ 
k
X"
REFERENCES,0.4820359281437126,"j=0
γj
#"
REFERENCES,0.48353293413173654,"≥˜Qmax(s, a) −
1
1 −γ"
REFERENCES,0.48502994011976047,"
ϵ(1 + γ) −γ(1 −λ)r(s, a) 1 −λγ "
REFERENCES,0.4865269461077844,"≥Qmax(s, a) −ϵ −
1
1 −γ"
REFERENCES,0.4880239520958084,"
ϵ(1 + γ) −γ(1 −λ)r(s, a) 1 −λγ "
REFERENCES,0.48952095808383234,"≥Qπ∗
j (s, a) −
1
1 −γ"
REFERENCES,0.49101796407185627,"
2ϵ + γ(1 −λ)r(s, a) 1 −λγ 
."
REFERENCES,0.49251497005988026,"This holds for every j, hence the result."
REFERENCES,0.4940119760479042,"Lemma B.6. Let ν be any policy, λ, ˆλ ∈[0, 1], and Qλ denote a value function with
respecting to diminishing rate λ. Then,"
REFERENCES,0.49550898203592814,"∥Qν
λ −Qν
ˆλ∥∞≤|λ −ˆλ|∥r∥∞"
REFERENCES,0.49700598802395207,"1 −γ
."
REFERENCES,0.49850299401197606,"Proof. The proof follows from the definition of Q: for every (s, a) ∈S × A,"
REFERENCES,0.5,"|Qν
λ(s, a) −Qν
ˆλ(s, a)| = Eπ "" ∞
X"
REFERENCES,0.5014970059880239,"k=0
γk 
λnt(st+k,k) −ˆλnt(st+k,k)
r(st+k)
st = s, at = a # ≤Eπ "" ∞
X"
REFERENCES,0.5029940119760479,"k=0
γk λnt(st+k,k) −ˆλnt(st+k,k) r(st+k)
st = s, at = a # = Eπ   ∞
X"
REFERENCES,0.5044910179640718,"k=0
γkr(st+k)
λ −ˆλ"
REFERENCES,0.5059880239520959,"nt(st+k,k)−1
X"
REFERENCES,0.5074850299401198,"j=0
λnt(st+k,k)−1−jˆλjst = s, at = a  "
REFERENCES,0.5089820359281437,"≤|λ −ˆλ|Eπ "" ∞
X"
REFERENCES,0.5104790419161677,"k=0
γkr(st+k)
st = s, at = a #"
REFERENCES,0.5119760479041916,≤|λ −ˆλ|∥r∥∞
REFERENCES,0.5134730538922155,"1 −γ
."
REFERENCES,0.5149700598802395,"Theorem 5.1 (GPI). Let {Mj}n
j=1 ⊆M and M ∈M be a set of tasks in an
environment M and let Qπ∗
j denote the action-value function of an optimal policy of
Mj when executed in M. Assume that the agent uses diminishing rate ˆλ that may
differ from the true environment diminishing rate λ. Given estimates ˜Qπj such that
∥Qπ∗
j −˜Qπj∥∞≤ϵ for all j, define
π(s) ∈argmax
a
max
j
˜Qπj(s, a). Then,"
REFERENCES,0.5164670658682635,"Qπ(s, a) ≥max
j
Qπ∗
j (s, a) −
1
1 −γ"
REFERENCES,0.5179640718562875,"
2ϵ + |λ −ˆλ|∥r∥∞+ γ(1 −λ)r(s, a) 1 −λγ 
."
REFERENCES,0.5194610778443114,"Proof. Let Qλ denote a value function with respect to diminishing constant λ. We
wish to bound"
REFERENCES,0.5209580838323353,"max
j
Q
π∗
j
ˆλ (s, a) −Qπ
λ(s, a),"
REFERENCES,0.5224550898203593,"i.e., the value of the GPI policy with respect to the true λ compared to the maximum
value of the constituent policies π∗
j used for GPI, which were used assuming ˆλ. By
the triangle inequality,"
REFERENCES,0.5239520958083832,"max
j
Q
π∗
j
ˆλ (s, a) −Qπ
λ(s, a) ≤max
j
Q
π∗
j
λ (s, a) −Qπ
λ(s, a) + | max
j
Q
π∗
j
λ (s, a) −max
j
Q
π∗
j
ˆλ (s, a)|"
REFERENCES,0.5254491017964071,"≤max
j
Q
π∗
j
λ (s, a) −Qπ
λ(s, a)
|
{z
}
(1)"
REFERENCES,0.5269461077844312,"+ max
j
|Q
π∗
j
λ (s, a) −Q
π∗
j
ˆλ (s, a)|
|
{z
}
(2) ."
REFERENCES,0.5284431137724551,We bound (1) by Lemma B.5 and (2) by Lemma B.6 to get the result.
REFERENCES,0.5299401197604791,"B.2
An Extension of Theorem 5.1"
REFERENCES,0.531437125748503,"Inspired by [18], we prove an extension of Theorem 5.1:"
REFERENCES,0.5329341317365269,"Theorem B.1. Let M ∈M be a task in an environment M with true diminishing
constant λ. Suppose we perform GPI assuming a diminishing constant ˆλ:"
REFERENCES,0.5344311377245509,"Let {Mj}n
j=1 and Mi be tasks in M and let Q
π∗
j
i
denote the action-
value function of an optimal policy of Mj when executed in Mi.
Given estimates ˜Q
πj
i
such that ∥Q
π∗
j
i
−˜Q
πj
i ∥∞≤ϵ for all j, define
π(s) ∈argmaxa maxj ˜Q
πj
i (s, a)."
REFERENCES,0.5359281437125748,"Let Qπ
ˆλ and Qπ∗
λ denote the action-value functions of π and the M-optimal policy π∗"
REFERENCES,0.5374251497005988,"when executed in M, respectively. Then,"
REFERENCES,0.5389221556886228,"∥Qπ∗
λ −Qπ
ˆλ∥∞≤
2
1 −γ 1"
REFERENCES,0.5404191616766467,"2|λ −ˆλ|∥r∥∞+ ϵ + ∥r −ri∥∞+ min
j
∥ri −rj∥∞"
REFERENCES,0.5419161676646707,"
+ 1 −λ"
REFERENCES,0.5434131736526946,"1 −λγ C,"
REFERENCES,0.5449101796407185,where C is a positive constant not depending on λ:
REFERENCES,0.5464071856287425,"C = γ 2∥r −ri∥∞+ 2 minj ∥ri −rj∥∞+ min (∥r∥∞, ∥ri∥∞) + min (∥ri∥∞, ∥r1∥∞, . . . , ∥rn∥∞)"
REFERENCES,0.5479041916167665,"1 −γ
."
REFERENCES,0.5494011976047904,"Note that when λ = 1, we recover Proposition 1 of [18] with an additional term
quantifying error incurred by ˆλ ̸= λ. The proof relies on two other technical lemmas,
presented below."
REFERENCES,0.5508982035928144,Lemma B.7.
REFERENCES,0.5523952095808383,"∥Qπ∗−Q
π∗
i
i ∥∞≤∥r −ri∥∞"
REFERENCES,0.5538922155688623,"1 −γ
+ γ(1 −λ)min (∥r∥∞, ∥ri∥∞) + ∥r −ri∥∞"
REFERENCES,0.5553892215568862,"(1 −γ)(1 −λγ)
."
REFERENCES,0.5568862275449101,"Proof. Define ∆i := ∥Qπ∗−Q
π∗
i
i ∥∞. For any (s, a) ∈S × A,"
REFERENCES,0.5583832335329342,"|Qπ∗(s, a) −Q
π∗
i
i (s, a)| =
r(s, a) + γ
X"
REFERENCES,0.5598802395209581,"s′
p(s′|s, a)
 
(λ −1)r(s, a)Φπ∗(s′, π∗(s′), s) + Qπ∗(s′, π∗(s′)
"
REFERENCES,0.561377245508982,"−ri(s, a) −γ
X"
REFERENCES,0.562874251497006,"s′
p(s′|s, a)

(λ −1)ri(s, a)Φπ∗
i (s′, π∗
i (s′), s) + Q
π∗
i
i (s′, π∗
i (s′)
"
REFERENCES,0.5643712574850299,"≤|r(s, a) −ri(s, a)| + γ
X"
REFERENCES,0.5658682634730539,"s′
p(s′|s, a)|Qπ∗(s, a) −Q
π∗
i
i (s, a)|"
REFERENCES,0.5673652694610778,"+ γ(λ −1)
X"
REFERENCES,0.5688622754491018,"s′
p(s′|s, a)
r(s, a)Φπ∗(s′, π∗(s′), s) −ri(s, a)Φπ∗
i (s′, π∗
i (s′), s)"
REFERENCES,0.5703592814371258,"≤∥r −ri∥∞+ γ∆i + γ(1 −λ)∥rΦπ∗−riΦπ∗
i ∥∞."
REFERENCES,0.5718562874251497,The third term decomposes as
REFERENCES,0.5733532934131736,"∥rΦπ∗−riΦπ∗
i ∥∞≤∥rΦπ∗−rΦπ∗
i ∥∞+ ∥rΦπ∗
i −riΦπ∗
i ∥∞"
REFERENCES,0.5748502994011976,≤∥r∥∞+ ∥r −ri∥∞
REFERENCES,0.5763473053892215,"1 −λγ
."
REFERENCES,0.5778443113772455,We could equivalently use the following decomposition:
REFERENCES,0.5793413173652695,"∥rΦπ∗−riΦπ∗
i ∥∞≤∥rΦπ∗−riΦπ∗∥∞+ ∥riΦπ∗−riΦπ∗
i ∥∞"
REFERENCES,0.5808383233532934,≤∥ri∥∞+ ∥r −ri∥∞
REFERENCES,0.5823353293413174,"1 −λγ
,"
REFERENCES,0.5838323353293413,and so
REFERENCES,0.5853293413173652,"∥rΦπ∗−riΦπ∗
i ∥∞≤min (∥r∥∞, ∥ri∥∞) + ∥r −ri∥∞"
REFERENCES,0.5868263473053892,"1 −λγ
."
REFERENCES,0.5883233532934131,"The inequalities above hold for all s, a and so"
REFERENCES,0.5898203592814372,"∆i ≤∥r −ri∥∞+ γ∆i + γ(1 −λ)min (∥r∥∞, ∥ri∥∞) + ∥r −ri∥∞ 1 −λγ"
REFERENCES,0.5913173652694611,=⇒∆i ≤∥r −ri∥∞
REFERENCES,0.592814371257485,"1 −γ
+ γ(1 −λ)min (∥r∥∞, ∥ri∥∞) + ∥r −ri∥∞"
REFERENCES,0.594311377245509,"(1 −γ)(1 −λγ)
."
REFERENCES,0.5958083832335329,Hence the result.
REFERENCES,0.5973053892215568,"Lemma B.8. For any policy π,"
REFERENCES,0.5988023952095808,"∥Qπ
i −Qπ∥∞≤∥r −ri∥∞"
REFERENCES,0.6002994011976048,"1 −γ
+ γ(1 −λ)
∥r −ri∥∞
(1 −γ)(1 −λγ)."
REFERENCES,0.6017964071856288,"Proof. Write ∆i := ∥Qπ
i −Qπ∥∞. Proceeding as in the previous lemma, for all
(s, a) ∈S × A, we have"
REFERENCES,0.6032934131736527,"|Qπ
i (s, a) −Qπ(s, a)| =
ri(s, a) + γ
X"
REFERENCES,0.6047904191616766,"s′
p(s′|s, a) ((λ −1)ri(s, a)Φπ(s′, π(s′), s) + Qπ
i (s′, π(s′)))"
REFERENCES,0.6062874251497006,"−r(s, a) −γ
X"
REFERENCES,0.6077844311377245,"s′
p(s′|s, a) ((λ −1)r(s, a)Φπ(s′, π(s′), s) + Qπ(s′, π(s′)))"
REFERENCES,0.6092814371257484,"≤|r(s, a) −ri(s, a)| + γ
X"
REFERENCES,0.6107784431137725,"s′
p(s′|s, a)(1 −λ)|r(s, a) −ri(s, a)|Φπ(s′, π(s′), s) + γ
X"
REFERENCES,0.6122754491017964,"s′
p(s′|s, a)|Qπ
i (s′, π(s′)) −Qπ(s′, π(s′))|"
REFERENCES,0.6137724550898204,"≤∥r −ri∥∞+ γ(1 −λ)∥r −ri∥∞
1
1 −λγ + γ∆′
i"
REFERENCES,0.6152694610778443,"=⇒∆′
i ≤∥r −ri∥∞+ γ(1 −λ)∥r −ri∥∞"
REFERENCES,0.6167664670658682,"1 −λγ
+ γ∆′
i"
REFERENCES,0.6182634730538922,"=⇒∆′
i ≤∥r −ri∥∞"
REFERENCES,0.6197604790419161,"1 −γ
+ γ(1 −λ)∥r −ri∥∞"
REFERENCES,0.6212574850299402,(1 −γ)(1 −λγ) .
REFERENCES,0.6227544910179641,"Finally, we prove Theorem B.1:"
REFERENCES,0.624251497005988,"Proof of Theorem B.1. By the triangle inequality,
∥Qπ∗
λ −Qπ
ˆλ∥∞≤∥Qπ∗
λ −Qπ
λ∥∞+ ∥Qπ
λ −Qπ
ˆλ∥∞.
By Lemma B.6, the second term is bounded above by"
REFERENCES,0.625748502994012,|λ −ˆλ|∥r∥∞
REFERENCES,0.6272455089820359,"1 −γ
."
REFERENCES,0.6287425149700598,"The first term decomposes as follows (dropping the λ subscript on all action-value
functions for clarity):"
REFERENCES,0.6302395209580839,"∥Qπ∗−Qπ∥∞≤∥Qπ∗−Q
π∗
i
i ∥∞
|
{z
}
(1)"
REFERENCES,0.6317365269461078,"+ ∥Q
π∗
i
i
−Qπ
i ∥∞
|
{z
}
(2)"
REFERENCES,0.6332335329341318,"+ ∥Qπ
i −Qπ∥∞
|
{z
}
(3) ."
REFERENCES,0.6347305389221557,"Applying Lemma B.5 to (2) (but with respect to Mi rather than M), we have that for
any j,"
REFERENCES,0.6362275449101796,"Q
π∗
i
i (s, a) −Qπ
i (s, a) ≤Q
π∗
i
i (s, a) −Q
π∗
j
i (s, a) +
1
1 −γ"
REFERENCES,0.6377245508982036,"
2ϵ + γ(1 −λ)ri(s, a) 1 −λγ "
REFERENCES,0.6392215568862275,"=⇒∥Q
π∗
i
i
−Qπ
i ∥∞≤∥Q
π∗
i
i
−Q
π∗
j
j ∥∞
|
{z
}
(2.1)"
REFERENCES,0.6407185628742516,"+ ∥Q
π∗
j
j −Q
π∗
j
i ∥∞
|
{z
}
(2.2)"
REFERENCES,0.6422155688622755,"+
1
1 −γ"
REFERENCES,0.6437125748502994,"
2ϵ + γ(1 −λ)∥ri∥∞ 1 −λγ 
."
REFERENCES,0.6452095808383234,"We bound (2.1) using Lemma B.7 and (2.2) using Lemma B.8 (but with respect to
Mj rather than M):"
REFERENCES,0.6467065868263473,"∥Q
π∗
i
i
−Q
π∗
j
j ∥∞+ ∥Q
π∗
j
j −Q
π∗
j
i ∥∞≤2∥ri −rj∥∞"
REFERENCES,0.6482035928143712,"1 −γ
+ γ(1 −λ)min (∥ri∥∞, ∥rj∥∞) + 2∥ri −rj∥∞"
REFERENCES,0.6497005988023952,"(1 −γ)(1 −λγ)
."
REFERENCES,0.6511976047904192,We then apply Lemma B.7 to (1) and Lemma B.8 to (3) to get the result.
REFERENCES,0.6526946107784432,"C
An nth Occupancy Representation"
REFERENCES,0.6541916167664671,"To generalize the first occupancy representation to account for reward functions of
this type, it’s natural to consider an Nth occupancy representation—that is, one
which accumulates value only for the first N occupancies of one state s′ starting
from another state s:
Definition C.1 (NR). For an MDP with finite S, the Nth-occupancy representation
(NR) for a policy π is given by F π ∈[0, N]|S|×|S| such that"
REFERENCES,0.655688622754491,"Φπ
(N)(s, s′) ≜Eπ "" ∞
X"
REFERENCES,0.657185628742515,"k=0
γt+k1 (st+k = s′, # ({j | st+j = s′, j ∈[0, k −1]}) < N)
st # . (C.1)"
REFERENCES,0.6586826347305389,"Intuitively, such a representation sums the first N (discounted) occupancies of s′"
REFERENCES,0.6601796407185628,"from time t to t + k starting from st = s. We can also note that Φπ
(1) is simply the
FR and Φ(0)(s, s′) = 0 ∀s, s′. As with the FR and the SR, we can derive a recursive
relationship for the NR:"
REFERENCES,0.6616766467065869,"Φπ
(N)(s, s′) = 1(st = s′)(1 + γEΦπ
(N−1)(st+1, s′)) + γ(1 −1(st = s′))EΦπ
(N)(st+1, s′),
(C.2)"
REFERENCES,0.6631736526946108,"where the expectation is wrt pπ(st+1|st). Once again, we can confirm that this is
consistent with the FR by noting that for N = 1, the NR recursion recovers the FR
recursion. Crucially, we also recover the SR recursion in the limit as N →∞:"
REFERENCES,0.6646706586826348,"lim
N7→∞Φπ
(N)(s, s′) = 1(st = s′)(1 + γEΦπ
(∞)(st+1, s′)) + γ(1 −1(st = s′))EΦπ
(∞)(st+1, s′)"
REFERENCES,0.6661676646706587,"= 1(st = s′) + γEΦπ
(∞)(st+1, s′)."
REFERENCES,0.6676646706586826,"This is consistent with the intuition that the SR accumulates every (discounted) state
occupancy in a potentially infinite time horizon of experience. While Definition C.1
admits a recursive form which is consistent with our intuition, Eq. (C.2) reveals
an inconvenient intractability: the Bellman target for Φπ
(N) requires the availability
of Φπ
(N−1). This is a challenge, because it means that if we’d like to learn any NR
for finite N > 1, the agent also must learn and store Φπ
(1), . . . Φπ
(N−1). Given these
challenges, the question of how to learn a tractable general occupancy representation
remains. From a neuroscientific perspective, a fixed depletion amount is also incon-
sistent with both behavioral observations and neural imaging [3], which indicate
instead that utility disappears at a fixed rate in proportion to the current remaining
utility, rather than in proportion to the original utility. We address these theoretical
and practical issues in the next section."
REFERENCES,0.6691616766467066,"D
Additional Related Work"
REFERENCES,0.6706586826347305,"Another important and relevant sub-field of reinforcement learning is work which
studies non-stationary rewards. Perhaps most relevant, the setting of DMU can
be seen as a special case of submodular planning and reward structures [31, 32].
[31] focus specifically on planning and not the form of diminishment we study,
while [32] is a concurrent work which focuses on the general class of submodular"
REFERENCES,0.6721556886227545,"reward problems and introduces a policy-based, REINFORCE-like method which is
necessarily on-policy. In contrast, we focus on a particular sub-class of problems
especially relevant to natural behavior and introduce a family of approaches which
exploit this reward structure, are value-based (and which can be used to modify
the critic in policy-based methods), and are compatible with off-policy learning.
Other important areas include convex [33] and constrained [34, 35] MDPs. In these
cases, non-stationarity is introduced by way of a primal-dual formulation of distinct
problem classes into min-max games."
REFERENCES,0.6736526946107785,"E
Further Experimental Details"
REFERENCES,0.6751497005988024,"E.1
Policy Evaluation"
REFERENCES,0.6766467065868264,"We perform policy evaluation for the policy shown in Fig. 4.1 on the 6 × 6 gridworld
shown. The discount factor γ was set to 0.9 for all experiments, which were run for
H = 10 steps per episode. The error metric was the mean squared error:"
REFERENCES,0.6781437125748503,"Qerror ≜
1
|S||A| X"
REFERENCES,0.6796407185628742,"s,a
(Qπ(s, a) −ˆQ(s, a))2,
(E.1)"
REFERENCES,0.6811377245508982,"where Qπ is the ground truth Q-values and ˆQ is the estimate. Transitions are
deterministic. For the dynamic programming result, we learned the λR using
Eq. (4.3) for λ ∈{0.5, 1.0} and then measured the resulting values by multiplying
the resulting λR by the associated reward vector r ∈{−1, 0, 1}36, which was −1 in
all wall states and +1 at the reward state g. We compared the results to the ground
truth values. Dynamic programming was run until the maximum Bellman error
across state-action pairs reduced below 5e-2. For the tabular TD learning result, we
ran the policy for three episodes starting from every available (non-wall) state in the
environment, and learned the λR for λ ∈{0.5, 1.0} as above, but using the online
TD update:
Φλ(st, at) ←Φλ(st, at) + αδt,
δt = 1(st) ⊙(1 + γλΦλ(st+1, at+1)) + γ(1 −1(st)) ⊙Φλ(st+1, at+1) −Φλ(st, at),
where at+1 ∼π(· | st+1). The learned Q-values were then computed in the same
way as the dynamic programming case and compared to the ground truth. For the
λF result, we first learned Laplacian eigenfunction base features as described in
[25] from a uniform exploration policy and normalized them to the range [0, 1]. We
parameterized the base feature network as a 2-layer MLP with ReLU activations
and 16 units in the hidden layer. We then used the base features to learn the λFs
as in the tabular case, but with the λF network parameterized as a three-layer MLP
with 16 units in each of the hidden layers and ReLU activations. All networks were
optimized using Adam with a learning rate of 3e-4. The tabular and neural network
experiments were repeated for three random seeds, the former was run for 1,500
episodes and the latter for 2,000."
REFERENCES,0.6826347305389222,"E.2
Policy Learning"
REFERENCES,0.6841317365269461,"We ran the experiments for Fig. 5.2 in a version of the TwoRooms environment
from the NeuroNav benchmark [22] with reward modified to decay with a specified"
REFERENCES,0.6856287425149701,Algorithm 1: Online Tabular Qλ-Learning Update
REFERENCES,0.687125748502994,"1: Require: Current λR-values Φ(t)
λ
∈R|S|×|A|×|S|, current reward vector r(t), observed
(st, at, st+1) tuple"
REFERENCES,0.688622754491018,"2: Compute Qλ-values: Q(t)
λ ←(Φ(t)
λ )Tr(t)"
REFERENCES,0.6901197604790419,"3: Select greedy action: at+1 ←argmaxa∈A Q(t)
λ (st+1, a)
4: Update Φλ:"
REFERENCES,0.6916167664670658,"Φ(t+1)
λ
(st, at) ←Φ(t)
λ (st, at) + αδ(t),
where"
REFERENCES,0.6931137724550899,"δ(t) = 1(st) ⊙(1 + γλΦ(t)
λ (st+1, at+1)) + γ(1 −1(st)) ⊙Φ(t)
λ (st+1, at+1) −Φ(t)
λ (st, at)."
REFERENCES,0.6946107784431138,"5: Return updated Φ(t+1)
λ"
REFERENCES,0.6961077844311377,"λtrue = 0.5 and discount factor γ = 0.95. The initial rewards in the top right
goal and the lower room goal locations were 5 and the top left goal had initial
reward 10. The observations in the neural network experiment were one-hot state
indicators. The tabular Qλ experiments run the algorithm in Algorithm 1 for 500
episodes for λ ∈{0.0, 0.5, 1.0}, with λtrue set to 0.5, repeated for three random
seeds. Experiments used a constant step size α = 0.1. There were five possible
actions: up, right, down, left, and stay. The recurrent A2C agents were based on
the implementation from the BSuite library [36] and were run for 7,500 episodes of
maximum length H = 100 with γ = 0.99 using the Adam optimizer with learning
rate 3e-4. The experiment was repeated for three random seeds. The RNN was an
LSTM with 128 hidden units and three output heads: one for the policy, one for the
value function, and one for the λF. The base features were one-hot representations
of the current state, 121-dimensional in this case."
REFERENCES,0.6976047904191617,"E.3
Tabular GPI"
REFERENCES,0.6991017964071856,"The agent is assumed to be given or have previously acquired four policies
{π0, π1, π2, π3} individually optimized to reach rewards located in each of the four
rooms of the environment. There are three reward locations {g0, g1, g2} scattered
across the rooms, each with its own initial reward ¯r = [5, 10, 5] and all with λ = 0.5.
At the beginning of each episode, an initial state s0 is sampled uniformly from the
set of available states. An episode terminates either when the maximum reward
remaining in any of the goal states is less than 0.1 or when the maximum number
of steps H = 40 is reached. Empty states carry a reward of 0, encountering a wall
gives a reward of −1, and the discount factor is set to γ = 0.97."
REFERENCES,0.7005988023952096,"For each of the four policies, we learn λRs with λ equal to 0, 0.5, and 1.0 using
standard dynamic programming (Bellman error curves plotted in Fig. F.3), and record
the returns obtained while performing GPE+GPI with each of these representations
over the course of 50 episodes. Bellman error curves for the λRs are In the left panel
of Fig. 5.3, we can indeed see that using the correct λ (0.5) nets the highest returns.
Example trajectories for each of λR are shown in the remaining panels."
REFERENCES,0.7020958083832335,"E.4
Pixel-Based GPI"
REFERENCES,0.7035928143712575,"In this case, the base policies Π were identical to those used in the tabular GPI
experiments. First, we collected a dataset consisting of 340 observation trajectories
(o0, o1, . . . , oH−1) ∈OH with H = 19 from each policy, totalling 6, 460 observa-
tions. Raw observations were 128 × 128 × 3 and were converted to grayscale. The
previous seven observations were stacked and used to train a Laplacian eigenfunction
base feature network in the same way as [25]. For observations less than seven
steps from the start of an episode, the remaining frames were filled in as all black
observations (i.e., zeros). The network consisted of four convolutional layers with
32 3 × 3 filters with strides (2, 2, 2, 1), each followed by a ReLU nonlinearity. This
was then flattened and passed through a Layer Norm layer [37] and a tanh non-
linearity before three fully fully connected layers, the first two with 64 units each
and ReLU nonlinearities and the final, output layer with 50 units. The output was
L2-normalized as in [25]. This network ϕ : O7 7→RD (with D = 50) was trained on
the stacked observations for 10 epochs using the Adam optimizer and learning rate
1e-4 with batch size B = 64. To perform policy evaluation, the resulting features,
evaluated on the dataset of stacked observations were collected into their own dataset
of (st, at+1, st+1, at+1) tuples, where st ≜ot−6:t. The “states” were normalized to
be between 0 and 1, and a vector w was fit to the actual associated rewards via
linear regression on the complete dataset. The λF network was then trained using a
form of neural fitted Q-iteration [FQI; 38] modified for policy evaluation with λFs
(Algorithm 2). The architecture for the λF network was identical to the base feature
network, with the exception that the hidden size of the fully connected layers was
128 and the output dimension was D|A| = 250. FQI was run for K = 20 outer loop
iterations, with each inner loop supervised learning setting run for L = 100 epochs
on the current dataset. Supervised learning was done using Adam with learning rate
3e-4 and batch size B = 64. Given the trained networks, GPI proceeded as in the
tabular case, i.e.,"
REFERENCES,0.7050898203592815,"at = argmax
a∈A
max
π∈Π wTφπ
θ(st, a).
(E.2)"
REFERENCES,0.7065868263473054,"50 episodes were run from random starting locations for H = 50 steps and the
returns measured. Learning curves for the base features and for λF fitting are shown
in Fig. E.1. The λF curve measures the mean squared error as in Eq. (E.1)."
REFERENCES,0.7080838323353293,"The feature visualizations were created by performing PCA to reduce the average
λF representations for observations at each state in the environment to 2D. Each
point in the scatter plot represents the reduced representation on the xy plane, and is
colored according to the λ-conditioned value of the underlying state."
REFERENCES,0.7095808383233533,"E.5
Continuous Control"
REFERENCES,0.7110778443113772,"λ-SAC
See Appendix I for details."
REFERENCES,0.7125748502994012,"E.6
Learning the λO with FB"
REFERENCES,0.7140718562874252,"Training the λO with the FB parameterization proceeds in much the same way as
in [12], but adjusted for a different norm and non-Markovian environment. We"
REFERENCES,0.7155688622754491,"0
2
4
6
8
Epochs 0 500 1000 1500 2000 2500 3000 3500 Loss"
REFERENCES,0.7170658682634731,Base Feature Learning
REFERENCES,0.718562874251497,"0
5
10
15
FQI Iterations 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.7200598802395209,Q  MSE
REFERENCES,0.7215568862275449,F Learning
REFERENCES,0.7230538922155688,"Figure E.1: Learning curves for λF policy evaluation. Results are averaged over three runs, with
shading indicating one unit of standard error."
REFERENCES,0.7245508982035929,Algorithm 2: Fitted Qλ-Iteration
REFERENCES,0.7260479041916168,"1: Require: Dataset of base features {ϕ(s) ∈RD}s∈S, decay rate λ, discount factor γ, reward
feature vector w ∈RD, batch size B, learning rate α
2: Initialize λF φθ parameters θ(1) (we drop the subscript λ and superscript π for concision)
3: for k = 1 . . . , K do
4:
// Stage 1: Construct dataset
5:
D ←∅
6:
for (s, a) ∈S × A do
7:
for (s′, a′) ∈S × A do"
REFERENCES,0.7275449101796407,"8:
D ←D∪ 

 
 "
REFERENCES,0.7290419161676647,"
(s, a), wT [ϕ(s) ⊙(1 + λγ ¯φθ(k)(s′, a′)) + γ(1 −ϕ(s)) ⊙¯φθ(k)(s′, a′)]
|
{z
}"
REFERENCES,0.7305389221556886,"≜y(s,a)  
 

 
"
REFERENCES,0.7320359281437125,"9:
end for
10:
end for
11:
// Stage 2: Supervised learning
12:
Randomly initialize θ0
13:
for ℓ= 1, . . . , L do
14:
Randomly shuffle D
15:
for {((s, a), y)}B
b=1 ∈D do"
REFERENCES,0.7335329341317365,"16:
θℓ←θℓ−1 −α∇θ 1"
"B
PB",0.7350299401197605,"2B
PB
b=1
 
yb −wTφθℓ−1(sb, ab)
2"
"B
PB",0.7365269461077845,"17:
end for
18:
end for
19:
θ(k+1) ←θL
20: end for"
"B
PB",0.7380239520958084,"summarize the learning procedure in Algorithm 3. The loss function L is derived in
Appendix H, with the addition of the following regularizer:"
"B
PB",0.7395209580838323,∥Es∼ρBω(s)Bω(s)⊤−I∥2.
"B
PB",0.7410179640718563,"This regularizer encourages B to be approximately orthonormal, which promotes
identifiability of Fθ and Bω [12]."
"B
PB",0.7425149700598802,"F
Additional Results"
"B
PB",0.7440119760479041,See surrounding sections.
"B
PB",0.7455089820359282,Algorithm 3: λO FB Learning
"B
PB",0.7470059880239521,"1: Require: Probability distribution ν over Rd, randomly initialized networks Fθ, Bω, learning rate
η, mini-batch size B, number of episodes E, number of epochs M, number of time steps per
episode T, number of gradient steps N, regularization coefficient β, Polyak coefficient α, initial
diminishing constant λ, discount factor γ, exploratory policy greediness ϵ, temperature τ
2: // Stage 1: Unsupervised learning phase
3: D ←∅
4: for epoch m = 1, . . . , M do
5:
for episode i = 1 . . . , E do
6:
Sample z ∼ν
7:
Observe initial state s1
8:
for t = 1, . . . , T do
9:
Select at ϵ−greedy with respect to Fθ(st, a, z)⊤z
10:
Observe reward rt(st) and next state st+1
11:
D ←D ∪{(st, at, rt(st), st+1)}
12:
end for
13:
end for
14:
for n = 1, . . . , N do
15:
Sample a minibatch {(sj, aj, rj(sj), sj+1)}j∈J ⊂D of size |J| = B
16:
Sample a minibatch {˜sj}j∈J ⊂D of size |J| = B"
"B
PB",0.7485029940119761,"17:
Sample a minibatch {s′
j}j∈J
iid∼µ of size |J| = B"
"B
PB",0.75,"18:
Sample a minibatch {zj}j∈J
iid∼ν of size |J| = B
19:
For every j ∈J, set πzj(·|sj+1) = softmax
 
Fθ−(sj+1, ·, zj)⊤zj/τ
 20:"
"B
PB",0.7514970059880239,"L(θ, ω) ←
1
2B2
X"
"B
PB",0.7529940119760479,"j,k∈J2 "
"B
PB",0.7544910179640718,"Fθ(sj, aj, zj)⊤Bω(s′
k) −γ
X"
"B
PB",0.7559880239520959,"a∈A
πzj(a|sj+1)Fθ−(sj+1, a, zj)⊤Bω−(s′
k) !2 −1 B X"
"B
PB",0.7574850299401198,"j∈J
Fθ(sj, aj, zj)⊤Bω(sj)"
"B
PB",0.7589820359281437,+ γ(1 −λ) B X
"B
PB",0.7604790419161677,"j∈J
µ(sj)Fθ(sj, aj, zj)⊤Bω(sj)
X"
"B
PB",0.7619760479041916,"a∈A
πzj(a|sj+1)Fθ−(sj+1, a, zj)⊤Bω−(sj) + β  1 B2
X"
"B
PB",0.7634730538922155,"j,k∈J2
Bω(sj)⊤¯Bω(˜sk) ¯Bω(sj)⊤¯Bω(˜sk) −1 B X"
"B
PB",0.7649700598802395,"j∈J
Bw(sj)⊤¯Bω(sj)  "
"B
PB",0.7664670658682635,"21:
Update θ and ω via one step of Adam on L
22:
Sample a minibatch {(sj, rj(sj), sj+1, rj+1(sj+1))}j∈J of size |J| = B from D
23:
Lλ(λ) ←
1
2B
P
j∈J 1(sj+1 = sj) (rj+1(sj+1) −λrj(sj))2"
"B
PB",0.7679640718562875,"24:
Update λ via one step of Adam on Lλ
25:
end for
26:
θ−←αθ−+ (1 −α)θ
27:
ω−←αω−+ (1 −α)ω
28: end for
29: // Stage 2: Exploitation phase for a single episode with initial reward r0(s)
30: zR ←P"
"B
PB",0.7694610778443114,"s∈S µ(s)r0(s)Bω(s)
31: Observe initial state s1
32: for t = 1, . . . , T do
33:
at ←argmaxa∈A F(st, a, zR)⊤zR
34:
Observe reward rt(s) and next state st+1
35:
zR ←P"
"B
PB",0.7709580838323353,"s∈S µ(s)rt(s)Bω(s)
36: end for s0 sg 1
2
3"
"B
PB",0.7724550898203593,Figure F.1: A simple grid and several policies.
"B
PB",0.7739520958083832,"0
20
40
60
Iteration 0.0 0.5 1.0 1.5 2.0"
"B
PB",0.7754491017964071,Bellman Error
"B
PB",0.7769461077844312,Noise = 0.0
"B
PB",0.7784431137724551,"0.0
0.25
0.5
0.75
1.0"
"B
PB",0.7799401197604791,"0
20
40
60
Iteration"
"B
PB",0.781437125748503,Noise = 0.2
"B
PB",0.7829341317365269,"Figure F.3: Convergence of dynamic programming on FourRooms with and without stochastic
transitions."
"B
PB",0.7844311377245509,"0.0
0.5
1.0
10.0 12.5 15.0 17.5 20.0 22.5"
"B
PB",0.7859281437125748,"Return = 0.5 s0
g0 g1 g2 = 0.0 s0
g0 g1 g2 = 1.0 s0
g0 g1 g2 0 5 10 15 20 25 30 35 40 t"
"B
PB",0.7874251497005988,"Figure F.4: GPI with noisy transitions in FourRooms. To verify that performance was maintained
even with stochastic transitions, we added a 20% probability that a given action would result in a
random transition to neighboring state. Results are consistent with Fig. 5.3, indicating the having the
correct value of λ produces better performance."
"B
PB",0.7889221556886228,"G
Advantage of the Correct λ"
"B
PB",0.7904191616766467,"Importantly, for GPE using the λR to work in this setting, the agent must either
learn or be provided with the updated reward vector rλ after each step/encounter
with a rewarded state. This is because the λR is forward-looking in that it measures
the (diminished) expected occupancies of states in the future without an explicit
mechanism for remembering previous visits. For simplicity in this case, we provide
this vector to the agent at each step—though if we view such a multitask agent as
simply as a module carrying out the directives of a higher-level module or policy
within a hierarchical framework as in, e.g., Feudal RL [39], the explicit provision of
reward information is not unrealistic. Regardless, a natural question in this case is
whether there is actually any value in using the λR with the corret value of λ in this s0 sg M
1 s0 sg M
2 s0 sg M
3 s0 sg 1
1.0 s0 sg 2
1.0 s0 sg 3
1.0 s0 sg 1
0.8 s0 sg 2
0.8 s0 sg 3
0.8 s0 sg 1
0.4 s0 sg 2
0.4 s0 sg 3
0.4 s0 sg 1
0.0 s0 sg 2
0.0 s0 sg 3
0.0 s0 sg F
1 s0 sg F
2 s0 sg F
3 0.0 0.2 0.4 0.6 0.8 1.0"
"B
PB",0.7919161676646707,"Figure F.2: Visualizing the SR, the λR and the FR. We can see that the Φπ
1 is equivalent to the SR
and Φπ
0 is equivalent to the FR, with intermediate values of λ providing a smooth transition between
the two."
"B
PB",0.7934131736526946,"setting: If the agent is provided with the correct reward vector, then wouldn’t policy
evaluation work with any λR?"
"B
PB",0.7949101796407185,"To see that this is not the case, consider the three-state toy MDP shown in Figure
Fig. G.1, where ¯r(s1) = 10, ¯r(s2) = 6, ¯r(s0) = 0, λ(s1) = 0, λ(s2) = 1.0, and
γ = 0.99. At time t = 0, the agent starts in s0. Performing policy evaluation with
λ(s1) = λ(s2) = 1 (i.e., with the SR) would lead the agent to go left to s1. However,"
"B
PB",0.7964071856287425,Figure G.1: A 3-state toy environment.
"B
PB",0.7979041916167665,"the reward would then disappear, and policy evaluation on the second step would
lead it to then move right to s0 and then s2, where it would stay for the remainder
of the episode. In contrast, performing PI with the correct values of λ would lead
the agent to go right to s2 and stay there. In the first two timesteps, the first policy
nets a total reward of 10 + 0 = 10, while the second policy nets 6 + 5.94 = 11.94.
(The remaining decisions are identical between the two policies.) This is a clear
example of the benefit of having the correct λ, as incorrect value estimation leads to
suboptimal decisions even when the correct reward vector/function is provided at
each step."
"B
PB",0.7994011976047904,"H
The λ Operator"
"B
PB",0.8008982035928144,"To learn the λO, we would like to define Φπ
λ(st, ds′) ≜φπ
λ(st, s′)µ(ds′) for some
base policy µ. However, this would lead to a contradiction:"
"B
PB",0.8023952095808383,"Φπ
λ(s, A ∪B) =
Z"
"B
PB",0.8038922155688623,"A
φπ
λ(s, ds′)µ(ds′) +
Z"
"B
PB",0.8053892215568862,"B
φπ
λ(s, ds′)µ(ds′) = Φπ
λ(s, A) + Φπ
λ(s, B)"
"B
PB",0.8068862275449101,"for all disjoint A, B, contradicting Lemma B.4."
"B
PB",0.8083832335329342,"For now, we describe how to learn the λO for discrete S, in which case we have
Φπ
λ(s, s′) = φπ
λ(s, s′)µ(s′), i.e., by learning φ we learn a weighted version of Φ. We
define the following norm, inspired by Touati et al. [25]:"
"B
PB",0.8098802395209581,"∥Φπ
λ∥2
ρ ≜E s∼ρ
s′∼µ"
"B
PB",0.811377245508982,"""Φπ
λ(s, s′)
µ(s′) 2# ,"
"B
PB",0.812874251497006,"where µ is any density on S. In the case of finite S, we let µ be the uniform density.
We then minimize the Bellman error for Φπ
λ with respect to ∥· ∥2
ρ,µ (dropping the
sub/superscripts on Φ and φ for clarity):"
"B
PB",0.8143712574850299,"L(Φ) = ∥φµ −(I ⊙(11T + λγP πφµ) + γ(11T + I) ⊙P πφµ)∥2
ρ,µ"
"B
PB",0.8158682634730539,"= Est∼ρ,s′∼µ
h
φ(st, s′) −1(st = s′) µ(s′)"
"B
PB",0.8173652694610778,+ γ(1 −λ)1(st = s′)
"B
PB",0.8188622754491018,"µ(s′)
Est+1∼pπ(·|st) ¯Φ(st+1, s′) −γEst+1∼pπ(·|st) ¯φ(st+1, s′)
2i"
"B
PB",0.8203592814371258,"+c
= Est,st+1∼ρ,s′∼µ
h
(φ(st, s′) −γ ¯φ(st+1, s′))2i"
"B
PB",0.8218562874251497,"−2Est,st+1∼ρ ""X"
"B
PB",0.8233532934131736,"s′
µ(s′)φ(st, s′)1(st = s′) µ(s′) #"
"B
PB",0.8248502994011976,"+ 2γ(1 −λ)Est,st+1∼ρ ""X"
"B
PB",0.8263473053892215,"s′
µ(s′)φ(st, s′) ¯φ(st+1, s′)µ(s′)1(st = s′) µ(s′) #"
"B
PB",0.8278443113772455,"+c
= Est,st+1∼ρ,s′∼µ
h
(φ(st, s′) −γ ¯φ(st+1, s′))2i
−2Est∼ρ[φ(st, st)]"
"B
PB",0.8293413173652695,"+ 2γ(1 −λ)Est,st+1∼ρ[µ(st)φ(st, st) ¯φ(st+1, st)],"
"B
PB",0.8308383233532934,"Note that we recover the SM loss when λ = 1. Also, an interesting interpretation is
that when the agent can never return to its previous state (i.e., φ(st+1, st) = 0), then
we also recover the SM loss, regardless of λ. In this way, the above loss appears to
“correct” for repeated state visits so that the measure only reflects the first visit."
"B
PB",0.8323353293413174,"L(Φ) = Est,at,st+1∼ρ,s′∼µ
h 
F(st, at, z)⊤B(s′) −γ ¯F(st+1, πz(st+1), z)⊤¯B(s′)
2i"
"B
PB",0.8338323353293413,"−2Est,at∼ρ

F(st, at, z)⊤B(st)
"
"B
PB",0.8353293413173652,"+ 2γ(1 −λ)Est,at,st+1∼ρ

µ(st)F(st, at, z)⊤B(st) ¯F(st+1, πz(st+1), z)⊤¯B(st)
 (H.1)"
"B
PB",0.8368263473053892,"Even though the λO is not a measure, we can use the above loss to the continuous
case, pretending as though we could take the Radon-Nikodym derivative Φ(s,ds′)"
"B
PB",0.8383233532934131,µ(ds′) .
"B
PB",0.8398203592814372,"H.1
Experimental Results with the FB Parameterization"
"B
PB",0.8413173652694611,"To show that knowing the correct value of λ leads to improved performance, we
trained λO with the FB parameterization on the FourRooms task of Fig. 5.3, but
with each episode initialized at a random start state and with two random goal
states. Average per-epoch reward is shown in Fig. H.2. We tested performance
with λtrue, λagent ∈{0.5, 1.0}, where λtrue denotes the true environment diminishing
rate and λagent denotes the diminishing rate that the agent uses. For the purpose of
illustration, we do not allow the agent to learn λ. We see in Fig. H.2 that using the
correct λ leads to significantly increased performance. In particular, the left plot
shows that assuming λ = 1, i.e., using the SR, in a diminishing environment can
lead to highly suboptimal performance."
"B
PB",0.842814371257485,Hyperparameters used are given in Table 1 (notation as in Algorithm 3).
"B
PB",0.844311377245509,"Hyperparameter
Value"
"B
PB",0.8458083832335329,"M
100
E
100
N
25
B
128
T
50
γ
0.99
α
0.95
η
0.001
τ
200
ϵ
1"
"B
PB",0.8473053892215568,Table 1: λO-FB hyperparameters.
"B
PB",0.8488023952095808,"0.5
1.0 agent 0 1 2 3"
"B
PB",0.8502994011976048,Average Expert Per-Epoch Reward
"B
PB",0.8517964071856288,true = 0.5
"B
PB",0.8532934131736527,"0.5
1.0 agent 0.0 12.5 25.0 37.5"
"B
PB",0.8547904191616766,Average Expert Per-Epoch Reward
"B
PB",0.8562874251497006,true = 1.0
"B
PB",0.8577844311377245,"Figure H.1: Performance of the λO-FB with two values of λ. Results averaged over six seeds and
10 episodes per seed. Error bars indicate standard error."
"B
PB",0.8592814371257484,"H.2
λO and the Marginal Value Theorem"
"B
PB",0.8607784431137725,"To study whether the agent’s behavior is similar to behavior predicted by the MVT,
we use a very simple task with constant starting state and vary the distance between
rewards (see Fig. H.1(a)). When an agent is in a reward state, we define an
MVT-optimal leaving time as follows (similar to that of [8] but accounting for the
non-stationarity of the reward)."
"B
PB",0.8622754491017964,"Let R denote the average per-episode reward received by a trained agent, r(st)
denote the reward received at time t in a given episode, Rt = Pt
u=0 r(su) denote the
total reward received until time t in the episode, and let T be episode length. Then,
on average, the agent should leave its current reward state at time t if the next reward
that it would receive by staying in st, i.e., λr(st), is less than R −Rt T
."
"B
PB",0.8637724550898204,"In other words, the agent should leave a reward state when its incoming reward falls
below the diminished average per-step reward of the environment. We compute R
by averaging reward received by a trained agent over many episodes."
"B
PB",0.8652694610778443,"Previous studies have trained agents that assume stationary reward to perform
foraging tasks, even though the reward in these tasks is non-stationary. These agents
can still achieve good performance and MVT-like behavior [8]. However, because"
"B
PB",0.8667664670658682,"Figure H.2: Analysis of MVT-like behavior of λO-FB. a) Three environments with equal start state
and structure but different distances between reward states. b) Difference between the agent’s leave
times and MVT-predicted leave times for γ = 0.99, with discounting taken into account. The agent
on average behaves similar to the discounted MVT. c) Difference between the agent’s leave times and
MVT-predicted leave times for γ = 1.0, i.e., with no discounting taken into account. The agent on
average behaves similar to the MVT. Results for (b) and (c) are averaged over three seeds. Error bars
indicate standard error."
"B
PB",0.8682634730538922,"they target the standard RL objective Eπ "" ∞
X"
"B
PB",0.8697604790419161,"k=0
γkr(st+k)
st = s # ,"
"B
PB",0.8712574850299402,"which requires γ < 1 for convergence, optimal behavior is recovered only with
respect to the discounted MVT, in which R (and in our case, Rt) weights rewards by
powers of γ [8]."
"B
PB",0.8727544910179641,"In Fig. H.1(b-c) we perform a similar analysis to that of [8] and show that, on
average over multiple distances between rewards, λO-FB performs similarly to the
discounted MVT for γ = 0.99 and the standard MVT for γ = 1.0. An advantage
of the λO is that it is finite for γ = 1.0 provided that λ < 1. Hence, as opposed
to previous work, we can recover the standard MVT without the need to adjust for
discounting."
"B
PB",0.874251497005988,"Hyperparameters used are given in Table 1 (notation as in Algorithm 3). I
SAC"
"B
PB",0.875748502994012,"Mitigating Value Overestimation
One well-known challenge in deep RL is that the use
of function approximation to compute values is prone to overestimation. Standard
approaches to mitigate this issue typically do so by using two value functions and
either taking the minimum mini∈{1,2} Qπ
i (s, a) to form the Bellman target for a given
(s, a) pair [40] or combining them in other ways [41]. However, creating multiple
networks is expensive in both computation and memory. Instead, we hypothesized
that it might be possible to address this issue by using λ-based values. To test this
idea, we modified the Soft Actor-Critic [SAC; 42] algorithm to compute λFs-based
values by augmenting the soft value target TsoftQ = rt + γEVsoft(st+1), where
Vsoft(st+1) is given by the expression"
"B
PB",0.8772455089820359,"Eat+1∼π(·|st+1)
h
¯Q(st+1, at+1) + (λ −1)wT(ϕ(st, at) ⊙φλ(st+1, at+1))"
"B
PB",0.8787425149700598,"−α log π(at+1 | st+1)
i"
"B
PB",0.8802395209580839,"A derivation as well as pseudocode for the modified loss is provided in Appendix E.5.
Observe that for λ = 1, we recover the standard SAC value target, corresponding to
an assumed stationary reward. We apply this modified SAC algorithm, which we
term λ-SAC to feature-based Mujoco continuous control tasks within OpenAI Gym
[43]. We found that concatenating the raw state and action observations ˜ϕt = [st, at]
and normalizing them to [0, 1] make effective regressors to the reward. That is, we
compute base features as"
"B
PB",0.8817365269461078,"ϕb
t =
˜ϕb
t −minb ˜ϕb
t
maxb ˜ϕb
t −minb ˜ϕb
t
,"
"B
PB",0.8832335329341318,"where b indexes (st, at) within a batch. Let X ∈[0, 1]B×D be the concatenated
matrix of features for a batch, where D = dim(S) + dim(A). Then,"
"B
PB",0.8847305389221557,"wt =
 
XTX
−1 XTr,"
"B
PB",0.8862275449101796,"where here r denotes the vector of rewards from the batch. In addition to using a
fixed λ value, ideally we’d like an agent to adaptively update λ to achieve the best
balance of optimism and pessimism in its value estimates. Following [44], we frame
this decision as a multi-armed bandit problem, discretizing λ into three possible
values {0, 0.5, 1.0} representing the arms of the bandit. At the start of each episode,
a random value of λ is sampled from these arms and used in the value function
update. The probability of each arm is updated using the Exponentially Weighted
Average Forecasting algorithm [45], which modulates the probabilities in proportion
to a feedback score. As in [44], we use the difference in cumulative (undiscounted)
reward between the current episode ℓand the previous one ℓ−1 as this feedback
signal: Rℓ−Rℓ−1. That is, the probability of selecting a given value of λ increases if
performance is improving and decreases if it’s decreasing. We use identical settings
for the bandit algorithm as in [44]. We call this variant λ-SAC."
"B
PB",0.8877245508982036,"We plot the results for SAC with two critics (as is standard), SAC with one critic, SAC
with a single critic trained with λF-based values (“x-SAC” denotes SAC trained
with a fixed λ = x), and λ-SAC trained on the HalfCheetah-v2 and Hopper-v2
Mujoco environments. All experiments were repeated over eight random seeds.
HalfCheetah-v2 was found by [44] to support “optimistic” value estimates in that
even without pessimism to reduce overestimation it was possible to perform well.
Consistent with this, we found that single-critic SAC matched the performance of
standard SAC, as did 1-SAC (which amounts to training a standard value function
with the auxiliary task of SF prediction). Fixing lower values of λ performed poorly,
indicating that over-pessimism is harmful in this environment. However, λ-SAC
eventually manages to learn to set λ = 1 and matches the final performance of the
best fixed algorithms. Similarly, in [44] it was observed that strong performance
in Hopper-v2 was associated with pessimistic value estimates. Consistent with
this, λ-SAC learns to select lower values of λ, again matching the performance of
SAC while only requiring one critic and significantly reducing the required FLOPS
Fig. I.2. We consider these results to be very preliminary, and hope to perform more
experiments on other environments. We also believe λ-SAC could be improved
by using the difference between the current episode’s total reward and the average
of the total rewards from previous episodes Rℓ−(ℓ−1)−1 Pℓ−1
i=1 Ri as a more"
"B
PB",0.8892215568862275,"stable feedback signal for the bandit. There is also non-stationarity in the base
features due to the per-batch normalization, which could also likely be improved.
Hyperparameters are described in Table 2."
"B
PB",0.8907185628742516,"Hyperparameter
Value"
"B
PB",0.8922155688622755,"Collection Steps
1000
Random Action Steps
10000
Network Hidden Layers
256:256
Learning Rate
3 × 10−4
Optimizer
Adam
Replay Buffer Size
1 × 106
Action Limit
[−1, 1]
Exponential Moving Avg. Parameters
5 × 10−3
(Critic Update:Environment Step) Ratio
1
(Policy Update:Environment Step) Ratio
1
Has Target Policy?
No
Expected Entropy Target
−dim(A)
Policy Log-Variance Limits
[−20, 2]
Bandit Learning Rate∗
0.1
λ Options∗
{0, 0.5, 1.0}"
"B
PB",0.8937125748502994,Table 2: Hyperparameters for SAC Mujoco experiments. ∗Only applicable to λ-SAC
"B
PB",0.8952095808383234,"0.0
0.2
0.4
0.6
0.8
1.0
Timesteps
1e6 -2500 0 2500 5000 7500 10000 12500"
"B
PB",0.8967065868263473,Test Reward
"B
PB",0.8982035928143712,HalfCheetah-v2
"B
PB",0.8997005988023952,"SAC (2 critics)
SAC"
-SAC,0.9011976047904192,"1-SAC
0.5-SAC"
-SAC,0.9026946107784432,0-SAC -SAC
-SAC,0.9041916167664671,"0.0
0.2
0.4
0.6
0.8
1.0
Timesteps
1e6 0 500 1000 1500 2000 2500"
-SAC,0.905688622754491,Hopper-v2
-SAC,0.907185628742515,"0.0
0.2
0.4
0.6
0.8
1.0
Timesteps
1e6 0.0 0.2 0.4 0.6 0.8 1.0"
-SAC,0.9086826347305389,Choice
-SAC,0.9101796407185628,"HalfCheetah-v2
Hopper-v2"
-SAC,0.9116766467065869,Figure I.1: λ-SAC (1 critic) matches the performance of SAC (2 critics) by adapting λ online.
-SAC,0.9131736526946108,"300000
350000
400000
FLOPS 13000 13200 13400 13600 13800"
-SAC,0.9146706586826348,Final Test Reward
-SAC,0.9161676646706587,HalfCheetah-v2
-SAC,0.9176646706586826,SAC (2 critics) -SAC
-SAC,0.9191616766467066,"300000
350000
400000
FLOPS 1600 1800 2000 2200 2400 2600"
-SAC,0.9206586826347305,Hopper-v2
-SAC,0.9221556886227545,Figure I.2: λ-SAC matches performance while saving in computational cost.
-SAC,0.9236526946107785,"J
Replenishing Rewards"
-SAC,0.9251497005988024,"We list below a few candidate reward replenishment schemes, which are visualized
in Fig. J.1."
-SAC,0.9266467065868264,"Time elapsed rewards
r(s, t) = λn(s,t)/m(s,t)¯r(s),
where m(s, t) is the time elapsed since the last visit to state s:"
-SAC,0.9281437125748503,"m(s, t) ≜t −max{j|st+j = s, 0 ≤j ≤t −1}."
-SAC,0.9296407185628742,"Due to the max term in m(s, t), the corresponding representation Eπ "" ∞
X"
-SAC,0.9311377245508982,"k=0
γkλn(s,t)/m(s,t)1(st+k = s′)
st = s #"
-SAC,0.9326347305389222,"does not admit a closed-form recursion. However, we empirically tested a version of
this type of reward with Qλ-learning in the TwoRooms environment, modified so that
the exponent on λ is n(s, t)/(0.1m(s, t)). This was done so that reward replenishes
at a slow rate, reducing the deviation from the standard diminishing setting. Episode
length was capped at H = 100. All other settings are identical to the Qλ experiment
described in Appendix E. Results are presented in Fig. J.2 and a GIF is included in
the supplementary material."
-SAC,0.9341317365269461,Eligibility trace rewards
-SAC,0.9356287425149701,"r(s, t) = "
-SAC,0.937125748502994,"1 −(1 −λd) t−1
X"
-SAC,0.938622754491018,"j=0
λt−j
r
1(st+j = s) !"
-SAC,0.9401197604790419,"¯r(s),"
-SAC,0.9416167664670658,"where λd, λr ∈[0, 1] are diminishment and replenishment constants, respectively.
Denoting the corresponding representation by Ωπ, i.e.,"
-SAC,0.9431137724550899,"Ωπ(s, s′) = E "" ∞
X"
-SAC,0.9446107784431138,"k=0
γk"
-SAC,0.9461077844311377,"1 −(1 −λd) k
X"
-SAC,0.9476047904191617,"j=0
λk−j
r
1(st+j = s′) !"
-SAC,0.9491017964071856,1(st+k = s′)
-SAC,0.9505988023952096,"st = s # ,"
-SAC,0.9520958083832335,we obtain the following recursion:
-SAC,0.9535928143712575,"Ωπ(s, s′) = 1(s = s′)(λd −γλr(1 −λd)Est+1∼pπ(·|s)M π
γλr(st+1, s′))"
-SAC,0.9550898203592815,"+ γEst+1∼pπ(·|s)Ωπ(st+1, s′),"
-SAC,0.9565868263473054,"where M π
γλr denotes the successor representation of π with discount factor γλr. This
representation could be learned by alternating TD learning between Ωπ and M π
γλr.
We leave this to future work."
-SAC,0.9580838323353293,"Total time rewards
r(s, t) = λn(s,t)
d
λn(s,t)−t
r
¯r(s),
where λd, λr ∈[0, 1] are diminishment and replenishment constants, respectively.
The corresponding representation is"
-SAC,0.9595808383233533,"P π(s, s′) = E "" ∞
X"
-SAC,0.9610778443113772,"k=0
γkλnt(s′,k)
d
λk−nt(s′,k)
r
1(st+k = s′)"
-SAC,0.9625748502994012,"st = s # ,"
-SAC,0.9640718562874252,"0
2
4
6
8
10
12
14
Time 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
-SAC,0.9655688622754491,Reward
-SAC,0.9670658682634731,Time Elapsed
-SAC,0.968562874251497,"0
2
4
6
8
10
12
14
Time 0.75 0.80 0.85 0.90 0.95 1.00"
-SAC,0.9700598802395209,Reward
-SAC,0.9715568862275449,Eligibility Trace
-SAC,0.9730538922155688,"0
2
4
6
8
10
12
14
Time 0.4 0.6 0.8 1.0 1.2"
-SAC,0.9745508982035929,Reward
-SAC,0.9760479041916168,Total Time
-SAC,0.9775449101796407,"Figure J.1: Visualizing three different replenishment schemes. For all schemes, ¯r(s) = 1 and
visits to s are at t = 2, 5. (Left) The time elapsed reward with λ = 0.5; (Middle) The eligibility trace
reward with λr = λd = 0.5; (Right) The total time reward with λd = 0.5, λr = 0.9."
-SAC,0.9790419161676647,"0
100
200
300
400
500
Episodes 0 5 10 15 20 25 30"
-SAC,0.9805389221556886,Return
-SAC,0.9820359281437125,Q  Time Elapsed Replenishing Rewards
-SAC,0.9835329341317365,"= 0.5
= 1.0"
-SAC,0.9850299401197605,"Figure J.2: Performance on TwoRooms with replenishing rewards. Return is averaged over five
runs, with shading indicating one unit of standard error."
-SAC,0.9865269461077845,which satisfies the following recursion:
-SAC,0.9880239520958084,"P π(s, s′) = 1(s = s′) + γ(λd1(s = s′) + 1
−1(s = s′))(λr(1 −1(s = s′)) + 1(s = s′))Est+1∼pπ(·|s)P π(st+1, s′)."
-SAC,0.9895209580838323,"While neither the reward nor the representation are guaranteed to be finite, P π could
be learned via TD updates capped at a suitable upper bound."
-SAC,0.9910179640718563,"K
λ vs. γ"
-SAC,0.9925149700598802,"We now briefly discuss the interaction between the temporal discount factor γ
commonly used in RL and the diminishing utility rate λ. The key distinction
between the two is that all rewards decay in value every time step with respect to
γ, regardless of whether a state is visited or not. With λ, however, decay is specific
to each state (or (s, a) pair) and only occurs when the agent visits that state. Thus,
γ decays reward in a global manner which is independent of the agent’s behavior,
and λ decays reward in a local manner which dependent on the agent’s behavior. In
combination, they have the beneficial effect of accelerating convergence in dynamic
programming (Fig. F.3). This indicates the potential for the use of higher discount
factors in practice, as paired with a decay factor λ, similar (or faster) convergence
rates could be observed even as agents are able to act with a longer effective temporal
horizon."
-SAC,0.9940119760479041,"L
Compute Resources"
-SAC,0.9955089820359282,"The λF-based experiments shown were run on a single NVIDIA GeForce GTX
1080 GPU. The recurrent A2C experiments took roughly 30 minutes, base feature
learning for policy composition took approximately 45 minutes, λF learning for
policy composition took approximately 10 hours, and the SAC experiments took
approximately 8 hours per run. The λF training required roughly 30GB of memory
due to the size of the dataset. All experiments in Section 6 and Appendix H were
run on a single RTX5000 GPU and each training and evaluation run took about 30
minutes. All other experiments were run on a 2020 MacBook Air laptop 1.1 GHz
Quad-Core Intel Core i5 CPU and took less than one hour to train."
-SAC,0.9970059880239521,"M
Broader Impact Statement"
-SAC,0.9985029940119761,"We consider this work to be primarily of a theoretical nature pertaining to sequential
decision-making primarily in the context of natural intelligence. While it may have
applications for more efficient training of artificial RL agents, it is hard to predict
long-term societal impacts."
