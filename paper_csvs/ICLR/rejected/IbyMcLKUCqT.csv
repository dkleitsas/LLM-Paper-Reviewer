Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001226993865030675,"Data augmentation is popular in the training of large neural networks; currently,
however, there is no clear theoretical comparison between different algorithmic
choices on how to use augmented data. In this paper, we take a small step in this
direction; we present a simple new statistical framework to analyze data augmen-
tation - speciﬁcally, one that captures what it means for one input sample to be an
augmentation of another, and also the richness of the augmented set. We use this
to interpret consistency regularization as a way to reduce function class complex-
ity, and characterize its generalization performance. Specializing this analysis for
linear regression shows that consistency regularization has strictly better sample
efﬁciency as compared to empirical risk minimization (ERM) on the augmented
set. In addition, we also provide generalization bounds under consistency reg-
ularization for logistic regression and two-layer neural networks. We perform
experiments that make a clean and apples-to-apples comparison (i.e. with no ex-
tra modeling or data tweaks) between ERM and consistency regularization using
CIFAR-100 and WideResNet; these demonstrate the superior efﬁcacy of consis-
tency regularization."
INTRODUCTION,0.00245398773006135,"1
INTRODUCTION"
INTRODUCTION,0.0036809815950920245,"Modern machine learning models, especially deep learning models, require abundant training sam-
ples. Since data collection and human annotation are expensive, data augmentation has been a
ubiquitous practice in creating artiﬁcial labeled samples and improving generalization performance.
This practice is corroborated by the fact that the semantics of images remain the same through sim-
ple translations like obscuring, ﬂipping, rotation, color jitter, rescaling (Shorten & Khoshgoftaar,
2019)."
INTRODUCTION,0.0049079754601227,"Conventional algorithms use data augmentation to expand the training data set (Krizhevsky et al.,
2012; Simard et al., 1998; Cubuk et al., 2018; Simonyan & Zisserman, 2014; He et al., 2016). As
an alternative, consistency regularization enforces the model to output similar predictions on the
original and augmented samples and contributes to many recent state-of-the-art supervised or semi-
supervised algorithms. This idea was ﬁrst proposed in (Bachman et al., 2014) and popularized
by Laine & Aila (2016); Sajjadi et al. (2016), and gained more attention recently with the success of
FixMatch (Sohn et al., 2020) for semi-supervised few-shot learning, and AdaMatch (Berthelot et al.,
2021) for domain adaptation."
INTRODUCTION,0.006134969325153374,"Several recent papers (see e.g. (Chen et al., 2020a; Mei et al., 2021; Lyle et al., 2019)) attempt
to provide a theoretical understanding of data augmentation (DA); they focus on establishing that
augmenting data saves on the number of labeled samples needed for the same level of accuracy.
However, none of these explicitly compare in an apples to apples way the efﬁcacy (in terms of the
number of augmented samples) of one algorithmic choice of how to use the augmented samples vs
another algorithmic choice. Another dimension un-explored in previous work is any characterization
of the quality of augmentation."
INTRODUCTION,0.007361963190184049,"In this paper, we hope to answer the following research question:"
INTRODUCTION,0.008588957055214725,"Is it possible to develop a theoretical framework to compare the sample efﬁciency of different
algorithms that use augmented data?"
INTRODUCTION,0.0098159509202454,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.011042944785276074,"We present a new theoretical framework that casts consistency regularization as a way to reduce
function class complexity, which immediately connects to the well-established theory on gener-
alization and gives rise to a generalization bound for consistency regularization under a general
bounded loss function. When specialized to linear regression, this new theoretical framework shows
that the consistency regularization is strictly more sample efﬁcient than empirical risk minimization
(ERM) on the augmented dataset. In addition, using this framework, we also provide generaliza-
tion bounds under consistency regularization for logistic regression, two-layer neural networks, and
expansion-based data augmentations."
INTRODUCTION,0.012269938650306749,"As summary, our main contributions are:"
INTRODUCTION,0.013496932515337423,"• A statistical framework of consistency regularization. We ﬁrst present a simple new
statistical framework to analyze data augmentation - with a formal theoretical deﬁnition
of data augmentation and its strength. We then use this framework to give a generalization
bound of consistency regularization and provide instantiations for linear regression, logistic
regression, two-layer neural networks, and expansion-based data augmentations.
• Theoretically proving the efﬁcacy of consistency regularization. When specializing our
framework with consistency regularization to linear/logistic regression, it yields a strictly
smaller generalization error than ERM with the same augmented data.
• Empirical comparisons between consistency regularization and ERM. We perform ex-
periments that make a clean and apples-apples comparison (i.e., with no extra modeling or
data tweaks) between consistency regularization and ERM using CIFAR-100 and WideRes-
Net. Our empirical results demonstrate the superior efﬁcacy of consistency regularization."
RELATED WORK,0.014723926380368098,"2
RELATED WORK"
RELATED WORK,0.015950920245398775,"Empirical ﬁndings.
Data augmentation (DA) is an essential recipe for almost every state-of-the-
art supervised learning algorithm since the seminal work of (Krizhevsky et al., 2012) (see reference
therein (Simard et al., 1998; Cubuk et al., 2018; Simonyan & Zisserman, 2014; He et al., 2016;
Kuchnik & Smith, 2018)). It started from adding augmented data to the training samples via (ran-
dom) perturbations, distortions, scales, crops, rotations, and horizontal ﬂips. More sophisticated
variants were subsequently designed; a non-exhaustive list includes Mixup (Zhang et al., 2017),
Cutout (DeVries & Taylor, 2017), and Cutmix (Yun et al., 2019). The choice of data augmentation
and their combinations require domain knowledge and experts’ heuristics, which triggered some
automated search algorithm to ﬁnd the best augmentation strategies (Lim et al., 2019; Cubuk et al.,
2019). The effects of different DAs have been systematically explored in (Tensmeyer & Martinez,
2016)."
RELATED WORK,0.01717791411042945,"Recent practices not only add augmented data to the training set but also enforce the predictor output
to be similar by adding consistency regularization (Bachman et al., 2014; Laine & Aila, 2016; Sohn
et al., 2020). One beneﬁt of consistency regularization is the feasibility of exploiting unlabeled data.
Therefore input consistency on augmented data also formed a major component to state-of-the-
art algorithms for semi-supervised learning (Laine & Aila, 2016; Sajjadi et al., 2016; Sohn et al.,
2020; Xie et al., 2020), self-supervised learning (Chen et al., 2020b), and unsupervised domain
adaptation (French et al., 2017; Berthelot et al., 2021)."
RELATED WORK,0.018404907975460124,"Theoretical studies.
Many interpret the effect of DA as some form of regularization (He et al.,
2019). Some work focuses on linear transformations and linear models (Wu et al., 2020) or kernel
classiﬁers (Dao et al., 2019). Convolutional neural networks by design enforce translation equiv-
ariance symmetry (Benton et al., 2020; Li et al., 2019); further studies have hard-coded CNN’s
invariance or equivariance to rotation (Cohen & Welling, 2016; Marcos et al., 2017; Worrall et al.,
2017; Zhou et al., 2017), scaling (Sosnovik et al., 2019; Worrall & Welling, 2019) and other types
of transformations."
RELATED WORK,0.0196319018404908,"A line of work view data augmentation as invariant learning by averaging over group actions (Chen
et al., 2020a; Mei et al., 2021; Lyle et al., 2019). They consider an ideal setting that is equivalent
to ERM with all possible augmented data, bringing a clean mathematical interpretation. We are
interested in a more realistic setting with limited augmented data. In this setting, it is crucial to
utilize the limited data with proper training methods, the difference of which cannot be revealed
under the previous studied settings."
RELATED WORK,0.020858895705521473,Under review as a conference paper at ICLR 2022
RELATED WORK,0.022085889570552148,"Some more recent work investigates the feature representation learning procedure with DA for self-
supervised learning tasks (Wen & Li, 2021; HaoChen et al., 2021; von K¨ugelgen et al., 2021; Garg
& Liang, 2020). Cai et al. (2021); Wei et al. (2021) studied the effect of data augmentation with
label propagation. Data augmentation is also deployed to improve robustness (Rajput et al., 2019),
to facilitate domain adaptation and domain generalization (Cai et al., 2021; Sagawa et al., 2019) ."
DATA AUGMENTATION CONSISTENCY AND HOW IT LEARNS EFFICIENTLY,0.023312883435582823,"3
DATA AUGMENTATION CONSISTENCY AND HOW IT LEARNS EFFICIENTLY"
DATA AUGMENTATION CONSISTENCY AND HOW IT LEARNS EFFICIENTLY,0.024539877300613498,"In this section, we ﬁrst formally deﬁne data augmentation and introduce the problem setup. We then
deﬁne data augmentation consistency (DAC) regularization and show how it effectively reduces the
function class complexity, which connects to a generalization bound for bounded loss functions
via Rademacher complexity. Subsequently, we specialize our general result to linear regression,
which ﬁrmly shows that the DAC regularization provably learns more efﬁciently than minimizing
the empirical risk on the augmented dataset. The following section will present more applications
(including logistic regression, neural network, etc.)."
PROBLEM SETUP AND DATA AUGMENTATION,0.025766871165644172,"3.1
PROBLEM SETUP AND DATA AUGMENTATION"
PROBLEM SETUP AND DATA AUGMENTATION,0.026993865030674847,"Consider the standard supervised learning problem setup: x ∈X are input features, and y ∈Y is
its label (or response). Let P ∗be the true distribution of (x, y) (i.e., the label distribution follows
y ∼P ∗(y|x)). We can then formally deﬁne data augmentation as:"
PROBLEM SETUP AND DATA AUGMENTATION,0.02822085889570552,"Deﬁnition 1 (Data augmentation). For any sample x ∈X, we say x′ ∈X is its augmentation, if
and only if P ∗(y|x) = P ∗(y|x′)."
PROBLEM SETUP AND DATA AUGMENTATION,0.029447852760736196,"The deﬁnition above speciﬁes what it means for one input sample to be an augmentation of another.
While the deﬁnition covers any x′ with the same label distribution as x, our results only use the aug-
mented samples that can be achieved via certain transformations (e.g., random cropping, rotation).
However, our deﬁnition does not cover augmentations that alter the labels (e.g., MixUp (Zhang et al.,
2017))."
PROBLEM SETUP AND DATA AUGMENTATION,0.03067484662576687,"Now we introduce the learning problem on an augmented dataset: Let (X, y) ∈X N × YN be a
training set consisting of N i.i.d. samples. Besides the original (X, y), each training sample in it is
provided with α augmented samples. The input features of the augmented dataset can be written as:"
PROBLEM SETUP AND DATA AUGMENTATION,0.03190184049079755,"e
A(X) = [x1; · · · ; xN; x1,1; · · · ; xN,1; · · · ; x1,α; · · · ; xN,α] ∈X (1+α)N,"
PROBLEM SETUP AND DATA AUGMENTATION,0.033128834355828224,"where xi is in the original training set and xi,j, ∀j ∈[α] are the augmentations of xi. The labels
of the augmented samples are kept the same, which can be denoted as f
My ∈Y(1+α)N, where
f
M ∈R(1+α)N×N is a vertical stack of (1 + α) identity mappings. Speciﬁcally, when the input xis
are d-dimensional real vectors, we have the following notion of augmentation strength daug:"
PROBLEM SETUP AND DATA AUGMENTATION,0.0343558282208589,"Deﬁnition 2 (Strength of augmentations). For any δ ∈(0, 1), let"
PROBLEM SETUP AND DATA AUGMENTATION,0.03558282208588957,"daug(δ) ≜argmax
daug
P e
A,X
h
rank

e
A(X) −f
MX

< daug
i
≤δ,
daug ≜daug (1/N) ."
PROBLEM SETUP AND DATA AUGMENTATION,0.03680981595092025,"Intuitively, strength of augmentations daug(δ) means that with probability at least 1 −δ, the aug-
mentations perturb at least daug(δ) dimensions; whereas daug can be intuitively understood as the
minimum number of dimensions that the augmentations in e
A(X) perturbed with high probability. A
lager daug corresponds to a stronger data augmentations. For instance, when e
A(X) = f
MX almost
surely (e.g., when the augmentations are identical copies of the original samples, corresponding to
the weakest augmentation – no augmentations at all), we have daug(δ) = daug = 0 for all δ ∈(0, 1).
On the other hand, if the augmentations are randomly generated, then it is more likely to see larger
daug (i.e., more dimensions being perturbed) with larger α (i.e., more augmentations)."
PROBLEM SETUP AND DATA AUGMENTATION,0.03803680981595092,"In the next subsection, we formally introduce “data augmentation consistency regularization” and
present a generalization bound under bounded loss functions. We subsequently specialize the bound
to linear regression and show that consistency regularization is strictly more sample efﬁcient than
empirical risk minimization (ERM) on the augmented dataset."
PROBLEM SETUP AND DATA AUGMENTATION,0.0392638036809816,Under review as a conference paper at ICLR 2022
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.04049079754601227,"3.2
DATA AUGMENTATION CONSISTENCY REGULARIZATION"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.04171779141104295,"Let H = {h : X →Y} be a well-speciﬁed function class (e.g. for regression problems, ∃h∗∈H,
s.t. h∗(x) = E[y|x]) that we hope to learn from. Without loss of generality, we assume that
each function h ∈H can be expressed as h = fh ◦φh, where φh ∈Φ = {φ : X →W} is a
proper representation mapping and fh ∈F = {f : W →Y} is a predictor on top of the learned
representation. We tend to decompose h such that φh is a powerful feature extraction function
whereas fh can be as simple as a linear combiner. For instance, in a deep neural network, all the
layers before the ﬁnal prediction layer can be viewed as feature extraction φh, and the predictor fh
corresponds to the ﬁnal linear combination of the features."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.04294478527607362,"For a loss function l : Y × Y →R and a metric ϱ properly deﬁned on the representation space W,
learning with data augmentation consistency (DAC) regularization can be expressed as:"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.044171779141104296,"argmin
h∈H N
X"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.04539877300613497,"i=1
l(h(xi), yi) + λ α
X j=1 N
X"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.046625766871165646,"i=1
ϱ (φh(xi), φh(xi,j))"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.04785276073619632,"|
{z
}
DAC regularization .
(1)"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.049079754601226995,"Note that the DAC regularization in Equation (1) can be easily implemented empirically as a regu-
larizer. Intuitively, DAC regularization penalizes the representation difference between the original
sample φh(xi) and the augmented sample φh(xi,j), with the belief that similar samples (i.e., origi-
nal and augmented samples) should have similar representations. When the data augmentations do
not alter the labels, it is reasonable to enforce a strong regularization (i.e., λ →∞) - given that
the conditional distribution of y does not change. The function bhdac learned with data augmentation
consistency regularization can then be written as the solution of a constrained optimization problem:"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.05030674846625767,"bhdac ≜argmin
h∈H N
X"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.051533742331288344,"i=1
l(h(xi), yi)
s.t.
φh(xi) = φh(xi,j), ∀i ∈[N], j ∈[α].
(2)"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.05276073619631902,"The constraint in Equation (2) effectively reduces the size of the function class H. To rigorously
capture such reduction, we deﬁne the following data augmentation consistency (DAC) operator over
H. Given the original training samples X and the augmented dataset e
A(X), we have:
Deﬁnition 3 (Data Augmentation Consistency Operator)."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.053987730061349694,"T dac
e
A,X(H) ≜{h | h ∈H, φh(xi) = φh(xi,j), ∀i ∈[N], j ∈[α]} ."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.05521472392638037,Original Function Class:
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.05644171779141104,Reduced Function Class:
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.05766871165644172,can be different.
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.05889570552147239,is the feature mapping of s.t.
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.06012269938650307,"Figure 1: The DAC regularization reduces
function class H to T dac
e
A,X (H)."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.06134969325153374,"Particularly, we assume that a proper representa-
tion class Φ is chosen with respect to the augmen-
tations such that h∗∈T dac
e
A,X(H). The DAC opera-
tor maps the original function class H to a (poten-
tially much smaller) subset T dac
e
A,X(H), where every"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.06257668711656442,"function h ∈T dac
e
A,X(H) gives consistent represen-
tation for all samples and their augmentations (i.e.,
φh(xi) = φh(xi,j)). It is now clear that with DAC
regularization, Equation (2) is effectively learning in
the function class T dac
e
A,X (H), which is a subset of H.
As we will show in the next subsection, the function
class size reduction is the key for efﬁcient learning
with data augmentations."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.0638036809815951,"One of the contributions of our paper is to view consistency regularization as function class complex-
ity reduction. Our next proposition connects the generalization bound to the Rademacher complexity
T dac
e
A,X(H) via standard analysis. We further provide various instantiations in the rest of our paper."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.06503067484662577,"Let L(h) denote the population loss induced by h ∈H, i.e., L(h) ≜E(x,y)∼P ∗[l(h(x), y)].
Proposition 1 (formally in Proposition 8). With Equation (2), if l is B-bounded and Cl-Lipschitz,
then for a ﬁxed T dac
e
A,X(H) and any δ ∈(0, 1), with probability at least 1 −δ, we have"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.06625766871165645,"L(bhdac) −L(h∗) ≤4Cl · RN

T dac
e
A,X(H)

+ r"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.06748466257668712,"2B2 log(2/δ) N
."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.0687116564417178,Under review as a conference paper at ICLR 2022
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.06993865030674846,"Remark 1. Our results can be immediately extended to use unlabeled data. Notice that the DAC
regularization only enforces the same predictions for the original and augmented samples, where no
ground truth label is needed. For clarity of exposition, we focus on the labeled dataset in the main
text and defer discussions on unlabeled data to the appendix."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.07116564417177915,"3.3
EFFICACY OF DAC REGULARIZATION: A GENTLE START WITH LINEAR REGRESSION"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.07239263803680981,"To see the efﬁcacy of DAC regularization (i.e., Equation (2)), we revisit a more commonly adopted
training method here - empirical risk minimization (ERM) on augmented data:"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.0736196319018405,"bhda−erm ≜argmin
h∈H N
X"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.07484662576687116,"i=1
l(h(xi), yi) + N
X i=1 α
X"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.07607361963190185,"j=1
l(h(xi,j), yi).
(3)"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.07730061349693251,"Now we show that the DAC regularization learns more efﬁciently than ERM. Consider the following
setting: given N observations X ∈RN×d, the responses y ∈RN are generated from a linear model
y = Xθ∗+ ϵ, where ϵ ∈RN is zero-mean noise with E

ϵϵ⊤
= σ2IN. Recall that e
A(X) is
the entire augmented dataset, and f
My corresponds to the labels. Our next result characterizes the"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.0785276073619632,"ﬁxed design excess risk of θ on e
A(X), which is deﬁned as L(θ) ≜
1
(1+α)N
 e
A(X)θ −e
A(X)θ∗
2"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.07975460122699386,"2.
Under regularity conditions (e.g., x is sub-Gaussian and N is not too small), it is not hard to extend
to random design, i.e., the more commonly acknowledged generalization bound with the same order."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.08098159509202454,"Given e
A(X), notice that by Deﬁnition 2, daug = rank

e
A(X) −f
MX

since there is no randomness"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.08220858895705521,"in e
A, X in ﬁxed design setting. For a linear regression model to be identiﬁable (i.e., having an unique
solution), we assume that e
A(X) has full column rank. We then have the following theorem on the
excess risks of learning by DAC regularization and by ERM on the augmented dataset.
Theorem 2 (Informal result on linear regression (formally in Theorem 6)). Learning with DAC
regularization, we have E
h
L(bθdac) −L(θ∗)
i
= (d−daug)σ2"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.0834355828220859,"N
, while learning with ERM directly on"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.08466257668711656,"the augmented dataset, we have E
h
L(bθda−erm) −L(θ∗)
i
= (d−daug+d′)σ2"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.08588957055214724,"N
, where d′ ∈[0, daug]."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.08711656441717791,"Formally, d′ is deﬁned as d′ ≜
tr((H e
A(X)−PS)f
Mf
M⊤)
1+α
, where H e
A(X) ≜e
A(X) e
A(X)†, and PS is the"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.08834355828220859,"orthogonal projector onto S ≜
n
f
MXθ | ∀θ ∈Rd, s.t.

e
A(X) −f
MX

θ = 0
o
."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.08957055214723926,"0
5
10
15
20
Different d′s induced by Example 1 0.1 0.2 0.3 0.4 0.5"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.09079754601226994,Excess Risk
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.09202453987730061,DAC with daug=25
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.09325153374233129,DA-ERM with daug=25
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.09447852760736196,DAC with daug=20
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.09570552147239264,DA-ERM with daug=20
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.09693251533742331,"Figure 2: Comparison of DAC regularization and
DA-ERM (Example 1).
The results precisely
match Theorem 2. DA-ERM depends on the d′ in-
duced by different augmentations, while the DAC
regularization works equally well and better than
the DA-ERM. Further, both DAC and DA-ERM
are affected by the “strength of augmentation”
daug."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.09815950920245399,"Here
we
present
an
explanation
for
d′.
Note that σ2 · f
Mf
M⊤
is the noise co-
variance matrix of the augmented dataset.
We have tr

PSf
Mf
M⊤
being the variance"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.09938650306748466,"of bθdac, and tr

H e
A(X)f
Mf
M⊤
being the"
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.10061349693251534,"variance of bθda−erm.
Therefore,
d′
∝
tr

H e
A(X) −PS

f
Mf
M⊤
is used to mea-
sure the difference. When H e
A(X) ̸= PS (a
common scenario as instantiated in Example 1),
we have DAC being strictly better than ERM on
augmented data.
Example 1. Consider a 30-dimensional lin-
ear regression. The original training set con-
tains 50 samples.
The inputs xis are gener-
ated independently from N(0, I30) and we set
θ∗= [θ∗
c; 0] with θ∗
c ∼N(0, I5) and 0 ∈R25.
The noise variance σ is set to 1. We break x into 3 parts [xc1, xe1, xe2] and take the following
augmentations: A([xc1; xe1; xe2]) = [xc1; 2xe1; −xe2], xc1 ∈Rdc1, xe1 ∈Rde1, xe2 ∈Rde2, where
dc1 + de1 + de2 = 30."
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.10184049079754601,Under review as a conference paper at ICLR 2022
DATA AUGMENTATION CONSISTENCY REGULARIZATION,0.10306748466257669,"Notice that the augmentation perturbs xe1 and xe2 and leaving xc1 unchanged, we therefore have
daug = 30 −dc1. By changing dc1 and de1, we can have different augmentations with different
daug, d′. The results for daug ∈{20, 25} and various d′s induced by different de1s are presented in
Figure 2. The excess risks precisely match Theorem 2. It conﬁrms that the DAC regularization is
strictly better than ERM on an augmented dataset for a wide variety of augmentations."
VARIOUS APPLICATIONS,0.10429447852760736,"4
VARIOUS APPLICATIONS"
VARIOUS APPLICATIONS,0.10552147239263804,"Now we ground our general result on the DAC regularization with a set of common applications, in-
cluding the logistic regression, two-layer neural networks, and expansion-based data augmentations.
For each of the applications, we ﬁrst specify the ground truth distribution P ∗, the function class H,
the loss function l, as well as the augmented dataset e
A(X). Then, we discuss the corresponding
excess risk L(bhdac)−L(h∗) for the DAC regularization (Equation (2)). We abridge the analysis in a
set of concrete examples with concise arguments while deferring the complete assumptions and the-
orems to Appendix B. As a supplementary remark, in addition to the popular in-distribution setting
where we consider a unique distribution P ∗for both training and testing, DAC regularization is also
known to improve out-of-distribution generalization for settings like domain adaptation. We defer
detailed discussion on such advantage of DAC regularization for linear regression in the domain
adaptation setting to Appendix B.4."
LOGISTIC REGRESSION,0.1067484662576687,"4.1
LOGISTIC REGRESSION"
LOGISTIC REGRESSION,0.10797546012269939,"Here we consider logistic regressions with X
=

x ∈Rd  ∥x∥2 ≤D
	
, Y = {0, 1}.
For
some unkown θ∗
∈
Rd with ∥θ∗∥
≤
C0, we have P ∗(y = 1|x)
=
σ
 
θ∗⊤x

, where
σ(·) is the sigmoid function σ(z)
≜
1/(1 + exp(−z)).
The function class H is H
=

h(x) = θ⊤x
 θ ∈Rd, ∥θ∥2 ≤C0
	
, such that predictions are given by by = σ
 
θ⊤x

. For binary
classiﬁcation, we use the logistic loss l(θ⊤x, y) = −y log(σ(θ⊤x)) −(1 −y) log(1 −σ(θ⊤x))."
LOGISTIC REGRESSION,0.10920245398773006,"Recall the strength of augmentations daug from Deﬁnition 2. Under proper regularity conditions (i.e.,
x is sub-Gaussian and N is not too small, see Appendix B), we have the following generalization
bound for learning logistic regression under DAC regularization.
Theorem 3 (Informal result on logistic regression with DAC (formally in Theorem 9)). Learning
logistic regression with the DAC regularization h(xi) = h(xi,j), with high probability:"
LOGISTIC REGRESSION,0.11042944785276074,L(bhdac) −L(h∗) ≲ r
LOGISTIC REGRESSION,0.1116564417177914,"d −daug + log N N
,"
LOGISTIC REGRESSION,0.11288343558282209,"where the strength of augmentation daug ∈[0, d −1]."
LOGISTIC REGRESSION,0.11411042944785275,"Intuitively, with high probability, minimizing empirical loss on augmented samples gives a gener-"
LOGISTIC REGRESSION,0.11533742331288344,"alization bound of L(bhda−erm) −L(h∗) ≲max
q"
LOGISTIC REGRESSION,0.1165644171779141,"d
(α+1)N ,
q"
LOGISTIC REGRESSION,0.11779141104294479,d−daug N
LOGISTIC REGRESSION,0.11901840490797547,"
at best, where the ﬁrst"
LOGISTIC REGRESSION,0.12024539877300613,"term corresponds to the generalization bound for a d-dimensional logistic regression with (α + 1)N
samples, and the second term follows as the augmentations fail to perturb a (d −daug)-dimensional
sub-space (and in which ERM can only rely on the N original samples for learning). The ﬁrst term
will dominate the max when there is limited augmented data (i.e., α is small)."
LOGISTIC REGRESSION,0.12147239263803682,"Comparing the two, we see that DAC is more efﬁcient than ERM. In particular, consider the scenario
that the limited data augmentations well perturb the data (e.g., α = 1 and daug = d −1). The ERM
gives a generalization error that scales as
p"
LOGISTIC REGRESSION,0.12269938650306748,"d/N, while DAC yields a dimension-free
p"
LOGISTIC REGRESSION,0.12392638036809817,"1/N error.
Please see Appendix E for a supporting numerical example, which demonstrates the beneﬁts of DAC
over DA-ERM for logistic regression, and empirically veriﬁes the impact of daug and α that matches
with our theoretical analysis."
TWO-LAYER NEURAL NETWORK,0.12515337423312883,"4.2
TWO-LAYER NEURAL NETWORK"
TWO-LAYER NEURAL NETWORK,0.1263803680981595,"In this section, we discuss a special case where the result for bounded losses in Proposition 8 can
be extended to the unbounded square loss. With X = Rd and Y = R, we consider a ground truth"
TWO-LAYER NEURAL NETWORK,0.1276073619631902,Under review as a conference paper at ICLR 2022
TWO-LAYER NEURAL NETWORK,0.12883435582822086,"distribution P ∗(y|x) induced by a two-layer ReLU network: y =
 
x⊤B∗"
TWO-LAYER NEURAL NETWORK,0.13006134969325153,"+ w∗+ z,
B∗
d×q ="
TWO-LAYER NEURAL NETWORK,0.1312883435582822,"[b∗
1 . . . b∗
k . . . b∗
d] , for some unknown h∗(x) ≜
 
x⊤B∗"
TWO-LAYER NEURAL NETWORK,0.1325153374233129,"+ w∗where (·)+ ≜max(0, ·) element-
wisely denotes the ReLU function, b∗
k ∈Sd−1 for all k ∈[q], and z ∼N
 
0, σ2
is the Gaussian
noise. In terms of the function class H, for some constant Cw ≥∥w∗∥1, we have:"
TWO-LAYER NEURAL NETWORK,0.13374233128834356,"H =

h(x) = (x⊤B)+w
 B = [b1 . . . bq] ∈Rd×q, ∥bk∥2 = 1 ∀j ∈[q], ∥w∥1 ≤Cw
	
,"
TWO-LAYER NEURAL NETWORK,0.13496932515337423,"such that h∗∈H. We use the standard square loss l(h(x), y) = 1"
TWO-LAYER NEURAL NETWORK,0.1361963190184049,2(h(x) −y)2.
TWO-LAYER NEURAL NETWORK,0.1374233128834356,"Let PN be the projector onto the null space of e
A(X) −f
MX. When the augmented sample set
e
A(X) is reasonably diverse (see Appendix B.2), regression over two-layer ReLU networks with the
DAC regularization generalizes as following:
Theorem 4 (Informal result on two-layer neural network with DAC (formally in Theorem 10)).
Assuming E
h
1
n
Pn
i=1 ∥PN xi∥2
2
PN
i
≤C2
N for some CN > 0, learning the two-layer ReLU"
TWO-LAYER NEURAL NETWORK,0.13865030674846626,"network with the DAC regularization on
 
x⊤
i B
"
TWO-LAYER NEURAL NETWORK,0.13987730061349693,"+ =
 
x⊤
i,jB
"
TWO-LAYER NEURAL NETWORK,0.1411042944785276,"+ gives, with high probability:"
TWO-LAYER NEURAL NETWORK,0.1423312883435583,"L

bhdac
−L (h∗) ≲σCwCN
√ N
."
TWO-LAYER NEURAL NETWORK,0.14355828220858896,"Recall the strength of augmentations daug from Deﬁnition 2.
Under proper regularity condi-
tions (i.e., x is sub-Gaussian and N is reasonably large, see Appendix B.2), we have CN ≲
p"
TWO-LAYER NEURAL NETWORK,0.14478527607361963,"d −daug with high probability.
Analogous to the logistic regression example, applying the
ERM directly on the augmented samples achieves no better than L(bhda−erm) −L(h∗)
≲"
TWO-LAYER NEURAL NETWORK,0.1460122699386503,"σCwmax
q"
TWO-LAYER NEURAL NETWORK,0.147239263803681,"d
(α+1)N ,
q"
TWO-LAYER NEURAL NETWORK,0.14846625766871166,d−daug N
TWO-LAYER NEURAL NETWORK,0.14969325153374233,"
with high probability. The comparison again illustrates the advan-"
TWO-LAYER NEURAL NETWORK,0.150920245398773,"tage of DAC regularization over ERM. The advantage is large when the limited data augmentations
are strong (i.e., large daug and small α)."
EXPANSION-BASED DATA AUGMENTATIONS,0.1521472392638037,"4.3
EXPANSION-BASED DATA AUGMENTATIONS"
EXPANSION-BASED DATA AUGMENTATIONS,0.15337423312883436,"In this section, we demonstrate that for the classiﬁcation problems, enforcing consistency on a dif-
ferent notion of data augmentations based on expansion also brings a considerable reduction in
complexity of the feasible function class."
EXPANSION-BASED DATA AUGMENTATIONS,0.15460122699386503,"Concretely, we consider a multi-class classiﬁcation problem: for an arbitrary set X, let Y = [K],
and h∗: X →[K] be the ground truth classiﬁer that partitions X: for each k ∈[K], let Xk ≜
{x ∈X | h∗(x) = k}, with Xi ∩Xj = ∅, ∀i ̸= j. Here we focus on the expansion-based data
augmentations deﬁned as following:
Deﬁnition 4 (Expansion-based data augmentations). Let A : X →2X be a function generating a
set of augmentations A(x) from a given x ∈X that satisﬁes the following:"
EXPANSION-BASED DATA AUGMENTATIONS,0.1558282208588957,"(a) Nontrivial augmentation and class invariant: {x} ⊊A(x) ⊆{x′ ∈X | h∗(x) = h∗(x′)} for
all x ∈X; and
(b) Non-trivial expansion: for all k ∈[K], given any ∅⊊S ⊊Xk, there exists some x′ /∈S such
that A(x) ∩A(x′) ̸= ∅."
EXPANSION-BASED DATA AUGMENTATIONS,0.1570552147239264,"We consider a general class of classiﬁers H ⊆{h : X →[K]} where the ground truth classiﬁer is re-
alizable, h∗∈H. With the zero-one loss l01 (h(x), y) = 1 {h(x) ̸= y}, and the corresponding pop-
ulation loss L01 (h) ≜Ex [1 {h (x) ̸= h∗(x)}], we learn a classiﬁer from H with the DAC regular-
ization, h(x) = h(x′), where x′s are expansion-based data augmentations generated by an A (Deﬁ-
nition 4). As a warm-up, we begin with a simpliﬁed setting where we enforce consistency over the
population in lieu of a ﬁnite set of training samples as in practice. Then, learning with the DAC reg-
ularization yields a classiﬁer bhdac ∈T dac
A,X (H) ≜{h ∈H | h(x) = h(x′) ∀x ∈X, x′ ∈A(x)}.
Theorem 5 (DAC with expansion-based data augmentations over population). Learning the classi-
ﬁer with DAC regularization over population, with high probability,"
EXPANSION-BASED DATA AUGMENTATIONS,0.15828220858895706,"L01

bhdac
−L01 (h∗) ≤K log K + log N N
."
EXPANSION-BASED DATA AUGMENTATIONS,0.15950920245398773,Under review as a conference paper at ICLR 2022
EXPANSION-BASED DATA AUGMENTATIONS,0.1607361963190184,"Particularly, with the DAC regularization, the generalization bound of the K-class classiﬁcation
problem is dimension-independent but only scales with the number of classes, eO(K)."
EXPANSION-BASED DATA AUGMENTATIONS,0.1619631901840491,"Furthermore, in Appendix B.3, we extend the result to a more practical setting where the consistency
is enforced over a ﬁnite training set. Notably, Theorem 5, along with the ﬁnite train set case in
Appendix B.3, is a reminiscence of Theorem 3.6 and 3.7 by Wei et al. (2021), as well as Theorem
2.1, 2.2, and 2.3 by Cai et al. (2021). We adapt these existing theories and provide a uniﬁed analysis
under our setting, which demonstrates the generality of our framework."
EXPERIMENTS,0.16319018404907976,"5
EXPERIMENTS"
EXPERIMENTS,0.16441717791411042,"In this section, we empirically verify that training with DAC learns more efﬁciently than empirical
risk minimization on an augmented dataset. The dataset is derived from CIFAR-100, where we
randomly select 10,000 labeled data as the training set (i.e., 100 labeled samples per class). During
the training time, given a training batch, we generate augmentations by RandAugment (Cubuk et al.,
2020). We set the number of augmentations per sample to 7 unless otherwise mentioned."
EXPERIMENTS,0.1656441717791411,"The experiments focus on comparisons of 1) training with consistency regularization, and 2) empir-
ical risk minimization on the augmented dataset (DA-ERM). In particular, we use the same network
architecture (a WideResNet-28-2 (Zagoruyko & Komodakis, 2016)) and the same training settings
(e.g., optimizer, learning rate schedule, etc) for both methods. We defer the detailed experiment
settings to Appendix D. Our test set is the standard CIFAR-100 test set, and we report the average
and standard deviation of the testing accuracy of 5 independent runs. The consistency regularizer is
implemented as the l2 distance of the model’s predictions on the original and augmented samples."
EXPERIMENTS,0.1668711656441718,"Efﬁcacy of DAC regularization. We ﬁrst show that the DAC regularization learns more efﬁciently
than ERM on the augmented dataset. The results are listed in Table 1. Notice that with proper choice
of λ (i.e., the multiplicative coefﬁcient before the DAC regularization, see Equation (1)), training
with DAC regularization signiﬁcantly improves over DA-ERM."
EXPERIMENTS,0.16809815950920245,"DA-ERM
DAC Regularization
λ = 0
λ = 1
λ = 5
λ = 10
λ = 20
69.40 ± 0.05
62.82 ± 0.21
68.63 ± 0.11
70.56 ± 0.07
70.52 ± 0.14
68.65 ± 0.27"
EXPERIMENTS,0.16932515337423312,Table 1: Testing accuracy of ERM and DAC regularization with different λ’s (regularization coeff.).
EXPERIMENTS,0.1705521472392638,"DAC regularization helps more when data is scarce. Our theoretical results suggest that the
DAC regularization brings more beneﬁts when the training data is scarce. The data scarcity can be
interpreted in two ways:"
EXPERIMENTS,0.17177914110429449,"(1) Labeled samples are scarce. We conduct experiments with different numbers of labeled sam-
ples, ranging from 1,000 (i.e., 10 images per class) to 20,000 samples (i.e., 200 images per class).
We generate 3 augmentations for each of the samples during the training time, and the results are
presented in Table 2. Notice that the DAC regularization gives a bigger improvement over DA-ERM
when the labeled samples are scarce. This matches the intuition that when there are sufﬁcient train-
ing samples, data augmentation is less necessary. Therefore, the difference between different ways
of utilizing the augmented samples becomes diminishing."
EXPERIMENTS,0.17300613496932515,"Number of Labeled Data
1000
10000
20000
DA-ERM
31.11 ± 0.30
68.89 ± 0.07
76.79 ± 0.13
DAC (λ = 10)
33.59 ± 0.41
70.71 ± 0.10
76.86 ± 0.16"
EXPERIMENTS,0.17423312883435582,Table 2: Testing accuracy of ERM and DAC regularization with different numbers of labeled data.
EXPERIMENTS,0.1754601226993865,"(2) Augmented samples are scarce. While keeping the number of labeled samples to be 10,000,
we evaluate the performance of the DAC regularization and DA-ERM with different numbers of
augmentations. The number of augmentations for each training sample ranges from 1 to 15, and the
results are listed in Table 3. The DAC regularization offers a more signiﬁcant improvement when
the number of augmentations is small. This clearly demonstrates that the DAC regularization learns
more efﬁciently from the limited number of augmentations."
EXPERIMENTS,0.17668711656441718,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.17791411042944785,"Number of Augmentations
1
3
7
15
DA-ERM
67.92 ± 0.08
69.04 ± 0.05
69.25 ± 0.16
69.30 ± 0.11
DAC (λ = 10)
70.06 ± 0.08
70.77 ± 0.20
70.74 ± 0.11
70.31 ± 0.12"
EXPERIMENTS,0.17914110429447852,Table 3: Testing accuracy of ERM and DAC regularization with different numbers of augmentations.
EXPERIMENTS,0.18036809815950922,"Original
Aug Strength = 1"
EXPERIMENTS,0.18159509202453988,"Aug Strength = 5
Aug Strength = 10"
EXPERIMENTS,0.18282208588957055,"Figure 3: Examples of different aug-
mentation strengths."
EXPERIMENTS,0.18404907975460122,"Proper augmentation brings good performance. To
achieve good performance, it is important to have proper
data augmentation - it needs to be strong such that it well
perturbs the input features, but it should also leave the
label distribution unchanged. Here we experiment with
different augmentation strengths, which is the number
of different random transformations (e.g., random crop-
ping, ﬂipping, etc.) applied to the training samples se-
quentially.
More transformations imply stronger aug-
mentations. The number of transformations ranges from
1 to 10, and the results are listed in Table 4. We see
that both DA-ERM and the DAC regularization beneﬁts
from a proper augmentation. When the augmentation is
too strong (e.g., Augmentation Strength 10, as shown in
Figure 3), the DAC regularization gives a worse perfor-
mance. It might be explained by falsely enforcing DAC
regularization where the labels of the augmented samples
have changed."
EXPERIMENTS,0.18527607361963191,"Augmentation Strength
1
2
5
10
DA-ERM
68.56 ± 0.12
69.32 ± 0.11
69.97 ± 0.14
69.66 ± 0.16
DAC (λ = 10)
70.66 ± 0.14
70.65 ± 0.07
70.01 ± 0.10
68.95 ± 0.27"
EXPERIMENTS,0.18650306748466258,Table 4: Testing accuracy of EMR and DAC regularization with various augmentation strengths.
EXPERIMENTS,0.18773006134969325,"Number of Unlabeled Data
5000
10000
20000
FixMatch
67.74
69.23
70.76
FixMatch + DAC (λ = 1)
71.24
72.7
74.04"
EXPERIMENTS,0.18895705521472392,"Table 5: DAC regularization helps FixMatch when the un-
labeled data is scarce."
EXPERIMENTS,0.1901840490797546,"Combining with a semi-supervised
learning
algorithm.
Here
we
show that the DAC regularization
can be easily extended to the semi-
supervised learning setting.
We
take the previously established semi-
supervised
learning
method
Fix-
Match (Sohn et al., 2020) as the baseline and adapt the FixMatch by combining it with the DAC
regularization. Namely, besides using FixMatch to learn from the unlabeled data, we additionally
generate augmentations for the labeled samples and apply the DAC regularization. In particular,
we focus on the data-scarce regime by only keeping 10,000 labeled samples and at most 20,000
unlabeled samples. Results are listed in Table 5. We see that the DAC regularization also improves
the performance of FixMatch when the unlabeled samples are scarce. This again demonstrates the
efﬁciency of learning with DAC regularization."
CONCLUSION,0.19141104294478528,"6
CONCLUSION"
CONCLUSION,0.19263803680981595,"We present a simple new theoretical framework for understanding the statistical efﬁciency of consis-
tency regularization with limited data augmentations. In particular, our proposed framework gives
a generalization bound for consistency regularization in general cases. We also provide instantia-
tions for linear regression, logistic regression, two-layer neural networks, and expansion-based data
augmentations. When specialized to linear regression/logistic regression, it shows that consistency
regularization yields a strictly smaller generalization error than ERM with augmented data. We also
provide apples-to-apples empirical comparisons between augmented ERM and consistency regular-
ization. These together demonstrate the superior efﬁcacy of consistency regularization."
CONCLUSION,0.19386503067484662,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.1950920245398773,"Reproducibility Statement
Our main theoretical results, including problem setup and (simpliﬁed)
theorem statements are in Sections 3 and 4. The formal theorem statements along with the proofs
can be found in Appendices A and B. We present empirical results in Section 5, and the detailed
experiment setup is in Appendix D. We also submit our experiment code in the supplementary
material."
REFERENCES,0.19631901840490798,REFERENCES
REFERENCES,0.19754601226993865,"Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in
neural information processing systems, 27:3365–3373, 2014."
REFERENCES,0.19877300613496932,"Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3(null):463–482, March 2003. ISSN 1532-4435."
REFERENCES,0.2,"Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invariances in
neural networks. arXiv preprint arXiv:2010.11882, 2020."
REFERENCES,0.20122699386503068,"David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch:
A uniﬁed approach to semi-supervised learning and domain adaptation.
arXiv preprint
arXiv:2106.04732, 2021."
REFERENCES,0.20245398773006135,"Tianle Cai, Ruiqi Gao, Jason D. Lee, and Qi Lei. A theory of label propagation for subpopulation
shift, 2021."
REFERENCES,0.20368098159509201,"Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmenta-
tion. Journal of Machine Learning Research, 21(245):1–71, 2020a."
REFERENCES,0.2049079754601227,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020b."
REFERENCES,0.20613496932515338,"Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pp. 2990–2999. PMLR, 2016."
REFERENCES,0.20736196319018405,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018."
REFERENCES,0.2085889570552147,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 113–123, 2019."
REFERENCES,0.2098159509202454,"Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702–703, 2020."
REFERENCES,0.21104294478527608,"Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher R´e. A kernel
theory of modern data augmentation. In International Conference on Machine Learning, pp.
1528–1537. PMLR, 2019."
REFERENCES,0.21226993865030674,"Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017."
REFERENCES,0.2134969325153374,"Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning
the representation, provably, 2020."
REFERENCES,0.2147239263803681,"Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adapta-
tion. arXiv preprint arXiv:1706.05208, 2017."
REFERENCES,0.21595092024539878,"Siddhant Garg and Yingyu Liang. Functional regularization for representation learning: A uniﬁed
theoretical perspective. arXiv preprint arXiv:2008.02447, 2020."
REFERENCES,0.21717791411042944,"Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised
deep learning with spectral contrastive loss. arXiv preprint arXiv:2106.04156, 2021."
REFERENCES,0.2184049079754601,Under review as a conference paper at ICLR 2022
REFERENCES,0.2196319018404908,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.22085889570552147,"Zhuoxun He, Lingxi Xie, Xin Chen, Ya Zhang, Yanfeng Wang, and Qi Tian. Data augmentation
revisited: Rethinking the distribution gap between clean and augmented data. arXiv preprint
arXiv:1909.09148, 2019."
REFERENCES,0.22208588957055214,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.2233128834355828,"Michael Kuchnik and Virginia Smith. Efﬁcient augmentation via data subsampling. arXiv preprint
arXiv:1810.05222, 2018."
REFERENCES,0.2245398773006135,"Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint
arXiv:1610.02242, 2016."
REFERENCES,0.22576687116564417,"Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013."
REFERENCES,0.22699386503067484,"Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019."
REFERENCES,0.2282208588957055,"Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment.
Advances in Neural Information Processing Systems, 32:6665–6675, 2019."
REFERENCES,0.2294478527607362,"Clare Lyle, Marta Kwiatkowksa, and Yarin Gal. An analysis of the effect of invariance on gen-
eralization in neural networks. In International conference on machine learning Workshop on
Understanding and Improving Generalization in Deep Learning, 2019."
REFERENCES,0.23067484662576687,"Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector ﬁeld
networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5048–
5057, 2017."
REFERENCES,0.23190184049079754,"Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. arXiv preprint arXiv:2102.13219, 2021."
REFERENCES,0.2331288343558282,"Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does
data augmentation lead to positive margin? In International Conference on Machine Learning,
pp. 5321–5330. PMLR, 2019."
REFERENCES,0.2343558282208589,"Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generaliza-
tion. arXiv preprint arXiv:1911.08731, 2019."
REFERENCES,0.23558282208588957,"Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. Advances in neural information
processing systems, 29:1163–1171, 2016."
REFERENCES,0.23680981595092024,"Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):1–48, 2019."
REFERENCES,0.23803680981595093,"Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance
in pattern recognition—tangent distance and tangent propagation. In Neural networks: tricks of
the trade, pp. 239–274. Springer, 1998."
REFERENCES,0.2392638036809816,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.24049079754601227,"Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning
with consistency and conﬁdence. arXiv preprint arXiv:2001.07685, 2020."
REFERENCES,0.24171779141104294,Under review as a conference paper at ICLR 2022
REFERENCES,0.24294478527607363,"Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders. Scale-equivariant steerable networks. arXiv
preprint arXiv:1910.11093, 2019."
REFERENCES,0.2441717791411043,"Christopher Tensmeyer and Tony Martinez. Improving invariance and equivariance properties of
convolutional neural networks. 2016."
REFERENCES,0.24539877300613497,"Julius von K¨ugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Sch¨olkopf, Michel
Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably
isolates content from style. arXiv preprint arXiv:2106.04619, 2021."
REFERENCES,0.24662576687116564,"Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/
9781108627771."
REFERENCES,0.24785276073619633,"Colin Wei and Tengyu Ma. Improved sample complexities for deep networks and robust classiﬁca-
tion via an all-layer margin, 2021."
REFERENCES,0.249079754601227,"Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training with
deep networks on unlabeled data, 2021."
REFERENCES,0.25030674846625767,"Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. arXiv preprint arXiv:2105.15134, 2021."
REFERENCES,0.25153374233128833,"Daniel E Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. arXiv preprint
arXiv:1905.11697, 2019."
REFERENCES,0.252760736196319,"Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5028–5037, 2017."
REFERENCES,0.25398773006134967,"Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher R´e. On the generalization effects of
linear transformations in data augmentation. In International Conference on Machine Learning,
pp. 10410–10420. PMLR, 2020."
REFERENCES,0.2552147239263804,"Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student
improves imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10687–10698, 2020."
REFERENCES,0.25644171779141106,"Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceed-
ings of the IEEE/CVF International Conference on Computer Vision, pp. 6023–6032, 2019."
REFERENCES,0.25766871165644173,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.2588957055214724,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017."
REFERENCES,0.26012269938650306,"Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 519–528, 2017."
REFERENCES,0.26134969325153373,Under review as a conference paper at ICLR 2022
REFERENCES,0.2625766871165644,"A
PROOFS FOR BOUNDED LOSS AND LINEAR REGRESSION"
REFERENCES,0.26380368098159507,"A.1
PROOF FOR DAC ON LINEAR REGRESSION"
REFERENCES,0.2650306748466258,"For ﬁxed e
A(X), we recall that daug = rank

e
A(X) −f
MX

since there is no randomness in e
A, X"
REFERENCES,0.26625766871165646,"in ﬁx design setting. Assuming that e
A(X) admits full column rank, we have the following theorem
on the excess risk of DAC and ERM:
Theorem 6 (Formal restatement of Theorem 2 on linear regression.). Learning with DAC regu-
larization, we have E
h
L(bθdac) −L(θ∗)
i
=
(d−daug)σ2"
REFERENCES,0.2674846625766871,"N
, while learning with ERM directly on the"
REFERENCES,0.2687116564417178,"augmented dataset, we have E
h
L(bθda−erm) −L(θ∗)
i
= (d−daug+d′)σ2"
REFERENCES,0.26993865030674846,"N
. d′ is deﬁned as"
REFERENCES,0.27116564417177913,"d′ ≜
tr

f
M⊤
H e
A(X) −PS

f
M
"
REFERENCES,0.2723926380368098,"1 + α
,"
REFERENCES,0.27361963190184047,"where d′
∈
[0, daug] with H e
A(X)
=
e
A(X)

e
A(X)⊤e
A(X)
−1 e
A(X)⊤
and PS
∈"
REFERENCES,0.2748466257668712,"R(α+1)N×(α+1)N
is
the
projection
matrix
to
S,
and
S
is
deﬁned
to
be
S
≜
n
f
MXθ | ∀θ ∈Rd, s.t.

e
A(X) −f
MX

θ = 0
o
."
REFERENCES,0.27607361963190186,"Proof. With L(θ) ≜
1
(1+α)N
 e
A(X)θ −e
A(X)θ∗
2"
REFERENCES,0.2773006134969325,"2, the excess risk of ERM on the augmented
training set satisﬁes that:"
REFERENCES,0.2785276073619632,"E
h
L(bθda−erm)
i
=
1
(1 + α)N E
 e
A(X)bθda−erm −e
A(X)θ∗
2 2 "
REFERENCES,0.27975460122699386,"=
1
(1 + α)N E
 e
A(X)( e
A(X)⊤e
A(X))−1 e
A(X)⊤( e
A(X)θ∗+ f
Mϵ) −e
A(X)θ∗
2 2 "
REFERENCES,0.2809815950920245,"=
1
(1 + α)N E
H e
A(X) e
A(X)θ∗+ H e
A(X)f
Mϵ −e
A(X)θ∗
2 2 "
REFERENCES,0.2822085889570552,"=
1
(1 + α)N E
H e
A(X)f
Mϵ

2 2 "
REFERENCES,0.28343558282208586,"=
1
(1 + α)N E
h
tr(ϵ⊤f
M⊤H e
A(X)f
Mϵ)
i =
σ2"
REFERENCES,0.2846625766871166,"(1 + α)N tr

f
M⊤H e
A(X)f
M

."
REFERENCES,0.28588957055214725,"Let C e
A(X) and Cf
M denote the column space of e
A(X) and f
M, respectively. Notice that S is a"
REFERENCES,0.2871165644171779,"subspace of both C e
A(X) and Cf
M. Observing that daug = rank

e
A(X) −f
MX

= rank (PS), we
have"
REFERENCES,0.2883435582822086,"E
h
L(bθda−erm)
i
=
σ2"
REFERENCES,0.28957055214723926,"(1 + α)N E
h
tr(f
M⊤H e
A(X)f
M)
i =
σ2"
REFERENCES,0.2907975460122699,"(1 + α)N E
h
tr(f
M⊤PSf
M)
i
+
σ2"
REFERENCES,0.2920245398773006,"(1 + α)N E
h
tr(f
M⊤(H e
A(X) −PS)f
M)
i =
σ2"
REFERENCES,0.29325153374233126,"(1 + α)N E
h
tr(f
M⊤PSf
M)
i
+ σ2 N · E"
REFERENCES,0.294478527607362,"""
tr(f
M⊤(H e
A(X) −PS)f
M) 1 + α #"
REFERENCES,0.29570552147239265,"By the data augmentation consistency constraint, we are essentially solving the linear regression
on the (d −daug)-dimensional space
n
θ | ( e
A(X) −f
MX)θ = 0
o
. The rest of proof is identical to
standard regression analysis, with features ﬁrst projected to S:"
REFERENCES,0.2969325153374233,Under review as a conference paper at ICLR 2022
REFERENCES,0.298159509202454,"E
h
L(bθdac)
i
=
1
(1 + α)N E
 e
A(X)bθdac −e
A(X)θ∗
2 2 "
REFERENCES,0.29938650306748466,"=
1
(1 + α)N E
 e
A(X)( e
A(X)⊤e
A(X))−1 e
A(X)⊤PS( e
A(X)θ∗+ f
Mϵ) −e
A(X)θ∗
2 2 "
REFERENCES,0.3006134969325153,"=
1
(1 + α)N E
H e
A(X)PS e
A(X)θ∗+ H e
A(X)PSf
Mϵ −e
A(X)θ∗
2 2 "
REFERENCES,0.301840490797546,"
∵
e
A(X)θ∗∈S, and H e
A(X)PS = PS since S ⊆C e
A(X)
"
REFERENCES,0.3030674846625767,"=
1
(1 + α)N E
PSf
Mϵ

2 2  =
σ2"
REFERENCES,0.3042944785276074,"(1 + α)N E
h
tr(f
M⊤PSf
M)
i"
REFERENCES,0.30552147239263805,"= (d −daug)σ2 N
. ■"
REFERENCES,0.3067484662576687,"B
APPLICATIONS OF LEARNING WITH DAC"
REFERENCES,0.3079754601226994,"To instantiate our general result on the DAC regularization, we start with a concrete application of
Proposition 8 on the logistic regression problem with bounded loss (Section 4.1, Appendix B.1),
followed by an extension to the unbounded square loss (Section 4.2, Appendix B.2). Subsequently,
we discuss an alternative notion of data augmentations based on expansion for the classiﬁcation
problems (Section 4.3, Appendix B.3). Finally, we present a supplementary example for the domain
adaptation in linear regression (Appendix B.4)."
REFERENCES,0.30920245398773005,"Before diving into the concrete applications, we recall the general setting, and introduce some addi-
tional notations used throughout the formal analysis."
REFERENCES,0.3104294478527607,"DAC regularization with unlabeled data.
Since the DAC regularization leverages only on the
unlabeled observable features but not their labels, the unlabeled training set in the DAC regularizer
and the labeled training set in the ﬁrst term of Equation (1) for label learning can be considered
separately. For clariﬁcation, in the formal analysis, we distinguish the labeled training set (X, y) ∈
X n × Yn from the unlabeled samples Xu ∈X N (possibly with different sizes N ≥n), while both
sample sets are drawn i.i.d. from P ∗. Particularly, in the supervised learning setting, the DAC
regularization is based on the observables from the labeled training set such that Xu = X (N = n).
While in the semi-supervised learning setting, Xu can be N unlabeled observations drawn i.i.d.
from the marginal distribution P ∗(x)."
REFERENCES,0.3116564417177914,"For the DAC regularization, we denote the augmentation of the unlabeled samples (excluding the
original samples Xu, in contrast to the augmented sample e
A(X) in the main text) as
b
A(Xu) =

xu
1,1; · · · ; xu
N,1; · · · ; xu
1,α; · · · ; xu
N,α

∈X αN,"
REFERENCES,0.3128834355828221,"where for each i ∈[N],

xu
i,j"
REFERENCES,0.3141104294478528,"j∈[α] is a set of α augmentations generated from xu
i , and let M ∈"
REFERENCES,0.31533742331288345,"RαN×N be the vertical stack of α N × N identity matrices. Then analogously, with the unlabeled
training set Xu, we can quantify the strength of the data augmentations b
A with"
REFERENCES,0.3165644171779141,"bdaug ≜rank

b
A(Xu) −MXu
= rank

e
A(Xu) −f
MXu
,"
REFERENCES,0.3177914110429448,"such that 0 ≤bdaug ≤min (d, αN) can be intuitively interpreted as the number of dimensions in
the span of the unlabeled samples, Row(Xu), perturbed by b
A. Moreover, we denote N as the
(d −bdaug)-dimensional null space of b
A(Xu) −MXu, and PN as the projection onto N such that"
REFERENCES,0.31901840490797545,"PN ≜Id −

b
A(Xu) −MXu† 
b
A(Xu) −MXu
."
REFERENCES,0.3202453987730061,Under review as a conference paper at ICLR 2022
REFERENCES,0.3214723926380368,"Correspondingly, let N ⊥be the orthogonal complement of N, and P⊥
N be the projection onto N ⊥"
REFERENCES,0.3226993865030675,"such that P⊥
N = Id −PN . Then by recalling Deﬁnition 2, we have the following:
Proposition 7. For any δ ∈(0, 1), with probability at least 1 −δ,"
REFERENCES,0.3239263803680982,"rank
 
P⊥
N

= dim
 
N ⊥
= bdaug ≥daug(δ),"
REFERENCES,0.32515337423312884,rank (PN ) = dim (N) = d −bdaug ≤d −daug(δ).
REFERENCES,0.3263803680981595,"Furthermore, for daug ≜daug (1/n), with high probability, bdaug ≥daug and d −bdaug ≤d −daug."
REFERENCES,0.3276073619631902,"In terms of (X, y) and Xu, the regularization formulation in Equation (1) can be restated as,"
REFERENCES,0.32883435582822085,"argmin
h∈H n
X"
REFERENCES,0.3300613496932515,"i=1
l(h(xi), yi) + λ α
X j=1 N
X"
REFERENCES,0.3312883435582822,"i=1
ϱ
 
φh(xu
i ), φh(xu
i,j)
"
REFERENCES,0.3325153374233129,"|
{z
}
DAC regularizer ,"
REFERENCES,0.3337423312883436,"and with the notation of b
A, the corresponding DAC operator is given by"
REFERENCES,0.33496932515337424,"T dac
b
A,Xu(H) ≜

h ∈H | φh(xu
i ) = φh(xu
i,j), ∀i ∈[N], j ∈[α]
	
,"
REFERENCES,0.3361963190184049,"where T dac
b
A,Xu and T dac
e
A,Xu are equivalent by construction."
REFERENCES,0.3374233128834356,"Marginal distribution and training set.
Here we formally state the common regularity conditions
for the logistic regression (Section 4.1) and the two-layer neural network (Section 4.2) examples in
Assumption 1 and Assumption 2, where we assume X ⊆Rd.
Assumption 1 (Regularity of marginal distribution). Let x ∼P ∗(x) be zero-mean E[x] = 0, with
the covairance E[xx⊤] = Σx ≻0. We assume that (Σ−1/2
x
x) is ρ2-subgaussian 1, and there exist
constants C ≥c = Θ(1) such that cId ≼Σx ≼CId."
REFERENCES,0.33865030674846625,"Assumption 2 (Sufﬁcient labeled data). For (X, y) ∈X n × Yn, with respect to Xu and b
A (Xu)"
REFERENCES,0.3398773006134969,"that characterize bdaug, we assume n ≫ρ4 
d −bdaug

."
REFERENCES,0.3411042944785276,"For a B-bounded loss function l : Y × Y →R (i.e., 0 ≤l ≤B) such that for all y ∈Y,
l(·, y) : Y →R is Cl-Lipschitz, we have the following:
Proposition 8 (Formal restatement of Proposition 1 on DAC with bounded loss). Learning with
DAC regularization (Equation (2)), for a ﬁxed T dac
b
A,Xu(H) and any δ ∈(0, 1), with probability at
least 1 −δ, we have"
REFERENCES,0.3423312883435583,"L(bhdac) −L(h∗) ≤4Cl · Rn(T dac
b
A,Xu(H)) + r"
REFERENCES,0.34355828220858897,"2B2 log(2/δ) n
,"
REFERENCES,0.34478527607361964,"where Rn(T dac
b
A,Xu(H)) represents the Rademacher complexity of T dac
b
A,Xu(H)."
REFERENCES,0.3460122699386503,Proof of Proposition 8. We ﬁrst decompose the expected excess risk as
REFERENCES,0.347239263803681,"L(bhdac) −L(h∗) =

L(bhdac) −bL(bhdac)

+

bL(bhdac) −bL(h∗)

+

bL(h∗) −L(h∗)

,"
REFERENCES,0.34846625766871164,"where bL(bhdac) −bL(h∗) ≤0 by the basic inequality, and as a consequence, for a ﬁxed Ho =
T dac
b
A,Xu(H),"
REFERENCES,0.3496932515337423,"L(bhdac) −L(h∗) ≤2
sup
h∈T dac
b
A,Xu(H)"
REFERENCES,0.350920245398773,"L(h) −bL(h)
 = 2 sup
h∈Ho"
REFERENCES,0.3521472392638037,"L(h) −bL(h)
 ."
REFERENCES,0.35337423312883437,"We denote g+(X, y) = suph∈Ho : L(h) −bL(h) and g−(X, y) = suph∈Ho : −L(h) + bL(h). Then,"
REFERENCES,0.35460122699386504,"P
h
L(bhdac) −L(h∗) ≥ϵ
i
≤P
h
g+(X, y) ≥ϵ 2"
REFERENCES,0.3558282208588957,"i
+ P
h
g−(X, y) ≥ϵ 2 i
."
REFERENCES,0.3570552147239264,"1A random vector v ∈Rd is ρ2-subgaussian if for any unit vector u ∈Sd−1, u⊤v is ρ2-subgaussian,
E

exp(s · u⊤v)

≤exp
 
s2ρ2/2

."
REFERENCES,0.35828220858895704,Under review as a conference paper at ICLR 2022
REFERENCES,0.3595092024539877,"We will derive a tail bound for g+(X, y) with the standard inequalities and symmetrization argu-
ment Wainwright (2019); Bartlett & Mendelson (2003), while the analogous statement holds for
g−(X, y)."
REFERENCES,0.36073619631901843,"Let (X(1), y(1)) be a sample set generated by replacing an arbitrary observation in (X, y) with any
(x, y) ∈X × Y. Since l is B-bounded, we have
g+(X, y) −g+(X(1), y(1))
 ≤B/n. Leveraging
the McDiarmid’s inequality Bartlett & Mendelson (2003),"
REFERENCES,0.3619631901840491,"P

g+(X, y) ≥E[g+(X, y)] + t

≤exp

−2nt2 B2 
."
REFERENCES,0.36319018404907977,"For an arbitrary sample set (X, y), let bL(X,y) (h) = 1"
REFERENCES,0.36441717791411044,"n
Pn
i=1 l (h(xi), yi) be the empirical risk of
h with respect to (X, y). Then, by a classical symmetrization argument (e.g., proof of Wainwright
(2019) Theorem 4.10), we can bound the expectation: for an independent sample set (X′, y′) ∈
X n × Yn drawn i.i.d. from P ∗,"
REFERENCES,0.3656441717791411,"E

g+(X, y)

=E(X,y)"
REFERENCES,0.36687116564417177,"
sup
h∈Ho
L(h) −bL(X,y)(h)
"
REFERENCES,0.36809815950920244,"=E(X,y)"
REFERENCES,0.3693251533742331,"
sup
h∈Ho
E(X′,y′)
h
bL(X′,y′)(h)
i
−bL(X,y)(h)
"
REFERENCES,0.37055214723926383,"=E(X,y)"
REFERENCES,0.3717791411042945,"
sup
h∈Ho
E(X′,y′)
h
bL(X′,y′)(h) −bL(X,y)(h)
 (X, y)
i"
REFERENCES,0.37300613496932516,"≤E(X,y)"
REFERENCES,0.37423312883435583,"
E(X′,y′)"
REFERENCES,0.3754601226993865,"
sup
h∈Ho
bL(X′,y′)(h) −bL(X,y)(h)
 (X, y)
"
REFERENCES,0.37668711656441717,(Law of iterated conditional expectation)
REFERENCES,0.37791411042944784,"=E(X,y,X′,y′)"
REFERENCES,0.3791411042944785,"
sup
h∈Ho
bL(X′,y′)(h) −bL(X,y)(h)
"
REFERENCES,0.3803680981595092,"Since (X, y) and (X′, y′) are independent and identically distributed, we can introduce i.i.d.
Rademacher random variables r = {ri ∈{−1, +1} | i ∈[n]} (independent of both (X, y) and
(X′, y′)) such that"
REFERENCES,0.3815950920245399,"E

g+(X, y)

≤E(X,y,X′,y′,r) """
REFERENCES,0.38282208588957056,"sup
h∈Ho"
N,0.38404907975460123,"1
n n
X"
N,0.3852760736196319,"i=1
ri · (l (h (x′
i) , y′
i) −l (h (xi) , yi)) #"
N,0.38650306748466257,"≤2 E(X,y,r) """
N,0.38773006134969323,"sup
h∈Ho"
N,0.3889570552147239,"1
n n
X"
N,0.3901840490797546,"i=1
ri · l (h (xi) , yi) #"
N,0.3914110429447853,≤2 Rn (l ◦Ho)
N,0.39263803680981596,"where l ◦Ho = {l(h(·), ·) : X × Y →R : h ∈Ho} is the loss function class.
Analogously,
E[g−(X, y)] ≤2Rn (l ◦Ho). Therefore, with probability at least 1 −δ,"
N,0.39386503067484663,L(bhdac) −L(h∗) ≤4Rn(l ◦Ho) + r
N,0.3950920245398773,2B2 log(2/δ) n
N,0.39631901840490796,"Finally, since l(·, y) is Cl-Lipschitz for all y ∈Y, by Ledoux & Talagrand (2013) Theorem 4.12, we
have Rn (l ◦Ho) ≤Cl · Rn (Ho).
■"
N,0.39754601226993863,"B.1
FORMAL RESULT ON LOGISTIC REGRESSION"
N,0.3987730061349693,"For the logistic regression problem in Section 4.1, with X =

x ∈Rd  ∥x∥2 ≤D
	
, Y = {0, 1},
training with the DAC regularization can be formulated explicitly as"
N,0.4,"bθdac = argmin
θ∈Rd
1
n n
X"
N,0.4012269938650307,"i=1
l
 
θ⊤xi, yi
"
N,0.40245398773006136,"s.t.
∥θ∥2 ≤C0,
b
A (Xu) θ = MXuθ,"
N,0.403680981595092,and yields bhdac(x) ≜x⊤bθdac.
N,0.4049079754601227,Under review as a conference paper at ICLR 2022
N,0.40613496932515336,"Theorem 9 (Formal restatement of Theorem 3 on logistic regression with DAC).
Let
E
h
1
n
Pn
i=1 ∥PN xi∥2
2
PN
i
≤C2
N for some CN > 0. Then, learning logistic regression with"
N,0.40736196319018403,"the DAC regularization h(xu
i ) = h(xu
i,j) yields that, for any δ ∈(0, 1), with probability at least
1 −δ:"
N,0.4085889570552147,"L(bhdac) −L(h∗) ≲C0CN + C0D
p"
N,0.4098159509202454,"log (1/δ)
√n
,"
N,0.4110429447852761,"where under Assumption 1 and Assumption 2, CN ≲
p"
N,0.41226993865030676,d −daug (δ/2).
N,0.4134969325153374,"Proof of Theorem 9. Notice that the logistic loss l(·, y) is 2-Lipschitz for all y ∈Y, as ∂l(z,y)"
N,0.4147239263803681,"∂z
=
σ(z)−y, whose absolute value is bounded by 2. Meanwhile, with ∥θ∥2 ≤C0 and ∥x∥2 ≤D for all
θ and x, the logistic loss is also bounded: l
 
θ⊤x, y

≤log 2+C0D. Then, applying Proposition 8,
we have that for any δ ∈(0, 1), with probability at least 1 −δ/2,"
N,0.41595092024539876,"L(bhdac) −L(h∗) ≲Rn

T dac
b
A,Xu(H)

+ C0D r"
N,0.4171779141104294,"log(1/δ) n
."
N,0.41840490797546015,"It left to bound the Rademacher complexity of T dac
b
A,Xu(H). First notice that the sigmoid function σ(·)
is a bijective function, the data augmentation consistency constraints are equivalent to

b
A(Xu) −MXu
θ = 0."
N,0.4196319018404908,"Thus, we have
T dac
b
A,Xu(H) =

h(x) = θ⊤x
 θ ∈N, ∥θ∥2 ≤C0
	
,"
N,0.4208588957055215,"with which we are ready to bound the Rademacher complexity of T dac
b
A,Xu(H)."
N,0.42208588957055215,"For the emprical Rademacher complexity, we have"
N,0.4233128834355828,"bRX

T dac
b
A,Xu(H)

= 1"
N,0.4245398773006135,nEϵi∼Rad( 1 2 ) 
N,0.42576687116564416,"
sup
θ∈T dac
b
A,Xu(H) n
X"
N,0.4269938650306748,"i=1
ϵiθ⊤xi   = 1"
N,0.42822085889570555,"nEϵi∼Rad( 1 2 ) """
N,0.4294478527607362,"sup
θ∈H n
X"
N,0.4306748466257669,"i=1
ϵiθ⊤PN xi # =C0"
N,0.43190184049079755,"n Eϵi∼Rad( 1 2 ) "" n
X"
N,0.4331288343558282,"i=1
ϵiPN xi 2 # ≤C0 n"
N,0.4343558282208589,"v
u
u
tEϵi∼Rad( 1 2 ) n
X"
N,0.43558282208588955,"i=1
∥ϵiPN xi∥2
2 =C0 n"
N,0.4368098159509202,"v
u
u
t n
X"
N,0.43803680981595094,"i=1
∥PN xi∥2
2"
N,0.4392638036809816,"= C0
√n s"
N,0.4404907975460123,"tr
 1"
N,0.44171779141104295,"nPN X⊤XPN 
."
N,0.4429447852760736,"Converting this to the population Rademacher complexity, we take expectation over X, while con-
ditioned on PN , and recall Jensen’s inequality,"
N,0.4441717791411043,"Rn

T dac
b
A,Xu(H)

≤C0
√nE ""s"
N,0.44539877300613495,"tr
 1"
N,0.4466257668711656,nPN X⊤XPN
N,0.44785276073619634,  PN #
N,0.449079754601227,"≤C0
√n s"
N,0.4503067484662577,"E

tr
 1"
N,0.45153374233128835,nPN X⊤XPN
N,0.452760736196319,  PN 
N,0.4539877300613497,"=C0CN
√n ."
N,0.45521472392638035,Under review as a conference paper at ICLR 2022
N,0.456441717791411,"Leveraging Lemma 5, we have that under Assumption 1 and Assumption 2, conditioned on PN ,
with high probability over X ∼P ∗,

1
nPN X⊤XPN"
N,0.45766871165644174,"2
≤1.1C ≲1"
N,0.4588957055214724,Then we can ﬁnd some CN > 0 with 1
N,0.4601226993865031,"n
Pn
i=1 ∥PN xi∥2
2 ≤C2
N such that,"
N,0.46134969325153374,"C2
N ≤

d −bdaug

·

1
nPN X⊤XPN"
N,0.4625766871165644,"2
≲d −bdaug."
N,0.4638036809815951,"By Proposition 7, we have CN ≤
p"
N,0.46503067484662575,d −daug (δ/2) with probability at least 1 −δ/2.
N,0.4662576687116564,"To obtain Theorem 3, we leverage the union bound and suppress the constants such that, with prob-
ability at least 1 −δ,"
N,0.46748466257668714,L(bhdac) −L(h∗) ≲ r
N,0.4687116564417178,"d −daug (δ/2) n
+ r"
N,0.4699386503067485,log(1/δ)
N,0.47116564417177914,"n
.
(4)"
N,0.4723926380368098,"Finally, taking δ = 2/n and applying the Jensen’s inequality to the two terms in Equation (4)
complete the proof.
■"
N,0.4736196319018405,"B.2
FORMAL RESULT ON TWO-LAYER NEURAL NETWORK"
N,0.47484662576687114,"In the two-layer neural network regression setting described in Section 4.2, training with the DAC
regularization can be formulated explicitly as"
N,0.47607361963190187,"bBdac, bwdac =
argmin
B∈Rd×q,w∈Rq
1
2n"
N,0.47730061349693254,"y −(XB)+ w
2
2"
N,0.4785276073619632,"s.t.
B = [b1 . . . bk . . . bq] , bk ∈Sd−1 ∀k ∈[q],
∥w∥1 ≤Cw

b
A (Xu) B
"
N,0.47975460122699387,"+ = (MXuB)+ ,"
N,0.48098159509202454,"and yields bhdac(x) ≜(x⊤bBdac)+ bwdac.
Theorem 10 (Formal restatement of Theorem 4 on two-layer neural network with DAC). Sampling
from P ∗that follows Assumption 1, let the augmented training set b
A (Xu) satisfy (a) αN ≥4bdaug,
(b) b
A(Xu) −MXu admits an absolutely continuous distribution over N ⊥, and P ∗fulﬁlls (c)"
N,0.4822085889570552,"E
h
1
n
Pn
i=1 ∥PN xi∥2
2
PN
i
≤C2
N for some CN > 0. Then, learning the two-layer ReLU network"
N,0.4834355828220859,"with the DAC regularization on

(xu
i )⊤B
"
N,0.48466257668711654,"+ =
 
xu
i,j
⊤B
"
N,0.48588957055214727,"+ gives that, with high probability,"
N,0.48711656441717793,"L

bhdac
−L (h∗) ≲σCwCN
√n
,"
N,0.4883435582822086,"where under Assumption 1 and Assumption 2, CN ≲
p"
N,0.48957055214723927,d −daug (δ) with probability at least 1 −δ.
N,0.49079754601226994,"Lemma 1. Under the assumptions in Theorem 10, every size-bdaug subset of rows in b
A(Xu)−MXu
is linearly independent almost surely."
N,0.4920245398773006,"Proof of Lemma 1. Since αN > bdaug, it is sufﬁcient to show that a random matrix with an absolutely
continuous distribution is totally invertible 2 almost surely."
N,0.49325153374233127,"It is known that for any dimension m
∈
N, an m × m square matrix S is singular if
det(S) = 0 where entries of S lie within the roots of the polynomial equation speciﬁed by the
determinant. Therefore, the set of all singular matrices in Rm×m has Lebesgue measure zero,
λ ({S ∈Rm×m | det(S) = 0}) = 0. Then, for an absolutely continuous probability measure µ
with respect to λ, we also have
Pµ

S ∈Rm×m is singular

= µ
 
S ∈Rm×m  det(S) = 0
	
= 0.
Since a general matrix R contains only ﬁnite number of submatrices, when R is drawn from an
absolutely continuous distribution, by the union bound, P [R cotains a singular submatrix] = 0.
That is, R is totally invertible almost surely.
■"
N,0.49447852760736194,2A matrix is totally invertible if all its square submatrices are invertible.
N,0.49570552147239266,Under review as a conference paper at ICLR 2022
N,0.49693251533742333,"Lemma 2. Under the assumptions in Theorem 10, the hidden layer in the two-layer ReLU network
learns N, the invariant subspace under data augmentations : with high probability,

x⊤bBdac"
N,0.498159509202454,"+ =

x⊤PN bBdac"
N,0.49938650306748467,"+
∀x ∈X."
N,0.5006134969325153,"Proof of Lemma 2. We will show that for all bk = PN bk + P⊥
N bk, k ∈[q], P⊥
N bk = 0 with high
probability, which then implies that given any x ∈X, (x⊤bk)+ = (x⊤PN bk)+ for all k ∈[q]."
N,0.501840490797546,"For any k ∈[q] associated with an arbitrary ﬁxed bk ∈Sd−1, let Xu
k ≜Xu
kPN +Xu
kP⊥
N ∈X Nk be
the inclusion-wisely maximum row subset of Xu such that Xu
kbk > 0 element-wisely. Meanwhile,
we denote b
A(Xu
k) = MkXu
kPN + b
A(Xu
k)P⊥
N ∈X αNk as the augmentation of Xu
k where Mk ∈
RαNk×Nk is the vertical stack of α identity matrices with size Nk × Nk. Then the DAC constraint
implies that ( b
A(Xu
k) −MkXu
k)P⊥
N bk = 0."
N,0.5030674846625767,"With Assumption 1, for a ﬁxed bk ∈Sd−1, P[x⊤bk > 0] = 1"
N,0.5042944785276073,"2. Then, with the Chernoff bound,"
N,0.505521472392638,"P

Nk < N"
N,0.5067484662576687,"2 −t

≤e−2t2 N ,"
N,0.5079754601226993,"which implies that, Nk ≥N"
N,0.50920245398773,4 with high probability.
N,0.5104294478527608,"Leveraging the assumptions in Theorem 10, αN ≥4bdaug implies that αNk ≥bdaug. Therefore by"
N,0.5116564417177915,"Lemma 1, Row

b
A(Xu
k) −MkXu
k

= N ⊥with probability 1. Thus, ( b
A(Xu
k) −MkXu
k)P⊥
N bk ="
N,0.5128834355828221,"0 enforces that P⊥
N bk = 0.
■"
N,0.5141104294478528,"Proof of Theorem 10. Lemma 2 implies that (XbBdac)+ = (XPN bBdac)+, and therefore,"
N,0.5153374233128835,"T dac
b
A,Xu(H) =

h(x) = (x⊤B)+w | B = [b1 . . . bq], bk ∈Sd−1, (XB)+ = (XPN B)+, ∥w∥1 ≤Cw
	
."
N,0.5165644171779141,"In other words, the DAC regularization reduces the feasible set for B such that"
N,0.5177914110429448,"B ∈B ≜{B = [b1 . . . bq] | ∥bk∥= 1 ∀k ∈[q], (XB)+ = (XPN B)+} ,
∥w∥1 ≤Cw."
N,0.5190184049079755,"Meanwhile, for the square loss, the corresponding excess risk is given by"
N,0.5202453987730061,"L

bhdac
−L (h∗) = EX  1"
N,0.5214723926380368,2n
N,0.5226993865030675,"(XbBdac)+ bwdac −(XB∗)+w∗
2 2 
."
N,0.5239263803680981,"Leveraging Equation (21) and (22) in Du et al. (2020), since (B∗, w∗) is feasible under the con-
straint, by the basic inequality,
y −(XbBdac)+ bwdac
2"
N,0.5251533742331288,"2 ≤∥y −(XB∗)+w∗∥2
2 ,
(5)"
N,0.5263803680981595,"where bBdac, B∗∈B and
bwdac
1 ≤Cw."
N,0.5276073619631901,"Analogously to the Rademacher complexities for the bounded losses, for the square loss, we recall
the deﬁnitions of the Gaussian complexities of a vector-valued function class Φ = {φ : X →Rq},"
N,0.5288343558282208,"bGX(Φ) ≜
E
gik∼N(0,1) i.i.d. """
N,0.5300613496932516,"sup
φ∈Φ"
N,0.5312883435582823,"1
n n
X i=1 q
X"
N,0.5325153374233129,"k=1
gikφk(xi) #"
N,0.5337423312883436,",
Gn(Φ) ≜E
h
bGX(Φ)
i
.
(6)"
N,0.5349693251533743,"In terms of the Gaussian complexity, for a ﬁxed T dac
b
A,Xu(H), EX  1"
N,0.5361963190184049,2n
N,0.5374233128834356,"(XbBdac)+ bwdac −(XB∗)+w∗
2 2 Xu
 ≤EX"
N,0.5386503067484663,"
z⊤
 1 n"
N,0.5398773006134969,"
(XbBdac)+ bwdac −(XB∗)+w∗  Xu
"
N,0.5411042944785276,"≲σ · EX
h
bGX

T dac
b
A,Xu(H)
  Xui"
N,0.5423312883435583,Under review as a conference paper at ICLR 2022
N,0.5435582822085889,where the empirical Gaussian complexity can be upper bounded by
N,0.5447852760736196,"bGX

T dac
b
A,Xu(H)

=
E
g∼N(0,In) """
N,0.5460122699386503,"sup
B∈B,∥w∥1≤R"
N,0.5472392638036809,"1
ng⊤(XB)+w # ≤Cw n E
g"
N,0.5484662576687117,"
sup
B∈B"
N,0.5496932515337424,"(XB)⊤
+g

∞  =Cw n E
g"
N,0.550920245398773,"
sup
b∈Sd−1 g⊤(XPN b)+"
N,0.5521472392638037,"
(Lemma 6, (·)+ is 1-Lipschitz) ≤Cw n E
g"
N,0.5533742331288344,"
sup
b∈Sd−1 g⊤XPN b
 =Cw"
N,0.554601226993865,"n E
g
PN X⊤g

2
 ≤Cw n 
E
g"
N,0.5558282208588957,"hPN X⊤g
2 2 i1/2 =Cw n q"
N,0.5570552147239264,tr(PN X⊤XPN )
N,0.558282208588957,"= Cw
√n"
N,0.5595092024539877,"v
u
u
t 1 n n
X"
N,0.5607361963190184,"i=1
∥PN xi∥2
2."
N,0.561963190184049,"Taking expectation over X ∼P ∗, we have that for a ﬁxed T dac
b
A,Xu(H) (i.e., conditioned on PN ),"
N,0.5631901840490797,"Gn

T dac
b
A,Xu(H)

≤CwCN
√n
."
N,0.5644171779141104,"Leveraging Lemma 5, we have that under Assumption 1 and Assumption 2, conditioned on PN ,
with high probability over X ∼P ∗,

1
nPN X⊤XPN"
N,0.5656441717791411,"2
≤1.1C ≲1"
N,0.5668711656441717,Then we can ﬁnd some CN > 0 with 1
N,0.5680981595092025,"n
Pn
i=1 ∥PN xi∥2
2 ≤C2
N such that,"
N,0.5693251533742332,"C2
N ≤

d −bdaug

·

1
nPN X⊤XPN"
N,0.5705521472392638,"2
≲d −bdaug."
N,0.5717791411042945,"By Proposition 7, we have CN ≤
p"
N,0.5730061349693252,"d −daug (δ) with probability at least 1 −δ.
■"
N,0.5742331288343558,"B.3
FORML RESULTS ON EXPANSION-BASED DATA AUGMENTATION"
N,0.5754601226993865,"For the formal analysis of the expansion-based data augmentations, we ﬁrst introduce some helpful
notations."
N,0.5766871165644172,"With respect to an augmentation function A : X →2X fulﬁlling the conditions in Deﬁnition 4, we
deﬁne the neighborhood for an arbitrary x ∈X, as well as for any S ⊆X, such that,"
N,0.5779141104294478,"NB(x) ≜

x′ ∈X
 A(x) ∩A(x′) ̸= ∅
	
,
NB(S) ≜∪x∈SNB(x)."
N,0.5791411042944785,"Then Deﬁnition 4(b) implies that the corresponding neighborhood function NB : X →2X of A has
non-trivial expansion:"
N,0.5803680981595092,"∅⊊S ∩Xk ⊊Xk ⇒(NB(S)\S) ∩Xk ̸= ∅
∀k ∈[K], S ⊆X."
N,0.5815950920245399,"Intuitively, the non-trivial expansion guarantees that the neighborhood of any proper subset of a
class always enlarges the subset within the class.
Remark 2. The ground truth classiﬁer is invariant throughout the neighborhood: for any aug-
mentation function A : X →2X satisfying the conditions in Deﬁnition 4, the corresponding neigh-
borhood function satisﬁes that, given any x ∈X, h∗(x) = h∗(x′) for all x′ ∈NB(x)."
N,0.5828220858895705,Under review as a conference paper at ICLR 2022
N,0.5840490797546012,"Notice that the previous data augmentation strength bdaug (Deﬁnition 2) is not well deﬁned for Deﬁni-
tion 4 (since X needs not be a subset of Rd). Therefore we need a new notion of data augmentation
strength.
Remark 3. The strength of expansion-based data augmentations (Deﬁnition 4) is characterized by
the size of A(x). Intuitively, for any x ∈X, the augmentations can be stronger (i.e., there exists
x′ ∈A(x) that is more different from x) if A(x) is inclusion-wisely larger."
N,0.5852760736196319,"In addition, for an arbitrary classiﬁer h ∈H, we denote the majority label with respect to h for each
class,"
N,0.5865030674846625,"byk ≜argmax
y∈[K]
Px

h(x) = y
 x ∈Xk

∀k ∈[K],"
N,0.5877300613496933,"along with the respective class-wise local and global minority sets,"
N,0.588957055214724,"Mk ≜

x ∈Xk
 h(x) ̸= byk
	
⊊Xk
∀k ∈[K],
M ≜ K
["
N,0.5901840490797546,"k=1
Mk."
N,0.5914110429447853,"Inﬁnite unlabeled data.
As a warm-up, we simplify the setting by enforcing consistency over the
population such that learning with the DAC constraints can be expressed as,"
N,0.592638036809816,"bhdac
X
≜argmin
h∈H
bLdac
01 (h) = 1 n n
X"
N,0.5938650306748466,"i=1
1 {h (xi) ̸= h∗(xi)}"
N,0.5950920245398773,"s.t.
h(x) = h (x′)
∀x ∈X, x′ ∈A(x)."
N,0.596319018404908,"We can also formulate algorithm in terms of the DAC operator,"
N,0.5975460122699386,"bhdac
X
≜
argmin
h∈T dac
A,X (H)
bLdac
01 (h),
T dac
A,X ≜{h ∈H | h(x) = h(x′) ∀x ∈X, x′ ∈A(x)} ."
N,0.5987730061349693,"Lemma 3. Given any h ∈T dac
A,X (H), for each k ∈[K], there exists some byk ∈[K] such that
h(x) = byk for all x ∈Xk."
N,0.6,"Proof of Lemma 3. It is sufﬁcient to show that Mk = ∅for all k ∈[K]. By contradiction, suppose
Mk ̸= ∅. Then by the non-trivial expansion assumption in Deﬁnition 4, (NB(Mk)\Mk) ∩Xk ̸= ∅.
That is, there exists some x ∈(NB(Mk)\Mk) ∩Xk. But on one hand, x ∈NB(Mk) implies
that we can ﬁnd some x′ ∈Mk and x′′ ∈A (x) ∩A (x′). Therefore by the consistency constraint
h ∈T dac
A,X (H), h(x′′) = h(x) = h(x′). On the other hand, x ∈Xk\Mk implies that h(x) = byk ̸=
h(x′), and leads to a contradiction.
■"
N,0.6012269938650306,"Restating Theorem 5 with the formal notations in the appendix: with high probability,"
N,0.6024539877300613,"L01

bhdac
X

−L01 (h∗) ≤K log K + log n n
."
N,0.603680981595092,"Proof of Theorem 5. By Lemma 3, since"
N,0.6049079754601226,"T dac
A,X (H) ⊆{h ∈H | h(x) = byk ∀x ∈Xk}
where
{byk ∈[K]}k∈[K] ,"
N,0.6061349693251534,"the consistency constraint yields a ﬁnite feasible classiﬁer class with
T dac
A,X (H)
 = K! ≤KK."
N,0.6073619631901841,"Meanwhile, since A(x) ⊆{x′ ∈X | h∗(x) = h∗(x′)}, by construction, we know that T dac
A,X (H) is
realizable (i.e., h∗∈T dac
A,X (H) such that bLdac
01 (h∗) = 0). Therefore,"
N,0.6085889570552148,"P
h
L01

bhdac
X

> ϵ
i
≤P
h
∃h ∈T dac
A,X (H)
 L01 (h) > ϵ ∧bLdac
01 (h) = 0
i
,"
N,0.6098159509202454,"where for every classiﬁer h ∈T dac
A,X (H),"
N,0.6110429447852761,"P
h
L01 (h) > ϵ ∧bLdac
01 (h) = 0
i
≤(1 −ϵ)n ≤e−ϵn."
N,0.6122699386503068,Under review as a conference paper at ICLR 2022
N,0.6134969325153374,"Then by the union bound, we have"
N,0.6147239263803681,"P
h
L01

bhdac
X

> ϵ
i
≤
X"
N,0.6159509202453988,"h∈T dac
A,X (H)
P
h
L01 (h) > ϵ ∧bLdac
01 (h) = 0
i"
N,0.6171779141104294,"≤
T dac
A,X (H)
 · e−ϵn."
N,0.6184049079754601,"The proof is completed by upper bounding the above, P
h
L01

bhdac
X

> ϵ
i
≤1/n, when taking"
N,0.6196319018404908,"ϵ =
log
T dac
A,X (H)
 + log n
n
≤K log K + log n n
,"
N,0.6208588957055214,and observing that L01 (h∗) = 0. ■
N,0.6220858895705521,"Finite unlabeled data.
For the ﬁnite unlabeled data case, we start by introducing some addi-
tional concepts and notations. We concretize the classiﬁer class H with a proper function class
F ⊆

f : X →RK	
such that H =
n
h(x) ≜argmaxk∈[K] f(x)k
 f ∈F
o
. Speciﬁcally, we
adapt the existing setting in Wei et al. (2021); Cai et al. (2021), and consider F as a class of fully
connected neural networks. To constrain the feasible hypothesis class through the DAC regulariza-
tion with ﬁnite unlabeled observations Xu = [xu
1, . . . , xu
N]⊤, we leverage, from Wei & Ma (2021),
the notion of all-layer-margin, m : F × X × Y →R≥0, that measures the maximum possible
perturbation in all layers of f while maintaining the prediction y. Precisely, given any f ∈F such
that f (x) = Wpϕ (. . . ϕ (W1x) . . . ) for some activation function ϕ : R →R and parameters

Wι ∈Rdι×dι−1	p
ι=1, we can write f = f2p−1 ◦· · · ◦f1 where f2ι−1(x) = Wιx for all ι ∈[p]
and f2ι(z) = ϕ(z) for ι ∈[p −1]. For an arbitrary set of perturbation vectors δ = (δ1, . . . , δ2p−1)
such that δ2ι−1, δ2ι ∈Rdι for all ι, let f(x, δ) be the perturbed neural network deﬁned recursively
such that"
N,0.6233128834355828,"ez1 = f1 (x) + ∥x∥2 δ1,
ezι = fι (ezι−1) + ∥ezι−1∥2 δι
∀ι = 2, . . . , 2p −1,
f(x, δ) = ez2p−1."
N,0.6245398773006134,"The all-layer-margin m(f, x, y) measures the minimum norm of the perturbation δ such that f(x, δ)
fails to provide the classiﬁcation y,"
N,0.6257668711656442,"m(f, x, y) ≜
min
δ=(δ1,...,δ2p−1)"
N,0.6269938650306749,"v
u
u
t"
N,0.6282208588957056,"2p−1
X"
N,0.6294478527607362,"ι=1
∥δι∥2
2
s.t.
argmax
k∈[K]
f(x, δ)k ̸= y.
(7)"
N,0.6306748466257669,"With the notion of all-layer-margin established, for any A : X →2X that satisﬁes conditions in
Deﬁnition 4, the robust margin is deﬁned as"
N,0.6319018404907976,"mA(f, x) ≜
sup
x′∈A(x)
m "
N,0.6331288343558282,"f, x′, argmax
k∈[k]
f(x)k ! ."
N,0.6343558282208589,"With merely ﬁnite unlabeled data, comparing to enforcing consistency over population, we need
stronger assumptions on data augmentations in addition to the ones in Deﬁnition 4, as speciﬁed in
Deﬁnition 5 below."
N,0.6355828220858896,"Deﬁnition 5 ((F, τ)-expansion-based data augmentation). With respect to the given function class
F and some proper constant τ > 0, Let A : X →2X be a function that satisﬁes conditions in
Deﬁnition 4, and additionally,"
N,0.6368098159509202,"sup
f∈F
inf
x∈X mA(f, x) > τ.
(8)"
N,0.6380368098159509,"Then for any sample x ∈X, x′ ∈X is a (F, τ)-expansion-based data augmentation of x induced
by A if x′ ∈A(x)."
N,0.6392638036809816,Under review as a conference paper at ICLR 2022
N,0.6404907975460122,"Remark 4. The existence of the (F, τ)-expansion-based data augmentations relies on the suitable
choice of τ with respect to the given F. For a ﬁxed F, as τ increases, Equation (8) imposes stronger
assumption on A (i.e., A(x) is forced to be inclusion-wisely smaller, where A becomes weaker),
while better theoretical guarantee (Theorem 12) can be achieved."
N,0.6417177914110429,"We consider a class of p-layer fully connected neural networks with maximum width q,"
N,0.6429447852760736,"F =

f : X →RK  f = f2p−1 ◦· · · ◦f1, f2ι−1(x) = Wιx, f2ι(z) = ϕ(z) ∀ι = 1, . . . , p
	
,"
N,0.6441717791411042,"where with Wι ∈Rdι×dι−1 for all ι ∈[p], q ≜maxι=0,...,p dι. The goal is to learn a classiﬁer
bhdac
Xu = argmaxk∈[K] bf dac
n,N(x)k where bf dac
n,N ∈F is a fully connected neural network. With respect
to F, we choose a suitably large τ > 0 such that there exists a A : X →2X fulﬁlling the conditions
in Deﬁnition 5. To enforce sufﬁciently strong consistency of f with ﬁnite unlabeled samples, we
incorporate the DAC regularization with a positive robust margin such that,"
N,0.645398773006135,"bhdac
Xu ≜argmin
h∈H
bLdac
01 (h) = 1 n n
X"
N,0.6466257668711657,"i=1
1 {h (xi) ̸= h∗(xi)}
(9)"
N,0.6478527607361964,"s.t.
mA(f, xu
i ) > τ
∀i ∈[N]."
N,0.649079754601227,"In terms of the DAC operator, the algorithm can be restated as bhdac
Xu ≜argminh∈T dac
A,Xu(H) bLdac
01 (h)
with"
N,0.6503067484662577,"T dac
A,Xu(H) ≜{h ∈H | mA(f, xu
i ) > τ
∀i ∈[N]} ."
N,0.6515337423312884,"Deﬁnition 6 (Expansion assumptions, Wei et al. (2021); Cai et al. (2021)). The marginal distribution
P ∗(x) satisﬁes"
N,0.652760736196319,"(a) (q, ξ)-constant expansion if given any S ⊆X with P ∗(S) ≥q and P ∗(S ∩Xk) ≤1"
FOR,0.6539877300613497,"2 for
all k ∈[K], P ∗(NB (S)) ≥min {P ∗(S) , ξ} + P ∗(S);
(b) (a, c)-multiplicative expansion if for all k ∈[K], given any S ⊆X with P ∗(S ∩Xk) ≤a,
P ∗(NB (S) ∩Xk) ≥min {c · P ∗(S ∩Xk) , 1}."
FOR,0.6552147239263804,"Proposition 11 (Wei et al. (2021) Theorem 3.7, Cai et al. (2021) Proposition 2.2). For any δ ∈
(0, 1), with probability at least 1 −δ/2, there exists some µ such that"
FOR,0.656441717791411,PP ∗[∃x′ ∈A(x) s.t. h(x) ̸= h(x′)] ≤µ ≤eO
FOR,0.6576687116564417,"Pp
ι=1
√q ∥Wι∥F
τ
√ N
+ r"
FOR,0.6588957055214724,log (1/δ) + p log N N !
FOR,0.660122699386503,"for all h ∈T dac
A,Xu(H), where eO (·) hides polylogarithmic factors in N and d."
FOR,0.6613496932515337,"Theorem 12 (DAC with expansion-based data augmentations over ﬁnite samples). With proper
choices of τ > 0, for any A : X →2X inducing (F, τ)-expansion-based data augmentations,
learning the classiﬁer with DAC regularization, Equation (9), provides that, for any δ ∈(0, 1), with
probability at least 1 −δ,"
FOR,0.6625766871165644,"L01

bhdac
Xu

−L01 (h∗) ≤4R + r"
FOR,0.6638036809815951,2 log(4/δ)
FOR,0.6650306748466258,"n
,
(10)"
FOR,0.6662576687116565,"In speciﬁc, for some q <
1
2, c > 1, with a sufﬁciently large unlabeled sample size N such that
µ ≤1"
FOR,0.6674846625766871,"4 min {c −1, 1},"
FOR,0.6687116564417178,"(a) when P ∗(x) satisﬁes (q, 2µ)-constant expansion, R ≤ r"
K LOG K,0.6699386503067485,2K log K
K LOG K,0.6711656441717792,"n
+ 2K max {q, 2µ},"
K LOG K,0.6723926380368098,(b) while when P ∗(x) satisﬁes ( 1
K LOG K,0.6736196319018405,"2, c)-multiplicative expansion, R ≤ s"
K LOG K,0.6748466257668712,2K log K
K LOG K,0.6760736196319018,"n
+
4Kµ
min {c −1, 1},"
K LOG K,0.6773006134969325,Under review as a conference paper at ICLR 2022
K LOG K,0.6785276073619632,"where we recall from Proposition 11 that, µ ≤eO"
K LOG K,0.6797546012269938,"Pp
ι=1
√q ∥Wι∥F
τ
√ N
+ r"
K LOG K,0.6809815950920245,log (1/δ) + p log N N ! .
K LOG K,0.6822085889570552,"Lemma 4 (Cai et al. (2021), Lemma A.1). For any h ∈T dac
A,Xu(H), when P ∗satisﬁes"
K LOG K,0.6834355828220859,"(a) (q, 2µ)-constant expansion with q < 1"
K LOG K,0.6846625766871166,"2, P ∗(M) ≤max {q, 2µ}; (b)   1"
K LOG K,0.6858895705521473,"2, c

-multiplicative expansion with c > 1 + 4µ, P ∗(M) ≤max
n
2µ
c−1, 2µ
o
."
K LOG K,0.6871165644171779,"Proof of Lemma 4. We start with the proof for Lemma 4 (a). By deﬁnition of Mk and byk, we know
that Mk = M ∩Xk ≤1"
K LOG K,0.6883435582822086,"2. Therefore, for any 0 < q < 1"
K LOG K,0.6895705521472393,"2, one of the following two cases holds:"
K LOG K,0.69079754601227,"(i) P ∗(M) < q;
(ii) P ∗(M) ≥q. Since P ∗(M ∩Xk) < 1"
K LOG K,0.6920245398773006,"2 for all k ∈[K] holds by construction, with the
(q, 2µ)-constant expansion, P ∗(NB (M)) ≥min {P ∗(M) , 2µ} + P ∗(M).
Meanwhile, since the ground truth classiﬁer h∗is invariant throughout the neighborhoods,
NB (Mk)∩NB (Mk′) = ∅for k ̸= k′, and therefore NB (M) \M = SK
k=1 NB (Mk) \Mk
with each NB (Mk) \Mk disjoint. Then, we observe that for each x ∈NB (M) \M,
here exists some k = h∗(x) such that x ∈NB (Mk) \Mk. x ∈Xk\Mk implies that
h (x) = byk, while x ∈NB (Mk) suggests that there exists some x′ ∈A (x) ∩A (x′′)
where x′′ ∈Mk such that either h (x′) = byk and h (x′) ̸= h (x′′) for x′ ∈A (x′′), or
h (x′) ̸= byk and h (x′) ̸= h (x) for x′ ∈A (x). Therefore, we have"
K LOG K,0.6932515337423313,P ∗(NB (M) \M) ≤2PP ∗[∃x′ ∈A(x) s.t. h(x) ̸= h(x′)] ≤2µ.
K LOG K,0.694478527607362,"Moreover, since P ∗(NB (M)) −P ∗(M) ≤P ∗(NB (M) \M) ≤2µ, we know that"
K LOG K,0.6957055214723926,"min {P ∗(M) , 2µ} + P ∗(M) ≤P ∗(NB (M)) ≤P ∗(M) + 2µ."
K LOG K,0.6969325153374233,"That is, P ∗(M) ≤2µ."
K LOG K,0.698159509202454,"Overall, we have P ∗(M) ≤max {q, 2µ}."
K LOG K,0.6993865030674846,"To show Lemma 4 (b), we recall from Wei et al. (2021) Lemma B.6 that for any c > 1 + 4µ,
  1"
K LOG K,0.7006134969325153,"2, c

-multiplicative expansion implies

2µ
c−1, 2µ

-constant expansion. Then leveraging the proof"
K LOG K,0.701840490797546,"for Lemma 4 (a), with q =
2µ
c−1, we have P ∗(M) ≤max
n
2µ
c−1, 2µ
o
.
■"
K LOG K,0.7030674846625767,"Proof of Theorem 12. To show Equation (10), we leverage the proof of Proposition 8, and observe
that B = 1 with the zero-one loss. Therefore, when conditioned on T dac
A,Xu(H), for any δ ∈(0, 1),
with probability at least 1 −δ/2,"
K LOG K,0.7042944785276074,"L01

bhdac
Xu

−L01 (h∗) ≤4Rn
 
l01 ◦T dac
A,Xu(H)

+ r"
K LOG K,0.7055214723926381,"2 log(4/δ) n
."
K LOG K,0.7067484662576687,"For the upper bounds of the Rademacher complexity, let eµ ≜suph∈T dac
A,Xu(H) P ∗(M) where M"
K LOG K,0.7079754601226994,"denotes the global minority set with respect to h ∈T dac
A,Xu(H). Lemma 4 suggests that"
K LOG K,0.7092024539877301,"(a) when P ∗satisﬁes (q, 2µ)-constant expansion for some q < 1"
K LOG K,0.7104294478527607,"2, eµ ≤max {q, 2µ}; while
(b) when P ∗satisﬁes ( 1"
K LOG K,0.7116564417177914,"2, c)-multiplicative expansion for some c > 1 + 4µ, eµ ≤
2µ
min{c−1,1}."
K LOG K,0.7128834355828221,"Then, it is sufﬁcient to show that, conditioned on T dac
A,Xu(H),"
K LOG K,0.7141104294478527,"Rn
 
l01 ◦T dac
A,Xu(H)

≤ r"
K LOG K,0.7153374233128834,2K log K
K LOG K,0.7165644171779141,"n
+ 2Keµ.
(11)"
K LOG K,0.7177914110429447,"To show this, we ﬁrst consider a ﬁxed set of n observations in X, X = [x1, . . . , xn]⊤∈X n. Let
the number of distinct behaviors over X in T dac
A,Xu(H) be"
K LOG K,0.7190184049079754,"s
 
T dac
A,Xu(H), X

≜

[h (x1) , . . . , h (xn)]
 h ∈T dac
A,Xu(H)
	 ."
K LOG K,0.7202453987730061,Under review as a conference paper at ICLR 2022
K LOG K,0.7214723926380369,"Then, by the Massart’s ﬁnite lemma, the empirical rademacher complexity with respect to X is
upper bounded by"
K LOG K,0.7226993865030675,"bRX
 
l01 ◦T dac
A,Xu(H)

≤"
K LOG K,0.7239263803680982,"v
u
u
t2 log s

T dac
A,Xu(H), X
 n
."
K LOG K,0.7251533742331289,"By the concavity of
p"
K LOG K,0.7263803680981595,"log (·), we know that,"
K LOG K,0.7276073619631902,"Rn
 
l01 ◦T dac
A,Xu(H)

=EX
h
bRX
 
l01 ◦T dac
A,Xu(H)
i
≤EX  "
K LOG K,0.7288343558282209,"v
u
u
t2 log s

T dac
A,Xu(H), X
 n   ≤"
K LOG K,0.7300613496932515,"v
u
u
t2 log EX
h
s

T dac
A,Xu(H), X
i"
K LOG K,0.7312883435582822,"n
.
(12)"
K LOG K,0.7325153374233129,Since P ∗(M) ≤eµ ≤1
K LOG K,0.7337423312883435,"2 for all h ∈T dac
A,Xu(H), we have that, conditioned on T dac
A,Xu(H),"
K LOG K,0.7349693251533742,"EX

s
 
T dac
A,Xu(H), X

≤ n
X r=0 n
r"
K LOG K,0.7361963190184049,"
eµr (1 −eµ)n−r · KK · Kr"
K LOG K,0.7374233128834355,"≤KK
n
X r=0 n
r"
K LOG K,0.7386503067484662,"
(eµK)r (1 −eµ)n−r"
K LOG K,0.7398773006134969,=KK (1 −eµ + Keµ)n
K LOG K,0.7411042944785277,≤KK · eKneµ.
K LOG K,0.7423312883435583,"Plugging this into Equation (12) yields Equation (11). Finally, the randomness in T dac
A,Xu(H) is
quantiﬁed by eµ, µ, and upper bounded by Proposition 11.
■"
K LOG K,0.743558282208589,"B.4
SUPPLEMENTARY EXAMPLE: DOMAIN ADAPTATION"
K LOG K,0.7447852760736197,"As a supplementary example, we demonstrate the possible failure of the ERM on augmented training
set, and how the DAC regularization can serve as a remedy, with an illustrative linear regression
problem in the domain adaptation setting: where the training samples are drawn from some source
distribution P s, while the excess risk is tested over a related but different distribution P t, known
as the target distribution. Speciﬁcally, assuming that EP s [y|x] and EP t [y|x] are distinct, but there
exists some invariant feature subspace Xr ⊂X such that P s [y|x ∈Xr] = P t [y|x ∈Xr], we aim
to demonstrate the advantage of the DAC regularization over the ERM on augmented training set,
with a provable separation in the respective excess risks. x"
K LOG K,0.7460122699386503,"ζiv
ζe e
y"
K LOG K,0.747239263803681,Figure 4: Causal graph shared by P s and P t.
K LOG K,0.7484662576687117,"Source and target distributions.
Formally, the source and target distributions are concretized
with the causal graph in Figure 4. For both P s and P t, the observable feature x is described via a
linear generative model in terms of two latent features, the ‘invariant’ feature ζiv ∈Rdiv and the
‘environmental’ feature ζe ∈Rde:"
K LOG K,0.7496932515337423,"x = g(ζiv, ζe) ≜S [ζiv; ζe] = Sivζiv + Seζe,"
K LOG K,0.750920245398773,Under review as a conference paper at ICLR 2022
K LOG K,0.7521472392638037,"where S = [Siv, Se] ∈Rd×(div+de) (div + de ≤d) consists of orthonormal columns. Let the label
y depends only on the invariant feature ζiv for both domains,"
K LOG K,0.7533742331288343,"y = (θ∗)⊤x + z = (θ∗)⊤Sivζiv + z,
z ∼N
 
0, σ2
,
z ⊥ζiv,"
K LOG K,0.754601226993865,"for some θ∗∈Range (Siv) such that P s [y|ζiv] = P t [y|ζiv], while the environmental feature ζe
is conditioned on y, ζiv, (along with the Gaussian noise z), and varies across different domains e
with EP s [y|x] ̸= EP t [y|x]. In other words, with the square loss l(h(x), y) = 1"
K LOG K,0.7558282208588957,"2(h(x) −y)2, the
optimal hypotheses that minimize the expected excess risk over the source and target distributions
are distinct. Therefore, learning via the ERM with training samples from P s can overﬁt the source
distribution, in which scenario identifying the invariant feature subspace Range (Siv) becomes in-
dispensable for achieving good generalization in the target domain."
K LOG K,0.7570552147239263,"Moreover, we assume the following regularity conditions on the source and target distributions:"
K LOG K,0.758282208588957,"Assumption 3 (Regularity conditions for P s and P t). Let P s satisfy Assumption 1, and P t satisfy
the following: let EP t[xx⊤] ≻0, and"
K LOG K,0.7595092024539877,"(a) for the invariant feature, ct,ivIdiv ≼EP t[ζivζ⊤
iv] ≼Ct,ivIdiv for some Ct,iv ≥ct,iv = Θ(1);
(b) for the environmental feature, EP t[ζeζ⊤
e ] ≽ct,eIde for some ct,e > 0, and EP t [z · ζe] = 0."
K LOG K,0.7607361963190185,"Training samples and data augmentations.
For a fair comparison between learning with the
DAC regularization and the ERM on augmented training set, we restrict to the supervised learning
setting: Xu = X ∈X n. Recall that we denote the augmented training sets, including and excluding
the original samples, respectively with"
K LOG K,0.7619631901840491,"e
A(X) = [x1; · · · ; xn; x1,1; · · · ; xn,1; · · · ; x1,α; · · · ; xn,α] ∈X (1+α)n,
b
A(X) = [x1,1; · · · ; xn,1; · · · ; x1,α; · · · ; xn,α] ∈X αn."
K LOG K,0.7631901840490798,"In particular, we consider a set of augmentations that only perturb the environmental feature ζe,
while keep the invariant feature ζiv intact,"
K LOG K,0.7644171779141105,"S⊤
ivxi = S⊤
ivxi,j,
S⊤
e xi ̸= S⊤
e xi,j
∀i ∈[n], j ∈[α].
(13)"
K LOG K,0.7656441717791411,"We recall the notion bdaug = rank

b
A (X) −MX

= rank

e
A (X) −f
MX

(notice that 0 ≤
bdaug ≤de), and assume that X and b
A(X) are representative enough:"
K LOG K,0.7668711656441718,"Assumption 4 (Diversity of X and b
A(X)). (X, y) ∈X n ×Yn is sufﬁciently large with n ≫ρ4div,
θ∗∈span {xi | i ∈[n]}, and bdaug = de."
K LOG K,0.7680981595092025,"Excess
risks
in
target
domain.
Learning
from
the
linear
hypothesis
class
H
=

h(x) = θ⊤x
 θ ∈Rd	
, with the DAC regularization on h (xi) = h (xi,j), we have"
K LOG K,0.7693251533742331,"bθdac =
argmin
θ∈T dac
b
A,X(H)"
K LOG K,0.7705521472392638,"1
2n ∥y −Xθ∥2
2 ,
T dac
b
A,X (H) =
n
h (x) = θ⊤x
 b
A(X)θ = MXθ
o
,"
K LOG K,0.7717791411042945,"while with the ERM on augmented training set,"
K LOG K,0.7730061349693251,"bθda−erm = argmin
θ∈Rd
1
2(1 + α)n"
K LOG K,0.7742331288343558,"f
My −e
A(X)θ

2 2 ,"
K LOG K,0.7754601226993865,"where M and f
M denote the vertical stacks of α and 1+α identity matrices of size n×n, respectively
as denoted earlier."
K LOG K,0.7766871165644171,"We
are
interested
in
the
excess
risk
on
P t:
Lt (θ) −Lt (θ∗)
where
Lt (θ)
≜
EP t(x,y)
 1"
K LOG K,0.7779141104294478,"2(y −x⊤θ)2
."
K LOG K,0.7791411042944786,"Theorem 13 (Domain adaptation with DAC). Under Assumption 3(a) and Assumption 4, bθdac sat-
isﬁes that, with constant probability,"
K LOG K,0.7803680981595092,"EP s
h
Lt(bθdac) −Lt(θ∗)
i
≲σ2div"
K LOG K,0.7815950920245399,"n
.
(14)"
K LOG K,0.7828220858895706,Under review as a conference paper at ICLR 2022
K LOG K,0.7840490797546013,"Theorem 14 (Domain adaptation with ERM on augmented samples). Under Assumption 3 and
Assumption 4, bθdac and bθda−erm satisﬁes that,"
K LOG K,0.7852760736196319,"EP s
h
Lt(bθda−erm) −Lt(θ∗)
i
≥EP s
h
Lt(bθdac) −Lt(θ∗)
i
+ ct,e · EERe,
(15)"
K LOG K,0.7865030674846626,for some EERe > 0.
K LOG K,0.7877300613496933,"In contrast to bθdac where the DAC constraints enforce S⊤
e bθdac = 0 with a sufﬁciently diverse b
A (X)
(Assumption 4), the ERM on augmented training set fails to ﬁlter out the environmental feature in
bθda−erm: S⊤
e bθda−erm ̸= 0. As a consequence, the expected excess risk of bθda−erm in the target
domain can be catastrophic when ct,e →∞, as instantiated by Example 2."
K LOG K,0.7889570552147239,"Proofs and instantiation.
We ﬁrst recall from the beginning of Appendix B that"
K LOG K,0.7901840490797546,"PN ≜Id −

b
A(X) −MX
† 
b
A(X) −MX
"
K LOG K,0.7914110429447853,"is the orthogonal projector onto the dimension-(d −bdaug) null space of b
A(X) −MX. Furthermore,
let Piv ≜SivS⊤
iv and Pe ≜SeS⊤
e be the orthogonal projectors onto the invariant and environmental
feature subspaces, respectively, such that x = Sivζiv + Seζe = (Piv + Pe) x for all x."
K LOG K,0.7926380368098159,"Proof of Theorem 13. By construction Equation (13),

b
A(X) −MX

Piv = 0, and it follows that"
K LOG K,0.7938650306748466,"Piv ≼PN . Meanwhile from Assumption 4, bdaug = de implies that dim (PN ) = div. Therefore,
Piv = PN , and the data augmentation consistency constraints can be restated as"
K LOG K,0.7950920245398773,"T dac
b
A,X(H) ≜

h (x) = θ⊤x
 PN θ = θ
	
=

h (x) = θ⊤x
 Pivθ = θ"
K LOG K,0.7963190184049079,"Then with θ∗∈span {xi | i ∈[n]} from Assumption 4,"
K LOG K,0.7975460122699386,bθdac −θ∗= 1
K LOG K,0.7987730061349694,"n
bΣ†
XivPivX⊤(XPivθ∗+ z) −θ∗= 1"
K LOG K,0.8,"n
bΣ†
XivPivX⊤z,"
K LOG K,0.8012269938650307,"where
bΣXiv
≜
1
nPivX⊤XPiv.
Since
bθdac −θ∗
∈
Col (Siv),
we
have"
K LOG K,0.8024539877300614,"EP t
h
z · x⊤Pe(bθdac −θ∗)
i
= 0. Therefore, let Σx,t ≜EP t[xx⊤], with high probability,"
K LOG K,0.803680981595092,"EP s
h
Lt(bθdac) −Lt(θ∗)
i
= EP s
1 2"
K LOG K,0.8049079754601227,"bθdac −θ∗
2 Σx,t  = tr"
K LOG K,0.8061349693251534,"1
2nEP s 
zz⊤
EP s "" 1"
K LOG K,0.807361963190184,"nPivX⊤XPiv †# Σx,t !"
K LOG K,0.8085889570552147,"= tr
σ2"
"N EP S
H",0.8098159509202454,"2n EP s
h
bΣ†
Xiv"
"N EP S
H",0.811042944785276,"i
Σx,t "
"N EP S
H",0.8122699386503067,"≤Ct,iv
σ2"
N TR,0.8134969325153374,"2n tr

EP s
h
bΣ†
Xiv"
N TR,0.8147239263803681,"i
(Lemma 5, w.h.p.) ≲σ2"
N TR,0.8159509202453987,"2n tr
 
EP s 
Pivxx⊤Piv
† ≤σ2"
N TR,0.8171779141104294,2nc tr(Piv) ≲σ2div 2n . ■
N TR,0.8184049079754602,"Proof of Theorem 14. Let bΣ e
A(X) ≜
1
(1+α)n e
A (X)⊤e
A (X). Then with θ∗∈span {xi | i ∈[n]}"
N TR,0.8196319018404908,"from Assumption 4, we have θ∗
=
bΣ†
e
A(X) bΣ e
A(X)θ∗.
Since θ∗
∈Col (Siv), f
MXθ∗
="
N TR,0.8208588957055215,"f
MXPivθ∗= e
A(X)θ∗. Then, the ERM on the augmented training set yields"
N TR,0.8220858895705522,"bθda−erm −θ∗=
1
(1 + α)n
bΣ†
e
A(X) e
A(X)⊤f
M(Xθ∗+ z) −bΣ†
e
A(X) bΣ e
A(X)θ∗"
N TR,0.8233128834355828,"=
1
(1 + α)n
bΣ†
e
A(X) e
A(X)⊤f
Mz."
N TR,0.8245398773006135,Under review as a conference paper at ICLR 2022
N TR,0.8257668711656442,"Meanwhile with EP t [z · ζe] = 0 from Assumption 3, we have EP t [z · Pex] = 0. Therefore, by
recalling that Σx,t ≜EP t[xx⊤],"
N TR,0.8269938650306748,Lt(θ) −Lt(θ∗) = EP t(x) 1
N TR,0.8282208588957055,"2
 
x⊤(θ −θ∗)
2 + z · x⊤Pe(θ −θ∗)

= 1"
N TR,0.8294478527607362,"2 ∥θ∗−θ∥2
Σx,t ,"
N TR,0.8306748466257668,such that the expected excess risk can be expressed as
N TR,0.8319018404907975,"EP s
h
Lt(bθda−erm) −Lt(θ∗)
i
=
1
2(1 + α)2n2 tr

EP s
h
bΣ†
e
A(X)"
N TR,0.8331288343558282,"
e
A(X)⊤f
Mzz⊤f
M⊤e
A(X)

bΣ†
e
A(X)"
N TR,0.8343558282208589,"i
Σx,t

,"
N TR,0.8355828220858895,"where let bΣ e
A(Xe) ≜Pe bΣ e
A(X)Pe,"
N TR,0.8368098159509203,"EP s
h
bΣ†
e
A(X)"
N TR,0.838036809815951,"
e
A(X)⊤f
Mzz⊤f
M⊤e
A(X)

bΣ†
e
A(X) i"
N TR,0.8392638036809816,"≽EP s
h
Piv bΣ†
e
A(X)Piv + Pe bΣ†
e
A(X)Pe

e
A(X)⊤f
Mzz⊤f
M⊤e
A(X)

Piv bΣ†
e
A(X)Piv + Pe bΣ†
e
A(X)Pe
i"
N TR,0.8404907975460123,"≽σ2(1 + α)2n · EP s
h
bΣ†
Xiv"
N TR,0.841717791411043,"i
+ EP s
h
bΣ†
e
A(Xe) e
A(Xe)⊤f
Mzz⊤f
M⊤e
A(Xe)bΣ†
e
A(Xe) i
."
N TR,0.8429447852760736,We denote
N TR,0.8441717791411043,"EERe ≜tr

EP s

1
2(1 + α)2n2 bΣ†
e
A(Xe) e
A(Xe)⊤f
Mzz⊤f
M⊤e
A(Xe)bΣ†
e
A(Xe) 
,"
N TR,0.845398773006135,and observe that
N TR,0.8466257668711656,"EERe = EP s ""
1
2"
N TR,0.8478527607361963,"1
(1 + α)n
bΣ†
e
A(Xe) e
A(Xe)⊤f
Mz 2 2 # > 0."
N TR,0.849079754601227,"Finally, we complete the proof by partitioning the lower bound for the target expected excess risk of
bθda−erm into the invariantand environmental parts such that"
N TR,0.8503067484662576,"EP s
h
Lt(bθda−erm) −Lt(θ∗)
i"
N TR,0.8515337423312883,"≥tr
σ2"
"N EP S
H",0.852760736196319,"2n EP s
h
bΣ†
Xiv"
"N EP S
H",0.8539877300613496,"i
Σx,t "
"N EP S
H",0.8552147239263803,"|
{z
}"
"N EP S
H",0.8564417177914111,=E[Lt(bθdac)−Lt(θ∗)]
"N EP S
H",0.8576687116564418,"+ tr

EP s

1
2(1 + α)2n2 bΣ†
e
A(Xe) e
A(Xe)⊤f
Mzz⊤f
M⊤e
A(Xe)bΣ†
e
A(Xe)"
"N EP S
H",0.8588957055214724,"
Σx,t "
"N EP S
H",0.8601226993865031,"|
{z
}
expected excess risk from environmental feature subspace≥ct,e·EERe"
"N EP S
H",0.8613496932515338,"≥EP s
h
Lt(bθdac) −Lt(θ∗)
i
+ ct,e · EERe. ■"
"N EP S
H",0.8625766871165644,"Now we construct a speciﬁc domain adaptation example with a large separation (i.e., proportional to
de) in the target excess risk between learning with the DAC regularization (i.e., bθdac) and with the
ERM on augmented training set (i.e., bθda−erm).
Example 2. We consider P s and P t that follow the same set of relations in Figure 4, except for the
distributions over e where P s (e) ̸= P t (e). Precisely, let the environmental feature ζe depend on
(ζiv, y, e):"
"N EP S
H",0.8638036809815951,"ζe = sign

y −(θ∗)⊤Sivζiv

e = sign(z)e,
z ∼N(0, σ2),
z ⊥e,"
"N EP S
H",0.8650306748466258,"where e ∼N (0, Ide) for P s(e) and e ∼N (0, σtIde) for P t(e), σt ≥ct,e (recall ct,e from
Assumption 3). Assume that the training set X is sufﬁciently large, n ≫de + log (1/δ) for some
given δ ∈(0, 1). Augmenting X with a simple by common type of data augmentations – the linear
transforms, we let"
"N EP S
H",0.8662576687116564,"e
A(X) = [X; (XA1) ; . . . ; (XAα)] ,
Aj = Piv + ujv⊤
j ,
uj, vj ∈Col (Se)
∀j ∈[α],"
"N EP S
H",0.8674846625766871,and deﬁne
"N EP S
H",0.8687116564417178,"ν1 ≜max {1} ∪{σmax(Aj) | j ∈[α]}
and
ν2 ≜σmin "
"N EP S
H",0.8699386503067484,"
1
1 + α  Id + α
X"
"N EP S
H",0.8711656441717791,"j=1
Ak    ,"
"N EP S
H",0.8723926380368098,Under review as a conference paper at ICLR 2022
"N EP S
H",0.8736196319018404,"where σmin(·) and σmax(·) refer to the minimum and maximum singular values, respectively. Then
under Assumption 3 and Assumption 4, with constant probability,"
"N EP S
H",0.8748466257668711,"EP s
h
Lt(bθda−erm) −Lt(θ∗)
i
≳EP s
h
Lt(bθdac) −Lt(θ∗)
i
+ ct,e · σ2de 2n ."
"N EP S
H",0.8760736196319019,"Proof of Example 2. With the speciﬁed distribution, for E = [e1; . . . ; en] ∈Rn×de,"
"N EP S
H",0.8773006134969326,"bΣ e
A(Xe) =
1
(1 + α)nSe "
"N EP S
H",0.8785276073619632,"E⊤E + α
X"
"N EP S
H",0.8797546012269939,"j=1
A⊤
j E⊤EAj "
"N EP S
H",0.8809815950920246,"S⊤
e ≼ν2
1
n SeE⊤ES⊤
e ,"
"N EP S
H",0.8822085889570552,"1
(1 + α)n
e
A(Xe)⊤f
Mz = "
"N EP S
H",0.8834355828220859,"
1
1 + α  Id + α
X"
"N EP S
H",0.8846625766871166,"j=1
Aj    "
"N EP S
H",0.8858895705521472,"⊤
1
nSeE⊤|z| ."
"N EP S
H",0.8871165644171779,"By Lemma 5, under Assumption 3 and Assumption 4, we have that with high probability, 0.9Ide ≼
1
nE⊤E ≼1.1Ide. Therefore with E and z being independent,"
"N EP S
H",0.8883435582822086,"EERe = EP s ""
1
2"
"N EP S
H",0.8895705521472392,"1
(1 + α)n
bΣ†
e
A(Xe) e
A(Xe)⊤f
Mz 2 2 # ≥σ2"
N,0.8907975460122699,"2n
ν2
2
ν4
1
tr  EP s "" 1"
N,0.8920245398773006,"nSeE⊤ES⊤
e †#! ≳σ2"
N,0.8932515337423312,"2n
ν2
2
ν4
1
tr
 
SeS⊤
e
 ≳σ2de 2n ,"
N,0.894478527607362,"and the rest follows from Theorem 14.
■"
N,0.8957055214723927,"C
TECHNICAL LEMMAS"
N,0.8969325153374234,"Lemma 5. We consider a random vector x ∈Rd with E[x] = 0, E[xx⊤] = Σ, and x = Σ−1/2x
3 being ρ2-subgaussian. Given an i.i.d. sample of x, X = [x1, . . . , xn]⊤, for any δ ∈(0, 1), if
n ≫ρ4d, then 0.9Σ ≼1"
N,0.898159509202454,nX⊤X ≼1.1Σ with probability high probability.
N,0.8993865030674847,"Proof. We ﬁrst denote PX ≜ΣΣ† as the orthogonal projector onto the subspace X ⊆Rd supported
by the distribution of x. With the assumptions E[x] = 0 and E[xx⊤] = Σ, we observe that E [x] = 0
and E
"
N,0.9006134969325154,"xx⊤
= E

xΣ−1x⊤
= PX . Given the sample set X of size n ≫ρ4 (d + log(1/δ)) for
some δ ∈(0, 1), we let U =
1
n
Pn
i=1 xiΣ−1x⊤
i −PX . Then the problem can be reduced to
showing that, with probability at least 1 −δ, ∥U∥2 ≤0.1. For this, we leverage the ϵ-net argument
as following."
N,0.901840490797546,"For an arbitrary v ∈X ∩Sd−1, we have"
N,0.9030674846625767,"v⊤Uv = 1 n n
X i=1"
N,0.9042944785276074," 
v⊤xiΣ−1x⊤
i v −1

= 1 n n
X i=1"
N,0.905521472392638," 
v⊤xi
2 −1

,"
N,0.9067484662576687,"where, given xi being ρ2-subgaussian, v⊤xi is ρ2-subgaussian. Since"
N,0.9079754601226994,"E
h 
v⊤xi
2i
= v⊤E
"
N,0.90920245398773,"xix⊤
i

v = 1,"
N,0.9104294478527607,"we know that
 
v⊤xi
2 −1 is 16ρ2-subexponential. Then, we recall the Bernstein’s inequality,"
N,0.9116564417177914,"P
v⊤Uv
 > ϵ

≤2 exp  −n"
MIN,0.912883435582822,2 min ϵ2
MIN,0.9141104294478528,"(16ρ2)2 ,
ϵ
16ρ2 !! ."
MIN,0.9153374233128835,"3In the case where Σ is rank-deﬁcient, we slightly abuse the notation such that Σ−1/2 and Σ−1 refer to the
respective pseudo-inverses."
MIN,0.9165644171779141,Under review as a conference paper at ICLR 2022
MIN,0.9177914110429448,"Let N ⊂X ∩Sd−1 be an ϵ1-net such that |N| = eO(d). Then for some 0 < ϵ2 ≤16ρ2, by the
union bound,"
MIN,0.9190184049079755,"P

max
v∈N :
v⊤Uv
 > ϵ2"
MIN,0.9202453987730062,"
≤2 |N| exp  −n"
MIN,0.9214723926380368,2 min
MIN,0.9226993865030675,"ϵ2
2
(16ρ2)2 ,
ϵ2
16ρ2 !! ≤exp "
MIN,0.9239263803680982,O (d) −n
MIN,0.9251533742331288,"2 ·
ϵ2
2
(16ρ2)2 ! ≤δ"
MIN,0.9263803680981595,"whenever n >
2(16ρ2)
2"
MIN,0.9276073619631902,"ϵ2
2
 
Θ (d) + log 1"
MIN,0.9288343558282208,"δ

. By taking δ = exp

−1"
MIN,0.9300613496932515,"4

ϵ2
16ρ2
2
n

, we have that"
MIN,0.9312883435582822,"max
v∈N
v⊤Uv
 ≤ϵ2 with high probability when n > 4

16ρ2 ϵ2"
MIN,0.9325153374233128,"2
Θ (d), and taking n ≫ρ4d is"
MIN,0.9337423312883436,sufﬁcient.
MIN,0.9349693251533743,"Now for any v ∈X ∩Sd−1, there exists some v′ ∈N such that ∥v −v′∥2 ≤ϵ1. Therefore,"
MIN,0.9361963190184049,"v⊤Uv
 =
v′⊤Uv′ + 2v′⊤U (v −v′) + (v −v′)⊤U (v −v′)"
MIN,0.9374233128834356,"≤

max
v∈N :
v⊤Uv


+ 2 ∥U∥2 ∥v′∥2 ∥v −v′∥2 + ∥U∥2 ∥v −v′∥2
2"
MIN,0.9386503067484663,"≤

max
v∈N :
v⊤Uv


+ ∥U∥2
 
2ϵ1 + ϵ2
1

."
MIN,0.939877300613497,"Taking the supremum over v ∈Sd−1, with probability at least 1 −δ,"
MIN,0.9411042944785276,"max
v∈X∩Sd−1 :
v⊤Uv
 = ∥U∥2 ≤ϵ2 + ∥U∥2
 
2ϵ1 + ϵ2
1

,
∥U∥2 ≤
ϵ2
2 −(1 + ϵ1)2 ."
MIN,0.9423312883435583,With ϵ1 = 1
MIN,0.943558282208589,"3 and ϵ2 =
1
45, we have
ϵ2
2−(1+ϵ1)2 =
1
10."
MIN,0.9447852760736196,"Overall, if n ≫ρ4d, then with high probability, we have ∥U∥2 ≤0.1.
■"
MIN,0.9460122699386503,"Lemma 6. Let U ⊆Rd be an arbitrary non-trivial subspace in Rd, and g ∼N (0, Id) be a
Gaussian random vector. Then for any continuous and Cl-Lipschitz function ϕ : R →R (i.e.,
|ϕ(u) −ϕ(u′)| ≤Cl · |u −u′| for all u, u′ ∈R), Eg"
MIN,0.947239263803681,"
sup
u∈U
g⊤ϕ(u)

≤Cl · Eg"
MIN,0.9484662576687116,"
sup
u∈U
g⊤u

,"
MIN,0.9496932515337423,"where ϕ acts on u entry-wisely, (ϕ(u))j = ϕ(uj). In other words, the Gaussian width of the image
set ϕ(U) ≜

ϕ(u) ∈Rd | u ∈U
	
is upper bounded by that of U scaled by the Lipschitz constant."
MIN,0.950920245398773,Under review as a conference paper at ICLR 2022
MIN,0.9521472392638037,Proof. Eg
MIN,0.9533742331288344,"
sup
u∈U
g⊤ϕ(u)

=1"
EG,0.9546012269938651,2Eg
EG,0.9558282208588957,"
sup
u∈U
g⊤ϕ(u) + sup
u′∈U
g⊤ϕ(u)
 =1"
EG,0.9570552147239264,2Eg
EG,0.9582822085889571,"
sup
u,u′∈U
g⊤(ϕ(u) −ϕ(u′))
 ≤1"
EG,0.9595092024539877,2Eg 
EG,0.9607361963190184,"sup
u,u′∈U d
X"
EG,0.9619631901840491,"j=1
|gj|
ϕ(uj) −ϕ(u′
j) "
EG,0.9631901840490797,"
∵ϕ is Cl-Lipschitz ≤Cl"
EG,0.9644171779141104,2 Eg 
EG,0.9656441717791411,"sup
u,u′∈U d
X"
EG,0.9668711656441717,"j=1
|gj|
uj −u′
j   =Cl"
EG,0.9680981595092024,2 Eg
EG,0.9693251533742331,"
sup
u,u′∈U
g⊤(u −u′)
 =Cl"
EG,0.9705521472392638,2 Eg
EG,0.9717791411042945,"
sup
u∈U
g⊤u + sup
u′∈U
g⊤(−u′)
"
EG,0.9730061349693252,=Cl · Eg
EG,0.9742331288343559,"
sup
u∈U
g⊤u
 ■"
EG,0.9754601226993865,"D
EXPERIMENT DETAILS"
EG,0.9766871165644172,"In this section, we provide the details of our experiments. Our code is adapted from the publicly
released repo: https://github.com/kekmodel/FixMatch-pytorch."
EG,0.9779141104294479,"Dataset: Our training dataset is derived from CIFAR-100, where the original dataset contains 50,000
training samples of 100 different classes. Out of the original 50,000 samples, we randomly select
10,000 labeled data as training set (i.e., 100 labeled samples per class). To see the impact of different
training samples, we also trained our model with dataset that contains 1,000 and 20,000 samples.
Evaluations are done on standard test set of CIFAR-100, which contains 10,000 testing samples."
EG,0.9791411042944785,"Data Augmentation: During the training time, given a training batch, we generate corresponding
augmented samples by RandAugment (Cubuk et al., 2020). We set the number of augmentations per
sample to 7, unless otherwise mentioned."
EG,0.9803680981595092,"To generate an augmented image, the RandAugment draws n transformations uniformaly at random
from 14 different augmentations, namely {identity, autoContrast, equalize, rotate, solarize, color,
posterize, contrast, brightness, sharpness, shear-x, shear-y, translate-x, translate-y}. The RandAug-
ment provides each transformation with a single scalar (1 to 10) to control the strength of each of
them, which we always set to 10 for all transformations. By default, we set n = 2 (i.e., using 2 ran-
dom transformations to generate an augmented sample). To see the impact of different augmentation
strength, we choose n ∈{1, 2, 5, 10}. Examples of augmented samples are shown in Figure 3."
EG,0.9815950920245399,"Parameter Setting: The batch size is set to 64 and the entire training process takes 215 steps.
During the training, we adopt the SGD optimizer with momentum set to 0.9, with learning rate for
step i being 0.03 × cos

i×7π
215×16

."
EG,0.9828220858895705,"Additional Settings for the semi-supervised learning results: For the results on semi-supervised
learning, besides the 10,000 labeled samples, we also draw additionally samples (ranging from
5,000 to 20,000) from the training set of the original CIFAR-100. We remove the labels of those
additionally sampled images, as they serve as “unlabeled” samples in the semi-supervised learning
setting. The FixMatch implementation follows the publicly available on in https://github.
com/kekmodel/FixMatch-pytorch."
EG,0.9840490797546012,"E
ILLUSTRATIVE EXAMPLE"
EG,0.9852760736196319,Under review as a conference paper at ICLR 2022
EG,0.9865030674846625,"2
4
6
8
10
12
Number of augmentations per sample (i.e., ) 0.6 0.7 0.8 0.9 1.0"
EG,0.9877300613496932,Test Error
EG,0.9889570552147239,DAC with daug=25
EG,0.9901840490797545,DA-ERM with daug=25
EG,0.9914110429447853,DAC with daug=20
EG,0.992638036809816,DA-ERM with daug=20
EG,0.9938650306748467,"Figure 5: Comparison of DAC regularization and
DA-ERM for logistic regression (Example 3).
The results precisely match Theorem 3 and the
discussion afterward. As suggested by the the-
oretical analysis, the performance of DAC only
depends on the daug, but not α. Further, given
the same augmented dataset, DA-ERM is always
worse than DAC. The gap is particularly signiﬁ-
cant when the number of augmentations is small
(i.e., a small α)."
EG,0.9950920245398773,"Here we present an illustrative example for lo-
gistic regression, which numerically shows the
beneﬁts of DAC over ERM, and also clearly
demonstrates the impact of different daug and
α."
EG,0.996319018404908,"Example 3. Consider a 30-dimensional logis-
tic regression. The original training set con-
tains 50 samples.
The inputs xis are gener-
ated independently from N(0, I30) and we set
θ∗= [θ∗
c; 0] with θ∗
c ∼N(0, I3) and 0 ∈R27."
EG,0.9975460122699387,"To generate the augmentations, we ﬁrst specify
a parameter daug and leave the ﬁrst 30 −daug
elements of each xi unchanged, and replace the
later daug elements of each xi with a new vector
randomly generated from N(0, Idaug). Further,
we generate α augmentations for each of the xi.
For any α ≥1, the augmentation will perturb
daug coordinates with probability 1."
EG,0.9987730061349693,"The results for daug ∈{20, 25} and various αs
are presented in Figure 5. The test error clearly
matches Theorem 3 and the discussion after-
ward: 1) The generalization performance of DAC only relies on daug but not α, and larger daug
gives smaller testing error. 2) The performance of DA-ERM crucially depends on α, when α is
small (i.e., limited augmentations), the performance between DAC and DA-ERM is very signiﬁcant.
And when we further increases α, DA-ERM can only approach to DAC but not out-perform DAC."
