Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009970089730807576,"Low-light image enhancement poses a significant challenge due to the limited
1"
ABSTRACT,0.0019940179461615153,"information captured by image sensors in low-light environments. Despite recent
2"
ABSTRACT,0.0029910269192422734,"improvements in deep learning models, the lack of paired training datasets remains
3"
ABSTRACT,0.003988035892323031,"a significant obstacle. Therefore, unsupervised methods have emerged as a promis-
4"
ABSTRACT,0.004985044865403789,"ing solution. In this work, we focus on the strength of curve-adjustment-based
5"
ABSTRACT,0.005982053838484547,"approaches to tackle unsupervised methods. The majority of existing unsupervised
6"
ABSTRACT,0.006979062811565304,"curve-adjustment approaches iteratively estimate higher order curve parameters
7"
ABSTRACT,0.007976071784646061,"to enhance the exposure of images while efficiently preserving the details of the
8"
ABSTRACT,0.00897308075772682,"images. However, the convergence of the enhancement procedure cannot be guar-
9"
ABSTRACT,0.009970089730807577,"anteed, leading to sensitivity to the number of iterations and limited performance.
10"
ABSTRACT,0.010967098703888335,"To address this problem, we consider the iterative curve-adjustment update process
11"
ABSTRACT,0.011964107676969093,"as a dynamic system and formulate it as a Neural Ordinary Differential Equations
12"
ABSTRACT,0.01296111665004985,"(NODE) for the first time, and this allows us to learn a continuous dynamics of
13"
ABSTRACT,0.013958125623130608,"the latent image. The strategy of utilizing NODE to leverage continuous dynamics
14"
ABSTRACT,0.014955134596211365,"in iterative methods enhances unsupervised learning and aids in achieving better
15"
ABSTRACT,0.015952143569292122,"convergence compared to discrete-space approaches. Consequently, we achieve
16"
ABSTRACT,0.01694915254237288,"state-of-the-art performance in unsupervised low-light image enhancement across
17"
ABSTRACT,0.01794616151545364,"various benchmark datasets.
18"
INTRODUCTION,0.018943170488534396,"1
Introduction
19"
INTRODUCTION,0.019940179461615155,"Images taken in various low-light environments suffer from insufficient light, leading to the capture
20"
INTRODUCTION,0.020937188434695914,"of limited information by the camera’s image sensor. Therefore, many studies have been conducted
21"
INTRODUCTION,0.02193419740777667,"to improve the quality of the low-light images and achieve images with optimal exposure levels.
22"
INTRODUCTION,0.022931206380857428,"In particular, recent supervision-based deep learning approaches [1, 2, 3] have shown remarkable
23"
INTRODUCTION,0.023928215353938187,"performance in enhancing low-light images. However, the process of collecting pairs of low-light
24"
INTRODUCTION,0.024925224327018942,"scenes and their corresponding ground-truth images for supervised learning is time consuming and
25"
INTRODUCTION,0.0259222333000997,"resource intensive. As a result, unsupervised approaches that rely solely on low-light images have
26"
INTRODUCTION,0.026919242273180457,"been proposed to address this problem.
27"
INTRODUCTION,0.027916251246261216,"Among many unsupervised low-light image enhancement approaches, curve-adjustment-based meth-
28"
INTRODUCTION,0.028913260219341975,"ods, conventionally used in photo editing software (e.g., Photoshop), have received much attention.
29"
INTRODUCTION,0.02991026919242273,"After the introduction of first learning-based curve-adjustment work by Yuan and Sun [7], iter-
30"
INTRODUCTION,0.03090727816550349,"ative curve-adjustment-based methods have been explored in various subsequent studies. These
31"
INTRODUCTION,0.031904287138584245,"unsupervised methods achieve enhancement without using the ground-truth images by fitting the
32"
INTRODUCTION,0.03290129611166501,"brightness values of pixels in the input image to specific curves. In addition, it is advantageous to
33"
INTRODUCTION,0.03389830508474576,"preserve local structural information adaptively by allowing efficient pixel-by-pixel computations.
34"
INTRODUCTION,0.03489531405782652,"For example, ZeroDCE [6, 8] introduced a fast and lightweight neural network to predict pixel-wise
35"
INTRODUCTION,0.03589232303090728,"curve parameter maps within a fixed iteration step. In addition, ReLLIE [9] produced more accurate
36 (a)"
INTRODUCTION,0.036889332003988036,"Input Image
ZeroDCE
RetinexFormer
CLODE (Ours) (b)"
INTRODUCTION,0.03788634097706879,"Figure 1: (a) Quantitative Evaluation: The average PSNR values on the LSRW [4] and LOL [5],
together with the respective parameter numbers for each model. (b) Visual Comparisons with
ZeroDCE [6] (unsupervised), RetinexFormer [2] (supervised) and proposed CLODE (unsupervised)."
INTRODUCTION,0.038883349950149554,"image enhancement results by using reinforcement learning to predict the curve parameter map at
37"
INTRODUCTION,0.03988035892323031,"each iteration step, with users able to adjust the number of iterations.
38"
INTRODUCTION,0.040877367896311065,"In general, these curve-adjustment-based methods, which have fewer parameters, offer the advantage
39"
INTRODUCTION,0.04187437686939183,"of fast and efficient training and also demonstrate the effectiveness of using higher-order curves
40"
INTRODUCTION,0.04287138584247258,"for low-light image adjustment. However, conventional iterative approaches in discrete-space with
41"
INTRODUCTION,0.04386839481555334,"fixed update steps do not arrive at the optimal solution and cannot guarantee convergence of the
42"
INTRODUCTION,0.0448654037886341,"optimization. Therefore, we alleviate this problem in the discrete-space updating process of existing
43"
INTRODUCTION,0.045862412761714856,"methods. In doing so, we bring out the strengths of curve fitting methods by reformulating the
44"
INTRODUCTION,0.04685942173479561,"iterative update formula into ordinary differential equations, which allows the iterative approach to be
45"
INTRODUCTION,0.047856430707876374,"transformed from discrete-space to continuous-space and find input-specific higher-order curves until
46"
INTRODUCTION,0.04885343968095713,"convergence within a specified tolerance. To be specific, we present the Neural Ordinary Differential
47"
INTRODUCTION,0.049850448654037885,"Equations (NODE) model for the low-light enhancement task for the first time. By solving the
48"
INTRODUCTION,0.05084745762711865,"NODE problem using conventional ODE solvers, we obtain better approximate solutions to the
49"
INTRODUCTION,0.0518444666001994,"curve-adjustment problem, producing more accurate results than conventional results from iterative
50"
INTRODUCTION,0.05284147557328016,"updates in discrete-space by exploring the continuous exposure dynamics. In this work, we introduce
51"
INTRODUCTION,0.053838484546360914,"Continuous exposure learning for Low-light image enhancement using neural Ordinary Differential
52"
INTRODUCTION,0.054835493519441676,"Equations (CLODE), which is the first dynamic system for low-light image enhancement. Our main
53"
INTRODUCTION,0.05583250249252243,"contributions can be summarized as follows:
54"
INTRODUCTION,0.05682951146560319,"• CLODE is the first approach to formulate the higher-order curve estimation problem as a
55"
INTRODUCTION,0.05782652043868395,"NODE problem, enabling effective and accurate solutions with standard ODE solvers.
56"
INTRODUCTION,0.058823529411764705,"• By transforming the discrete update formula into NODE, which is solvable in continuous-
57"
INTRODUCTION,0.05982053838484546,"space, we significantly enhance the unsupervised low-light image enhancement results across
58"
INTRODUCTION,0.06081754735792622,"various benchmark datasets as shown in Fig. 1. This effectively bridges the performance
59"
INTRODUCTION,0.06181455633100698,"gap between supervised and unsupervised approaches.
60"
INTRODUCTION,0.06281156530408774,"• CLODE also offers user controllability without altering the network architecture, enabling
61"
INTRODUCTION,0.06380857427716849,"users to manually adjust the desired level of exposure as needed.
62"
RELATED WORKS,0.06480558325024925,"2
Related works
63"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.06580259222333001,"2.1
Unsupervised Low-light Image Enhancement
64"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.06679960119641076,"Obtaining well-exposed ground-truth images paired with corresponding low-light images is inherently
65"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.06779661016949153,"challenging, which limits the use of supervised learning in low-light image enhancement. To address
66"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.06879361914257229,"this limitation, many unsupervised methods have been developed to tackle the problem. First, there
67"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.06979062811565304,"are some approaches [10, 11, 12, 13] that utilize the principles of retinex-theory. Among them,
68"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.0707876370887338,"PairLIE [13] utilizes retinex-theory to identify the reflectance and illumination, and employs gamma
69"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.07178464606181456,"correction with user-defined gamma values to enhance the illumination. In addition, UDCN [14] and
70"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.07278165503489531,"HEP [15] use histogram equalization results as a reference for exposure enhancement. Moreover,
71"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.07377866400797607,"recent approaches using GANs have shown remarkable improvements by additionally utilizing
72"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.07477567298105683,"unpaired images of normal exposed [16, 17]. Lastly, there are curve-adjustment-based methods [6, 8,
73"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.07577268195413758,"18, 9] that transform images through tone mapping. These methods have advanced the curve-fitting
74"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.07676969092721835,"techniques from traditional editing tools into deep learning-based approaches, enhancing images by
75"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.07776669990029911,"predicting the fitting curves pixel-by-pixel. By repeating the pixel-wise curve fitting and exposure
76"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.07876370887337986,"enhancement for a fixed number of iterations in discrete-space, these approaches aim to handle locally
77"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.07976071784646062,"varying exposure levels (i.e., single image with both underexposed and overexposed areas) in an
78"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.08075772681954138,"unsupervised manner. Our CLODE also follows this unsupervised curve-adjustment-based method
79"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.08175473579262213,"and reformulates the curve-fitting problem into a neural ordinary differential equation (NODE). By
80"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.08275174476570289,"solving the NODE problem using conventional ODE solvers, we increase the accuracy of curve fitting
81"
UNSUPERVISED LOW-LIGHT IMAGE ENHANCEMENT,0.08374875373878365,"and thus significantly improve the performance of low-light image enhancement.
82"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.0847457627118644,"2.2
Neural Ordinary Differential Equations
83"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.08574277168494517,"An ordinary differential equation (ODE) is a fundamental concept in mathematics that describes how
84"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.08673978065802593,"a function changes with respect to a single variable. It captures the relationship between a function
85"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.08773678963110668,"and its derivatives, providing a powerful tool for modeling dynamic systems, such as Newton’s
86"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.08873379860418744,"Second Law of Motion. To effectively apply the strength of ordinary differential equations to the
87"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.0897308075772682,"deep learning model, the concept of neural ordinary differential equations (NODE) is introduced
88"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.09072781655034895,"in [19]. The use of NODE facilitates model definition and evaluation, highlighting its effectiveness in
89"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.09172482552342971,"parameter efficiency, adaptive computation, and modeling continuous data. In order to effectively
90"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.09272183449651047,"capture more complicated functions, the Augmented Neural ODE (ANODE) [20] has been introduced.
91"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.09371884346959122,"Furthermore, for seamless continuous time-series modeling, Latent ODE [21] is proposed and recently,
92"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.09471585244267199,"ClimODE [22] proposed a continuous-time NODE models for numerical weather prediction. To be
93"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.09571286141575275,"specific, in the field of computer vision, the Vid-ODE approach [23] has been introduced to generate
94"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.0967098703888335,"continuous-time videos. NODEO [24] has presented a versatile architecture tailored for deformable
95"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.09770687936191426,"image registration, and a temporal deformation model using the capabilities of NODE has been
96"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.09870388833499502,"developed in [25] to address the challenges associated with future prediction tasks in the context
97"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.09970089730807577,"of 4D reconstruction. With advantages like continuous-space modeling, adaptive computation, and
98"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.10069790628115653,"memory efficiency, NODE [19] is utilized in various deep learning tasks. However, it has not been
99"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.1016949152542373,"extensively explored in the field of image restoration. While NODE-SR [26] has been introduced to
100"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.10269192422731804,"address the arbitrary scale super-resolution problem, our methodology marks the first application in
101"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.1036889332003988,"image exposure enhancement. In contrast to NODE-SR [26], which learns the continuous variation of
102"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.10468594217347957,"the scaling factor for the arbitrary scale super-resolution problem, our CLODE learns the continuous
103"
NEURAL ORDINARY DIFFERENTIAL EQUATIONS,0.10568295114656032,"variation of image exposure through curve-adjustment.
104"
PROPOSED METHOD,0.10667996011964108,"3
Proposed Method
105"
PRELIMINARY,0.10767696909272183,"3.1
Preliminary
106"
PRELIMINARY,0.10867397806580259,"In photo editing applications, the curve-adjustment method is often used to adjust the tone of
107"
PRELIMINARY,0.10967098703888335,"input images and provides effective exposure control. While this method is useful for pixel-wise
108"
PRELIMINARY,0.1106679960119641,"manipulation, it is not well suited for images that contain areas of extreme over- or under-exposure.
109"
PRELIMINARY,0.11166500498504486,"Additionally, a notable drawback of this approach is its reliance on manual adjustments (e.g., the
110"
PRELIMINARY,0.11266201395812563,"number of updates) by the user for each input image. This can be time-consuming and potentially less
111"
PRELIMINARY,0.11365902293120637,"accurate in certain scenarios. To address this problem, Yuan and Sun [7] have proposed a solution
112"
PRELIMINARY,0.11465603190428714,"that aims to mitigate the limitations of manual adjustments. They introduced an automated approach
113"
PRELIMINARY,0.1156530408773679,"that involves estimating an image-specific S-shaped nonlinear tone curve (referred to as an S-curve)
114"
PRELIMINARY,0.11665004985044865,"tailored to each input image. Specifically, for a given low-light image I0, where each pixel value is in
115"
PRELIMINARY,0.11764705882352941,"the range [0, 1], the S-curve formula for the enhanced image I
′
0 can be represented as follows:
116"
PRELIMINARY,0.11864406779661017,"I
′
0 = I0 + ϕs · P∆(I0) −ϕh · P∆(1 −I0),
(1)"
PRELIMINARY,0.11964107676969092,"where ϕs and ϕh represent parameters for the amount of shadow and highlight, respectively. The
117"
PRELIMINARY,0.12063808574277168,"function P∆serves as an increasing function for the adjustment that manipulates the intensity of
118"
PRELIMINARY,0.12163509471585245,"individual pixels within the input of the function.
119"
PRELIMINARY,0.1226321036889332,"While Eq.1 allows for adjusting the brightness of an entire image using a single global curve parameter,
120"
PRELIMINARY,0.12362911266201396,"existing iterative curve-adjustments approaches [6, 8, 9, 27] operate on a pixel-wise basis of the input
121"
PRELIMINARY,0.12462612163509472,"images. Furthermore, they introduce the necessity of higher-order curves, which enhances images by
122"
PRELIMINARY,0.12562313060817548,"fitting higher-order curves for fixed iteration steps while using a deep learning model to predict curve
123"
PRELIMINARY,0.12662013958125623,"parameters on a pixel-by-pixel basis. Specifically the update formula enhances an image In at the
124"
PRELIMINARY,0.12761714855433698,"n-th step to an image In+1 at the next step as follows:
125"
PRELIMINARY,0.12861415752741776,"In+1 = In + An ⊗In ⊗(1 −In),
(2)"
PRELIMINARY,0.1296111665004985,"where An ∈RC×H×W represents a pixel-wise varying curve parameter map and C, H, and W
126"
PRELIMINARY,0.13060817547357925,"represent the number of channels, height, and width of the image In, and ⊗operation denotes element-
127"
PRELIMINARY,0.13160518444666003,"wise multiplication. Note that, the elements of An corresponding to the curve parameters at each
128 𝐼!
𝐼"" 𝐼!"
PRELIMINARY,0.13260219341974078,"""𝐼!
𝒜!
Eq. 9
𝑑𝐼! 𝑑𝑡 𝑓"" 𝐼"" . . . 𝐼#"
PRELIMINARY,0.13359920239282153,"𝐼# = 𝑂𝐷𝐸_𝑆𝑜𝑙𝑣𝑒𝑟(𝐼$, 0, 𝑇, 𝑓"") 𝑡 𝑓"""
PRELIMINARY,0.1345962113659023,𝒇𝜽: ODEfunc
PRELIMINARY,0.13559322033898305,(b) ODEfunc
PRELIMINARY,0.1365902293120638,"Noise Removal 
Module (𝑔)"
PRELIMINARY,0.13758723828514458,"Curve Parameter 
Estimation (ℎ) Eq.5"
PRELIMINARY,0.13858424725822532,"Trajectory 
of improvement + +
+"
PRELIMINARY,0.13958125623130607,𝐼# = 𝐼$ + 5 $ #
PRELIMINARY,0.14057826520438685,"𝑓"" 𝐼!, 𝑡𝑑𝑡 . . . 𝑓"" . . . + ≈ 𝑓"""
PRELIMINARY,0.1415752741774676,"(a) Continuous update procedure of CLODE
Figure 2: (a) Illustration of continuous update procedure of CLODE. Optimal iterative update can
be achieved through the ODE equation. (b) Illustration of our ODEfunc fθ. ODEfunc contains the
Noise Removal (g), Curve Parameter Estimation (h) module, and Eq. 9 to obtain the derivative value.
Please refer to Appendix A.1.2 for more details."
PRELIMINARY,0.14257228315054835,"pixel location are in the range [−1, 1] and determine the quadratic curve for the pixel-wise exposure
129"
PRELIMINARY,0.14356929212362912,"adjustment during the enhancement process. Conventional curve-adjustment methods [6, 8, 9, 18]
130"
PRELIMINARY,0.14456630109670987,"iteratively follow this process for N times, fitting an appropriate higher-order curve to produce
131"
PRELIMINARY,0.14556331006979062,"the final well-exposed output image. On the contrary, our CLODE performs curve adjustment for
132"
PRELIMINARY,0.1465603190428714,"image enhancement by reformulating Eq. 2 as an ordinary differential equation. This approach
133"
PRELIMINARY,0.14755732801595214,"facilitates memory-efficient training and yields more accurate results through adaptive computation
134"
PRELIMINARY,0.1485543369890329,"using modern ODE solvers.
135"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.14955134596211367,"3.2
Continuous Exposure Learning for Low-light Image Enhancement using Neural ODEs
136"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.15054835493519442,"Although conventional curve-adjustment-based iterative methods offer advantages in terms of
137"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.15154536390827517,"lightweight network architecture and local robustness, these approaches cannot guarantee con-
138"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.15254237288135594,"vergence of the update process. ZeroDCE [6] empirically determines the iteration number N and
139"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1535393818544367,"enhances low-light images by iterating the curve-adjustment formula 8 (=N) times. While ReL-
140"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.15453639082751744,"LIE [9] provides users with optional flexibility, it requires manual selection of the value of N for
141"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.15553339980059822,"each input image to further improve image quality. To tackle this challenge in optimization, we
142"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.15653040877367896,"reformulate the curve-adjustment-based formula outlined in Eq. 2 as a Neural Ordinary Differential
143"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1575274177467597,"Equations (NODE). Then, we can solve the NODE with conventional ODE solvers (e.g., Euler, RK4,
144"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1585244267198405,"dopri5) which guarantees the convergence of loss within tolerances. Specifically, we reformulate
145"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.15952143569292124,"the original curve-adjustment-based formula by introducing a continuous state t instead of using the
146"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.16051844466600199,"discrete state n as follows:
147
It+1 = It + fθ(It, t),
(3)
where fθ is a neural network with trainable parameters θ that satisfies fθ(It, t) = At ⊗It ⊗(1 −It).
148"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.16151545363908276,"Then, we can parameterize the derivative of the enhanced image during the update using the network
149"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1625124626121635,"fθ if the continuous update step is very small, and it is given by,
150 dIt"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.16350947158524426,"dt = fθ(It, t).
(4)"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.16450648055832504,"By transforming the original curve fitting problem into a NODE problem with an initial condition I0,
151"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.16550348953140578,"we can estimate not only the derivative value of each state but also recover the enhanced image by
152"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.16650049850448653,"solving the problem, and the initial value problem is given by,
153"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1674975074775673,"IT = I0 +
Z T"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.16849451645064806,"0
fθ(It, t)dt,
(5)"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1694915254237288,"where IT denotes the well-exposed image at the final state T. Finally, the low-light image enhance-
154"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.17048853439680958,"ment process to output IT is accomplished by using the ODE solver as:
155"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.17148554336989033,"IT = ODE_Solver(I0, [0, T], fθ),
(6)
where ODE_Solver denotes a conventional algorithm for solving the ordinary differential equations.
156"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.17248255234297108,"In our experiments, CLODE adopts the well-known dopri5 (Dormand-Prince 5th order Runge-Kutta)
157"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.17347956131605186,"as an adaptive ODE solver, that determines an input-specific number of iterations for each input and
158"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1744765702891326,"dynamically adjusts the step size. Using the adaptive solver, we can adaptively compute the optimal
159"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.17547357926221335,"state for different exposure levels, thereby enabling a more accurate approximation of the solution.
160"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.17647058823529413,"This is in contrast to conventional methods, which use the same fixed number of iterations for all
161"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.17746759720837488,"input images and cannot guarantee optimality and convergence. To the best of our knowledge, our
162"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.17846460618145563,"approach is the first to define the low-light image enhancement problem as a novel NODE problem
163"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1794616151545364,"with an initial condition.
164"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.18045862412761715,"3.2.1
ODE function (ODEfunc)
165"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1814556331006979,"We can solve the NODE problem in Eq. 5 by integrating fθ over the time interval [0, T] with the given
166"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.18245264207377868,"initial value I0 (e.g., a low-light image). In practice, conventional ODE solvers are used to address
167"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.18344965104685942,"this problem, iteratively enhancing the low-light images using Eq. 3. In Fig. 2(a), we illustrate the
168"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.18444666001994017,"continuous update procedure of our CLODE approach. Notably, the ODE function (ODEfunc) fθ
169"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.18544366899302095,"computes continuous dynamics of the latent image and is a key element in the update procedure. The
170"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1864406779661017,"detailed configuration of our ODEfunc fθ is shown in Fig. 2(b). To be specific, our ODEfunc includes
171"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.18743768693918245,"Noise Removal (g) and the Curve Parameter Estimation (h) modules with trainable parameters, and
172"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.18843469591226322,outputs dIt
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.18943170488534397,"dt , the continuous dynamics of It. Please refer to Appendix A.1.2 for more details.
173"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.19042871385842472,"Noise Removal
In the ODEfunc, we first employ a pre-processing step to eliminate the artifacts
174"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1914257228315055,"from It and generate the denoised image ˜It in order to produce more accurate curve adjustment
175"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.19242273180458624,"parameters At. To minimize computational costs within the fθ, we employ a simple and lightweight
176"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.193419740777667,"three-layer convolutional neural network g as our Noise Removal module, expressed as follows:
177"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.19441674975074777,"˜It = g(It).
(7)"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.19541375872382852,"The refined image ˜It is then used as the input to the subsequent Curve Parameter Estimation stage.
178"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.19641076769690927,"Curve Parameter Estimation
Inspired by [7, 28], to enhance both under- and over- exposed
179"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.19740777666999004,"areas, we not only use the denoised image ˜It and its inverted version (1 −˜It) as inputs to the Curve
180"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.1984047856430708,"Parameter Estimation module. The formulation is given by:
181"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.19940179461615154,"At = h(˜It, 1 −˜It),
(8)"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.20039880358923232,"where At represents the curve parameter map at t, and h represents the Curve Parameter Estimation
182"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.20139581256231306,"module. For efficacy, this module is also a lightweight convolutional neural network. In particular,
183"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.2023928215353938,"we apply layer normalization [29] to all intermediate features. Notably, the use of layer normalization
184"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.2033898305084746,"enables CLODE to handle the diverse exposure ranges of input images. Furthermore, all convolutional
185"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.20438683948155534,"layers within the Curve Parameter Estimation module h take the continuous state t as a conditional
186"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.2053838484546361,"input, allowing for time-varying outputs during the integration interval [0, T] as in [19].
187"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.20638085742771686,"Continuous Dynamics
Lastly, the derivative value of the one-step state at t is computed in our
188"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.2073778664007976,"ODEfunc, and it is expressed as follows:
189 dIt"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.20837487537387836,"dt = At ⊗It ⊗(1 −It).
(9)"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.20937188434695914,"Notably, unlike conventional curve-adjustment-based update formulas that discretize update steps,
190"
CONTINUOUS EXPOSURE LEARNING FOR LOW-LIGHT IMAGE ENHANCEMENT USING NEURAL ODES,0.21036889332003988,"our continuous dynamics allows the desired level of accuracy and produces more accurate solutions.
191"
INFERENCE PROCESS OF CLODE,0.21136590229312063,"3.3
Inference Process of CLODE
192"
INFERENCE PROCESS OF CLODE,0.2123629112662014,"Inference Process
Given a low-light input image I0, CLODE undergoes successive image enhance-
193"
INFERENCE PROCESS OF CLODE,0.21335992023928216,"ment through fθ until convergence within the specified tolerance of the ODE solvers, resulting in a
194"
INFERENCE PROCESS OF CLODE,0.2143569292123629,"well-exposed image IT . Note that, the output image IT may contain some noise that is amplified
195"
INFERENCE PROCESS OF CLODE,0.21535393818544366,"during the image enhancement process. Therefore, we use the noise-free image ˜IT as our final
196"
INFERENCE PROCESS OF CLODE,0.21635094715852443,"outcome by applying the Noise Removal module g.
197"
INFERENCE PROCESS OF CLODE,0.21734795613160518,"User Controllable Design
CLODE learns the low-light exposure adjustment mechanism in the
198"
INFERENCE PROCESS OF CLODE,0.21834496510468593,"continuous-space, and is trained to output IT by integrating the states from 0 to T in Eq. 5 using a
199"
INFERENCE PROCESS OF CLODE,0.2193419740777667,"fixed T. However, as shown in Fig. 3, users can manually adjust the integration interval by changing
200"
INFERENCE PROCESS OF CLODE,0.22033898305084745,"the final state value T at the test stage, allowing them to output images with the preferred exposure
201"
INFERENCE PROCESS OF CLODE,0.2213359920239282,"level and even produce images darker than the input. In practice, by controlling the final state from
202"
INFERENCE PROCESS OF CLODE,0.22233300099700898,"−(T + ∆t) to (T + ∆t), the exposure level of the output image can be easily controlled to provide a
203"
INFERENCE PROCESS OF CLODE,0.22333000997008973,"more user-friendly exposure level.
204"
INFERENCE PROCESS OF CLODE,0.22432701894317048,"𝑰𝟎
""𝑰𝑻
""𝑰#𝑻
""𝑰$(𝑻$𝚫𝒕)
""𝑰#(𝑻$𝚫𝒕) …
…"
INFERENCE PROCESS OF CLODE,0.22532402791625125,"Figure 3: Illustration of User Controllable Design. By manually changing the integration interval
from −(T + ∆t) to +(T + ∆t), ours can produce results with different exposure levels."
ZERO-REFERENCE LOSS FUNCTIONS,0.226321036889332,"3.4
Zero-Reference Loss Functions
205"
ZERO-REFERENCE LOSS FUNCTIONS,0.22731804586241275,"To address the challenge posed by the lack of ground truth, we use five zero-reference loss functions
206"
ZERO-REFERENCE LOSS FUNCTIONS,0.22831505483549352,"for unsupervised training.
207"
ZERO-REFERENCE LOSS FUNCTIONS,0.22931206380857427,"Spatial Consistency Loss
While the given low-light input image I0 is enhanced during the update
208"
ZERO-REFERENCE LOSS FUNCTIONS,0.23030907278165502,"procedure, maintaining spatial consistency in the pixel brightness order is crucial for preserving
209"
ZERO-REFERENCE LOSS FUNCTIONS,0.2313060817547358,"image details. Specifically, we measure the difference in spatial consistency between the input image
210"
ZERO-REFERENCE LOSS FUNCTIONS,0.23230309072781655,"I0 and our prediction IT by comparing the differences in neighboring pixel values. Similar to [6],
211"
ZERO-REFERENCE LOSS FUNCTIONS,0.2333000997008973,"we compute the spatial consistency after applying 4-by-4 average pooling to both I0 and IT , and the
212"
ZERO-REFERENCE LOSS FUNCTIONS,0.23429710867397807,"spatial consistency loss Lspa is expressed as:
213"
ZERO-REFERENCE LOSS FUNCTIONS,0.23529411764705882,"Lspa = 1 K K
X i=1 X"
ZERO-REFERENCE LOSS FUNCTIONS,0.23629112662013957,"j∈Ω(i)
(|m4(IT )i −m4(IT )j| −|m4(I0)i −m4(I0)j|)2.
(10)"
ZERO-REFERENCE LOSS FUNCTIONS,0.23728813559322035,"The 4-by-4 average pooling operation is denoted as m4(·) and Ω(i) includes neighboring pixels in
214"
ZERO-REFERENCE LOSS FUNCTIONS,0.2382851445663011,"four directions (left, right, top, bottom) centered at position i. The normalization factor K denotes the
215"
ZERO-REFERENCE LOSS FUNCTIONS,0.23928215353938184,"number of pixels in the reduced image after the pooling operation, and K is given by H 4 × W"
ZERO-REFERENCE LOSS FUNCTIONS,0.24027916251246262,"4 × C.
216"
ZERO-REFERENCE LOSS FUNCTIONS,0.24127617148554337,"Exposure Loss
To enforce a consistent exposure level across pixels, conventional unsupervised
217"
ZERO-REFERENCE LOSS FUNCTIONS,0.24227318045862412,"methods incorporate exposure guidance into the loss function [6]. Similarly, we introduce a desired
218"
ZERO-REFERENCE LOSS FUNCTIONS,0.2432701894317049,"exposure level parameter E and define the exposure loss Lexp as:
219"
ZERO-REFERENCE LOSS FUNCTIONS,0.24426719840478564,"Lexp = ||m16(IT ) −E||2
2.
(11)"
ZERO-REFERENCE LOSS FUNCTIONS,0.2452642073778664,"In our experiments, we set the exposure level E to 0.6, which corresponds to the gray level in the
220"
ZERO-REFERENCE LOSS FUNCTIONS,0.24626121635094717,"RGB color space. To maintain the overall exposure level in the results, we minimize the difference
221"
ZERO-REFERENCE LOSS FUNCTIONS,0.2472582253240279,"between the pixel values of the predicted image IT and the desired exposure level E after performing
222"
ZERO-REFERENCE LOSS FUNCTIONS,0.24825523429710866,"a 16-by-16 average pooling operation m16(·) on the output image IT .
223"
ZERO-REFERENCE LOSS FUNCTIONS,0.24925224327018944,"Color Constancy Loss
In conventional zero-reference methods, two main approaches are used to
224"
ZERO-REFERENCE LOSS FUNCTIONS,0.2502492522432702,"enforce spatial color constancy: one based on the retinex-theory, and the other based on the Gray-
225"
ZERO-REFERENCE LOSS FUNCTIONS,0.25124626121635096,"World hypothesis in [30]. In this work, the color constancy loss Lcol is based on the Gray-World
226"
ZERO-REFERENCE LOSS FUNCTIONS,0.2522432701894317,"hypothesis as in [6, 15], and the formulation is given by,
227"
ZERO-REFERENCE LOSS FUNCTIONS,0.25324027916251246,"Lcol = (R −B)2 + (R −G)2 + (G −B)2,
(12)"
ZERO-REFERENCE LOSS FUNCTIONS,0.2542372881355932,"where R, G, and B are the mean pixel values of the red, green, and blue channels in the predicted
228"
ZERO-REFERENCE LOSS FUNCTIONS,0.25523429710867396,"image IT , respectively. We minimize the color constancy loss Lcol to correct the potential color
229"
ZERO-REFERENCE LOSS FUNCTIONS,0.25623130608175476,"deviations in the enhanced image.
230"
ZERO-REFERENCE LOSS FUNCTIONS,0.2572283150548355,"Parameter Regularization Loss
To prevent rapid changes of pixel values in nearby regions, we
231"
ZERO-REFERENCE LOSS FUNCTIONS,0.25822532402791626,"employ the spatial regularization to enforce smoothness among neighboring curve parameter values
232"
ZERO-REFERENCE LOSS FUNCTIONS,0.259222333000997,"in At, and the formulation is given by,
233"
ZERO-REFERENCE LOSS FUNCTIONS,0.26021934197407776,"Lparam = (|∇xA0| + |∇yA0|)2 + . . . + (|∇xAT −1| + |∇yAT −1|)2,
(13)"
ZERO-REFERENCE LOSS FUNCTIONS,0.2612163509471585,"where the linear operations ∇x and ∇y compute the horizontal and vertical gradients from the
234"
ZERO-REFERENCE LOSS FUNCTIONS,0.2622133599202393,"parameter map At, respectively. For better understanding, we represent T −1 as the stage before the
235"
ZERO-REFERENCE LOSS FUNCTIONS,0.26321036889332006,"final enhancement. We employ the parameter regularization loss at each update step (e.g., red points
236"
ZERO-REFERENCE LOSS FUNCTIONS,0.2642073778664008,"in Fig. 2 (a)) and accumulate the loss while solving the NODE problem.
237"
ZERO-REFERENCE LOSS FUNCTIONS,0.26520438683948155,"Noise Removal Loss
To estimate a spatially smooth At regardless of the noise in the image It, we
238"
ZERO-REFERENCE LOSS FUNCTIONS,0.2662013958125623,"use the Noise Removal module (g) to remove the noise. To train the Noise Removal module, we
239"
ZERO-REFERENCE LOSS FUNCTIONS,0.26719840478564305,"utilize a self-supervision-based loss Lnoise that follows the Noise2Noise approaches [31, 32, 33].
240"
ZERO-REFERENCE LOSS FUNCTIONS,0.26819541375872386,"Specifically, we employ the loss introduced in Zeroshot-N2N [33]. Our Lnoise has two components
241"
ZERO-REFERENCE LOSS FUNCTIONS,0.2691924227318046,"at state t: the residual loss Lt
res and the consistency loss Lt
cons. We minimize these losses using two
242"
ZERO-REFERENCE LOSS FUNCTIONS,0.27018943170488535,"different down-samplers; D1 and D2. Notably, D1 and D2 represent fixed 2D convolutional kernels:
243

0.5
0
0
0.5"
ZERO-REFERENCE LOSS FUNCTIONS,0.2711864406779661,"
and

0
0.5
0.5
0"
ZERO-REFERENCE LOSS FUNCTIONS,0.27218344965104685,"
, respectively. Notably, these kernels are used for downsampling through
244"
ZERO-REFERENCE LOSS FUNCTIONS,0.2731804586241276,"convolutions with a stride of two. First, our Lt
res fits the noise within It through a symmetric loss
245"
ZERO-REFERENCE LOSS FUNCTIONS,0.2741774675972084,"function similar to the approach in [34] and it yields:
246"
ZERO-REFERENCE LOSS FUNCTIONS,0.27517447657028915,"Lt
res = 1"
ZERO-REFERENCE LOSS FUNCTIONS,0.2761714855433699,"2(||D1(It) −g(D1(It)) −D2(It)||2
2 + ||D2(It) −g(D2(It)) −D1(It)||2
2).
(14)"
ZERO-REFERENCE LOSS FUNCTIONS,0.27716849451645065,"Next, as in [33], Lt
cons ensures spatial consistency by maintaining similarity in noise distributions,
247"
ZERO-REFERENCE LOSS FUNCTIONS,0.2781655034895314,"even if the order of denoising and downsampling is altered. Specifically, Lt
cons also adopts a
248"
ZERO-REFERENCE LOSS FUNCTIONS,0.27916251246261214,"symmetric loss and is defined as at each update step (e.g., red points in Fig. 2 (a)):
249"
ZERO-REFERENCE LOSS FUNCTIONS,0.28015952143569295,"Lt
cons = 1"
ZERO-REFERENCE LOSS FUNCTIONS,0.2811565304087737,"2(||D1(It)−g(D1(It))−D1(It −g(It))||2
2 +||D2(It)−g(D2(It))−D2(It −g(It))||2
2).
(15)
Therefore, our final noise removal loss Lnoise can be represented accumulating during the update
250"
ZERO-REFERENCE LOSS FUNCTIONS,0.28215353938185445,"procedure as:
251"
ZERO-REFERENCE LOSS FUNCTIONS,0.2831505483549352,"Lnoise = (L0
res + L0
cons) + . . . + (LT −1
res + LT −1
cons).
(16)
As with Eq. 13, we represent T −1 as the stage before the final enhancement. A more detailed
252"
ZERO-REFERENCE LOSS FUNCTIONS,0.28414755732801594,"description of the noise removal loss is provided in Appendix A.4.
253"
ZERO-REFERENCE LOSS FUNCTIONS,0.2851445663010967,"Final Objective Function
The final objective function to optimize is given as follows:
254"
ZERO-REFERENCE LOSS FUNCTIONS,0.28614157527417744,"Ltotal = wspa · Lspa + wexp · Lexp + wcol · Lcol + wparam · Lparam + wnoise · Lnoise,
(17)"
ZERO-REFERENCE LOSS FUNCTIONS,0.28713858424725824,"where wspa, wexp, wcol, wparam, and wnoise are hyper-parameters used to control the relative
255"
ZERO-REFERENCE LOSS FUNCTIONS,0.288135593220339,"significance of each associated loss during the training process.
256"
EXPERIMENTS,0.28913260219341974,"4
Experiments
257"
IMPLEMENTATION DETAILS,0.2901296111665005,"4.1
Implementation Details
258"
IMPLEMENTATION DETAILS,0.29112662013958124,"Please refer to Appendix A.1 for more implementation details and training scheme. The code will be
259"
IMPLEMENTATION DETAILS,0.292123629112662,"available upon acceptance.
260"
EXPERIMENTAL SETUP,0.2931206380857428,"4.2
Experimental Setup
261"
EXPERIMENTAL SETUP,0.29411764705882354,"In this work, we use the LOL [5] and SICE [35] Part1 datasets for training. The results of low-light
262"
EXPERIMENTAL SETUP,0.2951146560319043,"image enhancement are evaluated on the LOL and LSRW [4] benchmark datasets. In addition,
263"
EXPERIMENTAL SETUP,0.29611166500498504,"the SICE [35] Part2 dataset is used as a benchmark dataset for evaluation under various exposure
264"
EXPERIMENTAL SETUP,0.2971086739780658,"conditions. SICE Part2 contains 229 image sequences with different exposure levels, and we use
265"
EXPERIMENTAL SETUP,0.29810568295114653,"the entire sequences as the evaluation dataset. By default, each comparison model uses its official
266"
EXPERIMENTAL SETUP,0.29910269192422734,"network weights. In cases where the official code is available but weights are not provided, the
267"
EXPERIMENTAL SETUP,0.3000997008973081,"models are retrained using the official code and settings, except for ReLLIE [9]. We present the
268"
EXPERIMENTAL SETUP,0.30109670987038883,"performance of ReLLIE on the LOL dataset as reported in their original manuscript.
269"
QUANTITATIVE COMPARISONS,0.3020937188434696,"4.3
Quantitative Comparisons
270"
QUANTITATIVE COMPARISONS,0.30309072781655033,"First, we quantitatively compare the performance of low-light image enhancement on different
271"
QUANTITATIVE COMPARISONS,0.3040877367896311,"datasets. Notably, in the experimental results, CLODE represents our proposed method without
272"
QUANTITATIVE COMPARISONS,0.3050847457627119,"requiring additional user input (by default), while CLODE† represents the result of adjusting the final
273"
QUANTITATIVE COMPARISONS,0.30608175473579263,"state T to the user’s preferred level, as introduced in Sec. 3.3.
274"
QUANTITATIVE COMPARISONS,0.3070787637088734,"In Table 1, we compare the low-light image enhancement performance on the LSRW [4] and LOL [5]
275"
QUANTITATIVE COMPARISONS,0.30807577268195413,"benchmark datasets in terms of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM).
276"
QUANTITATIVE COMPARISONS,0.3090727816550349,"The term ""GT Mean"" refers to the evaluation method used by KinD [36] and LLFlow [1], which
277"
QUANTITATIVE COMPARISONS,0.3100697906281156,"matches the average value of the output pixels to that of the ground truth pixels. CLODE and CLODE†
278"
QUANTITATIVE COMPARISONS,0.31106679960119643,"outperform other unsupervised learning methods. Notably, CLODE† even surpasses the PSNR of
279"
QUANTITATIVE COMPARISONS,0.3120638085742772,"state-of-the-art supervised learning methods by 0.73 dB, when averaging the results from the LSRW
280"
QUANTITATIVE COMPARISONS,0.31306081754735793,"and LOL datasets in the rightmost columns, without using GT Mean. Moreover, two notable points
281"
QUANTITATIVE COMPARISONS,0.3140578265204387,"can be highlighted in Table 1. First, the effectiveness of using NODE to compute accurate higher
282"
QUANTITATIVE COMPARISONS,0.3150548354935194,"order curves is evident, as demonstrated by its superiority over curve-adjustment-based methods;
283"
QUANTITATIVE COMPARISONS,0.3160518444666002,"ZeroDCE [6] and ReLLIE [9]. Second, unlike other models trained on the same training dataset
284"
QUANTITATIVE COMPARISONS,0.317048853439681,"(LOL), our model shows robust performance on both the LSRW and LOL test datasets, indicating
285"
QUANTITATIVE COMPARISONS,0.3180458624127617,"that our model generalizes better than conventional approaches.
286"
QUANTITATIVE COMPARISONS,0.3190428713858425,"In Table 2, we demonstrate the robustness under various exposure conditions including both under-
287"
QUANTITATIVE COMPARISONS,0.3200398803589232,"and over- exposures, and evaluate the performance on SICE Part2 [35]. The results show that CLODE
288"
QUANTITATIVE COMPARISONS,0.32103688933200397,"exhibits robust performance compared to other models, even under various exposure conditions. It
289"
QUANTITATIVE COMPARISONS,0.3220338983050847,"outperforms other unsupervised learning methods, and even when compared to supervised learning
290"
QUANTITATIVE COMPARISONS,0.3230309072781655,"Table 1: Quantitative results on LSRW [4] and LOL [5] datasets. For a fair comparison, we
re-trained some models on LOL and marked them with *. Among the unsupervised approaches,
the best score is displayed in red, the second best in blue, and the third best in black. For more
comparison results in terms of non-reference metrics, please refer to Appendix A.4.3."
QUANTITATIVE COMPARISONS,0.3240279162512463,"Training
Method
#Params (M)
Train dataset
LSRW
LOL
Average
Normal
GT Mean
Normal
GT Mean
Normal
GT Mean
PSNR↑
SSIM↑
PSNR↑
SSIM↑
PSNR↑
SSIM↑
PSNR↑
SSIM↑
PSNR↑
SSIM↑
PSNR↑
SSIM↑"
QUANTITATIVE COMPARISONS,0.325024925224327,Supervised
QUANTITATIVE COMPARISONS,0.32602193419740777,"RetinexNet [5]
0.4446
LOL
15.49
0.355
16.55
0.371
16.77
0.419
17.65
0.648
16.13
0.387
17.10
0.510
URetinexNet [37]
0.3069
LOL, SICE
17.63
0.516
18.10
0.523
19.84
0.826
21.33
0.835
18.74
0.671
19.71
0.679
DRBN [38]
0.5556
LOL
16.15
0.542
17.68
0.548
16.29
0.617
19.55
0.746
16.22
0.580
18.62
0.647
KinD [36]
8.0160
LOL
16.47
0.493
19.86
0.504
17.65
0.775
20.87
0.802
17.06
0.634
20.36
0.653
LLFlow [1]
38.859
LOL
17.52
0.509
18.68
0.518
21.15
0.854
24.99
0.871
19.34
0.681
21.84
0.694
RetinexFormer [2]
1.6057
LOL
17.76
0.517
19.15
0.529
25.15
0.845
27.18
0.850
21.45
0.681
23.17
0.690"
QUANTITATIVE COMPARISONS,0.3270189431704885,Unsupervised
QUANTITATIVE COMPARISONS,0.32801595214356927,"SCI-easy [11]
0.0003
MIT-5K [39]
11.79
0.317
16.97
0.426
9.58
0.369
18.55
0.501
10.69
0.343
17.76
0.464
SCI-medium [11]
0.0003
LOL, LSRW
15.24
0.424
17.84
0.439
14.78
0.521
19.11
0.504
15.01
0.473
18.47
0.472
SCI-difficult [11]
0.0003
DARKFace [40]
15.16
0.408
18.04
0.424
13.81
0.526
19.64
0.510
14.48
0.467
18.84
0.467
SCI* [11]
0.0003
LOL
14.82
0.413
17.65
0.437
13.84
0.507
19.02
0.499
14.33
0.460
18.34
0.468
RUAS [10]
0.0034
LOL
14.27
0.470
17.10
0.509
16.41
0.500
18.65
0.520
15.34
0.485
17.88
0.514
ZeroDCE* [6]
0.0794
LOL
14.50
0.403
18.87
0.467
16.49
0.522
20.99
0.596
15.50
0.463
19.93
0.532
ReLLIE [9]
-
LOL
-
-
-
-
18.37
0.641
-
-
-
-
-
-
PairLIE [13]
0.3417
LOL, SICE
16.97
0.498
18.82
0.523
19.51
0.736
23.10
0.752
18.24
0.617
20.96
0.637
Night-Enhancement [17]
67.011
LOL
14.24
0.472
19.19
0.554
21.52
0.763
24.25
0.781
17.88
0.618
21.72
0.668
CLODE
0.2167
LOL
17.28
0.533
20.60
0.557
19.61
0.718
23.16
0.752
18.44
0.625
21.88
0.655
CLODE †
0.2167
LOL
20.77
0.562
20.94
0.568
23.58
0.754
24.47
0.759
22.18
0.658
22.71
0.664"
QUANTITATIVE COMPARISONS,0.32901296111665007,"Table 2: Quantitative results on SICE [35] Part2. For a fair comparison, we re-trained some models
on SICE Part 1 and marked them with *. Within the unsupervised approaches, the best score is
displayed in red, the second in blue and the third in black."
QUANTITATIVE COMPARISONS,0.3300099700897308,"Training
Method
Train dataset
Normal
GT Mean
PSNR↑
SSIM↑
LPIPS↓
NIQE↓
BRISQUE↓
PI↓
Entropy↑
PSNR↑
SSIM↑"
QUANTITATIVE COMPARISONS,0.33100697906281157,Supervised
QUANTITATIVE COMPARISONS,0.3320039880358923,"URetinexNet [37]
LOL, SICE
12.15
0.708
0.393
4.250
15.633
3.372
6.926
17.81
0.686
LLFlow* [1]
SICE
14.34
0.608
0.279
3.643
17.011
3.481
6.566
19.59
0.658
ECLNet [41]
SICE
13.99
0.562
0.290
4.279
24.570
3.520
6.919
16.66
0.690
FECNet [42]
SICE
14.25
0.600
0.291
3.786
17.454
3.025
7.035
16.47
0.639
RetinexFormer* [2]
SICE
19.12
0.570
0.369
4.452
24.768
4.573
7.025
20.97
0.578
RetinexFormer [2]
MIT-5K [39]
13.23
0.564
0.263
3.848
17.350
2.863
6.881
16.35
0.609"
QUANTITATIVE COMPARISONS,0.33300099700897307,Unsupervised
QUANTITATIVE COMPARISONS,0.3339980059820538,"SCI-easy [11]
MIT-5K [39]
9.87
0.486
0.372
4.276
21.850
3.226
6.113
16.44
0.622
SCI-medium [11]
LOL, LSRW
9.77
0.510
0.454
5.727
33.200
4.392
5.212
15.83
0.574
SCI-difficult [11]
DarkFace [40]
11.13
0.577
0.324
4.636
23.620
3.107
6.386
16.85
0.647
SCI* [11]
SICE
10.67
0.478
0.331
4.289
23.449
3.570
6.213
17.99
0.675
RUAS* [10]
SICE
9.12
0.408
0.539
8.097
52.923
6.004
5.101
15.52
0.531
ZeroDCE [6]
SICE
12.67
0.635
0.244
3.886
21.630
2.821
6.516
18.85
0.686
PairLIE [13]
LOL, SICE
13.39
0.619
0.305
5.268
36.536
3.548
6.376
19.22
0.663
Night-Enhancement* [17]
SICE
13.18
0.581
0.360
4.728
33.883
4.133
6.661
19.43
0.660
CLODE
SICE
15.01
0.687
0.239
4.050
18.663
3.005
7.006
19.64
0.706
CLODE†
SICE
16.18
0.707
0.200
4.026
18.210
2.970
7.045
21.55
0.813"
QUANTITATIVE COMPARISONS,0.3349950149551346,"methods, CLODE† and CLODE achieve the best and second best results, respectively. Despite being
291"
QUANTITATIVE COMPARISONS,0.33599202392821537,"an unsupervised method, CLODE narrows the performance gap with state-of-the-art supervised
292"
QUANTITATIVE COMPARISONS,0.3369890329012961,"methods. Additionally, it operates robustly under challenging conditions such as various exposure
293"
QUANTITATIVE COMPARISONS,0.33798604187437686,"conditions in SICE Part2, surpassing supervised approaches. These strengths distinguish CLODE
294"
QUANTITATIVE COMPARISONS,0.3389830508474576,"from other unsupervised learning methods.
295"
QUANTITATIVE COMPARISONS,0.33998005982053836,"Input
Ground-Truth
Night-Enhance [17]
LLFlow [1]
RetinexFormer [2]
CLODE (Ours)
SCI [11]
ZeroDCE [6]
PairLIE [13]"
QUANTITATIVE COMPARISONS,0.34097706879361916,"Figure 4: Visual comparisons. From top to bottom: LOL [5], under- and over-exposed image of the
SICE [35] Part2. For more visual results, please refer to Fig. 10 in the Appendix."
PERCEPTUAL AND VISUAL COMPARISONS,0.3419740777666999,"4.4
Perceptual and Visual Comparisons
296"
PERCEPTUAL AND VISUAL COMPARISONS,0.34297108673978066,"In Table 2, we also provide a perceptual comparison of the results with other methods. The evaluation
297"
PERCEPTUAL AND VISUAL COMPARISONS,0.3439680957128614,"is conducted on SICE Part2, which includes a combination of underexposed and overexposed images.
298"
PERCEPTUAL AND VISUAL COMPARISONS,0.34496510468594216,"To measure the perceptual quality, we adopt Learned Perceptual Image Patch Similarity (LPIPS) [43],
299"
PERCEPTUAL AND VISUAL COMPARISONS,0.3459621136590229,"and non-reference metrics; natural image quality evaluator (NIQE) [44], blind/referenceless image
300"
PERCEPTUAL AND VISUAL COMPARISONS,0.3469591226321037,"spatial quality evaluator (BRISQUE) [45], perception index (PI) [46], and Entropy [47]. In these
301"
PERCEPTUAL AND VISUAL COMPARISONS,0.34795613160518446,"four aspects, both CLODE and CLODE† show outstanding performance compared to existing
302"
PERCEPTUAL AND VISUAL COMPARISONS,0.3489531405782652,"unsupervised methods. The visual results are compared in Fig. 4. CLODE shows robust and natural
303"
PERCEPTUAL AND VISUAL COMPARISONS,0.34995014955134596,"image enhancement results compared to other comparison methods, regardless of the exposure
304"
PERCEPTUAL AND VISUAL COMPARISONS,0.3509471585244267,"conditions of the input image.
305"
PERCEPTUAL AND VISUAL COMPARISONS,0.35194416749750745,"Table 3: Comparative experiments according to using
NODE on LSRW [4]/LOL [5].The ""Discrete"" refers to
performing curve adjustment in discrete steps, similar
to the conventional methods [6, 9], and ""Continuous""
refers to the reformulation of NODE."
PERCEPTUAL AND VISUAL COMPARISONS,0.35294117647058826,"Method
Case
Step (N)
PSNR↑
SSIM↑
BRISQUE↓"
PERCEPTUAL AND VISUAL COMPARISONS,0.353938185443669,Discrete
PERCEPTUAL AND VISUAL COMPARISONS,0.35493519441674976,"(a1)
1
11.19/9.236
0.297/0.362
41.137/41.169
(b1)
5
16.12/17.47
0.419/0.716
31.421/33.042
(c1)
10
13.94/16.18
0.395/0.520
32.267/32.243
(d1)
20
12.95/14.94
0.373/0.506
33.537/34.941
(e1)
30
12.87/14.97
0.375/0.509
33.537/35.342"
PERCEPTUAL AND VISUAL COMPARISONS,0.3559322033898305,"Continuous
(f1)
≤30 (adaptive)
17.28/19.61
0.533/0.718
18.426/8.220"
PERCEPTUAL AND VISUAL COMPARISONS,0.35692921236291125,Discrete (w/o NODE)
PERCEPTUAL AND VISUAL COMPARISONS,0.357926221335992,"Input
output"
PERCEPTUAL AND VISUAL COMPARISONS,0.3589232303090728,Continuous (w/ NODE)
PERCEPTUAL AND VISUAL COMPARISONS,0.35992023928215355,"Input
output
Figure 5: Trajectories of improvement for (e1)
and (f1) in Table 3. PCA dimension reduction
is used to visualize the trajectories."
ABLATION STUDY,0.3609172482552343,"4.5
Ablation Study
306"
ABLATION STUDY,0.36191425722831505,"Effectiveness of NODE
To validate the impact of NODE, we adjust the curves using the architecture
307"
ABLATION STUDY,0.3629112662013958,"of CLODE in both discrete (w/o NODE) and continuous (w/ NODE) spaces, and we compare the
308"
ABLATION STUDY,0.36390827517447655,"results in Table 3. In the discrete setting, similar to [6], curve parameters for fixed update steps [1, 5,
309"
ABLATION STUDY,0.36490528414755735,"10, 20, 30] are estimated in parallel ((a1) - (e1)). In the continuous setting, however, curve parameters
310"
ABLATION STUDY,0.3659022931206381,"are estimated sequentially for non-fixed adaptive steps, up to a maximum of 30 steps ((f1)). Results
311"
ABLATION STUDY,0.36689930209371885,"in Table 3 demonstrate that the curve parameters produced during the sequential continuous update
312"
ABLATION STUDY,0.3678963110667996,"procedure are more accurate and verify superior performance of the proposed method over the update
313"
ABLATION STUDY,0.36889332003988035,"procedure in the conventional discrete setting. In addition, in Fig. 5, we visualize the trajectories
314"
ABLATION STUDY,0.3698903290129611,"of improvement by plotting PCA dimension reduction results of latent images during updates. We
315"
ABLATION STUDY,0.3708873379860419,"observe that when curve adjustments are made in continuous space by (f1), the trajectories converge
316"
ABLATION STUDY,0.37188434695912265,"more accurately at the final states compared to (e1). This demonstrates that using NODE to find the
317"
ABLATION STUDY,0.3728813559322034,"optimal state certainly contributes to image enhancement.
318"
ABLATION STUDY,0.37387836490528414,"Table 4: Impact of the modules in fθ.
Noise Removal and the layer normaliza-
tion (LN) significantly improve perfor-
mance."
ABLATION STUDY,0.3748753738783649,"Case
Noise Removal g
LN in h
PSNR↑
SSIM↑"
ABLATION STUDY,0.37587238285144564,"(a2)
14.72
0.538
(b2)
✓
15.19
0.489
(c2)
✓
18.67
0.577
(d2)
✓
✓
19.61
0.718"
ABLATION STUDY,0.37686939182452645,"Table 5: Execution time and performance.
Training
Method
PSNR/SSIM
#Params (M)
Time (S)"
ABLATION STUDY,0.3778664007976072,"Supervised
RetinexNet [5]
15.49/0.355
0.4446
0.337
LLFlow [1]
17.52/0.509
38.859
0.144
RetinexFormer [2]
17.76/0.517
1.6057
0.072"
ABLATION STUDY,0.37886340977068794,Unsupervised
ABLATION STUDY,0.3798604187437687,"SCI-medium [11]
15.24/0.424
0.0003
0.001
RUAS [10]
14.27/0.470
0.0034
0.006
ZeroDCE [6]
15.81/0.449
0.0794
0.004
PairLIE [13]
16.97/0.498
0.3417
0.008
CLODE
17.28/0.533
0.2167
0.056
CLODE-S
16.97/0.457
0.0004
0.005"
ABLATION STUDY,0.38085742771684944,"Effect of the Modules
In Table 4, we conduct ablation experiments on the modules used in
319"
ABLATION STUDY,0.3818544366899302,"ODEfunc fθ. We verify the effects of the Noise Removal module g and the layer normalization
320"
ABLATION STUDY,0.382851445663011,"(LN) in the Curve Parameter Estimation module h. Each module shows performance improvements
321"
ABLATION STUDY,0.38384845463609174,"compared to the baseline (a2). In particular, our final model (d2) achieves the largest performance
322"
ABLATION STUDY,0.3848454636091725,"gain in terms of PSNR/SSIM. Furthermore, case (c2), which includes layer normalization, has about
323"
ABLATION STUDY,0.38584247258225324,"a 4dB gain in PSNR compared to (a2), which does not include layer normalization. This shows
324"
ABLATION STUDY,0.386839481555334,"that during the image enhancement process in NODE, it is essential to use layer normalization to
325"
ABLATION STUDY,0.38783649052841473,"normalize each state. The visual results can be seen in Fig. 8 of the Appendix.
326"
LIMITATIONS,0.38883349950149554,"5
Limitations
327"
LIMITATIONS,0.3898305084745763,"Table 5 shows the performance of PSNR/SSIM, number of parameters, and execution time measured
328"
LIMITATIONS,0.39082751744765704,"in LSRW [4] using an NVIDIA RTX 4090. CLODE shows the advantage in model size compared to
329"
LIMITATIONS,0.3918245264207378,"supervised methods. The iterative ODE solving process of CLODE takes longer than lightweight
330"
LIMITATIONS,0.39282153539381853,"unsupervised models, but it exhibits faster speed and performance comparable to supervised meth-
331"
LIMITATIONS,0.3938185443668993,"ods. Additionally, a smaller version, CLODE-S in Appendix A.1.2 shows promising enhancement
332"
LIMITATIONS,0.3948155533399801,"performance with competitive inference time comparable to those of unsupervised models.
333"
CONCLUSIONS,0.39581256231306083,"6
Conclusions
334"
CONCLUSIONS,0.3968095712861416,"In this work, we address the unsupervised low-light image enhancement problem by formulating
335"
CONCLUSIONS,0.39780658025922233,"it as a NODE problem. We introduce a novel iterative curve-adjustment approach with NODE,
336"
CONCLUSIONS,0.3988035892323031,"transforming discrete iterative problems into continuous ones. CLODE exhibits superior convergence
337"
CONCLUSIONS,0.39980059820538383,"compared to other unsupervised iterative methods, especially in diverse low-light and multi-exposure
338"
CONCLUSIONS,0.40079760717846463,"scenarios. In conclusion, our method effectively narrows the performance gap between unsupervised
339"
CONCLUSIONS,0.4017946161515454,"and supervised methods across various datasets.
340"
REFERENCES,0.40279162512462613,"References
341"
REFERENCES,0.4037886340977069,"[1] Yufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-Pui Chau, and Alex Kot. Low-light
342"
REFERENCES,0.4047856430707876,"image enhancement with normalizing flow. In AAAI, volume 36, 2022.
343"
REFERENCES,0.4057826520438684,"[2] Yuanhao Cai, Hao Bian, Jing Lin, Haoqian Wang, Radu Timofte, and Yulun Zhang. Retinex-
344"
REFERENCES,0.4067796610169492,"former: One-stage retinex-based transformer for low-light image enhancement. 2023.
345"
REFERENCES,0.4077766699900299,"[3] Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, and Hui Yuan. Global structure-
346"
REFERENCES,0.4087736789631107,"aware diffusion process for low-light image enhancement. NeurIPS, 2023.
347"
REFERENCES,0.4097706879361914,"[4] Jiang Hai, Zhu Xuan, Ren Yang, Yutong Hao, Fengzhu Zou, Fang Lin, and Songchen Han.
348"
REFERENCES,0.4107676969092722,"R2rnet: Low-light image enhancement via real-low to real-normal network. Journal of Visual
349"
REFERENCES,0.4117647058823529,"Communication and Image Representation, 90, 2023.
350"
REFERENCES,0.4127617148554337,"[5] Wenhan Yang Jiaying Liu Chen Wei, Wenjing Wang. Deep retinex decomposition for low-light
351"
REFERENCES,0.4137587238285145,"enhancement. In BMVC, 2018.
352"
REFERENCES,0.4147557328015952,"[6] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and
353"
REFERENCES,0.41575274177467597,"Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In
354"
REFERENCES,0.4167497507477567,"CVPR, 2020.
355"
REFERENCES,0.41774675972083747,"[7] Lu Yuan and Jian Sun. Automatic exposure correction of consumer photographs. In ECCV.
356"
REFERENCES,0.4187437686939183,"Springer, 2012.
357"
REFERENCES,0.419740777666999,"[8] Chongyi Li, Chunle Guo, and Chen Change Loy. Learning to enhance low-light image via
358"
REFERENCES,0.42073778664007977,"zero-reference deep curve estimation. IEEE Transactions on Pattern Analysis and Machine
359"
REFERENCES,0.4217347956131605,"Intelligence, 44, 2021.
360"
REFERENCES,0.42273180458624127,"[9] Rongkai Zhang, Lanqing Guo, Siyu Huang, and Bihan Wen. Rellie: Deep reinforcement
361"
REFERENCES,0.423728813559322,"learning for customized low-light image enhancement. In ACMMM, 2021.
362"
REFERENCES,0.4247258225324028,"[10] Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongxuan Luo. Retinex-inspired unrolling
363"
REFERENCES,0.42572283150548357,"with cooperative prior architecture search for low-light image enhancement. In CVPR, 2021.
364"
REFERENCES,0.4267198404785643,"[11] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and
365"
REFERENCES,0.42771684945164506,"robust low-light image enhancement. In CVPR, 2022.
366"
REFERENCES,0.4287138584247258,"[12] Zunjin Zhao, Bangshu Xiong, Lei Wang, Qiaofeng Ou, Lei Yu, and Fa Kuang. Retinexdip: A
367"
REFERENCES,0.42971086739780656,"unified deep framework for low-light image enhancement. IEEE Transactions on Circuits and
368"
REFERENCES,0.4307078763708873,"Systems for Video Technology, 32, 2021.
369"
REFERENCES,0.4317048853439681,"[13] Zhenqi Fu, Yan Yang, Xiaotong Tu, Yue Huang, Xinghao Ding, and Kai-Kuang Ma. Learning a
370"
REFERENCES,0.43270189431704886,"simple low-light image enhancer from paired low-light instances. In CVPR, 2023.
371"
REFERENCES,0.4336989032901296,"[14] Qiuping Jiang, Yudong Mao, Runmin Cong, Wenqi Ren, Chao Huang, and Feng Shao. Un-
372"
REFERENCES,0.43469591226321036,"supervised decomposition and correction network for low-light image enhancement. IEEE
373"
REFERENCES,0.4356929212362911,"Transactions on Intelligent Transportation Systems, 23(10), 2022.
374"
REFERENCES,0.43668993020937186,"[15] Feng Zhang, Yuanjie Shao, Yishi Sun, Kai Zhu, Changxin Gao, and Nong Sang. Unsupervised
375"
REFERENCES,0.43768693918245266,"low-light image enhancement via histogram equalization prior. arXiv preprint arXiv:2112.01766,
376"
REFERENCES,0.4386839481555334,"2021.
377"
REFERENCES,0.43968095712861416,"[16] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan
378"
REFERENCES,0.4406779661016949,"Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision.
379"
REFERENCES,0.44167497507477566,"IEEE Transactions on image processing, 30, 2021.
380"
REFERENCES,0.4426719840478564,"[17] Yeying Jin, Wenhan Yang, and Robby T Tan. Unsupervised night image enhancement: When
381"
REFERENCES,0.4436689930209372,"layer decomposition meets light-effects suppression. In ECCV, 2022.
382"
REFERENCES,0.44466600199401796,"[18] Chongyi Li, Chunle Guo, Ruicheng Feng, Shangchen Zhou, and Chen Change Loy. Cudi: Curve
383"
REFERENCES,0.4456630109670987,"distillation for efficient and controllable exposure adjustment. arXiv preprint arXiv:2207.14273,
384"
REFERENCES,0.44666001994017945,"2022.
385"
REFERENCES,0.4476570289132602,"[19] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
386"
REFERENCES,0.44865403788634095,"differential equations. NeurIPS, 31, 2018.
387"
REFERENCES,0.44965104685942175,"[20] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. NeuraIPS, 32,
388"
REFERENCES,0.4506480558325025,"2019.
389"
REFERENCES,0.45164506480558325,"[21] Yulia Rubanova, Ricky TQ Chen, and David K Duvenaud. Latent ordinary differential equations
390"
REFERENCES,0.452642073778664,"for irregularly-sampled time series. NeuraIPS, 32, 2019.
391"
REFERENCES,0.45363908275174475,"[22] Yogesh Verma, Markus Heinonen, and Vikas Garg. Climode: Climate forecasting with physics-
392"
REFERENCES,0.4546360917248255,"informed neural odes. In ICLR, 2023.
393"
REFERENCES,0.4556331006979063,"[23] Sunghyun Park, Kangyeol Kim, Junsoo Lee, Jaegul Choo, Joonseok Lee, Sookyung Kim, and
394"
REFERENCES,0.45663010967098705,"Edward Choi. Vid-ode: Continuous-time video generation with neural ordinary differential
395"
REFERENCES,0.4576271186440678,"equation. In AAAI, volume 35, 2021.
396"
REFERENCES,0.45862412761714855,"[24] Yifan Wu, Tom Z Jiahao, Jiancong Wang, Paul A Yushkevich, M Ani Hsieh, and James C Gee.
397"
REFERENCES,0.4596211365902293,"Nodeo: A neural ordinary differential equation based optimization framework for deformable
398"
REFERENCES,0.46061814556331004,"image registration. In CVPR, 2022.
399"
REFERENCES,0.46161515453639085,"[25] Boyan Jiang, Yinda Zhang, Xingkui Wei, Xiangyang Xue, and Yanwei Fu. Learning composi-
400"
REFERENCES,0.4626121635094716,"tional representation for 4d captures with neural ode. In CVPR, 2021.
401"
REFERENCES,0.46360917248255235,"[26] Seobin Park and Tae Hyun Kim. Progressive image super-resolution via neural differential
402"
REFERENCES,0.4646061814556331,"equation. In ICASSP, 2022.
403"
REFERENCES,0.46560319042871384,"[27] Jiancheng Huang, Yifan Liu, and Shifeng Chen. Bootstrap diffusion model curve estimation for
404"
REFERENCES,0.4666001994017946,"high resolution low-light image enhancement. In PRICAI. Springer, 2023.
405"
REFERENCES,0.4675972083748754,"[28] Haoyuan Wang, Ke Xu, and Rynson WH Lau. Local color distributions prior for image
406"
REFERENCES,0.46859421734795614,"enhancement. In ECCV. Springer, 2022.
407"
REFERENCES,0.4695912263210369,"[29] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
408"
REFERENCES,0.47058823529411764,"arXiv:1607.06450, 2016.
409"
REFERENCES,0.4715852442671984,"[30] Gershon Buchsbaum. A spatial processor model for object colour perception. Journal of the
410"
REFERENCES,0.47258225324027914,"Franklin institute, 310, 1980.
411"
REFERENCES,0.47357926221335994,"[31] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala,
412"
REFERENCES,0.4745762711864407,"and Timo Aila. Noise2Noise: Learning image restoration without clean data. In ICML, 2018.
413"
REFERENCES,0.47557328015952144,"[32] Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2neighbor:
414"
REFERENCES,0.4765702891326022,"Self-supervised denoising from single noisy images. In CVPR, 2021.
415"
REFERENCES,0.47756729810568294,"[33] Youssef Mansour and Reinhard Heckel. Zero-shot noise2noise: Efficient image denoising
416"
REFERENCES,0.4785643070787637,"without any data. In CVPR, 2023.
417"
REFERENCES,0.4795613160518445,"[34] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR,
418"
REFERENCES,0.48055832502492524,"2021.
419"
REFERENCES,0.481555333998006,"[35] Jianrui Cai, Shuhang Gu, and Lei Zhang. Learning a deep single image contrast enhancer from
420"
REFERENCES,0.48255234297108673,"multi-exposure images. IEEE Transactions on Image Processing, 27, 2018.
421"
REFERENCES,0.4835493519441675,"[36] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling the darkness: A practical low-light
422"
REFERENCES,0.48454636091724823,"image enhancer. In ACMMM, 2019.
423"
REFERENCES,0.48554336989032904,"[37] Wenhui Wu, Jian Weng, Pingping Zhang, Xu Wang, Wenhan Yang, and Jianmin Jiang. Uretinex-
424"
REFERENCES,0.4865403788634098,"net: Retinex-based deep unfolding network for low-light image enhancement. In CVPR, 2022.
425"
REFERENCES,0.48753738783649053,"[38] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. Band representation-
426"
REFERENCES,0.4885343968095713,"based semi-supervised low-light image enhancement: Bridging the gap between signal fidelity
427"
REFERENCES,0.48953140578265203,"and perceptual quality. IEEE Transactions on image processing, 2021.
428"
REFERENCES,0.4905284147557328,"[39] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Frédo Durand. Learning photographic
429"
REFERENCES,0.4915254237288136,"global tonal adjustment with a database of input / output image pairs. In CVPR, 2011.
430"
REFERENCES,0.49252243270189433,"[40] Wenhan Yang, Ye Yuan, Wenqi Ren, Jiaying Liu, Walter J Scheirer, Zhangyang Wang, Taiheng
431"
REFERENCES,0.4935194416749751,"Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, et al. Advancing image understanding in poor
432"
REFERENCES,0.4945164506480558,"visibility environments: A collective benchmark study. IEEE Transactions on image processing,
433"
REFERENCES,0.4955134596211366,"29, 2020.
434"
REFERENCES,0.4965104685942173,"[41] Jie Huang, Man Zhou, Yajing Liu, Mingde Yao, Feng Zhao, and Zhiwei Xiong. Exposure-
435"
REFERENCES,0.49750747756729813,"consistency representation learning for exposure correction. In ACMMM, 2022.
436"
REFERENCES,0.4985044865403789,"[42] Jie Huang, Yajing Liu, Feng Zhao, Keyu Yan, Jinghao Zhang, Yukun Huang, Man Zhou,
437"
REFERENCES,0.4995014955134596,"and Zhiwei Xiong. Deep fourier-based exposure correction network with spatial-frequency
438"
REFERENCES,0.5004985044865404,"interaction. In ECCV, 2022.
439"
REFERENCES,0.5014955134596212,"[43] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea-
440"
REFERENCES,0.5024925224327019,"sonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE
441"
REFERENCES,0.5034895314057827,"conference on computer vision and pattern recognition, pages 586–595, 2018.
442"
REFERENCES,0.5044865403788634,"[44] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a “completely blind” image
443"
REFERENCES,0.5054835493519442,"quality analyzer. IEEE Signal processing letters, 20, 2012.
444"
REFERENCES,0.5064805583250249,"[45] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality
445"
REFERENCES,0.5074775672981057,"assessment in the spatial domain. IEEE Transactions on image processing, 21, 2012.
446"
REFERENCES,0.5084745762711864,"[46] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. The 2018
447"
REFERENCES,0.5094715852442672,"pirm challenge on perceptual image super-resolution. In ECCVW, 2018.
448"
REFERENCES,0.5104685942173479,"[47] Xiaoqiao Chen, Qingyi Zhang, Manhui Lin, Guangyi Yang, and Chu He. No-reference color
449"
REFERENCES,0.5114656031904287,"image quality assessment: From entropy to perceptual quality. EURASIP Journal on Image and
450"
REFERENCES,0.5124626121635095,"Video Processing, 2019.
451"
REFERENCES,0.5134596211365903,"[48] Ricky T. Q. Chen. torchdiffeq, 2018.
452"
REFERENCES,0.514456630109671,"[49] Mahmoud Afifi, Konstantinos G Derpanis, Bjorn Ommer, and Michael S Brown. Learning
453"
REFERENCES,0.5154536390827518,"multi-scale photo exposure correction. In CVPR, 2021.
454"
REFERENCES,0.5164506480558325,"A
Appendix
455"
REFERENCES,0.5174476570289133,"A.1
Implement Details
456"
REFERENCES,0.518444666001994,"The training set of images is resized to 128x128, we employ the Pytorch framework on NVIDIA
457"
REFERENCES,0.5194416749750748,"A6000 GPU with a batch size of 8. The ADAM optimizer is used with default parameters and a fixed
458"
REFERENCES,0.5204386839481555,"learning rate of 1e−5 to optimize the parameters of our network. The weights for the loss function
459"
REFERENCES,0.5214356929212363,"wcol, wparam, wspa, wexp and wnoise are set to 20, 200, 1, 10 and 1 respectively, to balance the scale
460"
REFERENCES,0.522432701894317,"of losses. Furthermore, we adopt torchdiffeq [48] for Neural ODEs implementation. The training
461"
REFERENCES,0.5234297108673978,"process is conducted for 100 epochs.
462"
REFERENCES,0.5244267198404786,"A.1.1
Implementation details of NODE
463"
REFERENCES,0.5254237288135594,"We utilize the adaptive ODE solver, dopri5 (Dormand-Prince Runge-Kutta of Order 5) for our work.
464"
REFERENCES,0.5264207377866401,"The maximum allowed step for the adaptive solver is set to 30. The relative and absolute tolerances
465"
REFERENCES,0.5274177467597209,"for the error rate calculation are set uniformly to 1e−5. The adaptive solver uses the error rate to
466"
REFERENCES,0.5284147557328016,"determine the steps. Also, the adaptive solver estimates an error rate for the current step t, and if
467"
REFERENCES,0.5294117647058824,"the error exceeds the allowable tolerances, the step is re-done with a smaller step size. This process
468"
REFERENCES,0.5304087736789631,"continues until the error becomes smaller than the provided tolerance. The error rate Γt at the t-th
469"
REFERENCES,0.5314057826520439,"step is computed as follows:
470"
REFERENCES,0.5324027916251246,"Γt = atol × rtol × norm(It),
(18)"
REFERENCES,0.5333998005982054,"where atol is absolute tolerance, and rtol is relative tolerate, and the norm being used is a mixed
471"
REFERENCES,0.5343968095712861,"L-infinity/RMS norm. We set both atol and rtol to 1e-5.
472"
REFERENCES,0.5353938185443669,"A.1.2
Details of the CLODE architecture
473"
REFERENCES,0.5363908275174477,"This section presents the architectural details of the CLODE network architecture, with a particular
474"
REFERENCES,0.5373878364905285,"focus on the ODEfunc module. The Noise Removal module g employs a simple and lightweight
475"
REFERENCES,0.5383848454636092,"three-layer convolutional network. In Curve Parameter Estimation module h, a shallow network
476"
REFERENCES,0.53938185443669,"with two branches is utilized, wherein filters of varying sizes are employed at each branch to capture
477"
REFERENCES,0.5403788634097707,"image features across different filter scales. We also provide architectural details of CLODE-S as
478"
REFERENCES,0.5413758723828515,"mentioned in Sec. 5 of the main manuscript. This version omits the Noise Removal module for speed
479"
REFERENCES,0.5423728813559322,"and uses a 2-layer network with 1x1 convolutions. 𝐼!
𝐼"" 𝐼!"
REFERENCES,0.543369890329013,"""𝐼!
𝒜!
Eq. 9
𝑑𝐼! 𝑑𝑡 𝑓"" 𝐼"" . . . 𝐼#"
REFERENCES,0.5443668993020937,"𝐼# = 𝑂𝐷𝐸_𝑆𝑜𝑙𝑣𝑒𝑟(𝐼$, 0, 𝑇, 𝑓"") 𝑡 𝑓"""
REFERENCES,0.5453639082751744,𝒇𝜽: ODEfunc
REFERENCES,0.5463609172482552,(b) ODEfunc
REFERENCES,0.5473579262213359,"Noise Removal 
Module (𝑔)"
REFERENCES,0.5483549351944168,"Curve Parameter 
Estimation (ℎ) Eq.5"
REFERENCES,0.5493519441674976,"Trajectory 
of improvement + +
+"
REFERENCES,0.5503489531405783,𝐼# = 𝐼$ + 5 $ #
REFERENCES,0.551345962113659,"𝑓"" 𝐼!, 𝑡𝑑𝑡 . . . 𝑓"" . . . + ≈ 𝑓"""
REFERENCES,0.5523429710867398,(a) Continuous update procedure of CLODE
REFERENCES,0.5533399800598205,"Figure 6: Illustration of architecture details of (a) modules of ODEfunc in CLODE and (b) ODEfunc
of CLODE-S. 480"
REFERENCES,0.5543369890329013,"A.2
Impact of Each Loss Functions
481"
REFERENCES,0.555333998005982,"CLODE combines five non-reference loss functions to train NODE, producing optimal improvements.
482"
REFERENCES,0.5563310069790628,"We present ablation experiments for each loss function, and the results are presented in Table 6 and
483"
REFERENCES,0.5573280159521435,"Fig. 7. The results of each image ablation experiment demonstrate that appropriate improvement
484"
REFERENCES,0.5583250249252243,"results can only be obtained when using CLODE with all loss functions. The characteristics of the
485"
REFERENCES,0.559322033898305,"loss function as observed in each ablation are as follows: ((a3) w/o Lexp): Brightness improvement
486"
REFERENCES,0.5603190428713859,"is not achieved in low-exposure enhancement. ((b3) w/o Lcol): Severe color distortion occurs in
487"
REFERENCES,0.5613160518444666,"over-exposure enhancement, damaging structural details. ((c3) w/o Lparam): Structural distortion
488"
REFERENCES,0.5623130608175474,"occurs, creating artifacts. ((d3) w/o Lspa): While showing better results than other experiments, it
489"
REFERENCES,0.5633100697906281,"(d3) w/o ℒ𝒔𝒑𝒂
Input
(b3) w/o ℒ𝒄𝒐𝒍
(a3) w/o ℒ𝒆𝒙𝒑
(c3) w/o ℒ𝒑𝒂𝒓𝒂𝒎
(e3) ℒ𝒆𝒙𝒑 + ℒ𝒄𝒐𝒍 + ℒ𝒑𝒂𝒓𝒂𝒎 + ℒ𝒔𝒑𝒂"
REFERENCES,0.5643070787637089,"Input
(e3) w/o ℒ𝒏𝒐𝒊𝒔𝒆
(f3) CLODE"
REFERENCES,0.5653040877367896,"Figure 7: Visual results for the ablation study of each loss function. CLODE combines five non-
reference loss functions in training for producing optimal enhancement results."
REFERENCES,0.5663010967098704,Table 6: Ablation study on each non-reference losses. The experiment is evaluated on LOL [5].
REFERENCES,0.5672981056829511,"Case
Lspa
Lexp
Lcol
Lparam
Lnoise
PSNR
SSIM"
REFERENCES,0.5682951146560319,"(a3)
✓
✓
✓
8.84
0.323
(b3)
✓
✓
✓
14.72
0.566
(c3)
✓
✓
✓
14.76
0.535
(d3)
✓
✓
✓
18.76
0.580
(e3)
✓
✓
✓
✓
18.92
0.582
(f3)
✓
✓
✓
✓
✓
19.61
0.718"
REFERENCES,0.5692921236291126,"occurs loss of structural details compared to (e3). ((e3) w/o LNoise): Compared to the proposed
490"
REFERENCES,0.5702891326021934,"version (f3), it produces improved results with noise present.
491"
REFERENCES,0.5712861415752741,"A.3
Visualization of curve parameter map A
492"
REFERENCES,0.5722831505483549,"We provide visual comparison results for the module ablation experiments in Sec. 4.5 of the main
493"
REFERENCES,0.5732801595214357,"manuscript. In the visual results without noise removal module (c2), we can observe the noise in
494"
REFERENCES,0.5742771684945165,"A. The enhanced result of (c2) using A with noise shows overall color discrepancy compared to
495"
REFERENCES,0.5752741774675972,"the ground-truth, in contrast to the enhanced result of (d2) where the noise removal module are
496"
REFERENCES,0.576271186440678,"applied. The enhanced result of (d2) shows robust color similarity with the ground-truth image. We
497"
REFERENCES,0.5772681954137587,"can confirm that removing noise for A is important for curve-adjustment-based method.
498"
REFERENCES,0.5782652043868395,"Input
Ground-Truth
(d2) 𝒜
(c2) 𝒜 
(c2) enhanced
(d2) enhanced"
REFERENCES,0.5792622133599202,"Figure 8: A visual comparison of the results for (c2) and (d2) from Table 4 in the main manuscript.
The enhanced result (d2) using A with noise removal module demonstrates improvement more similar
to the ground-truth."
REFERENCES,0.580259222333001,"A.4
Background of Noise Removal Loss
499"
REFERENCES,0.5812562313060817,"In Sec.3.4 we provide information about the zero-reference loss functions that we used. Unlike the
500"
REFERENCES,0.5822532402791625,"others, the Noise Removal Loss (Lnoise) requires more explanation due to its unfamiliarity in the
501"
REFERENCES,0.5832502492522432,"field of low-light enhancement, so we provide additional explanation for it.
502"
REFERENCES,0.584247258225324,"A.4.1
Noise2Noise background
503"
REFERENCES,0.5852442671984048,"In supervised denoising studies, neural networks are aimed at denoising the noisy image y to the
504"
REFERENCES,0.5862412761714856,"clean image x. Since the noisy y is an addition of the clean image x and the noise e, the network is
505"
REFERENCES,0.5872382851445663,"trained to map the noise e which is called Noise2Clean (N2C) method. If the network parameter is
506"
REFERENCES,0.5882352941176471,"ϕN2C, the object function of the supervised denoising method with the network gϕ can be written as:
507"
REFERENCES,0.5892323030907278,"ϕN2C = arg min
ϕ
E

||gϕ(y) −x||2
2

.
(19)"
REFERENCES,0.5902293120638086,"Denoising networks can also be trained to output the noisy image y2 from the noisy input image y1
508"
REFERENCES,0.5912263210368893,"that comes from the same clean image x. This noise-to-noise manner can be achieved by assuming
509"
REFERENCES,0.5922233300099701,"that the noise has a mean of zero as introduced in Noise2Noise (N2N) [31]. This is the objective
510"
REFERENCES,0.5932203389830508,"function for the N2N network parameter ϕN2N:
511"
REFERENCES,0.5942173479561316,"ϕN2N = arg min
ϕ
E

||gϕ(y2) −y1||2
2

.
(20)"
REFERENCES,0.5952143569292123,"The N2N manner shows close performance compare to N2C manner with sufficient training data since
512"
REFERENCES,0.5962113659022931,"the objective functions of N2C and N2N are aimed on the same network parameter. If ya = x + ea,
513"
REFERENCES,0.5972083748753739,"yb = x + eb, and the mean value of ea and eb are zero, the proof is as follows:
514"
REFERENCES,0.5982053838484547,"ϕN2C = arg min
ϕ
E

||gϕ(y2) −x||2
2
"
REFERENCES,0.5992023928215354,"= arg min
ϕ
E

||gϕ(y2)||2
2 −2x⊺gϕ(y2) + ||x||2
2
"
REFERENCES,0.6001994017946162,"= arg min
ϕ
E

||gϕ(y2)||2
2 −2x⊺gϕ(y2)
"
REFERENCES,0.6011964107676969,"ϕN2N = arg min
ϕ
E

||gϕ(y2) −y1||2
2
"
REFERENCES,0.6021934197407777,"= arg min
ϕ
E

||gϕ(y2) −(x + e1)||2
2
"
REFERENCES,0.6031904287138584,"= arg min
ϕ
E

||gϕ(y2)||2
2 −2x⊺gϕ(y2) −2e⊺
2gϕ(y2) + ||x + e1||2
2
"
REFERENCES,0.6041874376869392,"= arg min
ϕ
E

||gϕ(y2)||2
2 −2x⊺gϕ(y2) −2e⊺
2gϕ(y2)
"
REFERENCES,0.6051844466600199,"= arg min
ϕ
E

||gϕ(y2)||2
2 −2x⊺gϕ(y2)

. (21)"
REFERENCES,0.6061814556331007,"By Eq. 21 we can confirm that the object of ϕN2C and ϕN2N is the identical one.
515"
REFERENCES,0.6071784646061814,"A.4.2
Zeroshot Noise2Noise method
516"
REFERENCES,0.6081754735792622,"In spite of N2N approaches, it is hard to obtain two different noisy images from the same clean
517"
REFERENCES,0.609172482552343,"scene. To address this hurdle, the Neighbor2Neighbor [32] method is proposed. This allows a pair
518"
REFERENCES,0.6101694915254238,"of noisy images to be augmented from a single noisy image coming from the same clean image. In
519"
REFERENCES,0.6111665004985045,"Zeroshot-N2N [33], which is adopted in our proposed method, Neighbor2Neighbor is achieved by
520"
REFERENCES,0.6121635094715853,"using two different 2D convolutional kernels (D1 and D2) on noisy images. If the noisy image is y, a
521"
REFERENCES,0.613160518444666,"pair of down-sampled images y1, y2 can be represented as:
522"
REFERENCES,0.6141575274177468,"y1 = D1(y), y2 = D2(y).
(22)"
REFERENCES,0.6151545363908275,"For a noisy image y with a size of H × W × C, the size of y1 and y2 is H 2 × W"
REFERENCES,0.6161515453639083,"2 × C. With
523"
REFERENCES,0.617148554336989,"downsampled images y1 and y2, the loss optimizes gϕ to fit the noise as:
524"
REFERENCES,0.6181455633100698,"arg min
ϕ
||gϕ(y1) −y2||2
2.
(23)"
REFERENCES,0.6191425722831505,"Zeroshot-N2N [33] emphasizes that residual learning, a symmetry loss, and an additional coherence-
525"
REFERENCES,0.6201395812562313,"enhancing term are critical for good performance. Zeroshot-N2N proposes two different loss functions,
526"
REFERENCES,0.6211365902293121,"the residual loss Lres and the consistency loss Lcons. First, the residual loss optimizes the network
527"
REFERENCES,0.6221335992023929,"gϕ to fit the noise instead of image. The loss then becomes as:
528"
REFERENCES,0.6231306081754736,"arg min
ϕ
||y1 −gϕ(y1) −y2||2
2.
(24)"
REFERENCES,0.6241276171485544,"To fit the noise in y1 and y2 both, a symmetric loss [34] is applied as:
529"
REFERENCES,0.6251246261216351,Lres(ϕ) = 1
REFERENCES,0.6261216350947159,"2
 
||y1 −gϕ(y1) −y2||2
2 + ||y2 −gϕ(y2) −y1||2
2

.
(25)"
REFERENCES,0.6271186440677966,"Second, the method constrain consistency by making denoised output of the downsampled image and
530"
REFERENCES,0.6281156530408774,"downsampled result of the denoised image like:
531"
REFERENCES,0.6291126620139581,"arg min
ϕ
||y1 −gϕ(y1) −D1(y1 −gϕ(y1))||2
2.
(26)"
REFERENCES,0.6301096709870389,"Same as Eq. 25, with the adoption of a symmetric manner, the consistency loss is represented as:
532"
REFERENCES,0.6311066799601196,Lcons(ϕ) = 1
REFERENCES,0.6321036889332003,"2
 
||y1 −gϕ(y1) −D1(y1 −gϕ(y1))||2
2 + ||y2 −gϕ(y2) −D2(y2 −gϕ(y2))||2
2

. (27)"
REFERENCES,0.6331006979062812,"The noise removal loss function Lnoise in Zeroshot-N2N becomes the sum of Eq. 25 and Eq. 27,
533"
REFERENCES,0.634097706879362,"expressed as:
534"
REFERENCES,0.6350947158524427,"Lnoise = Lres + Lcons.
(28)"
REFERENCES,0.6360917248255235,"A.4.3
More Quantitative Results
535"
REFERENCES,0.6370887337986042,"We present the comparison results for non-reference metrics, which we did not include in Table 1.
536"
REFERENCES,0.638085742771685,"Table 7 demonstrates that CLODE outperforms other unsupervised methods in terms of perceptual
537"
REFERENCES,0.6390827517447657,"quality. Notably, it demonstrates competitive results in terms of BRISQUE and PI, even when
538"
REFERENCES,0.6400797607178464,"compared to state-of-the-art supervised methods.
539"
REFERENCES,0.6410767696909272,"Table 7: Comparison results on LSRW [4] and LOL [5] in terms of NIQE [44], BRISQUE [45],
PI [46] and Entropy [47]. Within the unsupervised approaches, the best score is displayed in Red.
LLNODE performs better than all other methods, including supervised methods, in terms of PI
(Perceptual Index)."
REFERENCES,0.6420737786640079,"Training
Method
LSRW
LOL
NIQE↓
BRISQUE↓
PI↓
Entropy↑
NIQE↓
BRISQUE↓
PI↓
Entropy↑"
REFERENCES,0.6430707876370887,Supervised
REFERENCES,0.6440677966101694,"Afifi et al. [49]
6.655
46.645
6.470
7.065
4.966
33.546
5.741
7.173
RetinexNet [5]
-
-
-
-
8.871
51.813
4.955
6.835
URetinexNet [37]
4.154
23.614
3.495
6.762
4.250
15.633
3.372
6.926
LLFlow [1]
3.756
26.671
3.176
7.369
5.709
35.022
4.530
7.141
RetinexFormer [2]
3.549
15.951
3.208
7.131
3.478
17.101
3.102
7.074"
REFERENCES,0.6450648055832503,Unsupervised
REFERENCES,0.646061814556331,"SCI-easy [11]
3.847
25.859
3.259
6.388
7.153
12.424
5.437
5.825
SCI-medium [11]
3.917
22.416
3.159
6.494
7.861
25.870
4.583
6.842
SCI-difficult [11]
4.368
20.692
3.851
5.975
8.060
26.823
4.664
6.675
RUAS [10]
5.426
38.854
4.939
6.442
6.303
11.977
4.571
7.194
ZeroDCE [6]
3.776
23.867
3.156
6.526
7.777
27.301
4.459
6.608
Night-Enhancement [17]
7.208
51.356
6.801
6.544
4.491
27.122
4.436
7.139
PairLIE [13]
3.684
29.816
3.426
6.923
4.083
20.592
3.052
6.823
CLODE
3.827
18.426
3.115
7.025
4.516
8.220
2.914
7.053"
REFERENCES,0.6470588235294118,"A.4.4
Comparison with other iterative methods
540"
REFERENCES,0.6480558325024925,"Fig. 9 shows the changes in performance over steps of each curve-adjustment-based method. Each
541"
REFERENCES,0.6490528414755733,"comparison method is retrained for 10 steps in the official code provided by the author. To fix the
542"
REFERENCES,0.650049850448654,"number of steps in CLODE to 10, we replace CLODE’s ODE solver with the Euler method, and
543"
REFERENCES,0.6510468594217348,"referred to it as CLODE-Euler. The results show that even within the same number of steps, CLODE-
544"
REFERENCES,0.6520438683948155,"Euler performs better than other curve adjustment-based methods. Furthermore, the proposed version,
545"
REFERENCES,0.6530408773678963,"CLODE, demonstrates higher performance compared to other methods in most iterative steps.
546"
REFERENCES,0.654037886340977,"In case of ReLLIE [9], it exhibits a decline in performance after 7 steps, emphasizing the need
547"
REFERENCES,0.6550348953140578,"for careful selection of the number of iterative steps itself to achieve optimal result, this makes the
548"
REFERENCES,0.6560319042871385,"method impractical to use.
549"
REFERENCES,0.6570289132602194,"0.0
0.2
0.4
0.6
0.8
1.0
Step (Scaled to 0-1) 8 10 12 14 16 18 20"
REFERENCES,0.6580259222333001,PSNR (dB)
REFERENCES,0.6590229312063809,Comparison with curve-adjustment-based methods
REFERENCES,0.6600199401794616,"ZeroDCE (10 steps)
ZeroDCE++ (10 steps)
ReLLIE (10 steps)
CLODE-Euler (10 steps)
CLODE (Adaptive steps)"
REFERENCES,0.6610169491525424,"Figure 9: Changes in PSNR (Peak Signal-to-Noise Ratio) over steps of CLODE, CLODE-Euler,
ReLLIE [9], ZeroDCE++ [8], and ZeroDCE [6]. As CLODE employs a continuous adaptive
step according to the input image, we represent the steps by scaling them from 0 to 1. CLODE
demonstrates superior performance compared to other methods at almost every step."
REFERENCES,0.6620139581256231,"A.5
More visual results
550"
REFERENCES,0.6630109670987039,"We show additional results for CLODE enhancement that we did not show in the main manuscript
551"
REFERENCES,0.6640079760717846,"due to lack of space. We present additional visual comparison results for PairLIE [13] and Night-
552"
REFERENCES,0.6650049850448654,"Enhancement [17], which demonstrated the best quantitative performance among the unsupervised
553"
REFERENCES,0.6660019940179461,"methods in Table 1 of the main manuscript, except for our proposed method (CLODE), in Fig. 10.
554"
REFERENCES,0.6669990029910269,"CLODE shows the most robust enhancement results across various image exposure conditions.
555"
REFERENCES,0.6679960119641076,"Fig. 11, Fig. 12, Fig. 13 and Fig. 14 show the results for CLODE and CLODE† on LOL [5] and
556"
REFERENCES,0.6689930209371885,"SICE [35] validation dataset. Additionally, Fig. 15 shows the visual results with different exposures
557"
REFERENCES,0.6699900299102692,"for photos extracted from MSEC [49] and the internet (Filckr: CC BY-NC 2.0).
558"
REFERENCES,0.67098703888335,"Input
(b)
CLODE
(a)"
REFERENCES,0.6719840478564307,"Figure 10: Comparative visualization results with (a) PairLIE [13] and (b) night-enhancement [17]
on LOL [5] and SICE [35]. Images are taken from LSRW [4] and SICE [35] Part2."
REFERENCES,0.6729810568295115,"Ground-Truth
CLODE
Input
CLODE†"
REFERENCES,0.6739780658025922,"Figure 11: Visualization results on LOL [5]. While CLODE demonstrates superior enhancement
results, user control with CLODE† produces images that more closely resemble the ground-truth
image."
REFERENCES,0.674975074775673,"Ground-Truth
CLODE
Input
CLODE†"
REFERENCES,0.6759720837487537,"Figure 12: Visualization results on LOL [5] and SICE [35]. While CLODE demonstrates superior
enhancement results, user control with CLODE† produces images that more closely resemble the
ground-truth image."
REFERENCES,0.6769690927218345,"Ground-Truth
Input
CLODE
CLODE†"
REFERENCES,0.6779661016949152,"Figure 13: Visualization results on SICE [35]. While CLODE demonstrates superior enhancement
results, user control with CLODE† produces images that more closely resemble the ground-truth
image."
REFERENCES,0.678963110667996,"Ground-Truth
Input
CLODE
CLODE†"
REFERENCES,0.6799601196410767,"Figure 14: Visualization results on SICE [35]. While CLODE demonstrates superior enhancement
results, user control with CLODE† produces images that more closely resemble the ground-truth
image."
REFERENCES,0.6809571286141576,"CLODE
Input
CLODE
Input"
REFERENCES,0.6819541375872383,By julochka (Flickr: CC BY-NC 2.0)
REFERENCES,0.6829511465603191,"Figure 15: Visualization results on MSEC [49] and extracted from internet (Flickr by julochka). Even
with diverse inputs of various exposures, CLODE show robust result in an unsupervised manner."
REFERENCES,0.6839481555333998,"NeurIPS Paper Checklist
559"
CLAIMS,0.6849451645064806,"1. Claims
560"
CLAIMS,0.6859421734795613,"Question: Do the main claims made in the abstract and introduction accurately reflect the
561"
CLAIMS,0.6869391824526421,"paper’s contributions and scope?
562"
CLAIMS,0.6879361914257228,"Answer: [Yes]
563"
CLAIMS,0.6889332003988036,"Justification: We presented the contributions and effects of our method in the abstract and
564"
CLAIMS,0.6899302093718843,"introduction, and demonstrated them through experiments.
565"
CLAIMS,0.6909272183449651,"Guidelines:
566"
CLAIMS,0.6919242273180458,"• The answer NA means that the abstract and introduction do not include the claims
567"
CLAIMS,0.6929212362911267,"made in the paper.
568"
CLAIMS,0.6939182452642074,"• The abstract and/or introduction should clearly state the claims made, including the
569"
CLAIMS,0.6949152542372882,"contributions made in the paper and important assumptions and limitations. A No or
570"
CLAIMS,0.6959122632103689,"NA answer to this question will not be perceived well by the reviewers.
571"
CLAIMS,0.6969092721834497,"• The claims made should match theoretical and experimental results, and reflect how
572"
CLAIMS,0.6979062811565304,"much the results can be expected to generalize to other settings.
573"
CLAIMS,0.6989032901296112,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
574"
CLAIMS,0.6999002991026919,"are not attained by the paper.
575"
LIMITATIONS,0.7008973080757727,"2. Limitations
576"
LIMITATIONS,0.7018943170488534,"Question: Does the paper discuss the limitations of the work performed by the authors?
577"
LIMITATIONS,0.7028913260219342,"Answer: [Yes]
578"
LIMITATIONS,0.7038883349950149,"Justification: There is a ""Limitation"" section containing information about the execution
579"
LIMITATIONS,0.7048853439680958,"speed of our method.
580"
LIMITATIONS,0.7058823529411765,"Guidelines:
581"
LIMITATIONS,0.7068793619142573,"• The answer NA means that the paper has no limitation while the answer No means that
582"
LIMITATIONS,0.707876370887338,"the paper has limitations, but those are not discussed in the paper.
583"
LIMITATIONS,0.7088733798604188,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
584"
LIMITATIONS,0.7098703888334995,"• The paper should point out any strong assumptions and how robust the results are to
585"
LIMITATIONS,0.7108673978065803,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
586"
LIMITATIONS,0.711864406779661,"model well-specification, asymptotic approximations only holding locally). The authors
587"
LIMITATIONS,0.7128614157527418,"should reflect on how these assumptions might be violated in practice and what the
588"
LIMITATIONS,0.7138584247258225,"implications would be.
589"
LIMITATIONS,0.7148554336989033,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
590"
LIMITATIONS,0.715852442671984,"only tested on a few datasets or with a few runs. In general, empirical results often
591"
LIMITATIONS,0.7168494516450648,"depend on implicit assumptions, which should be articulated.
592"
LIMITATIONS,0.7178464606181456,"• The authors should reflect on the factors that influence the performance of the approach.
593"
LIMITATIONS,0.7188434695912264,"For example, a facial recognition algorithm may perform poorly when image resolution
594"
LIMITATIONS,0.7198404785643071,"is low or images are taken in low lighting. Or a speech-to-text system might not be
595"
LIMITATIONS,0.7208374875373879,"used reliably to provide closed captions for online lectures because it fails to handle
596"
LIMITATIONS,0.7218344965104686,"technical jargon.
597"
LIMITATIONS,0.7228315054835494,"• The authors should discuss the computational efficiency of the proposed algorithms
598"
LIMITATIONS,0.7238285144566301,"and how they scale with dataset size.
599"
LIMITATIONS,0.7248255234297108,"• If applicable, the authors should discuss possible limitations of their approach to
600"
LIMITATIONS,0.7258225324027916,"address problems of privacy and fairness.
601"
LIMITATIONS,0.7268195413758723,"• While the authors might fear that complete honesty about limitations might be used by
602"
LIMITATIONS,0.7278165503489531,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
603"
LIMITATIONS,0.7288135593220338,"limitations that aren’t acknowledged in the paper. The authors should use their best
604"
LIMITATIONS,0.7298105682951147,"judgment and recognize that individual actions in favor of transparency play an impor-
605"
LIMITATIONS,0.7308075772681955,"tant role in developing norms that preserve the integrity of the community. Reviewers
606"
LIMITATIONS,0.7318045862412762,"will be specifically instructed to not penalize honesty concerning limitations.
607"
THEORY ASSUMPTIONS AND PROOFS,0.732801595214357,"3. Theory Assumptions and Proofs
608"
THEORY ASSUMPTIONS AND PROOFS,0.7337986041874377,"Question: For each theoretical result, does the paper provide the full set of assumptions and
609"
THEORY ASSUMPTIONS AND PROOFS,0.7347956131605184,"a complete (and correct) proof?
610"
THEORY ASSUMPTIONS AND PROOFS,0.7357926221335992,"Answer: [Yes]
611"
THEORY ASSUMPTIONS AND PROOFS,0.7367896311066799,"Justification: Our paper reports the theoretical approaches of the NODE reformulation
612"
THEORY ASSUMPTIONS AND PROOFS,0.7377866400797607,"process for curve adjustment equations.
613"
THEORY ASSUMPTIONS AND PROOFS,0.7387836490528414,"Guidelines:
614"
THEORY ASSUMPTIONS AND PROOFS,0.7397806580259222,"• The answer NA means that the paper does not include theoretical results.
615"
THEORY ASSUMPTIONS AND PROOFS,0.7407776669990029,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
616"
THEORY ASSUMPTIONS AND PROOFS,0.7417746759720838,"referenced.
617"
THEORY ASSUMPTIONS AND PROOFS,0.7427716849451645,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
618"
THEORY ASSUMPTIONS AND PROOFS,0.7437686939182453,"• The proofs can either appear in the main paper or the supplemental material, but if
619"
THEORY ASSUMPTIONS AND PROOFS,0.744765702891326,"they appear in the supplemental material, the authors are encouraged to provide a short
620"
THEORY ASSUMPTIONS AND PROOFS,0.7457627118644068,"proof sketch to provide intuition.
621"
THEORY ASSUMPTIONS AND PROOFS,0.7467597208374875,"• Inversely, any informal proof provided in the core of the paper should be complemented
622"
THEORY ASSUMPTIONS AND PROOFS,0.7477567298105683,"by formal proofs provided in appendix or supplemental material.
623"
THEORY ASSUMPTIONS AND PROOFS,0.748753738783649,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497507477567298,"4. Experimental Result Reproducibility
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7507477567298105,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7517447657028913,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.752741774675972,"of the paper (regardless of whether the code and data are provided or not)?
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7537387836490529,"Answer: [Yes]
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7547357926221336,"Justification: Our paper includes main experimental results as well as ablation experiment
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7557328015952144,"results.
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567298105682951,"Guidelines:
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7577268195413759,"• The answer NA means that the paper does not include experiments.
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7587238285144566,"• If the paper includes experiments, a No answer to this question will not be perceived
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597208374875374,"well by the reviewers: Making the paper reproducible is important, regardless of
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607178464606181,"whether the code and data are provided or not.
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7617148554336989,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7627118644067796,"to make their results reproducible or verifiable.
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7637088733798604,"• Depending on the contribution, reproducibility can be accomplished in various ways.
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7647058823529411,"For example, if the contribution is a novel architecture, describing the architecture fully
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.765702891326022,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7666999002991027,"be necessary to either make it possible for others to replicate the model with the same
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7676969092721835,"dataset, or provide access to the model. In general. releasing code and data is often
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7686939182452642,"one good way to accomplish this, but reproducibility can also be provided via detailed
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.769690927218345,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7706879361914257,"of a large language model), releasing of a model checkpoint, or other means that are
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7716849451645065,"appropriate to the research performed.
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7726819541375872,"• While NeurIPS does not require releasing code, the conference does require all submis-
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.773678963110668,"sions to provide some reasonable avenue for reproducibility, which may depend on the
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746759720837487,"nature of the contribution. For example
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7756729810568295,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7766699900299102,"to reproduce that algorithm.
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7776669990029911,"(b) If the contribution is primarily a new model architecture, the paper should describe
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7786640079760718,"the architecture clearly and fully.
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7796610169491526,"(c) If the contribution is a new model (e.g., a large language model), then there should
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7806580259222333,"either be a way to access this model for reproducing the results or a way to reproduce
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7816550348953141,"the model (e.g., with an open-source dataset or instructions for how to construct
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7826520438683948,"the dataset).
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7836490528414756,"(d) We recognize that reproducibility may be tricky in some cases, in which case
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7846460618145563,"authors are welcome to describe the particular way they provide for reproducibility.
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7856430707876371,"In the case of closed-source models, it may be that access to the model is limited in
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7866400797607178,"some way (e.g., to registered users), but it should be possible for other researchers
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7876370887337986,"to have some path to reproducing or verifying the results.
663"
OPEN ACCESS TO DATA AND CODE,0.7886340977068793,"5. Open access to data and code
664"
OPEN ACCESS TO DATA AND CODE,0.7896311066799602,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
665"
OPEN ACCESS TO DATA AND CODE,0.7906281156530409,"tions to faithfully reproduce the main experimental results, as described in supplemental
666"
OPEN ACCESS TO DATA AND CODE,0.7916251246261217,"material?
667"
OPEN ACCESS TO DATA AND CODE,0.7926221335992024,"Answer: [Yes]
668"
OPEN ACCESS TO DATA AND CODE,0.7936191425722832,"Justification: In our paper, we include the rationale behind NODE reformulation along
669"
OPEN ACCESS TO DATA AND CODE,0.7946161515453639,"with the workflow, and provide the network architecture in the appendix. This ensures
670"
OPEN ACCESS TO DATA AND CODE,0.7956131605184447,"reproducibility, and we will also provide the code upon acceptance.
671"
OPEN ACCESS TO DATA AND CODE,0.7966101694915254,"Guidelines:
672"
OPEN ACCESS TO DATA AND CODE,0.7976071784646062,"• The answer NA means that paper does not include experiments requiring code.
673"
OPEN ACCESS TO DATA AND CODE,0.7986041874376869,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
674"
OPEN ACCESS TO DATA AND CODE,0.7996011964107677,"public/guides/CodeSubmissionPolicy) for more details.
675"
OPEN ACCESS TO DATA AND CODE,0.8005982053838484,"• While we encourage the release of code and data, we understand that this might not be
676"
OPEN ACCESS TO DATA AND CODE,0.8015952143569293,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
677"
OPEN ACCESS TO DATA AND CODE,0.80259222333001,"including code, unless this is central to the contribution (e.g., for a new open-source
678"
OPEN ACCESS TO DATA AND CODE,0.8035892323030908,"benchmark).
679"
OPEN ACCESS TO DATA AND CODE,0.8045862412761715,"• The instructions should contain the exact command and environment needed to run to
680"
OPEN ACCESS TO DATA AND CODE,0.8055832502492523,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
681"
OPEN ACCESS TO DATA AND CODE,0.806580259222333,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
682"
OPEN ACCESS TO DATA AND CODE,0.8075772681954138,"• The authors should provide instructions on data access and preparation, including how
683"
OPEN ACCESS TO DATA AND CODE,0.8085742771684945,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
684"
OPEN ACCESS TO DATA AND CODE,0.8095712861415753,"• The authors should provide scripts to reproduce all experimental results for the new
685"
OPEN ACCESS TO DATA AND CODE,0.810568295114656,"proposed method and baselines. If only a subset of experiments are reproducible, they
686"
OPEN ACCESS TO DATA AND CODE,0.8115653040877367,"should state which ones are omitted from the script and why.
687"
OPEN ACCESS TO DATA AND CODE,0.8125623130608175,"• At submission time, to preserve anonymity, the authors should release anonymized
688"
OPEN ACCESS TO DATA AND CODE,0.8135593220338984,"versions (if applicable).
689"
OPEN ACCESS TO DATA AND CODE,0.8145563310069791,"• Providing as much information as possible in supplemental material (appended to the
690"
OPEN ACCESS TO DATA AND CODE,0.8155533399800599,"paper) is recommended, but including URLs to data and code is permitted.
691"
OPEN ACCESS TO DATA AND CODE,0.8165503489531406,"6. Experimental Setting/Details
692"
OPEN ACCESS TO DATA AND CODE,0.8175473579262214,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
693"
OPEN ACCESS TO DATA AND CODE,0.8185443668993021,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
694"
OPEN ACCESS TO DATA AND CODE,0.8195413758723828,"results?
695"
OPEN ACCESS TO DATA AND CODE,0.8205383848454636,"Answer: [Yes]
696"
OPEN ACCESS TO DATA AND CODE,0.8215353938185443,"Justification: We included details about the dataset used for training in the main manuscript,
697"
OPEN ACCESS TO DATA AND CODE,0.8225324027916251,"while other hyperparameters, weights, etc., are documented in the appendix.
698"
OPEN ACCESS TO DATA AND CODE,0.8235294117647058,"Guidelines:
699"
OPEN ACCESS TO DATA AND CODE,0.8245264207377866,"• The answer NA means that the paper does not include experiments.
700"
OPEN ACCESS TO DATA AND CODE,0.8255234297108675,"• The experimental setting should be presented in the core of the paper to a level of detail
701"
OPEN ACCESS TO DATA AND CODE,0.8265204386839482,"that is necessary to appreciate the results and make sense of them.
702"
OPEN ACCESS TO DATA AND CODE,0.827517447657029,"• The full details can be provided either with the code, in appendix, or as supplemental
703"
OPEN ACCESS TO DATA AND CODE,0.8285144566301097,"material.
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8295114656031904,"7. Experiment Statistical Significance
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8305084745762712,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8315054835493519,"information about the statistical significance of the experiments?
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8325024925224327,"Answer: [Yes]
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8334995014955134,"Justification: We provide detailed information on low-light performance validation, the
709"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8344965104685942,"impact of main contributions, and performance validation through ablation studies.
710"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8354935194416749,"Guidelines:
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8364905284147557,"• The answer NA means that the paper does not include experiments.
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8374875373878365,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8384845463609173,"dence intervals, or statistical significance tests, at least for the experiments that support
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.839481555333998,"the main claims of the paper.
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8404785643070788,"• The factors of variability that the error bars are capturing should be clearly stated (for
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8414755732801595,"example, train/test split, initialization, random drawing of some parameter, or overall
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8424725822532403,"run with given experimental conditions).
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.843469591226321,"• The method for calculating the error bars should be explained (closed form formula,
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8444666001994018,"call to a library function, bootstrap, etc.)
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8454636091724825,"• The assumptions made should be given (e.g., Normally distributed errors).
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8464606181455633,"• It should be clear whether the error bar is the standard deviation or the standard error
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.847457627118644,"of the mean.
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8484546360917248,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8494516450648056,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8504486540378864,"of Normality of errors is not verified.
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8514456630109671,"• For asymmetric distributions, the authors should be careful not to show in tables or
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8524426719840479,"figures symmetric error bars that would yield results that are out of range (e.g. negative
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8534396809571286,"error rates).
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8544366899302094,"• If error bars are reported in tables or plots, The authors should explain in the text how
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8554336989032901,"they were calculated and reference the corresponding figures or tables in the text.
731"
EXPERIMENTS COMPUTE RESOURCES,0.8564307078763709,"8. Experiments Compute Resources
732"
EXPERIMENTS COMPUTE RESOURCES,0.8574277168494516,"Question: For each experiment, does the paper provide sufficient information on the com-
733"
EXPERIMENTS COMPUTE RESOURCES,0.8584247258225324,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
734"
EXPERIMENTS COMPUTE RESOURCES,0.8594217347956131,"the experiments?
735"
EXPERIMENTS COMPUTE RESOURCES,0.8604187437686939,"Answer: [Yes]
736"
EXPERIMENTS COMPUTE RESOURCES,0.8614157527417746,"Justification: We provide information about the GPU resources used and the execution time.
737"
EXPERIMENTS COMPUTE RESOURCES,0.8624127617148555,"Guidelines:
738"
EXPERIMENTS COMPUTE RESOURCES,0.8634097706879362,"• The answer NA means that the paper does not include experiments.
739"
EXPERIMENTS COMPUTE RESOURCES,0.864406779661017,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
740"
EXPERIMENTS COMPUTE RESOURCES,0.8654037886340977,"or cloud provider, including relevant memory and storage.
741"
EXPERIMENTS COMPUTE RESOURCES,0.8664007976071785,"• The paper should provide the amount of compute required for each of the individual
742"
EXPERIMENTS COMPUTE RESOURCES,0.8673978065802592,"experimental runs as well as estimate the total compute.
743"
EXPERIMENTS COMPUTE RESOURCES,0.86839481555334,"• The paper should disclose whether the full research project required more compute
744"
EXPERIMENTS COMPUTE RESOURCES,0.8693918245264207,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
745"
EXPERIMENTS COMPUTE RESOURCES,0.8703888334995015,"didn’t make it into the paper).
746"
CODE OF ETHICS,0.8713858424725822,"9. Code Of Ethics
747"
CODE OF ETHICS,0.872382851445663,"Question: Does the research conducted in the paper conform, in every respect, with the
748"
CODE OF ETHICS,0.8733798604187437,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
749"
CODE OF ETHICS,0.8743768693918246,"Answer: [Yes]
750"
CODE OF ETHICS,0.8753738783649053,"Justification: We have reviewed the ethical guidelines (Code of Ethics) and ensured compli-
751"
CODE OF ETHICS,0.8763708873379861,"ance.
752"
CODE OF ETHICS,0.8773678963110668,"Guidelines:
753"
CODE OF ETHICS,0.8783649052841476,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
754"
CODE OF ETHICS,0.8793619142572283,"• If the authors answer No, they should explain the special circumstances that require a
755"
CODE OF ETHICS,0.8803589232303091,"deviation from the Code of Ethics.
756"
CODE OF ETHICS,0.8813559322033898,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
757"
CODE OF ETHICS,0.8823529411764706,"eration due to laws or regulations in their jurisdiction).
758"
BROADER IMPACTS,0.8833499501495513,"10. Broader Impacts
759"
BROADER IMPACTS,0.8843469591226321,"Question: Does the paper discuss both potential positive societal impacts and negative
760"
BROADER IMPACTS,0.8853439680957128,"societal impacts of the work performed?
761"
BROADER IMPACTS,0.8863409770687937,"Answer: [NA]
762"
BROADER IMPACTS,0.8873379860418744,"Justification: There is no societal impact of our work.
763"
BROADER IMPACTS,0.8883349950149552,"Guidelines:
764"
BROADER IMPACTS,0.8893320039880359,"• The answer NA means that there is no societal impact of the work performed.
765"
BROADER IMPACTS,0.8903290129611167,"• If the authors answer NA or No, they should explain why their work has no societal
766"
BROADER IMPACTS,0.8913260219341974,"impact or why the paper does not address societal impact.
767"
BROADER IMPACTS,0.8923230309072782,"• Examples of negative societal impacts include potential malicious or unintended uses
768"
BROADER IMPACTS,0.8933200398803589,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
769"
BROADER IMPACTS,0.8943170488534397,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
770"
BROADER IMPACTS,0.8953140578265204,"groups), privacy considerations, and security considerations.
771"
BROADER IMPACTS,0.8963110667996012,"• The conference expects that many papers will be foundational research and not tied
772"
BROADER IMPACTS,0.8973080757726819,"to particular applications, let alone deployments. However, if there is a direct path to
773"
BROADER IMPACTS,0.8983050847457628,"any negative applications, the authors should point it out. For example, it is legitimate
774"
BROADER IMPACTS,0.8993020937188435,"to point out that an improvement in the quality of generative models could be used to
775"
BROADER IMPACTS,0.9002991026919243,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
776"
BROADER IMPACTS,0.901296111665005,"that a generic algorithm for optimizing neural networks could enable people to train
777"
BROADER IMPACTS,0.9022931206380858,"models that generate Deepfakes faster.
778"
BROADER IMPACTS,0.9032901296111665,"• The authors should consider possible harms that could arise when the technology is
779"
BROADER IMPACTS,0.9042871385842473,"being used as intended and functioning correctly, harms that could arise when the
780"
BROADER IMPACTS,0.905284147557328,"technology is being used as intended but gives incorrect results, and harms following
781"
BROADER IMPACTS,0.9062811565304087,"from (intentional or unintentional) misuse of the technology.
782"
BROADER IMPACTS,0.9072781655034895,"• If there are negative societal impacts, the authors could also discuss possible mitigation
783"
BROADER IMPACTS,0.9082751744765702,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
784"
BROADER IMPACTS,0.909272183449651,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
785"
BROADER IMPACTS,0.9102691924227319,"feedback over time, improving the efficiency and accessibility of ML).
786"
SAFEGUARDS,0.9112662013958126,"11. Safeguards
787"
SAFEGUARDS,0.9122632103688934,"Question: Does the paper describe safeguards that have been put in place for responsible
788"
SAFEGUARDS,0.9132602193419741,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
789"
SAFEGUARDS,0.9142572283150548,"image generators, or scraped datasets)?
790"
SAFEGUARDS,0.9152542372881356,"Answer: [NA]
791"
SAFEGUARDS,0.9162512462612163,"Justification: Our proposed model does not include a high risk for misuse.
792"
SAFEGUARDS,0.9172482552342971,"Guidelines:
793"
SAFEGUARDS,0.9182452642073778,"• The answer NA means that the paper poses no such risks.
794"
SAFEGUARDS,0.9192422731804586,"• Released models that have a high risk for misuse or dual-use should be released with
795"
SAFEGUARDS,0.9202392821535393,"necessary safeguards to allow for controlled use of the model, for example by requiring
796"
SAFEGUARDS,0.9212362911266201,"that users adhere to usage guidelines or restrictions to access the model or implementing
797"
SAFEGUARDS,0.922233300099701,"safety filters.
798"
SAFEGUARDS,0.9232303090727817,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
799"
SAFEGUARDS,0.9242273180458624,"should describe how they avoided releasing unsafe images.
800"
SAFEGUARDS,0.9252243270189432,"• We recognize that providing effective safeguards is challenging, and many papers do
801"
SAFEGUARDS,0.9262213359920239,"not require this, but we encourage authors to take this into account and make a best
802"
SAFEGUARDS,0.9272183449651047,"faith effort.
803"
LICENSES FOR EXISTING ASSETS,0.9282153539381854,"12. Licenses for existing assets
804"
LICENSES FOR EXISTING ASSETS,0.9292123629112662,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
805"
LICENSES FOR EXISTING ASSETS,0.9302093718843469,"the paper, properly credited and are the license and terms of use explicitly mentioned and
806"
LICENSES FOR EXISTING ASSETS,0.9312063808574277,"properly respected?
807"
LICENSES FOR EXISTING ASSETS,0.9322033898305084,"Answer: [Yes]
808"
LICENSES FOR EXISTING ASSETS,0.9332003988035892,"Justification: We will accurately specify the original owner’s license and share the code
809"
LICENSES FOR EXISTING ASSETS,0.93419740777667,"upon acceptance. Additionally, the source and the original owner’s name for the single
810"
LICENSES FOR EXISTING ASSETS,0.9351944167497508,"image used in the appendix have been indicated on the image.
811"
LICENSES FOR EXISTING ASSETS,0.9361914257228315,"Guidelines:
812"
LICENSES FOR EXISTING ASSETS,0.9371884346959123,"• The answer NA means that the paper does not use existing assets.
813"
LICENSES FOR EXISTING ASSETS,0.938185443668993,"• The authors should cite the original paper that produced the code package or dataset.
814"
LICENSES FOR EXISTING ASSETS,0.9391824526420738,"• The authors should state which version of the asset is used and, if possible, include a
815"
LICENSES FOR EXISTING ASSETS,0.9401794616151545,"URL.
816"
LICENSES FOR EXISTING ASSETS,0.9411764705882353,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
817"
LICENSES FOR EXISTING ASSETS,0.942173479561316,"• For scraped data from a particular source (e.g., website), the copyright and terms of
818"
LICENSES FOR EXISTING ASSETS,0.9431704885343968,"service of that source should be provided.
819"
LICENSES FOR EXISTING ASSETS,0.9441674975074775,"• If assets are released, the license, copyright information, and terms of use in the
820"
LICENSES FOR EXISTING ASSETS,0.9451645064805583,"package should be provided. For popular datasets, paperswithcode.com/datasets
821"
LICENSES FOR EXISTING ASSETS,0.9461615154536391,"has curated licenses for some datasets. Their licensing guide can help determine the
822"
LICENSES FOR EXISTING ASSETS,0.9471585244267199,"license of a dataset.
823"
LICENSES FOR EXISTING ASSETS,0.9481555333998006,"• For existing datasets that are re-packaged, both the original license and the license of
824"
LICENSES FOR EXISTING ASSETS,0.9491525423728814,"the derived asset (if it has changed) should be provided.
825"
LICENSES FOR EXISTING ASSETS,0.9501495513459621,"• If this information is not available online, the authors are encouraged to reach out to
826"
LICENSES FOR EXISTING ASSETS,0.9511465603190429,"the asset’s creators.
827"
NEW ASSETS,0.9521435692921236,"13. New Assets
828"
NEW ASSETS,0.9531405782652044,"Question: Are new assets introduced in the paper well documented and is the documentation
829"
NEW ASSETS,0.9541375872382851,"provided alongside the assets?
830"
NEW ASSETS,0.9551345962113659,"Answer: [NA]
831"
NEW ASSETS,0.9561316051844466,"Justification: We do not introduce new assets in this paper.
832"
NEW ASSETS,0.9571286141575274,"Guidelines:
833"
NEW ASSETS,0.9581256231306082,"• The answer NA means that the paper does not release new assets.
834"
NEW ASSETS,0.959122632103689,"• Researchers should communicate the details of the dataset/code/model as part of their
835"
NEW ASSETS,0.9601196410767697,"submissions via structured templates. This includes details about training, license,
836"
NEW ASSETS,0.9611166500498505,"limitations, etc.
837"
NEW ASSETS,0.9621136590229312,"• The paper should discuss whether and how consent was obtained from people whose
838"
NEW ASSETS,0.963110667996012,"asset is used.
839"
NEW ASSETS,0.9641076769690927,"• At submission time, remember to anonymize your assets (if applicable). You can either
840"
NEW ASSETS,0.9651046859421735,"create an anonymized URL or include an anonymized zip file.
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9661016949152542,"14. Crowdsourcing and Research with Human Subjects
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967098703888335,"Question: For crowdsourcing experiments and research with human subjects, does the paper
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9680957128614157,"include the full text of instructions given to participants and screenshots, if applicable, as
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9690927218344965,"well as details about compensation (if any)?
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9700897308075773,"Answer: [NA]
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9710867397806581,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9720837487537388,"Guidelines:
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9730807577268196,"• The answer NA means that the paper does not involve crowdsourcing nor research with
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9740777666999003,"human subjects.
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9750747756729811,"• Including this information in the supplemental material is fine, but if the main contribu-
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760717846460618,"tion of the paper involves human subjects, then as much detail as possible should be
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9770687936191426,"included in the main paper.
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780658025922233,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790628115653041,"or other labor should be paid at least the minimum wage in the country of the data
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800598205383848,"collector.
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810568295114656,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820538384845464,"Subjects
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830508474576272,"Question: Does the paper describe potential risks incurred by study participants, whether
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840478564307079,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850448654037887,"approvals (or an equivalent approval/review based on the requirements of your country or
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860418743768694,"institution) were obtained?
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870388833499502,"Answer: [NA]
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880358923230309,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890329012961117,"Guidelines:
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900299102691924,"• The answer NA means that the paper does not involve crowdsourcing nor research with
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910269192422732,"human subjects.
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920239282153539,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930209371884346,"may be required for any human subjects research. If you obtained IRB approval, you
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940179461615155,"should clearly state this in the paper.
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950149551345963,"• We recognize that the procedures for this may vary significantly between institutions
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996011964107677,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970089730807578,"guidelines for their institution.
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980059820538385,"• For initial submissions, do not include any information that would break anonymity (if
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990029910269193,"applicable), such as the institution conducting the review.
875"
