Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006978367062107466,"We consider the well-studied problem of completing a rank-r, µ-incoherent matrix
M ∈Rd×d from incomplete observations. We focus on this problem in the semi-
random setting where each entry is independently revealed with probability at
least p =
poly(r,µ,log d)"
ABSTRACT,0.0013956734124214933,"d
. Whereas multiple nearly-linear time algorithms have
been established in the more specialized fully-random setting where each entry is
revealed with probablity exactly p, the only known nearly-linear time algorithm in
the semi-random setting is due to [17], whose sample complexity has a polynomial
dependence on the inverse accuracy and condition number and thus cannot achieve
high-accuracy recovery. Our main result is the first high-accuracy nearly-linear
time algorithm for solving semi-random matrix completion, and an extension to the
noisy observation setting. Our result builds upon the recent short-flat decomposition
framework of [42, 43] and leverages fast algorithms for flow problems on graphs
to solve adaptive reweighting subproblems efficiently."
INTRODUCTION,0.00209351011863224,"1
Introduction"
INTRODUCTION,0.0027913468248429866,"How can we ensure learning algorithms do not overfit to generative assumptions on their input
data? Since worst-case statistical inference problems are often intractable (i.e., without distributional
assumptions), to develop efficient algorithms, we often introduce a generative model as a proxy for
average-case, real-world behavior. However, if these algorithms are to be useful practically, we must
ensure they do not depend too strongly on any artificial properties of the generative model. Ideally,
they should require only what is statistically and algorithmically necessary, and no more."
INTRODUCTION,0.0034891835310537334,"In this paper, we consider this question in the context of matrix completion, one of the most
fundamental and well-studied linear inverse problems. In matrix completion, the goal is to recover
a symmetric rank-r matrix M ∈Rd×d where r ≪d,6 given a small (typically sublinear) number
of entries revealed independently and at random. In a seminal line of work [13, 11, 12, 61], it was
shown that under natural structural assumptions on M, one can fully recover M given ≈dr randomly
revealed entries in polynomial time, via semidefinite programming (SDP). Subsequently, there has
been extensive work attempting to improve upon statistical and computational guarantees for matrix
completion. In an oversimplification of the literature (see Section 1.3 for a more detailed and nuanced
discussion), this has resulted in two categories of matrix completion algorithms."
INTRODUCTION,0.00418702023726448,"• SDP-based algorithms achieve the nearly-optimal sample complexity, but are based on solving
SDPs, which requires time Ω(d3.5) even with state of the art solvers [39, 34]."
INTRODUCTION,0.004884856943475227,"• First-order iterative methods require a slightly higher sample complexity, namely, they require
m ≈pd2 revealed entries for an observation probability p = Ω(poly(r, log d)) · 1"
INTRODUCTION,0.005582693649685973,"d. However,"
INTRODUCTION,0.00628053035589672,"∗MIT, kelner@mit.edu.
†Microsoft Research, jerrl@microsoft.com.
‡MIT, cliu568@mit.edu.
§Stanford University, sidford@stanford.edu.
¶University of Texas at Austin, kjtian@cs.utexas.edu.
6Our results also hold for asymmetric, rectangular matrices. See the discussion at the end of Appendix A."
INTRODUCTION,0.006978367062107467,"they use faster optimization primitives and can be implemented in time eO(m · poly(r, log d)).
Throughout the introduction, we will somewhat informally refer to such algorithms as fast.7"
INTRODUCTION,0.007676203768318213,"Because matrix completion is particularly interesting to study when the target rank r is substantially
smaller than the dimension d (as information-theoretically, we require ≳dr observations), it may
appear that fast algorithms are able to nearly match the statistical guarantees of the SDP-based
methods, while offering substantially lower runtimes. In light of this, naïvely one might expect that
first-order iterative methods ought to be always preferred over the alternative. However, it has been
noted that in many real-world situations, these first-order iterative methods actually fail to perform
reliably, see e.g. [66, 70, 26, 68]. An oft-cited reason for this failure is exactly the tendency for these
sorts of methods to demonstrate the type of generative overfitting described above [66, 70, 4]. Indeed,
in real-world applications, the distribution of revealed entries is usually very far from uniformly
random [4], which can cause iterative methods that rely too heavily on this assumption to fail
dramatically. This begs the following natural question:"
INTRODUCTION,0.00837404047452896,Can we design fast algorithms for matrix completion that are robust to generative overfitting?
INTRODUCTION,0.009071877180739707,"In light of this discussion, we consider a semi-random variant of matrix completion, introduced
in [17]. Recall that in standard matrix completion, we assume every entry (i, j) ∈[d] × [d] is
revealed independently with probability p. In the p-semi-random model (Definition 2), we only know
p ∈(0, 1) such that every entry is revealed independently with an unknown probability pij ≥p.
Given that real-world data is unlikely to be generated exactly uniformly, it is reasonable to believe
that the semi-random model is a more faithful model of matrix completion in practice."
INTRODUCTION,0.009769713886950453,"In this model, the algorithm is given strictly more information than in the standard (uniform) matrix
completion setup. This is because the semi-random model is equivalent to the setting where we are
first given a standard matrix completion instance, and then additional (but potentially adversarially
chosen) entries are revealed.8 It is straightforward to show that the SDP-based approaches generalize
to succeed in the semi-random setting, since the adversary’s revealed entries just translate to additional
constraints in the convex program satisfied by the ground truth. However, as demonstrated in [17],
Appendix A, previously known first-order iterative methods can fail to converge in this semi-random
setting. We view this as strong theoretical evidence that such algorithms exhibit large amounts of
generative overfitting. We can now more concretely rephrase the previous question:"
INTRODUCTION,0.0104675505931612,Can we design fast algorithms for semi-random matrix completion?
INTRODUCTION,0.011165387299371946,"This paper focuses on the theoretical complexity of the problem; the relationship of the techniques
in this work to efficient matrix completion in practice is an interesting direction for future research.
Precisely, we seek to design an algorithm for semi-random matrix completion which works for
the same (or qualitatively similar) p as in the standard setting, and which runs in time eO(m ·
poly(r, log d))? Here m denotes the total number of revealed entries, a proxy for the input size; note
that in the semi-random setting, it could be that m ≫pd2."
INTRODUCTION,0.011863224005582694,"The main result of [17] is an efficient algorithm for semi-random matrix completion, albeit with some
important caveats, as we discuss shortly. Roughly speaking, for ϵ ∈(0, 1), [17] gave an algorithm
which, given observations of rank-r M with condition number κ from the p-semi-random model for
p = 1"
INTRODUCTION,0.01256106071179344,"d · eO(poly(r, κ, 1"
INTRODUCTION,0.013258897418004187,"ϵ )), outputs M′ so that ∥M′ −M∥2
F ≤ϵ∥M∥2
F with high probability. Moreover,
it runs in nearly-linear time eO(m · poly(r, κ, 1"
INTRODUCTION,0.013956734124214934,"ϵ )), where m is the number of revealed entries. While
this is a promising first step towards matrix completion in the semi-random model, it leaves several
important questions unanswered."
INTRODUCTION,0.01465457083042568,"• High-accuracy recovery. Both the sample complexity and runtime of the algorithm in [17] depend
polynomially on ϵ−1. In particular, the algorithm cannot recover the matrix M to ϵ−1 = poly(d)
levels of precision in nearly-linear time. Thus, even in the standard setting where we assume
that the bit complexity of the entries of M are polynomially bounded, [17] cannot achieve exact"
INTRODUCTION,0.015352407536636426,"7Here, and throughout the paper, we use the notation eO(f) := O(f logO(1) f) and similarly define eΩ.
8As clarified in Definition 2, it is important in this equivalence that the additionally revealed entries are not
adaptively chosen, i.e., do not depend on which random entries were observed. This manifests in our analysis
because an adaptive adversary causes significant difficulties with independent sample splitting (see Lemma 2).
There is precedent from the literature on fully-random matrix completion that the inability to sample split causes
analysis issues (see, e.g., Section 3 of [61]); extending to an adaptive adversary is a challenging open direction."
INTRODUCTION,0.016050244242847175,"recovery of the matrix in nearly-linear time. Unfortunately, this dependence seems inherent to the
techniques therein (see the discussion in Section 1.2)."
INTRODUCTION,0.01674808094905792,"• Condition number dependence. Another important downside of the [17] algorithm’s guarantee
is its polynomial dependences on κ in both the sample complexity and runtime. No dependence
on κ is required information-theoretically, or by the SDP-based methods. This aspect also seems
somewhat inherent to the non-convex iterative method used in [17]."
INTRODUCTION,0.017445917655268667,"• Noise tolerance. Finally, the guarantees in [17] require that M is exactly low-rank, i.e. it does
not generalize to handle noise in observations. In practice, this is unlikely, and guarantees should
ideally hold even when we only assume that the observed M is close to low-rank."
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.018143754361479414,"1.1
Our results
Our main result is a new fast algorithm for semi-random matrix completion that addresses all three of
the aforementioned issues. Specifically, we show the following main result."
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.01884159106769016,"Theorem 1 (informal, see Theorem 2). Let M⋆∈Rd×d be a symmetric, rank-r, µ-incoherent
matrix, let symmetric N ∈Rd×d satisfy ∥N∥∞≤∆, and let c
M = M⋆+ N. Let ϵ ∈(0, 1). There
is an algorithm that takes observations from c
M in the p-semi-random model for p = Q"
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.019539427773900907,"d , where
Q = poly(r, µ, log( d"
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.020237264480111653,"ϵ )), and outputs M as a rank-Q factorization such that with high probability,"
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.0209351011863224,∥M −M⋆∥∞≤Q (∆+ ϵ ∥M⋆∥∞) .
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.021632937892533146,The algorithm runs in nearly-linear time O(mQ) where m is the number of revealed entries.
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.022330774598743892,"Here, ∥·∥∞refers to the elementwise ℓ∞norm of a matrix, and ∥·∥F refers to the Frobenius norm. We
pause to comment on Theorem 2. First, in the noiseless setting, i.e., when ∆= 0, our algorithm has a
polylogarithmic dependence on ϵ−1, and thus achieves high-accuracy recovery in nearly-linear time.
In particular, as long as the bit complexity of the underlying matrix is polynomially bounded, we
achieve exact recovery in nearly-linear time. Moreover, our rate has no dependence on the condition
number κ. In both of these aspects, our result qualitatively improves upon [17], while retaining a
nearly-linear runtime and eO(d · poly(r)) sample complexity. We also remark that the incoherence
assumption in Theorem 2 (or a less standard variant thereof), which is formally defined in Definition 1,
is used in all low-rank matrix completion algorithms we are aware of, as the problem is intractable
without subspace regularity assumptions (see, e.g., discussion in [43])."
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.023028611304954642,"Finally, we observe that our guarantee applies to the noisy setting, i.e., ∆> 0. To our knowledge, this
is the first such guarantee in the literature for semi-random matrix completion, even for SDP-based
methods. We achieve guarantees in terms of the ℓ∞norm of the noise matrix N; moreover, the
overhead is a polynomial of only r log( d"
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.02372644801116539,"ϵ ), a preferable guarantee to the poly(d) overheads known to
be achievable in the standard, fully random setting by semidefinite programming [11]."
"OUR RESULTS
OUR MAIN RESULT IS A NEW FAST ALGORITHM FOR SEMI-RANDOM MATRIX COMPLETION THAT ADDRESSES ALL THREE OF",0.024424284717376135,"We do note that in the standard matrix completion setting, error guarantees are typically phrased in
terms of the Frobenius norm of the noise N [11, 43]. However, in the semi-random setting, some
measure of the “element-wise” noise level is unavoidable, because the semi-random noise could
be chosen to only reveal unusually large entries of the noise matrix. We believe it is an interesting
open question whether or not one can achieve somewhat more global measures of error such as the
Frobenius norm of large entries in the revealed noise, which are more comparable to guarantees
recently attained for semi-random sparse linear regression [42]."
OUR TECHNIQUES,0.02512212142358688,"1.2
Our techniques
We achieve these results via a fundamentally different approach than the prior work of [17], detailed
in Section 2. At a (very) high level, [17] relies on finding a single reweighting of the revealed
entries of M that is good in a global sense. This roughly corresponds to finding a mask of the
revealed entries, so that the masked matrix is guaranteed to be a good spectral approximation of the
ground truth matrix. Given such a mask, [17] then shows that a non-convex optimization method
will recover the true matrix, with high probability. While this is a very natural approach, this also
directly results in a poly( 1"
OUR TECHNIQUES,0.025819958129797628,"ϵ ) dependence in the runtime and sample complexity of the algorithm. This
is because the quality of the spectral approximation directly goes into the final accuracy of their
overall algorithm; ultimately, they need a (1 + ϵ)-multiplicative spectral approximation to the ground
truth. However, all known techniques for finding such a high-quality spectral approximation require a
poly( 1"
OUR TECHNIQUES,0.026517794836008374,ϵ ) dependence in both sample complexity and runtime.
OUR TECHNIQUES,0.02721563154221912,"We instead devise an iterative method that is guaranteed to make progress, assuming that the iterate
satisfies a local progress criterion. We then find a local reweighting of the matrix which allows us to
find a step which satisfies this criterion. While this local progress criterion is much more involved to
state, and this reweighting must now be done every iteration (rather than just once at the beginning of
the algorithm), this framework has an important advantage over the global criterion used previously.
Critically, now each step of our method need only seek a reweighting which achieves a constant
factor of relative progress to the target matrix. Thus, rather than needing a (1 + ϵ)-multiplicative
approximation at every step, we only need a constant multiplicative approximation to make progress.
This is the main insight which allows us to achieve polylogarithmic rates in 1 ϵ ."
OUR TECHNIQUES,0.027913468248429867,"Our algorithm builds upon the short-flat decomposition-based approach to designing iterative methods
for linear inverse problems introduced in [42], and specialized in [43] to matrix completion in the
standard setting. However, there are significant technical challenges in adapting this framework to
semi-random models. At a high level, the matrix completion problem is not well-conditioned, so a
crucial step in the [43] algorithm is dropping rows and columns of a difference matrix (between an
iterate and the target) which are estimated as too large. Random observations (held out via sample
splitting) are then used in matrix concentration inequalities to estimate the difference matrix on the
remaining indices, and dropped indices are later recovered."
OUR TECHNIQUES,0.028611304954640614,"Unfortunately, all of these steps crucially rely on independent sampling and hence break in the
presence of semi-random noise. We instead take a fresh look at what certificates of progress are
possible in the semi-random setting. Our key technical contribution, Algorithm 2, is a subroutine
which makes bicriteria progress: it either finds an adaptive reweighting which accurately estimates
the difference on a large submatrix after denoising, or certifies that a few rows and columns are
responsible for much of the difference and hence can be dropped. To achieve our nearly-linear runtime,
we crucially exploit graphical structure of feasible reweightings present in the randomly-revealed
entries, and use flow-based optimization algorithms to solve our reweighting subproblem."
RELATED WORK,0.02930914166085136,"1.3
Related work"
RELATED WORK,0.030006978367062107,"Here, we review related work on matrix completion and semi-random models. These lines of research
are vast and we focus on the research most relevant to our setting. For a more comprehensive review,
we refer the interested reader to surveys on these topics, e.g., [41, 60, 59, 63]."
RELATED WORK,0.030704815073272853,"Matrix completion.
Matrix completion was first introduced in [62] in the context of collaborative
filtering, e.g., for the Netflix Prize [6]. Since then, it has found applications in various areas, such as
signal processing [46, 64, 57], social network analysis [47], causal inference [4], and most recently, AI
alignment [1]. In a seminal line of work, [13, 11, 61, 12, 16] demonstrated that SDPs based on nuclear
norm minimization can solve matrix completion with sublinear sample complexity, and in polynomial
time. Another line of work focuses on first-order iterative methods for matrix completion, see
e.g., [44, 31, 32, 33, 36, 74, 67, 18, 72, 30]. These works demonstrate that optimization frameworks,
such as alternating minimization, can provably and efficiently recover the true matrix. Finally, yet
another line of work seeks to understand the non-convex landscape of descent-based methods for
matrix completion [65, 21, 28, 40, 73, 40, 73, 71]. In terms of theoretical runtimes, the state of the
art eO(dr3+o(1)), is given in [43]. While some of the aforementioned papers have implementations,
comprehensively evaluating the practical performance of all of these methods is an interesting
direction for future work. As noted in [17], none of these fast methods are known to succeed in the
presence of semi-random noise."
RELATED WORK,0.0314026517794836,"Semi-random models.
Semi-random models were first introduced in a pair of seminal papers [9,
24], in part as a means to test whether learning algorithms which succeed under random models
generalize to more realistic generative modeling assumptions. Most early work on semi-random
models focused on understanding semi-random variants of various constraint satisfaction problems,
see e.g. [25, 24, 45, 15, 48, 50]. More recently, they have also been studied in the context of learning
theory, in the context of clustering problems [23, 53, 49, 14, 29, 51, 52, 55], sparse and overcomplete
linear regression [37, 42], planted clique [54, 10], and more [27, 8]."
RELATED WORK,0.03210048848569435,"Of particular interest to us is the recent line of work on developing fast learning algorithms in
semi-random models [17, 42, 43, 27, 8]. The closest and most direct comparison to our work is
the aforementioned [17], which we quantitatively improve upon in a number of ways. It is worth
noting that the specific poly(r) ≈r6 dependence in the sample complexity of [17] is lower than in
Theorem 1. We view our result as a promising proof-of-concept of the tractability of semi-random"
RELATED WORK,0.03279832519190509,"matrix completion via fast algorithms; both our dependence and the dependence of [17] are somewhat
large at the moment, limiting immediate practical deployment, but we believe that it is an interest
direction for future research to tighten dependencies on problem parameters and seek more practical
algorithms."
TECHNICAL OVERVIEW,0.03349616189811584,"2
Technical overview"
TECHNICAL OVERVIEW,0.034193998604326585,"At a high level, our algorithm follows the paradigm in [42] for designing fast iterative algorithms that
are robust to semi-random models, which used this framework for sparse linear regression. Broadly,
the approach is to first find deterministic conditions which guarantee that a step of the iterative
method, which descends along a reweighted direction based on semi-random observations, makes
progress. We then develop a custom nearly-linear time algorithm for computing such a reweighting,
using optimization tools specialized to the regularity of graph-structured polytopes."
TECHNICAL OVERVIEW,0.034891835310537335,"Short-flat decompositions for progress.
We first discuss the verifiable conditions we impose
to certify progress of our iterative method. Throughout this section, we let our current iterate be
M ∈Rd×d and let the ground truth be M⋆∈Rd×d, assume both matrices have rank at most r, and
focus on the noiseless setting N = 0d×d for simplicity. We also assume for normalization purposes
that ∥M −M⋆∥F = 1, and our goal is to take a step M′ ←M + D so that"
TECHNICAL OVERVIEW,0.03558967201674808,∥(M + D) −M⋆∥F ≤1
TECHNICAL OVERVIEW,0.03628750872295883,"2 .
(1)"
TECHNICAL OVERVIEW,0.03698534542916958,"Our conditions are inspired by the approach in [43], which gives improved iterative algorithms
for standard matrix completion (without semi-random noise). Let p be the minimum observation
probability, let Ωbe the set of observed entries from M⋆, and let Ω⋆be the set of truly random
observations, i.e., when each entry is revealed with probability exactly p (we can find a coupling
so that Ω⋆⊆Ωalways). Inspired by a similar framework in the sparse recovery setting [42], the
key certificate for guaranteeing progress of the form (1) in [43] is that of a short-flat decomposition.
We say a candidate step D has a short-flat decomposition if we can write D = S + F where
∥S∥F ≤1 and ∥F∥op ≤cr−1"
TECHNICAL OVERVIEW,0.03768318213538032,"2 for a small constant c: here we think of S as the “short” part and F
as the “flat” part of the decomposition. If we can further ensure that enough signal is captured, i.e.,
⟨D, M⋆−M⟩≥1 −c, then [42, 43] give a short argument based on Hölder’s inequality that a step
in D, combined with a low-rank projection for denoising, will imply (1)."
TECHNICAL OVERVIEW,0.03838101884159107,"Thus, it suffices to find a matrix D that has these properties; for intuition, we would ideally like
to take D = S = M⋆−M and F = 0d×d, which would complete the matrix in one shot.
Naturally, however, we are limited by the fact that D should be supported on the observations Ω, as
otherwise we cannot make any guarantees on ⟨D, M⋆−M⟩. Naïvely, one would hope to simply
take the empirical observations D = 1"
TECHNICAL OVERVIEW,0.039078855547801813,"p[M −M⋆]Ω⋆(which zeroes entries outside Ω⋆), an unbiased
estimator of M⋆−M, and appeal to matrix concentration arguments to show existence of a short-flat
decomposition. Unfortunately, this is not the case, as entries, rows, or columns of M⋆−M which
are too large can arbitrarily hinder matrix concentration bounds, and [43] gives simple examples
showing this dependence on the imbalance is unavoidable for uniform reweightings."
TECHNICAL OVERVIEW,0.03977669225401256,"However, [43] made the important insight that when matrix concentration fails, it must be that
M −M⋆is overly concentrated on a few heavy rows and columns, and hence dropping a few rows
and columns significantly reduces ∥M −M⋆∥F. These rows and columns can later be recovered
using the low-rank assumption, once enough progress has been made. The algorithm in [43] then
works by repeatedly estimating heavy rows and columns, explicitly dropping them to form a set
S ⊆[d] of balanced indices, and then taking the step D = 1"
TECHNICAL OVERVIEW,0.040474528960223306,"p[M −M⋆]Ω⋆∩(S×S). This necessity of
dropping certain poorly-behaved indices is a crucial difference between the matrix completion setting
and simpler semi-random inverse problems such as sparse linear regression [42], which possess
stronger global regularity features such as the restricted isometry property (RIP)."
TECHNICAL OVERVIEW,0.041172365666434056,"Certifying progress in semi-random models.
In the semi-random matrix completion problem, we
need several modifications to the strategies of [42, 43], to handle the interplay between estimating
heavy indices and making certifiable progress. Firstly, we cannot accurately estimate the ℓ2 norm of
rows and columns of the true difference matrix M−M⋆from observations, because the semi-random
adversary can drastically distort our estimates. However, we show in Algorithm 1 that we can use the
ℓ∞norm of observed entries as a proxy for heaviness.9 Specifically, after dropping rows and columns"
TECHNICAL OVERVIEW,0.0418702023726448,9This norm distortion is the main source of poly(r) losses in our guarantees compared to the [43] algorithm.
TECHNICAL OVERVIEW,0.04256803907885555,"with the most observed large entries, we argue that all remaining rows and columns are well-balanced
except on few sparse errors, which we exclude from our potential."
TECHNICAL OVERVIEW,0.04326587578506629,"Next, even if appropriate rows and columns were dropped, we cannot use the empirical observations to
construct our step, as semi-random observations are no longer unbiased and the fully-random indices
Ω⋆are unknown. Instead, we use Ω⋆existentially to set up a convex program over reweightings
w ∈RΩ, to search for a candidate step D := P"
TECHNICAL OVERVIEW,0.04396371249127704,"(i,j)∈Ωwij[M −M⋆]ij. Roughly, our convex
program aims to find w satisfying the following constraints (see Lemma 7 for a formal statement)."
TECHNICAL OVERVIEW,0.044661549197487785,"1. For all i ∈[d], P"
TECHNICAL OVERVIEW,0.045359385903698535,"j∈[d]|(i,j)∈Ωwij[M −M⋆]2
ij = O( 1 d)."
TECHNICAL OVERVIEW,0.046057222609909285,"2. There exists S, F ∈Rd×d with D = S + F, ∥S∥F ≤1, and ∥F∥op = O(r−1 2 )."
TECHNICAL OVERVIEW,0.04675505931612003,"3. For a sufficiently small constant c, ⟨D, M −M⋆⟩= P"
TECHNICAL OVERVIEW,0.04745289602233078,"(i,j)∈Ωwij[M −M⋆]2
ij ≥1 −c."
TECHNICAL OVERVIEW,0.04815073272854152,"Item 1 serves to ensure our step D is sufficiently spread out amongst its rows and columns, which
serves two roles: it bounds how much progress is lost when excluding indices, and enforces crucial
problem regularity for our reweighting optimization algorithm. On the other hand, Items 2 and 3 are
the standard criteria for making iterative progress through the short-flat framework of [42, 43]. It is
straightforward to see that the ideal candidate 1"
TECHNICAL OVERVIEW,0.04884856943475227,"p[M −M⋆]Ω⋆∩(S×S) we mentioned earlier is feasible
for these constraints, and we show in Lemma 7 that finding any satisfying reweighting is enough to
ensure progress (with some mild technical changes). It remains to discuss how to find a candidate D
to ensure rapid convergence through the progress bound (1)."
TECHNICAL OVERVIEW,0.04954640614096301,"Flow-based adaptive reweighting.
A key technical contribution of this work is a nearly-linear
time algorithm finding a reweighting D(w) := P"
TECHNICAL OVERVIEW,0.05024424284717376,"(i,j)∈Ωwij[M −M⋆]ij that meets our progress
criteria. Due to our focus on fast algorithms, we require this step to run in O(|Ω| · poly(r)) time. Our
strategy is to form a joint objective over w obeying Item 1, and ∥S∥F ≤1, of the form"
TECHNICAL OVERVIEW,0.050942079553384506,"Fprog-flat(w, S) := −⟨D(w), M −M⋆⟩+ C · smax(D(w) −S).
(2)"
TECHNICAL OVERVIEW,0.051639916259595256,"Here, the parameter C is used to trade off the progress objective ⟨D(w), M −M⋆⟩(giving Item 3)
and the flatness objective smax(D(w) −S) (giving Item 2), where smax(M) = log Tr exp(M) is
a smooth approximation to the operator norm. That is, letting Z := W × S be the product space
between w ∈RΩsatisfying Item 1 and short matrices S ∈Rd×d, we show that finding (w, S) ∈Z
with low suboptimality error in Fprog-flat is enough to make progress through Lemma 7."
TECHNICAL OVERVIEW,0.052337752965806,"We optimize (2) by carefully applying a Frank-Wolfe method due to [35]. While naïve Frank-Wolfe
analysis requires global smoothness of the objective, this provably does not hold for our Fprog-flat (at
least, with a smoothness bound of ≈poly(r) required for nearly-linear time algorithms). However,
we leverage that the analysis in [35] shows that weaker regularity conditions, namely bounds on
the curvature constant, suffice for efficient convergence of an iterative method. Specifically, we
use the structure of our constraint set W to show that Fprog-flat is smooth in a restricted set of
directions within W which implies suitable bounds on the curvature constant (see, e.g., Lemma 18)
to remove a d factor from the naïve smoothness bound. We also observe that W is exactly a
rescaling of a bipartite matching polytope, so that we can use flow-based graph algorithms, e.g., the
approximate weighted matching algorithm of [22], to efficiently implement the linear optimization
oracles over W required by Frank-Wolfe methods. We then use techniques from numerical linear
algebra and polynomial approximation to implement linear optimization oracles over S, completing
our optimization subroutine for finding adaptive reweightings."
TECHNICAL OVERVIEW,0.05303558967201675,"One other technical hurdle arises in implementing this framework for finding a reweighting: the
existence of w⋆satisfying Items 1, 2, and 3 relies on having a good estimate of the current distance
∥M −M⋆∥F, after excluding certain rows and columns. In general, we can only upper bound this
quantity, but we show that we can also use our optimization subroutine for (2) to certify non-existence
of such a w⋆. In this latter case, we show that it must have been the case that a few rows and columns
were significantly heavier than the rest, and we can run Algorithm 1 to drop them. Our overall
iterative method, Algorithm 2, makes bicriteria progress and either terminates with a “drop” step
(when no feasible w⋆exists), or a “descent” step (when we find a good solution to (2))."
TECHNICAL OVERVIEW,0.05373342637822749,"Recovering dropped rows and columns.
Finally, we briefly discuss how to post-process an
estimate M that is close to M⋆on a large submatrix to recover dropped rows and columns. The
high-level outline is similar to [43], where we aim to find a “verified” set of rows and columns that"
TECHNICAL OVERVIEW,0.05443126308443824,"we completed well, and use the low-rank assumption to obtain accurate estimates to the remainder via
regression. However, there are several key differences. The main difference is that in the semi-random
model, we need to track errors in our estimate entrywise instead of in ℓ2, since a semi-randomness
could drastically distort the empirical ℓ2 error by adversarially revealing entries. We rely on a
structural result using tools from the theory on volumetric spanners [5, 7] (see Lemma 27) showing
that large entrywise errors must be localized to a small submatrix. At this point, we set up ℓ∞
regression problems to identify verified indices, solvable in nearly-linear time using recent advances
in linear programming [69]. Importantly, the overhead in the error guarantee of this recovery step is
only eO(poly(r)) (see Proposition 3), allowing us to achieve a recovery guarantee in Theorem 1 that
avoids poly(d) overheads in the presence of noise."
PRELIMINARIES,0.05512909979064899,"3
Preliminaries"
PRELIMINARIES,0.055826936496859735,"We now formally set up preliminaries required for analyzing our solution to the semi-random matrix
completion problem. First, we recall the standard definition of incoherence for subspaces."
PRELIMINARIES,0.056524773203070484,"Definition 1 (Incoherence). We say a dimension-r subspace V ⊆Rd is µ-incoherent if ∥ΠV ei∥2 ≤
p µr"
PRELIMINARIES,0.05722260990928123,"d for all i ∈[d] (where ei is the ith standard basis vector), and M ∈Rm×n is µ-incoherent if M
has µ-incoherent row and column spans."
PRELIMINARIES,0.05792044661549198,"We will work with symmetric matrices. We denote the space of d × d symmetric matrices by Sd×d.
In Appendix A, we give a reduction from general matrices to symmetric matrices showing that it
suffices to work in the symmetric case, up to constant factor loss in parameters. Next, we define the
semi-random observation model."
PRELIMINARIES,0.05861828332170272,"Definition 2 (Semi-random model). For M ∈Sd×d, we use Osr
p (M) to denote oracle access to M
in the p-semi-random model, where we call Osr
p a p-semi-random oracle. Specifically, in one call to
Osr
p with input M, for unknown observation probabilities {pι}ι∈[d]×[d] ⊆[p, 1], each ι ∈[d] × [d] is
independently included in a set I with probability pι. The oracle Osr
p then returns the set {[M]ι}ι∈I.
When an algorithm requires the ability to query Osr
p (M) for various values of p (specified in the
algorithm description), we list the input as Osr
[0,1](M)."
PRELIMINARIES,0.05931612002791347,"Note that Osr
p (M) and Osr
[0,1](M) inherit their definitions in [43] when all pι = p, for a fixed p ∈[0, 1].
Now we can formally define the semi-random matrix completion problem we study."
PRELIMINARIES,0.06001395673412421,"Definition 3 (Semi-random matrix completion). In the semi-random matrix completion problem,
parameterized by µ, ∆∈R≥0, p ∈[0, 1], d ∈N, and r⋆∈[d], there is an unknown rank-r⋆"
PRELIMINARIES,0.06071179344033496,"µ-incoherent matrix M⋆∈Sd×d. For some N ∈Sd×d satisfying ∥N∥∞≤∆, and M := M⋆+ N,
we receive the output of one call to Osr
p (M), and wish to output an approximation to M⋆."
PRELIMINARIES,0.061409630146545706,"Remark 1. In Appendix A, we give a sample-splitting reduction which lets us use Osr
p (M) to simulate
≈p"
PRELIMINARIES,0.062107466852756456,"q independent accesses to Osr
q (M) for smaller values q ≤p."
PRELIMINARIES,0.0628053035589672,We will also need a few technical definitions to explain the main ingredients in our algorithm.
PRELIMINARIES,0.06350314026517795,"Entropy and softmax.
We let ∆d×d := {M ∈Sd×d
⪰0 | Tr(M) = 1} denote the d × d spectraplex.
We let H : ∆d×d →R denote von Neumann (matrix) entropy, i.e., H(M) := ⟨M, log M⟩, and let
H∗: Sd×d →R denote its convex conjugate, which we call matrix softmax, defined by H∗(M) :=
log Tr exp M. More generally, for η > 0, we define"
PRELIMINARIES,0.0642009769713887,"H∗
η(M) := η log Tr exp
1"
PRELIMINARIES,0.06489881367759945,"η M

.
(3)"
PRELIMINARIES,0.06559665038381018,"We recall the following standard facts about H∗
η."
PRELIMINARIES,0.06629448709002093,"Fact 1 ([58]). The function H∗
η defined in (3) satisfies the following for all η > 0."
PRELIMINARIES,0.06699232379623168,"1. For all M ∈Sd×d, λ1(M) ≤H∗
η(M) ≤λ1(M) + η log d, where λ1(M) is the maximum
eigenvalue of M."
PRELIMINARIES,0.06769016050244243,"2. For all M ∈Sd×d, ∇H∗
η(M) =
exp( 1"
PRELIMINARIES,0.06838799720865317,"η M)
Tr exp( 1 η M)."
PRELIMINARIES,0.06908583391486392,"3. H∗
η is twice-differentiable and 1"
PRELIMINARIES,0.06978367062107467,η-smooth with respect to the norm ∥·∥op.
PRELIMINARIES,0.07048150732728542,"To use H∗
η to enforce operator norm bounds, we define the signed lift of a matrix M ∈Sd×d by"
PRELIMINARIES,0.07117934403349616,"slift(M) :=

M
0d×d
0d×d
−M"
PRELIMINARIES,0.0718771807397069,"
∈S2d×2d."
PRELIMINARIES,0.07257501744591766,Note that slift signs and lifts M so that its maximum eigenvalue is its operator norm.
PRELIMINARIES,0.0732728541521284,"Comparing matrices.
We use the following comparison definition from [43]."
PRELIMINARIES,0.07397069085833916,"Definition 4 (Closeness on a submatrix). We say M, M′ ∈Rm×n are ∆-close on a γ-submatrix if
there exist subsets A ⊆[m], B ⊆[n] satisfying |A| ≥m −γ min(m, n), |B| ≥n −γ min(m, n),
and
[M −M′]A×B

F ≤∆."
PRELIMINARIES,0.07466852756454989,"If γ = 0, we say M, M′ are ∆-close. When M, M′ are both symmetric, we require that A = B."
PRELIMINARIES,0.07536636427076064,"We note that in Definition 4, A, B are unknown; our analysis only uses this definition existentially."
PRELIMINARIES,0.07606420097697139,"4
Outline of proof of Theorem 1"
PRELIMINARIES,0.07676203768318214,"In this section, we overview the main components of the proof of Theorem 1, which are developed
and proved in full in the appendix sections. Our final matrix completion algorithm, Algorithm 8,
alternates between two main subroutines, Algorithm 3 (analyzed in the following Corollary 1) and
Algorithm 7 (analyzed in the following Proposition 3). We prove Theorem 2, a formal version of
Theorem 1, by combining Corollary 1 and Proposition 3 in Appendix E. We now explain and motivate
each of these pieces in the context of our technical overview in Section 2."
PRELIMINARIES,0.07745987438939288,"The first main component of our final algorithm, Algorithm 8, takes as input parameters α and γtot, as
well as an input matrix M ∈Sd×d satisfying ∥M −M⋆∥F ≤∆(i.e., M and the target matrix M⋆
are ∆-close), where ∆is sufficiently larger than the noise level ∥N∥∞. Its goal is to use semi-random
observations from c
M = M⋆+ N to return another matrix M′ ∈Sd×d such that c
M and M are
∆"
PRELIMINARIES,0.07815771109560363,"α -close on a γtot-submatrix (Definition 4). In other words, we are willing to give up on a γtot fraction
of rows and columns to make an α factor progress on the remaining submatrix. To achieve this result,
we first provide a helper claim in Proposition 1, an analysis of Algorithm 2 (DropOrDescent)."
PRELIMINARIES,0.07885554780181438,"Proposition 1. Let M ∈Sd×d be given as a rank-r factorization, let M⋆∈Sd×d be rank-r⋆,
and let U ⊆[d]. Suppose we know for γdrop, γsub ∈[0, 1], ∆> 0, that |[d] \ U| ≤γdropd, and
MU×U, M⋆
U×U are ∆-close on a γsub-submatrix. Let δ, ϵ ∈(0, 1"
PRELIMINARIES,0.07955338450802513,"10), θ, κ ≥1, and suppose for
ˆr := r + r⋆and an appropriate polynomial, p ∈ ""
κˆr"
PRELIMINARIES,0.08025122121423586,d · poly
PRELIMINARIES,0.08094905792044661,log( d
PRELIMINARIES,0.08164689462665736,"δ )
γsubϵ ! , 1 #"
PRELIMINARIES,0.08234473133286811,", γsub ≤
ϵ
36θ.
(4)"
PRELIMINARIES,0.08304256803907886,"Given one call to Osr
p (M⋆+ N) for ∥N∥∞≤ϵγsub∆"
PRELIMINARIES,0.0837404047452896,"20d , the following holds with probability ≥1 −δ."
PRELIMINARIES,0.08443824145150035,"1. If Algorithm 2 returns on Line 12, then MU ′×U ′, M⋆
U ′×U ′ are (1 −
ϵ
288)∆-close on a
γsub +
4
√"
PRELIMINARIES,0.0851360781577111,"θκ-submatrix, and |U \ U ′| ≤40(γsub + ( ˆr"
PRELIMINARIES,0.08583391486392185,θ)1/2)d log d.
PRELIMINARIES,0.08653175157013258,"2. If Algorithm 2 returns on Line 16, then M′
U×U, M⋆
U×U are 10ϵ
1
4 ∆-close on a 2γsub-
submatrix, and M′ is given as a rank-(3r + 2r⋆) factorization."
PRELIMINARIES,0.08722958827634333,The runtime of the algorithm is O 
PRELIMINARIES,0.08792742498255408,m · poly
PRELIMINARIES,0.08862526168876483,ˆr log( d
PRELIMINARIES,0.08932309839497557,"δ )
γsubϵ !! ,"
PRELIMINARIES,0.09002093510118632,"where m ≥d is the number of observed entries upon calling Osr
p (M⋆+ N)."
PRELIMINARIES,0.09071877180739707,"Proposition 1 is used to analyze one step of Algorithm 2, after we have already taken a few iterations
and explicitly removed γdropd rows and columns [d] \ U from consideration, so that our remaining
submatrices on U ×U are close on a γsub-submatrix. Our goal will ultimately to have γsub, γdrop ≤γtot"
PRELIMINARIES,0.09141660851360782,"2
throughout the algorithm. Algorithm 2 provides bicriteria guarantees: in the “drop” case of Item 1, it
slightly increases the γdrop parameter, increases γsub by an even smaller amount (where the tradeoff is"
PRELIMINARIES,0.09211444521981857,"given by a degree of freedom κ), and makes a small amount of progress (decreasing the closeness by
1 −O(ϵ)). In the “descent” case of Item 2, we instead decrease the closeness by a large O(poly(ϵ))
factor, while doubling γsub and roughly tripling the rank. By choosing the parameters appropriately,
we show that iterating upon Proposition 1 yields Corollary 1, proven in Appendix B."
PRELIMINARIES,0.0928122819260293,"Corollary 1. Let M, M⋆∈Sd×d be rank-r⋆, and suppose ∥M −M⋆∥F ≤∆. Let α ≥250 and
δ, γtot ∈(0, 1"
PRELIMINARIES,0.09351011863224006,"10). Algorithm 3 uses one call to Osr
p (M⋆+ N), where for appropriate polynomials,"
PRELIMINARIES,0.0942079553384508,"∥N∥∞≤
∆
d · poly( r⋆"
PRELIMINARIES,0.09490579204466155,γtot log( d
PRELIMINARIES,0.09560362875087229,"δ )) · α1+o(1) , p =
poly( r⋆"
PRELIMINARIES,0.09630146545708304,γtot log( d
PRELIMINARIES,0.09699930216329379,"δ )) · αo(1) d
,"
PRELIMINARIES,0.09769713886950454,"and computes a rank-r⋆· αo(1)-matrix M′, such that with probability ≥1 −δ, M′ and M⋆are
∆"
PRELIMINARIES,0.09839497557571528,α -close on a γtot-submatrix. The runtime of the algorithm is mαo(1) · poly( r⋆
PRELIMINARIES,0.09909281228192603,γtot log( d
PRELIMINARIES,0.09979064898813678,"δ )), where m is
the number of observed entries upon calling Osr
p (M⋆+ N)."
PRELIMINARIES,0.10048848569434753,"We now explain the key computational method we develop to implement Proposition 1. We will solve
a subproblem, defined in (5), to attempt to find a reweighting which makes progress. The subproblem
enforces the conditions on candidate weights w mentioned in Section 2, and is formally stated below:"
PRELIMINARIES,0.10118632240055828,"Fprog-flat(x, S) := −⟨1I, x⟩+ CH∗
η

slift

D
x v"
PRELIMINARIES,0.10188415910676901,"
−S

,"
PRELIMINARIES,0.10258199581297976,"where D(w) :=
X"
PRELIMINARIES,0.10327983251919051,"ι∈I
wιDι, vι = [D]2
ι for all ι ∈I, and x"
PRELIMINARIES,0.10397766922540126,"v denotes entrywise division.
(5)"
PRELIMINARIES,0.104675505931612,"The difference between (5) and our informal sketch in (2) is that we reparameterize x ←w ◦v where
v is the squared observations of the difference matrix D, and I is the set of observations. We also
introduce a tradeoff parameter η for controlling additive error bounds via Fact 1. Thus, the component
⟨1I, x⟩= ⟨v, w⟩captures the progress condition (Item 3), and the component H∗
η(slift(D( x"
PRELIMINARIES,0.10537334263782275,"v ) −S))
captures the flatness condition (Item 2). We optimize (5) over ∥S∥F ≤1 + ϵ, the set of short matrices,
and x belonging to X defined in (22) which we observe is a rescaling of a bipartite matching polytope
by a parameter k (Fact 3). This polytope is used to enforce the spreadness condition (Item 1)."
PRELIMINARIES,0.1060711793440335,"In the case we find a good solution to the subproblem, we can use the resulting solution to take a
descent step and give the guarantee in Item 2 of Proposition 3. Otherwise, we have certified that
excluding a few heavy rows and columns removed much of the progress that could be made (which
we prove is the only way a good solution does not exist), and hence obtain the guarantee in Item 1.
Our main guarantee for solving (5) at a fast rate is the following, proven in Appendix C."
PRELIMINARIES,0.10676901605024425,"Proposition 2. Let ϵ ∈[0, C] and δ ∈(0, 1), and suppose |[D]ι| ∈[ℓ, u] for all ι ∈I. There is an
algorithm computing (x, S), an ϵ-approximate minimizer to (5) with probability ≥1 −δ, in time"
PRELIMINARIES,0.10746685275645498,"O

(|I| + d) T 2 · poly
CkRη"
PRELIMINARIES,0.10816468946266573,"ϵ
log
d δ "
PRELIMINARIES,0.10886252616887648,for Rη := η−1(2k + ku
PRELIMINARIES,0.10956036287508723,"ℓ2d) and T = ⌈
8CηR2
η
ϵ
⌉, and S is given as a rank-r = O( C2"
PRELIMINARIES,0.11025819958129798,ϵ2 · T) factorization.
PRELIMINARIES,0.11095603628750872,"The last component in the proof of Theorem 1 is Algorithm 7, a “rounding"" step where we take
the output of Corollary 1, say M, which is guaranteed to be close to M⋆on a large submatrix,
and then postprocess it to a matrix f
M that is guaranteed to be close to M⋆everywhere with some
poly(r, µ, log d)-factor loss in the closeness. As long as the progress α made by Corollary 1 is larger
than this factor, we can simply alternate Corollary 1 and the rounding step to still make constant-factor
progress in each iteration. We prove the following result in Appendix D."
PRELIMINARIES,0.11165387299371947,"Proposition 3. Let M⋆∈Sd×d be rank-r⋆and µ-incoherent, and let c
M = M⋆+ N for N ∈Sd×d
with ∥N∥∞≤∆for some ∆≥0. Let M ∈Sd×d be given as a rank-r decomposition, with r ≥r⋆.
Further, for γ ∈(0, 1) suppose M and M⋆are d∆-close on a γ-submatrix. Finally, assume"
PRELIMINARIES,0.11235170969993022,"γ ≤
1
104µr log(d)."
PRELIMINARIES,0.11304954640614097,"Then for any δ ∈(0, 1) if p ≥1"
PRELIMINARIES,0.1137473831123517,d · poly(µr log( d
PRELIMINARIES,0.11444521981856245,"δ )) for an appropriate polynomial, Algorithm 7 uses
one call to Op(c
M) and with probability ≥1 −δ, outputs f
M ∈Sd×d such that
f
M −M⋆
∞≤poly

µr log
d δ"
PRELIMINARIES,0.1151430565247732,"
∆.
(6)"
PRELIMINARIES,0.11584089323098395,"Also, f
M is given as a rank-poly(µr log( d"
PRELIMINARIES,0.11653872993719469,δ )) factorization. The algorithm runs in m · poly(µr log( d
PRELIMINARIES,0.11723656664340544,"δ ))
time, where m is the number of observed entries upon calling Osr
p (c
M)."
PRELIMINARIES,0.11793440334961619,Acknowledgments and Disclosure of Funding
PRELIMINARIES,0.11863224005582694,"AL was supported in part by an NSF GRFP and a Hertz Fellowship. AS was supported in part by
a Microsoft Research Faculty Fellowship, NSF CAREER Grant CCF-1844855, NSF Grant CCF-
1955039, and a PayPal research award. JK was supported in part by NSF awards CCF-1955217,
CCF-1565235, and DMS-2022448. Part of this work was conducted while visiting the Simons
Institute for the Theory of Computing."
REFERENCES,0.11933007676203769,References
REFERENCES,0.12002791346824843,"[1] Prizes for matrix completion problems.
https://www.alignmentforum.org/posts/
pJrebDRBj9gfBE8qE/prizes-for-matrix-completion-problems, 2023."
REFERENCES,0.12072575017445918,"[2] Amol Aggarwal and Josh Alman. Optimal-degree polynomial approximations for exponentials
and gaussian kernel density estimation. In 37th Computational Complexity Conference, CCC
2022, volume 234 of LIPIcs, pages 22:1–22:23. Schloss Dagstuhl - Leibniz-Zentrum für
Informatik, 2022."
REFERENCES,0.12142358688066993,"[3] Zeyuan Allen-Zhu and Lorenzo Orecchia. Nearly linear-time packing and covering LP solvers.
Math. Program., 175(1-2):307–353, 2019."
REFERENCES,0.12212142358688068,"[4] Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi.
Matrix completion methods for causal panel data models. Journal of the American Statistical
Association, 116(536):1716–1730, 2021."
REFERENCES,0.12281926029309141,"[5] Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing.
Journal of Computer and System Sciences, 74(1):97–114, 2008."
REFERENCES,0.12351709699930216,[6] James Bennett and Stan Lanning. The netflix prize. 2007.
REFERENCES,0.12421493370551291,"[7] Aditya Bhaskara, Sepideh Mahabadi, and Ali Vakilian. Tight bounds for volumetric spanners
and applications. CoRR, abs/2310.00175, 2023."
REFERENCES,0.12491277041172366,"[8] Avrim Blum, Meghal Gupta, Gene Li, Naren Sarayu Manoj, Aadirupa Saha, and Yuanyuan Yang.
Dueling optimization with a monotone adversary. In International Conference on Algorithmic
Learning Theory, volume 237 of Proceedings of Machine Learning Research, pages 221–243.
PMLR, 2024."
REFERENCES,0.1256106071179344,"[9] Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. Journal
of Algorithms, 19(2):204–234, 1995."
REFERENCES,0.12630844382414516,"[10] Rares-Darius Buhai, Pravesh K Kothari, and David Steurer. Algorithms approaching the
threshold for semi-random planted clique. In Proceedings of the 55th Annual ACM Symposium
on Theory of Computing, pages 1918–1926, 2023."
REFERENCES,0.1270062805303559,"[11] Emmanuel J. Candès and Yaniv Plan. Matrix completion with noise. Proc. IEEE, 98(6):925–936,
2010."
REFERENCES,0.12770411723656663,"[12] Emmanuel J. Candès and Benjamin Recht. Exact matrix completion via convex optimization.
Commun. ACM, 55(6):111–119, 2012."
REFERENCES,0.1284019539427774,"[13] Emmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix
completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010."
REFERENCES,0.12909979064898813,"[14] Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs
via convex optimization. The Journal of Machine Learning Research, 15(1):2213–2238, 2014."
REFERENCES,0.1297976273551989,"[15] Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering sparse graphs. arXiv preprint
arXiv:1210.3335, 2(5), 2012."
REFERENCES,0.13049546406140963,"[16] Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, and Yuling Yan. Noisy matrix completion:
Understanding statistical guarantees for convex relaxation via nonconvex optimization. SIAM
journal on optimization, 30(4):3098–3121, 2020."
REFERENCES,0.13119330076762037,"[17] Yu Cheng and Rong Ge. Non-convex matrix completion against a semi-random adversary. In
Conference On Learning Theory, pages 1362–1394. PMLR, 2018."
REFERENCES,0.13189113747383113,"[18] Yeshwanth Cherapanamjeri, Kartik Gupta, and Prateek Jain. Nearly optimal robust matrix
completion. In International Conference on Machine Learning, pages 797–805. PMLR, 2017."
REFERENCES,0.13258897418004187,"[19] Michael B. Cohen, Jonathan A. Kelner, John Peebles, Richard Peng, Anup B. Rao, Aaron
Sidford, and Adrian Vladu. Almost-linear-time algorithms for markov chains and new spectral
primitives for directed graphs. In Proceedings of the 49th Annual ACM SIGACT Symposium on
Theory of Computing, STOC 2017, pages 410–419. ACM, 2017."
REFERENCES,0.1332868108862526,"[20] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and
lindenstrauss. Random Struct. Algorithms, 22(1):60–65, 2003."
REFERENCES,0.13398464759246337,"[21] Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic
gradient descent for some non-convex matrix problems. In International conference on machine
learning, pages 2332–2341. PMLR, 2015."
REFERENCES,0.1346824842986741,"[22] Ran Duan and Seth Pettie. Linear-time approximation for maximum weight matching. J. ACM,
61(1):1:1–1:23, 2014."
REFERENCES,0.13538032100488487,"[23] Micha Elsner and Warren Schudy. Bounding and comparing methods for correlation clustering
beyond ilp. In Proceedings of the Workshop on Integer Linear Programming for Natural
Language Processing, pages 19–27, 2009."
REFERENCES,0.1360781577110956,"[24] Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. Journal of Computer
and System Sciences, 63(4):639–671, 2001."
REFERENCES,0.13677599441730634,"[25] Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in a semiran-
dom graph. Random Structures & Algorithms, 16(2):195–208, 2000."
REFERENCES,0.1374738311235171,"[26] David Gamarnik and Sidhant Misra. A note on alternating minimization algorithm for the
matrix completion problem. IEEE Signal Processing Letters, 23(10):1340–1343, 2016."
REFERENCES,0.13817166782972784,"[27] Xing Gao and Yu Cheng. Robust matrix sensing in the semi-random model. In Proceedings of
the 37th Conference on Neural Information Processing Systems (NeurIPS), 2023."
REFERENCES,0.1388695045359386,"[28] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum.
Advances in neural information processing systems, 29, 2016."
REFERENCES,0.13956734124214934,"[29] Amir Globerson, Tim Roughgarden, David Sontag, and Cafer Yildirim. Tight error bounds for
structured prediction. arXiv preprint arXiv:1409.5834, 2014."
REFERENCES,0.14026517794836008,"[30] Yuzhou Gu, Zhao Song, Junze Yin, and Lichen Zhang. Low rank matrix completion via robust
alternating minimization in nearly linear time. In The Twelfth International Conference on
Learning Representations, ICLR 2024. OpenReview.net, 2024."
REFERENCES,0.14096301465457084,"[31] Suriya Gunasekar, Ayan Acharya, Neeraj Gaur, and Joydeep Ghosh. Noisy matrix completion
using alternating minimization. In Machine Learning and Knowledge Discovery in Databases -
European Conference, ECML PKDD 2013, volume 8189 of Lecture Notes in Computer Science,
pages 194–209. Springer, 2013."
REFERENCES,0.14166085136078158,"[32] Moritz Hardt. Understanding alternating minimization for matrix completion. In 2014 IEEE
55th Annual Symposium on Foundations of Computer Science, pages 651–660. IEEE, 2014."
REFERENCES,0.1423586880669923,"[33] Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. In
Proceedings of The 27th Conference on Learning Theory, COLT 2014, volume 35 of JMLR
Workshop and Conference Proceedings, pages 638–678. JMLR.org, 2014."
REFERENCES,0.14305652477320308,"[34] Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Solving SDP faster:
A robust IPM framework and efficient implementation. In 63rd IEEE Annual Symposium on
Foundations of Computer Science, FOCS 2022, pages 233–244. IEEE, 2022."
REFERENCES,0.1437543614794138,"[35] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In Proceed-
ings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA,
16-21 June 2013, volume 28 of JMLR Workshop and Conference Proceedings, pages 427–435.
JMLR.org, 2013."
REFERENCES,0.14445219818562458,"[36] Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with finite samples. In
Conference on Learning Theory, pages 1007–1034. PMLR, 2015."
REFERENCES,0.1451500348918353,"[37] Arun Jambulapati, Jerry Li, Christopher Musco, Kirankumar Shiragur, Aaron Sidford, and
Kevin Tian. Structured semidefinite programming for recovering structured preconditioners.
In Advances in Neural Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, 2023."
REFERENCES,0.14584787159804605,"[38] Arun Jambulapati and Kevin Tian. Revisiting area convexity: Faster box-simplex games and
spectrahedral generalizations. CoRR, abs/2303.15627, 2023."
REFERENCES,0.1465457083042568,"[39] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster
interior point method for semidefinite programming. In 61st IEEE Annual Symposium on
Foundations of Computer Science, FOCS 2020, pages 910–918, 2020."
REFERENCES,0.14724354501046755,"[40] Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Provable efficient online matrix completion
via non-convex stochastic gradient descent. Advances in Neural Information Processing Systems,
29, 2016."
REFERENCES,0.1479413817166783,"[41] Charles R Johnson. Matrix completion problems: a survey. In Matrix theory and applications,
volume 40, pages 171–198, 1990."
REFERENCES,0.14863921842288905,"[42] Jonathan Kelner, Jerry Li, Allen X Liu, Aaron Sidford, and Kevin Tian. Semi-random sparse
recovery in nearly-linear time. In The Thirty Sixth Annual Conference on Learning Theory,
pages 2352–2398. PMLR, 2023."
REFERENCES,0.14933705512909978,"[43] Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, and Kevin Tian. Matrix completion in
almost-verification time, 2023."
REFERENCES,0.15003489183531055,"[44] Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy
entries. In Advances in Neural Information Processing Systems 22: 23rd Annual Conference on
Neural Information Processing Systems 2009, pages 952–960. Curran Associates, Inc., 2009."
REFERENCES,0.15073272854152128,"[45] Alexandra Kolla, Konstantin Makarychev, and Yury Makarychev. How to play unique games
against a semi-random adversary: Study of semi-random models of unique games. In 2011
IEEE 52nd Annual Symposium on Foundations of Computer Science, pages 443–452. IEEE,
2011."
REFERENCES,0.15143056524773202,"[46] Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs and some of its
algorithmic applications. Combinatorica, 15:215–245, 1995."
REFERENCES,0.15212840195394278,"[47] Gunjan Mahindre, Anura P. Jayasumana, Kelum Gajamannage, and Randy Paffenroth. On
sampling and recovery of topology of directed social networks – a low-rank matrix completion
based approach. In 2019 IEEE 44th Conference on Local Computer Networks (LCN), pages
324–331, 2019."
REFERENCES,0.15282623866015352,"[48] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approximation
algorithms for semi-random partitioning problems. In Proceedings of the forty-fourth annual
ACM symposium on Theory of computing, pages 367–384, 2012."
REFERENCES,0.15352407536636428,"[49] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Sorting noisy data
with partial information. In Proceedings of the 4th conference on Innovations in Theoretical
Computer Science, pages 515–528, 2013."
REFERENCES,0.15422191207257502,"[50] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Constant factor
approximation for balanced cut in the pie model. In Proceedings of the forty-sixth annual ACM
symposium on Theory of computing, pages 41–49, 2014."
REFERENCES,0.15491974877878575,"[51] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Correlation clus-
tering with noisy partial information. In Conference on Learning Theory, pages 1321–1342.
PMLR, 2015."
REFERENCES,0.15561758548499652,"[52] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Learning commu-
nities in the presence of errors. In Conference on learning theory, pages 1258–1291. PMLR,
2016."
REFERENCES,0.15631542219120725,"[53] Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proceedings of
the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pages 712–728. SIAM,
2010."
REFERENCES,0.15701325889741802,"[54] Theo McKenzie, Hermish Mehta, and Luca Trevisan. A new algorithm for the robust semi-
random independent set problem. In Proceedings of the 2020 ACM-SIAM Symposium on
Discrete Algorithms, SODA 2020, pages 738–746. SIAM, 2020."
REFERENCES,0.15771109560362875,"[55] Ankur Moitra, William Perry, and Alexander S Wein. How robust are reconstruction thresholds
for community detection? In Proceedings of the forty-eighth annual ACM symposium on Theory
of Computing, pages 828–841, 2016."
REFERENCES,0.1584089323098395,"[56] Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger
and faster approximate singular value decomposition. In Advances in Neural Information
Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015,
pages 1396–1404, 2015."
REFERENCES,0.15910676901605025,"[57] Nagarajan Natarajan and Inderjit S. Dhillon.
Inductive matrix completion for predicting
gene–disease associations. Bioinformatics, 30(12):60–68, 2014."
REFERENCES,0.159804605722261,"[58] Yurii Nesterov. Smoothing technique and its applications in semidefinite optimization. Mathe-
matical Programming, Series A, 110:245–259, 2007."
REFERENCES,0.16050244242847173,"[59] Luong Trung Nguyen, Junhan Kim, and Byonghyo Shim. Low-rank matrix completion: A
contemporary survey. IEEE Access, 7:94215–94237, 2019."
REFERENCES,0.1612002791346825,"[60] Andy Ramlatchan, Mengyun Yang, Quan Liu, Min Li, Jianxin Wang, and Yaohang Li. A survey
of matrix completion methods for recommendation systems. Big Data Mining and Analytics,
1(4):308–323, 2018."
REFERENCES,0.16189811584089323,"[61] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning
Research, 12(12), 2011."
REFERENCES,0.162595952547104,"[62] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collabo-
rative prediction. In Proceedings of the 22nd international conference on Machine learning,
pages 713–719, 2005."
REFERENCES,0.16329378925331473,"[63] Tim Roughgarden. Beyond the Worst-Case Analysis of Algorithms. Cambridge University Press,
2021."
REFERENCES,0.16399162595952546,"[64] Anthony Man-Cho So and Yinyu Ye. Theory of semidefinite programming for sensor network
localization. Mathematical Programming, Series B, (109):367—-384, 2007."
REFERENCES,0.16468946266573622,"[65] Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint
arXiv:1510.06096, 2015."
REFERENCES,0.16538729937194696,"[66] Ruoyu Sun. Matrix completion via nonconvex factorization: Algorithms and theory. PhD thesis,
University of Minnesota, 2015."
REFERENCES,0.16608513607815772,"[67] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.
IEEE Transactions on Information Theory, 62(11):6535–6579, 2016."
REFERENCES,0.16678297278436846,"[68] Wei Tu, Peng Liu, Jingyu Zhao, Yi Liu, Linglong Kong, Guodong Li, Bei Jiang, Guangjian
Tian, and Hengshuai Yao. M-estimation in low-rank matrix factorization: a general framework.
In 2019 IEEE international conference on data mining (ICDM), pages 568–577. IEEE, 2019."
REFERENCES,0.1674808094905792,"[69] Jan van den Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense linear
programs in nearly linear time, 2021."
REFERENCES,0.16817864619678996,"[70] Bo Xin, Yizhou Wang, Wen Gao, and David Wipf. Exploring algorithmic limits of matrix rank
minimization under affine constraints. IEEE Transactions on Signal Processing, 64(19):4960–
4974, 2016."
REFERENCES,0.1688764829030007,"[71] Jialun Zhang, Hong-Ming Chiu, and Richard Y Zhang.
Accelerating sgd for highly ill-
conditioned huge-scale online matrix completion. Advances in Neural Information Processing
Systems, 35:37549–37562, 2022."
REFERENCES,0.16957431960921143,"[72] Shuai Zhang and Meng Wang. Correction of corrupted columns through fast robust hankel
matrix completion. IEEE Transactions on Signal Processing, 67(10):2580–2594, 2019."
REFERENCES,0.1702721563154222,"[73] Xiao Zhang, Simon Du, and Quanquan Gu. Fast and sample efficient inductive matrix comple-
tion via multi-phase procrustes flow. In International Conference on Machine Learning, pages
5756–5765. PMLR, 2018."
REFERENCES,0.17096999302163293,"[74] Tuo Zhao, Zhaoran Wang, and Han Liu. A nonconvex optimization framework for low rank
matrix estimation. Advances in Neural Information Processing Systems, 28, 2015."
REFERENCES,0.1716678297278437,"A
Additional preliminaries"
REFERENCES,0.17236566643405443,"In this section, we formalize some additional preliminaries and helpful notation which will be used in
the remainder of the appendices."
REFERENCES,0.17306350314026517,"General notation.
For n ∈N we denote [n] := {i ∈N | i ≤n}. We say a vector is s-sparse if it
has ≤s nonzero entries. When S ⊆T and T is clear from context, we let Sc := T \S. The all-zeroes
and all-ones vectors of dimension d are denoted 0d and 1d. Applied to a vector, ∥·∥p denotes the ℓp
norm for p ≥1. Applied to a matrix, ∥·∥p denotes the ℓp norm of the vector corresponding to the
flattened matrix. We use med(S) to denote the median of a set S. For t ∈R we let sign(t) = 1 if
t ≥0 and sign(t) = −1 otherwise. We let ◦denote the entrywise product of two vectors."
REFERENCES,0.17376133984647593,"Matrices.
Matrices are denoted in boldface. The number of nonzero entries of a matrix M is
denoted nnz(M), and the set of indices corresponding to these nonzero entries, i.e., the support,
is denoted supp(M). We define Tmv(M) as the amount of time it takes to compute Mv for any
vector v; note Tmv(M) = O(nnz(M)) if M has explicit entries, and if M ∈Rm×n is given as
a rank-r factorization then Tmv(M) = O((m + n)r). We equip Rm×n with the inner product
⟨A, B⟩:= Tr(A⊤B), denote the d × d identity by Id, and the all-zero m × n matrix by 0m×n.
The ith row and jth column of a matrix M are respectively denoted Mi: and M:j. For a collection
of matrices {Ai}i∈[k] ∈Rm×n with an associated operator A (clear from context), we denote
A(x) := P"
REFERENCES,0.17445917655268667,"i∈[k] xiAi for x ∈Rk. We say M is s-row column sparse (RCS) if its rows and columns
are s-sparse. For S ⊆[m], T ⊆[n], we let the submatrix of M ∈Rm×n indexed by S, T be denoted
MS×T . When S is a singleton {i} for i ∈[m], we denote this Mi,T and similarly define MS,j for
j ∈[n]."
REFERENCES,0.17515701325889743,"The Frobenius, operator, and trace (nuclear) norms of a matrix are denoted ∥·∥F, ∥·∥op, and ∥·∥tr
respectively, corresponding to the 2-norm, ∞-norm, and 1-norm of the singular values of the matrix.
We denote the symmetric d × d matrices by Sd×d and the positive semidefinite cone by Sd×d
⪰0 . We
say M ∈Rm×n has singular value decomposition (SVD) M = UΣV⊤if M is rank-r, Σ ∈Rr×r
is diagonal, and U ∈Rm×r, V ∈Rn×r have orthonormal columns. The ordered singular values
of M ∈Rm×n are denoted {σi(M)}i∈[min(m,n)] where σ1(M) is largest. Similarly we define
the ordered eigenvalues of M ∈Sd×d by {λi(M)}i∈[d]. For M ∈Sd×d with eigendecomposition
UΛU⊤, we define exp(M) := U exp(Λ)U⊤where on the right-hand side, exp is applied entrywise
on the diagonal. We also define log M for M ∈Sd×d
⪰0 and |M| for M ∈Sd×d so the functions are
applied to the eigenvalues in the appropriate basis. Finally, we let span(A) denote the column span
(or image) of a matrix A, and for a subspace V ⊂Rd, we let ΠV denote the orthogonal projection
matrix onto V ."
REFERENCES,0.17585484996510817,"Index sets.
For d ∈N, we let S(d) contain all symmetrized indices of the form:"
REFERENCES,0.1765526866713189,"{(i, i)} for i ∈[d], or {(i, j), (j, i)} for i, j ∈[d], i ̸= j."
REFERENCES,0.17725052337752967,"We use (i, j)♯as shorthand for the singleton {(i, i)} if i = j, and otherwise as shorthand for
{(i, j), (j, i)}. When I is a set of indices in [d]×[d], we let MI be the d×d matrix which zeroes out
all entries of M except those in ι. For a symmetrized index ι ∈S(d), we denote the scalar value of
the ι entry by [M]ι (note that this is well-defined even when ι contains two indices due to symmetry).
The zero-one matrix whose only nonzero entries are ι is denoted by Eι, so that Mι = [M]ιEι for all
ι ∈S(d). For I ⊆S(d) clear from context, we let E be the operator on w ∈RI which forms the
associated d × d matrix whose indices on I agree with w and is otherwise zero entrywise, i.e.,"
REFERENCES,0.1779483600837404,"E(w) :=
X"
REFERENCES,0.17864619678995114,"ι∈I
wιEι."
REFERENCES,0.1793440334961619,"Optimization.
For convex f : X →R with X ⊆Rd, we let ∂f(x) denote the subgradient set at
x ∈X. We let ∥y∥∗:= sup∥x∥≤1 x⊤y denote the dual norm to a norm ∥·∥. When M is a d×d linear
operator we use the notation M[u, v] := ⟨Mu, v⟩. This becomes relevant when M is a Hessian of a
matrix function (a linear operator on d × d matrices), so M[U, V] evaluates this operator on U, V."
REFERENCES,0.18004187020237264,"Reduction to symmetric matrices.
We briefly justify our focus on symmetric matrices in Defini-
tion 3. We begin by recalling two facts."
REFERENCES,0.1807397069085834,"Fact 2. Let m, n ∈N with m ≥n. There exists k ∈N such that kn ∈[ m"
REFERENCES,0.18143754361479414,"2 , m]."
REFERENCES,0.18213538032100487,"Lemma 1. Let m, n, k ∈N and let M ∈Rm×n be rank-r with SVD M = UΣV⊤. Suppose that
both U := span(U) and V := span(V) are µ-incoherent. Then, letting M′ ∈Rm×kn be defined as"
REFERENCES,0.18283321702721564,"M′ := (M
M
. . .
M
M)
|
{z
}
k times ,"
REFERENCES,0.18353105373342637,"and letting M′ have SVD M′ = UΣ′W⊤, then W := span(W) is µ-incoherent."
REFERENCES,0.18422889043963714,Proof. It is straightforward to check that a valid SVD is given by
REFERENCES,0.18492672714584787,"Σ′ =
√"
REFERENCES,0.1856245638520586,"kΣ, W⊤=
1
√ k"
REFERENCES,0.18632240055826937," 
V⊤
V⊤
. . .
V⊤
V⊤"
REFERENCES,0.1870202372644801,"|
{z
}
k times ."
REFERENCES,0.18771807397069085,"Because incoherence is invariant to the choice of SVD, the conclusion holds."
REFERENCES,0.1884159106769016,"Consider an asymmetric variant of the problem in Definition 3, where M⋆∈Rm×n for m ≥n is
rank-r⋆and µ-incoherent, and we have p-semi-random observation oracle access (defined analogously
to Definition 2) to M⋆. Let k be the integer from Fact 2, and let d
M⋆∈Rm×kn be constructed as
in Lemma 1 by concatenating M⋆k times. The row and column spans of d
M⋆are µ-incoherent, by
Lemma 1. Moreover, we can simulate p-semi-random observation oracle access to
"
REFERENCES,0.18911374738311235,"0m×m
d
M⋆
h
d
M⋆
i⊤
0kn×kn "
REFERENCES,0.1898115840893231,∈S(m+kn)×(m+kn)
REFERENCES,0.19050942079553385,"in the symmetric sense of Definition 3, given asymmetric oracle access to M⋆. It is straightforward
to check that because d
M⋆has dimensions within a constant factor of each other, the incoherence
parameter is only affected by a constant in this lifting process. For simplicity in the remainder of the
paper, we focus on the case of symmetric matrices, i.e., the problem statement in Definition 3."
REFERENCES,0.19120725750174458,"Sample splitting.
Finally, we recall a convenient subsampling claim from [43] that extends to
the semi-random model, which lets us use Osr
p (M) to simulate access to Osr
q (M) for smaller values
q ≤p."
REFERENCES,0.19190509420795535,"Lemma 2. Let {pk}k∈[K] ⊂(0, 1) satisfy pk ≤p ≤
1
K for all k ∈[K], and let M⋆∈Sd×d. We
can simulate sequential access to Osr
pk(M⋆) for all k ∈[K] with access to Osr
Kp(M⋆)."
REFERENCES,0.19260293091416608,"Proof. The claim is immediate from the observation that the proof of Lemma 1, [43] is monotone
in p. In particular, the same subsampling process in [43] preserves monotonicity of observation
probabilities and hence produces valid semi-random observation oracles."
REFERENCES,0.19330076762037685,"B
Semi-random partial matrix completion"
REFERENCES,0.19399860432658758,"In this section, we develop Algorithm 3, our main subroutine for partial matrix completion in the
semi-random model (Definition 2). Algorithm 3 uses recursive calls to Algorithm 2, DropOrDescent,
which implements a single step of our partial completion method."
REFERENCES,0.19469644103279832,"At a high level, one application of DropOrDescent is parameterized by a current number of dropped
rows γdrop, a current number of excluded rows γsub, a subset of undropped rows U = [d]\D satisfying
|D| ≤γdropd, and a rank-r matrix M such that MU×U is promised to be close to our target M⋆
U×U on
a γsub-submatrix (Definition 4). It then distinguishes between the following cases from semi-random,
noisy observations of M⋆, by solving a testing problem via optimization."
REFERENCES,0.19539427773900908,"1. In the first case, dropping an additional γ fraction of rows and columns decreases the
closeness parameter ∆by a small amount, i.e. a small subset of rows or columns are
responsible for a nontrivial portion of the mass. We call this case the “drop” case."
REFERENCES,0.19609211444521982,"2. In the second case, after removing a few rows and columns, the remaining mass is still
large. We show that in this case, by deleting a particular subset of problematic rows or
columns, we can use well-conditionedness properties of the remaining submatrix to decrease
∆by a constant factor. We do so by solving an optimization problem, which finds a spread
reweighting of observations with a short-flat decomposition. We call this case the “descent”
case."
REFERENCES,0.19678995115143055,"We analyze the descent case in Appendix B.1, where we demonstrate sufficient conditions on a
proposed reweighted step which guarantee progress under submatrix closeness. The candidate steps
we analyze in this section are all reweightings of truncated observations, which are found by solving
an appropriate optimization problem (see (20)). We then analyze the drop case in Appendix B.2,
where we show that either our current difference matrix has Frobenius norm which is unstable to
dropping few rows and columns, or a good solution exists which is compatible with our descent
algorithm. We give an explicit dropping procedure for handling the former case in Appendix B.3. We
finally put these cases together and give a win-win analysis of DropOrDescent in Appendix B.4."
REFERENCES,0.19748778785764132,"B.1
Descent steps
In this section, we give our progress analysis in the case where we can find sufficient signal in our
observations. Specifically, Lemma 7 gives sufficient conditions on a reweighted, truncated observation
matrix such that we can improve our submatrix closeness parameter by a constant factor."
REFERENCES,0.19818562456385205,"For D ∈Sd×d, we first define a clipping operation which will be used to control our reweightings.
We let clipu(D) be the matrix which clips all entries to the range [−u, u], formally defined by"
REFERENCES,0.19888346127006282,"[clipu(D)]ij = med (−u, Dij, u) .
(7)"
REFERENCES,0.19958129797627355,We also will use the following claim which follows from straightforward casework.
REFERENCES,0.2002791346824843,"Lemma 3. Let u ≥0, |a| ≤u, and a′ := med(−u, a + b, u). Then |a′ −a| ≤|b|."
REFERENCES,0.20097697138869505,"Proof. If |a + b| ≤u the claim is obvious. Otherwise, if a′ = u it must be the case that b ≥0, and
0 ≤a′ −a ≤b. Similarly, if a′ = −u we must have 0 ≥a′ −a ≥−b."
REFERENCES,0.2016748080949058,"We now introduce some additional notation. Let I ⊂S(d) be a set of symmetrized indices under
consideration (corresponding to observations). For w ∈RI and bD ∈Sd×d clear from context, we let"
REFERENCES,0.20237264480111655,"bD(w) :=
X"
REFERENCES,0.2030704815073273,"ι∈I
wι bDι.
(8)"
REFERENCES,0.20376831821353802,"For a parameter ρ, we define the set of “spread” reweightings of the observations bDI by Wρ := 
"
REFERENCES,0.2044661549197488,"w ∈RI
≥0 | max
i∈[d] X"
REFERENCES,0.20516399162595952,"j∈[d]|ι=(i,j)♯∈I
wι[ bD]2
ι ≤ρ 
"
REFERENCES,0.20586182833217026,".
(9)"
REFERENCES,0.20655966503838102,"Further, we require the following result on low-rank approximation."
REFERENCES,0.20725750174459176,"Lemma 4 (Theorem 1, [56]). Let δ, ϵ ∈(0, 1), let M ∈Sd×d, and let q ∈[d]. There is an algorithm
which returns Z ∈Rd×q with orthonormal columns such that
ZZ⊤MZZ⊤−M

op ≤(1 + ϵ)σq+1(M)"
REFERENCES,0.20795533845080252,"with probability ≥1 −δ, in time O "
REFERENCES,0.20865317515701326,"Tmv(M) · q log d δ
√ϵ ! ."
REFERENCES,0.209351011863224,"Moreover, we have for all i ∈[q], letting zi be the ith column of Z,
q"
REFERENCES,0.21004884856943476,"z⊤
i M2zi ∈[(1 −ϵ) σi (M) , (1 + ϵ) σi (M)] ."
REFERENCES,0.2107466852756455,"Finally, we provide ways to bound the singular values of an approximately low-rank matrix."
REFERENCES,0.21144452198185626,"Lemma 5. Let M, N ∈Sd×d have eigendecompositions M = UΛU⊤and N = VΣV⊤, where
Λ = diag (λ), Σ = diag (σ), and λ, σ ∈Rd are in nonincreasing order. Then, ∥M −N∥F ≥
∥λ −σ∥2."
REFERENCES,0.212142358688067,"Proof. Since ∥M∥2
F = ∥λ∥2
2 and ∥N∥2
F = ∥σ∥2
2, expanding the squares of both sides shows that it
suffices to prove that ⟨M, N⟩≤⟨λ, σ⟩, which follows from the von Neumann trace inequality."
REFERENCES,0.21284019539427773,"Lemma 6. Let ϵ ∈[0, 1], let M ∈Sd×d be rank-ˆr, and suppose ∥M −S∥F ≤ϵ for S ∈Sd×d. Then
for any s ∈N, we have σˆr+s+1(S) ≤
ϵ
s1/2 ."
REFERENCES,0.2135380321004885,"Proof. Let λ ∈Rd be the set of eigenvalues of M in nonincreasing order, and similarly let σ ∈Rd be
the nonincreasing eigenvalues of S. By assumption, λi = 0 for all i ≥ˆr + 1, and ∥λ −σ∥2 ≤ϵ by
Lemma 5. Thus, at most ˆr + s eigenvalues of S can be larger than
ϵ
s1/2 , giving the conclusion."
REFERENCES,0.21423586880669923,"We are now ready to give our main progress guarantee in Lemma 7. Specifically, we show that a
reweighting of truncated observations which has enough signal and admits a short-flat decomposition
is guaranteed to make significant progress, as long as it is sufficiently spread in the sense of (9)."
REFERENCES,0.21493370551290997,"Lemma 7. Let ϵ, γ ∈(0, 1"
REFERENCES,0.21563154221912073,"10), ℓ, u ≥0, and T ⊆S ⊆U ⊆[d] such that |U \ S| ≤γd, |S \ T| ≤γd,
and ℓ≤
ϵ
d. Suppose that for D, D⋆, N ∈Sd×d, we have D = D⋆+ N, for rank-ˆr D⋆with
∥D⋆
S×S∥F ≤1, ∥D⋆
T ×T ∥∞≤u, and ∥N∥∞≤γℓ. Let"
REFERENCES,0.21632937892533147,"bD := clipu(D),"
REFERENCES,0.21702721563154223,"and for I ⊆S(d) ∩(U × U), w ∈RI
≥0, suppose the following conditions hold for S, F ∈Sd×d"
REFERENCES,0.21772505233775297,supported on I ∩(U × U) with bD(w) = F + S.
REFERENCES,0.2184228890439637,"1. w ∈Wρ for ρ =
ϵ
2γd, recalling the definition (9)."
REFERENCES,0.21912072575017447,"2. ⟨bD, bD(w)⟩≥1 −ϵ."
REFERENCES,0.2198185624563852,3. ∥S∥F ≤1 + ϵ.
REFERENCES,0.22051639916259597,"4. ∥F∥op ≤
ϵ
ˆr1/2 ."
REFERENCES,0.2212142358688067,5. minι∈I |[ bD]ι| ≥ℓ.
REFERENCES,0.22191207257501744,"Then, letting Z ∈Rd×2ˆr be the output of Lemma 4 with parameters δ ∈(0, 1), ϵ ←1"
REFERENCES,0.2226099092812282,"2, M ←S, and
q ←2ˆr, we have with probability ≥1 −δ that


D⋆−ZZ⊤SZZ⊤ T ×T"
REFERENCES,0.22330774598743894,"F ≤10ϵ
1
4 ."
REFERENCES,0.22400558269364967,"Proof. As a starting point, we claim that"
REFERENCES,0.22470341939986044,"∥D⋆
T ×T −S∥F ≤
√"
REFERENCES,0.22540125610607117,"13ϵ.
(10)
To see this, we expand to show that
D⋆
T ×T −S
2
F ≤2 + 3ϵ −2

D⋆
T ×T , ST ×T"
REFERENCES,0.22609909281228194,"≤2 + 3ϵ −2

D⋆
T ×T ,
h
bD(w)
i T ×T"
REFERENCES,0.22679692951849267,"+ 2
D⋆
T ×T

tr ∥F∥op"
REFERENCES,0.2274947662247034,"≤2 + 5ϵ −2

bDT ×T ,
h
bD(w)
i T ×T"
REFERENCES,0.22819260293091417,"+ 2

h
D⋆−bD
i T ×T ∞"
REFERENCES,0.2288904396371249,"bD(w)

1"
REFERENCES,0.22958827634333567,"≤2 + 7ϵ −2

bDT ×T ,
h
bD(w)
i T ×T . (11)"
REFERENCES,0.2302861130495464,"The first line used Item 3 and ∥D⋆
S×S∥F ≤1, and the second used the (matrix) Hölder’s inequality.
The third used Item 4 and that ∥bDT ×T ∥tr ≤
√"
REFERENCES,0.23098394975575715,"ˆr∥bDT ×T ∥F, and the (vector) Hölder’s inequality. The
last line used ∥D⋆
T ×T ∥∞≤u, the definition of bD, and Lemma 3 to conclude

h
D⋆−bD
i T ×T"
REFERENCES,0.2316817864619679,"∞
≤∥NT ×T ∥∞≤γℓ,"
REFERENCES,0.23237962316817865,"and also bounded ∥bD(w)∥1 via Items 1 and 5, which show"
REFERENCES,0.23307745987438938,"∥bD(w)∥1 ≤
D
bD, bD(w)
E
· "
REFERENCES,0.23377529658060014,"max
ι∈I
1
[ bD]ι  ≤ρd"
REFERENCES,0.23447313328681088,"ℓ=
ϵ
2γℓ."
REFERENCES,0.23517096999302164,"Finally, since w is supported on U × U, applying |U \ T| ≤2γd, Item 1, and Item 2 yields

bDT ×T ,
h
bD(w)
i T ×T"
REFERENCES,0.23586880669923238,"≥
D
bD, bD(w)
E
−4γρd ≥1 −3ϵ.
(12)"
REFERENCES,0.23656664340544312,"Then, (10) follows by combining (11) and (12). Next, applying Lemma 6 using (10) shows that"
REFERENCES,0.23726448011165388,σ2ˆr+1(S) ≤ r 13ϵ ˆr .
REFERENCES,0.23796231681786462,"Therefore, assuming the algorithm in Lemma 4 succeeds, we have"
REFERENCES,0.23866015352407538,"ZZ⊤SZZ⊤
|
{z
}
:=S′
= S +
 
ZZ⊤SZZ⊤−S
"
REFERENCES,0.23935799023028612,"|
{z
}
:=F′"
REFERENCES,0.24005582693649685,", where ∥F′∥op ≤6√ϵ
√ ˆr
."
REFERENCES,0.24075366364270762,"At this point, we can write D(w) = S′ + (F + F′), where by the triangle inequality"
REFERENCES,0.24145150034891835,"∥F + F′∥op ≤7√ϵ · 1
√ ˆr
."
REFERENCES,0.2421493370551291,"Since ∥S′∥F ≤∥S∥F ≤1 + ϵ, the remainder of the argument is identical to the proof of (10) with the
worse flatness parameter, where we use that ∥S′
T ×T ∥F ≤∥S′∥F and ∥ST ×T ∥F ≤∥S∥F."
REFERENCES,0.24284717376133985,"B.2
Existence of good solutions
In this section, we define a stability condition (14) under which implies a solution exists which
satisfies the conditions in Lemma 7. We use this existence claim to certify progress in one of two
ways in our final algorithm. In Lemma 11, we show that under (14), there exists a reweighting
(with high probability over the semi-random observations) which is compatible with Lemma 7’s
requirements. We then show in Lemma 12 that in the absence of (14), one can increase the drop
parameter to improve the submatrix closeness bound. We will analyze an explicit dropping procedure
we use in this latter case in Appendix B.3. We first require a helper guarantee from prior work on
controlling entries of a low-rank matrix after dropping a few problematic rows and columns."
REFERENCES,0.2435450104675506,"Lemma 8 (Lemma 6, [43]). Let A ∈Sd×d be a rank-ˆr matrix with ∥A∥F ≤1, and let γ ∈[0, 1].
There is S′ ⊆[d] with |S′| ≥(1 −γ)d, such that"
REFERENCES,0.24424284717376135,∥AS′×S′∥∞≤ √
REFERENCES,0.2449406838799721,"ˆr
γd .
(13)"
REFERENCES,0.24563852058618282,"We also require a bound on the sampling error of random observations from a matrix with bounded
entries, rows, and columns, which is an application of the matrix Bernstein inequality."
REFERENCES,0.2463363572923936,"Lemma 9 (Lemma 7, [43]). Let p, δ ∈[0, 1"
REFERENCES,0.24703419399860432,"2], ρ, τ ≥0, and let A ∈Sd×d satisfy"
REFERENCES,0.2477320307048151,"∥Ai:∥2 ≤ρ for all i ∈[d], and |Aij| ≤τ for all (i, j) ∈[d] × [d]."
REFERENCES,0.24842986741102582,"Let Ω⊆S(d) be a random subset where each ι ∈S(d) is included in Ωwith probability p, and let
eA := 1"
REFERENCES,0.24912770411723656,"pAΩ. Then with probability at least 1 −δ,"
REFERENCES,0.24982554082344732,"A −eA

op ≤4 max ρ
√p s"
REFERENCES,0.2505233775296581,"log
d δ 
, τ"
REFERENCES,0.2512212142358688,"p log
d δ ! ."
REFERENCES,0.25191905094207956,"Finally, we need a guarantee on the concentration of empirical estimates of squared ℓ2 norms."
REFERENCES,0.2526168876482903,"Lemma 10 (Lemma 3, [43]). Let p, δ, α ∈(0, 1), let v ∈Rd have ∥v∥∞≤τ, and let ˜v ∈Rd have
each ˜vi independently set to vi with probability p, and 0 otherwise. Then with probability ≥1 −δ,
∥v∥2
2 −1"
REFERENCES,0.25331472435450103,"p ∥˜v∥2
2"
REFERENCES,0.2540125610607118,"≤max

α ∥v∥2
2 , 3τ 2 log 2 δ
αp 
."
REFERENCES,0.25471039776692256,We are now ready to give our main result on sufficient conditions for a good solution to exist.
REFERENCES,0.25540823447313327,"Lemma 11. Let δ, ϵ, γ, p ∈(0, 1"
REFERENCES,0.25610607117934403,"10), and S ⊆U ⊆[d] such that |U \ S| ≤γd. Let"
REFERENCES,0.2568039078855548,"ℓ:=
ϵ
20d, u := 2
√"
REFERENCES,0.2575017445917655,"ˆr
γd , p ≥240ˆr log( 6d"
REFERENCES,0.25819958129797627,"δ )
γ3ϵ2d
."
REFERENCES,0.25889741800418703,"Suppose that for D, D⋆, N ∈Sd×d, we have D = D⋆+ N, for rank-ˆr D⋆with ∥D⋆
S×S∥F ≤1
and ∥N∥∞≤γℓ. Further, for bD := clipu(D), let Ω⋆:= {ι ∈S(d) ∩(U × U) | |[ bD]ι| ≥ℓ}.
Finally, let S′ be the result of applying Lemma 8 with A ←D⋆
S×S, γ ←γ"
REFERENCES,0.2595952547103978,"2 , so ∥D⋆
S′×S′∥∞≤u,
and let T := S′ \ L where L is the indices of the ⌊γd"
REFERENCES,0.2602930914166085,"2 ⌋rows of bDS′×S′ with largest ℓ2 norms, so
|S \ T| ≤γd. Suppose
 bDT ×T

F ≥1 −ϵ"
REFERENCES,0.26099092812281927,"30.
(14)"
REFERENCES,0.26168876482903003,"Letting Ω⊆S(d) have each ι ∈S(d) independently included with probability p, and w⋆∈{0, 1 p}Ω"
REFERENCES,0.26238660153524074,have [w⋆]ι = 1
REFERENCES,0.2630844382414515,"p if and only if ι ∈(T × T) ∩Ω⋆, the following conditions all hold with probability"
REFERENCES,0.26378227494766227,"≥1 −δ over the randomness of Ω, and S⋆, F⋆∈Sd×d supported on (T ∩T) ∩Ω⋆with bD(w⋆) =
F⋆+ S⋆."
REFERENCES,0.264480111653873,"1. w⋆∈Wρ for ρ =
ϵ
2γd, recalling the definition (9). 2."
REFERENCES,0.26517794836008374,"D
bD, bD(w⋆)
E
≥1 −ϵ 2."
REFERENCES,0.2658757850662945,3. ∥S⋆∥F ≤1 + ϵ.
REFERENCES,0.2665736217725052,"4. ∥F⋆∥op ≤
γϵ
4ˆr1/2 ."
REFERENCES,0.267271458478716,"Proof. For notational convenience, let L⋆:= S(d)\Ω⋆be the set of indices ι ∈S(d) with |[ bD]ι| < ℓ.
We define our decomposition of bD(w⋆) as follows, where we recall w⋆is supported on (T × T) ∩Ω⋆:"
REFERENCES,0.26796929518492674,"bD(w⋆) := bD(T ×T )∩Ω⋆
|
{z
}
:=S⋆"
REFERENCES,0.2686671318911375,"+ bD(w⋆) −bD(T ×T )∩Ω⋆
|
{z
}
:=F⋆ ."
REFERENCES,0.2693649685973482,"Now, clearly ∥N∥F ≤d ∥N∥∞≤
ϵ
200, so ∥S⋆∥F ≤∥DS×S∥F ≤1 +
ϵ
200 where we used that every
entry of S⋆is smaller in magnitude than the corresponding entry in DS×S (it is zeroed out if it does
not belong to (T × T) ∩Ω⋆, and otherwise is clipped to a smaller range). This establishes Item 3.
The same logic shows that ∥bDS′×S′∥F ≤1 +
ϵ
200. Combined with (14) and the definition of T,"
REFERENCES,0.270062805303559,"max
i∈T"
REFERENCES,0.27076064200976974,"h
bDT ×T
i i: 2 2 ≤2"
REFERENCES,0.27145847871598044,γd · ϵ
REFERENCES,0.2721563154221912,"10 =
ϵ
5γd.
(15)"
REFERENCES,0.27285415212840197,"since P
i∈L∥[ bDT ×T ]i:∥2
2 ≤∥bDS′×S′∥2
F −∥bDT ×T ∥2
F ≤
ϵ
10, and L consisted of the largest rows
of S′ by ℓ2 norm. Therefore, applying Lemma 9 with A ←bD(T ×T )∩Ω⋆, τ ←u, and ρ2 ←
ϵ
5γd
establishes Item 4 with probability ≥1 −δ"
REFERENCES,0.2735519888346127,"3 using the lower bound on p. We further have by (14) that
 bD(T ×T )∩Ω⋆

F ≥1 −ϵ"
REFERENCES,0.27424982554082344,"30 −
 bDL⋆

F ≥1 −ϵ"
REFERENCES,0.2749476622470342,30 −dℓ≥1 −ϵ 12.
REFERENCES,0.2756454989532449,"Therefore, Lemma 10 with v set to the vectorized bD(T ×T )∩Ω⋆, τ ←u, and α ←ϵ"
SHOWS,0.2763433356594557,5 shows
SHOWS,0.27704117236566644,"bD(T ×T )∩Ω⋆

2"
SHOWS,0.2777390090718772,"F −
D
bD, bD(w⋆)
E
≤max "
SHOWS,0.2784368457780879,"α
 bD(T ×T )∩Ω⋆

2"
SHOWS,0.2791346824842987,"F , 3u2 log
  6 δ
 αp ! ≤ϵ 4"
SHOWS,0.27983251919050944,with probability ≥1 −δ
SHOWS,0.28053035589672015,"3; we again used our lower bound on p. Combining the above two displays
establishes Item 2. Finally, to see Item 1, fix a row i ∈T. We apply Lemma 10 once more with v set
to the vectorized [ bD(T ×T )∩Ω⋆]i: and α = 1"
SHOWS,0.2812281926029309,2 to obtain that with probability ≥1 −δ
SHOWS,0.2819260293091417,"3,

h
bD(w⋆)
i i: 2 2 ≤3 2"
SHOWS,0.2826238660153524,"h
bD(T ×T )∩Ω⋆
i i: 2"
SHOWS,0.28332170272156315,2 + 6u2 log( 6
SHOWS,0.2840195394277739,"δ )
p
≤2 ·
ϵ
5γd ≤
ϵ
2γd,"
SHOWS,0.2847173761339846,where we used (15) and our lower bound on p.
SHOWS,0.2854152128401954,We also provide a helper lemma for making progress if (14) fails to hold.
SHOWS,0.28611304954640615,"Lemma 12. In the setting of Lemma 11, if (14) is false, ∥D⋆
T ×T ∥F ≤1 −
ϵ
36."
SHOWS,0.2868108862526169,"Proof. This follows from Lemma 3 and the assumption on (14), which lets us bound
D⋆
T ×T

F ≤
 bDT ×T

F +

h
D⋆−bD
i T ×T F"
SHOWS,0.2875087229588276,"≤
 bDT ×T

F + d ∥N∥∞≤1 −ϵ"
SHOWS,0.2882065596650384,"30 +
ϵ
200 ≤1 −ϵ 36."
SHOWS,0.28890439637124915,"B.3
Drop steps
In the previous Appendix B.2, we developed Lemmas 11 and 12, which together say that either there
exists a good solution compatible with the progress analysis in Lemma 7 (if (14) holds), or there are
an ≈γ fraction of rows and columns which are responsible for an ≈ϵ fraction of the Frobenius norm
of the difference matrix (if (14) does not hold). In this section, we show in the latter scenario, there
is a procedure which explicitly drops a subset of rows and columns, such that on the remainder, a
slightly smaller submatrix makes an ≈ϵ fraction of progress. Importantly, we can ensure that the
submatrix parameter (i.e. γ in the sense of Definition 4) grows slowly using a tunable parameter,
which lets us iterate drop steps without losing too many rows and columns."
SHOWS,0.28960223307745986,"We begin by giving a subroutine (Algorithm 1), analogous to Section 4.1 of [43], which takes semi-
random observations from an appropriately-bounded difference matrix and explicitly drops a small
number of rows and columns, estimated from observations. We show that all remaining rows have
few large entries with high probability. Our drop step simply applies this subroutine."
SHOWS,0.2903000697836706,"Algorithm 1: Sparsify(Osr
[0,1](D), U, τ, γ, p, δ)"
SHOWS,0.2909979064898814,"1 Input: Osr
[0,1](D), U ⊆[d], τ ≥0, γ, p, δ ∈(0, 1)"
SHOWS,0.2916957431960921,2 S0 ←U
SHOWS,0.29239357990230286,3 tmax ←⌈3 log d⌉+ 1
SHOWS,0.2930914166085136,4 for 0 ≤t < tmax do
SHOWS,0.29378925331472433,"5
Dt ←Osr
p (DSt×St)"
SHOWS,0.2944870900209351,"6
for i ∈St do ri,t ←|{j ∈St | [Dt]ij was observed, and |[Dt]ij| ≥τ}|"
SHOWS,0.29518492672714586,"7
St+1 ←St \ Rt where Rt ⊂St corresponds to the ⌊γd⌋indices i with largest ri,t
8 end"
SHOWS,0.2958827634333566,9 return U ′ ←Stmax
SHOWS,0.29658060013956733,"Lemma 13. Let D ∈Sd×d, such that there is A ⊆U ⊆[d] with |U\A| ≤γd, ∥DA×A∥∞≤τ.
Let s ∈N and p ≥800"
SHOWS,0.2972784368457781,s log( d
SHOWS,0.29797627355198886,"δ ). Then with probability ≥1 −δ, Algorithm 1 returns U ′ ⊆U with
|U \ U ′| ≤4γd log d, and DU ′×U ′ has at most s entries with magnitudes ≥τ per row or column."
SHOWS,0.29867411025819957,"Proof. Let P ∈[0, 1]d×d be the true (symmetric) matrix of reveal probabilities of entries for calls to
Osr
p , following Definition 2. Note that all entries of P are at least p. We define the potential"
SHOWS,0.29937194696441033,"Φt :=
X"
SHOWS,0.3000697836706211,"(i,j)∈St×St
|[D⋆
t ]ij|≥τ Pij."
SHOWS,0.3007676203768318,We also denote the expected number of revealed large entries in each row and column as follows:
SHOWS,0.30146545708304257,"r⋆
i,t :=
X"
SHOWS,0.30216329378925333,"j∈St
|[D⋆
t ]ij|≥τ"
SHOWS,0.30286113049546404,Pij for all i ∈St.
SHOWS,0.3035589672016748,"By a standard Chernoff bound, with probability ≥1 −δ, for all 0 ≤t < tmax and all i ∈St,
ri,t −r⋆
i,t
 ≤max
1"
SHOWS,0.30425680390788556,"4r⋆
i,t, 50 log
d δ"
SHOWS,0.30495464061409633,"
.
(16)"
SHOWS,0.30565247732030704,"Condition on this event in the rest of the proof. Due to our symmetric observation model (Definition 2),
and since A × A has no entries ≥τ in magnitude, in any iteration, at least half of the contribution to
Φt must be due to remaining rows in Bt := St \ A, i.e. we have |Bt| ≤γd such that
X"
SHOWS,0.3063503140265178,"i∈Bt
r⋆
i,t ≥Φt 2 ."
SHOWS,0.30704815073272856,"Letting R⋆
i,t denote the ⌊γd⌋≥|Bt| indices in St with largest r⋆
i,t, we thus have from (16), X"
SHOWS,0.3077459874389393,"i∈Rt
ri,t ≥
X"
SHOWS,0.30844382414515004,"i∈R⋆
t
ri,t ≥3 4 X"
SHOWS,0.3091416608513608,"i∈R⋆
t
r⋆
i,t −50γd log
d δ 
≥3"
SHOWS,0.3098394975575715,"8Φt −50γd log
d δ 
."
SHOWS,0.31053733426378227,"Therefore, if Φt ≥400γd log( d"
SHOWS,0.31123517096999304,"δ ), we decrease Φt by at least a 1"
SHOWS,0.31193300767620374,"4 factor, so by our choice of tmax,
we have Φtmax−1 ≤400γd(log d"
SHOWS,0.3126308443824145,δ ) ≤γpsd
SHOWS,0.31332868108862527,"2 . This shows that there are at most γd remaining rows
i ∈Stmax−1 with r⋆
i,tmax−1 ≥ps"
SHOWS,0.31402651779483604,"2 . Now, any remaining row i ∈Stmax−1 with at least s entries with
magnitude ≥τ has r⋆
i,tmax−1 ≥ps, and therefore will be one of the largest γd remaining rows by
observed counts via (16). Hence, it will be dropped in the last iteration tmax as claimed."
SHOWS,0.31472435450104674,"We conclude by showing that the leftover poorly-behaved rows and columns (i.e. those with any large
entries) comprise a negligible fraction of the overall indices, using Lemma 8. This shows that by
excluding these few indices from our maintained submatrix (after explicitly dropping indices via
Algorithm 2), we make an ≈ϵ fraction of the progress if the condition in Lemma 12 is met."
SHOWS,0.3154221912072575,"Lemma 14. Let δ, ϵ, γ, p ∈(0, 1"
SHOWS,0.31612002791346827,"10), θ, κ ≥1, and S ⊆U ⊆[d] such that |U \S| ≤γd. Suppose that
for D, D⋆, N ∈Sd×d, we have D = D⋆+ N for rank-ˆr D⋆with
D⋆
S×S

F ≤1 and ∥N∥∞≤ϵ"
SHOWS,0.316817864619679,"d,
and that there is a subset T ⊆S with ∥D⋆
T ×T ∥F ≤1 −ϵ and |S \ T| ≤
ϵd"
SHOWS,0.31751570132588974,"θ . Then, applying
Algorithm 1 with inputs Osr
[0,1](D), U, δ, and τ ← √"
SHOWS,0.3182135380321005,"θ
5d , γ ←γ + 10 r"
SHOWS,0.3189113747383112,"ˆr
θ, p ≥800ˆrκ log( d"
SHOWS,0.319609211444522,"δ )
d
,
(17)"
SHOWS,0.32030704815073274,"returns a subset U ′ ⊆U such that with probability ≥1 −δ, there exists a subset S′ ⊆U ′ with"
SHOWS,0.32100488485694345,"|U \ U ′| ≤40  γ + r ˆr
θ !"
SHOWS,0.3217027215631542,"d log d, |U ′ \ S′| ≤

γ +
4
√ θκ"
SHOWS,0.322400558269365,"
d, and ∥DS′×S′∥F ≤1 −ϵ"
SHOWS,0.32309839497557574,8. (18)
SHOWS,0.32379623168178645,"Proof. First, by Lemma 8, there is a subset A ⊆S such that |S \ A| ≤10
q"
SHOWS,0.3244940683879972,"ˆr
θ, so that"
SHOWS,0.325191905094208,"∥DA×A∥∞≤
D⋆
A×A

∞+ ∥N∥∞≤ √"
SHOWS,0.3258897418004187,"θ
10d + ϵ d ≤ √"
SHOWS,0.32658757850662945,"θ
5d ."
SHOWS,0.3272854152128402,"Therefore, our application of Algorithm 1 with the parameters in (17) satisfies the hypotheses of
Lemma 13. Condition on the conclusion of Lemma 13 holding in the rest of the proof (with probability
≥1 −δ), giving us the first bound in (18). Also, there are at most
d
ˆrκ entries per row and column of
DU ′×U ′ with magnitude larger than τ in (17), so there are at most
d
ˆrκ entries per row and column of
D⋆
U ′×U ′ with magnitude larger than τ + ϵ"
SHOWS,0.3279832519190509,d ≤2τ.
SHOWS,0.3286810886252617,"Next, if ∥D⋆
S×S∥F ≤1 −ϵ"
SHOWS,0.32937892533147245,"3, the conclusion is obvious (taking S′ ←S ∩U ′). Otherwise, for
C := S \ T,"
SHOWS,0.33007676203768316,"D⋆
C×S
2
F ≥1 2"
SHOWS,0.3307745987438939,"D⋆
S×S
2
F −
D⋆
T ×T
2
F 
≥1 2"
SHOWS,0.3314724354501047,"
1 −ϵ 3"
SHOWS,0.33217027215631545,"2
−(1 −ϵ)2

≥ϵ 2."
SHOWS,0.33286810886252616,"By Lemma 8, there is a subset S′ ⊆S ∩U ′ with"
SHOWS,0.3335659455687369,"|(S ∩U ′) \ S′| ≤
4d
√"
SHOWS,0.3342637822749477,"θκ
,
D⋆
S′×S′

∞≤ √"
SHOWS,0.3349616189811584,"ˆrθκ
4d
.
(19)"
SHOWS,0.33565945568736916,"This set satisfies the second bound in (18), so it remains to prove the third, which follows from
D⋆
S′×S′
2
F ≤
D⋆
S×S
2
F −
D⋆
C×S
2
F −
D⋆
(C∩S′)×S′

2 F  ≤1 −ϵ"
SHOWS,0.3363572923935799,"2 +
D⋆
(C∩S′)×S′

2 F ≤1 −ϵ"
SHOWS,0.33705512909979063,2 + ϵd θ · 
SHOWS,0.3377529658060014,d · (2τ)2 + d ˆrκ · √
SHOWS,0.33845080251221216,"ˆrθκ
4d !2"
SHOWS,0.33914863921842286,≤1 −ϵ 4.
SHOWS,0.33984647592463363,The third line used that there are at most ϵd
SHOWS,0.3405443126308444,"θ rows in C, and each such row (restricted to S′) has all
but
d
rκ entries bounded in magnitude by τ, with the remainder bounded as in (19)."
SHOWS,0.34124214933705516,"B.4
Analysis of DropOrDescent and SRPartialCompletion
We now combine Lemma 7 and Lemma 14 to give a win-win analysis of a progress step,
DropOrDescent. This progress step either executes a descent step (analyzed via Lemma 7), or
a drop step (analyzed via Lemma 14); we use Lemmas 11 and 12 to demonstrate correctness of each
of these cases. For the runtime analysis, we rely on Proposition 6, the main result of Appendix C,
to solve a relevant optimization problem used by our algorithm, but all other parts of the proof are
self-contained."
SHOWS,0.34193998604326586,We provide pseudocode for the algorithm corresponding to Proposition 1 in Algorithm 2.
SHOWS,0.34263782274947663,"Algorithm 2: DropOrDescent(Osr
[0,1](M⋆+ N), M, U, γdrop, γsub, ∆, r, r⋆, δ, ϵ, θ, κ)"
SHOWS,0.3433356594556874,"1 Input: Osr
[0,1](M⋆+ N) where M⋆∈Sd×d is rank-r⋆and ∥N∥∞≤ϵγsub∆"
SHOWS,0.3440334961618981,"20d , U ⊆[d] with
|[d] \ U| ≤γdrop, rank-r M ∈Sd×d such that MU×U, M⋆
U×U are ∆-close on a γsub-submatrix,
δ, ϵ ∈(0, 1"
SHOWS,0.34473133286810886,"10), θ, κ ≥1"
SHOWS,0.34542916957431963,2 ˆr ←r + r⋆
SHOWS,0.34612700628053034,3 p1 ←240ˆr log( 30d
SHOWS,0.3468248429867411,"δ )
γ3
subϵ2d
, p2 ←800ˆrκ log( 5d"
SHOWS,0.34752267969295186,"δ )
d
, u ←2
√"
SHOWS,0.34822051639916257,"ˆr
γsubd, ℓ←
ϵ
20d"
SHOWS,0.34891835310537334,"4 I ←supp(Osr
p1(M⋆+ N)) \ {ι ∈S(d) | |[M⋆+ N −M]ι| ≤ℓ∆} ∩(U × U)"
SHOWS,0.3496161898115841,"5 vι ←∆−2 · [clipu∆(M⋆+ N −M)]2
ι for all ι ∈I"
SHOWS,0.35031402651779486,"6 (w, S) ←
ϵ
12-approximate minimizer to (20) with failure probability δ"
SHOWS,0.35101186322400557,"5, computed using
Proposition 6"
SHOWS,0.35170969993021634,"7 z ←output of Lemma 4 with M ←bD(w) −S, q ←1, ϵ ←γsub"
SHOWS,0.3524075366364271,"24 , δ ←δ 5"
SHOWS,0.3531053733426378,"8 V2 ←∆−1 · (z⊤( bD(w) −S)2z)
1
2"
SHOWS,0.35380321004884857,"9 V ←−⟨v, w⟩+ CV2"
SHOWS,0.35450104675505933,10 if V > −(1 −11ϵ
SHOWS,0.35519888346127004,12 ) then
SHOWS,0.3558967201674808,"11
U ′ ←Sparsify(Osr
[0,1]( 1"
SHOWS,0.35659455687369157,"∆(M⋆+ N −M)), U,
√"
SHOWS,0.3572923935799023,"θ
5d , γsub + 10
q"
SHOWS,0.35799023028611304,"ˆr
θ, p2, δ 5)"
SHOWS,0.3586880669923238,"12
Return: (“Drop”, U ′)"
END,0.35938590369853457,13 end
ELSE,0.3600837404047453,14 else
ELSE,0.36078157711095604,"15
Z ←output of Lemma 4 with M ←S, q ←2ˆr, ϵ ←1"
ELSE,0.3614794138171668,"2, δ ←δ"
ELSE,0.3621772505233775,"5
16
Return: (“Descent”, M′ ←M + ZZ⊤SZZ⊤)"
END,0.3628750872295883,17 end
END,0.36357292393579904,"Proposition 1. Let M ∈Sd×d be given as a rank-r factorization, let M⋆∈Sd×d be rank-r⋆,
and let U ⊆[d]. Suppose we know for γdrop, γsub ∈[0, 1], ∆> 0, that |[d] \ U| ≤γdropd, and
MU×U, M⋆
U×U are ∆-close on a γsub-submatrix. Let δ, ϵ ∈(0, 1"
END,0.36427076064200975,"10), θ, κ ≥1, and suppose for
ˆr := r + r⋆and an appropriate polynomial, p ∈ ""
κˆr"
END,0.3649685973482205,d · poly
END,0.3656664340544313,log( d
END,0.366364270760642,"δ )
γsubϵ ! , 1 #"
END,0.36706210746685275,", γsub ≤
ϵ
36θ.
(4)"
END,0.3677599441730635,"Given one call to Osr
p (M⋆+ N) for ∥N∥∞≤ϵγsub∆"
END,0.3684577808792743,"20d , the following holds with probability ≥1 −δ."
END,0.369155617585485,"1. If Algorithm 2 returns on Line 12, then MU ′×U ′, M⋆
U ′×U ′ are (1 −
ϵ
288)∆-close on a
γsub +
4
√"
END,0.36985345429169575,"θκ-submatrix, and |U \ U ′| ≤40(γsub + ( ˆr"
END,0.3705512909979065,θ)1/2)d log d.
END,0.3712491277041172,"2. If Algorithm 2 returns on Line 16, then M′
U×U, M⋆
U×U are 10ϵ
1
4 ∆-close on a 2γsub-
submatrix, and M′ is given as a rank-(3r + 2r⋆) factorization."
END,0.371946964410328,The runtime of the algorithm is O 
END,0.37264480111653875,m · poly
END,0.37334263782274946,ˆr log( d
END,0.3740404745289602,"δ )
γsubϵ !! ,"
END,0.374738311235171,"where m ≥d is the number of observed entries upon calling Osr
p (M⋆+ N)."
END,0.3754361479413817,"Proof. Throughout the proof, without loss we let ∆= 1, since we can scale M by
1
∆, take the
claimed step, and scale up by ∆(indeed, examination of Algorithm 2 shows it performs this scaling).
Further, let ˆr := r + r⋆, D⋆:= M⋆−M, D := D⋆+ N, and fix"
END,0.37613398464759246,"u := 2
√"
END,0.3768318213538032,"ˆr
γsubd, ℓ:=
ϵ
20d, ρ :=
ϵ
2γsubd."
END,0.377529658060014,"Also, let bD := clipu(D), let Ω⋆:= {ι ∈S(d) | |[ bD]ι| ≤ℓ} and let L⋆:= S(d) \ Ω⋆. Note that
Lemma 2 shows we can simulate p1-semi-random observation oracle access to bD := clipu(D) (as
required by Line 4) and the ≤4 log(d) calls to p2-semi-random observation oracle access to D (as
required by Sparsify in Line 11), given access to Osr
p (M⋆+ N), proving correctness of p in (4)."
END,0.3782274947662247,"Let I ⊆S(d) correspond to the output of Osr
p1(M + N) in Line 4, after removing indices in L⋆"
END,0.37892533147243546,"and outside U × U, which we couple to Ω⊆I, the result of calling Op1([M + N]U×U) (i.e. a
p1-semi-random observation oracle where all probabilities are p1), after removing indices in L⋆."
END,0.3796231681786462,"We now describe our algorithm. Define Wρ as in (9), and for w ∈RI, define bD(w) as in (8). Consider
the following optimization problem over Wρ × S, where S := {S ∈Sd×d | ∥S∥F ≤1 + ϵ}:"
END,0.38032100488485693,"min
w∈Wρ,S∈S F(w, S) := −⟨v, w⟩+ CH∗
η

slift

bD(w) −S

,"
END,0.3810188415910677,"where v ∈RI has vι = [ bDι]2 for all ι ∈I, C := √"
END,0.38171667829727846,"ˆr
2γsub
, η :=
ϵ
24C log(2d).
(20)"
END,0.38241451500348916,"We compute (w, S), an
ϵ
12-approximate minimizer to F, and V , satisfying |V −F(w, S)| ≤
ϵ
12.
Pseudocode for computing such a V with failure probability 1 −δ"
END,0.3831123517096999,"5 is provided in Line 9, and the
correctness and runtime of this step are analyzed at the end of this proof."
END,0.3838101884159107,1. If V > −(1 −11ϵ
END,0.3845080251221214,"12 ), we call Sparsify on (our semi-random access to) the matrix D ="
END,0.38520586182833216,"M⋆+ N −M with parameters U, τ ←
√"
END,0.3859036985345429,"θ
5d , γ ←γsub + 10
q"
END,0.3866015352407537,"ˆr
θ, p ←p, and δ ←δ"
WE,0.3872993719469644,"5. We
then return the new row subset U ′ ⊆U. We call this case the “drop” case."
WE,0.38799720865317516,"2. Otherwise, we update to M + ZZ⊤SZZ⊤, where Z ∈Rd×2ˆr is the output of Lemma 4"
WE,0.3886950453593859,with parameters δ ←δ
WE,0.38939288206559663,"5, ϵ ←1"
WE,0.3900907187718074,"2, M ←S, and q ←2ˆr. We call this case the “descent” case."
WE,0.39078855547801816,We first analyze the drop case. We claim that if V > −(1 −11ϵ
WE,0.39148639218422887,"12 ), it could not be the case that (14)
held. To see this, suppose (14) held. Then, with probability ≥1 −δ"
WE,0.39218422889043963,"5 (due to our lower bound (4) on
p), Lemma 11 provides us (w⋆, S⋆) ∈Wρ × S satisfying"
WE,0.3928820655966504,"bD(w⋆) = S⋆+ F⋆, ⟨v, w⋆⟩≥1 −ϵ"
WE,0.3935799023028611,"2, for ∥S⋆∥F ≤1 + ϵ, ∥F⋆∥op ≤γsubϵ 4
√ ˆr
."
WE,0.39427773900907187,"In fact, w⋆is only supported on Ω, the fully random coordinates. By using Fact 1, we conclude that"
WE,0.39497557571528263,"F(w⋆, S⋆) ≤−

1 −ϵ 2"
WE,0.3956734124214934,"
+ C

∥F⋆∥op + η log(2d)
"
WE,0.3963712491277041,"≤−

1 −ϵ 2"
WE,0.39706908583391487,"
+ Cγsubϵ
√"
WE,0.39776692254012563,"ˆr
+ ϵ"
WE,0.39846475924633634,"24 ≤−

1 −3ϵ 4 
."
WE,0.3991625959525471,"Therefore, any
ϵ
12-approximate minimizer (w, S) to F will satisfy F(w, S) ≤−(1 −5ϵ"
WE,0.39986043265875787,"6 ). Finally,
since we assumed our computed value V satisfies |V −F(w, S)| ≤
ϵ
12, this contradicts V >
−(1 −11ϵ"
WE,0.4005582693649686,"12 ) as claimed. Hence, (14) indeed did not hold, so by Lemma 12 and our bounds in (4), the
preconditions of Lemma 14 are met with ϵ ←
ϵ
36, proving correctness of the “drop” case with the
stated parameters."
WE,0.40125610607117934,"Next, suppose we are in the descent case, meaning we have (w, S) ∈Wρ × S satisfying F(w, S) ≤
−(1 −ϵ). By nonnegativity of the second summand in the definition of F, this shows
D
bD, bD(w)
E
= ⟨v, w⟩≥1 −ϵ."
WE,0.4019539427773901,"Moreover, because ⟨v, w⟩≤dρ =
ϵ
2γ by the definition of Wρ, we have"
WE,0.4026517794836008,"bD(w) −S

op ≤H∗
η

slift

bD(w) −S

≤
F(w, S) +
ϵ
2γ
C
≤
ϵ
√"
WE,0.4033496161898116,"ˆr
,
(21)"
WE,0.40404745289602234,where we used Fact 1 in the first inequality. Lemma 7 then yields correctness of the “descent” case.
WE,0.4047452896022331,"We next bound the runtime, which is the sum of three costs: the optimization solver, the cost of
obtaining an approximate value V , and the cost of Lemma 7 in the descent case. It is straightforward
to check that in the context of Proposition 6, which bounds the runtime of our optimization solver,"
WE,0.4054431263084438,"k =
ϵ
2γsub
, Rη = O
 ˆr log(d)"
WE,0.4061409630146546,"γ3
subϵ2"
WE,0.40683879972086534,"
, and T = O
 ˆr2 log(d)"
WE,0.40753663642707605,"γ6
subϵ4 
."
WE,0.4082344731332868,"Therefore, by our choices of C, η from before, the cost of Proposition 6 for δ"
FAILURE PROBABILITY IS,0.4089323098394976,5 failure probability is
FAILURE PROBABILITY IS,0.4096301465457083,"O

m + dˆr3 log(d)"
FAILURE PROBABILITY IS,0.41032798325191905,"γ8
subϵ6"
FAILURE PROBABILITY IS,0.4110258199581298,"
· ˆr4 log(d)"
FAILURE PROBABILITY IS,0.4117236566643405,"γ11.5
sub ϵ8 log1.5

d
δγsub 
."
FAILURE PROBABILITY IS,0.4124214933705513,"Next, consider the cost of obtaining V satisfying |V −F(w, S)|. We can explicitly compute the term
⟨v, w⟩, and because ηC log(2d) ≤
ϵ
24, it suffices to obtain an
ϵ
24C =
ϵγsub
12ˆr1/2 -additive approximation
to ∥bD(w) −S∥op by Fact 1. The guarantees of Proposition 6 give that ∥bD(w) −S∥op ≤
2ϵ
ˆr1/2 (see
(21)). Therefore, we may call Lemma 4 with q ←1, ϵ ←γsub"
FAILURE PROBABILITY IS,0.41311933007676205,"24 , and δ ←δ"
FAILURE PROBABILITY IS,0.4138171667829728,"6, in time which does"
FAILURE PROBABILITY IS,0.4145150034891835,"dominate the above, using the rank-O( ˆr2.5 log(d)"
FAILURE PROBABILITY IS,0.4152128401953943,"γ6
subϵ5
) factorization of S given to us by Proposition 6.
Additionally, the runtime of Lemma 7, i.e. the cost of running Lemma 4 with constant ϵ and q = 2ˆr,
does not dominate Proposition 6. The claimed runtime follows by combining these bounds."
FAILURE PROBABILITY IS,0.41591067690160505,"Finally, the failure probability comes from a union bound over Lemma 11, Lemma 14, the subroutine
in Proposition 6, and our two calls to Lemma 4, once for the descent step and once to compute the
value V . We set each of the failure probabilities of these five randomized steps to δ 5."
FAILURE PROBABILITY IS,0.41660851360781576,"By iterating on Proposition 1, we provide our full partial completion method, Algorithm 3."
FAILURE PROBABILITY IS,0.4173063503140265,"Corollary 1. Let M, M⋆∈Sd×d be rank-r⋆, and suppose ∥M −M⋆∥F ≤∆. Let α ≥250 and
δ, γtot ∈(0, 1"
FAILURE PROBABILITY IS,0.4180041870202373,"10). Algorithm 3 uses one call to Osr
p (M⋆+ N), where for appropriate polynomials,"
FAILURE PROBABILITY IS,0.418702023726448,"∥N∥∞≤
∆
d · poly( r⋆"
FAILURE PROBABILITY IS,0.41939986043265876,γtot log( d
FAILURE PROBABILITY IS,0.4200976971388695,"δ )) · α1+o(1) , p =
poly( r⋆"
FAILURE PROBABILITY IS,0.4207955338450802,γtot log( d
FAILURE PROBABILITY IS,0.421493370551291,"δ )) · αo(1) d
,"
FAILURE PROBABILITY IS,0.42219120725750175,"and computes a rank-r⋆· αo(1)-matrix M′, such that with probability ≥1 −δ, M′ and M⋆are
∆"
FAILURE PROBABILITY IS,0.4228890439637125,α -close on a γtot-submatrix. The runtime of the algorithm is mαo(1) · poly( r⋆
FAILURE PROBABILITY IS,0.4235868806699232,γtot log( d
FAILURE PROBABILITY IS,0.424284717376134,"δ )), where m is
the number of observed entries upon calling Osr
p (M⋆+ N)."
FAILURE PROBABILITY IS,0.42498255408234475,"Proof. We claim that throughout the course of Algorithm 3, the preconditions of Proposition 1 are
always met, i.e. γsub is always an upper bound on the number of excluded rows U \ S, γdrop is
always an upper bound on the number of dropped rows [d] \ U, and M always has rank ˆr −r⋆. To
show correctness of the rank bound, the rank r of our iterate only increases on drop steps, where
Proposition 1 ensures r + r⋆at most triples, so ˆr −r⋆(after the update on Line 11) remains a correct
bound on r throughout the algorithm. Also, ˆr ≤ˆrtot throughout, since there are ≤R descent steps."
FAILURE PROBABILITY IS,0.42568039078855546,"Algorithm 3: SRPartialCompletion(Osr
[0,1](M⋆+ N), M, ∆, r⋆, α, γtot, δ)"
FAILURE PROBABILITY IS,0.4263782274947662,"1 Input: Osr
[0,1](M⋆+ N) where M⋆∈Sd×d, rank-r⋆M ∈Sd×d such that ∥M −M⋆∥F ≤∆,
α ≥250, γtot, δ ∈(0, 1 10)"
FAILURE PROBABILITY IS,0.427076064200977,"2 Ndrop ←0, Ndescent ←0, R ←√log α, ϵ ←exp(−8√log α), K ←288√log α"
FAILURE PROBABILITY IS,0.4277739009071877,"ϵ
,
ˆrtot ←(2r⋆) · 3R"
FAILURE PROBABILITY IS,0.42847173761339846,"3 ˆr ←2r⋆, U ←[d], γtemp ←
ϵγ2
tot
106ˆrtotRK log(d), γsub ←
γtemp
160RK log(d) · 2−R, γdrop ←0"
FAILURE PROBABILITY IS,0.4291695743196092,4 θ ←25600ˆrtot(RK log(d))2
FAILURE PROBABILITY IS,0.42986741102581993,"γ2
tot
, κ ←16γ2
totR2K24R"
FAILURE PROBABILITY IS,0.4305652477320307,"ˆrtotγ2
temp
5 while Ndrop < RK and Ndescent < R do"
FAILURE PROBABILITY IS,0.43126308443824146,"6
if DropOrDescent(Osr
[0,1](M⋆+ N), M, U, γdrop, γsub, ∆, ˆr −r⋆, r⋆,
δ
R(K+1), ϵ, θ, κ) =
(“Drop”, U ′) then"
FAILURE PROBABILITY IS,0.4319609211444522,"7
U ←U ′, γdrop ←γdrop +
γtot
2RK , ∆←exp(−ϵ"
FAILURE PROBABILITY IS,0.43265875785066293,"288)∆, Ndrop ←Ndrop + 1"
END,0.4333565945568737,"8
end"
ELSE,0.43405443126308446,"9
else"
ELSE,0.43475226796929517,"10
(“Descent”, M′) ←
DropOrDescent(Osr
[0,1](M⋆+ N), M, U, γdrop, γsub, ∆, ˆr −r⋆, r⋆,
δ
R(K+1), ϵ, θ, κ)"
ELSE,0.43545010467550593,"11
ˆr ←3ˆr, γsub ←2γsub, M ←M′, ∆←10ϵ
1
4 ∆, Ndescent ←Ndescent + 1"
END,0.4361479413817167,"12
end"
END,0.4368457780879274,13 end
END,0.43754361479413817,14 Return: M
END,0.43824145150034893,"We next prove correctness of the γsub parameter throughout the algorithm. By Proposition 1, this
parameter doubles each time Algorithm 2 runs a descent step, which is correctly handled by Line 11.
Moreover, the only other time γsub increases is by
4
√"
END,0.43893928820655964,"θκ each time Algorithm 2 runs a drop step, via
the first part of Proposition 1. Note that the algorithm terminates once RK drop steps or R descent
steps have run, and by our choices of θ, κ, we have
4
√"
END,0.4396371249127704,"θκ
· RK ≤
γtemp
160RK log(d) · 2−R,"
END,0.44033496161898117,"which is our initial setting of γsub in Line 3. Therefore, since the worst-case growth of the γsub
parameter over RK drop steps and R descent steps is if we encounter all of the drop steps first (and
then repeatedly double R times), the value of γsub is correct throughout the algorithm."
END,0.44103279832519193,"Next, we show that the γdrop parameter remains correct throughout the algorithm. Note that the γsub
parameter is never larger than
γtemp
160RK log(d) ≤
γtot
160RK log(d). Therefore, the number of dropped rows
in each drop step, by Proposition 1 and our choice of θ, is at most 40"
END,0.44173063503140264,"γtot
160RK log d + r ˆrtot θ !"
END,0.4424284717376134,"d log d ≤
 γtot"
END,0.44312630844382417,4RK + γtot
RK,0.4438241451500349,4RK
RK,0.44452198185624564,"
· d = γtotd 2RK ."
RK,0.4452198185624564,"Therefore the update to γdrop in Line 7 is correct. We have also shown that over the course of the
algorithm, we never drop or exclude more than
γtotd
160RK log(d) + γtotd"
RK,0.4459176552686671,"2
≤γtotd rows in total. Additionally,
we verify the second condition in (4) holds with the largest value of γsub:
γtemp
160RK log(d) ≤
ϵ
36θ."
RK,0.4466154919748779,"Finally, after RK drop steps Proposition 1 shows that we have made an exp(−ϵ"
RK,0.44731332868108864,288 · RK) = 1
RK,0.44801116538729935,"α factor
progress on the closeness parameter ∆. Similarly, after R descent steps, since 10ϵ
1
4 ≤ϵ
1
8 by our
condition α ≥250, we have made an ϵ
R 8 = 1"
RK,0.4487090020935101,"α factor progress as well. The failure probability comes
from union bounding over the at most R(K + 1) calls to DropOrDescent."
RK,0.4494068387997209,"For the runtime bound, by observation all of the parameters ˆr, γ−1
sub , ϵ−1 in Proposition 1 are bounded
by a polynomial in r⋆· αo(1) throughout the algorithm. The correctness of the lower bound on p and
the upper bound on ∥N∥∞similarly follow from Proposition 1."
RK,0.45010467550593164,"C
Computing adaptive reweightings"
RK,0.45080251221214235,"In this section, we develop an optimization subroutine for solving the subproblems (20) arising in
our partial matrix completion method. We use an approximate Frank-Wolfe framework to reduce
our optimization to a sequence of linear optimization problems over our constraint set, which we
characterize as a matching polytope over a bipartite graph. We show that for the vector block of our
variable, we can cast these linear optimization problems as computing maximum weight bipartite
matchings, as explained in Appendix C.4. To efficiently implement the matrix block steps, we apply
standard tools from the numerical linear algebra and approximation theory literature in Appendix C.3.
We now define the problem we study in a self-contained way."
RK,0.4515003489183531,"Throughout this section, we fix a subset I ⊂S(d) and parameters k ∈R>0, ϵ ∈(0, 1), and define: X := 
"
RK,0.4521981856245639,"x ∈RI
≥0 | max
i∈[d] X"
RK,0.4528960223307746,"j∈[d]|ι=(i,j)♯∈I
xι ≤k d 
 ,"
RK,0.45359385903698535,"S :=

S ∈Sd×d | ∥S∥F ≤1 + ϵ
	
, Z := X × S. (22)"
RK,0.4542916957431961,"In other words, S is the set of “short” (Frobenius-norm bounded) matrices. Further, if x ∈X is
viewed as a symmetric d×d matrix, the constraints on X enforce that x has bounded row and column
sums. Clearly x ∈X =⇒∥x∥1 ≤k, so X is a subset of an ℓ1 ball of radius k. However, we can
more specifically capture X as a scaled bipartite matching polytope."
RK,0.4549895324494068,"Fact 3. Let I ⊂S(d) be identified with a d × d bipartite graph G = (L ∪R, E), where for
(i, j) ∈[d]×[d], we place an edge from the ith vertex in L (denoted ui) to the jth vertex in V (denoted
vj) if and only if (i, j)♯∈S(d). Let M be the bipartite matching polytope of G, i.e. M := 
"
RK,0.4556873691556176,"f ∈RE
≥0 | max
ui∈L X"
RK,0.45638520586182835,"vj∈R|(ui,vj)∈E
f(ui,vj) ≤1 
"
RK,0.45708304256803906,".
(23)"
RK,0.4577808792742498,Then d
RK,0.4584787159804606,"kX = M for X in (22), where we map coordinates in I to edges in E in the canonical way."
RK,0.45917655268667135,"By the characterization in Fact 3, we efficiently implement iterations of our Frank-Wolfe method
by calling an approximate maximum weight matching algorithm by [22]. We also use the structure
of the matching polytope to prove a key regularity property on our objective in Appendix C.2. We
finally require the definition of a bounded submatrix, whose entries lie in a given range."
RK,0.45987438939288205,"Definition 5 (Bounded submatrix). Let D ∈Sd×d and I ⊂S(d). We say that (D, I) is an
(ℓ, u)-bounded submatrix if for all ι ∈I, |[D]ι| ∈[ℓ, u]."
RK,0.4605722260990928,"When D and I are clear from context, we let D be the operator on RI such that"
RK,0.4612700628053036,"D(w) :=
X"
RK,0.4619678995115143,"ι∈I
wιDι.
(24)"
RK,0.46266573621772505,"With these definitions in hand, our goal in this section is to optimize the following jointly convex
function over (x, S) ∈Z, for an (ℓ, u)-bounded submatrix (D, I) and parameters C, η ∈R>0,
named Fprog-flat to denote its decomposition into progress and (approximate) flatness components:"
RK,0.4633635729239358,"Fprog-flat(x, S) := −⟨1I, x⟩+ CH∗
η

slift

D
x v"
RK,0.4640614096301465,"
−S

,"
RK,0.4647592463363573,"where vι = [D]2
ι for all ι ∈I, and x"
RK,0.46545708304256805,"v denotes entrywise division.
(25)"
RK,0.46615491974877876,"Note that (25) is exactly the problem in (20), up to reparameterizing x ←w ◦v. It may be helpful
to think of the parameter settings for (25) in the context (20), e.g., C ≈
√r"
RK,0.4668527564549895,"γ , η ≈1"
RK,0.4675505931612003,"C , u ≈
√r"
RK,0.46824842986741105,"d , ℓ≈
1
d, k ≈1"
RK,0.46894626657362176,"γ , where r and γ are the rank and closeness parameter of a current iterate."
RK,0.4696441032798325,"C.1
Approach based on the Frank-Wolfe method
To efficiently minimize f = Fprog-flat (defined in (25)) over Z (defined in (22)) we apply a variant
of Frank-Wolfe or conditional gradient methods due to [35]. This method essentially reduces the"
RK,0.4703419399860433,"problem to approximately solving minz∈Z ⟨∇f(q), z⟩for different values of q ∈Z. The algorithm
is given below as Algorithm 4.10 Below we define the curvature constant, the key quantity which
bounds the convergence of the method, as well as the main theorem from [35] which we use."
RK,0.471039776692254,"Algorithm 4: ApproxFrankWolfe(f, D, T, δ) (Algorithm 2 of [35] restated)"
RK,0.47173761339846476,"1 Input: Differentiable, convex f : Rd →R and compact, convex D ⊆Rd such that the curvature
constant of f with respect to D is Cf, iteration count K ≥1 and approximation tolerance δ ≥0"
RK,0.4724354501046755,2 Compute arbitrary z(0) ∈D
RK,0.47313328681088623,"3 for t = 0, . . . , T −1 do"
RK,0.473831123517097,"4
γ(t) ←
2
t+2"
RK,0.47452896022330776,"5
Compute s(t) ∈D such that

∇f(z(t)), s(t)
≤mins∈D

∇f(z(t)), s

+ δγ(t)Cf,D"
RK,0.47522679692951847,"2
6
z(t+1) ←(1 −γ(t))z(t) + γ(t)s(t)"
END,0.47592463363572923,7 end
END,0.47662247034194,8 return z(T )
END,0.47732030704815076,"Definition 6 (Curvature constant (restated from [35])). The curvature constant Cf,D of convex and
differentiable f : Rd →R with respect to compact domain D ⊆Rd is"
END,0.47801814375436147,"Cf,D :=
sup
z0,z1∈D,γ∈[0,1],zγ=z0+γ(z1−z0)"
END,0.47871598046057223,"2
γ2 (f(zγ) −f(z0) −⟨∇f(z0), zγ −z0⟩) ."
END,0.479413817166783,"Proposition 4 (Approximate Frank-Wolfe, Theorem 1 of [35] restated and specialized). Let f : Rd →
R be convex and differentiable, let D ⊆Rd be compact and convex, and let z⋆∈argminz∈Df(z).
For any K ≥1 and δ ≥0, z(T ) = ApproxFrankWolfe(f, D, T, δ) (Algorithm 4) satisfies"
END,0.4801116538729937,"f(z(T )) −f(z⋆) ≤2(1 + δ)Cf,D"
END,0.48080949057920447,"T + 2
."
END,0.48150732728541523,"To facilitate applying Proposition 4 to efficiently minimize Fprog-flat (25) over Z (22) we provide two
general lemmas. The first is Lemma 15 below, which implies that to implement Line 5 of Algorithm 4
it suffices to compute approximate gradients and and then approximately solve constrained linear
optimization problems for these approximate gradients."
END,0.48220516399162594,"Lemma 15 (Gradient approximation). For compact D ⊆Rd, g, ˆg ∈Rd, and ∆, M ≥0 suppose that"
END,0.4829030006978367,"| ⟨g −ˆg, s⟩| ≤∆and | ⟨g, s⟩| ≤M for all s ∈D"
END,0.48360083740404747,"If ⟨ˆg, ˆs⟩≤(1 −α) minx∈D ⟨ˆg, s⟩+ ∆for ˆs ∈D and α ∈[0, 1] then"
END,0.4842986741102582,"⟨g, ˆs⟩≤min
x∈D ⟨g, s⟩+ αM + 3∆."
END,0.48499651081646894,"Proof. For s⋆∈argmins∈D ⟨g, s⟩the assumptions imply that"
END,0.4856943475226797,"⟨g, s⟩= ⟨ˆg, s⟩+ ⟨ˆg −g, s⟩≤(1 −α) ⟨ˆg, s⋆⟩+ ∆+ ∆."
END,0.48639218422889047,The result then follows as
END,0.4870900209351012,"(1 −α) ⟨ˆg, s⋆⟩= (1 −α) ⟨g, s⋆⟩+ (1 −α) ⟨g −ˆg, s⋆⟩≤⟨g, s⋆⟩+ αM + ∆."
END,0.48778785764131194,"The second lemma is the following straightforward upper bound on the curvature constant (that is
perhaps related to the relationship to smoothness given in Lemma 7 of [35]). It shows that to upper
bound the curvature constant it suffices to upper bound ∇2f(z)[w, w] for all w, z ∈D."
END,0.4884856943475227,"Lemma 16 (Curvature bound). For convex, twice-differentiable f : Rd →R and compact D ⊆Rd if
∇2f(z)[w, w] ≤M for all w, z ∈D then the curvature constant f with respect to D is at most 2M."
END,0.4891835310537334,"10We perform minor changes to the names of the variables, choice of offsets, etc. to facilitate and simplify the
use of [35] to obtain our results."
END,0.4898813677599442,"Proof. Let z0, z1 ∈Z and γ ∈[0, 1] and define zγ := z0 + γ(z1 −z0) for all γ ∈[0, 1]. Note that"
END,0.49057920446615494,"f(zγ) −f(z0) −⟨∇f(z0), zγ −z0⟩= −γ ⟨∇f(z0), z1 −z0⟩+
Z γ"
END,0.49127704117236565,"0
⟨∇f(zt), z1 −z0⟩dt =
Z γ"
END,0.4919748778785764,"0
⟨∇f(zt) −∇f(z0), z1 −z0⟩dt ."
END,0.4926727145847872,"Additionally,"
END,0.4933705512909979,"⟨∇f(zt) −∇f(z0), z1 −z0⟩=
Z t"
END,0.49406838799720865,"0
∇2f(zα)[z1 −z0, z1 −z0]dα"
END,0.4947662247034194,"≤t · sup
z∈D
∇2f(z)[z1 −z0, z1 −z0] ."
END,0.4954640614096302,"Now, since f is convex, ∇2f(x) is positive semidefinite for all x ∈D and so"
END,0.4961618981158409,"∇2f(z)[z1 −z0, z1 −z0] ≤2∇2f(z)[z1, z1] + 2∇2f(z)[z0, z0] ≤2M ."
END,0.49685973482205165,Combining these bounds yields the desired bound of
END,0.4975575715282624,"f(zγ) −f(z0) −⟨∇f(z0), zγ −z0⟩≤
Z γ"
END,0.4982554082344731,"0
2Mtdt = Mγ2 ."
END,0.4989532449406839,"In the following Appendix C.2 we bound the curvature constant of the particular f = Fprog-flat defined
in (24) over Z and provide additional regularity bounds on this f over Z. Then in Appendices C.3
and C.4, we discuss the implementation of a single step of Algorithm 5 (specifically, Line 5 by
satisfying the conditions needed to apply Lemma 15). To fix some notation, we let zt := (xt, St) be
the tth iterate of Algorithm 5 (z(t) in the code). We also let"
END,0.49965108164689465,"Yt :=
exp

1
ηslift
 
D
  xt"
END,0.5003489183531054,"v

−St
"
END,0.5010467550593162,"Tr exp

1
ηslift
 
D
  xt"
END,0.5017445917655269,"v

−St
"
END,0.5024424284717376,"be the gradient of H∗
η(slift(D( x"
END,0.5031402651779484,"v ) −S)) before applying the chain rule, see Fact 1. Because of the
block-separable structure of slift, we have that"
END,0.5038381018841591,"Yt =
1
Tr(Lt) + Tr(Rt)"
END,0.5045359385903698,"
Lt
0d×d
0d×d
Rt 
,"
END,0.5052337752965806,"where Lt := exp
1 η"
END,0.5059316120027914,"
D
xt v"
END,0.5066294487090021,"
−St

, Rt := exp
1 η"
END,0.5073272854152129,"
St −D
xt v"
END,0.5080251221214236,"
.
(26)"
END,0.5087229588276343,"For notational simplicity, we finally define"
END,0.5094207955338451,"Bt := Lt −Rt, Tt := Tr(Lt) + Tr(Rt)."
END,0.5101186322400558,"We observe that by definition of the objective in (25), we have"
END,0.5108164689462665,[∇xf(zt)]ι = −1 + C
END,0.5115143056524774,"Tt
⟨Bt, Eι⟩for all ι ∈I, ∇Sf(zt) = C"
END,0.5122121423586881,"Tt
Bt."
END,0.5129099790648988,"Finally, gt := (ht, Ht) denotes the blocks of the approximate gradient computed in iteration t and
vt := (wt, Wt) denotes the blocks of the approximate linear objective optimizer in iteration t, as
suffice for applying Lemma 15. Throughout Appendices C.3 and C.4, we fix a single iteration t and
drop t from all subscripts, to decrease notational clutter."
END,0.5136078157711096,"C.2
Curvature constants and regularity
Here we bound the curvature constant of Fprog-flat as defined in (25) over Z defined in (22). To do
so, we first provide a bound, Lemma 17, on the operator norm of D(x) defined in (24), for x ∈X.
The proof follows from a previously established fact that the operator norm of any matrix (even if
rectangular) is upper bounded by the maximum ℓ1 norm of any its rows or columns (see e.g., Lemma
B.4, [19]); however, we directly give a self-contained proof for convenience."
END,0.5143056524773203,"Lemma 17. For all x ∈X (defined in (22)), we have ∥D(x)∥op ≤ku"
END,0.515003489183531,d for D as defined in (24).
END,0.5157013258897418,"Proof. For all z ∈Rd, the Cauchy-Schwarz inequality and the fact that D(x) is symmetric imply"
END,0.5163991625959525,"z⊤D(x)z
 =  X"
END,0.5170969993021632,"i,j∈[d]
[D(x)]ij zizj ≤  X"
END,0.5177948360083741,"i,j∈[d]
z2
i
[D(x)]ij   1
2  X"
END,0.5184926727145848,"i,j∈[d]
z2
j
[D(x)]ij   1
2 =
X"
END,0.5191905094207956,"i∈[d]
z2
i
X j∈[d]"
END,0.5198883461270063,"[D(x)]ij
 ."
END,0.520586182833217,"Additionally, for all i ∈[d] since (D, I) is an (ℓ, u)-bounded submatrix and x ∈X, X j∈[d]"
END,0.5212840195394278,"[D(x)]ij
 | =
X"
END,0.5219818562456385,"j∈[d]|ι=(i,j)♯∈I"
END,0.5226796929518492,"[D(x)]ij
 ≤ku d ."
END,0.5233775296580601,"Combining shows that |z⊤D(x)z| ≤
ku"
END,0.5240753663642708,"d ∥z∥2
2 for all z ∈Rd and yields the result as D(x) is
symmetric."
END,0.5247732030704815,"Lemma 17 improves upon the trivial bound on ∥D(x)∥op for x with ∥x∥1 ≤k. Note that if x has k on
the index corresponding to the largest entry of DI, then ∥D(x)∥op ≥ku, which is substantially larger
than ku"
END,0.5254710397766923,"d bound given in Lemma 17. The improved bound for x ∈X that we show in Lemma 17 is key
to obtaining bounds on the curvature constant of Fprog-flat over Z which are sufficient for obtaining
our desired runtimes (i.e., scaling polynomially only in r⋆, not d). We provide the curvature bound as
well as different bounds on the regularity of Fprog-flat over Z below."
END,0.526168876482903,"Lemma 18. For all z, z′ ∈Z with z = (x, S) for x ∈X and S ∈S as defined in (22) and for
f = Fprog-flat defined in (25),"
END,0.5268667131891137,"|⟨∇X f(z′), x⟩| ≤C ku"
END,0.5275645498953245,"ℓ2d + k, |⟨∇Sf(z′), S⟩| ≤2C, and ∇2f(z′)[z, z] ≤C η"
END,0.5282623866015352,"
2 + ku ℓ2d"
END,0.528960223307746,"2
. (27)"
END,0.5296580600139568,"Consequently, CFprog-flat,Z ≤2C"
END,0.5303558967201675,"η
 
2 + ku"
END,0.5310537334263782,"ℓ2d
2."
END,0.531751570132589,"Proof. Let z, z′
∈
Z be arbitrary.
Let A be the linear operator such that A(x, S)
=
slift
 
D
  x"
END,0.5324494068387997,"v

−S

. Let G be the gradient of H∗
η at Az′ and let H be the Hessian of H∗
η at Az′.
Note that the chain rule, Fact 1, and that the trace and operator norm are dual imply that, for
z = (x, S),"
END,0.5331472435450104,"|⟨∇f(z′), z⟩| = |−⟨1I, x⟩+ C ⟨G, Az⟩| ≤∥x∥1 + C ∥G∥tr ∥Az∥op ≤k + C ∥Az∥op ,"
END,0.5338450802512212,"∇2f(z′)[z, z] = CH[Az, Az] ≤C"
END,0.534542916957432,"η ∥Az∥2
op ."
END,0.5352407536636427,"However, for any (x, S) ∈Z = X × S,"
END,0.5359385903698535,"slift

D
x v"
END,0.5366364270760642,"
−S

op ≤
D
x v"
END,0.537334263782275,"
op + 1 + ϵ ≤ku"
END,0.5380321004884857,"ℓ2d + 2,
(28)"
END,0.5387299371946964,"where the first inequality used the triangle inequality and that the operator norm is bounded by the
Frobenius norm, and the second used the range of v, that ϵ ∈(0, 1), and Lemma 17. Combining these
inequalities yields ∇2f(z′)[z, z] ≤C"
END,0.5394277739009072,"η
 
2 + ku"
END,0.540125610607118,"ℓ2d
2. Applying the same argument with S = 0 and x ="
END,0.5408234473133287,"0 yields |⟨∇X f(z′), x⟩| ≤C ku"
END,0.5415212840195395,"ℓ2d +k and |⟨∇Sf(z′), S⟩| ≤2C. Finally, CFprog-flat,Z ≤2C"
END,0.5422191207257502,"η
 
2 + ku"
END,0.5429169574319609,"ℓ2d
2"
END,0.5436147941381717,follows from Lemma 16.
END,0.5443126308443824,"C.3
Matrix step implementation
In this section, we discuss how to approximate the S block of ∇Fprog-flat(z) := ∇Fprog-flat(x, S)
within the error allowed by Line 5 and Lemma 15, where (as discussed earlier) we denote our
approximation by"
END,0.5450104675505931,"H ≈∇Sf(x, S) = C"
END,0.5457083042568039,"T B, for T = Tr(L) + Tr(R), B = L −R,"
END,0.5464061409630147,following notation in (26). We then give implementation details for computing
END,0.5471039776692254,"W ≈min
M∈S ⟨H, M⟩,"
END,0.5478018143754362,"within the error threshold required by Line 5. Our approximately-optimal step matrix W will have
the convenient property of coming with an explicit low-rank representation, facilitating faster imple-
mentation of future iterations of our subproblem solver. We will give approximation guarantees in a
mixed multiplicative-additive format, whose conversion to additive error is facilitated by Lemma 15."
END,0.5484996510816469,The main result of this section is the following.
END,0.5491974877878576,"Lemma 19 (S step). There is an algorithm which takes as input x ∈X, S ∈S given as a rank-r
factorization, R ≥∥1"
END,0.5498953244940684,η(D( x
END,0.5505931612002791,"v ) −S)∥op, ∆∈[0, C], and α, δ ∈(0, 1), and computes W ∈S as a
rank-q factorization satisfying"
END,0.5512909979064898,"⟨W, H⟩≤(1 −α)argminW⋆∈S ⟨W⋆, H⟩+ ∆,
(29)"
END,0.5519888346127007,"for H ∈Sd×d with |⟨H −∇SFprog-flat(x, S), S′⟩| ≤∆for all S′ ∈S,
(30)"
END,0.5526866713189114,"with probability ≥1 −δ. Moreover, q = O( C2"
END,0.5533845080251221,"∆2 ), and the algorithm runs in time O "
END,0.5540823447313329,(|I| + dr) · R r log C
END,0.5547801814375436,"∆·
 1"
END,0.5554780181437544,α2 + C2 ∆2
END,0.5561758548499651,"
log
d δ ! ."
END,0.5568736915561758,"Approximating the gradient.
We begin by bounding the complexity of matrix-vector access to a
matrix eB ≈B −L −R, with a given tolerance for approximation error. Ultimately we will use the
approximate gradient H ←C"
END,0.5575715282623867,"T
eB
(31)"
END,0.5582693649685974,"to implement the update to S in our algorithm. Our access is based on a standard polynomial
approximation to the exponential which is well-known in the literature."
END,0.5589672016748081,"Lemma 20 ([2]). For α ≥0 and R ≥0, there is a polynomial p of degree O(R
q log 1"
END,0.5596650383810189,"α) such that
| exp(x) −p(x)| ≤α exp(x) for all x ∈[−R, R]."
END,0.5603628750872296,Applying Lemma 20 twice directly yields our approximation eB.
END,0.5610607117934403,"Corollary 2. Let x ∈X, S ∈S satisfy ∥1"
END,0.5617585484996511,η(D( x
END,0.5624563852058618,"v ) −S)∥op ≤R, and let ∆≥0. There is a matrix eB
such that, for L := exp( 1"
END,0.5631542219120725,η(D( x
END,0.5638520586182834,v ) −S)) and R := exp( 1
END,0.5645498953244941,η(S −D( x
END,0.5652477320307048,"v ))), and defining"
END,0.5659455687369156,"T := Tr(L) + Tr(R), B := L −R,"
END,0.5666434054431263,we have ∥eB∥tr ≤(1 + ∆
END,0.567341242149337,"8C )T, as well as C
T"
END,0.5680390788555478,"D
eB −B, S′E
≤∆for all S′ ∈S, Tmv(eB) = O "
END,0.5687369155617585,(|I| + Tmv(S)) · R r log C ∆ ! .
END,0.5694347522679692,"Proof. It suffices to provide matrices eL, eR such that"
END,0.5701325889741801,Tmv(eL) + Tmv( eR) = O 
END,0.5708304256803908,"(|I| + Tmv(S)) · R r log C ∆ ! ,"
END,0.5715282623866015,"eL −L

tr ≤∆"
END,0.5722260990928123,"8C Tr(L),
 eR −R

tr ≤∆"
END,0.572923935799023,8C Tr(R).
END,0.5736217725052338,"Indeed, letting eB := eL −eR, the bound on Tmv(eB) is then immediate, and by Hölder’s inequality
(with ∥S′∥op ≤∥S′∥F ≤2) and the triangle inequality, we also have the desired
C
T"
END,0.5743196092114445,"D
eB −B, S′E
≤2C T"
END,0.5750174459176552,"eB −B

tr ≤2C T"
END,0.5757152826238661,"eL −L

tr +
 eR −R

tr 
≤∆,"
END,0.5764131193300768,"by the definition of T = Tr(L) + Tr(R). We only discuss eL, as the case of eR is symmetric. We
let eL = p( 1"
END,0.5771109560362875,η(D( x
END,0.5778087927424983,"v ) −S)), where p is the polynomial from Lemma 20 with α ←
∆
8C . The bound"
END,0.578506629448709,"on Tmv(eL) follows from the degree of p, and the fact that Tmv(D( x"
END,0.5792044661549197,"v )) = O(|I|). Moreover, letting
{λi}i∈[d] ⊆[−R, R] be the eigenvalues of 1"
END,0.5799023028611305,η(D( x
END,0.5806001395673412,"v ) −S), we indeed have
eL −L

tr =
X"
END,0.581297976273552,"i∈[d]
|p(λi) −exp(λi)| ≤∆"
C,0.5819958129797628,8C X
C,0.5826936496859735,"i∈[d]
exp(λi) = ∆"
C,0.5833914863921842,8C Tr(L).
C,0.584089323098395,"Finally, the claim ∥eB∥tr ≤(1 + ∆"
C,0.5847871598046057,8C )T follows from the triangle inequality.
C,0.5854849965108164,"Approximating the S step.
We now build upon the access afforded by Corollary 2 to compute
W, our approximate solution to minM∈S⟨H, M⟩, where H := C"
C,0.5861828332170272,"T eB is our gradient approximation.
Specifically, we use the following standard tool with Lemma 4 to provide our approximate linear
optimization implementation over S for any matrix supporting vector multiplication access."
C,0.586880669923238,"Lemma 21. Let α, δ ∈(0, 1) and M ∈Sd×d. There is an algorithm which returns F ∈[(1 −
α) ∥M∥F , (1 + α) ∥M∥F] with probability ≥1 −δ in time O(Tmv(M)α−2 log d δ )."
C,0.5875785066294487,"Proof. Let Q ∈Rk×d with independently uniform random unit vector rows in Rd scaled down by
1
√"
C,0.5882763433356595,k for appropriately large k = Θ(α−2 log d
C,0.5889741800418702,"δ ). By the Johnson-Lindenstrauss lemma of [20], with
probability ≥1 −δ, we have for all i ∈[d] that
[MQ⊤]i:
2"
C,0.5896720167480809,"2 ∈
h
(1 −α) ∥Mi:∥2
2 , (1 + α) ∥Mi:∥2
2
i
."
C,0.5903698534542917,"Therefore, summing for all i ∈[d] proves that Tr(QM2Q⊤) = F satisfies the desired bound. We can
compute MQ⊤explicitly in time k · Tmv(M), and computing the Frobenius norm of MQ⊤∈Rd×k
does not dominate the stated running time."
C,0.5910676901605024,"Lemma 22. Let α, δ ∈(0, 1), ∆≥0, let H ∈Sd×d have ∥H∥tr ≤τ, and let W⋆:=
argminW∈S ⟨W, H⟩. There is an algorithm which with probability ≥1 −δ returns W = ZRZ⊤"
C,0.5917655268667132,"satisfying R ∈Sq×q and Z ∈Sd×q, with"
C,0.592463363572924,"⟨W, H⟩≤(1 −α) ⟨W⋆, H⟩+ ∆, q = O
 τ 2 ∆2"
C,0.5931612002791347,"
,
(32)"
C,0.5938590369853455,in time
C,0.5945568736915562,"O

Tmv(H) ·
 1"
C,0.5952547103977669,α2 + τ 2 ∆2
C,0.5959525471039777,"
log
d δ 
."
C,0.5966503838101884,"Proof. Note that in closed form, we have W⋆= −(1 + ϵ)
H
∥H∥F , and the optimal objective value is
⟨W⋆, H⟩= −∥H∥F. We first compute F using Lemma 21 such that"
C,0.5973482205163991,"∥H∥F ≤F ≤

1 + α 8"
C,0.59804605722261,"
∥H∥F ,"
C,0.5987438939288207,within the allotted time and failure probability δ
C,0.5994417306350314,"2. Then, letting W′ ←−(1 + ϵ) H"
C,0.6001395673412422,"F ∈S, we have"
C,0.6008374040474529,"⟨W′, H⟩−⟨W⋆, H⟩= −∥H∥2
F
F
+ (1 + ϵ) ∥H∥F ≤α"
C,0.6015352407536636,"2 ∥H∥F ≤−α ⟨W⋆, H⟩.
(33)"
C,0.6022330774598744,"Next, let W ←ZZ⊤W′ZZ⊤where Z is the result of Lemma 4 with q ←⌈9τ 2"
C,0.6029309141660851,"2∆2 ⌉, ϵ ←1"
C,0.6036287508722958,"2, and failure
probability δ"
C,0.6043265875785067,"2. We output R := Z⊤W′Z and Z, which can be computed within the allotted time. The
result follows by combining (33) with"
C,0.6050244242847174,"⟨W, H⟩−⟨W′, H⟩≤∥W′ −W∥op ∥H∥tr ≤3τ"
C,0.6057222609909281,2 σq+1(W′) ≤∆.
C,0.6064200976971389,"Here, we used that ∥W′∥F ≤2 implies ≤9τ 2"
C,0.6071179344033496,2∆2 singular values of W′ are larger than 2τ 3∆.
C,0.6078157711095603,We make one slight extension to Lemma 22 under implicit access to H.
C,0.6085136078157711,"Corollary 3. In the setting of Lemma 22, suppose H = 1"
C,0.6092114445219818,"A eB for an unknown constant A. There is
an algorithm which returns W = ZRZ⊤with R ∈Sq×q and Z ∈Sd×q satisfying the guarantee in
(32) with probability ≥1 −δ in time"
C,0.6099092812281927,"O

Tmv(eB) ·
 1"
C,0.6106071179344034,α2 + τ 2 ∆2
C,0.6113049546406141,"
log
d δ 
."
C,0.6120027913468249,"Proof. We follow the steps of Lemma 22 exactly, except we instead let W′ ←−(1 + ϵ)
eB
F ′ where F ′"
C,0.6127006280530356,satisfies ∥eB∥F ≤F ′ ≤(1 + α
C,0.6133984647592463,"8 )∥eB∥F, again with probability ≥1 −δ"
C,0.6140963014654571,"2, using Lemma 21. Because
the computation in Lemma 21 is scale-invariant, the distribution of W′ remains unchanged from
Lemma 22, and all remaining steps are the same as before."
C,0.6147941381716678,We combine the developments of this section to prove Lemma 19.
C,0.6154919748778785,Proof of Lemma 19. We define H = C
C,0.6161898115840894,"T eB as in (31) (using eB defined in Corollary 2), and take the
step given by Lemma 22 and Corollary 3. Recall from Corollary 2 that ∥H∥op ≤(1 + ∆"
C,0.6168876482903001,"8C )C ≤2C,
so we can set τ = 2C in Lemma 22. Moreover, in the context of Corollary 3,"
C,0.6175854849965108,Tmv(eB) = O 
C,0.6182833217027216,(|I| + dr) · R r log C ∆ !
C,0.6189811584089323,"The runtime bound then follows from Corollary 3. Finally, the rank bound q follows from (32)."
C,0.619678995115143,"C.4
Vector step implementation
In this section, analogously to Appendix C.3, we discuss how to approximate the X block of ∇f(z)
with a vector h, and give implementation details for computing"
C,0.6203768318213538,"w ≈min
v∈X ⟨h, v⟩."
C,0.6210746685275645,"We crucially use the characterization in Fact 3, which implies this is a weighted matching problem."
C,0.6217725052337753,The main result of this section is the following.
C,0.6224703419399861,"Lemma 23 (X step). There is an algorithm which takes as input x ∈X, S ∈S given as a rank-r
factorization, R ≥∥1"
C,0.6231681786461968,η(D( x
C,0.6238660153524075,"v ) −S)∥op, ∆∈[0, C], and α, δ ∈(0, 1), and computes y ∈X satisfying"
C,0.6245638520586183,"⟨y, h⟩≤(1 −α)argminy⋆∈X ⟨y⋆, h⟩,
(34)"
C,0.625261688764829,"for h ∈RI with | ⟨h −∇xFprog-flat(x, S), x′⟩| ≤∆for all x′ ∈X,
(35)"
C,0.6259595254710397,with probability ≥1 −δ. The algorithm runs in time O 
C,0.6266573621772505,(|I| + dr) · √
C,0.6273551988834613,Rk2C2 log1.5( RdkC
C,0.6280530355896721,"δ∆)
∆2
+ |I|"
C,0.6287508722958828,"α log
 1 α ! ."
C,0.6294487090020935,"Approximating the gradient.
Recall that [∇xf(x, S)]ι = −1 + C"
C,0.6301465457083043,"T ⟨B, Eι⟩for all ι ∈I, where
again B = L −R and T = Tr(L) + Tr(R) as in Appendix C.3. To produce our approximation
h ≈∇xf(x, S), we use the following helper guarantee."
C,0.630844382414515,"Lemma 24 (Lemma 20, [38]). Let ϵ, ϵ′, δ ∈(0, 1), {Ai}i∈[n] ∈Sd×d, R ≥log 1"
C,0.6315422191207257,"ϵ′ , and let M ∈
Sd×d satisfy ∥M∥op ≤R. There is an algorithm which computes {Vi}i∈[n] ⊆R such that for all
i ∈[n],
|Vi −⟨Ai, exp(M)⟩| ≤ϵ ⟨|Ai|, exp(M)⟩+ ϵ′Tr(|Ai|)Tr exp(M)
with probability ≥1 −δ in time O "
C,0.6322400558269365,Tmv(M) · √
C,0.6329378925331473,R log1.5( Rnd
C,0.633635729239358,"δϵϵ′ )
ϵ2
+  X"
C,0.6343335659455688,"i∈[n]
Tmv(Ai) "
C,0.6350314026517795,"· log nd δ
ϵ2  ."
C,0.6357292393579902,"With Lemma 24 established, our gradient approximation follows straightforwardly,"
C,0.636427076064201,"Corollary 4. Let x ∈X, S ∈S satisfy ∥1"
C,0.6371249127704117,η(D( x
C,0.6378227494766224,"v ) −S)∥op ≤R, and let ∆≥0 and δ ∈(0, 1). We
can compute h ∈RI in time O "
C,0.6385205861828332,(|I| + Tmv(S)) · √
C,0.639218422889044,Rk2C2 log1.5( RdkC
C,0.6399162595952547,"δ∆)
∆2 ! ,"
C,0.6406140963014655,"such that with probability ≥1 −δ,"
C,0.6413119330076762,"| ⟨h −∇xf(x, S), x′⟩| ≤∆for all x′ ∈X."
C,0.6420097697138869,Proof. Define T as in Corollary 2 throughout the proof. We let h ∈RI satisfy hI = −1 + C(Vι−Uι)
C,0.6427076064200977,"T
for all ι ∈I, for V, U ∈RI to be defined, which we can compute within the allotted time. Since
∥x′∥1 ≤k for all x′ ∈X, it suffices to show that ∥h −∇xf(x, S)∥∞≤∆ 2k."
C,0.6434054431263084,"To this end, throughout this proof let M := 1"
C,0.6441032798325191,η(D( x
C,0.64480111653873,"v ) −S), and define"
C,0.6454989532449407,"V ⋆
ι := ⟨exp(M), Eι⟩, U ⋆
ι := ⟨exp(−M), Eι⟩, for all ι ∈I."
C,0.6461967899511515,"By our choice of hι, we have"
C,0.6468946266573622,"|[h −∇xf(x, S)]ι| = C"
C,0.6475924633635729,"T (|Vι −V ⋆
ι | + |Uι −U ⋆
ι |) ."
C,0.6482903000697837,"We take {Vι}ι∈I to be the approximations given by Lemma 24 with the set of matrices {Eι}ι∈I and
M, δ ←δ"
C,0.6489881367759944,"2, and ϵ, ϵ′ ←
∆
4kC ; we define {Uι}ι∈I identically, except we apply Lemma 24 with −M
instead of M. Note that these approximations yield"
C,0.6496859734822051,"C
T (|Vι −V ⋆
ι | + |Uι −U ⋆
ι |) ≤C"
C,0.650383810188416,T (ϵ + ϵ′)Tr(|Eι|) (Tr exp(M) + Tr exp(−M))
C,0.6510816468946267,"≤2C(ϵ + ϵ′) ≤∆ k ,"
C,0.6517794836008374,"as desired. The first inequality used the guarantee in Lemma 24 with Hölder’s inequality, and the
second used the definition of T = Tr exp(M) + Tr exp(−M) and Tr(|Eι|) = 2. Finally, the runtime
follows directly from the bound in Lemma 24."
C,0.6524773203070482,"Approximating the x step.
Given our estimate h from Corollary 4, our approximate linear opti-
mization oracle follows from a known result on approximating maximum weighted matchings."
C,0.6531751570132589,"Proposition 5 (Theorem 3.12, [22]). Let G = (L ∪R, E) be a bipartite graph, let w ∈RE be a set
of weights, and let m := |E|. There is an algorithm which computes f ∈M, defined in (23), such
that ⟨w, f⟩≤(1 −α) minf ⋆∈M ⟨w, f ⋆⟩, in time O( m"
C,0.6538729937194696,α log 1 α).
C,0.6545708304256804,"We remark that Proposition 5 is stated in [3] for maximization of linear objectives in nonnegative
weight vectors w, i.e. maximum weight matchings. However, it is straightforward to see that this
implies the same result for minimization of linear objectives in arbitrary cost vectors w ∈RE, by
first dropping any positive coordinates (which can only increase the objective ⟨w, x⟩when x is
coordinatewise nonnegative), and then negating the result of [22]. Because our access to our cost
vector w of interest is implicit, we make a simple observation which extends Proposition 5."
C,0.6552686671318911,"Corollary 5. In the setting of Proposition 5, let C ∈R≥0, and suppose w = 1"
C,0.6559665038381018,"Av for an unknown
constant A > 0, and an explicit vector v ∈Rn. The guarantees of Proposition 5 hold true for inputs
(w, G, M) if run on inputs (v, G, C · M) instead."
C,0.6566643405443127,Proof. This follows directly from scale-invariance of the guarantees in Proposition 5.
C,0.6573621772505234,We now use these developments to give a short proof of Lemma 23.
C,0.6580600139567341,Proof of Lemma 23. We define hι = −1 + C(Vι−Uι)
C,0.6587578506629449,"T
entrywise (using notation in Corollary 4), and
take the step given by Proposition 5 and Corollary 5. The runtime follows immediately from these
results."
C,0.6594556873691556,"C.5
Runtime analysis for subproblem solver
In this section, we put together the pieces and provide a runtime bound for approximately solving
(25). We first provide Lemma 25 which provides the bounds for implementing Line 5 for Z and then
use this to provide the main result for approximately solving, (25), Proposition 6."
C,0.6601535240753663,"Lemma 25. There is an algorithm that for input z = (x, S) ∈X × S where S is given as a
rank-r factorization, R ≥∥1"
C,0.6608513607815771,η(D( x
C,0.6615491974877878,"v ) −S)∥op, ∆∈[0, C], and δ ∈(0, 1) outputs ˆz ∈Z with"
C,0.6622470341939986,"ˆz = (y, W) ∈X × S where W is output as a rank O( C2"
C,0.6629448709002094,"∆2 ) factorization such that with probability
≥1 −δ,
⟨∇Fprog-flat(z), ˆz⟩≤min
z′∈Z ⟨∇SFprog-flat(x, S), z′⟩+ ∆"
C,0.6636427076064201,in time O 
C,0.6643405443126309,(|I| + dr) ·  R C2 ∆2 r log C
C,0.6650383810188416,"∆log
d δ 
+ √"
C,0.6657362177250523,Rk2C2 log1.5( RdkC
C,0.6664340544312631,"δ ¯∆)
∆2 !"
C,0.6671318911374738,"+ |I|

k + Cku ∆ℓ2d"
C,0.6678297278436846,"
log
 Cku ∆ℓ2d ! ."
C,0.6685275645498954,"Proof. Applying Lemma 19 and Lemma 23 yields that for any αS, αX ∈(0, 1] and ¯∆∈[0, C]
(which we set later) we can obtain with probability ≥1 −δ (by a union bound) W ∈S as a rank-q
factorization for q = O( C2"
C,0.6692254012561061,¯∆2 ) and H ∈Sd×d satisfying (Lemma 19)
C,0.6699232379623168,"⟨H, W⟩≤(1 −αS) min
S′∈S ⟨H, S′⟩+ ¯∆and |⟨H −∇SFprog-flat(x, S), z′⟩| ≤¯∆for all S′ ∈S ,"
C,0.6706210746685276,and y ∈X and h ∈RI satisfying (Lemma 23)
C,0.6713189113747383,"⟨h, y⟩≤(1 −αX ) min
x′∈X ⟨h, x′⟩and |⟨h −∇X Fprog-flat(x, S), x′⟩| ≤¯∆for all x′ ∈X ,"
C,0.672016748080949,in time O 
C,0.6727145847871598,(|I| + dr)  R r log C
C,0.6734124214933705,"¯∆·
 1"
C,0.6741102581995813,"α2
S
+ C2 ¯∆2"
C,0.6748080949057921,"
log
d δ 
+ √"
C,0.6755059316120028,Rk2C2 log1.5( RdkC
C,0.6762037683182135,"δ ¯∆)
¯∆2 ! + |I|"
C,0.6769016050244243,"αX
log
 1 αX ! ."
C,0.677599441730635,Conditioning on these events and applying Lemma 15 and Lemma 18 yields that
C,0.6782972784368457,"⟨∇SFprog-flat(x, S), W⟩≤min
x′∈X ⟨∇SFprog-flat(x, S), x′⟩+ αS · 2C + 3 ¯∆. and"
C,0.6789951151430565,"⟨∇X Fprog-flat(x, S), y⟩≤min
S′∈S ⟨∇X Fprog-flat(x, S), S′⟩+ αX ·

C ku"
C,0.6796929518492673,"ℓ2d + k

+ 3 ¯∆."
C,0.680390788555478,"Picking αS =
∆
8C , αX = min{ ∆"
C,0.6810886252616888,8 · (k + Cku
C,0.6817864619678995,"ℓ2d )−1, 1} and ¯∆= ∆"
C,0.6824842986741103,24 then yields the result.
C,0.683182135380321,We now prove a more quantitative variant of Proposition 2.
C,0.6838799720865317,"Proposition 6. Let ϵ ∈[0, C] and δ ∈(0, 1), and suppose |[D]ι| ∈[ℓ, u] for all ι ∈I. There is an
algorithm computing (x, S), an ϵ-approximate minimizer to (5) with probability ≥1 −δ, in time O "
C,0.6845778087927425,"T

|I| + C2T"
C,0.6852756454989533,"ϵ2
· d

·  Rη
C2 ϵ2 r log C"
C,0.685973482205164,"ϵ log
dT δ 
+ p"
C,0.6866713189113748,Rηk2C2 log1.5( RηdkC
C,0.6873691556175855,"δϵ
)
ϵ2 !!"
C,0.6880669923237962,"+O

T|I| · CηRη"
C,0.688764829030007,"ϵ
log
CηRη ϵ 
,"
C,0.6894626657362177,for Rη := η−1(2k + ku
C,0.6901605024424284,"ℓ2d) and T = ⌈
8CηR2
η
ϵ
⌉, and S is given as a rank-r = O( C2"
C,0.6908583391486393,ϵ2 · T) factorization.
C,0.69155617585485,Proof. Throughout the proof let f := Fprog-flat for simplicity. Our algorithm computes z(T ) =
C,0.6922540125610607,"ApproxFrankWolfe(f, Z, T,
2CηR2
η
Cf,Z ) (Algorithm 4), where we note that Cf,Z need not necessarily
be computed. By Lemma 18 we know that Cf,Z ≤2CηR2
η and consequently Proposition 4 implies
that so long as each iteration of Algorithm 4 is implemented to the desired accuracy,"
C,0.6929518492672715,"f(z(T )) −f(z⋆) ≤2Cf,Z"
C,0.6936496859734822,"T + 2 + 4CηR2
η
T + 2 ≤8CηR2
η
T + 2 ≤ϵ ."
C,0.6943475226796929,"To implement the method we apply Lemma 25 with (∆, δ) in the lemma set to (
CηR2
η
T +2 , δ"
C,0.6950453593859037,"T ). This
suffices to have the desired output with the desired probability by union bound."
C,0.6957431960921144,"It remains to bound the computational cost of this algorithm and the rank of the output. Note that
CηR2
η
T
= Ω(ϵ−1) and Rη ≥∥1"
C,0.6964410327983251,η(D( x′
C,0.697138869504536,"v ) −S′)∥op for all (x′, S′) ∈X × S (by Lemma 17, see e.g.,
(28)). Additionally, if we initialize the method with (0d, 0d×d) then we see that the rank of the S
component of each computed s(t) is O( C2"
C,0.6978367062107467,"ϵ2 ) and that each S component of each z(t) can be maintained
as a rank O(t · C2"
C,0.6985345429169574,"ϵ2 ) factorization. The computational cost is then dominated by the cost of each
iteration which, by Lemma 25 is bounded as claimed after adjusting by T."
C,0.6992323796231682,"D
Postprocessing"
C,0.6999302163293789,"In this section, we give the second main component of our overall matrix completion algorithm in
Algorithm 7. This piece takes as input two matrices M, M⋆∈Sd×d which are close on a submatrix
(where we only have noisy semi-random observations from M⋆). It then performs a sequence of
postprocessing steps to produce a matrix f
M such that f
M is close to M⋆on the full matrix, as
measured in the ℓ∞norm. For the first step of Algorithm 7, we simply reuse our earlier subroutine
Sparsify (Algorithm 1), whose guarantees we recall here for convenience."
C,0.7006280530355897,"Lemma 13. Let D ∈Sd×d, such that there is A ⊆U ⊆[d] with |U\A| ≤γd, ∥DA×A∥∞≤τ.
Let s ∈N and p ≥800"
C,0.7013258897418004,s log( d
C,0.7020237264480111,"δ ). Then with probability ≥1 −δ, Algorithm 1 returns U ′ ⊆U with
|U \ U ′| ≤4γd log d, and DU ′×U ′ has at most s entries with magnitudes ≥τ per row or column."
C,0.702721563154222,"In Appendix D.1, we prove a structural fact about bounded spanning subsets of low-rank matrices.
We use this to analyze our fixing step in Appendices D.2 and D.3, where we use regression problems
on a subset of columns passing an appropriate test to complete the matrix."
C,0.7034193998604327,"D.1
Localizing errors via spanning subsets
Before giving our next postprocessing steps, we prove a structural property, which can be stated
concisely in the language of volumetric spanners [5]. Following definitions in [7], Lemma 26 shows
any n-sized set in Rd admits a 2d-approximate ℓ1-volumetric spanner of size d + 1."
C,0.7041172365666434,"Lemma 26. Given {vi}i∈[n] ⊂Rd, there exists S ⊆[n] with |S| ≤d + 1 such that for all i ∈[n],"
C,0.7048150732728542,"vi =
X"
C,0.7055129099790649,"j∈S
ajvj"
C,0.7062107466852756,with coefficients satisfying P
C,0.7069085833914864,j∈S |aj| ≤2d.
C,0.7076064200976971,"Proof. Consider the largest volume simplex among the {vi}i∈[d], say given by {vi}i∈[d+1] without
loss of generality. We claim all the other vectors are contained in the simplex with d + 1 vertices,
X"
C,0.7083042568039079,"j∈[d+1]
vj −dvi, for all i ∈[d + 1]."
C,0.7090020935101187,"To see this, fix i ∈[d + 1], and let the (d −1)-dimensional plane spanned by the other d points
be P. If any other vj lies outside the two planes parallel to P at equal distances, such that one
contains vi, then including vj would produce a larger-volume simplex. Taking the intersections of
the halfspaces defined by these planes produces the simplex described above. From this, we have the
desired property by convexity of the ℓ1 norm, applied to the coefficients defining this simplex."
C,0.7096999302163294,"Using Lemma 26, we obtain the following corollary on boundedness of large index subsets."
C,0.7103977669225401,"Lemma 27. Let D ∈Sd×d be rank-r, and assume each row of D has at most s entries with magnitude
≥τ. There exists S ⊆[d] with |[d] \ S| ≤(r + 1)s such that ∥[D]S:∥∞, ∥[D]:S∥∞≤2rτ."
C,0.7110956036287509,"Proof. Let UΣU⊤be an SVD of D, and let {ui}i∈[d] ∈Rr be the rows of U. By Lemma 26, we
can find a subset T such that all rows i ∈[d] can be written as"
C,0.7117934403349616,"ui =
X"
C,0.7124912770411723,"j∈T
ajuj, with
X"
C,0.7131891137473831,"j∈T
|aj| ≤2r."
C,0.7138869504535938,"This also implies every row of D can be written as a linear combination of the row vectors {Dj:}j∈T ,
such that the ℓ1 norm of the linear combination coefficients is at most 2r. Now, let S = [d]\R, where
R consists of all indices i ∈[d] such that |[D]ji| ≥τ for some j ∈T; by assumption, |R| ≤(r +1)s,
giving the size bound. Take some row Di: for i ∈[d], and let Di,S be the restriction of this row to
columns in S. By applying the given linear combination to the rows {Dj,S}j∈T , all entrywise at
most τ in magnitude, the triangle inequality shows ∥Di,S∥∞≤2rτ as claimed."
C,0.7145847871598046,"D.2
Testing columns
In this section, we present the second step of our postprocessing algorithm, where we design a test to
identify columns with no large errors. This step uses our structural claim in Lemma 27."
C,0.7152826238660154,"Algorithm 5: Test(M, j, ϕ, q, δ)"
C,0.7159804605722261,"1 Input: M ∈Rd×d, j ∈[d], ϕ ≥0, q, δ ∈(0, 1)"
C,0.7166782972784368,2 tmax ←⌈10 log 1 δ ⌉
C,0.7173761339846476,3 c ←0
C,0.7180739706908583,4 for t ∈[tmax] do
C,0.7187718073970691,"5
Sample T ⊆[d] by including each element independently with probability q"
C,0.7194696441032798,"6
if minv∈R|T | ∥M:T v −M:j∥∞+ ϕ ∥v∥∞≤2ϕ then c ←c + 1"
END,0.7201674808094906,7 end
END,0.7208653175157014,8 if c ≥1
END,0.7215631542219121,2tmax then Return: true
END,0.7222609909281228,9 else Return: false 10
END,0.7229588276343336,"To analyze Algorithm 5, we need the following basic claims which are standard in the literature."
END,0.7236566643405443,"Lemma 28. Let rank-r M ∈Rd×k be µ-incoherent with d ≥k, and let x ∈span(M). Then for all
i ∈[d], |xi| ≤( µr"
END,0.724354501046755,d )1/2 ∥x∥2.
END,0.7250523377529658,"Proof. Let U ∈Rd×r be an orthonormal basis for span(M) with rows {ui}i∈[d], so that x = Uv
with ∥x∥2 = ∥v∥2. The conclusion follows from"
END,0.7257501744591766,"|xi| = | ⟨ui, v⟩| ≤∥ui∥2 ∥x∥2 ≤
rµr"
END,0.7264480111653873,d ∥x∥2 .
END,0.7271458478715981,"Lemma 29. Let V ⊆Rd be a µ-incoherent subspace of dimension r, and let BV ∈Rd×r be an
orthonormal basis for V . Let S ⊆[d] have |S| ≥(1 −
1
3µr)d. Then the following properties hold."
END,0.7278436845778088,"• For any v ∈V , ∥vS∥2 ≥
q"
END,0.7285415212840195,"2
3 ∥v∥2."
END,0.7292393579902303,"• span([BV ]S:), as a subspace of R|S|, is 3µ-incoherent."
END,0.729937194696441,"Proof. Let A := [BV ]⊤
S:[BV ]S: = P"
END,0.7306350314026517,"i∈S bib⊤
i , where {bi}i∈[d] are rows of BV . By the definition
of incoherence, we have that ∥Ir −A∥op ≤Tr(Ir −A) = P"
END,0.7313328681088626,"i∈[d]\S Tr(bib⊤
i ) ≤1"
END,0.7320307048150733,"3, so that"
END,0.732728541521284,Ir ⪰A ⪰2 3Ir.
END,0.7334263782274948,"The lower bound then yields the first statement. Now, the matrix M := [BV ]S:A−1"
HAS ORTHONORMAL,0.7341242149337055,"2 has orthonormal
columns, and the same span as [BV ]S:. Furthermore, by incoherence and the fact that A ⪰2"
HAS ORTHONORMAL,0.7348220516399162,"3Ir, all
of the rows i ∈S of M have"
HAS ORTHONORMAL,0.735519888346127,∥Mi:∥2 ≤ r
HAS ORTHONORMAL,0.7362177250523377,"3
2 ∥[BV ]i:∥2 ≤ r 3µr 2d ."
HAS ORTHONORMAL,0.7369155617585486,"Finally, |S| ≥2d"
HAS ORTHONORMAL,0.7376133984647593,3 (since µ ≥1). Combining these claims proves the second statement.
HAS ORTHONORMAL,0.73831123517097,We also include the following application of the matrix Bernstein inequality from [43].
HAS ORTHONORMAL,0.7390090718771808,"Lemma 30 (Lemma 12, [43]). Let δ ∈(0, 1) and let V ⊆Rd be a µ-incoherent subspace of
dimension r. Let BV be an orthononormal basis for V , with rows {bi}i∈[d] ⊂Rr. Let S ⊆[d]
have |S| ≥(1 −
1
3µr)d. Let T ⊆S include each element independently with probability p ≥
100µr"
HAS ORTHONORMAL,0.7397069085833915,"d
log
  2r"
HAS ORTHONORMAL,0.7404047452896022,"δ

. Then with probability ≥1 −δ,"
HAS ORTHONORMAL,0.741102581995813,"p
2Ir ⪯
X"
HAS ORTHONORMAL,0.7418004187020237,"i∈T
bib⊤
i ⪯2pIr."
HAS ORTHONORMAL,0.7424982554082344,"Now, we are ready to analyze our testing algorithm and prove that it has the desired behavior."
HAS ORTHONORMAL,0.7431960921144453,"Lemma 31. Let M⋆∈Sd×d be rank-r⋆and µ-incoherent, and let M ∈Sd×d be rank-r for r ≥r⋆.
For s, τ > 0, assume M −M⋆has at most s entries with magnitudes ≥τ in each row. Then with
probability 1 −δ, if we run Algorithm 5 with parameters q, ϕ and j ∈[d] such that"
HAS ORTHONORMAL,0.743893928820656,q ≥1000µr⋆
HAS ORTHONORMAL,0.7445917655268667,"d
log
d δ"
HAS ORTHONORMAL,0.7452896022330775,"
, q ≤
1
20µsr⋆, ϕ =
p"
HAS ORTHONORMAL,0.7459874389392882,"qdr⋆τ,"
HAS ORTHONORMAL,0.7466852756454989,Algorithm 5 has the following behavior.
HAS ORTHONORMAL,0.7473831123517097,"• If ∥M⋆
:j −M:j∥∞≤4r⋆τ√µr⋆, Algorithm 5 returns true."
HAS ORTHONORMAL,0.7480809490579204,"• If ∥M⋆
:j −M:j∥∞≥10τqdµr⋆, Algorithm 5 returns false."
HAS ORTHONORMAL,0.7487787857641311,Proof. Let the SVD of M⋆be M⋆= U⋆Σ⋆(U⋆)⊤. We consider the two claims separately.
HAS ORTHONORMAL,0.749476622470342,"True case. Suppose that ∥M⋆
:j −M:j∥∞≤4r⋆τ√µr⋆. We will prove that each run of the inner
loop of the algorithm succeeds with probability ≥4"
HAS ORTHONORMAL,0.7501744591765527,"5. By using Lemma 27 with D ←M⋆−M, and
applying our upper and lower bounds on q, there exists S ⊆[d] with |S| ≥(1 −
1
3µr⋆)d such that for
all i ∈S, ∥M⋆
:i −M:i∥∞≤4r⋆τ. Next, by Lemma 30 and incoherence of M⋆, with probability
9
10,"
HAS ORTHONORMAL,0.7508722958827634,"(V⋆
:T ∩S)⊤V⋆
:T ∩S ⪰2 3qIr."
HAS ORTHONORMAL,0.7515701325889742,"This means there exists v ∈RT ∩S with ∥v∥2 ≤
q 3µr⋆"
HAS ORTHONORMAL,0.7522679692951849,"2qd , such that (V⋆
:T ∩S)⊤v = (V⋆
:j)⊤, which
also means M⋆
:T ∩Sv = M⋆
:j. We thus have for this vector v,"
HAS ORTHONORMAL,0.7529658060013956,"∥M:T ∩Sv −M:j∥∞+ ϕ ∥v∥∞≤∥M⋆
:T ∩Sv −M:T ∩Sv∥∞+
M⋆
:j −M:j

∞+ ϕ ∥v∥∞
≤4r⋆τ ∥v∥1 + 4r⋆τ√µr + ϕ ∥v∥∞
≤4r⋆τ ∥v∥2
p"
HAS ORTHONORMAL,0.7536636427076064,"|T| + 6r⋆τ√µr⋆≤20rτ√µr⋆≤2ϕ, (36)"
HAS ORTHONORMAL,0.7543614794138171,"where we used that with probability
9
10, |T| ≤2qd. By a union bound, each independent test thus
increments c with probability 4"
HAS ORTHONORMAL,0.755059316120028,"5 over the randomness of T, so a Chernoff bound gives the claim."
HAS ORTHONORMAL,0.7557571528262387,"False case. Next, suppose that ∥M⋆
:j −M:j∥∞≥10τqdµr⋆, and again consider a single run of the
algorithm. Let i0 ∈[d] be the entry such that |M⋆
i0j −Mi0j| ≥10τqdµr⋆. First, with probability at
least
9
10 over the randomness of the inner loop, for all j′ ∈T,
M⋆
i0j′ −Mi0j′ ≤τ,
(37)"
HAS ORTHONORMAL,0.7564549895324494,"because by assumption, there are at most s indices j′ where |M⋆
i0,j′ −Mi0,j′| ≥τ. Since qs ≤
1
20,
there is at least a
9
10 probability that none of these indices is selected in T by a Chernoff bound. As"
HAS ORTHONORMAL,0.7571528262386602,"before, with probability
9
10, there is a vector v with ∥v∥2 ≤
q 3µr⋆"
HAS ORTHONORMAL,0.7578506629448709,"2qd such that M⋆
:T v = M⋆
:j. Also,"
HAS ORTHONORMAL,0.7585484996510816,"again, with probability
9
10, we have |T| < 2qd. Now let R ⊆[d] be the set of rows i such that
[M⋆]i,T ∪{j} −[M]i,T ∪{j}

∞≤τ ."
HAS ORTHONORMAL,0.7592463363572924,"By assumption, each row in T ∪{j} can only remove s indices from R, so"
HAS ORTHONORMAL,0.7599441730635031,"|R| ≥d −s(|T| + 1) ≥d −2qsd ≥

1 −
1
10µr⋆ 
d."
HAS ORTHONORMAL,0.7606420097697139,"Consider any solution v′ to the problem in Line 6 which increments c. For v′ to be feasible, we must
have ∥v′∥∞≤2, so ∥v′∥1 ≤2|T|. Further,"
HAS ORTHONORMAL,0.7613398464759247,"∥M:T v′ −M:j∥∞≤2r⋆τ
p"
HAS ORTHONORMAL,0.7620376831821354,"qd.
(38)"
HAS ORTHONORMAL,0.7627355198883461,"Hence, for all i ∈R,
[M⋆]i,T (v −v′)
 ≤
M⋆
ij −Mij
 +
([M]i,T −[M⋆]i,T )v′ +
[M]i,T v′ −Mij"
HAS ORTHONORMAL,0.7634333565945569,"≤τ + ∥v′∥1 τ + 2r⋆τ
p"
HAS ORTHONORMAL,0.7641311933007676,"qd ≤3τqd
√"
HAS ORTHONORMAL,0.7648290300069783,"r⋆.
(39)"
HAS ORTHONORMAL,0.7655268667131891,"In the first line, we used that M⋆
:T v = M⋆
:j by definition. Also, note that M⋆
:T (v −v′) is in the span
of M⋆, so by Lemma 29, the fact that |R| ≥(1 −
1
10µr⋆)d, and (39),"
HAS ORTHONORMAL,0.7662247034193999,"∥M⋆
:T (v −v′)∥2 ≤ r 3
2"
HAS ORTHONORMAL,0.7669225401256106,"M⋆
R×T (v −v′)

2 ≤4τqd
√ r⋆d."
HAS ORTHONORMAL,0.7676203768318214,"Therefore, by incoherence of M⋆, Lemma 28 implies that for all i ∈[d],
[M⋆]i,T (v −v′)
 ≤4τqdµr⋆.
(40)"
HAS ORTHONORMAL,0.7683182135380321,"However, consider the index i0 defined at the beginning of this case. Similarly to (39),
M⋆
i0j −Mi0j
 ≤
[M]i0,T v′ −Mi0j
 +


[M]i0,T −[M⋆]i0,T

v′ +
[M⋆]i0,T (v −v′) ≤2
p"
HAS ORTHONORMAL,0.7690160502442428,"qdr⋆τ + τ · 2|T| + 4τqdµr⋆< 10τqdµr⋆,"
HAS ORTHONORMAL,0.7697138869504536,"where we used (37), (38), (40), and |T| ≤2qd. This contradicts the assumption that
M⋆
i0j −Mi0j
 ≥10τqdµr⋆."
HAS ORTHONORMAL,0.7704117236566643,We conclude that the problem on Line 6 fails to increment c with probability ≥2
HAS ORTHONORMAL,0.771109560362875,"3, by a union bound.
Since we repeat the loop 10 log( 1"
HAS ORTHONORMAL,0.7718073970690859,"δ ) times, with 1 −δ probability the test will reject, as claimed."
HAS ORTHONORMAL,0.7725052337752966,"Remark 2. From observation, the proof of Lemma 31 continues to hold if the problem on Line 6 is
solved up to accuracy 0.1ϕ. That is, the true case actually proves a stronger bound of 1.9ϕ in (36),
so it would continue to pass for any error ≤0.1ϕ. Similarly, even if we assumed the vector v in the
false case achieved a value of ≤2.1ϕ (instead of 2ϕ), we would still have a contradiction."
HAS ORTHONORMAL,0.7732030704815074,"Finally, we include a claim from prior work which bounds the complexity of implementing Line 6."
HAS ORTHONORMAL,0.7739009071877181,"Lemma 32. Following notation in Lemma 31, for any δ ∈(0, 1), assuming q = O( µr⋆"
HAS ORTHONORMAL,0.7745987438939288,d log( d
HAS ORTHONORMAL,0.7752965806001396,"δ )), the
optimization problem on Line 6 can be solved to accuracy 0.1ϕ with probability ≥1 −δ, in time"
HAS ORTHONORMAL,0.7759944173063503,"O

d · poly

µr⋆log
d δ 
."
HAS ORTHONORMAL,0.776692254012561,"Proof. For shorthand let M := M:T , k := |T|, and u := M:j in this proof, and assume that k ≤2qd,
which occurs with probability ≥1 −δ"
HAS ORTHONORMAL,0.7773900907187719,"2. We create a linear program,"
HAS ORTHONORMAL,0.7780879274249826,"min
x∈Rk+2Ax≤b c⊤x,"
HAS ORTHONORMAL,0.7787857641311933,"where x⊤=
 
v⊤
α
β

for α, β ∈R and v ∈Rk. We encode the constraints ∥Mv −u∥∞≤α,
∥v∥∞≤β using 2d + 2k linear constraints on x. Finally, we make the objective c⊤x = α + ϕβ.
The conclusion follows by applying the solver of [69], Theorem 1, with failure probability δ"
HAS ORTHONORMAL,0.7794836008374041,"2, since
k = O(µr⋆log( d"
HAS ORTHONORMAL,0.7801814375436148,δ )) and we have a O(d + k) × O(k) constraint matrix.
HAS ORTHONORMAL,0.7808792742498255,"Algorithm 6: Fix(Osr
p (c
M), M, T, ϕ, q, δ)"
HAS ORTHONORMAL,0.7815771109560363,"1 Input: Osr
p (c
M), M ∈Sd×d, T ⊆[d], ϕ ≥0, q, δ ∈(0, 1)"
HAS ORTHONORMAL,0.782274947662247,2 Sample B ⊆T by including each element independently with probability q
HAS ORTHONORMAL,0.7829727843684577,3 G ←∅
HAS ORTHONORMAL,0.7836706210746686,4 for b ∈B do
HAS ORTHONORMAL,0.7843684577808793,"5
if Test(M:T , b, ϕ, q,
δ
10d2 ) then G ←G ∪{b}"
END,0.78506629448709,6 end
END,0.7857641311933008,7 for j ∈[d] do
END,0.7864619678995115,"8
Sj ←indices in [d] of observed entries in our call to Osr
p (c
M)"
END,0.7871598046057222,"9
vj ←argminv∈R|G|ϕ√qd ∥v∥∞+ ∥MSj×Gv −c
MSj,j∥∞"
END,0.787857641311933,10 end
END,0.7885554780181437,"11 eV ←vertical concatenation of {v⊤
j }j∈[d]"
RETURN F,0.7892533147243544,"12 return f
M = M:G eV⊤"
RETURN F,0.7899511514305653,"D.3
Fixing columns via regression
We now complete our postprocessing algorithm. Given an estimate M that is entrywise close to M⋆,
except for a small number of potentially large errors in each row and column, we first run Algorithm 5
to identify a set of columns containing no large errors. We then set up regression problems to fix the
rest of the matrix (which may initially contain large errors on some entries)."
RETURN F,0.790648988136776,"Lemma 33. Let M⋆∈Sd×d be rank-r⋆and µ-incoherent, and let c
M = M⋆+ N for N ∈Sd×d
with ∥N∥∞≤τ. Let M ∈Sd×d be rank-r for r ≥r⋆. For s, τ > 0, assume [M −M⋆]:T has at
most s entries larger than τ in each row and column. Assume"
RETURN F,0.7913468248429868,"s ≤
d
20µr2 , |T| ≥

1 −
1
10µr"
RETURN F,0.7920446615491975,"
d, p ≥100µr"
RETURN F,0.7927424982554082,"d
log
d δ 
."
RETURN F,0.793440334961619,"Then with probability 1 −δ, if we run Algorithm 6 with parameters q, ϕ such that"
RETURN F,0.7941381716678297,q ≥1000µr
RETURN F,0.7948360083740404,"d
log
d δ"
RETURN F,0.7955338450802513,"
, q ≤
1
20µsr, ϕ =
p qdrτ,"
RETURN F,0.796231681786462,"then the output satisfies
f
M −M⋆
∞≤104q2d2(µr)3τ."
RETURN F,0.7969295184926727,"Proof. First, by Lemma 27 and applying our upper and lower bounds on q, there is a subset T ′ ⊆T
with |T ′| ≥(1 −
1
3µr)d such that for all j ∈T ′, ∥M:j −M⋆
:j∥∞≤4rτ. Next, by Lemma 31, with
probability 1 −δ"
RETURN F,0.7976273551988835,"10, we have B ∩T ′ ⊆G. Thus, by applying Lemma 30 with S ←T ′ and p ←q, we"
RETURN F,0.7983251919050942,have that with 1 −δ
RETURN F,0.7990230286113049,"10 probability, for any j ∈[d], there is a vector uj with ∥uj∥2 ≤
q"
RETURN F,0.7997208653175157,"3µr
2qd such that"
RETURN F,0.8004187020237264,"M⋆
:Guj = M⋆
:j. Also, by Lemma 31, we have with probability 1 −δ"
THAT,0.8011165387299372,10 that
THAT,0.801814375436148,"∥M⋆
:G −M:G∥∞≤10τqdµr."
THAT,0.8025122121423587,"Now fix a j ∈[d] and consider the execution of the loop in Lines 7 to 10 for this index j. Consider
uj as a candidate solution to the optimization problem. We have ϕ
p"
THAT,0.8032100488485694,"qd ∥uj∥∞+
MSj×Guj −c
MSj,j

∞≤2
p"
THAT,0.8039078855547802,"qdµr1.5τ +
MSj×Guj −M⋆
Sj×Guj

∞"
THAT,0.8046057222609909,"+
M⋆
Sj,j −c
MSj,j

∞
≤2
p"
THAT,0.8053035589672016,"qdµr1.5τ +
p"
THAT,0.8060013956734124,|G| ∥uj∥2 10τqdµr + τ
THAT,0.8066992323796232,"≤100τqd(µr)1.5,"
THAT,0.8073970690858339,where we use that with 1 −δ
THAT,0.8080949057920447,"10 probability, |G| ≤2qd. Thus, the solution vj in Line 9 must have
MSj×Gvj −c
MSj,j

∞≤100τqd(µr)1.5, ∥vj∥∞≤100µ1.5√r.
(41)"
THAT,0.8087927424982554,"This implies that
M⋆
Sj×Gvj −M⋆
Sj,j

∞≤
M⋆
Sj×Gvj −MSj×Gvj

∞+
MSj×Gvj −c
MSj,j

∞"
THAT,0.8094905792044662,"+
c
MSj,j −M⋆
Sj,j

∞
≤|G| ∥vj∥∞10τqdµr + 100τqd(µr)1.5 + τ"
THAT,0.8101884159106769,≤2500q2d2(µr)2.5τ.
THAT,0.8108862526168876,"Next, recall that span(M⋆) is µ-incoherent.
Let B ∈Rd×r have orthonormal columns with
span(B) = span(M⋆), with rows {bi}i∈[d]. By Lemma 30, with probability 1 −
δ
10d2 , Sj con-
tains a subset S′
j such that |S′
j| ≤2pd and X"
THAT,0.8115840893230984,"i∈S′
j
bib⊤
i ⪰p"
THAT,0.8122819260293092,"2Ir ⪰|S′
j|
4d Ir."
THAT,0.8129797627355199,"Thus, for all vectors z ∈span(M⋆), ∥z∥2 ≤
q"
D,0.8136775994417307,"4d
|S′
j|∥zS′
j:∥2. Using µ-incoherence of span(M⋆)"
D,0.8143754361479414,"again,"
D,0.8150732728541521,"M⋆
:Gvj −M⋆
:j

∞≤
M⋆
:Gvj −M⋆
:j

2 rµr d"
D,0.8157711095603629,"≤
M⋆
S′
j:Gvj −M⋆
S′
j:j

2 s"
D,0.8164689462665736,"4µr
|S′
j|"
D,0.8171667829727843,"≤
M⋆
S′
j:Gvj −M⋆
S′
j:j

∞ p 4µr"
D,0.8178646196789952,"≤5000q2d2(µr)3τ,"
D,0.8185624563852059,"where the first line used Lemma 28. Finally, recall from (41) that for vj to be an optimal solution, we
must have ∥vj∥∞≤100µ1.5√r. Since |G| ≤2qd, we have the desired bound for column j ∈[d]:
M:Gvj −M⋆
:j

∞≤∥M:Gvj −M⋆
:Gvj∥∞+
M⋆
:Gvj −M⋆
:j

∞
≤|G| ∥vj∥∞∥M:G −M⋆
:G∥∞+ 5000q2d2(µr)3τ"
D,0.8192602930914166,≤104q2d2(µr)3τ.
D,0.8199581297976274,Union bounding all of the failure probabilities then completes the proof.
D,0.8206559665038381,"Remark 3. Similarly to Remark 2, we note that the proof of Lemma 33 is tolerant to an error of
τqd(µr)1.5 for the problem in Line 9, because this implies (41) holds up to a negligible constant
factor, which does not affect correctness of the rest of the proof. Line 9 can again be solved to this
accuracy in time O(d · poly(µr log( d"
D,0.8213538032100488,"δ ))), with failure probability δ, as described in Lemma 32."
D,0.8220516399162596,"Finally, we put everything together and give our full postprocessing procedure."
D,0.8227494766224703,"Proposition 3. Let M⋆∈Sd×d be rank-r⋆and µ-incoherent, and let c
M = M⋆+ N for N ∈Sd×d
with ∥N∥∞≤∆for some ∆≥0. Let M ∈Sd×d be given as a rank-r decomposition, with r ≥r⋆.
Further, for γ ∈(0, 1) suppose M and M⋆are d∆-close on a γ-submatrix. Finally, assume"
D,0.823447313328681,"γ ≤
1
104µr log(d)."
D,0.8241451500348919,"Then for any δ ∈(0, 1) if p ≥1"
D,0.8248429867411026,d · poly(µr log( d
D,0.8255408234473133,"δ )) for an appropriate polynomial, Algorithm 7 uses
one call to Op(c
M) and with probability ≥1 −δ, outputs f
M ∈Sd×d such that
f
M −M⋆
∞≤poly

µr log
d δ"
D,0.8262386601535241,"
∆.
(6)"
D,0.8269364968597348,"Also, f
M is given as a rank-poly(µr log( d"
D,0.8276343335659456,δ )) factorization. The algorithm runs in m · poly(µr log( d
D,0.8283321702721563,"δ ))
time, where m is the number of observed entries upon calling Osr
p (c
M)."
D,0.829030006978367,"Proof. Let τ = 1000∆µr1.5 log d. By applying Lemma 6, [43], and the assumed bound on γ, there
is A′ ⊆[d] with |[d] \ A′| ≤
d
300µr log d such that
[M −c
M]A′×A′

∞≤∥[M −M⋆]A′×A′∥∞+ ∆≤τ."
D,0.8297278436845779,Now we run Algorithm 1 with
D,0.8304256803907886,p ←108µ2r2 log2( d
D,0.8311235170969993,"δ )
d
, s ←
d
105µ2r2 log d."
D,0.8318213538032101,"These satisfy the conditions in Lemma 13, so with probability 1 −
δ
10, we obtain S ⊆[d] with
|S| ≥d(1 −
1
300µr), and [M −c
M]S×S has ≤s entries with magnitude larger than τ in each row or
column, implying [M −M⋆]S×S has ≤s entries with magnitude larger than 2τ per row or column."
D,0.8325191905094208,"Next, we run Algorithm 6 on the submatrix with rows indexed by S, i.e. we use the observations
from c
MS: and input MS:. The subset that is input to Algorithm 6 will be T ←S, and we use
q = 3000µr"
D,0.8332170272156315,"d
log( d"
D,0.8339148639218423,"δ ). Note that we have q ≤
1
20µsr. Also by construction, |S| ≥d(1 −
1
300µr) and by"
D,0.834612700628053,"Lemma 29, M⋆
S: is 3µ-incoherent. Thus, Lemma 33 yields, with probability 1 −δ"
D,0.8353105373342637,"10, f
M such that
[f
M −M⋆]S:

∞≤104q2d2(µr)3τ ≤1014(µr)6.5 log2
d δ 
∆."
D,0.8360083740404746,"Finally, we transpose the matrix and then run Algorithm 6 again to complete the whole matrix, i.e.
with inputs M ←f
M⊤and T ←S. Overall, we compute rank-poly(µr log( d"
D,0.8367062107466853,"δ )) f
M such that
f
M −M⋆
∞≤poly

µr log
d δ 
∆."
D,0.837404047452896,"Finally, by convexity of norms, we can symmetrize f
M ←1"
D,0.8381018841591068,"2(f
M + f
M⊤) and this will only improve
the bound. Combining over all of the subroutines, the sample complexity and runtime are as claimed,
by appropriately applying the linear program solvers as described in Lemma 32 and Remark 3."
D,0.8387997208653175,We provide pseudocode for the procedure in Proposition 3 in Algorithm 7.
D,0.8394975575715282,"Algorithm 7: Postprocess(Osr
[0,1](c
M), M, γ, ∆, r, r⋆, µ, δ)"
D,0.840195394277739,"1 Input: Osr
[0,1](c
M) where c
M = M⋆+ N ∈Sd×d where M⋆is rank-r⋆and µ-incoherent and
∥N∥∞≤∆, M ∈Sd×d given as a rank-r factorization such that M, M⋆are d∆-close on a
γ-submatrix, δ ∈(0, 1)"
D,0.8408932309839497,"2 S ←Sparsify(Osr
[0,1](c
M −M), 1000∆µr1.5 log d,
1
300µr log d, 108µ2r2 log2( d"
D,0.8415910676901605,"δ )
d
, δ 10)"
D,0.8422889043963713,3 p ←100µr
D,0.842986741102582,"d
log( d"
D,0.8436845778087927,"δ ), q ←3000µr"
D,0.8443824145150035,"d
log( d"
D,0.8450802512212142,"δ ), τ ←1000∆µr1.5 log d"
F,0.845778087927425,"4 f
M ←Fix(Osr
p (c
MS:), MS:, S, √qdrτ, q, δ 10)"
F,0.8464759246336357,"5 r ←rank(f
M)"
F,0.8471737613398465,6 p ←100µr
F,0.8478715980460573,"d
log( d"
F,0.848569434752268,"δ ), q ←3000µr"
F,0.8492672714584787,"d
log( d"
F,0.8499651081646895,"δ ), τ ←1014(µr)6.5 log2( d δ )∆"
F,0.8506629448709002,"7 f
M ←Fix(Osr
p (c
M), f
M⊤, S, √qdrτ, q, δ 10)"
F,0.8513607815771109,8 Return: 1
F,0.8520586182833217,"2(f
M + f
M⊤)"
F,0.8527564549895325,"E
Semi-random matrix completion"
F,0.8534542916957432,"In this section, we finally combine the two main components of our algorithm, Algorithm 3 and
Algorithm 7, to give our final result, Theorem 2. We begin with a preprocessing step to estimate an
initial distance bound, which follows straightforwardly from an analogous result in [43]."
F,0.854152128401954,"Lemma 34. There is an algorithm, EstimateFrobNorm, which takes as input Osr
p (c
M) and parameters
∆≥0 and δ ∈(0, 1) where c
M = M⋆+ N where M⋆∈Sd×d is rank-r⋆and µ-incoherent and"
F,0.8548499651081647,∥N∥∞≤∆and p ≥120µr⋆log(d/δ)
F,0.8555478018143754,"d
. EstimateFrobNorm runs in time O(m) where m is the number
of observed entries and outputs an estimate V ∈R such that with probability ≥1 −δ,"
F,0.8562456385205862,∥M⋆∥F ≤V ≤10d2(∥M⋆∥F + ∆).
F,0.8569434752267969,"Proof. Let Ω⊆[d] × [d] be the set of observed entries. Our estimator for V will be V =
s"
"D
X",0.8576413119330076,"2d
X"
"D
X",0.8583391486392185,"(i,j)∈Ω
(|c
Mij| + ∆)2 ."
"D
X",0.8590369853454292,"Now we analyze this estimator. For each row i ∈[d], by Lemma 30, with probability at least 1 −δ"
"D
X",0.8597348220516399,"d,
if we let Si be the set of observed entries in row i,
X"
"D
X",0.8604326587578507,"j∈Si
(M⋆
ij)2 ≥p 2 X"
"D
X",0.8611304954640614,"j∈[d]
(M⋆
ij)2 ."
"D
X",0.8618283321702721,"Summing over all rows, we have by the triangle inequality |M⋆
ij| ≤|c
Mij| + ∆that"
"D
X",0.8625261688764829,"V 2 ≥dp ∥M⋆∥2
F ≥∥M⋆∥2
F"
"D
X",0.8632240055826936,"which proves one side of the claim. To prove the other direction, by the triangle inequality, V ≤
√"
D,0.8639218422889045,"2d

∥c
M∥F + d∆

≤10d2(∥M⋆∥F + ∆) ."
D,0.8646196789951152,"Now, we present the full algorithm for semi-random matrix completion."
D,0.8653175157013259,"Algorithm 8: SRCompletion(Osr
[0,1](c
M), ∆, r⋆, µ, δ)"
D,0.8660153524075367,"1 Input: Osr
[0,1](c
M) where c
M = M⋆+ N ∈Sd×d where M⋆is rank-r⋆and µ-incoherent and
∥N∥∞≤∆, δ ∈(0, 1)"
D,0.8667131891137474,2 M ←0d×d
D,0.8674110258199581,3 p0 ←200µr⋆log(d/δ) d
D,0.8681088625261689,"4 D ←EstimateFrobNorm(Osr
p0(c
M), δ 2, ∆)"
D,0.8688066992323796,"5 P ←poly
 
r⋆, µ, log
  dD δ∆
"
D,0.8695045359385903,6 Q ←poly(P)
D,0.8702023726448012,7 p ←Q
D,0.8709002093510119,"d
8 K ←⌈log D ∆⌉"
D,0.8715980460572226,9 while D ≥∆dQ do
D,0.8722958827634334,"10
M ←SRPartialCompletion

Osr
[0,1](M⋆+ N), M, D, r⋆, P,
1
106µP 2 ,
δ
4K
"
F,0.8729937194696441,"11
f
M ←Postprocess

Osr
[0,1](M⋆+ N), M,
1
106µP 2 , D"
F,0.8736915561758548,"dP , P o(1), r⋆, µ,
δ
4K
"
F,0.8743893928820656,"12
M ←PCAr⋆(f
M)"
F,0.8750872295882763,"13
D ←0.1D"
END,0.875785066294487,14 end
END,0.8764829030006979,"15 Return: f
M"
END,0.8771807397069086,"Remark 4. In Line 12 of Algorithm 8, PCAr⋆(·) denotes computing the top-r⋆PCA of a matrix. We
ensure that the input f
M is a rank r = poly(r⋆, µ, log( dD"
END,0.8778785764131193,"δ∆)) matrix given as a low-rank factorization,
and then this step can be implemented in time d · poly(r)."
END,0.8785764131193301,Now we prove our main theorem.
END,0.8792742498255408,"Theorem 2. Let M⋆∈Sd×d be rank-r⋆and µ-incoherent, let N ∈Sd×d have ∥N∥∞≤∆, and let
δ ∈(0, 1). Let Q = poly(r⋆, µ, log( d∥M⋆∥F"
END,0.8799720865317515,"δ∆
)) for a sufficiently large polynomial. Algorithm 8 can"
END,0.8806699232379623,"be implemented with one call to Osr
p (M⋆+ N) for p = Q"
END,0.881367759944173,"d and outputs f
M ∈Sd×d with rank at most
Q (represented in terms of its low-rank factorization) such that with probability ≥1 −δ,
f
M −M

∞≤Q∆."
END,0.8820655966503839,The algorithm runs in time O(mQ) where m is the total number of observed entries.
END,0.8827634333565946,"Proof. We
inductively
verify
the
conditions
of
Corollary
1
are
satisfied
whenever
SRPartialCompletion is called and the conditions of Proposition 3 are satisfied whenever
Postprocess is called. The base case for SRPartialCompletion is clear from the guarantees of
Lemma 34. Now assuming that the hypotheses of Corollary 1 are satisfied when SRPartialCompletion
is called, we know the output M has rank r ≤r⋆·P o(1) and is D"
END,0.8834612700628053,"α -close to M⋆on a
1
106µP 2 submatrix.
Now this implies that the hypotheses of Proposition 3 are satisfied when we call Postprocess and
thus we get that
f
M −M⋆
∞≤poly(r⋆)P o(1) D"
END,0.8841591067690161,"dP ≤
D
100d ."
END,0.8848569434752268,"Since M⋆is rank-r⋆, we have"
END,0.8855547801814375,"∥f
M −M⋆∥F ≤2∥PCAr⋆(f
M) −M⋆∥F ≤0.1D ."
END,0.8862526168876483,"This now implies that the hypotheses of Corollary 1 are satisfied at the beginning of the next iteration
(unless D ≤∆dQ in which case the loop ends). By the way we set K, the loop ends in at most K
iterations and thus the overall failure probability is at most δ. As long as none of the subroutines fails,
the above argument implies the output must satisfy
f
M −M⋆
∞≤∆Q"
END,0.886950453593859,"and that f
M has rank at most Q. Note that by Lemma 2, we can simulate all of the observations used
in the algorithm with a single sample Osr
p (M⋆+ N) with reveal probability"
END,0.8876482903000698,"p ≥
poly

r⋆, µ, log

d∥M⋆∥F δ∆
 d
."
END,0.8883461270062806,"The overall runtime guarantee follows by combining the runtime guarantees of Corollary 1, Proposi-
tion 3, and the PCA step (see Remark 4)."
END,0.8890439637124913,NeurIPS Paper Checklist
CLAIMS,0.889741800418702,1. Claims
CLAIMS,0.8904396371249128,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8911374738311235,Answer: [Yes]
CLAIMS,0.8918353105373342,"Justification: As stated in the abstract and introduction, this is a theory paper where we give
a new algorithm for semi-random matrix completion and prove that it achieves improved
guarantees in terms of runtime, accuracy and noise tolerance."
CLAIMS,0.892533147243545,Guidelines:
CLAIMS,0.8932309839497558,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper."
CLAIMS,0.8939288206559665,"• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers."
CLAIMS,0.8946266573621773,"• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings."
CLAIMS,0.895324494068388,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8960223307745987,2. Limitations
LIMITATIONS,0.8967201674808095,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8974180041870202,Answer: [Yes]
LIMITATIONS,0.8981158408932309,"Justification: We discuss the modeling assumptions and acknowledge that since this is a
theory paper, the modeling assumptions and runtimes of the algorithms may not be practical
yet (as mentioned in Section 1.3), but hope that the insights are useful and may have practical
implications in the future."
LIMITATIONS,0.8988136775994418,Guidelines:
LIMITATIONS,0.8995115143056525,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper."
LIMITATIONS,0.9002093510118633,"• The authors are encouraged to create a separate ""Limitations"" section in their paper."
LIMITATIONS,0.900907187718074,"• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be."
LIMITATIONS,0.9016050244242847,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated."
LIMITATIONS,0.9023028611304955,"• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon."
LIMITATIONS,0.9030006978367062,"• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size."
LIMITATIONS,0.9036985345429169,"• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness."
LIMITATIONS,0.9043963712491278,"• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9050942079553385,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9057920446615492,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.90648988136776,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9071877180739707,"Justification: The paper (and supplementary material) includes complete proofs of all of the
claims."
THEORY ASSUMPTIONS AND PROOFS,0.9078855547801814,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.9085833914863922,• The answer NA means that the paper does not include theoretical results.
THEORY ASSUMPTIONS AND PROOFS,0.9092812281926029,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced."
THEORY ASSUMPTIONS AND PROOFS,0.9099790648988136,• All assumptions should be clearly stated or referenced in the statement of any theorems.
THEORY ASSUMPTIONS AND PROOFS,0.9106769016050245,"• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition."
THEORY ASSUMPTIONS AND PROOFS,0.9113747383112352,"• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material."
THEORY ASSUMPTIONS AND PROOFS,0.9120725750174459,• Theorems and Lemmas that the proof relies upon should be properly referenced.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9127704117236567,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9134682484298674,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9141660851360781,Answer: [NA]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9148639218422889,Justification: The paper does not include experiments.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9155617585484996,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9162595952547103,• The answer NA means that the paper does not include experiments.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9169574319609212,"• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9176552686671319,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9183531053733427,"• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9190509420795534,"• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9197487787857641,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9204466154919749,"(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9211444521981856,"(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset)."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9218422889043963,"(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9225401256106072,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9232379623168179,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9239357990230286,Answer: [NA]
OPEN ACCESS TO DATA AND CODE,0.9246336357292394,Justification: The paper does not include experiments.
OPEN ACCESS TO DATA AND CODE,0.9253314724354501,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9260293091416608,• The answer NA means that paper does not include experiments requiring code.
OPEN ACCESS TO DATA AND CODE,0.9267271458478716,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details."
OPEN ACCESS TO DATA AND CODE,0.9274249825540823,"• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark)."
OPEN ACCESS TO DATA AND CODE,0.928122819260293,"• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details."
OPEN ACCESS TO DATA AND CODE,0.9288206559665039,"• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc."
OPEN ACCESS TO DATA AND CODE,0.9295184926727146,"• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why."
OPEN ACCESS TO DATA AND CODE,0.9302163293789253,"• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable)."
OPEN ACCESS TO DATA AND CODE,0.9309141660851361,"• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9316120027913468,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9323098394975575,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9330076762037683,Answer: [NA]
OPEN ACCESS TO DATA AND CODE,0.933705512909979,Justification: The paper does not include experiments.
OPEN ACCESS TO DATA AND CODE,0.9344033496161898,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9351011863224006,• The answer NA means that the paper does not include experiments.
OPEN ACCESS TO DATA AND CODE,0.9357990230286113,"• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them."
OPEN ACCESS TO DATA AND CODE,0.9364968597348221,"• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9371946964410328,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9378925331472435,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9385903698534543,Answer: [NA]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.939288206559665,Justification: The paper does not include experiments.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9399860432658758,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9406838799720866,• The answer NA means that the paper does not include experiments.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9413817166782973,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.942079553384508,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9427773900907188,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9434752267969295,"• The assumptions made should be given (e.g., Normally distributed errors)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9441730635031402,"• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.944870900209351,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9455687369155618,"• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9462665736217725,"• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9469644103279833,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.947662247034194,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9483600837404047,Answer: [NA]
EXPERIMENTS COMPUTE RESOURCES,0.9490579204466155,Justification: The paper does not include experiments.
EXPERIMENTS COMPUTE RESOURCES,0.9497557571528262,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9504535938590369,• The answer NA means that the paper does not include experiments.
EXPERIMENTS COMPUTE RESOURCES,0.9511514305652478,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage."
EXPERIMENTS COMPUTE RESOURCES,0.9518492672714585,"• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute."
EXPERIMENTS COMPUTE RESOURCES,0.9525471039776692,"• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.95324494068388,9. Code Of Ethics
CODE OF ETHICS,0.9539427773900907,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9546406140963015,Answer: [Yes]
CODE OF ETHICS,0.9553384508025122,Justification: We read the ethics guidelines and believe we meet them.
CODE OF ETHICS,0.9560362875087229,Guidelines:
CODE OF ETHICS,0.9567341242149338,• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
CODE OF ETHICS,0.9574319609211445,"• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics."
CODE OF ETHICS,0.9581297976273552,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.958827634333566,10. Broader Impacts
BROADER IMPACTS,0.9595254710397767,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9602233077459874,Answer: [NA]
BROADER IMPACTS,0.9609211444521982,"Justification: Our paper is about theoretical results and therefore substantial societal impact
would likely come only with further research and work. However, we do discuss how our
paper is motivated by algorithms more robust and less sensitive to distributional assumptions.
In this sense, our paper seems to be providing a potential helpful tool; positive or negative
societal impacts of this tool would likely come from future work applying our result."
BROADER IMPACTS,0.9616189811584089,Guidelines:
BROADER IMPACTS,0.9623168178646196,• The answer NA means that there is no societal impact of the work performed.
BROADER IMPACTS,0.9630146545708305,"• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact."
BROADER IMPACTS,0.9637124912770412,"• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
BROADER IMPACTS,0.9644103279832519,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster."
BROADER IMPACTS,0.9651081646894627,"• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology."
BROADER IMPACTS,0.9658060013956734,"• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9665038381018841,11. Safeguards
SAFEGUARDS,0.9672016748080949,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9678995115143056,Answer: [NA]
SAFEGUARDS,0.9685973482205164,Justification: The paper does not release data or models.
SAFEGUARDS,0.9692951849267272,Guidelines:
SAFEGUARDS,0.9699930216329379,• The answer NA means that the paper poses no such risks.
SAFEGUARDS,0.9706908583391486,"• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters."
SAFEGUARDS,0.9713886950453594,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images."
SAFEGUARDS,0.9720865317515701,"• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9727843684577809,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9734822051639916,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9741800418702024,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.9748778785764132,Justification: The paper does not use existing assets.
LICENSES FOR EXISTING ASSETS,0.9755757152826239,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9762735519888346,• The answer NA means that the paper does not use existing assets.
LICENSES FOR EXISTING ASSETS,0.9769713886950454,• The authors should cite the original paper that produced the code package or dataset.
LICENSES FOR EXISTING ASSETS,0.9776692254012561,"• The authors should state which version of the asset is used and, if possible, include a
URL."
LICENSES FOR EXISTING ASSETS,0.9783670621074668,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset."
LICENSES FOR EXISTING ASSETS,0.9790648988136776,"• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.9797627355198884,"• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset."
LICENSES FOR EXISTING ASSETS,0.9804605722260991,"• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9811584089323099,"• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9818562456385206,13. New Assets
NEW ASSETS,0.9825540823447313,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9832519190509421,Answer: [NA]
NEW ASSETS,0.9839497557571528,Justification: The paper does not release new assets.
NEW ASSETS,0.9846475924633635,Guidelines:
NEW ASSETS,0.9853454291695743,• The answer NA means that the paper does not release new assets.
NEW ASSETS,0.9860432658757851,"• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc."
NEW ASSETS,0.9867411025819958,"• The paper should discuss whether and how consent was obtained from people whose
asset is used."
NEW ASSETS,0.9874389392882066,"• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881367759944173,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988834612700628,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895324494068388,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902302861130495,Justification: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9909281228192603,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916259595254711,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923237962316818,"• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930216329378926,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937194696441033,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994417306350314,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951151430565248,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958129797627355,Justification: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965108164689462,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972086531751571,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979064898813678,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986043265875785,"• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9993021632937893,"• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
