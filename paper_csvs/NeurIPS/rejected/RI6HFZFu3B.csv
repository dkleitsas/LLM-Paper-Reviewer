Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022935779816513763,"Graph neural networks (GNNs), a type of neural network that can learn from graph-
1"
ABSTRACT,0.0045871559633027525,"structured data and learn the representation of nodes through aggregating neigh-
2"
ABSTRACT,0.006880733944954129,"borhood information, have shown superior performance in various downstream
3"
ABSTRACT,0.009174311926605505,"tasks. However, it is known that the performance of GNNs degrades gradually as
4"
ABSTRACT,0.011467889908256881,"the number of layers increases. In this paper, we evaluate the expressive power of
5"
ABSTRACT,0.013761467889908258,"GNNs from the perspective of subgraph aggregation. We reveal the potential cause
6"
ABSTRACT,0.016055045871559634,"of performance degradation for traditional deep GNNs, i.e., aggregated subgraph
7"
ABSTRACT,0.01834862385321101,"overlap, and we theoretically illustrate the fact that previous residual-based GNNs
8"
ABSTRACT,0.020642201834862386,"exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.
9"
ABSTRACT,0.022935779816513763,"Further, we find that the utilization of different subgraphs by previous models is
10"
ABSTRACT,0.02522935779816514,"often inflexible. Based on this, we propose a sampling-based node-level residual
11"
ABSTRACT,0.027522935779816515,"module (SNR) that can achieve a more flexible utilization of different hops of sub-
12"
ABSTRACT,0.02981651376146789,"graph aggregation by introducing node-level parameters sampled from a learnable
13"
ABSTRACT,0.03211009174311927,"distribution. Extensive experiments show that the performance of GNNs with our
14"
ABSTRACT,0.034403669724770644,"proposed SNR module outperform a comprehensive set of baselines.
15"
INTRODUCTION,0.03669724770642202,"1
Introduction
16"
INTRODUCTION,0.0389908256880734,"GNNs have emerged in recent years as the most powerful model for processing graph-structured data
17"
INTRODUCTION,0.04128440366972477,"and have performed very well in various fields, such as social networks [1], recommender systems
18"
INTRODUCTION,0.04357798165137615,"[2], and drug discovery [3]. Through the message-passing mechanism that propagates and aggregates
19"
INTRODUCTION,0.045871559633027525,"representations of neighboring nodes, GNNs provide a general framework for learning information
20"
INTRODUCTION,0.0481651376146789,"on graph structure.
21"
INTRODUCTION,0.05045871559633028,"Despite great success, according to previous studies [4, 5], GNNs show significant performance
22"
INTRODUCTION,0.052752293577981654,"degradation as the number of layers increases, which makes GNNs not able to take full advantage of
23"
INTRODUCTION,0.05504587155963303,"the multi-hop neighbor structure of nodes to obtain better node representations.
24"
INTRODUCTION,0.05733944954128441,"The main reason for this situation is now widely believed to be oversmoothing [4, 6, 5, 7]. However,
25"
INTRODUCTION,0.05963302752293578,"since ResNet [8] uses residual connection to solve a similar problem in computer vision and obtains
26"
INTRODUCTION,0.06192660550458716,"good results, several new works have been inspired to apply the idea of residual connection to GNNs
27"
INTRODUCTION,0.06422018348623854,"to alleviate oversmoothing and thus improve the expressive power. For example, JKNet [5] learns
28"
INTRODUCTION,0.06651376146788991,"node representations by aggregating the outputs of all previous layers at the last layer. DenseGCN [9]
29"
INTRODUCTION,0.06880733944954129,"concatenates the results of the current layer and all previous layers as the node representations of this
30"
INTRODUCTION,0.07110091743119266,"layer. APPNP [7] uses the initial residual connection to retain the initial feature information with
31"
INTRODUCTION,0.07339449541284404,"probability α, and utilizes the feature information aggregated at the current layer with probability
32"
INTRODUCTION,0.07568807339449542,"1 −α.
33"
INTRODUCTION,0.0779816513761468,"In this paper, we evaluate the expressive power of GNNs from the perspective of subgraph aggregation.
34"
INTRODUCTION,0.08027522935779817,"Based on this perspective, we show that the single high-hop subgraph aggregation of message-passing
35"
INTRODUCTION,0.08256880733944955,"GNNs is limited by the fact that high-hop subgraphs are prone to information overlap, which
36"
INTRODUCTION,0.08486238532110092,"makes the node representations obtained from k-hop subgraph aggregation indistinguishable, i.e.,
37"
INTRODUCTION,0.0871559633027523,"oversmoothing occurs.
38"
INTRODUCTION,0.08944954128440367,"Based on this perspective, we conduct a theoretical analysis of previous residual-based models and
39"
INTRODUCTION,0.09174311926605505,"find that previous methods are in fact able to utilize multiple subgraph aggregations to improve the
40"
INTRODUCTION,0.09403669724770643,"expressiveness of the model. However, most methods tend to utilize subgraph information by fixed
41"
INTRODUCTION,0.0963302752293578,"coefficients, which assumes that the information from the subgraph of the same hop are equally
42"
INTRODUCTION,0.09862385321100918,"important for different nodes, which leads to inflexibility in the model’s exploitation of subgraph
43"
INTRODUCTION,0.10091743119266056,"information and thus limits further improvement of the expressive power. Some existing methods try
44"
INTRODUCTION,0.10321100917431193,"to overcome this inflexibility but lead to overfitting by introducing more parameters, which in turn
45"
INTRODUCTION,0.10550458715596331,"affects the effectiveness of the model, which is demonstrated by the experiment.
46"
INTRODUCTION,0.10779816513761468,"Considering these limitations, we propose a Sampling-based Node-level Residual module (SNR).
47"
INTRODUCTION,0.11009174311926606,"Specifically, we adopt a more fine-grained node-level residual module to achieve a more flexible
48"
INTRODUCTION,0.11238532110091744,"exploitation of subgraph aggregation, which is proved by the theoretical analysis. On the other
49"
INTRODUCTION,0.11467889908256881,"hand, to avoid overfitting due to the introduction of more parameters, instead of learning the specific
50"
INTRODUCTION,0.11697247706422019,"parameters directly, we first learn a correlation distribution through reparameterization trick and
51"
INTRODUCTION,0.11926605504587157,"obtain the specific residual coefficients by sampling. Experiments verify that this sampling-based
52"
INTRODUCTION,0.12155963302752294,"approach can significantly alleviate overfitting.
53"
INTRODUCTION,0.12385321100917432,"Our Contributions. (1) We reinterpret the phenomenon that the effectiveness of traditional message-
54"
INTRODUCTION,0.12614678899082568,"passing GNNs decreases as the number of layers increases from the perspective of k-hop subgraph
55"
INTRODUCTION,0.12844036697247707,"overlap. (2) Based on the idea of subgraph aggregation, we theoretically analyze the previous residual-
56"
INTRODUCTION,0.13073394495412843,"based methods and find that they actually utilize multiple hop subgraph aggregation in different
57"
INTRODUCTION,0.13302752293577982,"ways to improve the expressive power of the model, and we point out the limitations of inflexibility
58"
INTRODUCTION,0.1353211009174312,"and overfitting in previous residual-based methods. (3) We propose a sampling-based node-level
59"
INTRODUCTION,0.13761467889908258,"residual module that allows more flexible exploitation of different k-hop subgraph aggregations while
60"
INTRODUCTION,0.13990825688073394,"alleviating overfitting due to more parameters. (4) Extensive experiments show that GNNs with the
61"
INTRODUCTION,0.14220183486238533,"proposed SNR module achieve better performance than other methods, as well as with higher training
62"
INTRODUCTION,0.1444954128440367,"efficiency, on semi-supervised tasks as well as on tasks requiring deep GNNs.
63"
PRELIMINARIES,0.14678899082568808,"2
Preliminaries
64"
NOTATIONS,0.14908256880733944,"2.1
Notations
65"
NOTATIONS,0.15137614678899083,"A connected undirected graph is represented by G = (V, E), where V = {v1, v2, . . . , vN} is the set
66"
NOTATIONS,0.1536697247706422,"of N nodes and E ⊆V × V is the set of edges. The feature of nodes is given in matrix H ∈RN×d
67"
NOTATIONS,0.1559633027522936,"where d indicates the length of feature. Let A ∈{0, 1}N×N denotes the adjacency matrix and
68"
NOTATIONS,0.15825688073394495,"Aij = 1 only if an edge exists between nodes vi and vj. D ∈RN×N is the diagonal degree matrix
69"
NOTATIONS,0.16055045871559634,"whose elements di computes the number of edges connected to node vi. ˜A = A + I is the adjacency
70"
NOTATIONS,0.1628440366972477,"matrix with self loop and ˜D = D + I.
71"
GRAPH NEURAL NETWORKS,0.1651376146788991,"2.2
Graph Neural Networks
72"
GRAPH NEURAL NETWORKS,0.16743119266055045,"A GNNs layer updates the representation of each node via aggregating itself and its neighbors’
73"
GRAPH NEURAL NETWORKS,0.16972477064220184,"representations. Specifically, a layer’s output H′ consists of new representations h′ of each node
74"
GRAPH NEURAL NETWORKS,0.1720183486238532,"computed as:
75"
GRAPH NEURAL NETWORKS,0.1743119266055046,"h′
i = fθ (hi, AGGREGATE ({hj | vj ∈V, (vi, vj) ∈E}))"
GRAPH NEURAL NETWORKS,0.17660550458715596,"where h′
i indicates the new representation of node vi and fθ denotes the update function. The
76"
GRAPH NEURAL NETWORKS,0.17889908256880735,"key to the performance of different GNNs is in the design of the fθ and AGGREGATE function.
77"
GRAPH NEURAL NETWORKS,0.1811926605504587,"Graph Convolutional Network (GCN)[10] is a classical massage-passing GNNs follows layer-wise
78"
GRAPH NEURAL NETWORKS,0.1834862385321101,"propagation rule:
79"
GRAPH NEURAL NETWORKS,0.18577981651376146,"Hk+1 = σ

˜D−1"
GRAPH NEURAL NETWORKS,0.18807339449541285,2 ˜A ˜D−1
HKWK,0.19036697247706422,"2 HkWk

(1)"
HKWK,0.1926605504587156,"where Hk is the feature matrix of the kth layer, Wk is a layer-specific learnable weight matrix, σ(·)
80"
HKWK,0.19495412844036697,"denotes an activation function.
81"
RESIDUAL CONNECTION,0.19724770642201836,"2.3
Residual Connection
82"
RESIDUAL CONNECTION,0.19954128440366972,"Several works have used residual connection to solve the problem of oversmoothing. Common
83"
RESIDUAL CONNECTION,0.2018348623853211,residual connection for GNNs are summarized below. Details are explained in Appendix A.
RESIDUAL CONNECTION,0.20412844036697247,Table 1: Common residual connection for GNNs.
RESIDUAL CONNECTION,0.20642201834862386,"Residual Connection
Corresponding GCN
Formula"
RESIDUAL CONNECTION,0.20871559633027523,"Res
ResGCN
Hk = Hk−1 + σ

˜D−1"
RESIDUAL CONNECTION,0.21100917431192662,2 ˜A ˜D−1
RESIDUAL CONNECTION,0.21330275229357798,"2 Hk−1Wk−1
"
RESIDUAL CONNECTION,0.21559633027522937,"InitialRes
APPNP
Hk = (1 −α) ˜D−1"
RESIDUAL CONNECTION,0.21788990825688073,2 ˜A ˜D−1
RESIDUAL CONNECTION,0.22018348623853212,2 Hk−1 + αH
RESIDUAL CONNECTION,0.22247706422018348,"Dense
DenseGCN
Hk = AGGdense(H, H1, . . . , Hk−1)"
RESIDUAL CONNECTION,0.22477064220183487,"JK
JKNet
Houtput = AGGjk(H1, . . . , Hk−1) 84"
MOTIVATION,0.22706422018348624,"3
Motivation
85"
MOTIVATION,0.22935779816513763,"Message-passing GNNs recursively update the features of each node by aggregating information
86"
MOTIVATION,0.231651376146789,"from its neighbors, allowing them to capture both the graph topology and node features. For a
87"
MOTIVATION,0.23394495412844038,"message-passing GNNs without a residual structure, the information domain of each node after
88"
MOTIVATION,0.23623853211009174,"k-layer aggregation is a related k-hop subgraph. Figure 1 shows that, after two aggregation operations,
89"
MOTIVATION,0.23853211009174313,"nodes on layer 2 obtain 1-hop neighbor and 2-hop neighbor information in layer 0, respectively.
90"
MOTIVATION,0.2408256880733945,"According to the definition of the k-hop subgraph, the information of the node on layer 2 in the figure
91"
MOTIVATION,0.24311926605504589,"is composed of all reachable nodes information shown on layer 0. We can consider the result of
92"
MOTIVATION,0.24541284403669725,"k-layer residual-free message-passing GNNs is equivalent to k-time aggregation of each node on its
93"
MOTIVATION,0.24770642201834864,"k-hop subgraph, which we call k-hop subgraph aggregation.
94"
MOTIVATION,0.25,Figure 1: k-hop subgraph.
MOTIVATION,0.25229357798165136,"It is evident that as the number of aggregation operations in-
95"
MOTIVATION,0.2545871559633027,"creases, the reachable information range of a node expands
96"
MOTIVATION,0.25688073394495414,"rapidly, that is, the size of its k-hop subgraph grows expo-
97"
MOTIVATION,0.2591743119266055,"nentially as k increases, leading to a significant increase
98"
MOTIVATION,0.26146788990825687,"in the overlap between the k-hop subgraphs of different
99"
MOTIVATION,0.26376146788990823,"nodes. As a result, the aggregation result of different nodes
100"
MOTIVATION,0.26605504587155965,"on their respective k-hop subgraphs becomes indistinguish-
101"
MOTIVATION,0.268348623853211,"able. Furthermore, in a specific graph dataset, nodes with
102"
MOTIVATION,0.2706422018348624,"higher degrees tend to have a larger range of k-hop sub-
103"
MOTIVATION,0.27293577981651373,"graphs compared to nodes with lower degrees. As a result,
104"
MOTIVATION,0.27522935779816515,"the subgraphs are more likely to overlap between nodes
105"
MOTIVATION,0.2775229357798165,"with higher degrees, making their aggregation results more
106"
MOTIVATION,0.2798165137614679,"likely to become similar and indistinguishable.
107"
MOTIVATION,0.28211009174311924,"To verify this point, we conduct experiments on three graph datasets, Cora, Citeseer, and Pubmed.
108"
MOTIVATION,0.28440366972477066,"First, we group the nodes according to their degrees by assigning nodes with degrees in the range of
109"
MOTIVATION,0.286697247706422,"[2i, 2i+1) to the i-th group. Subsequently, we perform aggregation with different layers of GCN and
110"
MOTIVATION,0.2889908256880734,"GAT, then calculate the degree of smoothing of the node representations within each group separately.
111"
MOTIVATION,0.29128440366972475,"We use the metric proposed in [11] to measure the smoothness of the node representations within
112"
MOTIVATION,0.29357798165137616,"each group, namely SMV, which calculates the average of the distances between the nodes within
113"
MOTIVATION,0.2958715596330275,"the group:
114"
MOTIVATION,0.2981651376146789,"SMV(X) =
1
N(N −1) X"
MOTIVATION,0.30045871559633025,"i̸=j
D (Xi,:, Xj,:)
(2)"
MOTIVATION,0.30275229357798167,"where D(·, ·) denotes the normalized Euclidean distance between two vectors:
115"
MOTIVATION,0.30504587155963303,"D(x, y) = 1 2"
MOTIVATION,0.3073394495412844,"x
∥x∥−
y
∥y∥ 2
(3)"
MOTIVATION,0.30963302752293576,"A smaller value of SMV indicates a greater similarity in node representations.
116"
MOTIVATION,0.3119266055045872,"We select the most representative result illustrated in Figure 2, which shows the result of GAT on
117"
MOTIVATION,0.31422018348623854,"Pubmed. The rest of the results are shown in the Appendix B. It can be seen that the groups of nodes
118"
MOTIVATION,0.3165137614678899,"with higher degree tend to be more likely to have high similarity in the representation of nodes within
119"
MOTIVATION,0.31880733944954126,"the group in different layers of the model. This finding supports our claim.
120"
MOTIVATION,0.3211009174311927,"Figure 2: SMV for node groups of dif-
ferent degrees."
MOTIVATION,0.32339449541284404,"After verifying the conclusion that subgraph overlap leads
121"
MOTIVATION,0.3256880733944954,"to oversmoothing through experiments, a natural idea is to
122"
MOTIVATION,0.32798165137614677,"alleviate the problem of large overlap of single subgraph
123"
MOTIVATION,0.3302752293577982,"by utilizing multiple hop subgraph aggregations, thereby
124"
MOTIVATION,0.33256880733944955,"alleviating oversmoothing. In the following section, we
125"
MOTIVATION,0.3348623853211009,"will demonstrate that the previous k-layer residual-based
126"
MOTIVATION,0.33715596330275227,"GNNs are actually different forms of integration of 1 to k
127"
MOTIVATION,0.3394495412844037,"hop subgraph aggregations.
128"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.34174311926605505,"3.1
Revisiting Previous Models in a New Perspective
129"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3440366972477064,"In the rest of this paper we will uniformly take GCN, a
130"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3463302752293578,"classical residual-free message-passing GNNs, as an exam-
131"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3486238532110092,"ple. We assume that H is non-negative, so the ELU function can be ignored. In addition, the weight
132"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.35091743119266056,"matrix is ignored for simplicity. Combined with the formula of GCN given in Equation 1, we can
133"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3532110091743119,"formulate the specific result of k-hop subgraph aggregation as NkH, where N = ˜D−1"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3555045871559633,2 ˜A ˜D−1
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3577981651376147,"2 . To
134"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.36009174311926606,"show more intuitively how different k-layer-based residual models utilize NjH, j = 0, 1, · · · , k. We
135"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3623853211009174,"derive the general term formulas of their final outputs, and the results are shown in Table 2. Details
136"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3646788990825688,of the derivation of the formula in this part are given in Appendix C.
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3669724770642202,Table 2: General term formulas of residual models.
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.36926605504587157,"Model Name
General Term Formula"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.37155963302752293,"ResGCN
Hk = Pk
j=0 Cj
kNjH"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3738532110091743,"APPNP
Hk = (1 −α)k NkH + α
k−1
P j=0 jP"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3761467889908257,"i=0
(−1)j−i (1 −α)i NiH"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.37844036697247707,"JKNet
Hk = AGGjk(NH, . . . , Nk−1H)"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.38073394495412843,"DenseGCN
— 137"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3830275229357798,"From the formula in the table, we can see that, in comparison to message-passing GNNs, residual-
138"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3853211009174312,"based variants of GNNs can utilize multiple k-hop subgraphs. There are two methods to exploit
139"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3876146788990826,"them: (1) Summation, such as ResGCN and APPNP. Such methods employ linear summation over
140"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.38990825688073394,"the aggregation of different hop subgraphs; (2) Aggregation functions, such as DenseNet and JKNet.
141"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3922018348623853,"Such methods make direct and explicit exploitation of different hop subgraph aggregations through
142"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3944954128440367,"methods such as concatenation.
143"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.3967889908256881,"However, for the first type of methods, they all employ a fixed, layer-level coefficient for linear
144"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.39908256880733944,"summation of the subgraph aggregation, which assumes that the information from the subgraph of the
145"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.4013761467889908,"same hop are equally important for different nodes. It will limit the expressive power of GNNs, which
146"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.4036697247706422,"reveals the need to design a more fine-grained node-level residual module that can more flexibly
147"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.4059633027522936,"utilize information from different k-hop subgraphs. For another type of method, they can achieve
148"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.40825688073394495,"finer-grained subgraph aggregation, but the experiment find that their performance is not improved
149"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.4105504587155963,"because of the more finer-grained structure, mainly because the introduction of more parameters
150"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.41284403669724773,"leads to overfitting Phenomenon. In general, neither of these two types of methods has achieved a
151"
REVISITING PREVIOUS MODELS IN A NEW PERSPECTIVE,0.4151376146788991,"more effective improvement in the expressive power of GNNs.
152"
THE PROPOSED METHOD,0.41743119266055045,"4
The Proposed Method
153"
THE PROPOSED METHOD,0.4197247706422018,"In order to solve the two limitations of flexibility and overfitting encountered by previous residual-
154"
THE PROPOSED METHOD,0.42201834862385323,"based models, we try to propose a node-level, more flexible, general residual module, which can
155"
THE PROPOSED METHOD,0.4243119266055046,"alleviate overfitting caused by more parameters at the same time. Based on this, we propose a
156"
THE PROPOSED METHOD,0.42660550458715596,"sampling-based node-level generic residual module SNR. We define SNR module as:
157"
THE PROPOSED METHOD,0.4288990825688073,"h(i)
k−1"
THE PROPOSED METHOD,0.43119266055045874,"′ = GraphConv

h(i)
k−1

(4) 158"
THE PROPOSED METHOD,0.4334862385321101,"h(i)
k
= h(i)
1 + sigmoid(p(i)
k−1)

h(i)
1 −h(i)
k−1"
THE PROPOSED METHOD,0.43577981651376146,"′
,
p(i)
k−1 ∼N(α(i)
k−1, β(i)
k−1"
THE PROPOSED METHOD,0.4380733944954128,"2)
(5)"
THE PROPOSED METHOD,0.44036697247706424,"where h(i)
k
denotes the representation of k-th layer of node i, h(i)
k"
THE PROPOSED METHOD,0.4426605504587156,"′ denotes the result obtained by
159"
THE PROPOSED METHOD,0.44495412844036697,"an arbitrary GNNs layer with h(i)
k−1 as input. p(i)
k is a random number sampled from N(α(i)
k , β(i)
k"
THE PROPOSED METHOD,0.44724770642201833,"2)
160"
THE PROPOSED METHOD,0.44954128440366975,"which associated with the i-th node at the k-th layer while α(i)
k
and β(i)
k
are learnable parameters
161"
THE PROPOSED METHOD,0.4518348623853211,"representing the mean and the standard deviation of this distribution, respectively. Next, we will
162"
THE PROPOSED METHOD,0.4541284403669725,"illustrate the superiority of the SNR module in terms of flexibility and overfitting alleviation.
163"
FLEXIBILITY,0.45642201834862384,"4.1
Flexibility
164"
FLEXIBILITY,0.45871559633027525,"In this section, we will analyze the expressive power of GCN with SNR module and show that SNR-
165"
FLEXIBILITY,0.4610091743119266,"GCN achieves a more flexible utilization of multiple subgraph aggregations. First of all, combined
166"
FLEXIBILITY,0.463302752293578,"with the previous definition 5, the matrix form of the recurrence formula of SNR-GCN can be written
167"
FLEXIBILITY,0.46559633027522934,"as:
168"
FLEXIBILITY,0.46788990825688076,"Hk = H1 + Λk−1

H1 −˜D−1/2 ˜A ˜D−1/2Hk−1

(6)"
FLEXIBILITY,0.4701834862385321,"where Λk is a diagonal matrix whose i-th diagonal element is equal to p(i)
k . We first try to obtain the
169"
FLEXIBILITY,0.4724770642201835,"general term formula of SNR-GCN according to the recursive formula and demonstrate SNR-GCN’s
170"
FLEXIBILITY,0.47477064220183485,"treatment of multiple subgraph aggregations. The following theorem can be proved:
171"
FLEXIBILITY,0.47706422018348627,"Theorem 1.
The general term formula of SNR-GCN can be deduced as:
Hk
=
172
Pk−1
i=2
Qk−1
j=i ˜Nj (Mi −Mi−1) + Qk−1
i=1 ˜Ni (H1 + M1) −Mk−1 where ˜Ni = −Λk−1N and
173"
FLEXIBILITY,0.4793577981651376,"Mk = −(ΛkN + I)−1 (I + Λk) H1.
174"
FLEXIBILITY,0.481651376146789,"The details of the proof are provided in Appendix D. From the general term formula of SNR-GCN,
175"
FLEXIBILITY,0.48394495412844035,"we can see that Mk is a linear transformation of H1. Therefore, the first two terms of the formula
176"
FLEXIBILITY,0.48623853211009177,"can be approximately regarded as a new form of subgraph aggregation. Further we can find that all
177"
FLEXIBILITY,0.48853211009174313,"1 to k hop subgraph aggregations appear in the formula, which ensures the expressive power. And
178"
FLEXIBILITY,0.4908256880733945,"because Λk are learnable diagonal matrixes , SNR-GCN’s subgraph aggregation is learnable and
179"
FLEXIBILITY,0.49311926605504586,"more flexible, which further makes expressive power stronger. Besides, when we set Λk = −αI, the
180"
FLEXIBILITY,0.4954128440366973,"first term will be 0, and the rest terms are equivalent to APPNP’s formula, which means SNR-GCN
181"
FLEXIBILITY,0.49770642201834864,"can be approximately regarded as a more fine-grained and expressive APPNP.
182"
OVERFITTING ALLEVIATION,0.5,"4.2
Overfitting Alleviation
183"
OVERFITTING ALLEVIATION,0.5022935779816514,"Another key point of SNR is that it introduces randomness to alleviate overfitting. In our initial idea,
184"
OVERFITTING ALLEVIATION,0.5045871559633027,"we attempt to build a generic module similar to the initial residual at the node level. Based on this,
185"
OVERFITTING ALLEVIATION,0.5068807339449541,"we initially designed the following modules:
186"
OVERFITTING ALLEVIATION,0.5091743119266054,"h(i)
k
= h(i)
1 + sigmoid(q(i)
k )

h(i)
1 −h(i)
k−1"
OVERFITTING ALLEVIATION,0.5114678899082569,"′
(7)"
OVERFITTING ALLEVIATION,0.5137614678899083,"where q(i)
k
is a learnable parameter which associated with the i-th node at the k-th layer. After
187"
OVERFITTING ALLEVIATION,0.5160550458715596,"conducting experiments, we discover that the model has a high risk of overfitting when adding this
188"
OVERFITTING ALLEVIATION,0.518348623853211,"module. However, we also find that if we do not learn q(i)
k
directly through backpropagation, but first
189"
OVERFITTING ALLEVIATION,0.5206422018348624,"learn a normal distribution associated with it via reparameterization trick and obtain q(i)
k
by sampling
190"
OVERFITTING ALLEVIATION,0.5229357798165137,"at each computation, the issue can be resolved, and the performance of the model significantly
191"
OVERFITTING ALLEVIATION,0.5252293577981652,"improves. To prove this, we perform an experimental verification. The details of the experiments are
192"
OVERFITTING ALLEVIATION,0.5275229357798165,"shown in the Appendix E.
193"
OVERFITTING ALLEVIATION,0.5298165137614679,"It is worth noting that GCNII and SNR-GCN share a similar architecture, so both can be viewed
194"
OVERFITTING ALLEVIATION,0.5321100917431193,"approximately as more refined APPNP-style models. However, when faced with the problem of
195"
OVERFITTING ALLEVIATION,0.5344036697247706,"overfitting due to more parameters, GCNII adds an identity matrix to mitigate the issue. Later
196"
OVERFITTING ALLEVIATION,0.536697247706422,"experiment results have shown that SNR-GCN’s learning distribution-sampling approach is more
197"
OVERFITTING ALLEVIATION,0.5389908256880734,"effective in alleviating overfitting.
198"
COMPLEXITY ANALYSIS,0.5412844036697247,"4.3
Complexity Analysis
199"
COMPLEXITY ANALYSIS,0.5435779816513762,"Taking vanilla GCN as an example, we analyzed the additional complexity of SNR in model and time.
200"
COMPLEXITY ANALYSIS,0.5458715596330275,"We assume that the number of nodes in the graph is n and hidden dimension is d.
201"
COMPLEXITY ANALYSIS,0.5481651376146789,"Model Complexity. As described in Section 4, at each layer the SNR module learns a mean and
202"
COMPLEXITY ANALYSIS,0.5504587155963303,"standard deviation of the corresponding distribution for each node, so the complexity can be calculated
203"
COMPLEXITY ANALYSIS,0.5527522935779816,"as O(n), and thus the additional complexity of the k-layer model equipped with SNR is O(kn).
204"
COMPLEXITY ANALYSIS,0.555045871559633,"Time Complexity. The time complexity of a vanilla GCN layer mainly comes from the matrix
205"
COMPLEXITY ANALYSIS,0.5573394495412844,"multiplication of N and H, hence its complexity is O(n2d). And the main computational parts of a
206"
COMPLEXITY ANALYSIS,0.5596330275229358,"SNR module are the sampling of p(i)
k , scalar multiplication and matrix addition, which correspond to
207"
COMPLEXITY ANALYSIS,0.5619266055045872,"a complexity of O(n), O(nd), and O(nd), respectively. Thus the time complexity of the SNR module is
208"
COMPLEXITY ANALYSIS,0.5642201834862385,"O(nd) and the time complexity of a GCN layer equipped the SNR module is O(n2d + nd). Therefore,
209"
COMPLEXITY ANALYSIS,0.5665137614678899,"the introduction of the SNR module does not significantly affect the computational efficiency.
210"
EXPERIMENT,0.5688073394495413,"5
Experiment
211"
EXPERIMENT,0.5711009174311926,"In this section, we aim to experimentally evaluate the effectiveness of SNR on real datasets. To
212"
EXPERIMENT,0.573394495412844,"achieve this, we will compare the performance of SNR with other methods and answer the following
213"
EXPERIMENT,0.5756880733944955,"research questions. Q1: How effective is SNR on classical tasks that prefer shallow models? Q2:
214"
EXPERIMENT,0.5779816513761468,"Can SNR help overcome oversmoothing in GNNs and enable the training of deeper models? Q3:
215"
EXPERIMENT,0.5802752293577982,"How effective is SNR on tasks that require deep GNNs? Q4: How efficient is the training of SNR?
216"
EXPERIMENT SETUP,0.5825688073394495,"5.1
Experiment Setup
217"
EXPERIMENT SETUP,0.5848623853211009,"In our study, we conduct experiments on four tasks: semi-supervised node classification (Q1),
218"
EXPERIMENT SETUP,0.5871559633027523,"alleviating performance drop in deeper GNNs (Q2), semi-supervised node classification with missing
219"
EXPERIMENT SETUP,0.5894495412844036,"vectors (Q3), and efficiency evaluation (Q4).
220"
EXPERIMENT SETUP,0.591743119266055,"Datasets. To assess the effectiveness of our proposed module, we have used four data sets that are
221"
EXPERIMENT SETUP,0.5940366972477065,"widely used in the field of GNN, including Cora, Citeseer, Pubmed [12], and CoraFull [13] for testing
222"
EXPERIMENT SETUP,0.5963302752293578,"purposes. In addition, we also use two webpage datasets collected from Wikipedia: Chameleon and
223"
EXPERIMENT SETUP,0.5986238532110092,"Squirrel [14]. Details on the characteristics of these datasets and the specific data-splitting procedures
224"
EXPERIMENT SETUP,0.6009174311926605,"used can be found in Appendix F.1.
225"
EXPERIMENT SETUP,0.6032110091743119,"Models. We consider two fundamental GNNs, GCN [10] and GAT [15]. For GCN, we test the
226"
EXPERIMENT SETUP,0.6055045871559633,"performance of SNR-GCN and its residual variant models, including ResGCN [9], APPNP [7],
227"
EXPERIMENT SETUP,0.6077981651376146,"DenseGCN [9], GCNII [16] and JKNet [5]. For GAT, we directly equip it with the following
228"
EXPERIMENT SETUP,0.6100917431192661,"residual module: Res, InitialRes, Dense, JK and SNR and test the performance. Additionally, for the
229"
EXPERIMENT SETUP,0.6123853211009175,"SSNC-MV task, we compare our proposed module with several classical oversmoothing mitigation
230"
EXPERIMENT SETUP,0.6146788990825688,"techniques, including BatchNorm [17], PairNorm [18], DGN [19], Decorr [11], DropEdge [20] and
231"
EXPERIMENT SETUP,0.6169724770642202,"other residual-based methods. Further details on these models and techniques can be found in the
232"
EXPERIMENT SETUP,0.6192660550458715,"following sections.
233"
EXPERIMENT SETUP,0.6215596330275229,"Implementations. For all benchmark and variant models, the linear layers in the models are
234"
EXPERIMENT SETUP,0.6238532110091743,"initialized with a standard normal distribution, and the convolutional layers are initialized with
235"
EXPERIMENT SETUP,0.6261467889908257,"Xavier initialization. The Adam optimizer [21] is used for all models. Further details on the specific
236"
EXPERIMENT SETUP,0.6284403669724771,"parameter settings used can be found in Appendix F.2. All models and datasets used in this paper are
237"
EXPERIMENT SETUP,0.6307339449541285,"implemented using the Deep Graph Library (DGL) [22]. All experiments are conducted on a server
238"
EXPERIMENT SETUP,0.6330275229357798,"with 15 vCPU Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz, A40 with 48GB GPU memory,
239"
EXPERIMENT SETUP,0.6353211009174312,"and 56GB main memory.
240"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6376146788990825,"5.2
Semi-supervised Node Classification
241"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6399082568807339,"To validate the performance of SNR, we apply the module to two fundamental GNNs, GCN and
242"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6422018348623854,"GAT, and test the accuracy according to the mentioned experimental setup, and compare it with four
243"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6444954128440367,"classic residual modules, DenseNet, ResNet, InitialResNet and JKNet. We vary the number of layers
244"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6467889908256881,"in the range of {1, 2, 3, · · · , 10} and select the best result among all layers. Specifically, we run
245"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6490825688073395,"10 times for each number of layers to obtain the mean accuracy along with the standard deviation.
246"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6513761467889908,"We select the best results among all layers and report them in the Table 3. We find that GNNs with
247"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6536697247706422,"Table 3: Summary of classification accuracy (%) results with various depths. The best results are in
bold and the second best results are underlined."
SEMI-SUPERVISED NODE CLASSIFICATION,0.6559633027522935,"Method
Cora
Citeseer
Pubmed
CoraFull
Chameleon
Squirrel"
SEMI-SUPERVISED NODE CLASSIFICATION,0.658256880733945,"GCN
80.16±1.15
70.20±0.62
78.26±0.61
68.40±0.33
68.00±2.30
51.69±1.83
ResGCN
79.01±1.26
69.27±0.66
78.08±0.51
67.98±0.51
65.26±2.47
47.43±1.14
APPNP
79.04±0.84
69.64±0.49
76.38±0.12
37.77±0.43
59.80±2.68
43.17±1.01
GCNII
78.53±0.67
69.55±1.14
76.17±0.70
68.30±0.26
64.76±2.43
52.83±1.51
DenseGCN
77.24±1.12
65.03±1.58
76.93±0.78
64.52±0.71
59.04±2.07
38.89±1.25
JKNet
78.16±1.21
65.33±1.66
78.10±0.55
66.11±0.49
55.75±2.93
35.95±1.10
SNR-GCN (Ours)
81.17±0.72
70.39±1.01
78.34±0.62
69.80±0.28
72.04±1.89
58.35±1.55"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6605504587155964,"GAT
79.24±1.18
69.51±1.07
77.59±0.80
67.39±0.32
65.81±2.13
50.16±2.42
Res-GAT
78.43±0.99
68.15±1.25
77.27±0.52
67.67±0.32
69.08±2.50
49.77±1.72
InitialRes-GAT
77.77±1.51
67.48±2.15
77.46±1.17
65.49±0.42
65.90±2.98
52.83±2.39
Dense-GAT
78.27±2.22
64.92±1.94
76.84±0.64
66.61±0.63
63.86±3.03
43.01±1.34
JK-GAT
78.91±1.71
65.59±2.62
77.70±0.64
67.69±0.65
56.14±2.68
37.25±1.01
SNR-GAT (Ours)
79.65±0.84
69.85±0.67
77.76±0.93
68.00±0.27
69.54±2.22
55.14±1.78"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6628440366972477,"the SNR module consistently achieve the best performance in all cases (Q1). However, from the
248"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6651376146788991,"experimental results, many models with residual modules have not achieved the expected results. In
249"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6674311926605505,"many cases, compared with the basic model, the accuracy is even reduced. According to previous
250"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6697247706422018,"research [18], we speculate that overfitting may have contributed to this phenomenon. To verify our
251"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6720183486238532,"hypothesis, we conduct further experiments. Given that most models in the previous experiments
252"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6743119266055045,"achieve their best performance with shallow models, we select models with two layers, train 500
253"
SEMI-SUPERVISED NODE CLASSIFICATION,0.676605504587156,"epochs, and report their accuracy on the training and validation sets at each epoch. The results are
254"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6788990825688074,"shown in Appendix G. Most models show signs of overfitting and SNR module demonstrates the best
255"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6811926605504587,"ability to alleviate overfitting. Specifically, in shallow GNNs with limited subgraph aggregation, most
256"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6834862385321101,"models have similar expressive abilities, and overfitting is the main factor affecting their performance.
257"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6857798165137615,"Our proposed method effectively alleviates overfitting by learning a more representative distribution,
258"
SEMI-SUPERVISED NODE CLASSIFICATION,0.6880733944954128,"resulting in a better performance than the base models.
259"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.6903669724770642,"5.3
Alleviating Performance Drop in Deeper GNNs
260"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.6926605504587156,"As the number of layers in GNNs increases, oversmoothing occurs, resulting in performance degrada-
261"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.694954128440367,"tion. Our objective is to investigate the performance of deep GNNs equipped with SNR and observe
262"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.6972477064220184,"the impact of oversmoothing on their performance. We evaluate the performance of GNNs with
263"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.6995412844036697,"different residual modules on 2, 16, and 32 layers using the Cora, Citeseer, and Pubmed datasets. The
264"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7018348623853211,"""None"" column represents vanilla GNNs without any additional modules. According to [16], APPNP
265"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7041284403669725,"is a shallow model, hence we use GCNII to represent GCN with initial residual connection instead.
266"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7064220183486238,"The same settings are used in section 5.4. The experimental results are presented in Table 4.
267"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7087155963302753,"From Table 4, we can observe that GNNs with SNR consistently outperform other residual methods
268"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7110091743119266,"and the base models in most of cases when given the same number of layers. SNR can significantly
269"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.713302752293578,"improve the performance of deep GNNs (Q2). For instance, on the Cora dataset, SNR improves the
270"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7155963302752294,"performance of 32-layer GCN and GAT by 53.69% and 56.20%, respectively. By flexibly utilizing
271"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7178899082568807,"multiple subgraph aggregation results with our SNR module, we can enhance the expressive power
272"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7201834862385321,"of the model and produce more distinctive node representations than those of regular GNNs, thereby
273"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7224770642201835,"overcoming the oversmoothing problem. These results suggest that we can train deep GNNs based
274"
ALLEVIATING PERFORMANCE DROP IN DEEPER GNNS,0.7247706422018348,"on SNR, making them suitable for tasks that require the use of deep GNNs.
275"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7270642201834863,"5.4
Semi-supervised Node Classification with Missing Vectors
276"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7293577981651376,"When do we need deep GNNs? [18] first proposed semi-supervised node classification with missing
277"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.731651376146789,"vectors (SSNC-MV), where nodes’ features are missing. SSNC-MV is a practical problem with
278"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7339449541284404,"various real-world applications. For example, new users on social networks usually lack personal
279"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7362385321100917,"information [23]. Obviously, we need more propagation steps to effectively aggregate information
280"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7385321100917431,"associated with existing users so that we can obtain representations of these new users. In this
281"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7408256880733946,"scenario, GNNs with more layers clearly perform better.
282"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7431192660550459,"Table 4: Node classification accuracy (%) on different number of layers. The best results are in bold
and the second best results are underlined."
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7454128440366973,"Dataset
Method
GCN
GAT
L2
L16
L32
L2
L16
L32 Cora"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7477064220183486,"None
79.50±0.84
69.83±2.47
25.31±12.49
79.11±1.55
75.44±1.08
22.74±7.47
Res
78.73±1.27
78.46±0.79
38.70±8.20
78.36±1.42
34.80±6.26
32.06±0.54
InitialRes
77.67±0.51
77.74±0.73
77.92±0.56
77.20±1.54
74.99±0.75
25.08±7.27
Dense
75.24±1.73
71.34±1.51
75.43±2.49
76.80±1.71
74.75±2.22
75.70±2.20
JK
76.28±1.73
72.39±3.20
75.03±1.11
78.06±0.51
76.66±1.39
23.29±8,45
SNR (Ours)
80.58±0.82
78.55±0.92
79.00±1.43
79.69±0.55
77.92±1.54
78.94±0.80"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.75,Citeseer
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7522935779816514,"None
68.31±1.40
54.07±2.48
34.84±1.60
68.64±1.20
59.16±2.44
24.37±3.59
Res
67.68±1.36
63.99±1.12
25.96±4.27
67.55±1.10
28.53±4.93
24.70±4.12
InitialRes
68.23±0.95
68.29±0.92
68.74±0.61
66.86±1.60
60.24±2.29
23.78±4.87
Dense
64.83±0.94
58.42±2.96
58.75±3.37
64.58±2.07
61.17±1.78
61.87±2.91
JK
64.69±1.44
58.38±3.36
58.63±4.76
65.84±2.02
62.64±1.66
23.09±4.02
SNR (Ours)
70.18±0.61
67.07±1.78
66.27±2.00
69.71±0.92
67.51±2.28
66.53±2.48"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7545871559633027,Pubmed
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7568807339449541,"None
77.53±0.73
76.16±0.96
51.29±11.71
77.07±0.52
77.49±0.65
53.20±9.18
Res
77.64±1.01
77.65±0.78
73.31±7.15
77.36±0.60
50.16±7.65
43.46±3.30
InitialRes
75.66±0.82
75.15±0.48
75.31±0.55
77.42±0.79
77.42±0.82
44.96±5.91
Dense
76.81±1.06
74.01±2.36
76.33±1.17
76.66±0.61
76.38±1.26
76.50±1.47
JK
77.61±0.78
76.31±1.45
76.59±1.53
77.48±0.84
77.75±0.77
40.84±0.23
SNR (Ours)
77.84±0.51
78.02±0.71
77.36±0.78
77.51±0.62
78.17±0.85
77.77+0.46"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7591743119266054,"Table 5: Test accuracy (%) on missing feature setting. The best results are in bold and the second
best results are underlined."
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7614678899082569,"GCN
GAT"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7637614678899083,"Method
Cora
Citeseer
Pubmed
Cora
Citeseer
Pubmed
Acc
#K
Acc
#K
Acc
#K
Acc
#K
Acc
#K
Acc
#K"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7660550458715596,"None
57.3
3
44.0
6
36.4
4
50.1
2
40.8
4
38.5
4
BatchNorm
71.8
20
45.1
25
70.4
30
72.7
5
48.7
5
60.7
4
PairNorm
65.6
20
43.6
25
63.1
30
68.8
8
50.3
6
63.2
20
DGN
76.3
20
50.2
30
72.0
30
75.8
8
54.5
5
72.3
20
DeCorr
73.8
20
49.1
30
73.3
15
72.8
15
46.5
6
72.4
15
DropEdge
67.0
6
44.2
8
69.3
6
67.2
6
48.2
6
67.2
6
Res
74.06±1.10
7
57.52±1.30
6
76.32±0.41
8
74.86±1.25
6
57.88±2.79
4
76.70±0.55
7
InitialRes
60.68±1.29
2
46.86±4.14
10
69.14±0.90
7
60.68±1.29
2
57.34±3.78
4
76.10±0.70
4
Dense
70.52±3.21
10
54.96±2.25
9
75.26±1.32
8
70.52±3.21
10
58.28±0.14
10
75.22±1.21
15
JK
72.68±2.61
8
57.54±1.14
10
76.44±1.51
20
72.68±2.63
8
58.82±2.02
5
76.12±0.87
10
SNR (Ours)
76.34±0.68
7
61.78±1.41
9
76.92±0.70
8
77.02±0.89
9
61.00±1.07
8
77.00±0.74
20"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.768348623853211,"Previous research has shown that normalization techniques can be effective in mitigating oversmooth-
283"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7706422018348624,"ing, and further, exploring deeper architectures. Therefore, we apply several techniques that can
284"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7729357798165137,"overcome oversmoothing and residual modules to GCN and GAT to compare their performance on
285"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7752293577981652,"tasks that require deep GNNs.
286"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7775229357798165,"We remove the node features in the validation and test set following the idea in [11, 18, 19]. We
287"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7798165137614679,"reuse the metrics that already reported in [11] for None, BatchNorm [17], PairNorm [18], DGN
288"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7821100917431193,"[19], DeCorr [11], and DropEdge [20]. For all residual-based models, the results are obtained by
289"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7844036697247706,"varying the number of layers in {1, 2, 3, · · · , 10, 15, · · · , 30} and running five times for each number
290"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.786697247706422,"of layers. We select the layer #K that achieves the best performance and report its average accuracy
291"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7889908256880734,"along with the standard deviation. The results are reported in Table 5.
292"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7912844036697247,"Our experiments show that GNNs with the SNR module outperform all previous methods (Q3).
293"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7935779816513762,"Additionally, we find that for most models, the number of layers to reach the best accuracy is relatively
294"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7958715596330275,"large, which indicates that it is necessary to perform more propagation to gather information from
295"
SEMI-SUPERVISED NODE CLASSIFICATION WITH MISSING VECTORS,0.7981651376146789,"further nodes so that we can obtain effective representations of nodes with missing features.
296"
EFFICIENCY EXPERIMENT,0.8004587155963303,"5.5
Efficiency Experiment
297"
EFFICIENCY EXPERIMENT,0.8027522935779816,"In real-world tasks, the rate at which a model achieves optimal performance through training is often
298"
EFFICIENCY EXPERIMENT,0.805045871559633,"important, and this affects the true effectiveness and time consumption of the model in real-world
299"
EFFICIENCY EXPERIMENT,0.8073394495412844,"applications. To enable concrete measurement and comparison, here we define the following metrics
300"
EFFICIENCY EXPERIMENT,0.8096330275229358,"for model training efficiency:
301"
EFFICIENCY EXPERIMENT,0.8119266055045872,Efficiency = Accuracy
EFFICIENCY EXPERIMENT,0.8142201834862385,"Time
(8)"
EFFICIENCY EXPERIMENT,0.8165137614678899,"where Accuracy denotes the accuracy of the model when it reaches its optimal performance and
302"
EFFICIENCY EXPERIMENT,0.8188073394495413,"Time denotes the time when the model reaches its optimal performance. The definition of this
303"
EFFICIENCY EXPERIMENT,0.8211009174311926,"formula shows that a larger Efficiency represents a higher performance per unit time improvement,
304"
EFFICIENCY EXPERIMENT,0.823394495412844,"and therefore a higher training efficiency.
305"
EFFICIENCY EXPERIMENT,0.8256880733944955,"Based on the above equation, we evaluate the training efficiency of vanilla GNNs and SNR-GNNs.
306"
EFFICIENCY EXPERIMENT,0.8279816513761468,"We use the 2, 4, 8, 16, 32, and 64-layer and average five Efficiency calculated for each layer of
307"
EFFICIENCY EXPERIMENT,0.8302752293577982,"the model. Specifically, each Efficiency is calculated based on the time for the model to reach the
308"
EFFICIENCY EXPERIMENT,0.8325688073394495,"highest accuracy on the validation set after 100 epochs of training and the accuracy achieved on the
309"
EFFICIENCY EXPERIMENT,0.8348623853211009,"test set at that time. Figure 3 shows the models’ Efficiency on Cora. The results on other datasets
310"
EFFICIENCY EXPERIMENT,0.8371559633027523,are shown in the Appendix H. It can be noticed that the training efficiency decreases as the number
EFFICIENCY EXPERIMENT,0.8394495412844036,Figure 3: Efficiency for different models at different layers. 311
EFFICIENCY EXPERIMENT,0.841743119266055,"of layers increases, which is due to the increase in training time caused by the rise in the number of
312"
EFFICIENCY EXPERIMENT,0.8440366972477065,"model parameters. However, in most cases, compared to vanilla GNNs, our SNR module is able to
313"
EFFICIENCY EXPERIMENT,0.8463302752293578,"maintain the highest training efficiency (Q4).
314"
CONCLUSION,0.8486238532110092,"6
Conclusion
315"
CONCLUSION,0.8509174311926605,"Our work proposes a new perspective for understanding the expressive power of GNNs: the k-hop
316"
CONCLUSION,0.8532110091743119,"subgraph aggregation theory. From this perspective, we have reinterpreted and experimentally
317"
CONCLUSION,0.8555045871559633,"validated the reason why the performance of message-passing GNNs decreases as the number of
318"
CONCLUSION,0.8577981651376146,"layers increases. Furthermore, we have evaluated the expressive power of previous residual-based
319"
CONCLUSION,0.8600917431192661,"GNNs based on this perspective. Building on these insights, we propose a new sampling-based
320"
CONCLUSION,0.8623853211009175,"generalized residual module SNR and show theoretically that SNR enables GNNs to more flexibly
321"
CONCLUSION,0.8646788990825688,"utilize information from multiple k-hop subgraphs, thus further improving the expressive power of
322"
CONCLUSION,0.8669724770642202,"GNNs. Extensive experiments demonstrate that the proposed SNR can effectively address the issues
323"
CONCLUSION,0.8692660550458715,"of overfitting in shallow layers and oversmoothing in deep layers that are commonly encountered in
324"
CONCLUSION,0.8715596330275229,"message-passing GNNs, and significantly improves the performance, particularly in SSNC-MV tasks.
325"
CONCLUSION,0.8738532110091743,"Our research will facilitate a deeper exploration of deep GNNs and enable a wider range of potential
326"
CONCLUSION,0.8761467889908257,"applications.
327"
REFERENCES,0.8784403669724771,"References
328"
REFERENCES,0.8807339449541285,"[1] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social
329"
REFERENCES,0.8830275229357798,"representations. In Proceedings of SIGKDD, pages 701–710, 2014.
330"
REFERENCES,0.8853211009174312,"[2] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. Graph
331"
REFERENCES,0.8876146788990825,"neural networks for social recommendation. In Proceedings of WWW, pages 417–426, 2019.
332"
REFERENCES,0.8899082568807339,"[3] David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli,
333"
REFERENCES,0.8922018348623854,"Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs
334"
REFERENCES,0.8944954128440367,"for learning molecular fingerprints. In Proceedings of NeurIPS, pages 2224–2232, 2015.
335"
REFERENCES,0.8967889908256881,"[4] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
336"
REFERENCES,0.8990825688073395,"for semi-supervised learning. In Proceedings of AAAI, pages 3538–3545, 2018.
337"
REFERENCES,0.9013761467889908,"[5] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and
338"
REFERENCES,0.9036697247706422,"Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In
339"
REFERENCES,0.9059633027522935,"Proceedings of ICML, pages 5449–5458, 2018.
340"
REFERENCES,0.908256880733945,"[6] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for
341"
REFERENCES,0.9105504587155964,"node classification. In Proceedings of ICLR, 2020.
342"
REFERENCES,0.9128440366972477,"[7] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate:
343"
REFERENCES,0.9151376146788991,"Graph neural networks meet personalized pagerank. In Proceedings of ICLR, 2019.
344"
REFERENCES,0.9174311926605505,"[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
345"
REFERENCES,0.9197247706422018,"recognition. In Proceedings of CVPR, pages 770–778, 2016.
346"
REFERENCES,0.9220183486238532,"[9] Guohao Li, Matthias Müller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as
347"
REFERENCES,0.9243119266055045,"deep as cnns? In Proceedings of ICCV, pages 9266–9275, 2019.
348"
REFERENCES,0.926605504587156,"[10] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
349"
REFERENCES,0.9288990825688074,"networks. In Proceedings of ICLR, 2017.
350"
REFERENCES,0.9311926605504587,"[11] Wei Jin, Xiaorui Liu, Yao Ma, Charu C. Aggarwal, and Jiliang Tang. Feature overcorrelation in
351"
REFERENCES,0.9334862385321101,"deep graph neural networks: A new perspective. In Proceedings of SIGKDD, pages 709–719,
352"
REFERENCES,0.9357798165137615,"2022.
353"
REFERENCES,0.9380733944954128,"[12] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-
354"
REFERENCES,0.9403669724770642,"Rad. Collective classification in network data. AI Magazine, 29(3):93–106, 2008.
355"
REFERENCES,0.9426605504587156,"[13] Aleksandar Bojchevski and Stephan Günnemann. Deep gaussian embedding of graphs: Unsu-
356"
REFERENCES,0.944954128440367,"pervised inductive learning via ranking. In Proceedings of ICLR, 2018.
357"
REFERENCES,0.9472477064220184,"[14] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding.
358"
REFERENCES,0.9495412844036697,"Journal of Complex Networks, 9(2), 2021.
359"
REFERENCES,0.9518348623853211,"[15] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, P. Lio’, and Yoshua
360"
REFERENCES,0.9541284403669725,"Bengio. Graph attention networks. Proceedings of ICLR, 2017.
361"
REFERENCES,0.9564220183486238,"[16] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
362"
REFERENCES,0.9587155963302753,"convolutional networks. In Proceedings of ICML, volume 119, pages 1725–1735, 2020.
363"
REFERENCES,0.9610091743119266,"[17] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
364"
REFERENCES,0.963302752293578,"by reducing internal covariate shift. In Proceedings of ICML, volume 37, pages 448–456, 2015.
365"
REFERENCES,0.9655963302752294,"[18] Lingxiao Zhao and L. Akoglu. Pairnorm: Tackling oversmoothing in gnns. Proceedings of
366"
REFERENCES,0.9678899082568807,"ICLR, 2019.
367"
REFERENCES,0.9701834862385321,"[19] Kaixiong Zhou, Xiao Huang, Yuening Li, D. Zha, Rui Chen, and Xia Hu. Towards deeper graph
368"
REFERENCES,0.9724770642201835,"neural networks with differentiable group normalization. Proceedings of NeurIPS, 2020.
369"
REFERENCES,0.9747706422018348,"[20] Y. Rong, Wen-bing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
370"
REFERENCES,0.9770642201834863,"convolutional networks on node classification. Proceedings of ICLR, 2019.
371"
REFERENCES,0.9793577981651376,"[21] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings
372"
REFERENCES,0.981651376146789,"of ICLR, 2015.
373"
REFERENCES,0.9839449541284404,"[22] Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou,
374"
REFERENCES,0.9862385321100917,"Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang
375"
REFERENCES,0.9885321100917431,"Li, Alexander J. Smola, and Zheng Zhang. Deep graph library: Towards efficient and scalable
376"
REFERENCES,0.9908256880733946,"deep learning on graphs. CoRR, abs/1909.01315, 2019.
377"
REFERENCES,0.9931192660550459,"[23] Al Mamunur Rashid, George Karypis, and John Riedl. Learning preferences of new users in
378"
REFERENCES,0.9954128440366973,"recommender systems: An information theoretic approach. Proceedings of SIGKDD, 10(2):90–
379"
REFERENCES,0.9977064220183486,"100, 2008.
380"
