Section,Section Appearance Order,Paragraph
TENCENT AI LAB,0.0,"1 Tencent AI Lab
2 ETH Zürich"
ABSTRACT,0.0018726591760299626,ABSTRACT
ABSTRACT,0.003745318352059925,"Valuation problems, such as feature interpretation, data valuation and model val-
uation for ensembles, become increasingly more important in many machine
learning applications. Such problems are commonly addressed via well-known
game-theoretic criteria, such as the Shapley value or Banzhaf value. In this work,
we present a novel energy-based treatment for cooperative games, with a theo-
retical justiﬁcation via the maximum entropy principle. Surprisingly, through
mean-ﬁeld variational inference in the energy-based model, we recover classical
game-theoretic valuation criteria by conducting one-step of ﬁxed point iteration for
maximizing the ELBO objective. This observation also further supports existing
criteria, as they can be seen as attempting to decouple the correlations among
players. By running the ﬁxed point iteration for multiple steps, we achieve a trajec-
tory of the variational valuations, among which we deﬁne the valuation with the
best conceivable decoupling error as the Variational Index. We prove that under
uniform initialization, these variational valuations all satisfy a set of game-theoretic
axioms. We empirically demonstrate that the proposed variational valuations enjoy
lower decoupling error and better valuation performance on certain synthetic and
real-world valuation problems.1"
INTRODUCTION,0.0056179775280898875,"1
INTRODUCTION"
INTRODUCTION,0.00749063670411985,Energy Based Model for
INTRODUCTION,0.009363295880149813,"a cooperative game (𝑁, 𝐹S ):"
INTRODUCTION,0.011235955056179775,𝑝S ∝exp 𝐹S /𝑇
INTRODUCTION,0.013108614232209739,"Player valuations in 
cooperative games ?"
INTRODUCTION,0.0149812734082397,"Valuation 
problems in ML"
INTRODUCTION,0.016853932584269662,"q Feature interpretation
q Data valuation
q Model valuation"
INTRODUCTION,0.018726591760299626,Shapley
INTRODUCTION,0.020599250936329586,Banzhaf
INTRODUCTION,0.02247191011235955,"Mean field variational parameters
𝐱as principled player valuations"
INTRODUCTION,0.024344569288389514,"min
𝐱KL(𝑞S; 𝐱||𝑝S )"
INTRODUCTION,0.026217228464419477,"𝑝S
𝑞S; 𝐱 𝐱! 𝐱∗ 𝐱!"
INTRODUCTION,0.028089887640449437,"Recovering
- Shapley
- Banzhaf ⋯ 𝐱𝐾"
INTRODUCTION,0.0299625468164794,"𝐾-step
Variational Values"
INTRODUCTION,0.031835205992509365,"fixed point
iteration"
INTRODUCTION,0.033707865168539325,"Figure 1: An energy based treatment of coop-
erative games leads to a series of new player
valuations: K-step variational values, satis-
fying basic valuation axioms: null player,
marginalism & symmetry."
INTRODUCTION,0.035580524344569285,"Valuation problems are becoming increasingly more
signiﬁcant in various machine learning applications,
ranging from feature interpretation (Lundberg and
Lee, 2017), data valuation (Ghorbani and Zou, 2019)
to model valuation for ensembles (Rozemberczki and
Sarkar, 2021). They are often formulated as a player
valuation problem in cooperative games. A cooper-
ative game (N, F(S)) consists of a grand coalition
N = {1, ..., n} of n players and a value function
(a.k.a. characteristic function) F(S) : 2N →R de-
scribing the collective payoff of a coalition/coopera-
tion S. A fundamental problem in cooperative game
theory is to assign an importance vector (i.e., solution
concept) φ(F) ∈Rn to n players."
INTRODUCTION,0.03745318352059925,"In this paper, we explore a probabilistic treatment
of cooperative games (N, F(S)). Such a treatment
makes it possible to conduct learning and inference
in a uniﬁed manner, and will yield connections with classical valuation criteria.
Concretely, we
seek a probability distribution over coalitions p(S = S)2, measuring the odds that a speciﬁc coalition"
INTRODUCTION,0.03932584269662921,"∗Correspondence to: Yatao Bian <yatao.bian@gmail.com>
1Project page & code: https://valuationgame.github.io
2Note that distributions over subsets of N are equivalent to distributions of |N| = n binary random variables
X1, ..., Xn ∈{0, 1}: We use Xi as the indicator function of the event i ∈S, or Xi = [i ∈S]. With slight
abuse of notation, we use S as a random variable represented as sets and often abbreviate p(S = S) as p(S)."
INTRODUCTION,0.04119850187265917,Published as a conference paper at ICLR 2022
INTRODUCTION,0.04307116104868914,"S happens. Generally, we consider distributions where the probability of a coalition p(S) grows
monotonically with the payoff F(S)."
INTRODUCTION,0.0449438202247191,"Among all the possible probability mass functions (pmfs), how should we construct the proper
p(S)? We advocate to choose the pmf with the maximum entropy H(p). This principle makes sense
since maximizing the entropy minimizes the amount of prior information built into the distribution.
In other words, it amounts to assuming nothing about what is unknown, i.e., choosing the most
“uniform” distribution. Now ﬁnding a proper p(S) becomes the following constrained optimization
problem: suppose each coalition S is associated with a payoff F(S) with probability p(S). We
would like to maximize the entropy H(p) = −P"
INTRODUCTION,0.04681647940074907,"S⊆N p(S) log p(S), subject to the constraints that
P"
INTRODUCTION,0.04868913857677903,"S p(S) = 1, p(S) ≥0 and P"
INTRODUCTION,0.05056179775280899,"S p(S)F(S) = µ (i.e., the average payoff is known as µ). Solving
this optimization problem (derivation in Appendix A), we reach the maximum entropy distribution:"
INTRODUCTION,0.052434456928838954,p(S) = exp(F(S)/T)
INTRODUCTION,0.054307116104868915,"Z
,
Z :=
X"
INTRODUCTION,0.056179775280898875,"S′⊆N exp(F(S′)/T),
(1)"
INTRODUCTION,0.05805243445692884,"where T > 0 is the temperature. This is an energy-based model (EBM, cf. LeCun et al., 2006) with
−F(S) as the energy function."
INTRODUCTION,0.0599250936329588,"The above energy-based treatment admits two beneﬁts: i) Where supervision is available, it enables
learning of value functions F(S) through efﬁcient training techniques for energy-based learning,
such as noise contrastive estimation (Gutmann and Hyvärinen, 2010) and score matching (Hyvärinen,
2005). ii) Approximate inference techniques such as variational inference or sampling can be adopted
to solve the valuation problem. Speciﬁcally, it enables to perform mean-ﬁeld variational inference
where parameters of the inferred surrogate distribution can be used as principled player valuations."
INTRODUCTION,0.06179775280898876,"Below, we explore mean-ﬁeld variational inference for the energy-based formulation (Fig. 1). Perhaps
surprisingly, by conducting only one-step ﬁxed point iteration for maximizing the mean-ﬁeld (ELBO)
objective, we recover classical valuation criteria, such as the Shapley value (Shapley, 1953) and the
Banzhaf value (Penrose, 1946; Banzhaf III, 1964). This observation also further supports existing
criteria, motivating them as decoupling the correlations among players via the mean-ﬁeld approach.
By running the ﬁxed point iteration for multiple steps, we achieve a trajectory of valuations, among
which we deﬁne the valuation with the best conceivable decoupling error as the Variational Index.
Our major contributions can be summarized as below:"
INTRODUCTION,0.06367041198501873,"i) We present a theoretically justiﬁed energy-based treatment for cooperative games. Through mean
ﬁeld inference, we provide a uniﬁed perspective on popular game-theoretic criteria. This provides an
alternative motivation of existing criteria via a decoupling perspective, i.e., decoupling correlations
among n players through the mean-ﬁeld approach. ii) In pursuit of better decoupling performance,
we propose to run ﬁxed point iteration for multiple steps, which generates a trajectory of valuations.
Under uniform initializations, they all satisfy a set of game-theoretic axioms, which are required for
being suitable valuation criteria. We deﬁne the valuation with the best conceivable decoupling error as
the Variational Index. iii) Synthetic and real-world experiments demonstrate intriguing properties of
the proposed Variational Index, including lower decoupling error and better valuation performance."
PRELIMINARIES AND BACKGROUND,0.06554307116104868,"2
PRELIMINARIES AND BACKGROUND"
PRELIMINARIES AND BACKGROUND,0.06741573033707865,"Notation. We assume ei ∈Rn being the standard ith basis vector and use boldface letters x ∈RN
and x ∈Rn interchangebly to indicate an n-dimensional vector, where xi is the ith entry of x. By
default, f(·) is used to denote a continuous function, and F(·) to represent a set function. For a
differentiable function f(·), ∇f(·) denotes its gradient. x|xi ←k is the operation of setting the ith
element of x to k, while keeping all other elements unchanged, i.e., x|xi ←k = x −xiei + kei. For
two sets S and T, S + T and S −T represent set union and set difference, respectively. |S| is the
cardinality of S. i is used to denote the singleton {i} with a bit abuse of notation."
PRELIMINARIES AND BACKGROUND,0.06928838951310862,"Existing valuation criteria. Various valuation criteria have been proposed from the area of coop-
erative games, amongst them the most famous ones are the Shapley value (Shapley, 1953) and the
Banzhaf value, which is extended from the Banzhaf power index (Penrose, 1946; Banzhaf III, 1964).
For the Shapley value, the importance assigned to player i is:"
PRELIMINARIES AND BACKGROUND,0.07116104868913857,"Shi =
X"
PRELIMINARIES AND BACKGROUND,0.07303370786516854,S⊆N−i[F(S + i) −F(S)]|S|!(n −|S| −1)!
PRELIMINARIES AND BACKGROUND,0.0749063670411985,"n!
.
(2)"
PRELIMINARIES AND BACKGROUND,0.07677902621722846,Published as a conference paper at ICLR 2022
PRELIMINARIES AND BACKGROUND,0.07865168539325842,"One can see that it gives less weight to n/2-sized coalitions. The Banzhaf value assigns the following
importance to player i:"
PRELIMINARIES AND BACKGROUND,0.08052434456928839,"Bai =
X"
PRELIMINARIES AND BACKGROUND,0.08239700374531835,"S⊆N−i[F(S + i) −F(S)]
1
2n−1 ,
(3)"
PRELIMINARIES AND BACKGROUND,0.08426966292134831,which uses uniform weights for all the coalitions. See Greco et al. (2015) for a comparison of them.
PRELIMINARIES AND BACKGROUND,0.08614232209737828,"Valuation problems in machine learning. Currently, most classes of valuation problems (Lundberg
and Lee, 2017; Ghorbani and Zou, 2019; Sim et al., 2020; Rozemberczki and Sarkar, 2021) use
Shapley value as the valuation criterion. Along with the rapid progress of model interpretation in the
past decades (Zeiler and Fergus, 2014; Ribeiro et al., 2016; Lundberg and Lee, 2017; Sundararajan
et al., 2017; Petsiuk et al., 2018; Wang et al., 2021a), attribution-based interpretation aims to assign
importance to the features for a speciﬁc data instance (x ∈RN, y) given a black-box model M. Here
each feature maps to a player in the game (N, F(S)), and the value function F(S) is usually the
model response, such as the predicted probability for classiﬁcation problems, when feeding a subset
S of features to M. The data valuation problem (Ghorbani and Zou, 2019) tries to assign values
to the samples in the training dataset N = {(xi, yi)}n
1 for general supervised machine learning:
one training sample corresponds to one player, and the value function F(S) indicates the predictor
performance on some test dataset given access to only a subset of the training samples in S. Model
valuation in ensembles (Rozemberczki and Sarkar, 2021) measures importance of individual models
in an ensemble in order to correctly label data points from a dataset, where each pre-trained model
maps to a player and the value function measures the predictive performance of subsets of models."
RELATED WORK,0.08801498127340825,"3
RELATED WORK"
RELATED WORK,0.0898876404494382,"Energy-based modeling. Energy based learning (LeCun et al., 2006) is a classical learning frame-
work that uses an energy function E(x) to measure the quality of a data point x. Energy based models
have been applied to different domains, such as data generation (Deng et al., 2020), out-of-distribution
detection (Liu et al., 2020), reinforcement learning (Haarnoja et al., 2017), memory modeling (Bar-
tunov et al., 2019), discriminative learning (Grathwohl et al., 2019; Gustafsson et al., 2020) and
biologically-plausible training (Scellier and Bengio, 2017). Energy based learning admits princi-
pled training methods, such as contrastive divergence (Hinton, 2002), noise contrastive estimation
(Gutmann and Hyvärinen, 2010) and score matching (Hyvärinen, 2005). For approximate inference,
sampling based approaches are mainly MCMC-style algorithms, such as stochastic gradient Langevin
dynamics (Welling and Teh, 2011). For a wide class of EBMs with submodular or supermodular
energies (Djolonga and Krause, 2014), there exist provable mean ﬁeld inference algorithms with
constant factor approximation guarantees (Bian et al., 2019; Sahin et al., 2020; Bian et al., 2020)."
RELATED WORK,0.09176029962546817,"Shapley values in machine learning. Shapley values have been extensively used for valuation
problems in machine learning, including attribution-based interpretation (Lipovetsky and Conklin,
2001; Cohen et al., 2007; Strumbelj and Kononenko, 2010; Owen, 2014; Datta et al., 2016; Lundberg
and Lee, 2017; Chen et al., 2018; Lundberg et al., 2018; Kumar et al., 2020; Williamson and Feng,
2020; Covert et al., 2020b; Wang et al., 2021b), data valuation (Ghorbani and Zou, 2019; Jia et al.,
2019b;a; Wang et al., 2020; Fan et al., 2021), collaborative machine learning (Sim et al., 2020) and
recently, model valuation in ensembles (Rozemberczki and Sarkar, 2021). For a detailed overview
of papers using Shapley values for feature interpretation, please see Covert et al. (2020a) and the
references therein. To alleviate the exponential computational cost of exact evaluation, various
methods have been proposed to approximate Shapley values in polynomial time (Ancona et al., 2017;
2019). Owen (1972) proposes the multilinear extension purely as a representation of cooperative
games and Okhrati and Lipani (2021) use it to develop sampling algorithms for Shapley values."
RELATED WORK,0.09363295880149813,"4
VALUATION FOR COOPERATIVE GAMES: A DECOUPLING PERSPECTIVE"
RELATED WORK,0.09550561797752809,"In the introduction, we have asserted that under the setting of cooperative games, the Boltzmann
distribution (see Eq. (1)) achieves the maximum entropy among all of the pmf functionals. One can
naturally view the importance assignment problem of cooperative games as a decoupling problem:
The n players in a game (N, F(S)) might be arbitrarily correlated in a very complicated manner.
However, in order to assign each of them an individual importance value, we have to decouple their
interactions, which can be viewed as a way to simplify their correlations."
RELATED WORK,0.09737827715355805,Published as a conference paper at ICLR 2022
RELATED WORK,0.09925093632958802,"We therefore consider a surrogate distribution q(S; x) governed by parameters in x. q has to be simple,
given our intention to decouple the correlations among the n players. A natural choice is to restrain
q(S; x) to be fully factorizable, which leads to a mean-ﬁeld approximation of p(S). The simplest
form of q(S; x) would be a n independent Bernoulli distribution, i.e., q(S; x) := Q"
RELATED WORK,0.10112359550561797,"i∈S xi
Q"
RELATED WORK,0.10299625468164794,"j /∈S(1−
xj), x ∈[0, 1]n. Given a divergence measure D(·∥·) for probability distributions, we can deﬁne the
best conceivable decoupling error to be the divergence between p and the best possible q.
Deﬁnition 1 (Best Conceivable Decoupling Error). Considering a cooperative game (N, F(S)), and
given a divergence measure D(·∥·) for probability distributions, the decoupling error is deﬁned as
the divergence between q and p: D(q∥p), and the best conceivable decoupling error is deﬁned as the
divergence between the best possible q and p:
D∗:= min
q
D(q∥p).
(4)"
RELATED WORK,0.10486891385767791,"Note that the best conceivable decoupling error D∗is closely related to the intrinsic coupling amongst
n players: if all the players are already independent with each other, then D∗could be zero."
MEAN FIELD OBJECTIVE FOR EBMS,0.10674157303370786,"4.1
MEAN FIELD OBJECTIVE FOR EBMS"
MEAN FIELD OBJECTIVE FOR EBMS,0.10861423220973783,"If we consider the decoupling error D(q∥p) to be the Kullback-Leibler divergence between q and
p, then we recover the mean ﬁeld approach3. Given the EBM formulation in Eq. (1), the classical
mean-ﬁeld inference approach aims to approximate p(S) by a fully factorized product distribution
q(S; x) := Q"
MEAN FIELD OBJECTIVE FOR EBMS,0.1104868913857678,"i∈S xi
Q"
MEAN FIELD OBJECTIVE FOR EBMS,0.11235955056179775,"j /∈S(1 −xj), x ∈[0, 1]n, by minimizing the distance measured w.r.t. the
Kullback-Leibler divergence between q and p. Since KL(q∥p) is non-negative, we have:"
MEAN FIELD OBJECTIVE FOR EBMS,0.11423220973782772,"0 ≤KL(q∥p) =
X"
MEAN FIELD OBJECTIVE FOR EBMS,0.11610486891385768,S⊆N q(S; x) log q(S; x)
MEAN FIELD OBJECTIVE FOR EBMS,0.11797752808988764,"p(S)
= −Eq(S;x)[log p(S)] −H(q(S; x))
(5)"
MEAN FIELD OBJECTIVE FOR EBMS,0.1198501872659176,= −Eq(S;x)[F(S)
MEAN FIELD OBJECTIVE FOR EBMS,0.12172284644194757,"T
−log Z] −H(q(S; x))
(6) = −
X S⊆N F(S) T Y"
MEAN FIELD OBJECTIVE FOR EBMS,0.12359550561797752,"i∈S
xi
Y"
MEAN FIELD OBJECTIVE FOR EBMS,0.1254681647940075,"j /∈S
(1 −xj) +
Xn"
MEAN FIELD OBJECTIVE FOR EBMS,0.12734082397003746,"i=1[xi log xi + (1 −xi) log(1 −xi)] + log Z.
(7)"
MEAN FIELD OBJECTIVE FOR EBMS,0.12921348314606743,In Eq. (6) we plug in the EBM formulation that log p(S) = F (S)
MEAN FIELD OBJECTIVE FOR EBMS,0.13108614232209737,"T
−log Z. Then one can get"
MEAN FIELD OBJECTIVE FOR EBMS,0.13295880149812733,"log Z ≥
X S⊆N F(S) T Y"
MEAN FIELD OBJECTIVE FOR EBMS,0.1348314606741573,"i∈S
xi
Y"
MEAN FIELD OBJECTIVE FOR EBMS,0.13670411985018727,"j /∈S
(1 −xj) −
Xn"
MEAN FIELD OBJECTIVE FOR EBMS,0.13857677902621723,i=1[xi log xi + (1 −xi) log(1 −xi)]
MEAN FIELD OBJECTIVE FOR EBMS,0.1404494382022472,"= f F
mt(x)"
MEAN FIELD OBJECTIVE FOR EBMS,0.14232209737827714,"T
+ H(q(S; x)) := (ELBO)
(8)"
MEAN FIELD OBJECTIVE FOR EBMS,0.1441947565543071,"where H(·) is the entropy, ELBO stands for the evidence lower bound, and"
MEAN FIELD OBJECTIVE FOR EBMS,0.14606741573033707,"f F
mt(x) :=
X"
MEAN FIELD OBJECTIVE FOR EBMS,0.14794007490636704,"S⊆N F(S)
Y"
MEAN FIELD OBJECTIVE FOR EBMS,0.149812734082397,"i∈S xi
Y"
MEAN FIELD OBJECTIVE FOR EBMS,0.15168539325842698,"j /∈S(1 −xj), x ∈[0, 1]n,
(9)"
MEAN FIELD OBJECTIVE FOR EBMS,0.15355805243445692,"is the multilinear extension of F(S) (Owen, 1972; Calinescu et al., 2007). Note that the multilinear
extension plays a central role in modern combinatorial optimizaiton techniques (Feige et al., 2011),
especially for guaranteed submodular maximization problems (Krause and Golovin, 2014)."
MEAN FIELD OBJECTIVE FOR EBMS,0.15543071161048688,"Maximizing (ELBO) in Eq. (8) amounts to minimizing the Kullback-Leibler divergence between q
and p. If one solves this optimization problem to optimality, one can obtain the q(S; x∗) with the
best conceivable decoupling error. Here x∗
i describes the odds that player i shall participate in the
game, so it can be naturally used to deﬁne the importance score of each player.
Deﬁnition 2 (Variational Index of Cooperative Games). Consider a cooperative game (N, F(S))
and its mean ﬁeld approximation. Let x∗be the variational marginals with the best conceivable
decoupling error, we deﬁne s∗:= Tσ−1(x∗) to be the variational index of the game. Formally,
x∗= arg minxKL(q(S; x)∥p(S)),
(10)"
MEAN FIELD OBJECTIVE FOR EBMS,0.15730337078651685,"where x∗can be obtained by maximizing the ELBO objective in Eq. (8), and σ−1(·) is the inverse of
the sigmoid function, i.e. σ−1(x) = log
x
1−x. For a vector it is applied element-wise."
MEAN FIELD OBJECTIVE FOR EBMS,0.15917602996254682,"3Notably, one could also apply the reverse KL divergence KL(p∥q), which would lead to an expectation
propagation (Minka, 2001) treatment of cooperative games."
MEAN FIELD OBJECTIVE FOR EBMS,0.16104868913857678,Published as a conference paper at ICLR 2022
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.16292134831460675,"4.2
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.1647940074906367,"Equilibrium condition. For coordinate i, the partial derivative of the multilinear extension is
∇if F
mt(x), and for the entropy term, it is ∇iH(q(S; x)) = log 1−xi"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.16666666666666666,"xi . By setting the partial derivative
of ELBO in Eq. (8) to be 0, we have the equilibrium condition:"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.16853932584269662,"x∗
i = σ(∇if F
mt(x∗)/T) =
 
1 + exp(−∇if F
mt(x∗)/T
−1,
∀i ∈N,
(11)"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.1704119850187266,"where σ is the sigmoid function. This equilibrium condition implies that one cannot change the value
assigned to any player in order to further improve the overall decoupling performance. It also implies
the ﬁxed point iteration xi ←σ(∇if F
mt(x)/T). When updating each coordinate sequentially, we
recover the classic naive mean ﬁeld algorithm as shown in Appendix C."
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.17228464419475656,"Instead, here we suggest to use the full-gradient method shown in Alg. 1 for maximizing the ELBO
objective. As we will see later, the resultant valuations satisfy certain game-theoretic axioms. It needs
an initial marginal vector x0 ∈[0, 1]n and the number of epochs K. After K steps of ﬁxed point
iteration, it returns the estimated marginal xK."
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.17415730337078653,Algorithm 1: Mean Field Inference with Full Gradient: MFI(x; K)
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.1760299625468165,"Input: A cooperative game (N, F(S)) with n players. Initial marginals x0 ←x ∈[0, 1]n.
#epochs K.
Output: Marginals after K steps of iteration: xK"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.17790262172284643,1 for k = 1 →K do
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.1797752808988764,"2
xk ←σ(∇f F
mt(xk−1)/T) =
 
1 + exp(−∇f F
mt(xk−1)/T
−1 ;"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.18164794007490637,"In case Alg. 1 solves the optimization problem to optimality, we obtain the Variational Index. How-
ever, maximizing ELBO is in general a non-convex/non-concave problem, and hence one can only
ensure reaching a stationary solution. Below, when we say Variational Index, we therefore refer to its
approximation obtained via Alg. 1 by default. Meanwhile, the MFI(x; K) subroutine also deﬁnes
a series of marginals, which enjoy interesting properties as we show in the next part. So we deﬁne
variational valuations through intermediate solutions of MFI(x; K)."
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.18352059925093633,"Deﬁnition 3 (K-Step Variational Values). Considering a cooperative game (N, F(S)) and its mean
ﬁeld approximation by Alg. 1, we deﬁne the K-Step Variational Values initialized at x as:"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.1853932584269663,"Tσ−1(MFI(x; K)),
(12)"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.18726591760299627,"where σ−1() is the inverse of the sigmoid function (σ−1(x) = log
x
1−x)."
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.1891385767790262,"Notice when running more steps, the K-Step variational value will be more close to the
Variational Index. The gradient ∇f F
mt(x) itself is deﬁned with respect to an exponential sum via the
multilinear extension. Next we show how it can be approximated via principled sampling methods."
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.19101123595505617,"Sampling methods for estimating the partial derivative. The partial derivative follows,"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.19288389513108614,"∇if F
mt(x) = Eq(S;(x|xi←1))[F(S)] −Eq(S;(x|xi←0))[F(S)]
(13)"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.1947565543071161,"= f F
mt(x|xi ←1) −f F
mt(x|xi ←0) =
X"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.19662921348314608,"S⊆N,S∋i
F(S)
Y"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.19850187265917604,"j∈S−i
xj
Y"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.20037453183520598,"j′ /∈S
(1 −xj′)
−
X"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.20224719101123595,"S⊆N−i
F(S)
Y"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.20411985018726592,"j∈S
xj
Y"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.20599250936329588,"j′ /∈S,j′̸=i
(1 −xj′) =
X"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.20786516853932585,"S⊆N−i
[F(S + i) −F(S)]
Y"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.20973782771535582,"j∈S
xj
Y"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.21161048689138576,"j′∈N−S−i
(1 −xj′) =
X"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.21348314606741572,"S⊆N−i
[F(S + i) −F(S)]q(S; (x|xi ←0)) = ES∼q(S;(x|xi←0)) [F(S + i) −F(S)]."
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.2153558052434457,"All of the variational criteria are based on the calculation of the partial derivative ∇if F
mt(x), which can
be approximated by Monte Carlo sampling since ∇if F
mt(x) = ES∼q(S;(x|xi←0)) [F(S + i) −F(S)]:
we ﬁrst sample m coalitions Sk, k = 1, ..., m from the surrogate distribution q(S; (x|xi ←0)),
then approximate the expectation by the average 1"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.21722846441947566,"m
Pm
k=1 [F(Sk + i) −F(Sk)]. According to the"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.21910112359550563,Published as a conference paper at ICLR 2022
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.2209737827715356,"Chernoff-Hoeffding bound (Hoeffding, 1963), the approximation will be arbitrarily close to the true
value with increasingly more samples: With probability at least 1 −exp(−mϵ2/2), it holds that
| 1"
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.22284644194756553,"m
Pm
k=1[F(Sk + i) −F(Sk)] −∇if F
mt(x)| ≤ϵ maxS |F(S + i) −F(S)|, for all ϵ > 0."
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.2247191011235955,"Roles of the initializer x0 and the temperature T. This can be understood in the following respects:
1) The initializer x0 represents the initial credit assignments to the n players, so it denotes the prior
knowledge/initial belief of the contributions of the players; 2) If one just runs Alg. 1 for one step, x0
matters greatly to the output. However, if one runs Alg. 1 for many steps, xk will converge to the
stationary points of the ELBO objective. Empirically, it takes around 5∼10 steps to converge. The
temperature T controls the “spreading” of importance assigned to the players: A higher T leads to
ﬂatter assignments, and a lower T leads to more concentrated assignments."
ALGORITHMS FOR CALCULATING THE VARIATIONAL INDEX,0.22659176029962547,"Computational efﬁciency of calculating Variational Index and variational values. Alg. 1 calcu-
lates the K-step variational values, 1-step variational value has the same computational cost as that
of Banzhaf value and of the integrand of the line integration of Shapley value in Eq. (15) below, since
they all need to evaluate ∇f F
mt(x). Sampling methods could help with approximating all of the three
criteria when there are a large number of players. The Variational Index can be approximated by the
K-step variational value, where the number K depends on when Alg. 1 converges. One can easily
show that, under the setting of maximizing ELBO, xk will converge to some stationary point x∗,
based on the analysis of mean ﬁeld approximation in Wainwright and Jordan (2008). We have also
empirically veriﬁed the convergence rate of Alg. 1 in Sec. 5.3, and ﬁnd that it converges within 5 to
10 steps. So the computational cost is roughly similar as that of Shapley value and Banzhaf value."
RECOVERING CLASSICAL CRITERIA,0.22846441947565543,"4.3
RECOVERING CLASSICAL CRITERIA"
RECOVERING CLASSICAL CRITERIA,0.2303370786516854,"Perhaps surprisingly, it is possible to recover classical valuation criteria via the K-step variational
values as in Def. 3. Firstly, for Banzhaf value, by comparing with Eq. (13) it reads,"
RECOVERING CLASSICAL CRITERIA,0.23220973782771537,"Bai =
X"
RECOVERING CLASSICAL CRITERIA,0.2340823970037453,"S⊆N−i[F(S + i) −F(S)]
1
2n−1 = ∇if F
mt(0.5 ∗1) = Tσ−1(MFI(0.5 ∗1; 1)),
(14)"
RECOVERING CLASSICAL CRITERIA,0.23595505617977527,"which is the 1-step variational value initialied at 0.5 ∗1. We can also recover the Shapley value
through its connection to the multilinear extension (Owen, 1972; Grabisch et al., 2000):"
RECOVERING CLASSICAL CRITERIA,0.23782771535580524,"Shi =
Z 1"
RECOVERING CLASSICAL CRITERIA,0.2397003745318352,"0
∇if F
mt(x1)dx =
Z 1"
RECOVERING CLASSICAL CRITERIA,0.24157303370786518,"0
Tσ−1(MFI(x1; 1))dx,
(15)"
RECOVERING CLASSICAL CRITERIA,0.24344569288389514,"where the integration denotes integrating the partial-derivative of the multilinear extension along
the main diagonal of the unit hypercube. A self-contained proof is given in Appendix D."
RECOVERING CLASSICAL CRITERIA,0.24531835205992508,"These insights offer a novel, uniﬁed interpretation of the two classical valuation indices: both the
Shapley value and Banzhaf value can be viewed as approximating the variational index by running
one step of ﬁxed point iteration for the decoupling (ELBO) objective. Speciﬁcally, for the Banzhaf
value, it initializes x at 0.5 ∗1, and runs one step of ﬁxed point iteration. For the Shapley value, it
also performs a one-step ﬁxed point approximation. However, instead of starting at a single initial
point, it averages over all possible initializations through the line integration in Eq. (15)."
RECOVERING CLASSICAL CRITERIA,0.24719101123595505,"Relation to probabilistic values. Probabilistic values for games (Weber, 1988; Monderer and Samet,
2002) capture a class of solution concepts, where the value of each player is given by some averaging
of the player’s marginal contributions to coalitions, and the weights depend on the coalitions only.
According to (Monderer and Samet, 2002, Equation (3.1)), a solution φ is called a probabilistic value,
if for each player i, there exists a probability pi ∈∆(Ci), such that φi is the expected marginal
contribution of i w.r.t. pi. Namely, φi = P"
RECOVERING CLASSICAL CRITERIA,0.24906367041198502,"S∈Ci pi(S)[F(S + i) −F(S)], where Ci is the set of all
subsets of N −i, and ∆(Ci) is the set of all probability measures on Ci. One can easily see that, for
any ﬁxed x, 1-step variational value in Def. 3 is a probabilistic value with pi(S) = q(S; (x|xi ←0)),
where q(S; x) is the surrogate distribution in our EBM framework."
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.250936329588015,"4.4
AXIOMATISATION OF K-STEP VARIATIONAL VALUES"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.25280898876404495,"Our EBM framework introduces a series of variational values controlled by T and the running
step number K. We now establish that the variational values Tσ−1(MFI(x; K)) in Def. 3 satisfy
certain game-theoretic axioms (see Appendix B for deﬁnitions of ﬁve common axioms: Null player,"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.2546816479400749,Published as a conference paper at ICLR 2022
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.2565543071161049,"breast cancer
synthetic
digits"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.25842696629213485,"14 groups, kmeans
𝑇= 0.5"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.2602996254681648,"16 groups, random,
𝑇= 0.1"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.26217228464419473,"12 groups,
random,
𝑇= 0.5"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.2640449438202247,"20 groups,
each w. 1 sample,
𝑇= 0.5"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.26591760299625467,"20 groups, each w.
50 samples, kmeans
𝑇= 1"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.26779026217228463,"10 groups,
kmeans,
𝑇= 0.1"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.2696629213483146,"Figure 2: Data removal results. Numbers in the legend are the decoupling errors. Columns: 1st:
synthetic data; 2nd: breast cancer data with 569 samples; 3rd: digits data with 1797 samples. Speciﬁc
conﬁgurations (e.g., temperature) are put inside the ﬁgure texts."
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.27153558052434457,"Symmetry, Marginalism, Additivity and Efﬁciency). We prove that all the variational values in the
trajectory satisfy three fundamental axioms: null player, marginalism and symmetry. The detailed
proof is deferred to Appendix E. We expect it to be very difﬁcult to ﬁnd equivalent axiomatisations
of the series of variational values, which we leave for future work. Meanwhile, our methods incur
a decoupling and fairness tradeoff by tuning the hyperparameters K and T."
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.27340823970037453,"Theorem 1 (Axiomatisation of K-Step Variational Values of Def. 3). If initialized uniformly, i.e.,
x0 = x1, x ∈[0, 1], all the variational values in the trajectory Tσ−1(MFI(x; k)), k = 1, 2, 3...
satisfy the null player, marginalism and symmetry axioms."
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.2752808988764045,"According to Theorem 1, our proposed K-step variational values satisfy the minimal set of axioms
often associated with appropriate valuation criteria. Note that speciﬁc realizations of the K-step
variational values can also satisfy more axioms, for example, the 1-step variational value initialized
at 0.5 ∗1 also satisﬁes the additivity axiom. Furthermore, we have the following observations:"
AXIOMATISATION OF K-STEP VARIATIONAL VALUES,0.27715355805243447,"Satisfying more axioms is not essential for valuation problems. Notably, in cooperative game
theory, one line of work is to seek for solution concepts that would satisfy more axioms. However,
for valuation problems in machine learning, this is arguably not essential. For example, similar as
argued by Ridaoui et al. (2018), efﬁciency does not make sense for certain games. We give a simple
illustration in Appendix F, which further shows that whether more axioms shall be considered really
depends on the speciﬁc scenario being modeled, which will be left for important future work."
EMPIRICAL STUDIES,0.27902621722846443,"5
EMPIRICAL STUDIES"
EMPIRICAL STUDIES,0.2808988764044944,"Throughout the experiments, we are trying to understand the following: 1) Would the pro-
posed Variational Index have lower decoupling error compared to others? 2) Could the proposed
Variational Index gain beneﬁts compared to the classical valuation criteria for valuation problems?"
EMPIRICAL STUDIES,0.28277153558052437,"Since we are mainly comparing the quality of different criteria, it is necessary to rule out the inﬂuence
of approximation errors when estimating their values. So we focus on small-sized problems where
one can compute the exact values of these criteria in a reasonable time. Usually this requires the
number of players to be no more than 25. Meanwhile, we have also conducted experiments with
a larger number of players in Appendix G.5, in order to show the efﬁciency of sampling methods.
We choose T empirically from the values of 0.1, 0.2, 0.5, 1.0. We choose K such that Alg. 1 would
converge. Usually, it takes around 5 to 10 steps to converge. We give all players a fair start, so x0
was intialized to be 0.5 × 1. Code is available at https://valuationgame.github.io."
EMPIRICAL STUDIES,0.2846441947565543,"We ﬁrst conduct synthetic experiments on submodular games (details defered to Appendix G.1), in
order to verify the quality of solutions in terms of the true marginals p(i ∈S). One can conclude"
EMPIRICAL STUDIES,0.28651685393258425,Published as a conference paper at ICLR 2022
EMPIRICAL STUDIES,0.2883895131086142,"logistic
regression"
EMPIRICAL STUDIES,0.2902621722846442,xgboost MLP
EMPIRICAL STUDIES,0.29213483146067415,"𝑇= 0.8
Data id: 24038
GT label: True"
EMPIRICAL STUDIES,0.2940074906367041,"𝑇= 0.5
Data id: 16139
GT label: True"
EMPIRICAL STUDIES,0.2958801498127341,"𝑇= 0.1
Data id: 7977
GT label: True"
EMPIRICAL STUDIES,0.29775280898876405,"Variational
Banzhaf
Shapley"
EMPIRICAL STUDIES,0.299625468164794,"Figure 3: First column: Change of predicted probabilities when removing features. The decoupling
error is included in the legend. Last three columns: waterfall plots of feature importance."
EMPIRICAL STUDIES,0.301498127340824,"that Variational Index obtains better performance in terms of MSE and Spearman’s rank correlation
compared to the one-point solutions (Shapley value and Banzhaf value) in all experiments. More
experimental results on data point and feature valuations are deferred to Appendix G."
EXPERIMENTS ON DATA VALUATIONS,0.30337078651685395,"5.1
EXPERIMENTS ON DATA VALUATIONS"
EXPERIMENTS ON DATA VALUATIONS,0.3052434456928839,"We follow the setting of Ghorbani and Zou (2019) and reuse the code of https://github.com/
amiratag/DataShapley. We conduct data removal: training samples are sorted according to
the valuations returned by different criteria, and then samples are removed in that order to check
how much the test accuracy drops. Intuitively, the best criteria would induce the fastest drop of
performance. We experiment with the following datasets: a) Synthetic datasets similar as that of
Ghorbani and Zou (2019); b) The breast cancer dataset, which is a binary classiﬁcation dataset
with 569 samples; c) The digits dataset, that is a 10-class classiﬁcation dataset with 1797 samples.
The above two datasets are both from UCI Machine Learning repository (https://archive.
ics.uci.edu/ml/index.php). Speciﬁcally, we cluster data points into groups and studied two
settings: 1) Grouping the samples randomly; 2) Clustering the samples with the k-means algorithm.
For simplicity, we always use equal group sizes. The data point removal corresponds to singleton
groups. Fig. 2 shows the results. One can observe that in certain situations the Variational Index
achieves the fastest drop rate. It always achieves the lowest decoupling error (as shown in the legends
in each of the ﬁgures). Sometimes Variational Index and Banzhaf show similar performance. We
expect that this is because the Banzhaf value is a one-step approximation of Variational Index, and
for the speciﬁc problem considered, the ranking of the solutions does not change after one-step of
ﬁxed point iteration. There are also situations where the rankings of the three criteria are not very
distinguishable, however, the speciﬁc values are also very different since the decoupling error differs."
EXPERIMENTS ON DATA VALUATIONS,0.30711610486891383,"5.2
EXPERIMENTS ON FEATURE VALUATIONS/ATTRIBUTIONS"
EXPERIMENTS ON DATA VALUATIONS,0.3089887640449438,"We follow the setting of Lundberg and Lee (2017) and reuse the code of https://github.com/
slundberg/shap with an MIT License. We train classiﬁers on the Adult dataset4, which predicts
whether an adult’s income exceeds 50k dollar per year based on census data. It has 48,842 instances
and 14 features such as age, workclass, occupation, sex and capital gain (12 of them used)."
EXPERIMENTS ON DATA VALUATIONS,0.31086142322097376,"Feature removal results. This experiment follows a similar fashion as the data removal experiment:
we remove the features one by one according to the order deﬁned by the returned criterion, then
observe the change of predicted probabilities. Fig. 3 reports the behavior of the three criteria. The ﬁrst
row shows the results from an xgboost classiﬁer (accuracy: 0.893), second row a logistic regression"
EXPERIMENTS ON DATA VALUATIONS,0.31273408239700373,4https://archive.ics.uci.edu/ml/datasets/adult
EXPERIMENTS ON DATA VALUATIONS,0.3146067415730337,Published as a conference paper at ICLR 2022
EXPERIMENTS ON DATA VALUATIONS,0.31647940074906367,"GT Labels: True
1
2
3
4
5
6
7
8
9
10
11
12"
EXPERIMENTS ON DATA VALUATIONS,0.31835205992509363,"Var. Index
Capital Gain Relationship
Age
Education-Num
Occupation
Marital Status
Capital Loss
Hours per week
Sex
Race
Workclass
Country"
EXPERIMENTS ON DATA VALUATIONS,0.3202247191011236,"Shapley
Capital Gain Relationship Education-Num
Age
Occupation
Marital Status Hours per week
Capital Loss
Sex
Workclass
Race
Country"
EXPERIMENTS ON DATA VALUATIONS,0.32209737827715357,"Banzhaf
Capital Gain Relationship Education-Num
Age
Occupation
Marital Status
Capital Loss
Hours per week
Sex
Race
Workclass
Country"
EXPERIMENTS ON DATA VALUATIONS,0.32397003745318353,"Variational
Shapley
Banzhaf"
EXPERIMENTS ON DATA VALUATIONS,0.3258426966292135,"Figure 4: Statistics on valuations with the xgboost classiﬁer. First row: box plot of valuations. We
always consider the predicted probability of the ground truth label. “True” means the samples with
positive ground truth label and “False” means with the negative ground truth label. Second row:
Average ranking of the 12 features. Colored texts denote different rankings among the three criteria."
EXPERIMENTS ON DATA VALUATIONS,0.32771535580524347,"classiﬁer (accuracy: 0.842), third row a multi-layer perceptron (accuracy: 0.861). For the probability
dropping results, Variational Index usually induces the fastest drop, and it always enjoys the smallest
decoupling error, as expected from its mean-ﬁeld nature. From the waterfall plots, one can see that
the three criteria indeed produce different rankings of the features. Take the ﬁrst row for example.
All criteria put “Capital Loss” and “Relationship” as the ﬁrst two features. However, the remaining
features have different ranking: Variational Index and Banzhaf indicate that “Marital Status” should
be ranked third, while Shapley ranks it in the fourth position. It is hard to tell which ranking is the
best because: 1) There is no golden standard to determine the true ranking of features; 2) Even if
there exists a ground truth ranking of some “perfect model”, the trained xgboost model here might
not be able to reproduce it, since it might not be aligned with the “perfect model”."
EXPERIMENTS ON DATA VALUATIONS,0.3295880149812734,"Average results. We further provide the bar plots and averaged ranking across the adult datasets in
Fig. 4. From the bar plots one can see that different criterion has slightly different values for each
feature on average. Average rankings in the table demonstrate the difference: The three methods do
not agree on the colored features, for example, “Age”, “Education-Num” and “Captical Loss”."
EXPERIMENTS ON DATA VALUATIONS,0.33146067415730335,"5.3
EMPIRICAL CONVERGENCE RESULTS OF ALG. 1"
EXPERIMENTS ON DATA VALUATIONS,0.3333333333333333,"Table 1 shows convergence results of Alg. 1 on feature and data valuation experiments. The value in
the cells are the stepwise difference of xk , ∥xk−xk−1∥2"
EXPERIMENTS ON DATA VALUATIONS,0.3352059925093633,"n
, which is a classical criterion to measure the
convergence of iterative algorithms. One can clearly see that Alg. 1 converges in 5 to 10 iterations."
EXPERIMENTS ON DATA VALUATIONS,0.33707865168539325,Table 1: Stepwise difference ∥xk−xk−1∥2
EXPERIMENTS ON DATA VALUATIONS,0.3389513108614232,"n
of Alg. 1 for different experiments."
EXPERIMENTS ON DATA VALUATIONS,0.3408239700374532,"Step/Iteration Num
1
2
3
5
9
10
Data Val (breast cancer)
0.0023
3.61e-6
1.53e-7
2.77e-10
9.12e-16
0
Data Val (digits)
0.00099
5.93e-7
1.46e-8
8.92e-12
9.25e-18
0
Data Val (synthetic)
0.00059
2.49e-8
3.13e-10
6.06e-14
0
0
Feature Val (xgboost)
0.0066
1.68e-5
8.71e-7
2.35e-9
1.75e-14
9.25e-16
Feature Val (LR)
0.0092
2.63e-5
1.44e-6
4.31e-9
2.14e-15
1.28e-16
Feature Val (MLP)
0.0040
4.86e-6
1.86e-7
2.84e-10
6.82e-16
3.20e-17"
EXPERIMENTS ON DATA VALUATIONS,0.34269662921348315,"DISCUSSIONS AND FUTURE WORK. We have presented an energy-based treatment of
cooperative games, in order to improve the valuation problem. It is very worthwhile to explore more
in the following directions: 1) Choosing the temperature T. The temperature controls the level of
fairness since, when T →∞, all players have equal importance, when T →0, whereas a player has
either 0 or 1 importance (assuming no ties). Perhaps one can use an annealing-style algorithm in
order to control the fairness level: starting with a high temperature and gradually decreasing it, one
can obtain a series of importance values under different fairness levels. 2) Given the probabilistic
treatment of cooperative games, one can naturally add priors over the players, in order to encode
more domain knowledge. It may also make sense to consider conditioning and marginalization in
light of practical applications. 3) It is very interesting to explore the interaction of a group of players
in the energy-based framework, which would result in an “interactive” index among size-k coalitions."
EXPERIMENTS ON DATA VALUATIONS,0.3445692883895131,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3464419475655431,ETHICS STATEMENT AND BROADER IMPACT
ETHICS STATEMENT,0.34831460674157305,"Besides the valuation problems explored in this work, cooperative game theory has already been
applied to a wide range of disciplines, to name a few, economics, political science, sociology, biology,
so this work could potentially contribute to broader domains as well."
ETHICS STATEMENT,0.350187265917603,"Meanwhile, we have to be aware of possible negative societal impacts, including: 1) negative side
effects of the technology itself, for example, possible unemployment issues due to the reduced
amount of the need of valuations by human beings; 2) applications in negative downstream tasks, for
instance, the data point valuation technique could make it easier to conduct underground transactions
of private data."
REPRODUCIBILITY STATEMENT,0.352059925093633,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3539325842696629,"All the datasets are publicly available as described in the main text. In order to ensure reproducibility,
we have made the efforts in the following respects: 1) Provide code as supplementary material. 2)
Provide self-contained proofs of the main claims in Appendices D and E; 3) Provide more details on
experimental conﬁgurations and experimental results in Appendix G."
REFERENCES,0.35580524344569286,REFERENCES
REFERENCES,0.35767790262172283,"M. Ancona, E. Ceolini, C. Öztireli, and M. Gross. Towards better understanding of gradient-based
attribution methods for deep neural networks. arXiv preprint arXiv:1711.06104, 2017."
REFERENCES,0.3595505617977528,"M. Ancona, C. Öztireli, and M. Gross. Explaining deep neural networks with a polynomial time
algorithm for shapley values approximation. arXiv preprint arXiv:1903.10992, 2019."
REFERENCES,0.36142322097378277,"J. F. Banzhaf III. Weighted voting doesn’t work: A mathematical analysis. Rutgers L. Rev., 19:317,
1964."
REFERENCES,0.36329588014981273,"S. Bartunov, J. W. Rae, S. Osindero, and T. P. Lillicrap. Meta-learning deep energy-based memory
models. arXiv preprint arXiv:1910.02720, 2019."
REFERENCES,0.3651685393258427,"Y. Bian, J. M. Buhmann, and A. Krause. Continuous submodular function maximization. arXiv
preprint arXiv:2006.13474, 2020."
REFERENCES,0.36704119850187267,"Y. A. Bian, J. M. Buhmann, and A. Krause. Optimal continuous dr-submodular maximization and
applications to provable mean ﬁeld inference. In Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 644–653,
Long Beach, California, USA, 09–15 Jun 2019. PMLR."
REFERENCES,0.36891385767790263,"G. Calinescu, C. Chekuri, M. Pál, and J. Vondrák. Maximizing a submodular set function subject to
a matroid constraint. In International Conference on Integer Programming and Combinatorial
Optimization, pages 182–196. Springer, 2007."
REFERENCES,0.3707865168539326,"J. Chen, L. Song, M. J. Wainwright, and M. I. Jordan. L-shapley and c-shapley: Efﬁcient model
interpretation for structured data. arXiv preprint arXiv:1808.02610, 2018."
REFERENCES,0.37265917602996257,"Y. Chun. A new axiomatization of the shapley value. Games and Economic Behavior, 1(2):119–130,
1989."
REFERENCES,0.37453183520599254,"S. Cohen, G. Dror, and E. Ruppin. Feature selection via coalitional game theory. Neural Computation,
19(7):1939–1961, 2007."
REFERENCES,0.37640449438202245,"I. Covert, S. Lundberg, and S.-I. Lee. Explaining by removing: A uniﬁed framework for model
explanation. arXiv preprint arXiv:2011.14878, 2020a."
REFERENCES,0.3782771535580524,"I. Covert, S. Lundberg, and S.-I. Lee. Understanding global feature contributions with additive
importance measures. arXiv preprint arXiv:2004.00668, 2020b."
REFERENCES,0.3801498127340824,"A. Datta, S. Sen, and Y. Zick. Algorithmic transparency via quantitative input inﬂuence: Theory and
experiments with learning systems. In 2016 IEEE symposium on security and privacy (SP), pages
598–617. IEEE, 2016."
REFERENCES,0.38202247191011235,Published as a conference paper at ICLR 2022
REFERENCES,0.3838951310861423,"Y. Deng, A. Bakhtin, M. Ott, A. Szlam, and M. Ranzato. Residual energy-based models for text
generation. arXiv preprint arXiv:2004.11714, 2020."
REFERENCES,0.3857677902621723,"J. Djolonga and A. Krause. From map to marginals: Variational inference in bayesian submodular
models. In Neural Information Processing Systems (NIPS), 2014."
REFERENCES,0.38764044943820225,"Z. Fan, H. Fang, Z. Zhou, J. Pei, M. P. Friedlander, C. Liu, and Y. Zhang. Improving fairness for data
valuation in federated learning. arXiv preprint arXiv:2109.09046, 2021."
REFERENCES,0.3895131086142322,"U. Feige, V. S. Mirrokni, and J. Vondrák. Maximizing non-monotone submodular functions. SIAM
Journal on Computing, 40(4):1133–1153, 2011."
REFERENCES,0.3913857677902622,"A. Ghorbani and J. Zou. Data shapley: Equitable valuation of data for machine learning. In
International Conference on Machine Learning, pages 2242–2251. PMLR, 2019."
REFERENCES,0.39325842696629215,"M. Grabisch. K-order additive discrete fuzzy measures and their representation. Fuzzy sets and
systems, 92(2):167–189, 1997."
REFERENCES,0.3951310861423221,"M. Grabisch, J.-L. Marichal, and M. Roubens. Equivalent representations of set functions. Mathe-
matics of Operations Research, 25(2):157–178, 2000."
REFERENCES,0.3970037453183521,"W. Grathwohl, K.-C. Wang, J.-H. Jacobsen, D. Duvenaud, M. Norouzi, and K. Swersky. Your
classiﬁer is secretly an energy based model and you should treat it like one. arXiv preprint
arXiv:1912.03263, 2019."
REFERENCES,0.398876404494382,"G. Greco, F. Lupia, and F. Scarcello. Structural tractability of shapley and banzhaf values in allocation
games. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015."
REFERENCES,0.40074906367041196,"F. K. Gustafsson, M. Danelljan, R. Timofte, and T. B. Schön. How to train your energy-based model
for regression. arXiv preprint arXiv:2005.01698, 2020."
REFERENCES,0.40262172284644193,"M. Gutmann and A. Hyvärinen. Noise-contrastive estimation: A new estimation principle for
unnormalized statistical models. In Proceedings of the Thirteenth International Conference on
Artiﬁcial Intelligence and Statistics, pages 297–304. JMLR Workshop and Conference Proceedings,
2010."
REFERENCES,0.4044943820224719,"T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based
policies. In International Conference on Machine Learning, pages 1352–1361. PMLR, 2017."
REFERENCES,0.40636704119850187,"P. L. Hammer and S. Rudeanu. Boolean methods in operations research and related areas, volume 7.
Springer Science & Business Media, 2012."
REFERENCES,0.40823970037453183,"G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural computation,
14(8):1771–1800, 2002."
REFERENCES,0.4101123595505618,"W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American statistical association, 58(301):13–30, 1963."
REFERENCES,0.41198501872659177,"A. Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research, 6(4), 2005."
REFERENCES,0.41385767790262173,"E. T. Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957a."
REFERENCES,0.4157303370786517,"E. T. Jaynes. Information theory and statistical mechanics. ii. Physical review, 108(2):171, 1957b."
REFERENCES,0.41760299625468167,"R. Jia, D. Dao, B. Wang, F. A. Hubis, N. Hynes, N. M. Gürel, B. Li, C. Zhang, D. Song, and C. J.
Spanos. Towards efﬁcient data valuation based on the shapley value. In The 22nd International
Conference on Artiﬁcial Intelligence and Statistics, pages 1167–1176. PMLR, 2019a."
REFERENCES,0.41947565543071164,"R. Jia, X. Sun, J. Xu, C. Zhang, B. Li, and D. Song. An empirical and comparative analysis of data
valuation with scalable algorithms. arXiv preprint arXiv:1911.07128, 2019b."
REFERENCES,0.42134831460674155,"A. Krause and D. Golovin. Submodular function maximization. Tractability, 3:71–104, 2014."
REFERENCES,0.4232209737827715,Published as a conference paper at ICLR 2022
REFERENCES,0.4250936329588015,"I. E. Kumar, C. Scheidegger, S. Venkatasubramanian, and S. Friedler. Shapley residuals: Quantifying
the limits of the shapley value for explanations. In ICML Workshop on Workshop on Human
Interpretability in Machine Learning (WHI), 2020."
REFERENCES,0.42696629213483145,"Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based learning.
Predicting structured data, 1(0), 2006."
REFERENCES,0.4288389513108614,"S. Lipovetsky and M. Conklin. Analysis of regression in game theory approach. Applied Stochastic
Models in Business and Industry, 17(4):319–330, 2001."
REFERENCES,0.4307116104868914,"W. Liu, X. Wang, J. D. Owens, and Y. Li. Energy-based out-of-distribution detection. arXiv preprint
arXiv:2010.03759, 2020."
REFERENCES,0.43258426966292135,"S. Lundberg and S.-I. Lee. A uniﬁed approach to interpreting model predictions. arXiv preprint
arXiv:1705.07874, 2017."
REFERENCES,0.4344569288389513,"S. M. Lundberg, G. G. Erion, and S.-I. Lee. Consistent individualized feature attribution for tree
ensembles. arXiv preprint arXiv:1802.03888, 2018."
REFERENCES,0.4363295880149813,"T. P. Minka. Expectation propagation for approximate bayesian inference. In Proceedings of the
Seventeenth conference on Uncertainty in artiﬁcial intelligence, pages 362–369, 2001."
REFERENCES,0.43820224719101125,"D. Monderer and D. Samet. Variations on the shapley value. Handbook of game theory with economic
applications, 3:2055–2076, 2002."
REFERENCES,0.4400749063670412,"R. Okhrati and A. Lipani. A multilinear sampling algorithm to estimate shapley values. In 2020 25th
International Conference on Pattern Recognition (ICPR), pages 7992–7999. IEEE, 2021."
REFERENCES,0.4419475655430712,"A. B. Owen. Sobol’indices and shapley value. SIAM/ASA Journal on Uncertainty Quantiﬁcation, 2
(1):245–251, 2014."
REFERENCES,0.4438202247191011,"G. Owen. Multilinear extensions of games. Management Science, 18(5-part-2):64–79, 1972."
REFERENCES,0.44569288389513106,"L. S. Penrose. The elementary statistics of majority voting. Journal of the Royal Statistical Society,
109(1):53–57, 1946."
REFERENCES,0.44756554307116103,"V. Petsiuk, A. Das, and K. Saenko. Rise: Randomized input sampling for explanation of black-box
models. arXiv preprint arXiv:1806.07421, 2018."
REFERENCES,0.449438202247191,"M. T. Ribeiro, S. Singh, and C. Guestrin. "" why should i trust you?"" explaining the predictions of
any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining, pages 1135–1144, 2016."
REFERENCES,0.45131086142322097,"M. Ridaoui, M. Grabisch, and C. Labreuche. An axiomatisation of the banzhaf value and interaction
index for multichoice games. In International Conference on Modeling Decisions for Artiﬁcial
Intelligence, pages 143–155. Springer, 2018."
REFERENCES,0.45318352059925093,"B. Rozemberczki and R. Sarkar. The shapley value of classiﬁers in ensemble games. arXiv preprint
arXiv:2101.02153, 2021."
REFERENCES,0.4550561797752809,"A. Sahin, Y. Bian, J. Buhmann, and A. Krause. From sets to multisets: provable variational inference
for probabilistic integer submodular models. In International Conference on Machine Learning,
pages 8388–8397. PMLR, 2020."
REFERENCES,0.45692883895131087,"B. Scellier and Y. Bengio. Equilibrium propagation: Bridging the gap between energy-based models
and backpropagation. Frontiers in computational neuroscience, 11:24, 2017."
REFERENCES,0.45880149812734083,"L. S. Shapley. A value for n-person games. Contributions to the Theory of Games, 2(28):307–317,
1953."
REFERENCES,0.4606741573033708,"R. H. L. Sim, Y. Zhang, M. C. Chan, and B. K. H. Low. Collaborative machine learning with incentive-
aware model rewards. In International Conference on Machine Learning, pages 8927–8936. PMLR,
2020."
REFERENCES,0.46254681647940077,Published as a conference paper at ICLR 2022
REFERENCES,0.46441947565543074,"E. Strumbelj and I. Kononenko. An efﬁcient explanation of individual classiﬁcations using game
theory. The Journal of Machine Learning Research, 11:1–18, 2010."
REFERENCES,0.46629213483146065,"M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In International
Conference on Machine Learning, pages 3319–3328. PMLR, 2017."
REFERENCES,0.4681647940074906,"S. Tschiatschek, J. Djolonga, and A. Krause. Learning probabilistic submodular diversity models
via noise contrastive estimation. In Proc. International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), 2016."
REFERENCES,0.4700374531835206,"M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference.
Now Publishers Inc, 2008."
REFERENCES,0.47191011235955055,"J. Wang, J. Wiens, and S. Lundberg. Shapley ﬂow: A graph-based approach to interpreting model
predictions. In International Conference on Artiﬁcial Intelligence and Statistics, pages 721–729.
PMLR, 2021a."
REFERENCES,0.4737827715355805,"R. Wang, X. Wang, and D. I. Inouye. Shapley explanation networks. arXiv preprint arXiv:2104.02297,
2021b."
REFERENCES,0.4756554307116105,"T. Wang, J. Rausch, C. Zhang, R. Jia, and D. Song. A principled approach to data valuation for
federated learning. In Federated Learning, pages 153–167. Springer, 2020."
REFERENCES,0.47752808988764045,"R. J. Weber. Probabilistic values for games. The Shapley Value. Essays in Honor of Lloyd S. Shapley,
pages 101–119, 1988."
REFERENCES,0.4794007490636704,"M. Welling and Y. W. Teh.
Bayesian learning via stochastic gradient langevin dynamics.
In
Proceedings of the 28th international conference on machine learning (ICML-11), pages 681–688.
Citeseer, 2011."
REFERENCES,0.4812734082397004,"B. Williamson and J. Feng. Efﬁcient nonparametric statistical inference on population feature
importance using shapley values. In International Conference on Machine Learning, pages
10282–10291. PMLR, 2020."
REFERENCES,0.48314606741573035,"H. P. Young. Monotonic solutions of cooperative games. International Journal of Game Theory, 14
(2):65–72, 1985."
REFERENCES,0.4850187265917603,"M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European
conference on computer vision, pages 818–833. Springer, 2014."
REFERENCES,0.4868913857677903,Published as a conference paper at ICLR 2022
REFERENCES,0.4887640449438202,"Appendix of “Energy-Based Learning for
Cooperative Games, with Applications to Valuation
Problems in Machine Learning”"
REFERENCES,0.49063670411985016,CONTENTS
REFERENCES,0.49250936329588013,"A Derivations of the Maximum Entropy Distribution
14"
REFERENCES,0.4943820224719101,"B
Common Axioms of Valuation Criteria
15"
REFERENCES,0.49625468164794007,"C The Naive Mean Field Algorithm
16"
REFERENCES,0.49812734082397003,"D Proof of Recovering Classical Criteria
16"
REFERENCES,0.5,"E
Proof of Theorem 1
17"
REFERENCES,0.50187265917603,"F
Miscellaneous Results in Sec. 4
18"
REFERENCES,0.5037453183520599,"G More Conﬁguration Details and Experimental Results
19"
REFERENCES,0.5056179775280899,"G.1
Synthetic Experiments on Submodular (FLID) Games . . . . . . . . . . . . . . . .
19"
REFERENCES,0.5074906367041199,"G.2
Details of the Models for Feature Valuations . . . . . . . . . . . . . . . . . . . . .
20"
REFERENCES,0.5093632958801498,"G.3
More Results on Feature Valuations
. . . . . . . . . . . . . . . . . . . . . . . . .
20"
REFERENCES,0.5112359550561798,"G.4
More Average Results on Feature Valuations . . . . . . . . . . . . . . . . . . . . .
20"
REFERENCES,0.5131086142322098,"G.5
Experiments with More Players . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21"
REFERENCES,0.5149812734082397,"G.6
Effect of Number of Samples in MCMC Sampling
. . . . . . . . . . . . . . . . .
22"
REFERENCES,0.5168539325842697,"A
DERIVATIONS OF THE MAXIMUM ENTROPY DISTRIBUTION"
REFERENCES,0.5187265917602997,"One may wonder why do we need energy-based treatment of valuation problems in machine learning?
Speciﬁcally, under the setting of cooperative games (N, F(S)), why we have to take the exponential
form of EBM p(S) ∝exp(F(S)/T)? Because one may also formulate it as something else, say,
p(S) ∝(1 + |F(S)|)?"
REFERENCES,0.5205992509363296,"In one word, because EBM is the maximum entropy distribution. Being a maximum entropy
distribution means minimizing the amount of prior information built into the distribution. Another
lens to understand it is that: Since the distribution with the maximum entropy is the one that makes
the fewest assumptions about the true distribution of data, the principle of maximum entropy can be
seen as an application of Occam’s razor. Meanwhile, many physical systems tend to move towards
maximal entropy conﬁgurations over time (Jaynes, 1957a;b)."
REFERENCES,0.5224719101123596,"In the following we will give a derivation to show that p(S) ∝exp(F(S)/T) is indeed the maximum
entropy distribution for a cooperative game. The derivation bellow is closely following Jaynes
(1957a;b) for statistical mechanics. Suppose each coalition S is associated with a payoff F(S) with
probability p(S). We would like to maximize the entropy H(p) = −P"
REFERENCES,0.5243445692883895,"S⊆N p(S) log p(S), subject
to the constraints that P"
REFERENCES,0.5262172284644194,"S p(S) = 1, p(S) ≥0 and P"
REFERENCES,0.5280898876404494,"S p(S)F(S) = µ (i.e., the average payoff is
known as µ)."
REFERENCES,0.5299625468164794,Writing down the Lagrangian
REFERENCES,0.5318352059925093,Published as a conference paper at ICLR 2022
REFERENCES,0.5337078651685393,"L(p, λ0, λ1) := −  X"
REFERENCES,0.5355805243445693,"S⊆N
p(S) log p(S) + λ0(
X"
REFERENCES,0.5374531835205992,"S⊆N
p(S) −1) + λ1(µ −
X"
REFERENCES,0.5393258426966292,"S⊆N
p(S)F(S)) "
REFERENCES,0.5411985018726592,"
(16)"
REFERENCES,0.5430711610486891,"Setting
∂L(p, λ0, λ1)"
REFERENCES,0.5449438202247191,"∂p(S)
= −[log p(S) + 1 + λ0 −λ1F(S)]
(17)"
REFERENCES,0.5468164794007491,"= 0
(18)
we get:
p(S) = exp[−(λ0 + 1 −λ1F(S))]
(19)"
REFERENCES,0.548689138576779,"λ0 + 1 = log
X"
REFERENCES,0.550561797752809,"S⊆N
exp(λ1F(S)) =: log Z
(20)"
REFERENCES,0.552434456928839,"is the log-partition function. So,"
REFERENCES,0.5543071161048689,p(S) = exp[λ1F(S))]
REFERENCES,0.5561797752808989,"Z
(21)"
REFERENCES,0.5580524344569289,Note that the maximum value of the entropy is
REFERENCES,0.5599250936329588,"Hmax = −
X"
REFERENCES,0.5617977528089888,"S⊆N
p(S) log p(S)
(22)"
REFERENCES,0.5636704119850188,"= λ0 + 1 −λ1
X"
REFERENCES,0.5655430711610487,"S⊆N
p(S)F(S)
(23)"
REFERENCES,0.5674157303370787,"= λ0 + 1 −λ1µ
(24)"
REFERENCES,0.5692883895131086,"So one can get,"
REFERENCES,0.5711610486891385,λ1 = −∂Hmax
REFERENCES,0.5730337078651685,"∂µ
=: 1"
REFERENCES,0.5749063670411985,"T
(25)"
REFERENCES,0.5767790262172284,which deﬁnes the inverse temperature.
REFERENCES,0.5786516853932584,So we reach the exponential form of p(S) as:
REFERENCES,0.5805243445692884,"p(S) =
exp[F(S)/T)]
P
S⊆N exp[F(S)/T].
(26)"
REFERENCES,0.5823970037453183,"B
COMMON AXIOMS OF VALUATION CRITERIA"
REFERENCES,0.5842696629213483,"Following the deﬁnitions in Covert et al. (2020a), the ﬁve common axioms are listed as bellow. φi(F)
denotes the value assigned to player i in the game (N, F(S)). Note that the notions might be slightly
different in classical literature of game theory."
REFERENCES,0.5861423220973783,"Null player: For a player i in the cooperative game (N, F(S)), if F(S + i) = F(S) holds for all
S ⊆N −i, then its value should be φi(F) = 0."
REFERENCES,0.5880149812734082,"Symmetry: For any two players i, j in the cooperative game (N, F(S)), if F(S + i) = F(S + j)
holds for all S ⊆N −i −j, then it holds that φi(F) = φj(F)."
REFERENCES,0.5898876404494382,"Marginalism: For two games (N, F(S)) and (N, G(S)) where all players have identical marginal
contributions, the players obtain equal valuations: F(S + i) −F(S) = G(S + i) −G(S) holds for
all (i, S), then it holds that φi(F) = φi(G)."
REFERENCES,0.5917602996254682,"Additivity: For two games (N, F(S)) and (N, G(S)), if they are combined, the total contribution of
a player is equal to the sum of its individual contributions on each game: φi(F +G) = φi(F)+φi(G)."
REFERENCES,0.5936329588014981,"Efﬁciency: The values add up to the difference in value between the grand coalition and the empty
coalition: P"
REFERENCES,0.5955056179775281,i∈N φi(F) = F(N) −F(∅).
REFERENCES,0.5973782771535581,Published as a conference paper at ICLR 2022
REFERENCES,0.599250936329588,"Remark 1 (The notion of “marginalism”). Speciﬁcally, the marginalism axiom was ﬁrst mentioned by
(Young, 1985, Equation 7 on page 70), where it was called the “independence” condition; Following
(Chun, 1989, page 121), it was formally called the “marginality”. This axiom requires a player’s
payoffs to depend only on his own marginal contributions – whenever they remain unchanged, his
payoffs should be unaffected."
REFERENCES,0.601123595505618,"Algorithm 2: NAIVE MEAN FIELD FOR CALCULATING THE VARIATIONAL INDEX
Input: A cooperative game (N, F(S)) with n players. Initial marginal x0 ∈[0, 1]n; #epochs K.
Output: The Variational Index s∗= σ−1(x∗)"
REFERENCES,0.602996254681648,1 x ←x0;
REFERENCES,0.6048689138576779,2 for epoch from 1 to K do
REFERENCES,0.6067415730337079,"3
for i = 1 →n do"
REFERENCES,0.6086142322097379,"4
let vi be the player being operated;"
REFERENCES,0.6104868913857678,"5
xvi ←σ(∇vif F
mt(x)/T) =
 
1 + exp(−∇vif F
mt(x)/T
−1 ;"
REFERENCES,0.6123595505617978,6 x∗←x;
REFERENCES,0.6142322097378277,"C
THE NAIVE MEAN FIELD ALGORITHM"
REFERENCES,0.6161048689138576,"The naive mean ﬁeld algorithm is one of the most classical algorithm for mean ﬁeld inference. It is
summarized in Alg. 2."
REFERENCES,0.6179775280898876,"D
PROOF OF RECOVERING CLASSICAL CRITERIA"
REFERENCES,0.6198501872659176,"For Banzhaf value, by comparing its deﬁnition in Eq. (2) with Eq. (13) it reads,"
REFERENCES,0.6217228464419475,"Bai =
X"
REFERENCES,0.6235955056179775,"S⊆N−i[F(S + i) −F(S)]
1
2n−1 = ∇if F
mt(0.5 ∗1) = Tσ−1(MFI(0.5 ∗1; 1)),
(27)"
REFERENCES,0.6254681647940075,which is the 1-step variational value initialied at 0.5 ∗1.
REFERENCES,0.6273408239700374,"For Shapley value, according to Grabisch et al. (2000), here we prove a stronger conclusion regarding
the generalization of Shapley value: Shapley interaction index, which is deﬁned for any coalition S:"
REFERENCES,0.6292134831460674,"ShS =
X"
REFERENCES,0.6310861423220974,T ⊆N−S
REFERENCES,0.6329588014981273,(n −|T| −|S|)!|T|!
REFERENCES,0.6348314606741573,(n −|S| + 1)! X
REFERENCES,0.6367041198501873,"L⊆S
(−1)|S|−|L|F(L + T).
(28)"
REFERENCES,0.6385767790262172,"Given Hammer and Rudeanu (2012), we have a second form of the multilinear extension as:"
REFERENCES,0.6404494382022472,"f F
mt(x) =
X"
REFERENCES,0.6423220973782772,"S⊆N
a(S)
Y"
REFERENCES,0.6441947565543071,"i∈S
xi, x ∈[0, 1]n,
(29)"
REFERENCES,0.6460674157303371,where a(S) := P
REFERENCES,0.6479400749063671,T ⊆S(−1)|S|−|T |F(T) is the Mobius transform of F(S).
REFERENCES,0.649812734082397,"Then, one can show the S-derivative of f F
mt(x) is (suppose S = {i1, ..., i|S|}),"
REFERENCES,0.651685393258427,"∆Sf F
mt(x) :=
∂|S|f F
mt(x)
∂xi1, ..., ∂xi|S|
=
X"
REFERENCES,0.653558052434457,"T ⊇S
a(T)
Y"
REFERENCES,0.6554307116104869,"i∈T −S
xi.
(30) So,"
REFERENCES,0.6573033707865169,"∆Sf F
mt(x1) =
X"
REFERENCES,0.6591760299625468,"T ⊇S
a(T)x|T |−|S|.
(31)"
REFERENCES,0.6610486891385767,"Then it holds,
Z 1"
REFERENCES,0.6629213483146067,"0
∆Sf F
mt(x1)dx =
Z 1 0 X"
REFERENCES,0.6647940074906367,"T ⊇S
a(T)x|T |−|S|dx
(32)"
REFERENCES,0.6666666666666666,"Published as a conference paper at ICLR 2022 =
X"
REFERENCES,0.6685393258426966,"T ⊇S
a(T)
Z 1"
REFERENCES,0.6704119850187266,"0
x|T |−|S|dx
(33) =
X"
REFERENCES,0.6722846441947565,"T ⊇S
a(T)(|T| −|S| + 1)−1
(34)"
REFERENCES,0.6741573033707865,"According to Grabisch (1997), we have ShS = P"
REFERENCES,0.6760299625468165,"T ⊇S a(T)(|T| −|S| + 1)−1, then we reach the
conclusion:"
REFERENCES,0.6779026217228464,"ShS =
Z 1"
REFERENCES,0.6797752808988764,"0
∆Sf F
mt(x1)dx.
(35)"
REFERENCES,0.6816479400749064,"When |S| = 1, we recover the conclusion for Shapley value."
REFERENCES,0.6835205992509363,"E
PROOF OF THEOREM 1"
REFERENCES,0.6853932584269663,"Theorem 1 (Axiomatisation of K-Step Variational Values of Def. 3). If initialized uniformly, i.e.,
x0 = x1, x ∈[0, 1], all the variational values in the trajectory Tσ−1(MFI(x; k)), k = 1, 2, 3...
satisfy the null player, marginalism and symmetry axioms."
REFERENCES,0.6872659176029963,"Proof of Theorem 1. In step k, we know that the value to player i is:"
REFERENCES,0.6891385767790262,"Tσ−1(MFI(x; k))i =
X"
REFERENCES,0.6910112359550562,"S⊆N−i
[F(S + i) −F(S)]
Y"
REFERENCES,0.6928838951310862,"j∈S
xj
Y"
REFERENCES,0.6947565543071161,"j′∈N−S−i
(1 −xj′)
(36)"
REFERENCES,0.6966292134831461,"For the null player property, since F(S + i) = F(S) always holds, it is easy to see that
Tσ−1(MFI(x; k))i = 0 holds for all i ∈N."
REFERENCES,0.6985018726591761,Now we will show that the symmetry property holds. The value to player i′ is:
REFERENCES,0.700374531835206,"Tσ−1(MFI(x; k))i′ =
X"
REFERENCES,0.702247191011236,"S′⊆N−i′
[F(S′ + i′) −F(S′)]
Y"
REFERENCES,0.704119850187266,"j∈S′
xj
Y"
REFERENCES,0.7059925093632958,"j′∈N−S′−i
(1 −xj′)
(37)"
REFERENCES,0.7078651685393258,"Now let us compare different terms in the summands of Eq. (36) and Eq. (37). We try to match the
summands one by one. There are two situations:"
REFERENCES,0.7097378277153558,"Situation I: For any S ⊆N −i −i′, we choose S′ = S."
REFERENCES,0.7116104868913857,"In this case we have F(S + i) −F(S) = F(S′ + i′) −F(S′). For the products of x we have:
Y"
REFERENCES,0.7134831460674157,"j∈S
xj
Y"
REFERENCES,0.7153558052434457,"j′∈N−S−i
(1 −xj′) −
Y"
REFERENCES,0.7172284644194756,"j∈S′
xj
Y"
REFERENCES,0.7191011235955056,"j′∈N−S′−i
(1 −xj′) =
(38) Y"
REFERENCES,0.7209737827715356,"j∈S
xj
Y"
REFERENCES,0.7228464419475655,"j′∈N−S−i−i′
(1 −xj′)[(1 −xi′) −(1 −xi)]
(39)"
REFERENCES,0.7247191011235955,"We know that xi′ = xi holds from step 0, by simple induction, we know that xi′ = xi holds for step
k as well. So in this situation, the summands equal to each other."
REFERENCES,0.7265917602996255,"Situation II: For any S = A + i′, we choose S′ = A + i, where A ⊆N −i −i′. In this case, it still
holds that F(S + i) −F(S) = F(S′ + i′) −F(S′). For the products of x we have:
Y"
REFERENCES,0.7284644194756554,"j∈S
xj
Y"
REFERENCES,0.7303370786516854,"j′∈N−S−i
(1 −xj′) −
Y"
REFERENCES,0.7322097378277154,"j∈S′
xj
Y"
REFERENCES,0.7340823970037453,"j′∈N−S′−i′
(1 −xj′) =
(40) Y"
REFERENCES,0.7359550561797753,"j∈A
xj
Y"
REFERENCES,0.7378277153558053,"j′∈N−A−i−i′
(1 −xj′)[xi′ −xi].
(41)"
REFERENCES,0.7397003745318352,"Again, by the simple induction, we know that xi′ = xi holds for step k."
REFERENCES,0.7415730337078652,The above two situations ﬁnishes the proof of symmetry.
REFERENCES,0.7434456928838952,"For the marginalisim axiom, one can see that the update step for the two games are identical, and
it is easy to deduce that they produce exactly the same trajectories, given that they have the same
initializations."
REFERENCES,0.7453183520599251,Published as a conference paper at ICLR 2022
REFERENCES,0.7471910112359551,"F
MISCELLANEOUS RESULTS IN SEC. 4"
REFERENCES,0.7490636704119851,"Gradient of entropy in Sec. 4.2
Note that q is a fully factorized product distribution q(S; x) :=
Q"
REFERENCES,0.7509363295880149,"i∈S xi
Q"
REFERENCES,0.7528089887640449,"j /∈S(1−xj), x ∈[0, 1]n, so its entropy H(q(S; x)) can be written as the sum of entropy of
n independent Bernoulli distributions. And the entropy of one Bernoulli distribution with parameter
xi is
−xi log xi −(1 −xi) log(1 −xi)"
REFERENCES,0.7546816479400749,"So we have,"
REFERENCES,0.7565543071161048,"∇iH(q(S; x)) = ∇i n
X"
REFERENCES,0.7584269662921348,"i=1
[−xi log xi −(1 −xi) log(1 −xi)]
(42)"
REFERENCES,0.7602996254681648,"= ∇i[−xi log xi −(1 −xi) log(1 −xi)]
(43)"
REFERENCES,0.7621722846441947,= log 1 −xi
REFERENCES,0.7640449438202247,"xi
(44)"
REFERENCES,0.7659176029962547,"Satisfying more axioms is not essential for valuation problems. Notably, in cooperative game
theory, one line of work is to seek for solution concepts that would satisfy more axioms. However,
for valuation problems in machine learning, this is arguably not essential. For example, similar as
what Ridaoui et al. (2018) argues, efﬁciency does not make sense for certain games."
REFERENCES,0.7677902621722846,"For a simple illustration, let us consider a voting game from a classiﬁcation model with 3 binary
features x ∈{0, 1}3 with weights w = [2, 1, 1]⊤: f(x) := 1{w⊤x≥3}. Now we are trying to ﬁnd
the valuation of each feature in N = {x1, x2, x3}. Naturally, the value function in the corresponding
voting game shall be F(S) = f(xS) where xS means setting the coordinates of x inside S to be
1 while leaving others to be 0. In this game let us count how many times each feature could ﬂip
the classiﬁcation result: for feature x1, there are three situations: F({1, 2}) −F({2}), F({1, 3}) −
F({3}) and F({1, 2, 3}) −F({2, 3}); for feature x2, there are one situation: F({1, 2}) −F({1});
for feature x3, there are one situation: F({1, 3}) −F({1}). Then the voting power (or valuation)
of each feature shall follows a 3 : 1 : 1 ratio. By simple calculations, one can see that the Banzhaf
values of the three features are 3 4, 1 4, 1"
REFERENCES,0.7696629213483146,"4, which is consistent with the ratio of the expected voting
power. However, the Shapley values of them are 4 6, 1 6, 1"
REFERENCES,0.7715355805243446,"6, which is not consistent due to satisfying the
efﬁciency axiom."
REFERENCES,0.7734082397003745,"By the above example we are trying to explain that for valuation problems, satisfying more axioms
is not necessary, sometimes even does not make sense. Whether more axioms shall be considered
and which sets of them shall be added really depend on the speciﬁc scenario, which will be left for
important future work."
REFERENCES,0.7752808988764045,"The “one-shot sampling trick” to accelerate Alg. 1.
Indeed, Variational Index needs 5 to 10
iterations to converge. In each iteration one has to evaluate the gradient of multilinear extension
∇f F
mt(x), which needs MCMC sampling to estimate the exponential sum."
REFERENCES,0.7771535580524345,"Here we suggest a “one-shot sampling trick”, when it is expensive to evaluate the value function
F(S). This trick could reuse the sampled values in each iteration, such that Alg. 1 could run with the
similar cost as calculating Banzhaf values."
REFERENCES,0.7790262172284644,The one-shot sampling trick is built upon one formulation taken from Eq. (13):
REFERENCES,0.7808988764044944,"∇if F
mt(x) =
X"
REFERENCES,0.7827715355805244,"S⊆N−i
[F(S + i) −F(S)]q(S; (x|xi ←0))"
REFERENCES,0.7846441947565543,"For coordinate i of ∇f F
mt(x), we can ﬁrstly sample m coalitions uniformly randomly from 2N−i, and
evaluate their marginal contributions. Then in each of the following iterations, one could reuse the
one-shot sampled marginal contributions to estimate the gradient according to the above equation. In
this way, we could make the cost of multi-step running of Alg. 1 similar as that of Banzhaf value in"
REFERENCES,0.7865168539325843,Published as a conference paper at ICLR 2022
REFERENCES,0.7883895131086143,"terms of the number of F(S) evaluations. Compared to the original iterative sampling from q, this
one-shot sampling might come with a variance-complexity tradeoff, for which we will explore as a
future work."
REFERENCES,0.7902621722846442,"G
MORE CONFIGURATION DETAILS AND EXPERIMENTAL RESULTS"
REFERENCES,0.7921348314606742,"G.1
SYNTHETIC EXPERIMENTS ON SUBMODULAR (FLID) GAMES 𝑛=
6 𝑛=
8 𝑛=
10"
REFERENCES,0.7940074906367042,"𝐷= 4
𝐷= 8"
REFERENCES,0.795880149812734,"Figure 5: Comparison of different importance measures for FLID games. Hidden dimensions: First
column D=4, second column D=8. n denotes # of players. Vertical lines means (marginals - 0.5)
since we would like to clearly show positive players (marginal > 0.5) and negative players (marginal
< 0.5)."
REFERENCES,0.797752808988764,"Here we deﬁne a synthetic game with the value function as a FLID (Tschiatschek et al., 2016)
objective F(S), which is a diversity boosting model satisfying the submodularity property. We
know that its multilinear extension admits polynomial time algorithms (Bian et al., 2019). Let
W ∈R|N|×D
+
be the weights, each row corresponds to the latent representation of an item, with D as"
REFERENCES,0.799625468164794,Published as a conference paper at ICLR 2022
REFERENCES,0.8014981273408239,the dimensionality. Then
REFERENCES,0.8033707865168539,"F(S) :=
X"
REFERENCES,0.8052434456928839,"i∈S ui +
XD"
REFERENCES,0.8071161048689138,"d=1(max
i∈S Wi,d −
X"
REFERENCES,0.8089887640449438,"i∈S Wi,d) =
X"
REFERENCES,0.8108614232209738,"i∈S u′
i +
XD"
REFERENCES,0.8127340823970037,"d=1 max
i∈S Wi,d, (45)"
REFERENCES,0.8146067415730337,"which models both coverage and diversity, and u′
i = ui−PD
d=1 Wi,d. In order to test the performance
of the proposed variational objectives, we consider small synthetic games with 6, 8 and 10 players
such that the ground truth marginals can be computed exhaustively. We would like to compare
with the true marginals p(i ∈S) since they represent the probability that player i participates in all
coalitions, which is hard to compute in general. The distance to the true marginals is also a natural
measure of the decoupling error as deﬁned in Def. 1. We apply a sigmoid function to Shapley value
and Banzhaf value in order to translate them to probabilities. We calculate the mean squared error
(MSE) and Spearman’s rank correlation (ρ) to the ground truth marginals p(i ∈S) and report them
in the ﬁgure legend. Fig. 5 collects the ﬁgures, one can see that the Variational Index clearly obtains
better performance in terms of MSE and Spearman’s rank correlation compared to the one-point
solutions (Shapley value and Banzhaf value) in all experiments."
REFERENCES,0.8164794007490637,"G.2
DETAILS OF THE MODELS FOR FEATURE VALUATIONS"
REFERENCES,0.8183520599250936,"For xgboost, the train accuracy is 0.8934307914376094, the speciﬁc conﬁguration with the xgboost
package is:"
REFERENCES,0.8202247191011236,"XGBClassifier ( base_score =0.5 ,
b o o s t e r = ’ g b t r e e ’ ,
c o l s a m p l e _ b y l e v e l =1 ,
colsample_bynode =1 ,
c o l sa m p le _ b yt r e e =1 , gamma=0 ,
gpu_id =−1,
importance_type = ’ gain ’ ,
i n t e r a c t i o n _ c o n s t r a i n t s = ’ ’ ,
l e a r n i n g _ r a t e =0.300000012 ,
max_delta_step =0 , max_depth =6 ,
min_child_weight =1 ,
missing =nan ,
m o n o t o n e _ c o n s t r a i n t s = ’ ( ) ’ ,
n _ e s t i m a t o r s =100 ,
n_jobs =56 ,
n u m _ p a r a l l e l _ t r e e =1 ,
random_state =0 ,
reg_alpha =0 ,
reg_lambda =1 ,
scale_pos_weight =1 ,
subsample =1 ,
tree_method = ’ exact ’ ,
v a l i d a t e _ p a r a m e t e r s =1 ,
v e r b o s i t y =None )"
REFERENCES,0.8220973782771536,"We used the sklearn package for the logistic regression and MLP classiﬁers. For logistic regression,
the accuracy is 0.8418967476428857, the speciﬁc conﬁguration is:"
REFERENCES,0.8239700374531835,"L o g i s t i c R e g r e s s i o n ( random_state =0 ,
s o l v e r ="" l i b l i n e a r "" , C=0.5)"
REFERENCES,0.8258426966292135,"For the MLP, its accuracy is 0.8614600288688923, and the conﬁguration is:"
REFERENCES,0.8277153558052435,"MLPClassifier ( random_state =0 ,
max_iter =300 ,
l e a r n i n g _ r a t e _ i n i t =0.002 ,
h i d d e n _ l a y e r _ s i z e s =(50 ,50))"
REFERENCES,0.8295880149812734,"G.3
MORE RESULTS ON FEATURE VALUATIONS"
REFERENCES,0.8314606741573034,"In this part we provide more results on feature removal in Figures 6 and 7. Fig. 6 shows similar
behavior as that shown in the main text. Fig. 7 provides not very distinguishable results of the three
criteria."
REFERENCES,0.8333333333333334,"G.4
MORE AVERAGE RESULTS ON FEATURE VALUATIONS"
REFERENCES,0.8352059925093633,"We provide additional statistical results with the MLP model (Fig. 9) and logistic regression model
(Fig. 10). Meanwhile, we also put the full statistics on the xgboost model in Fig. 8 since in the main
text the table of the samples with “False” grouthtruth label is skipped due to space limit. From Fig. 9
one can observe that Variational Index induces different rankings of the features compared to Shapley
and Banzhaf: Variational Index ranks “Marital Status” the second, while Shapley and Banzhaf put it
in the third location."
REFERENCES,0.8370786516853933,"It is also very interesting to see that the logistic regression model (with the lowest training accuracy
among the three models, shown in Fig. 10) provides different ranking for the ﬁrst two features
compared to MLP and xgboost. For the samples with “True” groundtruth labels, “Education-Num” is
the ﬁrst important feature for the logistic regression model, while “Captical Gain” was ranked ﬁrst
for the MLP and xgboost."
REFERENCES,0.8389513108614233,Published as a conference paper at ICLR 2022
REFERENCES,0.8408239700374532,"logistic
regression"
REFERENCES,0.8426966292134831,xgboost MLP
REFERENCES,0.8445692883895131,𝑇= 0.8
REFERENCES,0.846441947565543,𝑇= 0.1
REFERENCES,0.848314606741573,𝑇= 0.5
REFERENCES,0.850187265917603,"Figure 6: First column: Change of predicted probabilities when removing features. The decoupling
error is included in the legend. Last three columns: waterfall plots of feature importance from
Variational Index, Shapley and Banzhaf."
REFERENCES,0.8520599250936329,"logistic
regression"
REFERENCES,0.8539325842696629,xgboost MLP
REFERENCES,0.8558052434456929,𝑇= 0.8
REFERENCES,0.8576779026217228,𝑇= 0.1
REFERENCES,0.8595505617977528,𝑇= 0.5
REFERENCES,0.8614232209737828,"Figure 7: Not very distinguishable results. First column: Change of predicted probabilities when
removing features. The decoupling error is included in the legend. Last three columns: waterfall
plots of feature importance from Variational Index, Shapley and Banzhaf."
REFERENCES,0.8632958801498127,"G.5
EXPERIMENTS WITH MORE PLAYERS"
REFERENCES,0.8651685393258427,"Furthermore, we experiment with a bit larger number of players (n = 80) using MCMC sampling to
approximate the partial derivative in Eq. (13). The sampling based approximation works pretty fast.
Table 2 shows the top 15 ranked players returned by our method, Shapley value and Banzhaf value
for a synthetic data valuation problem. Note that the ranking of Variational Index is more similar to
that of Banzhaf value than that of Shapley value."
REFERENCES,0.8670411985018727,Published as a conference paper at ICLR 2022
REFERENCES,0.8689138576779026,"GT Labels: True
1
2
3
4
5
6
7
8
9
10
11
12"
REFERENCES,0.8707865168539326,"Var. Index
Capital Gain Relationship
Age
Education-Num
Occupation
Marital Status
Capital Loss
Hours per week
Sex
Race
Workclass
Country"
REFERENCES,0.8726591760299626,"Shapley
Capital Gain Relationship Education-Num
Age
Occupation
Marital Status Hours per week
Capital Loss
Sex
Workclass
Race
Country"
REFERENCES,0.8745318352059925,"Banzhaf
Capital Gain Relationship Education-Num
Age
Occupation
Marital Status
Capital Loss
Hours per week
Sex
Race
Workclass
Country"
REFERENCES,0.8764044943820225,"GT Labels: False
1
2
3
4
5
6
7
8
9
10
11
12"
REFERENCES,0.8782771535580525,"Var. Index
Capital Gain Relationship
Age
Education-Num
Occupation
Marital Status
Capital Loss
Hours per week
Sex
Race
Workclass
Country"
REFERENCES,0.8801498127340824,"Shapley
Capital Gain Relationship
Capital Loss
Age
Education-"
REFERENCES,0.8820224719101124,"Num
Marital Status Hours per week
Occupation
Work"
REFERENCES,0.8838951310861424,"class
Country
Race
Sex"
REFERENCES,0.8857677902621723,"Banzhaf
Capital Gain Relationship
Capital Loss
Age
Education-"
REFERENCES,0.8876404494382022,"Num
Marital Status Hours per week
Occupation
Work"
REFERENCES,0.8895131086142322,"class
Country
Race
Sex"
REFERENCES,0.8913857677902621,"Figure 8: Full statistics on valuations with the xgboost classiﬁer (in the main text the last row is
skipped due to space limit). First row: box plot of valuations returned by the three criteria. We always
consider the predicted probability of the ground truth label. “True” means the samples with positive
ground truth label and “False” means with the negative ground truth label. Second and third rows:
Average ranking of the 12 features. Colored texts denote different rankings among the three criteria."
REFERENCES,0.8932584269662921,"GT Labels: True
1
2
3
4
5
6
7
8
9
10
11
12"
REFERENCES,0.8951310861423221,"Var. Index
Capital Gain
Marital"
REFERENCES,0.897003745318352,Status
REFERENCES,0.898876404494382,Education-
REFERENCES,0.900749063670412,"Num
Relationship
Age
Hours per week
Occupation
Capital Loss
Sex
Country
Race
Workclass"
REFERENCES,0.9026217228464419,"Shapley
Capital Gain Education-"
REFERENCES,0.9044943820224719,"Num
Marital Status
Relationshi"
REFERENCES,0.9063670411985019,"p
Age
Hours per week
Occupation
Sex
Capital"
REFERENCES,0.9082397003745318,"Loss
Race
Country
Workclass"
REFERENCES,0.9101123595505618,"Banzhaf
Capital Gain Education-"
REFERENCES,0.9119850187265918,"Num
Marital Status
Relationship
Age
Hours per week
Occupation
Sex
Capital"
REFERENCES,0.9138576779026217,"Loss
Race
Country
Workclass"
REFERENCES,0.9157303370786517,"GT Labels: False
1
2
3
4
5
6
7
8
9
10
11
12"
REFERENCES,0.9176029962546817,"Var. Index
Capital Gain Relationship
Education-"
REFERENCES,0.9194756554307116,"Num
Capital Loss
Age
Marital Status Hours per week
Sex
Occupation
Country
Workclass
Race"
REFERENCES,0.9213483146067416,"Shapley
Capital Gain Relationship Education-Num
Capital"
REFERENCES,0.9232209737827716,"Loss
Marital Status
Age
Hours per week
Sex
Occupation Workclass Country
Race"
REFERENCES,0.9250936329588015,"Banzhaf
Capital Gain Relationship
Capital Loss
Marital Status
Education-"
REFERENCES,0.9269662921348315,"Num
Age
Hours per week
Sex
Occupation Workclass Country
Race"
REFERENCES,0.9288389513108615,"Figure 9: Statistics on valuations with the MLP classiﬁer. First row: box plot of valuations returned
by the three criteria. We always consider the predicted probability of the ground truth label. “True”
means the samples with positive ground truth label and “False” means with the negative ground truth
label. Second and third rows: Average ranking of the 12 features. Colored texts denote different
rankings among the three criteria."
REFERENCES,0.9307116104868914,"G.6
EFFECT OF NUMBER OF SAMPLES IN MCMC SAMPLING"
REFERENCES,0.9325842696629213,"Here we illustrate the accuracy sampling tradeoff when estimating the gradient of multilinear ex-
tension using MCMC sampling. The results is shown in Fig. 11. One can observe that with more
samples, Alg. 1 will converge faster (in fewer number of epochs)."
REFERENCES,0.9344569288389513,Published as a conference paper at ICLR 2022
REFERENCES,0.9363295880149812,"GT Labels: True
1
2
3
4
5
6
7
8
9
10
11
12"
REFERENCES,0.9382022471910112,"Var. Index
Education-"
REFERENCES,0.9400749063670412,"Num
Relationship Capital Gain
Hours per week Marital"
REFERENCES,0.9419475655430711,"Status
Age
Sex
Capital Loss
Race
Occupation
Country
Workclass"
REFERENCES,0.9438202247191011,"Shapley
Education-"
REFERENCES,0.9456928838951311,"Num
Capital Gain
Relationship
Hours per week"
REFERENCES,0.947565543071161,Marital
REFERENCES,0.949438202247191,"Status
Age
Sex
Occupation
Race
Capital Loss
Country
Workclass"
REFERENCES,0.951310861423221,"Banzhaf
Education-"
REFERENCES,0.9531835205992509,"Num
Capital Gain
Relationship
Hours per week"
REFERENCES,0.9550561797752809,Marital
REFERENCES,0.9569288389513109,"Status
Age
Sex
Occupation
Race
Capital Loss
Country
Workclass"
REFERENCES,0.9588014981273408,"GT Labels: False
1
2
3
4
5
6
7
8
9
10
11
12"
REFERENCES,0.9606741573033708,"Var. Index
Relationship Capital Gain
Education-"
REFERENCES,0.9625468164794008,"Num
Capital Loss
Age
Hours per week
Marital Status
Sex
Workclass
Race
Occupation
Country"
REFERENCES,0.9644194756554307,"Shapley
Relationship Capital Gain
Education- Num"
REFERENCES,0.9662921348314607,Capital
REFERENCES,0.9681647940074907,"Loss
Age
Hours per week
Marital Status
Sex
Workclass
Occupation
Race
Country"
REFERENCES,0.9700374531835206,"Banzhaf
Relationship Capital Gain
Education-"
REFERENCES,0.9719101123595506,"Num
Capital Loss
Age
Hours per week
Marital Status
Sex
Workclass
Occupation
Race
Country"
REFERENCES,0.9737827715355806,"Figure 10: Statistics on valuations with the logistic regression classiﬁer. First row: box plot of
valuations returned by the three criteria. We always consider the predicted probability of the ground
truth label. “True” means the samples with positive ground truth label and “False” means with the
negative ground truth label. Second and Third rows: Average ranking of the 12 features. Colored
texts denote different rankings among the three criteria."
REFERENCES,0.9756554307116105,Table 2: Indices of the top 15 ranked players returned by different methods.
REFERENCES,0.9775280898876404,"Rank of players
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Variational Index
9
13
11
3
54
7
18
36
32
42
46
40
6
18
23
Banzhaf value
9
13
11
3
54
18
7
32
46
36
2
27
40
17
10
Shaplay Value
52
33
27
1
4
58
32
14
42
46
40
6
18
23
47"
REFERENCES,0.9794007490636704,"2
4
6
8
10
12
14
# of Epochs"
REFERENCES,0.9812734082397003,0.0005
REFERENCES,0.9831460674157303,0.0010
REFERENCES,0.9850187265917603,0.0015
REFERENCES,0.9868913857677902,0.0020
REFERENCES,0.9887640449438202,0.0025
REFERENCES,0.9906367041198502,0.0030
REFERENCES,0.9925093632958801,0.0035
REFERENCES,0.9943820224719101,Stepwise Difference
REFERENCES,0.9962546816479401,"m = n
m = 5n
m = 10n"
REFERENCES,0.99812734082397,"Figure 11: Stepwise difference with difference number of samples m when estimating the gradient of
multilinear extension using MCMC sampling. The three curves: m = n, m = 5n, m = 10n."
