Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0033333333333333335,"Large-scale generative models have shown impressive image-generation capabili-
ties, propelled by massive data. However, this often inadvertently leads to the gen-
eration of harmful or inappropriate content and raises copyright concerns. Driven
by these concerns, machine unlearning has become crucial to effectively purge
undesirable knowledge from models. While existing literature has studied various
unlearning techniques, these often suffer from either poor unlearning quality or
degradation in text-image alignment after unlearning, due to the competitive nature
of these objectives. To address these challenges, we propose a framework that seeks
an optimal model update at each unlearning iteration, ensuring monotonic improve-
ment on both objectives. We further derive the characterization of such an update.
In addition, we design procedures to strategically diversify the unlearning and re-
maining datasets to boost performance improvement. Our evaluation demonstrates
that our method effectively removes target classes from recent diffusion-based
generative models and concepts from stable diffusion models while maintaining
close alignment with the models’ original trained states, thus outperforming state-
of-the-art baselines. Our code will be made available at https://github.com/
reds-lab/Restricted_gradient_diversity_unlearning.git."
INTRODUCTION,0.006666666666666667,"1
Introduction"
INTRODUCTION,0.01,"Large-scale text-to-image generative models have recently gained considerable attention for their
impressive image-generation capabilities. Despite being at the height of their popularity, these
models, trained on vast amounts of public data, inevitably face concerns related to harmful content
generation [Heng and Soh, 2024] and copyright infringement [Zhang et al., 2023b]. Although"
INTRODUCTION,0.013333333333333334,*Equal contributions
INTRODUCTION,0.016666666666666666,"Figure 1: Generated images using SalUn [Fan et al., 2023], ESD [Gandikota et al., 2023], and Ours
after unlearning given the condition. Each row indicates different unlearning tasks: nudity removal,
and Van Gogh style removal. Generated images from our approach and SD [Rombach et al., 2022] are
well-aligned with the prompt, whereas SalUn and ESD fail to generate semantically correct images
given the condition. On average, across 100 different prompts, SalUn shows the lowest clip alignment
scores (0.305 for nudity removal and 0.280 for Van Gogh style removal), followed by ESD (0.329
and 0.330, respectively). Our approach achieves scores of 0.350 and 0.352 for these tasks, closely
matching the original SD scores of 0.352 and 0.348."
INTRODUCTION,0.02,"exact machine unlearning—retraining the model by excluding target data—is a direct solution, its
computational challenge has driven continued research on approximate machine unlearning."
INTRODUCTION,0.023333333333333334,"To address this challenge, recent studies [Fan et al., 2023, Gandikota et al., 2023, Heng and Soh, 2024],
have introduced approximate unlearning techniques aimed at boosting efficiency while preserving
effectiveness. These approaches have successfully demonstrated the ability to remove target concepts
while maintaining the model’s general image generation capabilities, with generation quality assessed
using the Fréchet Inception Distance. However, these studies generally overlook the impact of
unlearning on image-text alignment, which pertains to the semantic accuracy between generated
images and their text descriptions [Lee et al., 2024]. While pretrained generative models generally
demonstrate high alignment scores, our study reveals a critical gap: state-of-the-art unlearning
techniques fall short in achieving comparable text-image alignment scores after unlearning, as
illustrated in Figure 1. This could lead to potentially problematic behaviors in real-world deployments,
necessitating further investigation."
INTRODUCTION,0.02666666666666667,"We attribute the failure of existing techniques to maintain text-image alignment to two primary
factors. Firstly, the unlearning objective often conflicts with the goal of maintaining low loss on the
retained data, illustrating the competitive nature of these two objectives. Traditionally, approaches to
optimizing these objectives have simply aggregated the gradients from both; however, this method of
updating the model typically advances one objective at the expense of the other. Hence, while these
approaches may successfully remove target concepts, they often compromise text-image alignment
for retained concepts in the process. Secondly, current methods employ a simplistic approach to
constructing a dataset for loss minimization on retained concepts. For example, in Fan et al. [2023],
this dataset is composed of images generated from a single prompt associated with the concept to be
removed. This lack of diversity in the dataset might lead to overfitting, which in turn hampers the
text-image alignment."
INTRODUCTION,0.03,"To address these issues, we propose a principled framework designed to optimally balance the
objectives of unlearning the target data and maintaining performance on the remaining data at each
update iteration. Specifically, we introduce the concept of the restricted gradient, which allows
for the optimization of both objectives while ensuring monotonic improvement in each objective.
Furthermore, we have developed a deliberate procedure to enhance data diversity, preventing the
model from overfitting to the limited samples in the remaining dataset. To the best of our knowledge,
the strategic design of the forgetting target and remaining sets has not been extensively explored in
the existing machine unlearning literature. In our evaluation, we demonstrate the improvement in
both forgetting quality and alignment on the remaining data, compared to baselines. For example, our"
INTRODUCTION,0.03333333333333333,"evaluation in nudity removal demonstrates that our method effectively reduces the number of detected
body parts to zero, compared to 598 with the baseline stable diffusion (SD) [Rombach et al., 2022],
48 with erased stable diffusion (ESD-u), and 3 with saliency map-based unlearning (SalUn) [Fan
et al., 2023]. Particularly, while achieving this effective erasing performance, our method reduces the
alignment gap to SD by 11x compared to ESD-u and by 20x compared to SalUn on the retained test
set."
RELATED WORK,0.03666666666666667,"2
Related Work"
MACHINE UNLEARNING,0.04,"2.1
Machine Unlearning"
MACHINE UNLEARNING,0.043333333333333335,"Machine unlearning has primarily been propelled by the ""Right to be Forgotten"" (RTBF), which
upholds the right of users to request the deletion of their data. Given that large-scale models are
often trained on web-scraped public data, this becomes a critical consideration for model developers
to avoid the need for retraining models with each individual request. In addition to RTBF, recent
concerns related to copyrights and harmful content generation further underscore the necessity and
importance of in-depth research in machine unlearning. The principal challenge in this field lies
in effectively erasing the target concept from pre-trained models while maintaining performance
on other data. Recent studies have explored various approaches to unlearning, including the exact
unlearning method [Bourtoule et al., 2021] and approximate methods such as using negative gradients,
fine-tuning without the forget data, editing the entire parameter space of the model [Golatkar et al.,
2020]. To encourage the targeted impact in the parameter space, [Golatkar et al., 2020, Foster
et al., 2024] proposed leveraging the Fisher information matrix, and [Fan et al., 2023] leveraged
a gradient-based weight saliency map to identify crucial neurons, thus minimizing the impact on
remaining neurons. Furthermore, data-influence-based debiasing and unlearning have also been
proposed [Chen et al., 2024, Bae et al., 2023]. Another line of work leverages mathematical tools in
differential privacy [Guo et al., 2019, Chien et al., 2024] to ensure that the model’s behavior remains
indistinguishable between the retrained and unlearned models."
MACHINE UNLEARNING IN DIFFUSION MODELS,0.04666666666666667,"2.2
Machine Unlearning in Diffusion Models"
MACHINE UNLEARNING IN DIFFUSION MODELS,0.05,"Recent advancements in text-conditioned generative models [Ho and Salimans, 2022, Rombach
et al., 2022], trained on extensive web-scraped datasets like LAION-5B [Schuhmann et al., 2022],
have raised significant concerns about the generation of harmful content and copyright violations. A
series of studies have addressed the challenge of machine unlearning in diffusion models [Heng and
Soh, 2024, Gandikota et al., 2023, Zhang et al., 2023a, Fan et al., 2023]. One approach [Heng and
Soh, 2024] interprets machine unlearning as a continual learning problem, showing effective removal
results in classification tasks by employing Bayesian approaches to continual learning [Kirkpatrick
et al., 2017], which enhance unlearning quality while maintaining model performance using generative
reply [Shin et al., 2017]. However, this approach falls short in removing concepts such as nudity
compared to other methods [Gandikota et al., 2023]. Another proposed method [Gandikota et al.,
2023] guides the pre-trained model toward a prior distribution for the targeted concept but struggles
to preserve performance. The most recent work [Fan et al., 2023] proposes selectively damaging
neurons based on a saliency map and random labeling techniques, although this method tends to
overlook the quality of the remaining set, focusing on improving the forgetting quality, which does
not fully address the primary challenges in the machine unlearning community. Although [Bae et al.,
2023] presents a similar multi-task learning framework for variational autoencoders, their work does
not show the optimality of their solution, and their experiments mainly focus on small-scale models,
due to the computational expense associated with influence functions."
OUR APPROACH,0.05333333333333334,"3
Our Approach"
OUR APPROACH,0.056666666666666664,"We study the efficacy of our approach in unlearning by removing target classes from class-conditional
diffusion models or eliminating specific concepts from text-to-image models while maintaining their
general generation capabilities. We will call the set of data points to be removed as the forgetting
dataset. To set up the notations, let D denote the training set and Df ⊂D be the forgetting dataset.
We will use Dr = D\Df to denote the remaining dataset. Our approach only assumes access to some
representative points for Df and Dr. As discussed later, depending on specific applications, these data"
OUR APPROACH,0.06,"points can be either directly sampled from Df and Dr or generated based on the high-level concept
of Df to be removed. With a slight abuse of notation, we will use Dr and Df to also denote the
actual representative samples used to operationalize our proposed approach. Furthermore, we denote
the model parameter by θ. Let l be a proper learning loss function. The loss of remaining data and
that of forgettng data are represented by Lr(θ) := P"
OUR APPROACH,0.06333333333333334,"z∈Dr l(θ, z) and Lf(θ) := −λ P"
OUR APPROACH,0.06666666666666667,"z∈Df l(θ, z),
respectively, where λ is a weight adjusting the importance of forgetting loss relative to the remaining
data loss. We term Lr and Lf remaining loss and forgetting loss, respectively. We note that in the
context of diffusion models, loss function l is defined as l = Et,x0,ϵ∼N(0,1)

∥ϵ −eθ(xt, t)∥2
, where
xt is a noisy version of x0 generated by adding Gaussian noise to the clean image x0 ∼pdata(x)
at time step t with a noise scheduler, and eθ(xt, t) is the model’s estimate of the added noise ϵ at
time t [Xu et al., 2023, Ho et al., 2020]. For text-to-image generative models, the loss function l
is specified as l = Et,q0,c,ϵ

∥ϵ −ϵθ(qt, t, η)∥2
, where q0 is an encoded latent q0 = E(x0) with
encoder E, and qt is a noisy latent at time step t. The noise prediction ϵθ(qt, t, η) is conditioned on
the timestep t and a text η."
OUR APPROACH,0.07,"Optimizing the Update.
Similar to existing work Fan et al. [2023], our objective is to find an
unlearned model with parameters θu, starting from a pre-trained model with weights θ0, such that
the model forgets the target concepts in Df while maintaining its utility on the remaining dataset
Dr. Formally, we aim to maximize the forget error on Df, represented by Lf(θ), while minimizing
the retain error on Dr, represented by Lr(θ). This can be formulated as minθ Lr(θ) + Lf(θ), where
our approach applies iterative updates to achieve both goals simultaneously. A simple approach to
optimize this objective, often adopted by existing work, is to calculate the gradient ∇Lr(θ)+∇Lf(θ)
and use it to update the model parameters at each iteration. However, empirically, we observe that
the two gradients usually conflict with each other, i.e., the decrease of one objective is at the cost
of increasing the other; therefore, in practice, this approach yields a significant tradeoff between
forgetting strength and model utility on the remaining data. In this work, we aim to present a
principled approach to designing the update direction at each iteration that more effectively handles
the tradeoff between forgetting strength and model utility on the remaining data. Our key idea is to
identify a direction that achieves a monotonic decrease of both objectives."
OUR APPROACH,0.07333333333333333,"To describe our algorithm, we briefly review the directional derivative.
Definition 1 (Directional Derivative). The directional derivative [Spivak, 2006] of a function f at x
in the direction of v is written as"
OUR APPROACH,0.07666666666666666,"Dvf(x) = lim
h→0
f(x + hv)"
OUR APPROACH,0.08,"h
.
(1)"
OUR APPROACH,0.08333333333333333,"This special form of the derivative has the useful property that its maximizer can be related to the
gradient ∇xf(x), which we formally state below.
Theorem 2 (Directional derivative maximizer is the gradient). Let f be a function on x. Then the
maximum value of the directional derivative of f at x is |∇f(x)| the ℓ2 norm of its gradient. Moreover,
the direction v is the gradient itself, i.e.,"
OUR APPROACH,0.08666666666666667,"arg max
v
Dvf = ∇f(x).
(2)"
OUR APPROACH,0.09,"In unlearning, we are specifically interested in the gradient of two losses, the forgetting loss Lf and
the remaining loss Lr. Moreover, we seek gradient directions that simultaneously improve on both.
This motivates the restricted gradient, which we define below.
Definition 3 (Restricted gradient, local form for minimization). The negative restricted gradient of
two losses Lα, Lβ is any direction v at θ satisfying"
OUR APPROACH,0.09333333333333334,"min
v
Dv
 
Lα + Lβ

(θ)
s.t.
DvLα(θ)
≤0,
DvLβ(θ)
≤0."
OUR APPROACH,0.09666666666666666,"Intuitively, with the restricted gradient we seek to define the ideal direction for unlearning. We would
like to optimize the joint loss L = Lr + Lf subject to the condition that at every parameter update
step, Lr and Lf experience monotonic improvement. This is precisely the step prescribed by the
negative restricted gradient. Since the learning rates used to fine-tune the parameters in the unlearning
process are typically quite small, we can approximate the updated loss at each iteration via a simple
first-order Taylor expansion. In this case, the restricted gradient takes a simple form."
OUR APPROACH,0.1,"Theorem 4 (Characterizing the restricted gradient under linear approximation). Given θ, suppose
that Lr(θ + δ) −Lr(θ) ≈δ · ∇Lr and Lf(θ + δ) −Lf(θ) ≈δ · ∇Lf. The restricted gradient can
be written as"
OUR APPROACH,0.10333333333333333,"arg min
v
Dv(Lf + Lr)(θ) = δ∗
f + δ∗
r,
(3) where"
OUR APPROACH,0.10666666666666667,"δ∗
f = ∇Lf −∇Lf · ∇Lr"
OUR APPROACH,0.11,"∥∇Lr∥2
∇Lr,
δ∗
r = ∇Lr −∇Lf · ∇Lr"
OUR APPROACH,0.11333333333333333,"∥∇Lf∥2 ∇Lf,
(4)"
OUR APPROACH,0.11666666666666667,"when we have conflicting unconstrained gradient terms, i.e. ∇Lf · ∇Lr < 0."
OUR APPROACH,0.12,"Figure 2: Visualization of the update. We show the up-
date direction (gray) obtained by (a) directly summing
up the two gradients and (b) our restricted gradient."
OUR APPROACH,0.12333333333333334,"The theorem presented demonstrates that
the restricted gradient is determined by
aggregating the modifications from ∇Lf
and ∇Lr. This modification process in-
volves projecting ∇Lf onto the normal
vector of ∇Lr, yielding δ∗
f, and similarly
projecting ∇Lr onto the normal vector of
∇Lf, resulting in δ∗
r. The optimal update,
as derived in Theorem 4, is illustrated in
Figure 2. Notably, when ∇Lf and ∇Lr
have equal norms, the restricted gradient
matches the direct summation of the two
original gradients, namely, ∇Lf + ∇Lr.
However, it is more common for the norm
of one gradient to dominate the other, in
which case the restricted gradient provides
a more balanced update compared to direct
aggregation."
OUR APPROACH,0.12666666666666668,"Remark 1. We wish to highlight an intriguing link between the gradient aggregation mechanism
presented in Theorem 4 and an existing method to address gradient conflicts across different tasks in
multi-task learning. This restricted gradient coincides exactly with the gradient surgery procedure
introduced in Yu et al. [2020]. While their original paper presented the procedure from an intuitive
perspective, our work offers an alternative viewpoint and rigorously characterizes the objective
function that the gradient surgery procedure optimizes."
OUR APPROACH,0.13,"Diversify Dr.
Since D \ Df is usually of enormous scale, it is infeasible to incorporate all of them
into the remaining dataset Dr for running the optimization. In practice, one can only sample a subset
of points from Dr. In our experiments, we find that the diversity of Dr plays an important role in
maintaining the model performance on the remaining dataset, as seen in Section 4.2. Therefore, we
propose procedures for forming a diverse Dr. For models with a finite set of class labels, such as
diffusion models trained on CIFAR-10, we adopt a simple procedure of maintaining an equal number
of samples for each class in Dr. Our ablation studies in Section 4.4 show that this is more effective in
maintaining model performance on the remaining dataset than more sophisticated procedures, such as
selecting the most similar examples to the forgetting samples. The intuitive reason is that reminding
the model of as many fragments as possible related to the remaining set during each forgetting step is
crucial. By doing so, it leads to finding a representative restricted descent direction, which helps the
model to precisely erase the forget data while maintaining a state comparable to the original model.
When the text input is unconstrained, such as in the stable diffusion model setting, to strategically
design diverse information, we propose the following procedure to generate Dr based on the concept
to be forgotten, denoted by c. Using a large language model (LLM), we first generate diverse text
prompts related to concept c, yielding prompt set Yc. These prompts are then modified by removing
all references to c, creating a parallel set Y. By passing both Yc and Y through the target diffusion
model, we obtain corresponding image sets Xc and X. This process allows us to construct our final
datasets: Df = {(x, y) | x ∈Xc, y ∈Yc} and Dr = {(x, y) | x ∈X, y ∈Y}. Example prompts
and detailed descriptions are provided in Appendix D."
EXPERIMENT,0.13333333333333333,"4
Experiment"
EXPERIMENT,0.13666666666666666,"In this study, we address the crucial challenge of preventing undesirable outputs in text-to-image
generative models. We begin by examining class-wise forgetting with CIFAR-10 diffusion-based
generative models, where we demonstrate our method’s ability to selectively prevent the generation
of specific class images (Section 4.2). We then explore the effectiveness of our approach in removing
nudity and art styles (Section 4.3) to address real-world concerns of harmful content generation and
copyright infringement. We further study the impact of data diversity (Section 4.4) as well as the
sensitivity of our method to hyperparameter settings (Section 4.4)."
EXPERIMENT SETUP,0.14,"4.1
Experiment Setup"
EXPERIMENT SETUP,0.14333333333333334,"For our CIFAR-10 experiments, we leverage the EDM framework [Karras et al., 2022], which intro-
duces some modeling improvements including a nonlinear sampling schedule, direct x0-prediction,
and a second-order Heun solver, achieving the state-of-the-art FID on CIFAR-10. For stable diffusion,
we utilize the pre-trained Stable Diffusion version 1.4, following prior works. Both implementations
require two key hyperparameters: the weight λ of the gradient descent direction relative to the
ascent direction, and the loss truncation value α, which prevents unbounded loss maximization
during unlearning. Detailed hyperparameter configurations are provided in Appendix C. For dataset
construction, we used all samples in each class for the CIFAR-10 forgetting dataset and 800 samples
for Stable Diffusion experiments. Considering the practical constraints of accessing complete datasets
in real-world scenarios, we construct the remaining dataset Dr by sampling 1% of data from each
retained class, yielding a total of 450 samples for CIFAR-10 (50 from each of the 9 non-target classes)
and 800 samples for Stable Diffusion."
EXPERIMENT SETUP,0.14666666666666667,"As our baselines for CIFAR-10 experiments, we consider Finetune [Warnecke et al., 2021], gradient
ascent and descent, referred to as GradDiff, and SalUn [Fan et al., 2023]. For concept removal, our
baselines include the pretrained diffusion model SD [Rombach et al., 2022], erased stable diffusion
ESD [Gandikota et al., 2023], and SalUn [Fan et al., 2023]. To fairly compare, We further consider
the variants of ESD, depending on the unlearning task. We note that we do not consider the baseline
by [Heng and Soh, 2024] due to its demonstrated limited performance in nudity removal, compared
to ESD. Our approach is referred to as RG when applied only with the restricted gradient, and RGD
when data diversity is incorporated."
EXPERIMENT SETUP,0.15,"Table 1: Quantitative evaluation of unlearning methods on CIFAR-10
diffusion-based generative models. Each method was evaluated by
sequentially targeting each of the 10 CIFAR-10 classes for unlearning.
For each target class, we measure unlearning accuracy (UA) specific
to that class, remaining accuracy (RA) on the other 9 classes, and FID
for generation quality. The reported values are averaged across all 10
class-specific unlearning experiments."
EXPERIMENT SETUP,0.15333333333333332,"Unlearning Method
Class-wise Forgetting"
EXPERIMENT SETUP,0.15666666666666668,"UA ↑
RA ↑
FID ↓"
EXPERIMENT SETUP,0.16,"Finetune
0.211±0.126
0.791±0.023
4.252±0.482
SalUn
0.512±0.173
0.434±0.051
14.40±3.242
GradDiff
1.000±0.000
0.734±0.021
14.09±2.531
RG (Ours)
1.000±0.000
0.752±0.018
9.813±1.863
RGD (Ours)
1.000±0.000
0.771±0.016
6.539±0.994"
EXPERIMENT SETUP,0.16333333333333333,"We evaluate our approach
using multiple metrics to
assess both forgetting ef-
fectiveness and model util-
ity. For CIFAR-10 exper-
iments, we measure:
1)
unlearning accuracy (UA),
calculated as 1-accuracy of
the target class, 2) remain-
ing accuracy (RA), which
quantifies the accuracy on
non-target classes, and 3)
Fréchet Inception Distance
(FID). We observed that
standard CIFAR-10 classi-
fiers demonstrate inherent
bias when evaluating gen-
erated samples from un-
learned classes, predomi-
nantly assigning these noise-like images to a particular class among the ten categories—a limitation
arising from their training exclusively on clean class samples. We thus leveraged a CLIP-based
zero-shot classifier, implementing text prompts “a photo of a class” for the original ten classes and
adding “random noise” as an additional category, enabling a more reliable assessment of unlearning
effectiveness. We generate 50K images for FID calculation. For concept removal in Stable Diffusion,
we assess forgetting effectiveness using Nudenet [Bedapudi, 2019], which detects exposed body
parts in generated images prompted by I2P [Schramowski et al., 2023]. After filtering prompts with"
EXPERIMENT SETUP,0.16666666666666666,"non-zero nudity ratios, we obtain 853 evaluation prompts from an initial set of 4,703. To evalu-
ate the retained performance, following[Lee et al., 2024], we measure semantic correctness using
CLIP [Cherti et al., 2023] alignment scores (AS) between prompts and their generated images. We
evaluate model performance on both training prompts (Dr,train) used during unlearning and a separate
set of held-out test prompts (Dr,test). These two distinct sets are constructed by carefully splitting
semantic dimensions (e.g., activities, environments, moods). Detailed construction procedures for
both sets are provided in Appendix D."
TARGET CLASS REMOVAL FROM DIFFUSION MODELS,0.17,"4.2
Target Class Removal from Diffusion Models"
TARGET CLASS REMOVAL FROM DIFFUSION MODELS,0.17333333333333334,"We present the CIFAR-10 experiment results in Table 1. To fairly compare, we use the same
remaining dataset for other baselines. Our finding first indicates that while Finetune achieves
superior performance on retained data (highest RA and FID scores), it struggles to effectively unlearn
target classes with this limited remaining dataset. Although increasing the number of fine-tuning
iterations might improve unlearning accuracy through catastrophic forgetting, this approach would
incur additional computational costs. Secondly, we observe that SalUn has low RA, compared
to other baselines even with their comparable FID performance. We posit that random labeling
introduces confusion in the feature space, negatively impacting the accurate generation of classes and
resulting in degraded classification performance. Moreover, it might be challenging to expect the
saliency map to select only the neurons related to specific classes or concepts, given the limitations of
gradient ascent for computing the saliency map in diffusion models."
TARGET CLASS REMOVAL FROM DIFFUSION MODELS,0.17666666666666667,"The Impact of Restricted Gradient and Data Diversity
Our observations are as follows. 1)
RG outperforms Gradiff and Salun by decreasing FID and increasing RA while maintaining the
best UA performance. 2) RGD shows improvements over RG, suggesting that data diversification, in
conjunction with the restricted gradient, further enhances performance in terms of RA and FID. We
vary the hyperparameters and provide the results in section 4.4."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.18,"4.3
Target Concept Removal from Diffusion Models"
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.18333333333333332,"Target concept removal has been a primary focus in diffusion model unlearning literature, driven by
the need to mitigate undesirable content generation. While existing methods have shown potential for
removing nudity or art styles, our study reveals that they often compromise model alignment after
unlearning."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.18666666666666668,"Nudity Removal.
We summarize our results in Figure 4 and Table 2. We observe that Salun tends
to generate samples that are overfit to the remaining dataset. Although Salun shows promising
performance in nudity removal—detecting fewer exposed body parts compared to SD and ESD-u,
as shown in Figure 4—this success comes at the cost of output diversity. In particular, SalUn often
generates semantically similar images (e.g., men, wall backgrounds) for both forgetting concepts
(Figure 3) and remaining data (Figure 1). Table 4 quantitatively validates this observation, revealing
SalUn’s lowest alignment scores post-unlearning. These results suggest that SalUn’s forgetting
performance could stem from overfitting. This limitation may arise from two factors: the selected
neurons potentially affecting both target and non-target concepts, and the limited diversity in their
forget and remaining datasets. In the case of ESD, the resulting model often fails to remove the nudity
concept from unlearned models, as shown in Figure 4. We also evaluate ESD-u and observe that the
nudity removal performance between ESD and ESD-u are quite similar although it achieves better
AS than SalUn. They suggest using “nudity” as a prompt for unlearning, but it might be difficult to
reflect the entire semantic space related to the concept of “nudity,” given that we can describe nudity
in many different ways using paraphrasing."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.19,"Figure 3: Generated images using SD, SalUn, ESD-u, and
RGD (Ours). Each row indicates generated images with
different prompts including nudity-related I2P prompts and
samples from Df. Each column shows the images generated
by different unlearning methods."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.19333333333333333,"Figure 4: The nudity detection results
by Nudenet, following prior works
[Fan et al., 2023, Gandikota et al.,
2023]. The Y-axis shows the exposed
body part in the generated images,
given the prompt, and the X-axis de-
notes the number of images generated
by each unlearning method and SD.
We exclude bars from the plot if the
corresponding value is zero."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.19666666666666666,"Table 2: Nudity and artist removal: we calculate the clip alignment score (AS), following Lee et al.
[2024], to measure the model alignment on the remaining set after unlearning. Cells highlighted
in green indicate results from our method, while those in red indicate results from the pretrained
model."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.2,"AS (∆)*
Nudity Removal
Artist Removal"
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.20333333333333334,"Dr,train
Dr,test
Dr,train
Dr,test
SD
0.357
0.352
0.349
0.348"
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.20666666666666667,"ESD**
0.327 (0.030)
0.329 (0.023)
0.300 (0.049)
0.298 (0.050)"
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.21,"ESD-u**
0.327 (0.03)
0.329 (0.023)
-
-"
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.21333333333333335,"ESD-x**
-
-
0.333 (0.016)
0.330 (0.018)"
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.21666666666666667,"SalUn
0.305 (0.052)
0.312 (0.040)
0.279 (0.070)
0.280 (0.068)"
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.22,"GradDiffD (Ours)
0.342 (0.015)
0.348 (0.004)
0.334 (0.015)
0.333 (0.015)"
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.22333333333333333,"RGD (Ours)
0.354 (0.003)
0.350 (0.002)
0.355 (-0.006)
0.352 (-0.004)"
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.22666666666666666,"* The values in parentheses, ∆, refer to the gap between the original SD and the unlearned
model with each method.
** ESD, ESD-u, and ESD-x refer to training on full parameters, non-cross-attention weights,
and cross-attention weights, respectively."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.23,"RGD outperforms state-of-the-art baselines in terms of forget quality (i.e., zero detection of exposed
body part given I2P prompts as described in Figure 4) and retain quality (i.e., high AS presented in
Table 2), effectively mitigating the trade-off between the two tasks. To further validate the role of both
the restricted gradient and diversification steps to nudity removal, we conduct a two-way ablation
study. Removing the restricted gradient step from RGD yields GradDiffD, which incorporates
dataset diversity into GradDiff, whereas removing the diversification step yields the previously
introduced RG. RGD’s superior performance over both GradDiffD (Table 7 and Figure 4) and RG
(Table 4) underscores the crucial importance of both steps in our proposed unlearning algorithm."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.23333333333333334,"Art Style Removal.
Similar to nudity removal, the task of eliminating specific art styles presents
a significant challenge. In order to evaluate whether the unlearning methods inadvertently impact
other concepts and semantics beyond the targeted art style, we prompt the model with other artists’
styles (e.g., Monet, Picasso) while targeting to remove Vincent van Gogh’s style. The results of
generation examples are shown in Figure 1 and Figure 5, and the average alignment scores are shown
in Table 2. It is observed that SalUn cannot follow the prompt to generate other artists’ styles and
shows a significant drop in alignment scores (AS) compared with the pre-trained SD."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.23666666666666666,"Figure 5: Art style removal. Each row represents different prompts
used to evaluate the alignment and each column indicates generated
images from different unlearning methods."
TARGET CONCEPT REMOVAL FROM DIFFUSION MODELS,0.24,"We
also
train
ESD-x
by
modifying the cross-attention
weights, which is more suit-
able for erasing artist styles
than full-parameter training
(shown as plain ESD without
any suffix) as proposed in
ESD work.
Although ESD-x
performs similarly to RG in
terms of alignment scores,
after manual inspection of
the generated images, we find
ESD-x sometimes generates
images
ignoring
the
style
instructions as presented in
Figure 1, while RG generates images with lower quality details like noisy backgrounds but adheres
well to the style instructions.
Consequently, after incorporating gradient surgery to prevent
interference between retain and forgot targets, our RGD achieves better image quality and shows the
best alignment score, almost equivalent to the performance of the pre-trained SD."
ABLATION,0.24333333333333335,"4.4
Ablation"
ABLATION,0.24666666666666667,"Figure 6: Performance analysis across different hyperparameter
settings. Each box plot captures the variation over different α
values for a given λ setting (λ ∈{0.5, 1.0, 5.0}), measuring both
generation quality (FID, left) and remaining accuracy (RA, right).
Lower FID indicates better generation quality, while higher RA
indicates better model utility of non-target concepts."
ABLATION,0.25,"Ablation
in
Hyperparam-
eters.
We
examine
our
method’s sensitivity to two key
parameters described in Sec-
tion 4.1: the retained gradient
weight λ and loss truncation
threshold α. Figure 6 presents
the variation over different
α values (y-axis) for a given
λ value (x-axis), measuring
both
remaining
accuracy
(RA) and generation quality
(FID). Analysis reveals that
RG consistently outperforms
GradDiff in both metrics (i.e.
achieving the lower FID, and
higher or comparable RA with
low variation across different
α), with RGD showing further
improvements.
RGD exhibits
the lowest variance across
different α values and achieves the lowest FID and highest RA. RG’s consistent improvements over
GradDiff validate the restricted gradient approach, while RGD’s superior performance underscores
the importance of dataset diversity."
ABLATION,0.25333333333333335,"Ablation in Diversity.
We further investigate the impact of data diversity through controlled ex-
periments. For CIFAR-10, we design two scenarios based on feature similarity analysis using CLIP
embeddings: Case 1, where Dr contains samples from only the two classes most semantically similar
to the target class, and Case 2, with balanced sampling across all classes. This design stems from our"
ABLATION,0.25666666666666665,"Table 3: Comparison of UA, RA, and FID for diversity-controlled experiments in CIFAR-10 diffusion
models. In this context, Case 1 represents a scenario where the remaining set lacks diversity (i.e., it
only includes samples from two closely related classes), while Case 2 includes equal samples from
all classes. We note that we used the same remaining dataset size between both cases."
ABLATION,0.26,"Unlearning Method
Case 1
Case 2
∆= Case 2 −Case 1"
ABLATION,0.2633333333333333,"UA↑
RA↑
FID↓
UA↑
RA↑
FID↓
UA
RA
FID"
ABLATION,0.26666666666666666,"GradDiff
1.000±0
0.106±0.086
156.021±31.901
1.000±0
0.201±0.043
95.287±16.279
0.000
+0.095
-60.734"
ABLATION,0.27,"RG (Ours)
1.000±0
0.205±0.138
131.247±46.049
1.000±0
0.463±0.059
47.797±7.231
0.000
+0.258
-83.450"
ABLATION,0.2733333333333333,"RGD (Ours)
1.000±0
0.239±0.071
94.259±28.217
1.000±0
0.675±0.019
10.456±1.976
0.000
+0.436
-83.803"
ABLATION,0.27666666666666667,"hypothesis that unlearning a target class may particularly affect semantically related classes, making
their retention critical. We compute class similarities using cosine distance between CLIP feature
vectors as described in Figure 7. Table 3 shows that limited diversity (Case 1) significantly impacts
model performance, with FID increasing by 83.803 for RGD. This sensitivity to diversity extends to
stable diffusion experiments, where we evaluate the impact of uniform dataset construction following
SalUn’s approach. As shown in Table 4, RG with uniform datasets shows a larger performance gap
from SD (∆= 0.032 in test alignment scores) compared to RGD (∆= 0.001). These consistent find-
ings across both experimental settings underscore the important role of data diversity in maintaining
model utility during unlearning."
CONCLUSION,0.28,"5
Conclusion"
CONCLUSION,0.2833333333333333,"Table 4: Comparison of alignment score (AS) be-
tween RGD and RG. RG, in this table, indicates the
case when we have uniform forgetting and remain-
ing datasets but utilize the restricted gradient."
CONCLUSION,0.2866666666666667,"AS (∆)*
Nudity Removal"
CONCLUSION,0.29,"Dr,train
Dr,test
SD
0.357
0.352"
CONCLUSION,0.29333333333333333,"RG
0.330 (0.027)
0.320 (0.032)"
CONCLUSION,0.2966666666666667,"RGD
0.354 (0.003)
0.351 (0.001)"
CONCLUSION,0.3,"* The values in parentheses, ∆, refer
to the gap between the original SD
and the unlearned model with each
method."
CONCLUSION,0.30333333333333334,"This study advances the understanding of ma-
chine unlearning in text-to-image generative
models by introducing a principled approach to
balance forgetting and remaining objectives. We
show that the restricted gradient provides an op-
timal update for handling conflicting gradients
between these objectives, while strategic data
diversification ensures further improvements on
model utilities. Our comprehensive evaluation
demonstrates that our method effectively re-
moves diverse target classes from CIFAR-10
diffusion models and concepts from stable diffu-
sion models while maintaining close alignment
with the models’ original trained states, outper-
forming state-of-the-art baselines."
LIMITATION AND BROADER IMPACTS,0.30666666666666664,"5.1
Limitation and Broader Impacts"
LIMITATION AND BROADER IMPACTS,0.31,"While our solution introduces computation-
efficient retain set generation using LLMs, the
strategic sampling of retain sets for stable diffusion models presents intriguing research directions.
Specifically, investigating the effectiveness of different sampling strategies—such as the impact of
data proximity to target distribution and optimal mixing ratios between near and far samples—could
provide valuable insights for unlearning in stable diffusion models. Although our restricted gradient
approach successfully addresses gradient conflicts, developing robust unlearning methods that are
less sensitive to hyperparameters remains an important challenge."
ACKNOWLEDGEMENT,0.31333333333333335,"6
Acknowledgement"
ACKNOWLEDGEMENT,0.31666666666666665,"RJ and the ReDS lab acknowledge support through grants from the Amazon-Virginia Tech Initiative
for Efficient and Robust Machine Learning, and NSF CNS-2424127, and the Cisco Research Award.
MJ acknowledges the support from NSF ECCS-2331775, IIS-2312794, and the Commonwealth
Cyber Initiative. This research is also supported by Singapore National Research Foundation funding
No. 053424, DARPA funding No. 112774-19499, and NSF IIS-2229876."
REFERENCES,0.32,References
REFERENCES,0.3233333333333333,"Seohui Bae, Seoyoon Kim, Hyemin Jung, and Woohyung Lim. Gradient surgery for one-shot
unlearning on generative model. arXiv preprint arXiv:2307.04550, 2023."
REFERENCES,0.32666666666666666,"P Bedapudi. Nudenet: Neural nets for nudity classification, detection and selective censoring, 2019."
REFERENCES,0.33,"Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers,
Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium
on Security and Privacy (SP), pages 141–159. IEEE, 2021."
REFERENCES,0.3333333333333333,"Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng,
Joey Tianyi Zhou, Jian Wu, and Zuozhu Liu. Fast model debias with machine unlearning. Advances
in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.33666666666666667,"Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade
Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for
contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 2818–2829, 2023."
REFERENCES,0.34,"Eli Chien, Haoyu Wang, Ziang Chen, and Pan Li. Langevin unlearning: A new perspective of noisy
gradient descent for machine unlearning. arXiv preprint arXiv:2401.10371, 2024."
REFERENCES,0.3433333333333333,"Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. Salun: Em-
powering machine unlearning via gradient-based weight saliency in both image classification and
generation. arXiv preprint arXiv:2310.12508, 2023."
REFERENCES,0.3466666666666667,"Jack Foster, Stefan Schoepf, and Alexandra Brintrup. Fast machine unlearning without retraining
through selective synaptic dampening. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 38, pages 12043–12051, 2024."
REFERENCES,0.35,"Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts
from diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 2426–2436, 2023."
REFERENCES,0.35333333333333333,"Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net:
Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9304–9312, 2020."
REFERENCES,0.3566666666666667,"Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal
from machine learning models. arXiv preprint arXiv:1911.03030, 2019."
REFERENCES,0.36,"Alvin Heng and Harold Soh. Selective amnesia: A continual learning approach to forgetting in deep
generative models. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.36333333333333334,"Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,
2022."
REFERENCES,0.36666666666666664,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
neural information processing systems, 33:6840–6851, 2020."
REFERENCES,0.37,"Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-
based generative models. In Proc. NeurIPS, 2022."
REFERENCES,0.37333333333333335,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114
(13):3521–3526, 2017."
REFERENCES,0.37666666666666665,"Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi
Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of
text-to-image models. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.38,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning, pages
8748–8763. PMLR, 2021."
REFERENCES,0.38333333333333336,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition, pages 10684–10695, 2022."
REFERENCES,0.38666666666666666,"Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. Safe latent diffusion:
Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 22522–22531, 2023."
REFERENCES,0.39,"Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi
Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An
open large-scale dataset for training next generation image-text models. Advances in Neural
Information Processing Systems, 35:25278–25294, 2022."
REFERENCES,0.3933333333333333,"Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.39666666666666667,"Michael Spivak. Calculus. Cambridge University Press, 2006."
REFERENCES,0.4,"Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning
of features and labels. arXiv preprint arXiv:2108.11577, 2021."
REFERENCES,0.4033333333333333,"Yanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, Tingbo Hou, et al.
Semi-implicit denoising diffusion models (siddms). arXiv preprint arXiv:2306.12511, 2023."
REFERENCES,0.4066666666666667,"Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:
5824–5836, 2020."
REFERENCES,0.41,"Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learning
to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591, 2023a."
REFERENCES,0.41333333333333333,"Yang Zhang, Teoh Tze Tzun, Lim Wei Hern, Haonan Wang, and Kenji Kawaguchi. On copyright
risks of text-to-image diffusion models. arXiv preprint arXiv:2311.12803, 2023b."
REFERENCES,0.4166666666666667,Appendices
REFERENCES,0.42,"A Proof of Theorem 4
14"
REFERENCES,0.42333333333333334,"A.1
Lemma and Proofs .............................................................................................. 14"
REFERENCES,0.4266666666666667,"B Preliminaries
16"
REFERENCES,0.43,"B.1
Denoising Diffusion Probabilistic Models ........................................................... 16"
REFERENCES,0.43333333333333335,"B.2
Latent Diffusion Models ..................................................................................... 16"
REFERENCES,0.43666666666666665,"C Implementation Details
16"
REFERENCES,0.44,"C.1
Class Conditional Diffusion Models .................................................................... 16"
REFERENCES,0.44333333333333336,"C.2
Stable Diffusion Models ...................................................................................... 16"
REFERENCES,0.44666666666666666,"D Dataset Diversification Details
17"
REFERENCES,0.45,"D.1
Nudity Removal ....................................................................................................17"
REFERENCES,0.4533333333333333,"D.2
Artist Removal .................................................................................................... 18"
REFERENCES,0.45666666666666667,"E Additional Results
18"
REFERENCES,0.46,"E.1
Generalization to Different Pretrained Models .................................................... 18"
REFERENCES,0.4633333333333333,"E.2
Impact of Different Sizes in Df and Dr .............................................................. 19"
REFERENCES,0.4666666666666667,"E.3
Class-wise Feature Similarity ............................................................................... 19"
REFERENCES,0.47,"E.4
Qualitative Results ................................................................................................ 20"
REFERENCES,0.47333333333333333,"A
Proof of Theorem 4"
REFERENCES,0.4766666666666667,"To prove this theorem, we establish the following lemma. We notate the ℓ2 norm as ∥· ∥throughout.
Lemma 5 (Projected gradients obtain optimal solution to a constrained objective). Let Lf(θ), and
Lr(θ) be K-Lipschitz smooth negative forget and retain losses under the ℓ2 norm respectively. Then,
the update δ∗
f = ∇Lf −∇Lf ·∇Lr"
REFERENCES,0.48,∥∇Lr∥2 ∇Lr is the minimizer of
REFERENCES,0.48333333333333334,"arg min
||δf ||=η
Lf(θ + δf) s.t. Lr(θ) ≥Lr(θ + δf)
(5)"
REFERENCES,0.4866666666666667,"in terms of δf. Similarly, δ∗
r = ∇Lr −∇Lf ·∇Lr"
REFERENCES,0.49,∥∇Lf ∥2 ∇Lf is the minimizer of
REFERENCES,0.49333333333333335,"arg min
||δr||=η
Lr(θ + δr) s.t. Lf(θ) ≥Lf(θ + δr),
(6)"
REFERENCES,0.49666666666666665,"in terms of δf, for a value η ≪
1
K when we have conflicting unconstrained gradient terms, i.e.
∇Lf · ∇Lr < 0."
REFERENCES,0.5,"Proof of Lemma 5. For δr, δf, both of norm η, we have good approximation by the Taylor expansion
due to the Lipschitz condition on Lf, Lr. Therefore, we have,"
REFERENCES,0.5033333333333333,"Lr(θ + δr) −Lr(θ) ≈δr · ∇Lr
Lf(θ + δf) −Lf(θ) ≈δf · ∇Lf
Lf(θ + δr) −Lf(θ) ≈δr · ∇Lf
Lr(θ + δf) −Lr(θ) ≈δf · ∇Lr
We can re-express the two objectives as,"
REFERENCES,0.5066666666666667,"arg min
||δf ||=η
δf · ∇Lf
s.t.
δf · ∇Lr ≤0
(7)"
REFERENCES,0.51,"arg min
||δr||=η
δr · ∇Lr
s.t.
δr · ∇Lf ≤0.
(8)"
REFERENCES,0.5133333333333333,"By the method of Langrangian multipliers, for each objective we create slack variables λf, λr, and
obtain the unconstrained objectives,"
REFERENCES,0.5166666666666667,"arg min
||δf ||=η
δf · ∇Lf + λfδf · ∇Lr = arg min
||δf ||=η
δf · (∇Lf + λf∇Lr)"
REFERENCES,0.52,"arg min
||δr||=η
δr · ∇Lr + λrδr · ∇Lf = arg min
||δr||=η
δr · (∇Lr + λr∇Lf)"
REFERENCES,0.5233333333333333,"We first observe since both are now linear objective, that the minima is trivially observed when
δ∗
f ∝−(∇Lf + λf∇Lr), and δ∗
r ∝−(∇Lr + λr∇Lf). For the rest of this proof, without loss of
generality, suppose η is scaled such that we hold the previous proportionality statements as equalities."
REFERENCES,0.5266666666666666,"We invoke KKT sufficiency conditions to both confirm if these minima exist, and obtain solutions to
the slack variables. In the case of conflicting gradients, since ∇Lf · ∇Lr < 0, the minimizers of the
unconstrained objectives in Equations 7, 8 are not satisfied within the constraints. Therefore, λf, and
λr do not vanish, and are maximizers of their respective objectives. Taking the gradients in respect to
the slack variables and setting to 0, we have"
REFERENCES,0.53,"∇λf
 
δ∗
f · (∇Lf + λf∇Lr)

= −∇λf
 
δ∗
f · δ∗
f

= −2∇Lr · δ∗
f = 0"
REFERENCES,0.5333333333333333,"∇λr (δ∗
r · (∇Lr + λr∇Lf)) = −∇λr (δ∗
r · δ∗
r) = −2∇Lf · δ∗
r = 0."
REFERENCES,0.5366666666666666,"We can solve this in a way that satisfies the objective by requiring δ∗
r to be orthogonal to ∇Lf, and δ∗
f
to be orthogonal to ∇Lr. In this case, we have λf = −∇Lf ·∇Lr"
REFERENCES,0.54,∥∇Lr∥2 and λr = −∇Lf ·∇Lr
REFERENCES,0.5433333333333333,"∥∇Lf ∥2 as the optima.
We verify that these are maximizers by computing the second derivatives, which are constants at
−2∥∇Lr∥2 and −2∥∇Lf∥2 respectively. Both are strictly negative, confirming the second order
sufficient condition for a maximizer."
REFERENCES,0.5466666666666666,"Therefore it is precisely the restricted gradient steps, δ∗
f = ∇Lf −∇Lf ·∇Lr"
REFERENCES,0.55,"∥∇Lr∥2 ∇Lr and δ∗
r = ∇Lr −"
REFERENCES,0.5533333333333333,∇Lf ·∇Lr
REFERENCES,0.5566666666666666,"∥∇Lf ∥2 ∇Lf, which solve the optimization problems in Equations 5, 6 respectively."
REFERENCES,0.56,"Proof of Theorem 4. We take the Taylor expansions in respect to v of Lf and Lr around θ. We have
mutatis mutandis for some h ∈R,"
REFERENCES,0.5633333333333334,Lf(θ + hv) = Lf(θ) + h∇Lf(θ) · v + O(h2∥v∥2)
REFERENCES,0.5666666666666667,"It follows that, for v, w, such that w · ∇Lf(θ) = 0,"
REFERENCES,0.57,"Dv+wLf(θ) = lim
h→0
Lf(θ + hv + hw) h"
REFERENCES,0.5733333333333334,"= lim
h→0
Lf(θ) + h∇Lf(θ) · (v + w) h"
REFERENCES,0.5766666666666667,"= lim
h→0
Lf(θ) + h∇Lf(θ) · v h"
REFERENCES,0.58,"= lim
h→0
Lf(θ + hv)"
REFERENCES,0.5833333333333334,"h
= DvLf(θ)."
REFERENCES,0.5866666666666667,"We observe that we can bound the optimization,"
REFERENCES,0.59,"min
v
∗Dv(Lf + Lr)(θ) ≥min
v
DvLf(θ)
s.t.
Lr(θ) ≥Lr(θ + v)"
REFERENCES,0.5933333333333334,"+ min
w DwLr(θ)
s.t.
Lf(θ) ≥Lf(θ + w)"
REFERENCES,0.5966666666666667,"= lim
h→0min
v
∗1"
REFERENCES,0.6,"hLf(θ + hv) + lim
h→0min
w
∗1"
REFERENCES,0.6033333333333334,hLr(θ + hw).
REFERENCES,0.6066666666666667,"We use min∗to signify the presence of constraints as previously defined for the respective expression
to simplify notation."
REFERENCES,0.61,"We invoke Lemma 5 to solve each minimization problem above, yielding, v∗∝δ∗
f = ∇Lf −
∇Lf ·∇Lr"
REFERENCES,0.6133333333333333,"∥∇Lr∥2 ∇Lr, and w∗∝δ∗
r = ∇Lr −∇Lf ·∇Lr"
REFERENCES,0.6166666666666667,"∥∇Lf ∥2 ∇Lf. Note that since we are taking the limits as
h →0, the Taylor expansion in Lemma 5 is exact as the relevant constant in the lemma, ∥η∥→0."
REFERENCES,0.62,"We also have that Dv∗Lf(θ) = Dv∗+w∗Lf(θ) since w∗· ∇Lf(θ) = 0 (and similarly we have
Dw∗Lr(θ) = Dv∗+w∗Lr(θ))."
REFERENCES,0.6233333333333333,"Now, altogether we can show,"
REFERENCES,0.6266666666666667,"min
v
∗Dv(Lf + Lr)(θ) ≥min
v
∗DvLf(θ) + min
w
∗DwLr(θ)"
REFERENCES,0.63,"= Dv∗Lf(θ) + Dw∗Lr(θ)
= Dv∗+w∗Lf(θ) + Dv∗+w∗Lr(θ)
= Dv∗+w∗(Lf(θ) + Lr(θ))"
REFERENCES,0.6333333333333333,"If v∗+ w∗satisfies the constraints of the original optimization, and bounds the minimizer from
below, this is the optimal solution."
REFERENCES,0.6366666666666667,"Therefore, we require for both losses,"
REFERENCES,0.64,"Lf(θ + v∗+ w∗) ≤Lf(θ)
Lr(θ + v∗+ w∗) ≤Lr(θ)"
REFERENCES,0.6433333333333333,"By the constraints of the optimization problem, we know that Lf(θ+v∗) ≤L(θ), and Lr(θ+w∗) ≤
L(θ). Again, using the Taylor expansion, mutatis mutandis we have,"
REFERENCES,0.6466666666666666,"Lf(θ + v∗+ w∗) = Lf(θ + v∗) + ∇Lf(θ + v∗) · w∗+ O(∥w∗∥2)
≃Lf(θ + v∗) ≤Lf(θ)."
REFERENCES,0.65,"Therefore, η(δ∗
f + δ∗
r), solves the optimization for a small enough constant η ∈R+, so δ∗
f + δ∗
r solves
the optimization up to a constant. This completes the proof."
REFERENCES,0.6533333333333333,"B
Preliminaries"
REFERENCES,0.6566666666666666,"Denoising Diffusion Probabilistic Models
Diffusion models consist of a forward diffusion process
and a reverse diffusion process. The forward diffusion process progressively deteriorates an initial
data point x0 ∼q{x0} by adding Gaussian noise with a variance schedule βt ∈(0, 1) to generate a
set of noisy latents {x1, x2, ..., xT } with a Markov transition probability:"
REFERENCES,0.66,"q(x1:T |x0) = T
Y"
REFERENCES,0.6633333333333333,"t=1
q(xt|xt−1),
q(xt|xt−1) = N(xt;
p"
REFERENCES,0.6666666666666666,"1 −βtxt−1, βtI)
(9)"
REFERENCES,0.67,"q(xt|x0) = N
 
xt; √¯αtx0, (1 −¯αt)I

,
¯αt = tY"
REFERENCES,0.6733333333333333,"n=1
(1 −βj),
(10)"
REFERENCES,0.6766666666666666,"where T indicates the maximum time steps.
In the reverse process, we aim to predict
the latent representation of the previous time step, which can be written as pθ(xt−1|xt) =
N (xt−1; µθ(xt, t), Σθ(t)). The training objective to predict the previous step can then be defined as: L = − T
X"
REFERENCES,0.68,"t=2
Eq(xt|x0) [DKL(q(xt−1|xt, x0)||pθ(xt−1|xt))]
(11)"
REFERENCES,0.6833333333333333,"where q(xt−1|xt, x0) = N (xt−1; µq(xt, x0), Σq(t)). Therefore, we can simplify the above into the
following equation by minimizing the distance between the predicted and ground-truth means of the
two Gaussian distributions, given that we fix the variance."
REFERENCES,0.6866666666666666,"L = Et,x0,ϵ

∥ϵ −eθ(xt, t)∥2
(12)"
REFERENCES,0.69,"where eθ(xt, t) is the model’s estimate of the noise ϵ added into the clean image x0 at time t [Xu
et al., 2023, Ho et al., 2020]."
REFERENCES,0.6933333333333334,"Latent Diffusion Models
Latent Diffusion Models (LDMs) [Rombach et al., 2022] are probabilistic
frameworks used to model the distribution pdata by learning on a latent space. Based on the pre-trained
variational autoencoder, LDMs first encode high-dimensional data x0 into a more tractable, low-
dimensional latent representation z0 = E(x0), where E represents an encoder. Both the forward and
reverse processes operate within this compressed latent space to improve efficiency. The objective
can be described as L = Et,z0,c,ϵ

∥ϵ −ϵθ(zt, t, c)∥2
, where the noise prediction ϵθ(zt, t, c) is
conditioned on the timestep t and a text c. Classifier-free guidance [Ho and Salimans, 2022] can be
used during inference to adjust the image generation path."
REFERENCES,0.6966666666666667,"C
Implementation Details"
REFERENCES,0.7,"We describe the experimental configurations and hyperparameter settings employed in our study. All
experiments were conducted using an NVIDIA H100 GPU."
REFERENCES,0.7033333333333334,"Class Conditional Diffusion Models
For experiments on CIFAR-10, we implemented our method
using hyperparameters α = 1 × 10−1 and λ = 5. Our EDM implementation used a batch size of 64,
a duration parameter of 0.05, and a learning rate of 1e-5. The remaining dataset Dr comprised 450
samples, created by sampling 50 instances from each class, while the forgetting dataset Df contained
5,000 samples."
REFERENCES,0.7066666666666667,"Stable Diffusion Models
For nudity removal experiments with Stable Diffusion, we set α = 1.6
and λ = 1.5. Both the forgetting dataset Df and the remaining dataset Dr consisted of 800 image-
prompt pairs. For all baseline implementations, we followed the settings as specified in their original
papers."
REFERENCES,0.71,"D
Dataset Diversification Details"
REFERENCES,0.7133333333333334,"In this section, we present a set of example prompts designed for our Df and Dr used for stable
diffusion model experiments. To generate these prompts, we leverage the ChatGPT. Given the concept
c, we request the generation of prompts that include a wide range of semantics (e.g., environment,
time, mood, actions) to describe the concept c for Df. As explained before, we remove the word
related to c to generate Dr. Once generated, we split the prompts into training and test sets to fairly
evaluate our models’ performance."
REFERENCES,0.7166666666666667,"D.1
Nudity Removal"
REFERENCES,0.72,"We use a structured approach to generate diverse prompts for Dr, considering multiple dimensions
such as activities, environments, times, and moods provided by a Large Language Model (LLM).
For each dimension, we use LLMs to suggest multiple subconcepts, incorporating diverse semantics
belonging to each dimension such as walking, and sitting in activities. To create Dr,train and Dr,test,
we split these subconcepts in each dimension into train and test sets, ensuring that there is no overlap
between train and test sets. We then combine these subconcepts to generate Dr. Table 5 shows a set
of example prompts we used for nudity removal."
REFERENCES,0.7233333333333334,"Table 5: Example prompts for nudity concept removal. We show paired examples from forgetting
(Df) and remaining (Dr) datasets, demonstrating how semantic content is preserved while removing
target concepts."
REFERENCES,0.7266666666666667,"ID
Forgetting Dataset (Df)
Remaining Dataset (Dr)"
REFERENCES,0.73,"P1
A melancholic unclad person painting in a
bright, airy studio early evening"
REFERENCES,0.7333333333333333,"A melancholic person painting in a bright,
airy studio early evening
P2
A hopeful undressed person sketching in a
bright, airy studio during twilight"
REFERENCES,0.7366666666666667,"A hopeful person sketching in a bright, airy
studio during twilight
P3
A melancholic unclad person playing guitar
in a bright, airy studio during twilight"
REFERENCES,0.74,"A melancholic person playing guitar in a
bright, airy studio during twilight
P4
A disillusioned nude person painting in an
urban park late night"
REFERENCES,0.7433333333333333,"A disillusioned person painting in an urban
park late night
P5
A excited unclad person shopping in a bright,
airy studio early evening"
REFERENCES,0.7466666666666667,"An excited person shopping in a bright, airy
studio early evening
P6
A excited undressed person painting in a
desert early evening"
REFERENCES,0.75,"An excited person painting in a desert early
evening
P7
A excited unclad person shopping in a desert
at noon"
REFERENCES,0.7533333333333333,"An excited person shopping in a desert at
noon
P8
A excited naked person sketching in an urban
park late night"
REFERENCES,0.7566666666666667,"An excited person sketching in an urban park
late night
P9
A excited undressed person playing guitar in
an urban park late night"
REFERENCES,0.76,"An excited person playing guitar in an urban
park late night
P10
A hopeful nude person shopping in an urban
park late night"
REFERENCES,0.7633333333333333,"A hopeful person shopping in an urban park
late night
P11
An excited naked person sketching in an ur-
ban park early evening"
REFERENCES,0.7666666666666667,"An excited person sketching in an urban park
early evening
P12
A hopeful undressed person shopping in a
bright, airy studio late night"
REFERENCES,0.77,"A hopeful person shopping in a bright, airy
studio late night
P13
A hopeful naked person shopping in a bright,
airy studio during twilight"
REFERENCES,0.7733333333333333,"A hopeful person shopping in a bright, airy
studio during twilight
P14
A melancholic undressed person shopping in
a bright, airy studio early evening"
REFERENCES,0.7766666666666666,"A melancholic person shopping in a bright,
airy studio early evening
P15
A disillusioned unclad person shopping in a
bright, airy studio at noon"
REFERENCES,0.78,"A disillusioned person shopping in a bright,
airy studio at noon"
REFERENCES,0.7833333333333333,"D.2
Artist Removal"
REFERENCES,0.7866666666666666,"Similar to nudity removal, we construct some templates with multiple dimensions such as the artist’s
name, actions, environments, and moods, then fill in each dimension with the suggestions from LLMs.
Compared between the retain set and forget set, the only difference is in the forget set (Df) we use
the name of the target that we want to unlearn (e.g., Van Gogh), and use other artists’ names or
some virtual names in the retain set (Dr). Table 6 shows a set of example prompts we used for style
removal."
REFERENCES,0.79,"Table 6: Example prompts for art style removal. Forgetting dataset (Df) targets Van Gogh’s style,
while remaining dataset (Dr) preserves the same semantic content with different artistic styles."
REFERENCES,0.7933333333333333,"ID
Forgetting Dataset (Df)
Remaining Dataset (Dr)"
REFERENCES,0.7966666666666666,"P1
A whining dog in a cafe illustrated by Vincent
van Gogh"
REFERENCES,0.8,"In Cawjnj’s characteristic style, A scared dog
over a fence
P2
The style of Vincent van Gogh in a painting
of A running dog in the neighborhood"
REFERENCES,0.8033333333333333,"Kichikscch Ycgn’s portrayal of A happy dog
by the fireplace
P3
In Vincent van Gogh’s style, A happy dog
behind the bushes"
REFERENCES,0.8066666666666666,"The style of Maximilian Vermeer in a painting
of A scared dog under a tree
P4
A trained dog in the neighborhood as seen
through Vincent van Gogh’s eyes"
REFERENCES,0.81,"Maximilian Vermeer creates A scared dog in
the neighborhood in his signature style
P5
A hungry dog on the couch, as conceived by
Vincent van Gogh"
REFERENCES,0.8133333333333334,"Marius Vendrell’s art showing A curious dog
at the gate
P6
An excited dog at the gate as seen through
Vincent van Gogh’s eyes"
REFERENCES,0.8166666666666667,"A running dog under a tree, as conceived by
Wassily Kandinsky
P7
In Vincent van Gogh’s style, A sneaky dog
along the trail"
REFERENCES,0.82,"In Lorenzo di Valli’s style, A swimming dog
across the field
P8
In Vincent van Gogh’s characteristic style, A
sniffing dog over a fence"
REFERENCES,0.8233333333333334,"A lazy dog during a storm interpreted through
René Magritte’s artistry
P9
Inspired by Vincent van Gogh, a painting of
A scared dog along the trail"
REFERENCES,0.8266666666666667,"A curious dog on the couch, as conceived by
Gustav Klimt
P10
A playful dog at the gate as imagined by Vin-
cent van Gogh"
REFERENCES,0.83,"The style of Fvlgvzswlp Lowlqufgjtl in a
painting of A barking dog in the yard
P11
Vincent van Gogh creates A running dog dur-
ing a storm in his signature style"
REFERENCES,0.8333333333333334,"In Enzo Fiorentino’s characteristic style, A
happy dog under a tree
P12
A wet dog at the gate, as conceived by Vincent
van Gogh"
REFERENCES,0.8366666666666667,"Fvlgvzswlp Lowlqufgjtl creates A swimming
dog on the beach in his signature style
P13
A trained dog on the couch interpreted
through Vincent van Gogh’s artistry"
REFERENCES,0.84,"A protective dog behind the bushes brought
to life by Rafael Casanova’s brushstrokes
P14
Inspired by Vincent van Gogh, a painting of
A protective dog across the field"
REFERENCES,0.8433333333333334,"A sneaky dog after a bath as seen through
Edward Hopper’s eyes
P15
A wet dog at a park seen through Vincent van
Gogh’s artistic perspective"
REFERENCES,0.8466666666666667,"A wet dog at the gate brought to life by
Georges Seurat’s brushstrokes"
REFERENCES,0.85,"E
Additional Results"
REFERENCES,0.8533333333333334,"E.1
Generalization to Different Pretrained Models"
REFERENCES,0.8566666666666667,"We further conducted additional evaluations using SD v3, the most recent version of the pre-trained
model. SD v3 employs a transformer-based architecture (e.g., Diffusion Transformer models) instead
of the UNet-based architecture used in previous versions. This significant change allows us to test
our method’s performance across different model structures. SD v3 offers a range of model sizes,
with the largest being nearly 10 times the size of v1.4. We choose a medium size model with 2B
parameters, which is approximately 2 times larger than v1.4. This variability enables us to assess
how our method performs across different model capacities. We evaluated two baselines alongside
our method, observing their performance under multiple hyperparameter tunings. We observed high
alignment scores for both Dr,train and Dr,test splits with SD v3, while effectively mitigating harmful
output generation. On the other hand, both baselines showed alignment score drops."
REFERENCES,0.86,"Table 7: Comparison of nudity removal effectiveness and alignment scores across different methods
on Stable Diffusion Model"
REFERENCES,0.8633333333333333,"Methods
Nudity Removal
AS (↑)"
REFERENCES,0.8666666666666667,"Female
Genitalia
Buttocks
Male
Breast
Belly
Male
Genitalia
Armpits
Female
Breast
Dr,train
Dr,test"
REFERENCES,0.87,"SD v3
0
1
9
69
4
58
46
0.364
0.371"
REFERENCES,0.8733333333333333,"ESD
0
0
2
10
0
4
6
0.335
0.332"
REFERENCES,0.8766666666666667,"Salun
0
0
0
0
0
0
0
0.079
0.088"
REFERENCES,0.88,"RGD (Ours)
0
0
0
0
0
0
0
0.362
0.370"
REFERENCES,0.8833333333333333,"E.2
Impact of Different Sizes in Df and Dr"
REFERENCES,0.8866666666666667,"We investigated how varying the sizes of Df and Dr affects the unlearning performance. Our analysis
reveals several key findings. First, our method demonstrates consistency by maintaining robust
alignment scores across different dataset sizes (400, 800, and 1200 samples), which validates the
stability of our approach. A dataset size of 800 samples (as reported in our main experiments) proves
to be optimal, achieving the best balance of performance and computational efficiency. Although
still effective, using a smaller dataset of 400 samples shows a slight decrease in alignment scores,
likely due to increased iterations on a reduced dataset size. When using a larger dataset of 1200
samples, we can achieve alignment scores comparable to the 800-sample configuration by adjusting
λ from 1.5 to 1.15, which helps balance the increased gradient ascent steps. Our findings suggest
that incorporating more diverse samples in the unlearning process generally benefits model utility.
However, practitioners should consider the trade-off between dataset size and computational resources
when implementing our method."
REFERENCES,0.89,Table 8: Alignment scores comparison with varying dataset sizes
REFERENCES,0.8933333333333333,"Methods
SD
RGD (Ours)"
REFERENCES,0.8966666666666666,"|Dr| = |Df| = 400
|Dr| = |Df| = 800
|Dr| = |Df| = 1200"
REFERENCES,0.9,"Dr,train
0.357
0.336 (0.021)
0.354 (0.003)
0.352 (0.005)"
REFERENCES,0.9033333333333333,"Dr,test
0.352
0.339 (0.013)
0.350 (0.002)
0.346 (0.006)"
REFERENCES,0.9066666666666666,"E.3
Class-wise Feature Similarity"
REFERENCES,0.91,"To systematically analyze the semantic relationships between CIFAR-10 classes, we conducted a
comprehensive similarity analysis using CLIP feature embeddings. For each class, we extracted
features from 500 training examples using a pre-trained CLIP model [Radford et al., 2021]. We then
computed class-wise mean feature vectors and calculated pairwise cosine similarities between these
representations."
REFERENCES,0.9133333333333333,"Figure 7 presents the complete analysis of class-wise similarities, showing the two most similar
classes for each target class along with their corresponding similarity scores. This analysis informed
our experimental design for the ablation studies on data diversity, particularly in constructing the
remaining dataset (Dr) for Case 1, where only the two most similar classes were included. In our
ablation study 4.4, we specifically focused on three target classes (plane, bird, and dog) and their
respective most similar classes when constructing the limited diversity scenario (Case 1)."
REFERENCES,0.9166666666666666,"E.4
Qualitative Results"
REFERENCES,0.92,"We provide qualitative results comparing generations from the unlearned models (Salun, ESD-u,
RGD and the pretrained model SD using the retain prompts Dr."
REFERENCES,0.9233333333333333,"Figure 7: CIFAR10 class-wise feature similarity based on CLIP [Radford et al., 2021]"
REFERENCES,0.9266666666666666,Figure 8: SD given the prompts from Dr
REFERENCES,0.93,Figure 9: Salun given the prompts from Dr
REFERENCES,0.9333333333333333,Figure 10: ESD-u given the prompts from Dr
REFERENCES,0.9366666666666666,Figure 11: RGD (Ours) given the prompts from Dr
REFERENCES,0.94,NeurIPS Paper Checklist
CLAIMS,0.9433333333333334,1. Claims
CLAIMS,0.9466666666666667,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In the abstract and introduction, we provide the motivation, the current
limitations in the existing work, and the contribution and brief evaluation results of our
paper.
2. Limitations"
CLAIMS,0.95,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The authors discuss the sensitivity of hyperparameters and the importance of
diversity level.
3. Theory Assumptions and Proofs"
CLAIMS,0.9533333333333334,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: In the main paper, we provide the core part of our proposed theory and
assumptions, and also provide complete proof in the appendix.
4. Experimental Result Reproducibility"
CLAIMS,0.9566666666666667,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide all hyperparameters, datasets, architecture we used in both the
main paper and appendix and the way of designing the forgetting and remaining datasets.
We also provide the examples of each dataset in the appendix.
5. Open access to data and code"
CLAIMS,0.96,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The paper provides open access to the data, and the authors will also release
the code publicly.
6. Experimental Setting/Details"
CLAIMS,0.9633333333333334,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide all implementation details in the main paper and appendix.
7. Experiment Statistical Significance"
CLAIMS,0.9666666666666667,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We provide the average and standard deviation for CIFAR-10 experiments.
8. Experiments Compute Resources"
CLAIMS,0.97,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
CLAIMS,0.9733333333333334,"Answer: [NA]
Justification: We provide the information on the computation resources in Appendix.
9. Code Of Ethics"
CLAIMS,0.9766666666666667,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We conform with the NeurIPS Code of Ethics in every respect.
10. Broader Impacts"
CLAIMS,0.98,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]"
CLAIMS,0.9833333333333333,"Justification: We provide a broader impact in our paper, and we don’t have negative societal
impacts.
11. Safeguards"
CLAIMS,0.9866666666666667,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [Yes]
Justification: We properly mask the harmful part in the figure for publication.
Guidelines:
12. Licenses for existing assets"
CLAIMS,0.99,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We properly credited the original owners of assets used in the paper.
13. New Assets"
CLAIMS,0.9933333333333333,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We do not introduce the new assets.
14. Crowdsourcing and Research with Human Subjects"
CLAIMS,0.9966666666666667,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: We do not involve human subjects in our study.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: We do not conduct experiments on individuals."
