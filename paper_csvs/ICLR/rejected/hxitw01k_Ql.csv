Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029498525073746312,"Reinforcement learning is generally difﬁcult for partially observable Markov de-
cision processes (POMDPs), which occurs when the agent’s observation is partial
or noisy. To seek good performance in POMDPs, one strategy is to endow the
agent with a ﬁnite memory, whose update is governed by the policy. However,
policy optimization is non-convex in that case and can lead to poor training per-
formance for random initialization. The performance can be empirically improved
by constraining the memory architecture, then sacriﬁcing optimality to facilitate
training. Here we study this trade-off in a two-hypothesis testing problem, akin
to the two-arm bandit problem. We compare two extreme cases: (i) the random
access memory where any transitions between M memory states are allowed and
(ii) a ﬁxed memory where the agent can access its last m actions and rewards. For
(i), the probability q to play the worst arm is known to be exponentially small in
M for the optimal policy. Our main result is to show that similar performance
can be reached for (ii) as well, despite the simplicity of the memory architecture:
using a conjecture on Gray-ordered binary necklaces, we ﬁnd policies for which
q is exponentially small in 2m, i.e. q ∼α2m with α < 1. In addition, we observe
empirically that training from random initialization leads to very poor results for
(i), and signiﬁcantly better results for (ii) thanks to the constraints on the memory
architecture."
INTRODUCTION,0.0058997050147492625,"1
INTRODUCTION"
INTRODUCTION,0.008849557522123894,"Reinforcement learning is aimed at ﬁnding the sequence of actions that should take an agent to
maximise a long-term reward (Sutton & Barto (2018)). This sequential decision-making is usually
modeled as a Markov decision process (MDP): at each time step, the agent chooses an action based
on a policy (a function that relates the agent’s state to its action), with the aim of maximizing its value
(the expected discounted sum of rewards). Deterministic optimal policies can be found through
dynamic programming (Bellman (1966)) when MDPs are discrete (both states and actions belong to
discrete sets) and the agent fully knows its environment (Watkins & Dayan (1992))."
INTRODUCTION,0.011799410029498525,"A practical difﬁculty arises when the agent only have a partial observation of its environment or
when this observation is imperfect or stochastic. The mathematical framework is then known as
a partially observable Markov decision process (POMDP) (Smallwood & Sondik (1973)). In this
framework, the agent’s state is replaced by the agent’s belief, which is the probability distribution
over all possible states. At each time step, the agent’s belief can be updated through Bayesian
inference to account for observations. In the belief space, the problem becomes fully observable
again and the POMDP can thus be solved as a “belief MDP”. However, the dimension of the belief
space is much larger than the state space and solving the belief MDP can be challenging in practical
problems. Some approaches seek to resolve this difﬁculty by approximating of the belief and the
value functions (Hauskrecht (2000); Roy et al. (2005); Silver & Veness (2010); Somani et al. (2013)),
or use deep model-free reinforcement learning where the neural network is complemented with a
memory (Oh et al. (2016); Khan et al. (2017)) or a recurrency (Hausknecht & Stone (2015); Li et al.
(2015)) to better approximate history-based policies."
INTRODUCTION,0.014749262536873156,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.017699115044247787,"Here we focus on the idea of Littman (1993), who proposed to give the agent a limited number of
bits of memory, an idea that has been developed independently in the robotics community where it
is known as a ﬁnite-state controller (Meuleau et al. (1999; 2013)). These works show that adding
a memory usually increases the performance in POMDPs. But to this day, attempts to ﬁnd opti-
mal memory allocation have been essentially empirical (Peshkin et al. (2001); Zhang et al. (2016);
Toro Icarte et al. (2020)). One central difﬁculty is that the value is a non-convex function of policy
for POMDPs (Jaakkola et al. (1995)): learning will thus generally get stuck in poor local maxima
for random policy initialization. This problem is even more acute when memory is large or when
all transitions between memory states are allowed. To improve learning, restricting the policy space
to speciﬁc memory architectures where most transitions are forbidden is key (Peshkin et al. (2001);
Zhang et al. (2016); Toro Icarte et al. (2020)). However, there is no general principles to optimize the
memory architectures or the policy initialization. In fact, this question is not understood satisfyingly
even in the simplest tasks- arguably a necessary step to later achieve a broad understanding."
INTRODUCTION,0.02064896755162242,"Here, we work out how the memory architecture affects optimal solutions in perhaps the simplest
POMDP, and ﬁnd that these solutions are intriguingly complex. Speciﬁcally, we consider the two-
hypothesis testing problem. At each time step, the agent chooses to pull one of two arms that yield
random rewards with different means. We compare two memory structures: (i) a random access
memory (RAM) in which all possible transitions between M distinct memory states are allowed;
(ii) a Memento memory in which the agent can access its last m actions and rewards."
INTRODUCTION,0.02359882005899705,"When the agent is provided with a RAM memory, we study the performance of a “column of conﬁ-
dence” policy (CCP): the agent keeps repeating the same action and updates its conﬁdence in it by
moving up and down the memory sites until it reaches the bottom of the column and the alternative
action is tried. The performance of this policy is assessed through the calculation of the expected
frequency q to play the worst arm (thus the smaller q, the better). For the CCP, q can be shown to
be exponentially small in M. This result is closely related to the work of Hellman & Cover (1970)
on hypothesis testing and its extension to ﬁnite horizon (Wilson (2014)). In practice, we ﬁnd that
learning a policy with a RAM memory and random initialization leads to poor results, far from
the performance of the column of conﬁdence policy. Restricting memory transitions to chain-like
transitions leads to much better results, although still sub-optimal."
INTRODUCTION,0.02654867256637168,"Our main ﬁndings concerns the Memento memory architecture. Surprisingly, despite the lack of
ﬂexibility of the memory structure, excellent policies exist. Speciﬁcally, using a conjecture on Gray-
ordered binary necklaces (Degni & Drisko (2007)), we ﬁnd a policy for which q is exponentially
small in 2m —which is considerably better than q ∼ln(m)/m, optimal for an agent that only plays
m times. For Memento memory, we also observe empirically that learning is faster and perform
better than in the RAM case."
INTRODUCTION,0.029498525073746312,"The code to reproduce the experiments is available at https://anonymous.4open.
science/r/two-hypothesis-BAB3,
and uses a function deﬁned here https://
anonymous.4open.science/r/gradientflow/gradientflow.
The experiments
where executed on CPUs for about 10 thousand CPU hours."
POMDPS AND THE TWO-HYPOTHESIS TESTING PROBLEM,0.032448377581120944,"2
POMDPS AND THE TWO-HYPOTHESIS TESTING PROBLEM"
GENERAL FORMULATION,0.035398230088495575,"2.1
GENERAL FORMULATION"
GENERAL FORMULATION,0.038348082595870206,"Deﬁnition
2.1
(POMDP).
A
discrete-time
POMDP
model
is
deﬁned
as
the
8-tuple
(S, A, T, R, Ω, O, p0, γ): S is a set of states, A is a set of actions, T is a conditional transition
probability function T(s′|s, a) where s′, s ∈S and a ∈A, R : S →R is the reward function1,
Ωis a set of observations, O(o|s) is a conditional observation probability with o ∈Ωand s ∈S,
p0(s) : S →R is the probability to start in a given state s, and γ ∈[0, 1) is the discount factor."
GENERAL FORMULATION,0.04129793510324484,"A state s ∈S speciﬁes everything about the world at a given time (the agent, its memory and all the
rest). The agent starts its journey in a state s ∈S with probability p0(s). Based on an observation
o ∈Ωobtained with probability O(o|s) the agent takes an action a ∈A. This action causes a
transition to the state s′ with probability T(s′|s, a) and the agent gains the reward R(s). And so on."
GENERAL FORMULATION,0.04424778761061947,"1In the literature, R also depends on the action: R : S × A →R. Our notation is not a loss of generality.
The set of state can be made bigger S →S × A in order to contain the last action."
GENERAL FORMULATION,0.0471976401179941,Under review as a conference paper at ICLR 2022
GENERAL FORMULATION,0.05014749262536873,"Deﬁnition 2.2 (Policy). A policy π(a|o) is a conditional probability of executing an action a ∈A
given an observation o ∈Ω.
Deﬁnition 2.3 (Policy State Transition). Given a policy π, the state transition Tπ is given by"
GENERAL FORMULATION,0.05309734513274336,"Tπ(s′|s) =
X"
GENERAL FORMULATION,0.05604719764011799,"o,a∈Ω×A
T(s′|s, a)π(a|o)O(o|s).
(1)"
GENERAL FORMULATION,0.058997050147492625,"Deﬁnition 2.4 (Expected sum of discounted rewards). The expected sum of future discounted re-
wards of a policy π is"
GENERAL FORMULATION,0.061946902654867256,"Gπ = E s0 ∼p0
s1 ∼Tπ(·|s0)
s2 ∼Tπ(·|s1)
. . . "" ∞
X"
GENERAL FORMULATION,0.06489675516224189,"t=0
γtR(st) # .
(2)"
GENERAL FORMULATION,0.06784660766961652,"Note that a POMDP with an expected sum of future discounted rewards with discount factor γ can be
reduced to an undiscounted POMDP (Altman (1999)), as we now recall (see proof in Appendix A):
Lemma 2.1. The discounted POMDP deﬁned in 2.1 with a discount γ is equivalent to an undis-
counted POMDP with a probability r = 1 −γ to be reset from any state toward an initial state. In
the undiscounted POMDP, the agent reaches a steady state p(s) which can be used to calculate the
expected sum of discounted rewards Gπ = 1"
GENERAL FORMULATION,0.07079646017699115,rEs∼p[R(s)].
OPTIMIZATION ALGORITHM,0.07374631268436578,"2.2
OPTIMIZATION ALGORITHM"
OPTIMIZATION ALGORITHM,0.07669616519174041,"To optimize a policy algorithmically, we apply gradient descent on the expected sum of discounted
rewards. First, we parametrize a policy with parameters w ∈R|A|×|Ω|, normalized to get a proba-
bility using the softmax function πw(a|o) =
exp(wao)
P"
OPTIMIZATION ALGORITHM,0.07964601769911504,"b exp(wbo). Then, we compute the transition matrix
˜Tπ, from which we obtain the steady state p using the power method (See Appendix B). Finally, we
calculate Gπ by the Lemma 2.1."
OPTIMIZATION ALGORITHM,0.08259587020648967,"Using an algorithm that keeps track of the operations (we use pytorch Paszke et al. (2017)), we
can compute the gradient of Gπ with respect to the parameters w and perform gradient descent with
adaptive time steps (i.e. a gradient ﬂow dynamics):
d
dtw = d"
OPTIMIZATION ALGORITHM,0.0855457227138643,"dwGπ(w).
(3)"
TWO-HYPOTHESIS TESTING PROBLEM,0.08849557522123894,"2.3
TWO-HYPOTHESIS TESTING PROBLEM"
TWO-HYPOTHESIS TESTING PROBLEM,0.09144542772861357,"The problem we consider is the two-hypothesis testing problem. We label two arms by the letters
A and B. The two arms gives a reward of +1 or −1 with a Bernoulli distribution. The probabilities
to obtain a positive reward are noted kA and kB respectively. The environment is entirely deﬁned
by the couple (kA, kB). With equal probability, the environment is in one of the following two
conﬁgurations (hypothesis):

kA = 1+µ"
TWO-HYPOTHESIS TESTING PROBLEM,0.0943952802359882,"2
kB = 1−µ"
TWO-HYPOTHESIS TESTING PROBLEM,0.09734513274336283,"2
(hypothesis HA)
kA = 1−µ"
TWO-HYPOTHESIS TESTING PROBLEM,0.10029498525073746,"2
kB = 1+µ"
TWO-HYPOTHESIS TESTING PROBLEM,0.10324483775811209,"2
(hypothesis HB)
(4)"
TWO-HYPOTHESIS TESTING PROBLEM,0.10619469026548672,"where the hypothesis HA (resp. HB) corresponds to A (resp. B) being the best arm. In expectation
over the environments, an agent that plays randomly or always the same arm will have a reward 0."
TWO-HYPOTHESIS TESTING PROBLEM,0.10914454277286136,"Note that this problem is similar to the Bandit problem, except that in the latter (kA, kB) can take any
value in the square [0, 1]2. When r →0, our results below can be generalized to the bandit problem,
as done in Cover & Hellman (1970) by recasting the latter as ﬁnding the correct hypothesis ( ‘Arm
A is better’ or ‘Arm B is better’)."
TWO-HYPOTHESIS TESTING PROBLEM,0.11209439528023599,"In our setup, the agent only knows its last arm played, reward obtained (if there were some) and the
state of its memory (different memories are described below). Based on that, it chooses an arm (play
A or B) and how to update its memory state."
TWO-HYPOTHESIS TESTING PROBLEM,0.11504424778761062,"In the POMDP formalism (c.f. 2.1), the state s contains the environment (kA, kB), the memory
state, the last arm played and reward obtained. We only consider agents that have a complete access"
TWO-HYPOTHESIS TESTING PROBLEM,0.11799410029498525,Under review as a conference paper at ICLR 2022
TWO-HYPOTHESIS TESTING PROBLEM,0.12094395280235988,"to their memory, therefore O is deterministic and simply projects s by removing the environment
information."
TWO-HYPOTHESIS TESTING PROBLEM,0.12389380530973451,"s = environment HA or HB), memory state, last arm played (A or B) and last reward (1 or −1)
o = memory state, last arm played and last reward
a = arm to play, memory update
(5)"
TWO-HYPOTHESIS TESTING PROBLEM,0.12684365781710916,"We deﬁne the function q(s), which is 1 if the agent just played the ”wrong” arm in state s, and 0 if
he played the correct one. The probability to play the wrong arm is then qπ = Es∼p[q(s)] where
p is the steady state of the problem with reset r. The expected sum of discounted gains Gπ can be
related to qπ as: Gπ = µ"
TWO-HYPOTHESIS TESTING PROBLEM,0.12979351032448377,"r (1 −2qπ). In the following, we will use qπ as a measure of performance,
trying to ﬁnd a policy π that minimizes qπ (and thus maximizes Gπ)."
TYPES OF MEMORY CONSIDERED,0.13274336283185842,"2.4
TYPES OF MEMORY CONSIDERED"
TYPES OF MEMORY CONSIDERED,0.13569321533923304,"Deﬁnition 2.5 (Random Access Memory (RAM)). RAM is the most ﬂexible memory setting. The
agent has M memory states and has full control over it. It has |A| = M × 2 possible actions: the
choice of the next memory state and which arm to play. There is a high degeneracy in the space
of strategies since any permutation of the memory states leads to the same performance. Note that,
since our agent can use the information of the last arm and reward, the total number of memory
states is in fact Meﬀ= 4M, which corresponds to 2 + log2 M bits."
TYPES OF MEMORY CONSIDERED,0.13864306784660768,"Deﬁnition 2.6 (Memento Memory2). The agent only has access to the information of its past m
actions and rewards. For instance, for m = 4, an observation could be AABB++-+. We use the
notation of the most recent action/reward on the right (here, the last action was B and the reward
was +1). If the agent plays A and obtains a positive reward, the next memory state would be
ABBA+-++. In this memory architecture, the agent writes in its memory only through the plays he
does (|A| = 2). Here, the number of bits is 2m and the total number of memory states is in fact
Meﬀ= 4m."
GAIN AND EXPLORATION WITH A RANDOM ACCESS MEMORY,0.1415929203539823,"3
GAIN AND EXPLORATION WITH A RANDOM ACCESS MEMORY"
GAIN AND EXPLORATION WITH A RANDOM ACCESS MEMORY,0.14454277286135694,"Hellman & Cover (1970); Cover & Hellman (1970) described an optimal policy for the RAM ar-
chitecture in the limit of a small reset r. In their optimal policy, the memory states i = 1...M are
organized linearly with transitions only occurring between i and i −1 (if the last observation sup-
ports HA) or i + 1 (if the last observation supports HB). In the limit r →0, transition probabilities
can be shown to be independent on i for 1 < i < M. The two extreme memory states i = 1 and
i = M are special as they present a vanishing exit rate ϵ →0. Thus, only these two states are visited
with ﬁnite probability in that limit. For such a policy, one obtains an optimal probability to play the
worst arm qHC(M) = αM−1/(αM−1 + 1), with α = (1 −µ)/(1 + µ). 3"
GAIN AND EXPLORATION WITH A RANDOM ACCESS MEMORY,0.14749262536873156,"Our set-up is slightly different, as we allow for the choice of arm to depend on both the memory
state and the information of the last arm played and reward obtained (Figure 1A-B). In that case, the
policy can be improved, as demonstrated by considering the column of conﬁdence policy."
GAIN AND EXPLORATION WITH A RANDOM ACCESS MEMORY,0.1504424778761062,"Deﬁnition 3.1 (column of conﬁdence policy (CCP)). It is a RAM policy with M memory states. It
is depicted in Figure 1C. Essentially, the agent uses its last arm played to effectively increase the
size of its memory by a factor 2."
GAIN AND EXPLORATION WITH A RANDOM ACCESS MEMORY,0.15339233038348082,"The probability q to play the worst arm by following CCP is derived in Appendix C for general r,
by writing the transition probability matrix Tϵ with a generic ϵ, for any of the two hypotheses HA or"
GAIN AND EXPLORATION WITH A RANDOM ACCESS MEMORY,0.15634218289085547,"2Memento is a Christopher Nolan’s ﬁlm where the hero has a short-term memory loss every few minutes.
Using photos and tattoos, the hero keeps track of information he will eventually forget, thus encoding informa-
tion into his own actions.
3To see why a linear policy is optimal, introduce λi = P(HA|i)/P(HB|i), with P(HA|i) the conditional
probability that HA is true if the memory state i is visited. Choose the labels i such that λ1 ≥λ2... ≥λM.
Then it can be shown that λi/λi+1 ≤α−1 (Hellman & Cover (1970)). The linear policy saturates this bound
for all i, leading to the maximal ratio that can be obtained between any two states λ1/λM = α1−M. This ratio
controls the gain in the limit ϵ →0 where only these two states are visited with ﬁnite probabilities."
GAIN AND EXPLORATION WITH A RANDOM ACCESS MEMORY,0.1592920353982301,Under review as a conference paper at ICLR 2022
A,0.16224188790560473,"7A
7B"
A,0.16519174041297935,"8A
8B"
A,0.168141592920354,"5A
5B"
A,0.1710914454277286,"6A
6B"
A,0.17404129793510326,"3A
3B"
A,0.17699115044247787,"4A
4B"
A,0.17994100294985252,"1A
1B"
A,0.18289085545722714,"2A
2B"
A,0.18584070796460178,"y =
 +1 ⇒↑
−1 ⇒change arm"
A,0.1887905604719764,"y =
 +1 ⇒↑
−1 ⇒↓"
A,0.19174041297935104,"y = −1 ⇒↓with prob. ϵ Tn+1 en
yn Tn−1"
A,0.19469026548672566,"en+1
yn+1 Tn en
yn Tn"
A,0.1976401179941003,"en+1
yn+1"
A,0.20058997050147492,"on
on+1
an"
A,0.20353982300884957,Cover & Hellman Cover & Hellman (1970):
A,0.20648967551622419,Our setup:
A,0.20943952802359883,& play same arm
A,0.21238938053097345,"column of conﬁdence policy:
A B C"
A,0.2153392330383481,"Figure 1:
A Update scheme from Cover & Hellman (1970) vs B our update scheme. Using the
same notation as Cover & Hellman (1970), yn is the reward obtained by playing the arm en and Tn
is the memory state (not to confuse with the transition probability T of Sec 2.1). To make the link
with Sec 2.1 in our setup the state sn correspond to the tuple (en, yn, Tn, kA, kB), the observation
on to the triplet (en, yn, Tn) and the action an is represented by the purple and green arrows. The
red arrow can be seen as part of the conditional probability T(sn+1|sn, an). C The column of
conﬁdence policy, that can be used in the RAM case, exempliﬁed for M = 8. The memory states
are organized into two columns. The distribution p0 initializes the agent’s memory into states 1A
or 1B with equal chance. The agent keeps playing the same arm, moving up and down a column
depending on the reward, unless it is in the memory state 1 (it then switches arm after a negative
reward). Once the agent reaches a state at the top of a column, it can only step down with small
probability ϵ if a negative reward is obtained. This two-column arrangment effectively double the
number of memory states, by creating 2M = 16 distinct states. In this policy, the value of the
memory can be viewed as a measure of the conﬁdence in the arm being played."
A,0.2182890855457227,"HB. From the stationary distribution Tϵ⃗p = ⃗p, one obtains q(ϵ), which reaches its minimum value
for: ϵ =
√ 2 p"
A,0.22123893805309736,"α −α3M−1 + αM(2µ −3) + α2M(2µ + 3)
√"
A,0.22418879056047197,1 −αM(1 −αM −µ −αMµ)
A,0.22713864306784662,"√r + O(r)
(6)"
A,0.23008849557522124,"The result ϵ ∼√r indicates a non-trivial balance between exploration and exploitation, in which the
time spent in each extreme state of the memory grows only as the square root of the horizon time
1/r. A similar result was obtained for hypothesis testing (Wilson (2014))."
A,0.23303834808259588,"For r →0 (a result generalized for any kB and kA in Appendix C), we obtain:"
A,0.2359882005899705,"qCCP(M) =
α2M−1"
A,0.23893805309734514,"α2M−1 + 1.
(7)"
A,0.24188790560471976,"Note that it is the optimal gain of a RAM memory of size 2M. Since our agent memory size is
Meﬀ= 4M (the factor 4 coming from the 2 possible past actions and two possible rewards), the
results of Hellman & Cover (1970); Cover & Hellman (1970) show that CCP is nearly optimal, in the
sense that no policies with one less bit of memory can do better. In Figure 2, we conﬁrm empirically
that it is at least a local policy optimum, as performing policy optimization near this solution leads
to no further improvements."
GAIN FOR MEMENTO MEMORY,0.2448377581120944,"4
GAIN FOR MEMENTO MEMORY"
GAIN FOR MEMENTO MEMORY,0.24778761061946902,"Are there efﬁcient policies when the agent memorizes the last m arms played and rewards obtained
(Memento memory cf. 2.6)? In the classical two-arm bandit problem, after a time m, the optimal
strategy selects the worst arm with probability q = O(ln m/m) (Auer et al. (2002)). It turns out that"
GAIN FOR MEMENTO MEMORY,0.25073746312684364,Under review as a conference paper at ICLR 2022
GAIN FOR MEMENTO MEMORY,0.2536873156342183,"10−5
10−3
10−1
r 10−5 10−4 10−3 10−2 10−1 q"
GAIN FOR MEMENTO MEMORY,0.25663716814159293,"µ = 0.1
M = 5
µ = 0.1
M = 10
µ = 0.1
M = 20
µ = 0.2
M = 5
µ = 0.2
M = 10
µ = 0.2
M = 20"
GAIN FOR MEMENTO MEMORY,0.25958702064896755,"100
101 M"
GAIN FOR MEMENTO MEMORY,0.26253687315634217,"µ = 0.1
r = 10−6
µ = 0.1
r = 10−4
µ = 0.1
r = 10−2
µ = 0.2
r = 10−6
µ = 0.2
r = 10−4
µ = 0.2
r = 10−2"
GAIN FOR MEMENTO MEMORY,0.26548672566371684,"Figure 2: Left Probability to play the worst arm q vs. the reset probability r for different memory
sizes M and two values of µ (the difference between the mean outcome of the two arms). Right q
vs. memory size M for different r and µ indicated in legend. The solid lines show the analytical
result corresponding to column of conﬁdence policy (CCP), whereas symbols show the results of the
learning algorithm (see Sec 2.2) for a RAM memory with a policy initialized close to the predicted
optimal column of conﬁdence policy (π0 ≈πCCP + 10−4). Learning does not ﬁnd strategies that
perform better than the optimal column of conﬁdence policy, even for large values of r, indicating
that it is a local optimum. In this ﬁgure, error bars would be smaller than the symbols."
GAIN FOR MEMENTO MEMORY,0.26843657817109146,"Figure 3: Necklace policy with memory of the m = 4 last arms played and rewards obtained (Me-
mento memory cf. 2.6). Memory states are organized into 3 (5 if we consider the two end states)
cycles of arms played, called necklaces in combinatorics. The memory state also contains the re-
wards obtained, but these are not explicitly shown here for the sake of readability. Most of time,
the agent stays in the same necklace by playing the oldest action he remembers. Each necklace
has 2 inputs and 2 outputs states (some states can be both input and output). The agent has a ﬁnite
probability to leave its current necklace only when the output state is maximally informative: all 4
rewards are + for A and −for B to move to the left, or the opposite to move to the right. Thicker
arrows represent high probability transition (deterministic in some situations); dashed arrows repre-
sent input/output transitions between necklaces; and thin arrows represent the small probabilities to
leave the two end states."
GAIN FOR MEMENTO MEMORY,0.2713864306784661,"our agent can use his own actions to encode events over a time much longer than m, leading to q
exponentially small in 2m in a stationary state with long horizon r →0."
GAIN FOR MEMENTO MEMORY,0.2743362831858407,Deﬁnition 4.1 (Necklace policy). The necklace policy is based on 4 key ingredients (Figure 3).
GAIN FOR MEMENTO MEMORY,0.27728613569321536,"(i) Most of the time, the agent plays the oldest action in its memory (i.e. the arm played m actions
before). Doing so, it memorizes actions cycle inside binary necklaces of length m (in combinatorics,
a necklace is an equivalent class of character strings under cyclic rotation, here the strings are words
of length m made of the letters A and B, hence binary). When m is prime, there are exactly N(m) =
2 + (2m −2)/m distinct necklaces. For any m, the number of necklaces can be derived from P´olya
(1937)’s enumeration theorem and is equal to N(m) =
1
m
P"
GAIN FOR MEMENTO MEMORY,0.28023598820059,"d|m ϕ(d)2m/d, where ϕ is the Euler’s
totient function."
GAIN FOR MEMENTO MEMORY,0.2831858407079646,Under review as a conference paper at ICLR 2022
GAIN FOR MEMENTO MEMORY,0.2861356932153392,"(ii) We provide a Gray order on the necklaces. It means that necklaces are numbered and two
successive necklaces can only differ by one letter. We order the necklaces from i = 1 for the
necklace where all actions are A to i = n(m) for the necklace where all actions are B. The necklace
i = 1 (resp. i = n(m)) also corresponds to the maximum conﬁdence in hypothesis HA (resp. HB).
In general, the longest possible chain of necklace, n(m), is unknown and less than the total number
N(m) of necklaces. But, when m is prime, it has been conjectured (and checked for m ≤37)
that there exists a Gray order of all distinct necklaces (Degni & Drisko (2007)): in other words,
n(m) = N(m) = 2 + (2m −2)/m, for m prime."
GAIN FOR MEMENTO MEMORY,0.2890855457227139,"(iii) The probability to exit a necklace is zero, except for two exit conﬁgurations for which this
probability is ϵ1 > 0 if two conditions are met. First, the memorized actions must allow the agent
to switch from the necklace i to the necklace i −1 or i + 1 by taking a new action. Second, the
sequence of rewards must be maximally informative: to switch to the necklace i −1 (i.e. gaining
conﬁdence in HA), all rewards have to be +1 for the arm A and −1 for the arm B and the opposite
to switch to the necklace i + 1."
GAIN FOR MEMENTO MEMORY,0.2920353982300885,"(iv) In the two extreme states, the probability of exit is ϵ0 when all rewards are negative. Below, we
consider the limit limϵ1→0 limϵ0→0 q(ϵ0, ϵ1) of this strategy. This order of limits ensures that only
the extreme states are visited with a ﬁnite probability and that the agent cycles many times in each
necklace before exit."
GAIN FOR MEMENTO MEMORY,0.2949852507374631,In order to compute the optimal gain of the necklace policy we introduce the following two lemmas.
GAIN FOR MEMENTO MEMORY,0.29793510324483774,"Lemma 4.1. Assume a discrete random walk on a chain of sites indexed by i, with probabilities ri
to step from i to i + 1 and li to step from i to i −1. Starting in site i, the probability to reach site
j + 1 (i ≤j) before site i −1 is"
GAIN FOR MEMENTO MEMORY,0.3008849557522124,"Pi→j =

1 + li"
GAIN FOR MEMENTO MEMORY,0.30383480825958703,"ri
+ li ri"
GAIN FOR MEMENTO MEMORY,0.30678466076696165,"li+1
ri+1
+ · · · + li"
GAIN FOR MEMENTO MEMORY,0.30973451327433627,"ri
. . . lj rj"
GAIN FOR MEMENTO MEMORY,0.31268436578171094,"−1
.
(8)"
GAIN FOR MEMENTO MEMORY,0.31563421828908556,Proof. The proof is developed in Appendix D.1.
GAIN FOR MEMENTO MEMORY,0.3185840707964602,"Lemma 4.2. Assume a discrete random walk on a chain of n + 2 sites indexed by i = 0, 1, . . . n + 1
(with probabilities ri to step to the right and li to the left). If r0 = ϵR and ln+1 = ϵL, in the limit
ϵ →0, the probability to be on site n + 1 is"
GAIN FOR MEMENTO MEMORY,0.3215339233038348,"p(n + 1) =  1 + L R n
Y i=1 li
ri !−1 .
(9)"
GAIN FOR MEMENTO MEMORY,0.32448377581120946,Proof. The proof is developed in Appendix D.2.
GAIN FOR MEMENTO MEMORY,0.3274336283185841,Using these two lemma the following theorem can be proved.
GAIN FOR MEMENTO MEMORY,0.3303834808259587,"Theorem 4.3. Consider the two-hypothesis problem described in (4) in the limit of small reset
r →0. The necklace policy described in Deﬁnition 4.1 with parameters (ϵ0, ϵ1) has a probability q
to play the worst arm satisfying q ≥q∗, with:"
GAIN FOR MEMENTO MEMORY,0.3333333333333333,"q∗=

1 + αm(1−n(m))−1
with α = 1 −µ"
GAIN FOR MEMENTO MEMORY,0.336283185840708,"1 + µ.
(10)"
GAIN FOR MEMENTO MEMORY,0.3392330383480826,The optimal necklace policy converges to q∗when r ≪ϵ0 ≪ϵ1 ≪1.
GAIN FOR MEMENTO MEMORY,0.3421828908554572,Proof. The detailed proof is contained in Appendix D.3.
GAIN FOR MEMENTO MEMORY,0.34513274336283184,"Note
At leading order q∗= α2m + o(α2m). This is because the number of distinct necklaces
N(m) only differs from 2+(2m −2)/m at second order, and because we expect to ﬁnd Gray orders
within those distinct necklaces whose length only differ from N(m) at second order."
GAIN FOR MEMENTO MEMORY,0.3480825958702065,Under review as a conference paper at ICLR 2022 0.0 0.1 0.2 0.3 0.4 0.5 q A
GAIN FOR MEMENTO MEMORY,0.35103244837758113,"RAM M = 8, r = 10−3"
GAIN FOR MEMENTO MEMORY,0.35398230088495575,"RAM random
RAM linear
RAM columns
RAM CCP C"
GAIN FOR MEMENTO MEMORY,0.35693215339233036,"Memento m = 3, r = 10−3"
GAIN FOR MEMENTO MEMORY,0.35988200589970504,"Memento random
Memento cycles
Memento necklace E"
GAIN FOR MEMENTO MEMORY,0.36283185840707965,"Meﬀ= 64, r = 10−5"
GAIN FOR MEMENTO MEMORY,0.36578171091445427,"Memento random
RAM random
CCP
necklace (r = 0)"
GAIN FOR MEMENTO MEMORY,0.3687315634218289,"100
103
106 t 0.0 0.1 0.2 0.3 0.4 0.5 q B"
GAIN FOR MEMENTO MEMORY,0.37168141592920356,"RAM M = 20, r = 10−3"
GAIN FOR MEMENTO MEMORY,0.3746312684365782,"RAM random
RAM linear
RAM columns
RAM CCP"
GAIN FOR MEMENTO MEMORY,0.3775811209439528,"100
103
106 t D"
GAIN FOR MEMENTO MEMORY,0.3805309734513274,"Memento m = 4, r = 10−3"
GAIN FOR MEMENTO MEMORY,0.3834808259587021,"Memento random
Memento cycles
Memento necklace"
GAIN FOR MEMENTO MEMORY,0.3864306784660767,"100
103
106 t F"
GAIN FOR MEMENTO MEMORY,0.3893805309734513,"Meﬀ= 256, r = 10−5"
GAIN FOR MEMENTO MEMORY,0.39233038348082594,"Memento random
RAM random
CCP
necklace (r = 0)"
GAIN FOR MEMENTO MEMORY,0.3952802359882006,"Figure 4: Dynamics of the optimization algorithm for different initialization methods. Probability
to play the worse arm q vs. algorithm time t (time deﬁned in (3)) for different seeds. 20 initialization
seeds are shown in light color and the median is shown in solid color. The wall time is caped to 1 hour
per optimization. A-B RAM with M = 8 and M = 20, we compare the random initialization, the
linear initialization where the jumps in memory states are initialized to be contiguous, the columns
initialization corresponding to linear initialization with the extra constraint that the last action is
repeated except for the memory state 1 and the CCP initialization, very close to the column of
conﬁdence policy (i.e. π0 ≈πCCP + 10−4, a difference that explains why the red curves can
decrease). C-D q vs. t for the Memento memory m = 3 and m = 4. We compare the random
initialization with the cycles initialization that repeats the oldest action, except if all the remembered
plays correspond to a maximally informative event (during training a path between the cycles has to
be learned) and the necklace where ϵ0 and ϵ1 has to be learned. E-F q vs. t for randomly initialized
policies. The values of m (3 and 4) and M (16 and 64) are chosen such that the total memory
needed to perform these strategies Meﬀis identical in each panel. The dashed line corresponds to
calculation of q for the CCP and for the necklace policy (for which we only have a prediction for
r = 0). In this ﬁgure µ = 0.1."
POLICY OPTIMIZATION AND LOCAL MINIMA,0.39823008849557523,"5
POLICY OPTIMIZATION AND LOCAL MINIMA"
POLICY OPTIMIZATION AND LOCAL MINIMA,0.40117994100294985,"To study empirically how learning depends on the memory architecture, we measure how the proba-
bility q(t) to play the worse arm after a training time t depends on the initialization of policy. For the
RAM memory, we ﬁnd that random initialization (blue curves) leads to very poor results (panels A
and B of Figure 4). Results however improve when a linear structure for memory states is imposed
(orange curves) and when the arms played are segregated on the two sides of that linear structure"
POLICY OPTIMIZATION AND LOCAL MINIMA,0.40412979351032446,Under review as a conference paper at ICLR 2022
POLICY OPTIMIZATION AND LOCAL MINIMA,0.40707964601769914,"to form two columns (green). However, even in that case, training does not converge towards the
optimal column of conﬁdence parameters, unless parameters are initialized near the optimal values
(red curves)."
POLICY OPTIMIZATION AND LOCAL MINIMA,0.41002949852507375,"By contrast, training with the Memento memory (consisting in the last m actions and rewards, cf.
2.6) appears less sensitive to initialization. As shown in the panel C and D of Figure 4, initializing the
policy randomly (blue) performs does not perform as well as initializing the policy with necklaces
(orange), however the difference is not signiﬁcant."
POLICY OPTIMIZATION AND LOCAL MINIMA,0.41297935103244837,"Although the RAM architecture is in principle more ﬂexible (and in fact include Memento mem-
ories), we ﬁnd that, for random initialization, the Memento architecture leads to actually better
policies after training. The comparison is shown in panels E and F of Figure 4, where the two
memory architectutes are compared keeping the effective memory size Meﬀconstant. This ﬁnd-
ing emphasizes the need to constrain memory architecture, so as to obtain smoother optimization
landscapes."
CONCLUSION,0.415929203539823,"6
CONCLUSION"
CONCLUSION,0.41887905604719766,Policies
CONCLUSION,0.4218289085545723,"Memory scheme
Memento (cf. 2.6)
RAM (cf. 2.5)
Policy
necklace (cf. 4.1)
CCP (cf. 3.1)
Effective memory
Meﬀ= 4m
Meﬀ= 4M
Performance (q−1 −1)
αm(1−n(m)) ∼α−2m
α−(2M−1)"
CONCLUSION,0.4247787610619469,"... as function of Meﬀ
∼α−√Meff
∼α−Meff/2"
CONCLUSION,0.4277286135693215,"... more generally for (kA > kB)

1−kB
1−kA"
CONCLUSION,0.4306784660766962,"m 
1/kB−1
1/kA−1
m(n(m)−2)/2
1−kB
1−kA"
CONCLUSION,0.4336283185840708,"
1/kB−1
1/kA−1
M−1"
CONCLUSION,0.4365781710914454,"Table 1: Comparison of the performances of the necklace and CCP strategies in the limit r →0.
As shown in the last line, the gain of these policies can be generalized to any distribution of the
Bernoulli probabilities kA and kB (but needs not be optimal then)."
CONCLUSION,0.43952802359882004,"Our results are summarized in Table 1 that compares the necklace policy (cf. 4.1) and the column of
conﬁdence policy (cf. 3.1). For each of these policies, we provide the optimal performance, reached
in the limit r →0. We conjecture that these policies are the optimal ones for the Memento and RAM
memory schemes respectively. Concerning the Memento memory, this conjecture is supported by
the simulations shown in Figure 3: the best numerical policies found for m = 3 and m = 4 are in
fact the necklace policy."
CONCLUSION,0.4424778761061947,"An interesting additional questions for the future is the generalization of these ideas to a broader set
of tasks. The CCP appears well-suited for multiple hypotheses testing (Chandrasekaran & Laksh-
manan (1978); Yakowitz et al. (1974)), where it would correspond to a “star” policy with a branch
for each hypothesis. Classifying optimal policies for more complex hierarchical tasks, such as those
involved in navigation (Theocharous et al. (2004); Toussaint et al. (2008)), would have practical
applications. Looking ahead, it would be interesting to understand if these ideas have applications
to other approaches dealing with POMDPs, including recurrent networks (Li et al. (2015)) whose
theoretical understanding remains very limited."
CONCLUSION,0.44542772861356933,"Finally, it is intriguing that for all memory structures studied, a linear organization of memory states
appears to be optimal. Despite the fact that our set-up is intrinsically digital, optimal policies ap-
proach an analog memory architecture with a single degree of freedom: it corresponds to the position
along the chain, and measures the relative belief of one hypothesis over the other. In neuroscience,
dominants models of decision making often present a single analogue variable being updated by
observations (Gold & Shadlen (2007); Rescorla & Wagner (1972)). It would be interesting to test
experimentally, in situations where the environment can change with a small probability r between
two distinct classes, if animals stick to two extreme believes, and leave them for exploration with
some rate ∼√r."
CONCLUSION,0.44837758112094395,Under review as a conference paper at ICLR 2022
REFERENCES,0.45132743362831856,REFERENCES
REFERENCES,0.45427728613569324,"Eitan Altman. Constrained Markov Decision Processes. Chapman and Hall/CRC, 1999."
REFERENCES,0.45722713864306785,"Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2):235–256, 2002."
REFERENCES,0.46017699115044247,"Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966."
REFERENCES,0.4631268436578171,"Balakrishnan Chandrasekaran and Kadathur B. Lakshmanan. Finite memory multiple hypothesis
testing: Close-to-optimal schemes for bernoulli problems. IEEE Transactions on Information
Theory, 24(6):755–759, 1978."
REFERENCES,0.46607669616519176,"Thomas M. Cover and Martin E. Hellman. The two-armed-bandit problem with time-invariant ﬁnite
memory. IEEE Transactions on Information Theory, 16(2):185–195, 1970. doi: 10.1109/TIT.
1970.1054427."
REFERENCES,0.4690265486725664,"Christopher Degni and Arthur A. Drisko. Gray-ordered binary necklaces. the electronic journal of
combinatorics, pp. R7–R7, 2007."
REFERENCES,0.471976401179941,"Joshua I. Gold and Michael N. Shadlen. The neural basis of decision making. Annual review of
neuroscience, 30, 2007."
REFERENCES,0.4749262536873156,"Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps.
arXiv preprint arXiv:1507.06527, 2015."
REFERENCES,0.4778761061946903,"Milos Hauskrecht. Value-function approximations for partially observable markov decision pro-
cesses. Journal of artiﬁcial intelligence research, 13:33–94, 2000."
REFERENCES,0.4808259587020649,"Martin E. Hellman and Thomas M. Cover. Learning with Finite Memory. The Annals of Mathemat-
ical Statistics, 41(3):765–782, 1970. ISSN 0003-4851."
REFERENCES,0.4837758112094395,"Tommi Jaakkola, Satinder P. Singh, and Michael I. Jordan. Reinforcement learning algorithm for
partially observable markov decision problems. Advances in neural information processing sys-
tems, pp. 345–352, 1995."
REFERENCES,0.48672566371681414,"Arbaaz Khan, Clark Zhang, Nikolay Atanasov, Konstantinos Karydis, Vijay Kumar, and Daniel D.
Lee. Memory augmented control networks. arXiv preprint arXiv:1709.05706, 2017."
REFERENCES,0.4896755162241888,"Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li Deng, and Ji He. Recurrent
reinforcement learning: a hybrid approach. arXiv preprint arXiv:1509.03044, 2015."
REFERENCES,0.49262536873156343,"Michael L. Littman. An optimization-based categorization of reinforcement learning environments.
From animals to animats, 2:262–270, 1993."
REFERENCES,0.49557522123893805,"Nicolas Meuleau, Kee-Eung Kim, Leslie Pack Kaelbling, and Anthony R. Cassandra.
Solv-
ing pomdps by searching the space of ﬁnite policies. In Kathryn B. Laskey and Henri Prade
(eds.), UAI ’99:
Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial In-
telligence, Stockholm, Sweden, July 30 - August 1, 1999, pp. 417–426. Morgan Kaufmann,
1999.
URL https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=
1&smnu=2&article_id=194&proceeding_id=15."
REFERENCES,0.49852507374631266,"Nicolas Meuleau, Leonid Peshkin, Kee-Eung Kim, and Leslie Pack Kaelbling. Learning ﬁnite-state
controllers for partially observable environments. arXiv preprint arXiv:1301.6721, 2013."
REFERENCES,0.5014749262536873,"Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, et al. Control of memory, active perception,
and action in minecraft.
In International Conference on Machine Learning, pp. 2790–2799.
PMLR, 2016."
REFERENCES,0.504424778761062,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic differentiation in
pytorch. 2017."
REFERENCES,0.5073746312684366,"Leonid Peshkin, Nicolas Meuleau, and Leslie Kaelbling. Learning policies with external memory.
arXiv preprint cs/0103003, 2001."
REFERENCES,0.5103244837758112,Under review as a conference paper at ICLR 2022
REFERENCES,0.5132743362831859,"George P´olya.
Kombinatorische anzahlbestimmungen f¨ur gruppen, graphen und chemische
verbindungen. Acta mathematica, 68(1):145–254, 1937."
REFERENCES,0.5162241887905604,"Robert A. Rescorla and Allan R. Wagner. A theory of Pavlovian conditioning: Variations in the
effectiveness of reinforcement and nonreinforcement, pp. 64–99. Appleton-Century-Crofts, 1972."
REFERENCES,0.5191740412979351,"Nicholas Roy, Geoffrey Gordon, and Sebastian Thrun.
Finding approximate pomdp solutions
through belief compression. Journal of artiﬁcial intelligence research, 23:1–40, 2005."
REFERENCES,0.5221238938053098,"David Silver and Joel Veness. Monte-carlo planning in large pomdps. Neural Information Processing
Systems, 2010."
REFERENCES,0.5250737463126843,"Richard D. Smallwood and Edward J. Sondik. The optimal control of partially observable markov
processes over a ﬁnite horizon. Operations research, 21(5):1071–1088, 1973."
REFERENCES,0.528023598820059,"Adhiraj Somani, Nan Ye, David Hsu, and Wee Sun Lee. Despot: Online pomdp planning with
regularization. In NIPS, volume 13, pp. 1772–1780, 2013."
REFERENCES,0.5309734513274337,"Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.5339233038348082,"Georgios Theocharous, Kevin Murphy, and Leslie Pack Kaelbling.
Representing hierarchical
pomdps as dbns for multi-scale robot localization. In IEEE International Conference on Robotics
and Automation, 2004. Proceedings. ICRA’04. 2004, volume 1, pp. 1045–1051. IEEE, 2004."
REFERENCES,0.5368731563421829,"Rodrigo Toro Icarte, Richard Valenzano, Toryn Q. Klassen, Phillip Christoffersen, Amir-massoud
Farahmand, and Sheila A. McIlraith. The act of remembering: A study in partially observable
reinforcement learning. arXiv:2010.01753 [cs], 2020."
REFERENCES,0.5398230088495575,"Marc Toussaint, Laurent Charlin, and Pascal Poupart. Hierarchical pomdp controller optimization
by likelihood maximization. In UAI, volume 24, pp. 562–570, 2008."
REFERENCES,0.5427728613569321,"Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992."
REFERENCES,0.5457227138643068,"Andrea Wilson. Bounded Memory and Biases in Information Processing. Econometrica, 82(6):
2257–2294, 2014. ISSN 1468-0262. doi: 10.3982/ECTA12188."
REFERENCES,0.5486725663716814,"Sidney Yakowitz et al. Multiple hypothesis testing by ﬁnite memory algorithms. The Annals of
Statistics, 2(2):323–336, 1974."
REFERENCES,0.551622418879056,"Marvin Zhang, Zoe McCarthy, Chelsea Finn, Sergey Levine, and Pieter Abbeel. Learning deep
neural network policies with continuous memory states. In 2016 IEEE International Conference
on Robotics and Automation (ICRA), pp. 520–527, 2016. doi: 10.1109/ICRA.2016.7487174."
REFERENCES,0.5545722713864307,Under review as a conference paper at ICLR 2022
REFERENCES,0.5575221238938053,Appendices
REFERENCES,0.56047197640118,"A
PROOF OF LEMMA 2.1"
REFERENCES,0.5634218289085545,Proof. The state transition matrix of the undiscounted POMDP is
REFERENCES,0.5663716814159292,"˜Tπ(s′|s) = rp0(s′) + (1 −r)Tπ(s′|s).
(11)"
REFERENCES,0.5693215339233039,"The steady state p(s) has to be stable through ˜Tπ, thus satisfying p(s′) = P"
REFERENCES,0.5722713864306784,"s ˜Tπ(s′|s)p(s). In the
tensor form, it can be written ⃗p = r⃗p0 + (1 −r)Tπ⃗p. Applying recursively this formula n times we
obtain:"
REFERENCES,0.5752212389380531,"⃗p = r n−1
X"
REFERENCES,0.5781710914454278,"t=0
(1 −r)tT t
π⃗p0 + (1 −r)nT n
π ⃗p.
(12)"
REFERENCES,0.5811209439528023,Translating it into an expectation value expression we obtain:
REFERENCES,0.584070796460177,Es∼p[f(s)] = r Est∼T tπp0
REFERENCES,0.5870206489675516,"""n−1
X"
REFERENCES,0.5899705014749262,"t=0
(1 −r)tf(st) #"
REFERENCES,0.5929203539823009,"+ (1 −r)nEs∼T n
π p[f(s)]
(13)"
REFERENCES,0.5958702064896755,"for any function f and where Tπp is understood as a matrix-vector product (note that Tπp ̸= p). By
replacing f by R and by taking the limit n →∞, for r = 1 −γ, we can identify (13) with (2), thus
obtaining the Lemma 2.1."
REFERENCES,0.5988200589970502,"B
IMPLEMENTATION DETAILS"
REFERENCES,0.6017699115044248,To compute the steady state we use the power method algorithm: Alg.1.
REFERENCES,0.6047197640117994,"When we have multiple independent environments (by environment we mean subset of S that the
agent cannot escape with its actions), S is the disjoint union of these environments: S = S1 + S2 +
. . . . If p0 factorize as follow p0(s) = P(Si)P(s|Si) for s ∈Si, we can compute the steady state by
computing those of each independent environments."
REFERENCES,0.6076696165191741,"The initial state of the memory of the agent can be optimized by allowing gradient ﬂow to modify
speciﬁc part of p0. It could mathematically be reformulated as special actions done on special initial
states to initialize the memory."
REFERENCES,0.6106194690265486,"Data: A transition matrix M
Result: The steady state
while the columns of M differ (with a given tolerance) do"
REFERENCES,0.6135693215339233,"MM →M;
end
return a column of M;"
REFERENCES,0.616519174041298,Algorithm 1: Power method
REFERENCES,0.6194690265486725,"C
EXACT COMPUTATION FOR COLUMN OF CONFIDENCE POLICY"
REFERENCES,0.6224188790560472,"The mathematica notebook is provided along with the code, see the link in Section 1."
REFERENCES,0.6253687315634219,Here we compute the optimal value of ϵ (that maximize the gain) given r and µ.
REFERENCES,0.6283185840707964,"First observe that the states (3B+, 1A-, ...) can be arranged along a line:"
REFERENCES,0.6312684365781711,"MA+
. . .
2A+
1A+
1B+
2B+
. . .
MB+
MA−
. . .
2A−
1A−
1B−
2B−
. . .
MB−
(14)"
REFERENCES,0.6342182890855457,where the probability transition only occurs between two consecutive states.
REFERENCES,0.6371681415929203,Under review as a conference paper at ICLR 2022
REFERENCES,0.640117994100295,If we merge the ± we have 2M states and we can compute their transition matrix without reset: Q = 
REFERENCES,0.6430678466076696,
REFERENCES,0.6460176991150443,"1−ϵkB
kA
ϵkB
0
kA"
REFERENCES,0.6489675516224189,"kB
0
..."
REFERENCES,0.6519174041297935,"kB
...
kA
...
0
ϵkA
kB
1−ϵkA "
REFERENCES,0.6548672566371682, (15)
REFERENCES,0.6578171091445427,where kA = (1 + µ)/2 and kB = (1 −µ)/2
REFERENCES,0.6607669616519174,"Including the reset, the transition probability becomes"
REFERENCES,0.6637168141592921,"Mij = (1 −r)Qij + rJi
(16)"
REFERENCES,0.6666666666666666,where J is 1/2 in the two central states 1A and 1B.
REFERENCES,0.6696165191740413,"The steady state pi is the solution of
X"
REFERENCES,0.672566371681416,"j
Mijpj = pi
⇐⇒
X"
REFERENCES,0.6755162241887905,"j
((1 −r)Qij −δij)pj = −Ji
(17)"
REFERENCES,0.6784660766961652,"We can split this problem in 5 regions: the A border, the A bulk, the center, the B bulk, the B border.
In the bulks the (17) is
(1 −r)kBpi−1 −pi + (1 −r)kApi+1 = 0
(18)"
REFERENCES,0.6814159292035398,"The solutions are of the form pi = c1wi
+ + c2wi
−with"
REFERENCES,0.6843657817109144,"w± = 1 ±
p"
REFERENCES,0.6873156342182891,"1 −(1 −r)2(1 −µ2)
(1 −r)(1 + µ)
(19)"
REFERENCES,0.6902654867256637,"To ﬁx the coefﬁcients c1, c2 in the two bulks we have 6 equations:"
REFERENCES,0.6932153392330384,• two on the left border (two ﬁrst lines of (17))
REFERENCES,0.696165191740413,"• two in the center, at the injection (two middle lines of (17))"
REFERENCES,0.6991150442477876,• two on the right border (two last lines of (17))
REFERENCES,0.7020648967551623,"Solving these equations, we can compute the probability to play the wrong arm (B, assuming µ > 0). q ="
REFERENCES,0.7050147492625368,"w+(1 + w+)(1 −w−)w2M
−(1 + w+(ϵ −1))(w−+ ϵ −1)
+(w+ ↔w−)
+(w−w+)M(w−w+ −1)((w−−1)(w+ −1)(w−+ w+)(ϵ −1) + 2w−w+ϵ2)"
REFERENCES,0.7079646017699115,"2(w−−w+)(w2M
+ w−(1 + w−(ϵ −1))(w+ + ϵ −1) −(w+ ↔w−))
(20)"
REFERENCES,0.7109144542772862,"in this expression of q we expressed r, µ and kA, kB in function of w±"
REFERENCES,0.7138643067846607,Optimizing q with respect to ϵ leads to
REFERENCES,0.7168141592920354,ϵ = (w−w+)−M
REFERENCES,0.7197640117994101,"(w−−1)wM
−(w+ −1)wM
+ (w−w+ −1)(wM
−w+ −w−wM
+ )2 −"
REFERENCES,0.7227138643067846,"v
u
u
u
u
u
u
t"
REFERENCES,0.7256637168141593,"(w−−1)(w+ −1)(w−w+)1+2M(w−w+ −1)2(wM
+ −wM
−)
 
"
REFERENCES,0.7286135693215339,"w3M
−w2
+(1 + w−)(1 + w+)
−(w+ ↔w−)
+w2M
+ w1+M
−
(2w+ + w−+ w+w−(7 + 2w+) + w2
−(w+ −1))
−(w+ ↔w−)  
"
REFERENCES,0.7315634218289085,(w−w+ −1) 
REFERENCES,0.7345132743362832,"
w2
−w2M
+ (1 + w−(w+ −1) + w+)
+(w+ ↔w−)
−2w1+M
−
w1+M
+
(1 + w−w+)   (21)"
REFERENCES,0.7374631268436578,Under review as a conference paper at ICLR 2022
REFERENCES,0.7404129793510325,"We can make the Taylor expansion of ϵ with respect to r ϵ =
√ 2 p"
REFERENCES,0.7433628318584071,"α −α3M−1 + αM(2µ −3) + α2M(2µ + 3)
√"
REFERENCES,0.7463126843657817,1 −αM(1 −αM −µ −αMµ)
REFERENCES,0.7492625368731564,"√r + O(r)
(22)"
REFERENCES,0.7522123893805309,with α = 1−µ 1+µ
REFERENCES,0.7551622418879056,"For M large, it converge quickly toward"
REFERENCES,0.7581120943952803,"ϵ =
r
2r
1 −µ2 + O(r)
(23)"
REFERENCES,0.7610619469026548,In the limit r →0 we get
REFERENCES,0.7640117994100295,"q =
α2M−1"
REFERENCES,0.7669616519174042,"α2M−1 + 1 + O(√r)
(24)"
REFERENCES,0.7699115044247787,"D
RANDOM WALK ALONG A CHAIN"
REFERENCES,0.7728613569321534,"D.1
PROBABILITY TO TRAVERSE THE CHAIN"
REFERENCES,0.775811209439528,"1 −(li + ri) ri
li"
REFERENCES,0.7787610619469026,"i
i + 1
i −1"
REFERENCES,0.7817109144542773,Figure 5: Markov chain
REFERENCES,0.7846607669616519,"Consider a random walk on a chain of sites with probabilities ri to move from site i to i + 1 and li
to move from i to i −1 (Figure 5). First, we want to compute the probability Pi→j (with i ≤j) that,
starting at site i, the walker visits the site j + 1 before the site i −1. Note that Pi→j only depends
on {lk}j
k=i and {rk}j
k=i. Note also that according to this deﬁnition Pi→i =
ri
li+ri . We also deﬁne
Pi←j as the probability to reach i −1 before j + 1 by starting in j, then we have Pi←i =
li
li+ri ."
REFERENCES,0.7876106194690266,"Starting in i, after one time step the walker is either (i) in i−1 (with probability li) and the probability
to reach j +1 before reaching i−1 becomes null, (ii) still in i (with probability 1−(li +ri)) and the
probability to each j + 1 before i −1 is still given by Pi→j, (iii) in i + 1 (with probability ri). Once
in i+1 there are two possibilities: either the walker never comes back to i and it reaches j +1 (with
probability Pi+1→j), or it does (with probability 1 −Pi+1→j) and it again has the same probability
Pi→j to reach j + 1 before i −1. Thus we have:"
REFERENCES,0.7905604719764012,"Pi→j = (1 −(li + ri))Pi→j + ri (Pi+1→j + (1 −Pi+1→j)Pi→j) .
(25)"
REFERENCES,0.7935103244837758,The quantities that matter are pi ≡ri/(li + ri). If we isolate Pi→j to the l.h.s. we get
REFERENCES,0.7964601769911505,"Pi→j =
piPi+1→j
1 −pi(1 −Pi+1→j).
(26)"
REFERENCES,0.799410029498525,Making the changes of variable Xi→j ≡1−Pi→j
REFERENCES,0.8023598820058997,"Pi→j
and xi ≡1−pi"
REFERENCES,0.8053097345132744,"pi
(note that xi = li/ri) we obtain"
REFERENCES,0.8082595870206489,"Xi→j = xi(1 + Xi+1→j).
(27)"
REFERENCES,0.8112094395280236,By repeating the formula we see that we get
REFERENCES,0.8141592920353983,"Xi→j = j
X k=i k
Y"
REFERENCES,0.8171091445427728,"l=i
xl = xi + xixi+1 + xixi+1xi+2 + · · · + xi · · · xj
(28)"
REFERENCES,0.8200589970501475,from which we can get Pi→j by the inverse transformation
REFERENCES,0.8230088495575221,"Pi→j =

1 + li"
REFERENCES,0.8259587020648967,"ri
+ li ri"
REFERENCES,0.8289085545722714,"li+1
ri+1
+ · · · + li"
REFERENCES,0.831858407079646,"ri
· · · lj rj"
REFERENCES,0.8348082595870207,"−1
.
(29)"
REFERENCES,0.8377581120943953,Under review as a conference paper at ICLR 2022
REFERENCES,0.8407079646017699,"D.2
CHAIN WITH FINAL STATES"
REFERENCES,0.8436578171091446,"Let us consider the same chain as above with a ﬁnite length n such that sites are labelled from i = 1
to n. Now let us add a ﬁnal state at each end of the chain (sites i = 0 and n + 1). We consider the
probabilities to leave the ﬁnal states asymptotically small, of order ϵ. More precisely, the probability
to move from i = 0 to 1 is ϵR (we call it R as it is a probability to go to the Right, although it
concerns the most left site) and the probability to move from n +1 to n is ϵL. These probability and
the probabilities p(0) and p(n + 1) to be on the site 0 and n + 1 are related by a balance of the ﬂow
from 0 to n + 1:"
REFERENCES,0.8466076696165191,"p(0)ϵRP1→n = p(n + 1)ϵLP1←n.
(30)"
REFERENCES,0.8495575221238938,"In the limit ϵ →0, the probabilities to be in the extreme states tends to 1 and we thus have p(0) =
1 −p(n + 1), yielding"
REFERENCES,0.8525073746312685,"p(n + 1) =

1 + L"
REFERENCES,0.855457227138643,"R
P1←n
P1→n"
REFERENCES,0.8584070796460177,"−1
.
(31)"
REFERENCES,0.8613569321533924,This result can be simpliﬁed using the equality
REFERENCES,0.8643067846607669,"P1←n
P1→n
=
1 + l1"
REFERENCES,0.8672566371681416,r1 + · · · + l1
REFERENCES,0.8702064896755162,r1 . . . ln
REFERENCES,0.8731563421828908,"rn
1 + rn"
REFERENCES,0.8761061946902655,ln + · · · + rn
REFERENCES,0.8790560471976401,"ln . . . r1 l1
= n
Y i=1"
REFERENCES,0.8820058997050148,"li
ri
,
(32)"
REFERENCES,0.8849557522123894,to ﬁnally obtain the formula
REFERENCES,0.887905604719764,"p(n + 1) =  1 + L R n
Y i=1 li
ri !−1"
REFERENCES,0.8908554572271387,".
(33)"
REFERENCES,0.8938053097345132,"D.3
CHAIN OF NECKLACES"
REFERENCES,0.8967551622418879,"To compute the gain of the necklace policy, the idea is to show that necklaces can be arranged on a
chain so that we can use (33) (see Figure 3)."
REFERENCES,0.8997050147492626,"For m prime, the number of non trivial necklaces is (2m −2)/m, the trivial necklaces being the two
ﬁnal states (i.e., the words AAA..A and BBB..B). Using the conjecture of Degni & Drisko (2007),
there is a Gray order on these necklaces when m is prime. In other words, there is a chain from
one ﬁnal state to the other that passes exactly once by each necklace, the difference between two
successive necklaces being exactly one bit (i.e. a single A is changed into B or vice versa)."
REFERENCES,0.9026548672566371,"In any case (m prime or not), we call n(m) the length of the longest chain with Gray order. We call
y(m) the length of the smallest necklace in that longest chain (arguably the smallest prime factor of
m)."
REFERENCES,0.9056047197640118,"A necklace is characterized by the numbers a and b of letters A and B in the m-long word, with
a + b = m. After at least one complete loop in the necklace (i.e. at least y(m) actions), the
probabilities to leave that necklace (when we are at the exit states) are"
REFERENCES,0.9085545722713865,"li = ϵ1ka
A(1 −kB)b,
(34)"
REFERENCES,0.911504424778761,"ri = ϵ1(1 −kA)akb
B.
(35)"
REFERENCES,0.9144542772861357,With the odds to do at least one loop increasing as ϵ1 goes to zero or y(m) goes to ∞.
REFERENCES,0.9174041297935103,"In the two ﬁnal states, and again after one loop, the probabilities to leave are ϵ0L = ϵ0(1 −kB)m
and ϵ0R = ϵ0(1 −kA)m. We can now use the formula (33) (when ϵ0 ≪ϵ1 ≪1), in which the"
REFERENCES,0.9203539823008849,Under review as a conference paper at ICLR 2022
REFERENCES,0.9233038348082596,"product simpliﬁes as n
Y i=1"
REFERENCES,0.9262536873156342,"li
ri
="
REFERENCES,0.9292035398230089,"n(m)−2
Y i=1"
REFERENCES,0.9321533923303835,"kai
A (1 −kB)bi"
REFERENCES,0.9351032448377581,"(1 −kA)aikbi
B
(36)"
REFERENCES,0.9380530973451328,"=
Qn(m)−2
i=1
(1/kB −1)bi
Qn(m)−2
i=1
(1/kA −1)ai
(37)"
REFERENCES,0.9410029498525073,"= (1/kB −1)
P i bi"
REFERENCES,0.943952802359882,"(1/kA −1)
P"
REFERENCES,0.9469026548672567,"i ai
(38)"
REFERENCES,0.9498525073746312,"=
1/kB −1"
REFERENCES,0.9528023598820059,1/kA −1
REFERENCES,0.9557522123893806,"m(n(m)−2)/2
since"
REFERENCES,0.9587020648967551,"n(m)−2
X"
REFERENCES,0.9616519174041298,"i=1
(ai + bi) = m(n(m) −2)
(39)"
REFERENCES,0.9646017699115044,where ai and bi are the occurrences of A and B in the necklace i.
REFERENCES,0.967551622418879,Inserting (39) into (33) and using the value of kA and kB given in (4) for hypothesis HA leads to
REFERENCES,0.9705014749262537,"p(n + 1) =

1 + αm(1−n(m))−1
,
with α = 1 −µ"
REFERENCES,0.9734513274336283,"1 + µ.
(40)"
REFERENCES,0.976401179941003,"and p(0) can be obtained by changing µ into −µ or α into 1/α, by symmetry of the necklace policy.
The probabilities under hypothesis HB are obtained by exchanging p(0) and p(n + 1), again by
symmetry."
REFERENCES,0.9793510324483776,"Under hypothesis HA (resp. HB), the value p(n + 1) (resp. p(0)) corresponds to the probability q∗
to play the worst arm if the probabilities of the non-ﬁnal necklaces are zero (which is asymptotically
true if ϵ0 ≪ϵ1). To reach q∗, we also need ϵ1 to be asymptotically small in order to guarantee at
least one loop in each necklace and ϵ0 has to be asymptotically larger than the reset r. In summary,
we need r ≪ϵ0 ≪ϵ1 ≪1 to reach asymptotically q∗, otherwise the probability q will be larger
than q∗."
REFERENCES,0.9823008849557522,"D.4
COLUMN OF CONFIDENCE POLICY WITH NO RESET"
REFERENCES,0.9852507374631269,"When there is no reset r = 0 we can compute the performance of the column of conﬁdence policy
for two arms of probabilities kA and kB. We obtain via (33) (assuming kA > kB)"
REFERENCES,0.9882005899705014,q−1 −1 = 1 −kB 1 −kA
REFERENCES,0.9911504424778761,1/kB −1
REFERENCES,0.9941002949852508,1/kA −1
REFERENCES,0.9970501474926253,"M−1
(41)"
