Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0020408163265306124,"The problem of constrained reinforcement learning (CRL) holds signiﬁcant impor-
1"
ABSTRACT,0.004081632653061225,"tance as it provides a framework for addressing critical safety satisfaction concerns
2"
ABSTRACT,0.006122448979591836,"in the ﬁeld of reinforcement learning (RL). However, with the introduction of
3"
ABSTRACT,0.00816326530612245,"constraint satisfaction, the current CRL methods necessitate the utilization of
4"
ABSTRACT,0.01020408163265306,"second-order optimization or primal-dual frameworks with additional Lagrangian
5"
ABSTRACT,0.012244897959183673,"multipliers, resulting in increased complexity and inefﬁciency during implementa-
6"
ABSTRACT,0.014285714285714285,"tion. To address these issues, we propose a novel ﬁrst-order feasible method named
7"
ABSTRACT,0.0163265306122449,"Constrained Proximal Policy Optimization (CPPO). By treating the CRL problem
8"
ABSTRACT,0.018367346938775512,"as a probabilistic inference problem, our approach integrates the Expectation-
9"
ABSTRACT,0.02040816326530612,"Maximization framework to solve it through two steps: 1) calculating the optimal
10"
ABSTRACT,0.022448979591836733,"policy distribution within the feasible region (E-step), and 2) conducting a ﬁrst-
11"
ABSTRACT,0.024489795918367346,"order update to adjust the current policy towards the optimal policy obtained in the
12"
ABSTRACT,0.026530612244897958,"E-step (M-step). We establish the relationship between the probability ratios and
13"
ABSTRACT,0.02857142857142857,"KL divergence to convert the E-step into a convex optimization problem. Further-
14"
ABSTRACT,0.030612244897959183,"more, we develop an iterative heuristic algorithm from a geometric perspective to
15"
ABSTRACT,0.0326530612244898,"solve this problem. Additionally, we introduce a conservative update mechanism to
16"
ABSTRACT,0.03469387755102041,"overcome the constraint violation issue that occurs in the existing feasible region
17"
ABSTRACT,0.036734693877551024,"method. Empirical evaluations conducted in complex and uncertain environments
18"
ABSTRACT,0.03877551020408163,"validate the effectiveness of our proposed method, as it performs at least as well as
19"
ABSTRACT,0.04081632653061224,"other baselines.
20"
INTRODUCTION,0.04285714285714286,"1
Introduction
21"
INTRODUCTION,0.044897959183673466,"In recent years, reinforcement learning (RL) has achieved huge success in various aspects (Le
22"
INTRODUCTION,0.04693877551020408,"et al., 2022; Li et al., 2022; Silver et al., 2018), especially in the ﬁeld of games. However, due
23"
INTRODUCTION,0.04897959183673469,"to the increased safety requirements in practice, researchers are starting to consider the constraint
24"
INTRODUCTION,0.05102040816326531,"satisfaction in RL. Compared with unconstrained RL, constrained RL (CRL) incorporates certain
25"
INTRODUCTION,0.053061224489795916,"constraints during the process of maximizing cumulated rewards, which provides a framework to
26"
INTRODUCTION,0.05510204081632653,"model several important topics in RL, such as safe RL (Paternain et al., 2022), highlighting the
27"
INTRODUCTION,0.05714285714285714,"importance of this problem in industrial applications.
28"
INTRODUCTION,0.05918367346938776,"The current methods for solving the CRL problem can be mainly classiﬁed into two categories:
29"
INTRODUCTION,0.061224489795918366,"primal-dual method (Paternain et al., 2022; Stooke et al., 2020; Zhang et al., 2020; Altman, 1999) and
30"
INTRODUCTION,0.06326530612244897,"feasible region method (Achiam et al., 2017; Yang et al., 2020). The primal-dual method introduces
31"
INTRODUCTION,0.0653061224489796,"the Lagrangian multiplier to convert the constrained optimization problem into an unconstrained dual
32"
INTRODUCTION,0.0673469387755102,"problem by penalizing the infeasible behaviours, promising the CRL problem to be resolved in a
33"
INTRODUCTION,0.06938775510204082,"ﬁrst-order manner. Despite the primal-dual framework providing a way to solve CRL in ﬁrst-order
34"
INTRODUCTION,0.07142857142857142,"manner, the update of the dual variable, i.e., the Lagrangian multiplier, tends to be slow and unstable,
35"
INTRODUCTION,0.07346938775510205,"affecting the overall convergent speed of the algorithms. In contrast, the feasible region method
36"
INTRODUCTION,0.07551020408163266,"provides a faster learning method by introducing the concept of the feasible region into the trust
37"
INTRODUCTION,0.07755102040816327,"region method. With either searching in the feasible region (Achiam et al., 2017) or projecting into
38"
INTRODUCTION,0.07959183673469387,"the feasible region (Yang et al., 2020), the feasible region method can guarantee the generated policies
39"
INTRODUCTION,0.08163265306122448,"stay in the feasible region. However, the introduction of the feasible region in the proposed method
40"
INTRODUCTION,0.0836734693877551,"relies on computationally expensive second-order optimization using the inverse Fisher information
41"
INTRODUCTION,0.08571428571428572,"matrix. This approach can lead to inaccurate estimations of the feasible region and potential constraint
42"
INTRODUCTION,0.08775510204081632,"violations, as reported in previous studies (Ray et al., 2019).
43"
INTRODUCTION,0.08979591836734693,"To address the existing issues mentioned above, this paper proposed the Constrained Proximal
44"
INTRODUCTION,0.09183673469387756,"Policy Optimization (CPPO) algorithm to solve the CRL problem in a ﬁrst-order, easy-to-implement
45"
INTRODUCTION,0.09387755102040816,"way. CPPO employs a two-step Expectation-Maximization approach to solve the problem by ﬁrstly
46"
INTRODUCTION,0.09591836734693877,"calculating the optimal policy (E-step) and then conducting a ﬁrst-order update to reduce the distance
47"
INTRODUCTION,0.09795918367346938,"between the current policy and the optimal policy (M-step), eliminating the usage of the Lagrangian
48"
INTRODUCTION,0.1,"multiplier and the second-order optimization. The main contributions of this work are summarized as
49"
INTRODUCTION,0.10204081632653061,"follows:
50"
INTRODUCTION,0.10408163265306122,"• To our best knowledge, the proposed method is the ﬁrst ﬁrst-order feasible region method
51"
INTRODUCTION,0.10612244897959183,"without using dual variables or second-order optimization, which signiﬁcantly reduces the
52"
INTRODUCTION,0.10816326530612246,"difﬁculties in tuning hyperparameters and the computing complexity.
53"
INTRODUCTION,0.11020408163265306,"• An Expectation-Maximization (EM) framework based on advantage value and proba-
54"
INTRODUCTION,0.11224489795918367,"bility ratio is proposed for solving the CRL problem efﬁciently. By converting the CRL
55"
INTRODUCTION,0.11428571428571428,"problem into a probabilistic inference problem, the CRL problem can solved in ﬁrst order
56"
INTRODUCTION,0.11632653061224489,"manner without dual variables.
57"
INTRODUCTION,0.11836734693877551,"• To solve the convex optimization problem in E-step, we established the relationship between
58"
INTRODUCTION,0.12040816326530612,"the probability ratios and KL divergence, and developed an iterative heuristic algorithm
59"
INTRODUCTION,0.12244897959183673,"from a geometric perspective.
60"
INTRODUCTION,0.12448979591836734,"• A recovery update is developed when the current policy encounters constraint violation. In-
61"
INTRODUCTION,0.12653061224489795,"spired by Bang-bang control, this update strategy can improve the performance of constraint
62"
INTRODUCTION,0.12857142857142856,"satisfaction and reduce the switch frequency between normal update and recovery update.
63"
INTRODUCTION,0.1306122448979592,"• The proposed method is evaluated in several benchmark environments. The results manifest
64"
INTRODUCTION,0.1326530612244898,"its comparable performance over other baselines in complex environments.
65"
INTRODUCTION,0.1346938775510204,"This paper is organized as follows. Section 2 introduces the concept of constrained markov decision
66"
INTRODUCTION,0.13673469387755102,"process and present an overview of related works in the ﬁeld. The Expectation-Maximization
67"
INTRODUCTION,0.13877551020408163,"framework and the technical details about the proposed constrained proximal policy optimization
68"
INTRODUCTION,0.14081632653061224,"method are proposed in Section 3. Section 4 veriﬁes the effectiveness of the proposed method through
69"
INTRODUCTION,0.14285714285714285,"several testing scenarios and an ablation study is conducted to show the effectiveness of the proposed
70"
INTRODUCTION,0.14489795918367346,"recovery update. Section 5 states the limitations and the boarder impact of the proposed method.
71"
INTRODUCTION,0.1469387755102041,"Finally, a conclusion is drawn in Section 6.
72"
PRELIMINARY AND RELATED WORK,0.1489795918367347,"2
Preliminary and Related Work
73"
CONSTRAINED MARKOV DECISION PROCESS,0.1510204081632653,"2.1
Constrained Markov Decision Process
74"
CONSTRAINED MARKOV DECISION PROCESS,0.15306122448979592,"Constrained Markov Decision Process(CMDP) is a mathematical framework for modelling decision-
75"
CONSTRAINED MARKOV DECISION PROCESS,0.15510204081632653,"making problems subjected to a set of cost constraints. A CMDP can be deﬁned by a tuple
76"
CONSTRAINED MARKOV DECISION PROCESS,0.15714285714285714,"(S, A, P, r, γ, µ, C), where S is the state space, A is the action space, P : S × A × S →(0, 1)
77"
CONSTRAINED MARKOV DECISION PROCESS,0.15918367346938775,"is the transition kernel, r : S × A →R is the reward function, γ →(0, 1) is the discount factor,
78"
CONSTRAINED MARKOV DECISION PROCESS,0.16122448979591836,"µ : S →(0, 1) is the initial state distribution, and C := {ci ∈C | ci : S × A →R, i = 1, 2, . . . , m}
79"
CONSTRAINED MARKOV DECISION PROCESS,0.16326530612244897,"is the set of m cost functions. For simplicity, we only consider a CRL problem with one constraint in
80"
CONSTRAINED MARKOV DECISION PROCESS,0.1653061224489796,"the following paper and use c to represent the cost function. Note that, although we restrict our discus-
81"
CONSTRAINED MARKOV DECISION PROCESS,0.1673469387755102,"sion to the case with only one constraint, the method proposed in this paper can be naturally extended
82"
CONSTRAINED MARKOV DECISION PROCESS,0.16938775510204082,"to the multiple constraint case. However, the result may not as elegant as the one constraint case.
83"
CONSTRAINED MARKOV DECISION PROCESS,0.17142857142857143,"Compared with the common Markov Decision Process(MDP), CMDP introduces a constraint on the
84"
CONSTRAINED MARKOV DECISION PROCESS,0.17346938775510204,"cumulated cost to restrict the agent’s policies. Considering a policy π(s | a) : S×A →(0, 1), the goal
85"
CONSTRAINED MARKOV DECISION PROCESS,0.17551020408163265,"of MDP is to ﬁnd the π that maximizes the expected discounted returns Jr(π) = Eτ [P∞
t=0 γtr(st)],
86"
CONSTRAINED MARKOV DECISION PROCESS,0.17755102040816326,"where τ is the trajectories generated based on π. Based on these settings, CMDP applied a threshold
87"
CONSTRAINED MARKOV DECISION PROCESS,0.17959183673469387,"d on the expected discounted cost returns Jc(π) = Eτ [P∞
t=0 γtc(st)]. Thus, the CMDP problem
88"
CONSTRAINED MARKOV DECISION PROCESS,0.1816326530612245,"can be formed as ﬁnding policy π∗that π∗= argmaxπ Jr(π)
s.t.
Jc(π∗) ≤d. The advan-
89"
CONSTRAINED MARKOV DECISION PROCESS,0.1836734693877551,"tage function A and the cost advantage function Ac is deﬁned as A(st, at) = Q(st, at) −V (st)
90"
CONSTRAINED MARKOV DECISION PROCESS,0.18571428571428572,"and Ac(st, at) = Qc(st, at) −Vc(st) where Q(st, at) = Eτ [P∞
t=0 γtr | s0 = st, a0 = at] and
91"
CONSTRAINED MARKOV DECISION PROCESS,0.18775510204081633,"V (st) = Eτ [P∞
t=0 γtr | s0 = st] are the corresponding Q-value and V-value for reward function,
92"
CONSTRAINED MARKOV DECISION PROCESS,0.18979591836734694,"and Qc(st, at) = Eτ [P∞
t=0 γtc | s0 = st, a0 = at] and Vc(st) = Eτ [P∞
t=0 γtc | s0 = st] are the
93"
CONSTRAINED MARKOV DECISION PROCESS,0.19183673469387755,"corresponding Q-value and V-value for cost function. Note that both A and Ac in the batch are
94"
CONSTRAINED MARKOV DECISION PROCESS,0.19387755102040816,"centered to moves theirs mean to 0, respectively.
95"
RELATED WORK,0.19591836734693877,"2.2
Related Work
96"
RELATED WORK,0.19795918367346937,"2.2.1
Proximal Policy Optimization (PPO)
97"
RELATED WORK,0.2,"Proximal policy optimization (PPO) (Schulman et al., 2017) is a renowned on-policy RL algorithm for
98"
RELATED WORK,0.20204081632653062,"its stable performance and easy implementation. Based on the ﬁrst-order optimization methodology,
99"
RELATED WORK,0.20408163265306123,"PPO addresses the challenge of the unconstrained RL problem through the surrogate objective
100"
RELATED WORK,0.20612244897959184,"function that proposed in Trust Region Policy Optimization (TRPO) (Schulman et al., 2015a). With
101"
RELATED WORK,0.20816326530612245,"the clipping and early stop trick, PPO can keep the new policy to stay within the trust region. Thanks
102"
RELATED WORK,0.21020408163265306,"to its stability and superior performance, the PPO algorithm has been employed in various subﬁelds
103"
RELATED WORK,0.21224489795918366,"of RL like multi-agent RL (Yu et al., 2021), Meta-RL (Yu et al., 2020). However, due to the extra
104"
RELATED WORK,0.21428571428571427,"constraint requirements, the direct application of PPO in CRL problems is not feasible. The extra
105"
RELATED WORK,0.2163265306122449,"constraint requirements cause PPO not only restricted by the trust region but also the constraint
106"
RELATED WORK,0.21836734693877552,"feasible region, which signiﬁcantly increases the challenge in conducting ﬁrst-order optimization.
107"
RELATED WORK,0.22040816326530613,"Despite the difﬁculties in the direct application of PPO in CRL, researchers are still searching for a
108"
RELATED WORK,0.22244897959183674,"PPO-like method to solve CRL problems with stable and superior performance.
109"
CONSTRAINED REINFORCEMENT LEARNING,0.22448979591836735,"2.2.2
Constrained Reinforcement Learning
110"
CONSTRAINED REINFORCEMENT LEARNING,0.22653061224489796,"The current methods for solving the CRL problem can be mainly divided into two categories: primal-
111"
CONSTRAINED REINFORCEMENT LEARNING,0.22857142857142856,"dual method (Paternain et al., 2022; Stooke et al., 2020; Zhang et al., 2020) and feasible region
112"
CONSTRAINED REINFORCEMENT LEARNING,0.23061224489795917,"method (Achiam et al., 2017; Yang et al., 2020). The primal-dual method converts the original
113"
CONSTRAINED REINFORCEMENT LEARNING,0.23265306122448978,"problem into a convex dual problem by introducing the Lagrangian multiplier. By updating the policy
114"
CONSTRAINED REINFORCEMENT LEARNING,0.23469387755102042,"parameters and Lagrangian multiplier iteratively, the policies obtained by the primal-dual method
115"
CONSTRAINED REINFORCEMENT LEARNING,0.23673469387755103,"will gradually converge towards a feasible solution. However, the usage of the Lagrange multiplier
116"
CONSTRAINED REINFORCEMENT LEARNING,0.23877551020408164,"introduces extra hyperparameters into the algorithm and slows down the convergence speed of the
117"
CONSTRAINED REINFORCEMENT LEARNING,0.24081632653061225,"algorithm due to the characteristic of the integral controller. Stooke et al. (2020) tries to solve this
118"
CONSTRAINED REINFORCEMENT LEARNING,0.24285714285714285,"issue by introducing PID control into the update of the Lagrangian multiplier, but this modiﬁcation
119"
CONSTRAINED REINFORCEMENT LEARNING,0.24489795918367346,"will introduce more hyperparameters and cause the algorithm to be complex. Different from the
120"
CONSTRAINED REINFORCEMENT LEARNING,0.24693877551020407,"primal-dual method, the feasible region method estimates the feasible region within the trust region
121"
CONSTRAINED REINFORCEMENT LEARNING,0.24897959183673468,"using linear approximation and subsequently determines the new policy based on the estimated
122"
CONSTRAINED REINFORCEMENT LEARNING,0.2510204081632653,"feasible region. A representative method is constrained policy optimization (CPO). By converting
123"
CONSTRAINED REINFORCEMENT LEARNING,0.2530612244897959,"the CRL to a quadratically constrained linear program, CPO (Achiam et al., 2017) can solve the
124"
CONSTRAINED REINFORCEMENT LEARNING,0.25510204081632654,"problem efﬁciently. However, the uncertainties inside the environment may cause an inaccurate cost
125"
CONSTRAINED REINFORCEMENT LEARNING,0.2571428571428571,"assessment, which will affect the estimation of the feasible region and cause the learned policy to fail
126"
CONSTRAINED REINFORCEMENT LEARNING,0.25918367346938775,"to meet the constraint requirements, as shown in Ray et al. (2019). Another issue of CPO is that it
127"
CONSTRAINED REINFORCEMENT LEARNING,0.2612244897959184,"uses the Fisher information matrix to estimate the KL divergence in quadratic approximation, which
128"
CONSTRAINED REINFORCEMENT LEARNING,0.26326530612244897,"is complex in computing and inﬂexible in network structure.
129"
CONSTRAINED REINFORCEMENT LEARNING,0.2653061224489796,"To address the second-order issue in CRL, several researchers (Zhang et al., 2020; Liu et al., 2022)
130"
CONSTRAINED REINFORCEMENT LEARNING,0.2673469387755102,"proposed the EM-based algorithm in a ﬁrst-order manner. FOCOPS (Zhang et al., 2020) obtain the
131"
CONSTRAINED REINFORCEMENT LEARNING,0.2693877551020408,"optimal policy from advantage value, akin to the maximum entropy RL, and perform a ﬁrst-order
132"
CONSTRAINED REINFORCEMENT LEARNING,0.2714285714285714,"update to reduce the KL divergence between the current policy and the optimal policy. Despite its
133"
CONSTRAINED REINFORCEMENT LEARNING,0.27346938775510204,"signiﬁcant improvement in performance compared to CPO, FOCOPS still necessitates the use of a
134"
CONSTRAINED REINFORCEMENT LEARNING,0.2755102040816326,"primal-dual method to attain a feasible optimal policy, which introduces a lot of hyperparameters for
135"
CONSTRAINED REINFORCEMENT LEARNING,0.27755102040816326,"tuning, resulting in a more complex tuning process. CVPO (Liu et al., 2022) extends the maximum
136"
CONSTRAINED REINFORCEMENT LEARNING,0.2795918367346939,"a posteriori policy optimization (MPO) (Abdolmaleki et al., 2018) method to the CRL problem,
137"
CONSTRAINED REINFORCEMENT LEARNING,0.2816326530612245,"allowing for the efﬁcient calculation of the optimal policy from Q value in an off-policy manner.
138"
CONSTRAINED REINFORCEMENT LEARNING,0.2836734693877551,"However, this algorithm still requires the primal-dual framework in optimal policy calculation and
139"
CONSTRAINED REINFORCEMENT LEARNING,0.2857142857142857,"necessitates additional samplings during the training, increasing the complexity of implementation.
140"
CONSTRAINED REINFORCEMENT LEARNING,0.28775510204081634,"Thus, the development of a simple-to-implement, ﬁrst-order algorithm with superior performance,
141"
CONSTRAINED REINFORCEMENT LEARNING,0.2897959183673469,"remains a foremost goal for researchers in the CRL subﬁeld.
142"
CONSTRAINED REINFORCEMENT LEARNING,0.29183673469387755,"3
Constrained Proximal Policy Optimization (CPPO)
143"
CONSTRAINED REINFORCEMENT LEARNING,0.2938775510204082,"As mentioned in Section 2, existing CRL methods often require second-order optimization for
144"
CONSTRAINED REINFORCEMENT LEARNING,0.29591836734693877,"feasible region estimation or the use of dual variables for cost satisfaction. These approaches can be
145"
CONSTRAINED REINFORCEMENT LEARNING,0.2979591836734694,"computationally expensive or result in slow convergence. To address these challenges, we proposed a
146"
CONSTRAINED REINFORCEMENT LEARNING,0.3,"two-step approach in an EM fashion named Constrained Proximal Policy Optimization (CPPO), the
147"
CONSTRAINED REINFORCEMENT LEARNING,0.3020408163265306,"details will be shown in this section.
148"
MODELLING CRL AS INFERENCE,0.3040816326530612,"3.1
Modelling CRL as Inference
149"
MODELLING CRL AS INFERENCE,0.30612244897959184,"Instead of directly pursuing an optimal policy to maximize rewards, our approach involves concep-
150"
MODELLING CRL AS INFERENCE,0.3081632653061224,"tualizing the problem of Constrained Reinforcement Learning (CRL) as a probabilistic inference
151"
MODELLING CRL AS INFERENCE,0.31020408163265306,"problem. This is achieved by assessing the reward performance and constraint satisfaction of state-
152"
MODELLING CRL AS INFERENCE,0.3122448979591837,"action pairs and subsequently increasing the likelihood of those pairs that demonstrate superior
153"
MODELLING CRL AS INFERENCE,0.3142857142857143,"reward performance while adhering to the constraint requirement. Suppose the event of state-action
154"
MODELLING CRL AS INFERENCE,0.3163265306122449,"pairs under policy πθ can maximize reward is represented by optimality variable O, we assume
155"
MODELLING CRL AS INFERENCE,0.3183673469387755,"the likelihood of state-action pairs being optimal is proportional to the exponential of its advantage
156"
MODELLING CRL AS INFERENCE,0.32040816326530613,"value: p(O = 1|(s, a)) ∝exp(A(s, a)/α) where α is a temperature parameter. Denote q(a | s)
157"
MODELLING CRL AS INFERENCE,0.3224489795918367,"is the feasible posterior distribution estimated from the sampled trajectories under current policy
158"
MODELLING CRL AS INFERENCE,0.32448979591836735,"π, pπ(a | s) is the probability distribution under policy π, and θ is the policy parameters. We can
159"
MODELLING CRL AS INFERENCE,0.32653061224489793,"have following evidence lower bound(ELBO) J (q, θ) using surrogate function(see Appendix B for
160"
MODELLING CRL AS INFERENCE,0.32857142857142857,"detailed proof)
161"
MODELLING CRL AS INFERENCE,0.3306122448979592,"log pπθ(O = 1) ≥Es∼dπ,a∼π"
MODELLING CRL AS INFERENCE,0.3326530612244898, q(a|s)
MODELLING CRL AS INFERENCE,0.3346938775510204,"pπ(a|s)A(s, a)

−αDKL(q ∥πθ) + log p(θ) = J (q, θ),
(1)"
MODELLING CRL AS INFERENCE,0.336734693877551,"where dπ is the state distribution under current policy π, p(θ) is a prior distribution of policy
162"
MODELLING CRL AS INFERENCE,0.33877551020408164,"parameters. Considering q(a | s) is a feasible policy distribution, we also have following constraint
163"
MODELLING CRL AS INFERENCE,0.3408163265306122,"(Achiam et al., 2017)
164"
MODELLING CRL AS INFERENCE,0.34285714285714286,"Jc(π) +
1
1 −γ Es∼dπ,a∼π"
MODELLING CRL AS INFERENCE,0.3448979591836735, q(a|s)
MODELLING CRL AS INFERENCE,0.3469387755102041,"pπ(a|s)Ac(s, a)

≤d,
(2)"
MODELLING CRL AS INFERENCE,0.3489795918367347,"where d is the cost constraint. By performing iterative optimization of the feasible posterior distri-
165"
MODELLING CRL AS INFERENCE,0.3510204081632653,"bution q (E-step) and the policy parameter θ (M-step), the lower bound J (q, θ) can be increased,
166"
MODELLING CRL AS INFERENCE,0.35306122448979593,"resulting in an enhancement in the likelihood of state-action pairs that have the potential to maximize
167"
MODELLING CRL AS INFERENCE,0.3551020408163265,"rewards.
168"
E-STEP,0.35714285714285715,"3.2
E-Step
169"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.35918367346938773,"3.2.1
Surrogate Constrained Policy Optimization
170"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.36122448979591837,"As mentioned in the previous section, we will ﬁrstly optimize the feasible posterior distribution q to
171"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.363265306122449,"maximize ELBO in E-step. The feasible posterior distribution q plays a crucial role in determining
172"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.3653061224489796,"the upper bound of the ELBO since the KL divergence is non-negative. Consequently, q needs to be
173"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.3673469387755102,"theoretically optimal to maximize the ELBO. By converting the soft KL constraint in Equation (1)
174"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.3693877551020408,"into a hard constraint and combining the cost constraint in Equation (2),the optimization problem of
175"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.37142857142857144,"q can be expressed as follows:
176"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.373469387755102,"maximize
q
Es∼dπ,a∼π"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.37551020408163266, q(a|s)
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.37755102040816324,"pπ(a|s)A(s, a)
"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.3795918367346939,"s.t.
Jc(π) +
1
1 −γ Es∼dπ,a∼π"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.3816326530612245, q(a|s)
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.3836734693877551,"pπ(a|s)Ac(s, a)

≤d, DKL(q ∥π) ≤δ,
(3)"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.38571428571428573,"where δ is the reverse KL divergence constraint that determine the trust region. During the E-step, it is
177"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.3877551020408163,"important to note that the optimization is independent of θ, meaning that the policy πθ remains ﬁxed
178"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.38979591836734695,"to the current sampled policy π. Even we know the closed-form expression of pπθ, it is impractical to
179"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.39183673469387753,"solve the closed-form expression of q from Equation (3), as we still needs the closed-form expression
180"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.39387755102040817,"of dπ for calculating. Therefore, we we opt to represent the solution of q in a non-parametric manner
181"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.39591836734693875,"by calculating the probability ratio v =
q(a|s)
pπ(a|s) for the sampled state-action pairs, allowing us to
182"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.3979591836734694,"avoid explicitly parameterizing q and instead leverage the probability ratio to guide the optimization
183"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4,"process. After relaxing the reverse KL divergence constraint with the estimated reverse KL divergence
184"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4020408163265306,"calculated through importance sampling, we can obtain
185"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.40408163265306124,"maximize
v
Es∼dπ,a∼π [vA(s, a)]"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4061224489795918,"s.t.
Es∼dπ,a∼π [vAc(s, a)] ≤d′"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.40816326530612246,"E
s∼dπ
a∼π
[v log v] ≤δ.
(4)"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.41020408163265304,"where d′ the scaled cost margin d′ = (1 −γ)(d −Jc(π)). Although Equation (4) is convex
186"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4122448979591837,"optimization problem that can be directly solved through existing convex optimization algorithm, the
187"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4142857142857143,"existence of non-polynomial KL constraint tends to cause the optimization to be computationally
188"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4163265306122449,"expensive. To overcome this issue, the following proposition is proposed to relax Equation (4) into
189"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.41836734693877553,"an linear optimization problem with quadratic constraint.
190"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4204081632653061,"Proposition 3.1. Denote v as the probability ratios
q(a|s)
pπ(a|s) calculated from sampled trajectories. If
191"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.42244897959183675,"there are a sufﬁcient number of sampled v, we have E[v] = 1 and E [v log v] ≤Var(v −1).
192"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.42448979591836733,"With Proposition 3.1, the relationship between reverse KL divergence and l2-norm of vector v −1
193"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.42653061224489797,"is constructed. Also, consider that the expectation of v equals 1, the optimization variable can be
194"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.42857142857142855,"changed from v to v −1. Let v denote the vector consists of v −1 and replace the reverse KL
195"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4306122448979592,"divergence constraint with the l2-norm constraint, Equation (4) can be rewritten in the form of vector
196"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4326530612244898,"multiplication
197"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4346938775510204,maximize
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.43673469387755104,"v
v · A"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4387755102040816,"s.t.
v · Ac ≤Nd′, ∥v∥2 ≤2Nδ′"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.44081632653061226,"E(v) = 0, v > −1 element-wise, (5)"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.44285714285714284,"where A and Ac are the advantage value vectors for reward and cost (for all sampled state-action pairs
198"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4448979591836735,"in one rollout) respectively, N is the number of state-action pair samples, δ′ is l2-norm constraint, and
199"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.44693877551020406,"the element-wise lower bound of v is −1, as v > 0. Thus, the optimal feasible posterior distribution
200"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4489795918367347,"q expressed through v can be obtained by solving the aforementioned optimization problem.
201"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.45102040816326533,"Remark 3.2. By replacing the non-polynomial KL constraint with an l2-norm constraint, the original
202"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4530612244897959,"optimization problem in Equation (4) can be reformulated as a geometric problem. This reformulation
203"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.45510204081632655,"enables the use of the proposed heuristic method to efﬁciently solve the problem without the need
204"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.45714285714285713,"for dual variables.
205"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.45918367346938777,"Remark 3.3. Our proposed method builds upon the idea presented in CVPO (Liu et al., 2022) of
206"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.46122448979591835,"treating the CRL problem as a probabilistic inference problem. However, our approach improves
207"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.463265306122449,"upon their idea in two signiﬁcant ways. Firstly, the probabilistic inference problem in our method is
208"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.46530612244897956,"constructed based on advantage value, which is more effective in reducing the bias in estimating the
209"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4673469387755102,"cost return, compared to the Q-value used in CVPO. Secondly, while CVPO tries to directly calculate
210"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.46938775510204084,"the value of q(a|s), our method employs the probability ratio v to represent q. By replacing q(a|s)
211"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.4714285714285714,"with v, our method only needs to ﬁnd a vector of v whose elements are positive and E[v] = 1, thereby
212"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.47346938775510206,"negating the need to sample multiple actions in one state to calculate the extra normalizer that ensures
213"
SURROGATE CONSTRAINED POLICY OPTIMIZATION,0.47551020408163264,"q is a valid distribution. This results in a signiﬁcant reduction in computational complexity.
214"
RECOVERY UPDATE,0.4775510204081633,"3.2.2
Recovery update
215"
RECOVERY UPDATE,0.47959183673469385,"Although the optimal solution q in Section 3.2.1 is applicable when the current policy is out of
216"
RECOVERY UPDATE,0.4816326530612245,"the feasible region, the inconsistent between optimal q and πθ and the inaccurate cost evaluations
217"
RECOVERY UPDATE,0.48367346938775513,"tends to result in the generation of infeasible policies, as demonstrated in Ray et al. (2019) where
218"
RECOVERY UPDATE,0.4857142857142857,"CPO fail to satisfy constraint. To overcome this issue, a recovery update strategy is proposed for
219"
RECOVERY UPDATE,0.48775510204081635,"pushing the agent back to the feasible region. This strategy aims to minimize costs while preserving
220"
RECOVERY UPDATE,0.4897959183673469,"or minimizing any reduction in overall reward return. In the event that it is not possible to recover
221"
RECOVERY UPDATE,0.49183673469387756,"from the infeasible region without compromising the reward return, the strategy aims to identify an
222"
RECOVERY UPDATE,0.49387755102040815,"optimal policy within the feasible region that minimizes the adverse impact on the reward return. The
223"
RECOVERY UPDATE,0.4959183673469388,"optimization problem in recovery update can be expressed as
224"
RECOVERY UPDATE,0.49795918367346936,"if v · A ≥0 not exists when v · Ac ≤Nd′:
maximize"
RECOVERY UPDATE,0.5,"v
v · A"
RECOVERY UPDATE,0.5020408163265306,"else:
minimize"
RECOVERY UPDATE,0.5040816326530613,"v
v · Ac"
RECOVERY UPDATE,0.5061224489795918,"s.t.
∥v∥2 ≤2Nδ′, E(v) = 0, v > −1 element-wise. (6)"
RECOVERY UPDATE,0.5081632653061224,"Figure 1 illustrates the recovery update strategy from the perspective of geometry. The blue, red,
225"
RECOVERY UPDATE,0.5102040816326531,"and yellow arrows represent the direction of minimizing cost, maximizing reward and the recovery
226"
RECOVERY UPDATE,0.5122448979591837,"update, respectively. The reward preservation region is deﬁned by the zero reward boundary, which is
227"
RECOVERY UPDATE,0.5142857142857142,"depicted as the dashed line perpendicular to the red arrow. As a result, the semi-circle encompassing
228"
RECOVERY UPDATE,0.5163265306122449,"the red arrow indicates a positive increment in reward. Case 1 and Case 3 illustrate the case when the
229"
RECOVERY UPDATE,0.5183673469387755,"reward preservation region has an intersection with the feasible region. In these cases, we choose
230"
RECOVERY UPDATE,0.5204081632653061,"the direction of minimizing cost within the reward preservation region, e.g., the recovery update
231"
RECOVERY UPDATE,0.5224489795918368,"direction is coincident with the dashed line in Case 1, and the recovery update direction is coincident
232"
RECOVERY UPDATE,0.5244897959183673,"with the blue arrow in Case 3. Case 2 shows the case when there is no intersection between the
233"
RECOVERY UPDATE,0.5265306122448979,"reward preservation region and the feasible region. In this case, the direction with the least damage
234"
RECOVERY UPDATE,0.5285714285714286,"to reward is chosen. If we use an angle α to represent the direction of update, then we can have
235"
RECOVERY UPDATE,0.5306122448979592,"α = Clip(α, max(θf, θA + π/2), π), where θA represents the direction of A, θf is the minimum
236"
RECOVERY UPDATE,0.5326530612244897,"angle that can point toward the feasible region.
237"
RECOVERY UPDATE,0.5346938775510204,Infeasible region A -Ac A -Ac
RECOVERY UPDATE,0.536734693877551,Update direction A -Ac
RECOVERY UPDATE,0.5387755102040817,"Feasible region
Zero reward boundary"
RECOVERY UPDATE,0.5408163265306123,"Case 1
Case 2
Case 3"
RECOVERY UPDATE,0.5428571428571428,Figure 1: The illustration of recovery update.
RECOVERY UPDATE,0.5448979591836735,"To further improve the constraint satisfaction performance, a switching mechanism inspired by bang-
238"
RECOVERY UPDATE,0.5469387755102041,"bang control (Lasalle, 1960) is introduced. As shown in Figure 2, the agent will initially conduct
239"
RECOVERY UPDATE,0.5489795918367347,"normal update in Section 3.2.1; when the agent violates the cost constraint, it will switch to recovery
240"
RECOVERY UPDATE,0.5510204081632653,"update to reduce the cost until the cost is lower than the lower switch cost. By incorporating this
241"
RECOVERY UPDATE,0.5530612244897959,"switching mechanism, a margin is created between the lower switch cost and the cost constraint.
242"
RECOVERY UPDATE,0.5551020408163265,"This margin allows for a period of normal updates before the recovery update strategy is invoked.
243"
RECOVERY UPDATE,0.5571428571428572,"As a result, this mechanism prevents frequent switching between the two strategies, leading to
244"
RECOVERY UPDATE,0.5591836734693878,"improved performance in both reward collection and cost satisfaction. This switching mechanism
245"
RECOVERY UPDATE,0.5612244897959183,"effectively balances the exploration of reward-maximizing actions with the need to maintain constraint
246"
RECOVERY UPDATE,0.563265306122449,"satisfaction.
247"
RECOVERY UPDATE,0.5653061224489796,"Cost
Cost 
constraint
Switch cost"
RECOVERY UPDATE,0.5673469387755102,Normal
RECOVERY UPDATE,0.5693877551020409,update
RECOVERY UPDATE,0.5714285714285714,Recovery
RECOVERY UPDATE,0.573469387755102,update
RECOVERY UPDATE,0.5755102040816327,"Figure 2: The switch mechanism inspired by bang-bang control. Once the current policy violates the
cost constraint, the agent will switch to recovery update until it reaches the switch cost."
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5775510204081633,"3.3
Heuristic algorithm from geometric interpretation
248"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5795918367346938,"Section 3.2 and Section 3.4 provide a framework for solving CRL problem in theory. However,
249"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5816326530612245,"solving Equation (5) and Equation (6) in Section 3.2 is a tricky task in practice. To reduce the
250"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5836734693877551,"computation complexity, an iterative heuristic algorithm is proposed to solve this optimization
251"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5857142857142857,"problem from geometric interpretation. Recall Equation (5), the l2-norm can be interpreted as a radius
252"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5877551020408164,"constraint from the geometric perspective. Additionally, both the objective function and the cost
253"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5897959183673469,"function are linear, indicating that the optimal solution lies on the boundary of the feasible region. By
254"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5918367346938775,"disregarding the element-wise bounds in Equation (5), we can consider the optimization problem as
255"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5938775510204082,"ﬁnding a optimal angle θ′ on the A-Ac plane, in accordance with Theorem 3.4. The optimal solution
256"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5959183673469388,"can be expressed as v = 2Nδ′(cos θ′ ˜Ac + sin θ′ ˜A), where ˜A and ˜Ac are the orthogonal unit vectors
257"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.5979591836734693,"of A and Ac respectively. Considering Assumption 3.5, we proposed a iterative heuristic algorithm
258"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6,"to solve Equation (5) by ﬁrstly calculating the optimal angle θ′ regardless the element-wise bound
259"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6020408163265306,"and obtain a initial solution v, then clip v according to the element-wise bound and mask the clipped
260"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6040816326530613,"value, and iteratively update the rest unmasked elements according to aforementioned steps until all
261"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6061224489795919,"elements in v are satisfy the element-wise bound. The detailed steps are outlined in Appendix C. For
262"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6081632653061224,"the recovery update in Section 3.2.2, the same algorithm can be used to ﬁnd the angle that satisfy
263"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.610204081632653,"v · Ac = Nd′ or v · A = 0.
264"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6122448979591837,"Theorem 3.4. Given a feasible optimization problem of the form:
265"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6142857142857143,maximize
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6163265306122448,"v
v · A"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6183673469387755,"s.t.
v · Ac ≤D, ∥v∥2 ≤2Nδ′"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6204081632653061,E(v) = E(A) = E(Ac) = 0
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6224489795918368,"where v, A, and Ac are N-dimensional vectors, then the optimal solution v will lie in the A-Ac
266"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6244897959183674,"plane determined by Ac and A.
267"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6265306122448979,"Assumption 3.5. If the optimization problem in Theorem 3.4 has a optimal solution vopt =
268"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6285714285714286,"[v1, v2, . . . ], and the same problem with element-wise lower bound constraint b has a optimal
269"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6306122448979592,"solution v′
opt = [v′
1, v′
2, . . . ], then v′
t = b where vt ≤b.
270"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6326530612244898,"Remark 3.6. By utilizing the proposed heuristic algorithm, the optimal solution to Equation (5) can
271"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6346938775510204,"be obtained in just a few iterations. The time complexity of each iteration is O(n), where n represents
272"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.636734693877551,"the number of unmasked elements. As a result, the computational complexity is signiﬁcantly reduced
273"
HEURISTIC ALGORITHM FROM GEOMETRIC INTERPRETATION,0.6387755102040816,"compared to conventional convex optimization methods.
274"
M-STEP,0.6408163265306123,"3.4
M-Step
275"
M-STEP,0.6428571428571429,"After determining the optimal feasible posterior distribution q to maximize the upper bound of ELBO,
276"
M-STEP,0.6448979591836734,"an M-step is implemented to maximize ELBO by updating policy parameters θ in a supervised
277"
M-STEP,0.6469387755102041,"learning manner. Recall the deﬁnition of ELBO in Equation (1) in Section 3.1, by dropping the part
278"
M-STEP,0.6489795918367347,"that independent from θ, we will obtain following optimization problem
279"
M-STEP,0.6510204081632653,"maximize
θ
−αDKL(q ∥πθ) + log p(θ).
(7)"
M-STEP,0.6530612244897959,"Note that if we assume p(θ) is a Gaussian distribution, then log p(θ) can be converted into DKL(π ∥
280"
M-STEP,0.6551020408163265,"πθ) (see Appendix B for details). Using the same trick in Section 3.2.1 to convert soft KL constraint
281"
M-STEP,0.6571428571428571,"to hard KL constraint, the supervised learning problem in M-step can be expressed as
282"
M-STEP,0.6591836734693878,"minimize
θ
DKL(q ∥πθ)"
M-STEP,0.6612244897959184,"s.t.
DKL(πθ ∥π) ≤δ,
(8)"
M-STEP,0.6632653061224489,"Note that DKL(πθ ∥π) is chosen to lower than δ so that the current policy π can be reached during
283"
M-STEP,0.6653061224489796,"the E-step in next update iteration to achieve robust update.
284"
M-STEP,0.6673469387755102,"For Equation (7), it is a common practice for researchers to directly minimize the KL divergence,
285"
M-STEP,0.6693877551020408,"like CVPO (Liu et al., 2022) and MPO (Abdolmaleki et al., 2018). However, recall Equation (6), it
286"
M-STEP,0.6714285714285714,"is evident that the value of surrogate reward and cost are deeply connected to the projection of v
287"
M-STEP,0.673469387755102,"onto the A-Ac plane, while KL divergence can hardly reﬂect this kind of relationship between v and
288"
M-STEP,0.6755102040816326,"surrogate value. Consequently, Consequently, we choose to replace the original KL objective function
289"
M-STEP,0.6775510204081633,"with the l2-norm E [∥v −pπθ/pπ∥2], where v is the optimal probability ratio obtained in E-step and
290"
M-STEP,0.6795918367346939,"pπθ/pπ is the probability ratio under policy parameter θ. With this replacement, the optimization
291"
M-STEP,0.6816326530612244,"problem can be treated as a ﬁxed-target tracking control problem. This perspective enables us to plan
292"
M-STEP,0.6836734693877551,"tracking trajectories that can consistently satisfy the cost constraint, enhancing the ability to maintain
293"
M-STEP,0.6857142857142857,"cost satisfaction throughout the learning process. The optimization problem after replacement can be
294"
M-STEP,0.6877551020408164,"rewritten as
295"
M-STEP,0.689795918367347,"minimize
θ
E"
M-STEP,0.6918367346938775,"
∥v −pπθ pπ
∥2"
M-STEP,0.6938775510204082,"
s.t.
DKL(πθ ∥π) ≤δ,
(9)"
M-STEP,0.6959183673469388,"To ensure the tracking trajectories can satisfy cost constraint at nearly all locations, we calculated the
296"
M-STEP,0.6979591836734694,"several recovery v′ under different δ′′ and guide
pπθ"
M-STEP,0.7,"pπ to different v according to the l2-norm of
pπθ"
M-STEP,0.7020408163265306,"pπ ,
297"
M-STEP,0.7040816326530612,"so that even ∥
pπθ"
M-STEP,0.7061224489795919,"pπ ∥2 is much smaller than 2Nδ′, the new policy can still satisfy the cost constraint.
298"
M-STEP,0.7081632653061225,"Moreover, inspired by the proportional navigation (Yanushevsky, 2018), we also modify the recovery
299"
M-STEP,0.710204081632653,"update gradient from (v −
pπθ"
M-STEP,0.7122448979591837,pπ ) ∂πθ
M-STEP,0.7142857142857143,"∂θ to ((β(v −
pπθ"
M-STEP,0.7163265306122449,pπ ) + (1 −β)A′c) ∂πθ
M-STEP,0.7183673469387755,"∂θ to reduce the cost during
300"
M-STEP,0.7204081632653061,"the tracking, where A′c is the projection of v −
pπθ"
M-STEP,0.7224489795918367,"pπ on cost advantage vector Ac. In according with
301"
M-STEP,0.7244897959183674,"Theorem 3.7, the lower-bound clipping mechanism similar with PPO is applied on updating
pπθ"
M-STEP,0.726530612244898,"pπ in
302"
M-STEP,0.7285714285714285,"M-step to satisfy the forward KL constraint (see Appendix C for details).
303"
M-STEP,0.7306122448979592,"Theorem 3.7. For a probability ratio vector v, if the variance of v is constant, then the upper bound
304"
M-STEP,0.7326530612244898,"of the approximated forward KL divergence DKL(πθ ∥π), will decrease as the element-wise lower
305"
M-STEP,0.7346938775510204,"bound of v increase.
306"
M-STEP,0.736734693877551,"Apart from E-step and M-step introduced in Section 3.2 and Section 3.4, our method shares the same
307"
M-STEP,0.7387755102040816,"Generalized Advantage Estimator (GAE) technique (Schulman et al., 2015b) with PPO in calculating
308"
M-STEP,0.7408163265306122,"the advantage value A and Ac. The main steps of CPPO are summarized in Appendix C.
309"
EXPERIMENT,0.7428571428571429,"4
Experiment
310"
EXPERIMENT,0.7448979591836735,"In this section, Safety Gym (Ray et al., 2019) benchmark environments and Circle environment
311"
EXPERIMENT,0.746938775510204,"(Achiam et al., 2017) are used to verify and evaluate the performance of the proposed method. Five
312"
EXPERIMENT,0.7489795918367347,"test scenarios, namely CarPush, PointGoal, PointPush, PointCircle, and AntCircle are evaluated.
313"
EXPERIMENT,0.7510204081632653,"The detailed information about the test scenarios can be seen in Appendix D. Three algorithms
314"
EXPERIMENT,0.753061224489796,"are chosen as the benchmarks to compare the learning curves and the constraint satisfaction: CPO
315"
EXPERIMENT,0.7551020408163265,"(Achiam et al., 2017), PPO-Lagrangian method (simpliﬁed as PPO_lag), and TRPO-Lagrangian
316"
EXPERIMENT,0.7571428571428571,"method (simpliﬁed as TRPO_lag) (Ray et al., 2019). CPO is chosen as the representative of the
317"
EXPERIMENT,0.7591836734693878,"feasible region method. PPO_lag and TRPO_lag are treated as the application of the primal-dual
318"
EXPERIMENT,0.7612244897959184,"method in ﬁrst-order optimization and second-order optimization. TRPO and PPO are also used in
319"
EXPERIMENT,0.763265306122449,"this section as unconstrained performance references. For a fair comparison, all of the algorithms
320"
EXPERIMENT,0.7653061224489796,"use the same policy network and critic network. The detail of the hyperparameter setting is listed in
321"
EXPERIMENT,0.7673469387755102,"Appendix E.
322 1 0 1 2 3"
EXPERIMENT,0.7693877551020408,Performance
EXPERIMENT,0.7714285714285715,carpush1 0 10 20
EXPERIMENT,0.773469387755102,pointgoal1 2.5 0.0 2.5 5.0
EXPERIMENT,0.7755102040816326,pointpush1 0 50 100
EXPERIMENT,0.7775510204081633,pointcircle 0 100
EXPERIMENT,0.7795918367346939,antcircle
EXPERIMENT,0.7816326530612245,"0.00
0.25
0.50
0.75
1.00
TotalEnvInteracts
1e7 20 40"
EXPERIMENT,0.7836734693877551,AverageEpCost
EXPERIMENT,0.7857142857142857,"0.00
0.25
0.50
0.75
1.00
TotalEnvInteracts
1e7 20 40"
EXPERIMENT,0.7877551020408163,"0.00
0.25
0.50
0.75
1.00
TotalEnvInteracts
1e7 0 20 40"
EXPERIMENT,0.789795918367347,"0.0
0.5
1.0
1.5
2.0
TotalEnvInteracts
1e5 0 5 10"
EXPERIMENT,0.7918367346938775,"0.00
0.25
0.50
0.75
1.00
TotalEnvInteracts
1e7 0 100 200 300"
EXPERIMENT,0.7938775510204081,"cppo
cpo
ppo_lag
trpo_lag
cost constraint"
EXPERIMENT,0.7959183673469388,"Figure 3: The learning curves for comparison, CPPO is the method proposed in this paper."
EXPERIMENT,0.7979591836734694,"Performance and Constraint Satisfaction: Figure 3 compares the learning curves of the proposed
323"
EXPERIMENT,0.8,"method and other benchmark algorithms in terms of the episodic return and the episodic cost. The
324"
EXPERIMENT,0.8020408163265306,"ﬁrst row records the undiscounted episodic return for performance comparison, and the second row is
325"
EXPERIMENT,0.8040816326530612,"the learning curves of the episodic cost for constraint satisfaction analysis, where the red dashed line
326"
EXPERIMENT,0.8061224489795918,"indicates the cost constraint. The learning curves for the Push and Goal environments are averaged
327"
EXPERIMENT,0.8081632653061225,"over 6 random seeds, while those for the Circle environments are averaged over 4 random seeds.
328"
EXPERIMENT,0.810204081632653,"The curve itself represents the mean value, and the shadow indicates the standard deviation. In
329"
EXPERIMENT,0.8122448979591836,"terms of performance comparison, it was observed that CPO can achieve the highest reward return
330"
EXPERIMENT,0.8142857142857143,"in PointGoal and PointCircle. The proposed CPPO method, on the other hand, achieves similar or
331"
EXPERIMENT,0.8163265306122449,"even higher reward return in the remaining test scenarios. However, when considering constraint
332"
EXPERIMENT,0.8183673469387756,"satisfaction, CPO fails to satisfy the constraint in all four tasks due to approximation errors, as
333"
EXPERIMENT,0.8204081632653061,"previously reported in Ray et al. (2019). In contrast, CPPO successfully satisﬁes the constraint
334"
EXPERIMENT,0.8224489795918367,"in all ﬁve environments, showing the effectiveness of the proposed recovery update . Referring to
335"
EXPERIMENT,0.8244897959183674,"the learning curves in Circle scenarios, it can be seen that the primal-dual based CRL methods, i.e.,
336"
EXPERIMENT,0.826530612244898,"PPO_lag and TRPO_lag, suffer from the slow and unstable update of the dual variable, causing the
337"
EXPERIMENT,0.8285714285714286,"conservative performance in PointCircle and slow cost satisfaction in AntCircle. On the other hand,
338"
EXPERIMENT,0.8306122448979592,"CPPO can achieves a faster learning speed in Circle environment by eliminating the need for the
339"
EXPERIMENT,0.8326530612244898,"dual variable. Overall, the experimental results demonstrate the effectiveness of CPPO in solving
340"
EXPERIMENT,0.8346938775510204,"the CRL problem.
341"
EXPERIMENT,0.8367346938775511,"Ablation Study: An ablation study was conducted to investigate the impact of the recovery update in
342"
EXPERIMENT,0.8387755102040816,"CPPO. Figure 4 presents the reward performance and cost satisfaction of CPPO with and without the
343"
EXPERIMENT,0.8408163265306122,"recovery update in the PointCircle environment. The results indicate that without the recovery update,
344"
EXPERIMENT,0.8428571428571429,"CPPO achieves higher reward performance; however, the cost reaches 15, which signiﬁcantly violates
345"
EXPERIMENT,0.8448979591836735,"the cost constraint. In contrast, when the recovery update is applied, CPPO successfully satisﬁes
346"
EXPERIMENT,0.8469387755102041,"the constraint, thereby demonstrating the importance of the recovery update in ensuring constraint
347"
EXPERIMENT,0.8489795918367347,satisfaction.
EXPERIMENT,0.8510204081632653,"0.0
0.5
1.0
1.5
2.0
TotalEnvInteracts
1e5 0 100 200 300"
EXPERIMENT,0.8530612244897959,Performance
EXPERIMENT,0.8551020408163266,"0.0
0.5
1.0
1.5
2.0
TotalEnvInteracts
1e5 5 10 15 20"
EXPERIMENT,0.8571428571428571,AverageEpCost
EXPERIMENT,0.8591836734693877,"cppo
cppo w/o recovery
cost constraint"
EXPERIMENT,0.8612244897959184,Figure 4: The comparison between CPPO with and without recovery update in PointCircle. 348
LIMITATIONS AND BOARDER IMPACT,0.863265306122449,"5
Limitations and Boarder Impact
349"
LIMITATIONS AND BOARDER IMPACT,0.8653061224489796,"Although our proposed method has shown its ability in test scenarios, there still exist some limitations.
350"
LIMITATIONS AND BOARDER IMPACT,0.8673469387755102,"Firstly, CPPO method is an on-policy constrained RL, which suffers from lower sampling efﬁciency
351"
LIMITATIONS AND BOARDER IMPACT,0.8693877551020408,"compared to other off-policy algorithms, potentially limiting its applicability in real-world scenarios.
352"
LIMITATIONS AND BOARDER IMPACT,0.8714285714285714,"Additionally, the convergence of our method is not yet proven. However, we believe that our work
353"
LIMITATIONS AND BOARDER IMPACT,0.8734693877551021,"will offer researchers a new EM perspective for using PPO-like algorithms to solve the problem
354"
LIMITATIONS AND BOARDER IMPACT,0.8755102040816326,"of constrained RL, thereby leading to the development of more efﬁcient and stable constrained RL
355"
LIMITATIONS AND BOARDER IMPACT,0.8775510204081632,"algorithms.
356"
CONCLUSION,0.8795918367346939,"6
Conclusion
357"
CONCLUSION,0.8816326530612245,"In this paper, we have introduced a novel ﬁrst-order Constrained Reinforcement Learning (CRL)
358"
CONCLUSION,0.8836734693877552,"method called CPPO. Our approach avoids the use of the primal-dual framework and instead treats the
359"
CONCLUSION,0.8857142857142857,"CRL problem as a probabilistic inference problem. By utilizing the Expectation-Maximization (EM)
360"
CONCLUSION,0.8877551020408163,"framework, we address the CRL problem through two key steps: the E-step, which focuses on deriving
361"
CONCLUSION,0.889795918367347,"a theoretically optimal policy distribution, and the M-step, which aims to minimize the difference
362"
CONCLUSION,0.8918367346938776,"between the current policy and the optimal policy. Through the non-parametric representation of the
363"
CONCLUSION,0.8938775510204081,"policy using probability ratios, we convert the CRL problem into a convex optimization problem
364"
CONCLUSION,0.8959183673469387,"with a clear geometric interpretation. As a result, we propose an iterative heuristic algorithm that
365"
CONCLUSION,0.8979591836734694,"efﬁciently solves this optimization problem without relying on the dual variable. Furthermore, we
366"
CONCLUSION,0.9,"introduce a recovery update strategy to handle approximation errors in cost evaluation and ensure
367"
CONCLUSION,0.9020408163265307,"constraint satisfaction when the current policy is infeasible. This strategy mitigates the impact of
368"
CONCLUSION,0.9040816326530612,"approximation errors and strengthens the capability of our method to satisfy constraints. Notably, our
369"
CONCLUSION,0.9061224489795918,"proposed method does not require second-order optimization techniques or the use of the primal-dual
370"
CONCLUSION,0.9081632653061225,"framework, which simpliﬁes the optimization process. Empirical experiments have been conducted to
371"
CONCLUSION,0.9102040816326531,"validate the effectiveness of our proposed method. The results demonstrate that our approach achieves
372"
CONCLUSION,0.9122448979591836,"comparable or even superior performance compared to other baseline methods. This showcases
373"
CONCLUSION,0.9142857142857143,"the advantages of our method in terms of simplicity, efﬁciency, and performance in the ﬁeld of
374"
CONCLUSION,0.9163265306122449,"Constrained Reinforcement Learning.
375"
REFERENCES,0.9183673469387755,"References
376"
REFERENCES,0.9204081632653062,"Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. Maximum a posteriori
377"
REFERENCES,0.9224489795918367,"policy optimisation. arXiv preprint arXiv:1806.06920, 2018.
378"
REFERENCES,0.9244897959183673,"Achiam, J., Held, D., Tamar, A., and Abbeel, P. Constrained policy optimization. In International conference on
379"
REFERENCES,0.926530612244898,"machine learning, pp. 22–31. PMLR, 2017.
380"
REFERENCES,0.9285714285714286,"Altman, E. Constrained Markov decision processes, volume 7. CRC press, 1999.
381"
REFERENCES,0.9306122448979591,"Lasalle, J. The ‘bang-bang’ principle. IFAC Proceedings Volumes, 1(1):503–507, 1960. ISSN 1474-6670. 1st
382"
REFERENCES,0.9326530612244898,"International IFAC Congress on Automatic and Remote Control, Moscow, USSR, 1960.
383"
REFERENCES,0.9346938775510204,"Le, N., Rathour, V. S., Yamazaki, K., Luu, K., and Savvides, M. Deep reinforcement learning in computer vision:
384"
REFERENCES,0.936734693877551,"a comprehensive survey. Artiﬁcial Intelligence Review, 55(4):2733–2819, 2022.
385"
REFERENCES,0.9387755102040817,"Li, Q., Peng, Z., Feng, L., Zhang, Q., Xue, Z., and Zhou, B. Metadrive: Composing diverse driving scenarios for
386"
REFERENCES,0.9408163265306122,"generalizable reinforcement learning. IEEE transactions on pattern analysis and machine intelligence, 2022.
387"
REFERENCES,0.9428571428571428,"Liu, Z., Cen, Z., Isenbaev, V., Liu, W., Wu, S., Li, B., and Zhao, D. Constrained variational policy optimization
388"
REFERENCES,0.9448979591836735,"for safe reinforcement learning. In International Conference on Machine Learning, pp. 13644–13668. PMLR,
389"
REFERENCES,0.9469387755102041,"2022.
390"
REFERENCES,0.9489795918367347,"Paternain, S., Calvo-Fullana, M., Chamon, L. F., and Ribeiro, A. Safe policies for reinforcement learning via
391"
REFERENCES,0.9510204081632653,"primal-dual methods. IEEE Transactions on Automatic Control, 2022.
392"
REFERENCES,0.9530612244897959,"Ray, A., Achiam, J., and Amodei, D. Benchmarking safe exploration in deep reinforcement learning. arXiv
393"
REFERENCES,0.9551020408163265,"preprint arXiv:1910.01708, 7:1, 2019.
394"
REFERENCES,0.9571428571428572,"Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region policy optimization. In International
395"
REFERENCES,0.9591836734693877,"conference on machine learning, pp. 1889–1897. PMLR, 2015a.
396"
REFERENCES,0.9612244897959183,"Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. High-dimensional continuous control using
397"
REFERENCES,0.963265306122449,"generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.
398"
REFERENCES,0.9653061224489796,"Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms.
399"
REFERENCES,0.9673469387755103,"arXiv preprint arXiv:1707.06347, 2017.
400"
REFERENCES,0.9693877551020408,"Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran,
401"
REFERENCES,0.9714285714285714,"D., Graepel, T., et al. A general reinforcement learning algorithm that masters chess, shogi, and go through
402"
REFERENCES,0.9734693877551021,"self-play. Science, 362(6419):1140–1144, 2018.
403"
REFERENCES,0.9755102040816327,"Stooke, A., Achiam, J., and Abbeel, P. Responsive safety in reinforcement learning by pid lagrangian methods.
404"
REFERENCES,0.9775510204081632,"In International Conference on Machine Learning, pp. 9133–9143. PMLR, 2020.
405"
REFERENCES,0.9795918367346939,"Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P. J. Projection-based constrained policy optimization.
406"
REFERENCES,0.9816326530612245,"arXiv preprint arXiv:2010.03152, 2020.
407"
REFERENCES,0.9836734693877551,"Yanushevsky, R. Modern missile guidance. CRC Press, 2018.
408"
REFERENCES,0.9857142857142858,"Yu, C., Velu, A., Vinitsky, E., Wang, Y., Bayen, A., and Wu, Y. The surprising effectiveness of ppo in cooperative,
409"
REFERENCES,0.9877551020408163,"multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
410"
REFERENCES,0.9897959183673469,"Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: A benchmark and
411"
REFERENCES,0.9918367346938776,"evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094–1100.
412"
REFERENCES,0.9938775510204082,"PMLR, 2020.
413"
REFERENCES,0.9959183673469387,"Zhang, Y., Vuong, Q., and Ross, K. First order constrained optimization in policy space. Advances in Neural
414"
REFERENCES,0.9979591836734694,"Information Processing Systems, 33:15338–15349, 2020.
415"
