Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011933174224343676,"Meta learning is a promising paradigm in the era of large models and task dis-
tributional robustness has become an indispensable consideration in real-world
scenarios. Recent advances have examined the effectiveness of tail task risk mini-
mization in fast adaptation robustness improvement [1]. This work contributes to
more theoretical investigations and practical enhancements in the field. Specifically,
we reduce the distributionally robust strategy to a max-min optimization problem,
constitute the Stackelberg equilibrium as the solution concept, and estimate the
convergence rate. In the presence of tail risk, we further derive the generalization
bound, establish connections with estimated quantiles, and practically improve the
studied strategy. Accordingly, extensive evaluations demonstrate the significance of
our proposal and its scalability to multimodal large models in boosting robustness."
INTRODUCTION,0.002386634844868735,"1
Introduction"
INTRODUCTION,0.003579952267303103,"The past few years have witnessed a surge of research interest in meta learning due to its great
potential in the academia and industry [2–5]. By leveraging previous experience, such a learning
paradigm can extract knowledge as priors and empower learning models with adaptability to unseen
tasks from a few examples [6]."
INTRODUCTION,0.00477326968973747,"Nevertheless, the investigation of the robustness needs to be more comprehensive from the task
distribution perspective. In particular, the recently developed large models heavily rely on the few-
shot learning capability and demand robustness of prediction in risk-sensitive scenarios [7]. For
example, when the GPT-like dialogue generation system [8–10] comes into medical consultancy
domains, imprecise answers can cause catastrophic consequences to patients, families, and even
societies in real-world scenarios. In light of these considerations, it is desirable to watch adaptation
differences across tasks when deploying meta learning models and promote task robustness study for
meeting substantial practical demands."
INTRODUCTION,0.0059665871121718375,"Recently, Wang et al. [1] proposes to increase task distributional robustness via employing the tail
risk minimization principle [11] for meta learning. In circumventing the optimization intractability
in the presence of nonconvex risk functions, a two-stage optimization strategy is adopted as the
heuristic to solve the problem. In brief, the strategy consists of two phases in iteration, respectively:
(i) estimating the risk quantile VaRα [11] with the crude Monte Carlo method [12] in the task space;
(ii) updating the meta learning model parameters from the screened subset of tasks. Such a strategy is
simple in implementation, with an improvement guarantee under certain conditions, and empirically
shows improved robustness when faced with task distributional shifts. Despite these advances, there
remain several unresolved theoretical or practical issues in the field."
INTRODUCTION,0.007159904534606206,∗Correspondence Authors.
INTRODUCTION,0.008353221957040573,"Existing limitations. This paper also works on the robustness of fast adaptation in the task space and
tries to fill gaps in [1]. Theoretically, we notice that in [1] (i) there constitutes no notion of solutions,
(ii) it lacks an algorithmic understanding of the two-stage optimization strategy, (iii) the analysis on
generalization capability is ignored in the tail risk of tasks. Empirically, the use of the crude Monte
Carlo might be less efficient in quantile estimates and suffers from a higher approximation error of
the VaRα, degrading the adaptation robustness. These bottlenecks may weaken the versatility of the
two-stage optimization strategy’s use in practice and require more understanding before deployment."
INTRODUCTION,0.00954653937947494,"Primary contributions. In response to the above-mentioned concerns, we propose translating
the two-stage optimization strategy for distributionally robust meta learning [1] into a max-min
optimization problem [13]. Intrinsically, this work models the optimization steps as a Stackelberg
game, and task selection and the sub-gradient optimizer work as the leader and follower players in
decision-making, respectively. The theoretical understanding is from two aspects:"
INTRODUCTION,0.010739856801909307,"1. We constitute the local Stackelberg equilibrium as a solution concept, estimate the convergence
rate, and characterize the asymptotic behavior in learning dynamics."
INTRODUCTION,0.011933174224343675,"2. We derive the generalization bound in the presence of the tail task risk, which connects quantile
estimates with fast adaptation capability in unseen tasks."
INTRODUCTION,0.013126491646778043,"Meanwhile, the empirical influence of VaRα estimators is examined, and we advance meta learners’
robustness by comprising more accurate quantile estimators."
LITERATURE REVIEW,0.014319809069212411,"2
Literature Review"
META LEARNING,0.015513126491646777,"2.1
Meta Learning"
META LEARNING,0.016706443914081145,"Meta learning, or learning to learn, is an increasingly popular paradigm to distill knowledge from
prior experience to unseen scenarios with a few examples [6]. Various meta learning methods have
emerged in the past decade, and this section overviews some dominant families."
META LEARNING,0.017899761336515514,"The context-based methods mainly use the encoder-decoder structure and represent tasks by latent
variables. Typical ones are in the form of the conditional exchangeable stochastic processes and
learn function distributions, such as neural processes [14], conditional neural processes [15] and their
extensions [16–24]. The optimization-based approaches seek the optimal meta initialization of model
parameters and update models from a few examples. Widely known are model agnostic meta learning
[25] and related variants [26–29], such as MetaCurvature [30], which learns curvature information
and transforms gradients in the inner-loop optimization. The metrics-based methods represent tasks
in geometry and perform well in few-shot image classification [31–33]. For example, MetaOptNet
[34] proposes to learn embeddings under a linear classifier and achieve SOTA few-shot classification
performance. There also exist other methods, e.g., hyper-networks [35, 36], memory-augmented
networks [37] and recurrent models [38]."
ROBUSTNESS & GENERALIZATION,0.01909307875894988,"2.2
Robustness & Generalization"
ROBUSTNESS & GENERALIZATION,0.02028639618138425,"The robustness concept in meta learning attracts recent attention, particularly when deploying
large models in real-world scenarios. Admittedly, previous literature works have investigated the
scenarios where the meta dataset’s input is corrupted [39, 40] or the model parameter is perturbed
[29]. Studies regarding the fast adaptation robustness in task distribution remain limited. Wang
et al. [41] explicitly generates task distribution for robust adaptation. Collins et al. [42] employs
the worst-case optimization for promoting MAML’s robustness to extreme worst cases. With the
help of tail risk minimization, Wang et al. [1] proposes two-stage optimization strategies to robustify
the fast adaptation. This work centers around [1] but stresses more theoretical understandings and
performance improvement points."
ROBUSTNESS & GENERALIZATION,0.021479713603818614,"As for generalization capability, there are a couple of works in meta learning. Chen et al. [43]
exploits the information theory to derive the bound for MAML’s like methods. From the data splitting
perspective, Bai et al. [44] formulates the theoretical foundation and connects it to optimality. In [45],
an average risk bound is constructed with the bias for improving performance. Importantly, prior
work [1] ignores the generalization analysis, and meta learner’s generalization in tail risk cases has
not been studied in the literature."
PRELIMINARIES,0.022673031026252982,"3
Preliminaries"
PRELIMINARIES,0.02386634844868735,"General notations. Let p(τ) be the task distribution in meta learning. We respectively express the
task space and the model parameter space as Ωτ and Θ. We denote the complete task set by T and
refer to Dτ as the meta dataset."
PRELIMINARIES,0.025059665871121718,"For instance, Dτ comprises a collection of data points {(xi, yi)}n+m
i=1
in regression. Dτ is ususally
prepared into the support set DS
τ for skill transfer and the query set DQ
τ to assess adaptation perfor-
mance. Take the conditional neural process [15] as an example, DS
τ = {(xi, yi)}n
i=1 works for task
representation with DQ
τ = {(xi, yi)}n+m
i=1
the all data points to fit in regression."
PRELIMINARIES,0.026252983293556086,"The meta risk function corresponds to a map ℓ: Dτ × Θ 7→R+, evaluating fast adaptation
performance. Given p(τ) and meta learning model parameters θ, we can induce the cumulative
distribution of the meta risk function value in the real space as Fℓ(l; θ) := P({ℓ(DQ
τ , DS
τ ; θ) ≤l; τ ∈
T , l ∈R+}), but there is no explicit parameterized form for Fℓin practice as Fℓis θ-dependent."
PRELIMINARIES,0.027446300715990454,"When it comes to the tail risk minimization, we commonly use the conditional value-at-risk (CVaRα)
with the probability threshold α ∈[0, 1). The quantile of our interest is called the value-at-risk
(VaRα) [11] with the definition: VaRα [ℓ(T , θ)] = infl∈R+{l|Fℓ(l; θ) ≥α, τ ∈T }. The resulting
normalized cumulative distribution F α
ℓ(l; θ) is defined as:"
PRELIMINARIES,0.028639618138424822,"F α
ℓ(l; θ) ="
PRELIMINARIES,0.029832935560859187,"(
0,
l < VaRα[ℓ(T , θ)]
Fℓ(l;θ)−α"
PRELIMINARIES,0.031026252983293555,"1−α
,
l ≥VaRα[ℓ(T , θ)]."
PRELIMINARIES,0.032219570405727926,"∀θ ∈Θ, the meta learning operator Mθ defines: Mθ : τ 7→ℓ(DQ
τ , DS
τ ; θ). Accordingly, the tail risk
task subspace Ωα,τ := S"
PRELIMINARIES,0.03341288782816229,"ℓ≥VaRα[ℓ(T ,θ)]

M−1
θ (ℓ)

, with the task distribution constrained in Ωα,τ by
pα(τ; θ). Please refer to Fig. 7 for illustrations of risk concepts.
Assumption 1. To proceed, we retain most assumptions from [1] for theoretical analysis, including:"
PRELIMINARIES,0.034606205250596656,"1. The meta risk function ℓ(DQ
τ , DS
τ ; θ) is βτ-Lipschitz continuous w.r.t. θ;"
PRELIMINARIES,0.03579952267303103,"2. The cumulative distribution Fℓ(l; θ) is βℓ-Lipschitz continuous w.r.t. l, and the normalized
density function pα(τ; θ) is βθ-Lipschitz continuous w.r.t. θ;"
PRELIMINARIES,0.03699284009546539,"3. For arbitrary valid θ ∈Θ and corresponding pα(τ; θ), ℓ(DQ
τ , DS
τ ; θ) is bounded:
supτ∈Ωα,τ ℓ(DQ
τ , DS
τ ; θ) ≤Lmax."
RISK MINIMIZATION PRINCIPLES,0.03818615751789976,"3.1
Risk Minimization Principles"
RISK MINIMIZATION PRINCIPLES,0.03937947494033413,This subsection revisits commonly used risk minimization principles in the meta learning field.
RISK MINIMIZATION PRINCIPLES,0.0405727923627685,"Expected risk minimization. The standard principle is the expected/empirical risk minimization
originated from statistical learning theory [46]. It minimizes meta risk based on the sampling chance
of tasks from the original task distribution:"
RISK MINIMIZATION PRINCIPLES,0.041766109785202864,"min
θ∈Θ E(θ) := Ep(τ)
h
ℓ(DQ
τ , DS
τ ; θ)
i
.
(1)"
RISK MINIMIZATION PRINCIPLES,0.04295942720763723,"Worst-case risk minimization. Noticing that the worst fast adaptation can be disastrous in some
risk sensitive scenarios, Collins et al. [42] proposes to conduct the worst-case optimization in meta
learning:"
RISK MINIMIZATION PRINCIPLES,0.0441527446300716,"min
θ∈Θ max
τ∈T Ew(θ) := ℓ(DQ
τ , DS
τ ; θ).
(2)"
RISK MINIMIZATION PRINCIPLES,0.045346062052505964,"However, as observed from experiments in [42], such a principle inevitably sacrifices too much
average performance for gains of worst-case robustness. Meanwhile, it requires a couple of imple-
mentation tricks and specialized algorithms in stabilizing optimization."
RISK MINIMIZATION PRINCIPLES,0.046539379474940336,"Expected tail risk minimization (CVaRα). To balance the average performance and the worst-case
performance, Wang et al. [1] minimizes the expected tail risk, or equivalently CVaRα risk measure:"
RISK MINIMIZATION PRINCIPLES,0.0477326968973747,"min
θ∈Θ Eα(θ) := Epα(τ;θ)
h
ℓ(DQ
τ , DS
τ ; θ)
i
.
(3)"
RISK MINIMIZATION PRINCIPLES,0.04892601431980907,"Due to no closed form of pα(τ; θ), Wang et al. [1] introduces a slack variable ξ ∈R and reformulates
the objective as follows:"
RISK MINIMIZATION PRINCIPLES,0.050119331742243436,"min
θ∈Θ,ξ∈R Eα(θ, ξ) :=
1
1 −α Z 1"
RISK MINIMIZATION PRINCIPLES,0.0513126491646778,"α
vβdβ = ξ +
1
1 −αEp(τ)
h
ℓ(DQ
τ , DS
τ ; θ) −ξ
+i
,
(4)"
RISK MINIMIZATION PRINCIPLES,0.05250596658711217,"where
vβ
:=
F −1
ℓ
(β) denotes the quantile statistics and

ℓ(DQ
τ , DS
τ ; θ) −ξ
+
:=
max{ℓ(DQ
τ , DS
τ ; θ) −ξ, 0} is the hinge risk."
RISK MINIMIZATION PRINCIPLES,0.05369928400954654,"The optimization objective involves the integral of quantiles in a continuous interval (α, 1], which is
intractable to precisely parameterize with neural networks. The form in Eq. (4) utilizes the duality
trick [11], enabling tractable sampling from the complete task space."
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.05489260143198091,"3.2
Examples & Two-stage Heuristic Strategies"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.05608591885441527,"Before delving deeper into the theoretical issues, we first present DR-MAML [1] as an instantiation
to explain the expected tail risk minimization.
Example 1 (DR-MAML [1]). Given p(τ) and vanilla MAML [25], the distributionally robust MAML
within CVaRα can be written as a bi-level optimization problem:"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.057279236276849645,"min
θ∈Θ
ξ∈R
ξ +
1
1 −αEp(τ)
h
ℓ(DQ
τ ; θ −λ∇θℓ(DS
τ ; θ)) −ξ
+i
,
(5)"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.05847255369928401,"where the gradient update w.r.t. the support set ∇θℓ(DS
τ ; θ) indicates the inner loop with a learning
rate λ. The outer loop executes the gradient updates w.r.t. Eq. (5) and seeks the robust meta
initialization in the parameter space."
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.059665871121718374,"Two-stage optimization strategies. Without loss of generality, we further detail the computa-
tional pipelines of Example 1 with two-stage optimization strategies. Note that MAML [25] is an
optimization-based meta learning method, and the implementation is to execute the sub-gradient
descent over a batch of tasks when updating the meta initialization θmeta:"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.060859188544152745,"θτi
t = θmeta
t
−λ1∇θℓ(DS
τi; θ), i = 1, . . . , B
(6a)
ˆξ = ˆF −1
MC-B(α),
(6b)"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.06205250596658711,"δ(τi) = 1[ℓ(DQ
τi; θτi
t ) ≥ˆξ], i = 1, . . . , B
(6c)"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.06324582338902147,"θmeta
t+1 ←θmeta
t
−λ2
h
B
X"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.06443914081145585,"i=1
∇θ[δ(τi) · ℓ(DQ
τi; θτi
t )]
i
.
(6d)"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.06563245823389022,Sub-gradient for Meta Parameter Update
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.06682577565632458,"Leader's Move (Stage-I):
(1) Risk Distribution Modeling with KDEs
(2) Optimal Subset Selection in the Task
Batch"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.06801909307875895,Follower's Move (Stage-II):
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.06921241050119331,"Meta Learning
Fast Adaptation"
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.07040572792362769,Meta Task Batch
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.07159904534606205,Risk Density Modeling
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.07279236276849642,"Figure 1: Illustration of optimization stages
in distributionally robust meta learning
from a Stackelberg game. Given the DR-
MAML example, the pipeline can be inter-
preted as bi-level optimization: the leader’s
move for characterizing tail task risk and the
follower’s move for robust fast adaptation."
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.07398568019093078,"Here, λ1 and λ2 are the inner loop and the outer
loop learning rates, and the subscript t records the
iteration number, with δ(τi) the indicator variable.
ˆFMC-B is the empirical distribution with B Monte
Carlo task samples. δ(τi) = 1 indicates the meta risk
ℓ(DQ
τi; θτi
t ) after fast adaptation falls into the defined
tail risk region, otherwise δ(τi) = 0."
EXAMPLES & TWO-STAGE HEURISTIC STRATEGIES,0.07517899761336516,"Throughout optimizing DR-MAML, Stage-I includes
the fast adaptation w.r.t. individual task in Eq. (6a),
and the quantile estimate in Eq. (6b). Stage-II ap-
plies the sub-gradient updates to the model parame-
ters in Eq. (6c)/(6d). These two stages repeat until
convergence is achieved."
THEORETICAL INVESTIGATIONS,0.07637231503579953,"4
Theoretical Investigations"
THEORETICAL INVESTIGATIONS,0.07756563245823389,"This section presents theoretical insights into two-stage optimization strategies. We perform analysis
from the algorithmic convergence, the asymptotic tail risk robustness, and the cross-task generalization
capability in meta learning. ... ... ... ... ..."
THEORETICAL INVESTIGATIONS,0.07875894988066826,Estimation
THEORETICAL INVESTIGATIONS,0.07995226730310262,Surrogate Function
THEORETICAL INVESTIGATIONS,0.081145584725537,Optimization
THEORETICAL INVESTIGATIONS,0.08233890214797136,Improvement
THEORETICAL INVESTIGATIONS,0.08353221957040573,"Guarantee
Existence & Solution Concept [Proposition 2]"
THEORETICAL INVESTIGATIONS,0.08472553699284009,Convergence Rate [Theorem 4.1]
THEORETICAL INVESTIGATIONS,0.08591885441527446,Generalization in Tail Risk [Theorem 4.3]
THEORETICAL INVESTIGATIONS,0.08711217183770883,CDF Approx. Error with KDE [Theorem 4.4]
THEORETICAL INVESTIGATIONS,0.0883054892601432,Stackelberg Game for Meta Learning Robustification
THEORETICAL INVESTIGATIONS,0.08949880668257756,"Asymptotic Performance Gap [Theorem 4.2]
Equilibrium"
THEORETICAL INVESTIGATIONS,0.09069212410501193,Main Empirical Results [Extensive Evaluation]
THEORETICAL INVESTIGATIONS,0.09188544152744631,"Figure 2: The sketch of theoretical and empirical contributions in two-stage robust strategies.
On the left side is the two-stage distributionally robust strategy [1]. The contributed theoretical
understanding is right-down, with the right-up the empirical improvement. Arrows show connections
between components."
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.09307875894988067,"4.1
Distributionally Robust Meta Learning as a Stackelberg Game"
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.09427207637231504,"Implementing the two-stage optimization strategy in meta learning requires first specifying the
stages’ order. The default is the minimization of the risk measure w.r.t. the parameter space after
the maximization of the risk measure w.r.t. the task subspace. Hence, we propose to connect it to
max-min optimization [13] and the Stackelberg game [47]."
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.0954653937947494,"Max-min optimization. With the pre-assigned decision-making orders, the studied problem can be
characterized as:"
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.09665871121718377,"max
q(τ)∈Qα min
θ∈Θ F(q, θ) := Eq(τ)
h
ℓ(DQ
τ , DS
τ ; θ)
i
,
(7)"
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.09785202863961814,"where Qα := {q(τ)|Tq ⊆T ,
R"
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.09904534606205251,"τ∈Tq p(τ)dτ = 1 −α} constitutes a collection of uncertainty sets [48]
over task subspace Tq, and q(τ) is the normalized probability density over the task subspace. Note
that in the expected tail risk minimization principle, there is no closed form of optimization objective
Eq. (4) as the tail risk is θ-dependent. It is approximately interpreted as the max-min optimization
when applied to the distribution over the uncertainty set Qα."
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.10023866348448687,Proposition 1. The uncertainty set Qα is convex and compact in terms of probability measures.
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.10143198090692124,"Practical optimization is achieved via mini-batch gradient estimates and sub-gradient updates with
the task size B in [1]; the feasible subsets correspond to all combinations of size ⌈B ∗(1 −α)⌉. Also,
Eq. (7) is non-differentiable w.r.t. q(τ), leaving previous approaches [49–52] unavailable in practice."
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.1026252983293556,"Stackelberg game & best responses. The example computational pipelines in Eq. (6) can be under-
stood as approximately solving a stochastic two-player zero-sum Stackelberg game. Mathematically,
such a game referred to as SG can be depicted as SG := ⟨PL, PF ; {q ∈Qα}, {θ ∈Θ}; F(q, θ)⟩."
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.10381861575178998,"Moreover, we translate the two-stage optimization as decisions made by two competitors, which are
illustrated in Fig. 1. The maximization operator executes in the task space, corresponding to the
leader PL in SG with the utility function F(q, θ). The follower PF attempts to execute sub-gradient
updates over the meta learners’ parameters via maximizing −F(q, θ)."
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.10501193317422435,"The two players compete to maximize separate utility functions in SG, which can be characterized as:"
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.10620525059665871,"SG : qt = arg max
q∈Qα Eq
h
ℓ(DQ
τ , DS
τ ; θt)
i"
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.10739856801909307,"|
{z
}
Leader Player"
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.10859188544152745,",
θt+1 = arg min
θ∈Θ Eqt
h
ℓ(DQ
τ , DS
τ ; θ)
i
,
|
{z
}
Follower Player
(8)"
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.10978520286396182,"where the leader player PL specifies the worst case combinations from the uncertainty set Qα, and
the follower PF reacts to the resulting normalized tail risk for increasing fast adaptation robustness."
DISTRIBUTIONALLY ROBUST META LEARNING AS A STACKELBERG GAME,0.11097852028639618,"It is worth noting that the update rules in Eq. (8) are also called best responses of players in game
theory. The above procedures can be deemed the bi-level optimization [53] since the update of the
meta learner implicitly depends on the leader’s last time decision."
SOLUTION CONCEPT & PROPERTIES,0.11217183770883055,"4.2
Solution Concept & Properties"
SOLUTION CONCEPT & PROPERTIES,0.11336515513126491,"The improvement guarantee has been demonstrated when employing two-stage optimization strategies
for minimizing the tail risk in [1]. Furthermore, we claim that under certain conditions, there converges
to a solution for the proposed Stackelberg game SG. The sufficient evidence is:"
SOLUTION CONCEPT & PROPERTIES,0.11455847255369929,1. The two-stage optimization [1] results in a monotonic sequence:
SOLUTION CONCEPT & PROPERTIES,0.11575178997613365,"Model Updates : · · · 7→{qt−1, θt} 7→{qt, θt+1} 7→· · ·
(9a)
Monotonic Improvement : · · · ≥F(qt−1, θt) ≥F(qt, θt+1) ≥· · · ;
(9b)"
SOLUTION CONCEPT & PROPERTIES,0.11694510739856802,"2. As ℓ≤Lmax, the objective Eq
h
ℓ(DQ
τ , DS
τ ; θ)
i
≤Lmax naturally holds ∀q ∈Qα and θ ∈Θ."
SOLUTION CONCEPT & PROPERTIES,0.11813842482100238,"Built on the boundness of risk functions and the theorem of improvement guarantee, such an
optimization process can finally converge [54]. Then, a crucial question arises concerning the
obtained solution: What is the notion of the convergence point in the game?"
SOLUTION CONCEPT & PROPERTIES,0.11933174224343675,"To answer this question, we need to formulate the corresponding solution concept in SG. Here, the
global Stackelberg equilibrium is introduced as follows.
Definition 1 (Global Stackelberg Equilibrium). Let (q∗, θ∗) ∈Qα × Θ be the solution. With the
leader q∗∈Qα and the follower θ∗∈Θ, (q∗, θ∗) is called a global Stackelberg equilibrium if the
following inequalities are satisfied, ∀q ∈Qα and ∀θ ∈Θ,"
SOLUTION CONCEPT & PROPERTIES,0.12052505966587113,"inf
θ′∈Θ F(q, θ′) ≤F(q∗, θ∗) ≤F(q∗, θ)."
SOLUTION CONCEPT & PROPERTIES,0.12171837708830549,"Proposition 2 (Existence of Equilibrium). Given the Assumption 1, there always exists the global
Stackelberg equilibrium as the Definition 1 for the studied SG."
SOLUTION CONCEPT & PROPERTIES,0.12291169451073986,"Nevertheless, the existence of the global Stackelberg equilibrium can be guaranteed; it is NP-hard to
obtain the equilibrium with existing optimization techniques. The same as that in [55], we turn to the
local Stackelberg equilibrium as the Definition 2, where the notion of the local Stackelberg game is
restricted in a neighborhood Q′
α × Θ′ in strategies.
Definition 2 (Local Stackelberg Equilibrium). Let (q∗, θ∗) ∈Qα × Θ be the solution. With the
leader q∗∈Qα and the follower θ∗∈Θ, (q∗, θ∗) is called a local Stackelberg equilibrium for the
leader if the following inequalities hold, ∀q ∈Q′
α,"
SOLUTION CONCEPT & PROPERTIES,0.12410501193317422,"inf
θ∈SΘ′(q∗) F(q∗, θ) ≥
inf
θ∈SΘ′(q) F(q, θ), where SΘ′(q) := {¯θ ∈Θ′|F(q, ¯θ) ≤F(q, θ), ∀θ ∈Θ′}."
SOLUTION CONCEPT & PROPERTIES,0.12529832935560858,"The nature of nonconvex programming comprises the above local optimum, and we introduce
concepts below for further analysis. It can be validated that F(q, θ) is a quasi-concave function w.r.t.
q, meaning that for any positive number l ∈R+, the set {q|q ∈Qα, F(q, θ) > l} is convex in Qα.
As a result, we deduce that there exists an implicit function h(·) : Θ →Qα such that the condition
holds h(θ) = q with q = arg max¯q∈Qα F(¯q, θ). For the implicit function h, along with ∇θF(q, θ),
we make the Assumption below."
SOLUTION CONCEPT & PROPERTIES,0.12649164677804295,"Assumption 2. The implicit function h(·) is βh-Lipschitz continuous w.r.t. θ ∈Θ, and ∇θF(q, θ) is
βq-Lipschitz continuous w.r.t. q ∈Qα."
CONVERGENCE RATE & GENERALIZATION BOUND,0.1276849642004773,"4.3
Convergence Rate & Generalization Bound"
CONVERGENCE RATE & GENERALIZATION BOUND,0.1288782816229117,"Learning to learn scales with the number of tasks, but the optimization process is computationally
expensive [56–60], particularly when large language models are meta learners [3, 61, 62]. In
training distributionally robust meta learners, estimating the convergence rate allows monitoring of
the convergence and designing early stopping criteria to reach a desirable performance, reducing
computational burdens [63]. Consequently, we turn to another question regarding the solution concept:
What is the convergence rate of the two-stage optimization algorithm?"
CONVERGENCE RATE & GENERALIZATION BOUND,0.13007159904534607,"The runtime complexity for the leader’s move can be easily estimated from subset selection, while
the analysis for the follower is non-trivial. Under certain conditions, we can derive the following
convergence rate theorem, where λ is the learning rate in gradient descent w.r.t. θ."
CONVERGENCE RATE & GENERALIZATION BOUND,0.13126491646778043,"Theorem 4.1 (Convergence Rate for the Second Player). Let the iteration sequence in op-
timization be:
· · · 7→{qt−1, θt} 7→{qt, θt+1} 7→· · · 7→{q∗, θ∗}, with the converged
equilibirum (q∗, θ∗).
Under the Assumption 2 and suppose that ||I −λ∇2
θθF(q∗, θ∗)||2 <
1 −λβqβh, we can have limt→∞
||θt+1−θ∗||2"
CONVERGENCE RATE & GENERALIZATION BOUND,0.1324582338902148,"||θt−θ∗||2
≤1, and the iteration converges with the rate
 
||I −λ∇2
θθF(q∗, θ∗)||2 + λβqβh

when t approaches infinity."
CONVERGENCE RATE & GENERALIZATION BOUND,0.13365155131264916,"Moreover, after executing the two-stage algorithm T time steps and given learned θmeta
T
, we can
establish a bound on the asymptotic performance gap w.r.t. CVaRα in Theorem 4.2. For expositional
clarity, we simplify ℓ(DQ
τ , DS
τ ; θ∗), ℓ(DQ
τ , DS
τ ; θmeta
T
), VaRα [ℓ(T , θ∗)], and VaRα [ℓ(T , θmeta
T
)] as ℓ∗,
ℓmeta, VaR∗
α, and VaRmeta
α
, respectively."
CONVERGENCE RATE & GENERALIZATION BOUND,0.13484486873508353,"Theorem 4.2 (Asymptotic Performance Gap in Tail Task Risk). Under the Assumption 1 and given a
batch of tasks {τi}B
i=1, we can have"
CONVERGENCE RATE & GENERALIZATION BOUND,0.1360381861575179,"CVaRα(θmeta
T
) −CVaRα(θ∗) ≤βτ∥θmeta
T
−θ∗∥+ VaR∗
α
1 −α"
CONVERGENCE RATE & GENERALIZATION BOUND,0.13723150357995226,"
P(T1) −P(T2)

,
(10)"
CONVERGENCE RATE & GENERALIZATION BOUND,0.13842482100238662,"where T1 = {τ : ℓ∗< VaR∗
α, ℓmeta ≥VaRmeta
α
}, T2 = {τ : ℓ∗≥VaR∗
α, ℓmeta < VaRmeta
α
}."
CONVERGENCE RATE & GENERALIZATION BOUND,0.13961813842482101,"For sufficiently large T, the first term can be bounded by a small number due to the convergence, and
the second term vanishes since limT →∞ℓmeta = ℓ∗and limT →∞VaRmeta
α
= VaR∗
α, respectively."
CONVERGENCE RATE & GENERALIZATION BOUND,0.14081145584725538,"Another crucial issue regarding meta learning lies in the fast adaptation capability in unseen cases.
This drives us to answer the following question: How does the resulting meta learner generalize in
the presence of tail task risk?"
CONVERGENCE RATE & GENERALIZATION BOUND,0.14200477326968974,"To this end, we first define R(θ∗) = Epα(τ) [ℓ∗], bR(θ∗) =
1
B
PB
i=1 δ(τi)ℓ(DQ
τi, DS
τi; θ∗), and
bRw(θ∗) = 1"
CONVERGENCE RATE & GENERALIZATION BOUND,0.1431980906921241,"B
PB
i=1
pα(τi)"
CONVERGENCE RATE & GENERALIZATION BOUND,0.14439140811455847,"p(τi) ℓ(DQ
τi, DS
τi; θ∗), where τi ∼p(τ). Also note that the support of pα(τ; θ∗)
is within that of p(τ), namely supp(pα(τ; θ∗)) ⊆supp(p(τ)). Then we can induce Theorem 4.3 w.r.t.
the tail risk generalization."
CONVERGENCE RATE & GENERALIZATION BOUND,0.14558472553699284,"Theorem 4.3 (Generalization Bound in the Tail Risk Cases). Given a collection of task samples
{τi}B
i=1 and corresponding meta datasets, we can derive the following generalization bound in the
presence of tail risk:"
CONVERGENCE RATE & GENERALIZATION BOUND,0.1467780429594272,R(θ∗) ≤bR(θ∗) + s
CONVERGENCE RATE & GENERALIZATION BOUND,0.14797136038186157,"2
 
α
1−αL2max + Vτi∼pα(τ)

ℓ(DQ
τi, DSτi; θ∗)

ln
  1 ϵ
 B"
CONVERGENCE RATE & GENERALIZATION BOUND,0.14916467780429593,"+
1
3(1 −α)
Lmax B"
CONVERGENCE RATE & GENERALIZATION BOUND,0.15035799522673032,"
2 ln
1 ϵ"
CONVERGENCE RATE & GENERALIZATION BOUND,0.1515513126491647,"
+ 3αB

, (11)"
CONVERGENCE RATE & GENERALIZATION BOUND,0.15274463007159905,"where the inequality holds with probability at least 1 −ϵ and ϵ ∈(0, 1), V[·] denotes the variance
operation, and Lmax is from the Assumption 1."
CONVERGENCE RATE & GENERALIZATION BOUND,0.15393794749403342,"In conjunction with the confidence ϵ and a task batch B of significant size, Theorem 4.3 reveals the
generalization bound given the meta-trained parameter θ∗. It is also associated with the variance
Vτi∼pα(τ)[ℓ(DQ
τi, DS
τi; θ∗)]. Besides, we also derive a specific bound in the case of MAML, and
details are attached in Appendix Theorem C.1."
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.15513126491646778,"4.4
Practical Enhancements & Implementations"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.15632458233890215,"Theorem 4.3 reveals that an accurate estimate of VaRα yields a precise variance (i.e.,
Vτi∼pα(τ)[ℓ(DQ
τi, DS
τi; θ∗)]), leading to more reliable bounds. Accordingly, this section offers
improvements over [1] via utilizing kernel density estimators (KDE) [64] for VaRα’s estimates.
Compared to crude Monte Carlo (MC) methods, KDE can handle arbitrary complex distributions,
capture local statistics well, and smoothen the cumulative function in a non-parametric way."
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.1575178997613365,"Specifically, we can construct KDE with a batch of task risk values {ℓ(DQ
τi, DS
τi; θ)}B
i=1:"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.15871121718377088,"Fℓ-KDE(l; θ) =
Z l −∞ 1
Bhℓ B
X"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.15990453460620524,"i=1
K
t −ℓ(DQ
τi, DS
τi; θ)
hℓ"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.1610978520286396,"
dt,
(12)"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.162291169451074,"where K : Rd →R is a kernel function, e.g., the Gaussian kernel, K(x) =
exp(−||x||2/2)
R
exp(−||x||2/2)dx, and
hℓis the smoothing bandwidth. Once the KDE is built, it enables access to the quantile from the
cumulative distribution functions or numeric integrals. The following Theorem 4.4 shows that KDE
serves as a reliable approximation for VaRα."
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.16348448687350836,"Theorem 4.4. Let F −1
ℓ-KDE(α; θ) = VaRKDE
α
[ℓ(T , θ)] and F −1
ℓ
(α; θ) = VaRα[ℓ(T , θ)]. Suppose that
K(x) is lower bounded by a constant, ∀x. For any ϵ > 0, with probability at least 1 −ϵ, we can have
the following bound:"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.16467780429594273,"sup
θ∈Θ"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.1658711217183771," 
F −1
ℓ-KDE(α; θ) −F −1
ℓ
(α; θ)

≤O

hℓ
√B ∗log B"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.16706443914081145,"
.
(13)"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.16825775656324582,"As implied, one can close the distribution approximation gap by adopting a smaller, more flexible
bandwidth. Additionally, KDE models offer a smooth estimate of the cumulative distribution function
and require no prior assumptions.
Remark 1. In addition to smoothness, flexibility, and distribution agnostic traits, KDE in adoption
can enhance the studied method’s generalization capability. The crude Monte Carlo used in [1]
typically incurs an error of approximately O( 1
√"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.16945107398568018,"B) in estimating quantiles [65]. In contrast, that of"
PRACTICAL ENHANCEMENTS & IMPLEMENTATIONS,0.17064439140811455,"KDE is no more than O(
hℓ
√B∗log B) from Theorem 4.4."
EMPIRICAL FINDINGS,0.1718377088305489,"5
Empirical Findings"
EMPIRICAL FINDINGS,0.1730310262529833,"Prior sections mainly focus on the theoretical understanding of two-stage distributionally robust
strategies. This section conducts extensive experiments on a broader range of benchmarks and
examines the improvement tricks, e.g., the use of KDE for quantile estimates, from empirical results."
EMPIRICAL FINDINGS,0.17422434367541767,"Benchmarks & baselines. We perform experiments on the few-shot regression, system identification,
image classification, and meta reinforcement learning, where most of them keep setups the same as
prior work [1, 42]. We evaluate the methods from risk minimization principles and corresponding
indicators, including expected/empirical risk minimization (Average), worst-case risk minimization
(Worst), and tail risk minimization (CVaRα)."
EMPIRICAL FINDINGS,0.17541766109785203,"MAML mainly works as the base meta learner, and we term the KDE-augmented DR-MAML as
DR-MAML+. Then we compare DR-MAML+ with several baselines, including vanilla MAML [25],
TR-MAML [42], DRO-MAML [66] and DR-MAML [1]."
EMPIRICAL FINDINGS,0.1766109785202864,"Average
Worst
CVaRα 1 2 3 4"
EMPIRICAL FINDINGS,0.17780429594272076,"5
Sinusoid 5-shot"
EMPIRICAL FINDINGS,0.17899761336515513,"Average
Worst
CVaRα 0.5 1.0 1.5 2.0 2.5 3.0"
EMPIRICAL FINDINGS,0.1801909307875895,"3.5
Sinusoid 10-shot"
EMPIRICAL FINDINGS,0.18138424821002386,Mean Square Errors
EMPIRICAL FINDINGS,0.18257756563245822,"MAML
TR-MAML
DRO-MAML
DR-MAML
DR-MAML+(Ours)"
EMPIRICAL FINDINGS,0.18377088305489261,"Figure 3: Meta testing performance in sinu-
soid regression problems (5 runs). The charts
report testing mean square errors (MSEs) over
490 unseen tasks [42] with α = 0.7, where
black vertical lines indicate standard error bars."
EMPIRICAL FINDINGS,0.18496420047732698,"Average
Worst
CVaRα 0.5 1.0 1.5"
EMPIRICAL FINDINGS,0.18615751789976134,"2.0
Pendulum 10-shot"
EMPIRICAL FINDINGS,0.1873508353221957,"Average
Worst
CVaRα 0.5 1.0 1.5"
EMPIRICAL FINDINGS,0.18854415274463007,"2.0
Pendulum 20-shot"
EMPIRICAL FINDINGS,0.18973747016706444,Mean Square Errors
EMPIRICAL FINDINGS,0.1909307875894988,"MAML
TR-MAML
DRO-MAML
DR-MAML
DR-MAML+(Ours)"
EMPIRICAL FINDINGS,0.19212410501193317,"Figure 4: Meta testing performance in Pen-
dulum 10-shot and 20-shot problems (5
runs). Reported are testing MSEs over 529 un-
seen tasks with α = 0.5, where black vertical
lines indicate standard error bars."
SINUSOID REGRESSION,0.19331742243436753,"5.1
Sinusoid Regression"
SINUSOID REGRESSION,0.19451073985680192,"The goal of the sinusoid regression [25] is to quickly fit an underlying function f(x) = A sin(x −B)
from K randomly sampled data points, and tasks are specified by (A, B). The meta-training and
testing setups are the same as that in [1, 42], where many easy functions with a tiny fraction of
difficult ones are included in the training."
SINUSOID REGRESSION,0.1957040572792363,"Result & analysis. As illustrated in Fig. 3, we can observe that DR-MAML+ consistently outper-
forms all baselines across average and CVaRα indicators in the 5-shot case. Though the average
performance slightly lags behind DR-MAML in the 10-shot case, DR-MAML+ surpasses other
baselines in both the Worst and CVaRα indicators. This implies that DR-MAML+ exhibits more"
SINUSOID REGRESSION,0.19689737470167065,"robustness in challenging task distributions, e.g., 5-shot case. Furthermore, the standard error asso-
ciated with our method is significantly smaller than others, underscoring the stability of DR-MAML+."
SYSTEM IDENTIFICATION,0.19809069212410502,"5.2
System Identification"
SYSTEM IDENTIFICATION,0.19928400954653938,"The system identification corresponds to learning a dynamics model from a few collected transitions
in physics systems. Here, we consider the Pendulum system and create diverse dynamical systems
by varying its mass m and length l, with (m, l) ∼U([0.4, 1.6], [0.4, 1.6]). A random policy collects
transitions for meta training, and 10 random transitions work as a support dataset."
SYSTEM IDENTIFICATION,0.20047732696897375,"Result & analysis. Fig. 4 shows no significant difference between 10-shot and 20-shot cases.
DR-MAML+ dominates the performance across all indicators in both cases. Due to the min-max
optimization, TR-MAML behaves well in the worst-case but sacrifices too much average performance.
Within the studied strategies, DR-MAML+ exhibits an advantage over DR-MAML regarding CVaRα."
FEW-SHOT IMAGE CLASSIFICATION,0.2016706443914081,"5.3
Few-shot Image Classification"
FEW-SHOT IMAGE CLASSIFICATION,0.20286396181384247,"We perform few-shot image classification on the mini-ImageNet dataset [67], with the same setup in
[42]. The task is a 5-way 1-shot classification problem. And 64 classes are selected for constructing
meta-training tasks, with the remaining 32 classes for meta-testing."
FEW-SHOT IMAGE CLASSIFICATION,0.20405727923627684,"Table 1: Average 5-way 1-shot classification accuracies in mini-ImageNet with reported
standard deviations (3 runs). With α = 0.5, the best results are in bold."
FEW-SHOT IMAGE CLASSIFICATION,0.2052505966587112,"Eight Meta-Training Tasks
Four Meta-Testing Tasks
Method
Average
Worst
CVaRα
Average
Worst
CVaRα"
FEW-SHOT IMAGE CLASSIFICATION,0.2064439140811456,"MAML [25]
70.1±2.2
48.0±4.5
63.2±2.6
46.6±0.4
44.7±0.7
44.6±0.7
TR-MAML [42]
63.2±1.3
60.7±1.6
62.1±1.2
48.5±0.6
45.9±0.8
46.6±0.5
DRO-MAML [66]
67.0±0.2
56.6±0.4
61.6±0.2
49.1±0.2
46.6±0.1
47.2±0.2
DR-MAML [1]
70.2±0.2
63.4±0.2
67.2±0.1
49.4±0.1
47.1±0.1
47.5±0.1
DR-MAML+(Ours)
70.4±0.1
63.8±0.2
67.5±0.1
49.9±0.1
47.2±0.1
48.1±0.1"
FEW-SHOT IMAGE CLASSIFICATION,0.20763723150357996,"Result & analysis. In Table 1, methods within a two-stage distributionally robust strategy, namely
DR-MAML and DR-MAML+, show superiority to others across all indicators in both training and
testing scenarios, which is similar to empirical findings in [1]. Interstingly, DR-MAML+ and DR-
MAML are comparable in most scenarios, and we attribute this to the small batch size in training,
which weakens KDE’s quantile approximation advantage."
META REINFORCEMENT LEARNING,0.20883054892601433,"5.4
Meta Reinforcement Learning"
META REINFORCEMENT LEARNING,0.2100238663484487,"Table 2: Meta testing returns in point robot navigation (4 runs).
The chart reports average return and CVaRα return with α = 0.5."
META REINFORCEMENT LEARNING,0.21121718377088305,"Method
Average
CVaRα"
META REINFORCEMENT LEARNING,0.21241050119331742,"MAML [25]
-21.1 ± 0.69
-29.2 ± 1.37
DRO-MAML [66]
-20.9 ± 0.41
-29.0 ± 0.66
DR-MAML [1]
-19.6 ± 0.49
-28.9 ± 1.20
DR-MAML+(Ours)
-19.2± 0.44
-28.4± 0.86"
META REINFORCEMENT LEARNING,0.21360381861575178,"Here, we take 2-D point robot
navigation as the meta reinforce-
ment learning benchmark in eval-
uation. The goal is to reach the
target destination with the help of
a few exploration transitions for
fast adaptation, and we retain the
setup in MAML [25]. In meta
testing, we randomly sample 80 navigation goals and examine methods’ navigation performance."
META REINFORCEMENT LEARNING,0.21479713603818615,"20
30
40
50
60
70
Task Batch Size 0.02 0.04 0.06"
META REINFORCEMENT LEARNING,0.2159904534606205,Sinusoid 5-shot
META REINFORCEMENT LEARNING,0.2171837708830549,"MC
KDE"
META REINFORCEMENT LEARNING,0.21837708830548927,"20
30
40
50
60
70
Task Batch Size 0.005 0.010 0.015 0.020 0.025"
META REINFORCEMENT LEARNING,0.21957040572792363,Sinusoid 10-shot
META REINFORCEMENT LEARNING,0.220763723150358,"MC
KDE"
META REINFORCEMENT LEARNING,0.22195704057279236,Approximation Error
META REINFORCEMENT LEARNING,0.22315035799522673,"Figure 5: VaRα approximation errors with the
crude MC and KDE. We compute the difference
between the estimated
ˆ
VaRα and the Oracle VaRα
in the absolute value | ˆ
VaRα −VaRα|."
META REINFORCEMENT LEARNING,0.2243436754176611,"Result & analysis. As reinforcement learning
methods fluctuate fiercely in worst-case indi-
cators, we only report Average and CVaRα re-
turns in Table 2. We observe that using studied
strategies in DR-MAML enhances the returns.
DR-MAML+ benefits from a more reliable quan-
tile estimate and achieves superior performance.
The application of distributional robustness to
reinforcement learning yields improvements in
returns."
ASSESSMENT OF QUANTILE ESTIMATORS,0.22553699284009546,"5.5
Assessment of Quantile Estimators"
ASSESSMENT OF QUANTILE ESTIMATORS,0.22673031026252982,"With the meta trained model, e.g., DR-MAML+ in sinusoid regression, we collect the testing task
risk values with different task batch sizes to estimate the VaRα from respectively the crude MC and
KDE. As observed from Fig. 5, the VaRα approximation error decreases with more tasks, and the
KDE produces more accurate estimates with a sharper decreasing trend. The above well verifies the
conclusion in Theorem 4.3."
EMPRICIAL RESULT SUMMARIZATION,0.22792362768496421,"5.6
Empricial Result Summarization"
EMPRICIAL RESULT SUMMARIZATION,0.22911694510739858,"Here, we summarize two points from the above empirical results and associated theorems. (i) From
Theorem 4.2/4.3 and Fig. 3/4/5: the VaRα estimate relates to the reliable generalization bound, and
cumulated tiny approximation errors along iterations potentially result in worse equilibrium. (ii)
From Theorem 4.3/4.4, Remark 1, Fig. 3, and Table 1/2: with the studied strategy, the KDE is a better
choice of task risk distribution modelling than the crude MC in tougher benchmarks, e.g., 5-shot
sinusoid regression, meta-testing mini-ImageNet classification, and point robot navigation."
COMPATIBILITY WITH LARGE MODELS,0.23031026252983294,"5.7
Compatibility with Large Models"
COMPATIBILITY WITH LARGE MODELS,0.2315035799522673,"Average
CVaR 70 75 80 85 90 95"
TIERED-IMAGENET,0.23269689737470167,"100
tiered-ImageNet"
TIERED-IMAGENET,0.23389021479713604,"Average
CVaR 75.0 77.5 80.0 82.5 85.0 87.5 90.0 92.5"
IMAGENETA,0.2350835322195704,"95.0
ImageNetA"
IMAGENETA,0.23627684964200477,"Average
CVaR 80.0 82.5 85.0 87.5 90.0 92.5 95.0 97.5"
IMAGENETSKETCH,0.23747016706443913,"100.0
ImageNetSketch"
IMAGENETSKETCH,0.2386634844868735,Classification Accuracies
IMAGENETSKETCH,0.2398568019093079,"CLIP
MaPLe
DR-MaPLe
DR-MaPLe+"
IMAGENETSKETCH,0.24105011933174225,"Figure 6: Meta testing results on 5-way 1-shot classification accuracies with reported standard
deviations (3 runs). The charts respectively report classification accuracies over 150 unseen tasks.
We further conduct few-shot image classification experiments in the presence of large model. Note
that CLIP [68] exhibits strong zero-shot adaptation capability; hence, we employ ""ViT-B/16""-based
CLIP as the backbone to enable few-shot learning in the same way as MaPLe with training setup
N_CTX = 2 and MAX_EPOCH = 30 [69], scaling to large neural networks in evaluation (See
Appendix Section D for details)."
IMAGENETSKETCH,0.24224343675417662,"Improved Robustness in Evaluation: As illustrated in Fig. 6, DR-MaPLe and DR-MaPLe+
consistently outperform baselines across both average and indicators in cases, demonstrating the
advantage of the two-stage strategy in enhancing the robustness of few-shot learning. DR-MaPLe+
achieves better results as KDE quantiles are more accurate with large batch sizes. These results
confirm the scalability and compatibility of our method on large models."
IMAGENETSKETCH,0.24343675417661098,"Learning Efficiency as Limitations: In terms of implementation time and memory cost, we retain
the setup the same as that in [1]: use the same maximum number of meta gradient updates for all
baselines in training processes, which means given α = 0.5, the tail risk minimization principle
requires double task batches to evaluate and screen sub-batches. It can be seen that both DR-MaPLe
and DR-MaPLe+ consume more memories, and the extra training time over MaPLe arises from the
evaluation and sub-batch screening in the forward pass. Such additional computations and memory
costs raise computational and memory efficiency issues for exchanging extra significant robustness
improvement in fast adaptation."
CONCLUSION,0.24463007159904535,"6
Conclusion"
CONCLUSION,0.2458233890214797,"To conclude, this paper proposes to understand the two-stage distributionally robust strategy from
optimization processes, define the convergence solution, and derive the generalization bound in the
presence of tail task risk. Extensive experiments validate the studied improvement tricks and reveal
more empirical properties of the studied strategy. We leave computational overhead reduction as a
promising topic for future exploration in robust fast adaptation."
CONCLUSION,0.24701670644391407,Acknowledgement
CONCLUSION,0.24821002386634844,"This work is funded by National Natural Science Foundation of China (NSFC) with the Number #
62306326. We express particular gratitude to friends who guide large model-relevant experiments."
REFERENCES,0.2494033412887828,References
REFERENCES,0.25059665871121717,"[1] Qi Wang, Yiqin Lv, Yanghe Feng, Zheng Xie, and Jincai Huang. A simple yet effective strategy
to robustify the meta learning paradigm. In Thirty-seventh Conference on Neural Information
Processing Systems, 2023."
REFERENCES,0.25178997613365156,"[2] Yuanfu Lu, Yuan Fang, and Chuan Shi. Meta-learning on heterogeneous information networks
for cold-start recommendation.
In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pages 1563–1573, 2020."
REFERENCES,0.2529832935560859,"[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020."
REFERENCES,0.2541766109785203,"[4] Yi Yuan, Gan Zheng, Kai-Kit Wong, and Khaled B Letaief. Meta-reinforcement learning
based resource allocation for dynamic v2x communications. IEEE Transactions on Vehicular
Technology, 70(9):8964–8977, 2021."
REFERENCES,0.2553699284009546,"[5] Brenden M Lake and Marco Baroni. Human-like systematic generalization through a meta-
learning neural network. Nature, pages 1–7, 2023."
REFERENCES,0.256563245823389,"[6] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in
neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44
(9):5149–5169, 2021."
REFERENCES,0.2577565632458234,"[7] Qi Wang, Yanghe Feng, Jincai Huang, Yiqin Lv, Zheng Xie, and Xiaoshan Gao. Large-scale
generative simulation artificial intelligence: The next hotspot. The Innovation, page 100516,
2023."
REFERENCES,0.25894988066825775,"[8] Young-Jun Lee, Chae-Gyun Lim, and Ho-Jin Choi. Does gpt-3 generate empathetic dialogues?
a novel in-context example selection method and automatic evaluation metric for empathetic
dialogue generation. In Proceedings of the 29th International Conference on Computational
Linguistics, pages 669–683, 2022."
REFERENCES,0.26014319809069214,"[9] Chen Tang, Hongbo Zhang, Tyler Loakman, Chenghua Lin, and Frank Guerin. Terminology-
aware medical dialogue generation. In ICASSP 2023-2023 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE, 2023."
REFERENCES,0.2613365155131265,"[10] Bharath Chintagunta, Namit Katariya, Xavier Amatriain, and Anitha Kannan. Medically
aware gpt-3 as a data generator for medical dialogue summarization. In Machine Learning for
Healthcare Conference, pages 354–372. PMLR, 2021."
REFERENCES,0.26252983293556087,"[11] R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk.
Journal of risk, 2:21–42, 2000."
REFERENCES,0.2637231503579952,"[12] Dirk P Kroese and Reuven Y Rubinstein. Monte carlo methods. Wiley Interdisciplinary Reviews:
Computational Statistics, 4(1):48–58, 2012."
REFERENCES,0.2649164677804296,"[13] John M Danskin. The theory of max-min, with applications. SIAM Journal on Applied
Mathematics, 14(4):641–664, 1966."
REFERENCES,0.26610978520286394,"[14] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami,
and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018."
REFERENCES,0.26730310262529833,"[15] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes.
In International Conference on Machine Learning, pages 1704–1713. PMLR, 2018."
REFERENCES,0.2684964200477327,"[16] Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard Turner.
Meta-learning probabilistic inference for prediction. In International Conference on Learning
Representations, 2018."
REFERENCES,0.26968973747016706,"[17] Qi Wang, Marco Federici, and Herke van Hoof. Bridge the inference gaps of neural pro-
cesses via expectation maximization. In The Eleventh International Conference on Learning
Representations, 2023. URL https://openreview.net/forum?id=A7v2DqLjZdq."
REFERENCES,0.27088305489260145,"[18] Andrew Foong, Wessel Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, and
Richard Turner. Meta-learning stationary stochastic process prediction with convolutional
neural processes. Advances in Neural Information Processing Systems, 33:8284–8295, 2020."
REFERENCES,0.2720763723150358,"[19] Qi Wang and Herke Van Hoof. Doubly stochastic variational inference for neural processes
with hierarchical latent variables. In International Conference on Machine Learning, pages
10018–10028. PMLR, 2020."
REFERENCES,0.2732696897374702,"[20] Muhammad Waleed Gondal, Shruti Joshi, Nasim Rahaman, Stefan Bauer, Manuel Wuthrich,
and Bernhard Scholkopf. Function contrastive learning of transferable meta-representations. In
Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24
July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages
3755–3765. PMLR, 2021."
REFERENCES,0.2744630071599045,"[21] Qi Wang and Herke van Hoof. Learning expressive meta-representations with mixture of expert
neural processes. In Advances in neural information processing systems, 2022."
REFERENCES,0.2756563245823389,"[22] Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh.
Bootstrapping neural processes. Advances in neural information processing systems, 33:6606–
6615, 2020."
REFERENCES,0.27684964200477324,"[23] Qi Wang and Herke Van Hoof. Model-based meta reinforcement learning using graph structured
surrogate models and amortized policy search.
In International Conference on Machine
Learning, pages 23055–23077. PMLR, 2022."
REFERENCES,0.27804295942720764,"[24] Jiayi Shen, Xiantong Zhen, Marcel Worring, et al. Episodic multi-task learning with heteroge-
neous neural processes. arXiv preprint arXiv:2310.18713, 2023."
REFERENCES,0.27923627684964203,"[25] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In International conference on machine learning, pages 1126–1135.
PMLR, 2017."
REFERENCES,0.28042959427207637,"[26] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with
implicit gradients. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.28162291169451076,"[27] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting
gradient-based meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018."
REFERENCES,0.2828162291169451,"[28] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning.
Advances in neural information processing systems, 31, 2018."
REFERENCES,0.2840095465393795,"[29] Momin Abbas, Quan Xiao, Lisha Chen, Pin-Yu Chen, and Tianyi Chen. Sharp-maml: Sharpness-
aware model-agnostic meta learning. In International Conference on Machine Learning, pages
10–32. PMLR, 2022."
REFERENCES,0.2852028639618138,"[30] Eunbyung Park and Junier B Oliva. Meta-curvature. Advances in neural information processing
systems, 32, 2019."
REFERENCES,0.2863961813842482,"[31] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning.
Advances in neural information processing systems, 30, 2017."
REFERENCES,0.28758949880668255,"[32] Kelsey Allen, Evan Shelhamer, Hanul Shin, and Joshua Tenenbaum. Infinite mixture prototypes
for few-shot learning. In International Conference on Machine Learning, pages 232–241.
PMLR, 2019."
REFERENCES,0.28878281622911695,"[33] Sergey Bartunov and Dmitry Vetrov. Few-shot generative modelling with generative matching
networks. In International Conference on Artificial Intelligence and Statistics, pages 670–678.
PMLR, 2018."
REFERENCES,0.28997613365155134,"[34] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019."
REFERENCES,0.2911694510739857,"[35] Jacob Beck, Matthew Thomas Jackson, Risto Vuorio, and Shimon Whiteson. Hypernetworks
in meta-reinforcement learning. In Conference on Robot Learning, pages 1478–1487. PMLR,
2023."
REFERENCES,0.29236276849642007,"[36] Dominic Zhao, Johannes von Oswald, Seijin Kobayashi, João Sacramento, and Benjamin F
Grewe. Meta-learning via hypernetworks. 4th Workshop on Meta-Learning at NeurIPS 2020,
2020."
REFERENCES,0.2935560859188544,"[37] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap.
Meta-learning with memory-augmented neural networks.
In International conference on
machine learning, pages 1842–1850. PMLR, 2016."
REFERENCES,0.2947494033412888,"[38] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2:
Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,
2016."
REFERENCES,0.29594272076372313,"[39] Ren Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan, and Meng Wang.
On fast adversarial robustness adaptation in model-agnostic meta-learning. In International
Conference on Learning Representations, 2020."
REFERENCES,0.2971360381861575,"[40] Micah Goldblum, Liam Fowl, and Tom Goldstein. Adversarially robust few-shot learning: A
meta-learning approach. Advances in Neural Information Processing Systems, 33:17886–17895,
2020."
REFERENCES,0.29832935560859186,"[41] Cheems Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, and Xiangyang Ji. Robust fast adaptation
from adversarially explicit task distribution generation. arXiv preprint arXiv:2407.19523, 2024."
REFERENCES,0.29952267303102625,"[42] Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai. Task-robust model-agnostic meta-
learning. Advances in Neural Information Processing Systems, 33:18860–18871, 2020."
REFERENCES,0.30071599045346065,"[43] Qi Chen, Changjian Shui, and Mario Marchand. Generalization bounds for meta-learning:
An information-theoretic analysis. Advances in Neural Information Processing Systems, 34:
25878–25890, 2021."
REFERENCES,0.301909307875895,"[44] Yu Bai, Minshuo Chen, Pan Zhou, Tuo Zhao, Jason Lee, Sham Kakade, Huan Wang, and
Caiming Xiong. How important is the train-validation split in meta-learning? In International
Conference on Machine Learning, pages 543–553. PMLR, 2021."
REFERENCES,0.3031026252983294,"[45] Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn
stochastic gradient descent with biased regularization. In International Conference on Machine
Learning, pages 1566–1575. PMLR, 2019."
REFERENCES,0.3042959427207637,"[46] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,
1999."
REFERENCES,0.3054892601431981,"[47] Tao Li and Suresh P Sethi. A review of dynamic stackelberg game models. Discrete &
Continuous Dynamical Systems-B, 22(1):125, 2017."
REFERENCES,0.30668257756563244,"[48] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.
Robust solutions of optimization problems affected by uncertain probabilities. Management
Science, 59(2):341–357, 2013."
REFERENCES,0.30787589498806683,"[49] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48
(3):334–334, 1997."
REFERENCES,0.30906921241050117,"[50] Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. In International Conference on Machine Learning, pages 6083–6093.
PMLR, 2020."
REFERENCES,0.31026252983293556,"[51] Pierre Loridan and Jacqueline Morgan. Weak via strong stackelberg problem: new results.
Journal of global Optimization, 8:263–287, 1996."
REFERENCES,0.31145584725536996,"[52] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters
by implicit differentiation. In International conference on artificial intelligence and statistics,
pages 1540–1552. PMLR, 2020."
REFERENCES,0.3126491646778043,"[53] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level
optimization for learning and vision from a unified perspective: A survey and beyond. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 44(12):10045–10067, 2021."
REFERENCES,0.3138424821002387,[54] Tom M Apostol. Mathematical analysis. 1974.
REFERENCES,0.315035799522673,"[55] Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in stackelberg
games: Equilibria characterization, convergence analysis, and empirical study. In International
Conference on Machine Learning, pages 3133–3144. PMLR, 2020."
REFERENCES,0.3162291169451074,"[56] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for
modern deep learning research. In Proceedings of the AAAI conference on artificial intelligence,
volume 34, pages 13693–13696, 2020."
REFERENCES,0.31742243436754175,"[57] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel
Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network
training. arXiv preprint arXiv:2104.10350, 2021."
REFERENCES,0.31861575178997614,"[58] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for
transfer. arXiv preprint arXiv:2102.01293, 2021."
REFERENCES,0.3198090692124105,"[59] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He,
Antong Li, Mengshen He, Zhengliang Liu, et al. Summary of chatgpt-related research and
perspective towards the future of large language models. Meta-Radiology, page 100017, 2023."
REFERENCES,0.32100238663484487,"[60] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva,
Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt
for good? on opportunities and challenges of large language models for education. Learning
and individual differences, 103:102274, 2023."
REFERENCES,0.3221957040572792,"[61] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. In Joint Conference of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing,
ACL-IJCNLP 2021, pages 3816–3830. Association for Computational Linguistics (ACL), 2021."
REFERENCES,0.3233890214797136,"[62] Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek Sharma, Fan Chen, and Lei Jiang.
Llmcarbon: Modeling the end-to-end carbon footprint of large language models. arXiv preprint
arXiv:2309.14393, 2023."
REFERENCES,0.324582338902148,"[63] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022."
REFERENCES,0.32577565632458233,"[64] Mats Rudemo. Empirical choice of histograms and kernel density estimators. Scandinavian
Journal of Statistics, pages 65–78, 1982."
REFERENCES,0.3269689737470167,"[65] Hui Dong and Marvin K Nakayama. A tutorial on quantile estimation via monte carlo. Monte
Carlo and Quasi-Monte Carlo Methods: MCQMC 2018, Rennes, France, July 1–6, pages 3–30,
2020."
REFERENCES,0.32816229116945106,"[66] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks. In International Conference on Learning Representations, 2020."
REFERENCES,0.32935560859188545,"[67] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks
for one shot learning. Advances in neural information processing systems, 29, 2016."
REFERENCES,0.3305489260143198,"[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning,
pages 8748–8763. PMLR, 2021."
REFERENCES,0.3317422434367542,"[69] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fa-
had Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 19113–19122, 2023."
REFERENCES,0.3329355608591885,"[70] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported trust region
optimization for offline reinforcement learning. In International Conference on Machine
Learning, pages 23829–23851. PMLR, 2023."
REFERENCES,0.3341288782816229,"[71] Hongchang Zhang, Yixiu Mao, Boyuan Wang, Shuncheng He, Yi Xu, and Xiangyang Ji. In-
sample actor critic for offline reinforcement learning. In The Eleventh International Conference
on Learning Representations, 2023."
REFERENCES,0.3353221957040573,"[72] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji.
Supported value
regularization for offline reinforcement learning. Advances in Neural Information Processing
Systems, 36, 2024."
REFERENCES,0.33651551312649164,"[73] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on stochastic
programming: modeling and theory. Society for industrial Mathematics, 2009."
REFERENCES,0.33770883054892603,"[74] Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang Ji. Counterfactual
conservative q learning for offline multi-agent reinforcement learning. Advances in Neural
Information Processing Systems, 36, 2024."
REFERENCES,0.33890214797136037,"[75] Yun Qu, Boyuan Wang, Jianzhun Shao, Yuhang Jiang, Chen Chen, Zhenbin Ye, Liu Linc,
Yang Feng, Lin Lai, Hongyang Qin, et al. Hokoff: real game dataset from honor of kings and
its offline reinforcement learning benchmarks. Advances in Neural Information Processing
Systems, 36, 2024."
REFERENCES,0.34009546539379476,"[76] Lorna I Paredes and Chew Tuan Seng. Controlled convergence theorem for banach-valued hl
integrals. Scientiae Mathematicae Japonicae, 56(2):347–358, 2002."
REFERENCES,0.3412887828162291,"[77] Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-
nonconcave minimax optimization? In International conference on machine learning, pages
4880–4889. PMLR, 2020."
REFERENCES,0.3424821002386635,"[78] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,
2004."
REFERENCES,0.3436754176610978,"[79] C Frappier and QI Rahman. On an inequality of s. bernstein. Canadian Journal of Mathematics,
34(4):932–944, 1982."
REFERENCES,0.3448687350835322,"[80] Rong Liu and Lijian Yang. Kernel estimation of multivariate cumulative distribution function.
Journal of Nonparametric Statistics, 20(8):661–677, 2008."
REFERENCES,0.3460620525059666,"[81] Srh Larochelle. Optimization as a model for few-shot learning. In International Conference on
Learning Representations, 2017."
REFERENCES,0.34725536992840095,"[82] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenen-
baum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot
classification. arXiv preprint arXiv:1803.00676, 2018."
REFERENCES,0.34844868735083534,"[83] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural
adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 15262–15271, 2021."
REFERENCES,0.3496420047732697,"[84] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global repre-
sentations by penalizing local predictive power. Advances in Neural Information Processing
Systems, 32, 2019."
REFERENCES,0.35083532219570407,"[85] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-
son, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,
Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
Py-
torch: An imperative style, high-performance deep learning library. In Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pages 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
bdbca288fee7f92f2bfa9f7012727740-Abstract.html."
REFERENCES,0.3520286396181384,"[86] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. {TensorFlow}: a system for
{Large-Scale} machine learning. In 12th USENIX symposium on operating systems design and
implementation (OSDI 16), pages 265–283, 2016."
REFERENCES,0.3532219570405728,Contents
INTRODUCTION,0.35441527446300713,"1
Introduction
1"
LITERATURE REVIEW,0.3556085918854415,"2
Literature Review
2"
LITERATURE REVIEW,0.3568019093078759,"2.1
Meta Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2"
LITERATURE REVIEW,0.35799522673031026,"2.2
Robustness & Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2"
PRELIMINARIES,0.35918854415274465,"3
Preliminaries
3"
PRELIMINARIES,0.360381861575179,"3.1
Risk Minimization Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3"
PRELIMINARIES,0.3615751789976134,"3.2
Examples & Two-stage Heuristic Strategies . . . . . . . . . . . . . . . . . . . . .
4"
THEORETICAL INVESTIGATIONS,0.3627684964200477,"4
Theoretical Investigations
4"
THEORETICAL INVESTIGATIONS,0.3639618138424821,"4.1
Distributionally Robust Meta Learning as a Stackelberg Game . . . . . . . . . . .
5"
THEORETICAL INVESTIGATIONS,0.36515513126491644,"4.2
Solution Concept & Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6"
CONVERGENCE RATE & GENERALIZATION BOUND,0.36634844868735084,"4.3
Convergence Rate & Generalization Bound
. . . . . . . . . . . . . . . . . . . . .
6"
CONVERGENCE RATE & GENERALIZATION BOUND,0.36754176610978523,"4.4
Practical Enhancements & Implementations . . . . . . . . . . . . . . . . . . . . .
7"
EMPIRICAL FINDINGS,0.36873508353221957,"5
Empirical Findings
8"
EMPIRICAL FINDINGS,0.36992840095465396,"5.1
Sinusoid Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8"
EMPIRICAL FINDINGS,0.3711217183770883,"5.2
System Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9"
FEW-SHOT IMAGE CLASSIFICATION,0.3723150357995227,"5.3
Few-shot Image Classification
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
9"
META REINFORCEMENT LEARNING,0.373508353221957,"5.4
Meta Reinforcement Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
9"
META REINFORCEMENT LEARNING,0.3747016706443914,"5.5
Assessment of Quantile Estimators . . . . . . . . . . . . . . . . . . . . . . . . . .
10"
META REINFORCEMENT LEARNING,0.37589498806682575,"5.6
Empricial Result Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . .
10"
META REINFORCEMENT LEARNING,0.37708830548926014,"5.7
Compatibility with Large Models . . . . . . . . . . . . . . . . . . . . . . . . . . .
10"
CONCLUSION,0.37828162291169454,"6
Conclusion
10"
CONCLUSION,0.3794749403341289,"A Quick Guide to This Work
19"
CONCLUSION,0.38066825775656327,"A.1 Technical Comparison in Robust Fast Adaptation . . . . . . . . . . . . . . . . . .
19"
CONCLUSION,0.3818615751789976,"A.2
Significance of Theoretical Understandings
. . . . . . . . . . . . . . . . . . . . .
19"
CONCLUSION,0.383054892601432,"A.3
Meanings of Indicators and Terms . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
CONCLUSION,0.38424821002386633,"A.4
Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
CONCLUSION,0.3854415274463007,"A.5
Broader Impact & Future Extensions . . . . . . . . . . . . . . . . . . . . . . . . .
20"
CONCLUSION,0.38663484486873506,"B
Pseudo Algorithms
20"
CONCLUSION,0.38782816229116945,"C Expressions, Theorems & Proofs
21"
CONCLUSION,0.38902147971360385,"C.1
Characterization of Optimization Processes
. . . . . . . . . . . . . . . . . . . . .
21"
CONCLUSION,0.3902147971360382,"C.2
Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22"
CONCLUSION,0.3914081145584726,"C.3
Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
CONCLUSION,0.3926014319809069,"C.4
Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
CONCLUSION,0.3937947494033413,"C.5
Proof of Quasi-concavity for F(q, θ) w.r.t. q . . . . . . . . . . . . . . . . . . . . .
24"
CONCLUSION,0.39498806682577564,"C.6
Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24"
CONCLUSION,0.39618138424821003,"C.7
Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25"
CONCLUSION,0.39737470167064437,"C.8
Proof of Theorem 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26"
CONCLUSION,0.39856801909307876,"C.9
Proof of Theorem 4.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28"
CONCLUSION,0.3997613365155131,"C.10 Additional Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28"
CONCLUSION,0.4009546539379475,"D Implementation Details
30"
CONCLUSION,0.4021479713603819,"D.1
Benchmark Details & Neural Architectures & Opensource Codes . . . . . . . . . .
30"
CONCLUSION,0.4033412887828162,"D.2
Modules in Python
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31"
CONCLUSION,0.4045346062052506,"E Additional Experimental Results
32"
CONCLUSION,0.40572792362768495,"E.1
Evaluation with Other Robust Meta Learners
. . . . . . . . . . . . . . . . . . . .
32"
CONCLUSION,0.40692124105011934,"E.2
Numeric Results in Tables and Histograms . . . . . . . . . . . . . . . . . . . . . .
32"
CONCLUSION,0.4081145584725537,"E.3
Sensitivity Analysis to Confidence Level . . . . . . . . . . . . . . . . . . . . . . .
33"
CONCLUSION,0.40930787589498807,"E.4
Further Exploration on Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . .
35"
CONCLUSION,0.4105011933174224,"F
Computational Platforms & Softwares
35"
CONCLUSION,0.4116945107398568,"A
Quick Guide to This Work"
CONCLUSION,0.4128878281622912,This section mainly includes explanations and clarifications on this work.
CONCLUSION,0.41408114558472553,"A.1
Technical Comparison in Robust Fast Adaptation"
CONCLUSION,0.4152744630071599,"Table 3: A summary of robust fast adaptation methods. We take MAML as an example, list related
methods, and report their characteristics in literature. We mainly report the statistics according to
whether existing literature works include the generalization analysis and convergence analysis. The
form of meta learner and the robustness type are generally connected."
CONCLUSION,0.41646778042959426,"Principle
Meta Learner
Generalization Convergence
Robustness Type"
CONCLUSION,0.41766109785202865,"MAML
minθ∈Θ Ep(τ)
h
ℓ(DQ
τ , DS
τ ; θ)
i"
CONCLUSION,0.418854415274463,"✓
✓
−−"
CONCLUSION,0.4200477326968974,"DRO-MAML [66]
maxq(τ)∈Q minθ∈Θ Eq(τ)
h
ℓ(DQ
τ , DS
τ ; θ)
i"
CONCLUSION,0.4212410501193317,"✗
✗
Uncertainty Set (Not tail risk)"
CONCLUSION,0.4224343675417661,"TR-MAML [42]
minθ∈Θ maxτ∈T ℓ(DQ
τ , DS
τ ; θ)
✓
✓
Worst-Case Task"
CONCLUSION,0.4236276849642005,"DR-MAML [1]
minθ∈Θ Epα(τ;θ)
h
ℓ(DQ
τ , DS
τ ; θ)
i"
CONCLUSION,0.42482100238663484,"✗
✗
Tail Task Risk"
CONCLUSION,0.42601431980906923,"DR-MAML+(Ours) maxq(τ)∈Qα minθ∈Θ Eq(τ)
h
ℓ(DQ
τ , DS
τ ; θ)
i"
CONCLUSION,0.42720763723150357,"✓
✓
Tail Task Risk"
CONCLUSION,0.42840095465393796,"Primary differences: As far as we know, literature work is quite limited regarding fast adaptation
robustness in the task space. TR-MAML and DR-MAML are the most recent and typical ones
that can handle task distributional shift scenarios well. As reported in Table 3, TR-MAML only
focuses on the worst-case, which considers a bit extreme and rarely occurred cases. DRO-MAML
is a new baseline, where the uncertainty set Q is included for robust fast adaptation, hence there
exists no theoretical analysis. As for the tail task risk, DR-MAML lacks generalization capability and
convergence rate analysis w.r.t. the meta learner. The meta learner in DR-MAML+ is a more specific
instantiation of that in DR-MAML. We claim that these theoretical understanding is necessary in the
presence of the robust fast adaptation due to its potential applications in large models."
CONCLUSION,0.4295942720763723,"Theoretical and empirical insights: In comparison, this work not only contributes to the Stackelberg
game for estimates, but also derives the generalization and the asymptotic performance gap in
iterations based on a normalized but non-differentiable probability density space. Note that we
lean more focus on theoretical understanding and pursuing SOTA performance is not the ultimate
purpose of this work. The connections between different quantile estimators and generalization
bound highlighted in Theorem 4.3/4.4 and Remark 1 reveal the theoretical advantage of KDEs over
crude Monte Carlo methods. This motivates us to replace crude Monte Carlo with KDEs. Such
a replacement as a simple implementation trick is supported by rigorous theoretical analysis. The
empirical results align with theoretical understanding."
CONCLUSION,0.4307875894988067,"In terms of improving the studied strategy, investigations in extensive experiments seem meaningful
for practical implementations, and some non-trivial discoveries together with improvement tricks are
also reported, such as the relationship between quantile estimate errors and adaptation robustness,
the batch size’s influence on several benchmarks, etc."
CONCLUSION,0.431980906921241,"In this work, the theoretical and empirical parts are connected in an implicit manner. The generaliza-
tion capability is empirically examined from experimental results, and the performance gap between
DR-MAML+ and DR-MAML can be attributed to the difference in generalization bounds. As for the
convergence trait and asymptotic performance, the insight might guide the optimization process in
training large models, such as early stopping criteria design."
CONCLUSION,0.4331742243436754,"A.2
Significance of Theoretical Understandings"
CONCLUSION,0.4343675417661098,"As pointed out in [3, 61, 62], large language models are few-shot learners. When a large model, such
as a large decision-making model in the future, comes into practice, fast adaptation robustness can be
a crucial issue as real-world scenarios are indeed risk-sensitive."
CONCLUSION,0.43556085918854415,"This work takes the latest work [1] as an example, and the interest is in the theoretical aspect. Most of
the assumptions in this work are from [1]. The baselines are typical and latest, while the benchmarks"
CONCLUSION,0.43675417661097854,"cover diverse downstream tasks. In multimodal few-shot image classification experiments, our
contributed points help guide the development of large models in terms of training and robustness
enhancement. Our investigations also provide insight into robust policy optimization, particularly
when safety is one necessary consideration [70–72]."
CONCLUSION,0.4379474940334129,"Shapiro et al.’s book [73] is a comprehensive resource that addresses stochastic modeling and
optimization methods, but it does not explore solution concepts in game theory or define generalization
bounds relevant to meta learning and deep learning. Instead, our work further enriches the stochastic
programming theory in meta learning, connects it to the Stackelberg game, and contributes to tail risk
generalization bounds, convergence rates, asymptotic properties, and so on. Therefore, the solution
concept and the theoretical properties are specific to our meta learning setup, distinctly from the
scope covered by [73]."
CONCLUSION,0.43914081145584727,"A.3
Meanings of Indicators and Terms"
CONCLUSION,0.4403341288782816,"Illustration of VaRα, CDF and others: Fig. 7 illustrates a typical probability distribution, cumula-
tive distribution, and the resulting mean, VaRα, and CVaRα. Given α ∈[0, 1), VaRα is the α quantile
of the risk distribution. Specially, VaR0.5 coincides with the mean. Upon the definition of VaRα,
CVaRα can be define as CVaRα = Ep(τ)
h
ℓ|ℓ≥VaRα
i
. That is, CVaRα is the expectation of the
risks of the 1 −α tail of the distribution. Relative to the original probability distribution, CVaRα can
be interpreted as a certain distribution shift, which reweighs arbitrary risk exceeding VaRα up to a
coefficient
1
1−α."
CONCLUSION,0.441527446300716,"Meaning of the asymptotic performance gap: We plot Fig. 8 to display the gap between the CVaRα
value in iterations and that in the convergence. The area difference depicts this gap."
CONCLUSION,0.44272076372315033,"A.4
Computational Complexity"
CONCLUSION,0.4439140811455847,"Analyzing computational complexity across all meta-learning methods is inherently challenging
due to the diversity in methodological approaches within the field. Meta-learning encompasses
a wide range of techniques, including gradient-based methods, which rely on iterative updates
to model parameters, and non-parametric methods, which may instead focus on instance-based
learning or kernel-based approaches. Therefore, the space complexity is specific to the meta-learning
method, while this work is agnostic to it. Here, we report the computational complexity for the DR-
MAML+ as O

B(B −α|M|)

while using KDE with the Gaussian kernel, and that of DR-MAML"
CONCLUSION,0.4451073985680191,"is O

B(log(B) −α|M|)

."
CONCLUSION,0.44630071599045346,"A.5
Broader Impact & Future Extensions"
CONCLUSION,0.44749403341288785,"This paper presents work whose goal is to advance the field of robust meta learning. There are many
potential societal consequences of our work, which we detail as follows."
CONCLUSION,0.4486873508353222,"The fast adaptation robustness is an urgent concern, particularly in large models and risk-sensitive
control. This work provides versatile insights for theoretical analysis and performance improvement
in the presence of tail task risk, and future explorations can be decision-making scenarios, such as
multi-agent policy optimization [74, 75], and computational/memory cost reduction."
CONCLUSION,0.4498806682577566,"B
Pseudo Algorithms"
CONCLUSION,0.4510739856801909,"For a better understanding of the game theoretical optimization, we take DR-MAML+ and DR-CNP+
as examples and include the Pseudo Algorithms 1/2 in this section. Particularly, the algorithms
specify the decision-making orders and highlight the use of KDE modules to build task risk value
distributions and estimate the quantile."
CONCLUSION,0.4522673031026253,"Figure 7: Diagram of risk concepts in this work. Here, the x-axis is the task risk value in fast
adaptation given a specific θ. The shadow-lined region illustrates the tail risk with a probability 1 −α
in the probability density. The area of the shadow-lined region after 1 −α normalization corresponds
to the expected tail risk CVaRα."
CONCLUSION,0.45346062052505964,"Figure 8: Illustration of the asymptotic behavior in approximating the equilibrium. Here, the
x-axis is the feasible task risk value in fast adaptation. The dark blue region indicates the histogram of
the task risk values in the local Stackelberg equilibrium (q∗, θ∗). The shallow blue region describes the
histogram of the task risk values at some iterated point (qT −1, θmeta
T
). The sets T1 and T2 respectively
collect the tasks resulting the opposite order."
CONCLUSION,0.45465393794749404,"C
Expressions, Theorems & Proofs"
CONCLUSION,0.45584725536992843,"C.1
Characterization of Optimization Processes"
CONCLUSION,0.45704057279236276,"Without loss of generality, we can also express the process of solving the studied Stackelberg game
as:"
CONCLUSION,0.45823389021479716,"max
q∈Qα F(q, θ∗(q))
s.t. θ∗(q) = arg min
θ∈Θ F(q, θ)
(13a: Leader’s Decision-Making)"
CONCLUSION,0.4594272076372315,"min
θ∈Θ F(q, θ),
(13b: Follower’s Decision-Making)"
CONCLUSION,0.4606205250596659,"where the optimization w.r.t. (q, θ) is the computation of the best responses for two adversarial
players. As a bi-level optimization in Eq. (14a)/(14b), the exact solution is intractable to obtain in a
theoretical sense, and the two-stage distributionallly robust optimization is a heuristic approach."
CONCLUSION,0.4618138424821002,"Meaning of the obtained equilibrium. Here, we can interpret the obtained solution (q∗, θ∗) from
solving Eq. (7) as follows. Given the follower’s decision θ∗and the induced task risk distribution
Fℓ(l; θ∗), the leader cannot further raise a proposal of a task subset with a probability 1 −α to
degradde the tailed expected performance. And this explains the meaning of robust fast adaptation
solution w.r.t. the tail task risk."
CONCLUSION,0.4630071599045346,"Algorithm 1: Meta-training DR-MAML+ as A Stackelberg Game
Input
:Task distribution p(τ); Confidence level α; Task batch size B; Learning rates: λ1 and
λ2.
Output :Meta-trained model parameter θ.
Randomly initialize the model parameter θ;
while not converged do"
CONCLUSION,0.46420047732696895,"Sample a batch of tasks {τi}B
i=1 ∼p(τ);
# The Leader Player’s Decision-Making
for i = 1 to B do"
CONCLUSION,0.46539379474940334,// inner loop via gradient descent as the fast adaptation
CONCLUSION,0.4665871121718377,"Evaluate the gradient: ∇θℓ(DS
τi; θ) in Eq. (5);
Perform task-specific gradient updates:
θi ←θ −λ1∇θℓ(DS
τi; θ);
end
// model the task risk distribution and estimate the quantile"
CONCLUSION,0.4677804295942721,"Evaluate performance LB = {ℓ(DQ
τi; θi)}B
i=1;
Estimate VaRα[ℓ(T , θ)] and set ξ = ˆξα in Eq. (5) with kernel density estimators;
Screen the subset L ˆ
B = {ℓ(DQ
ˆτi; θi)}K
i=1 with ˆξα for meta initialization updates;
# The Follower Player’s Decision-Making
Execute outer loop via gradient descent to increase adaptation robustness:
θ ←θ −λ2∇θ
PK
i=1 ℓ(DQ
ˆτi; θi) in Eq. (5);
end"
CONCLUSION,0.46897374701670647,"Algorithm 2: Meta Training DR-CNP+ as A Stackelberg Game
Input
:Task distribution p(τ); Confidence level α; Task batch size B; Learning rate λ.
Output :Meta-trained model parameter θ.
Randomly initialize the model parameter θ;
while not converged do"
CONCLUSION,0.4701670644391408,"Sample a batch of tasks {τi}B
i=1 ∼p(τ);
# The Leader Player’s Decision-Making
// model the task risk distribution and estimate the quantile"
CONCLUSION,0.4713603818615752,"Evaluate performance LB = {ℓ(DQ
τi; z, θi)}B
i=1;
Estimate VaRα[ℓ(T , θ)] ≈ˆξα with kernel density estimators;
Screen the subset L ˆ
B = {ℓ(DQ
ˆτi; z, θ)}K
i=1 with ˆξα for meta initialization updates;
# The Follower Player’s Decision-Making
Execute gradient descent to increase adaptation robustness:
θ ←θ −λ∇θ
PK
i=1 ℓ(DQ
ˆτi; z, θ);
end"
CONCLUSION,0.47255369928400953,"C.2
Assumptions"
CONCLUSION,0.4737470167064439,"We list all of the assumptions mentioned in this work. These assumptions further serve the demon-
stration of propositions and theorems in the main paper."
CONCLUSION,0.47494033412887826,"Assumption 1. To proceed, we retain most assumptions from [1] for theoretical analysis, including:"
CONCLUSION,0.47613365155131265,"1. The meta risk function ℓ(DQ
τ , DS
τ ; θ) is βτ-Lipschitz continuous w.r.t. θ;
2. The cumulative distribution Fℓ(l; θ) is βℓ-Lipschitz continuous w.r.t. l, and the normalized
density function pα(τ; θ) is βθ-Lipschitz continuous w.r.t. θ;
3. For arbitrary valid θ ∈Θ and corresponding pα(τ; θ), ℓ(DQ
τ , DS
τ ; θ) is bounded:
supτ∈Ωα,τ ℓ(DQ
τi, DS
τi; θ) ≤Lmax."
CONCLUSION,0.477326968973747,"Assumption 2. The implicit function h(·) is βh-Lipschitz continuous w.r.t. θ ∈Θ, and ∇θF(q, θ) is
βq-Lipschitz continuous w.r.t. q ∈Qα."
CONCLUSION,0.4785202863961814,"C.3
Proof of Proposition 1"
CONCLUSION,0.4797136038186158,Proposition 1. The uncertainty set Qα is convex and compact in terms of probability measures.
CONCLUSION,0.4809069212410501,"Proof: We firstly focus on the convexity of Qα. For any {q1 := q1(τ), q2 := q2(τ)} ∈Qα, we
partition these two task spaces with non-zero sampling probability mass respectively as T1 ∪TC and
T2 ∪TC. As displayed in Fig. 9, TC denotes the shared subset task between q1 and q2. Below we
show that λ1q1 + λ2q2 ∈Qα with λ1 + λ2 = 1. This is true because Z"
CONCLUSION,0.4821002386634845,"τ∈Tλ1q1+λ2q2
p(τ)dτ
(15a) =
Z"
CONCLUSION,0.48329355608591884,"τ /∈Tq1∪Tq2
p(τ)dτ +
Z τ∈TC"
CONCLUSION,0.48448687350835323,"
λ1p(τ) + λ2p(τ)

dτ +
Z"
CONCLUSION,0.48568019093078757,"τ∈T1
λ1p(τ)dτ +
Z"
CONCLUSION,0.48687350835322196,"τ∈T2
λ2p(τ)dτ (15b)"
CONCLUSION,0.4880668257756563,= 0 + λ1 Z
CONCLUSION,0.4892601431980907,"τ∈TC
p(τ)dτ +
Z"
CONCLUSION,0.4904534606205251,"τ∈T1
p(τ)dτ

+ λ2 Z"
CONCLUSION,0.4916467780429594,"τ∈TC
p(τ)dτ +
Z"
CONCLUSION,0.4928400954653938,"τ∈T2
p(τ)dτ

(15c) = λ1 Z"
CONCLUSION,0.49403341288782815,"τ∈Tq1
p(τ)dτ + λ2 Z"
CONCLUSION,0.49522673031026254,"τ∈Tq2
p(τ)dτ
(15d)"
CONCLUSION,0.4964200477326969,"= 1 −α.
(15e)"
CONCLUSION,0.49761336515513127,"We next demonstrate the compactness of Qα. The distance between two distributions ∀{q1, q2} ∈Qα
can be defined as:"
CONCLUSION,0.4988066825775656,"dQα(q1, q2) :=
Z τ∈T"
CONCLUSION,0.5,"q1(τ) −q2(τ)
dτ."
CONCLUSION,0.5011933174224343,"Since L1 space is a Banach space, the compactness is equivalent to the closedness and Boundedness
of Qα. Considering a sequence {qn(τ) ∈Qα} with the resulting limitation is q∗(τ), following the
Controlled Convergence Theorem [76], we know that"
CONCLUSION,0.5023866348448688,"lim
n→∞ Z"
CONCLUSION,0.5035799522673031,"τ∈T
pn(τ) −p∗(τ)dτ ≤lim
n→∞ Z τ∈T"
CONCLUSION,0.5047732696897375,"pn(τ) −p∗(τ)
dτ = lim
n→∞dQα(qn, q∗) = 0.
(16)"
CONCLUSION,0.5059665871121718,"Due to the symmetry of the distance, we can have limn→∞
R"
CONCLUSION,0.5071599045346062,"τ∈T p∗(τ) −pn(τ)dτ ≤0. Thus, Z"
CONCLUSION,0.5083532219570406,"τ∈T
p∗(τ)dτ = lim
n→∞ Z"
CONCLUSION,0.5095465393794749,"τ∈T
pn(τ)dτ = 1 −α."
CONCLUSION,0.5107398568019093,"That is, p∗(τ) ∈Qα, indicating that Qα is a closed set. As the boundedness is clear in the studied
problem, this completes the proof of Proposition 1.
■"
CONCLUSION,0.5119331742243437,"C.4
Proof of Proposition 2"
CONCLUSION,0.513126491646778,"Proposition 2 (Existence of Equilibrium) Given the Assumption 1, there always exists the global
Stackelberg equilibrium as the Definition 1 for the studied SG."
CONCLUSION,0.5143198090692124,"Proof: Note that Θ is compact as a subspace of the Euclidean space. And it is trivial to see that
F(q, θ) := Eq
h
ℓ(DQ
τ , DS
τ ; θ)
i
is continuous w.r.t. θ ∈Θ as ℓsatisfies the βτ-Lipschitz continuity in
the Assumption 1."
CONCLUSION,0.5155131264916468,"Here we need to show the continuity of F(q, θ) w.r.t. the collection of probability measures or
probability functions Qα. To this end, with ∀θ ∈Θ fixed, We consider two metric spaces (Qα, dQα)
and (L, RL). The map of our interest is g(q) = F(q, ·) : Qα 7→L ⊆R+."
CONCLUSION,0.5167064439140812,"Figure 9: Partition of the task subspace. Here we take two probability measure {q1, q2} ∈Qα
for illustration. T1 ∪TC and T2 ∪TC defines the corresponding task subspaces for q1 and q2 with
non-zero probability mass in the whole space T ."
CONCLUSION,0.5178997613365155,"Naturally, we can have the following inequality:
g(q1) −g(q2)
 =
Eq1
h
ℓ(DQ
τ , DS
τ ; θ)
i
−Eq2
h
ℓ(DQ
τ , DS
τ ; θ)
i
(17a)"
CONCLUSION,0.5190930787589498,"≤

Z"
CONCLUSION,0.5202863961813843,"τ∈TC
[q1(τ) −q2(τ)]ℓ(DQ
τ , DS
τ ; θ)dτ

(17b)"
CONCLUSION,0.5214797136038186,"+

Z"
CONCLUSION,0.522673031026253,"τ∈T1
q1(τ)ℓ(DQ
τ , DS
τ ; θ)dτ −
Z"
CONCLUSION,0.5238663484486874,"τ∈T2
q2(τ)ℓ(DQ
τ , DS
τ ; θ)dτ

(17c) ≤
Z τ∈TC"
CONCLUSION,0.5250596658711217,"q1(τ) −q2(τ)
ℓ(DQ
τ , DS
τ ; θ)dτ
(17d) +
Z τ∈T1"
CONCLUSION,0.5262529832935561,"q1(τ) −q2(τ)
ℓ(DQ
τ , DS
τ ; θ)dτ +
Z τ∈T2"
CONCLUSION,0.5274463007159904,"q1(τ) −q2(τ)
ℓ(DQ
τ , DS
τ ; θ)dτ
(17e)"
CONCLUSION,0.5286396181384249,≤3Lmax Z τ∈T
CONCLUSION,0.5298329355608592,"q1(τ) −q2(τ)
dτ = 3LmaxdQα(q1, q2),
(17f)"
CONCLUSION,0.5310262529832935,which implies 3Lmax-Lipschitz continuity of g(q) w.r.t. ∀q ∈Qα.
CONCLUSION,0.5322195704057279,"According to the Remark in [77], there always exists the global Stackelberg equilibrium as the
Definition 1 when Qα × Θ is compact and F(q, θ) is continuous. This completes the proof of
Proposition 2.
■"
CONCLUSION,0.5334128878281623,"C.5
Proof of Quasi-concavity for F(q, θ) w.r.t. q"
CONCLUSION,0.5346062052505967,"It can be validated that F(q, θ) is a quasi-concave function w.r.t. q, meaning that for any positive
number l ∈R+, the set {q|q ∈Qα, F(q, θ) > l} is convex in Qα."
CONCLUSION,0.535799522673031,"Proof: According to the conventional definition (i.e., the superlevel set is convex [78]), for all
λ1 + λ2 = 1, q1, q2 ∈{q|F(q, θ) > l}, we can have"
CONCLUSION,0.5369928400954654,"F(λ1q1 + λ2q2, θ) = Eλ1q1+λ2q2
h
ℓ(DQ
τ , DS
τ ; θ)
i
(18a)"
CONCLUSION,0.5381861575178998,"= λ1Eq1
h
ℓ(DQ
τ , DS
τ ; θ)
i
+ λ2Eq2
h
ℓ(DQ
τ , DS
τ ; θ)
i
(18b)"
CONCLUSION,0.5393794749403341,"= λ1F(q1, θ) + λ2F(q2, θ)
(18c)
> l.
(18d)"
CONCLUSION,0.5405727923627685,"Thus, λ1q1 + λ2q2 ∈{q|F(q, θ) > l} and the superlevel set is convex, implying that F(q, θ) is
quasi-concave w.r.t. q.
■"
CONCLUSION,0.5417661097852029,"C.6
Proof of Theorem 4.1"
CONCLUSION,0.5429594272076372,"Theorem 4.1 (Convergence Rate for the Second Player) Let the iteration sequence in opti-
mization be:
· · ·
7→
{qt−1, θt}
7→
{qt, θt+1}
7→
· · ·
7→
{q∗, θ∗}, with the converged
equilibirum (q∗, θ∗).
Under the Assumption 2 and suppose that ||I −λ∇2
θθF(q∗, θ∗)||2 <"
CONCLUSION,0.5441527446300716,"1 −λβqβh, we can have limt→∞
||θt+1−θ∗||2"
CONCLUSION,0.545346062052506,"||θt−θ∗||2
≤1, and the iteration converges with the rate
 
||I −λ∇2
θθF(q∗, θ∗)||2 + λβqβh

."
CONCLUSION,0.5465393794749404,"Proof: Let the resulting stationary point be [q∗, θ∗], we denote the difference terms by ˆq = q −q∗
and ˆθ = θ −θ∗. Then, according to the optimization step, we can have the following equations:"
CONCLUSION,0.5477326968973747,"θt+1 = θt −λ∇θF(qt; θt) =⇒ˆθt+1 = ˆθt −λ∇θF(qt; θt).
(19)"
CONCLUSION,0.548926014319809,"Now we perform the first-order Taylor expansion of the θ related function ∇θF(qt; θ) around θ∗and
can derive:"
CONCLUSION,0.5501193317422435,"∇θF(qt; θ) = ∇θF(qt; θ∗) + ∇2
θθF(qt; θ∗)(θ −θ∗) + O(||θ −θ∗||)
(20a)"
CONCLUSION,0.5513126491646778,"∇θF(qt; θt) ≃∇θF(qt; θ∗) + ∇2
θθF(qt; θ∗)(θt −θ∗).
(20b)"
CONCLUSION,0.5525059665871122,Then we have the following result with the help of Assumption 2:
CONCLUSION,0.5536992840095465,"∥∇θF(qt; θ∗)∥2 = ∥∇θF(qt; θ∗) −∇θF(q∗; θ∗)∥2
(21a)
= ∥∇θF(h(θt); θ∗) −∇θF(h(θ∗); θ∗)∥2
(21b)
≤βqdQα(h(θt), h(θ∗))
(21c)
≤βqβh∥θt −θ∗∥2.
(21d)"
CONCLUSION,0.5548926014319809,"With Eq. (19), Eq. (20) and Eq. (21), we can derive the equation that:"
CONCLUSION,0.5560859188544153,"ˆθt+1 = ˆθt −λ∇θF(qt; θt)
(22a)"
CONCLUSION,0.5572792362768496,"= ˆθt −λ
h
∇θF(qt; θ∗) + ∇2
θθF(qt; θ∗)ˆθt
i
(22b)"
CONCLUSION,0.5584725536992841,"=
h
I −λ∇2
θθF(qt; θ∗)
i
ˆθt −λ∇θF(qt; θ∗)
(22c)"
CONCLUSION,0.5596658711217184,"=⇒||ˆθt+1||2 ≤∥I −λ∇2
θθF(qt; θ∗)∥2||ˆθt||2 + λ∥∇θF(qt; θ∗)∥2
(22d)"
CONCLUSION,0.5608591885441527,"≤
 
∥I −λ∇2
θθF(qt; θ∗)∥2 + λβqβh

||∥ˆθt∥2
(22e)"
CONCLUSION,0.5620525059665871,"Thus, when ∥I −λ∇2
θθF(q∗; θ∗)∥2 < 1 −λβqβh, we have"
CONCLUSION,0.5632458233890215,"lim
t→∞
||ˆθt+1||2"
CONCLUSION,0.5644391408114559,"||ˆθt||2
≤lim
t→∞∥I −λ∇2
θθF(qt; θ∗)∥2 + λβqβh
(23a)"
CONCLUSION,0.5656324582338902,"= ∥I −λ∇2
θθF(q∗; θ∗)∥2 + λβqβh
(23b)
< 1.
(23c)"
CONCLUSION,0.5668257756563246,"This completes the proof of Theorem 4.1.
■"
CONCLUSION,0.568019093078759,"C.7
Proof of Theorem 4.2"
CONCLUSION,0.5692124105011933,"Theorem 4.2 (Asymptotics in the Tail Risk Cases) Under the Assumption 1 and given a batch of
tasks {τi}B
i=1, we can have"
CONCLUSION,0.5704057279236276,"CVaRα(θmeta
T
) −CVaRα(θ∗) ≤βτ∥θmeta
T
−θ∗∥+ VaR∗
α
1 −α"
CONCLUSION,0.5715990453460621,"
P(T1) −P(T2)

,
(24)"
CONCLUSION,0.5727923627684964,"where T1 = {τ : ℓ∗< VaR∗
α, ℓmeta ≥VaRmeta
α
}, T2 = {τ : ℓ∗≥VaR∗
α, ℓmeta < VaRmeta
α
}."
CONCLUSION,0.5739856801909308,"Proof. Given a batch of tasks {τ1, · · · , τB} and according to the definition of CVaRα, we have"
CONCLUSION,0.5751789976133651,"CVaRα(θmeta
T
) −CVaRα(θ∗)
(25a)"
CONCLUSION,0.5763723150357996,"=
1
1 −α Z"
CONCLUSION,0.5775656324582339,"{τ:ℓmeta≥VaRmeta
α
}
ℓmetap(τ)dτ −
1
1 −α Z"
CONCLUSION,0.5787589498806682,"{τ:ℓ∗≥VaR∗
α}
ℓ∗p(τ)dτ
(25b) =
Z"
CONCLUSION,0.5799522673031027,"τ
ℓmetapα(τ; θmeta
T
)dτ −
Z"
CONCLUSION,0.581145584725537,"τ
ℓ∗pα(τ; θ∗)dτ
(25c) =
Z τ"
CONCLUSION,0.5823389021479713,"
ℓmetapα(τ; θmeta
T
) −ℓ∗pα(τ; θmeta
T
)

dτ +
Z τ"
CONCLUSION,0.5835322195704057,"
ℓ∗pα(τ; θmeta
T
) −ℓ∗pα(τ; θ∗)

dτ
(25d) =
Z τ"
CONCLUSION,0.5847255369928401,"
ℓmeta −ℓ∗
pα(τ; θmeta
T
)dτ +
Z"
CONCLUSION,0.5859188544152745,"τ
ℓ∗
pα(τ; θmeta
T
) −pα(τ; θ∗)

dτ
(25e)"
CONCLUSION,0.5871121718377088,"≤βτ∥θmeta
T
−θ∗∥+
Z"
CONCLUSION,0.5883054892601431,"T1∪T2∪T3∪T4
ℓ∗
pα(τ; θmeta
T
) −pα(τ; θ∗)

dτ
(25f)"
CONCLUSION,0.5894988066825776,"= βτ∥θmeta
T
−θ∗∥+
Z"
CONCLUSION,0.5906921241050119,"T1∪T2
ℓ∗
pα(τ; θmeta
T
) −pα(τ; θ∗)

dτ
(25g)"
CONCLUSION,0.5918854415274463,"= βτ∥θmeta
T
−θ∗∥+
Z"
CONCLUSION,0.5930787589498807,"T1
ℓ∗pα(τ; θmeta
T
)dτ −
Z"
CONCLUSION,0.594272076372315,"T2
ℓ∗pα(τ; θ∗)dτ
(25h)"
CONCLUSION,0.5954653937947494,"= βτ∥θmeta
T
−θ∗∥+
1
1 −α Z"
CONCLUSION,0.5966587112171837,"T1
ℓ∗p(τ)dτ −
1
1 −α Z"
CONCLUSION,0.5978520286396182,"T2
ℓ∗p(τ)dτ
(25i)"
CONCLUSION,0.5990453460620525,"≤βτ∥θmeta
T
−θ∗∥+ VaR∗
α
1 −α Z"
CONCLUSION,0.6002386634844868,"T1
p(τ)dτ −VaR∗
α
1 −α Z"
CONCLUSION,0.6014319809069213,"T2
p(τ)dτ
(25j)"
CONCLUSION,0.6026252983293556,"= βτ∥θmeta
T
−θ∗∥+ VaR∗
α
1 −αP(T1) −VaR∗
α
1 −αP(T2).
(25k)"
CONCLUSION,0.60381861575179,"In inequality (25f), T1 = {τ : ℓ∗< VaR∗
α, ℓmeta ≥VaRmeta
α
}, T2 = {τ : ℓ∗≥VaR∗
α, ℓmeta <
VaRmeta
α
}, T3 = {τ : ℓ∗< VaR∗
α, ℓmeta < VaRmeta
α
}, T4 = {τ : ℓ∗≥VaR∗
α, ℓmeta ≥VaRmeta
α
}.
Moreover, this inequality holds due to the βτ−Lipschitz continuous of ℓ(Dτ; θ). In Eq. (25g),
pα(τ; θmeta
T
) = pα(τ; θ∗) = 0 when τ ∈T3, and pα(τ; θmeta
T
) = pα(τ; θ∗) = p(τ)"
CONCLUSION,0.6050119331742243,"1−α when τ ∈T4.
Thus, we complete the proof of Theorem 4.2.
■"
CONCLUSION,0.6062052505966588,"C.8
Proof of Theorem 4.3"
CONCLUSION,0.6073985680190931,"Theorem 4.3 (Generalization Bound in the Tail Risk Cases) Given a collection of task samples
{τi}B
i=1 and corresponding meta datasets, we can derive the following generalization bound in the
presence of tail risk:"
CONCLUSION,0.6085918854415274,R(θ∗) ≤bR(θ∗) + s
CONCLUSION,0.6097852028639618,"2
 
α
1−αL2max + Vτi∼pα(τ)

ℓ(DQ
τi, DSτi; θ∗)

ln
  1 ϵ
 B"
CONCLUSION,0.6109785202863962,"+
1
3(1 −α)
Lmax B"
CONCLUSION,0.6121718377088305,"
2 ln
1 ϵ"
CONCLUSION,0.6133651551312649,"
+ 3αB

, (26)"
CONCLUSION,0.6145584725536993,"where the inequality holds with probability at least 1 −ϵ and ϵ ∈(0, 1), V[·] denotes the variance
operation, and Lmax is from the Assumption 1."
CONCLUSION,0.6157517899761337,"Proof. R(θ∗) −bR(θ∗) can be decomposed to two parts, i.e.,"
CONCLUSION,0.616945107398568,"R(θ∗) −bR(θ∗) =

R(θ∗) −bRw(θ∗)

+

bRw(θ∗) −bR(θ∗)

.
(27)"
CONCLUSION,0.6181384248210023,"For the first part (i.e., R(θ∗) −bRw(θ∗)), we will adopt the Bernstein’s inequality to provide an
upper bound. Regarding pα(τi)"
CONCLUSION,0.6193317422434368,"p(τi) ℓ(DQ
τi, DS
τi; θ∗) −R(θ∗) as a random variable with respect to τi and
according to Assumption 1, we know that"
CONCLUSION,0.6205250596658711,pα(τi)
CONCLUSION,0.6217183770883055,"p(τi) ℓ(DQ
τi, DS
τi; θ∗) −R(θ∗) ≤
1
1 −αℓ(DQ
τi, DS
τi; θ∗) −R(θ∗) ≤
1
1 −αLmax.
(28)"
CONCLUSION,0.6229116945107399,"Thus, following Bernstein’s inequality [79], we know that P 1 B B
X i=1"
CONCLUSION,0.6241050119331742,pα(τi)
CONCLUSION,0.6252983293556086,"p(τi) ℓ(DQ
τi, DS
τi; θ∗) −R(θ∗)
 ≥ξ ! (29a)"
CONCLUSION,0.6264916467780429,"= P
 bRw(θ∗) −R(θ∗)
 ≥ξ

(29b) ≤exp "
CONCLUSION,0.6276849642004774,"−
Bξ2"
CONCLUSION,0.6288782816229117,"2Vτi
h
pα(τi)"
CONCLUSION,0.630071599045346,"p(τi) ℓ(DQ
τi, DSτi; θ∗) −R(θ∗)
i
+ 2"
CONCLUSION,0.6312649164677804,"3
1
1−αLmaxξ "
CONCLUSION,0.6324582338902148,",
(29c) where Vτi"
CONCLUSION,0.6336515513126492,pα(τi)
CONCLUSION,0.6348448687350835,"p(τi) ℓ(DQ
τi, DS
τi; θ∗) −R(θ∗)

(30a) = Vτi"
CONCLUSION,0.636038186157518,pα(τi)
CONCLUSION,0.6372315035799523,"p(τi) ℓ(DQ
τi, DS
τi; θ∗)

(30b) = Eτi"
CONCLUSION,0.6384248210023866,pα(τi)
CONCLUSION,0.639618138424821,"p(τi) ℓ(DQ
τi, DS
τi; θ∗)
2
−

Eτi"
CONCLUSION,0.6408114558472554,pα(τi)
CONCLUSION,0.6420047732696897,"p(τi) ℓ(DQ
τi, DS
τi; θ∗)
2
(30c) =
Z τ"
CONCLUSION,0.6431980906921241,pα(τ)
CONCLUSION,0.6443914081145584,"p(τ) ℓ(DQ
τ , DS
τ ; θ∗)
2
p(τ)dτ −
Z τ pα(τ)"
CONCLUSION,0.6455847255369929,"p(τ) ℓ(DQ
τ , DS
τ ; θ∗)p(τ)dτ
2
(30d) =
Z τ pα(τ)"
CONCLUSION,0.6467780429594272,"p(τ) ℓ(DQ
τ , DS
τ ; θ∗)2pα(τ)dτ −
Z"
CONCLUSION,0.6479713603818615,"τ
ℓ(DQ
τ , DS
τ ; θ∗)pα(τ)dτ
2
(30e)"
CONCLUSION,0.649164677804296,"=
1
1 −α Z"
CONCLUSION,0.6503579952267303,"τ
ℓ(DQ
τ , DS
τ ; θ∗)2pα(τ)dτ −
Z"
CONCLUSION,0.6515513126491647,"τ
ℓ(DQ
τ , DS
τ ; θ∗)pα(τ)dτ
2
(30f)"
CONCLUSION,0.652744630071599,"≤

1
1 −α −1
 Z"
CONCLUSION,0.6539379474940334,"τ
ℓ(DQ
τ , DS
τ ; θ∗)2pα(τ)dτ + Vτ∼pα(τ)

ℓ(DQ
τ , DS
τ ; θ∗)

(30g)"
CONCLUSION,0.6551312649164678,":=
α
1 −αL2
max + Vτ∼pα(τ).
(30h)"
CONCLUSION,0.6563245823389021,"Setting ϵ to match the upper bound in inequality (29c) shows that with probability at least 1 −ϵ, the
following bound holds:"
CONCLUSION,0.6575178997613366,"bRw(θ∗) −R(θ∗)
 ≤ s"
CONCLUSION,0.6587112171837709,"2(
α
1−αL2max + Vτ∼pα(τ)) ln
  1 ϵ
"
CONCLUSION,0.6599045346062052,"B
+ 2Lmax ln
  1 ϵ
"
CONCLUSION,0.6610978520286396,"3(1 −α)B .
(31)"
CONCLUSION,0.662291169451074,"For the second part (i.e., bRw(θ∗) −bR(θ∗)), we have"
CONCLUSION,0.6634844868735084,"bRw(θ∗) −bR(θ∗) = 1 B B
X i=1"
CONCLUSION,0.6646778042959427,pα(τi)
CONCLUSION,0.665871121718377,"p(τi) −δ(τi)

ℓ(DQ
τi, DS
τi; θ∗)
(32a) ≤Lmax B B
X i=1"
CONCLUSION,0.6670644391408115,pα(τi)
CONCLUSION,0.6682577565632458,"p(τi) −δ(τi)

(32b)"
CONCLUSION,0.6694510739856802,"= Lmax B B
X i=1"
CONCLUSION,0.6706443914081146,pα(τi)
CONCLUSION,0.6718377088305489,"p(τi) −1

δ(τi)
(32c)"
CONCLUSION,0.6730310262529833,"=
α
1 −α
Lmax B B
X"
CONCLUSION,0.6742243436754176,"i=1
δ(τi).
(32d)"
CONCLUSION,0.6754176610978521,"In summary, we can obtain an upper bound of R(θ∗) −bR(θ∗). That is,"
CONCLUSION,0.6766109785202864,"R(θ∗) −bR(θ∗) ≤
 bRw(θ∗) −R(θ∗)
 + bRw(θ∗) −bR(θ∗) ≤ s"
CONCLUSION,0.6778042959427207,"2(
α
1−αL2max + Vτ∼pα(τ)) ln
  1 ϵ
"
CONCLUSION,0.6789976133651552,"B
+ 2Lmax ln
  1 ϵ
"
CONCLUSION,0.6801909307875895,"3(1 −α)B
+
α
1 −α
Lmax B B
X"
CONCLUSION,0.6813842482100239,"i=1
δ(τi) ≤ s"
CONCLUSION,0.6825775656324582,"2(
α
1−αL2max + Vτ∼pα(τ)) ln
  1 ϵ
"
CONCLUSION,0.6837708830548926,"B
+
1
3(1 −α)
Lmax B"
CONCLUSION,0.684964200477327,"
2 ln
1 ϵ"
CONCLUSION,0.6861575178997613,"
+ 3αB

."
CONCLUSION,0.6873508353221957,"This completes the proof of Theorem 4.3.
■"
CONCLUSION,0.6885441527446301,"C.9
Proof of Theorem 4.4"
CONCLUSION,0.6897374701670644,"Theorem 4.4 Let F −1
ℓ-KDE(α; θ) = VaRKDE
α
[ℓ(T , θ)] and F −1
ℓ
(α; θ) = VaRα[ℓ(T , θ)]. Suppose that
K(x) is lower bounded by a constant, ∀x. For any ϵ > 0, with probability at least 1 −ϵ, we can have
the following bound:"
CONCLUSION,0.6909307875894988,"sup
θ∈Θ"
CONCLUSION,0.6921241050119332," 
F −1
ℓ-KDE(α; θ) −F −1
ℓ
(α; θ)

≤O

hℓ
√B ∗log B"
CONCLUSION,0.6933174224343676,"
.
(33)"
CONCLUSION,0.6945107398568019,"Proof. For any constant M, we firstly notice that"
CONCLUSION,0.6957040572792362,"Pτ1,··· ,τB"
CONCLUSION,0.6968973747016707,"
sup
θ∈Θ"
CONCLUSION,0.698090692124105," 
F −1
ℓ-KDE(α; θ) −F −1
ℓ
(α; θ)

≤M

≥1 −ϵ
(34a)"
CONCLUSION,0.6992840095465394,"⇔Pτ1,··· ,τB"
CONCLUSION,0.7004773269689738,"
sup
θ∈Θ"
CONCLUSION,0.7016706443914081," 
F −1
ℓ-KDE(α; θ) −F −1
ℓ
(α; θ)

≥M

≤ϵ
(34b)"
CONCLUSION,0.7028639618138425,"⇔Pτ1,··· ,τB"
CONCLUSION,0.7040572792362768,"
sup
θ∈Θ"
CONCLUSION,0.7052505966587113," 
Fℓ(t −M; θ) −Fℓ-KDE(t; θ)

≥0

≤ϵ,
t = F −1
ℓ-KDE(α; θ).
(34c)"
CONCLUSION,0.7064439140811456,"For any θ and t, we have"
CONCLUSION,0.7076372315035799,"Pτ1,··· ,τB
 
Fℓ(t −M; θ) −Fℓ-KDE(t; θ) ≥0

≤ϵ
(35a)"
CONCLUSION,0.7088305489260143,"⇔Pτ1,··· ,τB
 
Fℓ(t −M; θ) −Fℓ-KDE(t −M; θ) + Fℓ-KDE(t −M; θ) −Fℓ-KDE(t; θ) ≥0

≤ϵ
(35b)"
CONCLUSION,0.7100238663484487,"⇔Pτ1,··· ,τB
 
Fℓ(t −M; θ) −Fℓ-KDE(t −M; θ) ≥Fℓ-KDE(t; θ) −Fℓ-KDE(t −M; θ)

≤ϵ
(35c)"
CONCLUSION,0.711217183770883,"⇔Pτ1,··· ,τB
 
Fℓ(t; θ) −Fℓ-KDE(t; θ) ≥Fℓ-KDE(t + M; θ) −Fℓ-KDE(t; θ)

≤ϵ
(35d)"
CONCLUSION,0.7124105011933174,"⇔Pτ1,··· ,τB
 
Fℓ(t; θ) −Fℓ-KDE(t; θ) ≥M Bhℓ 
B
X"
CONCLUSION,0.7136038186157518,"i=1
K
 t −ℓ(DQ
τi, DS
τi; θ)
hℓ"
CONCLUSION,0.7147971360381862,"
+ o(M)

≤ϵ (35e)"
CONCLUSION,0.7159904534606205,"⇐Pτ1,··· ,τB
 
Fℓ(t; θ) −Fℓ-KDE(t; θ) ≥MKmin hℓ"
CONCLUSION,0.7171837708830548,"
≤ϵ,
(35f)"
CONCLUSION,0.7183770883054893,"where Kmin is the lower bound of the kernel function K(x), i.e., K(x) ≥Kmin, ∀x."
CONCLUSION,0.7195704057279236,"According to Theorem 3 of [80], we know that for any ϵ > 0, with probability at least 1 −ϵ, the
following inequality holds:"
CONCLUSION,0.720763723150358,"Pτ1,··· ,τB "
CONCLUSION,0.7219570405727923,"sup
θ∈Θ,t≥0"
CONCLUSION,0.7231503579952268," 
Fℓ(t; θ) −Fℓ-KDE(t; θ)

≥
C
√B ∗log B !"
CONCLUSION,0.7243436754176611,"≤ϵ,
(36)"
CONCLUSION,0.7255369928400954,"where C is a constant. Let M =
hℓC
Kmin
√B∗log B. Thus, the Eq. (35f) holds and we complete the proof
of Theorem 4.4.
■"
CONCLUSION,0.7267303102625299,"C.10
Additional Theorem"
CONCLUSION,0.7279236276849642,"To gain more theoretical insights into a popular meta-learning method—MAML [25], we provide the
following Theorem C.1. Before proceeding, we introduce some notations. During meta-training, a"
CONCLUSION,0.7291169451073986,"finite number of task instances are observed by first sampling a task from the distribution p(τ). Each
task Dτi comprises a collection of mi data points {(DS
i,j, DQ
i,j)}mi
j=1, which are distributed over Z
with each data point drawn from a distribution Di. For some risk ℓ, define the family of functions
FZ := {ℓ(θ −λ∇θℓ(DS
τi; θ), DQ
τi) : θ ∈Θ}. For each task Dτi, the Rademacher complexity of F
on mi samples is"
CONCLUSION,0.7303102625298329,"Ri
mi(FZ) = E(DS
i,j,DQ
i,j)∼(Di)miEϵ "
CONCLUSION,0.7315035799522673,"sup
θ∈Θ"
MI,0.7326968973747017,"1
mi mi
X"
MI,0.733890214797136,"j=1
ϵjℓ(θ −λ∇θℓ(DS
i,j; θ), DQ
i,j) "
MI,0.7350835322195705,",
(37)"
MI,0.7362768496420048,"where the ϵj’s are Rademacher random variables. Let Fi(θ) = EDiℓ(θ −λ∇θℓ(DS
i,j; θ), DQ
i,j),
ˆFi(θ) =
1
mi
Pmi
j=1 ℓ(θ −λ∇θℓ(DS
i,j; θ), DQ
i,j). Denote by θ∗the optimal model parameter under the
two-stage algorithm. Theorem C.1 provides generalization of the algorithm to new tasks.
Theorem C.1 (Generalization Bound for MAML in the Tail Risk Cases). For a new task τB+1 with
distribution DB+1, if DB+1 = PB
i=1 aiDi, then with probability at least 1 −δ for any δ > 0, we can
have"
MI,0.7374701670644391,"FB+1(θ∗) ≤max
i
ˆFi(θ∗) + B
X i=1 "
MI,0.7386634844868735,"2aiRi
mi(FZ) + ai s"
MI,0.7398568019093079,log (B/δ)
MI,0.7410501193317423,2mi 
MI,0.7422434367541766,".
(38)"
MI,0.7434367541766109,"Proof. The proof consists of two parts. We first explore the generalization to new instances of
previously-seen tasks. Then we solve the generalization to new tasks."
MI,0.7446300715990454,"Step 1. For any sample set A = {(DS
i,j, DQ
i,j)}mi
j=1, define Φ(A) = supℓ∈FZ Fi(θ) −ˆFi(θ). Let
A and A′ := {((DS
i,j)′, (DQ
i,j)′)}mi
j=1 be two samples that differ by exactly one point. According
to the fact supx f(x) −supx g(x) ≤supx(f(x) −g(x)), we know that Φ(A′) −Φ(A) ≤
1
mi
due to the difference in exactly one point. Similarly, we can obtain Φ(A) −Φ(A′) ≤
1
m, thus
Φ(A) −Φ(A′)
 ≤1"
MI,0.7458233890214797,"m. Following McDiarmid’s inequality, for any δ > 0, with probability at least
1 −δ"
MI,0.747016706443914,"2, we have"
MI,0.7482100238663485,Φ(A) ≤EA[Φ(A)] + s
MI,0.7494033412887828,log(2/δ)
MI,0.7505966587112172,"2mi
.
(39)"
MI,0.7517899761336515,We next bound the expectation of the right-hand side of inequality (39) as follows:
MI,0.752983293556086,"EA[Φ(A)] = EA
h
sup
ℓ∈FZ
Fi(θ) −ˆFi(θ)
i"
MI,0.7541766109785203,"= EA
h
sup
ℓ∈FZ
EA′
h
EA′(Fi(θ)) −ˆFi(θ)
ii
(40)"
MI,0.7553699284009546,"≤EA,A′
h
sup
ℓ∈FZ
EA′(Fi(θ)) −ˆFi(θ)
i
(41)"
MI,0.7565632458233891,"= EA,A′
h
sup
ℓ∈FZ"
MI,0.7577565632458234,"1
mi mi
X j=1"
MI,0.7589498806682577," 
ℓ(DS
i,j, DQ
i,j) −ℓ((DS
i,j)′, (DQ
i,j)′)
i
(42)"
MI,0.7601431980906921,"= EA,A′,ϵ
h
sup
ℓ∈FZ"
MI,0.7613365155131265,"1
mi mi
X"
MI,0.7625298329355609,"j=1
ϵj
 
ℓ(DS
i,j, DQ
i,j) −ℓ((DS
i,j)′, (DQ
i,j)′)
i"
MI,0.7637231503579952,"≤EA,ϵ
h
sup
ℓ∈FZ"
MI,0.7649164677804295,"1
mi mi
X"
MI,0.766109785202864,"j=1
ϵjℓ(DS
i,j, DQ
i,j)
i
+ EA′,ϵ
h
sup
ℓ∈FZ"
MI,0.7673031026252983,"1
mi mi
X"
MI,0.7684964200477327,"j=1
ϵjℓ((DS
i,j)′, (DQ
i,j)′)
i"
MI,0.7696897374701671,"= 2EA,ϵ
h
sup
ℓ∈FZ"
MI,0.7708830548926014,"1
mi mi
X"
MI,0.7720763723150358,"j=1
ϵjℓ(DS
i,j, DQ
i,j)
i"
MI,0.7732696897374701,= 2Rmi(FZ).
MI,0.7744630071599046,"Eq. (40) uses the law of total expectation. Inequality (41) holds by Jensen’s inequality and the
convexity of the supremum function. In Eq. (42), ℓ(DS
i,j, DQ
i,j) := ℓ(θ −λ∇θℓ(DS
i,j; θ), DQ
i,j),"
MI,0.7756563245823389,"where (DS
i,j, DQ
i,j) ∈A. Following inequality (39), we can know that"
MI,0.7768496420047732,Fi(θ) ≤ˆFi(θ) + 2Rmi(FZ) + s
MI,0.7780429594272077,log(2/δ)
MI,0.779236276849642,"2mi
.
(43)"
MI,0.7804295942720764,"Step 2. Since the new distribution DB+1 is the convex combination of Di, ∀i = 1, · · · , B, we have
FB+1(θ) = PB
i=1 aiFi(θ). Accordingly, with probability at least 1 −δ over the choice of samples
used to compute ˆF(θ),"
MI,0.7816229116945107,"FB+1(θ∗) = B
X"
MI,0.7828162291169452,"i=1
aiFi(θ∗) ≤ B
X i=1 "
MI,0.7840095465393795,ai ˆFi(θ∗) + 2aiRmi(FZ) + ai s
MI,0.7852028639618138,log(2/δ)
MI,0.7863961813842482,2mi 
MI,0.7875894988066826,",
(44)"
MI,0.788782816229117,which yields that
MI,0.7899761336515513,"FB+1(θ∗) ≤max
i
ˆFi(θ∗) + B
X i=1 "
MI,0.7911694510739857,"2aiRi
mi(FZ) + ai s"
MI,0.7923627684964201,log (2/δ)
MI,0.7935560859188544,2mi 
MI,0.7947494033412887,".
(45)"
MI,0.7959427207637232,"In summary, the two steps complete the proof of Theorem C.1.
■"
MI,0.7971360381861575,"D
Implementation Details"
MI,0.7983293556085919,"D.1
Benchmark Details & Neural Architectures & Opensource Codes"
MI,0.7995226730310262,"Here, we illustrate all meta learning benchmark purposes in Fig. 10, which includes sinusoid
regression, pendulum system identification, few-shot image classification, and meta reinforcement
learning. We no longer run experiments on the Omniglot dataset, as most baselines can achieve SOTA
performance and cannot tell the difference well from the openreview of [1]."
MI,0.8007159904534606,"（a）Sinusoid Regression
（d）Continuous Control 1
2"
MI,0.801909307875895,support dataset
MI,0.8031026252983293,class:
MI,0.8042959427207638,"class:
?
?
?"
MI,0.8054892601431981,query dataset
MI,0.8066825775656324,"（c）Few-Shot Image Classification
（b）System Identification"
MI,0.8078758949880668,Figure 10: Typical meta learning benchmarks in evaluation.
MI,0.8090692124105012,"Sinusoid regression: In [1, 42], a lot of easy tasks and limited challenging tasks are sampled for
meta-training, with the tasks from the whole space employed in evaluation. The default range of the
phase parameter is B ∈[0, π], while those of the amplitude are A ∈[0.1, 1.05] for easy tasks and
A ∈[4.95, 5.0] for challenging tasks. Generally, sinusoid functions with larger amplitudes are hard
to adapt from a few support data points. The mean square error works as the risk function to measure
the gap between the predicted value f(x) and the actual value. We set the task batch 50 for 5-shot
and 25 for 10-shot, and the maximum iteration number is 70000. We refer the reader to TR-MAML
and DR-MAML for all of the setups."
MI,0.8102625298329356,"We retain the neural architectures [42, 1] in for all MAML like methods. In detail, all methods
take a multilayer perceptron with two hidden layers and 40 ReLU activation units in each layer.
The inner loop is achieved via one stochastic gradient descent step. As for CNP like methods,
please refer to the vanilla set-up in [15] (The Github link is attached here: https://github.com/
google-deepmind/neural-processes)."
MI,0.8114558472553699,"As the task space is hugh, there is no way to exactly estimate the risk quantile. Hence, the Oracle
quantile in Fig. 5 is roughly computed from the sampled 100 tasks given the pretrained DR-MAML+.
The rationale behind this operation is that increasing the population number in statistics reduces the
quantile estimate bias."
MI,0.8126491646778043,"System identification: The pendulum system is a classical environment in the OpenAI gym (en-
vironment details are: https://github.com/openai/gym/blob/master/gym/envs/classic_"
MI,0.8138424821002387,"control/pendulum.py), and it is an actuated joint with one fixed end. The goal of system iden-
tification for the pendulum system is to predict the state transition given arbitrary actions with
several randomly collected transitions as the support dataset. The observation is a tuple in the form
(cos θ, sin θ, θ′), where θ ∈[π, π]. The action is in the range a ∈[−2.0, 2.0] and the torque is applied
to the pendulum body. The mass m and the length l of the pendulum follows a uniform distribution
(m, l) ∼U([0.4, 1.6] × [0.4, 1.6]), sampled variables configure a Markov decision process as the task.
In each batch, there are 16 tasks, and each task comprises 200 data points. Specifically, 10 few-shot
data points are randomly sampled to enable system identification per task, denoted as 10-shot. For
20-shot cases, the number of data points in support dataset is 20. And the maximum iteration
number is 5000."
MI,0.815035799522673,"For all MAML-like methods, the neural architecture used here is a multilayer perceptron with three
hidden layers of 128 hidden units each and the activation function is ReLU. The learning rate for
both the inner and outer loops is set at 1e-4."
MI,0.8162291169451074,"Few-shot image classification: The few-shot image classification is mostly described as an N-way
K-shot classification, where N classes with K-labeled instances for each are considered. The dataset
is organized in the same manner as that in [81, 42, 1]: These include 64 classes for meta-training,
with the rest 36 classes for meta-testing. We generate each task in the way: 8 meta-training tasks from
the class {6, 7, 7, 8, 8, 9, 9, 10} are randomly generated from 64 meta-training classes; the remaining
classes are organized similarly. As a result, each task is constructed from sampling one image from
five classes, corresponding to a 5-way 1-shot problem. The task batch is set 4 with a maximum
number of iterations of 60000 in meta-training."
MI,0.8174224343675418,"For all MAML-like methods, the neural architecture used here is a four-layer convolutional neural
network for the mini-ImageNet datasets. The inner loop is achieved via one stochastic gradient
descent step. We refer the reader to TR-MAML and DR-MAML for all of the setups (The Github
link is attached here https://github.com/lgcollins/tr-maml)."
MI,0.8186157517899761,"Meta reinforcement learning: 2D Navigation is a classical meta reinforcement learning benchmark
where efficient explorations matter. The task in 2D Navigation is to guide the point robot to take
move actions for a purpose of reaching a specific goal location from the step-wise reward. The reward
the agent receives from the environment is based on the distance to the goal, and 20 episodes work as
the support dataset for navigation fast adaptation. In terms of the task distribution, we sample tasks
from a uniform distribution U([−0.5, 0.5] × [−0.5, 0.5]) over goal locations."
MI,0.8198090692124105,"As for the neural architecture for policy network set-ups, we refer the reader to vanilla MAML (Github
link is attached here https://github.com/tristandeleu/pytorch-maml-rl) and CAVIA
(The Github link is attached here https://github.com/lmzintgraf/cavia/tree/master/rl).
And trust region policy optimization works for policy optimization."
MI,0.8210023866348448,Table 4: Computational and memory cost in MaPLe relevant experiments.
MI,0.8221957040572793,"Method
MaPLe
DR-MaPLe
DR-MaPLe+ (Ours)"
MI,0.8233890214797136,"Implementation Time
2.1 h
+1.7 h
+1.7 h
Memory Usage
41.57 G
+36.84G
+36.84G"
MI,0.8245823389021479,"Few-Shot Image Classification with MaPLe [69]: The stochastic gradient descent is the default
optimizer with the learning rate 0.0035, and A6000 GPUs work for computations. We examine tail
task risk minimization effectiveness on three large datasets. The class number split setup in datasets
(class number to train/validate/test) is TieredImageNet (351/97/160) [82], ImagenetA (128/32/40)
[83], and ImagenetSketch (640/160/200) [84]. Table 4 reports the overall training time and memory,
where the vanilla MaPLe serves as the anchor point, and + means additional costs from the two-stage
operation. For details of experimental implementations and setups, feel free to access our code at
https://github.com/lvyiqin/DRMAML."
MI,0.8257756563245824,"D.2
Modules in Python"
MI,0.8269689737470167,"This subsection includes the impelementation of KDE for the studied strategy. Here, the example of
the hinge loss is illustrated as follows."
"IMPORT
NUMPY AS NP",0.8281622911694511,"1 import
numpy as np"
"IMPORT
TORCH",0.8293556085918854,"2 import
torch"
FROM,0.8305489260143198,"3 from
scipy.stats
import
gaussian_kde"
FROM,0.8317422434367542,"4 from
scipy.optimize
import
brentq 5"
FROM,0.8329355608591885,"6 def loss(batch_loss , confidence_level ):"
FROM,0.834128878281623,"7
# estimate
the
VaR_alpha
according to kernel
density
estimator"
FROM,0.8353221957040573,"8
kde = gaussian_kde(batch_loss)"
FROM,0.8365155131264916,"9
try:"
FROM,0.837708830548926,"10
target_func = lambda x: kde. integrate_box_1d (-np.inf , x) -
confidence_level"
FROM,0.8389021479713604,"11
VaR_alpha = brentq(target_func , np.min(batch_loss), np.max(
batch_loss))"
EXCEPT,0.8400954653937948,"12
except
ValueError:"
EXCEPT,0.8412887828162291,"13
x = np.linspace(np.min(batch_loss), np.max(batch_loss), 1000)"
EXCEPT,0.8424821002386634,"14
pdf = kde.evaluate(x)"
EXCEPT,0.8436754176610979,"15
cdf = np.cumsum(pdf) / np.sum(pdf)"
EXCEPT,0.8448687350835322,"16
index = np.argmax(cdf
>= confidence_level )"
EXCEPT,0.8460620525059666,"17
VaR_alpha = x[index] 18"
EXCEPT,0.847255369928401,"19
# calculate
the meta loss"
EXCEPT,0.8484486873508353,"20
tail_loss = [i - VaR_alpha if (i - VaR_alpha) > 0 else
torch.
tensor (0.).cuda () for i in batch_loss]"
EXCEPT,0.8496420047732697,"21
new_batch_loss = torch.stack(tail_loss).mean ()"
EXCEPT,0.850835322195704,"22
factor = 1 / (1 - confidence_level )"
EXCEPT,0.8520286396181385,"23
loss_meta = VaR_alpha + factor * new_batch_loss"
RETURN,0.8532219570405728,"24
return
loss_meta"
RETURN,0.8544152744630071,Listing 1: The calculation process of CVaRα objective.
RETURN,0.8556085918854416,"E
Additional Experimental Results"
RETURN,0.8568019093078759,"Due to the page limit in the main paper, we include additional experiments and corresponding results
in this section."
RETURN,0.8579952267303103,"E.1
Evaluation with Other Robust Meta Learners"
RETURN,0.8591885441527446,"In addition to MAML, we apply a similar modification to CNP, which results in TR-CNP, DRO-CNP,
DR-CNP, and DR-CNP+ (DR-CNP with KDE for VaRα estimates). We report the meta testing results
on sinusoid regression and pendulum system identification benchmarks."
RETURN,0.860381861575179,"As illustrated in Table 5/6, all methods achieve comparable average performance in sinusoid and
pendulum system identification. Regarding CVaRα, DR-CNP’s improvement is relatively marginal
over others except DR-CNP+. Compared to MAML, CNP seems more sensitive to quantile estimate
accuracies when meeting with the studied strategies."
RETURN,0.8615751789976134,"Table 5: MSEs for Sinusoid 5-shot with reported standard deviations (5 runs). With α = 0.7,
the best results are in bold."
RETURN,0.8627684964200477,"Method
Average
Worst
CVaRα
CNP [15]
0.09±0.00
2.71±0.54
0.24±0.01
TR-CNP [42]
0.10±0.01
1.51±0.30
0.22±0.03
DRO-CNP [66]
0.09±0.02
2.54±1.81
0.21±0.05
DR-CNP [1]
0.09±0.01
1.62±0.45
0.20±0.02
DR-CNP+(Ours)
0.08±0.01
1.47±0.90
0.17±0.02"
RETURN,0.863961813842482,"E.2
Numeric Results in Tables and Histograms"
RETURN,0.8651551312649165,"As the improving tricks in this work are regarding the quantile estimators, here we particularly include
the quantitative results to show the difference between DR-MAML and DR-MAML+ in Table 7/8."
RETURN,0.8663484486873508,"Table 6: MSEs for Pendulum 10-shot with reported standard deviations (5 runs). With α = 0.5,
the best results are in bold."
RETURN,0.8675417661097852,"Method
Average
Worst
CVaRα
CNP [15]
0.75±0.01
1.51±0.23
0.87±0.02
TR-CNP [42]
0.76±0.00
1.24±0.02
0.85±0.01
DRO-CNP [66]
0.73±0.01
1.51±0.16
0.85±0.01
DR-CNP [1]
0.75±0.01
1.40±0.16
0.86±0.01
DR-CNP+(Ours)
0.72±0.01
1.36±0.07
0.82±0.00"
RETURN,0.8687350835322196,"Note that the studied distributionally robust strategy is on the tail risk minimization, and CVaRα
is the direct optimization indicator. As can be seen, DR-MAML+’s performance superiority over
DR-MAML is significant w.r.t. CVaRα values in 5-shot sinusoid regression and four mini-ImageNet
meta-testing tasks. These scenarios are more challenging than others as (i) the context information for
adaptation is limited in 5-shot data points and (ii) the distributional shift is severe in mini-ImageNet
meta-testing phase."
RETURN,0.869928400954654,"Table 7: Test average mean square errors (MSEs) with reported standard deviations for sinusoid
regression (5 runs). We respectively consider 5-shot and 10-shot cases with α = 0.7. The results
are evaluated across the 490 meta-test tasks, as in [42]. The best results are in bold."
-SHOT,0.8711217183770883,"5-shot
10-shot
Method
Average
Worst
CVaRα
Average
Worst
CVaRα
DR-MAML [1]
0.89±0.04
2.91±0.46
1.76±0.02
0.54±0.01
1.70±0.17
0.96±0.01
DR-MAML+(Ours)
0.87±0.02
2.78±0.22
1.65±0.02
0.59±0.02
1.51±0.11
0.95±0.02"
-SHOT,0.8723150357995226,"Table 8: Average 5-way 1-shot classification accuracies in mini-ImageNet with reported
standard deviations (3 runs). With α = 0.5, the best results are in bold. The higher, the better for
all values."
-SHOT,0.8735083532219571,"Eight Meta-Training Tasks
Four Meta-Testing Tasks
Method
Average
Worst
CVaRα
Average
Worst
CVaRα
DR-MAML [1]
70.2±0.2
63.4±0.2
67.2±0.1
49.4±0.1
47.1±0.1
47.5±0.1
DR-MAML+(Ours)
70.4±0.1
63.8±0.2
67.5±0.1
49.9±0.1
47.2±0.1
48.1±0.1"
-SHOT,0.8747016706443914,"We can attribute the performance differences of the two methods to the cumulative quantile estimation
errors using the crude MC. Even though the quantile estimation error in Fig. 5 difference is tiny
in each step, the cumulative error indeed affects the converged equilibrium a lot. This reflects the
advantage of the KDE’s used in DR-MAML+ when the task batch size cannot be set larger in practice."
-SHOT,0.8758949880668258,"We also investigate the task risk value distributions in pendulum system identification. To this end,
we visualize one run testing results for all methods in Fig. 11. It seems DR-MAML+’s result is more
skewed to the left than others."
-SHOT,0.8770883054892601,"Fig. 12 displays all methods’ performance w.r.t. the average and CVaRα returns along the meta-
training process. We exclude TR-MAML in visualization due to its worse performance and unstable
training properties. We can find that the DR-MAML exhibits a fast performance rise at the early
stage but its capability to continuously improve diminishes over time. DR-MAML+ consistently
outperforms other baselines in most cases. The above suggests that the KDE module achieves
performance gains over the crude MC when implemented with the two-stage distributionally robust
strategy for meta RL scenarios."
-SHOT,0.8782816229116945,"E.3
Sensitivity Analysis to Confidence Level"
-SHOT,0.8794749403341289,"To reveal the impact of confidence levels on model performance, we perform a sensitivity analysis
with respect to confidence levels. Since only DR-MAML and DR-MAML+ are influenced by the
confidence levels during the distributionally robust optimization across all baselines, we only compare
the performance of the two methods to highlight the differences between them. As shown in Fig."
-SHOT,0.8806682577565632,"0.5
1.0
1.5
2.0
Mean Squared Error (MSE) 0 20 40 60 80 100"
-SHOT,0.8818615751789977,Average=0.60
-SHOT,0.883054892601432,CVaRα=0.76
-SHOT,0.8842482100238663,DR-MAML+(Ours)
-SHOT,0.8854415274463007,"0.5
1.0
1.5
2.0
Mean Squared Error (MSE) 0 20 40 60 80 100"
-SHOT,0.8866348448687351,Average=0.62
-SHOT,0.8878281622911695,CVaRα=0.86
-SHOT,0.8890214797136038,"MAML
DR-MAML+(Ours)"
-SHOT,0.8902147971360382,"0.5
1.0
1.5
2.0
Mean Squared Error (MSE) 0 20 40 60 80 100 120"
-SHOT,0.8914081145584726,"Average=0.68
CVaRα=0.81"
-SHOT,0.8926014319809069,"TR-MAML
DR-MAML+(Ours)"
-SHOT,0.8937947494033412,"0.5
1.0
1.5
2.0
Mean Squared Error (MSE) 0 20 40 60 80 100"
-SHOT,0.8949880668257757,Average=0.61
-SHOT,0.89618138424821,CVaRα=0.82
-SHOT,0.8973747016706444,"DRO-MAML
DR-MAML+(Ours)"
-SHOT,0.8985680190930787,"0.5
1.0
1.5
2.0
Mean Squared Error (MSE) 0 20 40 60 80 100 120"
-SHOT,0.8997613365155132,Average=0.61
-SHOT,0.9009546539379475,CVaRα=0.78
-SHOT,0.9021479713603818,"DR-MAML
DR-MAML+(Ours)"
-SHOT,0.9033412887828163,"Figure 11: Histograms of meta-testing performance in system identification. With α = 0.5, we
visualize the comprision results of baselines and our DR-MAML+ in 10-shot prediction. The lower,
the better for Average and CVaRα values."
-SHOT,0.9045346062052506,"0
100
200
300
400
500
Steps −30 −25 −20 −15 −10"
-SHOT,0.905727923627685,Average Return
-SHOT,0.9069212410501193,"0
100
200
300
400
500
Steps −40 −30 −20 −10"
-SHOT,0.9081145584725537,CVaR Return
-SHOT,0.9093078758949881,Point Robot Navigation
-SHOT,0.9105011933174224,"MAML
DRO-MAML
DR-MAML
DR-MAML+(Ours)"
-SHOT,0.9116945107398569,"Figure 12: Learning curves for the point robot navigation task. Here, 20 trajectories work as
the support set for adaptation. The curves report the normalized returns and are averaged over four
random seeds, with α = 0.5."
-SHOT,0.9128878281622912,"13/14, we can observe that in both sinusoid 5-shot and 10-shot tasks, as the confidence level varies,
DR-MAML+ exhibits more stable performance than DR-MAML, indicating that DR-MAML+ has
a lower sensitivity to confidence levels. It can be illustrated that the crude Monte Carlo used in
DR-MAML is more unstable in terms of quantile estimation than the kernel density estimator used in
DR-MAML+. This can be due to the fact that the crude Monte Carlo method is more likely to get
stuck in the local optimal solution. In addition, it can be seen from Fig. 13/14 that the performance
of our developed DR-MAML+ is better than DR-MAML in most cases. DR-MAML+ exhibits lower
mean squared errors than DR-MAML in the average, worst, and CVaRα indicators, demonstrating
the advantages of more accurate quantile estimation in improving robustness."
-SHOT,0.9140811455847255,"0.1
0.3
0.5
0.7
0.9
Conﬁdence Level 0.6 0.7 0.8 0.9 1.0 1.1 1.2"
-SHOT,0.9152744630071599,Average
-SHOT,0.9164677804295943,"DR-MAML
DR-MAML+(Ours)"
-SHOT,0.9176610978520287,"0.1
0.3
0.5
0.7
0.9
Conﬁdence Level 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Worst"
-SHOT,0.918854415274463,"DR-MAML
DR-MAML+(Ours)"
-SHOT,0.9200477326968973,"0.1
0.3
0.5
0.7
0.9
Conﬁdence Level 1.0 1.5 2.0 2.5 3.0 CVaR"
-SHOT,0.9212410501193318,"DR-MAML
DR-MAML+(Ours)"
-SHOT,0.9224343675417661,Sinusoid 5-shot
-SHOT,0.9236276849642004,"Figure 13: Meta testing performance of DR-MAML and DR-MAML+ with different confidence
level on Sinusoid 5-shot tasks. In the plots, the vertical axis is the MSEs, the horizontal axis is the
confidence level, and the shaded area represents the standard deviation."
-SHOT,0.9248210023866349,"0.1
0.3
0.5
0.7
0.9
Conﬁdence Level 0.4 0.6 0.8 1.0 1.2"
-SHOT,0.9260143198090692,Average
-SHOT,0.9272076372315036,"DR-MAML
DR-MAML+(Ours)"
-SHOT,0.9284009546539379,"0.1
0.3
0.5
0.7
0.9
Conﬁdence Level 1 2 3 4 Worst"
-SHOT,0.9295942720763724,"DR-MAML
DR-MAML+(Ours)"
-SHOT,0.9307875894988067,"0.1
0.3
0.5
0.7
0.9
Conﬁdence Level 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 CVaR"
-SHOT,0.931980906921241,"DR-MAML
DR-MAML+(Ours)"
-SHOT,0.9331742243436754,Sinusoid 10-shot
-SHOT,0.9343675417661098,"Figure 14: Meta testing performance of DR-MAML and DR-MAML+ with different confidence
level on Sinusoid 10-shot tasks. In the plots, the vertical axis is the MSEs, the horizontal axis is the
confidence level, and the shaded area represents the standard deviation. X 0 1 2 3 4 5 Y 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Z −2 −1 0 1 2 3 MAML X 0 1 2 3 4 5 Y 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Z −1 0 1 2"
-SHOT,0.9355608591885441,TR-MAML X 0 1 2 3 4 5 Y 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Z 0 1 2 3
-SHOT,0.9367541766109785,DRO-MAML X 0 1 2 3 4 5 Y 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Z −1 0 1 2 3 4
-SHOT,0.9379474940334129,DR-MAML X 0 1 2 3 4 5 Y 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Z −2 0 2 4 6
-SHOT,0.9391408114558473,DR-MAML+
-SHOT,0.9403341288782816,"Figure 15: The fast adaptation risk landscape of meta-trained MAML, TR-MAML, DRO-
MAML, DR-MAML and DR-MAML+. The figure illustrates a 5-shot sinusoid regression
example, mapping to the function space f(x) = A sin(x −B). The X-axis and Y -axis represent the
amplitude parameter a and phase parameter b respectively. The plots exhibit testing MSEs on the
Z-axis across random trials of task generation."
-SHOT,0.9415274463007159,"E.4
Further Exploration on Adaptation"
-SHOT,0.9427207637231504,"We demonstrate the adaptation risk landscape of meta-trained MAML [25], TR-MAML [42], DRO-
MAML [66], DR-MAML [1] and our DR-MAML+ in Fig. 15. The adaptation risk landscape shows
the superiority of our method in optimizing within the expected tail risk minimization. Compared to
other methods, DR-MAML+ exhibits smoother and smaller risk profiles, illustrating its robustness
even in challenging tasks."
-SHOT,0.9439140811455847,"F
Computational Platforms & Softwares"
-SHOT,0.9451073985680191,"This work employs Pytorch [85] as the default deep learning toolkit when implementing the developed
methods. As for baselines, TR-MAMAL follows the standard implementation as work [42] and runs
with Tensorflow [86]. Others are implemented with Pytorch. All experimental results are computed
by NVIDIA RTX6000 GPUs and A800 GPUs."
-SHOT,0.9463007159904535,NeurIPS Paper Checklist
CLAIMS,0.9474940334128878,1. Claims
CLAIMS,0.9486873508353222,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9498806682577565,Answer: [Yes]
CLAIMS,0.951073985680191,Justification: The abstract and introduction clearly state the claims.
CLAIMS,0.9522673031026253,Guidelines:
CLAIMS,0.9534606205250596,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.954653937947494,2. Limitations
LIMITATIONS,0.9558472553699284,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9570405727923628,Answer: [Yes]
LIMITATIONS,0.9582338902147971,Justification: We discuss the limitations of the work in Sec 5.7.
LIMITATIONS,0.9594272076372315,Guidelines:
LIMITATIONS,0.9606205250596659,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9618138424821002,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9630071599045346,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.964200477326969,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9653937947494033,"Justification: Refer to Appendix C.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9665871121718377,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9677804295942721,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Refer to Appendix D.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9689737470167065,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.9701670644391408,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
THEORY ASSUMPTIONS AND PROOFS,0.9713603818615751,"Answer: [Yes]
Justification: Refer to Appendix D.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9725536992840096,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.9737470167064439,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Refer to Appendix D.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9749403341288783,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
THEORY ASSUMPTIONS AND PROOFS,0.9761336515513126,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Refer to Fig. 3/4/6/12/13/14 and Table 1/2/5/6/7/8.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.977326968973747,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
THEORY ASSUMPTIONS AND PROOFS,0.9785202863961814,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
THEORY ASSUMPTIONS AND PROOFS,0.9797136038186157,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Refer to Appendix F.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9809069212410502,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
THEORY ASSUMPTIONS AND PROOFS,0.9821002386634845,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We adhere to the NeurIPS Code of Ethics.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9832935560859188,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
THEORY ASSUMPTIONS AND PROOFS,0.9844868735083532,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Refer to Appendix A.5.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9856801909307876,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to"
THEORY ASSUMPTIONS AND PROOFS,0.986873508353222,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
THEORY ASSUMPTIONS AND PROOFS,0.9880668257756563,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The question is not applicable to the paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9892601431980907,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
THEORY ASSUMPTIONS AND PROOFS,0.9904534606205251,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The question is not applicable to the paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9916467780429594,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
THEORY ASSUMPTIONS AND PROOFS,0.9928400954653938,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
THEORY ASSUMPTIONS AND PROOFS,0.9940334128878282,"Answer: [NA]
Justification: The question is not applicable to the paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9952267303102625,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
THEORY ASSUMPTIONS AND PROOFS,0.9964200477326969,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The question is not applicable to the paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9976133651551312,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The question is not applicable to the paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9988066825775657,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
