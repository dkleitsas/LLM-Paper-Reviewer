Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003236245954692557,"In reinforcement learning (RL) for sequential decision making under uncertainty,
existing methods proposed for considering mean-variance (MV) trade-off suffer
from computational difﬁculties in computation of the gradient of the variance term.
In this paper, we aim to obtain MV-efﬁcient policies that achieve Pareto efﬁciency
regarding MV trade-off. To achieve this purpose, we train an agent to maximize
the expected quadratic utility function, in which the maximizer corresponds to
the Pareto efﬁcient policy. Our approach does not suffer from the computational
difﬁculties because it does not include gradient estimation of the variance. In
experiments, we conﬁrm the effectiveness of our proposed methods."
INTRODUCTION,0.006472491909385114,"1
INTRODUCTION"
INTRODUCTION,0.009708737864077669,"Reinforcement learning (RL) trains intelligent agents to solve sequential decision-making problems
(Puterman, 1994; Sutton & Barto, 1998). While a typical objective is to maximize the expected
cumulative reward, risk-aware RL has recently attracted much attention in real-world applications,
such as ﬁnance and robotics (Geibel & Wysotzki, 2005; Garcıa & Fern´andez, 2015). Various criteria
have been proposed to capture a risk, such as Value at Risk (Chow & Ghavamzadeh, 2014; Chow
et al., 2017) and variance (Markowitz, 1952; Luenberger et al., 1997; Tamar et al., 2012; Prashanth
& Ghavamzadeh, 2013). Among them, we consider the mean-variance RL (MVRL) methods that
attempt to train an agent while controlling the mean-variance (MV) trade-off (Tamar et al., 2012;
Prashanth & Ghavamzadeh, 2013; 2016; Xie et al., 2018; Bisi et al., 2020; Zhang et al., 2021b)."
INTRODUCTION,0.012944983818770227,"Existing MVRL methods (Tamar et al., 2012; Prashanth & Ghavamzadeh, 2013; 2016; Xie et al.,
2018), typically maximize the expected cumulative reward while keeping the variance of the cumu-
lative reward at a certain level or, equivalently, minimize the variance while keeping the expected
cumulative reward at a certain level. These MVRL methods simultaneously estimate the expected
reward or variance while training an agent and solve the constrained optimization problem relaxed
by penalized methods. These studies have reported that RL-based methods suffer from high compu-
tational difﬁculty owing to the double sampling issue (Section 3) when approximating the gradient
of the variance term (Tamar et al., 2012; Prashanth & Ghavamzadeh, 2013; 2016). To avoid this,
Tamar et al. (2012) and Prashanth & Ghavamzadeh (2013) proposed multi-time-scale stochastic
optimization. Further, Xie et al. (2018) proposed a method based on the Legendre-Fenchel duality
(Boyd & Vandenberghe, 2004). Although these methods avoid the double sampling issue, as we
experimentally report in Figure 2 of Section 6, there still remains the difﬁculty in training a policy."
INTRODUCTION,0.016181229773462782,"To avoid these difﬁculties, this paper considers another approach for MVRL by focusing on obtaining
a policy that is located on the Pareto efﬁcient frontier in the sense of MV trade-off; that is, we cannot
increase the expected reward without increasing the variance and decrease the variance without
decreasing the expected reward (Section 2.2). To achieve this purpose, we propose training a RL
agent by direct expected quadratic utility maximization (EQUMRL) with a policy gradient method
(Williams, 1988; 1992; Sutton et al., 2000; Baxter & Bartlett, 2001). We show that the maximizer of
the objective in EQUMRL is Pareto efﬁcient in the sense of MV trade-off."
INTRODUCTION,0.019417475728155338,"As an important property, EQUMRL does not suffer from the double-sampling problem because it
does not include the variance estimation, which is a cause of the problem. In conventional methods, we
can also obtain an MV-efﬁcient policy when we succeed in solving the constraint problem. However,"
INTRODUCTION,0.022653721682847898,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.025889967637540454,"as discussed in the work and shown in our experiments, such as Figure 2, those methods do not
perform well due to the difﬁculty in computation. Compared with them, EQUMRL is computationally
friendly and experimentally returns more MV-efﬁcient policies. In addition, the EQUMRL is well
suited to ﬁnancial applications, such as portfolio management because in economic theory, using
MV portfolio is justiﬁed as a maximizer of the expected quadratic utility function (Markowitz, 1952;
Luenberger et al., 1997). We introduce brief survey on expected quadratic utility maximization and
ﬁnance in Appendix A.We list the following three advantages to the EQUMRL approach:"
INTRODUCTION,0.02912621359223301,"(i) our proposed EQUMRL is able to learn Pareto efﬁcient policies and has plenty of interpretations
from various perspectives (Section 4.3);
(ii) our proposed EQUMRL does not suffer from the double sampling issue by avoiding explicit
approximation of the variance (Section 4);
(iii) we experimentally show that our proposed EQUMRL returns more Pareto efﬁcient policies than
existing methods (Section 6)."
INTRODUCTION,0.032362459546925564,"In the following sections, we ﬁrst formulate the problem setting in Sections 2–3. Then, we propose
the main algorithms in Section 4. Finally, we investigate the empirical effectiveness in Section 6."
PROBLEM SETTING,0.03559870550161812,"2
PROBLEM SETTING"
PROBLEM SETTING,0.038834951456310676,"We consider a standard setting of RL, where an agent interacts with an unfamiliar, dynamic, and
stochastic environment modeled by a Markov decision process (MDP) in discrete time. We deﬁne an
MDP as a tuple (S, A, r, P, P0), where S is a set of states, A is a set of actions, r : S × A →R is
a stochastic reward function with ﬁnite mean and variance, P : S × S × A →[0, 1] is a transition
kernel, and P0 : S →[0, 1] is an initial state distribution. The initial state S1 is sampled from P0.
Let πθ : A × S →[0, 1] be a parameterized stochastic policy mapping states to actions, where θ is
a tunable parameter, and we denote the parameter space by Θ. At time step t, an agent chooses an
action At following a policy πθ(· | St). We assume that the policy πθ is differentiable with respect to
θ; that is, ∂πθ(s|a)"
PROBLEM SETTING,0.042071197411003236,"∂θ
exists."
PROBLEM SETTING,0.045307443365695796,"Let us deﬁne the expected cumulative reward from time step t to u as Eπθ[Gt:u], where Gt:u =
Pu−t
i=0 γir(St+i, At+i), γ ∈(0, 1] is a discount factor and Eπθ denotes the expectation operator over
a policy πθ, and S1 is generated from P0. When γ = 1, to ensure that the cumulative reward is
well-deﬁned, we usually assume that all policies are proper (Bertsekas & Tsitsiklis, 1995); that is, for
any policy πθ, an agent goes to a recurrent state S∗with probability 1, and obtains 0 reward after
passing the recurrent state S∗at a stopping time τ. This ﬁnite horizon setting is called episodic MDPs
(Puterman, 1994). For brevity, we denote Gt:u as G when there is no ambiguity."
PROBLEM SETTING,0.04854368932038835,"In this paper, we consider the trade-off between the mean and variance of the reward. Let us denote
the variance of a random variable W under πθ by Vπθ(W). Note that Eπθ[G] and Vπθ(G) are ﬁnite
because r(St, At) has ﬁnite mean and variance."
PROBLEM SETTING,0.05177993527508091,"2.1
TRAJECTORY VARIANCE PERSPECTIVE."
PROBLEM SETTING,0.05501618122977346,A direction of MVRL is to consider the trajectory mean Eπθ[Gt:u] and trajectory variance Vπθ(Gt:u):
PROBLEM SETTING,0.05825242718446602,"Eπθ[G]
trade-off
⇐⇒Vπθ(G)."
PROBLEM SETTING,0.061488673139158574,"A typical method for considering the MV trade-off is to train a policy under some constrains. Tamar
et al. (2012), Prashanth & Ghavamzadeh (2013), and Xie et al. (2018) formulated MVRL as"
PROBLEM SETTING,0.06472491909385113,"max
θ∈Θ Eπθ[G] s.t. Vπθ(G) = η."
PROBLEM SETTING,0.06796116504854369,"We call an algorithm based on this formulation a trajectory MV-controlled RL. In the trajectory
MV-controlled RL, the goal is to maximize the expected cumulative reward, while controlling
the trajectory-variance at a certain level. To be more precise, their actual constraint condition
is Vπθ(G) ≤η. However, if Vπθ(G) = η is feasible, the optimizer satisﬁes the equality in
applications, where we need to consider MV trade-off, such as a ﬁnancial portfolio management.
Therefore, we only consider an equality constraint. To solve this problem, Tamar et al. (2012),
Prashanth & Ghavamzadeh (2013), and Xie et al. (2018) considered a penalized method deﬁned as"
PROBLEM SETTING,0.07119741100323625,Under review as a conference paper at ICLR 2022
PROBLEM SETTING,0.0744336569579288,"maxθ∈Θ Eπθ[G] −δg
 
Vπθ(G) −η

, where δ > 0 is a constant and g : R →R is a penalty function,
such as g(x) = x or g(x) = x2. See Remark 1 for more details."
PROBLEM SETTING,0.07766990291262135,"2.2
MV-EFFICIENT POLICY."
PROBLEM SETTING,0.08090614886731391,"To consider the MV trade-off avoiding the computational difﬁculty, this paper aims to train a Pareto
efﬁcient policy in the sense of the MV trade-off, where we cannot increase the mean (resp. decrease
the variance) without increasing the variance (resp. decreasing the mean). Following the existing
literature mainly in economics, ﬁnance, and operations research (Luenberger et al., 1997), we deﬁne
a trajectory MV-efﬁcient policy as a policy πθ such that there is no other policy with θ′ ∈Θ, where
Eπθ[G] ≤Eπθ′[G] and Vπθ[G] > Vπθ′[G], or Vπθ[G] ≥Vπθ′[G] and Eπθ[G] < Eπθ′[G]. Efﬁcient
frontier is deﬁned as a set of the MV-efﬁcient policies. From the deﬁnition of the MV-controlled
policies, the trained policies belong to the efﬁcient frontier."
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.08414239482200647,"3
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE"
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.08737864077669903,"In this paper, we consider training a policy by policy gradient methods. In MVRL, we usually require
the gradients of Eπθ[G] and Vπθ(G) = E

(G −Eπθ[G])2
. Tamar et al. (2012) and Prashanth &
Ghavamzadeh (2013) show that the gradients of Eπθ[G] and Eπθ

G2
are given as"
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.09061488673139159,"∇θEπθ[G] = Eπθ "" G τ
X"
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.09385113268608414,"t=1
∇θ log πθ(St, At) #"
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.0970873786407767,",
∇θEπθ

G2
= E """
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.10032362459546926,"G2
τ
X"
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.10355987055016182,"t=1
∇θ log πθ(St, At) # ."
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.10679611650485436,"Because optimizing the policy πθ directly using the gradients is computationally intractable, we
replace them with their unbiased estimators. Suppose that there is a simulator generating a trajectory
k with {(Sk
t , Ak
t , r(Sk
t , Ak
t ))}τ k
t=1, where τ k is the stopping time of the trajectory. Then, we can
construct unbiased estimators of Eπθ[G] and Eπθ

G2
as follows (Tamar et al., 2012):"
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.11003236245954692,"b∇θEπθ[G] = bGk
τ k
X"
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.11326860841423948,"t=1
∇θ log πθ(Sk
t , Ak
t ) and b∇θEπθ

G2
=

bGk2 τ k
X"
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.11650485436893204,"t=1
∇θ log πθ(Sk
t , Ak
t ), (1)"
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.11974110032362459,"where bGk is a sample approximation of Eπθ[G] at the episode k. Besides, because ∇θ
 
Eπθ[G]
2 ="
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.12297734627831715,"2Eπθ[G]∇θEπθ[G], the gradient of the variance is given as ∇θVπθ(G) = ∇θ

Eπθ[G2] −
 
Eπθ[G]
2
= Eπθ

G2 Pτ
t=1 ∇θ log πθ(St, At)

−2Eπθ[G]∇θEπθ[G]."
POLICY GRADIENT AND DOUBLE SAMPLING ISSUE,0.1262135922330097,"However, obtaining an unbiased estimator of ∇θ
 
Eπθ[G]
2 = 2Eπθ[G]∇θEπθ[G] is difﬁcult because
it requires sampling from two different trajectories for approximating Eπθ[G] and ∇θEπθ[G]. This
issue is called double sampling issue and makes the optimization problem difﬁcult when we include
the variance Vπθ(G) into the objective function directly. Tamar et al. (2012) and Prashanth &
Ghavamzadeh (2013) reported this double sampling issue caused from the gradient estimation of the
variance and proposed solutions based on multi-time-scale stochastic optimization. Recall that they
consider the penalized objective function maxθ∈Θ Eπθ[G]−δg
 
Vπθ(G)−η) to obtain MV-controlled
policies, where the double sampling issue caused by the gradient of δg
 
Vπθ(G) −η)."
EQUMRL WITH TRAJECTORY VARIANCE PERSPECTIVE,0.12944983818770225,"4
EQUMRL WITH TRAJECTORY VARIANCE PERSPECTIVE"
EQUMRL WITH TRAJECTORY VARIANCE PERSPECTIVE,0.13268608414239483,"In social sciences, we often assume that an agent maximizes the expected quadratic utility for
considering MV trade-off. Based on this, we propose the EQUMRL for obtaining an MV-efﬁcient
policy. For a cumulative reward G, we deﬁne the quadratic utility function as utrajectory(G; α, β) =
αG −1"
EQUMRL WITH TRAJECTORY VARIANCE PERSPECTIVE,0.13592233009708737,"2βG2, where α > 0 and β ≥0. In EQUMRL, we train a policy by maximizing the expected
value of the quadratic utility function,"
EQUMRL WITH TRAJECTORY VARIANCE PERSPECTIVE,0.13915857605177995,"Eπθ

utrajectory(G; α, β)

= αEπθ[G] −1"
EQUMRL WITH TRAJECTORY VARIANCE PERSPECTIVE,0.1423948220064725,"2βEπθ[G2].
(2)"
EQUMRL WITH TRAJECTORY VARIANCE PERSPECTIVE,0.14563106796116504,"The EQUMRL is agnostic to the learning method, that is, we can implement it with various existing
algorithms such as REINFORCE and actor-crtic."
EQUMRL WITH TRAJECTORY VARIANCE PERSPECTIVE,0.1488673139158576,Under review as a conference paper at ICLR 2022
EQUMRL AND MV EFFICIENCY,0.15210355987055016,"4.1
EQUMRL AND MV EFFICIENCY"
EQUMRL AND MV EFFICIENCY,0.1553398058252427,"The expected quadratic utility function Eπθ

utrajectory(G; α, β)

is known to be Pareto efﬁcient in
the sense of mean and variance when its optimal value satisﬁes Eπθ[G] ≤α"
EQUMRL AND MV EFFICIENCY,0.15857605177993528,"β . In order to conﬁrm this,
we can decompose the expected quadratic utility as"
EQUMRL AND MV EFFICIENCY,0.16181229773462782,"Eπθ

utrajectory(G; α, β)

= −1"
EQUMRL AND MV EFFICIENCY,0.1650485436893204,"2β

Eπθ[G] −α β"
EQUMRL AND MV EFFICIENCY,0.16828478964401294,"2
+ α2 2β −1"
EQUMRL AND MV EFFICIENCY,0.1715210355987055,"2βVπθ(G).
(3)"
EQUMRL AND MV EFFICIENCY,0.17475728155339806,"When a policy π ∈Π is the maximizer of the expected quadratic utility, it is equivalent to an
MV-efﬁcient policy (Borch, 1969; Baron, 1977; Luenberger et al., 1997). Following (Luenberger
et al., 1997, p.237–239), we explain this as follows:"
EQUMRL AND MV EFFICIENCY,0.1779935275080906,"1. Among policies πθ with a ﬁxed mean Eπθ[G] = µ, the policy with the lowest variance
maximizes the expected quadratic utility function because Eπθ

utrajectory(G; α, β)

=
αµ −1"
EQUMRL AND MV EFFICIENCY,0.18122977346278318,2βµ2 −1
EQUMRL AND MV EFFICIENCY,0.18446601941747573,2βVπθ(G) is a monotonous decreasing function on Vπθ(G);
EQUMRL AND MV EFFICIENCY,0.18770226537216828,"2. Among policies πθ with a ﬁxed variance Vπθ(G) = σ2 and mean Eπθ[G] ≤
α
β , the
policy with the highest mean maximizes the expected quadratic utility function because"
EQUMRL AND MV EFFICIENCY,0.19093851132686085,"Eπθ

utrajectory(G; α, β)

= −1"
EQUMRL AND MV EFFICIENCY,0.1941747572815534,"2β

Eπθ[G] −α"
EQUMRL AND MV EFFICIENCY,0.19741100323624594,"β
2
+ α2 2β −1"
EQUMRL AND MV EFFICIENCY,0.20064724919093851,2βσ2 is a monotonous increas-
EQUMRL AND MV EFFICIENCY,0.20388349514563106,ing function on Eπθ[G] ≤α β .
EQUMRL AND MV EFFICIENCY,0.20711974110032363,"Based on the above property, we propose maximizing the expected quadratic utility function in RL;
that is, training an agent to directly maximize the expected quadratic utility function for MV control
instead of solving a constrained optimization. We call the framework that makes the RL objective
function an expected quadratic utility EQUMRL."
EQUMRL AND MV EFFICIENCY,0.21035598705501618,"Unlike the expected cumulative reward maximization in the standard RL setting, at time t, it is
desirable to include the past cumulative reward to the state St because our objective function depends
on it even given St. Let us consider the objective at time t with the inﬁnite horizon setting:"
EQUMRL AND MV EFFICIENCY,0.21359223300970873,"αEπθ

G0:∞−βG2
0:∞
S0, A0, r(S0, A0), . . . , St−1, At−1, r(St−1, At−1), St
"
EQUMRL AND MV EFFICIENCY,0.2168284789644013,"= C + αγt
(1 −2βγtG0:t−1)Eπθ[Gt:∞|St] −βγtEπθ[G2
t:∞|St]
	
,"
EQUMRL AND MV EFFICIENCY,0.22006472491909385,"where C is a constant and recall that G0:t−1 = Pt−1
i=0 γir(Si, A0).Thus, for a better decision-making,
we include the past cumulative reward into the state space."
IMPLEMENTATION OF EQUMRL,0.22330097087378642,"4.2
IMPLEMENTATION OF EQUMRL"
IMPLEMENTATION OF EQUMRL,0.22653721682847897,"In this section, we introduce how to train a policy with the EQURL. We deﬁned the objective function
of the EQUMRL, and EQUMRL is an agnostic in learning method. As examples, we show an
implementation based on REINFORCE (Williams, 1992; Brockman et al., 2016) and Actor-Critic
(AC) methods (Williams & Peng, 1991; Mnih et al., 2016). We use unbiased estimators of the
gradients deﬁned in (1)."
IMPLEMENTATION OF EQUMRL,0.2297734627831715,"REINFORCE-based trajectory EQUMRL.
For an episode k with the length n, the proposed
algorithm replaces Eπθ

G

and Eπθ

G2
with the sample approximations Pn
t=1 γt−1r(St, At) and
 Pn
t=1 γt−1r(St, At)
2, respectively (Tamar et al., 2012); that is, the unbiased gradients are given as
(1). Therefore, for a sample approximation bGk of Eπθ

G2
at the episode k, we optimize the policy
with ascending the unbiased gradient"
IMPLEMENTATION OF EQUMRL,0.23300970873786409,"b∇Eπθ

utrajectory(G; α, β)

=

α bGk −1"
IMPLEMENTATION OF EQUMRL,0.23624595469255663,"2β

bGk2
n
X"
IMPLEMENTATION OF EQUMRL,0.23948220064724918,"t=1
∇θ log πθ(Sk
t , Ak
t )."
IMPLEMENTATION OF EQUMRL,0.24271844660194175,CWe summarize the algorithm as the pseudo-code in Algorithm 1.
IMPLEMENTATION OF EQUMRL,0.2459546925566343,"Here, we present three advantages of EQUMRL.The ﬁrst advantage concerns computation. In
EQUMRL, we do not suffer from the double sampling issue because the term (Eπθ[G])2, which
causes the problem, is absent, and we must only estimate the gradients of Eπθ [G] and Eπθ

G2
.
The second advantage is that it provides a variety of interpretations, as listed in Section 4.3."
IMPLEMENTATION OF EQUMRL,0.24919093851132687,Under review as a conference paper at ICLR 2022
IMPLEMENTATION OF EQUMRL,0.2524271844660194,Algorithm 1 REINFORCE-based EQUMRL
IMPLEMENTATION OF EQUMRL,0.255663430420712,"Initialize the policy parameter θ0;
for k = 1, 2, . . . do"
IMPLEMENTATION OF EQUMRL,0.2588996763754045,"Generate {(Sk
t , Ak
t , r(Sk
t , Ak
t ))}n
t=1 on πθ;
Update policy parameters θk+1 ←θk +
ηi b∇Eπθk

utrajectory(G; α, β)

.
end for"
IMPLEMENTATION OF EQUMRL,0.2621359223300971,"The third advantage is the ease of theoretical
analysis. For example, referring to the results
of Bertsekas & Tsitsiklis (1996), we derive
the following result on the convergence of the
gradient, which can be applied to simple pol-
icy gradient algorithms, not only our proposed
REINFORCE-based algorithm.
Theorem 1. Consider an update rule such that
θk+1 ←θk + ηi b∇Eπθk

utrajectory(G; α, β)

, where the learning rates ηk are non-negative and
satisfy P∞
k=0 ηk = ∞and P∞
k=0 η2
k < ∞. Suppose that (a) episode always ﬁnishes in ﬁnite
horizon n; (b) the policy πθ has always bounded ﬁrst and second partial derivatives.
Then,
limi→∞∇θEπθi[αG −1"
IMPLEMENTATION OF EQUMRL,0.26537216828478966,2βG2] = 0 almost surely.
IMPLEMENTATION OF EQUMRL,0.2686084142394822,"It is expected that non-asymptotic results can be derived by restricting the policy class and the
optimization algorithm as Agarwal et al. (2020) and Zhang et al. (2021a), but this is not the scope of
this paper, which aims to provide a general framework."
IMPLEMENTATION OF EQUMRL,0.27184466019417475,"AC-based trajectory EQUMRL.
Another implementation of the EQUMRL is to apply the AC
algorithm (Williams & Peng, 1991; Mnih et al., 2016). For an episode k with the length n, following
Prashanth & Ghavamzadeh (2013; 2016), we train the policy by a gradient deﬁned as"
IMPLEMENTATION OF EQUMRL,0.2750809061488673,"∇θ log πθ(Sk
t , Ak
t )"
IMPLEMENTATION OF EQUMRL,0.2783171521035599,"( 
α eGk
t:t+n−1 −1"
IMPLEMENTATION OF EQUMRL,0.2815533980582524,"2β eGk,2
t:t+n−1"
IMPLEMENTATION OF EQUMRL,0.284789644012945,"
−

αM (1)"
IMPLEMENTATION OF EQUMRL,0.28802588996763756,"ˆω(1)
k (Sk
t ) −1"
IMPLEMENTATION OF EQUMRL,0.2912621359223301,2βM (2)
IMPLEMENTATION OF EQUMRL,0.29449838187702265,"ˆω(2)
k (Sk
t )
 ) ,"
IMPLEMENTATION OF EQUMRL,0.2977346278317152,"where
eGk
t:t+n−1
=
bGk
t:t+n−1
+ γnM (1)"
IMPLEMENTATION OF EQUMRL,0.30097087378640774,"ˆω(1)
k (Sk
t+n),
eGk,2
t:t+n−1
=

bGk
t:t+n−1
2
+"
IMPLEMENTATION OF EQUMRL,0.3042071197411003,"2γn bGk
t:t+n−1M (1)"
IMPLEMENTATION OF EQUMRL,0.3074433656957929,"ˆω(1)
k (Sk
t+n) + γ2nM (2)"
IMPLEMENTATION OF EQUMRL,0.3106796116504854,"ˆω(2)
k (Sk
t+n), and M (1)"
IMPLEMENTATION OF EQUMRL,0.313915857605178,"ˆω(1)
k (Sk
t ) and M (2)"
IMPLEMENTATION OF EQUMRL,0.31715210355987056,"ˆω(2)
k (Sk
t ) are models"
IMPLEMENTATION OF EQUMRL,0.32038834951456313,"of E[Gt+1:∞] and E[G2
t+1:∞] with parameters ˆω(1)
k
and ˆω(2)
k . For more details, see Prashanth &
Ghavamzadeh (2013; 2016).
Remark 1 (Existing approaches). For the double sampling issue, Tamar et al. (2012) and Prashanth
& Ghavamzadeh (2013; 2016) proposed multi-time-scale stochastic optimization. Their approaches
are known to be sensitive to the choice of step-size schedules, which are not easy to control (Xie et al.,
2018). Xie et al. (2018) proposed using the Legendre-Fenchel dual transformation with coordinate
descent algorithm. First, based on Lagrangian relaxation, Xie et al. (2018) set an objective function
as maxθ∈Θ Eπθ[G] −δ (Vπθ(G) −η). Then, Xie et al. (2018) transformed the objective function as
maxθ∈Θ,y∈R 2y
 
Eπθ[G] + 1"
IMPLEMENTATION OF EQUMRL,0.32362459546925565,"2δ

−y2 −Eπθ

G2
and trained an agent by solving the optimization
problem via a coordinate descent algorithm. However, this approach does not reﬂect the constraint η
because the constraint condition η vanishes from the objective function. This problem is caused by
their objective function based on the penalty function g(x) = x: Eπθ[G] −δ (Vπθ(G) −η), where
the ﬁrst derivative does not include η. To avoid this problem, we need an iterative algorithm to decide
an optimal δ or change g(x) from x but it is not obvious how to incorporate them into the approach.
Remark 2 (Difference from existing MV approaches). Readers may assert that EQUMRL simply
omits (Eπθ[G])2 from existing MV-controlled RL methods, which usually includes the explicit
variance term in the objective function, and is the essentially the same. However, there are signiﬁcant
differences; one of the main ﬁndings of this paper is our formulation of a simpler RL problem
to obtain an MV-efﬁcient policy. Existing MV-controlled RL methods suffer from computational
difﬁculties caused by the double sampling issue. However, we can obtain MV-efﬁcient policy without
going through the difﬁcult problem. In addition, EQUMRL shows better performance in experiments
even from the viewpoint of the constrained problem because it is difﬁcult to choose parameters
to avoid the double sampling issue in existing approaches. Thus, the EQUMRL has advantage in
avoiding solving more difﬁcult constrained problems for considering MV trade-off."
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3268608414239482,"4.3
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3300970873786408,"We can interpret EQUMRL as an approach for (i) a targeting optimization problem to achieve an
expected cumulative reward ζ, (ii) an expected cumulative reward maximization with regularization,
and (iii) expected utility maximization via Taylor approximation."
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3333333333333333,Under review as a conference paper at ICLR 2022
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3365695792880259,"First, we can also interpret EQUMRL as mean squared error (MSE) minimization between a cumula-
tive reward R and a target return ζ; that is,"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.33980582524271846,"arg min
θ∈Θ
J(θ; ζ) = arg min
θ∈Θ
Eπθ
h 
ζ −G
2i
(4)"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.343042071197411,We can decompose the MSE into the bias and variance as
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.34627831715210355,"Eπθ
h 
ζ −G
2i
=
 
ζ −Eπθ[G]
2
|
{z
}
Bias"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.34951456310679613,"+ 2Eπθ
h 
ζ −Eπθ[G]
 
Eπθ[G] −G
i"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.35275080906148865,"|
{z
}
0"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3559870550161812,"+ Eπθ
h 
Eπθ[G] −G
2i"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3592233009708738,"|
{z
}
Variance"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.36245954692556637,"= ζ2 −2ζEπθ[G] +
 
Eπθ[G]
2 + Eπθ
h 
Eπθ[G] −G
2i
."
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3656957928802589,"Thus, the MSE minimization (4) is equivalent to EQUMRL (3), where ζ = α"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.36893203883495146,"β . The above equation
provides an important implication for the setting of ζ. If we know the reward is shifted by x, we only
have to adjust ζ to ζ + x. This is because ζ only affects the bias term in the above equation. The
equation also provides another insight. If our assumption maxπθ Eπθ[G] ≤ζ is violated, Eπθ[G]
will not be maximized with a ﬁxed variance and will be biased towards ζ; that is, EQUMRL cannot
ﬁnd the MV-efﬁcient policies. In applications, we can conﬁrm whether the optimization works by
checking whether average value of the empirically realized cumulative rewards is less than ζ."
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.37216828478964403,"Second, we can regard the quadratic utility function as an expected cumulative reward maximization
with a regularization term deﬁned as E

R2
; that is, minimization of the risk R(πθ):"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.37540453074433655,"R(θ) =
−Eπθ[G]
|
{z
}
Risk of expected cumulative reward maximization"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3786407766990291,"+ ψEπθ

G2"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3818770226537217,"|
{z
}
Regularization term"
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3851132686084142,"where ψ > 0 is a regulation parameter and ψ =
β
2α =
1
2ζ . As ψ →0 (ζ →∞), R(θ) →−Eπθ[G]."
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3883495145631068,"Third, the quadratic utility function is the quadratic Taylor approximation of a smooth utility function
u(G) because for G0 ∈R, we can expand it as u(G) ≈u(G0) + U ′(G0)(G −G0) + U ′′(G0)(G −
G0)2 + · · · ; that is, quadratic utility is an approximation of various risk-averse utility functions. This
property also supports the use of the quadratic utility function in practice (Kroll et al., 1984)."
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.39158576051779936,"It also should be noted that the EQUMRL is closely related to the ﬁelds of economics and ﬁnance,
where the ultimate goal is to maximize the utility of an agent, which is also referred to as an investor.
The quadratic utility function is a standard risk-averse utility function often assumed in ﬁnancial
theory Luenberger et al. (1997) to justify an MV portfolio; that is, an MV portfolio maximizes
an investor’s utility function if the utility function is quadratic (see Appendix A). Therefore, our
approach can be interpreted as a method that directly achieves the original goal.
Remark 3 (Speciﬁcation of utility function (hyper-parameter selection)). Next, we discuss how to
decide the parameters α and β, which are equivalent to ζ, ξ, and ψ. The meanings are equivalent
to constrained conditions of MVRL; that is, we predetermine these hyperparameters depending on
our attitude toward risk. For instance, we propose the following three directions for the parameter
choice. First, we can determine
β
2α = ψ based on economic theory or market research (Ziemba et al.,
1974; Kallberg et al., 1983) (Appendix A). Luenberger et al. (1997) proposed some questionnaires to
investors for the speciﬁcation. Second, we set ζ =
1
2ψ as the targeted reward that investors aim to
achieve. Third, through cross-validation, we can optimize the regularization parameter ψ to maximize
some criteria, such as the Sharpe ratio (Sharpe, 1966). However, we note that in time-series related
tasks, we cannot use standard cross-validation owing to dependency. Therefore, in our experiments
with a real-world ﬁnancial dataset, we show various results under different parameters.
Remark 4 (From MV-efﬁcient RL to MV-controlled RL). We can also apply the MV-efﬁcient RL
method as the MV-controlled RL method. The parameters of the expected quadratic utility function
correspond to the variance that we want to achieve. For example, the larger ζ is in (4), the larger
the variance will be. Although we do not know the explicit correspondence between the parameter
of the expected quadratic utility function and the variance, it is possible to control the variance by
choosing an appropriate policy from among those learned under several parameters. In Figure 2 in
Section 6, we show the mean and variance of several policies trained with several parameters. These
results are measured with test data, but by outputting multiple candidates with the training data (ex.
cross-validation), we can choose a policy with the desired variance."
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.3948220064724919,Under review as a conference paper at ICLR 2022
INTERPRETATIONS OF EQUMRL WITH GRADEINT ESTIMATION,0.39805825242718446,Figure 1: The CRs and Vars in the training process of the experiment using the synthetic dataset.
EQUMRL UNDER PER-STEP VARIANCE PERSPECTIVE,0.40129449838187703,"5
EQUMRL UNDER PER-STEP VARIANCE PERSPECTIVE"
EQUMRL UNDER PER-STEP VARIANCE PERSPECTIVE,0.4045307443365696,"Another direction of MVRL is to consider the trade-off between the per-step mean Eπθ[r(St, At)] and
per-step variance Vπθ(r(St, At)), Eπθ[r(St, At)]
trade-off
⇐⇒Vπθ(r(St, At)). As well as the trajectory
perspective, Bisi et al. (2020) and Zhang et al. (2021b) proposed training a policy to maximize the
penalized objective function, maxθ∈Θ
PT
t=1 γt−1
Eπθ[r(St, At)] −λVπθ(r(St, At))

. We call an
algorithm with this formulation a per-step MV-controlled RL. See Appendix B for more details."
EQUMRL UNDER PER-STEP VARIANCE PERSPECTIVE,0.4077669902912621,"We deﬁne a per-step MV-efﬁcient policy as a policy πθ such that there is no other policy
with θ′ ∈Θ, where, for each t, Eπθ[r(St, At)] ≤Eπθ′[r(St, At)] and Vπθ[r(St, At)] >
Vπθ′[r(St, At)], or Vπθ[r(St, At)] ≥Vπθ′[r(St, At)] and Eπθ[r(St, At)] < Eπθ′[r(St, At)].
For constants α, β > 0, we deﬁne the quadratic utility function for per-step reward setting
as uper-step(r(St, At); α, β) = αr(St, At) −βr2(St, At).
For an ﬁnite horizon case with
γ = 1, we consider maximizing the expected cumulative quadratic utility function deﬁned
as Eπθ
hPT
t=1 uper-step(r(St, At); α, β)
i
= αEπθ
hPT
t=1 r(St, At)
i
−βEπθ
hPT
t=1 r2(St, At)
i
.
When applying REINFORCE-based algorithm to train an agent, the sample approximation
of the gradient is given as b∇Eπθ
hPT
t=1 uper-step(r(Sk
t , Ak
t ); α, β)
i
= PT
t=1

αr(Sk
t , Ak
t ) −"
EQUMRL UNDER PER-STEP VARIANCE PERSPECTIVE,0.4110032362459547,"β
 
r(Sk
t , Ak
t )
2	
∇θ log πθ(Sk
t , Ak
t ). Appendix B contains the performance using a synthetic dataset."
EXPERIMENTS,0.41423948220064727,"6
EXPERIMENTS"
EXPERIMENTS,0.4174757281553398,"This section investigates the empirical performance of the proposed EQUMRL in trajectory variance
setting using synthetic and real-world ﬁnancial datasets. We train the policy by REINFORCE-based
algorithm. We conduct two experiments. In the ﬁrst experiments, following Tamar et al. (2012; 2014),
and Xie et al. (2018), we conduct portfolio management experiments with synthetic datasets. In the
third experiment, we conduct a portfolio management experiment with a dataset of Fama & French
(1992), a standard benchmark in ﬁnance. In Appendix C, following Tamar et al. (2012; 2014), and
Xie et al. (2018), we also conduct American-style option experiments with synthetic datasets. We
implemented algorithms following the Pytorch example1. For algorithms using neural networks, we
use a three layer perceptron, where the numbers of the units in two hidden layers are the same as that
of the input node, and that of the output node is 2. We note that in all results, naively maximizing the
reward or minimizing the variance do not ensure a better algorithm; we evaluate an algorithm based
on how it controls the MV trade-off. We denote the hyperparameter of the EQUMRL by ζ, which has
the same meaning as (α, β) and ψ. For all experiments, we adopt episodic MDPs; that is, γ = 1."
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.42071197411003236,"6.1
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET"
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.42394822006472493,"Following Tamar et al. (2012) and Xie et al. (2018), we consider a portfolio composed of two asset
types: a liquid asset with a ﬁxed interest rate rl, and a non-liquid asset with a time-dependent interest
rate taking either rlow
nl
or rhigh
nl
, and the transition follows a switching probability pswitch. An investor
can sell the liquid asset at every time step t = 1, 2, . . . , T but the non-liquid asset can only be sold
after the maturity W periods. This means that when holding 1 liquid asset, we obtain rl per period;
when holding 1 non-liquid asset at the t-th period, we obtain rlow
nl
or rhigh
nl
at the t + W-th period."
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.42718446601941745,1https://github.com/pytorch/examples/tree/master/reinforcement_learning
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.43042071197411,Under review as a conference paper at ICLR 2022
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.4336569579288026,"Table 1: The results of the portfolio management with a synthetic data. The upper table shows the
CRs and Vars over 100, 000 trials. The lower table shows the MSEs for ζ over 100, 000 trials."
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.4368932038834951,"REINFORCE
EQUM
Tamar
Xie
(ζ = ∞)
ζ = 10
ζ = 6
ζ = 4
V = 80
V = 50
λ = 100
λ = 10
CR
6.729
6.394
4.106
2.189
6.709
2.851
0.316
0.333
Var
32.551
31.210
24.424
18.518
32.586
21.573
15.883
15.992"
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.4401294498381877,"REINFORCE
EQUM
Target Value
(ζ = ∞)
ζ = 10
ζ = 8
ζ = 6
ζ = 4
ζ = 2
MSE from ζ = 10
51.669
53.399
55.890
65.307
83.061
105.492
ζ = 8
42.586
42.975
43.375
45.730
55.816
71.480
ζ = 6
41.503
40.551
38.860
34.154
36.570
45.467
ζ = 4
48.420
46.127
42.345
30.577
25.324
27.455
ζ = 2
63.337
59.703
53.830
35.000
22.078
17.442"
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.44336569579288027,"Besides, the non-liquid asset has a risk of not being paid with a probability prisk; that is, if the
non-liquid asset defaulted during the W periods, we could not obtain any rewards by having the
asset. An investor can change the portfolio by investing a ﬁxed fraction w of the total capital M in
the non-liquid asset at each time step. A typical investment strategy is to construct a portfolio using
both liquid and non-liquid assets for decreasing the variance. Following Xie et al. (2018), we set
rl = 1.001, rlow
nl
= 1.1, rhigh
nl
= 2, pswitch = 0.1, prisk = 0.05, W = 4, w = 0.2, and M = 1. As a
performance metric, we use the average cumulative reward (CR) and its variance (Var) when investing
for 50 periods. We compare the EQUMRL with the REINFORCE, MV-controlled methods proposed
by Tamar et al. (2012) (Tamar), and Xie et al. (2018) (Xie). We denote the variance constraint of
Tamar et al. (2012) as Var and Lagrange multiplier of Xie et al. (2018) as λ. For training Tamar, Xie,
and EQUMRL, we set the Adam optimizer with learning rate 0.01 and weight decay parameter 0.1.
For each algorithm, we report performances under various hyperparameters as much as possible."
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.44660194174757284,"First, we show CRs and Vars of the EQUMRL during the training process in Figure 1, where
we conduct 100 trials on the test environment to compute CRs and Vars for each episode. Here,
we also show the performance of REINFORCE, which corresponds to the EQUMRL with ζ =
α/β = ∞. As Figure 1 shows, the EQUMRL trains MV-efﬁcient policies well depending on the
parameter ζ. Next, we compare the EQUMRL on the test environment with the REINFORCE,
Tamar, and Xie. We conduct 100, 000 trials on the test environment to compute CRs and Vars."
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.44983818770226536,"Figure 2: MV efﬁciency of the portfolio manage-
ment experiment. Higher CRs and lower Vars
methods are MV Pareto efﬁcient."
PORTFOLIO MANAGEMENT WITH A SYNTHETIC DATASET,0.45307443365695793,"In Figure 2, we plot performances under
several hyperparameters, where the hori-
zontal axis denotes the Var, and the vertical
axis denotes the CR. Trained agents with
a higher CR and lower Var are Pareto efﬁ-
cient. As the result shows, the EQUMRL
returns more efﬁcient portfolios than the
others in almost all cases. We conjecture
that this is because while the EQUMRL is
an end-to-end optimization for obtaining an
efﬁcient agent, the other methods consist of
several steps for solving the constrained op-
timization, where those multiple steps can
be sources of the suboptimal result. We
show CRs and Vars of some of their results
in the upper table of Table 1. The MSEs
between ζ and CR are also shown in the
lower table of Table 1, where we can conﬁrm that the EQUMRL succeeded in minimizing the MSEs."
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.4563106796116505,"6.2
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET"
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.459546925566343,"We use standard benchmarks called Fama & French (FF) datasets2 to ensure the reproducibility (Fama
& French, 1992). Among FF datasets, we use the FF25, FF48 and FF100 datasets, where the FF25
and FF 100 dataset includes 25 and 100 assets formed based on size and book-to-market ratio; the
FF48 dataset contains 48 assets representing different industrial sectors. We use all datasets covering"
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.4627831715210356,"2https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.
html"
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.46601941747572817,Under review as a conference paper at ICLR 2022
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.4692556634304207,"Table 2: The performance of each portfolio model during the out-of-sample period (from July 2000
to June 2020) for FF25 dataset (upper table) , FF48 (middle table), and FF100 (lower table). For each
dataset, the best performance is highlighted in bold."
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.47249190938511326,"Method
EW
MV
EGO
BLD
Tamar
Xie
EQUM
V = 15
V = 30
V = 60
λ = 10
λ = 100
λ = 1000
ζ = 0.5
ζ = 0.75
ζ = 1.5
FF25
CR↑
0.80
0.09
0.81
0.84
1.15
1.01
0.80
0.90
1.15
1.15
1.53
1.25
1.11
Var↓
28.62
53.57
30.65
22.16
18.53
15.04
25.29
25.96
25.77
25.77
24.27
15.02
11.77
R/R↑
0.52
0.04
0.51
0.62
0.93
0.90
0.55
0.61
0.79
0.79
1.07
1.12
1.13
MaxDD↓
0.54
0.75
0.58
0.52
0.35
0.35
0.56
0.54
0.51
0.51
0.36
0.31
0.27
FF48
CR↑
0.81
0.11
0.97
0.75
0.68
0.38
0.82
0.50
1.08
1.01
1.60
1.05
0.91
Var↓
22.91
77.02
31.91
15.98
40.69
16.00
18.04
27.12
23.77
26.31
31.97
18.69
10.34
R/R↑
0.59
0.04
0.60
0.65
0.37
0.33
0.67
0.33
0.76
0.68
0.98
0.84
0.98
MaxDD↓
0.30
0.48
0.31
0.25
0.32
0.20
0.21
0.27
0.25
0.26
0.29
0.21
0.16
FF100
CR↑
0.81
0.11
0.81
0.85
1.13
0.79
0.67
0.57
0.98
1.24
0.95
0.95
1.43
Var↓
29.36
57.97
32.35
21.83
25.50
20.34
16.50
87.92
28.78
41.06
15.50
14.26
33.09
R/R↑
0.52
0.05
0.49
0.63
0.77
0.61
0.57
0.21
0.63
0.67
0.83
0.87
0.86
MaxDD↓
0.33
0.46
0.34
0.27
0.25
0.23
0.25
0.51
0.32
0.31
0.19
0.26
0.31"
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.47572815533980584,"monthly data from July 1980 to June 2020. Consider an episodic MDP. Let the action space be the set
of m assets, and ya,t and wa,t be the return and the portfolio weight of an asset a at time t. The reward
(portfolio return) at time 1 ≤t ≤T is deﬁned as yt = Pm
a=1 ya,twa,t −Λ Pm
a=1 |wa,t −wa,t−1|,
where Λ = 0.001 is the penalty of the portfolio weight turnover(change of the action). On the t-th
period, the agent observes a state St = ((ya,t−1, ..., ya,t−12, wa,t−1)a∈A, Pt−1
s=0 ys), and decides a
portfolio weight (wa,t)a∈A as wa,t = π(a, st). Between periods t and t + 1, the agent has an asset a
with the ratio wa,t.See Section 4.1 for the reason why we include sumt−1
s=0ys in the state."
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.47896440129449835,"We use the following portfolio models: an equally-weighted portfolio (EQ, DeMiguel et al., 2009);
a mean-variance portfolio (MV, Markowitz, 1952); a Kelly growth optimal portfolio Shen et al.
(2019); Portfolio blending via Thompson sampling (BLD, Shen & Wang, 2016). We also compare
our proposed method with the methods proposed by Tamar et al. (2012) and Xie et al. (2018).
Denote a method proposed by Tamar et al. (2012) by Tamar, and choose the parameter var from
{15, 30, 60}. Denote a method proposed by Xie et al. (2018) by Xie, and choose the parameter
λ from {10, 100, 1000}. Denote the REINFORCE-based trajectory EQUMRL by EQUMRL, and
choose the parameter ζ from {0.5, 0.75, 1.5}."
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.48220064724919093,"We apply the following standard measures in ﬁnance for evaluation (Brandt, 2010). The cumulative
reward (CR), annualized risk as the standard deviation of return (RISK) and risk-adjusted return
(R/R) are deﬁned as follows: CR = 1/T PT
t=1 yt, V = 1/T PT
t=1(yt −CR)2, and R/R =
√"
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.4854368932038835,"12 × CR/
√"
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.4886731391585761,"Var. R/R is the most important measure for a portfolio strategy and is often referred to
as the Sharpe Ratio (Sharpe, 1966). We also evaluate the maximum draw-down (MaxDD), which
is another widely used risk measure (Magdon-Ismail & Atiya, 2004). In particular, MaxDD is the
largest drop from a peak deﬁned as MaxDD = mint∈[1,T ]

0,
Wt
maxτ∈[1,t] Wτ −1

, where Wk is the"
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.4919093851132686,"cumulative return of the portfolio until time k; that is, Wt = Qt
t′=1(1 + yt′)."
PORTFOLIO MANAGEMENT WITH A REAL-WORLD DATASET,0.49514563106796117,"Table 2 reports the performances of the portfolios. In almost all cases, the EQUMRL achieves the
highest R/R and the lowest MaxDD. Therefore, we can conﬁrm that the EQUMRL has a high R/R,
and avoids a large drawdown. The real objective (minimizing variance with a penalty on return
targeting) for Tamar, MVP, and EQUMRL is shown in Appendix D. Except for FF48’s MVP, the
objective itself is smaller than that of EQUMRL. Since the values of the objective are proportional to
R/R, we can empirically conﬁrm that the better optimization, the better performance."
CONCLUSION,0.49838187702265374,"7
CONCLUSION"
CONCLUSION,0.5016181229773463,"In this paper, we proposed EQUMRL for MV-efﬁcient RL. Compared with the conventional MVRL
methods, EQUMRL is computationally friendly. The proposed EQUMRL also includes various inter-
pretations, such as targeting optimization and regularization, which expands the scope of applications
of the method. We investigated the effectiveness of the EQUMRL compared with the standard RL
and existing MVRL methods through experiments using synthetic and real-world datasets."
CONCLUSION,0.5048543689320388,Under review as a conference paper at ICLR 2022
REFERENCES,0.5080906148867314,REFERENCES
REFERENCES,0.511326860841424,"Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G. Optimality and approximation with
policy gradient methods in markov decision processes. In Abernethy, J. and Agarwal, S. (eds.),
Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of
Machine Learning Research, pp. 64–66, 09–12 Jul 2020."
REFERENCES,0.5145631067961165,"Baron, D. P. On the utility theoretic foundations of mean-variance analysis. The Journal of Finance,
32, 1977."
REFERENCES,0.517799352750809,"Baxter, J. and Bartlett, P. L. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial
Intelligence Research, 15:319–350, 2001."
REFERENCES,0.5210355987055016,"Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-dynamic programming: An overview. In Proceedings of
1995 34th IEEE Conference on Decision and Control, volume 1, pp. 560–564. IEEE, 1995."
REFERENCES,0.5242718446601942,"Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-dynamic programming. Athena Scientiﬁc, Belmont, MA,
1996."
REFERENCES,0.5275080906148867,"Bisi, L., Sabbioni, L., Vittori, E., Papini, M., and Restelli, M. Risk-averse trust region optimization
for reward-volatility reduction. In Proceedings of the Twenty-Ninth International Joint Conference
on Artiﬁcial Intelligence, 2020."
REFERENCES,0.5307443365695793,"Bodnar, T., Parolya, N., and Schmid, W. On the exact solution of the multi-period portfolio choice
problem for an exponential utility under return predictability. European Journal of Operational
Research, 246(2):528–542, 2015a."
REFERENCES,0.5339805825242718,"Bodnar, T., Parolya, N., and Schmid, W. A closed-form solution of the multi-period portfolio choice
problem for a quadratic utility function. Annals of Operations Research, 229(1):121–158, 2015b."
REFERENCES,0.5372168284789643,"Bodnar, T., Okhrin, Y., Vitlinskyy, V., and Zabolotskyy, T. Determination and estimation of risk
aversion coefﬁcients. Computational Management Xcience, 15(2):297–317, 2018."
REFERENCES,0.540453074433657,"Borch, K. A Note on Uncertainty and Indifference Curves. The Review of Economic Studies, 1969."
REFERENCES,0.5436893203883495,"Boyd, S. and Vandenberghe, L. Convex Optimization. Cambridge University Press, 2004."
REFERENCES,0.5469255663430421,"Brandt, M. W. Portfolio choice problems. In Handbook of Financial Econometrics: Tools and
Techniques, pp. 269–336. Elsevier, 2010."
REFERENCES,0.5501618122977346,"Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W.
Openai gym, 2016."
REFERENCES,0.5533980582524272,"Bulmus¸, T. and ¨Ozekici, S. Portfolio selection with hyperexponential utility functions. OR spectrum,
36(1):73–93, 2014."
REFERENCES,0.5566343042071198,"Chow, Y. and Ghavamzadeh, M. Algorithms for cvar optimization in mdps. Advances in Neural
Information Processing Systems, 27:3509–3517, 2014."
REFERENCES,0.5598705501618123,"Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M. Risk-constrained reinforcement learning
with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070–6120, 2017."
REFERENCES,0.5631067961165048,"DeMiguel, V., Garlappi, L., and Uppal, R. Optimal versus naive diversiﬁcation: How inefﬁcient is
the 1/n portfolio strategy? The Review of Financial Studies, 22(5):1915–1953, 2009."
REFERENCES,0.5663430420711975,"Fama, E. F. and French, K. R. The cross-section of expected stock returns. The Journal of Finance,
1992."
REFERENCES,0.56957928802589,"Fishburn, P. C. and Burr Porter, R. Optimal portfolios with one safe and one risky asset: Effects of
changes in rate of return and risk. Management science, 22(10):1064–1073, 1976."
REFERENCES,0.5728155339805825,"Garcıa, J. and Fern´andez, F. A comprehensive survey on safe reinforcement learning. Journal of
Machine Learning Research, 16(1):1437–1480, 2015."
REFERENCES,0.5760517799352751,"Geibel, P. and Wysotzki, F. Risk-sensitive reinforcement learning applied to control under constraints.
Journal of Artiﬁcial Intelligence Research, 24:81–108, 2005."
REFERENCES,0.5792880258899676,Under review as a conference paper at ICLR 2022
REFERENCES,0.5825242718446602,"Kallberg, J., Ziemba, W., et al. Comparison of alternative utility functions in portfolio selection
problems. Management Science, 29(11):1257–1276, 1983."
REFERENCES,0.5857605177993528,"Kroll, Y., Levy, H., and Markowitz, H. M. Mean-variance versus direct utility maximization. The
Journal of Finance, 39(1):47–61, 1984."
REFERENCES,0.5889967637540453,"Lintner, J. The valuation of risk assets and the selection of risky investments in stock portfolios and
capital budgets. Review of Economics and Statistics, 47(1):13–37, 1965."
REFERENCES,0.5922330097087378,"Luenberger, D. G. et al. Investment science. OUP Catalogue, 1997."
REFERENCES,0.5954692556634305,"Magdon-Ismail, M. and Atiya, A. F. Maximum drawdown. Risk Magazine, 2004."
REFERENCES,0.598705501618123,"Markowitz, H. Portfolio selection. The Journal of Finance, 1952."
REFERENCES,0.6019417475728155,"Markowitz, H. Portfolio selection: efﬁcient diversiﬁcation of investments. Yale university press,
1959."
REFERENCES,0.6051779935275081,"Markowitz, H. M. and Todd, G. P. Mean-Variance Analysis in Portfolio Choice and Capital Markets,
volume 66. John Wiley & Sons, 2000."
REFERENCES,0.6084142394822006,"Mas-Colell, A., Whinston, M. D., Green, J. R., et al. Microeconomic Theory, volume 1. Oxford
university press New York, 1995."
REFERENCES,0.6116504854368932,"Merton, R. C. Lifetime portfolio selection under uncertainty: The continuous-time case. The Review
of Economics and Statistics, pp. 247–257, 1969."
REFERENCES,0.6148867313915858,"Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K.
Asynchronous methods for deep reinforcement learning. In International Conference on Machine
Learning, pp. 1928–1937, 2016."
REFERENCES,0.6181229773462783,"Morgenstern, O. and Von Neumann, J. Theory of games and economic behavior. Princeton University
Press, 1953."
REFERENCES,0.6213592233009708,"Mossin, J. Equilibrium in a capital asset market. Econometrica: Journal of the econometric society,
pp. 768–783, 1966."
REFERENCES,0.6245954692556634,"Prashanth, L. and Ghavamzadeh, M. Actor-critic algorithms for risk-sensitive mdps. In Proceedings
of the 26th International Conference on Neural Information Processing Systems-Volume 1, pp.
252–260, 2013."
REFERENCES,0.627831715210356,"Prashanth, L. and Ghavamzadeh, M. Variance-constrained actor-critic algorithms for discounted and
average reward mdps. Machine Learning, 105(3):367–417, 2016."
REFERENCES,0.6310679611650486,"Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 1994."
REFERENCES,0.6343042071197411,"Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region policy optimization. In
International Conference on Machine Learning, pp. 1889–1897, 2015."
REFERENCES,0.6375404530744336,"Sharpe, W. F. Capital asset prices: A theory of market equilibrium under conditions of risk. The
Journal of Finance, 19(3):425–442, 1964."
REFERENCES,0.6407766990291263,"Sharpe, W. F. Mutual fund performance. The Journal of Business, 1966."
REFERENCES,0.6440129449838188,"Shen, W. and Wang, J. Portfolio blending via thompson sampling. In Proceedings of the Twenty-Fifth
International Joint Conference on Artiﬁcial Intelligence, pp. 1983–1989, 2016."
REFERENCES,0.6472491909385113,"Shen, W., Wang, B., Pu, J., and Wang, J. The kelly growth optimal portfolio with ensemble learning.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 1134–1141, 2019."
REFERENCES,0.6504854368932039,"Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. The MIT Press, 1998."
REFERENCES,0.6537216828478964,"Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y.
Policy gradient methods for
reinforcement learning with function approximation. In Advances in Neural Information Processing
Systems, pp. 1057–1063, 2000."
REFERENCES,0.656957928802589,Under review as a conference paper at ICLR 2022
REFERENCES,0.6601941747572816,"Tamar, A., Di Castro, D., and Mannor, S. Policy gradients with variance related risk criteria. In
Proceedings of the 29th International Coference on International Conference on Machine Learning,
pp. 1651–1658, 2012."
REFERENCES,0.6634304207119741,"Tamar, A., Mannor, S., and Xu, H. Scaling up robust mdps using function approximation. In
International Conference on Machine Learning, pp. 181–189, 2014."
REFERENCES,0.6666666666666666,"Tobin, J. Liquidity preference as behavior towards risk. The Review of Economic Studies, 25(2):
65–86, 1958."
REFERENCES,0.6699029126213593,"Williams, R. Toward a Theory of Reinforcement-learning Connectionist Systems. Northeastern
University, 1988."
REFERENCES,0.6731391585760518,"Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229–256, 1992."
REFERENCES,0.6763754045307443,"Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning
algorithms. Connection Science, 3(3):241–268, 1991."
REFERENCES,0.6796116504854369,"Xie, T., Liu, B., Xu, Y., Ghavamzadeh, M., Chow, Y., Lyu, D., and Yoon, D. A block coordinate
ascent algorithm for mean-variance optimization. Advances in Neural Information Processing
Systems, 31:1065–1075, 2018."
REFERENCES,0.6828478964401294,"Zhang, J., Kim, J., O’Donoghue, B., and Boyd, S. Sample efﬁcient reinforcement learning with
reinforce. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 35(12):10887–10895,
May 2021a."
REFERENCES,0.686084142394822,"Zhang, S., Liu, B., and Whiteson, S. Mean-variance policy iteration for risk-averse reinforcement
learning. In Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence, 2021b."
REFERENCES,0.6893203883495146,"Ziemba, W., Parkan, C., and Brooks-Hill, R. Calculation of investment portfolios with risk free
borrowing and lending. Management Science, 21(2), 1974."
REFERENCES,0.6925566343042071,Under review as a conference paper at ICLR 2022
REFERENCES,0.6957928802588996,"A
PRELIMINARIES OF ECONOMIC AND FINANCIAL THEORY"
REFERENCES,0.6990291262135923,"A.1
UTILITY THEORY"
REFERENCES,0.7022653721682848,"Utility theory is the foundation of the choice theory under uncertainty including economics and
ﬁnancial theory. A utility function u(·) measures agent’s relative preference for different levels of
total wealth W. According to Morgenstern & Von Neumann (1953), a rational agent makes an
investment decision to maximize the expected utility of wealth among a set of competing feasible
investment alternatives. For simplicity, the following assumptions are often made for the utility
function used in economics and ﬁnance. First, the utility function is assumed to be at least twice
continuous differentiable. The ﬁrst derivative of the utility function (the marginal utility of wealth) is
always positive, i.e., U
′(W) > 0, because of the assumption of non-satiation. The second assumption
concerns risk attitude of the agents called “risk averse”. When we assume that an agent is risk averse,
the utility function is described as a curve that increases monotonically and is concave. The most
often used utility function of a risk-averse agent is the quadratic utility function as follows:"
REFERENCES,0.7055016181229773,"u(W; α, β) = αW −β 1"
REFERENCES,0.7087378640776699,"2W 2
(5)"
REFERENCES,0.7119741100323624,"where α ≥0, β > 0. Taking the expected value of the quadratic utility function in (5) yields:"
REFERENCES,0.7152103559870551,"E[u(W; α, β)] = αE[W] −1"
REFERENCES,0.7184466019417476,"2βE[W 2]
(6)"
REFERENCES,0.7216828478964401,Substituting E[W 2] = V[W] + E[W]2 into (5) gives
REFERENCES,0.7249190938511327,"E[u(W; α, β)] = αE[W] −1"
REFERENCES,0.7281553398058253,"2β(V[W] + E[W]2)
(7)"
REFERENCES,0.7313915857605178,"Equation (7) shows that expected quadratic utility can be described in terms of mean E[W] and
variance V[W] of wealth. Therefore, the assumption of a quadratic utility function is crucial to the
mean-variance analysis.
Remark 5 (Approximation by quadratic utility function). Readers may be interested in how the
quadratic utility function approximates other risk averse utility functions. Kroll et al. (1984) empiri-
cally answered this question by comparing MV portfolio (maximizer of expected quadratic utility
function) and maximizers of other utility functions. In their study, maximizers of other utility func-
tions also almost located in MV Pareto efﬁcient frontier; that is, expected quadratic utility function
approximates other risk averse utility function well.
Remark 6 (Non-vNM utility functions). Unlike MV trade-off, utility functions maximized by some
recently proposed new risk criteria, such as VaR and Prospect theory, do not belong to traditional
vNM utility function."
REFERENCES,0.7346278317152104,"A.2
MARKOWITZ’S PORTFOLIO"
REFERENCES,0.7378640776699029,"Considering the mean-variance trade-off in a portfolio and economic activity is an essential task in
economics as Tamar et al. (2012) and Xie et al. (2018) pointed out. The mean-variance trade-off is
justiﬁed by assuming either quadratic utility function to the economic agent or multivariate normal
distribution to the ﬁnancial asset returns (Borch, 1969; Baron, 1977; Luenberger et al., 1997). This
means that if either the agent follows the quadratic utility function or asset return follows the normal
distribution, the agent’s expected utility function is maximized by maximizing the expected reward
and minimizing the variance. Therefore, the goal of Markowitz’s portfolio is not only to construct the
portfolio itself but also to maximize the expected utility function of the agent. See Figure 3."
REFERENCES,0.7411003236245954,"Markowitz (1952) proposed the following steps for constructing a MV controlled portfolio (Also see
Markowitz (1959), page 288, and Luenberger et al. (1997)):"
REFERENCES,0.7443365695792881,"• Constructing portfolios minimizing the variance under several reward constraint;
• Among the portfolios constructed in the ﬁrst step, the economic agent chooses a portfolio
maximizing the utility function."
REFERENCES,0.7475728155339806,Under review as a conference paper at ICLR 2022
REFERENCES,0.7508090614886731,"Figure 3: The concept of MV Preto efﬁcient frontier and difference between EQUMRL and existing
MVRL approaches."
REFERENCES,0.7540453074433657,"Conventional ﬁnancial methods often adopt this two-step approach because directly predicting the
reward and variance to maximize the expected utility function is difﬁcult; therefore, ﬁrst gathering
information based on analyses of an economist, then we construct the portfolio using the information
and provide the set of the portfolios to an economic agent. However, owing to the recent development
of machine learning, we can directly represent the complicated economic dynamics using ﬂexible
models, such as deep neural networks. In addition, as Tamar et al. (2012) and Xie et al. (2018)
reported, when constructing the mean-variance portfolio in RL, we suffer from the double sampling
issue. Therefore, this paper aims to achieve the original goal of the mean-variance approach; that is,
the expected utility maximization. Note that this idea is not restricted to ﬁnancial applications but can
be applied to applications where the agent utility can be represented only by the mean and variance."
REFERENCES,0.7572815533980582,"A.3
MARKOWITZ’S PORTFOLIO AND CAPITAL ASSET PRICING MODEL"
REFERENCES,0.7605177993527508,"Markowitz’s portfolio is known as the mean-variance portfolio (Markowitz, 1952; Markowitz &
Todd, 2000). Constructing the mean-variance portfolio is motivated by the agent’s expected utility
maximization. When the utility function is given as the quadratic utility function, or the ﬁnancial
asset returns follow the multivariate normal distribution, a portfolio maximizing the agent’s expected
utility function is given as a portfolio with minimum variance under a certain standard expected
reward."
REFERENCES,0.7637540453074434,"The Capital Asset Pricing Model (CAPM) theory is a concept which is closely related to Markowitz’s
portfolio (Sharpe, 1964; Lintner, 1965; Mossin, 1966). This theory theoretically explains the expected
return of investors when the investor invests in a ﬁnancial asset; that is, it derives the optimal price of
the ﬁnancial asset. To derive this theory, as well as Markowitz’s portfolio, we assume the quadratic
utility function to the investors or the multivariate normal distribution to the ﬁnancial assets."
REFERENCES,0.7669902912621359,"Merton (1969) extended the static portfolio selection problem to a dynamic case. Fishburn &
Burr Porter (1976) studied the sensitivity of the portfolio proportion when the safe and risky asset
distributions change under the quadratic utility function. Thus, there are various studies investigating
relationship between the utility function and risk-averse optimization (Tobin, 1958; Kroll et al., 1984;
Bulmus¸ & ¨Ozekici, 2014; Bodnar et al., 2015a;b)."
REFERENCES,0.7702265372168284,"A.4
MV PORTFOLIO AND MVRL"
REFERENCES,0.7734627831715211,"Traditional portfolio theory have attempted to maximize the expected quadratic utility function by
providing MV portfolios. This is because MV portfolio is easier to interpret than EQUMRL, and
we can obtain a solution by quadratic programming. One of the main goals of MVRL methods is
also to construct MV portfolio under a dynamic environment (Tamar et al., 2012). However, we
conjecture that there are three signiﬁcant differences between them. First, unlike static problem,
MVRL suffers computational difﬁculties. Second, while static problem solves quadratic programming
given the expected reward and variance, MVRL methods simultaneously estimate these values and
solve the constrained problem. Third, while static MV portfolios gives us an exact solution of the
constrained problem, MVRL often relaxes the constrained problem by penalized method, which"
REFERENCES,0.7766990291262136,Under review as a conference paper at ICLR 2022
REFERENCES,0.7799352750809061,"causes approximation errors. In particular, for the second and third points, the difference in how to
handle the estimators of expected reward and variance is essential."
REFERENCES,0.7831715210355987,"A.5
EMPIRICAL STUDIES ON THE UTILITY FUNCTIONS"
REFERENCES,0.7864077669902912,"The standard ﬁnancial theory is built on the assumption that the economic agent has the quadratic
utility function. For supporting this theory, there are several empirical studies to estimate the
parameters of the quadratic utility function. Markowitz & Todd (2000) discussed how the quadartic
utility function approximates the other risk-averse utility functions. Ziemba et al. (1974) investigated
the change of the portfolio proportion when the parameter of the quadratic utility function changes
using the Canadian ﬁnancial dataset. Recently, Bodnar et al. (2018) investigate the risk parameter (α
and β in our formulation of the quadratic utility function) using the markets indexes in the world.
They found that the utility function parameter depends on the market data model."
REFERENCES,0.7896440129449838,"A.6
CRITICISM"
REFERENCES,0.7928802588996764,"For the simple form of the quadratic utility function, the ﬁnancial models based on the utility are
widely accepted in practice. However, there is also criticism that the simple form cannot capture the
real-world complicated utility function. For instance, Kallberg et al. (1983) criticized the use of the
quadratic utility function and proposed using a utility function, including higher moments. This study
also provided empirical studies using U.S. ﬁnancial dataset for investigating the properties of the
alternative utility functions. However, to the best of our knowledge, ﬁnancial practitioners still prefer
ﬁnancial models based on the quadratic utility function. We consider this is because the simple form
gains the interpretability of the ﬁnancial models."
REFERENCES,0.7961165048543689,"A.7
ECONOMICS AND FINANCE"
REFERENCES,0.7993527508090615,"To mathematically describe an attitude toward risk, economics and ﬁnance developed expected
utility theory, which assumes the Bernoulli utility function u(R) on an agent. In the expected
utility theory, an agent acts to maximize the von Neumann-Morgenstern (vNM) utility function
U(F(r)) =
R
u(r)dF(r), where F(r) is a distribution of R (Mas-Colell et al., 1995). We can relate
the utility function form to agents with three different risk preferences: the utility function u(R)
is concave for risk averse agents; linear for risk neutral agents; and convex risk seeking agents.
For instance, an agent with u(R) = R corresponds to a risk neutral agent attempting to maximize
their expected cumulative reward in a standard RL problem. For more detailed explanation, see
Appendix A or standard textbooks of economics and ﬁnance, such as Mas-Colell et al. (1995) and
Luenberger et al. (1997)."
REFERENCES,0.8025889967637541,"To make the Bernoulli utility function more meaningful, we assume that it is increasing function
with regard to R; that is, R ≤α"
REFERENCES,0.8058252427184466,"β for all possible R. Even without the assumption, for a given pair of
(α, β), a optimal policy maximizing the expected utility does not change; that is, the assumption is
only related to interpretation of the quadratic utility function and does not affect the optimization."
REFERENCES,0.8090614886731392,"On the other hand, the constraint condition is determined by an investor to maximize its expected
utility (Luenberger et al., 1997). Finally, in theories of economics and ﬁnance, investors can maximize
their utility by choosing a portfolio from MV portfolios. In addition, when R ≤α"
REFERENCES,0.8122977346278317,"β for all possible
value of R, a policy maximizing the expected utility function is also located on MV Pareto efﬁcient
frontier."
REFERENCES,0.8155339805825242,"B
PER-STEP VARIANCE PERSPECTIVE"
REFERENCES,0.8187702265372169,"Bisi et al. (2020) deﬁnes the per-step reward random variable R, a discrete random variable
taking the values in the image of r, by deﬁning its probability mass function as p(R = x) =
P"
REFERENCES,0.8220064724919094,"s,a dπθ(s, a)1[r(s, a) = x], where 1 is the indicator function and dπθ is the normalized discounted
state-action distribution, dπθ = (1 −γ) P∞
t=0 γtPrµ0,πθ,p(St = s, At = a). Here, it is known that
for γ < 1, Eπθ[G] =
1
1−γ
P"
REFERENCES,0.8252427184466019,"s,a dπθ(s, a)r(s, a). It follows that Eπθ[R] = (1 −γ)J(πθ). Bisi et al."
REFERENCES,0.8284789644012945,"(2020) showed that the per-step variance Vπθ(R) ≤
Vπθ (r(St,At))"
REFERENCES,0.8317152103559871,"(1−γ)2
, which implies that the minimiza-"
REFERENCES,0.8349514563106796,Under review as a conference paper at ICLR 2022
REFERENCES,0.8381877022653722,"tion of the per-step variance Vπθ(r(St, At)) also minimizes trajectory-variance Vπθ(R). Therefore,
they train a policy πθ by maximizing Jκ(π) = Eπθ[r(St, At)] −κVπθ(r(St, At)), where κ > 0 is a
parameter of the penalty function. Bisi et al. (2020) reveals that Jκ(π) = Eπθ[R −κ(R −Eπθ[R])2],
which implies that optimizing Jκ(θ) is equivalent to optimize the canonical risk-neutral objec-
tive of a new MDP with the same as the original MDP except that the new reward function
r′(s, a) = r(s, a) −λ
 
r(s, a) −(1 −γ)J(π)
2. This reward function depends on the policy
π, making the reward function nonstationary and conventional RL method unusable. This problem is
called policy-dependent-reward issue. To solve this policy-dependent-reward issue, the methods of
Bisi et al. (2020) and Zhang et al. (2021b) are based on the trust region policy optimization (Schulman
et al., 2015) and coordinate descent with Legendre-Fenchel duality (Xie et al., 2018), respectively."
REFERENCES,0.8414239482200647,"For constants α, β > 0, we deﬁne the quadratic utility function for per-step reward setting as"
REFERENCES,0.8446601941747572,"uper-step(r(St, At); α, β) = αr(St, At) −βr2(St, At)."
REFERENCES,0.8478964401294499,"For an inﬁnite horizon case with γ < 1, we consider maximizing the following expected cumulative
quadratic utility function: Eπθ "" ∞
X"
REFERENCES,0.8511326860841424,"t=1
γt−1uper-step(r(St, At); α, β) #"
REFERENCES,0.8543689320388349,"= αEπθ "" ∞
X"
REFERENCES,0.8576051779935275,"t=1
γt−1r(St, At) # −βEπθ "" ∞
X"
REFERENCES,0.86084142394822,"t=1
γt−1r2(St, At) #"
REFERENCES,0.8640776699029126,"For an ﬁnite horizon case with γ = 1, we consider maximizing the following expected cumulative
quadratic utility function: Eπθ "" T
X"
REFERENCES,0.8673139158576052,"t=1
uper-step(r(St, At); α, β) #"
REFERENCES,0.8705501618122977,"= αEπθ "" T
X"
REFERENCES,0.8737864077669902,"t=1
r(St, At) # −βEπθ "" T
X"
REFERENCES,0.8770226537216829,"t=1
r2(St, At) #"
REFERENCES,0.8802588996763754,"When applying REINFORCE-based algorithm to train an agent by maximizing the objective function,
the sample approximation of the gradient, for instance, is given as b∇Eπθ "" T
X"
REFERENCES,0.883495145631068,"t=1
uper-step(r(Sk
t , Ak
t ); α, β) # = α T
X"
REFERENCES,0.8867313915857605,"t=1
r(Sk
t , Ak
t )∇θ log πθ(Sk
t , Ak
t ) −β T
X t=1"
REFERENCES,0.889967637540453," 
r(Sk
t , Ak
t )
2∇θ log πθ(Sk
t , Ak
t )"
REFERENCES,0.8932038834951457,"We investigate the empirical performance using the same setting as the portfolio management
experiment of Section 6.1. Let ζ be α/β in the trajectory EQUMRL, and ρ be α/β in the per-
step EQUMRL. Under both the trajectory and per-step EQUMRLs, we train policies with the
REINFORCE-based algorithm. The other settings are identical to that in Section 6.1. As Figure 4
shows, under both the trajectory and per-step EQUMRLs, the REINFORCE-based algorithms train
MV-efﬁcient policies well for the parameters ζ and ρ."
REFERENCES,0.8964401294498382,"C
AMERICAN-STYLE OPTION WITH A SYNTHETIC DATASET"
REFERENCES,0.8996763754045307,"An American-style option refers to a contract that we can execute an option right at any time before
the maturity time τ; that is, a buyer who bought a call option has a right to buy the asset with the
strike price Kcall at any time; a buyer who bought a put option has a right to sell the with the strike
price Kput at any time."
REFERENCES,0.9029126213592233,"In the setting of Tamar et al. (2014) and Xie et al. (2018), the buyer simultaneously buy call and
put options, which have the strike price Kcall = 1.5 and Kput = 1., respectively. The maturity
time is set as τ = 20. If the buyer executes the option at time t, the buyer obtains a reward
rt = max(0, Kput −xt) + max(0, xt −Wcall), where xt is an asset price. We set x0 = 1 and
deﬁne the stochastic process as follows: xt = xt−1fu with probability 0.45 and xt = xt−1fd with
probability 0.55, where fu and fd. These parameters follows Xie et al. (2018)."
REFERENCES,0.9061488673139159,"As well as Section 6.1, we compare the EQUMRL with policy gradient (EQUM) with the REIN-
FORCE, Tamar, and Xie. The other settings are also identical to Section 6.1. We show performances
under several hyperparameter in Figure 5 and CRs and Vars of some of their results in the upper table"
REFERENCES,0.9093851132686084,Under review as a conference paper at ICLR 2022
REFERENCES,0.912621359223301,"Figure 4: MV efﬁciency of the portfolio management experiment with the trajectory and per-step
EQUMRLs. Higher CRs and lower Vars methods are MV Pareto efﬁcient."
REFERENCES,0.9158576051779935,"Figure 5: MV efﬁciency of the American-style option experiment. Higher CRs and lower Vars
methods are MV Pareto efﬁcient."
REFERENCES,0.919093851132686,"of Table 3. We also denote MSE between ζ and achieved CR of in the lower table of Table 3. From
the table, we can ﬁnd that while EQUMRL minimize the MSE for lower ζ, the MSE of REINFORCE
is smaller for higher ζ. We consider this is because owing to the difﬁculty of MV control under this
setting, naively maximizing the CR minimizes the MSE more than considering the MV trade-off for
higher ζ."
REFERENCES,0.9223300970873787,"D
DETAILS OF EXPERIMENTS OF PORTFOLIO MANAGEMENT WITH A
REAL-WORLD DATASET"
REFERENCES,0.9255663430420712,"We use the following portfolio models. An equally-weighted portfolio (EW) weights the ﬁnancial
assets equally (DeMiguel et al., 2009). A mean-variance portfolio (MV) computes the optimal
variance under a mean constraint (Markowitz, 1952). To compute the mean vector and covariance
matrix, we use the latest 10 years (120 months) data. A Kelly growth optimal portfolio with
ensemble learning (EGO) is proposed by Shen et al. (2019). We set the number of resamples as"
REFERENCES,0.9288025889967637,Under review as a conference paper at ICLR 2022
REFERENCES,0.9320388349514563,"Table 3: The results of American-style option. The upper table shows the CRs and Vars over 100, 000
trials. The lower table shows The lower table shows the MSEs for ζ over 100, 000 trials."
REFERENCES,0.9352750809061489,"REINFORCE
EQUM
Tamar
Xie
(ζ = ∞)
ζ = 1.6
ζ = 0.8
ζ = 0.4
V = 1
V = 0.1
λ = 100
λ = 0.1
CR
0.353
0.352
0.341
0.322
0.352
0.349
0.335
0.333
Var
0.099
0.098
0.092
0.083
0.102
0.096
0.089
0.088"
REFERENCES,0.9385113268608414,"REINFORCE
EQUM
Target Value
(ζ = ∞)
ζ = 2.2
ζ = 1.8
ζ = 1.4
ζ = 1.0
ζ = 0.6
MSE from ζ = 2.2
3.512
3.512
3.517
3.523
3.532
3.572
ζ = 1.8
2.194
2.195
2.197
2.203
2.209
2.239
ζ = 1.4
1.197
1.197
1.198
1.202
1.206
1.226
ζ = 1.0
0.520
0.520
0.519
0.522
0.523
0.532
ζ = 0.6
0.162
0.163
0.160
0.161
0.160
0.159"
REFERENCES,0.941747572815534,"m1 = 50, the size of each resample m2 = 5τ, the number of periods of return data τ = 60,
the number of resampled subsets m3 = 50, and the size of each subset m4 = n0.7, where m is
number of assets; that is, m = 25 in FF25, m = 48 in FF48 and m = 100 in FF100. Portfolio
blending via Thompson sampling (BLD) is proposed by Shen & Wang (2016). We use the latest
10 years (120 months) data to compute for the sample covariance matrix and blending parameters.
Denote a method proposed by Tamar et al. (2012) by Tamar, and choose the parameter var from
{15, 30, 60}. Denote a method proposed by Xie et al. (2018) by Xie, and choose the parameter
λ from {10, 100, 1000}. Denote the REINFORCE-based trajectory EQUMRL by EQUMRL, and
choose the parameter ζ from {0.5, 0.75, 1.5}. To train the policies of MVRL methods, we assume the
stationarity on time-series. Given historical datasets (ya,t)t∈{1,...,T },a∈{1,...,m}, for each trajectory,
we simulate portfolio management and generate {(Sk
t , Ak
t , r(Sk
t , Ak
t ))}n
t=1. Recall that state is
given as St = ((ya,t−1, ..., ya,t−12, wa,t−1)a∈A, Pt−1
s=0 ys), and the portfolio weight as given as
wa,t = π(a, st) (see Section 6.2). Then, using the trajectory observations, we train the policies. We
show the pseudo-code of the modiﬁed REINFORCE-based trajectory EQMRL in Algorithm 2."
REFERENCES,0.9449838187702265,Algorithm 2 REINFORCE-based EQUMRL in Section 6.2
REFERENCES,0.948220064724919,"Historical dataset (ya,t)t∈{1,...,T },a∈{1,...,m}
Initialize the policy parameter θ0;
for k = 1, 2, . . . do"
REFERENCES,0.9514563106796117,"Generate {(Sk
t , Ak
t , r(Sk
t , Ak
t ))}n
t=1 on πθ by simulating the portfolio management using the
historical dataset (ya,t)t∈{1,...,T },a∈{1,...,m};
Update policy parameters θk+1 ←θk + ηi b∇Eπθk

utrajectory(G; α, β)

.
end for"
REFERENCES,0.9546925566343042,"Table 4 shows the performance of each portfolio model without the penalty of the turnover Λ = 0. We
report both results with and without the penalty of the turnover in the following. The average of real
objective (minimizing variance with a penalty on return targeting) for Tamar, MVP and EQUMRL
from July 2000 to June 2020 is shown in Table 5. We also divide the performance period into two
for robustness checks. Table 6 and 7 shows the ﬁrst-half results from July 2000 to June 2010 and
the second-half results from July 2010 to June 2020. In almost all cases, the EQUMRL achieves the
highest R/R."
REFERENCES,0.9579288025889967,"We plot results of Tamar, Xie, and EQUMRL with various parameters in Figure 6.
We
choose the parameter of Tamar (V) from {10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65}, Xie
(λ) from {10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000}, and EQUMRL (ζ) from
{0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1, 1.125, 1.25, 1.375, 1.5}. We only annotate the points
of V = 10, 25, 40, 55, λ = 10, 200, 500, 800, and ζ = 0.125, 0.5, 0.875, 1.25. Unlike Section 6.1,
it is not easy to control the mean and variance owing to the difﬁculty of predicting the real-world
ﬁnancial markets. However, the EQUMRL tends to return more efﬁcient results than the other
methods."
REFERENCES,0.9611650485436893,Under review as a conference paper at ICLR 2022
REFERENCES,0.9644012944983819,"Table 4: The performance of each portfolio model without the penalty of the turnover Λ = 0 during
the out-of-sample period (from July 2000 to June 2020) for FF25 dataset (upper table) , FF48 (middle
table), and FF100 (lower table). For each dataset, the best performance is highlighted in bold."
REFERENCES,0.9676375404530745,"Method
EW
MV
EGO
BLD
Tamar
Xie
EQUM
V = 15
V = 30
V = 60
λ = 10
λ = 100
λ = 1000
ζ = 0.5
ζ = 0.75
ζ = 1.5
FF25
CR↑
0.80
0.11
0.87
0.56
1.16
1.00
0.80
0.96
1.01
0.90
1.11
1.12
1.02
Var↓
28.62
53.58
30.63
12.01
18.41
15.10
25.22
24.94
14.98
18.37
11.77
21.29
16.25
R/R↑
0.52
0.05
0.55
0.56
0.94
0.89
0.55
0.67
0.90
0.73
1.13
0.84
0.87
MaxDD↓
0.54
0.75
0.57
0.37
0.35
0.35
0.55
0.46
0.34
0.49
0.27
0.33
0.30
FF48
CR↑
0.81
0.15
1.04
0.52
0.90
0.63
1.06
0.52
0.34
0.35
0.92
1.49
1.01
Var↓
22.91
76.89
31.87
9.65
40.13
15.81
18.07
11.79
6.72
6.70
15.11
30.96
27.28
R/R↑
0.59
0.06
0.64
0.58
0.49
0.55
0.86
0.52
0.46
0.47
0.82
0.93
0.67
MaxDD↓
0.30
0.48
0.31
0.18
0.32
0.19
0.21
0.20
0.17
0.17
0.20
0.27
0.28
FF100
CR↑
0.81
0.14
0.86
0.53
1.07
1.86
1.36
1.33
1.05
1.07
1.35
1.30
1.38
Var↓
29.36
57.90
32.40
11.79
31.61
85.74
20.05
43.65
20.94
87.24
20.62
16.56
27.50
R/R↑
0.52
0.06
0.53
0.54
0.66
0.69
1.05
0.70
0.79
0.40
1.03
1.10
0.91
MaxDD↓
0.33
0.46
0.34
0.22
0.29
0.57
0.22
0.36
0.23
0.51
0.23
0.20
0.28"
REFERENCES,0.970873786407767,"Table 5: The average of real objective (minimizing variance with a penalty on return targeting) for
Tamar, MVP and EQUMRL for FF25 dataset (upper panel) , FF48 (middle panel) and FF100 (lower
panel) from July 2020 to June 2020."
REFERENCES,0.9741100323624595,"Tamar
MVP
EQUM
V = 15
V = 30
V = 60
λ = 10
λ = 100
λ = 1000
ζ = 0.5
ζ = 0.75
ζ = 1.5
With Turnover Penalty Λ = 0.001
FF25
-5,466
-7,504
-7,646
-6,227
-6,398
-7,769
-7,853
-8,758
-9,027
FF48
-5,395
-7,399
-7,614
-6,647
-7,763
-7,677
-9,000
-8,436
-7,743
FF100
-6,711
-7,491
-6,233
-6,362
-5,354
-6,983
-8,681
-7,366
-8,362
Without Turnover Penalty Λ = 0
FF25
-5,312
-7,486
-7,329
-6,130
-6,390
-7,340
-7,664
-7,660
-8,911
FF48
-5,464
-7,053
-7,505
-6,489
-9,763
-7,506
-8,729
-8,456
-7,421
FF100
-6,749
-7,287
-5,915
-6,047
-5,446
-7,008
-9,484
-7,306
-8,280"
REFERENCES,0.9773462783171522,"Table 6: The performance of each portfolio without turnover penalty (Lambda = 0) during ﬁrst half
out-of-sample period (from July 2000 to June 2010) and second half out-of-sample period (from
July 2010 to June 2020) for FF25 dataset (upper panel) , FF48 (middle panel) and FF100 (lower
panel). Among the comparisons of the various portfolios, the best performance within each dataset is
highlighted in bold."
REFERENCES,0.9805825242718447,"Tamar
Xie
EQUM
FF25
EW
MV
EGO
BLD
V = 15
V = 30
V = 60
λ = 10
λ = 100
λ = 1000
ζ = 0.5
ζ = 0.75
ζ = 1.5
First-Half Period (from July 2000 to June 2010)
CR↑
0.58
-0.42
0.64
0.41
1.35
1.31
1.18
0.96
1.32
1.15
1.54
1.34
1.22
Var↓
31.21
69.36
33.29
12.50
14.47
11.34
11.78
21.38
10.99
11.50
9.96
22.79
17.10
R/R↑
0.36
-0.17
0.38
0.41
1.23
1.35
1.19
0.72
1.38
1.18
1.69
0.97
1.02
MaxDD↓
0.54
0.75
0.58
0.37
0.21
0.16
0.19
0.32
0.15
0.19
0.10
0.33
0.21
Second-Half Period (from July 2000 to June 2010)
CR↑
1.02
0.63
1.03
0.70
0.98
0.68
0.43
0.96
0.70
0.65
0.69
0.90
0.81
Var↓
25.92
37.24
27.92
11.47
22.30
18.66
38.39
28.50
18.77
25.10
13.21
19.69
15.32
R/R↑
0.69
0.36
0.67
0.72
0.72
0.55
0.24
0.62
0.56
0.45
0.66
0.70
0.72
MaxDD↓
0.31
0.50
0.31
0.22
0.35
0.35
0.55
0.46
0.34
0.49
0.27
0.32
0.30
Tamar
Xie
EQUM
FF48
EW
MV
EGO
BLD
V = 15
V = 30
V = 60
λ = 10
λ = 100
λ = 1000
ζ = 0.5
ζ = 0.75
ζ = 1.5
First-Half Period (from July 2000 to June 2010)
CR↑
0.60
0.41
0.75
0.34
1.57
0.68
1.35
1.03
-0.25
-0.26
1.23
1.72
1.09
Var↓
25.85
57.50
39.33
10.97
41.06
14.46
13.85
7.07
0.02
0.02
11.15
27.16
18.30
R/R↑
0.41
0.19
0.41
0.36
0.85
0.62
1.26
1.34
-6.36
-6.30
1.28
1.15
0.88
MaxDD↓
0.27
0.33
0.31
0.18
0.30
0.17
0.17
0.17
0.00
0.00
0.16
0.27
0.18
Second-Half Period (from July 2000 to June 2010)
CR↑
1.02
-0.12
1.33
0.69
0.23
0.59
0.77
0.01
0.94
0.97
0.61
1.27
0.92
Var↓
19.88
96.14
24.24
8.27
38.29
17.16
22.13
16.00
12.72
12.63
18.87
34.66
36.24
R/R↑
0.79
-0.04
0.93
0.84
0.13
0.49
0.57
0.01
0.91
0.94
0.48
0.75
0.53
MaxDD↓
0.27
0.48
0.27
0.18
0.28
0.19
0.21
0.17
0.17
0.17
0.20
0.26
0.28
Tamar
MVP
EQUM
FF100
EW
MV
EGO
BLD
V = 15
V = 30
V = 60
λ = 10
λ = 100
λ = 1000
ζ = 0.5
ζ = 0.75
ζ = 1.5
First-Half Period (from July 2000 to June 2010)
CR↑
0.61
-0.37
0.73
0.41
1.08
1.88
1.67
1.42
1.29
0.73
1.43
1.50
1.93
Var↓
32.02
79.35
35.11
12.43
19.67
109.44
14.33
40.80
13.16
92.80
17.16
12.80
21.45
R/R↑
0.37
-0.15
0.43
0.40
0.84
0.62
1.52
0.77
1.23
0.26
1.20
1.45
1.44
MaxDD↓
0.28
0.46
0.30
0.17
0.22
0.52
0.18
0.30
0.15
0.51
0.20
0.17
0.22
Second-Half Period (from July 2000 to June 2010)
CR↑
1.01
0.66
1.00
0.66
1.06
1.83
1.06
1.24
0.80
1.41
1.27
1.09
0.83
Var↓
26.61
35.92
29.64
11.12
43.56
62.05
25.58
46.48
28.61
81.45
24.07
20.23
32.94
R/R↑
0.68
0.38
0.63
0.68
0.56
0.81
0.73
0.63
0.52
0.54
0.90
0.84
0.50
MaxDD↓
0.32
0.33
0.32
0.22
0.27
0.34
0.22
0.33
0.23
0.45
0.22
0.19
0.27"
REFERENCES,0.9838187702265372,Under review as a conference paper at ICLR 2022
REFERENCES,0.9870550161812298,"Table 7: The performance of each portfolio with turnover penalty (Λ = 0.001) during ﬁrst half
out-of-sample period (from July 2000 to June 2010) and second half out-of-sample period (from
July 2010 to June 2020) for FF25 dataset (upper panel) , FF48 (middle panel) and FF100 (lower
panel). Among the comparisons of the various portfolios, the best performance within each dataset is
highlighted in bold."
REFERENCES,0.9902912621359223,"Tamar
Xie
EQUM
FF25
EW
MV
EGO
BLD
V = 2.5
V = 15
V = 30
λ = 60
λ = 100
λ = 1000
ζ = 0.5
ζ = 0.75
ζ = 1.5
First-Half Period(from July 2000 to June 2010)
CR↑
0.58
-0.43
0.64
0.56
1.32
1.31
1.19
0.93
1.17
1.17
1.58
1.55
1.54
Var↓
31.21
69.33
33.29
25.08
14.46
11.18
11.81
11.95
11.93
11.93
16.47
11.18
9.96
R/R↑
0.36
-0.18
0.38
0.39
1.21
1.36
1.20
0.93
1.17
1.17
1.35
1.61
1.69
MaxDD↓
0.54
0.75
0.58
0.52
0.21
0.15
0.19
0.20
0.17
0.17
0.26
0.13
0.10
Second-Half Period(from July 2000 to June 2010)
CR↑
1.02
0.60
0.99
1.12
0.97
0.70
0.41
0.87
1.13
1.13
1.48
0.95
0.69
Var↓
25.92
37.28
27.95
19.09
22.54
18.72
38.46
39.97
39.61
39.61
32.08
18.67
13.21
R/R↑
0.69
0.34
0.65
0.89
0.71
0.56
0.23
0.48
0.62
0.62
0.90
0.76
0.66
MaxDD↓
0.31
0.52
0.32
0.25
0.35
0.35
0.56
0.54
0.51
0.51
0.36
0.31
0.27
Tamar
Xie
EQUM
FF48
EW
MV
EGO
BLD
V = 2.5
V = 15
V = 30
λ = 60
λ = 100
λ = 1000
ζ = 0.5
ζ = 0.75
ζ = 1.5
First-Half Period(from July 2000 to June 2010)
CR↑
0.60
0.36
0.70
0.49
1.36
0.41
1.11
0.58
1.21
0.82
1.35
1.18
1.36
Var↓
25.85
57.76
39.39
18.69
41.27
14.49
13.70
18.51
17.61
19.28
29.97
20.19
6.04
R/R↑
0.41
0.16
0.38
0.39
0.73
0.37
1.04
0.47
1.00
0.64
0.85
0.91
1.92
MaxDD↓
0.27
0.33
0.31
0.25
0.30
0.18
0.17
0.18
0.20
0.19
0.25
0.17
0.10
Second-Half Period(from July 2000 to June 2010)
CR↑
1.02
-0.14
1.25
1.02
-0.00
0.36
0.52
0.43
0.94
1.20
1.86
0.92
0.45
Var↓
19.88
96.16
24.28
13.14
39.17
17.52
22.21
35.72
29.90
33.27
33.85
17.15
14.22
R/R↑
0.79
-0.05
0.88
0.98
-0.00
0.29
0.38
0.25
0.59
0.72
1.11
0.77
0.41
MaxDD↓
0.27
0.48
0.28
0.22
0.28
0.20
0.21
0.27
0.25
0.26
0.27
0.20
0.16
Tamar
MVP
EQUM
FF100
EW
MV
EGO
BLD
V = 2.5
V = 15
V = 30
λ = 60
λ = 100
λ = 1000
ζ = 0.5
ζ = 0.75
ζ = 1.5
First-Half Period(from July 2000 to June 2010)
CR↑
0.61
-0.41
0.67
0.59
1.07
0.98
1.23
0.24
1.26
1.39
1.19
1.17
1.22
Var↓
32.02
79.44
35.06
24.25
14.20
13.92
11.39
93.47
16.00
28.49
13.42
9.16
17.63
R/R↑
0.37
-0.16
0.39
0.41
0.99
0.91
1.26
0.09
1.09
0.90
1.12
1.34
1.01
MaxDD↓
0.28
0.46
0.30
0.25
0.17
0.18
0.18
0.51
0.21
0.21
0.17
0.15
0.22
Second-Half Period(from July 2000 to June 2010)
CR↑
1.01
0.63
0.95
1.12
1.18
0.60
0.11
0.90
0.70
1.09
0.70
0.72
1.64
Var↓
26.61
35.96
29.61
19.27
36.80
26.69
20.99
82.15
41.40
53.59
17.46
19.26
48.45
R/R↑
0.68
0.37
0.60
0.88
0.67
0.40
0.08
0.34
0.38
0.52
0.58
0.57
0.82
MaxDD↓
0.32
0.33
0.32
0.26
0.25
0.23
0.22
0.45
0.32
0.31
0.19
0.26
0.31"
REFERENCES,0.9935275080906149,"Figure 6: MV efﬁciency of the portfolio management experiment. Higher CRs and lower Vars
methods are MV Pareto efﬁcient. Left graph: FF25. Center graph: FF48. Right graph: FF100."
REFERENCES,0.9967637540453075,"Figure 7: MV efﬁciency of the portfolio management experiment. Higher CRs and lower Vars
methods are MV Pareto efﬁcient. Left graph: FF25. Center graph: FF48. Right graph: FF100."
