Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00036563071297989033,"We characterize the power-law asymptotics of learning curves for Gaussian process
regression (GPR) under the assumption that the eigenspectrum of the prior and
the eigenexpansion coefﬁcients of the target function follow a power law. Under
similar assumptions, we leverage the equivalence between GPR and kernel ridge
regression (KRR) to show the generalization error of KRR. Inﬁnitely wide neural
networks can be related to GPR with respect to the neural network GP kernel and the
neural tangent kernel, which in several cases is known to have a power-law spectrum.
Hence our methods can be applied to study the generalization error of inﬁnitely
wide neural networks. We present toy experiments demonstrating the theory."
INTRODUCTION,0.0007312614259597807,"1
INTRODUCTION"
INTRODUCTION,0.0010968921389396709,"Gaussian processes (GPs) provide a ﬂexible and interpretable framework for learning and adaptive
inference, and are widely used for constructing prior distributions in non-parametric Bayesian learning.
From an application perspective, one crucial question is how fast do GPs learn, i.e., how much training
data is needed to achieve a certain level of generalization performance. Theoretically, this is addressed
by analyzing so-called “learning curves”, which describe the generalization error as a function of
the training set size n. The rate at which the curve approaches zero determines the difﬁculty of
learning tasks and conveys important information about the asymptotic performance of GP learning
algorithms. In this paper, we study the learning curves for Gaussian process regression. Our main
result characterizes the asymptotics of the generalization error in cases where the eigenvalues of the
GP kernel and the coefﬁcients of the eigenexpansion of the target function have a power-law decay. In
the remainder of this introductory section, we review related work and outline our main contributions."
INTRODUCTION,0.0014625228519195613,"Gaussian processes
A GP model is a probabilistic model on an inﬁnite-dimensional parameter space
(Williams and Rasmussen, 2006; Orbanz and Teh, 2010). In GP regression (GPR), for example, this
space can be the set of all continuous functions. Assumptions about the learning problem are encoded
by way of a prior distribution over functions, which gets transformed into a posterior distribution given
some observed data. The mean of the posterior is then used for prediction. The model uses only a ﬁnite
subset of the available parameters to explain the data and this subset can grow arbitrarily large as more
data are observed. In this sense, GPs are “non-parametric” and contrast with parametric models, where
there is a ﬁxed number of parameters. For regression with Gaussian noise, a major appeal of the GP
formalism is that the posterior is analytically tractable. GPs are also one important part in learning with
kernel machines (Kanagawa et al., 2018) and modeling using GPs has recently gained considerable
traction in the neural network community."
INTRODUCTION,0.0018281535648994515,"Neural networks and kernel learning
From a GP viewpoint, there exists a well known correspon-
dence between kernel methods and inﬁnite neural networks (NNs) ﬁrst studied by Neal (1996). Neal
showed that the outputs of a randomly initialized one-hidden layer neural network (with appropriate
scaling of the variance of the initialization distribution) converges to a GP over functions in the limit
of an inﬁnite number of hidden units. Follow-up work extended this correspondence with analytical
expressions for the kernel covariance for shallow NNs by Williams (1997), and more recently for
deep fully-connected NNs (Lee et al., 2018; de G. Matthews et al., 2018), convolutional NNs with
many channels (Novak et al., 2019; Garriga-Alonso et al., 2019), and more general architectures
(Yang, 2019). The correspondence enables exact Bayesian inference in the associated GP model for"
INTRODUCTION,0.0021937842778793418,Published as a conference paper at ICLR 2022
INTRODUCTION,0.002559414990859232,"inﬁnite-width NNs on regression tasks and has led to some recent breakthroughs in our understanding
of overparameterized NNs (Jacot et al., 2018; Lee et al., 2019; Arora et al., 2019; Belkin et al., 2018;
Daniely et al., 2016; Yang and Salman, 2019; Bietti and Mairal, 2019). The most prominent kernels
associated with inﬁnite-width NNs are the Neural Network Gaussian Process (NNGP) kernel (Lee
et al., 2018; de G. Matthews et al., 2018), and the Neural Tangent Kernel (NTK) (Jacot et al., 2018).
Empirical studies have shown that inference with such inﬁnite network kernels is competitive with
standard gradient descent-based optimization for fully-connected architectures (Lee et al., 2020)."
INTRODUCTION,0.0029250457038391227,"Learning curves
A large-scale empirical characterization of the generalization performance of
state-of-the-art deep NNs showed that the associated learning curves often follow a power law of the
form n−β with the exponent β ranging between 0.07 and 0.35 depending on the data and the algorithm
(Hestness et al., 2017; Spigler et al., 2020). Power-law asymptotics of learning curves have been
theoretically studied in early works for the Gibbs learning algorithm (Amari et al., 1992; Amari and
Murata, 1993; Haussler et al., 1996) that showed a generalization error scaling with exponent β =0.5,
1 or 2 under certain assumptions. More recent results from statistical learning theory characterize
the shape of learning curves depending on the properties of the hypothesis class (Bousquet et al.,
2021). In the context of GPs, approximations and bounds on learning curves have been investigated
in several works (Sollich, 1999; Sollich and Halees, 2002; Sollich, 2001; Opper and Vivarelli, 1999;
Opper and Malzahn, 2002; Williams and Vivarelli, 2000; Malzahn and Opper, 2001a;b; Seeger et al.,
2008; Van Der Vaart and Van Zanten, 2011; Le Gratiet and Garnier, 2015), with recent extensions
to kernel regression from a spectral bias perspective (Bordelon et al., 2020; Canatar et al., 2021). For a
review on learning curves in relation to its shape and monotonicity, see Loog et al. (2019); Viering et al.
(2019); Viering and Loog (2021). A related but complementary line of work studies the convergence
rates and posterior consistency properties of Bayesian non-parametric models (Barron, 1998; Seeger
et al., 2008; Van Der Vaart and Van Zanten, 2011)."
INTRODUCTION,0.0032906764168190127,"Power-law decay of the GP kernel eigenspectrum
The rate of decay of the eigenvalues of the
GP kernel conveys important information about its smoothness. Intuitively, if a process is “rough”
with more power at high frequencies, then the eigenspectrum decays more slowly. On the other hand,
kernels that deﬁne smooth processes have a fast-decaying eigenspectrum (Stein, 2012; Williams and
Rasmussen, 2006). The precise eigenvalues (λp)p≥1 of the operators associated to many kernels and
input distributions are not known explicitly, except for a few special cases (Williams and Rasmussen,
2006). Often, however, the asymptotic properties are known. The asymptotic rate of decay of the
eigenvalues of stationary kernels for input distributions with bounded support is well understood
(Widom, 1963; Ritter et al., 1995). Ronen et al. (2019) showed that for inputs distributed uniformly on
a hypersphere, the eigenfunctions of the arc-cosine kernel are spherical harmonics and the eigenvalues
follow a power-law decay. The spectral properties of the NTK are integral to the analysis of training
convergence and generalization of NNs, and several recent works empirically justify and rely on a
power law assumption for the NTK spectrum (Bahri et al., 2021; Canatar et al., 2021; Lee et al., 2020;
Nitanda and Suzuki, 2021). Velikanov and Yarotsky (2021) showed that the asymptotics of the NTK
of inﬁnitely wide shallow ReLU networks follows a power-law that is determined primarily by the
singularities of the kernel and has the form λp ∝p−α with α=1+ 1"
INTRODUCTION,0.003656307129798903,"d, where d is the input dimension."
INTRODUCTION,0.0040219378427787935,"Asymptotics of the generalization error of kernel ridge regression (KRR)
There is a well known
equivalence between GPR and KRR with the additive noise in GPR playing the role of regularization
in KRR (Kanagawa et al., 2018). Analysis of the decay rates of the excess generalization error of
KRR has appeared in several works, e.g, in the noiseless case with constant regularization (Bordelon
et al., 2020; Spigler et al., 2020; Jun et al., 2019), and the noisy optimally regularized case (Caponnetto
and De Vito, 2007; Steinwart et al., 2009; Fischer and Steinwart, 2020) under the assumption that
the kernel eigenspectrum, and the eigenexpansion coefﬁcients of the target function follow a power
law. These assumptions, which are often called resp. the capacity and source conditions are related
to the effective dimension of the problem and the difﬁculty of learning the target function (Caponnetto
and De Vito, 2007; Blanchard and Mücke, 2018). Cui et al. (2021) present a unifying picture of the
excess error decay rates under the capacity and source conditions in terms of the interplay between
noise and regularization illustrating their results with real datasets."
INTRODUCTION,0.0043875685557586835,"Contributions
In this work, we characterize the asymptotics of the generalization error of GPR
and KRR under the capacity and source conditions. Our main contributions are as follows:"
INTRODUCTION,0.004753199268738574,Published as a conference paper at ICLR 2022
INTRODUCTION,0.005118829981718464,"• When the eigenspectrum of the prior decays with rate α and the eigenexpansion coefﬁcients of
the target function decay with rate β, we show that with high probability over the draw of n input
samples, the negative log-marginal likelihood behaves as Θ(nmax{ 1"
INTRODUCTION,0.005484460694698354,"α , 1−2β"
INTRODUCTION,0.005850091407678245,"α
+1}) (Theorem 7) and
the generalization error behaves as Θ(nmax{ 1"
INTRODUCTION,0.006215722120658135,"α −1, 1−2β"
INTRODUCTION,0.006581352833638025,"α
}) (Theorem 9). In the special case that the
model is correctly speciﬁed, i.e., the GP prior is the true one from which the target functions are
actually generated, our result implies that the generalization error behaves as O(n
1
α −1) recovering
as a special case a result due to Sollich and Halees (2002) (vide Remark 10).
• Under similar assumptions as in the previous item, we leverage the equivalence between GPR
and KRR to show that the excess generalization error of KRR behaves as Θ(nmax{ 1"
INTRODUCTION,0.006946983546617916,"α −1, 1−2β"
INTRODUCTION,0.007312614259597806,"α
})
(Theorem 12). In the noiseless case with constant regularization, our result implies that the
generalization error behaves as Θ(n
1−2β"
INTRODUCTION,0.007678244972577696,"α ) recovering as a special case a result due to Bordelon et al.
(2020). Specializing to the case of KRR with Gaussian design, we recover as a special case a result
due to Cui et al. (2021) (vide Remark 14).
For the unrealizable case, i.e., when the target function is outside the span of the eigenfunctions
with positive eigenvalues, we show that the generalization error converges to a constant.
• We present a few toy experiments demonstrating the theory for GPR with arc-cosine kernel without bi-
ases(resp. withbiases)whichistheconjugatekernelofaninﬁnitelywideshallownetworkwithtwoin-
puts and one hidden layer without biases (resp. with biases) (Cho and Saul, 2009; Ronen et al., 2019)."
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.008043875685557587,"2
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.008409506398537478,"In GP regression, our goal is to learn a target function f : Ω7→R between an input x ∈Ωand
output y ∈R based on training samples Dn = {(xi, yi)}n
i=1.
We consider an additive noise
model yi = f(xi) + ϵi, where ϵi
i.i.d.
∼N(0,σ2
true). If ρ denotes the marginal density of the inputs
xi, then the pairs (xi, yi) are generated according to the density q(x, y) = ρ(x)q(y|x), where
q(y|x)=N(y|f(x),σ2
true). We assume that there is a prior distribution Π0 on f which is deﬁned as a
zero-mean GP with continuous and bounded covariance function k:Ω×Ω→R, i.e., f ∼GP(0,k). This
means that for any ﬁnite set x=(x1,...,xn)T , the random vector f(x)=(f(x1),...,f(xn))T follows
the multivariate normal distribution N(0,Kn) with covariance matrix Kn =(k(xi,xj))n
i,j=1 ∈Rn×n.
By Bayes’ rule, the posterior distribution over f given the training data is given by"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.008775137111517367,"dΠn(f|Dn)=
1
Z(Dn) n
Y"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.009140767824497258,"i=1
N(yi|f(xi),σ2
model)dΠ0(f),"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.009506398537477149,"where Π0 is the prior distribution, Z(Dn) =
R Qn
i=1 N(yi|f(xi),σ2
model)dΠ0(f) is the marginal
likelihood or model evidence and σmodel is the sample variance used in GPR. In practice, we do not
know the exact value of σtrue and so our choice of σmodel can be different from σtrue. The GP prior
and the Gaussian noise assumption allows for exact Bayesian inference and the posterior distribution
over functions is again a GP with mean and covariance function given by"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.009872029250457038,"¯m(x)=KT
xx(Kn+σ2
modelIn)−1y,x∈Ω
(1)
¯k(x,x′)=k(x,x′)−KT
xx(Kn+σ2
modelIn)−1Kxx′,x,x′ ∈Ω,
(2)"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.010237659963436929,"where Kxx =(k(x1,x),...,k(xn,x))T and y=(y1,...,yn)T ∈Rn (Williams and Rasmussen, 2006, Eqs.
2.23-24)."
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.01060329067641682,"The performance of GPR depends on how well the posterior approximates f as the number of training
samples n tends to inﬁnity. The distance of the posterior to the ground truth can be measured in various
ways. We consider two such measures, namely the Bayesian generalization error (Seeger et al., 2008;
Haussler and Opper, 1997; Opper and Vivarelli, 1999) and the excess mean squared error (Sollich
and Halees, 2002; Le Gratiet and Garnier, 2015; Bordelon et al., 2020; Cui et al., 2021).
Deﬁnition 1 (Bayesian generalization error). The Bayesian generalization error is deﬁned as the
Kullback-Leibler divergence between the true density q(y|x) and the Bayesian predictive density
pn(y|x,Dn)=
R
N(y|f(x),σ2
model)dΠn(f|Dn),"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.010968921389396709,"G(Dn)=
Z
q(x,y)log
q(y|x)
pn(y|x,Dn)dxdy.
(3)"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.0113345521023766,Published as a conference paper at ICLR 2022
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.01170018281535649,"A related quantity of interest is the stochastic complexity (SC), also known as the free energy, which
is just the negative log-marginal likelihood. We shall primarily be concerned with a normalized version
of the stochastic complexity which is deﬁned as follows:"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.01206581352833638,"F 0(Dn)=−log
Z(Dn)
Qn
i=1q(yi|xi) =−log"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.01243144424131627,"R Qn
i=1N(yi|f(xi),σ2
model)dΠ0(f)
Qn
i=1q(yi|xi)
.
(4)"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.012797074954296161,"The generalization error (3) can be expressed in terms of the normalized SC as follows (Watanabe,
2009, Theorem 1.2):
G(Dn)=E(xn+1,yn+1)F 0(Dn+1)−F 0(Dn),
(5)
where Dn+1 =Dn∪{(xn+1,yn+1)} is obtained by augmenting Dn with a test point (xn+1,yn+1)."
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.01316270566727605,"If we only wish to measure the performance of the mean of the Bayesian posterior, then we can use
the excess mean squared error:
Deﬁnition 2 (Excess mean squared error). The excess mean squared error is deﬁned as"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.013528336380255941,"M(Dn)=E(xn+1,yn+1)( ¯m(xn+1)−yn+1)2−σ2
true =Exn+1( ¯m(xn+1)−f(xn+1))2.
(6)"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.013893967093235832,"Proposition 3 (Normalized stochastic complexity for GPR). Assume that σ2
model =σ2
true =σ2. The
normalized SC F 0(Dn) (4) for GPR with prior GP(0,k) is given as"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.014259597806215722,F 0(Dn)= 1
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.014625228519195612,2logdet(In+ Kn
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.014990859232175503,"σ2 )+
1
2σ2 yT (In+ Kn"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.015356489945155392,"σ2 )−1y−
1
2σ2 (y−f(x))T (y−f(x)),
(7)"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.015722120658135285,"where ϵ=(ϵ1,...,ϵn)T . The expectation of the normalized SC w.r.t. the noise ϵ is given as"
BAYESIAN LEARNING AND GENERALIZATION ERROR FOR GPS,0.016087751371115174,EϵF 0(Dn)= 1
LOGDET,0.016453382084095063,"2logdet
 
In+ Kn"
LOGDET,0.016819012797074956,"σ2

−1"
TR,0.017184643510054845,"2Tr

In−
 
In+ Kn"
TR,0.017550274223034734,"σ2
−1
+
1
2σ2 f(x)T  
In+ Kn"
TR,0.017915904936014627,"σ2
−1f(x).
(8)"
TR,0.018281535648994516,"This is a basic result and has applications in relation to model selection in GPR (Williams and
Rasmussen, 2006). For completeness, we give a proof of Proposition 3 in Appendix B. Seeger et al.
(2008, Theorem 1) gave an upper bound on the normalized stochastic complexity for the case when
f lies in the reproducing kernel Hilbert space (RKHS) of the GP prior. It is well known, however, that
sample paths of GP almost surely fall outside the corresponding RKHS (Van Der Vaart and Van Zanten,
2011) limiting the applicability of the result."
TR,0.018647166361974405,"We next derive the asymptotics of EϵF 0(Dn), the expected generalization error EϵG(Dn) =
EϵE(xn+1,yn+1)F 0(Dn+1)−EϵF 0(Dn), and the excess mean squared error EϵM(Dn)."
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.019012797074954298,"3
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.019378427787934187,"We begin by introducing some notations and assumptions. We assume that f ∈L2(Ω,ρ). By the
generalization of Mercer’s theorem (Steinwart and Scovel, 2012, Corollary 3.2), the covariance
function of the GP prior can be decomposed as k(x1, x2) = P∞
p=1 λpφp(x1)φp(x2) ρ-almost
surely, where (φp(x))p≥1 are the eigenfunctions of the operator Lk : L2(Ω, ρ) 7→L2(Ω, ρ);
(Lkf)(x)=
R"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.019744058500914076,"Ωk(x,s)f(s)dρ(s), and (λp)p≥1 are the corresponding positive eigenvalues. We index
the sequence of eigenvalues in decreasing order, that is λ1 ≥λ2 ≥···>0. The target function f(x) is
decomposed into the orthonormal set (φp(x))p≥1 and its orthogonal complement {φp(x):p≥1}⊥as f(x)= ∞
X"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.02010968921389397,"p=1
µpφp(x)+µ0φ0(x)∈L2(Ω,ρ),
(9)"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.020475319926873858,"where µ=(µ0,µ1,...,µp,...)T are the coefﬁcients of the decomposition, and φ0(x) satisﬁes ∥φ0(x)∥2 =
1 and φ0(x) ∈{φp(x) : p ≥1}⊥. For given sample inputs x, let φp(x) = (φp(x1),...,φp(xn))T ,
Φ = (φ0(x),φ1(x),...,φp(x),...) and Λ = diag{0,λ1,...,λp,...}. Then the covariance matrix Kn can
be written as Kn =ΦΛΦT , and the function values on the sample inputs can be written as f(x)=Φµ."
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.020840950639853747,"We shall make the following assumptions in order to derive the power-law asymptotics of the
normalized stochastic complexity and the generalization error of GPR:
Assumption 4 (Power law decay of eigenvalues). The eigenvalues (λp)p≥1 follow the power law"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.02120658135283364,"Cλp−α ≤λp ≤Cλp−α, ∀p≥1
(10)"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.02157221206581353,"where Cλ, Cλ and α are three positive constants which satisfy 0<Cλ ≤Cλ and α>1."
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.021937842778793418,Published as a conference paper at ICLR 2022
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.02230347349177331,"As mentioned in the introduction, this assumption, called the capacity condition, is fairly standard
in kernel learning and is adopted in many recent works (Bordelon et al., 2020; Canatar et al., 2021;
Jun et al., 2019; Bietti et al., 2021; Cui et al., 2021). Velikanov and Yarotsky (2021) derived the exact
value of the exponent α when the kernel function has a homogeneous singularity on its diagonal, which
is the case for instance for the arc-cosine kernel.
Assumption 5 (Power law decay of coefﬁcients of decomposition). Let Cµ,Cµ >0 and β >1/2 be
positive constants and let {pi}i≥1 be an increasing integer sequence such that supi≥1(pi+1−pi)<∞.
The coefﬁcients (µp)p≥1 of the decomposition (9) of the target function follow the power law"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.0226691042047532,"|µp|≤Cµp−β, ∀p≥1
and
|µpi|≥Cµpi
−β, ∀i≥1.
(11)"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.02303473491773309,"Since f ∈L2(Ω,ρ), we have P∞
p=0µ2
p < ∞. The condition β > 1/2 in Assumption 5 ensures that
the sum P∞
p=0µ2
p does not diverge. When the orthonormal basis (φp(x))p is the Fourier basis or the
spherical harmonics basis, the coefﬁcients (µp)p decay at least as fast as a power law so long as the
target function f(x) satisﬁes certain smoothness conditions (Bietti and Mairal, 2019). Velikanov
and Yarotsky (2021) gave examples of some natural classes of functions for which Assumption 5 is
satisﬁed, such as functions that have a bounded support with smooth boundary and are smooth on
the interior of this support, and derived the corresponding exponents β.
Assumption 6 (Boundedness of eigenfunctions). The eigenfunctions (φp(x))p≥0 satisfy
∥φ0∥∞≤Cφ
and
∥φp∥∞≤Cφpτ, p≥1,
(12)"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.02340036563071298,where Cφ and τ are two positive constants which satisfy τ < α−1 2 .
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.02376599634369287,"The second condition in (12) appears, for example, in Valdivia (2018, Hypothesis H1) and is less
restrictive than the assumption of uniformly bounded eigenfunctions that has appeared in several other
works in the GP literature, see, e.g., Braun (2006); Chatterji et al. (2019); Vakili et al. (2021). Deﬁne"
ASYMPTOTIC ANALYSIS OF GP REGRESSION WITH POWER-LAW PRIORS,0.02413162705667276,T1(Dn)= 1
LOGDET,0.024497257769652652,"2logdet

In+ ΦΛΦT"
LOGDET,0.02486288848263254,"σ2

−1"
TR,0.02522851919561243,"2Tr

In−

In+ ΦΛΦT"
TR,0.025594149908592323,"σ2
−1
,
(13)"
TR,0.025959780621572212,"T2(Dn)=
1
2σ2 f(x)T 
In+ ΦΛΦT"
TR,0.0263254113345521,"σ2
−1
f(x),
(14)"
TR,0.026691042047531994,"G1(Dn)=E(xn+1,yn+1)(T1(Dn+1)−T1(Dn)),
(15)"
TR,0.027056672760511883,"G2(Dn)=E(xn+1,yn+1)(T2(Dn+1)−T2(Dn)).
(16)"
TR,0.027422303473491772,"Using (8) and (5), we have EϵF 0(Dn)=T1(Dn)+T2(Dn) and EϵG(Dn)=G1(Dn)+G2(Dn). Intu-
itively, G1 corresponds to the effect of the noise on the generalization error irrespective of the target func-
tion f, whereas G2 corresponds to the ability of the model to ﬁt the target function. As we will see next in
Theorems 9 and 11, if α is large, then the error associated with the noise is smaller. When f is contained
in the span of the eigenfunctions {φp}p≥1, G2 decreases with increasing n, but if f contains an orthogo-
nal component, then the error remains constant and GP regression is not able to learn the target function."
ASYMPTOTICS OF THE NORMALIZED STOCHASTIC COMPLEXITY,0.027787934186471665,"3.1
ASYMPTOTICS OF THE NORMALIZED STOCHASTIC COMPLEXITY"
ASYMPTOTICS OF THE NORMALIZED STOCHASTIC COMPLEXITY,0.028153564899451554,"We derive the asymptotics of the normalized SC (8) for the following two cases: µ0 =0 and µ0 >0.
When µ0 =0, the target function f(x) lies in the span of all eigenfunctions with positive eigenvalues.
Theorem 7 (Asymptotics of the normalized SC, µ0
=
0). Assume that µ0
=
0 and
σ2
model = σ2
true = σ2 = Θ(1).
Under Assumptions 4, 5 and 6, with probability of at least
1 −n−q over sample inputs (xi)n
i=1, where 0 ≤q < min{ (2β−1)(α−1−2τ)"
ASYMPTOTICS OF THE NORMALIZED STOCHASTIC COMPLEXITY,0.028519195612431443,"4α2
, α−1−2τ"
ASYMPTOTICS OF THE NORMALIZED STOCHASTIC COMPLEXITY,0.028884826325411336,"2α
}, the expected
normalized SC (8) has the asymptotic behavior:
EϵF 0(Dn)=
 1"
ASYMPTOTICS OF THE NORMALIZED STOCHASTIC COMPLEXITY,0.029250457038391225,2logdet(I+ n
ASYMPTOTICS OF THE NORMALIZED STOCHASTIC COMPLEXITY,0.029616087751371114,σ2 Λ)−1
TR,0.029981718464351007,"2Tr
 
I−(I+ n"
TR,0.030347349177330896,"σ2 Λ)−1
+
n
2σ2 µT (I+ n"
TR,0.030712979890310785,"σ2 Λ)−1µ

(1+o(1))"
TR,0.031078610603290677,=Θ(nmax{ 1
TR,0.03144424131627057,"α , 1−2β"
TR,0.03180987202925046,"α
+1}).
(17)"
TR,0.03217550274223035,"The complete proof of Theorem 7 is given in Appendix D.1. We give a sketch of the proof below. In
the sequel, we use the notations O and Θ to denote the standard mathematical orders and the notation
˜O to suppress logarithmic factors."
TR,0.03254113345521024,Published as a conference paper at ICLR 2022
TR,0.03290676416819013,"Proof sketch of Theorem 7. By (8), (13) and (14) we have EϵF 0(Dn) = T1(Dn) + T2(Dn). In
order to analyze the terms T1(Dn) and T2(Dn), we will consider truncated versions of these
quantities and bound the corresponding residual errors. Given a truncation parameter R ∈N, let
ΦR = (φ0(x),φ1(x),...,φR(x)) ∈Rn×R be the truncated matrix of eigenfunctions evaluated at the
data points, ΛR = diag(0,λ1,...,λR) ∈R(R+1)×(R+1) and µR = (µ0,µ1,...,µR) ∈RR+1. We deﬁne
the truncated version of T1(Dn) as follows:"
TR,0.033272394881170016,"T1,R(Dn)= 1"
LOGDET,0.03363802559414991,"2logdet

In+ ΦRΛRΦT
R
σ2

−1"
TR,0.0340036563071298,"2Tr

In−(In+ ΦRΛRΦT
R
σ2
)−1
.
(18)"
TR,0.03436928702010969,"Similarly, deﬁne Φ>R = (φR+1(x), φR+2(x), ... , φp(x), ...), Λ>R = diag(λR+1, ... , λp, ...),
fR(x) = PR
p=1 µpφp(x), fR(x) = (fR(x1), ... , fR(xn))T , f>R(x) = f(x) −fR(x), and
f>R(x)=(f>R(x1),...,f>R(xn))T . The truncated version of T2(Dn) is then deﬁned as"
TR,0.03473491773308958,"T2,R(Dn)=
1
2σ2 fR(x)T (In+ ΦRΛRΦT
R
σ2
)−1fR(x)T .
(19)"
TR,0.03510054844606947,The proof consists of three steps:
TR,0.03546617915904936,"• Approximation step: In this step, we show that the asymptotics of T1,R resp. T2,R dominates that of
the residuals, |T1,R(Dn)−T1(Dn)| resp. |T2,R(Dn)−T2(Dn)| (see Lemma 32). This builds upon
ﬁrst showing that ∥Φ>RΛ>RΦT
>R∥2 = ˜O(max{nR−α,n
1
2 R
1−2α"
TR,0.035831809872029254,"2
,R1−α}) (see Lemma 25) and then
choosing R=n
1
α +κ where 0<κ< α−1−2τ"
TR,0.03619744058500914,"2α2
when we have ∥Φ>RΛ>RΦT
>R∥2 =o(1). Intuitively, the
choice of the truncation parameter R is governed by the fact that λR =Θ(R−α)=n−1+κα =o(n−1)."
TR,0.03656307129798903,"• Decomposition step: In this step, we decompose T1,R into a term independent of ΦR and a series
involving ΦT
RΦR−nIR, and likewise for T2,R (see Lemma 34). This builds upon ﬁrst showing using
the Woodbury matrix identity (Williams and Rasmussen, 2006, §A.3) that"
TR,0.03692870201096892,"T1,R(Dn)= 1"
TR,0.03729433272394881,2logdet(IR+ 1
TR,0.0376599634369287,"σ2 ΛRΦT
RΦR)−1"
TR,0.038025594149908595,"2TrΦR(σ2IR+ΛRΦT
RΦR)−1ΛRΦT
R,
(20)"
TR,0.038391224862888484,"T2,R(Dn)=
1
2σ2 µT
RΦT
RΦR(σ2IR+ΛRΦT
RΦR)−1µR,
(21)"
TR,0.038756855575868374,"and then Taylor expanding the matrix inverse (σ2IR + ΛRΦT
RΦR)−1 in (20) and (21) to
show that the ΦR-independent terms in the decomposition of T1,R and T2,R are, respectively,
1
2logdet(IR+ n"
TR,0.03912248628884826,σ2 ΛR)−1
TR,0.03948811700182815,"2Tr
 
IR−(IR+ n"
TR,0.03985374771480804,"σ2 ΛR)−1
, and
n
2σ2 µT
R(IR+ n"
TR,0.04021937842778794,σ2 ΛR)−1µR.
TR,0.040585009140767826,"• Concentration step: Finally, we use concentration inequalities to show that these ΦR-independent
terms dominate the series involving ΦT
RΦR−nIR (see Lemma 35) when we have"
TR,0.040950639853747715,"T1,R(Dn)=
  1"
TR,0.041316270566727605,2logdet(IR+ n
TR,0.041681901279707494,σ2 ΛR)−1
TR,0.04204753199268738,"2Tr
 
IR−(IR+ n"
TR,0.04241316270566728,"σ2 ΛR)−1
(1+o(1))=Θ(n
1
α ),"
TR,0.04277879341864717,"T2,R(Dn)=
  n"
TR,0.04314442413162706,"2σ2 µT
R(IR+ n"
TR,0.043510054844606946,"σ2 ΛR)−1µR

(1+o(1))="
TR,0.043875685557586835,"(
Θ(nmax{0, 1−2β"
TR,0.044241316270566725,"α
+1}),
α̸=2β−1,
Θ(logn),
α=2β−1."
TR,0.04460694698354662,"The key idea is to consider the matrix Λ1/2
R (I+ n"
TR,0.04497257769652651,"σ2 ΛR)−1/2ΦT
RΦR(I+ n"
TR,0.0453382084095064,"σ2 ΛR)−1/2Λ1/2
R
and show
that it concentrates around nΛR(I + n"
TR,0.04570383912248629,"σ2 )−1 (see Corollary 22). Note that an ordinary application
of the matrix Bernstein inequality to ΦT
RΦR−nIR yields ∥ΦT
RΦR−nI∥2 =O(R√n), which is not
sufﬁcient for our purposes, since this would give O(R√n)=o(n) only when α>2. In contrast, our
results are valid for α>1 and cover cases of practical interest, e.g., the NTK of inﬁnitely wide shallow
ReLU network (Velikanov and Yarotsky, 2021) and the arc-cosine kernels over high-dimensional
hyperspheres (Ronen et al., 2019) that have α=1+O( 1"
TR,0.04606946983546618,"d), where d is the input dimension."
TR,0.046435100548446066,"For µ0 >0, we note the following result:
Theorem 8 (Asymptotics of the normalized SC, µ0
>
0).
Assume µ0
>
0 and
σ2
model = σ2
true = σ2 = Θ(1).
Under Assumptions 4, 5 and 6, with probability of at least
1−n−q over sample inputs (xi)n
i=1, where 0≤q<min{ 2β−1"
TR,0.04680073126142596,"2
,α}·min{ α−1−2τ"
TR,0.04716636197440585,"2α2
, 2β−1"
TR,0.04753199268738574,"α2 }. the expected
normalized SC (8) has the asymptotic behavior: EϵF 0(Dn)=
1
2σ2 µ2
0n+o(n)."
TR,0.04789762340036563,"The proof of Theorem 8 is given in Appendix D.1 and follows from showing that when µ0 > 0,
T2,R(Dn) =
  n"
TR,0.04826325411334552,"2σ2 µT
R(IR+ n"
TR,0.04862888482632541,"σ2 ΛR)−1µR

(1 + o(1)) =
1
2σ2 µ2
0n + o(n) (see Lemma 38), which
dominates T1(Dn) and the residual |T2,R(Dn)−T2(Dn)|."
TR,0.048994515539305304,Published as a conference paper at ICLR 2022
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.04936014625228519,"3.2
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.04972577696526508,"In this section, we derive the asymptotics of the expected generalization error EϵG(Dn) by analyzing
the asymptotics of the components G1(Dn) and G2(Dn) in resp. (15) and (16) for the following two
cases: µ0 =0 and µ0 >0. First, we consider the case µ0 =0.
Theorem 9 (Asymptotics of the Bayesian generalization error, µ0 = 0). Let Assumptions 4, 5, and
6 hold. Assume that µ0 = 0 and σ2
model = σ2
true = σ2 = Θ(nt) where 1−
α
1+2τ < t < 1. Then with"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05009140767824497,"probability of at least 1−n−q over sample inputs (xi)n
i=1 where 0 ≤q < [α−(1+2τ)(1−t)](2β−1)"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05045703839122486,"4α2
, the
expectation of the Bayesian generalization error (3) w.r.t. the noise ϵ has the asymptotic behavior:"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05082266910420475,EϵG(Dn)= 1+o(1)
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.051188299817184646,"2σ2

Tr(I+ n"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.051553930530164535,σ2 Λ)−1Λ−∥Λ1/2(I+ n
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.051919561243144424,"σ2 Λ)−1∥2
F +∥(I+ n"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05228519195612431,"σ2 Λ)−1µ∥2
2
 = 1"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.0526508226691042,σ2 Θ(nmax{ (1−α)(1−t)
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05301645338208409,"α
, (1−2β)(1−t)"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05338208409506399,"α
}).
(22)"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05374771480804388,"The proof of Theorem 9 is given in Appendix D.2. Intuitively, for a given t, the exponent (1−α)(1−t)"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.054113345521023766,"α
in
(22) captures the rate at which the model suppresses the noise, while the exponent (1−2β)(1−t)"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.054478976234003655,"α
captures
the rate at which the model learns the target function. A larger β implies that the exponent (1−2β)(1−t)"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.054844606946983544,"α
is smaller and it is easier to learn the target. A larger α implies that the exponent (1−α)(1−t)"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05521023765996344,"α
is smaller
and the error associated with the noise is smaller as well. A larger α, however, also implies that the
exponent (1−2β)(1−t)"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05557586837294333,"α
is larger (recall that α>1 and β >1/2 by Assumptions 4 and 5, resp.), which
means that it is harder to learn the target.
Remark 10.
If f
∼
GP(0, k),
then using the Karhunen-Loève expansion we have
f(x) = P∞
p=1
p"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05594149908592322,"λpωpφp(x), where (ωp)∞
p=1 are i.i.d. standard Gaussian variables.
We can"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05630712979890311,"bound ωp almost surely as |ωp| ≤Clogp, where C = supp≥1
|ωp|
logp is a ﬁnite constant. Comparing
with the expansion of f(x) in (9), we ﬁnd that µp =
p"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.056672760511883,"λpωp = O(p−α/2logp) = O(p−α/2+ε) where
ε>0 is arbitrarily small. Choosing β =α/2−ε in (22), we have EϵG(Dn)=O(n
1
α −1+ 2ε"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.057038391224862886,"α ). This rate
matches that of an earlier result due to Sollich and Halees (2002), where it is shown that the asymptotic
learning curve (as measured by the expectation of the excess mean squared error, EfM(Dn)) scales
as n
1
α −1 when the model is correctly speciﬁed, i.e., f is a sample from the same Gaussian process
GP(0,k), and the eigenvalues decay as a power law for large i, λi ∼iα."
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05740402193784278,"For µ0 >0, we note the following result:
Theorem 11 (Asymptotics of the Bayesian generalization error, µ0 >0). Let Assumptions 4, 5, and
6 hold. Assume that µ0 > 0 and σ2
model = σ2
true = σ2 = Θ(nt) where 1−
α
1+2τ < t < 1. Then with"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05776965265082267,"probability of at least 1−n−q over sample inputs (xi)n
i=1, where 0≤q < [α−(1+2τ)(1−t)](2β−1)"
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05813528336380256,"4α2
, the
expectation of the Bayesian generalization error (3) w.r.t. the noise ϵ has the asymptotic behavior:
EϵG(Dn)=
1
2σ2 µ2
0+o(1)."
ASYMPTOTICS OF THE BAYESIAN GENERALIZATION ERROR,0.05850091407678245,"In general, if µ0 > 0, then the generalization error remains constant when n →∞. This means that
if the target function contains a component in the kernel of the operator Lk, then GP regression is not
able to learn the target function. The proof of Theorem 11 is given in Appendix D.2."
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.05886654478976234,"3.3
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.05923217550274223,"In this section we derive the asymptotics of the excess mean squared error in Deﬁnition 2.
Theorem 12 (Asymptotics of excess mean squared error). Let Assumptions 4, 5, and 6 hold. Assume
σ2
model =Θ(nt) where 1−
α
1+2τ <t<1. Then with probability of at least 1−n−q over sample inputs"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.059597806215722124,"(xi)n
i=1, where 0≤q< [α−(1+2τ)(1−t)](2β−1)"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.05996343692870201,"4α2
, the excess mean squared error (6) has the asymptotic:"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.0603290676416819,"EϵM(Dn)=(1+o(1))

σ2
true
σ2
model"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.06069469835466179,"
Tr(I+
n
σ2
model Λ)−1Λ−∥Λ1/2(I+
n
σ2
model Λ)−1∥2
F
"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.06106032906764168,"+∥(I+
n
σ2
model Λ)−1µ∥2
2"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.06142595978062157,"
=Θ

max{σ2
truen
1−α−t"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.061791590493601466,"α
,n
(1−2β)(1−t) α
}
"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.062157221206581355,"when µ0 =0, and EϵM(Dn)=µ2
0+o(1), when µ0 >0."
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.06252285191956124,Published as a conference paper at ICLR 2022
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.06288848263254114,The proof of Theorem 12 uses similar techniques as Theorem 9 and is given in Appendix D.3.
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.06325411334552103,"Remark 13 (Correspondence with kernel ridge regression). The kernel ridge regression (KRR)
estimator arises as a solution to the optimization problem"
ASYMPTOTICS OF THE EXCESS MEAN SQUARED ERROR,0.06361974405850092,"ˆf =argmin
f∈Hk"
N,0.06398537477148081,"1
n n
X"
N,0.0643510054844607,"i=1
(f(xi)−yi)2+λ∥f∥2
Hk,
(23)"
N,0.06471663619744059,"where the hypothesis space Hk is chosen to be an RKHS, and λ > 0 is a regularization parameter.
The solution to (23) is unique as a function, and is given by ˆf(x) = KT
xx(Kn + nλIn)−1y, which
coincides with the posterior mean function ¯m(x) of the GPR (1) if σ2
model = nλ (Kanagawa et al.,
2018, Proposition 3.6). Thus, the additive Gaussian noise in GPR plays the role of regularization
in KRR. Leveraging this well known equivalence between GPR and KRR we observe that Theorem 12
also describes the generalization error of KRR as measured by the excess mean squared error."
N,0.06508226691042047,"Remark 14. Cui et al. (2021) derived the asymptotics of the expected excess mean-squared error for
different regularization strengths and different scales of noise. In particular, for KRR with Gaussian
design where Λ1/2
R (φ1(x),...,φR(x))) is assumed to follow a Gaussian distribution N(0,ΛR), and
regularization λ=nt−1 where 1−α≤t, Cui et al. (2021, Eq. 10) showed that"
N,0.06544789762340036,"E{xi}n
i=1EϵM(Dn)=O

max{σ2
truen
1−α−t α
,n"
N,0.06581352833638025,(1−2β)(1−t)
N,0.06617915904936014,"α
}

.
(24)"
N,0.06654478976234003,"Let δ = n−q, where 0 ≤q <
[α−(1+2τ)(1−t)](2β−1)"
N,0.06691042047531992,"4α2
.
By Markov’s inequality, this implies"
N,0.06727605118829982,"that with probability of at least 1 −δ, EϵM(Dn) = O( 1"
N,0.06764168190127971,"δ max{σ2
truen
1−α−t"
N,0.0680073126142596,"α
, n
(1−2β)(1−t)"
N,0.06837294332723949,"α
}) ="
N,0.06873857404021938,"O(nqmax{σ2
truen
1−α−t"
N,0.06910420475319927,"α
,n
(1−2β)(1−t)"
N,0.06946983546617916,"α
}). Theorem 12 improves upon this by showing that with prob-
ability of at least 1−δ, we have an optimal bound EϵM(Dn)=Θ(max{σ2
truen
1−α−t"
N,0.06983546617915905,"α
,n
(1−2β)(1−t)"
N,0.07020109689213894,"α
}).
Furthermore, in contrast to the approach by Cui et al. (2021), we have no requirement on the
distribution of φp(x), and hence our result is more generally applicable. For example, Theorem 12
can be applied to KRR with the arc-cosine kernel when the Gaussian design assumption is not valid.
In the noiseless setting (σtrue =0) with constant regularization (t=0), Theorem 12 implies that the
mean squared error behaves as Θ(n
1−2β"
N,0.07056672760511883,"α ). This recovers a result in Bordelon et al. (2020, §2.2)."
N,0.07093235831809871,"Our upper bound in Theorem 12 matches with the ones derived in (Steinwart et al., 2009; Fischer
and Steinwart, 2020). Steinwart et al. (2009) and Fischer and Steinwart (2020) also derived algorithm
independent minmax lower bounds. In contrast to their results, our Theorem 12 gives lower bounds
for different regularization strengths λ."
EXPERIMENTS,0.0712979890310786,"4
EXPERIMENTS"
EXPERIMENTS,0.07166361974405851,"We illustrate our theory on a few toy experiments. We let the input x be uniformly distributed on a
unit circle, i.e., Ω=S1 and ρ=U(S1). The points on S1 can be represented by x=(cosθ,sinθ) where
θ∈[−π,π). We use the ﬁrst order arc-cosine kernel function without bias, k(1)
w/o bias(x1,x2)= 1"
EXPERIMENTS,0.0720292504570384,"π(sinψ+
(π−ψ)cosψ), where ψ = ⟨x1,x2⟩is the angle between x1 and x2. Hence Assumption 4 is satisﬁed
with α=4. We consider the target functions in Table 1, which satisfy Assumption 5 with the indicated
β, and µ0 indicates whether the function lies in the span of eigenfunctions of the kernel. For each target
we conduct GPR 20 times and report the mean and standard deviation of the normalized SC and the
Bayesian generalization error in Figure 1, which agree with the asymptotics predicted in Theorems 7
and 9. The details of the experiments appear in Appendix A, where we also show more experiments
conﬁrming our theory for zero- and second- order arc-cosine kernels, with and without biases."
CONCLUSION,0.07239488117001829,"5
CONCLUSION"
CONCLUSION,0.07276051188299817,"We described the learning curves for GPR for the case that the kernel and target function follow a
power law. This setting is frequently encountered in kernel learning and relates to recent advances
on neural networks. Our approach is based on a tight analysis of the concentration of the inner product
of empirical eigenfunctions ΦT Φ around nI. This allowed us to obtain more general results with more"
CONCLUSION,0.07312614259597806,Published as a conference paper at ICLR 2022
CONCLUSION,0.07349177330895795,"function value
β
µ0
EϵF 0(Dn)
EϵG(Dn)
f1
cos2θ
+∞
0
Θ(n1/4)
Θ(n−3/4)
f2
θ2
2
>0
Θ(n)
Θ(1)
f3
(|θ|−π/2)2
2
0
Θ(n1/4)
Θ(n−3/4) f4"
CONCLUSION,0.07385740402193784,"(
π/2−θ,
θ∈[0,π)
−π/2−θ,
θ∈[−π,0)
1
0
Θ(n3/4)
Θ(n−1/4)"
CONCLUSION,0.07422303473491773,"Table 1: Target functions used in the experiments for the ﬁrst order arc-cosine kernel without bias
k(1)
w/o bias, their values of β and µ0, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems."
CONCLUSION,0.07458866544789762,"Figure 1: Normalized SC (top) and Bayesian generalization error (bottom) for GPR with the kernel
k(1)
w/o bias and the target functions in Table 1. The orange curves show the linear regression ﬁt for the
experimental values (in blue) of the log Bayesian generalization error as a function of log n."
CONCLUSION,0.07495429616087751,"realistic assumptions than previous works. In particular, we recovered some results on learning curves
for GPR and KRR previously obtained under more restricted settings (vide Remarks 10 and 14)."
CONCLUSION,0.0753199268738574,"We showed that when β ≥α/2, meaning that the target function has a compact representation in terms
of the eigenfunctions of the kernel, the learning rate is as good as in the correctly speciﬁed case. In
addition, our result allows us to interpret β from a spectral bias perspective. When 1"
CONCLUSION,0.07568555758683729,2 < β ≤α
CONCLUSION,0.07605118829981719,"2 , the
larger the value of β, the faster the decay of the generalization error. This implies that low-frequency
functions are learned faster in terms of the number of training data points."
CONCLUSION,0.07641681901279708,"By leveraging the equivalence between GPR and KRR, we obtained a result on the generalization
error of KRR. In the inﬁnite-width limit, training fully-connected deep NNs with gradient descent
and inﬁnitesimally small learning rate under least-squared loss is equivalent to solving KRR with
respect to the NTK (Jacot et al., 2018; Lee et al., 2019; Domingos, 2020), which in several cases is
known to have a power-law spectrum (Velikanov and Yarotsky, 2021). Hence our methods can be
applied to study the generalization error of inﬁnitely wide neural networks. In future work, it would be
interesting to estimate the values of α and β for the NTK and the NNGP kernel of deep fully-connected
or convolutional NNs and real data distributions and test our theory in these cases. Similarly, it would
be interesting to consider extensions to ﬁnite width kernels."
CONCLUSION,0.07678244972577697,ACKNOWLEDGMENT
CONCLUSION,0.07714808043875686,"This project has received funding from the European Research Council (ERC) under the EU’s Horizon
2020 research and innovation programme (grant agreement no 757983)."
CONCLUSION,0.07751371115173675,Published as a conference paper at ICLR 2022
REFERENCES,0.07787934186471664,REFERENCES
REFERENCES,0.07824497257769653,"S. Amari and N. Murata. Statistical theory of learning curves under entropic loss criterion. Neural
Computation, 5(1):140–153, 1993."
REFERENCES,0.07861060329067641,"S. Amari, N. Fujita, and S. Shinomoto. Four types of learning curves. Neural Computation, 4(4):
605–618, 1992."
REFERENCES,0.0789762340036563,"S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang. On exact computation with an
inﬁnitely wide neural net. In Advances in Neural Information Processing Systems, volume 32, pages
8139–8148, 2019."
REFERENCES,0.07934186471663619,"Y. Bahri, E. Dyer, J. Kaplan, J. Lee, and U. Sharma. Explaining neural scaling laws. arXiv preprint
arXiv:2102.06701, 2021."
REFERENCES,0.07970749542961608,"A. R. Barron. Information-theoretic characterization of Bayes performance and the choice of priors in
parametric and nonparametric problems. In D. A. Bernardo J., Berger J. and S. A., editors, Bayesian
statistics, volume 6, pages 27–52. Oxford University Press, 1998."
REFERENCES,0.08007312614259597,"M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel learning.
In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 541–549,
2018."
REFERENCES,0.08043875685557587,"A. Bietti and J. Mairal. On the inductive bias of neural tangent kernels. In Advances in Neural
Information Processing Systems, volume 32, pages 12873–12884, 2019."
REFERENCES,0.08080438756855576,"A. Bietti, L. Venturi, and J. Bruna. On the sample complexity of learning with geometric stability.
arXiv preprint arXiv:2106.07148, 2021."
REFERENCES,0.08117001828153565,"G. Blanchard and N. Mücke. Optimal rates for regularization of statistical inverse learning problems.
Foundations of Computational Mathematics, 18(4):971–1013, 2018."
REFERENCES,0.08153564899451554,"B. Bordelon, A. Canatar, and C. Pehlevan. Spectrum dependent learning curves in kernel regression
and wide neural networks. In Proceedings of the 37th International Conference on Machine
Learning (ICML), pages 1024–1034, 2020."
REFERENCES,0.08190127970749543,"O. Bousquet, S. Hanneke, S. Moran, R. van Handel, and A. Yehudayoff. A theory of universal learning.
In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC),
pages 532–541, 2021."
REFERENCES,0.08226691042047532,"M. L. Braun. Accurate error bounds for the eigenvalues of the kernel matrix. The Journal of Machine
Learning Research, 7:2303–2328, 2006."
REFERENCES,0.08263254113345521,"A. Canatar, B. Bordelon, and C. Pehlevan.
Spectral bias and task-model alignment explain
generalization in kernel regression and inﬁnitely wide neural networks. Nature communications,
12(1):1–12, 2021."
REFERENCES,0.0829981718464351,"A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Foundations
of Computational Mathematics, 7(3):331–368, 2007."
REFERENCES,0.08336380255941499,"N. Chatterji, A. Pacchiano, and P. Bartlett. Online learning with kernel losses. In Proceedings of the
36th International Conference on Machine Learning (ICML), pages 971–980, 2019."
REFERENCES,0.08372943327239488,"Y. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in Neural Information
Processing Systems, volume 22, pages 342–350, 2009."
REFERENCES,0.08409506398537477,"H. Cui, B. Loureiro, F. Krzakala, and L. Zdeborová. Generalization error rates in kernel regression:
The crossover from the noiseless to noisy regime. arXiv preprint arXiv:2105.15004, 2021."
REFERENCES,0.08446069469835467,"A. Daniely, R. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The power
of initialization and a dual view on expressivity. In Advances In Neural Information Processing
Systems, volume 29, pages 2253–2261, 2016."
REFERENCES,0.08482632541133456,Published as a conference paper at ICLR 2022
REFERENCES,0.08519195612431445,"A. G. de G. Matthews, J. Hron, M. Rowland, R. E. Turner, and Z. Ghahramani. Gaussian process
behaviour in wide deep neural networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.08555758683729434,"P. Domingos. Every model learned by gradient descent is approximately a kernel machine. arXiv
preprint arXiv:2012.00152, 2020."
REFERENCES,0.08592321755027423,"S. Fischer and I. Steinwart. Sobolev norm learning rates for regularized least-squares algorithms.
Journal of Machine Learning Research, 21:1–38, 2020."
REFERENCES,0.08628884826325411,"A. Garriga-Alonso, C. E. Rasmussen, and L. Aitchison. Deep convolutional networks as shallow
gaussian processes. In International Conference on Learning Representations, 2019."
REFERENCES,0.086654478976234,"D. Haussler and M. Opper. Mutual information, metric entropy and cumulative relative entropy risk.
The Annals of Statistics, 25(6):2451–2492, 1997."
REFERENCES,0.08702010968921389,"D. Haussler, M. Kearns, H. S. Seung, and N. Tishby. Rigorous learning curve bounds from statistical
mechanics. Machine Learning, 25(2-3):195–236, 1996."
REFERENCES,0.08738574040219378,"J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. Patwary, M. Ali, Y. Yang, and
Y. Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017."
REFERENCES,0.08775137111517367,"A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in Neural Information Processing Systems, volume 31, pages 8571–8580,
2018."
REFERENCES,0.08811700182815356,"K.-S. Jun, A. Cutkosky, and F. Orabona. Kernel truncated randomized ridge regression: Optimal rates
and low noise acceleration. Advances in Neural Information Processing Systems, 32:15358–15367,
2019."
REFERENCES,0.08848263254113345,"M. Kanagawa, P. Hennig, D. Sejdinovic, and B. K. Sriperumbudur. Gaussian processes and kernel
methods: A review on connections and equivalences. arXiv preprint arXiv:1807.02582, 2018."
REFERENCES,0.08884826325411335,"L. Le Gratiet and J. Garnier. Asymptotic analysis of the learning curve for Gaussian process regression.
Machine Learning, 98(3):407–433, 2015."
REFERENCES,0.08921389396709324,"J. Lee, J. Sohl-Dickstein, J. Pennington, R. Novak, S. Schoenholz, and Y. Bahri. Deep neural networks
as gaussian processes. In International Conference on Learning Representations, 2018."
REFERENCES,0.08957952468007313,"J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington. Wide neural
networks of any depth evolve as linear models under gradient descent. In Advances in Neural
Information Processing Systems, volume 32, pages 8572–8583, 2019."
REFERENCES,0.08994515539305302,"J. Lee, S. Schoenholz, J. Pennington, B. Adlam, L. Xiao, R. Novak, and J. Sohl-Dickstein. Finite
versus inﬁnite neural networks: an empirical study. In Advances in Neural Information Processing
Systems, volume 33, pages 15156–15172, 2020."
REFERENCES,0.09031078610603291,"M. Loog, T. Viering, and A. Mey. Minimizers of the empirical risk and risk monotonicity. In Advances
in Neural Information Processing Systems, volume 32, pages 7478–7487, 2019."
REFERENCES,0.0906764168190128,"D. Malzahn and M. Opper. Learning curves for Gaussian processes regression: A framework for
good approximations. In Advances in Neural Information Processing Systems, volume 13, pages
273–279, 2001a."
REFERENCES,0.09104204753199269,"D. Malzahn and M. Opper. Learning curves for Gaussian processes models: Fluctuations and
universality. In International Conference on Artiﬁcial Neural Networks, pages 271–276, 2001b."
REFERENCES,0.09140767824497258,"R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg, 1996.
ISBN 0387947248."
REFERENCES,0.09177330895795247,"A. Nitanda and T. Suzuki. Optimal rates for averaged stochastic gradient descent under neural tangent
kernel regime. In International Conference on Learning Representations, 2021."
REFERENCES,0.09213893967093235,Published as a conference paper at ICLR 2022
REFERENCES,0.09250457038391224,"R. Novak, L. Xiao, Y. Bahri, J. Lee, G. Yang, D. A. Abolaﬁa, J. Pennington, and J. Sohl-Dickstein.
Bayesian deep convolutional networks with many channels are gaussian processes. In International
Conference on Learning Representations, 2019."
REFERENCES,0.09287020109689213,"M. Opper and D. Malzahn. A variational approach to learning curves. In Advances in Neural
Information Processing Systems, volume 14, pages 463–469, 2002."
REFERENCES,0.09323583180987204,"M. Opper and F. Vivarelli. General bounds on Bayes errors for regression with Gaussian processes.
In Advances in Neural Information Processing Systems, volume 11, pages 302–308, 1999."
REFERENCES,0.09360146252285192,"P. Orbanz and Y. W. Teh. Bayesian nonparametric models. In Encyclopedia of Machine Learning,
pages 81–89. Springer, 2010."
REFERENCES,0.09396709323583181,"K. Ritter, G. W. Wasilkowski, and H. Wo´zniakowski. Multivariate integration and approximation
for random ﬁelds satisfying Sacks-Ylvisaker conditions. The Annals of Applied Probability, pages
518–540, 1995."
REFERENCES,0.0943327239488117,"B. Ronen, D. Jacobs, Y. Kasten, and S. Kritchman. The convergence rate of neural networks for
learned functions of different frequencies. Advances in Neural Information Processing Systems,
32:4761–4771, 2019."
REFERENCES,0.09469835466179159,"M. W. Seeger, S. M. Kakade, and D. P. Foster. Information consistency of nonparametric Gaussian
process methods. IEEE Transactions on Information Theory, 54(5):2376–2382, 2008."
REFERENCES,0.09506398537477148,"P. Sollich. Learning curves for Gaussian processes. In Advances in Neural Information Processing
Systems, volume 11, pages 344–350, 1999."
REFERENCES,0.09542961608775137,"P. Sollich. Gaussian process regression with mismatched models. In Advances in Neural Information
Processing Systems, volume 13, pages 519–526, 2001."
REFERENCES,0.09579524680073126,"P. Sollich and A. Halees. Learning curves for Gaussian process regression: Approximations and
bounds. Neural Computation, 14(6):1393–1428, 2002."
REFERENCES,0.09616087751371115,"S. Spigler, M. Geiger, and M. Wyart. Asymptotic learning curves of kernel methods: empirical data
versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and Experiment, 2020
(12):124001, 2020."
REFERENCES,0.09652650822669104,"M. L. Stein. Interpolation of spatial data: Some theory for kriging. Springer Science & Business
Media, 2012."
REFERENCES,0.09689213893967093,"I. Steinwart and C. Scovel. Mercer’s theorem on general domains: On the interaction between
measures, kernels, and rkhss. Constructive Approximation, 35:363–417, 2012."
REFERENCES,0.09725776965265082,"I. Steinwart, D. R. Hush, C. Scovel, et al. Optimal rates for regularized least squares regression. In
Conference on Learning Theory, pages 79–93, 2009."
REFERENCES,0.09762340036563072,"J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389–434, 2012."
REFERENCES,0.09798903107861061,"S. Vakili, K. Khezeli, and V. Picheny. On information gain and regret bounds in Gaussian process
bandits. In International Conference on Artiﬁcial Intelligence and Statistics, pages 82–90, 2021."
REFERENCES,0.0983546617915905,"E. A. Valdivia. Relative concentration bounds for the spectrum of kernel matrices. arXiv preprint
arXiv:1812.02108, 2018."
REFERENCES,0.09872029250457039,"A. Van Der Vaart and H. Van Zanten. Information rates of nonparametric Gaussian process methods.
Journal of Machine Learning Research, 12(6), 2011."
REFERENCES,0.09908592321755028,"M. Velikanov and D. Yarotsky. Universal scaling laws in the gradient descent training of neural
networks. arXiv preprint arXiv:2105.00507, 2021."
REFERENCES,0.09945155393053016,"T. Viering and M. Loog. The shape of learning curves: A review. arXiv preprint arXiv:2103.10948,
2021."
REFERENCES,0.09981718464351005,Published as a conference paper at ICLR 2022
REFERENCES,0.10018281535648994,"T. Viering, A. Mey, and M. Loog. Open problem: Monotonicity of learning. In Conference on
Learning Theory, pages 3198–3201, 2019."
REFERENCES,0.10054844606946983,"S. Watanabe. Algebraic Geometry and Statistical Learning Theory. Cambridge University Press, 2009."
REFERENCES,0.10091407678244972,"H. Widom. Asymptotic behavior of the eigenvalues of certain integral equations. Transactions of
the American Mathematical Society, 109(2):278–295, 1963."
REFERENCES,0.10127970749542961,"C. K. Williams. Computing with inﬁnite networks. In Advances in Neural Information Processing
Systems, volume 9, pages 295–301, 1997."
REFERENCES,0.1016453382084095,"C. K. Williams and C. E. Rasmussen. Gaussian processes for machine learning. MIT press, 2006."
REFERENCES,0.1020109689213894,"C. K. Williams and F. Vivarelli. Upper and lower bounds on the learning curve for Gaussian processes.
Machine Learning, 40(1):77–102, 2000."
REFERENCES,0.10237659963436929,"G. Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes.
In Advances in Neural Information Processing Systems, volume 32, pages 9951–9960, 2019."
REFERENCES,0.10274223034734918,"G. Yang and H. Salman. A ﬁne-grained spectral perspective on neural networks. arXiv preprint
arXiv:1907.10599, 2019."
REFERENCES,0.10310786106032907,APPENDIX
REFERENCES,0.10347349177330896,"A
EXPERIMENTS FOR ARC-COSINE KERNELS OF DIFFERENT ORDERS"
REFERENCES,0.10383912248628885,"In our experiment, the input space and input distribution are Ω= S1 and ρ = U(S1), and we
use the ﬁrst order arc-cosine kernel function. (Cho and Saul, 2009) showed that this kernel is
the conjugate kernel of an inﬁnitely wide shallow ReLU network with two inputs and no biases
in the hidden layer. GP regression with prior GP(0,k) corresponds to Bayesian training of this
network (Lee et al., 2018). Under this setting, the eigenvalues and eigenfunctions are λ1 =
4
π2 ,
λ2 =λ3 = 1"
REFERENCES,0.10420475319926874,"4, λ2p =λ2p+1 =
4
π2((2p−2)2−1)2 , p≥2 and φ1(θ)=1, φ2(θ)=
√"
REFERENCES,0.10457038391224863,"2
2 cosθ, φ3(θ)=
√"
REFERENCES,0.10493601462522852,"2
2 sinθ,"
REFERENCES,0.1053016453382084,"φ2p(θ) =
√"
REFERENCES,0.1056672760511883,"2
2 cos(2p−2)θ,φ2p+1(θ) =
√"
REFERENCES,0.10603290676416818,"2
2 sin(2p−2)θ, p ≥2. Hence Assumption 4 is satisﬁed with
α=4, and the second part of Assumption 6 is satisﬁed with ∥φp∥≤
√"
REFERENCES,0.10639853747714809,"2
2 , p≥1."
REFERENCES,0.10676416819012798,"The training and test data are generated as follows: We independently sample training inputs
x1,...,xn and test input xn+1 from U(S1) and training outputs yi, i = 1,...,n from N(f(xi),σ2),
where we choose σ = 0.1. The Bayesian predictive density conditioned on the test point xn+1
N( ¯m(xn+1),¯k(xn+1,xn+1)) is obtained by (1) and (2). We compute the normalized SC by (7) and
the Bayesian generalization error by the Kullback-Leibler divergence between N(f(xn+1),σ2) and
N( ¯m(xn+1),¯k(xn+1,xn+1))."
REFERENCES,0.10712979890310786,"Next we present experiment results for arc-cosine kernels of different orders and arc-cosine kernels
with biases. Consider the ﬁrst order arc-cosine kernel function with biases,"
REFERENCES,0.10749542961608775,"k(1)
w/ bias(x1,x2)= 1"
REFERENCES,0.10786106032906764,"π(sin ¯ψ+(π−¯ψ)cos ¯ψ), where ¯ψ=arccos
  1"
REFERENCES,0.10822669104204753,"2(⟨x1,x2⟩+1)

.
(25)"
REFERENCES,0.10859232175502742,"Ronen et al. (2019) showed that this kernel is the conjugate kernel of an inﬁnitely wide shallow ReLU
network with two inputs and one hidden layer with biases, whose eigenvalues satisfy Assumption 4
with α = 4. The eigenfunctions of this kernel are the same as that of the ﬁrst-order arc-cosine
kernel without biases, k(1)
w/o bias in Section 4. We consider the target functions in Table 3, which
satisfy Assumption 5 with the indicated β, and µ0 indicates whether the function lies in the span
of eigenfunctions of the kernel. For each target we conduct GPR 20 times and report the mean and
standard deviation of the normalized SC and the Bayesian generalization error in Figure 3, which
agree with the asymptotics predicted in Theorems 7 and 9."
REFERENCES,0.10895795246800731,"Table 2 summarizes all the different kernel functions that we consider in our experiments with pointers
to the corresponding tables and ﬁgures."
REFERENCES,0.1093235831809872,"Summarizing the observations from these experiments, we see that the smoothness of the activation
function (which is controlled by the order of the arc-cosine kernel) inﬂuences the decay rate α of the"
REFERENCES,0.10968921389396709,Published as a conference paper at ICLR 2022
REFERENCES,0.11005484460694698,"kernel function
α
activation function
bias
pointer
k(1)
w/o bias"
REFERENCES,0.11042047531992688,"1
π (sinψ+(π−ψ)cosψ)
4
max{0,x}
no
Table 1/Figure 1"
REFERENCES,0.11078610603290677,"k(1)
w/ bias"
REFERENCES,0.11115173674588666,"1
π (sin ¯ψ+(π−¯ψ)cos ¯ψ)
4
max{0,x}
yes
Table 3/Figure 3"
REFERENCES,0.11151736745886655,"k(2)
w/o bias"
REFERENCES,0.11188299817184644,"1
π (3sinψcosψ+(π−ψ)(1+2cos2ψ))
6
(max{0,x})2
no
Table 4/Figure 4"
REFERENCES,0.11224862888482633,"k(2)
w/ bias"
REFERENCES,0.11261425959780622,"1
π (3sin ¯ψcos ¯ψ+(π−¯ψ)(1+2cos2 ¯ψ))
6
(max{0,x})2
yes
Table 5/Figure 5"
REFERENCES,0.1129798903107861,"k(0)
w/o bias"
REFERENCES,0.113345521023766,"1
π (sinψ+(π−ψ)cosψ)
2
1
2(1+sign(x))
no
Table 6/Figure 6"
REFERENCES,0.11371115173674588,"k(0)
w/ bias"
REFERENCES,0.11407678244972577,"1
π (sin ¯ψ+(π−ψ)cos ¯ψ)
2
1
2(1+sign(x))
yes
Table 7/Figure 7"
REFERENCES,0.11444241316270566,"Table 2: The different kernel functions used in our experiments, their values of α, the corresponding
neural network activation function along with a pointer to the tables showing the target functions used
for the kernels and the corresponding ﬁgures."
REFERENCES,0.11480804387568556,"eigenvalues. In general, when the activation function is smoother, the decay rate α is larger. Theorem 9
then implies that smooth activation functions are more capable in suppressing noise but slower in
learning the target. We also observe that networks with biases are more capable at learning functions
compared to networks without bias. For example, the function cos(2θ) cannot be learned by the zero
order arc-cosine kernel without biases (see Table 6 and Figure 6), but it can be learned by the zero
order arc-cosine kernel with biases (see Table 7 and Figure 7)."
REFERENCES,0.11517367458866545,"function value
β
µ0
EϵF 0(Dn)
EϵG(Dn)
f1
cos2θ
+∞
0
Θ(n1/4)
Θ(n−3/4)
f2
θ2
2
0
Θ(n1/4)
Θ(n−3/4)
f3
(|θ|−π/2)2
2
0
Θ(n1/4)
Θ(n−3/4) f4"
REFERENCES,0.11553930530164534,"π/2−θ,
θ∈[0,π)
−π/2−θ,
θ∈[−π,0)
1
0
Θ(n3/4)
Θ(n−1/4)"
REFERENCES,0.11590493601462523,"Table 3: Target functions used in the experiments for the ﬁrst order arc-cosine kernel with bias, k(1)
w/ bias,
their values of β and µ0, and theoretical rates for the normalized SC and the Bayesian generalization
error from our theorems."
REFERENCES,0.11627056672760512,"Figure 3: Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
k(1)
w/ bias and the target functions in Table 3. The orange curves show the linear regression ﬁt for the
experimental values (in blue) of the log Bayesian generalization error as a function of log n."
REFERENCES,0.11663619744058501,Published as a conference paper at ICLR 2022
REFERENCES,0.1170018281535649,"function value
β
µ0
EϵF 0(Dn)
EϵG(Dn)
f1
cos2θ
+∞
0
Θ(n1/6)
Θ(n−5/6)
f2
sign(θ)
1
0
Θ(n5/6)
Θ(n−1/6)
f3
π/2−|θ|
2
0
Θ(n1/2)
Θ(n−1/2) f4"
REFERENCES,0.11736745886654479,"π/2−θ,
θ∈[0,π)
−π/2−θ,
θ∈[−π,0)
1
>0
Θ(n)
Θ(1)"
REFERENCES,0.11773308957952468,"Table 4: Target functions used in the experiments for the second order arc-cosine kernel without
bias, k(2)
w/o bias, their values of β and µ0, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems."
REFERENCES,0.11809872029250457,"Figure 4: Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
k(2)
w/o bias and the target functions in Table 4."
REFERENCES,0.11846435100548446,"function value
β
µ0
EϵF 0(Dn)
EϵG(Dn)
f1
cos2θ
+∞
0
Θ(n1/6)
Θ(n−5/6)
f2
θ2
2
0
Θ(n1/2)
Θ(n−1/2)
f3
(|θ|−π/2)2
2
0
Θ(n1/2)
Θ(n−1/2) f4"
REFERENCES,0.11882998171846434,"π/2−θ,
θ∈[0,π)
−π/2−θ,
θ∈[−π,0)
1
0
Θ(n5/6)
Θ(n−1/6)"
REFERENCES,0.11919561243144425,"Table 5: Target functions used in the experiments for the second order arc-cosine kernel with bias,
k(2)
w/ bias, their values of β and µ0, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems."
REFERENCES,0.11956124314442414,"Figure 5: Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
k(2)
w/ bias and the target functions in Table 5."
REFERENCES,0.11992687385740403,Published as a conference paper at ICLR 2022
REFERENCES,0.12029250457038392,"function value
β
µ0
EϵF 0(Dn)
EϵG(Dn)
f1
cos2θ
+∞
>0
Θ(n)
Θ(1)
f2
sign(θ)
1
0
Θ(n1/2)
Θ(n−1/2)
f3
π/2−|θ|
2
0
Θ(n1/2)
Θ(n−1/2) f4"
REFERENCES,0.1206581352833638,"π/2−θ,
θ∈[0,π)
−π/2−θ,
θ∈[−π,0)
1
>0
Θ(n)
Θ(1)"
REFERENCES,0.1210237659963437,"Table 6: Target functions used in the experiments for the zero order arc-cosine kernel without bias,
k(0)
w/o bias, their values of β and µ0, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems."
REFERENCES,0.12138939670932358,"Figure 6: Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
k(0)
w/o bias and the target functions in Table 6."
REFERENCES,0.12175502742230347,"function value
β
µ0
EϵF 0(Dn)
EϵG(Dn)
f1
cos2θ
+∞
0
Θ(n1/2)
Θ(n−1/2)
f2
θ2
2
0
Θ(n1/2)
Θ(n−1/2)
f3
(|θ|−π/2)2
2
0
Θ(n1/2)
Θ(n−1/2) f4"
REFERENCES,0.12212065813528336,"π/2−θ,
θ∈[0,π)
−π/2−θ,
θ∈[−π,0)
1
0
Θ(n1/2)
Θ(n−1/2)"
REFERENCES,0.12248628884826325,"Table 7: Target functions used in the experiments for the zero order arc-cosine kernel with bias,
k(0)
w/ bias, their values of β and µ0, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems."
REFERENCES,0.12285191956124314,"Figure 7: Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
k(0)
w/ bias and the target functions in Table 7."
REFERENCES,0.12321755027422303,Published as a conference paper at ICLR 2022
REFERENCES,0.12358318098720293,"B
PROOFS RELATED TO THE MARGINAL LIKELIHOOD"
REFERENCES,0.12394881170018282,"Proof of Proposition 3. Let ¯y = (¯y1,...,¯yn)T be the outputs of the GP regression model on training
inputs x. Under the GP prior, the prior distribution of ¯y is N(0,Kn). Then the evidence of the model
is given as follows:"
REFERENCES,0.12431444241316271,"Zn =
Z Rn n
Y i=1 1
√"
REFERENCES,0.1246800731261426,2πσ e−(¯yi−yi)2
REFERENCES,0.12504570383912247,"2σ2
!
1
(2π)n/2det(Kn)1/2 e−1"
REFERENCES,0.12541133455210238,"2 ¯yT K−1
n
¯yd¯y"
REFERENCES,0.12577696526508228,"=
1
(2π)nσndet(Kn)1/2 Z Rne−1"
REFERENCES,0.12614259597806216,"2 ¯yT (K−1
n + 1"
REFERENCES,0.12650822669104206,σ2 I)¯y+ 1
REFERENCES,0.12687385740402193,"σ2 ¯yT y−
1
2σ2 yT yd¯y. (26)"
REFERENCES,0.12723948811700184,"Letting ˜K−1
n =K−1
n + 1"
REFERENCES,0.1276051188299817,σ2 I and µ= 1
REFERENCES,0.12797074954296161,"σ2 ˜Kny, we have"
REFERENCES,0.1283363802559415,"Zn =
1
(2π)nσndet(Kn)1/2 Z Rne−1"
REFERENCES,0.1287020109689214,"2 (¯y−µ)T ˜
K−1
n (¯y−µ)−
1
2σ2 yT y+ 1"
REFERENCES,0.12906764168190127,"2 µT ˜
K−1
n µd¯y"
REFERENCES,0.12943327239488117,"=
1
(2π)nσndet(Kn)1/2 (2π)n/2det( ˜Kn)1/2e−
1
2σ2 yT y+ 1"
REFERENCES,0.12979890310786105,"2 µT ˜
K−1
n µ"
REFERENCES,0.13016453382084095,"=
det( ˜Kn)1/2"
REFERENCES,0.13053016453382085,"(2π)n/2σndet(Kn)1/2 e−
1
2σ2 yT y+ 1"
REFERENCES,0.13089579524680073,"2 µT ˜
K−1
n µ. (27)"
REFERENCES,0.13126142595978063,The normalized evidence is
REFERENCES,0.1316270566727605,"Z0
n =
Zn
(2π)−n/2σ−ne−
1
2σ2 (y−f(x))T (y−f(x))"
REFERENCES,0.1319926873857404,= det( ˜Kn)1/2
REFERENCES,0.13235831809872028,"det(Kn)1/2 e−
1
2σ2 yT y+ 1"
REFERENCES,0.1327239488117002,"2 µT ˜
K−1
n µ+
1
2σ2 (y−f(x))T (y−f(x)). (28)"
REFERENCES,0.13308957952468006,So the normalized stochastic complexity is
REFERENCES,0.13345521023765997,"F 0(Dn)=−logZ0
n =−1"
REFERENCES,0.13382084095063984,2logdet( ˜Kn)1/2+ 1
REFERENCES,0.13418647166361974,2logdet(Kn)1/2+ 1
REFERENCES,0.13455210237659965,2σ2 yT y−1
REFERENCES,0.13491773308957952,"2µT ˜K−1
n µ−1"
REFERENCES,0.13528336380255943,2σ2 (y−f(x))T (y−f(x)) =−1
REFERENCES,0.1356489945155393,"2logdet(K−1
n + 1"
REFERENCES,0.1360146252285192,σ2 I)−1+ 1
REFERENCES,0.13638025594149908,2logdet(Kn)+ 1
REFERENCES,0.13674588665447898,2σ2 yT y−1
REFERENCES,0.13711151736745886,"2σ4 yT (K−1
n + 1"
REFERENCES,0.13747714808043876,σ2 I)−1y −1
REFERENCES,0.13784277879341864,2σ2 (y−f(x))T (y−f(x)) = 1
REFERENCES,0.13820840950639854,2logdet(I+ Kn
REFERENCES,0.13857404021937844,σ2 )+ 1
REFERENCES,0.13893967093235832,2σ2 yT (I+ Kn
REFERENCES,0.13930530164533822,σ2 )−1y−1
REFERENCES,0.1396709323583181,2σ2 (y−f(x))T (y−f(x)). = 1
REFERENCES,0.140036563071298,2logdet(I+ Kn
REFERENCES,0.14040219378427787,σ2 )+ 1
REFERENCES,0.14076782449725778,2σ2 f(x)T (I+ Kn
REFERENCES,0.14113345521023765,σ2 )−1f(x)+ 1
REFERENCES,0.14149908592321755,2σ2 ϵT (I+ Kn
REFERENCES,0.14186471663619743,σ2 )−1ϵ−1
REFERENCES,0.14223034734917733,2σ2 ϵT ϵ + 1
REFERENCES,0.1425959780621572,2σ2 ϵT (I+ Kn
REFERENCES,0.1429616087751371,σ2 )−1f(x)
REFERENCES,0.14332723948811701,".
(29)
After taking the expectation over noises ϵ, we get"
REFERENCES,0.1436928702010969,EϵF 0(Dn)= 1
REFERENCES,0.1440585009140768,2logdet(I+ Kn
REFERENCES,0.14442413162705667,σ2 )+ 1
REFERENCES,0.14478976234003657,2σ2 f(x)T (I+ Kn
REFERENCES,0.14515539305301645,σ2 )−1f(x)−1
REFERENCES,0.14552102376599635,2Tr(I−(I+ Kn
REFERENCES,0.14588665447897622,"σ2 )−1).
(30)"
REFERENCES,0.14625228519195613,This concludes the proof.
REFERENCES,0.146617915904936,"C
HELPER LEMMAS"
REFERENCES,0.1469835466179159,"Lemma 15. Assume that m →∞as n →∞. Given constants a1,a2,s1,s2 > 0, if s1 > 1 and
s2s3 >s1−1 , we have that
R
X i=1"
REFERENCES,0.1473491773308958,a1i−s1
REFERENCES,0.14771480804387568,(1+a2mi−s2)s3 =Θ(m 1−s1
REFERENCES,0.1480804387568556,"s2 ).
(31)"
REFERENCES,0.14844606946983546,Published as a conference paper at ICLR 2022
REFERENCES,0.14881170018281537,"If s1 >1 and s2s3 =s1−1, we have that R
X i=1"
REFERENCES,0.14917733089579524,a1i−s1
REFERENCES,0.14954296160877514,"(1+a2mi−s2)s3 =Θ(m−s3logm).
(32)"
REFERENCES,0.14990859232175502,"If s1 >1 and s2s3 <s1−1, we have that R
X i=1"
REFERENCES,0.15027422303473492,a1i−s1
REFERENCES,0.1506398537477148,"(1+a2mi−s2)s3 =Θ(m−s3).
(33)"
REFERENCES,0.1510054844606947,"Overall, if s1 >1 and m→∞, R
X i=1"
REFERENCES,0.15137111517367458,a1i−s1
REFERENCES,0.15173674588665448,(1+a2mi−s2)s3 =
REFERENCES,0.15210237659963438,"(
Θ(mmax{−s3, 1−s1"
REFERENCES,0.15246800731261426,"s2
}),
s2s3 ̸=s1−1, Θ(m 1−s1"
REFERENCES,0.15283363802559416,"s2 logm),
s2s3 =s1−1.
(34)"
REFERENCES,0.15319926873857403,"Proof of Lemma 15. First, when s1 >1 and s2s3 >s1−1, we have that R
X i=1"
REFERENCES,0.15356489945155394,a1i−s1
REFERENCES,0.1539305301645338,"(1+a2mi−s2)s3 ≤
a1
(1+a2m)s3 +
Z"
REFERENCES,0.15429616087751372,"[1,+∞]"
REFERENCES,0.1546617915904936,a1x−s1
REFERENCES,0.1550274223034735,(1+a2mx−s2)s3 dx
REFERENCES,0.15539305301645337,"=
a1
(1+a2m)s3 +m 1−s1 s2
Z"
REFERENCES,0.15575868372943327,"[1,+∞]"
REFERENCES,0.15612431444241318,"a1(
x
m1/s2 )−s1"
REFERENCES,0.15648994515539305,"(1+a2(
x
m1/s2 )−s2)s3 d
x
m1/s2"
REFERENCES,0.15685557586837295,"=
a1
(1+a2m)s3 +m 1−s1 s2
Z"
REFERENCES,0.15722120658135283,"[1/m1/s2,+∞]"
REFERENCES,0.15758683729433273,a1x−s1
REFERENCES,0.1579524680073126,(1+a2x−s2)s3 dx =Θ(m 1−s1 s2 ).
REFERENCES,0.1583180987202925,"On the other hand, we have R
X i=1"
REFERENCES,0.15868372943327239,a1i−s1
REFERENCES,0.1590493601462523,"(1+a2mi−s2)s3 ≥
Z"
REFERENCES,0.15941499085923216,"[1,R+1]"
REFERENCES,0.15978062157221207,a1x−s1
REFERENCES,0.16014625228519194,"(1+a2mx−s2)s3 dx =m 1−s1 s2
Z"
REFERENCES,0.16051188299817185,"[1,R+1]"
REFERENCES,0.16087751371115175,"a1(
x
m1/s2 )−s1"
REFERENCES,0.16124314442413162,"(1+a2(
x
m1/s2 )−s2)s3 d
x
m1/s2 =m 1−s1 s2
Z"
REFERENCES,0.16160877513711153,"[1/m1/s2,(R+1)/m1/s2]"
REFERENCES,0.1619744058500914,a1x−s1
REFERENCES,0.1623400365630713,(1+a2x−s2)s3 dx =Θ(m 1−s1 s2 ).
REFERENCES,0.16270566727605118,"Second, when s1 >1 and s2s3 =s1−1, we have that R
X i=1"
REFERENCES,0.16307129798903108,a1i−s1
REFERENCES,0.16343692870201096,"(1+a2mi−s2)s3 ≤
a1
(1+a2m)s3 +m 1−s1 s2
Z"
REFERENCES,0.16380255941499086,"[1/m1/s2,+∞]"
REFERENCES,0.16416819012797074,a1x−s1
REFERENCES,0.16453382084095064,(1+a2x−s2)s3 dx
REFERENCES,0.16489945155393054,"≤
a1
(1+a2m)s3 +m 1−s1"
REFERENCES,0.16526508226691042,s2 O(logm(1/s2)) =Θ(m 1−s1
REFERENCES,0.16563071297989032,s2 logn).
REFERENCES,0.1659963436928702,"On the other hand, we have R
X i=1"
REFERENCES,0.1663619744058501,a1i−s1
REFERENCES,0.16672760511882997,"(1+a2mi−s2)s3 ≥
Z"
REFERENCES,0.16709323583180988,"[1,R+1]"
REFERENCES,0.16745886654478975,a1x−s1
REFERENCES,0.16782449725776966,"(1+a2mx−s2)s3 dx =m 1−s1 s2
Z"
REFERENCES,0.16819012797074953,"[1,R+1]"
REFERENCES,0.16855575868372943,"a1(
x
m1/s2 )−s1"
REFERENCES,0.16892138939670934,"(1+a2(
x
m1/s2 )−s2)s3 d
x
m1/s2 =m 1−s1 s2
Z"
REFERENCES,0.1692870201096892,"[1/m1/s2,(R+1)/m1/s2]"
REFERENCES,0.16965265082266912,a1x−s1
REFERENCES,0.170018281535649,(1+a2x−s2)s3 dx =Θ(m 1−s1
REFERENCES,0.1703839122486289,s2 logn).
REFERENCES,0.17074954296160877,Published as a conference paper at ICLR 2022
REFERENCES,0.17111517367458867,"Third, when s1 >1 and s2s3 <s1−1, we have that R
X i=1"
REFERENCES,0.17148080438756855,a1i−s1
REFERENCES,0.17184643510054845,"(1+a2mi−s2)s3 ≤
a1
(1+a2m)s3 +m 1−s1 s2
Z"
REFERENCES,0.17221206581352833,"[1/m1/s2,+∞]"
REFERENCES,0.17257769652650823,a1x−s1
REFERENCES,0.1729433272394881,(1+a2x−s2)s3 dx
REFERENCES,0.173308957952468,"≤
a1
(1+a2m)s3 +m 1−s1"
REFERENCES,0.1736745886654479,s2 Θ(m(−1/s2)(1−s1+s2s3))
REFERENCES,0.17404021937842779,=Θ(m−s3).
REFERENCES,0.1744058500914077,"On the other hand, we have R
X i=1"
REFERENCES,0.17477148080438756,a1i−s1
REFERENCES,0.17513711151736747,"(1+a2mi−s2)s3 ≤
a1
(1+a2m)s3 +m 1−s1 s2
Z"
REFERENCES,0.17550274223034734,"[2/m1/s2,(R+1)/m1/s2]"
REFERENCES,0.17586837294332724,a1x−s1
REFERENCES,0.17623400365630712,(1+a2x−s2)s3 dx
REFERENCES,0.17659963436928702,"≤
a1
(1+a2m)s3 +m 1−s1"
REFERENCES,0.1769652650822669,s2 Θ(m(−1/s2)(1−s1+s2s3))
REFERENCES,0.1773308957952468,=Θ(m−s3).
REFERENCES,0.1776965265082267,"Overall, if s1 >1, R
X i=1"
REFERENCES,0.17806215722120658,a1i−s1
REFERENCES,0.17842778793418648,(1+a2mi−s2)s3 =
REFERENCES,0.17879341864716636,"(
Θ(mmax{−s3, 1−s1"
REFERENCES,0.17915904936014626,"s2
}),
s2s3 ̸=s1−1,
Θ(m−s3logn),
s2s3 =s1−1.
(35)"
REFERENCES,0.17952468007312614,"Lemma 16. Assume that R=m
1
s2 +κ for κ>0. Given constants a1,a2,s1,s2 >0 , if s1 ≤1, we have
that
R
X i=1"
REFERENCES,0.17989031078610604,a1i−s1
REFERENCES,0.18025594149908591,"(1+a2mi−s2)s3 = ˜O(max{m−s3,R1−s1}).
(36)"
REFERENCES,0.18062157221206582,"Proof of Lemma 16. First, when s1 ≤1 and s2s3 >s1−1, we have that R
X i=1"
REFERENCES,0.1809872029250457,a1i−s1
REFERENCES,0.1813528336380256,"(1+a2mi−s2)s3 ≤
a1
(1+a2m)s3 +
Z [1,R]"
REFERENCES,0.18171846435100547,a1x−s1
REFERENCES,0.18208409506398537,(1+a2mx−s2)s3 dx
REFERENCES,0.18244972577696528,"=
a1
(1+a2m)s3 +m 1−s1 s2
Z [1,R]"
REFERENCES,0.18281535648994515,"a1(
x
m1/s2 )−s1"
REFERENCES,0.18318098720292506,"(1+a2(
x
m1/s2 )−s2)s3 d
x
m1/s2"
REFERENCES,0.18354661791590493,"=
a1
(1+a2m)s3 +m 1−s1 s2
Z"
REFERENCES,0.18391224862888483,"[1/m1/s2,R/m1/s2]"
REFERENCES,0.1842778793418647,a1x−s1
REFERENCES,0.1846435100548446,(1+a2x−s2)s3 dx
REFERENCES,0.1850091407678245,"=
a1
(1+a2m)s3 + ˜O(m 1−s1"
REFERENCES,0.1853747714808044,"s2 (
R
m1/s2 )1−s1)"
REFERENCES,0.18574040219378427,"= ˜O(max{m−s3,R1−s1})."
REFERENCES,0.18610603290676417,"Second, when s1 ≤1 and s2s3 ≤s1−1, we have that R
X i=1"
REFERENCES,0.18647166361974407,a1i−s1
REFERENCES,0.18683729433272395,"(1+a2mi−s2)s3 ≤
a1
(1+a2m)s3 +m 1−s1 s2
Z"
REFERENCES,0.18720292504570385,"[1/m1/s2,R/m1/s2]"
REFERENCES,0.18756855575868372,a1x−s1
REFERENCES,0.18793418647166363,(1+a2x−s2)s3 dx
REFERENCES,0.1882998171846435,"≤
a1
(1+a2m)s3 +m 1−s1"
REFERENCES,0.1886654478976234,"s2
˜O(m(−1/s2)(1−s1+s2s3)+(
R
m1/s2 )1−s1)"
REFERENCES,0.18903107861060328,"= ˜O(max{m−s3,R1−s1})."
REFERENCES,0.18939670932358318,"Overall, if s1 ≤1,
R
X i=1"
REFERENCES,0.18976234003656306,a1i−s1
REFERENCES,0.19012797074954296,"(1+a2mi−s2)s3 = ˜O(max{m−s3,R1−s1}).
(37)"
REFERENCES,0.19049360146252287,Published as a conference paper at ICLR 2022
REFERENCES,0.19085923217550274,"Lemma 17. Assume that f ∈L2(Ω,ρ). Consider the random vector f(x) = (f(x1),...,f(xn))T ,
where x1,...,xn are drawn i.i.d from ρ. Then with probability of at least 1−δ1, we have"
REFERENCES,0.19122486288848264,"∥f(x)∥2
2 = n
X"
REFERENCES,0.19159049360146252,"i=1
f 2(xi)= ˜O

( 1"
REFERENCES,0.19195612431444242,"δ1 +1)n∥f∥2
2

,"
REFERENCES,0.1923217550274223,"where ∥f∥2
2 =
R"
REFERENCES,0.1926873857404022,x∈Ωf 2(x)dρ(x).
REFERENCES,0.19305301645338208,"Proof of Lemma 17. Given a positive number C ≥∥f∥2
2, applying Markov’s inequality we have"
REFERENCES,0.19341864716636198,P(f 2(X)>C)≤1
REFERENCES,0.19378427787934185,"C ∥f∥2
2."
REFERENCES,0.19414990859232176,"Let A be the event that for all sample inputs (xi)n
i=1, f 2(xi)≤C. Then"
REFERENCES,0.19451553930530163,P(A)≥1−nP(f 2(X)>C)≥1−1
REFERENCES,0.19488117001828154,"C n∥f∥2
2.
(38)"
REFERENCES,0.19524680073126144,"Deﬁne ¯f 2(x) = min{f 2(x),C}. Then E ¯f 2(X) ≤Ef 2(X) = ∥f∥2
2. So | ¯f 2(X) −E ¯f 2(X)| ≤
max{C,∥f∥2
2}=C Since 0≤¯f 2(x)≤C, we have"
REFERENCES,0.1956124314442413,"E( ¯f 4(X))≤CE( ¯f 2(X))≤C∥f∥2
2.
(39)"
REFERENCES,0.19597806215722122,"So we have
E| ¯f 2(X)−E ¯f 2(X)|2 ≤E( ¯f 4(X))≤C∥f∥2
2.
(40)
Applying Bernstein’s inequality, we have P( n
X"
REFERENCES,0.1963436928702011,"i=1
¯f 2(xi)>t+nE ¯f 2(X))≤exp  −
t2"
REFERENCES,0.196709323583181,"2(nE| ¯f 2(X)−E ¯f 2(X)|2)+ Ct 3 ) ! ≤exp  −
t2"
REFERENCES,0.19707495429616087,"2(nC∥f∥2
2+ Ct 3 ) ! ≤exp  −
t2"
REFERENCES,0.19744058500914077,"4max{nC∥f∥2
2, Ct 3 } ! ."
REFERENCES,0.19780621572212065,"Hence, with probability of at least 1−δ1/2 we have n
X"
REFERENCES,0.19817184643510055,"i=1
¯f 2(xi)≤max
r"
REFERENCES,0.19853747714808043,4Clog 2
REFERENCES,0.19890310786106033,"δ1
n∥f∥2
2,4C"
REFERENCES,0.19926873857404023,3 log 2 δ1
REFERENCES,0.1996343692870201,"
+nE ¯f 2(X)"
REFERENCES,0.2,"≤max
r"
REFERENCES,0.2003656307129799,4Clog 2
REFERENCES,0.2007312614259598,"δ1
n∥f∥2
2,4C"
REFERENCES,0.20109689213893966,3 log 2 δ1
REFERENCES,0.20146252285191957,"
+n∥f∥2
2. (41)"
REFERENCES,0.20182815356489944,"When event A happens, f 2(xi) = ¯f 2(xi) for all sample inputs. According to (38) and (41), with
probability at least 1−1"
REFERENCES,0.20219378427787935,"C n∥f∥2
2−δ1/2, we have n
X"
REFERENCES,0.20255941499085922,"i=1
f 2(xi)= n
X"
REFERENCES,0.20292504570383912,"i=1
¯f 2(xi)≤max
r"
REFERENCES,0.203290676416819,4Clog 2
REFERENCES,0.2036563071297989,"δ1
n∥f∥2
2,4C"
REFERENCES,0.2040219378427788,3 log 2 δ1
REFERENCES,0.20438756855575868,"
+n∥f∥2
2."
REFERENCES,0.20475319926873858,Choosing C = 2
REFERENCES,0.20511882998171846,"δ1 n∥f∥2
2, with probability of at least 1−δ1 we have n
X"
REFERENCES,0.20548446069469836,"i=1
f 2(xi)= n
X"
REFERENCES,0.20585009140767824,"i=1
¯f 2(xi)≤max
r"
REFERENCES,0.20621572212065814,"8
δ1
log 2"
REFERENCES,0.20658135283363802,"δ1
n2∥f∥4
2, 8"
REFERENCES,0.20694698354661792,"3δ1
n∥f∥2
2log 2 δ1"
REFERENCES,0.2073126142595978,"
+n∥f∥2
2 = ˜O

( 1"
REFERENCES,0.2076782449725777,"δ1 +1)n∥f∥2
2

."
REFERENCES,0.2080438756855576,"Lemma 18. Assume that f ∈L2(Ω,ρ). Consider the random vector f(x) = (f(x1),...,f(xn))T ,
where x1,...,xn are drawn i.i.d from ρ. Assume that ∥f∥∞= supx∈Ωf(x) ≤C. With probability of
at least 1−δ1, we have"
REFERENCES,0.20840950639853748,"∥f(x)∥2
2 = ˜O
q"
REFERENCES,0.20877513711151738,"C2n∥f∥2
2+C2

+n∥f∥2
2,"
REFERENCES,0.20914076782449725,"where ∥f∥2
2 =
R"
REFERENCES,0.20950639853747716,x∈Ωf 2(x)dρ(x).
REFERENCES,0.20987202925045703,Published as a conference paper at ICLR 2022
REFERENCES,0.21023765996343693,"Proof of Lemma 18. We have |f 2(X)−Ef 2(X)| ≤max{C2,∥f∥2
2} = C2 Since 0 ≤f 2(x) ≤C, we
have
E(f 4(X))≤C2E(f 2(X))≤C2∥f∥2
2.
(42)"
REFERENCES,0.2106032906764168,"So we have
E|f 2(X)−Ef 2(X)|2 ≤E(f 4(X))≤C2∥f∥2
2.
(43)"
REFERENCES,0.2109689213893967,"Applying Bernstein’s inequality, we have P( n
X"
REFERENCES,0.2113345521023766,"i=1
f 2(xi)>t+nEf 2(X))≤exp  −
t2"
REFERENCES,0.2117001828153565,"2(nE|f 2(X)−Ef 2(X)|2)+ C2t 3 ) ! ≤exp  −
t2"
REFERENCES,0.21206581352833637,"2(nC2∥f∥2
2+ C2t 3 ) ! ≤exp  −
t2"
REFERENCES,0.21243144424131627,"4max{nC2∥f∥2
2, C2t 3 } ! ."
REFERENCES,0.21279707495429617,"Hence, with probability of at least 1−δ1 we have n
X"
REFERENCES,0.21316270566727605,"i=1
f 2(xi)≤max
r"
REFERENCES,0.21352833638025595,4C2log 1
REFERENCES,0.21389396709323583,"δ1
n∥f∥2
2,4C2"
REFERENCES,0.21425959780621573,3 log 1 δ1
REFERENCES,0.2146252285191956,"
+nEf 2(X)"
REFERENCES,0.2149908592321755,"≤˜O

max
q"
REFERENCES,0.21535648994515538,"C2n∥f∥2
2,C2

+n∥f∥2
2"
REFERENCES,0.21572212065813529,"≤˜O
q"
REFERENCES,0.21608775137111516,"C2n∥f∥2
2+C2

+n∥f∥2
2. (44)"
REFERENCES,0.21645338208409506,"For the proofs in the reminder of this section, the deﬁnitions of the relevant quantities are given in
Section 3."
REFERENCES,0.21681901279707497,"Corollary 19. With probability of at least 1−δ1, we have"
REFERENCES,0.21718464351005484,"∥f>R(x)∥2
2 = ˜O

( 1"
REFERENCES,0.21755027422303475,"δ1 +1)nR1−2β
."
REFERENCES,0.21791590493601462,"Proof of Corollary 19. The L2 norm of f>R(x) is given by ∥f>R∥2
2 = P∞
p=R+1µ2
p ≤
Cµ
2β−1R1−2β.
Applying Lemma 17 we get the result."
REFERENCES,0.21828153564899452,"Corollary 20. For any ν ∈RR, with probability of at least 1−δ1 we have"
REFERENCES,0.2186471663619744,"∥ΦRν∥2
2 = ˜O

( 1"
REFERENCES,0.2190127970749543,"δ1 +1)n∥ν∥2
2

."
REFERENCES,0.21937842778793418,"Proof of Corollary 20. Let g(x)=PR
p=1νpφp(x). Then ΦRν =g(x). The L2 norm of g(x) is given"
REFERENCES,0.21974405850091408,"by ∥g∥2
2 =PR
p=1ν2
p =∥ν∥2
2. Applying Lemma 17 we get the result."
REFERENCES,0.22010968921389396,"Next we consider the quantity, ΦT
RΦR−nI. The key tool that we use is the matrix Bernstein inequality
that describes the upper tail of a sum of independent zero-mean random matrices."
REFERENCES,0.22047531992687386,"Lemma 21. Let D = diag{d1, ... , dR}, d1, ... , dR > 0 and dmax = max{d1, ... , dR}.
Let
M =max{PR
p=0d2
p∥φp∥2
∞,d2
max}. Then with probability of at least 1−δ, we have"
REFERENCES,0.22084095063985376,"∥D(ΦT
RΦR−nI)D∥2 ≤max
q"
REFERENCES,0.22120658135283364,nd2maxMlog R
REFERENCES,0.22157221206581354,"δ ,Mlog R"
REFERENCES,0.22193784277879341,"δ )

.
(45)"
REFERENCES,0.22230347349177332,Published as a conference paper at ICLR 2022
REFERENCES,0.2226691042047532,"Proof of Lemma 21. Let Yj = (φ1(xj), ... , φR(xj))T and Zj = DYj.
It is easy to verify that
E(ZjZT
j )=D2. Then the left hand side of (45) is Pn
j=1[ZjZT
j −E(ZjZT
j )]. We note that"
REFERENCES,0.2230347349177331,"∥ZjZT
j −E(ZjZT
j )∥2 ≤max{∥ZjZT
j ∥2,∥E(ZjZT
j )∥2}≤max{∥Zj∥2
2,d2
max}."
REFERENCES,0.22340036563071297,"For ∥Zj∥2
2, we have"
REFERENCES,0.22376599634369287,"∥Zj∥2
2 = R
X"
REFERENCES,0.22413162705667275,"p=0
d2
pφ2
p(xj)≤ R
X"
REFERENCES,0.22449725776965265,"p=0
d2
p∥φp∥2
∞,
(46)"
REFERENCES,0.22486288848263253,"we have
∥ZjZT
j −E(ZjZT
j )∥2 ≤max{PR
p=0d2
p∥φp∥2
∞,d2
max}."
REFERENCES,0.22522851919561243,"On the other hand,"
REFERENCES,0.22559414990859233,"E[(ZjZT
j −E(ZjZT
j ))2]=E[∥Zj∥2
2ZjZT
j ]−(E(ZjZT
j ))2. Since"
REFERENCES,0.2259597806215722,"E[∥Zj∥2
2ZjZT
j ]≼E[ R
X"
REFERENCES,0.2263254113345521,"p=0
d2
p∥φp∥2
∞ZjZT
j ],
(by (46)) = R
X"
REFERENCES,0.226691042047532,"p=0
d2
p∥φp∥2
∞E[ZjZT
j ],"
REFERENCES,0.2270566727605119,"we have
∥E[(ZjZT
j −E(ZjZT
j ))2]∥2 ≤max{PR
p=0d2
p∥φp∥2
∞∥E[ZjZT
j ]∥2,d4
max}"
REFERENCES,0.22742230347349177,"≤max{PR
p=0d2
p∥φp∥2
∞d2
max,d4
max}"
REFERENCES,0.22778793418647167,"≤d2
maxmax{PR
p=0d2
p∥φp∥2
∞,d2
max}."
REFERENCES,0.22815356489945154,"Using the matrix Bernstein inequality (Tropp, 2012, Theorem 6.1), we have P(∥ n
X"
REFERENCES,0.22851919561243145,"j=1
[ZjZT
j −E(ZjZT
j )]∥2 >t) ≤Rexp  
−t2"
REFERENCES,0.22888482632541132,"2(n∥E[(ZjZT
j −E(ZjZT
j ))2]∥2+
tmaxj∥ZjZT
j −E(ZjZT
j )∥2
3
)   ≤Rexp  
−t2"
REFERENCES,0.22925045703839123,"2(nd2maxmax{PR
p=0d2p∥φp∥2∞,d2max}+
tmax{PR
p=0d2p∥φp∥2∞,d2max} 3
)   =Rexp −t2"
REFERENCES,0.22961608775137113,"O(max{nd2maxmax{PR
p=0d2p∥φp∥2∞,d2max},tmax{PR
p=0d2p∥φp∥2∞,d2max}}) ! ."
REFERENCES,0.229981718464351,"Then with probability of at least 1−δ, we have ∥ n
X"
REFERENCES,0.2303473491773309,"j=1
[ZjZT
j −E(ZjZT
j )]∥2"
REFERENCES,0.23071297989031078,"≤max
q"
REFERENCES,0.23107861060329069,"nd2maxmax{PR
p=0d2
p∥φp∥2
∞,d2
max}log R"
REFERENCES,0.23144424131627056,"δ ,max
PR
p=0d2
p∥φp∥2
∞,d2
max
	
log R δ 
."
REFERENCES,0.23180987202925046,"Corollary 22. Suppose that the eigenvalues (λp)p≥1 satisfy Assumption 4, and the eigenfunctions
satisfy Assumption 6. Assume σ2 = Θ(nt) where 1−
α
1+2τ < t < 1 Let γ be a positive number such"
REFERENCES,0.23217550274223034,that 1+α+2τ−(1+2τ+2α)t
REFERENCES,0.23254113345521024,"2α(1−t)
<γ ≤1. Then with probability of at least 1−δ, we have ∥1"
REFERENCES,0.23290676416819012,σ2 (I+ n
REFERENCES,0.23327239488117002,"σ2 ΛR)−γ/2Λγ/2
R (ΦT
RΦR−nI)Λγ/2
R (I+ n"
REFERENCES,0.2336380255941499,σ2 ΛR)−γ/2∥2
REFERENCES,0.2340036563071298,"≤O

n
1+α+2τ−(1+2τ+2α)t"
REFERENCES,0.2343692870201097,"2α
−γ(1−t)q log R δ"
REFERENCES,0.23473491773308958,"
.
(47)"
REFERENCES,0.23510054844606948,Published as a conference paper at ICLR 2022
REFERENCES,0.23546617915904935,"Proof of Corollary 22. Use the same notation as in Lemma 21. Let D = (I +
n
σ2 ΛR)−γ/2Λγ/2
R ."
REFERENCES,0.23583180987202926,"Then d2
max ≤
σ2γ"
REFERENCES,0.23619744058500913,"nγ and PR
p=0 d2
p∥φp∥2
∞≤PR
p=0 C2
φ
λγ
pp2τ (1+ n"
REFERENCES,0.23656307129798904,σ2 λp)γ = O(( n
REFERENCES,0.2369287020109689,"σ2 )
1−γα+2τ"
REFERENCES,0.23729433272394881,"α
), where the
ﬁrst inequality follows from Assumptions 4 and 6 and the last equality from Lemma 15. Then
M =max{PR
p=0d2
p∥φp∥2
∞,d2
max}=O(( n"
REFERENCES,0.2376599634369287,"σ2 )
1−γα+2τ"
REFERENCES,0.2380255941499086,"α
). Applying Lemma 21, we have ∥1"
REFERENCES,0.2383912248628885,σ2 (I+ n
REFERENCES,0.23875685557586837,"σ2 ΛR)−γ/2Λγ/2
R (ΦT
RΦR−nI)Λγ/2
R (I+ n"
REFERENCES,0.23912248628884827,σ2 ΛR)−γ/2∥2 ≤1
REFERENCES,0.23948811700182815,"σ2 max
q n σ2γ"
REFERENCES,0.23985374771480805,nγ O(( n
REFERENCES,0.24021937842778793,"σ2 )
1−γα+2τ"
REFERENCES,0.24058500914076783,"α
)log R"
REFERENCES,0.2409506398537477,"δ ,O(( n"
REFERENCES,0.2413162705667276,"σ2 )
1−γα+2τ"
REFERENCES,0.24168190127970748,"α
)log R δ  =O( 1"
REFERENCES,0.2420475319926874,σ2 ( n
REFERENCES,0.24241316270566726,"σ2 )
1−2γα+2τ"
REFERENCES,0.24277879341864717,"2α
n
1
2 )=O(
q log R δ n"
REFERENCES,0.24314442413162707,(1−2γα+2τ)(1−t)
REFERENCES,0.24351005484460694,"2α
+ 1 2 −t) =O
q log R"
REFERENCES,0.24387568555758685,"δ n
1+α+2τ"
REFERENCES,0.24424131627056672,"2α
−(1+2τ+2α)t"
REFERENCES,0.24460694698354662,"2α
−γ(1−t)

. (48)"
REFERENCES,0.2449725776965265,"Corollary 23. Suppose that the eigenvalues (λp)p≥1 satisfy Assumption 4, and the eigenfunctions
satisfy Assumption 6. Let ˜Λ1,R = diag{1,λ1,...,λR}. Assume σ2 = Θ(nt) where t < 1 Let γ be a
positive number such that 1+2τ"
REFERENCES,0.2453382084095064,"α
<γ ≤1. Then with probability of at least 1−δ, we have"
REFERENCES,0.24570383912248628,∥(I+ n
REFERENCES,0.24606946983546618,"σ2 ΛR)−γ/2˜Λγ/2
1,R(ΦT
RΦR−nI)˜Λγ/2
1,R(I+ n"
REFERENCES,0.24643510054844606,"σ2 ΛR)−γ/2∥2 ≤O
q log R"
REFERENCES,0.24680073126142596,"δ n
1
2

.
(49)"
REFERENCES,0.24716636197440586,Proof of Corollary 23. Use the same notation as in Lemma 21. Let D=(I+ n
REFERENCES,0.24753199268738574,"σ2 ΛR)−γ/2˜Λγ/2
1,R. Then"
REFERENCES,0.24789762340036564,"d2
max ≤1 and PR
p=0d2
p∥φp∥2
∞≤C2
φ+PR
p=1C2
φ
λγ
pp2τ (1+ n"
REFERENCES,0.24826325411334552,"σ2 λp)γ =C2
φ+O(n
(1−γα+2τ)(1−t)"
REFERENCES,0.24862888482632542,"α
)=O(1) where
the ﬁrst inequality follows from Assumptions 4 and 6 and the second equality from Lemma 15. Then
M =max{PR
p=0d2
p∥φp∥2
∞,d2
max}=O(1). Applying Lemma 21, we have"
REFERENCES,0.2489945155393053,∥(I+ n
REFERENCES,0.2493601462522852,"σ2 ΛR)−γ/2Λγ/2
R (ΦT
RΦR−nI)Λγ/2
R (I+ n"
REFERENCES,0.24972577696526507,σ2 ΛR)−γ/2∥2
REFERENCES,0.25009140767824495,"≤max
q log R"
REFERENCES,0.25045703839122485,"δ nO(1),log R"
REFERENCES,0.25082266910420475,"δ O(1)
 =O
q log R"
REFERENCES,0.25118829981718466,"δ n
1
2

. (50)"
REFERENCES,0.25155393053016456,"Corollary 24. Suppose that the eigenvalues (λp)p≥1 satisfy Assumption 4, and the eigenfunctions
satisfy Assumption 6. Let ΦR+1:S =(φR+1(x),...,φS(x)), and ΛR+1:S =(λR+1,...,λS). Then with
probability of at least 1−δ, we have"
REFERENCES,0.2519195612431444,"∥Λ1/2
R+1:S(ΦT
R+1:SΦR+1:S−nI)Λ1/2
R+1:S∥2 ≤O
 
log S−R"
REFERENCES,0.2522851919561243,"δ
max{n
1
2 R
1−2α+2τ"
REFERENCES,0.2526508226691042,"2
,R1−α+2τ}

.
(51)"
REFERENCES,0.2530164533820841,"Proof of Corollary 24. Use the same notation as in Lemma 21.
Let D = Λ1/2
R+1:S.
Then
d2
max ≤CλR−α =O(R−α) and PS
p=R+1C2
φd2
pp2τ ≤PS
p=R+1C2
φCλp−αp2τ =O(R1−α+2τ), where"
REFERENCES,0.25338208409506396,"the ﬁrst inequality follows from Assumptions 4 and 6. Then M =max{PS
p=R+1C2
φd2
pp2τ,d2
max}=
O(R1−α+2τ). Applying Lemma 21, we have"
REFERENCES,0.25374771480804387,∥(I+ n
REFERENCES,0.25411334552102377,"σ2 ΛR)−γ/2Λγ/2
R (ΦT
RΦR−nI)Λγ/2
R (I+ n"
REFERENCES,0.2544789762340037,σ2 ΛR)−γ/2∥2
REFERENCES,0.2548446069469835,"≤max
q"
REFERENCES,0.2552102376599634,log S−R
REFERENCES,0.2555758683729433,"δ
nO(R−α)O(R1−α+2τ),log S−R"
REFERENCES,0.25594149908592323,"δ
O(R1−α+2τ))
"
REFERENCES,0.25630712979890313,"=O
 
log S−R"
REFERENCES,0.256672760511883,"δ
max{n
1
2 R
1−2α+2τ"
REFERENCES,0.2570383912248629,"2
,R1−α+2τ}

. (52)"
REFERENCES,0.2574040219378428,Published as a conference paper at ICLR 2022
REFERENCES,0.2577696526508227,"Lemma 25. Under the assumptions of Corollary 24, with probability of at least 1−δ, we have"
REFERENCES,0.25813528336380254,"∥Φ>RΛ>RΦT
>R∥2 = ˜O(max{nR−α,n
1
2 R
1−2α+2τ"
REFERENCES,0.25850091407678244,"2
,R1−α+2τ})."
REFERENCES,0.25886654478976234,"Proof of Lemma 25. For S ∈N, we have"
REFERENCES,0.25923217550274225,"∥Φ>SΛ>SΦT
>S∥2 ≤ ∞
X"
REFERENCES,0.2595978062157221,"p=S+1
∥Λpφp(x)φp(x)T ∥2 = ∞
X"
REFERENCES,0.259963436928702,"p=S+1
λp∥φp(x)∥2
2 ≤ ∞
X"
REFERENCES,0.2603290676416819,"p=S+1
λpnC2
φp2τ"
REFERENCES,0.2606946983546618,=O(nS1−α+2τ).
REFERENCES,0.2610603290676417,"Let S =R
α
α−1−2τ . Then we get ∥Φ>SΛ>SΦT
>S∥2 =O(nR−α)."
REFERENCES,0.26142595978062155,"Let ΦR+1:S =(φR+1(x),...,φS(x)), ΛR+1:S =(λR+1,...,λS). We then have"
REFERENCES,0.26179159049360146,"∥Φ>RΛ>RΦT
>R∥2 ≤∥Φ>SΛ>SΦT
>S∥2+∥ΦR+1:SΛR+1:SΦT
R+1:S∥2"
REFERENCES,0.26215722120658136,"≤O(nR−α)+∥Λ1/2
R+1:SΦT
R+1:SΦR+1:SΛ1/2
R+1:S∥2"
REFERENCES,0.26252285191956126,"≤O(nR−α)+n∥ΛR+1:S∥2+∥Λ1/2
R+1:S(ΦT
R+1:SΦR+1:S−nI)Λ1/2
R+1:S∥2"
REFERENCES,0.2628884826325411,"≤O(nR−α)+O(nR−α)+O(logR
α
α−1 −R"
REFERENCES,0.263254113345521,"δ
max{n
1
2 R
1−2α+2τ"
REFERENCES,0.2636197440585009,"2
,R1−α+2τ})"
REFERENCES,0.2639853747714808,"= ˜O(max{nR−α,n
1
2 R
1−2α+2τ"
REFERENCES,0.2643510054844607,"2
,R1−α+2τ}),"
REFERENCES,0.26471663619744057,where in the fourth inequality we use Corollary 24.
REFERENCES,0.26508226691042047,"Corollary 26. Assume that σ2 = Θ(1). If R = n
1
α +κ where 0 < κ < α−1−2τ"
REFERENCES,0.2654478976234004,"α(1+2τ), then with probability
of at least 1−δ, we have"
REFERENCES,0.2658135283363803,"∥(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
∥2 ≤∥
Φ>RΛ>RΦT
>R
σ2
∥2 = ˜O(n−κα)=o(1)."
REFERENCES,0.2661791590493601,"Proof of Corollary 26. By Lemma 25 and the assumption R=n
1
α +κ, we have"
REFERENCES,0.26654478976234003,"∥(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
∥2 ≤∥
Φ>RΛ>RΦT
>R
σ2
∥2"
REFERENCES,0.26691042047531993,"≤˜O(max{nR−α,n
1
2 R
1−2α+2τ"
REFERENCES,0.26727605118829983,"2
,R1−α+2τ})"
REFERENCES,0.2676416819012797,= ˜O(n−κα).
REFERENCES,0.2680073126142596,Lemma 27. Assume that ∥1
REFERENCES,0.2683729433272395,σ2 (I+ n
REFERENCES,0.2687385740402194,"σ2 ΛR)−γ/2Λγ/2
R (ΦT
RΦR−nI)Λγ/2
R (I+ n"
REFERENCES,0.2691042047531993,"σ2 ΛR)−γ/2∥2 <1 where
1+2τ"
REFERENCES,0.26946983546617914,"α
<γ ≤1. We then have (I+ 1"
REFERENCES,0.26983546617915904,"σ2 ΛRΦT
RΦR)−1"
REFERENCES,0.27020109689213895,=(I+ n
REFERENCES,0.27056672760511885,"σ2 ΛR)−1+ ∞
X"
REFERENCES,0.2709323583180987,"j=1
(−1)j  1"
REFERENCES,0.2712979890310786,σ2 (I+ n
REFERENCES,0.2716636197440585,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j(I+ n"
REFERENCES,0.2720292504570384,σ2 ΛR)−1.
REFERENCES,0.27239488117001825,Proof of Lemma 27. First note that ∥1
REFERENCES,0.27276051188299816,σ2 (I+ n
REFERENCES,0.27312614259597806,"σ2 ΛR)−1/2Λ1/2
R (ΦT
RΦR−nI)Λ1/2
R (I+ n"
REFERENCES,0.27349177330895796,σ2 ΛR)−1/2∥2 <∥1
REFERENCES,0.27385740402193787,σ2 (I+ n
REFERENCES,0.2742230347349177,"σ2 ΛR)−γ/2Λγ/2
R (ΦT
RΦR−nI)Λγ/2
R (I+ n"
REFERENCES,0.2745886654478976,σ2 ΛR)−γ/2∥2 <1.
REFERENCES,0.2749542961608775,Published as a conference paper at ICLR 2022
REFERENCES,0.2753199268738574,"Let ˜Λϵ,R = diag{ϵ,λ1,...,λR}. Since ΛR = diag{0,λ1,...,λR}, we have that when ϵ is sufﬁciently
small, ∥1"
REFERENCES,0.27568555758683727,σ2 (I + n
REFERENCES,0.2760511882998172,"σ2 ˜Λϵ,R)−1/2˜Λ1/2
ϵ,R(ΦT
RΦR −nI)˜Λ1/2
ϵ,R(I + n"
REFERENCES,0.2764168190127971,"σ2 ˜Λϵ,R)−1/2∥2 < 1. Since all diagonal
entries of ˜Λϵ,R are positive, we have (I+ 1"
REFERENCES,0.276782449725777,"σ2 ˜Λϵ,RΦT
RΦR)−1"
REFERENCES,0.2771480804387569,=(I+ n
REFERENCES,0.27751371115173673,"σ2 ˜Λϵ,R+ 1"
REFERENCES,0.27787934186471663,"σ2 ˜Λϵ,R(ΦT
RΦR−nI))−1"
REFERENCES,0.27824497257769654,"= ˜Λ1/2
ϵ,R(I+ n"
REFERENCES,0.27861060329067644,"σ2 ˜Λϵ,R)−1/2h
I+ 1"
REFERENCES,0.2789762340036563,σ2 (I+ n
REFERENCES,0.2793418647166362,"σ2 ˜Λϵ,R)−1/2˜Λ1/2
ϵ,R(ΦT
RΦR−nI)˜Λ1/2
ϵ,R(I+ n"
REFERENCES,0.2797074954296161,"σ2 ˜Λϵ,R)−1/2i−1 (I+ n"
REFERENCES,0.280073126142596,"σ2 ˜Λϵ,R)−1/2˜Λ−1/2
ϵ,R
=(I+ n"
REFERENCES,0.28043875685557584,"σ2 ˜Λϵ,R)−1 + ∞
X j=1 """
REFERENCES,0.28080438756855575,"(−1)j ˜Λ1/2
ϵ,R(I+ n"
REFERENCES,0.28117001828153565,"σ2 ˜Λϵ,R)−1/2
1
σ2 (I+ n"
REFERENCES,0.28153564899451555,"σ2 ˜Λϵ,R)−1/2˜Λ1/2
ϵ,R(ΦT
RΦR−nI)˜Λ1/2
ϵ,R(I+ n"
REFERENCES,0.28190127970749546,"σ2 ˜Λϵ,R)−1/2j (I+ n"
REFERENCES,0.2822669104204753,"σ2 ˜Λϵ,R)−1/2˜Λ−1/2
ϵ,R #"
REFERENCES,0.2826325411334552,=(I+ n
REFERENCES,0.2829981718464351,"σ2 ˜Λϵ,R)−1+ ∞
X"
REFERENCES,0.283363802559415,"j=1
(−1)j
1
σ2 (I+ n"
REFERENCES,0.28372943327239486,"σ2 ˜Λϵ,R)−1˜Λϵ,R(ΦT
RΦR−nI)
j
(I+ n"
REFERENCES,0.28409506398537476,"σ2 ˜Λϵ,R)−1."
REFERENCES,0.28446069469835467,"Letting ϵ→0, we get (I+ 1"
REFERENCES,0.28482632541133457,"σ2 ΛRΦT
RΦR)−1"
REFERENCES,0.2851919561243144,=(I+ n
REFERENCES,0.2855575868372943,"σ2 ΛR)−1+ ∞
X"
REFERENCES,0.2859232175502742,"j=1
(−1)j
 1"
REFERENCES,0.2862888482632541,σ2 (I+ n
REFERENCES,0.28665447897623403,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j
(I+ n"
REFERENCES,0.2870201096892139,σ2 ΛR)−1.
REFERENCES,0.2873857404021938,This concludes the proof.
REFERENCES,0.2877513711151737,"Lemma 28. If ∥(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
∥2 <1, then we have"
REFERENCES,0.2881170018281536,(I+ ΦΛΦT
REFERENCES,0.28848263254113343,"σ2
)−1−(I+ ΦRΛRΦT
R
σ2
)−1 = ∞
X"
REFERENCES,0.28884826325411334,"j=1
(−1)j
(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
j
(I+ ΦRΛRΦT
R
σ2
)−1. (53)"
REFERENCES,0.28921389396709324,"In particular, assume that σ2 =Θ(1). Let R=n
1
α +κ where 0<κ< α−1−2τ"
REFERENCES,0.28957952468007314,α(1+2τ). Then with probability of
REFERENCES,0.289945155393053,"at least 1−δ, for sufﬁciently large n, we have ∥(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
∥2 <1 and (53) holds."
REFERENCES,0.2903107861060329,"Proof of Lemma 28. Deﬁne Φ>R =(φR+1(x),φR+2(x),...), Λ>R =diag(λR+1,λR+2,...). Then we
have
(I+ ΦΛΦT"
REFERENCES,0.2906764168190128,"σ2
)−1−(I+ ΦRΛRΦT
R
σ2
)−1"
REFERENCES,0.2910420475319927,"=(I+ ΦRΛRΦT
R
σ2
+
Φ>RΛ>RΦT
>R
σ2
)−1−(I+ ΦRΛRΦT
R
σ2
)−1"
REFERENCES,0.2914076782449726,"=

I+(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
−1
−I

(I+ ΦRΛRΦT
R
σ2
)−1."
REFERENCES,0.29177330895795245,"By Corollary 26, for sufﬁciently large n, ∥(I + ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
∥2 < 1 with probability of
at least 1−δ. Hence"
REFERENCES,0.29213893967093235,(I+ ΦΛΦT
REFERENCES,0.29250457038391225,"σ2
)−1−(I+ ΦRΛRΦT
R
σ2
)−1"
REFERENCES,0.29287020109689216,"=

I+(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
−1
−I

(I+ ΦRΛRΦT
R
σ2
)−1 = ∞
X"
REFERENCES,0.293235831809872,"j=1
(−1)j
(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
j
(I+ ΦRΛRΦT
R
σ2
)−1."
REFERENCES,0.2936014625228519,Published as a conference paper at ICLR 2022
REFERENCES,0.2939670932358318,"Lemma 29. Assume that µ0 =0 and σ2 =Θ(nt) where 1−
α
1+2τ <t<1. Let R=n( 1"
REFERENCES,0.2943327239488117,α +κ)(1−t) where
REFERENCES,0.2946983546617916,0<κ< α−1−2τ+(1+2τ)t
REFERENCES,0.29506398537477146,"α2(1−t)
. Then when n is sufﬁciently large, with probability of at least 1−2δ we have"
REFERENCES,0.29542961608775137,∥(I+ 1
REFERENCES,0.29579524680073127,"σ2 ΦRΛRΦT
R)−1fR(x)∥2 = ˜O
q ( 1"
REFERENCES,0.2961608775137112,"δ +1)n·nmax{−(1−t), (1−2β)(1−t)"
REFERENCES,0.296526508226691,"2α
}

.
(54)"
REFERENCES,0.2968921389396709,"Proof of Lemma 29. Let Λ1:R
=
diag{λ1, ... , λR}, Φ1:R
=
(φ1(x), φ1(x), ... , φR(x))
and µ1:R
=
(µ1, ... , µR).
Since µ0
=
0, we have (I +
1
σ2 ΦRΛRΦT
R)−1fR(x)
=
(I+ 1"
REFERENCES,0.2972577696526508,"σ2 Φ1:RΛ1:RΦT
1:R)−1Φ1:Rµ1:R. Using the Woodbury matrix identity, we have that (I+ 1"
REFERENCES,0.29762340036563073,"σ2 Φ1:RΛ1:RΦT
1:R)−1Φ1:Rµ1:R =

I−Φ1:R(σ2I+Λ1:RΦT
1:RΦ1:R)−1Λ1:RΦT
1:R

Φ1:Rµ1:R
=Φ1:Rµ1:R−Φ1:R(σ2I+Λ1:RΦT
1:RΦ1:R)−1Λ1:RΦT
1:RΦ1:Rµ1:R
=Φ1:R(I+ 1"
REFERENCES,0.2979890310786106,"σ2 Λ1:RΦT
1:RΦ1:R)−1µ1:R.
(55)
Let A = (I +
n
σ2 Λ1:R)−1/2Λ1/2
1:R(ΦT
1:RΦ1:R −nI)Λ1/2
1:R(I +
n
σ2 Λ1:R)−1/2.By Corollary 22, with"
REFERENCES,0.2983546617915905,"probability of at least 1−δ, we have ∥1"
REFERENCES,0.2987202925045704,"σ2 A∥2 =
q log R"
REFERENCES,0.2990859232175503,"δ n
1−α+2τ"
REFERENCES,0.2994515539305302,"2α
−(1+2τ)t"
REFERENCES,0.29981718464351004,"2α
. When n is sufﬁciently large, ∥1"
REFERENCES,0.30018281535648994,"σ2 A∥2 =o(1) is less than 1 because 1−
α
1+2τ <t<1. By Lemma 27, we have (I+ 1"
REFERENCES,0.30054844606946984,"σ2 Λ1:RΦT
1:RΦ1:R)−1"
REFERENCES,0.30091407678244975,=(I+ n
REFERENCES,0.3012797074954296,"σ2 Λ1:R)−1+ ∞
X"
REFERENCES,0.3016453382084095,"j=1
(−1)j  1"
REFERENCES,0.3020109689213894,σ2 (I+ n
REFERENCES,0.3023765996343693,"σ2 Λ1:R)−1Λ1:R(ΦT
1:RΦ1:R−nI)
j(I+ n"
REFERENCES,0.30274223034734915,σ2 Λ1:R)−1.
REFERENCES,0.30310786106032905,We then have
REFERENCES,0.30347349177330896,∥(I+ 1
REFERENCES,0.30383912248628886,"σ2 Λ1:RΦT
1:RΦ1:R)−1µ1:R∥2 =  "
REFERENCES,0.30420475319926876,(I+ n
REFERENCES,0.3045703839122486,"σ2 Λ1:R)−1+ ∞
X"
REFERENCES,0.3049360146252285,"j=1
(−1)j  1"
REFERENCES,0.3053016453382084,σ2 (I+ n
REFERENCES,0.3056672760511883,"σ2 Λ1:R)−1Λ1:R(ΦT
1:RΦ1:R−nI)
j(I+ n"
REFERENCES,0.30603290676416817,σ2 Λ1:R)−1  µ1:R 2 ≤ 
REFERENCES,0.30639853747714807,∥(I+ n
REFERENCES,0.306764168190128,"σ2 Λ1:R)−1µ1:R∥2+ ∞
X j=1   1"
REFERENCES,0.3071297989031079,σ2 (I+ n
REFERENCES,0.3074954296160878,"σ2 Λ1:R)−1Λ1:R(ΦT
1:RΦ1:R−nI)
j(I+ n"
REFERENCES,0.3078610603290676,"σ2 Λ1:R)−1µ1:R

2  ."
REFERENCES,0.30822669104204753,"(56)
By Lemma 15 and Assumption 5, assuming that supi≥1pi+1−pi =h, we have"
REFERENCES,0.30859232175502743,∥(I+ n
REFERENCES,0.30895795246800734,σ2 Λ1:R)−1µ1:R∥2 ≤
REFERENCES,0.3093235831809872,"v
u
u
t R
X p=1"
REFERENCES,0.3096892138939671,C2µp−2β
REFERENCES,0.310054844606947,"(1+nCλp−α/σ2)2 =Θ(nmax{−(1−t), (1−2β)(1−t)"
REFERENCES,0.3104204753199269,"2α
}logk/2n),"
REFERENCES,0.31078610603290674,∥(I+ n
REFERENCES,0.31115173674588664,σ2 Λ1:R)−1µ1:R∥2 ≥
REFERENCES,0.31151736745886655,"v
u
u
t ⌊R h ⌋
X i=1"
REFERENCES,0.31188299817184645,C2µi−2β (1+ n
REFERENCES,0.31224862888482635,"σ2 Cλ(hi)−α)2 =Θ(nmax{−(1−t), (1−2β)(1−t)"
REFERENCES,0.3126142595978062,"2α
}logk/2n)"
REFERENCES,0.3129798903107861,"where k=
0,
2α̸=2β−1,
1,
2α=2β−1.. Overall we have"
REFERENCES,0.313345521023766,∥(I+ n
REFERENCES,0.3137111517367459,"σ2 Λ1:R)−1µ1:R∥2 =Θ(n(1−t)max{−1, 1−2β"
REFERENCES,0.31407678244972576,"2α
}logk/2n).
(57)"
REFERENCES,0.31444241316270566,Using the fact that ∥1
REFERENCES,0.31480804387568556,"σ2 A∥2 =
q log R"
REFERENCES,0.31517367458866546,"δ n
1−α+2τ"
REFERENCES,0.3155393053016453,"2α
−(1+2τ)t"
REFERENCES,0.3159049360146252,"2α
and ∥(I+ n"
REFERENCES,0.3162705667276051,"σ2 Λ1:R)−1Λ1:R∥2 ≤n−1, we have

  1"
REFERENCES,0.316636197440585,σ2 (I+ n
REFERENCES,0.3170018281535649,"σ2 Λ1:R)−1Λ1:R(ΦT
1:RΦ1:R−nI)
j(I+ n"
REFERENCES,0.31736745886654477,"σ2 Λ1:R)−1µ1:R

2"
REFERENCES,0.3177330895795247,"=
(I+ n"
REFERENCES,0.3180987202925046,σ2 Λ1:R)−1 2 Λ
REFERENCES,0.3184643510054845,"1
2
1:R( 1"
REFERENCES,0.31882998171846433,σ2 A)j(I+ n
REFERENCES,0.31919561243144423,σ2 Λ1:R)−1 2 Λ
REFERENCES,0.31956124314442413,"1
2
1:Rµ1:R 2"
REFERENCES,0.31992687385740404,≤˜O(n−1−t 2 )∥1
REFERENCES,0.3202925045703839,"σ2 A∥j
2∥(I+ n"
REFERENCES,0.3206581352833638,σ2 Λ1:R)−1
REFERENCES,0.3210237659963437,"2 Λ
−1"
REFERENCES,0.3213893967093236,"2
1:Rµ1:R∥2 (58)"
REFERENCES,0.3217550274223035,Published as a conference paper at ICLR 2022
REFERENCES,0.32212065813528334,By Lemma 16 and the assumption R=n( 1
REFERENCES,0.32248628884826325,"α +κ)(1−t),"
REFERENCES,0.32285191956124315,∥(I+ n
REFERENCES,0.32321755027422305,σ2 Λ1:R)−1
REFERENCES,0.3235831809872029,"2 Λ
−1"
REFERENCES,0.3239488117001828,"2
1:Rµ1:R∥2 ≤"
REFERENCES,0.3243144424131627,"v
u
u
t R
X p=1"
REFERENCES,0.3246800731261426,(Cλp−α)−1C2µp−2β
REFERENCES,0.3250457038391225,(1+nCλp−α/σ2)1
REFERENCES,0.32541133455210236,"= ˜O(max{n−(1−t)/2,R1/2−β+α/2})"
REFERENCES,0.32577696526508226,"= ˜O(max{n−(1−t)/2,n( 1"
REFERENCES,0.32614259597806217,2 + 1−2β
REFERENCES,0.32650822669104207,2α +κ(1/2−β+α/2))(1−t)}) (59)
REFERENCES,0.3268738574040219,"We then have

  1"
REFERENCES,0.3272394881170018,σ2 (I+ n
REFERENCES,0.3276051188299817,"σ2 Λ1:R)−1Λ1:R(ΦT
1:RΦ1:R−nI)
j(I+ n"
REFERENCES,0.3279707495429616,"σ2 Λ1:R)−1µ1:R

2
=∥1"
REFERENCES,0.3283363802559415,"σ2 A∥j
2 ˜O(max{n−(1−t),n( 1−2β"
REFERENCES,0.3287020109689214,"2α +κ(1/2−β+α/2))(1−t)})
(60)"
REFERENCES,0.3290676416819013,"By (56), (57) and (60), we have"
REFERENCES,0.3294332723948812,∥(I+ 1
REFERENCES,0.3297989031078611,"σ2 Λ1:RΦT
1:RΦ1:R)−1µ1:R∥2"
REFERENCES,0.33016453382084093,"=Θ(n(1−t)max{−1, 1−2β"
REFERENCES,0.33053016453382084,"2α }logk/2n)+ ∞
X"
REFERENCES,0.33089579524680074,"j=1
∥1"
REFERENCES,0.33126142595978064,"σ2 A∥j
2 ˜O(max{n−(1−t),n(1−t) 1−2β"
REFERENCES,0.3316270566727605,2α +κ(1−t)(1/2−β+α/2)})
REFERENCES,0.3319926873857404,"=Θ(n(1−t)max{−1, 1−2β"
REFERENCES,0.3323583180987203,"2α }logk/2n)+ ˜O(n
1−α+2τ"
REFERENCES,0.3327239488117002,"2α
−(1+2τ)t"
REFERENCES,0.33308957952468005,"2α
) ˜O(max{n−(1−t),n(1−t) 1−2β"
REFERENCES,0.33345521023765995,"2α +κ(1−t)(1/2−β+α/2)}).
(61)
By assumption κ< α−1−2τ+(1+2τ)t"
REFERENCES,0.33382084095063985,"α2(1−t)
, we have that"
REFERENCES,0.33418647166361976,κ(1−t)(1/2−β+α/2)+ 1−α+2τ
REFERENCES,0.33455210237659966,"2α
−(1+2τ)t"
REFERENCES,0.3349177330895795,"2α
<κα(1−t)/2+ 1−α+2τ"
REFERENCES,0.3352833638025594,"2α
−(1+2τ)t"
REFERENCES,0.3356489945155393,"2α
<0."
REFERENCES,0.3360146252285192,"Using (61), we then get"
REFERENCES,0.33638025594149906,∥(I+ 1
REFERENCES,0.33674588665447897,"σ2 Λ1:RΦT
1:RΦ1:R)−1µ1:R∥2 =Θ(n(1−t)max{−1, 1−2β"
REFERENCES,0.33711151736745887,2α }logk/2n)
REFERENCES,0.33747714808043877,= 1+o(1)
REFERENCES,0.3378427787934187,"σ2
∥(I+ n"
REFERENCES,0.3382084095063985,"σ2 Λ1:R)−1µ1:R∥2.
(62)"
REFERENCES,0.3385740402193784,"By Corollary 20, with probability of at least 1−δ, we have"
REFERENCES,0.33893967093235833,∥Φ1:R(I+ 1
REFERENCES,0.33930530164533823,"σ2 Λ1:RΦT
1:RΦ1:R)−1µ1:R∥2 = ˜O(
q ( 1"
REFERENCES,0.3396709323583181,δ +1)n∥(I+ 1
REFERENCES,0.340036563071298,"σ2 Λ1:RΦT
1:RΦ1:R)−1µ1:R∥2)"
REFERENCES,0.3404021937842779,"= ˜O(
q ( 1"
REFERENCES,0.3407678244972578,"δ +1)n·n(1−t)max{−1, 1−2β"
REFERENCES,0.34113345521023763,"2α }).
(63)"
REFERENCES,0.34149908592321754,"From (55), we get ∥(I + 1"
REFERENCES,0.34186471663619744,"σ2 Φ1:RΛ1:RΦT
1:R)−1Φ1:Rµ1:R∥2 = ˜O(
q ( 1"
REFERENCES,0.34223034734917734,"δ +1)n · n(1−t)max{−1, 1−2β"
REFERENCES,0.34259597806215725,"2α }).
This concludes the proof."
REFERENCES,0.3429616087751371,"Lemma 30. Assume that µ0 > 0 and σ2 = Θ(nt) where 1 −
α
1+2τ < t < 1. Let R = n
1
α +κ where"
REFERENCES,0.343327239488117,0<κ< α−1−2τ+(1+2τ)t
REFERENCES,0.3436928702010969,"α2
. Then when n is sufﬁciently large, with probability of at least 1−2δ, we have"
REFERENCES,0.3440585009140768,∥(I+ 1
REFERENCES,0.34442413162705665,"σ2 ΦRΛRΦT
R)−1fR(x)∥2 = ˜O
q ( 1"
REFERENCES,0.34478976234003655,"δ +1)n

.
(64)"
REFERENCES,0.34515539305301646,"Proof of Lemma 30. Using the Woodbury matrix identity, we have that (I+ 1"
REFERENCES,0.34552102376599636,"σ2 ΦRΛRΦT
R)−1fR(x)=

I−ΦR(σ2I+ΛRΦT
RΦR)−1ΛRΦT
R

ΦRµR
=ΦRµR−ΦR(σ2I+ΛRΦT
RΦR)−1ΛRΦT
RΦRµR
=ΦR(I+ 1"
REFERENCES,0.3458866544789762,"σ2 ΛRΦT
RΦR)−1µR. (65)"
REFERENCES,0.3462522851919561,"Let µR,1 =(µ0,0,...,0) and µR,2 =(0,µ1,...,µR). Then µR =µR,1+µR,2. Then we have"
REFERENCES,0.346617915904936,∥(I+ 1
REFERENCES,0.3469835466179159,"σ2 ΛRΦT
RΦR)−1µR∥2 =∥(I+ 1"
REFERENCES,0.3473491773308958,"σ2 ΛRΦT
RΦR)−1µR,1∥2+∥(I+ 1"
REFERENCES,0.34771480804387567,"σ2 ΛRΦT
RΦR)−1µR,2∥2.
(66)"
REFERENCES,0.34808043875685557,Published as a conference paper at ICLR 2022
REFERENCES,0.3484460694698355,"According to (62) in the proof of Lemma 29, we have ∥(I +
1
σ2 ΛRΦT
RΦR)−1µR,2∥2 =
˜O(nmax{−(1−t), (1−t)(1−2β)"
REFERENCES,0.3488117001828154,"2α
}). Next we estimate ∥(I+ 1"
REFERENCES,0.3491773308957952,"σ2 ΛRΦT
RΦR)−1µR,1∥2."
REFERENCES,0.3495429616087751,"Let
A=(I+ n"
REFERENCES,0.34990859232175503,"σ2 Λ1:R)−γ/2Λγ/2
1:R(ΦT
1:RΦ1:R−nI)Λγ/2
1:R(I+ n"
REFERENCES,0.35027422303473493,σ2 Λ1:R)−γ/2
REFERENCES,0.3506398537477148,"where
1
1−t( 1+α+2τ"
REFERENCES,0.3510054844606947,"2α
−(1+2τ+2α)t"
REFERENCES,0.3513711151736746,"2α
) < γ < 1. Since 1−
α
1+2τ < t < 1,
1
1−t( 1+α+2τ"
REFERENCES,0.3517367458866545,"2α
−(1+2τ+2α)t"
REFERENCES,0.3521023765996344,"2α
) < 1
so the range for γ is well-deﬁned.By Corollary 22, with probability of at least 1 −δ, we have ∥1"
REFERENCES,0.35246800731261424,"σ2 A∥2 = ˜O(
q log R"
REFERENCES,0.35283363802559414,"δ n
1+α+2τ"
REFERENCES,0.35319926873857405,"2α
−(1+2τ+2α)t"
REFERENCES,0.35356489945155395,"2α
−γ(1−t)) = o(1). When n is sufﬁciently large, ∥1"
REFERENCES,0.3539305301645338,"σ2 A∥2 is
less than 1 because 1−
α
1+2τ <t<1. By Lemma 27, we have (I+ 1"
REFERENCES,0.3542961608775137,"σ2 ΛRΦT
RΦR)−1"
REFERENCES,0.3546617915904936,=(I+ n
REFERENCES,0.3550274223034735,"σ2 ΛR)−1+ ∞
X"
REFERENCES,0.3553930530164534,"j=1
(−1)j  1"
REFERENCES,0.35575868372943326,σ2 (I+ n
REFERENCES,0.35612431444241316,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j(I+ n"
REFERENCES,0.35648994515539306,σ2 ΛR)−1.
REFERENCES,0.35685557586837297,We then have
REFERENCES,0.3572212065813528,∥(I+ 1
REFERENCES,0.3575868372943327,"σ2 ΛRΦT
RΦR)−1µR,1∥2 =  "
REFERENCES,0.3579524680073126,(I+ n
REFERENCES,0.3583180987202925,"σ2 ΛR)−1+ ∞
X"
REFERENCES,0.35868372943327237,"j=1
(−1)j  1"
REFERENCES,0.35904936014625227,σ2 (I+ n
REFERENCES,0.3594149908592322,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j(I+ n"
REFERENCES,0.3597806215722121,"σ2 ΛR)−1  µR,1 2 ≤ "
REFERENCES,0.360146252285192,∥(I+ n
REFERENCES,0.36051188299817183,"σ2 ΛR)−1µR,1∥2+ ∞
X j=1   1"
REFERENCES,0.36087751371115173,σ2 (I+ n
REFERENCES,0.36124314442413163,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j(I+ n"
REFERENCES,0.36160877513711154,"σ2 ΛR)−1µR,1

2  ."
REFERENCES,0.3619744058500914,"(67)
By Lemma 15,"
REFERENCES,0.3623400365630713,∥(I+ n
REFERENCES,0.3627056672760512,"σ2 ΛR)−1µR,1∥2 ≤"
REFERENCES,0.3630712979890311,"v
u
u
tµ2
0+ R
X p=1"
REFERENCES,0.36343692870201094,C2µp−2β
REFERENCES,0.36380255941499084,"(1+nCλp−α/σ2)2 =O(1).
(68)"
REFERENCES,0.36416819012797075,"Let ˜Λ1,R = diag{1, λ1, ... , λR} and I0,R = (0, 1, ... , 1). Then ΛR = ˜Λ1,RI0,R. Let B = (I +
n
σ2 ΛR)−γ/2˜Λγ/2
1,R(ΦT
RΦR −nI)˜Λγ/2
1,R(I + n"
REFERENCES,0.36453382084095065,"σ2 ΛR)−γ/2. According to Corollary 23, we have ∥B∥2 = O
 q log R"
REFERENCES,0.36489945155393055,"δ n
1
2 
. Using the fact that ∥1"
REFERENCES,0.3652650822669104,"σ2 A∥2 = ˜O
 q log R"
REFERENCES,0.3656307129798903,"δ n
1+α+2τ"
REFERENCES,0.3659963436928702,"2α
−(1+2τ+2α)t"
REFERENCES,0.3663619744058501,"2α
−γ(1−t)
, we have

  1"
REFERENCES,0.36672760511882996,σ2 (I+ n
REFERENCES,0.36709323583180986,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j(I+ n"
REFERENCES,0.36745886654478976,"σ2 ΛR)−1µR,1

2 = 1 σ2j (I+ n"
REFERENCES,0.36782449725776967,σ2 ΛR)−1+ γ
REFERENCES,0.36819012797074957,"2 Λ
1−γ"
R,0.3685557586837294,"2
R

A(I+ n"
R,0.3689213893967093,"σ2 ΛR)−1+γΛ1−γ
R
j−1
B(I+ n"
R,0.3692870201096892,σ2 ΛR)−1+ γ
R,0.3696526508226691,"2 µR,1 2 ≤1"
R,0.370018281535649,σ2 (n(−1+ γ
R,0.3703839122486289,"2 +(−1+γ)(j−1))(1−t) ˜O(
q log R"
R,0.3707495429616088,δ n(j−1)( 1+α+2τ
R,0.3711151736745887,"2α
−(1+2τ+2α)t"
R,0.37148080438756853,"2α
−γ(1−t)))
q log R"
R,0.37184643510054843,"δ n
1
2 ∥µR,1∥2"
R,0.37221206581352834,≤n(−1+ γ
R,0.37257769652650824,2 )(1−t)+ 1
R,0.37294332723948814,2 −t ˜O(n
R,0.373308957952468,[1−α+2τ−(1+2τ)t](j−1)
R,0.3736745886654479,"2α
)
q log R"
R,0.3740402193784278,"δ ∥µR,1∥2"
R,0.3744058500914077,= ˜O(n−1 2 + γ
R,0.37477148080438755,2 (1−t)+ [1−α+2τ−(1+2τ)t](j−1)
R,0.37513711151736745,"2α
).
(69)
Since
1
1−t( 1+α+2τ"
R,0.37550274223034735,"2α
−(1+2τ+2α)t"
R,0.37586837294332726,"2α
)<γ <1 and −1"
R,0.3762340036563071,"2 +
1
1−t( 1+α+2τ"
R,0.376599634369287,"2α
−(1+2τ+2α)t"
R,0.3769652650822669,"2α
) 1−t"
R,0.3773308957952468,"2 <0, we can let"
R,0.3776965265082267,"γ be a little bit larger than
1
1−t( 1+α+2τ"
R,0.37806215722120656,"2α
−(1+2τ+2α)t"
R,0.37842778793418647,"2α
) and make −1 2 + γ"
R,0.37879341864716637,"2 (1−t) < 0 holds. By (67),
(68), (69), we have
∥(I+ 1"
R,0.37915904936014627,"σ2 ΛRΦT
RΦR)−1µR,1∥2"
R,0.3795246800731261,"≤O(1)+ ∞
X"
R,0.379890310786106,"j=1
˜O(n−1 2 + γ"
R,0.3802559414990859,"2 (1−t)+ [1−α+2τ−(1+2τ)t](j−1) 2α
)"
R,0.38062157221206583,≤O(1)+o(1)=O(1). (70)
R,0.38098720292504573,Published as a conference paper at ICLR 2022
R,0.3813528336380256,"According to (66), we have ∥(I + 1"
R,0.3817184643510055,"σ2 ΛRΦT
RΦR)−1µR∥2 = ˜O(nmax{−(1−t), (1−t)(1−2β)"
R,0.3820840950639854,"2α
}) + O(1) =
O(1). By Corollary 20, with probability of at least 1−δ, we have"
R,0.3824497257769653,∥ΦR(I+ 1
R,0.38281535648994514,"σ2 ΛRΦT
RΦR)−1µR∥2 = ˜O(
q ( 1"
R,0.38318098720292504,δ +1)n∥(I+ 1
R,0.38354661791590494,"σ2 ΛRΦT
RΦR)−1µR∥2)"
R,0.38391224862888484,"= ˜O
q ( 1"
R,0.3842778793418647,"δ +1)n

."
R,0.3846435100548446,"From (65), we get ∥(I+ 1"
R,0.3850091407678245,"σ2 ΦRΛRΦT
R)−1fR(x)∥2 = ˜O
q ( 1"
R,0.3853747714808044,"δ +1)n

. This concludes the proof."
R,0.3857404021937843,"Lemma 31. Assume that σ2 = Θ(1). Let R = n
1
α +κ where 0 < κ < α−1−2τ"
R,0.38610603290676415,"α2
. Assume that µ0 = 0.
Then when n is sufﬁciently large, with probability of at least 1−3δ we have"
R,0.38647166361974405,∥(I+ ΦΛΦT
R,0.38683729433272396,"σ2
)−1fR(x)∥2 = ˜O(
q ( 1"
R,0.38720292504570386,"δ +1)n·nmax{−1, 1−2β"
R,0.3875685557586837,"2α }).
(71)"
R,0.3879341864716636,"Assume that µ0 >0. Then when n is sufﬁciently large, with probability of at least 1−3δ we have"
R,0.3882998171846435,∥(I+ ΦΛΦT
R,0.3886654478976234,"σ2
)−1fR(x)∥2 = ˜O(
q ( 1"
R,0.38903107861060326,"δ +1)n).
(72)"
R,0.38939670932358317,Proof of Lemma 31. We have
R,0.38976234003656307,(I+ ΦΛΦT
R,0.390127970749543,"σ2
)−1fR(x)"
R,0.3904936014625229,"=(I+ ΦRΛRΦT
R
σ2
)−1fR(x)+

(I+ ΦΛΦT"
R,0.3908592321755027,"σ2
)−1−(I+ ΦRΛRΦT
R
σ2
)−1
fR(x).
(73)"
R,0.3912248628884826,"When µ0 =0, by Lemma 29, with probability of at least 1−2δ, we have"
R,0.39159049360146253,∥(I+ 1
R,0.39195612431444243,"σ2 ΦRΛRΦT
R)−1fR(x)∥2 = ˜O(
q ( 1"
R,0.3923217550274223,"δ +1)n·nmax{−1, 1−2β"
R,0.3926873857404022,2α }).
R,0.3930530164533821,Since α−1−2τ
R,0.393418647166362,"α2
< α−1−2τ"
R,0.39378427787934184,"α(1+2τ), we apply Lemma 28 and Corollary 26 and get that with probability of at
least 1−δ, the second term in the right hand side of (73) is estimated as follows:"
R,0.39414990859232174,"∥

(I+ ΦΛΦT"
R,0.39451553930530164,"σ2
)−1−(I+ ΦRΛRΦT
R
σ2
)−1
fR(x)∥2 =∥ ∞
X"
R,0.39488117001828155,"j=1
(−1)j
(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
j
(I+ ΦRΛRΦT
R
σ2
)−1fR(x)∥2 = ∞
X j=1"
R,0.39524680073126145,"
(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2

j"
R,0.3956124314442413,"2∥(I+ ΦRΛRΦT
R
σ2
)−1fR(x)∥2 = ∞
X"
R,0.3959780621572212,"j=1
˜O(n−jκα) ˜O(
q ( 1"
R,0.3963436928702011,"δ +1)n·nmax{−1, 1−2β 2α
}) =o(
q ( 1"
R,0.396709323583181,"δ +1)n·nmax{−1, 1−2β"
R,0.39707495429616085,"2α
})."
R,0.39744058500914076,"Overall, from (73), we have that with probability 1−3δ,"
R,0.39780621572212066,∥(I+ ΦΛΦT
R,0.39817184643510056,"σ2
)−1fR(x)∥2 = ˜O(
q ( 1"
R,0.39853747714808047,"δ +1)n·nmax{−1, 1−2β"
R,0.3989031078610603,"2α
})."
R,0.3992687385740402,"When µ0 >0, using the same approach and Lemma 30, we can prove that ∥(I+ ΦΛΦT"
R,0.3996343692870201,"σ2
)−1fR(x)∥2 =
˜O(
q ( 1"
R,0.4,δ +1)n). This concludes the proof.
R,0.40036563071297987,"D
PROOF OF THE MAIN RESULTS"
R,0.4007312614259598,"D.1
PROOFS RELATED TO THE ASYMPTOTICS OF THE NORMALIZED STOCHASTIC COMPLEXITY"
R,0.4010968921389397,"Lemma 32. Under Assumptions 4, 5 and 6, with probability of at least 1−2δ we have, we have"
R,0.4014625228519196,"|T1,R(Dn)−T1(Dn)|= ˜O

1
σ2 (nR1−α+n1/2R1−α+τ +R1−α+2τ)

(74)"
R,0.4018281535648994,Published as a conference paper at ICLR 2022
R,0.40219378427787933,"If R = n
1
α +κ where κ > 0, we have |T1,R(Dn)−T1(Dn)| = o

1
σ2 n
1
α

. If we further assume that"
R,0.40255941499085923,0<κ< α−1−2τ
R,0.40292504570383914,"α2
, µ0 =0 and σ2 =Θ(1), then for sufﬁciently large n with probability of at least 1−4δ
we have"
R,0.40329067641681904,"|T2,R(Dn)−T2(Dn)|= ˜O

( 1"
R,0.4036563071297989,δ +1)nmax{( 1
R,0.4040219378427788,α +κ) 1−2β
R,0.4043875685557587,"2
,1+ 1−2β"
R,0.4047531992687386,"α
+ (1−2β)κ"
R,0.40511882998171844,"2
,−1−κα,1+ 1−2β"
R,0.40548446069469835,"α
−κα}
.
(75)"
R,0.40585009140767825,"Proof of Lemma 32. Deﬁne Φ>R
=
(φR+1(x), φR+2(x), ... , φp(x), ...),
and Λ>R
=
diag(λR+1,...,λp,...). We then have"
R,0.40621572212065815,"|T1(Dn)−T1,R(Dn)|=

1
2logdet(I+ 1"
R,0.406581352833638,σ2 ΦΛΦT )−1
R,0.4069469835466179,2logdet(I+ 1
R,0.4073126142595978,"σ2 ΦRΛRΦT
R) + 1 2"
R,0.4076782449725777,Tr(I+ ΦΛΦT
R,0.4080438756855576,"σ2
)−1−Tr(I+ ΦRΛRΦT
R
σ2
)−1
.
(76)"
R,0.40840950639853746,"As for the ﬁrst term in the right hand side of (76), we have

1
2logdet(I+ 1"
R,0.40877513711151736,σ2 ΦΛΦT )−1
R,0.40914076782449726,2logdet(I+ 1
R,0.40950639853747717,"σ2 ΦRΛRΦT
R)"
R,0.409872029250457,"=

1
2logdet

(I+ 1"
R,0.4102376599634369,"σ2 ΦRΛRΦT
R)−1(I+ 1"
R,0.4106032906764168,"σ2 ΦRΛRΦT
R+ 1"
R,0.4109689213893967,"σ2 Φ>RΛ>RΦT
>R)
"
R,0.4113345521023766,"=

1
2logdet

I+ 1"
R,0.4117001828153565,σ2 (I+ 1
R,0.4120658135283364,"σ2 ΦRΛRΦT
R)−1Φ>RΛ>RΦT
>R  =1 2"
R,0.4124314442413163,"Trlog

I+ 1"
R,0.4127970749542962,σ2 (I+ 1
R,0.41316270566727603,"σ2 ΦRΛRΦT
R)−1Φ>RΛ>RΦT
>R"
R,0.41352833638025593,. (77)
R,0.41389396709323584,"Given a concave function h and a matrix B ∈Rn×n whose eigenvalues ζ1,...,ζn are all positive, we
have that
Trh(B)=Pn
p=1h(ζi)≤nh( 1"
R,0.41425959780621574,"n
Pn
p=1ζi)≤nh( 1"
R,0.4146252285191956,"nTrB),
(78)
where we used Jensen’s inequality. Using h(x)=log(1+x) in (78), with probability 1−δ, we have
 1"
R,0.4149908592321755,2logdet(I+ 1
R,0.4153564899451554,σ2 ΦΛΦT )−1
R,0.4157221206581353,2logdet(I+ 1
R,0.4160877513711152,"σ2 ΦRΛRΦT
R) ≤n"
R,0.41645338208409505,2 log(1+ 1
R,0.41681901279707495,nTr( 1
R,0.41718464351005485,"σ2 (I+ ΦRΛRΦT
R
σ2
)−1Φ>RΛ>RΦT
>R)) ≤n"
R,0.41755027422303476,"2 log(1+
1
nσ2 ∥(I+ ΦRΛRΦT
R
σ2
)−1∥2Tr(Φ>RΛ>RΦT
>R)) ≤n"
R,0.4179159049360146,"2 log(1+
1
nσ2
P∞
p=R+1λp∥φp(x)∥2
2)≤
1
2σ2
P∞
p=R+1λp∥φp(x)∥2
2"
R,0.4182815356489945,"=
1
2σ2
P∞
p=R+1λp

C2
φ ˜O
p"
R,0.4186471663619744,"p2τn∥φp∥2
2+p2τ
+n∥φp∥2
2
"
R,0.4190127970749543,= ˜O( 1
R,0.41937842778793416,"σ2 nP∞
p=R+1λp+n1/2P∞
p=R+1λppτ +P∞
p=R+1λpp2τ)"
R,0.41974405850091406,"= ˜O

1
σ2 (nR1−α+n1/2R1−α+τ +R1−α+2τ)

=o

1
σ2 n
1
α

, (79)"
R,0.42010968921389397,"where in the second inequality we use the fact that TrAB ≤∥A∥2TrB when A and B are symmetric
positive deﬁnite matrices, and in the last inequality we use Lemma 18."
R,0.42047531992687387,"As for the second term in the right hand side of (76), let A=(I+ ΦRΛRΦT
R
σ2
)−1/2. Then we have"
R,0.4208409506398538,"1
2
Tr(I+ ΦΛΦT"
R,0.4212065813528336,"σ2
)−1−Tr(I+ ΦRΛRΦT
R
σ2
)−1 = 1 2"
R,0.4215722120658135,"TrA

I−(I+A(Φ>RΛ>RΦT
>R
σ2
)A)−1

A ≤1"
"TR
H",0.4219378427787934,"2Tr
h
I−(I+A(
Φ>RΛ>RΦT
>R
σ2
)A)−1i ≤n"
"TR
H",0.42230347349177333,2 (1−(1+ 1
"TR
H",0.4226691042047532,"nTrA(
Φ>RΛ>RΦT
>R
σ2
)A)−1)≤n"
"TR
H",0.4230347349177331,2 (1−(1+ 1
"TR
H",0.423400365630713,"nTr(
Φ>RΛ>RΦT
>R
σ2
))−1) ≤n"
"TR
H",0.4237659963436929,"2 (1−(1+
1
nσ2
P∞
p=R+1λp∥φp(x)∥2
2))−1)≤
1
2σ2
P∞
p=R+1λp∥φp(x)∥2
2"
"TR
H",0.42413162705667273,"= ˜O

1
σ2 (nR1−α+n1/2R1−α+τ +R1−α+2τ)

=o

1
σ2 n
1
α

,"
"TR
H",0.42449725776965264,Published as a conference paper at ICLR 2022
"TR
H",0.42486288848263254,"where in the ﬁrst inequality we use the fact that ∥A∥2 < 1 and TrABA ≤∥A∥2
2TrB when A and B
are symmetric positive deﬁnite matrices, in the second inequality we use h(x)=1−1/(1+x) in (78)
and in the last equality we use the last few steps of (79). This concludes the proof of the ﬁrst statement."
"TR
H",0.42522851919561244,"As for |T2(Dn)−T2,R(Dn)|, we have"
"TR
H",0.42559414990859235,"|T2(Dn)−T2,R(Dn)|=
f(x)T (I+ ΦΛΦT"
"TR
H",0.4259597806215722,"σ2
)−1f(x)−fR(x)T (I+ ΦΛΦT"
"TR
H",0.4263254113345521,"σ2
)−1fR(x)"
"TR
H",0.426691042047532,"+
fR(x)T (I+ ΦΛΦT"
"TR
H",0.4270566727605119,"σ2
)−1fR(x)−fR(x)T (I+ ΦRΛRΦT
R
σ2
)−1fR(x)
.
(80)"
"TR
H",0.42742230347349175,"For the ﬁrst term on the right-hand side of (80), we have
f(x)T (I+ ΦΛΦT"
"TR
H",0.42778793418647165,"σ2
)−1f(x)−fR(x)T (I+ ΦΛΦT"
"TR
H",0.42815356489945156,"σ2
)−1fR(x)"
"TR
H",0.42851919561243146,"≤2
f>R(x)T (I+ ΦΛΦT"
"TR
H",0.42888482632541136,"σ2
)−1fR(x)
+
f>R(x)T (I+ ΦΛΦT"
"TR
H",0.4292504570383912,"σ2
)−1f>R(x)"
"TR
H",0.4296160877513711,≤2∥f>R(x)∥2∥(I+ ΦΛΦT
"TR
H",0.429981718464351,"σ2
)−1fR(x)∥2+∥f>R(x)∥2∥(I+ ΦΛΦT"
"TR
H",0.4303473491773309,"σ2
)−1∥2∥f>R(x)∥2"
"TR
H",0.43071297989031077,≤2∥f>R(x)∥2∥(I+ ΦΛΦT
"TR
H",0.43107861060329067,"σ2
)−1fR(x)∥2+∥f>R(x)∥2
2."
"TR
H",0.43144424131627057,"Applying Corollary 19 and Lemma 31, with probability of at least 1−4δ, we have
f(x)T (I+ ΦΛΦT"
"TR
H",0.4318098720292505,"σ2
)−1f(x)−fR(x)T (I+ ΦΛΦT"
"TR
H",0.4321755027422303,"σ2
)−1fR(x)"
"TR
H",0.4325411334552102,"≤2 ˜O
q ( 1"
"TR
H",0.43290676416819013,"δ +1)nR1−2β

˜O(
q ( 1"
"TR
H",0.43327239488117003,"δ +1)n·nmax{−1, 1−2β"
"TR
H",0.43363802559414993,"2α
})+ ˜O(( 1"
"TR
H",0.4340036563071298,δ +1)nR1−2β)
"TR
H",0.4343692870201097,"=2 ˜O

( 1"
"TR
H",0.4347349177330896,δ +1)n1+( 1
"TR
H",0.4351005484460695,α +κ) 1−2β
"TR
H",0.43546617915904934,"2
+max{−1, 1−2β"
"TR
H",0.43583180987202924,"2α
}

+ ˜O(( 1"
"TR
H",0.43619744058500914,δ +1)n1+( 1
"TR
H",0.43656307129798905,α +κ)(1−2β))
"TR
H",0.4369287020109689,"=2 ˜O

( 1"
"TR
H",0.4372943327239488,δ +1)n1+( 1
"TR
H",0.4376599634369287,α +κ) 1−2β
"TR
H",0.4380255941499086,"2
+max{−1, 1−2β"
"TR
H",0.4383912248628885,"2α
}

,"
"TR
H",0.43875685557586835,where the last equality holds because ( 1
"TR
H",0.43912248628884826,α +κ) 1−2β
"TR
H",0.43948811700182816,"2
< 1−2β"
"TR
H",0.43985374771480806,"2α
when κ>0."
"TR
H",0.4402193784277879,"As for the second term on the right-hand side of (80), according to Lemma 28, Corollary 26 and
Lemma 29, we have
fR(x)T (I+ ΦΛΦT"
"TR
H",0.4405850091407678,"σ2
)−1fR(x)−fR(x)T (I+ ΦRΛRΦT
R
σ2
)−1fR(x) =  ∞
X"
"TR
H",0.4409506398537477,"j=1
(−1)jfR(x)T 
(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2
j
(I+ ΦRΛRΦT
R
σ2
)−1fR(x)  ≤ ∞
X"
"TR
H",0.4413162705667276,"j=1
∥(I+ ΦRΛRΦT
R
σ2
)−1∥j−1
2
·∥Φ>RΛ>RΦT
>R
σ2
∥j
2·∥(I+ ΦRΛRΦT
R
σ2
)−1fR(x)∥2
2 = ∞
X"
"TR
H",0.4416819012797075,"j=1
˜O(n−jκα) ˜O(( 1"
"TR
H",0.44204753199268737,"δ +1)n1+max{−2, 1−2β α
})"
"TR
H",0.4424131627056673,= ˜O(( 1
"TR
H",0.4427787934186472,"δ +1)n1+max{−2, 1−2β"
"TR
H",0.4431444241316271,"α
}−κα). (81)"
"TR
H",0.4435100548446069,"By (80), we have"
"TR
H",0.44387568555758683,"|T2(Dn)−T2,R(Dn)|= ˜O

( 1"
"TR
H",0.44424131627056673,δ +1)n1+( 1
"TR
H",0.44460694698354664,α +κ) 1−2β
"TR
H",0.4449725776965265,"2
+max{−1, 1−2β"
"TR
H",0.4453382084095064,"2α
}

+ ˜O

( 1"
"TR
H",0.4457038391224863,"δ +1)n1+max{−2, 1−2β"
"TR
H",0.4460694698354662,"α
}−κα
"
"TR
H",0.4464351005484461,"= ˜O

( 1"
"TR
H",0.44680073126142594,δ +1)nmax{( 1
"TR
H",0.44716636197440585,α +κ) 1−2β
"TR
H",0.44753199268738575,"2
,1+ 1−2β"
"TR
H",0.44789762340036565,"α
+ (1−2β)κ"
"TR
H",0.4482632541133455,"2
,−1−κα,1+ 1−2β"
"TR
H",0.4486288848263254,"α
−κα}

."
"TR
H",0.4489945155393053,This concludes the proof of the second statement.
"TR
H",0.4493601462522852,"In Lemma 32, we gave a bound for |T2,R(Dn)−T2(Dn)| when n
1
α < R < n
1
α + α−1−2τ"
"TR
H",0.44972577696526506,"α2
. For R > n,
we note the following lemma:"
"TR
H",0.45009140767824496,Published as a conference paper at ICLR 2022
"TR
H",0.45045703839122486,"Lemma 33. Let R = nC and σ2 = nt. Assume that C ≥1 and C(1 −α + 2τ) −t < 0. Under
Assumptions 4, 5 and 6, for sufﬁciently large n and with probability of at least 1−3δ we have"
"TR
H",0.45082266910420477,"|T2,R(Dn)−T2(Dn)|= ˜O

( 1"
"TR
H",0.45118829981718467,δ +1) 1
"TR
H",0.4515539305301645,"σ2 nRmax{1/2−β,1−α+2τ}
.
(82)"
"TR
H",0.4519195612431444,"Proof of Lemma 33. Deﬁne Φ>R
=
(φR+1(x), φR+2(x), ... , φp(x), ...),
and Λ>R
=
diag(λR+1,...,λp,...). Then we have"
"TR
H",0.4522851919561243,"|T2(Dn)−T2,R(Dn)|=
f(x)T (I+ ΦΛΦT"
"TR
H",0.4526508226691042,"σ2
)−1f(x)−fR(x)T (I+ ΦΛΦT"
"TR
H",0.45301645338208407,"σ2
)−1fR(x)"
"TR
H",0.453382084095064,"+
fR(x)T (I+ ΦΛΦT"
"TR
H",0.4537477148080439,"σ2
)−1fR(x)−fR(x)T (I+ ΦRΛRΦT
R
σ2
)−1fR(x)
.
(83)"
"TR
H",0.4541133455210238,"For the ﬁrst term on the right-hand side of (83), with probability 1−3δ we have
f(x)T (I+ ΦΛΦT"
"TR
H",0.45447897623400363,"σ2
)−1f(x)−fR(x)T (I+ ΦΛΦT"
"TR
H",0.45484460694698353,"σ2
)−1fR(x)"
"TR
H",0.45521023765996343,"≤2
f>R(x)T (I+ ΦΛΦT"
"TR
H",0.45557586837294334,"σ2
)−1fR(x)
+
f>R(x)T (I+ ΦΛΦT"
"TR
H",0.45594149908592324,"σ2
)−1f>R(x)"
"TR
H",0.4563071297989031,≤2∥f>R(x)∥2∥(I+ ΦΛΦT
"TR
H",0.456672760511883,"σ2
)−1∥2∥fR(x)∥2+∥f>R(x)∥2∥(I+ ΦΛΦT"
"TR
H",0.4570383912248629,"σ2
)−1∥2∥f>R(x)∥2"
"TR
H",0.4574040219378428,"≤2∥f>R(x)∥2∥fR(x)∥2+∥f>R(x)∥2
2 ≤2 ˜O r (1"
"TR
H",0.45776965265082264,"δ +1)nR1−2β
!
˜O( r (1"
"TR
H",0.45813528336380255,δ +1)n·∥f∥2)+ ˜O((1
"TR
H",0.45850091407678245,δ +1)nR1−2β)
"TR
H",0.45886654478976235,"= ˜O

(1"
"TR
H",0.45923217550274226,"δ +1)nR1/2−β

,"
"TR
H",0.4595978062157221,where we used Corollary 19 and Lemma 17 for the last inequality.
"TR
H",0.459963436928702,The assumption C(1 −α + 2τ) −t < 0 means that R1−α+2τ
"TR
H",0.4603290676416819,"σ2
= o(1). For the second term on the
right-hand side of (83), by Lemmas 28 and 25, we have
fR(x)T (I+ ΦΛΦT"
"TR
H",0.4606946983546618,"σ2
)−1fR(x)−fR(x)T (I+ ΦRΛRΦT
R
σ2
)−1fR(x) =  ∞
X"
"TR
H",0.46106032906764166,"j=1
(−1)jfR(x)T

(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2"
"TR
H",0.46142595978062156,"j
(I+ ΦRΛRΦT
R
σ2
)−1fR(x)  ≤ ∞
X"
"TR
H",0.46179159049360147,"j=1
∥(I+ ΦRΛRΦT
R
σ2
)−1∥j+1
2
·∥Φ>RΛ>RΦT
>R
σ2
∥j
2·∥fR(x)∥2
2 = ∞
X"
"TR
H",0.46215722120658137,"j=1
˜O( 1"
"TR
H",0.4625228519195612,σ2 Rj(1−α+2τ)) ˜O((1
"TR
H",0.4628884826325411,"δ +1)n∥f∥2
2)"
"TR
H",0.463254113345521,= ˜O((1
"TR
H",0.4636197440585009,δ +1) 1
"TR
H",0.46398537477148083,σ2 nR1−α+2τ). (84)
"TR
H",0.4643510054844607,"Using (83), we have"
"TR
H",0.4647166361974406,"|T2(Dn)−T2,R(Dn)|= ˜O

(1"
"TR
H",0.4650822669104205,"δ +1)nR1/2−β

+ ˜O((1"
"TR
H",0.4654478976234004,δ +1)n 1
"TR
H",0.46581352833638023,σ2 R1−α+2τ)
"TR
H",0.46617915904936014,"= ˜O

(1"
"TR
H",0.46654478976234004,δ +1)n 1
"TR
H",0.46691042047531994,"σ2 Rmax{1/2−β,1−α+2τ}

."
"TR
H",0.4672760511882998,"Next we consider the asympototics of T1,R(Dn) and T2,R(Dn)."
"TR
H",0.4676416819012797,Published as a conference paper at ICLR 2022
"TR
H",0.4680073126142596,"Lemma 34. Let A = (I +
n
σ2 ΛR)−γ/2Λγ/2
R (ΦT
RΦR −nI)Λγ/2
R (I +
n
σ2 ΛR)−γ/2. Assume that
∥A∥2 <1 where 1+2τ"
"TR
H",0.4683729433272395,"α
<γ ≤1. Then we have"
"TR
H",0.4687385740402194,"T2,R(Dn)=
n
2σ2 µT
R(I+ n"
"TR
H",0.46910420475319925,σ2 ΛR)−1µR+ 1
"TR
H",0.46946983546617915,"2
P∞
j=1(−1)j+1Ej, where"
"TR
H",0.46983546617915906,"Ej =µT
R
1
σ2 (I+ n"
"TR
H",0.47020109689213896,"σ2 ΛR)−1(ΦT
RΦR−nI)
  1"
"TR
H",0.4705667276051188,σ2 (I+ n
"TR
H",0.4709323583180987,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j−1(I+ n"
"TR
H",0.4712979890310786,σ2 ΛR)−1µR.
"TR
H",0.4716636197440585,"Proof of Lemma 34. Let ˜Λϵ,R = diag{ϵ,λ1,...,λR}. Since ΛR = diag{0,λ1,...,λR}, we have that
when ϵ is sufﬁciently small, ∥1"
"TR
H",0.4720292504570384,σ2 (I + n
"TR
H",0.47239488117001827,"σ2 ˜Λϵ,R)−1/2˜Λ1/2
ϵ,R(ΦT
RΦR −nI)˜Λ1/2
ϵ,R(I + n"
"TR
H",0.47276051188299817,"σ2 ˜Λϵ,R)−1/2∥2 < 1.
Since all diagonal entries of ˜Λϵ,R are positive, we have
1
2σ2 µT
RΦT
R(I+ 1"
"TR
H",0.47312614259597807,"σ2 ΦR ˜Λϵ,RΦT
R)−1ΦRµR = 1"
"TR
H",0.473491773308958,"2σ2 µT
RΦT
R
h
I−ΦR(σ2I+ ˜Λϵ,RΦT
RΦR)−1˜Λϵ,RΦT
R
i
ΦRµR = 1"
"TR
H",0.4738574040219378,"2σ2 µT
RΦT
RΦRµR−1"
"TR
H",0.4742230347349177,"2σ2 µT
RΦT
RΦR(σ2I+ ˜Λϵ,RΦT
RΦR)−1˜Λϵ,RΦT
RΦRµR = 1"
"TR
H",0.47458866544789763,"2µT
RΦT
RΦR(σ2I+ ˜Λϵ,RΦT
RΦR)−1µR = 1"
"TR
H",0.47495429616087753,"2µT
R ˜Λ−1
ϵ,R ˜Λϵ,RΦT
RΦR(σ2I+ ˜Λϵ,RΦT
RΦR)−1µR = 1"
"TR
H",0.4753199268738574,"2µT
R ˜Λ−1
ϵ,RµR−1"
"TR
H",0.4756855575868373,"2µT
R ˜Λ−1
ϵ,R(I+ 1"
"TR
H",0.4760511882998172,"σ2 ˜Λϵ,RΦT
RΦR)−1µR. (85)"
"TR
H",0.4764168190127971,"Using Lemma 27, we have
1
2µT
R ˜Λ−1
ϵ,RµR−1"
"TR
H",0.476782449725777,"2µT
R ˜Λ−1
ϵ,R(I+ 1"
"TR
H",0.47714808043875684,"σ2 ˜Λϵ,RΦT
RΦR)−1µR = 1"
"TR
H",0.47751371115173674,"2µT
R ˜Λ−1
ϵ,RµR−1"
"TR
H",0.47787934186471664,"2µT
R ˜Λ−1
ϵ,R(I+ n"
"TR
H",0.47824497257769655,"σ2 ˜Λϵ,R)−1µR + 1 2 ∞
X"
"TR
H",0.4786106032906764,"j=1
(−1)j+1µT
R ˜Λ−1
ϵ,R  1"
"TR
H",0.4789762340036563,σ2 (I+ n
"TR
H",0.4793418647166362,"σ2 ˜Λϵ,R)−1˜Λϵ,R(ΦT
RΦR−nI)
j
(I+ n"
"TR
H",0.4797074954296161,"σ2 ˜Λϵ,R)−1µR = n"
"TR
H",0.48007312614259595,"2σ2 µT
R(I+ n"
"TR
H",0.48043875685557585,"σ2 ˜Λϵ,R)−1µR + 1 2 ∞
X"
"TR
H",0.48080438756855576,"j=1
(−1)j+1µT
R
1
σ2 (I+ n"
"TR
H",0.48117001828153566,"σ2 ˜Λϵ,R)−1(ΦT
RΦR−nI)
 1"
"TR
H",0.48153564899451556,σ2 (I+ n
"TR
H",0.4819012797074954,"σ2 ˜Λϵ,R)−1˜Λϵ,R(ΦT
RΦR−nI)
j−1 (I+ n"
"TR
H",0.4822669104204753,"σ2 ˜Λϵ,R)−1µR
(86)
Letting ϵ→0, we get"
"TR
H",0.4826325411334552,"T2,R(Dn)= 1"
"TR
H",0.4829981718464351,"2σ2 µT
RΦT
R(I+ 1"
"TR
H",0.48336380255941497,"σ2 ΦRΛRΦT
R)−1ΦRµR = n"
"TR
H",0.48372943327239487,"2σ2 µT
R(I+ n"
"TR
H",0.4840950639853748,"σ2 ΛR)−1µR + 1 2 ∞
X j=1"
"TR
H",0.4844606946983547,"
(−1)j+1µT
R
1
σ2 (I+ n"
"TR
H",0.4848263254113345,"σ2 ΛR)−1(ΦT
RΦR−nI)
 1"
"TR
H",0.4851919561243144,σ2 (I+ n
"TR
H",0.48555758683729433,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j−1 (I+ n"
"TR
H",0.48592321755027423,σ2 ΛR)−1µR 
"TR
H",0.48628884826325414,This concludes the proof.
"TR
H",0.486654478976234,"Lemma 35. Assume that σ2 =Θ(1). Let R =n
1
α +κ where 0<κ< α−1−2τ"
"TR
H",0.4870201096892139,"2α2
. Under Assumptions 4,
5 and 6, with probability of at least 1−δ, we have"
"TR
H",0.4873857404021938,"T1,R(Dn)=
  1"
"TR
H",0.4877513711151737,2logdet(I+ n
"TR
H",0.48811700182815354,σ2 ΛR)−1
TR,0.48848263254113344,"2Tr
 
I−(I+ n"
TR,0.48884826325411335,"σ2 ΛR)−1
(1+o(1))=Θ(n
1
α ).
(87)"
TR,0.48921389396709325,Published as a conference paper at ICLR 2022
TR,0.48957952468007315,"Furthermore, if we assume µ0 =0, we have"
TR,0.489945155393053,"T2,R(Dn)=
  n"
TR,0.4903107861060329,"2σ2 µT
R(I+ n"
TR,0.4906764168190128,"σ2 ΛR)−1µR

(1+o(1))="
TR,0.4910420475319927,"(
Θ(nmax{0,1+ 1−2β"
TR,0.49140767824497256,"α
}),
α̸=2β−1,
Θ(logn),
α=2β−1.
(88)"
TR,0.49177330895795246,Proof of Lemma 35. Let
TR,0.49213893967093236,A=(I+ n
TR,0.49250457038391227,"σ2 ΛR)−γ/2Λγ/2
R (ΦT
RΦR−nI)Λγ/2
R (I+ n"
TR,0.4928702010968921,"σ2 ΛR)−γ/2,
(89)"
TR,0.493235831809872,where 1+α+2τ
TR,0.4936014625228519,"2α
<γ ≤1. By Corollary 22, with probability of at least 1−δ, we have"
TR,0.4939670932358318,"∥A∥2 = ˜O(n
1−2γα+α+2τ"
TR,0.4943327239488117,"2α
).
(90)"
TR,0.4946983546617916,"When n is sufﬁciently large, ∥A∥2 is less than 1. Let B =(I+ n"
TR,0.4950639853747715,"σ2 ΛR)−1/2Λ1/2
R (ΦT
RΦR−nI)Λ1/2
R (I+"
TR,0.4954296160877514,"n
σ2 ΛR)−1/2. Then ∥B∥2 = σ2(1−γ)"
TR,0.4957952468007313,"n1−γ ∥A∥2 = ˜O(n
1−α+2τ"
TR,0.49616087751371113,"2α
). Using the Woodbury matrix identity, we
compute T1,R(Dn) as follows:"
TR,0.49652650822669103,"T1,R(Dn)= 1"
TR,0.49689213893967094,2logdet(I+ 1
TR,0.49725776965265084,"σ2 ΛRΦT
RΦR)−1"
TR,0.4976234003656307,"2TrΦR(σ2I+ΛRΦT
RΦR)−1ΛRΦT
R = 1"
TR,0.4979890310786106,2logdet(I+ n
TR,0.4983546617915905,σ2 ΛR)+ 1
TR,0.4987202925045704,2logdet[I+ 1
TR,0.4990859232175503,σ2 (I+ n
TR,0.49945155393053015,"σ2 ΛR)−1/2Λ1/2
R (ΦT
RΦR−nI)Λ1/2
R (I+ n"
TR,0.49981718464351005,σ2 ΛR)−1/2] −1
TR,0.5001828153564899,"2Tr(σ2I+ΛΦT
RΦR)−1ΛΦT
RΦR
= 1"
TR,0.5005484460694698,2logdet(I+ n
TR,0.5009140767824497,σ2 ΛR)+ 1
TR,0.5012797074954296,2Trlog[I+ 1
TR,0.5016453382084095,σ2 B]−1
TR,0.5020109689213894,"2Tr(I−σ2(σ2I+ΛΦT
RΦR)−1)) = 1"
TR,0.5023765996343693,2logdet(I+ n
TR,0.5027422303473492,σ2 ΛR)+ 1
TR,0.5031078610603291,"2Tr ∞
X j=1"
TR,0.5034734917733089,"(−1)j−1 j
( 1"
TR,0.5038391224862888,σ2 B)j −1
TR,0.5042047531992687,2Tr 
TR,0.5045703839122486,I−(I+ n
TR,0.5049360146252285,"σ2 ΛR)−1+ ∞
X"
TR,0.5053016453382084,"j=1
(−1)j  1"
TR,0.5056672760511883,σ2 (I+ n
TR,0.5060329067641682,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j(I+ n"
TR,0.506398537477148,"σ2 ΛR)−1   =
  1"
TR,0.5067641681901279,2logdet(I+ n
TR,0.5071297989031078,σ2 ΛR)−1
TR,0.5074954296160877,"2Tr
 
I−(I+ n"
TR,0.5078610603290676,"σ2 ΛR)−1
+ 1"
TR,0.5082266910420475,"2Tr ∞
X j=1"
TR,0.5085923217550274,"(−1)j−1 j
( 1"
TR,0.5089579524680073,σ2 B)j −1
TR,0.5093235831809872,"2Tr  
∞
X"
TR,0.509689213893967,"j=1
(−1)j
1
σ2j (I+ n"
TR,0.5100548446069469,σ2 ΛR)−1/2Bj(I+ n
TR,0.5104204753199268,"σ2 ΛR)−1/2  ,"
TR,0.5107861060329068,"(91)
where in the last equality we apply Lemma 27."
TR,0.5111517367458867,"Let h(x)=log(1+x)−(1−
1
1+x). It is easy to verify that h(x) is increasing on [0,+∞). As for the
ﬁrst term on the right hand side of (91), we have"
TR,0.5115173674588666,"1
2logdet(I+ n"
TR,0.5118829981718465,σ2 ΛR)−1
TR,0.5122486288848264,"2Tr
 
I−(I+ n"
TR,0.5126142595978063,"σ2 ΛR)−1 = 1 2 R
X p=1"
TR,0.5129798903107861,"
log(1+ n"
TR,0.513345521023766,"σ2 λp)−(1−
1
1+ n"
TR,0.5137111517367459,"σ2 λp )
 = 1 2 R
X"
TR,0.5140767824497258,"p=1
h( n"
TR,0.5144424131627057,"σ2 λp)≤1 2 R
X"
TR,0.5148080438756856,"p=1
h( n"
TR,0.5151736745886655,σ2 Cλp−α) ≤1 2h( n
TR,0.5155393053016454,σ2 Cλ)+ 1 2 Z
TR,0.5159049360146253,"[1,R]
h( n"
TR,0.5162705667276051,σ2 Cλx−α)dx = 1 2h( n
TR,0.516636197440585,σ2 Cλ)+ 1
TR,0.5170018281535649,"2n1/α
Z"
TR,0.5173674588665448,"[1/n1/α,R/n1/α]
h( Cλ"
TR,0.5177330895795247,σ2 x−α)dx
TR,0.5180987202925046,"=Θ(n1/α),"
TR,0.5184643510054845,Published as a conference paper at ICLR 2022
TR,0.5188299817184644,"where in the last equality we use the fact that
R"
TR,0.5191956124314442,"[0,+∞]h(x−α)dx<∞. On the other hand, we have"
TR,0.5195612431444241,"1
2logdet(I+ n"
TR,0.519926873857404,σ2 ΛR)−1
TR,0.5202925045703839,"2Tr
 
I−(I+ n"
TR,0.5206581352833638,"σ2 ΛR)−1 = 1 2 R
X"
TR,0.5210237659963437,"p=1
h( n"
TR,0.5213893967093236,"σ2 λp)≥1 2 R
X"
TR,0.5217550274223035,"p=1
h( n"
TR,0.5221206581352834,σ2 Cλp−α) ≥1 2 Z
TR,0.5224862888482632,"[1,R+1]
h( n"
TR,0.5228519195612431,σ2 Cλx−α)dx = 1
TR,0.523217550274223,"2n1/α
Z"
TR,0.5235831809872029,"[1/n1/α,(R+1)/n1/α]
h( 1"
TR,0.5239488117001828,σ2 Cλx−α)dx
TR,0.5243144424131627,=Θ(n1/α).
TR,0.5246800731261426,"Overall, we have 1"
TR,0.5250457038391225,2logdet(I+ n
TR,0.5254113345521024,σ2 ΛR)−1
TR,0.5257769652650822,"2Tr
 
I−(I+ n"
TR,0.5261425959780621,"σ2 ΛR)−1
=Θ(n1/α)."
TR,0.526508226691042,"As for the second term on the right hand side of (91), we have

Tr ∞
X j=1"
TR,0.5268738574040219,"(−1)j−1 j
( 1"
TR,0.5272394881170018,"σ2 B)j ≤R ∞
X"
TR,0.5276051188299817,"j=1
∥1"
TR,0.5279707495429616,"σ2 B∥j
2 =R ∞
X j=1"
TR,0.5283363802559415,"1
σ2j ˜O(n"
TR,0.5287020109689214,"j(1−α+2τ) 2α
)"
TR,0.5290676416819012,"=R ˜O(n
1−α+2τ"
TR,0.5294332723948811,"2α
)= ˜O(n
1
α +κ+ 1−α+2τ 2α
)."
TR,0.529798903107861,"As for the third term on the right hand side of (91), we have

Tr  
∞
X"
TR,0.5301645338208409,"j=1
(−1)j
1
σ2j (I+ n"
TR,0.5305301645338208,σ2 ΛR)−1/2Bj(I+ n
TR,0.5308957952468008,"σ2 ΛR)−1/2    ≤ ∞
X j=1"
TR,0.5312614259597807,"Tr

1
σ2j (I+ n"
TR,0.5316270566727606,σ2 ΛR)−1/2Bj(I+ n
TR,0.5319926873857403,"σ2 ΛR)−1/2 ≤R ∞
X j=1 1"
TR,0.5323583180987203,σ2j (I+ n
TR,0.5327239488117002,σ2 ΛR)−1/2Bj(I+ n
TR,0.5330895795246801,"σ2 ΛR)−1/2
2 ≤R ∞
X j=1 1"
TR,0.53345521023766,σ2j (I+ n
TR,0.5338208409506399,σ2 ΛR)−1/2Bj(I+ n
TR,0.5341864716636198,"σ2 ΛR)−1/2
2 ≤R ∞
X j=1 1"
TR,0.5345521023765997,"σ2j Bj
2 = ˜O(n
1
α +κ+ 1−α+2τ 2α
)."
TR,0.5349177330895796,"Then the asymptotics of T1,R(Dn) is given by"
TR,0.5352833638025594,"T1,R(Dn)= 1"
TR,0.5356489945155393,2logdet(I+ n
TR,0.5360146252285192,σ2 ΛR)−1
TR,0.5363802559414991,"2Tr
 
I−(I+ n"
TR,0.536745886654479,"σ2 ΛR)−1
+ ˜O(n
1
α +κ+ 1−α+2τ"
TR,0.5371115173674589,"2α
)+ ˜O(n
1
α +κ+ 1−α+2τ 2α
)"
TR,0.5374771480804388,"=Θ(n1/α)+ ˜O(n
1
α +κ+ 1−α+2τ 2α
)"
TR,0.5378427787934187,"=Θ(n
1
α ),"
TR,0.5382084095063986,where in the last inequality we use the assumption that κ< α−1−2τ
TR,0.5385740402193784,"2α
. Since ˜O(n
1
α +κ+ 1−α+2τ"
TR,0.5389396709323583,"2α
) is lower
order term compared to Θ(n
1
α ), we further have"
TR,0.5393053016453382,"T1,R(Dn)=
  1"
TR,0.5396709323583181,2logdet(I+ n
TR,0.540036563071298,σ2 ΛR)−1
TR,0.5404021937842779,"2Tr
 
I−(I+ n"
TR,0.5407678244972578,"σ2 ΛR)−1
(1+o(1))."
TR,0.5411334552102377,This concludes the proof of the ﬁrst statement.
TR,0.5414990859232176,"Let Λ1:R =diag{λ1,...,λR}, Φ1:R =(φ1(x),φ1(x),...,φR(x)) and µ1:R =(µ1,...,µR). Since µ0 =0,
we have T2,R(Dn)=
1
2σ2 µT
1:RΦT
1:R(I+ 1"
TR,0.5418647166361974,"σ2 Φ1:RΛ1:RΦT
1:R)−1Φ1:Rµ1:R. According to Lemma 34, we"
TR,0.5422303473491773,Published as a conference paper at ICLR 2022
TR,0.5425959780621572,"have
T2,R(Dn)= n"
TR,0.5429616087751371,"2σ2 µT
1:R(I+ n"
TR,0.543327239488117,"σ2 Λ1:R)−1µ1:R + 1 2 ∞
X"
TR,0.5436928702010969,"j=1
(−1)j+1µT
1:R
1
σ2 (I+ n"
TR,0.5440585009140768,"σ2 Λ1:R)−1(ΦT
1:RΦ1:R−nI)
 1"
TR,0.5444241316270567,σ2 (I+ n
TR,0.5447897623400365,"σ2 Λ1:R)−1Λ1:R(ΦT
1:RΦ1:R−nI)
j−1 = n"
TR,0.5451553930530164,"2σ2 µT
1:R(I+ n"
TR,0.5455210237659963,"σ2 Λ1:R)−1µ1:R + 1 2 ∞
X j=1"
TR,0.5458866544789762,"
(−1)j+1 1"
TR,0.5462522851919561,"σ2j µT
1:R(I+ n"
TR,0.546617915904936,"σ2 Λ1:R)−1+γ/2Λ−γ/2
1:R A

(I+ n"
TR,0.5469835466179159,"σ2 Λ1:R)−1+γΛ1−γ
1:R A
j−1 (I+ n"
TR,0.5473491773308958,"σ2 Λ1:R)−1+γ/2Λ−γ/2
1:R µ1:R "
TR,0.5477148080438757,"(92)
where in the second to last equality we used the deﬁnition of A (89). As for the ﬁrst term on the right
hand side of (92), by Lemma 15, Assumption 4 and Assumption 5, we have"
TR,0.5480804387568555,"n
2σ2 µT
1:R(I+ n"
TR,0.5484460694698354,"σ2 Λ1:R)−1µ1:R ≤n 2σ2 R
X p=1"
TR,0.5488117001828153,"C2
µp−2β 1+ n"
TR,0.5491773308957952,σ2 Cλp−α =
TR,0.5495429616087751,"(
Θ(nmax{0,1+ 1−2β"
TR,0.549908592321755,"α
}),
α̸=2β−1,
Θ(logn),
α=2β−1."
TR,0.5502742230347349,"On the other hand, by Assumption 5, assuming that supi≥1pi+1−pi =h, we have"
TR,0.5506398537477148,"n
2σ2 µT
1:R(I+ n"
TR,0.5510054844606947,"σ2 Λ1:R)−1µ1:R ≥n 2σ2 ⌊R h ⌋
X i=1"
TR,0.5513711151736745,"C2
µp−2β
i
1+ n"
TR,0.5517367458866544,"σ2 Cλp−α
i ≥n 2σ2 ⌊R h ⌋
X i=1"
TR,0.5521023765996343,"C2
µi−2β 1+ n"
TR,0.5524680073126143,σ2 Cλ(hi)−α =
TR,0.5528336380255942,"(
Θ(nmax{0,1+ 1−2β"
TR,0.5531992687385741,"α
}),
α̸=2β−1,
Θ(logn),
α=2β−1."
TR,0.553564899451554,"Overall, we have
n
2σ2 µT
1:R(I+ n"
TR,0.5539305301645339,"σ2 Λ1:R)−1µ1:R =Θ(nmax{0,1+ 1−2β"
TR,0.5542961608775138,"α
}logkn), where k=
0,
α̸=2β−1,
1,
α=2β−1."
TR,0.5546617915904936,"By Lemma 16, we have"
TR,0.5550274223034735,∥(I+ n
TR,0.5553930530164534,"σ2 Λ1:R)−1+γ/2Λ−γ/2
1:R µ1:R∥2
2 ≤ R
X p=1"
TR,0.5557586837294333,"C2
µp−2β(Cλp−α)−γ (1+ n"
TR,0.5561243144424132,σ2 Cλp−α)2−γ
TR,0.5564899451553931,"= ˜O(max{n−2+γ,R1−2β+αγ})"
TR,0.556855575868373,"= ˜O(nmax{−2+γ, 1−2β"
TR,0.5572212065813529,"α
+γ+κ(1−2β+αγ)}). (93)"
TR,0.5575868372943327,"Using (90), the second term on the right hand side of (92) is computed as follows: 1
2 ∞
X j=1"
TR,0.5579524680073126,"
(−1)j+1 1"
TR,0.5583180987202925,"σ2j µT
1:R(I+ n"
TR,0.5586837294332724,"σ2 Λ1:R)−1+γ/2Λ−γ/2
1:R A

(I+ n"
TR,0.5590493601462523,"σ2 Λ1:R)−1+γΛ1−γ
1:R A
j−1 (I+ n"
TR,0.5594149908592322,"σ2 Λ1:R)−1+γ/2Λ−γ/2
1:R µ1:R  ≤1 2 ∞
X j=1"
TR,0.5597806215722121,"1
σ2j ∥A∥j n σ2"
TR,0.560146252285192,"(−1+γ)(j−1)
∥(I+ n"
TR,0.5605118829981719,"σ2 Λ1:R)−1+γ/2Λ−γ/2
1:R µ1:R∥2
2 ≤1 2 ∞
X j=1"
TR,0.5608775137111517,"1
σ2j ˜O(n"
TR,0.5612431444241316,j(1−2γα+α+2τ)
TR,0.5616087751371115,"2α
)
 n σ2"
TR,0.5619744058500914,"(−1+γ)(j−1) ˜O(nmax{−2+γ, 1−2β"
TR,0.5623400365630713,"α
+γ+κ(1−2β+αγ)})"
TR,0.5627056672760512,= ˜O(nmax{−2+γ+ 1−2γα+α+2τ
TR,0.5630712979890311,"2α
, 1−2β"
TR,0.563436928702011,"α
+γ+ 1−2γα+α+2τ"
TR,0.5638025594149909,"2α
+κ(1−2β+αγ)})"
TR,0.5641681901279707,= ˜O(nmax{−2+ 1+α+2τ
TR,0.5645338208409506,"2α
, 1−2β"
TR,0.5648994515539305,"α
+ 1+α+2τ"
TR,0.5652650822669104,"2α
+κ(1−2β+αγ)}). (94)"
TR,0.5656307129798903,Published as a conference paper at ICLR 2022
TR,0.5659963436928702,Since 1+α+2τ
TR,0.5663619744058501,"2α
< 1+α+2τ"
TR,0.56672760511883,"α+1+2τ =1, we have −2+ 1+α+2τ"
TR,0.5670932358318098,"2α
<0.Also we have 1−2β"
TR,0.5674588665447897,"α
+ 1+α+2τ"
TR,0.5678244972577696,"2α
+κ(1−2β+αγ) =1−2β"
TR,0.5681901279707495,"α
+1+ 1−α+2τ"
TR,0.5685557586837294,"2α
+κ(1−2β+αγ) ≤1−2β"
TR,0.5689213893967093,"α
+1+ 1−α+2τ"
TR,0.5692870201096892,"2α
+καγ <1−2β α
+1, (95)"
TR,0.5696526508226691,where the last inequality holds because κ< α−1−2τ
TR,0.570018281535649,"2α2
and γ ≤1. Hence we have"
TR,0.5703839122486288,"T2,R(Dn)= n"
TR,0.5707495429616087,"2σ2 µT
1:R(I+ n"
TR,0.5711151736745886,σ2 Λ1:R)−1µ1:R+ ˜O(nmax{−2+ 1+α+2τ
TR,0.5714808043875685,"2α
, 1−2β"
TR,0.5718464351005484,"α
+ 1+α+2τ"
TR,0.5722120658135283,"2α
+κ(1−2β+αγ)})"
TR,0.5725776965265083,"=Θ(nmax{0,1+ 1−2β"
TR,0.5729433272394882,"α
}logkn)+ ˜O(nmax{−2+ 1+α+2τ"
TR,0.5733089579524681,"2α
, 1−2β"
TR,0.5736745886654478,"α
+ 1+α+2τ"
TR,0.5740402193784278,"2α
+κ(1−2β+αγ)})"
TR,0.5744058500914077,"=Θ(nmax{0,1+ 1−2β"
TR,0.5747714808043876,"α
}logkn)."
TR,0.5751371115173675,"where k =
0,
α̸=2β−1,
1,
α=2β−1.. Since ˜O(nmax{−2+ 1+α+2τ"
TR,0.5755027422303474,"2α
, 1−2β"
TR,0.5758683729433273,"α
+ 1+α+2τ"
TR,0.5762340036563072,"2α
+κ(1−2β+αγ)}) is lower order"
TR,0.5765996343692871,"term compared to Θ(nmax{0,1+ 1−2β"
TR,0.5769652650822669,"α
}logkn), we further have"
TR,0.5773308957952468,"T2,R(Dn)=
 n"
TR,0.5776965265082267,"2σ2 µT
1:R(I+ n"
TR,0.5780621572212066,"σ2 Λ1:R)−1µ1:R

(1+o(1))"
TR,0.5784277879341865,This concludes the proof of the second statement.
TR,0.5787934186471664,"Lemma 36. Under Assumptions 4, 5 and 6, with probability of at least 1−5δ, we have"
TR,0.5791590493601463,"T1(Dn)=
1"
TR,0.5795246800731262,2logdet(I+ n
TR,0.579890310786106,σ2 Λ)−1
TR,0.5802559414990859,"2Tr

I−(I+ n"
TR,0.5806215722120658,"σ2 Λ)−1
(1+o(1))=Θ(n
1
α ),
(96)"
TR,0.5809872029250457,"Furthermore, let δ=n−q where 0≤q<min{ (2β−1)(α−1−2τ)"
TR,0.5813528336380256,"4α2
, α−1−2τ"
TR,0.5817184643510055,"2α
}. If we assume µ0 =0, we have"
TR,0.5820840950639854,"T2(Dn)=
 n"
TR,0.5824497257769653,2σ2 µT (I+ n
TR,0.5828153564899452,"σ2 Λ)−1µ

(1+o(1))="
TR,0.583180987202925,"(
Θ(nmax{0,1+ 1−2β"
TR,0.5835466179159049,"α
}),
α̸=2β−1,
Θ(logn),
α=2β−1.
(97)"
TR,0.5839122486288848,"Proof of Lemma 36. Let R=n
1
α +κ where 0≤κ< α−1−2τ"
TR,0.5842778793418647,"2α2
. By Lemmas 32 and 35, with probability
of at least 1−5δ we have
|T1,R(Dn)−T1(Dn)|= ˜O(n
1
α +κ(1−α)),
(98)
and"
TR,0.5846435100548446,"|T2,R(Dn)−T2(Dn)|= ˜O

(1"
TR,0.5850091407678245,δ +1)nmax{( 1
TR,0.5853747714808044,α +κ) 1−2β
TR,0.5857404021937843,"2
,1+ 1−2β"
TR,0.5861060329067642,"α
+ (1−2β)κ"
TR,0.586471663619744,"2
,−1−κα,1+ 1−2β"
TR,0.5868372943327239,"α
−κα}

(99)"
TR,0.5872029250457038,as well as
TR,0.5875685557586837,"T1,R(Dn)=
1"
TR,0.5879341864716636,2logdet(I+ n
TR,0.5882998171846435,σ2 ΛR)−1
TR,0.5886654478976234,"2Tr

I−(I+ n"
TR,0.5890310786106033,"σ2 ΛR)−1
(1+o(1))=Θ(n
1
α ),
(100) and"
TR,0.5893967093235832,"T2,R(Dn)=
 n"
TR,0.589762340036563,2σ2 µT (I+ n
TR,0.5901279707495429,"σ2 Λ)−1µ

(1+o(1))="
TR,0.5904936014625228,"(
Θ(nmax{0,1+ 1−2β"
TR,0.5908592321755027,"α
}),
α̸=2β−1,
Θ(logn),
α=2β−1.
(101)"
TR,0.5912248628884826,We then have
TR,0.5915904936014625,"T1(Dn)=T1,R(Dn)+T1,R(Dn)−T1(Dn)=Θ(n
1
α )+ ˜O(n
1
α +κ(1−α))=Θ(n
1
α )."
TR,0.5919561243144424,"Since ˜O(n
1
α +κ(1−α)) is lower order term compared to Θ(n
1
α ), we further have"
TR,0.5923217550274223,"T1(Dn)=
1"
TR,0.5926873857404021,2logdet(I+ n
TR,0.593053016453382,σ2 ΛR)−1
TR,0.593418647166362,"2Tr

I−(I+ n"
TR,0.5937842778793418,"σ2 ΛR)−1
(1+o(1))=Θ(n
1
α )"
TR,0.5941499085923218,Published as a conference paper at ICLR 2022
TR,0.5945155393053017,"Besides, we have"
TR,0.5948811700182816,logdet(I+ n
TR,0.5952468007312615,σ2 Λ)−logdet(I+ n
TR,0.5956124314442414,"σ2 ΛR) = ∞
X"
TR,0.5959780621572212,"p=R+1
log(1+ n"
TR,0.5963436928702011,"σ2 λp)≤n σ2 ∞
X"
TR,0.596709323583181,"p=R+1
λp ≤n σ2 ∞
X"
TR,0.5970749542961609,"p=R+1
Cλp−α = n"
TR,0.5974405850091408,σ2 O(R1−α) = n
TR,0.5978062157221207,σ2 O(n(1−α)( 1
TR,0.5981718464351006,α +κ))
TR,0.5985374771480805,"=o(n
1
α ).
Then we have log det(I +
n
σ2 ΛR) = log det(I +
n
σ2 Λ)(1 + o(1)).
Similarly we can prove
Tr
 
I−(I+ n"
TR,0.5989031078610604,"σ2 Λ)−1
= Tr
 
I−(I+ n"
TR,0.5992687385740402,"σ2 ΛR)−1
(1 + o(1)). This concludes the proof of the ﬁrst
statement."
TR,0.5996343692870201,"As for T2(Dn), we have
T2(Dn)=T2,R(Dn)+T2,R(Dn)−T2(Dn)"
TR,0.6,"=Θ(nmax{0,1+ 1−2β"
TR,0.6003656307129799,"α
}logkn)+ ˜O

(1"
TR,0.6007312614259598,δ +1)nmax{( 1
TR,0.6010968921389397,α +κ) 1−2β
TR,0.6014625228519196,"2
,1+ 1−2β"
TR,0.6018281535648995,"α
+ (1−2β)κ"
TR,0.6021937842778794,"2
,−1−κα,1+ 1−2β"
TR,0.6025594149908592,"α
−κα}
"
TR,0.6029250457038391,"=Θ(nmax{0,1+ 1−2β"
TR,0.603290676416819,"α
}logkn)+ ˜O

nq+max{( 1"
TR,0.6036563071297989,α +κ) 1−2β
TR,0.6040219378427788,"2
,1+ 1−2β"
TR,0.6043875685557587,"α
+ (1−2β)κ"
TR,0.6047531992687386,"2
,−1−κα,1+ 1−2β"
TR,0.6051188299817185,"α
−κα}"
TR,0.6054844606946983,"where we use δ=n−q, k=
0,
α̸=2β−1,
1,
α=2β−1.."
TR,0.6058500914076782,Since 0≤κ< α−1−2τ
TR,0.6062157221206581,"2α2
and 0≤q<min{ (2β−1)(α−1−2τ)"
TR,0.606581352833638,"4α2
, α−1−2τ"
TR,0.6069469835466179,"2α
}, we can choose κ< α−1−2τ"
TR,0.6073126142595978,"2α2
and κ
is arbitrarily close to α−1−2τ"
TR,0.6076782449725777,"2α2
such that 0≤q<min{ (2β−1)κ"
TR,0.6080438756855576,"2
,κα}. Then we have ( 1"
TR,0.6084095063985375,α +κ) 1−2β
TR,0.6087751371115173,"2
+q<0,
−1−κα+q<0, (1−2β)κ"
TR,0.6091407678244972,"2
+q<0 and −κα+q<0. So we have"
TR,0.6095063985374771,"T2,R(Dn)=Θ(nmax{0,1+ 1−2β"
TR,0.609872029250457,"α
}logkn)."
TR,0.6102376599634369,"Since
˜O

( 1"
TR,0.6106032906764168,δ +1)nmax{( 1
TR,0.6109689213893967,α +κ) 1−2β
TR,0.6113345521023766,"2
,1+ 1−2β"
TR,0.6117001828153565,"α
+ (1−2β)κ"
TR,0.6120658135283363,"2
,−1−κα,1+ 1−2β"
TR,0.6124314442413162,"α
−κα}
is
lower
order
term"
TR,0.6127970749542961,"compared to Θ(nmax{0,1+ 1−2β"
TR,0.613162705667276,"α
}logkn), we further have"
TR,0.613528336380256,"T2(Dn)=T2,R(Dn)(1+o(1))=
 n"
TR,0.6138939670932358,"2σ2 µT
R(I+ n"
TR,0.6142595978062158,"σ2 ΛR)−1µR

(1+o(1))."
TR,0.6146252285191957,"Furthermore, we have"
TR,0.6149908592321756,µT (I+ n
TR,0.6153564899451553,"σ2 Λ)−1µ−µT
R(I+ n"
TR,0.6157221206581353,"σ2 ΛR)−1µR = ∞
X p=R+1"
TR,0.6160877513711152,"µ2
p
(1+ n"
TR,0.6164533820840951,"σ2 λp) ≤ ∞
X"
TR,0.616819012797075,"p=R+1
µ2
p ≤n σ2 ∞
X"
TR,0.6171846435100549,"p=R+1
C2
µp−2β =O(R1−2β)"
TR,0.6175502742230348,=O(n(1−2β)( 1
TR,0.6179159049360147,α +κ))
TR,0.6182815356489945,"=o(n
1−2β α )."
TR,0.6186471663619744,Then we have µT (I + n
TR,0.6190127970749543,"σ2 Λ)−1µ = µT
R(I + n"
TR,0.6193784277879342,"σ2 ΛR)−1µR(1+o(1)). This concludes the proof of the
second statement."
TR,0.6197440585009141,Proof of Theorem 7. Using Lemma 36 and noting that 1
TR,0.620109689213894,"α >0, with probability of at least 1−5˜δ, we have"
TR,0.6204753199268739,"EϵF 0(Dn)=T1(Dn)+T2(Dn) =
1"
TR,0.6208409506398538,2logdet(I+ n
TR,0.6212065813528337,σ2 ΛR)−1
TR,0.6215722120658135,"2Tr

I−(I+ n"
TR,0.6219378427787934,σ2 ΛR)−1 + n
TR,0.6223034734917733,"2σ2 µT
R(I+ n"
TR,0.6226691042047532,σ2 ΛR)−1µR
TR,0.6230347349177331,"
(1+o(1))"
TR,0.623400365630713,=Θ(nmax{ 1
TR,0.6237659963436929,"α , 1−2β"
TR,0.6241316270566728,"α
+1})"
TR,0.6244972577696527,"Letting δ=5˜δ, we get the result."
TR,0.6248628884826325,Published as a conference paper at ICLR 2022
TR,0.6252285191956124,"In the case of µ0 >0, we have the following lemma:"
TR,0.6255941499085923,"Lemma 37. Assume that σ2 = Θ(1). Let R = n
1
α +κ where 0 < κ < α−1−2τ"
TR,0.6259597806215722,"α2
. Assume that µ0 > 0.
Under Assumptions 4, 5 and 6, for sufﬁciently large n with probability of at least 1−4δ we have"
TR,0.6263254113345521,"|T2,R(Dn)−T2(Dn)|= ˜O

(1"
TR,0.626691042047532,δ +1)nmax{1+( 1
TR,0.6270566727605119,α +κ) 1−2β
TR,0.6274223034734918,"2
,1−κα}

..
(102)"
TR,0.6277879341864717,"Proof of Lemma 37. As for |T2(Dn)−T2,R(Dn)|, we have"
TR,0.6281535648994515,"|T2(Dn)−T2,R(Dn)|=
f(x)T (I+ ΦΛΦT"
TR,0.6285191956124314,"σ2
)−1f(x)−fR(x)T (I+ ΦΛΦT"
TR,0.6288848263254113,"σ2
)−1fR(x)"
TR,0.6292504570383912,"+
fR(x)T (I+ ΦΛΦT"
TR,0.6296160877513711,"σ2
)−1fR(x)−fR(x)T (I+ ΦRΛRΦT
R
σ2
)−1fR(x)
."
TR,0.629981718464351,"(103)
For the ﬁrst term on the right-hand side of (103), we have
f(x)T (I+ ΦΛΦT"
TR,0.6303473491773309,"σ2
)−1f(x)−fR(x)T (I+ ΦΛΦT"
TR,0.6307129798903108,"σ2
)−1fR(x)"
TR,0.6310786106032906,"≤2
f>R(x)T (I+ ΦΛΦT"
TR,0.6314442413162705,"σ2
)−1fR(x)
+
f>R(x)T (I+ ΦΛΦT"
TR,0.6318098720292504,"σ2
)−1f>R(x)"
TR,0.6321755027422303,≤2∥f>R(x)∥2∥(I+ ΦΛΦT
TR,0.6325411334552102,"σ2
)−1fR(x)∥2+∥f>R(x)∥2∥(I+ ΦΛΦT"
TR,0.6329067641681901,"σ2
)−1∥2∥f>R(x)∥2"
TR,0.63327239488117,≤2∥f>R(x)∥2∥(I+ ΦΛΦT
TR,0.63363802559415,"σ2
)−1fR(x)∥2+∥f>R(x)∥2
2."
TR,0.6340036563071298,"Applying Corollary 19 and Lemma 31, with probability of at least 1−4δ, we have
f(x)T (I+ ΦΛΦT"
TR,0.6343692870201096,"σ2
)−1f(x)−fR(x)T (I+ ΦΛΦT"
TR,0.6347349177330895,"σ2
)−1fR(x) ≤2 ˜O r (1"
TR,0.6351005484460694,"δ +1)nR1−2β
!
˜O( r (1"
TR,0.6354661791590493,δ +1)n)+ ˜O((1
TR,0.6358318098720293,δ +1)nR1−2β)
TR,0.6361974405850092,"=2 ˜O

(1"
TR,0.6365630712979891,δ +1)n1+( 1
TR,0.636928702010969,α +κ) 1−2β
TR,0.6372943327239489,"2

+ ˜O((1"
TR,0.6376599634369287,δ +1)n1+( 1
TR,0.6380255941499086,α +κ)(1−2β))
TR,0.6383912248628885,"=2 ˜O

(1"
TR,0.6387568555758684,δ +1)n1+( 1
TR,0.6391224862888483,"α +κ) 1−2β 2

."
TR,0.6394881170018282,"As for the second term on the right-hand side of (80), according to Lemma 28, Corollary 26 and
Lemma 30, we have
fR(x)T (I+ ΦΛΦT"
TR,0.6398537477148081,"σ2
)−1fR(x)−fR(x)T (I+ ΦRΛRΦT
R
σ2
)−1fR(x) =  ∞
X"
TR,0.640219378427788,"j=1
(−1)jfR(x)T

(I+ ΦRΛRΦT
R
σ2
)−1 Φ>RΛ>RΦT
>R
σ2"
TR,0.6405850091407678,"j
(I+ ΦRΛRΦT
R
σ2
)−1fR(x)  ≤ ∞
X"
TR,0.6409506398537477,"j=1
∥(I+ ΦRΛRΦT
R
σ2
)−1∥j−1
2
·∥Φ>RΛ>RΦT
>R
σ2
∥j
2·∥(I+ ΦRΛRΦT
R
σ2
)−1fR(x)∥2
2 = ∞
X"
TR,0.6413162705667276,"j=1
˜O(n−jκα) ˜O((1"
TR,0.6416819012797075,δ +1)n)
TR,0.6420475319926874,= ˜O((1
TR,0.6424131627056673,δ +1)n1−κα). (104)
TR,0.6427787934186472,Published as a conference paper at ICLR 2022
TR,0.6431444241316271,"By (80), we have"
TR,0.643510054844607,"|T2(Dn)−T2,R(Dn)|= ˜O

(1"
TR,0.6438756855575868,δ +1)n1+( 1
TR,0.6442413162705667,α +κ) 1−2β
TR,0.6446069469835466,"2

+ ˜O((1"
TR,0.6449725776965265,δ +1)n1−κα)
TR,0.6453382084095064,"= ˜O

(1"
TR,0.6457038391224863,δ +1)nmax{1+( 1
TR,0.6460694698354662,α +κ) 1−2β
TR,0.6464351005484461,"2
,1−κα}

."
TR,0.646800731261426,"Lemma 38. Assume that σ2 = Θ(1). Let R = n
1
α +κ where 0 < κ < min{ α−1−2τ"
TR,0.6471663619744058,"2α2
, 2β−1"
TR,0.6475319926873857,"α2 }. Assume
that µ0 >0. Under Assumptions 4, 5 and 6, with probability of at least 1−δ, we have"
TR,0.6478976234003656,"T2,R(Dn)= n"
TR,0.6482632541133455,"2σ2 µ2
0+ ˜O(nmax{ 1+7α+2τ"
TR,0.6486288848263254,"8α
,1+ 1−2β"
TR,0.6489945155393053,"α
}).
(105)"
TR,0.6493601462522852,Proof of Lemma 38. Let
TR,0.6497257769652651,A=(I+ n
TR,0.650091407678245,"σ2 ΛR)−γ/2Λγ/2
R (ΦT
RΦR−nI)Λγ/2
R (I+ n"
TR,0.6504570383912248,"σ2 ΛR)−γ/2,
(106)"
TR,0.6508226691042047,where 1+α+2τ
TR,0.6511882998171846,"2α
<γ ≤1. By Corollary 22, with probability of at least 1−δ, we have"
TR,0.6515539305301645,"∥A∥2 = ˜O(n
1−2γα+α+2τ"
TR,0.6519195612431444,"2α
).
(107)"
TR,0.6522851919561243,"When n is sufﬁciently large, ∥A∥2 is less than 1. Let µR,1 = (µ0,0,...,0) and µR,2 = (0,µ1,...,µR).
Then µR =µR,1+µR,2. Let ˜Λ1,R =diag{1,λ1,...,λR} and I0,R =(0,1,...,1). Then ΛR = ˜Λ1,RI0,R.
Let B = (I +
n
σ2 ΛR)−1/2˜Λ1/2
1,R(ΦT
RΦR −nI)˜Λ1/2
1,R(I +
n
σ2 ΛR)−1/2. By Corollary 23, we have"
TR,0.6526508226691042,"∥B∥2 =O(
q log R"
TR,0.6530164533820841,"δ n
1
2 ). By Lemma 34, we have"
TR,0.6533820840950639,"T2,R(Dn)= n"
TR,0.6537477148080438,"2σ2 µT
R(I+ n"
TR,0.6541133455210237,"σ2 ΛR)−1µR + 1 2 ∞
X j=1"
TR,0.6544789762340036,"
(−1)j+1µT
R
1
σ2 (I+ n"
TR,0.6548446069469835,"σ2 ΛR)−1(ΦT
RΦR−nI)
 1"
TR,0.6552102376599634,σ2 (I+ n
TR,0.6555758683729433,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j−1 (I+ n"
TR,0.6559414990859233,σ2 ΛR)−1µR  (108)
TR,0.6563071297989032,"As for the ﬁrst term on the right hand side of (108), by Lemma 15, we have"
TR,0.656672760511883,"n
2σ2 µT (I+ n"
TR,0.6570383912248628,"σ2 Λ)−1µ≤n 2σ2  µ2
0+ R
X p=1"
TR,0.6574040219378428,"C2
µp−2β 1+ n"
TR,0.6577696526508227,σ2 Cλp−α ! = n
TR,0.6581352833638026,"2σ2 µ2
0+ ˜O(nmax{0,1+ 1−2β α
})."
TR,0.6585009140767825,"We deﬁne Q1,j, Q2,j and Q3,j by"
TR,0.6588665447897624,"Q1,j =µT
R,1
1
σ2 (I+ n"
TR,0.6592321755027423,"σ2 ΛR)−1(ΦT
RΦR−nI)
 1"
TR,0.6595978062157222,σ2 (I+ n
TR,0.659963436928702,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j−1 (I+ n"
TR,0.6603290676416819,"σ2 ΛR)−1µR,1"
TR,0.6606946983546618,"Q2,j =µT
R,1
1
σ2 (I+ n"
TR,0.6610603290676417,"σ2 ΛR)−1(ΦT
RΦR−nI)
 1"
TR,0.6614259597806216,σ2 (I+ n
TR,0.6617915904936015,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j−1 (I+ n"
TR,0.6621572212065814,"σ2 ΛR)−1µR,2"
TR,0.6625228519195613,"Q3,j =µT
R,2
1
σ2 (I+ n"
TR,0.6628884826325412,"σ2 ΛR)−1(ΦT
RΦR−nI)
 1"
TR,0.663254113345521,σ2 (I+ n
TR,0.6636197440585009,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j−1 (I+ n"
TR,0.6639853747714808,"σ2 ΛR)−1µR,2 (109)"
TR,0.6643510054844607,Published as a conference paper at ICLR 2022
TR,0.6647166361974406,"The quantity Q3,j actually shows up in the case of µ0 = 0 in the proof of Lemma 35. By (92), (94)
and (95), we have that | ∞
X"
TR,0.6650822669104205,"j=1
(−1)j+1Q3,j|=| ∞
X"
TR,0.6654478976234004,"j=1
(−1)j+1 ˜O(n"
TR,0.6658135283363803,(j−1)(1−α+2τ)
TR,0.6661791590493601,"2α
)o(nmax{0,1+ 1−2β"
TR,0.66654478976234,"α
})|=o(nmax{0,1+ 1−2β α
})."
TR,0.6669104204753199,"(110)
For Q1,j, we have"
TR,0.6672760511882998,"Q1,1 = 1"
TR,0.6676416819012797,"σ2j µT
R,1(I+ n"
TR,0.6680073126142596,σ2 ΛR)−1+ γ
TR,0.6683729433272395,2 B(I+ n
TR,0.6687385740402194,σ2 ΛR)−1+ γ
TR,0.6691042047531993,"2 µR,1 ≤1"
TR,0.6694698354661791,"σ2j ∥µR,1∥2
2∥(I+ n"
TR,0.669835466179159,σ2 ΛR)−1+ γ
TR,0.6702010968921389,"2 ∥2
2∥B∥2 =O( r logR"
TR,0.6705667276051188,"δ n
1
2 ),"
TR,0.6709323583180987,"where in the last equality we use ∥B∥2 =O(
q log R"
TR,0.6712979890310786,"δ n
1
2 ). For j ≥2, we have"
TR,0.6716636197440585,"Q1,j = 1"
TR,0.6720292504570384,"σ2j µT
R,1(I+ n"
TR,0.6723948811700183,σ2 ΛR)−1+ γ
B,0.6727605118829981,"2 B

(I+ n"
B,0.673126142595978,"σ2 ΛR)−1+γΛ1−γ
R
A
j−2
(I+ n"
B,0.6734917733089579,"σ2 ΛR)−1+γΛ1−γ
R"
B,0.6738574040219378,B(I+ n
B,0.6742230347349177,σ2 ΛR)−1+ γ
B,0.6745886654478976,"2 µR,1 ≤1"
B,0.6749542961608775,"σ2j ∥µR,1∥2
2∥(I+ n"
B,0.6753199268738574,σ2 ΛR)−1+ γ
B,0.6756855575868373,"2 ∥2
2∥B∥2
2∥A∥j−2
2
∥(I+ n"
B,0.6760511882998171,"σ2 ΛR)−1+γΛ1−γ
R
∥j−1
2"
B,0.676416819012797,=O(logR δ n·n
B,0.676782449725777,(j−2)(1−2γα+α+2τ)
B,0.6771480804387568,"2α
·n−(1−γ)(j−1))"
B,0.6775137111517368,=O(logR
B,0.6778793418647167,δ nγ·n
B,0.6782449725776966,"(j−2)(1−α+2τ) 2α
)."
B,0.6786106032906765,"Then we have | ∞
X"
B,0.6789762340036563,"j=1
(−1)j+1Q1,j|≤O( r logR"
B,0.6793418647166362,"δ n
1
2 )+ ∞
X"
B,0.6797074954296161,"j=2
O(logR"
B,0.680073126142596,δ nγ·n
B,0.6804387568555759,(j−2)(1−α+2τ)
B,0.6808043875685558,"2α
)=O(logR"
B,0.6811700182815357,"δ nγ)
(111)"
B,0.6815356489945156,"For Q2,j, we have"
B,0.6819012797074955,"Q2,j = 1"
B,0.6822669104204753,"σ2j µT
R,1(I+ n"
B,0.6826325411334552,σ2 ΛR)−1+ γ
B,0.6829981718464351,"2 B

(I+ n"
B,0.683363802559415,"σ2 ΛR)−1+γΛ1−γ
R
A
j−1
(I+ n"
B,0.6837294332723949,σ2 Λ)−1+ γ
B,0.6840950639853748,"2 ˜Λ
−γ"
B,0.6844606946983547,"2
1,RµR,2 ≤1"
B,0.6848263254113346,"σ2j ∥µR,1∥2∥B∥2∥A∥j−1
2
∥(I+ n"
B,0.6851919561243145,"σ2 ΛR)−1+γΛ1−γ
R
∥j−1
2
∥(I+ n"
B,0.6855575868372943,σ2 Λ)−1+ γ
B,0.6859232175502742,"2 ˜Λ
−γ"
B,0.6862888482632541,"2
1,RµR,2∥2 =O( r logR"
B,0.686654478976234,"δ n
1
2 ·n"
B,0.6870201096892139,(j−1)(1−α+2τ)
B,0.6873857404021938,"2α
)∥(I+ n"
B,0.6877513711151737,σ2 Λ)−1+ γ
B,0.6881170018281536,"2 ˜Λ
−γ"
B,0.6884826325411335,"2
1,RµR,2∥2."
B,0.6888482632541133,Since ∥(I + n
B,0.6892138939670932,σ2 Λ)−1+ γ
B,0.6895795246800731,"2 ˜Λ
−γ"
B,0.689945155393053,"2
1,RµR,2∥2 is actually the case of µ0 = 0, we can use (93) in the proof of
Lemma 35 and get"
B,0.6903107861060329,∥(I+ n
B,0.6906764168190128,σ2 Λ)−1+ γ
B,0.6910420475319927,"2 ˜Λ
−γ"
B,0.6914076782449726,"2
1,RµR,2∥2
2 =∥(I+ n"
B,0.6917733089579524,"σ2 Λ1:R)−1+γ/2Λ−γ/2
1:R µ1:R∥2
2"
B,0.6921389396709323,"= ˜O(nmax{−2+γ, 1−2β"
B,0.6925045703839122,"α
+γ+κ(1−2β+αγ)}"
B,0.6928702010968921,"= ˜O(nmax{−2+γ, 1−2β"
B,0.693235831809872,"α
+γ+κ(1−2β+αγ)})
=o(nγ), (112)"
B,0.6936014625228519,where in the last equality we use κ< 2β−1
B,0.6939670932358318,"α2 . Then we have | ∞
X"
B,0.6943327239488117,"j=1
(−1)j+1Q2,j|≤ ∞
X"
B,0.6946983546617916,"j=1
o( r logR"
B,0.6950639853747714,"δ n
1+γ 2 ·n"
B,0.6954296160877513,(j−1)(1−α+2τ)
B,0.6957952468007312,"2α
)=o( r logR"
B,0.6961608775137111,"δ n
1+γ"
B,0.696526508226691,"2 )
(113)"
B,0.696892138939671,Published as a conference paper at ICLR 2022
B,0.6972577696526508,Choosing γ = 1
B,0.6976234003656308,2(1+ 1+α+2τ
B,0.6979890310786107,"2α
)= 1+3α+2τ"
B,0.6983546617915904,"4α
<1, we have"
B,0.6987202925045704,"T2,R(Dn)= n"
B,0.6990859232175503,"2σ2 µT
R(I+ n"
B,0.6994515539305302,"σ2 ΛR)−1µR+ ∞
X"
B,0.6998171846435101,"j=1
(−1)j+1(Q1,j+Q2,j+Q3,j) = n"
B,0.70018281535649,"2σ2 µ2
0+ ˜O(nmax{0,1+ 1−2β"
B,0.7005484460694699,"α
})+o(nmax{0,1+ 1−2β"
B,0.7009140767824498,"α
})+O(logR"
B,0.7012797074954296,δ nγ)+o( r logR
B,0.7016453382084095,"δ n
1+γ 2 ) = n"
B,0.7020109689213894,"2σ2 µ2
0+ ˜O(nmax{ 1+γ"
B,0.7023765996343693,"2
,1+ 1−2β α
}) = n"
B,0.7027422303473492,"2σ2 µ2
0+ ˜O(nmax{ 1+7α+2τ"
B,0.7031078610603291,"8α
,1+ 1−2β α
})."
B,0.703473491773309,"Proof of Theorem 8. Let R
=
n
1
α +κ where 0
<
κ
<
min{ α−1−2τ"
B,0.7038391224862889,"2α2
,
2β−1"
B,0.7042047531992688,"α2 }.
Since
0 ≤q < min{ 2β−1"
B,0.7045703839122486,"2
, α} · min{ α−1−2τ"
B,0.7049360146252285,"2α2
, 2β−1"
B,0.7053016453382084,"α2 }, we can choose κ < min{ α−1−2τ"
B,0.7056672760511883,"2α2
, 2β−1"
B,0.7060329067641682,"α2 } and κ
is arbitrarily close to κ < min{ α−1−2τ"
B,0.7063985374771481,"2α2
, 2β−1"
B,0.706764168190128,α2 } such that 0 ≤q < min{ (2β−1)κ
B,0.7071297989031079,"2
,κα}. Then we have
( 1"
B,0.7074954296160878,α +κ) 1−2β
B,0.7078610603290676,"2
+q<0, and −κα+q<0. As for T2(Dn), we have"
B,0.7082266910420475,"T2(Dn)≤T2,R(Dn)+|T2,R(Dn)−T2(Dn)| = n"
B,0.7085923217550274,"2σ2 µ2
0+ ˜O(nmax{ 1+7α+2τ"
B,0.7089579524680073,"8α
,1+ 1−2β"
B,0.7093235831809872,"α
})+ ˜O

( 1"
B,0.7096892138939671,δ +1)nmax{1+( 1
B,0.710054844606947,α +κ) 1−2β
B,0.7104204753199269,"2
,1−κα} = n"
B,0.7107861060329068,"2σ2 µ2
0+ ˜O(nmax{ 1+7α+2τ"
B,0.7111517367458866,"8α
,1+ 1−2β"
B,0.7115173674588665,"α
})+ ˜O

nq+max{1+( 1"
B,0.7118829981718464,α +κ) 1−2β
B,0.7122486288848263,"2
,1−κα} = n"
B,0.7126142595978062,"2σ2 µ2
0+o(n)."
B,0.7129798903107861,"By Lemma 36, we have T1(Dn) = O(n
1
α ).
Hence EϵF 0(Dn) = T1(Dn) + T2(Dn) =
n
2σ2 µ2
0+o(n)."
B,0.713345521023766,"D.2
PROOFS RELATED TO THE ASYMPTOTICS OF THE GENERALIZATION ERROR"
B,0.7137111517367459,"Lemma 39. Assume σ2 = Θ(nt) where 1 −
α
1+2τ < t < 1. Let R = n(
2α−1
α(α−1) +1)(1−t). Under
Assumptions 4, 5 and 6, with probability of at least 1−δ over sample inputs (xi)n
i=1, we have"
B,0.7140767824497257,G1(Dn)= 1+o(1)
B,0.7144424131627056,"2σ2

Tr(I+ n"
B,0.7148080438756855,"σ2 ΛR)−1ΛR−∥Λ1/2
R (I+ n"
B,0.7151736745886654,"σ2 ΛR)−1∥2
F

= 1"
B,0.7155393053016453,"σ2 Θ

n
(1−α)(1−t)"
B,0.7159049360146252,"α

. (114)"
B,0.7162705667276051,"Proof of Lemma 39. Let G1,R(Dn) = E(xn+1,yn+1)(T1,R(Dn+1) −T1,R(Dn)), where R = nC for
some constant C. By Lemma 32, we have that"
B,0.716636197440585,"|G1(Dn)−G1,R(Dn)|=
E(xn+1,yn+1)[T1(Dn+1)−T1,R(Dn+1)]−[T1(Dn)−T1,R(Dn)]"
B,0.717001828153565,"=
E(xn+1,yn+1)O((n+1)R1−α)
+
O(nR1−α)] =O( 1"
B,0.7173674588665447,σ2 nR1−α). (115)
B,0.7177330895795246,"Deﬁne ηR =(φ0(xn+1),φ1(xn+1),...,φR(xn+1))T and eΦR =(ΦT
R,ηR)T . As for G1,R(Dn), we have"
B,0.7180987202925045,"G1,R(Dn)=E(xn+1,yn+1)(T1,R(Dn+1)−T1,R(Dn))"
B,0.7184643510054844,"=E(xn+1,yn+1)"
B,0.7188299817184644,"1
2logdet(I+
eΦRΛReΦT
R
σ2
)−1"
B,0.7191956124314443,"2Tr(I−(I+
eΦRΛReΦT
R
σ2
)−1) ! −
1"
B,0.7195612431444242,"2logdet(I+ ΦRΛRΦT
R
σ2
)−1"
B,0.7199268738574041,"2Tr(I−(I+ ΦRΛRΦT
R
σ2
)−1)
 = 1 2 "
B,0.720292504570384,"E(xn+1,yn+1)logdet(I+
f
ΦRΛR f
ΦR
T"
B,0.7206581352833638,"σ2
)−logdet(I+ ΦRΛRΦT
R
σ2
) ! −1 2 "
B,0.7210237659963437,"E(xn+1,yn+1)Tr(I−(I+
eΦRΛReΦT
R
σ2
)−1)−Tr(I−(I+ ΦRΛRΦT
R
σ2
)−1) ! . (116)"
B,0.7213893967093236,Published as a conference paper at ICLR 2022
B,0.7217550274223035,"As for the ﬁrst term in the right hand side (116), we have 1
2 "
B,0.7221206581352834,"E(xn+1,yn+1)logdet(I+
eΦRΛReΦT
R
σ2
)−logdet(I+ ΦRΛRΦT
R
σ2
) ! = 1 2 "
B,0.7224862888482633,"E(xn+1,yn+1)logdet(I+ ΛReΦT
ReΦR
σ2
)−logdet(I+ ΛRΦT
RΦR
σ2
) ! = 1 2"
B,0.7228519195612432,"
E(xn+1,yn+1)logdet(I+ ΛRΦT
RΦR+ηRηT
R
σ2
)−logdet(I+ ΛRΦT
RΦR
σ2
)
 = 1 2"
B,0.7232175502742231,"
E(xn+1,yn+1)logdet

(I+ ΛRΦT
RΦR
σ2
)−1(I+ ΛRΦT
RΦR
σ2
+ ΛRηRηT
R
σ2
)
 = 1 2"
B,0.723583180987203,"
E(xn+1,yn+1)logdet

I+(I+ ΛRΦT
RΦR
σ2
)−1 ΛRηRηT
R
σ2  = 1 2"
B,0.7239488117001828,"
E(xn+1,yn+1)log

1+ 1"
B,0.7243144424131627,"σ2 ηT
R(I+ ΛRΦT
RΦR
σ2
)−1ΛRηR  Let"
B,0.7246800731261426,A=(I+ n
B,0.7250457038391225,"σ2 ΛR)−1/2Λ1/2
R (ΦT
RΦR−nI)Λ1/2
R (I+ n"
B,0.7254113345521024,"σ2 ΛR)−1/2.
(117)"
B,0.7257769652650823,"According to Corollary 22, with probability of at least 1 −δ, we have ∥1"
B,0.7261425959780622,"σ2 A∥2
= O(
q log R"
B,0.7265082266910421,"δ n
1−α+2τ"
B,0.7268738574040219,"2α
−(1+2τ)t"
B,0.7272394881170018,"2α
) = o(1). When n is sufﬁciently large, ∥1"
B,0.7276051188299817,"σ2 A∥2 is less than 1. By
Lemma 27, we have"
B,0.7279707495429616,"ηT
R(I+ ΛRΦT
RΦR
σ2
)−1ΛRηR"
B,0.7283363802559415,"=ηT
R(I+ n"
B,0.7287020109689214,"σ2 ΛR)−1ΛRηR+ ∞
X"
B,0.7290676416819013,"j=1
(−1)jηT
R  1"
B,0.7294332723948812,σ2 (I+ n
B,0.7297989031078611,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j
(I+ n"
B,0.7301645338208409,σ2 ΛR)−1ΛRηR
B,0.7305301645338208,"=ηT
R(I+ n"
B,0.7308957952468007,"σ2 ΛR)−1ΛRηR+ ∞
X"
B,0.7312614259597806,"j=1
(−1)j 1"
B,0.7316270566727605,"σ2j ηT
R(I+ n"
B,0.7319926873857404,"σ2j ΛR)−1/2Λ1/2
R Aj(I+ n"
B,0.7323583180987203,"σ2 ΛR)−1/2Λ1/2
R ηR"
B,0.7327239488117002,"≤ηT
R(I+ n"
B,0.7330895795246801,"σ2 ΛR)−1ΛRηR+ ∞
X"
B,0.7334552102376599,"j=1
∥1"
B,0.7338208409506398,"σ2 A∥j
2∥(I+ n"
B,0.7341864716636197,"σ2 ΛR)−1/2Λ1/2
R ηR∥2
2 ≤ R
X"
B,0.7345521023765996,"p=1
φ2
p(xn+1)
Cλp−α"
B,0.7349177330895795,"1+nCλp−α/σ2 + ∞
X"
B,0.7352833638025594,"j=1
∥1"
B,0.7356489945155393,"σ2 A∥j
2 R
X"
B,0.7360146252285192,"p=1
φ2
p(xn+1)
Cλp−α"
B,0.7363802559414991,"1+nCλp−α/σ2 ≤ R
X p=1"
B,0.7367458866544789,Cλp−αp2τ
B,0.7371115173674588,"1+nCλp−α/σ2 + ∞
X"
B,0.7374771480804387,"j=1
∥1"
B,0.7378427787934186,"σ2 A∥j
2 R
X p=1"
B,0.7382084095063985,Cλp−αp2τ
B,0.7385740402193784,1+nCλp−α/σ2 ≤O(n
B,0.7389396709323584,"(1−α+2τ)(1−t) α
)+ ∞
X"
B,0.7393053016453383,"j=1
∥1"
B,0.739670932358318,"σ2 A∥j
2O(n"
B,0.740036563071298,"(1−α+2τ)(1−t) α
) =O(n"
B,0.7404021937842779,(1−α+2τ)(1−t)
B,0.7407678244972578,"α
)=o(1),
(118)"
B,0.7411334552102377,Published as a conference paper at ICLR 2022
B,0.7414990859232176,"where we use Lemma 15 in the last inequality. Next we have 1
2 "
B,0.7418647166361975,"E(xn+1,yn+1)logdet(I+
eΦRΛReΦT
R
σ2
)−logdet(I+ ΦRΛRΦT
R
σ2
) ! = 1 2"
B,0.7422303473491774,"
E(xn+1,yn+1)log

1+ 1"
B,0.7425959780621573,"σ2 ηT
R(I+ ΛRΦT
RΦR
σ2
)−1ΛRηR  = 1 2"
B,0.7429616087751371,"
E(xn+1,yn+1)  1"
B,0.743327239488117,"σ2 ηT
R(I+ ΛRΦT
RΦR
σ2
)−1ΛRηR"
B,0.7436928702010969,"
(1+o(1))
 = 1 2σ2"
B,0.7440585009140768,"
Tr(I+ ΛRΦT
RΦR
σ2
)−1ΛR"
B,0.7444241316270567,"
(1+o(1)),"
B,0.7447897623400366,"where in the last equality we use the fact that E(xn+1,yn+1)ηRηT
R =I. By Lemma 27, we have"
B,0.7451553930530165,"Tr(I+ ΛRΦT
RΦR
σ2
)−1ΛR"
B,0.7455210237659964,=Tr(I+ n
B,0.7458866544789763,"σ2 ΛR)−1ΛR+ ∞
X"
B,0.7462522851919561,"j=1
(−1)jTr
 1"
B,0.746617915904936,σ2 (I+ n
B,0.7469835466179159,"σ2 ΛR)−1ΛR(ΦT
RΦR−nI)
j
(I+ n"
B,0.7473491773308958,σ2 ΛR)−1ΛR
B,0.7477148080438757,=Tr(I+ n
B,0.7480804387568556,"σ2 ΛR)−1ΛR+ ∞
X"
B,0.7484460694698355,"j=1
(−1)jTr 1"
B,0.7488117001828154,σ2j (I+ n
B,0.7491773308957953,"σ2 ΛR)−1/2Λ1/2
R Aj(I+ n"
B,0.7495429616087751,"σ2 ΛR)−1/2Λ1/2
R ."
B,0.749908592321755,"By Lemma 15, we have"
B,0.7502742230347349,Tr(I+ n
B,0.7506398537477148,"σ2 ΛR)−1ΛR ≤ R
X p=1 Cλp−α"
B,0.7510054844606947,1+nCλp−α/σ2 =Θ(n
B,0.7513711151736746,"(1−α)(1−t) α
)"
B,0.7517367458866545,Tr(I+ n
B,0.7521023765996344,"σ2 ΛR)−1ΛR ≥ R
X p=1 Cλp−α"
B,0.7524680073126142,1+nCλp−α/σ2 =Θ(n
B,0.7528336380255941,"(1−α)(1−t) α
)."
B,0.753199268738574,"Overall,"
B,0.7535648994515539,Tr(I+ n
B,0.7539305301645338,σ2 ΛR)−1ΛR =Θ(n
B,0.7542961608775137,(1−α)(1−t)
B,0.7546617915904936,"α
).
(119)"
B,0.7550274223034735,Since ∥1
B,0.7553930530164534,"σ2 A∥j
2 =o(1), we have that the absolute values of diagonal entries of
1
σ2j Aj are at most o(1).
Let (Aj)p,p denote the (p,p)-th entry of the matrix Aj. Then we have
Tr 1"
B,0.7557586837294332,σ2j (I+ n
B,0.7561243144424131,"σ2 ΛR)−1/2Λ1/2
R Aj(I+ n"
B,0.756489945155393,"σ2 ΛR)−1/2Λ1/2
R  =  R
X p=1"
B,0.7568555758683729,"λp
1
σ2j (Aj)p,p
1+nλp/σ2 ≤ R
X p=1 λp∥1"
B,0.7572212065813528,"σ2j A∥j
2
1+nλp/σ2 =Θ(n"
B,0.7575868372943327,(1−α)(1−t)
B,0.7579524680073126,"α
) ˜O(n"
B,0.7583180987202925,j(1−α+2τ−(1+2τ)t)
B,0.7586837294332724,"2α
(logR)j/2),
(120)"
B,0.7590493601462522,"where in the last step we used (119). According to (119) and (120), we have 1
2 "
B,0.7594149908592321,"E(xn+1,yn+1)logdet(I+
eΦRΛReΦT
R
σ2
)−logdet(I+ ΦRΛRΦT
R
σ2
) ! = 1 2σ2"
B,0.759780621572212,"
Tr(I+ ΛRΦT
RΦR
σ2
)−1ΛR"
B,0.760146252285192,"
(1+o(1)) = 1"
B,0.7605118829981719,σ2 Θ(n
B,0.7608775137111518,(1−α)(1−t)
B,0.7612431444241317,"α
)+ 1 σ2 ∞
X"
B,0.7616087751371116,"j=1
Θ(n"
B,0.7619744058500915,(1−α)(1−t)
B,0.7623400365630713,"α
) ˜O(n"
B,0.7627056672760512,j(1−α+2τ−(1+2τ)t)
B,0.7630712979890311,"2α
(logR)j/2) = 1"
B,0.763436928702011,σ2 Θ(n
B,0.7638025594149909,(1−α)(1−t)
B,0.7641681901279708,"α
)+ 1"
B,0.7645338208409507,σ2 Θ(n
B,0.7648994515539306,(1−α)(1−t)
B,0.7652650822669104,"α
)o(1)= 1"
B,0.7656307129798903,σ2 Θ(n
B,0.7659963436928702,"(1−α)(1−t) α
) = 1 2σ2"
B,0.7663619744058501,"
Tr(I+ n"
B,0.76672760511883,"σ2 ΛR)−1ΛR

(1+o(1)). (121)"
B,0.7670932358318099,Published as a conference paper at ICLR 2022
B,0.7674588665447898,"Using the Woodbury matrix identity, the second term in the right hand side (116) is given by 1
2 "
B,0.7678244972577697,"E(xn+1,yn+1)Tr(I−(I+
eΦRΛReΦT
R
σ2
)−1−Tr(I−(I+ ΦRΛRΦT
R
σ2
)−1
! = 1 2"
B,0.7681901279707496,"
E(xn+1,yn+1)Tr( 1"
B,0.7685557586837294,σ2 eΦR(I+ 1
B,0.7689213893967093,"σ2 ΛReΦT
ReΦR)−1ΛReΦT
R−Tr( 1"
B,0.7692870201096892,σ2 ΦR(I+ 1
B,0.7696526508226691,"σ2 ΛRΦT
RΦR)−1ΛRΦT
R  = 1 2"
B,0.770018281535649,"
E(xn+1,yn+1)Tr( 1"
B,0.7703839122486289,σ2 (I+ 1
B,0.7707495429616088,"σ2 ΛReΦT
ReΦR)−1ΛReΦT
ReΦR−Tr( 1"
B,0.7711151736745887,σ2 (I+ 1
B,0.7714808043875686,"σ2 ΛRΦT
RΦR)−1ΛRΦT
RΦR  =−1 2"
B,0.7718464351005484,"
E(xn+1,yn+1)Tr(I+ 1"
B,0.7722120658135283,"σ2 ΛReΦT
ReΦR)−1−Tr(I+ 1"
B,0.7725776965265082,"σ2 ΛRΦT
RΦR)−1
 =−1 2"
B,0.7729433272394881,"
E(xn+1,yn+1)Tr(I+ 1"
B,0.773308957952468,"σ2 ΛRΦT
RΦR+ 1"
B,0.7736745886654479,"σ2 ΛRηRηT
R)−1−Tr(I+ 1"
B,0.7740402193784278,"σ2 ΛRΦT
RΦR)−1
 = 1 2σ2"
B,0.7744058500914077,"
E(xn+1,yn+1)Tr(I+ 1"
B,0.7747714808043875,"σ2 ΛRΦT
RΦR)−1ΛRηRηT
R(I+ 1"
B,0.7751371115173674,"σ2 ΛRΦT
RΦR)−1 1+ 1"
B,0.7755027422303473,"σ2 ηT
R(I+ 1"
B,0.7758683729433272,"σ2 ΛRΦT
RΦR)−1ΛRηR 
,"
B,0.7762340036563071,"where the last equality uses the Sherman–Morrison formula. According to (118), we get 1
2σ2"
B,0.776599634369287,"
E(xn+1,yn+1)Tr(I+ 1"
B,0.7769652650822669,"σ2 ΛRΦT
RΦR)−1ΛRηRηT
R(I+ 1"
B,0.7773308957952468,"σ2 ΛRΦT
RΦR)−1 1+ 1"
B,0.7776965265082267,"σ2 ηT
R(I+ 1"
B,0.7780621572212065,"σ2 ΛRΦT
RΦR)−1ΛRηR  = 1 2σ2"
B,0.7784277879341864,"
E(xn+1,yn+1)Tr(I+ 1"
B,0.7787934186471663,"σ2 ΛRΦT
RΦR)−1ΛRηRηT
R(I+ 1"
B,0.7791590493601462,"σ2 ΛRΦT
RΦR)−1(1+o(1))
"
B,0.7795246800731261,= 1+o(1)
B,0.779890310786106,"2σ2
Tr(I+ 1"
B,0.780255941499086,"σ2 ΛRΦT
RΦR)−1ΛR(I+ 1"
B,0.7806215722120659,"σ2 ΛRΦT
RΦR)−1"
B,0.7809872029250458,= 1+o(1)
B,0.7813528336380255,"2σ2
TrΛ1/2
R (I+ 1"
B,0.7817184643510054,"σ2 Λ1/2
R ΦT
RΦRΛ1/2
R )−1Λ1/2
R (I+ 1"
B,0.7820840950639854,"σ2 ΛRΦT
RΦR)−1"
B,0.7824497257769653,= 1+o(1)
B,0.7828153564899452,"2σ2
Tr(I+ 1"
B,0.7831809872029251,"σ2 Λ1/2
R ΦT
RΦRΛ1/2
R )−1Λ1/2
R (I+ 1"
B,0.783546617915905,"σ2 ΛRΦT
RΦR)−1Λ1/2
R"
B,0.7839122486288849,= 1+o(1)
B,0.7842778793418648,"2σ2
Tr(I+ 1"
B,0.7846435100548446,"σ2 Λ1/2
R ΦT
RΦRΛ1/2
R )−1ΛR(I+ 1"
B,0.7850091407678245,"σ2 Λ1/2
R ΦT
RΦRΛ1/2
R )−1"
B,0.7853747714808044,= 1+o(1)
B,0.7857404021937843,"2σ2
∥Λ1/2
R (I+ 1"
B,0.7861060329067642,"σ2 Λ1/2
R ΦT
RΦRΛ1/2
R )−1∥2
F"
B,0.7864716636197441,= 1+o(1)
B,0.786837294332724,"2σ2
∥Λ1/2
R (I+ n"
B,0.7872029250457039,σ2 ΛR)−1/2(I+ 1
B,0.7875685557586837,σ2 A)−1(I+ n
B,0.7879341864716636,"σ2 ΛR)−1/2∥2
F ,"
B,0.7882998171846435,"where in the penultimate equality we use Tr(BBT )=∥B∥2
F , ∥B∥F is the Frobenius norm of A, and
in the last equality we use the deﬁnition of A (117). Then we have"
B,0.7886654478976234,1+o(1)
B,0.7890310786106033,"2σ2
∥Λ1/2
R (I+ n"
B,0.7893967093235832,σ2 ΛR)−1/2(I+ 1
B,0.7897623400365631,σ2 A)−1(I+ n
B,0.790127970749543,"σ2 ΛR)−1/2∥2
F"
B,0.7904936014625229,= 1+o(1)
B,0.7908592321755027,"2σ2
∥Λ1/2
R (I+ n"
B,0.7912248628884826,"σ2 ΛR)−1/2(I+ ∞
X"
B,0.7915904936014625,"j=1
(−1)j 1"
B,0.7919561243144424,σ2j Aj)(I+ n
B,0.7923217550274223,"σ2 ΛR)−1/2∥2
F"
B,0.7926873857404022,= 1+o(1)
B,0.7930530164533821,"2σ2
∥Λ1/2
R (I+ n"
B,0.793418647166362,"σ2 ΛR)−1+ ∞
X"
B,0.7937842778793419,"j=1
(−1)j 1"
B,0.7941499085923217,"σ2j Λ1/2
R (I+ n"
B,0.7945155393053016,σ2 ΛR)−1/2Aj(I+ n
B,0.7948811700182815,"σ2 ΛR)−1/2∥2
F ."
B,0.7952468007312614,"(122)
By Lemma 15, we have"
B,0.7956124314442413,"∥Λ1/2
R (I+ n"
B,0.7959780621572212,σ2 ΛR)−1∥F ≤
B,0.7963436928702011,"v
u
u
t R
X p=1 Cλp−α"
B,0.796709323583181,(1+nCλp−α/σ2)2 =Θ(n
B,0.7970749542961609,"(1−α)(1−t) 2α
)"
B,0.7974405850091407,"∥Λ1/2
R (I+ n"
B,0.7978062157221206,σ2 ΛR)−1∥F ≥
B,0.7981718464351005,"v
u
u
t R
X p=1 Cλp−α"
B,0.7985374771480804,(1+nCλp−α/σ2)2 =Θ(n
B,0.7989031078610603,"(1−α)(1−t) 2α
)."
B,0.7992687385740402,Published as a conference paper at ICLR 2022
B,0.7996343692870201,"Overall, we have"
B,0.8,"∥Λ1/2
R (I+ n"
B,0.8003656307129798,σ2 ΛR)−1∥F =Θ(n
B,0.8007312614259597,(1−α)(1−t)
B,0.8010968921389396,"2α
).
(123)"
B,0.8014625228519195,Since ∥1
B,0.8018281535648994,"σ2 A∥2 =O(
q log R"
B,0.8021937842778794,"δ n
1−α+2τ"
B,0.8025594149908593,"2α
−(1+2τ)t"
B,0.8029250457038392,"2α
)=o(1), we have ∥1"
B,0.8032906764168191,"σ2j Λ1/2
R (I+ n"
B,0.8036563071297989,σ2 ΛR)−1/2Aj(I+ n
B,0.8040219378427788,σ2 ΛR)−1/2∥F
B,0.8043875685557587,"≤∥Λ1/2
R (I+ n"
B,0.8047531992687386,σ2 ΛR)−1/2∥F ∥1
B,0.8051188299817185,"σ2 A∥j
2∥(I+ n"
B,0.8054844606946984,σ2 ΛR)−1/2∥2 =O(n
B,0.8058500914076783,(1−α)(1−t)
B,0.8062157221206582,"2α
) ˜O(n"
B,0.8065813528336381,j(1−α+2τ−(1+2τ)t)
B,0.8069469835466179,"2α
(logR)j/2), (124)"
B,0.8073126142595978,"where in the ﬁrst inequality we use the fact that ∥AB∥F ≤∥A∥F ∥B∥2 when B is symmetric. By
Lemma 15, we have 1
σ2j"
B,0.8076782449725777,"TrΛ1/2
R (I+ n"
B,0.8080438756855576,"σ2 ΛR)−1Λ1/2
R (I+ n"
B,0.8084095063985375,σ2 ΛR)−1/2Aj(I+ n
B,0.8087751371115174,"σ2 ΛR)−1/2 =  R
X p=1"
B,0.8091407678244973,λp(( 1
B,0.8095063985374772,"σ2 A)j)p,p
(1+nλp/σ2)2 ≤ R
X p=1 λp∥1"
B,0.8098720292504571,"σ2 A∥j
2
(1+nλp/σ2)2 =Θ(n"
B,0.8102376599634369,(1−α)(1−t)
B,0.8106032906764168,"α
) ˜O(n"
B,0.8109689213893967,j(1−α+2τ−(1+2τ)t)
B,0.8113345521023766,"2α
(logR)j/2),"
B,0.8117001828153565,"(125)
According to (123), (124) and (125), we have 1
2 "
B,0.8120658135283364,"E(xn+1,yn+1)Tr(I−(I+
eΦRΛReΦT
R
σ2
)−1−Tr(I−(I+ ΦRΛRΦT
R
σ2
)−1
!"
B,0.8124314442413163,=1+o(1)
B,0.8127970749542962,"2σ2
Tr(I+ 1"
B,0.813162705667276,"σ2 ΛRΦT
RΦR)−1ΛR(I+ 1"
B,0.8135283363802559,"σ2 ΛRΦT
RΦR)−1"
B,0.8138939670932358,=1+o(1)
B,0.8142595978062157,"2σ2
∥Λ1/2
R (I+ n"
B,0.8146252285191956,"σ2 ΛR)−1+ ∞
X"
B,0.8149908592321755,"j=1
(−1)j 1"
B,0.8153564899451554,"σ2j Λ1/2
R (I+ n"
B,0.8157221206581353,σ2 ΛR)−1/2Aj(I+ n
B,0.8160877513711152,"σ2 ΛR)−1/2∥2
F"
B,0.816453382084095,=1+o(1) 2σ2
B,0.8168190127970749,"
∥Λ1/2
R (I+ n"
B,0.8171846435100548,"σ2 ΛR)−1∥2
F + ∞
X j=1"
B,0.8175502742230347,"1
σ2j Λ1/2
R (I+ n"
B,0.8179159049360146,σ2 ΛR)−1/2Aj(I+ n
B,0.8182815356489945,σ2 ΛR)−1/2 2 F
B,0.8186471663619744,"+2TrΛ1/2
R (I+ n"
B,0.8190127970749543,"σ2 ΛR)−1
∞
X"
B,0.8193784277879342,"j=1
(−1)j 1"
B,0.819744058500914,"σ2j Λ1/2
R (I+ n"
B,0.8201096892138939,σ2 ΛR)−1/2Aj(I+ n
B,0.8204753199268738,"σ2 ΛR)−1/2
"
B,0.8208409506398537,"=1+o(1) 2σ2 
Θ(n"
B,0.8212065813528336,"(1−α)(1−t) α
)+ ∞
X j=1"
B,0.8215722120658135,"1
σ2j O(n"
B,0.8219378427787934,(1−α)(1−t)
B,0.8223034734917734,"α
) ˜O(n"
B,0.8226691042047533,j(1−α+2τ−(1+2τ)t)
B,0.823034734917733,"2α
(logR)j/2) +2 ∞
X j=1"
B,0.823400365630713,"1
σ2j Θ(n"
B,0.8237659963436929,(1−α)(1−t)
B,0.8241316270566728,"α
) ˜O(n"
B,0.8244972577696527,j(1−α+2τ−(1+2τ)t)
B,0.8248628884826326,"2α
(logR)j/2)
 = 1"
B,0.8252285191956125,σ2 Θ(n
B,0.8255941499085924,(1−α)(1−t)
B,0.8259597806215722,"α
)= 1+o(1)"
B,0.8263254113345521,"2σ2
∥Λ1/2
R (I+ n"
B,0.826691042047532,"σ2 ΛR)−1∥2
F .
(126)
Combining (121) and (126) we get that G1,R(Dn)
=
1+o(1)"
B,0.8270566727605119,"2σ2 (Tr(I
+
n
σ2 ΛR)−1ΛR +"
B,0.8274223034734918,"∥Λ1/2
R (I +
n
σ2 ΛR)−1∥2
F )
=
1
σ2 Θ(n
(1−α)(1−t)"
B,0.8277879341864717,"α
).
From (115) we have that G1(Dn)
≤"
B,0.8281535648994516,"G1,R(Dn) + |G1(Dn) −G1,R(Dn)|
=
1
σ2 Θ(n
(1−α)(1−t)"
B,0.8285191956124315,"α
) + O(n 1"
B,0.8288848263254114,"σ2 R1−α).
Choosing"
B,0.8292504570383912,"R=n(
2α−1
α(α−1) +1)(1−t) we conclude the proof."
B,0.8296160877513711,"Lemma 40. Assume σ2 =Θ(nt) where 1−
α
1+2τ <t<1. Let S =nD. Assume that ∥ξ∥2 =1. When
n is sufﬁciently large, with probability of at least 1−2δ we have"
B,0.829981718464351,∥(I+ 1
B,0.8303473491773309,"σ2 ΦSΛSΦT
S)−1ΦSΛSξ∥2 =O(
q ( 1"
B,0.8307129798903108,"δ +1)n·n−(1−t)).
(127)"
B,0.8310786106032907,Published as a conference paper at ICLR 2022
B,0.8314442413162706,"Proof of Lemma 40. Using the Woodbury matrix identity, we have that"
B,0.8318098720292505,((I+ 1
B,0.8321755027422304,"σ2 ΦSΛSΦT
S)−1ΦSΛSξ =

I−ΦS(σ2I+ΛSΦT
SΦS)−1ΛSΦT
S

ΦSΛSξ"
B,0.8325411334552102,"=ΦSΛSξ−ΦS(σ2I+ΛSΦT
SΦS)−1ΛSΦT
SΦSΛSξ"
B,0.8329067641681901,=ΦS(I+ 1
B,0.83327239488117,"σ2 ΛSΦT
SΦS)−1ΛSξ. (128)"
B,0.8336380255941499,Let A = (I + n
B,0.8340036563071298,"σ2 ΛS)−γ/2Λγ/2
S
(ΦT
SΦS −nI)Λγ/2
S
(I + n"
B,0.8343692870201097,"σ2 ΛS)−γ/2, where γ > 1+α+2τ−(1+2τ+2α)t"
B,0.8347349177330896,"2α(1−t)
."
B,0.8351005484460695,"By Corollary 22, with probability of at least 1−δ, we have ∥1"
B,0.8354661791590493,"σ2 A∥2 = ˜O(n
1+α+2τ−(1+2τ+2α)t"
B,0.8358318098720292,"2α
−γ(1−t)).
When n is sufﬁciently large, ∥1"
B,0.8361974405850091,"σ2 A∥2 is less than 1. By Lemma 27, we have (I+ 1"
B,0.836563071297989,"σ2 ΛSΦT
SΦS)−1"
B,0.8369287020109689,=(I+ n
B,0.8372943327239488,"σ2 ΛS)−1+ ∞
X"
B,0.8376599634369287,"j=1
(−1)j
 1"
B,0.8380255941499086,σ2 (I+ n
B,0.8383912248628885,"σ2 ΛS)−1ΛS(ΦT
SΦS−nI)
j
(I+ n"
B,0.8387568555758683,σ2 ΛS)−1.
B,0.8391224862888482,Then we have
B,0.8394881170018281,∥(I+ 1
B,0.839853747714808,"σ2 ΛSΦT
SΦS)−1ΛSξ∥2 =  "
B,0.8402193784277879,(I+ n
B,0.8405850091407678,"σ2 ΛS)−1+ ∞
X"
B,0.8409506398537477,"j=1
(−1)j
 1"
B,0.8413162705667276,σ2 (I+ n
B,0.8416819012797075,"σ2 ΛS)−1ΛS(ΦT
SΦS−nI)
j
(I+ n"
B,0.8420475319926873,σ2 ΛS)−1  ΛSξ 2 ≤ 
B,0.8424131627056672,∥(I+ n
B,0.8427787934186471,"σ2 ΛS)−1ΛSξ∥2+ ∞
X j=1   1"
B,0.843144424131627,σ2 (I+ n
B,0.843510054844607,"σ2 ΛS)−1ΛS(ΦT
SΦS−nI)
j
(I+ n"
B,0.8438756855575869,σ2 ΛS)−1ΛSξ 2  .
B,0.8442413162705668,"(129)
For the ﬁrst term in the right hand side of the last equation, we have"
B,0.8446069469835467,∥(I+ n
B,0.8449725776965266,σ2 ΛS)−1ΛSξ∥2 ≤∥(I+ n
B,0.8453382084095064,σ2 ΛS)−1ΛS∥2∥ξ∥2 ≤σ2
B,0.8457038391224863,"n =O(n−(1−t)).
(130)"
B,0.8460694698354662,Using the fact that ∥1
B,0.8464351005484461,"σ2 A∥2 = ˜O(n
1+α+2τ−(1+2τ+2α)t"
B,0.846800731261426,"2α
−γ(1−t)) and ∥(I + n"
B,0.8471663619744059,"σ2 ΛS)−1ΛS∥2 ≤n−1, we
have  1"
B,0.8475319926873858,σ2 (I+ n
B,0.8478976234003657,"σ2 ΛS)−1ΛS(ΦT
SΦS−nI)
j
(I+ n"
B,0.8482632541133455,σ2 ΛS)−1ΛSξ 2 = 1 σ2j (I+ n
B,0.8486288848263254,σ2 ΛS)−1+ γ
B,0.8489945155393053,"2 Λ
1−γ"
S,0.8493601462522852,"2
S

A(I+ n"
S,0.8497257769652651,"σ2 ΛS)−1+γΛ1−γ
S
j−1
A(I+ n"
S,0.850091407678245,σ2 ΛS)−1+ γ
S,0.8504570383912249,"2 Λ
−γ"
S,0.8508226691042048,"2
S
ΛSξ

2"
S,0.8511882998171847,≤n(1−t)(−1+ γ
S,0.8515539305301645,2 +(−1+γ)(j−1)) ˜O(n
S,0.8519195612431444,j(1+α+2τ−(1+2τ+2α)t)
S,0.8522851919561243,"2α
−jγ(1−t))∥(I+ n"
S,0.8526508226691042,σ2 ΛS)−1+ γ
S,0.8530164533820841,"2 Λ
1−γ"
S,0.853382084095064,"2
S
ξ∥2"
S,0.8537477148080439,= ˜O(n−γ
S,0.8541133455210238,2 (1−t)+ (1−α+2τ−(1+2τ)t)j
S,0.8544789762340037,"2α
)∥(I+ n"
S,0.8548446069469835,σ2 ΛS)−1+ γ
S,0.8552102376599634,"2 Λ
1−γ"
S,0.8555758683729433,"2
S
∥2∥ξ∥2"
S,0.8559414990859232,= ˜O(n−γ
S,0.8563071297989031,2 (1−t)+ (1−α+2τ−(1+2τ)t)j
S,0.856672760511883,"2α
)O(n(−1+γ/2)(1−t))"
S,0.8570383912248629,= ˜O(n−(1−t)+ (1−α+2τ−(1+2τ)t)j
S,0.8574040219378428,"2α
).
(131)
Using (129), (130) and (131), we have"
S,0.8577696526508227,∥(I+ 1
S,0.8581352833638025,"σ2 ΛSΦT
SΦS)−1ΛSξ∥2 = "
S,0.8585009140767824,"˜O(n−(1−t))+ ∞
X"
S,0.8588665447897623,"j=1
˜O(n−1+ (1−α+2τ−(1+2τ)t)j 2α
)  "
S,0.8592321755027422,"=

˜O(n−(1−t))+ ˜O(n−1+ 1−α+2τ−(1+2τ)t"
S,0.8595978062157221,"2α
)
"
S,0.859963436928702,= ˜O(n−(1−t)). (132)
S,0.8603290676416819,Published as a conference paper at ICLR 2022
S,0.8606946983546618,"By Corollary 20, with probability of at least 1−δ, we have"
S,0.8610603290676416,∥ΦS(I+ 1
S,0.8614259597806215,"σ2 ΛSΦT
SΦS)−1ΛSξ∥2 = ˜O( r (1"
S,0.8617915904936014,δ +1)n∥(I+ 1
S,0.8621572212065813,"σ2 ΛSΦT
SΦS)−1ΛSξ∥2) = ˜O( r (1"
S,0.8625228519195612,δ +1)n·n−(1−t)).
S,0.8628884826325411,From (128) we get ∥(I + 1
S,0.863254113345521,"σ2 ΦSΛSΦT
S)−1fS(x)∥2 = ˜O(
q ( 1"
S,0.863619744058501,"δ +1)n · n−(1−t)). This concludes the
proof."
S,0.8639853747714809,"Lemma 41. Assume σ2 = Θ(nt) where 1 −
α
1+2τ < t < 1.
Let δ = n−q where 0 ≤q <"
S,0.8643510054844606,[α−(1+2τ)(1−t)](2β−1)
S,0.8647166361974405,"4α2
. Under Assumptions 4, 5 and 6, assume that µ0 = 0. Let R = n( 1"
S,0.8650822669104204,α +κ)(1−t)
S,0.8654478976234004,where 0<κ< α−1−2τ+(1+2τ)t
S,0.8658135283363803,"2α2(1−t)
. Then with probability of at least 1−6δ over sample inputs (xi)n
i=1,"
S,0.8661791590493602,we have G2(Dn)= (1+o(1))
S,0.8665447897623401,"2σ2
∥(I+ n"
S,0.86691042047532,"σ2 ΛR)−1µR∥2
2 = 1"
S,0.8672760511882999,"σ2 Θ(nmax{−2(1−t), (1−2β)(1−t)"
S,0.8676416819012797,"α
}logk/2n), where"
S,0.8680073126142596,"k=
0,
2α̸=2β−1,
1,
2α=2β−1.."
S,0.8683729433272395,"Proof of Lemma 41. Let S = nD. Let G2,S(Dn) = E(xn+1,yn+1)(T2,S(Dn+1) −T2,S(Dn)). By"
S,0.8687385740402194,"Lemma 33, when S >nmax{1,
−t
(α−1−2τ) } with probability of at least 1−3δ we have that
|G2(Dn)−G2,S(Dn)|=|E(xn+1,yn+1)[T2(Dn+1)−T2,S(Dn+1)]−[T2(Dn)−T2,S(Dn)]|"
S,0.8691042047531993,"=
E(xn+1,yn+1) ˜O

( 1"
S,0.8694698354661792,δ +1) 1
S,0.8698354661791591,"σ2 (n+1)Smax{1/2−β,1−α+2τ}
−˜O

( 1"
S,0.870201096892139,δ +1) 1
S,0.8705667276051189,"σ2 nSmax{1/2−β,1−α+2τ}"
S,0.8709323583180987,"= ˜O

( 1"
S,0.8712979890310786,δ +1) 1
S,0.8716636197440585,"σ2 nSmax{1/2−β,1−α+2τ}
(133)"
S,0.8720292504570384,"(134)
Let Λ1:S = diag{λ1,...,λS}, Φ1:S = (φ1(x),φ1(x),...,φS(x)) and µ1:S = (µ1,...,µS). Since
µ0 = 0, we have T2,S(Dn) =
1
2σ2 µT
1:SΦT
1:S(I +
1
σ2 Φ1:SΛ1:SΦT
1:S)−1Φ1:Sµ1:S. Deﬁne η1:S =
(φ1(xn+1),...,φS(xn+1))T and eΦ1:S =(ΦT
1:S,η1:S)T . In the proof of Lemma 34, we showed that"
S,0.8723948811700183,"T2,S(Dn)= 1"
S,0.8727605118829982,"2σ2 µT
1:SΦT
1:S(I+ 1"
S,0.8731261425959781,"σ2 Φ1:SΛ1:SΦT
1:S)−1Φ1:Sµ1:S = 1"
S,0.873491773308958,"2µT
1:SΛ−1
1:Sµ1:S−1"
S,0.8738574040219378,"2µT
1:SΛ−1
1:S(I+ 1"
S,0.8742230347349177,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S."
S,0.8745886654478976,"We have
G2,S(Dn)=E(xn+1,yn+1)(T2,S(Dn+1)−T2,S(Dn))"
S,0.8749542961608775,"=E(xn+1,yn+1) 1"
S,0.8753199268738574,"2µT
1:SΛ−1
1:Sµ1:S−1"
S,0.8756855575868373,"2µT
1:SΛ−1
1:S(I+ 1"
S,0.8760511882998172,"σ2 Λ1:S eΦT
S eΦS)−1µ1:S  −
1"
S,0.8764168190127971,"2µT
1:SΛ−1
1:Sµ1:S−1"
S,0.876782449725777,"2µT
1:SΛ−1
1:S(I+ 1"
S,0.8771480804387568,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S)
"
S,0.8775137111517367,"=E(xn+1,yn+1) 1"
S,0.8778793418647166,"2µT
1:SΛ−1
1:S(I+ 1"
S,0.8782449725776965,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S−1"
S,0.8786106032906764,"2µT
1:SΛ−1
1:S(I+ 1"
S,0.8789762340036563,"σ2 Λ1:S eΦT
S eΦS)−1µ1:S "
S,0.8793418647166362,"=E(xn+1,yn+1)  1"
S,0.8797074954296161,"2σ2 µT
1:SΛ−1
1:S
(I+ 1"
S,0.880073126142596,"σ2 Λ1:SΦT
1:SΦ1:S)−1Λ1:Sη1:SηT
1:S(I+ 1"
S,0.8804387568555758,"σ2 Λ1:SΦT
1:SΦ1:S)−1 1+ 1"
S,0.8808043875685557,"σ2 ηT
1:S(I+ 1"
S,0.8811700182815356,"σ2 Λ1:SΦT
1:SΦ1:S)−1Λ1:Sη1:S
µ1:S)
"
S,0.8815356489945155,"=E(xn+1,yn+1)  1"
S,0.8819012797074954,"2σ2
µT
1:S(I+ 1"
S,0.8822669104204753,"σ2 ΦT
1:SΦ1:SΛ1:S)−1η1:SηT
1:S(I+ 1"
S,0.8826325411334552,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S
1+ 1"
S,0.8829981718464351,"σ2 ηT
1:S(I+ 1"
S,0.883363802559415,"σ2 Λ1:SΦT
1:SΦ1:S)−1Λ1:Sη1:S
)
"
S,0.8837294332723948,"=E(xn+1,yn+1)"
S,0.8840950639853747,1+o(1)
S,0.8844606946983546,"2σ2
µT
1:S(I+ 1"
S,0.8848263254113345,"σ2 ΦT
1:SΦ1:SΛ1:S)−1η1:SηT
1:S(I+ 1"
S,0.8851919561243144,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S "
S,0.8855575868372944,= 1+o(1)
S,0.8859232175502743,"2σ2
µT
1:S(I+ 1"
S,0.8862888482632542,"σ2 ΦT
1:SΦ1:SΛ1:S)−1(I+ 1"
S,0.886654478976234,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S"
S,0.8870201096892139,= 1+o(1)
S,0.8873857404021938,"2σ2
∥(I+ 1"
S,0.8877513711151737,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S∥2
2,
(135)"
S,0.8881170018281536,Published as a conference paper at ICLR 2022
S,0.8884826325411335,"where in the fourth to last equality we used the Sherman–Morrison formula, in the third inequality
we used (118) , and in the last equality we used the fact that E(xn+1,yn+1)η1:SηT
1:S =I."
S,0.8888482632541134,"Let ˆµ1:R =(µ1,...,µR,0,...,0)∈RS. Then we have"
S,0.8892138939670933,∥(I+ 1
S,0.8895795246800732,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S∥2 ≤∥(I+ 1"
S,0.889945155393053,"σ2 Λ1:SΦT
1:SΦ1:S)−1 ˆµ1:R∥2+∥(I+ 1"
S,0.8903107861060329,"σ2 Λ1:SΦT
1:SΦ1:S)−1(µ1:S−ˆµ1:R)∥2,"
S,0.8906764168190128,∥(I+ 1
S,0.8910420475319927,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S∥2 ≥∥(I+ 1"
S,0.8914076782449726,"σ2 Λ1:SΦT
1:SΦ1:S)−1 ˆµ1:R∥2−∥(I+ 1"
S,0.8917733089579525,"σ2 Λ1:SΦT
1:SΦ1:S)−1(µ1:S−ˆµ1:R)∥2.
(136)
Let R = n( 1"
S,0.8921389396709324,α +κ)(1−t) where 0 < κ < α−1−2τ+(1+2τ)t
S,0.8925045703839123,"2α2(1−t)
. In Lemma 29, (62), we showed that with
probability of at least 1−δ,"
S,0.8928702010968922,∥(I+ 1
S,0.893235831809872,"σ2 Λ1:RΦT
1:RΦ1:R)−1µ1:R∥2 =Θ(n(1−t)max{−1, 1−2β"
S,0.8936014625228519,2α }logk/2n)
S,0.8939670932358318,=(1+o(1))∥(I+ n
S,0.8943327239488117,"σ2 Λ1:R)−1µ1:R∥2,
(137)"
S,0.8946983546617916,"where k =
0,
2α̸=2β−1,
1,
2α=2β−1.. The same proof holds if we replace Φ1:R with Φ1:S, Λ1:R with Λ1:S,"
S,0.8950639853747715,and µ1:R with ˆµ1:R. We have
S,0.8954296160877514,∥(I+ 1
S,0.8957952468007313,"σ2 Λ1:SΦT
1:SΦ1:S)−1 ˆµ1:R∥2 =Θ(n(1−t)max{−1, 1−2β"
S,0.8961608775137112,2α }logk/2n)
S,0.896526508226691,=(1+o(1))∥(I+ n
S,0.8968921389396709,"σ2 Λ1:S)−1 ˆµ1:R∥2.
(138)"
S,0.8972577696526508,"Next we bound ∥(I +
1
σ2 Λ1:SΦT
1:SΦ1:S)−1(µ1:S −ˆµ1:R)∥2.
By Assumption 5, we have that
∥µ1:S−ˆµ1:R∥2 =O(R
1−2β"
S,0.8976234003656307,"2
). For any ξ ∈RS and ∥ξ∥2 =1, using the Woodbury matrix identity, with
probability of at least 1−2δ we have"
S,0.8979890310786106,|ξT (I+ 1
S,0.8983546617915905,"σ2 Λ1:SΦT
1:SΦ1:S)−1(µ1:S−ˆµ1:R)|"
S,0.8987202925045704,"=|ξT

I−1"
S,0.8990859232175503,"σ2 Λ1:SΦT
1:S(I+ 1"
S,0.8994515539305301,"σ2 Φ1:SΛ1:SΦT
1:S)−1Φ1:S"
S,0.89981718464351,"
(µ1:S−ˆµ1:R)|"
S,0.9001828153564899,=|ξT (µ1:S−ˆµ1:R)−1
S,0.9005484460694698,"σ2 ξT Λ1:SΦT
1:S(I+ 1"
S,0.9009140767824497,"σ2 Φ1:SΛ1:SΦT
1:S)−1Φ1:S(µ1:S−ˆµ1:R)|"
S,0.9012797074954296,≤∥ξ∥2∥µ1:S−ˆµ1:R∥2+ 1
S,0.9016453382084095,"σ2 |ξT Λ1:SΦT
1:S(I+ 1"
S,0.9020109689213894,"σ2 Φ1:SΛ1:SΦT
1:S)−1Φ1:S(µ1:S−ˆµ1:R)|"
S,0.9023765996343693,"≤O(R
1−2β"
S,0.9027422303473491,"2
)+ 1"
S,0.903107861060329,σ2 ∥(I+ 1
S,0.9034734917733089,"σ2 Φ1:SΛ1:SΦT
1:S)−1Φ1:SΛ1:Sξ∥2∥Φ1:S(µ1:S−ˆµ1:R)∥2"
S,0.9038391224862888,"=O(R
1−2β"
S,0.9042047531992687,"2
)+ 1 σ2 O( r (1"
S,0.9045703839122486,δ +1)n·n−(1−t))O( r (1
S,0.9049360146252285,"δ +1)nR
1−2β 2
) =O((1"
S,0.9053016453382084,"δ +1)R
1−2β 2
),"
S,0.9056672760511884,"where in the second to last step we used Corollary 20 to show ∥Φ1:S(µ1:S −ˆµ1:R)∥2 = O(
q ( 1"
S,0.9060329067641681,"δ +1)nR
1−2β"
S,0.906398537477148,"2
) with probability of at least 1 −δ, and Lemma 40 to show that"
S,0.906764168190128,"∥(I +
1
σ2 Φ1:SΛ1:SΦT
1:S)−1Φ1:SΛ1:Sξ∥2 = O(
q ( 1"
S,0.9071297989031079,δ +1)n · n−1) with probability of at least
S,0.9074954296160878,1−δ. Since R=n( 1
S,0.9078610603290677,"α +κ)(1−t), we have"
S,0.9082266910420476,|ξT (I+ 1
S,0.9085923217550275,"σ2 Λ1:SΦT
1:SΦ1:S)−1(µ1:S−ˆµ1:R)|=O((1"
S,0.9089579524680073,δ +1)n
S,0.9093235831809872,(1−2β)(1−t)
S,0.9096892138939671,"2α
+ (1−2β)(1−t)κ 2
)."
S,0.910054844606947,"Since ξ is arbitrary, we have ∥(I +
1
σ2 Λ1:SΦT
1:SΦ1:S)−1(µ1:S −ˆµ1:R)∥2
=
O(( 1 δ
+"
S,0.9104204753199269,"1)n
(1−2β)(1−t)"
S,0.9107861060329068,"2α
+ (1−2β)(1−t)κ"
S,0.9111517367458867,"2
). Since 0 ≤q < [α−(1+2τ)(1−t)](2β−1)"
S,0.9115173674588666,"4α2
and 0 < κ < α−1−2τ+(1+2τ)t"
S,0.9118829981718465,"2α2(1−t)
,"
S,0.9122486288848263,we can choose κ < α−1−2τ+(1+2τ)t
S,0.9126142595978062,"2α2(1−t)
and κ is arbitrarily close to κ < α−1−2τ+(1+2τ)t"
S,0.9129798903107861,"2α2(1−t)
such that"
S,0.913345521023766,Published as a conference paper at ICLR 2022
S,0.9137111517367459,0≤q< (2β−1)(1−t)κ
S,0.9140767824497258,"2
. Then we have (1−2β)(1−t)κ"
S,0.9144424131627057,"2
+q<0. From (136) and (138), we have"
S,0.9148080438756856,∥(I+ 1
S,0.9151736745886655,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S∥2 =Θ(nmax{−(1−t), (1−2β)(1−t)"
S,0.9155393053016453,"2α
}logk/2n)+O((1"
S,0.9159049360146252,δ +1)n
S,0.9162705667276051,(1−2β)(1−t)
S,0.916636197440585,"2α
+ (1−2β)(1−t)κ 2
)"
S,0.9170018281535649,"=Θ(nmax{−(1−t), (1−2β)(1−t)"
S,0.9173674588665448,"2α
}logk/2n)+O((nq+ (1−2β)(1−t)"
S,0.9177330895795247,"2α
+ (1−2β)(1−t)κ 2
)"
S,0.9180987202925046,"=Θ(nmax{−(1−t), (1−2β)(1−t)"
S,0.9184643510054845,"2α
}logk/2n)"
S,0.9188299817184643,=(1+o(1))∥(I+ n
S,0.9191956124314442,σ2 Λ1:S)−1 ˆµ1:R∥2
S,0.9195612431444241,=(1+o(1))∥(I+ n
S,0.919926873857404,"σ2 ΛR)−1µR∥2.
(139)
Hence G2,S(Dn) = 1+o(1)"
S,0.9202925045703839,2σ2 ∥(I + 1
S,0.9206581352833638,"σ2 Λ1:SΦT
1:SΦ1:S)−1µ1:S∥2
2 = 1"
S,0.9210237659963437,"σ2 Θ(n(1−t)max{−2, 1−2β"
S,0.9213893967093236,"α
}logk/2n).
Then by (133), we have"
S,0.9217550274223034,G2(Dn)= 1
S,0.9221206581352833,"σ2 Θ(nmax{−2(1−t), (1−2β)(1−t)"
S,0.9224862888482632,"α
}logk/2n)+ ˜O

(1"
S,0.9228519195612431,δ +1) n
S,0.923217550274223,"σ2 Smax{1/2−β,1−α+2τ}

."
S,0.9235831809872029,"Choosing S =n
max

1,
−t
(α−1−2τ) ,

1+q+min{2, 2β−1"
S,0.9239488117001828,"α
}
min{β−1/2,α−1−2τ} +1

(1−t)
"
S,0.9243144424131627,", we get the result."
S,0.9246800731261426,Proof of Theorem 9. From Lemmas 39 and 41 and 1
S,0.9250457038391224,"α −1 > −2, we have that with probability of at
least 1−7˜δ,"
S,0.9254113345521023,EϵG(Dn)=1+o(1)
S,0.9257769652650822,"2σ2
(Tr(I+ n"
S,0.9261425959780621,"σ2 ΛR)−1ΛR−∥Λ1/2
R (I+ n"
S,0.926508226691042,"σ2 ΛR)−1∥2
F +∥(I+ n"
S,0.926873857404022,"σ2 ΛR)−1µR∥2
2) = 1"
S,0.9272394881170019,σ2 Θ(n
S,0.9276051188299818,(1−α)(1−t)
S,0.9279707495429617,"α
)+ 1"
S,0.9283363802559415,"σ2 Θ(nmax{−2(1−t), (1−2β)(1−t)"
S,0.9287020109689214,"α
}logk/2n) = 1"
S,0.9290676416819013,σ2 Θ(nmax{ (1−α)(1−t)
S,0.9294332723948812,"α
, (1−2β)(1−t)"
S,0.9297989031078611,"α
})
(140)"
S,0.930164533820841,"where k=
0,
2α̸=2β−1
1,
2α=2β−1, and R=n( 1"
S,0.9305301645338209,"α +κ)(1−t), κ>0."
S,0.9308957952468008,"Furthermore, we have"
S,0.9312614259597807,Tr(I+ n
S,0.9316270566727605,σ2 Λ)−1Λ−Tr(I+ n
S,0.9319926873857404,"σ2 ΛR)−1ΛR = ∞
X p=R+1"
S,0.9323583180987203,"λp
1+ n"
S,0.9327239488117002,"σ2 λp
≤ ∞
X p=R+1 Cλp−α 1+ n"
S,0.9330895795246801,"σ2 Cλp−α ≤ ∞
X"
S,0.93345521023766,"p=R+1
Cλp−α = n"
S,0.9338208409506399,σ2 O(R1−α)
S,0.9341864716636198,=O(n(1−α)(1−t)( 1
S,0.9345521023765996,α +κ)) =o(n
S,0.9349177330895795,"(1−α)(1−t) α
)."
S,0.9352833638025594,"Then we have
Tr(I+ n"
S,0.9356489945155393,σ2 ΛR)−1ΛR =Tr(I+ n
S,0.9360146252285192,"σ2 Λ)−1Λ(1+o(1)).
(141)"
S,0.9363802559414991,Similarly we can prove
S,0.936745886654479,"∥Λ1/2
R (I+ n"
S,0.9371115173674589,"σ2 ΛR)−1∥2
F =∥Λ1/2(I+ n"
S,0.9374771480804388,"σ2 Λ)−1∥2
F (1+o(1))
(142)"
S,0.9378427787934186,∥(I+ n
S,0.9382084095063985,"σ2 ΛR)−1µR∥2
2 =∥(I+ n"
S,0.9385740402193784,"σ2 Λ)−1µ∥2
2(1+o(1))
(143)"
S,0.9389396709323583,"Letting δ=7˜δ, the proof is complete."
S,0.9393053016453382,"In the case of µ0 >0, we have the following lemma:"
S,0.9396709323583181,Lemma 42. Let δ = n−q where 0 ≤q < [α−(1+2τ)(1−t)](2β−1)
S,0.940036563071298,"4α2
. Under Assumptions 4, 5 and 6,
assume that µ0 > 0. Then with probability of at least 1 −6δ over sample inputs (xi)n
i=1, we have
G2(Dn)=
1
2σ2 µ2
0+o(1)."
S,0.9404021937842779,Published as a conference paper at ICLR 2022
S,0.9407678244972578,"Proof of Lemma 42. Let S = nD. Let G2,S(Dn) = E(xn+1,yn+1)(T2,S(Dn+1) −T2,S(Dn)). By"
S,0.9411334552102376,"Lemma 33, when S >nmax{1,
−t
(α−1−2τ) }, with probability of at least 1−3δ we have that"
S,0.9414990859232175,"|G2(Dn)−G2,S(Dn)|=|E(xn+1,yn+1)[T2(Dn+1)−T2,S(Dn+1)]−[T2(Dn)−T2,S(Dn)]|"
S,0.9418647166361974,"=
E(xn+1,yn+1) ˜O

( 1"
S,0.9422303473491773,δ +1) 1
S,0.9425959780621572,"σ2 (n+1)Smax{1/2−β,1−α+2τ}
−˜O

( 1"
S,0.9429616087751371,δ +1) 1
S,0.943327239488117,"σ2 nSmax{1/2−β,1−α+2τ}"
S,0.9436928702010969,"= ˜O

( 1"
S,0.9440585009140768,δ +1) 1
S,0.9444241316270566,"σ2 nSmax{1/2−β,1−α+2τ}"
S,0.9447897623400365,"Let ΛS = diag{λ1, ... , λS}, ΦS = (φ1(x), φ1(x), ... , φS(x)) and µS = (µ1, ... , µS). Deﬁne
ηS = (φ0(xn+1),φ1(xn+1),...,φS(xn+1))T and eΦS = (ΦT
S,ηS)T . By the same technique as in the
proof of Lemma 34, we replace ΛR by ˜Λϵ,R =diag{ϵ,λ1,...,λR}, let ϵ→0 and show the counterpart
of the result (135) in the proof of Lemma 41:"
S,0.9451553930530164,"G2,S(Dn)=E(xn+1,yn+1)(T2,S(Dn+1)−T2,S(Dn))"
S,0.9455210237659963,"=E(xn+1,yn+1)  1"
S,0.9458866544789762,"2σ2
µT
S(I+ 1"
S,0.9462522851919561,"σ2 ΦT
SΦSΛS)−1ηSηT
S (I+ 1"
S,0.946617915904936,"σ2 ΛSΦT
SΦS)−1µS
1+ 1"
S,0.946983546617916,"σ2 ηT
S (I+ 1"
S,0.9473491773308957,"σ2 ΛSΦT
SΦS)−1ΛSηS
)
"
S,0.9477148080438756,"=E(xn+1,yn+1)"
S,0.9480804387568555,1+o(1)
S,0.9484460694698355,"2σ2
µT
S(I+ 1"
S,0.9488117001828154,"σ2 ΦT
SΦSΛS)−1ηSηT
S (I+ 1"
S,0.9491773308957953,"σ2 ΛSΦT
SΦS)−1µS "
S,0.9495429616087752,= 1+o(1)
S,0.9499085923217551,"2σ2
µT
S(I+ 1"
S,0.950274223034735,"σ2 ΦT
SΦSΛS)−1(I+ 1"
S,0.9506398537477148,"σ2 ΛSΦT
SΦS)−1µS"
S,0.9510054844606947,= 1+o(1)
S,0.9513711151736746,"2σ2
∥(I+ 1"
S,0.9517367458866545,"σ2 ΛSΦT
SΦS)−1µS∥2
2,
(144)
where in the fourth to last equality we used the Sherman–Morrison formula, in the third inequality
we used (118) , and in the last equality we used the fact that E(xn+1,yn+1)η1:SηT
1:S =I."
S,0.9521023765996344,"Let ˆµR =(µ0,µ1,...,µR,0,...,0)∈RS. Then we have"
S,0.9524680073126143,∥(I+ 1
S,0.9528336380255942,"σ2 ΛSΦT
SΦS)−1µS∥2 ≤∥(I+ 1"
S,0.9531992687385741,"σ2 ΛSΦT
SΦS)−1 ˆµR∥2+∥(I+ 1"
S,0.953564899451554,"σ2 ΛSΦT
SΦS)−1(µS−ˆµR)∥2,"
S,0.9539305301645338,∥(I+ 1
S,0.9542961608775137,"σ2 ΛSΦT
SΦS)−1µS∥2 ≥∥(I+ 1"
S,0.9546617915904936,"σ2 ΛSΦT
SΦS)−1 ˆµR∥2−∥(I+ 1"
S,0.9550274223034735,"σ2 ΛSΦT
SΦS)−1(µS−ˆµR)∥2.
(145)
Choose R=n( 1"
S,0.9553930530164534,α +κ)(1−t) where 0<κ< α−1−2τ+(1+2τ)t
S,0.9557586837294333,"α2(1−t)
. In Lemma 29, (62), we showed that with
probability of at least 1−δ,"
S,0.9561243144424132,∥(I+ 1
S,0.9564899451553931,"σ2 Λ1:RΦT
1:RΦ1:R)−1µ1:R∥2 =Θ(n(1−t)max{−1, 1−2β"
S,0.956855575868373,2α }logk/2n)
S,0.9572212065813528,=(1+o(1))∥(I+ n
S,0.9575868372943327,"σ2 Λ1:R)−1µ1:R∥2,
(146)"
S,0.9579524680073126,"where k =
0,
2α̸=2β−1,
1,
2α=2β−1.. The same proof holds if we replace Φ1:R with Φ1:S, Λ1:R with Λ1:S,"
S,0.9583180987202925,and µ1:R with ˆµ1:R. We have
S,0.9586837294332724,∥(I+ 1
S,0.9590493601462523,"σ2 Λ1:SΦT
1:SΦ1:S)−1 ˆµ1:R∥2 =Θ(n(1−t)max{−1, 1−2β"
S,0.9594149908592322,2α }logk/2n)
S,0.9597806215722121,=(1+o(1))∥(I+ n
S,0.9601462522851919,"σ2 Λ1:S)−1 ˆµ1:R∥2.
(147)"
S,0.9605118829981718,So we have
S,0.9608775137111517,∥(I+ 1
S,0.9612431444241316,"σ2 ΛSΦT
SΦS)−1 ˆµR∥2 =µ0+Θ(n(1−t)max{−1, 1−2β"
S,0.9616087751371115,2α }logk/2n)
S,0.9619744058500914,"=µ0+o(1).
(148)"
S,0.9623400365630713,"Next we bound ∥(I +
1
σ2 ΛSΦT
SΦS)−1(µS −ˆµR)∥2.
By Assumption 5, we have that
∥µS −ˆµR∥2 = O(R
1−2β"
S,0.9627056672760512,"2
).
For any ξ ∈RS and ∥ξ∥2 = 1, using the Woodbury matrix"
S,0.9630712979890311,Published as a conference paper at ICLR 2022
S,0.9634369287020109,"identity, with probability of at least 1−2δ we have"
S,0.9638025594149908,|ξT (I+ 1
S,0.9641681901279707,"σ2 ΛSΦT
SΦS)−1(µS−ˆµR)|"
S,0.9645338208409506,"=|ξT

I−1"
S,0.9648994515539305,"σ2 ΛSΦT
S(I+ 1"
S,0.9652650822669104,"σ2 ΦSΛSΦT
S)−1ΦS"
S,0.9656307129798903,"
(µS−ˆµR)|"
S,0.9659963436928702,=|ξT (µS−ˆµR)−1
S,0.9663619744058501,"σ2 ξT ΛSΦT
S(I+ 1"
S,0.9667276051188299,"σ2 ΦSΛSΦT
S)−1ΦS(µS−ˆµR)|"
S,0.9670932358318098,≤∥ξ∥2∥µS−ˆµR∥2+ 1
S,0.9674588665447897,"σ2 |ξT ΛSΦT
S(I+ 1"
S,0.9678244972577696,"σ2 ΦSΛSΦT
S)−1ΦS(µS−ˆµR)|"
S,0.9681901279707495,"≤O(R
1−2β"
S,0.9685557586837295,"2
)+ 1"
S,0.9689213893967094,σ2 ∥(I+ 1
S,0.9692870201096893,"σ2 ΦSΛSΦT
S)−1ΦSΛSξ∥2∥ΦS(µS−ˆµR)∥2"
S,0.969652650822669,"=O(R
1−2β"
S,0.970018281535649,"2
)+ 1 σ2 O( r (1"
S,0.9703839122486289,δ +1)n·n−(1−t))O( r (1
S,0.9707495429616088,"δ +1)nR
1−2β 2
) =O((1"
S,0.9711151736745887,"δ +1)R
1−2β 2
),"
S,0.9714808043875686,"whereinthesecondtolaststepweusedCorollary20toshow∥ΦS(µS−ˆµR)∥2 =O(
q ( 1"
S,0.9718464351005485,"δ +1)nR
1−2β 2
)"
S,0.9722120658135284,"with probability of at least 1 −δ, and Lemma 40 to show that ∥(I + 1"
S,0.9725776965265083,"σ2 ΦSΛSΦT
S)−1ΦSΛSξ∥2 = O(
q ( 1"
S,0.9729433272394881,δ +1)n·n−(1−t)) with probability of at least 1−δ. Since R=n( 1
S,0.973308957952468,"α +κ)(1−t), we have"
S,0.9736745886654479,|ξT (I+ 1
S,0.9740402193784278,"σ2 ΛSΦT
SΦS)−1(µS−ˆµR)|=O((1"
S,0.9744058500914077,δ +1)n
S,0.9747714808043876,(1−2β)(1−t)
S,0.9751371115173675,"2α
+ (1−2β)(1−t)κ 2
)."
S,0.9755027422303474,"Since
ξ
is arbitrary,
we have
∥(I
+
1
σ2 ΛSΦT
SΦS)−1(µS
−
ˆµR)∥2
=
O(( 1 δ
+"
S,0.9758683729433273,"1)n
(1−2β)(1−t)"
S,0.9762340036563071,"2α
+ (1−2β)(1−t)κ"
S,0.976599634369287,"2
). Since 0 ≤q < [α−(1+2τ)(1−t)](2β−1)"
S,0.9769652650822669,"4α2
and 0 < κ < α−1−2τ+(1+2τ)t"
S,0.9773308957952468,"2α2(1−t)
,"
S,0.9776965265082267,we can choose κ < α−1−2τ+(1+2τ)t
S,0.9780621572212066,"2α2(1−t)
and κ is arbitrarily close to κ < α−1−2τ+(1+2τ)t"
S,0.9784277879341865,"2α2(1−t)
such that"
S,0.9787934186471664,0≤q< (2β−1)(1−t)κ
S,0.9791590493601463,"2
. Then we have (1−2β)(1−t)κ"
S,0.9795246800731261,"2
+q<0. From (145) and (148), we have"
S,0.979890310786106,∥(I+ 1
S,0.9802559414990859,"σ2 ΛSΦT
SΦS)−1µS∥2 =µ0+Θ(n(1−t)max{−1, 1−2β"
S,0.9806215722120658,2α }logk/2n)+O((1
S,0.9809872029250457,δ +1)n
S,0.9813528336380256,(1−2β)(1−t)
S,0.9817184643510055,"2α
+ (1−2β)(1−t)κ 2
)"
S,0.9820840950639854,"=µ0+Θ(n(1−t)max{−1, 1−2β"
S,0.9824497257769652,"2α }logk/2n)
=µ0+o(1).
(149)
Hence G2,S(Dn) =
1+o(1)"
S,0.9828153564899451,"2σ2 ∥(I +
1
σ2 ΛSΦT
SΦS)−1µS∥2
2 =
1
2σ2 µ2
0 + o(1).
Then by (144),
G2(Dn)=
1
2σ2 µ2
0+o(1)+ ˜O
 
( 1"
S,0.983180987202925,"δ +1)nSmax{1/2−β,1−α}
."
S,0.9835466179159049,"Choosing S =n
max

1,
−t
(α−1−2τ) ,

1+q+min{2, 2β−1"
S,0.9839122486288848,"α
}
min{β−1/2,α−1−2τ} +1

(1−t)
"
S,0.9842778793418647,", we get the result."
S,0.9846435100548446,"Proof of Theorem 11. According to Lemma 42, G2(Dn) =
1
2σ2 µ2
0 +o(1). By Lemma 39, we have"
S,0.9850091407678245,"G1(Dn)=Θ(n
(1−α)(1−t)"
S,0.9853747714808044,"α
). Then EϵG(Dn)=G1(Dn)+G2(Dn)=
1
2σ2 µ2
0+o(1)."
S,0.9857404021937842,Published as a conference paper at ICLR 2022
S,0.9861060329067641,"D.3
PROOFS RELATED TO THE EXCESS MEAN SQUARED GENERALIZATION ERROR"
S,0.986471663619744,"Proof of Theorem 12. For µ0 =0, we can show that"
S,0.9868372943327239,EϵM(Dn)=EϵExn+1[ ¯m(xn+1)−f(xn+1)]2
S,0.9872029250457038,"=EϵExn+1[Kxn+1x(Kn+σ2
modelIn)−1y−f(xn+1)]2"
S,0.9875685557586837,"=EϵExn+1[ηT ΛΦT [ΦΛΦT +σ2
modelIn)−1(Φµ+ϵ)−ηT µ]2"
S,0.9879341864716636,"=EϵExn+1[ηT ΛΦT (ΦΛΦT +σ2
modelIn)−1ϵ]2"
S,0.9882998171846435,"+Exn+1

ηT  
ΛΦT (ΦΛΦT +σ2
modelIn)−1Φ−I

µ
2"
S,0.9886654478976235,"=σ2
trueTrΛΦT (ΦΛΦT +σ2
modelIn)−2ΦΛ"
S,0.9890310786106032,"+µT 
I+
1
σ2
model ΦT ΦΛ
−1
I+
1
σ2
model ΛΦT Φ
−1
µ"
S,0.9893967093235831,"= σ2
true
σ2
model Tr(I+ ΛΦT Φ"
S,0.989762340036563,"σ2
model )−1Λ−Tr(I+ ΛΦT Φ"
S,0.990127970749543,"σ2
model )−2Λ+∥(I+
1
σ2
model ΛΦT Φ)−1µ∥2
2."
S,0.9904936014625229,"According to (139) from the proof of Lemma 41, the truncation procedure (133) and (143), with
probability of at least 1−δ we have"
S,0.9908592321755028,"∥(I+
1
σ2
model ΛΦT Φ)−1µ∥2
2 =Θ(nmax{−2(1−t), (1−2β)(1−t)"
S,0.9912248628884827,"α
}logk/2n)=(1+o(1))∥(I+
n
σ2
model Λ)−1µ∥2
2,"
S,0.9915904936014626,"where k=
0,
2α̸=2β−1,
1,
2α=2β−1.."
S,0.9919561243144425,"According to (121) and (126) from the proof of Lemma 39, the truncation procedure (115), (141) and
(142), with probability of at least 1−δ we have"
S,0.9923217550274223,Tr(I+ ΛΦT Φ
S,0.9926873857404022,"σ2
model )−1Λ−Tr(I+ ΛΦT Φ"
S,0.9930530164533821,"σ2
model )−2Λ"
S,0.993418647166362,"=

Tr(I+
n
σ2
model Λ)−1Λ

(1+o(1))−∥Λ1/2(I+
n
σ2
model Λ)−1∥2
F (1+o(1))"
S,0.9937842778793419,"=Θ(n
(1−α)(1−t) α
)."
S,0.9941499085923218,Combining the above two equations we get
S,0.9945155393053017,"EϵM(Dn)=(1+o(1))

σ2
true
σ2
model"
S,0.9948811700182816,"
Tr(I+
n
σ2
model Λ)−1Λ−∥Λ1/2(I+
n
σ2
model Λ)−1∥2
F

+∥(I+
n
σ2
model Λ)−1µ∥2
2
"
S,0.9952468007312614,"= σ2
true
σ2
model Θ(n
(1−α)(1−t)"
S,0.9956124314442413,"α
)+Θ(nmax{−2(1−t), (1−2β)(1−t)"
S,0.9959780621572212,"α
}logk/2n)"
S,0.9963436928702011,"=σ2
trueΘ(n
1−α−t"
S,0.996709323583181,"α
)+Θ(nmax{−2(1−t), (1−2β)(1−t)"
S,0.9970749542961609,"α
}logk/2n)"
S,0.9974405850091408,"=Θ

max{σ2
truen
1−α−t"
S,0.9978062157221207,"α
,n
(1−2β)(1−t) α
}
"
S,0.9981718464351006,"When µ0 >0, according to (149) in the proof of Lemma 42 and the truncation procedure (133), with
probability of at least 1−δ we have"
S,0.9985374771480804,EϵM(Dn)=Θ(n
S,0.9989031078610603,(1−α)(1−t)
S,0.9992687385740402,"α
)+µ2
0+o(1)"
S,0.9996343692870201,"=µ2
0+o(1)."
