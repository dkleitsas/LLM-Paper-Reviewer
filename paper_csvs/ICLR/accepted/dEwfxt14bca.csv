Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002109704641350211,"Exploration remains a central challenge for reinforcement learning (RL). Virtu-
ally all existing methods share the feature of a monolithic behaviour policy that
changes only gradually (at best). In contrast, the exploratory behaviours of an-
imals and humans exhibit a rich diversity, namely including forms of switching
between modes. This paper presents an initial study of mode-switching, non-
monolithic exploration for RL. We investigate different modes to switch between,
at what timescales it makes sense to switch, and what signals make for good
switching triggers. We also propose practical algorithmic components that make
the switching mechanism adaptive and robust, which enables ﬂexibility without
an accompanying hyper-parameter-tuning burden. Finally, we report a promis-
ing and detailed analysis on Atari, using two-mode exploration and switching at
sub-episodic time-scales."
INTRODUCTION,0.004219409282700422,"1
INTRODUCTION"
INTRODUCTION,0.006329113924050633,"The trade-off between exploration and exploitation is described as the crux of learning and behaviour
across many domains, not just reinforcement learning (Sutton & Barto, 2018), but also in decision
making (Cohen et al., 2007), evolutionary biology (Cremer et al., 2019), ecology (Kembro et al.,
2019), neuroscience (e.g., focused versus diffuse search in visual attention (Wolfe et al., 1989),
dopamine regulations (Chakroun et al., 2020)), cognitive sciences (Hills et al., 2015), as well as
psychology and psychiatry (Addicott et al., 2017). In a nutshell, exploration is about the balance
between taking the familiar choice that is known to be rewarding and learning about unfamiliar
options of uncertain reward, but which could ultimately be more valuable than the familiar options."
INTRODUCTION,0.008438818565400843,"Ample literature has studied the question of how much to explore, that is how to set the overall
trade-off (and how to adjust it over the course of learning) (Jaksch et al., 2010; Capp´e et al., 2013;
Lattimore & Szepesv´ari, 2020; Thrun, 1992), and the question of how to explore, namely how
to choose exploratory actions (e.g., randomly, optimistically, intrinsically motivated, or otherwise)
(Schmidhuber, 1991; Oudeyer & Kaplan, 2009; Linke et al., 2019). In contrast, the question of when
to explore has been studied very little, possibly because it does not arise in bandit problems, where
a lot of exploration methods are rooted. The ‘when’ question and its multiple facets are the subjects
of this paper. We believe that addressing it could lead to more intentional forms of exploration."
INTRODUCTION,0.010548523206751054,"Consider an agent that has access to two modes of behaviour, an ‘explore’ mode and an ‘exploit’
mode (e.g., a random policy and a greedy policy, as in ε-greedy). Even when assuming that the
overall proportion of exploratory steps is ﬁxed, the agent still has multiple degrees of freedom: it
can explore more at the beginning of training and less in later phases; it may take single exploratory
steps or execute prolonged periods of exploration; it may prefer exploratory steps early or late within
an episode; and it could trigger the onset (or end) of an exploratory period based on various criteria.
Animals and humans exhibit non-trivial behaviour in all of these dimensions, presumably encoding
useful inductive biases that way (Power, 1999). Humans make use of multiple effective strategies,
such as selectively exploring options with high uncertainty (a form of directed, or information-
seeking exploration), and increasing the randomness of their choices when they are more uncertain
(Gershman, 2018; Gershman & Tzovaras, 2018; Ebitz et al., 2019). Monkeys use directed explo-
ration to manage explore-exploit trade-offs, and these signals are coded in motivational brain regions
(Costa et al., 2019). Patients with schizophrenia register changes in directed exploration and expe-"
INTRODUCTION,0.012658227848101266,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014767932489451477,"rience low-grade inﬂammation when shifting from exploitation to random exploration (Waltz et al.,
2020; Cathomas et al., 2021). This diversity is what motivates us to study which of these can beneﬁt
RL agents in turn, by expanding the class of exploratory behaviours beyond the commonly used
monolithic ones (where modes are merged homogeneously in time)."
METHODS,0.016877637130801686,"2
METHODS"
METHODS,0.0189873417721519,"The objective of an RL agent is to learn a policy that maximises external reward. At the high level,
it achieves this by interleaving two processes: generating new experience by interacting with the
environment using a behaviour policy (exploration) and updating its policy using this experience
(learning). As RL is applied to increasingly ambitious tasks, the challenge for exploration becomes
to keep producing diverse experience, because if something has not been encountered, it cannot
be learned. Our central argument is therefore simple: a monolithic, time-homogeneous behaviour
policy is strictly less diverse than a heterogeneous mode-switching one, and the former may ham-
string the agent’s performance. As an illustrative example, consider a human learning how to ride a
bike (explore), while maintaining their usual happiness through food, sleep, work (exploit): there is
a stark contrast between a monolithic, time-homogeneous behaviour that interleaves a twist of the
handlebar or a turn of a pedal once every few minutes or so, and the mode-switching behaviour that
dedicates prolonged periods of time exclusively to acquiring the new skill of cycling."
METHODS,0.02109704641350211,"While the choice of behaviour in pure exploit mode is straightforward, namely the greedy pursuit
of external reward (or best guess thereof), denoted by G, there are numerous viable choices for
behaviour in a pure explore mode (denoted by X). In this paper we consider two standard ones:
XU, the naive uniform random policy, and XI, an intrinsically motivated behaviour that exclusively
pursues a novelty measure based on random network distillation (RND, (Burda et al., 2018)). See
Section 4 and Appendix B for additional possibilities of X. In this paper we choose ﬁxed behaviours
for these modes, and focus solely on the question of when to switch between them. In our setting,
overall proportion of exploratory steps (the how much), denoted by pX , is not directly controlled but
derives from the when."
GRANULARITY,0.023206751054852322,"2.1
GRANULARITY"
GRANULARITY,0.02531645569620253,"An exploration period is an uninterrupted sequence of steps in explore mode. We consider four
choices of temporal granularity for exploratory periods, also illustrated in Figure 1."
GRANULARITY,0.027426160337552744,"Step-level exploration is the most common scenario, where the decision to explore is taken inde-
pendently at each step, affecting one action.1 The canonical example is ε-greedy (Fig.1:C)."
GRANULARITY,0.029535864978902954,"Experiment-level exploration is the other extreme, where all behaviour during training is produced
in explore mode, and learning is off-policy (the greedy policy is only used for evaluation). This
scenario is also very common, with most forms of intrinsic motivation falling into this category,
namely pursuing reward with an intrinsic bonus throughout training (Fig.1:A).2"
GRANULARITY,0.03164556962025317,"Episode-level exploration is the case where the mode is ﬁxed for an entire episode at a time (e.g.,
training games versus tournament matches in a sport), see Fig.1:B. This has been investigated for
simple cases, where the policy’s level of stochasticity is sampled at the beginning of each episode
(Horgan et al., 2018; Kapturowski et al., 2019; Zha et al., 2021)."
GRANULARITY,0.03375527426160337,"Intra-episodic exploration is what falls in-between step- and episode-level exploration, where ex-
ploration periods last for multiple steps, but less than a full episode. This is the least commonly
studied scenario, and will form the bulk of our investigations (Fig.1:D,E,F,G)."
GRANULARITY,0.035864978902953586,"We denote the length of an exploratory period by nX (and similarly nG for exploit mode). To
characterise granularity, our summary statistic of choice is medX := median(nX ). Note that there
are two possible units for these statistics: the raw steps or the proportion of the episode length L."
GRANULARITY,0.0379746835443038,"1The length of an exploratory period tends to be short, but it can be greater than 1, as multiple consecutive
step-wise decisions to explore can create longer periods.
2Note that it is also possible to interpret ε-greedy as experiment-level exploration, where the X policy is
ﬁxed to a noisy version of G."
GRANULARITY,0.04008438818565401,Published as a conference paper at ICLR 2022
GRANULARITY,0.04219409282700422,"Figure 1: Illustration of different types of temporal structure for two-mode exploration. Left: Each
line A-G depicts an excerpt of an experiment (black lines show episode boundaries, experiment
continues on the right), with colour denoting the active mode (blue is exploit, magenta is explore).
A is of experiment-level granularity, B episode-level, C step-level, and D-G are of intra-episodic
exploration granularity. Right: The same examples, mapped onto a characteristic plot of summary
statistics: overall exploratory proportion pX versus typical length of an exploratory period medX .
The yellow-shaded area highlights the intra-episodic part of space studied in this paper (some points
are not realisable, e.g., when pX ≈1 then medX must be large). C, D, E, F share the same
pX ≈0.2, while interleaving exploration modes in different ways. D and E share the same medX
value, and differ only on whether exploration periods are spread out, or happen toward the end."
GRANULARITY,0.04430379746835443,"The latter has different (relative) semantics, but may be more appropriate when episode lengths vary
widely across training. We denote it as rmedX := median(nX /L)."
SWITCHING MECHANISMS,0.046413502109704644,"2.2
SWITCHING MECHANISMS"
SWITCHING MECHANISMS,0.04852320675105485,"Granularity is but the coarsest facet of the ‘when’ question, but the more precise intra-episode tim-
ings matter too, namely when exactly to start and when to stop an exploratory period. This section
introduces two mechanisms, blind and informed switching. It is worth highlighting that, in general,
the mechanism (or its time resolution) for entering explore mode differs from the one for exiting it
(to enter exploit mode) – this asymmetry is crucial to obtain ﬂexible overall amounts of exploration.
If switching were symmetric, the proportion would be pX ≈0.5."
SWITCHING MECHANISMS,0.05063291139240506,"Blind switching
The simplest switching mechanism does not take any state into account (thus, we
call it blind) and is only concerned with producing switches at some desired time resolution. It can
be implemented deterministically through a counter (e.g., enter explore mode after 100 exploit mode
steps), or probabilistically (e.g., at each step, enter explore mode with probability 0.01). Its expected
duration can be parameterised in terms of raw steps, or in terms of fractional episode length. The
opposite of blind switching is informed switching, as discussed below."
SWITCHING MECHANISMS,0.052742616033755275,"Informed switching
Going beyond blind switching opens up another rich set of design choices,
with switching informed by the agent’s internal state. There are two parts: ﬁrst, a scalar trigger
signal is produced by the agent at each step, using its current information – drawing inspiration
from human behaviour, we view the triggering signal as a proxy for uncertainty (Schulz et al., 2019):
when uncertainty is high, the agent will switch to explore. Second, a binary switching decision is
taken based on the trigger signal (for example, by comparing it to a threshold). Again, triggering
will generally not be symmetric between entering and exiting an exploratory period."
SWITCHING MECHANISMS,0.05485232067510549,"To keep this paper focused, we will look at one such informed trigger, dubbed ‘value promise dis-
crepancy’ (see Appendix B for additional competitive variants). This is an online proxy of how
much of the reward that the agent’s past value estimate promised (k steps ago) have actually come
about. The intuition is that in uncertain parts of state space, this discrepancy will generally be larger
than when everything goes as expected. Formally,"
SWITCHING MECHANISMS,0.056962025316455694,"Dpromise(t −k, t) :="
SWITCHING MECHANISMS,0.05907172995780591,"V (st−k) − k−1
X"
SWITCHING MECHANISMS,0.06118143459915612,"i=0
γiRt−i −γkV (st) "
SWITCHING MECHANISMS,0.06329113924050633,"where V (s) is the agent’s value estimate at state s, R is the reward, and γ is a discount factor."
SWITCHING MECHANISMS,0.06540084388185655,Published as a conference paper at ICLR 2022
SWITCHING MECHANISMS,0.06751054852320675,"Starting mode
When periods last for a signiﬁcant fraction of episode length, it also matters how
the sequence is initialised, i.e., whether an episode starts in explore or in exploit mode, or more
generally, whether the agent explores more early in an episode or more later on. It is conceivable
that the best choice among these is domain dependent (see Figure 6): in most scenarios, the states
at the beginning of an episode have been visited many times, thus starting with exploit mode can be
beneﬁcial; in other domains however, early actions may disproportionately determine the available
future paths (e.g., build orders in StarCraft (Churchill & Buro, 2011))."
FLEXIBILITY WITHOUT ADDED BURDEN,0.06962025316455696,"2.3
FLEXIBILITY WITHOUT ADDED BURDEN"
FLEXIBILITY WITHOUT ADDED BURDEN,0.07172995780590717,"Our approach introduces additional ﬂexibility to the exploration process, even when holding the
speciﬁcs of the learning algorithm and the exploration mode ﬁxed. To prevent this from turning into
an undue hyper-parameter tuning burden, we recommend adding two additional mechanisms."
FLEXIBILITY WITHOUT ADDED BURDEN,0.07383966244725738,"Bandit adaptation
The two main added degrees of freedom in our intra-episodic switching set-up
are when (or how often) to enter explore mode, and when (or how quickly) to exit it. These can be
parameterised by a duration, termination probability or target rate (see Section 3.1). In either case,
we propose to follow Schaul et al. (2019) and Badia et al. (2020a), and delegate the adaptation of
these settings to a meta-controller, which is implemented as a non-stationary multi-armed bandit
that maximises episodic return. As an added beneﬁt, the ‘when’ of exploration can now become
adaptive to both the task, and the stage of learning."
FLEXIBILITY WITHOUT ADDED BURDEN,0.0759493670886076,"Homeostasis In practice, the scales of the informed trigger signals may vary substantially across
domains and across training time. For example, the magnitude of Dpromise will depend on reward
scales and density and can decrease over time as accuracy improves (the signals could also be noisy).
This means that naively setting a threshold hyper-parameter is impractical. For a simple remedy, we
have taken inspiration from neuroscience (Turrigiano & Nelson, 2004) to add homeostasis to the
binary switching mechanism, which tracks recent values of the signal and adapts the threshold for
switching so that a speciﬁc average target rate is obtained. This functions as an adaptive threshold,
making tuning straightforward because the target rate of switching is conﬁgured independently of
the scales of the trigger signal. See Appendix A for the details of the implementation."
RESULTS,0.07805907172995781,"3
RESULTS"
RESULTS,0.08016877637130802,"The design space we propose contains a number of atypical ideas for how to structure exploration.
For this reason, we opted to keep the rest of our experimental setup very conventional, and include
multiple comparable baselines, ablations and variations."
RESULTS,0.08227848101265822,"Setup: R2D2 on Atari
We conduct our investigations on a subset of games of the Atari Learn-
ing Environment (Bellemare et al., 2013), a common benchmark for the study of exploration. All
experiments are conducted across 7 games (FROSTBITE, GRAVITAR, H.E.R.O., MONTEZUMA’S
REVENGE, MS. PAC-MAN, PHOENIX, STAR GUNNER), the ﬁrst 5 of which are classiﬁed as hard
exploration games (Bellemare et al., 2016), using 3 seeds per game. For our agent, we use the R2D2
architecture (Kapturowski et al., 2019), which is a modern, distributed version of DQN (Mnih et al.,
2015) that employs a recurrent network to approximate its Q-value function. This is a common basis
used in exploration studies (Dabney et al., 2020; Badia et al., 2020b;a). The only major modiﬁcation
to conventional R2D2 is its exploration mechanism, where instead we implement all the variants of
mode-switching introduced in Section 2. Separately from the experience collected for learning, we
run an evaluator process that assesses the performance of the current greedy policy. This is what we
report in all our performance curves (see Appendix A for more details)."
RESULTS,0.08438818565400844,"Baselines
There are a few simple baselines worth comparing to, namely the pure explore mode
(pX = 1, Fig.1:A) and the pure exploit mode (pX = 0), as well as the step-wise interleaved ε-greedy
execution (Fig.1:C), where pX = 0.01 = ε (without additional episodic or intra-episodic structure).
Given its wide adoption in well-tuned prior work, we expect the latter to perform well overall. The
fourth baseline picks a mode for an entire episode at a time (Fig.1:B), with the probability of pick-
ing X being adapted by a bandit meta-controller. We denote these as experiment-level-X,
experiment-level-G, step-level-0.01 and episode-level-* respectively.
For
each of these, we have a version with uniform (XU) and intrinsic (XI) explore mode."
RESULTS,0.08649789029535865,Published as a conference paper at ICLR 2022
RESULTS,0.08860759493670886,Figure 2: Illustrating the space of design decisions for intra-episodic exploration (see also Figure 9).
VARIANTS OF INTRA-EPISODIC EXPLORATION,0.09071729957805907,"3.1
VARIANTS OF INTRA-EPISODIC EXPLORATION"
VARIANTS OF INTRA-EPISODIC EXPLORATION,0.09282700421940929,"As discussed in Section 2, there are multiple dimensions along which two-mode intra-episodic ex-
ploration can vary. The concrete ones for our experiments are:"
VARIANTS OF INTRA-EPISODIC EXPLORATION,0.0949367088607595,"• Explore mode: uniform random XU, or RND intrinsic reward XI (denoted XU and XI)."
VARIANTS OF INTRA-EPISODIC EXPLORATION,0.0970464135021097,"• Explore duration (nX ): this can be a ﬁxed number of steps (1, 10, 100), or one of these
is adaptively picked by a bandit (denoted by *), or the switching is symmetric between
entering and exiting explore mode (denoted by =)."
VARIANTS OF INTRA-EPISODIC EXPLORATION,0.09915611814345991,"• Trigger type: either blind or informed (based on value promise, see Section 2.2)."
VARIANTS OF INTRA-EPISODIC EXPLORATION,0.10126582278481013,"• Exploit duration (nG): for blind triggers, the exploit duration can be parameterised by ﬁxed
number of steps (10, 100, 1000, 10000), indirectly deﬁned by a probability of terminating
(0.1, 0.01, 0.001, 0.0001), or adaptively picked by a bandit over these choices (denoted by
n* or p*, respectively). For informed triggers, the exploit duration is indirectly parame-
terised by a target rate in (0.1, 0.01, 0.001, 0.0001), or a bandit over them (p*), which is in
turn transformed into an adaptive switching threshold by homeostasis (Section 2.2)."
VARIANTS OF INTRA-EPISODIC EXPLORATION,0.10337552742616034,• Starting mode: G greedy (default) or X explore (denoted by G or X).
VARIANTS OF INTRA-EPISODIC EXPLORATION,0.10548523206751055,"We can concisely refer to a particular instance by a tuple that lists these choices. For example,
XU-intra(100,informed,p*,X) denotes uniform random exploration XU, with ﬁxed 100-
step explore periods, triggered by the value-promise signal at a bandit-determined rate, and starting
in explore mode. See Figure 2 for an illustration."
PERFORMANCE RESULTS,0.10759493670886076,"3.2
PERFORMANCE RESULTS"
PERFORMANCE RESULTS,0.10970464135021098,"We start by reporting overall performance results, to reassure the reader that our method is viable
(and convince them to keep reading the more detailed and qualitative results in the following sec-
tions). Figure 3 shows performance across 7 Atari games according to two human-normalised aggre-
gation metrics (mean and median), comparing one form of intra-episodic exploration to all the base-
lines, separately for each explore mode (XU and XI). The headline result is that intra-episodic ex-
ploration improves over both step-level and episode-level baselines (as well as the pure experiment-
level modes that we would not expect to be very competitive). The full learning curves per game are
found in the appendix, and show scores on hard exploration games like MONTEZUMA’S REVENGE
or PHOENIX that are also competitive in absolute terms (at our compute budget of 2B frames)."
PERFORMANCE RESULTS,0.11181434599156118,"Note that there is a subtle difference to the learning setups between XU and XI, as the latter requires
training a separate head to estimate intrinsic reward values. This is present even in pure exploit
mode, where it acts as an auxiliary task only (Jaderberg et al., 2016), hence the differences in pure
greedy curves in Figure 3. For details, see Appendix A."
DIVERSITY RESULTS,0.11392405063291139,"3.3
DIVERSITY RESULTS"
DIVERSITY RESULTS,0.1160337552742616,"In a study like ours, the emphasis is not on measuring raw performance, but rather on characterising
the diversity of behaviours arising from the spectrum of proposed variants. A starting point is to
return to Figure 1 (right), and assess how much of the previously untouched space is now ﬁlled"
DIVERSITY RESULTS,0.11814345991561181,Published as a conference paper at ICLR 2022
DIVERSITY RESULTS,0.12025316455696203,"0.2
0.4
0.6
0.8
1.0
frames
1e9 0 1 2 3 4"
DIVERSITY RESULTS,0.12236286919831224,human-normalized score Mean
DIVERSITY RESULTS,0.12447257383966245,"XU-experiment-level-X
XU-experiment-level-G
XU-step-level-0.01
XU-episode-level-*
XU-intra(10,informed,p*,X)"
DIVERSITY RESULTS,0.12658227848101267,"0.2
0.4
0.6
0.8
1.0
frames
1e9 0.00 0.25 0.50 0.75 1.00 1.25 1.50"
DIVERSITY RESULTS,0.12869198312236288,"Median
Uniform random exploration"
DIVERSITY RESULTS,0.1308016877637131,"0.2
0.4
0.6
0.8
1.0
frames
1e9 0 1 2 3 4"
DIVERSITY RESULTS,0.13291139240506328,human-normalized score Mean
DIVERSITY RESULTS,0.1350210970464135,"XI-experiment-level-X
XI-experiment-level-G
XI-step-level-X+G
XI-episode-level-*
XI-intra(10,informed,p*,X)"
DIVERSITY RESULTS,0.1371308016877637,"0.2
0.4
0.6
0.8
1.0
frames
1e9 0.00 0.25 0.50 0.75 1.00 1.25 1.50"
DIVERSITY RESULTS,0.13924050632911392,"Median
Intrinsic reward exploration"
DIVERSITY RESULTS,0.14135021097046413,"Figure 3: Human-normalized performance results aggregated over 7 Atari games and 3 seeds, com-
paring the four levels of exploration granularity. Left two: uniform explore mode XU. Right two:
RND intrinsic reward explore mode XI. In each case, the baselines are pure modes X and G, step-
level switching with ε-greedy, and episodic switching (with a bandit-adapted proportion). Note that
the XI-step-level experiment uses both an intrinsic and an extrinsic reward, as in Burda et al. (2018)."
DIVERSITY RESULTS,0.14345991561181434,"0.0
0.2
0.4
0.6
0.8
1.0
rmed 10
2 10
1 p"
DIVERSITY RESULTS,0.14556962025316456,frostbite
DIVERSITY RESULTS,0.14767932489451477,"0.0
0.2
0.4
0.6
0.8
1.0
rmed"
DIVERSITY RESULTS,0.14978902953586498,gravitar
DIVERSITY RESULTS,0.1518987341772152,"0.0
0.2
0.4
0.6
0.8
1.0
rmed"
DIVERSITY RESULTS,0.1540084388185654,montezuma_revenge
DIVERSITY RESULTS,0.15611814345991562,"0.0
0.2
0.4
0.6
0.8
1.0
rmed"
DIVERSITY RESULTS,0.15822784810126583,phoenix 0 10000 20000 30000
DIVERSITY RESULTS,0.16033755274261605,Episode return
DIVERSITY RESULTS,0.16244725738396623,frostbite 0 2000 4000 6000
DIVERSITY RESULTS,0.16455696202531644,gravitar 0 1000 2000
DIVERSITY RESULTS,0.16666666666666666,montezuma_revenge 0 50000
DIVERSITY RESULTS,0.16877637130801687,100000
DIVERSITY RESULTS,0.17088607594936708,phoenix
DIVERSITY RESULTS,0.1729957805907173,"XU-episode-level-*
XU-intra(10,informed,0.001,G)
XU-intra(10,informed,p*,G)
XU-intra(p*,informed,p*,G)
XU-intra(=,informed,p*,G)
XU-intra(=,blind,n*,G)
XU-intra(n*,blind,n*,G)"
DIVERSITY RESULTS,0.1751054852320675,"0.0
0.2
0.4
0.6
0.8
1.0
rmed 10
2 10
1 p"
DIVERSITY RESULTS,0.17721518987341772,frostbite
DIVERSITY RESULTS,0.17932489451476794,"0.0
0.2
0.4
0.6
0.8
1.0
rmed"
DIVERSITY RESULTS,0.18143459915611815,gravitar
DIVERSITY RESULTS,0.18354430379746836,"0.0
0.2
0.4
0.6
0.8
1.0
rmed"
DIVERSITY RESULTS,0.18565400843881857,montezuma_revenge
DIVERSITY RESULTS,0.1877637130801688,"0.0
0.2
0.4
0.6
0.8
1.0
rmed"
DIVERSITY RESULTS,0.189873417721519,phoenix 0 2000 4000 6000 8000 10000
DIVERSITY RESULTS,0.19198312236286919,Episode return
DIVERSITY RESULTS,0.1940928270042194,frostbite 0 2000 4000 6000
DIVERSITY RESULTS,0.1962025316455696,gravitar 0 200 400 600
DIVERSITY RESULTS,0.19831223628691982,montezuma_revenge 0
DIVERSITY RESULTS,0.20042194092827004,100000
DIVERSITY RESULTS,0.20253164556962025,200000
DIVERSITY RESULTS,0.20464135021097046,300000
DIVERSITY RESULTS,0.20675105485232068,phoenix
DIVERSITY RESULTS,0.2088607594936709,"XI-episode-level-*
XI-intra(10,informed,0.001,G)
XI-intra(10,informed,p*,G)
XI-intra(p*,informed,p*,G)
XI-intra(=,informed,p*,G)
XI-intra(=,blind,n*,G)
XI-intra(n*,blind,n*,G)"
DIVERSITY RESULTS,0.2109704641350211,"Figure 4: Rows 1 and 3: Summary characteristics pX and rmedX of induced exploration behaviour,
for different variants of intra-episodic exploration (and an episodic baseline for comparison), on a
subset of 4 Atari games. Bandit adaptation can change these statistics over time, hence square
and cross markers show averages over ﬁrst and last 10% of training, respectively. Rows 2 and 4:
Corresponding ﬁnal scores (averaged over ﬁnal 10% of training). Error bars show the span between
min and max performance across 3 seeds. Note how different variants cover different parts of
characteristic space, and how the bandit adaptation shifts the statistics into different directions for
different games. See main text for further discussion and Appendix C for other games and variants."
DIVERSITY RESULTS,0.21308016877637131,"by intra-episodic variants, and how the ‘when’ characteristics translate into performance. Figure 4
answers these questions, and raises some new ones. First off, the raw amount of exploration pX
is not a sufﬁcient predictor of performance, implying that the temporal structure matters. It also
shows substantial bandit adaptation at work: compare the exploration statistics at the start (squares)
and end-points of training (crosses), and how these trajectories differ per game; a common pattern
is that reducing pX far below 0.5 is needed for high performance. Interestingly, these adaptations
are similar between XU and XI, despite very different explore modes (and differing performance"
DIVERSITY RESULTS,0.21518987341772153,Published as a conference paper at ICLR 2022
DIVERSITY RESULTS,0.21729957805907174,"Figure 5:
Left and center: Illustration of detailed temporal structure within individual episodes,
on FROSTBITE (top) and GRAVITAR (bottom), contrasting two trigger mechanisms. Each subplot
shows 15 randomly selected episodes (one per row) that share the same overall exploration amount
pX = 0.1. Each vertical bar (magenta) represents an exploration period of ﬁxed length nX =
10; each blue chunk represents an exploitation period. Left: blind, step-based trigger leads to
equally spaced exploration periods. Center: a trigger signal informed by value promise leads to very
different within-episode patterns, with some parts being densely explored, and others remaining in
exploit mode for very long. Right: the corresponding learning curves show a clear performance
beneﬁt for the informed trigger variant (orange) in this particular setting. Appendix C has similar
plots for many more variants and games."
DIVERSITY RESULTS,0.21940928270042195,"Figure 6: Starting mode effect. Final mean episode return for two blind intra-episode experiments
that differ only in start mode, greedy (blue) or explore (orange). Scores are normalised so that 1 is the
maximum result across the two start modes. Either choice can reliably boost or harm performance,
depending on the game. Left: uniform explore mode XU. Right: intrinsic reward explore mode XI."
DIVERSITY RESULTS,0.22151898734177214,Published as a conference paper at ICLR 2022
DIVERSITY RESULTS,0.22362869198312235,"0.0
0.2
0.4
0.6
0.8
1.0
p 0 2000 4000 6000 8000"
DIVERSITY RESULTS,0.22573839662447256,Episode return
DIVERSITY RESULTS,0.22784810126582278,"XU-intra(=,blind,100,G) on frostbite 0.2 0.4 0.6 0.8 1.0"
DIVERSITY RESULTS,0.229957805907173,"0.0
0.2
0.4
0.6
0.8
1.0
p 0 2000 4000 6000 8000"
DIVERSITY RESULTS,0.2320675105485232,Episode return
DIVERSITY RESULTS,0.23417721518987342,"XU-intra(=,blind,0.01,G) on frostbite 0.2 0.4 0.6 0.8 1.0"
DIVERSITY RESULTS,0.23628691983122363,"0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 1000 2000 3000 4000 5000 6000 7000 8000 9000"
DIVERSITY RESULTS,0.23839662447257384,Episode return
DIVERSITY RESULTS,0.24050632911392406,Frostbite
DIVERSITY RESULTS,0.24261603375527427,"XU-intra(=,blind,0.01,G): prob-based
XU-intra(=,blind,100,G): step-based"
DIVERSITY RESULTS,0.24472573839662448,"Figure 7: Left and center: Contrasting the behavioural characteristics between two forms of blind
switching, step-based (left) and probabilistic (center), on the example of FROSTBITE. Each point
is an actor episode, with colour indicating time in training (blue for early, red for late). Note the
higher diversity of pX when switching probabilistically. Right: Corresponding performance curves
indicate that the probabilistic switching (red) has a performance beneﬁt, possibly because it creates
the opportunity for ‘lucky’ episodes with much less randomness in a game where random actions
can easily kill the agent. For more games, please see the Appendix C."
DIVERSITY RESULTS,0.2468354430379747,"results). We would expect prolonged intrinsic exploration periods to be more useful than prolonged
random ones, and indeed, comparing the high-rmedX variant (purple) across XU and XI, it appears
more beneﬁcial for the latter. Zooming in on speciﬁc games, a few results stand out: in XU mode,
the only variant that escapes the inherent local optimum of PHOENIX is the blind, doubly adaptive
one (purple), with the bandits radically shifting the exploration statistics over the course of training.
In contrast, the best results on MONTEZUMA’S REVENGE are produced by the symmetric, informed
trigger variant (light green), which is forced to retain a high pX . Finally, FROSTBITE is the one
game where an informed trigger (red) clearly outperforms its blind equivalent (purple)."
DIVERSITY RESULTS,0.2489451476793249,"These insights are still limited to summary statistics, so Figure 5 looks in more depth at the detailed
temporal structure within episodes (as in Figure 1, left). Here the main comparison is between
blind and informed triggers, illustrating that the characteristics of the ﬁne-grained within-episode
structure can differ massively, despite attaining the same high-level statistics pX and medX . We can
see quite a lot of variation in the trigger structure – the moments we enter exploration are not evenly
spaced anymore. As a bonus, the less rigid structure of the informed trigger (and possibly the more
carefully chosen switch points) end up producing better performance too."
DIVERSITY RESULTS,0.2510548523206751,"Figure 6 sheds light on a complementary dimension, differentiating the effects of starting in explore
or exploit mode. In brief, each of these can be consistently beneﬁcial in some games, and consis-
tently harmful in others. Another observation here is the dynamics of the bandit adaptation: when
starting in exploit mode, it exhibits a preference for long initial exploit periods in many games (up
to 10000 steps), but that effect vanishes when starting in explore mode (see also Appendix C). More
subtle effects arise from the choice of parameterisation of switching rates. Figure 7 shows a stark
qualitative difference on how probabilistic switching differs from step-count based switching, with
the former spanning a much wider diversity of outcomes, which improves performance."
TAKE-AWAYS,0.25316455696202533,"3.4
TAKE-AWAYS"
TAKE-AWAYS,0.2552742616033755,"Summarising the empirical results in this section, two messages stand out. First, there seems to be a
sweet spot in terms of temporal granularity, and intra-episodic exploration is the right step towards
ﬁnding it. Second, the vastly increased design space of our proposed family of methods gives rise to
a large diversity of behavioural characteristics; and this diversity is not superﬁcial, it also translates
to meaningful performance differences, with different effects in different games, which cannot be
reduced to simplistic metrics, such as pX . In addition, we provide some sensible rules-of-thumb for
practitioners willing to join us on the journey of intra-episodic exploration. In general, it is useful
to let a bandit ﬁgure out the precise settings, but it is worth curating its choices to at most a handful.
Jointly using two bandits across factored dimensions is very adaptive, but can sometimes be harmful
when they decrease the signal-to-noise ratio in each other’s learning signal. Finally, the choice of the
uncertainty-based trigger should be informed by the switching modes (see Appendix B for details)."
TAKE-AWAYS,0.25738396624472576,Published as a conference paper at ICLR 2022
DISCUSSION,0.25949367088607594,"4
DISCUSSION"
DISCUSSION,0.2616033755274262,"Time-based exploration control
The emphasis of our paper was on the beneﬁts of heterogeneous
temporal structure in mode-switching exploration. Another potential advantage over monolithic
approaches is that it may be easier to tune hyper-parameters related to an explicit exploration budget
(e.g., via pX ) than to tune an intrinsic reward coefﬁcient, especially if extrinsic reward scales change
across tasks or time, and if the non-stationarity of the intrinsic reward affects its overall scale."
DISCUSSION,0.26371308016877637,"Diversity for diversity’s sake
One role of a general-purpose exploration method is to allow an
agent to get off the ground in a wide variety of domains. While this may clash with sample-efﬁcient
learning on speciﬁc domains, we believe that the former objective will come to dominate in the long
run. In this light, methods that exhibit more diverse behaviour are preferable for that reason alone
because they are more likely to escape local optima or misaligned priors."
DISCUSSION,0.26582278481012656,"Related work
While not the most common approach to exploration in RL, we are aware of some
notable work investigating non-trivial temporal structure. The ϵz-greedy algorithm (Dabney et al.,
2020) initiates contiguous chunks of directed behaviour (‘ﬂights’) with the length sampled from a
heavy-tailed distribution. In contrast to our proposal, these ﬂights act with a single constant action
instead of invoking an explore mode. Campos et al. (2021) pursue a similar idea, but with ﬂights
along pre-trained coverage policies, while Ecoffet et al. (2021) chain a ‘return-to-state’ policy to an
explore mode. Maybe closest to our XI setting is the work of Bagot et al. (2020), where periods of
intrinsic reward pursuit are explicitly invoked by the agent. Exploration with gradual change instead
of abrupt mode switches generally appears at long time-scales, such as when pursuing intrinsic
rewards (Schmidhuber, 2010; Oudeyer & Kaplan, 2009) but can also be effective at shorter time-
scales, e.g., Never-Give-Up (Badia et al., 2020b). Related work on the question of which states to
prefer for exploration decisions (Tokic, 2010) tends not to consider prolonged exploratory periods."
DISCUSSION,0.2679324894514768,"Relation to options
Ideas related to switching behaviours at intra-episodic time scales are well-
known outside of the context of exploration. In the options framework in hierarchical RL, the goal is
to chain together a sequence of sub-behaviours into a reward-maximising policy (Sutton et al., 1999;
Mankowitz et al., 2016). Some work has looked at using options for exploration too (Jinnai et al.,
2019a; Bougie & Ichise, 2021). In its full generality, the options framework is a substantially more
ambitious endeavour than our proposal, as it requires learning a full state-dependent hierarchical
policy that picks which option to start (and when), as well as jointly learning the options themselves."
DISCUSSION,0.270042194092827,"Limitations
Our proposed approach inherits many typical challenges for exploration methods,
such as sample efﬁciency or trading off risk. An aspect that is particular to the intra-episode switch-
ing case is the different nature of the off-policy-ness. The resulting effective policy can produce
state distributions that differ substantially from those of either of the two base mode behaviours that
are being interleaved. It can potentially visit parts of the state space that neither base policy would
reach if followed from the beginning of the episode. While a boon for exploration, this might pose a
challenge to learning, as it could require off-policy corrections that treat those states differently and
do not only correct for differences in action space. Our paper does not use (non-trivial) off-policy
correction (see Appendix A) as our initial experimentation showed it is not an essential component
in the current setting (see Figure 15). We leave this intriguing ﬁnding for future investigation."
DISCUSSION,0.2721518987341772,"Future work
With the dimensions laid out in Section 2, it should be clear that this paper can
but scratch the surface. We see numerous opportunities for future work, some of which we already
carried out initial investigations (see Appendix B). For starters, the mechanism could go beyond
two-mode and switch between exploit, explore, novelty and mastery (Thomaz & Breazeal, 2008),
or between many diverse forms of exploration, such as levels of optimism (Derman et al., 2020;
Moskovitz et al., 2021). Triggers could be expanded or reﬁned by using different estimations of
uncertainty, such as ensemble discrepancy (Wiering & Van Hasselt, 2008; Buckman et al., 2018),
amortised value errors (Flennerhag et al., 2020), or density models (Bellemare et al., 2016; Ostrovski
et al., 2017); or other signals, such as salience (Downar et al., 2002), minimal coverage (Jinnai et al.,
2019a;b), or empowerment (Klyubin et al., 2005; Gregor et al., 2016; Houthooft et al., 2016)."
DISCUSSION,0.2742616033755274,"Conclusion
We have presented an initial study of intra-episodic exploration, centred on the sce-
nario of switching between an explore and an exploit mode. We hope this has broadened the avail-
able forms of temporal structure in behaviour, leading to more diverse, adaptive and intentional
forms of exploration, in turn enabling RL to scale to ever more complex domains."
DISCUSSION,0.27637130801687765,Published as a conference paper at ICLR 2022
DISCUSSION,0.27848101265822783,ACKNOWLEDGMENTS
DISCUSSION,0.2805907172995781,"We would like to thank Audrunas Gruslys, Simon Osindero, Eszter V´ertes, David Silver, Dan Hor-
gan, Zita Marinho, Katrina McKinney, Claudia Clopath, David Amos, V´ıctor Campos, Remi Munos,
and the entire DeepMind team for discussions and support, and especially Pablo Sprechmann and
Luisa Zintgraf for detailed feedback on an earlier version."
REFERENCES,0.28270042194092826,REFERENCES
REFERENCES,0.2848101265822785,"M. A. Addicott, J. M. Pearson, M. M. Sweitzer, D. L. Barack, and M. L. Platt. A Primer on Foraging
and the Explore/Exploit Trade-Off for Psychiatry Research. Neuropsychopharmacology, 42(10):
1931–1939, Sep 2017."
REFERENCES,0.2869198312236287,"Adri`a Puigdom`enech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,
Daniel Guo, and Charles Blundell. Agent57: Outperforming the Atari human benchmark, 2020a."
REFERENCES,0.2890295358649789,"Adri`a Puigdom`enech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven
Kapturowski, Olivier Tieleman, Mart´ın Arjovsky, Alexander Pritzel, Andew Bolt, and Charles
Blundell. Never give up: Learning directed exploration strategies, 2020b."
REFERENCES,0.2911392405063291,"Louis Bagot, Kevin Mets, and Steven Latr´e. Learning intrinsically motivated options to stimulate
policy exploration, 2020."
REFERENCES,0.29324894514767935,"Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:
253–279, 2013."
REFERENCES,0.29535864978902954,"Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and R´emi
Munos. Unifying count-based exploration and intrinsic motivation. In Neural Information Pro-
cessing Systems, 2016."
REFERENCES,0.2974683544303797,"Nicolas Bougie and Ryutaro Ichise. Fast and slow curiosity for high-level exploration in reinforce-
ment learning. Applied Intelligence, 51(2):1086–1107, 2021."
REFERENCES,0.29957805907172996,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax."
REFERENCES,0.30168776371308015,"Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee.
Sample-
efﬁcient reinforcement learning with stochastic ensemble value expansion.
arXiv preprint
arXiv:1807.01675, 2018."
REFERENCES,0.3037974683544304,"David Budden, Matteo Hessel, Iurii Kemaev, Stephen Spencer, and Fabio Viola. Chex: Testing
made fun, in jax!, 2020a. URL http://github.com/deepmind/chex."
REFERENCES,0.3059071729957806,"David Budden, Matteo Hessel, John Quan, Steven Kapturowski, Kate Baumli, Surya Bhupatiraju,
Aurelia Guy, and Michael King. RLax: Reinforcement Learning in JAX, 2020b. URL http:
//github.com/deepmind/rlax."
REFERENCES,0.3080168776371308,"Yuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network
distillation. CoRR, abs/1810.12894, 2018."
REFERENCES,0.310126582278481,"V´ıctor Campos, Pablo Sprechmann, Steven Hansen, Andre Barreto, Steven Kapturowski, Alex
Vitvitskyi, Adri`a Puigdom`enech Badia, and Charles Blundell. Coverage as a principle for discov-
ering transferable behavior in reinforcement learning. arXiv preprint arXiv:2102.13515, 2021."
REFERENCES,0.31223628691983124,"Olivier Capp´e, Aur´elien Garivier, Odalric-Ambrym Maillard, R´emi Munos, Gilles Stoltz, et al.
Kullback–leibler upper conﬁdence bounds for optimal sequential allocation. Annals of Statis-
tics, 41(3):1516–1541, 2013."
REFERENCES,0.3143459915611814,Published as a conference paper at ICLR 2022
REFERENCES,0.31645569620253167,"Albin Cassirer, Gabriel Barth-Maron, Thibault Sottiaux, Manuel Kroiss, and Eugene Brevdo. Re-
verb: An efﬁcient data storage and transport system for ml research, 2020.
URL https:
//github.com/deepmind/reverb."
REFERENCES,0.31856540084388185,"Flurin Cathomas, Federica Klaus, Karoline Guetter, Hui-Kuan Chung, Anjali Raja Beharelle, To-
bias R. Spiller, Rebecca Schlegel, Erich Seifritz, Matthias N. Hartmann-Riemer, Philippe N. To-
bler, and Stefan Kaiser. Increased random exploration in schizophrenia is associated with inﬂam-
mation. npj Schizophrenia, 7(1):6, Feb 2021."
REFERENCES,0.3206751054852321,"K. Chakroun, D. Mathar, A. Wiehler, F. Ganzer, and J. Peters. Dopaminergic modulation of the
exploration/exploitation trade-off in human decision-making. Elife, 9, 06 2020."
REFERENCES,0.3227848101265823,"David Churchill and Michael Buro. Build order optimization in starcraft. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence and Interactive Digital Entertainment, volume 6(1),
2011."
REFERENCES,0.32489451476793246,"Jonathan D Cohen, Samuel M McClure, and Angela J Yu. Should I stay or should I go? How the
human brain manages the trade-off between exploitation and exploration. Philosophical Trans-
actions of the Royal Society B: Biological Sciences, 362(1481):933–942, 2007."
REFERENCES,0.3270042194092827,"Vincent D. Costa, Andrew R. Mitz, and Bruno B. Averbeck. Subcortical substrates of explore-exploit
decisions in primates. Neuron, 103(3):533–545.e5, August 2019."
REFERENCES,0.3291139240506329,"Jonas Cremer, Tomoya Honda, Ying Tang, Jerome Wong-Ng, Massimo Vergassola, and Terence
Hwa. Chemotaxis as a navigation strategy to boost range expansion. Nature, 575(7784):658–663,
11 2019."
REFERENCES,0.33122362869198313,"Will Dabney, Georg Ostrovski, and Andr´e Barreto.
Temporally-extended ϵ-greedy exploration,
2020."
REFERENCES,0.3333333333333333,"Esther Derman, Daniel Mankowitz, Timothy Mann, and Shie Mannor. A bayesian approach to robust
reinforcement learning. In Uncertainty in Artiﬁcial Intelligence, pp. 648–658. PMLR, 2020."
REFERENCES,0.33544303797468356,"Jonathan Downar, Adrian P Crawley, David J Mikulis, and Karen D Davis. A cortical network
sensitive to stimulus salience in a neutral behavioral context across multiple sensory modalities.
Journal of neurophysiology, 87(1):615–620, 2002."
REFERENCES,0.33755274261603374,"R. Becket Ebitz, Brianna J. Sleezer, Hank P. Jedema, Charles W. Bradberry, and Benjamin Y. Hay-
den. Tonic exploration governs both ﬂexibility and lapses. PLOS Computational Biology, 15(11):
e1007475, November 2019."
REFERENCES,0.339662447257384,"Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Go-explore: a
new approach for hard-exploration problems, 2021."
REFERENCES,0.34177215189873417,"Sebastian Flennerhag, Jane X Wang, Pablo Sprechmann, Francesco Visin, Alexandre Galashov,
Steven Kapturowski, Diana L Borsa, Nicolas Heess, Andre Barreto, and Razvan Pascanu. Tem-
poral difference uncertainties as a signal for exploration. arXiv preprint arXiv:2010.02255, 2020."
REFERENCES,0.3438818565400844,"Samuel J. Gershman. Deconstructing the human algorithms for exploration. Cognition, 173:34–42,
2018. ISSN 0010-0277."
REFERENCES,0.3459915611814346,"Samuel J. Gershman and Bastian Greshake Tzovaras. Dopaminergic genes are associated with both
directed and random exploration. bioRxiv, 2018."
REFERENCES,0.34810126582278483,"Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016."
REFERENCES,0.350210970464135,"Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020.
URL http://github.com/deepmind/dm-haiku."
REFERENCES,0.35232067510548526,"Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom Hennigan.
Optax: Composable gradient transformation and optimisation, in JAX!, 2020. URL http://
github.com/deepmind/optax."
REFERENCES,0.35443037974683544,Published as a conference paper at ICLR 2022
REFERENCES,0.35654008438818563,"T. T. Hills, P. M. Todd, D. Lazer, A. D. Redish, and I. D. Couzin. Exploration versus exploitation in
space, mind, and society. Trends Cogn Sci, 19(1):46–54, Jan 2015."
REFERENCES,0.35864978902953587,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.36075949367088606,"Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt,
and David Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933,
2018."
REFERENCES,0.3628691983122363,"Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. arXiv preprint arXiv:1605.09674, 2016."
REFERENCES,0.3649789029535865,"Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397, 2016."
REFERENCES,0.3670886075949367,"Thomas Jaksch, Ronald Ortner, and Peter Auer.
Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(4), 2010."
REFERENCES,0.3691983122362869,"Yuu Jinnai, Jee Won Park, David Abel, and George Konidaris. Discovering options for exploration
by minimizing cover time. In International Conference on Machine Learning, pp. 3130–3139.
PMLR, 2019a."
REFERENCES,0.37130801687763715,"Yuu Jinnai, Jee Won Park, Marlos C Machado, and George Konidaris. Exploration in reinforcement
learning with deep covering options. In International Conference on Learning Representations,
2019b."
REFERENCES,0.37341772151898733,"Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent ex-
perience replay in distributed reinforcement learning. In International Conference on Learning
Representations, 2019."
REFERENCES,0.3755274261603376,"J. M. Kembro, M. Lihoreau, J. Garriga, E. P. Raposo, and F. Bartumeus. Bumblebees learn foraging
routes through exploitation-exploration cycles. J R Soc Interface, 16(156):20190103, 07 2019."
REFERENCES,0.37763713080168776,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.379746835443038,"Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Empowerment: A universal agent-
centric measure of control. In 2005 IEEE Congress on Evolutionary Computation, volume 1, pp.
128–135. IEEE, 2005."
REFERENCES,0.3818565400843882,"Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.38396624472573837,"Cam Linke, Nadia M. Ady, Martha White, Thomas Degris, and Adam White. Adapting behaviour
via intrinsic reward: A survey and empirical study, 2019."
REFERENCES,0.3860759493670886,"Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018."
REFERENCES,0.3881856540084388,"Daniel J Mankowitz, Timothy Arthur Mann, and Shie Mannor. Adaptive skills adaptive partitions
(ASAP). In Neural Information Processing Systems, 2016."
REFERENCES,0.39029535864978904,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015."
REFERENCES,0.3924050632911392,"Ted Moskovitz, Jack Parker-Holder, Aldo Pacchiano, and Michael Arbel. Deep reinforcement learn-
ing with dynamic optimism. arXiv preprint arXiv:2102.03765, 2021."
REFERENCES,0.39451476793248946,"Georg Ostrovski, Marc G. Bellemare, A¨aron van den Oord, and R´emi Munos. Count-based explo-
ration with neural density models. CoRR, abs/1703.01310, 2017."
REFERENCES,0.39662447257383965,Published as a conference paper at ICLR 2022
REFERENCES,0.3987341772151899,"Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computa-
tional approaches. Frontiers in neurorobotics, 1:6, 2009."
REFERENCES,0.4008438818565401,"Thomas G Power. Play and exploration in children and animals. Psychology Press, 1999."
REFERENCES,0.4029535864978903,"Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In
International Conference on Learning Representations, Puerto Rico, 2016."
REFERENCES,0.4050632911392405,"Tom Schaul, Diana Borsa, David Ding, David Szepesvari, Georg Ostrovski, Will Dabney, and Simon
Osindero. Adapting behaviour for learning progress, 2019."
REFERENCES,0.40717299578059074,"Tom Schaul, Georg Ostrovski, Iurii Kemaev, and Diana Borsa. Return-based scaling: Yet another
normalisation trick for deep RL. arXiv preprint arXiv:2105.05347, 2021."
REFERENCES,0.4092827004219409,"J¨urgen Schmidhuber. Curious model-building control systems. In Proc. international joint confer-
ence on neural networks, pp. 1458–1463, 1991."
REFERENCES,0.41139240506329117,"J¨urgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990–2010). IEEE
Transactions on Autonomous Mental Development, 2(3):230–247, 2010."
REFERENCES,0.41350210970464135,"Eric Schulz, Rahul Bhui, Bradley C. Love, Bastien Brier, Michael T. Todd, and Samuel J. Gershman.
Structured, uncertainty-driven exploration in real-world consumer choice. Proceedings of the
National Academy of Sciences, 116(28):13903–13908, June 2019."
REFERENCES,0.41561181434599154,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.4177215189873418,"Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A frame-
work for temporal abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–
211, 1999."
REFERENCES,0.41983122362869196,"Andrea L Thomaz and Cynthia Breazeal. Experiments in socially guided exploration: Lessons
learned in building robots that learn with and without human teachers. Connection Science, 20
(2-3):91–110, 2008."
REFERENCES,0.4219409282700422,"Sebastian B Thrun. Efﬁcient exploration in reinforcement learning, 1992."
REFERENCES,0.4240506329113924,"Michel Tokic. Adaptive ε-greedy exploration in reinforcement learning based on value differences.
In Annual Conference on Artiﬁcial Intelligence, pp. 203–210. Springer, 2010."
REFERENCES,0.42616033755274263,"Gina G Turrigiano and Sacha B Nelson. Homeostatic plasticity in the developing nervous system.
Nature reviews neuroscience, 5(2):97–107, 2004."
REFERENCES,0.4282700421940928,"James A. Waltz, Robert C. Wilson, Matthew A. Albrecht, Michael J. Frank, and James M. Gold.
Differential effects of psychotic illness on directed and random exploration. Computational Psy-
chiatry, 4(0):18, August 2020."
REFERENCES,0.43037974683544306,"Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In Maria Florina Balcan and Kilian Q.
Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, vol-
ume 48 of Proceedings of Machine Learning Research, pp. 1995–2003, New York, New York,
USA, 20–22 Jun 2016. PMLR."
REFERENCES,0.43248945147679324,"Marco A Wiering and Hado Van Hasselt. Ensemble algorithms in reinforcement learning. IEEE
Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 38(4):930–936, 2008."
REFERENCES,0.4345991561181435,"J. M. Wolfe, K. R. Cave, and S. L. Franzel. Guided search: an alternative to the feature integration
model for visual search. J Exp Psychol Hum Percept Perform, 15(3):419–433, 1989."
REFERENCES,0.43670886075949367,"Daochen Zha, Wenye Ma, Lei Yuan, Xia Hu, and Ji Liu. Rank the episodes: A simple approach
for exploration in procedurally-generated environments. In International Conference on Learning
Representations, 2021."
REFERENCES,0.4388185654008439,Published as a conference paper at ICLR 2022
REFERENCES,0.4409282700421941,"A
DETAILED EXPERIMENTAL SETUP"
REFERENCES,0.4430379746835443,"A.1
ATARI ENVIRONMENT"
REFERENCES,0.4451476793248945,"We use a selection of games from the widely used Atari Learning Environment (ALE, Bellemare
et al. (2013)). It is conﬁgured to not expose the ‘life-loss’ signal, and use the full action set (18
discrete actions) for all games (not the per-game reduced effective action spaces). We also use the
sticky-action randomisation as in (Machado et al., 2018). Episodes time-out after 108k frames (i.e.
30 minutes of real-time game play)."
REFERENCES,0.4472573839662447,"Differently from most past Atari RL agents following DQN (Mnih et al., 2015), our agent uses the
raw 210 × 160 RGB frames as input to its value function (one at a time, without frame stacking),
though it still applies a max-pool operation over the most recent 2 frames to mitigate ﬂickering
inherent to the Atari simulator. As in most past work, an action-repeat of 4 is applied, over which
rewards are summed."
REFERENCES,0.44936708860759494,"A.2
AGENT"
REFERENCES,0.45147679324894513,"The agent used in our Atari experiments is a distributed implementation of a value- and replay-based
RL algorithm derived from the Recurrent Replay Distributed DQN (R2D2) architecture (Kaptur-
owski et al., 2019). This system comprises of a ﬂeet of 120 CPU-based actors (combined with a
single TPU for batch inference) concurrently generating experience and feeding it to a distributed
experience replay buffer, and a single TPU-based learner randomly sampling batches of experience
sequences from replay and performing updates of the recurrent value function by gradient descent
on a suitable RL loss."
REFERENCES,0.45358649789029537,"The value function is represented by a convolutional torso feeding into a linear layer, followed by
a recurrent LSTM (Hochreiter & Schmidhuber, 1997) core, whose output is processed by a further
linear layer before ﬁnally being output via a Dueling value head (Wang et al., 2016). The exact
parameterisation follows the slightly modiﬁed R2D2 presented by Dabney et al. (2020) and Schaul
et al. (2021). Refer to Table 1 for a full list of hyper-parameters. It is trained via stochastic gra-
dient descent on a multi-step TD loss (more precisely, a 5-step Q-learning loss) with the use of
a periodically updated target network (Mnih et al., 2015) for bootstrap target computation, using
mini-batches of sampled replay sequences. Replay sampling is performed using prioritized expe-
rience replay (Schaul et al., 2016) with priorities computed from sequences’ TD errors following
the scheme introduced by Kapturowski et al. (2019). As in R2D2, sequences of 80 observations
are used for replay, with a preﬁx of 20 observations used for burn-in. In a slight deviation from the
original, our agent uses a ﬁxed replay ratio of 1, i.e. the learner or actors get throttled dynamically
if the average number of times a sample gets replayed exceeds or falls below this value; this makes
experiments more reproducible and stable."
REFERENCES,0.45569620253164556,"Actors periodically pull the most recent network parameters from the learner to be used in their ex-
ploratory policy. In addition to feeding the replay buffer, all actors periodically report their reward,
discount and return histories to the learner, which then calculates running estimates of reward, dis-
count and return statistics to perform return-based scaling (Schaul et al., 2021). If applicable, the
episodic returns from the actors are also sent to the non-stationary bandit(s) that adapt the distribu-
tion over exploration parameters (e.g., target ratios ρ or period lengths nX ). In return, the bandit(s)
provide samples from that distribution to each actor at the start of a new episode, just like Schaul
et al. (2019)."
REFERENCES,0.4578059071729958,"Our agent is implemented with JAX (Bradbury et al., 2018), uses the Haiku (Hennigan et al., 2020),
Optax (Budden et al., 2020b), Chex (Budden et al., 2020a), and RLax (Hessel et al., 2020) libraries
for neural networks, optimisation, testing, and RL losses, respectively, and Reverb (Cassirer et al.,
2020) for distributed experience replay."
REFERENCES,0.459915611814346,"A.3
TRAINING AND EVALUATION PROTOCOLS"
REFERENCES,0.4620253164556962,"All our experiments ran for 200k learner updates. With a replay ratio of 1, sequence length of 80
(adjacent sequences overlapping by 40 observations), a batch size of 64, and an action-repeat of 4
this corresponds to a training budget of 200000×64×40×1×4 ≈2B environment frames (which"
REFERENCES,0.4641350210970464,Published as a conference paper at ICLR 2022
REFERENCES,0.46624472573839665,"Neural Network
Convolutional torso channels
32, 64, 128, 128
Convolutional torso kernel sizes
7, 5, 5, 3
Convolutional torso strides
4, 2, 2, 1
Pre-LSTM linear layer units
512
LSTM hidden units
512
Post-LSTM linear layer units
256
Dueling value head units
2 × 256 (separate linear layer for each of value and advantage)
Acting
Initial random No-Ops
None
Sticky actions
Yes (prob 0.25)
Action repeats
4
Number of actors
120
Actor parameter update interval
400 environment steps
Replay
Replay sequence length
80 (+ preﬁx of 20 of burn-in)
Replay buffer size
4 × 106 observations (105 part-overlapping sequences)
Priority exponent
0.9
Importance sampling exponent
0.6
Fixed replay ratio
1 update per sample (on average)
Learning
Multi-step Q-learning
k = 5
Off-policy corrections
None
Discount γ
0.997
Reward clipping
None
Return-based scaling
as in (Schaul et al., 2021)
Mini-batch size
64
Optimizer & settings
Adam (Kingma & Ba, 2014),
learning rate η = 2 × 10−4, ϵ = 10−8,
momentum β1 = 0.9, second moment β2 = 0.999
Gradient norm clipping
40
Target network update interval
400 updates
RND settings
Convolutional torso channels
32, 64, 64
Convolutional torso kernel sizes
8, 4, 3
Convolutional torso strides
4, 2, 1
MLP hidden units
128
Image downsampling stride
2 × 2"
REFERENCES,0.46835443037974683,Table 1: Hyper-parameters and settings.
REFERENCES,0.4704641350210971,"is less than 10% of the original R2D2 budget). In wall-clock-time, one such experiment takes about
12 hours using 2 TPUs (one for the batch inference, the other for the learner) and 120 CPUs."
REFERENCES,0.47257383966244726,"For evaluation, a separate actor (not feeding the replay buffer) is running alongside the agent using a
greedy policy (ε = 0), and pulling the most recent parameters at the beginning of each episode. We
follow standard evaluation methodology for Atari, reporting mean and median ‘human-normalised’
scores as introduced in (Mnih et al., 2015) (i.e. the episode returns are normalised so that 0 corre-
sponds to the score of a uniformly random policy while 1 corresponds to human performance), as
well as the mean ‘human-capped’ score which caps the per-game performance at human level. Error
bars or shaded curves correspond to the minimum and maximum values across these seeds."
REFERENCES,0.47468354430379744,"A.4
RANDOM NETWORK DISTILLATION"
REFERENCES,0.4767932489451477,"The agent setup for the XI experiments differs in a few ways from the default described above. First,
a separate network is trained via Random Network Distillation (RND, (Burda et al., 2018)), which
consists of a simple convnet with an MLP (no recurrence); for detailed settings, see RND section
in Table 1. The RND prediction network is updated jointly with the Q-value network, on the same"
REFERENCES,0.47890295358649787,Published as a conference paper at ICLR 2022
REFERENCES,0.4810126582278481,"data. The intrinsic reward derived from the RND loss is pursued at the same discount γ = 0.997 as
the external reward in G. The Q-value network is augmented with a second head that predicts the
Q-values for the intrinsic reward; this branches off after the ‘Post-LSTM linear layer’ (with 256),
and is the same type of dueling head, using the same scale normalisation method (Schaul et al.,
2021). In addition, the 5-step Q-learning is adapted to use a simple off-policy correction, namely
trace-cutting on non-greedy actions (akin to Watkins Q(λ) with λ = 1), separately for each learning
head.3 The XI policy is the greedy policy according to the Q-values of the second head. Note
that because of these differences in set-up, and especially because the second head can function
as an auxiliary learning target, it may be misleading to compare XI and XU results head-to-head:
we recommend looking at how things change within one of these settings (across variants of intra-
episodic exploration or the baselines), rather than between them."
REFERENCES,0.4831223628691983,"A.5
HOMEOSTASIS"
REFERENCES,0.48523206751054854,"The role of the homeostasis mechanism is to transform a sequence of scalar signals xt ∈R (for
1 ≤t ≤T) into a sequence of binary switching decisions yt ∈{0, 1} so that the average number of
switches approximates a desired target rate ρ, that is , 1 T
P"
REFERENCES,0.4873417721518987,"t yt ≈ρ, and high values of xt correspond
to a higher probability of yt = 1. Furthermore, the decision at any point yt can only be based on the
past signals x1:t. One way to achieve this is to exponentiate x (to turn it into a positive number x+)
and then set an adaptive threshold to determine when to switch. Algorithm 1 describes how this is
done in pseudo-code. The implementation deﬁnes a time-scale of interest τ := min(t, 100/ρ), and
uses it to track moving averages of three quantities, namely the mean and variance of x, as well as
the mean of x+."
REFERENCES,0.48945147679324896,"Algorithm 1 Homeostasis
Require: target rate ρ"
REFERENCES,0.49156118143459915,"1: initialize x ←0, x2 ←1, x+ ←1
2: for t ∈{1, . . . , T} do
3:
obtain next scalar signal return xt
4:
set time-scale τ ←min(t, 100 ρ )"
REFERENCES,0.4936708860759494,"5:
update moving average x ←(1 −1"
REFERENCES,0.4957805907172996,τ )x + 1
REFERENCES,0.4978902953586498,"τ xt
6:
update moving variance x2 ←(1 −1"
REFERENCES,0.5,τ )x2 + 1
REFERENCES,0.5021097046413502,τ (xt −x)2
REFERENCES,0.5042194092827004,"7:
standardise and exponentiate x+ ←exp

xt−x
√ x2 "
REFERENCES,0.5063291139240507,"8:
update transformed moving average x+ ←(1 −1"
REFERENCES,0.5084388185654009,τ )x+ + 1 τ x+
REFERENCES,0.510548523206751,"9:
sample yt ∼Bernoulli

min

1, ρ x+ x+
"
REFERENCES,0.5126582278481012,10: end for
REFERENCES,0.5147679324894515,"In our informed trigger experiments we use value promise as the particular choice of trigger signal
xt = Dpromise(t−k, t). As discussed in Section3.1, when using a bandit, its choices for target rates
are ρ ∈{0.1, 0.01, 0.001, 0.0001}."
REFERENCES,0.5168776371308017,"B
OTHER VARIANTS"
REFERENCES,0.5189873417721519,"The results we report in the main paper are but a subset of the possible variants that could be tried
in this rather large design space. In fact, we have done initial investigations on a few of these, which
we report below."
REFERENCES,0.5210970464135021,"B.1
ADDITIONAL EXPLORE MODES"
REFERENCES,0.5232067510548524,"Softer explore-exploit modes
The all-or-nothing setting with a greedy exploit mode and a uni-
form random explore mode is clear and simple, but it is plausible that less extreme choices could
work well too, such as an ε-greedy explore mode with ε = 0.4 and an ε-greedy exploit mode with"
REFERENCES,0.5253164556962026,"3While this seemed like important aspect to us, it turned out to make very little difference in hindsight, see
Figure 15."
REFERENCES,0.5274261603375527,Published as a conference paper at ICLR 2022
REFERENCES,0.5295358649789029,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.5316455696202531,Episode return
REFERENCES,0.5337552742616034,frostbite
REFERENCES,0.5358649789029536,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 2000 4000 6000 8000 10000"
REFERENCES,0.5379746835443038,gravitar
REFERENCES,0.540084388185654,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 5000 10000 15000 20000 hero"
REFERENCES,0.5421940928270043,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 100 200 300 400"
REFERENCES,0.5443037974683544,montezuma_revenge
REFERENCES,0.5464135021097046,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.5485232067510548,Episode return
REFERENCES,0.5506329113924051,ms_pacman
REFERENCES,0.5527426160337553,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 50000"
REFERENCES,0.5548523206751055,100000
REFERENCES,0.5569620253164557,150000
REFERENCES,0.5590717299578059,200000
REFERENCES,0.5611814345991561,250000
REFERENCES,0.5632911392405063,300000
REFERENCES,0.5654008438818565,350000
REFERENCES,0.5675105485232067,phoenix
REFERENCES,0.569620253164557,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 25000 50000 75000"
REFERENCES,0.5717299578059072,100000
REFERENCES,0.5738396624472574,125000
REFERENCES,0.5759493670886076,150000
REFERENCES,0.5780590717299579,175000
REFERENCES,0.580168776371308,200000
REFERENCES,0.5822784810126582,star_gunner
REFERENCES,0.5843881856540084,"XI-intra(10,informed-action,p*,G)
XI-intra(10,informed-valpromise,p*,G)
XI-intra(10,informed-variance,p*,G)"
REFERENCES,0.5864978902953587,"Figure 8: Preliminary results comparing different informed triggers: value-discrepancy, action-
mismatch, and variance-based, when using XI exploration mode."
REFERENCES,0.5886075949367089,"ε = 0.1. We denote this pairing as XS. Preliminary results (see Figure 16) indicate that overall per-
formance is mostly similar to XU, possibly less affected by the choice of granularity and triggers."
REFERENCES,0.5907172995780591,"Different discounts
Another category of explore mode (Xγ) is to pursue external reward but at a
different time-scale (e.g., a much shorter discount like γ = 0.97). This results in less of a switch
between explore and exploit modes, but rather in an alternation of long-term and short-term reward
pursuits, producing a different kind of behavioural diversity. So far, we do not have conclusive
results to report with this mode."
REFERENCES,0.5928270042194093,"B.2
ADDITIONAL INFORMED TRIGGERS"
REFERENCES,0.5949367088607594,"Action-mismatch-based triggers
Another type of informed trigger is to derive an uncertainty
estimate from the discrepancies across an ensemble. For example, we can train two heads that use
an identical Q-learning update but are initialised differently. From that, we can measure multiple
forms of discrepancy, a nice and robust one is to rank the actions according to each head and compute
how large the overlap among the top-k actions is."
REFERENCES,0.5970464135021097,"Variance-based triggers
Another type of informed trigger is to measure the variance of the Q-
values themselves, taken across such an ensemble (of two heads) and use that as an alternative
uncertainty-based trigger."
REFERENCES,0.5991561181434599,"Figure 8 shows preliminary results on how performance compares across these two new informed
triggers, in relation to the value-promise one from Section 2.2. Overall, the action-mismatch trigger
seems to have an edge, at least in this setting, and we plan to investigate this further in the future.
From other probing experiments, it appears that for other explore modes, different trigger signals
are more suitable."
REFERENCES,0.6012658227848101,"C
ADDITIONAL RESULTS"
REFERENCES,0.6033755274261603,"This section includes additional results. Wherever the main ﬁgures included a subset of games or
variants (Figures 4, 5, 7) we show full results here (Figures 11, 12, 13, respectively), and the aggre-
gated performances of Figure 3 are split out into individual games in Figure 10. Also, some of the
learning curves from Figures 4 and 11 are shown in Figure 16. In addition, Figure 14 illustrates how
the internal bandit probabilities evolve over time based on starting mode for the experiments shown
in Figure 6. Lastly, we provide some bonus illustrations for the intra-episodic design decisions in-
cluded in the namings of our variants in Figure 9 with the hope of facilitating the interpretation and
intuition for intra-episodic variants and their resulting behaviours."
REFERENCES,0.6054852320675106,Published as a conference paper at ICLR 2022
REFERENCES,0.6075949367088608,Figure 9: Extra example illustrating the space of design decisions for intra-episodic exploration.
REFERENCES,0.609704641350211,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 2500 5000 7500 10000 12500 15000 17500 20000"
REFERENCES,0.6118143459915611,Episode return
REFERENCES,0.6139240506329114,frostbite
REFERENCES,0.6160337552742616,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 1000 2000 3000 4000 5000 6000"
REFERENCES,0.6181434599156118,gravitar
REFERENCES,0.620253164556962,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 5000 10000 15000 20000 hero"
REFERENCES,0.6223628691983122,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 50 100 150 200 250"
REFERENCES,0.6244725738396625,montezuma_revenge
REFERENCES,0.6265822784810127,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.6286919831223629,Episode return
REFERENCES,0.630801687763713,ms_pacman
REFERENCES,0.6329113924050633,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 1000 2000 3000 4000 5000"
REFERENCES,0.6350210970464135,phoenix
REFERENCES,0.6371308016877637,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 20000 40000 60000 80000"
REFERENCES,0.6392405063291139,100000
REFERENCES,0.6413502109704642,120000
REFERENCES,0.6434599156118144,140000
REFERENCES,0.6455696202531646,star_gunner
REFERENCES,0.6476793248945147,"XU-episode-level-*
XU-experiment-level-G
XU-step-level-0.01
XU-experiment-level-X
XU-intra(10,informed,p*,X)"
REFERENCES,0.6497890295358649,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 2000 4000 6000 8000"
REFERENCES,0.6518987341772152,Episode return
REFERENCES,0.6540084388185654,frostbite
REFERENCES,0.6561181434599156,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 1000 2000 3000 4000 5000 6000"
REFERENCES,0.6582278481012658,gravitar
REFERENCES,0.6603375527426161,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 2500 5000 7500 10000 12500 15000 17500 20000 hero"
REFERENCES,0.6624472573839663,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 50 100 150 200 250"
REFERENCES,0.6645569620253164,montezuma_revenge
REFERENCES,0.6666666666666666,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 2000 4000 6000 8000 10000"
REFERENCES,0.6687763713080169,Episode return
REFERENCES,0.6708860759493671,ms_pacman
REFERENCES,0.6729957805907173,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 50000"
REFERENCES,0.6751054852320675,100000
REFERENCES,0.6772151898734177,150000
REFERENCES,0.679324894514768,200000
REFERENCES,0.6814345991561181,250000
REFERENCES,0.6835443037974683,300000
REFERENCES,0.6856540084388185,phoenix
REFERENCES,0.6877637130801688,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 25000 50000 75000"
REFERENCES,0.689873417721519,100000
REFERENCES,0.6919831223628692,125000
REFERENCES,0.6940928270042194,150000
REFERENCES,0.6962025316455697,175000
REFERENCES,0.6983122362869199,"200000
star_gunner"
REFERENCES,0.70042194092827,"XI-episode-level-*
XI-experiment-level-G
XI-step-level-X+G
XI-experiment-level-X
XI-intra(10,informed,p*,X)"
REFERENCES,0.7025316455696202,"Figure 10: Extension of Figure 3, showing performance results as the mean episode return (with
error bars spanning the min and max performance across 3 seeds) for the 7 Atari games tried. We
compare the four levels of exploration granularity as described in section 2.1 for XU mode (top two
rows) and XI mode (bottom two rows). Intra-episodic switching (red curve) is superior or compara-
ble to existing, monolithic or well-established approaches. Note that here we compare with the same
intra-episodic switching mechanism (i.e., with XU- or XI-intra(10,informed,p*,X)), but
there is at least one intra-episodic variant for each game which results in clear performance gains
(just that it is not the same intra-episodic variant across games or as the one shown here)."
REFERENCES,0.7046413502109705,Published as a conference paper at ICLR 2022
REFERENCES,0.7067510548523207,"0.0
0.2
0.4
0.6
0.8
1.0 10
2 10
1 p"
REFERENCES,0.7088607594936709,frostbite
REFERENCES,0.7109704641350211,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.7130801687763713,gravitar
REFERENCES,0.7151898734177216,"0.0
0.2
0.4
0.6
0.8
1.0 hero"
REFERENCES,0.7172995780590717,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.7194092827004219,montezuma_revenge
REFERENCES,0.7215189873417721,"0.0
0.2
0.4
0.6
0.8
1.0 10
2 10
1 p"
REFERENCES,0.7236286919831224,ms_pacman
REFERENCES,0.7257383966244726,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.7278481012658228,phoenix
REFERENCES,0.729957805907173,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.7320675105485233,star_gunner 0 10000 20000 30000
REFERENCES,0.7341772151898734,Episode return
REFERENCES,0.7362869198312236,frostbite 0 2000 4000 6000
REFERENCES,0.7383966244725738,gravitar 0 10000 20000 30000 hero 0 500 1000 1500 2000 2500
REFERENCES,0.740506329113924,montezuma_revenge 0 2500 5000 7500 10000 12500
REFERENCES,0.7426160337552743,Episode return
REFERENCES,0.7447257383966245,ms_pacman 0 25000 50000 75000
REFERENCES,0.7468354430379747,100000
REFERENCES,0.7489451476793249,125000
REFERENCES,0.7510548523206751,phoenix 0 50000
REFERENCES,0.7531645569620253,100000
REFERENCES,0.7552742616033755,150000
REFERENCES,0.7573839662447257,star_gunner
REFERENCES,0.759493670886076,"XU-episode-level-*
XU-intra(10,informed,0.001,G)
XU-intra(10,informed,p*,G)
XU-intra(p*,informed,p*,G)
XU-intra(=,informed,p*,G)
XU-intra(=,blind,n*,G)
XU-intra(n*,blind,n*,G)
XU-intra(10,blind,n*,G)
XU-intra(n*,blind,100,G)
XU-intra(=,blind,n*,X)
XU-intra(10,blind,90,X)"
REFERENCES,0.7616033755274262,"0.0
0.2
0.4
0.6
0.8
1.0 10
2 10
1 p"
REFERENCES,0.7637130801687764,frostbite
REFERENCES,0.7658227848101266,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.7679324894514767,gravitar
REFERENCES,0.770042194092827,"0.0
0.2
0.4
0.6
0.8
1.0 hero"
REFERENCES,0.7721518987341772,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.7742616033755274,montezuma_revenge
REFERENCES,0.7763713080168776,"0.0
0.2
0.4
0.6
0.8
1.0 10
2 10
1 p"
REFERENCES,0.7784810126582279,ms_pacman
REFERENCES,0.7805907172995781,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.7827004219409283,phoenix
REFERENCES,0.7848101265822784,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.7869198312236287,star_gunner 0 2000 4000 6000 8000 10000
REFERENCES,0.7890295358649789,Episode return
REFERENCES,0.7911392405063291,frostbite 0 2000 4000 6000
REFERENCES,0.7932489451476793,gravitar 0 5000 10000 15000 20000 hero 0 500 1000 1500
REFERENCES,0.7953586497890295,montezuma_revenge 0 2500 5000 7500 10000 12500
REFERENCES,0.7974683544303798,Episode return
REFERENCES,0.79957805907173,ms_pacman 0
REFERENCES,0.8016877637130801,100000
REFERENCES,0.8037974683544303,200000
REFERENCES,0.8059071729957806,300000
REFERENCES,0.8080168776371308,phoenix 0 50000
REFERENCES,0.810126582278481,100000
REFERENCES,0.8122362869198312,150000
REFERENCES,0.8143459915611815,200000
REFERENCES,0.8164556962025317,star_gunner
REFERENCES,0.8185654008438819,"XI-episode-level-*
XI-intra(10,informed,0.001,G)
XI-intra(10,informed,p*,G)
XI-intra(p*,informed,p*,G)
XI-intra(=,informed,p*,G)
XI-intra(=,blind,n*,G)
XI-intra(n*,blind,n*,G)
XI-intra(10,blind,n*,G)
XI-intra(=,blind,p*,G)
XI-intra(=,blind,n*,X)
XI-intra(10,blind,90,X)"
REFERENCES,0.820675105485232,"Figure 11: Extension of ﬁgure 4 to all Atari games and intra-episodic switching variants tried.
We show the characteristic space of exploration (summarized by rmedX and pX on the X and Y
axis, respectively) on rows 1, 2, 5, and 6, and how different explore-exploit proportions translate
to performance (error bars spanning the min and max performance across 3 seeds) on rows 3, 4,
7, and 8, for XU mode (top) and XI mode (bottom). Note how different intra-episodic switching
variants cover different parts of characteristic space and how the meta-controller adapts and changes
the exploration statistics over time. This ﬁgure shows how ﬁne-grained and varied intra-episodic
switching can be, and how it translates to rich, diverse, and beneﬁcial behaviours."
REFERENCES,0.8227848101265823,Published as a conference paper at ICLR 2022
REFERENCES,0.8248945147679325,"Figure 12: Extension of Figure 5 to the 7 Atari games we experimented with. First two columns:
temporal structures for a blind, step-based trigger; the 15 episodes we randomly selected correspond
to 100 and 1000 ﬁxed switching steps; the exploration period was ﬁxed to 10 steps. Last two
columns: temporal structures obtained with an equivalent informed trigger and corresponding to
target rates of 0.01 and 0.001, respectively."
REFERENCES,0.8270042194092827,Published as a conference paper at ICLR 2022 0 2000 4000 6000 8000
REFERENCES,0.8291139240506329,Episode return
REFERENCES,0.8312236286919831,"XU-intra(=,blind,100,G) on frostbite 0 1000 2000 3000 4000"
REFERENCES,0.8333333333333334,Episode return
REFERENCES,0.8354430379746836,"XU-intra(=,blind,100,G) on gravitar 0 2000 4000 6000 8000 10000 12000 14000"
REFERENCES,0.8375527426160337,Episode return
REFERENCES,0.8396624472573839,"XU-intra(=,blind,100,G) on hero 0 500 1000 1500 2000 2500"
REFERENCES,0.8417721518987342,Episode return
REFERENCES,0.8438818565400844,"XU-intra(=,blind,100,G) on montezuma_reveng 0 2000 4000 6000 8000"
REFERENCES,0.8459915611814346,Episode return
REFERENCES,0.8481012658227848,"XU-intra(=,blind,100,G) on ms_pacman 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.8502109704641351,Episode return
REFERENCES,0.8523206751054853,"XU-intra(=,blind,100,G) on phoenix"
REFERENCES,0.8544303797468354,"0.0
0.2
0.4
0.6
0.8
1.0
p 0 5000 10000 15000 20000 25000 30000 35000"
REFERENCES,0.8565400843881856,Episode return
REFERENCES,0.8586497890295358,"XU-intra(=,blind,100,G) on star_gunner 0 2000 4000 6000 8000"
REFERENCES,0.8607594936708861,Episode return
REFERENCES,0.8628691983122363,"XU-intra(=,blind,0.01,G) on frostbite 0 1000 2000 3000 4000"
REFERENCES,0.8649789029535865,Episode return
REFERENCES,0.8670886075949367,"XU-intra(=,blind,0.01,G) on gravitar 0 2000 4000 6000 8000 10000 12000 14000"
REFERENCES,0.869198312236287,Episode return
REFERENCES,0.8713080168776371,"XU-intra(=,blind,0.01,G) on hero 0 500 1000 1500 2000 2500"
REFERENCES,0.8734177215189873,Episode return
REFERENCES,0.8755274261603375,"XU-intra(=,blind,0.01,G) on montezuma_reven 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.8776371308016878,Episode return
REFERENCES,0.879746835443038,"XU-intra(=,blind,0.01,G) on ms_pacman 0 5000 10000 15000 20000"
REFERENCES,0.8818565400843882,Episode return
REFERENCES,0.8839662447257384,"XU-intra(=,blind,0.01,G) on phoenix"
REFERENCES,0.8860759493670886,"0.0
0.2
0.4
0.6
0.8
1.0
p 0 5000 10000 15000 20000 25000 30000 35000 40000"
REFERENCES,0.8881856540084389,Episode return
REFERENCES,0.890295358649789,"XU-intra(=,blind,0.01,G) on star_gunner"
REFERENCES,0.8924050632911392,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 2000 4000 6000 8000"
REFERENCES,0.8945147679324894,Episode return
REFERENCES,0.8966244725738397,frostbite
REFERENCES,0.8987341772151899,"XU-intra(=,blind,0.01,G)
XU-intra(=,blind,100,G)"
REFERENCES,0.9008438818565401,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 1000 2000 3000 4000 5000"
REFERENCES,0.9029535864978903,Episode return
REFERENCES,0.9050632911392406,gravitar
REFERENCES,0.9071729957805907,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 2000 4000 6000 8000 10000 12000 14000"
REFERENCES,0.9092827004219409,Episode return hero
REFERENCES,0.9113924050632911,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 500 1000 1500 2000 2500"
REFERENCES,0.9135021097046413,Episode return
REFERENCES,0.9156118143459916,montezuma_revenge
REFERENCES,0.9177215189873418,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 2000 4000 6000 8000 10000 12000"
REFERENCES,0.919831223628692,Episode return
REFERENCES,0.9219409282700421,ms_pacman
REFERENCES,0.9240506329113924,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 10000 20000 30000 40000 50000 60000 70000 80000"
REFERENCES,0.9261603375527426,Episode return
REFERENCES,0.9282700421940928,phoenix
REFERENCES,0.930379746835443,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 10000 20000 30000 40000 50000 60000"
REFERENCES,0.9324894514767933,Episode return
REFERENCES,0.9345991561181435,star_gunner
REFERENCES,0.9367088607594937,"Figure 13: Extension of Figure 7, showing behavioural characteristics (exploration proportion pX )
between two forms of blind switching, step-based (left) and probabilistic (center), with their corre-
sponding performances (right)."
REFERENCES,0.9388185654008439,Published as a conference paper at ICLR 2022
REFERENCES,0.9409282700421941,"Figure 14: Extension of Figure 6, showing the performance differences between two blind intra-
episode experiments, starting either in explore (X, rows 2 and 4) or in exploit mode (G, rows 1
and 3). We show the bandit arm probabilities for each of the step sizes nX and how they change
over the course of learning for XU (top two rows) and for XI modes (bottom two rows). Findings:
for symmetric blind triggers, starting with exploitation results in slower rates of switching (high
nX = nG like red and green); in contrast, starting with exploration results in behaviours promoting
higher switching rates (small nX = nG like blue and orange). Note that these preferences are not
matching perfectly across all games, and thus results are domain-dependent."
REFERENCES,0.9430379746835443,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.9451476793248945,Episode return
REFERENCES,0.9472573839662447,frostbite
REFERENCES,0.9493670886075949,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 1000 2000 3000 4000 5000 6000"
REFERENCES,0.9514767932489452,gravitar
REFERENCES,0.9535864978902954,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 2500 5000 7500 10000 12500 15000 17500 hero"
REFERENCES,0.9556962025316456,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75 1e5 0 50 100 150 200 250 300"
REFERENCES,0.9578059071729957,montezuma_revenge
REFERENCES,0.959915611814346,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 2000 4000 6000 8000 10000"
REFERENCES,0.9620253164556962,Episode return
REFERENCES,0.9641350210970464,ms_pacman
REFERENCES,0.9662447257383966,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0"
REFERENCES,0.9683544303797469,100000
REFERENCES,0.9704641350210971,200000
REFERENCES,0.9725738396624473,300000
REFERENCES,0.9746835443037974,400000
REFERENCES,0.9767932489451476,phoenix
REFERENCES,0.9789029535864979,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Learner steps
1e5 0 25000 50000 75000"
REFERENCES,0.9810126582278481,100000
REFERENCES,0.9831223628691983,125000
REFERENCES,0.9852320675105485,150000
REFERENCES,0.9873417721518988,star_gunner
REFERENCES,0.989451476793249,"XI-intra(=,blind,p*,G)"
REFERENCES,0.9915611814345991,"with off-policy correction
without off-policy correction"
REFERENCES,0.9936708860759493,"Figure 15: Results of a probe experiment around off-policy correction in XI mode. Red is Watkins
Q(λ) with λ = 1 while blue is uncorrected k-step Q-learning, in each case with returns of length at
most k = 5. The performance is the same (or even slightly better) without off-policy correction,
showing that it is not critical in our current setting."
REFERENCES,0.9957805907172996,Published as a conference paper at ICLR 2022
REFERENCES,0.9978902953586498,"Figure 16: Comparing 3 different X modes on the same 4 experimental settings and across 7 Atari
games: uniform exploration (XU, left), soft-epsilon-based exploration (XS, center), and intrinsic
exploration (XI, right)."
