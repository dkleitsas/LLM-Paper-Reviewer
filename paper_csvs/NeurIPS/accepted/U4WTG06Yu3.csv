Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0031847133757961785,"The Winograd algorithm is an efficient convolution implementation, which per-
forms calculations in the transformed domain. To further improve the computation
efficiency, recent works propose to combine it with model quantization. Although
Post-Training Quantization has the advantage of low computational cost and has
been successfully applied in many other scenarios, a severe accuracy drop exists
when utilizing it in Winograd convolution. Besides, despite the Winograd algorithm
consisting of four stages, most existing methods only quantize the element-wise
multiplication stage, leaving a considerable portion of calculations in full precision.
In this paper, observing the inconsistency among different transformation proce-
dures, we present PTQ-Aware Winograd (PAW) to optimize them collaboratively
under a unified objective function. Moreover, we explore the full quantization of
faster Winograd (tile size ≥4) for the first time. We further propose a hardware-
friendly method called Factorized Scale Quantization (FSQ), which can effectively
balance the significant range differences in the Winograd domain. Experiments
demonstrate the effectiveness of our method, e.g., with 8-bit quantization and a
tile size of 6, our method outperforms the previous Winograd PTQ method by
8.27% and 5.38% in terms of the top-1 accuracy on ResNet-18 and ResNet-34,
respectively."
INTRODUCTION,0.006369426751592357,"1
Introduction"
INTRODUCTION,0.009554140127388535,"Recently, Convolution Neural Networks (CNNs) have demonstrated state-of-the-art performance in
various computer vision tasks [1; 2; 3; 4]. However, the high computation and storage cost hinders
their deployment on resource-limited devices. To address this problem, various solutions have been
proposed in the literature, including network pruning [5], low-rank decomposition [6], network
quantization [7; 8; 9; 10; 11] and faster convolution implementation [12; 13]. In this paper, we focus
on network quantization, faster convolution implementation, and their combination."
INTRODUCTION,0.012738853503184714,"Model quantization converts the floating-point weights and activations to low-bit integers. According
to whether data with labels are required for training, quantization can be divided into two categories:
Quantization-Aware Training (QAT) [14; 15; 16; 17] and Post-Training Quantization (PTQ) [18; 19;
20; 21]. Although QAT can achieve promising performance, training the network requires many GPU
resources and a full training dataset. Therefore, it is not always practical when either the training
dataset is unavailable (e.g., privacy and commercial training data) or rapid deployment is required.
On the contrary, as PTQ only needs few-shot, unlabeled calibration data and fewer computation
resources, it is widely applicable in the industry."
INTRODUCTION,0.01592356687898089,†Corresponding authors.
INTRODUCTION,0.01910828025477707,"An alternative method for enhancing the speed of CNNs involves the development of faster convolu-
tion implementations, such as FFT [13] and Winograd [12]. Among them, the Winograd algorithm is
the most popular fast convolution operator [22]. Recent research [23; 24; 25] has focused on combin-
ing the Winograd algorithm with model quantization to further utilize the benefits of both techniques.
However, two significant problems remain: On the one hand, while Winograd convolution with larger
tile sizes (tile size≥4) can provide more significant acceleration, existing PTQ methods for such
convolution still suffer from drastic accuracy degradation. On the other hand, most existing works
[24; 26] only quantize the element-wise multiplications, while leaving the domain transformation
implemented by full-precision matrix multiplications, which can comprise a significant portion of the
computation (40.1% when the input and output channels are 32 [24]). However, according to our
experiments (Table 2 and Figure 2), these matrix multiplications are challenging to quantize because
of the significant distribution imbalance in the Winograd domain."
INTRODUCTION,0.022292993630573247,"In this paper, we first identify that quantization makes the transformation procedures in the Winograd
algorithm inconsistent. Based on this observation, we propose PTQ-Aware Winograd (PAW),
which optimizes transformation matrices collaboratively under a unified objective function. We then
explore the fully quantized Winograd convolution, which is the first time for large tile sizes. We
empirically find it is a non-trivial task because of the significant range differences of pixels in the
Winograd domain. Through theoretical analysis of this phenomenon, we discover that it is possible
to achieve comparable performance to per-pixel quantization without sacrificing computational
efficiency via Factorized Scale Quantization (FSQ), which factorizes the tile size scales into vector
size scales. Experiments are conducted to demonstrate the effectiveness of our method, e.g., with
8-bit quantization and a tile size of 6, our method outperforms the previous Winograd PTQ method
by 8.27% and 5.38% in terms of the top-1 accuracy on ResNet-18 and ResNet-34, respectively."
INTRODUCTION,0.025477707006369428,"Overall, our contributions in this work are threefold:"
INTRODUCTION,0.028662420382165606,"• We experimentally observe that quantization will disrupt the consistency between different
transformation procedures and propose PTQ-Aware Winograd, which utilizes a unified
optimization procedure to make the Winograd algorithm more robust to quantization."
INTRODUCTION,0.03184713375796178,"• Through extensive experiments and theoretical analysis, we propose Factorized Scale Quan-
tization, a hardware-friendly method suitable for the distribution characteristic of the Wino-
grad domain tensors."
INTRODUCTION,0.03503184713375796,"• To the best of our knowledge, we are the first to achieve full quantization of Winograd
convolution with large tile sizes (4 and 6). Experiments prove that our proposed method
shows significant improvements over previous PTQ methods, even under fully-quantization
settings."
RELATED WORKS,0.03821656050955414,"2
Related Works"
RELATED WORKS,0.041401273885350316,"Quantization-aware training (QAT) [8; 27; 28; 29; 30] is a technique that simulates quantization
noise during end-to-end training of neural networks. It uses discretized weights during both forward
and backward propagation and updates original full-precision weights. Since the gradient of the
quantization function is either zero or undefined everywhere, this procedure is done via a gradient
approximation method called straight-through estimator [31]. Recently, some methods also add
quantization related parameters to this training procedure, such as clipping range [32] and step size
[14]. Although QAT methods have promising performance, they need the whole dataset and huge
GPU resources."
RELATED WORKS,0.044585987261146494,"Post-training quantization (PTQ) [7; 21; 9; 18; 33] is a more lightweight method that does not
require retraining the network end-to-end. It only needs a small number of samples to estimate
activation distribution. Despite its efficiency, PTQ suffers from a more significant accuracy degra-
dation than QAT. The research community has actively proposed various methods to alleviate this
problem. For example, [21] observes the scaling equivariance of activation functions and proposes
to balance weights in consecutive layers. They also propose bias correction, which absorbs high
biases into the next layer. Recently, some methods have found that the change in feature maps is a
practical proxy signal for final task loss and use a layer-wise feature map reconstruction procedure
to boost performance. Bit-Split [7] turns the low-bit discrete optimization into multiple single-bit
optimization problems to find the optimal quantization scales and integer weights. AdaRound [18]"
RELATED WORKS,0.04777070063694268,"Table 1: Percentage of the computational burden of different operations in F(6,3) Winograd convolu-
tion. Experiments are conducted on different blocks of ResNet-20. ""Offline"" means this operation
can be preprocessed before inference."
RELATED WORKS,0.050955414012738856,"Operation
Block 0
Block 1
Block 2
Total"
RELATED WORKS,0.054140127388535034,"BT XB
22%
16%
10%
16%
U ⊙V
39%
56%
72%
56%
AT OA
39%
28%
18%
28%"
RELATED WORKS,0.05732484076433121,"GWGT
Offline
Offline
Offline
Offline"
RELATED WORKS,0.06050955414012739,"proposes optimizing the round function with a soft relaxation, which makes huge progress than the
original round-to-nearest function."
RELATED WORKS,0.06369426751592357,"Winograd convolution quantization Winograd is an fast convolution implementation first applied
by [12]. To further improve computational efficiency, many works [23; 24; 34; 35; 36] focus on
combining it with model quantization. For Winograd convolution networks with tile size ≥4, most
works can only achieve an acceptable accuracy drop when using QAT, where the strong effectiveness
of retraining the whole network hides the difficulty of quantization [23; 34; 36]. For instance, BQW
[24] observes the heterogeneity of channel ranges in transformed tensors and equalizes them via
migrating the scale variance from activations to weights. However, compared to the significant
performance improvements achieved in QAT, their method only achieves limited progress in the case
of PTQ. One of the reasons, we think, is that their method ignores the quantization impact on the
Winograd algorithm, which is more notable without fine-tuning the whole network. In this paper,
we will solve this problem and explore a more challenging scenario, fully quantized Winograd
convolution using PTQ."
BACKGROUND,0.06687898089171974,"3
Background"
WINOGRAD,0.07006369426751592,"3.1
Winograd"
WINOGRAD,0.0732484076433121,"Winograd proposes the minimum filtering algorithm of finite impulse response (FIR) filtering in
[37]. For r × r standard convolutions with filter size r, the algorithm transforms the convolution
operations to the Winograd domain and generates m × m (spatial) outputs at a time, which is denoted
as F(m, r). The parameter m is called tile size, which is used to balance the speedup and numerical
precision. The Winograd algorithm can be divided into four stages:"
WINOGRAD,0.07643312101910828,"Input transformation: Firstly, a patch X ∈RCi×a×a is extracted from the input data, with patch
size a = m + r −1. Then the c-th channel sub-tensor of X, denoted as Xc, is transformed into the
Winograd domain using input transformation matrix B ∈Ra×a. In this paper, the transformed inputs
are denoted as U ∈RCi×a×a:"
WINOGRAD,0.07961783439490445,"Uc = BT XcB,
c = 1, ..., Ci
(1)"
WINOGRAD,0.08280254777070063,"Weight transformation: Similarly, weights W ∈RCo×Ci×r×r are transformed into the Winograd
domain to get transformed weights V ∈RCo×Ci×a×a. This process can be performed offline before
inference because model weights are frozen after training."
WINOGRAD,0.08598726114649681,"Vf,c = GWf,cGT ,
G ∈Ra×r
where
f = 1, ..., Co,
c = 1, ..., Ci
(2)"
WINOGRAD,0.08917197452229299,"Element-wise multiplication: In the Winograd domain, the convolution operation is performed by
element-wise multiplications between U and V :"
WINOGRAD,0.09235668789808917,"Of = ΣCi
c Uc ⊙Vf,c
(3)"
WINOGRAD,0.09554140127388536,where ⊙denotes element-wise multiplication.
WINOGRAD,0.09872611464968153,"Output transformation: Finally, O ∈RCo×a×a are transformed back to the feature map domain,
and then, we get final outputs Y ∈RCo×m×m:"
WINOGRAD,0.10191082802547771,"Yf = AT OfA,
A ∈Ra×m
and
f = 1, ..., Co
(4)"
WINOGRAD,0.10509554140127389,"Depending on the particular choice of Winograd domain (i.e., polynomial domain), these transforma-
tion matrices A, B, and G in Eq. (1)-(4) can be different. For F(2,3), the most common choice of the
Winograd domain is f(x) = (x −1)(x + 1)x, then A, B and G can be constructed as follows[12].
More details are provided in the supplemental materials."
WINOGRAD,0.10828025477707007,"AT =

1
1
1
0
0
1
−1
−1"
WINOGRAD,0.11146496815286625,"
,
BT =  "
WINOGRAD,0.11464968152866242,"1
0
−1
0
0
1
1
0
0
−1
1
0
0
1
0
−1 "
WINOGRAD,0.1178343949044586,",
G =  "
WINOGRAD,0.12101910828025478,"1
0
0
1
2
1
2
1
2
1
2
−1"
WINOGRAD,0.12420382165605096,"2
1
2
0
0
1 "
WINOGRAD,0.12738853503184713,"
(5)"
WINOGRAD,0.1305732484076433,"Although the Winograd algorithm reduces the computational complexity of convolution operations by
performing them in the Winograd domain, these transformation steps also bring additional overheads.
An example is shown in Table 1. When the channel numbers Co and Ci are not large enough, these
transformation processes will incur considerable computational costs."
QUANTIZATION,0.1337579617834395,"3.2
Quantization"
QUANTIZATION,0.13694267515923567,"Quantization converts a full-precision tensor to an integer one. To improve hardware simplicity and
efficiency, we utilize symmetric uniform quantization without zero-point. For a tensor V , quantization
maps it to the integer values eV and de-quantization remaps it to float-point values Q(V ):"
QUANTIZATION,0.14012738853503184,"eV =

clip(V"
QUANTIZATION,0.14331210191082802,"s , −qmin, qmax)

(6)"
QUANTIZATION,0.1464968152866242,"Q(V ) = eV · s
(7)"
QUANTIZATION,0.14968152866242038,"where s represents a full-precision scalar called quantization scale, ⌊z⌉rounds z to the nearest
integer and clip(z, r1, r2) clamps z into the range [r1, r2]. For symmetric quantization with B bits,
qmin = −2B−1 and qmax = 2B−1."
QUANTIZATION,0.15286624203821655,"In Eq. (6), we utilize a scalar s to quantize the entire tensor, called per-tensor quantization. Previous
studies [24; 26] have shown that using per-pixel quantization, which provides independent scales for
different pixels of transformed tensors, can significantly improve performance:"
QUANTIZATION,0.15605095541401273,"eUc = ⌊Uc ⊘SU⌉
(8)"
QUANTIZATION,0.1592356687898089,"and
eVf,c = ⌊Vf,c ⊘SV ⌉
(9)"
QUANTIZATION,0.1624203821656051,"Here ⊘is the element-wise division and we emit the clip function. SU and SV ∈Ra×a are tile size
scales and can be factored out of the summation of Eq. (3). Thus, we can perform MAC operations in
fixed-point format:
Of = (ΣCi
c eUc ⊙eVf,c)
|
{z
}
int8"
QUANTIZATION,0.16560509554140126,"⊙(SU ⊙SV )
(10)"
METHOD,0.16878980891719744,"4
Method"
PTQ-AWARE WINOGRAD ALGORITHM,0.17197452229299362,"4.1
PTQ-aware Winograd algorithm"
PTQ-AWARE WINOGRAD ALGORITHM,0.1751592356687898,"While current state-of-the-art PTQ methods [18; 19; 9; 33] can achieve near-lossless 6-bit quantization
on popular non-Winograd CNNs, recent research has shown that 8-bit quantization of Winograd
convolutions still suffers from a significant accuracy drop (e.g., 9.71% for F(6,3) on ResNet-18 [24]).
We argue that one key reason why quantization and the Winograd algorithm are not well-compatible
is that quantization alters input and weight transformation procedures, making the whole algorithm
inconsistent."
PTQ-AWARE WINOGRAD ALGORITHM,0.17834394904458598,"To support our argument, we experiment on the first layer of ResNet-18 and take weight transformation
as an example. After quantization, the weight transformation procedure is changed from GWGT to
Q(GWGT ). Consequently, the other two transformation procedures, AT OA and BT XB, will not
be consistent with this new weight transformation. To illustrate it, we randomly perturb A and B and
compute the reconstruction loss Lquant and Lorigin with or without quantization, respectively:"
PTQ-AWARE WINOGRAD ALGORITHM,0.18152866242038215,"Lorigin = ΣCo
f ||AT (ΣCi
c (BT XcB) ⊙(GWf,cGT ))A −Yf||2
(11)"
PTQ-AWARE WINOGRAD ALGORITHM,0.18471337579617833,"738.90
738.95
739.00
739.05
739.10
739.15
Reconstruction Loss with Quantization"
PTQ-AWARE WINOGRAD ALGORITHM,0.18789808917197454,0.0000
PTQ-AWARE WINOGRAD ALGORITHM,0.1910828025477707,0.0005
PTQ-AWARE WINOGRAD ALGORITHM,0.1942675159235669,0.0010
PTQ-AWARE WINOGRAD ALGORITHM,0.19745222929936307,0.0015
PTQ-AWARE WINOGRAD ALGORITHM,0.20063694267515925,0.0020
PTQ-AWARE WINOGRAD ALGORITHM,0.20382165605095542,0.0025
PTQ-AWARE WINOGRAD ALGORITHM,0.2070063694267516,Reconstruction Loss without Quantization
PTQ-AWARE WINOGRAD ALGORITHM,0.21019108280254778,"Random A,B
Original A,B"
PTQ-AWARE WINOGRAD ALGORITHM,0.21337579617834396,"(a) Random A, B"
PTQ-AWARE WINOGRAD ALGORITHM,0.21656050955414013,"491.55
491.60
491.65
491.70
491.75
491.80
491.85
Reconstruction Loss with Quantization 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08"
PTQ-AWARE WINOGRAD ALGORITHM,0.2197452229299363,Reconstruction Loss without Quantization
PTQ-AWARE WINOGRAD ALGORITHM,0.2229299363057325,"Random A,G
Original A,G"
PTQ-AWARE WINOGRAD ALGORITHM,0.22611464968152867,"(b) Random A, G"
PTQ-AWARE WINOGRAD ALGORITHM,0.22929936305732485,"Figure 1: Reconstruction loss points obtained by randomly perturbing A, B (a) or A, G (b). The
bottom points indicate a minor loss without quantization. And the left points indicate a minor loss
with quantization."
PTQ-AWARE WINOGRAD ALGORITHM,0.23248407643312102,"Lquant = ΣCo
f ||AT (ΣCi
c (BT XcB) ⊙Q(GWf,cGT ))A −Yf||2
(12)
The results are shown in Figure 1(a). Although original A and B can produce desired results with
floating-point GWGT (the yellow triangle on the bottom), they are no longer optimal when changing
GWGT to Q(GWGT ). Concretely, about half of the random perturbed matrices lead to smaller
errors (the blue dots on the upper left of the yellow triangle). A similar phenomenon also exists for
input transformation, as shown in Figure 1(b). These experiments indicate that the inconsistency
between domain transformations caused by quantization renders the original matrices unsuitable
for quantization scenarios, which is the primary reason why Winograd convolutions are difficult to
quantize. In order to align these transformation procedures after quantization, we propose to adjust
transformation matrices via an optimization procedure as follows:"
PTQ-AWARE WINOGRAD ALGORITHM,0.2356687898089172,"argmin
A,B,G
EX∼D
h
ΣCo
f ||AT (ΣCi
c Q(BT XcB) ⊙Q(GWf,cGT ))A −Yf||2i
(13)"
PTQ-AWARE WINOGRAD ALGORITHM,0.23885350318471338,"Since the transformation procedures in the Winograd algorithm are adopted considering the effect of
PTQ, we name this method as PTQ-Aware Winograd (PAW). By using the straight-through estimator
[31] to approximate the gradient through the round function as a pass-through operation, we can
obtain the derivative of G (derivatives of A and B are shown in supplemental materials) and update it
using SGD:"
PTQ-AWARE WINOGRAD ALGORITHM,0.24203821656050956,"∂L
∂G = ΣCo
f ΣCi
c ( ∂L"
PTQ-AWARE WINOGRAD ALGORITHM,0.24522292993630573,"∂Of
⊙Q(Uc))GW T
f,c + ( ∂L"
PTQ-AWARE WINOGRAD ALGORITHM,0.2484076433121019,"∂Of
⊙Q(Uc))T GWf,c
(14)"
PTQ-AWARE WINOGRAD ALGORITHM,0.2515923566878981,"where
∂L
∂Of
= 2A(AT OfA −Y )AT
(15)"
FULLY-QUANTIZED WINOGRAD CONVOLUTION,0.25477707006369427,"4.2
Fully-quantized Winograd convolution"
MOTIVATION,0.25796178343949044,"4.2.1
Motivation"
MOTIVATION,0.2611464968152866,"Winograd convolution involves a sequence of computational steps from Eq. (1) to Eq. (4). To further
improve computation efficiency via full quantization, all these operations need to be quantized except"
MOTIVATION,0.2643312101910828,"Table 2: Exploration on 8-bit quantization of different components of Winograd convolution on
ResNet-18. Output tensor in the Winograd domain (O) is the primary cause of accuracy degeneration."
MOTIVATION,0.267515923566879,"FP
A
B
X
U
V
O
BT X
AT O"
MOTIVATION,0.27070063694267515,"Accuracy
69.76
69.74
69.66
69.67
69.40
69.40
0.20
69.22
66.56"
MOTIVATION,0.27388535031847133,"20
10
0
10
20
0 500 1000 1500 2000 2500 3000 3500"
MOTIVATION,0.2770700636942675,Layer2.1.conv1
MOTIVATION,0.2802547770700637,"O[4,4] std:0.28
O[5,5] std:3.76"
MOTIVATION,0.28343949044585987,"10
0
10
0 500 1000 1500 2000 2500"
MOTIVATION,0.28662420382165604,Layer3.1.conv1
MOTIVATION,0.2898089171974522,"O[4,4] std:0.13
O[5,5] std:3.61"
MOTIVATION,0.2929936305732484,"Figure 2: The ranges of O vary widely
between different pixels. In both layers,
the std of O5,5 is ten times larger than
that of O4,4."
MOTIVATION,0.2961783439490446,"Table 3:
The performance of different quantization
schemes for O. Although per-pixel quantization is ex-
cellent in maintaining model accuracy, it is not supported
in hardware."
MOTIVATION,0.29936305732484075,"Granularity
Accuracy
Hard-friendly"
MOTIVATION,0.30254777070063693,"Quantize O (per-tensor)
0.216
✓
Quantize O (per-pixel)
68.740
✗"
MOTIVATION,0.3057324840764331,"Quantize O (ours)
67.866
✓"
MOTIVATION,0.3089171974522293,"Eq. (2), which can be done offline. However, we empirically find that it is non-trivial to achieve
full quantization because that accuracy will drop to zero due to the quantization of O (i.e., the
output tensor in the Winograd domain), as shown in Table 2. After further investigation, we find
that the ranges of O vary widely between different pixels. For example, as shown in Figure 2, the
standard deviation (std) of O distributed at pixel (4,4) is ten times larger than that at pixel (5,5).
A shared quantization scale for them is difficult to achieve both small round error and clamping
error. A possible solution is to provide independent scales for them, i.e., to use per-pixel quantization
(Table 3). However, since O will take part in matrix multiplication (Eq. 4) instead of element-wise
multiplication, per-pixel quantization will lead to different scales in the summation dimension, which
makes it not feasible in general hardware."
FACTORIZED-SCALE QUANTIZATION,0.31210191082802546,"4.2.2
Factorized-scale quantization"
FACTORIZED-SCALE QUANTIZATION,0.31528662420382164,"Motivated by the strong range difference, we present the following theorem, which shows the cause
of this distribution characterization and proves the standard deviation of O can be factorized into two
vectors."
FACTORIZED-SCALE QUANTIZATION,0.3184713375796178,"Proposition 1. Assume all elements of W ∈RCi×r×r and X ∈RCi×a×a are independently and
identically distributed variables with zero mean (e.g., X ∼N(0, σX), W ∼N(0, σW )). If denote
BT = (bT
1 , bT
2 , ..., bT
a )T and G = (gT
1 , gT
2 , ..., gT
a )T , we have:"
FACTORIZED-SCALE QUANTIZATION,0.321656050955414,"V ar [Oij] = V ar

ΣCi
c (BT XcB ⊙GWGT )ij

= uivj
(16)"
FACTORIZED-SCALE QUANTIZATION,0.3248407643312102,"where ui = √CiσXσW ||bi||2||gi||2 and vj = √CiσXσW ||bj||2||g2
j ||."
FACTORIZED-SCALE QUANTIZATION,0.32802547770700635,"Proof. Firstly, since BT XcB and GWcGT are the linear combinations of independent and identical
variables with zero mean, we can calculate their mean and variance:"
FACTORIZED-SCALE QUANTIZATION,0.33121019108280253,"E

(BT XcB)ij

= 0
and
E

(GWcGT )ij

= 0,
(17)"
FACTORIZED-SCALE QUANTIZATION,0.3343949044585987,"V ar

(BT XcB)ij

= V ar

bT
i Xcbj

= ||bi||2||b2
j||σX,
(18)"
FACTORIZED-SCALE QUANTIZATION,0.3375796178343949,"V ar

(GWcGT )ij

= V ar

gT
i Wcgj

= ||gi||2||g2
j ||σW
(19)"
FACTORIZED-SCALE QUANTIZATION,0.34076433121019106,Then we can derive the variance of O :
FACTORIZED-SCALE QUANTIZATION,0.34394904458598724,"V ar [Oij]
(a)
= ΣCi
c V ar

(BT XcB)ij · (GWcGT )ij
"
FACTORIZED-SCALE QUANTIZATION,0.3471337579617834,"(b)
= ΣCi
c V ar

(BT XcB)ij

V ar

(GWcGT )ij

= Ci||bi||2||bj||2||gi||2||g2
j ||σXσW
(20)
The equation (a) holds because the variables in different channels are independent. The equation (b)
holds because the random variables U and V are independent of each other and have zero mean."
FACTORIZED-SCALE QUANTIZATION,0.3503184713375796,"After finding that the standard deviation of O can be decoupled along rows and columns separately,
an intuitive thought is that the per-pixel scales may also satisfy similar rules. A more theoretical
proof is provided as Theorem 1.                                                                   "
FACTORIZED-SCALE QUANTIZATION,0.3535031847133758,"𝑶′ = 𝐃𝐢𝐚𝐠𝛂∙𝐎
𝑶′′ = 𝐃𝐢𝐚𝐠𝛂∙𝐎∙𝐃𝐢𝐚𝐠𝜷 Row"
FACTORIZED-SCALE QUANTIZATION,0.35668789808917195,Column
FACTORIZED-SCALE QUANTIZATION,0.35987261146496813,"Std
Std
Std"
FACTORIZED-SCALE QUANTIZATION,0.3630573248407643,"Figure 3: An illustration of Factorized Scale Quantization. Since the standard deviation of O can
be decoupled along rows and columns separately, using two vector scales, α and β, is enough for
balancing O to a similar distribution."
FACTORIZED-SCALE QUANTIZATION,0.3662420382165605,"Theorem 1. Assuming X ∼N(0, σX), W ∼N(0, σW ), the optimal per-pixel scales S∗which
minimize quantization error in Eq. (21) can be factorized as: S∗= α ∗βT ."
FACTORIZED-SCALE QUANTIZATION,0.36942675159235666,"argmin
S
EX∼N(0,σX),W ∼N(0,σW )

||O −QS(O)||2
(21)"
FACTORIZED-SCALE QUANTIZATION,0.37261146496815284,"Proof. In Proposition 1, we have proven that:"
FACTORIZED-SCALE QUANTIZATION,0.37579617834394907,"V ar [Oij] =
p"
FACTORIZED-SCALE QUANTIZATION,0.37898089171974525,"CiσXσW ||bi||2||gi||2 ·
p"
FACTORIZED-SCALE QUANTIZATION,0.3821656050955414,"CiσXσW ||bj||2||g2
j ||
(22)"
FACTORIZED-SCALE QUANTIZATION,0.3853503184713376,"Observing that Oij is the summation of Ci independent variables, where Ci is the number of input
channels and is usually large (e.g., 128). According to the Central Limit Theorem [38], we can assume
Oij follows Gaussian distribution. In supplementary material, we will show that the optimal scale s
to minimize the mean-square error of quantizing Gaussian variables z ∼N(0, σ2) is proportional to
σ, i.e., s = Kσ, where K is a constant. Thus, we have:"
FACTORIZED-SCALE QUANTIZATION,0.3885350318471338,"Sij =
p"
FACTORIZED-SCALE QUANTIZATION,0.39171974522292996,"KCi||bi||2||gi||2σXσW ·
q"
FACTORIZED-SCALE QUANTIZATION,0.39490445859872614,"KCi||bj||2||g2
j ||σXσW = αi · βj
(23)"
FACTORIZED-SCALE QUANTIZATION,0.3980891719745223,"The benefit of factorizing per-pixel scales into two vectors is that we can move both scales into
transformation matrices. So we can facilitate per-tensor matrix multiplication implementation when
per-pixel quantization is utilized:"
FACTORIZED-SCALE QUANTIZATION,0.4012738853503185,AT OA ≈AT · (SO ⊙eO) · A
FACTORIZED-SCALE QUANTIZATION,0.40445859872611467,= AT · ((α ∗βT ) ⊙eO) · A
FACTORIZED-SCALE QUANTIZATION,0.40764331210191085,= AL eOAR (24)
FACTORIZED-SCALE QUANTIZATION,0.410828025477707,"where:
AL = AT · Diag(α),
AR = Diag(β) · A
and
eO = ⌊O ⊘SO⌉
(25)"
FACTORIZED-SCALE QUANTIZATION,0.4140127388535032,"Note that at this stage, matrix A is not quantized. After rescaling, we can further quantize these
transformation matrices and middle results to obtain fully-quantized Winograd convolution. In this
paper, this method is called Factorized Scale Quantization (FSQ) and an illustration of it is shown in
Figure 3."
OPTIMIZATION PROCEDURE,0.4171974522292994,"4.2.3
Optimization procedure"
OPTIMIZATION PROCEDURE,0.42038216560509556,"In this section, we focus on how to determine the optimal α and β.
Given N samples
{O1, O2, ..., ON}, we can minimize the quantization error via the following non-linear least square
regression problems:
argmin
α,β
ΣN
n ΣCo
f ||On
f −(α ∗βT ) ⊙f
On
f ||2
(26)"
OPTIMIZATION PROCEDURE,0.42356687898089174,"The optimization of Eq. (26) is non-trivial due to the integer constraint of eO. Thus we propose an
iteration method to solve it."
OPTIMIZATION PROCEDURE,0.4267515923566879,"Solving α when fixed β and eO. When β and eO are fixed, (26) is a quadratic function with respect to
α. Then the closed-form solution for α can be easily obtained as:"
OPTIMIZATION PROCEDURE,0.4299363057324841,"α∗=
ΣN
n ΣCo
f (On
f ⊙eOn
f ) · β"
OPTIMIZATION PROCEDURE,0.43312101910828027,"ΣN
n ΣCo
f ( eOn
f ⊙eOn
f ) · (β ⊙β)
(27)"
OPTIMIZATION PROCEDURE,0.43630573248407645,Solving β When fixed α and eO. The closed-form solution for β is similar to that of α:
OPTIMIZATION PROCEDURE,0.4394904458598726,"β∗=
ΣN
n ΣCo
f (On
f ⊙eOn
f )T · α"
OPTIMIZATION PROCEDURE,0.4426751592356688,"ΣN
n ΣCo
f ( eOn
f ⊙eOn
f )T · (α ⊙α)
(28)"
OPTIMIZATION PROCEDURE,0.445859872611465,"Solving eO when fixed α and β. When α and β are fixed, eO is simply the integer values by rounding
to the nearest:
eOn
f ∗=

On
f ⊘(αT · β)

(29)"
EXPERIMENTS,0.44904458598726116,"5
Experiments"
EXPERIMENTAL SETTINGS,0.45222929936305734,"5.1
Experimental settings"
EXPERIMENTAL SETTINGS,0.4554140127388535,"We use full-precision pre-trained models and replace all 3x3 convolutions (stride 1) with Winograd
convolutions. All convolutions, including the first and last layers, are quantized using symmetric
quantization. Following BQW[24], we use per-pixel quantization for transformed inputs U and
weights V . In order to verify the effectiveness of our method, we first construct a strong baseline
on Post-training Quantization Winograd. Concretely, we resort to Adaround [18] and LSQ [14] to
quantize U and V , respectively. Following BRECQ [19], we use 1024 unlabeled images and Adam
[39] optimizer with 20k iterations and a batch size of 32. Experiments show that our strong baseline
has surpassed previous state-of-the-art PTQ Winograd work. Based on it, we further verify our
method on the strong baseline. The learning rates of A, B, and G are set to (1e-4, 1e-4, 5e-4) by
default, and the reason will be shown in Section 5.4. All our experiments are conducted on NVIDIA
GeForce RTX 3090 24GB GPU servers and last for several hours."
PTQ-AWARE WINOGRAD,0.4585987261146497,"5.2
PTQ-Aware Winograd"
PTQ-AWARE WINOGRAD,0.46178343949044587,"In this section, we compare our PTQ-Aware Winograd method to the previous work BQW [24] with
comprehensive experiments settings, including various bitwidths, tile sizes, datasets and models."
PTQ-AWARE WINOGRAD,0.46496815286624205,"Table 4: PTQ results of ResNet-20 on CIFAR-10 with different tile sizes (4,6) and different bitwidths
(4,6,8)."
PTQ-AWARE WINOGRAD,0.4681528662420382,"Model
Winograd
Bits
Quantization Accuracy"
PTQ-AWARE WINOGRAD,0.4713375796178344,"Algorithm
Strong Baseline
BQW [24]
PAW"
PTQ-AWARE WINOGRAD,0.4745222929936306,"F(4,3)
4
22.46
17.33
62.50 (+45.17)
6
80.63
79.36
90.25 (+12.65)
ResNet-20
8
89.55
90.31
92.02 (+1.71)"
PTQ-AWARE WINOGRAD,0.47770700636942676,"(91.76%)
F(6,3)
4
13.51
10.00
21.71 (+11.71)
6
46.62
39.75
85.29 (+45.54)
8
81.44
81.44
91.10 (+9.66)"
PTQ-AWARE WINOGRAD,0.48089171974522293,"CIFAR-10. The results of different PTQ methods on ResNet-20 are shown in Table 4. Although
our strong baseline has been able to outperform previous method, our PTQ-Aware Winograd (PAW)
can even improve it further, especially with lower bitwidth or larger tile size. For example, our
methods surpass BQW [24] by 12.65 % and 45.54% when using 6-bit quantization on F(4,3) and
F(6,3), respectively, which means our method makes deploying CNNs with more limited computation
resources possible."
PTQ-AWARE WINOGRAD,0.4840764331210191,"ImageNet. We also conduct experiments on ImageNet, a more challenging image classification
dataset than CIFAR-10. In this experiment, we replace the convolutions in the last block with F(4,3)
Winograd convolution since F(6,3) Winograd convolution needs more padding for 7 × 7 input size.
Noting that BQW [24] only conducts PTQ with 8-bit because of severe accuracy degeneration on
lower bitwidth, we compare our method with our strong baseline. The results presented in Table 5
demonstrate two breakthroughs of our method. First, we are the first to achieve negligible accuracy
drop with 8-bit post-training quantization on Winograd convolution. Second, we are the first to
achieve an acceptable accuracy drop with 6-bit post-training quantization on Winograd convolution."
FACTORIZED SCALE QUANTIZATION,0.4872611464968153,"5.3
Factorized Scale Quantization"
FACTORIZED SCALE QUANTIZATION,0.49044585987261147,"In this section, we use Factorized Scale Quantization (FSQ) for O and per-pixel quantization for
U and V . Other components are conducted with per-tensor quantization. We conduct experiments
on ImageNet with bitwidths 6 and 8. In 6-bit quantization, we use 6-bit to quantize U, V , and
8-bit to quantize other components. The results shown in Table 5 indicate that our proposed FSQ
is the crucial step to fully quantizing Winograd convolutions, and it is well-compatible with PTQ-
Aware Winograd (PAW). Even with fully-quantization settings, our methods can achieve comparable
accuracy compared to BQW[24]."
FACTORIZED SCALE QUANTIZATION,0.49363057324840764,"Table 5: PTQ results of ResNets on ImageNet. Previous method BQW [24] does not quantize matrix
multiplications in Winograd transformation. ‘SB’ indicates our Strong Baseline."
FACTORIZED SCALE QUANTIZATION,0.4968152866242038,"Model
Tile
Bits
Partial Quantization
Full Quantization"
FACTORIZED SCALE QUANTIZATION,0.5,"SB
BQW [24]
PAW
PAW
FSQ
FSQ+PAW"
FACTORIZED SCALE QUANTIZATION,0.5031847133757962,"F(4,3)
6
50.86
N/A
65.15
0.26
44.21
64.34
ResNet-18
8
68.08
67.54
69.06
0.21
64.46
68.16
(69.76%)
F(6,3)
6
24.58
N/A
59.58
0.13
18.68
59.30
8
65.84
60.09
68.36
0.13
58.48
66.89"
FACTORIZED SCALE QUANTIZATION,0.5063694267515924,"F(4,3)
6
58.85
N/A
69.71
0.88
52.10
68.80
ResNet-34
8
72.11
71.86
72.67
0.96
69.51
71.75
(73.30%)
F(6,3)
6
27.91
N/A
63.56
0.19
22.06
62.11
8
69.68
66.52
71.90
0.29
63.60
69.72"
FACTORIZED SCALE QUANTIZATION,0.5095541401273885,"F(4,3)
6
73.25
N/A
74.94
22.44
72.21
74.75
ResNet-50
8
75.89
75.84
76.01
17.80
75.90
75.74
(76.15%)
F(6,3)
6
69.20
N/A
73.99
5.97
66.85
73.84
8
75.34
74.47
75.80
3.20
73.66
75.36"
ABLATION STUDIES,0.5127388535031847,"5.4
Ablation studies"
ABLATION STUDIES,0.5159235668789809,"To evaluate the effectiveness of our quantization-aware Winograd, we conduct an ablation study
on ResNet-18 by optimizing different combinations of A, B, and G. Empirically, we find that the
learning rate of G is more sensitive than those of A and B. Thus we fix the learning rates of A and B
to 1e-4 for all experiments and perform a grid search on the learning rate of G separately, which is
varied in the interval [1e-7, 1e-3]. The best results are shown in Table 6. When optimizing them, the
optimal learning rate for G is 5e-4, and the performance improvement reaches 1.22%."
ABLATION STUDIES,0.5191082802547771,"Table 6: Impact of different design choices for optimization of transformation matrices, on the
ImageNet validation accuracy (%) for ResNet-18."
ABLATION STUDIES,0.5222929936305732,"Optimized Matrices
None
A
B
G
A, B
B, G
A, G
A, B, G"
ABLATION STUDIES,0.5254777070063694,"Accuracy
68.04
68.07
68.24
68.03
68.87
68.28
68.03
69.26"
CONCLUSIONS,0.5286624203821656,"6
Conclusions"
CONCLUSIONS,0.5318471337579618,"This paper focuses on accelerating deep convolution neural networks by combining the Winograd
algorithm and model quantization. We propose a unified optimization procedure to improve the
compatibility between quantization and the Winograd algorithm. To achieve a more significant
speed-up via full quantization, we propose a theoretically supported and hardware-friendly method
called Factorized-Scale Quantization, which is suitable for the distribution characteristics of tensors
in the Winograd domain. Experiments are conducted to show that our method can boost performance
by a large margin, especially with lower bitwidth or larger tile size, which means our method makes
deploying CNNs with more limited computation resources possible."
ACKNOWLEDGEMENTS,0.535031847133758,"7
Acknowledgements"
ACKNOWLEDGEMENTS,0.5382165605095541,"This work was supported in part by the STI 2030-Major Projects (No.2021ZD0201504), the NSFC-
General Technology Collaborative Fund for Basic Research (No.U1936204), the Jiangsu Key Re-
search and Development Plan (No.BE2023016, No.BE2021012-2)."
REFERENCES,0.5414012738853503,References
REFERENCES,0.5445859872611465,"[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770–778. IEEE Computer Society, 2016."
REFERENCES,0.5477707006369427,"[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep
convolutional neural networks. Commun. ACM, 60(6):84–90, 2017."
REFERENCES,0.5509554140127388,"[3] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN. In IEEE
International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017,
pages 2980–2988. IEEE Computer Society, 2017."
REFERENCES,0.554140127388535,"[4] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for
image super-resolution. In 2018 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 2472–2481. Computer Vision
Foundation / IEEE Computer Society, 2018."
REFERENCES,0.5573248407643312,"[5] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural
network with pruning, trained quantization and huffman coding. 2016."
REFERENCES,0.5605095541401274,"[6] Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional
networks for classification and detection. IEEE Trans. Pattern Anal. Mach. Intell., 38(10):1943–
1955, 2016."
REFERENCES,0.5636942675159236,"[7] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training
network quantization via bit-split and stitching. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of
Proceedings of Machine Learning Research, pages 9847–9856. PMLR, 2020."
REFERENCES,0.5668789808917197,"[8] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G. Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efficient integer-arithmetic-only inference. In 2018 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 2704–2713.
Computer Vision Foundation / IEEE Computer Society, 2018."
REFERENCES,0.5700636942675159,"[9] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer.
Zeroq: A novel zero shot quantization framework. In 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages
13166–13175. Computer Vision Foundation / IEEE, 2020."
REFERENCES,0.5732484076433121,"[10] Weihan Chen, Peisong Wang, and Jian Cheng. Towards mixed-precision quantization of
neural networks via constrained optimization. In 2021 IEEE/CVF International Conference on
Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 5330–5339.
IEEE, 2021."
REFERENCES,0.5764331210191083,"[11] Peisong Wang, Xiangyu He, Gang Li, Tianli Zhao, and Jian Cheng. Sparsity-inducing binarized
neural networks. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020,
The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020, pages 12192–12199. AAAI Press, 2020."
REFERENCES,0.5796178343949044,"[12] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In 2016
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV,
USA, June 27-30, 2016, pages 4013–4021. IEEE Computer Society, 2016."
REFERENCES,0.5828025477707006,"[13] Michaël Mathieu, Mikael Henaff, and Yann LeCun. Fast training of convolutional networks
through ffts. 2014."
REFERENCES,0.5859872611464968,"[14] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and
Dharmendra S. Modha. Learned step size quantization. 2020."
REFERENCES,0.589171974522293,"[15] Huixia Li, Chenqian Yan, Shaohui Lin, Xiawu Zheng, Baochang Zhang, Fan Yang, and
Rongrong Ji. PAMS: quantized super-resolution via parameterized max scale. In Andrea
Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision -
ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part
XXV, volume 12370 of Lecture Notes in Computer Science, pages 564–580. Springer, 2020."
REFERENCES,0.5923566878980892,"[16] Xiangyu He, Zitao Mo, Ke Cheng, Weixiang Xu, Qinghao Hu, Peisong Wang, Qingshan Liu,
and Jian Cheng. Proxybnn: Learning binarized neural networks via proxy matrices. In Andrea
Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision -
ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part
III, volume 12348 of Lecture Notes in Computer Science, pages 223–241. Springer, 2020."
REFERENCES,0.5955414012738853,"[17] Weixiang Xu, Fanrong Li, Yingying Jiang, Yong A, Xiangyu He, Peisong Wang, and Jian
Cheng. Improving extreme low-bit quantization with soft threshold. IEEE Trans. Circuits Syst.
Video Technol., 33(4):1549–1563, 2023."
REFERENCES,0.5987261146496815,"[18] Markus Nagel, Rana Ali Amjad, Mart van Baalen, Christos Louizos, and Tijmen Blankevoort.
Up or down? adaptive rounding for post-training quantization. In Proceedings of the 37th
International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,
volume 119 of Proceedings of Machine Learning Research, pages 7197–7206. PMLR, 2020."
REFERENCES,0.6019108280254777,"[19] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and
Shi Gu. BRECQ: pushing the limit of post-training quantization by block reconstruction. 2021."
REFERENCES,0.6050955414012739,"[20] Peisong Wang, Weihan Chen, Xiangyu He, Qiang Chen, Qingshan Liu, and Jian Cheng.
Optimization-based post-training quantization with bit-split and stitching. IEEE Trans. Pattern
Anal. Mach. Intell., 45(2):2119–2135, 2023."
REFERENCES,0.60828025477707,"[21] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization
through weight equalization and bias correction. In 2019 IEEE/CVF International Conference
on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages
1325–1334. IEEE, 2019."
REFERENCES,0.6114649681528662,[22] NVIDIA TensorRT. https://docs.nvidia.com/deeplearning/tensorrt/index.html.
REFERENCES,0.6146496815286624,"[23] Javier Fernández-Marqués, Paul N. Whatmough, Andrew Mundy, and Matthew Mattina. Search-
ing for winograd-aware quantized networks. 2020."
REFERENCES,0.6178343949044586,"[24] Vladimir Chikin and Vladimir Kryzhanovskiy. Channel balancing for accurate quantization of
winograd convolutions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 12497–12506. IEEE, 2022."
REFERENCES,0.6210191082802548,"[25] Lingchuan Meng and John Brothers. Efficient winograd convolution via integer arithmetic.
CoRR, abs/1901.01965, 2019."
REFERENCES,0.6242038216560509,"[26] Renzo Andri, Beatrice Bussolino, Antonio Cipolletta, Lukas Cavigelli, and Zhe Wang. Going
further with winograd convolutions: Tap-wise quantization for efficient inference on 4x4 tiles.
In 55th IEEE/ACM International Symposium on Microarchitecture, MICRO 2022, Chicago, IL,
USA, October 1-5, 2022, pages 582–598. IEEE, 2022."
REFERENCES,0.6273885350318471,"[27] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In Bastian Leibe, Jiri Matas, Nicu
Sebe, and Max Welling, editors, Computer Vision - ECCV 2016 - 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of
Lecture Notes in Computer Science, pages 525–542. Springer, 2016."
REFERENCES,0.6305732484076433,"[28] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep
neural networks with binary weights during propagations. pages 3123–3131, 2015."
REFERENCES,0.6337579617834395,"[29] Zeyu Zhu, Fanrong Li, Zitao Mo, Qinghao Hu, Gang Li, Zejian Liu, Xiaoyao Liang, and Jian
Cheng. $\rm aˆ2q$: Aggregation-aware quantization for graph neural networks. In The Eleventh
International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,
2023. OpenReview.net, 2023."
REFERENCES,0.6369426751592356,"[30] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: hardware-aware automated
quantization with mixed precision. In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 8612–8620. Computer
Vision Foundation / IEEE, 2019."
REFERENCES,0.6401273885350318,"[31] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013."
REFERENCES,0.643312101910828,"[32] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi
Srinivasan, and Kailash Gopalakrishnan. PACT: parameterized clipping activation for quantized
neural networks. CoRR, abs/1805.06085, 2018."
REFERENCES,0.6464968152866242,"[33] Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: Randomly
dropping quantization for extremely low-bit post-training quantization. 2022."
REFERENCES,0.6496815286624203,"[34] Lingchuan Meng and John Brothers. Efficient winograd convolution via integer arithmetic.
CoRR, abs/1901.01965, 2019."
REFERENCES,0.6528662420382165,"[35] Guangli Li, Lei Liu, Xueying Wang, Xiu Ma, and Xiaobing Feng. Lance: efficient low-precision
quantized winograd convolution for neural networks based on graphics processing units. In
2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP
2020, Barcelona, Spain, May 4-8, 2020, pages 3842–3846. IEEE, 2020."
REFERENCES,0.6560509554140127,"[36] Renzo Andri, Beatrice Bussolino, Antonio Cipolletta, Lukas Cavigelli, and Zhe Wang. Going
further with winograd convolutions: Tap-wise quantization for efficient inference on 4x4 tiles.
In 55th IEEE/ACM International Symposium on Microarchitecture, MICRO 2022, Chicago, IL,
USA, October 1-5, 2022, pages 582–598. IEEE, 2022."
REFERENCES,0.6592356687898089,"[37] Shmuel Winograd. Arithmetic complexity of computations, volume 33. Siam, 1980."
REFERENCES,0.6624203821656051,"[38] Robert V. Brill. Applied Statistics and Probability for Engineers, volume 46. 2004."
REFERENCES,0.6656050955414012,[39] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015.
REFERENCES,0.6687898089171974,"A
Winograd transformation matrices"
REFERENCES,0.6719745222929936,"Depending on the particular choice of Winograd domain (i.e., polynomial domain), transformation
matrices A, B, and G in the Winograd algorithm can be different. In the paper, we present that the
most popular interpolation points for F(2,3) are [0, +1, −1] and then these transformation matrices
can be constructed as follows:"
REFERENCES,0.6751592356687898,"AT =

1
1
1
0
0
1
−1
−1"
REFERENCES,0.678343949044586,"
,
BT =  "
REFERENCES,0.6815286624203821,"1
0
−1
0
0
1
1
0
0
−1
1
0
0
1
0
−1 "
REFERENCES,0.6847133757961783,",
G =  "
REFERENCES,0.6878980891719745,"1
0
0
1
2
1
2
1
2
1
2
−1"
REFERENCES,0.6910828025477707,"2
1
2
0
0
1 "
REFERENCES,0.6942675159235668,"
(30)"
REFERENCES,0.697452229299363,"For F(4,3) and F(6,3), we choose the same transformation matrices as BQW [24]. For F(4,3), the
Winograd transformation matrices are as follows: AT =  "
REFERENCES,0.7006369426751592,"1
1
1
1
1
0
0
1
−1
2
−2
0
0
1
1
4
4
0
0
1
−1
8
−8
1 "
REFERENCES,0.7038216560509554,",
(31) BT = "
REFERENCES,0.7070063694267515,
REFERENCES,0.7101910828025477,"4
0
−5
0
1
0
0
−4
−4
1
1
0
0
4
−4
−1
1
0
0
−2
−1
2
1
0
0
2
−1
−2
1
0
0
4
0
−5
0
1 "
REFERENCES,0.7133757961783439,"
,
(32) G = "
REFERENCES,0.7165605095541401,
REFERENCES,0.7197452229299363,"1
4
0
0
−1 6
−1 6
−1 6
−1"
REFERENCES,0.7229299363057324,"6
1
6
−1"
REFERENCES,0.7261146496815286,"6
1
24
1
12
−1"
REFERENCES,0.7292993630573248,"6
1
24
−1 12
−1"
REFERENCES,0.732484076433121,"6
0
0
1 "
REFERENCES,0.7356687898089171,"
(33)"
REFERENCES,0.7388535031847133,"For F(6,3), the Winograd transformation matrices are as follows: AT =  "
REFERENCES,0.7420382165605095,"1
1
1
1
1
0
0
1
−1
2
−2
0
0
1
4
4
0
0
1
−1
8
−8
1 "
REFERENCES,0.7452229299363057,",
(34) BT = "
REFERENCES,0.7484076433121019,
REFERENCES,0.7515923566878981,"1
0
−21"
REFERENCES,0.7547770700636943,"4
0
21"
REFERENCES,0.7579617834394905,"4
0
−1
0
0
1
1
−17 4
−17"
REFERENCES,0.7611464968152867,"4
1
1
0
0
−1
1
17 4
−17"
REFERENCES,0.7643312101910829,"4
−1
1
0
0
1
2
1
4
−5 2
−5"
REFERENCES,0.767515923566879,"4
2
1
0
0
−1"
REFERENCES,0.7707006369426752,"2
1
4
5
2
−5"
REFERENCES,0.7738853503184714,"4
−2
1
0
0
2
4
−5"
REFERENCES,0.7770700636942676,"2
−5
1
2
1
0
0
−2
4
5
2
−5
−1"
REFERENCES,0.7802547770700637,"2
1
0
0
−1
0
21"
REFERENCES,0.7834394904458599,"4
0
−21 4
0
1 "
REFERENCES,0.7866242038216561,
REFERENCES,0.7898089171974523,",
(35) G = "
REFERENCES,0.7929936305732485,
REFERENCES,0.7961783439490446,"1
0
0
−2 9
−2 9
−2 9
−2"
REFERENCES,0.7993630573248408,"9
2
9
−2"
REFERENCES,0.802547770700637,"9
1
90
1
45
−2"
REFERENCES,0.8057324840764332,"45
1
90
−1"
REFERENCES,0.8089171974522293,"45
2
45
32
45
16
45
8
45
32
45
−16"
REFERENCES,0.8121019108280255,"45
8
45
0
0
1 "
REFERENCES,0.8152866242038217, (36)
REFERENCES,0.8184713375796179,"B
Derivatives of transformation matrices"
REFERENCES,0.821656050955414,"In the paper, in order to align these transformation procedures after quantization, we propose to adjust
transformation matrices via an optimization procedure as follows:"
REFERENCES,0.8248407643312102,"argmin
A,B,G
EX∼D
h
ΣCo
f ||AT (ΣCi
c Q(BT XcB) ⊙Q(GWf,cGT ))A −Yf||2i
(37)"
REFERENCES,0.8280254777070064,"By using the straight-through estimator [31] to approximate the gradient through the round function
as a pass-through operation, we can obtain the derivatives of A, B and G. In this paper, we directly
present the derivative of B. Here, a more comprehensive derivation is provided as follows:"
REFERENCES,0.8312101910828026,"∂L
∂Bij
= ΣCo
f
tr"
REFERENCES,0.8343949044585988,"(
∂L
∂OT
f
· ∂Of ∂Bij ) (38)"
REFERENCES,0.8375796178343949,"= ΣCo
f
tr"
REFERENCES,0.8407643312101911,"(
∂L
∂OT
f
·

ΣCi
c (δjiXcB) ⊙Q(Vf,c) + (BT Xcδi,j) ⊙Q(Vf,c)

) (39)"
REFERENCES,0.8439490445859873,"= ΣCo
f
ΣCi
c
tr"
REFERENCES,0.8471337579617835,"(
∂L
∂OT
f
· [(δjiXcB) ⊙Q(Vf,c)] + ∂L"
REFERENCES,0.8503184713375797,"∂OT
f
·

(BT Xcδij) ⊙Q(Vf,c)

) (40)"
REFERENCES,0.8535031847133758,"= ΣCo
f
ΣCi
c
tr

(δjiXcB)T ·
 ∂L"
REFERENCES,0.856687898089172,"∂Of
⊙Q(Vf,c)

+ (BT Xcδij)T ·
 ∂L"
REFERENCES,0.8598726114649682,"∂Of
⊙Q(Vf,c)
 (41)"
REFERENCES,0.8630573248407644,"= ΣCo
f
ΣCi
c"
REFERENCES,0.8662420382165605,"
XcB · ( ∂L"
REFERENCES,0.8694267515923567,"∂Of
⊙Q(Vf,c))T
"
REFERENCES,0.8726114649681529,"ij
+

XT
c B · ( ∂L"
REFERENCES,0.8757961783439491,"∂Of
⊙Q(Vf,c))
"
REFERENCES,0.8789808917197452,"ij
(42)"
REFERENCES,0.8821656050955414,"We have obtained the derivative of Bij, and now we can provide the expression for the derivative of
B:
∂L
∂B = ΣCo
f
ΣCi
c
XcB( ∂L"
REFERENCES,0.8853503184713376,"∂Of
⊙Q(Vf,c))T + XT
c B( ∂L"
REFERENCES,0.8885350318471338,"∂Of
⊙Q(Vf,c))
(43)"
REFERENCES,0.89171974522293,"The derivatives of A, G and Of can be computed in a similar manner:"
REFERENCES,0.8949044585987261,"∂L
∂A = ΣCo
f
OT
f A(AT OfA −Yf) + OfA(AT OfA −Yf)T
(44)"
REFERENCES,0.8980891719745223,"∂L
∂G = ΣCo
f
ΣCi
c
( ∂L"
REFERENCES,0.9012738853503185,"∂Of
⊙Q(Uc))GW T
f,c + ( ∂L"
REFERENCES,0.9044585987261147,"∂Of
⊙Q(Uc))T GWf,c
(45)"
REFERENCES,0.9076433121019108,"∂L
∂Of
= 2A(AT OfA −Y )AT
(46)"
REFERENCES,0.910828025477707,"C
Optimal quantization scale for Guassion varibles"
REFERENCES,0.9140127388535032,"In Theorem 1, in order to demonstrate that the optimal per-pixel scale S can be factorized into vectors,
we rely on the conclusion that the optimal scale s∗to minimize the mean-square error of quantization
of Gaussian variables z ∼N(0, σ2) is proportional to σ, i.e., s∗= Kσ, where K is a constant. Here,
we will provide a proof of it."
REFERENCES,0.9171974522292994,"Theorem 2. Assuming z ∼N(0, σ2), the optimal scale s∗to minimize the mean-square error of
quantization of z is proportional to the standard deviation σ, i.e., s∗= Kσ, where K is a constant."
REFERENCES,0.9203821656050956,"Proof. Because z ∼N(0, σ2), z can be reparameterized as z = σ · u, where u ∼N(0, 1)."
REFERENCES,0.9235668789808917,"E

(Q(z) −z)2
=
Z ∞"
REFERENCES,0.9267515923566879,"−∞
pz(z)(Q(z) −z)2dz
(47) =
Z ∞"
REFERENCES,0.9299363057324841,"−∞
pu(u)(Q(σu) −σu)2du
(48) =
Z ∞"
REFERENCES,0.9331210191082803,"−∞
pu(u)(clip(
jσu s"
REFERENCES,0.9363057324840764,"m
, −qmin, qmax) · s −σu)2du
(49)"
REFERENCES,0.9394904458598726,"= σ2
Z ∞"
REFERENCES,0.9426751592356688,"−∞
pu(u)(clip(
 u s/σ"
REFERENCES,0.945859872611465,"
, −qmin, qmax) · s"
REFERENCES,0.9490445859872612,"σ −u)2du
(50)"
REFERENCES,0.9522292993630573,= σ2h( s
REFERENCES,0.9554140127388535,"σ )
(51)"
REFERENCES,0.9585987261146497,"Eq. (47) can be treated as a function of s/σ when solving for s with σ as a known value. Assuming
K minimizes function h(x), i.e., K = argmin
x
h(x), we have:"
REFERENCES,0.9617834394904459,"s∗= argmin
s
E

(Q(z) −z)2
= argmin
s
σ2h( s"
REFERENCES,0.964968152866242,"σ ) = K · σ
(52)"
REFERENCES,0.9681528662420382,"D
Experiments on other architectures"
REFERENCES,0.9713375796178344,"In Section 5, we compare our methods to previous work BQW[24] on the ResNet model family with
comprehensive experiment settings, including various bit widths, tile sizes, and datasets. Here, we
present a similar analysis for two other popular architectures VGG and Squeezenet using the Cifar-10
dataset. The results are shown in Table 1 and Table 2. These results align with our analysis in Section
5. Our PTQ-Aware Winograd (PAW) method outperforms the strong baseline introduced in Section 5
and our FSQ method is well-compatible with PAW."
REFERENCES,0.9745222929936306,Table 1: PTQ results of VGG11 on CIFAR-10.
REFERENCES,0.9777070063694268,"Model
Tile
Bits
Partial Quantization
Full Quantization"
REFERENCES,0.9808917197452229,"Baseline
PAW
FSQ
FSQ+PAW"
REFERENCES,0.9840764331210191,"F(4,3)
6
89.13
91.56
86.59
91.55
VGG-11
8
92.02
92.28
90.82
91.83
(92.02%)
F(6,3)
6
75.10
89.94
68.98
90.34
8
91.27
91.88
88.44
91.63"
REFERENCES,0.9872611464968153,Table 2: PTQ results of SqueezeNet on CIFAR-10.
REFERENCES,0.9904458598726115,"Model
Tile
Bits
Partial Quantization
Full Quantization"
REFERENCES,0.9936305732484076,"Baseline
PAW
FSQ
FSQ+PAW"
REFERENCES,0.9968152866242038,"F(4,3)
6
89.69
91.98
88.66
91.78
SqueezeNet
8
92.61
92.68
92.01
92.80
(92.62%)
F(6,3)
6
80.50
90.67
76.48
91.26
8
92.37
92.61
90.54
92.42"
