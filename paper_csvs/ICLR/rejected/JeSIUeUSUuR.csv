Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0019723865877712033,"What makes an artiﬁcial neural network easier to train or to generalize better
than its peers? We introduce a notion of variability to view such issues under the
setting of a ﬁxed number of parameters which is, in general, a dominant cost-factor.
Experiments verify that variability correlates positively to the number of activations
and negatively to a phenomenon called Collapse to Constants, which is related but
not identical to vanishing gradient. Further experiments on stylized problems show
that variability is indeed a key performance indicator for fully-connected neural
networks. Guided by variability considerations, we propose a new architecture
called Householder-absolute neural layers, or Han-layers for short, to build high
variability networks with a guaranteed immunity to gradient vanishing or exploding.
On small stylized models, Han-layer networks exhibit a far superior generalization
ability over fully-connected networks. Extensive empirical results demonstrate
that, by judiciously replacing fully-connected layers in large-scale networks such
as MLP-Mixers, Han-layers can greatly reduce the number of model parameters
while maintaining or improving generalization performance. We will also brieﬂy
discuss current limitations of the proposed Han-layer architecture."
INTRODUCTION,0.0039447731755424065,"1
INTRODUCTION"
OVERVIEW,0.005917159763313609,"1.1
OVERVIEW"
OVERVIEW,0.007889546351084813,"Deep neural networks (DNNs) have greatly advanced the state of the arts in many machine learning
tasks such as image classiﬁcation, text categorization, speech recognition, to name just a few out of a
long list. Despite their tremendous successes, it remains to be fully understood why DNNs work so
well on so many tasks in machine learning. In this paper, we take an intuitive approach to providing
a new angle from which one can qualitatively investigate certain behaviors of DNNs and provide
explanations to some critical issues. Our new angle is based on a notion of variability for DNNs
which seems to have not been speciﬁcally examined in the past. Our study of variability focuses
primarily on function (forward propagation) values rather than on the gradient (back-propagation)
values, offering a different insight from gradient-based views."
OVERVIEW,0.009861932938856016,"In essence, the proposed variability is a qualitative surrogate of expressivity or expressiveness for
neural networks (see a recent survey paper (G¨uhring et al., 2020) and references thereof). Instead
of giving a precise, quantitative analysis on what kinds of functions can or cannot be expressed
by certain neural network models which in general can be exceedingly difﬁcult, we opt to give a
qualitative characterization that, to a degree, reﬂects the levels of expressivity, thus sidestepping
hurdle in analyses while still providing useful information on the expressiveness of neural networks.
In doing so, we aim to ﬁnd useful guidelines for developing new network architectures."
OVERVIEW,0.011834319526627219,"We show that for fully connected and ReLU-activated DNNs with a ﬁxed number of model parameters,
the higher variability is (within a reasonable range), the easier it is to train the model to reach high-
quality solutions. In particular, variability rises with a quantity called activation ratio, or AR (see (5)"
OVERVIEW,0.013806706114398421,Under review as a conference paper at ICLR 2022
OVERVIEW,0.015779092702169626,"for deﬁnition), and falls with the occurrence of a phenomenon called Collapse to Constant (C2C),
which is not identical (but related) to gradient vanishing. Guided by the variability viewpoint, we
propose a novel neural-layer design aimed to increase AR and decrease the chance of C2C at the
same time."
MAIN CONTRIBUTIONS,0.01775147928994083,"1.2
MAIN CONTRIBUTIONS"
MAIN CONTRIBUTIONS,0.01972386587771203,"The purpose of this work is to gain more insights into the behaviors of DNNs and then use them
to build new DNN models. We study the notion of variability that reﬂects DNN’s expressivity and
trainability. Guided by variability, we construct a new architecture called Householder-absolute
neural layer or network (Han-layer or HanNet) to attain higher variability with fewer parameters. As
the name suggests, in a fully connected layer or network (FC-layer or FCNet), we replace square
weight matrices with Householder reﬂectors and use the absolute-value function for activations. To
sum up, our contributions are both conceptual and practical, consisting of the following aspects."
MAIN CONTRIBUTIONS,0.021696252465483234,"We study the notion of variability, along with a measurement for it, for DNNs that adds a new angle
to view and explain the behaviors of DNNs. We construct HanNet to achieve a high variability and at
the same time guarantee an immunity to vanishing or exploding gradient. The chance of Collapse to
Constants, yet not completely eliminated in theory, has also been greatly diminished."
MAIN CONTRIBUTIONS,0.023668639053254437,"On stylized small problems, our experiments suggest that variability is indeed a key performance
indicator for ReLU-activated FCNets and that HanNets possess an unusually high level of generaliza-
tion ability. As is illustrated in Figure 4 in Section 5.2.1, a HanNet outperforms FCNets by a huge
margin, producing nearly perfect results."
MAIN CONTRIBUTIONS,0.02564102564102564,"On several standard datasets in regression and image classiﬁcation, our experiments indicate that
comparing to FCNets, HanNets can greatly reduce the number of model parameters, sometimes by
orders of magnitude, while still maintaining and often improving generalization performance. This
property represents a promising feature with potential impacts on large-scale applications."
MAIN CONTRIBUTIONS,0.027613412228796843,"Many important issues remain to be investigated in order to better understand the power and the
limits of HanNets, and to unlock their potentials in real-world applications."
RELATED WORK,0.029585798816568046,"2
RELATED WORK"
RELATED WORK,0.03155818540433925,"As is well known, initialization is a critical step towards the success or failure of neural network
training. We start our investigation by visualizing landscapes of DNNs with properly initialized
parameters with different activation functions. Landscapes of DNNs have been examined from
various angles and for different purposes, for example see (Kawaguchi, 2016; Li et al., 2017; Laurent
& Brecht, 2018; Kuditipudi et al., 2019; Fort et al., 2020; Sun et al., 2020). In our case, visualization
results lead to the concept of variability. The two key building blocks in our proposed HanNets are
(1) the absolute-value (ABS) function as an activation function and (2) the Householder reﬂectors
as the orthogonal weight matrices — two topics that previously appeared in literature within quite
different contexts from ours; for example, ABS functions in (Batruni, 1991; Lin & Unbehauen, 1992;
Karnewar, 2018) and Householder matrices in (Mhammedi et al., 2017; Tomczak & Welling, 2016;
Vorontsov et al., 2017; Wang et al., 2020; Wisdom et al., 2016; Zhang et al., 2018). In addition, other
related works will be referenced throughout the paper."
NOTATIONS,0.03353057199211045,"2.1
NOTATIONS"
NOTATIONS,0.03550295857988166,"Given an activation function φ(·) and a sequence of weight-bias parameters {(Wi, bi)} of suitable
sizes, we ﬁrst deﬁne layer functions"
NOTATIONS,0.03747534516765286,"ψi(·) ≡ψi(·, Wi, bi) := Wiφ(·) + bi, i = 1, 2, · · · ,
(1)"
NOTATIONS,0.03944773175542406,"each of which is the composition of an afﬁne function with the activation φ(·). As mentioned above,
we often drop the dependence of ψi on the parameter pair (Wi, bi) whenever no confusion arises. For"
NOTATIONS,0.04142011834319527,Under review as a conference paper at ICLR 2022
NOTATIONS,0.04339250493096647,"any positive integer k, deﬁne
Fk(x) := (ψk ◦· · · ◦ψ1)(x),
(2)"
NOTATIONS,0.045364891518737675,"which is the composition of ψ1, . . . , ψk, parameterized by the weight-bias pairs (Wi, bi) for i =
1, · · · , k. In general, Wi ∈Rni+1×ni, bi ∈Rni+1 and φi : Rni+1 →Rni+1, thus function Fk(x) is
from Rn1 to Rnk. For an L-layer DNN, the model output is FL(x, W, b), where W = {W1, ..., WL}
and b = {b1, ..., bL}. For any given parameter pair, the network maps an input x to an output
FL(x, W, b), which can be computed through the forward propagation:"
NOTATIONS,0.047337278106508875,"s0 = x; zk = Wksk−1 + bk, sk = φ(zk), k = 1, · · · , L.
(3)"
RISE AND FALL OF VARIABILITY,0.04930966469428008,"3
RISE AND FALL OF VARIABILITY"
RISE AND FALL OF VARIABILITY,0.05128205128205128,"We start with a set of simple experiments in Section 3.1 to observe the landscapes of neural network
functions FL(x) as the network depth L increases while the total number of model parameters is kept
approximately unchanged."
LANDSCAPE EXPERIMENTS,0.05325443786982249,"3.1
LANDSCAPE EXPERIMENTS"
LANDSCAPE EXPERIMENTS,0.055226824457593686,"To facilitate visualization, we add an input layer and an output layer, both of dimension 2, to the L
hidden layers. For convenience, we continue to use FL(x) to denote the extended network. The data
set is a set of grid points on the square [−1, 1]2 ⊂R2 over which we compute and plot the surface
z = ∥FL(x, W, b)∥2 for randomly sampled (W, b) with proper scalings. Three activation functions,
Sigmoid φ(t) = 1/(1 + e−t), ReLU φ(t) = max(0, t) and ABS φ(t) = |t|, are used. For ReLU we
use Kaiming initialization, and for other two functions we use Xavier initialization (Glorot & Bengio,
2010; He et al., 2015)."
LANDSCAPE EXPERIMENTS,0.05719921104536489,"We keep the total number of parameters in (W, b) at around 10,000. For each given depth value L,
we calculate the corresponding width d (rounded to the nearest integetr), then compute and plot z
over the grid on [−1, 1] for 5 different samples for (W, b). Figure 1 is the result of one single random
sample, while the complete plots are given in Appendix (see Figures 12, 13 and 14, where each row
consists of 5 plots corresponding to 5 random parameter samples)."
LANDSCAPE EXPERIMENTS,0.05917159763313609,Sigmoid 0.90 0.92 0.94 0.96 0.98 1.00
LANDSCAPE EXPERIMENTS,0.0611439842209073,0.9994
LANDSCAPE EXPERIMENTS,0.0631163708086785,0.9995
LANDSCAPE EXPERIMENTS,0.0650887573964497,0.9996
LANDSCAPE EXPERIMENTS,0.0670611439842209,0.9997
LANDSCAPE EXPERIMENTS,0.06903353057199212,0.9998
LANDSCAPE EXPERIMENTS,0.07100591715976332,0.9999 1.000
LANDSCAPE EXPERIMENTS,0.07297830374753451,0.99970
LANDSCAPE EXPERIMENTS,0.07495069033530571,0.99975
LANDSCAPE EXPERIMENTS,0.07692307692307693,0.99980
LANDSCAPE EXPERIMENTS,0.07889546351084813,0.9998
LANDSCAPE EXPERIMENTS,0.08086785009861933,0.9999
LANDSCAPE EXPERIMENTS,0.08284023668639054,0.9999
LANDSCAPE EXPERIMENTS,0.08481262327416174,1.0000 −3.5 −3.0 −2.5 −2.0 −1.5 −1.0 −0.5 0.0 0.96 0.98 1.00 1.02 1.04 ReLU −1.0 −0.5 0.0 0.5 1.0
LANDSCAPE EXPERIMENTS,0.08678500986193294,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 0.5 0.6 0.7 0.8 0.9 1.0"
LANDSCAPE EXPERIMENTS,0.08875739644970414,"0.800
0.825
0.850
0.875
0.900
0.925
0.950
0.975
1.000 0.96 0.98 1.00 1.02 1.04 ABS"
LANDSCAPE EXPERIMENTS,0.09072978303747535,"0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.5 0.6 0.7 0.8 0.9 1.0 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.5 0.6 0.7 0.8 0.9 1.0"
LANDSCAPE EXPERIMENTS,0.09270216962524655,HanNet 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.80 0.85 0.90 0.95 1.00 0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000 0.70 0.75 0.80 0.85 0.90 0.95 1.00
LANDSCAPE EXPERIMENTS,0.09467455621301775,"Figure 1: Landscape of ∥FL(x, W, b)∥2. Rows 1 to 3 are for FCNets with 3 activations: Sigmoid
(L = 2, 4, 6, 10, 15), ReLU and ABS (L = 2, 10, 20, 40, 60). Row 4 is for HanNets (to be deﬁned)
with the same L values as for ReLU and ABS."
LANDSCAPE EXPERIMENTS,0.09664694280078895,"Based on these plots, we make the following observations. For Sigmoid, the surfaces are rather
monotonous with few variations either in the x-space (within each plot) or in the parameter space"
LANDSCAPE EXPERIMENTS,0.09861932938856016,Under review as a conference paper at ICLR 2022
LANDSCAPE EXPERIMENTS,0.10059171597633136,"(across the 5 plots). In particular, in the x-space, we hardly see any peaks and valleys. Most
remarkably, with slightly larger L values, the surfaces become constants. We call this curious
phenomenon “Collapse to Constants” or simply C2C. For ReLU, the landscapes have much richer
expressions from the very beginning (L = 2). The amount of variations increases as L grows deeper
from 2 to 10 and even to 20. However, at L = 40 and 60, the phenomenon of C2C again shows up.
ABS Activation seems more C2C-resistive than ReLU. Many peaks and valleys still exist at deep
depths (L = 40, 60)."
LANDSCAPE EXPERIMENTS,0.10256410256410256,"Upon further examinations, it is clear that for large L not only the scalar function ∥FL(x, W, b)∥
tends to constants in x-space, but in fact the vector-valued function FL(x, W, b) itself tends to
constant vectors for all x ∈[−1, 1]2. We will explain this C2C phenomenon later."
VARIABILITY AND A MEASUREMENT FOR IT,0.10453648915187377,"3.2
VARIABILITY AND A MEASUREMENT FOR IT"
VARIABILITY AND A MEASUREMENT FOR IT,0.10650887573964497,"From the above experimental results, we see unmistakable differences in the outputs of network
functions FL(x, W, b) from one activation function to another and as the depth L grows. All the
differences are rooted in the richness of landscape variations in the data space where x varies, as well
as in the parameter space where (W, b) varies. We will refer to such richness of landscape variations
as variability of network functions."
VARIABILITY AND A MEASUREMENT FOR IT,0.10848126232741617,"Intuitively speaking, higher variability means not only higher expressiveness in data space, but also
better responsiveness to parameter changes. On the other hand, a network with low variability means
that it may have a poor approximation power and consequently may be difﬁcult to train. In particular,
a deep network would have lost all variability, in certain regions of the parameter space, when it
becomes a constant in the entire data space for parameters from those regions, which we refer to
as Collapse to Constant (C2C). In Appendix A.4, we treat C2C and its characteristics with more
details. Variability may serve as a predictive indicator on trainability of suitable DNNs. We present
numerical evidence in Appendix B, verifying that the pattern of variability is highly correlated to the
performance of corresponding DNNs."
VARIABILITY AND A MEASUREMENT FOR IT,0.11045364891518737,"In general, it is nontrivial to develop effective and computable measures for variability, which should
be a subject of investigations on its own right. Here we present a variability measurement that seems
to have worked well with two-dimensional data spaces. We deﬁne"
VARIABILITY AND A MEASUREMENT FOR IT,0.11242603550295859,"V3 := E(W,b)"
VARIABILITY AND A MEASUREMENT FOR IT,0.11439842209072978,"
∥f∥−1
∞ Z Ω"
VARIABILITY AND A MEASUREMENT FOR IT,0.11637080867850098,"∆3f
∆x3
i"
DX,0.11834319526627218,"1
dx

(4)"
DX,0.1203155818540434,"where f ≡f(FL(x; W, b)) is a scalar-valued function, the mean is taken over a certain region
of interest in the parameter space, and ∆3f/∆x3
i is a ﬁnite-difference approximation of the third
derivative over a grid. The phenomenon of C2C occurs when V3 is close to zero. For example, if f is
a least squares loss function and FL(·) is linear, then V3 vanishes. On the other hand, when FL(·)
has high nonlinearity, then the value V3 will be relatively large."
DX,0.1222879684418146,"In Figure 2, we plot the variability of FL(x; W, b) on Ω= [−1, 1]2, computed as described in
Appendix A.2, for network depth L varying from 3 to 45 (with increment 3). As we see on the left
plot for ReLU, the V3-variability initially increases with L until it reaches its peak at around L = 16.
Afterwards, it starts to decline until it vanishes suddenly at L = 25 because the network output
becomes constant at least at one parameter sample, making the geometric mean zero. On the middle
plot for absolute-value, besides a slightly higher proﬁle, after the peak variability declines but does
not vanish up to L = 60."
DX,0.1242603550295858,"3.3
RISE WITH ACTIVATION RATIO AND FALL WITH C2C"
DX,0.126232741617357,"It should be clear that variability comes from nonlinear activations in the model. As we see from the
previous variability experiments, variability initially always rises as the network depth increases until
it reaches a peak. There is a simple explanation for this. That is, when the total number of parameters
is ﬁxed, the number of activations always increases with the depth. The activation ratio (AR) for a"
DX,0.1282051282051282,Under review as a conference paper at ICLR 2022
DX,0.1301775147928994,"0
10
20
30
40
50
60
 0.000 0.002 0.004 0.006 0.008 0.010 0.012  "
DX,0.13214990138067062,(a) FCNet-ReLU
DX,0.1341222879684418,"0
10
20
30
40
50
60
 0.000 0.002 0.004 0.006 0.008 0.010 0.012  "
DX,0.13609467455621302,(b) FCNet-ABS
DX,0.13806706114398423,"0
10
20
30
40
50
60
 0.000 0.002 0.004 0.006 0.008 0.010 0.012  "
DX,0.14003944773175542,(c) HanNet
DX,0.14201183431952663,"Figure 2: Variability measured by V3 in (4) for FCNet-ReLU (left) and FCNet-ABS (middle) and
HanNets (right). Each bar represents a geometric mean of 3000 parameter samples. As depth L
grows, the width d decreases so that the total number of model parameters is approximately 4000.
The structure of HanNets is the same as that of FCNets with far fewer model parameters."
DX,0.14398422090729784,model is deﬁned below
DX,0.14595660749506903,AR = total number of individual activations
DX,0.14792899408284024,"total number of model parameters
.
(5)"
DX,0.14990138067061143,"The activation ratio is monotonically increasing with depth L. In particular, the rate of increase is
the largest at L = 1, which shows that the activation ratio increases most rapidly at the beginning,
explaining why variability rises quickly at the very beginning, see Appendix A.3 for a formula and
more details. In this work, we will also use AR as one of the metrics for variability. However,
we caution against the simplistic view that the higher the AR or variability is, the better. In fact,
experiments indicate that excessive nonlinearity tends to increase training difﬁculties."
DX,0.15187376725838264,"It should be clear that variability declines and eventually vanishes as C2C develops and materializes.
A detailed treatment, including a quantitative characterization of C2C, is given in Appendix A.4."
DX,0.15384615384615385,"4
HANNET: A HIGH VARIABILITY MODEL"
DX,0.15581854043392504,"A path to enhancing and maintaining variability of a DNN is to raise its resistance level to C2C and
increase its AR simultaneously. Our construction of Han-layers follows exactly this path."
DX,0.15779092702169625,"4.1
HOUSEHOLDER WEIGHTING: ORTHOGONAL AND HIGH AR"
DX,0.15976331360946747,It is well-known that Householder reﬂection matrix associated with a nonzero vector u ∈Rn is
DX,0.16173570019723865,"H(u) = I −2uu⊺/∥u∥2,
(6)"
DX,0.16370808678500987,"which is symmetric and orthogonal. As is well known, orthogonality is a desirable property for
weight matrices in DNNs. In addition, the degree of freedom in H(u) is n, one order of magnitude
smaller than O(n2) for a generic orthogonal matrix."
DX,0.16568047337278108,"Evidently, the activation ratio of a DNN can be increased by reducing the number of model parameters
without changing the network depth and width, such as by parametrizing an n × n weight matrix
with far less than O(n2) parameters (similar to what is done in CNNs). This motivates us to use a
Householder reﬂection matrix to replace a general matrix or a generic orthogonal matrix in a neural
layer so that the AR value associated with this layer is enlarged by a factor of O(n). In natural
language processing areas, it has been proposed to use a product of multiple Householder matrices to
parameterize weight matrices in recursive neural networks, for example (Mhammedi et al., 2017)."
HAN-LAYERS AND HIGH VARIABILITY,0.16765285996055226,"4.2
HAN-LAYERS AND HIGH VARIABILITY"
HAN-LAYERS AND HIGH VARIABILITY,0.16962524654832348,"A Householder-absolute neural layer, or Han-layer, is composed of a Householder matrix followed
by ABS activation (see Appendix C.1 for an algorithmic form), and a HanNet structure is created by
using multiple Han-layers. We have already visualized the landscape of a deep HanNet in Figure 1,
which has more peaks and valleys than the others implying a higher level of variability. In Figure"
HAN-LAYERS AND HIGH VARIABILITY,0.17159763313609466,Under review as a conference paper at ICLR 2022
HAN-LAYERS AND HIGH VARIABILITY,0.17357001972386588,"2, we calculate the variability measure V3 deﬁned in (4), where the geometric mean is used that
vanishes whenever C2C occurs. We observe that the variability of HanNets remains at a high level
without deterioration in the entire tested range."
IMMUNITY TO GRADIENT VANISHING OR EXPLODING,0.1755424063116371,"4.3
IMMUNITY TO GRADIENT VANISHING OR EXPLODING"
IMMUNITY TO GRADIENT VANISHING OR EXPLODING,0.17751479289940827,"Let the neural network function FL(x, W, b) be deﬁned in (2). The spectrum of a so-called GL-matrix
(see (12) in Appendix A.4) characterizes whether vanishing or exploding gradient would happen
or not. Roughly speaking, GL-matrix is the dominant part of the gradient. As L →∞, vanishing
gradient corresponds to GL →0 and exploding gradient to GL →∞. For HanNet, with the notation
u = {uk}L
k=1, the GL-matrix takes the form"
IMMUNITY TO GRADIENT VANISHING OR EXPLODING,0.1794871794871795,"GL(x, u, b) = L
Y"
IMMUNITY TO GRADIENT VANISHING OR EXPLODING,0.1814595660749507,"k=1
H(uk)∇φ(zk),
(7)"
IMMUNITY TO GRADIENT VANISHING OR EXPLODING,0.1834319526627219,"where H(uk) is deﬁned by (6) and ∇φ(zk) is the diagonal, Jacobian matrix of φ(t) = |t| evaluated
component-wise at a vector zk. Evidently, the diagonal matrices ∇φ(zk) are, with a high probability,
all orthogonal matrices with diagonal entries equal to ±1. Hence, the following result holds implying
that gradient can never vanish or explode (in a probability sense).
Proposition 1. Under a mild distribution assumption, the G-matrix for HanNet, that is, GL(x, u, b)
deﬁned in 7, remains orthogonal with probability one for any x, any ui ̸= 0 in u, any b, and any
integer L > 0."
IMMUNITY TO GRADIENT VANISHING OR EXPLODING,0.1854043392504931,"4.4
HAN/MLP-MIXER"
IMMUNITY TO GRADIENT VANISHING OR EXPLODING,0.1873767258382643,"In literature, the term Multi-Layer Perceptron (MLP) is often used exchangeably with FCNet.
Recently, MLP-dominated models have seen a wave of revivals for image recognition tasks (Tolstikhin
et al., 2021; Liu et al., 2021). MLP-dominated models (without multi-head attentions) are much
more concise than Transformer-based models (Dosovitskiy et al., 2020; Touvron et al., 2021) but
can still maintain testing performances on very large-scale datasets. The motivation of MLP-Mixer
(Tolstikhin et al., 2021) is to use the purely fully-connected layers to remove attention architectures.
An MLP-Mixer block is the elementary unit in MLP-Mixer models that consists of several FC-layers
and skip-connections to form the following map from input X to output Y ,
Z = X + GELU (W2 Layer Norm(W1X)) ,
(8)"
IMMUNITY TO GRADIENT VANISHING OR EXPLODING,0.1893491124260355,"Y = Z + GELU (Layer Norm(ZW3) W4) ,
(9)
where the ﬁrst row (8) is called token-mixing for cross-token communication and the second row
(9) is called channel-mixing for cross-channel communication, both being of MLP structure. Here
we form our Han-Mixer block by replacing all weight matrices Wi by Householder matrices Hi,
i = 1, 2, 3, 4, and all activation functions by the absolute function ABS, that is,
Z = ABS(H2ABS(H1X)), Y = ABS(ABS(ZH3)H4),
(10)
where we remove skip-connections and layer-normalizations (since HanNets does not suffer from
gradient problems). See Appendix C.2 for ﬁgurative illustrations of MLP-Mixer vs. Han-Mixer
blocks. The resulting Han/MLP-Mixer models are shown in Figure 3, where we arrange some Han-
Mixer blocks after MLP-Mixer blocks (which may be empty). The main reason for us to combine the
two types of Mixer blocks is that, short of drastically increasing network width, Han-Mixers alone
cannot always provide enough model parameters for large-scale datasets. In addition, we use the
convolution stem recommended by (Xiao et al., 2021) instead of the one in (Tolstikhin et al., 2021)."
EXPERIMENTS,0.1913214990138067,"5
EXPERIMENTS"
DATASETS,0.1932938856015779,"5.1
DATASETS"
DATASETS,0.1952662721893491,"To empirically investigate the efﬁcacy of HanNets in comparison to FCNets, we use multiple
datasets including a synthetic dataset Checkerboard, 2 classic regression datasets, Elevators (Dua"
DATASETS,0.19723865877712032,Under review as a conference paper at ICLR 2022 class
DATASETS,0.1992110453648915,"Global Avg
Pooling"
DATASETS,0.20118343195266272,"3x3 conv
stride 1 or 2"
DATASETS,0.20315581854043394,"1x1 conv
stride 1"
DATASETS,0.20512820512820512,Mixer Block
DATASETS,0.20710059171597633,Han Block
DATASETS,0.20907297830374755,Linear Classifier
DATASETS,0.21104536489151873,convolution stem × 4
DATASETS,0.21301775147928995,Figure 3: Overall structure of the tested Han/MLP-Mixer model (LM can be zero).
DATASETS,0.21499013806706113,"& Graff, 2017) and Cal Housing (Pace & Barry, 1997), and 4 widely used image classiﬁcation
datasets CIFAR10, CIFAR100, STL10, and ImageNet32 (Krizhevsky et al., 2009; Coates et al.,
2011; Chrabaszcz et al., 2017), where the last one is a down-sampled version of ImageNet (out of
affordability considerations). All dataset settings and training details are given in Appendix D.1."
RESULTS,0.21696252465483234,"5.2
RESULTS"
RESULTS,0.21893491124260356,"5.2.1
SYNTHETIC DATASET: CHECKERBOARD"
RESULTS,0.22090729783037474,"The ground truth and the training set, along with a couple of typical results, are plotted in Figure 4. − −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00"
RESULTS,0.22287968441814596,(a) Training set (25%) −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
RESULTS,0.22485207100591717,(b) FCNet: 85.2% test acc −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
RESULTS,0.22682445759368836,(c) HanNet: 99.5% test acc
RESULTS,0.22879684418145957,"Figure 4: The top row consists 3 functions (from left to right): the target binary function, a function
trained from a FCNet and one from a HanNet, respectively. The bottom row: (a) the training set with
25% of data points, (b) and (c) are top views of the two trained solutions (rounded to 0 or 1), along
with their test accuracies. The training accuracies are nearly 100% for both."
RESULTS,0.23076923076923078,"On this dataset, we compare a range of FCNets with HanNets (in fact, ResNets were also compared,
without better results than those from FCNets). We generate more than 200 pairs of FCNets and
HanNets with width varying from 20 to 100 (increment 10) and depth from 2 to 30. Figure 5 (a,
b) gives the test results in the heat-map form. We observe from Figure 5 that overall there exists a
large gap in testing accuracy between the FCNets (around 85%) and HanNet (over 99%) over a wide
range of test cases. The unexpected, near-perfect results obtained by HanNets, without any explicit
regularization, look quite stunning and are stable in the sense that similar results can be trained from
other random 25% vs. 75% data-splittings. We observe that over-parametrization is not a necessary
condition for near perfect solutions. In fact, 400 parameters are sufﬁcient to achieve near-zero testing
error in one of the cases in Figure 5(b)."
RESULTS,0.23274161735700197,"An ablation study in Appendix D.2 suggests that Householder weighting and ABS activating be
equally critical to the surprising performance of HanNets on the Checkerboard dataset. In addition, A
stability study in Appendix D.3 is done in which a portion of training labels is randomly ﬂipped, and
the results show that with seriously damaged labels HanNets still maintain a clear advantage over"
RESULTS,0.23471400394477318,Under review as a conference paper at ICLR 2022
RESULTS,0.23668639053254437,"20
30
40
50
60
70
80
90
100
"
RESULTS,0.23865877712031558,"2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
  0.64 0.72 0.80 0.88 0.96"
RESULTS,0.2406311637080868,"20
30
40
50
60
70
80
90
100
"
RESULTS,0.24260355029585798,"2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
  0.64 0.72 0.80 0.88 0.96"
RESULTS,0.2445759368836292,"0
5
10
15
20
25
 0.75 0.80 0.85 0.90 0.95 1.00"
RESULTS,0.2465483234714004,"	
 "
RESULTS,0.2485207100591716,"FCNet-s
FCNet-m
FCNet-l"
RESULTS,0.2504930966469428,"HanNet-s
HanNet-m
HanNet-l"
RESULTS,0.252465483234714,"Figure 5: (Left) FCNet: the best testing accuracy is up to 87%. (Mid) HanNet: the best testing
accuracy is over 99%. (Right) FCNet vs HanNet: the number of model parameters in each line is
ﬁxed, respectively, at 4000, 8000 and 20000 for FCNet-s, FCNet-m and FCNet-l, and at 1000, 3000,
5000 for the 3 lines for HanNets. In all experiments, the training errors are near zero. The plot shows
the existence of signiﬁcant gaps in generalization ability between FCNets and HanNets."
RESULTS,0.25443786982248523,"FCNets in testing performance. In addition, extensive experiments show that variability is closely
and positively related to trainability (see Appendix B for details)."
RESULTS,0.2564102564102564,"5.2.2
REGRESSION DATASETS: ELEVATORS AND CAL HOUSING"
RESULTS,0.2583826429980276,"On the two regression Datasets, we compare the performance of a HanNet with two FCNets. Table 1
lists the relevant statistics for the 3 DNNs where depth refers to the number of hidden layers (there
exist additional, data-size-dependent input/output layers). We see that in terms of parameter numbers
FCNet1 and HanNet are comparable peers, while FCNet2 has about 15 times more parameters."
RESULTS,0.2603550295857988,"Model
FCNet1
FCNet2
HanNet"
RESULTS,0.26232741617357,"Depth × Width (#param)
5 × 50 (10.9K)
5 × 200 (165K)
20 × 200 (10.6K)"
RESULTS,0.26429980276134124,Table 1: Essential statistics of DNNs compared (more details in Appendix D.4).
RESULTS,0.26627218934911245,"We present testing NRMSE (normalized root mean square error) loss values in Figure 11 in Ap-
pendix D.4, where the curves are average values over 5 trials with error bars. Clearly, HanNet
outperforms its “peer” FCNet1 by a notable margin, especially when fewer training samples are
used, and is competitive with (on 80% training data) or better than (on 20% training data) FCNet2
which uses 15 times more parameters. We mention that the best testing performance of HanNet
is statistically the same as that reported in (Tsang et al., 2018) with an FCNet model that uses
over 5 million parameters. Additionally, with fewer parameters, HanNet appears less inﬂuenced by
overﬁtting."
IMAGE CLASSIFICATION DATASETS,0.2682445759368836,"5.2.3
IMAGE CLASSIFICATION DATASETS"
IMAGE CLASSIFICATION DATASETS,0.2702169625246548,"In this section, we investigate HanNets’ generalization ability in image classiﬁcation by comparing
MLP-Mixers with our Han/MLP-Mixers presented in Subsection 4.4. The comparisons are done
with 3 standard image datasets and a down-sampled version of ImageNet (due to computing source
limitation). More experimental details are given in Appendix D.5."
IMAGE CLASSIFICATION DATASETS,0.27218934911242604,"Empirical evidences from MLP-based models (Tolstikhin et al., 2021) suggest that these models can
achieve the state-of-the-art results on very large-scale datasets. Since the datasets used in our tests
are not large enough to play to the full strengths of Mixer models, our aim here is not to observe
how close our results can approach the state-of-the-art, but how Han-layer structures can impact the
performance of Mixer models."
IMAGE CLASSIFICATION DATASETS,0.27416173570019725,"Tables 2 and 3 report test results from various MLP-Mixer and H/M-Mixer models. On the smaller
dataset STL10, HanMixers alone can already produce the best result. On larger datasets CIFAR10
and CIFAR100, enhanced performance from HanMixers are achieved either by adding a relatively
few parameters (e.g., MLPMixer (2 or 4) vs MLPMixer (2 or 4) + HanMixer (16)), or even with"
IMAGE CLASSIFICATION DATASETS,0.27613412228796846,Under review as a conference paper at ICLR 2022
IMAGE CLASSIFICATION DATASETS,0.2781065088757396,"#Param
STL10
CIFAR10
CIFAR100"
IMAGE CLASSIFICATION DATASETS,0.28007889546351084,"CNN stem
1.82 M
18.2 %
7.2 %
27.8 %
+MLPMixer (2)
2.89 M
18.2 %
5.3 %
27.2 %
+MLPMixer (4)
3.96 M
18.7 %
5.5 %
27.1 %
+MLPMixer (0) + HanMixer (16)
1.86 M
15.3 %
5.9 %
26.7 %
+MLPMixer (1) + HanMixer (16)
2.39 M
16.7 %
5.0 %
24.6 %
+MLPMixer (2) + HanMixer (16)
2.93 M
17.1 %
4.7 %
24.4 %
+MLPMixer (4) + HanMixer (16)
4.00 M
17.8 %
4.6 %
24.7 %"
IMAGE CLASSIFICATION DATASETS,0.28205128205128205,Table 2: Error rates (%) on CIFAR and STL10 datasets.
IMAGE CLASSIFICATION DATASETS,0.28402366863905326,"#Param
Top1 error
Top5 error"
IMAGE CLASSIFICATION DATASETS,0.2859960552268245,"CNN stem
8.28 M
55.7 %
31.2 %
+MLPMixer (4)
16.7 M
44.2 %
21.2 %
+MLPMixer (8)
27.2 M
42.6 %
20.2 %
+MLPMixer (0) + HanMixer (16)
8.35 M
48.9 %
26.1 %
+MLPMixer (4) + HanMixer (16)
16.8 M
41.1 %
19.2 %
WideResNet (Chrabaszcz et al., 2017)
37.1 M
41.0 %
18.9 %"
IMAGE CLASSIFICATION DATASETS,0.2879684418145957,"Table 3: : The top-1 and top-5 error rates on ImageNet32. On this dataset, among MLPMixer (i) for
i = 4, 8, 12, 16, i = 8 gives the best results,"
IMAGE CLASSIFICATION DATASETS,0.28994082840236685,"far fewer parameters (e.g., MLPMixer (4) vs MLPMixer (1)+HanMixer (16)). On ImageNet32, our
Han/MLP-Mixer combination model clearly outperforms pure MLPMixers and offers a competitive
performance with WideResNet while using only 40% parameters. In summary, the beneﬁts of using
HanMixers have been made evident in this set of experiments."
CONCLUSIONS,0.29191321499013806,"6
CONCLUSIONS"
CONCLUSIONS,0.2938856015779093,"As a qualitative surrogate for expressivity and trainability, variability provides a useful angle to view
certain critical behaviors of DNNs. Empirical evidence suggests that activation ratio and Collapse
to Constants are two major contributing factors to the gain and loss of variability, respectively.
Based on such insights, we propose a Han-layer architecture consisting of Householder weighting
and absolute-value activating. This new architecture greatly raises activation ratio and provides a
guaranteed immunity to vanishing or exploding gradient."
CONCLUSIONS,0.2958579881656805,"Extensive experimental results indicate that when used judiciously as replacements of fully connected
layers, Han-layers can signiﬁcantly reduce the number of model parameters (thus potentially the
associated computing costs), while maintaining or improving the level of generalization performance,
as is shown by our results on MLP-Mixer models for image classiﬁcation."
CONCLUSIONS,0.2978303747534517,"On the synthetic Checkerboard dataset, HanNets consistently exhibit a surprisingly high level of
generalization ability. It remains to be investigated that under what conditions and to what extent
such an ability could be realized on large-scale datasets."
CONCLUSIONS,0.29980276134122286,"Several limitations in our results are worth mentioning. So far, our measurement of variability is
limited to low-dimensional data spaces. It remains to be seen whether or not a computable extension
to high-dimensional spaces can be fruitfully done. As is currently deﬁned, a Han-layer is not as
ﬂexible as a fully connected layer since a Householder matrix must be square. Finally, we offer
two points of perspectives: (1) measured variability should be viewed within a reasonable range
(chaotic landscapes are generally bad); and (2) too many Han-layers, even though immune to gradient
problems, may still increase the difﬁculty in training due to high nonlinearity."
CONCLUSIONS,0.30177514792899407,Under review as a conference paper at ICLR 2022
REFERENCES,0.3037475345167653,REFERENCES
REFERENCES,0.3057199211045365,"Roy Batruni. A multilayer neural network with piecewise-linear structure and back-propagation
learning. IEEE Transactions on Neural Networks, 2(3):395–403, 1991."
REFERENCES,0.3076923076923077,"Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an
alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017."
REFERENCES,0.3096646942800789,"Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence
and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011."
REFERENCES,0.3116370808678501,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations, 2020."
REFERENCES,0.3136094674556213,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.3155818540433925,"Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometry and the time evolution of the neural tangent kernel. arXiv preprint arXiv:2010.15110,
2020."
REFERENCES,0.3175542406311637,"Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010."
REFERENCES,0.31952662721893493,"Ingo G¨uhring, Mones Raslan, and Gitta Kutyniok. Expressivity of deep neural networks. arXiv
preprint arXiv:2007.04759, 2020."
REFERENCES,0.3214990138067061,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.3234714003944773,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016."
REFERENCES,0.3254437869822485,"Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable beneﬁt of orthogonal initialization in
optimizing deep linear networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.32741617357001973,"Lei Huang, Xianglong Liu, Bo Lang, Adams Yu, Yongliang Wang, and Bo Li. Orthogonal weight
normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural
networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018."
REFERENCES,0.32938856015779094,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 448–456, Lille, France, 07–09 Jul 2015. PMLR."
REFERENCES,0.33136094674556216,"Animesh Karnewar. Aann: Absolute artiﬁcial neural network. In 2018 3rd International Conference
for Convergence in Technology (I2CT), pp. 1–6. IEEE, 2018."
REFERENCES,0.3333333333333333,"Kenji Kawaguchi. Deep learning without poor local minima. arXiv preprint arXiv:1605.07110, 2016."
REFERENCES,0.33530571992110453,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),
2015."
REFERENCES,0.33727810650887574,Under review as a conference paper at ICLR 2022
REFERENCES,0.33925049309664695,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, University of Toronto, 2009."
REFERENCES,0.34122287968441817,"Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, and
Rong Ge. Explaining landscape connectivity of low-cost solutions for multilayer nets. arXiv
preprint arXiv:1906.06247, 2019."
REFERENCES,0.3431952662721893,"Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In International conference on machine learning, pp. 2902–2907. PMLR, 2018."
REFERENCES,0.34516765285996054,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. arXiv preprint arXiv:1712.09913, 2017."
REFERENCES,0.34714003944773175,"J-N Lin and Rolf Unbehauen. Canonical piecewise-linear approximations. IEEE Transactions on
Circuits and Systems I: Fundamental Theory and Applications, 39(8):697–699, 1992."
REFERENCES,0.34911242603550297,"Hanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint
arXiv:2105.08050, 2021."
REFERENCES,0.3510848126232742,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2018."
REFERENCES,0.3530571992110454,"Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efﬁcient orthogonal
parametrisation of recurrent neural networks using householder reﬂections. In International
Conference on Machine Learning, pp. 2401–2409. PMLR, 2017."
REFERENCES,0.35502958579881655,"R Kelley Pace and Ronald Barry. Sparse spatial autoregressions. Statistics & Probability Letters, 33
(3):291–297, 1997."
REFERENCES,0.35700197238658776,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems, 32:
8026–8037, 2019."
REFERENCES,0.358974358974359,"Ruoyu Sun, Dawei Li, Shiyu Liang, Tian Ding, and Rayadurgam Srikant. The global landscape of
neural networks: An overview. IEEE Signal Processing Magazine, 37(5):95–108, 2020."
REFERENCES,0.3609467455621302,"Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An
all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021."
REFERENCES,0.3629191321499014,"Jakub M Tomczak and Max Welling. Improving variational auto-encoders using householder ﬂow.
arXiv preprint arXiv:1611.09630, 2016."
REFERENCES,0.36489151873767256,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´e
J´egou. Training data-efﬁcient image transformers & distillation through attention. In International
Conference on Machine Learning, pp. 10347–10357. PMLR, 2021."
REFERENCES,0.3668639053254438,"Michael Tsang, Hanpeng Liu, Sanjay Purushotham, Pavankumar Murali, and Yan Liu. Neural
interaction transparency (nit): Disentangling learned interactions for improved interpretability. In
NeurIPS, pp. 5809–5818, 2018."
REFERENCES,0.368836291913215,"Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. In International Conference on Machine Learning,
pp. 3570–3578. PMLR, 2017."
REFERENCES,0.3708086785009862,"Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu. Orthogonal convolutional
neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 11505–11515, 2020."
REFERENCES,0.3727810650887574,Under review as a conference paper at ICLR 2022
REFERENCES,0.3747534516765286,"Scott Wisdom, Thomas Powers, John R Hershey, Jonathan Le Roux, and Les E Atlas. Full-capacity
unitary recurrent neural networks. In NIPS, 2016."
REFERENCES,0.3767258382642998,"Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll´ar, and Ross Girshick. Early
convolutions help transformers see better. arXiv preprint arXiv:2106.14881, 2021."
REFERENCES,0.378698224852071,"Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efﬁcient
svd parameterization. In International Conference on Machine Learning, pp. 5806–5814. PMLR,
2018."
REFERENCES,0.3806706114398422,Under review as a conference paper at ICLR 2022
REFERENCES,0.3826429980276134,"A
VARIABILITY"
REFERENCES,0.38461538461538464,"A.1
EXISTING TECHNIQUES THAT ENHANCE VARIABILITY"
REFERENCES,0.3865877712031558,"Several existing techniques in deep learning can be interpreted from the viewpoint of enhancing the
variability of neural networks. For example, convolutional neural networks (CNN) use far fewer
parameters, in comparison to fully connected networks, at each layer, thus signiﬁcantly increasing
activation ratios and subsequently enhancing variability. In this view, achieving higher activation
ratios should be a signiﬁcant contributing factor to the great success of CNN."
REFERENCES,0.388560157790927,"Since C2C has a close relationship with vanishing gradient, it is not surprising that existing techniques
designed to alleviate the latter can also help with the former. Speciﬁcally, since G-matrices are limits
of C-matrices, techniques that slow down the size decrease of G-matrices usually also slow down
the size decrease of C-matrices. Such techniques include Residual Networks (or ResNet) (He et al.,
2016) and Batch Normalizations (Ioffe & Szegedy, 2015). A particularly simple and widely utilized
technique is to initialize weight matrices by orthogonal matrices (Hu et al., 2019; Huang et al., 2018)."
REFERENCES,0.3905325443786982,"A.2
SETTINGS OF V3 IN FIGURE 4"
REFERENCES,0.39250493096646943,"In our experiment, we use the least-squares function: f(·) := ∥· ∥2
2 and Ω= [−1, 1]2. The mean
over parameters will be computed as a sample mean. In particular, we use the geometric mean which
vanished if a single sampled value is zero. Therefore, once FL(x; W, b) becomes a constant (or linear
for that matter) on Ω, variability as measured by V3 vanishes."
REFERENCES,0.39447731755424065,"A.3
INCREASE OF ACTIVATION RATIO"
REFERENCES,0.39644970414201186,"For simplicity, let us consider fully connected (FC) network of L layers for which the number of
parameters in (W, b) is Nw = (d2 + d)L where d is the width of the layers. In this case, the number
of activations is Ld. The activation ratio (AR) ρ for this network is the total number of activations
divided by the total number of parameters, i.e., ρ = Ld/Nw, which can be written as a function of L.
By solving the equation Nw = (d2 + d)L for d, we obtain d = (1 +
p"
REFERENCES,0.398422090729783,"1 + 4Nw/L)/2. Therefore,
the activation ratio is
ρ(L) =
1
Nw"
REFERENCES,0.40039447731755423,"
L +
p"
REFERENCES,0.40236686390532544,"L2 + 4LNw

, L ≥1.
(11)"
REFERENCES,0.40433925049309666,"Furthermore, since"
REFERENCES,0.40631163708086787,"ρ′(L) =
1
Nw "
REFERENCES,0.40828402366863903,"1 +
1 + 2Nw/L
p"
REFERENCES,0.41025641025641024,"1 + 4Nw/L ! > 0,"
REFERENCES,0.41222879684418146,"the activation ratio is monotonically increasing with L. In particular, the rate of increase is the
largest at L = 1. This simple calculation shows that the activation ratio increases most rapidly at the
beginning, explaining why variability arises quickly at the very beginning."
REFERENCES,0.41420118343195267,"A.4
COLLAPSE TO CONSTANTS AND ITS CHARACTERIZATION"
REFERENCES,0.4161735700197239,"To compute the derivative of FL(x, W, b) with respect to the parameters, one uses the chain-rule to
obtain so-called back-propagation formulas, such as"
REFERENCES,0.4181459566074951,"∂
∂b1
FL(x, W, b) = ∂sL"
REFERENCES,0.42011834319526625,"∂b1
= ∇φ(z1)W T
2 ∇φ(z2) · · · W T
L ∇φ(zL),"
REFERENCES,0.42209072978303747,"where zk are computed in (3), ∇φ(zk) are diagonal matrices with scalar-valued φ′ applied component-
wise. It is well-known that the behavior of the derivatives is critically determined by the properties of
the above matrix product. For convenience and without loss of generality, we add W1 to the product
and deﬁne"
REFERENCES,0.4240631163708087,"GL ≡GL(x, W, b) := L
Y"
REFERENCES,0.4260355029585799,"k=1
W T
k ∇φ(zk),
(12)"
REFERENCES,0.4280078895463511,Under review as a conference paper at ICLR 2022
REFERENCES,0.42998027613412226,"which we will simply call the G-matrix associated with the network FL(x, W, b). It is well known in
deep learning that excessively small (or large) size of GL causes vanishing (or exploding) gradient,
which is a major source of difﬁculty in training."
REFERENCES,0.4319526627218935,"Now we deﬁne another matrix product called the C-matrix, by replacing the derivative ∇φ(zk) in
(12) by the ﬁnite difference at two points zk and ¯zk; that is,"
REFERENCES,0.4339250493096647,"CL ≡CL(x, ¯x, W, b) := L
Y"
REFERENCES,0.4358974358974359,"k=1
W T
k Dφ(zk, ¯zk),
(13)"
REFERENCES,0.4378698224852071,"where Dφ(·, ·) ∈Rd×d is diagonal deﬁned by"
REFERENCES,0.43984220907297833,"[Dφ(u, v)]ii = [φ(u) −φ(v)]i"
REFERENCES,0.4418145956607495,"[u −v]i
, i = 1, · · · , d,
(14)"
REFERENCES,0.4437869822485207,"with the convention 0/0 = 1, {zk} and {¯zk} are computed via the recursion (3) starting from x and
¯x, respectively. By their deﬁnitions, it is clear that G-matrices are limits of C-matrices."
REFERENCES,0.4457593688362919,The following proposition shows that C-matrices characterize the C2C phenomenon.
REFERENCES,0.4477317554240631,"Proposition 2. Let network FL(x, W, b) : Rd →Rd be deﬁned as in (2). For any two distinct points
x, ¯x ∈Rd, there holds
FL(x) −FL(¯x) = [CL(x, ¯x)]T (x −¯x).
(15)"
REFERENCES,0.44970414201183434,"Consequently,
lim
L→∞CL(x, ¯x) = 0 =⇒
lim
L→∞(FL(x) −FL(¯x)) = 0.
(16)"
REFERENCES,0.4516765285996055,"The veriﬁcation of this proposition is straightforward, so we omit it."
REFERENCES,0.4536489151873767,"We note that the difference going to zero in (16) does not imply that each individual sequence goes
to the same limit. On the contrary, such limits generally do not exist if the bias sequence {bk} is
bounded away from zero."
REFERENCES,0.4556213017751479,"Regarding the C2C phenomenon, the following remarks are in order."
REFERENCES,0.45759368836291914,"• Wherever CL(x, ¯x) is sufﬁciently small, the output values of the network for these two
inputs x and ¯x will be close to each other for large L."
REFERENCES,0.45956607495069035,"• CL(x, ¯x) will be small if, for all or many k, ∥W T
k Dφ(zk, ¯zk)∥are sufﬁciently smaller than
1 in some norm which occurs when one or both of the matrices is sufﬁciently small."
REFERENCES,0.46153846153846156,"• If CL(x, ¯x) is sufﬁciently small for all x in some region, then the corresponding outputs of
the network will be like a constant in this region. In particular, this can happen when weight
matrices in {Wk} are small for all or many k."
REFERENCES,0.4635108481262327,"If the weight matrices Wk, k = 1, · · · , L, are properly normalized (for example, all Wk are orthogonal
matrices), then the size of CL will be solely determined by that of Dφ(zk, ¯zk) for a given point
pair (x, ¯x), which in turn depends on activation φ in use. We consider the popular ReLU activation
φ(t) = max(0, t) and, again for the purpose of contrast, the absolute-value function φ(t) = |t|. In
both cases, the diagonal entries in (14) satisfy [Dφ(u, v)]ii ∈[−1, 1]."
REFERENCES,0.46548323471400394,"Proposition 3. Suppose that u, v ∈Rd be i.i.d. random variables with"
REFERENCES,0.46745562130177515,"Prob([u]i ≥0) = Prob([v]i ≥0) = p ∈(0, 1), i = 1, 2, · · · , d. Then"
REFERENCES,0.46942800788954636,Prob (|φ(u) −φ(v)| = |u −v|) =
REFERENCES,0.4714003944773176,"(
p2d,
φ(t) = max(0, t)
 
p2 + (1 −p)2d ,
φ(t) = |t|
(17)"
REFERENCES,0.47337278106508873,where the absolute values are taken component-wise.
REFERENCES,0.47534516765285995,Under review as a conference paper at ICLR 2022
REFERENCES,0.47731755424063116,"Proof. Consider the scalar case d = 1 with u ̸= v. For ReLU function φ(t) = max(0, t),

φ(u) −φ(v) u −v "
REFERENCES,0.47928994082840237," = 1,
u, v ≥0
< 1,
otherwise"
REFERENCES,0.4812623274161736,"where the probability for the ﬁrst case (ratio equal to 1) is p2. For absolute value φ(t) = |t|,

φ(u) −φ(v) u −v "
REFERENCES,0.4832347140039448," = 1,
uv ≥0
< 1,
otherwise"
REFERENCES,0.48520710059171596,where the probability for the ﬁrst case (ratio equal to 1) is p2 + (1 −p)2.
REFERENCES,0.48717948717948717,"Since all the components are i.i.d., by raising the above probabilities to their d-th power, we obtain
the corresponding probabilities for the vector case for d > 1."
REFERENCES,0.4891518737672584,"The proposition indicates that the probability for ReLU to preserve distances in Rd is much smaller
than that for absolute-value. For instance,"
REFERENCES,0.4911242603550296,"• when p = 1/2 the above two probabilities in (17) become 1/4d and 1/2d, respectively; this
is, the latter is 2d times larger than the former;"
REFERENCES,0.4930966469428008,"• when p = 1/4 the above two probabilities in (17) become (1/16)d and (10/16)d, respec-
tively; this is, the latter is 10d times larger than the former."
REFERENCES,0.49506903353057197,"Remark 4. When d is large, the probability is small for either function to maintain diagonals
|[Dφ(u, v)]ii| = 1 for all indices i. That is, probabilistically speaking, the two functions are
contractive, i.e., with a high probability, they shrink the distance between points after each application,
while ReLU is a more forceful contraction than absolute-value is, especially when the positive
probability p of input is small."
REFERENCES,0.4970414201183432,"In Figure 6, we present a couple of typical experimental results on C- and G-matrices. The left plot is
for ReLU activations only with random weight matrices and bias vectors speciﬁed, then multiplied
by the Xavier/Kaiming initialization constant
p"
REFERENCES,0.4990138067061144,"2/d. For a randomly select point pair (x, ¯x), we
compute the corresponding C-matrix, as well as the G-matrices at these two points, for depth L
from 1 to 1000. The curves depict the spectral norms of the matrices CL and GL (two of those) at
each depth value L. As we can see, although for L three curves roughly follow a similar pattern of
up-and-downs, the C-matrix still differs signiﬁcantly from the two G-matrices in that it becomes
about two-orders of magnitude smaller than the two G-matrices at the same depth. This signiﬁes an
important point that, at least in some cases, C2C should be considered as the leading cause for loss of
trainability, instead of vanishing gradient as commonly perceived."
REFERENCES,0.5009861932938856,"0
200
400
600
800
1000
number of hidden layers 10-5 10-4 10-3 10-2 10-1 100 101 102"
REFERENCES,0.5029585798816568,norm(C) and norm(G)
REFERENCES,0.504930966469428,Normal Weight Matrices (d = 150) ReLU
REFERENCES,0.5069033530571992,"C-matrix
G-matrix1
G-matrix2"
REFERENCES,0.5088757396449705,"0
200
400
600
800
1000
number of hidden layers 10-10 10-8 10-6 10-4 10-2 100"
REFERENCES,0.5108481262327417,norm(C)
REFERENCES,0.5128205128205128,Normal Weight Matrices (d = 50)
REFERENCES,0.514792899408284,"ReLU C-matrix
ABS C-matrix"
REFERENCES,0.5167652859960552,Figure 6: Sizes of C- and G-matrices for ReLU and absolute-value activations
REFERENCES,0.5187376725838264,Under review as a conference paper at ICLR 2022
REFERENCES,0.5207100591715976,"The right plot of Figure 6 is for the comparison of two different activation functions, ReLU, and
absolute-value. Starting from a random point-pair, we plot the C-matrices computed on a set of
random weight-bias pairs as speciﬁed with initialization multipliers
p"
REFERENCES,0.5226824457593688,"2/d and
p"
REFERENCES,0.52465483234714,"1/d for ReLU and
absolute-value, respectively. The plot shows clearly that ReLU, with considerably smaller C-matrices,
is much more prone to the negative inﬂuence of C2C than the absolute-value is. At L = 1000, the
C-matrix for ReLU is more than three orders of magnitude smaller than that for absolute-value."
REFERENCES,0.5266272189349113,"B
VARIABILITY VS. TRAINABILITY"
REFERENCES,0.5285996055226825,"As we have seen, for fully connected neural nets with a ﬁxed number of parameters, variability is
initially small, due to low activation ratio values, and then it rises to a peak with the growth of the
depth; after that, it starts to fall due to the progression of C2C phenomenon. In this section, we
present numerical evidence verifying that the pattern of variability change is highly correlated to
the performance of corresponding neural nets. This suggests that variability may well serve as a
predictive indicator of the performance of suitable neural nets."
REFERENCES,0.5305719921104537,"B.1
EXPERIMENT SETTING"
REFERENCES,0.5325443786982249,"Our experiments are done on a styled synthetic model checkerboard in which the data points are the
6561 mesh points of an 81 × 81 grid over the square [−1, 1]2 in R2. These mesh points are divided
into two sets, one corresponding to 0-labels and another to 1-labels, so that together they form an 8
by 8 checkerboard blocks, as is shown in Figure 7, where each of the 64 squares contains 81 grid
points and the surrounding edges contain 1377 points. The blocks take either 0 or 1 (blue or red)
label in an alternating pattern, and the surrounding edges all take the 0-label. In essence, we aim to
approximate the piecewise linear, non-smooth function shown in the right plot of Figure 7."
REFERENCES,0.534516765285996,"−1.0
−0.5
0.0
0.5
1.0 −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5364891518737672,Figure 7: Checkerboard: left plot for data points; right plot for labels; colors match binary labels.
REFERENCES,0.5384615384615384,"We train fully connected, rectangular neural nets (FCNets) with the same number of nodes or neurons
(but see below for exceptions) at each hidden layer. In each of our experiments, when we vary the
number of hidden layers L, we adjust the width d so that the total number of model parameters (i.e.,
weights and biases) is kept to a prescribed constant (though, in order to do so we sometimes have to
add or delete one or two nodes from some hidden layers). It is worth noting that as the depth grows
deeper, the width becomes narrower."
REFERENCES,0.5404339250493096,"We randomly split the dataset of 6561 points into two parts: 25% as a training set containing
m = 1640 samples, and the remaining 75% as a testing set. Denoting the training set by {xi}m
i=1, we
minimize the least squares loss function,"
REFERENCES,0.5424063116370809,"min
W,b fL(W, b) ≡ m
X"
REFERENCES,0.5443786982248521,"i=1
∥ˆFL(xi; W, b) −yi∥2
2,
(18)"
REFERENCES,0.5463510848126233,"where ˆFL(x, W, b) : R2 →R2 is constructed from the neural net function FL(x, W, b) in (2) plus an
input layer and an output layer, and each label vector yi ∈R2 is either (0, 0)T or (1, 1)T , representing"
REFERENCES,0.5483234714003945,Under review as a conference paper at ICLR 2022
REFERENCES,0.5502958579881657,"binary labels. We refer to the objective function value as the training loss and the percentage of
correctly classiﬁed data points as the training accuracy. We note that a 100% training accuracy
generally does not imply a 0-training loss."
REFERENCES,0.5522682445759369,"To ensure that the optimization calculation is done sufﬁciently, we apply the gradient descent method
(instead of SGD) with 40000 iterations without a stopping criterion. For each run, we always try 10
different initial learning rates (step-sizes) as in"
REFERENCES,0.5542406311637081,"{0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1.0}"
REFERENCES,0.5562130177514792,"and then pick the best result for output. During the 40000 iterations, learning rates are reduced
by a factor of 5 three times at the junctures corresponding to iterations 20000, 28000, and 36000,
respectively."
REFERENCES,0.5581854043392505,"We initialize weight matrices and bias vectors with i.i.d. standard normal random numbers, and
the weight matrices are then scaled by the Xavier/Kaiming initialization constants. The activation
function is ReLU and ABS. All trial instances are run with 10 different samples of random initial
parameters. For ﬁnal output, we always record and output the best result in terms of the training error
regardless of where it occurs during training (in the middle or at the end)."
REFERENCES,0.5601577909270217,"B.2
COMPUTATIONAL RESULTS"
REFERENCES,0.5621301775147929,"We show the results of training FCNets by GD of varying depths (x-axis) and parameters (from left
to right in each row). For each network, we compute the mean value of training/testing loss/accuracy.
All result is reasonably stable as shown by the std of 10 runs. Once over-parameterization happens,
there exists a bottom of the valley in Figure 8 to get or approach to the global optimum."
REFERENCES,0.5641025641025641,"We ﬁx the total number of parameters at Nw = 1600, 2400 and 3200. For each Nw value, we
solve Problem (18) with FCNets-ReLU and FCNets-ABS, following the procedure described in the
previous subsection, with the number of hidden layers L varying from 2 to 31 (with increment 1 up
to 20 then increment 2 afterwards) for ReLU and extending to 41 for ABS. The results, including
training and testing losses and accuracies, are summarized in Figure 8 containing 6 plots in 3 columns
corresponding to the 3 values of Nw."
REFERENCES,0.5660749506903353,"As previously shown, when networks grow deeper, variability ﬁrst increases and then decreases. As
we observe from Figure 8, there are remarkably similar patterns in all curves in Figure 8 for either
training and test accuracies or losses (for which the rise-and-fall pattern is ﬂipped)."
REFERENCES,0.5680473372781065,"In particular, we compare the left plot of Figure 2 with the third column of Figure 8. For these
two cases, the numbers of total model parameters are sufﬁciently close (3300 vs. 4000) to roughly
examine the locations of interests. Indeed, the peak of variability occurs at L = 12 in the left plot of
Figure 2, while the best performance of the FCNet happens between L = 10 and L = 15, as can be
seen in the third column of Figure 8."
REFERENCES,0.5700197238658777,"We offer the following interpretations of the experimental results, as pertinent to the relationship
between network variability and trainability."
REFERENCES,0.571992110453649,"• Variability in the data space indicates a model’s expressivity, and in turn, it implies sensitivity
in the parameter space. A well-designed and computable measure of variability, which
remains a topic of further research, could serve well as an indicator of trainability."
REFERENCES,0.5739644970414202,"• With low variability, models apparently have more local traps, making training difﬁcult. On
the other hand, near or around variability peaks, there appears to exist few or no local traps,
as evidenced by the middle sections in the plots of Figure 8 where the training process seems
to reach global optima with few or no exceptions, at least so in over-parametrized models."
REFERENCES,0.5759368836291914,"• As expected, over-parametrization helps trainability. However, it may not always beneﬁt
generalizations much or at all, as is evidenced by the curves of testing loss and accuracy in
Figure 8."
REFERENCES,0.5779092702169625,Under review as a conference paper at ICLR 2022
REFERENCES,0.5798816568047337,"1
7
13
19
25
31
Number of Layers -0.05 0 0.05 0.1 0.15 0.2 0.25 Loss"
REFERENCES,0.5818540433925049,"training loss
testing loss"
REFERENCES,0.5838264299802761,"1
9
17
25
33
41
Number of Layers -0.05 0 0.05 0.1 0.15 0.2 0.25 Loss"
REFERENCES,0.5857988165680473,"training loss
testing loss"
REFERENCES,0.5877712031558185,"1
7
13
19
25
31
Number of Layers 0.7 0.8 0.9 1"
REFERENCES,0.5897435897435898,Accuracy
REFERENCES,0.591715976331361,"training accuracy
testing accuracy"
REFERENCES,0.5936883629191322,"1
9
17
25
33
41
Number of Layers 0.6 0.7 0.8 0.9 1 1.1"
REFERENCES,0.5956607495069034,Accuracy
REFERENCES,0.5976331360946746,"training accuracy
testing accuracy"
REFERENCES,0.5996055226824457,"1
7
13
19
25
31
Number of Layers -0.05 0 0.05 0.1 0.15 0.2 Loss"
REFERENCES,0.6015779092702169,"training loss
testing loss"
REFERENCES,0.6035502958579881,"1
7
13
19
25
31
Number of Layers 0.7 0.8 0.9 1"
REFERENCES,0.6055226824457594,Accuracy
REFERENCES,0.6074950690335306,"training accuracy
testing accuracy"
REFERENCES,0.6094674556213018,"1
9
17
25
33
41
Number of Layers -0.05 0 0.05 0.1 0.15 0.2 0.25 Loss"
REFERENCES,0.611439842209073,"training loss
testing loss"
REFERENCES,0.6134122287968442,"1
9
17
25
33
41
Number of Layers 0.5 0.6 0.7 0.8 0.9 1 1.1"
REFERENCES,0.6153846153846154,Accuracy
REFERENCES,0.6173570019723866,"training accuracy
testing accuracy"
REFERENCES,0.6193293885601578,"1
7
13
19
25
31
Number of Layers -0.05 0 0.05 0.1 0.15 0.2 Loss"
REFERENCES,0.621301775147929,"training loss
testing loss"
REFERENCES,0.6232741617357002,"1
7
13
19
25
31
Number of Layers 0.6 0.7 0.8 0.9 1 1.1"
REFERENCES,0.6252465483234714,Accuracy
REFERENCES,0.6272189349112426,"training accuracy
testing accuracy"
REFERENCES,0.6291913214990138,"1
9
17
25
33
41
Number of Layers -0.05 0 0.05 0.1 0.15 0.2 0.25 Loss"
REFERENCES,0.631163708086785,"training loss
testing loss"
REFERENCES,0.6331360946745562,"1
9
17
25
33
41
Number of Layers 0.6 0.7 0.8 0.9 1 1.1"
REFERENCES,0.6351084812623274,Accuracy
REFERENCES,0.6370808678500987,"training accuracy
testing accuracy"
REFERENCES,0.6390532544378699,"Figure 8: FCNet-ReLU vs FCNet-ABS on Checkerboard. The 1st-row contains computed training
and testing losses, and the 2nd-row training and testing accuracies. Columns 1 to 3 (from left to
right) are for results corresponding to the number of model parameters equal to 1600, 2400, and 3200,
respectively (recall that the training set contains 1640 samples). Each group of curves depicts the
mean and variance of 10 random runs."
REFERENCES,0.6410256410256411,"• FCNet-ReLU is unable to reach near-zero loss values with more than 20 hidden layers, but
FCNet-ABS still succeeds even after the hidden-layer number exceeds 30, conﬁrming that
ABS works better than ReLU in deeper DNNs."
REFERENCES,0.6429980276134122,"C
HANNET"
REFERENCES,0.6449704142011834,"C.1
HANNET ALGORITHM"
REFERENCES,0.6469428007889546,Algorithm 1 HanLayer.
REFERENCES,0.6489151873767258,"Input: vector x and parameters (u, b).
Output: y = HanLayer(x; u, b)"
REFERENCES,0.650887573964497,"1: function HANLAYER(x, u, b)
2:
z = x −2 u⊺x"
REFERENCES,0.6528599605522682,"∥u∥2 u + b
▷Householder Reﬂection plus bias"
REFERENCES,0.6548323471400395,"3:
y = ABS(z)
▷Absolute-value Function Activation
4:
return y
5: end function"
REFERENCES,0.6568047337278107,"C.2
MIXER BLOCK"
REFERENCES,0.6587771203155819,Figure 9 presents two Mixer structures using FC-Layers and Han-Layers.
REFERENCES,0.6607495069033531,Under review as a conference paper at ICLR 2022
REFERENCES,0.6627218934911243,Layer Norm
REFERENCES,0.6646942800788954,FC Layer GELU
REFERENCES,0.6666666666666666,FC Layer
REFERENCES,0.6686390532544378,HH Layer ABS
REFERENCES,0.6706114398422091,HH Layer ABS
REFERENCES,0.6725838264299803,"Figure 9: Two implementations of the channel mixing module using MLP-Mixer and Han-Mixer,
respectively. HH denotes Householder."
REFERENCES,0.6745562130177515,"D
EXPERIMENT"
REFERENCES,0.6765285996055227,"D.1
EXPERIMENT SETTINGS"
REFERENCES,0.6785009861932939,"Our Checkerboard dataset is similar to the one in Section 4 but more complicated, shown as Figure 4
with 6561 mesh points in a 12 × 12 block checkerboard over the square [−1, 1]2. We consider, and
our experiments have conﬁrmed, that this dataset is a highly challenging one for standard DNNs."
REFERENCES,0.6804733727810651,"Datasets
dim
Training set N
Testing set N"
REFERENCES,0.6824457593688363,"Synthetic
Checkerboard
2
1640
4921"
REFERENCES,0.6844181459566075,"Regression
Elevators
18
3320 or 13279
13279 or 3320
Cal Housing
8
4128 or 16512
16512 or 4128"
REFERENCES,0.6863905325443787,"Image Classiﬁcation
CIFAR-10
3072
50000
10000
CIFAR-100
3072
50000
10000
STL-10
27648
5000
8000
ImageNet32
3072
1281167
50000"
REFERENCES,0.6883629191321499,"Table 4: Dataset statistics: N is the number of samples and dim the dimension of data vectors.
We use the mean-square error loss function for the checkerboard and the two regression datasets,
and the cross-entropy loss function to classify images. We use stochastic gradient descent (SGD)
with momentum to train Checkerboard. To solve each test instance, we run SGD using each of the
following 10 initial learning rates:"
REFERENCES,0.6903353057199211,"{0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1}"
REFERENCES,0.6923076923076923,"and select the best result as the output. In addition, the initial learning rate is annealed by a factor of
5 at the fractions 1/2, 7/10 and 9/10 of the training durations. Other parameter settings for SGD are
listed in Table 5 below."
REFERENCES,0.6942800788954635,"Dataset
Iteration
batch size
weight decay
momentum"
REFERENCES,0.6962524654832347,"Checkerboard
40000
100
0.0
0.9"
REFERENCES,0.6982248520710059,Table 5: SGD parameters.
REFERENCES,0.7001972386587771,"For the two regression datasets, we choose Adam (Kingma & Ba, 2015) as the training method which
seems to be the method of choice for several works in that area including (Tsang et al., 2018). For
these image classiﬁcation datasets, we choose a modiﬁed Adam called AdamW (Loshchilov & Hutter,
2018). Detailed settings are in Table 6 below."
REFERENCES,0.7021696252465484,"All the training is done using PyTorch (Paszke et al., 2019) running on a shared cluster. Finally, we
mention that we always run multiple times for each test instance, starting from different random
initializations of model parameters. All the reported values are the average of at least 5 runs. It is
noted that the results in each block on Figure 5 are the average of 9 runs. We randomly manufacture
the train set 3 times and do 3 trials on each set."
REFERENCES,0.7041420118343196,Under review as a conference paper at ICLR 2022
REFERENCES,0.7061143984220908,"Dataset
Optimizer
epochs
lr
batch size
weight decay"
REFERENCES,0.7080867850098619,"Elevators/ Cal Housing
Adam
300
0.001
100
0.0
CIFAR-10/ CIFAR-100
AdamW
600
0.001
256
0.1
STL-10
AdamW
300
0.001
64
0.1
Imagenet-32
AdamW
300
0.001
512
0.1"
REFERENCES,0.7100591715976331,Table 6: Adam/AdamW parameters.
REFERENCES,0.7120315581854043,"D.2
ABLATION STUDY ON CHECKERBOARD"
REFERENCES,0.7140039447731755,"Layer type
Activation type
Test accuracy
H
FC
ABS
ReLU"
REFERENCES,0.7159763313609467,"(a)
!
%
!
%
99.2%
(b)
!
%
%
!
66.2%
(c)
%
!
!
%
85.3%"
REFERENCES,0.717948717948718,"Table 7: Ablation study on a 100 × 17 network framework: Effects of layer and activation types on
performance. H denotes Householder and FC denotes fully-connected layers."
REFERENCES,0.7199211045364892,"To better grasp how much the two HanNets components, Householder weighting and ABS activating,
contribute to the surprising results on the Checkerboard dataset, we conduct an ablation study with
three different conﬁgurations in a 100 × 17 network framework. The results are listed in Table 7
where the layer type is either Householder (H) or fully connected (FC) and the activation type is either
ABS or ReLU. Table 7 suggests that Householder weighting and ABS activating be equally critical to
the 99% generalization accuracy of the HanNet. We investigate the heavily lacking of variability in
the networks with Householder matrix plus ReLU function affects the model performance."
REFERENCES,0.7218934911242604,"D.3
FLIPPING TRAINING LABELS ON CHECKERBOARD"
REFERENCES,0.7238658777120316,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
	 	
   0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000"
REFERENCES,0.7258382642998028," 	"
REFERENCES,0.727810650887574,"FCNet
HanNet"
REFERENCES,0.7297830374753451,(a) FCNet vs HanNet −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
REFERENCES,0.7317554240631163,(b) Training set (25%) −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
REFERENCES,0.7337278106508875,(c) FCNet: test 86.2% −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
REFERENCES,0.7357001972386588,(d) HanNet: test 95.2%
REFERENCES,0.73767258382643,"Figure 10: (a) Testing accuracy in FCNet and HanNet. (b) Training set: violet-colored points are
those whose 10% labels are ﬂipped (background color represents their original label). (c) FCNet: the
best testing accuracies are 86.5% in the model trained from (b). (d) HanNet: the best testing accuracy
is 95.2% ."
REFERENCES,0.7396449704142012,"To check the stability of the near-perfect result of the HanNet, we randomly ﬂip training labels from
0 to 20% on Problem 4. Figure 10 (a) presents the testing accuracy in HanNet and FCNet, where the
network architecture is 100 × 17 for HanNet and 100 × 6 for FCNet. The training result for FCNet is
given in Figure 10 (c), while Figure 10 (d) are, the best results for the HanNet in terms of testing
accuracy. The best result shows that HanNet can still learn well the original pattern from seriously
damaged labels."
REFERENCES,0.7416173570019724,Under review as a conference paper at ICLR 2022
REFERENCES,0.7435897435897436,"D.4
REGRESSION DATASETS"
REFERENCES,0.7455621301775148,"Table 1 presents the model speciﬁcations and statistics of datasets (Elevators and Cal Housing).
Figure 11 shows the NRMSE values on two FCNets and HanNet."
REFERENCES,0.747534516765286,"Model
Depth × Width
Elevators
Cal Housing
Parameters
AR(%)
Parameters
AR (%)"
REFERENCES,0.7495069033530573,"FCNet1
5 × 50
11201
2.23
10701
2.33
FCNet2
5 × 200
164801
0.60
162801
0.61
HanNet
20 × 200
11601
34.47
9601
41.66"
REFERENCES,0.7514792899408284,Table 8: Model speciﬁcations and statistics of datasets (Elevators and Cal Housing)
REFERENCES,0.7534516765285996,"0
50
100
150
200
250
300
Number of Epochs 0.04 0.06 0.08 0.1 0.12 0.14 0.16 NRMSE"
REFERENCES,0.7554240631163708,"HanNet testing
FCNet1 testing"
REFERENCES,0.757396449704142,"0
50
100
150
200
250
300
Number of Epochs 0.05 0.1 0.15 0.2 NRMSE"
REFERENCES,0.7593688362919132,"HanNet testing
FCNet1 testing"
REFERENCES,0.7613412228796844,"0
50
100
150
200
250
300
Number of Epochs 0.04 0.06 0.08 0.1 0.12 NRMSE"
REFERENCES,0.7633136094674556,"HanNet testing
FCNet2 testing"
REFERENCES,0.7652859960552268,"0
50
100
150
200
250
300
Number of Epochs 0.05 0.1 0.15 0.2 0.25 NRMSE"
REFERENCES,0.7672583826429981,"HanNet testing
FCNet2 testing"
REFERENCES,0.7692307692307693,"0
50
100
150
200
250
300
Number of Epochs 0.1 0.11 0.12 0.13 0.14 0.15 NRMSE"
REFERENCES,0.7712031558185405,"HanNet testing
FCNet1 testing"
REFERENCES,0.7731755424063116,"0
50
100
150
200
250
300
Number of Epochs 0.11 0.12 0.13 0.14 0.15 0.16 0.17 NRMSE"
REFERENCES,0.7751479289940828,"HanNet testing
FCNet1 testing"
REFERENCES,0.777120315581854,"0
50
100
150
200
250
300
Number of Epochs 0.1 0.11 0.12 0.13 0.14 0.15 NRMSE"
REFERENCES,0.7790927021696252,"HanNet testing
FCNet2 testing"
REFERENCES,0.7810650887573964,"0
50
100
150
200
250
300
Number of Epochs 0.11 0.12 0.13 0.14 0.15 0.16 NRMSE"
REFERENCES,0.7830374753451677,"HanNet testing
FCNet2 testing"
REFERENCES,0.7850098619329389,"Figure 11: All ﬁgures show testing performance where the top row is for the Elevators dataset; and
the bottom row for Cal Housing. From left to right: (1) HanNet (red) vs. FCNet1 (blue) on 80%
training data, (2) then on 20% training data; (3) HanNet (red) vs FCNet2 (blue) on 80% training data,
(4) then on 20% training data. Note that HanNet testing results are repeated."
REFERENCES,0.7869822485207101,"D.5
SETTINGS ON IMAGE CLASSIFICATION DATASETS"
REFERENCES,0.7889546351084813,"We summarize various conﬁgurations of each block on different data sets in Table 9. We run Mixers
using the following number of layers {1, 2, 4, 8, 12, 16} and select the best one on each dataset. We
use the convolution stem recommended by (Xiao et al., 2021) instead of the one in (Tolstikhin et al.,
2021). Our convolutional stem designs use ﬁve layers, including four 3 × 3 convolutions and a single
1 × 1 ﬁnal layer. The output channels are [64, 128, 256, 512, 512] on CIFAR10, CIFAR100 and
STL10, and [128, 256, 512, 1024, 1024] on ImageNet32."
REFERENCES,0.7909270216962525,"CIFAR
STL10
ImageNet32"
REFERENCES,0.7928994082840237,"Patch size
4 × 4
8 × 8
4 × 4
Sequence Length
64
144
64
Channel number
512
512
1024"
REFERENCES,0.7948717948717948,"Table 9: The architecture on one MLP-block or Han-block. For simpliﬁcation, in our experiments, all
hidden MLP weights are square matries."
REFERENCES,0.796844181459566,"E
PLOTS FROM LANDSCAPE EXPERIMENTS"
REFERENCES,0.7988165680473372,Under review as a conference paper at ICLR 2022 0.90 0.92 0.94 0.96 0.98 1.00 0.970 0.975 0.980 0.985 0.990 0.995 1.000
REFERENCES,0.8007889546351085,"0.992
0.993
0.994
0.995
0.996
0.997
0.998
0.999
1.000"
REFERENCES,0.8027613412228797,"0.9800
0.9825
0.9850
0.9875
0.9900
0.9925
0.9950
0.9975
1.000 0.96 0.97 0.98 0.99 1.00"
REFERENCES,0.8047337278106509,0.9994
REFERENCES,0.8067061143984221,0.9995
REFERENCES,0.8086785009861933,0.9996
REFERENCES,0.8106508875739645,0.9997
REFERENCES,0.8126232741617357,0.9998
REFERENCES,0.814595660749507,0.9999 1.000
REFERENCES,0.8165680473372781,"0.992
0.993
0.994
0.995
0.996
0.997
0.998
0.999
1.000"
REFERENCES,0.8185404339250493,0.9993
REFERENCES,0.8205128205128205,0.9994
REFERENCES,0.8224852071005917,0.9995
REFERENCES,0.8244575936883629,0.9996
REFERENCES,0.8264299802761341,0.9997
REFERENCES,0.8284023668639053,0.9998
REFERENCES,0.8303747534516766,0.9999 1.000
REFERENCES,0.8323471400394478,0.9993
REFERENCES,0.834319526627219,0.9994
REFERENCES,0.8362919132149902,0.9995
REFERENCES,0.8382642998027613,0.9996
REFERENCES,0.8402366863905325,0.9997
REFERENCES,0.8422090729783037,0.9998
REFERENCES,0.8441814595660749,0.9999 1.000 0.995 0.996 0.997 0.998 0.999 1.000
REFERENCES,0.8461538461538461,0.99970
REFERENCES,0.8481262327416174,0.99975
REFERENCES,0.8500986193293886,0.99980
REFERENCES,0.8520710059171598,0.9998
REFERENCES,0.854043392504931,0.9999
REFERENCES,0.8560157790927022,0.9999
REFERENCES,0.8579881656804734,1.0000
REFERENCES,0.8599605522682445,"0.99960
0.99965
0.99970
0.99975
0.99980
0.9998
0.9999"
REFERENCES,0.8619329388560157,0.9999
REFERENCES,0.863905325443787,1.0000
REFERENCES,0.8658777120315582,"0.999800
0.99982
0.99985
0.99987
0.99990
0.99992"
REFERENCES,0.8678500986193294,0.99995
REFERENCES,0.8698224852071006,0.99997
REFERENCES,0.8717948717948718,1.0000
REFERENCES,0.873767258382643,0.9975
REFERENCES,0.8757396449704142,0.9980
REFERENCES,0.8777120315581854,0.9985
REFERENCES,0.8796844181459567,0.9990
REFERENCES,0.8816568047337278,0.9995 1.000
REFERENCES,0.883629191321499,0.9995
REFERENCES,0.8856015779092702,0.9996
REFERENCES,0.8875739644970414,0.9997
REFERENCES,0.8895463510848126,0.9998
REFERENCES,0.8915187376725838,0.9999 −3.5 −3.0 −2.5 −2.0 −1.5 −1.0 −0.5 0.0 −7 −6 −5 −4 −3 −2 −1 0
REFERENCES,0.893491124260355,0.000008
REFERENCES,0.8954635108481263,0.000008
REFERENCES,0.8974358974358975,0.000009
REFERENCES,0.8994082840236687,0.000009
REFERENCES,0.9013806706114399,0.00000
REFERENCES,0.903353057199211,0.00000
REFERENCES,0.9053254437869822,0.00000
REFERENCES,0.9072978303747534,0.00001
REFERENCES,0.9092702169625246,0.0000082
REFERENCES,0.9112426035502958,0.000008
REFERENCES,0.9132149901380671,0.000008
REFERENCES,0.9151873767258383,0.000009
REFERENCES,0.9171597633136095,0.000009
REFERENCES,0.9191321499013807,0.000009
REFERENCES,0.9211045364891519,0.000009
REFERENCES,0.9230769230769231,0.00001 −3.5 −3.0 −2.5 −2.0 −1.5 −1.0 −0.5 0.0 0.96 0.98 1.00 1.02 1.04 −5 −4 −3 −2 −1 0 0.96 0.98 1.00 1.02 1.04 −1.0 −0.8 −0.6 −0.4 −0.2 0.0 −5 −4 −3 −2 −1 0
REFERENCES,0.9250493096646942,"Figure 12: Visualization of variability on FCNet-Sigmoid with 2,4,6,10,15 hidden layers. −1.0 −0.5 0.0 0.5 1.0 0.0 0.2 0.4 0.6 0.8 −1.0 −0.5 0.0 0.5 1.0 −0.25"
REFERENCES,0.9270216962524654,"0.00
0.25
0.50
0.75
1.00 1.25 1.50 1.75 −0.4 −0.2 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.9289940828402367,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.9309664694280079,"0.60
0.65
0.70
0.75
0.80
0.85 0.90 0.95 1.00 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0 0.2 0.4 0.6 0.8 1.0 0.965 0.970 0.975 0.980 0.985 0.990 0.995 1.000"
REFERENCES,0.9329388560157791,"0.800
0.825
0.850
0.875
0.900
0.925
0.950
0.975
1.000 0.986 0.988 0.990 0.992 0.994 0.996 0.998 1.000 0.94 0.95 0.96 0.97 0.98 0.99 1.00 −5 −4 −3 −2 −1 0"
REFERENCES,0.9349112426035503,0.00003
REFERENCES,0.9368836291913215,0.00004
REFERENCES,0.9388560157790927,0.00005
REFERENCES,0.9408284023668639,0.00006
REFERENCES,0.9428007889546351,0.0000
REFERENCES,0.9447731755424064,0.0000
REFERENCES,0.9467455621301775,0.0000
REFERENCES,0.9487179487179487,0.0001
REFERENCES,0.9506903353057199,0.99965
REFERENCES,0.9526627218934911,0.99970
REFERENCES,0.9546351084812623,0.99975
REFERENCES,0.9566074950690335,0.99980
REFERENCES,0.9585798816568047,0.9998
REFERENCES,0.960552268244576,0.9999
REFERENCES,0.9625246548323472,0.9999
REFERENCES,0.9644970414201184,1.0000 0.975 0.980 0.985 0.990 0.995 1.000 0.96 0.98 1.00 1.02 1.04
REFERENCES,0.9664694280078896,"Figure 13: Visualization of variability on FCNet-ReLU with 2,10,20,40,60 hidden layers."
REFERENCES,0.9684418145956607,Under review as a conference paper at ICLR 2022
REFERENCES,0.9704142011834319,"0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.9723865877712031,"0.60
0.65
0.70
0.75 0.80 0.85 0.90 0.95 1.00 0.88 0.90 0.92 0.94 0.96 0.98 1.00 0.2 0.4 0.6 0.8 1.0 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.9743589743589743,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0 0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0 0.75 0.80 0.85 0.90 0.95 1.00 0.5 0.6 0.7 0.8 0.9 1.0 0.75 0.80 0.85 0.90 0.95 1.00 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.88 0.90 0.92 0.94 0.96 0.98 1.00 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.970 0.975 0.980 0.985 0.990 0.995 1.000 0.80 0.85 0.90 0.95 1.00 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.9763313609467456,"0.9825
0.9850"
REFERENCES,0.9783037475345168,0.9875
REFERENCES,0.980276134122288,0.9900
REFERENCES,0.9822485207100592,0.9925
REFERENCES,0.9842209072978304,0.9950
REFERENCES,0.9861932938856016,0.9975 1.000 0.90 0.92 0.94 0.96 0.98 1.00
REFERENCES,0.9881656804733728,"Figure 14: Visualization of variability on FCNet-ABS with 2,10,20,40,60 hidden layers."
REFERENCES,0.9901380670611439,"0.60
0.65
0.70
0.75
0.80 0.85 0.90 0.95 1.00 0.2 0.4 0.6 0.8 1.0 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.2 0.4 0.6 0.8 1.0 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.80 0.85 0.90 0.95 1.00 0.80 0.85 0.90 0.95 1.00 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.9921104536489151,"0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00 0.80 0.85 0.90 0.95 1.00 0.75 0.80 0.85 0.90 0.95 1.00 0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000 0.80 0.85 0.90 0.95 1.00 0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000"
REFERENCES,0.9940828402366864,"0.800
0.825
0.850
0.875
0.900
0.925
0.950
0.975
1.000 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
REFERENCES,0.9960552268244576,"0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00 0.75 0.80 0.85 0.90 0.95 1.00 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.92 0.94 0.96 0.98 1.00 0.90 0.92 0.94 0.96 0.98 1.00"
REFERENCES,0.9980276134122288,"Figure 15: Visualization of variability on HanNet with 2,10,20,40,60 hidden layers."
