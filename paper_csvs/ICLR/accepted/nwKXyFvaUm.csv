Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021413276231263384,"In every communication round of federated learning, a random subset of clients
communicate their model updates back to the server which then aggregates them
all. The optimal size of this subset is not known and several studies have shown
that typically random selection does not perform very well in terms of convergence,
learning efﬁciency and fairness. We, in this paper, propose to select a small diverse
subset of clients, namely those carrying representative gradient information, and
we transmit only these updates to the server. Our aim is for updating via only a
subset to approximate updating via aggregating all client information. We achieve
this by choosing a subset that maximizes a submodular facility location function
deﬁned over gradient space. We introduce “federated averaging with diverse
client selection (DivFL)”. We provide a thorough analysis of its convergence
in the heterogeneous setting and apply it both to synthetic and to real datasets.
Empirical results show several beneﬁts of our approach, including improved
learning efﬁciency, faster convergence, and more uniform (i.e., fair) performance
across clients. We further show a communication-efﬁcient version of DivFL that
can still outperform baselines on the above metrics."
INTRODUCTION,0.004282655246252677,"1
INTRODUCTION"
INTRODUCTION,0.006423982869379015,"Federated learning (FL) involves collaboratively training of machine learning model across a large
number of clients while keeping client data local. Recent approaches to this problem repeatedly
alternate between device-local (stochastic) gradient descent steps and server-aggregation of the clients’
model updates (McMahan et al., 2017). In cross-device settings, a server and its model usually serve
several thousands of devices. Therefore, the communication between clients and the server can be
costly and slow, forming a huge impediment to FL’s viability."
INTRODUCTION,0.008565310492505354,"One property of the collection of clients that can mitigate these problems, however, is often not
exploited, and that is redundancy. Speciﬁcally, many clients might provide similar, and thus redundant,
gradient information for updating the server model. Therefore, transmitting all such updates to the
server is a waste of communication and computational resources. How best to select a representative
and more informative client set while adhering to practical constraints in federated learning is still
an open challenge. Although several selection criteria have been investigated in recent literature, e.g.,
sampling clients with probabilities proportional to their local dataset size (McMahan et al., 2017),
sampling clients of larger update norm with higher probability (Chen et al., 2020), and selecting clients
with higher losses (Balakrishnan et al., 2020; Cho et al., 2020), the redundancy and similarity of the
clients’ updates sent to the server is not represented and exploited in these approaches. In particular,
communicating multiple clients’ updates to the server may cause statistical and system inefﬁciency
if too many of them are too similar to each other. The commonly studied modular score/probability
for each individual client is incapable of capturing information as a property over a group of clients."
INTRODUCTION,0.010706638115631691,*Equal contributions
INTRODUCTION,0.01284796573875803,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014989293361884369,"Ideally, a diverse set of clients would be selected, thereby increasing the impact of under-represented
clients that contribute different information, and thereby improving fairness. This, in fact, is a topic
of increasing interest (Mohri et al., 2019; Cho et al., 2020; Dennis et al., 2021; Huang et al., 2021)."
INTRODUCTION,0.017130620985010708,"In this paper, we introduce diversity to client selection in FL, namely a strategy to measure how a
selected subset of clients can represent the whole when being aggregated on the server. Speciﬁcally, in
each communication round, we aim to ﬁnd a subset whose aggregated model update approximates the
aggregate update over all clients. By doing this, we aim to limit the impact of subset selection which
introduces variance in the model updates across round, that could otherwise slow the learning process.
Inspired by the CRAIG method of coreset selection for efﬁcient machine learning training (Mirza-
soleiman et al., 2020), we derive an upper bound of the approximation error as a supermodular
set function (in particular, the min-form of the facility location function (Cornuéjols et al., 1977))
evaluated on the selected subset. We can then apply submodular maximization (Fujishige, 2005; Iyer
et al., 2013; Wei et al., 2014) on a complement submodular function to (approximately) minimize
the error upper bound. We employ the greedy selection (Nemhauser et al., 1978) of a subset of
clients according to the marginal gain of the submodular function to achieve a solution with provable
approximation guarantee (Conforti & Cornuejols, 1984). By integrating the diverse client selection
into the most commonly studied FL scheme, i.e., Federated Averaging (FedAvg) (McMahan et al.,
2017), we propose DivFL that applies global model aggregation over a selected subset of clients
after multiple local steps on every client. We present theoretical convergence analysis of DivFL and
show its tolerance to the heterogeneity of data distributions across clients and large numbers of local
steps. However, our method differs from the CRAIG method where selection is performed based
on model updates (involving multiple epochs at the clients). In addition, our approach allows for
partial device participation where the server does not have access to all data at any communication
round, as is standard in FL (McMahan et al., 2017). In experiments, we compare DivFL with other
client selection approaches on both synthetic dataset and FEMNIST, wherein our method excels on
convergence, fairness, and learning efﬁciency."
BACKGROUND AND RELATED WORK,0.019271948608137045,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.021413276231263382,We consider a typical federated learning objective:
BACKGROUND AND RELATED WORK,0.023554603854389723,"min
w
f(w) = N
X"
BACKGROUND AND RELATED WORK,0.02569593147751606,"k=1
pkFk(w),"
BACKGROUND AND RELATED WORK,0.027837259100642397,"where for each client k ∈[N], pk is a pre-deﬁned weight (such that PN
k=1 pk = 1) that can be set to
1
N or the fraction of training samples, and Fk is the client-speciﬁc empirical loss. While there are
various possible modeling approaches, we consider this canonical objective of ﬁtting a single global
model to the non-identically distributed data across all clients (McMahan et al., 2017)."
BACKGROUND AND RELATED WORK,0.029978586723768737,"Client Selection in Federated Learning.
Client1 sampling is a critical problem particularly for
cross-device settings where it is prohibitive to communicate with all devices. Two common (or
default) strategies are (a) sampling the clients based on the number of local data points and uniformly
averaging the model updates, and (b) sampling the clients uniformly at random and aggregating the
model updates with weights proportional to the local samples (Li et al., 2020). There is also recent
work proposing advanced sampling techniques to incorporate dynamic systems constraints, accelerate
the convergence of federated optimization, or to obtain a better model with higher accuracy (Nishio
& Yonetani, 2019; Ribero & Vikalo, 2020; Cho et al., 2020; Lai et al., 2020). We investigate client
selection through the lens of encouraging client diversity at each communication round which largely
remains unexplored in previous work. The closest client selection method to ours is based on
clustering (e.g., selecting representative clients from separate clusters (Dennis et al., 2021)). We note
that performing (private) clustering in federated settings is still an open problem, and our method can
be viewed as a soft version of dynamic clustering at each round (discussed in the next paragraph).
The beneﬁts of gradient (or model) diversity has been demonstrated in other related contexts, such
as scaling up mini-batch stochastic gradient descent (SGD) (Yin et al., 2018). Enforcing sample or
gradient diversity during optimization also implicitly places more emphasis on the underrepresented"
BACKGROUND AND RELATED WORK,0.032119914346895075,"1Following conventions, we use the term ‘client’ for the problem of client selection. Throughout the paper,
we use ‘devices’ and ‘clients’ interchangeably."
BACKGROUND AND RELATED WORK,0.034261241970021415,Published as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.03640256959314775,"sub-population of clients, and can promote fairness deﬁned as representative disparity (Hashimoto
et al., 2018). Similar to previous work (e.g., Cho et al., 2020; Balakrishnan et al., 2020), we observe
our approach yields more fair solutions across the network in Section 5."
BACKGROUND AND RELATED WORK,0.03854389721627409,"Diverse Subset Selection via Submodularity.
Modular scores have been widely studied for subset
selection in machine learning and federated learning, e.g., a utility score for each sample or client
often measured by the loss. However, the diversity of a subset cannot be fully captured by such
modular scores since there is no score interaction. Diversity is often well modeled by a diminishing
return property, i.e., the (marginal) gain an element brings to a subset diminishes as more elements
added to the subset. There exists a rich and expressive family of functions, all of which are natural
for measuring diversity, and all having the diminishing returns property: given a ﬁnite ground set V
of size n, and any subset A ⊆B ⊆V and a v /∈B, a set function F : 2V →R is submodular if
F(v ∪A) −F(A) ≥F(v ∪B) −F(B).
(1)
This implies v is no less valuable to the smaller set A than to the larger set B. The marginal gain
of v conditioned on A is denoted f(v|A) ≜f(v ∪A) −f(A) and reﬂects the importance of v to A.
Submodular functions (Fujishige, 2005) have been widely used for diversity models (Lin & Bilmes,
2011; Batra et al., 2012; Prasad et al., 2014; Gillenwater et al., 2012; Bilmes & Bai, 2017)."
BACKGROUND AND RELATED WORK,0.04068522483940043,"Maximizing a submodular function usually encourages the diversity and reduces the redundancy of
a subset. This property has been utilized for data selection in active learning (Guillory & Bilmes,
2011), curriculum learning (Zhou & Bilmes, 2018), mini-batch partitioning (Wang et al., 2019),
gradient approximation (Mirzasoleiman et al., 2020), etc. Although the number of possible subsets
A is
 n
k

, enumerating them all to ﬁnd the maximum is intractable. Thanks to submodularity, fast
approximate algorithms (Nemhauser et al., 1978; Minoux, 1978; Mirzasoleiman et al., 2015) exist
to ﬁnd an approximately optimal A with provable bounds (Nemhauser et al., 1978; Conforti &
Cornuejols, 1984). Despite its success in data selection, submodularity has not been explored for
client selection in federated learning. Encouraging diversity amongst local gradients (or model
updates) of selected clients can effectively reduce redundant communication and promote fairness.
Moreover, it raises several new challenges in the FL setting, e.g., (1) it is unclear which submodular
function to optimize and in which space to measure the similarity/diversity between clients; (2) What
convergence guarantee can be obtained under practical assumptions such as heterogeneity among
clients, and (3) What are the effects of outdated client selection due to communication constraints?"
DIVERSE CLIENT SELECTION,0.042826552462526764,"3
DIVERSE CLIENT SELECTION"
DIVERSE CLIENT SELECTION,0.044967880085653104,"In this section, we introduce “federated averaging with diverse client selection” (or DivFL), a
method that incorporates diverse client selection into the most widely studied FL scheme, federated
averaging (FedAvg). We will ﬁrst derive a combinatorial objective for client selection via an
approximation of the full communication from all clients, which naturally morphs into a facility
location function in the gradient space that can be optimized by submodular maximization. We then
present the standard greedy algorithm that optimizes the objective by selecting a diverse subset of
clients at every communication round."
APPROXIMATION OF FULL COMMUNICATION,0.047109207708779445,"3.1
APPROXIMATION OF FULL COMMUNICATION"
APPROXIMATION OF FULL COMMUNICATION,0.04925053533190578,"We aim to ﬁnd a subset S of clients whose aggregated gradient can approximate the full aggregation
over all the N clients V = [N]. To formulate this problem, we start by following the logic
in Mirzasoleiman et al. (2020). Given a subset S, we deﬁne a mapping σ : V →S such that the
gradient information ∇Fk(vk) from client k is approximated by the gradient information from a
selected client σ(k) ∈S. For i ∈S, let Ci ≜{k ∈V |σ(k) = i} be the set of clients approximated
by client-i and γi ≜|Ci|. The full aggregated gradient can be written as
X"
APPROXIMATION OF FULL COMMUNICATION,0.05139186295503212,"k∈[N]
∇Fk(vk) =
X k∈[N]"
APPROXIMATION OF FULL COMMUNICATION,0.05353319057815846,"h
∇Fk(vk) −∇Fσ(k)(vσ(k))
i
+
X"
APPROXIMATION OF FULL COMMUNICATION,0.055674518201284794,"k∈S
γk∇Fk(vk).
(2)"
APPROXIMATION OF FULL COMMUNICATION,0.057815845824411134,"Subtracting the second term from both sides, taking the norms, and applying triangular inequality, we
can obtain an upper bound for the approximation to the aggregated gradient by S, i.e., X"
APPROXIMATION OF FULL COMMUNICATION,0.059957173447537475,"k∈[N]
∇Fk(vk) −
X"
APPROXIMATION OF FULL COMMUNICATION,0.06209850107066381,"k∈S
γk∇Fk(vk) ≤
X k∈[N]"
APPROXIMATION OF FULL COMMUNICATION,0.06423982869379015,"∇Fk(vk) −∇Fσ(k)(vσ(k))
 .
(3)"
APPROXIMATION OF FULL COMMUNICATION,0.06638115631691649,Published as a conference paper at ICLR 2022
APPROXIMATION OF FULL COMMUNICATION,0.06852248394004283,"The above inequality holds for any feasible mapping σ since the left hand side does not depend on σ.
So we can take the minimum of the right-hand side w.r.t. σ(k), ∀k ∈[N], i.e., X"
APPROXIMATION OF FULL COMMUNICATION,0.07066381156316917,"k∈[N]
∇Fk(vk) −
X"
APPROXIMATION OF FULL COMMUNICATION,0.0728051391862955,"k∈S
γk∇Fk(vk) ≤
X"
APPROXIMATION OF FULL COMMUNICATION,0.07494646680942184,"k∈[N]
min
i∈S
∇Fk(vk) −∇Fi(vi)
 ≜G(S).
(4)"
APPROXIMATION OF FULL COMMUNICATION,0.07708779443254818,"The right hand side provides a relaxed objective G(S) for minimizing the approximation error on the
left hand. Minimizing G(S) (or maximizing ¯G, a constant minus its negation) equals maximizing
a well-known submodular function, i.e., the facility location function (Cornuéjols et al., 1977). To
restrict the communication cost, we usually limit the number of selected clients to be no greater than
K, i.e., |S| ≤K. This resorts to a submodular maximization problem under cardinality constraint,
which is NP-hard but an approximation solution with 1 −e−1 bound can be achieved via the greedy
algorithm (Nemhauser et al., 1978)."
GREEDY SELECTION OF CLIENTS,0.07922912205567452,"3.2
GREEDY SELECTION OF CLIENTS"
GREEDY SELECTION OF CLIENTS,0.08137044967880086,"The naïve greedy algorithm for minimizing the upper bound of gradient approximation starts from
S ←∅, and adds one client k ∈V \S with the greatest marginal gain to S in every step, i.e.,"
GREEDY SELECTION OF CLIENTS,0.0835117773019272,"S ←S ∪k∗, k∗∈argmax
k∈V \S
[ ¯G(S) −¯G({k} ∪S)]
(5)"
GREEDY SELECTION OF CLIENTS,0.08565310492505353,"until |S| = K. Although it requires evaluating the marginal gain for all clients k ∈V \S in every
step, there exists several practical accelerated algorithms (Minoux, 1978; Mirzasoleiman et al., 2015)
to substantially reduce the number of clients participating in the evaluation. For example, stochastic
greedy algorithm (Mirzasoleiman et al., 2015) selects a client k∗from a small random subset of V \S
with size s in each step, i.e.,"
GREEDY SELECTION OF CLIENTS,0.08779443254817987,"S ←S ∪k∗, k∗∈
argmax
k∈rand(V \S, size=s)
[ ¯G(S) −¯G({k} ∪S)]
(6)"
GREEDY SELECTION OF CLIENTS,0.08993576017130621,"To incorporate the client selection into any federated learning algorithm, we apply the stochastic
greedy algorithm in each aggregation round and perform selection only from a random subset of
active clients in the network. The complete procedure is given in Algorithm 1 assuming the base
algorithm is Federated Averaging (FedAvg) (McMahan et al., 2017)."
GREEDY SELECTION OF CLIENTS,0.09207708779443255,"Algorithm 1 DivFL
Input: T, E, η, w0"
GREEDY SELECTION OF CLIENTS,0.09421841541755889,"1 for t = 0, · · · , T −1 do"
SERVER SELECTS A SUBSET OF K ACTIVE CLIENTS ST,0.09635974304068523,"2
Server selects a subset of K active clients St
using the stochastic greedy algorithm in Eq. (6),
and sends wt to them."
SERVER SELECTS A SUBSET OF K ACTIVE CLIENTS ST,0.09850107066381156,"3
for device k ∈St in parallel do"
SERVER SELECTS A SUBSET OF K ACTIVE CLIENTS ST,0.1006423982869379,"4
wk ←wt"
"SOLVE THE LOCAL SUB-PROBLEM OF CLIENT-K INEX-
ACTLY BY UPDATING WK FOR E LOCAL MINI-BATCH",0.10278372591006424,"5
Solve the local sub-problem of client-k inex-
actly by updating wk for E local mini-batch
SGD steps:
wk = wk −η∇Fk(wk)"
"SOLVE THE LOCAL SUB-PROBLEM OF CLIENT-K INEX-
ACTLY BY UPDATING WK FOR E LOCAL MINI-BATCH",0.10492505353319058,"6
Send ∆k
t := wk
t −wt back to Server"
END,0.10706638115631692,"7
end"
END,0.10920770877944326,"8
Server aggregates {∆k
t }:"
END,0.11134903640256959,"wt+1 ←wt +
1
|St| X"
END,0.11349036402569593,"k∈St
∆k
t"
END,0.11563169164882227,9 end
RETURN WT,0.11777301927194861,10 return wT
RETURN WT,0.11991434689507495,"On the left-hand side of Eq. (3)-(4), we aim
at approximating the full communication
by a weighted sum over selected clients in
S with weights {γi}i∈S. However, since
we relax the problem to minimizing its up-
per bound and the (stochastic) greedy solu-
tion is not guaranteed to achieve the global
minimum of the relaxed objective, the
weight associated with the greedy solution
S, i.e., γi = |Ci| with Ci = {k ∈V |i ∈
arg minj∈S
∇Fk(vk) −∇Fj(vj)
},
is
sub-optimal. In fact, given S, the optimal
weight {γi}i∈S can be achieved by directly
minimizing the left hand side of Eq. (3)-(4)
but it is infeasible because the full aggre-
gation P"
RETURN WT,0.12205567451820129,"k∈[N] ∇Fk(vk) is not available in
our setting. Though there might exist bet-
ter choices, we ﬁnd that simple uniform
weights work promisingly in all evaluated
scenarios of our experiments. The stochas-
tic greedy selection in line 2 of Algorithm 1
requires access to the gradients from all clients, which might be expensive in communication costs.
In practice, we may take several approaches to minimize the communication costs. One is to receive
periodic gradient updates from all clients every m communication rounds. Another is to only use the"
RETURN WT,0.12419700214132762,Published as a conference paper at ICLR 2022
RETURN WT,0.12633832976445397,"gradients from the selected clients at the current round to update part of the N × N dissimilarity ma-
trix. In our evaluation, we take the latter method with “no-overheads”. While this maybe suboptimal
since a large part of the dissimilarity matrix will contain stale gradients, we observe no signiﬁcant
loss in performance in our empirical studies."
CONVERGENCE ANALYSIS,0.1284796573875803,"4
CONVERGENCE ANALYSIS"
CONVERGENCE ANALYSIS,0.13062098501070663,"In this section, we provide theoretical analyses of the convergence behavior of Algorithm 1 for
strongly convex problems under practical assumptions of non-identically distributed data, partial
device participation, and local updating. While the current analysis only holds for the proposed client
selection algorithm applied to FedAvg, it can be naturally extended to other federated optimization
methods as well."
CONVERGENCE ANALYSIS,0.13276231263383298,"As discussed in Section 3.1, we draw connections between full gradient approximation and submodu-
lar function maximization. By solving a submodular maximization problem in the client selection
procedure, we effectively guarantee that the approximation error is small (see Eq. (4)). We state an
assumption on this below.
Assumption 1 (Gradient approximation error). At each communication round t, we assume the
server selects a set St of devices such that their aggregated gradients (with weights {γk}k∈St) is a
good approximation of the full gradients on all devices with error ϵ, i.e.,"
N,0.1349036402569593,"1
N X"
N,0.13704496788008566,"k∈St
γk∇Fk(vk
t ) −1 N X"
N,0.139186295503212,"k∈[N]
∇Fk(vk
t ) ≤ϵ."
N,0.14132762312633834,"The same assumption has been studied in previous works on coreset selection for mini-batch
SGD (Mirzasoleiman et al., 2020, Theorem 1). Note that ϵ is used as a measure to characterize how
good the approximation is and our theorem holds for any ϵ < ∞. Our algorithm (Algorithm 1) is
effectively minimizing an upper bound of ϵ to achieve a potentially small value via running submod-
ular maximization to select diverse clients (Eq. (4)). Next, we state other assumptions used in our
proof, which are standard in the federated optimization literature (e.g., Li et al., 2019).
Assumption 2. Each Fk (k ∈[N]) is L-smooth.
Assumption 3. Each Fk (k ∈[N]) is µ-strongly convex.
Assumption 4. For k ∈[N] and all t, in-device variance of stochastic gradients on random samples
ζ are bounded, i.e., E[∥∇Fk(wk
t , ζ) −∇Fk(wk
t )∥2] ≤σ2.
Assumption 5. For k ∈[N] and all t, the stochastic gradients on random samples ζ are uniformly
bounded, i.e., ∥∇Fk(wk
t , ζ)∥2 ≤G2.
Assumption 6 (Bounded heterogeneity). Statistical heterogeneity deﬁned as F ∗−P"
N,0.14346895074946467,"k∈[N] pkF ∗
k is
bounded by C, where F ∗:= minw f(w) and F ∗
k := minv Fk(v)."
N,0.145610278372591,"Let w∗∈arg minw f(w) and v∗
k ∈arg minv Fk(v) for k ∈[N]. In our analysis, we need to
bound another variant of heterogeneity ∥P"
N,0.14775160599571735,"k∈[N] pkv∗
k −w∗∥in the parameter space. Note that
under Assumption 3 (µ-strongly convexity), Assumption 6 implies that ∥P"
N,0.14989293361884368,"k∈[N] pkv∗
k −w∗∥is also
bounded by a constant."
N,0.15203426124197003,"Setup.
Following Li et al. (2019), we ﬂatten local SGD iterations at each communication round,
and index gradient evaluation steps with t (slightly abusing notation). We deﬁne virtual sequences
{vk
t }k∈[N] and {wk
t }k∈[N] where"
N,0.15417558886509636,"vk
t+1 = wk
t −ηt∇Fk(wk
t ),
wk
t+1
=
vk
t+1, if not aggregate,
select St+1 and average {vk
t+1}k∈St+1, otherwise."
N,0.15631691648822268,"While all devices virtually participate in the updates of {vk
t } at each virtual iteration t, the effective
updating rule of {wk
t } is the same as that in Algorithm 1. Further, let vt := P"
N,0.15845824411134904,"k∈[N] pkvk
t , wt :=
P"
N,0.16059957173447537,"k∈[N] pkwk
t . Therefore, wt =
vt
if not aggregate,
1
K
P"
N,0.16274089935760172,"k∈St vk
t
otherwise.
. The goal of deﬁning vt and wt"
N,0.16488222698072805,Published as a conference paper at ICLR 2022
N,0.1670235546038544,"is to relate the updates of vt to mini-batch SGD-style updates, and relate the actual updates of wt to
those of vt. We aim at approximating vt+1 by wt+1 (when aggregating) and next state a main lemma
bounding ∥wt+1 −vt+1∥."
N,0.16916488222698073,"Lemma 1. For any virtual iteration t, under Algorithm 1 and Assumptions 1-6, we have"
N,0.17130620985010706,"∥wt+1 −vt+1∥≤LGE(E −1)η2
t0 + Eϵηt0,"
N,0.1734475374732334,"where L is the smoothness parameter, G is the bounded stochastic gradient parameter, E is the
number of local iterations, and ηt0 is indexing the latest communication round."
N,0.17558886509635974,"We defer the proof to Appendix A. The main step involves using Assumption 1 to bound the
approximate error of gradients and relating accumulative gradients with model updates. With
Lemma 1, we state our convergence results as follows."
N,0.1777301927194861,"Theorem 1 (Convergence of Algorithm 1). Under Assumptions 1-6, we have"
N,0.17987152034261242,E[∥w∗−wt∥2] ≤O(1/t) + O(ϵ).
N,0.18201284796573874,"The non-vanishing term ϵ encodes the gradient approximation error (Assumption 1), and will become
zero when we select all clients at each round (i.e., K = N). In experiments, we observe that
DivFL allows us to achieve faster convergence (empirically) at the cost of additional solution bias (a
non-diminishing term dependent on ϵ)."
N,0.1841541755888651,"We provide a sketch of the proof here and defer complete analysis to Appendix A. Examine the
distances between wt+1 and w∗,"
N,0.18629550321199143,"∥wt+1 −w∗∥2 = ∥wt+1 −vt+1∥2 + ∥vt+1 −w∗∥2 + 2⟨wt+1 −vt+1, vt+1 −w∗⟩."
N,0.18843683083511778,"If iteration t is not an aggregation step, wt+1 = vt+1 and"
N,0.1905781584582441,"∥wt+1 −w∗∥2 = ∥vt+1 −w∗∥2,"
N,0.19271948608137046,which we can bound with Lemma 1 in Li et al. (2019):
N,0.1948608137044968,"E[∥vt+1 −w∗∥2] ≤(1 −ηtµ)E[∥wt −w∗∥2] + η2
t B
(7)"
N,0.19700214132762311,"for some constant B. If t is an aggregation step, we need to bound"
N,0.19914346895074947,"E[∥wt+1 −vt+1∥2] + E[∥vt+1 −w∗∥2] + 2E[⟨wt+1 −vt+1, vt+1 −w∗⟩]."
N,0.2012847965738758,"The second term can be bounded by Eq. (7), which contains (1 −ηtµ)E[∥wt −w∗∥2]. Therefore,
combined with Lemma 1, with a decaying step size, we can obtain a recursion on E[∥wt+1 −w∗∥2]
which leads to Theorem 1. We provide the complete proof in Appendix A."
EXPERIMENTS,0.20342612419700215,"5
EXPERIMENTS"
EXPERIMENTS,0.20556745182012848,"Setup.
We evaluate the DivFL approach utilizing both synthetic and real federated datasets from the
LEAF federated learning benchmark (Caldas et al., 2019). The synthetic dataset enables us to control
the data heterogeneity across clients for evaluation. We consider two baselines to compare our DivFL
against: a) random sampling without replacement, and b) the power-of-choice approach (Cho et al.,
2020) where clients with the largest training losses are selected. For DivFL, we consider an ideal
setting where 1-step gradients are queried from every device in the network for each global round. In
addition, we also evaluate the “no overhead” setting where (a) client updates from previous rounds
are utilized to update part of the dissimilarity matrix, and (b) we run the stochastic greedy algorithm
to avoid selecting from an entire set of clients. While the former provides the upper bound on the
performance, the “no overhead” approach and other variants are more suited to realistic settings. For
all the methods, we ﬁx the number of clients per round K = 10 and report the performance for other
choices of K in Appendix. Each selected client performs τ = 1 round of local model update before
sharing the updates with the server unless otherwise noted. We report the performance of DivFL
across metrics including convergence, fairness and learning efﬁciency. We further report the impact
of the subset size K on these metrics. Our code is publicly available at github.com/melodi-lab/divﬂ."
EXPERIMENTS,0.20770877944325483,Published as a conference paper at ICLR 2022
RESULTS ON THE SYNTHETIC DATASET,0.20985010706638116,"5.1
RESULTS ON THE SYNTHETIC DATASET"
RESULTS ON THE SYNTHETIC DATASET,0.21199143468950749,"We generate synthetic data following the setup described in Li et al. (2020). The parameters
and data are generated from Gaussian distributions and the model is logistic regression. y =
arg max(softmax(WT X + b)). We consider a total of 30 clients where the local dataset sizes for
each client follow the power law. We set the mini batch-size to 10 and learning rate η = 0.01.
We report training loss as well as the mean and variance of test accuracies versus the number of
communication rounds in Figure 1 for the synthetic IID setting. We observe three key beneﬁts of
DivFL compared to random sampling and power-of-choice approaches. On the one hand, DivFL
achieves a signiﬁcant convergence speedup (∼10× faster) to reach the same loss and accuracy relative
to random sampling and power-of-choice. The convergence speed could potentially be attributed
to the reduction in the variance of updates across epochs as DivFL aims to minimize the gradient
approximation error w.r.t the true gradient. Furthermore, DivFL also achieves the lowest loss and
highest accuracy among the client selection approaches. By reaching a lower variance of accuracy in
comparison to the baselines, DivFL also shows marginal improvement in fairness, even when the
data distribution is IID."
RESULTS ON THE SYNTHETIC DATASET,0.21413276231263384,"As one would expect, while being impractical, utilizing the gradient computation from all clients to
update the dissimilarity matrix provides an upper bound on the achievable performance. Our “no
overhead” approach to update the dissimilarity matrix partially from only the participating clients
still outperforms the baselines in the test accuracy and training loss, while preserving the faster
convergence of DivFL. In Appendix B.1, we report the above metrics for different choices of K."
RESULTS ON THE SYNTHETIC DATASET,0.21627408993576017,"0
50
100
150
200
250
300
# Rounds 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
RESULTS ON THE SYNTHETIC DATASET,0.21841541755888652,Training Loss
RESULTS ON THE SYNTHETIC DATASET,0.22055674518201285,Num. of Selected Clients K = 10
RESULTS ON THE SYNTHETIC DATASET,0.22269807280513917,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
RESULTS ON THE SYNTHETIC DATASET,0.22483940042826553,"0
50
100
150
200
250
300
# Rounds 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
RESULTS ON THE SYNTHETIC DATASET,0.22698072805139186,Testing Accuracy
RESULTS ON THE SYNTHETIC DATASET,0.2291220556745182,Num. of Selected Clients K = 10
RESULTS ON THE SYNTHETIC DATASET,0.23126338329764454,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
RESULTS ON THE SYNTHETIC DATASET,0.2334047109207709,"0
50
100
150
200
250
300
# Rounds 0.010 0.015 0.020 0.025 0.030 0.035"
RESULTS ON THE SYNTHETIC DATASET,0.23554603854389722,Accuracy Variance
RESULTS ON THE SYNTHETIC DATASET,0.23768736616702354,Num. of Selected Clients K = 10
RESULTS ON THE SYNTHETIC DATASET,0.2398286937901499,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
RESULTS ON THE SYNTHETIC DATASET,0.24197002141327623,"Figure 1: Training loss, mean and variance of test accuracies of DivFL compared with random
sampling and power-of-choice on the synthetic IID data. For DivFL (no overhead), we utilize only
the gradients from clients participating in the previous round. We see that DivFL achieves faster
convergence and converges to more accurate and slightly more fair solutions than all baselines."
RESULTS ON THE SYNTHETIC DATASET,0.24411134903640258,"We report the training loss as well as the mean and variance of testing accuracies for the synthetic
non-IID dataset in Figure 2. We notice the noisy updates of the random sampling approach and
slightly less noisy ones of the power-of-choice approach. In addition, both the baselines converge
to a less optimal solution than DivFL. The mean accuracy gains of DivFL are quite signiﬁcant
(10% points). In terms of convergence, power-of-choice approach converges in 2x fewer rounds
than random sampling to reach an accuracy of 70% but DivFL converges in 5x fewer rounds with
less noisy updates than both baselines. The fairness gains of DivFL for the non-IID setting is more
signiﬁcant. This is due to the higher degree of heterogeneity in client updates and the ability of
DivFL to ﬁnd diverse representative clients in the update space. In Appendix B.1, we provide more
ablation studies for different choices of the hyperparameter K."
RESULTS ON THE SYNTHETIC DATASET,0.2462526766595289,"0
50
100
150
200
250
300
# Rounds 0.5 1.0 1.5 2.0 2.5 3.0"
RESULTS ON THE SYNTHETIC DATASET,0.24839400428265523,Training Loss
RESULTS ON THE SYNTHETIC DATASET,0.2505353319057816,Num. of Selected Clients K = 10
RESULTS ON THE SYNTHETIC DATASET,0.25267665952890794,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
RESULTS ON THE SYNTHETIC DATASET,0.25481798715203424,"0
50
100
150
200
250
300
# Rounds 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
RESULTS ON THE SYNTHETIC DATASET,0.2569593147751606,Testing Accuracy
RESULTS ON THE SYNTHETIC DATASET,0.25910064239828695,Num. of Selected Clients K = 10
RESULTS ON THE SYNTHETIC DATASET,0.26124197002141325,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
RESULTS ON THE SYNTHETIC DATASET,0.2633832976445396,"0
50
100
150
200
250
300
# Rounds 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
RESULTS ON THE SYNTHETIC DATASET,0.26552462526766596,Accuracy Variance
RESULTS ON THE SYNTHETIC DATASET,0.2676659528907923,Num. of Selected Clients K = 10
RESULTS ON THE SYNTHETIC DATASET,0.2698072805139186,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
RESULTS ON THE SYNTHETIC DATASET,0.27194860813704497,"Figure 2: Training loss and test accuracy of DivFL compared with random sampling and
power-of-choice on synthetic non-IID data. DivFL converges faster and to more accurate solutions
and much improved fairness than all baselines."
RESULTS ON THE SYNTHETIC DATASET,0.2740899357601713,Published as a conference paper at ICLR 2022
RESULTS ON REAL DATASETS,0.2762312633832976,"5.2
RESULTS ON REAL DATASETS"
RESULTS ON REAL DATASETS,0.278372591006424,"We present extensive results on the robustness of DivFL on LEAF federated learning datasets. This
includes image datasets (FEMNIST, CelebA) and a language dataset (Shakespeare). For each of these
cases, we report the convergence behavior and fairness performance of DivFL."
RESULTS ON REAL DATASETS,0.28051391862955033,"For FEMNIST, there are a total of 500 clients where each client contains 3 out of 10 lowercase hand-
written characters. Clients use a CNN-based 10-class classiﬁer model with two 5x5-convolutional
and 2x2-maxpooling (with a stride of 2) layers followed by a dense layer with 128 activations. For
our experiments, CelebA contains 515 clients where each client is a celebrity and a CNN-based binary
classiﬁer is utilized with 4 3x3-convolutional and 2x2-maxpooling layers followed by a dense layer.
For Shakespeare, a two-layer LSTM classiﬁer containing 100 hidden units with an 8D embedding
layer is utilized. The task is next-character prediction with 80 classes of characters in total. There are
a total of 109 clients. The model takes as input a sequence of 80 characters, embeds the characters
into a learned 8-dimensional space and outputs one character per training sample after 2 LSTM layers
and a densely-connected layer. For the Shakespeare case, each client performs 5 local updates in
every global round. For the other two datasets, client updates are shared after only 1 local update."
CONVERGENCE BEHAVIOR,0.2826552462526767,"5.2.1
CONVERGENCE BEHAVIOR"
CONVERGENCE BEHAVIOR,0.284796573875803,"We ﬁrst present the convergence behavior of DivFL in comparison to the baselines for the above
3 datasets in Figure 3. On FEMNIST, DivFL converges faster than both random sampling and
power-of-choice approaches also with less noisy updates. DivFL with “no overhead” performs as
well as the ideal case. We note that in the case of FEMNIST, there are several clients that have
similar distribution over class labels. DivFL can be crucial in such cases where redundancies can be
minimized and diversity encouraged by maximizing a submodular function."
CONVERGENCE BEHAVIOR,0.28693790149892934,"On CelebA, DivFL has a faster convergence than both baselines in the ideal setting. However,
the “no overhead” setting converges at the same rate as random sampling followed by the power-
of-choice approach. In the case of Shakespeare, interestingly, both DivFL and power-of-choice
approaches converge at the same rate and faster than random sampling (by 6x). Overall, for CelebA
and Shakespeare datasets, we note that DivFL either converges at least at the same rate as the fastest
baseline."
CONVERGENCE BEHAVIOR,0.2890792291220557,"0
50
100
150
200
250
300
350
400
# Rounds 0.5 1.0 1.5 2.0 2.5"
CONVERGENCE BEHAVIOR,0.291220556745182,Training Loss (smoothened)
CONVERGENCE BEHAVIOR,0.29336188436830835,FEMNIST: Num. of Selected Clients K = 10
CONVERGENCE BEHAVIOR,0.2955032119914347,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
CONVERGENCE BEHAVIOR,0.29764453961456105,"0
100
200
300
400
500
600
# Rounds 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
CONVERGENCE BEHAVIOR,0.29978586723768735,Training Loss (smoothened)
CONVERGENCE BEHAVIOR,0.3019271948608137,CelebA: Num. of Selected Clients K = 10
CONVERGENCE BEHAVIOR,0.30406852248394006,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
CONVERGENCE BEHAVIOR,0.30620985010706636,"0
10
20
30
40
50
# Rounds 2.5 3.0 3.5 4.0 4.5 5.0"
CONVERGENCE BEHAVIOR,0.3083511777301927,Training Loss (smoothened)
CONVERGENCE BEHAVIOR,0.31049250535331907,Shakespeare: Num. of Selected Clients K = 10
CONVERGENCE BEHAVIOR,0.31263383297644537,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
CONVERGENCE BEHAVIOR,0.3147751605995717,"0
50
100
150
200
250
300
350
400
# Rounds 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
CONVERGENCE BEHAVIOR,0.3169164882226981,Testing Accuracy (smoothened)
CONVERGENCE BEHAVIOR,0.31905781584582443,"FEMNIST: Num. of Selected Clients K = 10
Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
CONVERGENCE BEHAVIOR,0.32119914346895073,"0
100
200
300
400
500
600
# Rounds 0.50 0.55 0.60 0.65 0.70 0.75 0.80"
CONVERGENCE BEHAVIOR,0.3233404710920771,Testing Accuracy (smoothened)
CONVERGENCE BEHAVIOR,0.32548179871520344,CelebA: Num. of Selected Clients K = 10
CONVERGENCE BEHAVIOR,0.32762312633832974,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
CONVERGENCE BEHAVIOR,0.3297644539614561,"0
5
10
15
20
25
30
35
40
# Rounds 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40"
CONVERGENCE BEHAVIOR,0.33190578158458245,Testing Accuracy (smoothened)
CONVERGENCE BEHAVIOR,0.3340471092077088,Shakespeare: Num. of Selected Clients K = 10
CONVERGENCE BEHAVIOR,0.3361884368308351,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
CONVERGENCE BEHAVIOR,0.33832976445396146,"Figure 3: Training loss and test accuracy of DivFL compared with random sampling and
power-of-choice on real datasets. We observe clear improvement for DivFL on FEMNIST. For the
other two datasets, the communication-efﬁcient DivFL “no-overhead” converges at the same rate
as the fastest baseline."
FAIRNESS,0.3404710920770878,"5.2.2
FAIRNESS"
FAIRNESS,0.3426124197002141,"We also compare the fairness performance of the different approaches for the above 3 real datasets in
Figure 4. We measure this through the variance of test accuracies vs training rounds and observe both
the trajectory as well as the ﬁnal variance of the test accuracies. For the image datasets FEMNIST and
CelebA, it’s clear that DivFL has better fairness after convergence with superior mean accuracy and
better/comparable variance of accuracies. This shows that diversity helps result in a more uniform
performance across clients. Interestingly for the case of Shakespeare, we observe the variance of"
FAIRNESS,0.34475374732334046,Published as a conference paper at ICLR 2022
FAIRNESS,0.3468950749464668,"accuracies for DivFL, especially under the “no overhead” setting is larger than both baselines. This
could possibly be the result of the sparsity in the gradient updates for language models, in general.
While the random sampling approach has lower accuracy variance, the mean accuracies are also
lower showing that it is not necessarily more “fair”."
FAIRNESS,0.3490364025695932,"0
50
100
150
200
250
300
350
400
# Rounds 0.02 0.04 0.06 0.08 0.10"
FAIRNESS,0.3511777301927195,Accuracy Variance (smoothened)
FAIRNESS,0.3533190578158458,FEMNIST: Num. of Selected Clients K = 10
FAIRNESS,0.3554603854389722,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
FAIRNESS,0.3576017130620985,"0
100
200
300
400
500
600
# Rounds 0.060 0.065 0.070 0.075 0.080 0.085 0.090 0.095"
FAIRNESS,0.35974304068522484,Accuracy Variance (smoothened)
FAIRNESS,0.3618843683083512,CelebA: Num. of Selected Clients K = 10
FAIRNESS,0.3640256959314775,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
FAIRNESS,0.36616702355460384,"0
5
10
15
20
25
30
35
40
# Rounds 0.000 0.001 0.002 0.003 0.004 0.005"
FAIRNESS,0.3683083511777302,Accuracy Variance (smoothened)
FAIRNESS,0.37044967880085655,Shakespeare: Num. of Selected Clients K = 10
FAIRNESS,0.37259100642398285,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
FAIRNESS,0.3747323340471092,"Figure 4: Variance of test accuracies over the 3 real datasets shows that DivFL has fairness beneﬁts
on the image datasets. Poorer fairness performance for Shakespeare could be attributed to the sparsity
in the gradient updates for language models."
IMPACT OF K AND LEARNING EFFICIENCY,0.37687366167023556,"5.2.3
IMPACT OF K AND LEARNING EFFICIENCY"
IMPACT OF K AND LEARNING EFFICIENCY,0.37901498929336186,"We also empirically evaluate how the choice of number of clients per round (K) affects the ﬁnal model
performance. We take the case of CelebA dataset and observe the (smoothened) mean and variance of
ﬁnal test accuracies after 400 rounds for different choices of K. The ﬁnal mean test accuracy generally
increases and then decreases for random sampling showing that more participation has the potential
to improve the ﬁnal model performance but not always due to potential risk of overﬁtting. We also
note that for DivFL, the test accuracy improves as K grows to 30, but does not fall off as steeply
as in the case of random sampling. On the one hand, this highlights the robustness of DivFL. On
the other hand, DivFL achieves the highest mean accuracy of about 0.79 for K = 30. This shows
considerably fewer client participation in comparison to K = 60 for random sampling and K = 80
for power-of-choice to achieve their respective highest test accuracies. This reveals that DivFL offers
improved learning efﬁciency by its ability to learn from fewer clients per round compared to the
baselines. It must also be noted that DivFL outperforms both baselines in ﬁnal mean and variance of
test accuracies for all choices of K in Figure 5."
IMPACT OF K AND LEARNING EFFICIENCY,0.3811563169164882,"20
40
60
80
100
Number of Clients per round (K) 0.600 0.625 0.650 0.675 0.700 0.725 0.750 0.775 0.800"
IMPACT OF K AND LEARNING EFFICIENCY,0.38329764453961457,Testing Accuracy
IMPACT OF K AND LEARNING EFFICIENCY,0.3854389721627409,"CelebA
Random Sampling
Power-of-Choice
DivFL (no overhead)"
IMPACT OF K AND LEARNING EFFICIENCY,0.3875802997858672,"20
40
60
80
100
Number of Clients per round (K) 0.055 0.060 0.065 0.070 0.075 0.080"
IMPACT OF K AND LEARNING EFFICIENCY,0.3897216274089936,Testing Accuracy Variance
IMPACT OF K AND LEARNING EFFICIENCY,0.39186295503211993,"CelebA
Random Sampling
Power-of-Choice
DivFL (no overhead)"
IMPACT OF K AND LEARNING EFFICIENCY,0.39400428265524623,"Figure 5: Final Testing Accuracies (smoothened) after 400 epochs on CelebA dataset shows the
power of DivFL to have robust learning beating baselines for all choices of K. Furthermore, DivFL
can achieve the highest test accuracy and lowest variance using the smallest number of clients per
round in comparison to baselines."
CONCLUSION,0.3961456102783726,"6
CONCLUSION"
CONCLUSION,0.39828693790149894,"In this paper, we posed the problem of selecting the most diverse subset of clients for federated
learning as the solution to maximizing a submodular facility location function deﬁned over gradient
space. To this end, we developed DivFL and provided a thorough analysis of its convergence in
heterogeneous settings. Through extensive empirical results on both synthetic and real datasets,
we demonstrated strong gains of DivFL over state-of-the-art baseline client selection strategies
across key metrics including convergence speed, fairness, and learning efﬁciency. Furthermore, our
communication-efﬁcient variant of DivFL holds all these advantages while making them amenable
to practical federated learning systems."
CONCLUSION,0.4004282655246253,Published as a conference paper at ICLR 2022
ETHICS AND REPRODUCIBILITY STATEMENT,0.4025695931477516,"7
ETHICS AND REPRODUCIBILITY STATEMENT"
ETHICS AND REPRODUCIBILITY STATEMENT,0.40471092077087795,"Our algorithm to select the most diverse set of clients not only aims to speed up learning but can
also allow for a fair representation of clients in the training process. This issue has been identiﬁed
as a key challenge in federated learning in several studies and algorithms exist that speciﬁcally
address this concern. In our paper, our approach is unique in addressing client diversity (without
any additional knowledge of client data) by encouraging diversity in the gradient space with (near)
optimality guarantees through the optimization framework."
ETHICS AND REPRODUCIBILITY STATEMENT,0.4068522483940043,"In order to make our paper reproducible, we provide a detailed proof of Theorem 1 in Appendix A.
Further, detailed experimental description including 1) data preparation, 2) model architecture, 3)
training algorithm and hyper-parameters are provided in Section 5 and Appendix B."
ETHICS AND REPRODUCIBILITY STATEMENT,0.4089935760171306,ACKNOWLEDGEMENTS
ETHICS AND REPRODUCIBILITY STATEMENT,0.41113490364025695,"This work was supported in part by the CONIX Research Center, one of six centers in JUMP, a
Semiconductor Research Corporation (SRC) program sponsored by DARPA."
ETHICS AND REPRODUCIBILITY STATEMENT,0.4132762312633833,Published as a conference paper at ICLR 2022
REFERENCES,0.41541755888650966,REFERENCES
REFERENCES,0.41755888650963596,"Ravikumar Balakrishnan, Mustafa Akdeniz, Sagar Dhakal, and Nageen Himayat. Resource man-
agement and fairness for federated learning over wireless edge networks. In 2020 IEEE 21st
International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), pp.
1–5, 2020. doi: 10.1109/SPAWC48557.2020.9154285."
REFERENCES,0.4197002141327623,"Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich. Diverse
m-best solutions in markov random ﬁelds. In ECCV, pp. 1–16, 2012."
REFERENCES,0.42184154175588867,"Jeffrey A. Bilmes and Wenruo Bai. Deep submodular functions. CoRR, abs/1701.08939, 2017. URL"
REFERENCES,0.42398286937901497,http://arxiv.org/abs/1701.08939.
REFERENCES,0.4261241970021413,"Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcný, H. Brendan
McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings, 2019."
REFERENCES,0.4282655246252677,"Wenlin Chen, Samuel Horvath, and Peter Richtarik. Optimal client sampling for federated learning,
2020."
REFERENCES,0.430406852248394,"Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence
analysis and power-of-choice selection strategies, 2020."
REFERENCES,0.43254817987152033,"Michele Conforti and Gerard Cornuejols. Submodular set functions, matroids and the greedy
algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem.
Discrete Applied Mathematics, 7(3):251–274, 1984."
REFERENCES,0.4346895074946467,"G. Cornuéjols, M. Fisher, and G.L. Nemhauser. On the uncapacitated location problem. Annals of
Discrete Mathematics, 1:163–177, 1977."
REFERENCES,0.43683083511777304,"Don Kurian Dennis, Tian Li, and Virginia Smith. Heterogeneity for the win: One-shot federated
clustering. arXiv preprint arXiv:2103.00697, 2021."
REFERENCES,0.43897216274089934,"Satoru Fujishige. Submodular functions and optimization. Annals of discrete mathematics. Elsevier,
2005."
REFERENCES,0.4411134903640257,"Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. Near-optimal map inference for determinantal
point processes. In NeurIPS, pp. 2735–2743, 2012."
REFERENCES,0.44325481798715205,"Andrew Guillory and Jeff Bilmes. Active semi-supervised learning using submodular functions. In
Uncertainty in Artiﬁcial Intelligence (UAI), Barcelona, Spain, July 2011. AUAI."
REFERENCES,0.44539614561027835,"Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without
demographics in repeated loss minimization. In International Conference on Machine Learning,
2018."
REFERENCES,0.4475374732334047,"Tiansheng Huang, Weiwei Lin, Li Shen, Keqin Li, and Albert Y. Zomaya. Stochastic client selection
for federated learning with volatile clients, 2021."
REFERENCES,0.44967880085653106,"Rishabh Iyer, Stefanie Jegelka, and Jeff A. Bilmes. Fast semidifferential-based submodular function
optimization. In ICML, 2013."
REFERENCES,0.4518201284796574,"Fan Lai, Xiangfeng Zhu, Harsha V Madhyastha, and Mosharaf Chowdhury. Oort: Efﬁcient federated
learning via guided participant selection. arxiv. org/abs/2010.06081, 2020."
REFERENCES,0.4539614561027837,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems,
2020."
REFERENCES,0.45610278372591007,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019."
REFERENCES,0.4582441113490364,"Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In ACL, pp.
510–520, 2011."
REFERENCES,0.4603854389721627,Published as a conference paper at ICLR 2022
REFERENCES,0.4625267665952891,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-Efﬁcient Learning of Deep Networks from Decentralized Data. In International
Conference on Artiﬁcial Intelligence and Statistics, 2017."
REFERENCES,0.46466809421841543,"Michel Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Opti-
mization Techniques, volume 7 of Lecture Notes in Control and Information Sciences, chapter 27,
pp. 234–243. Springer Berlin Heidelberg, 1978."
REFERENCES,0.4668094218415418,"Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrák, and Andreas
Krause. Lazier than lazy greedy. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence, pp. 1812–1818, 2015."
REFERENCES,0.4689507494646681,"Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efﬁcient training of
machine learning models. In International Conference on Machine Learning, 2020."
REFERENCES,0.47109207708779444,"Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Interna-
tional Conference on Machine Learning, 2019."
REFERENCES,0.4732334047109208,"G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing
submodular set functions. Mathematical Programming, 14(1):265–294, 1978."
REFERENCES,0.4753747323340471,"Takayuki Nishio and Ryo Yonetani. Client selection for federated learning with heterogeneous
resources in mobile edge. In ICC 2019-2019 IEEE International Conference on Communications,
2019."
REFERENCES,0.47751605995717344,"Adarsh Prasad, Stefanie Jegelka, and Dhruv Batra. Submodular meets structured: Finding diverse
subsets in exponentially-large structured item sets. In NeurIPS, pp. 2645–2653, 2014."
REFERENCES,0.4796573875802998,"Monica Ribero and Haris Vikalo. Communication-efﬁcient federated learning via optimal client
sampling. arXiv preprint arXiv:2007.15197, 2020."
REFERENCES,0.4817987152034261,"Shengjie Wang, Wenruo Bai, Chandrashekhar Lavania, and Jeff Bilmes. Fixing mini-batch sequences
with hierarchical robust partitioning. In Proceedings of the Twenty-Second International Conference
on Artiﬁcial Intelligence and Statistics, volume 89, pp. 3352–3361, 2019."
REFERENCES,0.48394004282655245,"Kai Wei, Rishabh Iyer, and Jeff Bilmes. Fast multi-stage submodular maximization. In ICML, 2014."
REFERENCES,0.4860813704496788,"Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, and Peter
Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. In International
Conference on Artiﬁcial Intelligence and Statistics, 2018."
REFERENCES,0.48822269807280516,"Tianyi Zhou and Jeff Bilmes. Minimax curriculum learning: Machine teaching with desirable
difﬁculties and scheduled diversity. In ICLR, 2018."
REFERENCES,0.49036402569593146,APPENDIX
REFERENCES,0.4925053533190578,"A
COMPLETE CONVERGENCE ANALYSIS"
REFERENCES,0.49464668094218417,"Below shows the convergence analysis. Many steps make a distinction between if we are doing an
aggregating steps (from the clients to the server), or not (when the clients do not communicate). We
assume that we aggregate every E time steps. Deﬁne virtual sequences {vk
t }k∈[N] and {wk
t }k∈[N]
where for all k ∈[N],"
REFERENCES,0.49678800856531047,"vk
t+1 = wk
t −ηt∇Fk(wk
t )
(8)"
REFERENCES,0.4989293361884368,"wk
t+1 =
vk
t+1
if not aggregating,
sample St+1 and average {vk
t+1}k∈St+1
otherwise.
(9)"
REFERENCES,0.5010706638115632,Published as a conference paper at ICLR 2022 Let
REFERENCES,0.5032119914346895,"vt :=
X"
REFERENCES,0.5053533190578159,"k∈[N]
pkvt
k,
(10)"
REFERENCES,0.5074946466809421,"wt :=
X"
REFERENCES,0.5096359743040685,"k∈[N]
pkwk
t .
(11)"
REFERENCES,0.5117773019271948,"where pk ≥0 is the given weight of the kth client and w.l.o.g., we assume P"
REFERENCES,0.5139186295503212,"k pk = 1. Therefore,"
REFERENCES,0.5160599571734475,"wt =
vt
if not aggregating, i.e., when t ̸= ℓE for some integer ℓ,
1
K
P"
REFERENCES,0.5182012847965739,"l∈St vl
t
otherwise.
(12) Let"
REFERENCES,0.5203426124197003,"gt :=
X"
REFERENCES,0.5224839400428265,"k∈[N]
pkFk(wk
t ; ζk
t ),
(13) and"
REFERENCES,0.5246252676659529,vt+1 = wt −ηt  X
REFERENCES,0.5267665952890792,"k∈[N]
pkFk(wk
t , ζk
t ) "
REFERENCES,0.5289079229122056,":= wt −ηtgt.
(14)"
REFERENCES,0.5310492505353319,We have
REFERENCES,0.5331905781584583,"∥wt+1 −w∗∥2 = ∥wt+1 −vt+1 + vt+1 −w∗∥2
(15)"
REFERENCES,0.5353319057815846,"= ∥wt+1 −vt+1∥2 + ∥vt+1 −w∗∥2 + 2⟨wt+1 −vt+1, vt+1 −w∗⟩.
(16)"
REFERENCES,0.5374732334047109,"If not aggregating,"
REFERENCES,0.5396145610278372,"wt+1 = vt+1.
(17) Hence"
REFERENCES,0.5417558886509636,"∥wt+1 −w∗∥2 = ∥vt+1 −w∗∥2.
(18)"
REFERENCES,0.5438972162740899,"Using Lemma 1 in Li et al. (2019), we know E[∥vt+1 −w∗∥2] ≤(1 −ηtµ)E[∥wt −w∗∥2] + η2
t B
holds for some constant B. If we are aggregating, we need to bound"
REFERENCES,0.5460385438972163,"E[∥wt+1 −vt+1∥2] + E[∥vt+1 −w∗∥2] + 2E[⟨wt+1 −vt+1, vt+1 −w∗⟩].
(19)"
REFERENCES,0.5481798715203426,"Let the last time of aggregation happens at step t0 = t+1−E, when we select a subset S (associated
with weights {γk}k∈S) using the greedy algorithm. Let ∆vk
τ be the updates on vk at the τ-th iteration,
i.e., ∆vk
τ := vk
τ+1 −vk
τ . Then vt+1 = wt0 + 1 N
P"
REFERENCES,0.550321199143469,"k∈[N]
Pt
τ=t0 ∆vk
τ . To bound the ﬁrst term above,"
REFERENCES,0.5524625267665952,∥wt+1 −vt+1∥=  
REFERENCES,0.5546038543897216,wt0 + 1 N X
REFERENCES,0.556745182012848,"k∈S
γk t
X"
REFERENCES,0.5588865096359743,"τ=t0
∆vk
τ ! − "
REFERENCES,0.5610278372591007,"wt0 + 1 N X k∈[N] t
X"
REFERENCES,0.563169164882227,"τ=t0
∆vk
τ   (20) =  t
X τ=t0  1 N X"
REFERENCES,0.5653104925053534,"k∈S
γk∆vk
τ −1 N X"
REFERENCES,0.5674518201284796,"k∈[N]
∆vk
τ   (21) ≤ t
X τ=t0 "
N,0.569593147751606,"1
N X"
N,0.5717344753747323,"k∈S
γk∆vk
τ −1 N X"
N,0.5738758029978587,"k∈[N]
∆vk
τ (22)"
N,0.576017130620985,"Similar to the CRAIG paper (Mirzasoleiman et al., 2020), we assume that the subset S selected in
step t0 = t + 1 −E provides an approximation of the full gradient such that"
N,0.5781584582441114,"1
N X"
N,0.5802997858672377,"k∈S
γk∇Fk(vk
t0) −1 N X"
N,0.582441113490364,"k∈[N]
∇Fk(vk
t0)"
N,0.5845824411134903,"≤ϵ,
(23)"
N,0.5867237687366167,Published as a conference paper at ICLR 2022
N,0.588865096359743,"For every local step τ ∈(t0, t], we use the same S to approximate the full gradient because we only
communicate the local gradients every E local steps. To bound the gradient approximation at step τ
using the stale S, we have "
N,0.5910064239828694,"1
N X"
N,0.5931477516059958,"k∈S
γk∇Fk(vk
τ ) −1 N X"
N,0.5952890792291221,"k∈[N]
∇Fk(vk
τ ) ≤"
N,0.5974304068522484,"1
N X"
N,0.5995717344753747,"k∈S
γk∇Fk(vk
τ ) −1 N X"
N,0.6017130620985011,"k∈S
γk∇Fk(vk
t0) + (24)"
N,0.6038543897216274,"1
N X"
N,0.6059957173447538,"k∈S
γk∇Fk(vk
t0) −1 N X"
N,0.6081370449678801,"k∈[N]
∇Fk(vk
t0) + (25)"
N,0.6102783725910065,"1
N X"
N,0.6124197002141327,"k∈[N]
∇Fk(vk
τ ) −1 N X"
N,0.6145610278372591,"k∈[N]
∇Fk(vk
t0) (26) ≤2LG τ
X"
N,0.6167023554603854,"ν=t0
ην + ϵ,
(27)"
N,0.6188436830835118,"where the ﬁrst and the third term on the right hand side are bounded using the L-smoothness of Fk(·)
and G-bounded norm of its stochastic gradient. Hence, we can continue to bound the ﬁrst term in
Eq. (22) by"
N,0.6209850107066381,"∥wt+1 −vt+1∥≤ t
X τ=t0 "
N,0.6231263383297645,"1
N X"
N,0.6252676659528907,"k∈S
γk∆vk
τ −1 N X"
N,0.6274089935760171,"k∈[N]
∆vk
τ (28) = t
X"
N,0.6295503211991434,"τ=t0
ητ "
N,0.6316916488222698,"1
N X"
N,0.6338329764453962,"k∈S
γk∇Fk(vk
τ ) −1 N X"
N,0.6359743040685225,"k∈[N]
∇Fk(vk
τ ) (29) ≤2LG t
X τ=t0 τ
X"
N,0.6381156316916489,"ν=t0
ητην + Eϵητ
(30)"
N,0.6402569593147751,"≤LGE(E −1)η2
t0 + Eϵηt0
(31)"
N,0.6423982869379015,"= LGE(E −1)

1 +
E −1
t + γ −(E −1)"
N,0.6445396145610278,"2
η2
t + Eϵ

1 +
E −1
t + γ −(E −1) 
ηt (32)"
N,0.6466809421841542,"where E is the number of local steps between two communication (aggregation) rounds. Therefore,
Eq. (19) can be bounded as follows:"
N,0.6488222698072805,"E[∥wt+1 −w∗∥2]
(33)"
N,0.6509635974304069,"≤E[∥wt+1 −vt+1∥2] + E[∥vt+1 −w∗∥2] + 2E[⟨wt+1 −vt+1, vt+1 −w∗⟩]
(34)"
N,0.6531049250535332,"≤
 
LGE(E −1)η2
t0 + Eϵηt0
2 +

(1 −ηtµ)E[∥wt −w∗∥2] + η2
t B

+ 2
 
LGE(E −1)η2
t0 + Eϵηt0

E[∥vt+1 −w∗∥]
(35)"
N,0.6552462526766595,"≤(1 −ηtµ)E[∥wt −w∗∥2] + Eϵρηt0 +

LGE(E −1)ρ + (LGE(E −1)ηt0 + Eϵ)2
η2
t0 + Bη2
t
(36)"
N,0.6573875802997858,"≤(1 −ηtµ)E[∥wt −w∗∥2] + ϵρEηt0 +

LGρ + (LGEηt0 + ϵ)2
E2η2
t0 + Bη2
t ,
(37)"
N,0.6595289079229122,Published as a conference paper at ICLR 2022
N,0.6616702355460385,"where ηt =
β
t+γ , ηt0 =
β
t−E+1+γ and E[∥vt+1 −w∗∥] ≤ρ, shown as follows."
N,0.6638115631691649,"E
vt+1 −w∗ ≤E "
N,0.6659528907922913,"vt+1 −
X"
N,0.6680942184154176,"i∈[N]
piv∗
i + E  X"
N,0.6702355460385439,"i∈[N]
piv∗
i −w∗ (38) ≤E "
N,0.6723768736616702,"vt+1 −
X"
N,0.6745182012847966,"i∈[N]
piv∗
i"
N,0.6766595289079229,"+ M
(39) ≤
X"
N,0.6788008565310493,"i∈[N]
E∥pi(vt+1
i
−v∗
i )∥+ M
(40) ≤
X i∈[N] pi"
N,0.6809421841541756,"µ E
∇fi(vt
i)
 + M
(41) ≤G"
N,0.683083511777302,"µ + M ≤ρ.
(42)"
N,0.6852248394004282,The ﬁnal convergence rates follows from Lemma 3 in Mirzasoleiman et al. (2020).
N,0.6873661670235546,"B
ADDITIONAL EXPERIMENTS"
N,0.6895074946466809,"B.1
MORE ABLATION STUDIES ON SYNTHETIC AND FEMNIST DATASETS"
N,0.6916488222698073,"We present more ﬁndings from training DivFL on synthetic IID and non-IID dataset for K = 20
in Figure 6, 7 and 8. In addition to the training loss, mean and variance of test accuracies, we also
plot the 10th percentile test accuracy that shows the worst-case performance of the different client
selection strategies. As in the case of K = 10, we observe gains across convergence, fairness and
improved model accuracy for DivFL. In addition, the 10th percentile accuracy further conﬁrms that
DivFL improves the worst-case clients’ accuracies."
N,0.6937901498929336,"0
50
100
150
200
250
300
# Rounds 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
N,0.69593147751606,Training Loss (smoothened)
N,0.6980728051391863,Num. of Selected Clients K = 20
N,0.7002141327623126,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.702355460385439,"0
50
100
150
200
250
300
# Rounds 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
N,0.7044967880085653,Testing Accuracy (smoothened)
N,0.7066381156316917,Num. of Selected Clients K = 20
N,0.708779443254818,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.7109207708779444,"0
10
20
30
40
50
60
# Rounds 0.010 0.015 0.020 0.025 0.030 0.035"
N,0.7130620985010707,Accuracy Variance (smoothened)
N,0.715203426124197,Num. of Selected Clients K = 20
N,0.7173447537473233,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.7194860813704497,"0
50
100
150
200
250
300
# Rounds 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
N,0.721627408993576,10th percentile Accuracy (smoothened)
N,0.7237687366167024,Num. of Selected Clients K = 20
N,0.7259100642398287,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.728051391862955,"Figure 6: We measure the training loss, mean and variance of test accuracies as well as 10th
percentile test accuracy for DivFL on synthetic IID dataset for K = 20."
N,0.7301927194860813,"B.2
IMPACT OF NUMBER OF LOCAL EPOCHS τ"
N,0.7323340471092077,"In our experiments, we already adopted local updating schemes with a ﬁxed number of local epochs
(τ) being 1 (running multiple local iterations). However, we further show the robustness of DivFL
under different choices of τ via observing the training loss and variance of test accuracies. The results
for Synthetic IID, non-IID and FEMNIST are presented in Figure 9, 10, 11, 12, 13 for different values
of τ. We observe that for different choices of the number of clients K and the number of local epochs"
N,0.734475374732334,Published as a conference paper at ICLR 2022
N,0.7366167023554604,"0
50
100
150
200
250
300
# Rounds 0.5 1.0 1.5 2.0 2.5 3.0"
N,0.7387580299785867,Training Loss (smoothened)
N,0.7408993576017131,Num. of Selected Clients K = 20
N,0.7430406852248393,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.7451820128479657,"0
50
100
150
200
250
300
# Rounds 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
N,0.7473233404710921,Testing Accuracy (smoothened)
N,0.7494646680942184,Num. of Selected Clients K = 20
N,0.7516059957173448,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.7537473233404711,"0
10
20
30
40
50
60
# Rounds 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
N,0.7558886509635975,Accuracy Variance (smoothened)
N,0.7580299785867237,Num. of Selected Clients K = 20
N,0.7601713062098501,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.7623126338329764,"0
50
100
150
200
250
300
# Rounds 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
N,0.7644539614561028,10th percentile Accuracy (smoothened)
N,0.7665952890792291,Num. of Selected Clients K = 20
N,0.7687366167023555,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.7708779443254818,"Figure 7: The training loss, mean and variance of test accuracies as well as 10th percentile test
accuracy for DivFL on synthetic non-IID dataset for K = 20."
N,0.7730192719486081,"0
50
100
150
200
250
300
350
400
# Rounds 0.5 1.0 1.5 2.0 2.5"
N,0.7751605995717344,Training Loss (smoothened)
N,0.7773019271948608,Num. of Selected Clients K = 20
N,0.7794432548179872,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.7815845824411135,"0
50
100
150
200
250
300
350
400
# Rounds 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
N,0.7837259100642399,Testing Accuracy (smoothened)
N,0.7858672376873662,Num. of Selected Clients K = 20
N,0.7880085653104925,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.7901498929336188,"0
50
100
150
200
250
300
350
400
# Rounds 0.02 0.04 0.06 0.08 0.10"
N,0.7922912205567452,Accuracy Variance (smoothened)
N,0.7944325481798715,Num. of Selected Clients K = 20
N,0.7965738758029979,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.7987152034261242,"0
50
100
150
200
250
300
350
400
# Rounds 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
N,0.8008565310492506,10th percentile Accuracy (smoothened)
N,0.8029978586723768,Num. of Selected Clients K = 20
N,0.8051391862955032,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.8072805139186295,"Figure 8: Training loss, mean and variance of test accuracies as well as 10th percentile test accuracy
for DivFL on FEMNIST dataset for K = 20."
N,0.8094218415417559,"τ, both variants of DivFL converge faster than both the baselines. The fairness gains over random
sampling are also preserved under large values of τ as shown in Figure 13."
N,0.8115631691648822,Published as a conference paper at ICLR 2022
N,0.8137044967880086,"0
50
100
150
200
250
300
# Rounds"
N,0.815845824411135,"0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25"
N,0.8179871520342612,Training Loss
N,0.8201284796573876,"K = 10, Local epochs  =5"
N,0.8222698072805139,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.8244111349036403,"0
50
100
150
200
250
300
# Rounds 0.5 1.0 1.5 2.0"
N,0.8265524625267666,Training Loss
N,0.828693790149893,"K = 10, Local epochs  =10"
N,0.8308351177730193,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.8329764453961456,"0
50
100
150
200
250
300
# Rounds"
N,0.8351177730192719,"0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25"
N,0.8372591006423983,Training Loss
N,0.8394004282655246,"K = 20, Local epochs  =5"
N,0.841541755888651,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.8436830835117773,"0
50
100
150
200
250
300
# Rounds 0.5 1.0 1.5 2.0"
N,0.8458244111349036,Training Loss
N,0.8479657387580299,"K = 20, Local epochs  =10"
N,0.8501070663811563,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.8522483940042827,Figure 9: Training Loss on Synthetic IID for different choices of τ.
N,0.854389721627409,"0
50
100
150
200
250
300
# Rounds 0.010 0.015 0.020 0.025 0.030"
N,0.8565310492505354,Accuracy Variance
N,0.8586723768736617,"K = 10, Local epochs  =5"
N,0.860813704496788,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.8629550321199143,"0
50
100
150
200
250
300
# Rounds 0.005 0.010 0.015 0.020 0.025 0.030 0.035"
N,0.8650963597430407,Accuracy Variance
N,0.867237687366167,"K = 10, Local epochs  =10"
N,0.8693790149892934,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.8715203426124197,"0
50
100
150
200
250
300
# Rounds 0.010 0.015 0.020 0.025 0.030 0.035"
N,0.8736616702355461,Accuracy Variance
N,0.8758029978586723,"K = 20, Local epochs  =5"
N,0.8779443254817987,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.880085653104925,"0
50
100
150
200
250
300
# Rounds 0.005 0.010 0.015 0.020 0.025 0.030 0.035"
N,0.8822269807280514,Accuracy Variance
N,0.8843683083511777,"K = 20, Local epochs  =10"
N,0.8865096359743041,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.8886509635974305,Figure 10: Variance of Test Accuracy on Synthetic IID
N,0.8907922912205567,"0
50
100
150
200
250
300
# Rounds 0.5 1.0 1.5 2.0 2.5 3.0"
N,0.892933618843683,Training Loss
N,0.8950749464668094,"K = 10, Local epochs  =5"
N,0.8972162740899358,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.8993576017130621,"0
50
100
150
200
250
300
# Rounds 0.5 1.0 1.5 2.0 2.5 3.0"
N,0.9014989293361885,Training Loss
N,0.9036402569593148,"K = 10, Local epochs  =10"
N,0.9057815845824411,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.9079229122055674,"0
50
100
150
200
250
300
# Rounds 0.5 1.0 1.5 2.0 2.5 3.0"
N,0.9100642398286938,Training Loss
N,0.9122055674518201,"K = 20, Local epochs  =5"
N,0.9143468950749465,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.9164882226980728,"0
50
100
150
200
250
300
# Rounds 0.5 1.0 1.5 2.0 2.5 3.0"
N,0.9186295503211992,Training Loss
N,0.9207708779443254,"K = 20, Local epochs  =10"
N,0.9229122055674518,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.9250535331905781,Figure 11: Training Loss on Synthetic non-IID for different choices of τ.
N,0.9271948608137045,Published as a conference paper at ICLR 2022
N,0.9293361884368309,"0
50
100
150
200
250
300
# Rounds"
N,0.9314775160599572,"0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16"
N,0.9336188436830836,Accuracy Variance
N,0.9357601713062098,"K = 10, Local epochs  =5"
N,0.9379014989293362,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.9400428265524625,"0
50
100
150
200
250
300
# Rounds"
N,0.9421841541755889,"0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16"
N,0.9443254817987152,Accuracy Variance
N,0.9464668094218416,"K = 10, Local epochs  =10"
N,0.9486081370449678,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.9507494646680942,"0
50
100
150
200
250
300
# Rounds"
N,0.9528907922912205,"0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16"
N,0.9550321199143469,Accuracy Variance
N,0.9571734475374732,"K = 20, Local epochs  =5"
N,0.9593147751605996,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.961456102783726,"0
50
100
150
200
250
300
# Rounds 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16"
N,0.9635974304068522,Accuracy Variance
N,0.9657387580299786,"K = 20, Local epochs  =10"
N,0.9678800856531049,"Random Sampling
Power-of-Choice
DivFL (ideal)
DivFL (no overhead)"
N,0.9700214132762313,Figure 12: Variance of Test Accuracy on Synthetic non-IID
N,0.9721627408993576,"0
50
100
150
200
250
300
350
400
# Rounds 0.5 1.0 1.5 2.0 2.5 3.0"
N,0.974304068522484,Training Loss (smoothened)
N,0.9764453961456103,"K = 10, Local epochs  =5"
N,0.9785867237687366,"Random Sampling
Power-of-Choice
DivFL (no overhead)"
N,0.9807280513918629,"0
50
100
150
200
250
300
350
400
# Rounds 0.2 0.4 0.6 0.8"
N,0.9828693790149893,Testing Accuracy (smoothened)
N,0.9850107066381156,"K = 10, Local epochs  =5"
N,0.987152034261242,"Random Sampling
Power-of-Choice
DivFL (no overhead)"
N,0.9892933618843683,"0
50
100
150
200
250
300
350
400
# Rounds 0.00 0.02 0.04 0.06 0.08 0.10"
N,0.9914346895074947,Accuracy Variance (smoothened)
N,0.9935760171306209,"K = 10, Local epochs  =5"
N,0.9957173447537473,"Random Sampling
Power-of-Choice
DivFL (no overhead)"
N,0.9978586723768736,"Figure 13: Faster convergence beneﬁts of DivFL over Random Sampling and Power-of-Choice
approaches is preserved even when clients run multiple local epochs before sharing model updates."
