Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0004110152075626798,"Despite the vast empirical evidence supporting the efficacy of adaptive optimization
1"
ABSTRACT,0.0008220304151253596,"methods in deep learning, their theoretical understanding is far from complete.
2"
ABSTRACT,0.0012330456226880395,"In this work, we introduce novel SDEs for commonly used adaptive optimizers:
3"
ABSTRACT,0.0016440608302507192,"SignSGD, RMSprop(W), and Adam(W). Our SDEs offer a quantitatively accurate
4"
ABSTRACT,0.002055076037813399,"description of these optimizers and help bring to light an intricate relationship
5"
ABSTRACT,0.002466091245376079,"between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD
6"
ABSTRACT,0.002877106452938759,"highlights a noteworthy and precise contrast to SGD in terms of convergence speed,
7"
ABSTRACT,0.0032881216605014385,"stationary distribution, and robustness to heavy-tail noise. We extend this analysis
8"
ABSTRACT,0.0036991368680641184,"to AdamW and RMSpropW, for which we observe that the role of noise is much
9"
ABSTRACT,0.004110152075626798,"more complex. Crucially, we support our theoretical analysis with experimental
10"
ABSTRACT,0.004521167283189478,"evidence by verifying our insights: this includes numerically integrating our SDEs
11"
ABSTRACT,0.004932182490752158,"using Euler-Maruyama discretization on various neural network architectures such
12"
ABSTRACT,0.005343197698314837,"as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the
13"
ABSTRACT,0.005754212905877518,"behavior of the respective optimizers, especially when compared to previous SDEs
14"
ABSTRACT,0.006165228113440197,"derived for Adam and RMSprop. We believe our approach can provide valuable
15"
ABSTRACT,0.006576243321002877,"insights into best training practices and novel scaling rules.
16"
INTRODUCTION,0.006987258528565557,"1
Introduction
17"
INTRODUCTION,0.007398273736128237,"Adaptive optimizers lay the foundation for effectively training of modern deep learning models.
18"
INTRODUCTION,0.007809288943690916,"These methods are typically employed to optimize an objective function expressed as a sum across N
19"
INTRODUCTION,0.008220304151253596,individual data points: minx∈Rd[f(x) := 1
INTRODUCTION,0.008631319358816275,"N
PN
i=1 fi(x)], where f, fi : Rd →R, i = 1, . . . , N.
20"
INTRODUCTION,0.009042334566378957,"Due to the practical difficulties of selecting the learning rate of stochastic gradient descent, adaptive
21"
INTRODUCTION,0.009453349773941636,"methods have grown in popularity over the past decade. At a high level, these optimizers adjust the
22"
INTRODUCTION,0.009864364981504316,"learning rate for each parameter based on the historical gradients. Popular optimizers that belong to
23"
INTRODUCTION,0.010275380189066995,"this family are RMSprop (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2015), SignSGD
24"
INTRODUCTION,0.010686395396629675,"(Bernstein et al., 2018), AdamW (Loshchilov and Hutter, 2019), and many other variants. SignSGD is
25"
INTRODUCTION,0.011097410604192354,"often used for compressing gradients in distributed machine learning (Karimireddy et al., 2019a), but
26"
INTRODUCTION,0.011508425811755036,"it also has gained popularity due to its connection to RMSprop and Adam (Balles and Hennig, 2018).
27"
INTRODUCTION,0.011919441019317715,"The latter algorithms have emerged as the standard methods for training modern large language
28"
INTRODUCTION,0.012330456226880395,"models, partly because of enhancements in signal propagation (Noci et al., 2022).
29"
INTRODUCTION,0.012741471434443074,"Although adaptive methods are widely favored in practice, their theoretical foundations remain enig-
30"
INTRODUCTION,0.013152486642005754,"matic. Recent research has illuminated some of their advantages: Zhang et al. (2020b) demonstrated
31"
INTRODUCTION,0.013563501849568433,"how gradient clipping addresses heavy-tailed gradient noise, Pan and Li (2022) related the success of
32"
INTRODUCTION,0.013974517057131115,"Adam over SGD to sharpness, and Yang et al. (2024) showed that adaptive methods handle large gra-
33"
INTRODUCTION,0.014385532264693794,"dients better than SGD. At the same time, many optimization studies focus on worst-case convergence
34"
INTRODUCTION,0.014796547472256474,"rates: These rates (e.g., Défossez et al. (2022)) are valuable, yet they provide an incomplete depiction
35"
INTRODUCTION,0.015207562679819153,"of algorithm behavior, showing no quantifiable advantage over standard SGD. One particular aspect
36"
INTRODUCTION,0.015618577887381833,"still lacking clarity is the precise role of noise in the algorithm trajectory.
37"
INTRODUCTION,0.016029593094944512,"Our investigation aims to study how gradient noise influences the dynamics of adaptive optimizers
38"
INTRODUCTION,0.016440608302507192,"and how it impacts their asymptotic behaviors in terms of expected loss and stationary distribution. In
39"
INTRODUCTION,0.01685162351006987,"particular, we want to understand which algorithms are more resilient to high (possibly heavy-tailed)
40"
INTRODUCTION,0.01726263871763255,"gradient noise levels. To do this, we rely on stochastic differential equations (SDEs) which have
41"
INTRODUCTION,0.017673653925195234,"become popular in the literature to study the behavior of optimization algorithms (Li et al., 2017;
42"
INTRODUCTION,0.018084669132757913,"Jastrzebski et al., 2018). These continuous-time models unlock powerful tools from Itô calculus,
43"
INTRODUCTION,0.018495684340320593,"enabling us to establish convergence bounds, determine stationary distributions, unveil implicit
44"
INTRODUCTION,0.018906699547883273,"regularization, and elucidate the intricate interplay between landscape and noise. Notably, SDEs
45"
INTRODUCTION,0.019317714755445952,"facilitate direct comparisons between optimizers by explicitly illustrating how each hyperparameter
46"
INTRODUCTION,0.01972872996300863,"and certain landscape features influence their dynamics (Compagnoni et al., 2024).
47"
INTRODUCTION,0.02013974517057131,"We begin by analyzing SignSGD, showing how the signal-to-noise ratio affects its dynamics and
48"
INTRODUCTION,0.02055076037813399,"elucidating the impact of noise at convergence. After analyzing the case where the gradient noise
49"
INTRODUCTION,0.02096177558569667,"exhibits infinite variance, we extend our analysis to Adam and RMSprop with decoupled weight
50"
INTRODUCTION,0.02137279079325935,"decay (Loshchilov and Hutter, 2019) – i.e. AdamW and RMSpropW: for both, we refine batch size
51"
INTRODUCTION,0.02178380600082203,"scaling rules and compare the role of noise to SignSGD. Our analysis provides some theoretical
52"
INTRODUCTION,0.02219482120838471,"grounding for the resilience of these adaptive methods to high noise levels. Importantly, we highlight
53"
INTRODUCTION,0.02260583641594739,"that Adam and RMSprop are byproducts of our analysis and that our novel SDEs are derived under
54"
INTRODUCTION,0.02301685162351007,"much weaker and more realistic assumptions than those in the literature (Malladi et al., 2022).
55"
INTRODUCTION,0.02342786683107275,"Contributions
We identify our key contributions as follows:
56"
INTRODUCTION,0.02383888203863543,"1. We derive the first SDE for SignSGD under very general assumptions: We show that SignSGD
57"
INTRODUCTION,0.02424989724619811,"exhibits three different phases of the dynamics and characterize the loss behavior in these phases,
58"
INTRODUCTION,0.02466091245376079,"including the stationary distribution and asymptotic loss value.
59"
INTRODUCTION,0.02507192766132347,"2. We demonstrate that for SignSGD, noise inversely affects the convergence rate of both the loss and
60"
INTRODUCTION,0.02548294286888615,"the iterates. Differently, it has a linear impact on the asymptotic expected loss and the asymptotic
61"
INTRODUCTION,0.025893958076448828,"variance of the iterates. This is in contrast to SGD, where noise does not influence the convergence
62"
INTRODUCTION,0.026304973284011508,"speed, but it has a quadratic effect on the loss and variance of the iterates. Finally, we show
63"
INTRODUCTION,0.026715988491574187,"that, even if the noise has infinite variance, SignSGD is very resilient: its performance is only
64"
INTRODUCTION,0.027127003699136867,"marginally impacted. In the same conditions, SGD would diverge.
65"
INTRODUCTION,0.027538018906699546,"3. We derive new, improved, SDEs for AdamW and RMSpropW and use them to (1) show a novel
66"
INTRODUCTION,0.02794903411426223,"batch size scaling rule and (2) inspect the stationary distribution and stationary loss value in
67"
INTRODUCTION,0.02836004932182491,"convex quadratics. In particular, we dive into the properties of weight decay: while for vanilla
68"
INTRODUCTION,0.02877106452938759,"Adam and RMSprop the effect of noise at convergence mimics SignSGD, something different
69"
INTRODUCTION,0.029182079736950268,"happens in AdamW and RMSpropW — Due to an intricate interaction between noise, curvature,
70"
INTRODUCTION,0.029593094944512947,"and regularization, weight decay plays a crucial stabilization role at high noise levels near the
71"
INTRODUCTION,0.030004110152075627,"minimizer.
72"
INTRODUCTION,0.030415125359638306,"4. We empirically verify every theoretical insight we derive. Importantly, we integrate our SDEs
73"
INTRODUCTION,0.030826140567200986,"with Euler-Maruyama to confirm that our SDEs faithfully track their respective optimizers. We do
74"
INTRODUCTION,0.031237155774763666,"so on an MLP, a CNN, a ResNet, and a Transformer. For RMSprop and Adam, our SDEs exhibit
75"
INTRODUCTION,0.03164817098232635,"superior modeling power than the SDEs already existing in the literature.
76"
RELATED WORK,0.032059186189889025,"2
Related work
77"
RELATED WORK,0.03247020139745171,"SDE approximations and applications.
(Li et al., 2017) introduced a formal theoretical framework
78"
RELATED WORK,0.032881216605014384,"aimed at deriving SDEs that effectively model the inherent stochastic nature of optimizers. Ever since,
79"
RELATED WORK,0.03329223181257707,"SDEs have found several applications in the field of machine learning, for instance in connection
80"
RELATED WORK,0.03370324702013974,"with stochastic optimal control to select the stepsize (Li et al., 2017, 2019) and batch size (Zhao
81"
RELATED WORK,0.034114262227702426,"et al., 2022), the derivation of convergence bounds and stationary distributions (Compagnoni et al.,
82"
RELATED WORK,0.0345252774352651,"2023, 2024), implicit regularization (Smith et al., 2021), and scaling rules (Jastrzebski et al., 2018).
83"
RELATED WORK,0.034936292642827785,"Previous work by Malladi et al. (2022) has already made strides in deriving SDE models for RMSprop
84"
RELATED WORK,0.03534730785039047,"and Adam, albeit under certain restrictive assumptions. They establish a scaling rule which they
85"
RELATED WORK,0.035758323057953144,"assert remains valid throughout the entirety of the dynamics. Unfortunately, their derivation is based
86"
RELATED WORK,0.03616933826551583,"on the approach of Jastrzebski et al. (2018) which is problematic in the general case (See Appendix
87"
RELATED WORK,0.0365803534730785,"E for a detailed discussion). Indeed, we demonstrate that the SDEs derived in Malladi et al. (2022)
88"
RELATED WORK,0.036991368680641186,"are only accurate around minima, indicating that their scaling rule is not globally valid. (Zhou et al.,
89"
RELATED WORK,0.03740238388820386,"2020a) also claimed to have derived a Lévy SDE for Adam. Unfortunately, the quality of their
90"
RELATED WORK,0.037813399095766545,"SDE approximation does not come with theoretical guarantees. Additionally, their SDE has random
91"
RELATED WORK,0.03822441430332922,"coefficients: an approach which is theoretically sound in very limited settings (Kohatsu-Higa et al.,
92"
RELATED WORK,0.038635429510891904,"1997; Bishop and Del Moral, 2019). Zhou et al. (2024) informally presented an SDE for (only) the
93"
RELATED WORK,0.03904644471845458,"parameters of AdamW: this is achieved under strong assumptions and various approximations, some
94"
RELATED WORK,0.03945745992601726,"of which are hard to motivate formally.
95"
RELATED WORK,0.03986847513357994,"Influence of noise on convergence.
Several empirical papers demonstrate that adaptive algorithms
96"
RELATED WORK,0.04027949034114262,"adjust better to the noise during training. Specifically, (Zhang et al., 2020b) noticed a consistent gap
97"
RELATED WORK,0.040690505548705305,"in the performance of SGD and Adam on language models and connected that phenomenon with
98"
RELATED WORK,0.04110152075626798,"heavy-tailed noise distributions. (Pascanu et al., 2013) suggests using gradient clipping to deal with
99"
RELATED WORK,0.041512535963830664,"heavy tail noise, and consequently several follow-up works analyzed clipped SGD under heavy-tailed
100"
RELATED WORK,0.04192355117139334,"noise (Zhang et al., 2020a; Mai and Johansson, 2021; Puchkin et al., 2024). Kunstner et al. (2024)
101"
RELATED WORK,0.04233456637895602,"present thorough numerical experiments illustrating that a significant contributor to heavy-tailed noise
102"
RELATED WORK,0.0427455815865187,"during language model training is class imbalance, where certain words occur much more frequently
103"
RELATED WORK,0.04315659679408138,"than others. They demonstrate that adaptive optimization methods such as Adam and SignSGD can
104"
RELATED WORK,0.04356761200164406,"better adapt to such class imbalances. However, the theoretical understanding of the influence of
105"
RELATED WORK,0.04397862720920674,"noise in the context of adaptive algorithms is much more limited. The first convergence results on
106"
RELATED WORK,0.04438964241676942,"Adam and RMSprop were derived under bounded stochastic gradients assumption (De et al., 2018;
107"
RELATED WORK,0.0448006576243321,"Zaheer et al., 2018; Chen et al., 2019; Défossez et al., 2022). Later, this noise model was relaxed
108"
RELATED WORK,0.04521167283189478,"to weak growth condition (Zhang et al., 2022; Wang et al., 2022) and its coordinate-wise version
109"
RELATED WORK,0.04562268803945746,"(Hong and Lin, 2023; Wang et al., 2024) and sub-gaussian noise (Li et al., 2023a). SignSGD and
110"
RELATED WORK,0.04603370324702014,"its momentum version Signum were originally studied as a method for compressed communication
111"
RELATED WORK,0.04644471845458282,"(Bernstein et al., 2018) under bounded variance assumption, but with a requirement of large batches.
112"
RELATED WORK,0.0468557336621455,"Several works provided counterexamples where SignSGD fails to converge if stochastic and full
113"
RELATED WORK,0.04726674886970818,"gradients are not correlated enough (Karimireddy et al., 2019b; Safaryan and Richtarik, 2021). In
114"
RELATED WORK,0.04767776407727086,"the case of AdamW, (Zhou et al., 2022, 2024) provide convergence guarantees under restrictive
115"
RELATED WORK,0.04808877928483354,"assumptions such as bounded gradient and bounded noise. All aforementioned results only show
116"
RELATED WORK,0.04849979449239622,"that SignSGD, Adam, and RMSprop at least do not perform worse than vanilla SGD. None of them
117"
RELATED WORK,0.048910809699958896,"studied how noise affects the dynamics of the algorithm: In this work, we attempt to close this gap.
118"
RELATED WORK,0.04932182490752158,"3
Formal statements & insights: the SDEs
119"
RELATED WORK,0.049732840115084255,"This section provides the general formulations of the SDEs of SignSGD (Theorem 3.2) and AdamW
120"
RELATED WORK,0.05014385532264694,"(Theorem 3.12). Due to the technical nature of the analysis, we refer the reader to the appendix for
121"
RELATED WORK,0.05055487053020962,"the complete formal statements and proofs.
122"
RELATED WORK,0.0509658857377723,"Assumptions and notation.
In this section, we assume that ∇fγ(x) = ∇f(x)+Z(x), E[Z(x)] = 0
123"
RELATED WORK,0.05137690094533498,"and, unless we study the cases where the gradient variance is unbounded, we write Cov(Z(x)) =
124"
RELATED WORK,0.051787916152897656,"Σ(x) where we omit the batch size unless relevant. To derive the stationary distribution around an
125"
RELATED WORK,0.05219893136046034,"optimum, we will approximate the loss function with a quadratic convex function f(x) = 1"
RELATED WORK,0.052609946568023015,"2x⊤Hx
126"
RELATED WORK,0.0530209617755857,"as commonly done in the literature (Ge et al., 2015; Levy, 2016; Jin et al., 2017; Poggio et al.,
127"
RELATED WORK,0.053431976983148374,"2017; Mandt et al., 2017; Compagnoni et al., 2023). Regarding the notation, η > 0 is the step
128"
RELATED WORK,0.05384299219071106,"size, the mini-batches {γk} are of size B ≥1 and modeled as i.i.d. random variables uniformly
129"
RELATED WORK,0.05425400739827373,"distributed on {1, . . . , N}. The β parameters refer to momentum parameters, γ > 0 is the (decoupled)
130"
RELATED WORK,0.054665022605836416,"L2-regularization parameter, and ϵ > 0 is a small scalar used for numerical stability.
131"
RELATED WORK,0.05507603781339909,"The following definition formalizes the idea that an SDE can be a “good model” to describe an
132"
RELATED WORK,0.055487053020961775,"optimizer. It is drawn from the field of numerical analysis of SDEs (see Mil’shtein (1986)) and it
133"
RELATED WORK,0.05589806822852446,"quantifies the disparity between the discrete and the continuous processes.
134"
RELATED WORK,0.056309083436087135,"Definition 3.1 (Weak Approximation). A continuous-time stochastic process {Xt}t∈[0,T ] is an order
135"
RELATED WORK,0.05672009864364982,"α weak approximation (or α-order SDE) of a discrete stochastic process {xk}⌊T/η⌋
k=0
if for every
136"
RELATED WORK,0.057131113851212494,"polynomial growth function g, there exists a positive constant C, independent of the stepsize η, such
137"
RELATED WORK,0.05754212905877518,"that maxk=0,...,⌊T/η⌋|Eg (xk) −Eg (Xkη)| ≤Cηα.
138"
SIGNSGD SDE,0.05795314426633785,"3.1
SignSGD SDE
139"
SIGNSGD SDE,0.058364159473900536,"In this section, we derive an SDE model for SignSGD, which we believe to be a novel addition to
140"
SIGNSGD SDE,0.05877517468146321,"the existing literature. This derivation will reveal the unique manner in which noise influences the
141"
SIGNSGD SDE,0.059186189889025895,"dynamics of SignSGD. First, we recall the update equation of SignSGD:
142"
SIGNSGD SDE,0.05959720509658857,"xk+1 = xk −ηsign (∇fγk(xk)) .
(1)"
SIGNSGD SDE,0.060008220304151254,"1
10000
20000
30000
40000
50000
Iterations 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Loss"
SIGNSGD SDE,0.06041923551171394,Losses - DNN
SIGNSGD SDE,0.06083025071927661,"SignSGD
SDE"
SIGNSGD SDE,0.061241265926839296,"1
8000
16000
24000
32000
40000
Iterations 0.0 0.5 1.0 1.5 2.0 2.5 Loss"
SIGNSGD SDE,0.06165228113440197,Losses - CNN
SIGNSGD SDE,0.062063296341964655,"SignSGD
SDE"
SIGNSGD SDE,0.06247431154952733,"1
1000
2000
3000
4000
5000
Iterations 0.0 0.5 1.0 1.5 2.0 Loss"
SIGNSGD SDE,0.06288532675709001,Losses - Transformer
SIGNSGD SDE,0.0632963419646527,"SignSGD
SDE"
SIGNSGD SDE,0.06370735717221537,"1
1000
2000
3000
4000
5000
Iterations 0.5 1.0 1.5 2.0 2.5 Loss"
SIGNSGD SDE,0.06411837237977805,Losses - ResNet
SIGNSGD SDE,0.06452938758734073,"SignSGD
SDE"
SIGNSGD SDE,0.06494040279490342,"Figure 1: Comparison of SignSGD and its SDE in terms of f(x): Our SDE successfully tracks the
dynamics of SignSGD on several architectures: DNN on the Breast Cancer dataset (Left); CNN on
MNIST (Center-Left); Transformer on MNIST (Center-Right); ResNet on CIFAR-10 (Right)."
SIGNSGD SDE,0.06535141800246609,"The following theorem derives a formal continuous-time model for SignSGD.
143"
SIGNSGD SDE,0.06576243321002877,"Theorem 3.2 (Informal Statement of Theorem C.5). Under sufficient regularity conditions, the
144"
SIGNSGD SDE,0.06617344841759146,"solution of the following SDE is an order 1 weak approximation of the discrete update of SignSGD:
145"
SIGNSGD SDE,0.06658446362515413,"dXt = −(1 −2P(∇fγ(Xt) < 0))dt + √η
q"
SIGNSGD SDE,0.06699547883271681,"¯Σ(Xt)dWt,
(2)"
SIGNSGD SDE,0.06740649404027949,"where ¯Σ(x) is the noise covariance ¯Σ(x) = E[ξγ(x)ξγ(x)⊤] and ξγ(x) := sign(∇fγ(x)) −1 +
146"
SIGNSGD SDE,0.06781750924784218,"2P(∇fγ(x) < 0) the noise in the sample sign (∇fγ(x)).
147"
SIGNSGD SDE,0.06822852445540485,"For didactic reasons, we next present a corollary of Theorem 3.2 that provides a more interpretable
148"
SIGNSGD SDE,0.06863953966296753,"SDE. Figure 1 shows the empirical validation of this model for various neural network classes: All
149"
SIGNSGD SDE,0.0690505548705302,"details are presented in Appendix F.
150"
SIGNSGD SDE,0.0694615700780929,"Corollary 3.3 (Informal Statement of Corollary C.7). Under the assumptions of Theorem 3.2, and
151"
SIGNSGD SDE,0.06987258528565557,"that the stochastic gradient is ∇fγ(x) = ∇f(x)+Z such that Z ∼N(0, Σ), Σ = diag(σ2
1, · · · , σ2
d),
152"
SIGNSGD SDE,0.07028360049321825,"the following SDE provides a 1 weak approximation of the discrete update of SignSGD
153"
SIGNSGD SDE,0.07069461570078094,dXt = −Erf Σ−1
SIGNSGD SDE,0.07110563090834361,"2 ∇f(Xt)
√ 2 !"
SIGNSGD SDE,0.07151664611590629,dt + √η
SIGNSGD SDE,0.07192766132346896,"v
u
u
tId −diag  Erf Σ−1"
SIGNSGD SDE,0.07233867653103165,"2 ∇f(Xt)
√ 2 !!2"
SIGNSGD SDE,0.07274969173859433,"dWt,
(3)"
SIGNSGD SDE,0.073160706946157,"where the error function Erf(x) and the square are applied component-wise.
154"
SIGNSGD SDE,0.07357172215371968,"While Eq. (3) may appear intricate at first glance, it becomes apparent upon closer inspection that
155"
SIGNSGD SDE,0.07398273736128237,"the properties of the Erf(·) function enable a detailed exploration of the dynamics of SignSGD. In
156"
SIGNSGD SDE,0.07439375256884505,"particular, we demonstrate that the dynamics of SignSGD can be categorized into three distinct
157"
SIGNSGD SDE,0.07480476777640772,"phases. The left of Figure 2 empirically verifies this result on a convex quadratic function.
158"
SIGNSGD SDE,0.0752157829839704,Lemma 3.4. Under the assumptions of Corollary 3.3 and signal-to-noise ratio Yt := Σ−1
SIGNSGD SDE,0.07562679819153309,"2 ∇f(Xt)
√"
SIGNSGD SDE,0.07603781339909577,"2
,
159"
SIGNSGD SDE,0.07644882860665844,1. Phase 1: If |Yt| > 3
SIGNSGD SDE,0.07685984381422113,"2, the SDE coincides with the ODE of SignGD:
160"
SIGNSGD SDE,0.07727085902178381,"dXt = −sign(∇f(Xt))dt;
(4)"
SIGNSGD SDE,0.07768187422934648,2. Phase 2: If 1 < |Yt| < 3
SIGNSGD SDE,0.07809288943690916,"2:1
161"
SIGNSGD SDE,0.07850390464447185,(a) mYt + q−≤dE[Xt]
SIGNSGD SDE,0.07891491985203453,"dt
≤mYt + q+;
162"
SIGNSGD SDE,0.0793259350595972,"(b) For any a > 0, P

∥Xt −E [Xt]∥2
2 > a

≤η"
SIGNSGD SDE,0.07973695026715988,"a
 
d −∥mYt + q−∥2
2

;
163"
SIGNSGD SDE,0.08014796547472257,"3. Phase 3: If |Yt| < 1, the SDE is
164"
SIGNSGD SDE,0.08055898068228524,dXt = − r
SIGNSGD SDE,0.08096999588984792,"2
π Σ−1"
SIGNSGD SDE,0.08138101109741061,2 ∇f(Xt)dt + √η r Id −2
SIGNSGD SDE,0.08179202630497329,"π diag

Σ−1"
SIGNSGD SDE,0.08220304151253596,"2 ∇f(Xt)
2
dWt.
(5)"
SIGNSGD SDE,0.08261405672009864,"1Let m and q1 are the slope and intercept of the line secant to the graph of Erf(x) between the points
(1, Erf(1)) and
  3"
SIGNSGD SDE,0.08302507192766133,"2, Erf
  3"
SIGNSGD SDE,0.083436087135224,"2
, while q2 is the intercept of the line tangent to the graph of Erf(x) and slope m,"
SIGNSGD SDE,0.08384710234278668,"(q+)i :=
q2
if ∂if(x) > 0
−q1
if ∂if(x) < 0 , (q−)i :=
q1
if ∂if(x) > 0
−q2
if ∂if(x) < 0 , and ˆq := max(q1, q2)."
SIGNSGD SDE,0.08425811755034936,"0.00
0.05
0.10
0.15
0.20
0.25
X1 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 X2"
SIGNSGD SDE,0.08466913275791205,Trajectories
SIGNSGD SDE,0.08508014796547472,"SignSGD
SDE (Full)
ODE (Phase 1)
SDE (Phase 3) 0.00 0.04 0.08 0.12 0.16 0.20 0.24 0.28"
SIGNSGD SDE,0.0854911631730374,"0
500
1000
1500
2000
2500
3000
Iterations 10
4 10
3 10
2 10
1 100 Loss"
SIGNSGD SDE,0.08590217838060009,Losses
SIGNSGD SDE,0.08631319358816276,"SignSGD
SDE
Bound (Phase 1)
Bound (Phase 3)"
SIGNSGD SDE,0.08672420879572544,0.000 0.002 0.004 [X1]
SIGNSGD SDE,0.08713522400328812,0.0000
SIGNSGD SDE,0.08754623921085081,0.0005
SIGNSGD SDE,0.08795725441841348,0.0010
SIGNSGD SDE,0.08836826962597616,0.0015
SIGNSGD SDE,0.08877928483353884,0.0020
SIGNSGD SDE,0.08919030004110153,0.0025 [X2]
SIGNSGD SDE,0.0896013152486642,"Empirical
Theor. Pred."
SIGNSGD SDE,0.09001233045622688,"0.0
2.5
5.0
Var[X1]1e
5 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
SIGNSGD SDE,0.09042334566378955,Var[X2]
E,0.09083436087135224,"1e
5"
E,0.09124537607891492,"Empirical
Theor. Pred."
E,0.0916563912864776,"102
103
104
105
106
Iterations 10
10 10
9 10
8 10
7 10
6 10
5 10
4 Loss"
E,0.09206740649404029,Losses
E,0.09247842170160296,SignSGD = 0.1
E,0.09288943690916564,SignSGD = 0.5
E,0.09330045211672831,SignSGD = 1.5
E,0.093711467324291,"Bound = 0.1
Bound = 0.5"
E,0.09412248253185368,Bound = 1.5
E,0.09453349773941636,"Figure 2: Phases of SignSGD: The ODE of Phase 1 and the SDE of Phase 3 overlap with the “Full”
SDE as per Lemma 3.4 (Left); Phases of the Loss: The bounds derived in Lemma 3.5 for the loss
during Phase 1 and Phase 3 correctly track the loss evolution (Center-Left); The dynamics of the
moments of Xt predicted in Lemma 3.7 track the empirical ones (Center-Right); If the schedulers
satisfy the condition in Lemma 3.9, the loss decays to 0 as prescribed. Otherwise, the loss does not
converge to 0 (Right)."
E,0.09494451294697903,"Remark: The behavior of SignSGD depends on the size of the signal-to-noise ratio. In particular, the
165"
E,0.09535552815454172,"SDE itself shows that in Phase 3, the inverse of the scale of the noise Σ−1"
E,0.0957665433621044,"2 premultiplies the gradient,
166"
E,0.09617755856966707,"thus affecting the rate of descent. This is not the case for SGD where Σ only influences the diffusion
167"
E,0.09658857377722976,"term.2 To better understand the role of the noise, we need to study how it affects the dynamics of the
168"
E,0.09699958898479244,"loss and compare it with SGD.
169"
E,0.09741060419235512,"Lemma 3.5. Let f be µ-strongly convex, Tr(∇2f(x)) ≤Lτ, and St := f(Xt) −f(X∗). Then,
170"
E,0.09782161939991779,"during
171"
E,0.09823263460748048,"1. Phase 1, the loss will reach 0 before t∗= 2
q S0"
E,0.09864364981504316,µ because St ≤1
E,0.09905466502260583,"4
 √µt −2√S0
2;
172"
E,0.09946568023016851,"2. Phase 2 with ∆:=

m
√"
E,0.0998766954377312,2σmax + ηµm2
E,0.10028771064529388,4σ2max
E,0.10069872585285655,"
: E[St] ≤S0e−2µ∆t + η"
E,0.10110974106041924,"2
(Lτ −µdˆq2)"
E,0.10152075626798192,"2µ∆
 
1 −e−2µ∆t
;
173"
E,0.1019317714755446,"3. Phase 3 with ∆:=
q"
E,0.10234278668310727,"2
π
1
σmax + η"
E,0.10275380189066996,"π
µ
σ2max"
E,0.10316481709823264,"
: E[St] ≤S0e−2µ∆t + η"
E,0.10357583230579531,"2
Lτ
2µ∆
 
1 −e−2µ∆t
.
174"
E,0.10398684751335799,"In Phase 1, the signal-to-noise ratio is large, meaning that SignSGD behaves like SignGD: Consistently
175"
E,0.10439786272092068,"with the analysis of SignGD in (Ma et al., 2022), this explains the fast initial convergence of the
176"
E,0.10480887792848335,"optimizer as well as of RMSprop and Adam. In this phase, the loss undergoes a steady decrease
177"
E,0.10521989313604603,"which ensures the emergence of Phase 2 which in turn triggers that of Phase 3 which is characterized
178"
E,0.10563090834360872,"by an exponential decay to an asymptotic loss level: As a practical example, we verify the dynamics
179"
E,0.1060419235511714,"of the expected loss around a minimum in the center-left of Figure 2.
180"
E,0.10645293875873407,"Lemma 3.6. For SGD, the expected loss satisfies: E[St] ≤S0e−2µt + η"
E,0.10686395396629675,"2
Lτ σ2
max
2µ
 
1 −e−2µt
.
181"
E,0.10727496917385944,"Remark: The two key observations are that:
182"
E,0.10768598438142211,"1. Both in Phase 2 and Phase 3, the noise level σmax inversely affects the exponential conver-
183"
E,0.10809699958898479,"gence speed, while this trend is not observed with SGD;
184"
E,0.10850801479654747,"2. The asymptotic loss of SignSGD is (almost) linear in σmax while that of SGD is quadratic.
185"
E,0.10891903000411016,"Additionally, we characterize the stationary distribution of SignSGD around a minimum: Empirical
186"
E,0.10933004521167283,"validation is provided in the center-right of Figure 2.
187"
E,0.10974106041923551,"Lemma 3.7. Let H = diag(λ1, . . . , λd) and Mt := e
−2
√"
E,0.11015207562679818,"2
π Σ−1"
E,0.11056309083436087,2 H+ η π Σ−1
E,0.11097410604192355,"2 H2
t. Then,
188"
E,0.11138512124948623,1. E [Xt] = e−√
E,0.11179613645704892,"2
π Σ−1"
E,0.11220715166461159,"2 HtX0
t→∞
→0;
189"
E,0.11261816687217427,"2. Cov [Xt] =

Mt −e−2√"
E,0.11302918207973695,"2
π Σ−1"
E,0.11344019728729964,"2 Ht
X2
0 + η 2
q"
E,0.11385121249486231,"2
πId + η"
E,0.11426222770242499,"πH
−1
H−1Σ
1
2 (Id −Mt) ,
190"
E,0.11467324290998766,"which as t →∞converges to η 2
q"
E,0.11508425811755035,"2
πId + η"
E,0.11549527332511303,"πH
−1
H−1Σ
1
2 .
191"
E,0.1159062885326757,"Lemma 3.8. Under the same assumptions as Lemma 3.7, the stationary distribution for SGD is:
192"
E,0.1163173037402384,"E [Xt] = e−HtX0
t→∞
→0
and
Cov [Xt] = η"
E,0.11672831894780107,"2H−1Σ
 
Id −e−2Ht t→∞
→
η
2H−1Σ.
193"
E,0.11713933415536375,"2Ths SDE of SGD is dXt = −∇f(Xt)dt + √ηΣ
1
2 dWt."
E,0.11755034936292642,"As we observed above, the noise inversely affects the convergence rate of the iterates of SignSGD
194"
E,0.11796136457048911,"while it does not impact that of SGD. Additionally, while both covariance matrices essentially scale
195"
E,0.11837237977805179,"inversely to the hessian, that of SignSGD scales with Σ
1
2 while that of SGD scales with Σ.
196"
E,0.11878339498561447,"We conclude this section by presenting a condition on the step size scheduler that ensures the
197"
E,0.11919441019317714,"asymptotic convergence of the expected loss to 0 in Phase 3. For general schedulers, we characterize
198"
E,0.11960542540073983,"precisely the speed of convergence and the factors influencing it. Empirical validation is provided in
199"
E,0.12001644060830251,"the right of Figure 2 for a convex quadratic.
200"
E,0.12042745581586518,"Lemma 3.9. Under the assumptions of Lemma 3.5, any step size scheduler ηt such that
201 Z ∞"
E,0.12083847102342787,"0
ηsds = ∞and lim
t→∞ηt = 0 =⇒E[f(Xt) −f(X∗)]
t→∞
→≲Lτσmax 4µ rπ"
E,0.12124948623099055,"2 ηt
t→∞
→0.
(6)"
E,0.12166050143855323,"Remark: Under the same conditions, SGD satisfies E[f(Xt) −f(X∗)]
t→∞
→≲Lτ σ2
max
4µ
ηt
t→∞
→0.
202"
E,0.1220715166461159,"Conclusion: As noted in Bernstein et al. (2018), the signal-to-noise ratio is key in determining
the dynamics of SignSGD. Our SDEs help clarify the mechanisms underlying the dynamics of
SignSGD: we show that the effect of noise is radically different from SGD: 1) It affects the rate
of convergence of the iterates, of the covariance of the iterates, and of the expected loss; 2) The
asymptotic loss value and covariance of the iterates scale in Σ
1
2 while for SGD it does so in
Σ. On the one hand, low levels of noise will ensure a faster and steadier loss decrease close to
minima for SignSGD than for SGD. On the other, SGD will converge to much lower loss values.
A symmetric argument holds for high levels of noise, which suggests that SignSGD is more
resilient to high levels of noise.
203"
HEAVY-TAILED NOISE,0.12248253185367859,"3.1.1
Heavy-tailed noise
204"
HEAVY-TAILED NOISE,0.12289354706124127,"Interestingly, we can replicate the efforts above also in case the noise structure is heavy-tailed as it is
205"
HEAVY-TAILED NOISE,0.12330456226880394,"distributed according to a Student’s t distribution. Notably, we derive the SDE for the case where the
206"
HEAVY-TAILED NOISE,0.12371557747636662,"noise has infinite variance and show how little marginal effect this has on the dynamics of SignSGD.
207"
HEAVY-TAILED NOISE,0.12412659268392931,"Lemma 3.10. Under the assumptions of Corollary 3.3 but the noise on the gradients U ∼tν(0, Id)
208"
HEAVY-TAILED NOISE,0.12453760789149199,"where ν ∈Z+: The following SDE is a 1 weak approximation of the discrete update of SignSGD
209"
HEAVY-TAILED NOISE,0.12494862309905466,"dXt = −2Ξ

Σ−1"
HEAVY-TAILED NOISE,0.12535963830661734,"2 ∇f(Xt)

dt + √η r"
HEAVY-TAILED NOISE,0.12577065351418001,"Id −4 diag

Ξ

Σ−1"
HEAVY-TAILED NOISE,0.12618166872174272,"2 ∇f(Xt)
2
dWt,
(7)"
HEAVY-TAILED NOISE,0.1265926839293054,"where Ξ(x) is defined as Ξ(x) := x
Γ( ν+1"
HEAVY-TAILED NOISE,0.12700369913686807,"2 )
√πνΓ( ν"
HEAVY-TAILED NOISE,0.12741471434443075,"2) 2F1

1
2, ν+1 2 ; 3"
HEAVY-TAILED NOISE,0.12782572955199342,2; −x2
HEAVY-TAILED NOISE,0.1282367447595561,"ν

and 2F1 (a, b; c; x) is the hyper-
210"
HEAVY-TAILED NOISE,0.12864775996711877,"geometric function. Above, the function Ξ(x) and the square are applied component-wise.
211"
HEAVY-TAILED NOISE,0.12905877517468145,"We now characterize the dynamics of SignSGD when the noise on the gradient has infinite variance.
212"
HEAVY-TAILED NOISE,0.12946979038224415,"Corollary 3.11. Under the assumptions of Lemma 3.10 and ν = 2, the dynamics in Phase 3 is:
213"
HEAVY-TAILED NOISE,0.12988080558980683,dXt = − r
HEAVY-TAILED NOISE,0.1302918207973695,"1
2Σ−1"
HEAVY-TAILED NOISE,0.13070283600493218,2 ∇f(Xt)dt + √η r Id −1
DIAG,0.13111385121249486,"2 diag

Σ−1"
DIAG,0.13152486642005753,"2 ∇f(Xt)
2
dWt.
(8)"
DIAG,0.1319358816276202,"Conclusion: We observe that the dynamics of SignSGD when the noise is Gaussian (Eq. (5)) and
when the noise is heavy-tailed with unbounded variance (Eq. (8)) are very similar: By comparing
the constants in front of the drift terms Σ−1"
DIAG,0.13234689683518291,"2 ∇f(Xt), they are only ∼10% apart, and the diffusion
coefficients are comparable. Not only do we once more showcase the resilience of SignSGD to
high levels of noise, but in alignment with (Zhang et al., 2020b), we provide theoretical support
to the success of Adam in such a scenario where SGD would diverge.
214"
DIAG,0.1327579120427456,"All the results derived above can be extended to this setting: this is left as an exercise for the reader.
215"
ADAMW SDE,0.13316892725030827,"3.2
AdamW SDE
216"
ADAMW SDE,0.13357994245787094,"In the last subsection, we showcased how SDEs can serve as powerful tools to understand the
217"
ADAMW SDE,0.13399095766543362,"dynamics of the simplest among coordinate-wise adaptive methods: SignSGD. Here, we extend the
218"
ADAMW SDE,0.1344019728729963,"1
0
1
X1 0.6 0.4 0.2 0.0 0.2 0.4 0.6 X2"
ADAMW SDE,0.13481298808055897,Trajectories
ADAMW SDE,0.13522400328812165,"AdamW
AdamW SDE
RMSpropW
RMSpropW SDE 0 3 6 9 12 15 18 21"
ADAMW SDE,0.13563501849568435,"0
10000
20000
30000
40000
50000
Iterations 10
3 10
2 10
1 100 101 Loss"
ADAMW SDE,0.13604603370324703,Losses
ADAMW SDE,0.1364570489108097,"AdamW
AdamW SDE
RMSpropW
RMSpropW SDE"
ADAMW SDE,0.13686806411837238,"2
1
0
1
2
X1 0.6 0.4 0.2 0.0 0.2 0.4 0.6 X2"
ADAMW SDE,0.13727907932593506,Trajectories
ADAMW SDE,0.13769009453349773,"AdamW
AdamW SDE
RMSpropW
RMSpropW SDE 2 1 0 1 2 3 4 5 6"
ADAMW SDE,0.1381011097410604,"0
5000
10000
15000
20000
25000
Iterations 10
3 10
2 10
1 100 Loss"
ADAMW SDE,0.1385121249486231,Losses
ADAMW SDE,0.1389231401561858,"AdamW
AdamW SDE
RMSpropW
RMSpropW SDE"
ADAMW SDE,0.13933415536374846,"Figure 3: The first two images compare the SDEs of AdamW and RMSpropW with the respective
optimizers in terms of trajectories and f(x) for a convex quadratic function while the other two
figures provide a comparison for an embedded saddle. In all cases, we observe good agreements."
ADAMW SDE,0.13974517057131114,"discussion to Adam with decoupled weight decay, i.e. AdamW:
219"
ADAMW SDE,0.14015618577887382,"vk+1 = β2vk + (1 −β2) (∇fγk(xk))2 ,
mk+1 = β1mk + (1 −β1)∇fγk(xk),"
ADAMW SDE,0.1405672009864365,"xk+1 = xk −η
ˆmk+1
p"
ADAMW SDE,0.14097821619399917,"ˆvk+1 + ϵ
−ηγxk,
ˆmk =
mk
1 −βk
1
,
ˆvk =
vk
1 −βk
2
,
(9)"
ADAMW SDE,0.14138923140156187,"which, of course, covers Adam, RMSprop, and RMSpropW depending on the values of γ and β1.
220"
ADAMW SDE,0.14180024660912455,"The following result proves the SDE of AdamW which we validate in Figure 3 for two simple
221"
ADAMW SDE,0.14221126181668722,"landscapes and in Figure 4 for a Transformer and a ResNet.
222"
ADAMW SDE,0.1426222770242499,"Theorem 3.12 (Informal Statement of Theorem C.31). Under sufficient regularity conditions, ρ1 =
223"
ADAMW SDE,0.14303329223181258,"O(η−ζ) s.t. ζ ∈(0, 1), and ρ2 = O(1), the order 1 weak approximation of AdamW is:
224"
ADAMW SDE,0.14344430743937525,dXt = − p
ADAMW SDE,0.14385532264693793,"γ2(t)
γ1(t) P −1
t
(Mt + ηρ1 (∇f (Xt) −Mt))dt −γXtdt
(10)"
ADAMW SDE,0.1442663378545006,"dMt = ρ1 (∇f (Xt) −Mt) dt + √ηρ1Σ1/2 (Xt) dWt
(11)"
ADAMW SDE,0.1446773530620633,"dVt = ρ2
 
(∇f(Xt))2 + diag (Σ (Xt)) −Vt

dt,
(12)"
ADAMW SDE,0.14508836826962598,"where βi = 1 −ηρi ∼1, γi(t) = 1 −e−ρit, and Pt = diag √Vt + ϵ
p"
ADAMW SDE,0.14549938347718866,"γ2(t)Id.
225"
ADAMW SDE,0.14591039868475134,"In contrast to Remark 4.3 of Malladi et al. (2022), which suggests that an SDE for RMSprop and
226"
ADAMW SDE,0.146321413892314,Adam is only viable if σ ≫∥∇f(x)∥and σ ∼1
ADAMW SDE,0.1467324290998767,"η, our derivation that does not need these assumptions:
227"
ADAMW SDE,0.14714344430743936,"See Remark C.25 for a deeper discussion, the implications, and the experimental comparison.
228"
ADAMW SDE,0.14755445951500207,"The following result demonstrates how the asymptotic expected loss of AdamW scales with the noise
229"
ADAMW SDE,0.14796547472256474,"level. Notably, it introduces the first scaling rule for AdamW, extending the one proposed for Adam
230"
ADAMW SDE,0.14837648993012742,"in (Malladi et al., 2022) to include weight decay scaling. It is crucial to understand that, unlike the
231"
ADAMW SDE,0.1487875051376901,"typical approach in the literature (see (Jastrzebski et al., 2018; Malladi et al., 2022)), our objective in
232"
ADAMW SDE,0.14919852034525277,"deriving these rules is not to maintain the dynamics of the optimizers or the SDE unchanged. Instead,
233"
ADAMW SDE,0.14960953555281545,"our goal is to offer a practical strategy for adjusting hyperparameters (e.g., from η to ˜η) to retain
234"
ADAMW SDE,0.15002055076037812,"certain performance metrics or optimizer properties as the batch size increases (e.g., from B to ˜B).
235"
ADAMW SDE,0.1504315659679408,"Therefore, in our upcoming analysis, we aim to derive scaling rules that preserve specific relevant
236"
ADAMW SDE,0.1508425811755035,"aspects of the dynamics, such as the convergence bound on the loss or the speed. For a more detailed
237"
ADAMW SDE,0.15125359638306618,"discussion motivating our approach, see Appendix E.
238"
ADAMW SDE,0.15166461159062886,"Lemma 3.13. If f is µ-strongly convex and L-smooth, Lτ := Tr(∇2f(x)), and (∇f(x))2 = O(η),
239"
ADAMW SDE,0.15207562679819153,"˜η = κη, ˜B = Bδ, and ˜ρi = αiρi, and ˜γ = ξγ, AdamW satisfies
240"
ADAMW SDE,0.1524866420057542,"E[f(Xt) −f(X∗)]
t→∞
≤
ηLτσL"
ADAMW SDE,0.15289765721331688,"2
κ
2µ
√"
ADAMW SDE,0.15330867242087956,"BδL + σξγ(L + µ)
.
(13)"
ADAMW SDE,0.15371968762844226,"We derive the novel scaling rule by 1) Preserving the upper bound, which requires that κ =
√"
ADAMW SDE,0.15413070283600494,"δ and
241"
ADAMW SDE,0.15454171804356762,"ξ = κ; 2) Preserving the relative speed of Mt, Vt and Xt, which requires that ˜βi = 1 −κ2(1 −βi).
242"
ADAMW SDE,0.1549527332511303,"The left of Figure 5 shows the empirical verification of the predicted loss value and scaling rule on
243"
ADAMW SDE,0.15536374845869297,"a convex quadratic function.3 Interestingly, and consistently with Lemma 3.13, such a value is not
244"
ADAMW SDE,0.15577476366625564,"3Table 1 in Appendix F.8 shows that our scaling rule works on DNNs: it confirms that failing to rescale the
weight decay parameter is suboptimal."
ADAMW SDE,0.15618577887381832,"1
400
800
1200
1600
2000
Iterations 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Loss"
ADAMW SDE,0.15659679408138102,Losses - Transformer
ADAMW SDE,0.1570078092889437,"AdamW
SDE"
ADAMW SDE,0.15741882449650638,"100
101
102
103
Iterations 0.0 0.5 1.0 1.5 2.0 2.5 Loss"
ADAMW SDE,0.15782983970406905,Losses - ResNet
ADAMW SDE,0.15824085491163173,"AdamW
SDE"
ADAMW SDE,0.1586518701191944,"1
400
800
1200
1600
2000
Iterations 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Loss"
ADAMW SDE,0.15906288532675708,Losses - Transformer
ADAMW SDE,0.15947390053431976,"RMSpropW
SDE"
ADAMW SDE,0.15988491574188246,"100
101
102
103
Iterations 0 2 4 6 8 10 12 Loss"
ADAMW SDE,0.16029593094944514,Losses - ResNet
ADAMW SDE,0.1607069461570078,"RMSpropW
SDE"
ADAMW SDE,0.1611179613645705,"Figure 4: The first two represent the comparison between AdamW and its SDE in terms of f(x). The
other two do the same for RMSpropW. In both cases, the first is a Transformer on MNIST and the
second a ResNet on CIFAR-10: Our SDEs match the respective optimizers."
ADAMW SDE,0.16152897657213316,"1
4000
8000
12000
16000
20000
Iterations 10
4 10
3 10
2 Loss"
ADAMW SDE,0.16193999177969584,Losses
ADAMW SDE,0.16235100698725852,"AdamW ( = 1)
AdamW R ( = 1)"
ADAMW SDE,0.16276202219482122,"AdamW NR ( = 1)
Theor. Pred. ( = 1)"
ADAMW SDE,0.1631730374023839,AdamW ( = 4)
ADAMW SDE,0.16358405260994657,AdamW R ( = 4)
ADAMW SDE,0.16399506781750925,AdamW NR ( = 4)
ADAMW SDE,0.16440608302507193,Theor. Pred. ( = 4)
ADAMW SDE,0.1648170982326346,"1
4000
8000
12000
16000
20000
Iterations 10
3 10
2 Loss"
ADAMW SDE,0.16522811344019728,Losses
ADAMW SDE,0.16563912864775995,"RMSpropW ( = 1)
RMSpropW R ( = 1)"
ADAMW SDE,0.16605014385532266,RMSpropW NR ( = 1)
ADAMW SDE,0.16646115906288533,"Theor. Pred. ( = 1)
RMSpropW ( = 4)"
ADAMW SDE,0.166872174270448,RMSpropW R ( = 4)
ADAMW SDE,0.16728318947801069,RMSpropW NR ( = 4)
ADAMW SDE,0.16769420468557336,Theor. Pred. ( = 4)
ADAMW SDE,0.16810521989313604,"0
10000
20000
30000
40000
50000
Iterations 10
4 10
2 100 102 Loss"
ADAMW SDE,0.1685162351006987,Losses - AdamW
ADAMW SDE,0.16892725030826142,"1 = 0.999, 
2 = 0.998"
ADAMW SDE,0.1693382655158241,"1 = 0.999, 
2 = 0.996"
ADAMW SDE,0.16974928072338677,"1 = 0.999, 
2 = 0.992"
ADAMW SDE,0.17016029593094945,"1 = 0.9, 
2 = 0.998"
ADAMW SDE,0.17057131113851212,"1 = 0.9, 
2 = 0.996"
ADAMW SDE,0.1709823263460748,"1 = 0.9, 
2 = 0.992"
ADAMW SDE,0.17139334155363747,"0
20000
t 10
7 10
6 10
5 10
4"
ADAMW SDE,0.17180435676120018,Var[X1]
ADAMW SDE,0.17221537196876285,"AdamW
RMSpropW
Theor. Pred."
ADAMW SDE,0.17262638717632553,"0
20000
t 10
7 10
6 10
5 10
4 10
3"
ADAMW SDE,0.1730374023838882,Var[X2]
ADAMW SDE,0.17344841759145088,"AdamW
RMSpropW
Theor. Pred."
ADAMW SDE,0.17385943279901356,"Figure 5: The loss predicted in Lemma 3.13 matches the experimental results on a convex quadratic
function. AdamW is run with regularization parameter γ = 1. AdamW R (AdamW Rescaled) is run
as we apply the scaling rule with κ = 2. AdamW NR (AdamW Not Rescaled) is run as we apply
the scaling rule with κ = 2 on all hyperparameters but γ, which is left unchanged: Our scaling rule
holds, and failing to rescale γ leads the optimizer not to preserve the asymptotic loss level. The same
happens for γ = 4 (Left); The same for RMSpropW (Center-Left); For AdamW, β1 and β2 influence
which basin will attract the dynamics and how fast this will converge, but not the asymptotic loss
level inside the basin (Center-Right). For both AdamW and RMSpropW, the variance at convergence
predicted in Lemma 3.14 matches the experimental results (Right)."
ADAMW SDE,0.17427044800657623,"influenced by the choice of βi: We argue that βi do not impact the asymptotic level of the loss, but
245"
ADAMW SDE,0.1746814632141389,"rather drive the selection of the basin and speed at which AdamW converges to it — The center-right
246"
ADAMW SDE,0.17509247842170161,"of Figure 5 exemplifies this on a simple nonconvex landscape.
247"
ADAMW SDE,0.1755034936292643,"We conclude this section with the stationary distribution of AdamW around a minimum which we
248"
ADAMW SDE,0.17591450883682697,"empirically validate on the right of Figure 5.
249"
ADAMW SDE,0.17632552404438964,"Lemma 3.14. The stationary distribution of AdamW is
250"
ADAMW SDE,0.17673653925195232,"(E[X∞], Cov[X∞]) =

0, η 2"
ADAMW SDE,0.177147554459515,"
Id + γH−1Σ
1
2
−1
H−1Σ
1
2

."
ADAMW SDE,0.17755856966707767,"RMSpropW
We derived the same results for RMSprop(W) and we reported them in Appendix
251"
ADAMW SDE,0.17796958487464037,"C.4: importantly, we validate the SDE in Figure 3 for two simple landscapes and in Figure 4 for a
252"
ADAMW SDE,0.17838060008220305,"Transformer and a ResNet. The results regarding the asymptotic loss level and stationary distributions
253"
ADAMW SDE,0.17879161528976573,"are validated in the center-left and right of Figure 5 for a convex quadratic function.
254"
ADAMW SDE,0.1792026304973284,"Conclusion: While for both SignSGD and Adam the asymptotic loss value and the covariance of
the iterates scale linearly with Σ
1
2 , we observe for AdamW this is more intricate: The interaction
between curvature, noise, and regularization implies that these two quantities are upper-bounded
in Σ
1
2 and increasing Σ to infinity does not lead to their explosion: Weight decay plays a crucial
stabilization role at high noise levels near the minimizer — See Figure 6 for a comparison across
optimizers. Finally, we argue that βi play a key role in selecting the basin and the convergence
speed to the asymptotic loss value rather than impacting the loss value itself.
255"
ADAMW SDE,0.17961364570489108,"4
Experiments: SDE validation
256"
ADAMW SDE,0.18002466091245375,"The point of our experiments is to validate the theoretical results derived from the SDEs. Therefore,
257"
ADAMW SDE,0.18043567612001643,"we first show that our SDEs faithfully represent the dynamics of their respective optimizers. To do
258"
ADAMW SDE,0.1808466913275791,"0
25000 50000 75000 100000"
ADAMW SDE,0.1812577065351418,"Iterations 10
7 10
5 10
3 10
1 101 103 Loss SGD"
ADAMW SDE,0.1816687217427045,"Optim. (
= 10
2)"
ADAMW SDE,0.18207973695026716,"Optim. (
= 10
1)"
ADAMW SDE,0.18249075215782984,"Optim. (
= 1)
Optim. (
= 10)"
ADAMW SDE,0.18290176736539251,"Optim. (
= 103)"
ADAMW SDE,0.1833127825729552,"Limit  (
= 10
2)"
ADAMW SDE,0.18372379778051787,"Limit  (
= 10
1)
Limit  (
= 1)
Limit  (
= 10)"
ADAMW SDE,0.18413481298808057,"Limit  (
= 103)"
ADAMW SDE,0.18454582819564325,"0
25000 50000 75000 100000"
ADAMW SDE,0.18495684340320592,"Iterations 10
6 10
5 10
4 10
3 10
2 10
1 100 Loss"
ADAMW SDE,0.1853678586107686,SignSGD
ADAMW SDE,0.18577887381833127,"0
25000 50000 75000 100000"
ADAMW SDE,0.18618988902589395,"Iterations 10
7 10
6 10
5 10
4 10
3 10
2 10
1 100 Loss Adam"
ADAMW SDE,0.18660090423345663,"0
25000 50000 75000 100000"
ADAMW SDE,0.18701191944101933,"Iterations 10
7 10
6 10
5 10
4 10
3 10
2 10
1 100 Loss AdamW"
ADAMW SDE,0.187422934648582,"Figure 6: For SGD (Left), SignSGD (Center-Left), Adam (Center-Right), and AdamW: For each
optimizer, we plot the loss value on a convex quadratic and compare its asymptotic value with the
limits predicted by our theory. As we take Σ = σ2Id, we confirm that the loss of SGD scales
quadratically in σ (Lemma 3.6), and linearly for SignSGD (Lemma 3.5) and Adam (Lemma 3.13
with γ = 0). For AdamW, the maximum asymptotic loss value is bounded in σ (Lemma 3.13 with
γ > 0). In accordance with the experiments, our theory predicts that adaptive methods are more
resilient to noise."
ADAMW SDE,0.18783394985614468,"so, we integrate the SDEs with Euler-Maruyama (Algorithm 1): This is particularly challenging and
259"
ADAMW SDE,0.18824496506370736,"expensive as one needs to calculate the full gradients of the DNNs at each iteration.4 We present the
260"
ADAMW SDE,0.18865598027127004,"first set of validation experiments on a variety of architectures and datasets: An MLP on the Breast
261"
ADAMW SDE,0.1890669954788327,"Cancer dataset, a CNN and a Transformer on MNIST, and a ResNet on CIFAR-10. All details are in
262"
ADAMW SDE,0.1894780106863954,"Appendix F.
263"
CONCLUSION,0.18988902589395806,"5
Conclusion
264"
CONCLUSION,0.19030004110152077,"We derived the first formal SDE for SignSGD, enabling us to demonstrate its dynamics traversing
265"
CONCLUSION,0.19071105630908344,"three discernible phases. We characterize how the signal-to-noise ratio drives the dynamics of the
266"
CONCLUSION,0.19112207151664612,"loss in each of these phases, and we derive the asymptotic value of the loss function, as well as the
267"
CONCLUSION,0.1915330867242088,"stationary distribution. Regarding the role of noise, we draw a straightforward comparison with
268"
CONCLUSION,0.19194410193177147,"SGD. For SignSGD, the noise level
√"
CONCLUSION,0.19235511713933415,"Σ has an inverse linear effect on the convergence speed of the
269"
CONCLUSION,0.19276613234689682,"loss and the iterates. However, it linearly affects the asymptotic expected loss and the asymptotic
270"
CONCLUSION,0.19317714755445953,"variance of the iterates. In contrast, for SGD, noise does not influence the convergence speed but
271"
CONCLUSION,0.1935881627620222,"has a quadratic impact on the loss level and variance. We also examine the scenario where the noise
272"
CONCLUSION,0.19399917796958488,"has infinite variance and demonstrate the resilience of SignSGD, showing that its performance is
273"
CONCLUSION,0.19441019317714756,"only marginally affected. Finally, we generalize the analysis to include AdamW and RMSpropW.
274"
CONCLUSION,0.19482120838471023,"Specifically, we leverage our novel SDEs to derive the asymptotic value of the loss function, their
275"
CONCLUSION,0.1952322235922729,"stationary distribution on a convex quadratic, and a novel scaling rule. The key insight is that, similarly
276"
CONCLUSION,0.19564323879983558,"to SignSGD, the loss level and covariance matrix of the iterates of Adam and RMSprop scale linearly
277"
CONCLUSION,0.1960542540073983,"in the noise level Σ
1
2 . For AdamW and RMSpropW, the complex interaction of noise, curvature, and
278"
CONCLUSION,0.19646526921496096,"regularization implies that these two quantities are bounded in terms of Σ
1
2 , showing that weight
279"
CONCLUSION,0.19687628442252364,"decay plays a crucial stabilization role at high noise levels near the minimizer. Interestingly, the
280"
CONCLUSION,0.19728729963008632,"SDEs for Adam and RMSprop are straightforward corollary of our general results and were derived
281"
CONCLUSION,0.197698314837649,"under much less restrictive and more realistic assumptions than those in the literature. Finally, we
282"
CONCLUSION,0.19810933004521167,"thoroughly validate all our theoretical results: We compare the dynamics of the various optimizers
283"
CONCLUSION,0.19852034525277434,"with the respective SDEs and find good agreement on simple landscapes and deep neural networks.
284"
CONCLUSION,0.19893136046033702,"For Adam and RMSprop, our SDEs track them better than those derived in (Malladi et al., 2022).
285"
CONCLUSION,0.19934237566789972,"Future work
We believe that our results can be extended to other optimizers commonly used in
286"
CONCLUSION,0.1997533908754624,"practice such as Signum, AdaGrad, AdaMax, and Nadam. Additionally, inspired by the insights
287"
CONCLUSION,0.20016440608302508,"from our SDE analysis, there is potential for designing new optimization algorithms that combine the
288"
CONCLUSION,0.20057542129058775,"strengths of existing methods while mitigating their weaknesses. For example, developing hybrid
289"
CONCLUSION,0.20098643649815043,"optimizers that adaptively switch between different strategies based on the training phase or current
290"
CONCLUSION,0.2013974517057131,"state of the optimization process could offer superior performance.
291"
CONCLUSION,0.20180846691327578,"4Many papers derived SDEs to model optimizers: most of them do not validate them, some do so on quadratic
functions, and Paquette et al. (2021); Compagnoni et al. (2023) do it on NNs: See Appendix A for details."
REFERENCES,0.20221948212083848,"References
292"
REFERENCES,0.20263049732840116,"An, J., Lu, J., and Ying, L. (2020). Stochastic modified equations for the asynchronous stochastic
293"
REFERENCES,0.20304151253596384,"gradient descent. Information and Inference: A Journal of the IMA, 9(4):851–873.
294"
REFERENCES,0.2034525277435265,"Ankirchner, S. and Perko, S. (2024). A comparison of continuous-time approximations to stochastic
295"
REFERENCES,0.2038635429510892,"gradient descent. Journal of Machine Learning Research, 25(13):1–55.
296"
REFERENCES,0.20427455815865186,"Ayadi, I. and Turinici, G. (2021). Stochastic runge-kutta methods and adaptive sgd-g2 stochastic
297"
REFERENCES,0.20468557336621454,"gradient descent. In 2020 25th International Conference on Pattern Recognition (ICPR), pages
298"
REFERENCES,0.20509658857377722,"8220–8227. IEEE.
299"
REFERENCES,0.20550760378133992,"Balles, L. and Hennig, P. (2018). Dissecting adam: The sign, magnitude and variance of stochastic
300"
REFERENCES,0.2059186189889026,"gradients. In International Conference on Machine Learning, pages 404–413. PMLR.
301"
REFERENCES,0.20632963419646527,"Barakat, A. and Bianchi, P. (2021). Convergence and dynamical behavior of the adam algorithm for
302"
REFERENCES,0.20674064940402795,"nonconvex stochastic optimization. SIAM Journal on Optimization, 31(1):244–274.
303"
REFERENCES,0.20715166461159062,"Bardi, M. and Kouhkouh, H. (2022). Deep relaxation of controlled stochastic gradient descent via
304"
REFERENCES,0.2075626798191533,"singular perturbations. arXiv preprint arXiv:2209.05564.
305"
REFERENCES,0.20797369502671598,"Bercher, A., Gonon, L., Jentzen, A., and Salimova, D. (2020). Weak error analysis for stochastic
306"
REFERENCES,0.20838471023427868,"gradient descent optimization algorithms. arXiv preprint arXiv:2007.02723.
307"
REFERENCES,0.20879572544184136,"Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anandkumar, A. (2018). signSGD: Compressed
308"
REFERENCES,0.20920674064940403,"optimisation for non-convex problems. In Proceedings of the 35th International Conference on
309"
REFERENCES,0.2096177558569667,"Machine Learning.
310"
REFERENCES,0.21002877106452938,"Bishop, A. N. and Del Moral, P. (2019). Stability properties of systems of linear stochastic differential
311"
REFERENCES,0.21043978627209206,"equations with random coefficients. SIAM Journal on Control and Optimization, 57(2):1023–1042.
312"
REFERENCES,0.21085080147965474,"Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke,
313"
REFERENCES,0.21126181668721744,"A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. (2018). JAX: composable transformations
314"
REFERENCES,0.21167283189478012,"of Python+NumPy programs.
315"
REFERENCES,0.2120838471023428,"Chen, P., Lu, J., and Xu, L. (2022). Approximation to stochastic variance reduced gradient langevin dy-
316"
REFERENCES,0.21249486230990547,"namics by stochastic delay differential equations. Applied Mathematics & Optimization, 85(2):15.
317"
REFERENCES,0.21290587751746815,"Chen, X., Liu, S., Sun, R., and Hong, M. (2019). On the convergence of a class of adam-type
318"
REFERENCES,0.21331689272503082,"algorithms for non-convex optimization. In International Conference on Learning Representations.
319"
REFERENCES,0.2137279079325935,"Compagnoni, E. M., Biggio, L., Orvieto, A., Proske, F. N., Kersting, H., and Lucchi, A. (2023). An
320"
REFERENCES,0.21413892314015617,"sde for modeling sam: Theory and insights. In International Conference on Machine Learning,
321"
REFERENCES,0.21454993834771888,"pages 25209–25253. PMLR.
322"
REFERENCES,0.21496095355528155,"Compagnoni, E. M., Orvieto, A., Kersting, H., Proske, F., and Lucchi, A. (2024). Sdes for minimax
323"
REFERENCES,0.21537196876284423,"optimization. In International Conference on Artificial Intelligence and Statistics, pages 4834–4842.
324"
REFERENCES,0.2157829839704069,"PMLR.
325"
REFERENCES,0.21619399917796958,"Cui, Z.-X., Fan, Q., and Jia, C. (2020). Momentum methods for stochastic optimization over
326"
REFERENCES,0.21660501438553226,"time-varying directed networks. Signal Processing, 174:107614.
327"
REFERENCES,0.21701602959309493,"Dambrine, M., Dossal, C., Puig, B., and Rondepierre, A. (2024). Stochastic differential equations for
328"
REFERENCES,0.21742704480065764,"modeling first order optimization methods. SIAM Journal on Optimization, 34(2):1402–1426.
329"
REFERENCES,0.2178380600082203,"De, S., Mukherjee, A., and Ullah, E. (2018). Convergence guarantees for rmsprop and adam in
330"
REFERENCES,0.218249075215783,"non-convex optimization and an empirical comparison to nesterov acceleration. arXiv preprint
331"
REFERENCES,0.21866009042334567,"arXiv:1807.06766.
332"
REFERENCES,0.21907110563090834,"Défossez, A., Bottou, L., Bach, F., and Usunier, N. (2022). A simple convergence proof of adam and
333"
REFERENCES,0.21948212083847102,"adagrad. Transactions on Machine Learning Research.
334"
REFERENCES,0.2198931360460337,"Deng, L. (2012). The mnist database of handwritten digit images for machine learning research.
335"
REFERENCES,0.22030415125359637,"IEEE Signal Processing Magazine, 29(6):141–142.
336"
REFERENCES,0.22071516646115907,"Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,
337"
REFERENCES,0.22112618166872175,"M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is
338"
REFERENCES,0.22153719687628443,"worth 16x16 words: Transformers for image recognition at scale. In International Conference on
339"
REFERENCES,0.2219482120838471,"Learning Representations.
340"
REFERENCES,0.22235922729140978,"Dua, D. and Graff, C. (2017). UCI machine learning repository.
341"
REFERENCES,0.22277024249897245,"Fontaine, X., De Bortoli, V., and Durmus, A. (2021). Convergence rates and approximation results
342"
REFERENCES,0.22318125770653513,"for sgd and its continuous-time counterpart. In Conference on Learning Theory, pages 1965–2058.
343"
REFERENCES,0.22359227291409783,"PMLR.
344"
REFERENCES,0.2240032881216605,"Ge, R., Huang, F., Jin, C., and Yuan, Y. (2015). Escaping from saddle points—online stochastic
345"
REFERENCES,0.22441430332922319,"gradient for tensor decomposition. In Conference on Learning Theory, pages 797–842.
346"
REFERENCES,0.22482531853678586,"Gess, B., Kassing, S., and Konarovskyi, V. (2024). Stochastic modified flows, mean-field limits and
347"
REFERENCES,0.22523633374434854,"dynamics of stochastic gradient descent. Journal of Machine Learning Research, 25(30):1–27.
348"
REFERENCES,0.22564734895191121,"Gu, H., Guo, X., and Li, X. (2021). Adversarial training for gradient descent: Analysis through its
349"
REFERENCES,0.2260583641594739,"continuous-time approximation. arXiv preprint arXiv:2105.08037.
350"
REFERENCES,0.2264693793670366,"Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser,
351"
REFERENCES,0.22688039457459927,"E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M.,
352"
REFERENCES,0.22729140978216195,"Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., Gérard-Marchant, P., Sheppard, K., Reddy,
353"
REFERENCES,0.22770242498972462,"T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. (2020). Array programming with
354"
REFERENCES,0.2281134401972873,"NumPy. Nature, 585(7825):357–362.
355"
REFERENCES,0.22852445540484997,"Higham, D. J. (2001). An algorithmic introduction to numerical simulation of stochastic differential
356"
REFERENCES,0.22893547061241265,"equations. SIAM review, 43(3):525–546.
357"
REFERENCES,0.22934648581997533,"Hong, Y. and Lin, J. (2023). High probability convergence of adam under unbounded gradients and
358"
REFERENCES,0.22975750102753803,"affine variance noise. arXiv preprint arXiv:2311.02000.
359"
REFERENCES,0.2301685162351007,"Hu, W., Li, C. J., and Zhou, X. (2019). On the global convergence of continuous–time stochastic
360"
REFERENCES,0.23057953144266338,"heavy–ball method for nonconvex optimization. In 2019 IEEE International Conference on Big
361"
REFERENCES,0.23099054665022606,"Data (Big Data), pages 94–104. IEEE.
362"
REFERENCES,0.23140156185778873,"Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., and Storkey, A. (2018).
363"
REFERENCES,0.2318125770653514,"Three factors influencing minima in sgd. ICANN 2018.
364"
REFERENCES,0.2322235922729141,"Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. (2017). How to escape saddle points
365"
REFERENCES,0.2326346074804768,"efficiently. In International Conference on Machine Learning, pages 1724–1732. PMLR.
366"
REFERENCES,0.23304562268803947,"Karatzas, I. and Shreve, S. (2014). Brownian motion and stochastic calculus, volume 113. springer.
367"
REFERENCES,0.23345663789560214,"Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. (2019a). Error feedback fixes signsgd and
368"
REFERENCES,0.23386765310316482,"other gradient compression schemes. In International Conference on Machine Learning, pages
369"
REFERENCES,0.2342786683107275,"3252–3261. PMLR.
370"
REFERENCES,0.23468968351829017,"Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. (2019b). Error feedback fixes SignSGD
371"
REFERENCES,0.23510069872585285,"and other gradient compression schemes. In Proceedings of the 36th International Conference on
372"
REFERENCES,0.23551171393341552,"Machine Learning.
373"
REFERENCES,0.23592272914097823,"Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In International
374"
REFERENCES,0.2363337443485409,"Conference on Learning Representations.
375"
REFERENCES,0.23674475955610358,"Kohatsu-Higa, A., León, J. A., and Nualart, D. (1997). Stochastic differential equations with random
376"
REFERENCES,0.23715577476366625,"coefficients. Bernoulli, pages 233–245.
377"
REFERENCES,0.23756678997122893,"Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.
378"
REFERENCES,0.2379778051787916,"Toronto, ON, Canada.
379"
REFERENCES,0.23838882038635428,"Kunin, D., Sagastuy-Brena, J., Gillespie, L., Margalit, E., Tanaka, H., Ganguli, S., and Yamins, D. L.
380"
REFERENCES,0.238799835593917,"(2023). The limiting dynamics of sgd: Modified loss, phase-space oscillations, and anomalous
381"
REFERENCES,0.23921085080147966,"diffusion. Neural Computation, 36(1):151–174.
382"
REFERENCES,0.23962186600904234,"Kunstner, F., Yadav, R., Milligan, A., Schmidt, M., and Bietti, A. (2024). Heavy-tailed class
383"
REFERENCES,0.24003288121660502,"imbalance and why adam outperforms gradient descent on language models. arXiv preprint
384"
REFERENCES,0.2404438964241677,"arXiv:2402.19449.
385"
REFERENCES,0.24085491163173037,"Lanconelli, A. and Lauria, C. S. (2022). A note on diffusion limits for stochastic gradient descent.
386"
REFERENCES,0.24126592683929304,"arXiv preprint arXiv:2210.11257.
387"
REFERENCES,0.24167694204685575,"Levy, K. Y. (2016). The power of normalization: Faster evasion of saddle points. arXiv preprint
388"
REFERENCES,0.24208795725441842,"arXiv:1611.04831.
389"
REFERENCES,0.2424989724619811,"Li, H., Rakhlin, A., and Jadbabaie, A. (2023a). Convergence of adam under relaxed assumptions. In
390"
REFERENCES,0.24290998766954378,"Thirty-seventh Conference on Neural Information Processing Systems.
391"
REFERENCES,0.24332100287710645,"Li, L. and Wang, Y. (2022). On uniform-in-time diffusion approximation for stochastic gradient
392"
REFERENCES,0.24373201808466913,"descent. arXiv preprint arXiv:2207.04922.
393"
REFERENCES,0.2441430332922318,"Li, Q., Tai, C., and Weinan, E. (2017). Stochastic modified equations and adaptive stochastic gradient
394"
REFERENCES,0.24455404849979448,"algorithms. In International Conference on Machine Learning, pages 2101–2110. PMLR.
395"
REFERENCES,0.24496506370735718,"Li, Q., Tai, C., and Weinan, E. (2019). Stochastic modified equations and dynamics of stochastic
396"
REFERENCES,0.24537607891491986,"gradient algorithms i: Mathematical foundations. The Journal of Machine Learning Research,
397"
REFERENCES,0.24578709412248254,"20(1):1474–1520.
398"
REFERENCES,0.2461981093300452,"Li, Z., Malladi, S., and Arora, S. (2021). On the validity of modeling SGD with stochastic differential
399"
REFERENCES,0.2466091245376079,"equations (SDEs). In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors,
400"
REFERENCES,0.24702013974517056,"Advances in Neural Information Processing Systems.
401"
REFERENCES,0.24743115495273324,"Li, Z., Wang, Y., and Wang, Z. (2023b). Fast equilibrium of sgd in generic situations. In The Twelfth
402"
REFERENCES,0.24784217016029594,"International Conference on Learning Representations.
403"
REFERENCES,0.24825318536785862,"Liu, T., Chen, Z., Zhou, E., and Zhao, T. (2021). A diffusion approximation theory of momentum
404"
REFERENCES,0.2486642005754213,"stochastic gradient descent in nonconvex optimization. Stochastic Systems.
405"
REFERENCES,0.24907521578298397,"Loshchilov, I. and Hutter, F. (2019). Decoupled weight decay regularization. In International
406"
REFERENCES,0.24948623099054665,"Conference on Learning Representations.
407"
REFERENCES,0.24989724619810932,"Ma, C., Wu, L., and Weinan, E. (2022). A qualitative study of the dynamic behavior for adaptive
408"
REFERENCES,0.25030826140567203,"gradient algorithms. In Mathematical and Scientific Machine Learning, pages 671–692. PMLR.
409"
REFERENCES,0.2507192766132347,"Mai, V. V. and Johansson, M. (2021). Stability and convergence of stochastic gradient clipping:
410"
REFERENCES,0.2511302918207974,"Beyond lipschitz continuity and smoothness. In International Conference on Machine Learning.
411"
REFERENCES,0.25154130702836003,"Malladi, S., Lyu, K., Panigrahi, A., and Arora, S. (2022). On the SDEs and scaling rules for adaptive
412"
REFERENCES,0.25195232223592273,"gradient algorithms. In Advances in Neural Information Processing Systems.
413"
REFERENCES,0.25236333744348544,"Mandt, S., Hoffman, M. D., and Blei, D. M. (2017). Stochastic gradient descent as approximate
414"
REFERENCES,0.2527743526510481,"bayesian inference. JMLR 2017.
415"
REFERENCES,0.2531853678586108,"Mao, X. (2007). Stochastic differential equations and applications. Elsevier.
416"
REFERENCES,0.25359638306617344,"Maulen-Soto, R., Fadili, J., Attouch, H., and Ochs, P. (2024). Stochastic inertial dynamics via time
417"
REFERENCES,0.25400739827373614,"scaling and averaging. arXiv preprint arXiv:2403.16775.
418"
REFERENCES,0.2544184134812988,"Maulén Soto, R. I. (2021). A continuous-time model of stochastic gradient descent: convergence
419"
REFERENCES,0.2548294286888615,"rates and complexities under lojasiewicz inequality. Universidad de Chile.
420"
REFERENCES,0.25524044389642414,"Milstein, G. N. (2013). Numerical integration of stochastic differential equations, volume 313.
421"
REFERENCES,0.25565145910398684,"Springer Science & Business Media.
422"
REFERENCES,0.25606247431154955,"Mil’shtein, G. (1986). Weak approximation of solutions of systems of stochastic differential equations.
423"
REFERENCES,0.2564734895191122,"Theory of Probability & Its Applications, 30(4):750–766.
424"
REFERENCES,0.2568845047266749,"Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. (2022). Signal
425"
REFERENCES,0.25729551993423755,"propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in
426"
REFERENCES,0.25770653514180025,"Neural Information Processing Systems, 35:27198–27211.
427"
REFERENCES,0.2581175503493629,"Øksendal, B. (1990). When is a stochastic integral a time change of a diffusion? Journal of theoretical
428"
REFERENCES,0.2585285655569256,"probability, 3(2):207–226.
429"
REFERENCES,0.2589395807644883,"Pan, Y. and Li, Y. (2022). Toward understanding why adam converges faster than SGD for transform-
430"
REFERENCES,0.25935059597205096,"ers. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop).
431"
REFERENCES,0.25976161117961366,"Paquette, C., Lee, K., Pedregosa, F., and Paquette, E. (2021). Sgd in the large: Average-case analysis,
432"
REFERENCES,0.2601726263871763,"asymptotics, and stepsize criticality. In Conference on Learning Theory, pages 3548–3626. PMLR.
433"
REFERENCES,0.260583641594739,"Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training recurrent neural
434"
REFERENCES,0.26099465680230166,"networks. In International conference on machine learning.
435"
REFERENCES,0.26140567200986436,"Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,
436"
REFERENCES,0.26181668721742707,"Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,
437"
REFERENCES,0.2622277024249897,"M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of
438"
REFERENCES,0.2626387176325524,"Machine Learning Research, 12:2825–2830.
439"
REFERENCES,0.26304973284011507,"Poggio, T., Kawaguchi, K., Liao, Q., Miranda, B., Rosasco, L., Boix, X., Hidary, J., and Mhaskar,
440"
REFERENCES,0.2634607480476778,"H. (2017). Theory of deep learning iii: explaining the non-overfitting puzzle. arXiv preprint
441"
REFERENCES,0.2638717632552404,"arXiv:1801.00173.
442"
REFERENCES,0.2642827784628031,"Puchkin, N., Gorbunov, E., Kutuzov, N., and Gasnikov, A. (2024). Breaking the heavy-tailed noise
443"
REFERENCES,0.26469379367036583,"barrier in stochastic optimization problems. In International Conference on Artificial Intelligence
444"
REFERENCES,0.2651048088779285,"and Statistics.
445"
REFERENCES,0.2655158240854912,"Safaryan, M. and Richtarik, P. (2021). Stochastic sign descent methods: New algorithms and better
446"
REFERENCES,0.26592683929305383,"theory. In Proceedings of the 38th International Conference on Machine Learning.
447"
REFERENCES,0.26633785450061653,"Smith, S. L., Dherin, B., Barrett, D. G. T., and De, S. (2021). On the origin of implicit regularization
448"
REFERENCES,0.2667488697081792,"in stochastic gradient descent. ArXiv, abs/2101.12176.
449"
REFERENCES,0.2671598849157419,"Soto, R. M., Fadili, J., and Attouch, H. (2022). An sde perspective on stochastic convex optimization.
450"
REFERENCES,0.2675709001233046,"arXiv preprint arXiv:2207.02750.
451"
REFERENCES,0.26798191533086724,"Su, L. and Lau, V. K. (2023). Accelerated federated learning over wireless fading channels with
452"
REFERENCES,0.26839293053842994,"adaptive stochastic momentum. IEEE Internet of Things Journal.
453"
REFERENCES,0.2688039457459926,"Sun, J., Yang, Y., Xun, G., and Zhang, A. (2023). Scheduling hyperparameters to improve generaliza-
454"
REFERENCES,0.2692149609535553,"tion: From centralized sgd to asynchronous sgd. ACM Transactions on Knowledge Discovery from
455"
REFERENCES,0.26962597616111794,"Data, 17(2):1–37.
456"
REFERENCES,0.27003699136868065,"Tieleman, T. and Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average
457"
REFERENCES,0.2704480065762433,"of its recent magnitude.
458"
REFERENCES,0.270859021783806,"Van Rossum, G. and Drake, F. L. (2009). Python 3 Reference Manual. CreateSpace, Scotts Valley,
459"
REFERENCES,0.2712700369913687,"CA.
460"
REFERENCES,0.27168105219893135,"Wang, B., Fu, J., Zhang, H., Zheng, N., and Chen, W. (2024). Closing the gap between the upper
461"
REFERENCES,0.27209206740649405,"bound and lower bound of adam’s iteration complexity. Advances in Neural Information Processing
462"
REFERENCES,0.2725030826140567,"Systems, 36.
463"
REFERENCES,0.2729140978216194,"Wang, B., Zhang, Y., Zhang, H., Meng, Q., Ma, Z.-M., Liu, T.-Y., and Chen, W. (2022). Provable
464"
REFERENCES,0.27332511302918205,"adaptivity in adam. arXiv preprint arXiv:2208.09900.
465"
REFERENCES,0.27373612823674476,"Wang, Y. and Wu, S. (2020). Asymptotic analysis via stochastic differential equations of gradient
466"
REFERENCES,0.27414714344430746,"descent algorithms in statistical and computational paradigms. Journal of machine learning
467"
REFERENCES,0.2745581586518701,"research, 21(199):1–103.
468"
REFERENCES,0.2749691738594328,"Wang, Z. and Mao, Y. (2022). Two facets of sde under an information-theoretic lens: Generalization
469"
REFERENCES,0.27538018906699546,"of sgd via training trajectories and via terminal states. arXiv preprint arXiv:2211.10691.
470"
REFERENCES,0.27579120427455817,"Yang, J., Li, X., Fatkhullin, I., and He, N. (2024). Two sides of one coin: the limits of untuned sgd
471"
REFERENCES,0.2762022194821208,"and the power of adaptive methods. Advances in Neural Information Processing Systems, 36.
472"
REFERENCES,0.2766132346896835,"Zaheer, M., Reddi, S., Sachan, D., Kale, S., and Kumar, S. (2018). Adaptive methods for nonconvex
473"
REFERENCES,0.2770242498972462,"optimization. Advances in neural information processing systems, 31.
474"
REFERENCES,0.27743526510480887,"Zhang, J., He, T., Sra, S., and Jadbabaie, A. (2020a). Why gradient clipping accelerates training: A
475"
REFERENCES,0.2778462803123716,"theoretical justification for adaptivity. In International Conference on Learning Representations.
476"
REFERENCES,0.2782572955199342,"Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. (2020b). Why are
477"
REFERENCES,0.2786683107274969,"adaptive methods good for attention models? Advances in Neural Information Processing Systems.
478"
REFERENCES,0.2790793259350596,"Zhang, Y., Chen, C., Shi, N., Sun, R., and Luo, Z.-Q. (2022). Adam can converge without any
479"
REFERENCES,0.2794903411426223,"modification on update rules. Advances in neural information processing systems.
480"
REFERENCES,0.279901356350185,"Zhang, Z., Li, Y., Luo, T., and Xu, Z.-Q. J. (2023). Stochastic modified equations and dynamics of
481"
REFERENCES,0.28031237155774763,"dropout algorithm. arXiv preprint arXiv:2305.15850.
482"
REFERENCES,0.28072338676531033,"Zhao, J., Lucchi, A., Proske, F. N., Orvieto, A., and Kersting, H. (2022). Batch size selection by
483"
REFERENCES,0.281134401972873,"stochastic optimal control. In Has it Trained Yet? NeurIPS 2022 Workshop.
484"
REFERENCES,0.2815454171804357,"Zhou, P., Feng, J., Ma, C., Xiong, C., Hoi, S. C. H., et al. (2020a). Towards theoretically understanding
485"
REFERENCES,0.28195643238799833,"why sgd generalizes better than adam in deep learning. Advances in Neural Information Processing
486"
REFERENCES,0.28236744759556104,"Systems, 33:21285–21296.
487"
REFERENCES,0.28277846280312374,"Zhou, P., Xie, X., Lin, Z., and Yan, S. (2024). Towards understanding convergence and generalization
488"
REFERENCES,0.2831894780106864,"of adamw. IEEE Transactions on Pattern Analysis and Machine Intelligence.
489"
REFERENCES,0.2836004932182491,"Zhou, P., Xie, X., and Shuicheng, Y. (2022). Win: Weight-decay-integrated nesterov accelera-
490"
REFERENCES,0.28401150842581174,"tion for adaptive gradient algorithms. In The Eleventh International Conference on Learning
491"
REFERENCES,0.28442252363337445,"Representations.
492"
REFERENCES,0.2848335388409371,"Zhou, X., Yuan, H., Li, C. J., and Sun, Q. (2020b). Stochastic modified equations for continuous
493"
REFERENCES,0.2852445540484998,"limit of stochastic admm. arXiv preprint arXiv:2003.03532.
494"
REFERENCES,0.28565556925606245,"Zhu, Y. and Ying, L. (2021). A sharp convergence rate for a model equation of the asynchronous
495"
REFERENCES,0.28606658446362515,"stochastic gradient descent. Communications in Mathematical Sciences.
496"
REFERENCES,0.28647759967118785,"A
Additional related works
497"
REFERENCES,0.2868886148787505,"In this section, we list some papers that derived or used SDEs to model optimizers. In particular, we
498"
REFERENCES,0.2872996300863132,"focus on the aspect of empirically verifying the validity of such SDEs in the sense that they indeed
499"
REFERENCES,0.28771064529387586,"track the respective optimizers. We divide these into three categories: Those that did not carry out
500"
REFERENCES,0.28812166050143856,"any type of validation, those that did it on simple landscapes (quadratic functions et similia), and
501"
REFERENCES,0.2885326757090012,"those that did small experiments or neural networks.
502"
REFERENCES,0.2889436909165639,"None of the following papers carried out any experimental validation of the approximating power of
503"
REFERENCES,0.2893547061241266,"the SDEs they derived. Many of them did not even validate the insights derived from the SDEs: (Liu
504"
REFERENCES,0.28976572133168926,"et al., 2021; Hu et al., 2019; Bercher et al., 2020; Zhu and Ying, 2021; Cui et al., 2020; Maulén Soto,
505"
REFERENCES,0.29017673653925197,"2021; Wang and Wu, 2020; Lanconelli and Lauria, 2022; Ayadi and Turinici, 2021; Soto et al., 2022;
506"
REFERENCES,0.2905877517468146,"Li and Wang, 2022; Wang and Mao, 2022; Bardi and Kouhkouh, 2022; Chen et al., 2022; Kunin
507"
REFERENCES,0.2909987669543773,"et al., 2023; Zhang et al., 2023; Sun et al., 2023; Li et al., 2023b; Gess et al., 2024; Dambrine et al.,
508"
REFERENCES,0.29140978216193997,"2024; Maulen-Soto et al., 2024).
509"
REFERENCES,0.29182079736950267,"The following ones carried out validation experiments on artificial landscapes, e.g. quadratic or
510"
REFERENCES,0.2922318125770654,"quartic function, or easy regression tasks: (Li et al., 2017, 2019; Zhou et al., 2020b; An et al., 2020;
511"
REFERENCES,0.292642827784628,"Fontaine et al., 2021; Gu et al., 2021; Su and Lau, 2023; Ankirchner and Perko, 2024).
512"
REFERENCES,0.2930538429921907,"The following papers carried out some experiments which include neural networks: (Paquette et al.,
513"
REFERENCES,0.2934648581997534,"2021; Compagnoni et al., 2023). In particular, they both simulate the SDEs with a numerical integrator
514"
REFERENCES,0.2938758734073161,"and compare them with the respective optimizers: The first validates the SDE on a shallow MLP
515"
REFERENCES,0.2942868886148787,"while the second does so on a shallow and a deep MLP. Regarding (Li et al., 2021; Malladi et al.,
516"
REFERENCES,0.29469790382244143,"2022), they do not validate their SDEs: Rather, their approach conceptually proceeds as follows:
517"
REFERENCES,0.29510891903000414,"1. Derive an SDE for an optimizer which we now dub “A”;
518"
REFERENCES,0.2955199342375668,"2. Notice that simulating the SDE is too expensive;
519"
REFERENCES,0.2959309494451295,"3. Define another discrete-time algorithm called SVAG which also has the same SDE as “A”
520"
REFERENCES,0.29634196465269214,"but does not numerically integrate the SDE as it does not even require access to it: It does
521"
REFERENCES,0.29675297986025484,"not need access neither to the drift nor to the diffusion term;
522"
REFERENCES,0.2971639950678175,"4. Simulate SVAG and show that it tracks “A” successfully;
523"
REFERENCES,0.2975750102753802,"5. Conclude that the SDE is a good approximation for “A”.
524"
REFERENCES,0.2979860254829429,"However, they never validated that the SDE is a good approximation for “A” or for SVAG either.
525"
REFERENCES,0.29839704069050554,"With the same logic, they could have done the following:
526"
REFERENCES,0.29880805589806825,"1. Derive an SDE for “A”;
527"
REFERENCES,0.2992190711056309,"2. Notice that simulating the SDE is too expensive;
528"
REFERENCES,0.2996300863131936,"3. Define another discrete-time algorithm called “B” which coincides with “A” and thus of
529"
REFERENCES,0.30004110152075625,"course shares the same SDE;
530"
REFERENCES,0.30045211672831895,"4. Simulate “B” and show that it tracks “A” perfectly;
531"
REFERENCES,0.3008631319358816,"5. Conclude that the SDE is a good approximation for “A”.
532"
REFERENCES,0.3012741471434443,"In particular, the only fact they prove is that SVAG is a discrete-time optimizer that shares the same
533"
REFERENCES,0.301685162351007,"SDE as “A” because it describes a discrete trajectory that is a 1st-order approximation of the SDE of
534"
REFERENCES,0.30209617755856966,"“A”. Technically speaking, “A” also does the same. One cannot conclude that the SDE derived for “A”
535"
REFERENCES,0.30250719276613236,"is a good model for “A” by simply comparing two algorithms “A” and “B” that share the same SDE.
536"
REFERENCES,0.302918207973695,"Otherwise, simply comparing an optimizer “A” with itself would do the trick. An SDE’s empirical
537"
REFERENCES,0.3033292231812577,"validation can only occur if the SDE is simulated with a numerical integrator that requires access to
538"
REFERENCES,0.30374023838882036,"the drift and diffusion terms (Higham, 2001; Milstein, 2013).
539"
REFERENCES,0.30415125359638306,"B
Stochastic calculus
540"
REFERENCES,0.30456226880394577,"In this section, we summarize some important results in the analysis of Stochastic Differential
541"
REFERENCES,0.3049732840115084,"Equations Mao (2007); Øksendal (1990). The notation and the results in this section will be used
542"
REFERENCES,0.3053842992190711,"extensively in all proofs in this paper. We assume the reader to have some familiarity with Brownian
543"
REFERENCES,0.30579531442663377,"motion and with the definition of stochastic integral (Ch. 1.4 and 1.5 in Mao (2007)).
544"
REFERENCES,0.3062063296341965,"B.1
Itô’s Lemma
545"
REFERENCES,0.3066173448417591,"We start with some notation: Let (Ω, F, {Ft}t≥0, P) be a filtered probability space. We say that an
event E ∈F holds almost surely (a.s.) in this space if P(E) = 1. We call Lp([a, b], Rd), with p > 0,
the family of Rd-valued Ft-adapted processes {ft}a≤t≤b such that
Z b"
REFERENCES,0.3070283600493218,"a
∥ft∥pdt ≤∞."
REFERENCES,0.30743937525688453,"Moreover, we denote by Mp([a, b], Rd), with p > 0, the family of Rd-valued processes {ft}a≤t≤b
546"
REFERENCES,0.3078503904644472,"in L([a, b], Rd) such that E
hR b
a ∥ft∥pdt
i
≤∞. We will write h ∈Lp  
R+, Rd
, with p > 0, if
547"
REFERENCES,0.3082614056720099,"h ∈Lp  
[0, T], Rd
for every T > 0. Similar definitions hold for matrix-valued functions using the
548"
REFERENCES,0.30867242087957253,"Frobenius norm ∥A∥:=
qP"
REFERENCES,0.30908343608713523,"ij |Aij|2.
549"
REFERENCES,0.3094944512946979,"Let W = {Wt}t≥0 be a one-dimensional Brownian motion defined on our probability space and let
550"
REFERENCES,0.3099054665022606,"X = {Xt}t≥0 be an Ft-adapted process taking values on Rd.
551"
REFERENCES,0.3103164817098233,"Definition B.1. Let the drift be b ∈L1  
R+, Rd
and the diffusion term be σ ∈L2  
R+, Rd×m
.
552"
REFERENCES,0.31072749691738594,"Xt is an Itô process if it takes the form
553"
REFERENCES,0.31113851212494864,"Xt = x0 +
Z t"
REFERENCES,0.3115495273325113,"0
bsds +
Z t"
REFERENCES,0.311960542540074,"0
σsdWs."
REFERENCES,0.31237155774763664,"We shall say that Xt has the stochastic differential
554"
REFERENCES,0.31278257295519934,"dXt = btdt + σtdWt.
(14) 555"
REFERENCES,0.31319358816276205,"Theorem B.2 (Itô’s Lemma). Let Xt be an Itô process with stochastic differential dXt = btdt +
556"
REFERENCES,0.3136046033703247,"σtdWt. Let f (x, t) be twice continuously differentiable in x and continuously differentiable in t,
557"
REFERENCES,0.3140156185778874,"taking values in R. Then f(Xt, t) is again an Itô process with stochastic differential
558"
REFERENCES,0.31442663378545005,"df(Xt, t) = ∂tf(Xt, t))dt+⟨∇f(Xt, t), bt⟩dt+ 1"
TR,0.31483764899301275,"2Tr
 
σtσ⊤
t ∇2f(Xt, t)

dt+⟨∇f(Xt, t), σt⟩dWt.
(15)"
TR,0.3152486642005754,"B.2
Stochastic Differential Equations
559"
TR,0.3156596794081381,"Stochastic Differential Equations (SDEs) are equations of the form
560"
TR,0.31607069461570075,"dXt = b(Xt, t)dt + σ(Xt, t)dWt."
TR,0.31648170982326346,"First of all, we need to define what it means for a stochastic process X = {Xt}t≥0 with values in Rd
561"
TR,0.31689272503082616,"to solve an SDE.
562"
TR,0.3173037402383888,"Definition B.3. Let Xt be as above with deterministic initial condition X0 = x0. Assume b :
563"
TR,0.3177147554459515,"Rd × [0, T] →Rd and σ : Rd × [0, T] →Rd×m are Borel measurable; Xt is called a solution to the
564"
TR,0.31812577065351416,"corresponding SDE if
565"
TR,0.31853678586107687,"1. Xt is continuous and Ft-adapted;
566"
TR,0.3189478010686395,"2. b ∈L1  
[0, T], Rd
;
567"
TR,0.3193588162762022,"3. σ ∈L2  
[0, T], Rd×m
;
568"
TR,0.3197698314837649,"4. For every t ∈[0, T]"
TR,0.32018084669132757,"Xt = x0 +
Z t"
TR,0.3205918618988903,"0
b(Xs, s)ds +
Z t"
TR,0.3210028771064529,"0
σ(Xs, s)dW(s) a.s."
TR,0.3214138923140156,"Moreover, the solution Xt is said to be unique if any other solution X⋆
t is such that"
TR,0.3218249075215783,"P {Xt = X⋆
t , for all 0 ≤t ≤T} = 1."
TR,0.322235922729141,"569
Notice that since the solution to an SDE is an Itô process, we can use Itô’s Lemma. The following
570"
TR,0.3226469379367037,"theorem gives a sufficient condition on b and σ for the existence of a solution to the corresponding
571"
TR,0.32305795314426633,"SDE.
572"
TR,0.32346896835182903,"Theorem B.4. Assume that there exist two positive constants ¯K and K such that
573"
TR,0.3238799835593917,"1. (Global Lipschitz condition) for all x, y ∈Rd and t ∈[0, T]"
TR,0.3242909987669544,"max{∥b(x, t) −b(y, t)∥2, ∥σ(x, t) −σ(y, t)∥2} ≤¯K∥x −y∥2;"
TR,0.32470201397451703,"2. (Linear growth condition) for all x ∈Rd and t ∈[0, T]"
TR,0.32511302918207974,"max{∥b(x, t)∥2, ∥σ(x, t)∥2} ≤K(1 + ∥x∥2)."
TR,0.32552404438964244,"Then, there exists a unique solution Xt to the corresponding SDE, and Xt ∈M2([0, T], Rd).
574"
TR,0.3259350595972051,"Numerical approximation.
Often, SDEs are solved numerically. The simplest algorithm to provide
575"
TR,0.3263460748047678,"a sample path (ˆxk)k≥0 for Xt, so that Xk∆t ≊ˆxk for some small ∆t and for all k∆t ≤M is called
576"
TR,0.32675709001233044,"Euler-Maruyama (Algorithm 1). For more details on this integration method and its approximation
577"
TR,0.32716810521989315,"properties, the reader can check Mao (2007).
578"
TR,0.3275791204274558,Algorithm 1 Euler-Maruyama Integration Method for SDEs
TR,0.3279901356350185,"input The drift b, the volatility σ, and the initial condition x0."
TR,0.3284011508425812,"Fix a stepsize ∆t;
Initialize ˆx0 = x0;
k = 0;
while k ≤
 T"
TR,0.32881216605014385,"∆t

do
Sample some d-dimensional Gaussian noise Zk ∼N(0, Id);
Compute ˆxk+1 = ˆxk + ∆t b(ˆxk, k∆t) +
√"
TR,0.32922318125770655,"∆t σ(ˆxk, k∆t)Zk;
k = k + 1;
end while
output The approximated sample path (ˆxk)0≤k≤⌊T ∆t⌋."
TR,0.3296341964652692,"C
Theoretical framework - Weak Approximation
579"
TR,0.3300452116728319,"In this section, we introduce the theoretical framework used in the paper, together with its assumptions
580"
TR,0.33045622688039455,"and notations.
581"
TR,0.33086724208795726,"First of all, many proofs will use Taylor expansions in powers of η. For ease of notation, we introduce
the shorthand that whenever we write O (ηα), we mean that there exists a function K(x) ∈G such
that the error terms are bounded by K(x)ηα. For example, we write"
TR,0.3312782572955199,"b(x + η) = b0(x) + ηb1(x) + O
 
η2"
TR,0.3316892725030826,"to mean: there exists K ∈G such that
|b(x + η) −b0(x) −ηb1(x)| ≤K(x)η2.
Additionally, we introduce the following shorthand:
582"
TR,0.3321002877106453,"• A multi-index is α = (α1, α2, . . . , αn) such that αj ∈{0, 1, 2, . . .};
583"
TR,0.33251130291820796,"• |α| := α1 + α2 + · · · + αn;
584"
TR,0.33292231812577067,"• α! := α1!α2! · · · αn!;
585"
TR,0.3333333333333333,"• For x = (x1, x2, . . . , xn) ∈Rn, we define xα := xα1
1 xα2
2 · · · xαn
n ;
586"
TR,0.333744348540896,"• For a multi-index β, ∂|β|
β f(x) :=
∂|β|"
TR,0.33415536374845867,"∂β1
x1 ∂β2
x2 ···∂βn
xn f(x);
587"
TR,0.33456637895602137,"• We also denote the partial derivative with respect to xi by ∂ei.
588 589"
TR,0.3349773941635841,"Definition C.1 (G Set). Let G denote the set of continuous functions Rd →R of at most polynomial
590"
TR,0.3353884093711467,"growth, i.e. g ∈G if there exists positive integers ν1, ν2 > 0 such that |g(x)| ≤ν1
 
1 + |x|2ν2
, for
591"
TR,0.3357994245787094,"all z ∈Rd.
592"
TR,0.3362104397862721,"The next results are inspired by Theorem 1 of Li et al. (2017) and are derived under some regularity
593"
TR,0.3366214549938348,"assumption on the function f.
594"
TR,0.3370324702013974,"Assumption C.2. Assume that the following conditions on f, fi, and their gradients are
satisfied:"
TR,0.33744348540896013,"• ∇f, ∇fi satisfy a Lipschitz condition: there exists L > 0 such that"
TR,0.33785450061652283,"|∇f(u) −∇f(v)| + n
X"
TR,0.3382655158240855,"i=1
|∇fi(u) −∇fi(v)| ≤L|u −v|;"
TR,0.3386765310316482,"• f, fi and its partial derivatives up to order 7 belong to G;
• ∇f, ∇fi satisfy a growth condition: there exists M > 0 such that"
TR,0.33908754623921084,"|∇f(x)| + n
X"
TR,0.33949856144677354,"i=1
|∇fi(x)| ≤M(1 + |x|). 595"
TR,0.3399095766543362,"Lemma C.3 (Lemma 1 Li et al. (2017)). Let 0 < η < 1. Consider a stochastic process
Xt, t ≥0 satisfying the SDE"
TR,0.3403205918618989,dXt = b (Xt) dt + √ησ (Xt) dWt
TR,0.3407316070694616,"with X0 = x ∈Rd and b, σ together with their derivatives belong to G. Define the one-step
difference ∆= Xη −x, and indicate the i-th component of ∆with ∆i. Then we have"
TR,0.34114262227702424,1. E∆i = biη + 1
HPD,0.34155363748458695,"2
hPd
j=1 bj∂ejbi
i
η2 + O
 
η3
∀i = 1, . . . , d;"
HPD,0.3419646526921496,"2. E∆i∆j =
h
bibj + σσT
(ij)
i
η2 + O
 
η3
∀i, j = 1, . . . , d;"
E QS,0.3423756678997123,"3. E Qs
j=1 ∆(ij) = O
 
η3
for all s ≥3, ij = 1, . . . , d."
E QS,0.34278668310727495,All functions above are evaluated at x. 596
E QS,0.34319769831483765,"Theorem C.4 (Theorem 2 and Lemma 5, Mil’shtein (1986)). Let Assumption C.2 hold and
let us define ¯∆= x1 −x to be the increment in the discrete-time algorithm, and indicate the
i-th component of ¯∆with ¯∆i. If in addition there exists K1, K2, K3, K4 ∈G so that 1."
E QS,0.34360871352240036,"E∆i −E ¯∆i
 ≤K1(x)η2,
∀i = 1, . . . , d; 2."
E QS,0.344019728729963,"E∆i∆j −E ¯∆i ¯∆j
 ≤K2(x)η2,
∀i, j = 1, . . . , d; 3."
E QS,0.3444307439375257,"E Qs
j=1 ∆ij −E Qs
j=1 ¯∆ij
 ≤K3(x)η2,
∀s ≥3,
∀ij ∈{1, . . . , d};"
E QS,0.34484175914508836,"4. E Q3
j=1
 ¯∆ij
 ≤K4(x)η2,
∀ij ∈{1, . . . , d}."
E QS,0.34525277435265106,"Then, there exists a constant C so that for all k = 0, 1, . . . , N we have"
E QS,0.3456637895602137,|Eg (Xkη) −Eg (xk)| ≤Cη. 597
E QS,0.3460748047677764,"C.1
Limitations
598"
E QS,0.34648581997533906,"Modeling of discrete-time algorithms using SDEs relies on Assumption C.2. As noted by Li et al.
599"
E QS,0.34689683518290176,"(2021), the approximation can fail when the stepsize η is large or if certain conditions on ∇f and the
600"
E QS,0.34730785039046447,"noise covariance matrix are not met. Although these issues can be addressed by increasing the order
601"
E QS,0.3477188655980271,"of the weak approximation, we believe that the primary purpose of SDEs is to serve as simplification
602"
E QS,0.3481298808055898,"tools that enhance our intuition: We would not benefit significantly from added complexity.
603"
E QS,0.34854089601315247,"C.2
Formal derivation - SignSGD
604"
E QS,0.34895191122071517,"In this subsection, we provide the first formal derivation of an SDE model for SignSGD. Let us
605"
E QS,0.3493629264282778,"consider the stochastic process Xt ∈Rd defined as the solution of
606"
E QS,0.3497739416358405,"dXt = −(1 −2P(∇fγ(Xt) < 0))dt + √η
q"
E QS,0.35018495684340323,"¯Σ(Xt)dWt,
(16)"
E QS,0.3505959720509659,"where
607"
E QS,0.3510069872585286,"¯Σ(x) = E[ξγ(x)ξγ(x)⊤],
(17)"
E QS,0.35141800246609123,"and ξγ(x) := sign(∇fγ(x)) −1 + 2P(∇fγ(x) < 0) the noise in the sample sign (∇fγ(x)). The
608"
E QS,0.35182901767365393,"following theorem guarantees that such a process is a 1-order SDE of the discrete-time algorithm of
609"
E QS,0.3522400328812166,"SignSGD
610"
E QS,0.3526510480887793,"xk+1 = xk −ηsign (fγk(xk)) ,
(18)"
E QS,0.353062063296342,"with x0 ∈Rd, η ∈R>0 is the step size, the mini-batches {γk} are modelled as i.i.d. random variables
611"
E QS,0.35347307850390464,"uniformly distributed on {1, · · · , N}, and of size B ≥1.
612"
E QS,0.35388409371146734,"Theorem C.5 (Stochastic modified equations). Let 0 < η < 1, T > 0 and set N = ⌊T/η⌋.
Let xk ∈Rd, 0 ≤k ≤N denote a sequence of SignSGD iterations defined by Eq. (18).
Consider the stochastic process Xt defined in Eq. (16) and fix some test function g ∈G and
suppose that g and its partial derivatives up to order 6 belong to G.
Then, under Assumption C.2, there exists a constant C > 0 independent of η such that for all
k = 0, 1, . . . , N, we have"
E QS,0.35429510891903,"|Eg (Xkη) −Eg (xk)| ≤Cη.
That is, the SDE (16) is an order 1 weak approximation of the SignSGD iterations (18). 613"
E QS,0.3547061241265927,"Lemma C.6. Under the assumptions of Theorem C.5, let 0 < η < 1 and consider xk, k ≥0
satisfying the SignSGD iterations"
E QS,0.35511713933415534,xk+1 = xk −ηsign (∇fγk(xk))
E QS,0.35552815454171804,"with x0 ∈Rd. From the definition the one-step difference ¯∆= x1 −x, then we have"
E QS,0.35593916974928075,"1. E ¯∆i = −(1 −2P (∂ifγ < 0)) η
∀i = 1, . . . , d;
2. E ¯∆i ¯∆j
=
 
(1 −2P (∂ifγ < 0)) (1 −2P (∂jfγ < 0)) + ¯Σ(ij)

η2
∀i, j
=
1, . . . , d;
3. E Qs
j=1 ¯∆ij = O
 
η3
∀s ≥3,
ij ∈{1, . . . , d}."
E QS,0.3563501849568434,All the functions above are evaluated at x. 614
E QS,0.3567612001644061,"Proof of Lemma C.6. First of all, we have that by definition
615"
E QS,0.35717221537196875,"E

xi
1 −xi
= −ηE [sign (∂ifγ(x) < 0)] ,
(19)"
E QS,0.35758323057953145,"which implies
616"
E QS,0.3579942457870941,"E ¯∆i = −(1 −2P (∂ifγ(x) < 0)) η
∀i = 1, . . . , d.
(20)"
E QS,0.3584052609946568,"Second, we have that by definition
617"
E QS,0.3588162762022195,"E
h
(x1 −x) (x1 −x)⊤i
=E
h
(sign (∂ifγ(x) < 0) −1 + 2P (∂ifγ(x) < 0))
(21)"
E QS,0.35922729140978216,"(sign (∂ifγ(x) < 0) −1 + 2P (∂ifγ(x) < 0))⊤i
η2,
(22)"
E QS,0.35963830661734486,"which implies that
618"
E QS,0.3600493218249075,"E ¯∆i ¯∆j = (1 −2P (∂ifγ < 0)) (1 −2P (∂jfγ < 0)) η2 + ¯Σ(ij)η2
∀i, j = 1, . . . , d.
(23)"
E QS,0.3604603370324702,"Finally, by definition
619 E s
Y"
E QS,0.36087135224003286,"j=1
¯∆ij = O
 
η3
∀s ≥3,
ij ∈{1, . . . , d},
(24)"
E QS,0.36128236744759556,"which concludes our proof.
620"
E QS,0.3616933826551582,"Proof of Theorem C.5. To prove this result, all we need to do is check the conditions in Theorem C.4.
621"
E QS,0.3621043978627209,"As we apply Lemma C.3, we make the following choices:
622"
E QS,0.3625154130702836,"• b(x) = −(1 −2P (∇fγ(x) < 0));
623"
E QS,0.36292642827784627,"• σ(x) =
p¯Σ(x).
624"
E QS,0.363337443485409,"First of all, we notice that ∀i = 1, . . . , d, it holds that
625"
E QS,0.3637484586929716,"• E ¯∆i
1. Lemma C.6
=
−(1 −2P (∂ifγ(x) < 0)) η;
626"
E QS,0.3641594739005343,"• E∆i
1. Lemma C.3
=
−(1 −2P (∂ifγ(x) < 0)) η + O
 
η2
.
627"
E QS,0.364570489108097,"Therefore, we have that for some K1(x) ∈G,
628"
E QS,0.3649815043156597,"E∆i −E ¯∆i
 ≤K1(x)η2,
∀i = 1, . . . , d.
(25)"
E QS,0.3653925195232224,"Additionally, we notice that ∀i, j = 1, . . . , d, it holds that
629"
E QS,0.36580353473078503,"• E ¯∆i ¯∆j
2. Lemma C.6
=
(1 −2P (∂ifγ(x) < 0)) (1 −2P (∂jfγ(x) < 0)) η2 + ¯Σ(ij)(x)η2;
630"
E QS,0.36621454993834773,"• E∆i∆j
2. Lemma C.3
=
 
(1 −2P (∂ifγ(x) < 0)) (1 −2P (∂jfγ(x) < 0)) + ¯Σ(ij)(x)

η2 +
631"
E QS,0.3666255651459104,"O
 
η3
.
632"
E QS,0.3670365803534731,"Therefore, we have that for some K2(x) ∈G,
633"
E QS,0.36744759556103573,"E∆i∆j −E ¯∆i ¯∆j
 ≤K2(x)η2,
∀i, j = 1, . . . , d.
(26)"
E QS,0.36785861076859844,"Additionally, we notice that ∀s ≥3, ∀ij ∈{1, . . . , d}, it holds that
634"
E QS,0.36826962597616114,"• E Qs
j=1 ¯∆ij
3. Lemma C.6
=
O
 
η3
;
635"
E QS,0.3686806411837238,"• E Qs
j=1 ∆ij
3. Lemma C.3
=
O
 
η3
.
636"
E QS,0.3690916563912865,"Therefore, we have that for some K3(x) ∈G,
637 E s
Y"
E QS,0.36950267159884914,"j=1
∆ij −E s
Y"
E QS,0.36991368680641185,"j=1
¯∆ij"
E QS,0.3703247020139745,"≤K3(x)η2.
(27)"
E QS,0.3707357172215372,"Additionally, for some K4(x) ∈G, ∀ij ∈{1, . . . , d},
638 E"
Y,0.3711467324290999,"3
Y j=1"
Y,0.37155774763666255,"¯∆(ij)

3. Lemma C.6"
Y,0.37196876284422525,"≤
K4(x)η2.
(28)"
Y,0.3723797780517879,"To conclude, Eq. (25), Eq. (26), Eq. (27), and Eq. (28) allow us to conclude the proof.
639"
Y,0.3727907932593506,"Corollary C.7. Let us take the same assumptions of Theorem C.5, and that the stochastic
gradient is ∇fγ(x) = ∇f(x) + U such that U ∼N(0, Σ) that does not depend on x. Then,
the following SDE provides a 1 weak approximation of the discrete update of SignSGD"
Y,0.37320180846691325,dXt = −Erf Σ−1
Y,0.37361282367447596,"2 ∇f(Xt)
√ 2 !"
Y,0.37402383888203866,dt + √η
Y,0.3744348540896013,"v
u
u
tId −diag  Erf Σ−1"
Y,0.374845869297164,"2 ∇f(Xt)
√ 2 !!2"
Y,0.37525688450472666,"dWt, (29)"
Y,0.37566789971228937,"where the error function Erf(x) and the square are applied component-wise, and Σ =
diag
 
σ2
1, · · · , σ2
d

. 640"
Y,0.376078914919852,"Proof of Corollary C.7. First of all, we observe that
641"
Y,0.3764899301274147,"1 −2P (∇fγ(x) < 0) = 1 −2P

∇f(x) + Σ
1
2 U < 0

= 1 −2Φ

−Σ−1"
Y,0.3769009453349774,"2 ∇f(x)

,
(30)"
Y,0.37731196054254007,"where Φ is the cumulative distribution function of the standardized normal distribution. Remembering
642"
Y,0.3777229757501028,"that
643"
Y,0.3781339909576654,Φ(x) = 1 2
Y,0.3785450061652281,"
1 + Erf
 x
√ 2"
Y,0.3789560213727908,"
,
(31)"
Y,0.3793670365803535,"we have that
644"
Y,0.3797780517879161,1 −2P (∇fγ(x) < 0) = 1 −21 2 
Y,0.38018906699547883,1 + Erf  −Σ−1
Y,0.38060008220304153,"2 ∇f(x)
√ 2 !! = Erf Σ−1"
Y,0.3810110974106042,"2 ∇f(x)
√ 2 !"
Y,0.3814221126181669,".
(32)"
Y,0.38183312782572953,"Similarly, one can prove that ¯Σ defined in (17) becomes
645"
Y,0.38224414303329224,¯Σ = Id −diag  Erf Σ−1
Y,0.3826551582408549,"2 ∇f(Xt)
√ 2 !!2"
Y,0.3830661734484176,".
(33) 646"
Y,0.3834771886559803,"Corollary C.8. Let us take the same assumptions of Theorem C.5, and that the stochastic
gradient is ∇fγ(x) = ∇f(x) +
√"
Y,0.38388820386354294,"ΣU such that U ∼tν(0, Id) that does not depend on x
and ν is a positive integer number. Then, the following SDE provides a 1 weak approximation
of the discrete update of SignSGD"
Y,0.38429921907110565,"dXt = −2Ξ

Σ−1"
Y,0.3847102342786683,"2 ∇f(Xt)

dt + √η r"
Y,0.385121249486231,"Id −4 diag

Ξ

Σ−1"
Y,0.38553226469379365,"2 ∇f(Xt)
2
dWt,
(34)"
Y,0.38594327990135635,where Ξ(x) is defined as
Y,0.38635429510891905,"Ξ(x) := x Γ
  ν+1 2
"
Y,0.3867653103164817,"√πνΓ
  ν"
Y,0.3871763255240444,"2
 2F1 1"
Y,0.38758734073160706,"2, ν + 1 2
; 3"
Y,0.38799835593916976,2; −x2 ν
Y,0.3884093711467324,"
,
(35)"
Y,0.3888203863542951,"and 2F1 (a, b; c; x) is the hypergeometric function. Above, function Ξ(x) and the square are
applied component-wise, and Σ = diag
 
σ2
1, · · · , σ2
d

. 647"
Y,0.3892314015618578,"Proof of Corollary C.8. First of all, we observe that
648"
Y,0.38964241676942046,"1 −2P (∇fγ(x) < 0) = 1 −2P

∇f(x) + Σ
1
2 U < 0

= 1 −2Fν

−Σ−1"
Y,0.39005343197698317,"2 ∇f(x)

,
(36)"
Y,0.3904644471845458,"where Fν (x) is the cumulative function of a t distribution with ν degrees of freedom. Remembering
649"
Y,0.3908754623921085,"that
650"
Y,0.39128647759967117,Fν (x) = 1
Y,0.39169749280723387,"2 + Ξ(x),
(37)"
Y,0.3921085080147966,"we have that
651"
Y,0.3925195232223592,"1 −2P (∇fγ(x) < 0) = 1 −2
1"
Y,0.3929305384299219,"2 + Ξ(x)

= −2Ξ(x).
(38)"
Y,0.3933415536374846,"Similarly, one can prove that ¯Σ defined in (17) becomes
652"
Y,0.3937525688450473,"¯Σ = Id −4 diag

Ξ

Σ−1"
Y,0.3941635840526099,"2 ∇f(Xt)
2
.
(39) 653"
Y,0.39457459926017263,Lemma C.9. Under the assumptions of Corollary C.7 and signal-to-noise ratio Yt := Σ−1
Y,0.3949856144677353,"2 ∇f(Xt)
√"
Y,0.395396629675298,"2
,
654"
Y,0.3958076448828607,1. Phase 1: If |Yt| > 3
Y,0.39621866009042334,"2, the SDE coincides with the ODE of SignGD:
655"
Y,0.39662967529798604,"dXt = −sign(∇f(Xt))dt;
(40)"
Y,0.3970406905055487,2. Phase 2: If 1 < |Yt| < 3
Y,0.3974517057131114,"2:
656"
Y,0.39786272092067404,(a) mYt + q−≤dE[Xt]
Y,0.39827373612823674,"dt
≤mYt + q+;
657"
Y,0.39868475133579945,"(b) P

∥Xt −E [Xt]∥2
2 > a

≤η"
Y,0.3990957665433621,"a
 
d −∥mYt + q−∥2
2

;
658"
Y,0.3995067817509248,"3. Phase 3: If |Yt| < 1, the SDE is
659"
Y,0.39991779695848745,dXt = − r
Y,0.40032881216605015,"2
π Σ−1"
Y,0.4007398273736128,2 ∇f(Xt)dt + √η r Id −2
Y,0.4011508425811755,"π diag

Σ−1"
Y,0.4015618577887382,"2 ∇f(Xt)
2
dWt.
(41)"
Y,0.40197287299630086,"Proof of Lemma C.9. Exploiting the regularity of the Erf function, we approximate the SDE in (29)
660"
Y,0.40238388820386356,"in three different regions:
661"
Y,0.4027949034114262,1. Phase 1: If |x| > 3
Y,0.4032059186189889,"2, Erf(x) ∼sign(x). Therefore, if

Σ−1"
Y,0.40361693382655156,"2 ∇f(Xt)
√ 2 > 3"
Y,0.40402794903411426,"2,
662"
Y,0.40443896424167697,"(a) Erf

Σ−1"
Y,0.4048499794492396,"2 ∇f(Xt)
√ 2"
Y,0.4052609946568023,"
∼sign

Σ−1"
Y,0.40567200986436497,"2 ∇f(Xt)
√ 2"
Y,0.4060830250719277,"
= sign (∇f(Xt));
663"
Y,0.4064940402794903,"(b) Erf

Σ−1"
Y,0.406905055487053,"2 ∇f(Xt)
√ 2"
Y,0.40731607069461573,"2
∼sign

Σ−1"
Y,0.4077270859021784,"2 ∇f(Xt)
√ 2"
Y,0.4081381011097411,"2
= (1, . . . , 1).
664"
Y,0.40854911631730373,"Therefore,
665"
Y,0.40896013152486643,dXt = −Erf Σ−1
Y,0.4093711467324291,"2 ∇f(Xt)
√ 2 !"
Y,0.4097821619399918,dt + √η
Y,0.41019317714755443,"v
u
u
tId −diag  Erf Σ−1"
Y,0.41060419235511714,"2 ∇f(Xt)
√ 2 !!2 dWt"
Y,0.41101520756267984,"∼−sign(∇f(Xt));
(42)"
Y,0.4114262227702425,"2. Phase 2: Let m and q1 are the slope and intercept of the line secant to the graph of Erf(x)
666"
Y,0.4118372379778052,"between the points (1, Erf(1)) and
  3"
Y,0.41224825318536784,"2, Erf
  3"
Y,0.41265926839293054,"2

, while q2 is the intercept of the line tangent
667"
Y,0.4130702836004932,to the graph of Erf(x) and slope m. If 1 < x < 3
Y,0.4134812988080559,"2, we have that
668"
Y,0.4138923140156186,"mx + q1 < Erf(x) < mx + q2.
(43)"
Y,0.41430332922318125,"Analogously, if −3"
Y,0.41471434443074395,"2 < x < −1
669"
Y,0.4151253596383066,"mx −q2 < Erf(x) < mx −q1.
(44)"
Y,0.4155363748458693,"Therefore, we have that if 1 <

Σ−1"
Y,0.41594739005343195,"2 ∇f(Xt)
√ 2 < 3"
Y,0.41635840526099466,"2, then
670 (a) m
√ 2Σ−1"
Y,0.41676942046855736,2 ∇f(Xt) + q−< Erf Σ−1
Y,0.41718043567612,"2 ∇f(Xt)
√ 2 ! < m
√ 2Σ−1"
Y,0.4175914508836827,"2 ∇f(Xt) + q+,
(45)"
Y,0.41800246609124536,"where
671"
Y,0.41841348129880807,"(q+)i :=
q2
if ∂if(x) > 0
−q1
if ∂if(x) < 0 ,
(46)"
Y,0.4188244965063707,"and
672"
Y,0.4192355117139334,"(q−)i :=
q1
if ∂if(x) > 0
−q2
if ∂if(x) < 0 ,
(47)"
Y,0.4196465269214961,"Therefore,
673 m
√ 2Σ−1"
Y,0.42005754212905877,2 ∇f(Xt) + q−≤dE [Xt]
Y,0.4204685573366215,"dt
≤m
√ 2Σ−1"
Y,0.4208795725441841,"2 ∇f(Xt) + q+;
(48)"
Y,0.4212905877517468,"(b) Similar to the above,
674  m
√ 2Σ−1"
Y,0.4217016029593095,"2 ∇f(Xt) + q−
2
≤Erf Σ−1"
Y,0.4221126181668722,"2 ∇f(Xt)
√ 2 !2"
Y,0.4225236333744349,"≤
 m
√ 2Σ−1"
Y,0.42293464858199753,"2 ∇f(Xt) + q+
2
."
Y,0.42334566378956023,"Therefore,
675"
Y,0.4237566789971229,"P

∥Xt −E [Xt]∥2
2 > a

≤P

∃i s.t. |Xi
t −E

Xi
t

|2 > a

(49) ≤
X"
Y,0.4241676942046856,"i
P

|Xi
t −E

Xi
t

| > √a
 ≤η a X i "
Y,0.42457870941224823,"1 −Erf Σ
−1"
I,0.42498972461981094,"2
i
∂if(Xt)
√ 2 !2"
I,0.4254007398273736,"
(50) < η a"
I,0.4258117550349363,"
d −∥m
√ 2Σ−1"
I,0.426222770242499,"2 ∇f(Xt) + q−∥2
2"
I,0.42663378545006164,"
.
(51)"
I,0.42704480065762435,"3. Phase 3: If |x| < 1, Erf(x) ∼
2
√π. Therefore, if

Σ−1"
I,0.427455815865187,"2 ∇f(Xt)
√ 2"
I,0.4278668310727497,"< 1,
676"
I,0.42827784628031235,"(a) Erf

Σ−1"
I,0.42868886148787505,"2 ∇f(Xt)
√ 2 
∼
q"
I,0.42909987669543775,"2
πΣ−1"
I,0.4295108919030004,"2 ∇f(Xt);
677 (b)"
I,0.4299219071105631,"
Erf

Σ−1"
I,0.43033292231812575,"2 ∇f(Xt)
√ 2"
I,0.43074393752568846,"2
∼2"
I,0.4311549527332511,"π

Σ−1"
I,0.4315659679408138,"2 ∇f(Xt)
2
.
678"
I,0.4319769831483765,"Therefore,
679"
I,0.43238799835593916,dXt = −Erf Σ−1
I,0.43279901356350187,"2 ∇f(Xt)
√ 2 !"
I,0.4332100287710645,dt + √η
I,0.4336210439786272,"v
u
u
tId −diag  Erf Σ−1"
I,0.43403205918618987,"2 ∇f(Xt)
√ 2 !!2 dWt ∼− r"
I,0.43444307439375257,"2
π Σ−1"
I,0.4348540896013153,2 ∇f(Xt)dt + √η r Id −2
I,0.4352651048088779,"π diag

Σ−1"
I,0.4356761200164406,"2 ∇f(Xt)
2
dWt.
(52) 680"
I,0.4360871352240033,"Lemma C.10 (Dynamics of Expected Loss). Let f be µ-strongly convex, Tr(∇2f(x)) ≤Lτ, and
681"
I,0.436498150431566,"St := f(Xt) −f(X∗). Then, during
682"
I,0.4369091656391286,"1. Phase 1, the dynamics will stop before t∗= 2
q S0"
I,0.43732018084669133,µ because St ≤1
I,0.43773119605425403,"4
 √µt −2√S0
2;
683"
I,0.4381422112618167,"2. Phase 2 with ∆:=

m
√"
I,0.4385532264693794,2σmax + ηµm2
I,0.43896424167694204,4σ2max
I,0.43937525688450474,"
: E[St] ≤S0e−2µ∆t + η"
I,0.4397862720920674,"2
(Lτ −µdˆq2)"
I,0.4401972872996301,"2µ∆
 
1 −e−2µ∆t
;
684"
I,0.44060830250719274,"3. Phase 3 with ∆:=
q"
I,0.44101931771475544,"2
π
1
σmax + η"
I,0.44143033292231815,"π
µ
σ2max"
I,0.4418413481298808,"
: E[St] ≤S0e−2µ∆t + η"
I,0.4422523633374435,"2
Lτ
2µ∆
 
1 −e−2µ∆t
.
685"
I,0.44266337854500615,"Proof of Lemma C.10. We prove each point by leveraging the shape of the law of Xt derived in
686"
I,0.44307439375256885,"Lemma C.9:
687"
I,0.4434854089601315,"1. Phase 1:
688"
I,0.4438964241676942,"d(f(Xt) −f(X∗)) = −∇f(Xt)sign(∇f(Xt)) = −∥∇f(Xt)∥1 ≤−∥∇f(Xt)∥2
(53)"
I,0.4443074393752569,"Since f is µ −PL, we have that −∥∇f(Xt)∥2
2 < −2µ(f(Xt) −f(X∗)), which implies
689"
I,0.44471845458281956,"that
690"
I,0.44512946979038226,f(Xt) −f(X∗) ≤1 4
I,0.4455404849979449,"√µt −2
p"
I,0.4459515002055076,"f(X0) −f(X∗)
2
,
(54)"
I,0.44636251541307026,"meaning that the dynamics will stop before t∗= 2
q"
I,0.44677353062063296,f(X0)−f(X∗)
I,0.44718454582819567,"µ
;
691"
I,0.4475955610357583,"2. Phase 2: By applying the Itô Lemma to f(Xt) −f(X∗) and that
692 m
√ 2Σ−1"
I,0.448006576243321,2 ∇f(Xt) + q−< Erf Σ−1
I,0.44841759145088367,"2 ∇f(Xt)
√ 2 ! < m
√ 2Σ−1"
I,0.44882860665844637,"2 ∇f(Xt) + q+,
(55)"
I,0.449239621866009,"we have that if ˆq := max(q1, q2),
693"
I,0.4496506370735717,"d(f(Xt) −f(X∗)) ≤−
 m
√ 2Σ−1"
I,0.45006165228113443,"2 ∇f(Xt) + q−
⊤
∇f(Xt)dt + O(Noise)
(56) + η"
TR,0.4504726674886971,"2Tr """
TR,0.4508836826962598,∇2f(Xt) 
TR,0.45129469790382243,"Id −diag
 m
√ 2Σ−1"
TR,0.45170571311138513,"2 ∇f(Xt) + q−
2!# (57) ≤−m
√"
TR,0.4521167283189478,"2
1
σmax
∥∇f(Xt)∥2
2dt −ˆq∥∇f(Xt)∥1dt + ηLτ"
DT,0.4525277435265105,"2 dt
(58) −ηµ"
DT,0.4529387587340732,"2 ∥m
√ 2Σ−1"
DT,0.45334977394163584,"2 ∇f(Xt) + q−∥2
2dt + O(Noise)
(59) ≤−m
√"
DT,0.45376078914919854,"2
1
σmax
∥∇f(Xt)∥2
2dt −ˆq∥∇f(Xt)∥1dt + ηLτ"
DT,0.4541718043567612,"2 dt
(60) −ηµm2"
DT,0.4545828195643239,"4σ2max
∥∇f(Xt)∥2
2dt −ηµdˆq2"
DT,0.45499383477188654,"2
dt − √"
DT,0.45540484997944924,"2mˆq
σmax
∥∇f(Xt)∥1dt (61)"
DT,0.4558158651870119,"+ O(Noise)
(62)"
DT,0.4562268803945746,"≤−2µ

m
√"
DT,0.4566378956021373,"2σmax
+ ηµm2"
DT,0.45704891080969995,4σ2max
DT,0.45745992601726265,"
(f(Xt) −f(X∗))dt
(63) + η"
DT,0.4578709412248253,"2
 
Lτ −µdˆq2
dt + O(Noise),
(64)"
DT,0.458281956432388,"which implies that if k := 2µ

m
√"
DT,0.45869297163995065,2σmax + ηµm2
DT,0.45910398684751336,4σ2max
DT,0.45951500205507606,"
,
694"
DT,0.4599260172626387,"E[f(Xt) −f(X∗)] ≤(f(X0) −f(X∗)))e−kt + η
 
Lτ −µdˆq2"
K,0.4603370324702014,"2k
 
1 −e−kt
.
(65)"
K,0.46074804767776406,"3. Phase 3: By applying the Itô Lemma to f(Xt) −f(X∗), we have that:
695"
K,0.46115906288532676,d(f(Xt) −f(X∗)) = − r
K,0.4615700780928894,"2
π ∇f(Xt)⊤Σ−1"
K,0.4619810933004521,"2 ∇f(Xt)dt + O(Noise)
(66) + η"
TR,0.4623921085080148,"2Tr

Id −2"
TR,0.46280312371557747,"π diag

Σ−1"
TR,0.4632141389231402,"2 ∇f(Xt)
2
∇2f(Xt)

dt
(67) ≤− r"
TR,0.4636251541307028,"2
π
1
σmax
∥∇f(Xt)∥2
2dt + O(Noise)
(68) + η"
TR,0.4640361693382655,"2Tr
 
∇2f(Xt)

dt −η"
TR,0.4644471845458282,"π
µ
σ2max
∥∇f(Xt)∥2
2dt
(69) ≤− r"
TR,0.4648581997533909,"2
π
1
σmax
+ η"
TR,0.4652692149609536,"π
µ
σ2max !"
TR,0.46568023016851623,"∥∇f(Xt)∥2
2dt
(70) + η"
TR,0.46609124537607893,"2Tr(∇2f(Xt))dt + O(Noise)
(71)"
TR,0.4665022605836416,"Since f is µ-Strongly Convex, f is also µ-PL. Therefore, we have
696"
TR,0.4669132757912043,d(f(Xt) −f(X∗)) ≤−2µ r
TR,0.46732429099876693,"2
π
1
σmax
+ η"
TR,0.46773530620632964,"π
µ
σ2max !"
TR,0.46814632141389234,"(f(Xt) −f(X∗))dt
(72) + η"
TR,0.468557336621455,"2Tr(∇2f(Xt))dt + O(Noise).
(73)"
TR,0.4689683518290177,"Therefore,
697"
TR,0.46937936703658034,dE[f(Xt) −f(X∗)] ≤−2µ r
TR,0.46979038224414305,"2
π
1
σmax
+ η"
TR,0.4702013974517057,"π
µ
σ2max !"
TR,0.4706124126592684,(E[f(Xt) −f(X∗)])dt + η
TR,0.47102342786683105,"2Lτdt, (74)"
TR,0.47143444307439375,"which implies that if k := 2µ
q"
TR,0.47184545828195645,"2
π
1
σmax + η"
TR,0.4722564734895191,"π
µ
σ2max"
TR,0.4726674886970818,"
,
698"
TR,0.47307850390464445,E[f(Xt) −f(X∗)] ≤(f(X0) −f(X∗)))e−kt + ηLτ
K,0.47348951911220716,"2k
 
1 −e−kt
.
(75) 699"
K,0.4739005343197698,"Lemma C.11. Under the assumptions of Lemma 3.5, for any step size scheduler ηt such that
700 Z ∞"
K,0.4743115495273325,"0
ηsds = ∞and lim
t→∞ηt = 0 =⇒E[f(Xt) −f(X∗)]
t→∞
→0.
(76)"
K,0.4747225647348952,"Proof of Lemma C.11. For any scheduler ηk used in
701"
K,0.47513357994245786,"xk+1 = xk −ηηksign (fγk(xk)) ,
(77)"
K,0.47554459515002057,"the SDE of Phase 3 is
702"
K,0.4759556103575832,dXt = − r
K,0.4763666255651459,"2
π Σ−1"
K,0.47677764077270857,2 ∇f(Xt)ηtdt + √ηηt r Id −2
K,0.47718865598027127,"π diag

Σ−1"
K,0.477599671187834,"2 ∇f(Xt)
2
dWt.
(78)"
K,0.4780106863953966,"Therefore, analogously to the calculations in Lemma C.10, we have that
703"
K,0.4784217016029593,E[f(Xt) −f(X∗)] ≤f(X0) −f(X∗) + ηLτ
R T,0.478832716810522,"2
R t
0 e
2µ
R s
0 √"
R T,0.4792437320180847,"2
π
1
σmax ηl+ η"
R T,0.4796547472256473,"π
µ
σ2max η2
l"
R T,0.48006576243321003,"
dlη2
sds"
R T,0.48047677764077273,"e
2µ
R t
0 √"
R T,0.4808877928483354,"2
π
1
σmax ηs+ η"
R T,0.4812988080558981,"π
µ
σ2max η2s"
R T,0.48170982326346073,"
ds
.
(79)"
R T,0.48212083847102344,"Therefore, using l’Hôpital’s rule we have that
704 Z ∞"
R T,0.4825318536785861,"0
ηsds = ∞and lim
t→∞ηt = 0 =⇒E[f(Xt) −f(X∗)]
t→∞
→0.
(80) 705"
R T,0.4829428688861488,"Lemma C.12. Let H = diag(λ1, . . . , λd) and Mt := e
−2
√"
R T,0.4833538840937115,"2
π Σ−1"
R T,0.48376489930127414,2 H+ η π Σ−1
R T,0.48417591450883685,"2 H2
t. Then,
706"
R T,0.4845869297163995,1. E [Xt] = e−√
R T,0.4849979449239622,"2
π Σ−1"
R T,0.48540896013152485,"2 HtX0;
707"
R T,0.48581997533908755,"2. V ar [Xt] =

Mt −e−2√"
R T,0.4862309905466502,"2
π Σ−1"
R T,0.4866420057542129,"2 Ht
X2
0 + η 2
q"
R T,0.4870530209617756,"2
πId + η"
R T,0.48746403616933826,"πH
−1
H−1Σ
1
2 (Id −Mt).
708"
R T,0.48787505137690096,"Proof of Lemma C.12. The proof is banal: The expected value derivation leverages the martingale
709"
R T,0.4882860665844636,"property of the Brownian motion while that of the variance uses the Ito Isomerty.
710"
R T,0.4886970817920263,"Lemma C.13. Let H = diag(λ1, . . . , λd). Then, E
h
X⊤
t HXt"
"I
IS EQUAL TO",0.48910809699958896,"2
i
is equal to
711 d
X i=1"
"I
IS EQUAL TO",0.48951911220715166,"λi(Xi
0)2"
E,0.48993012741471437,"2
e
−2λi √"
E,0.490341142622277,"2
π
1
σi + λiη πσ2
i"
E,0.4907521578298397,"
t +
η 4
q"
E,0.49116317303740237,"2
π
1
σi + λiη πσ2
i  "
E,0.49157418824496507,"1 −e
−2λi √"
E,0.4919852034525277,"2
π
1
σi + λiη πσ2
i 
t
!"
E,0.4923962186600904,".
(81)"
E,0.4928072338676531,"Proof of Lemma C.13. Since the matrix H is diagonal, we focus on a single component. We apply
712"
E,0.4932182490752158,"the Ito Lemma to λi(Xi
t)2"
E,0.4936292642827785,"2
:
713"
E,0.4940402794903411,"d
λi(Xi
t)2 2"
E,0.49445129469790383,"
= −2 r"
E,0.4948623099054665,"2
π
λi
σi"
E,0.4952733251130292,"λi(Xi
t)2"
E,0.4956843403205919,"2
dt + ηλi"
E,0.49609535552815454,"2 dt −2λ2
i η
πσ2
i"
E,0.49650637073571724,"λi(Xi
t)2"
E,0.4969173859432799,"2
+ O(Noise),
(82)"
E,0.4973284011508426,"which implies that
714"
E,0.49773941635840524,"E
λi(Xi
t)2 2"
E,0.49815043156596794,"
= λi(Xi
0)2"
E,0.49856144677353065,"2
e
−2
√"
E,0.4989724619810933,"2
π
λi
σi +
λ2
i η πσ2
i"
E,0.499383477188656,"
t +
η 4
q"
E,0.49979449239621865,"2
π
1
σi + λiη πσ2
i  "
E,0.5002055076037814,"1 −e
−2
√"
E,0.5006165228113441,"2
π
λi
σi +
λ2
i η πσ2
i 
t
! . (83)"
E,0.5010275380189066,"Therefore,
715"
E,0.5014385532264694,"E
X⊤
t HXt 2 
= d
X i=1"
E,0.5018495684340321,"λi(Xi
0)2"
E,0.5022605836415948,"2
e
−2λi √"
E,0.5026715988491575,"2
π
1
σi + λiη πσ2
i"
E,0.5030826140567201,"
t+
η 4
q"
E,0.5034936292642828,"2
π
1
σi + λiη πσ2
i  "
E,0.5039046444718455,"1 −e
−2λi √"
E,0.5043156596794082,"2
π
1
σi + λiη πσ2
i 
t
! . (84) 716"
E,0.5047266748869709,"Lemma C.14. Under the assumptions of Corollary C.8, where ∇fγ(x) = ∇f(x) +
√"
E,0.5051376900945335,"ΣU, we have
717"
E,0.5055487053020962,"that the dynamics of SignSGD in Phase 3 is:
718"
E,0.5059597205096589,dXt = − r
E,0.5063707357172216,"1
2Σ−1"
E,0.5067817509247842,2 ∇f(Xt)dt + √η r Id −1
DIAG,0.5071927661323469,"2 diag

Σ−1"
DIAG,0.5076037813399096,"2 ∇f(Xt)
2
dWt.
(85)"
DIAG,0.5080147965474723,"Proof of lemma C.14. We apply Eq. (34) with ν = 2 and linearly approximate Ξ(x) as |x| < 1,
719"
DIAG,0.508425811755035,"where 2Ξ(x) ∼
x
√"
DIAG,0.5088368269625976,"2.
720"
DIAG,0.5092478421701603,"C.3
Formal derivation - RMSprop
721"
DIAG,0.509658857377723,"2
0
2
X1 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 X2"
DIAG,0.5100698725852857,Trajectories
DIAG,0.5104808877928483,"RMSprop
SDE (Ours)
SDE (Malladi et al.) 0 10 20 30 40 50 60 70 80"
DIAG,0.510891903000411,"0
250
500
750 1000 1250 1500 1750 2000
Iterations 10
4 10
3 10
2 10
1 100 101 102 Loss"
DIAG,0.5113029182079737,Losses
DIAG,0.5117139334155364,"RMSprop
SDE (Ours)
SDE (Malladi et al.)"
DIAG,0.5121249486230991,"1.0
0.5
0.0
0.5
1.0
X1 1.0 0.5 0.0 0.5 1.0 X2"
DIAG,0.5125359638306617,Trajectories
DIAG,0.5129469790382244,"RMSprop
SDE (Ours)
SDE (Malladi et al.) 0.3 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4"
DIAG,0.5133579942457871,"0
200
400
600
800 1000 1200 1400 1600
Iterations 10
5 10
4 10
3 10
2 10
1 100 Loss"
DIAG,0.5137690094533498,Losses
DIAG,0.5141800246609125,"RMSprop
SDE (Ours)
SDE (Malladi et al.)"
DIAG,0.5145910398684751,"Figure 7: The first two subfigures on the left compare our SDE, that from Malladi et al. (2022), and
RMSprop in terms of trajectories and f(x), respectively, for a convex quadratic function. The others
subfigures do the same for an embedded saddle and one clearly observes that our derived SDE better
matches RMSprop."
DIAG,0.5150020550760378,"In this subsection, we provide our formal derivation of an SDE model for RMSprop. Let us consider
722"
DIAG,0.5154130702836005,"the stochastic process Lt := (Xt, Vt) ∈Rd × Rd defined as the solution of
723"
DIAG,0.5158240854911632,"dXt = −P −1
t
(∇f(Xt)dt + √ηΣ(Xt)
1
2 dWt)
(86)"
DIAG,0.5162351006987258,"dVt = ρ((∇f(Xt))2 + diag(Σ(Xt)) −Vt))dt,
(87)"
DIAG,0.5166461159062885,"where β = 1 −ηρ, ρ = O(1), and Pt := diag (Vt)"
DIAG,0.5170571311138512,"1
2 + ϵId.
724"
DIAG,0.5174681463214139,"Remark C.15. We observe that the term in blue is the only difference w.r.t. the SDE derived in
725"
DIAG,0.5178791615289766,"(Malladi et al., 2022) (see Theorem D.2): This is extremely relevant when the gradient size is not
726"
DIAG,0.5182901767365392,"negligible. Figure 7 shows the comparison between our SDE, the one derived in (Malladi et al., 2022),
727"
DIAG,0.5187011919441019,"and RMSprop itself: It is clear that even on simple landscapes, our SDE matches the algorithm much
728"
DIAG,0.5191122071516646,"better. Importantly, one can observe that the SDE derived in (Malladi et al., 2022) is only slightly
729"
DIAG,0.5195232223592273,"worse than ours at the end of the dynamics: As we show in Lemma C.17, Theorem D.2 is a corollary
730"
DIAG,0.51993423756679,"of Theorem C.16 when ∇f(x) = O(√η): It only describes the dynamics where the gradient is
731"
DIAG,0.5203452527743526,"vanishing. In Figure 8, we compare the two SDEs in question with RMSprop on an MLP, a CNN, a
732"
DIAG,0.5207562679819153,"ResNet, and a Transformer: Our SDE exhibits a superior description of the dynamics.
733"
DIAG,0.521167283189478,"0
250
500
750 1000 1250 1500 1750 2000
Iterations 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Loss"
DIAG,0.5215782983970407,Losses - DNN
DIAG,0.5219893136046033,"RMSprop
SDE (Ours)
SDE (Malladi et al.)"
DIAG,0.522400328812166,"0
250
500
750 1000 1250 1500 1750 2000
Iterations 0 1 2 3 4 Loss"
DIAG,0.5228113440197287,Losses - CNN
DIAG,0.5232223592272914,"RMSprop
SDE (Ours)
SDE (Malladi et al.)"
DIAG,0.5236333744348541,"0
250
500
750 1000 1250 1500 1750 2000
Iterations 10
1 100 101 Loss"
DIAG,0.5240443896424167,Losses - Transformer
DIAG,0.5244554048499794,"RMSprop
SDE (Ours)
SDE (Malladi et al.)"
DIAG,0.5248664200575421,"100
101
102
103
Iterations 0 5 10 15 20 25 Loss"
DIAG,0.5252774352651048,Losses - ResNet
DIAG,0.5256884504726674,"RMSprop
SDE (Ours)
SDE (Malladi et al.)"
DIAG,0.5260994656802301,"Figure 8: We compare our SDE, that from Malladi et al. (2022), and RMSprop in terms of f(x): The
first is an MLP on the Breast Cancer dataset, the second a CNN on MNIST, the third a Transformer
on MNIST, and the last a ResNet on CIFAR-10: Ours match the algorithms better."
DIAG,0.5265104808877928,"The following theorem guarantees that such a process is a 1-order SDE of the discrete-time algorithm
734"
DIAG,0.5269214960953555,"of RMSprop
735"
DIAG,0.5273325113029182,"xk+1 = xk −η ∇fγk(xk)
√vk+1 + ϵId
(88)"
DIAG,0.5277435265104808,"vk+1 = βvk + (1 −β) (∇fγk(xk))2
(89)"
DIAG,0.5281545417180435,"with (x0, v0) ∈Rd × Rd, η ∈R>0 is the step size, β = 1 −ρη for ρ = O(1), the mini-batches {γk}
736"
DIAG,0.5285655569256063,"are modelled as i.i.d. random variables uniformly distributed on {1, · · · , N}, and of size B ≥1.
737"
DIAG,0.528976572133169,"Theorem C.16 (Stochastic modified equations). Let 0 < η < 1, T > 0 and set N = ⌊T/η⌋.
Let lk := (xk, vk) ∈Rd × Rd, 0 ≤k ≤N denote a sequence of RMSprop iterations defined
by Eq. (88). Consider the stochastic process Lt defined in Eq. (86) and fix some test function
g ∈G and suppose that g and its partial derivatives up to order 6 belong to G.
Then, under Assumption C.2 and ρ = O(1) there exists a constant C > 0 independent of η
such that for all k = 0, 1, . . . , N, we have"
DIAG,0.5293875873407317,"|Eg (Lkη) −Eg (lk)| ≤Cη.
That is, the SDE (86) is an order 1 weak approximation of the RMSprop iterations (88). 738"
DIAG,0.5297986025482943,"Proof. The proof is virtually identical to that of Theorem C.5. Therefore, we only report the key
739"
DIAG,0.530209617755857,"steps necessary to conclude the thesis. First of all, we observe that since β = 1 −ηρ
740"
DIAG,0.5306206329634197,"vk+1 −vk = −ηρ

vk −(∇fγk(xk))2
.
(90)"
DIAG,0.5310316481709824,"Then,
741"
DIAG,0.531442663378545,"1
√vk+1
= s"
DIAG,0.5318536785861077,"vk
vk+1"
VK,0.5322646937936704,"1
vk
= s"
VK,0.5326757090012331,vk+1 + O(η) vk+1
VK,0.5330867242087958,"1
vk
= s"
VK,0.5334977394163584,1 + O(η) vk+1 r
VK,0.5339087546239211,"1
vk
∼
r"
VK,0.5343197698314838,"1
vk
(1 + O(η)).
(91)"
VK,0.5347307850390465,"Therefore, we work with the following algorithm as all the approximations below only carry an
742"
VK,0.5351418002466092,"additional error of order O(η2), which we can ignore. Therefore, we have that
743"
VK,0.5355528154541718,"xk+1 −xk = −η ∇fγk(xk)
√vk + ϵId
(92)"
VK,0.5359638306617345,"vk −vk−1 = −ηρ

vk−1 −
 
∇fγk−1(xk−1)
2
.
(93)"
VK,0.5363748458692972,"Therefore, if ∇fγj(xj) = ∇f(xj) + Zj(xj), E[Zj(xj)] = 0, and Cov(Zj(xj)) = Σ(xj)
744"
VK,0.5367858610768599,1. E[xk+1 −xk] = −η diag(vk + ϵId)−1
VK,0.5371968762844225,"2 ∇f(xk) ;
745"
VK,0.5376078914919852,"2. E[vk −vk−1] = ηρ
h
(∇f(xk−1))2 + diag(Σ(xk)) −vk−1
i
.
746"
VK,0.5380189066995479,"Then, we have that if Φk :=
∇f(xk)
√vk+ϵId −
∇fγk (xk)
√vk+ϵId
747 1."
VK,0.5384299219071106,"E[(xk+1 −xk)(xk+1 −xk)⊤] = E[(xk+1 −xk)]E[(xk+1 −xk)]⊤
(94)"
VK,0.5388409371146733,"+ η2E
h
(Φk) (Φk)⊤i
(95)"
VK,0.5392519523222359,"= E[(xk+1 −xk)]E[(xk+1 −xk)]⊤
(96)"
VK,0.5396629675297986,"+ η2(diag(vk) + ϵId)−1Σ(xk);
(97)"
VK,0.5400739827373613,"2. E[(vk −vk−1)(vk −vk−1)⊤] = E[(vk −vk−1)]E[(vk −vk−1)]⊤+ O(ρη2);
748"
VK,0.540484997944924,"3. E[(xk+1 −xk)(vk −vk−1)⊤] = E[(xk+1 −xk)]E[(vk −vk−1)⊤] + 0.
749"
VK,0.5408960131524866,"Therefore
750"
VK,0.5413070283600493,"dXt = −P −1
t
(∇f(Xt)dt + √ηΣ(Xt)
1
2 dWt)
(98)"
VK,0.541718043567612,"dVt = ρ(((∇f(Xt))2 + diag(Σ(Xt)) −Vt))dt.
(99) 751"
VK,0.5421290587751747,"Lemma C.17. If (∇f(x))2 = O(η), Theorem D.2 is a Corollary of Theorem C.16.
752"
VK,0.5425400739827374,"Proof. In the proof of Theorem C.16, one drops the term η(∇f(x))2 as it is of order η2.
753"
VK,0.5429510891903,"Corollary C.18. Under the assumptions of Theorem C.16 with Σ(x) = σ2Id, ˜η = κη, ˜B = Bδ, and
754"
VK,0.5433621043978627,"˜ρ = αρ,
755"
VK,0.5437731196054254,dXt = κ diag(Vt)−1
VK,0.5441841348129881,"2

−∇f(Xt)dt + 1
√ δ r η"
VK,0.5445951500205508,B σIddWt
VK,0.5450061652281134,"
(100)"
VK,0.5454171804356761,dVt = α
VK,0.5458281956432388,"κ ρ

(∇f(Xt))2 + σ2"
VK,0.5462392108508015,Bδ 1 −Vt
VK,0.5466502260583641,"
dt.
(101)"
VK,0.5470612412659268,"Lemma C.19 (Scaling Rule at Convergence). Under the assumptions of Corollary C.18, f is µ-
756"
VK,0.5474722564734895,"strongly convex, Lτ := Tr(∇2f(x)), and (∇f(x))2 = O(η), the asymptotic dynamics of the iterates
757"
VK,0.5478832716810522,"of RMSprop satisfies the classic scaling rule κ =
√"
VK,0.5482942868886149,"δ because
758"
VK,0.5487053020961775,"E[f(Xt) −f(X∗)]
t→∞
≤
ησLτ
4µ
√ B
κ
√"
VK,0.5491163173037402,"δ
.
(102)"
VK,0.5495273325113029,"By enforcing that the speed of Vt matches that of Xt, one needs ˜ρ = κ2ρ, which implies ˜β =
759"
VK,0.5499383477188656,"1 −κ2(1 −β).
760"
VK,0.5503493629264283,"Proof of Lemma C.19. In order to recover the scaling of β, we enforce that the rate at which Vt
761"
VK,0.5507603781339909,"converges to its limit matches the speed of Xt: We need ˜ρ = κ2ρ, which recovers the classic scaling
762"
VK,0.5511713933415536,"˜β = 1 −κ2(1 −β). Additionally, since (∇f(x))2 = O(η) we have that
763"
VK,0.5515824085491163,dXt = κ diag(Vt)−1
VK,0.551993423756679,"2

−∇f(Xt)dt + 1
√ δ r η"
VK,0.5524044389642416,B σIddWt
VK,0.5528154541718043,"
(103)"
VK,0.553226469379367,"dVt = κρ
 σ2"
VK,0.5536374845869297,Bδ 1 −Vt
VK,0.5540484997944924,"
dt.
(104)"
VK,0.554459515002055,"Therefore, Vt
t→∞
→
σ2
Bδ1, meaning that under these conditions:
764"
VK,0.5548705302096177,dXt = − √ Bδκ
VK,0.5552815454171804,"σ
∇f(Xt)dt + κ√ηIddWt,
(105)"
VK,0.5556925606247431,"which satisfies the following for µ-strongly convex functions
765"
VK,0.5561035758323057,dE[f(Xt) −f(X∗)] ≤−2κµ √
VK,0.5565145910398684,"Bδ
σ
E[f(Xt) −f(X∗)]dt + κ2ηLτ"
VK,0.5569256062474311,"2
dt,
(106)"
VK,0.5573366214549939,"meaning that E[f(Xt) −f(X∗)]
t→∞
≤
ησLτ
4µ
√ B
κ
√"
VK,0.5577476366625566,"δ.
766"
VK,0.5581586518701191,Since the asymptotic the loss is η
VK,0.5585696670776819,"2
Lτ σ
2µ
√ B
κ
√"
VK,0.5589806822852446,"δ does not depend on κ and δ if
κ
√"
VK,0.5593916974928073,"δ = 1, we recover the
767"
VK,0.55980271270037,"classic scaling rule.
768"
VK,0.5602137279079326,"Remark: Under the same conditions, SGD satisfies
769"
VK,0.5606247431154953,"dXt = −κ∇f(Xt)dt + κ 1
√ δ r η"
VK,0.561035758323058,"B σIddWt
(107)"
VK,0.5614467735306207,"and therefore
770"
VK,0.5618577887381833,E[f(Xt) −f(X∗)] ≤(f(X0) −f(X∗))e−2µκt + η
VK,0.562268803945746,"2
Lτσ2"
VK,0.5626798191533087,"2µB
κ
δ
 
1 −e−2µκt
,
(108)"
VK,0.5630908343608714,meaning that asymptotically the loss is η
VK,0.5635018495684341,"2
Lτ σ2"
VK,0.5639128647759967,"2µB
κ
δ which does not depend on κ and δ if κ"
VK,0.5643238799835594,"δ = 1.
771"
VK,0.5647348951911221,Lemma C.20. For f(x) := x⊤Hx
VK,0.5651459103986848,"2
, the stationary distribution of RMSprop is (E[X∞]], Cov(X∞)) =
772

0, η"
VK,0.5655569256062475,"2Σ
1
2 H−1
.
773"
VK,0.5659679408138101,"Proof. As (∇f(x))2 = O(η) and t →∞, we have
774"
VK,0.5663789560213728,dXt = −Σ−1
VK,0.5667899712289355,"2 HXtdt + √ηIddWt
(109)"
VK,0.5672009864364982,"which implies that
775"
VK,0.5676120016440608,Xt = e−Σ−1
HT,0.5680230168516235,"2 Ht

X0 + √η
Z t"
HT,0.5684340320591862,"0
eΣ−1"
HSDWS,0.5688450472667489,2 HsdWs
HSDWS,0.5692560624743116,"
.
(110)"
HSDWS,0.5696670776818742,"The thesis follows from the martingale property of Brownian motion and the Itô isometry.
776"
HSDWS,0.5700780928894369,"C.4
RMSpropW
777"
HSDWS,0.5704891080969996,"1
400
800
1200
1600
2000
Iterations 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Loss"
HSDWS,0.5709001233045623,Losses - DNN
HSDWS,0.5713111385121249,"AdamW
SDE"
HSDWS,0.5717221537196876,"1
400
800
1200
1600
2000
Iterations 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 Loss"
HSDWS,0.5721331689272503,Losses - CNN
HSDWS,0.572544184134813,"AdamW
SDE"
HSDWS,0.5729551993423757,"1
400
800
1200
1600
2000
Iterations 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Loss"
HSDWS,0.5733662145499383,Losses - DNN
HSDWS,0.573777229757501,"RMSpropW
SDE"
HSDWS,0.5741882449650637,"1
400
800
1200
1600
2000
Iterations 0 1 2 3 4 5 Loss"
HSDWS,0.5745992601726264,Losses - CNN
HSDWS,0.5750102753801891,"RMSpropW
SDE"
HSDWS,0.5754212905877517,"Figure 9: The first two represent the comparison between AdamW and its SDE in terms of f(x).
The other two do the same for RMSpropW. In both cases, the first is an MLP on the Breast Cancer
Dataset and the second a CNN on MNIST: Our SDEs match the respective optimizers."
HSDWS,0.5758323057953144,"In this subsection, we derive the SDE of RMSpropW defined as
778"
HSDWS,0.5762433210028771,"xk+1 = xk −η ∇fγk(xk)
√vk+1 + ϵId
−ηγxk
(111)"
HSDWS,0.5766543362104398,"vk+1 = βvk + (1 −β) (∇fγk(xk))2
(112)"
HSDWS,0.5770653514180024,"with (x0, v0) ∈Rd×Rd, η ∈R>0 is the step size, β = 1−ρη for ρ = O(1), γ > 0, the mini-batches
779"
HSDWS,0.5774763666255651,"{γk} are modelled as i.i.d. random variables uniformly distributed on {1, · · · , N}, and of size B ≥1.
780"
HSDWS,0.5778873818331278,"Theorem C.21. Under the same assumptions as Theorem C.16, the SDE of RMSpropW is
781"
HSDWS,0.5782983970406905,"dXt = −P −1
t
(∇f(Xt)dt + √ηΣ(Xt)
1
2 dWt) −γXtdt
(113)"
HSDWS,0.5787094122482532,"dVt = ρ((∇f(Xt))2 + diag(Σ(Xt)) −Vt))dt,
(114)"
HSDWS,0.5791204274558158,"where β = 1 −ηρ, ρ = O(1), γ > 0, and Pt := diag (Vt)"
HSDWS,0.5795314426633785,"1
2 + ϵId.
782"
HSDWS,0.5799424578709412,"Proof. The proof is the same as the of Theorem C.16 and the only difference is that ηγxk is
783"
HSDWS,0.5803534730785039,"approximated with γXtdt.
784"
HSDWS,0.5807644882860666,"Figure 4 and Figure 9 validate this result on a variety of architectures and datasets.
785"
HSDWS,0.5811755034936292,"Corollary C.22. Under the assumptions of Theorem C.21 with Σ(x) = σ2Id, ˜η = κη, ˜B = Bδ, and
786"
HSDWS,0.5815865187011919,"˜ρ = αρ, and ˜γ = ξγ,
787"
HSDWS,0.5819975339087546,dXt = κ diag(Vt)−1
HSDWS,0.5824085491163173,"2

−∇f(Xt)dt + 1
√ δ r η"
HSDWS,0.5828195643238799,B σIddWt
HSDWS,0.5832305795314426,"
−ξγκXtdt
(115)"
HSDWS,0.5836415947390053,dVt = α
HSDWS,0.584052609946568,"κ ρ

(∇f(Xt))2 + σ2"
HSDWS,0.5844636251541308,Bδ 1 −Vt
HSDWS,0.5848746403616933,"
dt.
(116)"
HSDWS,0.585285655569256,"Lemma C.23 (Scaling Rule at Convergence). Under the assumptions of Corollary C.22, f is µ-
788"
HSDWS,0.5856966707768188,"strongly convex and L-smooth, Lτ := Tr(∇2f(x)), and (∇f(x))2 = O(η), the asymptotic dynamics
789"
HSDWS,0.5861076859843815,"of the iterates of RMSpropW satisfies the novel scaling rule if κ =
√"
HSDWS,0.586518701191944,"δ and ξ = κ because
790"
HSDWS,0.5869297163995068,"E[f(Xt) −f(X∗)]
t→∞
≤
ηLτσL"
HSDWS,0.5873407316070695,"2
κ
2µ
√"
HSDWS,0.5877517468146322,"BδL + σξγ(L + µ)
.
(117)"
HSDWS,0.5881627620221949,"By enforcing that the speed of Vt matches that of Xt, one needs ˜ρ = κ2ρ, which implies ˜β =
791"
HSDWS,0.5885737772297575,"1 −κ2(1 −β).
792"
HSDWS,0.5889847924373202,"Proof of Lemma C.23. In order to recover the scaling of β, we enforce that the rate at which Vt
793"
HSDWS,0.5893958076448829,"converges to its limit matches the speed of Xt: We need ˜ρ = κ2ρ, which recovers the classic scaling
794"
HSDWS,0.5898068228524456,"˜β = 1 −κ2(1 −β). Additionally, since (∇f(x))2 = O(η) we have that
795"
HSDWS,0.5902178380600083,dXt = κ diag(Vt)−1
HSDWS,0.5906288532675709,"2

−∇f(Xt)dt + 1
√ δ r η"
HSDWS,0.5910398684751336,B σIddWt
HSDWS,0.5914508836826963,"
−κξγXtdt
(118)"
HSDWS,0.591861898890259,"dVt = κρ
 σ2"
HSDWS,0.5922729140978216,Bδ 1 −Vt
HSDWS,0.5926839293053843,"
dt.
(119)"
HSDWS,0.593094944512947,"Therefore, Vt
t→∞
→
σ2
Bδ1, meaning that under these conditions:
796"
HSDWS,0.5935059597205097,dXt = − √ Bδκ
HSDWS,0.5939169749280724,"σ
∇f(Xt)dt + κ√ηIddWt −κξγXtdt,
(120)"
HSDWS,0.594327990135635,"which satisfies the following for µ-strongly convex and L-smooth functions
797"
HSDWS,0.5947390053431977,dE[f(Xt) −f(X∗)] ≤κ  2µ √
HSDWS,0.5951500205507604,"Bδ
σ
+ ξγ

1 + µ L !"
HSDWS,0.5955610357583231,E[f(Xt) −f(X∗)]dt + κ2ηLτ
HSDWS,0.5959720509658858,"2
dt,
(121)"
HSDWS,0.5963830661734484,"meaning that E[f(Xt) −f(X∗)]
t→∞
≤
ηLτ σL"
HSDWS,0.5967940813810111,"2
κ
2µ
√"
HSDWS,0.5972050965885738,"BδL+σξγ(L+µ).
798"
HSDWS,0.5976161117961365,Since the asymptotic the loss ηLτ σL
HSDWS,0.5980271270036991,"2
κ
2µ
√"
HSDWS,0.5984381422112618,"BδL+σξγ(L+µ) does not depend on κ and δ and ξ if κ = ξ =
799 √"
HSDWS,0.5988491574188245,"δ, we recover the novel scaling rule.
800"
HSDWS,0.5992601726263872,"Lemma
C.24.
For
f(x)
:=
x⊤Hx"
HSDWS,0.5996711878339499,"2
,
the
stationary
distribution
of
RMSpropW
is
801"
HSDWS,0.6000822030415125,"(E[X∞]], Cov(X∞)) =

0, η"
HSDWS,0.6004932182490752,2(HΣ−1
HSDWS,0.6009042334566379,"2 + γId)−1
.
802"
HSDWS,0.6013152486642006,"Proof. As (∇f(x))2 = O(η) and t →∞, we have
803"
HSDWS,0.6017262638717632,dXt = −Σ−1
HSDWS,0.6021372790793259,"2 HXtdt + √ηIddWt −γXtdt
(122)"
HSDWS,0.6025482942868886,"which implies that
804"
HSDWS,0.6029593094944513,Xt = e−(Σ−1
HSDWS,0.603370324702014,"2 H+γId)t

X0 + √η
Z t"
HSDWS,0.6037813399095766,"0
e(Σ−1"
HSDWS,0.6041923551171393,2 H+γId)sdWs
HSDWS,0.604603370324702,"
.
(123)"
HSDWS,0.6050143855322647,"The thesis follows from the martingale property of Brownian motion and the Itô isometry.
805"
HSDWS,0.6054254007398274,"1
0
1
X1 1.0 0.5 0.0 0.5 1.0 X2"
HSDWS,0.60583641594739,Trajectories
HSDWS,0.6062474311549527,"Adam
SDE (Ours)
SDE (Malladi et al.) 0 3 6 9 12 15 18 21 24"
HSDWS,0.6066584463625154,"0
10000
20000
30000
40000
50000
Iterations 10
5 10
4 10
3 10
2 10
1 100 101 Loss"
HSDWS,0.6070694615700781,Losses
HSDWS,0.6074804767776407,"Adam
SDE (Ours)
SDE (Malladi et al.)"
HSDWS,0.6078914919852034,"1
0
1
X1 0.6 0.4 0.2 0.0 0.2 0.4 0.6 X2"
HSDWS,0.6083025071927661,Trajectories
HSDWS,0.6087135224003288,"Adam
SDE (Ours)
SDE (Malladi et al.) 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8"
HSDWS,0.6091245376078915,"0
5000
10000
15000
20000
25000
Iterations 10
4 10
3 10
2 10
1 100 Loss"
HSDWS,0.6095355528154541,Losses
HSDWS,0.6099465680230168,"Adam
SDE (Ours)
SDE (Malladi et al.)"
HSDWS,0.6103575832305795,"Figure 10: The first two on the left compare our SDE, that from Malladi et al. (2022), and Adam in
terms of trajectories and f(x), respectively, for a convex quadratic function. The others do the same
for an embedded saddle: Ours clearly matches Adam better."
HSDWS,0.6107685984381422,"C.5
Formal derivation - Adam
806"
HSDWS,0.6111796136457049,"In this subsection, we provide our formal derivation of an SDE model for Adam. Let us consider the
807"
HSDWS,0.6115906288532675,"stochastic process Lt := (Xt, Mt, Vt) ∈Rd × Rd × Rd defined as the solution of
808"
HSDWS,0.6120016440608302,dXt = − p
HSDWS,0.612412659268393,"γ2(t)
γ1(t) P −1
t
(Mt + ηρ1 (∇f (Xt) −Mt))dt
(124)"
HSDWS,0.6128236744759556,"dMt = ρ1 (∇f (Xt) −Mt) dt + √ηρ1Σ1/2 (Xt) dWt
(125)"
HSDWS,0.6132346896835182,"dVt = ρ2
 "
HSDWS,0.613645704891081,"(∇f(Xt))2 + diag (Σ (Xt)) −Vt

dt,
(126)"
HSDWS,0.6140567200986436,"where βi = 1 −ηρi, γi(t) = 1 −e−ρit, ρ1 = O(η−ζ) s.t. ζ ∈(0, 1), ρ2 = O(1), and Pt =
809"
HSDWS,0.6144677353062064,"diag √Vt + ϵ
p"
HSDWS,0.6148787505137691,"γ2(t)Id.
810"
HSDWS,0.6152897657213316,"Remark C.25. The terms in purple and in blue are the two differences w.r.t. that of (Malladi et al.,
811"
HSDWS,0.6157007809288944,"2022) which is reported in Theorem D.5. The first appears because we assume realistic values of β1
812"
HSDWS,0.6161117961364571,"while the second appears because we allow the gradient size to be non-negligible. For two simple
813"
HSDWS,0.6165228113440198,"landscapes, Figure 10 compares our SDE and that of Malladi et al. (2022) with Adam: In both
814"
HSDWS,0.6169338265515824,"cases, the first part of the dynamics is perfectly represented only by our SDE. While the discrepancy
815"
HSDWS,0.6173448417591451,"between the SDE of (Malladi et al., 2022) and Adam is asymptotically negligible in the convex
816"
HSDWS,0.6177558569667078,"setting, we observe that in the nonconvex case, it converges to a different local minimum than ours
817"
HSDWS,0.6181668721742705,"and of Adam. Finally, Theorem D.5 is a corollary of ours when (∇f(x))2 = O(η) and ρ1 = O(1):
818"
HSDWS,0.6185778873818332,"It only describes the dynamics where the gradient to noise ratio is vanishing and only for unrealistic
819"
HSDWS,0.6189889025893958,"values of β1 = 1 −ηρ1. In Figure 11, we compare the dynamics of our SDE, that of Malladi et al.
820"
HSDWS,0.6193999177969585,"(2022), and Adam on an MLP, a CNN, a ResNet, and a Transformer. One can clearly see that our
821"
HSDWS,0.6198109330045212,"SDE more accurately captures the dynamics. Details on these experiments are available in Appendix
822"
HSDWS,0.6202219482120839,"F.
823"
HSDWS,0.6206329634196466,"The following theorem guarantees that such a process is a 1-order SDE of the discrete-time algorithm
824"
HSDWS,0.6210439786272092,"of Adam
825"
HSDWS,0.6214549938347719,"vk+1 = β2vk + (1 −β2) (∇fγk(xk))2
(127)
mk+1 = β1mk + (1 −β1)∇fγk(xk)
(128)"
HSDWS,0.6218660090423346,"ˆmk = mk
 
1 −βk
1
−1
(129)"
HSDWS,0.6222770242498973,"ˆvk = vk
 
1 −βk
2
−1
(130)"
HSDWS,0.6226880394574599,"xk+1 = xk −η
ˆmk+1
p"
HSDWS,0.6230990546650226,"ˆvk+1 + ϵId
,
(131)"
HSDWS,0.6235100698725853,"with (x0, m0, v0) ∈Rd × Rd × Rd, η ∈R>0 is the step size, βi = 1 −ρiη for ρ1 = O(η−ζ) s.t.
826"
HSDWS,0.623921085080148,"ζ ∈(0, 1), ρ2 = O(1), the mini-batches {γk} are modelled as i.i.d. random variables uniformly
827"
HSDWS,0.6243321002877107,"distributed on {1, · · · , N}, and of size B ≥1.
828"
HSDWS,0.6247431154952733,"0
250
500
750 1000 1250 1500 1750 2000
Iterations 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Loss"
HSDWS,0.625154130702836,Losses - DNN
HSDWS,0.6255651459103987,"Adam
SDE (Ours)
SDE (Malladi et al.)"
HSDWS,0.6259761611179614,"0
250
500
750 1000 1250 1500 1750 2000
Iterations 0 1 2 3 4 5 6 Loss"
HSDWS,0.6263871763255241,Losses - CNN
HSDWS,0.6267981915330867,"Adam
SDE (Ours)
SDE (Malladi et al.)"
HSDWS,0.6272092067406494,"0
250
500
750 1000 1250 1500 1750 2000
Iterations 0 10 20 30 40 Loss"
HSDWS,0.6276202219482121,Losses - Transformer
HSDWS,0.6280312371557748,"Adam
SDE (Ours)
SDE (Malladi et al.)"
HSDWS,0.6284422523633374,"100
101
102
103
Iterations 100 101 102 Loss"
HSDWS,0.6288532675709001,Losses - ResNet
HSDWS,0.6292642827784628,"Adam
SDE (Ours)
SDE (Malladi et al.)"
HSDWS,0.6296752979860255,"Figure 11: We compare our SDE, that from Malladi et al. (2022), and Adam in terms of f(x): The
first is an MLP on the Breast Cancer dataset, the second a CNN on MNIST, the third a Transformer
on MNIST, and the last a ResNet on CIFAR-10: Ours match the algorithms better."
HSDWS,0.6300863131935882,"Theorem C.26 (Stochastic modified equations). Let 0 < η < 1, T > 0 and set N = ⌊T/η⌋.
Let lk := (xk, mk, vk) ∈Rd × Rd × Rd, 0 ≤k ≤N denote a sequence of Adam iterations
defined by Eq. (127). Consider the stochastic process Lt defined in Eq. (124) and fix some
test function g ∈G and suppose that g and its partial derivatives up to order 6 belong to G.
Then, under Assumption C.2 ρ1 = O(η−ζ) s.t. ζ ∈(0, 1), while ρ2 = O(1), there exists a
constant C > 0 independent of η such that for all k = 0, 1, . . . , N, we have"
HSDWS,0.6304973284011508,"|Eg (Lkη) −Eg (lk)| ≤Cη.
That is, the SDE (124) is an order 1 weak approximation of the Adam iterations (127). 829"
HSDWS,0.6309083436087135,"Proof. The proof is virtually identical to that of Theorem C.5. Therefore, we only report the key
830"
HSDWS,0.6313193588162762,"steps necessary to conclude the thesis. First of all, we observe that since β1 = 1 −ηρ1
831"
HSDWS,0.6317303740238389,"vk+1 −vk = −ηρ1

vk −(∇fγk(xk))2
.
(132)"
HSDWS,0.6321413892314015,"Then,
832"
HSDWS,0.6325524044389642,"1
√vk+1
= s"
HSDWS,0.6329634196465269,"vk
vk+1"
VK,0.6333744348540896,"1
vk
= s"
VK,0.6337854500616523,vk+1 + O(η) vk+1
VK,0.6341964652692149,"1
vk
= s"
VK,0.6346074804767776,1 + O(η) vk+1 r
VK,0.6350184956843403,"1
vk
∼
r"
VK,0.635429510891903,"1
vk
(1 + O(η)).
(133)"
VK,0.6358405260994657,"Therefore, we work with the following algorithm as all approximations only carry an additional error
833"
VK,0.6362515413070283,"of order O(η2), which we can ignore. Therefore, we have that
834"
VK,0.636662556514591,"vk −vk−1 = −ηρ2

vk−1 −
 
∇fγk−1(xk−1)
2
(134)"
VK,0.6370735717221537,"mk+1 −mk = −ηρ1 (mk −∇fγk(xk))
(135)"
VK,0.6374845869297164,"ˆmk = mk
 
1 −βk
1
−1
(136)"
VK,0.637895602137279,"ˆvk = vk
 
1 −βk
1
−1
(137)"
VK,0.6383066173448417,"xk+1 −xk = −
η
√vk + ϵId p"
VK,0.6387176325524044,1 −(1 −ηρ2)k
VK,0.6391286477599671,"1 −(1 −ηρ1)k+1 (mk + ηρ1(∇fγk(xk) −mk)).
(138)"
VK,0.6395396629675298,"Therefore, if ∇fγj(xj) = ∇f(xj) + Zj(xj) and E[Zj(xj)] = 0, and Cov(Zj(xj)) = Σ(xj), we
835"
VK,0.6399506781750924,"have that
836"
VK,0.6403616933826551,"1. E[vk −vk−1] = ηρ2
h
(∇f(xk−1))2 + diag(Σ(xk)) −vk−1
i
;
837"
VK,0.6407727085902178,"2. E[mk+1 −mk] = ηρ1 [∇f(xk) −mk] ;
838"
VK,0.6411837237977805,"3. E[xk+1 −xk] = −
η
√vk+ϵId √"
VK,0.6415947390053433,1−(1−ηρ2)k
VK,0.6420057542129058,"1−(1−ηρ1)k+1 (mk + ηρ1(∇f(xk) −mk)) .
839"
VK,0.6424167694204685,"Then, we have
840"
VK,0.6428277846280313,"1. E[(xk+1 −xk)(xk+1 −xk)⊤] = E[(xk+1 −xk)]E[(xk+1 −xk)]⊤+ O(η4ρ2
1);
841"
VK,0.643238799835594,"2. E[(xk+1 −xk)(mk −mk−1)⊤] = E[(xk+1 −xk)]E[(mk −mk−1)]⊤+ 0;
842"
VK,0.6436498150431565,"3. E[(xk+1 −xk)(vk −vk−1)⊤] = E[(xk+1 −xk)]E[(vk −vk−1)]⊤+ 0;
843"
VK,0.6440608302507193,"4. E[(vk −vk−1)(vk −vk−1)⊤] = E[(vk −vk−1)]E[(vk −vk−1)]⊤+ O(η2ρ2
2);
844"
VK,0.644471845458282,"5. E[(mk −mk−1)(mk −mk−1)⊤] = E[(mk −mk−1)]E[(mk −mk−1)]⊤+ η2ρ2
1Σ(xk−1);
845"
VK,0.6448828606658447,"6. E[(vk −vk−1)(mk −mk−1)⊤] = E[(vk −vk−1)]E[(mk −mk−1)]⊤+ O(η2ρ1ρ2).
846"
VK,0.6452938758734074,"Since in real-world applications, ρ1 = O(η−ζ) s.t. ζ ∈(0, 1), while ρ2 = O(1), we have
847"
VK,0.64570489108097,dXt = − p
VK,0.6461159062885327,"γ2(t)
γ1(t) P −1
t
(Mt + ηρ1 (∇f (Xt) −Mt))dt
(139)"
VK,0.6465269214960954,"dMt = ρ1 (∇f (Xt) −Mt) dt + √ηρ1Σ1/2 (Xt) dWt
(140)"
VK,0.6469379367036581,"dVt = ρ2
 
(∇f(Xt))2 + diag (Σ (Xt)) −Vt

dt.
(141)"
VK,0.6473489519112207,"where βi = 1 −ηρi, γi(t) = 1 −e−ρit, and Pt = diag √Vt + ϵ
p"
VK,0.6477599671187834,"γ2(t)Id.
848"
VK,0.6481709823263461,"Corollary C.27. Under the assumptions of Theorem C.26 with Σ(x) = σ2Id, ˜η = κη, ˜B = Bδ,
849"
VK,0.6485819975339088,"˜ρ1 = α1ρ1, and ˜ρ2 = α2ρ2
850"
VK,0.6489930127414715,dXt = −κ p
VK,0.6494040279490341,"γ2(t)
γ1(t) P −1
t
(Mt + ηα1ρ1 (∇f (Xt) −Mt))dt
(142)"
VK,0.6498150431565968,dMt = α1ρ1
VK,0.6502260583641595,"κ
(∇f (Xt) −Mt) dt + √η α1ρ1 κ
σ
√"
VK,0.6506370735717222,"Bδ
IddWt
(143)"
VK,0.6510480887792849,dVt = α2ρ2 κ
VK,0.6514591039868475,"
(∇f(Xt))2 + σ2"
VK,0.6518701191944102,Bδ Id −Vt
VK,0.6522811344019729,"
dt.
(144)"
VK,0.6526921496095356,"Lemma C.28. Under the assumptions of Corollary C.27, f is µ-strongly convex, Lτ := Tr(∇2f(x)),
851"
VK,0.6531031648170982,"and (∇f(x))2 = O(η), the asymptotic dynamics of the iterates of Adam satisfies the classic scaling
852"
VK,0.6535141800246609,"rule κ =
√"
VK,0.6539251952322236,"δ because E[f(Xt)]
t→∞
≤
ησLτ 4
√ B
κ
√"
VK,0.6543362104397863,"δ. To enforce that the speed of Mt and Vt match that of
853"
VK,0.654747225647349,"Xt, one needs ˜ρi = κ2ρi, which implies ˜βi = 1 −κ2(1 −βi).
854"
VK,0.6551582408549116,"Proof. First of all, we need to ensure that the relative speeds of Xt, Mt, and Vt match. Therefore,
855"
VK,0.6555692560624743,"we select αi = κ2, which recovers the scaling rules for ˜βi = 1 −κ2(1 −βi). Then, recalling that
856"
VK,0.655980271270037,"(∇f(x))2 = O(η), we have that as t →∞, Vt →σ2"
VK,0.6563912864775997,"Bδ, and Mt →∇f(Xt) with high probability.
857"
VK,0.6568023016851624,"Therefore,
858"
VK,0.657213316892725,dXt = −κ √
VK,0.6576243321002877,"Bδ
σ
∇f(Xt)dt
(145)"
VK,0.6580353473078504,"dMt = κ√ηρ1
σ
√"
VK,0.6584463625154131,"Bδ
dWt
(146)"
VK,0.6588573777229757,"dVt = 0.
(147)"
VK,0.6592683929305384,"Therefore, if H(Xt, Vt) := f(Xt) + Lτ δB"
VK,0.6596794081381011,"ρ2σ2
∥Mt∥2
2
2
and ξ ∈(0, 1) we have that by Itô’s lemma,
859"
VK,0.6600904233456638,"dH(Xt, Vt) = −(∇f(Xt))⊤ κ √"
VK,0.6605014385532265,"Bδ
σ
∇f(Xt) !"
VK,0.6609124537607891,"dt +
LτδB"
VK,0.6613234689683518,ρ2σ2 Mt
VK,0.6617344841759145,"
κ√ηρ1
σ
√"
VK,0.6621454993834772,"Bδ
dWt
(148) + 1 2 LτδB ρ2σ2"
VK,0.6625565145910398,"
κ2ηρ2 σ2"
VK,0.6629675297986025,"Bδ dt
(149) = −  κ √ Bδ
σ !"
VK,0.6633785450061652,"∥∇f(Xt)∥2
2dt + Noise + κ2ηλ"
DT,0.6637895602137279,"2
dt
(150) = −  κ √ Bδ
σ"
DT,0.6642005754212906,"!
 
ξ∥∇f(Xt)∥2
2 + (1 −ξ)∥∇f(Xt)∥2
2

dt + Noise + κ2ηλ"
DT,0.6646115906288532,"2
dt (151) ≤−2κµ √"
DT,0.6650226058364159,"Bδ
σ
ξ

f(Xt) + 1 −ξ"
DT,0.6654336210439786,"µξ
∥∇f(Xt)∥2
2
2"
DT,0.6658446362515413,"
dt + Noise + κ2ηλ"
DT,0.666255651459104,"2
dt.
(152)"
DT,0.6666666666666666,Let us now select ξ such that 1−ξ
DT,0.6670776818742293,"µξ
= Lτ δB"
DT,0.667488697081792,"ρ2σ2 , this means that ξ =
σ2ρ2"
DT,0.6678997122893547,"σ2ρ2+µLτ σB ∈(0, 1) and 1"
DT,0.6683107274969173,"ξ =
860"
DT,0.66872174270448,1 + µ Lτ δB
DT,0.6691327579120427,"ρ2σ2 . Since Mt →∇f(Xt), we have that
861"
DT,0.6695437731196054,"dH(Xt, Vt) ≤−2κµ √"
DT,0.6699547883271681,"Bδ
σ
ξH(Xt, Vt)dt + κ2ηλ"
DT,0.6703658035347307,"2
dt + Noise.
(153)"
DT,0.6707768187422934,"Therefore,
862"
DT,0.6711878339498561,E[f(Xt)]
DT,0.6715988491574189,"ξ
=

1 + µLτδB ρ2σ2"
DT,0.6720098643649816,"
E[f(Xt)] ≤E[H(Xt, Vt)]
t→∞
≤
1
ξ
ησLτ
4µ
√ B
κ
√"
DT,0.6724208795725442,"δ
,
(154)"
DT,0.6728318947801069,"which implies that
863"
DT,0.6732429099876696,"E[f(Xt)]
t→∞
≤
ησLτ
4µ
√ B
κ
√"
DT,0.6736539251952323,"δ
.
(155)"
DT,0.6740649404027949,"Analogously,
864"
DT,0.6744759556103576,"E[f(Xt) −f(X∗)]
t→∞
≤
ησLτ
4µ
√ B
κ
√"
DT,0.6748869708179203,"δ
.
(156)"
DT,0.675297986025483,"which gives the square root scaling rule.
865"
DT,0.6757090012330457,"Lemma C.29. Under the assumptions of Corollary C.27, f(x) = x⊤Hx"
DT,0.6761200164406083,"2
s.t. H = diag(λ1, · · · , λd)
866"
DT,0.676531031648171,"and (∇f(x))2 = O(η), the dynamics of Adam implies that f(Xt) →
ησd
4
√ B
κ
√"
DT,0.6769420468557337,"δ.
867"
DT,0.6773530620632964,"Proof. Recalling that (∇f(x))2 = O(η), we have that as t →∞, Vt →σ2"
DT,0.677764077270859,"Bδ, and Mt →λXt with
868"
DT,0.6781750924784217,"high probability. Therefore, in the one-dimensional case
869"
DT,0.6785861076859844,dXt = −κ √
DT,0.6789971228935471,"Bδ
σ
λXtdt
(157)"
DT,0.6794081381011098,"dMt = κ√ηρ1
σ
√"
DT,0.6798191533086724,"Bδ
dWt
(158)"
DT,0.6802301685162351,"dVt = 0.
(159)"
DT,0.6806411837237978,"Therefore, if H(Xt, Vt) := λX2
t
2
+ λδB"
DT,0.6810521989313605,"ρ2σ2
M 2
t
2 , 5 we have that by Itô’s lemma,
870"
DT,0.6814632141389232,"5Inspired by (Barakat and Bianchi, 2021)"
DT,0.6818742293464858,"dH(Xt, Vt) = −(λXt)  κ √"
DT,0.6822852445540485,"Bδ
σ
λXt !"
DT,0.6826962597616112,"dt +
 λδB"
DT,0.6831072749691739,ρ2σ2 Mt
DT,0.6835182901767365,"
κ√ηρ1
σ
√"
DT,0.6839293053842992,"Bδ
dWt
(160) + 1 2  λδB ρ2σ2"
DT,0.6843403205918619,"
κ2ηρ2 σ2"
DT,0.6847513357994246,"Bδ dt
(161)"
DT,0.6851623510069873,= −2κλ √
DT,0.6855733662145499,"Bδ
σ
f(Xt)dt + κ2ηρ2σ2"
DT,0.6859843814221126,"2Bδ
λδB
ρ2σ2 dt + Noise.
(162)"
DT,0.6863953966296753,= −2κλ √
DT,0.686806411837238,"Bδ
σ
f(Xt)dt + κ2ηλ"
DT,0.6872174270448007,"2
dt + Noise.
(163)"
DT,0.6876284422523633,"Once again, since Mt →λXt, we have that
871"
DT,0.688039457459926,"H(Xt, Vt) = λX2
t
2
+ λδB"
DT,0.6884504726674887,"ρ2σ2
M 2
t
2
→λX2
t
2
+ λ λδB"
DT,0.6888614878750514,"ρ2σ2
λX2
t
2
=

1 + λ λδB ρ2σ2"
DT,0.689272503082614," λX2
t
2
=: Kf(Xt)."
DT,0.6896835182901767,"(164)
Therefore,
872"
DT,0.6900945334977394,KdE[f(Xt)] = −2κλ √
DT,0.6905055487053021,"Bδ
σ
E[f(Xt)]dt + κ2ηλ"
DT,0.6909165639128648,"2
dt,
(165)"
DT,0.6913275791204274,"which implies that E[f(Xt)] →
ησ
4
√ B
κ
√"
DT,0.6917385943279901,"δ, which also gives the square root scaling rule. The general-
873"
DT,0.6921496095355528,"ization to d dimension is analogous and one needs to sum across all the dimensions.
874"
DT,0.6925606247431155,Lemma C.30. Let f(x) := x⊤Hx
DT,0.6929716399506781,"2
where H = diag(λ1, . . . , λd). The stationary distribution of
875"
DT,0.6933826551582408,"Adam is (E[X∞]], Cov(X∞)) =

0, η"
DT,0.6937936703658035,"2Σ
1
2 H−1
.
876"
DT,0.6942046855733662,"Proof. The expected value follows immediately from the fact that
877"
DT,0.6946157007809289,dXt = −Σ−1
XTDT,0.6950267159884915,"2 Xtdt
(166)"
XTDT,0.6954377311960542,"For the covariance, we focus on the one-dimensional case. We define H(Xt, Vt) := X2
t
2 +
λ2
2σ2ρ2
M 2
t
2 .
878"
XTDT,0.6958487464036169,"With the same arguments as Lemma C.29, we have
879"
XTDT,0.6962597616111796,d(Xt)2 = −λ
XTDT,0.6966707768187423,"σ X2
t dt + η"
XTDT,0.6970817920263049,"2dt + Noise,
(167)"
XTDT,0.6974928072338676,"which implies that
880"
XTDT,0.6979038224414303,"E[X2
t ]
t→0
→η"
XTDT,0.698314837648993,"2
σ
λ.
(168)"
XTDT,0.6987258528565556,"The thesis follows by applying the same logic to multiple dimensions.
881"
XTDT,0.6991368680641183,"C.6
AdamW
882"
XTDT,0.699547883271681,"In this subsection, we derive the SDE of AdamW defined as defined as
883"
XTDT,0.6999588984792438,"vk+1 = β2vk + (1 −β2) (∇fγk(xk))2
(169)
mk+1 = β1mk + (1 −β1)∇fγk(xk)
(170)"
XTDT,0.7003699136868065,"ˆmk = mk
 
1 −βk
1
−1
(171)"
XTDT,0.700780928894369,"ˆvk = vk
 
1 −βk
2
−1
(172)"
XTDT,0.7011919441019318,"xk+1 = xk −η
ˆmk+1
p"
XTDT,0.7016029593094945,"ˆvk+1 + ϵId
−ηγxk
(173)"
XTDT,0.7020139745170572,"with (x0, m0, v0) ∈Rd × Rd × Rd, η ∈R>0 is the step size, βi = 1 −ρiη for ρ1 = O(η−ζ)
884"
XTDT,0.7024249897246199,"s.t. ζ ∈(0, 1), ρ2 = O(1), γ > 0, the mini-batches {γk} are modelled as i.i.d. random variables
885"
XTDT,0.7028360049321825,"uniformly distributed on {1, · · · , N}, and of size B ≥1.
886"
XTDT,0.7032470201397452,"Theorem C.31. Under the same assumptions as Theorem C.26, the SDE of AdamW is
887"
XTDT,0.7036580353473079,dXt = − p
XTDT,0.7040690505548706,"γ2(t)
γ1(t) P −1
t
(Mt + ηρ1 (∇f (Xt) −Mt))dt −γXtdt
(174)"
XTDT,0.7044800657624332,"dMt = ρ1 (∇f (Xt) −Mt) dt + √ηρ1Σ1/2 (Xt) dWt
(175)"
XTDT,0.7048910809699959,"dVt = ρ2
 
(∇f(Xt))2 + diag (Σ (Xt)) −Vt

dt.
(176)"
XTDT,0.7053020961775586,"where βi = 1 −ηρi, γ > 0, γi(t) = 1 −e−ρit, and Pt = diag √Vt + ϵ
p"
XTDT,0.7057131113851213,"γ2(t)Id.
888"
XTDT,0.706124126592684,"Proof. The proof is the same as the of Theorem C.26 and the only difference is that ηγxk is
889"
XTDT,0.7065351418002466,"approximated with γXtdt.
890"
XTDT,0.7069461570078093,"Figure 4 and Figure 9 validate this result on a variety of architectures and datasets.
891"
XTDT,0.707357172215372,"Corollary C.32. Under the assumptions of Theorem C.31 with Σ(x) = σ2Id, ˜η = κη, ˜B = Bδ,
892"
XTDT,0.7077681874229347,"˜ρ1 = α1ρ1, ˜γ : ξγ, and ˜ρ2 = α2ρ2
893"
XTDT,0.7081792026304973,dXt = −κ p
XTDT,0.70859021783806,"γ2(t)
γ1(t) P −1
t
(Mt + ηα1ρ1 (∇f (Xt) −Mt))dt −κξγXtdt
(177)"
XTDT,0.7090012330456227,dMt = α1ρ1
XTDT,0.7094122482531854,"κ
(∇f (Xt) −Mt) dt + √η α1ρ1 κ
σ
√"
XTDT,0.7098232634607481,"Bδ
IddWt
(178)"
XTDT,0.7102342786683107,dVt = α2ρ2 κ
XTDT,0.7106452938758734,"
(∇f(Xt))2 + σ2"
XTDT,0.7110563090834361,Bδ Id −Vt
XTDT,0.7114673242909988,"
dt.
(179)"
XTDT,0.7118783394985615,"Lemma C.33 (Scaling Rule at Convergence). Under the assumptions of Corollary C.32, f is µ-
894"
XTDT,0.7122893547061241,"strongly convex and L-smooth, Lτ := Tr(∇2f(x)), and (∇f(x))2 = O(η), the asymptotic dynamics
895"
XTDT,0.7127003699136868,"of the iterates of AdamW satisfies the novel scaling rule if κ =
√"
XTDT,0.7131113851212495,"δ and ξ = κ because
896"
XTDT,0.7135224003288122,"E[f(Xt) −f(X∗)]
t→∞
≤
ηLτσL"
XTDT,0.7139334155363748,"2
κ
2µ
√"
XTDT,0.7143444307439375,"BδL + σξγ(L + µ)
(180)"
XTDT,0.7147554459515002,"By enforcing that the speed of Vt matches that of Xt, one needs ˜ρ = κ2ρ, which implies ˜βi =
897"
XTDT,0.7151664611590629,"1 −κ2(1 −βi).
898"
XTDT,0.7155774763666256,"Proof. The proof is the same as Lemma C.28 where we also use L-smoothness as in Lemma C.23.
899"
XTDT,0.7159884915741882,Lemma C.34. For f(x) := x⊤Hx
XTDT,0.7163995067817509,"2
, the stationary distribution of AdamW is (E[X∞]], Cov(X∞)) =
900

0, η"
XTDT,0.7168105219893136,2(HΣ−1
XTDT,0.7172215371968763,"2 + γId)−1
.
901"
XTDT,0.717632552404439,"Proof. The proof is the same as Lemma C.30.
902"
XTDT,0.7180435676120016,"D
SDEs from the literature
903"
XTDT,0.7184545828195643,"Theorem D.1 (Original Malladi’s Statement). Let σ0 := ση, ϵ0 := ϵη, and c2 := 1−β"
XTDT,0.718865598027127,"η2 . Define the
904"
XTDT,0.7192766132346897,"state of the SDE as Lt = (Xt, ut) and the dynamics as
905"
XTDT,0.7196876284422523,"dXt = −P −1
t

∇f (Xt) dt + σ0Σ1/2 (Xt) dWt

(181)"
XTDT,0.720098643649815,"dut = c2 (diag (Σ (Xt)) −ut) dt
(182)"
XTDT,0.7205096588573777,"where Pt := σ0 diag (ut)1/2 + ϵ0Id.
906"
XTDT,0.7209206740649404,"Theorem D.2 (Informal Statement of Theorem C.2 Malladi et al. (2022)). Under sufficient regularity
907"
XTDT,0.7213316892725031,"conditions and ∇f(x) = O(√η), the following SDE is an order 1 weak approximation of RMSprop:
908"
XTDT,0.7217427044800657,"dXt = −P −1
t
(∇f(Xt)dt + √ηΣ(Xt)
1
2 dWt)
(183)
dVt = ρ(diag(Σ(Xt)) −Vt))dt,
(184)"
XTDT,0.7221537196876284,"where β = 1 −ηρ, ρ = O(1), and Pt := diag (Vt)"
XTDT,0.7225647348951911,"1
2 + ϵId.
909"
XTDT,0.7229757501027538,"Lemma D.3. Theorem D.1 and Theorem D.2 are equivalent.
910"
XTDT,0.7233867653103164,"Proof. It follows applying time rescaling t := ηξ and observing that Wt = Wηξ = √ηWξ.
911"
XTDT,0.7237977805178791,"Theorem D.4 (Original Malladi’s Statement). Let c1 := (1 −β1) /η2, c2 := (1 −β2) /η2 and define
912"
XTDT,0.7242087957254418,"σ0, ϵ0 in Theorem D.1. Let γ1(t) := 1 −exp (−c1t) and γ2(t) := 1 −exp (−c2t). Define the state
913"
XTDT,0.7246198109330045,"of the SDE as Lt = (Xt, mt, ut) and the dynamics as
914"
XTDT,0.7250308261405672,dXt = − p
XTDT,0.7254418413481298,"γ2(t)
γ1(t) P −1
t
mtdt
(185)"
XTDT,0.7258528565556925,"dmt = c1 (∇f (Xt) −mt) dt + σ0c1Σ1/2 (Xt) dWt,
(186)
dut = c2 (diag (Σ (Xt)) −ut) dt,
(187)"
XTDT,0.7262638717632552,"where Pt := σ0 diag (ut)1/2 + ϵ0
p"
XTDT,0.726674886970818,"γ2(t)Id.
915"
XTDT,0.7270859021783806,"Theorem D.5 (Informal Statement of Theorem D.2 Malladi et al. (2022)). Under sufficient regularity
916"
XTDT,0.7274969173859432,"conditions and ∇f(x) = O(√η), the following SDE is an order 1 weak approximation of Adam:
917"
XTDT,0.727907932593506,dXt = − p
XTDT,0.7283189478010687,"γ2(t)
γ1(t) P −1
t
Mtdt
(188)"
XTDT,0.7287299630086314,"dMt = ρ1 (∇f (Xt) −Mt) dt + √ηρ1Σ1/2 (Xt) dWt
(189)
dVt = ρ2 (diag (Σ (Xt)) −Vt) dt.
(190)"
XTDT,0.729140978216194,"where βi = 1 −ηρi, γi(t) = 1 −e−ρit, ρi = O(1), and Pt = diag √Vt + ϵ
p"
XTDT,0.7295519934237567,"γ2(t)Id.
918"
XTDT,0.7299630086313194,"Lemma D.6. Theorem D.4 and Theorem D.5 are equivalent.
919"
XTDT,0.7303740238388821,"Proof. It follows applying time rescaling t := ηξ and observing that Wt = Wηξ = √ηWξ.
920"
XTDT,0.7307850390464448,"E
SDE cannot be derived nor used naively
921"
XTDT,0.7311960542540074,"In this section, we provide a gentle introduction to the meaning of deriving an SDE model for an
922"
XTDT,0.7316070694615701,"optimizer and discuss how SDEs have been used to derive scaling rules. To aid the intuition of the
923"
XTDT,0.7320180846691328,"reader, we informally derive an SDE for SGD with learning rate η, mini-batches γB of size B, and
924"
XTDT,0.7324290998766955,"starting point x0 = x, which we dub SGD(η,B). The iterates are given by:
925"
XTDT,0.7328401150842582,"xk+1 = xk −η∇fγB
k (xk)
(191)"
XTDT,0.7332511302918208,"which for Uk := √η(∇f(xk) −∇fγB
k (xk)), we rewrite as
926"
XTDT,0.7336621454993835,"xk −η∇f(xk) + √ηUk,
(192)"
XTDT,0.7340731607069462,where E[Uk] = 0 and Cov(Uk) = η
XTDT,0.7344841759145089,B Σ(xk) = η
XTDT,0.7348951911220715,"B
1
n
PB
i=0(∇f(xk)−∇fi(xk))(∇f(x)−∇fi(xk))⊤.
927"
XTDT,0.7353062063296342,"If we now consider the SDE
928"
XTDT,0.7357172215371969,"dXt = −∇f(Xt)dt +
r η"
XTDT,0.7361282367447596,"B Σ(Xt)
1
2 dWt,
(193)"
XTDT,0.7365392519523223,"its Euler-Maruyama discretization with pace ∆t = η and Zk ∼N(0, Id) is
929"
XTDT,0.7369502671598849,"Xk+1 = Xk −η∇f(Xk) + √η
r η"
XTDT,0.7373612823674476,"B Σ(Xt)
1
2 Zk.
(194)"
XTDT,0.7377722975750103,"Since the Eq. (191) and Eq. (194) share the first two moments, it is reasonable that by identifying
930"
XTDT,0.738183312782573,"t = kη, the SDE in Eq. (193) is a good model to describe the iterates of SGD in Eq. (191).
931"
XTDT,0.7385943279901356,"Informally, we need a “good model”, which is an SDE that is close to the real optimizer. This is
932"
XTDT,0.7390053431976983,"formalized in the following definition which comes from the field of numerical analysis of SDEs (see
933"
XTDT,0.739416358405261,"Mil’shtein (1986)) and bounds the disparity between the the discrete and the continuous process.
934"
XTDT,0.7398273736128237,"Definition E.1 (Weak Approximation). A continuous-time stochastic process {Xt}t∈[0,T ] is an order
935"
XTDT,0.7402383888203864,"α weak approximation (or α-order SDE) of a discrete stochastic process {xk}⌊T/η⌋
k=0
if for every
936"
XTDT,0.740649404027949,"polynomial growth function g, there exists a positive constant C, independent of the stepsize η, such
937"
XTDT,0.7410604192355117,"that maxk=0,...,⌊T/η⌋|Eg (xk) −Eg (Xkη)| ≤Cηα.
938"
XTDT,0.7414714344430744,"To see if an SDE satisfies such a definition, one has to check that for ¯∆= x1 −x and ∆= Xη −x,
939 1."
XTDT,0.7418824496506371,"E∆i −E ¯∆i
 = O(η2),
∀i = 1, . . . , d;
940 2."
XTDT,0.7422934648581998,"E∆i∆j −E ¯∆i ¯∆j
 = O(η2),
∀i, j = 1, . . . , d.
941"
XTDT,0.7427044800657624,"Example: Let us prove that the SDE in Eq. (193) is a valid approximation of SGD(η,B): The first
942"
XTDT,0.7431154952733251,"condition is easily verified. Coming to the second condition we have that
943"
XTDT,0.7435265104808878,1. E∆i∆j = η2∂if(x)∂jf(x) + η2
XTDT,0.7439375256884505,"B Σ(x);
944"
XTDT,0.7443485408960131,2. E ¯∆i ¯∆j = η2∂if(x)∂jf(x) + η2
XTDT,0.7447595561035758,"B Σ(x) + O(η3);
945"
XTDT,0.7451705713111385,"whose difference is of order η3 and thus satisfies the condition. However, we observe that if the
946"
XTDT,0.7455815865187012,"scale of the noise is too small w.r.t η, i.e. Σ(x) = O(ηα) for α ≥0, then the simplest SDE model
947"
XTDT,0.7459926017262639,"describing SGD(η,B) is the ODE dXt = −∇f(Xt)dt as in that case
948"
XTDT,0.7464036169338265,"1. E∆i∆j = η2∂if(x)∂jf(x) + O(η2+α);
949"
XTDT,0.7468146321413892,"2. E ¯∆i ¯∆j = η2∂if(x)∂jf(x) + O(η2),
950"
XTDT,0.7472256473489519,"whose difference is also of order η2. Much differently, if Σ(x) = O(η−α) for α > 0, the simplest
951"
XTDT,0.7476366625565146,"model is the SDE in Eq. (193). We highlight that simplest does not mean best: The SDE is more
952"
XTDT,0.7480476777640773,"accurate than the ODE even in a regime with low noise, but this observation serves as a provocation.
953"
XTDT,0.7484586929716399,"One has to pay attention when deriving SDEs: Some models are more realistic than others.
954"
XTDT,0.7488697081792026,"Let us dig deeper into this thought as we derive two SDEs for SGD with learning rate ˜η := κη and
955"
XTDT,0.7492807233867653,"batch size ˜B := δB for κ > 1 and δ > 1, which we dub SGD(˜η, ˜
B). The first is derived considering
956"
XTDT,0.749691738594328,"that the learning rate is ˜η and carries an error of order O(˜η) w.r.t. SGD(˜η, ˜
B)
957"
XTDT,0.7501027538018906,dXt = −∇f(Xt)dt + r
XTDT,0.7505137690094533,"˜η
˜B
Σ(Xt)
1
2 dWt = −∇f(Xt)dt +
r ηκ"
XTDT,0.750924784217016,"Bδ Σ(Xt)
1
2 dWt.
(195)"
XTDT,0.7513357994245787,"The second one instead is derived considering η as the learning rate and κ as a constant “scheduler”.
958"
XTDT,0.7517468146321414,"Consistently with (Li et al., 2017), the SDE which carries an error of order O(η) w.r.t SGD(˜η, ˜
B) is
959"
XTDT,0.752157829839704,"dXt = −κ∇f(Xt)dt + κ
r η"
XTDT,0.7525688450472667,"Bδ Σ(Xt)
1
2 dWt.
(196)"
XTDT,0.7529798602548294,"While they both are valid models, there are three reasons why one should prefer the latter:
960"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7533908754623921,"1. It fully reflects the fact that a larger learning rate results in a faster and noisier dynamics
961"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7538018906699548,"2. It has intrinsically less error than the other;
962"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7542129058775174,"3. It is consistent with the optimizer in that there is no combination of κ and δ that can ever
963"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7546239210850801,"leave the dynamics unchanged.
964"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7550349362926428,"E.1
Deriving scaling rules
965"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7554459515002055,"Jastrzebski et al. (2018) observed that only the ratio between η and B matters in determining the
966"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7558569667077681,"dynamics of Eq. (194). Therefore, they argue that for κ = δ the SDE for SGD(κη,δB) coincides with
967"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7562679819153308,"that of SGD(η,B) and that this implies that the path properties of the optimizers are the same. On the
968"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7566789971228935,"contrary, the path of SGD(η,B) strongly depends on the hyperparameters: The speed and volatility of
969"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7570900123304563,"the dynamics are driven by η, and no choice of B can undo this. We remind the reader that the goal of
970"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.757501027538019,"these rules is not to keep the dynamics of the optimizers unaltered, but rather to give a practical way
971"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7579120427455815,"to change a hyperparameter, e.g. η, and have a principled way to adjust the others, e.g. B, such that
972"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7583230579531443,"the performance of the optimizer is preserved. Therefore, we propose deriving scaling rules as we
973"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.758734073160707,"preserve certain relevant quantities of the dynamics such as the convergence bound on the expected
974"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7591450883682697,"loss or the speed. To show this quantitative, we use this rationale to derive the scaling rule of SGD as
975"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7595561035758323,"we aim at preserving the asymptotic loss level.
976"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.759967118783395,"Lemma E.2. If f is a µ strongly convex function, Lτ ≤Tr(∇2f(x)) and Σ(x) = σ2Id, then:
977"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7603781339909577,"1. Under the dynamics of Eq. (193) we have:
978"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7607891491985204,E[f(Xt) −f(X∗)] ≤(f(X0) −f(X∗))e−2µt + η
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7612001644060831,"2
Lτσ2"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7616111796136457,"2µB
 
1 −e−2µt
;
(197)"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7620221948212084,"2. Under the dynamics of Eq. (195) we have:
979"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7624332100287711,E[f(Xt) −f(X∗)] ≤(f(X0) −f(X∗))e−2µt + η
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7628442252363338,"2
Lτσ2"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7632552404438965,"2µB
κ
δ
 
1 −e−2µt
;
(198)"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7636662556514591,"3. Under the dynamics of Eq. (196) we have:
980"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7640772708590218,E[f(Xt) −f(X∗)] ≤(f(X0) −f(X∗))e−2µκt + η
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7644882860665845,"2
Lτσ2"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7648993012741472,"2µB
κ
δ
 
1 −e−2µκt
.
(199)"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7653103164817098,"The first bound implies that the asymptotic limit of the expected loss for SGD(η,B) is η"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7657213316892725,"2
Lτ σ2"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7661323468968352,"2µB . The
981"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7665433621043979,"last two bounds predict that the asymptotic loss level for SGD(˜η, ˜
B) is η"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7669543773119606,"2
Lτ σ2"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7673653925195232,"2µB
κ
δ . Since the objective
982"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7677764077270859,"of the scaling rule is to find κ and δ such that SGD(˜η, ˜
B) achieves the same loss level as SGD(η,B),
983"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7681874229346486,"we recover the linear scaling rule setting κ = δ. However, only the last bound can correctly capture
984"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7685984381422113,"the fact that the dynamics of SGD(˜η, ˜
B) is κ times faster than that of SGD(η,B).
985"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.769009453349774,"We conclude the discussion with a simple sample of how deriving a scaling rule from the SDE itself
986"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7694204685573366,"inevitably leads to the wrong conclusion. We define the following algorithm which is inspired by
987"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7698314837648993,"AdamW and which we dub SGDW:
988"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.770242498972462,"xk+1 = xk −η∇fγk(xk) −ηγxk.
(200)
Lemma E.3. The SDE of SGDW is
989"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7706535141800247,"dXt = −∇f(Xt)dt +
r η"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7710645293875873,"B Σ(Xt)
1
2 dWt −γXtdt.
(201)"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.77147554459515,"Therefore, one would naively deduce that to keep the SDE unchanged, one can simply use the linear
990"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7718865598027127,"scaling rule of SGD and leave γ unaltered. However, one can easily derive the upper bound on the
991"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7722975750102754,"expected loss for a convex quadratic function and observe that to preserve that, it is imperative to
992"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7727085902178381,"scale γ by κ as well.
993"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7731196054254007,"We thus conclude that:
994"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7735306206329634,"1. Eq. (196) is a better model for SGD(˜η, ˜
B) as it represents the dynamics more accurately;
995"
IT FULLY REFLECTS THE FACT THAT A LARGER LEARNING RATE RESULTS IN A FASTER AND NOISIER DYNAMICS,0.7739416358405261,"2. Maintaining the shape of the SDE does not preserve the path properties of the optimizer;
996"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7743526510480888,"3. Deriving a scaling rule uniquely from the SDE might lead to the wrong conclusions in the
997"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7747636662556514,"general case.
998"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7751746814632141,"Remark E.4. We highlight that Theorem 5.3 of Malladi et al. (2022) claimed to have formally derived
999"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7755856966707768,"one for RMSprop: In line with (Jastrzebski et al., 2018), they argue that if they were to find a scaling
1000"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7759967118783395,"rule that would leave their SDE unchanged, this would imply that even the dynamics of the iterates of
1001"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7764077270859022,"RMSprop itself would be unchanged. First, we remind the reader that an SDE is formally defined
1002"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7768187422934648,"as an equation that drives the dynamics plus an initial condition (See (Karatzas and Shreve, 2014),
1003"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7772297575010275,"Section 5). While their scaling rule does leave the equation unchanged, it alters the initial condition,
1004"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7776407727085902,"thus changing the SDE itself: This invalidates their claim and proof. Second, contrary to their claim,
1005"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7780517879161529,"the rule is only valid near convergence as their SDE is only valid there. Third, Lemma E.2 offers a
1006"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7784628031237156,"shred of concrete evidence that keeping the SDE unchanged does not imply that the path properties
1007"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7788738183312782,"of the optimizers are preserved. Fourth, Lemma E.3 is a piece of concrete evidence that deriving
1008"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7792848335388409,"scaling rules directly and naively from the SDE might lead to the wrong conclusions.
1009"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7796958487464036,"F
Experiments
1010"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7801068639539663,"In this section, we provide the modeling choices and instructions to replicate our experiments. All
1011"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7805178791615289,"experiments we run on one NVIDIA GeForce RTX 3090 GPU. The code is implemented in Python 3
1012"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7809288943690916,"(Van Rossum and Drake, 2009) mainly using Numpy (Harris et al., 2020), scikit-learn (Pedregosa
1013"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7813399095766543,"et al., 2011), and JAX (Bradbury et al., 2018).
1014"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.781750924784217,"F.1
SignSGD: SDE validation (Figure 1)
1015"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7821619399917797,"In this subsection, we describe the experiments we run to produce Figure 1: The loss dynamics of
1016"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7825729551993423,"SignSGD and that of our SDE match on average.
1017"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.782983970406905,"DNN on Breast Cancer Dataset (Dua and Graff, 2017)
This paragraph refers to the left of Figure
1018"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7833949856144677,"1. The DNN has 10 dense layers with 20 neurons each activated with a ReLu. We minimize the binary
1019"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7838060008220304,"cross-entropy loss. We run SignSGD for 50000 epochs as we calculate the full gradient and inject it
1020"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7842170160295932,"with Gaussian noise Z ∼N(0, σ2Id) where σ = 1. The learning rate is η = 0.001. Similarly, we
1021"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7846280312371557,"integrate the SignSGD SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are
1022"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7850390464447184,"averaged over 3 runs and the shaded areas are the average ± the standard deviation.
1023"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7854500616522812,"CNN on MNIST (Deng, 2012)
This paragraph refers to the center-left of Figure 1. The CNN
1024"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7858610768598439,"has a (3, 3, 32) convolutional layer with stride 1, followed by a ReLu activation, a (2, 2) max pool
1025"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7862720920674064,"layer with stride (2, 2), a (3, 3, 32) convolutional layer with stride 1, a ReLu activation, a (2, 2) max
1026"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7866831072749692,"pool layer with stride (2, 2). Then the activations are flattened and passed through a dense layer that
1027"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7870941224825319,"compresses them into 128 dimensions, a final ReLu activation, and a final dense layer into the output
1028"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7875051376900946,"dimension 10. The output finally goes through a softmax as we minimize the cross-entropy loss. We
1029"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7879161528976573,"run SignSGD for 40000 epochs as we calculate the full gradient and inject it with Gaussian noise
1030"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7883271681052199,"Z ∼N(0, σ2Id) where σ = 1. The learning rate is η = 0.001. Similarly, we integrate the SignSGD
1031"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7887381833127826,"SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged over 3 run
1032"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7891491985203453,"and the shaded areas are the average ± the standard deviation.
1033"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.789560213727908,"Transformer on MNIST
This paragraph refers to the center-right of Figure 1. The Architecture is
1034"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7899712289354706,"a scaled-down version of (Dosovitskiy et al., 2021), where the hyperparameters are patch size=28,
1035"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7903822441430333,"out features=10, width=48, depth=3, num heads=6, and dim ffn=192. We minimize the cross-entropy
1036"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.790793259350596,"loss as we run SignSGD for 5000 epochs as we calculate the full gradient and inject it with Gaussian
1037"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7912042745581587,"noise Z ∼N(0, σ2Id) where σ = 1. The learning rate is η = 0.001. Similarly, we integrate the
1038"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7916152897657214,"SignSGD SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged
1039"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.792026304973284,"over 3 runs and the shaded areas are the average ± the standard deviation.
1040"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7924373201808467,"ResNet on CIFAR-10 (Krizhevsky et al., 2009)
This paragraph refers to the right of Figure 1.
1041"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7928483353884094,"The ResNet has a (3, 3, 128) convolutional layer with stride 1, followed by a ReLu activation, a
1042"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7932593505959721,"second (3, 3, 64) convolutional layer with stride 1, followed by a residual connection from the first
1043"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7936703658035348,"convolutional layer, then a (2, 2) max pool layer with stride (2, 2). Then the activations are flattened
1044"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7940813810110974,"and passed through a dense layer that compresses them into 128 dimensions, a final ReLu activation,
1045"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7944923962186601,"and a final dense layer into the output dimension 10. The output finally goes through a softmax as we
1046"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7949034114262228,"minimize the cross-entropy loss. We run SignSGD for 5000 epochs as we calculate the full gradient
1047"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7953144266337855,"and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 1. The learning rate is η = 0.001.
1048"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7957254418413481,"Similarly, we integrate the SignSGD SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with ∆t = η.
1049"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7961364570489108,"Results are averaged over 3 runs and the shaded areas are the average ± the standard deviation.
1050"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7965474722564735,"F.2
SignSGD: insights validation (Figure 2)
1051"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7969584874640362,"In this subsection, we describe the experiments we run to produce Figure 2: We successfully validate
1052"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7973695026715989,"them all.
1053"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7977805178791615,"Phases: Lemma 3.4 and Lemma 3.5
In this paragraph, we describe how we validated the existence
1054"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7981915330867242,"of the phases of SignSGD as predicted in Lemma 3.4 and Lemma 3.5. To produce the left of Figure
1055"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7986025482942869,"2), we simulated the full SDE (Eq. (16)) and the one describing Phase 3 (Eq. (5)). The optimized
1056"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7990135635018496,function is f(x) = x⊤Hx
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7994245787094123,"2
for H = diag(1, 2), x0 drawn (and fixed for all runs) from a normal
1057"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.7998355939169749,"distribution N(0, 0.01), η = 0.001, and Σ = σ2Id where σ = 0.1. We integrate the SDEs with
1058"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8002466091245376,"Euler-Maruyama (Algorithm 1) with ∆t = η and for 3000 iterations. Results are averaged over 500
1059"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8006576243321003,"runs and the shaded areas are the average ± the standard deviation. Clearly, the two SDEs share the
1060"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.801068639539663,"same dynamics.
1061"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8014796547472256,"To produce the center-left of Figure 2, we repeat the above as x0 drawn (and fixed for all runs) from
1062"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8018906699547883,"a normal distribution N(0, 1). Then, we plot the average loss values together with the theoretical
1063"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.802301685162351,"prediction of Phase 1 and Phase 3: They perfectly overlap.
1064"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8027127003699137,"Stationary distribution: Lemma 3.7
In this paragraph, we describe how we validated the conver-
1065"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8031237155774764,"gence behavior predicted in Lemma 3.7. To produce the center-right of Figure 2), we run SignSGD on
1066"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.803534730785039,f(x) = x⊤Hx
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8039457459926017,"2
for H = diag(1, 2), x0 = (0.001, 0.001), η = 0.001 and Σ = σ2Id where σ = 0.1.
1067"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8043567612001644,"We run this for 5000 times and report the evolution of the moments. Then, we add lines representing
1068"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8047677764077271,"the theoretical predictions derived in Lemma 3.7: They match.
1069"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8051787916152897,"Schedulers: Lemma 3.9
In this paragraph, we describe how we validated the convergence behavior
1070"
DERIVING A SCALING RULE UNIQUELY FROM THE SDE MIGHT LEAD TO THE WRONG CONCLUSIONS IN THE,0.8055898068228524,"predicted in Lemma 3.9. To produce the right of Figure 2, we run SignSGD on f(x) = x⊤Hx"
FOR,0.8060008220304151,"2
for
1071"
FOR,0.8064118372379778,"H = diag(1, 2), x0 = (0.01, 0.01), η = 0.01 and Σ = σ2Id where σ = 0.1. We used the scheduler
1072"
FOR,0.8068228524455405,"ηγ
t =
1
(t+1)γ for γ ∈{0.1, 0.5, 1.5}. For the first two choices of γ, ηγ
t satisfies our sufficient
1073"
FOR,0.8072338676531031,"condition for the convergence of SignSGD: In the figure, we observe that indeed SignSGD converges
1074"
FOR,0.8076448828606658,"to 0 with the same speed as the one predicted in the Lemma. For γ = 1.5, we observe that SignSGD
1075"
FOR,0.8080558980682285,"does not converge following the theoretical curve because it does not satisfy our sufficient condition.
1076"
FOR,0.8084669132757912,"Results are averaged over 500 runs.
1077"
FOR,0.8088779284833539,"F.3
RMSprop: SDE validation (Figure 7 and Figure 8)
1078"
FOR,0.8092889436909165,"In this subsection, we describe the experiments we run to produce Figure 7 and Figure 8: The
1079"
FOR,0.8096999588984792,"dynamics of our SDE matches that of RMSprop better than the SDE derived in (Malladi et al., 2022).
1080"
FOR,0.8101109741060419,"Quadratic convex function
This paragraph refers to the left and center-left of Figure 7. We
1081"
FOR,0.8105219893136046,optimize the function f(x) = x⊤Hx
FOR,0.8109330045211672,"2
where H = diag(10, 2). We run RMSprop for 2000 epochs as
1082"
FOR,0.8113440197287299,"we calculate the full gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 0.1. The
1083"
FOR,0.8117550349362926,"learning rate is η = 0.01, β = 0.99. Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of
1084"
FOR,0.8121660501438553,"Malladi (Eq. (183)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged over
1085"
FOR,0.812577065351418,"500 runs and the shaded areas are the average ± the standard deviation: Our SDE matches RMSprop
1086"
FOR,0.8129880805589806,"much better.
1087"
FOR,0.8133990957665433,"Embedded saddle
This paragraph refers to the center-right and right of Figure 7. We optimize the
1088"
FOR,0.813810110974106,"function f(x) = x⊤Hx 2
+ 1"
FOR,0.8142211261816688,"4λ P2
i=1 x4
i −ξ"
FOR,0.8146321413892315,"3
P2
i=1 x3
i where H = diag(−1, 2), λ = 1, and ξ = 0.1.
1089"
FOR,0.815043156596794,"We run RMSprop for 1600 epochs as we calculate the full gradient and inject it with Gaussian noise
1090"
FOR,0.8154541718043568,"Z ∼N(0, σ2Id) where σ = 0.01. The learning rate is η = 0.01, β = 0.99. Similarly, we integrate
1091"
FOR,0.8158651870119195,"our RMSprop SDE (Eq. (86)) and that of Malladi (Eq. (183)) with Euler-Maruyama (Algorithm 1)
1092"
FOR,0.8162762022194822,"with ∆t = η. Results are averaged over 500 runs and the shaded areas are the average ± the standard
1093"
FOR,0.8166872174270448,"deviation: Our SDE matches RMSprop much better.
1094"
FOR,0.8170982326346075,"DNN on Breast Cancer Dataset
This paragraph refers to the left of Figure 8. The architecture and
1095"
FOR,0.8175092478421702,"loss are the same as used above for SignSGD. We run RMSprop for 2000 epochs as we calculate the
1096"
FOR,0.8179202630497329,"full gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 10−2. The learning rate
1097"
FOR,0.8183312782572956,"is η = 10−4, β = 0.9995. Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of Malladi
1098"
FOR,0.8187422934648582,"(Eq. (183)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged over 3 runs and
1099"
FOR,0.8191533086724209,"the shaded areas are the average ± the standard deviation: Our SDE matches RMSprop much better.
1100"
FOR,0.8195643238799836,"CNN on MNIST
This paragraph refers to the center-left of Figure 8. The architecture and loss
1101"
FOR,0.8199753390875463,"are the same as used above for SignSGD. We run RMSprop for 2000 epochs as we calculate the full
1102"
FOR,0.8203863542951089,"gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 10−2. The learning rate is
1103"
FOR,0.8207973695026716,"η = 10−3, β = 0.995. Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of Malladi (Eq.
1104"
FOR,0.8212083847102343,"(183)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged over 3 run and the
1105"
FOR,0.821619399917797,"shaded areas are the average ± the standard deviation: Our SDE matches RMSprop much better.
1106"
FOR,0.8220304151253597,"Transformer on MNIST
This paragraph refers to the center-right of Figure 8. The architecture
1107"
FOR,0.8224414303329223,"and loss are the same as used above for SignSGD. We run RMSprop for 2000 epochs as we calculate
1108"
FOR,0.822852445540485,"the full gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 10−2. The learning
1109"
FOR,0.8232634607480477,"rate is η = 10−3, β = 0.995. Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of
1110"
FOR,0.8236744759556104,"Malladi (Eq. (183)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged over
1111"
FOR,0.8240854911631731,"3 runs and the shaded areas are the average ± the standard deviation: Our SDE matches RMSprop
1112"
FOR,0.8244965063707357,"much better.
1113"
FOR,0.8249075215782984,"ResNet on CIFAR-10
This paragraph refers to the right of Figure 8. The architecture and loss
1114"
FOR,0.8253185367858611,"are the same as used above for SignSGD. We run RMSprop for 500 epochs as we calculate the full
1115"
FOR,0.8257295519934238,"gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 10−4. The learning rate is
1116"
FOR,0.8261405672009864,"η = 10−4, β = 0.9999. Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of Malladi
1117"
FOR,0.8265515824085491,"(Eq. (183)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged over 3 runs and
1118"
FOR,0.8269625976161118,"the shaded areas are the average ± the standard deviation: Our SDE matches RMSprop much better.
1119"
FOR,0.8273736128236745,"F.4
Adam: SDE validation (Figure 10 and Figure 11)
1120"
FOR,0.8277846280312372,"In this subsection, we describe the experiments we run to produce Figure 11 and Figure 10: The
1121"
FOR,0.8281956432387998,"dynamics of our SDE matches that of Adam better than that derived in (Malladi et al., 2022).
1122"
FOR,0.8286066584463625,"Quadratic convex function
This paragraph refers to the left and center-left of Figure 10. We
1123"
FOR,0.8290176736539252,optimize the function f(x) = x⊤Hx
FOR,0.8294286888614879,"2
where H = diag(10, 2). We run Adam for 50000 epochs as we
1124"
FOR,0.8298397040690506,"calculate the full gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 0.01. The
1125"
FOR,0.8302507192766132,"learning rate is η = 0.001, β1 = 0.9, and β2 = 0.999. Similarly, we integrate our Adam SDE (Eq.
1126"
FOR,0.8306617344841759,"(124)) and that of Malladi (Eq. (188)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results
1127"
FOR,0.8310727496917386,"are averaged over 500 runs and the shaded areas are the average ± the standard deviation: Our SDE
1128"
FOR,0.8314837648993013,"matches Adam much better.
1129"
FOR,0.8318947801068639,"Embedded saddle
This paragraph refers to the center-right and right of Figure 10. We optimize the
1130"
FOR,0.8323057953144266,"function f(x) = x⊤Hx 2
+ 1"
FOR,0.8327168105219893,"4λ P2
i=1 x4
i −ξ"
FOR,0.833127825729552,"3
P2
i=1 x3
i where H = diag(−1, 2), λ = 1, and ξ = 0.1.
1131"
FOR,0.8335388409371147,"We run Adam as we calculate the full gradient and inject it with Gaussian noise Z ∼N(0, σ2Id)
1132"
FOR,0.8339498561446773,"where σ = 0.1. The learning rate is η = 0.001, β1 = 0.9, and β2 = 0.999. Similarly, we integrate
1133"
FOR,0.83436087135224,"our Adam SDE (Eq. (124)) and that of Malladi (Eq. (188)) with Euler-Maruyama (Algorithm 1) with
1134"
FOR,0.8347718865598027,"∆t = η. Results are averaged over 500 runs and the shaded areas are the average ± the standard
1135"
FOR,0.8351829017673654,"deviation: Our SDE matches Adam much better.
1136"
FOR,0.835593916974928,"DNN on Breast Cancer Dataset
This paragraph refers to the left of Figure 11. The architecture
1137"
FOR,0.8360049321824907,"and loss are the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the
1138"
FOR,0.8364159473900534,"full gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 10−2. The learning rate
1139"
FOR,0.8368269625976161,"is η = 10−4, β1 = 0.99, and β2 = 0.999. Similarly, we integrate our Adam SDE (Eq. (124)) and
1140"
FOR,0.8372379778051788,"that of Malladi (Eq. (188)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged
1141"
FOR,0.8376489930127414,"over 3 runs and the shaded areas are the average ± the standard deviation: Our SDE matches Adam
1142"
FOR,0.8380600082203041,"much better.
1143"
FOR,0.8384710234278668,"CNN on MNIST
This paragraph refers to the center-left of Figure 11. The architecture and loss are
1144"
FOR,0.8388820386354295,"the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the full gradient
1145"
FOR,0.8392930538429922,"and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 10−2. The learning rate is η = 10−2,
1146"
FOR,0.8397040690505548,"β1 = 0.9, and β2 = 0.99. Similarly, we integrate our Adam SDE (Eq. (124)) and that of Malladi (Eq.
1147"
FOR,0.8401150842581175,"(188)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged over 3 runs and the
1148"
FOR,0.8405260994656802,"shaded areas are the average ± the standard deviation: Our SDE matches Adam much better.
1149"
FOR,0.840937114673243,"Transformer on MNIST
This paragraph refers to the center-right of Figure 11. The architecture
1150"
FOR,0.8413481298808055,"and loss are the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the
1151"
FOR,0.8417591450883682,"full gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 10−2. The learning rate
1152"
FOR,0.842170160295931,"is η = 10−2, β1 = 0.9, and β2 = 0.99. Similarly, we integrate our Adam SDE (Eq. (124)) and that
1153"
FOR,0.8425811755034937,"of Malladi (Eq. (188)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged over
1154"
FOR,0.8429921907110564,"3 runs and the shaded areas are the average ± the standard deviation: Our SDE matches Adam much
1155"
FOR,0.843403205918619,"better.
1156"
FOR,0.8438142211261817,"ResNet on CIFAR-10
This paragraph refers to the right of Figure 11. The architecture and loss are
1157"
FOR,0.8442252363337444,"the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the full gradient
1158"
FOR,0.8446362515413071,"and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 10−5. The learning rate is η = 10−5,
1159"
FOR,0.8450472667488698,"β1 = 0.99, and β2 = 0.9999. Similarly, we integrate our Adam SDE (Eq. (124)) and that of Malladi
1160"
FOR,0.8454582819564324,"(Eq. (188)) with Euler-Maruyama (Algorithm 1) with ∆t = η. Results are averaged over 3 runs and
1161"
FOR,0.8458692971639951,"the shaded areas are the average ± the standard deviation: Our SDE matches Adam much better.
1162"
FOR,0.8462803123715578,"F.5
RMSpropW & AdamW: SDE validation (Figure 3, Figure 4)
1163"
FOR,0.8466913275791205,"The settings are exactly the same as those for RMSprop and Adam. The regularization parameter
1164"
FOR,0.8471023427866831,"used is always γ = 0.01. We observe that our SDEs match the respective algorithm with a good
1165"
FOR,0.8475133579942458,"agreement.
1166"
FOR,0.8479243732018085,"F.6
RMSpropW & AdamW: insights validation (Figure 5)
1167"
FOR,0.8483353884093712,"In this subsection, we describe the experiments we run to produce Figure 5: The theoretically
1168"
FOR,0.8487464036169339,"predicted asymptotic loss value and moments of RMSpropW and AdamW match those empirically
1169"
FOR,0.8491574188244965,"found.
1170"
FOR,0.8495684340320592,"Asymptotic loss & scaling rule of AdamW
This paragraph refers to the left of Figure 5. We
1171"
FOR,0.8499794492396219,optimize the function f(x) = x⊤Hx
FOR,0.8503904644471846,"2
where H = diag(1, 3). We run AdamW for 20000 epochs as
1172"
FOR,0.8508014796547472,"we calculate the full gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 1. The
1173"
FOR,0.8512124948623099,"learning rate is η = 0.001, β1 = 0.9, and β2 = 0.999. Experiments are run for both γ = 1 and
1174"
FOR,0.8516235100698726,"γ = 4. The rescaled versions of the algorithms AdamW R follow the novel scaling rule with κ = 2.
1175"
FOR,0.8520345252774353,"AdamW NR follows the scaling rule but not for γ which is left unchanged. We plot the evolution of
1176"
FOR,0.852445540484998,"the loss values with the theoretical predictions of Lemma C.28: Results are averaged over 500 runs.
1177"
FOR,0.8528565556925606,"Asymptotic loss & scaling rule of RMSpropW
This paragraph refers to the center-left of Figure
1178"
FOR,0.8532675709001233,"5: The only difference with the previous paragraph is that we use RMSpropW with β = 0.999.
1179"
FOR,0.853678586107686,"AdamW: the role of the βs
This paragraph refers to the center-right of Figure 5. We optimize
1180"
FOR,0.8540896013152487,"the function f(x) =
x⊤Hx 2
+ 1"
FOR,0.8545006165228114,"4λ P2
i=1 x4
i −ξ"
FOR,0.854911631730374,"3
P2
i=1 x3
i where H = diag(−1, 2), λ = 1, and
1181"
FOR,0.8553226469379367,"ξ = 0.1. We run AdamW as we calculate the full gradient and inject it with Gaussian noise
1182"
FOR,0.8557336621454994,"Z ∼N(0, σ2Id) where σ = 0.1. The learning rate is η = 0.001, γ = 0.1, β1 ∈{0.99, 0.999},
1183"
FOR,0.8561446773530621,"and β2 ∈{0.992, 0.996, 0.998}: Clearly, three combinations go into a minimum and three go into
1184"
FOR,0.8565556925606247,"the other. For each minimum, the three optimizers converge to the same asymptotic loss value
1185"
FOR,0.8569667077681874,"independently on the values of β1 and β2. We argue that β1, and β2 select the basin and the speed of
1186"
FOR,0.8573777229757501,"convergence, not the asymptotic loss value: This is consistent with Lemma 3.13.
1187"
FOR,0.8577887381833128,"Stationary distribution
This paragraph refers to the right of Figure 5. We optimize the function
1188"
FOR,0.8581997533908755,f(x) = x⊤Hx
FOR,0.8586107685984381,"2
where H = diag(1, 3). We run Adam for 20000 epochs as we calculate the full
1189"
FOR,0.8590217838060008,"gradient and inject it with Gaussian noise Z ∼N(0, σ2Id) where σ = 0.01. The learning rate is
1190"
FOR,0.8594327990135635,"η = 0.001, γ = 4, β = 0.999, β1 = 0.9, and β2 = 0.999. We plot the evolution of the average
1191"
FOR,0.8598438142211262,"variances with the theoretical predictions of Lemma C.24 and Lemma 3.14: Results are averaged
1192"
FOR,0.8602548294286889,"over 100 runs.
1193"
FOR,0.8606658446362515,"F.7
Effect of noise - validation (Figure 6)
1194"
FOR,0.8610768598438142,"In this subsection, we describe the experiments run to produce Figure 6: All bounds on the asymptotic
1195"
FOR,0.8614878750513769,"expected loss value for SGD, SignSGD, Adam, and AdamW are perfectly verified.
1196"
FOR,0.8618988902589396,We optimize the loss f(x) = x⊤Hx
FOR,0.8623099054665022,"2
where H = diag(1, 1) as we run each optimizer for 100000
1197"
FOR,0.8627209206740649,"iterations with η = 0.01. We repeat this procedure five times, one for each σ ∈{0.01, 0.1, 1, 10, 100}.
1198"
FOR,0.8631319358816276,"As we train, we inject noise on the gradient as distributed as N(0, σ2Id). We plot the average loss
1199"
FOR,0.8635429510891903,"together with the respective limits predicted by our Lemmas. For each optimizer and each σ, the
1200"
FOR,0.863953966296753,"average asymptotic loss matches the predicted limit. Therefore, we verify that the loss of SGD scales
1201"
FOR,0.8643649815043156,"quadratically in σ, that of Adam and SignSGD scales linearly, and that of AdamW is limited in σ.
1202"
FOR,0.8647759967118783,"F.8
Increasing weight decay with the batch size
1203"
FOR,0.865187011919441,"The analysis of Malladi et al. (2022) suggests that, when scaling batch size B by a factor κ one has
1204"
FOR,0.8655980271270037,"to scale up (↑) the learning rate η by a factor √κ and scale down (↓) β2 to the value 1 −κ(1 −β2).
1205"
FOR,0.8660090423345663,"Our SDE analysis confirms similar rules (Lemma 3.13) but additionally suggests scaling up the
1206"
FOR,0.866420057542129,"decoupled weight decay parameter γ by a factor √κ. We test this in two settings: VGG11 and
1207"
FOR,0.8668310727496917,"ResNet34 (convolutional networks) on CIFAR-10 classification. We select a base batch size of 256,
1208"
FOR,0.8672420879572544,"and run AdamW with η = 0.001, β2 = 0.99, and γ = 0.1. We consider scaling the batch by a factor
1209"
FOR,0.8676531031648171,"4: In Table 1, we show the effect of updating each hyperparameter with the proposed rule and we
1210"
FOR,0.8680641183723797,"denote by a “·” the model parameters of the base run with B = 256. We train for 150 epochs the
1211"
FOR,0.8684751335799424,"model with B = 256, and 150 × 4 the model with B = 4 × 256. Experiments are repeated 3 times.
1212"
FOR,0.8688861487875051,"We find that, while improvements are marginal, they are consistent with our theoretical results.
1213"
FOR,0.8692971639950678,"B
η
β2
λ
VGG11 (Test Acc ↑)
ResNet 34 (Test Acc ↑)
·
·
·
·
90.581 ± 0.295
94.396 ± 0.126
↑
·
·
·
90.502 ± 0.093
94.296 ± 0.220
↑
↑
·
·
90.767 ± 0.119
94.507 ± 0.148
↑
↑
↓
·
90.703 ± 0.271
94.590 ± 0.188
↑
↑
↓
↑
90.966 ± 0.252
94.639 ± 0.192"
FOR,0.8697081792026305,Table 1: Scaling with the batch size: Effect of adapting AdamW hyperparameters. 1214
FOR,0.8701191944101931,"NeurIPS Paper Checklist
1215"
CLAIMS,0.8705302096177558,"1. Claims
1216"
CLAIMS,0.8709412248253185,"Question: Do the main claims made in the abstract and introduction accurately reflect the
1217"
CLAIMS,0.8713522400328813,"paper’s contributions and scope?
1218"
CLAIMS,0.8717632552404438,"Answer: [Yes]
1219"
CLAIMS,0.8721742704480066,"Justification: The abstract is a high-level description of what we achieve. The results are
1220"
CLAIMS,0.8725852856555693,"clearly presented in Section 3 and validated in the figures. Details are in the appendix.
1221"
CLAIMS,0.872996300863132,"Guidelines:
1222"
CLAIMS,0.8734073160706947,"• The answer NA means that the abstract and introduction do not include the claims
1223"
CLAIMS,0.8738183312782573,"made in the paper.
1224"
CLAIMS,0.87422934648582,"• The abstract and/or introduction should clearly state the claims made, including the
1225"
CLAIMS,0.8746403616933827,"contributions made in the paper and important assumptions and limitations. A No or
1226"
CLAIMS,0.8750513769009454,"NA answer to this question will not be perceived well by the reviewers.
1227"
CLAIMS,0.8754623921085081,"• The claims made should match theoretical and experimental results, and reflect how
1228"
CLAIMS,0.8758734073160707,"much the results can be expected to generalize to other settings.
1229"
CLAIMS,0.8762844225236334,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
1230"
CLAIMS,0.8766954377311961,"are not attained by the paper.
1231"
LIMITATIONS,0.8771064529387588,"2. Limitations
1232"
LIMITATIONS,0.8775174681463214,"Question: Does the paper discuss the limitations of the work performed by the authors?
1233"
LIMITATIONS,0.8779284833538841,"Answer: [Yes]
1234"
LIMITATIONS,0.8783394985614468,"Justification: See Section C.1.
1235"
LIMITATIONS,0.8787505137690095,"Guidelines:
1236"
LIMITATIONS,0.8791615289765722,"• The answer NA means that the paper has no limitation while the answer No means that
1237"
LIMITATIONS,0.8795725441841348,"the paper has limitations, but those are not discussed in the paper.
1238"
LIMITATIONS,0.8799835593916975,"• The authors are encouraged to create a separate ”Limitations” section in their paper.
1239"
LIMITATIONS,0.8803945745992602,"• The paper should point out any strong assumptions and how robust the results are to
1240"
LIMITATIONS,0.8808055898068229,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
1241"
LIMITATIONS,0.8812166050143855,"model well-specification, asymptotic approximations only holding locally). The authors
1242"
LIMITATIONS,0.8816276202219482,"should reflect on how these assumptions might be violated in practice and what the
1243"
LIMITATIONS,0.8820386354295109,"implications would be.
1244"
LIMITATIONS,0.8824496506370736,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
1245"
LIMITATIONS,0.8828606658446363,"only tested on a few datasets or with a few runs. In general, empirical results often
1246"
LIMITATIONS,0.8832716810521989,"depend on implicit assumptions, which should be articulated.
1247"
LIMITATIONS,0.8836826962597616,"• The authors should reflect on the factors that influence the performance of the approach.
1248"
LIMITATIONS,0.8840937114673243,"For example, a facial recognition algorithm may perform poorly when image resolution
1249"
LIMITATIONS,0.884504726674887,"is low or images are taken in low lighting. Or a speech-to-text system might not be
1250"
LIMITATIONS,0.8849157418824497,"used reliably to provide closed captions for online lectures because it fails to handle
1251"
LIMITATIONS,0.8853267570900123,"technical jargon.
1252"
LIMITATIONS,0.885737772297575,"• The authors should discuss the computational efficiency of the proposed algorithms
1253"
LIMITATIONS,0.8861487875051377,"and how they scale with dataset size.
1254"
LIMITATIONS,0.8865598027127004,"• If applicable, the authors should discuss possible limitations of their approach to
1255"
LIMITATIONS,0.886970817920263,"address problems of privacy and fairness.
1256"
LIMITATIONS,0.8873818331278257,"• While the authors might fear that complete honesty about limitations might be used by
1257"
LIMITATIONS,0.8877928483353884,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
1258"
LIMITATIONS,0.8882038635429511,"limitations that aren’t acknowledged in the paper. The authors should use their best
1259"
LIMITATIONS,0.8886148787505138,"judgment and recognize that individual actions in favor of transparency play an impor-
1260"
LIMITATIONS,0.8890258939580764,"tant role in developing norms that preserve the integrity of the community. Reviewers
1261"
LIMITATIONS,0.8894369091656391,"will be specifically instructed to not penalize honesty concerning limitations.
1262"
THEORY ASSUMPTIONS AND PROOFS,0.8898479243732018,"3. Theory Assumptions and Proofs
1263"
THEORY ASSUMPTIONS AND PROOFS,0.8902589395807645,"Question: For each theoretical result, does the paper provide the full set of assumptions and
1264"
THEORY ASSUMPTIONS AND PROOFS,0.8906699547883272,"a complete (and correct) proof?
1265"
THEORY ASSUMPTIONS AND PROOFS,0.8910809699958898,"Answer: [Yes]
1266"
THEORY ASSUMPTIONS AND PROOFS,0.8914919852034525,"Justification: In the main paper, Theorems, Lemmas, and Corollaries state the assumptions
1267"
THEORY ASSUMPTIONS AND PROOFS,0.8919030004110152,"and theses. Sometimes, these are simplified for the sake of clarity: Complete and formal
1268"
THEORY ASSUMPTIONS AND PROOFS,0.8923140156185779,"statements including proofs are in the Appendices.
1269"
THEORY ASSUMPTIONS AND PROOFS,0.8927250308261405,"Guidelines:
1270"
THEORY ASSUMPTIONS AND PROOFS,0.8931360460337032,"• The answer NA means that the paper does not include theoretical results.
1271"
THEORY ASSUMPTIONS AND PROOFS,0.8935470612412659,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
1272"
THEORY ASSUMPTIONS AND PROOFS,0.8939580764488286,"referenced.
1273"
THEORY ASSUMPTIONS AND PROOFS,0.8943690916563913,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
1274"
THEORY ASSUMPTIONS AND PROOFS,0.8947801068639539,"• The proofs can either appear in the main paper or the supplemental material, but if
1275"
THEORY ASSUMPTIONS AND PROOFS,0.8951911220715166,"they appear in the supplemental material, the authors are encouraged to provide a short
1276"
THEORY ASSUMPTIONS AND PROOFS,0.8956021372790793,"proof sketch to provide intuition.
1277"
THEORY ASSUMPTIONS AND PROOFS,0.896013152486642,"• Inversely, any informal proof provided in the core of the paper should be complemented
1278"
THEORY ASSUMPTIONS AND PROOFS,0.8964241676942046,"by formal proofs provided in appendix or supplemental material.
1279"
THEORY ASSUMPTIONS AND PROOFS,0.8968351829017673,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
1280"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.89724619810933,"4. Experimental Result Reproducibility
1281"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8976572133168927,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
1282"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8980682285244554,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
1283"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.898479243732018,"of the paper (regardless of whether the code and data are provided or not)?
1284"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8988902589395807,"Answer: [Yes]
1285"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8993012741471434,"Justification: We provide all the hyperparameters necessary to replicate our experiments.
1286"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8997122893547062,"Datasets are all publicly available: Breast Cancer, MNIST, and CIFAR-10.
1287"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9001233045622689,"Guidelines:
1288"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9005343197698314,"• The answer NA means that the paper does not include experiments.
1289"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9009453349773942,"• If the paper includes experiments, a No answer to this question will not be perceived
1290"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9013563501849569,"well by the reviewers: Making the paper reproducible is important, regardless of
1291"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9017673653925196,"whether the code and data are provided or not.
1292"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9021783806000822,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
1293"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9025893958076449,"to make their results reproducible or verifiable.
1294"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9030004110152076,"• Depending on the contribution, reproducibility can be accomplished in various ways.
1295"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9034114262227703,"For example, if the contribution is a novel architecture, describing the architecture fully
1296"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.903822441430333,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
1297"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9042334566378956,"be necessary to either make it possible for others to replicate the model with the same
1298"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9046444718454583,"dataset, or provide access to the model. In general. releasing code and data is often
1299"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.905055487053021,"one good way to accomplish this, but reproducibility can also be provided via detailed
1300"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9054665022605837,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
1301"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9058775174681464,"of a large language model), releasing of a model checkpoint, or other means that are
1302"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.906288532675709,"appropriate to the research performed.
1303"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9066995478832717,"• While NeurIPS does not require releasing code, the conference does require all submis-
1304"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9071105630908344,"sions to provide some reasonable avenue for reproducibility, which may depend on the
1305"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9075215782983971,"nature of the contribution. For example
1306"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9079325935059597,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
1307"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9083436087135224,"to reproduce that algorithm.
1308"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9087546239210851,"(b) If the contribution is primarily a new model architecture, the paper should describe
1309"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9091656391286478,"the architecture clearly and fully.
1310"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9095766543362105,"(c) If the contribution is a new model (e.g., a large language model), then there should
1311"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9099876695437731,"either be a way to access this model for reproducing the results or a way to reproduce
1312"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9103986847513358,"the model (e.g., with an open-source dataset or instructions for how to construct
1313"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9108096999588985,"the dataset).
1314"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9112207151664612,"(d) We recognize that reproducibility may be tricky in some cases, in which case
1315"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9116317303740238,"authors are welcome to describe the particular way they provide for reproducibility.
1316"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9120427455815865,"In the case of closed-source models, it may be that access to the model is limited in
1317"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9124537607891492,"some way (e.g., to registered users), but it should be possible for other researchers
1318"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9128647759967119,"to have some path to reproducing or verifying the results.
1319"
OPEN ACCESS TO DATA AND CODE,0.9132757912042746,"5. Open access to data and code
1320"
OPEN ACCESS TO DATA AND CODE,0.9136868064118372,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
1321"
OPEN ACCESS TO DATA AND CODE,0.9140978216193999,"tions to faithfully reproduce the main experimental results, as described in supplemental
1322"
OPEN ACCESS TO DATA AND CODE,0.9145088368269626,"material?
1323"
OPEN ACCESS TO DATA AND CODE,0.9149198520345253,"Answer: [Yes]
1324"
OPEN ACCESS TO DATA AND CODE,0.915330867242088,"Justification: Most of the codes have been released in the supplementary material. The
1325"
OPEN ACCESS TO DATA AND CODE,0.9157418824496506,"missing ones are simply the implementations of the numerical integration of the SDEs,
1326"
OPEN ACCESS TO DATA AND CODE,0.9161528976572133,"which consist of applying Euler-Maruyama: All code will be released in an appropriate
1327"
OPEN ACCESS TO DATA AND CODE,0.916563912864776,"GitHub repository upon publication.
1328"
OPEN ACCESS TO DATA AND CODE,0.9169749280723387,"Guidelines:
1329"
OPEN ACCESS TO DATA AND CODE,0.9173859432799013,"• The answer NA means that paper does not include experiments requiring code.
1330"
OPEN ACCESS TO DATA AND CODE,0.917796958487464,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
1331"
OPEN ACCESS TO DATA AND CODE,0.9182079736950267,"public/guides/CodeSubmissionPolicy) for more details.
1332"
OPEN ACCESS TO DATA AND CODE,0.9186189889025894,"• While we encourage the release of code and data, we understand that this might not be
1333"
OPEN ACCESS TO DATA AND CODE,0.9190300041101521,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
1334"
OPEN ACCESS TO DATA AND CODE,0.9194410193177147,"including code, unless this is central to the contribution (e.g., for a new open-source
1335"
OPEN ACCESS TO DATA AND CODE,0.9198520345252774,"benchmark).
1336"
OPEN ACCESS TO DATA AND CODE,0.9202630497328401,"• The instructions should contain the exact command and environment needed to run to
1337"
OPEN ACCESS TO DATA AND CODE,0.9206740649404028,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1338"
OPEN ACCESS TO DATA AND CODE,0.9210850801479655,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1339"
OPEN ACCESS TO DATA AND CODE,0.9214960953555281,"• The authors should provide instructions on data access and preparation, including how
1340"
OPEN ACCESS TO DATA AND CODE,0.9219071105630908,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1341"
OPEN ACCESS TO DATA AND CODE,0.9223181257706535,"• The authors should provide scripts to reproduce all experimental results for the new
1342"
OPEN ACCESS TO DATA AND CODE,0.9227291409782162,"proposed method and baselines. If only a subset of experiments are reproducible, they
1343"
OPEN ACCESS TO DATA AND CODE,0.9231401561857788,"should state which ones are omitted from the script and why.
1344"
OPEN ACCESS TO DATA AND CODE,0.9235511713933415,"• At submission time, to preserve anonymity, the authors should release anonymized
1345"
OPEN ACCESS TO DATA AND CODE,0.9239621866009042,"versions (if applicable).
1346"
OPEN ACCESS TO DATA AND CODE,0.9243732018084669,"• Providing as much information as possible in supplemental material (appended to the
1347"
OPEN ACCESS TO DATA AND CODE,0.9247842170160296,"paper) is recommended, but including URLs to data and code is permitted.
1348"
OPEN ACCESS TO DATA AND CODE,0.9251952322235922,"6. Experimental Setting/Details
1349"
OPEN ACCESS TO DATA AND CODE,0.9256062474311549,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1350"
OPEN ACCESS TO DATA AND CODE,0.9260172626387176,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1351"
OPEN ACCESS TO DATA AND CODE,0.9264282778462803,"results?
1352"
OPEN ACCESS TO DATA AND CODE,0.9268392930538429,"Answer: [Yes]
1353"
OPEN ACCESS TO DATA AND CODE,0.9272503082614056,"Justification: We describe all the experimental settings in Section F.
1354"
OPEN ACCESS TO DATA AND CODE,0.9276613234689683,"Guidelines:
1355"
OPEN ACCESS TO DATA AND CODE,0.928072338676531,"• The answer NA means that the paper does not include experiments.
1356"
OPEN ACCESS TO DATA AND CODE,0.9284833538840938,"• The experimental setting should be presented in the core of the paper to a level of detail
1357"
OPEN ACCESS TO DATA AND CODE,0.9288943690916563,"that is necessary to appreciate the results and make sense of them.
1358"
OPEN ACCESS TO DATA AND CODE,0.929305384299219,"• The full details can be provided either with the code, in appendix, or as supplemental
1359"
OPEN ACCESS TO DATA AND CODE,0.9297163995067818,"material.
1360"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9301274147143445,"7. Experiment Statistical Significance
1361"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9305384299219072,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1362"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9309494451294698,"information about the statistical significance of the experiments?
1363"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9313604603370325,"Answer: [Yes]
1364"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9317714755445952,"Justification: Our figures report error bars when relevant.
1365"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9321824907521579,"Guidelines:
1366"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9325935059597205,"• The answer NA means that the paper does not include experiments.
1367"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9330045211672832,"• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
1368"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9334155363748459,"dence intervals, or statistical significance tests, at least for the experiments that support
1369"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9338265515824086,"the main claims of the paper.
1370"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9342375667899713,"• The factors of variability that the error bars are capturing should be clearly stated (for
1371"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9346485819975339,"example, train/test split, initialization, random drawing of some parameter, or overall
1372"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9350595972050966,"run with given experimental conditions).
1373"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9354706124126593,"• The method for calculating the error bars should be explained (closed form formula,
1374"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.935881627620222,"call to a library function, bootstrap, etc.)
1375"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9362926428277847,"• The assumptions made should be given (e.g., Normally distributed errors).
1376"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9367036580353473,"• It should be clear whether the error bar is the standard deviation or the standard error
1377"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.93711467324291,"of the mean.
1378"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9375256884504727,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1379"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9379367036580354,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1380"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.938347718865598,"of Normality of errors is not verified.
1381"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9387587340731607,"• For asymmetric distributions, the authors should be careful not to show in tables or
1382"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9391697492807234,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1383"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9395807644882861,"error rates).
1384"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9399917796958488,"• If error bars are reported in tables or plots, The authors should explain in the text how
1385"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9404027949034114,"they were calculated and reference the corresponding figures or tables in the text.
1386"
EXPERIMENTS COMPUTE RESOURCES,0.9408138101109741,"8. Experiments Compute Resources
1387"
EXPERIMENTS COMPUTE RESOURCES,0.9412248253185368,"Question: For each experiment, does the paper provide sufficient information on the com-
1388"
EXPERIMENTS COMPUTE RESOURCES,0.9416358405260995,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1389"
EXPERIMENTS COMPUTE RESOURCES,0.9420468557336621,"the experiments?
1390"
EXPERIMENTS COMPUTE RESOURCES,0.9424578709412248,"Answer: [Yes]
1391"
EXPERIMENTS COMPUTE RESOURCES,0.9428688861487875,"Justification: As we state in Section F, we run our experiments on an NVIDIA GeForce
1392"
EXPERIMENTS COMPUTE RESOURCES,0.9432799013563502,"RTX 3090.
1393"
EXPERIMENTS COMPUTE RESOURCES,0.9436909165639129,"Guidelines:
1394"
EXPERIMENTS COMPUTE RESOURCES,0.9441019317714755,"• The answer NA means that the paper does not include experiments.
1395"
EXPERIMENTS COMPUTE RESOURCES,0.9445129469790382,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1396"
EXPERIMENTS COMPUTE RESOURCES,0.9449239621866009,"or cloud provider, including relevant memory and storage.
1397"
EXPERIMENTS COMPUTE RESOURCES,0.9453349773941636,"• The paper should provide the amount of compute required for each of the individual
1398"
EXPERIMENTS COMPUTE RESOURCES,0.9457459926017263,"experimental runs as well as estimate the total compute.
1399"
EXPERIMENTS COMPUTE RESOURCES,0.9461570078092889,"• The paper should disclose whether the full research project required more compute
1400"
EXPERIMENTS COMPUTE RESOURCES,0.9465680230168516,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1401"
EXPERIMENTS COMPUTE RESOURCES,0.9469790382244143,"didn’t make it into the paper).
1402"
CODE OF ETHICS,0.947390053431977,"9. Code Of Ethics
1403"
CODE OF ETHICS,0.9478010686395396,"Question: Does the research conducted in the paper conform, in every respect, with the
1404"
CODE OF ETHICS,0.9482120838471023,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1405"
CODE OF ETHICS,0.948623099054665,"Answer: [Yes]
1406"
CODE OF ETHICS,0.9490341142622277,"Justification: All we do is derive some convergence bounds and similar results.
1407"
CODE OF ETHICS,0.9494451294697904,"Guidelines:
1408"
CODE OF ETHICS,0.949856144677353,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1409"
CODE OF ETHICS,0.9502671598849157,"• If the authors answer No, they should explain the special circumstances that require a
1410"
CODE OF ETHICS,0.9506781750924784,"deviation from the Code of Ethics.
1411"
CODE OF ETHICS,0.9510891903000411,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1412"
CODE OF ETHICS,0.9515002055076038,"eration due to laws or regulations in their jurisdiction).
1413"
BROADER IMPACTS,0.9519112207151664,"10. Broader Impacts
1414"
BROADER IMPACTS,0.9523222359227291,"Question: Does the paper discuss both potential positive societal impacts and negative
1415"
BROADER IMPACTS,0.9527332511302918,"societal impacts of the work performed?
1416"
BROADER IMPACTS,0.9531442663378545,"Answer: [Yes]
1417"
BROADER IMPACTS,0.9535552815454171,"Justification: It can have a positive impact as it helps understand adaptive optimizers better.
1418"
BROADER IMPACTS,0.9539662967529798,"Possibly, it might help reduce the cost of fine-tuning thanks to our novel scaling law.
1419"
BROADER IMPACTS,0.9543773119605425,"Guidelines:
1420"
BROADER IMPACTS,0.9547883271681052,"• The answer NA means that there is no societal impact of the work performed.
1421"
BROADER IMPACTS,0.955199342375668,"• If the authors answer NA or No, they should explain why their work has no societal
1422"
BROADER IMPACTS,0.9556103575832305,"impact or why the paper does not address societal impact.
1423"
BROADER IMPACTS,0.9560213727907932,"• Examples of negative societal impacts include potential malicious or unintended uses
1424"
BROADER IMPACTS,0.956432387998356,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1425"
BROADER IMPACTS,0.9568434032059187,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1426"
BROADER IMPACTS,0.9572544184134812,"groups), privacy considerations, and security considerations.
1427"
BROADER IMPACTS,0.957665433621044,"• The conference expects that many papers will be foundational research and not tied
1428"
BROADER IMPACTS,0.9580764488286067,"to particular applications, let alone deployments. However, if there is a direct path to
1429"
BROADER IMPACTS,0.9584874640361694,"any negative applications, the authors should point it out. For example, it is legitimate
1430"
BROADER IMPACTS,0.9588984792437321,"to point out that an improvement in the quality of generative models could be used to
1431"
BROADER IMPACTS,0.9593094944512947,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1432"
BROADER IMPACTS,0.9597205096588574,"that a generic algorithm for optimizing neural networks could enable people to train
1433"
BROADER IMPACTS,0.9601315248664201,"models that generate Deepfakes faster.
1434"
BROADER IMPACTS,0.9605425400739828,"• The authors should consider possible harms that could arise when the technology is
1435"
BROADER IMPACTS,0.9609535552815455,"being used as intended and functioning correctly, harms that could arise when the
1436"
BROADER IMPACTS,0.9613645704891081,"technology is being used as intended but gives incorrect results, and harms following
1437"
BROADER IMPACTS,0.9617755856966708,"from (intentional or unintentional) misuse of the technology.
1438"
BROADER IMPACTS,0.9621866009042335,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1439"
BROADER IMPACTS,0.9625976161117962,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1440"
BROADER IMPACTS,0.9630086313193588,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1441"
BROADER IMPACTS,0.9634196465269215,"feedback over time, improving the efficiency and accessibility of ML).
1442"
SAFEGUARDS,0.9638306617344842,"11. Safeguards
1443"
SAFEGUARDS,0.9642416769420469,"Question: Does the paper describe safeguards that have been put in place for responsible
1444"
SAFEGUARDS,0.9646526921496096,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1445"
SAFEGUARDS,0.9650637073571722,"image generators, or scraped datasets)?
1446"
SAFEGUARDS,0.9654747225647349,"Answer: [NA]
1447"
SAFEGUARDS,0.9658857377722976,"Justification: The paper poses no such risks.
1448"
SAFEGUARDS,0.9662967529798603,"Guidelines:
1449"
SAFEGUARDS,0.966707768187423,"• The answer NA means that the paper poses no such risks.
1450"
SAFEGUARDS,0.9671187833949856,"• Released models that have a high risk for misuse or dual-use should be released with
1451"
SAFEGUARDS,0.9675297986025483,"necessary safeguards to allow for controlled use of the model, for example by requiring
1452"
SAFEGUARDS,0.967940813810111,"that users adhere to usage guidelines or restrictions to access the model or implementing
1453"
SAFEGUARDS,0.9683518290176737,"safety filters.
1454"
SAFEGUARDS,0.9687628442252363,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1455"
SAFEGUARDS,0.969173859432799,"should describe how they avoided releasing unsafe images.
1456"
SAFEGUARDS,0.9695848746403617,"• We recognize that providing effective safeguards is challenging, and many papers do
1457"
SAFEGUARDS,0.9699958898479244,"not require this, but we encourage authors to take this into account and make a best
1458"
SAFEGUARDS,0.9704069050554871,"faith effort.
1459"
LICENSES FOR EXISTING ASSETS,0.9708179202630497,"12. Licenses for existing assets
1460"
LICENSES FOR EXISTING ASSETS,0.9712289354706124,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1461"
LICENSES FOR EXISTING ASSETS,0.9716399506781751,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1462"
LICENSES FOR EXISTING ASSETS,0.9720509658857378,"properly respected?
1463"
LICENSES FOR EXISTING ASSETS,0.9724619810933004,"Answer: [Yes]
1464"
LICENSES FOR EXISTING ASSETS,0.9728729963008631,"Justification: We cite the used datasets. The rest is all our code and we cite the most relevant
1465"
LICENSES FOR EXISTING ASSETS,0.9732840115084258,"libraries used.
1466"
LICENSES FOR EXISTING ASSETS,0.9736950267159885,"Guidelines:
1467"
LICENSES FOR EXISTING ASSETS,0.9741060419235512,"• The answer NA means that the paper does not use existing assets.
1468"
LICENSES FOR EXISTING ASSETS,0.9745170571311138,"• The authors should cite the original paper that produced the code package or dataset.
1469"
LICENSES FOR EXISTING ASSETS,0.9749280723386765,"• The authors should state which version of the asset is used and, if possible, include a
1470"
LICENSES FOR EXISTING ASSETS,0.9753390875462392,"URL.
1471"
LICENSES FOR EXISTING ASSETS,0.9757501027538019,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1472"
LICENSES FOR EXISTING ASSETS,0.9761611179613646,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1473"
LICENSES FOR EXISTING ASSETS,0.9765721331689272,"service of that source should be provided.
1474"
LICENSES FOR EXISTING ASSETS,0.9769831483764899,"• If assets are released, the license, copyright information, and terms of use in the
1475"
LICENSES FOR EXISTING ASSETS,0.9773941635840526,"package should be provided. For popular datasets, paperswithcode.com/datasets
1476"
LICENSES FOR EXISTING ASSETS,0.9778051787916153,"has curated licenses for some datasets. Their licensing guide can help determine the
1477"
LICENSES FOR EXISTING ASSETS,0.9782161939991779,"license of a dataset.
1478"
LICENSES FOR EXISTING ASSETS,0.9786272092067406,"• For existing datasets that are re-packaged, both the original license and the license of
1479"
LICENSES FOR EXISTING ASSETS,0.9790382244143033,"the derived asset (if it has changed) should be provided.
1480"
LICENSES FOR EXISTING ASSETS,0.979449239621866,"• If this information is not available online, the authors are encouraged to reach out to
1481"
LICENSES FOR EXISTING ASSETS,0.9798602548294287,"the asset’s creators.
1482"
NEW ASSETS,0.9802712700369913,"13. New Assets
1483"
NEW ASSETS,0.980682285244554,"Question: Are new assets introduced in the paper well documented and is the documentation
1484"
NEW ASSETS,0.9810933004521167,"provided alongside the assets?
1485"
NEW ASSETS,0.9815043156596794,"Answer: [NA]
1486"
NEW ASSETS,0.9819153308672421,"Justification: The paper does not release new assets
1487"
NEW ASSETS,0.9823263460748047,"Guidelines:
1488"
NEW ASSETS,0.9827373612823674,"• The answer NA means that the paper does not release new assets.
1489"
NEW ASSETS,0.9831483764899301,"• Researchers should communicate the details of the dataset/code/model as part of their
1490"
NEW ASSETS,0.9835593916974928,"submissions via structured templates. This includes details about training, license,
1491"
NEW ASSETS,0.9839704069050554,"limitations, etc.
1492"
NEW ASSETS,0.9843814221126181,"• The paper should discuss whether and how consent was obtained from people whose
1493"
NEW ASSETS,0.9847924373201808,"asset is used.
1494"
NEW ASSETS,0.9852034525277436,"• At submission time, remember to anonymize your assets (if applicable). You can either
1495"
NEW ASSETS,0.9856144677353063,"create an anonymized URL or include an anonymized zip file.
1496"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860254829428688,"14. Crowdsourcing and Research with Human Subjects
1497"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864364981504316,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1498"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868475133579943,"include the full text of instructions given to participants and screenshots, if applicable, as
1499"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987258528565557,"well as details about compensation (if any)?
1500"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876695437731196,"Answer: [NA]
1501"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880805589806823,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1502"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988491574188245,"Guidelines:
1503"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889025893958077,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1504"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893136046033704,"human subjects.
1505"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989724619810933,"• Including this information in the supplemental material is fine, but if the main contribu-
1506"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901356350184957,"tion of the paper involves human subjects, then as much detail as possible should be
1507"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905466502260584,"included in the main paper.
1508"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9909576654336211,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1509"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913686806411838,"or other labor should be paid at least the minimum wage in the country of the data
1510"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917796958487464,"collector.
1511"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921907110563091,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1512"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926017262638718,"Subjects
1513"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930127414714345,"Question: Does the paper describe potential risks incurred by study participants, whether
1514"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934237566789971,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1515"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938347718865598,"approvals (or an equivalent approval/review based on the requirements of your country or
1516"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942457870941225,"institution) were obtained?
1517"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946568023016852,"Answer: [NA]
1518"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950678175092479,"Justification: The paper does not involve crowdsourcing nor research with human subjects
1519"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954788327168105,"Guidelines:
1520"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958898479243732,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1521"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9963008631319359,"human subjects.
1522"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967118783394986,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1523"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9971228935470613,"may be required for any human subjects research. If you obtained IRB approval, you
1524"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9975339087546239,"should clearly state this in the paper.
1525"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979449239621866,"• We recognize that the procedures for this may vary significantly between institutions
1526"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983559391697493,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1527"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998766954377312,"guidelines for their institution.
1528"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991779695848746,"• For initial submissions, do not include any information that would break anonymity (if
1529"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9995889847924373,"applicable), such as the institution conducting the review.
1530"
