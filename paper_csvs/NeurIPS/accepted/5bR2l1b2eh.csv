Section,Section Appearance Order,Paragraph
CARNEGIE MELLON UNIVERISTY,0.0,"1Carnegie Mellon Univeristy
2Stevens Institute of Technology
3FAIR at Meta
{yangzho6, zhuominc, beidic}@andrew.cmu.edu
zxu79@stevens.edu victorialin@meta.com"
ABSTRACT,0.002777777777777778,Abstract
ABSTRACT,0.005555555555555556,"With the blossom of large language models (LLM), inference efficiency becomes
increasingly important. Various approximate methods are proposed to reduce the
cost at inference time. Contextual Sparsity (CS) is appealing for its training-free
nature and its ability to reach a higher compression ratio seemingly without
significant performance degradation. However, after a comprehensive evaluation
of contextual sparsity methods on various complex generation tasks, we find that
although CS succeeds in prompt-understanding tasks, it significantly degrades
the model performance for reasoning, deduction, and knowledge-based tasks.
Despite the gap in end-to-end accuracy, we observed that sparse models and original
models often share the general problem-solving logic and require only a few token
corrections to recover the original model performance. This paper introduces
SIRIUS1, an efficient correction mechanism, which significantly boosts CS models
on reasoning tasks while maintaining its efficiency gain. SIRIUS is evaluated on
6 models with 8 difficult generation tasks in reasoning, deduction, and coding
and shows consistent effectiveness and efficiency. Also, we carefully develop a
system implementation for SIRIUS and show that SIRIUS delivers theoretical latency
reduction with roughly 20% reduction in latency for 8B model on-chip and 35%
reduction in latency for 70B model offloading. We open-source our implementation
of Sirius at https://github.com/Infini-AI-Lab/Sirius.git."
INTRODUCTION,0.008333333333333333,"1
Introduction"
INTRODUCTION,0.011111111111111112,"Large Language Models (LLM), such as OpenAI et al. (2024) (GPT-4), Team et al. (2024) (Gemini),
and Touvron et al. (2023) (Llama) have demonstrated their proficiency in a wide range of natural
language processing applications such as content creation, summarization, and impressive and complex
reasoning tasks. However, their deployment is very challenging, especially in latency-sensitive
settings (Kaplan et al., 2020). Exploiting the model sparsity is a natural way to reduce the model
parameter size and computational cost with a long history (LeCun et al., 1989; Tibshirani, 1996). More
recently, many studies have shown that contextual sparsity (Liu et al., 2023b; Li et al., 2022; Dong
et al., 2024; Lee et al., 2024), which highly correlates to the prompt or the context, can greatly speed
up LLM inference without quality degradation."
INTRODUCTION,0.013888888888888888,"However, in this paper, we first demonstrate a critical and fundamental problem with contextual
sparsity (CS): while generally robust in classification tasks and generation tasks that mainly rely on
prompt understanding (e.g., summarization, chat question-answering), we found that CS models
struggle at high-level reasoning and understanding tasks."
INTRODUCTION,0.016666666666666666,"1We draw inspiration from the astronomical concept, in which SIRIUS refers to a two-body star system, where
one is the brightest star ever detected, while the other is a dim star."
INTRODUCTION,0.019444444444444445,"(a) Comparing Sparsity Method 
on CNN/DailyMail and GSM8K"
INTRODUCTION,0.022222222222222223,"(b) Contextual Sparsity applying 
to Llama-3-70B-Instruct"
INTRODUCTION,0.025,"(c) Correcting only 11% of tokens 
brings Csparse to full performance"
INTRODUCTION,0.027777777777777776,"Figure 1: Contextual sparse models struggle at challenging text generation tests that require high-
level reasoning and understanding, e.g. GSM8K. On these tasks, contextually sparse models lead to
significant quality degradation. In (a), we contrast CS Llama-3-8B-Instruct on GSM8K (green) and
CNN DailyMail (coral). (b) Contextual Sparsity Llama-3-70B-Instruct crashes at 50% global sparsity,
making the smaller dense model Llama-3-8B-Instruct (green star) a significantly more efficient choice
than the sparse 70B model. (c) Sparse model crashing at reasoning tasks has patterns, and ideally only
correcting 11% unlikely tokens recovers the sparse model performance fully."
INTRODUCTION,0.030555555555555555,"For example, in Figure 1 (a), we contrast between the Text Summarization task (CNN/DailyMail)
and Arithmetic Reasoning (GSM8K) with contextual sparsity methods on Llama-3-8B-Instruct.
Varying sparsity levels, Llama-3-8B-Instruct with contextual sparsity performs consistently worse
on GSM8K than CNN/DailyMail. With roughly 50% sparsity globally, the sparse model degradation
is still reasonable on text summarization (right axis in color coral) compared to almost collapsed on
arithmetic reasoning (left axis in color green). However, for reasoning tasks, can we simply live with
a higher density ratio to preserve more performance? Unfortunately, the answer is NO. Besides
the significant efficiency loss, shown in Figure 1 (b), the Llama-3-70B-Instruct model with contextual
sparsity also crashes at around 50% sparsity globally. The 50% sparse model still has 4× the parameter
size compared to the smaller dense model (Llama-3-8B-Instruct), while still performing worse on
GSM8K-CoT, rendering contextual sparsity utterly not useful for complex reasoning tasks."
INTRODUCTION,0.03333333333333333,"We conduct an in-depth study on the CS model failure cases and notice that the overall reasoning
pathway of these sparse models is usually sound and adheres to the full model. The fatal mistakes
are always caused by some middle tokens and propagate towards the end, examples can be seen in
Figure 4. Following this observation, we conduct a simple experiment with 65% Contextual Sparse
Llama-3-8B-Instruct on GSM8K as presented in Figure 1 (c). We run both the sparse and the full
models together for the same prompt and compare two generation output token-by-token."
INTRODUCTION,0.03611111111111111,"Surprisingly, the trend increases steeply with the percentage of corrected tokens. Correction of only
6% tokens in the sparse model’s generation recovers most of GSM8K accuracy (<5% to full), and 11%
to recover the full performance. The results show potential for an efficient and powerful correction
mechanism to maintain the sparse efficiency while boosting its performance. Contextual sparsity uses
a dynamic sparsity pattern and naturally requires the full model to be in GPU memory during runtime,
allowing the full model to be used efficiently for infrequent correction. Even though only very few
need to be corrected, locating these mistaken tokens efficiently turns out to be challenging."
INTRODUCTION,0.03888888888888889,"Ideally, we want a correction system to have the following properties: 1) Effective, the sparse model
quality degradation can be improved to the full model vicinity; 2) Cheap, the full model only gives
minimal intervention; 3) Adaptive, the system is efficient across various reasoning datasets."
INTRODUCTION,0.041666666666666664,"In this paper, we carefully analyze and formulate correction efficiency in Section 2. We extensively
categorize the strengths and weaknesses of CS in Section 3. In Section 4, We systematically design
SIRIUS, a correction method covering all three desired properties. (When?) In Section 4.1, we show
that the sparse model can be both confident or uncertain when making mistakes, rendering the signal
from sparse unreliable for determining when to correct. SIRIUS is a period-based method with the
period as a hyperparameter. (How?) In Sections 4.1 and B.2, we introduce novel KV Cache direct
rewriting, minimal rollbacks, and hardware-efficient tree building to help increase the effective period
of full model correction, thus, ensuring the correction efficiency."
INTRODUCTION,0.044444444444444446,Wrong KV Cache
INTRODUCTION,0.04722222222222222,"Full Model
(Call infrequently, doing correction)   
Sparse Model  
(Sparse Pattern Is Contextually Aware) +"
INTRODUCTION,0.05,New Input
INTRODUCTION,0.05277777777777778,"Intermediate mistakes eventually 
leads to wrong end-result"
INTRODUCTION,0.05555555555555555,"Intermediate mistake  
Wrong Result"
INTRODUCTION,0.058333333333333334,Rewriting the KV Cache
INTRODUCTION,0.06111111111111111,Correcting tokens Full Model deemed Unlikely +
INTRODUCTION,0.06388888888888888,Interleaving with high-quality token
INTRODUCTION,0.06666666666666667,"Infrequently, we use the in-memory 
Full Model to check the past chunk"
INTRODUCTION,0.06944444444444445,Correct Result
INTRODUCTION,0.07222222222222222,"With correction, 
Sirius gets it correct"
INTRODUCTION,0.075,"Figure 2: Overview of Sirius. Contextual Sparsity requires full model weights to be placed on the GPU
memory. While the sparse model doesn’t perform well on complex reasoning tasks, Sirius uses the Full
Model to correct the Sparse model. The full model is called fairly infrequently. During the correction,
the Full Model will rewrite the KV Cache, interleave with high-quality tokens to the sparse outputs,
and then roll back only when the token is deemed extremely unlikely by the Full Model."
INTRODUCTION,0.07777777777777778,"In Section 5, we empirically evaluated SIRIUS on 6 different models with 8 different reasoning tasks
and showed that SIRIUS is generally effective and efficient. On GSM8K and Llama-3-8B-Instruct
specifically, we boost the fine-grained sparsity from 58% to 72% with 4% increase in effective
parameter size and coarse-grained sparsity from 38% to 70% with the cost of 5% effective parameter
size. We also show that SIRIUS delivers the promised efficiency on mainstream GPUs in both on-chip
and offloading settings."
RELATED WORKS AND PROBLEM FORMULATION,0.08055555555555556,"2
Related Works and Problem Formulation"
RELATED WORKS AND PROBLEM FORMULATION,0.08333333333333333,"In this section, we first present the classification of the prior Contextual Sparsity methods and narrate
important efficiency metrics in. Also, we present careful analysis and quantitative comparisons on
why Speculative Decoding is inefficient in recovering contextual sparsity. Due to space constraints,
we refer to Appendix A.2. For extended related works on model compression, contextual sparsity,
and speculative decoding, we present in Appendix A.1."
RELATED WORKS AND PROBLEM FORMULATION,0.08611111111111111,"Contextual Sparsity Classification - Contextual sparsity (CS) methods are usually training-free, easy
to use, and seemingly effective, making them highly attractive to ML practitioners looking to reduce
LLM inference costs. CS exists naturally in MLP layers of the LLM, which occupies roughly 70%
of the LLM total weights (Dong et al., 2024; Lee et al., 2024)). The contextual sparsity selection is as
follows: given the context, only a limited number of the most relevant neurons are selected based on the
input activation. The rest contributed to the output far less is discarded. We refer to two main directions
of contextual sparsity methods as Coarse-grained Sparsity (CSparse) Methods (Dong et al. (2024))
- that within the same input prompt, the sparsity pattern is fixed for all tokens generated. Fine-grained
Sparsity (FSparse) Methods (Lee et al. (2024)) - that exploits the per-token sparsity to save resources."
RELATED WORKS AND PROBLEM FORMULATION,0.08888888888888889,"Average Parameters Used Per Token - A key metric is used to evaluate the efficiency of our proposed
method, the Average Parameter Used per token decoded (later referred to as APU). LLM inference
is memory I/O bound (Leviathan et al., 2023; Kim et al., 2023). The latency of generating every single
token is dominated by the memory loading time from the GPU HBM to SRAM. On the other hand, SIR-
IUS relies on full model parallel verifying a chunk of tokens. Although from the FLOPs standpoint, the
amount of compute performed per evaluation step is the number of input token times of a single token in-
put process, the latency of parallel verification is still roughly the same as taking a single token (Verified
further in 10, length 64 is only 1.1 ms longer than length 1), because the inference is memory bound."
RELATED WORKS AND PROBLEM FORMULATION,0.09166666666666666,"SIRIUS operates in the memory-bound regime (single inference sequence length smaller than or equal
to 64). Thus, the average parameter count of a model gives us a rough judgment of the latency of
inference. Formally, for a full LLM to have Cfull number of parameters, and its sparse counterpart
of a certain predetermined sparsity Csparse. The average advancement length (later we refer to as
AAL) in the number of tokens between two consecutive LLM corrections can be represented as nAAL."
RELATED WORKS AND PROBLEM FORMULATION,0.09444444444444444,The average parameters used per token (APU) are the following
RELATED WORKS AND PROBLEM FORMULATION,0.09722222222222222,APU= nsparseCsparse+Cfull
RELATED WORKS AND PROBLEM FORMULATION,0.1,"nAAL
(1)"
RELATED WORKS AND PROBLEM FORMULATION,0.10277777777777777,"We want the metric to be as small as possible, and obviously, we want nAAL to be as large as possible."
RELATED WORKS AND PROBLEM FORMULATION,0.10555555555555556,"Another thing to note is that we always compare the system’s APU against the full model’s APU, which
is Cfull. If we divided the above equation by Cfull, we can have an equivalent parameter density of
the system defined based on Iglobalsparsity, which is Csparse/Cfull."
RELATED WORKS AND PROBLEM FORMULATION,0.10833333333333334,Effective Density= nsparseIglobalsparsity+1
RELATED WORKS AND PROBLEM FORMULATION,0.1111111111111111,"nAAL
(2)"
RELATED WORKS AND PROBLEM FORMULATION,0.11388888888888889,"Later, if we use period nperiod, the equation can be rewritten as"
RELATED WORKS AND PROBLEM FORMULATION,0.11666666666666667,Effective Density= (nperiod−1)Iglobalsparsity+1
RELATED WORKS AND PROBLEM FORMULATION,0.11944444444444445,"nAAL
(3)"
RELATED WORKS AND PROBLEM FORMULATION,0.12222222222222222,"Later when presenting SIRIUS, we mainly specify nperiod with nAAL to evaluate its efficiency. Notice
that Iglobalsparsity is determined by the sparsity method, SIRIUS cannot change it anymore."
OBSERVATIONS,0.125,"3
Observations"
OBSERVATIONS,0.12777777777777777,"In this section, we present a detailed study of the strengths and weaknesses of Contextual Sparsity
(CS). 3.1 presents the strengths of CS. 3.2 presents the weaknesses of CS. Additionally, we show that
given the similar parameter size, the more well-trained the model is, the more CS degradation will
be for the model. Due to limited space, we present the details in Appendix B.1. 3.3 shows our findings
when looking into the failure cases of the CS model in complex reasoning generation tasks."
OBSERVATIONS,0.13055555555555556,"In the following series of experiments, we build our implementation2 of fine-grained sparsity based
on Lee et al. (2024) and coarse-grained sparsity based on Dong et al. (2024). The default sparsity for
both methods is 50% for the MLP component of the model (whole MLP for coarse-grained sparsity
and Up and Down linear layers only for fine-grained sparsity). We mainly use this default setting in
most experiment tables in the paper without explicitly mentioning it. Otherwise, we will explicitly
specify the different sparsity levels we used."
OBSERVATIONS,0.13333333333333333,"3.1
Contextual Sparsity: Where Does It Succeed?"
OBSERVATIONS,0.1361111111111111,"For tasks on prompt understanding, CS generally performs well and gives consistent and strong
output. We evaluate CS models on machine summarization (CNNDailyMail, See et al. (2017)),
and Conversational Question Answering (CoQA, Reddy et al. (2019)). The results show that the
correctly selected contextual sparsity in the MLP layers and the full attention layers can fully extract
and understand the local prompt information. More details are presented in Figure 3, where we show
that by varying the sparsity level, the language model’s performance on CNN/DailyMail is robust
even when the activation sparsity drops to below 20%, which translates to around 44% global density."
OBSERVATIONS,0.1388888888888889,"For tasks accessing factuality and hallucination, we select the generation portion of the TruthfulQA
dataset (Lin et al., 2022). Results are shown in Table 1, where we evaluate the techniques on 5 different
LLMs. Interestingly, we find that the Fine-grained sparsity is often better than the dense model baseline
across different models. This finding is consistent with previous works Laser (Sharma et al., 2023)
and Dola (Chuang et al., 2024). They both observed that compressing the original LLM in a carefully
designed way would lead to improvement in factuality and better de-hallucination. Laser comes from
the low-rank approximation of the MLP layers, while Dola proposes a factuality-aware layer-skipping
algorithm. Based on their findings, hallucination occurs when parts of the weights aren’t as well-versed
in the given input as the other parts. They expose the “averaging” effect that blurs the factuality of
the output. Removing these neurons gives rise to better facutality and less hallucination. Our studies
look at the same problem from a neuron sparsity standpoint."
OBSERVATIONS,0.14166666666666666,"2Since Lee et al. (2024) doesn’t open-source its implementation and it relies on the threshold for determining
the sparsity pattern, replicating the method isn’t straightforward. Using a threshold also increases the difficulty of
determining the actual density of the sparse model. Our implementation uses topk on the Gate Layer activations.
The rest is implemented as described in the original method."
OBSERVATIONS,0.14444444444444443,"Table 1: We show the difference between cases when Contextual Sparsity (CS) succeeds or fails. CS
is generally good at prompt understanding tasks and tasks that measure the trustworthiness of the
language models while not good at tasks that require reasoning and world knowledge understanding."
OBSERVATIONS,0.14722222222222223,"Where CS Succeeds
CNN/DailyMail
CoQA
TruthfulQA
Experiment Settings
Unitxt Rouge
EM/F1
Rouge-1/2 ACC"
OBSERVATIONS,0.15,"Llama-3-8B-Instruct
0.1237
0.6153/0.7825
0.4945/0.3647
Llama-3-8B-Instruct-CSparse
0.1144
0.6633/0.7977
0.4725/0.3403
Llama-3-8B-Instruct-FSparse
0.1166
0.6625/0.7984
0.5043/0.3305"
OBSERVATIONS,0.1527777777777778,"Llama-2-7B-Chat
0.1489
0.5982/0.7580
0.4480/0.3831
Llama-2-7B-Chat-CSparse
0.1448
0.6117/0.7639
0.4529/0.3843
Llama-2-7B-Chat-FSparse
0.1521
0.5898/0.7540
0.4565/0.3660"
OBSERVATIONS,0.15555555555555556,"Where CS Fails
GSM8K
HumanEval
MMLU*
Experiment Settings
ACC (strict/flexible)
Pass@1 (GD)
Accuracy"
OBSERVATIONS,0.15833333333333333,"Llama-3-8B-Instruct
0.7551/0.7544
0.560
0.6231
Llama-3-8B-Instruct-CSparse
0.3859/0.3874
0.207
0.5558
Llama-3-8B-Instruct-FSparse
0.5868/0.5891
0.457
0.5304"
OBSERVATIONS,0.16111111111111112,"Llama-2-7B-Chat
0.2396/0.2462
0.140
0.492
Llama-2-7B-Chat-CSparse
0.1334/0.1380
0.067
0.4637
Llama-2-7B-Chat-FSparse
0.1979/0.2017
0.134
0.4768"
OBSERVATIONS,0.1638888888888889,"* MMLU is a classification task, not generation tasks. We use MMLU-FLAN-COT"
OBSERVATIONS,0.16666666666666666,"Figure 3: We contrast between Contextual Sparsity on prompt understanding task and complex
generation tasks that require reasoning. (a) Both CSparse and FSparse are robust on CNN/DailyMail
for various sparsity; (b) and (c) Show that both CSparse and FSparse crash on GSM8K and HumanEval
at the global sparsity that they are still robust in prompt understanding tasks."
OBSERVATIONS,0.16944444444444445,"3.2
Contextual Sparsity: Where Does It Fail?"
OBSERVATIONS,0.17222222222222222,"On the other hand, contextual sparsity severely struggles when the generation tasks rely solely on the
model’s own reasoning and world knowledge understanding ability. Here we show the Llama-3-8B-
Instruct and the Llama-2-7B-Chat models in Table 1, refer to Table 12 for evaluations on more models.
Notice that since fine-grained sparsity method needs the activation from Gate MLP for selecting sparsity,
while coarse-grained sparsity has a predetermined pattern after prefilling and can sparsify the Gate
MLP. Even though both are at 50% activation sparsity, the coarse-grained sparsity method effectively
achieves higher parameter savings than fine-grained sparsity in practice. Here we evaluate the sparse
techniques using 5-shot CoT on the GSM8K dataset (Cobbe et al., 2021). We found that across all
the models we evaluated, both sparsity methods lead to significant accuracy degradation. We include
HumanEval (Chen et al., 2021), a coding task that requires complex reasoning and planning ability.
We found that both sparsity methods exhibit similar performance degradation when it comes to coding.
Shown in Figure 3, two tasks see sparsity significantly drop performance after 50% activation sparsity."
OBSERVATIONS,0.175,"For knowledge recall and world knowledge understanding, we specifically test on MMLU-Flan-CoT
(Chung et al., 2022) the CoT text generation version of the MMLU dataset (Hendrycks et al.,
2021). Table 1 shows the results. Stronger models like Llama-3-8B-Instruct suffer from significant"
OBSERVATIONS,0.17777777777777778,"degradation too. Furthermore, we found that given the similar parameter size, the more well-trained
the models are, the higher its degradation from the contextual sparsity, more details in Appendix B.1."
OBSERVATIONS,0.18055555555555555,"Question: Marie ordered one chicken meal that costs $12, 5 
packs of milk that costs $3 each, 4 apples that cost $1.50 
each, and some boxes of pizza. Marie paid a total of $50. 
How many boxes of pizza did Marie order if each box costs 
$8.50?"
OBSERVATIONS,0.18333333333333332,"Answer: The total cost of the chicken meal, milk, and 
apples is 12 + 5 * 3 + 4 * 1.5 = <<12+5*3+4*1.5=27>>27 
dollars (33 dollars) ……
Miscalculation"
OBSERVATIONS,0.18611111111111112,"Question: A merchant wants to make a choice of 
purchase between 2 purchase plans: jewelry worth $5,000 
or electronic gadgets worth $8,000. His financial advisor 
speculates that the jewelry market will go up 2.5% while 
the electronic gadgets market will rise 1.2% within the 
same month. If the merchant is looking to maximize profit 
at the end of this month by making a choice, how much 
profit would this be?"
OBSERVATIONS,0.18888888888888888,Miscalculation
OBSERVATIONS,0.19166666666666668,"Answer: The jewelry will increase in value by 2.5% which 
is 5,000 * 0.025 = $1250 ($125) ……"
OBSERVATIONS,0.19444444444444445,"Question: Janet’s ducks lay 16 eggs per day. She eats three 
for breakfast every morning and bakes muffins for her 
friends every day with four. She sells the remainder at the 
farmers' market daily for $2 per fresh duck egg. How much 
in dollars does she make every day at the farmers' market?"
OBSERVATIONS,0.19722222222222222,"Answer (Sparse): 
Janet eats 3 + 4 = <<3+4=7>>7 eggs per day.
She lays 16 - 7 = <<16-7=9>>9 eggs per day.
She sells 9 - 7 = <<9-7=2>>2 eggs per day.
She makes 2 * 2 = <<2*2=4>>4 dollars per day.
#### 4"
OBSERVATIONS,0.2,Different Reasoning Path
OBSERVATIONS,0.20277777777777778,"Answer (Full): 
Janet eats 3 eggs and bakes 4 eggs, so she has 16 - 3 - 4 = 
<<16-3-4=9>>9 eggs left.
She sells 9 eggs for $2 each, so she makes 9 * 2 = 
<<9*2=18>>18 dollars.
#### 18"
OBSERVATIONS,0.20555555555555555,"Question: Raymond and Samantha are cousins. Raymond 
was born 6 years before Samantha. Raymond had a son at 
the age of 23. If Samantha is now 31, how many years ago 
was Raymond's son born?"
OBSERVATIONS,0.20833333333333334,"Answer: Samantha is 31, so she is 31 - 6 = 25 years old. 
Raymond is 25 - 23 = 2 years older than Samantha, so he is 
25 years old ......
Unreasonable Statement"
OBSERVATIONS,0.2111111111111111,"Question: A candle melts by 2 centimeters every hour 
that it burns. How many centimeters shorter will a candle 
be after burning from 1:00 PM to 5:00 PM?"
OBSERVATIONS,0.21388888888888888,Unreasonable Statement
OBSERVATIONS,0.21666666666666667,"Answer: The candle will be 4 centimeters shorter after 
5:00 PM because it will be 4 hours x 2 centimeters = 
<<4*2=8>>8 centimeters shorter.
#### 4
Remarks: There is conflicting statement in reasoning, 
leading to the wrong end result."
OBSERVATIONS,0.21944444444444444,"Figure 4: Examples of contextual sparse model making the identified three different types of mistakes.
Most mistakes occur because the model makes calculation mistakes or has a wrong reasoning step
compared to the full model. We also observe that there are rare cases where the model makes insensible
statements in the middle that make the end result wrong."
OBSERVATIONS,0.2222222222222222,"3.3
A Closer Look on GSM8K Quality Degradation"
OBSERVATIONS,0.225,"To study the inability of the sparse model in deduction, we conduct a case study on the sequence-level
coarse-grained sparsity methods Dong et al. (2024) with the Llama-3-8B-Instruct model. We visually
inspect extensive cases where the sparse model and dense differ in answers. Generally, the sparse
model always produces highly similar answers to the dense model: the similar approach or logic flow
when approaching the same problem and even the same number of sentences before the first mistake
occurs or in success cases. However, the key differences are usually caused by the following three
categories of small token-level mistakes: (1) frequent miscalculation in the intermediate steps, (2)
wrong reasoning in intermediate steps, and (3) insensible and random statements. For each of the
above-summarized cases, we find failure question-answer pairs provided in Figure 4. These mistakes
happen in the middle of arguments and propagate to the wrong end result."
OBSERVATIONS,0.22777777777777777,"Similar observations can also be found for fine-grained sparse methods with different model types.
Interestingly, we find that even with these mistakes, the sparse model can still fully generate coherent
tokens and make further reasoning assuming their prior steps are correct."
OBSERVATIONS,0.23055555555555557,"We hypothesize that the gap between the full model and these sparse counterparts is at these key
tokens. The following simple experiment is conducted to further verify our hypothesis. We run
the coarse-grained sparse model and the full model with the same input prompt and for every token
the sparse model generates, the full model is used to check the likelihood of these decoded tokens,
mainly removing tokens with low likelihood. By varying the likelihood threshold, we can control
the frequency of the correction. The experiments are conducted for both Llama-3-8B-Instruct and
Llama-2-7B-Chat Touvron et al. (2023) models with coarse-grained sparsity. The results are shown in
Figure 1(c). In both cases, we found that a very small amount of correction would drastically improve
the sparse model performance, showing a steep gradient when the percentage of corrected tokens is
small. With merely 10% of tokens needing to be corrected, the sparse model can completely match
the full model’s performance. The experiment verifies our hypothesis that by correcting the small
portion of key tokens, the sparse model can meet the large model’s performance."
METHODS,0.23333333333333334,"4
Methods"
METHODS,0.2361111111111111,"Though we find a minor portion of tokens needed to be corrected for the contextual sparsity model
to fully recover performance, the challenge remains: how to locate these mistaken tokens with the
minimal number of parallel verification rounds of the full model? In this section, we show that the
sparse model provides signals that cannot be trusted 4.1. Then, we describe in detail the various
correction techniques in 4.1. Because of the space limit, we put how to boost the sparse generation
with hardware-efficient tree building B.2."
METHODS,0.2388888888888889,"(b) 
(c) 
(a)"
METHODS,0.24166666666666667,"Figure 5: In (a), we present an example that illustrates why the signals from the sparse model are
unreliable. It is a figure plotting entropy versus generated tokens. At the tokens where the sparse made
the mistake (red), the entropy isn’t in large spikes which signifies chaos and low confidence, rather it is
even quite low, compared to nearby entropy spikes. In (b) and (c), we view Sirius as a compression
method by itself. We compare Sirius with contextual sparse methods and show that given the same
parameter used, Sirius performs better than Contextual Sparse Methods on GSM8K."
METHODS,0.24444444444444444,"4.1
Sparse Model’s Self-Awareness Cannot Be Trusted"
METHODS,0.24722222222222223,"Intuitively, rather than fixing the nsparse number, letting the system decide when to call the LLM
for evaluation would then give more flexible nsparse. Nevertheless, we argue that the sparse model’s
output probability distribution cannot be used as a metric for accuracy decisions. We empirically
experiment with various methods to utilize the information contained in the sparse model’s output
distribution. However, varying the threshold leads to nsparse being too short when the threshold is
strict or failing to correct when the threshold is lenient. We then discovered that the sparse model
has very limited self-awareness of its own mistakes. To make the observation concrete, we present
a small example in Figure 6 a piece of text where the sparse model makes a mistake while the full
model succeeds. The red bars signify the error location. The token entropy is neither high nor at zero,
making it impossible to effectively use a threshold to control the number nsparse."
HOW TO CORRECT THE SPARSE OUTPUT TOKENS,0.25,"4.2
How to Correct the Sparse Output Tokens"
HOW TO CORRECT THE SPARSE OUTPUT TOKENS,0.25277777777777777,"The full overview of SIRIUS is presented in Figure 2 and Algorithm 1. The full model is called once
every kernel size. The KV cache is shared between the sparse and the full model. The KV cache is
mostly populated by the sparse model, which is called for every token. During correction, the full
model takes in the last kernel size of tokens and generates its KVs for the past kernel size tokens
in parallel, these KVs are directly written to their corresponding positions in the shared KV Cache.
Empirically, we found that full model’s KV helps the sparse model’s output. When LLM is called to
evaluate the sparse model’s output, it uses its own predicted likelihood to determine whether to accept
or reject the sparse model’s past output. The decision is based on comparing the likelihood against
a preset threshold. Detailed ablation for threshold is in B.3. Besides the above-mentioned techniques,
we also found that the ""second/third"" choices of the sparse models’ rejected token position offer >
80% coverage of the LLM accepted tokens. The observation motivates us to build a hardware-friendly
tree on the sparse model generating side that doesn’t sacrifice the performance while significantly
boosting the nAAL or efficiency. Due to the space limit, a great amount of details is in Appendix B.2."
EXPERIMENTS,0.25555555555555554,"5
Experiments"
EXPERIMENTS,0.25833333333333336,"In this section, we empirically evaluate SIRIUS to correct CS models on various generation tasks in
complex reasoning. We show that SIRIUS is consistent in various tasks, effective in helping CS models
recover their performance, and efficient in correction with low additional overhead."
EXPERIMENTS,0.2611111111111111,"• In 5.1, we evaluated SIRIUS on six models with 8 different datasets. SIRIUS is consistently effective
and efficient. Specifically, on GSM8K, SIRIUS corrects FSparse Llama-3-8B-Instruct from 58%
accuracy to 72% with only 4% increase in parameter density and corrects CSparse model from 38%
to 70% with 5% density.
• In 5.2, we presents more details on our system implementation for SIRIUS. We show that SIRIUS
delivers its theoretical efficient promise, achieving roughly 20% reduction in latency compared to
full on-chip on various hardware. SIRIUS further achieves 35% speedup to full in offloading settings."
EXPERIMENTS,0.2638888888888889,Algorithm 1 Sirius
EXPERIMENTS,0.26666666666666666,"Require: Prompt [x1,...,xt], full model MF , and sparse model MS sharing weights and sharing KV
Cache C, cache_pos is the location where new k and v are written to C, kernel size n
Require: forward function FORWARD, threshold r, which is a value used by MF to judge whether
the token occurs likely enough
Require: StoppingCriteriaMet() downstream task-specific, returns a boolean"
EXPERIMENTS,0.26944444444444443,"1: while not StoppingCriteriaMet() do
2:
i←0
3:
kernel←empty
4:
cache_pos←0
5:
while i<n do
6:
set
ˆ
pt+i ←FORWARD(MC, C, [x1,...,xt], [xt+1, ..., xt+i−1], cache_pos)
7:
▷Running sparse model
8:
cache_pos ←cache_pos+1
9:
sample
ˆ
xt+i ∼ˆ
pt+i
10:
kernel←cat(kernel, ˆ
xt+i)
11:
i←i+1
▷Before exiting, kernel [xt,...,xt+n]
12:
end while
13:
cache_pos ←cache_pos subtracts n
▷Enables Full to directly rewrites KV Cache
14:
set [qt,...,qt+i]←FORWARD(MC,CS, cache_pos, kernel)
15:
for j from 0, n do
16:
if qt+j < r then
▷Full rejects
17:
break
▷j stores the first token position being rejected
18:
end if
19:
end for
20:
cache_pos ←j+1
▷Rollback
21:
kernel←empty
22:
sample xt+j+1 ∼pt+j
▷Interleaving Key Token
23: end while"
EXPERIMENTS,0.2722222222222222,"• We also present ablation with rich details on how each component of SIRIUS contributes to its
performance and how threshold is used to trade off efficiency and performance. Due to space limit,
we place it in Appendix B.3."
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.275,"5.1
Sirius Significantly Recovers CS Degradation with Low Cost"
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.2777777777777778,"Models and Datasets - To comprehensively evaluate SIRIUS performance, we deploy six mainstream
LLMs with sizes ranging from 7B to 13B: Llama-2-7B, 13B, and Llama-3-8B with their instruction
finetuned counterparts, all from Llama family. Following Wei et al. (2022) in LLM reasoning, we also
tested CS models on two popular types of reasoning generation tasks: arithmetic and commonsense
reasoning. On the Arithmetic side, besides GSM8K, we also evaluate CS models on AQuA-RAT.
On the Common Sense side, we use CSQA Saha et al. (2018), StrategyQAGeva et al. (2021), Date,
and Sports, last two from Big Bench Suite bench authors (2023). Most of these tasks are originally
classification tasks. Following the instruction in Wei et al. (2022), we manually compose COT prompts
to transform these into logic argument generation tasks. Besides, CS models do not perform well in
coding, which requires forming logical arguments and planning. We select HumanEval Chen et al.
(2021) and MBPP+ Liu et al. (2023a) to evaluate SIRIUS."
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.28055555555555556,"For arithmetic reasoning and coding, we use 50% neuron sparsity for both CSparse and FSparse.
FSparse relies on the gate layer to be dense, leading to higher global density than CSparse. Since
commonsense reasoning tasks are generally less logically challenging comparatively, we lowered
the neuron sparsity level to 40%."
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.2833333333333333,"Main Results - Due to space limits, we only select the best treewidth of SIRIUS for GSM8K, CSQA,
and HumanEval for the main results in Table 2. Extensive studies on the rest 5 datasets with different
treewidth are presented in the Appendix C. From Table 2, we can see that SIRIUS is consistently
effective and efficient across all different classes of tasks. Specifically for Llama-3-8B-Instruct, besides
GSM8K, SIRIUS corrects FSparse and CSparse, on CSQA, from 61% and 64% accuracy to 70% with
cost only 3% sparsity for FSparse and 7% for CSparse respectively. On HumanEval, SIRIUS corrects
FSparse from 45% to 61% with 4% sparsity overhead even surpassing the full model’s performance,"
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.2861111111111111,"Table 2: We show SIRIUS effectiveness and efficiency in the following table. We select GSM8K for
Arithmetic Reasoning, CSQA for Commonsense Reasoning, and HumanEval for code generation.
Under the ""SIRIUS Perf. "" column, A(B) is shown. A denotes the accuracy after SIRIUS correction
in the dataset evaluated, while (B) represents the optimal treewidth selected under the current model
dataset settings. Under the column of ""AAL"", X/Y is shown, where X is the AAL, while Y is the period. GSM8K"
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.28888888888888886,"Model
Full Perf.
CSparse Perf.
CSparse
SIRIUS Perf.
AAL
Effective
Density
Density
Llama-3-8B-Instruct
0.7536
0.3844
0.65
0.7051 (8)
15.22/16
0.706
Llama-3-8B
0.4966
0.2085
0.65
0.4177 (8)
15.29/16
0.703
Llama-2-7B-Chat
0.2403
0.1334
0.69
0.2244 (8)
15.00/16
0.757
Llama-2-7B
0.1357
0.0758
0.69
0.1183 (6)
15.87/16
0.715
Llama-2-13B-Chat
0.3548
0.2714
0.68
0.3381 (4)
15.34/16
0.730
Llama-2-13B
0.2282
0.1759
0.68
0.2418 (1)
15.34/16
0.730"
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.2916666666666667,"Model
Full Perf.
FSparse Perf.
FSparse
SIRIUS Perf.
AAL
Effective
Density
Density
Llama-3-8B-Instruct
0.7536
0.5868
0.76
0.7278 (4)
15.37/16
0.807
Llama-3-8B
0.4966
0.3199
0.76
0.4579 (2)
15.03/16
0.825
Llama-2-7B-Chat
0.2403
0.1971
0.79
0.2388 (6)
15.69/16
0.819
Llama-2-7B
0.1357
0.1137
0.79
0.1410 (4)
15.91/16
0.807
Llama-2-13B-Chat
0.3548
0.3222
0.78
0.3533 (1)
15.08/16
0.842
Llama-2-13B
0.2282
0.2191
0.78
0.2372 (4)
15.92/16
0.797
CSQA"
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.29444444444444445,"Model
Full Perf.
CSparse Perf.
CSparse
SIRIUS Perf.
AAL
Effective
Density
Density
Llama-3-8B-Instruct
0.7073
0.6470
0.58
0.7076 (8)
14.76/16
0.657
Llama-3-8B
0.6437
0.5585
0.58
0.6429 (8)
15.43/16
0.628
Llama-2-7B-Chat
0.6248
0.5200
0.62
0.6175 (8)
15.07/16
0.683
Llama-2-7B
0.4742
0.4414
0.62
0.4742 (8)
15.80/16
0.652
Llama-2-13B-Chat
0.6879
0.5536
0.61
0.6691 (4)
11.43/12
0.674
Llama-2-13B
0.6109
0.5601
0.61
0.6060 (4)
15.72/16
0.645"
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.2972222222222222,"Model
Full Perf.
FSparse Perf.
FSparse
SIRIUS Perf.
AAL
Effective
Density
Density
Llama-3-8B-Instruct
0.7073
0.6158
0.72
0.7043 (8)
15.66/16
0.753
Llama-3-8B
0.6437
0.533
0.72
0.6388 (1)
15.00/16
0.786
Llama-2-7B-Chat
0.6248
0.6167
0.75
0.6380 (4)
15.09/16
0.811
Llama-2-7B
0.4742
0.4717
0.75
0.5012 (6)
15.89/16
0.771
Llama-2-13B-Chat
0.6879
0.533
0.74
0.6691 (4)
14.30/16
0.846
Llama-2-13B
0.6109
0.5700
0.74
0.5864 (4)
15.72/16
0.770
HumanEval"
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.3,"Model
Full Perf.
CSparse Perf.
CSparse
SIRIUS Perf.
AAL
Effective
Density
Density
Llama-3-8B-Instruct
0.561
0.207
0.65
0.524 (8)
14.67/16
0.733
Llama-3-8B
0.262
0.067
0.65
0.243 (8)
15.10/16
0.691
Llama-2-7B-Chat
0.140
0.067
0.69
0.159 (8)
10.88/12
0.789
Llama-2-7B
0.116
0.079
0.69
0.128 (8)
14.84/16
0.765
Llama-2-13B-Chat
0.189
0.122
0.68
0.171 (8)
11.12/12
0.762
Llama-2-13B
0.262
0.067
0.68
0.244 (8)
15.10/16
0.741"
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.30277777777777776,"Model
Full Perf.
FSparse Perf.
FSparse
SIRIUS Perf.
AAL
Effective
Density
Density
Llama-3-8B-Instruct
0.561
0.457
0.76
0.616 (6)
15.42/16
0.804
Llama-3-8B
0.262
0.189
0.76
0.298 (6)
15.54/16
0.797
Llama-2-7B-Chat
0.140
0.134
0.79
0.165 (6)
15.27/16
0.841
Llama-2-7B
0.116
0.116
0.79
0.165 (6)
15.86/16
0.810
Llama-2-13B-Chat
0.189
0.146
0.78
0.183 (6)
15.34/16
0.827
Llama-2-13B
0.246
0.233
0.78
0.259 (4)
15.85/16
0.801"
SIRIUS SIGNIFICANTLY RECOVERS CS DEGRADATION WITH LOW COST,0.3055555555555556,"and from 20% to 52% with 8% sparsity as cost. Besides, Llama-3-8B-Instruct, SIRIUS corrects all 6
models with additional sparsity overhead smaller than 10% across these three datasets, further showing
its strong efficiency. Besides results in Table 2, in Appendix C, we show that SIRIUS consistently
shows great effectiveness with high efficiency across the rest of the 5 datasets."
WALLCLOCK SPEEDUP,0.30833333333333335,"5.2
Wallclock Speedup"
WALLCLOCK SPEEDUP,0.3111111111111111,"Table 3: Performance and Speedup Ratios on GSM8K-COT with Different Hardware Configurations.
Settings
ACC
A40
Ratio
L40
Ratio
A100
Ratio
H100
Ratio"
WALLCLOCK SPEEDUP,0.3138888888888889,"CSparse
0.3601
20.7 ms
0.66
15.6 ms
0.67
9.6 ms
0.72
6.6
0.76
Sirius
0.7127
24.1 ms
0.78
18.2 ms
0.78
11.1 ms
0.83
7.7 ms
0.88
Full
0.7612
30.9 ms
1.0
23.2 ms
1.0
13.3 ms
1.0
8.6 ms
1.0"
WALLCLOCK SPEEDUP,0.31666666666666665,"Here we show that Sirius delivers its promised efficiency claim for on-chip and offloading settings.
Because the fine-grained sparsity Lee et al. (2024) relies on a custom CUDA kernel to achieve the
target generation speedup not open-sourced, we focus on coarse-grained sparsity on GSM-8K COT,
and the input sequence length with average prefill length 900."
WALLCLOCK SPEEDUP,0.3194444444444444,Table 4: Llama-3-70B-Instruct with Offloading.
WALLCLOCK SPEEDUP,0.32222222222222224,"Settings
Sparse
Sirius
Full"
WALLCLOCK SPEEDUP,0.325,"Performance
0.7407
0.8719
0.9014
Latency (s)
3.57 s
3.68 s
5.72 s
Ratio to Full
0.6241
0.6434
1.0"
WALLCLOCK SPEEDUP,0.3277777777777778,"Firstly, we consider the on-chip setting run-
ning Llama-3-8B-Instruct on a single GPU. The
sparse model (APU 0.65) achieves 36.01% ac-
curacy on GSM8K-COT, while the full model
achieves 76.12% accuracy on GSM8K-COT.
With kernel size 10, SIRIUS achieves 0.74 APU
with accuracy 71.27% accuracy. We use torch
compile to optimize the inference latency and
limit the overhead other than running model in-
ference. The average latency generated per token is used to compute latency. Results are shown in Table
3. On average, SIRIUS delivers the promised latency reduction from APU calculations. The speedup
ratio on A40 and L40 closely aligns with the theoretical APU reported. On the other hand, A100 and
H100 compute MLP more efficiently than it compute attention, making the latency ratio between
computing MLP and attention not perfectly aligned with their ratio in parameter size. Therefore, we
see that even the sparse model baseline has slightly higher latency as expected. We increase the kernel
size from 10 to 16 for these two devices, where accuracy reaches 0.7089 and the AAL reaches 13.67.
For A100 and H100, building a hardware-efficient tree is nearly free of cost and highly effective. For
numbers in Table 3, we use the width 4 tree that boosts the AAL to 15.01 out of 16. More details are in
Appendix B.2."
WALLCLOCK SPEEDUP,0.33055555555555555,"Secondly, we consider the offloading setting which is the only way for resource-limited users to run
70B models by loading only the weights in use to GPU memory, while the others are offloaded to the
CPU. Results are shown in Table 4. We use a single L40 48GB with a PCIe bus bandwidth of 25 GB/s
to run Llama-3-70B-Instruct with batch size 1. Llama-3-70B-Instruct has roughly 80% of parameters
to be MLP, which gives the theoretical APU for Griffin to be 0.6. Sparse + Sirius gives 0.649 APU,
which is roughly what our system achieved."
CONCLUSION,0.3333333333333333,"6
Conclusion"
CONCLUSION,0.33611111111111114,"We observe that contextual sparse methods significantly degrade reasoning and deduction tasks. SIRIUS,
an efficient correction mechanism, enables accurate LLM inference with contextual sparsity. With
roughly 11% to 18% sparsity increase, SIRIUS improves fine-grained and coarse-grained sparsity
significantly in their performance while maintaining their efficiency gain."
CONCLUSION,0.3388888888888889,"Further, SIRIUS is still relying on rollback to correct the tokens that are deemed unlikely, which
is inefficient. On the other hand, making the weak-strong model synergy systems that match the
performance of the strong while keeping the efficiency of the weak, without strictly matching the strong
models’ output distribution remains an interesting and unsolved problem. We leave these topics to
future works."
ACKNOWLEDGEMENT,0.3416666666666667,"7
Acknowledgement"
ACKNOWLEDGEMENT,0.34444444444444444,"We would like to thank Feng Liang and Yunong Liu for their helpful feedback during the exploration
and writing. We also want to give a special thanks to Hanshi Sun for providing insights and suggestions
for efficient implementation and speedup."
REFERENCES,0.3472222222222222,References
REFERENCES,0.35,"bench authors, B. (2023). Beyond the imitation game: Quantifying and extrapolating the capabilities of language
models. Transactions on Machine Learning Research."
REFERENCES,0.3527777777777778,"Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. (2023). Accelerating large language
model decoding with speculative sampling."
REFERENCES,0.35555555555555557,"Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,
Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,
S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings,
D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N.,
Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,
V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B.,
Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. (2021). Evaluating large language models trained
on code."
REFERENCES,0.35833333333333334,"Chuang, Y.-S., Xie, Y., Luo, H., Kim, Y., Glass, J., and He, P. (2024). Dola: Decoding by contrasting layers
improves factuality in large language models."
REFERENCES,0.3611111111111111,"Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S.,
Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K.,
Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J.,
Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. (2022). Scaling instruction-finetuned language models."
REFERENCES,0.3638888888888889,"Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,
R., Hesse, C., and Schulman, J. (2021). Training verifiers to solve math word problems."
REFERENCES,0.36666666666666664,"Dong, H., Chen, B., and Chi, Y. (2024). Prompt-prompted mixture of experts for efficient llm generation."
REFERENCES,0.36944444444444446,"Frantar, E. and Alistarh, D. (2023). Sparsegpt: Massive language models can be accurately pruned in one-shot."
REFERENCES,0.37222222222222223,"Freitag, M. and Al-Onaizan, Y. (2017). Beam search strategies for neural machine translation. arXiv preprint
arXiv:1702.01806."
REFERENCES,0.375,"Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and Berant, J. (2021). Did Aristotle Use a Laptop? A
Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for
Computational Linguistics (TACL)."
REFERENCES,0.37777777777777777,"Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2021). Measuring
massive multitask language understanding."
REFERENCES,0.38055555555555554,"Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and
Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361."
REFERENCES,0.38333333333333336,"Kim, S., Mangalam, K., Moon, S., Malik, J., Mahoney, M. W., Gholami, A., and Keutzer, K. (2023). Speculative
decoding with big little decoder."
REFERENCES,0.3861111111111111,"LeCun, Y., Denker, J., and Solla, S. (1989). Optimal brain damage. Advances in neural information processing
systems, 2."
REFERENCES,0.3888888888888889,"Lee, J.-Y., Lee, D., Zhang, G., Tiwari, M., and Mirhoseini, A. (2024). Cats: Contextually-aware thresholding for
sparsity in large language models."
REFERENCES,0.39166666666666666,"Leviathan, Y., Kalman, M., and Matias, Y. (2023). Fast inference from transformers via speculative decoding."
REFERENCES,0.39444444444444443,"Li, Z., You, C., Bhojanapalli, S., Li, D., Rawat, A. S., Reddi, S. J., Ye, K., Chern, F., Yu, F., Guo, R., and Kumar, S.
(2022). The lazy neuron phenomenon: On emergence of activation sparsity in transformers."
REFERENCES,0.3972222222222222,"Lin, S., Hilton, J., and Evans, O. (2022). Truthfulqa: Measuring how models mimic human falsehoods."
REFERENCES,0.4,"Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. (2017). Program induction by rationale generation: Learning
to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146."
REFERENCES,0.4027777777777778,"Liu, J., Xia, C. S., Wang, Y., and Zhang, L. (2023a). Is your code generated by chatGPT really correct? rigorous
evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information
Processing Systems."
REFERENCES,0.40555555555555556,"Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen, B.
(2023b). Deja vu: Contextual sparsity for efficient llms at inference time."
REFERENCES,0.4083333333333333,"Ma, X., Fang, G., and Wang, X. (2023). Llm-pruner: On the structural pruning of large language models."
REFERENCES,0.4111111111111111,"OpenAI, Achiam, J., et al. (2024). Gpt-4 technical report."
REFERENCES,0.41388888888888886,"Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020).
Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning
research, 21(140):1–67."
REFERENCES,0.4166666666666667,"Reddy, S., Chen, D., and Manning, C. D. (2019). Coqa: A conversational question answering challenge."
REFERENCES,0.41944444444444445,"Saha, A., Pahuja, V., Khapra, M. M., Sankaranarayanan, K., and Chandar, S. (2018). Complex sequential question
answering: Towards learning to converse over linked question answer pairs with a knowledge graph."
REFERENCES,0.4222222222222222,"See, A., Liu, P. J., and Manning, C. D. (2017). Get to the point: Summarization with pointer-generator networks."
REFERENCES,0.425,"Sharma, P., Ash, J. T., and Misra, D. (2023). The truth is in there: Improving reasoning in language models with
layer-selective rank reduction."
REFERENCES,0.42777777777777776,"Song, Y., Mi, Z., Xie, H., and Chen, H. (2023). Powerinfer: Fast large language model serving with a consumer-
grade gpu."
REFERENCES,0.4305555555555556,"Sun, H., Chen, Z., Yang, X., Tian, Y., and Chen, B. (2024a). Triforce: Lossless acceleration of long sequence
generation with hierarchical speculative decoding."
REFERENCES,0.43333333333333335,"Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. (2024b). A simple and effective pruning approach for large language
models."
REFERENCES,0.4361111111111111,"Team, G., Anil, R., et al. (2024). Gemini: A family of highly capable multimodal models."
REFERENCES,0.4388888888888889,"Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society
Series B: Statistical Methodology, 58(1):267–288."
REFERENCES,0.44166666666666665,"Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu,
W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M.,
Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich,
D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J.,
Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R.,
Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
A., Stojnic, R., Edunov, S., and Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models."
REFERENCES,0.4444444444444444,"Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022). Chain-of-thought
prompting elicits reasoning in large language models. Advances in neural information processing systems,
35:24824–24837."
REFERENCES,0.44722222222222224,"Xia, M., Gao, T., Zeng, Z., and Chen, D. (2024). Sheared llama: Accelerating language model pre-training via
structured pruning."
REFERENCES,0.45,"Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov,
T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. (2022).
Opt: Open pre-trained transformer language models."
REFERENCES,0.4527777777777778,Appendix
REFERENCES,0.45555555555555555,Appendix Table of Contents
REFERENCES,0.4583333333333333,A. Additional Background
EXTENDED RELATED WORKS,0.46111111111111114,"1. Extended Related Works
2. Why Not Using Speculative Decoding to Correct the Sparse Model?"
EXTENDED RELATED WORKS,0.4638888888888889,B. Supplemental Experiments
EXTENDED RELATED WORKS,0.4666666666666667,"1. Given Similar Parameters, More Well-trained Parameters Suffers More
2. Hardware-Friendly Tree Building Process
3. Ablation: Various Aspects of SIRIUS is Tested and Challenged
4. Large Model Experiments
5. Variable Sequence Length with Batch Size One
6. Error Occurs at Which Position inside a Chunk
7. Miscellaneous Results
8. Llama-2 and Llama-3 Models on GSM8K-COT"
EXTENDED RELATED WORKS,0.46944444444444444,C. Additional Results on Reasoning
ARITHMETIC REASONING,0.4722222222222222,"1. Arithmetic Reasoning
2. CommonSense Reasoning
3. Code"
ARITHMETIC REASONING,0.475,"A
Additional Background"
ARITHMETIC REASONING,0.4777777777777778,"A.1
Extended Related Works"
ARITHMETIC REASONING,0.48055555555555557,"Pruning in LLM Sparsity in neural networks has been widely studied. In the context of LLM, sparsity
is studied under two branches - unstructured and structured. On the unstructured sparsity side, Frantar
and Alistarh (2023) (SparseGPT) is a ground-breaking work that formulates pruning as a solving a
series of sparse regression problems and proposes a fine solver for the problem. Sun et al. (2024b)
(Wanda) introduces input activations into the pruning decision and achieves strong results in inducing
LLM sparsity. On the structured side, LLMPruner Ma et al. (2023) and Sheared Llama Xia et al.
(2024) each proposes different meticulous pruning algorithms and restoring weights through either
parameter-efficient finetuning or efficient full weights training."
ARITHMETIC REASONING,0.48333333333333334,"Contextual Sparsity Many recent works on LLM sparsity notice that the sparse pattern is highly
related to the input or context. Deja Vu Liu et al. (2023b) revealed that for OPT models Zhang et al.
(2022) the contextual sparsity is as high as 85%, meaning that 80% of the parameters can be pruned
that won’t hurt the token decoded quality given the prompt. Deja Vu formulates the problem of neuron
selection as a near-neighbor search problem: finding neurons that are the most similar to the input
activations. PowerInfer Song et al. (2023) extends the contextual sparsity to benefit the heterogeneous
setting. Compared to the rest of the model, MLP layers tend to possess significant contextual sparsity
and can be effectively exploited in a training-free manner. Concurrently, Griffin Dong et al. (2024)
discovers the phenomenon of flocking, where MLP neurons have temporal locality, where given a fixed
prompt, similar neurons tend to get activated throughout the following generation. Flocking is shown
to occur in most activation types and open-source LLMs. Griffin selects the same set of heated neurons
with 50% sparsity throughout the generation of each input prompt, which we refer to as coarse-grained
sparsity. CATS Lee et al. (2024) successfully exploits per-token contextual sparsity in the MLP layers
for inference latency reduction. They resample a new set of neurons per every new input token, which
we categorize it as fine-grained contextual sparsity. Our paper mainly focuses on the training-free MLP
sparsity techniques. Although these recent works show minimal accuracy degradation in classification
and easy text summarization tasks, they both severely degrade in generation quality under tasks that
require high-level reasoning and understanding ability. Our work serves as a low-cost complementary
tool, aiming to push these elegant and promising techniques for mainstream use cases."
ARITHMETIC REASONING,0.4861111111111111,"Also, previous contextual sparsity methods haven’t fully and exhaustively evaluated their benefits and
limitations in downstream generation tasks. To fully study this technique, we extensively go through"
ARITHMETIC REASONING,0.4888888888888889,"open-source LLMs in diverse performance and sizes on diverse generation tasks and datasets to locate
where these sparse models maintain the performance or fail."
ARITHMETIC REASONING,0.49166666666666664,"Speculative Decoding Besides model compression techniques, Speculative decoding Leviathan et al.
(2023), Chen et al. (2023), Kim et al. (2023) is another important LLM inference latency reduction
method. Compared to LLM, small transformer models are much more computationally accessible and
can effectively model short-range tokens. Therefore, smaller models are asked to speculate short-term
future tokens, which the LLM takes in in parallel to trade in FLOPs with memory loading time. During
verification, most speculative decoding methods pursue lossless acceleration, leading to frequent
rollback during rejection. In contrast, Sirius solves a very different problem. Our method aims to
maximally preserve the efficiency of sparse models while boosting its performance. Sparse models,
pruned directly from LLM, are much stronger at modeling a longer range of text than draft models, thus
requiring much less help from the LLM. Our work aims to find the minimum amount of LLM overhead
while boosting its performance to the LLM level. Given the resemblance and relevance of Speculative
Decoding to our method Sirius, we will elaborate more in-depth on their differences and Speculative
Decoding’s inefficiencies when it comes to helping the Sparse method in A.2."
ARITHMETIC REASONING,0.49444444444444446,"A.2
Why Not Using the Speculative Decoding to Correct the Sparse Model?"
ARITHMETIC REASONING,0.49722222222222223,"Figure 6: Speculative Decoding has limitation in
efficiency when correcting sparse models."
ARITHMETIC REASONING,0.5,"When Speculative Decoding is used to correct
sparse using the full model, we will show that the
efficiency of the overall process will be largely
limited. We followed the common practice from
speculative decoding and measured the accep-
tance rate on different datasets C4 Raffel et al.
(2020) and GSM8K Cobbe et al. (2021). Take the
Coarse-grained sparse model as an example. For
Llama-3-8B as the full model, the 50% sparse
(APU 0.65) model will produce an acceptance
rate of 0.71 on C4 and 0.89 on GSM8K. Specu-
lative decoding also use parallel verification in
the period-basis. Naturally, to keep the system
efficiency high, we need to (1) enlarge the period
and (2) increase the average number of tokens ac-
cepted (AAL) given the gamma (period - 1) value.
Take the acceptance rate of 0.89 on GSM8K as an
example, following the formulation in Leviathan
et al. (2023), we can calculate the expected number of accepted tokens for every gamma term in the
Speculative Decoding literature. AAL = 1−α(γ+1)"
ARITHMETIC REASONING,0.5027777777777778,"1−α
. The trend (green) is plotted in Figure 6"
ARITHMETIC REASONING,0.5055555555555555,"We can notice the trend that the average advance length starts to plateau as the gamma becomes larger.
Take the gamma of 16 as an example, the period is then 17. The average advance length is only 7.84.
The APU is (16 * 0.65 + 1)/7.84 = 1.45, which is larger than the full model 1. The blue line in Figure 6
shows the relationship between APU and gamma."
ARITHMETIC REASONING,0.5083333333333333,"Because of the plateauing effect, for an acceptance rate of 0.89, the best gamma is 2 (period = 3). The
optimal APU is 0.86, compared with 0.65 coarse-grained sparse APU. A similar picture can be applied
to Fine-grained sparsity as well. The key reasons for the observation are two-fold: (1) the contextually
sparse models are too big to be the draft model of the speculative decoding system and to have a large
period; (2) Speculative decoding preserves the original model’s performance so that the acceptance
criteria are usually very strict, which is also not suitable for large period and high average advance
length. Following the same spirit, Sun et al. (2024a) also uses a large draft model to do self-speculation,
but for them, the authors select gamma = 1 to achieve the optimal speedup of their system. In contrast,
SIRIUS brings <0.76 APU in this case with period ≥10. Specifically, with a threshold of 0.1, Sirius
can correct Llama-3-8B coarse-grained sparsity from 20.85% to 43.9%, compared to the 49.66% full
model. With a period of 16 tokens (gamma = 15), Sirius on average can accept 13.4 tokens out of a
kernel size of 16 and over 9 tokens out of a kernel size of 10, translating to APU < 0.76, significantly
lower than SD does."
ARITHMETIC REASONING,0.5111111111111111,"B
Supplemental Details in SIRIUS observations, Design, and Experiments"
ARITHMETIC REASONING,0.5138888888888888,Neuron Index
ARITHMETIC REASONING,0.5166666666666667,Neuron Index
ARITHMETIC REASONING,0.5194444444444445,"Tokens
Tokens"
ARITHMETIC REASONING,0.5222222222222223,Prompt Understanding Tasks
ARITHMETIC REASONING,0.525,"Reasoning Tasks 
selected sparse"
ARITHMETIC REASONING,0.5277777777777778,"(a)
(b)"
ARITHMETIC REASONING,0.5305555555555556,"Figure 7: (a) Illustration on why Contextual Sparsity has uneven per-
formance on different tasks. The activation heat map (red) has the
brighter the color the larger in magnitude. On top, we also show the
neuron sparsity selected. The graph points signify that the pattern in the
prompt understanding task is easier to capture. (b) An additional graph
of correcting Csparse Llama-2-7B-Chat. It is similar to the previous
experiment on 8B. Only 10% tokens being corrected results in complete
performance recovery."
ARITHMETIC REASONING,0.5333333333333333,"In this section, we pro-
vide several supplemental
experiments to the picture.
First, we run SIRIUS on
Llama-3-70B. However, be-
cause of computational lim-
its, we cannot run SIRIUS
with the tree on Llama-3-
70B with the scale we did
for other models. Neverthe-
less, we do show that 70B
has roughly the same pattern
aswehaveseenbefore, large
model sparsity also some-
how struggles on reasoning
tasks. Second, we provide
additional proof for the par-
allel verification efficiency
statement. After that, I show
results on where the error is
located in the chunk size of
16 tokens. The error is dis-
tributed almost uniformly. Last but not least, we also apply SIRIUS on datasets that are reasoning. Lastly,
we provide more results on the comparison between models of similar size but have a huge performance
gap. We show that given the similar parameter size, the trend is for a more well-trained, powerful model
to degrade more from contextual sparsity. We present here more illustration on contextual sparsity and
Llama-2-7B-Chat experiments with fixing only a minor portion of tokens is shown in Figure 7."
ARITHMETIC REASONING,0.5361111111111111,"B.1
Given Similar Parameter Size Well-trained Models Suffer More
We observe another interesting phenomenon: given the similar parameter size, the more well-trained
the model is, the more performance degradation contextual sparsity would make on the full models.
Here we present two pairs of results. First, we look at the performance between Llama-3-8B-Instruct
and Llama-2-7B-Chat with Llama-3-70B-Instruct and Llama-2-70B-Chat. All models are evaluated
on GSM8K-COT. We draw these models in CSparse in Figure 8, and the readers can find more results
in Appendix B.8. We can see figures from top to bottom, where even at lower density (more elements
are not selected), Llama-2-7B-Chat and Llama-2-70B-Chat suffer from less performance degradation
(blue) compared to the Llama-3-8B-Instruct and Llama-3-70B-Instruct models. Furthermore, suppose
we focus on Llama-3-70B-Instruct for global density at 60% or lower. In that case, the performance
(coral) is degraded significantly, which is comparable or even lower to Llama-3-8B-Instruct full model
performance at 0.76, Even at 50% density, the 70B model still has more than 40B parameters, much
more expensive than the 8B model. The observation fully manifests the difficulty of using CS in
complex reasoning tasks."
ARITHMETIC REASONING,0.5388888888888889,"B.2
Hardware Friendly Tree Building Process"
ARITHMETIC REASONING,0.5416666666666666,"Table 5: The second and third most likely to-
kens from sparse models offer potential for
boosting efficiency."
ARITHMETIC REASONING,0.5444444444444444,"Sparsity
2nd Hit
3rd Hit
Miss
Coverage%"
ARITHMETIC REASONING,0.5472222222222223,"FSparse
79%
11%
9%
90%
CSparse
65%
17%
16%
82%"
ARITHMETIC REASONING,0.55,"In this section, we first look at the insights behind
whether building the tree can help efficiency, then we
detail the specific steps towards tree pruning."
ARITHMETIC REASONING,0.5527777777777778,"The goal for the Sirius system is to make nAAL to be
as large as possible. Despite the full model sharing
KVs with the sparse model, Sirius still encounters
costly rollbacks because of sparse greedily decoded
tokens being rejected. Interestingly, we look closely
into where the sparse model is likely to make a mistake
on GSM8K and AQuA-RAT-COT Ling et al. (2017) with Sirius on Llama-3-8B-Instruct and a kernel
size of 16. More details are shown in Appendix B.6. The error distributes almost uniformly across all
positions of the kernel size. Also, when the token makes the mistake, besides the greedily decoded"
ARITHMETIC REASONING,0.5555555555555556,"Figure 8: Given the similar model parameters, the more well-trained the model is, the worse the
degradation would be. (Compare the figures vertically between Llama-3 and Llama-2 family models)."
ARITHMETIC REASONING,0.5583333333333333,"tokens, we find that other tokens of lower likelihood offer the potential to boost efficiency. Surprisingly,
we found that out of the cases where the greedily decoded tokens are rejected, the probability that the
second or third most likely tokens from the sparse being accepted by the full model is reasonably high."
TH ROUND,0.5611111111111111,"0th Round 
(token from 
Full Model)"
ST ROUND,0.5638888888888889,1st Round
ST ROUND,0.5666666666666667,"(1) Each drafts k 
children"
ST ROUND,0.5694444444444444,"(2) Computes 
Cumulative likelihood 
for every path"
ST ROUND,0.5722222222222222,"(3) Pick Top-k tokens 
and discard the rest to 
keep k branches"
ND ROUND,0.575,2nd Round Have Need
ND ROUND,0.5777777777777777,[Space] 3
ND ROUND,0.5805555555555556,"#branches each round is fixed at treewidth (k), tree building continues Kim Will Has is needs"
ND ROUND,0.5833333333333334,"Drafting and Filtering Process for 
the nth round"
ND ROUND,0.5861111111111111,Figure 9: Illustration of Tree Building Process.
ND ROUND,0.5888888888888889,"Shown in Table 5, we test on part of the
GSM8K rejected cases. The ""Second
Hit"" is defined as the count of the sec-
ond most likely tokens being accepted
by the full model when the greedily
decoded token is rejected, while the
""Third Hit"" is defined as the count of
the third most likely token being ac-
cepted when the first two are rejected.
Both sparsity method has a high ac-
ceptance rate, or ""Coverage"", from the
second and third most likely tokens
when the most likely token is rejected,
showing huge potential for gains in ef-
ficiency."
ND ROUND,0.5916666666666667,"To capitalize the potential from the second to third tokens, we propose to build a tree during the sparse
generating process (lines 6 to 11 in Algorithm 1. The tree algorithm is similar to Beam Search Freitag
and Al-Onaizan (2017). However, to make sure that the tree building and tree parallel correction
processes can achieve speedup over cases that don’t build trees, we impose strong restrictions on the
tree structure we build. For a fixed kernel size, we limit every step to having a fixed number of leaves,
or treewidth, through tree pruning based on ranking the cumulative log-likelihood of the path. The
resulting tree has a fixed shape for a given kernel size and tree width, but only the interconnection pattern
between steps varies based on the pruning and ranking within each step. The details are illustrated in
Figure 9. During verification, out of the treewidth complete paths, we select the one that reaches the
longest advance length. In practice, we found that for kernel size 16, when the treewidth is increased to"
ND ROUND,0.5944444444444444,"8, the optimal verification tree is around 64. From Section A.2, we see that the parallel verification of
the tree of 64 roughly equals the time the full input 1 token."
ND ROUND,0.5972222222222222,"Therefore, a treewidth of 8 is set as the maximum treewidth when building the tree for kernel size 16
for later. We show that building a tree makes the system significantly more efficient while retaining
correction effects."
ND ROUND,0.6,"Table 6: Performance and Speedup Ratios on GSM8K-COT with tree building, latency measurement
in millisecond."
ND ROUND,0.6027777777777777,"Settings
ACC
A100
Ratio
H100
Ratio"
ND ROUND,0.6055555555555555,"CSparse
0.3601
9.6
0.72
6.6
0.76
Sirius (No Tree)
0.7127
11.8
0.88
8.2
0.95
Sirius (With Tree)
0.7089
11.1
0.83
7.7
0.88
Full
0.7612
13.3
1.0
8.6
1.0"
ND ROUND,0.6083333333333333,"In Table 6, we present more details complementing Table 3 on the use of tree-building for A100 and
H100. We ablate the use of a width 4 tree by not using the tree under kernel size 16. We observe that the
speedup improvements solely from the increase in AAL from 13.67 to 15.01 for kernel size 16."
ND ROUND,0.6111111111111112,"B.3
Ablation: Various Aspects of Sirius Are Tested and Challenged"
ND ROUND,0.6138888888888889,"Table 7: Ablation on Components in Sirius.
CSparse
GSM8K 20%
FSparse
GSM8K 20%"
ND ROUND,0.6166666666666667,"Llama3-8B-Instruct
0.7538/0.7538
Llama3-8B-Instruct
0.7538/0.7538
+ CSparse
0.3674/0.3674
+ FSparse
0.5644/0.5644
+ CSparse + Interleave
0.3826/0.3826
+ FSparse + Interleave
0.6288/0.6288
+ CSparse + KV Rewrite
0.4735/0.4735
+ FSparse + KV Rewrite
0.6629/0.6629
+ CSparse + KV Rewrite
0.4886/0.4886
+ FSparse + KV Rewrite
0.6780/0.6818
+ Interleave
+ Interleave
+ CSparse + Roll back
0.6591/0.6591
+ FSparse + Roll back
0.7273/0.7273
+ Interleave
+ Interleave
+ CSparse + KV Rewrite
0.6667/0.6667
+ FSparse + KV Rewrite
0.7273/0.7311
+ Interleave + Rollback
+ Interleave + Rollback"
ND ROUND,0.6194444444444445,"Probing Components To understand the contribution and the utility of each component of Sirius, we
ablate all components of Sirius in Table 7. We started by only letting the LLM correct the token it is
evaluating (interleaving only). Then, we add on top of it the KV cache correction, and then the rollback.
All these three techniques are effective when applied solely. Rollback seems to be the most effective
technique. Even when applied alone, rollback asserts significant correction to both the CSparse and
FSparse models. Interestingly, KV Cache is also effective alone, bringing a 12% increment for CSparse
and an 11% accuracy increase for FSparse. Relatively, interleaving is the weakest. Surprisingly, adding
both KV rewriting and rollback is only marginally better than rollback alone. Although it is tempting to
think KV Cache rewriting is not useful with rollback, the improvement KV Cache Rewriting brings
is a gain in efficiency. When adding the KV Cache Rewriting on top of Roll Back and interleave it
significantly improves the efficiency of the correction. For CSparse, adding KV rewrite increases AAL
from 12.77 to 13.80."
ND ROUND,0.6222222222222222,"Table 8: Ablation on the threshold for correction (FSparse Llama-3-8B-Instruct).
Threshold
Full
Sparse
0.05
0.1
0.3
0.5
0.7
0.9"
ND ROUND,0.625,"Accuracy
0.7803
0.5884
0.7247
0.7399
0.7399
0.7677
0.7702
0.7652
AAL
N/A
N/A
15.2
14.6
11.6
8.5
6.2
4.2"
ND ROUND,0.6277777777777778,"Likelihood threshold to balance Correction and Efficiency We found that the likelihood threshold
is important for managing the Sirius correction and efficiency tradeoff. We present results in Table 8.
We ablate this setting on a 30% subsampled GSM8K dataset, and only strict accuracy is reported. The"
ND ROUND,0.6305555555555555,"performance is the score, while the efficiency is measured by Average Advance Length (AAL). We
can find that with the increase of threshold, the scores generally improve, while the efficiency metric
decreases."
ND ROUND,0.6333333333333333,"Building Wider Tree We study the effect of increasing the treewidth. In fact, for every number from
SIRIUS in Table 2, we are selecting from a group of results by different treewidth. We present all of
this treewidth and its corresponding accuracy and efficiency numbers in the Appendix C. Importantly,
raising treewidth always improves AAL. Although different choices of treewidth usually give similar
accuracy scores, there is hardly a pattern on which treewidth always gives the best accuracy. The
optimal treewidth can only be found through empirical studies."
ND ROUND,0.6361111111111111,"B.4
Large Model Experiments"
ND ROUND,0.6388888888888888,"To diversify the evaluation of Sirius, we also evaluate Sirius’s Effectiveness on the Llama-3-70B-
Instruct model. MMLU is subsampled 10%, while CNN/DailyMail is subsampled 30%. The following
table contrasts with Llama-3-8B-Instruct. We use strict match/flexible extract accuracy for GSM-
8K-COT, accuracy for MMLU, F1/EM score for CoQA, Rouge-1/2/L score for CNN/DailyMail, and
Rouge-1/2 ACC for TruthfulQA."
ND ROUND,0.6416666666666667,"Table 9: Large model results on miscellaneous datasets.
GSM-8K-COT
MMLU
CoQA
CNN/DailyMail
TruthfulQA
Llama-3-70B-In
0.9014/0.9022
0.7456
0.6567/0.8069
0.1016/0.0206/0.0964
0.5116/0.4247
+ CSparse
0.7407/0.7483
0.7018
0.6497/0.8046
0.1019/0.0208/0.0967
0.4541/0.3807
+ FSparse
0.8726/0.8772
0.7193
0.6497/0.8035
0.1015/0.0206/0.0963
0.4835/0.3905
Llama-3-8B-In
0.7612/0.7672
0.6272
0.6153/0.7825
0.1015/0.0204/0.0963
0.4945/0.3647
+ CSparse
0.3601/0.3647
0.5307
0.6003/0.7735
0.1016/0.0206/0.0964
0.5067/0.3953
+ FSparse
0.6103/0.6202
0.4825
0.5828/0.7577
0.1017/0.0204/0.0965
0.5202/0.3941"
ND ROUND,0.6444444444444445,"B.5
Variable Sequence Length with Batch Size One"
ND ROUND,0.6472222222222223,Table 10: A100 Latency versus Input Sequence Length.
ND ROUND,0.65,"Input Sequence Length
A100 Latency (ms)"
ND ROUND,0.6527777777777778,"1
0.0133
2
0.0135
4
0.0136
8
0.0138
16
0.0140
32
0.0149
64
0.0144
96
0.0171"
ND ROUND,0.6555555555555556,"Here we show the benchmark latency on A100, where the input tensor to Llama-3-8B-Instruct has a
shape of batch size 1 and a different input sequence length. To get the hardware optimal readings, we
use torch compile to compile the whole forward pass of the model. We show that the latency only goes
up insignificantly to 64, but the trend of increment to 96 is a bit steep."
ND ROUND,0.6583333333333333,"B.6
Error Occurs At Which Position inside a Chunk"
ND ROUND,0.6611111111111111,"We look at the distribution of where the error would be inside a kernel of 16 tokens. We run through
Sirius with a kernel size of 16 on the entire GSM-8K and AQuA-RAT-COT dataset. The histogram is
shown in Figure 10. We found that the error occurs in a uniform pattern, where it is hard to see any
particular region where the tokens are likely to occur the most."
ND ROUND,0.6638888888888889,"B.7
Miscellaneous Results"
ND ROUND,0.6666666666666666,"Besides, the results on the complex reasoning tasks, we evaluate Sirius on slightly more diverse tasks in
Table 12."
ND ROUND,0.6694444444444444,"Figure 10: We look at the histogram of the number of errors versus the position among a period of
sixteen tokens on average. We have two different datasets of Arithmetic Reasoning GSM-8K and
AQuA-RAT-COT. We can see that the number of errors is distributed almost evenly for both datasets."
ND ROUND,0.6722222222222223,"GSM-8K
AQuA-RAT-COT"
ND ROUND,0.675,"Position inside 16 
Position inside 16"
ND ROUND,0.6777777777777778,"B.8
Llama-2 and Llama-3 Models on GSM8K-COT"
ND ROUND,0.6805555555555556,Table 11: Detail on Llama-2 and Llama-3 family models with CS.
ND ROUND,0.6833333333333333,"Llama-3-70B-Instruct
Accuracy
Degradation
Llama-3-8B-Instruct
Accuracy
Degradation
Full
0.9205
Full
0.7462
Csparse 60%
0.8144
0.1061
Csparse 50%
0.7652
0.1553
Csparse 50%
0.3636
0.3826
Csparse 40%
0.6023
0.3182
Csparse 40%
0.1856
0.5606
Csparse 30%
0.3144
0.6061
Csparse 30%
0.0644
0.6818
Fsparse 50%
0.8864
0.0341
Fsparse 50%
0.6477
0.0985
Fsparse 40%
0.8485
0.0720
Fsparse 40%
0.4053
0.3409
Fsparse 30%
0.7386
0.1819
Fsparse 30%
0.0265
0.7197
Fsparse 20%
0.2803
0.6402"
ND ROUND,0.6861111111111111,"Llama-2-70B-Chat
Accuracy
Degradation
Llama-2-7B-Chat
Accuracy
Degradation
Full
0.4508
Full
0.1856
Csparse 50%
0.3939
0.0569
Csparse 50%
0.1515
0.0341
Csparse 40%
0.3447
0.1061
Csparse 40%
0.1098
0.0758
Csparse 30%
0.2689
0.1819
Csparse 30%
0.0720
0.1136
Fsparse 50%
0.3864
0.0644
Fsparse 50%
0.1629
0.0227
Fsparse 40%
0.3902
0.0606
Fsparse 40%
0.1364
0.0492
Fsparse 30%
0.2689
0.1819
Fsparse 30%
0.1212
0.0644"
ND ROUND,0.6888888888888889,"Here we present more experiments for the comparison between Llama-2 and Llama-3 family models,
which is first mentioned in Section B.1, where we also include FSparse methods together with the
CSparse method. The results are in Table 11."
ND ROUND,0.6916666666666667,Table 12: Miscellaneous Results: 5 models on different Three Different datasets.
ND ROUND,0.6944444444444444,"Experiment Setting
CoQA
AGIEval (Math)
MMLU-FLAN-COT"
ND ROUND,0.6972222222222222,"Llama-2-7B-Chat
0.5982/0.7580
0.072
0.4925
Llama-2-7B-Chat-FSparse
0.5898/0.7540
0.077
0.4768
Llama-2-7B-Chat-FSparse-SIRIUS
0.5908/0.7540
0.081
0.4670
Llama-2-7B-Chat-CSparse
0.6117/0.7639
0.065
0.4637
Llama-2-7B-Chat-CSparse-SIRIUS
0.6117/0.7664
0.078
0.4794"
ND ROUND,0.7,"Llama-3-8B-Instruct
0.6153/0.7825
0.213
0.6231
Llama-3-8B-Instruct-FSparse
0.5828/0.7577
0.172
0.5304
Llama-3-8B-Instruct-FSparse-SIRIUS
0.5868/0.7591
0.196
0.5709
Llama-3-8B-Instruct-CSparse
0.6003/0.7735
0.154
0.5558
Llama-3-8B-Instruct-CSparse-SIRIUS
0.6005/0.7728
0.178
0.6003"
ND ROUND,0.7027777777777777,"Llama-2-13B-Chat
0.6408/0.7896
0.092
0.5317
Llama-2-13B-Chat-FSparse
0.6320/0.7837
0.087
0.5082
Llama-2-13B-Chat-FSparse-SIRIUS
0.6340/0.7859
0.089
0.5219
Llama-2-13B-Chat-CSparse
0.6350/0.7841
0.088
0.5127
Llama-2-13B-Chat-CSparse-SIRIUS
0.6363/0.7847
0.1
0.5127"
ND ROUND,0.7055555555555556,"Llama-2-7B
0.6388/0.7735
0.101
0.4520
Llama-2-7B-FSparse
0.6352/0.7697
0.09
0.4435
Llama-2-7B-FSparse-SIRIUS
0.6352/0.7697
0.092
0.4415
Llama-2-7B-CSparse
0.6338/0.7700
0.086
0.4213
Llama-2-7B-CSparse-SIRIUS
0.6372/0.7709
0.093
0.4317"
ND ROUND,0.7083333333333334,"Llama-3-8B
0.6727/0.8055
0.163
0.5754
Llama-3-8B-FSparse
0.6625/0.7984
0.152
0.5349
Llama-3-8B-FSparse-SIRIUS
0.6625/0.7984
0.154
0.5532
Llama-3-8B-CSparse
0.6633/0.7977
0.131
0.5049
Llama-3-8B-CSparse-SIRIUS
0.6670/0.7995
0.15
0.5428"
ND ROUND,0.7111111111111111,"C
Additional Results on Reasoning"
ND ROUND,0.7138888888888889,"Due to page restrictions, we only show GSM8K, CSQA, and HumanEval in the paper. Below we
show additional results to the numbers presented in the paper. We present tables of a similar format.
Please notice that the leftmost column writes a number that represents the treewidth in the given
settings. Also, we show the results of SIRIUS on the other five datasets AQuA-RAT-COT (Arithmetic
Reasoning), Sports (Commonsense Reasoning), Date (Commonsense Reasoning), and StrategyQA
(CommonSense Reasoning), and MBPP+ (coding)."
ND ROUND,0.7166666666666667,"C.1
Arithmetic Reasoning"
ND ROUND,0.7194444444444444,"In this section, we present GSM8K and AQuA RAT COT evaluation results with the efficiency metric
AAL. Sirius is shown to be effective on these two reasoning tasks about arithmetic. Below we show the
raw AAL score associated with efficiency for all models and the performance of different treewidths."
ND ROUND,0.7222222222222222,"C.2
CommonSense Reasoning"
ND ROUND,0.725,"We followed the COT paper and evaluated Sirius on CSQA, Sports, StrategyQA, and Dates. Sparse
methods are capable of outputting high-quality output similar to the full model at the 0.5 mark, which is
different than on other datasets. However, we tune the sparsity level to 0.4 (0.6 dense, 0.4 removed), and
it starts to have performance degradation. Sirius can compensate them with relatively high efficiency)"
ND ROUND,0.7277777777777777,"C.3
Code"
ND ROUND,0.7305555555555555,"We also have a coding portion that evaluates Sirius on HumanEval. Sirius performs well similar to
other datasets. Besides, we also have results on MBPP+. The results show SIRIUS effectiveness and
efficiency again."
ND ROUND,0.7333333333333333,"Table 13: SIRIUS Tree on GSM8K.
Experiment Settings
Llama-3-8B-Instruct-FSparse
Llama-3-8B-Instruct-CSparse
treewidth
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.7536/0.7544
N/A
0.7536/0.7544
N/A
Sparse Performance
0.5868/0.5891
N/A
0.3844/0.3867
N/A
1
0.7316/0.7324
14.5903
0.6983/0.7005
13.1903
2
0.7172/0.7172
14.9554
0.7089/0.7096
14.1517
4
0.7278/0.7309
15.3705
0.7119/0.7111
14.8393
6
0.7195/0.7187
15.5979
0.7081/0.7074
15.0682
8
0.7202/0.7218
15.5548
0.7051/0.7058
15.2291
Llama-3-8B-FSparse
Llama-3-8B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.4966/0.5042
N/A
0.4966/0.5042
N/A
Sparse Performance
0.3199/0.3260
N/A
0.2085/0.2168
N/A
1
0.4526/0.4572
14.6946
0.439/0.445
13.361
2
0.4579/0.4640
15.0355
0.4299/0.4367
14.3061
4
0.4579/0.4540
15.0355
0.4223/0.4306
14.9721
6
0.4450/0.4503
15.4834
0.4177/0.4238
15.1435
8
0.4352/0.4428
15.5863
0.4177/0.4238
15.2939
Llama-2-13B-Chat-FSparse
Llama-2-13B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.3548/0.3647
N/A
0.3548/0.3647
N/A
Sparse Performance
0.3222/0.3275
N/A
0.2714/0.2767
N/A
1
0.3533/0.3472
15.085
0.3412/0.3472
14.0153
4
0.3412/0.3374
15.7576
0.3381/0.3412
15.3491
Llama-2-13B-FSparse
Llama-2-13B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.2282/0.2312
N/A
0.2282/0.2312
N/A
Sparse Performance
0.2191/0.2229
N/A
0.1759/0.1797
N/A
1
0.2328/0.2381
15.6759
0.2418/0.2472
15.3415
4
0.2372/0.2403
15.9283
0.2077/0.2100
15.825
Llama-2-7B-Chat-FSparse
Llama-2-7B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.2403/0.2426
N/A
0.2403/0.2426
N/A
Sparse Performance
0.1971/0.1994
N/A
0.1334/0.1372
N/A
1
0.2282/0.2312
14.8172
0.2214/0.2229
12.6888
2
0.2297/0.2305
15.1784
0.2252/0.2359
13.8875
4
0.2305/0.2282
15.5467
0.2183/0.2214
14.6751
6
0.2388/0.2411
15.691
0.2199/0.2206
14.8575
8
0.2312/0.2343
15.735
0.2244/0.2252
15.0017
Llama-2-7B-FSparse
Llama-2-7B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.1357/0.1403
N/A
0.1357/0.1403
N/A
Sparse Performance
0.1137/0.1168
N/A
0.0758/0.0804
N/A
1
0.1183/0.1205
15.6864
0.1152/0.1168
15.1096
2
0.1334/0.1357
15.7893
0.113/0.116
15.5358
4
0.1410/0.1448
15.9161
0.113/0.116
15.8558 (53.341)
6
0.1289/0.1312
15.9662
0.1183/0.1205
15.8715
8
0.1236/0.1259
15.9568
0.1114/0.1145
15.8939"
ND ROUND,0.7361111111111112,"Table 14: SIRIUS on AQuA-RAT-COT.
Experiment Settings
Llama-3-8B-Instruct-FSparse
Llama-3-8B-Instruct-CSparse
treewidth
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.515748
N/A
0.515748
N/A
Sparse Performance
0.42126
N/A
0.271654
N/A
1
0.429134
13.1945
0.468504
10.3322
4
0.488189
14.4722
0.44095
10.6228
6
0.496063
15.115
0.452756
10.8348
8
0.476378
15.3721
0.456693
11.0874
Llama-3-8B-FSparse
Llama-3-8B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.456693
N/A
0.456693
N/A
Sparse Performance
0.287402
N/A
0.228346
N/A
1
0.377953
12.6665
0.377945
12.6665
4
0.385827
14.46
0.397638
10.2826
6
0.366142
15.0671
0.370079
10.6753
8
0.42126
15.0995
0.38189
11.0019
Llama-2-13B-Chat-FSparse
Llama-2-13B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.232283
N/A
0.232283
N/A
Sparse Performance
0.251969
N/A
0.208661
N/A
1
0.275591
15.5163
0.26378
9.17465
4
0.279528
15.3995
0.259843
10.8726
Llama-2-13B-FSparse
Llama-2-13B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.149606
N/A
0.149606
N/A
Sparse Performance
0.165354
N/A
0.149606
N/A
1
0.185039
15.4652
0.161417
11.0874
4
0.220472
15.6346
0.192913
11.8238
Llama-2-7B-Chat-FSparse
Llama-2-7B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.251969
N/A
0.251969
N/A
Sparse Performance
0.283465
N/A
0.220472
N/A
1
0.248031
15.5294
0.251969
12.6424
4
0.259843
15.7254
0.244096
14.5794
Llama-2-7B-FSparse
Llama-2-7B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.15748
N/A
0.15748
N/A
Sparse Performance
0.153543
N/A
0.177165
N/A
1
0.185039
15.423
0.141732
15.4122
4
0.161417
15.651
0.145669
15.3788"
ND ROUND,0.7388888888888889,"Table 15: SIRIUS on CSQA.
Experiment Settings
Llama-3-8B-Instruct-FSparse
Llama-3-8B-Instruct-CSparse
treewidth
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.707341
N/A
0.707341
N/A
Sparse Performance
0.615889
N/A
0.647011
N/A
1
0.699427
12.2108
0.724816
11.0512
4
0.687961
13.2734
0.709255
13.5876
6
0.714169
13.7842
0.720721
13.3097
8
0.710893
14.1173
0.707617
14.76893
Llama-3-8B-FSparse
Llama-3-8B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.643735
N/A
0.643735
N/A
Sparse Performance
0.53317
N/A
0.558559
N/A
1
0.638821
15.0088
0.618346
12.7426
4
0.630631
14.6151
0.63964
14.8704
6
0.625717
14.9905
0.640459
15.1968
8
0.617527
15.2534
0.642916
15.4355
Llama-2-13B-Chat-FSparse
Llama-2-13B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.687961
N/A
0.687961
N/A
Sparse Performance
0.53317
N/A
0.553645
N/A
1
0.657658
13.0868
0.649468
9.2183
4
0.669124
14.309
0.669124
11.438
Llama-2-13B-FSparse
Llama-2-13B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.610975
N/A
0.610975
N/A
Sparse Performance
0.570025
N/A
0.560197
N/A
1
0.578215
15.2554
0.58231
14.7381
4
0.586405
15.7213
0.606061
15.7284
Llama-2-7B-Chat-FSparse
Llama-2-7B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.624898
N/A
0.624898
N/A
Sparse Performance
0.616708
N/A
0.520066
N/A
1
0.632269
14.1015
0.608518
11.4607
4
0.638002
15.0978
0.605242
14.0366
6
0.605242
15.4365
0.611794
14.7197
8
0.621622
15.552
0.617527
15.0799
Llama-2-7B-FSparse
Llama-2-7B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.474201
N/A
0.474201
N/A
Sparse Performance
0.471744
N/A
0.441441
N/A
1
0.488124
15.5119
0.461916
14.703
4
0.494676
15.9141
0.486486
15.5972
6
0.501229
15.8922
0.476658
15.7315
8
0.473382
15.9247
0.474201
15.802"
ND ROUND,0.7416666666666667,"Table 16: SIRIUS on Sports.
Experiment Settings
Llama-3-8B-Instruct-FSparse
Llama-3-8B-Instruct-CSparse
treewidth
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.943299
N/A
0.943299
N/A
Sparse Performance
0.864948
N/A
0.879381
N/A
1
0.937113
12.3652
0.946392
9.95237
4
0.941237
14.5248
0.943299
11.5858
6
0.942268
14.8651
0.943299
14.0954
8
0.939175
14.9832
0.941237
14.7718
Llama-3-8B-FSparse
Llama-3-8B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.898969
N/A
0.898969
N/A
Sparse Performance
0.748454
N/A
0.720619
N/A
1
0.86653
15.5259
0.845361
13.5897
4
0.849485
15.5917
0.847423
15.2325
6
0.863918
15.5256
0.843299
15.4376
8
0.869072
15.6014
0.841237
15.5023
Llama-2-13B-Chat-FSparse
Llama-2-13B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.742268
N/A
0.742268
N/A
Sparse Performance
0.690722
N/A
0.584536
N/A
1
0.710309
13.9767
0.717659
7.72686
4
0.735052
14.9247
0.728953
10.7298
Llama-2-13B-FSparse
Llama-2-13B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.709278
N/A
0.709278
N/A
Sparse Performance
0.635052
N/A
0.558763
N/A
1
0.669072
15.4924
0.639175
14.3603
4
0.657732
15.9845
0.658763
15.48
Llama-2-7B-Chat-FSparse
Llama-2-7B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.731959
N/A
0.731959
N/A
Sparse Performance
0.652677
N/A
0.596907
N/A
1
0.704124
14.3861
0.712371
11.1517
4
0.712371
15.5904
0.71134
13.7394
6
0.709278
15.7475
0.71134
13.9857
8
0.698969
15.9927
0.715464
14.3817
Llama-2-7B-FSparse
Llama-2-7B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.545361
N/A
0.545361
N/A
Sparse Performance
0.536082
N/A
0.528866
N/A
1
0.524742
15.6754
0.536082
14.1031
4
0.547423
15.937
0.538144
15.6263
6
0.545361
15.9807
0.540206
15.7243
8
0.545361
15.9927
0.549485
15.811"
ND ROUND,0.7444444444444445,"Table 17: SIRIUS on Date.
Experiment Settings
Llama-3-8B-Instruct-FSparse
Llama-3-8B-Instruct-CSparse
treewidth
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.657224
N/A
0.657224
N/A
Sparse Performance
0.518414
N/A
0.532578
N/A
1
0.688385
14.2885
0.671388
14.6771
4
0.671388
15.357
0.685552
15.6324
6
0.679887
15.2435
0.688385
15.1663
8
0.674221
15.2654
0.694051
15.4293
Llama-3-8B-FSparse
Llama-3-8B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.583569
N/A
0.583569
N/A
Sparse Performance
0.399433
N/A
0.424929
N/A
1
0.535014
15.4236
0.535411
14.4364
4
0.543909
15.4782
0.546742
15.606
6
0.546742
15.6365
0.526912
15.7718
8
0.549575
15.7159
0.541076
15.7997
Llama-2-13B-Chat-FSparse
Llama-2-13B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.524079
N/A
0.524079
N/A
Sparse Performance
0.498584
N/A
0.419263
N/A
1
0.490085
13.9589
0.461756
14.1419
4
0.524079
15.432
0.478992
15.8545
Llama-2-13B-FSparse
Llama-2-13B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.501416
N/A
0.501416
N/A
Sparse Performance
0.464589
N/A
0.390935
N/A
1
0.447592
15.5992
0.461756
15.3896
4
0.492918
15.9129
0.484419
15.8357
Llama-2-7B-Chat-FSparse
Llama-2-7B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.320113
N/A
0.320113
N/A
Sparse Performance
0.339943
N/A
0.3002823
N/A
1
0.31728
14.4663
0.305949
5.75938
4
0.345609
15.6588
0.325779
14.7519
6
0.342776
15.742
0.314448
14.5768
8
0.348442
15.7692
0.308782
14.3627
Llama-2-7B-FSparse
Llama-2-7B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.33711
N/A
0.33711
N/A
Sparse Performance
0.314448
N/A
0.235127
N/A
1
0.342776
15.5144
0.269122
15.3598
4
0.342776
15.9141
0.266289
15.8553
6
0.328612
15.943
0.274788
15.9266
8
0.322946
15.9671
0.271955
15.956"
ND ROUND,0.7472222222222222,"Table 18: SIRIUS on StrategyQA.
Experiment Settings
Llama-3-8B-Instruct-FSparse
Llama-3-8B-Instruct-CSparse
treewidth
Performance
AAL (out of 16)
Performance
AAL (out of 10)
Original Performance
0.770241
N/A
0.770241
N/A
Sparse Performance
0.713348
N/A
0.562363
N/A
1
0.741794
8.98893
0.737418
6.60992
4
0.741794
9.48412
0.746171
7.92521
6
0.743982
9.53292
0.728665
8.22667
8
0.743982
9.55946
0.708972
8.97268
Llama-3-8B-FSparse
Llama-3-8B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.649891
N/A
0.649891
N/A
Sparse Performance
0.599562
N/A
0.439825
N/A
1
0.623632
9.46018
0.531729
8.68633
4
0.623632
9.74383
0.560175
9.44497
6
0.632385
9.83975
0.560175
9.58122
8
0.680525
9.80493
0.555799
9.67198
Llama-2-13B-Chat-FSparse
Llama-2-13B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.695842
N/A
0.695842
N/A
Sparse Performance
0.706783
N/A
0.634573
N/A
1
0.71116
9.48266
0.682713
6.74106
4
0.667396
9.83767
0.715536
8.09959
6
0.671772
9.88989
0.693654
8.82263
Llama-2-13B-FSparse
Llama-2-13B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.63895
N/A
0.63895
N/A
Sparse Performance
0.693654
N/A
0.533917
N/A
1
0.695842
9.8979
0.595186
8.77388
4
0.682713
9.96438
0.643326
9.47368
6
0.689278
9.9789
0.63895
9.56319
Llama-2-7B-Chat-FSparse
Llama-2-7B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.654267
N/A
0.654267
N/A
Sparse Performance
0.678337
N/A
0.612691
N/A
1
0.684902
9.64754
0.669584
6.55818
4
0.691466
9.79539
0.671772
7.88988
6
0.68709
9.86474
0.643326
8.28982
8
0.689278
9.86488
0.66302
8.43513
Llama-2-7B-FSparse
Llama-2-7B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.599562
N/A
0.599562
N/A
Sparse Performance
0.592998
N/A
0.538293
N/A
1
0.612691
9.73256
0.568928
8.38473
4
0.599562
9.93662
0.560175
9.36272
6
0.617068
9.95582
0.536105
9.35857
8
0.610503
9.96658
0.544858
9.4642"
ND ROUND,0.75,"Table 19: SIRIUS on HumanEval.
Experiment Settings
Llama-3-8B-Instruct-FSparse
Llama-3-8B-Instruct-CSparse
treewidth
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.560975609756098
N/A
0.560975609756098
N/A
Sparse Performance
0.457317073170732
N/A
0.207317073170732
N/A
1
0.585365853658537
14.7624
0.554878048780488
12.1326
4
0.579268292682927
15.2299
0.530487804878049
14.0546
6
0.615853658536585
15.4209
0.518292682926829
14.4431
8
0.585365853658537
15.5009
0.524390243902439
14.6725
Llama-3-8B-FSparse
Llama-3-8B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.26219512195122
N/A
0.26219512195122
N/A
Sparse Performance
0.189024390243902
N/A
0.0670731707317073
N/A
1
0.231707317073171
15.1878
0.109756097560976
12.1402
4
0.274390243902439
15.2827
0.219512195121951
13.7718
6
0.26219512195122
14.5355
0.207317073170732
14.7776
8
0.29268
15.6305
0.24390243902439
15.1074
Llama-2-13B-Chat-FSparse
Llama-2-13B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.189024390243902
N/A
0.189024390243902
N/A
Sparse Performance
0.146341463414634
N/A
0.121951219512195
N/A
1
0.170731707317073
14.3976
0.189024390243902
9.6447
4
0.182926829268293
15.1956
0.176829268292683
10.7946
6
0.182926829268293
15.3494
0.170731707317073
11.0149
8
0.176829268292683
15.4067
0.170731707317073
11.1252
Llama-2-13B-FSparse
Llama-2-13B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.176829268292683
N/A
0.176829268292683
N/A
Sparse Performance
0.158536585365854
N/A
0.0975609756097561
N/A
1
0.146341463414634
15.2129
N/A
N/A
4
0.158536585365854
15.9093
0.146341463414634
14.1813
6
0.170731707317073
15.9211
0.134146341463415
14.5866
8
0.176829268292683
15.9015
0.134146341463415
14.7508
Llama-2-7B-Chat-FSparse
Llama-2-7B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.140243902439024
N/A
0.140243902439024
N/A
Sparse Performance
0.134146341463415
N/A
0.0670731707317073
N/A
1
0.134146341463415
14.055
0.140243902439024
8.83176
4
0.146341463414634
14.8504
0.146341463414634
10.1263
6
0.152439024390244
15.1924
0.152439024390244
10.5576
8
0.164634146341463
15.2742
0.158536585365854
10.822
Llama-2-7B-FSparse
Llama-2-7B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.115853658536585
N/A
0.115853658536585
N/A
Sparse Performance
0.115853658536585
N/A
0.0792682926829268
N/A
1
0.115853658536585
15.5268
0.121951219512195
12.6604
4
0.128048780487805
15.8167
0.121951219512195
14.4053
6
0.164634146341463
15.8615
0.121951219512195
14.8296
8
0.109756097560976
15.9189
0.128048780487805
14.8443"
ND ROUND,0.7527777777777778,"Table 20: SIRIUS on MBPP+.
Experiment Settings
Llama-3-8B-Instruct-FSparse
Llama-3-8B-Instruct-CSparse
treewidth
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.584656084656085
N/A
0.584656084656085
N/A
Sparse Performance
0.531746031746032
N/A
0.248677248677249
N/A
1
0.537037037037037
14.7267
0.563492063492064
11.5415
4
0.563492063492064
15.2699
0.566137566137566
13.5896
6
0.552910052910053
15.3782
0.571428571428571
14.0547
8
0.552910052910053
15.4689
0.566137566137566
14.7648
Llama-3-8B-FSparse
Llama-3-8B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.518518518518519
N/A
0.518518518518519
N/A
Sparse Performance
0.433862433862434
N/A
0.161375661375661
N/A
1
0.4894
14.8849
0.415343915343915
12.7016
4
0.484126984126984
15.4346
0.407407407407407
14.0936
6
0.473544973544974
15.3581
0.433862433862434
14.5662
8
0.468253968253968
15.6088
0.41005291005291
14.4752
Llama-2-13B-Chat-FSparse
Llama-2-13B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.23015873015873
N/A
0.27
N/A
Sparse Performance
0.19047619047619
N/A
0.1
N/A
1
0.201058201058201
13.8235
0.26
9.32827
4
0.232804232804233
14.8394
0.26
10.7346
6
0.224867724867725
15.0801
0.26
10.8897
8
0.227513227513228
15.2373
0.25
11.0214
Llama-2-13B-FSparse
Llama-2-13B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 16)
Original Performance
0.246031746031746
N/A
0.21
N/A
Sparse Performance
0.232804232804233
N/A
0.13
N/A
1
0.214285714285714
14.8374
0.22
14.5174
4
0.259259259259259
15.8547
0.24
15.6461
6
0.235449735449735
15.9197
0.23
15.7174
8
0.246031746031746
15.9094
0.22
15.7179
Llama-2-7B-Chat-FSparse
Llama-2-7B-Chat-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.261904761904762
N/A
0.261904761904762
N/A
Sparse Performance
0.224867724867725
N/A
0.100529100529101
N/A
1
0.238095238095238
14.1571
0.214285714285714
8.61325
4
0.26984126984127
14.9264
0.23015873015873
10.2517
6
0.238095238095238
15.2194
0.227513227513228
10.5845
8
0.272486772486773
15.3086
0.235449735449735
10.7621
10
N/A
N/A
0.232804232804233
10.8962
Llama-2-7B-FSparse
Llama-2-7B-CSparse
Performance
AAL (out of 16)
Performance
AAL (out of 12)
Original Performance
0.253968253968254
N/A
0.253968253968254
N/A
Sparse Performance
0.201058201058201
N/A
0.0793650793650794
N/A
1
0.216931216931217
14.6103
0.171957671957672
10.6643
4
0.238095238095238
15.5672
0.185185185185185
11.5561
6
0.224867724867725
15.6273
0.195767195767196
11.6547
8
0.240740740740741
15.5569
0.203703703703704
11.6753"
ND ROUND,0.7555555555555555,NeurIPS Paper Checklist
ND ROUND,0.7583333333333333,"The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit."
ND ROUND,0.7611111111111111,"Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:"
ND ROUND,0.7638888888888888,"• You should answer [Yes] , [No] , or [NA] .
• [NA] means either that the question is Not Applicable for that particular paper or the relevant
information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA)."
ND ROUND,0.7666666666666667,"The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper."
ND ROUND,0.7694444444444445,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
proper justification is given (e.g., ""error bars are not reported because it would be too computationally
expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found."
ND ROUND,0.7722222222222223,"IMPORTANT, please:"
ND ROUND,0.775,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
• Keep the checklist subsection headings, questions/answers and guidelines below.
• Do not modify the questions and only use the provided macros for your answers."
ND ROUND,0.7777777777777778,A. Claims
ND ROUND,0.7805555555555556,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification:
Guidelines:"
ND ROUND,0.7833333333333333,"• The answer NA means that the abstract and introduction do not include the claims made
in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or NA
answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
B. Limitations"
ND ROUND,0.7861111111111111,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [No]
Justification: [TODO]"
ND ROUND,0.7888888888888889,Guidelines:
ND ROUND,0.7916666666666666,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was only
tested on a few datasets or with a few runs. In general, empirical results often depend on
implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be used
reliably to provide closed captions for online lectures because it fails to handle technical
jargon.
• The authors should discuss the computational efficiency of the proposed algorithms and
how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an important
role in developing norms that preserve the integrity of the community. Reviewers will be
specifically instructed to not penalize honesty concerning limitations."
ND ROUND,0.7944444444444444,C. Theory Assumptions and Proofs
ND ROUND,0.7972222222222223,"Question: For each theoretical result, does the paper provide the full set of assumptions and a
complete (and correct) proof?"
ND ROUND,0.8,Answer: [NA]
ND ROUND,0.8027777777777778,Justification:
ND ROUND,0.8055555555555556,Guidelines:
ND ROUND,0.8083333333333333,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if they
appear in the supplemental material, the authors are encouraged to provide a short proof
sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
ND ROUND,0.8111111111111111,D. Experimental Result Reproducibility
ND ROUND,0.8138888888888889,"Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
ND ROUND,0.8166666666666667,Answer: [Yes]
ND ROUND,0.8194444444444444,Justification:
ND ROUND,0.8222222222222222,Guidelines:
ND ROUND,0.825,• The answer NA means that the paper does not include experiments.
ND ROUND,0.8277777777777777,"• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of whether
the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example"
ND ROUND,0.8305555555555556,"1. If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
2. If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
3. If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct the
dataset).
4. We recognize that reproducibility may be tricky in some cases, in which case authors
are welcome to describe the particular way they provide for reproducibility. In the
case of closed-source models, it may be that access to the model is limited in some
way (e.g., to registered users), but it should be possible for other researchers to have
some path to reproducing or verifying the results.
E. Open access to data and code"
ND ROUND,0.8333333333333334,"Question: Does the paper provide open access to the data and code, with sufficient instructions
to faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [No]
Justification:
Guidelines:"
ND ROUND,0.8361111111111111,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
ND ROUND,0.8388888888888889,F. Experimental Setting/Details
ND ROUND,0.8416666666666667,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
ND ROUND,0.8444444444444444,Answer: [Yes]
ND ROUND,0.8472222222222222,Justification:
ND ROUND,0.85,Guidelines:
ND ROUND,0.8527777777777777,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
ND ROUND,0.8555555555555555,G. Experiment Statistical Significance
ND ROUND,0.8583333333333333,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
ND ROUND,0.8611111111111112,Answer: [No]
ND ROUND,0.8638888888888889,Justification:
ND ROUND,0.8666666666666667,Guidelines:
ND ROUND,0.8694444444444445,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the
main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula, call
to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error of
the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
ND ROUND,0.8722222222222222,H. Experiments Compute Resources
ND ROUND,0.875,"Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the
experiments?"
ND ROUND,0.8777777777777778,Answer: [Yes]
ND ROUND,0.8805555555555555,Justification:
ND ROUND,0.8833333333333333,Guidelines:
ND ROUND,0.8861111111111111,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or
cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute."
ND ROUND,0.8888888888888888,"• The paper should disclose whether the full research project required more compute than
the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t
make it into the paper)."
ND ROUND,0.8916666666666667,I. Code Of Ethics
ND ROUND,0.8944444444444445,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
ND ROUND,0.8972222222222223,Answer: [Yes]
ND ROUND,0.9,Justification:
ND ROUND,0.9027777777777778,Guidelines:
ND ROUND,0.9055555555555556,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special considera-
tion due to laws or regulations in their jurisdiction)."
ND ROUND,0.9083333333333333,J. Broader Impacts
ND ROUND,0.9111111111111111,"Question: Does the paper discuss both potential positive societal impacts and negative societal
impacts of the work performed?"
ND ROUND,0.9138888888888889,Answer: [No]
ND ROUND,0.9166666666666666,Justification:
ND ROUND,0.9194444444444444,Guidelines:
ND ROUND,0.9222222222222223,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is being
used as intended and functioning correctly, harms that could arise when the technology is
being used as intended but gives incorrect results, and harms following from (intentional
or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
ND ROUND,0.925,K. Safeguards
ND ROUND,0.9277777777777778,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
ND ROUND,0.9305555555555556,Answer: [NA]
ND ROUND,0.9333333333333333,Justification:
ND ROUND,0.9361111111111111,Guidelines:
ND ROUND,0.9388888888888889,• The answer NA means that the paper poses no such risks.
ND ROUND,0.9416666666666667,"• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best faith
effort."
ND ROUND,0.9444444444444444,L. Licenses for existing assets
ND ROUND,0.9472222222222222,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
ND ROUND,0.95,Answer: [NA]
ND ROUND,0.9527777777777777,Justification:
ND ROUND,0.9555555555555556,Guidelines:
ND ROUND,0.9583333333333334,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of the
derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to the
asset’s creators."
ND ROUND,0.9611111111111111,M. New Assets
ND ROUND,0.9638888888888889,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
ND ROUND,0.9666666666666667,Answer: [NA]
ND ROUND,0.9694444444444444,Justification:
ND ROUND,0.9722222222222222,Guidelines:
ND ROUND,0.975,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
ND ROUND,0.9777777777777777,N. Crowdsourcing and Research with Human Subjects
ND ROUND,0.9805555555555555,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as well
as details about compensation (if any)?"
ND ROUND,0.9833333333333333,Answer: [NA]
ND ROUND,0.9861111111111112,Justification:
ND ROUND,0.9888888888888889,Guidelines:
ND ROUND,0.9916666666666667,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
O. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
ND ROUND,0.9944444444444445,"Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals
(or an equivalent approval/review based on the requirements of your country or institution)
were obtained?
Answer: [NA]
Justification:
Guidelines:"
ND ROUND,0.9972222222222222,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
