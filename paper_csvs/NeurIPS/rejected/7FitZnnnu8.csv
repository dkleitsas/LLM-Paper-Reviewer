Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018214936247723133,"Estimating the parameters of a probabilistic directed graphical model from in-
1"
ABSTRACT,0.0036429872495446266,"complete data remains a long-standing challenge. This is because, in the pres-
2"
ABSTRACT,0.00546448087431694,"ence of latent variables, both the likelihood function and posterior distribution are
3"
ABSTRACT,0.007285974499089253,"intractable without further assumptions about structural dependencies or model
4"
ABSTRACT,0.009107468123861567,"classes.
While existing learning methods are fundamentally based on likeli-
5"
ABSTRACT,0.01092896174863388,"hood maximization, here we offer a new view of the parameter learning problem
6"
ABSTRACT,0.012750455373406194,"through the lens of optimal transport. This perspective licenses a framework that
7"
ABSTRACT,0.014571948998178506,"operates on many directed graphs without making unrealistic assumptions on the
8"
ABSTRACT,0.01639344262295082,"posterior over the latent variables or resorting to black-box variational approxima-
9"
ABSTRACT,0.018214936247723135,"tions. We develop a theoretical framework and support it with extensive empirical
10"
ABSTRACT,0.020036429872495445,"evidence demonstrating the flexibility and versatility of our approach. Across
11"
ABSTRACT,0.02185792349726776,"experiments, we show that not only can our method recover the ground-truth pa-
12"
ABSTRACT,0.023679417122040074,"rameters but it also performs competitively on downstream applications, notably
13"
ABSTRACT,0.025500910746812388,"the non-trivial task of discrete representation learning.
14"
INTRODUCTION,0.0273224043715847,"1
Introduction
15"
INTRODUCTION,0.029143897996357013,"Learning probabilistic directed graphical models (DGMs, also known as Bayesian networks) with
16"
INTRODUCTION,0.030965391621129327,"latent variables is an important ongoing challenge in machine learning and statistics. This paper
17"
INTRODUCTION,0.03278688524590164,"focuses on parameter learning, i.e., estimating the parameters of a DGM given its known structure.
18"
INTRODUCTION,0.03460837887067395,"Learning DGMs has a long history, dating back to classical indirect likelihood-maximization ap-
19"
INTRODUCTION,0.03642987249544627,"proaches such as expectation maximization [EM, 15]. However, despite all its success stories, EM
20"
INTRODUCTION,0.03825136612021858,"is well-known to suffer from local optima issues. More importantly, EM becomes inapplicable when
21"
INTRODUCTION,0.04007285974499089,"the posterior distribution is intractable, which arises fairly often in practice.
22"
INTRODUCTION,0.04189435336976321,"A large family of related methods based on variational inference [VI, 30, 27] have demonstrated
23"
INTRODUCTION,0.04371584699453552,"tremendous potential in this case, where the evidence lower bound (ELBO) is not only used for
24"
INTRODUCTION,0.04553734061930783,"posterior approximation but also for point estimation of the model parameters. Such an approach
25"
INTRODUCTION,0.04735883424408015,"has proved surprisingly effective and robust to overfitting, especially when having a small number of
26"
INTRODUCTION,0.04918032786885246,"parameters. From a high-level perspective, both EM and VI are based on likelihood maximization
27"
INTRODUCTION,0.051001821493624776,"in the presence of latent variables, which ultimately requires carrying out expectations over the
28"
INTRODUCTION,0.052823315118397086,"commonly intractable posterior. In order to address this challenge, a large spectrum of methods
29"
INTRODUCTION,0.0546448087431694,"have been proposed in the literature and we refer the reader to [5] for an excellent discussion of
30"
INTRODUCTION,0.056466302367941715,"these approaches. Here we characterize them between two extremes. At one extreme, restrictive
31"
INTRODUCTION,0.058287795992714025,"assumptions about the structure (e.g., as in mean-field approximations) or the model class (e.g.,
32"
INTRODUCTION,0.060109289617486336,"using conjugate exponential families) must be made to simplify the task. At the other extreme, when
33"
INTRODUCTION,0.061930783242258654,"no assumptions are made, most existing black-box methods exploit very little information about the
34"
INTRODUCTION,0.06375227686703097,"structure of the known probabilistic model (for example, in black-box and stochastic variational
35"
INTRODUCTION,0.06557377049180328,"inference [44, 27], hierarchical approaches [45] and normalizing flows [42]).
36"
INTRODUCTION,0.06739526411657559,"Addressing the problem at its core, we hereby propose an alternative strategy to likelihood maxi-
37"
INTRODUCTION,0.0692167577413479,"mization that does not require the estimation of expectations over the posterior distribution. Con-
38"
INTRODUCTION,0.07103825136612021,"cretely, parameter learning is now viewed through the lens of optimal transport [54], where the data
39"
INTRODUCTION,0.07285974499089254,"distribution is the source and the true model distribution is the target. Instead of minimizing a Kull-
40"
INTRODUCTION,0.07468123861566485,"back‚ÄìLeibler (KL) divergence (which likelihood maximization methods are essentially doing), here
41"
INTRODUCTION,0.07650273224043716,"we aim to find a point estimate Œ∏‚àóthat minimizes the Wasserstein distance [WD, 31] between these
42"
INTRODUCTION,0.07832422586520947,"two distributions.
43"
INTRODUCTION,0.08014571948998178,"This perspective allows us to leverage desirable properties of the WD in comparison with other
44"
INTRODUCTION,0.08196721311475409,"metrics. These properties have motivated the recent surge in generative models, e.g., Wasserstein
45"
INTRODUCTION,0.08378870673952642,"GANs [1, 9] and Wasserstein Auto-encoders [50]. Indeed, the WD is shown to be well-behaved
46"
INTRODUCTION,0.08561020036429873,"in situations where standard metrics such as the KL or JS (Jensen-Shannon) divergences are either
47"
INTRODUCTION,0.08743169398907104,"infinite or undefined [43, 4]. The WD thus characterizes a more meaningful distance, especially
48"
INTRODUCTION,0.08925318761384335,"when the two distributions reside in low-dimensional manifolds [9]. Ultimately, this novel view
49"
INTRODUCTION,0.09107468123861566,"enables us to pursue an ambitious goal towards a model-agnostic and scalable learning framework.
50"
INTRODUCTION,0.09289617486338798,"Contributions.
We present an entirely different view that casts parameter estimation as an optimal
51"
INTRODUCTION,0.0947176684881603,"transport problem [54], where the goal is to find the optimal plan transporting ‚Äúmass‚Äù from the data
52"
INTRODUCTION,0.0965391621129326,"distribution to the model distribution. To achieve this, our method minimizes the WD between these
53"
INTRODUCTION,0.09836065573770492,"two distributions. This permits a flexible framework applicable to any type of variable and graphical
54"
INTRODUCTION,0.10018214936247723,"structure. In summary, we make the following contributions:
55"
INTRODUCTION,0.10200364298724955,"‚Ä¢ We introduce OTP-DAG - an Optimal Transport framework for Parameter Learning in Directed
56"
INTRODUCTION,0.10382513661202186,"Acyclic Graphical models. OTP-DAG is an alternative line of thinking about parameter learning.
57"
INTRODUCTION,0.10564663023679417,"Diverging from the existing frameworks, the underlying idea is to find the parameter set associated
58"
INTRODUCTION,0.10746812386156648,"with the distribution that yields the lowest transportation cost from the data distribution.
59"
INTRODUCTION,0.1092896174863388,"‚Ä¢ We present theoretical developments showing that minimizing the transport cost is equivalent to
60"
INTRODUCTION,0.1111111111111111,"minimizing the reconstruction error between the observed data and the model generation. This
61"
INTRODUCTION,0.11293260473588343,"renders a tractable training objective to be solved efficiently with stochastic optimization.
62"
INTRODUCTION,0.11475409836065574,"‚Ä¢ We provide empirical evidence demonstrating the versatility of our method on various graphical
63"
INTRODUCTION,0.11657559198542805,"structures. OTP-DAG is shown to successfully recover the ground-truth parameters and achieve
64"
INTRODUCTION,0.11839708561020036,"competitive performance across a range of downstream applications.
65"
BACKGROUND AND RELATED WORK,0.12021857923497267,"2
Background and Related Work
66"
BACKGROUND AND RELATED WORK,0.122040072859745,"We first introduce the notations and basic concepts used throughout the paper. We reserve bold
67"
BACKGROUND AND RELATED WORK,0.12386156648451731,"capital letters (i.e., G) for notations related to graphs. We use calligraphic letters (i.e. X) for spaces,
68"
BACKGROUND AND RELATED WORK,0.12568306010928962,"italic capital letters (i.e. X) for random variables, and lower case letters (i.e. x) for their values.
69"
BACKGROUND AND RELATED WORK,0.12750455373406194,"A directed graph G = (V, E) consists of a set of nodes V and an edge set E ‚äÜV2 of ordered
70"
BACKGROUND AND RELATED WORK,0.12932604735883424,"pairs of nodes with (v, v) /‚ààE for any v ‚ààV (one without self-loops). For a pair of nodes i, j with
71"
BACKGROUND AND RELATED WORK,0.13114754098360656,"(i, j) ‚ààE, there is an arrow pointing from i to j and we write i ‚Üíj. Two nodes i and j are adjacent
72"
BACKGROUND AND RELATED WORK,0.13296903460837886,"if either (i, j) ‚ààE or (j, i) ‚ààE. If there is an arrow from i to j then i is a parent of j and j is a child
73"
BACKGROUND AND RELATED WORK,0.13479052823315119,"of i. A Bayesian network structure G = (V, E) is a directed acyclic graph (DAG), in which the
74"
BACKGROUND AND RELATED WORK,0.1366120218579235,"nodes represent random variables X = [Xi]n
i=1 with index set V := {1, ..., n}. Let PAXi denote
75"
BACKGROUND AND RELATED WORK,0.1384335154826958,"the set of variables associated with parents of node i in G.
76"
BACKGROUND AND RELATED WORK,0.14025500910746813,"In this work, we tackle the classic yet important problem of learning the parameters of a directed
77"
BACKGROUND AND RELATED WORK,0.14207650273224043,"graph from partially observed data. Let O ‚äÜV and XO = [Xi]i‚ààO be the set of observed nodes
78"
BACKGROUND AND RELATED WORK,0.14389799635701275,"and H := V\O be the set of hidden nodes. Let PŒ∏ and Pd respectively denote the distribution
79"
BACKGROUND AND RELATED WORK,0.14571948998178508,"induced by the graphical model and the empirical one induced by the complete (yet unknown) data.
80"
BACKGROUND AND RELATED WORK,0.14754098360655737,"Given a fixed graphical structure G and some set of i.i.d data points, we aim to find the point es-
81"
BACKGROUND AND RELATED WORK,0.1493624772313297,"timate Œ∏‚àóthat best fits the observed data XO. The conventional approach is to minimize the KL
82"
BACKGROUND AND RELATED WORK,0.151183970856102,"divergence between the model distribution and the empirical data distribution over observed data
83"
BACKGROUND AND RELATED WORK,0.15300546448087432,"i.e., DKL(Pd(XO), PŒ∏(XO)), which is equivalent to maximizing the likelihood PŒ∏(XO) w.r.t Œ∏.
84"
BACKGROUND AND RELATED WORK,0.15482695810564662,"In the presence of latent variables, the marginal likelihood, given as PŒ∏(XO) =
R"
BACKGROUND AND RELATED WORK,0.15664845173041894,"XH PŒ∏(X)dXH,
85"
BACKGROUND AND RELATED WORK,0.15846994535519127,"is generally intractable. Standard approaches then resort to maximizing a bound on the marginal
86"
BACKGROUND AND RELATED WORK,0.16029143897996356,"log-likelihood, known as the evidence lower bound (ELBO), which is essentially the objective of
87"
BACKGROUND AND RELATED WORK,0.1621129326047359,"EM [38] and VI [30]. Optimization of the ELBO for parameter learning in practice requires many
88"
BACKGROUND AND RELATED WORK,0.16393442622950818,"considerations. For vanilla EM, the algorithm only works if the true posterior density can be com-
89"
BACKGROUND AND RELATED WORK,0.1657559198542805,"puted exactly. Furthermore, EM is originally a batch algorithm, thereby converging slowly on large
90"
BACKGROUND AND RELATED WORK,0.16757741347905283,"datasets [36]. Subsequently, researchers have tried exploring other methods for scalability, including
91"
BACKGROUND AND RELATED WORK,0.16939890710382513,"attempts to combine EM with approximate inference [56, 40, 14, 10, 13, 36, 41].
92"
BACKGROUND AND RELATED WORK,0.17122040072859745,"When exact inference is infeasible, a variational approximation is the go-to solution. Along this
93"
BACKGROUND AND RELATED WORK,0.17304189435336975,"line, research efforts have concentrated on ensuring tractability of the ELBO via the mean-field
94"
BACKGROUND AND RELATED WORK,0.17486338797814208,"assumption [11] and its relaxation known as structured mean field [47]. Scalability has been one
95"
BACKGROUND AND RELATED WORK,0.1766848816029144,"of the main challenges facing the early VI formulations since it is a batch algorithm. This has
96"
BACKGROUND AND RELATED WORK,0.1785063752276867,"triggered the development of stochastic variational inference (SVI) [27, 26, 16, 29, 8, 7] which
97"
BACKGROUND AND RELATED WORK,0.18032786885245902,"applies stochastic optimization to solve VI objectives. Another line of work is collapsed VI that
98"
BACKGROUND AND RELATED WORK,0.18214936247723132,"explicitly integrates out certain model parameters or latent variables in an analytic manner [23,
99"
BACKGROUND AND RELATED WORK,0.18397085610200364,"32, 48, 34]. Without a closed form, one could resort to Markov chain Monte Carlo [18, 19, 21],
100"
BACKGROUND AND RELATED WORK,0.18579234972677597,"which however tends to be slow. More accurate variational posteriors also exist, namely, through
101"
BACKGROUND AND RELATED WORK,0.18761384335154827,"hierarchical variational models [45], implicit posteriors [49, 58, 37, 49], normalizing flows [33], or
102"
BACKGROUND AND RELATED WORK,0.1894353369763206,"copula distribution [51]. To avoid computing the ELBO analytically, one can obtain an unbiased
103"
BACKGROUND AND RELATED WORK,0.1912568306010929,"gradient estimator using Monte Carlo and re-parameterization tricks [44, 57]. As mentioned in
104"
BACKGROUND AND RELATED WORK,0.1930783242258652,"the introduction, an excellent summary of these approaches is discussed in [5, ¬ß6]. Extensions of
105"
BACKGROUND AND RELATED WORK,0.19489981785063754,"VI to other divergence measures than KL divergence e.g., Œ±‚àídivergence or f‚àídivergence, also
106"
BACKGROUND AND RELATED WORK,0.19672131147540983,"exist [35, 24, 55]. In the causal inference literature, a related direction is to learn both the graphical
107"
BACKGROUND AND RELATED WORK,0.19854280510018216,"structure and parameters of the corresponding structural equation model [60, 17]. These frameworks
108"
BACKGROUND AND RELATED WORK,0.20036429872495445,"are often limited to additive noise models while assuming no latent confounders.
109"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.20218579234972678,"3
Optimal Transport for Learning Directed Graphical Models
110"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2040072859744991,"We begin by explaining how parameter learning can be reformulated into an optimal transport prob-
111"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2058287795992714,"lem [53] and thereafter introduce our novel theoretical contribution.
112"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.20765027322404372,"We consider a DAG G(V, E) over random variables X = [Xi]n
i=1 that represents the data generative
113"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.20947176684881602,"process of an underlying system. The system consists of X as the set of endogenous variables
114"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.21129326047358835,"and U = {Ui}n
i=1 as the set of exogenous variables representing external factors affecting the
115"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.21311475409836064,"system. Associated with every Xi is an exogenous variable Ui whose values are sampled from a
116"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.21493624772313297,"prior distribution P(U) independently from other exogenous variables. For the purpose of this work,
117"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2167577413479053,"our framework operates on an extended graph consisting of both endogenous and exogenous nodes
118"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2185792349726776,"(See Figure 1b). In the graph G, Ui is represented by a node with no ancestors that has an outgoing
119"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2204007285974499,"arrow towards node i. Consequently, for every endogenous variable, its parent set PAXi is extended
120"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2222222222222222,"to include an exogenous variable and possibly some other endogenous variables. Henceforth, every
121"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.22404371584699453,"distribution PŒ∏i
 
Xi|PAXi

can be reparameterized into a deterministic assignment
122"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.22586520947176686,"Xi = œài
 
PAXi, Ui

, for i = 1, ..., n."
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.22768670309653916,"The ultimate goal is to estimate Œ∏ = {Œ∏i}n
i=1 as the parameters of the set of deterministic functions
123"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.22950819672131148,"œà = {œài}n
i=1. We will use the notation œàŒ∏ to emphasize this connection from now on. ùëøùüè ùëøùüê ùëøùüë ùëøùüí"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.23132969034608378,"(a) DAG ùëºùüí
ùëºùüê ùëºùüë ùëºùüè
ùëøùüè ùëøùüê ùëøùüë ùëøùüí"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2331511839708561,"(b) Extended DAG ùëºùüí
ùëºùüê ùëºùüë ùëºùüè
ùëøùüè ùëøùüê ùëøùüë ùëøùüí"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.23497267759562843,(c) Algorithmic DAG
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.23679417122040072,"Figure 1: (a) A DAG represents a system of 4 endogenous variables where X1, X3 are observed
(black-shaded) and X2, X4 are hidden variables (non-shaded). (b): The extended DAG that includes
an additional set of independent exogenous variables U1, U2, U3, U4 (grey-shaded) acting on each
endogenous variable. U1, U2, U3, U4 ‚àºP(U) where P(U) is a prior product distribution. (c)
Visualization of our backward-forward algorithm, where the dashed arcs represent the backward
maps involved in optimization.
124"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.23861566484517305,"Given the data distribution Pd(XO) and the model distribution PŒ∏(XO) over the observed set O,
125"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.24043715846994534,"the optimal transport (OT) goal is to find the parameter set Œ∏ that minimizes the cost of transport
126"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.24225865209471767,"between these two distributions. The Kantorovich‚Äôs formulation of the problem is given by
127"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.24408014571949,"Wc
 
Pd; PŒ∏

:=
inf
Œì‚àºP(X‚àºPd,Y ‚àºPŒ∏)E(X,Y )‚àºŒì

c(X, Y )

,
(1)"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2459016393442623,"where P(X ‚àºPd, Y ‚àºPŒ∏) is a set of all joint distributions of
 
Pd; PŒ∏

and c : XO √ó XO 7‚ÜíR+ is
128"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.24772313296903462,"any measurable cost function over XO (i.e., the product space of the spaces of observed variables)
129"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2495446265938069,"that is defined as c(XO, YO) := P"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.25136612021857924,"i‚ààO ci(Xi, Yi) where ci is a measurable cost function over a
130"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.25318761384335153,"space of a certain observed variable.
131"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2550091074681239,"Let PŒ∏(PAXi, Ui) denote the joint distribution of PAXi and Ui factorized according to the graphical
132"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2568306010928962,"model. Let Ui denote the space over random variable Ui. The key ingredient of our theoretical
133"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2586520947176685,"development is local backward mapping. For every observed node i ‚ààO, we define a stochastic
134"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2604735883424408,"‚Äúbackward‚Äù map œïi : Xi 7‚ÜíŒ†k‚ààPAXi Xk √ó Ui such that œïi ‚ààC(Xi) where C(Xi) is the constraint
135"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.26229508196721313,"set given as
136"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2641165755919854,"C(Xi) :=

œïi : œïi#Pd(Xi) = PŒ∏(PAXi, Ui)
	
."
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2659380692167577,"Essentially, œïi pushes the data marginal of Xi forward to the model marginal of its parent variables.
137"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2677595628415301,"If PAXi are latent variables, œïi can be viewed as a stochastic decoder mapping Xi to the conditional
138"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.26958105646630237,"density œïi(PAXi|Xi).
139"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.27140255009107467,"Theorem 1 presents the main theoretical contribution of our paper. Our OT problem is concerned
140"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.273224043715847,"with finding the optimal set of deterministic ‚Äúforward‚Äù maps œàŒ∏ and stochastic ""backward"" maps
141

œïi ‚ààC(Xi)"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2750455373406193,"i‚ààO that minimizes the cost of transporting the mass from Pd to PŒ∏ over O. While
142"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2768670309653916,"the formulation in Eq. (1) is not trainable, we show that the problem is reduced to minimizing the
143"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2786885245901639,"reconstruction error between the data generated from PŒ∏ and the observed data. To understand how
144"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.28051001821493626,"reconstruction works, let us examine Figure 1c. Given X1 and X3 as observed nodes, we sample
145"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.28233151183970856,"X1 ‚àºPd(X1), X3 ‚àºPd(X3) and evaluate the local densities œï1(PAX1|X1), œï3(PAX3|X3) where
146"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.28415300546448086,"PAX1 = {X2, X4, U1} and PAX3 = {X4, U3}. The next step is to sample PAX1 ‚àºœï1(PAX1|X1)
147"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2859744990892532,"and PAX3 ‚àºœï3(PAX3|X3), which are plugged back to the model œàŒ∏ to obtain the reconstructions
148"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2877959927140255,"f
X1 = œàŒ∏1(PAX1) and f
X3 = œàŒ∏3(PAX3). We wish to learn Œ∏ such that X1 and X3 are reconstructed
149"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2896174863387978,"correctly. For a general graphical model, this optimization objective is formalized as
150"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.29143897996357016,"Theorem 1 For every œïi as defined above and fixed œàŒ∏,
151"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.29326047358834245,"Wc
 
Pd(XO); PŒ∏(XO)

=
inf

œïi‚ààC(Xi)
"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.29508196721311475,"i‚ààO
EXO‚àºPd(XO),PAXO‚àºœï(XO)

c
 
XO, œàŒ∏(PAXO)

,
(2)"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.29690346083788705,"where PAXO :=

[Xij]j‚ààPAXi
"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.2987249544626594,"i‚ààO.
152"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3005464480874317,"The proof is provided in Appendix A. It is seen that Theorem 1 set ups a trainable form for our
153"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.302367941712204,"optimization solution. Notice that the quality of the reconstruction hinges on how well the back-
154"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.30418943533697634,"ward maps approximate the true local densities. To ensure approximation fidelity, every back-
155"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.30601092896174864,"ward function œïi must satisfy its push-forward constraint defined by C. In the above example,
156"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.30783242258652094,"the backward maps œïi and œï3 must be constructed such that œï1#(X1) = PŒ∏(X2, X4, U1) and
157"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.30965391621129323,"œï3#(X3) = PŒ∏(X4, U3). This gives us a constraint optimization problem, and we relax the con-
158"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3114754098360656,"straints by adding a penalty to the above objective.
159"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3132969034608379,"The final optimization objective is therefore given as
160"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3151183970856102,"JW S = inf
œà,œï
EXO‚àºPd(XO),PAXO‚àºœï(XO)

c
 
XO, œàŒ∏(PAXO)

+ Œ∑ D
 
œï, PŒ∏

,
(3)"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.31693989071038253,"where D is any arbitrary divergence measure and Œ∑ > 0 is a trade-off hyper-parameter. D
 
œï, PŒ∏

is
161"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.31876138433515483,"a short-hand for divergence between all pairs of backward and forward distributions.
162"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3205828779599271,"This theoretical result provides us with several interesting properties: (1) to minimize the global
163"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3224043715846995,"OT cost between the model distribution and the data distribution, one only needs to characterize the
164"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3242258652094718,"local densities by specifying the backward maps from every observed node to its parents and opti-
165"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.32604735883424407,"mizing them with appropriate cost metrics; (2) all model parameters are optimized simultaneously
166"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.32786885245901637,"within a single framework whether the variables are continuous or discrete ; (3) the computational
167"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3296903460837887,"process can be automated without deriving an analytic lower bound or restricting to certain graph-
168"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.331511839708561,"ical structures. In connection with VI, OTP-DAG is also optimization-based. We in fact leverage
169"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3333333333333333,"modern VI techniques of reparameterization and amortized inference [6] for solving it efficiently
170"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.33515482695810567,"via stochastic gradient descent. However, unlike such advances as hierarchical VI, our method does
171"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.33697632058287796,"not place any prior over the variational distribution on the latent variables underlying the variational
172"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.33879781420765026,"posterior [45]. For providing a guarantee, OTP-DAG relies on the condition that the backward maps
173"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3406193078324226,"are sufficiently expressive to cover the push-forward constraints. We prove further in Appendix A
174"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3424408014571949,"that given a suitably rich family of backward functions, our algorithm OTP-DAG can converge to the
175"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3442622950819672,"ground-truth parameters. Details on our algorithm can be found in Appendix B. In the next section,
176"
OPTIMAL TRANSPORT FOR LEARNING DIRECTED GRAPHICAL MODELS,0.3460837887067395,"we illustrate how OTP-DAG algorithm is realized in practical applications.
177"
APPLICATIONS,0.34790528233151186,"4
Applications
178"
APPLICATIONS,0.34972677595628415,"We apply OTP-DAG on 3 widely-used graphical models for a total of 5 different sub-tasks. Here we
179"
APPLICATIONS,0.35154826958105645,"aim to demonstrate the versatility of OTP-DAG: OTP-DAG can be exploited for various purposes
180"
APPLICATIONS,0.3533697632058288,"through a single learning procedure. In terms of estimation accuracy, OTP-DAG is capable of re-
181"
APPLICATIONS,0.3551912568306011,"covering the ground-truth parameters while achieving the comparable or better performance level of
182"
APPLICATIONS,0.3570127504553734,"existing frameworks across downstream tasks.1
183"
APPLICATIONS,0.3588342440801457,"We consider various directed probabilistic models with either continuous or discrete variables. We
184"
APPLICATIONS,0.36065573770491804,"begin with (1) Latent Dirichlet Allocation [12] for topic modeling and (2) Hidden Markov Model
185"
APPLICATIONS,0.36247723132969034,"(HMM) for sequential modeling tasks. We conclude with a more challenging setting: (3) Discrete
186"
APPLICATIONS,0.36429872495446264,"Representation Learning (Discrete RepL) that cannot simply be solved by EM or MAP (maximum a
187"
APPLICATIONS,0.366120218579235,"posteriori). It in fact invokes deep generative modeling via a pioneering development called Vector
188"
APPLICATIONS,0.3679417122040073,"Quantization Variational Auto-Encoder (VQ-VAE) [52]. We investigate an application of OTP-DAG
189"
APPLICATIONS,0.3697632058287796,"algorithm to learning discrete representations by grounding it into a parameter learning problem.
190"
APPLICATIONS,0.37158469945355194,"Note that our goal is not to achieve the state-of-the-art performance, rather to prove OTP-DAG as a
191"
APPLICATIONS,0.37340619307832423,"versatile approach for learning parameters of directed graphical models. Figure 2 illustrates the em-
192"
APPLICATIONS,0.37522768670309653,"pirical DAG structures of the 3 applications. Unlike the standard visualization where the parameters
193"
APPLICATIONS,0.3770491803278688,"are considered hidden nodes, our graph separates model parameters from latent variables and only
194"
APPLICATIONS,0.3788706739526412,"illustrates random variables and their dependencies (except the special setting of Discrete RepL). We
195"
APPLICATIONS,0.3806921675774135,"also omit the exogenous variables associated with the hidden nodes for visibility, since only those
196"
APPLICATIONS,0.3825136612021858,"acting on the observed nodes are relevant for computation. There is also a noticeable difference
197"
APPLICATIONS,0.3843351548269581,"between Figure 2 and Figure 1c: the empirical version does not involve learning the backward maps
198"
APPLICATIONS,0.3861566484517304,"for the exogenous variables. This stems from an experimental observation that sampling the noise
199"
APPLICATIONS,0.3879781420765027,"from an appropriate prior distribution at random suffices to yield accurate estimation. We find it
200"
APPLICATIONS,0.38979963570127507,"to be beneficial in that training complexity can be greatly reduced. In the following, we report the
201"
APPLICATIONS,0.39162112932604737,"main experimental results, leaving the discussion of the formulation and technicalities in Appendix
202"
APPLICATIONS,0.39344262295081966,"C. In all tables, we report the average results over 5 random initializations and the best ones are
203"
APPLICATIONS,0.39526411657559196,"highlighted in bold. In addition, ‚Üë, ‚Üìindicate higher/lower performance is better, respectively.
204 ùëº ùëæ
ùíÅ
ùúΩ ùëµ ùë¥"
APPLICATIONS,0.3970856102003643,"(a) LDA ùëøùüê ùíÅùüè ùëøùüè ùíÅùüê
ùíÅùüë ùëøùüë
ùëøùüí ùíÅùüí"
APPLICATIONS,0.3989071038251366,"ùëºùüè
ùëºùüê
ùëºùüë
ùëºùüí"
APPLICATIONS,0.4007285974499089,(b) HMM ùíÅ ùëø ùùÅùë™ ùëº ùë™
APPLICATIONS,0.40255009107468126,(c) Discrete RepL
APPLICATIONS,0.40437158469945356,"Figure 2: Empirical structure of (a) latent Dirichlet allocation model (in plate notation), (b) standard
hidden Markov model, and (c) discrete representation learning."
LATENT DIRICHLET ALLOCATION,0.40619307832422585,"4.1
Latent Dirichlet Allocation
205"
LATENT DIRICHLET ALLOCATION,0.4080145719489982,"Let us consider a corpus D of M independent documents where each document is a sequence of N
206"
LATENT DIRICHLET ALLOCATION,0.4098360655737705,"words denoted by W = (W1, W2, ¬∑ ¬∑ ¬∑ , WN). Documents are represented as random mixtures over
207"
LATENT DIRICHLET ALLOCATION,0.4116575591985428,"K latent topics, each of which is characterized by a distribution over words. Let V be the size of a
208"
LATENT DIRICHLET ALLOCATION,0.4134790528233151,"vocabulary indexed by {1, ¬∑ ¬∑ ¬∑ , V }. Latent Dirichlet Allocation (LDA) [12] dictates the following
209"
LATENT DIRICHLET ALLOCATION,0.41530054644808745,"generative process for every document in the corpus:
210"
LATENT DIRICHLET ALLOCATION,0.41712204007285975,1Our code is anonymously published at https://anonymous.4open.science/r/OTP-7944/.
LATENT DIRICHLET ALLOCATION,0.41894353369763204,"1. Sample Œ∏ ‚àºDir(Œ±) with Œ± < 1,
211"
LATENT DIRICHLET ALLOCATION,0.4207650273224044,"2. Sample Œ≥k ‚àºDir(Œ≤) where k ‚àà{1, ¬∑ ¬∑ ¬∑ , K},
212"
LATENT DIRICHLET ALLOCATION,0.4225865209471767,"3. For each of the word positions n ‚àà{1, ¬∑ ¬∑ ¬∑ , N},
213"
LATENT DIRICHLET ALLOCATION,0.424408014571949,"‚Ä¢ Sample a topic Zn ‚àºMulti-nominal(Œ∏),
214"
LATENT DIRICHLET ALLOCATION,0.4262295081967213,"‚Ä¢ Sample a word Wn ‚àºMulti-nominal(Œ≥k),
215"
LATENT DIRICHLET ALLOCATION,0.42805100182149364,"where Dir(.) is a Dirichlet distribution.
Œ∏ is a K‚àídimensional vector that lies in the (K ‚àí
216"
LATENT DIRICHLET ALLOCATION,0.42987249544626593,"1)‚àísimplex and Œ≥k is a V ‚àídimensional vector represents the word distribution corresponding to
217"
LATENT DIRICHLET ALLOCATION,0.43169398907103823,"topic k. In the standard model, Œ±, Œ≤, K are hyper-parameters and Œ∏, Œ≥ are learnable parameters.
218"
LATENT DIRICHLET ALLOCATION,0.4335154826958106,"Throughout the experiments, the number of topics K is assumed known and fixed.
219"
LATENT DIRICHLET ALLOCATION,0.4353369763205829,"Parameter Estimation.
To test whether OTP-DAG can recover the true parameters, we generate
220"
LATENT DIRICHLET ALLOCATION,0.4371584699453552,"synthetic data in a simplified setting: the word probabilities are parameterized by a K √ó V matrix
221"
LATENT DIRICHLET ALLOCATION,0.43897996357012753,"Œ≥ where Œ≥kn := P(Wn = 1|Zn = 1); Œ≥ is now a fixed quantity to be estimated. We set Œ± = 1/K
222"
LATENT DIRICHLET ALLOCATION,0.4408014571948998,"uniformly and generate small datasets for different number of topics K and sample size N. Inspired
223"
LATENT DIRICHLET ALLOCATION,0.4426229508196721,"by the setup of [20], for every topic k, the word distribution Œ≥k can be represented as a square grid
224"
LATENT DIRICHLET ALLOCATION,0.4444444444444444,"where each cell, corresponding to a word, is assigned an integer value of either 0 and 1, indicating
225"
LATENT DIRICHLET ALLOCATION,0.44626593806921677,"whether a certain word is allocated to the kth topic or not. As a result, each topic is associated with a
226"
LATENT DIRICHLET ALLOCATION,0.44808743169398907,"specific pattern. For simplicity, we represent topics using horizontal or vertical patterns (See Figure
227"
LATENT DIRICHLET ALLOCATION,0.44990892531876137,"3). Following the above generative model, we sample 3 sets of data w.r.t 3 sets of configuration
228"
LATENT DIRICHLET ALLOCATION,0.4517304189435337,"triplets {K, M, N}: {10, 1000, 100}, {20, 5000, 200} and {30, 10000, 300}.
229"
LATENT DIRICHLET ALLOCATION,0.453551912568306,"We compare OTP-DAG with Batch EM [38] and SVI [25, 27]. For the baselines, only Œ≥ is learnable
230"
LATENT DIRICHLET ALLOCATION,0.4553734061930783,"whereas Œ± is set fixed to be uniform, whereas for our method OTP-DAG, we take on a more chal-
231"
LATENT DIRICHLET ALLOCATION,0.45719489981785066,"lenging task of learning both parameters. We report the fidelity of the estimation of Œ≥ in Table 1
232"
LATENT DIRICHLET ALLOCATION,0.45901639344262296,"wherein OTP-DAG is shown to yield estimates closest to the ground-truth values. At the same time,
233"
LATENT DIRICHLET ALLOCATION,0.46083788706739526,"our estimates for Œ± (averaged over K) are nearly 100% faithful at 0.10, 0.049, 0.033 (recall that the
234"
LATENT DIRICHLET ALLOCATION,0.46265938069216755,"ground-truth Œ± is uniform over K where K = 10, 20, 30 respectively).
235"
LATENT DIRICHLET ALLOCATION,0.4644808743169399,"Figure 3 illustrates the model topic distribution at the end of training. OTP-DAG recovers all of
236"
LATENT DIRICHLET ALLOCATION,0.4663023679417122,"the ground-truth patterns, and as further shown Figure 4, most of the patterns in fact converge well
237"
LATENT DIRICHLET ALLOCATION,0.4681238615664845,before training ends.
LATENT DIRICHLET ALLOCATION,0.46994535519125685,Ground truth OTP EM SVI
LATENT DIRICHLET ALLOCATION,0.47176684881602915,"Figure 3: The topic-word distributions recovered from each method after 300‚àíepoch training. A
grid corresponds to the word distribution of a topic. We use horizontal and vertical patterns in
different colors to distinguish topics from one another. OTP-DAG recovers all ground-truth patterns. 238"
LATENT DIRICHLET ALLOCATION,0.47358834244080145,"Topic Evaluation.
In this application, we use OTP-DAG to infer the topics of 3 real-world
239"
LATENT DIRICHLET ALLOCATION,0.47540983606557374,"datasets:2 20 News Group, BBC News and DBLP. We here revert to the original generative process
240"
LATENT DIRICHLET ALLOCATION,0.4772313296903461,"where the topic-word distribution follows a Dirichlet distribution parameterized by the concentra-
241"
LATENT DIRICHLET ALLOCATION,0.4790528233151184,"tion parameters Œ≤, instead of having Œ≥ as a fixed quantity. Œ≤ is now initialized as a matrix of real
242"
LATENT DIRICHLET ALLOCATION,0.4808743169398907,"values
 
Œ≤ ‚ààRK√óV 
representing the log concentration values. Table 2 reports the quality of the
243"
LATENT DIRICHLET ALLOCATION,0.48269581056466304,"inferred topics from OTP-DAG, in comparison with Batch EM and SVI. For every topic k, we select
244"
LATENT DIRICHLET ALLOCATION,0.48451730418943534,"top 10 most related words according to Œ≥k to represent it. Topic quality is evaluated via the diversity
245"
LATENT DIRICHLET ALLOCATION,0.48633879781420764,"and coherence of the selected words. Diversity refers to the proportion of unique words, whereas
246"
LATENT DIRICHLET ALLOCATION,0.48816029143898,"Coherence is measured with normalized pointwise mutual information [2], reflecting the extent to
247"
LATENT DIRICHLET ALLOCATION,0.4899817850637523,"which the words in a topic are associated with a common theme.
248"
LATENT DIRICHLET ALLOCATION,0.4918032786885246,2https://github.com/MIND-Lab/OCTIS.
LATENT DIRICHLET ALLOCATION,0.4936247723132969,"Table 1: Fidelity of estimates of the topic-word distribution Œ≥ across 3 settings. Fidelity is measured
via KL, JS divergence and Hellinger (HL) distance [22] with the ground-truth distributions."
LATENT DIRICHLET ALLOCATION,0.49544626593806923,"Metric
K
M
N
OTP-DAG (Ours)
Batch EM
SVI"
LATENT DIRICHLET ALLOCATION,0.4972677595628415,"KL ‚Üì
10
1, 000
100
0.90 ¬± 0.14
1.61 ¬± 0.02
1.52 ¬± 0.12
JS ‚Üì
10
1, 000
100
0.68 ¬± 0.04
0.98 ¬± 0.06
0.97 ¬± 0.09
HL ‚Üì
10
1, 000
100
2.61 ¬± 0.08
2.69 ¬± 0.03
2.71 ¬± 0.09"
LATENT DIRICHLET ALLOCATION,0.4990892531876138,"KL ‚Üì
20
5, 000
200
1.29 ¬± 0.23
2.31 ¬± 0.11
2.28 ¬± 0.04
JS ‚Üì
20
5, 000
200
1.49 ¬± 0.12
1.63 ¬± 0.06
1.61 ¬± 0.03
HL ‚Üì
20
5, 000
200
3.91 ¬± 0.03
4.26 ¬± 0.08
4.26 ¬± 0.10"
LATENT DIRICHLET ALLOCATION,0.5009107468123861,"KL ‚Üì
30
10, 000
300
1.63 ¬± 0.01
2.69 ¬± 0.07
2.66 ¬± 0.11
JS ‚Üì
30
10, 000
300
1.53 ¬± 0.01
2.03 ¬± 0.04
2.02 ¬± 0.07
HL ‚Üì
30
10, 000
300
4.98 ¬± 0.02
5.26 ¬± 0.08
5.21 ¬± 0.09"
LATENT DIRICHLET ALLOCATION,0.5027322404371585,Ground truth
LATENT DIRICHLET ALLOCATION,0.5045537340619308,Epoch 0
LATENT DIRICHLET ALLOCATION,0.5063752276867031,Epoch 100
LATENT DIRICHLET ALLOCATION,0.5081967213114754,Epoch 200
LATENT DIRICHLET ALLOCATION,0.5100182149362478,Epoch 300
LATENT DIRICHLET ALLOCATION,0.51183970856102,"Figure 4: Converging patterns of 10 ran-
dom topics from our OTP-DAG after
100, 200, 300 iterations."
LATENT DIRICHLET ALLOCATION,0.5136612021857924,"Table 2: Coherence and Diversity of the inferred
topics for the 3 real-world datasets (K = 10)"
LATENT DIRICHLET ALLOCATION,0.5154826958105647,"Metric
OTP-DAG (Ours)
Batch EM
SVI"
NEWS GROUP,0.517304189435337,20 News Group
NEWS GROUP,0.5191256830601093,"Coherence (%) ‚Üë
7.98 ¬± 0.69
6.71 ¬± 0.16
5.90 ¬± 0.51
Diversity (%) ‚Üë
75.33 ¬± 2.08
72.33 ¬± 1.15
85.33 ¬± 5.51"
NEWS GROUP,0.5209471766848816,BBC News
NEWS GROUP,0.5227686703096539,"Coherence (%) ‚Üë
9.79 ¬± 0.58
8.67 ¬± 0.62
7.84 ¬± 0.49
Diversity (%) ‚Üë
86.00 ¬± 2.89
86.00 ¬± 1.00
91.00 ¬± 2.31 DBLP"
NEWS GROUP,0.5245901639344263,"Coherence (%) ‚Üë
3.90 ¬± 0.76
4.52 ¬± 0.53
1.47 ¬± 0.39
Diversity (%) ‚Üë
84.67 ¬± 3.51
81.33 ¬± 1.15
92.67 ¬± 2.52
249"
HIDDEN MARKOV MODELS,0.5264116575591985,"4.2
Hidden Markov Models
250"
HIDDEN MARKOV MODELS,0.5282331511839709,"Poisson Time-series Data Segmentation.
This application deals with time-series data following a
251"
HIDDEN MARKOV MODELS,0.5300546448087432,"Poisson hidden Markov model (See Figure 2b). Given a time series of T steps, the task is to segment
252"
HIDDEN MARKOV MODELS,0.5318761384335154,"the data stream into K different states, each of which is associated with a Poisson observation model
253"
HIDDEN MARKOV MODELS,0.5336976320582878,"with rate Œªk. The observation at each step t is given as
254"
HIDDEN MARKOV MODELS,0.5355191256830601,"P(Xt|Zt = k) = Poi(Xt|Œªk),
for k = 1, ¬∑ ¬∑ ¬∑ , K."
HIDDEN MARKOV MODELS,0.5373406193078324,"Following [39], we use a uniform prior over the initial state. The Markov chain stays in the current
255"
HIDDEN MARKOV MODELS,0.5391621129326047,"state with probability p and otherwise transitions to one of the other K ‚àí1 states uniformly at
256"
HIDDEN MARKOV MODELS,0.5409836065573771,"random. The transition distribution is given as
257"
HIDDEN MARKOV MODELS,0.5428051001821493,"Z1 ‚àºCat
1 4, 1 4, 1 4, 1 4"
HIDDEN MARKOV MODELS,0.5446265938069217,"
,
Zt|Zt‚àí1 ‚àºCat
  p
if Zt = Zt‚àí1
1‚àíp
4‚àí1
otherwise  "
HIDDEN MARKOV MODELS,0.546448087431694,"Let P(Z1) and P(Zt|Zt‚àí1) respectively denote these prior transition distributions. We generate a
258"
HIDDEN MARKOV MODELS,0.5482695810564663,"synthetic dataset D of 200 observations at rates Œª = {12, 87, 60, 33} with change points occurring
259"
HIDDEN MARKOV MODELS,0.5500910746812386,"at times (40, 60, 55). We would like to learn the concentration parameters Œª1:K = [Œªk]K
k=1 through
260"
HIDDEN MARKOV MODELS,0.5519125683060109,"which segmentation can be realized, assuming that the number of states K = 4 is known."
HIDDEN MARKOV MODELS,0.5537340619307832,Table 3: Estimates of Œª1:4 at various transition probabilities p and L1 distance to the true values.
HIDDEN MARKOV MODELS,0.5555555555555556,"p
Œª1 = 12
Œª2 = 87
Œª3 = 60
Œª4 = 33
Œª1 = 12
Œª2 = 87
Œª3 = 60
Œª4 = 33"
HIDDEN MARKOV MODELS,0.5573770491803278,"OTP-DAG Estimates (Ours)
MAP Estimates"
HIDDEN MARKOV MODELS,0.5591985428051002,"0.05
11.83
87.20
60.61
33.40
14.88
85.22
71.42
40.39
0.15
11.62
87.04
59.69
32.85
12.31
87.11
61.86
33.90
0.35
11.77
86.76
60.01
33.26
12.08
87.28
60.44
33.17
0.55
11.76
86.98
60.15
33.38
12.05
87.12
60.12
33.01
0.75
11.63
86.46
60.04
33.57
12.05
86.96
59.98
32.94
0.95
11.57
86.92
60.36
33.06
12.05
86.92
59.94
32.93"
HIDDEN MARKOV MODELS,0.5610200364298725,"L1 ‚Üì
0.30
0.19
0.25
0.30
0.57
0.40
2.32
1.43 261"
HIDDEN MARKOV MODELS,0.5628415300546448,"Table 3 demonstrates the quality of our estimates, in comparison with MAP estimates. Our es-
262"
HIDDEN MARKOV MODELS,0.5646630236794171,"timation approaches the ground-truth values comparably to MAP. We note that the MAP solution
263"
HIDDEN MARKOV MODELS,0.5664845173041895,"requires the analytical marginal likelihood of the model, which is not necessary for our method. Fig-
264"
HIDDEN MARKOV MODELS,0.5683060109289617,"ure 5a reports the most probable state for each observation, inferred from our backward distribution
265"
HIDDEN MARKOV MODELS,0.5701275045537341,"œï(X1:T ). It can be seen that the partition overall aligns with the true generative process the data.
266"
HIDDEN MARKOV MODELS,0.5719489981785064,"0
25
50
75
100
125
150
175
200
time 20 40 60 80 100"
HIDDEN MARKOV MODELS,0.5737704918032787,latent rate
HIDDEN MARKOV MODELS,0.575591985428051,Inferred latent rate over time
HIDDEN MARKOV MODELS,0.5774134790528234,"inferred rate
observed counts"
HIDDEN MARKOV MODELS,0.5792349726775956,(a) Poisson time-series segmentation
HIDDEN MARKOV MODELS,0.581056466302368,"4
8
12
16
20
24"
HIDDEN MARKOV MODELS,0.5828779599271403,No. hidden states K 0 2 4 6 8 10
HIDDEN MARKOV MODELS,0.5846994535519126,Negative Log-Likelihood
HIDDEN MARKOV MODELS,0.5865209471766849,"OTP
MAP 5 10 15 20 25 30 35 40"
HIDDEN MARKOV MODELS,0.5883424408014571,Time (minutes)
HIDDEN MARKOV MODELS,0.5901639344262295,(b) Polyphonic music modeling
HIDDEN MARKOV MODELS,0.5919854280510018,"Figure 5: (a) Segmentation of Poisson time series inferred from the backward distribution œï(X1:T ).
(b) Training time ‚Üì(in minutes) and Negative log-likelihood ‚Üìon the test dataset at various K."
HIDDEN MARKOV MODELS,0.5938069216757741,"Polyphonic Music Modeling.
We consider another application of HMM to model sequences of
267"
HIDDEN MARKOV MODELS,0.5956284153005464,"polyphonic music. The data under analysis is the corpus of 382 harmonized chorales by J. S. Bach
268"
HIDDEN MARKOV MODELS,0.5974499089253188,"[3]. The training set consists of N = 229 sequences, each of which has a maximum length of
269"
HIDDEN MARKOV MODELS,0.599271402550091,"T = 129 and D = 51 notes. The data matrix is a Boolean tensor of size N √ó T √ó D. We follow the
270"
HIDDEN MARKOV MODELS,0.6010928961748634,"standard preprocessing where 37 redundant notes are dropped.3
271"
HIDDEN MARKOV MODELS,0.6029143897996357,"The observation at each time step is modeled using a factored observation distribution of the form
272"
HIDDEN MARKOV MODELS,0.604735883424408,"P(Xt|Zt = k) = D
Y"
HIDDEN MARKOV MODELS,0.6065573770491803,"d=1
Ber(Xtd|Bd(k)),"
HIDDEN MARKOV MODELS,0.6083788706739527,"where Bd(k) = P(Xtd = 1|Zt = k) and k = 1, ¬∑ ¬∑ ¬∑ , K. Similarly, we use a uniform prior over
273"
HIDDEN MARKOV MODELS,0.6102003642987249,"the initial state. Following [39], the transition probabilities are sampled from a Dirichlet distribution
274"
HIDDEN MARKOV MODELS,0.6120218579234973,"with concentration parameters Œ±1:K, where Œ±k = 1 if the state remains and 0.1 otherwise,
275"
HIDDEN MARKOV MODELS,0.6138433515482696,"Z1 ‚àºCat
 
1/K
	
,
Zt|Zt‚àí1 ‚àºCat
 
p

,
p ‚àºDir
 
1.0
if Zt = Zt‚àí1
0.1
otherwise  
."
HIDDEN MARKOV MODELS,0.6156648451730419,"The parameter set Œ∏ is a matrix size D √ó K where each element Œ∏ij ‚àà[0, 1] parameterizes Bdk(.).
276"
HIDDEN MARKOV MODELS,0.6174863387978142,"The goal is to learn these probabilities with underlying HMM sharing the same structure as Figure
277"
HIDDEN MARKOV MODELS,0.6193078324225865,"2b. The main difference is that the previous application only deals with one sequence, while here we
278"
HIDDEN MARKOV MODELS,0.6211293260473588,"consider a batch of sequences. For larger datasets, estimating MAP of an HMM can be expensive.
279"
HIDDEN MARKOV MODELS,0.6229508196721312,"Figure 5b reports negative log-likelihood of the learned models on the test set, along with training
280"
HIDDEN MARKOV MODELS,0.6247723132969034,"time (in minutes) at different values of K. Our fitted HMM closely approaches the level of perfor-
281"
HIDDEN MARKOV MODELS,0.6265938069216758,"mance of MAP. Both models are optimized using mini-batch gradient descent, yet OTP-DAG runs
282"
HIDDEN MARKOV MODELS,0.6284153005464481,"in constant time (approx. 3 minutes), significantly faster than solving MAP with SGD.
283"
LEARNING DISCRETE REPRESENTATIONS,0.6302367941712204,"4.3
Learning Discrete Representations
284"
LEARNING DISCRETE REPRESENTATIONS,0.6320582877959927,"Many types of data exist in the form of discrete symbols e.g., words in texts, or pixels in images.
285"
LEARNING DISCRETE REPRESENTATIONS,0.6338797814207651,"This motivates the need to explore the latent discrete representations of the data, which can be useful
286"
LEARNING DISCRETE REPRESENTATIONS,0.6357012750455373,"for planning and symbolic reasoning tasks. Viewing discrete representation learning as a parameter
287"
LEARNING DISCRETE REPRESENTATIONS,0.6375227686703097,"learning problem, we endow it with a probabilistic generative process as illustrated in Figure 2c.
288"
LEARNING DISCRETE REPRESENTATIONS,0.639344262295082,"The problem deals with a latent space C ‚ààRK√óD composed of K discrete latent sub-spaces of D
289"
LEARNING DISCRETE REPRESENTATIONS,0.6411657559198543,"dimensionality. The probability a data point belongs to a discrete sub-space c ‚àà{1, ¬∑ ¬∑ ¬∑ , K} follows
290"
LEARNING DISCRETE REPRESENTATIONS,0.6429872495446266,"a K‚àíway categorical distribution œÄ = [œÄ1, ¬∑ ¬∑ ¬∑ , œÄK]. In the language of VQ-VAE, each c is referred
291"
LEARNING DISCRETE REPRESENTATIONS,0.644808743169399,"to as a codeword and the set of codewords is called a codebook. Let Z ‚ààRD denote the latent
292"
LEARNING DISCRETE REPRESENTATIONS,0.6466302367941712,"variable in a sub-space. On each sub-space, we impose a Gaussian distribution parameterized by
293"
LEARNING DISCRETE REPRESENTATIONS,0.6484517304189436,"¬µc, Œ£c where Œ£c is diagonal. The data generative process is described as follows:
294"
LEARNING DISCRETE REPRESENTATIONS,0.6502732240437158,"1. Sample c ‚àºCat(œÄ),
295"
LEARNING DISCRETE REPRESENTATIONS,0.6520947176684881,"2. Sample Z ‚àºN(¬µc, Œ£c)
296"
LEARNING DISCRETE REPRESENTATIONS,0.6539162112932605,3https://pyro.ai/examples/hmm.html.
LEARNING DISCRETE REPRESENTATIONS,0.6557377049180327,"3. Quantize ¬µc = Q(Z),
297"
LEARNING DISCRETE REPRESENTATIONS,0.6575591985428051,"4. X = œàŒ∏(Z, ¬µc).
298"
LEARNING DISCRETE REPRESENTATIONS,0.6593806921675774,"where œà is a highly non-convex function with unknown parameters Œ∏ and often parameterized with
299"
LEARNING DISCRETE REPRESENTATIONS,0.6612021857923497,"a deep neural network. Q refers to the quantization of Z to ¬µc defined as ¬µc = Q(Z) where
300"
LEARNING DISCRETE REPRESENTATIONS,0.663023679417122,"c = argminc dz
 
Z; ¬µc

and dz =
q"
LEARNING DISCRETE REPRESENTATIONS,0.6648451730418944,"(Z ‚àí¬µc)T Œ£‚àí1
c (Z ‚àí¬µc) is the Mahalanobis distance.
301"
LEARNING DISCRETE REPRESENTATIONS,0.6666666666666666,"The goal is to learn the parameter set {œÄ, ¬µ, Œ£, Œ∏} with ¬µ = [¬µk]K
k=1, Œ£ = [Œ£k]K
k=1 such that the
302"
LEARNING DISCRETE REPRESENTATIONS,0.668488160291439,"model captures the key properties of the data. Fitting OTP-DAG to the observed data requires
303"
LEARNING DISCRETE REPRESENTATIONS,0.6703096539162113,"constructing a backward map œï : X 7‚ÜíRD from the input space back to the latent space. In
304"
LEARNING DISCRETE REPRESENTATIONS,0.6721311475409836,"connection with vector quantization, the backward map is defined via Q and an encoder fe as
305"
LEARNING DISCRETE REPRESENTATIONS,0.6739526411657559,"œï(X) =

fe(X), Q(fe(X))

,
Z = fe(X),
¬µc = Q(Z)."
LEARNING DISCRETE REPRESENTATIONS,0.6757741347905283,"Following VQ-VAE [52], our practical implementation considers Z as an M‚àícomponent latent
306"
LEARNING DISCRETE REPRESENTATIONS,0.6775956284153005,"embedding. We experiment with images in this application and compare OTP-DAG with VQ-VAE
307"
LEARNING DISCRETE REPRESENTATIONS,0.6794171220400729,"on 3 popular datasets: CIFAR10, MNIST and SVHN. Since the true parameters are unknown, we
308"
LEARNING DISCRETE REPRESENTATIONS,0.6812386156648452,"assess how well the latent space characterizes the input data through the quality of the reconstruction
309"
LEARNING DISCRETE REPRESENTATIONS,0.6830601092896175,"of the original images. Our analysis considers various metrics measuring the difference/similarity
310"
LEARNING DISCRETE REPRESENTATIONS,0.6848816029143898,"between the two images on patch (SSIM), pixel (PSNR), feature (LPIPS) and dataset (FID) levels.
311"
LEARNING DISCRETE REPRESENTATIONS,0.6867030965391621,"We also compute Perplexity to evaluate the degree to which the latent representations Z spread
312"
LEARNING DISCRETE REPRESENTATIONS,0.6885245901639344,"uniformly over K sub-spaces. Table 4 reports our superior performance in preserving high-quality
313"
LEARNING DISCRETE REPRESENTATIONS,0.6903460837887068,"information of the input images. VQ-VAE suffers from poorer performance mainly due to an issue
314"
LEARNING DISCRETE REPRESENTATIONS,0.692167577413479,"called codebook collapse [59] where most of latent vectors are quantized to few discrete codewords,
315"
LEARNING DISCRETE REPRESENTATIONS,0.6939890710382514,"while the others are left vacant. Meanwhile, our framework allows for control over the number of
316"
LEARNING DISCRETE REPRESENTATIONS,0.6958105646630237,"latent representations assigned to each codeword through learning œÄ, ensuring all codewords are
317"
LEARNING DISCRETE REPRESENTATIONS,0.697632058287796,"utilized. See Appendix C.3 for detailed formulation and qualitative examples.
318"
LEARNING DISCRETE REPRESENTATIONS,0.6994535519125683,Table 4: Quality of the image reconstructions (K = 512).
LEARNING DISCRETE REPRESENTATIONS,0.7012750455373407,"Dataset
Method
Latent Size
SSIM ‚Üë
PSNR ‚Üë
LPIPS ‚Üì
rFID ‚Üì
Perplexity ‚Üë"
LEARNING DISCRETE REPRESENTATIONS,0.7030965391621129,"CIFAR10
VQ-VAE
8 √ó 8
0.70
23.14
0.35
77.3
69.8
OTP-DAG (Ours)
8 √ó 8
0.80
25.40
0.23
56.5
498.6"
LEARNING DISCRETE REPRESENTATIONS,0.7049180327868853,"MNIST
VQ-VAE
8 √ó 8
0.98
33.37
0.02
4.8
47.2
OTP-DAG (Ours)
8 √ó 8
0.98
33.62
0.01
3.3
474.6"
LEARNING DISCRETE REPRESENTATIONS,0.7067395264116576,"SVHN
VQ-VAE
8 √ó 8
0.88
26.94
0.17
38.5
114.6
OTP-DAG (Ours)
8 √ó 8
0.94
32.56
0.08
25.2
462.8"
LIMITATIONS,0.7085610200364298,"5
Limitations
319"
LIMITATIONS,0.7103825136612022,"Our framework employs amortized optimization that requires continuous relaxation or reparameter-
320"
LIMITATIONS,0.7122040072859745,"ization of the underlying model distribution to ensure the gradients can be back-propagated effec-
321"
LIMITATIONS,0.7140255009107468,"tively. For discrete distributions and for some continuous ones (e.g., Gamma distribution), this is not
322"
LIMITATIONS,0.7158469945355191,"easy to attain. To this end, a recent proposal on Generalized Reparameterization Gradient [46] is
323"
LIMITATIONS,0.7176684881602914,"a viable solution. OTP-DAG also relies on the expressivity of the backward maps. Since our back-
324"
LIMITATIONS,0.7194899817850637,"ward mapping only considers local dependencies, it is however simpler to find a good approximation
325"
LIMITATIONS,0.7213114754098361,"compared to VI where the variational approximator should ideally characterize the entire global de-
326"
LIMITATIONS,0.7231329690346083,"pendencies in the graph. We use neural networks to model the backward conditionals. With enough
327"
LIMITATIONS,0.7249544626593807,"data, network complexity, and training time, the difference between the modeled distribution and
328"
LIMITATIONS,0.726775956284153,"the true conditional can be assumed to be smaller than an arbitrary constant œµ based on the universal
329"
LIMITATIONS,0.7285974499089253,"approximation theorem [28].
330"
CONCLUSION AND FUTURE WORK,0.7304189435336976,"6
Conclusion and Future Work
331"
CONCLUSION AND FUTURE WORK,0.73224043715847,"This paper contributes a novel approach based on optimal transport to learning parameters of di-
332"
CONCLUSION AND FUTURE WORK,0.7340619307832422,"rected graphical models. The proposed algorithm OTP-DAG is general and applicable to any di-
333"
CONCLUSION AND FUTURE WORK,0.7358834244080146,"rected graph with latent variables regardless of variable types and structural dependencies. As for
334"
CONCLUSION AND FUTURE WORK,0.7377049180327869,"future research, this new perspective opens up promising avenues, for instance applying OTP-DAG
335"
CONCLUSION AND FUTURE WORK,0.7395264116575592,"to structural learning problems where edge existence and directionality can be parameterized for
336"
CONCLUSION AND FUTURE WORK,0.7413479052823315,"continuous optimization, or extending it to learning undirected graphical models.
337"
REFERENCES,0.7431693989071039,"References
338"
REFERENCES,0.7449908925318761,"[1] Jonas Adler and Sebastian Lunz. Banach wasserstein gan. Advances in neural information
339"
REFERENCES,0.7468123861566485,"processing systems, 31, 2018. 2
340"
REFERENCES,0.7486338797814208,"[2] Nikolaos Aletras and Mark Stevenson. Evaluating topic coherence using distributional seman-
341"
REFERENCES,0.7504553734061931,"tics. In Proceedings of the 10th international conference on computational semantics (IWCS
342"
REFERENCES,0.7522768670309654,"2013)‚ÄìLong Papers, pages 13‚Äì22, 2013. 6
343"
REFERENCES,0.7540983606557377,"[3] Moray Allan and Christopher Williams. Harmonising chorales by probabilistic inference. Ad-
344"
REFERENCES,0.75591985428051,"vances in neural information processing systems, 17, 2004. 8
345"
REFERENCES,0.7577413479052824,"[4] Luca Ambrogioni, Umut G√º√ßl√º, Yagmur G√º√ßl√ºt√ºrk, Max Hinne, Marcel A.J. Van Gerven, and
346"
REFERENCES,0.7595628415300546,"Eric Maris. Wasserstein variational inference. Advances in Neural Information Processing
347"
REFERENCES,0.761384335154827,"Systems, 2018-December(NeurIPS):2473‚Äì2482, 2018. 2
348"
REFERENCES,0.7632058287795993,"[5] Luca Ambrogioni, Kate Lin, Emily Fertig, Sharad Vikram, Max Hinne, Dave Moore, and
349"
REFERENCES,0.7650273224043715,"Marcel van Gerven. Automatic structured variational inference. In Arindam Banerjee and
350"
REFERENCES,0.7668488160291439,"Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artificial In-
351"
REFERENCES,0.7686703096539163,"telligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages
352"
REFERENCES,0.7704918032786885,"676‚Äì684. PMLR, 13‚Äì15 Apr 2021. 1, 3
353"
REFERENCES,0.7723132969034608,"[6] Brandon Amos. Tutorial on amortized optimization for learning to optimize over continuous
354"
REFERENCES,0.7741347905282332,"domains. arXiv preprint arXiv:2202.00665, 2022. 5
355"
REFERENCES,0.7759562841530054,"[7] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Ten-
356"
REFERENCES,0.7777777777777778,"sor decompositions for learning latent variable models. Journal of machine learning research,
357"
REFERENCES,0.7795992714025501,"15:2773‚Äì2832, 2014. 3
358"
REFERENCES,0.7814207650273224,"[8] Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mix-
359"
REFERENCES,0.7832422586520947,"ture models and hidden markov models.
In Conference on Learning Theory, pages 33‚Äì1.
360"
REFERENCES,0.785063752276867,"JMLR Workshop and Conference Proceedings, 2012. 3
361"
REFERENCES,0.7868852459016393,"[9] Martin Arjovsky, Soumith Chintala, and L√©on Bottou. Wasserstein generative adversarial net-
362"
REFERENCES,0.7887067395264117,"works. In International conference on machine learning, pages 214‚Äì223. PMLR, 2017. 2
363"
REFERENCES,0.7905282331511839,"[10] Matthew J Beal and Zoubin Ghahramani. Variational bayesian learning of directed graphical
364"
REFERENCES,0.7923497267759563,"models with hidden variables. 2006. 3
365"
REFERENCES,0.7941712204007286,"[11] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning,
366"
REFERENCES,0.7959927140255009,"volume 4. Springer, 2006. 3
367"
REFERENCES,0.7978142076502732,"[12] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of
368"
REFERENCES,0.7996357012750456,"machine Learning research, 3(Jan):993‚Äì1022, 2003. 5
369"
REFERENCES,0.8014571948998178,"[13] Olivier Capp√© and Eric Moulines.
On-line expectation‚Äìmaximization algorithm for latent
370"
REFERENCES,0.8032786885245902,"data models.
Journal of the Royal Statistical Society: Series B (Statistical Methodology),
371"
REFERENCES,0.8051001821493625,"71(3):593‚Äì613, 2009. 3
372"
REFERENCES,0.8069216757741348,"[14] Bernard Delyon, Marc Lavielle, and Eric Moulines. Convergence of a stochastic approximation
373"
REFERENCES,0.8087431693989071,"version of the em algorithm. Annals of statistics, pages 94‚Äì128, 1999. 3
374"
REFERENCES,0.8105646630236795,"[15] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data
375"
REFERENCES,0.8123861566484517,"Via the EM Algorithm . Journal of the Royal Statistical Society: Series B (Methodological),
376"
REFERENCES,0.8142076502732241,"39(1):1‚Äì22, 1977. 1
377"
REFERENCES,0.8160291438979964,"[16] Nick Foti, Jason Xu, Dillon Laird, and Emily Fox. Stochastic variational inference for hidden
378"
REFERENCES,0.8178506375227687,"markov models. Advances in neural information processing systems, 27, 2014. 3
379"
REFERENCES,0.819672131147541,"[17] Tomas Geffner, Javier Antoran, Adam Foster, Wenbo Gong, Chao Ma, Emre Kiciman, Amit
380"
REFERENCES,0.8214936247723132,"Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, et al. Deep end-to-end causal inference.
381"
REFERENCES,0.8233151183970856,"arXiv preprint arXiv:2202.02195, 2022. 3
382"
REFERENCES,0.825136612021858,"[18] Alan E Gelfand and Adrian FM Smith. Sampling-based approaches to calculating marginal
383"
REFERENCES,0.8269581056466302,"densities. Journal of the American statistical association, 85(410):398‚Äì409, 1990. 3
384"
REFERENCES,0.8287795992714025,"[19] Walter R Gilks, Sylvia Richardson, and David Spiegelhalter. Markov chain Monte Carlo in
385"
REFERENCES,0.8306010928961749,"practice. CRC press, 1995. 3
386"
REFERENCES,0.8324225865209471,"[20] Thomas L Griffiths and Mark Steyvers. Finding scientific topics. Proceedings of the National
387"
REFERENCES,0.8342440801457195,"academy of Sciences, 101(suppl_1):5228‚Äì5235, 2004. 6
388"
REFERENCES,0.8360655737704918,"[21] John Hammersley. Monte carlo methods. Springer Science & Business Media, 2013. 3
389"
REFERENCES,0.8378870673952641,"[22] Ernst Hellinger.
Neue begr√ºndung der theorie quadratischer formen von unendlichvielen
390"
REFERENCES,0.8397085610200364,"ver√§nderlichen. Journal f√ºr die reine und angewandte Mathematik, 1909(136):210‚Äì271, 1909.
391 7
392"
REFERENCES,0.8415300546448088,"[23] James Hensman, Magnus Rattray, and Neil Lawrence. Fast variational inference in the conju-
393"
REFERENCES,0.843351548269581,"gate exponential family. Advances in neural information processing systems, 25, 2012. 3
394"
REFERENCES,0.8451730418943534,"[24] Jose Hernandez-Lobato, Yingzhen Li, Mark Rowland, Thang Bui, Daniel Hern√°ndez-Lobato,
395"
REFERENCES,0.8469945355191257,"and Richard Turner. Black-box alpha divergence minimization. In International conference on
396"
REFERENCES,0.848816029143898,"machine learning, pages 1511‚Äì1520. PMLR, 2016. 3
397"
REFERENCES,0.8506375227686703,"[25] Matthew Hoffman, Francis Bach, and David Blei. Online learning for latent dirichlet alloca-
398"
REFERENCES,0.8524590163934426,"tion. advances in neural information processing systems, 23, 2010. 6
399"
REFERENCES,0.8542805100182149,"[26] Matthew D Hoffman and David M Blei. Structured stochastic variational inference. In Artifi-
400"
REFERENCES,0.8561020036429873,"cial Intelligence and Statistics, pages 361‚Äì369, 2015. 3
401"
REFERENCES,0.8579234972677595,"[27] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational
402"
REFERENCES,0.8597449908925319,"inference. Journal of Machine Learning Research, 2013. 1, 3, 6
403"
REFERENCES,0.8615664845173042,"[28] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
404"
REFERENCES,0.8633879781420765,"universal approximators. Neural networks, 2(5):359‚Äì366, 1989. 9
405"
REFERENCES,0.8652094717668488,"[29] Matthew Johnson and Alan Willsky. Stochastic variational inference for bayesian time series
406"
REFERENCES,0.8670309653916212,"models. In International Conference on Machine Learning, pages 1854‚Äì1862. PMLR, 2014.
407 3
408"
REFERENCES,0.8688524590163934,"[30] Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An intro-
409"
REFERENCES,0.8706739526411658,"duction to variational methods for graphical models. Machine learning, 37:183‚Äì233, 1999. 1,
410 2
411"
REFERENCES,0.8724954462659381,"[31] Leonid V Kantorovich. Mathematical methods of organizing and planning production. Man-
412"
REFERENCES,0.8743169398907104,"agement science, 6(4):366‚Äì422, 1960. 2
413"
REFERENCES,0.8761384335154827,"[32] Nathaniel J King and Neil D Lawrence. Fast variational inference for gaussian process models
414"
REFERENCES,0.8779599271402551,"through kl-correction. In Machine Learning: ECML 2006: 17th European Conference on
415"
REFERENCES,0.8797814207650273,"Machine Learning Berlin, Germany, September 18-22, 2006 Proceedings 17, pages 270‚Äì281.
416"
REFERENCES,0.8816029143897997,"Springer, 2006. 3
417"
REFERENCES,0.8834244080145719,"[33] Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
418"
REFERENCES,0.8852459016393442,"Improved variational inference with inverse autoregressive flow. Advances in neural informa-
419"
REFERENCES,0.8870673952641166,"tion processing systems, 29, 2016. 3
420"
REFERENCES,0.8888888888888888,"[34] Miguel L√°zaro-Gredilla, Steven Van Vaerenbergh, and Neil D Lawrence. Overlapping mixtures
421"
REFERENCES,0.8907103825136612,"of gaussian processes for the data association problem. Pattern recognition, 45(4):1386‚Äì1395,
422"
REFERENCES,0.8925318761384335,"2012. 3
423"
REFERENCES,0.8943533697632058,"[35] Yingzhen Li and Richard E Turner. R√©nyi divergence variational inference. Advances in neural
424"
REFERENCES,0.8961748633879781,"information processing systems, 29, 2016. 3
425"
REFERENCES,0.8979963570127505,"[36] Percy Liang and Dan Klein. Online em for unsupervised models. In Proceedings of human
426"
REFERENCES,0.8998178506375227,"language technologies: The 2009 annual conference of the North American chapter of the
427"
REFERENCES,0.9016393442622951,"association for computational linguistics, pages 611‚Äì619, 2009. 3
428"
REFERENCES,0.9034608378870674,"[37] Dmitry Molchanov, Valery Kharitonov, Artem Sobolev, and Dmitry Vetrov. Doubly semi-
429"
REFERENCES,0.9052823315118397,"implicit variational inference. In The 22nd International Conference on Artificial Intelligence
430"
REFERENCES,0.907103825136612,"and Statistics, pages 2593‚Äì2602. PMLR, 2019. 3
431"
REFERENCES,0.9089253187613844,"[38] Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine,
432"
REFERENCES,0.9107468123861566,"13(6):47‚Äì60, 1996. 2, 6
433"
REFERENCES,0.912568306010929,"[39] Kevin P Murphy. Probabilistic machine learning: Advanced topics. MIT Press, 2023. 7, 8
434"
REFERENCES,0.9143897996357013,"[40] Radford M Neal and Geoffrey E Hinton. A view of the em algorithm that justifies incremental,
435"
REFERENCES,0.9162112932604736,"sparse, and other variants. Learning in graphical models, pages 355‚Äì368, 1998. 3
436"
REFERENCES,0.9180327868852459,"[41] Ronald C Neath et al. On convergence properties of the monte carlo em algorithm. Advances
437"
REFERENCES,0.9198542805100182,"in modern statistical theory and applications: a Festschrift in Honor of Morris L. Eaton, pages
438"
REFERENCES,0.9216757741347905,"43‚Äì62, 2013. 3
439"
REFERENCES,0.9234972677595629,"[42] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
440"
REFERENCES,0.9253187613843351,"Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of
441"
REFERENCES,0.9271402550091075,"Machine Learning Research, 22:1‚Äì64, 2021. 1
442"
REFERENCES,0.9289617486338798,"[43] Gabriel Peyr√©, Marco Cuturi, et al. Computational optimal transport. Center for Research in
443"
REFERENCES,0.930783242258652,"Economics and Statistics Working Papers, (2017-86), 2017. 2
444"
REFERENCES,0.9326047358834244,"[44] Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial
445"
REFERENCES,0.9344262295081968,"intelligence and statistics, pages 814‚Äì822. PMLR, 2014. 1, 3
446"
REFERENCES,0.936247723132969,"[45] Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In Interna-
447"
REFERENCES,0.9380692167577414,"tional conference on machine learning, pages 324‚Äì333. PMLR, 2016. 1, 3, 5
448"
REFERENCES,0.9398907103825137,"[46] Francisco R Ruiz, Titsias RC AUEB, David Blei, et al. The generalized reparameterization
449"
REFERENCES,0.941712204007286,"gradient. Advances in neural information processing systems, 29, 2016. 9
450"
REFERENCES,0.9435336976320583,"[47] Lawrence Saul and Michael Jordan. Exploiting tractable substructures in intractable networks.
451"
REFERENCES,0.9453551912568307,"Advances in neural information processing systems, 8, 1995. 3
452"
REFERENCES,0.9471766848816029,"[48] Yee Teh, David Newman, and Max Welling. A collapsed variational bayesian inference algo-
453"
REFERENCES,0.9489981785063752,"rithm for latent dirichlet allocation. Advances in neural information processing systems, 19,
454"
REFERENCES,0.9508196721311475,"2006. 3
455"
REFERENCES,0.9526411657559198,"[49] Michalis K Titsias and Francisco Ruiz. Unbiased implicit variational inference. In The 22nd In-
456"
REFERENCES,0.9544626593806922,"ternational Conference on Artificial Intelligence and Statistics, pages 167‚Äì176. PMLR, 2019.
457 3
458"
REFERENCES,0.9562841530054644,"[50] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
459"
REFERENCES,0.9581056466302368,"encoders. arXiv preprint arXiv:1711.01558, 2017. 2
460"
REFERENCES,0.9599271402550091,"[51] Dustin Tran, David Blei, and Edo M Airoldi. Copula variational inference. Advances in neural
461"
REFERENCES,0.9617486338797814,"information processing systems, 28, 2015. 3
462"
REFERENCES,0.9635701275045537,"[52] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances
463"
REFERENCES,0.9653916211293261,"in neural information processing systems, 30, 2017. 5, 9
464"
REFERENCES,0.9672131147540983,"[53] C√©dric Villani. Topics in optimal transportation, volume 58. AMS Graduate Studies in Math-
465"
REFERENCES,0.9690346083788707,"ematics, 2003. 3
466"
REFERENCES,0.970856102003643,"[54] C√©dric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009. 2
467"
REFERENCES,0.9726775956284153,"[55] Neng Wan, Dapeng Li, and Naira Hovakimyan. F-divergence variational inference. Advances
468"
REFERENCES,0.9744990892531876,"in neural information processing systems, 33:17370‚Äì17379, 2020. 3
469"
REFERENCES,0.97632058287796,"[56] Greg CG Wei and Martin A Tanner. A monte carlo implementation of the em algorithm and
470"
REFERENCES,0.9781420765027322,"the poor man‚Äôs data augmentation algorithms. Journal of the American statistical Association,
471"
REFERENCES,0.9799635701275046,"85(411):699‚Äì704, 1990. 3
472"
REFERENCES,0.9817850637522769,"[57] Ming Xu, Matias Quiroz, Robert Kohn, and Scott A Sisson. Variance reduction properties of
473"
REFERENCES,0.9836065573770492,"the reparameterization trick. In The 22nd International Conference on Artificial Intelligence
474"
REFERENCES,0.9854280510018215,"and Statistics, pages 2711‚Äì2720. PMLR, 2019. 3
475"
REFERENCES,0.9872495446265938,"[58] Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In International
476"
REFERENCES,0.9890710382513661,"Conference on Machine Learning, pages 5660‚Äì5669. PMLR, 2018. 3
477"
REFERENCES,0.9908925318761385,"[59] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku,
478"
REFERENCES,0.9927140255009107,"Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with
479"
REFERENCES,0.994535519125683,"improved vqgan. In International Conference on Learning Representations. 9
480"
REFERENCES,0.9963570127504554,"[60] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural
481"
REFERENCES,0.9981785063752276,"networks. In International Conference on Machine Learning, pages 7154‚Äì7163. PMLR, 2019.
482 3
483"
