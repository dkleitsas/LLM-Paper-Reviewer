Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022172949002217295,"Although recent advances in higher-order Graph Neural Networks (GNNs) im-
prove the theoretical expressivity and molecular property predictive performance,
they often fall short of the empirical performance of models that explicitly use
fragment information as inductive bias. However, for these approaches, there
exists no theoretic expressivity study. In this work, we propose the Fragment-
WL test, an extension to the well-known Weisfeiler & Leman (WL) test, which
enables the theoretic analysis of these fragment-biased GNNs. Building on the
insights gained from the Fragment-WL test, we develop a new GNN architecture
and a fragmentation with infinite vocabulary that significantly boosts expressive-
ness. We show the effectiveness of our model on synthetic and real-world data
where we outperform all GNNs on Peptides and have 12% lower error than all
GNNs on ZINC and 34% lower error than other fragment-biased models. Fur-
thermore, we show that our model exhibits superior generalization capabilities
compared to the latest transformer-based architectures, positioning it as a robust
solution for a range of molecular modeling tasks."
INTRODUCTION,0.004434589800443459,"1
Introduction"
INTRODUCTION,0.0066518847006651885,"A common issue with Graph Neural Networks (GNNs) is their lack of expressiveness, including
their inability to recognize substructures, which could limit their empirical performance [40]. In
machine learning for chemistry, frequently occurring substructures, or fragments, are key predictors
of molecular properties [39]. These fragments become even more crucial in larger systems like
proteins [44]."
INTRODUCTION,0.008869179600886918,"To address this, recent methods enhance GNNs by improving their ability to distinguish non-
isomorphic graphs, with the Weisfeiler & Leman (WL) test used to meassure expressiveness [52; 40].
However, these approaches often emphasize theoretical expressiveness over practical performance
and suffer from poor generalization to data that does not perfectly fit the training distribution[6].
Fragment-biased GNNs which incorporate fragment information directly as inductive biases tend to
perform better but typically lack theoretical analysis [17; 51]."
INTRODUCTION,0.011086474501108648,"In this work, we bridge the gap between theory and practical performance by introducing the
Fragment-WL test, which extends the standard WL test to analyze fragment-biased models. We also
propose a new model that integrates a general fragmentation within the message-passing framework,
improving generalization to out-of-distribution data. Our model allows for a novel fragmentation of
molecular graphs with an infinite vocabulary composed of basic building blocks. We demonstrate
state-of-the-art performance across various molecular datasets and tasks, including outperforming
transformer-based architectures in some cases."
INTRODUCTION,0.013303769401330377,∗Equal contribution.
INTRODUCTION,0.015521064301552107,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
INTRODUCTION,0.017738359201773836,Our core contributions are as follows:
INTRODUCTION,0.019955654101995565,"• We provide a more fine-grained hierarchy on the ex- pressivity for a multitude of models that
incorporate substructures as inductive bias.
• We propose a new general architecture that performs message passing along substructures
together with a new fragmentation for molecules, achieving highest expressivity.
• We evaluate predictive power, long-range performance, and generalization across extensive
experiments."
LIT REVIEW,0.022172949002217297,"2
Weisfeiler & Lehman Go Fragments"
LIT REVIEW,0.024390243902439025,"Existing fragment-biased GNNs vary not only in their vocabulary (what substructures are considered)
but also in how fragmentation information is integrated into the model (see Related Work in Ap-
pendix B). Approaches range from including fragment information as node features (NF) [4], learning
an explicit represantation for each fragment (FR) that exchanges messages with the underlying nodes
[51; 54] or, building a higher-level graph (HLG) on which neighboring fragments can exchange
additional messages [17]. This variability makes a direct comparison of the expressiveness of these
models challenging. To address this, we develop in Appendix C new variants of the Weisfeiler &
Lehman (WL) test (NF-WL, FR-WL, HLG-WL) that model these different approaches to incorporate
fragment information. Our Fragment-WL test framework allows us to bound the expressivity of most
existing fragment-biased models (see Table 2 in Appendix C) and proves that the expressivity strictly
increases from NF to FR to HLG approaches (see Theorems C.6 to C.8 in Appendix C). Formally,
we establish the following hierarchy:"
LIT REVIEW,0.026607538802660754,"2-WL < NF-WL < FR-WL < HLG-WL.
(1)"
LIT REVIEW,0.028824833702882482,"This hierarchy demonstrates that the method of incorporating fragment information significantly
impacts model expressivity, with higher-level abstractions yielding more powerful models."
IMPLEMENTATION/METHODS,0.031042128603104215,"3
Fragment Graph Neural Network"
IMPLEMENTATION/METHODS,0.03325942350332594,"Based on the insights of the previous section, we propose our new model architecture and a new
fragmentation scheme with infinite vocabulary consisting of only basic building blocks. Given the
higher expressiveness, our model can differentiate complex substructures given only these basic
building blocks, as it is able to learn the dependencies on the higher-level graph. We empirically
confirm this in Section 4."
IMPLEMENTATION/METHODS,0.03547671840354767,"3.1
Model"
IMPLEMENTATION/METHODS,0.037694013303769404,"Building on the theoretical findings that a higher-level graph (see Theorem C.8) improves expressive-
ness, we propose FragNet, a general model for any fragmentation F that performs message-passing
on the original graph and a higher-level graph of fragments, i.e., the HLG approach. Correspondingly,
we have learned representations for every node v ∈V and every fragment f ∈F. Conceptually, each
node v receives messages from all its neighbors, and all fragment of which it is part of. Similarly,
each fragment f receives messages from neighboring fragments and all nodes that are part of it. The
aggregated messages together with the previous representation are then used to update the representa-
tion. The message passing scheme is illustrated in Figure 1 and precisely defined in Appendix E. The
final graph-level output is computed by aggregating all representations after T layers. Note that the
complexity of our FragNet model is linear in the number of nodes and fragments (assuming that each
node is only part of a constant number of fragments)."
IMPLEMENTATION/METHODS,0.03991130820399113,"Additionally, our FragNet model achieves the highest expressiveness in our Fragment-WL hierarchy
and also compared to other fragment-biased GNNs.
Theorem 3.1. FragNets are at most as powerful as HLG-WL. Additionally, when using injective
neighborhood aggregators and a sufficient number of layers, FragNets are as powerful as HLG-WL."
IMPLEMENTATION/METHODS,0.04212860310421286,"3.2
Molecular fragmentation"
IMPLEMENTATION/METHODS,0.04434589800443459,"Apart from the question of how to use a fragmentation, there is the equally important question of how
to fragment the graph in the first place. The challenge of fragmentation lies in balancing two goals:"
IMPLEMENTATION/METHODS,0.04656319290465632,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
IMPLEMENTATION/METHODS,0.04878048780487805,"Figure 1: Overview of our model and our frag-
mentation. The molecular graph is fragmented
with our rings-paths fragmentation into three cy-
cles, three paths, and a junction node.The figure
shows the messages mt
F →f, mt
V→f to one frag-
ment f, and the messages mt
V→v, mt
F →v to one
vertex v."
IMPLEMENTATION/METHODS,0.050997782705099776,"Figure 2: Ordinal encoding applied to a 2-path,
4-path, 5-ring, and 6-ring. The encoding com-
prises two components: one learned embedding
e for every fragment class (i.e., path, cycle, or
junction) and another learned embedding s that
is proportionally scaled based on the fragment
size."
IMPLEMENTATION/METHODS,0.05321507760532151,"1. Capture all important substructures.
2. Facilitate generalization."
IMPLEMENTATION/METHODS,0.05543237250554324,"A fragmentation that is too coarse may miss key features, while too fine-grained risks overfitting
and hampers finding graph similarities. Obviously, the optimal scheme depends on the application.
Existing methods for molecules focus either on a single substructure [54] or use chemical properties
that require large vocabularies [10]."
IMPLEMENTATION/METHODS,0.057649667405764965,"Our approach completely fragments molecular graphs using only rings and paths. First, minimal
rings are extracted. Next, the remaining edges are connected at nodes of degree two to form paths.
Lastly, junction nodes are introduced where three or more fragments meet, reducing cycles in the
higher-level graph. Figure 1 illustrates this process."
IMPLEMENTATION/METHODS,0.0598669623059867,"Ordinal encoding.
Previous works either use no encoding for the types of fragments [17] or a
simple one-hot encoding [4]. However, to facilitate the generalization capabilities of a model, the
encodings of similar fragments should also be similar. In our approach, we introduce an ordinal
fragment encoding, which accomplishes this by incorporating two embeddings: one for the fragment
class (i.e., class(f) ∈{path, cycle, junction}) and another that is proportionally scaled based on the
fragment size; see Figure 2. More formally:"
IMPLEMENTATION/METHODS,0.06208425720620843,"h0
f = (eclass(f), |f| · sclass(f)),"
IMPLEMENTATION/METHODS,0.06430155210643015,"where s and e are different learned embeddings for the classes of cycle, path, and junction fragments.
This approach enables the encoding of an infinite vocabulary, accommodating even completely unseen
fragments while concurrently supporting effective model generalization."
RESULTS/EXPERIMENTS,0.06651884700665188,"4
Results"
RESULTS/EXPERIMENTS,0.06873614190687362,"While we have theoretically demonstrated that our model attains the highest expressiveness within
our Fragment-WL hierarchy, we also empirically evaluate its expressiveness by examining its ability
to count substructures. Additionally, we explore its overall predictive effectiveness, and its capacity
to generalize. Additional experiments in Appendix F demonstrate FragNet’s improved capability for
long-range communication."
RESULTS/EXPERIMENTS,0.07095343680709534,"Expressiveness. To evaluate how well our model can learn to recognize chemically important
substructures in molecular graphs, we first identify the most common substructures in the ZINC
10k dataset [22] using a chemically-inspired fragmentation scheme, specifically MAGNet [24].
Subsequently, we train our model to predict substructure counts. Our model is able to identify all
substructures close to perfection as demonstrated in Table 10 in Appendix F. Notably, our model
achieves high accuracy even for intricate substructures not present in our vocabulary. This underscores
that our fragmentation, based solely on rings and paths, together with our ordinal encoding and the
higher-level message passing, proves sufficient for the model to recognize more complex substructures."
RESULTS/EXPERIMENTS,0.07317073170731707,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
RESULTS/EXPERIMENTS,0.07538802660753881,"Type
Model
Peptides-
ZINC"
RESULTS/EXPERIMENTS,0.07760532150776053,"Struct
Func
10k
Full
(MAE ↓)
(AP ↑)
(MAE ↓)
(MAE ↓)"
RESULTS/EXPERIMENTS,0.07982261640798226,"Transformer
GPS
0.2500 ± 0.0012
0.6535 ± 0.0041
0.070 ± 0.006
-
GRIT
0.2460 ± 0.0012
0.6988 ± 0.0082
0.059 ± 0.002
0.023 ± 0.001"
RESULTS/EXPERIMENTS,0.082039911308204,"Basic GNNs
GCN
0.3496 ± 0.0013
0.5930 ± 0.0023
0.367 ± 0.011
0.113 ± 0.002
GIN
0.3547 ± 0.0045
0.5498 ± 0.0079
0.526 ± 0.051
0.088 ± 0.002"
RESULTS/EXPERIMENTS,0.08425720620842572,"Topological
CIN++
0.2523 ± 0.0013
0.6569 ± 0.0117
0.077 ± 0.004
0.027 ± 0.007"
RESULTS/EXPERIMENTS,0.08647450110864745,"Fragment-Biased
HIMP
0.2503 ± 0.0008
0.5668 ± 0.0149
0.151 ± 0.006
0.036 ± 0.002
FragNet (ours)
0.2462 ± 0.0021
0.6678 ± 0.005
0.0775 ± 0.005
0.0237 ± 0.00"
RESULTS/EXPERIMENTS,0.08869179600886919,"Table 1: Predictive performance for multiple models on Peptides-struct/-func and ZINC. Best
Transformer and best GNN are highlighted. A comparison with more models is in Tables 7 and 8."
RESULTS/EXPERIMENTS,0.09090909090909091,"This is essential for application in, e.g., fragment-based molecule generation, as the task of the encoder
is to encode information about such substructures."
RESULTS/EXPERIMENTS,0.09312638580931264,"Predictive Performance. To evaluate the predictive performance on real-world molecular dataset, we
use the long-range peptides benchmark [14] and the large-scale molecular benchmark ZINC [46]. The
Peptides-struct and Peptides-func datasets are commonly used to benchmark long-range performance
of GNNs and transformers. The task in the ZINC dataset is to predict the penalized logP of molecules,
a measure of drug-likeness. A summary of all models we compare against, the used hyperparameters
of our model, the used datasets and the experimental details for each experiment can be found in
Appendix G. All our models adhere to the 500k parameter budget for both datasets. We do not use
any additional feature augmentation, such as positional encodings. As shown in Section 4 , our model
achieves state-of-the-art performance among GNNs on all datasets, additionally surpassing nearly
all graph transformers, which typically excel in long-range tasks due to their quadratic complexity.
GRIT [36] is the only model that consistently outperforms ours."
RESULTS/EXPERIMENTS,0.09534368070953436,"Generalization. To test the generalization capabilities of our model with the ordinal fragment
encoding, we use a test set containing out-of-distribution molecules with completely unseen fragments.
For this, we use the ZINC dataset and remove all molecules containing a 7-ring from the training
data. After training, we test on all molecules from the test set, thus also containing 7-rings that were
not seen during training. The results in Table 11 demonstrate that our model achieves an error 1.8
times lower than GRIT, showcasing the superior generalization capabilities. Our better generalization
capabilities can also be seen in the normal ZINC benchmark. In Table 12, we group the ZINC dataset
into groups based on the frequency of the rarest fragment. Our model outperforms HIMP everywhere
and GRIT for graphs containing rare fragments while GRIT shows better perfromance for molecules
containing frequent fragments. Lastly, we test our model’s capability to transfer the knowledge
to a completely different dataset. We train on ZINC and predict the penalized logP on QM9 [48].
FragNet achieves the lowest MAE of 1.12, outperforming GRIT (MAE 1.22) and HIMP (MAE 3.43),
suggesting that our model generalizes better to unseen data distributions due to our inductive bias
and corresponding fragmentation. In summary, we showcased the generalization capabilities of our
model on both a completely unseen dataset and a slightly shifted data distribution. The generalization
capabilities also help our model perform better on rare fragments."
CONCLUSION/DISCUSSION ,0.0975609756097561,"5
Conclusion and Limitations"
CONCLUSION/DISCUSSION ,0.09977827050997783,"In this work, we introduced the Fragment-WL test, a new expressivity measure that provides a
hierarchy for fragment-biased GNNs. Using this framework, we developed an expressive model
that outperforms all GNN approaches and most transformer architectures on molecular property
prediction benchmarks. Our model demonstrates strong generalization capabilities with linear
complexity, making it a robust solution for molecular modeling tasks."
CONCLUSION/DISCUSSION ,0.10199556541019955,"However, our method has limitations. The fragmentation and ordinal encoding are tailored to
molecules and are less effective on large, densely connected graphs like citation or social networks,
where they introduce noise. Additionally, while we outperform GRIT on rare fragments, GRIT
performs slightly better on frequent ones (Table 12). Future work could improve fragment-biased
models for frequent data and explore their use in other domains."
CONCLUSION/DISCUSSION ,0.10421286031042129,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
CONCLUSION/DISCUSSION ,0.10643015521064302,Acknowledgements
CONCLUSION/DISCUSSION ,0.10864745011086474,"This project is supported by the Bavarian Ministry of Economic Affairs, Regional Development and
Energy with funds from the Hightech Agenda Bayern. Additionally, it is supported by the Helmholtz
Association under the joint research school “Munich School for Data Science - MUDS"" and by the
German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036B."
CONCLUSION/DISCUSSION ,0.11086474501108648,Impact Statement
CONCLUSION/DISCUSSION ,0.1130820399113082,"Among other contributions, this work presents an approach for predicting the properties of molecules.
In the area of machine learning for drug discovery, such methods can sometimes be used for harmful
purposes. This also applies to our research, since it might help to discover or create dangerous
substances. Despite these concerns, we believe that the benefits of our work outweigh the risks."
REFERENCES,0.11529933481152993,References
REFERENCES,0.11751662971175167,"[1] Bemis, G. W. and Murcko, M. A. The properties of known drugs. 1. molecular frameworks.
Journal of Medicinal Chemistry, 1996."
REFERENCES,0.1197339246119734,"[2] Bodnar, C., Frasca, F., Wang, Y. G., Otter, N., Montúfar, G., Liò, P., and Bronstein, M. Weisfeiler
and Lehman Go Topological: Message Passing Simplicial Networks, 2021."
REFERENCES,0.12195121951219512,"[3] Bodnar, C., Frasca, F., Otter, N., Wang, Y. G., Liò, P., Montúfar, G., and Bronstein, M. Weisfeiler
and Lehman Go Cellular: CW Networks, 2022."
REFERENCES,0.12416851441241686,"[4] Bouritsas, G., Frasca, F., Zafeiriou, S., and Bronstein, M. M. Improving Graph Neural Network
Expressivity via Subgraph Isomorphism Counting. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2023."
REFERENCES,0.12638580931263857,"[5] Bresson, X. and Laurent, T. Residual gated graph convnets, 2018."
REFERENCES,0.1286031042128603,"[6] Campi, F., Gosch, L., Wollschläger, T., Scholten, Y., and Günnemann, S. Expressivity of Graph
Neural Networks Through the Lens of Adversarial Robustness, 2023."
REFERENCES,0.13082039911308205,"[7] Chen, Z., Chen, L., Villar, S., and Bruna, J. Can Graph Neural Networks Count Substructures?,
2020."
REFERENCES,0.13303769401330376,"[8] Chen, Z., Villar, S., Chen, L., and Bruna, J. On the equivalence between graph isomorphism
testing and function approximation with gnns, 2023."
REFERENCES,0.1352549889135255,"[9] Degen, J., Wegscheid-Gerlach, C., Zaliani, A., and Rarey, M. On the art of compiling and using
’drug-like’ chemical fragment spaces. ChemMedChem, 2008."
REFERENCES,0.13747228381374724,"[10] Degen, J., Wegscheid-Gerlach, C., Zaliani, A., and Rarey, M. On the art of compiling and using
’drug-like’ chemical fragment spaces. ChemMedChem, 2008."
REFERENCES,0.13968957871396895,"[11] Di Giovanni, F., Giusti, L., Barbero, F., Luise, G., Lio’, P., and Bronstein, M. On Over-
Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology,
2023."
REFERENCES,0.1419068736141907,"[12] Du, Y., Fu, T., Sun, J., and Liu, S. Molgensurvey: A systematic survey in machine learning
models for molecule design. arXiv preprint arXiv:2203.14500, 2022."
REFERENCES,0.14412416851441243,"[13] Dwivedi, V. P. and Bresson, X. A Generalization of Transformer Networks to Graphs, 2021."
REFERENCES,0.14634146341463414,"[14] Dwivedi, V. P., Joshi, C. K., Luu, A. T., Laurent, T., Bengio, Y., and Bresson, X. Benchmarking
Graph Neural Networks, 2022."
REFERENCES,0.14855875831485588,"[15] Falcon, W. and The PyTorch Lightning team. PyTorch Lightning, 2019."
REFERENCES,0.15077605321507762,"[16] Fey, M. and Lenssen, J. E. Fast graph representation learning with PyTorch Geometric. In ICLR
Workshop on Representation Learning on Graphs and Manifolds, 2019."
REFERENCES,0.15299334811529933,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
REFERENCES,0.15521064301552107,"[17] Fey, M., Yuen, J.-G., and Weichert, F. Hierarchical Inter-Message Passing for Learning on
Molecular Graphs, 2020."
REFERENCES,0.1574279379157428,"[18] Frasca, F., Bevilacqua, B., Bronstein, M. M., and Maron, H. Understanding and Extending
Subgraph GNNs by Rethinking Their Symmetries, 2022."
REFERENCES,0.15964523281596452,"[19] Geisler, S., Li, Y., Mankowitz, D., Cemgil, A. T., Günnemann, S., and Paduraru, C. Transformers
meet directed graphs, 2023."
REFERENCES,0.16186252771618626,"[20] Geng, Z., Xie, S., Xia, Y., Wu, L., Qin, T., Wang, J., Zhang, Y., Wu, F., and Liu, T.-Y. De
novo molecular generation via connection-aware motif mining. International Conference on
Learning Representations, 2023."
REFERENCES,0.164079822616408,"[21] Giusti, L., Reu, T., Ceccarelli, F., Bodnar, C., and Liò, P. CIN++: Enhancing Topological
Message Passing, 2023."
REFERENCES,0.1662971175166297,"[22] Gómez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hernández-Lobato, J. M., Sánchez-Lengeling,
B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and Aspuru-Guzik, A.
Automatic chemical design using a data-driven continuous representation of molecules, 2017."
REFERENCES,0.16851441241685144,"[23] He, X., Hooi, B., Laurent, T., Perold, A., Lecun, Y., and Bresson, X. A Generalization of
ViT/MLP-Mixer to Graphs. In International Conference on Machine Learning, 2023."
REFERENCES,0.17073170731707318,"[24] Hetzel, L., Sommer, J., Rieck, B., Theis, F. J., and Günnemann, S. MAGNet: Motif-Agnostic
Generation of Molecules from Shapes. arXiv, 2023."
REFERENCES,0.1729490022172949,"[25] Hu, W., Liu, Y., Chen, X., Chai, W., Chen, H., Wang, H., and Wang, G. Deep learning methods
for small molecule drug discovery: A survey. IEEE Transactions on Artificial Intelligence,
2023."
REFERENCES,0.17516629711751663,"[26] Huang, N. and Villar, S. A short tutorial on the weisfeiler-lehman test and its variants. Interna-
tional Conference on Acoustics Speech and Signal Processing, 2021."
REFERENCES,0.17738359201773837,"[27] Huang, Y., Peng, X., Ma, J., and Zhang, M. Boosting the cycle counting power of graph neural
networks with I2-GNNs. International Conference on Learning Representations, 2022."
REFERENCES,0.17960088691796008,"[28] Inae, E., Liu, G., and Jiang, M. Motif-aware attribute masking for molecular graph pre-training.
arXiv preprint arXiv:2309.04589, 2023."
REFERENCES,0.18181818181818182,"[29] Jin, W., Barzilay, R., and Jaakkola, T. Junction Tree Variational Autoencoder for Molecular
Graph Generation, 2019."
REFERENCES,0.18403547671840353,"[30] Jin, W., Barzilay, R., and Jaakkola, T. Hierarchical generation of molecular graphs using
structural motifs. In International Conference on Machine Learning. PMLR, 2020."
REFERENCES,0.18625277161862527,"[31] Kiefer, S. and Neuen, D. The power of the weisfeiler–leman algorithm to decompose graphs.
SIAM Journal on Discrete Mathematics, 2022."
REFERENCES,0.188470066518847,"[32] Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks,
2017."
REFERENCES,0.19068736141906872,"[33] Kong, X., Huang, W., Tan, Z., and Liu, Y. Molecule Generation by Principal Subgraph Mining
and Assembling, 2022."
REFERENCES,0.19290465631929046,"[34] Kreuzer, D., Beaini, D., Hamilton, W. L., Létourneau, V., and Tossou, P. Rethinking graph
transformers with spectral attention, 2021."
REFERENCES,0.1951219512195122,"[35] Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference
on Learning Representations, 2017."
REFERENCES,0.1973392461197339,"[36] Ma, L., Lin, C., Lim, D., Romero-Soriano, A., Dokania, P. K., Coates, M., Torr, P., and Lim,
S.-N. Graph Inductive Biases in Transformers without Message Passing, 2023."
REFERENCES,0.19955654101995565,"[37] Maron, H., Ben-Hamu, H., Serviansky, H., and Lipman, Y. Provably powerful graph networks.
Advances in neural information processing systems, 2019."
REFERENCES,0.2017738359201774,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
REFERENCES,0.2039911308203991,"[38] Maziarz, K., Jackson-Flux, H., Cameron, P., Sirockin, F., Schneider, N., Stiefl, N., Segler, M.,
and Brockschmidt, M. Learning to Extend Molecular Scaffolds with Structural Motifs, 2022."
REFERENCES,0.20620842572062084,"[39] Merlot, C., Domine, D., Cleva, C., and Church, D. J. Chemical substructures in drug discovery.
Drug Discovery Today, 2003."
REFERENCES,0.20842572062084258,"[40] Morris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G., and Grohe, M.
Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks, 2021."
REFERENCES,0.2106430155210643,"[41] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani,
A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative
style, high-performance deep learning library, 2019."
REFERENCES,0.21286031042128603,"[42] Puny, O., Lim, D., Kiani, B. T., Maron, H., and Lipman, Y. Equivariant polynomials for graph
neural networks, 2023."
REFERENCES,0.21507760532150777,"[43] Rampášek, L., Galkin, M., Dwivedi, V. P., Luu, A. T., Wolf, G., and Beaini, D. Recipe for a
General, Powerful, Scalable Graph Transformer, 2023."
REFERENCES,0.21729490022172948,"[44] Singh, R. and Saha, M. Identifying structural motifs in proteins. Pacific Symposium on
Biocomputing, 2003."
REFERENCES,0.21951219512195122,"[45] Sommer, J., Hetzel, L., Lüdke, D., Theis, F. J., and Günnemann, S. The Power of Motifs as
Inductive Bias for Learning Molecular Distributions, 2023."
REFERENCES,0.22172949002217296,"[46] Sterling, T. and Irwin, J. J. Zinc 15 – ligand discovery for everyone. Journal of Chemical
Information and Modeling, 2015."
REFERENCES,0.22394678492239467,"[47] Thiede, E. H., Zhou, W., and Kondor, R. Autobahn: Automorphism-based graph neural nets.
ArXiv, 2021."
REFERENCES,0.2261640798226164,"[48] Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K.,
and Pande, V. S. Moleculenet: A benchmark for molecular machine learning. arXiv: Learning,
2017."
REFERENCES,0.22838137472283815,"[49] Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How Powerful are Graph Neural Networks?, 2019."
REFERENCES,0.23059866962305986,"[50] Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y. Do Transformers
Really Perform Bad for Graph Representation?, 2021."
REFERENCES,0.2328159645232816,"[51] Zang, X., Zhao, X., and Tang, B. Hierarchical molecular graph self-supervised learning for
property prediction. Communications Chemistry, 2023."
REFERENCES,0.23503325942350334,"[52] Zhang, B., Fan, C., Liu, S., Huang, K., Zhao, X., Huang, J., and Liu, Z. The expressive power
of graph neural networks: A survey. arXiv preprint arXiv:2308.08235, 2023."
REFERENCES,0.23725055432372505,"[53] Zhang, Z., Liu, Q., Wang, H., Lu, C., and Lee, C.-K. Motif-based graph self-supervised learning
for molecular property prediction. Advances in Neural Information Processing Systems, 2021."
REFERENCES,0.2394678492239468,"[54] Zhu, J., Wu, K., Wang, B., Xia, Y., Xie, S., Meng, Q., Wu, L., Qin, T., Zhou, W., Li, H., and
Liu, T.-Y. O-GNN: incorporating ring priors into molecular modeling, 2022."
OTHER,0.24168514412416853,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.24390243902439024,"A
Background"
OTHER,0.24611973392461198,"Notation. A graph G := (V, E, X) consists of a set of vertices V, a set of (undirected) edges
E ⊆V × V and d node features X ∈R|V|×d for every node v ∈V. The set of nodes that are
adjacent to v is denoted by N(v). Two graphs G1 = (V1, E1, X1) and G2 = (V2, E2, X2) are
isomorphic if there exists a bijection b : V1 →V2 that preserves edges and node features, that is,
{v, w} ∈E1 ⇔{b(v), b(w)} ∈E2 and X1
v = X2
w. For a subset of nodes U ⊆V, we denote the
induced subgraph with respect to these nodes by G[U]."
OTHER,0.24833702882483372,"Expressiveness. We can classify the expressiveness of functions over graphs by their capability
to distinguish non-isomorphic graphs. We say that a function f is (in parts) more powerful than
a function g if there exist two non-isomorphic graphs G1, G2 such that f(G1) ̸= f(G2) whereas
g(G1) = g(G2). The function f is strictly more powerful than g (we write f > g) if f is more
powerful than g and g is not (in parts) more powerful than f."
OTHER,0.25055432372505543,"Weisfeiler & Leman. The Weisfeiler & Leman graph isomorphism test is an iterative graph coloring
algorithm that bounds the expressive power of MPNNs [31]. In each iteration, it produces a color for
each node based on its neighboring nodes’ colors. Starting with a vertex color based only on features
c0
v = HASH(Xv), we calculate the update for the color c of node v in iteration t:"
OTHER,0.25277161862527714,"c(t)
v
= HASH

c(t−1)
v
, {{c(t−1)
w
| w ∈N(v)}}

.
(2)"
OTHER,0.2549889135254989,"The algorithm terminates once the set of unique colors does not increase. Two non-isomorphic graphs
can be distinguished if the multiset of colors differs at the end. As this test cannot distinguish all
non-isomorphic graphs, it can be extended to strictly more powerful versions, k-WL, incorporating
k-tuples of nodes to determine the color. For more background information, we refer to Morris et al.
[40]. Importantly, 2-WL is equivalent to the previously described WL test [26]."
OTHER,0.2572062084257206,"Fragmentations. A vocabulary Y is a set of graphs (potentially including node features) representing
important substructures, e.g., cycles. A fragment of a graph G is an induced subgraph G[f] isomorphic
to a graph from the vocabulary. We will identify a fragment simply by the subset of nodes f ⊆V.
All fragments f that are isomorphic to the same graph of the vocabulary have the same type(f). A
fragmentation scheme F is a permutation invariant function that maps a graph G to a set of fragments
F(G) =: F, which is called a fragmentation. Note that there might exist subgraphs isomorphic to a
graph in Y that are not in F(G). For example, even if Y contains 5-cycles, not all 5-cycles in G need
to be in F(G). If, for all graphs G, F(G) includes every subgraph isomorphic to a graph V ∈Y, we
say that the fragmentation scheme F recovers V ."
OTHER,0.25942350332594233,"B
Related work"
OTHER,0.2616407982261641,"Expressiveness of GNNs. Message-Passing Neural Networks (MPNN)2 are limited in their ex-
pressiveness. Their ability to distinguish between non-isomorphic graphs is confined to the 2-WL
algorithm, restricting their discriminative power [49]. Moreover, when it comes to recognizing
substructures, MPNNs are unable to accurately count almost all types of substructures [7]. This
limitation stems from their reliance on purely local messages, which—despite facilitating excellent
linear space and time complexity—renders them blind to higher-level structural information within
graphs."
OTHER,0.2638580931263858,"Higher-order GNNs. In response to the inability to effectively learn substructures, the introduction
of more powerful GNN architectures aims to overcome this limitation and enable comprehensive
substructure learning. Morris et al. [40] draw inspiration from the multidimensional k-WL algorithm
and diverge from learning node-specific representations by considering each k-tuple of nodes instead.
Although this improves expressiveness, its complexity increases exponentially. Subgraph GNNs
comprise an alternative to improve substructure identification, decomposing a graph into smaller
subgraphs for GNN application. The resulting subgraph representations are pooled before a final
graph level representation is derived [27; 18]. With some strategies for extracting subgraphs, subgraph
GNNs can identify basic substructures such as 4-cycles [27]. Puny et al. [42] extend the WL test
for higher-order GNNs to the graph polynomial counting problem as a new expressivity measure,
highlighting the importance of more fine-grained tests for GNNs. However, the limitations of higher-
order GNNs lie in their inability to effectively learn more intricate substructures, accompanied by an"
OTHER,0.2660753880266075,2We use MPNNs and GNNs interchangeably.
OTHER,0.2682926829268293,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.270509977827051,"increase in time complexity. Recent findings also suggest susceptibility to adversarial attacks and
out-of-distribution data, hinting at challenges in robustly learning substructures [6]."
OTHER,0.2727272727272727,"Fragment-Biased GNNs. Another line of work provides fragment information to GNNs as an explicit
inductive bias. These fragment-biased models vary not only in their vocabulary but also in the way
fragmentation information is integrated into the model. Node features: Bouritsas et al. [4] introduce
GSN-v, which uses the number of cycles or cliques as an additional node feature. Learned fragment
representation: Instead of treating fragmentation information as a fixed feature, other models learn
representations for each fragment by aggregating information from the corresponding nodes. Zhu
et al. [54] use a vocabulary of only cycles whereas Zang et al. [51] present HiMol, which fragments
a molecular graph, based on chemical properties. Higher-level graph: A natural extension of the
learned fragment representation is a higher-level graph of fragments where neighboring fragments
influence each other. Thiede et al. [47] use equivariant computations along the paths of length 3 to 6
and cycles of sizes 5 and 6. Fey et al. [17] build a higher-level junction tree using rings and edges.
Yet, none of the existing works compare—theoretically or experimentally—how to encode and use
substructure information in the model. Additionally, most works only focus on a single substructure
that does not allow to fragment the complete graph."
OTHER,0.2749445676274945,"Topological GNNs use higher-level topological structures such as simplicial complexes [2] or CW-
Networks [3; 21] in their message-passing schemes. While coming from a different theoretical
direction than substructure-biased GNNs, in practice, they use cliques or cycles as learned fragment
representations or in a higher-level graph."
OTHER,0.2771618625277162,"Graph Transformer. Recently, models such as Graph Transformers [50; 36; 19] and ViT/MLP-
Mixers [23] for graphs adapted successful models from other domains to graph data. Their ability
to recognize substructures depends on the positional encoding used. Almost all recent models use
random walk encodings, which can help to discover simple substructures like cycles."
OTHER,0.2793791574279379,"Fragmentation Schemes. Fragmentation methods in the chemical domain aim to divide a molecular
graph into subgraphs with distinct structures or properties. There are various strategies to achieve this,
such as separating probable reactants [9], categorizing molecules into distinct structural classes [1],
or breaking apart acyclic bonds [38; 29]. Unlike these methods, data-driven approaches like those
outlined in Kong et al. [33] and Geng et al. [20] focus on deriving subgraphs directly from a dataset
without relying on predefined rules for decomposition."
OTHER,0.28159645232815966,"C
Weisfeiler & Leman Go Fragments"
OTHER,0.2838137472283814,"Existing fragment-biased MPNNs vary in their underlying fragmentation scheme and how the
fragment information is incorporated into the model. This variability makes a direct comparison
of the expressiveness of these models difficult. To address this challenge, we propose a new, more
fine-grained version of the WL test, called Fragment-WL test, that incorporates detailed structural
elements. We derive a hierarchy of tests that capture how fragmentation information is incorporated
into existing substructure-biased models, while leaving out all variability that does not influence the
expressivity."
OTHER,0.2860310421286031,"Our Fragment-WL test also subsumes existing WL variants designed for simplicial complexes
and CW-cells [2; 3], providing a more unified framework for assessing the expressiveness of both
substructure-biased and topological GNNs. Furthermore, our proposed Fragment-WL test highlights
the significance of how fragment information is incorporated into the model, emphasizing that the
integration methodology plays a crucial role for determining the model’s expressive power."
OTHER,0.28824833702882485,"Fragment-WL entails multiple variants with increasing expressiveness in distinguishing isomorphic
graphs. In the following, we first provide a general framework and then define the individual
Fragment-WL versions that perform the original WL test on different augmented graphs. We start
with a definition of WL tests on augmented graphs:
Definition C.1. A g-WL test is a function that performs the WL test on the augmented graph g(G),
i.e.
g-WL(G) := WL(g(G))
where g is a function mapping from graphs to graphs, i.e., g : (V, E, X) 7→(V′, E′, X′)."
OTHER,0.29046563192904656,"There are three ways in which a fragmentation F is used in existing fragment-biased GNNs: as
an additional node feature, as learned fragment representation, and as a higher-level graph. We"
OTHER,0.2926829268292683,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.29490022172949004,"Figure 3: Example graph G with corresponding augmented variants. NF(G) includes node features,
FR(G) also includes a representation for each fragment and HLG(G) also has connections between
neighboring fragment represenations."
OTHER,0.29711751662971175,"instantiate g with the corresponding functions to augment the graph with the respective features.
First, we use additional node features. We extend the individual node features with the information of
the fragments that the node is contained in. We concatenate this information to the already existing
features. Formally, we define this augmentation function in the following way."
OTHER,0.29933481152993346,"Definition C.2. We define the node feature function as NF(V, E, X) = (V, E, XNF) with:"
OTHER,0.30155210643015523,"XNF
v
= Xv ∥λ

{{type(f) | v ∈f, f ∈F}}

,"
OTHER,0.30376940133037694,"where λ represents any injective function and ∥indicates the concatenation operation. We instantiate
g with NF to create the NF-WL test."
OTHER,0.30598669623059865,"Another prominent way is using representations for each fragment and messages that are flowing
from the lower level nodes to their entailing fragment and backwards. This means that we introduce a
new vertex for each fragment and connect it to all its corresponding vertices in the original graph. We
depict this graph FR(G) in Figure 3. We define this augmentation function in the following.
Definition C.3. We define the fragment representation function as FR(V, E, X) = (VFR, EFR, XFR)
with:"
OTHER,0.3082039911308204,"VFR := V ∪F,"
OTHER,0.31042128603104213,"EFR := E ∪

{f, v} | ∀f ∈F, ∀v ∈f
	
,"
OTHER,0.31263858093126384,"XFR
i
:=
Xi
i ∈V
type(i)
i ∈F"
OTHER,0.3148558758314856,"Lastly, we allow messages to be exchanged between neighboring fragments, thus creating a higher-
level graph on which information can flow. To this end, we add edges between fragments that have
neighboring nodes in G and thus construct a graph of the higher-level fragments, see HLG(G) in
Figure 3.
Definition
C.4.
The
higher-level
graph
augmentation
functions
is
HLG(V, E, X)
=
(VHLG, EHLG, XHLG) with:"
OTHER,0.3170731707317073,"VHLG := VFR,
XHLG := XFR,"
OTHER,0.31929046563192903,"EHLG := EFR ∪

{f, k} | f, k ∈F, f ∩k ̸= ∅"
OTHER,0.3215077605321508,"Equipped with the formal definitions, we will now compare the power of performing the WL test
on these transformed graphs to the original graph3. The power of the Fragment-WL test depends on
the fragmentation scheme F. With a sufficiently advanced fragmentation scheme, even NF-WL can
become arbitrarily powerful.
Theorem C.5. There exist fragmentation schemes such that NF-WL, FR-WL and HLG-WL are all
strictly more powerful than k-WL for any k."
OTHER,0.3237250554323725,3All proofs are detailed in Appendix D
OTHER,0.3259423503325942,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.328159645232816,"Figure 4: Graphs G1 and G2 with their corresponding higher-level graph of fragments. The edges of
the fragment representation to the vertices of G1 and G2 are omitted. G1 and G2 are indistinguishable
by WL, NF-WL and FR-WL but distinguishable by HLG-WL as the higher-level graphs exhibit
different connections from the 3-ring nodes to the 4-ring node."
OTHER,0.3303769401330377,"However, in practice, mostly fragmentation schemes with a vocabulary of rings, paths, and cliques
are used for fragment-biased GNNs [17; 4; 54]. So, from now on, we will restrict ourselves to such
fragmentation in our theoretical analysis. Next, we show that it matters how to incorporate fragment
information and that the WL variants become more powerful through higher-level abstraction."
OTHER,0.3325942350332594,"Integrating fragment information from any non-trivial substructure as an additional node feature
already increases expressiveness beyond 2-WL.
Theorem C.6. NF-WL is strictly more powerful than 2-WL for fragmentation schemes F that recover
any substructure with more than two nodes."
OTHER,0.3348115299334812,"This shows that the classical 2-WL test cannot reveal differences in the expressivity of fragment-
biased GNNs since using any substructure as a node feature already increases expressivity beyond
2-WL. Our Fragment-WL test, however, provides a more fine-grained alternative that reveals that
higher-level abstraction through a learned fragment representation strictly increases the expressivity
compared to node features:
Theorem C.7. FR-WL is strictly more powerful than NF-WL for fragmentation schemes F recovering
3-cycles."
OTHER,0.3370288248337029,"Building a higher-level graph of fragments further increases the expressivity. Figure 4 shows an
example of two graphs that are indistinguishable by 2-WL, NF-WL, and FR-WL but distinguishable
by HLG-WL. Formally, we express this in the following theorem.
Theorem C.8. HLG-WL is strictly more powerful than FR-WL for fragmentation schemes F recov-
ering 3-cycles."
OTHER,0.3392461197339246,"Hence, we have shown that the expressivity increases strictly monotonically from 2-WL to HLG-WL
for fragmentation schemes recovering 3-cycles:"
OTHER,0.34146341463414637,"2-WL < NF-WL < FR-WL < HLG-WL
(3)"
OTHER,0.3436807095343681,"Regarding the higher-dimensional k-WL hierarchy, our Fragment-WL hierarchy cannot be bounded
by 3-WL if the fragmentation can recover 5 cycles."
OTHER,0.3458980044345898,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.34811529933481156,"Theorem C.9. HLG-WL is in parts more powerful than 3-WL for fragmentation schemes F recovering
5-cycles."
OTHER,0.35033259423503327,"Our developed Fragment-WL hierarchy models the different ways in which fragment information is
used in most fragment-biased and topological GNNs. This new measure of expressiveness allows the
comparison and ordering of these existing methods; see Table 2 for an overview."
OTHER,0.352549889135255,"In summary, our Fragment-WL test provides a new alternative measure of expressivity compared to
the original WL test. Our developed hierarchy reveals that it matters how to incorporate fragmen-
tation information, i.e., higher-level abstraction increases expressivity. Additionally, it allows for a
comparison of the expressiveness of most existing fragment-biased and topological GNNs."
OTHER,0.35476718403547675,"D
Proofs"
OTHER,0.35698447893569846,"This chapter presents the proofs for the theorems introduced in Appendix C, and the expressiveness
analysis of existing fragment-biased and topological GNNs. We will first introduce general concepts
that will later help to bound the expressiveness of different models and our Fragment WL test."
OTHER,0.35920177383592017,"D.1
Color refinement and expressiveness: Useful definitions and lemmas"
OTHER,0.3614190687361419,"To prove the power of different graph coloring algorithms, it will be useful to first introduce the
definition of color refinement. The intuition is that a ""finer"" coloring contains more information than
a ""corser"" coloring.
Definition D.1. Let c, d be colorings of a graph G. The coloring c refines d (we write c ⊑d) if there
exists a function h such that h(cv) = dv for all v ∈V."
OTHER,0.36363636363636365,"We will sometimes write cv ⊑dv if c ⊑d and the set of vertices V is clear from the context.
Example D.1. Let c(t) be the coloring of iteration t of the WL test. Then one can easily show that
c(t)
v
⊑c(l)
v for l ≤t as c(t)
v
contains the information of all previous colorings c(l)
v ."
OTHER,0.36585365853658536,"Note that an alternative definition of color refinement [2] is: c ⊑d iff cv = cw implies dv = dw for
all v, w ∈V. It is easy to see that the two definitions are equivalent. Next, we extend our definition
to arbitrary functions and not just colorings.
Definition D.2. Let a, b be two functions over the set of vertices V. Then a ⊑b if there exists a
function h such that h(a(v)) = b(v) for all v ∈V."
OTHER,0.36807095343680707,"Intuitively, a ⊑b means that we can compute b(v) from the result of a(v). So a(v) contains more
or the same information as b(v). Again, we will sometimes simply write av ⊑bv with av := a(v),
bv := b(v) if a ⊑b and if the set of vertices V is clear from the context.
Example D.2. Let c(t) be the coloring of iteration t of the WL test. Then, because of the injectiveness
of the hash function HASH in Equation (2):"
OTHER,0.37028824833702884,"c(t)
v
⊑{{c(t−1)
w
| w ∈N(v)}}."
OTHER,0.37250554323725055,"Note that we use the simplified notation here. The right and left-hand side are actually the functions
that map from v ∈V to these terms. Intuitively, this shows that one can compute the previous color
of all neighbors from the color of a node."
OTHER,0.37472283813747226,"It is easy to see that the refinement relation is transitive, i.e., a ⊑b and b ⊑c imply a ⊑c."
OTHER,0.376940133037694,"We will now formally define the expressive power of an algorithm with respect to the ability to
distinguish non-isomorphic subgraphs.
Definition D.3. A function f is (in parts) more powerful than a function g if there exist two non-
isomorphic graphs G1, G2 such that f can distinguish them"
OTHER,0.37915742793791574,f(G1) ̸= f(G2)
OTHER,0.38137472283813745,"whereas g cannot distinguish them
g(G1) = g(G2)."
OTHER,0.3835920177383592,"Note that this relation is not anti-symmetric, i.e. f can be (in parts) more powerful than g, and
conversely, g can also be (in parts) more powerful than f. Hence, we introduce the following stronger
anti-symmetric relation:"
OTHER,0.3858093126385809,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.38802660753880264,Definition D.4. A function f is strictly more powerful than g (denoted as f > g) if
OTHER,0.3902439024390244,1. f is more powerful than g
OTHER,0.3924611973392461,2. and g is not (in parts) more powerful than f.
OTHER,0.3946784922394678,"Additionally, we write we write g ≤f if a function g is not more powerful than f."
OTHER,0.3968957871396896,"Next, we will prove a connection between color refinement and expressiveness: a function that always
produces a finer coloring cannot be less powerful than a function with a coarser coloring.
Lemma D.5. Let f, g be functions with f ⊑g for all graphs. Then, g is not more powerful than f,
i.e., g ≤f."
OTHER,0.3991130820399113,"Proof. Assume for the sake of contradiction that there exist non-isomorphic graphs G1, G2 that can
be distinguished by g but not by f. Let d be the coloring obtained with g, and c be the coloring
obtained with f. The multiset of colors d has to differ for G1 and G2, i.e. there exists a color α with"
OTHER,0.401330376940133,"D1
α := {v | dv = α, v ∈V1}"
OTHER,0.4035476718403548,"D2
α := {v | dv = α, v ∈V2}"
OTHER,0.4057649667405765,such that
OTHER,0.4079822616407982,"|D1
α| ̸= |D2
α|.
(4)"
OTHER,0.41019955654101997,"Since c refines d no node v in D1
α and D2
α can share a color cv with another node not in D1
α and D2
α.
Hence, the set of colors c of nodes in D1
α and D2
α is disjoint from the set of colors c for the other
nodes in the graph. But because of 4 there has to exist a color β with"
OTHER,0.4124168514412417,"C1
β := {v | cv = β, v ∈V1} ⊆D1
α
C2
β := {v | cv = β, v ∈V2} ⊆D2
α
and"
OTHER,0.4146341463414634,"|C1
β| ̸= |C2
β|."
OTHER,0.41685144124168516,This contradicts the initial assumption.
OTHER,0.4190687361419069,"Note that all our augmentation functions introduced in Appendix C only add information to the graph,
or more formally:
Definition D.6. A function g from graphs to graphs is called additive if the set of nodes and edges
does not decrease, i.e., g(V, E, X) = (V′, E′, X′) with V ⊆V′ and E ⊆E′."
OTHER,0.4212860310421286,"But by adding too much information, one could completely destroy the initial structure of the graph
(e.g., make every graph a complete graph). Hence, we need the additional condition that it should be
possible to recover the original graph from the augmented graph.
Definition D.7. An additive function g from graphs to graphs (i.e, g(G) = G′) is called reversible if
there exists a function h for vertices v ∈V′ such that"
OTHER,0.42350332594235035,"h(X′
v) =
Xv
v ∈V
⊥
v /∈V
(5)"
OTHER,0.42572062084257206,"and a function ε for edges e = {u, v} ∈E′ such that"
OTHER,0.4279379157427938,"ε(X′
u, X′
v) =
1
e ∈E
0
e /∈E.
(6)"
OTHER,0.43015521064301554,"By only adding such reversible information the WL test cannot become less powerful:
Lemma D.8. Let g be a reversible function. Then g-WL is not less powerful than WL."
OTHER,0.43237250554323725,"Proof. Let c(t) be the coloring obtained by the t-th iteration of the WL-test, and d(t) the coloring of
g-WL. We will show by induction that there exists a function h such that h(d(t)
v ) = c(t)
v
for v ∈V,
i.e d(t)
v
⊑c(t)
v . For t = 0, this follows immediately from Equation (5). For the induction step, note"
OTHER,0.43458980044345896,"d(t)
v
⊑{{d(t−1)
u
| u ∈NG′(v)}}."
OTHER,0.43680709534368073,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.43902439024390244,"Now note that the function ε (Equation (6)) makes it possible to reconstruct the neighborhood in G
based on the features X′ and neighborhood in G′. Since d(t) only refines the features X′, we can
also reconstruct the neighborhood in G based on d(t) and, hence,"
OTHER,0.44124168514412415,"{{d(t−1)
u
| u ∈NG′(v)}} ⊑{{d(t−1)
u
| u ∈NG(v)}}"
OTHER,0.4434589800443459,and by induction hypothesis
OTHER,0.44567627494456763,"{{d(t−1)
u
| u ∈NG(v)}} ⊑{{c(t−1)
u
| u ∈NG(v)}}."
OTHER,0.44789356984478934,"So, taken together, we have"
OTHER,0.4501108647450111,"d(t)
v
⊑{{c(t−1)
u
| u ∈NG(v)}}.
(7)"
OTHER,0.4523281596452328,"Additionally, note that"
OTHER,0.45454545454545453,"d(t)
v
⊑d(t−1)
v"
OTHER,0.4567627494456763,"IH
⊑c(t−1)
v
.
(8)"
OTHER,0.458980044345898,"By combining Equation (7) and Equation (8), we get"
OTHER,0.4611973392461197,"d(t)
v
⊑

c(t−1)
v
, {{c(t−1)
u
| u ∈NG(v)}}

⊑c(t)
v ."
OTHER,0.4634146341463415,"Hence, d refines c, and by Lemma D.5 g-WL is not less powerful than WL."
OTHER,0.4656319290465632,"Now that we have found a lower bound of the expressiveness, we will also give an upper bound of
the expressiveness. The idea is that a graph augmentation function does not increase the power of
a coloring algorithm f (e.g., the WL test) if all the information added by g can also be computed
from the coloring obtained with f. Or, to put it differently, a graph augmentation function g does
not increase the expressiveness if it is possible to compute the set of colors on g(G) from the set of
colors on G.
Lemma D.9. Let g be a function that augments a graph, i.e., a function from graphs to graphs. Let f
be a coloring function. If there exists a function h such that"
OTHER,0.4678492239467849,f(g(G)) = h(f(G))
OTHER,0.4700665188470067,then f ◦g is not more powerful than f.
OTHER,0.4722838137472284,"Proof. Let G1, G2 be two non-isomorphic graphs that are distinguishable by f ◦g. Then"
OTHER,0.4745011086474501,f(g(G1)) ̸= f(g(G2))
OTHER,0.47671840354767187,h(f(G1)) ̸= h(f(G2))
OTHER,0.4789356984478936,"It follows that f(G1) ̸= f(G2). Hence, f ◦g is not more powerful than f."
OTHER,0.4811529933481153,"Note that the condition in the lemma is similar to the definition of color refinement. However, we
cannot use color refinement directly because the function g could add or delete nodes, making a direct
comparison of nodes between G1 and G2 impossible. Consequently, we have to use a more global
view rather than the more localized approach of color refinement."
OTHER,0.48337028824833705,"D.2
Graph augmentation functions"
OTHER,0.48558758314855877,"We will now analyze the change in expressiveness with some graph augmentation functions that
model message-passing schemes that are frequently used in practice."
OTHER,0.4878048780487805,"D.2.1
Fragment augmentations"
OTHER,0.49002217294900224,"We will first give the proofs for the augmentation functions from Appendix C that incorporate
fragment information."
OTHER,0.49223946784922396,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.49445676274944567,"Proof of Theorem C.5.
Theorem C.5. There exist fragmentation schemes such that NF-WL, FR-WL and HLG-WL are all
strictly more powerful than k-WL for any k."
OTHER,0.49667405764966743,"Proof. Consider a fragmentation scheme that decomposes a graph into every possible subgraph (so
the vocabulary is the set of all possible graphs). It is known that for every k there exist two non-
isomorphic graphs G1 and G2 that are indistinguishable by k-WL. But as G1 and G2 are themselves
part of the vocabulary and the fragmentation, they are trivially distinguishable by NF-WL, FR-WL,
and HLG-WL."
OTHER,0.49889135254988914,"Proof of Theorem C.6.
Theorem C.6. NF-WL is strictly more powerful than 2-WL for fragmentation schemes F that recover
any substructure with more than two nodes."
OTHER,0.5011086474501109,"Proof. Chen et al. [7] showed that the WL test cannot count the number of (induced) subgraphs with
at least three nodes, i.e. for any substructure S with more than three nodes, there exist non-isomorphic
graphs G1 and G2 such that 2-WL cannot distinguish them but they have a different count of S. So,
let X be the substructure that F recovers. Then, WL cannot count the number of occurrences of X,
whereas NF-WL can trivially count the number of X in a graph. Hence, NF-WL is more powerful
than WL. Since NF is a reversible function, NF-WL is by Lemma D.8 also not less powerful than
WL, making NF-WL strictly more powerful than WL."
OTHER,0.5033259423503326,"Proof of Theorem C.7.
Before coming to the proof of Theorem C.7, we will prove the following
useful lemma:
Lemma D.10. Two graphs G1 = (V1, E1, X1), G2 = (V2, E2, X2) are undistinguishable by WL if
the set of node features is the same
X1 = X2 := X"
OTHER,0.5055432372505543,and all nodes with the same node feature have the same neighborhood
OTHER,0.5077605321507761,"∀i, j ∈V1 ∪V2 :
Xi = Xj ⇒{Xn | n ∈N(i)} = {Xm | m ∈N(j)}.
(9)"
OTHER,0.5099778270509978,"Proof. We will show by induction over t that the color of all nodes with the same node features is the
same:"
OTHER,0.5121951219512195,"∀i, j ∈V1 ∪V2 :"
OTHER,0.5144124168514412,"Xi = Xj ⇒ct
i = ct
j."
OTHER,0.516629711751663,"For t = 0, this follows immediately from c0
i = HASH(Xi).
For t > 0, we have for nodes i, j with Xi = Xj"
OTHER,0.5188470066518847,"ct
i = HASH

ct−1
i
, {ct−1
n
| n ∈N(i)}
"
OTHER,0.5210643015521065,"= HASH

ct−1
j
, {ct−1
n
| n ∈N(i)}

(by IH)"
OTHER,0.5232815964523282,"= HASH

ct−1
j
, {ct−1
m
| m ∈N(j)}

(by 9 and IH)"
OTHER,0.5254988913525499,"= ct
j"
OTHER,0.5277161862527716,"As both graphs have the same node features X1 = X2 the set of colors is also the same in each
iteration t of the WL-test. Hence, the graphs are indistinguishable by the WL-test"
OTHER,0.5299334811529933,"Theorem C.7. FR-WL is strictly more powerful than NF-WL for fragmentation schemes F recovering
3-cycles."
OTHER,0.532150776053215,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.5343680709534369,"Figure 5: Graph G1 and graph G2 that are indistinguishable by NF-WL but distinguishable by
FR-WL. Node features are represented by the color of the nodes."
OTHER,0.5365853658536586,"Proof. With Lemma D.10 we can now prove Theorem C.7: We first show that FR-WL is more
powerful than NF-WL. Consider the two graphs G1, G2 depicted in Figure 5 with two different node
features colored violet and blue. Note that when not considering the node features the graphs are
identical and each node is isomorphic to every other node. As every fragmentation scheme has to be
permutation invariant, every node is assigned the same additional node feature in NF (which, thus,
holds no additional information to distinguish the graphs). Now, observe that the two graphs fulfill
the conditions of Lemma D.10. Therefore, the graphs are indistinguishable by NF-WL. In contrast,
FR-WL can distinguish the two graphs as G1 contains 3-cycles with three violet nodes whereas G2
does not contain such 3-cycles. Hence, the fragment representation for these 3-cycles differs and
distinguishes the two graphs."
OTHER,0.5388026607538803,"Now, it remains to show that NF-WL is not more powerful than FR-WL. Let ct
v, dt
v be the colorings
of the t-th iteration of FR-WL and NF-WL, respectively. We will show by induction over t that
ct+1
v
⊑dt
v. To simplify the notation, let N↑(v) := {f | v ∈f, f ∈F} be the set of fragments that v
is part of. For t = 0, note that after the first iteration of the FR-WL test each vertex v ∈G receives
information about the fragment it is part of"
OTHER,0.541019955654102,"c1
v = HASH

c0
v, {{c0
n | n ∈NG(v) ⊎N↑(v)}}
"
OTHER,0.5432372505543237,"= HASH

c0
v, {{c0
n | n ∈N(v)}} ⊎{{c0
f | f ∈N↑(v)}}
"
OTHER,0.5454545454545454,"⊑

c0
v, {{c0
f | f ∈N↑(v)}}
"
OTHER,0.5476718403547672,"=

Xv, {{type(f) | f ∈N↑(v)}}
"
OTHER,0.549889135254989,"⊑d0
v
For the induction step (t −1) →t, we have"
OTHER,0.5521064301552107,"ct
v = HASH

ct−1
v
, {{ct−1
n
| n ∈NG(v) ⊎N↑(v)}}
"
OTHER,0.5543237250554324,"⊑

ct−1
v
, {{ct−1
n
| n ∈NG(v)}}
"
OTHER,0.5565410199556541,"⊑

dt−2
v
, {{dt−2
n
| n ∈NG(v)}}
"
OTHER,0.5587583148558758,"⊑dt−1
v
This concludes the induction step. Hence, we have c ⊑d, and by Lemma D.8 NF-WL is not more
powerful than FR-WL."
OTHER,0.5609756097560976,"Proof of Theorem C.8.
Theorem C.8. HLG-WL is strictly more powerful than FR-WL for fragmentation schemes F recov-
ering 3-cycles."
OTHER,0.5631929046563193,"Proof. Consider the two graphs G1, G2 depicted in Figure 4 with two different node features colored
green and red. Note that the two graphs fulfill the conditions of Lemma D.10. Furthermore, even
the graphs FR(G1) and FR(G2) fulfill these conditions. Hence, the graphs G1, G2 cannot be
distinguished by FR-WL."
OTHER,0.565410199556541,"In the higher level graph of G1 each 3-cycle representation is connected to two other 3-cycle
representations. Contrarily, in the higher level graph of G2 each 3-cycle representation is connected"
OTHER,0.5676274944567627,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.5698447893569845,"to only one 3-cycle representations. Hence, the coloring will differ and HLG-WL distinguishes the
two graphs."
OTHER,0.5720620842572062,"Now, it remains to show that FR-WL is not more powerful than HLG-WL. This follows immediately
from Lemma D.8 and the fact that the change in the graph from FR to HLG is reversible."
OTHER,0.5742793791574279,"Proof of Theorem C.9.
Theorem C.9. HLG-WL is in parts more powerful than 3-WL for fragmentation schemes F recovering
5-cycles."
OTHER,0.5764966740576497,"Proof. The proof follows a similar proof by Bodnar et al. [3]. Consider the Rook’s 4×4 and
Shrikhande graph (both in the family of strongly regular graphs SR(16, 6, 2, 2)). The Shrikhande
graph possesses 5-cycles while the Rook’s graph does not. Hence, HLG-WL can trivially disitnguish
those two graphs with a fragmentation recovering 5-cycles. However, it is known that 3-WL cannot
distinguish those two graphs [3]."
OTHER,0.5787139689578714,"D.3
Additional graph augmentation function"
OTHER,0.5809312638580931,"We will now consider two additional graph augmentation functions that are often used in practice:
a learned representation for each edge and a learned representation for the complete graph that is
connected to all other nodes. While these augmentations might be beneficial in practice, we will
show that they do not increase expressiveness."
OTHER,0.5831485587583148,"Edge representation.
We will first formally define the edge representation augmentation (ER). For
every edge, we introduce a new node that is connected to its two endpoints.
Definition D.11. The edge representation graph augmentation function is ER(V, E, X)
=
(VER, EER, XER) with
VER := V ∪E,"
OTHER,0.5853658536585366,"EER := E ∪

{e, v} | e = {u, v} ∈E"
OTHER,0.5875831485587583,"XER
i
:=
Xi
i ∈V
α
i ∈E
where α is some new label."
OTHER,0.5898004434589801,"Now, we can show that this augmentation does not increase expressiveness compared to the WL test.
Lemma D.12. ER-WL is as powerful as WL."
OTHER,0.5920177383592018,"Proof. We will first show that ER-WL is not more powerful than WL: Let c(i), d(i) be the colorings
of the WL test and of the ER-WL test, respectively. Then we will show that"
OTHER,0.5942350332594235,"We will use Lemma D.9 and give a function h that maps a coloring c(i) of the WL test without edge
representation to a coloring d(i) of ER-WL: Note that with the color c(i)
v
of a node v one can compute
the colors c(i−1)
u
of all neighboring nodes u. Hence, we can determine from c(i) the following multiset"
OTHER,0.5964523281596452,"{{(c(i)
v , c(i−1)
u
) | e = (u, v) ∈E}}"
OTHER,0.5986696230598669,"which allows us to compute the corresponding edge representations d(i)
e ."
OTHER,0.6008869179600886,"We will now show that ER-WL is not less powerful than WL. This follows directly from Lemma D.8
as ER is a reversible function."
OTHER,0.6031042128603105,"Graph representation.
We will now formally define the learned graph representation (GR), some-
times called virtual node.
Definition D.13.
The graph rerpesentation augmentation function is GR(V, E, X)
=
(VGR, EGR, XGR) with
VGR := V ∪{g},"
OTHER,0.6053215077605322,"EGR := E ∪

{v, g} | v ∈V}"
OTHER,0.6075388026607539,"XGR
i
:=
Xi
i ∈V
α
i = g"
OTHER,0.6097560975609756,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.6119733924611973,"Table 2: Overview of the vocabulary and expressiveness of existing topological and fragment-biased
models. The bounds for GSN-v, O-GNN, and HIMP are tight, i.e. when using a sufficient number
of layers and injective neighborhood aggregators, the models are as powerful as the corresponding
Fragment-WL test."
OTHER,0.614190687361419,"Model
Bounded by
Vocabulary"
OTHER,0.6164079822616408,"GSN-v4
≤NF-WL
Cliques or Rings"
OTHER,0.6186252771618626,"O-GNN
≤FR-WL
Rings"
OTHER,0.6208425720620843,"HIMP
≤HLG-WL
Rings"
OTHER,0.623059866962306,"MPSN
≤HLG-WL
Simplicial complexes
(in practice cliques)"
OTHER,0.6252771618625277,"CIN
≤HLG-WL
CW complexes
(in practice rings & edges)"
OTHER,0.6274944567627494,"CIN++
≤HLG-WL
CW complexes
(in practice rings & edges)"
OTHER,0.6297117516629712,where α is some new label.
OTHER,0.6319290465631929,"Similar to the edge representation, the graph representation does not increase expressiveness:
Lemma D.14. GR-WL is as powerful as WL."
OTHER,0.6341463414634146,"Proof. We will first show that GR-WL is not more powerful than WL. We will use Lemma D.9 and
give a function h that maps a coloring c(i) of the WL test without graph representation to a coloring
d(i) of GR-WL: We will show this by induction over i. For i = 0, this follows immediately from
the definition of GR. For the induction step, assume that there exists such a function from c(t−1) to
d(t−1). Note that the graph representation d(t)
g
is computed as:"
OTHER,0.6363636363636364,"d(t)
g
= HASH

d(t−1)
g
, {{d(t−1)
v
| v ∈NG′(g)}}
"
OTHER,0.6385809312638581,"= HASH

d(t−1)
g
, {{d(t−1)
v
| v ∈V}}

."
OTHER,0.6407982261640798,"which can be derived from c(t−1) by induction hypothesis. With this we can trivially compute d(t)
v
from c(t) for all other nodes v, too."
OTHER,0.6430155210643016,"Since GR is a reversible function, by Lemma D.8 GR-WL is not less powerful than WL. Hence,
GR-WL is as powerful as WL."
OTHER,0.6452328159645233,"D.4
Expressiveness of existing models"
OTHER,0.647450110864745,"We will use our Fragment-WL tests to compare the expressiveness of existing fragment-biased GNN
models. Table 2 gives an overview of the vocabulary of existing fragment-biased and topological
GNNs. Additionally, it shows the expressiveness in our Fragment-WL hierarchy."
OTHER,0.6496674057649667,"D.4.1
GSN-v"
OTHER,0.6518847006651884,"GSN-v [4] incorporate fragment information as an additional node feature. The additional node
features consist of the counts of fragment types a node is part of. Their framework also differentiates
between different (non-symmetric) positions inside the fragment (e.g., first node in path vs. second
node in path) that correspond to different orbits. While their framework can use any fragmentation
scheme, in all real-world experiments, they only use rings or cliques. Note that for rings and cliques,
no different orbits exist, i.e., each node in the substructure has the same orbit. Hence, this information
becomes irrelevant."
OTHER,0.6541019955654102,"4We evaluate the GSN-v that is used in practice, i.e. with a vocabulary of cliques and rings. Note that the
theory of the authors allows for potentially more expressive instantiations."
OTHER,0.656319290465632,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.6585365853658537,"Theorem D.15. GSN-v using rings and/or cliques as vocabulary is at most as powerful as NF-WL.
Additionally, when using injective neighborhood aggregators and a sufficient number of layers, GNSs
are as powerful as NF-WL with a fragmentation scheme based on rings and cliques."
OTHER,0.6607538802660754,"Proof. GSN-v appends the node features by the counts of substructures and the respective orbits each
node is part of. After that, a standard GNN is applied to the graph."
OTHER,0.6629711751662971,"Note that in a ring or a clique, each node has exactly the same orbit. So, for a vocabulary based on
cliques and rings the appended information degenerates to solely the substructure counts. Further,
note that this substructure count function is an injective function λ as defined in Definition C.2. Hence,
when using injective neighborhood aggregators and an MLP update function with a sufficiently large
number of layers such that it can approximate the HASH function, GSN-v exactly models the NF-WL
test. Hence, GSN-v is exactly as powerful as NF-WL."
OTHER,0.6651884700665188,"D.4.2
O-GNNs"
OTHER,0.6674057649667405,"Besides representations for nodes, O-GNNs [54] use explicit representation for rings, edges, and the
whole graph.
Theorem D.16. O-GNNs [54] are at most as powerful as FR-WL. Additionally, when using injective
neighborhood aggregators and a sufficient number of layers, O-GNNs are as powerful as FR-WL
with a fragmentation scheme based solely on rings."
OTHER,0.6696230598669624,"Proof. O-GNNs (when using injective neighborhood aggregators instead of their original sum
aggregators and an MLP with a sufficiently large number of layers such that it can approximate the
HASH function) models performing the WL test on an FR, ER, and GR augmented graph. As shown
in Lemmas D.12 and D.14, the edge representation and the graph representation do not influence the
expressivity. Hence, O-GNNs are exactly as powerful as FR-WL with a fragmentation scheme based
solely on rings."
OTHER,0.6718403547671841,"D.4.3
HIMP"
OTHER,0.6740576496674058,"HIMP [17] builds a higher-level junction tree based on rings and edges for message passing on the
original graph, the higher-level junction tree, and between those two.
Theorem D.17. HIMP is at most as powerful as HLG-WL. Additionally, when using injective
neighborhood aggregators and a sufficient number of layers, HIMP is as powerful as HLG-WL with
a fragmentation scheme based on rings and edges."
OTHER,0.6762749445676275,"Proof. HIMP (when using injective neighborhood aggregators instead of their original sum aggrega-
tors and an MLP with a sufficiently large number of layers such that it can approximate the HASH
function) exactly models performing the WL test on an HLG augmented graph. Hence, HIMP is
exactly as powerful as FR-WL with a fragmentation scheme based on rings and edges."
OTHER,0.6784922394678492,"D.4.4
FragNet"
OTHER,0.6807095343680709,"Next, we consider the expressiveness of our FragNet model:
Theorem E.1. FragNets are at most as powerful as HLG-WL. Additionally, when using injective
neighborhood aggregators and a sufficient number of layers, FragNets are as powerful as HLG-WL."
OTHER,0.6829268292682927,"Proof. For the proof, we rely on Lemma D.12, the finding that an explicit edge representation does
not augment expressiveness. Notice that our model, when using injective neighborhood aggregators
and an MLP with a sufficiently large number of layers such that it can approximate the HASH
function, exactly models performing the WL test on an HLG and ER augmented graph. As shown in
Lemma D.12, ER does not change the expressiveness. Hence, our model is exactly as powerful as
HLG-WL."
OTHER,0.6851441241685144,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.6873614190687362,"D.4.5
Topological GNNs"
OTHER,0.6895787139689579,"We will now consider topological GNNs. We will start by comparing HLG-WL with CWL, a variant
of the WL test operating on CW complexes [3]. In the CWL framework, every graph is (permutation
invariantly) mapped to a set of cells X, a CW complex (using a skeleton-preserving lifting map).
Let Xi denote the set of cells with dimension i. Then, X0 corresponds to all vertices V and X1 to
all edges E. For higher dimensions, the results depend on the particular cellular lifting map. For
instance, X2 could correspond to all cycles.
Theorem D.18. HLG-WL is not less powerful than CWL, with a fragmentation scheme F that
corresponds to the cellular lifting map used by CWL."
OTHER,0.6917960088691796,"Proof. Let F be the fragmentation that corresponds to X without the vertices: F = X \ V. Let
c(t), b(t) be the coloring of iteration t of HLG-WL and CWL, respectively. We will show that
b(t) ⊑c(2t) which implies that b ⊑c after termination."
OTHER,0.6940133037694013,"We will show this by induction over t. For t = 0, this follows immediately from the fact that the node
features in HLG-WL are finer than the features of cells in CWL."
OTHER,0.6962305986696231,Now we will show c(t) ⊑b(2t) assuming c(t−1) ⊑b(2t−2):
OTHER,0.6984478935698448,"The idea of the induction step is that the hash update function HASH receives more information in
HLG-WL compared to CWL. Let us first consider vertices, i.e., X0. The update function in CWL for
v ∈V = X0 is:"
OTHER,0.7006651884700665,"b(t)
v
= HASH

b(t−1)
v
, {{(b(t−1)
w
, b(t−1)
e
) | e = {v, w} ∈E}}
"
OTHER,0.7028824833702882,Now note that the update function in HLG-WL for v ∈V is:
OTHER,0.70509977827051,"c(2t)
v
= HASH

c(2t−1)
v
, {{c(2t−1)
w
| w ∈NHLG(G)(v)}}
"
OTHER,0.7073170731707317,"⊑HASH

c(2t−1)
v
, {{c(2t−1)
e
| e = {v, w} ∈E}}
"
OTHER,0.7095343680709535,"⊑HASH

c(2t−1)
v
, {{(c(2t−2)
e
, c(2t−2)
w
) | e = {v, w} ∈E}}
"
OTHER,0.7117516629711752,"⊑HASH

b(t−1)
v
, {{(b(t−1)
e
, b(t−1)
w
) | e = {v, w} ∈E}}
"
OTHER,0.7139689578713969,"= b(t)
v"
OTHER,0.7161862527716186,The first step follows from e ∈NHLG(G)(v) as the edges are part of the fragmentation F. The second
OTHER,0.7184035476718403,"step follows from c(2t−1)
e
⊑(c(2t−2)
e
, c(2t−2)
w
, c(2t−2
v
). The third step uses the induction hypothesis."
OTHER,0.720620842572062,"Now, we will consider a cell x ∈Xk ⊆F. The update function in CWL is"
OTHER,0.7228381374722838,"b(t)
x
= HASH

b(t−1)
x
, {{(b(t−1)
u
, b(t−1)
o
) | x ≺u, o ≺u}}, {{b(t−1)
l
| l ≺x}}
"
OTHER,0.7250554323725056,"where x ≺y means that x with dimension k is part of the cell y of dimension k + 1. For example,
e ≺r if e is an edge in a ring r ∈Xk+1. For details, we refer to Bodnar et al. [2]."
OTHER,0.7272727272727273,The update function in HLG-WL for a fragment x ∈F ∩Xk in G′ := HLG(G) is:
OTHER,0.729490022172949,"c(2t)
x
= HASH

c(2t−1)
x
, {{c(2t−1)
w
| w ∈NG′(x)}}
"
OTHER,0.7317073170731707,"⊑HASH

c(2t−1)
x
, {{c(2t−1)
u
| u ∈NG′(x) ∩Xk+1}}, {{c(2t−1)
l
| l ∈NG′(x) ∩Xk−1}}
"
OTHER,0.7339246119733924,"⊑HASH

c(2t−1)
x
, {{c(2t−1)
u
| x ≺u}}, {{c(2t−1)
l
| l ≺x}}
"
OTHER,0.7361419068736141,"⊑HASH

c(2t−1)
x
, {{(c(2t−2)
u
, c(2t−2)
o
) | x ≺u, o ≺u}}, {{c(2t−1)
l
| l ≺x}}
"
OTHER,0.738359201773836,"⊑HASH

b(t−1)
x
, {{(b(t−1)
u
, b(t−1)
o
) | x ≺u, o ≺u}}, {{b(t−1)
l
| l ≺x}}
"
OTHER,0.7405764966740577,"= b(t)
x"
OTHER,0.7427937915742794,"The steps are very similar to the vertex case above. This concludes the proof that c refines b. By
Lemma D.5 this implies that HLG-WL is not less powerful than CWL using a fragmentation that
corresponds to the cellular complex."
OTHER,0.7450110864745011,"As CWL bounds the expressiveness of CIN [3] and CIN++ [21], we get the following corollary:"
OTHER,0.7472283813747228,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.7494456762749445,"Corollary D.19. CIN and CIN++ are at most as powerful as HLG-WL with a fragmentation scheme
that corresponds to the cellular lifting map."
OTHER,0.7516629711751663,"Additionally, CWL subsumes the WL version, SWL, introduced by Bodnar et al. [2] for simplicial
complexes. The cellular complex just corresponds to all cliques of the graph.
Corollary D.20. HLG-WL, with a fragmentation scheme recovering cliques, is not less powerful
than SWL."
OTHER,0.753880266075388,"As MPSNs [2] are bounded by SWL, we have the following result for MPSNs:
Corollary D.21. MPSNs are at most as powerful as HLG-WL with a fragmentation scheme based on
cliques."
OTHER,0.7560975609756098,"E
Model"
OTHER,0.7583148558758315,In the following we explicitly describe the update and output mechanisms of our model.
OTHER,0.7605321507760532,"The
learned
representation
ht
v
at
step
or
layer
t
of
a
node
receives
a
mes-
sage
from
neighboring
nodes
mt
V→v
and
fragments
mt
F →v.
Similarly,
the
learned
representation
ht
f
of
a
fragment
receives
a
message
from
neighboring
fragments mt
F →f and nodes mt
V→f."
OTHER,0.7627494456762749,"mt
V→v = AGG

{{MLP(ht−1
u
, ht−1
e
) | e = {u, v} ∈E}}

mt
F →v = AGG

{{ht−1
f
| v ∈f, f ∈F}}
"
OTHER,0.7649667405764967,"mt
F →f = AGG

{{ht−1
g
| g ∈NF (f)}}

mt
V→f = AGG

{{ht−1
v
| v ∈f, v ∈V}}
"
OTHER,0.7671840354767184,"where NF (f) denotes the neighbors of fragment f in the higher-level graph of fragments. The
hidden representations are updated by combining the incoming messages with the previous hidden
representation:"
OTHER,0.7694013303769401,"ht
v = MLP(ht−1
v
, mt
V→v, mt
F →v)
ht
f = MLP(ht−1
f
, mt
V→f, mt
F →f)
ht
e = MLP(ht−1
e
, ht−1
u
, ht−1
v
)"
OTHER,0.7716186252771619,"The final graph-level readout after T layers is computed by aggregating the multiset of node represen-
tation, edge representations, and fragment representations:"
OTHER,0.7738359201773836,"OUT({{hT
v | v ∈V}}, {{hT
e | e ∈E}}, {{hT
f | f ∈F}})"
OTHER,0.7760532150776053,"Note that the complexity of our FragNet model is linear in the number of nodes and fragments
(assuming that each node is only part of a constant number of fragments)."
OTHER,0.7782705099778271,"Additionally, our FragNet model achieves the highest expressiveness in our Fragment-WL hierarchy
and also compared to other fragment-biased GNNs.
Theorem E.1. FragNets are at most as powerful as HLG-WL. Additionally, when using injective
neighborhood aggregators and a sufficient number of layers, FragNets are as powerful as HLG-WL."
OTHER,0.7804878048780488,"F
Further experiments"
OTHER,0.7827050997782705,"F.1
Long-Range tests"
OTHER,0.7849223946784922,We provide more experiments to measure the long-range capabilities of FragNet.
OTHER,0.7871396895787139,Commute time
OTHER,0.7893569844789357,"In addition to the recovery rates in ??, we also consider commute times. The commute time between
the nodes a and b is the expected time for a random walker from a to reach b and return again to
a. Di Giovanni et al. [11] have proposed the commute time as a measure for over-squashing. To
compute and compare commute times across different fragmentations, we connected all nodes in
each fragmentation that could exchange a message within one layer. Figure 6 shows the commute
from the star node to every other node for the same graph as in ??. The close alignment between
commute time and recovery rate supports the theoretical findings by Di Giovanni et al. [11] and
further emphasizes the potentially enhanced long-range capabilities of our model. Additionally, we"
OTHER,0.7915742793791575,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.7937915742793792,"Figure 6: Commute time from the star node to all other nodes. The first graph has no fragmentation,
the second one a rings fragmentation (like in CIN/CIN++), the third a rings and paths fragmentation
(like our model)."
OTHER,0.7960088691796009,"also compute commute times on a molecule from the ZINC dataset that contains more fragments (see
Figure 7)."
OTHER,0.7982261640798226,"Quantitatively, we compute average commute times on a random sample of molecules from the
peptides dataset for a model without any fragmentation and for FragNet (RingsPaths fragmentation
with a higher-level graph). We observe that the addition of a higher-level graph reduces commute
times by 16%."
OTHER,0.8004434589800443,"Table 3: Average commute times between all nodes on a random sample of 50 molecular graphs
from the peptides dataset with and without the higher-level graph."
OTHER,0.802660753880266,"Normal Molecular Graph
5056
Molecular Graph + HLG
4253"
OTHER,0.8048780487804879,"F.2
Distribution of Fragments"
OTHER,0.8070953436807096,"Figure 8 illustrates the distribution of fragment sizes, i.e., path lengths and ring lengths, extracted by
our RingsPaths fragmentation method across the ZINC-10k, ZINC-full, and peptides datasets. It is
worth noting that the peptides dataset features some exceptionally large rings."
OTHER,0.8093126385809313,"F.3
Ablation Studies"
OTHER,0.811529933481153,"In the following, we test the design choices of our model and fragmentation. First, we test FragNet
without the different fragment information or ordinal encoding on ZINC and Peptides. We show the
results in Table 4. We observe that a reduction in expressiveness generally leads to a reduction in
performance. An exception is the use of fragment representations (FR-WL, i.e. FragNet - Higher-level
graph) which shows a higher error on ZINC 10k and Peptides Struct. This is similar to the pattern
that we observe in the message reconstruction toy experiment we show in ?? where the additional
fragment representations increase the importance of the current substructure and do not contain a
message from other parts of the molecule."
OTHER,0.8137472283813747,"Furthermore, we compare different fragmentation schemes in combination with FragNet. In Table 5,
we observe that our RingsPath fragmentation scheme performs the best across the different datasets."
OTHER,0.8159645232815964,"In Figure 9, we look at how large the vocabulary size has to be per fraction of fragmented atoms. That
is, for an increasing vocabulary, we observce how many atoms belong to a fragment. The steeper the
increase the better. We can observe that on ZINC-10k BBB, BRICS and Rings are not able to assign
a fragment to each atom no matter how large the vocabulary size. Magnet achieves full fragmentation
but slower compared to our RingsPaths which is the most vocabulary efficient. We further show the
necessary vocabulary size on ZINC-Full and Peptides for RingsPaths in Table 6."
OTHER,0.8181818181818182,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.8203991130820399,"Figure 7: Commute time from the star node to all other nodes. The first graph has no fragmentation,
the second one a rings fragmentation (like in CIN/CIN++), the third a rings and paths fragmentation
(like our model)."
OTHER,0.8226164079822617,"G
Experimental details"
OTHER,0.8248337028824834,"In the following, we will describe details for all our experiments. Unless otherwise stated, for our
FragNet, we use a 2-layer fully connected neural network with ReLU activations and batch norm
as the MLP update function. For the aggregation method AGG, we use a sum aggregation for
messages within the original or higher-level graph and a mean aggregation for messages between
the original graph and the higher-level graph. For training, we use the AdamW [35] optimizer and
gradient clipping with a value of 1. The model has been implemented in PyTorch [41] using the
PyTorchGeometric [16] and the PyTorch Lightning [15] library. It is in parts adapted from HIMP
[17]. All results for other methods are taken from Rampášek et al. [43] and Giusti et al. [21]."
OTHER,0.8270509977827051,"G.1
ZINC and peptides"
OTHER,0.8292682926829268,"We compare our model against standard GNNs, like GCN [32], GIN [49] or GatedGCN [5], higher-
order GNNs such as RingGNN [8] or 3WLGNN [37], topological GNNs, especially CIN and CIN++
[2; 3] and other fragment-biased GNNs such as HIMP [17] and GSN [4]. While our main focus
lies on the evaluation against other message-passing GNNs, especially the group of fragment-biased
GNNs, we compare against Transformer architectures for completeness. We test Graphormer [50],"
OTHER,0.8314855875831486,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.8337028824833703,"Figure 8: Distribution of sizes of path and ring fragments for the a) ZINC-10k, b) ZINC-full, and c)
Peptides dataset."
OTHER,0.835920177383592,"Table 4: Ablation of different expressivity choices for FragNet. Additionally, we ablate the ordinal
encoding."
OTHER,0.8381374722838137,"Model
ZINC
Peptides"
OTHER,0.8403547671840355,"10k (MAE ↓)
Struct (MAE ↓)
Func (AP ↑)"
OTHER,0.8425720620842572,"FragNet (=HLG-WL)
0.0775 ± 0.004
0.246 ± 0.002
0.668 ± 0.003
−Higher-level graph (=FR-WL)
0.0872 ± 0.004
0.256 ± 0.003
0.661 ± 0.005
−Fragment representation (=NF-WL)
0.0994 ± 0.007
0.247 ± 0.003
0.654 ± 0.005
−All fragment information (=WL)
0.1609 ± 0.003
0.249 ± 0.001
0.652 ± 0.005"
OTHER,0.844789356984479,"FragNet −ordinal encoding
0.0945 ± 0.006
0.249 ± 0.001
0.666 ± 0.004"
OTHER,0.8470066518847007,"GPS [43], GT [13], SAN [34] and GRIT [36]. The detailed results can be found in Table 7 and
Table 8. The hyperparameters of our model for ZINC (10k and full) and peptides (struct and func)
can be found in 9. Note that we adhere to the 500K parameter budget. Each experiment is repeated
over three different seeds except for the ZINC-full experiment, where we only have a single run
because of computational and time limitations."
OTHER,0.8492239467849224,"G.2
Expressiveness"
OTHER,0.8514412416851441,"We use the MagNet [24] fragmentation to fragment all graphs in the ZINC-subset dataset. We
sort the fragments by number of occurrences in the training set. For each of the 28 most common
substructure we train our model to predict the counts of these substructures. As model parameters we"
OTHER,0.8536585365853658,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.8558758314855875,Table 5: Performance of FragNet with different fragmentation schemes.
OTHER,0.8580931263858093,"Fragmentation Scheme
ZINC
Peptides"
OTHER,0.8603104212860311,"10k (MAE ↓)
Struct (MAE ↓)
Func (AP ↑)"
OTHER,0.8625277161862528,"BBB
0.127
0.252 ± 0.002
0.637 ± 0.003
BRICS
0.127
0.247 ± 0.008
0.658 ± 0.011
Magnet
0.098
-
-"
OTHER,0.8647450110864745,"Rings
0.078
0.249 ± 0.001
0.659 ± 0.007
RingsPaths (ours)
0.077
0.246 ± 0.002
0.668 ± 0.005"
OTHER,0.8669623059866962,"Figure 9: Fraction of atoms in ZINC-10k dataset that are part of a fragment as a function of
vocabulary size. A fraction of 1 indicates that all molecules in the dataset can be completely
fragmented. We compare the chemically inspired fragmentation schemes BBB, BRICS, and MagNet
with a fragmentation based just on rings and our RingsPaths fragmentation. The substructures in the
vocabulary are sorted by the frequency in which they appear in the molecules."
OTHER,0.8691796008869179,"use three layers of message passing with a hidden dimension of 120. For the final readout function
we use a sum aggregation and a two layer MLP. We train our model using the MAE loss for 200
epochs with a learning rate of 0.001 and a reduce-on-plateau learning rate scheduling. We report the
accuracy (percentage of graphs where rounded prediction equals the ground-truth count) on the test
set. Table 10 shows the complete table of all substructures."
OTHER,0.8713968957871396,"G.3
Long-range Interaction: Recovery rate"
OTHER,0.8736141906873615,"In our synthetic long-range experiment, we consider a graph consisting of two rings connected by
a path ??. One node in the graph is the designated source node (marked by a star). The feature of
the source node is initialized with one-hot-encoding of one of 10 different classes. All other node
features are initialized with a constant encoding. For every node t in the graph, we train a separate
model to predict the class of the source node s, i.e. the target node t has to reconstruct a message
from the source node. The number of layers of the models is max(d(s, t), 3), ensuring that the target
can receive messages from the source. We train the model with the cross entropy loss between the
prediction at the target node and the true class of the source. We compare the results of models that
have no fragmentation, a ring fragmentation and a ring-path fragmentation. We use a our model
without batchnorm and a hidden dimension of 64. We train the model for a maximum of 200 epochs
with a starting learn rate of 0.001 and average the results over at least five seeds."
OTHER,0.8758314855875832,"G.4
Generalization: Leave 7-rings out"
OTHER,0.8780487804878049,"Table 11 shows the training and test errors of the experiment where we remove all molecules
containing 7-rings from the training set."
OTHER,0.8802660753880266,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.8824833702882483,Table 6: Vocabulary sizes for RingsPaths on different datasets.
OTHER,0.88470066518847,"ZINC-10k
ZINC-Full
Peptides"
OTHER,0.8869179600886918,"Vocabulary Size
18
28
100"
OTHER,0.8891352549889135,"Table 7: Predictive performance for multiple models on Peptides-struct and func. Best Transformer
and best GNN are highlighted."
OTHER,0.8913525498891353,"Type
Model
Peptides-"
OTHER,0.893569844789357,"Struct
Func
(MAE ↓)
(AP ↑)"
OTHER,0.8957871396895787,Transformer
OTHER,0.8980044345898004,"GPS
0.2500 ± 0.0012
0.6535 ± 0.0041
SAN+LapPE
0.2683 ± 0.0043
0.6384 ± 0.0121
SAN+RWSE
0.2545 ± 0.0012
0.6439 ± 0.0075
GRIT
0.2460 ± 0.0012
0.6988 ± 0.0082"
OTHER,0.9002217294900222,Basic GNNs
OTHER,0.9024390243902439,"GCN
0.3496 ± 0.0013
0.5930 ± 0.0023
GIN
0.3547 ± 0.0045
0.5498 ± 0.0079
GatedGCN
0.3420 ± 0.0013
0.5864 ± 0.0035
GatedGCN+RWSE
0.3357 ± 0.0006
0.6069 ± 0.0035"
OTHER,0.9046563192904656,"Topological
CIN++
0.2523 ± 0.0013
0.6569 ± 0.0117"
OTHER,0.9068736141906873,"Fragment-Biased
HIMP
0.2503 ± 0.0008
0.5668 ± 0.0149
FragNet (ours)
0.2462 ± 0.0021
0.6678 ± 0.005"
OTHER,0.9090909090909091,"G.5
Generalization: Rarity"
OTHER,0.9113082039911308,"For the experiment in Table 12, we report the MAE of the ZINC-full validation set grouped by the
frequency of the rarest fragment in the molecule. The frequency of a fragment is defined as the
fraction of molecules that contain the fragment. As the fragmentation scheme, we use the simple
Rings fragmentation."
OTHER,0.9135254988913526,"G.6
Generalization: QM9"
OTHER,0.9157427937915743,"To perform our generalization experiment on QM9, we transform the edge and node features of the
molecular graphs in QM9 so that they have the same node features and edge features as the graphs in
the ZINC dataset. Additionally, we do not use any molecular graphs that contain atom types that do
not appear in the ZINC dataset. We calculate penalized logP as ground truth. Then, we trained our
model and GRIT on ZINC-full and tested them on the transformed QM9 dataset."
OTHER,0.917960088691796,"H
Downstream tasks using substructures"
OTHER,0.9201773835920177,"Many other molecular tasks beyond property prediction can benefit from substructure information,
highlighting the broader potential applications of our model."
OTHER,0.9223946784922394,"Motifs for Drug Discovery Motifs and specific substructures are important inductive biases in
molecular generation, optimization, and scaffolding tasks [25; 45; 12]. Employing a set of fragments
can simplify the generation process and increase the chemical validity of the generated molecules.
Given a fragmentation procedure, the fragments are aggregated into a vocabulary of motifs through
complete enumeration of the dataset [29; 30; 20], top-k selection [33; 38] or consolidation into
Murcko scaffolds [24]. Encoders for molecule generation often integrate motifs as node features or
via additional higher-level encoder networks, as the decoder is explicitly tasked with reconstructing
the set of motifs from a given embedding."
OTHER,0.9246119733924612,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.926829268292683,"Table 8: Predictive performance for multiple models on ZINC 10k and ZINC full. Best Transformer
and best GNN are highlighted."
OTHER,0.9290465631929047,"Type
Model
ZINC"
OTHER,0.9312638580931264,"10k
Full
(MAE ↓)
(MAE ↓)"
OTHER,0.9334811529933481,Transformer
OTHER,0.9356984478935698,"Graphormer
0.122 ± 0.006
0.052 ± 0.005
GPS
0.070 ± 0.006
-
GT
0.226 ± 0.014
-
SAN
0.139 ± 0.006
-
Graphormer-URPE
0.086 ± 0.007
0.028 ± 0.002
Graphormer-GD
0.081 ± 0.009
0.025 ± 0.004
GRIT
0.059 ± 0.002
0.023 ± 0.001"
OTHER,0.9379157427937915,Basic GNNs
OTHER,0.9401330376940134,"GCN
0.367 ± 0.011
0.113 ± 0.002
GIN
0.526 ± 0.051
0.088 ± 0.002
GAT
0.384 ± 0.007
0.111 ± 0.002
GraphSAGE
0.398 ± 0.002
0.126 ± 0.003"
OTHER,0.9423503325942351,"Higher-order
RingGNN
0.353 ± 0.019
-
3WLGNN
0.303 ± 0.068
-"
OTHER,0.9445676274944568,"Topological
CIN-Small
0.094 ± 0.004
0.044 ± 0.003
CIN++
0.077 ± 0.004
0.027 ± 0.007"
OTHER,0.9467849223946785,Fragment-Biased
OTHER,0.9490022172949002,"HIMP
0.151 ± 0.006
0.036 ± 0.002
GSN
0.115 ± 0.012
-
Autobahn
0.106 ± 0.004
0.029 ± 0.001
FragNet (ours)
0.0775 ± 0.005
0.0237 ± 0.00"
OTHER,0.9512195121951219,"Pretaining In the context of using GNNs for drug discovery, incorporating motifs as part of a
pretraining phase has been shown to improve representation learning capabilities. Zang et al. [51]
integrates higher-level structures as nodes in a graph and leverages the graph’s hierarchy for self-
supervised pretraining. Similarly, Zhang et al. [53] propose a GNN that operates on a two-tiered
graph and predicts the sequence of motifs during network pretraining. To improve the encoding of
higher-level structures, Inae et al. [28] suggest a motif-aware pretraining technique, which masks
entire motifs during the pretraining phase."
OTHER,0.9534368070953437,"I
Future Work"
OTHER,0.9556541019955654,"Future work could address several limitations of our current method. First, our fragmentation
approach and ordinal encoding, while effective for molecular graphs, are not well-suited to large,
densely connected graphs such as citation or social networks, where the generation of numerous
meaningless fragments can introduce significant noise. Second, while our generalization experiments
demonstrate superior performance compared to GRIT, the results in Table 12 indicate that GRIT
outperforms our method on molecules with frequent fragments. Future efforts could focus on refining
fragment-biased models to enhance their performance in these scenarios."
OTHER,0.9578713968957872,Expressivity and Generalization: Fragment-Biases for Molecular GNNs
OTHER,0.9600886917960089,"peptides-struct
peptides-func
ZINC-10k
ZINC-full"
OTHER,0.9623059866962306,"num_layers
3
2
5
3
hidden_channels
110
128
64
120
num_layers_out
3
3
3
2
frag-reduction
sum
sum
max
max
out-reduction
mean
mean
mean
mean
dropout
0.05
0.15
0
0
lr
0.001
0.001
0.001
0.001
weight decay
0
0
0.001
0
ema decay
0.99
0.99
0.99
0.99
scheduler
Cosine
ReduceonPlateau
Cosine
ReduceonPlateau
patience
-
30
-
15
factor
-
0.5
-
0.9
batchsize
32
128
32
128
max epochs
300
400
2000
1000
num parameters
440K
440K
221K
494K"
OTHER,0.9645232815964523,Table 9: Hyperparameter configuration of our model for the ZINC and peptides benchmarks
OTHER,0.9667405764966741,"Table 10: Fragment counts for the 42 most common MagNet fragments in ZINC and accuracy scores
of our model in predicting the counts."
OTHER,0.9689578713968958,Fragment
OTHER,0.9711751662971175,"Count
12862
7548
6198
5629
3904
2204
1799
1772
1348
1330
1071
741
573
375
Accuracy
1.0
0.997
0.999
0.986
0.99
0.969
0.999
1.0
0.963
0.997
0.997
0.933
0.999
0.954"
OTHER,0.9733924611973392,Fragment
OTHER,0.975609756097561,"Count
208
204
176
156
113
113
90
80
77
66
54
45
37
32
Accuracy
0.999
0.988
0.983
0.982
0.996
0.995
0.993
0.991
0.998
0.995
0.996
0.996
0.996
0.998"
OTHER,0.9778270509977827,Fragment
OTHER,0.9800443458980045,"Count
32
31
28
25
23
19
19
18
18
17
15
15
15
13
Accuracy
0.998
0.998
0.996
0.997
1.0
0.998
0.997
0.998
1.0
1.0
0.998
0.998
1.0
0.999"
OTHER,0.9822616407982262,"Table 11: We remove all molecules that contain 7-rings from the training set and test on all molecules,
i.e., also the ones with 7-rings."
OTHER,0.9844789356984479,"Model
ZINC 10k"
OTHER,0.9866962305986696,"training
test
(MAE ↓)
(MAE ↓)"
OTHER,0.9889135254988913,"GRIT
0.02
0.61"
OTHER,0.991130820399113,"FragNet (ours)
0.08
0.34"
OTHER,0.9933481152993349,"Table 12: Comparison of the MAE of GRIT and our model on ZINC-full. Graphs are grouped by the
frequency of their rarest fragment."
OTHER,0.9955654101995566,"Frequency of
< 0.1%
< 1%
< 10%
≥10%
rarest Fragment"
OTHER,0.9977827050997783,"HIMP (MAE)
14.4
0.48
0.15
0.030
GRIT (MAE)
9.5
0.26
0.026
0.018
FragNet (ours) (MAE)
5.3
0.15
0.045
0.021"
