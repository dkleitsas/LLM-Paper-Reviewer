Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016501650165016502,"Despite the impressive numerical performance of quasi-Newton and Ander-
1"
ABSTRACT,0.0033003300330033004,"son/nonlinear acceleration methods, their global convergence rates have remained
2"
ABSTRACT,0.0049504950495049506,"elusive for over 50 years. This paper addresses this long-standing question by
3"
ABSTRACT,0.006600660066006601,"introducing a framework that derives novel and adaptive quasi-Newton or non-
4"
ABSTRACT,0.00825082508250825,"linear/Anderson acceleration schemes. Under mild assumptions, the proposed
5"
ABSTRACT,0.009900990099009901,"iterative methods exhibit explicit, non-asymptotic convergence rates that blend
6"
ABSTRACT,0.01155115511551155,"those of gradient descent and Cubic Regularized Newton’s method. Notably, these
7"
ABSTRACT,0.013201320132013201,"rates are achieved adaptively, as the method autonomously determines the optimal
8"
ABSTRACT,0.01485148514851485,"step size using a simple backtracking strategy. The proposed approach also includes
9"
ABSTRACT,0.0165016501650165,"an accelerated version that improves the convergence rate on convex functions.
10"
ABSTRACT,0.018151815181518153,"Numerical experiments demonstrate the efficiency of the proposed framework,
11"
ABSTRACT,0.019801980198019802,"even compared to a fine-tuned BFGS algorithm with line search.
12"
INTRODUCTION,0.02145214521452145,"1
Introduction
13"
INTRODUCTION,0.0231023102310231,"Consider the problem of finding the minimizer x⋆of the unconstrained minimization problem
14"
INTRODUCTION,0.024752475247524754,"f(x⋆) = f ⋆= min
x∈Rd f(x),"
INTRODUCTION,0.026402640264026403,"where d is the problem’s dimension, and the function f has a Lipschitz continuous Hessian.
15"
INTRODUCTION,0.028052805280528052,"Assumption 1. The function f(x) has a Lipschitz continuous Hessian with a constant L,
16"
INTRODUCTION,0.0297029702970297,"∀y, z ∈Rd,
∥∇2f(z) −∇2f(y)∥≤L∥z −y∥.
(1)"
INTRODUCTION,0.03135313531353135,"In this paper, ∥.∥stands for the maximal singular value of a matrix and for the ℓ2 norm for a vector.
17"
INTRODUCTION,0.033003300330033,"Many twice-differentiable problems like logistic or least-squares regression satisfy Assumption 1.
18"
INTRODUCTION,0.034653465346534656,"The Lipschitz continuity of the Hessian is crucial when analyzing second-order algorithms, as it
19"
INTRODUCTION,0.036303630363036306,"extends the concept of smoothness to the second order. The groundbreaking work by Nesterov et al.
20"
INTRODUCTION,0.037953795379537955,"[45] has sparked a renewed interest in second-order methods, revealing the remarkable convergence
21"
INTRODUCTION,0.039603960396039604,"rate improvement of Newton’s method on problems satisfying Assumption 1 when augmented with
22"
INTRODUCTION,0.041254125412541254,"cubic regularization. For instance, if the problem is also convex, accelerated gradient descent typically
23"
INTRODUCTION,0.0429042904290429,achieves O( 1
INTRODUCTION,0.04455445544554455,"t2 ), while accelerated second-order methods achieve O( 1"
INTRODUCTION,0.0462046204620462,"t3 ). Recent advancements have
24"
INTRODUCTION,0.04785478547854786,"further pushed the boundaries, achieving even faster convergence rates of up to O(
1
t7/2 ) through the
25"
INTRODUCTION,0.04950495049504951,"utilization of hybrid methods [42, 14] or direct acceleration of second-order methods [43, 27, 39].
26"
INTRODUCTION,0.05115511551155116,"Unfortunately, second-order methods may not always be feasible, particularly in high-dimensional
27"
INTRODUCTION,0.052805280528052806,"problems common in machine learning. The limitation is that exact second-order methods require
28"
INTRODUCTION,0.054455445544554455,"solving a linear system that involves the Hessian of the function f. This main limitation motivated
29"
INTRODUCTION,0.056105610561056105,"alternative approaches that balance the efficiency of second-order methods and the scalability of
30"
INTRODUCTION,0.057755775577557754,"first-order methods, such as inexact/subspace/stochastic techniques, nonlinear/Anderson acceleration,
31"
INTRODUCTION,0.0594059405940594,"and quasi-Newton methods.
32"
CONTRIBUTIONS,0.06105610561056106,"1.1
Contributions
33"
CONTRIBUTIONS,0.0627062706270627,"Despite the impressive numerical performance of quasi-Newton methods and nonlinear acceleration
34"
CONTRIBUTIONS,0.06435643564356436,"schemes, there is currently no knowledge about their global explicit convergence rates. In fact, global
35"
CONTRIBUTIONS,0.066006600660066,"convergence cannot be guaranteed without using either exact or Wolfe-line search techniques. This
36"
CONTRIBUTIONS,0.06765676567656766,"raises the following long-standing question that has remained unanswered for over 50 years:
37"
CONTRIBUTIONS,0.06930693069306931,"What are the non-asymptotic global convergence rates of quasi-Newton
38"
CONTRIBUTIONS,0.07095709570957096,"and Anderson/nonlinear acceleration methods?
39"
CONTRIBUTIONS,0.07260726072607261,"This paper provides a partial answer by introducing generic updates (see algorithms 1 to 3) that can
40"
CONTRIBUTIONS,0.07425742574257425,"be viewed as cubic-regularized quasi-Newton methods or regularized nonlinear acceleration schemes.
41"
CONTRIBUTIONS,0.07590759075907591,"Under mild assumptions, the iterative methods constructed within the proposed framework (see
42"
CONTRIBUTIONS,0.07755775577557755,"algorithms 3 and 6) exhibit explicit, global and non-asymptotic convergence rates that interpolate the
43"
CONTRIBUTIONS,0.07920792079207921,"one of first order and second order methods (more details in appendix A):
44"
CONTRIBUTIONS,0.08085808580858085,• Convergence rate on non-convex problems (Theorem 4): mini ∥∇f(xi)∥≤O(t−2
CONTRIBUTIONS,0.08250825082508251,3 + t−1
CONTRIBUTIONS,0.08415841584158416,"3 ),
45"
CONTRIBUTIONS,0.0858085808580858,"• Convergence rate on (star-)convex problems (Theorems 5 and 6): f(xt) −f ⋆≤O(t−2 + t−1),
46"
CONTRIBUTIONS,0.08745874587458746,"• Accelerated rate on convex problems (Theorem 8): f(xt) −f ⋆≤O(t−3 + t−2).
47"
RELATED WORK,0.0891089108910891,"1.2
Related work
48"
RELATED WORK,0.09075907590759076,"Inexact, subspace, and stochastic methods.
Instead of explicitly computing the Hessian matrix
49"
RELATED WORK,0.0924092409240924,"and Newton’s step, these methods compute an approximation using sampling [2], inexact Hessian
50"
RELATED WORK,0.09405940594059406,"computation [29, 19], or random subspaces [20, 31, 34]. By adopting a low-rank approximation for the
51"
RELATED WORK,0.09570957095709572,"Hessian, these approaches substantially reduce per-iteration costs without significantly compromising
52"
RELATED WORK,0.09735973597359736,"the convergence rate. The convergence speed in such cases often represents an interpolation between
53"
RELATED WORK,0.09900990099009901,"the rates observed in gradient descent methods and (cubic) Newton’s method.
54"
RELATED WORK,0.10066006600660066,"Nonlinear/Anderson acceleration.
Nonlinear acceleration techniques, including Anderson accel-
55"
RELATED WORK,0.10231023102310231,"eration [1], have a long standing history [3, 4, 28]. Driven by their promising empirical performance,
56"
RELATED WORK,0.10396039603960396,"they recently gained interest in their convergence analysis [61, 26, 60, 37, 66, 64, 69, 68, 53, 62,
57"
RELATED WORK,0.10561056105610561,"63, 6, 57, 8, 54]. In essence, Anderson acceleration is an optimization technique that enhances
58"
RELATED WORK,0.10726072607260725,"convergence by extrapolating a sequence of iterates using a combination of previous gradients and
59"
RELATED WORK,0.10891089108910891,"corresponding iterates. Comprehensive reviews and analyses of these techniques can be found in
60"
RELATED WORK,0.11056105610561057,"notable sources such as [37, 7, 36, 35, 5, 17]. However, these methods do not generalize well outside
61"
RELATED WORK,0.11221122112211221,"quadratic minimization and their convergence rate can only be guaranteed asymptotically when using
62"
RELATED WORK,0.11386138613861387,"a line-search or regularization techniques [59, 65, 53].
63"
RELATED WORK,0.11551155115511551,"Quasi-Newton methods.
Quasi-Newton schemes are renowned for their exceptional efficiency
64"
RELATED WORK,0.11716171617161716,"in continuous optimization. These methods replace the exact Hessian matrix (or its inverse) in
65"
RELATED WORK,0.1188118811881188,"Newton’s step with an approximation that is updated iteratively during the method’s execution. The
66"
RELATED WORK,0.12046204620462046,"most widely used algorithms in this category include DFP [18, 25] and BFGS [58, 30, 24, 10, 9].
67"
RELATED WORK,0.12211221122112212,"Most of the existing convergence results predominantly focus on the asymptotic super-linear rate of
68"
RELATED WORK,0.12376237623762376,"convergence [67, 32, 12, 11, 15, 22, 72, 70, 71]. However, recent research on quasi-Newton updates
69"
RELATED WORK,0.1254125412541254,"has unveiled explicit and non-asymptotic rates of convergence [49, 51, 50, 40, 41]. Nonetheless,
70"
RELATED WORK,0.12706270627062707,"these analyses suffer from several significant drawbacks, such as assuming an infinite memory
71"
RELATED WORK,0.12871287128712872,"size and/or requiring access to the Hessian matrix. These limitations fundamentally undermine the
72"
RELATED WORK,0.13036303630363036,"essence of quasi-Newton methods, which are typically designed to be Hessian-free and maintain low
73"
RELATED WORK,0.132013201320132,"per-iteration cost through their low-memory requirement and low-rank structure.
74"
RELATED WORK,0.13366336633663367,"Recently, Kamzolov et al. [38] introduced an adaptive regularization technique combined with
75"
RELATED WORK,0.1353135313531353,"cubic regularization, with global, explicit (accelerated) convergence rates for any quasi-Newton
76"
RELATED WORK,0.13696369636963696,"method. The method incorporates a backtracking line search on the secant inexactness inequality
77"
RELATED WORK,0.13861386138613863,"that introduces a quadratic regularization. However, this algorithm relies on prior knowledge of the
78"
RELATED WORK,0.14026402640264027,"Lipschitz constant specified in Assumption 1. Unfortunately, the paper does not provide an adaptive
79"
RELATED WORK,0.1419141914191419,"method to find jointly the Lipschitz constant as well, as it is a priory too costly to know which
80"
RELATED WORK,0.14356435643564355,"parameter to update. This aspect makes the method impractical in real-world scenarios.
81"
RELATED WORK,0.14521452145214522,"Paper Organization
Section 2 introduces the proposed novel generic updates and some essential
82"
RELATED WORK,0.14686468646864687,"theoretical results. Section 3 presents the convergence analysis of the iterative algorithm, which
83"
RELATED WORK,0.1485148514851485,"uses one of the proposed updates. Section 4 is dedicated to the accelerated version of the proposed
84"
RELATED WORK,0.15016501650165018,"framework. Section 5 presents examples of methods generated by the proposed framework.
85"
TYPE-I AND TYPE-II STEP,0.15181518151815182,"2
Type-I and Type-II Step
86"
TYPE-I AND TYPE-II STEP,0.15346534653465346,"This section first examines a remarkable property shared by quasi-Newton and Anderson acceleration:
87"
TYPE-I AND TYPE-II STEP,0.1551155115511551,"the sequence of iterates of these methods can be expressed as a combination of directions formed by
88"
TYPE-I AND TYPE-II STEP,0.15676567656765678,"previous iterates and the current gradient. Building upon this observation, section 2.1 investigates
89"
TYPE-I AND TYPE-II STEP,0.15841584158415842,"how to obtain second-order information without directly computing the Hessian of the function f by
90"
TYPE-I AND TYPE-II STEP,0.16006600660066006,"approximating the Hessian within the subspace formed by these directions. Subsequently, section 2.2
91"
TYPE-I AND TYPE-II STEP,0.1617161716171617,"demonstrates how to utilize this approximation to establish an upper bound for the function f and its
92"
TYPE-I AND TYPE-II STEP,0.16336633663366337,"gradient norm ∥∇f(x)∥. Minimizing these upper bounds, respectively, leads to a type-I and type-II
93"
TYPE-I AND TYPE-II STEP,0.16501650165016502,"method.
94"
TYPE-I AND TYPE-II STEP,0.16666666666666666,"Motivation: what quasi-Newton and nonlinear acceleration schemes actually do?
The BFGS
95"
TYPE-I AND TYPE-II STEP,0.16831683168316833,"update is a widely used quasi-Newton method for unconstrained optimization. It approximates the
96"
TYPE-I AND TYPE-II STEP,0.16996699669966997,"inverse Hessian matrix using updates based on previous gradients and iterates. The update reads
97"
TYPE-I AND TYPE-II STEP,0.1716171617161716,"xt+1 = xt −htHt∇f(xt), Ht = Ht−1

I −gtdT
t
gT
t dt"
TYPE-I AND TYPE-II STEP,0.17326732673267325,"
+ dt

dT
t
dT
t gt+gT
t Ht−1dt
(gT
t dt)2
−gT
t Ht−1"
TYPE-I AND TYPE-II STEP,0.17491749174917492,"gT
t dt "
TYPE-I AND TYPE-II STEP,0.17656765676567657,"where Ht is the approximation of the inverse Hessian at iteration t, ht is the step size, dt = xt −xt−1
98"
TYPE-I AND TYPE-II STEP,0.1782178217821782,"is the step direction, gt = ∇f(xt) −∇f(xt−1) is the gradient difference. After unfolding the
99"
TYPE-I AND TYPE-II STEP,0.17986798679867988,"equation, the BFGS update can be seen as a combination of the di’s and ∇f(xt),
100"
TYPE-I AND TYPE-II STEP,0.18151815181518152,"xt+1 −xt = H0P0 . . . Pt∇f(xt) + Pt
i=1 αidi,
(2)"
TYPE-I AND TYPE-II STEP,0.18316831683168316,"where Pi are projection matrices in Rd×d and αi are coefficients. Similar reasoning can be applied to
101"
TYPE-I AND TYPE-II STEP,0.1848184818481848,"other quasi-Newton formulas (see appendix B for more details).
102"
TYPE-I AND TYPE-II STEP,0.18646864686468648,"This observation aligns with the principles of Anderson acceleration methods. Considering the same
103"
TYPE-I AND TYPE-II STEP,0.18811881188118812,"vectors dt and gt, Anderson acceleration updates xt+1 as:
104"
TYPE-I AND TYPE-II STEP,0.18976897689768976,"α⋆= minα ∥∇f(xt) + Pt−1
i=0 αiri∥,
xt+1 −xt = Pt
i=0 α⋆
i (di −htgi) ,"
TYPE-I AND TYPE-II STEP,0.19141914191419143,"where ht is the relaxation parameter, which can be seen as the step size of the method. As all
105"
TYPE-I AND TYPE-II STEP,0.19306930693069307,"xi’s belong to the span of previous gradients, the update is similar to (2), see appendix B for more
106"
TYPE-I AND TYPE-II STEP,0.19471947194719472,"details. This is not surprising, as it has been shown that Anderson acceleration can be viewed as a
107"
TYPE-I AND TYPE-II STEP,0.19636963696369636,"quasi-Newton method [23]. Some studies have explored the relationship between these two classes
108"
TYPE-I AND TYPE-II STEP,0.19801980198019803,"of optimization techniques and established strong connections in terms of their algorithmic behavior
109"
TYPE-I AND TYPE-II STEP,0.19966996699669967,"[23, 73, 56, 13].
110"
TYPE-I AND TYPE-II STEP,0.20132013201320131,"Hence, quasi-Newton algorithms and nonlinear/Anderson acceleration methods utilize previous
111"
TYPE-I AND TYPE-II STEP,0.20297029702970298,"directions di and the current gradient ∇f(xt) in subsequent iterations. However, their convergence
112"
TYPE-I AND TYPE-II STEP,0.20462046204620463,"is guaranteed only if a line search is used, and their convergence speed is heavily dependent on H0
113"
TYPE-I AND TYPE-II STEP,0.20627062706270627,"(quasi-Newton) or ht (Anderson acceleration) [48].
114"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.2079207920792079,"2.1
Error Bounds on the Hessian-Vector Product Approximation by a Difference of Gradients
115"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.20957095709570958,"Consider the following d × N matrices that represent the algorithm’s memory,
116"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.21122112211221122,"Y = [y1, . . . , yN],
Z = [z1, . . . , zN],
D = Y −Z,
G = [. . . , ∇f(yi) −∇f(zi), . . .].
(3)"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.21287128712871287,"For example, to mimic quasi-Newton techniques, the matrices Y and Z can be defined such that,
117"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.2145214521452145,"D = [. . . , xt−i+1 −xt−i, . . .],
G = [. . . , ∇f(xt−i+1) −∇f(xt−i), . . .], i = 1 . . . N."
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.21617161716171618,"Motivated by (2), this paper studies the following update, defined as a linear combination of the
118"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.21782178217821782,"previous directions di,
119"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.21947194719471946,"x+ −x = Dα
where
α ∈RN.
(4)
The objective is to determine the optimal coefficients α based on the information contained in the
120"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.22112211221122113,"matrices defined in (3). Notably, the absence of the gradient in the update (4) distinguishes this
121"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.22277227722772278,"approach from (2), allowing for the development of an adaptive method that eliminates the need for
122"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.22442244224422442,"an initial matrix H0 (quasi-Newton methods) or a mixing parameter ht (Anderson acceleration).
123"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.22607260726072606,"Under assumption (1), the following bounds hold for all x, y, z, x+ ∈Rd [45],
124"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.22772277227722773,∥∇f(y) −∇f(z) −∇2f(z)(y −z)∥≤L
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.22937293729372937,"2 ∥y −z∥2,
(5)
f(x+) −f(x) −∇f(x)(x+ −x) −1"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.23102310231023102,"2(x+ −x)T ∇2f(x)(x+ −x)
 ≤L"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.23267326732673269,"6 ∥x+ −x∥3.
(6)"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.23432343234323433,"The accuracy of the estimation of the matrix ∇2f(x), depends on the error vector ε,
125"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.23597359735973597,"ε
def
= [ε1, . . . , εN],
and
εi
def
= ∥di∥(∥di∥+ 2∥zi −x∥) .
(7)"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.2376237623762376,"The following Theorem 1 explicitly bounds the error of approximating ∇2f(x)D by G.
126"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.23927392739273928,"Theorem 1. Let the function f satisfy Assumption 1. Let x+ be defined as in (4) and the matrices
127"
ERROR BOUNDS ON THE HESSIAN-VECTOR PRODUCT APPROXIMATION BY A DIFFERENCE OF GRADIENTS,0.24092409240924093,"D, G be defined as in (3) and vector ε as in (7). Then, for all w ∈Rd and α ∈RN,
128 −L∥w∥"
PN,0.24257425742574257,"2
PN
i=1 |αi|εi ≤wT (∇2f(x)D −G)α ≤L∥w∥"
PN,0.24422442244224424,"2
PN
i=1 |αi|εi,
(8)"
PN,0.24587458745874588,∥wT (∇2f(x)D −G)∥≤L∥w∥
PN,0.24752475247524752,"2
∥ε∥.
(9)"
PN,0.24917491749174916,"Proof sketch and interpretation.
The theorem states that the Hessian-vector product ∇2f(x)(y−z)
129"
PN,0.2508250825082508,"can be approximated by the difference of gradients ∇f(y) −∇f(z), providing a cost-effective
130"
PN,0.2524752475247525,"approach to estimate ∇2f without computing it. This property is the basis of quasi-Newton methods.
131"
PN,0.25412541254125415,"The detailed proof can be found in appendix F. The main idea of the proof is as follows. From (5)
132"
PN,0.25577557755775576,"with y = yi and z = zi, writing di = yi −zi, and Assumption 1,
133"
PN,0.25742574257425743,∥∇f(yi) −∇f(zi) −∇2f(x)(yi −zi)∥≤L
PN,0.2590759075907591,2 ∥di∥2 + ∥∇2f(x) −∇2f(z)∥∥di∥≤L 2 εi.
PN,0.2607260726072607,"The first term in εi bounds the error of (5), while the second comes from the distance between (5)
134"
PN,0.2623762376237624,"and the current point x where the Hessian is estimated. Then, it suffices to combine the inequalities
135"
PN,0.264026402640264,"with coefficients α to obtain Theorem 1.
136"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.26567656765676567,"2.2
Type I and Type II Inequalities and Methods
137"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.26732673267326734,"In the literature, Type-I methods often refer to algorithms that aim to minimize the function value
138"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.26897689768976896,"f(x), while type-II methods minimize the gradient norm ∥∇f(x)∥[23, 73, 13]. Applying the bounds
139"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2706270627062706,"(6) and (5) to the update in (4) yields the following Type-I and Type-II upper bounds, respectively.
140"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2722772277227723,"Theorem 2. Let the function f satisfy Assumption 1. Let x+ be defined as in (4), the matrices D, G
141"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2739273927392739,"be defined as in (3) and ε be defined as in (7). Then, for all α ∈RN,
142"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2755775577557756,f(x+) ≤f(x) + ∇f(x)T Dα + αT Hα
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.27722772277227725,"2
+ L∥Dα∥3"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.27887788778877887,"6
,
H
def
= GT D+DT G+IL∥D∥∥ε∥"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.28052805280528054,"2
(10)"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.28217821782178215,∥∇f(x+)∥≤∥∇f(x) + Gα∥+ L
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2838283828382838,"2
 PN
i=1 |αi|εi + ∥Dα∥2
,
(11)"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2854785478547855,"The proof can be found in appendix F. Minimizing eqs. (10) and (11) leads to algorithms 1 and 2,
143"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2871287128712871,"respectively, whose constant L is replaced by a parameter M, found by backtracking line-search. A
144"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2887788778877888,"study of the (strong) link between these proposed algorithms and nonlinear/Anderson acceleration
145"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.29042904290429045,"and quasi-Newton methods can be found in appendix B.
146"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.29207920792079206,"Solving the sub-problems
In algorithms 1 and 2, the coefficients α are computed by solving a
147"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.29372937293729373,"minimization sub-problem in O(N 3 + Nd) (see appendix C for more details). Usually, N is rather
148"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2953795379537954,"small (e.g. between 5 and 100); hence solving the subproblem is negligible compared to computing a
149"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.297029702970297,"new gradient ∇f(x). Here is the summary:
150"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.2986798679867987,"• In algorithm 1, the subproblem can be solved easily by a convex problem in two variables,
151"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.30033003300330036,"which involves an eigenvalue decomposition of the matrix H ∈RN×N [45].
152"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.30198019801980197,"• In algorithm 2, the subproblem can be cast into a linear-quadratic problem of O(N)
153"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.30363036303630364,"variables and constraints that can be solved efficiently with SDP solvers (e.g., SDPT3).
154"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.30528052805280526,"Algorithm 1 Type-I Subroutine with Backtracking Line-search
Require: First-order oracle for f, matrices G, D, vector ε, iterate x, initial smoothness M0."
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.3069306930693069,1: Initialize M ←M0
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.3085808580858086,"2
2: do
3:
M ←2M
and
H ←GT D+DT G"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.3102310231023102,"2
+ IN
M∥D∥∥ε∥"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.3118811881188119,"2
4:
α⋆←arg minα f(x) + ∇f(x)T Dα + 1"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.31353135313531355,2αT Hα + M∥Dα∥3
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.31518151815181517,"6
5:
x+ ←x + Dα"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.31683168316831684,6: while f(x+) ≥f(x) + ∇f(x)T Dα⋆+ 1
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.3184818481848185,2[α⋆]T Hα⋆+ M∥Dα⋆∥3
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.3201320132013201,"6
7: return x+, M"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.3217821782178218,Algorithm 2 Type-II Subroutine with Backtracking Line-search
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.3234323432343234,"Same as algorithm 1, but minimize and check the upper bound (11) instead of (10) on lines 4 and 6."
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.3250825082508251,"3
Iterative Type-I Method: Framework and Rates of Convergences
155"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.32673267326732675,"The rest of the paper analyzes the convergence rate of methods that use algorithm 1 as a subroutine;
156"
TYPE I AND TYPE II INEQUALITIES AND METHODS,0.32838283828382836,"see algorithm 3. The analysis of methods that uses algorithm 2 is left for future work.
157"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.33003300330033003,"3.1
Main Assumptions and Design Requirements
158"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3316831683168317,"This section lists the important assumptions on the function f. Some subsequent results require an
159"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3333333333333333,"upper bound on the radius of the sub-level set of f at f(x0).
160"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.334983498349835,"Assumption 2. The radius of the sub-level set {x : f(x) ≤f(x0)} is bounded by R < ∞.
161"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.33663366336633666,"To ensure the convergence toward f(x⋆), some results require f to be star-convex or convex.
162"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.33828382838283827,"Assumption 3. The function f is star convex if, for all x ∈Rd and ∀τ ∈[0, 1],
163"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.33993399339933994,f((1 −τ)x + τx⋆) ≤(1 −τ)f(x) + τf(x⋆).
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3415841584158416,"Assumption 4. The function f is convex if, for all y, z ∈Rd, f(y) ≥f(z) + ∇f(z)(y −z).
164"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3432343234323432,"The matrices Y, Z, D must meet some conditions listed below as ""requirements"" (see section 5 for
165"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3448844884488449,"details). All convergence results rely on one of these conditions on the projector onto span(D),
166"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3465346534653465,"Pt
def
= Dt(DT
t Dt)−1DT
t .
(12)"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3481848184818482,"Requirement 1a. For all t, the projector Pt of the stochastic matrix Dt satisfies E[Pt] = N"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.34983498349834985,"d I.
167"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.35148514851485146,"Requirement 1b. For all t, the projector Pt satisfies Pt∇f(xt) = ∇f(xt).
168"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.35313531353135313,"The first condition guarantees that, in expectation, the matrix Dt spans partially the gradient ∇f(xt),
169"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3547854785478548,since E[Pt∇f(xt)] = N
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3564356435643564,"d ∇f(xt). The second condition simply requires the possibility to move
170"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3580858085808581,"towards the current gradient when taking the step x + Dα. This condition resonates with the idea
171"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.35973597359735976,"presented in (2), where the step x+ −x combines previous directions and the current gradient ∇f(xt).
172"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3613861386138614,"In addition, it is required that the norm of ∥ε∥does not grow too quickly, hence the next assumption.
173"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.36303630363036304,"Requirement 2. For all t, the relative error ∥εt∥"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.36468646864686466,"∥Dt∥is bounded by δ.
174"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.36633663366336633,"The Requirement 2 is also non-restrictive, as it simply prevents taking secant equations at yi −zi and
175"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.367986798679868,"zi −xi too far apart. Most of the time, δ satisfies δ ≤O(R).
176"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3696369636963696,"Finally, the condition number of the matrix D also has to be bounded.
177"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3712871287128713,"Requirement 3. For all t, the matrix Dt is full-column rank, which implies that DT
t Dt is invertible.
178"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.37293729372937295,"In addition, its condition number κDt
def
=
p"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.37458745874587457,"∥DT
t Dt∥∥(DT
t Dt)−1∥is bounded by κ.
179"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.37623762376237624,"The condition on the rank of D is not overly restrictive. In most practical scenarios, this condition is
180"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3778877887788779,"typically satisfied without issue. However, the second condition might be hard to meet, but section 5
181"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3795379537953795,"studies strategies that prevent κD from exploding by taking orthogonal directions or pruning D.
182"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3811881188118812,"Algorithm 3 Generic Iterative Type-I Methods
Require: First-order oracle f, initial iterate and smoothness x0, M0, number of iterations T."
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.38283828382838286,"for t = 0, . . . , T −1 do"
MAIN ASSUMPTIONS AND DESIGN REQUIREMENTS,0.3844884488448845,"Update Gt, Dt, εt (see section 5).
xt+1, Mt+1 ←[algorithm 1](f, Gt, Dt, εt, xt, (Mt/2))
end for
return xT"
RATES OF CONVERGENCE,0.38613861386138615,"3.2
Rates of Convergence
183"
RATES OF CONVERGENCE,0.38778877887788776,"When f satisfies Assumption 1, algorithm 3 ensures a minimal function decrease at each step.
184"
RATES OF CONVERGENCE,0.38943894389438943,"Theorem 3. Let f satisfy Assumption 1. Then, at each iteration t ≥0, algorithm 3 achieves
185"
RATES OF CONVERGENCE,0.3910891089108911,f(xt+1) ≤f(xt) −Mt+1
RATES OF CONVERGENCE,0.3927392739273927,"12 ∥xt+1 −xt∥3,
Mt+1 < max

2L ;
M0"
T,0.3943894389438944,"2t
	
.
(13)"
T,0.39603960396039606,"Under some mild assumptions, algorithm 3 converges to a critical point for non-convex functions.
186"
T,0.3976897689768977,"Theorem 4. Let f satisfy Assumption 1, and assume that f is bounded below by f ∗. Let Require-
187"
T,0.39933993399339934,"ments 1b to 3 hold, and Mt ≥Mmin. Then, algorithm 3 starting at x0 with M0 achieves
188"
T,0.400990099009901,"min
i=1, ..., t∥∇f(xi)∥≤max"
T,0.40264026402640263,"(
3L
t2/3"
T,0.4042904290429043,"
12f(x0) −f ⋆ Mmin"
T,0.40594059405940597,"2/3
;
 C1 t1/3"
T,0.4075907590759076," 
12f(x0) −f ⋆ Mmin 1/3) ,"
T,0.40924092409240925,"where C1 = δL

κ+2κ2"
T,0.41089108910891087,"2

+ maxi∈[0,t] ∥(I −Pi)∇2f(xi)Pi∥."
T,0.41254125412541254,"Going further, algorithm 3 converges to an optimum when the function is star-convex.
189"
T,0.4141914191419142,"Theorem 5. Assume f satisfy Assumptions 1 to 3. Let Requirements 1b to 3 hold. Then, algorithm 3
190"
T,0.4158415841584158,"starting at x0 with M0 achieves, for t ≥1,
191"
T,0.4174917491749175,"(f(xt) −f ⋆) ≤6
f(xt) −f ⋆"
T,0.41914191419141916,"t(t + 1)(t + 2) +
1
(t + 1)(t + 2)
L(3R)3"
T,0.4207920792079208,"2
+
1
t + 2
C2(3R)2 4
,"
T,0.42244224422442245,"where
C2
def
= δL κ+2κ2"
T,0.4240924092409241,"2
+ maxi∈[0,t] ∥∇2f(xi) −Pi∇2f(xi)Pi∥."
T,0.42574257425742573,"Finally, the next theorem shows that when algorithm 3 uses a stochastic D that satisfies Require-
192"
T,0.4273927392739274,"ment 1a, then f(xt) also converges in expectation to f(x⋆) when f is convex.
193"
T,0.429042904290429,"Theorem 6. Assume f satisfy Assumptions 1, 2 and 4. Let Requirements 1a, 2 and 3 hold. Then, in
194"
T,0.4306930693069307,"expectation over the matrices Di, algorithm 3 starting at x0 with M0 achieves, for t ≥1,
195"
T,0.43234323432343236,"EDt[f(xt) −f ⋆] ≤
1 1 + 1 4
 N"
T,0.43399339933993397,"d t
3 (f(x0) −f ⋆) +
1
 N"
T,0.43564356435643564,"d t
2
L(3R)3"
T,0.4372937293729373,"2
+
1
 N"
T,0.4389438943894389,"d t
 C3(3R)2 2
,"
T,0.4405940594059406,"where
C3
def
= δL κ+2κ2"
T,0.44224422442244227,"2
+ (d−N)"
T,0.4438943894389439,"d
maxi∈[0,t] ∥∇2f(xi)∥."
T,0.44554455445544555,"Interpretation
The rates presented in Theorems 4 to 6 combine the ones of cubic regularized
196"
T,0.4471947194719472,"Newton’s method and gradient descent (or coordinate descent, as in Theorem 6) for functions with
197"
T,0.44884488448844884,"Lipschitz-continuous Hessian. As C1, C2, and C3 decrease, the rates approach those of cubic Newton.
198"
T,0.4504950495049505,"The constants C1, C2, and C3 quantify the error of approximating D∇2f(x)D by H in (10) into
199"
T,0.4521452145214521,"two terms. The first represents the error made by approximating ∇2f(x)D by G, while the second
200"
T,0.4537953795379538,"describes the low-rank approximation of ∇2f(x) in the subspace spanned by the columns of D. The
201"
T,0.45544554455445546,"approximation is more explicit in C3, where increasing N reduces the constant up to N = d.
202"
T,0.4570957095709571,"To retrieve the convergence rate of Newton’s method with cubic regularization, the approximation
203"
T,0.45874587458745875,"needs to satisfy three properties: 1) the points contained in Yt and Zt must be close to each other,
204"
T,0.4603960396039604,"and to xt to reduce δ and ∥ε∥; 2) the condition number of D should be close to 1 to reduce κ; 3) D
205"
T,0.46204620462046203,"should span a maximum dimension in Rd to improve the approximation of ∇2f(x) by P∇2f(x)P.
206"
T,0.4636963696369637,"For example, Zt = xt1T
N, Dt = hIN with h small, and Yt = Zt + Dt achieve these conditions. This
207"
T,0.46534653465346537,"(naive) strategy estimates all directional second derivatives with a finite difference for all coordinates
208"
T,0.466996699669967,"and is equivalent to performing a Newton’s step in terms of complexity.
209"
T,0.46864686468646866,"Algorithm 4 Type-I subroutine with backtracking for the accelerated method
Require: First-order oracle f, matrices G, D, vector ε, iterate x, smoothness M0, minimal norm ∆"
T,0.47029702970297027,Initialize M ←M0
T,0.47194719471947194,"2 , γ ←1"
T,0.4735973597359736,"4
∥ε∥
∥D∥
 
1 + κ2
D

, ExitFlag ←False
while ExitFlag is False do"
T,0.4752475247524752,"Update M
and
H ←GT D+DT G"
T,0.4768976897689769,"2
+ IN
M∥D∥∥ε∥"
T,0.47854785478547857,"2
α∗←arg minα f(x) + ∇f(x)T Dα + 1"
T,0.4801980198019802,2αT Hα + M∥Dα∥3
T,0.48184818481848185,"6
x+ ←x + Dα"
T,0.4834983498349835,"If −∇f(x+)T Dα ≥∥∇f(x+)∥3/2
√"
M,0.48514851485148514,3M
M,0.4867986798679868,"4
and ∥Dα∥≥∆then ExitFlag ←LargeStep"
M,0.4884488448844885,"If −f(x+)T Dα ≥
∥∇f(x+)∥2"
M,0.4900990099009901,M(γ+ ∥Dα∥
M,0.49174917491749176,"2
) then ExitFlag ←SmallStep"
M,0.4933993399339934,"end while
return x+, α, M, γ, ExitFlag"
M,0.49504950495049505,"Algorithm 5 Adaptive Accelerated Type-I Algorithm (Sketch, see appendix D for the full version)
Require: First-order oracle f, initial iterate and smoothness x0, M0, number of iterations T."
M,0.4966996699669967,"Initialize G0, D0, ε0, λ(1)
0 , λ(2)
0 , ∆, x1, M1, (M0)1.
for t = 1, . . . , T −1 do"
M,0.49834983498349833,"Update Gt, Dt, εt.
do"
M,0.5,"Compute vt ←arg min Φt, set yt =
t
t+3xt +
3
t+3vt, and update (M0)t
{xt+1, ExitFlag} ←[algorithm 4](f, Gt, Dt, εt, yt, (M0)t, ∆)"
M,0.5016501650165016,if Φt+1(vt+1) ≤f(xt+1) then %% Parameters adjustment if needed
M,0.5033003300330033,"ValidBound ←False
if ExitFlag is SmallStep then λ(1)
t
←2λ(1)
t , otherwise λ(2)
t
←2λ(2)
t
else"
M,0.504950495049505,"ValidBound ←True %% Successful iteration
end if
while ValidBound is False
end for
return xT"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5066006600660066,"4
Accelerated Algorithm for Convex Functions
210"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5082508250825083,"This section introduces algorithm 5, an accelerated variant of algorithm 3 for convex functions,
211"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5099009900990099,"designed using the estimate sequence technique from [43]. It consists in iteratively building a
212"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5115511551155115,"function Φt(x), a regularized lower bound on f, that reads
213"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5132013201320133,"Φt(x) =
1
Pt
i=0 bi"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5148514851485149,"Pt
i=0 bi (f(xi) + ∇f(xi)(x −xi)) + λ(1)
t
∥x−x0∥2"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5165016501650165,"2
+ λ(2)
t
∥x−x0∥3 6

,"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5181518151815182,"where λ(1,2)
t
are non-decreasing. The key aspects of acceleration are as follows (see section 4 for more
214"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5198019801980198,"details): 1) The accelerated algorithm makes a step at a linear combination between vt, the optimum
215"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5214521452145214,"of Φt, and the previous iterate xt. 2) It uses a modified version of algorithm 1, see algorithm 4.
216"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.523102310231023,"3) Under some conditions, the step size can be considered as ""large"", i.e., similar to a cubic-Newton
217"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5247524752475248,"step. The ∆> 0 ensures the step is sufficiently large to ensure theoretical convergence - but setting
218"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5264026402640264,"∆= 0 does not seem to impact the numerical convergence. The presence of both small and large
219"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.528052805280528,"steps is crucial to obtain the theoretical rate of convergence.
220"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5297029702970297,"Theorem 7. Assume f satisfy Assumptions 1, 2 and 4. Let Requirements 1b to 3 hold. Then,
221"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5313531353135313,"algorithm 5 starting at x0 with M0 achieves, for all ∆> 0 and for t ≥1,
222"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.533003300330033,"f(xt) −f ⋆≤(M0)2
max
L  3R t + 3"
ACCELERATED ALGORITHM FOR CONVEX FUNCTIONS,0.5346534653465347,"2
+ 4(M0)max 3
√"
MAX,0.5363036303630363,"3
max

1 ;
2
∆
	  3R t + 3 3
+"
MAX,0.5379537953795379,˜λ(1)R2
MAX,0.5396039603960396,"2
+
˜λ(2)R3"
MAX,0.5412541254125413,"6
(t + 1)3
."
MAX,0.5429042904290429,"where ˜λ(1) = 0.5 · δ
 
Lκ + M1κ2
+ ∥∇f(x0) −P0∇f(x0)P0∥,
˜λ(2) = M1 + L,"
MAX,0.5445544554455446,(M0)max = L
MAX,0.5462046204620462,"2 (2∆+ (2κ2 + κ)δ) + (2
√"
MAX,0.5478547854785478,3 −1) max0≤i≤t ∥(I −Pi)∇2f(xi)Pi∥.
MAX,0.5495049504950495,"Interpretation
The interpretation is similar to the one from Section 3. Ignoring ˜λ(1,2), the rate of
223"
MAX,0.5511551155115512,"Theorem 7 combines the one of accelerated gradient and accelerated cubic Newton [44, 43]. The
224"
MAX,0.5528052805280528,"constant M0 blends the Lipschitz constant of the Hessian L with its approximation errors (2κ2 + κ)δ
225"
MAX,0.5544554455445545,"and ∥(I −P)∇2f(x)∥. The better the Hessian is approximated, the smaller the constant.
226"
MAX,0.5561056105610561,"5
Some update strategies for matrices Y, Z, D, G
227"
MAX,0.5577557755775577,"The framework presented in this paper is characterized by its generality, requiring only minimal
228"
MAX,0.5594059405940595,"assumptions on the matrix D and vector ε. This section explores different strategies for updating the
229"
MAX,0.5610561056105611,"matrices from (3), which can be classified into two categories: online and batch techniques.
230"
MAX,0.5627062706270627,"Recommended method. Among all the methods presented in this section, the most promising
231"
MAX,0.5643564356435643,"technique seems to be the Orthogonal Forward Estimates Only, as it ensures that the condition
232"
MAX,0.566006600660066,"number κD = 1 and the norm of the error vector ∥ε∥is small.
233"
ONLINE TECHNIQUES,0.5676567656765676,"5.1
Online Techniques
234"
ONLINE TECHNIQUES,0.5693069306930693,"The online technique updates the matrix D while algorithms 3 and 5 are running. To achieve
235"
ONLINE TECHNIQUES,0.570957095709571,"Requirement 1b, the method employs either a steepest or orthogonal forward estimate, defined as
236 xt+ 1"
ONLINE TECHNIQUES,0.5726072607260726,"2 = xt −h∇f(xt)
(steepest)
or
xt+ 1"
ONLINE TECHNIQUES,0.5742574257425742,2 = xt −h(I −Pt−1) ∇f(xt)
ONLINE TECHNIQUES,0.5759075907590759,"∥∇f(xt)∥
(orthogonal)."
ONLINE TECHNIQUES,0.5775577557755776,"Then, it include xt+ 1"
ONLINE TECHNIQUES,0.5792079207920792,"2 −xt in the matrix Dt. The projector Pt−1 is defined in (12), and parameter h
237"
ONLINE TECHNIQUES,0.5808580858085809,"can be a fixed small value (e.g., h = 10−9). This section investigates three different strategies for
238"
ONLINE TECHNIQUES,0.5825082508250825,"storing past information: Iterates only, Forward Estimates Only, and Greedy, listed below.
239"
ONLINE TECHNIQUES,0.5841584158415841,Yt = [xt+ 1
ONLINE TECHNIQUES,0.5858085808580858,"2 , xt, xt−1, . . . , xt−N+1],
Zt = [xt, xt−1, . . . , xt−N]
(Iterates only)"
ONLINE TECHNIQUES,0.5874587458745875,Yt = [xt+ 1
ONLINE TECHNIQUES,0.5891089108910891,"2 , xt−1"
ONLINE TECHNIQUES,0.5907590759075908,"2 , . . . , , xt−N+ 1"
ONLINE TECHNIQUES,0.5924092409240924,"2 ],
Zt = [xt, xt−1, . . . , xt−N]
(Forward Estimates Only)"
ONLINE TECHNIQUES,0.594059405940594,Yt = [xt+ 1
ONLINE TECHNIQUES,0.5957095709570958,"2 , xt, xt−1"
ONLINE TECHNIQUES,0.5973597359735974,"2 , . . . , xt−N+1"
ONLINE TECHNIQUES,0.599009900990099,"2 ],
Zt = [xt, xt−1"
ONLINE TECHNIQUES,0.6006600660066007,"2 , . . . , xt−N"
ONLINE TECHNIQUES,0.6023102310231023,"2 ]
(Greedy)"
ONLINE TECHNIQUES,0.6039603960396039,"Iterates only: In the case of quasi-Newton updates and Nonlinear/Anderson acceleration, the iterates
240"
ONLINE TECHNIQUES,0.6056105610561056,"are constructed using the equation xt+1 −xt ∈∇f(xt) + span{xt−i+1 −xt−i}i=1...N. The update
241"
ONLINE TECHNIQUES,0.6072607260726073,"draws inspiration from this observation. However, it does not provide control over the condition
242"
ONLINE TECHNIQUES,0.6089108910891089,"number of Dt or the norm ∥ε∥. To address this, one can either accept a potentially high condition
243"
ONLINE TECHNIQUES,0.6105610561056105,"number or remove the oldest points in D and G until the condition number is bounded (e.g., κ = 109).
244"
ONLINE TECHNIQUES,0.6122112211221122,"Forward Estimates Only: This method provides more control over the iterates added to Y and Z.
245"
ONLINE TECHNIQUES,0.6138613861386139,When using the orthogonal technique to compute xi+ 1
ONLINE TECHNIQUES,0.6155115511551155,"2 reduces the constants in Theorems 4, 5 and 7:
246"
ONLINE TECHNIQUES,0.6171617161716172,"the condition number of D is equal to 1 as DT D = h2I, and the norm of ε is small (∥ε∥≤O(h)).
247"
ONLINE TECHNIQUES,0.6188118811881188,"Greedy: The greedy approach involves storing both the iterates and the forward approximations. It
248"
ONLINE TECHNIQUES,0.6204620462046204,"shares the same drawback as the Iterates only strategy but retains at least the most recent information
249"
ONLINE TECHNIQUES,0.6221122112211221,"about the Hessian-vector product approximation, thereby reducing the ∥zi −xi∥term in ε (7).
250"
BATCH TECHNIQUES,0.6237623762376238,"5.2
Batch Techniques
251"
BATCH TECHNIQUES,0.6254125412541254,"Instead of making individual updates, an alternative approach is to compute them collectively, centered
252"
BATCH TECHNIQUES,0.6270627062706271,"on xt. This technique generates a matrix Dt consisting of N orthogonal directions d1, · · · , dN of
253"
BATCH TECHNIQUES,0.6287128712871287,"norm h. The corresponding Yt, Zt, Gt matrices are then defined as follows:
254"
BATCH TECHNIQUES,0.6303630363036303,"Yt = [xt + d1, . . . , xt + dn],
Zt = [xt, . . . , xt],
Gt = [. . . , ∇f(xt + di) −∇f(xt), . . .].
This section explores two batch techniques that generate orthogonal directions: Orthogonalization
255"
BATCH TECHNIQUES,0.6320132013201321,"and Random Subspace. Both lead to δ = 3h and κ = 1 in Requirements 2 and 3. However, they
256"
BATCH TECHNIQUES,0.6336633663366337,"require N additional gradient computations at each iteration (instead of one for the online techniques).
257"
BATCH TECHNIQUES,0.6353135313531353,"For clarity, in the experiments, only the Greedy version is considered.
258"
BATCH TECHNIQUES,0.636963696369637,"Orthogonalization:
This technique involves using any online technique discussed in the previous
259"
BATCH TECHNIQUES,0.6386138613861386,"section and storing the directions in a matrix ˜Dt. Then, it constructs the matrices Dt by performing
260"
BATCH TECHNIQUES,0.6402640264026402,"an orthogonalization procedure on ˜Dt, such as the QR algorithm. This approach provides Hessian
261"
BATCH TECHNIQUES,0.641914191419142,"estimates in relevant directions, which can be more beneficial than random ones.
262"
BATCH TECHNIQUES,0.6435643564356436,"0
100
200
300
400
500
10-10 10-5 100"
BATCH TECHNIQUES,0.6452145214521452,Iterate Only
BATCH TECHNIQUES,0.6468646864686468,Forward Estimate Only
BATCH TECHNIQUES,0.6485148514851485,Greedy
BATCH TECHNIQUES,0.6501650165016502,Random subspace
BATCH TECHNIQUES,0.6518151815181518,Orthogonalized greedy
BATCH TECHNIQUES,0.6534653465346535,Accelerated Forward Only
BATCH TECHNIQUES,0.6551155115511551,lbfgs minfunc
BATCH TECHNIQUES,0.6567656765676567,"0
100
200
300
400
500
10-10 10-5 100"
BATCH TECHNIQUES,0.6584158415841584,Iterate Only
BATCH TECHNIQUES,0.6600660066006601,Forward Estimate Only
BATCH TECHNIQUES,0.6617161716171617,Greedy
BATCH TECHNIQUES,0.6633663366336634,Random subspace
BATCH TECHNIQUES,0.665016501650165,Orthogonalized greedy
BATCH TECHNIQUES,0.6666666666666666,Accelerated Forward Only
BATCH TECHNIQUES,0.6683168316831684,lbfgs minfunc
BATCH TECHNIQUES,0.66996699669967,"Figure 1: Comparison between the type-1 methods proposed in this paper and the optimized imple-
mentation of ℓ-BFGS from minFunc [52] with default parameters, except for the memory size. All
methods use a memory size of N = 25."
BATCH TECHNIQUES,0.6716171617161716,"Random Subspace:
Inspired by [34], this technique randomly generates Dt at each iteration by
263"
BATCH TECHNIQUES,0.6732673267326733,"either taking Dt to be N random (rescaled) canonical vectors or by using the Q matrix from the QR
264"
BATCH TECHNIQUES,0.6749174917491749,"decomposition of a random N × D matrix. This ensures that Dt satisfies Requirement 1a. For clarity,
265"
BATCH TECHNIQUES,0.6765676567656765,"in the experiments, only the QR version is considered.
266"
NUMERICAL EXPERIMENTS,0.6782178217821783,"6
Numerical Experiments
267"
NUMERICAL EXPERIMENTS,0.6798679867986799,"This section compares the methods generated by this paper’s framework to the fine-tuned ℓ-BFGS
268"
NUMERICAL EXPERIMENTS,0.6815181518151815,"algorithm from minFunc [52]. More experiments are conducted in appendix E. The tested methods
269"
NUMERICAL EXPERIMENTS,0.6831683168316832,"are the Type-I iterative algorithms (algorithm 3 with the techniques from section 5). The step size
270"
NUMERICAL EXPERIMENTS,0.6848184818481848,"of the forward estimation was set to h = 10−9, and the condition number κDt is maintained below
271"
NUMERICAL EXPERIMENTS,0.6864686468646864,"κ = 109 with the iterates only and Greedy techniques. The accelerated algorithm 6 is used only with
272"
NUMERICAL EXPERIMENTS,0.6881188118811881,"the Forward Estimates Only technique. The compared methods are evaluated on a logistic regression
273"
NUMERICAL EXPERIMENTS,0.6897689768976898,"problem with no regularization on the Madelon UCI dataset [33]. The results are shown in fig. 1.
274"
NUMERICAL EXPERIMENTS,0.6914191419141914,"Regarding the number of iterations, the greedy orthogonalized version outperforms the others due to
275"
NUMERICAL EXPERIMENTS,0.693069306930693,"the orthogonality of directions (resulting in a condition number of one) and the meaningfulness of
276"
NUMERICAL EXPERIMENTS,0.6947194719471947,"previous gradients/iterates. However, in terms of gradient oracle calls, the recommended method,
277"
NUMERICAL EXPERIMENTS,0.6963696369636964,"orthogonal forward iterates only, achieves the best performance by striking a balance between the
278"
NUMERICAL EXPERIMENTS,0.698019801980198,"cost per iteration (only two gradients per iteration) and efficiency (small and orthogonal directions,
279"
NUMERICAL EXPERIMENTS,0.6996699669966997,"reducing theoretical constants). Surprisingly, the accelerated method’s performance is suboptimal,
280"
NUMERICAL EXPERIMENTS,0.7013201320132013,"possibly because it tightens the theoretical analysis, diminishing its inherent adaptivity.
281"
NUMERICAL EXPERIMENTS,0.7029702970297029,"7
Conclusion, Limitation, and Future work
282"
NUMERICAL EXPERIMENTS,0.7046204620462047,"This paper introduces a generic framework for developing novel quasi-Newton and Ander-
283"
NUMERICAL EXPERIMENTS,0.7062706270627063,"son/Nonlinear acceleration schemes, offering a global convergence rate in various scenarios, including
284"
NUMERICAL EXPERIMENTS,0.7079207920792079,"accelerated convergence on convex functions, with minimal assumptions and design requirements.
285"
NUMERICAL EXPERIMENTS,0.7095709570957096,"One limitation of the current approach is requiring an additional gradient step for the forward
286"
NUMERICAL EXPERIMENTS,0.7112211221122112,"estimate, as discussed in Section 5. However, this forward estimate is crucial in enabling the
287"
NUMERICAL EXPERIMENTS,0.7128712871287128,"algorithm’s adaptivity, eliminating the need to initialize a matrix H0 (quasi-Newton) or employ a
288"
NUMERICAL EXPERIMENTS,0.7145214521452146,"mixing parameter h0 (Anderson acceleration).
289"
NUMERICAL EXPERIMENTS,0.7161716171617162,"In future research, although unsuitable for large-scale problems, the method presented in this paper
290"
NUMERICAL EXPERIMENTS,0.7178217821782178,"can achieve super-linear convergence rates, as with infinite memory, they would be as fast as cubic
291"
NUMERICAL EXPERIMENTS,0.7194719471947195,"Newton methods. Utilizing the average-case analysis framework from existing literature, such as [47,
292"
NUMERICAL EXPERIMENTS,0.7211221122112211,"55, 21, 16, 46], could also improve the constants in Theorems 4 and 5 to match those in Theorem 6.
293"
NUMERICAL EXPERIMENTS,0.7227722772277227,"Furthermore, exploring convergence rates for type-2 methods, which are believed to be effective for
294"
NUMERICAL EXPERIMENTS,0.7244224422442245,"variational inequalities, is a worthwhile direction.
295"
NUMERICAL EXPERIMENTS,0.7260726072607261,"Ultimately, the results presented in this paper open new avenues for researchs. It may also provide a
296"
NUMERICAL EXPERIMENTS,0.7277227722772277,"potential foundation for investigating additional properties of existing quasi-Newton methods and
297"
NUMERICAL EXPERIMENTS,0.7293729372937293,"may even lead to the discovery of convergence rates for an adaptive, cubic-regularized BFGS variant.
298"
REFERENCES,0.731023102310231,"References
299"
REFERENCES,0.7326732673267327,"[1]
Donald G Anderson. “Iterative procedures for nonlinear integral equations”. In: Journal of the
300"
REFERENCES,0.7343234323432343,"ACM (JACM) 12.4 (1965), pp. 547–560.
301"
REFERENCES,0.735973597359736,"[2]
Kimon Antonakopoulos, Ali Kavis, and Volkan Cevher. “Extra-Newton: A First Approach to
302"
REFERENCES,0.7376237623762376,"Noise-Adaptive Accelerated Second-Order Methods”. In: arXiv preprint arXiv:2211.01832
303"
REFERENCES,0.7392739273927392,"(2022).
304"
REFERENCES,0.740924092409241,"[3]
Claude Brezinski. “Application de l’ε-algorithme à la résolution des systèmes non linéaires”.
305"
REFERENCES,0.7425742574257426,"In: Comptes Rendus de l’Académie des Sciences de Paris 271.A (1970), pp. 1174–1177.
306"
REFERENCES,0.7442244224422442,"[4]
Claude Brezinski. “Sur un algorithme de résolution des systèmes non linéaires”. In: Comptes
307"
REFERENCES,0.7458745874587459,"Rendus de l’Académie des Sciences de Paris 272.A (1971), pp. 145–148.
308"
REFERENCES,0.7475247524752475,"[5]
Claude Brezinski and Michela Redivo–Zaglia. “The genesis and early developments of Aitken’s
309"
REFERENCES,0.7491749174917491,"process, Shanks’ transformation, the ε–algorithm, and related fixed point methods”. In: Nu-
310"
REFERENCES,0.7508250825082509,"merical Algorithms 80.1 (2019), pp. 11–133.
311"
REFERENCES,0.7524752475247525,"[6]
Claude Brezinski, Michela Redivo-Zaglia, and Yousef Saad. “Shanks sequence transformations
312"
REFERENCES,0.7541254125412541,"and Anderson acceleration”. In: SIAM Review 60.3 (2018), pp. 646–669.
313"
REFERENCES,0.7557755775577558,"[7]
Claude Brezinski and M Redivo Zaglia. Extrapolation methods: theory and practice. Elsevier,
314"
REFERENCES,0.7574257425742574,"1991.
315"
REFERENCES,0.759075907590759,"[8]
Claude Brezinski et al. “Shanks and Anderson-type acceleration techniques for systems of
316"
REFERENCES,0.7607260726072608,"nonlinear equations”. In: arXiv:2007.05716 (2020).
317"
REFERENCES,0.7623762376237624,"[9]
Charles G Broyden. “The convergence of a class of double-rank minimization algorithms: 2.
318"
REFERENCES,0.764026402640264,"The new algorithm”. In: IMA journal of applied mathematics 6.3 (1970), pp. 222–231.
319"
REFERENCES,0.7656765676567657,"[10]
Charles George Broyden. “The convergence of a class of double-rank minimization algorithms
320"
REFERENCES,0.7673267326732673,"1. general considerations”. In: IMA Journal of Applied Mathematics 6.1 (1970), pp. 76–90.
321"
REFERENCES,0.768976897689769,"[11]
Richard H Byrd and Jorge Nocedal. “A tool for the analysis of quasi-Newton methods with
322"
REFERENCES,0.7706270627062707,"application to unconstrained minimization”. In: SIAM Journal on Numerical Analysis 26.3
323"
REFERENCES,0.7722772277227723,"(1989), pp. 727–739.
324"
REFERENCES,0.7739273927392739,"[12]
Richard H Byrd, Jorge Nocedal, and Ya-Xiang Yuan. “Global convergence of a cass of quasi-
325"
REFERENCES,0.7755775577557755,"Newton methods on convex problems”. In: SIAM Journal on Numerical Analysis 24.5 (1987),
326"
REFERENCES,0.7772277227722773,"pp. 1171–1190.
327"
REFERENCES,0.7788778877887789,"[13]
Marco Canini and Peter Richtárik. “Direct nonlinear acceleration”. In: Operational Research
328"
REFERENCES,0.7805280528052805,"2192 (2022), p. 4406.
329"
REFERENCES,0.7821782178217822,"[14]
Yair Carmon et al. “Recapp: Crafting a more efficient catalyst for convex optimization”. In:
330"
REFERENCES,0.7838283828382838,"International Conference on Machine Learning. PMLR. 2022, pp. 2658–2685.
331"
REFERENCES,0.7854785478547854,"[15]
Andrew R Conn, Nicholas IM Gould, and Ph L Toint. “Convergence of quasi-Newton matrices
332"
REFERENCES,0.7871287128712872,"generated by the symmetric rank one update”. In: Mathematical programming 50.1-3 (1991),
333"
REFERENCES,0.7887788778877888,"pp. 177–195.
334"
REFERENCES,0.7904290429042904,"[16]
Leonardo Cunha et al. “Only tails matter: Average-Case Universality and Robustness in the
335"
REFERENCES,0.7920792079207921,"Convex Regime”. In: 2022.
336"
REFERENCES,0.7937293729372937,"[17]
Alexandre d’Aspremont, Damien Scieur, Adrien Taylor, et al. “Acceleration methods”. In:
337"
REFERENCES,0.7953795379537953,"Foundations and Trends® in Optimization 5.1-2 (2021), pp. 1–245.
338"
REFERENCES,0.7970297029702971,"[18]
William C Davidon. “Variable metric method for minimization”. In: SIAM Journal on Opti-
339"
REFERENCES,0.7986798679867987,"mization 1.1 (1991), pp. 1–17.
340"
REFERENCES,0.8003300330033003,"[19]
Nikita Doikov, El Mahdi Chayti, and Martin Jaggi. “Second-order optimization with lazy
341"
REFERENCES,0.801980198019802,"Hessians”. In: arXiv preprint arXiv:2212.00781 (2022).
342"
REFERENCES,0.8036303630363036,"[20]
Nikita Doikov, Peter Richtárik, et al. “Randomized block cubic Newton method”. In: Interna-
343"
REFERENCES,0.8052805280528053,"tional Conference on Machine Learning. PMLR. 2018, pp. 1290–1298.
344"
REFERENCES,0.806930693069307,"[21]
Carles Domingo-Enrich, Fabian Pedregosa, and Damien Scieur. “Average-case acceleration
345"
REFERENCES,0.8085808580858086,"for bilinear games and normal matrices”. In: arXiv preprint arXiv:2010.02076 (2020).
346"
REFERENCES,0.8102310231023102,"[22]
John R Engels and Hector J Martinez. “Local and superlinear convergence for partially known
347"
REFERENCES,0.8118811881188119,"quasi-Newton methods”. In: SIAM Journal on Optimization 1.1 (1991), pp. 42–56.
348"
REFERENCES,0.8135313531353136,"[23]
Haw-Ren Fang and Yousef Saad. “Two classes of multisecant methods for nonlinear accelera-
349"
REFERENCES,0.8151815181518152,"tion”. In: Numerical Linear Algebra with Applications 16.3 (2009), pp. 197–221.
350"
REFERENCES,0.8168316831683168,"[24]
Roger Fletcher. “A new approach to variable metric algorithms”. In: The computer journal
351"
REFERENCES,0.8184818481848185,"13.3 (1970), pp. 317–322.
352"
REFERENCES,0.8201320132013201,"[25]
Roger Fletcher and Michael JD Powell. “A rapidly convergent descent method for minimiza-
353"
REFERENCES,0.8217821782178217,"tion”. In: The computer journal 6.2 (1963), pp. 163–168.
354"
REFERENCES,0.8234323432343235,"[26]
William F Ford and Avram Sidi. “Recursive algorithms for vector extrapolation methods”. In:
355"
REFERENCES,0.8250825082508251,"Applied numerical mathematics 4.6 (1988), pp. 477–489.
356"
REFERENCES,0.8267326732673267,"[27]
Alexander Gasnikov et al. “Near optimal methods for minimizing convex functions with
357"
REFERENCES,0.8283828382838284,"lipschitz p-th derivatives”. In: Conference on Learning Theory. PMLR. 2019, pp. 1392–1393.
358"
REFERENCES,0.83003300330033,"[28]
Eckart Gekeler. “On the solution of systems of equations by the epsilon algorithm of Wynn”.
359"
REFERENCES,0.8316831683168316,"In: Mathematics of Computation 26.118 (1972), pp. 427–436.
360"
REFERENCES,0.8333333333333334,"[29]
Saeed Ghadimi, Han Liu, and Tong Zhang. “Second-order methods with cubic regularization
361"
REFERENCES,0.834983498349835,"under inexact information”. In: arXiv preprint arXiv:1710.05782 (2017).
362"
REFERENCES,0.8366336633663366,"[30]
Donald Goldfarb. “A family of variable-metric methods derived by variational means”. In:
363"
REFERENCES,0.8382838283828383,"Mathematics of computation 24.109 (1970), pp. 23–26.
364"
REFERENCES,0.8399339933993399,"[31]
Robert Gower et al. “Rsn: Randomized subspace newton”. In: Advances in Neural Information
365"
REFERENCES,0.8415841584158416,"Processing Systems 32 (2019).
366"
REFERENCES,0.8432343234323433,"[32]
Andreas Griewank and Ph L Toint. “Local convergence analysis for partitioned quasi-Newton
367"
REFERENCES,0.8448844884488449,"updates”. In: Numerische Mathematik 39.3 (1982), pp. 429–448.
368"
REFERENCES,0.8465346534653465,"[33]
Isabelle Guyon. “Design of experiments of the NIPS 2003 variable selection benchmark”. In:
369"
REFERENCES,0.8481848184818482,"NIPS 2003 workshop on feature extraction and feature selection. Vol. 253. 2003.
370"
REFERENCES,0.8498349834983498,"[34]
Filip Hanzely et al. “Stochastic subspace cubic Newton method”. In: International Conference
371"
REFERENCES,0.8514851485148515,"on Machine Learning. PMLR. 2020, pp. 4027–4038.
372"
REFERENCES,0.8531353135313532,"[35]
K Jbilou and H Sadok. “Vector extrapolation methods. Applications and numerical com-
373"
REFERENCES,0.8547854785478548,"parison”. In: Journal of Computational and Applied Mathematics 122.1-2 (2000), pp. 149–
374"
REFERENCES,0.8564356435643564,"165.
375"
REFERENCES,0.858085808580858,"[36]
Khalide Jbilou and Hassane Sadok. “Analysis of some vector extrapolation methods for solving
376"
REFERENCES,0.8597359735973598,"systems of linear equations”. In: Numerische Mathematik 70.1 (1995), pp. 73–89.
377"
REFERENCES,0.8613861386138614,"[37]
Khalide Jbilou and Hassane Sadok. “Some results about vector extrapolation methods and
378"
REFERENCES,0.863036303630363,"related fixed-point iterations”. In: Journal of Computational and Applied Mathematics 36.3
379"
REFERENCES,0.8646864686468647,"(1991), pp. 385–398.
380"
REFERENCES,0.8663366336633663,"[38]
Dmitry Kamzolov et al. “Accelerated Adaptive Cubic Regularized Quasi-Newton Methods”.
381"
REFERENCES,0.8679867986798679,"In: arXiv preprint arXiv:2302.04987 (2023).
382"
REFERENCES,0.8696369636963697,"[39]
Dmitry Kovalev and Alexander Gasnikov. “The first optimal acceleration of high-order methods
383"
REFERENCES,0.8712871287128713,"in smooth convex optimization”. In: arXiv preprint arXiv:2205.09647 (2022).
384"
REFERENCES,0.8729372937293729,"[40]
Dachao Lin, Haishan Ye, and Zhihua Zhang. “Explicit convergence rates of greedy and random
385"
REFERENCES,0.8745874587458746,"quasi-Newton methods”. In: Journal of Machine Learning Research 23.162 (2022), pp. 1–40.
386"
REFERENCES,0.8762376237623762,"[41]
Dachao Lin, Haishan Ye, and Zhihua Zhang. “Greedy and random quasi-newton methods
387"
REFERENCES,0.8778877887788779,"with faster explicit superlinear convergence”. In: Advances in Neural Information Processing
388"
REFERENCES,0.8795379537953796,"Systems 34 (2021), pp. 6646–6657.
389"
REFERENCES,0.8811881188118812,"[42]
Renato DC Monteiro and Benar Fux Svaiter. “An accelerated hybrid proximal extragradient
390"
REFERENCES,0.8828382838283828,"method for convex optimization and its implications to second-order methods”. In: SIAM
391"
REFERENCES,0.8844884488448845,"Journal on Optimization 23.2 (2013), pp. 1092–1125.
392"
REFERENCES,0.8861386138613861,"[43]
Yurii Nesterov. “Accelerating the cubic regularization of Newton’s method on convex prob-
393"
REFERENCES,0.8877887788778878,"lems”. In: Mathematical Programming 112.1 (2008), pp. 159–181.
394"
REFERENCES,0.8894389438943895,"[44]
Yurii Nesterov. Introductory lectures on convex optimization. Springer, 2004.
395"
REFERENCES,0.8910891089108911,"[45]
Yurii Nesterov and Boris T Polyak. “Cubic regularization of Newton method and its global
396"
REFERENCES,0.8927392739273927,"performance”. In: Mathematical Programming 108.1 (2006), pp. 177–205.
397"
REFERENCES,0.8943894389438944,"[46]
Courtney Paquette et al. “Halting Time is predictable for large models: A universality property
398"
REFERENCES,0.8960396039603961,"and average-case analysis”. In: Foundations of Computational Mathematics (2022).
399"
REFERENCES,0.8976897689768977,"[47]
Fabian Pedregosa and Damien Scieur. “Acceleration through spectral density estimation”. In:
400"
REFERENCES,0.8993399339933993,"Proceedings of the 37th International Conference on Machine Learning (ICML). 2020.
401"
REFERENCES,0.900990099009901,"[48]
MJD Powell. “How bad are the BFGS and DFP methods when the objective function is
402"
REFERENCES,0.9026402640264026,"quadratic?” In: Mathematical Programming 34 (1986), pp. 34–47.
403"
REFERENCES,0.9042904290429042,"[49]
Anton Rodomanov and Yurii Nesterov. “Greedy quasi-Newton methods with explicit superlin-
404"
REFERENCES,0.905940594059406,"ear convergence”. In: SIAM Journal on Optimization 31.1 (2021), pp. 785–811.
405"
REFERENCES,0.9075907590759076,"[50]
Anton Rodomanov and Yurii Nesterov. “New results on superlinear convergence of classical
406"
REFERENCES,0.9092409240924092,"quasi-Newton methods”. In: Journal of optimization theory and applications 188 (2021),
407"
REFERENCES,0.9108910891089109,"pp. 744–769.
408"
REFERENCES,0.9125412541254125,"[51]
Anton Rodomanov and Yurii Nesterov. “Rates of superlinear convergence for classical quasi-
409"
REFERENCES,0.9141914191419142,"Newton methods”. In: Mathematical Programming (2021), pp. 1–32.
410"
REFERENCES,0.9158415841584159,"[52]
Mark Schmidt. “minFunc: unconstrained differentiable multivariate optimization in Matlab”.
411"
REFERENCES,0.9174917491749175,"In: Software available at http://www. cs. ubc. ca/˜ schmidtm/Software/minFunc. htm (2005).
412"
REFERENCES,0.9191419141914191,"[53]
Damien Scieur, Alexandre d’Aspremont, and Francis Bach. “Regularized nonlinear accelera-
413"
REFERENCES,0.9207920792079208,"tion”. In: Advances in Neural Information Processing Systems (NIPS). 2016.
414"
REFERENCES,0.9224422442244224,"[54]
Damien Scieur, Alexandre d’Aspremont, and Francis Bach. “Regularized nonlinear accelera-
415"
REFERENCES,0.9240924092409241,"tion”. In: Mathematical Programming (2020).
416"
REFERENCES,0.9257425742574258,"[55]
Damien Scieur and Fabian Pedregosa. “Universal Asymptotic Optimality of Polyak Momen-
417"
REFERENCES,0.9273927392739274,"tum”. In: Proceedings of the 37th International Conference on Machine Learning (ICML).
418"
REFERENCES,0.929042904290429,"2020.
419"
REFERENCES,0.9306930693069307,"[56]
Damien Scieur et al. “Generalization of Quasi-Newton methods: application to robust symmet-
420"
REFERENCES,0.9323432343234324,"ric multisecant updates”. In: International Conference on Artificial Intelligence and Statistics.
421"
REFERENCES,0.933993399339934,"PMLR. 2021, pp. 550–558.
422"
REFERENCES,0.9356435643564357,"[57]
Damien Scieur et al. “Online Regularized Nonlinear Acceleration”. In: arXiv:1805.09639
423"
REFERENCES,0.9372937293729373,"(2018).
424"
REFERENCES,0.9389438943894389,"[58]
David F Shanno. “Conditioning of quasi-Newton methods for function minimization”. In:
425"
REFERENCES,0.9405940594059405,"Mathematics of computation 24.111 (1970), pp. 647–656.
426"
REFERENCES,0.9422442244224423,"[59]
Avram Sidi. “Convergence and stability properties of minimal polynomial and reduced rank
427"
REFERENCES,0.9438943894389439,"extrapolation algorithms”. In: SIAM Journal on Numerical Analysis 23.1 (1986), pp. 197–209.
428"
REFERENCES,0.9455445544554455,"[60]
Avram Sidi. “Efficient implementation of minimal polynomial and reduced rank extrapolation
429"
REFERENCES,0.9471947194719472,"methods”. In: Journal of Computational and Applied Mathematics 36.3 (1991), pp. 305–337.
430"
REFERENCES,0.9488448844884488,"[61]
Avram Sidi. “Extrapolation vs. projection methods for linear systems of equations”. In: Journal
431"
REFERENCES,0.9504950495049505,"of Computational and Applied Mathematics 22.1 (1988), pp. 71–88.
432"
REFERENCES,0.9521452145214522,"[62]
Avram Sidi. “Minimal polynomial and reduced rank extrapolation methods are related”. In:
433"
REFERENCES,0.9537953795379538,"Advances in Computational Mathematics 43.1 (2017), pp. 151–170.
434"
REFERENCES,0.9554455445544554,"[63]
Avram Sidi. Vector extrapolation methods with applications. SIAM, 2017.
435"
REFERENCES,0.9570957095709571,"[64]
Avram Sidi. “Vector extrapolation methods with applications to solution of large systems of
436"
REFERENCES,0.9587458745874587,"equations and to PageRank computations”. In: Computers & Mathematics with Applications
437"
REFERENCES,0.9603960396039604,"56.1 (2008), pp. 1–24.
438"
REFERENCES,0.9620462046204621,"[65]
Avram Sidi and Jacob Bridger. “Convergence and stability analyses for some vector extrapola-
439"
REFERENCES,0.9636963696369637,"tion methods in the presence of defective iteration matrices”. In: Journal of Computational
440"
REFERENCES,0.9653465346534653,"and Applied Mathematics 22.1 (1988), pp. 35–61.
441"
REFERENCES,0.966996699669967,"[66]
Avram Sidi and Yair Shapira. “Upper bounds for convergence rates of acceleration methods
442"
REFERENCES,0.9686468646864687,"with initial iterations”. In: Numerical Algorithms 18.2 (1998), pp. 113–132.
443"
REFERENCES,0.9702970297029703,"[67]
Andrzej Stachurski. “Superlinear convergence of Broyden’s bounded θ-class of methods”. In:
444"
REFERENCES,0.971947194719472,"Mathematical Programming 20.1 (1981), pp. 196–212.
445"
REFERENCES,0.9735973597359736,"[68]
Alex Toth and CT Kelley. “Convergence analysis for Anderson acceleration”. In: SIAM Journal
446"
REFERENCES,0.9752475247524752,"on Numerical Analysis 53.2 (2015), pp. 805–819.
447"
REFERENCES,0.976897689768977,"[69]
Homer F Walker and Peng Ni. “Anderson acceleration for fixed-point iterations”. In: SIAM
448"
REFERENCES,0.9785478547854786,"Journal on Numerical Analysis 49.4 (2011), pp. 1715–1735.
449"
REFERENCES,0.9801980198019802,"[70]
Zengxin Wei et al. “The superlinear convergence of a modified BFGS-type method for uncon-
450"
REFERENCES,0.9818481848184818,"strained optimization”. In: Computational optimization and applications 29 (2004), pp. 315–
451"
REFERENCES,0.9834983498349835,"332.
452"
REFERENCES,0.9851485148514851,"[71]
Hiroshi Yabe, Hideho Ogasawara, and Masayuki Yoshino. “Local and superlinear convergence
453"
REFERENCES,0.9867986798679867,"of quasi-Newton methods based on modified secant conditions”. In: Journal of Computational
454"
REFERENCES,0.9884488448844885,"and Applied Mathematics 205.1 (2007), pp. 617–632.
455"
REFERENCES,0.9900990099009901,"[72]
Hiroshi Yabe and Naokazu Yamaki. “Local and superlinear convergence of structured quasi-
456"
REFERENCES,0.9917491749174917,"Newton methods for nonlinear optimization”. In: Journal of the Operations Research Society
457"
REFERENCES,0.9933993399339934,"of Japan 39.4 (1996), pp. 541–557.
458"
REFERENCES,0.995049504950495,"[73]
Junzi Zhang, Brendan O’Donoghue, and Stephen Boyd. “Globally convergent type-I Anderson
459"
REFERENCES,0.9966996699669967,"acceleration for nonsmooth fixed-point iterations”. In: SIAM Journal on Optimization 30.4
460"
REFERENCES,0.9983498349834984,"(2020), pp. 3170–3197.
461"
